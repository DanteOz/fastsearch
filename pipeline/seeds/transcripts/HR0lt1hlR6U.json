{"text": " Welcome to lesson 10, which I've rather enthusiastically titled Wrapping Up Our CNN, but looking at how many things we want to cover, I've added a nearly to the end, and I'm not actually sure how nearly we'll get there. We'll see. We'll probably have a few more things to cover next week as well. I just wanted to remind you after hearing from a few folks during the week who are very sad that they're not quite keeping up with everything, that's totally okay. Don't worry. As I mentioned in lesson one, I'm trying to give you enough here to keep you busy until the next part two next year. So you can dive into the bits you're interested in and go back and look over stuff, and yeah, don't feel like you have to understand everything within a week of first hearing it. And also, if you're not putting in the time during the homework or you didn't put in the time during the homework in the last part, you know, expect to have to go back and recover things, particularly because a lot of the stuff we covered in part one, I'm kind of assuming that you're deeply comfortable with at this point, not because you're stupid if you're not, but just because it gives you the opportunity to go back and restudy it and practice and experiment until you are deeply comfortable. So yeah, if you're finding it whizzing along at a pace, that is because it is whizzing along at a pace. Also, it's covering a lot of more software engineering kind of stuff, which for the people who are practicing software engineers, you'll be thinking this is all pretty straightforward, and for those of you that are not, you'll be thinking, wow, there's a lot here. Part of that is because I think data scientists need to be good software engineers, so I'm trying to show you some of these things. But, you know, it's stuff which people can spend years learning, and so hopefully this is the start of a long process for you that haven't done software engineering before of becoming better software engineers, and there are some useful tips, hopefully. So to remind you, we're trying to recreate Fast AI and much of PyTorch from these foundations, and starting to make things even better, and today you'll actually see some bits. Well, in fact, you've already seen some bits that are going to be even better. I think the next version of Fast AI will have this new callback system, which I think is better than the old one, and today we're going to be showing you some new previously unpublished research which will be finding its way into Fast AI and maybe other libraries as well also. So we're going to try and stick to, and we will stick to using nothing but these foundations. And we're working through developing a modern CNN model, and we've got to the point where we've done our training loop at this point, and we've got a nice flexible training loop. So from here, the rest of it, when I say we're going to finish out a modern CNN model, it's not just going to be some basic getting by model, but we're actually going to endeavor to get something that is approximately state of the art on ImageNet in the next week or two. So that's the goal, and in our testing at this point, we're feeling pretty good about showing you some stuff that maybe hasn't been seen before on ImageNet results. So that's where we're going to try and head as a group. And so these are some of the things that we're going to be covering to get there. One of the things you might not have seen before in this section called optimization is LAM. The reason for this is that this was going to be some of the unpublished research we were going to show you, which is a new optimization algorithm that we've been developing. The framework is still going to be new, but actually the particular approach to using it was published by Google two days ago. So we've kind of been scooped there. So this is a cool paper, really great, and they introduce a new optimization algorithm called LAM, which we'll be showing you how to implement it very easily. And if you're wondering how we're able to do that so fast, it's because we've kind of been working on the same thing ourselves for a few weeks now. So then from next week, we'll start also developing a completely new FastAI module called FastAI.audio. So you'll be seeing how to actually create modules and how to write Jupyter documentation and tests. And we're going to be learning about audio, such as complex numbers and Fourier transforms, which if you're like me, at this point you're going, oh, what? No. Because I managed to spend my life avoiding complex numbers and Fourier transforms on the whole. But don't worry, it'll be okay. It's actually not at all bad, or at least the bits we need to learn about are not at all bad, and you'll totally get it even if you've never ever touched these before. We'll be learning about audio formats and spectrograms, doing data augmentation and things that aren't images, and some particular kinds of loss functions and architectures for audio. And, you know, as much as anything, it'll just be a great kind of exercise in, okay, I've got some different data type that's not in FastAI. How do I build up all the bits I need to make it work? Then we'll be looking at neural translation as a way to learn about sequence to sequence with attention models, and then we'll be going deeper and deeper into attention models looking at transformer. And it's even more fantastic, descendant transformer Excel. And then we'll wrap up our Python adventures with a deep dive into some really interesting vision topics, which is going to require building some bigger models. So we'll talk about how to build your own deep learning box, how to run big experiments on AWS with a new library we've developed called FastEC2. And then we're going to see exactly what happened last course when we did that UNet super resolution image generation, what are some of the pieces there. And we've actually got some really exciting new results to show you, which have been done in collaboration with some really cool partners. So I'm looking forward to showing you that. To give you a tip, generative video models is what we're going to be looking at. And then we'll be looking at some interesting different applications, device, cycleGAN, and object detection. And then Swift, of course. So the Swift lessons are coming together nicely, really excited about them. And we'll be covering as much of the same territory as we can. But obviously it'll be in Swift and it'll be in only two lessons, so it won't be everything. But we'll try to give you enough of a taste that you'll feel like you understand why Swift is important and how to get started with building something similar in Swift. And maybe building out the whole thing in Swift will take the next 12 months. Who knows? We'll see. So we're going to start today on 05A foundations. And what we're going to do is we're going to recover some of the software engineering and math basics that we were relying on last week, and going into a little bit more detail. Specifically, we'll be looking at callbacks and variance, and a couple of other Python concepts like Dunder special methods. If you're familiar with those things, feel free to skip ahead if you're watching the video until we get to the new material. But callbacks, as I'm sure you've seen, are super important for fast AI, and in general they're a really useful technique for software engineering. And great for researchers because they allow you to build things that you can quickly adjust and add things in and pull them out again. So really great for research as well. So what is a callback? Let's look at an example. So here's a function called f, which prints out hi. And I'm going to create a button. And I'm going to create this button using ipywidgets, which is a framework for creating GUI widgets in Python. So if I run, if I say w, then it shows me a button, which says click me. And I can click on it, and nothing happens. So how do I get something to happen? Well, what I need to do is I need to pass a function to the ipywidgets framework to say, please run this function when you click on this button. So ipywidget.doc says that there's a onClick method, which can register a function to be called when the button is clicked. So let's try running that method passing at f, my function. OK. So now nothing happened. It didn't run anything. But now if I click on here, oh, hi. So what happened is I told w that when a click occurs, you should call back to my f function and run it. So anybody who's done GUI programming will be extremely comfortable with this idea. And if you haven't, this will be kind of mind-bending. So f is a callback. It's not a particular class. It doesn't have a particular signature. It's not a particular library. It's a concept. It's a function that we treat as an object. So look, we're not calling the function. We don't have any parentheses after f. We're passing the function itself to this method. And it says, please call back to me when something happens. And in this case, it's when I click. OK. So there's our starting point. And these kinds of functions, these kinds of callbacks that are used in a GUI in particular framework when some event happens are often called events. So if you've heard of events, they're a kind of callback. And then callbacks are a kind of what we would call a function pointer. I mean, they can be much more general than that, as you'll see. But it's basically a way of passing in something to say, call back to this when something happens. Now, by the way, these widgets are really worth looking at if you're interested in building some analytical GUIs. Here's a great example from the Plotly documentation of the kinds of things you can create with widgets. And it's not just for creating applications for others to use. But if you want to experiment with different types of function or hyperparameters or explore some data you've collected, widgets are a great way to do that. And as you can see, they're very, very easy to use. In part one, you saw the image labeling stuff that was built with widgets like this. So that's how you can use somebody else's callback. How do we create our own callback? So let's create a callback. And the event that it's going to call back on is after a calculation is complete. So let's create a function called slow calculation. And it's going to do five calculations. It's going to add i squared to a result. And then it's going to take a second to do it, because we're going to add a sleep there. So this is kind of something like an epoch of deep learning. It's some calculation that takes a while. So if we call slow calculation, then it's going to take five seconds to calculate the sum of i squared. And there it's done it. So I'd really like to know how's it going, get some progress. So we could take that, and we could add something that you pass in a callback. And we just add one line of code that says, if there's a callback, then call it and pass in the epoch number. So then we could create a function called show progress that prints out, awesome, we finished epoch number epoch. And look, it takes a parameter, and we're passing a parameter. So therefore, we could now call slow calculation and pass in show progress, and it will call back to our function after each epoch. So this is our starting point for our callback. Now, what will tend to happen, you'll notice, with stuff that we do in fast AI. We'll start somewhere like this. For many of you, it's trivially easy. And at some point during the next hour or two, you might reach a point where you're feeling totally lost. And the trick is to go back, if you're watching the video, to the point where it was trivially easy and figure out the bit where you suddenly noticed you were totally lost and find the bit in the middle where you kind of missed a bit. Because we're going to just keep building up from trivially easy stuff, just like we did with that matrix multiplication. Right? So we're going to gradually build up from here and look at more and more interesting callbacks. But we're starting with this wonderfully short and simple line of code. So rather than defining a function just for the purpose of using it once, we can actually define the function at the point we use it using lambda notation. So lambda notation is just another way of creating a function. So rather than saying def, we say lambda. And then rather than putting in parentheses the arguments, we put them before a colon. And then we list the thing we want to do. So this is identical to the previous one. It's just a convenience for times where you want to define the callback at the same time that you use it. It can make your code a little bit more concise. What if you wanted to have something where you could define what exclamation to use in the string as well? So we've now got two things. We can't pass this show progress to slow calculation. Let's try it. Right? It tries to call back. And it calls, remember CB is now show progress. So it's passing show progress. And it's passing epoch as exclamation. And then epoch is missing. So that's an error. We've called a function with two arguments with only one. So we have to convert this into a function with only one argument. So lambda O is a function with only one argument. And this function calls show progress with a particular exclamation. Okay? So we've converted something with two arguments into something with one argument. We might want to make it really easy to allow people to create different progress indicators with different exclamations. So we could create a function called make show progress that returns that lambda. Okay? So now we could say make show progress. So we could do that here. Make show progress. Okay? And that's the same thing. All right? This is a little bit awkward. So generally you might see it done like this instead. You see this in fast AI all the time. We define the function inside it. Okay? But this is basically just the same as our lambda. And then we return that function. So this is kind of interesting because you might think of defining a function as being like a declarative thing that as soon as you define it that now it's part of the thing that's like compiled. If you see your C++, that's how they work. In Python that's not how they work. When you define a function, you're actually saying the same, basically the same as this, which is there's a variable with this name, which is a function. Right? And that's how come then we can actually take something that's passed to this function and use it inside here. Right? So this is actually every time we call make show progress, it's going to create a new function, underscore inner internally with a different exclamation. And so it will work the same as before. Okay? So this thing where you create a function that actually stores some information from the external context and like it can be different every time, that's called a closure. Okay? So it's a concept you'll come across a lot, particularly if you're a JavaScript programmer. So we could say f2 equals make show progress terrific. Right? And so that now contains that closure. So it actually remembers what exclamation you passed it. Okay? Because it's so often that you want to take a function that takes two parameters and turn it into a function that takes one parameter, Python and most languages have a way to do that, which is called partial function application. So the standard library functions has this thing called partial. So if you take call partial and you pass it a function and then you pass in some arguments for that function, it returns a new function that now just takes, which that parameter is always a given. So let's check it out. So we could run it like that or we could say f2 equals this partial function application. And so if I say f2 shift tab, then you can see this is now a function that just takes epoch. It just takes epoch because show progress took two parameters. We've already passed it one. So this now takes one parameter, which is what we need. So that's why we could pass that to as our callback. Okay. So we've seen a lot of those techniques already last week. Most of what we saw last week, though, did not use a function as a callback, but used a class as a callback. So we could do exactly the same thing, but pretty much any place you can use a closure, you can also use a class. Instead of storing it away inside the closure, some state, we can store our state, in this case the explanation, inside self, passing it into init. Right? So here's exactly the same thing as we saw before, but as a class. Dundacall is a special magic name, which will be called if you take an object, so in this case a progress showing callback object, and call it with parentheses. So if I go CB, hi, you see I'm taking that object and I'm treating it as if it's a function, and that will call Dundacall. If you've used other languages, like in C++, this is called a functor. More generally it's called a callable in Python. So it's a kind of something that a lot of languages have. All right, so now we can use that as a callback, just like before. All right, next thing to look at is for our callback is we're going to use star args and star star kw args, or otherwise known as quags. For those of you that don't know what these mean, let's create a function that takes star args and star star kw args and prints out args and kw args. So if I call that function, I could pass it 3a thing1 equals hello, and you'll see that all the things that are passed as positional arguments end up in a tuple called args, and all the things passed as keyword arguments end up as a dictionary called quags. That's literally all these things do. All right? And so PyTorch uses that, for example, when you create an nn.sequential, it takes what you pass in as a star args, right? You just pass them directly and it turns it into a tuple. So why do we use this? There's a few reasons we use it, but one of the common ways to use it is if you kind of want to wrap some other class or object, then you can take a bunch of stuff as star star quags and pass it off to some other functional object. We're getting better at this and we're removing a lot of the usages, but in the early days of Fast AI version 1, we actually were overusing quags. So quite often, we would kind of, there would be a lot of stuff that wasn't obviously in the parameter list of a function that ended up in quags and then we would pass it down to, I don't know, the PyTorch data loader initializer or something. And so we've been gradually removing those usages because like it's mainly most helpful for kind of quick and dirty throwing things together. In R, they actually use an ellipsis for the same thing. They kind of overuse it. Quite often, it's hard to see what's going on. You might have noticed in Matplotlib, a lot of times the thing you're trying to pass to Matplotlib isn't there in the shift tab. When you hit shift tab, it's the same thing they're using quags. So there are some downsides to using it, but there are some places you really want to use it. For example, take a look at this. Let's take Rewrite slow calculation, but this time we're going to allow the user to create a callback that is called before the calculation occurs and after the calculation that occurs. And the after calculation one's a bit tricky because it's going to take two parameters now. It's going to take both the epoch number and also what have we calculated so far, right? So we can't just call CB parentheses I. We actually now have to assume that it's got some particular methods. So here is, for example, a print step callback, right? Which before calculation just says I'm about to start and after calculation it says I'm done and there it's running. So in this case, this callback didn't actually care about the epoch number or about the value, right? And so it just has star args, star, star quags in both places. It doesn't have to worry about exactly what's being passed in because it's not using them. So this is quite a good kind of use of this is to basically create a function that's going to be used somewhere else and you don't care about one or more of the parameters or you want to make things more flexible. So in this case, we don't get an error saying, because if we remove this, which looks like we should be able to do because we don't use anything, but here's the problem. It tried to call before calc I. And before calc doesn't take an I, right? So if you put in both positional and keyword arguments, it'll always work everywhere. And so here we can actually use them. So let's actually use epoch and value to print out those details. So now you can see there it is printing them out. And in this case, I've put star, star quags at the end because maybe, you know, in the future there'll be some other things that are passed in and we want to make sure this doesn't break. So it kind of makes it more resilient. The next thing we might want to do with callbacks is to actually change something. So a couple of things that we did last week. One was we wanted to be able to cancel out of a loop to stop early. The other thing we might want to do is actually change the value of something. So in order to stop early, we could check. And also the other thing we might want to do is say, well, what if you don't want to define before calc or after calc? We wouldn't want everything to break. So we could actually check whether a callback's defined and only call it if it is. And we could actually check the return value and then do something based on the return value. So here's something which will cancel out of our loop if the value that's been calculated so far is over 10. So here we stop. Okay? What if you actually want to change the way the calculation's being done? So we could even change the way the calculation's being done by taking our calculation function, putting it into a class. And so now the value that it's calculated is an attribute of the class, and so now we could actually do something, a callback, that reaches back inside the calculator and changes it. Right? So this is going to double the result if it's less than three. So if we run this, right, we now actually have to call this because it's a class, but you can see it's giving a different value. And so we're also taking advantage of this in the callbacks that we're using. So this is kind of the ultimately flexible callback system. And so you'll see in this case we actually have to pass the calculator object to the callback. So the way we do that is we've defined a callback method here which checks to see whether it's defined, and if it is, it grabs it, and then it calls it, passing in the calculator object itself, so it's now available. And so what we actually did last week is we didn't call this callback, we called this done to call, which means we were able to do it like this. Okay? Now, you know, which do you prefer? It's kind of up to you, right? I mean, we had so many callbacks being called that I felt the extra noise of giving it a name was a bit messy. On the other hand, you might feel that calling a callback isn't something you expect done to call to do, in which case you can do it that way. So there's pros and cons, neither is right or wrong. Okay. So that's callbacks. We've been using dunder thingies a lot. Dunder thingies look like this, and in Python a dunder thingy is special somehow. Most languages kind of let you define special behaviors. For example, in C++ there's an operator keyword where if you define a function that says operator something like plus, you're defining the plus operator. So most languages tend to have like special magic names you can give things that make something a constructor or a destructor or operator. I like in Python that all of the magic names actually look magic. They all look like that, which I think is actually a really good way to do it. So the Python docs have a data model reference where they tell you about all these special method names. And you can go through and you can see what are all the special things you can get your method to do. Like you can override how it behaves with less than or equal to or et cetera, et cetera. There's a particular list I suggest you know, and this is the list. Okay. So you can go to those docs and see what these things do because we use all of these in this course. So here's an example. Here's a sloppy adder plus. You pass in some number that you're going to add up. And then when you add two things together, it will give you the result of adding them up, but it will be wrong by.01. And that is called Dunder add because that's what happens when you see plus. This is called Dunder init because this is what happens when an object gets constructed. And this is called Dunder repra because this is what gets called when you print it out. So now I can create a one adder and a two adder and I can plus them together and I can see the result. Okay. So that's kind of an example of how these special Dunder methods work. Okay. So that's a bit of that Python stuff. There's another bit of code stuff that I wanted to show you which you'll need to be doing a lot of, which is you need to be really good at browsing source code. If you're going to be contributing stuff to Fast AI or to the Fast AI for Swift for TensorFlow or just building your own more complex projects, you need to be able to jump around source code. Or even just to find out how PyTorch does something if you're doing some research, you need to really understand what's going on under the hood. This is a list of things you should know how to do in your editor of choice. Any editor that can't do all of these things is worth replacing with one that can. Most editors can do these things. Emacs can, Visual Studio Code can, Sublime can, and the editor I use most of the time, Vim, can as well. I'll show you what these things are in Vim. On the forums, there are already some topics saying how to do these things in other editors. If you don't find one that seems any good, feel free to create your own topic if you've got some tips about how to do these things or other useful things in your editor of choice. So I'm going to show you in Vim for no particular reason, just because I use Vim. So here's my editor, it's called Vim. One of the things I like about Vim is I can use it in a terminal, which I find super helpful because I'm working on remote machines all the time, and I would like to be as at least as productive in a terminal as I am on my local computer. And so the first thing you should be able to do is to jump to a symbol. A symbol would be like a class or a function or something like that. So for example, I might want to be able to jump straight to the definition of create CNN, but I can't quite remember the name of the function create CNN, so I would go colon tag create. I'm pretty sure it's create underscore something, so then I'd press tab a few times, and it would loop through. Ah, there it is, create CNN. And then I'd hit enter. So that's the first thing that your editor should do is it should make it easy to jump to a tag even if you can't remember exactly what it is. The second thing it should do is that you should be able to click on something like CNN learner and hit a button, which in Vim's case is control right square bracket, and it should take you to the definition of that thing. So okay, that's great, there's CNN learner. What's this thing called, a data bunch, right square bracket. Okay, there's data bunch. You'll also see that my Vim is folding things, classes and functions to make it easier for me to see exactly what's in this file. In some editors, this is called outlining. In some, it's called folding. Most editors should do this. Then there should be a way to go back to where you were before. In Vim, that's control T for going back up the tag stack. So here's my CNN learner. Here's my create CNN. And so you can see in this way, it makes it nice and easy to kind of jump around a little bit. Something I find super helpful is to also be able to jump into the source code of libraries I'm using. So for example, here's chiming normal. So I've got my Vim configured, so if I hit control right square bracket on that, it takes me to the definition of chiming normal in the PyTorch source code. And I find doc strings kind of annoying, so I have mine folded up by default, but I can always open them up. If you use Vim, the way to do that is to add additional tags for any packages that you want to be able to jump to. I'm sure most editors will do something pretty similar. Now that I've seen how chiming normal works, I can use the same control T to jump back to where I was in my fast AI source code. Then the only other thing that's particularly important to know how to do is to just do more general searches. So let's say I wanted to find all the places that I've used lambda, since we talked about lambda today. I have a particular thing I use called ACK. I can say ACK lambda, and here is a list of all of the places I've used lambda. And I could click on one, and it will jump to the code where it's used. Again, most editors should do something like that for you. So I find with that basic set of stuff, you should be able to get around pretty well. If you're a professional software engineer, I know you know all this. If you're not, hopefully you're feeling pretty excited right now to discover that editors can do more than you realized. And so sometimes people jump on our GitHub and say, I don't know how to find out what a function is that you're calling, because you don't list all your imports at the top of the screen. This is a great place where you should be using your editor to tell you. And in fact, one place that GUI editors can be pretty good is often if you actually just point at something, they will pop up something saying exactly where is that symbol coming from. I don't have that set up in Vim, so I just have to hit the right square bracket to see where something's coming from. Okay, so that's some tips about stuff that you should be able to do when you're browsing source code. And if you don't know how to do it yet, please Google or look at the forums and practice. Something else we were looking at a lot last week and you need to know pretty well is variance. So just a quick refresher on what variance is, or for those of you who haven't studied it before, here's what variance is. Variance is the average of how far away each data point is from the mean. So here's some data, right, and here's the mean of that data. And so the average distance for each data point from the mean is T, the data points, minus M, top mean. Oh, that's zero. That didn't work. Oh, well of course it didn't work. The mean is defined as the thing which is in the middle, right? So of course that's always zero. So we need to do something else that doesn't have the positives and negatives cancel out. So there's two main ways we fix it. One is by squaring each thing before we take the mean, like so. The other is taking the absolute value of each thing, so turning all the negatives and positives before we take the mean. So they're both common fixes for this problem. You can see though the first is now on a totally different scale, right? The numbers were like 1, 2, 4, 8, and this is 47. So we need to undo that squaring. So after we've squared, we then take the square root at the end. So here are two numbers that represent how far things are away from the mean, or in other words, how much do they vary? If everything's pretty close to similar to each other, those two numbers will be small. If they're wildly different to each other, those two numbers will be big. This one here is called the standard deviation, and it's defined as the square root of this one here, which is called the variance. And this one here is called the mean absolute deviation. You could replace this m mean with various other things like median, for example. So we have one outlier here, 18. So in the case of the one where we took a square in the middle of it, this number is higher, right? Because the square takes that 18 and makes it much bigger. So in other words, standard deviation is more sensitive to outliers than mean absolute deviation. So for that reason, the mean absolute deviation is very often the thing you want to be using, because in machine learning, outliers are more of a problem than a help a lot of the time. But mathematicians and statisticians tend to work with standard deviation rather than mean absolute deviation, because it makes their math proofs easier, and that's the only reason. They'll tell you otherwise, but that's the only reason. So the mean absolute deviation is really underused, and it actually is a really great measure to use, and you should definitely get used to it. There's a lot of places where I kind of noticed that replacing things involving squareds with things involving absolute values, the absolute value things just often work better. It's a good tip to remember that there's this kind of long-held assumption. We have to use the squared thing everywhere, but it actually often doesn't work as well. This is our definition of variance. Notice that this is the same. So this is written in math. This written in math looks like this, and it's another way of writing the variance. It's important because it's super handy, and it's super handy because in this one here, we have to go through the whole data set once. Once to calculate the mean of the data, and then a second time to get the squareds of the differences. This is really nice because in this case, we only have to keep track of two numbers, the squareds of the data and the sum of the data. And as you'll see shortly, this kind of way of doing things is generally therefore just easier to work with. So even though this is kind of the definition of the variance that makes intuitive sense, this is the definition of variance that you normally want to implement. And so there it is in math. The other thing we see quite a bit is covariance and correlation. So if we take our same data set, let's now create a second data set, which is double t times a little bit of random noise. So here's that plotted. Let's now look at the difference between each item of t and its mean and multiply it by each item of u and its mean. So there's those values. And let's look at the mean of that. So what's this number? So it's the average of the difference of how far away the x value is from the mean of the x value is, the x's, multiplied by each difference between the y value and how far away from the y mean it is. Let's compare this number to the same number calculated with this data set, where this data set is just some random numbers compared to v. And let's now calculate the exact same product, the exact same mean. This number is much smaller than this number. Why is this number much smaller? So if you think about it, if these are kind of all lined up nicely, then every time it's higher than the average on the x-axis, it's also higher than the average on the y-axis. So you have two big positive numbers, and vice versa, two big negative numbers. So in either case, you end up with, when you multiply them together, a big positive number. So this is adding up a whole bunch of big positive numbers. So in other words, this number tells you how much these two things vary in the same way, kind of how lined up are they on this graph. And so this one, when one is big, the other is not necessarily big. When one is small, the other is not necessarily very small. So this is the covariance. And you can also calculate it in this way, which might look somewhat similar to what we saw before with our different variance calculation. And again, this is kind of the easier way to use it. So as I say here, from now on, I don't want you to ever look at an equation or type an equation in LaTeX without typing it in Python, calculating some values, and plotting them. Because this is the only way we get a sense in here of what these things mean. And so in this case, we're going to take our covariance, and we're going to divide it by the product of the standard deviations. And this gives us a new number. And this is called correlation, or more specifically, Pearson correlation coefficient. Yeah, so we don't cover covariance and Pearson correlation coefficient too much in the course, but it is one of these things which it's often nice to see how things vary. But remember, it's telling you really about how things vary linearly. So if you want to know how things vary nonlinearly, you have to create something called a neural network and check the loss and the metrics. But it's kind of interesting to see also how variance and covariance, you can see they're much the same thing. Where else one of them, in fact, basically you can think of it this way. One of them is e of x squared. In other words, x and x are kind of the same thing. It's e of x times x. Where else this is two different things, e of x times y. And so rather than having here we had e of x squared here and e of x squared here. If you replace the second x with a y, you get that, and you get that. So they're literally the same thing. And then again here, if x and x are the same, then this is just sigma squared. So the last thing I want to quickly talk about a little bit more is softmax. This was our final log softmax definition from the other day. And this is the formula, the same thing as an equation. And this is our cross entropy loss, remember. So these are all important concepts we're going to be using a lot. So I just wanted to kind of clarify something that a lot of researchers that are published in big name conferences get wrong, which is when should you and shouldn't you use softmax. So this is our softmax page from our entropy example spreadsheet, where we were looking at cat, dog, plane, fish building. And so we had various outputs. This is just the activations that we might have gotten out of the last layer of our model. And this is just e to the power of each of those activations. And this is just the sum of all of those e to the power ofs. And then this is e to the power of divided by the sum, which is softmax. And of course, they all add up to one. This is like some image number one that gave these activations. Here's some other image number two, which gave these activations, which are very different to these. But the softmaxes are identical. That and that are identical. So that's weird. How has that happened? Well, it's happened because in every case, the e to the power of this divided by the sum of the e to the power ofs ended up in the same ratio. So in other words, even though fish is only 0.63 here, but it's 2 here, once you take e to the power of, it's the same percentage of the sum. And so we end up with the same softmax. Why does that matter? Well, in this model, it seems like being a fish is associated with having an activation of maybe like 2-ish. And this is only like 0.6-ish. So maybe there's no fish in this. But what's actually happened is there's no cats or dogs or planes or fishes or buildings. So in the end then, because softmax has to add to 1, it has to pick something. So it's fish that comes through. And what's more is because we do this e to the power of, the thing that's a little bit higher, it pushes much higher because it's exponential. So softmax likes to pick one thing and make it big. And they have to add something. And they have to add up to 1. So the problem here is that I would guess that maybe image 2 doesn't have any of these things in it. And we had to pick something. So it said, oh, I'm pretty sure there's a fish. Or maybe the problem actually is that this image had a cat and a fish and a building. But again, because softmax, they have to add to 1. And one of them is going to be much bigger than the others. So I don't know exactly which of these happened. But it's definitely not true that they both have an equal probability of having a fish in them. So to put this another way, softmax is a terrible idea unless you know that every one of your, if you're doing image recognition, every one of your images, or if you're doing audio or tabular or whatever, every one of your items has one, no more than one, and definitely at least one example of the thing you care about in it. Because if it doesn't have any of cat, dog, plane, fish, or building, it's still going to tell you with high probability that it has one of those things. Even if it has more than just one of cat, dog, plane, fish, or building, it'll pick one of them and tell you it's pretty sure it's got that one. So what do you do if there could be no things or there could be more than one of these things? Well, instead, you use binomial, regular old binomial, which is e to the x divided by 1 plus e to the x. It's exactly the same as softmax if your two categories are, has the thing and doesn't have the thing, because they're like p and 1 minus p. So you can convince yourself of that during the week. So in this case, let's take image one and let's go 1.02. 1.02 divided by 1 plus 1.02. And ditto for each of our different ones. And then let's do the same thing for image two. And you can see now the numbers are different, as we would hope. And so for image one, it's kind of saying, oh, it looks like there might be a cat in it. If we assume 0.5 is a cutoff, there's probably a fish in it. And it seems likely that there's a building in it. Whereas for image two, it's saying, I don't think there's anything in there but maybe a fish. And this is what we want. And so when you think about it, like for image recognition, probably most of the time, you don't want softmax. So why do we always use softmax? Because we all grew up with ImageNet. And ImageNet was specifically curated, so it only has one of the classes in ImageNet in it. And it always has one of those classes in it. An alternative, if you want to be able to handle the what if none of these classes are in a case, is you could create another category called background or doesn't exist or null or missing. So let's say you created this missing category. So now there's six, cat, dog, plane, fish, building or missing. Nothing. A lot of researchers have tried that. But it's actually a terrible idea and it doesn't work. And the reason it doesn't work is because to be able to successfully predict missing, the penultimate layer activations have to have the features in it that is what a not cat, dog, plane, fish, fish or building looks like. So how do you describe a not cat, dog, plane, fish or building? What are the things that would activate high? Is it shininess? Is it fur? Is it sunshine? Is it edges? No. There's none of those things. There is no set of features that when they're all high, is clearly a not cat, dog, plane, fish or building. So that's just not a kind of object. So a neural net can kind of try to hack its way around it by creating a negative model of every other single type and create a kind of not one of any of those other things. But that's very hard for it. Whereas creating simply a binomial, does it or doesn't it have this for every one of the classes, is really easy for it, right? Because it just doesn't have a cat. Yes or no? Does it have a dog? Yes or no? And so forth. So lots and lots of well regarded academic papers make this mistake. So look out for it. And if you do come across an academic paper that's using Softmax and you think, does that actually work with Softmax? And you think maybe the answer is no. Try replicating it without Softmax. And you may just find you get a better result. An example of somewhere where Softmax is obviously a good idea or something like Softmax is obviously a good idea. Language modeling. What's the next word? It's definitely at least one word. It's definitely not more than one word. So you want Softmax. So I'm not saying Softmax is always a dumb idea. But it's often a dumb idea. So that's something to look out for. Next thing I want to do is I want to build a learning rate finder. And to build a learning rate finder, we need to use this test callback kind of idea, this ability to stop somewhere. Problem is, as you may have noticed, this I want to stop somewhere callback wasn't working in our new refactoring where we created this runner class. And the reason it wasn't working is because we were turning true to mean cancel. But even after we do that, it still goes on to do the next batch. And even if we set self.stop, even after we do that, it'll go on to the next epoch. So to actually stop it, you would have to return false from every single callback that's checked to make sure it really stops. Or you would have to add something that checks for self.stop in lots of places. It would be a real pain. It's also not as flexible as we would like. So what I want to show you today is something which I think is really interesting, which is using the idea of exceptions as a kind of control flow statement. You may think of exceptions as just being a way of handling errors. But actually, exceptions are a very versatile way of writing very neat code that will be very helpful for your users. Let me show you what I mean. So let's start by just grabbing our MNIST data set as before and creating our data bunches before. And here's our callback as before and our train eval callback as before. But there's a couple of things I'm going to do differently. The first is, and this is a bit unrelated, but I think it's a useful refactoring, is previously inside runner in Dunder Call, we went through each callback in order. And we checked to see whether it existed, whether that particular method exists in that callback. And if it was, we called it and checked whether it returns true or false. It actually makes more sense for this to be inside the callback class, not inside the runner class. Because by putting it into the callback class, the callback class is now taking a, has a Dunder Call which takes a callback name, and it can do this stuff. And what it means is that now your users who want to create their own callbacks, let's say they wanted to create a callback that printed out the callback name for every callback every time it was run. Or let's say they wanted to add a break point, like a set trace, that happened every time the callback was run. Or they could now create their own inherit from callback and actually replace Dunder Call itself with something that added this behavior they want. Or they could add something that looks at three or four different callback names and attaches to all of them. So this is a nice little extra piece of flexibility. It's not the key thing I wanted to show you, but it's an example of a nice little refactoring. The key thing I wanted to show you is that I've created three new types of exception. So an exception in Python is just a class that inherits from exception. And most of the time, you don't have to give it any other behavior. So to create a class that's just like its parent, but it just has a new name and no more behavior, you just say pass. So pass means this has all the same attributes and everything as the parent. But it's got a different name. So why do we do that? Well, you might get a sense from the names cancelTrainException, cancelEpochException, cancelBatchException. The idea is that we're going to let people's callbacks cancel anything. It'll cancel at one of these levels. So if they cancel a batch, it will keep going with the next batch, but not finish this one. If they cancel an epoch, it'll keep going with the next epoch. That will cancel this one. cancelTrain will stop the training altogether. So how would cancelTrainException work? Well, here's the same runner where you had before. But now, fit, we already had try finally to make sure that our after fit and removeLearner happened, even if there was an exception. I've added one line of code, except cancelTrainException. And if that happens, then optionally, it could call some after cancelTrainCallback. But most importantly, no error occurs. It just keeps on going to the finally block and will elegantly and happily finish up. So we can cancel training. So now, our test callback can after step, it'll just print out what step we're up to. And if it's greater than or equal to 10, we'll raise cancelTrainException. And so now, when we say run.fit, it just prints out up to 10 and stops. There's no stack trace. There's no error. This is using an exception as a control flow technique, not as an error handling technique. So another example, inside all batches, I go through all my batches in a try block, except if there's a cancelEpochException, in which case I optionally call an after cancelEpoch callback and then continue to the next epoch. Or inside one batch, I try to do all the stuff for a batch, except if there's a cancelBatchException, I will optionally call the after cancelBatch callback and then continue to the next batch. So this is like a super neat way that we've allowed any callback writer to stop any one of these three levels of things happening. So in this case, we're using cancelTrainException to stop training. So we can now use that to create a learning rate finder. So the basic approach of the learning rate finder is that there's something in beginBatch which, just like our parameter scheduler, is using an exponential curve to set the learning rate. So this is identical to the program scheduler. And then after each step, it checks to see whether we've done more than the maximum number of iterations, which is defaulted to 100, or whether the loss is much worse than the best we've had so far. And if either of those happens, we will raise cancelTrainException. So to be clear, this neat exception-based approach to control flow isn't being used in the FastAI version 1 at the moment. But it's very likely that FastAI 1.1 or 2 will switch to this approach, because it's just so much more convenient and flexible. And then assuming we haven't canceled, just see if the loss is better than our best loss. And if it is, then set best loss to the loss. So now we can create a learner. We can add the LR find. We can fit. And you can see that it only does less than 100 epochs before it stops, because the loss got a lot worse. And so now we know that we want something about there for our learning rate. OK, so now we have a learning rate finder. So let's go ahead and create a CNN, and specifically a SCUDA CNN. So we'll keep doing the same stuff we've been doing, get our MNIST data, normalize it. Here's a nice little refactoring, because we very often want to normalize with this data set and normalize both data sets using this data sets mean and standard deviation. Let's create a function called normalize2, which does that and returns the normalized training set and the normalized validation set. So we can now use that. Make sure that it's behaved properly. That looks good. Create our data bunch. And so now we're going to create a CNN model. And the CNN is just a sequential model that contains a bunch of stride-to convolutions. And remember, the input's 28 by 28. So after the first, it'll be 14 by 14, then 7 by 7, then 4 by 4, then 2 by 2. Then we'll do our average pooling, flatten it, and a linear layer. And then we're done. Now remember, our original data is vectors of length 768. They're not 28 by 28. So we need to do a x.view one channel by 28 by 28, because that's what nn.conv2d expects. And then minus 1, the batch size remains whatever it was before. So we need to somehow include this function in our nn.sequential. PyTorch doesn't support that by default. We could write our own class with a forward function. But nn.sequential is convenient for lots of ways. It has a nice representation. You can do all kinds of customizations with it. So instead, we create a layer called lambda, an nn.module called lambda. You just pass it a function. And the forward is simply to call that function. And so now we can say lambda mnist resize. And that will call that function to be called. And here, lambda flatten simply calls this function to be called, which removes that 1,1 axis at the end after the adaptive average pooling. So now we've got a CNN model. We can grab our callback functions and our optimizer and our runner. And we can run it. And six seconds later, we get back one epoch's result. So at this point now, getting a bit slow. So let's make it faster. So let's use CUDA. Let's pop it on the GPU. So we need to do two things. We need to put the model on the GPU, which specifically means the model's parameters on the GPU. So remember, a model contains two kinds of numbers. Parameters, they're the things that you're updating, the things that it stores. And there's the activations. There's the things that it's calculating. So it's the parameters that we need to actually put on the GPU. And the inputs to the model and the loss function. So in other words, the things that come out of the data loader, we need to put those on the GPU. How do we do that? With a callback, of course. So here's a CUDA callback. When you initialize it, you pass it a device. And then when you begin fitting, you move the model to that device. So model.2.2 is part of PyTorch. It moves something with parameters or a tensor to a device. And you can create a device by calling torch.device, pass it the string CUDA, and whatever GPU number you want to use. If you only have one GPU, it's device 0. Then when we begin a batch, let's go back and look at our runner. So now, when we begin a batch, we've put x batch and y batch inside self.xb and self.yb. So that means we can change them. So let's set the runner's xb and the runner's yb to whatever they were before, but move to the device. So that's it. That's going to run everything on CUDA. That's all we need. This is kind of flexible, because we can put things on any device we want. Maybe more easily is just to call this once, which is torch.cuda.setdevice. And you don't even need to do this if you've only got one GPU. And then everything by default will now be sent to that device. And then instead of saying.2 device, we can just say.cuda. And so since we're doing pretty much everything with just one GPU for this course, this is the one we're going to export. So just model.cuda, xb.cuda, yb.cuda. So that's our CUDA callback. So let's add that to our callback functions, grab our model and our runner, and fit. And now we can do three epochs in five seconds versus one epoch in six seconds. So that's a lot better. And for a much deeper model, it'll be dozens of times faster. So this is literally all we need to use CUDA. So that was nice and easy. Now we want to make it easier to create different kinds of architectures, make things a bit easier. So the first thing we should do is recognize that we go conv relu a lot. So let's pop that into a function called conv2d that just goes conv relu. Since we use a kernel size of three and a stride of two in this MNIST model a lot, let's make those the defaults. Also, this model we can't reuse for anything except MNIST because it has a MNIST resize at the start. So we need to remove that. So if we're going to remove that, something else is going to have to do the resizing. And of course, the answer to that is a callback. So here's a callback which transforms the independent variable, the x, for a batch. And so you pass it some transformation function, which it stores away, and then begin batch simply replaces the batch with the result of that transformation function. So now we can simply append another callback, which is the partial function application of that callback with this function. And this function is just to view something at 1 by 28 by 28. And you can see here we've used the trick we saw earlier of using underscore inner to define a function and then return it. So this is something which creates a new view function that views it in this size. So for those of you that aren't that comfortable with closures and partial function application, this is a great piece of code to study, experiment, make sure that you feel comfortable with it. So using this approach, we now have the MNIST view resizing as a callback, which means we can remove it from the model. So now we can create a generic getCNNModel function that returns a sequential model containing some arbitrary set of layers, containing some arbitrary set of filters. So we're going to say, OK, this is the number of filters I have per layer, 8, 16, 32, 32. And so here is my getCNNLayers. And the last few layers is the average pooling, flattening, and the linear layer. The first few layers is for every one of those filters, length of the filters. It's a Conv2D from that filter to the next one. And then what's the kernel size? The kernel size depends. It's a kernel size of 5 for the first layer or 3 otherwise. Why is that? Well, the number of filters we had for the first layer was 8. And that's a pretty reasonable starting point for a small model to start with 8 filters. And remember, our image had a single channel. And I imagine if we had a single channel and we were using 3 by 3 filters. So as that convolution kernel scrolls through the image, at each point in time, it's looking at a 3 by 3 window. And it's just one channel. So in total, there's nine input activations that it's looking at. And then it spits those into a dot product with, sorry, it spits those into eight dot products. So a matrix multiplication, I should say. 8 by 9. And out of that will come a vector of length 8. Right? Because we said we wanted 8 filters. So that's what a convolution does. And this seems pretty pointless because we started with nine numbers and we ended it with eight numbers. So all we're really doing is just reordering them. It's not really doing any useful computation. So there's no point making your first layer basically just shuffle the numbers into a different order. So what you'll find happens in, for example, most ImageNet models, most ImageNet models, they're a little bit different because they have three channels. So it's actually 3 by 3 by 3, which is 27. But it's still kind of like, quite often with ImageNet models, the first layer will be like 32 channels. So like going from 27 to 32 is literally losing information. So most ImageNet models, they actually make the first layer 7 by 7, not 3 by 3. And so for a similar reason, we're going to make our first layer 5 by 5. So we'll have 25 inputs for our eight outputs. So this is the kind of things that you want to be thinking about when you're designing or reviewing an architecture is like how many numbers are actually going into that little dot product that happens inside your CNN kernel. OK, so that's something which can give us a CNN model. So let's pop it all together into a little function that just grabs an optimization function, grabs an optimizer, grabs a learner, grabs a runner. And at this point, if you can't remember what any of these things does, remember we've built them all by hand from scratch. So go back and see what we wrote. There's no magic here. And so let's look if we say getCNNModel passing in 8, 16, 32, 32. Here you can see 8, 16, 32, 32. Here's our 5 by 5. The rest are 3 by 3. They all have a stride of 2 and then a linear layer. And then trade. OK, so at this point, we've got a fairly general simple CNN creator that we can fit. And so let's try to find out what's going on inside. How do we make this number higher? How do we make it train more stably? How do we make it train more quickly? Well, we really want to see what's going on inside. We know already that different ways of initializing changes the variance of different layers. How do we find out if it's saturating somewhere, if it's too small, if it's too big? What's going on? So what if we replace nn.sequential with our own sequential model class? And if you remember back, we've already built our own sequential model class before. And it just had these two lines of code plus return. So let's keep the same two lines of code, but also add two more lines of code that grabs the mean of the outputs and the standard deviation of the outputs and saves them away inside a bunch of lists. So here's a list for every layer for means and a list for every layer for standard deviations. So let's calculate the mean and standard deviations, pop them inside those two lists. And so now it's a sequential model that also keeps track of what's going on, the telemetry of the model. So we can now create it in the same way as usual, fit it in the same way as usual, but now our model has two extra things in it. It has an act means and an act standard deviations. So let's plot the act means for every one of those lists that we had. And here it is, right? Here's all of the different means. And you can see it looks absolutely awful. What happens early in training is every layer, the means get exponentially bigger until they suddenly collapse. And then it happens again, and it suddenly collapses, and it happens again, and it suddenly collapses until eventually it kind of starts training. So you might think, well, it's eventually training, so isn't this okay? But my concern would be this thing where it kind of falls off a cliff, there's lots of parameters in our model, right? Are we sure that all of them are getting back into reasonable places? Or is it just that a few of them have got back into a reasonable place? Like maybe the vast majority of them have like zero gradients at this point. I don't know. It seems very likely that this awful training profile early in training is leaving our model in a really sad state. That's my guess. And we're gonna check it to see later. But for now, we're just gonna say, let's try to make this not happen. And let's also look at the standard deviations, and you see exactly the same thing. This just looks really bad. So let's look at just the first 10 means, and they all look okay. They're all pretty close-ish to zero, which is about what we want. But more importantly, let's look at the standard deviations for the first 10 batches. And this is a problem. The first layer has a variance not too far, the first standard deviation not too far away from one, but then not surprisingly, the next layer is lower. The next layer is lower. As we would expect, because the first layer is less than one, the following layers are getting exponentially further away from one, until the last layer is really close to zero. So now we can kind of see what was going on here, is that our final layers were getting basically no activations, they were basically getting no gradients. So gradually it was moving into spaces where they actually at least had some gradient. But by the time they kind of got there, the gradient was so fast that they were kind of falling off a cliff and having to start again. So this is the thing we're gonna try and fix. And we think we already know how to, we can use some initialization. Yes, Rachel. Did you say that if we went from 27 numbers to 32, that we were losing information? Could you say more about what that means? I guess we're not losing information where that was poorly said. We're wasting information, I guess. Where like if you start with 27 numbers and you do some matrix multiplication and end up with 32 numbers, you're now taking more space for the same information you started with. And the whole point of a neural network layer is to pull out some interesting features. So you would expect to have less total activations going on because you're trying to say, oh, in this area, I've kind of pulled this set of pixels down into something that says how far this is or how much of a diagonal line does this have or whatever. So increasing the number of, the actual number of activations we have for a particular position is a total waste of time. We're not doing any useful, we're wasting a lot of calculation. We can talk more about that on the forum if that's still not clear. Okay, so this is, so this idea of creating telemetry for your model is really vital. This approach to doing it where you actually write a whole new class that only can do one kind of telemetry is clearly stupid. And so we clearly need a better way to do it. And what's the better way to do it? It's callbacks, of course. Except we can't use our callbacks because we don't have a callback that says when you calculate this layer, callback to our code. We have no way to do that. So we actually need to use a feature inside PyTorch that can callback into our code when a layer is calculated, either the forward pass or the backward pass. And for reasons I can't begin to imagine, PyTorch doesn't call them callbacks, they're called hooks. But it's the same thing. It's a callback, okay? And so we can say, for any module, we can say register forward hook and pass in a function. This is a callback. This is a callback that will be called when this module's forward pass is calculated. Or you could say register backward hook and that will call this function when this module's backward pass is calculated. So to replace that previous thing with hooks, we can simply create a couple of global variables to store our means and standard deviations for every layer. We can create a function to callback to to calculate the mean and standard deviation. And if you Google for the documentation for register forward hook, you will find that it will tell you that the callback will be called with three things. The module that's doing the callback, the input to the module, and the output of that module, either the forward or the backward pass is appropriate. In our case, it's the output we want. And then we've got a fourth thing here, because this is the layer number we're looking at, and we used partial to connect the appropriate closure with each layer. So once we've done that, we can call fit and we can do exactly the same thing. Okay, so this is the same thing, just much more convenient. And because this is such a handy thing to be able to do, Fast.ai has a hook class, so we can create our own hook class now, which allows us to, rather than having this kind of messy global state, we can instead put the state inside the hook. So let's create a class called hook that when you initializes it, it registers a forward hook on some function, right? And what it's gonna do is it's gonna recall back to this object. So we pass in self with the partial. And so that way we can get access to the hook. We can pop inside it our two empty lists when we first call this to store away our means and standard deviations. And then we can just append our means and standard deviations. So now we just go hooks equals hook for layer in children of my model. I'm gonna grab the first two layers because I don't care so much about the linear layers. It's really the conf layers that are most interesting. And so now this does exactly the same thing. Since we do this a lot, let's put that into a class too called hooks. So here's our hooks class, which simply calls hook for every module in some list of modules. Now, something to notice is that when you're done using a hooked module, you should call hook.remove because otherwise if you keep registering more hooks on the same module, they're all gonna get called and eventually you're gonna run out of memory. So one thing I did in our hook class was I created a Dunder Dell. This is called automatically when Python cleans up some memory. So when it's done with your hook, it will automatically call remove, which in turn will remove the hook. So I then have a similar thing in hooks. So when hooks is done, it calls self.remove, which in turn goes through every one of my registered hooks and removes them. You'll see that somehow I'm able to go for H in self, but I haven't registered any kind of iterator here. And the trick is I've created something called a list container just above, which is super handy. It basically defines all the things you would expect to see in a list using all of the various special Dunder methods. And then some, it actually has some of the behavior of NumPy as well. We're not allowed to use NumPy in our foundations, so we use this instead. And this actually also works a bit better than NumPy for this stuff, because NumPy does some weird casting and weird edge cases. So for example, with this list container, it's got Dunder get item. So that's the thing that gets called when you call something with square brackets, right? So if you index into it with an int, then we just pass that off to the enclosed list, because we gave it a list to enclose. If you send it a list of bools, like false, false, false, false, false, true, false, then it will return all of the things where that's true. Or you can index it into it with a list, in which case it will return all of the index, the things that are indexed by that list, for instance. And it's got a length, which just passes off to length, and an iterator that passes off to iterator, and so forth. And then we've also defined the representation for it, such that if you print it out, it just prints out the contents, unless there's more than 10 things in it, in which case it shows dot, dot, dot. So with a nice little base class like this, so you can create really useful little base classes in much less than a screen full of code. And then we can use them, and we will use them everywhere from now on. So now we've created our own listy class that has hooks in it. And so now we can just use it like this. We can just say hooks equals books, everything in our model, with that function we had before, appendStats. We can print it out to see all the hooks. We can grab a batch of data. So now we've got one batch of data, and check its mean and standard deviation is about zero, one, as you would expect. We can pass it through the first layer of our model, model zero is the first layer of our model, which is the first convolution. And our mean is not quite zero, and our standard deviation is quite a lot less than one, as we kind of know what's gonna happen. So now we'll just go ahead and initialize it with kai-ming, and after that, variance is quite close to one. And our standard deviation, sorry, and our mean as expected is quite close to 0.5, because of the relu. So now we can go ahead and create our hooks, and do a fit, and we can plot the first 10 means and standard deviations, and then we can plot all the means and standard deviations, and there it all is. And this time we're doing it after we've initialized all the layers of our model. And as you can see, we don't have that awful exponential crash, exponential crash, exponential crash. So this is looking much better, and you can see early on in training, our variances all look, our standard deviations all look much closer to one. So this is looking super hopeful. I've used a width block. A width block is something that will create this object, give it this name, and when it's finished, it will do something. The something it does is to call your dunderExit method here, which will dot remove. So here's a nice way to ensure that things are cleaned up. For example, your hooks are removed. So that's why we have a dunderEnter, that's what happens when you start the width block, dunderExit when you finish the width block. So this is looking very hopeful, but it's not quite what we wanted to know. Really the concern was, is does this actually do something bad? Is it actually, or does it just train fine afterwards? So something bad really is more about how many of the activations are really, really small? How well is it actually getting everything activated nicely? So what we could do is we could adjust our appendStats. So not only does it have a mean and a standard deviation, but it's also got a histogram. So we could create a histogram of the activations, pop them into 40 bins between zero and 10. We don't need to go underneath zero because we have a relu, so we know that there's none underneath zero. So let's again run this. We will use our chiming initialization. And what we find is that even with that, if we make our learning rate really high, 0.9, we can still get this same behavior. And so here's plotting the entire histogram. And I should say thank you to Stefano for the original code here from our San Francisco study group to plot these nicely. So you can see this kind of grow, collapse, grow, collapse, grow, collapse thing. The biggest concern for me though is this yellow line at the bottom. The yellow line, yellow is where most of the histogram is. I actually, what I really care about is how much yellow is there. So let's say the first two histogram bins are zero or nearly zero. So let's get the sum of how much is in those two bins and divide by the sum of all of the bins. And so that's gonna tell us what percentage of the activations are zero or nearly zero. And let's plot that for each of the first four layers. And you can see that in the last layer, it's just as we suspected. Over 90% of the activations are actually zero. So if you were training your model like this, right, it could eventually look like it's training nicely without you realizing that 90% of your activations were totally wasted. And so you're never gonna get great results by wasting 90% of your activations. So let's try and fix it. Let's try and be able to train at a nice high learning rate and not have this happen. And so the trick is, is we're gonna try a few things, but the main one is we're gonna use our better ReLU. And so we've created a generalized ReLU class where now we can pass in things like an amount to subtract from the ReLU. Because remember we thought subtracting half from the ReLU might be a good idea. We can also use leaky ReLU. And maybe things that are too big are also a problem. So let's also optionally have a maximum value. So in this generalized ReLU, if you passed a leakiness, then we'll use leaky ReLU. Otherwise we'll use normal ReLU. You could very easily write these leaky ReLU by hand, but I'm just trying to make it run a little faster by taking advantage of PyTorch. If you said I wanna subtract something from it, then go ahead and subtract that from it. If I said there's some maximum value, go ahead and clamp it at that maximum value. So here's our generalized ReLU. And so now let's have our conv layer and getCNN layers both take a star star quags and just pass them on through so that eventually they end up passed to our generalized ReLU. And so that way we're gonna be able to create a CNN and say what ReLU characteristics do we want. Nice and easily. And even getCNN model will pass down quags as well. So now that our ReLU can go negative, because it's leaky and because it's subtracting stuff, we'll need to change our histogram. So it goes from minus seven to seven rather than from zero to 10. So we'll also need to change our definition of getMin so that the middle few bits of the histogram are zero rather than the first two. And now we can just go ahead and train this model just like before and plot just like before. And this is looking pretty hopeful. Let's keep looking at the rest. So here's the first one, two, three, four layers. So compared to that, which is expand, does die, expand, die, expand, die, we're now seeing this is really, really nice. We're now seeing this is looking much better. It's straight away. It's using the full richness of the possible activations. There's no death going on. But our real question is how much is in this yellow line? There's a question. And let's see. In the final layer, look at that, less than 20%. Right, so we're now using nearly all of our activations by being careful about our initialization and our value. And then, but we're still training at a nice high learning rate. So this is looking great. Could you explain again how to read the histograms? Sure. So the four histograms, let's go back to the earlier one. So the four histograms are simply the four layers. So layer, the first, after the first conv, second, third, fourth. And the x-axis is the iteration. So each one is just one more iteration as most of our plots show. The y-axis is how many activations are the highest they can be or the lowest they can be. So what this one here is showing us, for example, is that there are some activations that are at the max and some activations are in the middle and some activations at the bottom. Whereas this one here is showing us that all of the activations are basically zero. So what this shows us in this histogram is that now we're going all the way from plus seven to minus seven because we can have negatives. This is zero. It's showing us that most of them are zero because yellow is the most energy. But there are activations throughout everything from the bottom to the top and a few less than zero, as we would expect because we have a leaky value and we also have that minus. We're not doing minus 0.5, we're doing minus 0.4 because leaky value means that we don't need to subtract half anymore. We subtract a bit less than half. And so then this line is telling us what percentage of them are zero or nearly zero. And so this is one of those things which is good to run lots of experiments in the notebook yourself to get a sense of what's actually in these histograms. So you can just go ahead and have a look at each hook's stats. And the third thing in it will be the histograms so you can see what shape is it and how is it calculated and so forth. Okay, so now that we've done that, this is looking really good. So what actually happens if we train like this? So let's do a one cycle training. So use that combined sheds we built last week. 50-50, two phases, cosine scheduling, cosine annealing, so gradual warmup, gradual cool down, and then run it for eight epochs. And there we go, we're doing really well. We're getting up to 98%. So this kind of, we hardly were really training in a thing. We were just trying to get something that looked good. And once we had something that looked good in terms of the telemetry, it's really training really well. One option I added, by the way, in initCNN was I added a uniform Boolean, which will set the initialization function to chiming normal if it's false, which is what we've been using so far, or chiming uniform if it's true. Chiming uniform, so now I've just trained the same model with uniform equals true. A lot of people think that uniform is better than normal because a uniform random number is less often close to zero. And so the thinking is that maybe uniform random, uniform initialization might cause it to kind of have a better richness of activations. I haven't studied this closely. I'm not sure I've seen a careful analysis in a paper. In this case, 9822 versus 9826, they're looking pretty similar, but that's just something else that it's there to play with. So at this point, we've got a pretty nice bunch of things you can look at now, and so you can see as your kind of problem to play with during the week is how accurate can you make a model just using the layers we've created so far? And for the ones that are great accuracy, what does the telemetry look like? How can you tell whether it's gonna be good? And then what insights can you gain from that to make it even better? So in the end, try to beat me, right? Try to beat 98%. You'll find you can beat it pretty easily with some playing around, but do some experiments. All right, so that's kind of about what we can do with initialization. You can go further with, as we discussed, with SelU or with FixUp. Like there are these really finely tuned initialization methods that you can do a thousand layers deep, but they're super fiddly. So generally, I would use something like the layer-wise sequential unit variance, LSUV thing that we saw earlier in, oh sorry, we haven't done that one yet. Okay, we're gonna do that next. Okay, so, forget I said that. So that's kind of about as far as we can get with basic initialization. To go further, we really need to use normalization, of which the most commonly known approach to normalization in the model is batch normalization. So let's look at batch normalization. So batch normalization has been around since, I think, about 2005. This is the paper. And they first of all describe a bit about why they thought batch normalization was a good idea. And by about page three, they provide the algorithm. So it's one of those things that if you don't read a lot of math, it might look a bit scary. But then when you look at it for a little bit longer, you suddenly notice that this is literally just the main, sum divided by the count. And this is the mean of the difference to the mean squared, and it's the mean of that. Oh, that's just what we looked at, that's variance. And this is just subtract the main, divide by the standard deviation. Oh, that's just normalization. So like once you look at it a second time, you realize we've done all this. We've just done it with code, not with math. And so then the only thing they do is after they've normalized it in the usual way, is that they then multiply it by gamma, and they add beta. What are gamma and beta? They are parameters to be learned. What does that mean? This is the most important line here. Remember that there are two types of numbers in a neural network. There are two types of numbers in a neural network, parameters and activations. Activations are things we calculate, parameters are things we learn. So these are just numbers that we learn. So that's all the information we need to implement BatchNorm. So let's go ahead and do it. So first of all, we'll grab our data as before, create our callbacks as before. Here's our pre-BatchNorm version, 96.5%. And the highest I could get was a 0.4 learning rate this way. And so now let's try BatchNorm. So here's BatchNorm. So let's look at the forward first. We're gonna get the mean and the variance. And the way we do that is we call update stats, and the mean is just the mean. And the variance is just the variance. And then we subtract the mean, and we divide by the square root of the variance. And then we multiply by, and then I didn't call them gamma and beta, because why use Greek letters when, because who remembers which one's gamma and which one's beta? Let's use English. The thing we multiply, we'll call the molts, and the things we add, we'll call the adds. And so molts and adds are parameters. We multiply by a parameter that initially is just a bunch of ones, so it does nothing. And we add a parameter which is initially just a bunch of zeros, so it does nothing. But they're parameters so they can learn, just like our, remember our original linear layer we created by hand just looked like this. In fact, if you think about it, adds is just bias. Right, it's identical to the bias we created earlier. So then there's a few extra little things we have to think about. One is what happens at inference time, right? So during training, we normalize. But the problem is that if we normalize in the same way at inference time, if we get like a totally different kind of image, we might kind of remove all of the things that are interesting about it. So what we do is while we're training, we keep a exponentially weighted moving average of the means and the variances. I'll talk more about what that means in a moment, but basically we've got the kind of the, a running average of the last few batches means and a running average of the last few batches variances. And so then when we're not training, in other words at inference time, we don't use the mean and variance of this mini-batch, we use that running average mean and variance that we've been keeping track of, okay? So how do we calculate that running average? Well, we don't just create something called self.vars, we go self.registerbuffervars. Now that creates something called self.vars. So why didn't we just say self.vars equals torch.ones? Why do we say self.registerbuffer? It's almost exactly the same as saying self.vars equals torch.ones, but it does a couple of nice things. The first is that if we move the model to the GPU, anything that's registered as a buffer will be moved to the GPU as well, right? And if we didn't do that, then it's gonna try and do this calculation down here. And if the vars and means aren't on the GPU, but everything else is on the GPU, we'll get an error. It'll say, oh, you're trying to add this thing on the CPU to this thing on the GPU, and it'll fail. So that's one nice thing about registerbuffer. The other nice thing is that the variances and the means, these running averages, they're part of the model, right? When we do inference, in order to calculate our predictions, we actually need to know what those numbers are. So if we save the model, we have to save those variances and means. So registerbuffer also causes them to be saved along with everything else in the model. So that's what registerbuffer does. So the variances, we start them out at ones. The means, we start them out at zeros. We then calculate the mean and variance of the minibatch, and we average out the axes zero, two, and three. So in other words, we average over all the batches, and we average over all of the X and Y coordinates. So all we're left with is a mean for each channel, or a mean for each filter, right? KeepDimEqual's true means that it's gonna leave an empty unit axis in positions zero, two, and three, so it'll still broadcast nicely. So now we wanna take a running average. A running average, so normally, if we wanna take a moving average, right? If we've got like a bunch of data points, right? We want a moving average, we would kind of like grab five at a time, and we would like take their moving average, take the average of those five, and then we take the next five, and we'd like take their average, and we keep doing that like a few at a time. We don't wanna do that here though, because these batch norm statistics, every single activation has one, so it's giant, right? Like models can have hundreds of millions of activations. We don't wanna have to save a whole history of every single one of those just so that we can calculate an average. So there's a handy trick for this, which is instead to use an exponentially weighted moving average. And basically what we do is we start out with this first point, and we say, okay, our first average is just the first point. Okay, so let's say, I don't know, that's three, right? And then the second point is five, and what we do is we, to take an exponentially weighted moving average, we first of all need some number, which we call momentum, let's say it's 0.9. So for the second value, so for the first value, our exponentially weighted moving average, which we'll call mu, equals three. And then for the second one, we take mu one, we multiply it by our momentum, and then we add our second value, five, and we multiply it by one minus our momentum. So in other words, it's mainly whatever it used to be before, plus a little bit of the new thing. And then mu two, sorry, mu three equals mu two times 0.9, plus, and maybe this one here is four, the new one times 0.1. So we're basically continuing to say, it's mainly the thing before, plus a little bit of the new one. And so what you end up with is something where, like by the time we get to here, the amount of influence of each of the previous data points, once you calculate it out, it turns out to be exponentially decayed. So it's a moving average with an exponential decay, with a benefit that we only ever have to keep track of one value. So that's what an exponentially weight of moving average is. This thing we do here, where we basically say we've got some function, where we say it's some previous value times 0.9, say, plus some other value times one minus that thing. This is called a linear interpolation. That's a bit of this and a bit of this other thing, and the two together make one. Linear interpolation in PyTorch is spelled Lerp. So we take the means and then we Lerp with our new mean using this amount of momentum. Unfortunately, Lerp uses the exact opposite of the normal sense of momentum. So momentum of 0.1 in batch norm actually means momentum of 0.9 in normal mean. In normal person speak. So this is actually how nn.batchnorm works as well. So when you see, so batch norm momentum is the opposite of what you would expect. I wish they'd given it a different name. They didn't, sadly, so this is what we're stuck with. So this is the running average means and standard deviations. So now we can go ahead and use that. So now we can create a new conv layer, which you can optionally say whether you want batch norm, if you do, we append a batch norm layer. If we do append a batch norm layer, we remove the bias layer. Because remember I said that the ads in batch norm just is a bias, right? So there's no point having a bias layer anymore. So we'll remove the unnecessary bias layer. And so now we can go ahead and initialize our CNN. This is a slightly more convenient initialization now that's actually gonna go in and recursively initialize every module inside our module. The weights and the standard deviations. And then we will train it with our hooks. And you can see our mean starts at 0 exactly, and our standard deviation starts at 1 exactly. So our training has entirely gotten rid of all of the exponential growth and sudden crash stuff that we had before. There's something interesting going on at the very end of training, which I don't quite know what that is. I mean, when I say the end of training, we've only done one epoch. But this is looking a lot better than anything we've seen before. I mean, that's just a very nice looking curve. And so we're now able to get up to learning rates up to 1. We've got 97% accuracy after just three epochs. This is looking very encouraging. So now that we've built our own batch norm, we're allowed to use PyTorch's batch norm. And we get pretty much the same results. Sometimes it's 97, sometimes it's 98. This is just random variation. So now that we've got that, let's try going crazy. Let's try using our little one cycle learning scheduler we had. And let's try and go all the way up to a learning rate of 2. And look at that, we totally can. And we're now up towards nearly 99% accuracy. So batch norm really is quite fantastic. Batch norm has a bit of a problem though, which is that you can't apply it to what we call online learning tasks. In other words, if you have a batch size of 1, right? So you're getting a single item at a time and learning from that item. What's the variance of that batch? The variance of a batch of 1 is infinite, right? So we can't use batch norm in that case. Well, what if we're doing a segmentation task where we can only have a batch size of 2 or 4, which we've seen plenty of times in part 1? That's gonna be a problem, right? Because across all of our layers, across all of our training, across all of the channels, the batch size of 2, at some point, those two values are gonna be the same or nearly the same. And so we then divide by that variance, which is about 0, we have infinity, right? So we have this problem where anytime you have a small batch size, you're gonna get unstable or impossible training. It's also gonna be really hard for RNNs. Because for RNNs, remember, it looks something like this, right? We have this hidden state and we use the same weight matrix again and again and again, right? Remember, we can unroll it and it looks like this. If you've forgotten, go back to lesson seven. And then we can even stack them together into two RNNs. One RNN fits to another RNN. And if we unroll that, it looks like this. And remember, these state time step to time step transitions, if we're doing IMDB with a movie review with 2,000 words, there's 2,000 of these. And this is the same weight matrix each time. And the number of these circles will vary. It's the number of time steps will vary from document to document. So how would you do batch norm, right? How would you say what's the running average of means and variances? Cuz you can't put a different one between each of these unrolled layers, because this is a for loop, remember? So we can't have different values every time. So it's not at all clear how you would insert batch norm into an RNN. So batch norm has these two deficiencies. How do we handle very small batch sizes, all the way down to batch size of one? How do we handle RNNs? So this paper called layer normalization suggests a solution to this. And the layer normalization paper from Jimmy Barr and Kiros and Jeffrey Hinton who just won the Turing Award with Joshua Benjio and Jan LeCun, which is kind of the Nobel Prize of Computer Science. They created this paper, which like many papers, when you read it, it looks reasonably terrifying, particularly once you start looking at all this stuff. But actually, when we take this paper and we convert it to code, it's this. Now, we're just not to say the paper's garbage, it's just that the paper has lots of explanation about what's going on and what do we find out and what does that mean, right? But the actual what's layer norm, it's the same as batch norm. But rather than saying x.mean 0, 2, 3, you say x.mean 1, 2, 3, and you remove all the running averages. So this is layer norm, right, with none of that running average stuff. And the reason we don't need the running averages anymore is because we're not taking the mean across all the items in the batch. Every image has its own mean, every image has its own standard deviation. So there's no concept of having to average across things in a batch, right? And so that's all layer norm is. We also average over the channels. So we average over the channels in the x and the y for each image individually. So we don't have to keep track of any running averages. The problem is that when we do that and we train, even at a lower learning rate of 0.8, it doesn't work, right? Layer norm's not as good. So it's a work around we can use, but because we don't have the running averages at inference time, and more importantly, because we don't have a different normalization for each channel, we're just throwing them all together and pretending they're the same and they're not, right? So layer norm helps, but it's nowhere near as good as batch norm, okay? But for RNNs, it's kind of what you have to use is something like this. So here's the thought experiment. What if you're using layer norm on the very first, on the actual input data, and you're trying to distinguish between foggy days and sunny days? So foggy days will have less activations on average, because they're less bright, and they will have less contrast. In other words, they have lower variance. So layer norm would cause the variances to be normalized to be the same, and the means to be normalized to be the same. So now the sunny day picture and the hazy day picture would have the same overall kind of activations and amount of contrast. And so the answer to this question is, no, you couldn't. With layer norm, you would literally not be able to tell the difference between pictures of sunny days and pictures of foggy days. Now, it's not only if you put the layer norm on the input data, which you wouldn't do, but everywhere in the middle layers, it's the same, right? Anywhere where the overall level of activation or the amount of difference of activation is something that is part of what you care about, it throws it away. It's designed to throw it away. Furthermore, if your inference time is using things from kind of a different distribution where that different distribution is important, it throws that away. So layer norm's a partial hacky work around for some very genuine problems. There's also something called instance norm. And instance norm is basically the same thing as layer norm. It's a bit easier to read in the paper because they actually lay out all the indexes. So a particular output for a particular batch, for a particular channel, for a particular x, for a particular y is equal to the input for that batch and channel in x and y minus the mean for the batch and the channel. So in other words, it's the same as layer norm, but now it's mean 2,3, rather than mean 1,2,3. So you can see how all these different papers, when you turn them into code, they're tiny variations, right? Instance norm, even at a learning rate of 0.1, doesn't learn anything at all. Why can't it classify anything? Because we're now taking the mean, removing the difference in means and the difference in activations for every channel and for every image. Which means we've literally thrown away all the things that allow us to classify. Does that mean that instance norm is stupid? No, certainly not. It wasn't designed for classification. It was designed for style transfer, where the authors guessed that these differences in contrast and overall amount were not important. So it's not something they should remove from trying to create things that were like different types of pictures. It turned out to work really well. But you gotta be careful, right? You can't just go in and say, here's another normalization thing, I'll try it. You gotta actually know what it's for, to know where it's gonna work. So then finally, there's a paper called Group Norm, which has this wonderful picture, and it shows the differences. Batch norm is averaging over the batch and the height and the width, and is different for each channel. Layer norm, it's averaging for each channel, for each height, for each width, and is different for each element of the batch. Instance norm is averaging over height and width, and is different for each channel and each batch. And then group norm is the same as instance norm, but they arbitrarily group a few channels together and do that. So group norm is kind of a more general way to do it. And in the PyTorch docs, they point out that you can actually turn group norm into instance norm or group norm into layer norm, depending on how you group things up. So there's all kinds of attempts to work around the problem that we can't use small batch sizes and we can't use RNNs with batch norm. But none of them are as good as batch norm. So what do we do? Well, I don't know how to fix the RNN problem, but I think I know how to fix the batch size problem. So let's start by taking a look at the batch size problem in practice. Let's create a new data bunch with a batch size of two. Okay, and so here's our conv layer as before with our batch norm. And let's use a learning rate of 0.4 and fit that. And the first thing you'll notice is that it takes a long time. Right, small batch sizes take a long time because it's just lots and lots of kernel launches on the GPU, it's just a lot of overhead, right? Something like this might even run faster on the CPU. And then you'll notice that it's only 26% accurate, which is awful. Why is it awful? Because of what I said, the small batch size is causing a huge problem. Because quite often, there's one channel in one layer where the variance is really small because those two numbers just happen to be really close, and so it blows out the activations out to a billion and everything falls apart. There is one thing we could try to do to fix this really easily, which is to use epsilon. What's epsilon? Let's go take a look at our code. Here's our batch norm. Look, we don't divide by the square root of variance. We divide by the square root of variance plus epsilon, where epsilon is 1e-5. Epsilon's a number that computer scientists and mathematicians, they use this Greek letter very frequently to mean some very small number. And in computer science, it's normally a small number that you add to avoid floating point rounding problems and stuff like that. So it's very common to see it on the bottom of a division to avoid dividing by such small numbers that you kind of can't calculate things in floating point properly. But our view is that epsilon is actually a fantastic hyperparameter that you should be using to train things better. And here's a great example. With batch norm, what if we didn't set epsilon to 1e-5, but what if we set it to 0.1? If we set epsilon to 0.1, then that basically would cause this to never make the overall activations be multiplied by anything more than 10. Right? Sorry, that would be 0.01, because we're taking the square root. So if you set it to 0.01, right? Because we'd be saying, let's say the variance was 0. It would be 0 plus 0.01 square root, right? So it ends up dividing by 0.1, which ends up multiplying by 10. So even in the worst case, it's not gonna blow out. I mean, it's still not great, because there actually are huge differences in variance between different channels and different layers. But at least this would cause it to not fall apart. So option number one would be, use a much higher epsilon value. And we'll keep coming back to this idea that epsilon appears in lots of places in deep learning, and we should use it as a hyperparameter we control and take advantage of. But we have a better idea. We think we have a better idea, which is we've built a new algorithm called running batch norm. And running batch norm, I think, is the first true solution to the small batch size batch norm problem. And like everything we do at Fast AI, it's ridiculously simple. And I don't know why no one's done it before. Maybe they have, and I've missed it. And the ridiculously simple thing is this. In the forward function for running batch norm, don't divide by the batch standard deviation. Don't subtract the batch mean, but instead use the moving average statistics at training time as well, not just at inference time. Why does this help? Because let's say you're using a batch size of two, right? Then from time to time, in this particular layer, in this particular channel, you happen to get two values that are really close together and they have a variance really close to zero. But that's fine, right? Because you're only taking 0.1 of that and 0.9 of whatever you had before. That's how running averages work, right? So if previously the variance was 1, now it's not 1e-5, it's just 0.9, right? So in this way, as long as you don't get really unlucky and have the very first batch be dreadful. Because you're using this moving average, you never have this problem. So let's take a look. We'll look at the code in a moment, but let's do the same thing. 0.4, we're gonna use our running batch norm. We train it for one epoch, and instead of 26% accuracy, it's 91% accuracy, right? So it totally nails it. In one epoch, just a two batch size and a pretty high learning rate. There's quite a few details we have to get right to make this work. But they're all details that we're gonna see in lots of other places in this course. We're just kind of seeing them here for the first time. So I'm gonna show you all of the details, but don't get overwhelmed, we'll keep coming back to them. The first detail is something very simple, which is in normal batch norm, we take the running average of variance. But you can't take the running average of variance. It doesn't make sense to take the running average of variance. It's a variance. You can't just average a bunch of variances. Particularly because they might even be different batch sizes, right? Cuz batch size isn't necessarily constant, right? Instead, as we learned earlier in the class, the way that we want to calculate variance is like this. Sum of expected value of mean of x squared minus mean of x squared. So let's do that. Let's just, as I mentioned, we can do, let's keep track of the squares and the sums. So we register a buffer called sums and we register a buffer called squares and we just go x.sum over 0 to 3 dimensions. And x times x.sum, so squared, right? And then we'll take the LERP, the exponentially weighted moving average of the sums and the squareds. And then for the variance, we will do squareds divided by count minus squared mean, right? So it's that formula, okay? So that's detail number one that we have to be careful of. Detail number two is that the batch size could vary from many batch to many batch. So we should also register a buffer for count and take an exponentially weighted moving average of the counts of the batch sizes. So that basically tells us, so what do we need to divide by each time? The amount we need to divide by each time is the total number of elements in the mini batch divided by the number of channels. That's basically grid x times grid y times batch size. So let's take an exponentially weighted moving average of the count and then that's what we will divide by for both our means and variances. That's detail number two. Detail number three is that we need to do something called debiasing. So debiasing is this. We want to make sure that at every point, and we're gonna look at this in more detail when we look at optimizers, we wanna make sure that every point that no observation is weighted too highly, right? And the problem is that the normal way of doing moving averages, the very first point gets far too much weight because it appears in the first moving average and the second and the third and the fourth, right? So there's a really simple way to fix this, which is that you initialize both sums and squares to zeros, right? And then you do a LERP in the usual way, right? And let's see what happens when we do this. So let's say our values are 10 and then 20. These are the first two values we get. So actually, we already need to talk about the first value. So the value, so actually, let's say the value is 10, right? So we initialize our mean to 0 at the very start of training, right? And then the value that comes in is 10, right? So we would expect the moving average to be 10. But our LERP formula says it's equal to our previous value, which is 0, times 0.9 plus our new value times 0.1. Equals 0 plus 1, equals 1. It's 10 times too small, okay? But that's very easy to correct for, because we know it's always gonna be wrong by that amount, so we then divide it by 0.1. And that fixes it. And then the second value has exactly the same problem. It's got too much 0 in it. But this time, it's actually gonna be divided by, let's not call it 0.1, let's call it 1-0.9. Because when you work through the math, you'll see the second one, it's gonna be divided by 1-0.9 squared, and so forth, okay? So this thing here, where we divide by that, that's called debiasing. It's gonna appear again when we look at optimization. So you can see what we do is we have an exponentially weighted debiasing amount where we simply keep multiplying momentum times the previous debiasing amount. So initially, it's just equal to momentum, and then momentum squared, and then momentum cubed, and so forth. So then we do what I just said. We divide by the debiasing amount. Okay, and then there's just one more thing we do, which is, remember how I said you might get really unlucky that your first mini-batch is just really close to zero, and we don't want that to destroy everything? So I just say, if you haven't seen more than a total of 20 items yet, just clamp the variance to be no smaller than 0.01, just to avoid blowing out of the water. And then the last two lines are the same, okay? So that's it, right? It's all pretty straightforward arithmetic. It's a very straightforward idea. But when we put it all together, it's shockingly effective. And so then we can try an interesting thought experiment. So here's another thing to try during the week. What's the best accuracy you can get in a single epoch? So say run.fit 1. And with this convolutional with running batch norm layer, and a batch size of 32, and a linear schedule from 1 to 0.2, I got 97.5%. I only tried a couple of things, so I haven't, this is definitely something that I hope you can beat me at. But it's really good to kind of create interesting little games to play in research, we call them toy problems. Almost everything in research is basically toy problems. Come up with toy problems and try to find good solutions to them. So your toy problem, another toy problem for this week is, what's the best you can get using, yeah, whatever kind of normalization you like, whatever kind of architecture you like, as long as it only uses concepts we've used up to lesson seven to get the best accuracy you can in one epoch. Yeah, that's basically it. So what's the future of running batch norm? I mean, it's kind of early days, right? We haven't published this research yet. We haven't done all the kind of ablation studies and stuff we need to do yet. At this stage though, I'm really excited about this. Every time I've tried it on something, it's been working really well. The last time that we had something in a lesson that we said, this is unpublished research that we're excited about, it turned into ULM fit, which is now a really widely used algorithm and was published at the ACL. So fingers crossed that this turns out to be something really terrific as well. But either way, you've kind of got to see the process, cuz literally building these notebooks was the process I used to create this algorithm, so you've seen the exact process that I used to build up this idea and do some initial testing of it. So hopefully that's been fun for you, and see you next week. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.1, "text": " Welcome to lesson 10, which I've rather enthusiastically titled", "tokens": [4027, 281, 6898, 1266, 11, 597, 286, 600, 2831, 18076, 22808, 19841], "temperature": 0.0, "avg_logprob": -0.15180804512717508, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.007808270864188671}, {"id": 1, "seek": 0, "start": 8.1, "end": 10.74, "text": " Wrapping Up Our CNN, but looking at how many things we want", "tokens": [343, 424, 3759, 5858, 2621, 24859, 11, 457, 1237, 412, 577, 867, 721, 321, 528], "temperature": 0.0, "avg_logprob": -0.15180804512717508, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.007808270864188671}, {"id": 2, "seek": 0, "start": 10.74, "end": 13.18, "text": " to cover, I've added a nearly to the end,", "tokens": [281, 2060, 11, 286, 600, 3869, 257, 6217, 281, 264, 917, 11], "temperature": 0.0, "avg_logprob": -0.15180804512717508, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.007808270864188671}, {"id": 3, "seek": 0, "start": 13.18, "end": 15.860000000000001, "text": " and I'm not actually sure how nearly we'll get there.", "tokens": [293, 286, 478, 406, 767, 988, 577, 6217, 321, 603, 483, 456, 13], "temperature": 0.0, "avg_logprob": -0.15180804512717508, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.007808270864188671}, {"id": 4, "seek": 0, "start": 15.860000000000001, "end": 17.16, "text": " We'll see.", "tokens": [492, 603, 536, 13], "temperature": 0.0, "avg_logprob": -0.15180804512717508, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.007808270864188671}, {"id": 5, "seek": 0, "start": 17.16, "end": 18.46, "text": " We'll probably have a few more things", "tokens": [492, 603, 1391, 362, 257, 1326, 544, 721], "temperature": 0.0, "avg_logprob": -0.15180804512717508, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.007808270864188671}, {"id": 6, "seek": 0, "start": 18.46, "end": 20.54, "text": " to cover next week as well.", "tokens": [281, 2060, 958, 1243, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.15180804512717508, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.007808270864188671}, {"id": 7, "seek": 0, "start": 20.54, "end": 25.18, "text": " I just wanted to remind you after hearing", "tokens": [286, 445, 1415, 281, 4160, 291, 934, 4763], "temperature": 0.0, "avg_logprob": -0.15180804512717508, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.007808270864188671}, {"id": 8, "seek": 0, "start": 25.18, "end": 28.740000000000002, "text": " from a few folks during the week who are very sad", "tokens": [490, 257, 1326, 4024, 1830, 264, 1243, 567, 366, 588, 4227], "temperature": 0.0, "avg_logprob": -0.15180804512717508, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.007808270864188671}, {"id": 9, "seek": 2874, "start": 28.74, "end": 31.06, "text": " that they're not quite keeping up with everything,", "tokens": [300, 436, 434, 406, 1596, 5145, 493, 365, 1203, 11], "temperature": 0.0, "avg_logprob": -0.15022446345357063, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.00017614805256016552}, {"id": 10, "seek": 2874, "start": 31.06, "end": 33.14, "text": " that's totally okay.", "tokens": [300, 311, 3879, 1392, 13], "temperature": 0.0, "avg_logprob": -0.15022446345357063, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.00017614805256016552}, {"id": 11, "seek": 2874, "start": 33.14, "end": 34.44, "text": " Don't worry.", "tokens": [1468, 380, 3292, 13], "temperature": 0.0, "avg_logprob": -0.15022446345357063, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.00017614805256016552}, {"id": 12, "seek": 2874, "start": 34.44, "end": 38.739999999999995, "text": " As I mentioned in lesson one, I'm trying to give you enough here", "tokens": [1018, 286, 2835, 294, 6898, 472, 11, 286, 478, 1382, 281, 976, 291, 1547, 510], "temperature": 0.0, "avg_logprob": -0.15022446345357063, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.00017614805256016552}, {"id": 13, "seek": 2874, "start": 38.739999999999995, "end": 44.66, "text": " to keep you busy until the next part two next year.", "tokens": [281, 1066, 291, 5856, 1826, 264, 958, 644, 732, 958, 1064, 13], "temperature": 0.0, "avg_logprob": -0.15022446345357063, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.00017614805256016552}, {"id": 14, "seek": 2874, "start": 44.66, "end": 49.58, "text": " So you can dive into the bits you're interested in and go back", "tokens": [407, 291, 393, 9192, 666, 264, 9239, 291, 434, 3102, 294, 293, 352, 646], "temperature": 0.0, "avg_logprob": -0.15022446345357063, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.00017614805256016552}, {"id": 15, "seek": 2874, "start": 49.58, "end": 53.06, "text": " and look over stuff, and yeah, don't feel like you have", "tokens": [293, 574, 670, 1507, 11, 293, 1338, 11, 500, 380, 841, 411, 291, 362], "temperature": 0.0, "avg_logprob": -0.15022446345357063, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.00017614805256016552}, {"id": 16, "seek": 2874, "start": 53.06, "end": 57.18, "text": " to understand everything within a week of first hearing it.", "tokens": [281, 1223, 1203, 1951, 257, 1243, 295, 700, 4763, 309, 13], "temperature": 0.0, "avg_logprob": -0.15022446345357063, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.00017614805256016552}, {"id": 17, "seek": 5718, "start": 57.18, "end": 61.62, "text": " And also, if you're not putting in the time during the homework", "tokens": [400, 611, 11, 498, 291, 434, 406, 3372, 294, 264, 565, 1830, 264, 14578], "temperature": 0.0, "avg_logprob": -0.08532192910364432, "compression_ratio": 1.900763358778626, "no_speech_prob": 1.669298580964096e-05}, {"id": 18, "seek": 5718, "start": 61.62, "end": 63.3, "text": " or you didn't put in the time during the homework", "tokens": [420, 291, 994, 380, 829, 294, 264, 565, 1830, 264, 14578], "temperature": 0.0, "avg_logprob": -0.08532192910364432, "compression_ratio": 1.900763358778626, "no_speech_prob": 1.669298580964096e-05}, {"id": 19, "seek": 5718, "start": 63.3, "end": 66.7, "text": " in the last part, you know, expect to have to go back", "tokens": [294, 264, 1036, 644, 11, 291, 458, 11, 2066, 281, 362, 281, 352, 646], "temperature": 0.0, "avg_logprob": -0.08532192910364432, "compression_ratio": 1.900763358778626, "no_speech_prob": 1.669298580964096e-05}, {"id": 20, "seek": 5718, "start": 66.7, "end": 69.02, "text": " and recover things, particularly because a lot", "tokens": [293, 8114, 721, 11, 4098, 570, 257, 688], "temperature": 0.0, "avg_logprob": -0.08532192910364432, "compression_ratio": 1.900763358778626, "no_speech_prob": 1.669298580964096e-05}, {"id": 21, "seek": 5718, "start": 69.02, "end": 71.94, "text": " of the stuff we covered in part one, I'm kind of assuming", "tokens": [295, 264, 1507, 321, 5343, 294, 644, 472, 11, 286, 478, 733, 295, 11926], "temperature": 0.0, "avg_logprob": -0.08532192910364432, "compression_ratio": 1.900763358778626, "no_speech_prob": 1.669298580964096e-05}, {"id": 22, "seek": 5718, "start": 71.94, "end": 75.98, "text": " that you're deeply comfortable with at this point,", "tokens": [300, 291, 434, 8760, 4619, 365, 412, 341, 935, 11], "temperature": 0.0, "avg_logprob": -0.08532192910364432, "compression_ratio": 1.900763358778626, "no_speech_prob": 1.669298580964096e-05}, {"id": 23, "seek": 5718, "start": 75.98, "end": 78.5, "text": " not because you're stupid if you're not,", "tokens": [406, 570, 291, 434, 6631, 498, 291, 434, 406, 11], "temperature": 0.0, "avg_logprob": -0.08532192910364432, "compression_ratio": 1.900763358778626, "no_speech_prob": 1.669298580964096e-05}, {"id": 24, "seek": 5718, "start": 78.5, "end": 80.62, "text": " but just because it gives you the opportunity to go back", "tokens": [457, 445, 570, 309, 2709, 291, 264, 2650, 281, 352, 646], "temperature": 0.0, "avg_logprob": -0.08532192910364432, "compression_ratio": 1.900763358778626, "no_speech_prob": 1.669298580964096e-05}, {"id": 25, "seek": 5718, "start": 80.62, "end": 83.18, "text": " and restudy it and practice and experiment", "tokens": [293, 1472, 532, 88, 309, 293, 3124, 293, 5120], "temperature": 0.0, "avg_logprob": -0.08532192910364432, "compression_ratio": 1.900763358778626, "no_speech_prob": 1.669298580964096e-05}, {"id": 26, "seek": 5718, "start": 83.18, "end": 84.9, "text": " until you are deeply comfortable.", "tokens": [1826, 291, 366, 8760, 4619, 13], "temperature": 0.0, "avg_logprob": -0.08532192910364432, "compression_ratio": 1.900763358778626, "no_speech_prob": 1.669298580964096e-05}, {"id": 27, "seek": 8490, "start": 84.9, "end": 90.7, "text": " So yeah, if you're finding it whizzing along at a pace,", "tokens": [407, 1338, 11, 498, 291, 434, 5006, 309, 315, 8072, 278, 2051, 412, 257, 11638, 11], "temperature": 0.0, "avg_logprob": -0.10845446932143059, "compression_ratio": 1.937984496124031, "no_speech_prob": 1.1123974218207877e-05}, {"id": 28, "seek": 8490, "start": 90.7, "end": 92.66000000000001, "text": " that is because it is whizzing along at a pace.", "tokens": [300, 307, 570, 309, 307, 315, 8072, 278, 2051, 412, 257, 11638, 13], "temperature": 0.0, "avg_logprob": -0.10845446932143059, "compression_ratio": 1.937984496124031, "no_speech_prob": 1.1123974218207877e-05}, {"id": 29, "seek": 8490, "start": 92.66000000000001, "end": 95.14, "text": " Also, it's covering a lot", "tokens": [2743, 11, 309, 311, 10322, 257, 688], "temperature": 0.0, "avg_logprob": -0.10845446932143059, "compression_ratio": 1.937984496124031, "no_speech_prob": 1.1123974218207877e-05}, {"id": 30, "seek": 8490, "start": 95.14, "end": 98.82000000000001, "text": " of more software engineering kind of stuff, which for the people", "tokens": [295, 544, 4722, 7043, 733, 295, 1507, 11, 597, 337, 264, 561], "temperature": 0.0, "avg_logprob": -0.10845446932143059, "compression_ratio": 1.937984496124031, "no_speech_prob": 1.1123974218207877e-05}, {"id": 31, "seek": 8490, "start": 98.82000000000001, "end": 100.82000000000001, "text": " who are practicing software engineers,", "tokens": [567, 366, 11350, 4722, 11955, 11], "temperature": 0.0, "avg_logprob": -0.10845446932143059, "compression_ratio": 1.937984496124031, "no_speech_prob": 1.1123974218207877e-05}, {"id": 32, "seek": 8490, "start": 100.82000000000001, "end": 102.74000000000001, "text": " you'll be thinking this is all pretty straightforward,", "tokens": [291, 603, 312, 1953, 341, 307, 439, 1238, 15325, 11], "temperature": 0.0, "avg_logprob": -0.10845446932143059, "compression_ratio": 1.937984496124031, "no_speech_prob": 1.1123974218207877e-05}, {"id": 33, "seek": 8490, "start": 102.74000000000001, "end": 105.86000000000001, "text": " and for those of you that are not, you'll be thinking, wow,", "tokens": [293, 337, 729, 295, 291, 300, 366, 406, 11, 291, 603, 312, 1953, 11, 6076, 11], "temperature": 0.0, "avg_logprob": -0.10845446932143059, "compression_ratio": 1.937984496124031, "no_speech_prob": 1.1123974218207877e-05}, {"id": 34, "seek": 8490, "start": 105.86000000000001, "end": 107.22, "text": " there's a lot here.", "tokens": [456, 311, 257, 688, 510, 13], "temperature": 0.0, "avg_logprob": -0.10845446932143059, "compression_ratio": 1.937984496124031, "no_speech_prob": 1.1123974218207877e-05}, {"id": 35, "seek": 8490, "start": 107.22, "end": 110.78, "text": " Part of that is because I think data scientists need", "tokens": [4100, 295, 300, 307, 570, 286, 519, 1412, 7708, 643], "temperature": 0.0, "avg_logprob": -0.10845446932143059, "compression_ratio": 1.937984496124031, "no_speech_prob": 1.1123974218207877e-05}, {"id": 36, "seek": 8490, "start": 110.78, "end": 112.5, "text": " to be good software engineers, so I'm trying", "tokens": [281, 312, 665, 4722, 11955, 11, 370, 286, 478, 1382], "temperature": 0.0, "avg_logprob": -0.10845446932143059, "compression_ratio": 1.937984496124031, "no_speech_prob": 1.1123974218207877e-05}, {"id": 37, "seek": 8490, "start": 112.5, "end": 114.26, "text": " to show you some of these things.", "tokens": [281, 855, 291, 512, 295, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.10845446932143059, "compression_ratio": 1.937984496124031, "no_speech_prob": 1.1123974218207877e-05}, {"id": 38, "seek": 11426, "start": 114.26, "end": 118.26, "text": " But, you know, it's stuff which people can spend years learning,", "tokens": [583, 11, 291, 458, 11, 309, 311, 1507, 597, 561, 393, 3496, 924, 2539, 11], "temperature": 0.0, "avg_logprob": -0.13436367747547862, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.891821011668071e-05}, {"id": 39, "seek": 11426, "start": 118.26, "end": 121.58000000000001, "text": " and so hopefully this is the start of a long process for you", "tokens": [293, 370, 4696, 341, 307, 264, 722, 295, 257, 938, 1399, 337, 291], "temperature": 0.0, "avg_logprob": -0.13436367747547862, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.891821011668071e-05}, {"id": 40, "seek": 11426, "start": 121.58000000000001, "end": 123.46000000000001, "text": " that haven't done software engineering before", "tokens": [300, 2378, 380, 1096, 4722, 7043, 949], "temperature": 0.0, "avg_logprob": -0.13436367747547862, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.891821011668071e-05}, {"id": 41, "seek": 11426, "start": 123.46000000000001, "end": 125.10000000000001, "text": " of becoming better software engineers,", "tokens": [295, 5617, 1101, 4722, 11955, 11], "temperature": 0.0, "avg_logprob": -0.13436367747547862, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.891821011668071e-05}, {"id": 42, "seek": 11426, "start": 125.10000000000001, "end": 128.5, "text": " and there are some useful tips, hopefully.", "tokens": [293, 456, 366, 512, 4420, 6082, 11, 4696, 13], "temperature": 0.0, "avg_logprob": -0.13436367747547862, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.891821011668071e-05}, {"id": 43, "seek": 11426, "start": 128.5, "end": 135.34, "text": " So to remind you, we're trying to recreate Fast AI and much", "tokens": [407, 281, 4160, 291, 11, 321, 434, 1382, 281, 25833, 15968, 7318, 293, 709], "temperature": 0.0, "avg_logprob": -0.13436367747547862, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.891821011668071e-05}, {"id": 44, "seek": 11426, "start": 135.34, "end": 143.54000000000002, "text": " of PyTorch from these foundations, and starting", "tokens": [295, 9953, 51, 284, 339, 490, 613, 22467, 11, 293, 2891], "temperature": 0.0, "avg_logprob": -0.13436367747547862, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.891821011668071e-05}, {"id": 45, "seek": 14354, "start": 143.54, "end": 144.84, "text": " to make things even better,", "tokens": [281, 652, 721, 754, 1101, 11], "temperature": 0.0, "avg_logprob": -0.13768465622611667, "compression_ratio": 1.8020833333333333, "no_speech_prob": 1.568151310493704e-05}, {"id": 46, "seek": 14354, "start": 144.84, "end": 146.73999999999998, "text": " and today you'll actually see some bits.", "tokens": [293, 965, 291, 603, 767, 536, 512, 9239, 13], "temperature": 0.0, "avg_logprob": -0.13768465622611667, "compression_ratio": 1.8020833333333333, "no_speech_prob": 1.568151310493704e-05}, {"id": 47, "seek": 14354, "start": 146.73999999999998, "end": 148.04, "text": " Well, in fact, you've already seen some bits", "tokens": [1042, 11, 294, 1186, 11, 291, 600, 1217, 1612, 512, 9239], "temperature": 0.0, "avg_logprob": -0.13768465622611667, "compression_ratio": 1.8020833333333333, "no_speech_prob": 1.568151310493704e-05}, {"id": 48, "seek": 14354, "start": 148.04, "end": 149.14, "text": " that are going to be even better.", "tokens": [300, 366, 516, 281, 312, 754, 1101, 13], "temperature": 0.0, "avg_logprob": -0.13768465622611667, "compression_ratio": 1.8020833333333333, "no_speech_prob": 1.568151310493704e-05}, {"id": 49, "seek": 14354, "start": 149.14, "end": 152.14, "text": " I think the next version of Fast AI will have this new callback", "tokens": [286, 519, 264, 958, 3037, 295, 15968, 7318, 486, 362, 341, 777, 818, 3207], "temperature": 0.0, "avg_logprob": -0.13768465622611667, "compression_ratio": 1.8020833333333333, "no_speech_prob": 1.568151310493704e-05}, {"id": 50, "seek": 14354, "start": 152.14, "end": 154.94, "text": " system, which I think is better than the old one, and today we're", "tokens": [1185, 11, 597, 286, 519, 307, 1101, 813, 264, 1331, 472, 11, 293, 965, 321, 434], "temperature": 0.0, "avg_logprob": -0.13768465622611667, "compression_ratio": 1.8020833333333333, "no_speech_prob": 1.568151310493704e-05}, {"id": 51, "seek": 14354, "start": 154.94, "end": 159.14, "text": " going to be showing you some new previously unpublished research", "tokens": [516, 281, 312, 4099, 291, 512, 777, 8046, 20994, 836, 4173, 2132], "temperature": 0.0, "avg_logprob": -0.13768465622611667, "compression_ratio": 1.8020833333333333, "no_speech_prob": 1.568151310493704e-05}, {"id": 52, "seek": 14354, "start": 159.14, "end": 161.78, "text": " which will be finding its way into Fast AI", "tokens": [597, 486, 312, 5006, 1080, 636, 666, 15968, 7318], "temperature": 0.0, "avg_logprob": -0.13768465622611667, "compression_ratio": 1.8020833333333333, "no_speech_prob": 1.568151310493704e-05}, {"id": 53, "seek": 14354, "start": 161.78, "end": 165.29999999999998, "text": " and maybe other libraries as well also.", "tokens": [293, 1310, 661, 15148, 382, 731, 611, 13], "temperature": 0.0, "avg_logprob": -0.13768465622611667, "compression_ratio": 1.8020833333333333, "no_speech_prob": 1.568151310493704e-05}, {"id": 54, "seek": 14354, "start": 165.29999999999998, "end": 167.54, "text": " So we're going to try and stick to, and we will stick", "tokens": [407, 321, 434, 516, 281, 853, 293, 2897, 281, 11, 293, 321, 486, 2897], "temperature": 0.0, "avg_logprob": -0.13768465622611667, "compression_ratio": 1.8020833333333333, "no_speech_prob": 1.568151310493704e-05}, {"id": 55, "seek": 14354, "start": 167.54, "end": 170.7, "text": " to using nothing but these foundations.", "tokens": [281, 1228, 1825, 457, 613, 22467, 13], "temperature": 0.0, "avg_logprob": -0.13768465622611667, "compression_ratio": 1.8020833333333333, "no_speech_prob": 1.568151310493704e-05}, {"id": 56, "seek": 17070, "start": 170.7, "end": 174.33999999999997, "text": " And we're working through developing a modern CNN model,", "tokens": [400, 321, 434, 1364, 807, 6416, 257, 4363, 24859, 2316, 11], "temperature": 0.0, "avg_logprob": -0.11643389404797164, "compression_ratio": 1.792828685258964, "no_speech_prob": 1.3845153262082022e-05}, {"id": 57, "seek": 17070, "start": 174.33999999999997, "end": 175.64, "text": " and we've got to the point", "tokens": [293, 321, 600, 658, 281, 264, 935], "temperature": 0.0, "avg_logprob": -0.11643389404797164, "compression_ratio": 1.792828685258964, "no_speech_prob": 1.3845153262082022e-05}, {"id": 58, "seek": 17070, "start": 175.64, "end": 177.33999999999997, "text": " where we've done our training loop at this point,", "tokens": [689, 321, 600, 1096, 527, 3097, 6367, 412, 341, 935, 11], "temperature": 0.0, "avg_logprob": -0.11643389404797164, "compression_ratio": 1.792828685258964, "no_speech_prob": 1.3845153262082022e-05}, {"id": 59, "seek": 17070, "start": 177.33999999999997, "end": 181.89999999999998, "text": " and we've got a nice flexible training loop.", "tokens": [293, 321, 600, 658, 257, 1481, 11358, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.11643389404797164, "compression_ratio": 1.792828685258964, "no_speech_prob": 1.3845153262082022e-05}, {"id": 60, "seek": 17070, "start": 181.89999999999998, "end": 187.5, "text": " So from here, the rest of it, when I say we're going to finish", "tokens": [407, 490, 510, 11, 264, 1472, 295, 309, 11, 562, 286, 584, 321, 434, 516, 281, 2413], "temperature": 0.0, "avg_logprob": -0.11643389404797164, "compression_ratio": 1.792828685258964, "no_speech_prob": 1.3845153262082022e-05}, {"id": 61, "seek": 17070, "start": 187.5, "end": 189.94, "text": " out a modern CNN model, it's not just going", "tokens": [484, 257, 4363, 24859, 2316, 11, 309, 311, 406, 445, 516], "temperature": 0.0, "avg_logprob": -0.11643389404797164, "compression_ratio": 1.792828685258964, "no_speech_prob": 1.3845153262082022e-05}, {"id": 62, "seek": 17070, "start": 189.94, "end": 193.33999999999997, "text": " to be some basic getting by model, but we're actually going", "tokens": [281, 312, 512, 3875, 1242, 538, 2316, 11, 457, 321, 434, 767, 516], "temperature": 0.0, "avg_logprob": -0.11643389404797164, "compression_ratio": 1.792828685258964, "no_speech_prob": 1.3845153262082022e-05}, {"id": 63, "seek": 17070, "start": 193.33999999999997, "end": 196.94, "text": " to endeavor to get something that is approximately state", "tokens": [281, 34975, 281, 483, 746, 300, 307, 10447, 1785], "temperature": 0.0, "avg_logprob": -0.11643389404797164, "compression_ratio": 1.792828685258964, "no_speech_prob": 1.3845153262082022e-05}, {"id": 64, "seek": 17070, "start": 196.94, "end": 200.06, "text": " of the art on ImageNet in the next week or two.", "tokens": [295, 264, 1523, 322, 29903, 31890, 294, 264, 958, 1243, 420, 732, 13], "temperature": 0.0, "avg_logprob": -0.11643389404797164, "compression_ratio": 1.792828685258964, "no_speech_prob": 1.3845153262082022e-05}, {"id": 65, "seek": 20006, "start": 200.06, "end": 205.26, "text": " So that's the goal, and in our testing at this point,", "tokens": [407, 300, 311, 264, 3387, 11, 293, 294, 527, 4997, 412, 341, 935, 11], "temperature": 0.0, "avg_logprob": -0.14062987532571097, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.0143786969128996e-05}, {"id": 66, "seek": 20006, "start": 205.26, "end": 207.94, "text": " we're feeling pretty good about showing you some stuff", "tokens": [321, 434, 2633, 1238, 665, 466, 4099, 291, 512, 1507], "temperature": 0.0, "avg_logprob": -0.14062987532571097, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.0143786969128996e-05}, {"id": 67, "seek": 20006, "start": 207.94, "end": 212.44, "text": " that maybe hasn't been seen before on ImageNet results.", "tokens": [300, 1310, 6132, 380, 668, 1612, 949, 322, 29903, 31890, 3542, 13], "temperature": 0.0, "avg_logprob": -0.14062987532571097, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.0143786969128996e-05}, {"id": 68, "seek": 20006, "start": 212.44, "end": 216.06, "text": " So that's where we're going to try and head as a group.", "tokens": [407, 300, 311, 689, 321, 434, 516, 281, 853, 293, 1378, 382, 257, 1594, 13], "temperature": 0.0, "avg_logprob": -0.14062987532571097, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.0143786969128996e-05}, {"id": 69, "seek": 20006, "start": 216.06, "end": 217.66, "text": " And so these are some of the things that we're going", "tokens": [400, 370, 613, 366, 512, 295, 264, 721, 300, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.14062987532571097, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.0143786969128996e-05}, {"id": 70, "seek": 20006, "start": 217.66, "end": 222.1, "text": " to be covering to get there.", "tokens": [281, 312, 10322, 281, 483, 456, 13], "temperature": 0.0, "avg_logprob": -0.14062987532571097, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.0143786969128996e-05}, {"id": 71, "seek": 20006, "start": 222.1, "end": 225.26, "text": " One of the things you might not have seen before", "tokens": [1485, 295, 264, 721, 291, 1062, 406, 362, 1612, 949], "temperature": 0.0, "avg_logprob": -0.14062987532571097, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.0143786969128996e-05}, {"id": 72, "seek": 20006, "start": 225.26, "end": 228.24, "text": " in this section called optimization is LAM.", "tokens": [294, 341, 3541, 1219, 19618, 307, 441, 2865, 13], "temperature": 0.0, "avg_logprob": -0.14062987532571097, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.0143786969128996e-05}, {"id": 73, "seek": 22824, "start": 228.24, "end": 230.34, "text": " The reason for this is that this was going to be some", "tokens": [440, 1778, 337, 341, 307, 300, 341, 390, 516, 281, 312, 512], "temperature": 0.0, "avg_logprob": -0.08359360694885254, "compression_ratio": 1.7269372693726937, "no_speech_prob": 2.3179876734502614e-05}, {"id": 74, "seek": 22824, "start": 230.34, "end": 232.3, "text": " of the unpublished research we were going to show you,", "tokens": [295, 264, 20994, 836, 4173, 2132, 321, 645, 516, 281, 855, 291, 11], "temperature": 0.0, "avg_logprob": -0.08359360694885254, "compression_ratio": 1.7269372693726937, "no_speech_prob": 2.3179876734502614e-05}, {"id": 75, "seek": 22824, "start": 232.3, "end": 234.54000000000002, "text": " which is a new optimization algorithm", "tokens": [597, 307, 257, 777, 19618, 9284], "temperature": 0.0, "avg_logprob": -0.08359360694885254, "compression_ratio": 1.7269372693726937, "no_speech_prob": 2.3179876734502614e-05}, {"id": 76, "seek": 22824, "start": 234.54000000000002, "end": 236.94, "text": " that we've been developing.", "tokens": [300, 321, 600, 668, 6416, 13], "temperature": 0.0, "avg_logprob": -0.08359360694885254, "compression_ratio": 1.7269372693726937, "no_speech_prob": 2.3179876734502614e-05}, {"id": 77, "seek": 22824, "start": 236.94, "end": 238.74, "text": " The framework is still going to be new,", "tokens": [440, 8388, 307, 920, 516, 281, 312, 777, 11], "temperature": 0.0, "avg_logprob": -0.08359360694885254, "compression_ratio": 1.7269372693726937, "no_speech_prob": 2.3179876734502614e-05}, {"id": 78, "seek": 22824, "start": 238.74, "end": 240.94, "text": " but actually the particular approach", "tokens": [457, 767, 264, 1729, 3109], "temperature": 0.0, "avg_logprob": -0.08359360694885254, "compression_ratio": 1.7269372693726937, "no_speech_prob": 2.3179876734502614e-05}, {"id": 79, "seek": 22824, "start": 240.94, "end": 245.26000000000002, "text": " to using it was published by Google two days ago.", "tokens": [281, 1228, 309, 390, 6572, 538, 3329, 732, 1708, 2057, 13], "temperature": 0.0, "avg_logprob": -0.08359360694885254, "compression_ratio": 1.7269372693726937, "no_speech_prob": 2.3179876734502614e-05}, {"id": 80, "seek": 22824, "start": 245.26000000000002, "end": 246.94, "text": " So we've kind of been scooped there.", "tokens": [407, 321, 600, 733, 295, 668, 19555, 292, 456, 13], "temperature": 0.0, "avg_logprob": -0.08359360694885254, "compression_ratio": 1.7269372693726937, "no_speech_prob": 2.3179876734502614e-05}, {"id": 81, "seek": 22824, "start": 246.94, "end": 249.5, "text": " So this is a cool paper, really great,", "tokens": [407, 341, 307, 257, 1627, 3035, 11, 534, 869, 11], "temperature": 0.0, "avg_logprob": -0.08359360694885254, "compression_ratio": 1.7269372693726937, "no_speech_prob": 2.3179876734502614e-05}, {"id": 82, "seek": 22824, "start": 249.5, "end": 252.0, "text": " and they introduce a new optimization algorithm called", "tokens": [293, 436, 5366, 257, 777, 19618, 9284, 1219], "temperature": 0.0, "avg_logprob": -0.08359360694885254, "compression_ratio": 1.7269372693726937, "no_speech_prob": 2.3179876734502614e-05}, {"id": 83, "seek": 22824, "start": 252.0, "end": 255.0, "text": " LAM, which we'll be showing you how", "tokens": [441, 2865, 11, 597, 321, 603, 312, 4099, 291, 577], "temperature": 0.0, "avg_logprob": -0.08359360694885254, "compression_ratio": 1.7269372693726937, "no_speech_prob": 2.3179876734502614e-05}, {"id": 84, "seek": 25500, "start": 255.0, "end": 258.5, "text": " to implement it very easily.", "tokens": [281, 4445, 309, 588, 3612, 13], "temperature": 0.0, "avg_logprob": -0.14802115067191746, "compression_ratio": 1.6221374045801527, "no_speech_prob": 3.882317469106056e-05}, {"id": 85, "seek": 25500, "start": 258.5, "end": 260.46, "text": " And if you're wondering how we're able to do that so fast,", "tokens": [400, 498, 291, 434, 6359, 577, 321, 434, 1075, 281, 360, 300, 370, 2370, 11], "temperature": 0.0, "avg_logprob": -0.14802115067191746, "compression_ratio": 1.6221374045801527, "no_speech_prob": 3.882317469106056e-05}, {"id": 86, "seek": 25500, "start": 260.46, "end": 262.06, "text": " it's because we've kind of been working", "tokens": [309, 311, 570, 321, 600, 733, 295, 668, 1364], "temperature": 0.0, "avg_logprob": -0.14802115067191746, "compression_ratio": 1.6221374045801527, "no_speech_prob": 3.882317469106056e-05}, {"id": 87, "seek": 25500, "start": 262.06, "end": 264.86, "text": " on the same thing ourselves for a few weeks now.", "tokens": [322, 264, 912, 551, 4175, 337, 257, 1326, 3259, 586, 13], "temperature": 0.0, "avg_logprob": -0.14802115067191746, "compression_ratio": 1.6221374045801527, "no_speech_prob": 3.882317469106056e-05}, {"id": 88, "seek": 25500, "start": 264.86, "end": 271.5, "text": " So then from next week, we'll start also developing a", "tokens": [407, 550, 490, 958, 1243, 11, 321, 603, 722, 611, 6416, 257], "temperature": 0.0, "avg_logprob": -0.14802115067191746, "compression_ratio": 1.6221374045801527, "no_speech_prob": 3.882317469106056e-05}, {"id": 89, "seek": 25500, "start": 271.5, "end": 275.66, "text": " completely new FastAI module called FastAI.audio.", "tokens": [2584, 777, 15968, 48698, 10088, 1219, 15968, 48698, 13, 46069, 13], "temperature": 0.0, "avg_logprob": -0.14802115067191746, "compression_ratio": 1.6221374045801527, "no_speech_prob": 3.882317469106056e-05}, {"id": 90, "seek": 25500, "start": 275.66, "end": 278.14, "text": " So you'll be seeing how to actually create modules", "tokens": [407, 291, 603, 312, 2577, 577, 281, 767, 1884, 16679], "temperature": 0.0, "avg_logprob": -0.14802115067191746, "compression_ratio": 1.6221374045801527, "no_speech_prob": 3.882317469106056e-05}, {"id": 91, "seek": 25500, "start": 278.14, "end": 281.06, "text": " and how to write Jupyter documentation and tests.", "tokens": [293, 577, 281, 2464, 22125, 88, 391, 14333, 293, 6921, 13], "temperature": 0.0, "avg_logprob": -0.14802115067191746, "compression_ratio": 1.6221374045801527, "no_speech_prob": 3.882317469106056e-05}, {"id": 92, "seek": 25500, "start": 281.06, "end": 282.74, "text": " And we're going to be learning about audio,", "tokens": [400, 321, 434, 516, 281, 312, 2539, 466, 6278, 11], "temperature": 0.0, "avg_logprob": -0.14802115067191746, "compression_ratio": 1.6221374045801527, "no_speech_prob": 3.882317469106056e-05}, {"id": 93, "seek": 28274, "start": 282.74, "end": 285.44, "text": " such as complex numbers and Fourier transforms,", "tokens": [1270, 382, 3997, 3547, 293, 36810, 35592, 11], "temperature": 0.0, "avg_logprob": -0.14317870506873498, "compression_ratio": 1.755639097744361, "no_speech_prob": 9.457815031055361e-05}, {"id": 94, "seek": 28274, "start": 285.44, "end": 288.46000000000004, "text": " which if you're like me, at this point you're going, oh, what?", "tokens": [597, 498, 291, 434, 411, 385, 11, 412, 341, 935, 291, 434, 516, 11, 1954, 11, 437, 30], "temperature": 0.0, "avg_logprob": -0.14317870506873498, "compression_ratio": 1.755639097744361, "no_speech_prob": 9.457815031055361e-05}, {"id": 95, "seek": 28274, "start": 288.46000000000004, "end": 292.6, "text": " No. Because I managed to spend my life avoiding complex numbers", "tokens": [883, 13, 1436, 286, 6453, 281, 3496, 452, 993, 20220, 3997, 3547], "temperature": 0.0, "avg_logprob": -0.14317870506873498, "compression_ratio": 1.755639097744361, "no_speech_prob": 9.457815031055361e-05}, {"id": 96, "seek": 28274, "start": 292.6, "end": 294.36, "text": " and Fourier transforms on the whole.", "tokens": [293, 36810, 35592, 322, 264, 1379, 13], "temperature": 0.0, "avg_logprob": -0.14317870506873498, "compression_ratio": 1.755639097744361, "no_speech_prob": 9.457815031055361e-05}, {"id": 97, "seek": 28274, "start": 294.36, "end": 297.66, "text": " But don't worry, it'll be okay.", "tokens": [583, 500, 380, 3292, 11, 309, 603, 312, 1392, 13], "temperature": 0.0, "avg_logprob": -0.14317870506873498, "compression_ratio": 1.755639097744361, "no_speech_prob": 9.457815031055361e-05}, {"id": 98, "seek": 28274, "start": 297.66, "end": 301.0, "text": " It's actually not at all bad, or at least the bits we need", "tokens": [467, 311, 767, 406, 412, 439, 1578, 11, 420, 412, 1935, 264, 9239, 321, 643], "temperature": 0.0, "avg_logprob": -0.14317870506873498, "compression_ratio": 1.755639097744361, "no_speech_prob": 9.457815031055361e-05}, {"id": 99, "seek": 28274, "start": 301.0, "end": 302.54, "text": " to learn about are not at all bad,", "tokens": [281, 1466, 466, 366, 406, 412, 439, 1578, 11], "temperature": 0.0, "avg_logprob": -0.14317870506873498, "compression_ratio": 1.755639097744361, "no_speech_prob": 9.457815031055361e-05}, {"id": 100, "seek": 28274, "start": 302.54, "end": 304.66, "text": " and you'll totally get it even", "tokens": [293, 291, 603, 3879, 483, 309, 754], "temperature": 0.0, "avg_logprob": -0.14317870506873498, "compression_ratio": 1.755639097744361, "no_speech_prob": 9.457815031055361e-05}, {"id": 101, "seek": 28274, "start": 304.66, "end": 307.66, "text": " if you've never ever touched these before.", "tokens": [498, 291, 600, 1128, 1562, 9828, 613, 949, 13], "temperature": 0.0, "avg_logprob": -0.14317870506873498, "compression_ratio": 1.755639097744361, "no_speech_prob": 9.457815031055361e-05}, {"id": 102, "seek": 28274, "start": 307.66, "end": 310.86, "text": " We'll be learning about audio formats and spectrograms,", "tokens": [492, 603, 312, 2539, 466, 6278, 25879, 293, 6177, 340, 1342, 82, 11], "temperature": 0.0, "avg_logprob": -0.14317870506873498, "compression_ratio": 1.755639097744361, "no_speech_prob": 9.457815031055361e-05}, {"id": 103, "seek": 31086, "start": 310.86, "end": 313.84000000000003, "text": " doing data augmentation and things that aren't images,", "tokens": [884, 1412, 14501, 19631, 293, 721, 300, 3212, 380, 5267, 11], "temperature": 0.0, "avg_logprob": -0.12650393743584626, "compression_ratio": 1.698051948051948, "no_speech_prob": 1.3418856724456418e-05}, {"id": 104, "seek": 31086, "start": 313.84000000000003, "end": 316.34000000000003, "text": " and some particular kinds of loss functions", "tokens": [293, 512, 1729, 3685, 295, 4470, 6828], "temperature": 0.0, "avg_logprob": -0.12650393743584626, "compression_ratio": 1.698051948051948, "no_speech_prob": 1.3418856724456418e-05}, {"id": 105, "seek": 31086, "start": 316.34000000000003, "end": 318.74, "text": " and architectures for audio.", "tokens": [293, 6331, 1303, 337, 6278, 13], "temperature": 0.0, "avg_logprob": -0.12650393743584626, "compression_ratio": 1.698051948051948, "no_speech_prob": 1.3418856724456418e-05}, {"id": 106, "seek": 31086, "start": 318.74, "end": 321.0, "text": " And, you know, as much as anything,", "tokens": [400, 11, 291, 458, 11, 382, 709, 382, 1340, 11], "temperature": 0.0, "avg_logprob": -0.12650393743584626, "compression_ratio": 1.698051948051948, "no_speech_prob": 1.3418856724456418e-05}, {"id": 107, "seek": 31086, "start": 321.0, "end": 323.34000000000003, "text": " it'll just be a great kind of exercise in, okay,", "tokens": [309, 603, 445, 312, 257, 869, 733, 295, 5380, 294, 11, 1392, 11], "temperature": 0.0, "avg_logprob": -0.12650393743584626, "compression_ratio": 1.698051948051948, "no_speech_prob": 1.3418856724456418e-05}, {"id": 108, "seek": 31086, "start": 323.34000000000003, "end": 326.0, "text": " I've got some different data type that's not in FastAI.", "tokens": [286, 600, 658, 512, 819, 1412, 2010, 300, 311, 406, 294, 15968, 48698, 13], "temperature": 0.0, "avg_logprob": -0.12650393743584626, "compression_ratio": 1.698051948051948, "no_speech_prob": 1.3418856724456418e-05}, {"id": 109, "seek": 31086, "start": 326.0, "end": 330.74, "text": " How do I build up all the bits I need to make it work?", "tokens": [1012, 360, 286, 1322, 493, 439, 264, 9239, 286, 643, 281, 652, 309, 589, 30], "temperature": 0.0, "avg_logprob": -0.12650393743584626, "compression_ratio": 1.698051948051948, "no_speech_prob": 1.3418856724456418e-05}, {"id": 110, "seek": 31086, "start": 330.74, "end": 333.56, "text": " Then we'll be looking at neural translation as a way to learn", "tokens": [1396, 321, 603, 312, 1237, 412, 18161, 12853, 382, 257, 636, 281, 1466], "temperature": 0.0, "avg_logprob": -0.12650393743584626, "compression_ratio": 1.698051948051948, "no_speech_prob": 1.3418856724456418e-05}, {"id": 111, "seek": 31086, "start": 333.56, "end": 335.84000000000003, "text": " about sequence to sequence with attention models,", "tokens": [466, 8310, 281, 8310, 365, 3202, 5245, 11], "temperature": 0.0, "avg_logprob": -0.12650393743584626, "compression_ratio": 1.698051948051948, "no_speech_prob": 1.3418856724456418e-05}, {"id": 112, "seek": 31086, "start": 335.84000000000003, "end": 337.44, "text": " and then we'll be going deeper and deeper", "tokens": [293, 550, 321, 603, 312, 516, 7731, 293, 7731], "temperature": 0.0, "avg_logprob": -0.12650393743584626, "compression_ratio": 1.698051948051948, "no_speech_prob": 1.3418856724456418e-05}, {"id": 113, "seek": 31086, "start": 337.44, "end": 339.8, "text": " into attention models looking at transformer.", "tokens": [666, 3202, 5245, 1237, 412, 31782, 13], "temperature": 0.0, "avg_logprob": -0.12650393743584626, "compression_ratio": 1.698051948051948, "no_speech_prob": 1.3418856724456418e-05}, {"id": 114, "seek": 33980, "start": 339.8, "end": 345.06, "text": " And it's even more fantastic, descendant transformer Excel.", "tokens": [400, 309, 311, 754, 544, 5456, 11, 16333, 394, 31782, 19060, 13], "temperature": 0.0, "avg_logprob": -0.1454293932233538, "compression_ratio": 1.5677655677655677, "no_speech_prob": 1.1658047696982976e-05}, {"id": 115, "seek": 33980, "start": 345.06, "end": 350.7, "text": " And then we'll wrap up our Python adventures with a deep dive", "tokens": [400, 550, 321, 603, 7019, 493, 527, 15329, 20905, 365, 257, 2452, 9192], "temperature": 0.0, "avg_logprob": -0.1454293932233538, "compression_ratio": 1.5677655677655677, "no_speech_prob": 1.1658047696982976e-05}, {"id": 116, "seek": 33980, "start": 350.7, "end": 353.40000000000003, "text": " into some really interesting vision topics,", "tokens": [666, 512, 534, 1880, 5201, 8378, 11], "temperature": 0.0, "avg_logprob": -0.1454293932233538, "compression_ratio": 1.5677655677655677, "no_speech_prob": 1.1658047696982976e-05}, {"id": 117, "seek": 33980, "start": 353.40000000000003, "end": 356.46000000000004, "text": " which is going to require building some bigger models.", "tokens": [597, 307, 516, 281, 3651, 2390, 512, 3801, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1454293932233538, "compression_ratio": 1.5677655677655677, "no_speech_prob": 1.1658047696982976e-05}, {"id": 118, "seek": 33980, "start": 356.46000000000004, "end": 359.06, "text": " So we'll talk about how to build your own deep learning box,", "tokens": [407, 321, 603, 751, 466, 577, 281, 1322, 428, 1065, 2452, 2539, 2424, 11], "temperature": 0.0, "avg_logprob": -0.1454293932233538, "compression_ratio": 1.5677655677655677, "no_speech_prob": 1.1658047696982976e-05}, {"id": 119, "seek": 33980, "start": 359.06, "end": 361.86, "text": " how to run big experiments on AWS", "tokens": [577, 281, 1190, 955, 12050, 322, 17650], "temperature": 0.0, "avg_logprob": -0.1454293932233538, "compression_ratio": 1.5677655677655677, "no_speech_prob": 1.1658047696982976e-05}, {"id": 120, "seek": 33980, "start": 361.86, "end": 365.06, "text": " with a new library we've developed called FastEC2.", "tokens": [365, 257, 777, 6405, 321, 600, 4743, 1219, 15968, 8140, 17, 13], "temperature": 0.0, "avg_logprob": -0.1454293932233538, "compression_ratio": 1.5677655677655677, "no_speech_prob": 1.1658047696982976e-05}, {"id": 121, "seek": 33980, "start": 365.06, "end": 368.90000000000003, "text": " And then we're going to see exactly what happened last course", "tokens": [400, 550, 321, 434, 516, 281, 536, 2293, 437, 2011, 1036, 1164], "temperature": 0.0, "avg_logprob": -0.1454293932233538, "compression_ratio": 1.5677655677655677, "no_speech_prob": 1.1658047696982976e-05}, {"id": 122, "seek": 36890, "start": 368.9, "end": 373.26, "text": " when we did that UNet super resolution image generation,", "tokens": [562, 321, 630, 300, 8229, 302, 1687, 8669, 3256, 5125, 11], "temperature": 0.0, "avg_logprob": -0.14572860399881998, "compression_ratio": 1.6787003610108304, "no_speech_prob": 1.4284044482337777e-05}, {"id": 123, "seek": 36890, "start": 373.26, "end": 374.9, "text": " what are some of the pieces there.", "tokens": [437, 366, 512, 295, 264, 3755, 456, 13], "temperature": 0.0, "avg_logprob": -0.14572860399881998, "compression_ratio": 1.6787003610108304, "no_speech_prob": 1.4284044482337777e-05}, {"id": 124, "seek": 36890, "start": 374.9, "end": 377.29999999999995, "text": " And we've actually got some really exciting new results", "tokens": [400, 321, 600, 767, 658, 512, 534, 4670, 777, 3542], "temperature": 0.0, "avg_logprob": -0.14572860399881998, "compression_ratio": 1.6787003610108304, "no_speech_prob": 1.4284044482337777e-05}, {"id": 125, "seek": 36890, "start": 377.29999999999995, "end": 379.7, "text": " to show you, which have been done in collaboration", "tokens": [281, 855, 291, 11, 597, 362, 668, 1096, 294, 9363], "temperature": 0.0, "avg_logprob": -0.14572860399881998, "compression_ratio": 1.6787003610108304, "no_speech_prob": 1.4284044482337777e-05}, {"id": 126, "seek": 36890, "start": 379.7, "end": 381.09999999999997, "text": " with some really cool partners.", "tokens": [365, 512, 534, 1627, 4462, 13], "temperature": 0.0, "avg_logprob": -0.14572860399881998, "compression_ratio": 1.6787003610108304, "no_speech_prob": 1.4284044482337777e-05}, {"id": 127, "seek": 36890, "start": 381.09999999999997, "end": 383.15999999999997, "text": " So I'm looking forward to showing you that.", "tokens": [407, 286, 478, 1237, 2128, 281, 4099, 291, 300, 13], "temperature": 0.0, "avg_logprob": -0.14572860399881998, "compression_ratio": 1.6787003610108304, "no_speech_prob": 1.4284044482337777e-05}, {"id": 128, "seek": 36890, "start": 383.15999999999997, "end": 387.29999999999995, "text": " To give you a tip, generative video models is what we're going", "tokens": [1407, 976, 291, 257, 4125, 11, 1337, 1166, 960, 5245, 307, 437, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.14572860399881998, "compression_ratio": 1.6787003610108304, "no_speech_prob": 1.4284044482337777e-05}, {"id": 129, "seek": 36890, "start": 387.29999999999995, "end": 388.59999999999997, "text": " to be looking at.", "tokens": [281, 312, 1237, 412, 13], "temperature": 0.0, "avg_logprob": -0.14572860399881998, "compression_ratio": 1.6787003610108304, "no_speech_prob": 1.4284044482337777e-05}, {"id": 130, "seek": 36890, "start": 388.59999999999997, "end": 389.9, "text": " And then we'll be looking", "tokens": [400, 550, 321, 603, 312, 1237], "temperature": 0.0, "avg_logprob": -0.14572860399881998, "compression_ratio": 1.6787003610108304, "no_speech_prob": 1.4284044482337777e-05}, {"id": 131, "seek": 36890, "start": 389.9, "end": 392.26, "text": " at some interesting different applications, device,", "tokens": [412, 512, 1880, 819, 5821, 11, 4302, 11], "temperature": 0.0, "avg_logprob": -0.14572860399881998, "compression_ratio": 1.6787003610108304, "no_speech_prob": 1.4284044482337777e-05}, {"id": 132, "seek": 36890, "start": 392.26, "end": 394.9, "text": " cycleGAN, and object detection.", "tokens": [6586, 27699, 11, 293, 2657, 17784, 13], "temperature": 0.0, "avg_logprob": -0.14572860399881998, "compression_ratio": 1.6787003610108304, "no_speech_prob": 1.4284044482337777e-05}, {"id": 133, "seek": 39490, "start": 394.9, "end": 400.85999999999996, "text": " And then Swift, of course.", "tokens": [400, 550, 25539, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.12852710026961106, "compression_ratio": 1.6812227074235808, "no_speech_prob": 2.2821897800895385e-05}, {"id": 134, "seek": 39490, "start": 400.85999999999996, "end": 404.62, "text": " So the Swift lessons are coming together nicely,", "tokens": [407, 264, 25539, 8820, 366, 1348, 1214, 9594, 11], "temperature": 0.0, "avg_logprob": -0.12852710026961106, "compression_ratio": 1.6812227074235808, "no_speech_prob": 2.2821897800895385e-05}, {"id": 135, "seek": 39490, "start": 404.62, "end": 406.02, "text": " really excited about them.", "tokens": [534, 2919, 466, 552, 13], "temperature": 0.0, "avg_logprob": -0.12852710026961106, "compression_ratio": 1.6812227074235808, "no_speech_prob": 2.2821897800895385e-05}, {"id": 136, "seek": 39490, "start": 406.02, "end": 410.29999999999995, "text": " And we'll be covering as much of the same territory as we can.", "tokens": [400, 321, 603, 312, 10322, 382, 709, 295, 264, 912, 11360, 382, 321, 393, 13], "temperature": 0.0, "avg_logprob": -0.12852710026961106, "compression_ratio": 1.6812227074235808, "no_speech_prob": 2.2821897800895385e-05}, {"id": 137, "seek": 39490, "start": 410.29999999999995, "end": 412.06, "text": " But obviously it'll be in Swift and it'll be", "tokens": [583, 2745, 309, 603, 312, 294, 25539, 293, 309, 603, 312], "temperature": 0.0, "avg_logprob": -0.12852710026961106, "compression_ratio": 1.6812227074235808, "no_speech_prob": 2.2821897800895385e-05}, {"id": 138, "seek": 39490, "start": 412.06, "end": 414.41999999999996, "text": " in only two lessons, so it won't be everything.", "tokens": [294, 787, 732, 8820, 11, 370, 309, 1582, 380, 312, 1203, 13], "temperature": 0.0, "avg_logprob": -0.12852710026961106, "compression_ratio": 1.6812227074235808, "no_speech_prob": 2.2821897800895385e-05}, {"id": 139, "seek": 39490, "start": 414.41999999999996, "end": 418.58, "text": " But we'll try to give you enough of a taste that you'll feel", "tokens": [583, 321, 603, 853, 281, 976, 291, 1547, 295, 257, 3939, 300, 291, 603, 841], "temperature": 0.0, "avg_logprob": -0.12852710026961106, "compression_ratio": 1.6812227074235808, "no_speech_prob": 2.2821897800895385e-05}, {"id": 140, "seek": 39490, "start": 418.58, "end": 422.17999999999995, "text": " like you understand why Swift is important and how to get started", "tokens": [411, 291, 1223, 983, 25539, 307, 1021, 293, 577, 281, 483, 1409], "temperature": 0.0, "avg_logprob": -0.12852710026961106, "compression_ratio": 1.6812227074235808, "no_speech_prob": 2.2821897800895385e-05}, {"id": 141, "seek": 42218, "start": 422.18, "end": 425.22, "text": " with building something similar in Swift.", "tokens": [365, 2390, 746, 2531, 294, 25539, 13], "temperature": 0.0, "avg_logprob": -0.14617958403470224, "compression_ratio": 1.6586345381526104, "no_speech_prob": 6.24058657194837e-06}, {"id": 142, "seek": 42218, "start": 425.22, "end": 426.7, "text": " And maybe building out the whole thing", "tokens": [400, 1310, 2390, 484, 264, 1379, 551], "temperature": 0.0, "avg_logprob": -0.14617958403470224, "compression_ratio": 1.6586345381526104, "no_speech_prob": 6.24058657194837e-06}, {"id": 143, "seek": 42218, "start": 426.7, "end": 429.5, "text": " in Swift will take the next 12 months.", "tokens": [294, 25539, 486, 747, 264, 958, 2272, 2493, 13], "temperature": 0.0, "avg_logprob": -0.14617958403470224, "compression_ratio": 1.6586345381526104, "no_speech_prob": 6.24058657194837e-06}, {"id": 144, "seek": 42218, "start": 429.5, "end": 430.8, "text": " Who knows?", "tokens": [2102, 3255, 30], "temperature": 0.0, "avg_logprob": -0.14617958403470224, "compression_ratio": 1.6586345381526104, "no_speech_prob": 6.24058657194837e-06}, {"id": 145, "seek": 42218, "start": 430.8, "end": 432.1, "text": " We'll see.", "tokens": [492, 603, 536, 13], "temperature": 0.0, "avg_logprob": -0.14617958403470224, "compression_ratio": 1.6586345381526104, "no_speech_prob": 6.24058657194837e-06}, {"id": 146, "seek": 42218, "start": 432.1, "end": 439.62, "text": " So we're going to start today on 05A foundations.", "tokens": [407, 321, 434, 516, 281, 722, 965, 322, 1958, 20, 32, 22467, 13], "temperature": 0.0, "avg_logprob": -0.14617958403470224, "compression_ratio": 1.6586345381526104, "no_speech_prob": 6.24058657194837e-06}, {"id": 147, "seek": 42218, "start": 439.62, "end": 442.46000000000004, "text": " And what we're going to do is we're going to recover some", "tokens": [400, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 8114, 512], "temperature": 0.0, "avg_logprob": -0.14617958403470224, "compression_ratio": 1.6586345381526104, "no_speech_prob": 6.24058657194837e-06}, {"id": 148, "seek": 42218, "start": 442.46000000000004, "end": 445.18, "text": " of the software engineering and math basics", "tokens": [295, 264, 4722, 7043, 293, 5221, 14688], "temperature": 0.0, "avg_logprob": -0.14617958403470224, "compression_ratio": 1.6586345381526104, "no_speech_prob": 6.24058657194837e-06}, {"id": 149, "seek": 42218, "start": 445.18, "end": 446.94, "text": " that we were relying on last week,", "tokens": [300, 321, 645, 24140, 322, 1036, 1243, 11], "temperature": 0.0, "avg_logprob": -0.14617958403470224, "compression_ratio": 1.6586345381526104, "no_speech_prob": 6.24058657194837e-06}, {"id": 150, "seek": 42218, "start": 446.94, "end": 448.7, "text": " and going into a little bit more detail.", "tokens": [293, 516, 666, 257, 707, 857, 544, 2607, 13], "temperature": 0.0, "avg_logprob": -0.14617958403470224, "compression_ratio": 1.6586345381526104, "no_speech_prob": 6.24058657194837e-06}, {"id": 151, "seek": 42218, "start": 448.7, "end": 451.22, "text": " Specifically, we'll be looking at callbacks", "tokens": [26058, 11, 321, 603, 312, 1237, 412, 818, 17758], "temperature": 0.0, "avg_logprob": -0.14617958403470224, "compression_ratio": 1.6586345381526104, "no_speech_prob": 6.24058657194837e-06}, {"id": 152, "seek": 45122, "start": 451.22, "end": 455.98, "text": " and variance, and a couple of other Python concepts", "tokens": [293, 21977, 11, 293, 257, 1916, 295, 661, 15329, 10392], "temperature": 0.0, "avg_logprob": -0.13948937466270045, "compression_ratio": 1.6006944444444444, "no_speech_prob": 1.1841959349112585e-05}, {"id": 153, "seek": 45122, "start": 455.98, "end": 458.38000000000005, "text": " like Dunder special methods.", "tokens": [411, 413, 6617, 2121, 7150, 13], "temperature": 0.0, "avg_logprob": -0.13948937466270045, "compression_ratio": 1.6006944444444444, "no_speech_prob": 1.1841959349112585e-05}, {"id": 154, "seek": 45122, "start": 458.38000000000005, "end": 460.82000000000005, "text": " If you're familiar with those things, feel free to skip ahead", "tokens": [759, 291, 434, 4963, 365, 729, 721, 11, 841, 1737, 281, 10023, 2286], "temperature": 0.0, "avg_logprob": -0.13948937466270045, "compression_ratio": 1.6006944444444444, "no_speech_prob": 1.1841959349112585e-05}, {"id": 155, "seek": 45122, "start": 460.82000000000005, "end": 464.82000000000005, "text": " if you're watching the video until we get to the new material.", "tokens": [498, 291, 434, 1976, 264, 960, 1826, 321, 483, 281, 264, 777, 2527, 13], "temperature": 0.0, "avg_logprob": -0.13948937466270045, "compression_ratio": 1.6006944444444444, "no_speech_prob": 1.1841959349112585e-05}, {"id": 156, "seek": 45122, "start": 464.82000000000005, "end": 468.62, "text": " But callbacks, as I'm sure you've seen, are super important", "tokens": [583, 818, 17758, 11, 382, 286, 478, 988, 291, 600, 1612, 11, 366, 1687, 1021], "temperature": 0.0, "avg_logprob": -0.13948937466270045, "compression_ratio": 1.6006944444444444, "no_speech_prob": 1.1841959349112585e-05}, {"id": 157, "seek": 45122, "start": 468.62, "end": 473.14000000000004, "text": " for fast AI, and in general they're a really useful technique", "tokens": [337, 2370, 7318, 11, 293, 294, 2674, 436, 434, 257, 534, 4420, 6532], "temperature": 0.0, "avg_logprob": -0.13948937466270045, "compression_ratio": 1.6006944444444444, "no_speech_prob": 1.1841959349112585e-05}, {"id": 158, "seek": 45122, "start": 473.14000000000004, "end": 474.78000000000003, "text": " for software engineering.", "tokens": [337, 4722, 7043, 13], "temperature": 0.0, "avg_logprob": -0.13948937466270045, "compression_ratio": 1.6006944444444444, "no_speech_prob": 1.1841959349112585e-05}, {"id": 159, "seek": 45122, "start": 474.78000000000003, "end": 477.22, "text": " And great for researchers because they allow you", "tokens": [400, 869, 337, 10309, 570, 436, 2089, 291], "temperature": 0.0, "avg_logprob": -0.13948937466270045, "compression_ratio": 1.6006944444444444, "no_speech_prob": 1.1841959349112585e-05}, {"id": 160, "seek": 45122, "start": 477.22, "end": 480.66, "text": " to build things that you can quickly adjust and add things", "tokens": [281, 1322, 721, 300, 291, 393, 2661, 4369, 293, 909, 721], "temperature": 0.0, "avg_logprob": -0.13948937466270045, "compression_ratio": 1.6006944444444444, "no_speech_prob": 1.1841959349112585e-05}, {"id": 161, "seek": 48066, "start": 480.66, "end": 482.14000000000004, "text": " in and pull them out again.", "tokens": [294, 293, 2235, 552, 484, 797, 13], "temperature": 0.0, "avg_logprob": -0.18449664868806537, "compression_ratio": 1.5707070707070707, "no_speech_prob": 7.071613254083786e-06}, {"id": 162, "seek": 48066, "start": 482.14000000000004, "end": 484.26000000000005, "text": " So really great for research as well.", "tokens": [407, 534, 869, 337, 2132, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.18449664868806537, "compression_ratio": 1.5707070707070707, "no_speech_prob": 7.071613254083786e-06}, {"id": 163, "seek": 48066, "start": 484.26000000000005, "end": 485.98, "text": " So what is a callback?", "tokens": [407, 437, 307, 257, 818, 3207, 30], "temperature": 0.0, "avg_logprob": -0.18449664868806537, "compression_ratio": 1.5707070707070707, "no_speech_prob": 7.071613254083786e-06}, {"id": 164, "seek": 48066, "start": 485.98, "end": 488.26000000000005, "text": " Let's look at an example.", "tokens": [961, 311, 574, 412, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.18449664868806537, "compression_ratio": 1.5707070707070707, "no_speech_prob": 7.071613254083786e-06}, {"id": 165, "seek": 48066, "start": 488.26000000000005, "end": 494.54, "text": " So here's a function called f, which prints out hi.", "tokens": [407, 510, 311, 257, 2445, 1219, 283, 11, 597, 22305, 484, 4879, 13], "temperature": 0.0, "avg_logprob": -0.18449664868806537, "compression_ratio": 1.5707070707070707, "no_speech_prob": 7.071613254083786e-06}, {"id": 166, "seek": 48066, "start": 494.54, "end": 498.46000000000004, "text": " And I'm going to create a button.", "tokens": [400, 286, 478, 516, 281, 1884, 257, 2960, 13], "temperature": 0.0, "avg_logprob": -0.18449664868806537, "compression_ratio": 1.5707070707070707, "no_speech_prob": 7.071613254083786e-06}, {"id": 167, "seek": 48066, "start": 498.46000000000004, "end": 501.34000000000003, "text": " And I'm going to create this button using ipywidgets,", "tokens": [400, 286, 478, 516, 281, 1884, 341, 2960, 1228, 28501, 88, 17697, 16284, 11], "temperature": 0.0, "avg_logprob": -0.18449664868806537, "compression_ratio": 1.5707070707070707, "no_speech_prob": 7.071613254083786e-06}, {"id": 168, "seek": 48066, "start": 501.34000000000003, "end": 506.06, "text": " which is a framework for creating GUI widgets in Python.", "tokens": [597, 307, 257, 8388, 337, 4084, 17917, 40, 43355, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.18449664868806537, "compression_ratio": 1.5707070707070707, "no_speech_prob": 7.071613254083786e-06}, {"id": 169, "seek": 50606, "start": 506.06, "end": 512.98, "text": " So if I run, if I say w, then it shows me a button,", "tokens": [407, 498, 286, 1190, 11, 498, 286, 584, 261, 11, 550, 309, 3110, 385, 257, 2960, 11], "temperature": 0.0, "avg_logprob": -0.1482108639132592, "compression_ratio": 1.6195652173913044, "no_speech_prob": 4.756560883834027e-05}, {"id": 170, "seek": 50606, "start": 512.98, "end": 513.7, "text": " which says click me.", "tokens": [597, 1619, 2052, 385, 13], "temperature": 0.0, "avg_logprob": -0.1482108639132592, "compression_ratio": 1.6195652173913044, "no_speech_prob": 4.756560883834027e-05}, {"id": 171, "seek": 50606, "start": 513.7, "end": 517.58, "text": " And I can click on it, and nothing happens.", "tokens": [400, 286, 393, 2052, 322, 309, 11, 293, 1825, 2314, 13], "temperature": 0.0, "avg_logprob": -0.1482108639132592, "compression_ratio": 1.6195652173913044, "no_speech_prob": 4.756560883834027e-05}, {"id": 172, "seek": 50606, "start": 517.58, "end": 519.34, "text": " So how do I get something to happen?", "tokens": [407, 577, 360, 286, 483, 746, 281, 1051, 30], "temperature": 0.0, "avg_logprob": -0.1482108639132592, "compression_ratio": 1.6195652173913044, "no_speech_prob": 4.756560883834027e-05}, {"id": 173, "seek": 50606, "start": 519.34, "end": 524.86, "text": " Well, what I need to do is I need to pass a function", "tokens": [1042, 11, 437, 286, 643, 281, 360, 307, 286, 643, 281, 1320, 257, 2445], "temperature": 0.0, "avg_logprob": -0.1482108639132592, "compression_ratio": 1.6195652173913044, "no_speech_prob": 4.756560883834027e-05}, {"id": 174, "seek": 50606, "start": 524.86, "end": 530.06, "text": " to the ipywidgets framework to say, please run this function", "tokens": [281, 264, 28501, 88, 17697, 16284, 8388, 281, 584, 11, 1767, 1190, 341, 2445], "temperature": 0.0, "avg_logprob": -0.1482108639132592, "compression_ratio": 1.6195652173913044, "no_speech_prob": 4.756560883834027e-05}, {"id": 175, "seek": 50606, "start": 530.06, "end": 532.26, "text": " when you click on this button.", "tokens": [562, 291, 2052, 322, 341, 2960, 13], "temperature": 0.0, "avg_logprob": -0.1482108639132592, "compression_ratio": 1.6195652173913044, "no_speech_prob": 4.756560883834027e-05}, {"id": 176, "seek": 53226, "start": 532.26, "end": 537.66, "text": " So ipywidget.doc says that there's a onClick method, which", "tokens": [407, 28501, 88, 17697, 847, 13, 39966, 1619, 300, 456, 311, 257, 322, 9966, 618, 3170, 11, 597], "temperature": 0.0, "avg_logprob": -0.18781148303638806, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.6684627300710417e-05}, {"id": 177, "seek": 53226, "start": 537.66, "end": 539.78, "text": " can register a function to be called", "tokens": [393, 7280, 257, 2445, 281, 312, 1219], "temperature": 0.0, "avg_logprob": -0.18781148303638806, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.6684627300710417e-05}, {"id": 178, "seek": 53226, "start": 539.78, "end": 541.06, "text": " when the button is clicked.", "tokens": [562, 264, 2960, 307, 23370, 13], "temperature": 0.0, "avg_logprob": -0.18781148303638806, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.6684627300710417e-05}, {"id": 179, "seek": 53226, "start": 541.06, "end": 546.7, "text": " So let's try running that method passing at f, my function.", "tokens": [407, 718, 311, 853, 2614, 300, 3170, 8437, 412, 283, 11, 452, 2445, 13], "temperature": 0.0, "avg_logprob": -0.18781148303638806, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.6684627300710417e-05}, {"id": 180, "seek": 53226, "start": 546.7, "end": 547.74, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.18781148303638806, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.6684627300710417e-05}, {"id": 181, "seek": 53226, "start": 547.74, "end": 550.1, "text": " So now nothing happened.", "tokens": [407, 586, 1825, 2011, 13], "temperature": 0.0, "avg_logprob": -0.18781148303638806, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.6684627300710417e-05}, {"id": 182, "seek": 53226, "start": 550.1, "end": 551.74, "text": " It didn't run anything.", "tokens": [467, 994, 380, 1190, 1340, 13], "temperature": 0.0, "avg_logprob": -0.18781148303638806, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.6684627300710417e-05}, {"id": 183, "seek": 53226, "start": 551.74, "end": 556.5, "text": " But now if I click on here, oh, hi.", "tokens": [583, 586, 498, 286, 2052, 322, 510, 11, 1954, 11, 4879, 13], "temperature": 0.0, "avg_logprob": -0.18781148303638806, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.6684627300710417e-05}, {"id": 184, "seek": 55650, "start": 556.5, "end": 563.9, "text": " So what happened is I told w that when a click occurs,", "tokens": [407, 437, 2011, 307, 286, 1907, 261, 300, 562, 257, 2052, 11843, 11], "temperature": 0.0, "avg_logprob": -0.09191176576434441, "compression_ratio": 1.6061946902654867, "no_speech_prob": 3.5559285151975928e-06}, {"id": 185, "seek": 55650, "start": 563.9, "end": 570.18, "text": " you should call back to my f function and run it.", "tokens": [291, 820, 818, 646, 281, 452, 283, 2445, 293, 1190, 309, 13], "temperature": 0.0, "avg_logprob": -0.09191176576434441, "compression_ratio": 1.6061946902654867, "no_speech_prob": 3.5559285151975928e-06}, {"id": 186, "seek": 55650, "start": 570.18, "end": 571.82, "text": " So anybody who's done GUI programming", "tokens": [407, 4472, 567, 311, 1096, 17917, 40, 9410], "temperature": 0.0, "avg_logprob": -0.09191176576434441, "compression_ratio": 1.6061946902654867, "no_speech_prob": 3.5559285151975928e-06}, {"id": 187, "seek": 55650, "start": 571.82, "end": 574.14, "text": " will be extremely comfortable with this idea.", "tokens": [486, 312, 4664, 4619, 365, 341, 1558, 13], "temperature": 0.0, "avg_logprob": -0.09191176576434441, "compression_ratio": 1.6061946902654867, "no_speech_prob": 3.5559285151975928e-06}, {"id": 188, "seek": 55650, "start": 574.14, "end": 577.66, "text": " And if you haven't, this will be kind of mind-bending.", "tokens": [400, 498, 291, 2378, 380, 11, 341, 486, 312, 733, 295, 1575, 12, 65, 2029, 13], "temperature": 0.0, "avg_logprob": -0.09191176576434441, "compression_ratio": 1.6061946902654867, "no_speech_prob": 3.5559285151975928e-06}, {"id": 189, "seek": 55650, "start": 577.66, "end": 580.42, "text": " So f is a callback.", "tokens": [407, 283, 307, 257, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.09191176576434441, "compression_ratio": 1.6061946902654867, "no_speech_prob": 3.5559285151975928e-06}, {"id": 190, "seek": 55650, "start": 580.42, "end": 582.34, "text": " It's not a particular class.", "tokens": [467, 311, 406, 257, 1729, 1508, 13], "temperature": 0.0, "avg_logprob": -0.09191176576434441, "compression_ratio": 1.6061946902654867, "no_speech_prob": 3.5559285151975928e-06}, {"id": 191, "seek": 55650, "start": 582.34, "end": 583.94, "text": " It doesn't have a particular signature.", "tokens": [467, 1177, 380, 362, 257, 1729, 13397, 13], "temperature": 0.0, "avg_logprob": -0.09191176576434441, "compression_ratio": 1.6061946902654867, "no_speech_prob": 3.5559285151975928e-06}, {"id": 192, "seek": 55650, "start": 583.94, "end": 585.62, "text": " It's not a particular library.", "tokens": [467, 311, 406, 257, 1729, 6405, 13], "temperature": 0.0, "avg_logprob": -0.09191176576434441, "compression_ratio": 1.6061946902654867, "no_speech_prob": 3.5559285151975928e-06}, {"id": 193, "seek": 58562, "start": 585.62, "end": 587.5, "text": " It's a concept.", "tokens": [467, 311, 257, 3410, 13], "temperature": 0.0, "avg_logprob": -0.0993593404958914, "compression_ratio": 1.6936936936936937, "no_speech_prob": 8.93930973688839e-06}, {"id": 194, "seek": 58562, "start": 587.5, "end": 591.0600000000001, "text": " It's a function that we treat as an object.", "tokens": [467, 311, 257, 2445, 300, 321, 2387, 382, 364, 2657, 13], "temperature": 0.0, "avg_logprob": -0.0993593404958914, "compression_ratio": 1.6936936936936937, "no_speech_prob": 8.93930973688839e-06}, {"id": 195, "seek": 58562, "start": 591.0600000000001, "end": 593.46, "text": " So look, we're not calling the function.", "tokens": [407, 574, 11, 321, 434, 406, 5141, 264, 2445, 13], "temperature": 0.0, "avg_logprob": -0.0993593404958914, "compression_ratio": 1.6936936936936937, "no_speech_prob": 8.93930973688839e-06}, {"id": 196, "seek": 58562, "start": 593.46, "end": 595.14, "text": " We don't have any parentheses after f.", "tokens": [492, 500, 380, 362, 604, 34153, 934, 283, 13], "temperature": 0.0, "avg_logprob": -0.0993593404958914, "compression_ratio": 1.6936936936936937, "no_speech_prob": 8.93930973688839e-06}, {"id": 197, "seek": 58562, "start": 595.14, "end": 599.74, "text": " We're passing the function itself to this method.", "tokens": [492, 434, 8437, 264, 2445, 2564, 281, 341, 3170, 13], "temperature": 0.0, "avg_logprob": -0.0993593404958914, "compression_ratio": 1.6936936936936937, "no_speech_prob": 8.93930973688839e-06}, {"id": 198, "seek": 58562, "start": 599.74, "end": 604.3, "text": " And it says, please call back to me when something happens.", "tokens": [400, 309, 1619, 11, 1767, 818, 646, 281, 385, 562, 746, 2314, 13], "temperature": 0.0, "avg_logprob": -0.0993593404958914, "compression_ratio": 1.6936936936936937, "no_speech_prob": 8.93930973688839e-06}, {"id": 199, "seek": 58562, "start": 604.3, "end": 607.42, "text": " And in this case, it's when I click.", "tokens": [400, 294, 341, 1389, 11, 309, 311, 562, 286, 2052, 13], "temperature": 0.0, "avg_logprob": -0.0993593404958914, "compression_ratio": 1.6936936936936937, "no_speech_prob": 8.93930973688839e-06}, {"id": 200, "seek": 58562, "start": 607.42, "end": 607.9, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.0993593404958914, "compression_ratio": 1.6936936936936937, "no_speech_prob": 8.93930973688839e-06}, {"id": 201, "seek": 58562, "start": 607.9, "end": 609.26, "text": " So there's our starting point.", "tokens": [407, 456, 311, 527, 2891, 935, 13], "temperature": 0.0, "avg_logprob": -0.0993593404958914, "compression_ratio": 1.6936936936936937, "no_speech_prob": 8.93930973688839e-06}, {"id": 202, "seek": 58562, "start": 609.26, "end": 612.98, "text": " And these kinds of functions, these kinds of callbacks", "tokens": [400, 613, 3685, 295, 6828, 11, 613, 3685, 295, 818, 17758], "temperature": 0.0, "avg_logprob": -0.0993593404958914, "compression_ratio": 1.6936936936936937, "no_speech_prob": 8.93930973688839e-06}, {"id": 203, "seek": 61298, "start": 612.98, "end": 617.26, "text": " that are used in a GUI in particular framework", "tokens": [300, 366, 1143, 294, 257, 17917, 40, 294, 1729, 8388], "temperature": 0.0, "avg_logprob": -0.14505308182513127, "compression_ratio": 1.6804511278195489, "no_speech_prob": 2.0460420273593627e-05}, {"id": 204, "seek": 61298, "start": 617.26, "end": 620.4200000000001, "text": " when some event happens are often called events.", "tokens": [562, 512, 2280, 2314, 366, 2049, 1219, 3931, 13], "temperature": 0.0, "avg_logprob": -0.14505308182513127, "compression_ratio": 1.6804511278195489, "no_speech_prob": 2.0460420273593627e-05}, {"id": 205, "seek": 61298, "start": 620.4200000000001, "end": 623.0600000000001, "text": " So if you've heard of events, they're a kind of callback.", "tokens": [407, 498, 291, 600, 2198, 295, 3931, 11, 436, 434, 257, 733, 295, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.14505308182513127, "compression_ratio": 1.6804511278195489, "no_speech_prob": 2.0460420273593627e-05}, {"id": 206, "seek": 61298, "start": 623.0600000000001, "end": 626.14, "text": " And then callbacks are a kind of what we would call a function", "tokens": [400, 550, 818, 17758, 366, 257, 733, 295, 437, 321, 576, 818, 257, 2445], "temperature": 0.0, "avg_logprob": -0.14505308182513127, "compression_ratio": 1.6804511278195489, "no_speech_prob": 2.0460420273593627e-05}, {"id": 207, "seek": 61298, "start": 626.14, "end": 627.54, "text": " pointer.", "tokens": [23918, 13], "temperature": 0.0, "avg_logprob": -0.14505308182513127, "compression_ratio": 1.6804511278195489, "no_speech_prob": 2.0460420273593627e-05}, {"id": 208, "seek": 61298, "start": 627.54, "end": 629.82, "text": " I mean, they can be much more general than that, as you'll see.", "tokens": [286, 914, 11, 436, 393, 312, 709, 544, 2674, 813, 300, 11, 382, 291, 603, 536, 13], "temperature": 0.0, "avg_logprob": -0.14505308182513127, "compression_ratio": 1.6804511278195489, "no_speech_prob": 2.0460420273593627e-05}, {"id": 209, "seek": 61298, "start": 629.82, "end": 631.82, "text": " But it's basically a way of passing in something", "tokens": [583, 309, 311, 1936, 257, 636, 295, 8437, 294, 746], "temperature": 0.0, "avg_logprob": -0.14505308182513127, "compression_ratio": 1.6804511278195489, "no_speech_prob": 2.0460420273593627e-05}, {"id": 210, "seek": 61298, "start": 631.82, "end": 635.7, "text": " to say, call back to this when something happens.", "tokens": [281, 584, 11, 818, 646, 281, 341, 562, 746, 2314, 13], "temperature": 0.0, "avg_logprob": -0.14505308182513127, "compression_ratio": 1.6804511278195489, "no_speech_prob": 2.0460420273593627e-05}, {"id": 211, "seek": 61298, "start": 635.7, "end": 638.82, "text": " Now, by the way, these widgets are really worth looking at", "tokens": [823, 11, 538, 264, 636, 11, 613, 43355, 366, 534, 3163, 1237, 412], "temperature": 0.0, "avg_logprob": -0.14505308182513127, "compression_ratio": 1.6804511278195489, "no_speech_prob": 2.0460420273593627e-05}, {"id": 212, "seek": 63882, "start": 638.82, "end": 644.0600000000001, "text": " if you're interested in building some analytical GUIs.", "tokens": [498, 291, 434, 3102, 294, 2390, 512, 29579, 17917, 6802, 13], "temperature": 0.0, "avg_logprob": -0.10165739561382094, "compression_ratio": 1.5791666666666666, "no_speech_prob": 1.0781754099298269e-05}, {"id": 213, "seek": 63882, "start": 644.0600000000001, "end": 647.6600000000001, "text": " Here's a great example from the Plotly documentation", "tokens": [1692, 311, 257, 869, 1365, 490, 264, 2149, 310, 356, 14333], "temperature": 0.0, "avg_logprob": -0.10165739561382094, "compression_ratio": 1.5791666666666666, "no_speech_prob": 1.0781754099298269e-05}, {"id": 214, "seek": 63882, "start": 647.6600000000001, "end": 651.98, "text": " of the kinds of things you can create with widgets.", "tokens": [295, 264, 3685, 295, 721, 291, 393, 1884, 365, 43355, 13], "temperature": 0.0, "avg_logprob": -0.10165739561382094, "compression_ratio": 1.5791666666666666, "no_speech_prob": 1.0781754099298269e-05}, {"id": 215, "seek": 63882, "start": 651.98, "end": 654.86, "text": " And it's not just for creating applications for others to use.", "tokens": [400, 309, 311, 406, 445, 337, 4084, 5821, 337, 2357, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.10165739561382094, "compression_ratio": 1.5791666666666666, "no_speech_prob": 1.0781754099298269e-05}, {"id": 216, "seek": 63882, "start": 654.86, "end": 659.74, "text": " But if you want to experiment with different types of function", "tokens": [583, 498, 291, 528, 281, 5120, 365, 819, 3467, 295, 2445], "temperature": 0.0, "avg_logprob": -0.10165739561382094, "compression_ratio": 1.5791666666666666, "no_speech_prob": 1.0781754099298269e-05}, {"id": 217, "seek": 63882, "start": 659.74, "end": 664.3800000000001, "text": " or hyperparameters or explore some data you've collected,", "tokens": [420, 9848, 2181, 335, 6202, 420, 6839, 512, 1412, 291, 600, 11087, 11], "temperature": 0.0, "avg_logprob": -0.10165739561382094, "compression_ratio": 1.5791666666666666, "no_speech_prob": 1.0781754099298269e-05}, {"id": 218, "seek": 63882, "start": 664.3800000000001, "end": 666.58, "text": " widgets are a great way to do that.", "tokens": [43355, 366, 257, 869, 636, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.10165739561382094, "compression_ratio": 1.5791666666666666, "no_speech_prob": 1.0781754099298269e-05}, {"id": 219, "seek": 66658, "start": 666.58, "end": 671.58, "text": " And as you can see, they're very, very easy to use.", "tokens": [400, 382, 291, 393, 536, 11, 436, 434, 588, 11, 588, 1858, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.07849331576414782, "compression_ratio": 1.5933014354066986, "no_speech_prob": 6.643019332841504e-06}, {"id": 220, "seek": 66658, "start": 671.58, "end": 676.4200000000001, "text": " In part one, you saw the image labeling stuff", "tokens": [682, 644, 472, 11, 291, 1866, 264, 3256, 40244, 1507], "temperature": 0.0, "avg_logprob": -0.07849331576414782, "compression_ratio": 1.5933014354066986, "no_speech_prob": 6.643019332841504e-06}, {"id": 221, "seek": 66658, "start": 676.4200000000001, "end": 680.94, "text": " that was built with widgets like this.", "tokens": [300, 390, 3094, 365, 43355, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.07849331576414782, "compression_ratio": 1.5933014354066986, "no_speech_prob": 6.643019332841504e-06}, {"id": 222, "seek": 66658, "start": 680.94, "end": 683.98, "text": " So that's how you can use somebody else's callback.", "tokens": [407, 300, 311, 577, 291, 393, 764, 2618, 1646, 311, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.07849331576414782, "compression_ratio": 1.5933014354066986, "no_speech_prob": 6.643019332841504e-06}, {"id": 223, "seek": 66658, "start": 683.98, "end": 686.58, "text": " How do we create our own callback?", "tokens": [1012, 360, 321, 1884, 527, 1065, 818, 3207, 30], "temperature": 0.0, "avg_logprob": -0.07849331576414782, "compression_ratio": 1.5933014354066986, "no_speech_prob": 6.643019332841504e-06}, {"id": 224, "seek": 66658, "start": 686.58, "end": 688.94, "text": " So let's create a callback.", "tokens": [407, 718, 311, 1884, 257, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.07849331576414782, "compression_ratio": 1.5933014354066986, "no_speech_prob": 6.643019332841504e-06}, {"id": 225, "seek": 66658, "start": 688.94, "end": 691.62, "text": " And the event that it's going to call back on", "tokens": [400, 264, 2280, 300, 309, 311, 516, 281, 818, 646, 322], "temperature": 0.0, "avg_logprob": -0.07849331576414782, "compression_ratio": 1.5933014354066986, "no_speech_prob": 6.643019332841504e-06}, {"id": 226, "seek": 66658, "start": 691.62, "end": 694.74, "text": " is after a calculation is complete.", "tokens": [307, 934, 257, 17108, 307, 3566, 13], "temperature": 0.0, "avg_logprob": -0.07849331576414782, "compression_ratio": 1.5933014354066986, "no_speech_prob": 6.643019332841504e-06}, {"id": 227, "seek": 69474, "start": 694.74, "end": 699.26, "text": " So let's create a function called slow calculation.", "tokens": [407, 718, 311, 1884, 257, 2445, 1219, 2964, 17108, 13], "temperature": 0.0, "avg_logprob": -0.1267153024673462, "compression_ratio": 2.0, "no_speech_prob": 1.5936158888507634e-05}, {"id": 228, "seek": 69474, "start": 699.26, "end": 701.1, "text": " And it's going to do five calculations.", "tokens": [400, 309, 311, 516, 281, 360, 1732, 20448, 13], "temperature": 0.0, "avg_logprob": -0.1267153024673462, "compression_ratio": 2.0, "no_speech_prob": 1.5936158888507634e-05}, {"id": 229, "seek": 69474, "start": 701.1, "end": 704.26, "text": " It's going to add i squared to a result.", "tokens": [467, 311, 516, 281, 909, 741, 8889, 281, 257, 1874, 13], "temperature": 0.0, "avg_logprob": -0.1267153024673462, "compression_ratio": 2.0, "no_speech_prob": 1.5936158888507634e-05}, {"id": 230, "seek": 69474, "start": 704.26, "end": 706.1800000000001, "text": " And then it's going to take a second to do it,", "tokens": [400, 550, 309, 311, 516, 281, 747, 257, 1150, 281, 360, 309, 11], "temperature": 0.0, "avg_logprob": -0.1267153024673462, "compression_ratio": 2.0, "no_speech_prob": 1.5936158888507634e-05}, {"id": 231, "seek": 69474, "start": 706.1800000000001, "end": 707.86, "text": " because we're going to add a sleep there.", "tokens": [570, 321, 434, 516, 281, 909, 257, 2817, 456, 13], "temperature": 0.0, "avg_logprob": -0.1267153024673462, "compression_ratio": 2.0, "no_speech_prob": 1.5936158888507634e-05}, {"id": 232, "seek": 69474, "start": 707.86, "end": 711.1, "text": " So this is kind of something like an epoch of deep learning.", "tokens": [407, 341, 307, 733, 295, 746, 411, 364, 30992, 339, 295, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.1267153024673462, "compression_ratio": 2.0, "no_speech_prob": 1.5936158888507634e-05}, {"id": 233, "seek": 69474, "start": 711.1, "end": 713.38, "text": " It's some calculation that takes a while.", "tokens": [467, 311, 512, 17108, 300, 2516, 257, 1339, 13], "temperature": 0.0, "avg_logprob": -0.1267153024673462, "compression_ratio": 2.0, "no_speech_prob": 1.5936158888507634e-05}, {"id": 234, "seek": 69474, "start": 713.38, "end": 717.1, "text": " So if we call slow calculation, then it's", "tokens": [407, 498, 321, 818, 2964, 17108, 11, 550, 309, 311], "temperature": 0.0, "avg_logprob": -0.1267153024673462, "compression_ratio": 2.0, "no_speech_prob": 1.5936158888507634e-05}, {"id": 235, "seek": 69474, "start": 717.1, "end": 721.0600000000001, "text": " going to take five seconds to calculate the sum of i squared.", "tokens": [516, 281, 747, 1732, 3949, 281, 8873, 264, 2408, 295, 741, 8889, 13], "temperature": 0.0, "avg_logprob": -0.1267153024673462, "compression_ratio": 2.0, "no_speech_prob": 1.5936158888507634e-05}, {"id": 236, "seek": 69474, "start": 721.0600000000001, "end": 722.14, "text": " And there it's done it.", "tokens": [400, 456, 309, 311, 1096, 309, 13], "temperature": 0.0, "avg_logprob": -0.1267153024673462, "compression_ratio": 2.0, "no_speech_prob": 1.5936158888507634e-05}, {"id": 237, "seek": 72214, "start": 722.14, "end": 726.66, "text": " So I'd really like to know how's it going, get some progress.", "tokens": [407, 286, 1116, 534, 411, 281, 458, 577, 311, 309, 516, 11, 483, 512, 4205, 13], "temperature": 0.0, "avg_logprob": -0.13947270921439178, "compression_ratio": 1.8016877637130801, "no_speech_prob": 1.4063427443034016e-05}, {"id": 238, "seek": 72214, "start": 726.66, "end": 729.34, "text": " So we could take that, and we could add something", "tokens": [407, 321, 727, 747, 300, 11, 293, 321, 727, 909, 746], "temperature": 0.0, "avg_logprob": -0.13947270921439178, "compression_ratio": 1.8016877637130801, "no_speech_prob": 1.4063427443034016e-05}, {"id": 239, "seek": 72214, "start": 729.34, "end": 733.42, "text": " that you pass in a callback.", "tokens": [300, 291, 1320, 294, 257, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.13947270921439178, "compression_ratio": 1.8016877637130801, "no_speech_prob": 1.4063427443034016e-05}, {"id": 240, "seek": 72214, "start": 733.42, "end": 736.6999999999999, "text": " And we just add one line of code that says, if there's a callback,", "tokens": [400, 321, 445, 909, 472, 1622, 295, 3089, 300, 1619, 11, 498, 456, 311, 257, 818, 3207, 11], "temperature": 0.0, "avg_logprob": -0.13947270921439178, "compression_ratio": 1.8016877637130801, "no_speech_prob": 1.4063427443034016e-05}, {"id": 241, "seek": 72214, "start": 736.6999999999999, "end": 741.78, "text": " then call it and pass in the epoch number.", "tokens": [550, 818, 309, 293, 1320, 294, 264, 30992, 339, 1230, 13], "temperature": 0.0, "avg_logprob": -0.13947270921439178, "compression_ratio": 1.8016877637130801, "no_speech_prob": 1.4063427443034016e-05}, {"id": 242, "seek": 72214, "start": 741.78, "end": 744.54, "text": " So then we could create a function called show progress", "tokens": [407, 550, 321, 727, 1884, 257, 2445, 1219, 855, 4205], "temperature": 0.0, "avg_logprob": -0.13947270921439178, "compression_ratio": 1.8016877637130801, "no_speech_prob": 1.4063427443034016e-05}, {"id": 243, "seek": 72214, "start": 744.54, "end": 748.14, "text": " that prints out, awesome, we finished epoch number epoch.", "tokens": [300, 22305, 484, 11, 3476, 11, 321, 4335, 30992, 339, 1230, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.13947270921439178, "compression_ratio": 1.8016877637130801, "no_speech_prob": 1.4063427443034016e-05}, {"id": 244, "seek": 72214, "start": 748.14, "end": 751.74, "text": " And look, it takes a parameter, and we're passing a parameter.", "tokens": [400, 574, 11, 309, 2516, 257, 13075, 11, 293, 321, 434, 8437, 257, 13075, 13], "temperature": 0.0, "avg_logprob": -0.13947270921439178, "compression_ratio": 1.8016877637130801, "no_speech_prob": 1.4063427443034016e-05}, {"id": 245, "seek": 75174, "start": 751.74, "end": 757.1800000000001, "text": " So therefore, we could now call slow calculation and pass", "tokens": [407, 4412, 11, 321, 727, 586, 818, 2964, 17108, 293, 1320], "temperature": 0.0, "avg_logprob": -0.1510584224354137, "compression_ratio": 1.5708333333333333, "no_speech_prob": 2.0579977899615187e-06}, {"id": 246, "seek": 75174, "start": 757.1800000000001, "end": 761.9, "text": " in show progress, and it will call back to our function", "tokens": [294, 855, 4205, 11, 293, 309, 486, 818, 646, 281, 527, 2445], "temperature": 0.0, "avg_logprob": -0.1510584224354137, "compression_ratio": 1.5708333333333333, "no_speech_prob": 2.0579977899615187e-06}, {"id": 247, "seek": 75174, "start": 761.9, "end": 763.58, "text": " after each epoch.", "tokens": [934, 1184, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.1510584224354137, "compression_ratio": 1.5708333333333333, "no_speech_prob": 2.0579977899615187e-06}, {"id": 248, "seek": 75174, "start": 763.58, "end": 766.58, "text": " So this is our starting point for our callback.", "tokens": [407, 341, 307, 527, 2891, 935, 337, 527, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.1510584224354137, "compression_ratio": 1.5708333333333333, "no_speech_prob": 2.0579977899615187e-06}, {"id": 249, "seek": 75174, "start": 766.58, "end": 770.1, "text": " Now, what will tend to happen, you'll notice,", "tokens": [823, 11, 437, 486, 3928, 281, 1051, 11, 291, 603, 3449, 11], "temperature": 0.0, "avg_logprob": -0.1510584224354137, "compression_ratio": 1.5708333333333333, "no_speech_prob": 2.0579977899615187e-06}, {"id": 250, "seek": 75174, "start": 770.1, "end": 772.38, "text": " with stuff that we do in fast AI.", "tokens": [365, 1507, 300, 321, 360, 294, 2370, 7318, 13], "temperature": 0.0, "avg_logprob": -0.1510584224354137, "compression_ratio": 1.5708333333333333, "no_speech_prob": 2.0579977899615187e-06}, {"id": 251, "seek": 75174, "start": 772.38, "end": 775.54, "text": " We'll start somewhere like this.", "tokens": [492, 603, 722, 4079, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1510584224354137, "compression_ratio": 1.5708333333333333, "no_speech_prob": 2.0579977899615187e-06}, {"id": 252, "seek": 75174, "start": 775.54, "end": 777.5, "text": " For many of you, it's trivially easy.", "tokens": [1171, 867, 295, 291, 11, 309, 311, 1376, 85, 2270, 1858, 13], "temperature": 0.0, "avg_logprob": -0.1510584224354137, "compression_ratio": 1.5708333333333333, "no_speech_prob": 2.0579977899615187e-06}, {"id": 253, "seek": 75174, "start": 777.5, "end": 781.34, "text": " And at some point during the next hour or two,", "tokens": [400, 412, 512, 935, 1830, 264, 958, 1773, 420, 732, 11], "temperature": 0.0, "avg_logprob": -0.1510584224354137, "compression_ratio": 1.5708333333333333, "no_speech_prob": 2.0579977899615187e-06}, {"id": 254, "seek": 78134, "start": 781.34, "end": 784.02, "text": " you might reach a point where you're feeling totally lost.", "tokens": [291, 1062, 2524, 257, 935, 689, 291, 434, 2633, 3879, 2731, 13], "temperature": 0.0, "avg_logprob": -0.11318629666378624, "compression_ratio": 1.8849840255591055, "no_speech_prob": 1.7777470930013806e-05}, {"id": 255, "seek": 78134, "start": 784.02, "end": 786.58, "text": " And the trick is to go back, if you're watching the video,", "tokens": [400, 264, 4282, 307, 281, 352, 646, 11, 498, 291, 434, 1976, 264, 960, 11], "temperature": 0.0, "avg_logprob": -0.11318629666378624, "compression_ratio": 1.8849840255591055, "no_speech_prob": 1.7777470930013806e-05}, {"id": 256, "seek": 78134, "start": 786.58, "end": 789.5, "text": " to the point where it was trivially easy and figure", "tokens": [281, 264, 935, 689, 309, 390, 1376, 85, 2270, 1858, 293, 2573], "temperature": 0.0, "avg_logprob": -0.11318629666378624, "compression_ratio": 1.8849840255591055, "no_speech_prob": 1.7777470930013806e-05}, {"id": 257, "seek": 78134, "start": 789.5, "end": 791.86, "text": " out the bit where you suddenly noticed you were totally lost", "tokens": [484, 264, 857, 689, 291, 5800, 5694, 291, 645, 3879, 2731], "temperature": 0.0, "avg_logprob": -0.11318629666378624, "compression_ratio": 1.8849840255591055, "no_speech_prob": 1.7777470930013806e-05}, {"id": 258, "seek": 78134, "start": 791.86, "end": 794.58, "text": " and find the bit in the middle where you kind of missed a bit.", "tokens": [293, 915, 264, 857, 294, 264, 2808, 689, 291, 733, 295, 6721, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.11318629666378624, "compression_ratio": 1.8849840255591055, "no_speech_prob": 1.7777470930013806e-05}, {"id": 259, "seek": 78134, "start": 794.58, "end": 795.9, "text": " Because we're going to just keep building", "tokens": [1436, 321, 434, 516, 281, 445, 1066, 2390], "temperature": 0.0, "avg_logprob": -0.11318629666378624, "compression_ratio": 1.8849840255591055, "no_speech_prob": 1.7777470930013806e-05}, {"id": 260, "seek": 78134, "start": 795.9, "end": 798.14, "text": " up from trivially easy stuff, just like we did", "tokens": [493, 490, 1376, 85, 2270, 1858, 1507, 11, 445, 411, 321, 630], "temperature": 0.0, "avg_logprob": -0.11318629666378624, "compression_ratio": 1.8849840255591055, "no_speech_prob": 1.7777470930013806e-05}, {"id": 261, "seek": 78134, "start": 798.14, "end": 799.7800000000001, "text": " with that matrix multiplication.", "tokens": [365, 300, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.11318629666378624, "compression_ratio": 1.8849840255591055, "no_speech_prob": 1.7777470930013806e-05}, {"id": 262, "seek": 78134, "start": 799.7800000000001, "end": 802.26, "text": " Right? So we're going to gradually build up from here and look", "tokens": [1779, 30, 407, 321, 434, 516, 281, 13145, 1322, 493, 490, 510, 293, 574], "temperature": 0.0, "avg_logprob": -0.11318629666378624, "compression_ratio": 1.8849840255591055, "no_speech_prob": 1.7777470930013806e-05}, {"id": 263, "seek": 78134, "start": 802.26, "end": 804.26, "text": " at more and more interesting callbacks.", "tokens": [412, 544, 293, 544, 1880, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.11318629666378624, "compression_ratio": 1.8849840255591055, "no_speech_prob": 1.7777470930013806e-05}, {"id": 264, "seek": 78134, "start": 804.26, "end": 808.6600000000001, "text": " But we're starting with this wonderfully short", "tokens": [583, 321, 434, 2891, 365, 341, 38917, 2099], "temperature": 0.0, "avg_logprob": -0.11318629666378624, "compression_ratio": 1.8849840255591055, "no_speech_prob": 1.7777470930013806e-05}, {"id": 265, "seek": 78134, "start": 808.6600000000001, "end": 810.86, "text": " and simple line of code.", "tokens": [293, 2199, 1622, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.11318629666378624, "compression_ratio": 1.8849840255591055, "no_speech_prob": 1.7777470930013806e-05}, {"id": 266, "seek": 81086, "start": 810.86, "end": 816.7, "text": " So rather than defining a function just for the purpose", "tokens": [407, 2831, 813, 17827, 257, 2445, 445, 337, 264, 4334], "temperature": 0.0, "avg_logprob": -0.08609836751764471, "compression_ratio": 1.8903508771929824, "no_speech_prob": 1.4062755326449405e-05}, {"id": 267, "seek": 81086, "start": 816.7, "end": 819.98, "text": " of using it once, we can actually define the function", "tokens": [295, 1228, 309, 1564, 11, 321, 393, 767, 6964, 264, 2445], "temperature": 0.0, "avg_logprob": -0.08609836751764471, "compression_ratio": 1.8903508771929824, "no_speech_prob": 1.4062755326449405e-05}, {"id": 268, "seek": 81086, "start": 819.98, "end": 822.9, "text": " at the point we use it using lambda notation.", "tokens": [412, 264, 935, 321, 764, 309, 1228, 13607, 24657, 13], "temperature": 0.0, "avg_logprob": -0.08609836751764471, "compression_ratio": 1.8903508771929824, "no_speech_prob": 1.4062755326449405e-05}, {"id": 269, "seek": 81086, "start": 822.9, "end": 825.86, "text": " So lambda notation is just another way of creating a function.", "tokens": [407, 13607, 24657, 307, 445, 1071, 636, 295, 4084, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.08609836751764471, "compression_ratio": 1.8903508771929824, "no_speech_prob": 1.4062755326449405e-05}, {"id": 270, "seek": 81086, "start": 825.86, "end": 829.0600000000001, "text": " So rather than saying def, we say lambda.", "tokens": [407, 2831, 813, 1566, 1060, 11, 321, 584, 13607, 13], "temperature": 0.0, "avg_logprob": -0.08609836751764471, "compression_ratio": 1.8903508771929824, "no_speech_prob": 1.4062755326449405e-05}, {"id": 271, "seek": 81086, "start": 829.0600000000001, "end": 831.54, "text": " And then rather than putting in parentheses the arguments,", "tokens": [400, 550, 2831, 813, 3372, 294, 34153, 264, 12869, 11], "temperature": 0.0, "avg_logprob": -0.08609836751764471, "compression_ratio": 1.8903508771929824, "no_speech_prob": 1.4062755326449405e-05}, {"id": 272, "seek": 81086, "start": 831.54, "end": 833.3000000000001, "text": " we put them before a colon.", "tokens": [321, 829, 552, 949, 257, 8255, 13], "temperature": 0.0, "avg_logprob": -0.08609836751764471, "compression_ratio": 1.8903508771929824, "no_speech_prob": 1.4062755326449405e-05}, {"id": 273, "seek": 81086, "start": 833.3000000000001, "end": 835.7, "text": " And then we list the thing we want to do.", "tokens": [400, 550, 321, 1329, 264, 551, 321, 528, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.08609836751764471, "compression_ratio": 1.8903508771929824, "no_speech_prob": 1.4062755326449405e-05}, {"id": 274, "seek": 81086, "start": 835.7, "end": 839.14, "text": " So this is identical to the previous one.", "tokens": [407, 341, 307, 14800, 281, 264, 3894, 472, 13], "temperature": 0.0, "avg_logprob": -0.08609836751764471, "compression_ratio": 1.8903508771929824, "no_speech_prob": 1.4062755326449405e-05}, {"id": 275, "seek": 83914, "start": 839.14, "end": 841.8199999999999, "text": " It's just a convenience for times where you want", "tokens": [467, 311, 445, 257, 19283, 337, 1413, 689, 291, 528], "temperature": 0.0, "avg_logprob": -0.12929895401000976, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.0415531000471674e-06}, {"id": 276, "seek": 83914, "start": 841.8199999999999, "end": 846.14, "text": " to define the callback at the same time that you use it.", "tokens": [281, 6964, 264, 818, 3207, 412, 264, 912, 565, 300, 291, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.12929895401000976, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.0415531000471674e-06}, {"id": 277, "seek": 83914, "start": 846.14, "end": 850.3, "text": " It can make your code a little bit more concise.", "tokens": [467, 393, 652, 428, 3089, 257, 707, 857, 544, 44882, 13], "temperature": 0.0, "avg_logprob": -0.12929895401000976, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.0415531000471674e-06}, {"id": 278, "seek": 83914, "start": 850.3, "end": 852.78, "text": " What if you wanted to have something", "tokens": [708, 498, 291, 1415, 281, 362, 746], "temperature": 0.0, "avg_logprob": -0.12929895401000976, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.0415531000471674e-06}, {"id": 279, "seek": 83914, "start": 852.78, "end": 856.42, "text": " where you could define what exclamation to use", "tokens": [689, 291, 727, 6964, 437, 1624, 43233, 281, 764], "temperature": 0.0, "avg_logprob": -0.12929895401000976, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.0415531000471674e-06}, {"id": 280, "seek": 83914, "start": 856.42, "end": 857.6999999999999, "text": " in the string as well?", "tokens": [294, 264, 6798, 382, 731, 30], "temperature": 0.0, "avg_logprob": -0.12929895401000976, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.0415531000471674e-06}, {"id": 281, "seek": 83914, "start": 857.6999999999999, "end": 859.98, "text": " So we've now got two things.", "tokens": [407, 321, 600, 586, 658, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.12929895401000976, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.0415531000471674e-06}, {"id": 282, "seek": 83914, "start": 859.98, "end": 865.86, "text": " We can't pass this show progress to slow calculation.", "tokens": [492, 393, 380, 1320, 341, 855, 4205, 281, 2964, 17108, 13], "temperature": 0.0, "avg_logprob": -0.12929895401000976, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.0415531000471674e-06}, {"id": 283, "seek": 86586, "start": 865.86, "end": 870.74, "text": " Let's try it.", "tokens": [961, 311, 853, 309, 13], "temperature": 0.0, "avg_logprob": -0.14296934708305029, "compression_ratio": 1.7897196261682242, "no_speech_prob": 7.29626390238991e-06}, {"id": 284, "seek": 86586, "start": 870.74, "end": 873.1, "text": " Right? It tries to call back.", "tokens": [1779, 30, 467, 9898, 281, 818, 646, 13], "temperature": 0.0, "avg_logprob": -0.14296934708305029, "compression_ratio": 1.7897196261682242, "no_speech_prob": 7.29626390238991e-06}, {"id": 285, "seek": 86586, "start": 873.1, "end": 875.7, "text": " And it calls, remember CB is now show progress.", "tokens": [400, 309, 5498, 11, 1604, 18745, 307, 586, 855, 4205, 13], "temperature": 0.0, "avg_logprob": -0.14296934708305029, "compression_ratio": 1.7897196261682242, "no_speech_prob": 7.29626390238991e-06}, {"id": 286, "seek": 86586, "start": 875.7, "end": 877.22, "text": " So it's passing show progress.", "tokens": [407, 309, 311, 8437, 855, 4205, 13], "temperature": 0.0, "avg_logprob": -0.14296934708305029, "compression_ratio": 1.7897196261682242, "no_speech_prob": 7.29626390238991e-06}, {"id": 287, "seek": 86586, "start": 877.22, "end": 880.02, "text": " And it's passing epoch as exclamation.", "tokens": [400, 309, 311, 8437, 30992, 339, 382, 1624, 43233, 13], "temperature": 0.0, "avg_logprob": -0.14296934708305029, "compression_ratio": 1.7897196261682242, "no_speech_prob": 7.29626390238991e-06}, {"id": 288, "seek": 86586, "start": 880.02, "end": 881.42, "text": " And then epoch is missing.", "tokens": [400, 550, 30992, 339, 307, 5361, 13], "temperature": 0.0, "avg_logprob": -0.14296934708305029, "compression_ratio": 1.7897196261682242, "no_speech_prob": 7.29626390238991e-06}, {"id": 289, "seek": 86586, "start": 881.42, "end": 882.7, "text": " So that's an error.", "tokens": [407, 300, 311, 364, 6713, 13], "temperature": 0.0, "avg_logprob": -0.14296934708305029, "compression_ratio": 1.7897196261682242, "no_speech_prob": 7.29626390238991e-06}, {"id": 290, "seek": 86586, "start": 882.7, "end": 885.66, "text": " We've called a function with two arguments with only one.", "tokens": [492, 600, 1219, 257, 2445, 365, 732, 12869, 365, 787, 472, 13], "temperature": 0.0, "avg_logprob": -0.14296934708305029, "compression_ratio": 1.7897196261682242, "no_speech_prob": 7.29626390238991e-06}, {"id": 291, "seek": 86586, "start": 885.66, "end": 887.5, "text": " So we have to convert this", "tokens": [407, 321, 362, 281, 7620, 341], "temperature": 0.0, "avg_logprob": -0.14296934708305029, "compression_ratio": 1.7897196261682242, "no_speech_prob": 7.29626390238991e-06}, {"id": 292, "seek": 86586, "start": 887.5, "end": 889.86, "text": " into a function with only one argument.", "tokens": [666, 257, 2445, 365, 787, 472, 6770, 13], "temperature": 0.0, "avg_logprob": -0.14296934708305029, "compression_ratio": 1.7897196261682242, "no_speech_prob": 7.29626390238991e-06}, {"id": 293, "seek": 86586, "start": 889.86, "end": 893.86, "text": " So lambda O is a function with only one argument.", "tokens": [407, 13607, 422, 307, 257, 2445, 365, 787, 472, 6770, 13], "temperature": 0.0, "avg_logprob": -0.14296934708305029, "compression_ratio": 1.7897196261682242, "no_speech_prob": 7.29626390238991e-06}, {"id": 294, "seek": 89386, "start": 893.86, "end": 896.42, "text": " And this function calls show progress", "tokens": [400, 341, 2445, 5498, 855, 4205], "temperature": 0.0, "avg_logprob": -0.0981900254074408, "compression_ratio": 1.8709677419354838, "no_speech_prob": 4.289270236768061e-06}, {"id": 295, "seek": 89386, "start": 896.42, "end": 899.1, "text": " with a particular exclamation.", "tokens": [365, 257, 1729, 1624, 43233, 13], "temperature": 0.0, "avg_logprob": -0.0981900254074408, "compression_ratio": 1.8709677419354838, "no_speech_prob": 4.289270236768061e-06}, {"id": 296, "seek": 89386, "start": 899.1, "end": 901.46, "text": " Okay? So we've converted something with two arguments", "tokens": [1033, 30, 407, 321, 600, 16424, 746, 365, 732, 12869], "temperature": 0.0, "avg_logprob": -0.0981900254074408, "compression_ratio": 1.8709677419354838, "no_speech_prob": 4.289270236768061e-06}, {"id": 297, "seek": 89386, "start": 901.46, "end": 903.5, "text": " into something with one argument.", "tokens": [666, 746, 365, 472, 6770, 13], "temperature": 0.0, "avg_logprob": -0.0981900254074408, "compression_ratio": 1.8709677419354838, "no_speech_prob": 4.289270236768061e-06}, {"id": 298, "seek": 89386, "start": 903.5, "end": 906.86, "text": " We might want to make it really easy to allow people", "tokens": [492, 1062, 528, 281, 652, 309, 534, 1858, 281, 2089, 561], "temperature": 0.0, "avg_logprob": -0.0981900254074408, "compression_ratio": 1.8709677419354838, "no_speech_prob": 4.289270236768061e-06}, {"id": 299, "seek": 89386, "start": 906.86, "end": 909.02, "text": " to create different progress indicators", "tokens": [281, 1884, 819, 4205, 22176], "temperature": 0.0, "avg_logprob": -0.0981900254074408, "compression_ratio": 1.8709677419354838, "no_speech_prob": 4.289270236768061e-06}, {"id": 300, "seek": 89386, "start": 909.02, "end": 910.74, "text": " with different exclamations.", "tokens": [365, 819, 1624, 4326, 763, 13], "temperature": 0.0, "avg_logprob": -0.0981900254074408, "compression_ratio": 1.8709677419354838, "no_speech_prob": 4.289270236768061e-06}, {"id": 301, "seek": 89386, "start": 910.74, "end": 913.38, "text": " So we could create a function called make show progress", "tokens": [407, 321, 727, 1884, 257, 2445, 1219, 652, 855, 4205], "temperature": 0.0, "avg_logprob": -0.0981900254074408, "compression_ratio": 1.8709677419354838, "no_speech_prob": 4.289270236768061e-06}, {"id": 302, "seek": 89386, "start": 913.38, "end": 916.58, "text": " that returns that lambda.", "tokens": [300, 11247, 300, 13607, 13], "temperature": 0.0, "avg_logprob": -0.0981900254074408, "compression_ratio": 1.8709677419354838, "no_speech_prob": 4.289270236768061e-06}, {"id": 303, "seek": 89386, "start": 916.58, "end": 920.62, "text": " Okay? So now we could say make show progress.", "tokens": [1033, 30, 407, 586, 321, 727, 584, 652, 855, 4205, 13], "temperature": 0.0, "avg_logprob": -0.0981900254074408, "compression_ratio": 1.8709677419354838, "no_speech_prob": 4.289270236768061e-06}, {"id": 304, "seek": 92062, "start": 920.62, "end": 926.14, "text": " So we could do that here.", "tokens": [407, 321, 727, 360, 300, 510, 13], "temperature": 0.0, "avg_logprob": -0.18884849548339844, "compression_ratio": 1.4, "no_speech_prob": 4.356817953521386e-06}, {"id": 305, "seek": 92062, "start": 926.14, "end": 928.42, "text": " Make show progress.", "tokens": [4387, 855, 4205, 13], "temperature": 0.0, "avg_logprob": -0.18884849548339844, "compression_ratio": 1.4, "no_speech_prob": 4.356817953521386e-06}, {"id": 306, "seek": 92062, "start": 933.14, "end": 935.66, "text": " Okay? And that's the same thing.", "tokens": [1033, 30, 400, 300, 311, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.18884849548339844, "compression_ratio": 1.4, "no_speech_prob": 4.356817953521386e-06}, {"id": 307, "seek": 92062, "start": 935.66, "end": 937.46, "text": " All right?", "tokens": [1057, 558, 30], "temperature": 0.0, "avg_logprob": -0.18884849548339844, "compression_ratio": 1.4, "no_speech_prob": 4.356817953521386e-06}, {"id": 308, "seek": 92062, "start": 937.46, "end": 939.22, "text": " This is a little bit awkward.", "tokens": [639, 307, 257, 707, 857, 11411, 13], "temperature": 0.0, "avg_logprob": -0.18884849548339844, "compression_ratio": 1.4, "no_speech_prob": 4.356817953521386e-06}, {"id": 309, "seek": 92062, "start": 939.22, "end": 942.14, "text": " So generally you might see it done like this instead.", "tokens": [407, 5101, 291, 1062, 536, 309, 1096, 411, 341, 2602, 13], "temperature": 0.0, "avg_logprob": -0.18884849548339844, "compression_ratio": 1.4, "no_speech_prob": 4.356817953521386e-06}, {"id": 310, "seek": 92062, "start": 942.14, "end": 944.14, "text": " You see this in fast AI all the time.", "tokens": [509, 536, 341, 294, 2370, 7318, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.18884849548339844, "compression_ratio": 1.4, "no_speech_prob": 4.356817953521386e-06}, {"id": 311, "seek": 92062, "start": 944.14, "end": 947.9, "text": " We define the function inside it.", "tokens": [492, 6964, 264, 2445, 1854, 309, 13], "temperature": 0.0, "avg_logprob": -0.18884849548339844, "compression_ratio": 1.4, "no_speech_prob": 4.356817953521386e-06}, {"id": 312, "seek": 94790, "start": 947.9, "end": 950.9399999999999, "text": " Okay? But this is basically just the same as our lambda.", "tokens": [1033, 30, 583, 341, 307, 1936, 445, 264, 912, 382, 527, 13607, 13], "temperature": 0.0, "avg_logprob": -0.07732726610623873, "compression_ratio": 1.79182156133829, "no_speech_prob": 1.0451174603076652e-05}, {"id": 313, "seek": 94790, "start": 950.9399999999999, "end": 952.9, "text": " And then we return that function.", "tokens": [400, 550, 321, 2736, 300, 2445, 13], "temperature": 0.0, "avg_logprob": -0.07732726610623873, "compression_ratio": 1.79182156133829, "no_speech_prob": 1.0451174603076652e-05}, {"id": 314, "seek": 94790, "start": 952.9, "end": 956.06, "text": " So this is kind of interesting because you might think", "tokens": [407, 341, 307, 733, 295, 1880, 570, 291, 1062, 519], "temperature": 0.0, "avg_logprob": -0.07732726610623873, "compression_ratio": 1.79182156133829, "no_speech_prob": 1.0451174603076652e-05}, {"id": 315, "seek": 94790, "start": 956.06, "end": 959.5, "text": " of defining a function as being like a declarative thing", "tokens": [295, 17827, 257, 2445, 382, 885, 411, 257, 16694, 1166, 551], "temperature": 0.0, "avg_logprob": -0.07732726610623873, "compression_ratio": 1.79182156133829, "no_speech_prob": 1.0451174603076652e-05}, {"id": 316, "seek": 94790, "start": 959.5, "end": 961.78, "text": " that as soon as you define it that now it's part", "tokens": [300, 382, 2321, 382, 291, 6964, 309, 300, 586, 309, 311, 644], "temperature": 0.0, "avg_logprob": -0.07732726610623873, "compression_ratio": 1.79182156133829, "no_speech_prob": 1.0451174603076652e-05}, {"id": 317, "seek": 94790, "start": 961.78, "end": 963.18, "text": " of the thing that's like compiled.", "tokens": [295, 264, 551, 300, 311, 411, 36548, 13], "temperature": 0.0, "avg_logprob": -0.07732726610623873, "compression_ratio": 1.79182156133829, "no_speech_prob": 1.0451174603076652e-05}, {"id": 318, "seek": 94790, "start": 963.18, "end": 966.1, "text": " If you see your C++, that's how they work.", "tokens": [759, 291, 536, 428, 383, 25472, 11, 300, 311, 577, 436, 589, 13], "temperature": 0.0, "avg_logprob": -0.07732726610623873, "compression_ratio": 1.79182156133829, "no_speech_prob": 1.0451174603076652e-05}, {"id": 319, "seek": 94790, "start": 966.1, "end": 967.86, "text": " In Python that's not how they work.", "tokens": [682, 15329, 300, 311, 406, 577, 436, 589, 13], "temperature": 0.0, "avg_logprob": -0.07732726610623873, "compression_ratio": 1.79182156133829, "no_speech_prob": 1.0451174603076652e-05}, {"id": 320, "seek": 94790, "start": 967.86, "end": 971.74, "text": " When you define a function, you're actually saying the same,", "tokens": [1133, 291, 6964, 257, 2445, 11, 291, 434, 767, 1566, 264, 912, 11], "temperature": 0.0, "avg_logprob": -0.07732726610623873, "compression_ratio": 1.79182156133829, "no_speech_prob": 1.0451174603076652e-05}, {"id": 321, "seek": 94790, "start": 971.74, "end": 975.3, "text": " basically the same as this, which is there's a variable", "tokens": [1936, 264, 912, 382, 341, 11, 597, 307, 456, 311, 257, 7006], "temperature": 0.0, "avg_logprob": -0.07732726610623873, "compression_ratio": 1.79182156133829, "no_speech_prob": 1.0451174603076652e-05}, {"id": 322, "seek": 97530, "start": 975.3, "end": 978.9399999999999, "text": " with this name, which is a function.", "tokens": [365, 341, 1315, 11, 597, 307, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.112068103707355, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.646473022759892e-06}, {"id": 323, "seek": 97530, "start": 978.9399999999999, "end": 982.74, "text": " Right? And that's how come then we can actually take something", "tokens": [1779, 30, 400, 300, 311, 577, 808, 550, 321, 393, 767, 747, 746], "temperature": 0.0, "avg_logprob": -0.112068103707355, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.646473022759892e-06}, {"id": 324, "seek": 97530, "start": 982.74, "end": 987.14, "text": " that's passed to this function and use it inside here.", "tokens": [300, 311, 4678, 281, 341, 2445, 293, 764, 309, 1854, 510, 13], "temperature": 0.0, "avg_logprob": -0.112068103707355, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.646473022759892e-06}, {"id": 325, "seek": 97530, "start": 987.14, "end": 990.8199999999999, "text": " Right? So this is actually every time we call make show progress,", "tokens": [1779, 30, 407, 341, 307, 767, 633, 565, 321, 818, 652, 855, 4205, 11], "temperature": 0.0, "avg_logprob": -0.112068103707355, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.646473022759892e-06}, {"id": 326, "seek": 97530, "start": 990.8199999999999, "end": 994.66, "text": " it's going to create a new function, underscore inner internally", "tokens": [309, 311, 516, 281, 1884, 257, 777, 2445, 11, 37556, 7284, 19501], "temperature": 0.0, "avg_logprob": -0.112068103707355, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.646473022759892e-06}, {"id": 327, "seek": 97530, "start": 994.66, "end": 997.3, "text": " with a different exclamation.", "tokens": [365, 257, 819, 1624, 43233, 13], "temperature": 0.0, "avg_logprob": -0.112068103707355, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.646473022759892e-06}, {"id": 328, "seek": 97530, "start": 997.3, "end": 1003.6999999999999, "text": " And so it will work the same as before.", "tokens": [400, 370, 309, 486, 589, 264, 912, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.112068103707355, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.646473022759892e-06}, {"id": 329, "seek": 100370, "start": 1003.7, "end": 1006.3000000000001, "text": " Okay? So this thing where you create a function", "tokens": [1033, 30, 407, 341, 551, 689, 291, 1884, 257, 2445], "temperature": 0.0, "avg_logprob": -0.11853017409642537, "compression_ratio": 1.5346938775510204, "no_speech_prob": 6.144038707134314e-06}, {"id": 330, "seek": 100370, "start": 1006.3000000000001, "end": 1008.0600000000001, "text": " that actually stores some information", "tokens": [300, 767, 9512, 512, 1589], "temperature": 0.0, "avg_logprob": -0.11853017409642537, "compression_ratio": 1.5346938775510204, "no_speech_prob": 6.144038707134314e-06}, {"id": 331, "seek": 100370, "start": 1008.0600000000001, "end": 1010.58, "text": " from the external context and like it can be different every time,", "tokens": [490, 264, 8320, 4319, 293, 411, 309, 393, 312, 819, 633, 565, 11], "temperature": 0.0, "avg_logprob": -0.11853017409642537, "compression_ratio": 1.5346938775510204, "no_speech_prob": 6.144038707134314e-06}, {"id": 332, "seek": 100370, "start": 1010.58, "end": 1011.9000000000001, "text": " that's called a closure.", "tokens": [300, 311, 1219, 257, 24653, 13], "temperature": 0.0, "avg_logprob": -0.11853017409642537, "compression_ratio": 1.5346938775510204, "no_speech_prob": 6.144038707134314e-06}, {"id": 333, "seek": 100370, "start": 1011.9000000000001, "end": 1013.9000000000001, "text": " Okay? So it's a concept you'll come across a lot,", "tokens": [1033, 30, 407, 309, 311, 257, 3410, 291, 603, 808, 2108, 257, 688, 11], "temperature": 0.0, "avg_logprob": -0.11853017409642537, "compression_ratio": 1.5346938775510204, "no_speech_prob": 6.144038707134314e-06}, {"id": 334, "seek": 100370, "start": 1013.9000000000001, "end": 1017.1800000000001, "text": " particularly if you're a JavaScript programmer.", "tokens": [4098, 498, 291, 434, 257, 15778, 32116, 13], "temperature": 0.0, "avg_logprob": -0.11853017409642537, "compression_ratio": 1.5346938775510204, "no_speech_prob": 6.144038707134314e-06}, {"id": 335, "seek": 100370, "start": 1017.1800000000001, "end": 1029.22, "text": " So we could say f2 equals make show progress terrific.", "tokens": [407, 321, 727, 584, 283, 17, 6915, 652, 855, 4205, 20899, 13], "temperature": 0.0, "avg_logprob": -0.11853017409642537, "compression_ratio": 1.5346938775510204, "no_speech_prob": 6.144038707134314e-06}, {"id": 336, "seek": 100370, "start": 1029.22, "end": 1033.54, "text": " Right? And so that now contains that closure.", "tokens": [1779, 30, 400, 370, 300, 586, 8306, 300, 24653, 13], "temperature": 0.0, "avg_logprob": -0.11853017409642537, "compression_ratio": 1.5346938775510204, "no_speech_prob": 6.144038707134314e-06}, {"id": 337, "seek": 103354, "start": 1033.54, "end": 1038.42, "text": " So it actually remembers what exclamation you passed it.", "tokens": [407, 309, 767, 26228, 437, 1624, 43233, 291, 4678, 309, 13], "temperature": 0.0, "avg_logprob": -0.13217033248349844, "compression_ratio": 1.6839622641509433, "no_speech_prob": 6.04875413046102e-06}, {"id": 338, "seek": 103354, "start": 1038.42, "end": 1047.6599999999999, "text": " Okay? Because it's so often that you want to take a function", "tokens": [1033, 30, 1436, 309, 311, 370, 2049, 300, 291, 528, 281, 747, 257, 2445], "temperature": 0.0, "avg_logprob": -0.13217033248349844, "compression_ratio": 1.6839622641509433, "no_speech_prob": 6.04875413046102e-06}, {"id": 339, "seek": 103354, "start": 1047.6599999999999, "end": 1051.5, "text": " that takes two parameters and turn it into a function", "tokens": [300, 2516, 732, 9834, 293, 1261, 309, 666, 257, 2445], "temperature": 0.0, "avg_logprob": -0.13217033248349844, "compression_ratio": 1.6839622641509433, "no_speech_prob": 6.04875413046102e-06}, {"id": 340, "seek": 103354, "start": 1051.5, "end": 1055.06, "text": " that takes one parameter, Python and most languages have a way", "tokens": [300, 2516, 472, 13075, 11, 15329, 293, 881, 8650, 362, 257, 636], "temperature": 0.0, "avg_logprob": -0.13217033248349844, "compression_ratio": 1.6839622641509433, "no_speech_prob": 6.04875413046102e-06}, {"id": 341, "seek": 103354, "start": 1055.06, "end": 1058.82, "text": " to do that, which is called partial function application.", "tokens": [281, 360, 300, 11, 597, 307, 1219, 14641, 2445, 3861, 13], "temperature": 0.0, "avg_logprob": -0.13217033248349844, "compression_ratio": 1.6839622641509433, "no_speech_prob": 6.04875413046102e-06}, {"id": 342, "seek": 103354, "start": 1058.82, "end": 1062.42, "text": " So the standard library functions has this thing called partial.", "tokens": [407, 264, 3832, 6405, 6828, 575, 341, 551, 1219, 14641, 13], "temperature": 0.0, "avg_logprob": -0.13217033248349844, "compression_ratio": 1.6839622641509433, "no_speech_prob": 6.04875413046102e-06}, {"id": 343, "seek": 106242, "start": 1062.42, "end": 1066.38, "text": " So if you take call partial and you pass it a function", "tokens": [407, 498, 291, 747, 818, 14641, 293, 291, 1320, 309, 257, 2445], "temperature": 0.0, "avg_logprob": -0.1044932145338792, "compression_ratio": 1.7465437788018434, "no_speech_prob": 6.143965492810821e-06}, {"id": 344, "seek": 106242, "start": 1066.38, "end": 1069.54, "text": " and then you pass in some arguments for that function,", "tokens": [293, 550, 291, 1320, 294, 512, 12869, 337, 300, 2445, 11], "temperature": 0.0, "avg_logprob": -0.1044932145338792, "compression_ratio": 1.7465437788018434, "no_speech_prob": 6.143965492810821e-06}, {"id": 345, "seek": 106242, "start": 1069.54, "end": 1073.66, "text": " it returns a new function that now just takes,", "tokens": [309, 11247, 257, 777, 2445, 300, 586, 445, 2516, 11], "temperature": 0.0, "avg_logprob": -0.1044932145338792, "compression_ratio": 1.7465437788018434, "no_speech_prob": 6.143965492810821e-06}, {"id": 346, "seek": 106242, "start": 1073.66, "end": 1076.02, "text": " which that parameter is always a given.", "tokens": [597, 300, 13075, 307, 1009, 257, 2212, 13], "temperature": 0.0, "avg_logprob": -0.1044932145338792, "compression_ratio": 1.7465437788018434, "no_speech_prob": 6.143965492810821e-06}, {"id": 347, "seek": 106242, "start": 1076.02, "end": 1077.5800000000002, "text": " So let's check it out.", "tokens": [407, 718, 311, 1520, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.1044932145338792, "compression_ratio": 1.7465437788018434, "no_speech_prob": 6.143965492810821e-06}, {"id": 348, "seek": 106242, "start": 1077.5800000000002, "end": 1084.3000000000002, "text": " So we could run it like that or we could say f2 equals this partial", "tokens": [407, 321, 727, 1190, 309, 411, 300, 420, 321, 727, 584, 283, 17, 6915, 341, 14641], "temperature": 0.0, "avg_logprob": -0.1044932145338792, "compression_ratio": 1.7465437788018434, "no_speech_prob": 6.143965492810821e-06}, {"id": 349, "seek": 106242, "start": 1084.3000000000002, "end": 1086.3000000000002, "text": " function application.", "tokens": [2445, 3861, 13], "temperature": 0.0, "avg_logprob": -0.1044932145338792, "compression_ratio": 1.7465437788018434, "no_speech_prob": 6.143965492810821e-06}, {"id": 350, "seek": 106242, "start": 1086.3000000000002, "end": 1091.5, "text": " And so if I say f2 shift tab, then you can see this is now a function", "tokens": [400, 370, 498, 286, 584, 283, 17, 5513, 4421, 11, 550, 291, 393, 536, 341, 307, 586, 257, 2445], "temperature": 0.0, "avg_logprob": -0.1044932145338792, "compression_ratio": 1.7465437788018434, "no_speech_prob": 6.143965492810821e-06}, {"id": 351, "seek": 109150, "start": 1091.5, "end": 1092.82, "text": " that just takes epoch.", "tokens": [300, 445, 2516, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.12027265314470258, "compression_ratio": 1.748898678414097, "no_speech_prob": 9.665875040809624e-06}, {"id": 352, "seek": 109150, "start": 1092.82, "end": 1097.42, "text": " It just takes epoch because show progress took two parameters.", "tokens": [467, 445, 2516, 30992, 339, 570, 855, 4205, 1890, 732, 9834, 13], "temperature": 0.0, "avg_logprob": -0.12027265314470258, "compression_ratio": 1.748898678414097, "no_speech_prob": 9.665875040809624e-06}, {"id": 353, "seek": 109150, "start": 1097.42, "end": 1098.9, "text": " We've already passed it one.", "tokens": [492, 600, 1217, 4678, 309, 472, 13], "temperature": 0.0, "avg_logprob": -0.12027265314470258, "compression_ratio": 1.748898678414097, "no_speech_prob": 9.665875040809624e-06}, {"id": 354, "seek": 109150, "start": 1098.9, "end": 1101.7, "text": " So this now takes one parameter, which is what we need.", "tokens": [407, 341, 586, 2516, 472, 13075, 11, 597, 307, 437, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.12027265314470258, "compression_ratio": 1.748898678414097, "no_speech_prob": 9.665875040809624e-06}, {"id": 355, "seek": 109150, "start": 1101.7, "end": 1106.14, "text": " So that's why we could pass that to as our callback.", "tokens": [407, 300, 311, 983, 321, 727, 1320, 300, 281, 382, 527, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.12027265314470258, "compression_ratio": 1.748898678414097, "no_speech_prob": 9.665875040809624e-06}, {"id": 356, "seek": 109150, "start": 1106.14, "end": 1111.54, "text": " Okay. So we've seen a lot of those techniques already last week.", "tokens": [1033, 13, 407, 321, 600, 1612, 257, 688, 295, 729, 7512, 1217, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.12027265314470258, "compression_ratio": 1.748898678414097, "no_speech_prob": 9.665875040809624e-06}, {"id": 357, "seek": 109150, "start": 1111.54, "end": 1116.42, "text": " Most of what we saw last week, though, did not use a function", "tokens": [4534, 295, 437, 321, 1866, 1036, 1243, 11, 1673, 11, 630, 406, 764, 257, 2445], "temperature": 0.0, "avg_logprob": -0.12027265314470258, "compression_ratio": 1.748898678414097, "no_speech_prob": 9.665875040809624e-06}, {"id": 358, "seek": 109150, "start": 1116.42, "end": 1120.34, "text": " as a callback, but used a class as a callback.", "tokens": [382, 257, 818, 3207, 11, 457, 1143, 257, 1508, 382, 257, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.12027265314470258, "compression_ratio": 1.748898678414097, "no_speech_prob": 9.665875040809624e-06}, {"id": 359, "seek": 112034, "start": 1120.34, "end": 1122.74, "text": " So we could do exactly the same thing,", "tokens": [407, 321, 727, 360, 2293, 264, 912, 551, 11], "temperature": 0.0, "avg_logprob": -0.12989709206989833, "compression_ratio": 1.7, "no_speech_prob": 3.7265492665028432e-06}, {"id": 360, "seek": 112034, "start": 1122.74, "end": 1125.34, "text": " but pretty much any place you can use a closure,", "tokens": [457, 1238, 709, 604, 1081, 291, 393, 764, 257, 24653, 11], "temperature": 0.0, "avg_logprob": -0.12989709206989833, "compression_ratio": 1.7, "no_speech_prob": 3.7265492665028432e-06}, {"id": 361, "seek": 112034, "start": 1125.34, "end": 1126.9399999999998, "text": " you can also use a class.", "tokens": [291, 393, 611, 764, 257, 1508, 13], "temperature": 0.0, "avg_logprob": -0.12989709206989833, "compression_ratio": 1.7, "no_speech_prob": 3.7265492665028432e-06}, {"id": 362, "seek": 112034, "start": 1126.9399999999998, "end": 1130.8999999999999, "text": " Instead of storing it away inside the closure, some state,", "tokens": [7156, 295, 26085, 309, 1314, 1854, 264, 24653, 11, 512, 1785, 11], "temperature": 0.0, "avg_logprob": -0.12989709206989833, "compression_ratio": 1.7, "no_speech_prob": 3.7265492665028432e-06}, {"id": 363, "seek": 112034, "start": 1130.8999999999999, "end": 1134.06, "text": " we can store our state, in this case the explanation,", "tokens": [321, 393, 3531, 527, 1785, 11, 294, 341, 1389, 264, 10835, 11], "temperature": 0.0, "avg_logprob": -0.12989709206989833, "compression_ratio": 1.7, "no_speech_prob": 3.7265492665028432e-06}, {"id": 364, "seek": 112034, "start": 1134.06, "end": 1137.4199999999998, "text": " inside self, passing it into init.", "tokens": [1854, 2698, 11, 8437, 309, 666, 3157, 13], "temperature": 0.0, "avg_logprob": -0.12989709206989833, "compression_ratio": 1.7, "no_speech_prob": 3.7265492665028432e-06}, {"id": 365, "seek": 112034, "start": 1137.4199999999998, "end": 1140.1, "text": " Right? So here's exactly the same thing as we saw before,", "tokens": [1779, 30, 407, 510, 311, 2293, 264, 912, 551, 382, 321, 1866, 949, 11], "temperature": 0.0, "avg_logprob": -0.12989709206989833, "compression_ratio": 1.7, "no_speech_prob": 3.7265492665028432e-06}, {"id": 366, "seek": 112034, "start": 1140.1, "end": 1142.78, "text": " but as a class.", "tokens": [457, 382, 257, 1508, 13], "temperature": 0.0, "avg_logprob": -0.12989709206989833, "compression_ratio": 1.7, "no_speech_prob": 3.7265492665028432e-06}, {"id": 367, "seek": 112034, "start": 1142.78, "end": 1147.6999999999998, "text": " Dundacall is a special magic name, which will be called", "tokens": [413, 997, 326, 336, 307, 257, 2121, 5585, 1315, 11, 597, 486, 312, 1219], "temperature": 0.0, "avg_logprob": -0.12989709206989833, "compression_ratio": 1.7, "no_speech_prob": 3.7265492665028432e-06}, {"id": 368, "seek": 114770, "start": 1147.7, "end": 1154.3400000000001, "text": " if you take an object, so in this case a progress showing callback", "tokens": [498, 291, 747, 364, 2657, 11, 370, 294, 341, 1389, 257, 4205, 4099, 818, 3207], "temperature": 0.0, "avg_logprob": -0.189782386130475, "compression_ratio": 1.5360824742268042, "no_speech_prob": 6.540242793562356e-06}, {"id": 369, "seek": 114770, "start": 1154.3400000000001, "end": 1157.06, "text": " object, and call it with parentheses.", "tokens": [2657, 11, 293, 818, 309, 365, 34153, 13], "temperature": 0.0, "avg_logprob": -0.189782386130475, "compression_ratio": 1.5360824742268042, "no_speech_prob": 6.540242793562356e-06}, {"id": 370, "seek": 114770, "start": 1157.06, "end": 1161.9, "text": " So if I go CB, hi, you see I'm taking that object", "tokens": [407, 498, 286, 352, 18745, 11, 4879, 11, 291, 536, 286, 478, 1940, 300, 2657], "temperature": 0.0, "avg_logprob": -0.189782386130475, "compression_ratio": 1.5360824742268042, "no_speech_prob": 6.540242793562356e-06}, {"id": 371, "seek": 114770, "start": 1161.9, "end": 1163.6200000000001, "text": " and I'm treating it as if it's a function,", "tokens": [293, 286, 478, 15083, 309, 382, 498, 309, 311, 257, 2445, 11], "temperature": 0.0, "avg_logprob": -0.189782386130475, "compression_ratio": 1.5360824742268042, "no_speech_prob": 6.540242793562356e-06}, {"id": 372, "seek": 114770, "start": 1163.6200000000001, "end": 1166.7, "text": " and that will call Dundacall.", "tokens": [293, 300, 486, 818, 413, 997, 326, 336, 13], "temperature": 0.0, "avg_logprob": -0.189782386130475, "compression_ratio": 1.5360824742268042, "no_speech_prob": 6.540242793562356e-06}, {"id": 373, "seek": 114770, "start": 1166.7, "end": 1170.3400000000001, "text": " If you've used other languages, like in C++,", "tokens": [759, 291, 600, 1143, 661, 8650, 11, 411, 294, 383, 25472, 11], "temperature": 0.0, "avg_logprob": -0.189782386130475, "compression_ratio": 1.5360824742268042, "no_speech_prob": 6.540242793562356e-06}, {"id": 374, "seek": 114770, "start": 1170.3400000000001, "end": 1173.3400000000001, "text": " this is called a functor.", "tokens": [341, 307, 1219, 257, 1019, 1672, 13], "temperature": 0.0, "avg_logprob": -0.189782386130475, "compression_ratio": 1.5360824742268042, "no_speech_prob": 6.540242793562356e-06}, {"id": 375, "seek": 117334, "start": 1173.34, "end": 1178.1, "text": " More generally it's called a callable in Python.", "tokens": [5048, 5101, 309, 311, 1219, 257, 818, 712, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.24421163286481584, "compression_ratio": 1.4968944099378882, "no_speech_prob": 7.888946129241958e-06}, {"id": 376, "seek": 117334, "start": 1178.1, "end": 1183.02, "text": " So it's a kind of something that a lot of languages have.", "tokens": [407, 309, 311, 257, 733, 295, 746, 300, 257, 688, 295, 8650, 362, 13], "temperature": 0.0, "avg_logprob": -0.24421163286481584, "compression_ratio": 1.4968944099378882, "no_speech_prob": 7.888946129241958e-06}, {"id": 377, "seek": 117334, "start": 1183.02, "end": 1189.82, "text": " All right, so now we can use that as a callback, just like before.", "tokens": [1057, 558, 11, 370, 586, 321, 393, 764, 300, 382, 257, 818, 3207, 11, 445, 411, 949, 13], "temperature": 0.0, "avg_logprob": -0.24421163286481584, "compression_ratio": 1.4968944099378882, "no_speech_prob": 7.888946129241958e-06}, {"id": 378, "seek": 117334, "start": 1189.82, "end": 1197.6999999999998, "text": " All right, next thing to look at is for our callback is we're going", "tokens": [1057, 558, 11, 958, 551, 281, 574, 412, 307, 337, 527, 818, 3207, 307, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.24421163286481584, "compression_ratio": 1.4968944099378882, "no_speech_prob": 7.888946129241958e-06}, {"id": 379, "seek": 119770, "start": 1197.7, "end": 1203.54, "text": " to use star args and star star kw args, or otherwise known as quags.", "tokens": [281, 764, 3543, 3882, 82, 293, 3543, 3543, 23846, 3882, 82, 11, 420, 5911, 2570, 382, 421, 12109, 13], "temperature": 0.0, "avg_logprob": -0.1390074348449707, "compression_ratio": 1.783068783068783, "no_speech_prob": 8.01333135314053e-06}, {"id": 380, "seek": 119770, "start": 1203.54, "end": 1205.82, "text": " For those of you that don't know what these mean,", "tokens": [1171, 729, 295, 291, 300, 500, 380, 458, 437, 613, 914, 11], "temperature": 0.0, "avg_logprob": -0.1390074348449707, "compression_ratio": 1.783068783068783, "no_speech_prob": 8.01333135314053e-06}, {"id": 381, "seek": 119770, "start": 1205.82, "end": 1211.1000000000001, "text": " let's create a function that takes star args and star star kw args", "tokens": [718, 311, 1884, 257, 2445, 300, 2516, 3543, 3882, 82, 293, 3543, 3543, 23846, 3882, 82], "temperature": 0.0, "avg_logprob": -0.1390074348449707, "compression_ratio": 1.783068783068783, "no_speech_prob": 8.01333135314053e-06}, {"id": 382, "seek": 119770, "start": 1211.1000000000001, "end": 1215.22, "text": " and prints out args and kw args.", "tokens": [293, 22305, 484, 3882, 82, 293, 23846, 3882, 82, 13], "temperature": 0.0, "avg_logprob": -0.1390074348449707, "compression_ratio": 1.783068783068783, "no_speech_prob": 8.01333135314053e-06}, {"id": 383, "seek": 119770, "start": 1215.22, "end": 1221.5800000000002, "text": " So if I call that function, I could pass it 3a thing1 equals hello,", "tokens": [407, 498, 286, 818, 300, 2445, 11, 286, 727, 1320, 309, 805, 64, 551, 16, 6915, 7751, 11], "temperature": 0.0, "avg_logprob": -0.1390074348449707, "compression_ratio": 1.783068783068783, "no_speech_prob": 8.01333135314053e-06}, {"id": 384, "seek": 119770, "start": 1221.5800000000002, "end": 1223.26, "text": " and you'll see that all the things that are passed", "tokens": [293, 291, 603, 536, 300, 439, 264, 721, 300, 366, 4678], "temperature": 0.0, "avg_logprob": -0.1390074348449707, "compression_ratio": 1.783068783068783, "no_speech_prob": 8.01333135314053e-06}, {"id": 385, "seek": 122326, "start": 1223.26, "end": 1228.14, "text": " as positional arguments end up in a tuple called args,", "tokens": [382, 2535, 304, 12869, 917, 493, 294, 257, 2604, 781, 1219, 3882, 82, 11], "temperature": 0.0, "avg_logprob": -0.11227849692352547, "compression_ratio": 1.6455696202531647, "no_speech_prob": 6.854100320197176e-06}, {"id": 386, "seek": 122326, "start": 1228.14, "end": 1231.62, "text": " and all the things passed as keyword arguments end", "tokens": [293, 439, 264, 721, 4678, 382, 20428, 12869, 917], "temperature": 0.0, "avg_logprob": -0.11227849692352547, "compression_ratio": 1.6455696202531647, "no_speech_prob": 6.854100320197176e-06}, {"id": 387, "seek": 122326, "start": 1231.62, "end": 1234.1, "text": " up as a dictionary called quags.", "tokens": [493, 382, 257, 25890, 1219, 421, 12109, 13], "temperature": 0.0, "avg_logprob": -0.11227849692352547, "compression_ratio": 1.6455696202531647, "no_speech_prob": 6.854100320197176e-06}, {"id": 388, "seek": 122326, "start": 1234.1, "end": 1236.5, "text": " That's literally all these things do.", "tokens": [663, 311, 3736, 439, 613, 721, 360, 13], "temperature": 0.0, "avg_logprob": -0.11227849692352547, "compression_ratio": 1.6455696202531647, "no_speech_prob": 6.854100320197176e-06}, {"id": 389, "seek": 122326, "start": 1236.5, "end": 1237.82, "text": " All right?", "tokens": [1057, 558, 30], "temperature": 0.0, "avg_logprob": -0.11227849692352547, "compression_ratio": 1.6455696202531647, "no_speech_prob": 6.854100320197176e-06}, {"id": 390, "seek": 122326, "start": 1237.82, "end": 1239.98, "text": " And so PyTorch uses that, for example,", "tokens": [400, 370, 9953, 51, 284, 339, 4960, 300, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.11227849692352547, "compression_ratio": 1.6455696202531647, "no_speech_prob": 6.854100320197176e-06}, {"id": 391, "seek": 122326, "start": 1239.98, "end": 1244.86, "text": " when you create an nn.sequential, it takes what you pass", "tokens": [562, 291, 1884, 364, 297, 77, 13, 11834, 2549, 11, 309, 2516, 437, 291, 1320], "temperature": 0.0, "avg_logprob": -0.11227849692352547, "compression_ratio": 1.6455696202531647, "no_speech_prob": 6.854100320197176e-06}, {"id": 392, "seek": 122326, "start": 1244.86, "end": 1247.3, "text": " in as a star args, right?", "tokens": [294, 382, 257, 3543, 3882, 82, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11227849692352547, "compression_ratio": 1.6455696202531647, "no_speech_prob": 6.854100320197176e-06}, {"id": 393, "seek": 122326, "start": 1247.3, "end": 1250.5, "text": " You just pass them directly and it turns it into a tuple.", "tokens": [509, 445, 1320, 552, 3838, 293, 309, 4523, 309, 666, 257, 2604, 781, 13], "temperature": 0.0, "avg_logprob": -0.11227849692352547, "compression_ratio": 1.6455696202531647, "no_speech_prob": 6.854100320197176e-06}, {"id": 394, "seek": 122326, "start": 1250.5, "end": 1252.94, "text": " So why do we use this?", "tokens": [407, 983, 360, 321, 764, 341, 30], "temperature": 0.0, "avg_logprob": -0.11227849692352547, "compression_ratio": 1.6455696202531647, "no_speech_prob": 6.854100320197176e-06}, {"id": 395, "seek": 125294, "start": 1252.94, "end": 1256.46, "text": " There's a few reasons we use it, but one of the common ways", "tokens": [821, 311, 257, 1326, 4112, 321, 764, 309, 11, 457, 472, 295, 264, 2689, 2098], "temperature": 0.0, "avg_logprob": -0.11155213151022653, "compression_ratio": 1.5949367088607596, "no_speech_prob": 8.529841579729691e-06}, {"id": 396, "seek": 125294, "start": 1256.46, "end": 1261.5800000000002, "text": " to use it is if you kind of want to wrap some other class or object,", "tokens": [281, 764, 309, 307, 498, 291, 733, 295, 528, 281, 7019, 512, 661, 1508, 420, 2657, 11], "temperature": 0.0, "avg_logprob": -0.11155213151022653, "compression_ratio": 1.5949367088607596, "no_speech_prob": 8.529841579729691e-06}, {"id": 397, "seek": 125294, "start": 1261.5800000000002, "end": 1265.46, "text": " then you can take a bunch of stuff as star star quags", "tokens": [550, 291, 393, 747, 257, 3840, 295, 1507, 382, 3543, 3543, 421, 12109], "temperature": 0.0, "avg_logprob": -0.11155213151022653, "compression_ratio": 1.5949367088607596, "no_speech_prob": 8.529841579729691e-06}, {"id": 398, "seek": 125294, "start": 1265.46, "end": 1270.06, "text": " and pass it off to some other functional object.", "tokens": [293, 1320, 309, 766, 281, 512, 661, 11745, 2657, 13], "temperature": 0.0, "avg_logprob": -0.11155213151022653, "compression_ratio": 1.5949367088607596, "no_speech_prob": 8.529841579729691e-06}, {"id": 399, "seek": 125294, "start": 1270.06, "end": 1272.8600000000001, "text": " We're getting better at this and we're removing a lot of the usages,", "tokens": [492, 434, 1242, 1101, 412, 341, 293, 321, 434, 12720, 257, 688, 295, 264, 505, 1660, 11], "temperature": 0.0, "avg_logprob": -0.11155213151022653, "compression_ratio": 1.5949367088607596, "no_speech_prob": 8.529841579729691e-06}, {"id": 400, "seek": 125294, "start": 1272.8600000000001, "end": 1275.18, "text": " but in the early days of Fast AI version 1,", "tokens": [457, 294, 264, 2440, 1708, 295, 15968, 7318, 3037, 502, 11], "temperature": 0.0, "avg_logprob": -0.11155213151022653, "compression_ratio": 1.5949367088607596, "no_speech_prob": 8.529841579729691e-06}, {"id": 401, "seek": 125294, "start": 1275.18, "end": 1277.78, "text": " we actually were overusing quags.", "tokens": [321, 767, 645, 670, 7981, 421, 12109, 13], "temperature": 0.0, "avg_logprob": -0.11155213151022653, "compression_ratio": 1.5949367088607596, "no_speech_prob": 8.529841579729691e-06}, {"id": 402, "seek": 127778, "start": 1277.78, "end": 1283.78, "text": " So quite often, we would kind of, there would be a lot of stuff", "tokens": [407, 1596, 2049, 11, 321, 576, 733, 295, 11, 456, 576, 312, 257, 688, 295, 1507], "temperature": 0.0, "avg_logprob": -0.11228374640146892, "compression_ratio": 1.6094890510948905, "no_speech_prob": 3.0894182145857485e-06}, {"id": 403, "seek": 127778, "start": 1283.78, "end": 1286.74, "text": " that wasn't obviously in the parameter list of a function", "tokens": [300, 2067, 380, 2745, 294, 264, 13075, 1329, 295, 257, 2445], "temperature": 0.0, "avg_logprob": -0.11228374640146892, "compression_ratio": 1.6094890510948905, "no_speech_prob": 3.0894182145857485e-06}, {"id": 404, "seek": 127778, "start": 1286.74, "end": 1290.34, "text": " that ended up in quags and then we would pass it down to,", "tokens": [300, 4590, 493, 294, 421, 12109, 293, 550, 321, 576, 1320, 309, 760, 281, 11], "temperature": 0.0, "avg_logprob": -0.11228374640146892, "compression_ratio": 1.6094890510948905, "no_speech_prob": 3.0894182145857485e-06}, {"id": 405, "seek": 127778, "start": 1290.34, "end": 1293.74, "text": " I don't know, the PyTorch data loader initializer or something.", "tokens": [286, 500, 380, 458, 11, 264, 9953, 51, 284, 339, 1412, 3677, 260, 5883, 6545, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.11228374640146892, "compression_ratio": 1.6094890510948905, "no_speech_prob": 3.0894182145857485e-06}, {"id": 406, "seek": 127778, "start": 1293.74, "end": 1296.06, "text": " And so we've been gradually removing those usages", "tokens": [400, 370, 321, 600, 668, 13145, 12720, 729, 505, 1660], "temperature": 0.0, "avg_logprob": -0.11228374640146892, "compression_ratio": 1.6094890510948905, "no_speech_prob": 3.0894182145857485e-06}, {"id": 407, "seek": 127778, "start": 1296.06, "end": 1300.98, "text": " because like it's mainly most helpful for kind of quick", "tokens": [570, 411, 309, 311, 8704, 881, 4961, 337, 733, 295, 1702], "temperature": 0.0, "avg_logprob": -0.11228374640146892, "compression_ratio": 1.6094890510948905, "no_speech_prob": 3.0894182145857485e-06}, {"id": 408, "seek": 127778, "start": 1300.98, "end": 1302.82, "text": " and dirty throwing things together.", "tokens": [293, 9360, 10238, 721, 1214, 13], "temperature": 0.0, "avg_logprob": -0.11228374640146892, "compression_ratio": 1.6094890510948905, "no_speech_prob": 3.0894182145857485e-06}, {"id": 409, "seek": 127778, "start": 1302.82, "end": 1307.7, "text": " In R, they actually use an ellipsis for the same thing.", "tokens": [682, 497, 11, 436, 767, 764, 364, 8284, 2600, 271, 337, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.11228374640146892, "compression_ratio": 1.6094890510948905, "no_speech_prob": 3.0894182145857485e-06}, {"id": 410, "seek": 130770, "start": 1307.7, "end": 1309.02, "text": " They kind of overuse it.", "tokens": [814, 733, 295, 670, 438, 309, 13], "temperature": 0.0, "avg_logprob": -0.1342610539616765, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.3287519777804846e-06}, {"id": 411, "seek": 130770, "start": 1309.02, "end": 1310.9, "text": " Quite often, it's hard to see what's going on.", "tokens": [20464, 2049, 11, 309, 311, 1152, 281, 536, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.1342610539616765, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.3287519777804846e-06}, {"id": 412, "seek": 130770, "start": 1310.9, "end": 1314.78, "text": " You might have noticed in Matplotlib, a lot of times the thing you're trying", "tokens": [509, 1062, 362, 5694, 294, 6789, 564, 310, 38270, 11, 257, 688, 295, 1413, 264, 551, 291, 434, 1382], "temperature": 0.0, "avg_logprob": -0.1342610539616765, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.3287519777804846e-06}, {"id": 413, "seek": 130770, "start": 1314.78, "end": 1318.1000000000001, "text": " to pass to Matplotlib isn't there in the shift tab.", "tokens": [281, 1320, 281, 6789, 564, 310, 38270, 1943, 380, 456, 294, 264, 5513, 4421, 13], "temperature": 0.0, "avg_logprob": -0.1342610539616765, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.3287519777804846e-06}, {"id": 414, "seek": 130770, "start": 1318.1000000000001, "end": 1321.02, "text": " When you hit shift tab, it's the same thing they're using quags.", "tokens": [1133, 291, 2045, 5513, 4421, 11, 309, 311, 264, 912, 551, 436, 434, 1228, 421, 12109, 13], "temperature": 0.0, "avg_logprob": -0.1342610539616765, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.3287519777804846e-06}, {"id": 415, "seek": 130770, "start": 1321.02, "end": 1323.3, "text": " So there are some downsides to using it,", "tokens": [407, 456, 366, 512, 21554, 1875, 281, 1228, 309, 11], "temperature": 0.0, "avg_logprob": -0.1342610539616765, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.3287519777804846e-06}, {"id": 416, "seek": 130770, "start": 1323.3, "end": 1326.02, "text": " but there are some places you really want to use it.", "tokens": [457, 456, 366, 512, 3190, 291, 534, 528, 281, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.1342610539616765, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.3287519777804846e-06}, {"id": 417, "seek": 130770, "start": 1326.02, "end": 1327.8600000000001, "text": " For example, take a look at this.", "tokens": [1171, 1365, 11, 747, 257, 574, 412, 341, 13], "temperature": 0.0, "avg_logprob": -0.1342610539616765, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.3287519777804846e-06}, {"id": 418, "seek": 130770, "start": 1327.8600000000001, "end": 1331.98, "text": " Let's take Rewrite slow calculation, but this time we're going", "tokens": [961, 311, 747, 497, 1023, 35002, 2964, 17108, 11, 457, 341, 565, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.1342610539616765, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.3287519777804846e-06}, {"id": 419, "seek": 130770, "start": 1331.98, "end": 1334.5, "text": " to allow the user to create a callback", "tokens": [281, 2089, 264, 4195, 281, 1884, 257, 818, 3207], "temperature": 0.0, "avg_logprob": -0.1342610539616765, "compression_ratio": 1.7307692307692308, "no_speech_prob": 1.3287519777804846e-06}, {"id": 420, "seek": 133450, "start": 1334.5, "end": 1338.02, "text": " that is called before the calculation occurs", "tokens": [300, 307, 1219, 949, 264, 17108, 11843], "temperature": 0.0, "avg_logprob": -0.1204388439655304, "compression_ratio": 1.7155963302752293, "no_speech_prob": 4.785060809808783e-06}, {"id": 421, "seek": 133450, "start": 1338.02, "end": 1341.66, "text": " and after the calculation that occurs.", "tokens": [293, 934, 264, 17108, 300, 11843, 13], "temperature": 0.0, "avg_logprob": -0.1204388439655304, "compression_ratio": 1.7155963302752293, "no_speech_prob": 4.785060809808783e-06}, {"id": 422, "seek": 133450, "start": 1341.66, "end": 1344.02, "text": " And the after calculation one's a bit tricky because it's going", "tokens": [400, 264, 934, 17108, 472, 311, 257, 857, 12414, 570, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.1204388439655304, "compression_ratio": 1.7155963302752293, "no_speech_prob": 4.785060809808783e-06}, {"id": 423, "seek": 133450, "start": 1344.02, "end": 1345.86, "text": " to take two parameters now.", "tokens": [281, 747, 732, 9834, 586, 13], "temperature": 0.0, "avg_logprob": -0.1204388439655304, "compression_ratio": 1.7155963302752293, "no_speech_prob": 4.785060809808783e-06}, {"id": 424, "seek": 133450, "start": 1345.86, "end": 1348.7, "text": " It's going to take both the epoch number", "tokens": [467, 311, 516, 281, 747, 1293, 264, 30992, 339, 1230], "temperature": 0.0, "avg_logprob": -0.1204388439655304, "compression_ratio": 1.7155963302752293, "no_speech_prob": 4.785060809808783e-06}, {"id": 425, "seek": 133450, "start": 1348.7, "end": 1352.86, "text": " and also what have we calculated so far, right?", "tokens": [293, 611, 437, 362, 321, 15598, 370, 1400, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1204388439655304, "compression_ratio": 1.7155963302752293, "no_speech_prob": 4.785060809808783e-06}, {"id": 426, "seek": 133450, "start": 1352.86, "end": 1358.46, "text": " So we can't just call CB parentheses I. We actually now have to assume", "tokens": [407, 321, 393, 380, 445, 818, 18745, 34153, 286, 13, 492, 767, 586, 362, 281, 6552], "temperature": 0.0, "avg_logprob": -0.1204388439655304, "compression_ratio": 1.7155963302752293, "no_speech_prob": 4.785060809808783e-06}, {"id": 427, "seek": 133450, "start": 1358.46, "end": 1361.98, "text": " that it's got some particular methods.", "tokens": [300, 309, 311, 658, 512, 1729, 7150, 13], "temperature": 0.0, "avg_logprob": -0.1204388439655304, "compression_ratio": 1.7155963302752293, "no_speech_prob": 4.785060809808783e-06}, {"id": 428, "seek": 136198, "start": 1361.98, "end": 1366.94, "text": " So here is, for example, a print step callback, right?", "tokens": [407, 510, 307, 11, 337, 1365, 11, 257, 4482, 1823, 818, 3207, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.08714302447663635, "compression_ratio": 1.6811023622047243, "no_speech_prob": 7.182824901974527e-06}, {"id": 429, "seek": 136198, "start": 1366.94, "end": 1370.46, "text": " Which before calculation just says I'm about to start", "tokens": [3013, 949, 17108, 445, 1619, 286, 478, 466, 281, 722], "temperature": 0.0, "avg_logprob": -0.08714302447663635, "compression_ratio": 1.6811023622047243, "no_speech_prob": 7.182824901974527e-06}, {"id": 430, "seek": 136198, "start": 1370.46, "end": 1375.74, "text": " and after calculation it says I'm done and there it's running.", "tokens": [293, 934, 17108, 309, 1619, 286, 478, 1096, 293, 456, 309, 311, 2614, 13], "temperature": 0.0, "avg_logprob": -0.08714302447663635, "compression_ratio": 1.6811023622047243, "no_speech_prob": 7.182824901974527e-06}, {"id": 431, "seek": 136198, "start": 1375.74, "end": 1378.6200000000001, "text": " So in this case, this callback didn't actually care", "tokens": [407, 294, 341, 1389, 11, 341, 818, 3207, 994, 380, 767, 1127], "temperature": 0.0, "avg_logprob": -0.08714302447663635, "compression_ratio": 1.6811023622047243, "no_speech_prob": 7.182824901974527e-06}, {"id": 432, "seek": 136198, "start": 1378.6200000000001, "end": 1381.94, "text": " about the epoch number or about the value, right?", "tokens": [466, 264, 30992, 339, 1230, 420, 466, 264, 2158, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.08714302447663635, "compression_ratio": 1.6811023622047243, "no_speech_prob": 7.182824901974527e-06}, {"id": 433, "seek": 136198, "start": 1381.94, "end": 1387.8600000000001, "text": " And so it just has star args, star, star quags in both places.", "tokens": [400, 370, 309, 445, 575, 3543, 3882, 82, 11, 3543, 11, 3543, 421, 12109, 294, 1293, 3190, 13], "temperature": 0.0, "avg_logprob": -0.08714302447663635, "compression_ratio": 1.6811023622047243, "no_speech_prob": 7.182824901974527e-06}, {"id": 434, "seek": 136198, "start": 1387.8600000000001, "end": 1390.06, "text": " It doesn't have to worry about exactly what's being passed", "tokens": [467, 1177, 380, 362, 281, 3292, 466, 2293, 437, 311, 885, 4678], "temperature": 0.0, "avg_logprob": -0.08714302447663635, "compression_ratio": 1.6811023622047243, "no_speech_prob": 7.182824901974527e-06}, {"id": 435, "seek": 136198, "start": 1390.06, "end": 1391.82, "text": " in because it's not using them.", "tokens": [294, 570, 309, 311, 406, 1228, 552, 13], "temperature": 0.0, "avg_logprob": -0.08714302447663635, "compression_ratio": 1.6811023622047243, "no_speech_prob": 7.182824901974527e-06}, {"id": 436, "seek": 139182, "start": 1391.82, "end": 1395.02, "text": " So this is quite a good kind of use of this is", "tokens": [407, 341, 307, 1596, 257, 665, 733, 295, 764, 295, 341, 307], "temperature": 0.0, "avg_logprob": -0.15856048517059862, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.4063514754525386e-05}, {"id": 437, "seek": 139182, "start": 1395.02, "end": 1400.5, "text": " to basically create a function that's going to be used somewhere else", "tokens": [281, 1936, 1884, 257, 2445, 300, 311, 516, 281, 312, 1143, 4079, 1646], "temperature": 0.0, "avg_logprob": -0.15856048517059862, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.4063514754525386e-05}, {"id": 438, "seek": 139182, "start": 1400.5, "end": 1403.22, "text": " and you don't care about one or more of the parameters or you want", "tokens": [293, 291, 500, 380, 1127, 466, 472, 420, 544, 295, 264, 9834, 420, 291, 528], "temperature": 0.0, "avg_logprob": -0.15856048517059862, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.4063514754525386e-05}, {"id": 439, "seek": 139182, "start": 1403.22, "end": 1405.1799999999998, "text": " to make things more flexible.", "tokens": [281, 652, 721, 544, 11358, 13], "temperature": 0.0, "avg_logprob": -0.15856048517059862, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.4063514754525386e-05}, {"id": 440, "seek": 139182, "start": 1405.1799999999998, "end": 1411.62, "text": " So in this case, we don't get an error saying, because if we remove this,", "tokens": [407, 294, 341, 1389, 11, 321, 500, 380, 483, 364, 6713, 1566, 11, 570, 498, 321, 4159, 341, 11], "temperature": 0.0, "avg_logprob": -0.15856048517059862, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.4063514754525386e-05}, {"id": 441, "seek": 139182, "start": 1411.62, "end": 1415.02, "text": " which looks like we should be able to do because we don't use anything,", "tokens": [597, 1542, 411, 321, 820, 312, 1075, 281, 360, 570, 321, 500, 380, 764, 1340, 11], "temperature": 0.0, "avg_logprob": -0.15856048517059862, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.4063514754525386e-05}, {"id": 442, "seek": 139182, "start": 1415.02, "end": 1417.5, "text": " but here's the problem.", "tokens": [457, 510, 311, 264, 1154, 13], "temperature": 0.0, "avg_logprob": -0.15856048517059862, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.4063514754525386e-05}, {"id": 443, "seek": 139182, "start": 1417.5, "end": 1419.86, "text": " It tried to call before calc I.", "tokens": [467, 3031, 281, 818, 949, 2104, 66, 286, 13], "temperature": 0.0, "avg_logprob": -0.15856048517059862, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.4063514754525386e-05}, {"id": 444, "seek": 141986, "start": 1419.86, "end": 1422.9399999999998, "text": " And before calc doesn't take an I, right?", "tokens": [400, 949, 2104, 66, 1177, 380, 747, 364, 286, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13925443996082654, "compression_ratio": 1.6631205673758864, "no_speech_prob": 4.222712050250266e-06}, {"id": 445, "seek": 141986, "start": 1422.9399999999998, "end": 1427.3799999999999, "text": " So if you put in both positional and keyword arguments,", "tokens": [407, 498, 291, 829, 294, 1293, 2535, 304, 293, 20428, 12869, 11], "temperature": 0.0, "avg_logprob": -0.13925443996082654, "compression_ratio": 1.6631205673758864, "no_speech_prob": 4.222712050250266e-06}, {"id": 446, "seek": 141986, "start": 1427.3799999999999, "end": 1429.82, "text": " it'll always work everywhere.", "tokens": [309, 603, 1009, 589, 5315, 13], "temperature": 0.0, "avg_logprob": -0.13925443996082654, "compression_ratio": 1.6631205673758864, "no_speech_prob": 4.222712050250266e-06}, {"id": 447, "seek": 141986, "start": 1429.82, "end": 1434.34, "text": " And so here we can actually use them.", "tokens": [400, 370, 510, 321, 393, 767, 764, 552, 13], "temperature": 0.0, "avg_logprob": -0.13925443996082654, "compression_ratio": 1.6631205673758864, "no_speech_prob": 4.222712050250266e-06}, {"id": 448, "seek": 141986, "start": 1434.34, "end": 1438.06, "text": " So let's actually use epoch and value to print out those details.", "tokens": [407, 718, 311, 767, 764, 30992, 339, 293, 2158, 281, 4482, 484, 729, 4365, 13], "temperature": 0.0, "avg_logprob": -0.13925443996082654, "compression_ratio": 1.6631205673758864, "no_speech_prob": 4.222712050250266e-06}, {"id": 449, "seek": 141986, "start": 1438.06, "end": 1442.1799999999998, "text": " So now you can see there it is printing them out.", "tokens": [407, 586, 291, 393, 536, 456, 309, 307, 14699, 552, 484, 13], "temperature": 0.0, "avg_logprob": -0.13925443996082654, "compression_ratio": 1.6631205673758864, "no_speech_prob": 4.222712050250266e-06}, {"id": 450, "seek": 141986, "start": 1442.1799999999998, "end": 1445.6999999999998, "text": " And in this case, I've put star, star quags at the end because maybe,", "tokens": [400, 294, 341, 1389, 11, 286, 600, 829, 3543, 11, 3543, 421, 12109, 412, 264, 917, 570, 1310, 11], "temperature": 0.0, "avg_logprob": -0.13925443996082654, "compression_ratio": 1.6631205673758864, "no_speech_prob": 4.222712050250266e-06}, {"id": 451, "seek": 141986, "start": 1445.6999999999998, "end": 1447.9399999999998, "text": " you know, in the future there'll be some other things that are passed", "tokens": [291, 458, 11, 294, 264, 2027, 456, 603, 312, 512, 661, 721, 300, 366, 4678], "temperature": 0.0, "avg_logprob": -0.13925443996082654, "compression_ratio": 1.6631205673758864, "no_speech_prob": 4.222712050250266e-06}, {"id": 452, "seek": 141986, "start": 1447.9399999999998, "end": 1449.78, "text": " in and we want to make sure this doesn't break.", "tokens": [294, 293, 321, 528, 281, 652, 988, 341, 1177, 380, 1821, 13], "temperature": 0.0, "avg_logprob": -0.13925443996082654, "compression_ratio": 1.6631205673758864, "no_speech_prob": 4.222712050250266e-06}, {"id": 453, "seek": 144978, "start": 1449.78, "end": 1452.98, "text": " So it kind of makes it more resilient.", "tokens": [407, 309, 733, 295, 1669, 309, 544, 23699, 13], "temperature": 0.0, "avg_logprob": -0.11055509008542456, "compression_ratio": 1.78, "no_speech_prob": 1.3210996257839724e-05}, {"id": 454, "seek": 144978, "start": 1452.98, "end": 1457.5, "text": " The next thing we might want to do with callbacks is", "tokens": [440, 958, 551, 321, 1062, 528, 281, 360, 365, 818, 17758, 307], "temperature": 0.0, "avg_logprob": -0.11055509008542456, "compression_ratio": 1.78, "no_speech_prob": 1.3210996257839724e-05}, {"id": 455, "seek": 144978, "start": 1457.5, "end": 1460.86, "text": " to actually change something.", "tokens": [281, 767, 1319, 746, 13], "temperature": 0.0, "avg_logprob": -0.11055509008542456, "compression_ratio": 1.78, "no_speech_prob": 1.3210996257839724e-05}, {"id": 456, "seek": 144978, "start": 1460.86, "end": 1463.5, "text": " So a couple of things that we did last week.", "tokens": [407, 257, 1916, 295, 721, 300, 321, 630, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.11055509008542456, "compression_ratio": 1.78, "no_speech_prob": 1.3210996257839724e-05}, {"id": 457, "seek": 144978, "start": 1463.5, "end": 1468.46, "text": " One was we wanted to be able to cancel out of a loop to stop early.", "tokens": [1485, 390, 321, 1415, 281, 312, 1075, 281, 10373, 484, 295, 257, 6367, 281, 1590, 2440, 13], "temperature": 0.0, "avg_logprob": -0.11055509008542456, "compression_ratio": 1.78, "no_speech_prob": 1.3210996257839724e-05}, {"id": 458, "seek": 144978, "start": 1468.46, "end": 1469.86, "text": " The other thing we might want", "tokens": [440, 661, 551, 321, 1062, 528], "temperature": 0.0, "avg_logprob": -0.11055509008542456, "compression_ratio": 1.78, "no_speech_prob": 1.3210996257839724e-05}, {"id": 459, "seek": 144978, "start": 1469.86, "end": 1473.1, "text": " to do is actually change the value of something.", "tokens": [281, 360, 307, 767, 1319, 264, 2158, 295, 746, 13], "temperature": 0.0, "avg_logprob": -0.11055509008542456, "compression_ratio": 1.78, "no_speech_prob": 1.3210996257839724e-05}, {"id": 460, "seek": 144978, "start": 1473.1, "end": 1478.18, "text": " So in order to stop early, we could check.", "tokens": [407, 294, 1668, 281, 1590, 2440, 11, 321, 727, 1520, 13], "temperature": 0.0, "avg_logprob": -0.11055509008542456, "compression_ratio": 1.78, "no_speech_prob": 1.3210996257839724e-05}, {"id": 461, "seek": 147818, "start": 1478.18, "end": 1481.02, "text": " And also the other thing we might want to do is say, well,", "tokens": [400, 611, 264, 661, 551, 321, 1062, 528, 281, 360, 307, 584, 11, 731, 11], "temperature": 0.0, "avg_logprob": -0.07841940720876057, "compression_ratio": 1.7609561752988048, "no_speech_prob": 1.7777854736777954e-05}, {"id": 462, "seek": 147818, "start": 1481.02, "end": 1484.26, "text": " what if you don't want to define before calc or after calc?", "tokens": [437, 498, 291, 500, 380, 528, 281, 6964, 949, 2104, 66, 420, 934, 2104, 66, 30], "temperature": 0.0, "avg_logprob": -0.07841940720876057, "compression_ratio": 1.7609561752988048, "no_speech_prob": 1.7777854736777954e-05}, {"id": 463, "seek": 147818, "start": 1484.26, "end": 1485.9, "text": " We wouldn't want everything to break.", "tokens": [492, 2759, 380, 528, 1203, 281, 1821, 13], "temperature": 0.0, "avg_logprob": -0.07841940720876057, "compression_ratio": 1.7609561752988048, "no_speech_prob": 1.7777854736777954e-05}, {"id": 464, "seek": 147818, "start": 1485.9, "end": 1489.94, "text": " So we could actually check whether a callback's defined", "tokens": [407, 321, 727, 767, 1520, 1968, 257, 818, 3207, 311, 7642], "temperature": 0.0, "avg_logprob": -0.07841940720876057, "compression_ratio": 1.7609561752988048, "no_speech_prob": 1.7777854736777954e-05}, {"id": 465, "seek": 147818, "start": 1489.94, "end": 1492.02, "text": " and only call it if it is.", "tokens": [293, 787, 818, 309, 498, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.07841940720876057, "compression_ratio": 1.7609561752988048, "no_speech_prob": 1.7777854736777954e-05}, {"id": 466, "seek": 147818, "start": 1492.02, "end": 1494.8600000000001, "text": " And we could actually check the return value", "tokens": [400, 321, 727, 767, 1520, 264, 2736, 2158], "temperature": 0.0, "avg_logprob": -0.07841940720876057, "compression_ratio": 1.7609561752988048, "no_speech_prob": 1.7777854736777954e-05}, {"id": 467, "seek": 147818, "start": 1494.8600000000001, "end": 1497.42, "text": " and then do something based on the return value.", "tokens": [293, 550, 360, 746, 2361, 322, 264, 2736, 2158, 13], "temperature": 0.0, "avg_logprob": -0.07841940720876057, "compression_ratio": 1.7609561752988048, "no_speech_prob": 1.7777854736777954e-05}, {"id": 468, "seek": 147818, "start": 1497.42, "end": 1502.5800000000002, "text": " So here's something which will cancel out of our loop", "tokens": [407, 510, 311, 746, 597, 486, 10373, 484, 295, 527, 6367], "temperature": 0.0, "avg_logprob": -0.07841940720876057, "compression_ratio": 1.7609561752988048, "no_speech_prob": 1.7777854736777954e-05}, {"id": 469, "seek": 147818, "start": 1502.5800000000002, "end": 1506.9, "text": " if the value that's been calculated so far is over 10.", "tokens": [498, 264, 2158, 300, 311, 668, 15598, 370, 1400, 307, 670, 1266, 13], "temperature": 0.0, "avg_logprob": -0.07841940720876057, "compression_ratio": 1.7609561752988048, "no_speech_prob": 1.7777854736777954e-05}, {"id": 470, "seek": 150690, "start": 1506.9, "end": 1510.74, "text": " So here we stop.", "tokens": [407, 510, 321, 1590, 13], "temperature": 0.0, "avg_logprob": -0.16506295733981663, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.785278750001453e-06}, {"id": 471, "seek": 150690, "start": 1510.74, "end": 1514.3400000000001, "text": " Okay? What if you actually want", "tokens": [1033, 30, 708, 498, 291, 767, 528], "temperature": 0.0, "avg_logprob": -0.16506295733981663, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.785278750001453e-06}, {"id": 472, "seek": 150690, "start": 1514.3400000000001, "end": 1518.7800000000002, "text": " to change the way the calculation's being done?", "tokens": [281, 1319, 264, 636, 264, 17108, 311, 885, 1096, 30], "temperature": 0.0, "avg_logprob": -0.16506295733981663, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.785278750001453e-06}, {"id": 473, "seek": 150690, "start": 1518.7800000000002, "end": 1523.18, "text": " So we could even change the way the calculation's being done", "tokens": [407, 321, 727, 754, 1319, 264, 636, 264, 17108, 311, 885, 1096], "temperature": 0.0, "avg_logprob": -0.16506295733981663, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.785278750001453e-06}, {"id": 474, "seek": 150690, "start": 1523.18, "end": 1529.42, "text": " by taking our calculation function, putting it into a class.", "tokens": [538, 1940, 527, 17108, 2445, 11, 3372, 309, 666, 257, 1508, 13], "temperature": 0.0, "avg_logprob": -0.16506295733981663, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.785278750001453e-06}, {"id": 475, "seek": 150690, "start": 1529.42, "end": 1534.14, "text": " And so now the value that it's calculated is an attribute", "tokens": [400, 370, 586, 264, 2158, 300, 309, 311, 15598, 307, 364, 19667], "temperature": 0.0, "avg_logprob": -0.16506295733981663, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.785278750001453e-06}, {"id": 476, "seek": 153414, "start": 1534.14, "end": 1538.0200000000002, "text": " of the class, and so now we could actually do something,", "tokens": [295, 264, 1508, 11, 293, 370, 586, 321, 727, 767, 360, 746, 11], "temperature": 0.0, "avg_logprob": -0.0850958988584321, "compression_ratio": 1.684873949579832, "no_speech_prob": 2.0580375803547213e-06}, {"id": 477, "seek": 153414, "start": 1538.0200000000002, "end": 1542.94, "text": " a callback, that reaches back inside the calculator", "tokens": [257, 818, 3207, 11, 300, 14235, 646, 1854, 264, 24993], "temperature": 0.0, "avg_logprob": -0.0850958988584321, "compression_ratio": 1.684873949579832, "no_speech_prob": 2.0580375803547213e-06}, {"id": 478, "seek": 153414, "start": 1542.94, "end": 1546.3000000000002, "text": " and changes it.", "tokens": [293, 2962, 309, 13], "temperature": 0.0, "avg_logprob": -0.0850958988584321, "compression_ratio": 1.684873949579832, "no_speech_prob": 2.0580375803547213e-06}, {"id": 479, "seek": 153414, "start": 1546.3000000000002, "end": 1548.0600000000002, "text": " Right? So this is going to double the result", "tokens": [1779, 30, 407, 341, 307, 516, 281, 3834, 264, 1874], "temperature": 0.0, "avg_logprob": -0.0850958988584321, "compression_ratio": 1.684873949579832, "no_speech_prob": 2.0580375803547213e-06}, {"id": 480, "seek": 153414, "start": 1548.0600000000002, "end": 1550.0600000000002, "text": " if it's less than three.", "tokens": [498, 309, 311, 1570, 813, 1045, 13], "temperature": 0.0, "avg_logprob": -0.0850958988584321, "compression_ratio": 1.684873949579832, "no_speech_prob": 2.0580375803547213e-06}, {"id": 481, "seek": 153414, "start": 1550.0600000000002, "end": 1555.38, "text": " So if we run this, right, we now actually have to call this", "tokens": [407, 498, 321, 1190, 341, 11, 558, 11, 321, 586, 767, 362, 281, 818, 341], "temperature": 0.0, "avg_logprob": -0.0850958988584321, "compression_ratio": 1.684873949579832, "no_speech_prob": 2.0580375803547213e-06}, {"id": 482, "seek": 153414, "start": 1555.38, "end": 1556.6200000000001, "text": " because it's a class,", "tokens": [570, 309, 311, 257, 1508, 11], "temperature": 0.0, "avg_logprob": -0.0850958988584321, "compression_ratio": 1.684873949579832, "no_speech_prob": 2.0580375803547213e-06}, {"id": 483, "seek": 153414, "start": 1556.6200000000001, "end": 1559.38, "text": " but you can see it's giving a different value.", "tokens": [457, 291, 393, 536, 309, 311, 2902, 257, 819, 2158, 13], "temperature": 0.0, "avg_logprob": -0.0850958988584321, "compression_ratio": 1.684873949579832, "no_speech_prob": 2.0580375803547213e-06}, {"id": 484, "seek": 153414, "start": 1559.38, "end": 1561.46, "text": " And so we're also taking advantage of this", "tokens": [400, 370, 321, 434, 611, 1940, 5002, 295, 341], "temperature": 0.0, "avg_logprob": -0.0850958988584321, "compression_ratio": 1.684873949579832, "no_speech_prob": 2.0580375803547213e-06}, {"id": 485, "seek": 153414, "start": 1561.46, "end": 1563.5, "text": " in the callbacks that we're using.", "tokens": [294, 264, 818, 17758, 300, 321, 434, 1228, 13], "temperature": 0.0, "avg_logprob": -0.0850958988584321, "compression_ratio": 1.684873949579832, "no_speech_prob": 2.0580375803547213e-06}, {"id": 486, "seek": 156350, "start": 1563.5, "end": 1569.86, "text": " So this is kind of the ultimately flexible callback system.", "tokens": [407, 341, 307, 733, 295, 264, 6284, 11358, 818, 3207, 1185, 13], "temperature": 0.0, "avg_logprob": -0.12846239193065748, "compression_ratio": 1.511111111111111, "no_speech_prob": 1.2289085589145543e-06}, {"id": 487, "seek": 156350, "start": 1569.86, "end": 1572.86, "text": " And so you'll see in this case we actually have", "tokens": [400, 370, 291, 603, 536, 294, 341, 1389, 321, 767, 362], "temperature": 0.0, "avg_logprob": -0.12846239193065748, "compression_ratio": 1.511111111111111, "no_speech_prob": 1.2289085589145543e-06}, {"id": 488, "seek": 156350, "start": 1572.86, "end": 1580.9, "text": " to pass the calculator object to the callback.", "tokens": [281, 1320, 264, 24993, 2657, 281, 264, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.12846239193065748, "compression_ratio": 1.511111111111111, "no_speech_prob": 1.2289085589145543e-06}, {"id": 489, "seek": 156350, "start": 1580.9, "end": 1588.86, "text": " So the way we do that is we've defined a callback method here", "tokens": [407, 264, 636, 321, 360, 300, 307, 321, 600, 7642, 257, 818, 3207, 3170, 510], "temperature": 0.0, "avg_logprob": -0.12846239193065748, "compression_ratio": 1.511111111111111, "no_speech_prob": 1.2289085589145543e-06}, {"id": 490, "seek": 156350, "start": 1588.86, "end": 1592.14, "text": " which checks to see whether it's defined, and if it is,", "tokens": [597, 13834, 281, 536, 1968, 309, 311, 7642, 11, 293, 498, 309, 307, 11], "temperature": 0.0, "avg_logprob": -0.12846239193065748, "compression_ratio": 1.511111111111111, "no_speech_prob": 1.2289085589145543e-06}, {"id": 491, "seek": 159214, "start": 1592.14, "end": 1594.7, "text": " it grabs it, and then it calls it,", "tokens": [309, 30028, 309, 11, 293, 550, 309, 5498, 309, 11], "temperature": 0.0, "avg_logprob": -0.14944990291151888, "compression_ratio": 1.565934065934066, "no_speech_prob": 4.637802703655325e-06}, {"id": 492, "seek": 159214, "start": 1594.7, "end": 1597.46, "text": " passing in the calculator object itself,", "tokens": [8437, 294, 264, 24993, 2657, 2564, 11], "temperature": 0.0, "avg_logprob": -0.14944990291151888, "compression_ratio": 1.565934065934066, "no_speech_prob": 4.637802703655325e-06}, {"id": 493, "seek": 159214, "start": 1597.46, "end": 1600.42, "text": " so it's now available.", "tokens": [370, 309, 311, 586, 2435, 13], "temperature": 0.0, "avg_logprob": -0.14944990291151888, "compression_ratio": 1.565934065934066, "no_speech_prob": 4.637802703655325e-06}, {"id": 494, "seek": 159214, "start": 1600.42, "end": 1604.5800000000002, "text": " And so what we actually did last week is we didn't call this", "tokens": [400, 370, 437, 321, 767, 630, 1036, 1243, 307, 321, 994, 380, 818, 341], "temperature": 0.0, "avg_logprob": -0.14944990291151888, "compression_ratio": 1.565934065934066, "no_speech_prob": 4.637802703655325e-06}, {"id": 495, "seek": 159214, "start": 1604.5800000000002, "end": 1611.5800000000002, "text": " callback, we called this done to call, which means we were able", "tokens": [818, 3207, 11, 321, 1219, 341, 1096, 281, 818, 11, 597, 1355, 321, 645, 1075], "temperature": 0.0, "avg_logprob": -0.14944990291151888, "compression_ratio": 1.565934065934066, "no_speech_prob": 4.637802703655325e-06}, {"id": 496, "seek": 159214, "start": 1611.5800000000002, "end": 1613.9, "text": " to do it like this.", "tokens": [281, 360, 309, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.14944990291151888, "compression_ratio": 1.565934065934066, "no_speech_prob": 4.637802703655325e-06}, {"id": 497, "seek": 159214, "start": 1613.9, "end": 1620.98, "text": " Okay? Now, you know, which do you prefer?", "tokens": [1033, 30, 823, 11, 291, 458, 11, 597, 360, 291, 4382, 30], "temperature": 0.0, "avg_logprob": -0.14944990291151888, "compression_ratio": 1.565934065934066, "no_speech_prob": 4.637802703655325e-06}, {"id": 498, "seek": 162098, "start": 1620.98, "end": 1622.3, "text": " It's kind of up to you, right?", "tokens": [467, 311, 733, 295, 493, 281, 291, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11314957358620384, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.844772436423227e-06}, {"id": 499, "seek": 162098, "start": 1622.3, "end": 1624.74, "text": " I mean, we had so many callbacks being called", "tokens": [286, 914, 11, 321, 632, 370, 867, 818, 17758, 885, 1219], "temperature": 0.0, "avg_logprob": -0.11314957358620384, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.844772436423227e-06}, {"id": 500, "seek": 162098, "start": 1624.74, "end": 1629.82, "text": " that I felt the extra noise of giving it a name was a bit messy.", "tokens": [300, 286, 2762, 264, 2857, 5658, 295, 2902, 309, 257, 1315, 390, 257, 857, 16191, 13], "temperature": 0.0, "avg_logprob": -0.11314957358620384, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.844772436423227e-06}, {"id": 501, "seek": 162098, "start": 1629.82, "end": 1631.14, "text": " On the other hand, you might feel", "tokens": [1282, 264, 661, 1011, 11, 291, 1062, 841], "temperature": 0.0, "avg_logprob": -0.11314957358620384, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.844772436423227e-06}, {"id": 502, "seek": 162098, "start": 1631.14, "end": 1633.82, "text": " that calling a callback isn't something you expect done", "tokens": [300, 5141, 257, 818, 3207, 1943, 380, 746, 291, 2066, 1096], "temperature": 0.0, "avg_logprob": -0.11314957358620384, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.844772436423227e-06}, {"id": 503, "seek": 162098, "start": 1633.82, "end": 1637.42, "text": " to call to do, in which case you can do it that way.", "tokens": [281, 818, 281, 360, 11, 294, 597, 1389, 291, 393, 360, 309, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.11314957358620384, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.844772436423227e-06}, {"id": 504, "seek": 162098, "start": 1637.42, "end": 1643.18, "text": " So there's pros and cons, neither is right or wrong.", "tokens": [407, 456, 311, 6267, 293, 1014, 11, 9662, 307, 558, 420, 2085, 13], "temperature": 0.0, "avg_logprob": -0.11314957358620384, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.844772436423227e-06}, {"id": 505, "seek": 162098, "start": 1643.18, "end": 1648.58, "text": " Okay. So that's callbacks.", "tokens": [1033, 13, 407, 300, 311, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.11314957358620384, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.844772436423227e-06}, {"id": 506, "seek": 164858, "start": 1648.58, "end": 1653.1799999999998, "text": " We've been using dunder thingies a lot.", "tokens": [492, 600, 668, 1228, 274, 6617, 551, 530, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.11569022048603404, "compression_ratio": 1.6274509803921569, "no_speech_prob": 4.495059783948818e-06}, {"id": 507, "seek": 164858, "start": 1653.1799999999998, "end": 1656.1799999999998, "text": " Dunder thingies look like this,", "tokens": [413, 6617, 551, 530, 574, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.11569022048603404, "compression_ratio": 1.6274509803921569, "no_speech_prob": 4.495059783948818e-06}, {"id": 508, "seek": 164858, "start": 1656.1799999999998, "end": 1661.6599999999999, "text": " and in Python a dunder thingy is special somehow.", "tokens": [293, 294, 15329, 257, 274, 6617, 551, 88, 307, 2121, 6063, 13], "temperature": 0.0, "avg_logprob": -0.11569022048603404, "compression_ratio": 1.6274509803921569, "no_speech_prob": 4.495059783948818e-06}, {"id": 509, "seek": 164858, "start": 1661.6599999999999, "end": 1666.5, "text": " Most languages kind of let you define special behaviors.", "tokens": [4534, 8650, 733, 295, 718, 291, 6964, 2121, 15501, 13], "temperature": 0.0, "avg_logprob": -0.11569022048603404, "compression_ratio": 1.6274509803921569, "no_speech_prob": 4.495059783948818e-06}, {"id": 510, "seek": 164858, "start": 1666.5, "end": 1670.3, "text": " For example, in C++ there's an operator keyword", "tokens": [1171, 1365, 11, 294, 383, 25472, 456, 311, 364, 12973, 20428], "temperature": 0.0, "avg_logprob": -0.11569022048603404, "compression_ratio": 1.6274509803921569, "no_speech_prob": 4.495059783948818e-06}, {"id": 511, "seek": 164858, "start": 1670.3, "end": 1673.58, "text": " where if you define a function that says operator something", "tokens": [689, 498, 291, 6964, 257, 2445, 300, 1619, 12973, 746], "temperature": 0.0, "avg_logprob": -0.11569022048603404, "compression_ratio": 1.6274509803921569, "no_speech_prob": 4.495059783948818e-06}, {"id": 512, "seek": 164858, "start": 1673.58, "end": 1677.1799999999998, "text": " like plus, you're defining the plus operator.", "tokens": [411, 1804, 11, 291, 434, 17827, 264, 1804, 12973, 13], "temperature": 0.0, "avg_logprob": -0.11569022048603404, "compression_ratio": 1.6274509803921569, "no_speech_prob": 4.495059783948818e-06}, {"id": 513, "seek": 167718, "start": 1677.18, "end": 1683.42, "text": " So most languages tend to have like special magic names you can", "tokens": [407, 881, 8650, 3928, 281, 362, 411, 2121, 5585, 5288, 291, 393], "temperature": 0.0, "avg_logprob": -0.12489980061848958, "compression_ratio": 1.6453488372093024, "no_speech_prob": 4.6376680984394625e-06}, {"id": 514, "seek": 167718, "start": 1683.42, "end": 1685.42, "text": " give things that make something a constructor", "tokens": [976, 721, 300, 652, 746, 257, 47479], "temperature": 0.0, "avg_logprob": -0.12489980061848958, "compression_ratio": 1.6453488372093024, "no_speech_prob": 4.6376680984394625e-06}, {"id": 515, "seek": 167718, "start": 1685.42, "end": 1688.0600000000002, "text": " or a destructor or operator.", "tokens": [420, 257, 2677, 14535, 420, 12973, 13], "temperature": 0.0, "avg_logprob": -0.12489980061848958, "compression_ratio": 1.6453488372093024, "no_speech_prob": 4.6376680984394625e-06}, {"id": 516, "seek": 167718, "start": 1688.0600000000002, "end": 1689.94, "text": " I like in Python that all", "tokens": [286, 411, 294, 15329, 300, 439], "temperature": 0.0, "avg_logprob": -0.12489980061848958, "compression_ratio": 1.6453488372093024, "no_speech_prob": 4.6376680984394625e-06}, {"id": 517, "seek": 167718, "start": 1689.94, "end": 1693.42, "text": " of the magic names actually look magic.", "tokens": [295, 264, 5585, 5288, 767, 574, 5585, 13], "temperature": 0.0, "avg_logprob": -0.12489980061848958, "compression_ratio": 1.6453488372093024, "no_speech_prob": 4.6376680984394625e-06}, {"id": 518, "seek": 167718, "start": 1693.42, "end": 1696.1000000000001, "text": " They all look like that,", "tokens": [814, 439, 574, 411, 300, 11], "temperature": 0.0, "avg_logprob": -0.12489980061848958, "compression_ratio": 1.6453488372093024, "no_speech_prob": 4.6376680984394625e-06}, {"id": 519, "seek": 167718, "start": 1696.1000000000001, "end": 1699.02, "text": " which I think is actually a really good way to do it.", "tokens": [597, 286, 519, 307, 767, 257, 534, 665, 636, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.12489980061848958, "compression_ratio": 1.6453488372093024, "no_speech_prob": 4.6376680984394625e-06}, {"id": 520, "seek": 169902, "start": 1699.02, "end": 1707.22, "text": " So the Python docs have a data model reference", "tokens": [407, 264, 15329, 45623, 362, 257, 1412, 2316, 6408], "temperature": 0.0, "avg_logprob": -0.13202638096279568, "compression_ratio": 1.703862660944206, "no_speech_prob": 1.9947012788179563e-06}, {"id": 521, "seek": 169902, "start": 1707.22, "end": 1709.94, "text": " where they tell you about all these special method names.", "tokens": [689, 436, 980, 291, 466, 439, 613, 2121, 3170, 5288, 13], "temperature": 0.0, "avg_logprob": -0.13202638096279568, "compression_ratio": 1.703862660944206, "no_speech_prob": 1.9947012788179563e-06}, {"id": 522, "seek": 169902, "start": 1709.94, "end": 1713.02, "text": " And you can go through and you can see what are all the special", "tokens": [400, 291, 393, 352, 807, 293, 291, 393, 536, 437, 366, 439, 264, 2121], "temperature": 0.0, "avg_logprob": -0.13202638096279568, "compression_ratio": 1.703862660944206, "no_speech_prob": 1.9947012788179563e-06}, {"id": 523, "seek": 169902, "start": 1713.02, "end": 1714.66, "text": " things you can get your method to do.", "tokens": [721, 291, 393, 483, 428, 3170, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.13202638096279568, "compression_ratio": 1.703862660944206, "no_speech_prob": 1.9947012788179563e-06}, {"id": 524, "seek": 169902, "start": 1714.66, "end": 1717.3, "text": " Like you can override how it behaves with less than", "tokens": [1743, 291, 393, 42321, 577, 309, 36896, 365, 1570, 813], "temperature": 0.0, "avg_logprob": -0.13202638096279568, "compression_ratio": 1.703862660944206, "no_speech_prob": 1.9947012788179563e-06}, {"id": 525, "seek": 169902, "start": 1717.3, "end": 1721.62, "text": " or equal to or et cetera, et cetera.", "tokens": [420, 2681, 281, 420, 1030, 11458, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.13202638096279568, "compression_ratio": 1.703862660944206, "no_speech_prob": 1.9947012788179563e-06}, {"id": 526, "seek": 169902, "start": 1721.62, "end": 1723.62, "text": " There's a particular list I suggest you know,", "tokens": [821, 311, 257, 1729, 1329, 286, 3402, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.13202638096279568, "compression_ratio": 1.703862660944206, "no_speech_prob": 1.9947012788179563e-06}, {"id": 527, "seek": 169902, "start": 1723.62, "end": 1725.1399999999999, "text": " and this is the list.", "tokens": [293, 341, 307, 264, 1329, 13], "temperature": 0.0, "avg_logprob": -0.13202638096279568, "compression_ratio": 1.703862660944206, "no_speech_prob": 1.9947012788179563e-06}, {"id": 528, "seek": 169902, "start": 1725.1399999999999, "end": 1726.7, "text": " Okay. So you can go to those docs", "tokens": [1033, 13, 407, 291, 393, 352, 281, 729, 45623], "temperature": 0.0, "avg_logprob": -0.13202638096279568, "compression_ratio": 1.703862660944206, "no_speech_prob": 1.9947012788179563e-06}, {"id": 529, "seek": 172670, "start": 1726.7, "end": 1729.02, "text": " and see what these things do because we use all", "tokens": [293, 536, 437, 613, 721, 360, 570, 321, 764, 439], "temperature": 0.0, "avg_logprob": -0.13724250504464813, "compression_ratio": 1.7790697674418605, "no_speech_prob": 2.3186628823168576e-05}, {"id": 530, "seek": 172670, "start": 1729.02, "end": 1732.42, "text": " of these in this course.", "tokens": [295, 613, 294, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.13724250504464813, "compression_ratio": 1.7790697674418605, "no_speech_prob": 2.3186628823168576e-05}, {"id": 531, "seek": 172670, "start": 1732.42, "end": 1734.14, "text": " So here's an example.", "tokens": [407, 510, 311, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.13724250504464813, "compression_ratio": 1.7790697674418605, "no_speech_prob": 2.3186628823168576e-05}, {"id": 532, "seek": 172670, "start": 1734.14, "end": 1737.1000000000001, "text": " Here's a sloppy adder plus.", "tokens": [1692, 311, 257, 43684, 909, 260, 1804, 13], "temperature": 0.0, "avg_logprob": -0.13724250504464813, "compression_ratio": 1.7790697674418605, "no_speech_prob": 2.3186628823168576e-05}, {"id": 533, "seek": 172670, "start": 1737.1000000000001, "end": 1740.14, "text": " You pass in some number that you're going to add up.", "tokens": [509, 1320, 294, 512, 1230, 300, 291, 434, 516, 281, 909, 493, 13], "temperature": 0.0, "avg_logprob": -0.13724250504464813, "compression_ratio": 1.7790697674418605, "no_speech_prob": 2.3186628823168576e-05}, {"id": 534, "seek": 172670, "start": 1740.14, "end": 1742.5, "text": " And then when you add two things together,", "tokens": [400, 550, 562, 291, 909, 732, 721, 1214, 11], "temperature": 0.0, "avg_logprob": -0.13724250504464813, "compression_ratio": 1.7790697674418605, "no_speech_prob": 2.3186628823168576e-05}, {"id": 535, "seek": 172670, "start": 1742.5, "end": 1744.18, "text": " it will give you the result of adding them up,", "tokens": [309, 486, 976, 291, 264, 1874, 295, 5127, 552, 493, 11], "temperature": 0.0, "avg_logprob": -0.13724250504464813, "compression_ratio": 1.7790697674418605, "no_speech_prob": 2.3186628823168576e-05}, {"id": 536, "seek": 172670, "start": 1744.18, "end": 1746.6200000000001, "text": " but it will be wrong by.01.", "tokens": [457, 309, 486, 312, 2085, 538, 2411, 10607, 13], "temperature": 0.0, "avg_logprob": -0.13724250504464813, "compression_ratio": 1.7790697674418605, "no_speech_prob": 2.3186628823168576e-05}, {"id": 537, "seek": 172670, "start": 1746.6200000000001, "end": 1749.06, "text": " And that is called Dunder add", "tokens": [400, 300, 307, 1219, 413, 6617, 909], "temperature": 0.0, "avg_logprob": -0.13724250504464813, "compression_ratio": 1.7790697674418605, "no_speech_prob": 2.3186628823168576e-05}, {"id": 538, "seek": 172670, "start": 1749.06, "end": 1751.42, "text": " because that's what happens when you see plus.", "tokens": [570, 300, 311, 437, 2314, 562, 291, 536, 1804, 13], "temperature": 0.0, "avg_logprob": -0.13724250504464813, "compression_ratio": 1.7790697674418605, "no_speech_prob": 2.3186628823168576e-05}, {"id": 539, "seek": 172670, "start": 1751.42, "end": 1753.5, "text": " This is called Dunder init because this is what happens", "tokens": [639, 307, 1219, 413, 6617, 3157, 570, 341, 307, 437, 2314], "temperature": 0.0, "avg_logprob": -0.13724250504464813, "compression_ratio": 1.7790697674418605, "no_speech_prob": 2.3186628823168576e-05}, {"id": 540, "seek": 172670, "start": 1753.5, "end": 1755.3400000000001, "text": " when an object gets constructed.", "tokens": [562, 364, 2657, 2170, 17083, 13], "temperature": 0.0, "avg_logprob": -0.13724250504464813, "compression_ratio": 1.7790697674418605, "no_speech_prob": 2.3186628823168576e-05}, {"id": 541, "seek": 175534, "start": 1755.34, "end": 1758.06, "text": " And this is called Dunder repra because this is what gets called", "tokens": [400, 341, 307, 1219, 413, 6617, 1085, 424, 570, 341, 307, 437, 2170, 1219], "temperature": 0.0, "avg_logprob": -0.15800746736072357, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.710847861133516e-06}, {"id": 542, "seek": 175534, "start": 1758.06, "end": 1759.3799999999999, "text": " when you print it out.", "tokens": [562, 291, 4482, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.15800746736072357, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.710847861133516e-06}, {"id": 543, "seek": 175534, "start": 1759.3799999999999, "end": 1762.4599999999998, "text": " So now I can create a one adder and a two adder", "tokens": [407, 586, 286, 393, 1884, 257, 472, 909, 260, 293, 257, 732, 909, 260], "temperature": 0.0, "avg_logprob": -0.15800746736072357, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.710847861133516e-06}, {"id": 544, "seek": 175534, "start": 1762.4599999999998, "end": 1765.82, "text": " and I can plus them together and I can see the result.", "tokens": [293, 286, 393, 1804, 552, 1214, 293, 286, 393, 536, 264, 1874, 13], "temperature": 0.0, "avg_logprob": -0.15800746736072357, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.710847861133516e-06}, {"id": 545, "seek": 175534, "start": 1765.82, "end": 1767.26, "text": " Okay. So that's kind of an example", "tokens": [1033, 13, 407, 300, 311, 733, 295, 364, 1365], "temperature": 0.0, "avg_logprob": -0.15800746736072357, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.710847861133516e-06}, {"id": 546, "seek": 175534, "start": 1767.26, "end": 1774.78, "text": " of how these special Dunder methods work.", "tokens": [295, 577, 613, 2121, 413, 6617, 7150, 589, 13], "temperature": 0.0, "avg_logprob": -0.15800746736072357, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.710847861133516e-06}, {"id": 547, "seek": 175534, "start": 1774.78, "end": 1782.4599999999998, "text": " Okay. So that's a bit of that Python stuff.", "tokens": [1033, 13, 407, 300, 311, 257, 857, 295, 300, 15329, 1507, 13], "temperature": 0.0, "avg_logprob": -0.15800746736072357, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.710847861133516e-06}, {"id": 548, "seek": 175534, "start": 1782.4599999999998, "end": 1784.6599999999999, "text": " There's another bit of code stuff that I wanted", "tokens": [821, 311, 1071, 857, 295, 3089, 1507, 300, 286, 1415], "temperature": 0.0, "avg_logprob": -0.15800746736072357, "compression_ratio": 1.7014218009478672, "no_speech_prob": 4.710847861133516e-06}, {"id": 549, "seek": 178466, "start": 1784.66, "end": 1789.74, "text": " to show you which you'll need to be doing a lot of,", "tokens": [281, 855, 291, 597, 291, 603, 643, 281, 312, 884, 257, 688, 295, 11], "temperature": 0.0, "avg_logprob": -0.1381648926507859, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.3211125406087376e-05}, {"id": 550, "seek": 178466, "start": 1789.74, "end": 1792.9, "text": " which is you need to be really good at browsing source code.", "tokens": [597, 307, 291, 643, 281, 312, 534, 665, 412, 38602, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1381648926507859, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.3211125406087376e-05}, {"id": 551, "seek": 178466, "start": 1792.9, "end": 1796.98, "text": " If you're going to be contributing stuff to Fast AI", "tokens": [759, 291, 434, 516, 281, 312, 19270, 1507, 281, 15968, 7318], "temperature": 0.0, "avg_logprob": -0.1381648926507859, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.3211125406087376e-05}, {"id": 552, "seek": 178466, "start": 1796.98, "end": 1801.38, "text": " or to the Fast AI for Swift for TensorFlow", "tokens": [420, 281, 264, 15968, 7318, 337, 25539, 337, 37624], "temperature": 0.0, "avg_logprob": -0.1381648926507859, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.3211125406087376e-05}, {"id": 553, "seek": 178466, "start": 1801.38, "end": 1806.3000000000002, "text": " or just building your own more complex projects,", "tokens": [420, 445, 2390, 428, 1065, 544, 3997, 4455, 11], "temperature": 0.0, "avg_logprob": -0.1381648926507859, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.3211125406087376e-05}, {"id": 554, "seek": 178466, "start": 1806.3000000000002, "end": 1808.22, "text": " you need to be able to jump around source code.", "tokens": [291, 643, 281, 312, 1075, 281, 3012, 926, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1381648926507859, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.3211125406087376e-05}, {"id": 555, "seek": 178466, "start": 1808.22, "end": 1810.8200000000002, "text": " Or even just to find out how PyTorch does something", "tokens": [1610, 754, 445, 281, 915, 484, 577, 9953, 51, 284, 339, 775, 746], "temperature": 0.0, "avg_logprob": -0.1381648926507859, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.3211125406087376e-05}, {"id": 556, "seek": 178466, "start": 1810.8200000000002, "end": 1812.1000000000001, "text": " if you're doing some research,", "tokens": [498, 291, 434, 884, 512, 2132, 11], "temperature": 0.0, "avg_logprob": -0.1381648926507859, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.3211125406087376e-05}, {"id": 557, "seek": 181210, "start": 1812.1, "end": 1816.86, "text": " you need to really understand what's going on under the hood.", "tokens": [291, 643, 281, 534, 1223, 437, 311, 516, 322, 833, 264, 13376, 13], "temperature": 0.0, "avg_logprob": -0.10803653643681453, "compression_ratio": 1.643835616438356, "no_speech_prob": 1.4508077583741397e-05}, {"id": 558, "seek": 181210, "start": 1816.86, "end": 1818.8999999999999, "text": " This is a list of things you should know how to do", "tokens": [639, 307, 257, 1329, 295, 721, 291, 820, 458, 577, 281, 360], "temperature": 0.0, "avg_logprob": -0.10803653643681453, "compression_ratio": 1.643835616438356, "no_speech_prob": 1.4508077583741397e-05}, {"id": 559, "seek": 181210, "start": 1818.8999999999999, "end": 1820.3799999999999, "text": " in your editor of choice.", "tokens": [294, 428, 9839, 295, 3922, 13], "temperature": 0.0, "avg_logprob": -0.10803653643681453, "compression_ratio": 1.643835616438356, "no_speech_prob": 1.4508077583741397e-05}, {"id": 560, "seek": 181210, "start": 1820.3799999999999, "end": 1825.58, "text": " Any editor that can't do all of these things is worth replacing", "tokens": [2639, 9839, 300, 393, 380, 360, 439, 295, 613, 721, 307, 3163, 19139], "temperature": 0.0, "avg_logprob": -0.10803653643681453, "compression_ratio": 1.643835616438356, "no_speech_prob": 1.4508077583741397e-05}, {"id": 561, "seek": 181210, "start": 1825.58, "end": 1826.8999999999999, "text": " with one that can.", "tokens": [365, 472, 300, 393, 13], "temperature": 0.0, "avg_logprob": -0.10803653643681453, "compression_ratio": 1.643835616438356, "no_speech_prob": 1.4508077583741397e-05}, {"id": 562, "seek": 181210, "start": 1826.8999999999999, "end": 1828.2199999999998, "text": " Most editors can do these things.", "tokens": [4534, 31446, 393, 360, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.10803653643681453, "compression_ratio": 1.643835616438356, "no_speech_prob": 1.4508077583741397e-05}, {"id": 563, "seek": 181210, "start": 1828.2199999999998, "end": 1834.2199999999998, "text": " Emacs can, Visual Studio Code can, Sublime can,", "tokens": [3968, 44937, 393, 11, 23187, 13500, 15549, 393, 11, 8511, 40941, 393, 11], "temperature": 0.0, "avg_logprob": -0.10803653643681453, "compression_ratio": 1.643835616438356, "no_speech_prob": 1.4508077583741397e-05}, {"id": 564, "seek": 181210, "start": 1834.2199999999998, "end": 1838.02, "text": " and the editor I use most of the time, Vim, can as well.", "tokens": [293, 264, 9839, 286, 764, 881, 295, 264, 565, 11, 691, 332, 11, 393, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.10803653643681453, "compression_ratio": 1.643835616438356, "no_speech_prob": 1.4508077583741397e-05}, {"id": 565, "seek": 183802, "start": 1838.02, "end": 1842.1, "text": " I'll show you what these things are in Vim.", "tokens": [286, 603, 855, 291, 437, 613, 721, 366, 294, 691, 332, 13], "temperature": 0.0, "avg_logprob": -0.12690649623364475, "compression_ratio": 1.737991266375546, "no_speech_prob": 9.51482343225507e-06}, {"id": 566, "seek": 183802, "start": 1842.1, "end": 1845.22, "text": " On the forums, there are already some topics saying how", "tokens": [1282, 264, 26998, 11, 456, 366, 1217, 512, 8378, 1566, 577], "temperature": 0.0, "avg_logprob": -0.12690649623364475, "compression_ratio": 1.737991266375546, "no_speech_prob": 9.51482343225507e-06}, {"id": 567, "seek": 183802, "start": 1845.22, "end": 1847.42, "text": " to do these things in other editors.", "tokens": [281, 360, 613, 721, 294, 661, 31446, 13], "temperature": 0.0, "avg_logprob": -0.12690649623364475, "compression_ratio": 1.737991266375546, "no_speech_prob": 9.51482343225507e-06}, {"id": 568, "seek": 183802, "start": 1847.42, "end": 1849.58, "text": " If you don't find one that seems any good,", "tokens": [759, 291, 500, 380, 915, 472, 300, 2544, 604, 665, 11], "temperature": 0.0, "avg_logprob": -0.12690649623364475, "compression_ratio": 1.737991266375546, "no_speech_prob": 9.51482343225507e-06}, {"id": 569, "seek": 183802, "start": 1849.58, "end": 1852.82, "text": " feel free to create your own topic if you've got some tips", "tokens": [841, 1737, 281, 1884, 428, 1065, 4829, 498, 291, 600, 658, 512, 6082], "temperature": 0.0, "avg_logprob": -0.12690649623364475, "compression_ratio": 1.737991266375546, "no_speech_prob": 9.51482343225507e-06}, {"id": 570, "seek": 183802, "start": 1852.82, "end": 1854.98, "text": " about how to do these things or other useful things", "tokens": [466, 577, 281, 360, 613, 721, 420, 661, 4420, 721], "temperature": 0.0, "avg_logprob": -0.12690649623364475, "compression_ratio": 1.737991266375546, "no_speech_prob": 9.51482343225507e-06}, {"id": 571, "seek": 183802, "start": 1854.98, "end": 1856.46, "text": " in your editor of choice.", "tokens": [294, 428, 9839, 295, 3922, 13], "temperature": 0.0, "avg_logprob": -0.12690649623364475, "compression_ratio": 1.737991266375546, "no_speech_prob": 9.51482343225507e-06}, {"id": 572, "seek": 183802, "start": 1856.46, "end": 1861.62, "text": " So I'm going to show you in Vim for no particular reason,", "tokens": [407, 286, 478, 516, 281, 855, 291, 294, 691, 332, 337, 572, 1729, 1778, 11], "temperature": 0.0, "avg_logprob": -0.12690649623364475, "compression_ratio": 1.737991266375546, "no_speech_prob": 9.51482343225507e-06}, {"id": 573, "seek": 183802, "start": 1861.62, "end": 1864.22, "text": " just because I use Vim.", "tokens": [445, 570, 286, 764, 691, 332, 13], "temperature": 0.0, "avg_logprob": -0.12690649623364475, "compression_ratio": 1.737991266375546, "no_speech_prob": 9.51482343225507e-06}, {"id": 574, "seek": 186422, "start": 1864.22, "end": 1868.06, "text": " So here's my editor, it's called Vim.", "tokens": [407, 510, 311, 452, 9839, 11, 309, 311, 1219, 691, 332, 13], "temperature": 0.0, "avg_logprob": -0.11865323539671858, "compression_ratio": 1.6482213438735178, "no_speech_prob": 1.6185478671104647e-05}, {"id": 575, "seek": 186422, "start": 1868.06, "end": 1871.78, "text": " One of the things I like about Vim is I can use it in a terminal,", "tokens": [1485, 295, 264, 721, 286, 411, 466, 691, 332, 307, 286, 393, 764, 309, 294, 257, 14709, 11], "temperature": 0.0, "avg_logprob": -0.11865323539671858, "compression_ratio": 1.6482213438735178, "no_speech_prob": 1.6185478671104647e-05}, {"id": 576, "seek": 186422, "start": 1871.78, "end": 1874.22, "text": " which I find super helpful because I'm working", "tokens": [597, 286, 915, 1687, 4961, 570, 286, 478, 1364], "temperature": 0.0, "avg_logprob": -0.11865323539671858, "compression_ratio": 1.6482213438735178, "no_speech_prob": 1.6185478671104647e-05}, {"id": 577, "seek": 186422, "start": 1874.22, "end": 1876.66, "text": " on remote machines all the time, and I would like to be", "tokens": [322, 8607, 8379, 439, 264, 565, 11, 293, 286, 576, 411, 281, 312], "temperature": 0.0, "avg_logprob": -0.11865323539671858, "compression_ratio": 1.6482213438735178, "no_speech_prob": 1.6185478671104647e-05}, {"id": 578, "seek": 186422, "start": 1876.66, "end": 1878.9, "text": " as at least as productive in a terminal", "tokens": [382, 412, 1935, 382, 13304, 294, 257, 14709], "temperature": 0.0, "avg_logprob": -0.11865323539671858, "compression_ratio": 1.6482213438735178, "no_speech_prob": 1.6185478671104647e-05}, {"id": 579, "seek": 186422, "start": 1878.9, "end": 1882.26, "text": " as I am on my local computer.", "tokens": [382, 286, 669, 322, 452, 2654, 3820, 13], "temperature": 0.0, "avg_logprob": -0.11865323539671858, "compression_ratio": 1.6482213438735178, "no_speech_prob": 1.6185478671104647e-05}, {"id": 580, "seek": 186422, "start": 1882.26, "end": 1885.74, "text": " And so the first thing you should be able", "tokens": [400, 370, 264, 700, 551, 291, 820, 312, 1075], "temperature": 0.0, "avg_logprob": -0.11865323539671858, "compression_ratio": 1.6482213438735178, "no_speech_prob": 1.6185478671104647e-05}, {"id": 581, "seek": 186422, "start": 1885.74, "end": 1888.38, "text": " to do is to jump to a symbol.", "tokens": [281, 360, 307, 281, 3012, 281, 257, 5986, 13], "temperature": 0.0, "avg_logprob": -0.11865323539671858, "compression_ratio": 1.6482213438735178, "no_speech_prob": 1.6185478671104647e-05}, {"id": 582, "seek": 186422, "start": 1888.38, "end": 1891.74, "text": " A symbol would be like a class or a function", "tokens": [316, 5986, 576, 312, 411, 257, 1508, 420, 257, 2445], "temperature": 0.0, "avg_logprob": -0.11865323539671858, "compression_ratio": 1.6482213438735178, "no_speech_prob": 1.6185478671104647e-05}, {"id": 583, "seek": 186422, "start": 1891.74, "end": 1893.06, "text": " or something like that.", "tokens": [420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.11865323539671858, "compression_ratio": 1.6482213438735178, "no_speech_prob": 1.6185478671104647e-05}, {"id": 584, "seek": 189306, "start": 1893.06, "end": 1901.4199999999998, "text": " So for example, I might want to be able to jump straight", "tokens": [407, 337, 1365, 11, 286, 1062, 528, 281, 312, 1075, 281, 3012, 2997], "temperature": 0.0, "avg_logprob": -0.17890567779541017, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6184974811039865e-05}, {"id": 585, "seek": 189306, "start": 1901.4199999999998, "end": 1907.5, "text": " to the definition of create CNN, but I can't quite remember the name", "tokens": [281, 264, 7123, 295, 1884, 24859, 11, 457, 286, 393, 380, 1596, 1604, 264, 1315], "temperature": 0.0, "avg_logprob": -0.17890567779541017, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6184974811039865e-05}, {"id": 586, "seek": 189306, "start": 1907.5, "end": 1912.82, "text": " of the function create CNN, so I would go colon tag create.", "tokens": [295, 264, 2445, 1884, 24859, 11, 370, 286, 576, 352, 8255, 6162, 1884, 13], "temperature": 0.0, "avg_logprob": -0.17890567779541017, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6184974811039865e-05}, {"id": 587, "seek": 189306, "start": 1912.82, "end": 1915.1, "text": " I'm pretty sure it's create underscore something,", "tokens": [286, 478, 1238, 988, 309, 311, 1884, 37556, 746, 11], "temperature": 0.0, "avg_logprob": -0.17890567779541017, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6184974811039865e-05}, {"id": 588, "seek": 189306, "start": 1915.1, "end": 1918.62, "text": " so then I'd press tab a few times, and it would loop through.", "tokens": [370, 550, 286, 1116, 1886, 4421, 257, 1326, 1413, 11, 293, 309, 576, 6367, 807, 13], "temperature": 0.0, "avg_logprob": -0.17890567779541017, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6184974811039865e-05}, {"id": 589, "seek": 189306, "start": 1918.62, "end": 1920.62, "text": " Ah, there it is, create CNN.", "tokens": [2438, 11, 456, 309, 307, 11, 1884, 24859, 13], "temperature": 0.0, "avg_logprob": -0.17890567779541017, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6184974811039865e-05}, {"id": 590, "seek": 189306, "start": 1920.62, "end": 1922.1799999999998, "text": " And then I'd hit enter.", "tokens": [400, 550, 286, 1116, 2045, 3242, 13], "temperature": 0.0, "avg_logprob": -0.17890567779541017, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.6184974811039865e-05}, {"id": 591, "seek": 192218, "start": 1922.18, "end": 1926.14, "text": " So that's the first thing that your editor should do is it should make it easy", "tokens": [407, 300, 311, 264, 700, 551, 300, 428, 9839, 820, 360, 307, 309, 820, 652, 309, 1858], "temperature": 0.0, "avg_logprob": -0.12078339306276235, "compression_ratio": 1.8272058823529411, "no_speech_prob": 4.069028000230901e-05}, {"id": 592, "seek": 192218, "start": 1926.14, "end": 1929.7, "text": " to jump to a tag even if you can't remember exactly what it is.", "tokens": [281, 3012, 281, 257, 6162, 754, 498, 291, 393, 380, 1604, 2293, 437, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.12078339306276235, "compression_ratio": 1.8272058823529411, "no_speech_prob": 4.069028000230901e-05}, {"id": 593, "seek": 192218, "start": 1929.7, "end": 1932.98, "text": " The second thing it should do is that you should be able to click", "tokens": [440, 1150, 551, 309, 820, 360, 307, 300, 291, 820, 312, 1075, 281, 2052], "temperature": 0.0, "avg_logprob": -0.12078339306276235, "compression_ratio": 1.8272058823529411, "no_speech_prob": 4.069028000230901e-05}, {"id": 594, "seek": 192218, "start": 1932.98, "end": 1936.26, "text": " on something like CNN learner and hit a button,", "tokens": [322, 746, 411, 24859, 33347, 293, 2045, 257, 2960, 11], "temperature": 0.0, "avg_logprob": -0.12078339306276235, "compression_ratio": 1.8272058823529411, "no_speech_prob": 4.069028000230901e-05}, {"id": 595, "seek": 192218, "start": 1936.26, "end": 1939.18, "text": " which in Vim's case is control right square bracket,", "tokens": [597, 294, 691, 332, 311, 1389, 307, 1969, 558, 3732, 16904, 11], "temperature": 0.0, "avg_logprob": -0.12078339306276235, "compression_ratio": 1.8272058823529411, "no_speech_prob": 4.069028000230901e-05}, {"id": 596, "seek": 192218, "start": 1939.18, "end": 1941.9, "text": " and it should take you to the definition of that thing.", "tokens": [293, 309, 820, 747, 291, 281, 264, 7123, 295, 300, 551, 13], "temperature": 0.0, "avg_logprob": -0.12078339306276235, "compression_ratio": 1.8272058823529411, "no_speech_prob": 4.069028000230901e-05}, {"id": 597, "seek": 192218, "start": 1941.9, "end": 1945.5, "text": " So okay, that's great, there's CNN learner.", "tokens": [407, 1392, 11, 300, 311, 869, 11, 456, 311, 24859, 33347, 13], "temperature": 0.0, "avg_logprob": -0.12078339306276235, "compression_ratio": 1.8272058823529411, "no_speech_prob": 4.069028000230901e-05}, {"id": 598, "seek": 192218, "start": 1945.5, "end": 1948.54, "text": " What's this thing called, a data bunch, right square bracket.", "tokens": [708, 311, 341, 551, 1219, 11, 257, 1412, 3840, 11, 558, 3732, 16904, 13], "temperature": 0.0, "avg_logprob": -0.12078339306276235, "compression_ratio": 1.8272058823529411, "no_speech_prob": 4.069028000230901e-05}, {"id": 599, "seek": 192218, "start": 1948.54, "end": 1950.6200000000001, "text": " Okay, there's data bunch.", "tokens": [1033, 11, 456, 311, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.12078339306276235, "compression_ratio": 1.8272058823529411, "no_speech_prob": 4.069028000230901e-05}, {"id": 600, "seek": 195062, "start": 1950.62, "end": 1956.78, "text": " You'll also see that my Vim is folding things, classes and functions", "tokens": [509, 603, 611, 536, 300, 452, 691, 332, 307, 25335, 721, 11, 5359, 293, 6828], "temperature": 0.0, "avg_logprob": -0.11937420246964794, "compression_ratio": 1.636734693877551, "no_speech_prob": 8.267125849670265e-06}, {"id": 601, "seek": 195062, "start": 1956.78, "end": 1960.7399999999998, "text": " to make it easier for me to see exactly what's in this file.", "tokens": [281, 652, 309, 3571, 337, 385, 281, 536, 2293, 437, 311, 294, 341, 3991, 13], "temperature": 0.0, "avg_logprob": -0.11937420246964794, "compression_ratio": 1.636734693877551, "no_speech_prob": 8.267125849670265e-06}, {"id": 602, "seek": 195062, "start": 1960.7399999999998, "end": 1963.1399999999999, "text": " In some editors, this is called outlining.", "tokens": [682, 512, 31446, 11, 341, 307, 1219, 484, 31079, 13], "temperature": 0.0, "avg_logprob": -0.11937420246964794, "compression_ratio": 1.636734693877551, "no_speech_prob": 8.267125849670265e-06}, {"id": 603, "seek": 195062, "start": 1963.1399999999999, "end": 1964.86, "text": " In some, it's called folding.", "tokens": [682, 512, 11, 309, 311, 1219, 25335, 13], "temperature": 0.0, "avg_logprob": -0.11937420246964794, "compression_ratio": 1.636734693877551, "no_speech_prob": 8.267125849670265e-06}, {"id": 604, "seek": 195062, "start": 1964.86, "end": 1966.82, "text": " Most editors should do this.", "tokens": [4534, 31446, 820, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.11937420246964794, "compression_ratio": 1.636734693877551, "no_speech_prob": 8.267125849670265e-06}, {"id": 605, "seek": 195062, "start": 1966.82, "end": 1972.7399999999998, "text": " Then there should be a way to go back to where you were before.", "tokens": [1396, 456, 820, 312, 257, 636, 281, 352, 646, 281, 689, 291, 645, 949, 13], "temperature": 0.0, "avg_logprob": -0.11937420246964794, "compression_ratio": 1.636734693877551, "no_speech_prob": 8.267125849670265e-06}, {"id": 606, "seek": 195062, "start": 1972.7399999999998, "end": 1976.1399999999999, "text": " In Vim, that's control T for going back up the tag stack.", "tokens": [682, 691, 332, 11, 300, 311, 1969, 314, 337, 516, 646, 493, 264, 6162, 8630, 13], "temperature": 0.0, "avg_logprob": -0.11937420246964794, "compression_ratio": 1.636734693877551, "no_speech_prob": 8.267125849670265e-06}, {"id": 607, "seek": 195062, "start": 1976.1399999999999, "end": 1978.34, "text": " So here's my CNN learner.", "tokens": [407, 510, 311, 452, 24859, 33347, 13], "temperature": 0.0, "avg_logprob": -0.11937420246964794, "compression_ratio": 1.636734693877551, "no_speech_prob": 8.267125849670265e-06}, {"id": 608, "seek": 195062, "start": 1978.34, "end": 1980.34, "text": " Here's my create CNN.", "tokens": [1692, 311, 452, 1884, 24859, 13], "temperature": 0.0, "avg_logprob": -0.11937420246964794, "compression_ratio": 1.636734693877551, "no_speech_prob": 8.267125849670265e-06}, {"id": 609, "seek": 198034, "start": 1980.34, "end": 1984.06, "text": " And so you can see in this way, it makes it nice and easy to kind", "tokens": [400, 370, 291, 393, 536, 294, 341, 636, 11, 309, 1669, 309, 1481, 293, 1858, 281, 733], "temperature": 0.0, "avg_logprob": -0.11759329206160916, "compression_ratio": 1.6285714285714286, "no_speech_prob": 7.2960779107233975e-06}, {"id": 610, "seek": 198034, "start": 1984.06, "end": 1987.9399999999998, "text": " of jump around a little bit.", "tokens": [295, 3012, 926, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.11759329206160916, "compression_ratio": 1.6285714285714286, "no_speech_prob": 7.2960779107233975e-06}, {"id": 611, "seek": 198034, "start": 1987.9399999999998, "end": 1992.1399999999999, "text": " Something I find super helpful is to also be able to jump", "tokens": [6595, 286, 915, 1687, 4961, 307, 281, 611, 312, 1075, 281, 3012], "temperature": 0.0, "avg_logprob": -0.11759329206160916, "compression_ratio": 1.6285714285714286, "no_speech_prob": 7.2960779107233975e-06}, {"id": 612, "seek": 198034, "start": 1992.1399999999999, "end": 1994.3799999999999, "text": " into the source code of libraries I'm using.", "tokens": [666, 264, 4009, 3089, 295, 15148, 286, 478, 1228, 13], "temperature": 0.0, "avg_logprob": -0.11759329206160916, "compression_ratio": 1.6285714285714286, "no_speech_prob": 7.2960779107233975e-06}, {"id": 613, "seek": 198034, "start": 1994.3799999999999, "end": 1996.54, "text": " So for example, here's chiming normal.", "tokens": [407, 337, 1365, 11, 510, 311, 417, 332, 278, 2710, 13], "temperature": 0.0, "avg_logprob": -0.11759329206160916, "compression_ratio": 1.6285714285714286, "no_speech_prob": 7.2960779107233975e-06}, {"id": 614, "seek": 198034, "start": 1996.54, "end": 1998.6599999999999, "text": " So I've got my Vim configured,", "tokens": [407, 286, 600, 658, 452, 691, 332, 30538, 11], "temperature": 0.0, "avg_logprob": -0.11759329206160916, "compression_ratio": 1.6285714285714286, "no_speech_prob": 7.2960779107233975e-06}, {"id": 615, "seek": 198034, "start": 1998.6599999999999, "end": 2002.3799999999999, "text": " so if I hit control right square bracket on that, it takes me", "tokens": [370, 498, 286, 2045, 1969, 558, 3732, 16904, 322, 300, 11, 309, 2516, 385], "temperature": 0.0, "avg_logprob": -0.11759329206160916, "compression_ratio": 1.6285714285714286, "no_speech_prob": 7.2960779107233975e-06}, {"id": 616, "seek": 198034, "start": 2002.3799999999999, "end": 2006.4599999999998, "text": " to the definition of chiming normal in the PyTorch source code.", "tokens": [281, 264, 7123, 295, 417, 332, 278, 2710, 294, 264, 9953, 51, 284, 339, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.11759329206160916, "compression_ratio": 1.6285714285714286, "no_speech_prob": 7.2960779107233975e-06}, {"id": 617, "seek": 198034, "start": 2006.4599999999998, "end": 2009.1799999999998, "text": " And I find doc strings kind of annoying, so I have mine folded", "tokens": [400, 286, 915, 3211, 13985, 733, 295, 11304, 11, 370, 286, 362, 3892, 23940], "temperature": 0.0, "avg_logprob": -0.11759329206160916, "compression_ratio": 1.6285714285714286, "no_speech_prob": 7.2960779107233975e-06}, {"id": 618, "seek": 200918, "start": 2009.18, "end": 2012.54, "text": " up by default, but I can always open them up.", "tokens": [493, 538, 7576, 11, 457, 286, 393, 1009, 1269, 552, 493, 13], "temperature": 0.0, "avg_logprob": -0.12135277172126392, "compression_ratio": 1.5198237885462555, "no_speech_prob": 5.953957042947877e-06}, {"id": 619, "seek": 200918, "start": 2012.54, "end": 2022.5, "text": " If you use Vim, the way to do that is to add additional tags", "tokens": [759, 291, 764, 691, 332, 11, 264, 636, 281, 360, 300, 307, 281, 909, 4497, 18632], "temperature": 0.0, "avg_logprob": -0.12135277172126392, "compression_ratio": 1.5198237885462555, "no_speech_prob": 5.953957042947877e-06}, {"id": 620, "seek": 200918, "start": 2022.5, "end": 2024.8600000000001, "text": " for any packages that you want to be able to jump to.", "tokens": [337, 604, 17401, 300, 291, 528, 281, 312, 1075, 281, 3012, 281, 13], "temperature": 0.0, "avg_logprob": -0.12135277172126392, "compression_ratio": 1.5198237885462555, "no_speech_prob": 5.953957042947877e-06}, {"id": 621, "seek": 200918, "start": 2024.8600000000001, "end": 2028.54, "text": " I'm sure most editors will do something pretty similar.", "tokens": [286, 478, 988, 881, 31446, 486, 360, 746, 1238, 2531, 13], "temperature": 0.0, "avg_logprob": -0.12135277172126392, "compression_ratio": 1.5198237885462555, "no_speech_prob": 5.953957042947877e-06}, {"id": 622, "seek": 200918, "start": 2028.54, "end": 2030.54, "text": " Now that I've seen how chiming normal works,", "tokens": [823, 300, 286, 600, 1612, 577, 417, 332, 278, 2710, 1985, 11], "temperature": 0.0, "avg_logprob": -0.12135277172126392, "compression_ratio": 1.5198237885462555, "no_speech_prob": 5.953957042947877e-06}, {"id": 623, "seek": 200918, "start": 2030.54, "end": 2033.8200000000002, "text": " I can use the same control T to jump back to where I was", "tokens": [286, 393, 764, 264, 912, 1969, 314, 281, 3012, 646, 281, 689, 286, 390], "temperature": 0.0, "avg_logprob": -0.12135277172126392, "compression_ratio": 1.5198237885462555, "no_speech_prob": 5.953957042947877e-06}, {"id": 624, "seek": 200918, "start": 2033.8200000000002, "end": 2036.3400000000001, "text": " in my fast AI source code.", "tokens": [294, 452, 2370, 7318, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.12135277172126392, "compression_ratio": 1.5198237885462555, "no_speech_prob": 5.953957042947877e-06}, {"id": 625, "seek": 203634, "start": 2036.34, "end": 2042.78, "text": " Then the only other thing that's particularly important to know how", "tokens": [1396, 264, 787, 661, 551, 300, 311, 4098, 1021, 281, 458, 577], "temperature": 0.0, "avg_logprob": -0.15472039674457752, "compression_ratio": 1.6172248803827751, "no_speech_prob": 2.5462641133344732e-05}, {"id": 626, "seek": 203634, "start": 2042.78, "end": 2045.3, "text": " to do is to just do more general searches.", "tokens": [281, 360, 307, 281, 445, 360, 544, 2674, 26701, 13], "temperature": 0.0, "avg_logprob": -0.15472039674457752, "compression_ratio": 1.6172248803827751, "no_speech_prob": 2.5462641133344732e-05}, {"id": 627, "seek": 203634, "start": 2045.3, "end": 2050.1, "text": " So let's say I wanted to find all the places that I've used lambda,", "tokens": [407, 718, 311, 584, 286, 1415, 281, 915, 439, 264, 3190, 300, 286, 600, 1143, 13607, 11], "temperature": 0.0, "avg_logprob": -0.15472039674457752, "compression_ratio": 1.6172248803827751, "no_speech_prob": 2.5462641133344732e-05}, {"id": 628, "seek": 203634, "start": 2050.1, "end": 2051.86, "text": " since we talked about lambda today.", "tokens": [1670, 321, 2825, 466, 13607, 965, 13], "temperature": 0.0, "avg_logprob": -0.15472039674457752, "compression_ratio": 1.6172248803827751, "no_speech_prob": 2.5462641133344732e-05}, {"id": 629, "seek": 203634, "start": 2051.86, "end": 2055.54, "text": " I have a particular thing I use called ACK.", "tokens": [286, 362, 257, 1729, 551, 286, 764, 1219, 8157, 42, 13], "temperature": 0.0, "avg_logprob": -0.15472039674457752, "compression_ratio": 1.6172248803827751, "no_speech_prob": 2.5462641133344732e-05}, {"id": 630, "seek": 203634, "start": 2055.54, "end": 2060.66, "text": " I can say ACK lambda, and here is a list of all", "tokens": [286, 393, 584, 8157, 42, 13607, 11, 293, 510, 307, 257, 1329, 295, 439], "temperature": 0.0, "avg_logprob": -0.15472039674457752, "compression_ratio": 1.6172248803827751, "no_speech_prob": 2.5462641133344732e-05}, {"id": 631, "seek": 203634, "start": 2060.66, "end": 2063.2999999999997, "text": " of the places I've used lambda.", "tokens": [295, 264, 3190, 286, 600, 1143, 13607, 13], "temperature": 0.0, "avg_logprob": -0.15472039674457752, "compression_ratio": 1.6172248803827751, "no_speech_prob": 2.5462641133344732e-05}, {"id": 632, "seek": 206330, "start": 2063.3, "end": 2070.5800000000004, "text": " And I could click on one, and it will jump to the code where it's used.", "tokens": [400, 286, 727, 2052, 322, 472, 11, 293, 309, 486, 3012, 281, 264, 3089, 689, 309, 311, 1143, 13], "temperature": 0.0, "avg_logprob": -0.07936979719429962, "compression_ratio": 1.6392857142857142, "no_speech_prob": 1.300551048188936e-05}, {"id": 633, "seek": 206330, "start": 2070.5800000000004, "end": 2073.82, "text": " Again, most editors should do something like that for you.", "tokens": [3764, 11, 881, 31446, 820, 360, 746, 411, 300, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.07936979719429962, "compression_ratio": 1.6392857142857142, "no_speech_prob": 1.300551048188936e-05}, {"id": 634, "seek": 206330, "start": 2073.82, "end": 2076.7400000000002, "text": " So I find with that basic set of stuff,", "tokens": [407, 286, 915, 365, 300, 3875, 992, 295, 1507, 11], "temperature": 0.0, "avg_logprob": -0.07936979719429962, "compression_ratio": 1.6392857142857142, "no_speech_prob": 1.300551048188936e-05}, {"id": 635, "seek": 206330, "start": 2076.7400000000002, "end": 2078.34, "text": " you should be able to get around pretty well.", "tokens": [291, 820, 312, 1075, 281, 483, 926, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.07936979719429962, "compression_ratio": 1.6392857142857142, "no_speech_prob": 1.300551048188936e-05}, {"id": 636, "seek": 206330, "start": 2078.34, "end": 2080.1800000000003, "text": " If you're a professional software engineer,", "tokens": [759, 291, 434, 257, 4843, 4722, 11403, 11], "temperature": 0.0, "avg_logprob": -0.07936979719429962, "compression_ratio": 1.6392857142857142, "no_speech_prob": 1.300551048188936e-05}, {"id": 637, "seek": 206330, "start": 2080.1800000000003, "end": 2081.94, "text": " I know you know all this.", "tokens": [286, 458, 291, 458, 439, 341, 13], "temperature": 0.0, "avg_logprob": -0.07936979719429962, "compression_ratio": 1.6392857142857142, "no_speech_prob": 1.300551048188936e-05}, {"id": 638, "seek": 206330, "start": 2081.94, "end": 2084.26, "text": " If you're not, hopefully you're feeling pretty excited right now", "tokens": [759, 291, 434, 406, 11, 4696, 291, 434, 2633, 1238, 2919, 558, 586], "temperature": 0.0, "avg_logprob": -0.07936979719429962, "compression_ratio": 1.6392857142857142, "no_speech_prob": 1.300551048188936e-05}, {"id": 639, "seek": 206330, "start": 2084.26, "end": 2087.3, "text": " to discover that editors can do more than you realized.", "tokens": [281, 4411, 300, 31446, 393, 360, 544, 813, 291, 5334, 13], "temperature": 0.0, "avg_logprob": -0.07936979719429962, "compression_ratio": 1.6392857142857142, "no_speech_prob": 1.300551048188936e-05}, {"id": 640, "seek": 206330, "start": 2087.3, "end": 2091.46, "text": " And so sometimes people jump on our GitHub and say,", "tokens": [400, 370, 2171, 561, 3012, 322, 527, 23331, 293, 584, 11], "temperature": 0.0, "avg_logprob": -0.07936979719429962, "compression_ratio": 1.6392857142857142, "no_speech_prob": 1.300551048188936e-05}, {"id": 641, "seek": 209146, "start": 2091.46, "end": 2095.58, "text": " I don't know how to find out what a function is that you're calling,", "tokens": [286, 500, 380, 458, 577, 281, 915, 484, 437, 257, 2445, 307, 300, 291, 434, 5141, 11], "temperature": 0.0, "avg_logprob": -0.11199282165756799, "compression_ratio": 1.7406143344709897, "no_speech_prob": 4.197850284981541e-05}, {"id": 642, "seek": 209146, "start": 2095.58, "end": 2099.02, "text": " because you don't list all your imports at the top of the screen.", "tokens": [570, 291, 500, 380, 1329, 439, 428, 41596, 412, 264, 1192, 295, 264, 2568, 13], "temperature": 0.0, "avg_logprob": -0.11199282165756799, "compression_ratio": 1.7406143344709897, "no_speech_prob": 4.197850284981541e-05}, {"id": 643, "seek": 209146, "start": 2099.02, "end": 2103.78, "text": " This is a great place where you should be using your editor to tell you.", "tokens": [639, 307, 257, 869, 1081, 689, 291, 820, 312, 1228, 428, 9839, 281, 980, 291, 13], "temperature": 0.0, "avg_logprob": -0.11199282165756799, "compression_ratio": 1.7406143344709897, "no_speech_prob": 4.197850284981541e-05}, {"id": 644, "seek": 209146, "start": 2103.78, "end": 2107.58, "text": " And in fact, one place that GUI editors can be pretty good is often", "tokens": [400, 294, 1186, 11, 472, 1081, 300, 17917, 40, 31446, 393, 312, 1238, 665, 307, 2049], "temperature": 0.0, "avg_logprob": -0.11199282165756799, "compression_ratio": 1.7406143344709897, "no_speech_prob": 4.197850284981541e-05}, {"id": 645, "seek": 209146, "start": 2107.58, "end": 2110.26, "text": " if you actually just point at something, they will pop", "tokens": [498, 291, 767, 445, 935, 412, 746, 11, 436, 486, 1665], "temperature": 0.0, "avg_logprob": -0.11199282165756799, "compression_ratio": 1.7406143344709897, "no_speech_prob": 4.197850284981541e-05}, {"id": 646, "seek": 209146, "start": 2110.26, "end": 2113.9, "text": " up something saying exactly where is that symbol coming from.", "tokens": [493, 746, 1566, 2293, 689, 307, 300, 5986, 1348, 490, 13], "temperature": 0.0, "avg_logprob": -0.11199282165756799, "compression_ratio": 1.7406143344709897, "no_speech_prob": 4.197850284981541e-05}, {"id": 647, "seek": 209146, "start": 2113.9, "end": 2116.02, "text": " I don't have that set up in Vim, so I just have", "tokens": [286, 500, 380, 362, 300, 992, 493, 294, 691, 332, 11, 370, 286, 445, 362], "temperature": 0.0, "avg_logprob": -0.11199282165756799, "compression_ratio": 1.7406143344709897, "no_speech_prob": 4.197850284981541e-05}, {"id": 648, "seek": 209146, "start": 2116.02, "end": 2120.62, "text": " to hit the right square bracket to see where something's coming from.", "tokens": [281, 2045, 264, 558, 3732, 16904, 281, 536, 689, 746, 311, 1348, 490, 13], "temperature": 0.0, "avg_logprob": -0.11199282165756799, "compression_ratio": 1.7406143344709897, "no_speech_prob": 4.197850284981541e-05}, {"id": 649, "seek": 212062, "start": 2120.62, "end": 2127.14, "text": " Okay, so that's some tips about stuff that you should be able to do", "tokens": [1033, 11, 370, 300, 311, 512, 6082, 466, 1507, 300, 291, 820, 312, 1075, 281, 360], "temperature": 0.0, "avg_logprob": -0.12844757522855485, "compression_ratio": 1.626984126984127, "no_speech_prob": 1.2804513062292244e-05}, {"id": 650, "seek": 212062, "start": 2127.14, "end": 2128.46, "text": " when you're browsing source code.", "tokens": [562, 291, 434, 38602, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.12844757522855485, "compression_ratio": 1.626984126984127, "no_speech_prob": 1.2804513062292244e-05}, {"id": 651, "seek": 212062, "start": 2128.46, "end": 2130.38, "text": " And if you don't know how to do it yet,", "tokens": [400, 498, 291, 500, 380, 458, 577, 281, 360, 309, 1939, 11], "temperature": 0.0, "avg_logprob": -0.12844757522855485, "compression_ratio": 1.626984126984127, "no_speech_prob": 1.2804513062292244e-05}, {"id": 652, "seek": 212062, "start": 2130.38, "end": 2133.98, "text": " please Google or look at the forums and practice.", "tokens": [1767, 3329, 420, 574, 412, 264, 26998, 293, 3124, 13], "temperature": 0.0, "avg_logprob": -0.12844757522855485, "compression_ratio": 1.626984126984127, "no_speech_prob": 1.2804513062292244e-05}, {"id": 653, "seek": 212062, "start": 2133.98, "end": 2138.02, "text": " Something else we were looking at a lot last week and you need", "tokens": [6595, 1646, 321, 645, 1237, 412, 257, 688, 1036, 1243, 293, 291, 643], "temperature": 0.0, "avg_logprob": -0.12844757522855485, "compression_ratio": 1.626984126984127, "no_speech_prob": 1.2804513062292244e-05}, {"id": 654, "seek": 212062, "start": 2138.02, "end": 2140.8199999999997, "text": " to know pretty well is variance.", "tokens": [281, 458, 1238, 731, 307, 21977, 13], "temperature": 0.0, "avg_logprob": -0.12844757522855485, "compression_ratio": 1.626984126984127, "no_speech_prob": 1.2804513062292244e-05}, {"id": 655, "seek": 212062, "start": 2140.8199999999997, "end": 2144.3399999999997, "text": " So just a quick refresher on what variance is, or for those of you", "tokens": [407, 445, 257, 1702, 17368, 511, 322, 437, 21977, 307, 11, 420, 337, 729, 295, 291], "temperature": 0.0, "avg_logprob": -0.12844757522855485, "compression_ratio": 1.626984126984127, "no_speech_prob": 1.2804513062292244e-05}, {"id": 656, "seek": 212062, "start": 2144.3399999999997, "end": 2148.46, "text": " who haven't studied it before, here's what variance is.", "tokens": [567, 2378, 380, 9454, 309, 949, 11, 510, 311, 437, 21977, 307, 13], "temperature": 0.0, "avg_logprob": -0.12844757522855485, "compression_ratio": 1.626984126984127, "no_speech_prob": 1.2804513062292244e-05}, {"id": 657, "seek": 214846, "start": 2148.46, "end": 2153.1, "text": " Variance is the average of how far away each data point is from the mean.", "tokens": [32511, 719, 307, 264, 4274, 295, 577, 1400, 1314, 1184, 1412, 935, 307, 490, 264, 914, 13], "temperature": 0.0, "avg_logprob": -0.14395586649576822, "compression_ratio": 1.8709677419354838, "no_speech_prob": 1.2218561096233316e-05}, {"id": 658, "seek": 214846, "start": 2153.1, "end": 2157.82, "text": " So here's some data, right, and here's the mean of that data.", "tokens": [407, 510, 311, 512, 1412, 11, 558, 11, 293, 510, 311, 264, 914, 295, 300, 1412, 13], "temperature": 0.0, "avg_logprob": -0.14395586649576822, "compression_ratio": 1.8709677419354838, "no_speech_prob": 1.2218561096233316e-05}, {"id": 659, "seek": 214846, "start": 2157.82, "end": 2163.82, "text": " And so the average distance for each data point from the mean is T,", "tokens": [400, 370, 264, 4274, 4560, 337, 1184, 1412, 935, 490, 264, 914, 307, 314, 11], "temperature": 0.0, "avg_logprob": -0.14395586649576822, "compression_ratio": 1.8709677419354838, "no_speech_prob": 1.2218561096233316e-05}, {"id": 660, "seek": 214846, "start": 2163.82, "end": 2167.14, "text": " the data points, minus M, top mean.", "tokens": [264, 1412, 2793, 11, 3175, 376, 11, 1192, 914, 13], "temperature": 0.0, "avg_logprob": -0.14395586649576822, "compression_ratio": 1.8709677419354838, "no_speech_prob": 1.2218561096233316e-05}, {"id": 661, "seek": 214846, "start": 2167.14, "end": 2168.94, "text": " Oh, that's zero.", "tokens": [876, 11, 300, 311, 4018, 13], "temperature": 0.0, "avg_logprob": -0.14395586649576822, "compression_ratio": 1.8709677419354838, "no_speech_prob": 1.2218561096233316e-05}, {"id": 662, "seek": 214846, "start": 2168.94, "end": 2170.14, "text": " That didn't work.", "tokens": [663, 994, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.14395586649576822, "compression_ratio": 1.8709677419354838, "no_speech_prob": 1.2218561096233316e-05}, {"id": 663, "seek": 214846, "start": 2170.14, "end": 2171.62, "text": " Oh, well of course it didn't work.", "tokens": [876, 11, 731, 295, 1164, 309, 994, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.14395586649576822, "compression_ratio": 1.8709677419354838, "no_speech_prob": 1.2218561096233316e-05}, {"id": 664, "seek": 214846, "start": 2171.62, "end": 2176.3, "text": " The mean is defined as the thing which is in the middle, right?", "tokens": [440, 914, 307, 7642, 382, 264, 551, 597, 307, 294, 264, 2808, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14395586649576822, "compression_ratio": 1.8709677419354838, "no_speech_prob": 1.2218561096233316e-05}, {"id": 665, "seek": 214846, "start": 2176.3, "end": 2177.7400000000002, "text": " So of course that's always zero.", "tokens": [407, 295, 1164, 300, 311, 1009, 4018, 13], "temperature": 0.0, "avg_logprob": -0.14395586649576822, "compression_ratio": 1.8709677419354838, "no_speech_prob": 1.2218561096233316e-05}, {"id": 666, "seek": 217774, "start": 2177.74, "end": 2181.3799999999997, "text": " So we need to do something else that doesn't have the positives", "tokens": [407, 321, 643, 281, 360, 746, 1646, 300, 1177, 380, 362, 264, 35127], "temperature": 0.0, "avg_logprob": -0.09201957498277936, "compression_ratio": 1.7309236947791165, "no_speech_prob": 2.994420356117189e-06}, {"id": 667, "seek": 217774, "start": 2181.3799999999997, "end": 2183.2999999999997, "text": " and negatives cancel out.", "tokens": [293, 40019, 10373, 484, 13], "temperature": 0.0, "avg_logprob": -0.09201957498277936, "compression_ratio": 1.7309236947791165, "no_speech_prob": 2.994420356117189e-06}, {"id": 668, "seek": 217774, "start": 2183.2999999999997, "end": 2185.5, "text": " So there's two main ways we fix it.", "tokens": [407, 456, 311, 732, 2135, 2098, 321, 3191, 309, 13], "temperature": 0.0, "avg_logprob": -0.09201957498277936, "compression_ratio": 1.7309236947791165, "no_speech_prob": 2.994420356117189e-06}, {"id": 669, "seek": 217774, "start": 2185.5, "end": 2192.4599999999996, "text": " One is by squaring each thing before we take the mean, like so.", "tokens": [1485, 307, 538, 2339, 1921, 1184, 551, 949, 321, 747, 264, 914, 11, 411, 370, 13], "temperature": 0.0, "avg_logprob": -0.09201957498277936, "compression_ratio": 1.7309236947791165, "no_speech_prob": 2.994420356117189e-06}, {"id": 670, "seek": 217774, "start": 2192.4599999999996, "end": 2196.54, "text": " The other is taking the absolute value of each thing,", "tokens": [440, 661, 307, 1940, 264, 8236, 2158, 295, 1184, 551, 11], "temperature": 0.0, "avg_logprob": -0.09201957498277936, "compression_ratio": 1.7309236947791165, "no_speech_prob": 2.994420356117189e-06}, {"id": 671, "seek": 217774, "start": 2196.54, "end": 2200.1, "text": " so turning all the negatives and positives before we take the mean.", "tokens": [370, 6246, 439, 264, 40019, 293, 35127, 949, 321, 747, 264, 914, 13], "temperature": 0.0, "avg_logprob": -0.09201957498277936, "compression_ratio": 1.7309236947791165, "no_speech_prob": 2.994420356117189e-06}, {"id": 672, "seek": 217774, "start": 2200.1, "end": 2203.74, "text": " So they're both common fixes for this problem.", "tokens": [407, 436, 434, 1293, 2689, 32539, 337, 341, 1154, 13], "temperature": 0.0, "avg_logprob": -0.09201957498277936, "compression_ratio": 1.7309236947791165, "no_speech_prob": 2.994420356117189e-06}, {"id": 673, "seek": 217774, "start": 2203.74, "end": 2207.22, "text": " You can see though the first is now on a totally different scale, right?", "tokens": [509, 393, 536, 1673, 264, 700, 307, 586, 322, 257, 3879, 819, 4373, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.09201957498277936, "compression_ratio": 1.7309236947791165, "no_speech_prob": 2.994420356117189e-06}, {"id": 674, "seek": 220722, "start": 2207.22, "end": 2210.58, "text": " The numbers were like 1, 2, 4, 8, and this is 47.", "tokens": [440, 3547, 645, 411, 502, 11, 568, 11, 1017, 11, 1649, 11, 293, 341, 307, 16953, 13], "temperature": 0.0, "avg_logprob": -0.13055889665587875, "compression_ratio": 1.7479674796747968, "no_speech_prob": 3.446507889748318e-06}, {"id": 675, "seek": 220722, "start": 2210.58, "end": 2212.8599999999997, "text": " So we need to undo that squaring.", "tokens": [407, 321, 643, 281, 23779, 300, 2339, 1921, 13], "temperature": 0.0, "avg_logprob": -0.13055889665587875, "compression_ratio": 1.7479674796747968, "no_speech_prob": 3.446507889748318e-06}, {"id": 676, "seek": 220722, "start": 2212.8599999999997, "end": 2216.9399999999996, "text": " So after we've squared, we then take the square root at the end.", "tokens": [407, 934, 321, 600, 8889, 11, 321, 550, 747, 264, 3732, 5593, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.13055889665587875, "compression_ratio": 1.7479674796747968, "no_speech_prob": 3.446507889748318e-06}, {"id": 677, "seek": 220722, "start": 2216.9399999999996, "end": 2222.18, "text": " So here are two numbers that represent how far things are away from the mean,", "tokens": [407, 510, 366, 732, 3547, 300, 2906, 577, 1400, 721, 366, 1314, 490, 264, 914, 11], "temperature": 0.0, "avg_logprob": -0.13055889665587875, "compression_ratio": 1.7479674796747968, "no_speech_prob": 3.446507889748318e-06}, {"id": 678, "seek": 220722, "start": 2222.18, "end": 2224.4199999999996, "text": " or in other words, how much do they vary?", "tokens": [420, 294, 661, 2283, 11, 577, 709, 360, 436, 10559, 30], "temperature": 0.0, "avg_logprob": -0.13055889665587875, "compression_ratio": 1.7479674796747968, "no_speech_prob": 3.446507889748318e-06}, {"id": 679, "seek": 220722, "start": 2224.4199999999996, "end": 2226.98, "text": " If everything's pretty close to similar to each other,", "tokens": [759, 1203, 311, 1238, 1998, 281, 2531, 281, 1184, 661, 11], "temperature": 0.0, "avg_logprob": -0.13055889665587875, "compression_ratio": 1.7479674796747968, "no_speech_prob": 3.446507889748318e-06}, {"id": 680, "seek": 220722, "start": 2226.98, "end": 2228.8599999999997, "text": " those two numbers will be small.", "tokens": [729, 732, 3547, 486, 312, 1359, 13], "temperature": 0.0, "avg_logprob": -0.13055889665587875, "compression_ratio": 1.7479674796747968, "no_speech_prob": 3.446507889748318e-06}, {"id": 681, "seek": 220722, "start": 2228.8599999999997, "end": 2234.06, "text": " If they're wildly different to each other, those two numbers will be big.", "tokens": [759, 436, 434, 34731, 819, 281, 1184, 661, 11, 729, 732, 3547, 486, 312, 955, 13], "temperature": 0.0, "avg_logprob": -0.13055889665587875, "compression_ratio": 1.7479674796747968, "no_speech_prob": 3.446507889748318e-06}, {"id": 682, "seek": 223406, "start": 2234.06, "end": 2237.7, "text": " This one here is called the standard deviation,", "tokens": [639, 472, 510, 307, 1219, 264, 3832, 25163, 11], "temperature": 0.0, "avg_logprob": -0.19719059084668572, "compression_ratio": 1.6740331491712708, "no_speech_prob": 1.1842728781630285e-05}, {"id": 683, "seek": 223406, "start": 2237.7, "end": 2240.74, "text": " and it's defined as the square root of this one here,", "tokens": [293, 309, 311, 7642, 382, 264, 3732, 5593, 295, 341, 472, 510, 11], "temperature": 0.0, "avg_logprob": -0.19719059084668572, "compression_ratio": 1.6740331491712708, "no_speech_prob": 1.1842728781630285e-05}, {"id": 684, "seek": 223406, "start": 2240.74, "end": 2242.82, "text": " which is called the variance.", "tokens": [597, 307, 1219, 264, 21977, 13], "temperature": 0.0, "avg_logprob": -0.19719059084668572, "compression_ratio": 1.6740331491712708, "no_speech_prob": 1.1842728781630285e-05}, {"id": 685, "seek": 223406, "start": 2242.82, "end": 2247.02, "text": " And this one here is called the mean absolute deviation.", "tokens": [400, 341, 472, 510, 307, 1219, 264, 914, 8236, 25163, 13], "temperature": 0.0, "avg_logprob": -0.19719059084668572, "compression_ratio": 1.6740331491712708, "no_speech_prob": 1.1842728781630285e-05}, {"id": 686, "seek": 223406, "start": 2247.02, "end": 2253.22, "text": " You could replace this m mean with various other things like median,", "tokens": [509, 727, 7406, 341, 275, 914, 365, 3683, 661, 721, 411, 26779, 11], "temperature": 0.0, "avg_logprob": -0.19719059084668572, "compression_ratio": 1.6740331491712708, "no_speech_prob": 1.1842728781630285e-05}, {"id": 687, "seek": 223406, "start": 2253.22, "end": 2257.7799999999997, "text": " for example.", "tokens": [337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.19719059084668572, "compression_ratio": 1.6740331491712708, "no_speech_prob": 1.1842728781630285e-05}, {"id": 688, "seek": 223406, "start": 2257.7799999999997, "end": 2262.22, "text": " So we have one outlier here, 18.", "tokens": [407, 321, 362, 472, 484, 2753, 510, 11, 2443, 13], "temperature": 0.0, "avg_logprob": -0.19719059084668572, "compression_ratio": 1.6740331491712708, "no_speech_prob": 1.1842728781630285e-05}, {"id": 689, "seek": 226222, "start": 2262.22, "end": 2266.58, "text": " So in the case of the one where we took a square in the middle of it,", "tokens": [407, 294, 264, 1389, 295, 264, 472, 689, 321, 1890, 257, 3732, 294, 264, 2808, 295, 309, 11], "temperature": 0.0, "avg_logprob": -0.11779533823331197, "compression_ratio": 1.6919642857142858, "no_speech_prob": 4.425423867360223e-06}, {"id": 690, "seek": 226222, "start": 2266.58, "end": 2268.62, "text": " this number is higher, right?", "tokens": [341, 1230, 307, 2946, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11779533823331197, "compression_ratio": 1.6919642857142858, "no_speech_prob": 4.425423867360223e-06}, {"id": 691, "seek": 226222, "start": 2268.62, "end": 2271.8199999999997, "text": " Because the square takes that 18 and makes it much bigger.", "tokens": [1436, 264, 3732, 2516, 300, 2443, 293, 1669, 309, 709, 3801, 13], "temperature": 0.0, "avg_logprob": -0.11779533823331197, "compression_ratio": 1.6919642857142858, "no_speech_prob": 4.425423867360223e-06}, {"id": 692, "seek": 226222, "start": 2271.8199999999997, "end": 2276.22, "text": " So in other words, standard deviation is more sensitive to outliers", "tokens": [407, 294, 661, 2283, 11, 3832, 25163, 307, 544, 9477, 281, 484, 23646], "temperature": 0.0, "avg_logprob": -0.11779533823331197, "compression_ratio": 1.6919642857142858, "no_speech_prob": 4.425423867360223e-06}, {"id": 693, "seek": 226222, "start": 2276.22, "end": 2278.74, "text": " than mean absolute deviation.", "tokens": [813, 914, 8236, 25163, 13], "temperature": 0.0, "avg_logprob": -0.11779533823331197, "compression_ratio": 1.6919642857142858, "no_speech_prob": 4.425423867360223e-06}, {"id": 694, "seek": 226222, "start": 2278.74, "end": 2286.7, "text": " So for that reason, the mean absolute deviation is very often the thing you", "tokens": [407, 337, 300, 1778, 11, 264, 914, 8236, 25163, 307, 588, 2049, 264, 551, 291], "temperature": 0.0, "avg_logprob": -0.11779533823331197, "compression_ratio": 1.6919642857142858, "no_speech_prob": 4.425423867360223e-06}, {"id": 695, "seek": 226222, "start": 2286.7, "end": 2289.3799999999997, "text": " want to be using, because in machine learning,", "tokens": [528, 281, 312, 1228, 11, 570, 294, 3479, 2539, 11], "temperature": 0.0, "avg_logprob": -0.11779533823331197, "compression_ratio": 1.6919642857142858, "no_speech_prob": 4.425423867360223e-06}, {"id": 696, "seek": 228938, "start": 2289.38, "end": 2293.58, "text": " outliers are more of a problem than a help a lot of the time.", "tokens": [484, 23646, 366, 544, 295, 257, 1154, 813, 257, 854, 257, 688, 295, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.1438247333873402, "compression_ratio": 1.838174273858921, "no_speech_prob": 2.429706546536181e-05}, {"id": 697, "seek": 228938, "start": 2293.58, "end": 2297.82, "text": " But mathematicians and statisticians tend to work with standard deviation", "tokens": [583, 32811, 2567, 293, 29588, 2567, 3928, 281, 589, 365, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.1438247333873402, "compression_ratio": 1.838174273858921, "no_speech_prob": 2.429706546536181e-05}, {"id": 698, "seek": 228938, "start": 2297.82, "end": 2300.6600000000003, "text": " rather than mean absolute deviation, because it makes their math proofs", "tokens": [2831, 813, 914, 8236, 25163, 11, 570, 309, 1669, 641, 5221, 8177, 82], "temperature": 0.0, "avg_logprob": -0.1438247333873402, "compression_ratio": 1.838174273858921, "no_speech_prob": 2.429706546536181e-05}, {"id": 699, "seek": 228938, "start": 2300.6600000000003, "end": 2302.7000000000003, "text": " easier, and that's the only reason.", "tokens": [3571, 11, 293, 300, 311, 264, 787, 1778, 13], "temperature": 0.0, "avg_logprob": -0.1438247333873402, "compression_ratio": 1.838174273858921, "no_speech_prob": 2.429706546536181e-05}, {"id": 700, "seek": 228938, "start": 2302.7000000000003, "end": 2305.3, "text": " They'll tell you otherwise, but that's the only reason.", "tokens": [814, 603, 980, 291, 5911, 11, 457, 300, 311, 264, 787, 1778, 13], "temperature": 0.0, "avg_logprob": -0.1438247333873402, "compression_ratio": 1.838174273858921, "no_speech_prob": 2.429706546536181e-05}, {"id": 701, "seek": 228938, "start": 2305.3, "end": 2309.6600000000003, "text": " So the mean absolute deviation is really underused,", "tokens": [407, 264, 914, 8236, 25163, 307, 534, 833, 4717, 11], "temperature": 0.0, "avg_logprob": -0.1438247333873402, "compression_ratio": 1.838174273858921, "no_speech_prob": 2.429706546536181e-05}, {"id": 702, "seek": 228938, "start": 2309.6600000000003, "end": 2313.26, "text": " and it actually is a really great measure to use,", "tokens": [293, 309, 767, 307, 257, 534, 869, 3481, 281, 764, 11], "temperature": 0.0, "avg_logprob": -0.1438247333873402, "compression_ratio": 1.838174273858921, "no_speech_prob": 2.429706546536181e-05}, {"id": 703, "seek": 231326, "start": 2313.26, "end": 2320.1800000000003, "text": " and you should definitely get used to it.", "tokens": [293, 291, 820, 2138, 483, 1143, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.14959790858816593, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.668516208359506e-05}, {"id": 704, "seek": 231326, "start": 2320.1800000000003, "end": 2323.98, "text": " There's a lot of places where I kind of noticed that replacing things", "tokens": [821, 311, 257, 688, 295, 3190, 689, 286, 733, 295, 5694, 300, 19139, 721], "temperature": 0.0, "avg_logprob": -0.14959790858816593, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.668516208359506e-05}, {"id": 705, "seek": 231326, "start": 2323.98, "end": 2327.0200000000004, "text": " involving squareds with things involving absolute values,", "tokens": [17030, 8889, 82, 365, 721, 17030, 8236, 4190, 11], "temperature": 0.0, "avg_logprob": -0.14959790858816593, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.668516208359506e-05}, {"id": 706, "seek": 231326, "start": 2327.0200000000004, "end": 2329.82, "text": " the absolute value things just often work better.", "tokens": [264, 8236, 2158, 721, 445, 2049, 589, 1101, 13], "temperature": 0.0, "avg_logprob": -0.14959790858816593, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.668516208359506e-05}, {"id": 707, "seek": 231326, "start": 2329.82, "end": 2334.9, "text": " It's a good tip to remember that there's this kind of long-held assumption.", "tokens": [467, 311, 257, 665, 4125, 281, 1604, 300, 456, 311, 341, 733, 295, 938, 12, 23504, 15302, 13], "temperature": 0.0, "avg_logprob": -0.14959790858816593, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.668516208359506e-05}, {"id": 708, "seek": 231326, "start": 2334.9, "end": 2338.0200000000004, "text": " We have to use the squared thing everywhere,", "tokens": [492, 362, 281, 764, 264, 8889, 551, 5315, 11], "temperature": 0.0, "avg_logprob": -0.14959790858816593, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.668516208359506e-05}, {"id": 709, "seek": 231326, "start": 2338.0200000000004, "end": 2342.6600000000003, "text": " but it actually often doesn't work as well.", "tokens": [457, 309, 767, 2049, 1177, 380, 589, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.14959790858816593, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.668516208359506e-05}, {"id": 710, "seek": 234266, "start": 2342.66, "end": 2346.02, "text": " This is our definition of variance.", "tokens": [639, 307, 527, 7123, 295, 21977, 13], "temperature": 0.0, "avg_logprob": -0.12896263599395752, "compression_ratio": 1.723756906077348, "no_speech_prob": 4.8602714741718955e-06}, {"id": 711, "seek": 234266, "start": 2346.02, "end": 2350.2999999999997, "text": " Notice that this is the same.", "tokens": [13428, 300, 341, 307, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.12896263599395752, "compression_ratio": 1.723756906077348, "no_speech_prob": 4.8602714741718955e-06}, {"id": 712, "seek": 234266, "start": 2350.2999999999997, "end": 2352.7, "text": " So this is written in math.", "tokens": [407, 341, 307, 3720, 294, 5221, 13], "temperature": 0.0, "avg_logprob": -0.12896263599395752, "compression_ratio": 1.723756906077348, "no_speech_prob": 4.8602714741718955e-06}, {"id": 713, "seek": 234266, "start": 2355.7, "end": 2359.62, "text": " This written in math looks like this, and it's another way", "tokens": [639, 3720, 294, 5221, 1542, 411, 341, 11, 293, 309, 311, 1071, 636], "temperature": 0.0, "avg_logprob": -0.12896263599395752, "compression_ratio": 1.723756906077348, "no_speech_prob": 4.8602714741718955e-06}, {"id": 714, "seek": 234266, "start": 2359.62, "end": 2361.8199999999997, "text": " of writing the variance.", "tokens": [295, 3579, 264, 21977, 13], "temperature": 0.0, "avg_logprob": -0.12896263599395752, "compression_ratio": 1.723756906077348, "no_speech_prob": 4.8602714741718955e-06}, {"id": 715, "seek": 234266, "start": 2361.8199999999997, "end": 2364.94, "text": " It's important because it's super handy,", "tokens": [467, 311, 1021, 570, 309, 311, 1687, 13239, 11], "temperature": 0.0, "avg_logprob": -0.12896263599395752, "compression_ratio": 1.723756906077348, "no_speech_prob": 4.8602714741718955e-06}, {"id": 716, "seek": 234266, "start": 2364.94, "end": 2368.3799999999997, "text": " and it's super handy because in this one here,", "tokens": [293, 309, 311, 1687, 13239, 570, 294, 341, 472, 510, 11], "temperature": 0.0, "avg_logprob": -0.12896263599395752, "compression_ratio": 1.723756906077348, "no_speech_prob": 4.8602714741718955e-06}, {"id": 717, "seek": 234266, "start": 2368.3799999999997, "end": 2370.42, "text": " we have to go through the whole data set once.", "tokens": [321, 362, 281, 352, 807, 264, 1379, 1412, 992, 1564, 13], "temperature": 0.0, "avg_logprob": -0.12896263599395752, "compression_ratio": 1.723756906077348, "no_speech_prob": 4.8602714741718955e-06}, {"id": 718, "seek": 237042, "start": 2370.42, "end": 2373.46, "text": " Once to calculate the mean of the data,", "tokens": [3443, 281, 8873, 264, 914, 295, 264, 1412, 11], "temperature": 0.0, "avg_logprob": -0.1089890256841132, "compression_ratio": 1.6634615384615385, "no_speech_prob": 6.338887942547444e-06}, {"id": 719, "seek": 237042, "start": 2373.46, "end": 2379.46, "text": " and then a second time to get the squareds of the differences.", "tokens": [293, 550, 257, 1150, 565, 281, 483, 264, 8889, 82, 295, 264, 7300, 13], "temperature": 0.0, "avg_logprob": -0.1089890256841132, "compression_ratio": 1.6634615384615385, "no_speech_prob": 6.338887942547444e-06}, {"id": 720, "seek": 237042, "start": 2379.46, "end": 2383.38, "text": " This is really nice because in this case,", "tokens": [639, 307, 534, 1481, 570, 294, 341, 1389, 11], "temperature": 0.0, "avg_logprob": -0.1089890256841132, "compression_ratio": 1.6634615384615385, "no_speech_prob": 6.338887942547444e-06}, {"id": 721, "seek": 237042, "start": 2383.38, "end": 2385.7400000000002, "text": " we only have to keep track of two numbers,", "tokens": [321, 787, 362, 281, 1066, 2837, 295, 732, 3547, 11], "temperature": 0.0, "avg_logprob": -0.1089890256841132, "compression_ratio": 1.6634615384615385, "no_speech_prob": 6.338887942547444e-06}, {"id": 722, "seek": 237042, "start": 2385.7400000000002, "end": 2390.34, "text": " the squareds of the data and the sum of the data.", "tokens": [264, 8889, 82, 295, 264, 1412, 293, 264, 2408, 295, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1089890256841132, "compression_ratio": 1.6634615384615385, "no_speech_prob": 6.338887942547444e-06}, {"id": 723, "seek": 237042, "start": 2390.34, "end": 2394.14, "text": " And as you'll see shortly, this kind of way", "tokens": [400, 382, 291, 603, 536, 13392, 11, 341, 733, 295, 636], "temperature": 0.0, "avg_logprob": -0.1089890256841132, "compression_ratio": 1.6634615384615385, "no_speech_prob": 6.338887942547444e-06}, {"id": 724, "seek": 237042, "start": 2394.14, "end": 2399.26, "text": " of doing things is generally therefore just easier to work with.", "tokens": [295, 884, 721, 307, 5101, 4412, 445, 3571, 281, 589, 365, 13], "temperature": 0.0, "avg_logprob": -0.1089890256841132, "compression_ratio": 1.6634615384615385, "no_speech_prob": 6.338887942547444e-06}, {"id": 725, "seek": 239926, "start": 2399.26, "end": 2403.38, "text": " So even though this is kind of the definition", "tokens": [407, 754, 1673, 341, 307, 733, 295, 264, 7123], "temperature": 0.0, "avg_logprob": -0.10818233722593726, "compression_ratio": 1.6464088397790055, "no_speech_prob": 6.540153663081583e-06}, {"id": 726, "seek": 239926, "start": 2403.38, "end": 2408.1800000000003, "text": " of the variance that makes intuitive sense,", "tokens": [295, 264, 21977, 300, 1669, 21769, 2020, 11], "temperature": 0.0, "avg_logprob": -0.10818233722593726, "compression_ratio": 1.6464088397790055, "no_speech_prob": 6.540153663081583e-06}, {"id": 727, "seek": 239926, "start": 2408.1800000000003, "end": 2409.6600000000003, "text": " this is the definition of variance", "tokens": [341, 307, 264, 7123, 295, 21977], "temperature": 0.0, "avg_logprob": -0.10818233722593726, "compression_ratio": 1.6464088397790055, "no_speech_prob": 6.540153663081583e-06}, {"id": 728, "seek": 239926, "start": 2409.6600000000003, "end": 2413.5, "text": " that you normally want to implement.", "tokens": [300, 291, 5646, 528, 281, 4445, 13], "temperature": 0.0, "avg_logprob": -0.10818233722593726, "compression_ratio": 1.6464088397790055, "no_speech_prob": 6.540153663081583e-06}, {"id": 729, "seek": 239926, "start": 2413.5, "end": 2416.34, "text": " And so there it is in math.", "tokens": [400, 370, 456, 309, 307, 294, 5221, 13], "temperature": 0.0, "avg_logprob": -0.10818233722593726, "compression_ratio": 1.6464088397790055, "no_speech_prob": 6.540153663081583e-06}, {"id": 730, "seek": 239926, "start": 2416.34, "end": 2418.98, "text": " The other thing we see quite a bit is covariance", "tokens": [440, 661, 551, 321, 536, 1596, 257, 857, 307, 49851, 719], "temperature": 0.0, "avg_logprob": -0.10818233722593726, "compression_ratio": 1.6464088397790055, "no_speech_prob": 6.540153663081583e-06}, {"id": 731, "seek": 239926, "start": 2418.98, "end": 2421.42, "text": " and correlation.", "tokens": [293, 20009, 13], "temperature": 0.0, "avg_logprob": -0.10818233722593726, "compression_ratio": 1.6464088397790055, "no_speech_prob": 6.540153663081583e-06}, {"id": 732, "seek": 239926, "start": 2421.42, "end": 2425.34, "text": " So if we take our same data set, let's now", "tokens": [407, 498, 321, 747, 527, 912, 1412, 992, 11, 718, 311, 586], "temperature": 0.0, "avg_logprob": -0.10818233722593726, "compression_ratio": 1.6464088397790055, "no_speech_prob": 6.540153663081583e-06}, {"id": 733, "seek": 242534, "start": 2425.34, "end": 2432.26, "text": " create a second data set, which is double t times", "tokens": [1884, 257, 1150, 1412, 992, 11, 597, 307, 3834, 256, 1413], "temperature": 0.0, "avg_logprob": -0.13060401862775775, "compression_ratio": 1.5705128205128205, "no_speech_prob": 1.1300327969365753e-05}, {"id": 734, "seek": 242534, "start": 2432.26, "end": 2434.38, "text": " a little bit of random noise.", "tokens": [257, 707, 857, 295, 4974, 5658, 13], "temperature": 0.0, "avg_logprob": -0.13060401862775775, "compression_ratio": 1.5705128205128205, "no_speech_prob": 1.1300327969365753e-05}, {"id": 735, "seek": 242534, "start": 2434.38, "end": 2436.54, "text": " So here's that plotted.", "tokens": [407, 510, 311, 300, 43288, 13], "temperature": 0.0, "avg_logprob": -0.13060401862775775, "compression_ratio": 1.5705128205128205, "no_speech_prob": 1.1300327969365753e-05}, {"id": 736, "seek": 242534, "start": 2439.42, "end": 2444.42, "text": " Let's now look at the difference between each item of t", "tokens": [961, 311, 586, 574, 412, 264, 2649, 1296, 1184, 3174, 295, 256], "temperature": 0.0, "avg_logprob": -0.13060401862775775, "compression_ratio": 1.5705128205128205, "no_speech_prob": 1.1300327969365753e-05}, {"id": 737, "seek": 242534, "start": 2444.42, "end": 2450.26, "text": " and its mean and multiply it by each item of u and its mean.", "tokens": [293, 1080, 914, 293, 12972, 309, 538, 1184, 3174, 295, 344, 293, 1080, 914, 13], "temperature": 0.0, "avg_logprob": -0.13060401862775775, "compression_ratio": 1.5705128205128205, "no_speech_prob": 1.1300327969365753e-05}, {"id": 738, "seek": 242534, "start": 2450.26, "end": 2454.7400000000002, "text": " So there's those values.", "tokens": [407, 456, 311, 729, 4190, 13], "temperature": 0.0, "avg_logprob": -0.13060401862775775, "compression_ratio": 1.5705128205128205, "no_speech_prob": 1.1300327969365753e-05}, {"id": 739, "seek": 245474, "start": 2454.74, "end": 2458.58, "text": " And let's look at the mean of that.", "tokens": [400, 718, 311, 574, 412, 264, 914, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.10585469181097827, "compression_ratio": 1.8241206030150754, "no_speech_prob": 6.6431948653189465e-06}, {"id": 740, "seek": 245474, "start": 2458.58, "end": 2460.4199999999996, "text": " So what's this number?", "tokens": [407, 437, 311, 341, 1230, 30], "temperature": 0.0, "avg_logprob": -0.10585469181097827, "compression_ratio": 1.8241206030150754, "no_speech_prob": 6.6431948653189465e-06}, {"id": 741, "seek": 245474, "start": 2460.4199999999996, "end": 2462.3399999999997, "text": " So it's the average of the difference", "tokens": [407, 309, 311, 264, 4274, 295, 264, 2649], "temperature": 0.0, "avg_logprob": -0.10585469181097827, "compression_ratio": 1.8241206030150754, "no_speech_prob": 6.6431948653189465e-06}, {"id": 742, "seek": 245474, "start": 2462.3399999999997, "end": 2466.7799999999997, "text": " of how far away the x value is from the mean of the x value", "tokens": [295, 577, 1400, 1314, 264, 2031, 2158, 307, 490, 264, 914, 295, 264, 2031, 2158], "temperature": 0.0, "avg_logprob": -0.10585469181097827, "compression_ratio": 1.8241206030150754, "no_speech_prob": 6.6431948653189465e-06}, {"id": 743, "seek": 245474, "start": 2466.7799999999997, "end": 2473.04, "text": " is, the x's, multiplied by each difference between the y value", "tokens": [307, 11, 264, 2031, 311, 11, 17207, 538, 1184, 2649, 1296, 264, 288, 2158], "temperature": 0.0, "avg_logprob": -0.10585469181097827, "compression_ratio": 1.8241206030150754, "no_speech_prob": 6.6431948653189465e-06}, {"id": 744, "seek": 245474, "start": 2473.04, "end": 2476.8599999999997, "text": " and how far away from the y mean it is.", "tokens": [293, 577, 1400, 1314, 490, 264, 288, 914, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.10585469181097827, "compression_ratio": 1.8241206030150754, "no_speech_prob": 6.6431948653189465e-06}, {"id": 745, "seek": 245474, "start": 2476.8599999999997, "end": 2479.7, "text": " Let's compare this number to the same number calculated", "tokens": [961, 311, 6794, 341, 1230, 281, 264, 912, 1230, 15598], "temperature": 0.0, "avg_logprob": -0.10585469181097827, "compression_ratio": 1.8241206030150754, "no_speech_prob": 6.6431948653189465e-06}, {"id": 746, "seek": 245474, "start": 2479.7, "end": 2484.62, "text": " with this data set, where this data set is just", "tokens": [365, 341, 1412, 992, 11, 689, 341, 1412, 992, 307, 445], "temperature": 0.0, "avg_logprob": -0.10585469181097827, "compression_ratio": 1.8241206030150754, "no_speech_prob": 6.6431948653189465e-06}, {"id": 747, "seek": 248462, "start": 2484.62, "end": 2488.94, "text": " some random numbers compared to v.", "tokens": [512, 4974, 3547, 5347, 281, 371, 13], "temperature": 0.0, "avg_logprob": -0.09730652929509728, "compression_ratio": 1.7598039215686274, "no_speech_prob": 7.183162779256236e-06}, {"id": 748, "seek": 248462, "start": 2488.94, "end": 2491.2999999999997, "text": " And let's now calculate the exact same product,", "tokens": [400, 718, 311, 586, 8873, 264, 1900, 912, 1674, 11], "temperature": 0.0, "avg_logprob": -0.09730652929509728, "compression_ratio": 1.7598039215686274, "no_speech_prob": 7.183162779256236e-06}, {"id": 749, "seek": 248462, "start": 2491.2999999999997, "end": 2492.66, "text": " the exact same mean.", "tokens": [264, 1900, 912, 914, 13], "temperature": 0.0, "avg_logprob": -0.09730652929509728, "compression_ratio": 1.7598039215686274, "no_speech_prob": 7.183162779256236e-06}, {"id": 750, "seek": 248462, "start": 2492.66, "end": 2496.66, "text": " This number is much smaller than this number.", "tokens": [639, 1230, 307, 709, 4356, 813, 341, 1230, 13], "temperature": 0.0, "avg_logprob": -0.09730652929509728, "compression_ratio": 1.7598039215686274, "no_speech_prob": 7.183162779256236e-06}, {"id": 751, "seek": 248462, "start": 2496.66, "end": 2498.8199999999997, "text": " Why is this number much smaller?", "tokens": [1545, 307, 341, 1230, 709, 4356, 30], "temperature": 0.0, "avg_logprob": -0.09730652929509728, "compression_ratio": 1.7598039215686274, "no_speech_prob": 7.183162779256236e-06}, {"id": 752, "seek": 248462, "start": 2498.8199999999997, "end": 2503.58, "text": " So if you think about it, if these are kind of all lined up", "tokens": [407, 498, 291, 519, 466, 309, 11, 498, 613, 366, 733, 295, 439, 17189, 493], "temperature": 0.0, "avg_logprob": -0.09730652929509728, "compression_ratio": 1.7598039215686274, "no_speech_prob": 7.183162779256236e-06}, {"id": 753, "seek": 248462, "start": 2503.58, "end": 2506.42, "text": " nicely, then every time it's higher", "tokens": [9594, 11, 550, 633, 565, 309, 311, 2946], "temperature": 0.0, "avg_logprob": -0.09730652929509728, "compression_ratio": 1.7598039215686274, "no_speech_prob": 7.183162779256236e-06}, {"id": 754, "seek": 248462, "start": 2506.42, "end": 2509.2599999999998, "text": " than the average on the x-axis, it's", "tokens": [813, 264, 4274, 322, 264, 2031, 12, 24633, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.09730652929509728, "compression_ratio": 1.7598039215686274, "no_speech_prob": 7.183162779256236e-06}, {"id": 755, "seek": 248462, "start": 2509.2599999999998, "end": 2512.46, "text": " also higher than the average on the y-axis.", "tokens": [611, 2946, 813, 264, 4274, 322, 264, 288, 12, 24633, 13], "temperature": 0.0, "avg_logprob": -0.09730652929509728, "compression_ratio": 1.7598039215686274, "no_speech_prob": 7.183162779256236e-06}, {"id": 756, "seek": 251246, "start": 2512.46, "end": 2516.62, "text": " So you have two big positive numbers, and vice versa,", "tokens": [407, 291, 362, 732, 955, 3353, 3547, 11, 293, 11964, 25650, 11], "temperature": 0.0, "avg_logprob": -0.12094603538513184, "compression_ratio": 1.7980295566502462, "no_speech_prob": 4.710830125986831e-06}, {"id": 757, "seek": 251246, "start": 2516.62, "end": 2518.2200000000003, "text": " two big negative numbers.", "tokens": [732, 955, 3671, 3547, 13], "temperature": 0.0, "avg_logprob": -0.12094603538513184, "compression_ratio": 1.7980295566502462, "no_speech_prob": 4.710830125986831e-06}, {"id": 758, "seek": 251246, "start": 2518.2200000000003, "end": 2519.94, "text": " So in either case, you end up with,", "tokens": [407, 294, 2139, 1389, 11, 291, 917, 493, 365, 11], "temperature": 0.0, "avg_logprob": -0.12094603538513184, "compression_ratio": 1.7980295566502462, "no_speech_prob": 4.710830125986831e-06}, {"id": 759, "seek": 251246, "start": 2519.94, "end": 2523.34, "text": " when you multiply them together, a big positive number.", "tokens": [562, 291, 12972, 552, 1214, 11, 257, 955, 3353, 1230, 13], "temperature": 0.0, "avg_logprob": -0.12094603538513184, "compression_ratio": 1.7980295566502462, "no_speech_prob": 4.710830125986831e-06}, {"id": 760, "seek": 251246, "start": 2523.34, "end": 2527.62, "text": " So this is adding up a whole bunch of big positive numbers.", "tokens": [407, 341, 307, 5127, 493, 257, 1379, 3840, 295, 955, 3353, 3547, 13], "temperature": 0.0, "avg_logprob": -0.12094603538513184, "compression_ratio": 1.7980295566502462, "no_speech_prob": 4.710830125986831e-06}, {"id": 761, "seek": 251246, "start": 2527.62, "end": 2532.06, "text": " So in other words, this number tells you", "tokens": [407, 294, 661, 2283, 11, 341, 1230, 5112, 291], "temperature": 0.0, "avg_logprob": -0.12094603538513184, "compression_ratio": 1.7980295566502462, "no_speech_prob": 4.710830125986831e-06}, {"id": 762, "seek": 251246, "start": 2532.06, "end": 2537.2200000000003, "text": " how much these two things vary in the same way,", "tokens": [577, 709, 613, 732, 721, 10559, 294, 264, 912, 636, 11], "temperature": 0.0, "avg_logprob": -0.12094603538513184, "compression_ratio": 1.7980295566502462, "no_speech_prob": 4.710830125986831e-06}, {"id": 763, "seek": 251246, "start": 2537.2200000000003, "end": 2540.18, "text": " kind of how lined up are they on this graph.", "tokens": [733, 295, 577, 17189, 493, 366, 436, 322, 341, 4295, 13], "temperature": 0.0, "avg_logprob": -0.12094603538513184, "compression_ratio": 1.7980295566502462, "no_speech_prob": 4.710830125986831e-06}, {"id": 764, "seek": 254018, "start": 2540.18, "end": 2543.02, "text": " And so this one, when one is big,", "tokens": [400, 370, 341, 472, 11, 562, 472, 307, 955, 11], "temperature": 0.0, "avg_logprob": -0.11822926243649254, "compression_ratio": 1.699421965317919, "no_speech_prob": 4.93684774482972e-06}, {"id": 765, "seek": 254018, "start": 2543.02, "end": 2544.3799999999997, "text": " the other is not necessarily big.", "tokens": [264, 661, 307, 406, 4725, 955, 13], "temperature": 0.0, "avg_logprob": -0.11822926243649254, "compression_ratio": 1.699421965317919, "no_speech_prob": 4.93684774482972e-06}, {"id": 766, "seek": 254018, "start": 2544.3799999999997, "end": 2549.06, "text": " When one is small, the other is not necessarily very small.", "tokens": [1133, 472, 307, 1359, 11, 264, 661, 307, 406, 4725, 588, 1359, 13], "temperature": 0.0, "avg_logprob": -0.11822926243649254, "compression_ratio": 1.699421965317919, "no_speech_prob": 4.93684774482972e-06}, {"id": 767, "seek": 254018, "start": 2549.06, "end": 2554.14, "text": " So this is the covariance.", "tokens": [407, 341, 307, 264, 49851, 719, 13], "temperature": 0.0, "avg_logprob": -0.11822926243649254, "compression_ratio": 1.699421965317919, "no_speech_prob": 4.93684774482972e-06}, {"id": 768, "seek": 254018, "start": 2554.14, "end": 2558.62, "text": " And you can also calculate it in this way,", "tokens": [400, 291, 393, 611, 8873, 309, 294, 341, 636, 11], "temperature": 0.0, "avg_logprob": -0.11822926243649254, "compression_ratio": 1.699421965317919, "no_speech_prob": 4.93684774482972e-06}, {"id": 769, "seek": 254018, "start": 2558.62, "end": 2561.7799999999997, "text": " which might look somewhat similar to what we saw before", "tokens": [597, 1062, 574, 8344, 2531, 281, 437, 321, 1866, 949], "temperature": 0.0, "avg_logprob": -0.11822926243649254, "compression_ratio": 1.699421965317919, "no_speech_prob": 4.93684774482972e-06}, {"id": 770, "seek": 254018, "start": 2561.7799999999997, "end": 2564.8199999999997, "text": " with our different variance calculation.", "tokens": [365, 527, 819, 21977, 17108, 13], "temperature": 0.0, "avg_logprob": -0.11822926243649254, "compression_ratio": 1.699421965317919, "no_speech_prob": 4.93684774482972e-06}, {"id": 771, "seek": 256482, "start": 2564.82, "end": 2571.1000000000004, "text": " And again, this is kind of the easier way to use it.", "tokens": [400, 797, 11, 341, 307, 733, 295, 264, 3571, 636, 281, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.11418141375531207, "compression_ratio": 1.5073170731707317, "no_speech_prob": 2.123344074789202e-06}, {"id": 772, "seek": 256482, "start": 2571.1000000000004, "end": 2576.86, "text": " So as I say here, from now on, I don't", "tokens": [407, 382, 286, 584, 510, 11, 490, 586, 322, 11, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.11418141375531207, "compression_ratio": 1.5073170731707317, "no_speech_prob": 2.123344074789202e-06}, {"id": 773, "seek": 256482, "start": 2576.86, "end": 2578.5800000000004, "text": " want you to ever look at an equation", "tokens": [528, 291, 281, 1562, 574, 412, 364, 5367], "temperature": 0.0, "avg_logprob": -0.11418141375531207, "compression_ratio": 1.5073170731707317, "no_speech_prob": 2.123344074789202e-06}, {"id": 774, "seek": 256482, "start": 2578.5800000000004, "end": 2582.54, "text": " or type an equation in LaTeX without typing it in Python,", "tokens": [420, 2010, 364, 5367, 294, 2369, 14233, 55, 1553, 18444, 309, 294, 15329, 11], "temperature": 0.0, "avg_logprob": -0.11418141375531207, "compression_ratio": 1.5073170731707317, "no_speech_prob": 2.123344074789202e-06}, {"id": 775, "seek": 256482, "start": 2582.54, "end": 2585.78, "text": " calculating some values, and plotting them.", "tokens": [28258, 512, 4190, 11, 293, 41178, 552, 13], "temperature": 0.0, "avg_logprob": -0.11418141375531207, "compression_ratio": 1.5073170731707317, "no_speech_prob": 2.123344074789202e-06}, {"id": 776, "seek": 256482, "start": 2585.78, "end": 2588.46, "text": " Because this is the only way we get a sense in here", "tokens": [1436, 341, 307, 264, 787, 636, 321, 483, 257, 2020, 294, 510], "temperature": 0.0, "avg_logprob": -0.11418141375531207, "compression_ratio": 1.5073170731707317, "no_speech_prob": 2.123344074789202e-06}, {"id": 777, "seek": 256482, "start": 2588.46, "end": 2592.5, "text": " of what these things mean.", "tokens": [295, 437, 613, 721, 914, 13], "temperature": 0.0, "avg_logprob": -0.11418141375531207, "compression_ratio": 1.5073170731707317, "no_speech_prob": 2.123344074789202e-06}, {"id": 778, "seek": 259250, "start": 2592.5, "end": 2595.94, "text": " And so in this case, we're going to take our covariance,", "tokens": [400, 370, 294, 341, 1389, 11, 321, 434, 516, 281, 747, 527, 49851, 719, 11], "temperature": 0.0, "avg_logprob": -0.16496576801423105, "compression_ratio": 1.8203125, "no_speech_prob": 4.2644682253012434e-05}, {"id": 779, "seek": 259250, "start": 2595.94, "end": 2599.22, "text": " and we're going to divide it by the product of the standard", "tokens": [293, 321, 434, 516, 281, 9845, 309, 538, 264, 1674, 295, 264, 3832], "temperature": 0.0, "avg_logprob": -0.16496576801423105, "compression_ratio": 1.8203125, "no_speech_prob": 4.2644682253012434e-05}, {"id": 780, "seek": 259250, "start": 2599.22, "end": 2601.58, "text": " deviations.", "tokens": [31219, 763, 13], "temperature": 0.0, "avg_logprob": -0.16496576801423105, "compression_ratio": 1.8203125, "no_speech_prob": 4.2644682253012434e-05}, {"id": 781, "seek": 259250, "start": 2601.58, "end": 2604.1, "text": " And this gives us a new number.", "tokens": [400, 341, 2709, 505, 257, 777, 1230, 13], "temperature": 0.0, "avg_logprob": -0.16496576801423105, "compression_ratio": 1.8203125, "no_speech_prob": 4.2644682253012434e-05}, {"id": 782, "seek": 259250, "start": 2604.1, "end": 2606.94, "text": " And this is called correlation, or more specifically,", "tokens": [400, 341, 307, 1219, 20009, 11, 420, 544, 4682, 11], "temperature": 0.0, "avg_logprob": -0.16496576801423105, "compression_ratio": 1.8203125, "no_speech_prob": 4.2644682253012434e-05}, {"id": 783, "seek": 259250, "start": 2606.94, "end": 2609.78, "text": " Pearson correlation coefficient.", "tokens": [39041, 20009, 17619, 13], "temperature": 0.0, "avg_logprob": -0.16496576801423105, "compression_ratio": 1.8203125, "no_speech_prob": 4.2644682253012434e-05}, {"id": 784, "seek": 259250, "start": 2609.78, "end": 2612.14, "text": " Yeah, so we don't cover covariance and Pearson", "tokens": [865, 11, 370, 321, 500, 380, 2060, 49851, 719, 293, 39041], "temperature": 0.0, "avg_logprob": -0.16496576801423105, "compression_ratio": 1.8203125, "no_speech_prob": 4.2644682253012434e-05}, {"id": 785, "seek": 259250, "start": 2612.14, "end": 2614.58, "text": " correlation coefficient too much in the course,", "tokens": [20009, 17619, 886, 709, 294, 264, 1164, 11], "temperature": 0.0, "avg_logprob": -0.16496576801423105, "compression_ratio": 1.8203125, "no_speech_prob": 4.2644682253012434e-05}, {"id": 786, "seek": 259250, "start": 2614.58, "end": 2617.34, "text": " but it is one of these things which it's often nice to see", "tokens": [457, 309, 307, 472, 295, 613, 721, 597, 309, 311, 2049, 1481, 281, 536], "temperature": 0.0, "avg_logprob": -0.16496576801423105, "compression_ratio": 1.8203125, "no_speech_prob": 4.2644682253012434e-05}, {"id": 787, "seek": 259250, "start": 2617.34, "end": 2618.42, "text": " how things vary.", "tokens": [577, 721, 10559, 13], "temperature": 0.0, "avg_logprob": -0.16496576801423105, "compression_ratio": 1.8203125, "no_speech_prob": 4.2644682253012434e-05}, {"id": 788, "seek": 259250, "start": 2618.42, "end": 2620.38, "text": " But remember, it's telling you really about how", "tokens": [583, 1604, 11, 309, 311, 3585, 291, 534, 466, 577], "temperature": 0.0, "avg_logprob": -0.16496576801423105, "compression_ratio": 1.8203125, "no_speech_prob": 4.2644682253012434e-05}, {"id": 789, "seek": 262038, "start": 2620.38, "end": 2622.6600000000003, "text": " things vary linearly.", "tokens": [721, 10559, 43586, 13], "temperature": 0.0, "avg_logprob": -0.177479388108894, "compression_ratio": 1.7716535433070866, "no_speech_prob": 5.224567576078698e-05}, {"id": 790, "seek": 262038, "start": 2622.6600000000003, "end": 2624.6600000000003, "text": " So if you want to know how things vary nonlinearly,", "tokens": [407, 498, 291, 528, 281, 458, 577, 721, 10559, 2107, 28263, 356, 11], "temperature": 0.0, "avg_logprob": -0.177479388108894, "compression_ratio": 1.7716535433070866, "no_speech_prob": 5.224567576078698e-05}, {"id": 791, "seek": 262038, "start": 2624.6600000000003, "end": 2627.26, "text": " you have to create something called a neural network", "tokens": [291, 362, 281, 1884, 746, 1219, 257, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.177479388108894, "compression_ratio": 1.7716535433070866, "no_speech_prob": 5.224567576078698e-05}, {"id": 792, "seek": 262038, "start": 2627.26, "end": 2630.5, "text": " and check the loss and the metrics.", "tokens": [293, 1520, 264, 4470, 293, 264, 16367, 13], "temperature": 0.0, "avg_logprob": -0.177479388108894, "compression_ratio": 1.7716535433070866, "no_speech_prob": 5.224567576078698e-05}, {"id": 793, "seek": 262038, "start": 2630.5, "end": 2632.82, "text": " But it's kind of interesting to see also", "tokens": [583, 309, 311, 733, 295, 1880, 281, 536, 611], "temperature": 0.0, "avg_logprob": -0.177479388108894, "compression_ratio": 1.7716535433070866, "no_speech_prob": 5.224567576078698e-05}, {"id": 794, "seek": 262038, "start": 2632.82, "end": 2635.38, "text": " how variance and covariance, you can see they're", "tokens": [577, 21977, 293, 49851, 719, 11, 291, 393, 536, 436, 434], "temperature": 0.0, "avg_logprob": -0.177479388108894, "compression_ratio": 1.7716535433070866, "no_speech_prob": 5.224567576078698e-05}, {"id": 795, "seek": 262038, "start": 2635.38, "end": 2637.7000000000003, "text": " much the same thing.", "tokens": [709, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.177479388108894, "compression_ratio": 1.7716535433070866, "no_speech_prob": 5.224567576078698e-05}, {"id": 796, "seek": 262038, "start": 2637.7000000000003, "end": 2640.02, "text": " Where else one of them, in fact, basically you", "tokens": [2305, 1646, 472, 295, 552, 11, 294, 1186, 11, 1936, 291], "temperature": 0.0, "avg_logprob": -0.177479388108894, "compression_ratio": 1.7716535433070866, "no_speech_prob": 5.224567576078698e-05}, {"id": 797, "seek": 262038, "start": 2640.02, "end": 2640.98, "text": " can think of it this way.", "tokens": [393, 519, 295, 309, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.177479388108894, "compression_ratio": 1.7716535433070866, "no_speech_prob": 5.224567576078698e-05}, {"id": 798, "seek": 262038, "start": 2640.98, "end": 2644.82, "text": " One of them is e of x squared.", "tokens": [1485, 295, 552, 307, 308, 295, 2031, 8889, 13], "temperature": 0.0, "avg_logprob": -0.177479388108894, "compression_ratio": 1.7716535433070866, "no_speech_prob": 5.224567576078698e-05}, {"id": 799, "seek": 262038, "start": 2644.82, "end": 2647.2200000000003, "text": " In other words, x and x are kind of the same thing.", "tokens": [682, 661, 2283, 11, 2031, 293, 2031, 366, 733, 295, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.177479388108894, "compression_ratio": 1.7716535433070866, "no_speech_prob": 5.224567576078698e-05}, {"id": 800, "seek": 262038, "start": 2647.2200000000003, "end": 2648.86, "text": " It's e of x times x.", "tokens": [467, 311, 308, 295, 2031, 1413, 2031, 13], "temperature": 0.0, "avg_logprob": -0.177479388108894, "compression_ratio": 1.7716535433070866, "no_speech_prob": 5.224567576078698e-05}, {"id": 801, "seek": 264886, "start": 2648.86, "end": 2651.94, "text": " Where else this is two different things, e of x times y.", "tokens": [2305, 1646, 341, 307, 732, 819, 721, 11, 308, 295, 2031, 1413, 288, 13], "temperature": 0.0, "avg_logprob": -0.1544277124237596, "compression_ratio": 1.7579908675799087, "no_speech_prob": 1.3210539691499434e-05}, {"id": 802, "seek": 264886, "start": 2651.94, "end": 2657.6200000000003, "text": " And so rather than having here we had e of x squared here", "tokens": [400, 370, 2831, 813, 1419, 510, 321, 632, 308, 295, 2031, 8889, 510], "temperature": 0.0, "avg_logprob": -0.1544277124237596, "compression_ratio": 1.7579908675799087, "no_speech_prob": 1.3210539691499434e-05}, {"id": 803, "seek": 264886, "start": 2657.6200000000003, "end": 2659.94, "text": " and e of x squared here.", "tokens": [293, 308, 295, 2031, 8889, 510, 13], "temperature": 0.0, "avg_logprob": -0.1544277124237596, "compression_ratio": 1.7579908675799087, "no_speech_prob": 1.3210539691499434e-05}, {"id": 804, "seek": 264886, "start": 2659.94, "end": 2663.98, "text": " If you replace the second x with a y, you get that,", "tokens": [759, 291, 7406, 264, 1150, 2031, 365, 257, 288, 11, 291, 483, 300, 11], "temperature": 0.0, "avg_logprob": -0.1544277124237596, "compression_ratio": 1.7579908675799087, "no_speech_prob": 1.3210539691499434e-05}, {"id": 805, "seek": 264886, "start": 2663.98, "end": 2665.2200000000003, "text": " and you get that.", "tokens": [293, 291, 483, 300, 13], "temperature": 0.0, "avg_logprob": -0.1544277124237596, "compression_ratio": 1.7579908675799087, "no_speech_prob": 1.3210539691499434e-05}, {"id": 806, "seek": 264886, "start": 2665.2200000000003, "end": 2668.7000000000003, "text": " So they're literally the same thing.", "tokens": [407, 436, 434, 3736, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.1544277124237596, "compression_ratio": 1.7579908675799087, "no_speech_prob": 1.3210539691499434e-05}, {"id": 807, "seek": 264886, "start": 2668.7000000000003, "end": 2670.98, "text": " And then again here, if x and x are the same,", "tokens": [400, 550, 797, 510, 11, 498, 2031, 293, 2031, 366, 264, 912, 11], "temperature": 0.0, "avg_logprob": -0.1544277124237596, "compression_ratio": 1.7579908675799087, "no_speech_prob": 1.3210539691499434e-05}, {"id": 808, "seek": 264886, "start": 2670.98, "end": 2673.02, "text": " then this is just sigma squared.", "tokens": [550, 341, 307, 445, 12771, 8889, 13], "temperature": 0.0, "avg_logprob": -0.1544277124237596, "compression_ratio": 1.7579908675799087, "no_speech_prob": 1.3210539691499434e-05}, {"id": 809, "seek": 264886, "start": 2676.02, "end": 2678.6200000000003, "text": " So the last thing I want to quickly talk about a little bit", "tokens": [407, 264, 1036, 551, 286, 528, 281, 2661, 751, 466, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.1544277124237596, "compression_ratio": 1.7579908675799087, "no_speech_prob": 1.3210539691499434e-05}, {"id": 810, "seek": 267862, "start": 2678.62, "end": 2679.22, "text": " more is softmax.", "tokens": [544, 307, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.19052978356679282, "compression_ratio": 1.583710407239819, "no_speech_prob": 1.5445299140992574e-05}, {"id": 811, "seek": 267862, "start": 2683.42, "end": 2687.8599999999997, "text": " This was our final log softmax definition", "tokens": [639, 390, 527, 2572, 3565, 2787, 41167, 7123], "temperature": 0.0, "avg_logprob": -0.19052978356679282, "compression_ratio": 1.583710407239819, "no_speech_prob": 1.5445299140992574e-05}, {"id": 812, "seek": 267862, "start": 2687.8599999999997, "end": 2688.7799999999997, "text": " from the other day.", "tokens": [490, 264, 661, 786, 13], "temperature": 0.0, "avg_logprob": -0.19052978356679282, "compression_ratio": 1.583710407239819, "no_speech_prob": 1.5445299140992574e-05}, {"id": 813, "seek": 267862, "start": 2688.7799999999997, "end": 2695.54, "text": " And this is the formula, the same thing as an equation.", "tokens": [400, 341, 307, 264, 8513, 11, 264, 912, 551, 382, 364, 5367, 13], "temperature": 0.0, "avg_logprob": -0.19052978356679282, "compression_ratio": 1.583710407239819, "no_speech_prob": 1.5445299140992574e-05}, {"id": 814, "seek": 267862, "start": 2695.54, "end": 2699.74, "text": " And this is our cross entropy loss, remember.", "tokens": [400, 341, 307, 527, 3278, 30867, 4470, 11, 1604, 13], "temperature": 0.0, "avg_logprob": -0.19052978356679282, "compression_ratio": 1.583710407239819, "no_speech_prob": 1.5445299140992574e-05}, {"id": 815, "seek": 267862, "start": 2699.74, "end": 2701.38, "text": " So these are all important concepts", "tokens": [407, 613, 366, 439, 1021, 10392], "temperature": 0.0, "avg_logprob": -0.19052978356679282, "compression_ratio": 1.583710407239819, "no_speech_prob": 1.5445299140992574e-05}, {"id": 816, "seek": 267862, "start": 2701.38, "end": 2702.18, "text": " we're going to be using a lot.", "tokens": [321, 434, 516, 281, 312, 1228, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.19052978356679282, "compression_ratio": 1.583710407239819, "no_speech_prob": 1.5445299140992574e-05}, {"id": 817, "seek": 267862, "start": 2702.18, "end": 2704.2599999999998, "text": " So I just wanted to kind of clarify something", "tokens": [407, 286, 445, 1415, 281, 733, 295, 17594, 746], "temperature": 0.0, "avg_logprob": -0.19052978356679282, "compression_ratio": 1.583710407239819, "no_speech_prob": 1.5445299140992574e-05}, {"id": 818, "seek": 267862, "start": 2704.2599999999998, "end": 2707.94, "text": " that a lot of researchers that are published in big name", "tokens": [300, 257, 688, 295, 10309, 300, 366, 6572, 294, 955, 1315], "temperature": 0.0, "avg_logprob": -0.19052978356679282, "compression_ratio": 1.583710407239819, "no_speech_prob": 1.5445299140992574e-05}, {"id": 819, "seek": 270794, "start": 2707.94, "end": 2710.54, "text": " conferences get wrong, which is when should you", "tokens": [22032, 483, 2085, 11, 597, 307, 562, 820, 291], "temperature": 0.0, "avg_logprob": -0.10583661803117035, "compression_ratio": 1.8977272727272727, "no_speech_prob": 5.561553189181723e-05}, {"id": 820, "seek": 270794, "start": 2710.54, "end": 2712.98, "text": " and shouldn't you use softmax.", "tokens": [293, 4659, 380, 291, 764, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.10583661803117035, "compression_ratio": 1.8977272727272727, "no_speech_prob": 5.561553189181723e-05}, {"id": 821, "seek": 270794, "start": 2712.98, "end": 2716.46, "text": " So this is our softmax page from our entropy example", "tokens": [407, 341, 307, 527, 2787, 41167, 3028, 490, 527, 30867, 1365], "temperature": 0.0, "avg_logprob": -0.10583661803117035, "compression_ratio": 1.8977272727272727, "no_speech_prob": 5.561553189181723e-05}, {"id": 822, "seek": 270794, "start": 2716.46, "end": 2719.94, "text": " spreadsheet, where we were looking at cat, dog, plane,", "tokens": [27733, 11, 689, 321, 645, 1237, 412, 3857, 11, 3000, 11, 5720, 11], "temperature": 0.0, "avg_logprob": -0.10583661803117035, "compression_ratio": 1.8977272727272727, "no_speech_prob": 5.561553189181723e-05}, {"id": 823, "seek": 270794, "start": 2719.94, "end": 2721.46, "text": " fish building.", "tokens": [3506, 2390, 13], "temperature": 0.0, "avg_logprob": -0.10583661803117035, "compression_ratio": 1.8977272727272727, "no_speech_prob": 5.561553189181723e-05}, {"id": 824, "seek": 270794, "start": 2721.46, "end": 2723.18, "text": " And so we had various outputs.", "tokens": [400, 370, 321, 632, 3683, 23930, 13], "temperature": 0.0, "avg_logprob": -0.10583661803117035, "compression_ratio": 1.8977272727272727, "no_speech_prob": 5.561553189181723e-05}, {"id": 825, "seek": 270794, "start": 2723.18, "end": 2724.7400000000002, "text": " This is just the activations that we", "tokens": [639, 307, 445, 264, 2430, 763, 300, 321], "temperature": 0.0, "avg_logprob": -0.10583661803117035, "compression_ratio": 1.8977272727272727, "no_speech_prob": 5.561553189181723e-05}, {"id": 826, "seek": 270794, "start": 2724.7400000000002, "end": 2727.62, "text": " might have gotten out of the last layer of our model.", "tokens": [1062, 362, 5768, 484, 295, 264, 1036, 4583, 295, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10583661803117035, "compression_ratio": 1.8977272727272727, "no_speech_prob": 5.561553189181723e-05}, {"id": 827, "seek": 270794, "start": 2727.62, "end": 2730.9, "text": " And this is just e to the power of each of those activations.", "tokens": [400, 341, 307, 445, 308, 281, 264, 1347, 295, 1184, 295, 729, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.10583661803117035, "compression_ratio": 1.8977272727272727, "no_speech_prob": 5.561553189181723e-05}, {"id": 828, "seek": 270794, "start": 2730.9, "end": 2734.26, "text": " And this is just the sum of all of those e to the power ofs.", "tokens": [400, 341, 307, 445, 264, 2408, 295, 439, 295, 729, 308, 281, 264, 1347, 295, 82, 13], "temperature": 0.0, "avg_logprob": -0.10583661803117035, "compression_ratio": 1.8977272727272727, "no_speech_prob": 5.561553189181723e-05}, {"id": 829, "seek": 270794, "start": 2734.26, "end": 2737.1, "text": " And then this is e to the power of divided by the sum,", "tokens": [400, 550, 341, 307, 308, 281, 264, 1347, 295, 6666, 538, 264, 2408, 11], "temperature": 0.0, "avg_logprob": -0.10583661803117035, "compression_ratio": 1.8977272727272727, "no_speech_prob": 5.561553189181723e-05}, {"id": 830, "seek": 273710, "start": 2737.1, "end": 2738.58, "text": " which is softmax.", "tokens": [597, 307, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.12153667970137162, "compression_ratio": 1.7102803738317758, "no_speech_prob": 4.0061306208372116e-05}, {"id": 831, "seek": 273710, "start": 2738.58, "end": 2740.2599999999998, "text": " And of course, they all add up to one.", "tokens": [400, 295, 1164, 11, 436, 439, 909, 493, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.12153667970137162, "compression_ratio": 1.7102803738317758, "no_speech_prob": 4.0061306208372116e-05}, {"id": 832, "seek": 273710, "start": 2743.02, "end": 2744.9, "text": " This is like some image number one", "tokens": [639, 307, 411, 512, 3256, 1230, 472], "temperature": 0.0, "avg_logprob": -0.12153667970137162, "compression_ratio": 1.7102803738317758, "no_speech_prob": 4.0061306208372116e-05}, {"id": 833, "seek": 273710, "start": 2744.9, "end": 2746.66, "text": " that gave these activations.", "tokens": [300, 2729, 613, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.12153667970137162, "compression_ratio": 1.7102803738317758, "no_speech_prob": 4.0061306208372116e-05}, {"id": 834, "seek": 273710, "start": 2746.66, "end": 2749.1, "text": " Here's some other image number two,", "tokens": [1692, 311, 512, 661, 3256, 1230, 732, 11], "temperature": 0.0, "avg_logprob": -0.12153667970137162, "compression_ratio": 1.7102803738317758, "no_speech_prob": 4.0061306208372116e-05}, {"id": 835, "seek": 273710, "start": 2749.1, "end": 2751.18, "text": " which gave these activations, which", "tokens": [597, 2729, 613, 2430, 763, 11, 597], "temperature": 0.0, "avg_logprob": -0.12153667970137162, "compression_ratio": 1.7102803738317758, "no_speech_prob": 4.0061306208372116e-05}, {"id": 836, "seek": 273710, "start": 2751.18, "end": 2753.54, "text": " are very different to these.", "tokens": [366, 588, 819, 281, 613, 13], "temperature": 0.0, "avg_logprob": -0.12153667970137162, "compression_ratio": 1.7102803738317758, "no_speech_prob": 4.0061306208372116e-05}, {"id": 837, "seek": 273710, "start": 2753.54, "end": 2757.2999999999997, "text": " But the softmaxes are identical.", "tokens": [583, 264, 2787, 41167, 279, 366, 14800, 13], "temperature": 0.0, "avg_logprob": -0.12153667970137162, "compression_ratio": 1.7102803738317758, "no_speech_prob": 4.0061306208372116e-05}, {"id": 838, "seek": 273710, "start": 2757.2999999999997, "end": 2759.54, "text": " That and that are identical.", "tokens": [663, 293, 300, 366, 14800, 13], "temperature": 0.0, "avg_logprob": -0.12153667970137162, "compression_ratio": 1.7102803738317758, "no_speech_prob": 4.0061306208372116e-05}, {"id": 839, "seek": 273710, "start": 2759.54, "end": 2760.66, "text": " So that's weird.", "tokens": [407, 300, 311, 3657, 13], "temperature": 0.0, "avg_logprob": -0.12153667970137162, "compression_ratio": 1.7102803738317758, "no_speech_prob": 4.0061306208372116e-05}, {"id": 840, "seek": 273710, "start": 2760.66, "end": 2762.2599999999998, "text": " How has that happened?", "tokens": [1012, 575, 300, 2011, 30], "temperature": 0.0, "avg_logprob": -0.12153667970137162, "compression_ratio": 1.7102803738317758, "no_speech_prob": 4.0061306208372116e-05}, {"id": 841, "seek": 273710, "start": 2762.2599999999998, "end": 2765.98, "text": " Well, it's happened because in every case,", "tokens": [1042, 11, 309, 311, 2011, 570, 294, 633, 1389, 11], "temperature": 0.0, "avg_logprob": -0.12153667970137162, "compression_ratio": 1.7102803738317758, "no_speech_prob": 4.0061306208372116e-05}, {"id": 842, "seek": 276598, "start": 2765.98, "end": 2768.78, "text": " the e to the power of this divided", "tokens": [264, 308, 281, 264, 1347, 295, 341, 6666], "temperature": 0.0, "avg_logprob": -0.10810579712857905, "compression_ratio": 1.664835164835165, "no_speech_prob": 3.1875074455456343e-06}, {"id": 843, "seek": 276598, "start": 2768.78, "end": 2773.94, "text": " by the sum of the e to the power ofs ended up in the same ratio.", "tokens": [538, 264, 2408, 295, 264, 308, 281, 264, 1347, 295, 82, 4590, 493, 294, 264, 912, 8509, 13], "temperature": 0.0, "avg_logprob": -0.10810579712857905, "compression_ratio": 1.664835164835165, "no_speech_prob": 3.1875074455456343e-06}, {"id": 844, "seek": 276598, "start": 2773.94, "end": 2777.7400000000002, "text": " So in other words, even though fish is only 0.63 here,", "tokens": [407, 294, 661, 2283, 11, 754, 1673, 3506, 307, 787, 1958, 13, 29491, 510, 11], "temperature": 0.0, "avg_logprob": -0.10810579712857905, "compression_ratio": 1.664835164835165, "no_speech_prob": 3.1875074455456343e-06}, {"id": 845, "seek": 276598, "start": 2777.7400000000002, "end": 2781.7400000000002, "text": " but it's 2 here, once you take e to the power of,", "tokens": [457, 309, 311, 568, 510, 11, 1564, 291, 747, 308, 281, 264, 1347, 295, 11], "temperature": 0.0, "avg_logprob": -0.10810579712857905, "compression_ratio": 1.664835164835165, "no_speech_prob": 3.1875074455456343e-06}, {"id": 846, "seek": 276598, "start": 2781.7400000000002, "end": 2785.1, "text": " it's the same percentage of the sum.", "tokens": [309, 311, 264, 912, 9668, 295, 264, 2408, 13], "temperature": 0.0, "avg_logprob": -0.10810579712857905, "compression_ratio": 1.664835164835165, "no_speech_prob": 3.1875074455456343e-06}, {"id": 847, "seek": 276598, "start": 2785.1, "end": 2788.1, "text": " And so we end up with the same softmax.", "tokens": [400, 370, 321, 917, 493, 365, 264, 912, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.10810579712857905, "compression_ratio": 1.664835164835165, "no_speech_prob": 3.1875074455456343e-06}, {"id": 848, "seek": 276598, "start": 2788.1, "end": 2789.58, "text": " Why does that matter?", "tokens": [1545, 775, 300, 1871, 30], "temperature": 0.0, "avg_logprob": -0.10810579712857905, "compression_ratio": 1.664835164835165, "no_speech_prob": 3.1875074455456343e-06}, {"id": 849, "seek": 278958, "start": 2789.58, "end": 2797.06, "text": " Well, in this model, it seems like being a fish", "tokens": [1042, 11, 294, 341, 2316, 11, 309, 2544, 411, 885, 257, 3506], "temperature": 0.0, "avg_logprob": -0.12064674573066907, "compression_ratio": 1.5470588235294118, "no_speech_prob": 2.090415364364162e-06}, {"id": 850, "seek": 278958, "start": 2797.06, "end": 2801.94, "text": " is associated with having an activation of maybe like 2-ish.", "tokens": [307, 6615, 365, 1419, 364, 24433, 295, 1310, 411, 568, 12, 742, 13], "temperature": 0.0, "avg_logprob": -0.12064674573066907, "compression_ratio": 1.5470588235294118, "no_speech_prob": 2.090415364364162e-06}, {"id": 851, "seek": 278958, "start": 2801.94, "end": 2803.9, "text": " And this is only like 0.6-ish.", "tokens": [400, 341, 307, 787, 411, 1958, 13, 21, 12, 742, 13], "temperature": 0.0, "avg_logprob": -0.12064674573066907, "compression_ratio": 1.5470588235294118, "no_speech_prob": 2.090415364364162e-06}, {"id": 852, "seek": 278958, "start": 2803.9, "end": 2806.94, "text": " So maybe there's no fish in this.", "tokens": [407, 1310, 456, 311, 572, 3506, 294, 341, 13], "temperature": 0.0, "avg_logprob": -0.12064674573066907, "compression_ratio": 1.5470588235294118, "no_speech_prob": 2.090415364364162e-06}, {"id": 853, "seek": 278958, "start": 2806.94, "end": 2809.54, "text": " But what's actually happened is there's", "tokens": [583, 437, 311, 767, 2011, 307, 456, 311], "temperature": 0.0, "avg_logprob": -0.12064674573066907, "compression_ratio": 1.5470588235294118, "no_speech_prob": 2.090415364364162e-06}, {"id": 854, "seek": 278958, "start": 2809.54, "end": 2812.54, "text": " no cats or dogs or planes or fishes or buildings.", "tokens": [572, 11111, 420, 7197, 420, 14952, 420, 41734, 420, 7446, 13], "temperature": 0.0, "avg_logprob": -0.12064674573066907, "compression_ratio": 1.5470588235294118, "no_speech_prob": 2.090415364364162e-06}, {"id": 855, "seek": 281254, "start": 2812.54, "end": 2821.98, "text": " So in the end then, because softmax has to add to 1,", "tokens": [407, 294, 264, 917, 550, 11, 570, 2787, 41167, 575, 281, 909, 281, 502, 11], "temperature": 0.0, "avg_logprob": -0.27352663040161135, "compression_ratio": 1.6984924623115578, "no_speech_prob": 9.08000220078975e-06}, {"id": 856, "seek": 281254, "start": 2821.98, "end": 2824.34, "text": " it has to pick something.", "tokens": [309, 575, 281, 1888, 746, 13], "temperature": 0.0, "avg_logprob": -0.27352663040161135, "compression_ratio": 1.6984924623115578, "no_speech_prob": 9.08000220078975e-06}, {"id": 857, "seek": 281254, "start": 2824.34, "end": 2826.02, "text": " So it's fish that comes through.", "tokens": [407, 309, 311, 3506, 300, 1487, 807, 13], "temperature": 0.0, "avg_logprob": -0.27352663040161135, "compression_ratio": 1.6984924623115578, "no_speech_prob": 9.08000220078975e-06}, {"id": 858, "seek": 281254, "start": 2826.02, "end": 2829.82, "text": " And what's more is because we do this e to the power of,", "tokens": [400, 437, 311, 544, 307, 570, 321, 360, 341, 308, 281, 264, 1347, 295, 11], "temperature": 0.0, "avg_logprob": -0.27352663040161135, "compression_ratio": 1.6984924623115578, "no_speech_prob": 9.08000220078975e-06}, {"id": 859, "seek": 281254, "start": 2829.82, "end": 2831.42, "text": " the thing that's a little bit higher,", "tokens": [264, 551, 300, 311, 257, 707, 857, 2946, 11], "temperature": 0.0, "avg_logprob": -0.27352663040161135, "compression_ratio": 1.6984924623115578, "no_speech_prob": 9.08000220078975e-06}, {"id": 860, "seek": 281254, "start": 2831.42, "end": 2834.62, "text": " it pushes much higher because it's exponential.", "tokens": [309, 21020, 709, 2946, 570, 309, 311, 21510, 13], "temperature": 0.0, "avg_logprob": -0.27352663040161135, "compression_ratio": 1.6984924623115578, "no_speech_prob": 9.08000220078975e-06}, {"id": 861, "seek": 281254, "start": 2834.62, "end": 2838.74, "text": " So softmax likes to pick one thing and make it big.", "tokens": [407, 2787, 41167, 5902, 281, 1888, 472, 551, 293, 652, 309, 955, 13], "temperature": 0.0, "avg_logprob": -0.27352663040161135, "compression_ratio": 1.6984924623115578, "no_speech_prob": 9.08000220078975e-06}, {"id": 862, "seek": 281254, "start": 2838.74, "end": 2840.34, "text": " And they have to add something.", "tokens": [400, 436, 362, 281, 909, 746, 13], "temperature": 0.0, "avg_logprob": -0.27352663040161135, "compression_ratio": 1.6984924623115578, "no_speech_prob": 9.08000220078975e-06}, {"id": 863, "seek": 284034, "start": 2840.34, "end": 2842.7400000000002, "text": " And they have to add up to 1.", "tokens": [400, 436, 362, 281, 909, 493, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.2229730747948008, "compression_ratio": 1.6875, "no_speech_prob": 3.763391578104347e-05}, {"id": 864, "seek": 284034, "start": 2842.7400000000002, "end": 2846.38, "text": " So the problem here is that I would guess that maybe image 2", "tokens": [407, 264, 1154, 510, 307, 300, 286, 576, 2041, 300, 1310, 3256, 568], "temperature": 0.0, "avg_logprob": -0.2229730747948008, "compression_ratio": 1.6875, "no_speech_prob": 3.763391578104347e-05}, {"id": 865, "seek": 284034, "start": 2846.38, "end": 2848.94, "text": " doesn't have any of these things in it.", "tokens": [1177, 380, 362, 604, 295, 613, 721, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.2229730747948008, "compression_ratio": 1.6875, "no_speech_prob": 3.763391578104347e-05}, {"id": 866, "seek": 284034, "start": 2848.94, "end": 2850.2200000000003, "text": " And we had to pick something.", "tokens": [400, 321, 632, 281, 1888, 746, 13], "temperature": 0.0, "avg_logprob": -0.2229730747948008, "compression_ratio": 1.6875, "no_speech_prob": 3.763391578104347e-05}, {"id": 867, "seek": 284034, "start": 2850.2200000000003, "end": 2852.6600000000003, "text": " So it said, oh, I'm pretty sure there's a fish.", "tokens": [407, 309, 848, 11, 1954, 11, 286, 478, 1238, 988, 456, 311, 257, 3506, 13], "temperature": 0.0, "avg_logprob": -0.2229730747948008, "compression_ratio": 1.6875, "no_speech_prob": 3.763391578104347e-05}, {"id": 868, "seek": 284034, "start": 2852.6600000000003, "end": 2859.5, "text": " Or maybe the problem actually is that this image had a cat", "tokens": [1610, 1310, 264, 1154, 767, 307, 300, 341, 3256, 632, 257, 3857], "temperature": 0.0, "avg_logprob": -0.2229730747948008, "compression_ratio": 1.6875, "no_speech_prob": 3.763391578104347e-05}, {"id": 869, "seek": 284034, "start": 2859.5, "end": 2862.46, "text": " and a fish and a building.", "tokens": [293, 257, 3506, 293, 257, 2390, 13], "temperature": 0.0, "avg_logprob": -0.2229730747948008, "compression_ratio": 1.6875, "no_speech_prob": 3.763391578104347e-05}, {"id": 870, "seek": 284034, "start": 2862.46, "end": 2865.34, "text": " But again, because softmax, they have to add to 1.", "tokens": [583, 797, 11, 570, 2787, 41167, 11, 436, 362, 281, 909, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.2229730747948008, "compression_ratio": 1.6875, "no_speech_prob": 3.763391578104347e-05}, {"id": 871, "seek": 284034, "start": 2865.34, "end": 2868.6600000000003, "text": " And one of them is going to be much bigger than the others.", "tokens": [400, 472, 295, 552, 307, 516, 281, 312, 709, 3801, 813, 264, 2357, 13], "temperature": 0.0, "avg_logprob": -0.2229730747948008, "compression_ratio": 1.6875, "no_speech_prob": 3.763391578104347e-05}, {"id": 872, "seek": 286866, "start": 2868.66, "end": 2871.2599999999998, "text": " So I don't know exactly which of these happened.", "tokens": [407, 286, 500, 380, 458, 2293, 597, 295, 613, 2011, 13], "temperature": 0.0, "avg_logprob": -0.2114806504085146, "compression_ratio": 1.7759336099585061, "no_speech_prob": 7.4108825174334925e-06}, {"id": 873, "seek": 286866, "start": 2871.2599999999998, "end": 2874.7, "text": " But it's definitely not true that they both", "tokens": [583, 309, 311, 2138, 406, 2074, 300, 436, 1293], "temperature": 0.0, "avg_logprob": -0.2114806504085146, "compression_ratio": 1.7759336099585061, "no_speech_prob": 7.4108825174334925e-06}, {"id": 874, "seek": 286866, "start": 2874.7, "end": 2878.7, "text": " have an equal probability of having a fish in them.", "tokens": [362, 364, 2681, 8482, 295, 1419, 257, 3506, 294, 552, 13], "temperature": 0.0, "avg_logprob": -0.2114806504085146, "compression_ratio": 1.7759336099585061, "no_speech_prob": 7.4108825174334925e-06}, {"id": 875, "seek": 286866, "start": 2878.7, "end": 2882.7, "text": " So to put this another way, softmax is a terrible idea", "tokens": [407, 281, 829, 341, 1071, 636, 11, 2787, 41167, 307, 257, 6237, 1558], "temperature": 0.0, "avg_logprob": -0.2114806504085146, "compression_ratio": 1.7759336099585061, "no_speech_prob": 7.4108825174334925e-06}, {"id": 876, "seek": 286866, "start": 2882.7, "end": 2885.5, "text": " unless you know that every one of your,", "tokens": [5969, 291, 458, 300, 633, 472, 295, 428, 11], "temperature": 0.0, "avg_logprob": -0.2114806504085146, "compression_ratio": 1.7759336099585061, "no_speech_prob": 7.4108825174334925e-06}, {"id": 877, "seek": 286866, "start": 2885.5, "end": 2887.1, "text": " if you're doing image recognition,", "tokens": [498, 291, 434, 884, 3256, 11150, 11], "temperature": 0.0, "avg_logprob": -0.2114806504085146, "compression_ratio": 1.7759336099585061, "no_speech_prob": 7.4108825174334925e-06}, {"id": 878, "seek": 286866, "start": 2887.1, "end": 2890.1, "text": " every one of your images, or if you're doing audio", "tokens": [633, 472, 295, 428, 5267, 11, 420, 498, 291, 434, 884, 6278], "temperature": 0.0, "avg_logprob": -0.2114806504085146, "compression_ratio": 1.7759336099585061, "no_speech_prob": 7.4108825174334925e-06}, {"id": 879, "seek": 286866, "start": 2890.1, "end": 2894.74, "text": " or tabular or whatever, every one of your items has one,", "tokens": [420, 4421, 1040, 420, 2035, 11, 633, 472, 295, 428, 4754, 575, 472, 11], "temperature": 0.0, "avg_logprob": -0.2114806504085146, "compression_ratio": 1.7759336099585061, "no_speech_prob": 7.4108825174334925e-06}, {"id": 880, "seek": 286866, "start": 2894.74, "end": 2897.66, "text": " no more than one, and definitely at least one", "tokens": [572, 544, 813, 472, 11, 293, 2138, 412, 1935, 472], "temperature": 0.0, "avg_logprob": -0.2114806504085146, "compression_ratio": 1.7759336099585061, "no_speech_prob": 7.4108825174334925e-06}, {"id": 881, "seek": 289766, "start": 2897.66, "end": 2901.1, "text": " example of the thing you care about in it.", "tokens": [1365, 295, 264, 551, 291, 1127, 466, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.1937534511089325, "compression_ratio": 1.8874458874458875, "no_speech_prob": 1.428526684321696e-05}, {"id": 882, "seek": 289766, "start": 2901.1, "end": 2904.1, "text": " Because if it doesn't have any of cat, dog, plane, fish,", "tokens": [1436, 498, 309, 1177, 380, 362, 604, 295, 3857, 11, 3000, 11, 5720, 11, 3506, 11], "temperature": 0.0, "avg_logprob": -0.1937534511089325, "compression_ratio": 1.8874458874458875, "no_speech_prob": 1.428526684321696e-05}, {"id": 883, "seek": 289766, "start": 2904.1, "end": 2907.8999999999996, "text": " or building, it's still going to tell you with high probability", "tokens": [420, 2390, 11, 309, 311, 920, 516, 281, 980, 291, 365, 1090, 8482], "temperature": 0.0, "avg_logprob": -0.1937534511089325, "compression_ratio": 1.8874458874458875, "no_speech_prob": 1.428526684321696e-05}, {"id": 884, "seek": 289766, "start": 2907.8999999999996, "end": 2909.7, "text": " that it has one of those things.", "tokens": [300, 309, 575, 472, 295, 729, 721, 13], "temperature": 0.0, "avg_logprob": -0.1937534511089325, "compression_ratio": 1.8874458874458875, "no_speech_prob": 1.428526684321696e-05}, {"id": 885, "seek": 289766, "start": 2909.7, "end": 2912.2999999999997, "text": " Even if it has more than just one of cat, dog, plane, fish,", "tokens": [2754, 498, 309, 575, 544, 813, 445, 472, 295, 3857, 11, 3000, 11, 5720, 11, 3506, 11], "temperature": 0.0, "avg_logprob": -0.1937534511089325, "compression_ratio": 1.8874458874458875, "no_speech_prob": 1.428526684321696e-05}, {"id": 886, "seek": 289766, "start": 2912.2999999999997, "end": 2914.02, "text": " or building, it'll pick one of them", "tokens": [420, 2390, 11, 309, 603, 1888, 472, 295, 552], "temperature": 0.0, "avg_logprob": -0.1937534511089325, "compression_ratio": 1.8874458874458875, "no_speech_prob": 1.428526684321696e-05}, {"id": 887, "seek": 289766, "start": 2914.02, "end": 2916.2999999999997, "text": " and tell you it's pretty sure it's got that one.", "tokens": [293, 980, 291, 309, 311, 1238, 988, 309, 311, 658, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.1937534511089325, "compression_ratio": 1.8874458874458875, "no_speech_prob": 1.428526684321696e-05}, {"id": 888, "seek": 289766, "start": 2916.2999999999997, "end": 2921.8599999999997, "text": " So what do you do if there could be no things", "tokens": [407, 437, 360, 291, 360, 498, 456, 727, 312, 572, 721], "temperature": 0.0, "avg_logprob": -0.1937534511089325, "compression_ratio": 1.8874458874458875, "no_speech_prob": 1.428526684321696e-05}, {"id": 889, "seek": 289766, "start": 2921.8599999999997, "end": 2924.66, "text": " or there could be more than one of these things?", "tokens": [420, 456, 727, 312, 544, 813, 472, 295, 613, 721, 30], "temperature": 0.0, "avg_logprob": -0.1937534511089325, "compression_ratio": 1.8874458874458875, "no_speech_prob": 1.428526684321696e-05}, {"id": 890, "seek": 292466, "start": 2924.66, "end": 2930.2599999999998, "text": " Well, instead, you use binomial, regular old binomial,", "tokens": [1042, 11, 2602, 11, 291, 764, 5171, 47429, 11, 3890, 1331, 5171, 47429, 11], "temperature": 0.0, "avg_logprob": -0.21977530585394967, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.1443609512061812e-05}, {"id": 891, "seek": 292466, "start": 2930.2599999999998, "end": 2934.46, "text": " which is e to the x divided by 1 plus e to the x.", "tokens": [597, 307, 308, 281, 264, 2031, 6666, 538, 502, 1804, 308, 281, 264, 2031, 13], "temperature": 0.0, "avg_logprob": -0.21977530585394967, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.1443609512061812e-05}, {"id": 892, "seek": 292466, "start": 2934.46, "end": 2939.06, "text": " It's exactly the same as softmax if your two categories are,", "tokens": [467, 311, 2293, 264, 912, 382, 2787, 41167, 498, 428, 732, 10479, 366, 11], "temperature": 0.0, "avg_logprob": -0.21977530585394967, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.1443609512061812e-05}, {"id": 893, "seek": 292466, "start": 2939.06, "end": 2941.2599999999998, "text": " has the thing and doesn't have the thing,", "tokens": [575, 264, 551, 293, 1177, 380, 362, 264, 551, 11], "temperature": 0.0, "avg_logprob": -0.21977530585394967, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.1443609512061812e-05}, {"id": 894, "seek": 292466, "start": 2941.2599999999998, "end": 2943.46, "text": " because they're like p and 1 minus p.", "tokens": [570, 436, 434, 411, 280, 293, 502, 3175, 280, 13], "temperature": 0.0, "avg_logprob": -0.21977530585394967, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.1443609512061812e-05}, {"id": 895, "seek": 292466, "start": 2943.46, "end": 2946.06, "text": " So you can convince yourself of that during the week.", "tokens": [407, 291, 393, 13447, 1803, 295, 300, 1830, 264, 1243, 13], "temperature": 0.0, "avg_logprob": -0.21977530585394967, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.1443609512061812e-05}, {"id": 896, "seek": 292466, "start": 2946.06, "end": 2953.8599999999997, "text": " So in this case, let's take image one and let's go 1.02.", "tokens": [407, 294, 341, 1389, 11, 718, 311, 747, 3256, 472, 293, 718, 311, 352, 502, 13, 12756, 13], "temperature": 0.0, "avg_logprob": -0.21977530585394967, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.1443609512061812e-05}, {"id": 897, "seek": 295386, "start": 2953.86, "end": 2957.46, "text": " 1.02 divided by 1 plus 1.02.", "tokens": [502, 13, 12756, 6666, 538, 502, 1804, 502, 13, 12756, 13], "temperature": 0.0, "avg_logprob": -0.15682727319222908, "compression_ratio": 1.5933014354066986, "no_speech_prob": 1.7231135643669404e-05}, {"id": 898, "seek": 295386, "start": 2957.46, "end": 2960.46, "text": " And ditto for each of our different ones.", "tokens": [400, 274, 34924, 337, 1184, 295, 527, 819, 2306, 13], "temperature": 0.0, "avg_logprob": -0.15682727319222908, "compression_ratio": 1.5933014354066986, "no_speech_prob": 1.7231135643669404e-05}, {"id": 899, "seek": 295386, "start": 2960.46, "end": 2963.26, "text": " And then let's do the same thing for image two.", "tokens": [400, 550, 718, 311, 360, 264, 912, 551, 337, 3256, 732, 13], "temperature": 0.0, "avg_logprob": -0.15682727319222908, "compression_ratio": 1.5933014354066986, "no_speech_prob": 1.7231135643669404e-05}, {"id": 900, "seek": 295386, "start": 2963.26, "end": 2967.86, "text": " And you can see now the numbers are different, as we would hope.", "tokens": [400, 291, 393, 536, 586, 264, 3547, 366, 819, 11, 382, 321, 576, 1454, 13], "temperature": 0.0, "avg_logprob": -0.15682727319222908, "compression_ratio": 1.5933014354066986, "no_speech_prob": 1.7231135643669404e-05}, {"id": 901, "seek": 295386, "start": 2967.86, "end": 2971.26, "text": " And so for image one, it's kind of saying, oh,", "tokens": [400, 370, 337, 3256, 472, 11, 309, 311, 733, 295, 1566, 11, 1954, 11], "temperature": 0.0, "avg_logprob": -0.15682727319222908, "compression_ratio": 1.5933014354066986, "no_speech_prob": 1.7231135643669404e-05}, {"id": 902, "seek": 295386, "start": 2971.26, "end": 2974.86, "text": " it looks like there might be a cat in it.", "tokens": [309, 1542, 411, 456, 1062, 312, 257, 3857, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.15682727319222908, "compression_ratio": 1.5933014354066986, "no_speech_prob": 1.7231135643669404e-05}, {"id": 903, "seek": 295386, "start": 2974.86, "end": 2979.46, "text": " If we assume 0.5 is a cutoff, there's probably a fish in it.", "tokens": [759, 321, 6552, 1958, 13, 20, 307, 257, 1723, 4506, 11, 456, 311, 1391, 257, 3506, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.15682727319222908, "compression_ratio": 1.5933014354066986, "no_speech_prob": 1.7231135643669404e-05}, {"id": 904, "seek": 297946, "start": 2979.46, "end": 2983.86, "text": " And it seems likely that there's a building in it.", "tokens": [400, 309, 2544, 3700, 300, 456, 311, 257, 2390, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.08474746636584797, "compression_ratio": 1.6291666666666667, "no_speech_prob": 3.1381200642499607e-06}, {"id": 905, "seek": 297946, "start": 2983.86, "end": 2985.86, "text": " Whereas for image two, it's saying,", "tokens": [13813, 337, 3256, 732, 11, 309, 311, 1566, 11], "temperature": 0.0, "avg_logprob": -0.08474746636584797, "compression_ratio": 1.6291666666666667, "no_speech_prob": 3.1381200642499607e-06}, {"id": 906, "seek": 297946, "start": 2985.86, "end": 2989.86, "text": " I don't think there's anything in there but maybe a fish.", "tokens": [286, 500, 380, 519, 456, 311, 1340, 294, 456, 457, 1310, 257, 3506, 13], "temperature": 0.0, "avg_logprob": -0.08474746636584797, "compression_ratio": 1.6291666666666667, "no_speech_prob": 3.1381200642499607e-06}, {"id": 907, "seek": 297946, "start": 2989.86, "end": 2991.46, "text": " And this is what we want.", "tokens": [400, 341, 307, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.08474746636584797, "compression_ratio": 1.6291666666666667, "no_speech_prob": 3.1381200642499607e-06}, {"id": 908, "seek": 297946, "start": 2991.46, "end": 2995.66, "text": " And so when you think about it, like for image recognition,", "tokens": [400, 370, 562, 291, 519, 466, 309, 11, 411, 337, 3256, 11150, 11], "temperature": 0.0, "avg_logprob": -0.08474746636584797, "compression_ratio": 1.6291666666666667, "no_speech_prob": 3.1381200642499607e-06}, {"id": 909, "seek": 297946, "start": 2995.66, "end": 2999.86, "text": " probably most of the time, you don't want softmax.", "tokens": [1391, 881, 295, 264, 565, 11, 291, 500, 380, 528, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.08474746636584797, "compression_ratio": 1.6291666666666667, "no_speech_prob": 3.1381200642499607e-06}, {"id": 910, "seek": 297946, "start": 2999.86, "end": 3002.46, "text": " So why do we always use softmax?", "tokens": [407, 983, 360, 321, 1009, 764, 2787, 41167, 30], "temperature": 0.0, "avg_logprob": -0.08474746636584797, "compression_ratio": 1.6291666666666667, "no_speech_prob": 3.1381200642499607e-06}, {"id": 911, "seek": 297946, "start": 3002.46, "end": 3004.86, "text": " Because we all grew up with ImageNet.", "tokens": [1436, 321, 439, 6109, 493, 365, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.08474746636584797, "compression_ratio": 1.6291666666666667, "no_speech_prob": 3.1381200642499607e-06}, {"id": 912, "seek": 297946, "start": 3004.86, "end": 3007.06, "text": " And ImageNet was specifically curated,", "tokens": [400, 29903, 31890, 390, 4682, 47851, 11], "temperature": 0.0, "avg_logprob": -0.08474746636584797, "compression_ratio": 1.6291666666666667, "no_speech_prob": 3.1381200642499607e-06}, {"id": 913, "seek": 300706, "start": 3007.06, "end": 3011.86, "text": " so it only has one of the classes in ImageNet in it.", "tokens": [370, 309, 787, 575, 472, 295, 264, 5359, 294, 29903, 31890, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.11924948011125837, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.520640216767788e-05}, {"id": 914, "seek": 300706, "start": 3011.86, "end": 3016.86, "text": " And it always has one of those classes in it.", "tokens": [400, 309, 1009, 575, 472, 295, 729, 5359, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.11924948011125837, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.520640216767788e-05}, {"id": 915, "seek": 300706, "start": 3016.86, "end": 3021.66, "text": " An alternative, if you want to be able to handle the what if none", "tokens": [1107, 8535, 11, 498, 291, 528, 281, 312, 1075, 281, 4813, 264, 437, 498, 6022], "temperature": 0.0, "avg_logprob": -0.11924948011125837, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.520640216767788e-05}, {"id": 916, "seek": 300706, "start": 3021.66, "end": 3023.46, "text": " of these classes are in a case,", "tokens": [295, 613, 5359, 366, 294, 257, 1389, 11], "temperature": 0.0, "avg_logprob": -0.11924948011125837, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.520640216767788e-05}, {"id": 917, "seek": 300706, "start": 3023.46, "end": 3026.46, "text": " is you could create another category called background", "tokens": [307, 291, 727, 1884, 1071, 7719, 1219, 3678], "temperature": 0.0, "avg_logprob": -0.11924948011125837, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.520640216767788e-05}, {"id": 918, "seek": 300706, "start": 3026.46, "end": 3030.86, "text": " or doesn't exist or null or missing.", "tokens": [420, 1177, 380, 2514, 420, 18184, 420, 5361, 13], "temperature": 0.0, "avg_logprob": -0.11924948011125837, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.520640216767788e-05}, {"id": 919, "seek": 300706, "start": 3030.86, "end": 3033.06, "text": " So let's say you created this missing category.", "tokens": [407, 718, 311, 584, 291, 2942, 341, 5361, 7719, 13], "temperature": 0.0, "avg_logprob": -0.11924948011125837, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.520640216767788e-05}, {"id": 920, "seek": 303306, "start": 3033.06, "end": 3039.06, "text": " So now there's six, cat, dog, plane, fish, building or missing.", "tokens": [407, 586, 456, 311, 2309, 11, 3857, 11, 3000, 11, 5720, 11, 3506, 11, 2390, 420, 5361, 13], "temperature": 0.0, "avg_logprob": -0.09921286826909975, "compression_ratio": 1.5693069306930694, "no_speech_prob": 4.029313004139112e-06}, {"id": 921, "seek": 303306, "start": 3039.06, "end": 3044.06, "text": " Nothing. A lot of researchers have tried that.", "tokens": [6693, 13, 316, 688, 295, 10309, 362, 3031, 300, 13], "temperature": 0.0, "avg_logprob": -0.09921286826909975, "compression_ratio": 1.5693069306930694, "no_speech_prob": 4.029313004139112e-06}, {"id": 922, "seek": 303306, "start": 3044.06, "end": 3047.86, "text": " But it's actually a terrible idea and it doesn't work.", "tokens": [583, 309, 311, 767, 257, 6237, 1558, 293, 309, 1177, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.09921286826909975, "compression_ratio": 1.5693069306930694, "no_speech_prob": 4.029313004139112e-06}, {"id": 923, "seek": 303306, "start": 3047.86, "end": 3051.06, "text": " And the reason it doesn't work is because to be able", "tokens": [400, 264, 1778, 309, 1177, 380, 589, 307, 570, 281, 312, 1075], "temperature": 0.0, "avg_logprob": -0.09921286826909975, "compression_ratio": 1.5693069306930694, "no_speech_prob": 4.029313004139112e-06}, {"id": 924, "seek": 303306, "start": 3051.06, "end": 3054.2599999999998, "text": " to successfully predict missing,", "tokens": [281, 10727, 6069, 5361, 11], "temperature": 0.0, "avg_logprob": -0.09921286826909975, "compression_ratio": 1.5693069306930694, "no_speech_prob": 4.029313004139112e-06}, {"id": 925, "seek": 303306, "start": 3054.2599999999998, "end": 3058.66, "text": " the penultimate layer activations have to have the features in it", "tokens": [264, 3435, 723, 2905, 4583, 2430, 763, 362, 281, 362, 264, 4122, 294, 309], "temperature": 0.0, "avg_logprob": -0.09921286826909975, "compression_ratio": 1.5693069306930694, "no_speech_prob": 4.029313004139112e-06}, {"id": 926, "seek": 305866, "start": 3058.66, "end": 3064.06, "text": " that is what a not cat, dog, plane, fish, fish or building looks like.", "tokens": [300, 307, 437, 257, 406, 3857, 11, 3000, 11, 5720, 11, 3506, 11, 3506, 420, 2390, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.07546760819175026, "compression_ratio": 1.8995433789954337, "no_speech_prob": 3.2376419767388143e-06}, {"id": 927, "seek": 305866, "start": 3064.06, "end": 3068.2599999999998, "text": " So how do you describe a not cat, dog, plane, fish or building?", "tokens": [407, 577, 360, 291, 6786, 257, 406, 3857, 11, 3000, 11, 5720, 11, 3506, 420, 2390, 30], "temperature": 0.0, "avg_logprob": -0.07546760819175026, "compression_ratio": 1.8995433789954337, "no_speech_prob": 3.2376419767388143e-06}, {"id": 928, "seek": 305866, "start": 3068.2599999999998, "end": 3069.8599999999997, "text": " What are the things that would activate high?", "tokens": [708, 366, 264, 721, 300, 576, 13615, 1090, 30], "temperature": 0.0, "avg_logprob": -0.07546760819175026, "compression_ratio": 1.8995433789954337, "no_speech_prob": 3.2376419767388143e-06}, {"id": 929, "seek": 305866, "start": 3069.8599999999997, "end": 3071.06, "text": " Is it shininess?", "tokens": [1119, 309, 37124, 1324, 30], "temperature": 0.0, "avg_logprob": -0.07546760819175026, "compression_ratio": 1.8995433789954337, "no_speech_prob": 3.2376419767388143e-06}, {"id": 930, "seek": 305866, "start": 3071.06, "end": 3071.8599999999997, "text": " Is it fur?", "tokens": [1119, 309, 2687, 30], "temperature": 0.0, "avg_logprob": -0.07546760819175026, "compression_ratio": 1.8995433789954337, "no_speech_prob": 3.2376419767388143e-06}, {"id": 931, "seek": 305866, "start": 3071.8599999999997, "end": 3073.06, "text": " Is it sunshine?", "tokens": [1119, 309, 25219, 30], "temperature": 0.0, "avg_logprob": -0.07546760819175026, "compression_ratio": 1.8995433789954337, "no_speech_prob": 3.2376419767388143e-06}, {"id": 932, "seek": 305866, "start": 3073.06, "end": 3075.06, "text": " Is it edges?", "tokens": [1119, 309, 8819, 30], "temperature": 0.0, "avg_logprob": -0.07546760819175026, "compression_ratio": 1.8995433789954337, "no_speech_prob": 3.2376419767388143e-06}, {"id": 933, "seek": 305866, "start": 3075.06, "end": 3077.06, "text": " No. There's none of those things.", "tokens": [883, 13, 821, 311, 6022, 295, 729, 721, 13], "temperature": 0.0, "avg_logprob": -0.07546760819175026, "compression_ratio": 1.8995433789954337, "no_speech_prob": 3.2376419767388143e-06}, {"id": 934, "seek": 305866, "start": 3077.06, "end": 3079.8599999999997, "text": " There is no set of features that when they're all high,", "tokens": [821, 307, 572, 992, 295, 4122, 300, 562, 436, 434, 439, 1090, 11], "temperature": 0.0, "avg_logprob": -0.07546760819175026, "compression_ratio": 1.8995433789954337, "no_speech_prob": 3.2376419767388143e-06}, {"id": 935, "seek": 305866, "start": 3079.8599999999997, "end": 3083.2599999999998, "text": " is clearly a not cat, dog, plane, fish or building.", "tokens": [307, 4448, 257, 406, 3857, 11, 3000, 11, 5720, 11, 3506, 420, 2390, 13], "temperature": 0.0, "avg_logprob": -0.07546760819175026, "compression_ratio": 1.8995433789954337, "no_speech_prob": 3.2376419767388143e-06}, {"id": 936, "seek": 305866, "start": 3083.2599999999998, "end": 3085.66, "text": " So that's just not a kind of object.", "tokens": [407, 300, 311, 445, 406, 257, 733, 295, 2657, 13], "temperature": 0.0, "avg_logprob": -0.07546760819175026, "compression_ratio": 1.8995433789954337, "no_speech_prob": 3.2376419767388143e-06}, {"id": 937, "seek": 308566, "start": 3085.66, "end": 3090.8599999999997, "text": " So a neural net can kind of try to hack its way around it", "tokens": [407, 257, 18161, 2533, 393, 733, 295, 853, 281, 10339, 1080, 636, 926, 309], "temperature": 0.0, "avg_logprob": -0.07466074477794558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 1.0952070624625776e-05}, {"id": 938, "seek": 308566, "start": 3090.8599999999997, "end": 3095.06, "text": " by creating a negative model of every other single type", "tokens": [538, 4084, 257, 3671, 2316, 295, 633, 661, 2167, 2010], "temperature": 0.0, "avg_logprob": -0.07466074477794558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 1.0952070624625776e-05}, {"id": 939, "seek": 308566, "start": 3095.06, "end": 3097.8599999999997, "text": " and create a kind of not one of any of those other things.", "tokens": [293, 1884, 257, 733, 295, 406, 472, 295, 604, 295, 729, 661, 721, 13], "temperature": 0.0, "avg_logprob": -0.07466074477794558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 1.0952070624625776e-05}, {"id": 940, "seek": 308566, "start": 3097.8599999999997, "end": 3099.66, "text": " But that's very hard for it.", "tokens": [583, 300, 311, 588, 1152, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.07466074477794558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 1.0952070624625776e-05}, {"id": 941, "seek": 308566, "start": 3099.66, "end": 3103.8599999999997, "text": " Whereas creating simply a binomial, does it", "tokens": [13813, 4084, 2935, 257, 5171, 47429, 11, 775, 309], "temperature": 0.0, "avg_logprob": -0.07466074477794558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 1.0952070624625776e-05}, {"id": 942, "seek": 308566, "start": 3103.8599999999997, "end": 3108.06, "text": " or doesn't it have this for every one of the classes,", "tokens": [420, 1177, 380, 309, 362, 341, 337, 633, 472, 295, 264, 5359, 11], "temperature": 0.0, "avg_logprob": -0.07466074477794558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 1.0952070624625776e-05}, {"id": 943, "seek": 308566, "start": 3108.06, "end": 3109.46, "text": " is really easy for it, right?", "tokens": [307, 534, 1858, 337, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.07466074477794558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 1.0952070624625776e-05}, {"id": 944, "seek": 308566, "start": 3109.46, "end": 3110.8599999999997, "text": " Because it just doesn't have a cat.", "tokens": [1436, 309, 445, 1177, 380, 362, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.07466074477794558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 1.0952070624625776e-05}, {"id": 945, "seek": 308566, "start": 3110.8599999999997, "end": 3111.46, "text": " Yes or no?", "tokens": [1079, 420, 572, 30], "temperature": 0.0, "avg_logprob": -0.07466074477794558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 1.0952070624625776e-05}, {"id": 946, "seek": 308566, "start": 3111.46, "end": 3112.06, "text": " Does it have a dog?", "tokens": [4402, 309, 362, 257, 3000, 30], "temperature": 0.0, "avg_logprob": -0.07466074477794558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 1.0952070624625776e-05}, {"id": 947, "seek": 308566, "start": 3112.06, "end": 3113.2599999999998, "text": " Yes or no?", "tokens": [1079, 420, 572, 30], "temperature": 0.0, "avg_logprob": -0.07466074477794558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 1.0952070624625776e-05}, {"id": 948, "seek": 308566, "start": 3113.2599999999998, "end": 3114.66, "text": " And so forth.", "tokens": [400, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.07466074477794558, "compression_ratio": 1.7113821138211383, "no_speech_prob": 1.0952070624625776e-05}, {"id": 949, "seek": 311466, "start": 3114.66, "end": 3122.06, "text": " So lots and lots of well regarded academic papers", "tokens": [407, 3195, 293, 3195, 295, 731, 26047, 7778, 10577], "temperature": 0.0, "avg_logprob": -0.11717513963287952, "compression_ratio": 1.6651982378854626, "no_speech_prob": 1.5445017197635025e-05}, {"id": 950, "seek": 311466, "start": 3122.06, "end": 3124.46, "text": " make this mistake.", "tokens": [652, 341, 6146, 13], "temperature": 0.0, "avg_logprob": -0.11717513963287952, "compression_ratio": 1.6651982378854626, "no_speech_prob": 1.5445017197635025e-05}, {"id": 951, "seek": 311466, "start": 3124.46, "end": 3126.46, "text": " So look out for it.", "tokens": [407, 574, 484, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.11717513963287952, "compression_ratio": 1.6651982378854626, "no_speech_prob": 1.5445017197635025e-05}, {"id": 952, "seek": 311466, "start": 3126.46, "end": 3128.66, "text": " And if you do come across an academic paper that's using", "tokens": [400, 498, 291, 360, 808, 2108, 364, 7778, 3035, 300, 311, 1228], "temperature": 0.0, "avg_logprob": -0.11717513963287952, "compression_ratio": 1.6651982378854626, "no_speech_prob": 1.5445017197635025e-05}, {"id": 953, "seek": 311466, "start": 3128.66, "end": 3134.06, "text": " Softmax and you think, does that actually work with Softmax?", "tokens": [16985, 41167, 293, 291, 519, 11, 775, 300, 767, 589, 365, 16985, 41167, 30], "temperature": 0.0, "avg_logprob": -0.11717513963287952, "compression_ratio": 1.6651982378854626, "no_speech_prob": 1.5445017197635025e-05}, {"id": 954, "seek": 311466, "start": 3134.06, "end": 3135.66, "text": " And you think maybe the answer is no.", "tokens": [400, 291, 519, 1310, 264, 1867, 307, 572, 13], "temperature": 0.0, "avg_logprob": -0.11717513963287952, "compression_ratio": 1.6651982378854626, "no_speech_prob": 1.5445017197635025e-05}, {"id": 955, "seek": 311466, "start": 3135.66, "end": 3137.46, "text": " Try replicating it without Softmax.", "tokens": [6526, 3248, 30541, 309, 1553, 16985, 41167, 13], "temperature": 0.0, "avg_logprob": -0.11717513963287952, "compression_ratio": 1.6651982378854626, "no_speech_prob": 1.5445017197635025e-05}, {"id": 956, "seek": 311466, "start": 3137.46, "end": 3140.2599999999998, "text": " And you may just find you get a better result.", "tokens": [400, 291, 815, 445, 915, 291, 483, 257, 1101, 1874, 13], "temperature": 0.0, "avg_logprob": -0.11717513963287952, "compression_ratio": 1.6651982378854626, "no_speech_prob": 1.5445017197635025e-05}, {"id": 957, "seek": 311466, "start": 3140.2599999999998, "end": 3142.8599999999997, "text": " An example of somewhere where Softmax is obviously", "tokens": [1107, 1365, 295, 4079, 689, 16985, 41167, 307, 2745], "temperature": 0.0, "avg_logprob": -0.11717513963287952, "compression_ratio": 1.6651982378854626, "no_speech_prob": 1.5445017197635025e-05}, {"id": 958, "seek": 314286, "start": 3142.86, "end": 3145.46, "text": " a good idea or something like Softmax is obviously", "tokens": [257, 665, 1558, 420, 746, 411, 16985, 41167, 307, 2745], "temperature": 0.0, "avg_logprob": -0.11422156451041238, "compression_ratio": 1.759433962264151, "no_speech_prob": 6.14390864939196e-06}, {"id": 959, "seek": 314286, "start": 3145.46, "end": 3146.86, "text": " a good idea.", "tokens": [257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.11422156451041238, "compression_ratio": 1.759433962264151, "no_speech_prob": 6.14390864939196e-06}, {"id": 960, "seek": 314286, "start": 3146.86, "end": 3148.26, "text": " Language modeling.", "tokens": [24445, 15983, 13], "temperature": 0.0, "avg_logprob": -0.11422156451041238, "compression_ratio": 1.759433962264151, "no_speech_prob": 6.14390864939196e-06}, {"id": 961, "seek": 314286, "start": 3148.26, "end": 3150.26, "text": " What's the next word?", "tokens": [708, 311, 264, 958, 1349, 30], "temperature": 0.0, "avg_logprob": -0.11422156451041238, "compression_ratio": 1.759433962264151, "no_speech_prob": 6.14390864939196e-06}, {"id": 962, "seek": 314286, "start": 3150.26, "end": 3152.46, "text": " It's definitely at least one word.", "tokens": [467, 311, 2138, 412, 1935, 472, 1349, 13], "temperature": 0.0, "avg_logprob": -0.11422156451041238, "compression_ratio": 1.759433962264151, "no_speech_prob": 6.14390864939196e-06}, {"id": 963, "seek": 314286, "start": 3152.46, "end": 3154.86, "text": " It's definitely not more than one word.", "tokens": [467, 311, 2138, 406, 544, 813, 472, 1349, 13], "temperature": 0.0, "avg_logprob": -0.11422156451041238, "compression_ratio": 1.759433962264151, "no_speech_prob": 6.14390864939196e-06}, {"id": 964, "seek": 314286, "start": 3154.86, "end": 3156.86, "text": " So you want Softmax.", "tokens": [407, 291, 528, 16985, 41167, 13], "temperature": 0.0, "avg_logprob": -0.11422156451041238, "compression_ratio": 1.759433962264151, "no_speech_prob": 6.14390864939196e-06}, {"id": 965, "seek": 314286, "start": 3156.86, "end": 3159.06, "text": " So I'm not saying Softmax is always a dumb idea.", "tokens": [407, 286, 478, 406, 1566, 16985, 41167, 307, 1009, 257, 10316, 1558, 13], "temperature": 0.0, "avg_logprob": -0.11422156451041238, "compression_ratio": 1.759433962264151, "no_speech_prob": 6.14390864939196e-06}, {"id": 966, "seek": 314286, "start": 3159.06, "end": 3161.86, "text": " But it's often a dumb idea.", "tokens": [583, 309, 311, 2049, 257, 10316, 1558, 13], "temperature": 0.0, "avg_logprob": -0.11422156451041238, "compression_ratio": 1.759433962264151, "no_speech_prob": 6.14390864939196e-06}, {"id": 967, "seek": 314286, "start": 3161.86, "end": 3163.26, "text": " So that's something to look out for.", "tokens": [407, 300, 311, 746, 281, 574, 484, 337, 13], "temperature": 0.0, "avg_logprob": -0.11422156451041238, "compression_ratio": 1.759433962264151, "no_speech_prob": 6.14390864939196e-06}, {"id": 968, "seek": 314286, "start": 3167.86, "end": 3171.26, "text": " Next thing I want to do is I want to build a learning rate", "tokens": [3087, 551, 286, 528, 281, 360, 307, 286, 528, 281, 1322, 257, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.11422156451041238, "compression_ratio": 1.759433962264151, "no_speech_prob": 6.14390864939196e-06}, {"id": 969, "seek": 317126, "start": 3171.26, "end": 3172.86, "text": " finder.", "tokens": [915, 260, 13], "temperature": 0.0, "avg_logprob": -0.07662863783783964, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.3186203179648146e-05}, {"id": 970, "seek": 317126, "start": 3172.86, "end": 3175.0600000000004, "text": " And to build a learning rate finder,", "tokens": [400, 281, 1322, 257, 2539, 3314, 915, 260, 11], "temperature": 0.0, "avg_logprob": -0.07662863783783964, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.3186203179648146e-05}, {"id": 971, "seek": 317126, "start": 3175.0600000000004, "end": 3181.0600000000004, "text": " we need to use this test callback kind of idea,", "tokens": [321, 643, 281, 764, 341, 1500, 818, 3207, 733, 295, 1558, 11], "temperature": 0.0, "avg_logprob": -0.07662863783783964, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.3186203179648146e-05}, {"id": 972, "seek": 317126, "start": 3181.0600000000004, "end": 3184.6600000000003, "text": " this ability to stop somewhere.", "tokens": [341, 3485, 281, 1590, 4079, 13], "temperature": 0.0, "avg_logprob": -0.07662863783783964, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.3186203179648146e-05}, {"id": 973, "seek": 317126, "start": 3184.6600000000003, "end": 3187.6600000000003, "text": " Problem is, as you may have noticed,", "tokens": [11676, 307, 11, 382, 291, 815, 362, 5694, 11], "temperature": 0.0, "avg_logprob": -0.07662863783783964, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.3186203179648146e-05}, {"id": 974, "seek": 317126, "start": 3187.6600000000003, "end": 3190.86, "text": " this I want to stop somewhere callback", "tokens": [341, 286, 528, 281, 1590, 4079, 818, 3207], "temperature": 0.0, "avg_logprob": -0.07662863783783964, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.3186203179648146e-05}, {"id": 975, "seek": 317126, "start": 3190.86, "end": 3193.86, "text": " wasn't working in our new refactoring", "tokens": [2067, 380, 1364, 294, 527, 777, 1895, 578, 3662], "temperature": 0.0, "avg_logprob": -0.07662863783783964, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.3186203179648146e-05}, {"id": 976, "seek": 317126, "start": 3193.86, "end": 3195.86, "text": " where we created this runner class.", "tokens": [689, 321, 2942, 341, 24376, 1508, 13], "temperature": 0.0, "avg_logprob": -0.07662863783783964, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.3186203179648146e-05}, {"id": 977, "seek": 317126, "start": 3195.86, "end": 3197.46, "text": " And the reason it wasn't working is", "tokens": [400, 264, 1778, 309, 2067, 380, 1364, 307], "temperature": 0.0, "avg_logprob": -0.07662863783783964, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.3186203179648146e-05}, {"id": 978, "seek": 319746, "start": 3197.46, "end": 3202.86, "text": " because we were turning true to mean cancel.", "tokens": [570, 321, 645, 6246, 2074, 281, 914, 10373, 13], "temperature": 0.0, "avg_logprob": -0.08143764431193723, "compression_ratio": 1.7587719298245614, "no_speech_prob": 1.473826341680251e-05}, {"id": 979, "seek": 319746, "start": 3202.86, "end": 3205.26, "text": " But even after we do that, it still", "tokens": [583, 754, 934, 321, 360, 300, 11, 309, 920], "temperature": 0.0, "avg_logprob": -0.08143764431193723, "compression_ratio": 1.7587719298245614, "no_speech_prob": 1.473826341680251e-05}, {"id": 980, "seek": 319746, "start": 3205.26, "end": 3207.46, "text": " goes on to do the next batch.", "tokens": [1709, 322, 281, 360, 264, 958, 15245, 13], "temperature": 0.0, "avg_logprob": -0.08143764431193723, "compression_ratio": 1.7587719298245614, "no_speech_prob": 1.473826341680251e-05}, {"id": 981, "seek": 319746, "start": 3207.46, "end": 3210.46, "text": " And even if we set self.stop, even after we do that,", "tokens": [400, 754, 498, 321, 992, 2698, 13, 13559, 11, 754, 934, 321, 360, 300, 11], "temperature": 0.0, "avg_logprob": -0.08143764431193723, "compression_ratio": 1.7587719298245614, "no_speech_prob": 1.473826341680251e-05}, {"id": 982, "seek": 319746, "start": 3210.46, "end": 3212.06, "text": " it'll go on to the next epoch.", "tokens": [309, 603, 352, 322, 281, 264, 958, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.08143764431193723, "compression_ratio": 1.7587719298245614, "no_speech_prob": 1.473826341680251e-05}, {"id": 983, "seek": 319746, "start": 3212.06, "end": 3213.66, "text": " So to actually stop it, you would", "tokens": [407, 281, 767, 1590, 309, 11, 291, 576], "temperature": 0.0, "avg_logprob": -0.08143764431193723, "compression_ratio": 1.7587719298245614, "no_speech_prob": 1.473826341680251e-05}, {"id": 984, "seek": 319746, "start": 3213.66, "end": 3218.78, "text": " have to return false from every single callback that's checked", "tokens": [362, 281, 2736, 7908, 490, 633, 2167, 818, 3207, 300, 311, 10033], "temperature": 0.0, "avg_logprob": -0.08143764431193723, "compression_ratio": 1.7587719298245614, "no_speech_prob": 1.473826341680251e-05}, {"id": 985, "seek": 319746, "start": 3218.78, "end": 3220.46, "text": " to make sure it really stops.", "tokens": [281, 652, 988, 309, 534, 10094, 13], "temperature": 0.0, "avg_logprob": -0.08143764431193723, "compression_ratio": 1.7587719298245614, "no_speech_prob": 1.473826341680251e-05}, {"id": 986, "seek": 319746, "start": 3220.46, "end": 3223.86, "text": " Or you would have to add something that checks for self.stop", "tokens": [1610, 291, 576, 362, 281, 909, 746, 300, 13834, 337, 2698, 13, 13559], "temperature": 0.0, "avg_logprob": -0.08143764431193723, "compression_ratio": 1.7587719298245614, "no_speech_prob": 1.473826341680251e-05}, {"id": 987, "seek": 319746, "start": 3223.86, "end": 3225.06, "text": " in lots of places.", "tokens": [294, 3195, 295, 3190, 13], "temperature": 0.0, "avg_logprob": -0.08143764431193723, "compression_ratio": 1.7587719298245614, "no_speech_prob": 1.473826341680251e-05}, {"id": 988, "seek": 322506, "start": 3225.06, "end": 3228.2599999999998, "text": " It would be a real pain.", "tokens": [467, 576, 312, 257, 957, 1822, 13], "temperature": 0.0, "avg_logprob": -0.12972932248502164, "compression_ratio": 1.6680161943319838, "no_speech_prob": 7.182902663771529e-06}, {"id": 989, "seek": 322506, "start": 3228.2599999999998, "end": 3230.86, "text": " It's also not as flexible as we would like.", "tokens": [467, 311, 611, 406, 382, 11358, 382, 321, 576, 411, 13], "temperature": 0.0, "avg_logprob": -0.12972932248502164, "compression_ratio": 1.6680161943319838, "no_speech_prob": 7.182902663771529e-06}, {"id": 990, "seek": 322506, "start": 3230.86, "end": 3232.7599999999998, "text": " So what I want to show you today is something", "tokens": [407, 437, 286, 528, 281, 855, 291, 965, 307, 746], "temperature": 0.0, "avg_logprob": -0.12972932248502164, "compression_ratio": 1.6680161943319838, "no_speech_prob": 7.182902663771529e-06}, {"id": 991, "seek": 322506, "start": 3232.7599999999998, "end": 3234.56, "text": " which I think is really interesting, which", "tokens": [597, 286, 519, 307, 534, 1880, 11, 597], "temperature": 0.0, "avg_logprob": -0.12972932248502164, "compression_ratio": 1.6680161943319838, "no_speech_prob": 7.182902663771529e-06}, {"id": 992, "seek": 322506, "start": 3234.56, "end": 3242.06, "text": " is using the idea of exceptions as a kind of control flow", "tokens": [307, 1228, 264, 1558, 295, 22847, 382, 257, 733, 295, 1969, 3095], "temperature": 0.0, "avg_logprob": -0.12972932248502164, "compression_ratio": 1.6680161943319838, "no_speech_prob": 7.182902663771529e-06}, {"id": 993, "seek": 322506, "start": 3242.06, "end": 3243.66, "text": " statement.", "tokens": [5629, 13], "temperature": 0.0, "avg_logprob": -0.12972932248502164, "compression_ratio": 1.6680161943319838, "no_speech_prob": 7.182902663771529e-06}, {"id": 994, "seek": 322506, "start": 3243.66, "end": 3245.36, "text": " You may think of exceptions as just", "tokens": [509, 815, 519, 295, 22847, 382, 445], "temperature": 0.0, "avg_logprob": -0.12972932248502164, "compression_ratio": 1.6680161943319838, "no_speech_prob": 7.182902663771529e-06}, {"id": 995, "seek": 322506, "start": 3245.36, "end": 3247.66, "text": " being a way of handling errors.", "tokens": [885, 257, 636, 295, 13175, 13603, 13], "temperature": 0.0, "avg_logprob": -0.12972932248502164, "compression_ratio": 1.6680161943319838, "no_speech_prob": 7.182902663771529e-06}, {"id": 996, "seek": 322506, "start": 3247.66, "end": 3250.2599999999998, "text": " But actually, exceptions are a very versatile way", "tokens": [583, 767, 11, 22847, 366, 257, 588, 25057, 636], "temperature": 0.0, "avg_logprob": -0.12972932248502164, "compression_ratio": 1.6680161943319838, "no_speech_prob": 7.182902663771529e-06}, {"id": 997, "seek": 322506, "start": 3250.2599999999998, "end": 3253.1, "text": " of writing very neat code that will", "tokens": [295, 3579, 588, 10654, 3089, 300, 486], "temperature": 0.0, "avg_logprob": -0.12972932248502164, "compression_ratio": 1.6680161943319838, "no_speech_prob": 7.182902663771529e-06}, {"id": 998, "seek": 322506, "start": 3253.1, "end": 3255.02, "text": " be very helpful for your users.", "tokens": [312, 588, 4961, 337, 428, 5022, 13], "temperature": 0.0, "avg_logprob": -0.12972932248502164, "compression_ratio": 1.6680161943319838, "no_speech_prob": 7.182902663771529e-06}, {"id": 999, "seek": 325502, "start": 3255.02, "end": 3256.2599999999998, "text": " Let me show you what I mean.", "tokens": [961, 385, 855, 291, 437, 286, 914, 13], "temperature": 0.0, "avg_logprob": -0.19339749145507812, "compression_ratio": 1.632183908045977, "no_speech_prob": 1.8057762645184994e-05}, {"id": 1000, "seek": 325502, "start": 3258.96, "end": 3261.96, "text": " So let's start by just grabbing our MNIST data set as before", "tokens": [407, 718, 311, 722, 538, 445, 23771, 527, 376, 45, 19756, 1412, 992, 382, 949], "temperature": 0.0, "avg_logprob": -0.19339749145507812, "compression_ratio": 1.632183908045977, "no_speech_prob": 1.8057762645184994e-05}, {"id": 1001, "seek": 325502, "start": 3261.96, "end": 3263.86, "text": " and creating our data bunches before.", "tokens": [293, 4084, 527, 1412, 3840, 279, 949, 13], "temperature": 0.0, "avg_logprob": -0.19339749145507812, "compression_ratio": 1.632183908045977, "no_speech_prob": 1.8057762645184994e-05}, {"id": 1002, "seek": 325502, "start": 3263.86, "end": 3266.5, "text": " And here's our callback as before and our train eval callback", "tokens": [400, 510, 311, 527, 818, 3207, 382, 949, 293, 527, 3847, 1073, 304, 818, 3207], "temperature": 0.0, "avg_logprob": -0.19339749145507812, "compression_ratio": 1.632183908045977, "no_speech_prob": 1.8057762645184994e-05}, {"id": 1003, "seek": 325502, "start": 3266.5, "end": 3268.96, "text": " as before.", "tokens": [382, 949, 13], "temperature": 0.0, "avg_logprob": -0.19339749145507812, "compression_ratio": 1.632183908045977, "no_speech_prob": 1.8057762645184994e-05}, {"id": 1004, "seek": 325502, "start": 3268.96, "end": 3272.7599999999998, "text": " But there's a couple of things I'm going to do differently.", "tokens": [583, 456, 311, 257, 1916, 295, 721, 286, 478, 516, 281, 360, 7614, 13], "temperature": 0.0, "avg_logprob": -0.19339749145507812, "compression_ratio": 1.632183908045977, "no_speech_prob": 1.8057762645184994e-05}, {"id": 1005, "seek": 325502, "start": 3272.7599999999998, "end": 3274.66, "text": " The first is, and this is a bit unrelated,", "tokens": [440, 700, 307, 11, 293, 341, 307, 257, 857, 38967, 11], "temperature": 0.0, "avg_logprob": -0.19339749145507812, "compression_ratio": 1.632183908045977, "no_speech_prob": 1.8057762645184994e-05}, {"id": 1006, "seek": 325502, "start": 3274.66, "end": 3276.56, "text": " but I think it's a useful refactoring,", "tokens": [457, 286, 519, 309, 311, 257, 4420, 1895, 578, 3662, 11], "temperature": 0.0, "avg_logprob": -0.19339749145507812, "compression_ratio": 1.632183908045977, "no_speech_prob": 1.8057762645184994e-05}, {"id": 1007, "seek": 325502, "start": 3276.56, "end": 3281.56, "text": " is previously inside runner in Dunder Call,", "tokens": [307, 8046, 1854, 24376, 294, 413, 6617, 7807, 11], "temperature": 0.0, "avg_logprob": -0.19339749145507812, "compression_ratio": 1.632183908045977, "no_speech_prob": 1.8057762645184994e-05}, {"id": 1008, "seek": 325502, "start": 3281.56, "end": 3284.7599999999998, "text": " we went through each callback in order.", "tokens": [321, 1437, 807, 1184, 818, 3207, 294, 1668, 13], "temperature": 0.0, "avg_logprob": -0.19339749145507812, "compression_ratio": 1.632183908045977, "no_speech_prob": 1.8057762645184994e-05}, {"id": 1009, "seek": 328476, "start": 3284.76, "end": 3288.26, "text": " And we checked to see whether it existed,", "tokens": [400, 321, 10033, 281, 536, 1968, 309, 13135, 11], "temperature": 0.0, "avg_logprob": -0.13638036809069046, "compression_ratio": 1.7878787878787878, "no_speech_prob": 6.642851985816378e-06}, {"id": 1010, "seek": 328476, "start": 3288.26, "end": 3291.36, "text": " whether that particular method exists in that callback.", "tokens": [1968, 300, 1729, 3170, 8198, 294, 300, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.13638036809069046, "compression_ratio": 1.7878787878787878, "no_speech_prob": 6.642851985816378e-06}, {"id": 1011, "seek": 328476, "start": 3291.36, "end": 3294.36, "text": " And if it was, we called it and checked", "tokens": [400, 498, 309, 390, 11, 321, 1219, 309, 293, 10033], "temperature": 0.0, "avg_logprob": -0.13638036809069046, "compression_ratio": 1.7878787878787878, "no_speech_prob": 6.642851985816378e-06}, {"id": 1012, "seek": 328476, "start": 3294.36, "end": 3295.76, "text": " whether it returns true or false.", "tokens": [1968, 309, 11247, 2074, 420, 7908, 13], "temperature": 0.0, "avg_logprob": -0.13638036809069046, "compression_ratio": 1.7878787878787878, "no_speech_prob": 6.642851985816378e-06}, {"id": 1013, "seek": 328476, "start": 3298.6200000000003, "end": 3300.76, "text": " It actually makes more sense for this", "tokens": [467, 767, 1669, 544, 2020, 337, 341], "temperature": 0.0, "avg_logprob": -0.13638036809069046, "compression_ratio": 1.7878787878787878, "no_speech_prob": 6.642851985816378e-06}, {"id": 1014, "seek": 328476, "start": 3300.76, "end": 3307.6600000000003, "text": " to be inside the callback class, not inside the runner class.", "tokens": [281, 312, 1854, 264, 818, 3207, 1508, 11, 406, 1854, 264, 24376, 1508, 13], "temperature": 0.0, "avg_logprob": -0.13638036809069046, "compression_ratio": 1.7878787878787878, "no_speech_prob": 6.642851985816378e-06}, {"id": 1015, "seek": 328476, "start": 3307.6600000000003, "end": 3309.6200000000003, "text": " Because by putting it into the callback class,", "tokens": [1436, 538, 3372, 309, 666, 264, 818, 3207, 1508, 11], "temperature": 0.0, "avg_logprob": -0.13638036809069046, "compression_ratio": 1.7878787878787878, "no_speech_prob": 6.642851985816378e-06}, {"id": 1016, "seek": 328476, "start": 3309.6200000000003, "end": 3313.1600000000003, "text": " the callback class is now taking a,", "tokens": [264, 818, 3207, 1508, 307, 586, 1940, 257, 11], "temperature": 0.0, "avg_logprob": -0.13638036809069046, "compression_ratio": 1.7878787878787878, "no_speech_prob": 6.642851985816378e-06}, {"id": 1017, "seek": 331316, "start": 3313.16, "end": 3315.52, "text": " has a Dunder Call which takes a callback name,", "tokens": [575, 257, 413, 6617, 7807, 597, 2516, 257, 818, 3207, 1315, 11], "temperature": 0.0, "avg_logprob": -0.13406785329182944, "compression_ratio": 1.8689320388349515, "no_speech_prob": 7.766640919726342e-06}, {"id": 1018, "seek": 331316, "start": 3315.52, "end": 3318.3199999999997, "text": " and it can do this stuff.", "tokens": [293, 309, 393, 360, 341, 1507, 13], "temperature": 0.0, "avg_logprob": -0.13406785329182944, "compression_ratio": 1.8689320388349515, "no_speech_prob": 7.766640919726342e-06}, {"id": 1019, "seek": 331316, "start": 3318.3199999999997, "end": 3322.8599999999997, "text": " And what it means is that now your users who", "tokens": [400, 437, 309, 1355, 307, 300, 586, 428, 5022, 567], "temperature": 0.0, "avg_logprob": -0.13406785329182944, "compression_ratio": 1.8689320388349515, "no_speech_prob": 7.766640919726342e-06}, {"id": 1020, "seek": 331316, "start": 3322.8599999999997, "end": 3325.22, "text": " want to create their own callbacks,", "tokens": [528, 281, 1884, 641, 1065, 818, 17758, 11], "temperature": 0.0, "avg_logprob": -0.13406785329182944, "compression_ratio": 1.8689320388349515, "no_speech_prob": 7.766640919726342e-06}, {"id": 1021, "seek": 331316, "start": 3325.22, "end": 3328.8799999999997, "text": " let's say they wanted to create a callback that printed out", "tokens": [718, 311, 584, 436, 1415, 281, 1884, 257, 818, 3207, 300, 13567, 484], "temperature": 0.0, "avg_logprob": -0.13406785329182944, "compression_ratio": 1.8689320388349515, "no_speech_prob": 7.766640919726342e-06}, {"id": 1022, "seek": 331316, "start": 3328.8799999999997, "end": 3333.08, "text": " the callback name for every callback every time it was run.", "tokens": [264, 818, 3207, 1315, 337, 633, 818, 3207, 633, 565, 309, 390, 1190, 13], "temperature": 0.0, "avg_logprob": -0.13406785329182944, "compression_ratio": 1.8689320388349515, "no_speech_prob": 7.766640919726342e-06}, {"id": 1023, "seek": 331316, "start": 3333.08, "end": 3335.62, "text": " Or let's say they wanted to add a break point,", "tokens": [1610, 718, 311, 584, 436, 1415, 281, 909, 257, 1821, 935, 11], "temperature": 0.0, "avg_logprob": -0.13406785329182944, "compression_ratio": 1.8689320388349515, "no_speech_prob": 7.766640919726342e-06}, {"id": 1024, "seek": 331316, "start": 3335.62, "end": 3339.3799999999997, "text": " like a set trace, that happened every time the callback was", "tokens": [411, 257, 992, 13508, 11, 300, 2011, 633, 565, 264, 818, 3207, 390], "temperature": 0.0, "avg_logprob": -0.13406785329182944, "compression_ratio": 1.8689320388349515, "no_speech_prob": 7.766640919726342e-06}, {"id": 1025, "seek": 331316, "start": 3339.3799999999997, "end": 3340.12, "text": " run.", "tokens": [1190, 13], "temperature": 0.0, "avg_logprob": -0.13406785329182944, "compression_ratio": 1.8689320388349515, "no_speech_prob": 7.766640919726342e-06}, {"id": 1026, "seek": 334012, "start": 3340.12, "end": 3344.7599999999998, "text": " Or they could now create their own inherit from callback", "tokens": [1610, 436, 727, 586, 1884, 641, 1065, 21389, 490, 818, 3207], "temperature": 0.0, "avg_logprob": -0.13249690843665082, "compression_ratio": 1.76, "no_speech_prob": 3.18750517180888e-06}, {"id": 1027, "seek": 334012, "start": 3344.7599999999998, "end": 3349.08, "text": " and actually replace Dunder Call itself with something", "tokens": [293, 767, 7406, 413, 6617, 7807, 2564, 365, 746], "temperature": 0.0, "avg_logprob": -0.13249690843665082, "compression_ratio": 1.76, "no_speech_prob": 3.18750517180888e-06}, {"id": 1028, "seek": 334012, "start": 3349.08, "end": 3351.96, "text": " that added this behavior they want.", "tokens": [300, 3869, 341, 5223, 436, 528, 13], "temperature": 0.0, "avg_logprob": -0.13249690843665082, "compression_ratio": 1.76, "no_speech_prob": 3.18750517180888e-06}, {"id": 1029, "seek": 334012, "start": 3351.96, "end": 3354.2, "text": " Or they could add something that looks", "tokens": [1610, 436, 727, 909, 746, 300, 1542], "temperature": 0.0, "avg_logprob": -0.13249690843665082, "compression_ratio": 1.76, "no_speech_prob": 3.18750517180888e-06}, {"id": 1030, "seek": 334012, "start": 3354.2, "end": 3356.16, "text": " at three or four different callback names", "tokens": [412, 1045, 420, 1451, 819, 818, 3207, 5288], "temperature": 0.0, "avg_logprob": -0.13249690843665082, "compression_ratio": 1.76, "no_speech_prob": 3.18750517180888e-06}, {"id": 1031, "seek": 334012, "start": 3356.16, "end": 3357.72, "text": " and attaches to all of them.", "tokens": [293, 49404, 281, 439, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.13249690843665082, "compression_ratio": 1.76, "no_speech_prob": 3.18750517180888e-06}, {"id": 1032, "seek": 334012, "start": 3357.72, "end": 3361.8599999999997, "text": " So this is a nice little extra piece of flexibility.", "tokens": [407, 341, 307, 257, 1481, 707, 2857, 2522, 295, 12635, 13], "temperature": 0.0, "avg_logprob": -0.13249690843665082, "compression_ratio": 1.76, "no_speech_prob": 3.18750517180888e-06}, {"id": 1033, "seek": 334012, "start": 3361.8599999999997, "end": 3363.72, "text": " It's not the key thing I wanted to show you,", "tokens": [467, 311, 406, 264, 2141, 551, 286, 1415, 281, 855, 291, 11], "temperature": 0.0, "avg_logprob": -0.13249690843665082, "compression_ratio": 1.76, "no_speech_prob": 3.18750517180888e-06}, {"id": 1034, "seek": 334012, "start": 3363.72, "end": 3367.16, "text": " but it's an example of a nice little refactoring.", "tokens": [457, 309, 311, 364, 1365, 295, 257, 1481, 707, 1895, 578, 3662, 13], "temperature": 0.0, "avg_logprob": -0.13249690843665082, "compression_ratio": 1.76, "no_speech_prob": 3.18750517180888e-06}, {"id": 1035, "seek": 334012, "start": 3367.16, "end": 3368.56, "text": " The key thing I wanted to show you", "tokens": [440, 2141, 551, 286, 1415, 281, 855, 291], "temperature": 0.0, "avg_logprob": -0.13249690843665082, "compression_ratio": 1.76, "no_speech_prob": 3.18750517180888e-06}, {"id": 1036, "seek": 336856, "start": 3368.56, "end": 3373.36, "text": " is that I've created three new types of exception.", "tokens": [307, 300, 286, 600, 2942, 1045, 777, 3467, 295, 11183, 13], "temperature": 0.0, "avg_logprob": -0.13911418616771698, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1037, "seek": 336856, "start": 3373.36, "end": 3376.64, "text": " So an exception in Python is just a class", "tokens": [407, 364, 11183, 294, 15329, 307, 445, 257, 1508], "temperature": 0.0, "avg_logprob": -0.13911418616771698, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1038, "seek": 336856, "start": 3376.64, "end": 3378.44, "text": " that inherits from exception.", "tokens": [300, 9484, 1208, 490, 11183, 13], "temperature": 0.0, "avg_logprob": -0.13911418616771698, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1039, "seek": 336856, "start": 3378.44, "end": 3379.88, "text": " And most of the time, you don't have", "tokens": [400, 881, 295, 264, 565, 11, 291, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.13911418616771698, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1040, "seek": 336856, "start": 3379.88, "end": 3381.88, "text": " to give it any other behavior.", "tokens": [281, 976, 309, 604, 661, 5223, 13], "temperature": 0.0, "avg_logprob": -0.13911418616771698, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1041, "seek": 336856, "start": 3381.88, "end": 3384.64, "text": " So to create a class that's just like its parent,", "tokens": [407, 281, 1884, 257, 1508, 300, 311, 445, 411, 1080, 2596, 11], "temperature": 0.0, "avg_logprob": -0.13911418616771698, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1042, "seek": 336856, "start": 3384.64, "end": 3386.92, "text": " but it just has a new name and no more behavior,", "tokens": [457, 309, 445, 575, 257, 777, 1315, 293, 572, 544, 5223, 11], "temperature": 0.0, "avg_logprob": -0.13911418616771698, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1043, "seek": 336856, "start": 3386.92, "end": 3388.56, "text": " you just say pass.", "tokens": [291, 445, 584, 1320, 13], "temperature": 0.0, "avg_logprob": -0.13911418616771698, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1044, "seek": 336856, "start": 3388.56, "end": 3393.08, "text": " So pass means this has all the same attributes and everything", "tokens": [407, 1320, 1355, 341, 575, 439, 264, 912, 17212, 293, 1203], "temperature": 0.0, "avg_logprob": -0.13911418616771698, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1045, "seek": 336856, "start": 3393.08, "end": 3394.7599999999998, "text": " as the parent.", "tokens": [382, 264, 2596, 13], "temperature": 0.0, "avg_logprob": -0.13911418616771698, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1046, "seek": 336856, "start": 3394.7599999999998, "end": 3397.16, "text": " But it's got a different name.", "tokens": [583, 309, 311, 658, 257, 819, 1315, 13], "temperature": 0.0, "avg_logprob": -0.13911418616771698, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1047, "seek": 336856, "start": 3397.16, "end": 3398.44, "text": " So why do we do that?", "tokens": [407, 983, 360, 321, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.13911418616771698, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1048, "seek": 339844, "start": 3398.44, "end": 3402.2000000000003, "text": " Well, you might get a sense from the names cancelTrainException,", "tokens": [1042, 11, 291, 1062, 483, 257, 2020, 490, 264, 5288, 10373, 51, 7146, 11149, 7311, 11], "temperature": 0.0, "avg_logprob": -0.1343020512507512, "compression_ratio": 1.9141630901287554, "no_speech_prob": 1.1843009815493133e-05}, {"id": 1049, "seek": 339844, "start": 3402.2000000000003, "end": 3405.36, "text": " cancelEpochException, cancelBatchException.", "tokens": [10373, 36, 2259, 339, 11149, 7311, 11, 10373, 33, 852, 11149, 7311, 13], "temperature": 0.0, "avg_logprob": -0.1343020512507512, "compression_ratio": 1.9141630901287554, "no_speech_prob": 1.1843009815493133e-05}, {"id": 1050, "seek": 339844, "start": 3405.36, "end": 3408.76, "text": " The idea is that we're going to let people's callbacks cancel", "tokens": [440, 1558, 307, 300, 321, 434, 516, 281, 718, 561, 311, 818, 17758, 10373], "temperature": 0.0, "avg_logprob": -0.1343020512507512, "compression_ratio": 1.9141630901287554, "no_speech_prob": 1.1843009815493133e-05}, {"id": 1051, "seek": 339844, "start": 3408.76, "end": 3409.8, "text": " anything.", "tokens": [1340, 13], "temperature": 0.0, "avg_logprob": -0.1343020512507512, "compression_ratio": 1.9141630901287554, "no_speech_prob": 1.1843009815493133e-05}, {"id": 1052, "seek": 339844, "start": 3409.8, "end": 3411.68, "text": " It'll cancel at one of these levels.", "tokens": [467, 603, 10373, 412, 472, 295, 613, 4358, 13], "temperature": 0.0, "avg_logprob": -0.1343020512507512, "compression_ratio": 1.9141630901287554, "no_speech_prob": 1.1843009815493133e-05}, {"id": 1053, "seek": 339844, "start": 3411.68, "end": 3413.56, "text": " So if they cancel a batch, it will", "tokens": [407, 498, 436, 10373, 257, 15245, 11, 309, 486], "temperature": 0.0, "avg_logprob": -0.1343020512507512, "compression_ratio": 1.9141630901287554, "no_speech_prob": 1.1843009815493133e-05}, {"id": 1054, "seek": 339844, "start": 3413.56, "end": 3416.52, "text": " keep going with the next batch, but not finish this one.", "tokens": [1066, 516, 365, 264, 958, 15245, 11, 457, 406, 2413, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.1343020512507512, "compression_ratio": 1.9141630901287554, "no_speech_prob": 1.1843009815493133e-05}, {"id": 1055, "seek": 339844, "start": 3416.52, "end": 3420.28, "text": " If they cancel an epoch, it'll keep going with the next epoch.", "tokens": [759, 436, 10373, 364, 30992, 339, 11, 309, 603, 1066, 516, 365, 264, 958, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.1343020512507512, "compression_ratio": 1.9141630901287554, "no_speech_prob": 1.1843009815493133e-05}, {"id": 1056, "seek": 339844, "start": 3420.28, "end": 3421.8, "text": " That will cancel this one.", "tokens": [663, 486, 10373, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.1343020512507512, "compression_ratio": 1.9141630901287554, "no_speech_prob": 1.1843009815493133e-05}, {"id": 1057, "seek": 339844, "start": 3421.8, "end": 3426.4, "text": " cancelTrain will stop the training altogether.", "tokens": [10373, 51, 7146, 486, 1590, 264, 3097, 19051, 13], "temperature": 0.0, "avg_logprob": -0.1343020512507512, "compression_ratio": 1.9141630901287554, "no_speech_prob": 1.1843009815493133e-05}, {"id": 1058, "seek": 342640, "start": 3426.4, "end": 3429.04, "text": " So how would cancelTrainException work?", "tokens": [407, 577, 576, 10373, 51, 7146, 11149, 7311, 589, 30], "temperature": 0.0, "avg_logprob": -0.13641542351764183, "compression_ratio": 1.6095617529880477, "no_speech_prob": 7.2962379817909095e-06}, {"id": 1059, "seek": 342640, "start": 3429.04, "end": 3431.88, "text": " Well, here's the same runner where you had before.", "tokens": [1042, 11, 510, 311, 264, 912, 24376, 689, 291, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.13641542351764183, "compression_ratio": 1.6095617529880477, "no_speech_prob": 7.2962379817909095e-06}, {"id": 1060, "seek": 342640, "start": 3431.88, "end": 3436.32, "text": " But now, fit, we already had try finally", "tokens": [583, 586, 11, 3318, 11, 321, 1217, 632, 853, 2721], "temperature": 0.0, "avg_logprob": -0.13641542351764183, "compression_ratio": 1.6095617529880477, "no_speech_prob": 7.2962379817909095e-06}, {"id": 1061, "seek": 342640, "start": 3436.32, "end": 3440.64, "text": " to make sure that our after fit and removeLearner happened,", "tokens": [281, 652, 988, 300, 527, 934, 3318, 293, 4159, 11020, 22916, 2011, 11], "temperature": 0.0, "avg_logprob": -0.13641542351764183, "compression_ratio": 1.6095617529880477, "no_speech_prob": 7.2962379817909095e-06}, {"id": 1062, "seek": 342640, "start": 3440.64, "end": 3442.12, "text": " even if there was an exception.", "tokens": [754, 498, 456, 390, 364, 11183, 13], "temperature": 0.0, "avg_logprob": -0.13641542351764183, "compression_ratio": 1.6095617529880477, "no_speech_prob": 7.2962379817909095e-06}, {"id": 1063, "seek": 342640, "start": 3442.12, "end": 3447.2000000000003, "text": " I've added one line of code, except cancelTrainException.", "tokens": [286, 600, 3869, 472, 1622, 295, 3089, 11, 3993, 10373, 51, 7146, 11149, 7311, 13], "temperature": 0.0, "avg_logprob": -0.13641542351764183, "compression_ratio": 1.6095617529880477, "no_speech_prob": 7.2962379817909095e-06}, {"id": 1064, "seek": 342640, "start": 3447.2000000000003, "end": 3449.4, "text": " And if that happens, then optionally, it", "tokens": [400, 498, 300, 2314, 11, 550, 3614, 379, 11, 309], "temperature": 0.0, "avg_logprob": -0.13641542351764183, "compression_ratio": 1.6095617529880477, "no_speech_prob": 7.2962379817909095e-06}, {"id": 1065, "seek": 342640, "start": 3449.4, "end": 3452.1600000000003, "text": " could call some after cancelTrainCallback.", "tokens": [727, 818, 512, 934, 10373, 51, 7146, 46113, 3207, 13], "temperature": 0.0, "avg_logprob": -0.13641542351764183, "compression_ratio": 1.6095617529880477, "no_speech_prob": 7.2962379817909095e-06}, {"id": 1066, "seek": 342640, "start": 3452.1600000000003, "end": 3455.88, "text": " But most importantly, no error occurs.", "tokens": [583, 881, 8906, 11, 572, 6713, 11843, 13], "temperature": 0.0, "avg_logprob": -0.13641542351764183, "compression_ratio": 1.6095617529880477, "no_speech_prob": 7.2962379817909095e-06}, {"id": 1067, "seek": 345588, "start": 3455.88, "end": 3459.6, "text": " It just keeps on going to the finally block", "tokens": [467, 445, 5965, 322, 516, 281, 264, 2721, 3461], "temperature": 0.0, "avg_logprob": -0.12154388427734375, "compression_ratio": 1.554054054054054, "no_speech_prob": 9.516099453321658e-06}, {"id": 1068, "seek": 345588, "start": 3459.6, "end": 3465.1600000000003, "text": " and will elegantly and happily finish up.", "tokens": [293, 486, 14459, 3627, 293, 19909, 2413, 493, 13], "temperature": 0.0, "avg_logprob": -0.12154388427734375, "compression_ratio": 1.554054054054054, "no_speech_prob": 9.516099453321658e-06}, {"id": 1069, "seek": 345588, "start": 3465.1600000000003, "end": 3466.6, "text": " So we can cancel training.", "tokens": [407, 321, 393, 10373, 3097, 13], "temperature": 0.0, "avg_logprob": -0.12154388427734375, "compression_ratio": 1.554054054054054, "no_speech_prob": 9.516099453321658e-06}, {"id": 1070, "seek": 345588, "start": 3466.6, "end": 3471.6400000000003, "text": " So now, our test callback can after step,", "tokens": [407, 586, 11, 527, 1500, 818, 3207, 393, 934, 1823, 11], "temperature": 0.0, "avg_logprob": -0.12154388427734375, "compression_ratio": 1.554054054054054, "no_speech_prob": 9.516099453321658e-06}, {"id": 1071, "seek": 345588, "start": 3471.6400000000003, "end": 3473.7200000000003, "text": " it'll just print out what step we're up to.", "tokens": [309, 603, 445, 4482, 484, 437, 1823, 321, 434, 493, 281, 13], "temperature": 0.0, "avg_logprob": -0.12154388427734375, "compression_ratio": 1.554054054054054, "no_speech_prob": 9.516099453321658e-06}, {"id": 1072, "seek": 345588, "start": 3473.7200000000003, "end": 3475.8, "text": " And if it's greater than or equal to 10,", "tokens": [400, 498, 309, 311, 5044, 813, 420, 2681, 281, 1266, 11], "temperature": 0.0, "avg_logprob": -0.12154388427734375, "compression_ratio": 1.554054054054054, "no_speech_prob": 9.516099453321658e-06}, {"id": 1073, "seek": 345588, "start": 3475.8, "end": 3478.48, "text": " we'll raise cancelTrainException.", "tokens": [321, 603, 5300, 10373, 51, 7146, 11149, 7311, 13], "temperature": 0.0, "avg_logprob": -0.12154388427734375, "compression_ratio": 1.554054054054054, "no_speech_prob": 9.516099453321658e-06}, {"id": 1074, "seek": 345588, "start": 3478.48, "end": 3483.7200000000003, "text": " And so now, when we say run.fit, it just prints out up to 10", "tokens": [400, 370, 586, 11, 562, 321, 584, 1190, 13, 6845, 11, 309, 445, 22305, 484, 493, 281, 1266], "temperature": 0.0, "avg_logprob": -0.12154388427734375, "compression_ratio": 1.554054054054054, "no_speech_prob": 9.516099453321658e-06}, {"id": 1075, "seek": 345588, "start": 3483.7200000000003, "end": 3484.6800000000003, "text": " and stops.", "tokens": [293, 10094, 13], "temperature": 0.0, "avg_logprob": -0.12154388427734375, "compression_ratio": 1.554054054054054, "no_speech_prob": 9.516099453321658e-06}, {"id": 1076, "seek": 348468, "start": 3484.68, "end": 3485.96, "text": " There's no stack trace.", "tokens": [821, 311, 572, 8630, 13508, 13], "temperature": 0.0, "avg_logprob": -0.12596289616710735, "compression_ratio": 1.6728971962616823, "no_speech_prob": 2.4060650503088254e-06}, {"id": 1077, "seek": 348468, "start": 3485.96, "end": 3487.12, "text": " There's no error.", "tokens": [821, 311, 572, 6713, 13], "temperature": 0.0, "avg_logprob": -0.12596289616710735, "compression_ratio": 1.6728971962616823, "no_speech_prob": 2.4060650503088254e-06}, {"id": 1078, "seek": 348468, "start": 3487.12, "end": 3490.3999999999996, "text": " This is using an exception as a control flow technique,", "tokens": [639, 307, 1228, 364, 11183, 382, 257, 1969, 3095, 6532, 11], "temperature": 0.0, "avg_logprob": -0.12596289616710735, "compression_ratio": 1.6728971962616823, "no_speech_prob": 2.4060650503088254e-06}, {"id": 1079, "seek": 348468, "start": 3490.3999999999996, "end": 3493.16, "text": " not as an error handling technique.", "tokens": [406, 382, 364, 6713, 13175, 6532, 13], "temperature": 0.0, "avg_logprob": -0.12596289616710735, "compression_ratio": 1.6728971962616823, "no_speech_prob": 2.4060650503088254e-06}, {"id": 1080, "seek": 348468, "start": 3493.16, "end": 3497.96, "text": " So another example, inside all batches,", "tokens": [407, 1071, 1365, 11, 1854, 439, 15245, 279, 11], "temperature": 0.0, "avg_logprob": -0.12596289616710735, "compression_ratio": 1.6728971962616823, "no_speech_prob": 2.4060650503088254e-06}, {"id": 1081, "seek": 348468, "start": 3497.96, "end": 3502.52, "text": " I go through all my batches in a try block,", "tokens": [286, 352, 807, 439, 452, 15245, 279, 294, 257, 853, 3461, 11], "temperature": 0.0, "avg_logprob": -0.12596289616710735, "compression_ratio": 1.6728971962616823, "no_speech_prob": 2.4060650503088254e-06}, {"id": 1082, "seek": 348468, "start": 3502.52, "end": 3505.64, "text": " except if there's a cancelEpochException,", "tokens": [3993, 498, 456, 311, 257, 10373, 36, 2259, 339, 11149, 7311, 11], "temperature": 0.0, "avg_logprob": -0.12596289616710735, "compression_ratio": 1.6728971962616823, "no_speech_prob": 2.4060650503088254e-06}, {"id": 1083, "seek": 348468, "start": 3505.64, "end": 3510.0, "text": " in which case I optionally call an after cancelEpoch callback", "tokens": [294, 597, 1389, 286, 3614, 379, 818, 364, 934, 10373, 36, 2259, 339, 818, 3207], "temperature": 0.0, "avg_logprob": -0.12596289616710735, "compression_ratio": 1.6728971962616823, "no_speech_prob": 2.4060650503088254e-06}, {"id": 1084, "seek": 348468, "start": 3510.0, "end": 3513.68, "text": " and then continue to the next epoch.", "tokens": [293, 550, 2354, 281, 264, 958, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.12596289616710735, "compression_ratio": 1.6728971962616823, "no_speech_prob": 2.4060650503088254e-06}, {"id": 1085, "seek": 351368, "start": 3513.68, "end": 3517.48, "text": " Or inside one batch, I try to do all the stuff for a batch,", "tokens": [1610, 1854, 472, 15245, 11, 286, 853, 281, 360, 439, 264, 1507, 337, 257, 15245, 11], "temperature": 0.0, "avg_logprob": -0.09745382812787902, "compression_ratio": 1.6726457399103138, "no_speech_prob": 7.527718935307348e-06}, {"id": 1086, "seek": 351368, "start": 3517.48, "end": 3520.9199999999996, "text": " except if there's a cancelBatchException,", "tokens": [3993, 498, 456, 311, 257, 10373, 33, 852, 11149, 7311, 11], "temperature": 0.0, "avg_logprob": -0.09745382812787902, "compression_ratio": 1.6726457399103138, "no_speech_prob": 7.527718935307348e-06}, {"id": 1087, "seek": 351368, "start": 3520.9199999999996, "end": 3524.08, "text": " I will optionally call the after cancelBatch callback", "tokens": [286, 486, 3614, 379, 818, 264, 934, 10373, 33, 852, 818, 3207], "temperature": 0.0, "avg_logprob": -0.09745382812787902, "compression_ratio": 1.6726457399103138, "no_speech_prob": 7.527718935307348e-06}, {"id": 1088, "seek": 351368, "start": 3524.08, "end": 3527.52, "text": " and then continue to the next batch.", "tokens": [293, 550, 2354, 281, 264, 958, 15245, 13], "temperature": 0.0, "avg_logprob": -0.09745382812787902, "compression_ratio": 1.6726457399103138, "no_speech_prob": 7.527718935307348e-06}, {"id": 1089, "seek": 351368, "start": 3527.52, "end": 3530.3199999999997, "text": " So this is like a super neat way that we've", "tokens": [407, 341, 307, 411, 257, 1687, 10654, 636, 300, 321, 600], "temperature": 0.0, "avg_logprob": -0.09745382812787902, "compression_ratio": 1.6726457399103138, "no_speech_prob": 7.527718935307348e-06}, {"id": 1090, "seek": 351368, "start": 3530.3199999999997, "end": 3534.24, "text": " allowed any callback writer to stop", "tokens": [4350, 604, 818, 3207, 9936, 281, 1590], "temperature": 0.0, "avg_logprob": -0.09745382812787902, "compression_ratio": 1.6726457399103138, "no_speech_prob": 7.527718935307348e-06}, {"id": 1091, "seek": 351368, "start": 3534.24, "end": 3539.8399999999997, "text": " any one of these three levels of things happening.", "tokens": [604, 472, 295, 613, 1045, 4358, 295, 721, 2737, 13], "temperature": 0.0, "avg_logprob": -0.09745382812787902, "compression_ratio": 1.6726457399103138, "no_speech_prob": 7.527718935307348e-06}, {"id": 1092, "seek": 351368, "start": 3539.8399999999997, "end": 3543.3999999999996, "text": " So in this case, we're using cancelTrainException", "tokens": [407, 294, 341, 1389, 11, 321, 434, 1228, 10373, 51, 7146, 11149, 7311], "temperature": 0.0, "avg_logprob": -0.09745382812787902, "compression_ratio": 1.6726457399103138, "no_speech_prob": 7.527718935307348e-06}, {"id": 1093, "seek": 354340, "start": 3543.4, "end": 3545.52, "text": " to stop training.", "tokens": [281, 1590, 3097, 13], "temperature": 0.0, "avg_logprob": -0.12290623502911262, "compression_ratio": 1.7167381974248928, "no_speech_prob": 1.5206446732918266e-05}, {"id": 1094, "seek": 354340, "start": 3545.52, "end": 3550.2400000000002, "text": " So we can now use that to create a learning rate finder.", "tokens": [407, 321, 393, 586, 764, 300, 281, 1884, 257, 2539, 3314, 915, 260, 13], "temperature": 0.0, "avg_logprob": -0.12290623502911262, "compression_ratio": 1.7167381974248928, "no_speech_prob": 1.5206446732918266e-05}, {"id": 1095, "seek": 354340, "start": 3550.2400000000002, "end": 3553.08, "text": " So the basic approach of the learning rate finder", "tokens": [407, 264, 3875, 3109, 295, 264, 2539, 3314, 915, 260], "temperature": 0.0, "avg_logprob": -0.12290623502911262, "compression_ratio": 1.7167381974248928, "no_speech_prob": 1.5206446732918266e-05}, {"id": 1096, "seek": 354340, "start": 3553.08, "end": 3555.48, "text": " is that there's something in beginBatch which,", "tokens": [307, 300, 456, 311, 746, 294, 1841, 33, 852, 597, 11], "temperature": 0.0, "avg_logprob": -0.12290623502911262, "compression_ratio": 1.7167381974248928, "no_speech_prob": 1.5206446732918266e-05}, {"id": 1097, "seek": 354340, "start": 3555.48, "end": 3557.64, "text": " just like our parameter scheduler,", "tokens": [445, 411, 527, 13075, 12000, 260, 11], "temperature": 0.0, "avg_logprob": -0.12290623502911262, "compression_ratio": 1.7167381974248928, "no_speech_prob": 1.5206446732918266e-05}, {"id": 1098, "seek": 354340, "start": 3557.64, "end": 3564.48, "text": " is using an exponential curve to set the learning rate.", "tokens": [307, 1228, 364, 21510, 7605, 281, 992, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.12290623502911262, "compression_ratio": 1.7167381974248928, "no_speech_prob": 1.5206446732918266e-05}, {"id": 1099, "seek": 354340, "start": 3564.48, "end": 3567.84, "text": " So this is identical to the program scheduler.", "tokens": [407, 341, 307, 14800, 281, 264, 1461, 12000, 260, 13], "temperature": 0.0, "avg_logprob": -0.12290623502911262, "compression_ratio": 1.7167381974248928, "no_speech_prob": 1.5206446732918266e-05}, {"id": 1100, "seek": 354340, "start": 3567.84, "end": 3570.84, "text": " And then after each step, it checks", "tokens": [400, 550, 934, 1184, 1823, 11, 309, 13834], "temperature": 0.0, "avg_logprob": -0.12290623502911262, "compression_ratio": 1.7167381974248928, "no_speech_prob": 1.5206446732918266e-05}, {"id": 1101, "seek": 354340, "start": 3570.84, "end": 3573.0, "text": " to see whether we've done more than the maximum number", "tokens": [281, 536, 1968, 321, 600, 1096, 544, 813, 264, 6674, 1230], "temperature": 0.0, "avg_logprob": -0.12290623502911262, "compression_ratio": 1.7167381974248928, "no_speech_prob": 1.5206446732918266e-05}, {"id": 1102, "seek": 357300, "start": 3573.0, "end": 3576.12, "text": " of iterations, which is defaulted to 100,", "tokens": [295, 36540, 11, 597, 307, 7576, 292, 281, 2319, 11], "temperature": 0.0, "avg_logprob": -0.13284572226102234, "compression_ratio": 1.5777777777777777, "no_speech_prob": 1.4969472431403119e-05}, {"id": 1103, "seek": 357300, "start": 3576.12, "end": 3580.72, "text": " or whether the loss is much worse than the best we've", "tokens": [420, 1968, 264, 4470, 307, 709, 5324, 813, 264, 1151, 321, 600], "temperature": 0.0, "avg_logprob": -0.13284572226102234, "compression_ratio": 1.5777777777777777, "no_speech_prob": 1.4969472431403119e-05}, {"id": 1104, "seek": 357300, "start": 3580.72, "end": 3582.2, "text": " had so far.", "tokens": [632, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.13284572226102234, "compression_ratio": 1.5777777777777777, "no_speech_prob": 1.4969472431403119e-05}, {"id": 1105, "seek": 357300, "start": 3582.2, "end": 3586.56, "text": " And if either of those happens, we will raise cancelTrainException.", "tokens": [400, 498, 2139, 295, 729, 2314, 11, 321, 486, 5300, 10373, 51, 7146, 11149, 7311, 13], "temperature": 0.0, "avg_logprob": -0.13284572226102234, "compression_ratio": 1.5777777777777777, "no_speech_prob": 1.4969472431403119e-05}, {"id": 1106, "seek": 357300, "start": 3586.56, "end": 3590.04, "text": " So to be clear, this neat exception-based approach", "tokens": [407, 281, 312, 1850, 11, 341, 10654, 11183, 12, 6032, 3109], "temperature": 0.0, "avg_logprob": -0.13284572226102234, "compression_ratio": 1.5777777777777777, "no_speech_prob": 1.4969472431403119e-05}, {"id": 1107, "seek": 357300, "start": 3590.04, "end": 3593.04, "text": " to control flow isn't being used in the FastAI version 1", "tokens": [281, 1969, 3095, 1943, 380, 885, 1143, 294, 264, 15968, 48698, 3037, 502], "temperature": 0.0, "avg_logprob": -0.13284572226102234, "compression_ratio": 1.5777777777777777, "no_speech_prob": 1.4969472431403119e-05}, {"id": 1108, "seek": 357300, "start": 3593.04, "end": 3593.88, "text": " at the moment.", "tokens": [412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.13284572226102234, "compression_ratio": 1.5777777777777777, "no_speech_prob": 1.4969472431403119e-05}, {"id": 1109, "seek": 357300, "start": 3593.88, "end": 3597.36, "text": " But it's very likely that FastAI 1.1 or 2", "tokens": [583, 309, 311, 588, 3700, 300, 15968, 48698, 502, 13, 16, 420, 568], "temperature": 0.0, "avg_logprob": -0.13284572226102234, "compression_ratio": 1.5777777777777777, "no_speech_prob": 1.4969472431403119e-05}, {"id": 1110, "seek": 357300, "start": 3597.36, "end": 3600.64, "text": " will switch to this approach, because it's just so much more", "tokens": [486, 3679, 281, 341, 3109, 11, 570, 309, 311, 445, 370, 709, 544], "temperature": 0.0, "avg_logprob": -0.13284572226102234, "compression_ratio": 1.5777777777777777, "no_speech_prob": 1.4969472431403119e-05}, {"id": 1111, "seek": 357300, "start": 3600.64, "end": 3602.8, "text": " convenient and flexible.", "tokens": [10851, 293, 11358, 13], "temperature": 0.0, "avg_logprob": -0.13284572226102234, "compression_ratio": 1.5777777777777777, "no_speech_prob": 1.4969472431403119e-05}, {"id": 1112, "seek": 360280, "start": 3602.8, "end": 3604.5600000000004, "text": " And then assuming we haven't canceled,", "tokens": [400, 550, 11926, 321, 2378, 380, 24839, 11], "temperature": 0.0, "avg_logprob": -0.09785077969233195, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.2606169548234902e-05}, {"id": 1113, "seek": 360280, "start": 3604.5600000000004, "end": 3606.7200000000003, "text": " just see if the loss is better than our best loss.", "tokens": [445, 536, 498, 264, 4470, 307, 1101, 813, 527, 1151, 4470, 13], "temperature": 0.0, "avg_logprob": -0.09785077969233195, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.2606169548234902e-05}, {"id": 1114, "seek": 360280, "start": 3606.7200000000003, "end": 3608.92, "text": " And if it is, then set best loss to the loss.", "tokens": [400, 498, 309, 307, 11, 550, 992, 1151, 4470, 281, 264, 4470, 13], "temperature": 0.0, "avg_logprob": -0.09785077969233195, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.2606169548234902e-05}, {"id": 1115, "seek": 360280, "start": 3613.2400000000002, "end": 3615.52, "text": " So now we can create a learner.", "tokens": [407, 586, 321, 393, 1884, 257, 33347, 13], "temperature": 0.0, "avg_logprob": -0.09785077969233195, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.2606169548234902e-05}, {"id": 1116, "seek": 360280, "start": 3615.52, "end": 3618.0800000000004, "text": " We can add the LR find.", "tokens": [492, 393, 909, 264, 441, 49, 915, 13], "temperature": 0.0, "avg_logprob": -0.09785077969233195, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.2606169548234902e-05}, {"id": 1117, "seek": 360280, "start": 3618.0800000000004, "end": 3620.48, "text": " We can fit.", "tokens": [492, 393, 3318, 13], "temperature": 0.0, "avg_logprob": -0.09785077969233195, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.2606169548234902e-05}, {"id": 1118, "seek": 360280, "start": 3620.48, "end": 3623.7200000000003, "text": " And you can see that it only does less than 100 epochs", "tokens": [400, 291, 393, 536, 300, 309, 787, 775, 1570, 813, 2319, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.09785077969233195, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.2606169548234902e-05}, {"id": 1119, "seek": 360280, "start": 3623.7200000000003, "end": 3629.5600000000004, "text": " before it stops, because the loss got a lot worse.", "tokens": [949, 309, 10094, 11, 570, 264, 4470, 658, 257, 688, 5324, 13], "temperature": 0.0, "avg_logprob": -0.09785077969233195, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.2606169548234902e-05}, {"id": 1120, "seek": 362956, "start": 3629.56, "end": 3632.96, "text": " And so now we know that we want something about there", "tokens": [400, 370, 586, 321, 458, 300, 321, 528, 746, 466, 456], "temperature": 0.0, "avg_logprob": -0.17146923375684162, "compression_ratio": 1.4545454545454546, "no_speech_prob": 1.6963844245765358e-05}, {"id": 1121, "seek": 362956, "start": 3632.96, "end": 3635.6, "text": " for our learning rate.", "tokens": [337, 527, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.17146923375684162, "compression_ratio": 1.4545454545454546, "no_speech_prob": 1.6963844245765358e-05}, {"id": 1122, "seek": 362956, "start": 3635.6, "end": 3638.32, "text": " OK, so now we have a learning rate finder.", "tokens": [2264, 11, 370, 586, 321, 362, 257, 2539, 3314, 915, 260, 13], "temperature": 0.0, "avg_logprob": -0.17146923375684162, "compression_ratio": 1.4545454545454546, "no_speech_prob": 1.6963844245765358e-05}, {"id": 1123, "seek": 362956, "start": 3642.0, "end": 3648.56, "text": " So let's go ahead and create a CNN, and specifically", "tokens": [407, 718, 311, 352, 2286, 293, 1884, 257, 24859, 11, 293, 4682], "temperature": 0.0, "avg_logprob": -0.17146923375684162, "compression_ratio": 1.4545454545454546, "no_speech_prob": 1.6963844245765358e-05}, {"id": 1124, "seek": 362956, "start": 3648.56, "end": 3650.84, "text": " a SCUDA CNN.", "tokens": [257, 9028, 9438, 32, 24859, 13], "temperature": 0.0, "avg_logprob": -0.17146923375684162, "compression_ratio": 1.4545454545454546, "no_speech_prob": 1.6963844245765358e-05}, {"id": 1125, "seek": 362956, "start": 3650.84, "end": 3654.16, "text": " So we'll keep doing the same stuff we've been doing,", "tokens": [407, 321, 603, 1066, 884, 264, 912, 1507, 321, 600, 668, 884, 11], "temperature": 0.0, "avg_logprob": -0.17146923375684162, "compression_ratio": 1.4545454545454546, "no_speech_prob": 1.6963844245765358e-05}, {"id": 1126, "seek": 362956, "start": 3654.16, "end": 3658.64, "text": " get our MNIST data, normalize it.", "tokens": [483, 527, 376, 45, 19756, 1412, 11, 2710, 1125, 309, 13], "temperature": 0.0, "avg_logprob": -0.17146923375684162, "compression_ratio": 1.4545454545454546, "no_speech_prob": 1.6963844245765358e-05}, {"id": 1127, "seek": 365864, "start": 3658.64, "end": 3660.3199999999997, "text": " Here's a nice little refactoring,", "tokens": [1692, 311, 257, 1481, 707, 1895, 578, 3662, 11], "temperature": 0.0, "avg_logprob": -0.1452691768243061, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.9831553800031543e-05}, {"id": 1128, "seek": 365864, "start": 3660.3199999999997, "end": 3664.4, "text": " because we very often want to normalize with this data set", "tokens": [570, 321, 588, 2049, 528, 281, 2710, 1125, 365, 341, 1412, 992], "temperature": 0.0, "avg_logprob": -0.1452691768243061, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.9831553800031543e-05}, {"id": 1129, "seek": 365864, "start": 3664.4, "end": 3666.8399999999997, "text": " and normalize both data sets using this data sets mean", "tokens": [293, 2710, 1125, 1293, 1412, 6352, 1228, 341, 1412, 6352, 914], "temperature": 0.0, "avg_logprob": -0.1452691768243061, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.9831553800031543e-05}, {"id": 1130, "seek": 365864, "start": 3666.8399999999997, "end": 3668.12, "text": " and standard deviation.", "tokens": [293, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.1452691768243061, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.9831553800031543e-05}, {"id": 1131, "seek": 365864, "start": 3668.12, "end": 3671.3199999999997, "text": " Let's create a function called normalize2, which does that", "tokens": [961, 311, 1884, 257, 2445, 1219, 2710, 1125, 17, 11, 597, 775, 300], "temperature": 0.0, "avg_logprob": -0.1452691768243061, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.9831553800031543e-05}, {"id": 1132, "seek": 365864, "start": 3671.3199999999997, "end": 3674.24, "text": " and returns the normalized training set and the normalized", "tokens": [293, 11247, 264, 48704, 3097, 992, 293, 264, 48704], "temperature": 0.0, "avg_logprob": -0.1452691768243061, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.9831553800031543e-05}, {"id": 1133, "seek": 365864, "start": 3674.24, "end": 3675.6, "text": " validation set.", "tokens": [24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.1452691768243061, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.9831553800031543e-05}, {"id": 1134, "seek": 365864, "start": 3675.6, "end": 3679.3599999999997, "text": " So we can now use that.", "tokens": [407, 321, 393, 586, 764, 300, 13], "temperature": 0.0, "avg_logprob": -0.1452691768243061, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.9831553800031543e-05}, {"id": 1135, "seek": 365864, "start": 3679.3599999999997, "end": 3680.8399999999997, "text": " Make sure that it's behaved properly.", "tokens": [4387, 988, 300, 309, 311, 48249, 6108, 13], "temperature": 0.0, "avg_logprob": -0.1452691768243061, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.9831553800031543e-05}, {"id": 1136, "seek": 365864, "start": 3680.8399999999997, "end": 3682.68, "text": " That looks good.", "tokens": [663, 1542, 665, 13], "temperature": 0.0, "avg_logprob": -0.1452691768243061, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.9831553800031543e-05}, {"id": 1137, "seek": 365864, "start": 3682.68, "end": 3685.48, "text": " Create our data bunch.", "tokens": [20248, 527, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.1452691768243061, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.9831553800031543e-05}, {"id": 1138, "seek": 365864, "start": 3685.48, "end": 3688.3599999999997, "text": " And so now we're going to create a CNN model.", "tokens": [400, 370, 586, 321, 434, 516, 281, 1884, 257, 24859, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1452691768243061, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.9831553800031543e-05}, {"id": 1139, "seek": 368836, "start": 3688.36, "end": 3691.1200000000003, "text": " And the CNN is just a sequential model", "tokens": [400, 264, 24859, 307, 445, 257, 42881, 2316], "temperature": 0.0, "avg_logprob": -0.1160546208991379, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.5444971722899936e-05}, {"id": 1140, "seek": 368836, "start": 3691.1200000000003, "end": 3695.88, "text": " that contains a bunch of stride-to convolutions.", "tokens": [300, 8306, 257, 3840, 295, 1056, 482, 12, 1353, 3754, 15892, 13], "temperature": 0.0, "avg_logprob": -0.1160546208991379, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.5444971722899936e-05}, {"id": 1141, "seek": 368836, "start": 3695.88, "end": 3698.2400000000002, "text": " And remember, the input's 28 by 28.", "tokens": [400, 1604, 11, 264, 4846, 311, 7562, 538, 7562, 13], "temperature": 0.0, "avg_logprob": -0.1160546208991379, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.5444971722899936e-05}, {"id": 1142, "seek": 368836, "start": 3698.2400000000002, "end": 3701.56, "text": " So after the first, it'll be 14 by 14, then 7 by 7,", "tokens": [407, 934, 264, 700, 11, 309, 603, 312, 3499, 538, 3499, 11, 550, 1614, 538, 1614, 11], "temperature": 0.0, "avg_logprob": -0.1160546208991379, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.5444971722899936e-05}, {"id": 1143, "seek": 368836, "start": 3701.56, "end": 3704.08, "text": " then 4 by 4, then 2 by 2.", "tokens": [550, 1017, 538, 1017, 11, 550, 568, 538, 568, 13], "temperature": 0.0, "avg_logprob": -0.1160546208991379, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.5444971722899936e-05}, {"id": 1144, "seek": 368836, "start": 3704.08, "end": 3706.96, "text": " Then we'll do our average pooling, flatten it,", "tokens": [1396, 321, 603, 360, 527, 4274, 7005, 278, 11, 24183, 309, 11], "temperature": 0.0, "avg_logprob": -0.1160546208991379, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.5444971722899936e-05}, {"id": 1145, "seek": 368836, "start": 3706.96, "end": 3709.2400000000002, "text": " and a linear layer.", "tokens": [293, 257, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1160546208991379, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.5444971722899936e-05}, {"id": 1146, "seek": 368836, "start": 3709.2400000000002, "end": 3711.52, "text": " And then we're done.", "tokens": [400, 550, 321, 434, 1096, 13], "temperature": 0.0, "avg_logprob": -0.1160546208991379, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.5444971722899936e-05}, {"id": 1147, "seek": 368836, "start": 3711.52, "end": 3716.04, "text": " Now remember, our original data is vectors of length 768.", "tokens": [823, 1604, 11, 527, 3380, 1412, 307, 18875, 295, 4641, 24733, 23, 13], "temperature": 0.0, "avg_logprob": -0.1160546208991379, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.5444971722899936e-05}, {"id": 1148, "seek": 368836, "start": 3716.04, "end": 3717.8, "text": " They're not 28 by 28.", "tokens": [814, 434, 406, 7562, 538, 7562, 13], "temperature": 0.0, "avg_logprob": -0.1160546208991379, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.5444971722899936e-05}, {"id": 1149, "seek": 371780, "start": 3717.8, "end": 3725.1200000000003, "text": " So we need to do a x.view one channel by 28 by 28,", "tokens": [407, 321, 643, 281, 360, 257, 2031, 13, 1759, 472, 2269, 538, 7562, 538, 7562, 11], "temperature": 0.0, "avg_logprob": -0.10875778030930904, "compression_ratio": 1.540084388185654, "no_speech_prob": 5.8626583268051036e-06}, {"id": 1150, "seek": 371780, "start": 3725.1200000000003, "end": 3728.7200000000003, "text": " because that's what nn.conv2d expects.", "tokens": [570, 300, 311, 437, 297, 77, 13, 1671, 85, 17, 67, 33280, 13], "temperature": 0.0, "avg_logprob": -0.10875778030930904, "compression_ratio": 1.540084388185654, "no_speech_prob": 5.8626583268051036e-06}, {"id": 1151, "seek": 371780, "start": 3728.7200000000003, "end": 3730.8, "text": " And then minus 1, the batch size remains", "tokens": [400, 550, 3175, 502, 11, 264, 15245, 2744, 7023], "temperature": 0.0, "avg_logprob": -0.10875778030930904, "compression_ratio": 1.540084388185654, "no_speech_prob": 5.8626583268051036e-06}, {"id": 1152, "seek": 371780, "start": 3730.8, "end": 3732.8, "text": " whatever it was before.", "tokens": [2035, 309, 390, 949, 13], "temperature": 0.0, "avg_logprob": -0.10875778030930904, "compression_ratio": 1.540084388185654, "no_speech_prob": 5.8626583268051036e-06}, {"id": 1153, "seek": 371780, "start": 3732.8, "end": 3735.96, "text": " So we need to somehow include this function", "tokens": [407, 321, 643, 281, 6063, 4090, 341, 2445], "temperature": 0.0, "avg_logprob": -0.10875778030930904, "compression_ratio": 1.540084388185654, "no_speech_prob": 5.8626583268051036e-06}, {"id": 1154, "seek": 371780, "start": 3735.96, "end": 3738.32, "text": " in our nn.sequential.", "tokens": [294, 527, 297, 77, 13, 11834, 2549, 13], "temperature": 0.0, "avg_logprob": -0.10875778030930904, "compression_ratio": 1.540084388185654, "no_speech_prob": 5.8626583268051036e-06}, {"id": 1155, "seek": 371780, "start": 3738.32, "end": 3740.7200000000003, "text": " PyTorch doesn't support that by default.", "tokens": [9953, 51, 284, 339, 1177, 380, 1406, 300, 538, 7576, 13], "temperature": 0.0, "avg_logprob": -0.10875778030930904, "compression_ratio": 1.540084388185654, "no_speech_prob": 5.8626583268051036e-06}, {"id": 1156, "seek": 371780, "start": 3740.7200000000003, "end": 3743.6800000000003, "text": " We could write our own class with a forward function.", "tokens": [492, 727, 2464, 527, 1065, 1508, 365, 257, 2128, 2445, 13], "temperature": 0.0, "avg_logprob": -0.10875778030930904, "compression_ratio": 1.540084388185654, "no_speech_prob": 5.8626583268051036e-06}, {"id": 1157, "seek": 371780, "start": 3743.6800000000003, "end": 3746.6400000000003, "text": " But nn.sequential is convenient for lots of ways.", "tokens": [583, 297, 77, 13, 11834, 2549, 307, 10851, 337, 3195, 295, 2098, 13], "temperature": 0.0, "avg_logprob": -0.10875778030930904, "compression_ratio": 1.540084388185654, "no_speech_prob": 5.8626583268051036e-06}, {"id": 1158, "seek": 374664, "start": 3746.64, "end": 3748.3599999999997, "text": " It has a nice representation.", "tokens": [467, 575, 257, 1481, 10290, 13], "temperature": 0.0, "avg_logprob": -0.12415494170843386, "compression_ratio": 1.7623762376237624, "no_speech_prob": 8.013092156033963e-06}, {"id": 1159, "seek": 374664, "start": 3748.3599999999997, "end": 3751.16, "text": " You can do all kinds of customizations with it.", "tokens": [509, 393, 360, 439, 3685, 295, 2375, 14455, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.12415494170843386, "compression_ratio": 1.7623762376237624, "no_speech_prob": 8.013092156033963e-06}, {"id": 1160, "seek": 374664, "start": 3751.16, "end": 3755.7599999999998, "text": " So instead, we create a layer called lambda,", "tokens": [407, 2602, 11, 321, 1884, 257, 4583, 1219, 13607, 11], "temperature": 0.0, "avg_logprob": -0.12415494170843386, "compression_ratio": 1.7623762376237624, "no_speech_prob": 8.013092156033963e-06}, {"id": 1161, "seek": 374664, "start": 3755.7599999999998, "end": 3758.2, "text": " an nn.module called lambda.", "tokens": [364, 297, 77, 13, 8014, 2271, 1219, 13607, 13], "temperature": 0.0, "avg_logprob": -0.12415494170843386, "compression_ratio": 1.7623762376237624, "no_speech_prob": 8.013092156033963e-06}, {"id": 1162, "seek": 374664, "start": 3758.2, "end": 3760.24, "text": " You just pass it a function.", "tokens": [509, 445, 1320, 309, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12415494170843386, "compression_ratio": 1.7623762376237624, "no_speech_prob": 8.013092156033963e-06}, {"id": 1163, "seek": 374664, "start": 3760.24, "end": 3764.6, "text": " And the forward is simply to call that function.", "tokens": [400, 264, 2128, 307, 2935, 281, 818, 300, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12415494170843386, "compression_ratio": 1.7623762376237624, "no_speech_prob": 8.013092156033963e-06}, {"id": 1164, "seek": 374664, "start": 3764.6, "end": 3768.56, "text": " And so now we can say lambda mnist resize.", "tokens": [400, 370, 586, 321, 393, 584, 13607, 275, 77, 468, 50069, 13], "temperature": 0.0, "avg_logprob": -0.12415494170843386, "compression_ratio": 1.7623762376237624, "no_speech_prob": 8.013092156033963e-06}, {"id": 1165, "seek": 374664, "start": 3768.56, "end": 3771.64, "text": " And that will call that function to be called.", "tokens": [400, 300, 486, 818, 300, 2445, 281, 312, 1219, 13], "temperature": 0.0, "avg_logprob": -0.12415494170843386, "compression_ratio": 1.7623762376237624, "no_speech_prob": 8.013092156033963e-06}, {"id": 1166, "seek": 374664, "start": 3771.64, "end": 3775.24, "text": " And here, lambda flatten simply calls", "tokens": [400, 510, 11, 13607, 24183, 2935, 5498], "temperature": 0.0, "avg_logprob": -0.12415494170843386, "compression_ratio": 1.7623762376237624, "no_speech_prob": 8.013092156033963e-06}, {"id": 1167, "seek": 377524, "start": 3775.24, "end": 3779.9599999999996, "text": " this function to be called, which removes that 1,1 axis", "tokens": [341, 2445, 281, 312, 1219, 11, 597, 30445, 300, 502, 11, 16, 10298], "temperature": 0.0, "avg_logprob": -0.1399678729829334, "compression_ratio": 1.5336322869955157, "no_speech_prob": 4.710768735094462e-06}, {"id": 1168, "seek": 377524, "start": 3779.9599999999996, "end": 3782.9599999999996, "text": " at the end after the adaptive average pooling.", "tokens": [412, 264, 917, 934, 264, 27912, 4274, 7005, 278, 13], "temperature": 0.0, "avg_logprob": -0.1399678729829334, "compression_ratio": 1.5336322869955157, "no_speech_prob": 4.710768735094462e-06}, {"id": 1169, "seek": 377524, "start": 3782.9599999999996, "end": 3786.7999999999997, "text": " So now we've got a CNN model.", "tokens": [407, 586, 321, 600, 658, 257, 24859, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1399678729829334, "compression_ratio": 1.5336322869955157, "no_speech_prob": 4.710768735094462e-06}, {"id": 1170, "seek": 377524, "start": 3786.7999999999997, "end": 3790.08, "text": " We can grab our callback functions and our optimizer", "tokens": [492, 393, 4444, 527, 818, 3207, 6828, 293, 527, 5028, 6545], "temperature": 0.0, "avg_logprob": -0.1399678729829334, "compression_ratio": 1.5336322869955157, "no_speech_prob": 4.710768735094462e-06}, {"id": 1171, "seek": 377524, "start": 3790.08, "end": 3790.72, "text": " and our runner.", "tokens": [293, 527, 24376, 13], "temperature": 0.0, "avg_logprob": -0.1399678729829334, "compression_ratio": 1.5336322869955157, "no_speech_prob": 4.710768735094462e-06}, {"id": 1172, "seek": 377524, "start": 3790.72, "end": 3792.2799999999997, "text": " And we can run it.", "tokens": [400, 321, 393, 1190, 309, 13], "temperature": 0.0, "avg_logprob": -0.1399678729829334, "compression_ratio": 1.5336322869955157, "no_speech_prob": 4.710768735094462e-06}, {"id": 1173, "seek": 377524, "start": 3792.2799999999997, "end": 3798.9199999999996, "text": " And six seconds later, we get back one epoch's result.", "tokens": [400, 2309, 3949, 1780, 11, 321, 483, 646, 472, 30992, 339, 311, 1874, 13], "temperature": 0.0, "avg_logprob": -0.1399678729829334, "compression_ratio": 1.5336322869955157, "no_speech_prob": 4.710768735094462e-06}, {"id": 1174, "seek": 377524, "start": 3798.9199999999996, "end": 3802.4399999999996, "text": " So at this point now, getting a bit slow.", "tokens": [407, 412, 341, 935, 586, 11, 1242, 257, 857, 2964, 13], "temperature": 0.0, "avg_logprob": -0.1399678729829334, "compression_ratio": 1.5336322869955157, "no_speech_prob": 4.710768735094462e-06}, {"id": 1175, "seek": 377524, "start": 3802.4399999999996, "end": 3804.3199999999997, "text": " So let's make it faster.", "tokens": [407, 718, 311, 652, 309, 4663, 13], "temperature": 0.0, "avg_logprob": -0.1399678729829334, "compression_ratio": 1.5336322869955157, "no_speech_prob": 4.710768735094462e-06}, {"id": 1176, "seek": 380432, "start": 3804.32, "end": 3806.44, "text": " So let's use CUDA.", "tokens": [407, 718, 311, 764, 29777, 7509, 13], "temperature": 0.0, "avg_logprob": -0.11369545276348407, "compression_ratio": 1.8793103448275863, "no_speech_prob": 1.4284983990364708e-05}, {"id": 1177, "seek": 380432, "start": 3806.44, "end": 3807.88, "text": " Let's pop it on the GPU.", "tokens": [961, 311, 1665, 309, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.11369545276348407, "compression_ratio": 1.8793103448275863, "no_speech_prob": 1.4284983990364708e-05}, {"id": 1178, "seek": 380432, "start": 3807.88, "end": 3809.32, "text": " So we need to do two things.", "tokens": [407, 321, 643, 281, 360, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.11369545276348407, "compression_ratio": 1.8793103448275863, "no_speech_prob": 1.4284983990364708e-05}, {"id": 1179, "seek": 380432, "start": 3809.32, "end": 3813.96, "text": " We need to put the model on the GPU, which specifically means", "tokens": [492, 643, 281, 829, 264, 2316, 322, 264, 18407, 11, 597, 4682, 1355], "temperature": 0.0, "avg_logprob": -0.11369545276348407, "compression_ratio": 1.8793103448275863, "no_speech_prob": 1.4284983990364708e-05}, {"id": 1180, "seek": 380432, "start": 3813.96, "end": 3818.84, "text": " the model's parameters on the GPU.", "tokens": [264, 2316, 311, 9834, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.11369545276348407, "compression_ratio": 1.8793103448275863, "no_speech_prob": 1.4284983990364708e-05}, {"id": 1181, "seek": 380432, "start": 3818.84, "end": 3821.0, "text": " So remember, a model contains two kinds of numbers.", "tokens": [407, 1604, 11, 257, 2316, 8306, 732, 3685, 295, 3547, 13], "temperature": 0.0, "avg_logprob": -0.11369545276348407, "compression_ratio": 1.8793103448275863, "no_speech_prob": 1.4284983990364708e-05}, {"id": 1182, "seek": 380432, "start": 3821.0, "end": 3823.84, "text": " Parameters, they're the things that you're updating,", "tokens": [34882, 6202, 11, 436, 434, 264, 721, 300, 291, 434, 25113, 11], "temperature": 0.0, "avg_logprob": -0.11369545276348407, "compression_ratio": 1.8793103448275863, "no_speech_prob": 1.4284983990364708e-05}, {"id": 1183, "seek": 380432, "start": 3823.84, "end": 3825.1200000000003, "text": " the things that it stores.", "tokens": [264, 721, 300, 309, 9512, 13], "temperature": 0.0, "avg_logprob": -0.11369545276348407, "compression_ratio": 1.8793103448275863, "no_speech_prob": 1.4284983990364708e-05}, {"id": 1184, "seek": 380432, "start": 3825.1200000000003, "end": 3826.32, "text": " And there's the activations.", "tokens": [400, 456, 311, 264, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.11369545276348407, "compression_ratio": 1.8793103448275863, "no_speech_prob": 1.4284983990364708e-05}, {"id": 1185, "seek": 380432, "start": 3826.32, "end": 3828.0800000000004, "text": " There's the things that it's calculating.", "tokens": [821, 311, 264, 721, 300, 309, 311, 28258, 13], "temperature": 0.0, "avg_logprob": -0.11369545276348407, "compression_ratio": 1.8793103448275863, "no_speech_prob": 1.4284983990364708e-05}, {"id": 1186, "seek": 380432, "start": 3828.0800000000004, "end": 3829.52, "text": " So it's the parameters that we need", "tokens": [407, 309, 311, 264, 9834, 300, 321, 643], "temperature": 0.0, "avg_logprob": -0.11369545276348407, "compression_ratio": 1.8793103448275863, "no_speech_prob": 1.4284983990364708e-05}, {"id": 1187, "seek": 380432, "start": 3829.52, "end": 3831.7200000000003, "text": " to actually put on the GPU.", "tokens": [281, 767, 829, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.11369545276348407, "compression_ratio": 1.8793103448275863, "no_speech_prob": 1.4284983990364708e-05}, {"id": 1188, "seek": 383172, "start": 3831.72, "end": 3834.64, "text": " And the inputs to the model and the loss function.", "tokens": [400, 264, 15743, 281, 264, 2316, 293, 264, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.11569958045834401, "compression_ratio": 1.5726495726495726, "no_speech_prob": 3.3931023608602118e-06}, {"id": 1189, "seek": 383172, "start": 3834.64, "end": 3836.72, "text": " So in other words, the things that come out of the data", "tokens": [407, 294, 661, 2283, 11, 264, 721, 300, 808, 484, 295, 264, 1412], "temperature": 0.0, "avg_logprob": -0.11569958045834401, "compression_ratio": 1.5726495726495726, "no_speech_prob": 3.3931023608602118e-06}, {"id": 1190, "seek": 383172, "start": 3836.72, "end": 3838.9199999999996, "text": " loader, we need to put those on the GPU.", "tokens": [3677, 260, 11, 321, 643, 281, 829, 729, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.11569958045834401, "compression_ratio": 1.5726495726495726, "no_speech_prob": 3.3931023608602118e-06}, {"id": 1191, "seek": 383172, "start": 3838.9199999999996, "end": 3840.68, "text": " How do we do that?", "tokens": [1012, 360, 321, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.11569958045834401, "compression_ratio": 1.5726495726495726, "no_speech_prob": 3.3931023608602118e-06}, {"id": 1192, "seek": 383172, "start": 3840.68, "end": 3842.8399999999997, "text": " With a callback, of course.", "tokens": [2022, 257, 818, 3207, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.11569958045834401, "compression_ratio": 1.5726495726495726, "no_speech_prob": 3.3931023608602118e-06}, {"id": 1193, "seek": 383172, "start": 3842.8399999999997, "end": 3845.24, "text": " So here's a CUDA callback.", "tokens": [407, 510, 311, 257, 29777, 7509, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.11569958045834401, "compression_ratio": 1.5726495726495726, "no_speech_prob": 3.3931023608602118e-06}, {"id": 1194, "seek": 383172, "start": 3845.24, "end": 3848.8799999999997, "text": " When you initialize it, you pass it a device.", "tokens": [1133, 291, 5883, 1125, 309, 11, 291, 1320, 309, 257, 4302, 13], "temperature": 0.0, "avg_logprob": -0.11569958045834401, "compression_ratio": 1.5726495726495726, "no_speech_prob": 3.3931023608602118e-06}, {"id": 1195, "seek": 383172, "start": 3848.8799999999997, "end": 3850.8399999999997, "text": " And then when you begin fitting, you", "tokens": [400, 550, 562, 291, 1841, 15669, 11, 291], "temperature": 0.0, "avg_logprob": -0.11569958045834401, "compression_ratio": 1.5726495726495726, "no_speech_prob": 3.3931023608602118e-06}, {"id": 1196, "seek": 383172, "start": 3850.8399999999997, "end": 3853.8799999999997, "text": " move the model to that device.", "tokens": [1286, 264, 2316, 281, 300, 4302, 13], "temperature": 0.0, "avg_logprob": -0.11569958045834401, "compression_ratio": 1.5726495726495726, "no_speech_prob": 3.3931023608602118e-06}, {"id": 1197, "seek": 383172, "start": 3853.8799999999997, "end": 3857.7999999999997, "text": " So model.2.2 is part of PyTorch.", "tokens": [407, 2316, 13, 17, 13, 17, 307, 644, 295, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.11569958045834401, "compression_ratio": 1.5726495726495726, "no_speech_prob": 3.3931023608602118e-06}, {"id": 1198, "seek": 385780, "start": 3857.8, "end": 3862.6800000000003, "text": " It moves something with parameters or a tensor", "tokens": [467, 6067, 746, 365, 9834, 420, 257, 40863], "temperature": 0.0, "avg_logprob": -0.13732117956334894, "compression_ratio": 1.4615384615384615, "no_speech_prob": 4.157206603849772e-06}, {"id": 1199, "seek": 385780, "start": 3862.6800000000003, "end": 3863.76, "text": " to a device.", "tokens": [281, 257, 4302, 13], "temperature": 0.0, "avg_logprob": -0.13732117956334894, "compression_ratio": 1.4615384615384615, "no_speech_prob": 4.157206603849772e-06}, {"id": 1200, "seek": 385780, "start": 3863.76, "end": 3866.76, "text": " And you can create a device by calling torch.device,", "tokens": [400, 291, 393, 1884, 257, 4302, 538, 5141, 27822, 13, 40343, 573, 11], "temperature": 0.0, "avg_logprob": -0.13732117956334894, "compression_ratio": 1.4615384615384615, "no_speech_prob": 4.157206603849772e-06}, {"id": 1201, "seek": 385780, "start": 3866.76, "end": 3870.6000000000004, "text": " pass it the string CUDA, and whatever GPU number", "tokens": [1320, 309, 264, 6798, 29777, 7509, 11, 293, 2035, 18407, 1230], "temperature": 0.0, "avg_logprob": -0.13732117956334894, "compression_ratio": 1.4615384615384615, "no_speech_prob": 4.157206603849772e-06}, {"id": 1202, "seek": 385780, "start": 3870.6000000000004, "end": 3871.32, "text": " you want to use.", "tokens": [291, 528, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.13732117956334894, "compression_ratio": 1.4615384615384615, "no_speech_prob": 4.157206603849772e-06}, {"id": 1203, "seek": 385780, "start": 3871.32, "end": 3875.4, "text": " If you only have one GPU, it's device 0.", "tokens": [759, 291, 787, 362, 472, 18407, 11, 309, 311, 4302, 1958, 13], "temperature": 0.0, "avg_logprob": -0.13732117956334894, "compression_ratio": 1.4615384615384615, "no_speech_prob": 4.157206603849772e-06}, {"id": 1204, "seek": 385780, "start": 3875.4, "end": 3878.96, "text": " Then when we begin a batch, let's go back", "tokens": [1396, 562, 321, 1841, 257, 15245, 11, 718, 311, 352, 646], "temperature": 0.0, "avg_logprob": -0.13732117956334894, "compression_ratio": 1.4615384615384615, "no_speech_prob": 4.157206603849772e-06}, {"id": 1205, "seek": 385780, "start": 3878.96, "end": 3880.2400000000002, "text": " and look at our runner.", "tokens": [293, 574, 412, 527, 24376, 13], "temperature": 0.0, "avg_logprob": -0.13732117956334894, "compression_ratio": 1.4615384615384615, "no_speech_prob": 4.157206603849772e-06}, {"id": 1206, "seek": 388024, "start": 3880.24, "end": 3885.7999999999997, "text": " So now, when we begin a batch, we've", "tokens": [407, 586, 11, 562, 321, 1841, 257, 15245, 11, 321, 600], "temperature": 0.0, "avg_logprob": -0.14736113828771255, "compression_ratio": 1.6162162162162161, "no_speech_prob": 4.092810286238091e-06}, {"id": 1207, "seek": 388024, "start": 3885.7999999999997, "end": 3892.04, "text": " put x batch and y batch inside self.xb and self.yb.", "tokens": [829, 2031, 15245, 293, 288, 15245, 1854, 2698, 13, 87, 65, 293, 2698, 13, 88, 65, 13], "temperature": 0.0, "avg_logprob": -0.14736113828771255, "compression_ratio": 1.6162162162162161, "no_speech_prob": 4.092810286238091e-06}, {"id": 1208, "seek": 388024, "start": 3892.04, "end": 3894.56, "text": " So that means we can change them.", "tokens": [407, 300, 1355, 321, 393, 1319, 552, 13], "temperature": 0.0, "avg_logprob": -0.14736113828771255, "compression_ratio": 1.6162162162162161, "no_speech_prob": 4.092810286238091e-06}, {"id": 1209, "seek": 388024, "start": 3894.56, "end": 3898.3599999999997, "text": " So let's set the runner's xb and the runner's yb", "tokens": [407, 718, 311, 992, 264, 24376, 311, 2031, 65, 293, 264, 24376, 311, 288, 65], "temperature": 0.0, "avg_logprob": -0.14736113828771255, "compression_ratio": 1.6162162162162161, "no_speech_prob": 4.092810286238091e-06}, {"id": 1210, "seek": 388024, "start": 3898.3599999999997, "end": 3903.56, "text": " to whatever they were before, but move to the device.", "tokens": [281, 2035, 436, 645, 949, 11, 457, 1286, 281, 264, 4302, 13], "temperature": 0.0, "avg_logprob": -0.14736113828771255, "compression_ratio": 1.6162162162162161, "no_speech_prob": 4.092810286238091e-06}, {"id": 1211, "seek": 388024, "start": 3903.56, "end": 3904.56, "text": " So that's it.", "tokens": [407, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.14736113828771255, "compression_ratio": 1.6162162162162161, "no_speech_prob": 4.092810286238091e-06}, {"id": 1212, "seek": 388024, "start": 3904.56, "end": 3907.3199999999997, "text": " That's going to run everything on CUDA.", "tokens": [663, 311, 516, 281, 1190, 1203, 322, 29777, 7509, 13], "temperature": 0.0, "avg_logprob": -0.14736113828771255, "compression_ratio": 1.6162162162162161, "no_speech_prob": 4.092810286238091e-06}, {"id": 1213, "seek": 388024, "start": 3907.3199999999997, "end": 3909.3199999999997, "text": " That's all we need.", "tokens": [663, 311, 439, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.14736113828771255, "compression_ratio": 1.6162162162162161, "no_speech_prob": 4.092810286238091e-06}, {"id": 1214, "seek": 390932, "start": 3909.32, "end": 3911.28, "text": " This is kind of flexible, because we can put things", "tokens": [639, 307, 733, 295, 11358, 11, 570, 321, 393, 829, 721], "temperature": 0.0, "avg_logprob": -0.12417178995469037, "compression_ratio": 1.6962962962962962, "no_speech_prob": 9.81796983978711e-06}, {"id": 1215, "seek": 390932, "start": 3911.28, "end": 3913.84, "text": " on any device we want.", "tokens": [322, 604, 4302, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.12417178995469037, "compression_ratio": 1.6962962962962962, "no_speech_prob": 9.81796983978711e-06}, {"id": 1216, "seek": 390932, "start": 3913.84, "end": 3917.36, "text": " Maybe more easily is just to call this once, which", "tokens": [2704, 544, 3612, 307, 445, 281, 818, 341, 1564, 11, 597], "temperature": 0.0, "avg_logprob": -0.12417178995469037, "compression_ratio": 1.6962962962962962, "no_speech_prob": 9.81796983978711e-06}, {"id": 1217, "seek": 390932, "start": 3917.36, "end": 3919.32, "text": " is torch.cuda.setdevice.", "tokens": [307, 27822, 13, 66, 11152, 13, 3854, 40343, 573, 13], "temperature": 0.0, "avg_logprob": -0.12417178995469037, "compression_ratio": 1.6962962962962962, "no_speech_prob": 9.81796983978711e-06}, {"id": 1218, "seek": 390932, "start": 3919.32, "end": 3922.1600000000003, "text": " And you don't even need to do this if you've only got one GPU.", "tokens": [400, 291, 500, 380, 754, 643, 281, 360, 341, 498, 291, 600, 787, 658, 472, 18407, 13], "temperature": 0.0, "avg_logprob": -0.12417178995469037, "compression_ratio": 1.6962962962962962, "no_speech_prob": 9.81796983978711e-06}, {"id": 1219, "seek": 390932, "start": 3922.1600000000003, "end": 3923.84, "text": " And then everything by default will now", "tokens": [400, 550, 1203, 538, 7576, 486, 586], "temperature": 0.0, "avg_logprob": -0.12417178995469037, "compression_ratio": 1.6962962962962962, "no_speech_prob": 9.81796983978711e-06}, {"id": 1220, "seek": 390932, "start": 3923.84, "end": 3925.2400000000002, "text": " be sent to that device.", "tokens": [312, 2279, 281, 300, 4302, 13], "temperature": 0.0, "avg_logprob": -0.12417178995469037, "compression_ratio": 1.6962962962962962, "no_speech_prob": 9.81796983978711e-06}, {"id": 1221, "seek": 390932, "start": 3925.2400000000002, "end": 3927.1600000000003, "text": " And then instead of saying.2 device,", "tokens": [400, 550, 2602, 295, 1566, 2411, 17, 4302, 11], "temperature": 0.0, "avg_logprob": -0.12417178995469037, "compression_ratio": 1.6962962962962962, "no_speech_prob": 9.81796983978711e-06}, {"id": 1222, "seek": 390932, "start": 3927.1600000000003, "end": 3929.7200000000003, "text": " we can just say.cuda.", "tokens": [321, 393, 445, 584, 2411, 66, 11152, 13], "temperature": 0.0, "avg_logprob": -0.12417178995469037, "compression_ratio": 1.6962962962962962, "no_speech_prob": 9.81796983978711e-06}, {"id": 1223, "seek": 390932, "start": 3929.7200000000003, "end": 3932.0800000000004, "text": " And so since we're doing pretty much everything with just one", "tokens": [400, 370, 1670, 321, 434, 884, 1238, 709, 1203, 365, 445, 472], "temperature": 0.0, "avg_logprob": -0.12417178995469037, "compression_ratio": 1.6962962962962962, "no_speech_prob": 9.81796983978711e-06}, {"id": 1224, "seek": 390932, "start": 3932.0800000000004, "end": 3935.6800000000003, "text": " GPU for this course, this is the one we're going to export.", "tokens": [18407, 337, 341, 1164, 11, 341, 307, 264, 472, 321, 434, 516, 281, 10725, 13], "temperature": 0.0, "avg_logprob": -0.12417178995469037, "compression_ratio": 1.6962962962962962, "no_speech_prob": 9.81796983978711e-06}, {"id": 1225, "seek": 393568, "start": 3935.68, "end": 3940.56, "text": " So just model.cuda, xb.cuda, yb.cuda.", "tokens": [407, 445, 2316, 13, 66, 11152, 11, 2031, 65, 13, 66, 11152, 11, 288, 65, 13, 66, 11152, 13], "temperature": 0.0, "avg_logprob": -0.09514896851733215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 4.029213414469268e-06}, {"id": 1226, "seek": 393568, "start": 3940.56, "end": 3942.24, "text": " So that's our CUDA callback.", "tokens": [407, 300, 311, 527, 29777, 7509, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.09514896851733215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 4.029213414469268e-06}, {"id": 1227, "seek": 393568, "start": 3942.24, "end": 3944.9199999999996, "text": " So let's add that to our callback functions,", "tokens": [407, 718, 311, 909, 300, 281, 527, 818, 3207, 6828, 11], "temperature": 0.0, "avg_logprob": -0.09514896851733215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 4.029213414469268e-06}, {"id": 1228, "seek": 393568, "start": 3944.9199999999996, "end": 3947.44, "text": " grab our model and our runner, and fit.", "tokens": [4444, 527, 2316, 293, 527, 24376, 11, 293, 3318, 13], "temperature": 0.0, "avg_logprob": -0.09514896851733215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 4.029213414469268e-06}, {"id": 1229, "seek": 393568, "start": 3947.44, "end": 3951.04, "text": " And now we can do three epochs in five seconds versus one", "tokens": [400, 586, 321, 393, 360, 1045, 30992, 28346, 294, 1732, 3949, 5717, 472], "temperature": 0.0, "avg_logprob": -0.09514896851733215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 4.029213414469268e-06}, {"id": 1230, "seek": 393568, "start": 3951.04, "end": 3952.08, "text": " epoch in six seconds.", "tokens": [30992, 339, 294, 2309, 3949, 13], "temperature": 0.0, "avg_logprob": -0.09514896851733215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 4.029213414469268e-06}, {"id": 1231, "seek": 393568, "start": 3952.08, "end": 3953.44, "text": " So that's a lot better.", "tokens": [407, 300, 311, 257, 688, 1101, 13], "temperature": 0.0, "avg_logprob": -0.09514896851733215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 4.029213414469268e-06}, {"id": 1232, "seek": 393568, "start": 3953.44, "end": 3955.6, "text": " And for a much deeper model, it'll", "tokens": [400, 337, 257, 709, 7731, 2316, 11, 309, 603], "temperature": 0.0, "avg_logprob": -0.09514896851733215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 4.029213414469268e-06}, {"id": 1233, "seek": 393568, "start": 3955.6, "end": 3957.96, "text": " be dozens of times faster.", "tokens": [312, 18431, 295, 1413, 4663, 13], "temperature": 0.0, "avg_logprob": -0.09514896851733215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 4.029213414469268e-06}, {"id": 1234, "seek": 393568, "start": 3957.96, "end": 3961.3599999999997, "text": " So this is literally all we need to use CUDA.", "tokens": [407, 341, 307, 3736, 439, 321, 643, 281, 764, 29777, 7509, 13], "temperature": 0.0, "avg_logprob": -0.09514896851733215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 4.029213414469268e-06}, {"id": 1235, "seek": 393568, "start": 3961.3599999999997, "end": 3964.52, "text": " So that was nice and easy.", "tokens": [407, 300, 390, 1481, 293, 1858, 13], "temperature": 0.0, "avg_logprob": -0.09514896851733215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 4.029213414469268e-06}, {"id": 1236, "seek": 396452, "start": 3964.52, "end": 3966.72, "text": " Now we want to make it easier to create different kinds", "tokens": [823, 321, 528, 281, 652, 309, 3571, 281, 1884, 819, 3685], "temperature": 0.0, "avg_logprob": -0.14416803189409458, "compression_ratio": 1.678714859437751, "no_speech_prob": 1.1842599633382633e-05}, {"id": 1237, "seek": 396452, "start": 3966.72, "end": 3970.16, "text": " of architectures, make things a bit easier.", "tokens": [295, 6331, 1303, 11, 652, 721, 257, 857, 3571, 13], "temperature": 0.0, "avg_logprob": -0.14416803189409458, "compression_ratio": 1.678714859437751, "no_speech_prob": 1.1842599633382633e-05}, {"id": 1238, "seek": 396452, "start": 3970.16, "end": 3971.96, "text": " So the first thing we should do is recognize", "tokens": [407, 264, 700, 551, 321, 820, 360, 307, 5521], "temperature": 0.0, "avg_logprob": -0.14416803189409458, "compression_ratio": 1.678714859437751, "no_speech_prob": 1.1842599633382633e-05}, {"id": 1239, "seek": 396452, "start": 3971.96, "end": 3974.7599999999998, "text": " that we go conv relu a lot.", "tokens": [300, 321, 352, 3754, 1039, 84, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.14416803189409458, "compression_ratio": 1.678714859437751, "no_speech_prob": 1.1842599633382633e-05}, {"id": 1240, "seek": 396452, "start": 3974.7599999999998, "end": 3977.28, "text": " So let's pop that into a function called conv2d that", "tokens": [407, 718, 311, 1665, 300, 666, 257, 2445, 1219, 3754, 17, 67, 300], "temperature": 0.0, "avg_logprob": -0.14416803189409458, "compression_ratio": 1.678714859437751, "no_speech_prob": 1.1842599633382633e-05}, {"id": 1241, "seek": 396452, "start": 3977.28, "end": 3979.08, "text": " just goes conv relu.", "tokens": [445, 1709, 3754, 1039, 84, 13], "temperature": 0.0, "avg_logprob": -0.14416803189409458, "compression_ratio": 1.678714859437751, "no_speech_prob": 1.1842599633382633e-05}, {"id": 1242, "seek": 396452, "start": 3979.08, "end": 3981.7599999999998, "text": " Since we use a kernel size of three and a stride of two", "tokens": [4162, 321, 764, 257, 28256, 2744, 295, 1045, 293, 257, 1056, 482, 295, 732], "temperature": 0.0, "avg_logprob": -0.14416803189409458, "compression_ratio": 1.678714859437751, "no_speech_prob": 1.1842599633382633e-05}, {"id": 1243, "seek": 396452, "start": 3981.7599999999998, "end": 3985.92, "text": " in this MNIST model a lot, let's make those the defaults.", "tokens": [294, 341, 376, 45, 19756, 2316, 257, 688, 11, 718, 311, 652, 729, 264, 7576, 82, 13], "temperature": 0.0, "avg_logprob": -0.14416803189409458, "compression_ratio": 1.678714859437751, "no_speech_prob": 1.1842599633382633e-05}, {"id": 1244, "seek": 396452, "start": 3985.92, "end": 3993.16, "text": " Also, this model we can't reuse for anything except MNIST", "tokens": [2743, 11, 341, 2316, 321, 393, 380, 26225, 337, 1340, 3993, 376, 45, 19756], "temperature": 0.0, "avg_logprob": -0.14416803189409458, "compression_ratio": 1.678714859437751, "no_speech_prob": 1.1842599633382633e-05}, {"id": 1245, "seek": 399316, "start": 3993.16, "end": 3996.2, "text": " because it has a MNIST resize at the start.", "tokens": [570, 309, 575, 257, 376, 45, 19756, 50069, 412, 264, 722, 13], "temperature": 0.0, "avg_logprob": -0.11367473945961343, "compression_ratio": 1.6796536796536796, "no_speech_prob": 9.368312021251768e-06}, {"id": 1246, "seek": 399316, "start": 3996.2, "end": 3997.72, "text": " So we need to remove that.", "tokens": [407, 321, 643, 281, 4159, 300, 13], "temperature": 0.0, "avg_logprob": -0.11367473945961343, "compression_ratio": 1.6796536796536796, "no_speech_prob": 9.368312021251768e-06}, {"id": 1247, "seek": 399316, "start": 3997.72, "end": 3999.08, "text": " So if we're going to remove that,", "tokens": [407, 498, 321, 434, 516, 281, 4159, 300, 11], "temperature": 0.0, "avg_logprob": -0.11367473945961343, "compression_ratio": 1.6796536796536796, "no_speech_prob": 9.368312021251768e-06}, {"id": 1248, "seek": 399316, "start": 3999.08, "end": 4001.8399999999997, "text": " something else is going to have to do the resizing.", "tokens": [746, 1646, 307, 516, 281, 362, 281, 360, 264, 725, 3319, 13], "temperature": 0.0, "avg_logprob": -0.11367473945961343, "compression_ratio": 1.6796536796536796, "no_speech_prob": 9.368312021251768e-06}, {"id": 1249, "seek": 399316, "start": 4001.8399999999997, "end": 4005.52, "text": " And of course, the answer to that is a callback.", "tokens": [400, 295, 1164, 11, 264, 1867, 281, 300, 307, 257, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.11367473945961343, "compression_ratio": 1.6796536796536796, "no_speech_prob": 9.368312021251768e-06}, {"id": 1250, "seek": 399316, "start": 4005.52, "end": 4009.7999999999997, "text": " So here's a callback which transforms", "tokens": [407, 510, 311, 257, 818, 3207, 597, 35592], "temperature": 0.0, "avg_logprob": -0.11367473945961343, "compression_ratio": 1.6796536796536796, "no_speech_prob": 9.368312021251768e-06}, {"id": 1251, "seek": 399316, "start": 4009.7999999999997, "end": 4014.8799999999997, "text": " the independent variable, the x, for a batch.", "tokens": [264, 6695, 7006, 11, 264, 2031, 11, 337, 257, 15245, 13], "temperature": 0.0, "avg_logprob": -0.11367473945961343, "compression_ratio": 1.6796536796536796, "no_speech_prob": 9.368312021251768e-06}, {"id": 1252, "seek": 399316, "start": 4014.8799999999997, "end": 4018.08, "text": " And so you pass it some transformation function,", "tokens": [400, 370, 291, 1320, 309, 512, 9887, 2445, 11], "temperature": 0.0, "avg_logprob": -0.11367473945961343, "compression_ratio": 1.6796536796536796, "no_speech_prob": 9.368312021251768e-06}, {"id": 1253, "seek": 399316, "start": 4018.08, "end": 4021.3199999999997, "text": " which it stores away, and then begin batch simply", "tokens": [597, 309, 9512, 1314, 11, 293, 550, 1841, 15245, 2935], "temperature": 0.0, "avg_logprob": -0.11367473945961343, "compression_ratio": 1.6796536796536796, "no_speech_prob": 9.368312021251768e-06}, {"id": 1254, "seek": 402132, "start": 4021.32, "end": 4023.8, "text": " replaces the batch with the result of that transformation", "tokens": [46734, 264, 15245, 365, 264, 1874, 295, 300, 9887], "temperature": 0.0, "avg_logprob": -0.09176111745310354, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.212462277384475e-05}, {"id": 1255, "seek": 402132, "start": 4023.8, "end": 4024.96, "text": " function.", "tokens": [2445, 13], "temperature": 0.0, "avg_logprob": -0.09176111745310354, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.212462277384475e-05}, {"id": 1256, "seek": 402132, "start": 4024.96, "end": 4027.56, "text": " So now we can simply append another callback,", "tokens": [407, 586, 321, 393, 2935, 34116, 1071, 818, 3207, 11], "temperature": 0.0, "avg_logprob": -0.09176111745310354, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.212462277384475e-05}, {"id": 1257, "seek": 402132, "start": 4027.56, "end": 4033.2000000000003, "text": " which is the partial function application of that callback", "tokens": [597, 307, 264, 14641, 2445, 3861, 295, 300, 818, 3207], "temperature": 0.0, "avg_logprob": -0.09176111745310354, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.212462277384475e-05}, {"id": 1258, "seek": 402132, "start": 4033.2000000000003, "end": 4036.1200000000003, "text": " with this function.", "tokens": [365, 341, 2445, 13], "temperature": 0.0, "avg_logprob": -0.09176111745310354, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.212462277384475e-05}, {"id": 1259, "seek": 402132, "start": 4036.1200000000003, "end": 4039.6400000000003, "text": " And this function is just to view something", "tokens": [400, 341, 2445, 307, 445, 281, 1910, 746], "temperature": 0.0, "avg_logprob": -0.09176111745310354, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.212462277384475e-05}, {"id": 1260, "seek": 402132, "start": 4039.6400000000003, "end": 4042.92, "text": " at 1 by 28 by 28.", "tokens": [412, 502, 538, 7562, 538, 7562, 13], "temperature": 0.0, "avg_logprob": -0.09176111745310354, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.212462277384475e-05}, {"id": 1261, "seek": 402132, "start": 4042.92, "end": 4047.1600000000003, "text": " And you can see here we've used the trick we saw earlier", "tokens": [400, 291, 393, 536, 510, 321, 600, 1143, 264, 4282, 321, 1866, 3071], "temperature": 0.0, "avg_logprob": -0.09176111745310354, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.212462277384475e-05}, {"id": 1262, "seek": 402132, "start": 4047.1600000000003, "end": 4050.8, "text": " of using underscore inner to define a function", "tokens": [295, 1228, 37556, 7284, 281, 6964, 257, 2445], "temperature": 0.0, "avg_logprob": -0.09176111745310354, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.212462277384475e-05}, {"id": 1263, "seek": 405080, "start": 4050.8, "end": 4051.88, "text": " and then return it.", "tokens": [293, 550, 2736, 309, 13], "temperature": 0.0, "avg_logprob": -0.09310478859759391, "compression_ratio": 1.6481481481481481, "no_speech_prob": 2.2958490717428504e-06}, {"id": 1264, "seek": 405080, "start": 4051.88, "end": 4055.04, "text": " So this is something which creates a new view function", "tokens": [407, 341, 307, 746, 597, 7829, 257, 777, 1910, 2445], "temperature": 0.0, "avg_logprob": -0.09310478859759391, "compression_ratio": 1.6481481481481481, "no_speech_prob": 2.2958490717428504e-06}, {"id": 1265, "seek": 405080, "start": 4055.04, "end": 4057.6000000000004, "text": " that views it in this size.", "tokens": [300, 6809, 309, 294, 341, 2744, 13], "temperature": 0.0, "avg_logprob": -0.09310478859759391, "compression_ratio": 1.6481481481481481, "no_speech_prob": 2.2958490717428504e-06}, {"id": 1266, "seek": 405080, "start": 4057.6000000000004, "end": 4059.88, "text": " So for those of you that aren't that", "tokens": [407, 337, 729, 295, 291, 300, 3212, 380, 300], "temperature": 0.0, "avg_logprob": -0.09310478859759391, "compression_ratio": 1.6481481481481481, "no_speech_prob": 2.2958490717428504e-06}, {"id": 1267, "seek": 405080, "start": 4059.88, "end": 4064.1600000000003, "text": " comfortable with closures and partial function application,", "tokens": [4619, 365, 2611, 1303, 293, 14641, 2445, 3861, 11], "temperature": 0.0, "avg_logprob": -0.09310478859759391, "compression_ratio": 1.6481481481481481, "no_speech_prob": 2.2958490717428504e-06}, {"id": 1268, "seek": 405080, "start": 4064.1600000000003, "end": 4067.28, "text": " this is a great piece of code to study, experiment,", "tokens": [341, 307, 257, 869, 2522, 295, 3089, 281, 2979, 11, 5120, 11], "temperature": 0.0, "avg_logprob": -0.09310478859759391, "compression_ratio": 1.6481481481481481, "no_speech_prob": 2.2958490717428504e-06}, {"id": 1269, "seek": 405080, "start": 4067.28, "end": 4072.1200000000003, "text": " make sure that you feel comfortable with it.", "tokens": [652, 988, 300, 291, 841, 4619, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.09310478859759391, "compression_ratio": 1.6481481481481481, "no_speech_prob": 2.2958490717428504e-06}, {"id": 1270, "seek": 405080, "start": 4072.1200000000003, "end": 4079.0800000000004, "text": " So using this approach, we now have the MNIST view resizing", "tokens": [407, 1228, 341, 3109, 11, 321, 586, 362, 264, 376, 45, 19756, 1910, 725, 3319], "temperature": 0.0, "avg_logprob": -0.09310478859759391, "compression_ratio": 1.6481481481481481, "no_speech_prob": 2.2958490717428504e-06}, {"id": 1271, "seek": 407908, "start": 4079.08, "end": 4082.64, "text": " as a callback, which means we can remove it from the model.", "tokens": [382, 257, 818, 3207, 11, 597, 1355, 321, 393, 4159, 309, 490, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.08269301084714516, "compression_ratio": 1.5890410958904109, "no_speech_prob": 1.3006630979361944e-05}, {"id": 1272, "seek": 407908, "start": 4082.64, "end": 4087.64, "text": " So now we can create a generic getCNNModel function", "tokens": [407, 586, 321, 393, 1884, 257, 19577, 483, 34, 45, 45, 44, 41147, 2445], "temperature": 0.0, "avg_logprob": -0.08269301084714516, "compression_ratio": 1.5890410958904109, "no_speech_prob": 1.3006630979361944e-05}, {"id": 1273, "seek": 407908, "start": 4087.64, "end": 4090.36, "text": " that returns a sequential model containing", "tokens": [300, 11247, 257, 42881, 2316, 19273], "temperature": 0.0, "avg_logprob": -0.08269301084714516, "compression_ratio": 1.5890410958904109, "no_speech_prob": 1.3006630979361944e-05}, {"id": 1274, "seek": 407908, "start": 4090.36, "end": 4093.24, "text": " some arbitrary set of layers, containing", "tokens": [512, 23211, 992, 295, 7914, 11, 19273], "temperature": 0.0, "avg_logprob": -0.08269301084714516, "compression_ratio": 1.5890410958904109, "no_speech_prob": 1.3006630979361944e-05}, {"id": 1275, "seek": 407908, "start": 4093.24, "end": 4096.44, "text": " some arbitrary set of filters.", "tokens": [512, 23211, 992, 295, 15995, 13], "temperature": 0.0, "avg_logprob": -0.08269301084714516, "compression_ratio": 1.5890410958904109, "no_speech_prob": 1.3006630979361944e-05}, {"id": 1276, "seek": 407908, "start": 4096.44, "end": 4099.48, "text": " So we're going to say, OK, this is the number of filters", "tokens": [407, 321, 434, 516, 281, 584, 11, 2264, 11, 341, 307, 264, 1230, 295, 15995], "temperature": 0.0, "avg_logprob": -0.08269301084714516, "compression_ratio": 1.5890410958904109, "no_speech_prob": 1.3006630979361944e-05}, {"id": 1277, "seek": 407908, "start": 4099.48, "end": 4103.96, "text": " I have per layer, 8, 16, 32, 32.", "tokens": [286, 362, 680, 4583, 11, 1649, 11, 3165, 11, 8858, 11, 8858, 13], "temperature": 0.0, "avg_logprob": -0.08269301084714516, "compression_ratio": 1.5890410958904109, "no_speech_prob": 1.3006630979361944e-05}, {"id": 1278, "seek": 407908, "start": 4103.96, "end": 4107.88, "text": " And so here is my getCNNLayers.", "tokens": [400, 370, 510, 307, 452, 483, 34, 45, 45, 43, 320, 433, 13], "temperature": 0.0, "avg_logprob": -0.08269301084714516, "compression_ratio": 1.5890410958904109, "no_speech_prob": 1.3006630979361944e-05}, {"id": 1279, "seek": 410788, "start": 4107.88, "end": 4112.64, "text": " And the last few layers is the average pooling, flattening,", "tokens": [400, 264, 1036, 1326, 7914, 307, 264, 4274, 7005, 278, 11, 24183, 278, 11], "temperature": 0.0, "avg_logprob": -0.1541686157385508, "compression_ratio": 1.7621621621621621, "no_speech_prob": 9.972665793611668e-06}, {"id": 1280, "seek": 410788, "start": 4112.64, "end": 4114.68, "text": " and the linear layer.", "tokens": [293, 264, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1541686157385508, "compression_ratio": 1.7621621621621621, "no_speech_prob": 9.972665793611668e-06}, {"id": 1281, "seek": 410788, "start": 4114.68, "end": 4117.96, "text": " The first few layers is for every one of those filters,", "tokens": [440, 700, 1326, 7914, 307, 337, 633, 472, 295, 729, 15995, 11], "temperature": 0.0, "avg_logprob": -0.1541686157385508, "compression_ratio": 1.7621621621621621, "no_speech_prob": 9.972665793611668e-06}, {"id": 1282, "seek": 410788, "start": 4117.96, "end": 4118.88, "text": " length of the filters.", "tokens": [4641, 295, 264, 15995, 13], "temperature": 0.0, "avg_logprob": -0.1541686157385508, "compression_ratio": 1.7621621621621621, "no_speech_prob": 9.972665793611668e-06}, {"id": 1283, "seek": 410788, "start": 4118.88, "end": 4123.76, "text": " It's a Conv2D from that filter to the next one.", "tokens": [467, 311, 257, 2656, 85, 17, 35, 490, 300, 6608, 281, 264, 958, 472, 13], "temperature": 0.0, "avg_logprob": -0.1541686157385508, "compression_ratio": 1.7621621621621621, "no_speech_prob": 9.972665793611668e-06}, {"id": 1284, "seek": 410788, "start": 4123.76, "end": 4128.4400000000005, "text": " And then what's the kernel size?", "tokens": [400, 550, 437, 311, 264, 28256, 2744, 30], "temperature": 0.0, "avg_logprob": -0.1541686157385508, "compression_ratio": 1.7621621621621621, "no_speech_prob": 9.972665793611668e-06}, {"id": 1285, "seek": 410788, "start": 4128.4400000000005, "end": 4131.12, "text": " The kernel size depends.", "tokens": [440, 28256, 2744, 5946, 13], "temperature": 0.0, "avg_logprob": -0.1541686157385508, "compression_ratio": 1.7621621621621621, "no_speech_prob": 9.972665793611668e-06}, {"id": 1286, "seek": 413112, "start": 4131.12, "end": 4138.599999999999, "text": " It's a kernel size of 5 for the first layer or 3 otherwise.", "tokens": [467, 311, 257, 28256, 2744, 295, 1025, 337, 264, 700, 4583, 420, 805, 5911, 13], "temperature": 0.0, "avg_logprob": -0.12990491143588362, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.7330460195807973e-06}, {"id": 1287, "seek": 413112, "start": 4138.599999999999, "end": 4140.92, "text": " Why is that?", "tokens": [1545, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.12990491143588362, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.7330460195807973e-06}, {"id": 1288, "seek": 413112, "start": 4140.92, "end": 4149.48, "text": " Well, the number of filters we had for the first layer was 8.", "tokens": [1042, 11, 264, 1230, 295, 15995, 321, 632, 337, 264, 700, 4583, 390, 1649, 13], "temperature": 0.0, "avg_logprob": -0.12990491143588362, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.7330460195807973e-06}, {"id": 1289, "seek": 413112, "start": 4149.48, "end": 4151.44, "text": " And that's a pretty reasonable starting point", "tokens": [400, 300, 311, 257, 1238, 10585, 2891, 935], "temperature": 0.0, "avg_logprob": -0.12990491143588362, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.7330460195807973e-06}, {"id": 1290, "seek": 413112, "start": 4151.44, "end": 4153.599999999999, "text": " for a small model to start with 8 filters.", "tokens": [337, 257, 1359, 2316, 281, 722, 365, 1649, 15995, 13], "temperature": 0.0, "avg_logprob": -0.12990491143588362, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.7330460195807973e-06}, {"id": 1291, "seek": 413112, "start": 4153.599999999999, "end": 4158.32, "text": " And remember, our image had a single channel.", "tokens": [400, 1604, 11, 527, 3256, 632, 257, 2167, 2269, 13], "temperature": 0.0, "avg_logprob": -0.12990491143588362, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.7330460195807973e-06}, {"id": 1292, "seek": 413112, "start": 4158.32, "end": 4160.92, "text": " And I imagine if we had a single channel", "tokens": [400, 286, 3811, 498, 321, 632, 257, 2167, 2269], "temperature": 0.0, "avg_logprob": -0.12990491143588362, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.7330460195807973e-06}, {"id": 1293, "seek": 416092, "start": 4160.92, "end": 4164.04, "text": " and we were using 3 by 3 filters.", "tokens": [293, 321, 645, 1228, 805, 538, 805, 15995, 13], "temperature": 0.0, "avg_logprob": -0.10760582963081256, "compression_ratio": 1.484472049689441, "no_speech_prob": 6.540320555359358e-06}, {"id": 1294, "seek": 416092, "start": 4164.04, "end": 4168.52, "text": " So as that convolution kernel scrolls through the image,", "tokens": [407, 382, 300, 45216, 28256, 11369, 82, 807, 264, 3256, 11], "temperature": 0.0, "avg_logprob": -0.10760582963081256, "compression_ratio": 1.484472049689441, "no_speech_prob": 6.540320555359358e-06}, {"id": 1295, "seek": 416092, "start": 4168.52, "end": 4173.16, "text": " at each point in time, it's looking at a 3 by 3 window.", "tokens": [412, 1184, 935, 294, 565, 11, 309, 311, 1237, 412, 257, 805, 538, 805, 4910, 13], "temperature": 0.0, "avg_logprob": -0.10760582963081256, "compression_ratio": 1.484472049689441, "no_speech_prob": 6.540320555359358e-06}, {"id": 1296, "seek": 416092, "start": 4173.16, "end": 4177.2, "text": " And it's just one channel.", "tokens": [400, 309, 311, 445, 472, 2269, 13], "temperature": 0.0, "avg_logprob": -0.10760582963081256, "compression_ratio": 1.484472049689441, "no_speech_prob": 6.540320555359358e-06}, {"id": 1297, "seek": 416092, "start": 4177.2, "end": 4183.0, "text": " So in total, there's nine input activations", "tokens": [407, 294, 3217, 11, 456, 311, 4949, 4846, 2430, 763], "temperature": 0.0, "avg_logprob": -0.10760582963081256, "compression_ratio": 1.484472049689441, "no_speech_prob": 6.540320555359358e-06}, {"id": 1298, "seek": 416092, "start": 4183.0, "end": 4185.04, "text": " that it's looking at.", "tokens": [300, 309, 311, 1237, 412, 13], "temperature": 0.0, "avg_logprob": -0.10760582963081256, "compression_ratio": 1.484472049689441, "no_speech_prob": 6.540320555359358e-06}, {"id": 1299, "seek": 418504, "start": 4185.04, "end": 4193.96, "text": " And then it spits those into a dot product with, sorry,", "tokens": [400, 550, 309, 637, 1208, 729, 666, 257, 5893, 1674, 365, 11, 2597, 11], "temperature": 0.0, "avg_logprob": -0.259533421198527, "compression_ratio": 1.4296296296296296, "no_speech_prob": 4.222691131872125e-06}, {"id": 1300, "seek": 418504, "start": 4193.96, "end": 4195.48, "text": " it spits those into eight dot products.", "tokens": [309, 637, 1208, 729, 666, 3180, 5893, 3383, 13], "temperature": 0.0, "avg_logprob": -0.259533421198527, "compression_ratio": 1.4296296296296296, "no_speech_prob": 4.222691131872125e-06}, {"id": 1301, "seek": 418504, "start": 4195.48, "end": 4198.76, "text": " So a matrix multiplication, I should say.", "tokens": [407, 257, 8141, 27290, 11, 286, 820, 584, 13], "temperature": 0.0, "avg_logprob": -0.259533421198527, "compression_ratio": 1.4296296296296296, "no_speech_prob": 4.222691131872125e-06}, {"id": 1302, "seek": 418504, "start": 4202.12, "end": 4204.68, "text": " 8 by 9.", "tokens": [1649, 538, 1722, 13], "temperature": 0.0, "avg_logprob": -0.259533421198527, "compression_ratio": 1.4296296296296296, "no_speech_prob": 4.222691131872125e-06}, {"id": 1303, "seek": 418504, "start": 4204.68, "end": 4210.56, "text": " And out of that will come a vector of length 8.", "tokens": [400, 484, 295, 300, 486, 808, 257, 8062, 295, 4641, 1649, 13], "temperature": 0.0, "avg_logprob": -0.259533421198527, "compression_ratio": 1.4296296296296296, "no_speech_prob": 4.222691131872125e-06}, {"id": 1304, "seek": 421056, "start": 4210.56, "end": 4213.200000000001, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.18317848985845392, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.4144670785753988e-06}, {"id": 1305, "seek": 421056, "start": 4213.200000000001, "end": 4215.76, "text": " Because we said we wanted 8 filters.", "tokens": [1436, 321, 848, 321, 1415, 1649, 15995, 13], "temperature": 0.0, "avg_logprob": -0.18317848985845392, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.4144670785753988e-06}, {"id": 1306, "seek": 421056, "start": 4215.76, "end": 4219.360000000001, "text": " So that's what a convolution does.", "tokens": [407, 300, 311, 437, 257, 45216, 775, 13], "temperature": 0.0, "avg_logprob": -0.18317848985845392, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.4144670785753988e-06}, {"id": 1307, "seek": 421056, "start": 4219.360000000001, "end": 4222.4400000000005, "text": " And this seems pretty pointless because we started", "tokens": [400, 341, 2544, 1238, 32824, 570, 321, 1409], "temperature": 0.0, "avg_logprob": -0.18317848985845392, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.4144670785753988e-06}, {"id": 1308, "seek": 421056, "start": 4222.4400000000005, "end": 4226.76, "text": " with nine numbers and we ended it with eight numbers.", "tokens": [365, 4949, 3547, 293, 321, 4590, 309, 365, 3180, 3547, 13], "temperature": 0.0, "avg_logprob": -0.18317848985845392, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.4144670785753988e-06}, {"id": 1309, "seek": 421056, "start": 4226.76, "end": 4229.76, "text": " So all we're really doing is just reordering them.", "tokens": [407, 439, 321, 434, 534, 884, 307, 445, 319, 765, 1794, 552, 13], "temperature": 0.0, "avg_logprob": -0.18317848985845392, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.4144670785753988e-06}, {"id": 1310, "seek": 421056, "start": 4229.76, "end": 4232.4800000000005, "text": " It's not really doing any useful computation.", "tokens": [467, 311, 406, 534, 884, 604, 4420, 24903, 13], "temperature": 0.0, "avg_logprob": -0.18317848985845392, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.4144670785753988e-06}, {"id": 1311, "seek": 421056, "start": 4232.4800000000005, "end": 4238.64, "text": " So there's no point making your first layer basically just", "tokens": [407, 456, 311, 572, 935, 1455, 428, 700, 4583, 1936, 445], "temperature": 0.0, "avg_logprob": -0.18317848985845392, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.4144670785753988e-06}, {"id": 1312, "seek": 423864, "start": 4238.64, "end": 4242.160000000001, "text": " shuffle the numbers into a different order.", "tokens": [39426, 264, 3547, 666, 257, 819, 1668, 13], "temperature": 0.0, "avg_logprob": -0.14358543596769635, "compression_ratio": 1.6350710900473933, "no_speech_prob": 9.515893907519057e-06}, {"id": 1313, "seek": 423864, "start": 4242.160000000001, "end": 4244.6, "text": " So what you'll find happens in, for example,", "tokens": [407, 437, 291, 603, 915, 2314, 294, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.14358543596769635, "compression_ratio": 1.6350710900473933, "no_speech_prob": 9.515893907519057e-06}, {"id": 1314, "seek": 423864, "start": 4244.6, "end": 4247.6, "text": " most ImageNet models, most ImageNet models,", "tokens": [881, 29903, 31890, 5245, 11, 881, 29903, 31890, 5245, 11], "temperature": 0.0, "avg_logprob": -0.14358543596769635, "compression_ratio": 1.6350710900473933, "no_speech_prob": 9.515893907519057e-06}, {"id": 1315, "seek": 423864, "start": 4247.6, "end": 4250.6, "text": " they're a little bit different because they have three channels.", "tokens": [436, 434, 257, 707, 857, 819, 570, 436, 362, 1045, 9235, 13], "temperature": 0.0, "avg_logprob": -0.14358543596769635, "compression_ratio": 1.6350710900473933, "no_speech_prob": 9.515893907519057e-06}, {"id": 1316, "seek": 423864, "start": 4250.6, "end": 4259.0, "text": " So it's actually 3 by 3 by 3, which is 27.", "tokens": [407, 309, 311, 767, 805, 538, 805, 538, 805, 11, 597, 307, 7634, 13], "temperature": 0.0, "avg_logprob": -0.14358543596769635, "compression_ratio": 1.6350710900473933, "no_speech_prob": 9.515893907519057e-06}, {"id": 1317, "seek": 423864, "start": 4259.0, "end": 4262.88, "text": " But it's still kind of like, quite often with ImageNet", "tokens": [583, 309, 311, 920, 733, 295, 411, 11, 1596, 2049, 365, 29903, 31890], "temperature": 0.0, "avg_logprob": -0.14358543596769635, "compression_ratio": 1.6350710900473933, "no_speech_prob": 9.515893907519057e-06}, {"id": 1318, "seek": 423864, "start": 4262.88, "end": 4267.240000000001, "text": " models, the first layer will be like 32 channels.", "tokens": [5245, 11, 264, 700, 4583, 486, 312, 411, 8858, 9235, 13], "temperature": 0.0, "avg_logprob": -0.14358543596769635, "compression_ratio": 1.6350710900473933, "no_speech_prob": 9.515893907519057e-06}, {"id": 1319, "seek": 426724, "start": 4267.24, "end": 4271.28, "text": " So like going from 27 to 32 is literally losing information.", "tokens": [407, 411, 516, 490, 7634, 281, 8858, 307, 3736, 7027, 1589, 13], "temperature": 0.0, "avg_logprob": -0.10652215140206474, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.857264689737349e-06}, {"id": 1320, "seek": 426724, "start": 4271.28, "end": 4273.96, "text": " So most ImageNet models, they actually", "tokens": [407, 881, 29903, 31890, 5245, 11, 436, 767], "temperature": 0.0, "avg_logprob": -0.10652215140206474, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.857264689737349e-06}, {"id": 1321, "seek": 426724, "start": 4273.96, "end": 4279.5199999999995, "text": " make the first layer 7 by 7, not 3 by 3.", "tokens": [652, 264, 700, 4583, 1614, 538, 1614, 11, 406, 805, 538, 805, 13], "temperature": 0.0, "avg_logprob": -0.10652215140206474, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.857264689737349e-06}, {"id": 1322, "seek": 426724, "start": 4279.5199999999995, "end": 4281.28, "text": " And so for a similar reason, we're", "tokens": [400, 370, 337, 257, 2531, 1778, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.10652215140206474, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.857264689737349e-06}, {"id": 1323, "seek": 426724, "start": 4281.28, "end": 4284.36, "text": " going to make our first layer 5 by 5.", "tokens": [516, 281, 652, 527, 700, 4583, 1025, 538, 1025, 13], "temperature": 0.0, "avg_logprob": -0.10652215140206474, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.857264689737349e-06}, {"id": 1324, "seek": 426724, "start": 4284.36, "end": 4288.5599999999995, "text": " So we'll have 25 inputs for our eight outputs.", "tokens": [407, 321, 603, 362, 3552, 15743, 337, 527, 3180, 23930, 13], "temperature": 0.0, "avg_logprob": -0.10652215140206474, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.857264689737349e-06}, {"id": 1325, "seek": 426724, "start": 4288.5599999999995, "end": 4290.2, "text": " So this is the kind of things that you", "tokens": [407, 341, 307, 264, 733, 295, 721, 300, 291], "temperature": 0.0, "avg_logprob": -0.10652215140206474, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.857264689737349e-06}, {"id": 1326, "seek": 426724, "start": 4290.2, "end": 4294.2, "text": " want to be thinking about when you're designing or reviewing", "tokens": [528, 281, 312, 1953, 466, 562, 291, 434, 14685, 420, 19576], "temperature": 0.0, "avg_logprob": -0.10652215140206474, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.857264689737349e-06}, {"id": 1327, "seek": 426724, "start": 4294.2, "end": 4296.96, "text": " an architecture is like how many numbers are actually", "tokens": [364, 9482, 307, 411, 577, 867, 3547, 366, 767], "temperature": 0.0, "avg_logprob": -0.10652215140206474, "compression_ratio": 1.6046511627906976, "no_speech_prob": 2.857264689737349e-06}, {"id": 1328, "seek": 429696, "start": 4296.96, "end": 4300.0, "text": " going into that little dot product that", "tokens": [516, 666, 300, 707, 5893, 1674, 300], "temperature": 0.0, "avg_logprob": -0.17064331718113113, "compression_ratio": 1.68359375, "no_speech_prob": 1.2804450307157822e-05}, {"id": 1329, "seek": 429696, "start": 4300.0, "end": 4303.64, "text": " happens inside your CNN kernel.", "tokens": [2314, 1854, 428, 24859, 28256, 13], "temperature": 0.0, "avg_logprob": -0.17064331718113113, "compression_ratio": 1.68359375, "no_speech_prob": 1.2804450307157822e-05}, {"id": 1330, "seek": 429696, "start": 4303.64, "end": 4308.4, "text": " OK, so that's something which can give us a CNN model.", "tokens": [2264, 11, 370, 300, 311, 746, 597, 393, 976, 505, 257, 24859, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17064331718113113, "compression_ratio": 1.68359375, "no_speech_prob": 1.2804450307157822e-05}, {"id": 1331, "seek": 429696, "start": 4308.4, "end": 4310.76, "text": " So let's pop it all together into a little function that just", "tokens": [407, 718, 311, 1665, 309, 439, 1214, 666, 257, 707, 2445, 300, 445], "temperature": 0.0, "avg_logprob": -0.17064331718113113, "compression_ratio": 1.68359375, "no_speech_prob": 1.2804450307157822e-05}, {"id": 1332, "seek": 429696, "start": 4310.76, "end": 4314.2, "text": " grabs an optimization function, grabs an optimizer,", "tokens": [30028, 364, 19618, 2445, 11, 30028, 364, 5028, 6545, 11], "temperature": 0.0, "avg_logprob": -0.17064331718113113, "compression_ratio": 1.68359375, "no_speech_prob": 1.2804450307157822e-05}, {"id": 1333, "seek": 429696, "start": 4314.2, "end": 4316.36, "text": " grabs a learner, grabs a runner.", "tokens": [30028, 257, 33347, 11, 30028, 257, 24376, 13], "temperature": 0.0, "avg_logprob": -0.17064331718113113, "compression_ratio": 1.68359375, "no_speech_prob": 1.2804450307157822e-05}, {"id": 1334, "seek": 429696, "start": 4316.36, "end": 4318.8, "text": " And at this point, if you can't remember what any of these things", "tokens": [400, 412, 341, 935, 11, 498, 291, 393, 380, 1604, 437, 604, 295, 613, 721], "temperature": 0.0, "avg_logprob": -0.17064331718113113, "compression_ratio": 1.68359375, "no_speech_prob": 1.2804450307157822e-05}, {"id": 1335, "seek": 429696, "start": 4318.8, "end": 4322.08, "text": " does, remember we've built them all by hand from scratch.", "tokens": [775, 11, 1604, 321, 600, 3094, 552, 439, 538, 1011, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.17064331718113113, "compression_ratio": 1.68359375, "no_speech_prob": 1.2804450307157822e-05}, {"id": 1336, "seek": 429696, "start": 4322.08, "end": 4325.28, "text": " So go back and see what we wrote.", "tokens": [407, 352, 646, 293, 536, 437, 321, 4114, 13], "temperature": 0.0, "avg_logprob": -0.17064331718113113, "compression_ratio": 1.68359375, "no_speech_prob": 1.2804450307157822e-05}, {"id": 1337, "seek": 432528, "start": 4325.28, "end": 4328.28, "text": " There's no magic here.", "tokens": [821, 311, 572, 5585, 510, 13], "temperature": 0.0, "avg_logprob": -0.17646409711267194, "compression_ratio": 1.4672897196261683, "no_speech_prob": 8.397288183914497e-06}, {"id": 1338, "seek": 432528, "start": 4328.28, "end": 4333.32, "text": " And so let's look if we say getCNNModel passing in 8, 16,", "tokens": [400, 370, 718, 311, 574, 498, 321, 584, 483, 34, 45, 45, 44, 41147, 8437, 294, 1649, 11, 3165, 11], "temperature": 0.0, "avg_logprob": -0.17646409711267194, "compression_ratio": 1.4672897196261683, "no_speech_prob": 8.397288183914497e-06}, {"id": 1339, "seek": 432528, "start": 4333.32, "end": 4335.04, "text": " 32, 32.", "tokens": [8858, 11, 8858, 13], "temperature": 0.0, "avg_logprob": -0.17646409711267194, "compression_ratio": 1.4672897196261683, "no_speech_prob": 8.397288183914497e-06}, {"id": 1340, "seek": 432528, "start": 4335.04, "end": 4338.16, "text": " Here you can see 8, 16, 32, 32.", "tokens": [1692, 291, 393, 536, 1649, 11, 3165, 11, 8858, 11, 8858, 13], "temperature": 0.0, "avg_logprob": -0.17646409711267194, "compression_ratio": 1.4672897196261683, "no_speech_prob": 8.397288183914497e-06}, {"id": 1341, "seek": 432528, "start": 4338.16, "end": 4339.36, "text": " Here's our 5 by 5.", "tokens": [1692, 311, 527, 1025, 538, 1025, 13], "temperature": 0.0, "avg_logprob": -0.17646409711267194, "compression_ratio": 1.4672897196261683, "no_speech_prob": 8.397288183914497e-06}, {"id": 1342, "seek": 432528, "start": 4339.36, "end": 4341.04, "text": " The rest are 3 by 3.", "tokens": [440, 1472, 366, 805, 538, 805, 13], "temperature": 0.0, "avg_logprob": -0.17646409711267194, "compression_ratio": 1.4672897196261683, "no_speech_prob": 8.397288183914497e-06}, {"id": 1343, "seek": 432528, "start": 4341.04, "end": 4344.92, "text": " They all have a stride of 2 and then a linear layer.", "tokens": [814, 439, 362, 257, 1056, 482, 295, 568, 293, 550, 257, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.17646409711267194, "compression_ratio": 1.4672897196261683, "no_speech_prob": 8.397288183914497e-06}, {"id": 1344, "seek": 432528, "start": 4344.92, "end": 4345.88, "text": " And then trade.", "tokens": [400, 550, 4923, 13], "temperature": 0.0, "avg_logprob": -0.17646409711267194, "compression_ratio": 1.4672897196261683, "no_speech_prob": 8.397288183914497e-06}, {"id": 1345, "seek": 432528, "start": 4345.88, "end": 4350.719999999999, "text": " OK, so at this point, we've got a fairly general simple CNN", "tokens": [2264, 11, 370, 412, 341, 935, 11, 321, 600, 658, 257, 6457, 2674, 2199, 24859], "temperature": 0.0, "avg_logprob": -0.17646409711267194, "compression_ratio": 1.4672897196261683, "no_speech_prob": 8.397288183914497e-06}, {"id": 1346, "seek": 432528, "start": 4350.719999999999, "end": 4353.679999999999, "text": " creator that we can fit.", "tokens": [14181, 300, 321, 393, 3318, 13], "temperature": 0.0, "avg_logprob": -0.17646409711267194, "compression_ratio": 1.4672897196261683, "no_speech_prob": 8.397288183914497e-06}, {"id": 1347, "seek": 435368, "start": 4353.68, "end": 4357.08, "text": " And so let's try to find out what's going on inside.", "tokens": [400, 370, 718, 311, 853, 281, 915, 484, 437, 311, 516, 322, 1854, 13], "temperature": 0.0, "avg_logprob": -0.07993970823682044, "compression_ratio": 1.8981481481481481, "no_speech_prob": 5.77184164285427e-06}, {"id": 1348, "seek": 435368, "start": 4357.08, "end": 4359.16, "text": " How do we make this number higher?", "tokens": [1012, 360, 321, 652, 341, 1230, 2946, 30], "temperature": 0.0, "avg_logprob": -0.07993970823682044, "compression_ratio": 1.8981481481481481, "no_speech_prob": 5.77184164285427e-06}, {"id": 1349, "seek": 435368, "start": 4359.16, "end": 4361.68, "text": " How do we make it train more stably?", "tokens": [1012, 360, 321, 652, 309, 3847, 544, 342, 1188, 30], "temperature": 0.0, "avg_logprob": -0.07993970823682044, "compression_ratio": 1.8981481481481481, "no_speech_prob": 5.77184164285427e-06}, {"id": 1350, "seek": 435368, "start": 4361.68, "end": 4364.56, "text": " How do we make it train more quickly?", "tokens": [1012, 360, 321, 652, 309, 3847, 544, 2661, 30], "temperature": 0.0, "avg_logprob": -0.07993970823682044, "compression_ratio": 1.8981481481481481, "no_speech_prob": 5.77184164285427e-06}, {"id": 1351, "seek": 435368, "start": 4364.56, "end": 4366.8, "text": " Well, we really want to see what's going on inside.", "tokens": [1042, 11, 321, 534, 528, 281, 536, 437, 311, 516, 322, 1854, 13], "temperature": 0.0, "avg_logprob": -0.07993970823682044, "compression_ratio": 1.8981481481481481, "no_speech_prob": 5.77184164285427e-06}, {"id": 1352, "seek": 435368, "start": 4366.8, "end": 4370.64, "text": " We know already that different ways of initializing", "tokens": [492, 458, 1217, 300, 819, 2098, 295, 5883, 3319], "temperature": 0.0, "avg_logprob": -0.07993970823682044, "compression_ratio": 1.8981481481481481, "no_speech_prob": 5.77184164285427e-06}, {"id": 1353, "seek": 435368, "start": 4370.64, "end": 4373.52, "text": " changes the variance of different layers.", "tokens": [2962, 264, 21977, 295, 819, 7914, 13], "temperature": 0.0, "avg_logprob": -0.07993970823682044, "compression_ratio": 1.8981481481481481, "no_speech_prob": 5.77184164285427e-06}, {"id": 1354, "seek": 435368, "start": 4373.52, "end": 4376.4800000000005, "text": " How do we find out if it's saturating somewhere,", "tokens": [1012, 360, 321, 915, 484, 498, 309, 311, 21160, 990, 4079, 11], "temperature": 0.0, "avg_logprob": -0.07993970823682044, "compression_ratio": 1.8981481481481481, "no_speech_prob": 5.77184164285427e-06}, {"id": 1355, "seek": 435368, "start": 4376.4800000000005, "end": 4378.6, "text": " if it's too small, if it's too big?", "tokens": [498, 309, 311, 886, 1359, 11, 498, 309, 311, 886, 955, 30], "temperature": 0.0, "avg_logprob": -0.07993970823682044, "compression_ratio": 1.8981481481481481, "no_speech_prob": 5.77184164285427e-06}, {"id": 1356, "seek": 435368, "start": 4378.6, "end": 4380.52, "text": " What's going on?", "tokens": [708, 311, 516, 322, 30], "temperature": 0.0, "avg_logprob": -0.07993970823682044, "compression_ratio": 1.8981481481481481, "no_speech_prob": 5.77184164285427e-06}, {"id": 1357, "seek": 438052, "start": 4380.52, "end": 4384.72, "text": " So what if we replace nn.sequential", "tokens": [407, 437, 498, 321, 7406, 297, 77, 13, 11834, 2549], "temperature": 0.0, "avg_logprob": -0.07969089886089703, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.902271489801933e-06}, {"id": 1358, "seek": 438052, "start": 4384.72, "end": 4387.84, "text": " with our own sequential model class?", "tokens": [365, 527, 1065, 42881, 2316, 1508, 30], "temperature": 0.0, "avg_logprob": -0.07969089886089703, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.902271489801933e-06}, {"id": 1359, "seek": 438052, "start": 4387.84, "end": 4390.120000000001, "text": " And if you remember back, we've already built our own", "tokens": [400, 498, 291, 1604, 646, 11, 321, 600, 1217, 3094, 527, 1065], "temperature": 0.0, "avg_logprob": -0.07969089886089703, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.902271489801933e-06}, {"id": 1360, "seek": 438052, "start": 4390.120000000001, "end": 4391.72, "text": " sequential model class before.", "tokens": [42881, 2316, 1508, 949, 13], "temperature": 0.0, "avg_logprob": -0.07969089886089703, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.902271489801933e-06}, {"id": 1361, "seek": 438052, "start": 4391.72, "end": 4394.84, "text": " And it just had these two lines of code plus return.", "tokens": [400, 309, 445, 632, 613, 732, 3876, 295, 3089, 1804, 2736, 13], "temperature": 0.0, "avg_logprob": -0.07969089886089703, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.902271489801933e-06}, {"id": 1362, "seek": 438052, "start": 4394.84, "end": 4397.0, "text": " So let's keep the same two lines of code,", "tokens": [407, 718, 311, 1066, 264, 912, 732, 3876, 295, 3089, 11], "temperature": 0.0, "avg_logprob": -0.07969089886089703, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.902271489801933e-06}, {"id": 1363, "seek": 438052, "start": 4397.0, "end": 4399.56, "text": " but also add two more lines of code", "tokens": [457, 611, 909, 732, 544, 3876, 295, 3089], "temperature": 0.0, "avg_logprob": -0.07969089886089703, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.902271489801933e-06}, {"id": 1364, "seek": 438052, "start": 4399.56, "end": 4402.080000000001, "text": " that grabs the mean of the outputs", "tokens": [300, 30028, 264, 914, 295, 264, 23930], "temperature": 0.0, "avg_logprob": -0.07969089886089703, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.902271489801933e-06}, {"id": 1365, "seek": 438052, "start": 4402.080000000001, "end": 4404.360000000001, "text": " and the standard deviation of the outputs", "tokens": [293, 264, 3832, 25163, 295, 264, 23930], "temperature": 0.0, "avg_logprob": -0.07969089886089703, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.902271489801933e-06}, {"id": 1366, "seek": 438052, "start": 4404.360000000001, "end": 4408.92, "text": " and saves them away inside a bunch of lists.", "tokens": [293, 19155, 552, 1314, 1854, 257, 3840, 295, 14511, 13], "temperature": 0.0, "avg_logprob": -0.07969089886089703, "compression_ratio": 1.8222222222222222, "no_speech_prob": 2.902271489801933e-06}, {"id": 1367, "seek": 440892, "start": 4408.92, "end": 4412.76, "text": " So here's a list for every layer for means", "tokens": [407, 510, 311, 257, 1329, 337, 633, 4583, 337, 1355], "temperature": 0.0, "avg_logprob": -0.08324944178263347, "compression_ratio": 1.8818181818181818, "no_speech_prob": 5.3380981626105495e-06}, {"id": 1368, "seek": 440892, "start": 4412.76, "end": 4417.28, "text": " and a list for every layer for standard deviations.", "tokens": [293, 257, 1329, 337, 633, 4583, 337, 3832, 31219, 763, 13], "temperature": 0.0, "avg_logprob": -0.08324944178263347, "compression_ratio": 1.8818181818181818, "no_speech_prob": 5.3380981626105495e-06}, {"id": 1369, "seek": 440892, "start": 4417.28, "end": 4420.04, "text": " So let's calculate the mean and standard deviations,", "tokens": [407, 718, 311, 8873, 264, 914, 293, 3832, 31219, 763, 11], "temperature": 0.0, "avg_logprob": -0.08324944178263347, "compression_ratio": 1.8818181818181818, "no_speech_prob": 5.3380981626105495e-06}, {"id": 1370, "seek": 440892, "start": 4420.04, "end": 4422.56, "text": " pop them inside those two lists.", "tokens": [1665, 552, 1854, 729, 732, 14511, 13], "temperature": 0.0, "avg_logprob": -0.08324944178263347, "compression_ratio": 1.8818181818181818, "no_speech_prob": 5.3380981626105495e-06}, {"id": 1371, "seek": 440892, "start": 4422.56, "end": 4424.4, "text": " And so now it's a sequential model", "tokens": [400, 370, 586, 309, 311, 257, 42881, 2316], "temperature": 0.0, "avg_logprob": -0.08324944178263347, "compression_ratio": 1.8818181818181818, "no_speech_prob": 5.3380981626105495e-06}, {"id": 1372, "seek": 440892, "start": 4424.4, "end": 4427.08, "text": " that also keeps track of what's going on,", "tokens": [300, 611, 5965, 2837, 295, 437, 311, 516, 322, 11], "temperature": 0.0, "avg_logprob": -0.08324944178263347, "compression_ratio": 1.8818181818181818, "no_speech_prob": 5.3380981626105495e-06}, {"id": 1373, "seek": 440892, "start": 4427.08, "end": 4429.04, "text": " the telemetry of the model.", "tokens": [264, 4304, 5537, 627, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.08324944178263347, "compression_ratio": 1.8818181818181818, "no_speech_prob": 5.3380981626105495e-06}, {"id": 1374, "seek": 440892, "start": 4430.04, "end": 4432.28, "text": " So we can now create it in the same way as usual,", "tokens": [407, 321, 393, 586, 1884, 309, 294, 264, 912, 636, 382, 7713, 11], "temperature": 0.0, "avg_logprob": -0.08324944178263347, "compression_ratio": 1.8818181818181818, "no_speech_prob": 5.3380981626105495e-06}, {"id": 1375, "seek": 440892, "start": 4432.28, "end": 4434.04, "text": " fit it in the same way as usual,", "tokens": [3318, 309, 294, 264, 912, 636, 382, 7713, 11], "temperature": 0.0, "avg_logprob": -0.08324944178263347, "compression_ratio": 1.8818181818181818, "no_speech_prob": 5.3380981626105495e-06}, {"id": 1376, "seek": 440892, "start": 4434.04, "end": 4436.08, "text": " but now our model has two extra things in it.", "tokens": [457, 586, 527, 2316, 575, 732, 2857, 721, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.08324944178263347, "compression_ratio": 1.8818181818181818, "no_speech_prob": 5.3380981626105495e-06}, {"id": 1377, "seek": 443608, "start": 4436.08, "end": 4438.96, "text": " It has an act means and an act standard deviations.", "tokens": [467, 575, 364, 605, 1355, 293, 364, 605, 3832, 31219, 763, 13], "temperature": 0.0, "avg_logprob": -0.10991340098173721, "compression_ratio": 1.560747663551402, "no_speech_prob": 1.028931546898093e-05}, {"id": 1378, "seek": 443608, "start": 4439.84, "end": 4443.72, "text": " So let's plot the act means", "tokens": [407, 718, 311, 7542, 264, 605, 1355], "temperature": 0.0, "avg_logprob": -0.10991340098173721, "compression_ratio": 1.560747663551402, "no_speech_prob": 1.028931546898093e-05}, {"id": 1379, "seek": 443608, "start": 4443.72, "end": 4445.76, "text": " for every one of those lists that we had.", "tokens": [337, 633, 472, 295, 729, 14511, 300, 321, 632, 13], "temperature": 0.0, "avg_logprob": -0.10991340098173721, "compression_ratio": 1.560747663551402, "no_speech_prob": 1.028931546898093e-05}, {"id": 1380, "seek": 443608, "start": 4446.76, "end": 4447.88, "text": " And here it is, right?", "tokens": [400, 510, 309, 307, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.10991340098173721, "compression_ratio": 1.560747663551402, "no_speech_prob": 1.028931546898093e-05}, {"id": 1381, "seek": 443608, "start": 4447.88, "end": 4449.64, "text": " Here's all of the different means.", "tokens": [1692, 311, 439, 295, 264, 819, 1355, 13], "temperature": 0.0, "avg_logprob": -0.10991340098173721, "compression_ratio": 1.560747663551402, "no_speech_prob": 1.028931546898093e-05}, {"id": 1382, "seek": 443608, "start": 4450.76, "end": 4454.48, "text": " And you can see it looks absolutely awful.", "tokens": [400, 291, 393, 536, 309, 1542, 3122, 11232, 13], "temperature": 0.0, "avg_logprob": -0.10991340098173721, "compression_ratio": 1.560747663551402, "no_speech_prob": 1.028931546898093e-05}, {"id": 1383, "seek": 443608, "start": 4455.44, "end": 4459.4, "text": " What happens early in training is every layer,", "tokens": [708, 2314, 2440, 294, 3097, 307, 633, 4583, 11], "temperature": 0.0, "avg_logprob": -0.10991340098173721, "compression_ratio": 1.560747663551402, "no_speech_prob": 1.028931546898093e-05}, {"id": 1384, "seek": 443608, "start": 4459.4, "end": 4462.2, "text": " the means get exponentially bigger", "tokens": [264, 1355, 483, 37330, 3801], "temperature": 0.0, "avg_logprob": -0.10991340098173721, "compression_ratio": 1.560747663551402, "no_speech_prob": 1.028931546898093e-05}, {"id": 1385, "seek": 443608, "start": 4463.2, "end": 4465.48, "text": " until they suddenly collapse.", "tokens": [1826, 436, 5800, 15584, 13], "temperature": 0.0, "avg_logprob": -0.10991340098173721, "compression_ratio": 1.560747663551402, "no_speech_prob": 1.028931546898093e-05}, {"id": 1386, "seek": 446548, "start": 4465.48, "end": 4468.0, "text": " And then it happens again, and it suddenly collapses,", "tokens": [400, 550, 309, 2314, 797, 11, 293, 309, 5800, 48765, 11], "temperature": 0.0, "avg_logprob": -0.1327224354167561, "compression_ratio": 1.7282051282051283, "no_speech_prob": 3.555871217031381e-06}, {"id": 1387, "seek": 446548, "start": 4468.0, "end": 4469.879999999999, "text": " and it happens again, and it suddenly collapses", "tokens": [293, 309, 2314, 797, 11, 293, 309, 5800, 48765], "temperature": 0.0, "avg_logprob": -0.1327224354167561, "compression_ratio": 1.7282051282051283, "no_speech_prob": 3.555871217031381e-06}, {"id": 1388, "seek": 446548, "start": 4469.879999999999, "end": 4473.599999999999, "text": " until eventually it kind of starts training.", "tokens": [1826, 4728, 309, 733, 295, 3719, 3097, 13], "temperature": 0.0, "avg_logprob": -0.1327224354167561, "compression_ratio": 1.7282051282051283, "no_speech_prob": 3.555871217031381e-06}, {"id": 1389, "seek": 446548, "start": 4476.32, "end": 4481.32, "text": " So you might think, well, it's eventually training,", "tokens": [407, 291, 1062, 519, 11, 731, 11, 309, 311, 4728, 3097, 11], "temperature": 0.0, "avg_logprob": -0.1327224354167561, "compression_ratio": 1.7282051282051283, "no_speech_prob": 3.555871217031381e-06}, {"id": 1390, "seek": 446548, "start": 4481.679999999999, "end": 4483.24, "text": " so isn't this okay?", "tokens": [370, 1943, 380, 341, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1327224354167561, "compression_ratio": 1.7282051282051283, "no_speech_prob": 3.555871217031381e-06}, {"id": 1391, "seek": 446548, "start": 4484.28, "end": 4487.2, "text": " But my concern would be this thing", "tokens": [583, 452, 3136, 576, 312, 341, 551], "temperature": 0.0, "avg_logprob": -0.1327224354167561, "compression_ratio": 1.7282051282051283, "no_speech_prob": 3.555871217031381e-06}, {"id": 1392, "seek": 446548, "start": 4487.2, "end": 4489.16, "text": " where it kind of falls off a cliff,", "tokens": [689, 309, 733, 295, 8804, 766, 257, 22316, 11], "temperature": 0.0, "avg_logprob": -0.1327224354167561, "compression_ratio": 1.7282051282051283, "no_speech_prob": 3.555871217031381e-06}, {"id": 1393, "seek": 446548, "start": 4490.0, "end": 4494.48, "text": " there's lots of parameters in our model, right?", "tokens": [456, 311, 3195, 295, 9834, 294, 527, 2316, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1327224354167561, "compression_ratio": 1.7282051282051283, "no_speech_prob": 3.555871217031381e-06}, {"id": 1394, "seek": 449448, "start": 4494.48, "end": 4496.08, "text": " Are we sure that all of them", "tokens": [2014, 321, 988, 300, 439, 295, 552], "temperature": 0.0, "avg_logprob": -0.05548164809959522, "compression_ratio": 1.6989247311827957, "no_speech_prob": 8.013076694624033e-06}, {"id": 1395, "seek": 449448, "start": 4496.08, "end": 4498.799999999999, "text": " are getting back into reasonable places?", "tokens": [366, 1242, 646, 666, 10585, 3190, 30], "temperature": 0.0, "avg_logprob": -0.05548164809959522, "compression_ratio": 1.6989247311827957, "no_speech_prob": 8.013076694624033e-06}, {"id": 1396, "seek": 449448, "start": 4498.799999999999, "end": 4500.32, "text": " Or is it just that a few of them", "tokens": [1610, 307, 309, 445, 300, 257, 1326, 295, 552], "temperature": 0.0, "avg_logprob": -0.05548164809959522, "compression_ratio": 1.6989247311827957, "no_speech_prob": 8.013076694624033e-06}, {"id": 1397, "seek": 449448, "start": 4500.32, "end": 4501.639999999999, "text": " have got back into a reasonable place?", "tokens": [362, 658, 646, 666, 257, 10585, 1081, 30], "temperature": 0.0, "avg_logprob": -0.05548164809959522, "compression_ratio": 1.6989247311827957, "no_speech_prob": 8.013076694624033e-06}, {"id": 1398, "seek": 449448, "start": 4501.639999999999, "end": 4503.44, "text": " Like maybe the vast majority of them", "tokens": [1743, 1310, 264, 8369, 6286, 295, 552], "temperature": 0.0, "avg_logprob": -0.05548164809959522, "compression_ratio": 1.6989247311827957, "no_speech_prob": 8.013076694624033e-06}, {"id": 1399, "seek": 449448, "start": 4503.44, "end": 4505.919999999999, "text": " have like zero gradients at this point.", "tokens": [362, 411, 4018, 2771, 2448, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.05548164809959522, "compression_ratio": 1.6989247311827957, "no_speech_prob": 8.013076694624033e-06}, {"id": 1400, "seek": 449448, "start": 4505.919999999999, "end": 4507.04, "text": " I don't know.", "tokens": [286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.05548164809959522, "compression_ratio": 1.6989247311827957, "no_speech_prob": 8.013076694624033e-06}, {"id": 1401, "seek": 449448, "start": 4507.04, "end": 4511.4, "text": " It seems very likely that this awful training profile", "tokens": [467, 2544, 588, 3700, 300, 341, 11232, 3097, 7964], "temperature": 0.0, "avg_logprob": -0.05548164809959522, "compression_ratio": 1.6989247311827957, "no_speech_prob": 8.013076694624033e-06}, {"id": 1402, "seek": 449448, "start": 4511.4, "end": 4514.12, "text": " early in training is leaving our model", "tokens": [2440, 294, 3097, 307, 5012, 527, 2316], "temperature": 0.0, "avg_logprob": -0.05548164809959522, "compression_ratio": 1.6989247311827957, "no_speech_prob": 8.013076694624033e-06}, {"id": 1403, "seek": 449448, "start": 4514.12, "end": 4516.04, "text": " in a really sad state.", "tokens": [294, 257, 534, 4227, 1785, 13], "temperature": 0.0, "avg_logprob": -0.05548164809959522, "compression_ratio": 1.6989247311827957, "no_speech_prob": 8.013076694624033e-06}, {"id": 1404, "seek": 449448, "start": 4516.04, "end": 4516.879999999999, "text": " That's my guess.", "tokens": [663, 311, 452, 2041, 13], "temperature": 0.0, "avg_logprob": -0.05548164809959522, "compression_ratio": 1.6989247311827957, "no_speech_prob": 8.013076694624033e-06}, {"id": 1405, "seek": 449448, "start": 4516.879999999999, "end": 4519.0, "text": " And we're gonna check it to see later.", "tokens": [400, 321, 434, 799, 1520, 309, 281, 536, 1780, 13], "temperature": 0.0, "avg_logprob": -0.05548164809959522, "compression_ratio": 1.6989247311827957, "no_speech_prob": 8.013076694624033e-06}, {"id": 1406, "seek": 449448, "start": 4519.0, "end": 4521.08, "text": " But for now, we're just gonna say,", "tokens": [583, 337, 586, 11, 321, 434, 445, 799, 584, 11], "temperature": 0.0, "avg_logprob": -0.05548164809959522, "compression_ratio": 1.6989247311827957, "no_speech_prob": 8.013076694624033e-06}, {"id": 1407, "seek": 449448, "start": 4521.08, "end": 4523.36, "text": " let's try to make this not happen.", "tokens": [718, 311, 853, 281, 652, 341, 406, 1051, 13], "temperature": 0.0, "avg_logprob": -0.05548164809959522, "compression_ratio": 1.6989247311827957, "no_speech_prob": 8.013076694624033e-06}, {"id": 1408, "seek": 452336, "start": 4523.36, "end": 4526.4, "text": " And let's also look at the standard deviations,", "tokens": [400, 718, 311, 611, 574, 412, 264, 3832, 31219, 763, 11], "temperature": 0.0, "avg_logprob": -0.10428909528053414, "compression_ratio": 1.7112068965517242, "no_speech_prob": 8.664304004923906e-06}, {"id": 1409, "seek": 452336, "start": 4526.4, "end": 4529.0, "text": " and you see exactly the same thing.", "tokens": [293, 291, 536, 2293, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.10428909528053414, "compression_ratio": 1.7112068965517242, "no_speech_prob": 8.664304004923906e-06}, {"id": 1410, "seek": 452336, "start": 4529.0, "end": 4530.36, "text": " This just looks really bad.", "tokens": [639, 445, 1542, 534, 1578, 13], "temperature": 0.0, "avg_logprob": -0.10428909528053414, "compression_ratio": 1.7112068965517242, "no_speech_prob": 8.664304004923906e-06}, {"id": 1411, "seek": 452336, "start": 4532.36, "end": 4536.599999999999, "text": " So let's look at just the first 10 means,", "tokens": [407, 718, 311, 574, 412, 445, 264, 700, 1266, 1355, 11], "temperature": 0.0, "avg_logprob": -0.10428909528053414, "compression_ratio": 1.7112068965517242, "no_speech_prob": 8.664304004923906e-06}, {"id": 1412, "seek": 452336, "start": 4536.599999999999, "end": 4537.5599999999995, "text": " and they all look okay.", "tokens": [293, 436, 439, 574, 1392, 13], "temperature": 0.0, "avg_logprob": -0.10428909528053414, "compression_ratio": 1.7112068965517242, "no_speech_prob": 8.664304004923906e-06}, {"id": 1413, "seek": 452336, "start": 4537.5599999999995, "end": 4540.36, "text": " They're all pretty close-ish to zero,", "tokens": [814, 434, 439, 1238, 1998, 12, 742, 281, 4018, 11], "temperature": 0.0, "avg_logprob": -0.10428909528053414, "compression_ratio": 1.7112068965517242, "no_speech_prob": 8.664304004923906e-06}, {"id": 1414, "seek": 452336, "start": 4540.36, "end": 4541.759999999999, "text": " which is about what we want.", "tokens": [597, 307, 466, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.10428909528053414, "compression_ratio": 1.7112068965517242, "no_speech_prob": 8.664304004923906e-06}, {"id": 1415, "seek": 452336, "start": 4541.759999999999, "end": 4544.12, "text": " But more importantly, let's look at the standard deviations", "tokens": [583, 544, 8906, 11, 718, 311, 574, 412, 264, 3832, 31219, 763], "temperature": 0.0, "avg_logprob": -0.10428909528053414, "compression_ratio": 1.7112068965517242, "no_speech_prob": 8.664304004923906e-06}, {"id": 1416, "seek": 452336, "start": 4544.12, "end": 4546.4, "text": " for the first 10 batches.", "tokens": [337, 264, 700, 1266, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.10428909528053414, "compression_ratio": 1.7112068965517242, "no_speech_prob": 8.664304004923906e-06}, {"id": 1417, "seek": 452336, "start": 4546.4, "end": 4547.92, "text": " And this is a problem.", "tokens": [400, 341, 307, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.10428909528053414, "compression_ratio": 1.7112068965517242, "no_speech_prob": 8.664304004923906e-06}, {"id": 1418, "seek": 452336, "start": 4547.92, "end": 4552.719999999999, "text": " The first layer has a variance not too far,", "tokens": [440, 700, 4583, 575, 257, 21977, 406, 886, 1400, 11], "temperature": 0.0, "avg_logprob": -0.10428909528053414, "compression_ratio": 1.7112068965517242, "no_speech_prob": 8.664304004923906e-06}, {"id": 1419, "seek": 455272, "start": 4552.72, "end": 4555.64, "text": " the first standard deviation not too far away from one,", "tokens": [264, 700, 3832, 25163, 406, 886, 1400, 1314, 490, 472, 11], "temperature": 0.0, "avg_logprob": -0.1557997731329168, "compression_ratio": 1.809090909090909, "no_speech_prob": 5.255310043139616e-06}, {"id": 1420, "seek": 455272, "start": 4555.64, "end": 4559.16, "text": " but then not surprisingly, the next layer is lower.", "tokens": [457, 550, 406, 17600, 11, 264, 958, 4583, 307, 3126, 13], "temperature": 0.0, "avg_logprob": -0.1557997731329168, "compression_ratio": 1.809090909090909, "no_speech_prob": 5.255310043139616e-06}, {"id": 1421, "seek": 455272, "start": 4559.16, "end": 4560.72, "text": " The next layer is lower.", "tokens": [440, 958, 4583, 307, 3126, 13], "temperature": 0.0, "avg_logprob": -0.1557997731329168, "compression_ratio": 1.809090909090909, "no_speech_prob": 5.255310043139616e-06}, {"id": 1422, "seek": 455272, "start": 4560.72, "end": 4564.4400000000005, "text": " As we would expect, because the first layer is less than one,", "tokens": [1018, 321, 576, 2066, 11, 570, 264, 700, 4583, 307, 1570, 813, 472, 11], "temperature": 0.0, "avg_logprob": -0.1557997731329168, "compression_ratio": 1.809090909090909, "no_speech_prob": 5.255310043139616e-06}, {"id": 1423, "seek": 455272, "start": 4564.4400000000005, "end": 4566.64, "text": " the following layers are getting exponentially", "tokens": [264, 3480, 7914, 366, 1242, 37330], "temperature": 0.0, "avg_logprob": -0.1557997731329168, "compression_ratio": 1.809090909090909, "no_speech_prob": 5.255310043139616e-06}, {"id": 1424, "seek": 455272, "start": 4566.64, "end": 4569.68, "text": " further away from one, until the last layer", "tokens": [3052, 1314, 490, 472, 11, 1826, 264, 1036, 4583], "temperature": 0.0, "avg_logprob": -0.1557997731329168, "compression_ratio": 1.809090909090909, "no_speech_prob": 5.255310043139616e-06}, {"id": 1425, "seek": 455272, "start": 4570.6, "end": 4573.2, "text": " is really close to zero.", "tokens": [307, 534, 1998, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.1557997731329168, "compression_ratio": 1.809090909090909, "no_speech_prob": 5.255310043139616e-06}, {"id": 1426, "seek": 455272, "start": 4574.52, "end": 4578.6, "text": " So now we can kind of see what was going on here,", "tokens": [407, 586, 321, 393, 733, 295, 536, 437, 390, 516, 322, 510, 11], "temperature": 0.0, "avg_logprob": -0.1557997731329168, "compression_ratio": 1.809090909090909, "no_speech_prob": 5.255310043139616e-06}, {"id": 1427, "seek": 455272, "start": 4578.6, "end": 4581.84, "text": " is that our final layers were getting", "tokens": [307, 300, 527, 2572, 7914, 645, 1242], "temperature": 0.0, "avg_logprob": -0.1557997731329168, "compression_ratio": 1.809090909090909, "no_speech_prob": 5.255310043139616e-06}, {"id": 1428, "seek": 458184, "start": 4581.84, "end": 4583.16, "text": " basically no activations,", "tokens": [1936, 572, 2430, 763, 11], "temperature": 0.0, "avg_logprob": -0.09539654773214588, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.3549750039819628e-05}, {"id": 1429, "seek": 458184, "start": 4583.16, "end": 4585.16, "text": " they were basically getting no gradients.", "tokens": [436, 645, 1936, 1242, 572, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.09539654773214588, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.3549750039819628e-05}, {"id": 1430, "seek": 458184, "start": 4585.16, "end": 4587.68, "text": " So gradually it was moving into spaces", "tokens": [407, 13145, 309, 390, 2684, 666, 7673], "temperature": 0.0, "avg_logprob": -0.09539654773214588, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.3549750039819628e-05}, {"id": 1431, "seek": 458184, "start": 4587.68, "end": 4590.28, "text": " where they actually at least had some gradient.", "tokens": [689, 436, 767, 412, 1935, 632, 512, 16235, 13], "temperature": 0.0, "avg_logprob": -0.09539654773214588, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.3549750039819628e-05}, {"id": 1432, "seek": 458184, "start": 4590.28, "end": 4592.88, "text": " But by the time they kind of got there,", "tokens": [583, 538, 264, 565, 436, 733, 295, 658, 456, 11], "temperature": 0.0, "avg_logprob": -0.09539654773214588, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.3549750039819628e-05}, {"id": 1433, "seek": 458184, "start": 4592.88, "end": 4596.84, "text": " the gradient was so fast that they were kind of", "tokens": [264, 16235, 390, 370, 2370, 300, 436, 645, 733, 295], "temperature": 0.0, "avg_logprob": -0.09539654773214588, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.3549750039819628e-05}, {"id": 1434, "seek": 458184, "start": 4596.84, "end": 4599.56, "text": " falling off a cliff and having to start again.", "tokens": [7440, 766, 257, 22316, 293, 1419, 281, 722, 797, 13], "temperature": 0.0, "avg_logprob": -0.09539654773214588, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.3549750039819628e-05}, {"id": 1435, "seek": 458184, "start": 4600.68, "end": 4603.360000000001, "text": " So this is the thing we're gonna try and fix.", "tokens": [407, 341, 307, 264, 551, 321, 434, 799, 853, 293, 3191, 13], "temperature": 0.0, "avg_logprob": -0.09539654773214588, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.3549750039819628e-05}, {"id": 1436, "seek": 458184, "start": 4603.360000000001, "end": 4604.88, "text": " And we think we already know how to,", "tokens": [400, 321, 519, 321, 1217, 458, 577, 281, 11], "temperature": 0.0, "avg_logprob": -0.09539654773214588, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.3549750039819628e-05}, {"id": 1437, "seek": 458184, "start": 4604.88, "end": 4607.04, "text": " we can use some initialization.", "tokens": [321, 393, 764, 512, 5883, 2144, 13], "temperature": 0.0, "avg_logprob": -0.09539654773214588, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.3549750039819628e-05}, {"id": 1438, "seek": 458184, "start": 4607.04, "end": 4607.88, "text": " Yes, Rachel.", "tokens": [1079, 11, 14246, 13], "temperature": 0.0, "avg_logprob": -0.09539654773214588, "compression_ratio": 1.7302904564315353, "no_speech_prob": 2.3549750039819628e-05}, {"id": 1439, "seek": 460788, "start": 4607.88, "end": 4612.88, "text": " Did you say that if we went from 27 numbers to 32,", "tokens": [2589, 291, 584, 300, 498, 321, 1437, 490, 7634, 3547, 281, 8858, 11], "temperature": 0.0, "avg_logprob": -0.2669247733222114, "compression_ratio": 1.7382198952879582, "no_speech_prob": 5.058391980128363e-05}, {"id": 1440, "seek": 460788, "start": 4614.2, "end": 4615.92, "text": " that we were losing information?", "tokens": [300, 321, 645, 7027, 1589, 30], "temperature": 0.0, "avg_logprob": -0.2669247733222114, "compression_ratio": 1.7382198952879582, "no_speech_prob": 5.058391980128363e-05}, {"id": 1441, "seek": 460788, "start": 4615.92, "end": 4617.92, "text": " Could you say more about what that means?", "tokens": [7497, 291, 584, 544, 466, 437, 300, 1355, 30], "temperature": 0.0, "avg_logprob": -0.2669247733222114, "compression_ratio": 1.7382198952879582, "no_speech_prob": 5.058391980128363e-05}, {"id": 1442, "seek": 460788, "start": 4620.32, "end": 4623.16, "text": " I guess we're not losing information", "tokens": [286, 2041, 321, 434, 406, 7027, 1589], "temperature": 0.0, "avg_logprob": -0.2669247733222114, "compression_ratio": 1.7382198952879582, "no_speech_prob": 5.058391980128363e-05}, {"id": 1443, "seek": 460788, "start": 4623.16, "end": 4625.96, "text": " where that was poorly said.", "tokens": [689, 300, 390, 22271, 848, 13], "temperature": 0.0, "avg_logprob": -0.2669247733222114, "compression_ratio": 1.7382198952879582, "no_speech_prob": 5.058391980128363e-05}, {"id": 1444, "seek": 460788, "start": 4625.96, "end": 4628.92, "text": " We're wasting information, I guess.", "tokens": [492, 434, 20457, 1589, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.2669247733222114, "compression_ratio": 1.7382198952879582, "no_speech_prob": 5.058391980128363e-05}, {"id": 1445, "seek": 460788, "start": 4628.92, "end": 4632.12, "text": " Where like if you start with 27 numbers", "tokens": [2305, 411, 498, 291, 722, 365, 7634, 3547], "temperature": 0.0, "avg_logprob": -0.2669247733222114, "compression_ratio": 1.7382198952879582, "no_speech_prob": 5.058391980128363e-05}, {"id": 1446, "seek": 460788, "start": 4632.12, "end": 4633.76, "text": " and you do some matrix multiplication", "tokens": [293, 291, 360, 512, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.2669247733222114, "compression_ratio": 1.7382198952879582, "no_speech_prob": 5.058391980128363e-05}, {"id": 1447, "seek": 460788, "start": 4633.76, "end": 4635.4800000000005, "text": " and end up with 32 numbers,", "tokens": [293, 917, 493, 365, 8858, 3547, 11], "temperature": 0.0, "avg_logprob": -0.2669247733222114, "compression_ratio": 1.7382198952879582, "no_speech_prob": 5.058391980128363e-05}, {"id": 1448, "seek": 463548, "start": 4635.48, "end": 4639.959999999999, "text": " you're now taking more space", "tokens": [291, 434, 586, 1940, 544, 1901], "temperature": 0.0, "avg_logprob": -0.13055087150411404, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.137317308865022e-06}, {"id": 1449, "seek": 463548, "start": 4639.959999999999, "end": 4642.12, "text": " for the same information you started with.", "tokens": [337, 264, 912, 1589, 291, 1409, 365, 13], "temperature": 0.0, "avg_logprob": -0.13055087150411404, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.137317308865022e-06}, {"id": 1450, "seek": 463548, "start": 4642.12, "end": 4645.879999999999, "text": " And the whole point of a neural network layer", "tokens": [400, 264, 1379, 935, 295, 257, 18161, 3209, 4583], "temperature": 0.0, "avg_logprob": -0.13055087150411404, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.137317308865022e-06}, {"id": 1451, "seek": 463548, "start": 4645.879999999999, "end": 4648.2, "text": " is to pull out some interesting features.", "tokens": [307, 281, 2235, 484, 512, 1880, 4122, 13], "temperature": 0.0, "avg_logprob": -0.13055087150411404, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.137317308865022e-06}, {"id": 1452, "seek": 463548, "start": 4648.2, "end": 4650.24, "text": " So you would expect to have", "tokens": [407, 291, 576, 2066, 281, 362], "temperature": 0.0, "avg_logprob": -0.13055087150411404, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.137317308865022e-06}, {"id": 1453, "seek": 463548, "start": 4652.639999999999, "end": 4655.959999999999, "text": " less total activations going on", "tokens": [1570, 3217, 2430, 763, 516, 322], "temperature": 0.0, "avg_logprob": -0.13055087150411404, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.137317308865022e-06}, {"id": 1454, "seek": 463548, "start": 4655.959999999999, "end": 4656.919999999999, "text": " because you're trying to say,", "tokens": [570, 291, 434, 1382, 281, 584, 11], "temperature": 0.0, "avg_logprob": -0.13055087150411404, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.137317308865022e-06}, {"id": 1455, "seek": 463548, "start": 4656.919999999999, "end": 4661.919999999999, "text": " oh, in this area, I've kind of pulled this set of pixels", "tokens": [1954, 11, 294, 341, 1859, 11, 286, 600, 733, 295, 7373, 341, 992, 295, 18668], "temperature": 0.0, "avg_logprob": -0.13055087150411404, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.137317308865022e-06}, {"id": 1456, "seek": 463548, "start": 4662.759999999999, "end": 4664.959999999999, "text": " down into something that says how far this is", "tokens": [760, 666, 746, 300, 1619, 577, 1400, 341, 307], "temperature": 0.0, "avg_logprob": -0.13055087150411404, "compression_ratio": 1.5784753363228698, "no_speech_prob": 8.137317308865022e-06}, {"id": 1457, "seek": 466496, "start": 4664.96, "end": 4668.64, "text": " or how much of a diagonal line does this have or whatever.", "tokens": [420, 577, 709, 295, 257, 21539, 1622, 775, 341, 362, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.15165009609488553, "compression_ratio": 1.5918367346938775, "no_speech_prob": 7.765706868667621e-06}, {"id": 1458, "seek": 466496, "start": 4668.64, "end": 4673.64, "text": " So increasing the number of,", "tokens": [407, 5662, 264, 1230, 295, 11], "temperature": 0.0, "avg_logprob": -0.15165009609488553, "compression_ratio": 1.5918367346938775, "no_speech_prob": 7.765706868667621e-06}, {"id": 1459, "seek": 466496, "start": 4675.32, "end": 4677.24, "text": " the actual number of activations we have", "tokens": [264, 3539, 1230, 295, 2430, 763, 321, 362], "temperature": 0.0, "avg_logprob": -0.15165009609488553, "compression_ratio": 1.5918367346938775, "no_speech_prob": 7.765706868667621e-06}, {"id": 1460, "seek": 466496, "start": 4677.24, "end": 4680.96, "text": " for a particular position is a total waste of time.", "tokens": [337, 257, 1729, 2535, 307, 257, 3217, 5964, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.15165009609488553, "compression_ratio": 1.5918367346938775, "no_speech_prob": 7.765706868667621e-06}, {"id": 1461, "seek": 466496, "start": 4680.96, "end": 4682.36, "text": " We're not doing any useful,", "tokens": [492, 434, 406, 884, 604, 4420, 11], "temperature": 0.0, "avg_logprob": -0.15165009609488553, "compression_ratio": 1.5918367346938775, "no_speech_prob": 7.765706868667621e-06}, {"id": 1462, "seek": 466496, "start": 4683.4, "end": 4685.16, "text": " we're wasting a lot of calculation.", "tokens": [321, 434, 20457, 257, 688, 295, 17108, 13], "temperature": 0.0, "avg_logprob": -0.15165009609488553, "compression_ratio": 1.5918367346938775, "no_speech_prob": 7.765706868667621e-06}, {"id": 1463, "seek": 466496, "start": 4689.32, "end": 4692.24, "text": " We can talk more about that on the forum", "tokens": [492, 393, 751, 544, 466, 300, 322, 264, 17542], "temperature": 0.0, "avg_logprob": -0.15165009609488553, "compression_ratio": 1.5918367346938775, "no_speech_prob": 7.765706868667621e-06}, {"id": 1464, "seek": 469224, "start": 4692.24, "end": 4696.2, "text": " if that's still not clear.", "tokens": [498, 300, 311, 920, 406, 1850, 13], "temperature": 0.0, "avg_logprob": -0.10078964070377187, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.494997028814396e-06}, {"id": 1465, "seek": 469224, "start": 4696.2, "end": 4699.88, "text": " Okay, so this is,", "tokens": [1033, 11, 370, 341, 307, 11], "temperature": 0.0, "avg_logprob": -0.10078964070377187, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.494997028814396e-06}, {"id": 1466, "seek": 469224, "start": 4699.88, "end": 4703.2, "text": " so this idea of creating telemetry for your model", "tokens": [370, 341, 1558, 295, 4084, 4304, 5537, 627, 337, 428, 2316], "temperature": 0.0, "avg_logprob": -0.10078964070377187, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.494997028814396e-06}, {"id": 1467, "seek": 469224, "start": 4703.2, "end": 4704.48, "text": " is really vital.", "tokens": [307, 534, 11707, 13], "temperature": 0.0, "avg_logprob": -0.10078964070377187, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.494997028814396e-06}, {"id": 1468, "seek": 469224, "start": 4704.48, "end": 4705.88, "text": " This approach to doing it", "tokens": [639, 3109, 281, 884, 309], "temperature": 0.0, "avg_logprob": -0.10078964070377187, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.494997028814396e-06}, {"id": 1469, "seek": 469224, "start": 4705.88, "end": 4707.96, "text": " where you actually write a whole new class", "tokens": [689, 291, 767, 2464, 257, 1379, 777, 1508], "temperature": 0.0, "avg_logprob": -0.10078964070377187, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.494997028814396e-06}, {"id": 1470, "seek": 469224, "start": 4707.96, "end": 4711.8, "text": " that only can do one kind of telemetry is clearly stupid.", "tokens": [300, 787, 393, 360, 472, 733, 295, 4304, 5537, 627, 307, 4448, 6631, 13], "temperature": 0.0, "avg_logprob": -0.10078964070377187, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.494997028814396e-06}, {"id": 1471, "seek": 469224, "start": 4712.96, "end": 4715.08, "text": " And so we clearly need a better way to do it.", "tokens": [400, 370, 321, 4448, 643, 257, 1101, 636, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.10078964070377187, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.494997028814396e-06}, {"id": 1472, "seek": 469224, "start": 4715.08, "end": 4716.96, "text": " And what's the better way to do it?", "tokens": [400, 437, 311, 264, 1101, 636, 281, 360, 309, 30], "temperature": 0.0, "avg_logprob": -0.10078964070377187, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.494997028814396e-06}, {"id": 1473, "seek": 469224, "start": 4716.96, "end": 4718.44, "text": " It's callbacks, of course.", "tokens": [467, 311, 818, 17758, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.10078964070377187, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.494997028814396e-06}, {"id": 1474, "seek": 469224, "start": 4719.48, "end": 4721.16, "text": " Except we can't use our callbacks", "tokens": [16192, 321, 393, 380, 764, 527, 818, 17758], "temperature": 0.0, "avg_logprob": -0.10078964070377187, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.494997028814396e-06}, {"id": 1475, "seek": 472116, "start": 4721.16, "end": 4723.68, "text": " because we don't have a callback that says", "tokens": [570, 321, 500, 380, 362, 257, 818, 3207, 300, 1619], "temperature": 0.0, "avg_logprob": -0.08888053894042969, "compression_ratio": 1.735042735042735, "no_speech_prob": 8.664166671223938e-06}, {"id": 1476, "seek": 472116, "start": 4723.68, "end": 4728.08, "text": " when you calculate this layer, callback to our code.", "tokens": [562, 291, 8873, 341, 4583, 11, 818, 3207, 281, 527, 3089, 13], "temperature": 0.0, "avg_logprob": -0.08888053894042969, "compression_ratio": 1.735042735042735, "no_speech_prob": 8.664166671223938e-06}, {"id": 1477, "seek": 472116, "start": 4728.08, "end": 4729.4, "text": " We have no way to do that.", "tokens": [492, 362, 572, 636, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.08888053894042969, "compression_ratio": 1.735042735042735, "no_speech_prob": 8.664166671223938e-06}, {"id": 1478, "seek": 472116, "start": 4730.48, "end": 4735.48, "text": " So we actually need to use a feature inside PyTorch", "tokens": [407, 321, 767, 643, 281, 764, 257, 4111, 1854, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.08888053894042969, "compression_ratio": 1.735042735042735, "no_speech_prob": 8.664166671223938e-06}, {"id": 1479, "seek": 472116, "start": 4736.68, "end": 4740.48, "text": " that can callback into our code when a layer is calculated,", "tokens": [300, 393, 818, 3207, 666, 527, 3089, 562, 257, 4583, 307, 15598, 11], "temperature": 0.0, "avg_logprob": -0.08888053894042969, "compression_ratio": 1.735042735042735, "no_speech_prob": 8.664166671223938e-06}, {"id": 1480, "seek": 472116, "start": 4740.48, "end": 4743.08, "text": " either the forward pass or the backward pass.", "tokens": [2139, 264, 2128, 1320, 420, 264, 23897, 1320, 13], "temperature": 0.0, "avg_logprob": -0.08888053894042969, "compression_ratio": 1.735042735042735, "no_speech_prob": 8.664166671223938e-06}, {"id": 1481, "seek": 472116, "start": 4743.08, "end": 4745.96, "text": " And for reasons I can't begin to imagine,", "tokens": [400, 337, 4112, 286, 393, 380, 1841, 281, 3811, 11], "temperature": 0.0, "avg_logprob": -0.08888053894042969, "compression_ratio": 1.735042735042735, "no_speech_prob": 8.664166671223938e-06}, {"id": 1482, "seek": 472116, "start": 4745.96, "end": 4749.24, "text": " PyTorch doesn't call them callbacks, they're called hooks.", "tokens": [9953, 51, 284, 339, 1177, 380, 818, 552, 818, 17758, 11, 436, 434, 1219, 26485, 13], "temperature": 0.0, "avg_logprob": -0.08888053894042969, "compression_ratio": 1.735042735042735, "no_speech_prob": 8.664166671223938e-06}, {"id": 1483, "seek": 472116, "start": 4749.24, "end": 4750.44, "text": " But it's the same thing.", "tokens": [583, 309, 311, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.08888053894042969, "compression_ratio": 1.735042735042735, "no_speech_prob": 8.664166671223938e-06}, {"id": 1484, "seek": 475044, "start": 4750.44, "end": 4752.0, "text": " It's a callback, okay?", "tokens": [467, 311, 257, 818, 3207, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.06932437419891357, "compression_ratio": 2.0177514792899407, "no_speech_prob": 7.4110143941652495e-06}, {"id": 1485, "seek": 475044, "start": 4752.0, "end": 4756.839999999999, "text": " And so we can say, for any module,", "tokens": [400, 370, 321, 393, 584, 11, 337, 604, 10088, 11], "temperature": 0.0, "avg_logprob": -0.06932437419891357, "compression_ratio": 2.0177514792899407, "no_speech_prob": 7.4110143941652495e-06}, {"id": 1486, "seek": 475044, "start": 4756.839999999999, "end": 4761.639999999999, "text": " we can say register forward hook and pass in a function.", "tokens": [321, 393, 584, 7280, 2128, 6328, 293, 1320, 294, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.06932437419891357, "compression_ratio": 2.0177514792899407, "no_speech_prob": 7.4110143941652495e-06}, {"id": 1487, "seek": 475044, "start": 4761.639999999999, "end": 4762.48, "text": " This is a callback.", "tokens": [639, 307, 257, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.06932437419891357, "compression_ratio": 2.0177514792899407, "no_speech_prob": 7.4110143941652495e-06}, {"id": 1488, "seek": 475044, "start": 4762.48, "end": 4764.679999999999, "text": " This is a callback that will be called", "tokens": [639, 307, 257, 818, 3207, 300, 486, 312, 1219], "temperature": 0.0, "avg_logprob": -0.06932437419891357, "compression_ratio": 2.0177514792899407, "no_speech_prob": 7.4110143941652495e-06}, {"id": 1489, "seek": 475044, "start": 4764.679999999999, "end": 4768.879999999999, "text": " when this module's forward pass is calculated.", "tokens": [562, 341, 10088, 311, 2128, 1320, 307, 15598, 13], "temperature": 0.0, "avg_logprob": -0.06932437419891357, "compression_ratio": 2.0177514792899407, "no_speech_prob": 7.4110143941652495e-06}, {"id": 1490, "seek": 475044, "start": 4768.879999999999, "end": 4771.5599999999995, "text": " Or you could say register backward hook", "tokens": [1610, 291, 727, 584, 7280, 23897, 6328], "temperature": 0.0, "avg_logprob": -0.06932437419891357, "compression_ratio": 2.0177514792899407, "no_speech_prob": 7.4110143941652495e-06}, {"id": 1491, "seek": 475044, "start": 4771.5599999999995, "end": 4773.96, "text": " and that will call this function", "tokens": [293, 300, 486, 818, 341, 2445], "temperature": 0.0, "avg_logprob": -0.06932437419891357, "compression_ratio": 2.0177514792899407, "no_speech_prob": 7.4110143941652495e-06}, {"id": 1492, "seek": 475044, "start": 4773.96, "end": 4777.5199999999995, "text": " when this module's backward pass is calculated.", "tokens": [562, 341, 10088, 311, 23897, 1320, 307, 15598, 13], "temperature": 0.0, "avg_logprob": -0.06932437419891357, "compression_ratio": 2.0177514792899407, "no_speech_prob": 7.4110143941652495e-06}, {"id": 1493, "seek": 477752, "start": 4777.52, "end": 4781.280000000001, "text": " So to replace that previous thing with hooks,", "tokens": [407, 281, 7406, 300, 3894, 551, 365, 26485, 11], "temperature": 0.0, "avg_logprob": -0.047077047581575354, "compression_ratio": 1.7444933920704846, "no_speech_prob": 4.425026418175548e-06}, {"id": 1494, "seek": 477752, "start": 4781.280000000001, "end": 4784.240000000001, "text": " we can simply create a couple of global variables", "tokens": [321, 393, 2935, 1884, 257, 1916, 295, 4338, 9102], "temperature": 0.0, "avg_logprob": -0.047077047581575354, "compression_ratio": 1.7444933920704846, "no_speech_prob": 4.425026418175548e-06}, {"id": 1495, "seek": 477752, "start": 4784.240000000001, "end": 4787.4400000000005, "text": " to store our means and standard deviations for every layer.", "tokens": [281, 3531, 527, 1355, 293, 3832, 31219, 763, 337, 633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.047077047581575354, "compression_ratio": 1.7444933920704846, "no_speech_prob": 4.425026418175548e-06}, {"id": 1496, "seek": 477752, "start": 4787.4400000000005, "end": 4791.4400000000005, "text": " We can create a function to callback to", "tokens": [492, 393, 1884, 257, 2445, 281, 818, 3207, 281], "temperature": 0.0, "avg_logprob": -0.047077047581575354, "compression_ratio": 1.7444933920704846, "no_speech_prob": 4.425026418175548e-06}, {"id": 1497, "seek": 477752, "start": 4791.4400000000005, "end": 4793.88, "text": " to calculate the mean and standard deviation.", "tokens": [281, 8873, 264, 914, 293, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.047077047581575354, "compression_ratio": 1.7444933920704846, "no_speech_prob": 4.425026418175548e-06}, {"id": 1498, "seek": 477752, "start": 4793.88, "end": 4796.360000000001, "text": " And if you Google for the documentation", "tokens": [400, 498, 291, 3329, 337, 264, 14333], "temperature": 0.0, "avg_logprob": -0.047077047581575354, "compression_ratio": 1.7444933920704846, "no_speech_prob": 4.425026418175548e-06}, {"id": 1499, "seek": 477752, "start": 4796.360000000001, "end": 4797.76, "text": " for register forward hook,", "tokens": [337, 7280, 2128, 6328, 11], "temperature": 0.0, "avg_logprob": -0.047077047581575354, "compression_ratio": 1.7444933920704846, "no_speech_prob": 4.425026418175548e-06}, {"id": 1500, "seek": 477752, "start": 4798.88, "end": 4801.52, "text": " you will find that it will tell you", "tokens": [291, 486, 915, 300, 309, 486, 980, 291], "temperature": 0.0, "avg_logprob": -0.047077047581575354, "compression_ratio": 1.7444933920704846, "no_speech_prob": 4.425026418175548e-06}, {"id": 1501, "seek": 477752, "start": 4801.52, "end": 4806.040000000001, "text": " that the callback will be called with three things.", "tokens": [300, 264, 818, 3207, 486, 312, 1219, 365, 1045, 721, 13], "temperature": 0.0, "avg_logprob": -0.047077047581575354, "compression_ratio": 1.7444933920704846, "no_speech_prob": 4.425026418175548e-06}, {"id": 1502, "seek": 480604, "start": 4806.04, "end": 4808.16, "text": " The module that's doing the callback,", "tokens": [440, 10088, 300, 311, 884, 264, 818, 3207, 11], "temperature": 0.0, "avg_logprob": -0.07084051278921275, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.5935527699184604e-05}, {"id": 1503, "seek": 480604, "start": 4808.16, "end": 4812.28, "text": " the input to the module, and the output of that module,", "tokens": [264, 4846, 281, 264, 10088, 11, 293, 264, 5598, 295, 300, 10088, 11], "temperature": 0.0, "avg_logprob": -0.07084051278921275, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.5935527699184604e-05}, {"id": 1504, "seek": 480604, "start": 4812.28, "end": 4815.68, "text": " either the forward or the backward pass is appropriate.", "tokens": [2139, 264, 2128, 420, 264, 23897, 1320, 307, 6854, 13], "temperature": 0.0, "avg_logprob": -0.07084051278921275, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.5935527699184604e-05}, {"id": 1505, "seek": 480604, "start": 4815.68, "end": 4818.08, "text": " In our case, it's the output we want.", "tokens": [682, 527, 1389, 11, 309, 311, 264, 5598, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.07084051278921275, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.5935527699184604e-05}, {"id": 1506, "seek": 480604, "start": 4818.08, "end": 4820.08, "text": " And then we've got a fourth thing here,", "tokens": [400, 550, 321, 600, 658, 257, 6409, 551, 510, 11], "temperature": 0.0, "avg_logprob": -0.07084051278921275, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.5935527699184604e-05}, {"id": 1507, "seek": 480604, "start": 4820.08, "end": 4822.2, "text": " because this is the layer number we're looking at,", "tokens": [570, 341, 307, 264, 4583, 1230, 321, 434, 1237, 412, 11], "temperature": 0.0, "avg_logprob": -0.07084051278921275, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.5935527699184604e-05}, {"id": 1508, "seek": 480604, "start": 4822.2, "end": 4827.2, "text": " and we used partial to connect the appropriate closure", "tokens": [293, 321, 1143, 14641, 281, 1745, 264, 6854, 24653], "temperature": 0.0, "avg_logprob": -0.07084051278921275, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.5935527699184604e-05}, {"id": 1509, "seek": 480604, "start": 4827.2, "end": 4828.84, "text": " with each layer.", "tokens": [365, 1184, 4583, 13], "temperature": 0.0, "avg_logprob": -0.07084051278921275, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.5935527699184604e-05}, {"id": 1510, "seek": 480604, "start": 4828.84, "end": 4831.12, "text": " So once we've done that, we can call fit", "tokens": [407, 1564, 321, 600, 1096, 300, 11, 321, 393, 818, 3318], "temperature": 0.0, "avg_logprob": -0.07084051278921275, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.5935527699184604e-05}, {"id": 1511, "seek": 480604, "start": 4831.12, "end": 4832.92, "text": " and we can do exactly the same thing.", "tokens": [293, 321, 393, 360, 2293, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.07084051278921275, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.5935527699184604e-05}, {"id": 1512, "seek": 480604, "start": 4832.92, "end": 4834.5199999999995, "text": " Okay, so this is the same thing,", "tokens": [1033, 11, 370, 341, 307, 264, 912, 551, 11], "temperature": 0.0, "avg_logprob": -0.07084051278921275, "compression_ratio": 1.811764705882353, "no_speech_prob": 1.5935527699184604e-05}, {"id": 1513, "seek": 483452, "start": 4834.52, "end": 4836.040000000001, "text": " just much more convenient.", "tokens": [445, 709, 544, 10851, 13], "temperature": 0.0, "avg_logprob": -0.09266643524169922, "compression_ratio": 1.5991735537190082, "no_speech_prob": 7.2961620389833115e-06}, {"id": 1514, "seek": 483452, "start": 4836.040000000001, "end": 4839.4400000000005, "text": " And because this is such a handy thing to be able to do,", "tokens": [400, 570, 341, 307, 1270, 257, 13239, 551, 281, 312, 1075, 281, 360, 11], "temperature": 0.0, "avg_logprob": -0.09266643524169922, "compression_ratio": 1.5991735537190082, "no_speech_prob": 7.2961620389833115e-06}, {"id": 1515, "seek": 483452, "start": 4839.4400000000005, "end": 4841.56, "text": " Fast.ai has a hook class,", "tokens": [15968, 13, 1301, 575, 257, 6328, 1508, 11], "temperature": 0.0, "avg_logprob": -0.09266643524169922, "compression_ratio": 1.5991735537190082, "no_speech_prob": 7.2961620389833115e-06}, {"id": 1516, "seek": 483452, "start": 4841.56, "end": 4844.120000000001, "text": " so we can create our own hook class now,", "tokens": [370, 321, 393, 1884, 527, 1065, 6328, 1508, 586, 11], "temperature": 0.0, "avg_logprob": -0.09266643524169922, "compression_ratio": 1.5991735537190082, "no_speech_prob": 7.2961620389833115e-06}, {"id": 1517, "seek": 483452, "start": 4844.120000000001, "end": 4845.6, "text": " which allows us to,", "tokens": [597, 4045, 505, 281, 11], "temperature": 0.0, "avg_logprob": -0.09266643524169922, "compression_ratio": 1.5991735537190082, "no_speech_prob": 7.2961620389833115e-06}, {"id": 1518, "seek": 483452, "start": 4845.6, "end": 4848.68, "text": " rather than having this kind of messy global state,", "tokens": [2831, 813, 1419, 341, 733, 295, 16191, 4338, 1785, 11], "temperature": 0.0, "avg_logprob": -0.09266643524169922, "compression_ratio": 1.5991735537190082, "no_speech_prob": 7.2961620389833115e-06}, {"id": 1519, "seek": 483452, "start": 4849.72, "end": 4852.68, "text": " we can instead put the state inside the hook.", "tokens": [321, 393, 2602, 829, 264, 1785, 1854, 264, 6328, 13], "temperature": 0.0, "avg_logprob": -0.09266643524169922, "compression_ratio": 1.5991735537190082, "no_speech_prob": 7.2961620389833115e-06}, {"id": 1520, "seek": 483452, "start": 4852.68, "end": 4854.320000000001, "text": " So let's create a class called hook", "tokens": [407, 718, 311, 1884, 257, 1508, 1219, 6328], "temperature": 0.0, "avg_logprob": -0.09266643524169922, "compression_ratio": 1.5991735537190082, "no_speech_prob": 7.2961620389833115e-06}, {"id": 1521, "seek": 483452, "start": 4854.320000000001, "end": 4855.64, "text": " that when you initializes it,", "tokens": [300, 562, 291, 5883, 5660, 309, 11], "temperature": 0.0, "avg_logprob": -0.09266643524169922, "compression_ratio": 1.5991735537190082, "no_speech_prob": 7.2961620389833115e-06}, {"id": 1522, "seek": 483452, "start": 4855.64, "end": 4860.64, "text": " it registers a forward hook on some function, right?", "tokens": [309, 38351, 257, 2128, 6328, 322, 512, 2445, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.09266643524169922, "compression_ratio": 1.5991735537190082, "no_speech_prob": 7.2961620389833115e-06}, {"id": 1523, "seek": 486064, "start": 4860.64, "end": 4865.64, "text": " And what it's gonna do is it's gonna recall back", "tokens": [400, 437, 309, 311, 799, 360, 307, 309, 311, 799, 9901, 646], "temperature": 0.0, "avg_logprob": -0.11445932480895403, "compression_ratio": 1.7450980392156863, "no_speech_prob": 2.0460922314669006e-05}, {"id": 1524, "seek": 486064, "start": 4865.88, "end": 4867.320000000001, "text": " to this object.", "tokens": [281, 341, 2657, 13], "temperature": 0.0, "avg_logprob": -0.11445932480895403, "compression_ratio": 1.7450980392156863, "no_speech_prob": 2.0460922314669006e-05}, {"id": 1525, "seek": 486064, "start": 4867.320000000001, "end": 4870.0, "text": " So we pass in self with the partial.", "tokens": [407, 321, 1320, 294, 2698, 365, 264, 14641, 13], "temperature": 0.0, "avg_logprob": -0.11445932480895403, "compression_ratio": 1.7450980392156863, "no_speech_prob": 2.0460922314669006e-05}, {"id": 1526, "seek": 486064, "start": 4870.0, "end": 4873.52, "text": " And so that way we can get access to the hook.", "tokens": [400, 370, 300, 636, 321, 393, 483, 2105, 281, 264, 6328, 13], "temperature": 0.0, "avg_logprob": -0.11445932480895403, "compression_ratio": 1.7450980392156863, "no_speech_prob": 2.0460922314669006e-05}, {"id": 1527, "seek": 486064, "start": 4873.52, "end": 4878.04, "text": " We can pop inside it our two empty lists", "tokens": [492, 393, 1665, 1854, 309, 527, 732, 6707, 14511], "temperature": 0.0, "avg_logprob": -0.11445932480895403, "compression_ratio": 1.7450980392156863, "no_speech_prob": 2.0460922314669006e-05}, {"id": 1528, "seek": 486064, "start": 4878.04, "end": 4879.08, "text": " when we first call this", "tokens": [562, 321, 700, 818, 341], "temperature": 0.0, "avg_logprob": -0.11445932480895403, "compression_ratio": 1.7450980392156863, "no_speech_prob": 2.0460922314669006e-05}, {"id": 1529, "seek": 486064, "start": 4879.08, "end": 4881.84, "text": " to store away our means and standard deviations.", "tokens": [281, 3531, 1314, 527, 1355, 293, 3832, 31219, 763, 13], "temperature": 0.0, "avg_logprob": -0.11445932480895403, "compression_ratio": 1.7450980392156863, "no_speech_prob": 2.0460922314669006e-05}, {"id": 1530, "seek": 486064, "start": 4881.84, "end": 4883.280000000001, "text": " And then we can just append our means", "tokens": [400, 550, 321, 393, 445, 34116, 527, 1355], "temperature": 0.0, "avg_logprob": -0.11445932480895403, "compression_ratio": 1.7450980392156863, "no_speech_prob": 2.0460922314669006e-05}, {"id": 1531, "seek": 486064, "start": 4883.280000000001, "end": 4884.240000000001, "text": " and standard deviations.", "tokens": [293, 3832, 31219, 763, 13], "temperature": 0.0, "avg_logprob": -0.11445932480895403, "compression_ratio": 1.7450980392156863, "no_speech_prob": 2.0460922314669006e-05}, {"id": 1532, "seek": 486064, "start": 4884.240000000001, "end": 4887.96, "text": " So now we just go hooks equals", "tokens": [407, 586, 321, 445, 352, 26485, 6915], "temperature": 0.0, "avg_logprob": -0.11445932480895403, "compression_ratio": 1.7450980392156863, "no_speech_prob": 2.0460922314669006e-05}, {"id": 1533, "seek": 488796, "start": 4887.96, "end": 4892.96, "text": " hook for layer in children of my model.", "tokens": [6328, 337, 4583, 294, 2227, 295, 452, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12732252420163623, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.1842790627270006e-05}, {"id": 1534, "seek": 488796, "start": 4893.6, "end": 4894.96, "text": " I'm gonna grab the first two layers", "tokens": [286, 478, 799, 4444, 264, 700, 732, 7914], "temperature": 0.0, "avg_logprob": -0.12732252420163623, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.1842790627270006e-05}, {"id": 1535, "seek": 488796, "start": 4894.96, "end": 4896.64, "text": " because I don't care so much about the linear layers.", "tokens": [570, 286, 500, 380, 1127, 370, 709, 466, 264, 8213, 7914, 13], "temperature": 0.0, "avg_logprob": -0.12732252420163623, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.1842790627270006e-05}, {"id": 1536, "seek": 488796, "start": 4896.64, "end": 4899.16, "text": " It's really the conf layers that are most interesting.", "tokens": [467, 311, 534, 264, 1497, 7914, 300, 366, 881, 1880, 13], "temperature": 0.0, "avg_logprob": -0.12732252420163623, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.1842790627270006e-05}, {"id": 1537, "seek": 488796, "start": 4899.16, "end": 4901.84, "text": " And so now this does exactly the same thing.", "tokens": [400, 370, 586, 341, 775, 2293, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.12732252420163623, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.1842790627270006e-05}, {"id": 1538, "seek": 488796, "start": 4904.16, "end": 4907.12, "text": " Since we do this a lot,", "tokens": [4162, 321, 360, 341, 257, 688, 11], "temperature": 0.0, "avg_logprob": -0.12732252420163623, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.1842790627270006e-05}, {"id": 1539, "seek": 488796, "start": 4907.12, "end": 4911.0, "text": " let's put that into a class too called hooks.", "tokens": [718, 311, 829, 300, 666, 257, 1508, 886, 1219, 26485, 13], "temperature": 0.0, "avg_logprob": -0.12732252420163623, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.1842790627270006e-05}, {"id": 1540, "seek": 488796, "start": 4911.0, "end": 4913.64, "text": " So here's our hooks class,", "tokens": [407, 510, 311, 527, 26485, 1508, 11], "temperature": 0.0, "avg_logprob": -0.12732252420163623, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.1842790627270006e-05}, {"id": 1541, "seek": 488796, "start": 4913.64, "end": 4917.04, "text": " which simply calls hook for every module", "tokens": [597, 2935, 5498, 6328, 337, 633, 10088], "temperature": 0.0, "avg_logprob": -0.12732252420163623, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.1842790627270006e-05}, {"id": 1542, "seek": 491704, "start": 4917.04, "end": 4918.4, "text": " in some list of modules.", "tokens": [294, 512, 1329, 295, 16679, 13], "temperature": 0.0, "avg_logprob": -0.09974352827349912, "compression_ratio": 1.618421052631579, "no_speech_prob": 9.368450264446437e-06}, {"id": 1543, "seek": 491704, "start": 4921.48, "end": 4923.72, "text": " Now, something to notice is that", "tokens": [823, 11, 746, 281, 3449, 307, 300], "temperature": 0.0, "avg_logprob": -0.09974352827349912, "compression_ratio": 1.618421052631579, "no_speech_prob": 9.368450264446437e-06}, {"id": 1544, "seek": 491704, "start": 4923.72, "end": 4927.76, "text": " when you're done using a hooked module,", "tokens": [562, 291, 434, 1096, 1228, 257, 20410, 10088, 11], "temperature": 0.0, "avg_logprob": -0.09974352827349912, "compression_ratio": 1.618421052631579, "no_speech_prob": 9.368450264446437e-06}, {"id": 1545, "seek": 491704, "start": 4927.76, "end": 4931.04, "text": " you should call hook.remove", "tokens": [291, 820, 818, 6328, 13, 2579, 1682], "temperature": 0.0, "avg_logprob": -0.09974352827349912, "compression_ratio": 1.618421052631579, "no_speech_prob": 9.368450264446437e-06}, {"id": 1546, "seek": 491704, "start": 4931.04, "end": 4933.48, "text": " because otherwise if you keep registering more hooks", "tokens": [570, 5911, 498, 291, 1066, 47329, 544, 26485], "temperature": 0.0, "avg_logprob": -0.09974352827349912, "compression_ratio": 1.618421052631579, "no_speech_prob": 9.368450264446437e-06}, {"id": 1547, "seek": 491704, "start": 4933.48, "end": 4935.12, "text": " on the same module,", "tokens": [322, 264, 912, 10088, 11], "temperature": 0.0, "avg_logprob": -0.09974352827349912, "compression_ratio": 1.618421052631579, "no_speech_prob": 9.368450264446437e-06}, {"id": 1548, "seek": 491704, "start": 4935.12, "end": 4936.72, "text": " they're all gonna get called", "tokens": [436, 434, 439, 799, 483, 1219], "temperature": 0.0, "avg_logprob": -0.09974352827349912, "compression_ratio": 1.618421052631579, "no_speech_prob": 9.368450264446437e-06}, {"id": 1549, "seek": 491704, "start": 4936.72, "end": 4939.0, "text": " and eventually you're gonna run out of memory.", "tokens": [293, 4728, 291, 434, 799, 1190, 484, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.09974352827349912, "compression_ratio": 1.618421052631579, "no_speech_prob": 9.368450264446437e-06}, {"id": 1550, "seek": 491704, "start": 4939.0, "end": 4940.96, "text": " So one thing I did in our hook class", "tokens": [407, 472, 551, 286, 630, 294, 527, 6328, 1508], "temperature": 0.0, "avg_logprob": -0.09974352827349912, "compression_ratio": 1.618421052631579, "no_speech_prob": 9.368450264446437e-06}, {"id": 1551, "seek": 491704, "start": 4940.96, "end": 4943.64, "text": " was I created a Dunder Dell.", "tokens": [390, 286, 2942, 257, 413, 6617, 33319, 13], "temperature": 0.0, "avg_logprob": -0.09974352827349912, "compression_ratio": 1.618421052631579, "no_speech_prob": 9.368450264446437e-06}, {"id": 1552, "seek": 491704, "start": 4943.64, "end": 4945.24, "text": " This is called automatically", "tokens": [639, 307, 1219, 6772], "temperature": 0.0, "avg_logprob": -0.09974352827349912, "compression_ratio": 1.618421052631579, "no_speech_prob": 9.368450264446437e-06}, {"id": 1553, "seek": 494524, "start": 4945.24, "end": 4948.32, "text": " when Python cleans up some memory.", "tokens": [562, 15329, 16912, 493, 512, 4675, 13], "temperature": 0.0, "avg_logprob": -0.07391126128448837, "compression_ratio": 1.6557377049180328, "no_speech_prob": 7.888335858297069e-06}, {"id": 1554, "seek": 494524, "start": 4948.32, "end": 4950.28, "text": " So when it's done with your hook,", "tokens": [407, 562, 309, 311, 1096, 365, 428, 6328, 11], "temperature": 0.0, "avg_logprob": -0.07391126128448837, "compression_ratio": 1.6557377049180328, "no_speech_prob": 7.888335858297069e-06}, {"id": 1555, "seek": 494524, "start": 4950.28, "end": 4953.5199999999995, "text": " it will automatically call remove,", "tokens": [309, 486, 6772, 818, 4159, 11], "temperature": 0.0, "avg_logprob": -0.07391126128448837, "compression_ratio": 1.6557377049180328, "no_speech_prob": 7.888335858297069e-06}, {"id": 1556, "seek": 494524, "start": 4953.5199999999995, "end": 4956.12, "text": " which in turn will remove the hook.", "tokens": [597, 294, 1261, 486, 4159, 264, 6328, 13], "temperature": 0.0, "avg_logprob": -0.07391126128448837, "compression_ratio": 1.6557377049180328, "no_speech_prob": 7.888335858297069e-06}, {"id": 1557, "seek": 494524, "start": 4957.04, "end": 4960.48, "text": " So I then have a similar thing in hooks.", "tokens": [407, 286, 550, 362, 257, 2531, 551, 294, 26485, 13], "temperature": 0.0, "avg_logprob": -0.07391126128448837, "compression_ratio": 1.6557377049180328, "no_speech_prob": 7.888335858297069e-06}, {"id": 1558, "seek": 494524, "start": 4961.5599999999995, "end": 4964.76, "text": " So when hooks is done,", "tokens": [407, 562, 26485, 307, 1096, 11], "temperature": 0.0, "avg_logprob": -0.07391126128448837, "compression_ratio": 1.6557377049180328, "no_speech_prob": 7.888335858297069e-06}, {"id": 1559, "seek": 494524, "start": 4964.76, "end": 4966.92, "text": " it calls self.remove,", "tokens": [309, 5498, 2698, 13, 2579, 1682, 11], "temperature": 0.0, "avg_logprob": -0.07391126128448837, "compression_ratio": 1.6557377049180328, "no_speech_prob": 7.888335858297069e-06}, {"id": 1560, "seek": 494524, "start": 4966.92, "end": 4971.5199999999995, "text": " which in turn goes through every one of my registered hooks", "tokens": [597, 294, 1261, 1709, 807, 633, 472, 295, 452, 13968, 26485], "temperature": 0.0, "avg_logprob": -0.07391126128448837, "compression_ratio": 1.6557377049180328, "no_speech_prob": 7.888335858297069e-06}, {"id": 1561, "seek": 497152, "start": 4971.52, "end": 4977.56, "text": " and removes them.", "tokens": [293, 30445, 552, 13], "temperature": 0.0, "avg_logprob": -0.1038226966398308, "compression_ratio": 1.4696969696969697, "no_speech_prob": 3.966938493249472e-06}, {"id": 1562, "seek": 497152, "start": 4977.56, "end": 4982.56, "text": " You'll see that somehow I'm able to go for H in self,", "tokens": [509, 603, 536, 300, 6063, 286, 478, 1075, 281, 352, 337, 389, 294, 2698, 11], "temperature": 0.0, "avg_logprob": -0.1038226966398308, "compression_ratio": 1.4696969696969697, "no_speech_prob": 3.966938493249472e-06}, {"id": 1563, "seek": 497152, "start": 4983.92, "end": 4988.280000000001, "text": " but I haven't registered any kind of iterator here.", "tokens": [457, 286, 2378, 380, 13968, 604, 733, 295, 17138, 1639, 510, 13], "temperature": 0.0, "avg_logprob": -0.1038226966398308, "compression_ratio": 1.4696969696969697, "no_speech_prob": 3.966938493249472e-06}, {"id": 1564, "seek": 497152, "start": 4988.280000000001, "end": 4990.120000000001, "text": " And the trick is I've created something called", "tokens": [400, 264, 4282, 307, 286, 600, 2942, 746, 1219], "temperature": 0.0, "avg_logprob": -0.1038226966398308, "compression_ratio": 1.4696969696969697, "no_speech_prob": 3.966938493249472e-06}, {"id": 1565, "seek": 497152, "start": 4990.120000000001, "end": 4993.4400000000005, "text": " a list container just above,", "tokens": [257, 1329, 10129, 445, 3673, 11], "temperature": 0.0, "avg_logprob": -0.1038226966398308, "compression_ratio": 1.4696969696969697, "no_speech_prob": 3.966938493249472e-06}, {"id": 1566, "seek": 497152, "start": 4993.4400000000005, "end": 4995.200000000001, "text": " which is super handy.", "tokens": [597, 307, 1687, 13239, 13], "temperature": 0.0, "avg_logprob": -0.1038226966398308, "compression_ratio": 1.4696969696969697, "no_speech_prob": 3.966938493249472e-06}, {"id": 1567, "seek": 497152, "start": 4995.200000000001, "end": 4997.120000000001, "text": " It basically defines all the things", "tokens": [467, 1936, 23122, 439, 264, 721], "temperature": 0.0, "avg_logprob": -0.1038226966398308, "compression_ratio": 1.4696969696969697, "no_speech_prob": 3.966938493249472e-06}, {"id": 1568, "seek": 497152, "start": 4997.120000000001, "end": 5000.84, "text": " you would expect to see in a list", "tokens": [291, 576, 2066, 281, 536, 294, 257, 1329], "temperature": 0.0, "avg_logprob": -0.1038226966398308, "compression_ratio": 1.4696969696969697, "no_speech_prob": 3.966938493249472e-06}, {"id": 1569, "seek": 500084, "start": 5000.84, "end": 5004.4800000000005, "text": " using all of the various special Dunder methods.", "tokens": [1228, 439, 295, 264, 3683, 2121, 413, 6617, 7150, 13], "temperature": 0.0, "avg_logprob": -0.10831875024839889, "compression_ratio": 1.6877470355731226, "no_speech_prob": 8.939434337662533e-06}, {"id": 1570, "seek": 500084, "start": 5004.4800000000005, "end": 5005.8, "text": " And then some,", "tokens": [400, 550, 512, 11], "temperature": 0.0, "avg_logprob": -0.10831875024839889, "compression_ratio": 1.6877470355731226, "no_speech_prob": 8.939434337662533e-06}, {"id": 1571, "seek": 500084, "start": 5005.8, "end": 5009.96, "text": " it actually has some of the behavior of NumPy as well.", "tokens": [309, 767, 575, 512, 295, 264, 5223, 295, 22592, 47, 88, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.10831875024839889, "compression_ratio": 1.6877470355731226, "no_speech_prob": 8.939434337662533e-06}, {"id": 1572, "seek": 500084, "start": 5009.96, "end": 5012.12, "text": " We're not allowed to use NumPy in our foundations,", "tokens": [492, 434, 406, 4350, 281, 764, 22592, 47, 88, 294, 527, 22467, 11], "temperature": 0.0, "avg_logprob": -0.10831875024839889, "compression_ratio": 1.6877470355731226, "no_speech_prob": 8.939434337662533e-06}, {"id": 1573, "seek": 500084, "start": 5012.12, "end": 5013.84, "text": " so we use this instead.", "tokens": [370, 321, 764, 341, 2602, 13], "temperature": 0.0, "avg_logprob": -0.10831875024839889, "compression_ratio": 1.6877470355731226, "no_speech_prob": 8.939434337662533e-06}, {"id": 1574, "seek": 500084, "start": 5013.84, "end": 5015.88, "text": " And this actually also works a bit better than NumPy", "tokens": [400, 341, 767, 611, 1985, 257, 857, 1101, 813, 22592, 47, 88], "temperature": 0.0, "avg_logprob": -0.10831875024839889, "compression_ratio": 1.6877470355731226, "no_speech_prob": 8.939434337662533e-06}, {"id": 1575, "seek": 500084, "start": 5015.88, "end": 5016.72, "text": " for this stuff,", "tokens": [337, 341, 1507, 11], "temperature": 0.0, "avg_logprob": -0.10831875024839889, "compression_ratio": 1.6877470355731226, "no_speech_prob": 8.939434337662533e-06}, {"id": 1576, "seek": 500084, "start": 5016.72, "end": 5019.24, "text": " because NumPy does some weird casting", "tokens": [570, 22592, 47, 88, 775, 512, 3657, 17301], "temperature": 0.0, "avg_logprob": -0.10831875024839889, "compression_ratio": 1.6877470355731226, "no_speech_prob": 8.939434337662533e-06}, {"id": 1577, "seek": 500084, "start": 5019.24, "end": 5021.12, "text": " and weird edge cases.", "tokens": [293, 3657, 4691, 3331, 13], "temperature": 0.0, "avg_logprob": -0.10831875024839889, "compression_ratio": 1.6877470355731226, "no_speech_prob": 8.939434337662533e-06}, {"id": 1578, "seek": 500084, "start": 5021.12, "end": 5023.28, "text": " So for example,", "tokens": [407, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.10831875024839889, "compression_ratio": 1.6877470355731226, "no_speech_prob": 8.939434337662533e-06}, {"id": 1579, "seek": 500084, "start": 5023.28, "end": 5024.32, "text": " with this list container,", "tokens": [365, 341, 1329, 10129, 11], "temperature": 0.0, "avg_logprob": -0.10831875024839889, "compression_ratio": 1.6877470355731226, "no_speech_prob": 8.939434337662533e-06}, {"id": 1580, "seek": 500084, "start": 5024.32, "end": 5025.88, "text": " it's got Dunder get item.", "tokens": [309, 311, 658, 413, 6617, 483, 3174, 13], "temperature": 0.0, "avg_logprob": -0.10831875024839889, "compression_ratio": 1.6877470355731226, "no_speech_prob": 8.939434337662533e-06}, {"id": 1581, "seek": 500084, "start": 5025.88, "end": 5027.6, "text": " So that's the thing that gets called", "tokens": [407, 300, 311, 264, 551, 300, 2170, 1219], "temperature": 0.0, "avg_logprob": -0.10831875024839889, "compression_ratio": 1.6877470355731226, "no_speech_prob": 8.939434337662533e-06}, {"id": 1582, "seek": 502760, "start": 5027.6, "end": 5030.92, "text": " when you call something with square brackets, right?", "tokens": [562, 291, 818, 746, 365, 3732, 26179, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11599050823010897, "compression_ratio": 1.7096774193548387, "no_speech_prob": 3.373603249201551e-05}, {"id": 1583, "seek": 502760, "start": 5030.92, "end": 5035.84, "text": " So if you index into it with an int,", "tokens": [407, 498, 291, 8186, 666, 309, 365, 364, 560, 11], "temperature": 0.0, "avg_logprob": -0.11599050823010897, "compression_ratio": 1.7096774193548387, "no_speech_prob": 3.373603249201551e-05}, {"id": 1584, "seek": 502760, "start": 5035.84, "end": 5039.56, "text": " then we just pass that off to the enclosed list,", "tokens": [550, 321, 445, 1320, 300, 766, 281, 264, 42089, 1329, 11], "temperature": 0.0, "avg_logprob": -0.11599050823010897, "compression_ratio": 1.7096774193548387, "no_speech_prob": 3.373603249201551e-05}, {"id": 1585, "seek": 502760, "start": 5039.56, "end": 5042.120000000001, "text": " because we gave it a list to enclose.", "tokens": [570, 321, 2729, 309, 257, 1329, 281, 20987, 541, 13], "temperature": 0.0, "avg_logprob": -0.11599050823010897, "compression_ratio": 1.7096774193548387, "no_speech_prob": 3.373603249201551e-05}, {"id": 1586, "seek": 502760, "start": 5042.120000000001, "end": 5045.68, "text": " If you send it a list of bools,", "tokens": [759, 291, 2845, 309, 257, 1329, 295, 748, 19385, 11], "temperature": 0.0, "avg_logprob": -0.11599050823010897, "compression_ratio": 1.7096774193548387, "no_speech_prob": 3.373603249201551e-05}, {"id": 1587, "seek": 502760, "start": 5046.92, "end": 5050.0, "text": " like false, false, false, false, false, true, false,", "tokens": [411, 7908, 11, 7908, 11, 7908, 11, 7908, 11, 7908, 11, 2074, 11, 7908, 11], "temperature": 0.0, "avg_logprob": -0.11599050823010897, "compression_ratio": 1.7096774193548387, "no_speech_prob": 3.373603249201551e-05}, {"id": 1588, "seek": 502760, "start": 5050.0, "end": 5054.56, "text": " then it will return all of the things where that's true.", "tokens": [550, 309, 486, 2736, 439, 295, 264, 721, 689, 300, 311, 2074, 13], "temperature": 0.0, "avg_logprob": -0.11599050823010897, "compression_ratio": 1.7096774193548387, "no_speech_prob": 3.373603249201551e-05}, {"id": 1589, "seek": 505456, "start": 5054.56, "end": 5058.360000000001, "text": " Or you can index it into it with a list,", "tokens": [1610, 291, 393, 8186, 309, 666, 309, 365, 257, 1329, 11], "temperature": 0.0, "avg_logprob": -0.09160588257504206, "compression_ratio": 1.8502024291497976, "no_speech_prob": 1.4970407391956542e-05}, {"id": 1590, "seek": 505456, "start": 5058.360000000001, "end": 5061.400000000001, "text": " in which case it will return all of the index,", "tokens": [294, 597, 1389, 309, 486, 2736, 439, 295, 264, 8186, 11], "temperature": 0.0, "avg_logprob": -0.09160588257504206, "compression_ratio": 1.8502024291497976, "no_speech_prob": 1.4970407391956542e-05}, {"id": 1591, "seek": 505456, "start": 5061.400000000001, "end": 5063.88, "text": " the things that are indexed by that list, for instance.", "tokens": [264, 721, 300, 366, 8186, 292, 538, 300, 1329, 11, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.09160588257504206, "compression_ratio": 1.8502024291497976, "no_speech_prob": 1.4970407391956542e-05}, {"id": 1592, "seek": 505456, "start": 5063.88, "end": 5064.8, "text": " And it's got a length,", "tokens": [400, 309, 311, 658, 257, 4641, 11], "temperature": 0.0, "avg_logprob": -0.09160588257504206, "compression_ratio": 1.8502024291497976, "no_speech_prob": 1.4970407391956542e-05}, {"id": 1593, "seek": 505456, "start": 5064.8, "end": 5066.120000000001, "text": " which just passes off to length,", "tokens": [597, 445, 11335, 766, 281, 4641, 11], "temperature": 0.0, "avg_logprob": -0.09160588257504206, "compression_ratio": 1.8502024291497976, "no_speech_prob": 1.4970407391956542e-05}, {"id": 1594, "seek": 505456, "start": 5066.120000000001, "end": 5068.320000000001, "text": " and an iterator that passes off to iterator,", "tokens": [293, 364, 17138, 1639, 300, 11335, 766, 281, 17138, 1639, 11], "temperature": 0.0, "avg_logprob": -0.09160588257504206, "compression_ratio": 1.8502024291497976, "no_speech_prob": 1.4970407391956542e-05}, {"id": 1595, "seek": 505456, "start": 5070.160000000001, "end": 5071.320000000001, "text": " and so forth.", "tokens": [293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.09160588257504206, "compression_ratio": 1.8502024291497976, "no_speech_prob": 1.4970407391956542e-05}, {"id": 1596, "seek": 505456, "start": 5071.320000000001, "end": 5074.400000000001, "text": " And then we've also defined the representation for it,", "tokens": [400, 550, 321, 600, 611, 7642, 264, 10290, 337, 309, 11], "temperature": 0.0, "avg_logprob": -0.09160588257504206, "compression_ratio": 1.8502024291497976, "no_speech_prob": 1.4970407391956542e-05}, {"id": 1597, "seek": 505456, "start": 5074.400000000001, "end": 5076.8, "text": " such that if you print it out,", "tokens": [1270, 300, 498, 291, 4482, 309, 484, 11], "temperature": 0.0, "avg_logprob": -0.09160588257504206, "compression_ratio": 1.8502024291497976, "no_speech_prob": 1.4970407391956542e-05}, {"id": 1598, "seek": 505456, "start": 5076.8, "end": 5078.84, "text": " it just prints out the contents,", "tokens": [309, 445, 22305, 484, 264, 15768, 11], "temperature": 0.0, "avg_logprob": -0.09160588257504206, "compression_ratio": 1.8502024291497976, "no_speech_prob": 1.4970407391956542e-05}, {"id": 1599, "seek": 505456, "start": 5078.84, "end": 5080.68, "text": " unless there's more than 10 things in it,", "tokens": [5969, 456, 311, 544, 813, 1266, 721, 294, 309, 11], "temperature": 0.0, "avg_logprob": -0.09160588257504206, "compression_ratio": 1.8502024291497976, "no_speech_prob": 1.4970407391956542e-05}, {"id": 1600, "seek": 505456, "start": 5080.68, "end": 5083.6, "text": " in which case it shows dot, dot, dot.", "tokens": [294, 597, 1389, 309, 3110, 5893, 11, 5893, 11, 5893, 13], "temperature": 0.0, "avg_logprob": -0.09160588257504206, "compression_ratio": 1.8502024291497976, "no_speech_prob": 1.4970407391956542e-05}, {"id": 1601, "seek": 508360, "start": 5083.6, "end": 5086.04, "text": " So with a nice little base class like this,", "tokens": [407, 365, 257, 1481, 707, 3096, 1508, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.10832849612905959, "compression_ratio": 1.7314049586776858, "no_speech_prob": 6.8541230575647205e-06}, {"id": 1602, "seek": 508360, "start": 5086.04, "end": 5090.200000000001, "text": " so you can create really useful little base classes", "tokens": [370, 291, 393, 1884, 534, 4420, 707, 3096, 5359], "temperature": 0.0, "avg_logprob": -0.10832849612905959, "compression_ratio": 1.7314049586776858, "no_speech_prob": 6.8541230575647205e-06}, {"id": 1603, "seek": 508360, "start": 5090.200000000001, "end": 5093.0, "text": " in much less than a screen full of code.", "tokens": [294, 709, 1570, 813, 257, 2568, 1577, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.10832849612905959, "compression_ratio": 1.7314049586776858, "no_speech_prob": 6.8541230575647205e-06}, {"id": 1604, "seek": 508360, "start": 5093.0, "end": 5093.96, "text": " And then we can use them,", "tokens": [400, 550, 321, 393, 764, 552, 11], "temperature": 0.0, "avg_logprob": -0.10832849612905959, "compression_ratio": 1.7314049586776858, "no_speech_prob": 6.8541230575647205e-06}, {"id": 1605, "seek": 508360, "start": 5093.96, "end": 5096.200000000001, "text": " and we will use them everywhere from now on.", "tokens": [293, 321, 486, 764, 552, 5315, 490, 586, 322, 13], "temperature": 0.0, "avg_logprob": -0.10832849612905959, "compression_ratio": 1.7314049586776858, "no_speech_prob": 6.8541230575647205e-06}, {"id": 1606, "seek": 508360, "start": 5096.200000000001, "end": 5100.84, "text": " So now we've created our own listy class", "tokens": [407, 586, 321, 600, 2942, 527, 1065, 1329, 88, 1508], "temperature": 0.0, "avg_logprob": -0.10832849612905959, "compression_ratio": 1.7314049586776858, "no_speech_prob": 6.8541230575647205e-06}, {"id": 1607, "seek": 508360, "start": 5100.84, "end": 5103.200000000001, "text": " that has hooks in it.", "tokens": [300, 575, 26485, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.10832849612905959, "compression_ratio": 1.7314049586776858, "no_speech_prob": 6.8541230575647205e-06}, {"id": 1608, "seek": 508360, "start": 5103.200000000001, "end": 5105.4800000000005, "text": " And so now we can just use it like this.", "tokens": [400, 370, 586, 321, 393, 445, 764, 309, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.10832849612905959, "compression_ratio": 1.7314049586776858, "no_speech_prob": 6.8541230575647205e-06}, {"id": 1609, "seek": 508360, "start": 5105.4800000000005, "end": 5108.68, "text": " We can just say hooks equals books,", "tokens": [492, 393, 445, 584, 26485, 6915, 3642, 11], "temperature": 0.0, "avg_logprob": -0.10832849612905959, "compression_ratio": 1.7314049586776858, "no_speech_prob": 6.8541230575647205e-06}, {"id": 1610, "seek": 508360, "start": 5108.68, "end": 5110.360000000001, "text": " everything in our model,", "tokens": [1203, 294, 527, 2316, 11], "temperature": 0.0, "avg_logprob": -0.10832849612905959, "compression_ratio": 1.7314049586776858, "no_speech_prob": 6.8541230575647205e-06}, {"id": 1611, "seek": 508360, "start": 5110.360000000001, "end": 5113.0, "text": " with that function we had before, appendStats.", "tokens": [365, 300, 2445, 321, 632, 949, 11, 34116, 4520, 1720, 13], "temperature": 0.0, "avg_logprob": -0.10832849612905959, "compression_ratio": 1.7314049586776858, "no_speech_prob": 6.8541230575647205e-06}, {"id": 1612, "seek": 511300, "start": 5113.0, "end": 5115.08, "text": " We can print it out to see all the hooks.", "tokens": [492, 393, 4482, 309, 484, 281, 536, 439, 264, 26485, 13], "temperature": 0.0, "avg_logprob": -0.11273401555880694, "compression_ratio": 1.8566037735849057, "no_speech_prob": 2.840897286660038e-05}, {"id": 1613, "seek": 511300, "start": 5115.92, "end": 5117.96, "text": " We can grab a batch of data.", "tokens": [492, 393, 4444, 257, 15245, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.11273401555880694, "compression_ratio": 1.8566037735849057, "no_speech_prob": 2.840897286660038e-05}, {"id": 1614, "seek": 511300, "start": 5119.32, "end": 5121.08, "text": " So now we've got one batch of data,", "tokens": [407, 586, 321, 600, 658, 472, 15245, 295, 1412, 11], "temperature": 0.0, "avg_logprob": -0.11273401555880694, "compression_ratio": 1.8566037735849057, "no_speech_prob": 2.840897286660038e-05}, {"id": 1615, "seek": 511300, "start": 5121.92, "end": 5124.6, "text": " and check its mean and standard deviation is about zero, one,", "tokens": [293, 1520, 1080, 914, 293, 3832, 25163, 307, 466, 4018, 11, 472, 11], "temperature": 0.0, "avg_logprob": -0.11273401555880694, "compression_ratio": 1.8566037735849057, "no_speech_prob": 2.840897286660038e-05}, {"id": 1616, "seek": 511300, "start": 5124.6, "end": 5126.08, "text": " as you would expect.", "tokens": [382, 291, 576, 2066, 13], "temperature": 0.0, "avg_logprob": -0.11273401555880694, "compression_ratio": 1.8566037735849057, "no_speech_prob": 2.840897286660038e-05}, {"id": 1617, "seek": 511300, "start": 5126.08, "end": 5128.88, "text": " We can pass it through the first layer of our model,", "tokens": [492, 393, 1320, 309, 807, 264, 700, 4583, 295, 527, 2316, 11], "temperature": 0.0, "avg_logprob": -0.11273401555880694, "compression_ratio": 1.8566037735849057, "no_speech_prob": 2.840897286660038e-05}, {"id": 1618, "seek": 511300, "start": 5128.88, "end": 5130.92, "text": " model zero is the first layer of our model,", "tokens": [2316, 4018, 307, 264, 700, 4583, 295, 527, 2316, 11], "temperature": 0.0, "avg_logprob": -0.11273401555880694, "compression_ratio": 1.8566037735849057, "no_speech_prob": 2.840897286660038e-05}, {"id": 1619, "seek": 511300, "start": 5130.92, "end": 5132.6, "text": " which is the first convolution.", "tokens": [597, 307, 264, 700, 45216, 13], "temperature": 0.0, "avg_logprob": -0.11273401555880694, "compression_ratio": 1.8566037735849057, "no_speech_prob": 2.840897286660038e-05}, {"id": 1620, "seek": 511300, "start": 5132.6, "end": 5134.96, "text": " And our mean is not quite zero,", "tokens": [400, 527, 914, 307, 406, 1596, 4018, 11], "temperature": 0.0, "avg_logprob": -0.11273401555880694, "compression_ratio": 1.8566037735849057, "no_speech_prob": 2.840897286660038e-05}, {"id": 1621, "seek": 511300, "start": 5134.96, "end": 5138.08, "text": " and our standard deviation is quite a lot less than one,", "tokens": [293, 527, 3832, 25163, 307, 1596, 257, 688, 1570, 813, 472, 11], "temperature": 0.0, "avg_logprob": -0.11273401555880694, "compression_ratio": 1.8566037735849057, "no_speech_prob": 2.840897286660038e-05}, {"id": 1622, "seek": 511300, "start": 5138.08, "end": 5140.16, "text": " as we kind of know what's gonna happen.", "tokens": [382, 321, 733, 295, 458, 437, 311, 799, 1051, 13], "temperature": 0.0, "avg_logprob": -0.11273401555880694, "compression_ratio": 1.8566037735849057, "no_speech_prob": 2.840897286660038e-05}, {"id": 1623, "seek": 511300, "start": 5141.08, "end": 5142.68, "text": " So now we'll just go ahead and initialize it", "tokens": [407, 586, 321, 603, 445, 352, 2286, 293, 5883, 1125, 309], "temperature": 0.0, "avg_logprob": -0.11273401555880694, "compression_ratio": 1.8566037735849057, "no_speech_prob": 2.840897286660038e-05}, {"id": 1624, "seek": 514268, "start": 5142.68, "end": 5145.88, "text": " with kai-ming, and after that,", "tokens": [365, 350, 1301, 12, 2810, 11, 293, 934, 300, 11], "temperature": 0.0, "avg_logprob": -0.17209100301286814, "compression_ratio": 1.8673469387755102, "no_speech_prob": 1.5205652744043618e-05}, {"id": 1625, "seek": 514268, "start": 5145.88, "end": 5147.92, "text": " variance is quite close to one.", "tokens": [21977, 307, 1596, 1998, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.17209100301286814, "compression_ratio": 1.8673469387755102, "no_speech_prob": 1.5205652744043618e-05}, {"id": 1626, "seek": 514268, "start": 5147.92, "end": 5149.72, "text": " And our standard deviation, sorry,", "tokens": [400, 527, 3832, 25163, 11, 2597, 11], "temperature": 0.0, "avg_logprob": -0.17209100301286814, "compression_ratio": 1.8673469387755102, "no_speech_prob": 1.5205652744043618e-05}, {"id": 1627, "seek": 514268, "start": 5149.72, "end": 5154.400000000001, "text": " and our mean as expected is quite close to 0.5,", "tokens": [293, 527, 914, 382, 5176, 307, 1596, 1998, 281, 1958, 13, 20, 11], "temperature": 0.0, "avg_logprob": -0.17209100301286814, "compression_ratio": 1.8673469387755102, "no_speech_prob": 1.5205652744043618e-05}, {"id": 1628, "seek": 514268, "start": 5154.400000000001, "end": 5155.52, "text": " because of the relu.", "tokens": [570, 295, 264, 1039, 84, 13], "temperature": 0.0, "avg_logprob": -0.17209100301286814, "compression_ratio": 1.8673469387755102, "no_speech_prob": 1.5205652744043618e-05}, {"id": 1629, "seek": 514268, "start": 5157.240000000001, "end": 5161.84, "text": " So now we can go ahead and create our hooks,", "tokens": [407, 586, 321, 393, 352, 2286, 293, 1884, 527, 26485, 11], "temperature": 0.0, "avg_logprob": -0.17209100301286814, "compression_ratio": 1.8673469387755102, "no_speech_prob": 1.5205652744043618e-05}, {"id": 1630, "seek": 514268, "start": 5161.84, "end": 5163.320000000001, "text": " and do a fit,", "tokens": [293, 360, 257, 3318, 11], "temperature": 0.0, "avg_logprob": -0.17209100301286814, "compression_ratio": 1.8673469387755102, "no_speech_prob": 1.5205652744043618e-05}, {"id": 1631, "seek": 514268, "start": 5163.320000000001, "end": 5166.88, "text": " and we can plot the first 10 means and standard deviations,", "tokens": [293, 321, 393, 7542, 264, 700, 1266, 1355, 293, 3832, 31219, 763, 11], "temperature": 0.0, "avg_logprob": -0.17209100301286814, "compression_ratio": 1.8673469387755102, "no_speech_prob": 1.5205652744043618e-05}, {"id": 1632, "seek": 514268, "start": 5166.88, "end": 5170.360000000001, "text": " and then we can plot all the means and standard deviations,", "tokens": [293, 550, 321, 393, 7542, 439, 264, 1355, 293, 3832, 31219, 763, 11], "temperature": 0.0, "avg_logprob": -0.17209100301286814, "compression_ratio": 1.8673469387755102, "no_speech_prob": 1.5205652744043618e-05}, {"id": 1633, "seek": 514268, "start": 5170.360000000001, "end": 5171.72, "text": " and there it all is.", "tokens": [293, 456, 309, 439, 307, 13], "temperature": 0.0, "avg_logprob": -0.17209100301286814, "compression_ratio": 1.8673469387755102, "no_speech_prob": 1.5205652744043618e-05}, {"id": 1634, "seek": 517172, "start": 5171.72, "end": 5176.280000000001, "text": " And this time we're doing it after we've initialized", "tokens": [400, 341, 565, 321, 434, 884, 309, 934, 321, 600, 5883, 1602], "temperature": 0.0, "avg_logprob": -0.10407228727598448, "compression_ratio": 1.7522522522522523, "no_speech_prob": 8.529946171620395e-06}, {"id": 1635, "seek": 517172, "start": 5176.280000000001, "end": 5177.88, "text": " all the layers of our model.", "tokens": [439, 264, 7914, 295, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10407228727598448, "compression_ratio": 1.7522522522522523, "no_speech_prob": 8.529946171620395e-06}, {"id": 1636, "seek": 517172, "start": 5177.88, "end": 5179.4400000000005, "text": " And as you can see,", "tokens": [400, 382, 291, 393, 536, 11], "temperature": 0.0, "avg_logprob": -0.10407228727598448, "compression_ratio": 1.7522522522522523, "no_speech_prob": 8.529946171620395e-06}, {"id": 1637, "seek": 517172, "start": 5179.4400000000005, "end": 5183.4400000000005, "text": " we don't have that awful exponential crash,", "tokens": [321, 500, 380, 362, 300, 11232, 21510, 8252, 11], "temperature": 0.0, "avg_logprob": -0.10407228727598448, "compression_ratio": 1.7522522522522523, "no_speech_prob": 8.529946171620395e-06}, {"id": 1638, "seek": 517172, "start": 5183.4400000000005, "end": 5184.84, "text": " exponential crash, exponential crash.", "tokens": [21510, 8252, 11, 21510, 8252, 13], "temperature": 0.0, "avg_logprob": -0.10407228727598448, "compression_ratio": 1.7522522522522523, "no_speech_prob": 8.529946171620395e-06}, {"id": 1639, "seek": 517172, "start": 5184.84, "end": 5187.360000000001, "text": " So this is looking much better,", "tokens": [407, 341, 307, 1237, 709, 1101, 11], "temperature": 0.0, "avg_logprob": -0.10407228727598448, "compression_ratio": 1.7522522522522523, "no_speech_prob": 8.529946171620395e-06}, {"id": 1640, "seek": 517172, "start": 5187.360000000001, "end": 5189.2, "text": " and you can see early on in training,", "tokens": [293, 291, 393, 536, 2440, 322, 294, 3097, 11], "temperature": 0.0, "avg_logprob": -0.10407228727598448, "compression_ratio": 1.7522522522522523, "no_speech_prob": 8.529946171620395e-06}, {"id": 1641, "seek": 517172, "start": 5189.2, "end": 5191.68, "text": " our variances all look,", "tokens": [527, 1374, 21518, 439, 574, 11], "temperature": 0.0, "avg_logprob": -0.10407228727598448, "compression_ratio": 1.7522522522522523, "no_speech_prob": 8.529946171620395e-06}, {"id": 1642, "seek": 517172, "start": 5191.68, "end": 5194.76, "text": " our standard deviations all look much closer to one.", "tokens": [527, 3832, 31219, 763, 439, 574, 709, 4966, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.10407228727598448, "compression_ratio": 1.7522522522522523, "no_speech_prob": 8.529946171620395e-06}, {"id": 1643, "seek": 517172, "start": 5194.76, "end": 5196.6, "text": " So this is looking super hopeful.", "tokens": [407, 341, 307, 1237, 1687, 20531, 13], "temperature": 0.0, "avg_logprob": -0.10407228727598448, "compression_ratio": 1.7522522522522523, "no_speech_prob": 8.529946171620395e-06}, {"id": 1644, "seek": 517172, "start": 5198.4400000000005, "end": 5200.52, "text": " I've used a width block.", "tokens": [286, 600, 1143, 257, 11402, 3461, 13], "temperature": 0.0, "avg_logprob": -0.10407228727598448, "compression_ratio": 1.7522522522522523, "no_speech_prob": 8.529946171620395e-06}, {"id": 1645, "seek": 520052, "start": 5200.52, "end": 5204.68, "text": " A width block is something that will create this object,", "tokens": [316, 11402, 3461, 307, 746, 300, 486, 1884, 341, 2657, 11], "temperature": 0.0, "avg_logprob": -0.12269975457872663, "compression_ratio": 1.7074235807860263, "no_speech_prob": 1.4284107237472199e-05}, {"id": 1646, "seek": 520052, "start": 5205.72, "end": 5207.64, "text": " give it this name,", "tokens": [976, 309, 341, 1315, 11], "temperature": 0.0, "avg_logprob": -0.12269975457872663, "compression_ratio": 1.7074235807860263, "no_speech_prob": 1.4284107237472199e-05}, {"id": 1647, "seek": 520052, "start": 5207.64, "end": 5211.040000000001, "text": " and when it's finished, it will do something.", "tokens": [293, 562, 309, 311, 4335, 11, 309, 486, 360, 746, 13], "temperature": 0.0, "avg_logprob": -0.12269975457872663, "compression_ratio": 1.7074235807860263, "no_speech_prob": 1.4284107237472199e-05}, {"id": 1648, "seek": 520052, "start": 5211.040000000001, "end": 5216.040000000001, "text": " The something it does is to call your dunderExit method here,", "tokens": [440, 746, 309, 775, 307, 281, 818, 428, 274, 6617, 11149, 270, 3170, 510, 11], "temperature": 0.0, "avg_logprob": -0.12269975457872663, "compression_ratio": 1.7074235807860263, "no_speech_prob": 1.4284107237472199e-05}, {"id": 1649, "seek": 520052, "start": 5217.120000000001, "end": 5219.040000000001, "text": " which will dot remove.", "tokens": [597, 486, 5893, 4159, 13], "temperature": 0.0, "avg_logprob": -0.12269975457872663, "compression_ratio": 1.7074235807860263, "no_speech_prob": 1.4284107237472199e-05}, {"id": 1650, "seek": 520052, "start": 5219.040000000001, "end": 5222.84, "text": " So here's a nice way to ensure that things are cleaned up.", "tokens": [407, 510, 311, 257, 1481, 636, 281, 5586, 300, 721, 366, 16146, 493, 13], "temperature": 0.0, "avg_logprob": -0.12269975457872663, "compression_ratio": 1.7074235807860263, "no_speech_prob": 1.4284107237472199e-05}, {"id": 1651, "seek": 520052, "start": 5222.84, "end": 5224.96, "text": " For example, your hooks are removed.", "tokens": [1171, 1365, 11, 428, 26485, 366, 7261, 13], "temperature": 0.0, "avg_logprob": -0.12269975457872663, "compression_ratio": 1.7074235807860263, "no_speech_prob": 1.4284107237472199e-05}, {"id": 1652, "seek": 520052, "start": 5226.88, "end": 5228.4800000000005, "text": " So that's why we have a dunderEnter,", "tokens": [407, 300, 311, 983, 321, 362, 257, 274, 6617, 16257, 391, 11], "temperature": 0.0, "avg_logprob": -0.12269975457872663, "compression_ratio": 1.7074235807860263, "no_speech_prob": 1.4284107237472199e-05}, {"id": 1653, "seek": 520052, "start": 5228.4800000000005, "end": 5230.320000000001, "text": " that's what happens when you start the width block,", "tokens": [300, 311, 437, 2314, 562, 291, 722, 264, 11402, 3461, 11], "temperature": 0.0, "avg_logprob": -0.12269975457872663, "compression_ratio": 1.7074235807860263, "no_speech_prob": 1.4284107237472199e-05}, {"id": 1654, "seek": 523032, "start": 5230.32, "end": 5232.4, "text": " dunderExit when you finish the width block.", "tokens": [274, 6617, 11149, 270, 562, 291, 2413, 264, 11402, 3461, 13], "temperature": 0.0, "avg_logprob": -0.14229305971016004, "compression_ratio": 1.6291666666666667, "no_speech_prob": 2.5215222194674425e-06}, {"id": 1655, "seek": 523032, "start": 5235.44, "end": 5237.92, "text": " So this is looking very hopeful,", "tokens": [407, 341, 307, 1237, 588, 20531, 11], "temperature": 0.0, "avg_logprob": -0.14229305971016004, "compression_ratio": 1.6291666666666667, "no_speech_prob": 2.5215222194674425e-06}, {"id": 1656, "seek": 523032, "start": 5237.92, "end": 5240.5199999999995, "text": " but it's not quite what we wanted to know.", "tokens": [457, 309, 311, 406, 1596, 437, 321, 1415, 281, 458, 13], "temperature": 0.0, "avg_logprob": -0.14229305971016004, "compression_ratio": 1.6291666666666667, "no_speech_prob": 2.5215222194674425e-06}, {"id": 1657, "seek": 523032, "start": 5240.5199999999995, "end": 5241.799999999999, "text": " Really the concern was,", "tokens": [4083, 264, 3136, 390, 11], "temperature": 0.0, "avg_logprob": -0.14229305971016004, "compression_ratio": 1.6291666666666667, "no_speech_prob": 2.5215222194674425e-06}, {"id": 1658, "seek": 523032, "start": 5241.799999999999, "end": 5244.799999999999, "text": " is does this actually do something bad?", "tokens": [307, 775, 341, 767, 360, 746, 1578, 30], "temperature": 0.0, "avg_logprob": -0.14229305971016004, "compression_ratio": 1.6291666666666667, "no_speech_prob": 2.5215222194674425e-06}, {"id": 1659, "seek": 523032, "start": 5245.799999999999, "end": 5249.44, "text": " Is it actually, or does it just train fine afterwards?", "tokens": [1119, 309, 767, 11, 420, 775, 309, 445, 3847, 2489, 10543, 30], "temperature": 0.0, "avg_logprob": -0.14229305971016004, "compression_ratio": 1.6291666666666667, "no_speech_prob": 2.5215222194674425e-06}, {"id": 1660, "seek": 523032, "start": 5249.44, "end": 5252.639999999999, "text": " So something bad really is more about", "tokens": [407, 746, 1578, 534, 307, 544, 466], "temperature": 0.0, "avg_logprob": -0.14229305971016004, "compression_ratio": 1.6291666666666667, "no_speech_prob": 2.5215222194674425e-06}, {"id": 1661, "seek": 523032, "start": 5252.639999999999, "end": 5255.96, "text": " how many of the activations are really, really small?", "tokens": [577, 867, 295, 264, 2430, 763, 366, 534, 11, 534, 1359, 30], "temperature": 0.0, "avg_logprob": -0.14229305971016004, "compression_ratio": 1.6291666666666667, "no_speech_prob": 2.5215222194674425e-06}, {"id": 1662, "seek": 523032, "start": 5255.96, "end": 5260.16, "text": " How well is it actually getting everything activated nicely?", "tokens": [1012, 731, 307, 309, 767, 1242, 1203, 18157, 9594, 30], "temperature": 0.0, "avg_logprob": -0.14229305971016004, "compression_ratio": 1.6291666666666667, "no_speech_prob": 2.5215222194674425e-06}, {"id": 1663, "seek": 526016, "start": 5260.16, "end": 5263.32, "text": " So what we could do is we could adjust our appendStats.", "tokens": [407, 437, 321, 727, 360, 307, 321, 727, 4369, 527, 34116, 4520, 1720, 13], "temperature": 0.0, "avg_logprob": -0.1450669270641399, "compression_ratio": 1.654708520179372, "no_speech_prob": 1.4737984201929066e-05}, {"id": 1664, "seek": 526016, "start": 5263.32, "end": 5266.92, "text": " So not only does it have a mean and a standard deviation,", "tokens": [407, 406, 787, 775, 309, 362, 257, 914, 293, 257, 3832, 25163, 11], "temperature": 0.0, "avg_logprob": -0.1450669270641399, "compression_ratio": 1.654708520179372, "no_speech_prob": 1.4737984201929066e-05}, {"id": 1665, "seek": 526016, "start": 5266.92, "end": 5268.36, "text": " but it's also got a histogram.", "tokens": [457, 309, 311, 611, 658, 257, 49816, 13], "temperature": 0.0, "avg_logprob": -0.1450669270641399, "compression_ratio": 1.654708520179372, "no_speech_prob": 1.4737984201929066e-05}, {"id": 1666, "seek": 526016, "start": 5269.48, "end": 5272.8, "text": " So we could create a histogram of the activations,", "tokens": [407, 321, 727, 1884, 257, 49816, 295, 264, 2430, 763, 11], "temperature": 0.0, "avg_logprob": -0.1450669270641399, "compression_ratio": 1.654708520179372, "no_speech_prob": 1.4737984201929066e-05}, {"id": 1667, "seek": 526016, "start": 5272.8, "end": 5277.8, "text": " pop them into 40 bins between zero and 10.", "tokens": [1665, 552, 666, 3356, 41275, 1296, 4018, 293, 1266, 13], "temperature": 0.0, "avg_logprob": -0.1450669270641399, "compression_ratio": 1.654708520179372, "no_speech_prob": 1.4737984201929066e-05}, {"id": 1668, "seek": 526016, "start": 5277.8, "end": 5280.72, "text": " We don't need to go underneath zero because we have a relu,", "tokens": [492, 500, 380, 643, 281, 352, 7223, 4018, 570, 321, 362, 257, 1039, 84, 11], "temperature": 0.0, "avg_logprob": -0.1450669270641399, "compression_ratio": 1.654708520179372, "no_speech_prob": 1.4737984201929066e-05}, {"id": 1669, "seek": 526016, "start": 5280.72, "end": 5283.48, "text": " so we know that there's none underneath zero.", "tokens": [370, 321, 458, 300, 456, 311, 6022, 7223, 4018, 13], "temperature": 0.0, "avg_logprob": -0.1450669270641399, "compression_ratio": 1.654708520179372, "no_speech_prob": 1.4737984201929066e-05}, {"id": 1670, "seek": 526016, "start": 5283.48, "end": 5286.04, "text": " So let's again run this.", "tokens": [407, 718, 311, 797, 1190, 341, 13], "temperature": 0.0, "avg_logprob": -0.1450669270641399, "compression_ratio": 1.654708520179372, "no_speech_prob": 1.4737984201929066e-05}, {"id": 1671, "seek": 528604, "start": 5286.04, "end": 5290.72, "text": " We will use our chiming initialization.", "tokens": [492, 486, 764, 527, 18375, 278, 5883, 2144, 13], "temperature": 0.0, "avg_logprob": -0.2660180727640788, "compression_ratio": 1.4277456647398843, "no_speech_prob": 3.1874933483777568e-06}, {"id": 1672, "seek": 528604, "start": 5293.56, "end": 5298.56, "text": " And what we find is that even with that,", "tokens": [400, 437, 321, 915, 307, 300, 754, 365, 300, 11], "temperature": 0.0, "avg_logprob": -0.2660180727640788, "compression_ratio": 1.4277456647398843, "no_speech_prob": 3.1874933483777568e-06}, {"id": 1673, "seek": 528604, "start": 5301.92, "end": 5305.48, "text": " if we make our learning rate really high, 0.9,", "tokens": [498, 321, 652, 527, 2539, 3314, 534, 1090, 11, 1958, 13, 24, 11], "temperature": 0.0, "avg_logprob": -0.2660180727640788, "compression_ratio": 1.4277456647398843, "no_speech_prob": 3.1874933483777568e-06}, {"id": 1674, "seek": 528604, "start": 5306.48, "end": 5308.44, "text": " we can still get this same behavior.", "tokens": [321, 393, 920, 483, 341, 912, 5223, 13], "temperature": 0.0, "avg_logprob": -0.2660180727640788, "compression_ratio": 1.4277456647398843, "no_speech_prob": 3.1874933483777568e-06}, {"id": 1675, "seek": 528604, "start": 5308.44, "end": 5311.16, "text": " And so here's plotting the entire histogram.", "tokens": [400, 370, 510, 311, 41178, 264, 2302, 49816, 13], "temperature": 0.0, "avg_logprob": -0.2660180727640788, "compression_ratio": 1.4277456647398843, "no_speech_prob": 3.1874933483777568e-06}, {"id": 1676, "seek": 528604, "start": 5311.16, "end": 5312.88, "text": " And I should say thank you to Stefano", "tokens": [400, 286, 820, 584, 1309, 291, 281, 43421, 3730], "temperature": 0.0, "avg_logprob": -0.2660180727640788, "compression_ratio": 1.4277456647398843, "no_speech_prob": 3.1874933483777568e-06}, {"id": 1677, "seek": 531288, "start": 5312.88, "end": 5316.52, "text": " for the original code here from our San Francisco study group", "tokens": [337, 264, 3380, 3089, 510, 490, 527, 5271, 12279, 2979, 1594], "temperature": 0.0, "avg_logprob": -0.12781847739706234, "compression_ratio": 1.7095238095238094, "no_speech_prob": 2.178157228627242e-05}, {"id": 1678, "seek": 531288, "start": 5317.6, "end": 5319.64, "text": " to plot these nicely.", "tokens": [281, 7542, 613, 9594, 13], "temperature": 0.0, "avg_logprob": -0.12781847739706234, "compression_ratio": 1.7095238095238094, "no_speech_prob": 2.178157228627242e-05}, {"id": 1679, "seek": 531288, "start": 5319.64, "end": 5322.400000000001, "text": " So you can see this kind of grow, collapse,", "tokens": [407, 291, 393, 536, 341, 733, 295, 1852, 11, 15584, 11], "temperature": 0.0, "avg_logprob": -0.12781847739706234, "compression_ratio": 1.7095238095238094, "no_speech_prob": 2.178157228627242e-05}, {"id": 1680, "seek": 531288, "start": 5322.400000000001, "end": 5324.24, "text": " grow, collapse, grow, collapse thing.", "tokens": [1852, 11, 15584, 11, 1852, 11, 15584, 551, 13], "temperature": 0.0, "avg_logprob": -0.12781847739706234, "compression_ratio": 1.7095238095238094, "no_speech_prob": 2.178157228627242e-05}, {"id": 1681, "seek": 531288, "start": 5325.4400000000005, "end": 5327.84, "text": " The biggest concern for me though", "tokens": [440, 3880, 3136, 337, 385, 1673], "temperature": 0.0, "avg_logprob": -0.12781847739706234, "compression_ratio": 1.7095238095238094, "no_speech_prob": 2.178157228627242e-05}, {"id": 1682, "seek": 531288, "start": 5327.84, "end": 5330.64, "text": " is this yellow line at the bottom.", "tokens": [307, 341, 5566, 1622, 412, 264, 2767, 13], "temperature": 0.0, "avg_logprob": -0.12781847739706234, "compression_ratio": 1.7095238095238094, "no_speech_prob": 2.178157228627242e-05}, {"id": 1683, "seek": 531288, "start": 5330.64, "end": 5334.68, "text": " The yellow line, yellow is where most of the histogram is.", "tokens": [440, 5566, 1622, 11, 5566, 307, 689, 881, 295, 264, 49816, 307, 13], "temperature": 0.0, "avg_logprob": -0.12781847739706234, "compression_ratio": 1.7095238095238094, "no_speech_prob": 2.178157228627242e-05}, {"id": 1684, "seek": 531288, "start": 5334.68, "end": 5336.08, "text": " I actually, what I really care about", "tokens": [286, 767, 11, 437, 286, 534, 1127, 466], "temperature": 0.0, "avg_logprob": -0.12781847739706234, "compression_ratio": 1.7095238095238094, "no_speech_prob": 2.178157228627242e-05}, {"id": 1685, "seek": 531288, "start": 5336.08, "end": 5339.28, "text": " is how much yellow is there.", "tokens": [307, 577, 709, 5566, 307, 456, 13], "temperature": 0.0, "avg_logprob": -0.12781847739706234, "compression_ratio": 1.7095238095238094, "no_speech_prob": 2.178157228627242e-05}, {"id": 1686, "seek": 533928, "start": 5339.28, "end": 5344.28, "text": " So let's say the first two histogram bins", "tokens": [407, 718, 311, 584, 264, 700, 732, 49816, 41275], "temperature": 0.0, "avg_logprob": -0.057652335166931155, "compression_ratio": 1.7875647668393781, "no_speech_prob": 1.618667556613218e-05}, {"id": 1687, "seek": 533928, "start": 5344.32, "end": 5347.04, "text": " are zero or nearly zero.", "tokens": [366, 4018, 420, 6217, 4018, 13], "temperature": 0.0, "avg_logprob": -0.057652335166931155, "compression_ratio": 1.7875647668393781, "no_speech_prob": 1.618667556613218e-05}, {"id": 1688, "seek": 533928, "start": 5347.04, "end": 5350.32, "text": " So let's get the sum of how much is in those two bins", "tokens": [407, 718, 311, 483, 264, 2408, 295, 577, 709, 307, 294, 729, 732, 41275], "temperature": 0.0, "avg_logprob": -0.057652335166931155, "compression_ratio": 1.7875647668393781, "no_speech_prob": 1.618667556613218e-05}, {"id": 1689, "seek": 533928, "start": 5350.32, "end": 5353.4, "text": " and divide by the sum of all of the bins.", "tokens": [293, 9845, 538, 264, 2408, 295, 439, 295, 264, 41275, 13], "temperature": 0.0, "avg_logprob": -0.057652335166931155, "compression_ratio": 1.7875647668393781, "no_speech_prob": 1.618667556613218e-05}, {"id": 1690, "seek": 533928, "start": 5353.4, "end": 5356.8, "text": " And so that's gonna tell us what percentage", "tokens": [400, 370, 300, 311, 799, 980, 505, 437, 9668], "temperature": 0.0, "avg_logprob": -0.057652335166931155, "compression_ratio": 1.7875647668393781, "no_speech_prob": 1.618667556613218e-05}, {"id": 1691, "seek": 533928, "start": 5356.8, "end": 5360.16, "text": " of the activations are zero or nearly zero.", "tokens": [295, 264, 2430, 763, 366, 4018, 420, 6217, 4018, 13], "temperature": 0.0, "avg_logprob": -0.057652335166931155, "compression_ratio": 1.7875647668393781, "no_speech_prob": 1.618667556613218e-05}, {"id": 1692, "seek": 533928, "start": 5361.5599999999995, "end": 5365.16, "text": " And let's plot that for each of the first four layers.", "tokens": [400, 718, 311, 7542, 300, 337, 1184, 295, 264, 700, 1451, 7914, 13], "temperature": 0.0, "avg_logprob": -0.057652335166931155, "compression_ratio": 1.7875647668393781, "no_speech_prob": 1.618667556613218e-05}, {"id": 1693, "seek": 533928, "start": 5365.16, "end": 5368.04, "text": " And you can see that in the last layer,", "tokens": [400, 291, 393, 536, 300, 294, 264, 1036, 4583, 11], "temperature": 0.0, "avg_logprob": -0.057652335166931155, "compression_ratio": 1.7875647668393781, "no_speech_prob": 1.618667556613218e-05}, {"id": 1694, "seek": 536804, "start": 5368.04, "end": 5369.6, "text": " it's just as we suspected.", "tokens": [309, 311, 445, 382, 321, 26439, 13], "temperature": 0.0, "avg_logprob": -0.06901530909344433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.7230848243343644e-05}, {"id": 1695, "seek": 536804, "start": 5370.8, "end": 5375.4, "text": " Over 90% of the activations are actually zero.", "tokens": [4886, 4289, 4, 295, 264, 2430, 763, 366, 767, 4018, 13], "temperature": 0.0, "avg_logprob": -0.06901530909344433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.7230848243343644e-05}, {"id": 1696, "seek": 536804, "start": 5375.4, "end": 5378.28, "text": " So if you were training your model like this, right,", "tokens": [407, 498, 291, 645, 3097, 428, 2316, 411, 341, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.06901530909344433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.7230848243343644e-05}, {"id": 1697, "seek": 536804, "start": 5378.28, "end": 5381.12, "text": " it could eventually look like it's training nicely", "tokens": [309, 727, 4728, 574, 411, 309, 311, 3097, 9594], "temperature": 0.0, "avg_logprob": -0.06901530909344433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.7230848243343644e-05}, {"id": 1698, "seek": 536804, "start": 5381.12, "end": 5383.88, "text": " without you realizing that 90% of your activations", "tokens": [1553, 291, 16734, 300, 4289, 4, 295, 428, 2430, 763], "temperature": 0.0, "avg_logprob": -0.06901530909344433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.7230848243343644e-05}, {"id": 1699, "seek": 536804, "start": 5383.88, "end": 5385.56, "text": " were totally wasted.", "tokens": [645, 3879, 19496, 13], "temperature": 0.0, "avg_logprob": -0.06901530909344433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.7230848243343644e-05}, {"id": 1700, "seek": 536804, "start": 5385.56, "end": 5387.64, "text": " And so you're never gonna get great results", "tokens": [400, 370, 291, 434, 1128, 799, 483, 869, 3542], "temperature": 0.0, "avg_logprob": -0.06901530909344433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.7230848243343644e-05}, {"id": 1701, "seek": 536804, "start": 5387.64, "end": 5390.48, "text": " by wasting 90% of your activations.", "tokens": [538, 20457, 4289, 4, 295, 428, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.06901530909344433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.7230848243343644e-05}, {"id": 1702, "seek": 536804, "start": 5390.48, "end": 5391.72, "text": " So let's try and fix it.", "tokens": [407, 718, 311, 853, 293, 3191, 309, 13], "temperature": 0.0, "avg_logprob": -0.06901530909344433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.7230848243343644e-05}, {"id": 1703, "seek": 536804, "start": 5392.6, "end": 5395.5199999999995, "text": " Let's try and be able to train at a nice high learning rate", "tokens": [961, 311, 853, 293, 312, 1075, 281, 3847, 412, 257, 1481, 1090, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.06901530909344433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.7230848243343644e-05}, {"id": 1704, "seek": 536804, "start": 5395.5199999999995, "end": 5397.16, "text": " and not have this happen.", "tokens": [293, 406, 362, 341, 1051, 13], "temperature": 0.0, "avg_logprob": -0.06901530909344433, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.7230848243343644e-05}, {"id": 1705, "seek": 539716, "start": 5397.16, "end": 5399.92, "text": " And so the trick is, is we're gonna try a few things,", "tokens": [400, 370, 264, 4282, 307, 11, 307, 321, 434, 799, 853, 257, 1326, 721, 11], "temperature": 0.0, "avg_logprob": -0.08641582139780823, "compression_ratio": 1.7481203007518797, "no_speech_prob": 1.184276698040776e-05}, {"id": 1706, "seek": 539716, "start": 5399.92, "end": 5404.599999999999, "text": " but the main one is we're gonna use our better ReLU.", "tokens": [457, 264, 2135, 472, 307, 321, 434, 799, 764, 527, 1101, 1300, 43, 52, 13], "temperature": 0.0, "avg_logprob": -0.08641582139780823, "compression_ratio": 1.7481203007518797, "no_speech_prob": 1.184276698040776e-05}, {"id": 1707, "seek": 539716, "start": 5404.599999999999, "end": 5407.88, "text": " And so we've created a generalized ReLU class", "tokens": [400, 370, 321, 600, 2942, 257, 44498, 1300, 43, 52, 1508], "temperature": 0.0, "avg_logprob": -0.08641582139780823, "compression_ratio": 1.7481203007518797, "no_speech_prob": 1.184276698040776e-05}, {"id": 1708, "seek": 539716, "start": 5407.88, "end": 5410.68, "text": " where now we can pass in things like", "tokens": [689, 586, 321, 393, 1320, 294, 721, 411], "temperature": 0.0, "avg_logprob": -0.08641582139780823, "compression_ratio": 1.7481203007518797, "no_speech_prob": 1.184276698040776e-05}, {"id": 1709, "seek": 539716, "start": 5410.68, "end": 5412.599999999999, "text": " an amount to subtract from the ReLU.", "tokens": [364, 2372, 281, 16390, 490, 264, 1300, 43, 52, 13], "temperature": 0.0, "avg_logprob": -0.08641582139780823, "compression_ratio": 1.7481203007518797, "no_speech_prob": 1.184276698040776e-05}, {"id": 1710, "seek": 539716, "start": 5412.599999999999, "end": 5414.4, "text": " Because remember we thought subtracting half", "tokens": [1436, 1604, 321, 1194, 16390, 278, 1922], "temperature": 0.0, "avg_logprob": -0.08641582139780823, "compression_ratio": 1.7481203007518797, "no_speech_prob": 1.184276698040776e-05}, {"id": 1711, "seek": 539716, "start": 5414.4, "end": 5416.12, "text": " from the ReLU might be a good idea.", "tokens": [490, 264, 1300, 43, 52, 1062, 312, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.08641582139780823, "compression_ratio": 1.7481203007518797, "no_speech_prob": 1.184276698040776e-05}, {"id": 1712, "seek": 539716, "start": 5416.12, "end": 5418.44, "text": " We can also use leaky ReLU.", "tokens": [492, 393, 611, 764, 476, 15681, 1300, 43, 52, 13], "temperature": 0.0, "avg_logprob": -0.08641582139780823, "compression_ratio": 1.7481203007518797, "no_speech_prob": 1.184276698040776e-05}, {"id": 1713, "seek": 539716, "start": 5418.44, "end": 5420.68, "text": " And maybe things that are too big are also a problem.", "tokens": [400, 1310, 721, 300, 366, 886, 955, 366, 611, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.08641582139780823, "compression_ratio": 1.7481203007518797, "no_speech_prob": 1.184276698040776e-05}, {"id": 1714, "seek": 539716, "start": 5420.68, "end": 5423.76, "text": " So let's also optionally have a maximum value.", "tokens": [407, 718, 311, 611, 3614, 379, 362, 257, 6674, 2158, 13], "temperature": 0.0, "avg_logprob": -0.08641582139780823, "compression_ratio": 1.7481203007518797, "no_speech_prob": 1.184276698040776e-05}, {"id": 1715, "seek": 539716, "start": 5423.76, "end": 5426.0, "text": " So in this generalized ReLU,", "tokens": [407, 294, 341, 44498, 1300, 43, 52, 11], "temperature": 0.0, "avg_logprob": -0.08641582139780823, "compression_ratio": 1.7481203007518797, "no_speech_prob": 1.184276698040776e-05}, {"id": 1716, "seek": 542600, "start": 5426.0, "end": 5429.24, "text": " if you passed a leakiness, then we'll use leaky ReLU.", "tokens": [498, 291, 4678, 257, 17143, 1324, 11, 550, 321, 603, 764, 476, 15681, 1300, 43, 52, 13], "temperature": 0.0, "avg_logprob": -0.11071299039400541, "compression_ratio": 1.701195219123506, "no_speech_prob": 2.3186707039712928e-05}, {"id": 1717, "seek": 542600, "start": 5430.36, "end": 5432.44, "text": " Otherwise we'll use normal ReLU.", "tokens": [10328, 321, 603, 764, 2710, 1300, 43, 52, 13], "temperature": 0.0, "avg_logprob": -0.11071299039400541, "compression_ratio": 1.701195219123506, "no_speech_prob": 2.3186707039712928e-05}, {"id": 1718, "seek": 542600, "start": 5432.44, "end": 5436.64, "text": " You could very easily write these leaky ReLU by hand,", "tokens": [509, 727, 588, 3612, 2464, 613, 476, 15681, 1300, 43, 52, 538, 1011, 11], "temperature": 0.0, "avg_logprob": -0.11071299039400541, "compression_ratio": 1.701195219123506, "no_speech_prob": 2.3186707039712928e-05}, {"id": 1719, "seek": 542600, "start": 5436.64, "end": 5439.48, "text": " but I'm just trying to make it run a little faster", "tokens": [457, 286, 478, 445, 1382, 281, 652, 309, 1190, 257, 707, 4663], "temperature": 0.0, "avg_logprob": -0.11071299039400541, "compression_ratio": 1.701195219123506, "no_speech_prob": 2.3186707039712928e-05}, {"id": 1720, "seek": 542600, "start": 5439.48, "end": 5441.4, "text": " by taking advantage of PyTorch.", "tokens": [538, 1940, 5002, 295, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.11071299039400541, "compression_ratio": 1.701195219123506, "no_speech_prob": 2.3186707039712928e-05}, {"id": 1721, "seek": 542600, "start": 5441.4, "end": 5443.76, "text": " If you said I wanna subtract something from it,", "tokens": [759, 291, 848, 286, 1948, 16390, 746, 490, 309, 11], "temperature": 0.0, "avg_logprob": -0.11071299039400541, "compression_ratio": 1.701195219123506, "no_speech_prob": 2.3186707039712928e-05}, {"id": 1722, "seek": 542600, "start": 5443.76, "end": 5446.16, "text": " then go ahead and subtract that from it.", "tokens": [550, 352, 2286, 293, 16390, 300, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.11071299039400541, "compression_ratio": 1.701195219123506, "no_speech_prob": 2.3186707039712928e-05}, {"id": 1723, "seek": 542600, "start": 5446.16, "end": 5448.36, "text": " If I said there's some maximum value,", "tokens": [759, 286, 848, 456, 311, 512, 6674, 2158, 11], "temperature": 0.0, "avg_logprob": -0.11071299039400541, "compression_ratio": 1.701195219123506, "no_speech_prob": 2.3186707039712928e-05}, {"id": 1724, "seek": 542600, "start": 5448.36, "end": 5451.28, "text": " go ahead and clamp it at that maximum value.", "tokens": [352, 2286, 293, 17690, 309, 412, 300, 6674, 2158, 13], "temperature": 0.0, "avg_logprob": -0.11071299039400541, "compression_ratio": 1.701195219123506, "no_speech_prob": 2.3186707039712928e-05}, {"id": 1725, "seek": 542600, "start": 5451.28, "end": 5452.8, "text": " So here's our generalized ReLU.", "tokens": [407, 510, 311, 527, 44498, 1300, 43, 52, 13], "temperature": 0.0, "avg_logprob": -0.11071299039400541, "compression_ratio": 1.701195219123506, "no_speech_prob": 2.3186707039712928e-05}, {"id": 1726, "seek": 545280, "start": 5452.8, "end": 5456.52, "text": " And so now let's have our conv layer and getCNN layers", "tokens": [400, 370, 586, 718, 311, 362, 527, 3754, 4583, 293, 483, 34, 45, 45, 7914], "temperature": 0.0, "avg_logprob": -0.2754577155252105, "compression_ratio": 1.576036866359447, "no_speech_prob": 6.143999144114787e-06}, {"id": 1727, "seek": 545280, "start": 5456.52, "end": 5460.8, "text": " both take a star star quags and just pass them on through", "tokens": [1293, 747, 257, 3543, 3543, 421, 12109, 293, 445, 1320, 552, 322, 807], "temperature": 0.0, "avg_logprob": -0.2754577155252105, "compression_ratio": 1.576036866359447, "no_speech_prob": 6.143999144114787e-06}, {"id": 1728, "seek": 545280, "start": 5462.2, "end": 5464.4400000000005, "text": " so that eventually they end up passed", "tokens": [370, 300, 4728, 436, 917, 493, 4678], "temperature": 0.0, "avg_logprob": -0.2754577155252105, "compression_ratio": 1.576036866359447, "no_speech_prob": 6.143999144114787e-06}, {"id": 1729, "seek": 545280, "start": 5464.4400000000005, "end": 5466.2, "text": " to our generalized ReLU.", "tokens": [281, 527, 44498, 1300, 43, 52, 13], "temperature": 0.0, "avg_logprob": -0.2754577155252105, "compression_ratio": 1.576036866359447, "no_speech_prob": 6.143999144114787e-06}, {"id": 1730, "seek": 545280, "start": 5466.2, "end": 5469.72, "text": " And so that way we're gonna be able to create a CNN", "tokens": [400, 370, 300, 636, 321, 434, 799, 312, 1075, 281, 1884, 257, 24859], "temperature": 0.0, "avg_logprob": -0.2754577155252105, "compression_ratio": 1.576036866359447, "no_speech_prob": 6.143999144114787e-06}, {"id": 1731, "seek": 545280, "start": 5469.72, "end": 5472.52, "text": " and say what ReLU characteristics do we want.", "tokens": [293, 584, 437, 1300, 43, 52, 10891, 360, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.2754577155252105, "compression_ratio": 1.576036866359447, "no_speech_prob": 6.143999144114787e-06}, {"id": 1732, "seek": 545280, "start": 5472.52, "end": 5473.360000000001, "text": " Nice and easily.", "tokens": [5490, 293, 3612, 13], "temperature": 0.0, "avg_logprob": -0.2754577155252105, "compression_ratio": 1.576036866359447, "no_speech_prob": 6.143999144114787e-06}, {"id": 1733, "seek": 545280, "start": 5475.08, "end": 5477.84, "text": " And even getCNN model will pass down quags as well.", "tokens": [400, 754, 483, 34, 45, 45, 2316, 486, 1320, 760, 421, 12109, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.2754577155252105, "compression_ratio": 1.576036866359447, "no_speech_prob": 6.143999144114787e-06}, {"id": 1734, "seek": 547784, "start": 5477.84, "end": 5480.04, "text": " So now that our ReLU can go negative,", "tokens": [407, 586, 300, 527, 1300, 43, 52, 393, 352, 3671, 11], "temperature": 0.0, "avg_logprob": -0.3928410277074697, "compression_ratio": 1.645, "no_speech_prob": 4.936804089084035e-06}, {"id": 1735, "seek": 547784, "start": 5480.04, "end": 5482.6, "text": " because it's leaky and because it's subtracting stuff,", "tokens": [570, 309, 311, 476, 15681, 293, 570, 309, 311, 16390, 278, 1507, 11], "temperature": 0.0, "avg_logprob": -0.3928410277074697, "compression_ratio": 1.645, "no_speech_prob": 4.936804089084035e-06}, {"id": 1736, "seek": 547784, "start": 5482.6, "end": 5484.24, "text": " we'll need to change our histogram.", "tokens": [321, 603, 643, 281, 1319, 527, 49816, 13], "temperature": 0.0, "avg_logprob": -0.3928410277074697, "compression_ratio": 1.645, "no_speech_prob": 4.936804089084035e-06}, {"id": 1737, "seek": 547784, "start": 5484.24, "end": 5486.2, "text": " So it goes from minus seven to seven", "tokens": [407, 309, 1709, 490, 3175, 3407, 281, 3407], "temperature": 0.0, "avg_logprob": -0.3928410277074697, "compression_ratio": 1.645, "no_speech_prob": 4.936804089084035e-06}, {"id": 1738, "seek": 547784, "start": 5486.2, "end": 5488.12, "text": " rather than from zero to 10.", "tokens": [2831, 813, 490, 4018, 281, 1266, 13], "temperature": 0.0, "avg_logprob": -0.3928410277074697, "compression_ratio": 1.645, "no_speech_prob": 4.936804089084035e-06}, {"id": 1739, "seek": 547784, "start": 5489.28, "end": 5491.360000000001, "text": " So we'll also need to change our", "tokens": [407, 321, 603, 611, 643, 281, 1319, 527], "temperature": 0.0, "avg_logprob": -0.3928410277074697, "compression_ratio": 1.645, "no_speech_prob": 4.936804089084035e-06}, {"id": 1740, "seek": 547784, "start": 5493.4800000000005, "end": 5494.84, "text": " definition of", "tokens": [7123, 295], "temperature": 0.0, "avg_logprob": -0.3928410277074697, "compression_ratio": 1.645, "no_speech_prob": 4.936804089084035e-06}, {"id": 1741, "seek": 547784, "start": 5496.68, "end": 5497.76, "text": " getMin", "tokens": [483, 30031], "temperature": 0.0, "avg_logprob": -0.3928410277074697, "compression_ratio": 1.645, "no_speech_prob": 4.936804089084035e-06}, {"id": 1742, "seek": 547784, "start": 5497.76, "end": 5499.8, "text": " so that the middle few", "tokens": [370, 300, 264, 2808, 1326], "temperature": 0.0, "avg_logprob": -0.3928410277074697, "compression_ratio": 1.645, "no_speech_prob": 4.936804089084035e-06}, {"id": 1743, "seek": 547784, "start": 5500.92, "end": 5504.12, "text": " bits of the histogram are zero rather than the first two.", "tokens": [9239, 295, 264, 49816, 366, 4018, 2831, 813, 264, 700, 732, 13], "temperature": 0.0, "avg_logprob": -0.3928410277074697, "compression_ratio": 1.645, "no_speech_prob": 4.936804089084035e-06}, {"id": 1744, "seek": 550412, "start": 5504.12, "end": 5506.8, "text": " And now we can just go ahead and train this model", "tokens": [400, 586, 321, 393, 445, 352, 2286, 293, 3847, 341, 2316], "temperature": 0.0, "avg_logprob": -0.4253396426930147, "compression_ratio": 1.6633165829145728, "no_speech_prob": 4.860244189330842e-06}, {"id": 1745, "seek": 550412, "start": 5506.8, "end": 5509.4, "text": " just like before and plot just like before.", "tokens": [445, 411, 949, 293, 7542, 445, 411, 949, 13], "temperature": 0.0, "avg_logprob": -0.4253396426930147, "compression_ratio": 1.6633165829145728, "no_speech_prob": 4.860244189330842e-06}, {"id": 1746, "seek": 550412, "start": 5510.44, "end": 5511.28, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.4253396426930147, "compression_ratio": 1.6633165829145728, "no_speech_prob": 4.860244189330842e-06}, {"id": 1747, "seek": 550412, "start": 5512.88, "end": 5514.4, "text": " this is looking pretty hopeful.", "tokens": [341, 307, 1237, 1238, 20531, 13], "temperature": 0.0, "avg_logprob": -0.4253396426930147, "compression_ratio": 1.6633165829145728, "no_speech_prob": 4.860244189330842e-06}, {"id": 1748, "seek": 550412, "start": 5514.4, "end": 5515.88, "text": " Let's keep looking at the rest.", "tokens": [961, 311, 1066, 1237, 412, 264, 1472, 13], "temperature": 0.0, "avg_logprob": -0.4253396426930147, "compression_ratio": 1.6633165829145728, "no_speech_prob": 4.860244189330842e-06}, {"id": 1749, "seek": 550412, "start": 5515.88, "end": 5518.48, "text": " So here's the first one, two, three, four layers.", "tokens": [407, 510, 311, 264, 700, 472, 11, 732, 11, 1045, 11, 1451, 7914, 13], "temperature": 0.0, "avg_logprob": -0.4253396426930147, "compression_ratio": 1.6633165829145728, "no_speech_prob": 4.860244189330842e-06}, {"id": 1750, "seek": 550412, "start": 5518.48, "end": 5519.8, "text": " So compared to", "tokens": [407, 5347, 281], "temperature": 0.0, "avg_logprob": -0.4253396426930147, "compression_ratio": 1.6633165829145728, "no_speech_prob": 4.860244189330842e-06}, {"id": 1751, "seek": 550412, "start": 5522.68, "end": 5527.2, "text": " that, which is expand, does die, expand, die, expand, die,", "tokens": [300, 11, 597, 307, 5268, 11, 775, 978, 11, 5268, 11, 978, 11, 5268, 11, 978, 11], "temperature": 0.0, "avg_logprob": -0.4253396426930147, "compression_ratio": 1.6633165829145728, "no_speech_prob": 4.860244189330842e-06}, {"id": 1752, "seek": 550412, "start": 5527.2, "end": 5528.5599999999995, "text": " we're now seeing", "tokens": [321, 434, 586, 2577], "temperature": 0.0, "avg_logprob": -0.4253396426930147, "compression_ratio": 1.6633165829145728, "no_speech_prob": 4.860244189330842e-06}, {"id": 1753, "seek": 550412, "start": 5529.5199999999995, "end": 5531.24, "text": " this is really, really nice.", "tokens": [341, 307, 534, 11, 534, 1481, 13], "temperature": 0.0, "avg_logprob": -0.4253396426930147, "compression_ratio": 1.6633165829145728, "no_speech_prob": 4.860244189330842e-06}, {"id": 1754, "seek": 553124, "start": 5531.24, "end": 5534.44, "text": " We're now seeing this is looking much better.", "tokens": [492, 434, 586, 2577, 341, 307, 1237, 709, 1101, 13], "temperature": 0.0, "avg_logprob": -0.21768951416015625, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.805612919270061e-05}, {"id": 1755, "seek": 553124, "start": 5534.44, "end": 5535.5599999999995, "text": " It's straight away.", "tokens": [467, 311, 2997, 1314, 13], "temperature": 0.0, "avg_logprob": -0.21768951416015625, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.805612919270061e-05}, {"id": 1756, "seek": 553124, "start": 5535.5599999999995, "end": 5538.84, "text": " It's using the full richness of the possible activations.", "tokens": [467, 311, 1228, 264, 1577, 44506, 295, 264, 1944, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.21768951416015625, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.805612919270061e-05}, {"id": 1757, "seek": 553124, "start": 5538.84, "end": 5541.08, "text": " There's no death going on.", "tokens": [821, 311, 572, 2966, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.21768951416015625, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.805612919270061e-05}, {"id": 1758, "seek": 553124, "start": 5541.08, "end": 5545.16, "text": " But our real question is how much is in this yellow line?", "tokens": [583, 527, 957, 1168, 307, 577, 709, 307, 294, 341, 5566, 1622, 30], "temperature": 0.0, "avg_logprob": -0.21768951416015625, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.805612919270061e-05}, {"id": 1759, "seek": 553124, "start": 5545.16, "end": 5546.28, "text": " There's a question.", "tokens": [821, 311, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.21768951416015625, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.805612919270061e-05}, {"id": 1760, "seek": 553124, "start": 5547.639999999999, "end": 5549.32, "text": " And let's see.", "tokens": [400, 718, 311, 536, 13], "temperature": 0.0, "avg_logprob": -0.21768951416015625, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.805612919270061e-05}, {"id": 1761, "seek": 553124, "start": 5549.32, "end": 5554.32, "text": " In the final layer, look at that, less than 20%.", "tokens": [682, 264, 2572, 4583, 11, 574, 412, 300, 11, 1570, 813, 945, 6856], "temperature": 0.0, "avg_logprob": -0.21768951416015625, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.805612919270061e-05}, {"id": 1762, "seek": 553124, "start": 5554.5599999999995, "end": 5557.16, "text": " Right, so we're now using", "tokens": [1779, 11, 370, 321, 434, 586, 1228], "temperature": 0.0, "avg_logprob": -0.21768951416015625, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.805612919270061e-05}, {"id": 1763, "seek": 553124, "start": 5557.16, "end": 5560.679999999999, "text": " nearly all of our activations", "tokens": [6217, 439, 295, 527, 2430, 763], "temperature": 0.0, "avg_logprob": -0.21768951416015625, "compression_ratio": 1.6036866359447004, "no_speech_prob": 1.805612919270061e-05}, {"id": 1764, "seek": 556068, "start": 5560.68, "end": 5565.68, "text": " by being careful about our initialization and our value.", "tokens": [538, 885, 5026, 466, 527, 5883, 2144, 293, 527, 2158, 13], "temperature": 0.0, "avg_logprob": -0.20589738421969944, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.2804573998437263e-05}, {"id": 1765, "seek": 556068, "start": 5567.400000000001, "end": 5569.16, "text": " And then, but we're still training", "tokens": [400, 550, 11, 457, 321, 434, 920, 3097], "temperature": 0.0, "avg_logprob": -0.20589738421969944, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.2804573998437263e-05}, {"id": 1766, "seek": 556068, "start": 5569.16, "end": 5570.6, "text": " at a nice high learning rate.", "tokens": [412, 257, 1481, 1090, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.20589738421969944, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.2804573998437263e-05}, {"id": 1767, "seek": 556068, "start": 5570.6, "end": 5572.16, "text": " So this is looking great.", "tokens": [407, 341, 307, 1237, 869, 13], "temperature": 0.0, "avg_logprob": -0.20589738421969944, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.2804573998437263e-05}, {"id": 1768, "seek": 556068, "start": 5574.280000000001, "end": 5577.04, "text": " Could you explain again how to read the histograms?", "tokens": [7497, 291, 2903, 797, 577, 281, 1401, 264, 49816, 82, 30], "temperature": 0.0, "avg_logprob": -0.20589738421969944, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.2804573998437263e-05}, {"id": 1769, "seek": 556068, "start": 5578.76, "end": 5580.04, "text": " Sure.", "tokens": [4894, 13], "temperature": 0.0, "avg_logprob": -0.20589738421969944, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.2804573998437263e-05}, {"id": 1770, "seek": 556068, "start": 5580.04, "end": 5583.08, "text": " So the four histograms, let's go back to the earlier one.", "tokens": [407, 264, 1451, 49816, 82, 11, 718, 311, 352, 646, 281, 264, 3071, 472, 13], "temperature": 0.0, "avg_logprob": -0.20589738421969944, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.2804573998437263e-05}, {"id": 1771, "seek": 556068, "start": 5583.08, "end": 5585.320000000001, "text": " So the four histograms are simply the four layers.", "tokens": [407, 264, 1451, 49816, 82, 366, 2935, 264, 1451, 7914, 13], "temperature": 0.0, "avg_logprob": -0.20589738421969944, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.2804573998437263e-05}, {"id": 1772, "seek": 556068, "start": 5585.320000000001, "end": 5589.56, "text": " So layer, the first, after the first conv, second, third, fourth.", "tokens": [407, 4583, 11, 264, 700, 11, 934, 264, 700, 3754, 11, 1150, 11, 2636, 11, 6409, 13], "temperature": 0.0, "avg_logprob": -0.20589738421969944, "compression_ratio": 1.6964285714285714, "no_speech_prob": 1.2804573998437263e-05}, {"id": 1773, "seek": 558956, "start": 5589.56, "end": 5594.56, "text": " And the x-axis is the iteration.", "tokens": [400, 264, 2031, 12, 24633, 307, 264, 24784, 13], "temperature": 0.0, "avg_logprob": -0.2566182613372803, "compression_ratio": 1.576158940397351, "no_speech_prob": 2.5069706680369563e-05}, {"id": 1774, "seek": 558956, "start": 5595.04, "end": 5597.8, "text": " So each one is just one more iteration", "tokens": [407, 1184, 472, 307, 445, 472, 544, 24784], "temperature": 0.0, "avg_logprob": -0.2566182613372803, "compression_ratio": 1.576158940397351, "no_speech_prob": 2.5069706680369563e-05}, {"id": 1775, "seek": 558956, "start": 5597.8, "end": 5600.120000000001, "text": " as most of our plots show.", "tokens": [382, 881, 295, 527, 28609, 855, 13], "temperature": 0.0, "avg_logprob": -0.2566182613372803, "compression_ratio": 1.576158940397351, "no_speech_prob": 2.5069706680369563e-05}, {"id": 1776, "seek": 558956, "start": 5600.120000000001, "end": 5605.120000000001, "text": " The y-axis is how many activations", "tokens": [440, 288, 12, 24633, 307, 577, 867, 2430, 763], "temperature": 0.0, "avg_logprob": -0.2566182613372803, "compression_ratio": 1.576158940397351, "no_speech_prob": 2.5069706680369563e-05}, {"id": 1777, "seek": 558956, "start": 5606.64, "end": 5611.64, "text": " are the highest they can be or the lowest they can be.", "tokens": [366, 264, 6343, 436, 393, 312, 420, 264, 12437, 436, 393, 312, 13], "temperature": 0.0, "avg_logprob": -0.2566182613372803, "compression_ratio": 1.576158940397351, "no_speech_prob": 2.5069706680369563e-05}, {"id": 1778, "seek": 558956, "start": 5612.6, "end": 5616.92, "text": " So what this one here is showing us, for example,", "tokens": [407, 437, 341, 472, 510, 307, 4099, 505, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.2566182613372803, "compression_ratio": 1.576158940397351, "no_speech_prob": 2.5069706680369563e-05}, {"id": 1779, "seek": 561692, "start": 5616.92, "end": 5620.24, "text": " is that there are some activations that are at the max", "tokens": [307, 300, 456, 366, 512, 2430, 763, 300, 366, 412, 264, 11469], "temperature": 0.0, "avg_logprob": -0.2193254297429865, "compression_ratio": 1.9615384615384615, "no_speech_prob": 2.9769298635073937e-05}, {"id": 1780, "seek": 561692, "start": 5620.24, "end": 5621.76, "text": " and some activations are in the middle", "tokens": [293, 512, 2430, 763, 366, 294, 264, 2808], "temperature": 0.0, "avg_logprob": -0.2193254297429865, "compression_ratio": 1.9615384615384615, "no_speech_prob": 2.9769298635073937e-05}, {"id": 1781, "seek": 561692, "start": 5621.76, "end": 5623.76, "text": " and some activations at the bottom.", "tokens": [293, 512, 2430, 763, 412, 264, 2767, 13], "temperature": 0.0, "avg_logprob": -0.2193254297429865, "compression_ratio": 1.9615384615384615, "no_speech_prob": 2.9769298635073937e-05}, {"id": 1782, "seek": 561692, "start": 5623.76, "end": 5625.4800000000005, "text": " Whereas this one here is showing us", "tokens": [13813, 341, 472, 510, 307, 4099, 505], "temperature": 0.0, "avg_logprob": -0.2193254297429865, "compression_ratio": 1.9615384615384615, "no_speech_prob": 2.9769298635073937e-05}, {"id": 1783, "seek": 561692, "start": 5625.4800000000005, "end": 5629.28, "text": " that all of the activations are basically zero.", "tokens": [300, 439, 295, 264, 2430, 763, 366, 1936, 4018, 13], "temperature": 0.0, "avg_logprob": -0.2193254297429865, "compression_ratio": 1.9615384615384615, "no_speech_prob": 2.9769298635073937e-05}, {"id": 1784, "seek": 561692, "start": 5630.92, "end": 5635.4400000000005, "text": " So what this shows us in this histogram", "tokens": [407, 437, 341, 3110, 505, 294, 341, 49816], "temperature": 0.0, "avg_logprob": -0.2193254297429865, "compression_ratio": 1.9615384615384615, "no_speech_prob": 2.9769298635073937e-05}, {"id": 1785, "seek": 561692, "start": 5635.4400000000005, "end": 5638.4400000000005, "text": " is that now we're going all the way from plus seven", "tokens": [307, 300, 586, 321, 434, 516, 439, 264, 636, 490, 1804, 3407], "temperature": 0.0, "avg_logprob": -0.2193254297429865, "compression_ratio": 1.9615384615384615, "no_speech_prob": 2.9769298635073937e-05}, {"id": 1786, "seek": 561692, "start": 5638.4400000000005, "end": 5640.88, "text": " to minus seven because we can have negatives.", "tokens": [281, 3175, 3407, 570, 321, 393, 362, 40019, 13], "temperature": 0.0, "avg_logprob": -0.2193254297429865, "compression_ratio": 1.9615384615384615, "no_speech_prob": 2.9769298635073937e-05}, {"id": 1787, "seek": 561692, "start": 5640.88, "end": 5642.2, "text": " This is zero.", "tokens": [639, 307, 4018, 13], "temperature": 0.0, "avg_logprob": -0.2193254297429865, "compression_ratio": 1.9615384615384615, "no_speech_prob": 2.9769298635073937e-05}, {"id": 1788, "seek": 561692, "start": 5642.2, "end": 5644.36, "text": " It's showing us that most of them are zero", "tokens": [467, 311, 4099, 505, 300, 881, 295, 552, 366, 4018], "temperature": 0.0, "avg_logprob": -0.2193254297429865, "compression_ratio": 1.9615384615384615, "no_speech_prob": 2.9769298635073937e-05}, {"id": 1789, "seek": 564436, "start": 5644.36, "end": 5648.719999999999, "text": " because yellow is the most energy.", "tokens": [570, 5566, 307, 264, 881, 2281, 13], "temperature": 0.0, "avg_logprob": -0.1553087790035507, "compression_ratio": 1.6757990867579908, "no_speech_prob": 1.341935512755299e-05}, {"id": 1790, "seek": 564436, "start": 5651.04, "end": 5653.799999999999, "text": " But there are activations throughout everything", "tokens": [583, 456, 366, 2430, 763, 3710, 1203], "temperature": 0.0, "avg_logprob": -0.1553087790035507, "compression_ratio": 1.6757990867579908, "no_speech_prob": 1.341935512755299e-05}, {"id": 1791, "seek": 564436, "start": 5653.799999999999, "end": 5658.36, "text": " from the bottom to the top and a few less than zero,", "tokens": [490, 264, 2767, 281, 264, 1192, 293, 257, 1326, 1570, 813, 4018, 11], "temperature": 0.0, "avg_logprob": -0.1553087790035507, "compression_ratio": 1.6757990867579908, "no_speech_prob": 1.341935512755299e-05}, {"id": 1792, "seek": 564436, "start": 5658.36, "end": 5661.2, "text": " as we would expect because we have a leaky value", "tokens": [382, 321, 576, 2066, 570, 321, 362, 257, 476, 15681, 2158], "temperature": 0.0, "avg_logprob": -0.1553087790035507, "compression_ratio": 1.6757990867579908, "no_speech_prob": 1.341935512755299e-05}, {"id": 1793, "seek": 564436, "start": 5661.2, "end": 5663.719999999999, "text": " and we also have that minus.", "tokens": [293, 321, 611, 362, 300, 3175, 13], "temperature": 0.0, "avg_logprob": -0.1553087790035507, "compression_ratio": 1.6757990867579908, "no_speech_prob": 1.341935512755299e-05}, {"id": 1794, "seek": 564436, "start": 5663.719999999999, "end": 5666.96, "text": " We're not doing minus 0.5, we're doing minus 0.4", "tokens": [492, 434, 406, 884, 3175, 1958, 13, 20, 11, 321, 434, 884, 3175, 1958, 13, 19], "temperature": 0.0, "avg_logprob": -0.1553087790035507, "compression_ratio": 1.6757990867579908, "no_speech_prob": 1.341935512755299e-05}, {"id": 1795, "seek": 564436, "start": 5666.96, "end": 5669.48, "text": " because leaky value means that we don't need", "tokens": [570, 476, 15681, 2158, 1355, 300, 321, 500, 380, 643], "temperature": 0.0, "avg_logprob": -0.1553087790035507, "compression_ratio": 1.6757990867579908, "no_speech_prob": 1.341935512755299e-05}, {"id": 1796, "seek": 564436, "start": 5669.48, "end": 5670.679999999999, "text": " to subtract half anymore.", "tokens": [281, 16390, 1922, 3602, 13], "temperature": 0.0, "avg_logprob": -0.1553087790035507, "compression_ratio": 1.6757990867579908, "no_speech_prob": 1.341935512755299e-05}, {"id": 1797, "seek": 567068, "start": 5670.68, "end": 5674.4400000000005, "text": " We subtract a bit less than half.", "tokens": [492, 16390, 257, 857, 1570, 813, 1922, 13], "temperature": 0.0, "avg_logprob": -0.09255459434107731, "compression_ratio": 1.5971563981042654, "no_speech_prob": 1.4062244190427009e-05}, {"id": 1798, "seek": 567068, "start": 5674.4400000000005, "end": 5677.04, "text": " And so then this line is telling us", "tokens": [400, 370, 550, 341, 1622, 307, 3585, 505], "temperature": 0.0, "avg_logprob": -0.09255459434107731, "compression_ratio": 1.5971563981042654, "no_speech_prob": 1.4062244190427009e-05}, {"id": 1799, "seek": 567068, "start": 5677.04, "end": 5681.04, "text": " what percentage of them are zero or nearly zero.", "tokens": [437, 9668, 295, 552, 366, 4018, 420, 6217, 4018, 13], "temperature": 0.0, "avg_logprob": -0.09255459434107731, "compression_ratio": 1.5971563981042654, "no_speech_prob": 1.4062244190427009e-05}, {"id": 1800, "seek": 567068, "start": 5685.400000000001, "end": 5687.4800000000005, "text": " And so this is one of those things", "tokens": [400, 370, 341, 307, 472, 295, 729, 721], "temperature": 0.0, "avg_logprob": -0.09255459434107731, "compression_ratio": 1.5971563981042654, "no_speech_prob": 1.4062244190427009e-05}, {"id": 1801, "seek": 567068, "start": 5687.4800000000005, "end": 5690.240000000001, "text": " which is good to run lots of experiments", "tokens": [597, 307, 665, 281, 1190, 3195, 295, 12050], "temperature": 0.0, "avg_logprob": -0.09255459434107731, "compression_ratio": 1.5971563981042654, "no_speech_prob": 1.4062244190427009e-05}, {"id": 1802, "seek": 567068, "start": 5690.240000000001, "end": 5691.68, "text": " in the notebook yourself to get a sense", "tokens": [294, 264, 21060, 1803, 281, 483, 257, 2020], "temperature": 0.0, "avg_logprob": -0.09255459434107731, "compression_ratio": 1.5971563981042654, "no_speech_prob": 1.4062244190427009e-05}, {"id": 1803, "seek": 567068, "start": 5691.68, "end": 5694.56, "text": " of what's actually in these histograms.", "tokens": [295, 437, 311, 767, 294, 613, 49816, 82, 13], "temperature": 0.0, "avg_logprob": -0.09255459434107731, "compression_ratio": 1.5971563981042654, "no_speech_prob": 1.4062244190427009e-05}, {"id": 1804, "seek": 567068, "start": 5694.56, "end": 5696.72, "text": " So you can just go ahead and have a look", "tokens": [407, 291, 393, 445, 352, 2286, 293, 362, 257, 574], "temperature": 0.0, "avg_logprob": -0.09255459434107731, "compression_ratio": 1.5971563981042654, "no_speech_prob": 1.4062244190427009e-05}, {"id": 1805, "seek": 567068, "start": 5696.72, "end": 5698.56, "text": " at each hook's stats.", "tokens": [412, 1184, 6328, 311, 18152, 13], "temperature": 0.0, "avg_logprob": -0.09255459434107731, "compression_ratio": 1.5971563981042654, "no_speech_prob": 1.4062244190427009e-05}, {"id": 1806, "seek": 569856, "start": 5698.56, "end": 5701.72, "text": " And the third thing in it will be the histograms", "tokens": [400, 264, 2636, 551, 294, 309, 486, 312, 264, 49816, 82], "temperature": 0.0, "avg_logprob": -0.1259863651715792, "compression_ratio": 1.5555555555555556, "no_speech_prob": 6.240798938961234e-06}, {"id": 1807, "seek": 569856, "start": 5701.72, "end": 5703.080000000001, "text": " so you can see what shape is it", "tokens": [370, 291, 393, 536, 437, 3909, 307, 309], "temperature": 0.0, "avg_logprob": -0.1259863651715792, "compression_ratio": 1.5555555555555556, "no_speech_prob": 6.240798938961234e-06}, {"id": 1808, "seek": 569856, "start": 5703.080000000001, "end": 5705.04, "text": " and how is it calculated and so forth.", "tokens": [293, 577, 307, 309, 15598, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1259863651715792, "compression_ratio": 1.5555555555555556, "no_speech_prob": 6.240798938961234e-06}, {"id": 1809, "seek": 569856, "start": 5707.4400000000005, "end": 5709.400000000001, "text": " Okay, so now that we've done that,", "tokens": [1033, 11, 370, 586, 300, 321, 600, 1096, 300, 11], "temperature": 0.0, "avg_logprob": -0.1259863651715792, "compression_ratio": 1.5555555555555556, "no_speech_prob": 6.240798938961234e-06}, {"id": 1810, "seek": 569856, "start": 5710.4800000000005, "end": 5712.64, "text": " this is looking really good.", "tokens": [341, 307, 1237, 534, 665, 13], "temperature": 0.0, "avg_logprob": -0.1259863651715792, "compression_ratio": 1.5555555555555556, "no_speech_prob": 6.240798938961234e-06}, {"id": 1811, "seek": 569856, "start": 5712.64, "end": 5716.320000000001, "text": " So what actually happens if we train like this?", "tokens": [407, 437, 767, 2314, 498, 321, 3847, 411, 341, 30], "temperature": 0.0, "avg_logprob": -0.1259863651715792, "compression_ratio": 1.5555555555555556, "no_speech_prob": 6.240798938961234e-06}, {"id": 1812, "seek": 569856, "start": 5716.320000000001, "end": 5720.200000000001, "text": " So let's do a one cycle training.", "tokens": [407, 718, 311, 360, 257, 472, 6586, 3097, 13], "temperature": 0.0, "avg_logprob": -0.1259863651715792, "compression_ratio": 1.5555555555555556, "no_speech_prob": 6.240798938961234e-06}, {"id": 1813, "seek": 569856, "start": 5720.200000000001, "end": 5723.160000000001, "text": " So use that combined sheds we built last week.", "tokens": [407, 764, 300, 9354, 402, 5147, 321, 3094, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.1259863651715792, "compression_ratio": 1.5555555555555556, "no_speech_prob": 6.240798938961234e-06}, {"id": 1814, "seek": 569856, "start": 5723.160000000001, "end": 5727.240000000001, "text": " 50-50, two phases, cosine scheduling,", "tokens": [2625, 12, 2803, 11, 732, 18764, 11, 23565, 29055, 11], "temperature": 0.0, "avg_logprob": -0.1259863651715792, "compression_ratio": 1.5555555555555556, "no_speech_prob": 6.240798938961234e-06}, {"id": 1815, "seek": 572724, "start": 5727.24, "end": 5731.599999999999, "text": " cosine annealing, so gradual warmup, gradual cool down,", "tokens": [23565, 22256, 4270, 11, 370, 32890, 4561, 1010, 11, 32890, 1627, 760, 11], "temperature": 0.0, "avg_logprob": -0.11653961854822495, "compression_ratio": 1.7467811158798283, "no_speech_prob": 6.853883860458154e-06}, {"id": 1816, "seek": 572724, "start": 5731.599999999999, "end": 5733.8, "text": " and then run it for eight epochs.", "tokens": [293, 550, 1190, 309, 337, 3180, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.11653961854822495, "compression_ratio": 1.7467811158798283, "no_speech_prob": 6.853883860458154e-06}, {"id": 1817, "seek": 572724, "start": 5733.8, "end": 5735.2, "text": " And there we go, we're doing really well.", "tokens": [400, 456, 321, 352, 11, 321, 434, 884, 534, 731, 13], "temperature": 0.0, "avg_logprob": -0.11653961854822495, "compression_ratio": 1.7467811158798283, "no_speech_prob": 6.853883860458154e-06}, {"id": 1818, "seek": 572724, "start": 5735.2, "end": 5737.639999999999, "text": " We're getting up to 98%.", "tokens": [492, 434, 1242, 493, 281, 20860, 6856], "temperature": 0.0, "avg_logprob": -0.11653961854822495, "compression_ratio": 1.7467811158798283, "no_speech_prob": 6.853883860458154e-06}, {"id": 1819, "seek": 572724, "start": 5737.639999999999, "end": 5742.12, "text": " So this kind of, we hardly were really training in a thing.", "tokens": [407, 341, 733, 295, 11, 321, 13572, 645, 534, 3097, 294, 257, 551, 13], "temperature": 0.0, "avg_logprob": -0.11653961854822495, "compression_ratio": 1.7467811158798283, "no_speech_prob": 6.853883860458154e-06}, {"id": 1820, "seek": 572724, "start": 5742.12, "end": 5746.5599999999995, "text": " We were just trying to get something that looked good.", "tokens": [492, 645, 445, 1382, 281, 483, 746, 300, 2956, 665, 13], "temperature": 0.0, "avg_logprob": -0.11653961854822495, "compression_ratio": 1.7467811158798283, "no_speech_prob": 6.853883860458154e-06}, {"id": 1821, "seek": 572724, "start": 5746.5599999999995, "end": 5748.12, "text": " And once we had something that looked good", "tokens": [400, 1564, 321, 632, 746, 300, 2956, 665], "temperature": 0.0, "avg_logprob": -0.11653961854822495, "compression_ratio": 1.7467811158798283, "no_speech_prob": 6.853883860458154e-06}, {"id": 1822, "seek": 572724, "start": 5748.12, "end": 5749.679999999999, "text": " in terms of the telemetry,", "tokens": [294, 2115, 295, 264, 4304, 5537, 627, 11], "temperature": 0.0, "avg_logprob": -0.11653961854822495, "compression_ratio": 1.7467811158798283, "no_speech_prob": 6.853883860458154e-06}, {"id": 1823, "seek": 572724, "start": 5749.679999999999, "end": 5752.2, "text": " it's really training really well.", "tokens": [309, 311, 534, 3097, 534, 731, 13], "temperature": 0.0, "avg_logprob": -0.11653961854822495, "compression_ratio": 1.7467811158798283, "no_speech_prob": 6.853883860458154e-06}, {"id": 1824, "seek": 572724, "start": 5754.16, "end": 5757.04, "text": " One option I added, by the way,", "tokens": [1485, 3614, 286, 3869, 11, 538, 264, 636, 11], "temperature": 0.0, "avg_logprob": -0.11653961854822495, "compression_ratio": 1.7467811158798283, "no_speech_prob": 6.853883860458154e-06}, {"id": 1825, "seek": 575704, "start": 5757.04, "end": 5762.0, "text": " in initCNN was I added a uniform Boolean,", "tokens": [294, 3157, 34, 45, 45, 390, 286, 3869, 257, 9452, 23351, 28499, 11], "temperature": 0.0, "avg_logprob": -0.11531534472715507, "compression_ratio": 1.6790697674418604, "no_speech_prob": 1.3006769222556613e-05}, {"id": 1826, "seek": 575704, "start": 5762.0, "end": 5764.72, "text": " which will set the initialization function", "tokens": [597, 486, 992, 264, 5883, 2144, 2445], "temperature": 0.0, "avg_logprob": -0.11531534472715507, "compression_ratio": 1.6790697674418604, "no_speech_prob": 1.3006769222556613e-05}, {"id": 1827, "seek": 575704, "start": 5764.72, "end": 5768.36, "text": " to chiming normal if it's false,", "tokens": [281, 18375, 278, 2710, 498, 309, 311, 7908, 11], "temperature": 0.0, "avg_logprob": -0.11531534472715507, "compression_ratio": 1.6790697674418604, "no_speech_prob": 1.3006769222556613e-05}, {"id": 1828, "seek": 575704, "start": 5768.36, "end": 5770.16, "text": " which is what we've been using so far,", "tokens": [597, 307, 437, 321, 600, 668, 1228, 370, 1400, 11], "temperature": 0.0, "avg_logprob": -0.11531534472715507, "compression_ratio": 1.6790697674418604, "no_speech_prob": 1.3006769222556613e-05}, {"id": 1829, "seek": 575704, "start": 5770.16, "end": 5772.92, "text": " or chiming uniform if it's true.", "tokens": [420, 18375, 278, 9452, 498, 309, 311, 2074, 13], "temperature": 0.0, "avg_logprob": -0.11531534472715507, "compression_ratio": 1.6790697674418604, "no_speech_prob": 1.3006769222556613e-05}, {"id": 1830, "seek": 575704, "start": 5772.92, "end": 5776.6, "text": " Chiming uniform, so now I've just trained the same model", "tokens": [761, 332, 278, 9452, 11, 370, 586, 286, 600, 445, 8895, 264, 912, 2316], "temperature": 0.0, "avg_logprob": -0.11531534472715507, "compression_ratio": 1.6790697674418604, "no_speech_prob": 1.3006769222556613e-05}, {"id": 1831, "seek": 575704, "start": 5776.6, "end": 5777.92, "text": " with uniform equals true.", "tokens": [365, 9452, 6915, 2074, 13], "temperature": 0.0, "avg_logprob": -0.11531534472715507, "compression_ratio": 1.6790697674418604, "no_speech_prob": 1.3006769222556613e-05}, {"id": 1832, "seek": 575704, "start": 5779.08, "end": 5782.92, "text": " A lot of people think that uniform is better than normal", "tokens": [316, 688, 295, 561, 519, 300, 9452, 307, 1101, 813, 2710], "temperature": 0.0, "avg_logprob": -0.11531534472715507, "compression_ratio": 1.6790697674418604, "no_speech_prob": 1.3006769222556613e-05}, {"id": 1833, "seek": 575704, "start": 5782.92, "end": 5785.72, "text": " because a uniform random number", "tokens": [570, 257, 9452, 4974, 1230], "temperature": 0.0, "avg_logprob": -0.11531534472715507, "compression_ratio": 1.6790697674418604, "no_speech_prob": 1.3006769222556613e-05}, {"id": 1834, "seek": 578572, "start": 5785.72, "end": 5787.84, "text": " is less often close to zero.", "tokens": [307, 1570, 2049, 1998, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.07512742905389695, "compression_ratio": 1.551440329218107, "no_speech_prob": 5.014554517401848e-06}, {"id": 1835, "seek": 578572, "start": 5787.84, "end": 5792.400000000001, "text": " And so the thinking is that maybe uniform random,", "tokens": [400, 370, 264, 1953, 307, 300, 1310, 9452, 4974, 11], "temperature": 0.0, "avg_logprob": -0.07512742905389695, "compression_ratio": 1.551440329218107, "no_speech_prob": 5.014554517401848e-06}, {"id": 1836, "seek": 578572, "start": 5792.400000000001, "end": 5794.76, "text": " uniform initialization might cause it", "tokens": [9452, 5883, 2144, 1062, 3082, 309], "temperature": 0.0, "avg_logprob": -0.07512742905389695, "compression_ratio": 1.551440329218107, "no_speech_prob": 5.014554517401848e-06}, {"id": 1837, "seek": 578572, "start": 5794.76, "end": 5797.88, "text": " to kind of have a better richness of activations.", "tokens": [281, 733, 295, 362, 257, 1101, 44506, 295, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.07512742905389695, "compression_ratio": 1.551440329218107, "no_speech_prob": 5.014554517401848e-06}, {"id": 1838, "seek": 578572, "start": 5797.88, "end": 5799.52, "text": " I haven't studied this closely.", "tokens": [286, 2378, 380, 9454, 341, 8185, 13], "temperature": 0.0, "avg_logprob": -0.07512742905389695, "compression_ratio": 1.551440329218107, "no_speech_prob": 5.014554517401848e-06}, {"id": 1839, "seek": 578572, "start": 5799.52, "end": 5803.12, "text": " I'm not sure I've seen a careful analysis in a paper.", "tokens": [286, 478, 406, 988, 286, 600, 1612, 257, 5026, 5215, 294, 257, 3035, 13], "temperature": 0.0, "avg_logprob": -0.07512742905389695, "compression_ratio": 1.551440329218107, "no_speech_prob": 5.014554517401848e-06}, {"id": 1840, "seek": 578572, "start": 5803.12, "end": 5806.280000000001, "text": " In this case, 9822 versus 9826,", "tokens": [682, 341, 1389, 11, 20860, 7490, 5717, 20860, 10880, 11], "temperature": 0.0, "avg_logprob": -0.07512742905389695, "compression_ratio": 1.551440329218107, "no_speech_prob": 5.014554517401848e-06}, {"id": 1841, "seek": 578572, "start": 5806.280000000001, "end": 5808.12, "text": " they're looking pretty similar,", "tokens": [436, 434, 1237, 1238, 2531, 11], "temperature": 0.0, "avg_logprob": -0.07512742905389695, "compression_ratio": 1.551440329218107, "no_speech_prob": 5.014554517401848e-06}, {"id": 1842, "seek": 578572, "start": 5808.12, "end": 5811.320000000001, "text": " but that's just something else that it's there to play with.", "tokens": [457, 300, 311, 445, 746, 1646, 300, 309, 311, 456, 281, 862, 365, 13], "temperature": 0.0, "avg_logprob": -0.07512742905389695, "compression_ratio": 1.551440329218107, "no_speech_prob": 5.014554517401848e-06}, {"id": 1843, "seek": 581132, "start": 5811.32, "end": 5816.28, "text": " So at this point, we've got a pretty nice bunch of things", "tokens": [407, 412, 341, 935, 11, 321, 600, 658, 257, 1238, 1481, 3840, 295, 721], "temperature": 0.0, "avg_logprob": -0.087816588083903, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.425260613061255e-06}, {"id": 1844, "seek": 581132, "start": 5816.28, "end": 5819.36, "text": " you can look at now, and so you can see", "tokens": [291, 393, 574, 412, 586, 11, 293, 370, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.087816588083903, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.425260613061255e-06}, {"id": 1845, "seek": 581132, "start": 5819.36, "end": 5822.679999999999, "text": " as your kind of problem to play with during the week", "tokens": [382, 428, 733, 295, 1154, 281, 862, 365, 1830, 264, 1243], "temperature": 0.0, "avg_logprob": -0.087816588083903, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.425260613061255e-06}, {"id": 1846, "seek": 581132, "start": 5822.679999999999, "end": 5826.5199999999995, "text": " is how accurate can you make a model", "tokens": [307, 577, 8559, 393, 291, 652, 257, 2316], "temperature": 0.0, "avg_logprob": -0.087816588083903, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.425260613061255e-06}, {"id": 1847, "seek": 581132, "start": 5826.5199999999995, "end": 5828.88, "text": " just using the layers we've created so far?", "tokens": [445, 1228, 264, 7914, 321, 600, 2942, 370, 1400, 30], "temperature": 0.0, "avg_logprob": -0.087816588083903, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.425260613061255e-06}, {"id": 1848, "seek": 581132, "start": 5828.88, "end": 5833.36, "text": " And for the ones that are great accuracy,", "tokens": [400, 337, 264, 2306, 300, 366, 869, 14170, 11], "temperature": 0.0, "avg_logprob": -0.087816588083903, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.425260613061255e-06}, {"id": 1849, "seek": 581132, "start": 5833.36, "end": 5834.639999999999, "text": " what does the telemetry look like?", "tokens": [437, 775, 264, 4304, 5537, 627, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.087816588083903, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.425260613061255e-06}, {"id": 1850, "seek": 581132, "start": 5834.639999999999, "end": 5837.0, "text": " How can you tell whether it's gonna be good?", "tokens": [1012, 393, 291, 980, 1968, 309, 311, 799, 312, 665, 30], "temperature": 0.0, "avg_logprob": -0.087816588083903, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.425260613061255e-06}, {"id": 1851, "seek": 581132, "start": 5837.0, "end": 5838.5199999999995, "text": " And then what insights can you gain from that", "tokens": [400, 550, 437, 14310, 393, 291, 6052, 490, 300], "temperature": 0.0, "avg_logprob": -0.087816588083903, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.425260613061255e-06}, {"id": 1852, "seek": 581132, "start": 5838.5199999999995, "end": 5840.28, "text": " to make it even better?", "tokens": [281, 652, 309, 754, 1101, 30], "temperature": 0.0, "avg_logprob": -0.087816588083903, "compression_ratio": 1.7056451612903225, "no_speech_prob": 4.425260613061255e-06}, {"id": 1853, "seek": 584028, "start": 5840.28, "end": 5844.0, "text": " So in the end, try to beat me, right?", "tokens": [407, 294, 264, 917, 11, 853, 281, 4224, 385, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16531430429487087, "compression_ratio": 1.3974358974358974, "no_speech_prob": 8.800157957011834e-06}, {"id": 1854, "seek": 584028, "start": 5844.0, "end": 5846.12, "text": " Try to beat 98%.", "tokens": [6526, 281, 4224, 20860, 6856], "temperature": 0.0, "avg_logprob": -0.16531430429487087, "compression_ratio": 1.3974358974358974, "no_speech_prob": 8.800157957011834e-06}, {"id": 1855, "seek": 584028, "start": 5846.12, "end": 5848.16, "text": " You'll find you can beat it pretty easily", "tokens": [509, 603, 915, 291, 393, 4224, 309, 1238, 3612], "temperature": 0.0, "avg_logprob": -0.16531430429487087, "compression_ratio": 1.3974358974358974, "no_speech_prob": 8.800157957011834e-06}, {"id": 1856, "seek": 584028, "start": 5849.12, "end": 5851.5599999999995, "text": " with some playing around, but do some experiments.", "tokens": [365, 512, 2433, 926, 11, 457, 360, 512, 12050, 13], "temperature": 0.0, "avg_logprob": -0.16531430429487087, "compression_ratio": 1.3974358974358974, "no_speech_prob": 8.800157957011834e-06}, {"id": 1857, "seek": 584028, "start": 5855.2, "end": 5860.2, "text": " All right, so that's kind of about what we can do", "tokens": [1057, 558, 11, 370, 300, 311, 733, 295, 466, 437, 321, 393, 360], "temperature": 0.0, "avg_logprob": -0.16531430429487087, "compression_ratio": 1.3974358974358974, "no_speech_prob": 8.800157957011834e-06}, {"id": 1858, "seek": 584028, "start": 5862.88, "end": 5864.12, "text": " with initialization.", "tokens": [365, 5883, 2144, 13], "temperature": 0.0, "avg_logprob": -0.16531430429487087, "compression_ratio": 1.3974358974358974, "no_speech_prob": 8.800157957011834e-06}, {"id": 1859, "seek": 586412, "start": 5864.12, "end": 5869.12, "text": " You can go further with, as we discussed,", "tokens": [509, 393, 352, 3052, 365, 11, 382, 321, 7152, 11], "temperature": 0.0, "avg_logprob": -0.27993204453412224, "compression_ratio": 1.4855769230769231, "no_speech_prob": 2.8399961593095213e-05}, {"id": 1860, "seek": 586412, "start": 5870.36, "end": 5874.24, "text": " with SelU or with FixUp.", "tokens": [365, 10736, 52, 420, 365, 25538, 22164, 13], "temperature": 0.0, "avg_logprob": -0.27993204453412224, "compression_ratio": 1.4855769230769231, "no_speech_prob": 2.8399961593095213e-05}, {"id": 1861, "seek": 586412, "start": 5874.24, "end": 5875.8, "text": " Like there are these really finely tuned", "tokens": [1743, 456, 366, 613, 534, 31529, 10870], "temperature": 0.0, "avg_logprob": -0.27993204453412224, "compression_ratio": 1.4855769230769231, "no_speech_prob": 2.8399961593095213e-05}, {"id": 1862, "seek": 586412, "start": 5875.8, "end": 5877.44, "text": " initialization methods that you can do", "tokens": [5883, 2144, 7150, 300, 291, 393, 360], "temperature": 0.0, "avg_logprob": -0.27993204453412224, "compression_ratio": 1.4855769230769231, "no_speech_prob": 2.8399961593095213e-05}, {"id": 1863, "seek": 586412, "start": 5877.44, "end": 5880.76, "text": " a thousand layers deep, but they're super fiddly.", "tokens": [257, 4714, 7914, 2452, 11, 457, 436, 434, 1687, 283, 14273, 356, 13], "temperature": 0.0, "avg_logprob": -0.27993204453412224, "compression_ratio": 1.4855769230769231, "no_speech_prob": 2.8399961593095213e-05}, {"id": 1864, "seek": 586412, "start": 5880.76, "end": 5883.76, "text": " So generally, I would use something like the layer-wise", "tokens": [407, 5101, 11, 286, 576, 764, 746, 411, 264, 4583, 12, 3711], "temperature": 0.0, "avg_logprob": -0.27993204453412224, "compression_ratio": 1.4855769230769231, "no_speech_prob": 2.8399961593095213e-05}, {"id": 1865, "seek": 586412, "start": 5883.76, "end": 5888.76, "text": " sequential unit variance, LSUV thing that we saw earlier", "tokens": [42881, 4985, 21977, 11, 441, 20214, 53, 551, 300, 321, 1866, 3071], "temperature": 0.0, "avg_logprob": -0.27993204453412224, "compression_ratio": 1.4855769230769231, "no_speech_prob": 2.8399961593095213e-05}, {"id": 1866, "seek": 588876, "start": 5888.76, "end": 5893.76, "text": " in, oh sorry, we haven't done that one yet.", "tokens": [294, 11, 1954, 2597, 11, 321, 2378, 380, 1096, 300, 472, 1939, 13], "temperature": 0.0, "avg_logprob": -0.15630227973662228, "compression_ratio": 1.5055555555555555, "no_speech_prob": 4.860070021095453e-06}, {"id": 1867, "seek": 588876, "start": 5895.12, "end": 5896.8, "text": " Okay, we're gonna do that next.", "tokens": [1033, 11, 321, 434, 799, 360, 300, 958, 13], "temperature": 0.0, "avg_logprob": -0.15630227973662228, "compression_ratio": 1.5055555555555555, "no_speech_prob": 4.860070021095453e-06}, {"id": 1868, "seek": 588876, "start": 5896.8, "end": 5899.6, "text": " Okay, so, forget I said that.", "tokens": [1033, 11, 370, 11, 2870, 286, 848, 300, 13], "temperature": 0.0, "avg_logprob": -0.15630227973662228, "compression_ratio": 1.5055555555555555, "no_speech_prob": 4.860070021095453e-06}, {"id": 1869, "seek": 588876, "start": 5902.0, "end": 5904.4800000000005, "text": " So that's kind of about as far as we can get", "tokens": [407, 300, 311, 733, 295, 466, 382, 1400, 382, 321, 393, 483], "temperature": 0.0, "avg_logprob": -0.15630227973662228, "compression_ratio": 1.5055555555555555, "no_speech_prob": 4.860070021095453e-06}, {"id": 1870, "seek": 588876, "start": 5904.4800000000005, "end": 5907.84, "text": " with basic initialization.", "tokens": [365, 3875, 5883, 2144, 13], "temperature": 0.0, "avg_logprob": -0.15630227973662228, "compression_ratio": 1.5055555555555555, "no_speech_prob": 4.860070021095453e-06}, {"id": 1871, "seek": 588876, "start": 5908.88, "end": 5913.280000000001, "text": " To go further, we really need to use normalization,", "tokens": [1407, 352, 3052, 11, 321, 534, 643, 281, 764, 2710, 2144, 11], "temperature": 0.0, "avg_logprob": -0.15630227973662228, "compression_ratio": 1.5055555555555555, "no_speech_prob": 4.860070021095453e-06}, {"id": 1872, "seek": 588876, "start": 5914.400000000001, "end": 5916.2, "text": " of which the most commonly known approach", "tokens": [295, 597, 264, 881, 12719, 2570, 3109], "temperature": 0.0, "avg_logprob": -0.15630227973662228, "compression_ratio": 1.5055555555555555, "no_speech_prob": 4.860070021095453e-06}, {"id": 1873, "seek": 591620, "start": 5916.2, "end": 5920.12, "text": " to normalization in the model is batch normalization.", "tokens": [281, 2710, 2144, 294, 264, 2316, 307, 15245, 2710, 2144, 13], "temperature": 0.0, "avg_logprob": -0.1179123031959105, "compression_ratio": 1.7486631016042782, "no_speech_prob": 2.902233063650783e-06}, {"id": 1874, "seek": 591620, "start": 5920.96, "end": 5923.2, "text": " So let's look at batch normalization.", "tokens": [407, 718, 311, 574, 412, 15245, 2710, 2144, 13], "temperature": 0.0, "avg_logprob": -0.1179123031959105, "compression_ratio": 1.7486631016042782, "no_speech_prob": 2.902233063650783e-06}, {"id": 1875, "seek": 591620, "start": 5923.2, "end": 5927.92, "text": " So batch normalization has been around since,", "tokens": [407, 15245, 2710, 2144, 575, 668, 926, 1670, 11], "temperature": 0.0, "avg_logprob": -0.1179123031959105, "compression_ratio": 1.7486631016042782, "no_speech_prob": 2.902233063650783e-06}, {"id": 1876, "seek": 591620, "start": 5927.92, "end": 5930.5199999999995, "text": " I think, about 2005.", "tokens": [286, 519, 11, 466, 14394, 13], "temperature": 0.0, "avg_logprob": -0.1179123031959105, "compression_ratio": 1.7486631016042782, "no_speech_prob": 2.902233063650783e-06}, {"id": 1877, "seek": 591620, "start": 5930.5199999999995, "end": 5931.5199999999995, "text": " This is the paper.", "tokens": [639, 307, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.1179123031959105, "compression_ratio": 1.7486631016042782, "no_speech_prob": 2.902233063650783e-06}, {"id": 1878, "seek": 591620, "start": 5932.5599999999995, "end": 5935.8, "text": " And they first of all describe a bit about", "tokens": [400, 436, 700, 295, 439, 6786, 257, 857, 466], "temperature": 0.0, "avg_logprob": -0.1179123031959105, "compression_ratio": 1.7486631016042782, "no_speech_prob": 2.902233063650783e-06}, {"id": 1879, "seek": 591620, "start": 5935.8, "end": 5938.92, "text": " why they thought batch normalization was a good idea.", "tokens": [983, 436, 1194, 15245, 2710, 2144, 390, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.1179123031959105, "compression_ratio": 1.7486631016042782, "no_speech_prob": 2.902233063650783e-06}, {"id": 1880, "seek": 591620, "start": 5938.92, "end": 5943.92, "text": " And by about page three, they provide the algorithm.", "tokens": [400, 538, 466, 3028, 1045, 11, 436, 2893, 264, 9284, 13], "temperature": 0.0, "avg_logprob": -0.1179123031959105, "compression_ratio": 1.7486631016042782, "no_speech_prob": 2.902233063650783e-06}, {"id": 1881, "seek": 594392, "start": 5943.92, "end": 5945.88, "text": " So it's one of those things that if you don't read", "tokens": [407, 309, 311, 472, 295, 729, 721, 300, 498, 291, 500, 380, 1401], "temperature": 0.0, "avg_logprob": -0.25100940816542683, "compression_ratio": 1.8577075098814229, "no_speech_prob": 2.5069921321119182e-05}, {"id": 1882, "seek": 594392, "start": 5945.88, "end": 5949.0, "text": " a lot of math, it might look a bit scary.", "tokens": [257, 688, 295, 5221, 11, 309, 1062, 574, 257, 857, 6958, 13], "temperature": 0.0, "avg_logprob": -0.25100940816542683, "compression_ratio": 1.8577075098814229, "no_speech_prob": 2.5069921321119182e-05}, {"id": 1883, "seek": 594392, "start": 5949.0, "end": 5951.4400000000005, "text": " But then when you look at it for a little bit longer,", "tokens": [583, 550, 562, 291, 574, 412, 309, 337, 257, 707, 857, 2854, 11], "temperature": 0.0, "avg_logprob": -0.25100940816542683, "compression_ratio": 1.8577075098814229, "no_speech_prob": 2.5069921321119182e-05}, {"id": 1884, "seek": 594392, "start": 5951.4400000000005, "end": 5954.96, "text": " you suddenly notice that this is literally just the main,", "tokens": [291, 5800, 3449, 300, 341, 307, 3736, 445, 264, 2135, 11], "temperature": 0.0, "avg_logprob": -0.25100940816542683, "compression_ratio": 1.8577075098814229, "no_speech_prob": 2.5069921321119182e-05}, {"id": 1885, "seek": 594392, "start": 5954.96, "end": 5956.84, "text": " sum divided by the count.", "tokens": [2408, 6666, 538, 264, 1207, 13], "temperature": 0.0, "avg_logprob": -0.25100940816542683, "compression_ratio": 1.8577075098814229, "no_speech_prob": 2.5069921321119182e-05}, {"id": 1886, "seek": 594392, "start": 5956.84, "end": 5961.84, "text": " And this is the mean of the difference", "tokens": [400, 341, 307, 264, 914, 295, 264, 2649], "temperature": 0.0, "avg_logprob": -0.25100940816542683, "compression_ratio": 1.8577075098814229, "no_speech_prob": 2.5069921321119182e-05}, {"id": 1887, "seek": 594392, "start": 5961.84, "end": 5964.92, "text": " to the mean squared, and it's the mean of that.", "tokens": [281, 264, 914, 8889, 11, 293, 309, 311, 264, 914, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.25100940816542683, "compression_ratio": 1.8577075098814229, "no_speech_prob": 2.5069921321119182e-05}, {"id": 1888, "seek": 594392, "start": 5964.92, "end": 5967.52, "text": " Oh, that's just what we looked at, that's variance.", "tokens": [876, 11, 300, 311, 445, 437, 321, 2956, 412, 11, 300, 311, 21977, 13], "temperature": 0.0, "avg_logprob": -0.25100940816542683, "compression_ratio": 1.8577075098814229, "no_speech_prob": 2.5069921321119182e-05}, {"id": 1889, "seek": 594392, "start": 5967.52, "end": 5969.56, "text": " And this is just subtract the main,", "tokens": [400, 341, 307, 445, 16390, 264, 2135, 11], "temperature": 0.0, "avg_logprob": -0.25100940816542683, "compression_ratio": 1.8577075098814229, "no_speech_prob": 2.5069921321119182e-05}, {"id": 1890, "seek": 594392, "start": 5969.56, "end": 5971.12, "text": " divide by the standard deviation.", "tokens": [9845, 538, 264, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.25100940816542683, "compression_ratio": 1.8577075098814229, "no_speech_prob": 2.5069921321119182e-05}, {"id": 1891, "seek": 594392, "start": 5971.12, "end": 5972.52, "text": " Oh, that's just normalization.", "tokens": [876, 11, 300, 311, 445, 2710, 2144, 13], "temperature": 0.0, "avg_logprob": -0.25100940816542683, "compression_ratio": 1.8577075098814229, "no_speech_prob": 2.5069921321119182e-05}, {"id": 1892, "seek": 597252, "start": 5972.52, "end": 5974.160000000001, "text": " So like once you look at it a second time,", "tokens": [407, 411, 1564, 291, 574, 412, 309, 257, 1150, 565, 11], "temperature": 0.0, "avg_logprob": -0.261906398161677, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.1842958883789834e-05}, {"id": 1893, "seek": 597252, "start": 5974.160000000001, "end": 5975.360000000001, "text": " you realize we've done all this.", "tokens": [291, 4325, 321, 600, 1096, 439, 341, 13], "temperature": 0.0, "avg_logprob": -0.261906398161677, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.1842958883789834e-05}, {"id": 1894, "seek": 597252, "start": 5975.360000000001, "end": 5979.360000000001, "text": " We've just done it with code, not with math.", "tokens": [492, 600, 445, 1096, 309, 365, 3089, 11, 406, 365, 5221, 13], "temperature": 0.0, "avg_logprob": -0.261906398161677, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.1842958883789834e-05}, {"id": 1895, "seek": 597252, "start": 5979.360000000001, "end": 5981.76, "text": " And so then the only thing they do is after they've", "tokens": [400, 370, 550, 264, 787, 551, 436, 360, 307, 934, 436, 600], "temperature": 0.0, "avg_logprob": -0.261906398161677, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.1842958883789834e-05}, {"id": 1896, "seek": 597252, "start": 5981.76, "end": 5984.120000000001, "text": " normalized it in the usual way,", "tokens": [48704, 309, 294, 264, 7713, 636, 11], "temperature": 0.0, "avg_logprob": -0.261906398161677, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.1842958883789834e-05}, {"id": 1897, "seek": 597252, "start": 5984.120000000001, "end": 5986.88, "text": " is that they then multiply it by gamma,", "tokens": [307, 300, 436, 550, 12972, 309, 538, 15546, 11], "temperature": 0.0, "avg_logprob": -0.261906398161677, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.1842958883789834e-05}, {"id": 1898, "seek": 597252, "start": 5986.88, "end": 5988.320000000001, "text": " and they add beta.", "tokens": [293, 436, 909, 9861, 13], "temperature": 0.0, "avg_logprob": -0.261906398161677, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.1842958883789834e-05}, {"id": 1899, "seek": 597252, "start": 5988.320000000001, "end": 5989.92, "text": " What are gamma and beta?", "tokens": [708, 366, 15546, 293, 9861, 30], "temperature": 0.0, "avg_logprob": -0.261906398161677, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.1842958883789834e-05}, {"id": 1900, "seek": 597252, "start": 5989.92, "end": 5992.52, "text": " They are parameters to be learned.", "tokens": [814, 366, 9834, 281, 312, 3264, 13], "temperature": 0.0, "avg_logprob": -0.261906398161677, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.1842958883789834e-05}, {"id": 1901, "seek": 597252, "start": 5993.320000000001, "end": 5994.160000000001, "text": " What does that mean?", "tokens": [708, 775, 300, 914, 30], "temperature": 0.0, "avg_logprob": -0.261906398161677, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.1842958883789834e-05}, {"id": 1902, "seek": 597252, "start": 5994.160000000001, "end": 5996.92, "text": " This is the most important line here.", "tokens": [639, 307, 264, 881, 1021, 1622, 510, 13], "temperature": 0.0, "avg_logprob": -0.261906398161677, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.1842958883789834e-05}, {"id": 1903, "seek": 597252, "start": 5996.92, "end": 5999.72, "text": " Remember that there are two types of numbers", "tokens": [5459, 300, 456, 366, 732, 3467, 295, 3547], "temperature": 0.0, "avg_logprob": -0.261906398161677, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.1842958883789834e-05}, {"id": 1904, "seek": 597252, "start": 5999.72, "end": 6000.56, "text": " in a neural network.", "tokens": [294, 257, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.261906398161677, "compression_ratio": 1.696969696969697, "no_speech_prob": 1.1842958883789834e-05}, {"id": 1905, "seek": 600056, "start": 6000.56, "end": 6002.96, "text": " There are two types of numbers in a neural network,", "tokens": [821, 366, 732, 3467, 295, 3547, 294, 257, 18161, 3209, 11], "temperature": 0.0, "avg_logprob": -0.1001815475335642, "compression_ratio": 1.6919831223628692, "no_speech_prob": 8.39788481243886e-06}, {"id": 1906, "seek": 600056, "start": 6002.96, "end": 6005.68, "text": " parameters and activations.", "tokens": [9834, 293, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.1001815475335642, "compression_ratio": 1.6919831223628692, "no_speech_prob": 8.39788481243886e-06}, {"id": 1907, "seek": 600056, "start": 6005.68, "end": 6008.0, "text": " Activations are things we calculate,", "tokens": [28550, 763, 366, 721, 321, 8873, 11], "temperature": 0.0, "avg_logprob": -0.1001815475335642, "compression_ratio": 1.6919831223628692, "no_speech_prob": 8.39788481243886e-06}, {"id": 1908, "seek": 600056, "start": 6008.0, "end": 6009.88, "text": " parameters are things we learn.", "tokens": [9834, 366, 721, 321, 1466, 13], "temperature": 0.0, "avg_logprob": -0.1001815475335642, "compression_ratio": 1.6919831223628692, "no_speech_prob": 8.39788481243886e-06}, {"id": 1909, "seek": 600056, "start": 6009.88, "end": 6013.6, "text": " So these are just numbers that we learn.", "tokens": [407, 613, 366, 445, 3547, 300, 321, 1466, 13], "temperature": 0.0, "avg_logprob": -0.1001815475335642, "compression_ratio": 1.6919831223628692, "no_speech_prob": 8.39788481243886e-06}, {"id": 1910, "seek": 600056, "start": 6013.6, "end": 6017.240000000001, "text": " So that's all the information we need to implement BatchNorm.", "tokens": [407, 300, 311, 439, 264, 1589, 321, 643, 281, 4445, 363, 852, 45, 687, 13], "temperature": 0.0, "avg_logprob": -0.1001815475335642, "compression_ratio": 1.6919831223628692, "no_speech_prob": 8.39788481243886e-06}, {"id": 1911, "seek": 600056, "start": 6017.240000000001, "end": 6019.320000000001, "text": " So let's go ahead and do it.", "tokens": [407, 718, 311, 352, 2286, 293, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.1001815475335642, "compression_ratio": 1.6919831223628692, "no_speech_prob": 8.39788481243886e-06}, {"id": 1912, "seek": 600056, "start": 6019.320000000001, "end": 6021.4400000000005, "text": " So first of all, we'll grab our data as before,", "tokens": [407, 700, 295, 439, 11, 321, 603, 4444, 527, 1412, 382, 949, 11], "temperature": 0.0, "avg_logprob": -0.1001815475335642, "compression_ratio": 1.6919831223628692, "no_speech_prob": 8.39788481243886e-06}, {"id": 1913, "seek": 600056, "start": 6021.4400000000005, "end": 6024.080000000001, "text": " create our callbacks as before.", "tokens": [1884, 527, 818, 17758, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.1001815475335642, "compression_ratio": 1.6919831223628692, "no_speech_prob": 8.39788481243886e-06}, {"id": 1914, "seek": 600056, "start": 6024.080000000001, "end": 6029.080000000001, "text": " Here's our pre-BatchNorm version, 96.5%.", "tokens": [1692, 311, 527, 659, 12, 33, 852, 45, 687, 3037, 11, 24124, 13, 20, 6856], "temperature": 0.0, "avg_logprob": -0.1001815475335642, "compression_ratio": 1.6919831223628692, "no_speech_prob": 8.39788481243886e-06}, {"id": 1915, "seek": 602908, "start": 6029.08, "end": 6034.08, "text": " And the highest I could get was a 0.4 learning rate this way.", "tokens": [400, 264, 6343, 286, 727, 483, 390, 257, 1958, 13, 19, 2539, 3314, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.13542246288723417, "compression_ratio": 1.5195530726256983, "no_speech_prob": 9.665624929766636e-06}, {"id": 1916, "seek": 602908, "start": 6039.64, "end": 6040.92, "text": " And so now let's try BatchNorm.", "tokens": [400, 370, 586, 718, 311, 853, 363, 852, 45, 687, 13], "temperature": 0.0, "avg_logprob": -0.13542246288723417, "compression_ratio": 1.5195530726256983, "no_speech_prob": 9.665624929766636e-06}, {"id": 1917, "seek": 602908, "start": 6040.92, "end": 6042.0, "text": " So here's BatchNorm.", "tokens": [407, 510, 311, 363, 852, 45, 687, 13], "temperature": 0.0, "avg_logprob": -0.13542246288723417, "compression_ratio": 1.5195530726256983, "no_speech_prob": 9.665624929766636e-06}, {"id": 1918, "seek": 602908, "start": 6043.12, "end": 6045.8, "text": " So let's look at the forward first.", "tokens": [407, 718, 311, 574, 412, 264, 2128, 700, 13], "temperature": 0.0, "avg_logprob": -0.13542246288723417, "compression_ratio": 1.5195530726256983, "no_speech_prob": 9.665624929766636e-06}, {"id": 1919, "seek": 602908, "start": 6047.2, "end": 6051.4, "text": " We're gonna get the mean and the variance.", "tokens": [492, 434, 799, 483, 264, 914, 293, 264, 21977, 13], "temperature": 0.0, "avg_logprob": -0.13542246288723417, "compression_ratio": 1.5195530726256983, "no_speech_prob": 9.665624929766636e-06}, {"id": 1920, "seek": 602908, "start": 6051.4, "end": 6054.08, "text": " And the way we do that is we call update stats,", "tokens": [400, 264, 636, 321, 360, 300, 307, 321, 818, 5623, 18152, 11], "temperature": 0.0, "avg_logprob": -0.13542246288723417, "compression_ratio": 1.5195530726256983, "no_speech_prob": 9.665624929766636e-06}, {"id": 1921, "seek": 602908, "start": 6054.08, "end": 6057.8, "text": " and the mean is just the mean.", "tokens": [293, 264, 914, 307, 445, 264, 914, 13], "temperature": 0.0, "avg_logprob": -0.13542246288723417, "compression_ratio": 1.5195530726256983, "no_speech_prob": 9.665624929766636e-06}, {"id": 1922, "seek": 605780, "start": 6057.8, "end": 6060.88, "text": " And the variance is just the variance.", "tokens": [400, 264, 21977, 307, 445, 264, 21977, 13], "temperature": 0.0, "avg_logprob": -0.1108669596394216, "compression_ratio": 1.92, "no_speech_prob": 1.1659317351586651e-05}, {"id": 1923, "seek": 605780, "start": 6060.88, "end": 6062.88, "text": " And then we subtract the mean,", "tokens": [400, 550, 321, 16390, 264, 914, 11], "temperature": 0.0, "avg_logprob": -0.1108669596394216, "compression_ratio": 1.92, "no_speech_prob": 1.1659317351586651e-05}, {"id": 1924, "seek": 605780, "start": 6062.88, "end": 6066.28, "text": " and we divide by the square root of the variance.", "tokens": [293, 321, 9845, 538, 264, 3732, 5593, 295, 264, 21977, 13], "temperature": 0.0, "avg_logprob": -0.1108669596394216, "compression_ratio": 1.92, "no_speech_prob": 1.1659317351586651e-05}, {"id": 1925, "seek": 605780, "start": 6066.28, "end": 6068.400000000001, "text": " And then we multiply by,", "tokens": [400, 550, 321, 12972, 538, 11], "temperature": 0.0, "avg_logprob": -0.1108669596394216, "compression_ratio": 1.92, "no_speech_prob": 1.1659317351586651e-05}, {"id": 1926, "seek": 605780, "start": 6068.400000000001, "end": 6070.84, "text": " and then I didn't call them gamma and beta,", "tokens": [293, 550, 286, 994, 380, 818, 552, 15546, 293, 9861, 11], "temperature": 0.0, "avg_logprob": -0.1108669596394216, "compression_ratio": 1.92, "no_speech_prob": 1.1659317351586651e-05}, {"id": 1927, "seek": 605780, "start": 6070.84, "end": 6072.4400000000005, "text": " because why use Greek letters when,", "tokens": [570, 983, 764, 10281, 7825, 562, 11], "temperature": 0.0, "avg_logprob": -0.1108669596394216, "compression_ratio": 1.92, "no_speech_prob": 1.1659317351586651e-05}, {"id": 1928, "seek": 605780, "start": 6072.4400000000005, "end": 6073.92, "text": " because who remembers which one's gamma", "tokens": [570, 567, 26228, 597, 472, 311, 15546], "temperature": 0.0, "avg_logprob": -0.1108669596394216, "compression_ratio": 1.92, "no_speech_prob": 1.1659317351586651e-05}, {"id": 1929, "seek": 605780, "start": 6073.92, "end": 6074.76, "text": " and which one's beta?", "tokens": [293, 597, 472, 311, 9861, 30], "temperature": 0.0, "avg_logprob": -0.1108669596394216, "compression_ratio": 1.92, "no_speech_prob": 1.1659317351586651e-05}, {"id": 1930, "seek": 605780, "start": 6074.76, "end": 6075.68, "text": " Let's use English.", "tokens": [961, 311, 764, 3669, 13], "temperature": 0.0, "avg_logprob": -0.1108669596394216, "compression_ratio": 1.92, "no_speech_prob": 1.1659317351586651e-05}, {"id": 1931, "seek": 605780, "start": 6075.68, "end": 6078.360000000001, "text": " The thing we multiply, we'll call the molts,", "tokens": [440, 551, 321, 12972, 11, 321, 603, 818, 264, 8015, 1373, 11], "temperature": 0.0, "avg_logprob": -0.1108669596394216, "compression_ratio": 1.92, "no_speech_prob": 1.1659317351586651e-05}, {"id": 1932, "seek": 605780, "start": 6078.360000000001, "end": 6081.24, "text": " and the things we add, we'll call the adds.", "tokens": [293, 264, 721, 321, 909, 11, 321, 603, 818, 264, 10860, 13], "temperature": 0.0, "avg_logprob": -0.1108669596394216, "compression_ratio": 1.92, "no_speech_prob": 1.1659317351586651e-05}, {"id": 1933, "seek": 605780, "start": 6081.24, "end": 6086.08, "text": " And so molts and adds are parameters.", "tokens": [400, 370, 8015, 1373, 293, 10860, 366, 9834, 13], "temperature": 0.0, "avg_logprob": -0.1108669596394216, "compression_ratio": 1.92, "no_speech_prob": 1.1659317351586651e-05}, {"id": 1934, "seek": 608608, "start": 6086.08, "end": 6088.88, "text": " We multiply by a parameter that initially is", "tokens": [492, 12972, 538, 257, 13075, 300, 9105, 307], "temperature": 0.0, "avg_logprob": -0.11783302582062043, "compression_ratio": 1.7672413793103448, "no_speech_prob": 7.646305675734766e-06}, {"id": 1935, "seek": 608608, "start": 6088.88, "end": 6091.36, "text": " just a bunch of ones, so it does nothing.", "tokens": [445, 257, 3840, 295, 2306, 11, 370, 309, 775, 1825, 13], "temperature": 0.0, "avg_logprob": -0.11783302582062043, "compression_ratio": 1.7672413793103448, "no_speech_prob": 7.646305675734766e-06}, {"id": 1936, "seek": 608608, "start": 6091.36, "end": 6093.96, "text": " And we add a parameter which is initially", "tokens": [400, 321, 909, 257, 13075, 597, 307, 9105], "temperature": 0.0, "avg_logprob": -0.11783302582062043, "compression_ratio": 1.7672413793103448, "no_speech_prob": 7.646305675734766e-06}, {"id": 1937, "seek": 608608, "start": 6093.96, "end": 6096.96, "text": " just a bunch of zeros, so it does nothing.", "tokens": [445, 257, 3840, 295, 35193, 11, 370, 309, 775, 1825, 13], "temperature": 0.0, "avg_logprob": -0.11783302582062043, "compression_ratio": 1.7672413793103448, "no_speech_prob": 7.646305675734766e-06}, {"id": 1938, "seek": 608608, "start": 6096.96, "end": 6099.32, "text": " But they're parameters so they can learn,", "tokens": [583, 436, 434, 9834, 370, 436, 393, 1466, 11], "temperature": 0.0, "avg_logprob": -0.11783302582062043, "compression_ratio": 1.7672413793103448, "no_speech_prob": 7.646305675734766e-06}, {"id": 1939, "seek": 608608, "start": 6099.32, "end": 6102.5599999999995, "text": " just like our, remember our original linear layer", "tokens": [445, 411, 527, 11, 1604, 527, 3380, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.11783302582062043, "compression_ratio": 1.7672413793103448, "no_speech_prob": 7.646305675734766e-06}, {"id": 1940, "seek": 608608, "start": 6102.5599999999995, "end": 6105.24, "text": " we created by hand just looked like this.", "tokens": [321, 2942, 538, 1011, 445, 2956, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.11783302582062043, "compression_ratio": 1.7672413793103448, "no_speech_prob": 7.646305675734766e-06}, {"id": 1941, "seek": 608608, "start": 6105.24, "end": 6108.24, "text": " In fact, if you think about it, adds is just bias.", "tokens": [682, 1186, 11, 498, 291, 519, 466, 309, 11, 10860, 307, 445, 12577, 13], "temperature": 0.0, "avg_logprob": -0.11783302582062043, "compression_ratio": 1.7672413793103448, "no_speech_prob": 7.646305675734766e-06}, {"id": 1942, "seek": 608608, "start": 6109.36, "end": 6112.96, "text": " Right, it's identical to the bias we created earlier.", "tokens": [1779, 11, 309, 311, 14800, 281, 264, 12577, 321, 2942, 3071, 13], "temperature": 0.0, "avg_logprob": -0.11783302582062043, "compression_ratio": 1.7672413793103448, "no_speech_prob": 7.646305675734766e-06}, {"id": 1943, "seek": 611296, "start": 6112.96, "end": 6116.24, "text": " So then there's a few extra little things", "tokens": [407, 550, 456, 311, 257, 1326, 2857, 707, 721], "temperature": 0.0, "avg_logprob": -0.08758432776839645, "compression_ratio": 1.7053571428571428, "no_speech_prob": 8.013228580239229e-06}, {"id": 1944, "seek": 611296, "start": 6116.24, "end": 6117.96, "text": " we have to think about.", "tokens": [321, 362, 281, 519, 466, 13], "temperature": 0.0, "avg_logprob": -0.08758432776839645, "compression_ratio": 1.7053571428571428, "no_speech_prob": 8.013228580239229e-06}, {"id": 1945, "seek": 611296, "start": 6117.96, "end": 6122.04, "text": " One is what happens at inference time, right?", "tokens": [1485, 307, 437, 2314, 412, 38253, 565, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.08758432776839645, "compression_ratio": 1.7053571428571428, "no_speech_prob": 8.013228580239229e-06}, {"id": 1946, "seek": 611296, "start": 6122.04, "end": 6125.32, "text": " So during training, we normalize.", "tokens": [407, 1830, 3097, 11, 321, 2710, 1125, 13], "temperature": 0.0, "avg_logprob": -0.08758432776839645, "compression_ratio": 1.7053571428571428, "no_speech_prob": 8.013228580239229e-06}, {"id": 1947, "seek": 611296, "start": 6125.32, "end": 6127.2, "text": " But the problem is that if we normalize", "tokens": [583, 264, 1154, 307, 300, 498, 321, 2710, 1125], "temperature": 0.0, "avg_logprob": -0.08758432776839645, "compression_ratio": 1.7053571428571428, "no_speech_prob": 8.013228580239229e-06}, {"id": 1948, "seek": 611296, "start": 6127.2, "end": 6129.0, "text": " in the same way at inference time,", "tokens": [294, 264, 912, 636, 412, 38253, 565, 11], "temperature": 0.0, "avg_logprob": -0.08758432776839645, "compression_ratio": 1.7053571428571428, "no_speech_prob": 8.013228580239229e-06}, {"id": 1949, "seek": 611296, "start": 6129.0, "end": 6133.28, "text": " if we get like a totally different kind of image,", "tokens": [498, 321, 483, 411, 257, 3879, 819, 733, 295, 3256, 11], "temperature": 0.0, "avg_logprob": -0.08758432776839645, "compression_ratio": 1.7053571428571428, "no_speech_prob": 8.013228580239229e-06}, {"id": 1950, "seek": 611296, "start": 6133.28, "end": 6137.6, "text": " we might kind of remove all of the things", "tokens": [321, 1062, 733, 295, 4159, 439, 295, 264, 721], "temperature": 0.0, "avg_logprob": -0.08758432776839645, "compression_ratio": 1.7053571428571428, "no_speech_prob": 8.013228580239229e-06}, {"id": 1951, "seek": 611296, "start": 6137.6, "end": 6139.28, "text": " that are interesting about it.", "tokens": [300, 366, 1880, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.08758432776839645, "compression_ratio": 1.7053571428571428, "no_speech_prob": 8.013228580239229e-06}, {"id": 1952, "seek": 611296, "start": 6139.28, "end": 6142.08, "text": " So what we do is while we're training,", "tokens": [407, 437, 321, 360, 307, 1339, 321, 434, 3097, 11], "temperature": 0.0, "avg_logprob": -0.08758432776839645, "compression_ratio": 1.7053571428571428, "no_speech_prob": 8.013228580239229e-06}, {"id": 1953, "seek": 614208, "start": 6142.08, "end": 6147.08, "text": " we keep a exponentially weighted moving average", "tokens": [321, 1066, 257, 37330, 32807, 2684, 4274], "temperature": 0.0, "avg_logprob": -0.101015768760492, "compression_ratio": 1.9955357142857142, "no_speech_prob": 1.1842044841614552e-05}, {"id": 1954, "seek": 614208, "start": 6147.2, "end": 6149.48, "text": " of the means and the variances.", "tokens": [295, 264, 1355, 293, 264, 1374, 21518, 13], "temperature": 0.0, "avg_logprob": -0.101015768760492, "compression_ratio": 1.9955357142857142, "no_speech_prob": 1.1842044841614552e-05}, {"id": 1955, "seek": 614208, "start": 6149.48, "end": 6151.44, "text": " I'll talk more about what that means in a moment,", "tokens": [286, 603, 751, 544, 466, 437, 300, 1355, 294, 257, 1623, 11], "temperature": 0.0, "avg_logprob": -0.101015768760492, "compression_ratio": 1.9955357142857142, "no_speech_prob": 1.1842044841614552e-05}, {"id": 1956, "seek": 614208, "start": 6151.44, "end": 6153.36, "text": " but basically we've got the kind of the,", "tokens": [457, 1936, 321, 600, 658, 264, 733, 295, 264, 11], "temperature": 0.0, "avg_logprob": -0.101015768760492, "compression_ratio": 1.9955357142857142, "no_speech_prob": 1.1842044841614552e-05}, {"id": 1957, "seek": 614208, "start": 6153.36, "end": 6156.28, "text": " a running average of the last few batches means", "tokens": [257, 2614, 4274, 295, 264, 1036, 1326, 15245, 279, 1355], "temperature": 0.0, "avg_logprob": -0.101015768760492, "compression_ratio": 1.9955357142857142, "no_speech_prob": 1.1842044841614552e-05}, {"id": 1958, "seek": 614208, "start": 6156.28, "end": 6159.08, "text": " and a running average of the last few batches variances.", "tokens": [293, 257, 2614, 4274, 295, 264, 1036, 1326, 15245, 279, 1374, 21518, 13], "temperature": 0.0, "avg_logprob": -0.101015768760492, "compression_ratio": 1.9955357142857142, "no_speech_prob": 1.1842044841614552e-05}, {"id": 1959, "seek": 614208, "start": 6159.08, "end": 6162.04, "text": " And so then when we're not training,", "tokens": [400, 370, 550, 562, 321, 434, 406, 3097, 11], "temperature": 0.0, "avg_logprob": -0.101015768760492, "compression_ratio": 1.9955357142857142, "no_speech_prob": 1.1842044841614552e-05}, {"id": 1960, "seek": 614208, "start": 6162.04, "end": 6164.36, "text": " in other words at inference time,", "tokens": [294, 661, 2283, 412, 38253, 565, 11], "temperature": 0.0, "avg_logprob": -0.101015768760492, "compression_ratio": 1.9955357142857142, "no_speech_prob": 1.1842044841614552e-05}, {"id": 1961, "seek": 614208, "start": 6164.36, "end": 6167.68, "text": " we don't use the mean and variance of this mini-batch,", "tokens": [321, 500, 380, 764, 264, 914, 293, 21977, 295, 341, 8382, 12, 65, 852, 11], "temperature": 0.0, "avg_logprob": -0.101015768760492, "compression_ratio": 1.9955357142857142, "no_speech_prob": 1.1842044841614552e-05}, {"id": 1962, "seek": 614208, "start": 6167.68, "end": 6170.64, "text": " we use that running average mean and variance", "tokens": [321, 764, 300, 2614, 4274, 914, 293, 21977], "temperature": 0.0, "avg_logprob": -0.101015768760492, "compression_ratio": 1.9955357142857142, "no_speech_prob": 1.1842044841614552e-05}, {"id": 1963, "seek": 617064, "start": 6170.64, "end": 6172.96, "text": " that we've been keeping track of, okay?", "tokens": [300, 321, 600, 668, 5145, 2837, 295, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.10507298479176531, "compression_ratio": 1.6576086956521738, "no_speech_prob": 3.2887010092963465e-06}, {"id": 1964, "seek": 617064, "start": 6175.08, "end": 6177.360000000001, "text": " So how do we calculate that running average?", "tokens": [407, 577, 360, 321, 8873, 300, 2614, 4274, 30], "temperature": 0.0, "avg_logprob": -0.10507298479176531, "compression_ratio": 1.6576086956521738, "no_speech_prob": 3.2887010092963465e-06}, {"id": 1965, "seek": 617064, "start": 6178.4400000000005, "end": 6183.4400000000005, "text": " Well, we don't just create something called self.vars,", "tokens": [1042, 11, 321, 500, 380, 445, 1884, 746, 1219, 2698, 13, 85, 685, 11], "temperature": 0.0, "avg_logprob": -0.10507298479176531, "compression_ratio": 1.6576086956521738, "no_speech_prob": 3.2887010092963465e-06}, {"id": 1966, "seek": 617064, "start": 6184.52, "end": 6187.4400000000005, "text": " we go self.registerbuffervars.", "tokens": [321, 352, 2698, 13, 3375, 1964, 65, 1245, 260, 85, 685, 13], "temperature": 0.0, "avg_logprob": -0.10507298479176531, "compression_ratio": 1.6576086956521738, "no_speech_prob": 3.2887010092963465e-06}, {"id": 1967, "seek": 617064, "start": 6187.4400000000005, "end": 6191.96, "text": " Now that creates something called self.vars.", "tokens": [823, 300, 7829, 746, 1219, 2698, 13, 85, 685, 13], "temperature": 0.0, "avg_logprob": -0.10507298479176531, "compression_ratio": 1.6576086956521738, "no_speech_prob": 3.2887010092963465e-06}, {"id": 1968, "seek": 617064, "start": 6191.96, "end": 6196.0, "text": " So why didn't we just say self.vars equals torch.ones?", "tokens": [407, 983, 994, 380, 321, 445, 584, 2698, 13, 85, 685, 6915, 27822, 13, 2213, 30], "temperature": 0.0, "avg_logprob": -0.10507298479176531, "compression_ratio": 1.6576086956521738, "no_speech_prob": 3.2887010092963465e-06}, {"id": 1969, "seek": 617064, "start": 6196.0, "end": 6198.8, "text": " Why do we say self.registerbuffer?", "tokens": [1545, 360, 321, 584, 2698, 13, 3375, 1964, 65, 1245, 260, 30], "temperature": 0.0, "avg_logprob": -0.10507298479176531, "compression_ratio": 1.6576086956521738, "no_speech_prob": 3.2887010092963465e-06}, {"id": 1970, "seek": 619880, "start": 6198.8, "end": 6201.68, "text": " It's almost exactly the same as saying self.vars", "tokens": [467, 311, 1920, 2293, 264, 912, 382, 1566, 2698, 13, 85, 685], "temperature": 0.0, "avg_logprob": -0.06720517205853835, "compression_ratio": 1.674911660777385, "no_speech_prob": 1.1125483069918118e-05}, {"id": 1971, "seek": 619880, "start": 6201.68, "end": 6205.88, "text": " equals torch.ones, but it does a couple of nice things.", "tokens": [6915, 27822, 13, 2213, 11, 457, 309, 775, 257, 1916, 295, 1481, 721, 13], "temperature": 0.0, "avg_logprob": -0.06720517205853835, "compression_ratio": 1.674911660777385, "no_speech_prob": 1.1125483069918118e-05}, {"id": 1972, "seek": 619880, "start": 6205.88, "end": 6210.76, "text": " The first is that if we move the model to the GPU,", "tokens": [440, 700, 307, 300, 498, 321, 1286, 264, 2316, 281, 264, 18407, 11], "temperature": 0.0, "avg_logprob": -0.06720517205853835, "compression_ratio": 1.674911660777385, "no_speech_prob": 1.1125483069918118e-05}, {"id": 1973, "seek": 619880, "start": 6210.76, "end": 6212.76, "text": " anything that's registered as a buffer", "tokens": [1340, 300, 311, 13968, 382, 257, 21762], "temperature": 0.0, "avg_logprob": -0.06720517205853835, "compression_ratio": 1.674911660777385, "no_speech_prob": 1.1125483069918118e-05}, {"id": 1974, "seek": 619880, "start": 6212.76, "end": 6215.400000000001, "text": " will be moved to the GPU as well, right?", "tokens": [486, 312, 4259, 281, 264, 18407, 382, 731, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.06720517205853835, "compression_ratio": 1.674911660777385, "no_speech_prob": 1.1125483069918118e-05}, {"id": 1975, "seek": 619880, "start": 6215.400000000001, "end": 6217.24, "text": " And if we didn't do that,", "tokens": [400, 498, 321, 994, 380, 360, 300, 11], "temperature": 0.0, "avg_logprob": -0.06720517205853835, "compression_ratio": 1.674911660777385, "no_speech_prob": 1.1125483069918118e-05}, {"id": 1976, "seek": 619880, "start": 6217.24, "end": 6219.64, "text": " then it's gonna try and do this calculation down here.", "tokens": [550, 309, 311, 799, 853, 293, 360, 341, 17108, 760, 510, 13], "temperature": 0.0, "avg_logprob": -0.06720517205853835, "compression_ratio": 1.674911660777385, "no_speech_prob": 1.1125483069918118e-05}, {"id": 1977, "seek": 619880, "start": 6219.64, "end": 6222.2, "text": " And if the vars and means aren't on the GPU,", "tokens": [400, 498, 264, 46130, 293, 1355, 3212, 380, 322, 264, 18407, 11], "temperature": 0.0, "avg_logprob": -0.06720517205853835, "compression_ratio": 1.674911660777385, "no_speech_prob": 1.1125483069918118e-05}, {"id": 1978, "seek": 619880, "start": 6222.2, "end": 6224.68, "text": " but everything else is on the GPU, we'll get an error.", "tokens": [457, 1203, 1646, 307, 322, 264, 18407, 11, 321, 603, 483, 364, 6713, 13], "temperature": 0.0, "avg_logprob": -0.06720517205853835, "compression_ratio": 1.674911660777385, "no_speech_prob": 1.1125483069918118e-05}, {"id": 1979, "seek": 619880, "start": 6224.68, "end": 6226.76, "text": " It'll say, oh, you're trying to add this thing on the CPU", "tokens": [467, 603, 584, 11, 1954, 11, 291, 434, 1382, 281, 909, 341, 551, 322, 264, 13199], "temperature": 0.0, "avg_logprob": -0.06720517205853835, "compression_ratio": 1.674911660777385, "no_speech_prob": 1.1125483069918118e-05}, {"id": 1980, "seek": 622676, "start": 6226.76, "end": 6229.280000000001, "text": " to this thing on the GPU, and it'll fail.", "tokens": [281, 341, 551, 322, 264, 18407, 11, 293, 309, 603, 3061, 13], "temperature": 0.0, "avg_logprob": -0.08023022015889486, "compression_ratio": 1.7439024390243902, "no_speech_prob": 6.0487900555017404e-06}, {"id": 1981, "seek": 622676, "start": 6229.280000000001, "end": 6231.360000000001, "text": " So that's one nice thing about registerbuffer.", "tokens": [407, 300, 311, 472, 1481, 551, 466, 7280, 65, 1245, 260, 13], "temperature": 0.0, "avg_logprob": -0.08023022015889486, "compression_ratio": 1.7439024390243902, "no_speech_prob": 6.0487900555017404e-06}, {"id": 1982, "seek": 622676, "start": 6231.360000000001, "end": 6234.360000000001, "text": " The other nice thing is that the variances and the means,", "tokens": [440, 661, 1481, 551, 307, 300, 264, 1374, 21518, 293, 264, 1355, 11], "temperature": 0.0, "avg_logprob": -0.08023022015889486, "compression_ratio": 1.7439024390243902, "no_speech_prob": 6.0487900555017404e-06}, {"id": 1983, "seek": 622676, "start": 6234.360000000001, "end": 6238.24, "text": " these running averages, they're part of the model, right?", "tokens": [613, 2614, 42257, 11, 436, 434, 644, 295, 264, 2316, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.08023022015889486, "compression_ratio": 1.7439024390243902, "no_speech_prob": 6.0487900555017404e-06}, {"id": 1984, "seek": 622676, "start": 6238.24, "end": 6242.6, "text": " When we do inference, in order to calculate our predictions,", "tokens": [1133, 321, 360, 38253, 11, 294, 1668, 281, 8873, 527, 21264, 11], "temperature": 0.0, "avg_logprob": -0.08023022015889486, "compression_ratio": 1.7439024390243902, "no_speech_prob": 6.0487900555017404e-06}, {"id": 1985, "seek": 622676, "start": 6242.6, "end": 6245.96, "text": " we actually need to know what those numbers are.", "tokens": [321, 767, 643, 281, 458, 437, 729, 3547, 366, 13], "temperature": 0.0, "avg_logprob": -0.08023022015889486, "compression_ratio": 1.7439024390243902, "no_speech_prob": 6.0487900555017404e-06}, {"id": 1986, "seek": 622676, "start": 6245.96, "end": 6247.68, "text": " So if we save the model,", "tokens": [407, 498, 321, 3155, 264, 2316, 11], "temperature": 0.0, "avg_logprob": -0.08023022015889486, "compression_ratio": 1.7439024390243902, "no_speech_prob": 6.0487900555017404e-06}, {"id": 1987, "seek": 622676, "start": 6247.68, "end": 6251.2, "text": " we have to save those variances and means.", "tokens": [321, 362, 281, 3155, 729, 1374, 21518, 293, 1355, 13], "temperature": 0.0, "avg_logprob": -0.08023022015889486, "compression_ratio": 1.7439024390243902, "no_speech_prob": 6.0487900555017404e-06}, {"id": 1988, "seek": 622676, "start": 6251.2, "end": 6255.360000000001, "text": " So registerbuffer also causes them to be saved", "tokens": [407, 7280, 65, 1245, 260, 611, 7700, 552, 281, 312, 6624], "temperature": 0.0, "avg_logprob": -0.08023022015889486, "compression_ratio": 1.7439024390243902, "no_speech_prob": 6.0487900555017404e-06}, {"id": 1989, "seek": 625536, "start": 6255.36, "end": 6257.04, "text": " along with everything else in the model.", "tokens": [2051, 365, 1203, 1646, 294, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.0876067275301032, "compression_ratio": 1.8146341463414635, "no_speech_prob": 5.255305040918756e-06}, {"id": 1990, "seek": 625536, "start": 6257.04, "end": 6258.679999999999, "text": " So that's what registerbuffer does.", "tokens": [407, 300, 311, 437, 7280, 65, 1245, 260, 775, 13], "temperature": 0.0, "avg_logprob": -0.0876067275301032, "compression_ratio": 1.8146341463414635, "no_speech_prob": 5.255305040918756e-06}, {"id": 1991, "seek": 625536, "start": 6259.5199999999995, "end": 6262.679999999999, "text": " So the variances, we start them out at ones.", "tokens": [407, 264, 1374, 21518, 11, 321, 722, 552, 484, 412, 2306, 13], "temperature": 0.0, "avg_logprob": -0.0876067275301032, "compression_ratio": 1.8146341463414635, "no_speech_prob": 5.255305040918756e-06}, {"id": 1992, "seek": 625536, "start": 6262.679999999999, "end": 6265.12, "text": " The means, we start them out at zeros.", "tokens": [440, 1355, 11, 321, 722, 552, 484, 412, 35193, 13], "temperature": 0.0, "avg_logprob": -0.0876067275301032, "compression_ratio": 1.8146341463414635, "no_speech_prob": 5.255305040918756e-06}, {"id": 1993, "seek": 625536, "start": 6265.12, "end": 6268.36, "text": " We then calculate the mean and variance of the minibatch,", "tokens": [492, 550, 8873, 264, 914, 293, 21977, 295, 264, 923, 897, 852, 11], "temperature": 0.0, "avg_logprob": -0.0876067275301032, "compression_ratio": 1.8146341463414635, "no_speech_prob": 5.255305040918756e-06}, {"id": 1994, "seek": 625536, "start": 6268.36, "end": 6273.36, "text": " and we average out the axes zero, two, and three.", "tokens": [293, 321, 4274, 484, 264, 35387, 4018, 11, 732, 11, 293, 1045, 13], "temperature": 0.0, "avg_logprob": -0.0876067275301032, "compression_ratio": 1.8146341463414635, "no_speech_prob": 5.255305040918756e-06}, {"id": 1995, "seek": 625536, "start": 6273.4, "end": 6275.96, "text": " So in other words, we average over all the batches,", "tokens": [407, 294, 661, 2283, 11, 321, 4274, 670, 439, 264, 15245, 279, 11], "temperature": 0.0, "avg_logprob": -0.0876067275301032, "compression_ratio": 1.8146341463414635, "no_speech_prob": 5.255305040918756e-06}, {"id": 1996, "seek": 625536, "start": 6275.96, "end": 6280.28, "text": " and we average over all of the X and Y coordinates.", "tokens": [293, 321, 4274, 670, 439, 295, 264, 1783, 293, 398, 21056, 13], "temperature": 0.0, "avg_logprob": -0.0876067275301032, "compression_ratio": 1.8146341463414635, "no_speech_prob": 5.255305040918756e-06}, {"id": 1997, "seek": 628028, "start": 6280.28, "end": 6285.28, "text": " So all we're left with is a mean for each channel,", "tokens": [407, 439, 321, 434, 1411, 365, 307, 257, 914, 337, 1184, 2269, 11], "temperature": 0.0, "avg_logprob": -0.3926885555952023, "compression_ratio": 1.430939226519337, "no_speech_prob": 9.08033052837709e-06}, {"id": 1998, "seek": 628028, "start": 6285.28, "end": 6288.28, "text": " or a mean for each filter, right?", "tokens": [420, 257, 914, 337, 1184, 6608, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.3926885555952023, "compression_ratio": 1.430939226519337, "no_speech_prob": 9.08033052837709e-06}, {"id": 1999, "seek": 628028, "start": 6289.5599999999995, "end": 6293.679999999999, "text": " KeepDimEqual's true means that it's gonna leave", "tokens": [5527, 35, 332, 36, 22345, 311, 2074, 1355, 300, 309, 311, 799, 1856], "temperature": 0.0, "avg_logprob": -0.3926885555952023, "compression_ratio": 1.430939226519337, "no_speech_prob": 9.08033052837709e-06}, {"id": 2000, "seek": 628028, "start": 6293.679999999999, "end": 6298.08, "text": " an empty unit axis in positions zero, two, and three,", "tokens": [364, 6707, 4985, 10298, 294, 8432, 4018, 11, 732, 11, 293, 1045, 11], "temperature": 0.0, "avg_logprob": -0.3926885555952023, "compression_ratio": 1.430939226519337, "no_speech_prob": 9.08033052837709e-06}, {"id": 2001, "seek": 628028, "start": 6298.08, "end": 6300.08, "text": " so it'll still broadcast nicely.", "tokens": [370, 309, 603, 920, 9975, 9594, 13], "temperature": 0.0, "avg_logprob": -0.3926885555952023, "compression_ratio": 1.430939226519337, "no_speech_prob": 9.08033052837709e-06}, {"id": 2002, "seek": 628028, "start": 6301.36, "end": 6305.12, "text": " So now we wanna take a running average.", "tokens": [407, 586, 321, 1948, 747, 257, 2614, 4274, 13], "temperature": 0.0, "avg_logprob": -0.3926885555952023, "compression_ratio": 1.430939226519337, "no_speech_prob": 9.08033052837709e-06}, {"id": 2003, "seek": 630512, "start": 6305.12, "end": 6309.2, "text": " A running average, so normally,", "tokens": [316, 2614, 4274, 11, 370, 5646, 11], "temperature": 0.0, "avg_logprob": -0.1527909924907069, "compression_ratio": 2.034313725490196, "no_speech_prob": 2.7693579340848373e-06}, {"id": 2004, "seek": 630512, "start": 6309.2, "end": 6311.64, "text": " if we wanna take a moving average, right?", "tokens": [498, 321, 1948, 747, 257, 2684, 4274, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1527909924907069, "compression_ratio": 2.034313725490196, "no_speech_prob": 2.7693579340848373e-06}, {"id": 2005, "seek": 630512, "start": 6311.64, "end": 6316.5199999999995, "text": " If we've got like a bunch of data points, right?", "tokens": [759, 321, 600, 658, 411, 257, 3840, 295, 1412, 2793, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1527909924907069, "compression_ratio": 2.034313725490196, "no_speech_prob": 2.7693579340848373e-06}, {"id": 2006, "seek": 630512, "start": 6316.5199999999995, "end": 6317.599999999999, "text": " We want a moving average,", "tokens": [492, 528, 257, 2684, 4274, 11], "temperature": 0.0, "avg_logprob": -0.1527909924907069, "compression_ratio": 2.034313725490196, "no_speech_prob": 2.7693579340848373e-06}, {"id": 2007, "seek": 630512, "start": 6317.599999999999, "end": 6321.16, "text": " we would kind of like grab five at a time,", "tokens": [321, 576, 733, 295, 411, 4444, 1732, 412, 257, 565, 11], "temperature": 0.0, "avg_logprob": -0.1527909924907069, "compression_ratio": 2.034313725490196, "no_speech_prob": 2.7693579340848373e-06}, {"id": 2008, "seek": 630512, "start": 6321.16, "end": 6322.88, "text": " and we would like take their moving average,", "tokens": [293, 321, 576, 411, 747, 641, 2684, 4274, 11], "temperature": 0.0, "avg_logprob": -0.1527909924907069, "compression_ratio": 2.034313725490196, "no_speech_prob": 2.7693579340848373e-06}, {"id": 2009, "seek": 630512, "start": 6322.88, "end": 6324.64, "text": " take the average of those five,", "tokens": [747, 264, 4274, 295, 729, 1732, 11], "temperature": 0.0, "avg_logprob": -0.1527909924907069, "compression_ratio": 2.034313725490196, "no_speech_prob": 2.7693579340848373e-06}, {"id": 2010, "seek": 630512, "start": 6324.64, "end": 6326.12, "text": " and then we take the next five,", "tokens": [293, 550, 321, 747, 264, 958, 1732, 11], "temperature": 0.0, "avg_logprob": -0.1527909924907069, "compression_ratio": 2.034313725490196, "no_speech_prob": 2.7693579340848373e-06}, {"id": 2011, "seek": 630512, "start": 6326.12, "end": 6327.5599999999995, "text": " and we'd like take their average,", "tokens": [293, 321, 1116, 411, 747, 641, 4274, 11], "temperature": 0.0, "avg_logprob": -0.1527909924907069, "compression_ratio": 2.034313725490196, "no_speech_prob": 2.7693579340848373e-06}, {"id": 2012, "seek": 630512, "start": 6327.5599999999995, "end": 6330.16, "text": " and we keep doing that like a few at a time.", "tokens": [293, 321, 1066, 884, 300, 411, 257, 1326, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.1527909924907069, "compression_ratio": 2.034313725490196, "no_speech_prob": 2.7693579340848373e-06}, {"id": 2013, "seek": 630512, "start": 6332.16, "end": 6334.08, "text": " We don't wanna do that here though,", "tokens": [492, 500, 380, 1948, 360, 300, 510, 1673, 11], "temperature": 0.0, "avg_logprob": -0.1527909924907069, "compression_ratio": 2.034313725490196, "no_speech_prob": 2.7693579340848373e-06}, {"id": 2014, "seek": 633408, "start": 6334.08, "end": 6338.36, "text": " because these batch norm statistics,", "tokens": [570, 613, 15245, 2026, 12523, 11], "temperature": 0.0, "avg_logprob": -0.08852393501683285, "compression_ratio": 1.5732758620689655, "no_speech_prob": 7.296269814105472e-06}, {"id": 2015, "seek": 633408, "start": 6338.36, "end": 6342.5199999999995, "text": " every single activation has one, so it's giant, right?", "tokens": [633, 2167, 24433, 575, 472, 11, 370, 309, 311, 7410, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.08852393501683285, "compression_ratio": 1.5732758620689655, "no_speech_prob": 7.296269814105472e-06}, {"id": 2016, "seek": 633408, "start": 6342.5199999999995, "end": 6347.5199999999995, "text": " Like models can have hundreds of millions of activations.", "tokens": [1743, 5245, 393, 362, 6779, 295, 6803, 295, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.08852393501683285, "compression_ratio": 1.5732758620689655, "no_speech_prob": 7.296269814105472e-06}, {"id": 2017, "seek": 633408, "start": 6347.72, "end": 6349.96, "text": " We don't wanna have to save a whole history", "tokens": [492, 500, 380, 1948, 362, 281, 3155, 257, 1379, 2503], "temperature": 0.0, "avg_logprob": -0.08852393501683285, "compression_ratio": 1.5732758620689655, "no_speech_prob": 7.296269814105472e-06}, {"id": 2018, "seek": 633408, "start": 6349.96, "end": 6351.8, "text": " of every single one of those", "tokens": [295, 633, 2167, 472, 295, 729], "temperature": 0.0, "avg_logprob": -0.08852393501683285, "compression_ratio": 1.5732758620689655, "no_speech_prob": 7.296269814105472e-06}, {"id": 2019, "seek": 633408, "start": 6351.8, "end": 6353.84, "text": " just so that we can calculate an average.", "tokens": [445, 370, 300, 321, 393, 8873, 364, 4274, 13], "temperature": 0.0, "avg_logprob": -0.08852393501683285, "compression_ratio": 1.5732758620689655, "no_speech_prob": 7.296269814105472e-06}, {"id": 2020, "seek": 633408, "start": 6354.76, "end": 6357.92, "text": " So there's a handy trick for this,", "tokens": [407, 456, 311, 257, 13239, 4282, 337, 341, 11], "temperature": 0.0, "avg_logprob": -0.08852393501683285, "compression_ratio": 1.5732758620689655, "no_speech_prob": 7.296269814105472e-06}, {"id": 2021, "seek": 633408, "start": 6357.92, "end": 6359.88, "text": " which is instead to use", "tokens": [597, 307, 2602, 281, 764], "temperature": 0.0, "avg_logprob": -0.08852393501683285, "compression_ratio": 1.5732758620689655, "no_speech_prob": 7.296269814105472e-06}, {"id": 2022, "seek": 633408, "start": 6359.88, "end": 6362.64, "text": " an exponentially weighted moving average.", "tokens": [364, 37330, 32807, 2684, 4274, 13], "temperature": 0.0, "avg_logprob": -0.08852393501683285, "compression_ratio": 1.5732758620689655, "no_speech_prob": 7.296269814105472e-06}, {"id": 2023, "seek": 636264, "start": 6362.64, "end": 6365.76, "text": " And basically what we do is we start out with this first point,", "tokens": [400, 1936, 437, 321, 360, 307, 321, 722, 484, 365, 341, 700, 935, 11], "temperature": 0.0, "avg_logprob": -0.13480691909790038, "compression_ratio": 1.6621004566210045, "no_speech_prob": 1.7777994798962027e-05}, {"id": 2024, "seek": 636264, "start": 6367.68, "end": 6372.68, "text": " and we say, okay, our first average is just the first point.", "tokens": [293, 321, 584, 11, 1392, 11, 527, 700, 4274, 307, 445, 264, 700, 935, 13], "temperature": 0.0, "avg_logprob": -0.13480691909790038, "compression_ratio": 1.6621004566210045, "no_speech_prob": 1.7777994798962027e-05}, {"id": 2025, "seek": 636264, "start": 6372.72, "end": 6375.96, "text": " Okay, so let's say, I don't know, that's three, right?", "tokens": [1033, 11, 370, 718, 311, 584, 11, 286, 500, 380, 458, 11, 300, 311, 1045, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13480691909790038, "compression_ratio": 1.6621004566210045, "no_speech_prob": 1.7777994798962027e-05}, {"id": 2026, "seek": 636264, "start": 6375.96, "end": 6379.08, "text": " And then the second point is five,", "tokens": [400, 550, 264, 1150, 935, 307, 1732, 11], "temperature": 0.0, "avg_logprob": -0.13480691909790038, "compression_ratio": 1.6621004566210045, "no_speech_prob": 1.7777994798962027e-05}, {"id": 2027, "seek": 636264, "start": 6379.08, "end": 6380.8, "text": " and what we do is we,", "tokens": [293, 437, 321, 360, 307, 321, 11], "temperature": 0.0, "avg_logprob": -0.13480691909790038, "compression_ratio": 1.6621004566210045, "no_speech_prob": 1.7777994798962027e-05}, {"id": 2028, "seek": 636264, "start": 6380.8, "end": 6383.64, "text": " to take an exponentially weighted moving average,", "tokens": [281, 747, 364, 37330, 32807, 2684, 4274, 11], "temperature": 0.0, "avg_logprob": -0.13480691909790038, "compression_ratio": 1.6621004566210045, "no_speech_prob": 1.7777994798962027e-05}, {"id": 2029, "seek": 636264, "start": 6383.64, "end": 6385.4800000000005, "text": " we first of all need some number,", "tokens": [321, 700, 295, 439, 643, 512, 1230, 11], "temperature": 0.0, "avg_logprob": -0.13480691909790038, "compression_ratio": 1.6621004566210045, "no_speech_prob": 1.7777994798962027e-05}, {"id": 2030, "seek": 636264, "start": 6385.4800000000005, "end": 6389.280000000001, "text": " which we call momentum, let's say it's 0.9.", "tokens": [597, 321, 818, 11244, 11, 718, 311, 584, 309, 311, 1958, 13, 24, 13], "temperature": 0.0, "avg_logprob": -0.13480691909790038, "compression_ratio": 1.6621004566210045, "no_speech_prob": 1.7777994798962027e-05}, {"id": 2031, "seek": 638928, "start": 6389.28, "end": 6392.2, "text": " So for the second value, so for the first value,", "tokens": [407, 337, 264, 1150, 2158, 11, 370, 337, 264, 700, 2158, 11], "temperature": 0.0, "avg_logprob": -0.22828116240324797, "compression_ratio": 1.7621951219512195, "no_speech_prob": 2.8572655992320506e-06}, {"id": 2032, "seek": 638928, "start": 6392.2, "end": 6394.96, "text": " our exponentially weighted moving average,", "tokens": [527, 37330, 32807, 2684, 4274, 11], "temperature": 0.0, "avg_logprob": -0.22828116240324797, "compression_ratio": 1.7621951219512195, "no_speech_prob": 2.8572655992320506e-06}, {"id": 2033, "seek": 638928, "start": 6394.96, "end": 6398.0, "text": " which we'll call mu, equals three.", "tokens": [597, 321, 603, 818, 2992, 11, 6915, 1045, 13], "temperature": 0.0, "avg_logprob": -0.22828116240324797, "compression_ratio": 1.7621951219512195, "no_speech_prob": 2.8572655992320506e-06}, {"id": 2034, "seek": 638928, "start": 6398.0, "end": 6399.5199999999995, "text": " And then for the second one,", "tokens": [400, 550, 337, 264, 1150, 472, 11], "temperature": 0.0, "avg_logprob": -0.22828116240324797, "compression_ratio": 1.7621951219512195, "no_speech_prob": 2.8572655992320506e-06}, {"id": 2035, "seek": 638928, "start": 6402.2, "end": 6407.2, "text": " we take mu one, we multiply it by our momentum,", "tokens": [321, 747, 2992, 472, 11, 321, 12972, 309, 538, 527, 11244, 11], "temperature": 0.0, "avg_logprob": -0.22828116240324797, "compression_ratio": 1.7621951219512195, "no_speech_prob": 2.8572655992320506e-06}, {"id": 2036, "seek": 638928, "start": 6409.4, "end": 6413.639999999999, "text": " and then we add our second value, five,", "tokens": [293, 550, 321, 909, 527, 1150, 2158, 11, 1732, 11], "temperature": 0.0, "avg_logprob": -0.22828116240324797, "compression_ratio": 1.7621951219512195, "no_speech_prob": 2.8572655992320506e-06}, {"id": 2037, "seek": 641364, "start": 6413.64, "end": 6419.360000000001, "text": " and we multiply it by one minus our momentum.", "tokens": [293, 321, 12972, 309, 538, 472, 3175, 527, 11244, 13], "temperature": 0.0, "avg_logprob": -0.2076514370470162, "compression_ratio": 1.4943181818181819, "no_speech_prob": 3.500786760923802e-06}, {"id": 2038, "seek": 641364, "start": 6419.360000000001, "end": 6423.240000000001, "text": " So in other words, it's mainly whatever it used to be before,", "tokens": [407, 294, 661, 2283, 11, 309, 311, 8704, 2035, 309, 1143, 281, 312, 949, 11], "temperature": 0.0, "avg_logprob": -0.2076514370470162, "compression_ratio": 1.4943181818181819, "no_speech_prob": 3.500786760923802e-06}, {"id": 2039, "seek": 641364, "start": 6423.240000000001, "end": 6424.76, "text": " plus a little bit of the new thing.", "tokens": [1804, 257, 707, 857, 295, 264, 777, 551, 13], "temperature": 0.0, "avg_logprob": -0.2076514370470162, "compression_ratio": 1.4943181818181819, "no_speech_prob": 3.500786760923802e-06}, {"id": 2040, "seek": 641364, "start": 6425.76, "end": 6430.76, "text": " And then mu two, sorry, mu three equals mu two times 0.9,", "tokens": [400, 550, 2992, 732, 11, 2597, 11, 2992, 1045, 6915, 2992, 732, 1413, 1958, 13, 24, 11], "temperature": 0.0, "avg_logprob": -0.2076514370470162, "compression_ratio": 1.4943181818181819, "no_speech_prob": 3.500786760923802e-06}, {"id": 2041, "seek": 641364, "start": 6433.88, "end": 6436.4800000000005, "text": " plus, and maybe this one here is four,", "tokens": [1804, 11, 293, 1310, 341, 472, 510, 307, 1451, 11], "temperature": 0.0, "avg_logprob": -0.2076514370470162, "compression_ratio": 1.4943181818181819, "no_speech_prob": 3.500786760923802e-06}, {"id": 2042, "seek": 641364, "start": 6437.56, "end": 6441.12, "text": " the new one times 0.1.", "tokens": [264, 777, 472, 1413, 1958, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.2076514370470162, "compression_ratio": 1.4943181818181819, "no_speech_prob": 3.500786760923802e-06}, {"id": 2043, "seek": 644112, "start": 6441.12, "end": 6444.16, "text": " So we're basically continuing to say,", "tokens": [407, 321, 434, 1936, 9289, 281, 584, 11], "temperature": 0.0, "avg_logprob": -0.1483344841003418, "compression_ratio": 1.625, "no_speech_prob": 9.817231330089271e-06}, {"id": 2044, "seek": 644112, "start": 6444.16, "end": 6448.0, "text": " it's mainly the thing before, plus a little bit of the new one.", "tokens": [309, 311, 8704, 264, 551, 949, 11, 1804, 257, 707, 857, 295, 264, 777, 472, 13], "temperature": 0.0, "avg_logprob": -0.1483344841003418, "compression_ratio": 1.625, "no_speech_prob": 9.817231330089271e-06}, {"id": 2045, "seek": 644112, "start": 6448.0, "end": 6451.12, "text": " And so what you end up with is something where,", "tokens": [400, 370, 437, 291, 917, 493, 365, 307, 746, 689, 11], "temperature": 0.0, "avg_logprob": -0.1483344841003418, "compression_ratio": 1.625, "no_speech_prob": 9.817231330089271e-06}, {"id": 2046, "seek": 644112, "start": 6452.5599999999995, "end": 6454.16, "text": " like by the time we get to here,", "tokens": [411, 538, 264, 565, 321, 483, 281, 510, 11], "temperature": 0.0, "avg_logprob": -0.1483344841003418, "compression_ratio": 1.625, "no_speech_prob": 9.817231330089271e-06}, {"id": 2047, "seek": 644112, "start": 6455.5599999999995, "end": 6460.92, "text": " the amount of influence of each of the previous data points,", "tokens": [264, 2372, 295, 6503, 295, 1184, 295, 264, 3894, 1412, 2793, 11], "temperature": 0.0, "avg_logprob": -0.1483344841003418, "compression_ratio": 1.625, "no_speech_prob": 9.817231330089271e-06}, {"id": 2048, "seek": 644112, "start": 6462.08, "end": 6463.16, "text": " once you calculate it out,", "tokens": [1564, 291, 8873, 309, 484, 11], "temperature": 0.0, "avg_logprob": -0.1483344841003418, "compression_ratio": 1.625, "no_speech_prob": 9.817231330089271e-06}, {"id": 2049, "seek": 644112, "start": 6463.16, "end": 6466.08, "text": " it turns out to be exponentially decayed.", "tokens": [309, 4523, 484, 281, 312, 37330, 21039, 292, 13], "temperature": 0.0, "avg_logprob": -0.1483344841003418, "compression_ratio": 1.625, "no_speech_prob": 9.817231330089271e-06}, {"id": 2050, "seek": 644112, "start": 6466.08, "end": 6469.5599999999995, "text": " So it's a moving average with an exponential decay,", "tokens": [407, 309, 311, 257, 2684, 4274, 365, 364, 21510, 21039, 11], "temperature": 0.0, "avg_logprob": -0.1483344841003418, "compression_ratio": 1.625, "no_speech_prob": 9.817231330089271e-06}, {"id": 2051, "seek": 646956, "start": 6469.56, "end": 6474.400000000001, "text": " with a benefit that we only ever have to keep track of one value.", "tokens": [365, 257, 5121, 300, 321, 787, 1562, 362, 281, 1066, 2837, 295, 472, 2158, 13], "temperature": 0.0, "avg_logprob": -0.22630603173199823, "compression_ratio": 1.4642857142857142, "no_speech_prob": 2.812934326357208e-06}, {"id": 2052, "seek": 646956, "start": 6475.68, "end": 6478.400000000001, "text": " So that's what an exponentially weight of moving average is.", "tokens": [407, 300, 311, 437, 364, 37330, 3364, 295, 2684, 4274, 307, 13], "temperature": 0.0, "avg_logprob": -0.22630603173199823, "compression_ratio": 1.4642857142857142, "no_speech_prob": 2.812934326357208e-06}, {"id": 2053, "seek": 646956, "start": 6478.400000000001, "end": 6483.52, "text": " This thing we do here,", "tokens": [639, 551, 321, 360, 510, 11], "temperature": 0.0, "avg_logprob": -0.22630603173199823, "compression_ratio": 1.4642857142857142, "no_speech_prob": 2.812934326357208e-06}, {"id": 2054, "seek": 646956, "start": 6484.76, "end": 6488.04, "text": " where we basically say we've got some function,", "tokens": [689, 321, 1936, 584, 321, 600, 658, 512, 2445, 11], "temperature": 0.0, "avg_logprob": -0.22630603173199823, "compression_ratio": 1.4642857142857142, "no_speech_prob": 2.812934326357208e-06}, {"id": 2055, "seek": 646956, "start": 6488.04, "end": 6494.320000000001, "text": " where we say it's some previous value times 0.9,", "tokens": [689, 321, 584, 309, 311, 512, 3894, 2158, 1413, 1958, 13, 24, 11], "temperature": 0.0, "avg_logprob": -0.22630603173199823, "compression_ratio": 1.4642857142857142, "no_speech_prob": 2.812934326357208e-06}, {"id": 2056, "seek": 649432, "start": 6494.32, "end": 6503.32, "text": " say, plus some other value times one minus that thing.", "tokens": [584, 11, 1804, 512, 661, 2158, 1413, 472, 3175, 300, 551, 13], "temperature": 0.0, "avg_logprob": -0.24765189858370049, "compression_ratio": 1.6453488372093024, "no_speech_prob": 4.425454790180083e-06}, {"id": 2057, "seek": 649432, "start": 6505.32, "end": 6506.84, "text": " This is called a linear interpolation.", "tokens": [639, 307, 1219, 257, 8213, 44902, 399, 13], "temperature": 0.0, "avg_logprob": -0.24765189858370049, "compression_ratio": 1.6453488372093024, "no_speech_prob": 4.425454790180083e-06}, {"id": 2058, "seek": 649432, "start": 6507.96, "end": 6510.04, "text": " That's a bit of this and a bit of this other thing,", "tokens": [663, 311, 257, 857, 295, 341, 293, 257, 857, 295, 341, 661, 551, 11], "temperature": 0.0, "avg_logprob": -0.24765189858370049, "compression_ratio": 1.6453488372093024, "no_speech_prob": 4.425454790180083e-06}, {"id": 2059, "seek": 649432, "start": 6510.04, "end": 6511.599999999999, "text": " and the two together make one.", "tokens": [293, 264, 732, 1214, 652, 472, 13], "temperature": 0.0, "avg_logprob": -0.24765189858370049, "compression_ratio": 1.6453488372093024, "no_speech_prob": 4.425454790180083e-06}, {"id": 2060, "seek": 649432, "start": 6512.599999999999, "end": 6516.16, "text": " Linear interpolation in PyTorch is spelled Lerp.", "tokens": [14670, 289, 44902, 399, 294, 9953, 51, 284, 339, 307, 34388, 441, 260, 79, 13], "temperature": 0.0, "avg_logprob": -0.24765189858370049, "compression_ratio": 1.6453488372093024, "no_speech_prob": 4.425454790180083e-06}, {"id": 2061, "seek": 651616, "start": 6516.16, "end": 6522.96, "text": " So we take the means and then we Lerp with our new mean", "tokens": [407, 321, 747, 264, 1355, 293, 550, 321, 441, 260, 79, 365, 527, 777, 914], "temperature": 0.0, "avg_logprob": -0.37537840138310974, "compression_ratio": 1.588235294117647, "no_speech_prob": 5.4221254686126485e-06}, {"id": 2062, "seek": 651616, "start": 6522.96, "end": 6525.76, "text": " using this amount of momentum.", "tokens": [1228, 341, 2372, 295, 11244, 13], "temperature": 0.0, "avg_logprob": -0.37537840138310974, "compression_ratio": 1.588235294117647, "no_speech_prob": 5.4221254686126485e-06}, {"id": 2063, "seek": 651616, "start": 6527.76, "end": 6535.96, "text": " Unfortunately, Lerp uses the exact opposite of the normal sense of momentum.", "tokens": [8590, 11, 441, 260, 79, 4960, 264, 1900, 6182, 295, 264, 2710, 2020, 295, 11244, 13], "temperature": 0.0, "avg_logprob": -0.37537840138310974, "compression_ratio": 1.588235294117647, "no_speech_prob": 5.4221254686126485e-06}, {"id": 2064, "seek": 651616, "start": 6535.96, "end": 6540.599999999999, "text": " So momentum of 0.1 in batch norm", "tokens": [407, 11244, 295, 1958, 13, 16, 294, 15245, 2026], "temperature": 0.0, "avg_logprob": -0.37537840138310974, "compression_ratio": 1.588235294117647, "no_speech_prob": 5.4221254686126485e-06}, {"id": 2065, "seek": 651616, "start": 6540.599999999999, "end": 6544.92, "text": " actually means momentum of 0.9 in normal mean.", "tokens": [767, 1355, 11244, 295, 1958, 13, 24, 294, 2710, 914, 13], "temperature": 0.0, "avg_logprob": -0.37537840138310974, "compression_ratio": 1.588235294117647, "no_speech_prob": 5.4221254686126485e-06}, {"id": 2066, "seek": 654492, "start": 6544.92, "end": 6546.84, "text": " In normal person speak.", "tokens": [682, 2710, 954, 1710, 13], "temperature": 0.0, "avg_logprob": -0.21386811284735652, "compression_ratio": 1.5739910313901346, "no_speech_prob": 3.169180126860738e-05}, {"id": 2067, "seek": 654492, "start": 6549.04, "end": 6553.64, "text": " So this is actually how nn.batchnorm works as well.", "tokens": [407, 341, 307, 767, 577, 297, 77, 13, 65, 852, 13403, 1985, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.21386811284735652, "compression_ratio": 1.5739910313901346, "no_speech_prob": 3.169180126860738e-05}, {"id": 2068, "seek": 654492, "start": 6553.64, "end": 6559.04, "text": " So when you see, so batch norm momentum is the opposite of what you would expect.", "tokens": [407, 562, 291, 536, 11, 370, 15245, 2026, 11244, 307, 264, 6182, 295, 437, 291, 576, 2066, 13], "temperature": 0.0, "avg_logprob": -0.21386811284735652, "compression_ratio": 1.5739910313901346, "no_speech_prob": 3.169180126860738e-05}, {"id": 2069, "seek": 654492, "start": 6559.04, "end": 6560.68, "text": " I wish they'd given it a different name.", "tokens": [286, 3172, 436, 1116, 2212, 309, 257, 819, 1315, 13], "temperature": 0.0, "avg_logprob": -0.21386811284735652, "compression_ratio": 1.5739910313901346, "no_speech_prob": 3.169180126860738e-05}, {"id": 2070, "seek": 654492, "start": 6560.68, "end": 6562.92, "text": " They didn't, sadly, so this is what we're stuck with.", "tokens": [814, 994, 380, 11, 22023, 11, 370, 341, 307, 437, 321, 434, 5541, 365, 13], "temperature": 0.0, "avg_logprob": -0.21386811284735652, "compression_ratio": 1.5739910313901346, "no_speech_prob": 3.169180126860738e-05}, {"id": 2071, "seek": 654492, "start": 6564.08, "end": 6566.76, "text": " So this is the running average means and standard deviations.", "tokens": [407, 341, 307, 264, 2614, 4274, 1355, 293, 3832, 31219, 763, 13], "temperature": 0.0, "avg_logprob": -0.21386811284735652, "compression_ratio": 1.5739910313901346, "no_speech_prob": 3.169180126860738e-05}, {"id": 2072, "seek": 654492, "start": 6568.24, "end": 6572.08, "text": " So now we can go ahead and use that.", "tokens": [407, 586, 321, 393, 352, 2286, 293, 764, 300, 13], "temperature": 0.0, "avg_logprob": -0.21386811284735652, "compression_ratio": 1.5739910313901346, "no_speech_prob": 3.169180126860738e-05}, {"id": 2073, "seek": 657208, "start": 6572.08, "end": 6576.24, "text": " So now we can create a new conv layer, which you can optionally say whether you", "tokens": [407, 586, 321, 393, 1884, 257, 777, 3754, 4583, 11, 597, 291, 393, 3614, 379, 584, 1968, 291], "temperature": 0.0, "avg_logprob": -0.1523265838623047, "compression_ratio": 1.774468085106383, "no_speech_prob": 1.8342399926041253e-05}, {"id": 2074, "seek": 657208, "start": 6576.24, "end": 6580.72, "text": " want batch norm, if you do, we append a batch norm layer.", "tokens": [528, 15245, 2026, 11, 498, 291, 360, 11, 321, 34116, 257, 15245, 2026, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1523265838623047, "compression_ratio": 1.774468085106383, "no_speech_prob": 1.8342399926041253e-05}, {"id": 2075, "seek": 657208, "start": 6580.72, "end": 6584.28, "text": " If we do append a batch norm layer, we remove the bias layer.", "tokens": [759, 321, 360, 34116, 257, 15245, 2026, 4583, 11, 321, 4159, 264, 12577, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1523265838623047, "compression_ratio": 1.774468085106383, "no_speech_prob": 1.8342399926041253e-05}, {"id": 2076, "seek": 657208, "start": 6584.28, "end": 6589.12, "text": " Because remember I said that the ads in batch norm just is a bias, right?", "tokens": [1436, 1604, 286, 848, 300, 264, 10342, 294, 15245, 2026, 445, 307, 257, 12577, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1523265838623047, "compression_ratio": 1.774468085106383, "no_speech_prob": 1.8342399926041253e-05}, {"id": 2077, "seek": 657208, "start": 6589.12, "end": 6591.72, "text": " So there's no point having a bias layer anymore.", "tokens": [407, 456, 311, 572, 935, 1419, 257, 12577, 4583, 3602, 13], "temperature": 0.0, "avg_logprob": -0.1523265838623047, "compression_ratio": 1.774468085106383, "no_speech_prob": 1.8342399926041253e-05}, {"id": 2078, "seek": 657208, "start": 6593.08, "end": 6596.2, "text": " So we'll remove the unnecessary bias layer.", "tokens": [407, 321, 603, 4159, 264, 19350, 12577, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1523265838623047, "compression_ratio": 1.774468085106383, "no_speech_prob": 1.8342399926041253e-05}, {"id": 2079, "seek": 657208, "start": 6596.2, "end": 6599.48, "text": " And so now we can go ahead and initialize our CNN.", "tokens": [400, 370, 586, 321, 393, 352, 2286, 293, 5883, 1125, 527, 24859, 13], "temperature": 0.0, "avg_logprob": -0.1523265838623047, "compression_ratio": 1.774468085106383, "no_speech_prob": 1.8342399926041253e-05}, {"id": 2080, "seek": 659948, "start": 6599.48, "end": 6605.08, "text": " This is a slightly more convenient initialization now that's actually", "tokens": [639, 307, 257, 4748, 544, 10851, 5883, 2144, 586, 300, 311, 767], "temperature": 0.0, "avg_logprob": -0.3150095056604456, "compression_ratio": 1.4866666666666666, "no_speech_prob": 8.397642886848189e-06}, {"id": 2081, "seek": 659948, "start": 6605.08, "end": 6612.36, "text": " gonna go in and recursively initialize every module inside our module.", "tokens": [799, 352, 294, 293, 20560, 3413, 5883, 1125, 633, 10088, 1854, 527, 10088, 13], "temperature": 0.0, "avg_logprob": -0.3150095056604456, "compression_ratio": 1.4866666666666666, "no_speech_prob": 8.397642886848189e-06}, {"id": 2082, "seek": 659948, "start": 6612.36, "end": 6618.04, "text": " The weights and the standard deviations.", "tokens": [440, 17443, 293, 264, 3832, 31219, 763, 13], "temperature": 0.0, "avg_logprob": -0.3150095056604456, "compression_ratio": 1.4866666666666666, "no_speech_prob": 8.397642886848189e-06}, {"id": 2083, "seek": 659948, "start": 6620.599999999999, "end": 6625.24, "text": " And then we will train it with our hooks.", "tokens": [400, 550, 321, 486, 3847, 309, 365, 527, 26485, 13], "temperature": 0.0, "avg_logprob": -0.3150095056604456, "compression_ratio": 1.4866666666666666, "no_speech_prob": 8.397642886848189e-06}, {"id": 2084, "seek": 662524, "start": 6625.24, "end": 6629.5199999999995, "text": " And you can see our mean starts at 0 exactly, and", "tokens": [400, 291, 393, 536, 527, 914, 3719, 412, 1958, 2293, 11, 293], "temperature": 0.0, "avg_logprob": -0.20223291955813014, "compression_ratio": 1.6367521367521367, "no_speech_prob": 5.255282758298563e-06}, {"id": 2085, "seek": 662524, "start": 6629.5199999999995, "end": 6632.5199999999995, "text": " our standard deviation starts at 1 exactly.", "tokens": [527, 3832, 25163, 3719, 412, 502, 2293, 13], "temperature": 0.0, "avg_logprob": -0.20223291955813014, "compression_ratio": 1.6367521367521367, "no_speech_prob": 5.255282758298563e-06}, {"id": 2086, "seek": 662524, "start": 6635.44, "end": 6642.4, "text": " So our training has entirely gotten rid of all of the exponential", "tokens": [407, 527, 3097, 575, 7696, 5768, 3973, 295, 439, 295, 264, 21510], "temperature": 0.0, "avg_logprob": -0.20223291955813014, "compression_ratio": 1.6367521367521367, "no_speech_prob": 5.255282758298563e-06}, {"id": 2087, "seek": 662524, "start": 6642.4, "end": 6645.719999999999, "text": " growth and sudden crash stuff that we had before.", "tokens": [4599, 293, 3990, 8252, 1507, 300, 321, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.20223291955813014, "compression_ratio": 1.6367521367521367, "no_speech_prob": 5.255282758298563e-06}, {"id": 2088, "seek": 662524, "start": 6646.76, "end": 6649.96, "text": " There's something interesting going on at the very end of training,", "tokens": [821, 311, 746, 1880, 516, 322, 412, 264, 588, 917, 295, 3097, 11], "temperature": 0.0, "avg_logprob": -0.20223291955813014, "compression_ratio": 1.6367521367521367, "no_speech_prob": 5.255282758298563e-06}, {"id": 2089, "seek": 662524, "start": 6649.96, "end": 6652.08, "text": " which I don't quite know what that is.", "tokens": [597, 286, 500, 380, 1596, 458, 437, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.20223291955813014, "compression_ratio": 1.6367521367521367, "no_speech_prob": 5.255282758298563e-06}, {"id": 2090, "seek": 665208, "start": 6652.08, "end": 6655.64, "text": " I mean, when I say the end of training, we've only done one epoch.", "tokens": [286, 914, 11, 562, 286, 584, 264, 917, 295, 3097, 11, 321, 600, 787, 1096, 472, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.17083460284817603, "compression_ratio": 1.5533980582524272, "no_speech_prob": 7.295483101188438e-06}, {"id": 2091, "seek": 665208, "start": 6655.64, "end": 6661.68, "text": " But this is looking a lot better than anything we've seen before.", "tokens": [583, 341, 307, 1237, 257, 688, 1101, 813, 1340, 321, 600, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.17083460284817603, "compression_ratio": 1.5533980582524272, "no_speech_prob": 7.295483101188438e-06}, {"id": 2092, "seek": 665208, "start": 6661.68, "end": 6664.68, "text": " I mean, that's just a very nice looking curve.", "tokens": [286, 914, 11, 300, 311, 445, 257, 588, 1481, 1237, 7605, 13], "temperature": 0.0, "avg_logprob": -0.17083460284817603, "compression_ratio": 1.5533980582524272, "no_speech_prob": 7.295483101188438e-06}, {"id": 2093, "seek": 665208, "start": 6664.68, "end": 6672.68, "text": " And so we're now able to get up to learning rates up to 1.", "tokens": [400, 370, 321, 434, 586, 1075, 281, 483, 493, 281, 2539, 6846, 493, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.17083460284817603, "compression_ratio": 1.5533980582524272, "no_speech_prob": 7.295483101188438e-06}, {"id": 2094, "seek": 665208, "start": 6672.68, "end": 6677.32, "text": " We've got 97% accuracy after just three epochs.", "tokens": [492, 600, 658, 23399, 4, 14170, 934, 445, 1045, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.17083460284817603, "compression_ratio": 1.5533980582524272, "no_speech_prob": 7.295483101188438e-06}, {"id": 2095, "seek": 665208, "start": 6677.32, "end": 6678.72, "text": " This is looking very encouraging.", "tokens": [639, 307, 1237, 588, 14580, 13], "temperature": 0.0, "avg_logprob": -0.17083460284817603, "compression_ratio": 1.5533980582524272, "no_speech_prob": 7.295483101188438e-06}, {"id": 2096, "seek": 667872, "start": 6678.72, "end": 6681.280000000001, "text": " So now that we've built our own batch norm,", "tokens": [407, 586, 300, 321, 600, 3094, 527, 1065, 15245, 2026, 11], "temperature": 0.0, "avg_logprob": -0.19753679207393102, "compression_ratio": 1.6460176991150441, "no_speech_prob": 1.0450702575326432e-05}, {"id": 2097, "seek": 667872, "start": 6681.280000000001, "end": 6684.76, "text": " we're allowed to use PyTorch's batch norm.", "tokens": [321, 434, 4350, 281, 764, 9953, 51, 284, 339, 311, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.19753679207393102, "compression_ratio": 1.6460176991150441, "no_speech_prob": 1.0450702575326432e-05}, {"id": 2098, "seek": 667872, "start": 6684.76, "end": 6686.96, "text": " And we get pretty much the same results.", "tokens": [400, 321, 483, 1238, 709, 264, 912, 3542, 13], "temperature": 0.0, "avg_logprob": -0.19753679207393102, "compression_ratio": 1.6460176991150441, "no_speech_prob": 1.0450702575326432e-05}, {"id": 2099, "seek": 667872, "start": 6686.96, "end": 6688.92, "text": " Sometimes it's 97, sometimes it's 98.", "tokens": [4803, 309, 311, 23399, 11, 2171, 309, 311, 20860, 13], "temperature": 0.0, "avg_logprob": -0.19753679207393102, "compression_ratio": 1.6460176991150441, "no_speech_prob": 1.0450702575326432e-05}, {"id": 2100, "seek": 667872, "start": 6688.92, "end": 6690.240000000001, "text": " This is just random variation.", "tokens": [639, 307, 445, 4974, 12990, 13], "temperature": 0.0, "avg_logprob": -0.19753679207393102, "compression_ratio": 1.6460176991150441, "no_speech_prob": 1.0450702575326432e-05}, {"id": 2101, "seek": 667872, "start": 6692.400000000001, "end": 6694.8, "text": " So now that we've got that, let's try going crazy.", "tokens": [407, 586, 300, 321, 600, 658, 300, 11, 718, 311, 853, 516, 3219, 13], "temperature": 0.0, "avg_logprob": -0.19753679207393102, "compression_ratio": 1.6460176991150441, "no_speech_prob": 1.0450702575326432e-05}, {"id": 2102, "seek": 667872, "start": 6694.8, "end": 6699.96, "text": " Let's try using our little one cycle learning scheduler we had.", "tokens": [961, 311, 853, 1228, 527, 707, 472, 6586, 2539, 12000, 260, 321, 632, 13], "temperature": 0.0, "avg_logprob": -0.19753679207393102, "compression_ratio": 1.6460176991150441, "no_speech_prob": 1.0450702575326432e-05}, {"id": 2103, "seek": 667872, "start": 6699.96, "end": 6704.8, "text": " And let's try and go all the way up to a learning rate of 2.", "tokens": [400, 718, 311, 853, 293, 352, 439, 264, 636, 493, 281, 257, 2539, 3314, 295, 568, 13], "temperature": 0.0, "avg_logprob": -0.19753679207393102, "compression_ratio": 1.6460176991150441, "no_speech_prob": 1.0450702575326432e-05}, {"id": 2104, "seek": 670480, "start": 6704.8, "end": 6710.52, "text": " And look at that, we totally can.", "tokens": [400, 574, 412, 300, 11, 321, 3879, 393, 13], "temperature": 0.0, "avg_logprob": -0.18100259417579287, "compression_ratio": 1.4328358208955223, "no_speech_prob": 2.058004611171782e-06}, {"id": 2105, "seek": 670480, "start": 6710.52, "end": 6715.08, "text": " And we're now up towards nearly 99% accuracy.", "tokens": [400, 321, 434, 586, 493, 3030, 6217, 11803, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.18100259417579287, "compression_ratio": 1.4328358208955223, "no_speech_prob": 2.058004611171782e-06}, {"id": 2106, "seek": 670480, "start": 6715.08, "end": 6718.2, "text": " So batch norm really is quite fantastic.", "tokens": [407, 15245, 2026, 534, 307, 1596, 5456, 13], "temperature": 0.0, "avg_logprob": -0.18100259417579287, "compression_ratio": 1.4328358208955223, "no_speech_prob": 2.058004611171782e-06}, {"id": 2107, "seek": 670480, "start": 6720.320000000001, "end": 6723.76, "text": " Batch norm has a bit of a problem though, which is that", "tokens": [363, 852, 2026, 575, 257, 857, 295, 257, 1154, 1673, 11, 597, 307, 300], "temperature": 0.0, "avg_logprob": -0.18100259417579287, "compression_ratio": 1.4328358208955223, "no_speech_prob": 2.058004611171782e-06}, {"id": 2108, "seek": 670480, "start": 6727.04, "end": 6730.400000000001, "text": " you can't apply it to what we call online learning tasks.", "tokens": [291, 393, 380, 3079, 309, 281, 437, 321, 818, 2950, 2539, 9608, 13], "temperature": 0.0, "avg_logprob": -0.18100259417579287, "compression_ratio": 1.4328358208955223, "no_speech_prob": 2.058004611171782e-06}, {"id": 2109, "seek": 670480, "start": 6730.400000000001, "end": 6733.88, "text": " In other words, if you have a batch size of 1, right?", "tokens": [682, 661, 2283, 11, 498, 291, 362, 257, 15245, 2744, 295, 502, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18100259417579287, "compression_ratio": 1.4328358208955223, "no_speech_prob": 2.058004611171782e-06}, {"id": 2110, "seek": 673388, "start": 6733.88, "end": 6737.24, "text": " So you're getting a single item at a time and", "tokens": [407, 291, 434, 1242, 257, 2167, 3174, 412, 257, 565, 293], "temperature": 0.0, "avg_logprob": -0.12905746036105686, "compression_ratio": 1.712, "no_speech_prob": 7.411043497995706e-06}, {"id": 2111, "seek": 673388, "start": 6737.24, "end": 6738.28, "text": " learning from that item.", "tokens": [2539, 490, 300, 3174, 13], "temperature": 0.0, "avg_logprob": -0.12905746036105686, "compression_ratio": 1.712, "no_speech_prob": 7.411043497995706e-06}, {"id": 2112, "seek": 673388, "start": 6739.400000000001, "end": 6741.76, "text": " What's the variance of that batch?", "tokens": [708, 311, 264, 21977, 295, 300, 15245, 30], "temperature": 0.0, "avg_logprob": -0.12905746036105686, "compression_ratio": 1.712, "no_speech_prob": 7.411043497995706e-06}, {"id": 2113, "seek": 673388, "start": 6741.76, "end": 6745.72, "text": " The variance of a batch of 1 is infinite, right?", "tokens": [440, 21977, 295, 257, 15245, 295, 502, 307, 13785, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12905746036105686, "compression_ratio": 1.712, "no_speech_prob": 7.411043497995706e-06}, {"id": 2114, "seek": 673388, "start": 6745.72, "end": 6748.76, "text": " So we can't use batch norm in that case.", "tokens": [407, 321, 393, 380, 764, 15245, 2026, 294, 300, 1389, 13], "temperature": 0.0, "avg_logprob": -0.12905746036105686, "compression_ratio": 1.712, "no_speech_prob": 7.411043497995706e-06}, {"id": 2115, "seek": 673388, "start": 6748.76, "end": 6751.96, "text": " Well, what if we're doing a segmentation task where we can", "tokens": [1042, 11, 437, 498, 321, 434, 884, 257, 9469, 399, 5633, 689, 321, 393], "temperature": 0.0, "avg_logprob": -0.12905746036105686, "compression_ratio": 1.712, "no_speech_prob": 7.411043497995706e-06}, {"id": 2116, "seek": 673388, "start": 6751.96, "end": 6753.92, "text": " only have a batch size of 2 or 4,", "tokens": [787, 362, 257, 15245, 2744, 295, 568, 420, 1017, 11], "temperature": 0.0, "avg_logprob": -0.12905746036105686, "compression_ratio": 1.712, "no_speech_prob": 7.411043497995706e-06}, {"id": 2117, "seek": 673388, "start": 6753.92, "end": 6756.8, "text": " which we've seen plenty of times in part 1?", "tokens": [597, 321, 600, 1612, 7140, 295, 1413, 294, 644, 502, 30], "temperature": 0.0, "avg_logprob": -0.12905746036105686, "compression_ratio": 1.712, "no_speech_prob": 7.411043497995706e-06}, {"id": 2118, "seek": 673388, "start": 6756.8, "end": 6757.84, "text": " That's gonna be a problem, right?", "tokens": [663, 311, 799, 312, 257, 1154, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12905746036105686, "compression_ratio": 1.712, "no_speech_prob": 7.411043497995706e-06}, {"id": 2119, "seek": 673388, "start": 6757.84, "end": 6760.8, "text": " Because across all of our layers, across all of our training,", "tokens": [1436, 2108, 439, 295, 527, 7914, 11, 2108, 439, 295, 527, 3097, 11], "temperature": 0.0, "avg_logprob": -0.12905746036105686, "compression_ratio": 1.712, "no_speech_prob": 7.411043497995706e-06}, {"id": 2120, "seek": 676080, "start": 6760.8, "end": 6765.68, "text": " across all of the channels, the batch size of 2, at some point,", "tokens": [2108, 439, 295, 264, 9235, 11, 264, 15245, 2744, 295, 568, 11, 412, 512, 935, 11], "temperature": 0.0, "avg_logprob": -0.1856040573120117, "compression_ratio": 1.5851528384279476, "no_speech_prob": 4.784958491654834e-06}, {"id": 2121, "seek": 676080, "start": 6765.68, "end": 6769.84, "text": " those two values are gonna be the same or nearly the same.", "tokens": [729, 732, 4190, 366, 799, 312, 264, 912, 420, 6217, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.1856040573120117, "compression_ratio": 1.5851528384279476, "no_speech_prob": 4.784958491654834e-06}, {"id": 2122, "seek": 676080, "start": 6769.84, "end": 6773.4800000000005, "text": " And so we then divide by that variance, which is about 0,", "tokens": [400, 370, 321, 550, 9845, 538, 300, 21977, 11, 597, 307, 466, 1958, 11], "temperature": 0.0, "avg_logprob": -0.1856040573120117, "compression_ratio": 1.5851528384279476, "no_speech_prob": 4.784958491654834e-06}, {"id": 2123, "seek": 676080, "start": 6773.4800000000005, "end": 6775.76, "text": " we have infinity, right?", "tokens": [321, 362, 13202, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1856040573120117, "compression_ratio": 1.5851528384279476, "no_speech_prob": 4.784958491654834e-06}, {"id": 2124, "seek": 676080, "start": 6775.76, "end": 6780.16, "text": " So we have this problem where anytime you have a small batch", "tokens": [407, 321, 362, 341, 1154, 689, 13038, 291, 362, 257, 1359, 15245], "temperature": 0.0, "avg_logprob": -0.1856040573120117, "compression_ratio": 1.5851528384279476, "no_speech_prob": 4.784958491654834e-06}, {"id": 2125, "seek": 676080, "start": 6780.16, "end": 6783.56, "text": " size, you're gonna get unstable or impossible training.", "tokens": [2744, 11, 291, 434, 799, 483, 23742, 420, 6243, 3097, 13], "temperature": 0.0, "avg_logprob": -0.1856040573120117, "compression_ratio": 1.5851528384279476, "no_speech_prob": 4.784958491654834e-06}, {"id": 2126, "seek": 676080, "start": 6784.88, "end": 6786.92, "text": " It's also gonna be really hard for RNNs.", "tokens": [467, 311, 611, 799, 312, 534, 1152, 337, 45702, 45, 82, 13], "temperature": 0.0, "avg_logprob": -0.1856040573120117, "compression_ratio": 1.5851528384279476, "no_speech_prob": 4.784958491654834e-06}, {"id": 2127, "seek": 678692, "start": 6786.92, "end": 6792.2, "text": " Because for RNNs, remember, it looks something like this, right?", "tokens": [1436, 337, 45702, 45, 82, 11, 1604, 11, 309, 1542, 746, 411, 341, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16045320561501833, "compression_ratio": 1.7488584474885844, "no_speech_prob": 1.892368345579598e-05}, {"id": 2128, "seek": 678692, "start": 6792.2, "end": 6794.52, "text": " We have this hidden state and", "tokens": [492, 362, 341, 7633, 1785, 293], "temperature": 0.0, "avg_logprob": -0.16045320561501833, "compression_ratio": 1.7488584474885844, "no_speech_prob": 1.892368345579598e-05}, {"id": 2129, "seek": 678692, "start": 6794.52, "end": 6798.8, "text": " we use the same weight matrix again and again and again, right?", "tokens": [321, 764, 264, 912, 3364, 8141, 797, 293, 797, 293, 797, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16045320561501833, "compression_ratio": 1.7488584474885844, "no_speech_prob": 1.892368345579598e-05}, {"id": 2130, "seek": 678692, "start": 6798.8, "end": 6802.36, "text": " Remember, we can unroll it and it looks like this.", "tokens": [5459, 11, 321, 393, 517, 3970, 309, 293, 309, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.16045320561501833, "compression_ratio": 1.7488584474885844, "no_speech_prob": 1.892368345579598e-05}, {"id": 2131, "seek": 678692, "start": 6803.76, "end": 6805.4800000000005, "text": " If you've forgotten, go back to lesson seven.", "tokens": [759, 291, 600, 11832, 11, 352, 646, 281, 6898, 3407, 13], "temperature": 0.0, "avg_logprob": -0.16045320561501833, "compression_ratio": 1.7488584474885844, "no_speech_prob": 1.892368345579598e-05}, {"id": 2132, "seek": 678692, "start": 6806.64, "end": 6810.0, "text": " And then we can even stack them together into two RNNs.", "tokens": [400, 550, 321, 393, 754, 8630, 552, 1214, 666, 732, 45702, 45, 82, 13], "temperature": 0.0, "avg_logprob": -0.16045320561501833, "compression_ratio": 1.7488584474885844, "no_speech_prob": 1.892368345579598e-05}, {"id": 2133, "seek": 678692, "start": 6810.0, "end": 6811.92, "text": " One RNN fits to another RNN.", "tokens": [1485, 45702, 45, 9001, 281, 1071, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.16045320561501833, "compression_ratio": 1.7488584474885844, "no_speech_prob": 1.892368345579598e-05}, {"id": 2134, "seek": 678692, "start": 6811.92, "end": 6815.36, "text": " And if we unroll that, it looks like this.", "tokens": [400, 498, 321, 517, 3970, 300, 11, 309, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.16045320561501833, "compression_ratio": 1.7488584474885844, "no_speech_prob": 1.892368345579598e-05}, {"id": 2135, "seek": 681536, "start": 6815.36, "end": 6820.32, "text": " And remember, these state time step to time step transitions,", "tokens": [400, 1604, 11, 613, 1785, 565, 1823, 281, 565, 1823, 23767, 11], "temperature": 0.0, "avg_logprob": -0.14935500496312193, "compression_ratio": 1.6172248803827751, "no_speech_prob": 4.637504844140494e-06}, {"id": 2136, "seek": 681536, "start": 6820.32, "end": 6825.32, "text": " if we're doing IMDB with a movie review with 2,000 words,", "tokens": [498, 321, 434, 884, 21463, 27735, 365, 257, 3169, 3131, 365, 568, 11, 1360, 2283, 11], "temperature": 0.0, "avg_logprob": -0.14935500496312193, "compression_ratio": 1.6172248803827751, "no_speech_prob": 4.637504844140494e-06}, {"id": 2137, "seek": 681536, "start": 6825.32, "end": 6826.5199999999995, "text": " there's 2,000 of these.", "tokens": [456, 311, 568, 11, 1360, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.14935500496312193, "compression_ratio": 1.6172248803827751, "no_speech_prob": 4.637504844140494e-06}, {"id": 2138, "seek": 681536, "start": 6828.08, "end": 6830.839999999999, "text": " And this is the same weight matrix each time.", "tokens": [400, 341, 307, 264, 912, 3364, 8141, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.14935500496312193, "compression_ratio": 1.6172248803827751, "no_speech_prob": 4.637504844140494e-06}, {"id": 2139, "seek": 681536, "start": 6830.839999999999, "end": 6833.5199999999995, "text": " And the number of these circles will vary.", "tokens": [400, 264, 1230, 295, 613, 13040, 486, 10559, 13], "temperature": 0.0, "avg_logprob": -0.14935500496312193, "compression_ratio": 1.6172248803827751, "no_speech_prob": 4.637504844140494e-06}, {"id": 2140, "seek": 681536, "start": 6833.5199999999995, "end": 6837.28, "text": " It's the number of time steps will vary from document to document.", "tokens": [467, 311, 264, 1230, 295, 565, 4439, 486, 10559, 490, 4166, 281, 4166, 13], "temperature": 0.0, "avg_logprob": -0.14935500496312193, "compression_ratio": 1.6172248803827751, "no_speech_prob": 4.637504844140494e-06}, {"id": 2141, "seek": 681536, "start": 6837.28, "end": 6840.24, "text": " So how would you do batch norm, right?", "tokens": [407, 577, 576, 291, 360, 15245, 2026, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14935500496312193, "compression_ratio": 1.6172248803827751, "no_speech_prob": 4.637504844140494e-06}, {"id": 2142, "seek": 684024, "start": 6840.24, "end": 6847.44, "text": " How would you say what's the running average of means and variances?", "tokens": [1012, 576, 291, 584, 437, 311, 264, 2614, 4274, 295, 1355, 293, 1374, 21518, 30], "temperature": 0.0, "avg_logprob": -0.14107349940708705, "compression_ratio": 1.622568093385214, "no_speech_prob": 4.784909378940938e-06}, {"id": 2143, "seek": 684024, "start": 6847.44, "end": 6851.76, "text": " Cuz you can't put a different one between each of these unrolled layers,", "tokens": [27017, 291, 393, 380, 829, 257, 819, 472, 1296, 1184, 295, 613, 517, 28850, 7914, 11], "temperature": 0.0, "avg_logprob": -0.14107349940708705, "compression_ratio": 1.622568093385214, "no_speech_prob": 4.784909378940938e-06}, {"id": 2144, "seek": 684024, "start": 6851.76, "end": 6853.84, "text": " because this is a for loop, remember?", "tokens": [570, 341, 307, 257, 337, 6367, 11, 1604, 30], "temperature": 0.0, "avg_logprob": -0.14107349940708705, "compression_ratio": 1.622568093385214, "no_speech_prob": 4.784909378940938e-06}, {"id": 2145, "seek": 684024, "start": 6853.84, "end": 6855.96, "text": " So we can't have different values every time.", "tokens": [407, 321, 393, 380, 362, 819, 4190, 633, 565, 13], "temperature": 0.0, "avg_logprob": -0.14107349940708705, "compression_ratio": 1.622568093385214, "no_speech_prob": 4.784909378940938e-06}, {"id": 2146, "seek": 684024, "start": 6857.24, "end": 6862.88, "text": " So it's not at all clear how you would insert batch norm into an RNN.", "tokens": [407, 309, 311, 406, 412, 439, 1850, 577, 291, 576, 8969, 15245, 2026, 666, 364, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.14107349940708705, "compression_ratio": 1.622568093385214, "no_speech_prob": 4.784909378940938e-06}, {"id": 2147, "seek": 684024, "start": 6862.88, "end": 6865.719999999999, "text": " So batch norm has these two deficiencies.", "tokens": [407, 15245, 2026, 575, 613, 732, 19248, 31294, 13], "temperature": 0.0, "avg_logprob": -0.14107349940708705, "compression_ratio": 1.622568093385214, "no_speech_prob": 4.784909378940938e-06}, {"id": 2148, "seek": 684024, "start": 6865.719999999999, "end": 6869.32, "text": " How do we handle very small batch sizes, all the way down to batch size of one?", "tokens": [1012, 360, 321, 4813, 588, 1359, 15245, 11602, 11, 439, 264, 636, 760, 281, 15245, 2744, 295, 472, 30], "temperature": 0.0, "avg_logprob": -0.14107349940708705, "compression_ratio": 1.622568093385214, "no_speech_prob": 4.784909378940938e-06}, {"id": 2149, "seek": 686932, "start": 6869.32, "end": 6870.4, "text": " How do we handle RNNs?", "tokens": [1012, 360, 321, 4813, 45702, 45, 82, 30], "temperature": 0.0, "avg_logprob": -0.336330136385831, "compression_ratio": 1.3918918918918919, "no_speech_prob": 1.5534019439655822e-06}, {"id": 2150, "seek": 686932, "start": 6872.5199999999995, "end": 6879.639999999999, "text": " So this paper called layer normalization suggests a solution to this.", "tokens": [407, 341, 3035, 1219, 4583, 2710, 2144, 13409, 257, 3827, 281, 341, 13], "temperature": 0.0, "avg_logprob": -0.336330136385831, "compression_ratio": 1.3918918918918919, "no_speech_prob": 1.5534019439655822e-06}, {"id": 2151, "seek": 686932, "start": 6880.759999999999, "end": 6888.12, "text": " And the layer normalization paper from Jimmy Barr and", "tokens": [400, 264, 4583, 2710, 2144, 3035, 490, 15709, 28694, 293], "temperature": 0.0, "avg_logprob": -0.336330136385831, "compression_ratio": 1.3918918918918919, "no_speech_prob": 1.5534019439655822e-06}, {"id": 2152, "seek": 686932, "start": 6888.12, "end": 6894.5199999999995, "text": " Kiros and Jeffrey Hinton who just won the Turing Award with", "tokens": [591, 21633, 293, 28721, 389, 12442, 567, 445, 1582, 264, 314, 1345, 13894, 365], "temperature": 0.0, "avg_logprob": -0.336330136385831, "compression_ratio": 1.3918918918918919, "no_speech_prob": 1.5534019439655822e-06}, {"id": 2153, "seek": 689452, "start": 6894.52, "end": 6899.320000000001, "text": " Joshua Benjio and Jan LeCun, which is kind of the Nobel Prize of Computer Science.", "tokens": [24005, 3964, 73, 1004, 293, 4956, 1456, 34, 409, 11, 597, 307, 733, 295, 264, 24611, 22604, 295, 22289, 8976, 13], "temperature": 0.0, "avg_logprob": -0.2876142324860563, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.095199604606023e-05}, {"id": 2154, "seek": 689452, "start": 6900.360000000001, "end": 6904.56, "text": " They created this paper, which like many papers, when you read it,", "tokens": [814, 2942, 341, 3035, 11, 597, 411, 867, 10577, 11, 562, 291, 1401, 309, 11], "temperature": 0.0, "avg_logprob": -0.2876142324860563, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.095199604606023e-05}, {"id": 2155, "seek": 689452, "start": 6904.56, "end": 6908.120000000001, "text": " it looks reasonably terrifying,", "tokens": [309, 1542, 23551, 18106, 11], "temperature": 0.0, "avg_logprob": -0.2876142324860563, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.095199604606023e-05}, {"id": 2156, "seek": 689452, "start": 6908.120000000001, "end": 6911.84, "text": " particularly once you start looking at all this stuff.", "tokens": [4098, 1564, 291, 722, 1237, 412, 439, 341, 1507, 13], "temperature": 0.0, "avg_logprob": -0.2876142324860563, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.095199604606023e-05}, {"id": 2157, "seek": 689452, "start": 6911.84, "end": 6919.56, "text": " But actually, when we take this paper and we convert it to code, it's this.", "tokens": [583, 767, 11, 562, 321, 747, 341, 3035, 293, 321, 7620, 309, 281, 3089, 11, 309, 311, 341, 13], "temperature": 0.0, "avg_logprob": -0.2876142324860563, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.095199604606023e-05}, {"id": 2158, "seek": 689452, "start": 6919.56, "end": 6922.240000000001, "text": " Now, we're just not to say the paper's garbage,", "tokens": [823, 11, 321, 434, 445, 406, 281, 584, 264, 3035, 311, 14150, 11], "temperature": 0.0, "avg_logprob": -0.2876142324860563, "compression_ratio": 1.5384615384615385, "no_speech_prob": 1.095199604606023e-05}, {"id": 2159, "seek": 692224, "start": 6922.24, "end": 6925.36, "text": " it's just that the paper has lots of explanation about what's going on and", "tokens": [309, 311, 445, 300, 264, 3035, 575, 3195, 295, 10835, 466, 437, 311, 516, 322, 293], "temperature": 0.0, "avg_logprob": -0.1995160041316863, "compression_ratio": 1.7892561983471074, "no_speech_prob": 2.046111876552459e-05}, {"id": 2160, "seek": 692224, "start": 6925.36, "end": 6927.8, "text": " what do we find out and what does that mean, right?", "tokens": [437, 360, 321, 915, 484, 293, 437, 775, 300, 914, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1995160041316863, "compression_ratio": 1.7892561983471074, "no_speech_prob": 2.046111876552459e-05}, {"id": 2161, "seek": 692224, "start": 6927.8, "end": 6931.5199999999995, "text": " But the actual what's layer norm, it's the same as batch norm.", "tokens": [583, 264, 3539, 437, 311, 4583, 2026, 11, 309, 311, 264, 912, 382, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.1995160041316863, "compression_ratio": 1.7892561983471074, "no_speech_prob": 2.046111876552459e-05}, {"id": 2162, "seek": 692224, "start": 6931.5199999999995, "end": 6937.8, "text": " But rather than saying x.mean 0, 2, 3, you say x.mean 1, 2, 3,", "tokens": [583, 2831, 813, 1566, 2031, 13, 1398, 282, 1958, 11, 568, 11, 805, 11, 291, 584, 2031, 13, 1398, 282, 502, 11, 568, 11, 805, 11], "temperature": 0.0, "avg_logprob": -0.1995160041316863, "compression_ratio": 1.7892561983471074, "no_speech_prob": 2.046111876552459e-05}, {"id": 2163, "seek": 692224, "start": 6937.8, "end": 6939.4, "text": " and you remove all the running averages.", "tokens": [293, 291, 4159, 439, 264, 2614, 42257, 13], "temperature": 0.0, "avg_logprob": -0.1995160041316863, "compression_ratio": 1.7892561983471074, "no_speech_prob": 2.046111876552459e-05}, {"id": 2164, "seek": 692224, "start": 6941.639999999999, "end": 6947.32, "text": " So this is layer norm, right, with none of that running average stuff.", "tokens": [407, 341, 307, 4583, 2026, 11, 558, 11, 365, 6022, 295, 300, 2614, 4274, 1507, 13], "temperature": 0.0, "avg_logprob": -0.1995160041316863, "compression_ratio": 1.7892561983471074, "no_speech_prob": 2.046111876552459e-05}, {"id": 2165, "seek": 692224, "start": 6948.4, "end": 6952.2, "text": " And the reason we don't need the running averages anymore is because", "tokens": [400, 264, 1778, 321, 500, 380, 643, 264, 2614, 42257, 3602, 307, 570], "temperature": 0.0, "avg_logprob": -0.1995160041316863, "compression_ratio": 1.7892561983471074, "no_speech_prob": 2.046111876552459e-05}, {"id": 2166, "seek": 695220, "start": 6952.2, "end": 6957.28, "text": " we're not taking the mean across all the items in the batch.", "tokens": [321, 434, 406, 1940, 264, 914, 2108, 439, 264, 4754, 294, 264, 15245, 13], "temperature": 0.0, "avg_logprob": -0.13160032867103494, "compression_ratio": 1.7549019607843137, "no_speech_prob": 1.2605541996890679e-05}, {"id": 2167, "seek": 695220, "start": 6957.28, "end": 6962.599999999999, "text": " Every image has its own mean, every image has its own standard deviation.", "tokens": [2048, 3256, 575, 1080, 1065, 914, 11, 633, 3256, 575, 1080, 1065, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.13160032867103494, "compression_ratio": 1.7549019607843137, "no_speech_prob": 1.2605541996890679e-05}, {"id": 2168, "seek": 695220, "start": 6962.599999999999, "end": 6967.72, "text": " So there's no concept of having to average across things in a batch, right?", "tokens": [407, 456, 311, 572, 3410, 295, 1419, 281, 4274, 2108, 721, 294, 257, 15245, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13160032867103494, "compression_ratio": 1.7549019607843137, "no_speech_prob": 1.2605541996890679e-05}, {"id": 2169, "seek": 695220, "start": 6967.72, "end": 6972.28, "text": " And so that's all layer norm is.", "tokens": [400, 370, 300, 311, 439, 4583, 2026, 307, 13], "temperature": 0.0, "avg_logprob": -0.13160032867103494, "compression_ratio": 1.7549019607843137, "no_speech_prob": 1.2605541996890679e-05}, {"id": 2170, "seek": 695220, "start": 6972.28, "end": 6974.639999999999, "text": " We also average over the channels.", "tokens": [492, 611, 4274, 670, 264, 9235, 13], "temperature": 0.0, "avg_logprob": -0.13160032867103494, "compression_ratio": 1.7549019607843137, "no_speech_prob": 1.2605541996890679e-05}, {"id": 2171, "seek": 695220, "start": 6974.639999999999, "end": 6980.12, "text": " So we average over the channels in the x and the y for each image individually.", "tokens": [407, 321, 4274, 670, 264, 9235, 294, 264, 2031, 293, 264, 288, 337, 1184, 3256, 16652, 13], "temperature": 0.0, "avg_logprob": -0.13160032867103494, "compression_ratio": 1.7549019607843137, "no_speech_prob": 1.2605541996890679e-05}, {"id": 2172, "seek": 698012, "start": 6980.12, "end": 6982.16, "text": " So we don't have to keep track of any running averages.", "tokens": [407, 321, 500, 380, 362, 281, 1066, 2837, 295, 604, 2614, 42257, 13], "temperature": 0.0, "avg_logprob": -0.17152023315429688, "compression_ratio": 1.6858407079646018, "no_speech_prob": 6.14402642895584e-06}, {"id": 2173, "seek": 698012, "start": 6983.36, "end": 6986.04, "text": " The problem is that when we do that and we train,", "tokens": [440, 1154, 307, 300, 562, 321, 360, 300, 293, 321, 3847, 11], "temperature": 0.0, "avg_logprob": -0.17152023315429688, "compression_ratio": 1.6858407079646018, "no_speech_prob": 6.14402642895584e-06}, {"id": 2174, "seek": 698012, "start": 6986.04, "end": 6990.64, "text": " even at a lower learning rate of 0.8, it doesn't work, right?", "tokens": [754, 412, 257, 3126, 2539, 3314, 295, 1958, 13, 23, 11, 309, 1177, 380, 589, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17152023315429688, "compression_ratio": 1.6858407079646018, "no_speech_prob": 6.14402642895584e-06}, {"id": 2175, "seek": 698012, "start": 6990.64, "end": 6991.88, "text": " Layer norm's not as good.", "tokens": [35166, 2026, 311, 406, 382, 665, 13], "temperature": 0.0, "avg_logprob": -0.17152023315429688, "compression_ratio": 1.6858407079646018, "no_speech_prob": 6.14402642895584e-06}, {"id": 2176, "seek": 698012, "start": 6993.5199999999995, "end": 6996.5599999999995, "text": " So it's a work around we can use, but", "tokens": [407, 309, 311, 257, 589, 926, 321, 393, 764, 11, 457], "temperature": 0.0, "avg_logprob": -0.17152023315429688, "compression_ratio": 1.6858407079646018, "no_speech_prob": 6.14402642895584e-06}, {"id": 2177, "seek": 698012, "start": 6996.5599999999995, "end": 7000.5599999999995, "text": " because we don't have the running averages at inference time, and", "tokens": [570, 321, 500, 380, 362, 264, 2614, 42257, 412, 38253, 565, 11, 293], "temperature": 0.0, "avg_logprob": -0.17152023315429688, "compression_ratio": 1.6858407079646018, "no_speech_prob": 6.14402642895584e-06}, {"id": 2178, "seek": 698012, "start": 7000.5599999999995, "end": 7008.68, "text": " more importantly, because we don't have a different normalization for each channel,", "tokens": [544, 8906, 11, 570, 321, 500, 380, 362, 257, 819, 2710, 2144, 337, 1184, 2269, 11], "temperature": 0.0, "avg_logprob": -0.17152023315429688, "compression_ratio": 1.6858407079646018, "no_speech_prob": 6.14402642895584e-06}, {"id": 2179, "seek": 700868, "start": 7008.68, "end": 7011.8, "text": " we're just throwing them all together and pretending they're the same and", "tokens": [321, 434, 445, 10238, 552, 439, 1214, 293, 22106, 436, 434, 264, 912, 293], "temperature": 0.0, "avg_logprob": -0.16211862479690956, "compression_ratio": 1.6186770428015564, "no_speech_prob": 6.143927294033347e-06}, {"id": 2180, "seek": 700868, "start": 7011.8, "end": 7012.84, "text": " they're not, right?", "tokens": [436, 434, 406, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16211862479690956, "compression_ratio": 1.6186770428015564, "no_speech_prob": 6.143927294033347e-06}, {"id": 2181, "seek": 700868, "start": 7012.84, "end": 7017.96, "text": " So layer norm helps, but it's nowhere near as good as batch norm, okay?", "tokens": [407, 4583, 2026, 3665, 11, 457, 309, 311, 11159, 2651, 382, 665, 382, 15245, 2026, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.16211862479690956, "compression_ratio": 1.6186770428015564, "no_speech_prob": 6.143927294033347e-06}, {"id": 2182, "seek": 700868, "start": 7017.96, "end": 7022.8, "text": " But for RNNs, it's kind of what you have to use is something like this.", "tokens": [583, 337, 45702, 45, 82, 11, 309, 311, 733, 295, 437, 291, 362, 281, 764, 307, 746, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.16211862479690956, "compression_ratio": 1.6186770428015564, "no_speech_prob": 6.143927294033347e-06}, {"id": 2183, "seek": 700868, "start": 7024.4400000000005, "end": 7025.52, "text": " So here's the thought experiment.", "tokens": [407, 510, 311, 264, 1194, 5120, 13], "temperature": 0.0, "avg_logprob": -0.16211862479690956, "compression_ratio": 1.6186770428015564, "no_speech_prob": 6.143927294033347e-06}, {"id": 2184, "seek": 700868, "start": 7026.88, "end": 7032.68, "text": " What if you're using layer norm on the very first, on the actual input data,", "tokens": [708, 498, 291, 434, 1228, 4583, 2026, 322, 264, 588, 700, 11, 322, 264, 3539, 4846, 1412, 11], "temperature": 0.0, "avg_logprob": -0.16211862479690956, "compression_ratio": 1.6186770428015564, "no_speech_prob": 6.143927294033347e-06}, {"id": 2185, "seek": 700868, "start": 7032.68, "end": 7035.84, "text": " and you're trying to distinguish between foggy days and sunny days?", "tokens": [293, 291, 434, 1382, 281, 20206, 1296, 13648, 1480, 1708, 293, 20412, 1708, 30], "temperature": 0.0, "avg_logprob": -0.16211862479690956, "compression_ratio": 1.6186770428015564, "no_speech_prob": 6.143927294033347e-06}, {"id": 2186, "seek": 703584, "start": 7035.84, "end": 7041.4400000000005, "text": " So foggy days will have less activations on average,", "tokens": [407, 13648, 1480, 1708, 486, 362, 1570, 2430, 763, 322, 4274, 11], "temperature": 0.0, "avg_logprob": -0.21884760042516196, "compression_ratio": 1.8235294117647058, "no_speech_prob": 1.6027862557166372e-06}, {"id": 2187, "seek": 703584, "start": 7041.4400000000005, "end": 7046.96, "text": " because they're less bright, and they will have less contrast.", "tokens": [570, 436, 434, 1570, 4730, 11, 293, 436, 486, 362, 1570, 8712, 13], "temperature": 0.0, "avg_logprob": -0.21884760042516196, "compression_ratio": 1.8235294117647058, "no_speech_prob": 1.6027862557166372e-06}, {"id": 2188, "seek": 703584, "start": 7046.96, "end": 7049.360000000001, "text": " In other words, they have lower variance.", "tokens": [682, 661, 2283, 11, 436, 362, 3126, 21977, 13], "temperature": 0.0, "avg_logprob": -0.21884760042516196, "compression_ratio": 1.8235294117647058, "no_speech_prob": 1.6027862557166372e-06}, {"id": 2189, "seek": 703584, "start": 7049.360000000001, "end": 7054.4800000000005, "text": " So layer norm would cause the variances to be normalized to be the same,", "tokens": [407, 4583, 2026, 576, 3082, 264, 1374, 21518, 281, 312, 48704, 281, 312, 264, 912, 11], "temperature": 0.0, "avg_logprob": -0.21884760042516196, "compression_ratio": 1.8235294117647058, "no_speech_prob": 1.6027862557166372e-06}, {"id": 2190, "seek": 703584, "start": 7055.72, "end": 7058.2, "text": " and the means to be normalized to be the same.", "tokens": [293, 264, 1355, 281, 312, 48704, 281, 312, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.21884760042516196, "compression_ratio": 1.8235294117647058, "no_speech_prob": 1.6027862557166372e-06}, {"id": 2191, "seek": 703584, "start": 7059.64, "end": 7061.96, "text": " So now the sunny day picture and", "tokens": [407, 586, 264, 20412, 786, 3036, 293], "temperature": 0.0, "avg_logprob": -0.21884760042516196, "compression_ratio": 1.8235294117647058, "no_speech_prob": 1.6027862557166372e-06}, {"id": 2192, "seek": 706196, "start": 7061.96, "end": 7067.6, "text": " the hazy day picture would have the same overall kind of activations and", "tokens": [264, 324, 1229, 786, 3036, 576, 362, 264, 912, 4787, 733, 295, 2430, 763, 293], "temperature": 0.0, "avg_logprob": -0.11635641897878339, "compression_ratio": 1.7962264150943397, "no_speech_prob": 2.7264088657830143e-06}, {"id": 2193, "seek": 706196, "start": 7067.6, "end": 7068.6, "text": " amount of contrast.", "tokens": [2372, 295, 8712, 13], "temperature": 0.0, "avg_logprob": -0.11635641897878339, "compression_ratio": 1.7962264150943397, "no_speech_prob": 2.7264088657830143e-06}, {"id": 2194, "seek": 706196, "start": 7068.6, "end": 7072.16, "text": " And so the answer to this question is, no, you couldn't.", "tokens": [400, 370, 264, 1867, 281, 341, 1168, 307, 11, 572, 11, 291, 2809, 380, 13], "temperature": 0.0, "avg_logprob": -0.11635641897878339, "compression_ratio": 1.7962264150943397, "no_speech_prob": 2.7264088657830143e-06}, {"id": 2195, "seek": 706196, "start": 7072.16, "end": 7075.32, "text": " With layer norm, you would literally not be able to tell the difference between", "tokens": [2022, 4583, 2026, 11, 291, 576, 3736, 406, 312, 1075, 281, 980, 264, 2649, 1296], "temperature": 0.0, "avg_logprob": -0.11635641897878339, "compression_ratio": 1.7962264150943397, "no_speech_prob": 2.7264088657830143e-06}, {"id": 2196, "seek": 706196, "start": 7075.32, "end": 7078.4800000000005, "text": " pictures of sunny days and pictures of foggy days.", "tokens": [5242, 295, 20412, 1708, 293, 5242, 295, 13648, 1480, 1708, 13], "temperature": 0.0, "avg_logprob": -0.11635641897878339, "compression_ratio": 1.7962264150943397, "no_speech_prob": 2.7264088657830143e-06}, {"id": 2197, "seek": 706196, "start": 7078.4800000000005, "end": 7082.4, "text": " Now, it's not only if you put the layer norm on the input data,", "tokens": [823, 11, 309, 311, 406, 787, 498, 291, 829, 264, 4583, 2026, 322, 264, 4846, 1412, 11], "temperature": 0.0, "avg_logprob": -0.11635641897878339, "compression_ratio": 1.7962264150943397, "no_speech_prob": 2.7264088657830143e-06}, {"id": 2198, "seek": 706196, "start": 7082.4, "end": 7086.8, "text": " which you wouldn't do, but everywhere in the middle layers, it's the same, right?", "tokens": [597, 291, 2759, 380, 360, 11, 457, 5315, 294, 264, 2808, 7914, 11, 309, 311, 264, 912, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11635641897878339, "compression_ratio": 1.7962264150943397, "no_speech_prob": 2.7264088657830143e-06}, {"id": 2199, "seek": 706196, "start": 7086.8, "end": 7089.64, "text": " Anywhere where the overall level of activation or", "tokens": [2639, 1992, 689, 264, 4787, 1496, 295, 24433, 420], "temperature": 0.0, "avg_logprob": -0.11635641897878339, "compression_ratio": 1.7962264150943397, "no_speech_prob": 2.7264088657830143e-06}, {"id": 2200, "seek": 708964, "start": 7089.64, "end": 7094.200000000001, "text": " the amount of difference of activation is something that is part of what you care", "tokens": [264, 2372, 295, 2649, 295, 24433, 307, 746, 300, 307, 644, 295, 437, 291, 1127], "temperature": 0.0, "avg_logprob": -0.19036956628163657, "compression_ratio": 1.7605042016806722, "no_speech_prob": 3.844832008326193e-06}, {"id": 2201, "seek": 708964, "start": 7094.200000000001, "end": 7097.320000000001, "text": " about, it throws it away.", "tokens": [466, 11, 309, 19251, 309, 1314, 13], "temperature": 0.0, "avg_logprob": -0.19036956628163657, "compression_ratio": 1.7605042016806722, "no_speech_prob": 3.844832008326193e-06}, {"id": 2202, "seek": 708964, "start": 7097.320000000001, "end": 7098.4400000000005, "text": " It's designed to throw it away.", "tokens": [467, 311, 4761, 281, 3507, 309, 1314, 13], "temperature": 0.0, "avg_logprob": -0.19036956628163657, "compression_ratio": 1.7605042016806722, "no_speech_prob": 3.844832008326193e-06}, {"id": 2203, "seek": 708964, "start": 7099.76, "end": 7104.240000000001, "text": " Furthermore, if your inference time is using things from kind of a different", "tokens": [23999, 11, 498, 428, 38253, 565, 307, 1228, 721, 490, 733, 295, 257, 819], "temperature": 0.0, "avg_logprob": -0.19036956628163657, "compression_ratio": 1.7605042016806722, "no_speech_prob": 3.844832008326193e-06}, {"id": 2204, "seek": 708964, "start": 7104.240000000001, "end": 7108.96, "text": " distribution where that different distribution is important, it throws that away.", "tokens": [7316, 689, 300, 819, 7316, 307, 1021, 11, 309, 19251, 300, 1314, 13], "temperature": 0.0, "avg_logprob": -0.19036956628163657, "compression_ratio": 1.7605042016806722, "no_speech_prob": 3.844832008326193e-06}, {"id": 2205, "seek": 708964, "start": 7108.96, "end": 7114.08, "text": " So layer norm's a partial hacky work around for some very genuine problems.", "tokens": [407, 4583, 2026, 311, 257, 14641, 10339, 88, 589, 926, 337, 512, 588, 16699, 2740, 13], "temperature": 0.0, "avg_logprob": -0.19036956628163657, "compression_ratio": 1.7605042016806722, "no_speech_prob": 3.844832008326193e-06}, {"id": 2206, "seek": 711408, "start": 7114.08, "end": 7118.4, "text": " There's also something called instance norm.", "tokens": [821, 311, 611, 746, 1219, 5197, 2026, 13], "temperature": 0.0, "avg_logprob": -0.19886020549292704, "compression_ratio": 1.9663461538461537, "no_speech_prob": 1.0451390153320972e-05}, {"id": 2207, "seek": 711408, "start": 7118.4, "end": 7123.88, "text": " And instance norm is basically the same thing as layer norm.", "tokens": [400, 5197, 2026, 307, 1936, 264, 912, 551, 382, 4583, 2026, 13], "temperature": 0.0, "avg_logprob": -0.19886020549292704, "compression_ratio": 1.9663461538461537, "no_speech_prob": 1.0451390153320972e-05}, {"id": 2208, "seek": 711408, "start": 7123.88, "end": 7126.96, "text": " It's a bit easier to read in the paper because they actually lay out all", "tokens": [467, 311, 257, 857, 3571, 281, 1401, 294, 264, 3035, 570, 436, 767, 2360, 484, 439], "temperature": 0.0, "avg_logprob": -0.19886020549292704, "compression_ratio": 1.9663461538461537, "no_speech_prob": 1.0451390153320972e-05}, {"id": 2209, "seek": 711408, "start": 7126.96, "end": 7128.2, "text": " the indexes.", "tokens": [264, 8186, 279, 13], "temperature": 0.0, "avg_logprob": -0.19886020549292704, "compression_ratio": 1.9663461538461537, "no_speech_prob": 1.0451390153320972e-05}, {"id": 2210, "seek": 711408, "start": 7128.2, "end": 7131.68, "text": " So a particular output for a particular batch, for a particular channel, for", "tokens": [407, 257, 1729, 5598, 337, 257, 1729, 15245, 11, 337, 257, 1729, 2269, 11, 337], "temperature": 0.0, "avg_logprob": -0.19886020549292704, "compression_ratio": 1.9663461538461537, "no_speech_prob": 1.0451390153320972e-05}, {"id": 2211, "seek": 711408, "start": 7131.68, "end": 7135.6, "text": " a particular x, for a particular y is equal to the input for that batch and", "tokens": [257, 1729, 2031, 11, 337, 257, 1729, 288, 307, 2681, 281, 264, 4846, 337, 300, 15245, 293], "temperature": 0.0, "avg_logprob": -0.19886020549292704, "compression_ratio": 1.9663461538461537, "no_speech_prob": 1.0451390153320972e-05}, {"id": 2212, "seek": 711408, "start": 7135.6, "end": 7142.0, "text": " channel in x and y minus the mean for the batch and the channel.", "tokens": [2269, 294, 2031, 293, 288, 3175, 264, 914, 337, 264, 15245, 293, 264, 2269, 13], "temperature": 0.0, "avg_logprob": -0.19886020549292704, "compression_ratio": 1.9663461538461537, "no_speech_prob": 1.0451390153320972e-05}, {"id": 2213, "seek": 714200, "start": 7142.0, "end": 7146.2, "text": " So in other words, it's the same as layer norm, but now it's mean 2,3,", "tokens": [407, 294, 661, 2283, 11, 309, 311, 264, 912, 382, 4583, 2026, 11, 457, 586, 309, 311, 914, 568, 11, 18, 11], "temperature": 0.0, "avg_logprob": -0.16852603338461006, "compression_ratio": 1.5909090909090908, "no_speech_prob": 5.0936337174789514e-06}, {"id": 2214, "seek": 714200, "start": 7146.2, "end": 7148.96, "text": " rather than mean 1,2,3.", "tokens": [2831, 813, 914, 502, 11, 17, 11, 18, 13], "temperature": 0.0, "avg_logprob": -0.16852603338461006, "compression_ratio": 1.5909090909090908, "no_speech_prob": 5.0936337174789514e-06}, {"id": 2215, "seek": 714200, "start": 7148.96, "end": 7151.2, "text": " So you can see how all these different papers,", "tokens": [407, 291, 393, 536, 577, 439, 613, 819, 10577, 11], "temperature": 0.0, "avg_logprob": -0.16852603338461006, "compression_ratio": 1.5909090909090908, "no_speech_prob": 5.0936337174789514e-06}, {"id": 2216, "seek": 714200, "start": 7151.2, "end": 7154.4, "text": " when you turn them into code, they're tiny variations, right?", "tokens": [562, 291, 1261, 552, 666, 3089, 11, 436, 434, 5870, 17840, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16852603338461006, "compression_ratio": 1.5909090909090908, "no_speech_prob": 5.0936337174789514e-06}, {"id": 2217, "seek": 714200, "start": 7156.12, "end": 7159.6, "text": " Instance norm, even at a learning rate of 0.1, doesn't learn anything at all.", "tokens": [2730, 719, 2026, 11, 754, 412, 257, 2539, 3314, 295, 1958, 13, 16, 11, 1177, 380, 1466, 1340, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.16852603338461006, "compression_ratio": 1.5909090909090908, "no_speech_prob": 5.0936337174789514e-06}, {"id": 2218, "seek": 714200, "start": 7161.32, "end": 7162.6, "text": " Why can't it classify anything?", "tokens": [1545, 393, 380, 309, 33872, 1340, 30], "temperature": 0.0, "avg_logprob": -0.16852603338461006, "compression_ratio": 1.5909090909090908, "no_speech_prob": 5.0936337174789514e-06}, {"id": 2219, "seek": 714200, "start": 7163.64, "end": 7169.72, "text": " Because we're now taking the mean, removing the difference in means and", "tokens": [1436, 321, 434, 586, 1940, 264, 914, 11, 12720, 264, 2649, 294, 1355, 293], "temperature": 0.0, "avg_logprob": -0.16852603338461006, "compression_ratio": 1.5909090909090908, "no_speech_prob": 5.0936337174789514e-06}, {"id": 2220, "seek": 716972, "start": 7169.72, "end": 7174.64, "text": " the difference in activations for every channel and for every image.", "tokens": [264, 2649, 294, 2430, 763, 337, 633, 2269, 293, 337, 633, 3256, 13], "temperature": 0.0, "avg_logprob": -0.15456260724014112, "compression_ratio": 1.6553191489361703, "no_speech_prob": 6.438810032705078e-06}, {"id": 2221, "seek": 716972, "start": 7174.64, "end": 7178.8, "text": " Which means we've literally thrown away all the things that allow us to classify.", "tokens": [3013, 1355, 321, 600, 3736, 11732, 1314, 439, 264, 721, 300, 2089, 505, 281, 33872, 13], "temperature": 0.0, "avg_logprob": -0.15456260724014112, "compression_ratio": 1.6553191489361703, "no_speech_prob": 6.438810032705078e-06}, {"id": 2222, "seek": 716972, "start": 7178.8, "end": 7181.16, "text": " Does that mean that instance norm is stupid?", "tokens": [4402, 300, 914, 300, 5197, 2026, 307, 6631, 30], "temperature": 0.0, "avg_logprob": -0.15456260724014112, "compression_ratio": 1.6553191489361703, "no_speech_prob": 6.438810032705078e-06}, {"id": 2223, "seek": 716972, "start": 7181.16, "end": 7182.68, "text": " No, certainly not.", "tokens": [883, 11, 3297, 406, 13], "temperature": 0.0, "avg_logprob": -0.15456260724014112, "compression_ratio": 1.6553191489361703, "no_speech_prob": 6.438810032705078e-06}, {"id": 2224, "seek": 716972, "start": 7182.68, "end": 7185.8, "text": " It wasn't designed for classification.", "tokens": [467, 2067, 380, 4761, 337, 21538, 13], "temperature": 0.0, "avg_logprob": -0.15456260724014112, "compression_ratio": 1.6553191489361703, "no_speech_prob": 6.438810032705078e-06}, {"id": 2225, "seek": 716972, "start": 7185.8, "end": 7191.68, "text": " It was designed for style transfer, where the authors guessed that these", "tokens": [467, 390, 4761, 337, 3758, 5003, 11, 689, 264, 16552, 21852, 300, 613], "temperature": 0.0, "avg_logprob": -0.15456260724014112, "compression_ratio": 1.6553191489361703, "no_speech_prob": 6.438810032705078e-06}, {"id": 2226, "seek": 716972, "start": 7191.68, "end": 7196.6, "text": " differences in contrast and overall amount were not important.", "tokens": [7300, 294, 8712, 293, 4787, 2372, 645, 406, 1021, 13], "temperature": 0.0, "avg_logprob": -0.15456260724014112, "compression_ratio": 1.6553191489361703, "no_speech_prob": 6.438810032705078e-06}, {"id": 2227, "seek": 719660, "start": 7196.6, "end": 7200.72, "text": " So it's not something they should remove from trying to create things that were", "tokens": [407, 309, 311, 406, 746, 436, 820, 4159, 490, 1382, 281, 1884, 721, 300, 645], "temperature": 0.0, "avg_logprob": -0.24020529200888088, "compression_ratio": 1.5985663082437276, "no_speech_prob": 3.0893088478478603e-06}, {"id": 2228, "seek": 719660, "start": 7200.72, "end": 7202.320000000001, "text": " like different types of pictures.", "tokens": [411, 819, 3467, 295, 5242, 13], "temperature": 0.0, "avg_logprob": -0.24020529200888088, "compression_ratio": 1.5985663082437276, "no_speech_prob": 3.0893088478478603e-06}, {"id": 2229, "seek": 719660, "start": 7202.320000000001, "end": 7203.400000000001, "text": " It turned out to work really well.", "tokens": [467, 3574, 484, 281, 589, 534, 731, 13], "temperature": 0.0, "avg_logprob": -0.24020529200888088, "compression_ratio": 1.5985663082437276, "no_speech_prob": 3.0893088478478603e-06}, {"id": 2230, "seek": 719660, "start": 7204.4400000000005, "end": 7205.4800000000005, "text": " But you gotta be careful, right?", "tokens": [583, 291, 3428, 312, 5026, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.24020529200888088, "compression_ratio": 1.5985663082437276, "no_speech_prob": 3.0893088478478603e-06}, {"id": 2231, "seek": 719660, "start": 7205.4800000000005, "end": 7209.240000000001, "text": " You can't just go in and say, here's another normalization thing, I'll try it.", "tokens": [509, 393, 380, 445, 352, 294, 293, 584, 11, 510, 311, 1071, 2710, 2144, 551, 11, 286, 603, 853, 309, 13], "temperature": 0.0, "avg_logprob": -0.24020529200888088, "compression_ratio": 1.5985663082437276, "no_speech_prob": 3.0893088478478603e-06}, {"id": 2232, "seek": 719660, "start": 7209.240000000001, "end": 7212.280000000001, "text": " You gotta actually know what it's for, to know where it's gonna work.", "tokens": [509, 3428, 767, 458, 437, 309, 311, 337, 11, 281, 458, 689, 309, 311, 799, 589, 13], "temperature": 0.0, "avg_logprob": -0.24020529200888088, "compression_ratio": 1.5985663082437276, "no_speech_prob": 3.0893088478478603e-06}, {"id": 2233, "seek": 719660, "start": 7213.4400000000005, "end": 7216.52, "text": " So then finally, there's a paper called Group Norm,", "tokens": [407, 550, 2721, 11, 456, 311, 257, 3035, 1219, 10500, 8702, 11], "temperature": 0.0, "avg_logprob": -0.24020529200888088, "compression_ratio": 1.5985663082437276, "no_speech_prob": 3.0893088478478603e-06}, {"id": 2234, "seek": 719660, "start": 7216.52, "end": 7220.04, "text": " which has this wonderful picture, and it shows the differences.", "tokens": [597, 575, 341, 3715, 3036, 11, 293, 309, 3110, 264, 7300, 13], "temperature": 0.0, "avg_logprob": -0.24020529200888088, "compression_ratio": 1.5985663082437276, "no_speech_prob": 3.0893088478478603e-06}, {"id": 2235, "seek": 722004, "start": 7220.04, "end": 7227.68, "text": " Batch norm is averaging over the batch and", "tokens": [363, 852, 2026, 307, 47308, 670, 264, 15245, 293], "temperature": 0.0, "avg_logprob": -0.180661255972726, "compression_ratio": 2.0606060606060606, "no_speech_prob": 3.0237235478125513e-05}, {"id": 2236, "seek": 722004, "start": 7227.68, "end": 7231.8, "text": " the height and the width, and is different for each channel.", "tokens": [264, 6681, 293, 264, 11402, 11, 293, 307, 819, 337, 1184, 2269, 13], "temperature": 0.0, "avg_logprob": -0.180661255972726, "compression_ratio": 2.0606060606060606, "no_speech_prob": 3.0237235478125513e-05}, {"id": 2237, "seek": 722004, "start": 7233.24, "end": 7238.24, "text": " Layer norm, it's averaging for each channel, for each height, for each width,", "tokens": [35166, 2026, 11, 309, 311, 47308, 337, 1184, 2269, 11, 337, 1184, 6681, 11, 337, 1184, 11402, 11], "temperature": 0.0, "avg_logprob": -0.180661255972726, "compression_ratio": 2.0606060606060606, "no_speech_prob": 3.0237235478125513e-05}, {"id": 2238, "seek": 722004, "start": 7238.24, "end": 7240.72, "text": " and is different for each element of the batch.", "tokens": [293, 307, 819, 337, 1184, 4478, 295, 264, 15245, 13], "temperature": 0.0, "avg_logprob": -0.180661255972726, "compression_ratio": 2.0606060606060606, "no_speech_prob": 3.0237235478125513e-05}, {"id": 2239, "seek": 722004, "start": 7240.72, "end": 7246.6, "text": " Instance norm is averaging over height and", "tokens": [2730, 719, 2026, 307, 47308, 670, 6681, 293], "temperature": 0.0, "avg_logprob": -0.180661255972726, "compression_ratio": 2.0606060606060606, "no_speech_prob": 3.0237235478125513e-05}, {"id": 2240, "seek": 724660, "start": 7246.6, "end": 7251.0, "text": " width, and is different for each channel and each batch.", "tokens": [11402, 11, 293, 307, 819, 337, 1184, 2269, 293, 1184, 15245, 13], "temperature": 0.0, "avg_logprob": -0.12586480320090115, "compression_ratio": 1.7972350230414746, "no_speech_prob": 5.014547241444234e-06}, {"id": 2241, "seek": 724660, "start": 7251.0, "end": 7254.240000000001, "text": " And then group norm is the same as instance norm, but", "tokens": [400, 550, 1594, 2026, 307, 264, 912, 382, 5197, 2026, 11, 457], "temperature": 0.0, "avg_logprob": -0.12586480320090115, "compression_ratio": 1.7972350230414746, "no_speech_prob": 5.014547241444234e-06}, {"id": 2242, "seek": 724660, "start": 7254.240000000001, "end": 7259.360000000001, "text": " they arbitrarily group a few channels together and do that.", "tokens": [436, 19071, 3289, 1594, 257, 1326, 9235, 1214, 293, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.12586480320090115, "compression_ratio": 1.7972350230414746, "no_speech_prob": 5.014547241444234e-06}, {"id": 2243, "seek": 724660, "start": 7259.360000000001, "end": 7264.84, "text": " So group norm is kind of a more general way to do it.", "tokens": [407, 1594, 2026, 307, 733, 295, 257, 544, 2674, 636, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.12586480320090115, "compression_ratio": 1.7972350230414746, "no_speech_prob": 5.014547241444234e-06}, {"id": 2244, "seek": 724660, "start": 7264.84, "end": 7269.320000000001, "text": " And in the PyTorch docs, they point out that you can actually turn group norm into", "tokens": [400, 294, 264, 9953, 51, 284, 339, 45623, 11, 436, 935, 484, 300, 291, 393, 767, 1261, 1594, 2026, 666], "temperature": 0.0, "avg_logprob": -0.12586480320090115, "compression_ratio": 1.7972350230414746, "no_speech_prob": 5.014547241444234e-06}, {"id": 2245, "seek": 724660, "start": 7269.320000000001, "end": 7273.56, "text": " instance norm or group norm into layer norm, depending on how you group things up.", "tokens": [5197, 2026, 420, 1594, 2026, 666, 4583, 2026, 11, 5413, 322, 577, 291, 1594, 721, 493, 13], "temperature": 0.0, "avg_logprob": -0.12586480320090115, "compression_ratio": 1.7972350230414746, "no_speech_prob": 5.014547241444234e-06}, {"id": 2246, "seek": 727356, "start": 7273.56, "end": 7279.160000000001, "text": " So there's all kinds of attempts to work around the problem that we can't use", "tokens": [407, 456, 311, 439, 3685, 295, 15257, 281, 589, 926, 264, 1154, 300, 321, 393, 380, 764], "temperature": 0.0, "avg_logprob": -0.3648005284761128, "compression_ratio": 1.4879518072289157, "no_speech_prob": 2.0580155251082033e-06}, {"id": 2247, "seek": 727356, "start": 7279.160000000001, "end": 7285.280000000001, "text": " small batch sizes and we can't use RNNs with batch norm.", "tokens": [1359, 15245, 11602, 293, 321, 393, 380, 764, 45702, 45, 82, 365, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.3648005284761128, "compression_ratio": 1.4879518072289157, "no_speech_prob": 2.0580155251082033e-06}, {"id": 2248, "seek": 727356, "start": 7285.280000000001, "end": 7290.84, "text": " But none of them are as good as batch norm.", "tokens": [583, 6022, 295, 552, 366, 382, 665, 382, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.3648005284761128, "compression_ratio": 1.4879518072289157, "no_speech_prob": 2.0580155251082033e-06}, {"id": 2249, "seek": 727356, "start": 7290.84, "end": 7296.52, "text": " So what do we do?", "tokens": [407, 437, 360, 321, 360, 30], "temperature": 0.0, "avg_logprob": -0.3648005284761128, "compression_ratio": 1.4879518072289157, "no_speech_prob": 2.0580155251082033e-06}, {"id": 2250, "seek": 727356, "start": 7298.52, "end": 7301.72, "text": " Well, I don't know how to fix the RNN problem, but", "tokens": [1042, 11, 286, 500, 380, 458, 577, 281, 3191, 264, 45702, 45, 1154, 11, 457], "temperature": 0.0, "avg_logprob": -0.3648005284761128, "compression_ratio": 1.4879518072289157, "no_speech_prob": 2.0580155251082033e-06}, {"id": 2251, "seek": 730172, "start": 7301.72, "end": 7304.280000000001, "text": " I think I know how to fix the batch size problem.", "tokens": [286, 519, 286, 458, 577, 281, 3191, 264, 15245, 2744, 1154, 13], "temperature": 0.0, "avg_logprob": -0.22119808197021484, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.5688976418459788e-05}, {"id": 2252, "seek": 730172, "start": 7306.320000000001, "end": 7309.16, "text": " So let's start by taking a look at the batch size problem in practice.", "tokens": [407, 718, 311, 722, 538, 1940, 257, 574, 412, 264, 15245, 2744, 1154, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.22119808197021484, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.5688976418459788e-05}, {"id": 2253, "seek": 730172, "start": 7309.16, "end": 7312.400000000001, "text": " Let's create a new data bunch with a batch size of two.", "tokens": [961, 311, 1884, 257, 777, 1412, 3840, 365, 257, 15245, 2744, 295, 732, 13], "temperature": 0.0, "avg_logprob": -0.22119808197021484, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.5688976418459788e-05}, {"id": 2254, "seek": 730172, "start": 7314.240000000001, "end": 7318.280000000001, "text": " Okay, and so here's our conv layer as before with our batch norm.", "tokens": [1033, 11, 293, 370, 510, 311, 527, 3754, 4583, 382, 949, 365, 527, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.22119808197021484, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.5688976418459788e-05}, {"id": 2255, "seek": 730172, "start": 7319.320000000001, "end": 7323.88, "text": " And let's use a learning rate of 0.4 and fit that.", "tokens": [400, 718, 311, 764, 257, 2539, 3314, 295, 1958, 13, 19, 293, 3318, 300, 13], "temperature": 0.0, "avg_logprob": -0.22119808197021484, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.5688976418459788e-05}, {"id": 2256, "seek": 730172, "start": 7325.96, "end": 7328.12, "text": " And the first thing you'll notice is that it takes a long time.", "tokens": [400, 264, 700, 551, 291, 603, 3449, 307, 300, 309, 2516, 257, 938, 565, 13], "temperature": 0.0, "avg_logprob": -0.22119808197021484, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.5688976418459788e-05}, {"id": 2257, "seek": 732812, "start": 7328.12, "end": 7333.0, "text": " Right, small batch sizes take a long time because it's just lots and", "tokens": [1779, 11, 1359, 15245, 11602, 747, 257, 938, 565, 570, 309, 311, 445, 3195, 293], "temperature": 0.0, "avg_logprob": -0.2022674280569094, "compression_ratio": 1.594488188976378, "no_speech_prob": 2.6425477699376643e-06}, {"id": 2258, "seek": 732812, "start": 7333.0, "end": 7337.24, "text": " lots of kernel launches on the GPU, it's just a lot of overhead, right?", "tokens": [3195, 295, 28256, 31841, 322, 264, 18407, 11, 309, 311, 445, 257, 688, 295, 19922, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2022674280569094, "compression_ratio": 1.594488188976378, "no_speech_prob": 2.6425477699376643e-06}, {"id": 2259, "seek": 732812, "start": 7337.24, "end": 7339.4, "text": " Something like this might even run faster on the CPU.", "tokens": [6595, 411, 341, 1062, 754, 1190, 4663, 322, 264, 13199, 13], "temperature": 0.0, "avg_logprob": -0.2022674280569094, "compression_ratio": 1.594488188976378, "no_speech_prob": 2.6425477699376643e-06}, {"id": 2260, "seek": 732812, "start": 7340.76, "end": 7345.92, "text": " And then you'll notice that it's only 26% accurate, which is awful.", "tokens": [400, 550, 291, 603, 3449, 300, 309, 311, 787, 7551, 4, 8559, 11, 597, 307, 11232, 13], "temperature": 0.0, "avg_logprob": -0.2022674280569094, "compression_ratio": 1.594488188976378, "no_speech_prob": 2.6425477699376643e-06}, {"id": 2261, "seek": 732812, "start": 7345.92, "end": 7347.4, "text": " Why is it awful?", "tokens": [1545, 307, 309, 11232, 30], "temperature": 0.0, "avg_logprob": -0.2022674280569094, "compression_ratio": 1.594488188976378, "no_speech_prob": 2.6425477699376643e-06}, {"id": 2262, "seek": 732812, "start": 7347.4, "end": 7353.36, "text": " Because of what I said, the small batch size is causing a huge problem.", "tokens": [1436, 295, 437, 286, 848, 11, 264, 1359, 15245, 2744, 307, 9853, 257, 2603, 1154, 13], "temperature": 0.0, "avg_logprob": -0.2022674280569094, "compression_ratio": 1.594488188976378, "no_speech_prob": 2.6425477699376643e-06}, {"id": 2263, "seek": 732812, "start": 7353.36, "end": 7357.5599999999995, "text": " Because quite often, there's one channel in one layer", "tokens": [1436, 1596, 2049, 11, 456, 311, 472, 2269, 294, 472, 4583], "temperature": 0.0, "avg_logprob": -0.2022674280569094, "compression_ratio": 1.594488188976378, "no_speech_prob": 2.6425477699376643e-06}, {"id": 2264, "seek": 735756, "start": 7357.56, "end": 7361.120000000001, "text": " where the variance is really small because those two numbers just happen to", "tokens": [689, 264, 21977, 307, 534, 1359, 570, 729, 732, 3547, 445, 1051, 281], "temperature": 0.0, "avg_logprob": -0.20133924996981056, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.255263658909826e-06}, {"id": 2265, "seek": 735756, "start": 7361.120000000001, "end": 7365.52, "text": " be really close, and so it blows out the activations out to a billion and", "tokens": [312, 534, 1998, 11, 293, 370, 309, 18458, 484, 264, 2430, 763, 484, 281, 257, 5218, 293], "temperature": 0.0, "avg_logprob": -0.20133924996981056, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.255263658909826e-06}, {"id": 2266, "seek": 735756, "start": 7365.52, "end": 7366.320000000001, "text": " everything falls apart.", "tokens": [1203, 8804, 4936, 13], "temperature": 0.0, "avg_logprob": -0.20133924996981056, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.255263658909826e-06}, {"id": 2267, "seek": 735756, "start": 7369.320000000001, "end": 7373.4800000000005, "text": " There is one thing we could try to do to fix this really easily,", "tokens": [821, 307, 472, 551, 321, 727, 853, 281, 360, 281, 3191, 341, 534, 3612, 11], "temperature": 0.0, "avg_logprob": -0.20133924996981056, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.255263658909826e-06}, {"id": 2268, "seek": 735756, "start": 7373.4800000000005, "end": 7375.92, "text": " which is to use epsilon.", "tokens": [597, 307, 281, 764, 17889, 13], "temperature": 0.0, "avg_logprob": -0.20133924996981056, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.255263658909826e-06}, {"id": 2269, "seek": 735756, "start": 7377.04, "end": 7378.4800000000005, "text": " What's epsilon?", "tokens": [708, 311, 17889, 30], "temperature": 0.0, "avg_logprob": -0.20133924996981056, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.255263658909826e-06}, {"id": 2270, "seek": 735756, "start": 7378.4800000000005, "end": 7379.68, "text": " Let's go take a look at our code.", "tokens": [961, 311, 352, 747, 257, 574, 412, 527, 3089, 13], "temperature": 0.0, "avg_logprob": -0.20133924996981056, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.255263658909826e-06}, {"id": 2271, "seek": 735756, "start": 7384.8, "end": 7386.6, "text": " Here's our batch norm.", "tokens": [1692, 311, 527, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.20133924996981056, "compression_ratio": 1.5555555555555556, "no_speech_prob": 5.255263658909826e-06}, {"id": 2272, "seek": 738660, "start": 7386.6, "end": 7392.200000000001, "text": " Look, we don't divide by the square root of variance.", "tokens": [2053, 11, 321, 500, 380, 9845, 538, 264, 3732, 5593, 295, 21977, 13], "temperature": 0.0, "avg_logprob": -0.19611955243487691, "compression_ratio": 1.685, "no_speech_prob": 8.397736564802472e-06}, {"id": 2273, "seek": 738660, "start": 7392.200000000001, "end": 7395.76, "text": " We divide by the square root of variance plus epsilon,", "tokens": [492, 9845, 538, 264, 3732, 5593, 295, 21977, 1804, 17889, 11], "temperature": 0.0, "avg_logprob": -0.19611955243487691, "compression_ratio": 1.685, "no_speech_prob": 8.397736564802472e-06}, {"id": 2274, "seek": 738660, "start": 7395.76, "end": 7397.320000000001, "text": " where epsilon is 1e-5.", "tokens": [689, 17889, 307, 502, 68, 12, 20, 13], "temperature": 0.0, "avg_logprob": -0.19611955243487691, "compression_ratio": 1.685, "no_speech_prob": 8.397736564802472e-06}, {"id": 2275, "seek": 738660, "start": 7399.400000000001, "end": 7403.160000000001, "text": " Epsilon's a number that computer scientists and mathematicians,", "tokens": [462, 16592, 311, 257, 1230, 300, 3820, 7708, 293, 32811, 2567, 11], "temperature": 0.0, "avg_logprob": -0.19611955243487691, "compression_ratio": 1.685, "no_speech_prob": 8.397736564802472e-06}, {"id": 2276, "seek": 738660, "start": 7403.160000000001, "end": 7408.200000000001, "text": " they use this Greek letter very frequently to mean some very small number.", "tokens": [436, 764, 341, 10281, 5063, 588, 10374, 281, 914, 512, 588, 1359, 1230, 13], "temperature": 0.0, "avg_logprob": -0.19611955243487691, "compression_ratio": 1.685, "no_speech_prob": 8.397736564802472e-06}, {"id": 2277, "seek": 738660, "start": 7409.280000000001, "end": 7413.96, "text": " And in computer science, it's normally a small number that you add", "tokens": [400, 294, 3820, 3497, 11, 309, 311, 5646, 257, 1359, 1230, 300, 291, 909], "temperature": 0.0, "avg_logprob": -0.19611955243487691, "compression_ratio": 1.685, "no_speech_prob": 8.397736564802472e-06}, {"id": 2278, "seek": 741396, "start": 7413.96, "end": 7419.12, "text": " to avoid floating point rounding problems and stuff like that.", "tokens": [281, 5042, 12607, 935, 48237, 2740, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.12843192141989004, "compression_ratio": 1.6217391304347826, "no_speech_prob": 7.0717319431423675e-06}, {"id": 2279, "seek": 741396, "start": 7419.12, "end": 7424.6, "text": " So it's very common to see it on the bottom of a division", "tokens": [407, 309, 311, 588, 2689, 281, 536, 309, 322, 264, 2767, 295, 257, 10044], "temperature": 0.0, "avg_logprob": -0.12843192141989004, "compression_ratio": 1.6217391304347826, "no_speech_prob": 7.0717319431423675e-06}, {"id": 2280, "seek": 741396, "start": 7424.6, "end": 7428.96, "text": " to avoid dividing by such small numbers that you kind of can't calculate things", "tokens": [281, 5042, 26764, 538, 1270, 1359, 3547, 300, 291, 733, 295, 393, 380, 8873, 721], "temperature": 0.0, "avg_logprob": -0.12843192141989004, "compression_ratio": 1.6217391304347826, "no_speech_prob": 7.0717319431423675e-06}, {"id": 2281, "seek": 741396, "start": 7428.96, "end": 7429.88, "text": " in floating point properly.", "tokens": [294, 12607, 935, 6108, 13], "temperature": 0.0, "avg_logprob": -0.12843192141989004, "compression_ratio": 1.6217391304347826, "no_speech_prob": 7.0717319431423675e-06}, {"id": 2282, "seek": 741396, "start": 7431.64, "end": 7438.96, "text": " But our view is that epsilon is actually a fantastic hyperparameter", "tokens": [583, 527, 1910, 307, 300, 17889, 307, 767, 257, 5456, 9848, 2181, 335, 2398], "temperature": 0.0, "avg_logprob": -0.12843192141989004, "compression_ratio": 1.6217391304347826, "no_speech_prob": 7.0717319431423675e-06}, {"id": 2283, "seek": 741396, "start": 7438.96, "end": 7442.28, "text": " that you should be using to train things better.", "tokens": [300, 291, 820, 312, 1228, 281, 3847, 721, 1101, 13], "temperature": 0.0, "avg_logprob": -0.12843192141989004, "compression_ratio": 1.6217391304347826, "no_speech_prob": 7.0717319431423675e-06}, {"id": 2284, "seek": 741396, "start": 7442.28, "end": 7443.68, "text": " And here's a great example.", "tokens": [400, 510, 311, 257, 869, 1365, 13], "temperature": 0.0, "avg_logprob": -0.12843192141989004, "compression_ratio": 1.6217391304347826, "no_speech_prob": 7.0717319431423675e-06}, {"id": 2285, "seek": 744368, "start": 7443.68, "end": 7448.16, "text": " With batch norm, what if we didn't set epsilon to 1e-5,", "tokens": [2022, 15245, 2026, 11, 437, 498, 321, 994, 380, 992, 17889, 281, 502, 68, 12, 20, 11], "temperature": 0.0, "avg_logprob": -0.21813804058989217, "compression_ratio": 1.6523605150214593, "no_speech_prob": 1.321107629337348e-05}, {"id": 2286, "seek": 744368, "start": 7448.16, "end": 7449.8, "text": " but what if we set it to 0.1?", "tokens": [457, 437, 498, 321, 992, 309, 281, 1958, 13, 16, 30], "temperature": 0.0, "avg_logprob": -0.21813804058989217, "compression_ratio": 1.6523605150214593, "no_speech_prob": 1.321107629337348e-05}, {"id": 2287, "seek": 744368, "start": 7451.320000000001, "end": 7456.200000000001, "text": " If we set epsilon to 0.1, then that basically would cause this", "tokens": [759, 321, 992, 17889, 281, 1958, 13, 16, 11, 550, 300, 1936, 576, 3082, 341], "temperature": 0.0, "avg_logprob": -0.21813804058989217, "compression_ratio": 1.6523605150214593, "no_speech_prob": 1.321107629337348e-05}, {"id": 2288, "seek": 744368, "start": 7456.200000000001, "end": 7463.320000000001, "text": " to never make the overall activations be multiplied by anything more than 10.", "tokens": [281, 1128, 652, 264, 4787, 2430, 763, 312, 17207, 538, 1340, 544, 813, 1266, 13], "temperature": 0.0, "avg_logprob": -0.21813804058989217, "compression_ratio": 1.6523605150214593, "no_speech_prob": 1.321107629337348e-05}, {"id": 2289, "seek": 744368, "start": 7464.84, "end": 7465.52, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.21813804058989217, "compression_ratio": 1.6523605150214593, "no_speech_prob": 1.321107629337348e-05}, {"id": 2290, "seek": 744368, "start": 7465.52, "end": 7467.76, "text": " Sorry, that would be 0.01, because we're taking the square root.", "tokens": [4919, 11, 300, 576, 312, 1958, 13, 10607, 11, 570, 321, 434, 1940, 264, 3732, 5593, 13], "temperature": 0.0, "avg_logprob": -0.21813804058989217, "compression_ratio": 1.6523605150214593, "no_speech_prob": 1.321107629337348e-05}, {"id": 2291, "seek": 744368, "start": 7467.76, "end": 7469.240000000001, "text": " So if you set it to 0.01, right?", "tokens": [407, 498, 291, 992, 309, 281, 1958, 13, 10607, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21813804058989217, "compression_ratio": 1.6523605150214593, "no_speech_prob": 1.321107629337348e-05}, {"id": 2292, "seek": 744368, "start": 7469.240000000001, "end": 7472.88, "text": " Because we'd be saying, let's say the variance was 0.", "tokens": [1436, 321, 1116, 312, 1566, 11, 718, 311, 584, 264, 21977, 390, 1958, 13], "temperature": 0.0, "avg_logprob": -0.21813804058989217, "compression_ratio": 1.6523605150214593, "no_speech_prob": 1.321107629337348e-05}, {"id": 2293, "seek": 747288, "start": 7472.88, "end": 7477.400000000001, "text": " It would be 0 plus 0.01 square root, right?", "tokens": [467, 576, 312, 1958, 1804, 1958, 13, 10607, 3732, 5593, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1698676889592951, "compression_ratio": 1.607843137254902, "no_speech_prob": 3.0894664178049425e-06}, {"id": 2294, "seek": 747288, "start": 7477.400000000001, "end": 7481.28, "text": " So it ends up dividing by 0.1, which ends up multiplying by 10.", "tokens": [407, 309, 5314, 493, 26764, 538, 1958, 13, 16, 11, 597, 5314, 493, 30955, 538, 1266, 13], "temperature": 0.0, "avg_logprob": -0.1698676889592951, "compression_ratio": 1.607843137254902, "no_speech_prob": 3.0894664178049425e-06}, {"id": 2295, "seek": 747288, "start": 7481.28, "end": 7484.88, "text": " So even in the worst case, it's not gonna blow out.", "tokens": [407, 754, 294, 264, 5855, 1389, 11, 309, 311, 406, 799, 6327, 484, 13], "temperature": 0.0, "avg_logprob": -0.1698676889592951, "compression_ratio": 1.607843137254902, "no_speech_prob": 3.0894664178049425e-06}, {"id": 2296, "seek": 747288, "start": 7486.2, "end": 7491.68, "text": " I mean, it's still not great, because there actually are huge differences", "tokens": [286, 914, 11, 309, 311, 920, 406, 869, 11, 570, 456, 767, 366, 2603, 7300], "temperature": 0.0, "avg_logprob": -0.1698676889592951, "compression_ratio": 1.607843137254902, "no_speech_prob": 3.0894664178049425e-06}, {"id": 2297, "seek": 747288, "start": 7491.68, "end": 7494.76, "text": " in variance between different channels and different layers.", "tokens": [294, 21977, 1296, 819, 9235, 293, 819, 7914, 13], "temperature": 0.0, "avg_logprob": -0.1698676889592951, "compression_ratio": 1.607843137254902, "no_speech_prob": 3.0894664178049425e-06}, {"id": 2298, "seek": 747288, "start": 7494.76, "end": 7497.16, "text": " But at least this would cause it to not fall apart.", "tokens": [583, 412, 1935, 341, 576, 3082, 309, 281, 406, 2100, 4936, 13], "temperature": 0.0, "avg_logprob": -0.1698676889592951, "compression_ratio": 1.607843137254902, "no_speech_prob": 3.0894664178049425e-06}, {"id": 2299, "seek": 747288, "start": 7497.16, "end": 7501.64, "text": " So option number one would be, use a much higher epsilon value.", "tokens": [407, 3614, 1230, 472, 576, 312, 11, 764, 257, 709, 2946, 17889, 2158, 13], "temperature": 0.0, "avg_logprob": -0.1698676889592951, "compression_ratio": 1.607843137254902, "no_speech_prob": 3.0894664178049425e-06}, {"id": 2300, "seek": 750164, "start": 7501.64, "end": 7505.160000000001, "text": " And we'll keep coming back to this idea that epsilon appears in lots of places", "tokens": [400, 321, 603, 1066, 1348, 646, 281, 341, 1558, 300, 17889, 7038, 294, 3195, 295, 3190], "temperature": 0.0, "avg_logprob": -0.3064231671785053, "compression_ratio": 1.669811320754717, "no_speech_prob": 8.800798241281882e-06}, {"id": 2301, "seek": 750164, "start": 7505.160000000001, "end": 7509.360000000001, "text": " in deep learning, and we should use it as a hyperparameter we control", "tokens": [294, 2452, 2539, 11, 293, 321, 820, 764, 309, 382, 257, 9848, 2181, 335, 2398, 321, 1969], "temperature": 0.0, "avg_logprob": -0.3064231671785053, "compression_ratio": 1.669811320754717, "no_speech_prob": 8.800798241281882e-06}, {"id": 2302, "seek": 750164, "start": 7509.360000000001, "end": 7510.4800000000005, "text": " and take advantage of.", "tokens": [293, 747, 5002, 295, 13], "temperature": 0.0, "avg_logprob": -0.3064231671785053, "compression_ratio": 1.669811320754717, "no_speech_prob": 8.800798241281882e-06}, {"id": 2303, "seek": 750164, "start": 7513.64, "end": 7515.4800000000005, "text": " But we have a better idea.", "tokens": [583, 321, 362, 257, 1101, 1558, 13], "temperature": 0.0, "avg_logprob": -0.3064231671785053, "compression_ratio": 1.669811320754717, "no_speech_prob": 8.800798241281882e-06}, {"id": 2304, "seek": 750164, "start": 7516.8, "end": 7520.68, "text": " We think we have a better idea, which is we've built a new algorithm", "tokens": [492, 519, 321, 362, 257, 1101, 1558, 11, 597, 307, 321, 600, 3094, 257, 777, 9284], "temperature": 0.0, "avg_logprob": -0.3064231671785053, "compression_ratio": 1.669811320754717, "no_speech_prob": 8.800798241281882e-06}, {"id": 2305, "seek": 750164, "start": 7520.68, "end": 7522.72, "text": " called running batch norm.", "tokens": [1219, 2614, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.3064231671785053, "compression_ratio": 1.669811320754717, "no_speech_prob": 8.800798241281882e-06}, {"id": 2306, "seek": 750164, "start": 7522.72, "end": 7529.12, "text": " And running batch norm, I think, is the first true solution", "tokens": [400, 2614, 15245, 2026, 11, 286, 519, 11, 307, 264, 700, 2074, 3827], "temperature": 0.0, "avg_logprob": -0.3064231671785053, "compression_ratio": 1.669811320754717, "no_speech_prob": 8.800798241281882e-06}, {"id": 2307, "seek": 752912, "start": 7529.12, "end": 7533.8, "text": " to the small batch size batch norm problem.", "tokens": [281, 264, 1359, 15245, 2744, 15245, 2026, 1154, 13], "temperature": 0.0, "avg_logprob": -0.36693867770108307, "compression_ratio": 1.5951219512195123, "no_speech_prob": 8.93903779797256e-06}, {"id": 2308, "seek": 752912, "start": 7533.8, "end": 7538.5199999999995, "text": " And like everything we do at Fast AI, it's ridiculously simple.", "tokens": [400, 411, 1203, 321, 360, 412, 15968, 7318, 11, 309, 311, 41358, 2199, 13], "temperature": 0.0, "avg_logprob": -0.36693867770108307, "compression_ratio": 1.5951219512195123, "no_speech_prob": 8.93903779797256e-06}, {"id": 2309, "seek": 752912, "start": 7538.5199999999995, "end": 7541.28, "text": " And I don't know why no one's done it before.", "tokens": [400, 286, 500, 380, 458, 983, 572, 472, 311, 1096, 309, 949, 13], "temperature": 0.0, "avg_logprob": -0.36693867770108307, "compression_ratio": 1.5951219512195123, "no_speech_prob": 8.93903779797256e-06}, {"id": 2310, "seek": 752912, "start": 7541.28, "end": 7543.16, "text": " Maybe they have, and I've missed it.", "tokens": [2704, 436, 362, 11, 293, 286, 600, 6721, 309, 13], "temperature": 0.0, "avg_logprob": -0.36693867770108307, "compression_ratio": 1.5951219512195123, "no_speech_prob": 8.93903779797256e-06}, {"id": 2311, "seek": 752912, "start": 7543.16, "end": 7546.04, "text": " And the ridiculously simple thing is this.", "tokens": [400, 264, 41358, 2199, 551, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.36693867770108307, "compression_ratio": 1.5951219512195123, "no_speech_prob": 8.93903779797256e-06}, {"id": 2312, "seek": 752912, "start": 7546.04, "end": 7552.04, "text": " In the forward function for running batch norm,", "tokens": [682, 264, 2128, 2445, 337, 2614, 15245, 2026, 11], "temperature": 0.0, "avg_logprob": -0.36693867770108307, "compression_ratio": 1.5951219512195123, "no_speech_prob": 8.93903779797256e-06}, {"id": 2313, "seek": 752912, "start": 7552.04, "end": 7557.64, "text": " don't divide by the batch standard deviation.", "tokens": [500, 380, 9845, 538, 264, 15245, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.36693867770108307, "compression_ratio": 1.5951219512195123, "no_speech_prob": 8.93903779797256e-06}, {"id": 2314, "seek": 755764, "start": 7557.64, "end": 7564.04, "text": " Don't subtract the batch mean, but instead use the moving average", "tokens": [1468, 380, 16390, 264, 15245, 914, 11, 457, 2602, 764, 264, 2684, 4274], "temperature": 0.0, "avg_logprob": -0.19627011906016956, "compression_ratio": 1.5810810810810811, "no_speech_prob": 5.173474164621439e-06}, {"id": 2315, "seek": 755764, "start": 7564.04, "end": 7571.64, "text": " statistics at training time as well, not just at inference time.", "tokens": [12523, 412, 3097, 565, 382, 731, 11, 406, 445, 412, 38253, 565, 13], "temperature": 0.0, "avg_logprob": -0.19627011906016956, "compression_ratio": 1.5810810810810811, "no_speech_prob": 5.173474164621439e-06}, {"id": 2316, "seek": 755764, "start": 7572.92, "end": 7574.64, "text": " Why does this help?", "tokens": [1545, 775, 341, 854, 30], "temperature": 0.0, "avg_logprob": -0.19627011906016956, "compression_ratio": 1.5810810810810811, "no_speech_prob": 5.173474164621439e-06}, {"id": 2317, "seek": 755764, "start": 7574.64, "end": 7580.0, "text": " Because let's say you're using a batch size of two, right?", "tokens": [1436, 718, 311, 584, 291, 434, 1228, 257, 15245, 2744, 295, 732, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19627011906016956, "compression_ratio": 1.5810810810810811, "no_speech_prob": 5.173474164621439e-06}, {"id": 2318, "seek": 755764, "start": 7580.0, "end": 7584.240000000001, "text": " Then from time to time, in this particular layer, in this particular", "tokens": [1396, 490, 565, 281, 565, 11, 294, 341, 1729, 4583, 11, 294, 341, 1729], "temperature": 0.0, "avg_logprob": -0.19627011906016956, "compression_ratio": 1.5810810810810811, "no_speech_prob": 5.173474164621439e-06}, {"id": 2319, "seek": 755764, "start": 7584.240000000001, "end": 7587.4400000000005, "text": " channel, you happen to get two values that are really close together and", "tokens": [2269, 11, 291, 1051, 281, 483, 732, 4190, 300, 366, 534, 1998, 1214, 293], "temperature": 0.0, "avg_logprob": -0.19627011906016956, "compression_ratio": 1.5810810810810811, "no_speech_prob": 5.173474164621439e-06}, {"id": 2320, "seek": 758744, "start": 7587.44, "end": 7589.96, "text": " they have a variance really close to zero.", "tokens": [436, 362, 257, 21977, 534, 1998, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.170491209975234, "compression_ratio": 1.5608695652173914, "no_speech_prob": 8.397170859097969e-06}, {"id": 2321, "seek": 758744, "start": 7589.96, "end": 7591.5599999999995, "text": " But that's fine, right?", "tokens": [583, 300, 311, 2489, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.170491209975234, "compression_ratio": 1.5608695652173914, "no_speech_prob": 8.397170859097969e-06}, {"id": 2322, "seek": 758744, "start": 7591.5599999999995, "end": 7596.36, "text": " Because you're only taking 0.1 of that and 0.9 of whatever you had before.", "tokens": [1436, 291, 434, 787, 1940, 1958, 13, 16, 295, 300, 293, 1958, 13, 24, 295, 2035, 291, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.170491209975234, "compression_ratio": 1.5608695652173914, "no_speech_prob": 8.397170859097969e-06}, {"id": 2323, "seek": 758744, "start": 7596.36, "end": 7598.719999999999, "text": " That's how running averages work, right?", "tokens": [663, 311, 577, 2614, 42257, 589, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.170491209975234, "compression_ratio": 1.5608695652173914, "no_speech_prob": 8.397170859097969e-06}, {"id": 2324, "seek": 758744, "start": 7598.719999999999, "end": 7607.36, "text": " So if previously the variance was 1, now it's not 1e-5, it's just 0.9, right?", "tokens": [407, 498, 8046, 264, 21977, 390, 502, 11, 586, 309, 311, 406, 502, 68, 12, 20, 11, 309, 311, 445, 1958, 13, 24, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.170491209975234, "compression_ratio": 1.5608695652173914, "no_speech_prob": 8.397170859097969e-06}, {"id": 2325, "seek": 758744, "start": 7607.36, "end": 7610.919999999999, "text": " So in this way, as long as you don't get really unlucky and", "tokens": [407, 294, 341, 636, 11, 382, 938, 382, 291, 500, 380, 483, 534, 38838, 293], "temperature": 0.0, "avg_logprob": -0.170491209975234, "compression_ratio": 1.5608695652173914, "no_speech_prob": 8.397170859097969e-06}, {"id": 2326, "seek": 758744, "start": 7610.919999999999, "end": 7613.719999999999, "text": " have the very first batch be dreadful.", "tokens": [362, 264, 588, 700, 15245, 312, 22236, 906, 13], "temperature": 0.0, "avg_logprob": -0.170491209975234, "compression_ratio": 1.5608695652173914, "no_speech_prob": 8.397170859097969e-06}, {"id": 2327, "seek": 761372, "start": 7613.72, "end": 7620.76, "text": " Because you're using this moving average, you never have this problem.", "tokens": [1436, 291, 434, 1228, 341, 2684, 4274, 11, 291, 1128, 362, 341, 1154, 13], "temperature": 0.0, "avg_logprob": -0.20605498866031044, "compression_ratio": 1.4666666666666666, "no_speech_prob": 8.529853403160814e-06}, {"id": 2328, "seek": 761372, "start": 7621.92, "end": 7623.320000000001, "text": " So let's take a look.", "tokens": [407, 718, 311, 747, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.20605498866031044, "compression_ratio": 1.4666666666666666, "no_speech_prob": 8.529853403160814e-06}, {"id": 2329, "seek": 761372, "start": 7623.320000000001, "end": 7626.16, "text": " We'll look at the code in a moment, but let's do the same thing.", "tokens": [492, 603, 574, 412, 264, 3089, 294, 257, 1623, 11, 457, 718, 311, 360, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.20605498866031044, "compression_ratio": 1.4666666666666666, "no_speech_prob": 8.529853403160814e-06}, {"id": 2330, "seek": 761372, "start": 7626.16, "end": 7629.240000000001, "text": " 0.4, we're gonna use our running batch norm.", "tokens": [1958, 13, 19, 11, 321, 434, 799, 764, 527, 2614, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.20605498866031044, "compression_ratio": 1.4666666666666666, "no_speech_prob": 8.529853403160814e-06}, {"id": 2331, "seek": 761372, "start": 7630.6, "end": 7639.320000000001, "text": " We train it for one epoch, and instead of 26% accuracy, it's 91% accuracy, right?", "tokens": [492, 3847, 309, 337, 472, 30992, 339, 11, 293, 2602, 295, 7551, 4, 14170, 11, 309, 311, 31064, 4, 14170, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20605498866031044, "compression_ratio": 1.4666666666666666, "no_speech_prob": 8.529853403160814e-06}, {"id": 2332, "seek": 761372, "start": 7639.320000000001, "end": 7640.92, "text": " So it totally nails it.", "tokens": [407, 309, 3879, 15394, 309, 13], "temperature": 0.0, "avg_logprob": -0.20605498866031044, "compression_ratio": 1.4666666666666666, "no_speech_prob": 8.529853403160814e-06}, {"id": 2333, "seek": 764092, "start": 7640.92, "end": 7646.72, "text": " In one epoch, just a two batch size and a pretty high learning rate.", "tokens": [682, 472, 30992, 339, 11, 445, 257, 732, 15245, 2744, 293, 257, 1238, 1090, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.18516192069420448, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.494673703447916e-06}, {"id": 2334, "seek": 764092, "start": 7648.88, "end": 7654.4, "text": " There's quite a few details we have to get right to make this work.", "tokens": [821, 311, 1596, 257, 1326, 4365, 321, 362, 281, 483, 558, 281, 652, 341, 589, 13], "temperature": 0.0, "avg_logprob": -0.18516192069420448, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.494673703447916e-06}, {"id": 2335, "seek": 764092, "start": 7656.32, "end": 7662.04, "text": " But they're all details that we're gonna see in lots of other places in this course.", "tokens": [583, 436, 434, 439, 4365, 300, 321, 434, 799, 536, 294, 3195, 295, 661, 3190, 294, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.18516192069420448, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.494673703447916e-06}, {"id": 2336, "seek": 764092, "start": 7662.04, "end": 7664.76, "text": " We're just kind of seeing them here for the first time.", "tokens": [492, 434, 445, 733, 295, 2577, 552, 510, 337, 264, 700, 565, 13], "temperature": 0.0, "avg_logprob": -0.18516192069420448, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.494673703447916e-06}, {"id": 2337, "seek": 764092, "start": 7664.76, "end": 7666.76, "text": " So I'm gonna show you all of the details, but", "tokens": [407, 286, 478, 799, 855, 291, 439, 295, 264, 4365, 11, 457], "temperature": 0.0, "avg_logprob": -0.18516192069420448, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.494673703447916e-06}, {"id": 2338, "seek": 764092, "start": 7666.76, "end": 7668.4400000000005, "text": " don't get overwhelmed, we'll keep coming back to them.", "tokens": [500, 380, 483, 19042, 11, 321, 603, 1066, 1348, 646, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.18516192069420448, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.494673703447916e-06}, {"id": 2339, "seek": 766844, "start": 7668.44, "end": 7674.0, "text": " The first detail is something very simple,", "tokens": [440, 700, 2607, 307, 746, 588, 2199, 11], "temperature": 0.0, "avg_logprob": -0.18856294872691332, "compression_ratio": 1.9205607476635513, "no_speech_prob": 6.240300990612013e-06}, {"id": 2340, "seek": 766844, "start": 7674.0, "end": 7679.32, "text": " which is in normal batch norm, we take the running average of variance.", "tokens": [597, 307, 294, 2710, 15245, 2026, 11, 321, 747, 264, 2614, 4274, 295, 21977, 13], "temperature": 0.0, "avg_logprob": -0.18856294872691332, "compression_ratio": 1.9205607476635513, "no_speech_prob": 6.240300990612013e-06}, {"id": 2341, "seek": 766844, "start": 7680.599999999999, "end": 7683.2, "text": " But you can't take the running average of variance.", "tokens": [583, 291, 393, 380, 747, 264, 2614, 4274, 295, 21977, 13], "temperature": 0.0, "avg_logprob": -0.18856294872691332, "compression_ratio": 1.9205607476635513, "no_speech_prob": 6.240300990612013e-06}, {"id": 2342, "seek": 766844, "start": 7683.2, "end": 7685.4, "text": " It doesn't make sense to take the running average of variance.", "tokens": [467, 1177, 380, 652, 2020, 281, 747, 264, 2614, 4274, 295, 21977, 13], "temperature": 0.0, "avg_logprob": -0.18856294872691332, "compression_ratio": 1.9205607476635513, "no_speech_prob": 6.240300990612013e-06}, {"id": 2343, "seek": 766844, "start": 7685.4, "end": 7686.719999999999, "text": " It's a variance.", "tokens": [467, 311, 257, 21977, 13], "temperature": 0.0, "avg_logprob": -0.18856294872691332, "compression_ratio": 1.9205607476635513, "no_speech_prob": 6.240300990612013e-06}, {"id": 2344, "seek": 766844, "start": 7686.719999999999, "end": 7689.12, "text": " You can't just average a bunch of variances.", "tokens": [509, 393, 380, 445, 4274, 257, 3840, 295, 1374, 21518, 13], "temperature": 0.0, "avg_logprob": -0.18856294872691332, "compression_ratio": 1.9205607476635513, "no_speech_prob": 6.240300990612013e-06}, {"id": 2345, "seek": 766844, "start": 7690.759999999999, "end": 7694.24, "text": " Particularly because they might even be different batch sizes, right?", "tokens": [32281, 570, 436, 1062, 754, 312, 819, 15245, 11602, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18856294872691332, "compression_ratio": 1.9205607476635513, "no_speech_prob": 6.240300990612013e-06}, {"id": 2346, "seek": 766844, "start": 7694.24, "end": 7696.96, "text": " Cuz batch size isn't necessarily constant, right?", "tokens": [27017, 15245, 2744, 1943, 380, 4725, 5754, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18856294872691332, "compression_ratio": 1.9205607476635513, "no_speech_prob": 6.240300990612013e-06}, {"id": 2347, "seek": 769696, "start": 7696.96, "end": 7701.32, "text": " Instead, as we learned earlier in the class,", "tokens": [7156, 11, 382, 321, 3264, 3071, 294, 264, 1508, 11], "temperature": 0.0, "avg_logprob": -0.21742630004882812, "compression_ratio": 1.5632183908045978, "no_speech_prob": 1.165899902844103e-05}, {"id": 2348, "seek": 769696, "start": 7701.32, "end": 7706.28, "text": " the way that we want to calculate variance is like this.", "tokens": [264, 636, 300, 321, 528, 281, 8873, 21977, 307, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.21742630004882812, "compression_ratio": 1.5632183908045978, "no_speech_prob": 1.165899902844103e-05}, {"id": 2349, "seek": 769696, "start": 7707.68, "end": 7713.96, "text": " Sum of expected value of mean of x squared minus mean of x squared.", "tokens": [8626, 295, 5176, 2158, 295, 914, 295, 2031, 8889, 3175, 914, 295, 2031, 8889, 13], "temperature": 0.0, "avg_logprob": -0.21742630004882812, "compression_ratio": 1.5632183908045978, "no_speech_prob": 1.165899902844103e-05}, {"id": 2350, "seek": 769696, "start": 7713.96, "end": 7714.76, "text": " So let's do that.", "tokens": [407, 718, 311, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.21742630004882812, "compression_ratio": 1.5632183908045978, "no_speech_prob": 1.165899902844103e-05}, {"id": 2351, "seek": 769696, "start": 7714.76, "end": 7720.76, "text": " Let's just, as I mentioned, we can do, let's keep track of the squares and", "tokens": [961, 311, 445, 11, 382, 286, 2835, 11, 321, 393, 360, 11, 718, 311, 1066, 2837, 295, 264, 19368, 293], "temperature": 0.0, "avg_logprob": -0.21742630004882812, "compression_ratio": 1.5632183908045978, "no_speech_prob": 1.165899902844103e-05}, {"id": 2352, "seek": 769696, "start": 7720.76, "end": 7721.4, "text": " the sums.", "tokens": [264, 34499, 13], "temperature": 0.0, "avg_logprob": -0.21742630004882812, "compression_ratio": 1.5632183908045978, "no_speech_prob": 1.165899902844103e-05}, {"id": 2353, "seek": 772140, "start": 7721.4, "end": 7727.599999999999, "text": " So we register a buffer called sums and", "tokens": [407, 321, 7280, 257, 21762, 1219, 34499, 293], "temperature": 0.0, "avg_logprob": -0.25809227792840256, "compression_ratio": 1.5853658536585367, "no_speech_prob": 5.5075311138352845e-06}, {"id": 2354, "seek": 772140, "start": 7727.599999999999, "end": 7734.759999999999, "text": " we register a buffer called squares and we just go x.sum over 0 to 3 dimensions.", "tokens": [321, 7280, 257, 21762, 1219, 19368, 293, 321, 445, 352, 2031, 13, 82, 449, 670, 1958, 281, 805, 12819, 13], "temperature": 0.0, "avg_logprob": -0.25809227792840256, "compression_ratio": 1.5853658536585367, "no_speech_prob": 5.5075311138352845e-06}, {"id": 2355, "seek": 772140, "start": 7736.08, "end": 7740.759999999999, "text": " And x times x.sum, so squared, right?", "tokens": [400, 2031, 1413, 2031, 13, 82, 449, 11, 370, 8889, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.25809227792840256, "compression_ratio": 1.5853658536585367, "no_speech_prob": 5.5075311138352845e-06}, {"id": 2356, "seek": 772140, "start": 7740.759999999999, "end": 7743.839999999999, "text": " And then we'll take the LERP,", "tokens": [400, 550, 321, 603, 747, 264, 441, 1598, 47, 11], "temperature": 0.0, "avg_logprob": -0.25809227792840256, "compression_ratio": 1.5853658536585367, "no_speech_prob": 5.5075311138352845e-06}, {"id": 2357, "seek": 772140, "start": 7743.839999999999, "end": 7747.92, "text": " the exponentially weighted moving average of the sums and the squareds.", "tokens": [264, 37330, 32807, 2684, 4274, 295, 264, 34499, 293, 264, 8889, 82, 13], "temperature": 0.0, "avg_logprob": -0.25809227792840256, "compression_ratio": 1.5853658536585367, "no_speech_prob": 5.5075311138352845e-06}, {"id": 2358, "seek": 774792, "start": 7747.92, "end": 7753.4800000000005, "text": " And then for the variance, we will do squareds divided by count", "tokens": [400, 550, 337, 264, 21977, 11, 321, 486, 360, 8889, 82, 6666, 538, 1207], "temperature": 0.0, "avg_logprob": -0.22066227595011392, "compression_ratio": 1.5204678362573099, "no_speech_prob": 3.66876702173613e-06}, {"id": 2359, "seek": 774792, "start": 7753.4800000000005, "end": 7756.8, "text": " minus squared mean, right?", "tokens": [3175, 8889, 914, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.22066227595011392, "compression_ratio": 1.5204678362573099, "no_speech_prob": 3.66876702173613e-06}, {"id": 2360, "seek": 774792, "start": 7756.8, "end": 7760.84, "text": " So it's that formula, okay?", "tokens": [407, 309, 311, 300, 8513, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.22066227595011392, "compression_ratio": 1.5204678362573099, "no_speech_prob": 3.66876702173613e-06}, {"id": 2361, "seek": 774792, "start": 7763.2, "end": 7767.4400000000005, "text": " So that's detail number one that we have to be careful of.", "tokens": [407, 300, 311, 2607, 1230, 472, 300, 321, 362, 281, 312, 5026, 295, 13], "temperature": 0.0, "avg_logprob": -0.22066227595011392, "compression_ratio": 1.5204678362573099, "no_speech_prob": 3.66876702173613e-06}, {"id": 2362, "seek": 774792, "start": 7769.12, "end": 7776.08, "text": " Detail number two is that the batch size could vary from many batch to many batch.", "tokens": [4237, 864, 1230, 732, 307, 300, 264, 15245, 2744, 727, 10559, 490, 867, 15245, 281, 867, 15245, 13], "temperature": 0.0, "avg_logprob": -0.22066227595011392, "compression_ratio": 1.5204678362573099, "no_speech_prob": 3.66876702173613e-06}, {"id": 2363, "seek": 777608, "start": 7776.08, "end": 7780.84, "text": " So we should also register a buffer for count and", "tokens": [407, 321, 820, 611, 7280, 257, 21762, 337, 1207, 293], "temperature": 0.0, "avg_logprob": -0.1767653815353973, "compression_ratio": 1.694300518134715, "no_speech_prob": 9.223102097166702e-06}, {"id": 2364, "seek": 777608, "start": 7780.84, "end": 7785.2, "text": " take an exponentially weighted moving average of the counts of the batch sizes.", "tokens": [747, 364, 37330, 32807, 2684, 4274, 295, 264, 14893, 295, 264, 15245, 11602, 13], "temperature": 0.0, "avg_logprob": -0.1767653815353973, "compression_ratio": 1.694300518134715, "no_speech_prob": 9.223102097166702e-06}, {"id": 2365, "seek": 777608, "start": 7787.2, "end": 7793.08, "text": " So that basically tells us, so what do we need to divide by each time?", "tokens": [407, 300, 1936, 5112, 505, 11, 370, 437, 360, 321, 643, 281, 9845, 538, 1184, 565, 30], "temperature": 0.0, "avg_logprob": -0.1767653815353973, "compression_ratio": 1.694300518134715, "no_speech_prob": 9.223102097166702e-06}, {"id": 2366, "seek": 777608, "start": 7793.08, "end": 7798.4, "text": " The amount we need to divide by each time is the total number of elements", "tokens": [440, 2372, 321, 643, 281, 9845, 538, 1184, 565, 307, 264, 3217, 1230, 295, 4959], "temperature": 0.0, "avg_logprob": -0.1767653815353973, "compression_ratio": 1.694300518134715, "no_speech_prob": 9.223102097166702e-06}, {"id": 2367, "seek": 777608, "start": 7798.4, "end": 7801.84, "text": " in the mini batch divided by the number of channels.", "tokens": [294, 264, 8382, 15245, 6666, 538, 264, 1230, 295, 9235, 13], "temperature": 0.0, "avg_logprob": -0.1767653815353973, "compression_ratio": 1.694300518134715, "no_speech_prob": 9.223102097166702e-06}, {"id": 2368, "seek": 780184, "start": 7801.84, "end": 7806.92, "text": " That's basically grid x times grid y times batch size.", "tokens": [663, 311, 1936, 10748, 2031, 1413, 10748, 288, 1413, 15245, 2744, 13], "temperature": 0.0, "avg_logprob": -0.2060419795024826, "compression_ratio": 1.5888324873096447, "no_speech_prob": 2.5215090317942668e-06}, {"id": 2369, "seek": 780184, "start": 7806.92, "end": 7812.16, "text": " So let's take an exponentially weighted moving average of the count and", "tokens": [407, 718, 311, 747, 364, 37330, 32807, 2684, 4274, 295, 264, 1207, 293], "temperature": 0.0, "avg_logprob": -0.2060419795024826, "compression_ratio": 1.5888324873096447, "no_speech_prob": 2.5215090317942668e-06}, {"id": 2370, "seek": 780184, "start": 7812.16, "end": 7816.6, "text": " then that's what we will divide by for both our means and variances.", "tokens": [550, 300, 311, 437, 321, 486, 9845, 538, 337, 1293, 527, 1355, 293, 1374, 21518, 13], "temperature": 0.0, "avg_logprob": -0.2060419795024826, "compression_ratio": 1.5888324873096447, "no_speech_prob": 2.5215090317942668e-06}, {"id": 2371, "seek": 780184, "start": 7818.08, "end": 7819.08, "text": " That's detail number two.", "tokens": [663, 311, 2607, 1230, 732, 13], "temperature": 0.0, "avg_logprob": -0.2060419795024826, "compression_ratio": 1.5888324873096447, "no_speech_prob": 2.5215090317942668e-06}, {"id": 2372, "seek": 780184, "start": 7821.76, "end": 7825.96, "text": " Detail number three is that we need to do something called debiasing.", "tokens": [4237, 864, 1230, 1045, 307, 300, 321, 643, 281, 360, 746, 1219, 3001, 72, 3349, 13], "temperature": 0.0, "avg_logprob": -0.2060419795024826, "compression_ratio": 1.5888324873096447, "no_speech_prob": 2.5215090317942668e-06}, {"id": 2373, "seek": 782596, "start": 7825.96, "end": 7830.24, "text": " So debiasing is this.", "tokens": [407, 3001, 72, 3349, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.2575775698611611, "compression_ratio": 1.5683060109289617, "no_speech_prob": 1.2678950724875904e-06}, {"id": 2374, "seek": 782596, "start": 7830.24, "end": 7834.84, "text": " We want to make sure that at every point,", "tokens": [492, 528, 281, 652, 988, 300, 412, 633, 935, 11], "temperature": 0.0, "avg_logprob": -0.2575775698611611, "compression_ratio": 1.5683060109289617, "no_speech_prob": 1.2678950724875904e-06}, {"id": 2375, "seek": 782596, "start": 7834.84, "end": 7840.24, "text": " and we're gonna look at this in more detail when we look at optimizers,", "tokens": [293, 321, 434, 799, 574, 412, 341, 294, 544, 2607, 562, 321, 574, 412, 5028, 22525, 11], "temperature": 0.0, "avg_logprob": -0.2575775698611611, "compression_ratio": 1.5683060109289617, "no_speech_prob": 1.2678950724875904e-06}, {"id": 2376, "seek": 782596, "start": 7840.24, "end": 7847.2, "text": " we wanna make sure that every point that no observation is weighted too highly, right?", "tokens": [321, 1948, 652, 988, 300, 633, 935, 300, 572, 14816, 307, 32807, 886, 5405, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2575775698611611, "compression_ratio": 1.5683060109289617, "no_speech_prob": 1.2678950724875904e-06}, {"id": 2377, "seek": 782596, "start": 7847.2, "end": 7852.28, "text": " And the problem is that the normal way of doing moving averages,", "tokens": [400, 264, 1154, 307, 300, 264, 2710, 636, 295, 884, 2684, 42257, 11], "temperature": 0.0, "avg_logprob": -0.2575775698611611, "compression_ratio": 1.5683060109289617, "no_speech_prob": 1.2678950724875904e-06}, {"id": 2378, "seek": 785228, "start": 7852.28, "end": 7856.92, "text": " the very first point gets far too much weight because it appears in the first", "tokens": [264, 588, 700, 935, 2170, 1400, 886, 709, 3364, 570, 309, 7038, 294, 264, 700], "temperature": 0.0, "avg_logprob": -0.13835491381193463, "compression_ratio": 1.5963302752293578, "no_speech_prob": 8.397902092838194e-06}, {"id": 2379, "seek": 785228, "start": 7856.92, "end": 7860.4, "text": " moving average and the second and the third and the fourth, right?", "tokens": [2684, 4274, 293, 264, 1150, 293, 264, 2636, 293, 264, 6409, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13835491381193463, "compression_ratio": 1.5963302752293578, "no_speech_prob": 8.397902092838194e-06}, {"id": 2380, "seek": 785228, "start": 7860.4, "end": 7864.4, "text": " So there's a really simple way to fix this, which is that you initialize", "tokens": [407, 456, 311, 257, 534, 2199, 636, 281, 3191, 341, 11, 597, 307, 300, 291, 5883, 1125], "temperature": 0.0, "avg_logprob": -0.13835491381193463, "compression_ratio": 1.5963302752293578, "no_speech_prob": 8.397902092838194e-06}, {"id": 2381, "seek": 785228, "start": 7864.4, "end": 7870.08, "text": " both sums and squares to zeros, right?", "tokens": [1293, 34499, 293, 19368, 281, 35193, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13835491381193463, "compression_ratio": 1.5963302752293578, "no_speech_prob": 8.397902092838194e-06}, {"id": 2382, "seek": 785228, "start": 7870.08, "end": 7876.04, "text": " And then you do a LERP in the usual way, right?", "tokens": [400, 550, 291, 360, 257, 441, 1598, 47, 294, 264, 7713, 636, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13835491381193463, "compression_ratio": 1.5963302752293578, "no_speech_prob": 8.397902092838194e-06}, {"id": 2383, "seek": 785228, "start": 7876.04, "end": 7877.719999999999, "text": " And let's see what happens when we do this.", "tokens": [400, 718, 311, 536, 437, 2314, 562, 321, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.13835491381193463, "compression_ratio": 1.5963302752293578, "no_speech_prob": 8.397902092838194e-06}, {"id": 2384, "seek": 787772, "start": 7877.72, "end": 7884.72, "text": " So let's say our values are", "tokens": [407, 718, 311, 584, 527, 4190, 366], "temperature": 0.0, "avg_logprob": -0.32374817984444754, "compression_ratio": 1.5378787878787878, "no_speech_prob": 8.939045073930174e-06}, {"id": 2385, "seek": 787772, "start": 7884.72, "end": 7892.92, "text": " 10 and then 20.", "tokens": [1266, 293, 550, 945, 13], "temperature": 0.0, "avg_logprob": -0.32374817984444754, "compression_ratio": 1.5378787878787878, "no_speech_prob": 8.939045073930174e-06}, {"id": 2386, "seek": 787772, "start": 7892.92, "end": 7894.96, "text": " These are the first two values we get.", "tokens": [1981, 366, 264, 700, 732, 4190, 321, 483, 13], "temperature": 0.0, "avg_logprob": -0.32374817984444754, "compression_ratio": 1.5378787878787878, "no_speech_prob": 8.939045073930174e-06}, {"id": 2387, "seek": 787772, "start": 7894.96, "end": 7898.52, "text": " So actually, we already need to talk about the first value.", "tokens": [407, 767, 11, 321, 1217, 643, 281, 751, 466, 264, 700, 2158, 13], "temperature": 0.0, "avg_logprob": -0.32374817984444754, "compression_ratio": 1.5378787878787878, "no_speech_prob": 8.939045073930174e-06}, {"id": 2388, "seek": 787772, "start": 7898.52, "end": 7902.92, "text": " So the value, so actually, let's say the value is 10, right?", "tokens": [407, 264, 2158, 11, 370, 767, 11, 718, 311, 584, 264, 2158, 307, 1266, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.32374817984444754, "compression_ratio": 1.5378787878787878, "no_speech_prob": 8.939045073930174e-06}, {"id": 2389, "seek": 790292, "start": 7902.92, "end": 7910.6, "text": " So we initialize our mean to 0 at the very start of training, right?", "tokens": [407, 321, 5883, 1125, 527, 914, 281, 1958, 412, 264, 588, 722, 295, 3097, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17561855770292736, "compression_ratio": 1.4550264550264551, "no_speech_prob": 4.565897597785806e-06}, {"id": 2390, "seek": 790292, "start": 7910.6, "end": 7913.72, "text": " And then the value that comes in is 10, right?", "tokens": [400, 550, 264, 2158, 300, 1487, 294, 307, 1266, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17561855770292736, "compression_ratio": 1.4550264550264551, "no_speech_prob": 4.565897597785806e-06}, {"id": 2391, "seek": 790292, "start": 7913.72, "end": 7916.36, "text": " So we would expect the moving average to be 10.", "tokens": [407, 321, 576, 2066, 264, 2684, 4274, 281, 312, 1266, 13], "temperature": 0.0, "avg_logprob": -0.17561855770292736, "compression_ratio": 1.4550264550264551, "no_speech_prob": 4.565897597785806e-06}, {"id": 2392, "seek": 790292, "start": 7916.36, "end": 7922.0, "text": " But our LERP formula says it's equal to our previous value,", "tokens": [583, 527, 441, 1598, 47, 8513, 1619, 309, 311, 2681, 281, 527, 3894, 2158, 11], "temperature": 0.0, "avg_logprob": -0.17561855770292736, "compression_ratio": 1.4550264550264551, "no_speech_prob": 4.565897597785806e-06}, {"id": 2393, "seek": 790292, "start": 7922.0, "end": 7929.96, "text": " which is 0, times 0.9 plus our new value times 0.1.", "tokens": [597, 307, 1958, 11, 1413, 1958, 13, 24, 1804, 527, 777, 2158, 1413, 1958, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.17561855770292736, "compression_ratio": 1.4550264550264551, "no_speech_prob": 4.565897597785806e-06}, {"id": 2394, "seek": 792996, "start": 7929.96, "end": 7935.56, "text": " Equals 0 plus 1, equals 1.", "tokens": [15624, 1124, 1958, 1804, 502, 11, 6915, 502, 13], "temperature": 0.0, "avg_logprob": -0.23052068857046273, "compression_ratio": 1.4489795918367347, "no_speech_prob": 9.516085810901131e-06}, {"id": 2395, "seek": 792996, "start": 7935.56, "end": 7937.8, "text": " It's 10 times too small, okay?", "tokens": [467, 311, 1266, 1413, 886, 1359, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.23052068857046273, "compression_ratio": 1.4489795918367347, "no_speech_prob": 9.516085810901131e-06}, {"id": 2396, "seek": 792996, "start": 7937.8, "end": 7942.2, "text": " But that's very easy to correct for, because we know it's always gonna be", "tokens": [583, 300, 311, 588, 1858, 281, 3006, 337, 11, 570, 321, 458, 309, 311, 1009, 799, 312], "temperature": 0.0, "avg_logprob": -0.23052068857046273, "compression_ratio": 1.4489795918367347, "no_speech_prob": 9.516085810901131e-06}, {"id": 2397, "seek": 792996, "start": 7942.2, "end": 7949.2, "text": " wrong by that amount, so we then divide it by 0.1.", "tokens": [2085, 538, 300, 2372, 11, 370, 321, 550, 9845, 309, 538, 1958, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.23052068857046273, "compression_ratio": 1.4489795918367347, "no_speech_prob": 9.516085810901131e-06}, {"id": 2398, "seek": 792996, "start": 7949.2, "end": 7949.92, "text": " And that fixes it.", "tokens": [400, 300, 32539, 309, 13], "temperature": 0.0, "avg_logprob": -0.23052068857046273, "compression_ratio": 1.4489795918367347, "no_speech_prob": 9.516085810901131e-06}, {"id": 2399, "seek": 792996, "start": 7951.4800000000005, "end": 7955.08, "text": " And then the second value has exactly the same problem.", "tokens": [400, 550, 264, 1150, 2158, 575, 2293, 264, 912, 1154, 13], "temperature": 0.0, "avg_logprob": -0.23052068857046273, "compression_ratio": 1.4489795918367347, "no_speech_prob": 9.516085810901131e-06}, {"id": 2400, "seek": 792996, "start": 7955.08, "end": 7957.64, "text": " It's got too much 0 in it.", "tokens": [467, 311, 658, 886, 709, 1958, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.23052068857046273, "compression_ratio": 1.4489795918367347, "no_speech_prob": 9.516085810901131e-06}, {"id": 2401, "seek": 795764, "start": 7957.64, "end": 7961.8, "text": " But this time, it's actually gonna be divided by,", "tokens": [583, 341, 565, 11, 309, 311, 767, 799, 312, 6666, 538, 11], "temperature": 0.0, "avg_logprob": -0.23923480385228207, "compression_ratio": 1.6022099447513811, "no_speech_prob": 6.74777493259171e-06}, {"id": 2402, "seek": 795764, "start": 7961.8, "end": 7968.12, "text": " let's not call it 0.1, let's call it 1-0.9.", "tokens": [718, 311, 406, 818, 309, 1958, 13, 16, 11, 718, 311, 818, 309, 502, 12, 15, 13, 24, 13], "temperature": 0.0, "avg_logprob": -0.23923480385228207, "compression_ratio": 1.6022099447513811, "no_speech_prob": 6.74777493259171e-06}, {"id": 2403, "seek": 795764, "start": 7968.12, "end": 7972.92, "text": " Because when you work through the math, you'll see the second one,", "tokens": [1436, 562, 291, 589, 807, 264, 5221, 11, 291, 603, 536, 264, 1150, 472, 11], "temperature": 0.0, "avg_logprob": -0.23923480385228207, "compression_ratio": 1.6022099447513811, "no_speech_prob": 6.74777493259171e-06}, {"id": 2404, "seek": 795764, "start": 7972.92, "end": 7978.4800000000005, "text": " it's gonna be divided by 1-0.9 squared, and so forth, okay?", "tokens": [309, 311, 799, 312, 6666, 538, 502, 12, 15, 13, 24, 8889, 11, 293, 370, 5220, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.23923480385228207, "compression_ratio": 1.6022099447513811, "no_speech_prob": 6.74777493259171e-06}, {"id": 2405, "seek": 795764, "start": 7979.92, "end": 7983.4800000000005, "text": " So this thing here, where we divide by that, that's called debiasing.", "tokens": [407, 341, 551, 510, 11, 689, 321, 9845, 538, 300, 11, 300, 311, 1219, 3001, 72, 3349, 13], "temperature": 0.0, "avg_logprob": -0.23923480385228207, "compression_ratio": 1.6022099447513811, "no_speech_prob": 6.74777493259171e-06}, {"id": 2406, "seek": 798348, "start": 7983.48, "end": 7988.599999999999, "text": " It's gonna appear again when we look at optimization.", "tokens": [467, 311, 799, 4204, 797, 562, 321, 574, 412, 19618, 13], "temperature": 0.0, "avg_logprob": -0.22111165523529053, "compression_ratio": 1.671875, "no_speech_prob": 5.173799308977323e-06}, {"id": 2407, "seek": 798348, "start": 7988.599999999999, "end": 7994.32, "text": " So you can see what we do is we have an exponentially weighted debiasing amount", "tokens": [407, 291, 393, 536, 437, 321, 360, 307, 321, 362, 364, 37330, 32807, 3001, 72, 3349, 2372], "temperature": 0.0, "avg_logprob": -0.22111165523529053, "compression_ratio": 1.671875, "no_speech_prob": 5.173799308977323e-06}, {"id": 2408, "seek": 798348, "start": 7995.5199999999995, "end": 8000.2, "text": " where we simply keep multiplying momentum times the previous", "tokens": [689, 321, 2935, 1066, 30955, 11244, 1413, 264, 3894], "temperature": 0.0, "avg_logprob": -0.22111165523529053, "compression_ratio": 1.671875, "no_speech_prob": 5.173799308977323e-06}, {"id": 2409, "seek": 798348, "start": 8001.599999999999, "end": 8002.5199999999995, "text": " debiasing amount.", "tokens": [3001, 72, 3349, 2372, 13], "temperature": 0.0, "avg_logprob": -0.22111165523529053, "compression_ratio": 1.671875, "no_speech_prob": 5.173799308977323e-06}, {"id": 2410, "seek": 798348, "start": 8002.5199999999995, "end": 8006.44, "text": " So initially, it's just equal to momentum, and then momentum squared,", "tokens": [407, 9105, 11, 309, 311, 445, 2681, 281, 11244, 11, 293, 550, 11244, 8889, 11], "temperature": 0.0, "avg_logprob": -0.22111165523529053, "compression_ratio": 1.671875, "no_speech_prob": 5.173799308977323e-06}, {"id": 2411, "seek": 798348, "start": 8006.44, "end": 8008.959999999999, "text": " and then momentum cubed, and so forth.", "tokens": [293, 550, 11244, 36510, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.22111165523529053, "compression_ratio": 1.671875, "no_speech_prob": 5.173799308977323e-06}, {"id": 2412, "seek": 800896, "start": 8008.96, "end": 8016.2, "text": " So then we do what I just said.", "tokens": [407, 550, 321, 360, 437, 286, 445, 848, 13], "temperature": 0.0, "avg_logprob": -0.19168204295484326, "compression_ratio": 1.4946236559139785, "no_speech_prob": 3.4465094813640462e-06}, {"id": 2413, "seek": 800896, "start": 8016.2, "end": 8018.72, "text": " We divide by the debiasing amount.", "tokens": [492, 9845, 538, 264, 3001, 72, 3349, 2372, 13], "temperature": 0.0, "avg_logprob": -0.19168204295484326, "compression_ratio": 1.4946236559139785, "no_speech_prob": 3.4465094813640462e-06}, {"id": 2414, "seek": 800896, "start": 8021.8, "end": 8025.84, "text": " Okay, and then there's just one more thing we do, which is,", "tokens": [1033, 11, 293, 550, 456, 311, 445, 472, 544, 551, 321, 360, 11, 597, 307, 11], "temperature": 0.0, "avg_logprob": -0.19168204295484326, "compression_ratio": 1.4946236559139785, "no_speech_prob": 3.4465094813640462e-06}, {"id": 2415, "seek": 800896, "start": 8025.84, "end": 8031.24, "text": " remember how I said you might get really unlucky that your first mini-batch", "tokens": [1604, 577, 286, 848, 291, 1062, 483, 534, 38838, 300, 428, 700, 8382, 12, 65, 852], "temperature": 0.0, "avg_logprob": -0.19168204295484326, "compression_ratio": 1.4946236559139785, "no_speech_prob": 3.4465094813640462e-06}, {"id": 2416, "seek": 800896, "start": 8031.24, "end": 8035.44, "text": " is just really close to zero, and we don't want that to destroy everything?", "tokens": [307, 445, 534, 1998, 281, 4018, 11, 293, 321, 500, 380, 528, 300, 281, 5293, 1203, 30], "temperature": 0.0, "avg_logprob": -0.19168204295484326, "compression_ratio": 1.4946236559139785, "no_speech_prob": 3.4465094813640462e-06}, {"id": 2417, "seek": 803544, "start": 8035.44, "end": 8039.599999999999, "text": " So I just say, if you haven't seen more than a total of 20 items yet,", "tokens": [407, 286, 445, 584, 11, 498, 291, 2378, 380, 1612, 544, 813, 257, 3217, 295, 945, 4754, 1939, 11], "temperature": 0.0, "avg_logprob": -0.17187171512179905, "compression_ratio": 1.54, "no_speech_prob": 9.080305972020142e-06}, {"id": 2418, "seek": 803544, "start": 8040.839999999999, "end": 8043.799999999999, "text": " just clamp the variance to be no smaller than 0.01,", "tokens": [445, 17690, 264, 21977, 281, 312, 572, 4356, 813, 1958, 13, 10607, 11], "temperature": 0.0, "avg_logprob": -0.17187171512179905, "compression_ratio": 1.54, "no_speech_prob": 9.080305972020142e-06}, {"id": 2419, "seek": 803544, "start": 8043.799999999999, "end": 8046.16, "text": " just to avoid blowing out of the water.", "tokens": [445, 281, 5042, 15068, 484, 295, 264, 1281, 13], "temperature": 0.0, "avg_logprob": -0.17187171512179905, "compression_ratio": 1.54, "no_speech_prob": 9.080305972020142e-06}, {"id": 2420, "seek": 803544, "start": 8048.08, "end": 8051.719999999999, "text": " And then the last two lines are the same, okay?", "tokens": [400, 550, 264, 1036, 732, 3876, 366, 264, 912, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.17187171512179905, "compression_ratio": 1.54, "no_speech_prob": 9.080305972020142e-06}, {"id": 2421, "seek": 803544, "start": 8051.719999999999, "end": 8054.24, "text": " So that's it, right?", "tokens": [407, 300, 311, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17187171512179905, "compression_ratio": 1.54, "no_speech_prob": 9.080305972020142e-06}, {"id": 2422, "seek": 803544, "start": 8054.24, "end": 8058.919999999999, "text": " It's all pretty straightforward arithmetic.", "tokens": [467, 311, 439, 1238, 15325, 42973, 13], "temperature": 0.0, "avg_logprob": -0.17187171512179905, "compression_ratio": 1.54, "no_speech_prob": 9.080305972020142e-06}, {"id": 2423, "seek": 803544, "start": 8058.919999999999, "end": 8061.32, "text": " It's a very straightforward idea.", "tokens": [467, 311, 257, 588, 15325, 1558, 13], "temperature": 0.0, "avg_logprob": -0.17187171512179905, "compression_ratio": 1.54, "no_speech_prob": 9.080305972020142e-06}, {"id": 2424, "seek": 806132, "start": 8061.32, "end": 8069.08, "text": " But when we put it all together, it's shockingly effective.", "tokens": [583, 562, 321, 829, 309, 439, 1214, 11, 309, 311, 5588, 12163, 4942, 13], "temperature": 0.0, "avg_logprob": -0.20237880282931858, "compression_ratio": 1.49, "no_speech_prob": 3.966883468820015e-06}, {"id": 2425, "seek": 806132, "start": 8069.08, "end": 8072.16, "text": " And so then we can try an interesting thought experiment.", "tokens": [400, 370, 550, 321, 393, 853, 364, 1880, 1194, 5120, 13], "temperature": 0.0, "avg_logprob": -0.20237880282931858, "compression_ratio": 1.49, "no_speech_prob": 3.966883468820015e-06}, {"id": 2426, "seek": 806132, "start": 8072.16, "end": 8073.88, "text": " So here's another thing to try during the week.", "tokens": [407, 510, 311, 1071, 551, 281, 853, 1830, 264, 1243, 13], "temperature": 0.0, "avg_logprob": -0.20237880282931858, "compression_ratio": 1.49, "no_speech_prob": 3.966883468820015e-06}, {"id": 2427, "seek": 806132, "start": 8075.32, "end": 8080.28, "text": " What's the best accuracy you can get in a single epoch?", "tokens": [708, 311, 264, 1151, 14170, 291, 393, 483, 294, 257, 2167, 30992, 339, 30], "temperature": 0.0, "avg_logprob": -0.20237880282931858, "compression_ratio": 1.49, "no_speech_prob": 3.966883468820015e-06}, {"id": 2428, "seek": 806132, "start": 8081.32, "end": 8085.24, "text": " So say run.fit 1.", "tokens": [407, 584, 1190, 13, 6845, 502, 13], "temperature": 0.0, "avg_logprob": -0.20237880282931858, "compression_ratio": 1.49, "no_speech_prob": 3.966883468820015e-06}, {"id": 2429, "seek": 808524, "start": 8085.24, "end": 8091.5199999999995, "text": " And with this convolutional with running batch norm layer,", "tokens": [400, 365, 341, 45216, 304, 365, 2614, 15245, 2026, 4583, 11], "temperature": 0.0, "avg_logprob": -0.25755210255467614, "compression_ratio": 1.4536585365853658, "no_speech_prob": 3.905277026206022e-06}, {"id": 2430, "seek": 808524, "start": 8091.5199999999995, "end": 8098.76, "text": " and a batch size of 32, and a linear schedule from 1 to 0.2,", "tokens": [293, 257, 15245, 2744, 295, 8858, 11, 293, 257, 8213, 7567, 490, 502, 281, 1958, 13, 17, 11], "temperature": 0.0, "avg_logprob": -0.25755210255467614, "compression_ratio": 1.4536585365853658, "no_speech_prob": 3.905277026206022e-06}, {"id": 2431, "seek": 808524, "start": 8098.76, "end": 8101.5199999999995, "text": " I got 97.5%.", "tokens": [286, 658, 23399, 13, 20, 6856], "temperature": 0.0, "avg_logprob": -0.25755210255467614, "compression_ratio": 1.4536585365853658, "no_speech_prob": 3.905277026206022e-06}, {"id": 2432, "seek": 808524, "start": 8101.5199999999995, "end": 8102.84, "text": " I only tried a couple of things, so", "tokens": [286, 787, 3031, 257, 1916, 295, 721, 11, 370], "temperature": 0.0, "avg_logprob": -0.25755210255467614, "compression_ratio": 1.4536585365853658, "no_speech_prob": 3.905277026206022e-06}, {"id": 2433, "seek": 808524, "start": 8102.84, "end": 8107.44, "text": " I haven't, this is definitely something that I hope you can beat me at.", "tokens": [286, 2378, 380, 11, 341, 307, 2138, 746, 300, 286, 1454, 291, 393, 4224, 385, 412, 13], "temperature": 0.0, "avg_logprob": -0.25755210255467614, "compression_ratio": 1.4536585365853658, "no_speech_prob": 3.905277026206022e-06}, {"id": 2434, "seek": 808524, "start": 8107.44, "end": 8112.719999999999, "text": " But it's really good to kind of create interesting little", "tokens": [583, 309, 311, 534, 665, 281, 733, 295, 1884, 1880, 707], "temperature": 0.0, "avg_logprob": -0.25755210255467614, "compression_ratio": 1.4536585365853658, "no_speech_prob": 3.905277026206022e-06}, {"id": 2435, "seek": 811272, "start": 8112.72, "end": 8116.84, "text": " games to play in research, we call them toy problems.", "tokens": [2813, 281, 862, 294, 2132, 11, 321, 818, 552, 12058, 2740, 13], "temperature": 0.0, "avg_logprob": -0.16515106515786082, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.320838732179254e-05}, {"id": 2436, "seek": 811272, "start": 8116.84, "end": 8119.280000000001, "text": " Almost everything in research is basically toy problems.", "tokens": [12627, 1203, 294, 2132, 307, 1936, 12058, 2740, 13], "temperature": 0.0, "avg_logprob": -0.16515106515786082, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.320838732179254e-05}, {"id": 2437, "seek": 811272, "start": 8119.280000000001, "end": 8123.6, "text": " Come up with toy problems and try to find good solutions to them.", "tokens": [2492, 493, 365, 12058, 2740, 293, 853, 281, 915, 665, 6547, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.16515106515786082, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.320838732179254e-05}, {"id": 2438, "seek": 811272, "start": 8123.6, "end": 8127.240000000001, "text": " So your toy problem, another toy problem for this week is,", "tokens": [407, 428, 12058, 1154, 11, 1071, 12058, 1154, 337, 341, 1243, 307, 11], "temperature": 0.0, "avg_logprob": -0.16515106515786082, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.320838732179254e-05}, {"id": 2439, "seek": 811272, "start": 8127.240000000001, "end": 8132.8, "text": " what's the best you can get using, yeah, whatever kind of normalization you like,", "tokens": [437, 311, 264, 1151, 291, 393, 483, 1228, 11, 1338, 11, 2035, 733, 295, 2710, 2144, 291, 411, 11], "temperature": 0.0, "avg_logprob": -0.16515106515786082, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.320838732179254e-05}, {"id": 2440, "seek": 811272, "start": 8133.92, "end": 8137.400000000001, "text": " whatever kind of architecture you like, as long as it only uses concepts we've", "tokens": [2035, 733, 295, 9482, 291, 411, 11, 382, 938, 382, 309, 787, 4960, 10392, 321, 600], "temperature": 0.0, "avg_logprob": -0.16515106515786082, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.320838732179254e-05}, {"id": 2441, "seek": 813740, "start": 8137.4, "end": 8143.36, "text": " used up to lesson seven to get the best accuracy you can in one epoch.", "tokens": [1143, 493, 281, 6898, 3407, 281, 483, 264, 1151, 14170, 291, 393, 294, 472, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.162012515681805, "compression_ratio": 1.550660792951542, "no_speech_prob": 4.682557118940167e-05}, {"id": 2442, "seek": 813740, "start": 8148.36, "end": 8150.16, "text": " Yeah, that's basically it.", "tokens": [865, 11, 300, 311, 1936, 309, 13], "temperature": 0.0, "avg_logprob": -0.162012515681805, "compression_ratio": 1.550660792951542, "no_speech_prob": 4.682557118940167e-05}, {"id": 2443, "seek": 813740, "start": 8150.16, "end": 8153.32, "text": " So what's the future of running batch norm?", "tokens": [407, 437, 311, 264, 2027, 295, 2614, 15245, 2026, 30], "temperature": 0.0, "avg_logprob": -0.162012515681805, "compression_ratio": 1.550660792951542, "no_speech_prob": 4.682557118940167e-05}, {"id": 2444, "seek": 813740, "start": 8153.32, "end": 8155.4, "text": " I mean, it's kind of early days, right?", "tokens": [286, 914, 11, 309, 311, 733, 295, 2440, 1708, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.162012515681805, "compression_ratio": 1.550660792951542, "no_speech_prob": 4.682557118940167e-05}, {"id": 2445, "seek": 813740, "start": 8155.4, "end": 8158.679999999999, "text": " We haven't published this research yet.", "tokens": [492, 2378, 380, 6572, 341, 2132, 1939, 13], "temperature": 0.0, "avg_logprob": -0.162012515681805, "compression_ratio": 1.550660792951542, "no_speech_prob": 4.682557118940167e-05}, {"id": 2446, "seek": 813740, "start": 8158.679999999999, "end": 8163.0, "text": " We haven't done all the kind of ablation studies and stuff we need to do yet.", "tokens": [492, 2378, 380, 1096, 439, 264, 733, 295, 410, 24278, 5313, 293, 1507, 321, 643, 281, 360, 1939, 13], "temperature": 0.0, "avg_logprob": -0.162012515681805, "compression_ratio": 1.550660792951542, "no_speech_prob": 4.682557118940167e-05}, {"id": 2447, "seek": 813740, "start": 8163.0, "end": 8165.0, "text": " At this stage though, I'm really excited about this.", "tokens": [1711, 341, 3233, 1673, 11, 286, 478, 534, 2919, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.162012515681805, "compression_ratio": 1.550660792951542, "no_speech_prob": 4.682557118940167e-05}, {"id": 2448, "seek": 816500, "start": 8165.0, "end": 8167.88, "text": " Every time I've tried it on something, it's been working really well.", "tokens": [2048, 565, 286, 600, 3031, 309, 322, 746, 11, 309, 311, 668, 1364, 534, 731, 13], "temperature": 0.0, "avg_logprob": -0.21485280990600586, "compression_ratio": 1.5964912280701755, "no_speech_prob": 2.9762253689114004e-05}, {"id": 2449, "seek": 816500, "start": 8170.2, "end": 8174.12, "text": " The last time that we had something in a lesson that we said,", "tokens": [440, 1036, 565, 300, 321, 632, 746, 294, 257, 6898, 300, 321, 848, 11], "temperature": 0.0, "avg_logprob": -0.21485280990600586, "compression_ratio": 1.5964912280701755, "no_speech_prob": 2.9762253689114004e-05}, {"id": 2450, "seek": 816500, "start": 8174.12, "end": 8178.36, "text": " this is unpublished research that we're excited about, it turned into ULM fit,", "tokens": [341, 307, 20994, 836, 4173, 2132, 300, 321, 434, 2919, 466, 11, 309, 3574, 666, 624, 43, 44, 3318, 11], "temperature": 0.0, "avg_logprob": -0.21485280990600586, "compression_ratio": 1.5964912280701755, "no_speech_prob": 2.9762253689114004e-05}, {"id": 2451, "seek": 816500, "start": 8178.36, "end": 8183.96, "text": " which is now a really widely used algorithm and was published at the ACL.", "tokens": [597, 307, 586, 257, 534, 13371, 1143, 9284, 293, 390, 6572, 412, 264, 43873, 13], "temperature": 0.0, "avg_logprob": -0.21485280990600586, "compression_ratio": 1.5964912280701755, "no_speech_prob": 2.9762253689114004e-05}, {"id": 2452, "seek": 816500, "start": 8188.76, "end": 8192.68, "text": " So fingers crossed that this turns out to be something really terrific as well.", "tokens": [407, 7350, 14622, 300, 341, 4523, 484, 281, 312, 746, 534, 20899, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.21485280990600586, "compression_ratio": 1.5964912280701755, "no_speech_prob": 2.9762253689114004e-05}, {"id": 2453, "seek": 819268, "start": 8192.68, "end": 8197.0, "text": " But either way, you've kind of got to see the process,", "tokens": [583, 2139, 636, 11, 291, 600, 733, 295, 658, 281, 536, 264, 1399, 11], "temperature": 0.0, "avg_logprob": -0.16270736501186708, "compression_ratio": 1.5803108808290156, "no_speech_prob": 9.964995115296915e-06}, {"id": 2454, "seek": 819268, "start": 8197.0, "end": 8201.04, "text": " cuz literally building these notebooks was the process I used to create this", "tokens": [11910, 3736, 2390, 613, 43782, 390, 264, 1399, 286, 1143, 281, 1884, 341], "temperature": 0.0, "avg_logprob": -0.16270736501186708, "compression_ratio": 1.5803108808290156, "no_speech_prob": 9.964995115296915e-06}, {"id": 2455, "seek": 819268, "start": 8201.04, "end": 8207.800000000001, "text": " algorithm, so you've seen the exact process that I used to build up this", "tokens": [9284, 11, 370, 291, 600, 1612, 264, 1900, 1399, 300, 286, 1143, 281, 1322, 493, 341], "temperature": 0.0, "avg_logprob": -0.16270736501186708, "compression_ratio": 1.5803108808290156, "no_speech_prob": 9.964995115296915e-06}, {"id": 2456, "seek": 819268, "start": 8207.800000000001, "end": 8210.44, "text": " idea and do some initial testing of it.", "tokens": [1558, 293, 360, 512, 5883, 4997, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.16270736501186708, "compression_ratio": 1.5803108808290156, "no_speech_prob": 9.964995115296915e-06}, {"id": 2457, "seek": 819268, "start": 8210.44, "end": 8213.720000000001, "text": " So hopefully that's been fun for you, and see you next week.", "tokens": [407, 4696, 300, 311, 668, 1019, 337, 291, 11, 293, 536, 291, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.16270736501186708, "compression_ratio": 1.5803108808290156, "no_speech_prob": 9.964995115296915e-06}, {"id": 2458, "seek": 821372, "start": 8213.72, "end": 8223.72, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50864], "temperature": 0.0, "avg_logprob": -0.7676581541697184, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.00022973277373239398}], "language": "en"}