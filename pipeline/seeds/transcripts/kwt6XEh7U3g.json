{"text": " Okay, so we're here at the Melbourne R meetup and we are talking about some techniques that Jeremy Howard has used to do as well as he can in a variety of Kaggle competitions. And we're going to start by having a look at some of the tools that I've found useful in predictive modelling in general and in Kaggle competitions in particular. So I've tried to write down here what I think are some of the key steps. So after you download data from a Kaggle competition, you end up with CSV files, generally speaking, CSV files, which can be in all kinds of formats. So here's the first thing you see when you open up the time series CSV file. It's not very hopeful, is it? So each of these columns is actually, oh here we come, is actually quarterly time series data. And so because, well for various reasons, it's kind of each one's different lengths and they kind of start further along. The particular way that this was provided didn't really suit the tools that I was using. And in fact, if I remember correctly, that's already, I've already adjusted it slightly because it originally came in rows rather than columns. Yeah, that's right. This is how it originally came in rows rather than columns. So this is where this kind of data manipulation toolbox comes in. There's all kinds of ways to swap rows and columns around, which is where I started. The really simple approach is to select the whole lot, copy it, and then paste it and say transpose in Excel. And that's one way to do it. And then having done that, I ended up with something which I could open up in. Let's have a look at the original file. This is the original file in Vim, which is my text editor of choice. This is actually a really good time to get rid of all those kind of leading commas because they kind of confuse me. So this is where stuff like Vim is great. Even things like Notepad++ and Emacs and any of these kind of power user text editors will work fine. As long as you know how to use regular expressions, and if you don't, I'm not going to show you now, but you should definitely look it up. So in this case, I'm just going to go, okay, let's use a regular expression. So I say, yes, the substitute on the whole file, start with any number of commas and replace it with nothing at all. And you can see, Vim, I'm done. So I can now save that and I've got a nice easy file that I can start using. So that's why I've listed this idea of data manipulation tools in my toolbox. And to me, Vim or some regular expression powered text editor which can handle large files is something to be familiar with. So just in case you didn't catch that, that is regular expressions. Definitely the most powerful tool for doing text and data manipulation that I know of. Sometimes they're just called regexes. The most powerful types of regular expressions, I would say, would be the ones that are in Perl. They've been widely used elsewhere. Any C program that uses the PCRE engine has the same regular expressions as Perl, more or less. C sharpen.net have the same regular expressions as Perl, more or less. So this is a nice example of one bunch of people getting it right and everybody else plagiarizing. Vim's regular expressions are slightly different, unfortunately, which annoys me no end, but they still do the job. So yeah, make sure you've got a good text editor that you know well how to use. Something with a good macro facility is nice as well. Again, Vim's great for that. You can record a series of keystrokes and hit a button and it repeats it basically on every line. I also wrote Perl here because to me, Perl is a rather unloved programming language. But if you think back to where it comes from, it was originally developed as the Swiss Army chainsaw of text processing tools. And today that is something it still does, I think, better than any other tool. It has amazing command line options you can pass to it that do things like run the following command on every line in the file or run the following line on every command in the file and then print it out. There's a command line option to back up each file before changing it to large dot back. I find with Perl I can do stuff which would take me a much, much longer time than any other tool. Even simple little things like I was hacking some data on the weekend where I had to concatenate a whole bunch of files, but only the first one I wanted to keep the first line because there are a whole bunch of CSV files in which they had a headline I had to delete. So in Perl, in fact it's probably still going to be sitting here in my history. So in Perl that's basically minus n means do this on every single row. Minus e means I'm not even going to write a script file. I'm going to give you the thing to do it right here on the command line. And here's a piece of rather difficult to comprehend Perl. But trust me what it says is if the line number is greater than one then print that line. So here's something to strip the first line from every file. So this kind of stuff you can do in Perl is great. And I see a lot of people on the forums who complain about the format of the data wasn't quite what I expected or not quite convenient. Can you please change it for me? And I always think, well, this is part of data science. This is part of data hacking. This is data munging or data manipulation. There's actually a really great book called, I don't know if it's hard to find nowadays, but I loved it called Data Munging in Perl. And it's a whole book about all the cool stuff you can do in Perl in a line or two. So okay, I've now got the data into a form where I can kind of load it up into some tool and start looking at it. What's the tool I normally start with? I normally start with Excel. Now your first reaction might be to think Excel, not so good for big files. To which my reaction would be, if you're just looking at the data for the first time, why are you looking at a big file? Start by sampling it. And again, this is the kind of thing you can do in your data manipulation piece. That thing I just showed you in Perl, if that's a rand is greater than 0.9 and print, that's going to sample every 10 rows, more or less. So get your, if you've got a huge data file, get it to a size that you can easily start playing with it, which normally means some random sampling. So I like to look at it in Excel and I will show you for a particular competition how I go about doing that. So let's have a look, for example, at a couple. So here's one which the New South Wales government recently ran, which was to predict how long it's going to take cars to travel along each segment of the M4 motorway in each direction. The data for this is a lot of columns, because every column is another route and lots and lots of rows, every row is another two minute observation and very hard to get a feel for what's going on. There were various terrific attempts on the forum at trying to create animated pictures of what the road looks like over time. I did something extremely low tech, which is something I'm proud of, which is I created a simple little macro in Excel, which selected each column and then went conditional formatting, colour scales, red to green, and I ran that on each column and I got this picture. So here's each route on this road and here's how long it took to go on that route at this time. And isn't this interesting, because I can immediately see what traffic jams look like. See how they kind of flow as you start getting a traffic jam here, they flow along the road as time goes on and you can then start to see at what kind of times they happen and where they tend to start. So here's a really big jam and it's interesting isn't it, because if we go into Sydney in the afternoon, then obviously you start getting these jams up here and during the afternoon it progresses, you can see the jam moving so that at 5pm, looks like there's actually a couple of them, and at the other end of the road it stays jammed until everybody's cleared out through the freeway. So you get a real feel for it, and even when it's not peak hour, and even in some of the period areas which aren't so busy, you can see, that's interesting, there are basically parts of the freeway which out of peak hour they're basically constant travel time. And the colours are immediately showing me, you see how easy it is. So when we actually got on the phone with the RTA to take them through the winning model, actually the people that won this competition were kind enough to organise a screencast with all the people in the RTA and from Kaggle to show the winning model. And the people from RTA said, well this is interesting because you tell me in your model, they said what we looked at was we basically created a model that looked at for a particular time, for a particular route, we looked at the times and routes just before and around it on both sides. And I remember one of the guys said, that's weird because normally these kind of queues track it down and go in one direction, so why would you look at both sides? And so I was able to quickly say, okay guys, that's true, have a look at this. So if you go to the other end, you can see how sometimes although queues kind of form in one direction, they can kind of slide away in the other direction, for example. So by looking at this kind of picture, you can see what your model is going to have to be able to model. So you can see what kind of inputs it's going to have and how it's going to have to be set up and you can immediately see that if you created the model that basically tried to predict each thing based on the previous few periods of the routes around it, whatever modeling technique you're using, you're probably going to get a pretty good answer. And interestingly, the guys that won this competition, this is basically all they did, really nice simple model, they used random forests as it happens, which we'll talk about soon. They added a couple of extra things, which was I think the rate of change of time, but that was basically all. So really good example of how visualization can quite quickly tell you what you need to do. I'll show you another example. This is a recent competition that was set up by the datastats.com blog. And what it was, was they wanted to try and create a recommendation system for our packages. So they got a bunch of users to say, okay, this user for this package doesn't have it installed. This user for this package does have it installed. So you can kind of see how this is structured. They added a bunch of additional potential predictors for you. How many dependencies does this package have? How many suggestions does this package have? How many imports? How many of those task views on CRAN is it included in? Is it a core package? Is it a recommended package? Who maintains it? And so forth. So I found this not particularly easy to get my head around what this looks like. So I used my number one most favorite tool for data visualization and analysis, which is a pivot table. A pivot table is something which dynamically lets you slice and dice your data. If you've used maybe Tableau or something like that, you'll know the feel. This is kind of like Tableau, it doesn't cost $1,000. No, I mean, Tableau's got cool stuff as well. But this is fantastic for most things I find I need to do. And so in this case, I simply drag user ID up to the top and I dragged package name down to the side and just quickly threw this into a matrix basically. And so you can see here what this data looks like, which is that those nasty people at dataverses.com has deleted a whole bunch of things in this matrix. So that's the stuff that they want us to predict. And then we can see that generally as you expect, there's ones and zeros. There's some weird shit going on here where some people have things apparently there twice, which suggests to me maybe there's something funny with data collection. And there's other interesting things. There are some things which seem to be quite widely installed. Most people don't install most packages. And there is this mysterious user number five, who is the world's biggest R package slut. He or she installs everything that they can. And I can only imagine that ADACGH is particularly hard to install because not even user number five managed to get around to it. So you can see how kind of creating a simple little picture like this, I can get a sense of what's going on. So I took that data in the R package competition and I kind of thought, well, if I just knew for a particular... So let's say this empty cell is the one we're trying to predict. So if I just knew in general how commonly acceptance sampling was installed and how often user number one installed stuff, I probably got a good sense of the probability of user number one installing acceptance sampling. So to me, one of the interesting points here was to think, actually, I don't think I care about any of this stuff. So I jumped into R and all I did was I basically said, okay, read that CSV file in. There's a whole bunch of rows here because this is my entire solution. But I'm just going to show you the rows I used for solution number one. So read in the whole lot. So although user is a number, treat it as a factor because user number one is not 50 times worse than user number 50. Those trues and falses turn them into one to zero to make life a bit easier. And now apply mean, the mean function to each user across their installations and apply the mean function to each package across that package's installations. So I now have got a couple of lookups basically that tell me user number 50 installs this percent of packages. This particular package is installed by this percent of users. And then I just stuck them basically back into my file of predictors. So I basically did these simple lookups for each row to look up the user and find out for that row the mean for that user and the mean for that package. And that was actually it. At that point, I then created a GLM in which I had, I created a GLM in which obviously I had my ones and zeros of installations as the thing I was predicting. And my first version I had UP and PP. So these two probabilities as my predictors. In fact, no, in first version it was even easier than that. All I did in fact was I took the max of those two things. So Pmax, if you're not familiar with R is just something that does a max on each row individually. In R nearly everything works on vectors by default except for max. So that's why we have to use Pmax. That's something well worth knowing. So I just took the max. So I, you know, this user installs 30% of things and this package is installed by 40% of users. So the max of the two is 40%. And I actually created a GLM with just one predictor. The benchmark that was created by the data people for this used a GLM on all of those predictors, including all kinds of latent analysis of the manual pages and maintaining names and God knows what. And they had a AUC of 0.8. This five line of code thing had an AUC of 0.95. So you know, the message here is don't overcomplicate things. If people give you data, don't assume that you need to use it and, you know, look at pictures. So if we have a look at kind of my progress in there. So here's my first attempt, which was basically to multiply the use probability by the package probability. And you can see one of the nice things in Kaggle is you get a history of your results. So here's my 0.84 AUC. And then I changed it to using the maximum of two and there's my 0.95 AUC. And I thought, oh, that was good. I imagine how powerful this will be when I use all that data that they gave us with a fancy random forest. And it went backwards. Okay. So you can really see that actually a bit of focused simple analysis can often take you a lot further. So if we look to the next page, we can kind of see where I kind of kept thinking random forests, they ought to buddy works, more random forests and that went backwards. And then I started adding in a few extra things. And then actually I thought, you know, there is one piece of data which is really useful, which is that dependency graph. If somebody has installed package A and it depends on package B, and I know they've got package A, then I also know they've got package B. So I added that case. That's the kind of thing I find a bit difficult to do in R because I think R is a slightly shit programming language. So I did that piece in a language which I quite like, which is C sharp, imported it back to R. And then as you can see, each time I send something off to Kaggle, I generally copy and paste into my notes, just the line of code that I ran so I can see exactly what it was. So here I added this dependency graph and I jumped up to.98. That's basically as far as I got in this competition, which was enough for sixth place. I made a really stupid mistake. Yes, if somebody has package A and it depends on package B, then obviously that means they got package B. I did that. If somebody doesn't have package B and package A depends on it, then you know they definitely don't have package A. I forgot that piece. And so when I went back and put that in after the competition was over and I realized I'd forgotten it, I realized it could have come about second if I'd just done that. In fact, to get in the top three in this competition, that's probably as much modeling as you needed. So I think you can do well in these comps without necessarily being an R expert or necessarily being a stats expert. But you do need to kind of dig into the toolbox appropriately. So let's go back to my extensive slide presentation. So you can see here we talked about data manipulation, about interactive analysis. We've talked a bit about visualizations and I include there even simple things like those tables we did. As I just indicated, in my toolbox is some kind of general purpose program tool. And to me, there's kind of three or four clear leaders in this space. And I know from speaking to people in the data science world, about half the people I speak to don't really know how to program. You definitely should because otherwise all you can do is use stuff that other people have made for you. And I would be picking from these tools. So I like the highly misunderstood C sharp and I would combine it with these particular libraries for... Yes, question? Yeah, I was just wondering whether you saw R as complementary or competing? Yeah, complementary and I'll come to that in the very next bullet point. So this general purpose programming tool is for the stuff that R doesn't do that well. And even the guy that wrote R, Ross Lotko, says he's not that fond nowadays of various things of R as a kind of an underlying language. Whereas there are other languages which are just so powerful and so rich and so beautiful. I should have actually included some of the functional languages in here too like Haskell would be another great choice. But if you've got a good powerful language, a good powerful matrix library and a good powerful machine learning toolkit, you're doing great. So Python is fantastic. Python also has a really, really nice REPL. A REPL is like where you type in a line of code like an R and it immediately gives you the results and you can keep looking through like that. You can use IPython, which is a really fantastic REPL for Python. And in fact, the other really nice thing in Python is Matplotlib, which gives you a really nice charting library. Much less elegant, but just as effective for C sharp and just as free is the MS chart controls. I've written a kind of a functional layer on top of those to make them easier to do analysis with, but they're super fast and super powerful. So that only takes 10 minutes. If you use C++, that also works great. There's a really brilliant thing, very, very underutilized called Eigen, which originally came from the KDE project and just provides an amazingly powerful kind of vector and scientific programming kind of language on top of C++. Java to me is something that used to be on a par with C sharp back in the 1.0, 1.1.1. days. It's looking a bit sad nowadays, but on the other hand, it has just about the most powerful general purpose machine learning library on top of it, which is weaker. So there's a lot to be said for using that combination. In the end, if you're a data scientist who doesn't yet know how to program, my message is learn to program. And I don't think it matters too much which one you pick. I would be picking one of these, but without it, you're going to be struggling to go beyond what the tools provide. Question at the back. Yeah, okay, so the question was about visualization tools and equivalent to SAS jump. Fairly available. Yeah, I would have a look at something like Ggobi. Ggobi is a fascinating tool which kind of has and not free, but in the same kind of area if we talked about Tableau. It supports this concept of brushing, which is this idea that you can create a whole bunch of plots and scatter plots and parallel coordinate plots and all kinds of plots and you can highlight one area of one plot and it will show you where those points are in all the other plots and so in terms of kind of really powerful visualization libraries, I think Ggobi would be where I would go. Having said that, it's amazing how little I use it in real life because things like Excel and what I'm about to come to, which is ggplot2, although much less fancy than things like jump and Tableau and Ggobi, support a kind of hypothesis-driven problem solving approach very well. Something else that I do is I tend to try to create visualizations which meet my particular needs. So we talked about the time series problem and the time series problem is one in which I used a very simple ten line JavaScript piece of code to plot every single time series in a huge big mess like this. Now you kind of might think, well if you're plotting hundreds and hundreds of time series, how much insight are you really getting from that? But I found it was amazing how just scrolling through hundreds of time series, how much my brain picked up. And what I then did was when I started modeling this was I then turned these into something a bit better which was to basically repeat it. At this time I showed both the orange, which is the actuals, and the blues, which is my predictions. And then I put the metric of how successful this particular time series was. So I kind of found that using more focused kind of visualization development, in this case I could immediately see whereabouts were these, which numbers were high. So here's one here, point one, that's a bit higher than the others. And I could immediately kind of see what have I done wrong and I could get a feel of how my modeling was going straight. So I tend to think you don't necessarily need particularly sophisticated visualization tools, they just need to be fairly flexible and you need to know how to drive them to give you what you need. So through this kind of visualization I was able to make sure every single chart in this competition, if it wasn't matching well, and I'd look at it and I'd say, yeah it's not matching well because there was just a shock in some period which couldn't possibly be predicted so that's okay. And so this was one of the competitions that I won and I really think that this visualization approach was key. So I mentioned I was going to come back to one really interesting plotting tool which is ggplot2. ggplot2 is created by a particularly amazing New Zealander who seems to have more time than everybody else in the world combined and creates all these fantastic tools. Thank you Hadley. I was going to show you what I meant by a really powerful but kind of simple plotting tool. Here's something really fascinating, right? You know how creating scatter plots with lots and lots of data is really hard because you end up with just big black blobs, right? So here's a really simple idea which is why don't you give each point in the data a kind of a level of transparency so that the more they sit on top of each other, it's like transparent disks stacking up and getting darker and darker. So in the amazing art package called ggplot2, you can add, so here's something that says plot the carats of a diamond against its price and I want you to vary, it's called the alpha channel to the graphics dex amongst you, you know that means kind of the level of transparency and I want you to basically set the alpha channel for each point to be 1 over 10 or 1 over 100, 1 over 200 and you end up with these plots which actually show you kind of the heat, you know, the amount of that area and it's just so much better than any other approach to scatter plots that I've ever seen. So simple and just one little line of code in your ggplot. I'll show you another couple of examples and this by the way is in a completely free chapter of the book that he's got up on his website. There's a fantastic book, you should definitely buy it by the author of the package about ggplot2 but this one and most important chapter is available free on his website so check it out. I'll show you another couple of examples. Everything's done just right. Here's a simple approach of plotting a lowest smoother through a bunch of data, always handy but every time you plot something you should see the confidence intervals, no problem. This does it by default. The best kind of plot kind of thing you want to see normally is a lower smoother so if you ask for a fit it gives you the lowest move by default, it gives you the confidence interval by default so it's kind of, it makes it hard to create really bad graphs in ggplot2 although some people have managed I've noticed. Things like box plots all stacked up next to each other, such an easy way of seeing in this case how the color of diamonds varies. They've all got roughly the same median but some of them have really long tails in their prices. What a really powerful plotting device and so impressive that in this chapter of the book he shows a few options. Here's what would happen if you used a jitter approach and he's got another one down here which is here's what would happen if you use that alpha transparency approach and you can really compare the different approaches. So ggplot2 is something which, and I'll scroll through these so you can see what kind of stuff you can do, is a really important part of the toolbox. Here's another one I love, right? Okay so we do lots of scatter plots and scatter plots are really powerful and sometimes we actually want to see how if the points are kind of ordered chronologically how did they change over time. So one way to do that is to connect them up with a line. Pretty bloody hard to read. So if you take this exact thing but just add this simple thing. Set the color to be related to the year of the date and then bang. Now you can see by following the color exactly how this is ordered and so you can see we've got his one end here and his one end here. So ggplot2 again has done fantastic things to make us understand this data more easily. One other thing I will mention is carrot. How many people here have used the carrot package? So I'm not going to show you carrot but I will tell you this. If you go into R and you type some model equals train on my data, create an svn, that's what a command carrot looks like. It's got a command called train and you can pass in a string which is any of 300 different, I think it's about 300 different possible models, classification and regression models and then you can add various things in here about saying I want you to center the data first please and I'll do a PCA on it first please and it just, you know, it kind of puts all of the pieces together. It can do things like remove columns from the data which hardly vary at all and therefore use some modeling. You can do that automatically. It can automatically remove columns from the data that are hardly linear but most powerfully it's got this wrapper that basically lets you take any of hundreds and hundreds of R's most powerful algorithms, really hard to use and they all now can be done through one command and here's the cool bit, right? Imagine we're doing an svn. I don't know how many of you have tried to do svns but they're really hard to get a good result because they depend so much on the parameters. In this version it automatically does a grid search to automatically find the best parameters so you just create one command and it does svn for you. So you definitely should be using a character. There's one more thing in the toolbox I wanted to mention which is you need to use some kind of version control tool. How many people here have used a version control tool like git, cbs, svn? So let me give you an example from our terrific designer at Kaggle. He's recently been changing some of the HTML on our site and he checked into this version control tool that we use and it's so nice, right? Because I can go back to any file now and I can see exactly what was changed and when I can go through and I can say, okay, I remember that thing broke at about this time. What changed? Oh, I think it was this file here. Okay, that line was deleted. This line was changed. This section of this line was changed. Okay. And you can see with my version control tool it's keeping track of everything I can do. Can you see how powerful this is for modeling? Because you go back through your submission history at Kaggle and you say, oh, shit, I used to be getting 0.97 AUC. Now I'm getting 0.93. I'm sure I'm doing everything the same. Go back into your version control tool and have a look at the history, so the commits list and you can go back to the date where Kaggle shows you that you had a really shit hot result and you can't now remember how the hell you did it. And you go back to that date and you go, oh yeah, it's this one here. And you go and you have a look and you see what changed. And it can do all kinds of cool stuff like it can merge back in results from earlier pushes or you can undo the change you made between these two dates, so on and so forth. And most importantly, at the end of the competition when you win and Anthony sends you an email, it's fantastic, send us your winning model and you go, oh, I don't have the winning model anymore. No problem. You can go back into your version control tool and ask for it as it was on the day that you had that fantastic answer. So there's my toolkit. There's quite a lot of other things I wanted to show you, but I don't have time to do. So what I'm going to do is I'm going to jump to this interesting one, which was about predicting which grants would be successful or unsuccessful at the University of Melbourne based on data about the people involved in the grant and all kinds of metadata about the application. This one's interesting because I won it by a fair margin. The 0.967 is kind of 25% of the available error. It's interesting to think what did I do right this time and how did I set this up? Basically what I did in this was I used a random forest. So I'm going to show you guys a bit about random forests. What's also interesting in this is I didn't use R at all. That's not to say that R couldn't have come up with a pretty interesting answer. The guy who came second in this comp used SAS, but I think he used like 12 gig of RAM, multi-core, huge thing. Mine ran on my laptop in two seconds. So I'll show you an approach which is very efficient as well as being very powerful. So I did this all in C sharp and the reason that I didn't use R for this is because the data was kind of complex. Each grant had a whole bunch of people attached to it. It was done in a denormalized form. I don't know how many of you guys are familiar with kind of normalization strategies, but basically denormalized form basically means you had a whole bunch of information about the grant, you know, kind of the dates and blah, blah, blah. And then there was a whole bunch of columns about person one. Did they have a PhD? And then there's a whole bunch of columns about person two, right? And so forth for I think it was about 13 people. Very, very difficult to model this extremely wide and extremely, you know, messy data set. It's the kind of thing that general purpose computing tools are pretty good at. So I pulled this into C sharp and created a grants data class where basically I went, okay, read through this file and I created this thing called grants data and for each line I split it on a comma and I added that grant to this grants data. For those people who maybe aren't so familiar with general purpose computing, general purpose programming languages, you might be surprised to see how readable they are. You know, this idea I can say for each something in lines dot select the lines split by comma. You know, if you haven't used anything with portrait, you might be surprised that something like C sharp looks so easy. File dot read lines dot skip some lines. This is just to skip the first line of the header. And in fact, later on I discovered the first couple of years of data were not very predictive of today. So I actually skipped all of those. And the other nice thing about these kind of tools is, okay, what does this dot add do? I can whack one button and bang, I'm into the definition of dot add. You know, these kind of IDE features are really helpful. And this is equally true of most Python and Java and C++ editors as well. So the kind of stuff that I was able to do here was to create all kinds of interesting derived variables. Like here's one called max year birth. So this one is one that goes through all of the people on this application and finds the one with the largest year of birth. Okay, again, it's just a single line of code. If you kind of get around the kind of curly brackets and other things like that, the actual logic is extremely easy to understand, you know, things like do any of them have a PhD? Well, if there's no people in it, none of them do. Otherwise, oh, this is just one person has a PhD down here somewhere I've got. I can find it. Any has a PhD. Bang, straight to there. There you go. Does any person have a PhD? So I create all these different derived fields. I use pivot tables to kind of work out which ones seem to be quite predictive before I put these together. And so what did I do with this? Well, I wanted to create a random forest. Now random forests in are a very powerful, very general purpose tool. But the R implementation of them has some pretty nasty limitations. For example, if you have a categorical variable, in other words, a factor, it can't have any more than 32 levels. If you have a continuous variable, so like an integer or a double or whatever, it can't have any nulls. So there are these nasty limitations that make it quite difficult. It's particularly difficult to use in this case because things like the RFCD codes have hundreds and hundreds of levels. And all the continuous variables were full of nulls. And in fact, if I remember correctly, even the factors aren't allowed to have nulls, which I find a bit weird because to me, null is just another factor. They're male or they're female or they're unknown. It's still something I should get a model on. So I created a system that basically made it easy for me to create a data set up model on. So I made this decision. I decided that for doubles that had nulls in them, I created something which basically simply added two columns. One column which was, is that column null or not? One or zero? And another column which is the actual data from that column, so whatever it was, 2.3, that one's a null, sorry, 2.36, blah, blah, blah, blah. And wherever there was a null, I just replaced it with the median. So I now had two columns where I used to have one and both of them are now modelable. Why is that, why the median? Actually it doesn't matter because every place where this is the median, there's a one over here. So in my model, I'm going to use this as a predictor. I'm going to use this as a predictor. So if all of the places that that data column was originally null all meant something interesting, then it'll be picked up by this is null version of the column. So to me, this is something which I do, which I did automatically because it's clearly the obvious way to deal with null values. And then as I said in the categorical variables, I just said, okay, the factors, if there's a null, just treat it as another level. And then finally in the categorical, in the factors, I said, okay, take all of the levels and if there are more observations than, I think it was 25, then keep it. Or maybe it's more than that. I think if there's more levels, maybe if there's more observations than 100, then keep it. If there's more observations than 25, less than 100. And it was quite predictive, in other words, that level was different to the others in terms of application success, then keep it. Otherwise, merge all the rest into one super level called the rest. So that way I basically was able to create a data set which actually I could then fit into R, although I think in this case I actually ended up using my own random forest implementation. So should we have a quick talk about random forests and how they work? So to me, there's kind of basically two main types of model, right? There's these kind of parametric models, models with parameters, things where you say, oh, this bit's linear and this bit's interactive with this bit and this bit's kind of logarithmic and I specify how I think this system looks. And all the modeling tool does is it fills in parameters. Okay, this is the slope of that linear bit. This is the slope of that logarithmic bit. This is how these two interact. So things like GLM, very well known parametric tools. Then there are these kind of non-parametric or semi-parametric models which are things where I don't do any of that. I just say, here's my data. I don't know how it's related to each other. Just build a model. And so things like support vector machines, neural nets, random forests, decision trees all have that kind of flexibility. Non-parametric models are not necessarily better than parametric models. Think back to that example of the R package competition where really all I wanted was some weights to say how does this kind of this max column relate? And if all you really wanted some weights, all you wanted some parameters and so GLM's perfect. GLM's certainly can overfit, but there are ways of creating GLM's that don't. For example, you can use stepwise regression or the much more fancy modern version, you can use GLM net, which is basically another tool for doing GLM's which doesn't overfit. But anytime you don't really know what the model form is, this is where you'd use a non-parametric tool. Random forests are great because they're super, super fast and extremely flexible and they don't really have any parameters in the attitude so they're pretty hard to get it wrong. So let me show you how that works. A random forest is simply, in fact we shouldn't even use this term random forest because a random forest is a trademark term. So we will call it a ensemble of decision trees. And in fact the trademark term random forest, I think that was 2001, that wasn't where this ensemble of decision trees was invented. It goes all the way back to 1995. In fact it was actually kind of independently developed by three different people in 95, 96 and 97. The random forest implementation is really just one way of doing it. It all rests on a really fascinating observation, which is that if you have a model that is really, really, really shit but it's not quite random, it's slightly better than nothing. And if you've got 10,000 of these models that are all different to each other and they're all shit in different ways but they're all better than nothing, the average of those 10,000 models will actually be fantastically powerful as a model of its own. So this is the wisdom of crowds or ensemble learning techniques. You can kind of see why, right? Because if out of these 10,000 models they're all kind of crap in different ways, they're all a bit random, right? They're all a bit less better than nothing. 9,999 of them might basically be useless but one of them just happened upon the true structure of the data. So the other 9,999 will kind of average out if they're unbiased, not correlated with each other, they'll all average out to whatever the average of the data is. So any difference in the predictions of this ensemble will all come down to that one model which happened to have actually figured it out right. Now that's an extreme version, right? But that's basically the concept behind all these ensemble techniques. And if you want to invent your own ensemble technique, all you have to do is come up with some learner, some underlying model which is, which you can randomize in some way and each one will be a bit different and you run it lots of times. And generally speaking this whole approach we call random subspace. So random subspace techniques, let me show you how unbelievably easy this is. Take any model, any kind of modeling algorithm you like. Here's our data. Here's all the rows. Here's all the columns. Okay, I'm now going to create a random subspace. Some of the columns, some of the rows. Okay, so let's now build a model using that subset of rows and that subset of columns. It's not going to be as perfect at recognizing the training data as using the full lot, but you know, it's one way of building a model. Let's say I build a second model. This time I'll use this subspace, a different set of rows and a different set of columns. No, absolutely not. But I didn't want to draw 4,000 lines, so let's pretend. Yeah, so in fact what I'm really doing each time here is I'm pulling out a bunch of random rows and a bunch of random columns. You have a random something that goes both ways. Correct. And this is a random subspace. It's just one way of creating a random subspace, but it's a nice easy one. And because I didn't do very well at linear algebra, in fact I'm just a philosophy graduate and I don't know any linear algebra, I don't know what subspace means well enough to do it properly, but this certainly works. And this is all decision trees do. So now imagine that we're going to do this and for each one of these different random subspaces we're going to build a decision tree. How do we build a decision tree? Easy. Let's create some data. So let's say we've got age, sex, is smoker, and lung capacity. And we're trying to predict people's lung capacity. So we've got a whole bunch of data there. Dirty, male, yes, whatever. So to build a decision tree, let's assume that this is the particular subset of columns and rows in a random subspace. So let's build a decision tree. So to build a decision tree, what I do is I say, okay, on which variable, on which predictor, and at which point of that predictor can I do a single split which makes the biggest difference possible in my dependent variable? So it might turn out that if I looked at his smoker, yes and no, that the average lung capacity for all of the smokers might be 30 and the average for all of the non-smokers might be 70. So literally all I've done is I've just gone through each of these and calculated the average for the two groups. And I found the one split that makes that as big a difference as possible. And then I keep doing that. So in those people that are non-smokers, I now, interestingly, with the random forest or these decision tree ensemble algorithms, generally speaking, at each point, I select a different group of columns. So I randomly select a new group of columns, but I'm going to use the same rows. I obviously have to use the same rows because I'm kind of taking them down the tree. So now it turns out that if we look at age amongst the people that are non-smokers, if you're less than 18 versus greater than 18 is the number one biggest thing in this random subspace that makes the difference. And that's like 50 and that's like 80. And so this is how I create a decision tree. Okay. So at each point, I've taken a different random subset of columns. For the whole tree, I've used the same random subset of rows. And at the end of that, I keep going until every one of my leaves either has only one or two data points left or all of the data points at that leaf all have exactly the same outcome, the same lung capacity, for example. And at that point, I finished making my decision tree. So now I put that aside and I say, okay, that is decision tree number one. Put that aside and now go back and take a different set of rows and repeat the whole process. And that gives me decision tree number two. And I do that a thousand times, whatever. And at the end of that, I've now got a thousand decision trees. And for each thing I want to predict, I then stick that thing I want to predict down every one of these decision trees. So the first thing I'm trying to predict might be a non-smoker who's 16 years old, blah, blah, blah, blah, blah. And that gives me a prediction. So the predictions for these things at the very bottom is simply what's the average of the dependent variable, in this case, the lung capacity for that group. So that gives me 50 in decision tree one and it might be 30 in decision tree two and 14 in decision tree three. I just take the average on all of those. And that's given me what I wanted, which is a whole bunch of independent, unbiased, not completely crap models. How not completely crap are they? Well, the nice thing is we can pick, right? If you want to be super cautious and you really need to make sure you're avoiding overfitting, then what you do is you make sure your random subspaces are smaller. You pick less rows and less columns so that then each tree is shitter than average. Where else if you want to be quick, you make each one have more rows and more columns so it better reflects the true data that you've got. Obviously the less rows and less columns you have each time, the less powerful each tree is and therefore the more trees you need. And the nice thing about this is that building each of these trees takes like a 10,000th of a second or less. It depends on how much data you've got, but you can build thousands of trees in a few seconds are the kind of data sets I look at. So generally speaking, this isn't an issue. And here's the really cool thing. In this tree, I built it with these rows, which means that these rows I didn't use to build my tree, which means these rows are out of sample for that tree. And what that means is I don't need to have a separate cross validation data set. What it means is I can create a table now of my full data set and for each one I can say okay row number one, how good am I at predicting row number one? Well here's all of my trees from one to a thousand. Row number one is in fact one of the things that was included when I created tree number one. So I won't use it here, but row number one wasn't included when I built tree two. It wasn't included in the random subspace for tree three and it was included in the one before. So what I do is row number one, I send down trees two and three and I get predictions for everything that it wasn't in and average them out and that gives me this fantastic thing which is an out of band estimate for row one. And I do that for every row. So none of this, all of this stuff which is being predicted here is actually not using any of the data that was used to build the trees and therefore it is truly out of sample or out of band and therefore when I put this all together to create my final whatever it is AUC or log likelihood or R-square or SSE or whatever and then I send that off to Kaggle. Kaggle should give you pretty much the same answer because you're by definition not over fitting. Yes you do. I was just wondering if it is possible to say just pick a tree that has the best, instead of averaging out in those cases, is it possible to just pick that one tree that actually has the best performance and would you recommend it? Yes you can, no I wouldn't. So the question was can you just pick one tree and would that tree, picking that one tree be better than what we've just done? And let's think about what, that's a really important question and let's think about why that won't work. The whole purpose of this was to not over fit. So the whole purpose of this was to say each of these trees is pretty crap but it's better than nothing and so we average them all out, it tells us something about the true data, each one can't over fit on its own. If I now go back and do anything to those trees, if I try and prune them, which is in the old fashioned decision tree algorithms, or if I weight them or if I pick a subset of them, I'm now introducing bias based on the training set predictivity. So anytime I introduce bias, I now break the laws of ensemble methods fundamentally. So the other thing I'd say is there's no point, right? Because if you have something where actually you've got so much training data that out of sample isn't a big problem or whatever, you just use bigger subspaces and less trees. And in fact, the only reason you do that is for time and because this approach is so fast anyway, I wouldn't even bother then, you see. And the nice thing about this is, is that you can say, okay, I'm going to use kind of this many columns and this many rows in each subspace, right? And I've got to start building my trees and I build tree number one and I get this out of band error. Tree number two, this out of band error. Tree number three, this out of band error. And the nice thing is I can watch and see and it will be monotonic. Well, not exactly monotonic, but kind of bumping monotonic. It will keep getting better on average. And I can get to a point where I say, okay, that's good enough, I'll stop. And as I say, normally it's talking four or five seconds. It's just time's not an issue. But if you're talking about huge data sets, you can't sample them. This is a way you can watch it go. So this is a technique that I used in the grants prediction competition. I did a bunch of things to make it even more random than this. One of the big problems here, both in terms of time and lack of randomness, is that all of these continuous variables, the official random forest algorithm searches through every possible breakpoint to find the very best, which means that every single time that you use that particular variable, particularly if it's in the same spot, like at the top of the tree, it's going to do the same split. In the version I wrote, actually all it does is every time it comes across a continuous variable, it randomly picks three breakpoints. So it might try 50, 70, and 19. And it just finds the best of those three. And to me, this is the secret of good ensemble algorithms, is to make every one as different to every other tree as possible. Does the distribution of the population variable you're trying to predict matter? No, not at all. So the question was, does the distribution of the dependent variable matter? And the answer is it doesn't. And the reason it doesn't is because we're using a tree. So the nice thing about a tree is, let's imagine that the dependent variable was maybe very long tail distribution, like so. The nice thing is that as it looks at the independent variables, it's looking at the difference in two groups and trying to find the biggest difference between those two groups. So regardless of the distribution, it's more like a rank measure, isn't it? It's picked a particular breakpoint, and it's saying which one finds the biggest difference between the two groups. So regardless of the distribution of the dependent variable, it's still going to find the same breakpoints because it's really a non-parametric measure. We're using something like, for example, Gini or some kind of other measure of the information gain of that to build the decision tree. So this is true of really all decision tree approaches, in fact. Does it work with the highly imbalanced data set? Yes and no. So the question is, does it work for a highly imbalanced data set? Sometimes some versions can, and some versions can't. The approaches which use more randomization are more likely to work okay. But the problem is, in highly imbalanced data sets, you can quite quickly end up with nodes which are all the same value. So I actually have often found I get better results if I do some stratified sampling. So that, for example, think about the R competition where most people don't have in store, other than user number five, 99% of packages. So in that case, I tend to say, all right, at least half of that data set is so obviously zero. Let's just call it zero and just work with the rest. I do find I often get better answers, but it does depend. Would it be better to, instead of using random tree for the process, use another algorithm in the forest? Or the common balance? Well, you can't call it forests if you use a different algorithm other than a tree. Yes, you can use other random subspace methods. You can use another class five. Yes, you absolutely can. A lot of people have been going down that path. It would have to be fast. So GLMnet would be a good example because that's very fast. But GLMnet is parametric. The nice thing about decision trees is that they're totally flexible. They don't assume any particular data structure. They kind of are almost unlimited in the amount of interactions they can handle. And you can build thousands of them very quickly. But there are certainly people who are creating other types of random subspace ensemble methods and, I believe, some of them are quite effective. Interestingly, I can't remember where I saw it, but I have seen some papers which show evidence that it doesn't really matter. If you've got a truly flexible underlying model and you make it random enough and you create enough of them, it doesn't really matter which one you use or how you do it, which is a nice result. It kind of suggests that we don't have to spend lots of time trying to come up with better and better and better predictive modeling, generic predictive modeling tools. If you think about things, there are better versions of this, in quotes, like a rotation forest and then there's things like GBMs, gradient boosting machines and so forth. In practice, they can be faster for certain types of situation. But the general result here is that these ensemble methods are as flexible as you need them to be. Right at the back. How do you define the optimal size of the subspace? So the question is how do you define the optimal size of the subspace? And that's a really terrific question. The answer to it is really nice and it's that you really don't have to. Generally speaking, the less rows and the less columns you use, the more trees you need, the less you'll overfit and the better results you'll get. The nice thing normally is that for most data sets, because of the speed of random forest, you can pretty much always pick a row count and a column count that's small enough that you're absolutely sure it's going to be fine. Sometimes it can become an issue. Maybe you've got really huge data sets or maybe you've got really big problems with data imbalances or hardly any training data. And in these cases, you can use the kind of approaches which would be familiar to most of us around creating a grid of a few different values of the column count and the row count and trying a few out and watching that graph of as you add more trees, how does it improve? But the truth is it's so unsensitive to this that if you pick a number of columns of somewhere between 10% and 50% of the total and a number of rows of between 10% and 50% of the total, and you'll be fine. And then you just keep adding more trees until either you're sick of waiting or it's obviously flat or if you do a thousand trees, again, these are all, it really doesn't matter. It's not sensitive to that assumption on the whole. The R routine samples with replacement. Yeah, the R routine actually, the R routine actually, so this idea of a random subspace, there are different ways of creating this random subspace. And one key one is can I go out and pull out a row again that I've already pulled out before? The R random forest and the portrait encoder which is based by default let you pull something out multiple times and by default in fact pull out if you've got n rows, it will pull out n rows. But because it's pulled out some multiple times on average it will cover I think 63.2% of the rows. I don't have the best results when I use that, but it doesn't matter because in R random forest options you can choose is it with or without sampling and how many tables is in. Yes, I absolutely think it makes a difference. Yes to me, I'm sure it depends on the data set, but you know, I guess I always enter Kaggle competitions which are in areas that I've never entered before kind of domain wise or algorithm wise. So I guess I'd be getting a good spread of different types of situation and in the ones I've looked at sampling without replacement is kind of more random and I also tend to pick much lower n than 63.2%. I tend to use more like 10 or 20% of the data in my random subspecies. So I guess I can say that's my experience, but I'm sure it depends on the data set and I'm not sure it's terribly sensitive to it anyway. I always put it into two branches. So there's a few possibilities here as you split in your decision tree. In this case, I've got something here which is a binary variable. So obviously that has to be fit into two. In this case, I've got something here which is a continuous variable. Now I split it into two, but if actually it's going to be optimal to split it into three, then if the variable appears again at the next level, it can always split it into another two at that other split point. I can. Absolutely I can. So it just depends whether when I did that, remember at every level I repeat the sampling of a different bunch of columns. I could absolutely have the same column again in that group and it could so happen that again I find the split point which is the best in that group. If you're doing 10,000 trees with 100 levels each, it's going to happen lots of times. So the nice thing is that if the true underlying system is a single univariate logarithmic relationship, these trees will absolutely find that eventually. Do you care about pruning the trees? Definitely don't prune the trees. If you prune the trees, you introduce bias. So the key thing here which makes this so fast and so easy but also so powerful is you don't prune the trees. The other thing is that the serum is usually not going to be able to do that. So it doesn't necessarily because your split point will be such that the two halves will not necessarily be balanced in count. In 17 not in 15. Yeah that's right because in the under 18 group you could have not that many people and in the over 18 group you can have quite a lot of people so the weighted average of the two will come to 17. Have you ever compared this with Gradient Boosting Machines? With Gradient Boosting Machines? Yeah. Yeah absolutely I have. Gradient Boosting Machines are interesting. They're a lot harder to understand. Gradient Boosting Machines, I mean they're still basically ensemble technique and they're more working with kind of the residuals of previous models. There's a few pieces of theory around Gradient Boosting Machines which are nicer than random forests. They ought to be faster and they ought to be more well directed and you can do things like say with a Gradient Boosting Machine this particular column has a monotonic relationship with a dependent variable so you can actually add constraints in which you can't do with random forests. In my experience I don't need the extra speed of GBMs because I just never have found it necessary. I find them harder to, they've got more parameters to deal with so I haven't found them useful for me and I know a lot of data mining competitions and also a lot of real world predictive modeling problems people try both and end up with random forests. Well we're probably just about out of time so maybe if there's any more questions I can chat to you guys afterwards. Thanks very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.6, "text": " Okay, so we're here at the Melbourne R meetup and we are talking about some techniques that", "tokens": [1033, 11, 370, 321, 434, 510, 412, 264, 27496, 497, 1677, 1010, 293, 321, 366, 1417, 466, 512, 7512, 300], "temperature": 0.0, "avg_logprob": -0.17501583384044134, "compression_ratio": 1.403225806451613, "no_speech_prob": 0.010320295579731464}, {"id": 1, "seek": 0, "start": 9.6, "end": 17.96, "text": " Jeremy Howard has used to do as well as he can in a variety of Kaggle competitions.", "tokens": [17809, 17626, 575, 1143, 281, 360, 382, 731, 382, 415, 393, 294, 257, 5673, 295, 48751, 22631, 26185, 13], "temperature": 0.0, "avg_logprob": -0.17501583384044134, "compression_ratio": 1.403225806451613, "no_speech_prob": 0.010320295579731464}, {"id": 2, "seek": 0, "start": 17.96, "end": 26.2, "text": " And we're going to start by having a look at some of the tools that I've found useful", "tokens": [400, 321, 434, 516, 281, 722, 538, 1419, 257, 574, 412, 512, 295, 264, 3873, 300, 286, 600, 1352, 4420], "temperature": 0.0, "avg_logprob": -0.17501583384044134, "compression_ratio": 1.403225806451613, "no_speech_prob": 0.010320295579731464}, {"id": 3, "seek": 2620, "start": 26.2, "end": 31.52, "text": " in predictive modelling in general and in Kaggle competitions in particular.", "tokens": [294, 35521, 42253, 294, 2674, 293, 294, 48751, 22631, 26185, 294, 1729, 13], "temperature": 0.0, "avg_logprob": -0.13347770812663626, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.0003293021582067013}, {"id": 4, "seek": 2620, "start": 31.52, "end": 37.56, "text": " So I've tried to write down here what I think are some of the key steps.", "tokens": [407, 286, 600, 3031, 281, 2464, 760, 510, 437, 286, 519, 366, 512, 295, 264, 2141, 4439, 13], "temperature": 0.0, "avg_logprob": -0.13347770812663626, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.0003293021582067013}, {"id": 5, "seek": 2620, "start": 37.56, "end": 46.84, "text": " So after you download data from a Kaggle competition, you end up with CSV files, generally speaking,", "tokens": [407, 934, 291, 5484, 1412, 490, 257, 48751, 22631, 6211, 11, 291, 917, 493, 365, 48814, 7098, 11, 5101, 4124, 11], "temperature": 0.0, "avg_logprob": -0.13347770812663626, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.0003293021582067013}, {"id": 6, "seek": 2620, "start": 46.84, "end": 49.84, "text": " CSV files, which can be in all kinds of formats.", "tokens": [48814, 7098, 11, 597, 393, 312, 294, 439, 3685, 295, 25879, 13], "temperature": 0.0, "avg_logprob": -0.13347770812663626, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.0003293021582067013}, {"id": 7, "seek": 2620, "start": 49.84, "end": 54.56, "text": " So here's the first thing you see when you open up the time series CSV file.", "tokens": [407, 510, 311, 264, 700, 551, 291, 536, 562, 291, 1269, 493, 264, 565, 2638, 48814, 3991, 13], "temperature": 0.0, "avg_logprob": -0.13347770812663626, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.0003293021582067013}, {"id": 8, "seek": 5456, "start": 54.56, "end": 57.160000000000004, "text": " It's not very hopeful, is it?", "tokens": [467, 311, 406, 588, 20531, 11, 307, 309, 30], "temperature": 0.0, "avg_logprob": -0.2396749524927851, "compression_ratio": 1.4470588235294117, "no_speech_prob": 0.00015591295959893614}, {"id": 9, "seek": 5456, "start": 57.160000000000004, "end": 63.64, "text": " So each of these columns is actually, oh here we come, is actually quarterly time series", "tokens": [407, 1184, 295, 613, 13766, 307, 767, 11, 1954, 510, 321, 808, 11, 307, 767, 38633, 565, 2638], "temperature": 0.0, "avg_logprob": -0.2396749524927851, "compression_ratio": 1.4470588235294117, "no_speech_prob": 0.00015591295959893614}, {"id": 10, "seek": 5456, "start": 63.64, "end": 69.4, "text": " data.", "tokens": [1412, 13], "temperature": 0.0, "avg_logprob": -0.2396749524927851, "compression_ratio": 1.4470588235294117, "no_speech_prob": 0.00015591295959893614}, {"id": 11, "seek": 5456, "start": 69.4, "end": 76.32000000000001, "text": " And so because, well for various reasons, it's kind of each one's different lengths", "tokens": [400, 370, 570, 11, 731, 337, 3683, 4112, 11, 309, 311, 733, 295, 1184, 472, 311, 819, 26329], "temperature": 0.0, "avg_logprob": -0.2396749524927851, "compression_ratio": 1.4470588235294117, "no_speech_prob": 0.00015591295959893614}, {"id": 12, "seek": 5456, "start": 76.32000000000001, "end": 79.64, "text": " and they kind of start further along.", "tokens": [293, 436, 733, 295, 722, 3052, 2051, 13], "temperature": 0.0, "avg_logprob": -0.2396749524927851, "compression_ratio": 1.4470588235294117, "no_speech_prob": 0.00015591295959893614}, {"id": 13, "seek": 7964, "start": 79.64, "end": 85.8, "text": " The particular way that this was provided didn't really suit the tools that I was using.", "tokens": [440, 1729, 636, 300, 341, 390, 5649, 994, 380, 534, 5722, 264, 3873, 300, 286, 390, 1228, 13], "temperature": 0.0, "avg_logprob": -0.16651077892469324, "compression_ratio": 1.7181818181818183, "no_speech_prob": 3.590646156226285e-05}, {"id": 14, "seek": 7964, "start": 85.8, "end": 90.68, "text": " And in fact, if I remember correctly, that's already, I've already adjusted it slightly", "tokens": [400, 294, 1186, 11, 498, 286, 1604, 8944, 11, 300, 311, 1217, 11, 286, 600, 1217, 19871, 309, 4748], "temperature": 0.0, "avg_logprob": -0.16651077892469324, "compression_ratio": 1.7181818181818183, "no_speech_prob": 3.590646156226285e-05}, {"id": 15, "seek": 7964, "start": 90.68, "end": 95.0, "text": " because it originally came in rows rather than columns.", "tokens": [570, 309, 7993, 1361, 294, 13241, 2831, 813, 13766, 13], "temperature": 0.0, "avg_logprob": -0.16651077892469324, "compression_ratio": 1.7181818181818183, "no_speech_prob": 3.590646156226285e-05}, {"id": 16, "seek": 7964, "start": 95.0, "end": 97.04, "text": " Yeah, that's right.", "tokens": [865, 11, 300, 311, 558, 13], "temperature": 0.0, "avg_logprob": -0.16651077892469324, "compression_ratio": 1.7181818181818183, "no_speech_prob": 3.590646156226285e-05}, {"id": 17, "seek": 7964, "start": 97.04, "end": 99.76, "text": " This is how it originally came in rows rather than columns.", "tokens": [639, 307, 577, 309, 7993, 1361, 294, 13241, 2831, 813, 13766, 13], "temperature": 0.0, "avg_logprob": -0.16651077892469324, "compression_ratio": 1.7181818181818183, "no_speech_prob": 3.590646156226285e-05}, {"id": 18, "seek": 7964, "start": 99.76, "end": 105.32, "text": " So this is where this kind of data manipulation toolbox comes in.", "tokens": [407, 341, 307, 689, 341, 733, 295, 1412, 26475, 44593, 1487, 294, 13], "temperature": 0.0, "avg_logprob": -0.16651077892469324, "compression_ratio": 1.7181818181818183, "no_speech_prob": 3.590646156226285e-05}, {"id": 19, "seek": 10532, "start": 105.32, "end": 109.75999999999999, "text": " There's all kinds of ways to swap rows and columns around, which is where I started.", "tokens": [821, 311, 439, 3685, 295, 2098, 281, 18135, 13241, 293, 13766, 926, 11, 597, 307, 689, 286, 1409, 13], "temperature": 0.0, "avg_logprob": -0.12665561919516705, "compression_ratio": 1.5475113122171946, "no_speech_prob": 1.6187001165235415e-05}, {"id": 20, "seek": 10532, "start": 109.75999999999999, "end": 116.36, "text": " The really simple approach is to select the whole lot, copy it, and then paste it and", "tokens": [440, 534, 2199, 3109, 307, 281, 3048, 264, 1379, 688, 11, 5055, 309, 11, 293, 550, 9163, 309, 293], "temperature": 0.0, "avg_logprob": -0.12665561919516705, "compression_ratio": 1.5475113122171946, "no_speech_prob": 1.6187001165235415e-05}, {"id": 21, "seek": 10532, "start": 116.36, "end": 119.19999999999999, "text": " say transpose in Excel.", "tokens": [584, 25167, 294, 19060, 13], "temperature": 0.0, "avg_logprob": -0.12665561919516705, "compression_ratio": 1.5475113122171946, "no_speech_prob": 1.6187001165235415e-05}, {"id": 22, "seek": 10532, "start": 119.19999999999999, "end": 122.41999999999999, "text": " And that's one way to do it.", "tokens": [400, 300, 311, 472, 636, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.12665561919516705, "compression_ratio": 1.5475113122171946, "no_speech_prob": 1.6187001165235415e-05}, {"id": 23, "seek": 10532, "start": 122.41999999999999, "end": 131.76, "text": " And then having done that, I ended up with something which I could open up in.", "tokens": [400, 550, 1419, 1096, 300, 11, 286, 4590, 493, 365, 746, 597, 286, 727, 1269, 493, 294, 13], "temperature": 0.0, "avg_logprob": -0.12665561919516705, "compression_ratio": 1.5475113122171946, "no_speech_prob": 1.6187001165235415e-05}, {"id": 24, "seek": 10532, "start": 131.76, "end": 133.48, "text": " Let's have a look at the original file.", "tokens": [961, 311, 362, 257, 574, 412, 264, 3380, 3991, 13], "temperature": 0.0, "avg_logprob": -0.12665561919516705, "compression_ratio": 1.5475113122171946, "no_speech_prob": 1.6187001165235415e-05}, {"id": 25, "seek": 13348, "start": 133.48, "end": 140.35999999999999, "text": " This is the original file in Vim, which is my text editor of choice.", "tokens": [639, 307, 264, 3380, 3991, 294, 691, 332, 11, 597, 307, 452, 2487, 9839, 295, 3922, 13], "temperature": 0.0, "avg_logprob": -0.15700069655719986, "compression_ratio": 1.6121673003802282, "no_speech_prob": 9.516030331724323e-06}, {"id": 26, "seek": 13348, "start": 140.35999999999999, "end": 145.32, "text": " This is actually a really good time to get rid of all those kind of leading commas because", "tokens": [639, 307, 767, 257, 534, 665, 565, 281, 483, 3973, 295, 439, 729, 733, 295, 5775, 800, 296, 570], "temperature": 0.0, "avg_logprob": -0.15700069655719986, "compression_ratio": 1.6121673003802282, "no_speech_prob": 9.516030331724323e-06}, {"id": 27, "seek": 13348, "start": 145.32, "end": 148.01999999999998, "text": " they kind of confuse me.", "tokens": [436, 733, 295, 28584, 385, 13], "temperature": 0.0, "avg_logprob": -0.15700069655719986, "compression_ratio": 1.6121673003802282, "no_speech_prob": 9.516030331724323e-06}, {"id": 28, "seek": 13348, "start": 148.01999999999998, "end": 150.39999999999998, "text": " So this is where stuff like Vim is great.", "tokens": [407, 341, 307, 689, 1507, 411, 691, 332, 307, 869, 13], "temperature": 0.0, "avg_logprob": -0.15700069655719986, "compression_ratio": 1.6121673003802282, "no_speech_prob": 9.516030331724323e-06}, {"id": 29, "seek": 13348, "start": 150.39999999999998, "end": 154.79999999999998, "text": " Even things like Notepad++ and Emacs and any of these kind of power user text editors will", "tokens": [2754, 721, 411, 1726, 595, 345, 25472, 293, 3968, 44937, 293, 604, 295, 613, 733, 295, 1347, 4195, 2487, 31446, 486], "temperature": 0.0, "avg_logprob": -0.15700069655719986, "compression_ratio": 1.6121673003802282, "no_speech_prob": 9.516030331724323e-06}, {"id": 30, "seek": 13348, "start": 154.79999999999998, "end": 155.94, "text": " work fine.", "tokens": [589, 2489, 13], "temperature": 0.0, "avg_logprob": -0.15700069655719986, "compression_ratio": 1.6121673003802282, "no_speech_prob": 9.516030331724323e-06}, {"id": 31, "seek": 13348, "start": 155.94, "end": 161.07999999999998, "text": " As long as you know how to use regular expressions, and if you don't, I'm not going to show you", "tokens": [1018, 938, 382, 291, 458, 577, 281, 764, 3890, 15277, 11, 293, 498, 291, 500, 380, 11, 286, 478, 406, 516, 281, 855, 291], "temperature": 0.0, "avg_logprob": -0.15700069655719986, "compression_ratio": 1.6121673003802282, "no_speech_prob": 9.516030331724323e-06}, {"id": 32, "seek": 16108, "start": 161.08, "end": 163.64000000000001, "text": " now, but you should definitely look it up.", "tokens": [586, 11, 457, 291, 820, 2138, 574, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.33184683663504466, "compression_ratio": 1.5610859728506787, "no_speech_prob": 7.253855437738821e-05}, {"id": 33, "seek": 16108, "start": 163.64000000000001, "end": 167.4, "text": " So in this case, I'm just going to go, okay, let's use a regular expression.", "tokens": [407, 294, 341, 1389, 11, 286, 478, 445, 516, 281, 352, 11, 1392, 11, 718, 311, 764, 257, 3890, 6114, 13], "temperature": 0.0, "avg_logprob": -0.33184683663504466, "compression_ratio": 1.5610859728506787, "no_speech_prob": 7.253855437738821e-05}, {"id": 34, "seek": 16108, "start": 167.4, "end": 173.88000000000002, "text": " So I say, yes, the substitute on the whole file, start with any number of commas and", "tokens": [407, 286, 584, 11, 2086, 11, 264, 15802, 322, 264, 1379, 3991, 11, 722, 365, 604, 1230, 295, 800, 296, 293], "temperature": 0.0, "avg_logprob": -0.33184683663504466, "compression_ratio": 1.5610859728506787, "no_speech_prob": 7.253855437738821e-05}, {"id": 35, "seek": 16108, "start": 173.88000000000002, "end": 176.16000000000003, "text": " replace it with nothing at all.", "tokens": [7406, 309, 365, 1825, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.33184683663504466, "compression_ratio": 1.5610859728506787, "no_speech_prob": 7.253855437738821e-05}, {"id": 36, "seek": 16108, "start": 176.16000000000003, "end": 179.96, "text": " And you can see, Vim, I'm done.", "tokens": [400, 291, 393, 536, 11, 691, 332, 11, 286, 478, 1096, 13], "temperature": 0.0, "avg_logprob": -0.33184683663504466, "compression_ratio": 1.5610859728506787, "no_speech_prob": 7.253855437738821e-05}, {"id": 37, "seek": 16108, "start": 179.96, "end": 186.04000000000002, "text": " So I can now save that and I've got a nice easy file that I can start using.", "tokens": [407, 286, 393, 586, 3155, 300, 293, 286, 600, 658, 257, 1481, 1858, 3991, 300, 286, 393, 722, 1228, 13], "temperature": 0.0, "avg_logprob": -0.33184683663504466, "compression_ratio": 1.5610859728506787, "no_speech_prob": 7.253855437738821e-05}, {"id": 38, "seek": 18604, "start": 186.04, "end": 193.12, "text": " So that's why I've listed this idea of data manipulation tools in my toolbox.", "tokens": [407, 300, 311, 983, 286, 600, 10052, 341, 1558, 295, 1412, 26475, 3873, 294, 452, 44593, 13], "temperature": 0.0, "avg_logprob": -0.18527950457672573, "compression_ratio": 1.4917127071823204, "no_speech_prob": 7.76659362600185e-06}, {"id": 39, "seek": 18604, "start": 193.12, "end": 199.39999999999998, "text": " And to me, Vim or some regular expression powered text editor which can handle large", "tokens": [400, 281, 385, 11, 691, 332, 420, 512, 3890, 6114, 17786, 2487, 9839, 597, 393, 4813, 2416], "temperature": 0.0, "avg_logprob": -0.18527950457672573, "compression_ratio": 1.4917127071823204, "no_speech_prob": 7.76659362600185e-06}, {"id": 40, "seek": 18604, "start": 199.39999999999998, "end": 202.51999999999998, "text": " files is something to be familiar with.", "tokens": [7098, 307, 746, 281, 312, 4963, 365, 13], "temperature": 0.0, "avg_logprob": -0.18527950457672573, "compression_ratio": 1.4917127071823204, "no_speech_prob": 7.76659362600185e-06}, {"id": 41, "seek": 18604, "start": 202.51999999999998, "end": 210.92, "text": " So just in case you didn't catch that, that is regular expressions.", "tokens": [407, 445, 294, 1389, 291, 994, 380, 3745, 300, 11, 300, 307, 3890, 15277, 13], "temperature": 0.0, "avg_logprob": -0.18527950457672573, "compression_ratio": 1.4917127071823204, "no_speech_prob": 7.76659362600185e-06}, {"id": 42, "seek": 21092, "start": 210.92, "end": 216.92, "text": " Definitely the most powerful tool for doing text and data manipulation that I know of.", "tokens": [12151, 264, 881, 4005, 2290, 337, 884, 2487, 293, 1412, 26475, 300, 286, 458, 295, 13], "temperature": 0.0, "avg_logprob": -0.17631324561866554, "compression_ratio": 1.7932489451476794, "no_speech_prob": 1.5934983821352944e-05}, {"id": 43, "seek": 21092, "start": 216.92, "end": 220.16, "text": " Sometimes they're just called regexes.", "tokens": [4803, 436, 434, 445, 1219, 319, 432, 47047, 13], "temperature": 0.0, "avg_logprob": -0.17631324561866554, "compression_ratio": 1.7932489451476794, "no_speech_prob": 1.5934983821352944e-05}, {"id": 44, "seek": 21092, "start": 220.16, "end": 225.6, "text": " The most powerful types of regular expressions, I would say, would be the ones that are in", "tokens": [440, 881, 4005, 3467, 295, 3890, 15277, 11, 286, 576, 584, 11, 576, 312, 264, 2306, 300, 366, 294], "temperature": 0.0, "avg_logprob": -0.17631324561866554, "compression_ratio": 1.7932489451476794, "no_speech_prob": 1.5934983821352944e-05}, {"id": 45, "seek": 21092, "start": 225.6, "end": 226.6, "text": " Perl.", "tokens": [3026, 75, 13], "temperature": 0.0, "avg_logprob": -0.17631324561866554, "compression_ratio": 1.7932489451476794, "no_speech_prob": 1.5934983821352944e-05}, {"id": 46, "seek": 21092, "start": 226.6, "end": 228.92, "text": " They've been widely used elsewhere.", "tokens": [814, 600, 668, 13371, 1143, 14517, 13], "temperature": 0.0, "avg_logprob": -0.17631324561866554, "compression_ratio": 1.7932489451476794, "no_speech_prob": 1.5934983821352944e-05}, {"id": 47, "seek": 21092, "start": 228.92, "end": 235.35999999999999, "text": " Any C program that uses the PCRE engine has the same regular expressions as Perl, more", "tokens": [2639, 383, 1461, 300, 4960, 264, 6465, 3850, 2848, 575, 264, 912, 3890, 15277, 382, 3026, 75, 11, 544], "temperature": 0.0, "avg_logprob": -0.17631324561866554, "compression_ratio": 1.7932489451476794, "no_speech_prob": 1.5934983821352944e-05}, {"id": 48, "seek": 21092, "start": 235.35999999999999, "end": 236.35999999999999, "text": " or less.", "tokens": [420, 1570, 13], "temperature": 0.0, "avg_logprob": -0.17631324561866554, "compression_ratio": 1.7932489451476794, "no_speech_prob": 1.5934983821352944e-05}, {"id": 49, "seek": 21092, "start": 236.35999999999999, "end": 239.44, "text": " C sharpen.net have the same regular expressions as Perl, more or less.", "tokens": [383, 31570, 13, 7129, 362, 264, 912, 3890, 15277, 382, 3026, 75, 11, 544, 420, 1570, 13], "temperature": 0.0, "avg_logprob": -0.17631324561866554, "compression_ratio": 1.7932489451476794, "no_speech_prob": 1.5934983821352944e-05}, {"id": 50, "seek": 23944, "start": 239.44, "end": 244.92, "text": " So this is a nice example of one bunch of people getting it right and everybody else", "tokens": [407, 341, 307, 257, 1481, 1365, 295, 472, 3840, 295, 561, 1242, 309, 558, 293, 2201, 1646], "temperature": 0.0, "avg_logprob": -0.1569195915670956, "compression_ratio": 1.5201612903225807, "no_speech_prob": 3.21887455356773e-05}, {"id": 51, "seek": 23944, "start": 244.92, "end": 245.92, "text": " plagiarizing.", "tokens": [33756, 9448, 3319, 13], "temperature": 0.0, "avg_logprob": -0.1569195915670956, "compression_ratio": 1.5201612903225807, "no_speech_prob": 3.21887455356773e-05}, {"id": 52, "seek": 23944, "start": 245.92, "end": 251.6, "text": " Vim's regular expressions are slightly different, unfortunately, which annoys me no end, but", "tokens": [691, 332, 311, 3890, 15277, 366, 4748, 819, 11, 7015, 11, 597, 46277, 749, 385, 572, 917, 11, 457], "temperature": 0.0, "avg_logprob": -0.1569195915670956, "compression_ratio": 1.5201612903225807, "no_speech_prob": 3.21887455356773e-05}, {"id": 53, "seek": 23944, "start": 251.6, "end": 254.28, "text": " they still do the job.", "tokens": [436, 920, 360, 264, 1691, 13], "temperature": 0.0, "avg_logprob": -0.1569195915670956, "compression_ratio": 1.5201612903225807, "no_speech_prob": 3.21887455356773e-05}, {"id": 54, "seek": 23944, "start": 254.28, "end": 260.82, "text": " So yeah, make sure you've got a good text editor that you know well how to use.", "tokens": [407, 1338, 11, 652, 988, 291, 600, 658, 257, 665, 2487, 9839, 300, 291, 458, 731, 577, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.1569195915670956, "compression_ratio": 1.5201612903225807, "no_speech_prob": 3.21887455356773e-05}, {"id": 55, "seek": 23944, "start": 260.82, "end": 263.0, "text": " Something with a good macro facility is nice as well.", "tokens": [6595, 365, 257, 665, 18887, 8973, 307, 1481, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1569195915670956, "compression_ratio": 1.5201612903225807, "no_speech_prob": 3.21887455356773e-05}, {"id": 56, "seek": 23944, "start": 263.0, "end": 264.72, "text": " Again, Vim's great for that.", "tokens": [3764, 11, 691, 332, 311, 869, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.1569195915670956, "compression_ratio": 1.5201612903225807, "no_speech_prob": 3.21887455356773e-05}, {"id": 57, "seek": 26472, "start": 264.72, "end": 269.48, "text": " You can record a series of keystrokes and hit a button and it repeats it basically on", "tokens": [509, 393, 2136, 257, 2638, 295, 2141, 27616, 5993, 293, 2045, 257, 2960, 293, 309, 35038, 309, 1936, 322], "temperature": 0.0, "avg_logprob": -0.15547280252715687, "compression_ratio": 1.4644549763033174, "no_speech_prob": 3.4800843422999606e-05}, {"id": 58, "seek": 26472, "start": 269.48, "end": 270.48, "text": " every line.", "tokens": [633, 1622, 13], "temperature": 0.0, "avg_logprob": -0.15547280252715687, "compression_ratio": 1.4644549763033174, "no_speech_prob": 3.4800843422999606e-05}, {"id": 59, "seek": 26472, "start": 270.48, "end": 280.04, "text": " I also wrote Perl here because to me, Perl is a rather unloved programming language.", "tokens": [286, 611, 4114, 3026, 75, 510, 570, 281, 385, 11, 3026, 75, 307, 257, 2831, 517, 752, 937, 9410, 2856, 13], "temperature": 0.0, "avg_logprob": -0.15547280252715687, "compression_ratio": 1.4644549763033174, "no_speech_prob": 3.4800843422999606e-05}, {"id": 60, "seek": 26472, "start": 280.04, "end": 286.22, "text": " But if you think back to where it comes from, it was originally developed as the Swiss Army", "tokens": [583, 498, 291, 519, 646, 281, 689, 309, 1487, 490, 11, 309, 390, 7993, 4743, 382, 264, 21965, 9583], "temperature": 0.0, "avg_logprob": -0.15547280252715687, "compression_ratio": 1.4644549763033174, "no_speech_prob": 3.4800843422999606e-05}, {"id": 61, "seek": 26472, "start": 286.22, "end": 290.76000000000005, "text": " chainsaw of text processing tools.", "tokens": [12626, 1607, 295, 2487, 9007, 3873, 13], "temperature": 0.0, "avg_logprob": -0.15547280252715687, "compression_ratio": 1.4644549763033174, "no_speech_prob": 3.4800843422999606e-05}, {"id": 62, "seek": 29076, "start": 290.76, "end": 295.56, "text": " And today that is something it still does, I think, better than any other tool.", "tokens": [400, 965, 300, 307, 746, 309, 920, 775, 11, 286, 519, 11, 1101, 813, 604, 661, 2290, 13], "temperature": 0.0, "avg_logprob": -0.17310880304692866, "compression_ratio": 1.8106796116504855, "no_speech_prob": 5.5614669690839946e-05}, {"id": 63, "seek": 29076, "start": 295.56, "end": 302.78, "text": " It has amazing command line options you can pass to it that do things like run the following", "tokens": [467, 575, 2243, 5622, 1622, 3956, 291, 393, 1320, 281, 309, 300, 360, 721, 411, 1190, 264, 3480], "temperature": 0.0, "avg_logprob": -0.17310880304692866, "compression_ratio": 1.8106796116504855, "no_speech_prob": 5.5614669690839946e-05}, {"id": 64, "seek": 29076, "start": 302.78, "end": 306.96, "text": " command on every line in the file or run the following line on every command in the file", "tokens": [5622, 322, 633, 1622, 294, 264, 3991, 420, 1190, 264, 3480, 1622, 322, 633, 5622, 294, 264, 3991], "temperature": 0.0, "avg_logprob": -0.17310880304692866, "compression_ratio": 1.8106796116504855, "no_speech_prob": 5.5614669690839946e-05}, {"id": 65, "seek": 29076, "start": 306.96, "end": 309.56, "text": " and then print it out.", "tokens": [293, 550, 4482, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.17310880304692866, "compression_ratio": 1.8106796116504855, "no_speech_prob": 5.5614669690839946e-05}, {"id": 66, "seek": 29076, "start": 309.56, "end": 315.15999999999997, "text": " There's a command line option to back up each file before changing it to large dot back.", "tokens": [821, 311, 257, 5622, 1622, 3614, 281, 646, 493, 1184, 3991, 949, 4473, 309, 281, 2416, 5893, 646, 13], "temperature": 0.0, "avg_logprob": -0.17310880304692866, "compression_ratio": 1.8106796116504855, "no_speech_prob": 5.5614669690839946e-05}, {"id": 67, "seek": 31516, "start": 315.16, "end": 321.84000000000003, "text": " I find with Perl I can do stuff which would take me a much, much longer time than any", "tokens": [286, 915, 365, 3026, 75, 286, 393, 360, 1507, 597, 576, 747, 385, 257, 709, 11, 709, 2854, 565, 813, 604], "temperature": 0.0, "avg_logprob": -0.1816030664646879, "compression_ratio": 1.6322869955156951, "no_speech_prob": 2.0783725631190464e-05}, {"id": 68, "seek": 31516, "start": 321.84000000000003, "end": 324.16, "text": " other tool.", "tokens": [661, 2290, 13], "temperature": 0.0, "avg_logprob": -0.1816030664646879, "compression_ratio": 1.6322869955156951, "no_speech_prob": 2.0783725631190464e-05}, {"id": 69, "seek": 31516, "start": 324.16, "end": 328.0, "text": " Even simple little things like I was hacking some data on the weekend where I had to concatenate", "tokens": [2754, 2199, 707, 721, 411, 286, 390, 31422, 512, 1412, 322, 264, 6711, 689, 286, 632, 281, 1588, 7186, 473], "temperature": 0.0, "avg_logprob": -0.1816030664646879, "compression_ratio": 1.6322869955156951, "no_speech_prob": 2.0783725631190464e-05}, {"id": 70, "seek": 31516, "start": 328.0, "end": 332.40000000000003, "text": " a whole bunch of files, but only the first one I wanted to keep the first line because", "tokens": [257, 1379, 3840, 295, 7098, 11, 457, 787, 264, 700, 472, 286, 1415, 281, 1066, 264, 700, 1622, 570], "temperature": 0.0, "avg_logprob": -0.1816030664646879, "compression_ratio": 1.6322869955156951, "no_speech_prob": 2.0783725631190464e-05}, {"id": 71, "seek": 31516, "start": 332.40000000000003, "end": 336.20000000000005, "text": " there are a whole bunch of CSV files in which they had a headline I had to delete.", "tokens": [456, 366, 257, 1379, 3840, 295, 48814, 7098, 294, 597, 436, 632, 257, 28380, 286, 632, 281, 12097, 13], "temperature": 0.0, "avg_logprob": -0.1816030664646879, "compression_ratio": 1.6322869955156951, "no_speech_prob": 2.0783725631190464e-05}, {"id": 72, "seek": 33620, "start": 336.2, "end": 348.32, "text": " So in Perl, in fact it's probably still going to be sitting here in my history.", "tokens": [407, 294, 3026, 75, 11, 294, 1186, 309, 311, 1391, 920, 516, 281, 312, 3798, 510, 294, 452, 2503, 13], "temperature": 0.0, "avg_logprob": -0.2040835129587274, "compression_ratio": 1.591549295774648, "no_speech_prob": 1.618661372049246e-05}, {"id": 73, "seek": 33620, "start": 348.32, "end": 355.8, "text": " So in Perl that's basically minus n means do this on every single row.", "tokens": [407, 294, 3026, 75, 300, 311, 1936, 3175, 297, 1355, 360, 341, 322, 633, 2167, 5386, 13], "temperature": 0.0, "avg_logprob": -0.2040835129587274, "compression_ratio": 1.591549295774648, "no_speech_prob": 1.618661372049246e-05}, {"id": 74, "seek": 33620, "start": 355.8, "end": 358.2, "text": " Minus e means I'm not even going to write a script file.", "tokens": [2829, 301, 308, 1355, 286, 478, 406, 754, 516, 281, 2464, 257, 5755, 3991, 13], "temperature": 0.0, "avg_logprob": -0.2040835129587274, "compression_ratio": 1.591549295774648, "no_speech_prob": 1.618661372049246e-05}, {"id": 75, "seek": 33620, "start": 358.2, "end": 361.53999999999996, "text": " I'm going to give you the thing to do it right here on the command line.", "tokens": [286, 478, 516, 281, 976, 291, 264, 551, 281, 360, 309, 558, 510, 322, 264, 5622, 1622, 13], "temperature": 0.0, "avg_logprob": -0.2040835129587274, "compression_ratio": 1.591549295774648, "no_speech_prob": 1.618661372049246e-05}, {"id": 76, "seek": 33620, "start": 361.53999999999996, "end": 365.12, "text": " And here's a piece of rather difficult to comprehend Perl.", "tokens": [400, 510, 311, 257, 2522, 295, 2831, 2252, 281, 38183, 3026, 75, 13], "temperature": 0.0, "avg_logprob": -0.2040835129587274, "compression_ratio": 1.591549295774648, "no_speech_prob": 1.618661372049246e-05}, {"id": 77, "seek": 36512, "start": 365.12, "end": 370.28000000000003, "text": " But trust me what it says is if the line number is greater than one then print that line.", "tokens": [583, 3361, 385, 437, 309, 1619, 307, 498, 264, 1622, 1230, 307, 5044, 813, 472, 550, 4482, 300, 1622, 13], "temperature": 0.0, "avg_logprob": -0.20608718996125508, "compression_ratio": 1.6703296703296704, "no_speech_prob": 2.8408987418515608e-05}, {"id": 78, "seek": 36512, "start": 370.28000000000003, "end": 375.24, "text": " So here's something to strip the first line from every file.", "tokens": [407, 510, 311, 746, 281, 12828, 264, 700, 1622, 490, 633, 3991, 13], "temperature": 0.0, "avg_logprob": -0.20608718996125508, "compression_ratio": 1.6703296703296704, "no_speech_prob": 2.8408987418515608e-05}, {"id": 79, "seek": 36512, "start": 375.24, "end": 377.84000000000003, "text": " So this kind of stuff you can do in Perl is great.", "tokens": [407, 341, 733, 295, 1507, 291, 393, 360, 294, 3026, 75, 307, 869, 13], "temperature": 0.0, "avg_logprob": -0.20608718996125508, "compression_ratio": 1.6703296703296704, "no_speech_prob": 2.8408987418515608e-05}, {"id": 80, "seek": 36512, "start": 377.84000000000003, "end": 383.44, "text": " And I see a lot of people on the forums who complain about the format of the data wasn't", "tokens": [400, 286, 536, 257, 688, 295, 561, 322, 264, 26998, 567, 11024, 466, 264, 7877, 295, 264, 1412, 2067, 380], "temperature": 0.0, "avg_logprob": -0.20608718996125508, "compression_ratio": 1.6703296703296704, "no_speech_prob": 2.8408987418515608e-05}, {"id": 81, "seek": 36512, "start": 383.44, "end": 385.24, "text": " quite what I expected or not quite convenient.", "tokens": [1596, 437, 286, 5176, 420, 406, 1596, 10851, 13], "temperature": 0.0, "avg_logprob": -0.20608718996125508, "compression_ratio": 1.6703296703296704, "no_speech_prob": 2.8408987418515608e-05}, {"id": 82, "seek": 36512, "start": 385.24, "end": 386.92, "text": " Can you please change it for me?", "tokens": [1664, 291, 1767, 1319, 309, 337, 385, 30], "temperature": 0.0, "avg_logprob": -0.20608718996125508, "compression_ratio": 1.6703296703296704, "no_speech_prob": 2.8408987418515608e-05}, {"id": 83, "seek": 36512, "start": 386.92, "end": 389.56, "text": " And I always think, well, this is part of data science.", "tokens": [400, 286, 1009, 519, 11, 731, 11, 341, 307, 644, 295, 1412, 3497, 13], "temperature": 0.0, "avg_logprob": -0.20608718996125508, "compression_ratio": 1.6703296703296704, "no_speech_prob": 2.8408987418515608e-05}, {"id": 84, "seek": 36512, "start": 389.56, "end": 391.12, "text": " This is part of data hacking.", "tokens": [639, 307, 644, 295, 1412, 31422, 13], "temperature": 0.0, "avg_logprob": -0.20608718996125508, "compression_ratio": 1.6703296703296704, "no_speech_prob": 2.8408987418515608e-05}, {"id": 85, "seek": 39112, "start": 391.12, "end": 395.32, "text": " This is data munging or data manipulation.", "tokens": [639, 307, 1412, 275, 1063, 278, 420, 1412, 26475, 13], "temperature": 0.0, "avg_logprob": -0.145623085715554, "compression_ratio": 1.5672268907563025, "no_speech_prob": 2.392205351497978e-05}, {"id": 86, "seek": 39112, "start": 395.32, "end": 401.72, "text": " There's actually a really great book called, I don't know if it's hard to find nowadays,", "tokens": [821, 311, 767, 257, 534, 869, 1446, 1219, 11, 286, 500, 380, 458, 498, 309, 311, 1152, 281, 915, 13434, 11], "temperature": 0.0, "avg_logprob": -0.145623085715554, "compression_ratio": 1.5672268907563025, "no_speech_prob": 2.392205351497978e-05}, {"id": 87, "seek": 39112, "start": 401.72, "end": 404.72, "text": " but I loved it called Data Munging in Perl.", "tokens": [457, 286, 4333, 309, 1219, 11888, 376, 1063, 278, 294, 3026, 75, 13], "temperature": 0.0, "avg_logprob": -0.145623085715554, "compression_ratio": 1.5672268907563025, "no_speech_prob": 2.392205351497978e-05}, {"id": 88, "seek": 39112, "start": 404.72, "end": 412.88, "text": " And it's a whole book about all the cool stuff you can do in Perl in a line or two.", "tokens": [400, 309, 311, 257, 1379, 1446, 466, 439, 264, 1627, 1507, 291, 393, 360, 294, 3026, 75, 294, 257, 1622, 420, 732, 13], "temperature": 0.0, "avg_logprob": -0.145623085715554, "compression_ratio": 1.5672268907563025, "no_speech_prob": 2.392205351497978e-05}, {"id": 89, "seek": 39112, "start": 412.88, "end": 417.88, "text": " So okay, I've now got the data into a form where I can kind of load it up into some tool", "tokens": [407, 1392, 11, 286, 600, 586, 658, 264, 1412, 666, 257, 1254, 689, 286, 393, 733, 295, 3677, 309, 493, 666, 512, 2290], "temperature": 0.0, "avg_logprob": -0.145623085715554, "compression_ratio": 1.5672268907563025, "no_speech_prob": 2.392205351497978e-05}, {"id": 90, "seek": 39112, "start": 417.88, "end": 419.44, "text": " and start looking at it.", "tokens": [293, 722, 1237, 412, 309, 13], "temperature": 0.0, "avg_logprob": -0.145623085715554, "compression_ratio": 1.5672268907563025, "no_speech_prob": 2.392205351497978e-05}, {"id": 91, "seek": 41944, "start": 419.44, "end": 421.6, "text": " What's the tool I normally start with?", "tokens": [708, 311, 264, 2290, 286, 5646, 722, 365, 30], "temperature": 0.0, "avg_logprob": -0.16890851102134055, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.8630969861987978e-05}, {"id": 92, "seek": 41944, "start": 421.6, "end": 423.76, "text": " I normally start with Excel.", "tokens": [286, 5646, 722, 365, 19060, 13], "temperature": 0.0, "avg_logprob": -0.16890851102134055, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.8630969861987978e-05}, {"id": 93, "seek": 41944, "start": 423.76, "end": 430.04, "text": " Now your first reaction might be to think Excel, not so good for big files.", "tokens": [823, 428, 700, 5480, 1062, 312, 281, 519, 19060, 11, 406, 370, 665, 337, 955, 7098, 13], "temperature": 0.0, "avg_logprob": -0.16890851102134055, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.8630969861987978e-05}, {"id": 94, "seek": 41944, "start": 430.04, "end": 433.16, "text": " To which my reaction would be, if you're just looking at the data for the first time, why", "tokens": [1407, 597, 452, 5480, 576, 312, 11, 498, 291, 434, 445, 1237, 412, 264, 1412, 337, 264, 700, 565, 11, 983], "temperature": 0.0, "avg_logprob": -0.16890851102134055, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.8630969861987978e-05}, {"id": 95, "seek": 41944, "start": 433.16, "end": 435.76, "text": " are you looking at a big file?", "tokens": [366, 291, 1237, 412, 257, 955, 3991, 30], "temperature": 0.0, "avg_logprob": -0.16890851102134055, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.8630969861987978e-05}, {"id": 96, "seek": 41944, "start": 435.76, "end": 438.72, "text": " Start by sampling it.", "tokens": [6481, 538, 21179, 309, 13], "temperature": 0.0, "avg_logprob": -0.16890851102134055, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.8630969861987978e-05}, {"id": 97, "seek": 41944, "start": 438.72, "end": 442.15999999999997, "text": " And again, this is the kind of thing you can do in your data manipulation piece.", "tokens": [400, 797, 11, 341, 307, 264, 733, 295, 551, 291, 393, 360, 294, 428, 1412, 26475, 2522, 13], "temperature": 0.0, "avg_logprob": -0.16890851102134055, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.8630969861987978e-05}, {"id": 98, "seek": 41944, "start": 442.15999999999997, "end": 448.2, "text": " That thing I just showed you in Perl, if that's a rand is greater than 0.9 and print, that's", "tokens": [663, 551, 286, 445, 4712, 291, 294, 3026, 75, 11, 498, 300, 311, 257, 367, 474, 307, 5044, 813, 1958, 13, 24, 293, 4482, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.16890851102134055, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.8630969861987978e-05}, {"id": 99, "seek": 44820, "start": 448.2, "end": 452.44, "text": " going to sample every 10 rows, more or less.", "tokens": [516, 281, 6889, 633, 1266, 13241, 11, 544, 420, 1570, 13], "temperature": 0.0, "avg_logprob": -0.1202461196155083, "compression_ratio": 1.4827586206896552, "no_speech_prob": 1.1842757885460742e-05}, {"id": 100, "seek": 44820, "start": 452.44, "end": 456.8, "text": " So get your, if you've got a huge data file, get it to a size that you can easily start", "tokens": [407, 483, 428, 11, 498, 291, 600, 658, 257, 2603, 1412, 3991, 11, 483, 309, 281, 257, 2744, 300, 291, 393, 3612, 722], "temperature": 0.0, "avg_logprob": -0.1202461196155083, "compression_ratio": 1.4827586206896552, "no_speech_prob": 1.1842757885460742e-05}, {"id": 101, "seek": 44820, "start": 456.8, "end": 460.36, "text": " playing with it, which normally means some random sampling.", "tokens": [2433, 365, 309, 11, 597, 5646, 1355, 512, 4974, 21179, 13], "temperature": 0.0, "avg_logprob": -0.1202461196155083, "compression_ratio": 1.4827586206896552, "no_speech_prob": 1.1842757885460742e-05}, {"id": 102, "seek": 44820, "start": 460.36, "end": 467.52, "text": " So I like to look at it in Excel and I will show you for a particular competition how", "tokens": [407, 286, 411, 281, 574, 412, 309, 294, 19060, 293, 286, 486, 855, 291, 337, 257, 1729, 6211, 577], "temperature": 0.0, "avg_logprob": -0.1202461196155083, "compression_ratio": 1.4827586206896552, "no_speech_prob": 1.1842757885460742e-05}, {"id": 103, "seek": 44820, "start": 467.52, "end": 471.28, "text": " I go about doing that.", "tokens": [286, 352, 466, 884, 300, 13], "temperature": 0.0, "avg_logprob": -0.1202461196155083, "compression_ratio": 1.4827586206896552, "no_speech_prob": 1.1842757885460742e-05}, {"id": 104, "seek": 47128, "start": 471.28, "end": 478.23999999999995, "text": " So let's have a look, for example, at a couple.", "tokens": [407, 718, 311, 362, 257, 574, 11, 337, 1365, 11, 412, 257, 1916, 13], "temperature": 0.0, "avg_logprob": -0.11507392791380365, "compression_ratio": 1.5308056872037914, "no_speech_prob": 1.0952589946100488e-05}, {"id": 105, "seek": 47128, "start": 478.23999999999995, "end": 483.71999999999997, "text": " So here's one which the New South Wales government recently ran, which was to predict how long", "tokens": [407, 510, 311, 472, 597, 264, 1873, 4242, 16495, 2463, 3938, 5872, 11, 597, 390, 281, 6069, 577, 938], "temperature": 0.0, "avg_logprob": -0.11507392791380365, "compression_ratio": 1.5308056872037914, "no_speech_prob": 1.0952589946100488e-05}, {"id": 106, "seek": 47128, "start": 483.71999999999997, "end": 492.55999999999995, "text": " it's going to take cars to travel along each segment of the M4 motorway in each direction.", "tokens": [309, 311, 516, 281, 747, 5163, 281, 3147, 2051, 1184, 9469, 295, 264, 376, 19, 5932, 676, 294, 1184, 3513, 13], "temperature": 0.0, "avg_logprob": -0.11507392791380365, "compression_ratio": 1.5308056872037914, "no_speech_prob": 1.0952589946100488e-05}, {"id": 107, "seek": 47128, "start": 492.55999999999995, "end": 500.0, "text": " The data for this is a lot of columns, because every column is another route and lots and", "tokens": [440, 1412, 337, 341, 307, 257, 688, 295, 13766, 11, 570, 633, 7738, 307, 1071, 7955, 293, 3195, 293], "temperature": 0.0, "avg_logprob": -0.11507392791380365, "compression_ratio": 1.5308056872037914, "no_speech_prob": 1.0952589946100488e-05}, {"id": 108, "seek": 50000, "start": 500.0, "end": 505.56, "text": " lots of rows, every row is another two minute observation and very hard to get a feel for", "tokens": [3195, 295, 13241, 11, 633, 5386, 307, 1071, 732, 3456, 14816, 293, 588, 1152, 281, 483, 257, 841, 337], "temperature": 0.0, "avg_logprob": -0.1996921759385329, "compression_ratio": 1.558252427184466, "no_speech_prob": 1.2804116522602271e-05}, {"id": 109, "seek": 50000, "start": 505.56, "end": 507.48, "text": " what's going on.", "tokens": [437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.1996921759385329, "compression_ratio": 1.558252427184466, "no_speech_prob": 1.2804116522602271e-05}, {"id": 110, "seek": 50000, "start": 507.48, "end": 512.44, "text": " There were various terrific attempts on the forum at trying to create animated pictures", "tokens": [821, 645, 3683, 20899, 15257, 322, 264, 17542, 412, 1382, 281, 1884, 18947, 5242], "temperature": 0.0, "avg_logprob": -0.1996921759385329, "compression_ratio": 1.558252427184466, "no_speech_prob": 1.2804116522602271e-05}, {"id": 111, "seek": 50000, "start": 512.44, "end": 515.4, "text": " of what the road looks like over time.", "tokens": [295, 437, 264, 3060, 1542, 411, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.1996921759385329, "compression_ratio": 1.558252427184466, "no_speech_prob": 1.2804116522602271e-05}, {"id": 112, "seek": 50000, "start": 515.4, "end": 524.24, "text": " I did something extremely low tech, which is something I'm proud of, which is I created", "tokens": [286, 630, 746, 4664, 2295, 7553, 11, 597, 307, 746, 286, 478, 4570, 295, 11, 597, 307, 286, 2942], "temperature": 0.0, "avg_logprob": -0.1996921759385329, "compression_ratio": 1.558252427184466, "no_speech_prob": 1.2804116522602271e-05}, {"id": 113, "seek": 52424, "start": 524.24, "end": 532.84, "text": " a simple little macro in Excel, which selected each column and then went conditional formatting,", "tokens": [257, 2199, 707, 18887, 294, 19060, 11, 597, 8209, 1184, 7738, 293, 550, 1437, 27708, 39366, 11], "temperature": 0.0, "avg_logprob": -0.17547953259813917, "compression_ratio": 1.6233183856502242, "no_speech_prob": 6.962043244129745e-06}, {"id": 114, "seek": 52424, "start": 532.84, "end": 540.32, "text": " colour scales, red to green, and I ran that on each column and I got this picture.", "tokens": [8267, 17408, 11, 2182, 281, 3092, 11, 293, 286, 5872, 300, 322, 1184, 7738, 293, 286, 658, 341, 3036, 13], "temperature": 0.0, "avg_logprob": -0.17547953259813917, "compression_ratio": 1.6233183856502242, "no_speech_prob": 6.962043244129745e-06}, {"id": 115, "seek": 52424, "start": 540.32, "end": 547.2, "text": " So here's each route on this road and here's how long it took to go on that route at this", "tokens": [407, 510, 311, 1184, 7955, 322, 341, 3060, 293, 510, 311, 577, 938, 309, 1890, 281, 352, 322, 300, 7955, 412, 341], "temperature": 0.0, "avg_logprob": -0.17547953259813917, "compression_ratio": 1.6233183856502242, "no_speech_prob": 6.962043244129745e-06}, {"id": 116, "seek": 52424, "start": 547.2, "end": 548.2, "text": " time.", "tokens": [565, 13], "temperature": 0.0, "avg_logprob": -0.17547953259813917, "compression_ratio": 1.6233183856502242, "no_speech_prob": 6.962043244129745e-06}, {"id": 117, "seek": 52424, "start": 548.2, "end": 554.16, "text": " And isn't this interesting, because I can immediately see what traffic jams look like.", "tokens": [400, 1943, 380, 341, 1880, 11, 570, 286, 393, 4258, 536, 437, 6419, 361, 4070, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.17547953259813917, "compression_ratio": 1.6233183856502242, "no_speech_prob": 6.962043244129745e-06}, {"id": 118, "seek": 55416, "start": 554.16, "end": 559.24, "text": " See how they kind of flow as you start getting a traffic jam here, they flow along the road", "tokens": [3008, 577, 436, 733, 295, 3095, 382, 291, 722, 1242, 257, 6419, 7872, 510, 11, 436, 3095, 2051, 264, 3060], "temperature": 0.0, "avg_logprob": -0.20544874668121338, "compression_ratio": 1.7731481481481481, "no_speech_prob": 3.1198964279610664e-05}, {"id": 119, "seek": 55416, "start": 559.24, "end": 564.8, "text": " as time goes on and you can then start to see at what kind of times they happen and", "tokens": [382, 565, 1709, 322, 293, 291, 393, 550, 722, 281, 536, 412, 437, 733, 295, 1413, 436, 1051, 293], "temperature": 0.0, "avg_logprob": -0.20544874668121338, "compression_ratio": 1.7731481481481481, "no_speech_prob": 3.1198964279610664e-05}, {"id": 120, "seek": 55416, "start": 564.8, "end": 565.8, "text": " where they tend to start.", "tokens": [689, 436, 3928, 281, 722, 13], "temperature": 0.0, "avg_logprob": -0.20544874668121338, "compression_ratio": 1.7731481481481481, "no_speech_prob": 3.1198964279610664e-05}, {"id": 121, "seek": 55416, "start": 565.8, "end": 573.4, "text": " So here's a really big jam and it's interesting isn't it, because if we go into Sydney in", "tokens": [407, 510, 311, 257, 534, 955, 7872, 293, 309, 311, 1880, 1943, 380, 309, 11, 570, 498, 321, 352, 666, 21065, 294], "temperature": 0.0, "avg_logprob": -0.20544874668121338, "compression_ratio": 1.7731481481481481, "no_speech_prob": 3.1198964279610664e-05}, {"id": 122, "seek": 55416, "start": 573.4, "end": 579.24, "text": " the afternoon, then obviously you start getting these jams up here and during the afternoon", "tokens": [264, 6499, 11, 550, 2745, 291, 722, 1242, 613, 361, 4070, 493, 510, 293, 1830, 264, 6499], "temperature": 0.0, "avg_logprob": -0.20544874668121338, "compression_ratio": 1.7731481481481481, "no_speech_prob": 3.1198964279610664e-05}, {"id": 123, "seek": 57924, "start": 579.24, "end": 585.36, "text": " it progresses, you can see the jam moving so that at 5pm, looks like there's actually", "tokens": [309, 41929, 11, 291, 393, 536, 264, 7872, 2684, 370, 300, 412, 1025, 14395, 11, 1542, 411, 456, 311, 767], "temperature": 0.0, "avg_logprob": -0.17990627761714714, "compression_ratio": 1.7509433962264151, "no_speech_prob": 7.966221892274916e-05}, {"id": 124, "seek": 57924, "start": 585.36, "end": 592.32, "text": " a couple of them, and at the other end of the road it stays jammed until everybody's", "tokens": [257, 1916, 295, 552, 11, 293, 412, 264, 661, 917, 295, 264, 3060, 309, 10834, 7872, 1912, 1826, 2201, 311], "temperature": 0.0, "avg_logprob": -0.17990627761714714, "compression_ratio": 1.7509433962264151, "no_speech_prob": 7.966221892274916e-05}, {"id": 125, "seek": 57924, "start": 592.32, "end": 594.12, "text": " cleared out through the freeway.", "tokens": [19725, 484, 807, 264, 1737, 676, 13], "temperature": 0.0, "avg_logprob": -0.17990627761714714, "compression_ratio": 1.7509433962264151, "no_speech_prob": 7.966221892274916e-05}, {"id": 126, "seek": 57924, "start": 594.12, "end": 598.5600000000001, "text": " So you get a real feel for it, and even when it's not peak hour, and even in some of the", "tokens": [407, 291, 483, 257, 957, 841, 337, 309, 11, 293, 754, 562, 309, 311, 406, 10651, 1773, 11, 293, 754, 294, 512, 295, 264], "temperature": 0.0, "avg_logprob": -0.17990627761714714, "compression_ratio": 1.7509433962264151, "no_speech_prob": 7.966221892274916e-05}, {"id": 127, "seek": 57924, "start": 598.5600000000001, "end": 603.52, "text": " period areas which aren't so busy, you can see, that's interesting, there are basically", "tokens": [2896, 3179, 597, 3212, 380, 370, 5856, 11, 291, 393, 536, 11, 300, 311, 1880, 11, 456, 366, 1936], "temperature": 0.0, "avg_logprob": -0.17990627761714714, "compression_ratio": 1.7509433962264151, "no_speech_prob": 7.966221892274916e-05}, {"id": 128, "seek": 57924, "start": 603.52, "end": 608.58, "text": " parts of the freeway which out of peak hour they're basically constant travel time.", "tokens": [3166, 295, 264, 1737, 676, 597, 484, 295, 10651, 1773, 436, 434, 1936, 5754, 3147, 565, 13], "temperature": 0.0, "avg_logprob": -0.17990627761714714, "compression_ratio": 1.7509433962264151, "no_speech_prob": 7.966221892274916e-05}, {"id": 129, "seek": 60858, "start": 608.58, "end": 612.2, "text": " And the colours are immediately showing me, you see how easy it is.", "tokens": [400, 264, 16484, 366, 4258, 4099, 385, 11, 291, 536, 577, 1858, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.17807234128316243, "compression_ratio": 1.8254545454545454, "no_speech_prob": 0.0001122847679653205}, {"id": 130, "seek": 60858, "start": 612.2, "end": 617.36, "text": " So when we actually got on the phone with the RTA to take them through the winning model,", "tokens": [407, 562, 321, 767, 658, 322, 264, 2593, 365, 264, 497, 8241, 281, 747, 552, 807, 264, 8224, 2316, 11], "temperature": 0.0, "avg_logprob": -0.17807234128316243, "compression_ratio": 1.8254545454545454, "no_speech_prob": 0.0001122847679653205}, {"id": 131, "seek": 60858, "start": 617.36, "end": 621.64, "text": " actually the people that won this competition were kind enough to organise a screencast", "tokens": [767, 264, 561, 300, 1582, 341, 6211, 645, 733, 1547, 281, 50110, 257, 2568, 3734], "temperature": 0.0, "avg_logprob": -0.17807234128316243, "compression_ratio": 1.8254545454545454, "no_speech_prob": 0.0001122847679653205}, {"id": 132, "seek": 60858, "start": 621.64, "end": 624.96, "text": " with all the people in the RTA and from Kaggle to show the winning model.", "tokens": [365, 439, 264, 561, 294, 264, 497, 8241, 293, 490, 48751, 22631, 281, 855, 264, 8224, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17807234128316243, "compression_ratio": 1.8254545454545454, "no_speech_prob": 0.0001122847679653205}, {"id": 133, "seek": 60858, "start": 624.96, "end": 631.12, "text": " And the people from RTA said, well this is interesting because you tell me in your model,", "tokens": [400, 264, 561, 490, 497, 8241, 848, 11, 731, 341, 307, 1880, 570, 291, 980, 385, 294, 428, 2316, 11], "temperature": 0.0, "avg_logprob": -0.17807234128316243, "compression_ratio": 1.8254545454545454, "no_speech_prob": 0.0001122847679653205}, {"id": 134, "seek": 60858, "start": 631.12, "end": 635.4000000000001, "text": " they said what we looked at was we basically created a model that looked at for a particular", "tokens": [436, 848, 437, 321, 2956, 412, 390, 321, 1936, 2942, 257, 2316, 300, 2956, 412, 337, 257, 1729], "temperature": 0.0, "avg_logprob": -0.17807234128316243, "compression_ratio": 1.8254545454545454, "no_speech_prob": 0.0001122847679653205}, {"id": 135, "seek": 63540, "start": 635.4, "end": 642.12, "text": " time, for a particular route, we looked at the times and routes just before and around", "tokens": [565, 11, 337, 257, 1729, 7955, 11, 321, 2956, 412, 264, 1413, 293, 18242, 445, 949, 293, 926], "temperature": 0.0, "avg_logprob": -0.17666397423579774, "compression_ratio": 1.6653846153846155, "no_speech_prob": 6.0132362705189735e-05}, {"id": 136, "seek": 63540, "start": 642.12, "end": 643.8, "text": " it on both sides.", "tokens": [309, 322, 1293, 4881, 13], "temperature": 0.0, "avg_logprob": -0.17666397423579774, "compression_ratio": 1.6653846153846155, "no_speech_prob": 6.0132362705189735e-05}, {"id": 137, "seek": 63540, "start": 643.8, "end": 649.1999999999999, "text": " And I remember one of the guys said, that's weird because normally these kind of queues", "tokens": [400, 286, 1604, 472, 295, 264, 1074, 848, 11, 300, 311, 3657, 570, 5646, 613, 733, 295, 631, 1247], "temperature": 0.0, "avg_logprob": -0.17666397423579774, "compression_ratio": 1.6653846153846155, "no_speech_prob": 6.0132362705189735e-05}, {"id": 138, "seek": 63540, "start": 649.1999999999999, "end": 653.64, "text": " track it down and go in one direction, so why would you look at both sides?", "tokens": [2837, 309, 760, 293, 352, 294, 472, 3513, 11, 370, 983, 576, 291, 574, 412, 1293, 4881, 30], "temperature": 0.0, "avg_logprob": -0.17666397423579774, "compression_ratio": 1.6653846153846155, "no_speech_prob": 6.0132362705189735e-05}, {"id": 139, "seek": 63540, "start": 653.64, "end": 658.52, "text": " And so I was able to quickly say, okay guys, that's true, have a look at this.", "tokens": [400, 370, 286, 390, 1075, 281, 2661, 584, 11, 1392, 1074, 11, 300, 311, 2074, 11, 362, 257, 574, 412, 341, 13], "temperature": 0.0, "avg_logprob": -0.17666397423579774, "compression_ratio": 1.6653846153846155, "no_speech_prob": 6.0132362705189735e-05}, {"id": 140, "seek": 63540, "start": 658.52, "end": 663.1999999999999, "text": " So if you go to the other end, you can see how sometimes although queues kind of form", "tokens": [407, 498, 291, 352, 281, 264, 661, 917, 11, 291, 393, 536, 577, 2171, 4878, 631, 1247, 733, 295, 1254], "temperature": 0.0, "avg_logprob": -0.17666397423579774, "compression_ratio": 1.6653846153846155, "no_speech_prob": 6.0132362705189735e-05}, {"id": 141, "seek": 66320, "start": 663.2, "end": 667.9000000000001, "text": " in one direction, they can kind of slide away in the other direction, for example.", "tokens": [294, 472, 3513, 11, 436, 393, 733, 295, 4137, 1314, 294, 264, 661, 3513, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1210880024092538, "compression_ratio": 1.8987341772151898, "no_speech_prob": 3.882587407133542e-05}, {"id": 142, "seek": 66320, "start": 667.9000000000001, "end": 673.0, "text": " So by looking at this kind of picture, you can see what your model is going to have to", "tokens": [407, 538, 1237, 412, 341, 733, 295, 3036, 11, 291, 393, 536, 437, 428, 2316, 307, 516, 281, 362, 281], "temperature": 0.0, "avg_logprob": -0.1210880024092538, "compression_ratio": 1.8987341772151898, "no_speech_prob": 3.882587407133542e-05}, {"id": 143, "seek": 66320, "start": 673.0, "end": 674.5200000000001, "text": " be able to model.", "tokens": [312, 1075, 281, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1210880024092538, "compression_ratio": 1.8987341772151898, "no_speech_prob": 3.882587407133542e-05}, {"id": 144, "seek": 66320, "start": 674.5200000000001, "end": 678.32, "text": " So you can see what kind of inputs it's going to have and how it's going to have to be set", "tokens": [407, 291, 393, 536, 437, 733, 295, 15743, 309, 311, 516, 281, 362, 293, 577, 309, 311, 516, 281, 362, 281, 312, 992], "temperature": 0.0, "avg_logprob": -0.1210880024092538, "compression_ratio": 1.8987341772151898, "no_speech_prob": 3.882587407133542e-05}, {"id": 145, "seek": 66320, "start": 678.32, "end": 682.36, "text": " up and you can immediately see that if you created the model that basically tried to", "tokens": [493, 293, 291, 393, 4258, 536, 300, 498, 291, 2942, 264, 2316, 300, 1936, 3031, 281], "temperature": 0.0, "avg_logprob": -0.1210880024092538, "compression_ratio": 1.8987341772151898, "no_speech_prob": 3.882587407133542e-05}, {"id": 146, "seek": 66320, "start": 682.36, "end": 691.36, "text": " predict each thing based on the previous few periods of the routes around it, whatever", "tokens": [6069, 1184, 551, 2361, 322, 264, 3894, 1326, 13804, 295, 264, 18242, 926, 309, 11, 2035], "temperature": 0.0, "avg_logprob": -0.1210880024092538, "compression_ratio": 1.8987341772151898, "no_speech_prob": 3.882587407133542e-05}, {"id": 147, "seek": 69136, "start": 691.36, "end": 695.6800000000001, "text": " modeling technique you're using, you're probably going to get a pretty good answer.", "tokens": [15983, 6532, 291, 434, 1228, 11, 291, 434, 1391, 516, 281, 483, 257, 1238, 665, 1867, 13], "temperature": 0.0, "avg_logprob": -0.21302798727284308, "compression_ratio": 1.6654804270462633, "no_speech_prob": 3.763445420190692e-05}, {"id": 148, "seek": 69136, "start": 695.6800000000001, "end": 699.96, "text": " And interestingly, the guys that won this competition, this is basically all they did,", "tokens": [400, 25873, 11, 264, 1074, 300, 1582, 341, 6211, 11, 341, 307, 1936, 439, 436, 630, 11], "temperature": 0.0, "avg_logprob": -0.21302798727284308, "compression_ratio": 1.6654804270462633, "no_speech_prob": 3.763445420190692e-05}, {"id": 149, "seek": 69136, "start": 699.96, "end": 705.44, "text": " really nice simple model, they used random forests as it happens, which we'll talk about", "tokens": [534, 1481, 2199, 2316, 11, 436, 1143, 4974, 21700, 382, 309, 2314, 11, 597, 321, 603, 751, 466], "temperature": 0.0, "avg_logprob": -0.21302798727284308, "compression_ratio": 1.6654804270462633, "no_speech_prob": 3.763445420190692e-05}, {"id": 150, "seek": 69136, "start": 705.44, "end": 706.44, "text": " soon.", "tokens": [2321, 13], "temperature": 0.0, "avg_logprob": -0.21302798727284308, "compression_ratio": 1.6654804270462633, "no_speech_prob": 3.763445420190692e-05}, {"id": 151, "seek": 69136, "start": 706.44, "end": 712.24, "text": " They added a couple of extra things, which was I think the rate of change of time, but", "tokens": [814, 3869, 257, 1916, 295, 2857, 721, 11, 597, 390, 286, 519, 264, 3314, 295, 1319, 295, 565, 11, 457], "temperature": 0.0, "avg_logprob": -0.21302798727284308, "compression_ratio": 1.6654804270462633, "no_speech_prob": 3.763445420190692e-05}, {"id": 152, "seek": 69136, "start": 712.24, "end": 714.44, "text": " that was basically all.", "tokens": [300, 390, 1936, 439, 13], "temperature": 0.0, "avg_logprob": -0.21302798727284308, "compression_ratio": 1.6654804270462633, "no_speech_prob": 3.763445420190692e-05}, {"id": 153, "seek": 69136, "start": 714.44, "end": 718.96, "text": " So really good example of how visualization can quite quickly tell you what you need to", "tokens": [407, 534, 665, 1365, 295, 577, 25801, 393, 1596, 2661, 980, 291, 437, 291, 643, 281], "temperature": 0.0, "avg_logprob": -0.21302798727284308, "compression_ratio": 1.6654804270462633, "no_speech_prob": 3.763445420190692e-05}, {"id": 154, "seek": 69136, "start": 718.96, "end": 719.96, "text": " do.", "tokens": [360, 13], "temperature": 0.0, "avg_logprob": -0.21302798727284308, "compression_ratio": 1.6654804270462633, "no_speech_prob": 3.763445420190692e-05}, {"id": 155, "seek": 71996, "start": 719.96, "end": 723.4000000000001, "text": " I'll show you another example.", "tokens": [286, 603, 855, 291, 1071, 1365, 13], "temperature": 0.0, "avg_logprob": -0.22183106820794601, "compression_ratio": 1.4974358974358974, "no_speech_prob": 3.321254553156905e-05}, {"id": 156, "seek": 71996, "start": 723.4000000000001, "end": 733.44, "text": " This is a recent competition that was set up by the datastats.com blog.", "tokens": [639, 307, 257, 5162, 6211, 300, 390, 992, 493, 538, 264, 1137, 525, 1720, 13, 1112, 6968, 13], "temperature": 0.0, "avg_logprob": -0.22183106820794601, "compression_ratio": 1.4974358974358974, "no_speech_prob": 3.321254553156905e-05}, {"id": 157, "seek": 71996, "start": 733.44, "end": 740.2, "text": " And what it was, was they wanted to try and create a recommendation system for our packages.", "tokens": [400, 437, 309, 390, 11, 390, 436, 1415, 281, 853, 293, 1884, 257, 11879, 1185, 337, 527, 17401, 13], "temperature": 0.0, "avg_logprob": -0.22183106820794601, "compression_ratio": 1.4974358974358974, "no_speech_prob": 3.321254553156905e-05}, {"id": 158, "seek": 71996, "start": 740.2, "end": 748.1600000000001, "text": " So they got a bunch of users to say, okay, this user for this package doesn't have it", "tokens": [407, 436, 658, 257, 3840, 295, 5022, 281, 584, 11, 1392, 11, 341, 4195, 337, 341, 7372, 1177, 380, 362, 309], "temperature": 0.0, "avg_logprob": -0.22183106820794601, "compression_ratio": 1.4974358974358974, "no_speech_prob": 3.321254553156905e-05}, {"id": 159, "seek": 71996, "start": 748.1600000000001, "end": 749.52, "text": " installed.", "tokens": [8899, 13], "temperature": 0.0, "avg_logprob": -0.22183106820794601, "compression_ratio": 1.4974358974358974, "no_speech_prob": 3.321254553156905e-05}, {"id": 160, "seek": 74952, "start": 749.52, "end": 753.0799999999999, "text": " This user for this package does have it installed.", "tokens": [639, 4195, 337, 341, 7372, 775, 362, 309, 8899, 13], "temperature": 0.0, "avg_logprob": -0.262627936698295, "compression_ratio": 1.7662337662337662, "no_speech_prob": 3.269713488407433e-05}, {"id": 161, "seek": 74952, "start": 753.0799999999999, "end": 755.88, "text": " So you can kind of see how this is structured.", "tokens": [407, 291, 393, 733, 295, 536, 577, 341, 307, 18519, 13], "temperature": 0.0, "avg_logprob": -0.262627936698295, "compression_ratio": 1.7662337662337662, "no_speech_prob": 3.269713488407433e-05}, {"id": 162, "seek": 74952, "start": 755.88, "end": 759.52, "text": " They added a bunch of additional potential predictors for you.", "tokens": [814, 3869, 257, 3840, 295, 4497, 3995, 6069, 830, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.262627936698295, "compression_ratio": 1.7662337662337662, "no_speech_prob": 3.269713488407433e-05}, {"id": 163, "seek": 74952, "start": 759.52, "end": 761.64, "text": " How many dependencies does this package have?", "tokens": [1012, 867, 36606, 775, 341, 7372, 362, 30], "temperature": 0.0, "avg_logprob": -0.262627936698295, "compression_ratio": 1.7662337662337662, "no_speech_prob": 3.269713488407433e-05}, {"id": 164, "seek": 74952, "start": 761.64, "end": 763.64, "text": " How many suggestions does this package have?", "tokens": [1012, 867, 13396, 775, 341, 7372, 362, 30], "temperature": 0.0, "avg_logprob": -0.262627936698295, "compression_ratio": 1.7662337662337662, "no_speech_prob": 3.269713488407433e-05}, {"id": 165, "seek": 74952, "start": 763.64, "end": 765.3199999999999, "text": " How many imports?", "tokens": [1012, 867, 41596, 30], "temperature": 0.0, "avg_logprob": -0.262627936698295, "compression_ratio": 1.7662337662337662, "no_speech_prob": 3.269713488407433e-05}, {"id": 166, "seek": 74952, "start": 765.3199999999999, "end": 769.04, "text": " How many of those task views on CRAN is it included in?", "tokens": [1012, 867, 295, 729, 5633, 6809, 322, 14123, 1770, 307, 309, 5556, 294, 30], "temperature": 0.0, "avg_logprob": -0.262627936698295, "compression_ratio": 1.7662337662337662, "no_speech_prob": 3.269713488407433e-05}, {"id": 167, "seek": 74952, "start": 769.04, "end": 770.52, "text": " Is it a core package?", "tokens": [1119, 309, 257, 4965, 7372, 30], "temperature": 0.0, "avg_logprob": -0.262627936698295, "compression_ratio": 1.7662337662337662, "no_speech_prob": 3.269713488407433e-05}, {"id": 168, "seek": 74952, "start": 770.52, "end": 771.52, "text": " Is it a recommended package?", "tokens": [1119, 309, 257, 9628, 7372, 30], "temperature": 0.0, "avg_logprob": -0.262627936698295, "compression_ratio": 1.7662337662337662, "no_speech_prob": 3.269713488407433e-05}, {"id": 169, "seek": 74952, "start": 771.52, "end": 772.72, "text": " Who maintains it?", "tokens": [2102, 33385, 309, 30], "temperature": 0.0, "avg_logprob": -0.262627936698295, "compression_ratio": 1.7662337662337662, "no_speech_prob": 3.269713488407433e-05}, {"id": 170, "seek": 74952, "start": 772.72, "end": 775.9399999999999, "text": " And so forth.", "tokens": [400, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.262627936698295, "compression_ratio": 1.7662337662337662, "no_speech_prob": 3.269713488407433e-05}, {"id": 171, "seek": 77594, "start": 775.94, "end": 781.32, "text": " So I found this not particularly easy to get my head around what this looks like.", "tokens": [407, 286, 1352, 341, 406, 4098, 1858, 281, 483, 452, 1378, 926, 437, 341, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.12701630879597492, "compression_ratio": 1.5740740740740742, "no_speech_prob": 1.1124958291475195e-05}, {"id": 172, "seek": 77594, "start": 781.32, "end": 789.1600000000001, "text": " So I used my number one most favorite tool for data visualization and analysis, which", "tokens": [407, 286, 1143, 452, 1230, 472, 881, 2954, 2290, 337, 1412, 25801, 293, 5215, 11, 597], "temperature": 0.0, "avg_logprob": -0.12701630879597492, "compression_ratio": 1.5740740740740742, "no_speech_prob": 1.1124958291475195e-05}, {"id": 173, "seek": 77594, "start": 789.1600000000001, "end": 790.1600000000001, "text": " is a pivot table.", "tokens": [307, 257, 14538, 3199, 13], "temperature": 0.0, "avg_logprob": -0.12701630879597492, "compression_ratio": 1.5740740740740742, "no_speech_prob": 1.1124958291475195e-05}, {"id": 174, "seek": 77594, "start": 790.1600000000001, "end": 796.5200000000001, "text": " A pivot table is something which dynamically lets you slice and dice your data.", "tokens": [316, 14538, 3199, 307, 746, 597, 43492, 6653, 291, 13153, 293, 10313, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12701630879597492, "compression_ratio": 1.5740740740740742, "no_speech_prob": 1.1124958291475195e-05}, {"id": 175, "seek": 77594, "start": 796.5200000000001, "end": 801.8800000000001, "text": " If you've used maybe Tableau or something like that, you'll know the feel.", "tokens": [759, 291, 600, 1143, 1310, 25535, 1459, 420, 746, 411, 300, 11, 291, 603, 458, 264, 841, 13], "temperature": 0.0, "avg_logprob": -0.12701630879597492, "compression_ratio": 1.5740740740740742, "no_speech_prob": 1.1124958291475195e-05}, {"id": 176, "seek": 80188, "start": 801.88, "end": 806.16, "text": " This is kind of like Tableau, it doesn't cost $1,000.", "tokens": [639, 307, 733, 295, 411, 25535, 1459, 11, 309, 1177, 380, 2063, 1848, 16, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.18781423568725586, "compression_ratio": 1.55859375, "no_speech_prob": 3.2190462661674246e-05}, {"id": 177, "seek": 80188, "start": 806.16, "end": 808.2, "text": " No, I mean, Tableau's got cool stuff as well.", "tokens": [883, 11, 286, 914, 11, 25535, 1459, 311, 658, 1627, 1507, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.18781423568725586, "compression_ratio": 1.55859375, "no_speech_prob": 3.2190462661674246e-05}, {"id": 178, "seek": 80188, "start": 808.2, "end": 811.56, "text": " But this is fantastic for most things I find I need to do.", "tokens": [583, 341, 307, 5456, 337, 881, 721, 286, 915, 286, 643, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.18781423568725586, "compression_ratio": 1.55859375, "no_speech_prob": 3.2190462661674246e-05}, {"id": 179, "seek": 80188, "start": 811.56, "end": 818.92, "text": " And so in this case, I simply drag user ID up to the top and I dragged package name down", "tokens": [400, 370, 294, 341, 1389, 11, 286, 2935, 5286, 4195, 7348, 493, 281, 264, 1192, 293, 286, 25717, 7372, 1315, 760], "temperature": 0.0, "avg_logprob": -0.18781423568725586, "compression_ratio": 1.55859375, "no_speech_prob": 3.2190462661674246e-05}, {"id": 180, "seek": 80188, "start": 818.92, "end": 824.74, "text": " to the side and just quickly threw this into a matrix basically.", "tokens": [281, 264, 1252, 293, 445, 2661, 11918, 341, 666, 257, 8141, 1936, 13], "temperature": 0.0, "avg_logprob": -0.18781423568725586, "compression_ratio": 1.55859375, "no_speech_prob": 3.2190462661674246e-05}, {"id": 181, "seek": 80188, "start": 824.74, "end": 829.8, "text": " And so you can see here what this data looks like, which is that those nasty people at", "tokens": [400, 370, 291, 393, 536, 510, 437, 341, 1412, 1542, 411, 11, 597, 307, 300, 729, 17923, 561, 412], "temperature": 0.0, "avg_logprob": -0.18781423568725586, "compression_ratio": 1.55859375, "no_speech_prob": 3.2190462661674246e-05}, {"id": 182, "seek": 82980, "start": 829.8, "end": 834.3599999999999, "text": " dataverses.com has deleted a whole bunch of things in this matrix.", "tokens": [1412, 840, 279, 13, 1112, 575, 22981, 257, 1379, 3840, 295, 721, 294, 341, 8141, 13], "temperature": 0.0, "avg_logprob": -0.15580380179665304, "compression_ratio": 1.7078651685393258, "no_speech_prob": 2.212507024523802e-05}, {"id": 183, "seek": 82980, "start": 834.3599999999999, "end": 837.16, "text": " So that's the stuff that they want us to predict.", "tokens": [407, 300, 311, 264, 1507, 300, 436, 528, 505, 281, 6069, 13], "temperature": 0.0, "avg_logprob": -0.15580380179665304, "compression_ratio": 1.7078651685393258, "no_speech_prob": 2.212507024523802e-05}, {"id": 184, "seek": 82980, "start": 837.16, "end": 841.0, "text": " And then we can see that generally as you expect, there's ones and zeros.", "tokens": [400, 550, 321, 393, 536, 300, 5101, 382, 291, 2066, 11, 456, 311, 2306, 293, 35193, 13], "temperature": 0.0, "avg_logprob": -0.15580380179665304, "compression_ratio": 1.7078651685393258, "no_speech_prob": 2.212507024523802e-05}, {"id": 185, "seek": 82980, "start": 841.0, "end": 845.52, "text": " There's some weird shit going on here where some people have things apparently there twice,", "tokens": [821, 311, 512, 3657, 4611, 516, 322, 510, 689, 512, 561, 362, 721, 7970, 456, 6091, 11], "temperature": 0.0, "avg_logprob": -0.15580380179665304, "compression_ratio": 1.7078651685393258, "no_speech_prob": 2.212507024523802e-05}, {"id": 186, "seek": 82980, "start": 845.52, "end": 850.3199999999999, "text": " which suggests to me maybe there's something funny with data collection.", "tokens": [597, 13409, 281, 385, 1310, 456, 311, 746, 4074, 365, 1412, 5765, 13], "temperature": 0.0, "avg_logprob": -0.15580380179665304, "compression_ratio": 1.7078651685393258, "no_speech_prob": 2.212507024523802e-05}, {"id": 187, "seek": 82980, "start": 850.3199999999999, "end": 853.0799999999999, "text": " And there's other interesting things.", "tokens": [400, 456, 311, 661, 1880, 721, 13], "temperature": 0.0, "avg_logprob": -0.15580380179665304, "compression_ratio": 1.7078651685393258, "no_speech_prob": 2.212507024523802e-05}, {"id": 188, "seek": 82980, "start": 853.0799999999999, "end": 858.5999999999999, "text": " There are some things which seem to be quite widely installed.", "tokens": [821, 366, 512, 721, 597, 1643, 281, 312, 1596, 13371, 8899, 13], "temperature": 0.0, "avg_logprob": -0.15580380179665304, "compression_ratio": 1.7078651685393258, "no_speech_prob": 2.212507024523802e-05}, {"id": 189, "seek": 85860, "start": 858.6, "end": 862.44, "text": " Most people don't install most packages.", "tokens": [4534, 561, 500, 380, 3625, 881, 17401, 13], "temperature": 0.0, "avg_logprob": -0.15047963784665477, "compression_ratio": 1.6213991769547325, "no_speech_prob": 3.373215440660715e-05}, {"id": 190, "seek": 85860, "start": 862.44, "end": 868.28, "text": " And there is this mysterious user number five, who is the world's biggest R package slut.", "tokens": [400, 456, 307, 341, 13831, 4195, 1230, 1732, 11, 567, 307, 264, 1002, 311, 3880, 497, 7372, 41496, 13], "temperature": 0.0, "avg_logprob": -0.15047963784665477, "compression_ratio": 1.6213991769547325, "no_speech_prob": 3.373215440660715e-05}, {"id": 191, "seek": 85860, "start": 868.28, "end": 873.16, "text": " He or she installs everything that they can.", "tokens": [634, 420, 750, 3625, 82, 1203, 300, 436, 393, 13], "temperature": 0.0, "avg_logprob": -0.15047963784665477, "compression_ratio": 1.6213991769547325, "no_speech_prob": 3.373215440660715e-05}, {"id": 192, "seek": 85860, "start": 873.16, "end": 879.28, "text": " And I can only imagine that ADACGH is particularly hard to install because not even user number", "tokens": [400, 286, 393, 787, 3811, 300, 9135, 4378, 4269, 307, 4098, 1152, 281, 3625, 570, 406, 754, 4195, 1230], "temperature": 0.0, "avg_logprob": -0.15047963784665477, "compression_ratio": 1.6213991769547325, "no_speech_prob": 3.373215440660715e-05}, {"id": 193, "seek": 85860, "start": 879.28, "end": 882.48, "text": " five managed to get around to it.", "tokens": [1732, 6453, 281, 483, 926, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.15047963784665477, "compression_ratio": 1.6213991769547325, "no_speech_prob": 3.373215440660715e-05}, {"id": 194, "seek": 85860, "start": 882.48, "end": 887.6800000000001, "text": " So you can see how kind of creating a simple little picture like this, I can get a sense", "tokens": [407, 291, 393, 536, 577, 733, 295, 4084, 257, 2199, 707, 3036, 411, 341, 11, 286, 393, 483, 257, 2020], "temperature": 0.0, "avg_logprob": -0.15047963784665477, "compression_ratio": 1.6213991769547325, "no_speech_prob": 3.373215440660715e-05}, {"id": 195, "seek": 88768, "start": 887.68, "end": 892.52, "text": " of what's going on.", "tokens": [295, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.0958662936561986, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.1443289369926788e-05}, {"id": 196, "seek": 88768, "start": 892.52, "end": 900.4, "text": " So I took that data in the R package competition and I kind of thought, well, if I just knew", "tokens": [407, 286, 1890, 300, 1412, 294, 264, 497, 7372, 6211, 293, 286, 733, 295, 1194, 11, 731, 11, 498, 286, 445, 2586], "temperature": 0.0, "avg_logprob": -0.0958662936561986, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.1443289369926788e-05}, {"id": 197, "seek": 88768, "start": 900.4, "end": 901.4, "text": " for a particular...", "tokens": [337, 257, 1729, 485], "temperature": 0.0, "avg_logprob": -0.0958662936561986, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.1443289369926788e-05}, {"id": 198, "seek": 88768, "start": 901.4, "end": 904.4399999999999, "text": " So let's say this empty cell is the one we're trying to predict.", "tokens": [407, 718, 311, 584, 341, 6707, 2815, 307, 264, 472, 321, 434, 1382, 281, 6069, 13], "temperature": 0.0, "avg_logprob": -0.0958662936561986, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.1443289369926788e-05}, {"id": 199, "seek": 88768, "start": 904.4399999999999, "end": 910.0799999999999, "text": " So if I just knew in general how commonly acceptance sampling was installed and how", "tokens": [407, 498, 286, 445, 2586, 294, 2674, 577, 12719, 20351, 21179, 390, 8899, 293, 577], "temperature": 0.0, "avg_logprob": -0.0958662936561986, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.1443289369926788e-05}, {"id": 200, "seek": 88768, "start": 910.0799999999999, "end": 916.76, "text": " often user number one installed stuff, I probably got a good sense of the probability of user", "tokens": [2049, 4195, 1230, 472, 8899, 1507, 11, 286, 1391, 658, 257, 665, 2020, 295, 264, 8482, 295, 4195], "temperature": 0.0, "avg_logprob": -0.0958662936561986, "compression_ratio": 1.6025641025641026, "no_speech_prob": 2.1443289369926788e-05}, {"id": 201, "seek": 91676, "start": 916.76, "end": 919.84, "text": " number one installing acceptance sampling.", "tokens": [1230, 472, 20762, 20351, 21179, 13], "temperature": 0.0, "avg_logprob": -0.183495873803491, "compression_ratio": 1.64, "no_speech_prob": 2.4298027710756287e-05}, {"id": 202, "seek": 91676, "start": 919.84, "end": 924.08, "text": " So to me, one of the interesting points here was to think, actually, I don't think I care", "tokens": [407, 281, 385, 11, 472, 295, 264, 1880, 2793, 510, 390, 281, 519, 11, 767, 11, 286, 500, 380, 519, 286, 1127], "temperature": 0.0, "avg_logprob": -0.183495873803491, "compression_ratio": 1.64, "no_speech_prob": 2.4298027710756287e-05}, {"id": 203, "seek": 91676, "start": 924.08, "end": 926.56, "text": " about any of this stuff.", "tokens": [466, 604, 295, 341, 1507, 13], "temperature": 0.0, "avg_logprob": -0.183495873803491, "compression_ratio": 1.64, "no_speech_prob": 2.4298027710756287e-05}, {"id": 204, "seek": 91676, "start": 926.56, "end": 936.96, "text": " So I jumped into R and all I did was I basically said, okay, read that CSV file in.", "tokens": [407, 286, 13864, 666, 497, 293, 439, 286, 630, 390, 286, 1936, 848, 11, 1392, 11, 1401, 300, 48814, 3991, 294, 13], "temperature": 0.0, "avg_logprob": -0.183495873803491, "compression_ratio": 1.64, "no_speech_prob": 2.4298027710756287e-05}, {"id": 205, "seek": 91676, "start": 936.96, "end": 939.2, "text": " There's a whole bunch of rows here because this is my entire solution.", "tokens": [821, 311, 257, 1379, 3840, 295, 13241, 510, 570, 341, 307, 452, 2302, 3827, 13], "temperature": 0.0, "avg_logprob": -0.183495873803491, "compression_ratio": 1.64, "no_speech_prob": 2.4298027710756287e-05}, {"id": 206, "seek": 91676, "start": 939.2, "end": 942.24, "text": " But I'm just going to show you the rows I used for solution number one.", "tokens": [583, 286, 478, 445, 516, 281, 855, 291, 264, 13241, 286, 1143, 337, 3827, 1230, 472, 13], "temperature": 0.0, "avg_logprob": -0.183495873803491, "compression_ratio": 1.64, "no_speech_prob": 2.4298027710756287e-05}, {"id": 207, "seek": 91676, "start": 942.24, "end": 945.4399999999999, "text": " So read in the whole lot.", "tokens": [407, 1401, 294, 264, 1379, 688, 13], "temperature": 0.0, "avg_logprob": -0.183495873803491, "compression_ratio": 1.64, "no_speech_prob": 2.4298027710756287e-05}, {"id": 208, "seek": 94544, "start": 945.44, "end": 950.08, "text": " So although user is a number, treat it as a factor because user number one is not 50", "tokens": [407, 4878, 4195, 307, 257, 1230, 11, 2387, 309, 382, 257, 5952, 570, 4195, 1230, 472, 307, 406, 2625], "temperature": 0.0, "avg_logprob": -0.21184274192168334, "compression_ratio": 1.8155737704918034, "no_speech_prob": 2.014470192079898e-05}, {"id": 209, "seek": 94544, "start": 950.08, "end": 954.48, "text": " times worse than user number 50.", "tokens": [1413, 5324, 813, 4195, 1230, 2625, 13], "temperature": 0.0, "avg_logprob": -0.21184274192168334, "compression_ratio": 1.8155737704918034, "no_speech_prob": 2.014470192079898e-05}, {"id": 210, "seek": 94544, "start": 954.48, "end": 958.8800000000001, "text": " Those trues and falses turn them into one to zero to make life a bit easier.", "tokens": [3950, 504, 1247, 293, 16720, 279, 1261, 552, 666, 472, 281, 4018, 281, 652, 993, 257, 857, 3571, 13], "temperature": 0.0, "avg_logprob": -0.21184274192168334, "compression_ratio": 1.8155737704918034, "no_speech_prob": 2.014470192079898e-05}, {"id": 211, "seek": 94544, "start": 958.8800000000001, "end": 964.2, "text": " And now apply mean, the mean function to each user across their installations and apply", "tokens": [400, 586, 3079, 914, 11, 264, 914, 2445, 281, 1184, 4195, 2108, 641, 41932, 293, 3079], "temperature": 0.0, "avg_logprob": -0.21184274192168334, "compression_ratio": 1.8155737704918034, "no_speech_prob": 2.014470192079898e-05}, {"id": 212, "seek": 94544, "start": 964.2, "end": 968.9200000000001, "text": " the mean function to each package across that package's installations.", "tokens": [264, 914, 2445, 281, 1184, 7372, 2108, 300, 7372, 311, 41932, 13], "temperature": 0.0, "avg_logprob": -0.21184274192168334, "compression_ratio": 1.8155737704918034, "no_speech_prob": 2.014470192079898e-05}, {"id": 213, "seek": 94544, "start": 968.9200000000001, "end": 973.4000000000001, "text": " So I now have got a couple of lookups basically that tell me user number 50 installs this", "tokens": [407, 286, 586, 362, 658, 257, 1916, 295, 574, 7528, 1936, 300, 980, 385, 4195, 1230, 2625, 3625, 82, 341], "temperature": 0.0, "avg_logprob": -0.21184274192168334, "compression_ratio": 1.8155737704918034, "no_speech_prob": 2.014470192079898e-05}, {"id": 214, "seek": 97340, "start": 973.4, "end": 975.92, "text": " percent of packages.", "tokens": [3043, 295, 17401, 13], "temperature": 0.0, "avg_logprob": -0.13319629714602516, "compression_ratio": 1.7473684210526317, "no_speech_prob": 7.889086191426031e-06}, {"id": 215, "seek": 97340, "start": 975.92, "end": 980.76, "text": " This particular package is installed by this percent of users.", "tokens": [639, 1729, 7372, 307, 8899, 538, 341, 3043, 295, 5022, 13], "temperature": 0.0, "avg_logprob": -0.13319629714602516, "compression_ratio": 1.7473684210526317, "no_speech_prob": 7.889086191426031e-06}, {"id": 216, "seek": 97340, "start": 980.76, "end": 987.04, "text": " And then I just stuck them basically back into my file of predictors.", "tokens": [400, 550, 286, 445, 5541, 552, 1936, 646, 666, 452, 3991, 295, 6069, 830, 13], "temperature": 0.0, "avg_logprob": -0.13319629714602516, "compression_ratio": 1.7473684210526317, "no_speech_prob": 7.889086191426031e-06}, {"id": 217, "seek": 97340, "start": 987.04, "end": 993.6, "text": " So I basically did these simple lookups for each row to look up the user and find out", "tokens": [407, 286, 1936, 630, 613, 2199, 574, 7528, 337, 1184, 5386, 281, 574, 493, 264, 4195, 293, 915, 484], "temperature": 0.0, "avg_logprob": -0.13319629714602516, "compression_ratio": 1.7473684210526317, "no_speech_prob": 7.889086191426031e-06}, {"id": 218, "seek": 97340, "start": 993.6, "end": 999.68, "text": " for that row the mean for that user and the mean for that package.", "tokens": [337, 300, 5386, 264, 914, 337, 300, 4195, 293, 264, 914, 337, 300, 7372, 13], "temperature": 0.0, "avg_logprob": -0.13319629714602516, "compression_ratio": 1.7473684210526317, "no_speech_prob": 7.889086191426031e-06}, {"id": 219, "seek": 97340, "start": 999.68, "end": 1000.68, "text": " And that was actually it.", "tokens": [400, 300, 390, 767, 309, 13], "temperature": 0.0, "avg_logprob": -0.13319629714602516, "compression_ratio": 1.7473684210526317, "no_speech_prob": 7.889086191426031e-06}, {"id": 220, "seek": 100068, "start": 1000.68, "end": 1011.52, "text": " At that point, I then created a GLM in which I had, I created a GLM in which obviously", "tokens": [1711, 300, 935, 11, 286, 550, 2942, 257, 16225, 44, 294, 597, 286, 632, 11, 286, 2942, 257, 16225, 44, 294, 597, 2745], "temperature": 0.0, "avg_logprob": -0.2568169775463286, "compression_ratio": 1.5759162303664922, "no_speech_prob": 9.516023965261411e-06}, {"id": 221, "seek": 100068, "start": 1011.52, "end": 1015.3599999999999, "text": " I had my ones and zeros of installations as the thing I was predicting.", "tokens": [286, 632, 452, 2306, 293, 35193, 295, 41932, 382, 264, 551, 286, 390, 32884, 13], "temperature": 0.0, "avg_logprob": -0.2568169775463286, "compression_ratio": 1.5759162303664922, "no_speech_prob": 9.516023965261411e-06}, {"id": 222, "seek": 100068, "start": 1015.3599999999999, "end": 1017.92, "text": " And my first version I had UP and PP.", "tokens": [400, 452, 700, 3037, 286, 632, 20074, 293, 37369, 13], "temperature": 0.0, "avg_logprob": -0.2568169775463286, "compression_ratio": 1.5759162303664922, "no_speech_prob": 9.516023965261411e-06}, {"id": 223, "seek": 100068, "start": 1017.92, "end": 1023.64, "text": " So these two probabilities as my predictors.", "tokens": [407, 613, 732, 33783, 382, 452, 6069, 830, 13], "temperature": 0.0, "avg_logprob": -0.2568169775463286, "compression_ratio": 1.5759162303664922, "no_speech_prob": 9.516023965261411e-06}, {"id": 224, "seek": 100068, "start": 1023.64, "end": 1026.8, "text": " In fact, no, in first version it was even easier than that.", "tokens": [682, 1186, 11, 572, 11, 294, 700, 3037, 309, 390, 754, 3571, 813, 300, 13], "temperature": 0.0, "avg_logprob": -0.2568169775463286, "compression_ratio": 1.5759162303664922, "no_speech_prob": 9.516023965261411e-06}, {"id": 225, "seek": 102680, "start": 1026.8, "end": 1032.28, "text": " All I did in fact was I took the max of those two things.", "tokens": [1057, 286, 630, 294, 1186, 390, 286, 1890, 264, 11469, 295, 729, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.22827120484976932, "compression_ratio": 1.6007905138339922, "no_speech_prob": 3.2191041100304574e-05}, {"id": 226, "seek": 102680, "start": 1032.28, "end": 1036.3999999999999, "text": " So Pmax, if you're not familiar with R is just something that does a max on each row", "tokens": [407, 430, 41167, 11, 498, 291, 434, 406, 4963, 365, 497, 307, 445, 746, 300, 775, 257, 11469, 322, 1184, 5386], "temperature": 0.0, "avg_logprob": -0.22827120484976932, "compression_ratio": 1.6007905138339922, "no_speech_prob": 3.2191041100304574e-05}, {"id": 227, "seek": 102680, "start": 1036.3999999999999, "end": 1037.3999999999999, "text": " individually.", "tokens": [16652, 13], "temperature": 0.0, "avg_logprob": -0.22827120484976932, "compression_ratio": 1.6007905138339922, "no_speech_prob": 3.2191041100304574e-05}, {"id": 228, "seek": 102680, "start": 1037.3999999999999, "end": 1043.12, "text": " In R nearly everything works on vectors by default except for max.", "tokens": [682, 497, 6217, 1203, 1985, 322, 18875, 538, 7576, 3993, 337, 11469, 13], "temperature": 0.0, "avg_logprob": -0.22827120484976932, "compression_ratio": 1.6007905138339922, "no_speech_prob": 3.2191041100304574e-05}, {"id": 229, "seek": 102680, "start": 1043.12, "end": 1044.9199999999998, "text": " So that's why we have to use Pmax.", "tokens": [407, 300, 311, 983, 321, 362, 281, 764, 430, 41167, 13], "temperature": 0.0, "avg_logprob": -0.22827120484976932, "compression_ratio": 1.6007905138339922, "no_speech_prob": 3.2191041100304574e-05}, {"id": 230, "seek": 102680, "start": 1044.9199999999998, "end": 1048.2, "text": " That's something well worth knowing.", "tokens": [663, 311, 746, 731, 3163, 5276, 13], "temperature": 0.0, "avg_logprob": -0.22827120484976932, "compression_ratio": 1.6007905138339922, "no_speech_prob": 3.2191041100304574e-05}, {"id": 231, "seek": 102680, "start": 1048.2, "end": 1049.8, "text": " So I just took the max.", "tokens": [407, 286, 445, 1890, 264, 11469, 13], "temperature": 0.0, "avg_logprob": -0.22827120484976932, "compression_ratio": 1.6007905138339922, "no_speech_prob": 3.2191041100304574e-05}, {"id": 232, "seek": 102680, "start": 1049.8, "end": 1056.12, "text": " So I, you know, this user installs 30% of things and this package is installed by 40%", "tokens": [407, 286, 11, 291, 458, 11, 341, 4195, 3625, 82, 2217, 4, 295, 721, 293, 341, 7372, 307, 8899, 538, 3356, 4], "temperature": 0.0, "avg_logprob": -0.22827120484976932, "compression_ratio": 1.6007905138339922, "no_speech_prob": 3.2191041100304574e-05}, {"id": 233, "seek": 105612, "start": 1056.12, "end": 1057.12, "text": " of users.", "tokens": [295, 5022, 13], "temperature": 0.0, "avg_logprob": -0.17366220316755662, "compression_ratio": 1.546218487394958, "no_speech_prob": 1.3845382454746868e-05}, {"id": 234, "seek": 105612, "start": 1057.12, "end": 1058.12, "text": " So the max of the two is 40%.", "tokens": [407, 264, 11469, 295, 264, 732, 307, 3356, 6856], "temperature": 0.0, "avg_logprob": -0.17366220316755662, "compression_ratio": 1.546218487394958, "no_speech_prob": 1.3845382454746868e-05}, {"id": 235, "seek": 105612, "start": 1058.12, "end": 1062.76, "text": " And I actually created a GLM with just one predictor.", "tokens": [400, 286, 767, 2942, 257, 16225, 44, 365, 445, 472, 6069, 284, 13], "temperature": 0.0, "avg_logprob": -0.17366220316755662, "compression_ratio": 1.546218487394958, "no_speech_prob": 1.3845382454746868e-05}, {"id": 236, "seek": 105612, "start": 1062.76, "end": 1069.0, "text": " The benchmark that was created by the data people for this used a GLM on all of those", "tokens": [440, 18927, 300, 390, 2942, 538, 264, 1412, 561, 337, 341, 1143, 257, 16225, 44, 322, 439, 295, 729], "temperature": 0.0, "avg_logprob": -0.17366220316755662, "compression_ratio": 1.546218487394958, "no_speech_prob": 1.3845382454746868e-05}, {"id": 237, "seek": 105612, "start": 1069.0, "end": 1075.6399999999999, "text": " predictors, including all kinds of latent analysis of the manual pages and maintaining", "tokens": [6069, 830, 11, 3009, 439, 3685, 295, 48994, 5215, 295, 264, 9688, 7183, 293, 14916], "temperature": 0.0, "avg_logprob": -0.17366220316755662, "compression_ratio": 1.546218487394958, "no_speech_prob": 1.3845382454746868e-05}, {"id": 238, "seek": 105612, "start": 1075.6399999999999, "end": 1076.9199999999998, "text": " names and God knows what.", "tokens": [5288, 293, 1265, 3255, 437, 13], "temperature": 0.0, "avg_logprob": -0.17366220316755662, "compression_ratio": 1.546218487394958, "no_speech_prob": 1.3845382454746868e-05}, {"id": 239, "seek": 105612, "start": 1076.9199999999998, "end": 1080.1999999999998, "text": " And they had a AUC of 0.8.", "tokens": [400, 436, 632, 257, 7171, 34, 295, 1958, 13, 23, 13], "temperature": 0.0, "avg_logprob": -0.17366220316755662, "compression_ratio": 1.546218487394958, "no_speech_prob": 1.3845382454746868e-05}, {"id": 240, "seek": 105612, "start": 1080.1999999999998, "end": 1085.5, "text": " This five line of code thing had an AUC of 0.95.", "tokens": [639, 1732, 1622, 295, 3089, 551, 632, 364, 7171, 34, 295, 1958, 13, 15718, 13], "temperature": 0.0, "avg_logprob": -0.17366220316755662, "compression_ratio": 1.546218487394958, "no_speech_prob": 1.3845382454746868e-05}, {"id": 241, "seek": 108550, "start": 1085.5, "end": 1093.12, "text": " So you know, the message here is don't overcomplicate things.", "tokens": [407, 291, 458, 11, 264, 3636, 510, 307, 500, 380, 670, 1112, 4770, 473, 721, 13], "temperature": 0.0, "avg_logprob": -0.1902716181299708, "compression_ratio": 1.639676113360324, "no_speech_prob": 1.2218719348311424e-05}, {"id": 242, "seek": 108550, "start": 1093.12, "end": 1098.04, "text": " If people give you data, don't assume that you need to use it and, you know, look at", "tokens": [759, 561, 976, 291, 1412, 11, 500, 380, 6552, 300, 291, 643, 281, 764, 309, 293, 11, 291, 458, 11, 574, 412], "temperature": 0.0, "avg_logprob": -0.1902716181299708, "compression_ratio": 1.639676113360324, "no_speech_prob": 1.2218719348311424e-05}, {"id": 243, "seek": 108550, "start": 1098.04, "end": 1099.56, "text": " pictures.", "tokens": [5242, 13], "temperature": 0.0, "avg_logprob": -0.1902716181299708, "compression_ratio": 1.639676113360324, "no_speech_prob": 1.2218719348311424e-05}, {"id": 244, "seek": 108550, "start": 1099.56, "end": 1103.14, "text": " So if we have a look at kind of my progress in there.", "tokens": [407, 498, 321, 362, 257, 574, 412, 733, 295, 452, 4205, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.1902716181299708, "compression_ratio": 1.639676113360324, "no_speech_prob": 1.2218719348311424e-05}, {"id": 245, "seek": 108550, "start": 1103.14, "end": 1109.46, "text": " So here's my first attempt, which was basically to multiply the use probability by the package", "tokens": [407, 510, 311, 452, 700, 5217, 11, 597, 390, 1936, 281, 12972, 264, 764, 8482, 538, 264, 7372], "temperature": 0.0, "avg_logprob": -0.1902716181299708, "compression_ratio": 1.639676113360324, "no_speech_prob": 1.2218719348311424e-05}, {"id": 246, "seek": 108550, "start": 1109.46, "end": 1110.88, "text": " probability.", "tokens": [8482, 13], "temperature": 0.0, "avg_logprob": -0.1902716181299708, "compression_ratio": 1.639676113360324, "no_speech_prob": 1.2218719348311424e-05}, {"id": 247, "seek": 108550, "start": 1110.88, "end": 1114.24, "text": " And you can see one of the nice things in Kaggle is you get a history of your results.", "tokens": [400, 291, 393, 536, 472, 295, 264, 1481, 721, 294, 48751, 22631, 307, 291, 483, 257, 2503, 295, 428, 3542, 13], "temperature": 0.0, "avg_logprob": -0.1902716181299708, "compression_ratio": 1.639676113360324, "no_speech_prob": 1.2218719348311424e-05}, {"id": 248, "seek": 111424, "start": 1114.24, "end": 1117.72, "text": " So here's my 0.84 AUC.", "tokens": [407, 510, 311, 452, 1958, 13, 25494, 7171, 34, 13], "temperature": 0.0, "avg_logprob": -0.21342006751469203, "compression_ratio": 1.522633744855967, "no_speech_prob": 2.6273724870407023e-05}, {"id": 249, "seek": 111424, "start": 1117.72, "end": 1121.76, "text": " And then I changed it to using the maximum of two and there's my 0.95 AUC.", "tokens": [400, 550, 286, 3105, 309, 281, 1228, 264, 6674, 295, 732, 293, 456, 311, 452, 1958, 13, 15718, 7171, 34, 13], "temperature": 0.0, "avg_logprob": -0.21342006751469203, "compression_ratio": 1.522633744855967, "no_speech_prob": 2.6273724870407023e-05}, {"id": 250, "seek": 111424, "start": 1121.76, "end": 1125.72, "text": " And I thought, oh, that was good.", "tokens": [400, 286, 1194, 11, 1954, 11, 300, 390, 665, 13], "temperature": 0.0, "avg_logprob": -0.21342006751469203, "compression_ratio": 1.522633744855967, "no_speech_prob": 2.6273724870407023e-05}, {"id": 251, "seek": 111424, "start": 1125.72, "end": 1130.08, "text": " I imagine how powerful this will be when I use all that data that they gave us with a", "tokens": [286, 3811, 577, 4005, 341, 486, 312, 562, 286, 764, 439, 300, 1412, 300, 436, 2729, 505, 365, 257], "temperature": 0.0, "avg_logprob": -0.21342006751469203, "compression_ratio": 1.522633744855967, "no_speech_prob": 2.6273724870407023e-05}, {"id": 252, "seek": 111424, "start": 1130.08, "end": 1132.28, "text": " fancy random forest.", "tokens": [10247, 4974, 6719, 13], "temperature": 0.0, "avg_logprob": -0.21342006751469203, "compression_ratio": 1.522633744855967, "no_speech_prob": 2.6273724870407023e-05}, {"id": 253, "seek": 111424, "start": 1132.28, "end": 1133.76, "text": " And it went backwards.", "tokens": [400, 309, 1437, 12204, 13], "temperature": 0.0, "avg_logprob": -0.21342006751469203, "compression_ratio": 1.522633744855967, "no_speech_prob": 2.6273724870407023e-05}, {"id": 254, "seek": 111424, "start": 1133.76, "end": 1134.76, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.21342006751469203, "compression_ratio": 1.522633744855967, "no_speech_prob": 2.6273724870407023e-05}, {"id": 255, "seek": 111424, "start": 1134.76, "end": 1138.88, "text": " So you can really see that actually a bit of focused simple analysis can often take", "tokens": [407, 291, 393, 534, 536, 300, 767, 257, 857, 295, 5178, 2199, 5215, 393, 2049, 747], "temperature": 0.0, "avg_logprob": -0.21342006751469203, "compression_ratio": 1.522633744855967, "no_speech_prob": 2.6273724870407023e-05}, {"id": 256, "seek": 111424, "start": 1138.88, "end": 1140.86, "text": " you a lot further.", "tokens": [291, 257, 688, 3052, 13], "temperature": 0.0, "avg_logprob": -0.21342006751469203, "compression_ratio": 1.522633744855967, "no_speech_prob": 2.6273724870407023e-05}, {"id": 257, "seek": 114086, "start": 1140.86, "end": 1146.4799999999998, "text": " So if we look to the next page, we can kind of see where I kind of kept thinking random", "tokens": [407, 498, 321, 574, 281, 264, 958, 3028, 11, 321, 393, 733, 295, 536, 689, 286, 733, 295, 4305, 1953, 4974], "temperature": 0.0, "avg_logprob": -0.2275270399500112, "compression_ratio": 1.817490494296578, "no_speech_prob": 2.5068895411095582e-05}, {"id": 258, "seek": 114086, "start": 1146.4799999999998, "end": 1150.36, "text": " forests, they ought to buddy works, more random forests and that went backwards.", "tokens": [21700, 11, 436, 13416, 281, 10340, 1985, 11, 544, 4974, 21700, 293, 300, 1437, 12204, 13], "temperature": 0.0, "avg_logprob": -0.2275270399500112, "compression_ratio": 1.817490494296578, "no_speech_prob": 2.5068895411095582e-05}, {"id": 259, "seek": 114086, "start": 1150.36, "end": 1153.28, "text": " And then I started adding in a few extra things.", "tokens": [400, 550, 286, 1409, 5127, 294, 257, 1326, 2857, 721, 13], "temperature": 0.0, "avg_logprob": -0.2275270399500112, "compression_ratio": 1.817490494296578, "no_speech_prob": 2.5068895411095582e-05}, {"id": 260, "seek": 114086, "start": 1153.28, "end": 1158.04, "text": " And then actually I thought, you know, there is one piece of data which is really useful,", "tokens": [400, 550, 767, 286, 1194, 11, 291, 458, 11, 456, 307, 472, 2522, 295, 1412, 597, 307, 534, 4420, 11], "temperature": 0.0, "avg_logprob": -0.2275270399500112, "compression_ratio": 1.817490494296578, "no_speech_prob": 2.5068895411095582e-05}, {"id": 261, "seek": 114086, "start": 1158.04, "end": 1159.84, "text": " which is that dependency graph.", "tokens": [597, 307, 300, 33621, 4295, 13], "temperature": 0.0, "avg_logprob": -0.2275270399500112, "compression_ratio": 1.817490494296578, "no_speech_prob": 2.5068895411095582e-05}, {"id": 262, "seek": 114086, "start": 1159.84, "end": 1165.76, "text": " If somebody has installed package A and it depends on package B, and I know they've got", "tokens": [759, 2618, 575, 8899, 7372, 316, 293, 309, 5946, 322, 7372, 363, 11, 293, 286, 458, 436, 600, 658], "temperature": 0.0, "avg_logprob": -0.2275270399500112, "compression_ratio": 1.817490494296578, "no_speech_prob": 2.5068895411095582e-05}, {"id": 263, "seek": 114086, "start": 1165.76, "end": 1168.8, "text": " package A, then I also know they've got package B.", "tokens": [7372, 316, 11, 550, 286, 611, 458, 436, 600, 658, 7372, 363, 13], "temperature": 0.0, "avg_logprob": -0.2275270399500112, "compression_ratio": 1.817490494296578, "no_speech_prob": 2.5068895411095582e-05}, {"id": 264, "seek": 116880, "start": 1168.8, "end": 1172.9199999999998, "text": " So I added that case.", "tokens": [407, 286, 3869, 300, 1389, 13], "temperature": 0.0, "avg_logprob": -0.21400408537491508, "compression_ratio": 1.5769230769230769, "no_speech_prob": 2.7965994377154857e-05}, {"id": 265, "seek": 116880, "start": 1172.9199999999998, "end": 1177.24, "text": " That's the kind of thing I find a bit difficult to do in R because I think R is a slightly", "tokens": [663, 311, 264, 733, 295, 551, 286, 915, 257, 857, 2252, 281, 360, 294, 497, 570, 286, 519, 497, 307, 257, 4748], "temperature": 0.0, "avg_logprob": -0.21400408537491508, "compression_ratio": 1.5769230769230769, "no_speech_prob": 2.7965994377154857e-05}, {"id": 266, "seek": 116880, "start": 1177.24, "end": 1178.8999999999999, "text": " shit programming language.", "tokens": [4611, 9410, 2856, 13], "temperature": 0.0, "avg_logprob": -0.21400408537491508, "compression_ratio": 1.5769230769230769, "no_speech_prob": 2.7965994377154857e-05}, {"id": 267, "seek": 116880, "start": 1178.8999999999999, "end": 1183.08, "text": " So I did that piece in a language which I quite like, which is C sharp, imported it", "tokens": [407, 286, 630, 300, 2522, 294, 257, 2856, 597, 286, 1596, 411, 11, 597, 307, 383, 8199, 11, 25524, 309], "temperature": 0.0, "avg_logprob": -0.21400408537491508, "compression_ratio": 1.5769230769230769, "no_speech_prob": 2.7965994377154857e-05}, {"id": 268, "seek": 116880, "start": 1183.08, "end": 1188.1599999999999, "text": " back to R. And then as you can see, each time I send something off to Kaggle, I generally", "tokens": [646, 281, 497, 13, 400, 550, 382, 291, 393, 536, 11, 1184, 565, 286, 2845, 746, 766, 281, 48751, 22631, 11, 286, 5101], "temperature": 0.0, "avg_logprob": -0.21400408537491508, "compression_ratio": 1.5769230769230769, "no_speech_prob": 2.7965994377154857e-05}, {"id": 269, "seek": 116880, "start": 1188.1599999999999, "end": 1193.08, "text": " copy and paste into my notes, just the line of code that I ran so I can see exactly what", "tokens": [5055, 293, 9163, 666, 452, 5570, 11, 445, 264, 1622, 295, 3089, 300, 286, 5872, 370, 286, 393, 536, 2293, 437], "temperature": 0.0, "avg_logprob": -0.21400408537491508, "compression_ratio": 1.5769230769230769, "no_speech_prob": 2.7965994377154857e-05}, {"id": 270, "seek": 116880, "start": 1193.08, "end": 1194.22, "text": " it was.", "tokens": [309, 390, 13], "temperature": 0.0, "avg_logprob": -0.21400408537491508, "compression_ratio": 1.5769230769230769, "no_speech_prob": 2.7965994377154857e-05}, {"id": 271, "seek": 119422, "start": 1194.22, "end": 1200.68, "text": " So here I added this dependency graph and I jumped up to.98.", "tokens": [407, 510, 286, 3869, 341, 33621, 4295, 293, 286, 13864, 493, 281, 2411, 22516, 13], "temperature": 0.0, "avg_logprob": -0.13291800649542557, "compression_ratio": 1.7125984251968505, "no_speech_prob": 3.7049499951535836e-05}, {"id": 272, "seek": 119422, "start": 1200.68, "end": 1205.92, "text": " That's basically as far as I got in this competition, which was enough for sixth place.", "tokens": [663, 311, 1936, 382, 1400, 382, 286, 658, 294, 341, 6211, 11, 597, 390, 1547, 337, 15102, 1081, 13], "temperature": 0.0, "avg_logprob": -0.13291800649542557, "compression_ratio": 1.7125984251968505, "no_speech_prob": 3.7049499951535836e-05}, {"id": 273, "seek": 119422, "start": 1205.92, "end": 1208.24, "text": " I made a really stupid mistake.", "tokens": [286, 1027, 257, 534, 6631, 6146, 13], "temperature": 0.0, "avg_logprob": -0.13291800649542557, "compression_ratio": 1.7125984251968505, "no_speech_prob": 3.7049499951535836e-05}, {"id": 274, "seek": 119422, "start": 1208.24, "end": 1213.88, "text": " Yes, if somebody has package A and it depends on package B, then obviously that means they", "tokens": [1079, 11, 498, 2618, 575, 7372, 316, 293, 309, 5946, 322, 7372, 363, 11, 550, 2745, 300, 1355, 436], "temperature": 0.0, "avg_logprob": -0.13291800649542557, "compression_ratio": 1.7125984251968505, "no_speech_prob": 3.7049499951535836e-05}, {"id": 275, "seek": 119422, "start": 1213.88, "end": 1216.46, "text": " got package B. I did that.", "tokens": [658, 7372, 363, 13, 286, 630, 300, 13], "temperature": 0.0, "avg_logprob": -0.13291800649542557, "compression_ratio": 1.7125984251968505, "no_speech_prob": 3.7049499951535836e-05}, {"id": 276, "seek": 119422, "start": 1216.46, "end": 1220.48, "text": " If somebody doesn't have package B and package A depends on it, then you know they definitely", "tokens": [759, 2618, 1177, 380, 362, 7372, 363, 293, 7372, 316, 5946, 322, 309, 11, 550, 291, 458, 436, 2138], "temperature": 0.0, "avg_logprob": -0.13291800649542557, "compression_ratio": 1.7125984251968505, "no_speech_prob": 3.7049499951535836e-05}, {"id": 277, "seek": 119422, "start": 1220.48, "end": 1223.14, "text": " don't have package A. I forgot that piece.", "tokens": [500, 380, 362, 7372, 316, 13, 286, 5298, 300, 2522, 13], "temperature": 0.0, "avg_logprob": -0.13291800649542557, "compression_ratio": 1.7125984251968505, "no_speech_prob": 3.7049499951535836e-05}, {"id": 278, "seek": 122314, "start": 1223.14, "end": 1226.68, "text": " And so when I went back and put that in after the competition was over and I realized I'd", "tokens": [400, 370, 562, 286, 1437, 646, 293, 829, 300, 294, 934, 264, 6211, 390, 670, 293, 286, 5334, 286, 1116], "temperature": 0.0, "avg_logprob": -0.15694376400538854, "compression_ratio": 1.6954887218045114, "no_speech_prob": 2.753234548436012e-05}, {"id": 279, "seek": 122314, "start": 1226.68, "end": 1231.0400000000002, "text": " forgotten it, I realized it could have come about second if I'd just done that.", "tokens": [11832, 309, 11, 286, 5334, 309, 727, 362, 808, 466, 1150, 498, 286, 1116, 445, 1096, 300, 13], "temperature": 0.0, "avg_logprob": -0.15694376400538854, "compression_ratio": 1.6954887218045114, "no_speech_prob": 2.753234548436012e-05}, {"id": 280, "seek": 122314, "start": 1231.0400000000002, "end": 1238.76, "text": " In fact, to get in the top three in this competition, that's probably as much modeling as you needed.", "tokens": [682, 1186, 11, 281, 483, 294, 264, 1192, 1045, 294, 341, 6211, 11, 300, 311, 1391, 382, 709, 15983, 382, 291, 2978, 13], "temperature": 0.0, "avg_logprob": -0.15694376400538854, "compression_ratio": 1.6954887218045114, "no_speech_prob": 2.753234548436012e-05}, {"id": 281, "seek": 122314, "start": 1238.76, "end": 1243.5600000000002, "text": " So I think you can do well in these comps without necessarily being an R expert or necessarily", "tokens": [407, 286, 519, 291, 393, 360, 731, 294, 613, 715, 82, 1553, 4725, 885, 364, 497, 5844, 420, 4725], "temperature": 0.0, "avg_logprob": -0.15694376400538854, "compression_ratio": 1.6954887218045114, "no_speech_prob": 2.753234548436012e-05}, {"id": 282, "seek": 122314, "start": 1243.5600000000002, "end": 1244.5600000000002, "text": " being a stats expert.", "tokens": [885, 257, 18152, 5844, 13], "temperature": 0.0, "avg_logprob": -0.15694376400538854, "compression_ratio": 1.6954887218045114, "no_speech_prob": 2.753234548436012e-05}, {"id": 283, "seek": 122314, "start": 1244.5600000000002, "end": 1252.0, "text": " But you do need to kind of dig into the toolbox appropriately.", "tokens": [583, 291, 360, 643, 281, 733, 295, 2528, 666, 264, 44593, 23505, 13], "temperature": 0.0, "avg_logprob": -0.15694376400538854, "compression_ratio": 1.6954887218045114, "no_speech_prob": 2.753234548436012e-05}, {"id": 284, "seek": 125200, "start": 1252.0, "end": 1258.36, "text": " So let's go back to my extensive slide presentation.", "tokens": [407, 718, 311, 352, 646, 281, 452, 13246, 4137, 5860, 13], "temperature": 0.0, "avg_logprob": -0.17202530409160413, "compression_ratio": 1.5285714285714285, "no_speech_prob": 4.985422492609359e-05}, {"id": 285, "seek": 125200, "start": 1258.36, "end": 1263.72, "text": " So you can see here we talked about data manipulation, about interactive analysis.", "tokens": [407, 291, 393, 536, 510, 321, 2825, 466, 1412, 26475, 11, 466, 15141, 5215, 13], "temperature": 0.0, "avg_logprob": -0.17202530409160413, "compression_ratio": 1.5285714285714285, "no_speech_prob": 4.985422492609359e-05}, {"id": 286, "seek": 125200, "start": 1263.72, "end": 1270.32, "text": " We've talked a bit about visualizations and I include there even simple things like those", "tokens": [492, 600, 2825, 257, 857, 466, 5056, 14455, 293, 286, 4090, 456, 754, 2199, 721, 411, 729], "temperature": 0.0, "avg_logprob": -0.17202530409160413, "compression_ratio": 1.5285714285714285, "no_speech_prob": 4.985422492609359e-05}, {"id": 287, "seek": 125200, "start": 1270.32, "end": 1272.96, "text": " tables we did.", "tokens": [8020, 321, 630, 13], "temperature": 0.0, "avg_logprob": -0.17202530409160413, "compression_ratio": 1.5285714285714285, "no_speech_prob": 4.985422492609359e-05}, {"id": 288, "seek": 125200, "start": 1272.96, "end": 1280.0, "text": " As I just indicated, in my toolbox is some kind of general purpose program tool.", "tokens": [1018, 286, 445, 16176, 11, 294, 452, 44593, 307, 512, 733, 295, 2674, 4334, 1461, 2290, 13], "temperature": 0.0, "avg_logprob": -0.17202530409160413, "compression_ratio": 1.5285714285714285, "no_speech_prob": 4.985422492609359e-05}, {"id": 289, "seek": 128000, "start": 1280.0, "end": 1284.56, "text": " And to me, there's kind of three or four clear leaders in this space.", "tokens": [400, 281, 385, 11, 456, 311, 733, 295, 1045, 420, 1451, 1850, 3523, 294, 341, 1901, 13], "temperature": 0.0, "avg_logprob": -0.1560838222503662, "compression_ratio": 1.6150234741784038, "no_speech_prob": 4.005730806966312e-05}, {"id": 290, "seek": 128000, "start": 1284.56, "end": 1289.24, "text": " And I know from speaking to people in the data science world, about half the people", "tokens": [400, 286, 458, 490, 4124, 281, 561, 294, 264, 1412, 3497, 1002, 11, 466, 1922, 264, 561], "temperature": 0.0, "avg_logprob": -0.1560838222503662, "compression_ratio": 1.6150234741784038, "no_speech_prob": 4.005730806966312e-05}, {"id": 291, "seek": 128000, "start": 1289.24, "end": 1293.8, "text": " I speak to don't really know how to program.", "tokens": [286, 1710, 281, 500, 380, 534, 458, 577, 281, 1461, 13], "temperature": 0.0, "avg_logprob": -0.1560838222503662, "compression_ratio": 1.6150234741784038, "no_speech_prob": 4.005730806966312e-05}, {"id": 292, "seek": 128000, "start": 1293.8, "end": 1297.6, "text": " You definitely should because otherwise all you can do is use stuff that other people", "tokens": [509, 2138, 820, 570, 5911, 439, 291, 393, 360, 307, 764, 1507, 300, 661, 561], "temperature": 0.0, "avg_logprob": -0.1560838222503662, "compression_ratio": 1.6150234741784038, "no_speech_prob": 4.005730806966312e-05}, {"id": 293, "seek": 128000, "start": 1297.6, "end": 1298.84, "text": " have made for you.", "tokens": [362, 1027, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.1560838222503662, "compression_ratio": 1.6150234741784038, "no_speech_prob": 4.005730806966312e-05}, {"id": 294, "seek": 128000, "start": 1298.84, "end": 1302.12, "text": " And I would be picking from these tools.", "tokens": [400, 286, 576, 312, 8867, 490, 613, 3873, 13], "temperature": 0.0, "avg_logprob": -0.1560838222503662, "compression_ratio": 1.6150234741784038, "no_speech_prob": 4.005730806966312e-05}, {"id": 295, "seek": 130212, "start": 1302.12, "end": 1313.8, "text": " So I like the highly misunderstood C sharp and I would combine it with these particular", "tokens": [407, 286, 411, 264, 5405, 33870, 383, 8199, 293, 286, 576, 10432, 309, 365, 613, 1729], "temperature": 0.0, "avg_logprob": -0.32618554433186847, "compression_ratio": 1.4565217391304348, "no_speech_prob": 3.5907603887608275e-05}, {"id": 296, "seek": 130212, "start": 1313.8, "end": 1314.8, "text": " libraries for...", "tokens": [15148, 337, 485], "temperature": 0.0, "avg_logprob": -0.32618554433186847, "compression_ratio": 1.4565217391304348, "no_speech_prob": 3.5907603887608275e-05}, {"id": 297, "seek": 130212, "start": 1314.8, "end": 1315.8, "text": " Yes, question?", "tokens": [1079, 11, 1168, 30], "temperature": 0.0, "avg_logprob": -0.32618554433186847, "compression_ratio": 1.4565217391304348, "no_speech_prob": 3.5907603887608275e-05}, {"id": 298, "seek": 130212, "start": 1315.8, "end": 1323.8799999999999, "text": " Yeah, I was just wondering whether you saw R as complementary or competing?", "tokens": [865, 11, 286, 390, 445, 6359, 1968, 291, 1866, 497, 382, 40705, 420, 15439, 30], "temperature": 0.0, "avg_logprob": -0.32618554433186847, "compression_ratio": 1.4565217391304348, "no_speech_prob": 3.5907603887608275e-05}, {"id": 299, "seek": 130212, "start": 1323.8799999999999, "end": 1330.04, "text": " Yeah, complementary and I'll come to that in the very next bullet point.", "tokens": [865, 11, 40705, 293, 286, 603, 808, 281, 300, 294, 264, 588, 958, 11632, 935, 13], "temperature": 0.0, "avg_logprob": -0.32618554433186847, "compression_ratio": 1.4565217391304348, "no_speech_prob": 3.5907603887608275e-05}, {"id": 300, "seek": 133004, "start": 1330.04, "end": 1336.8799999999999, "text": " So this general purpose programming tool is for the stuff that R doesn't do that well.", "tokens": [407, 341, 2674, 4334, 9410, 2290, 307, 337, 264, 1507, 300, 497, 1177, 380, 360, 300, 731, 13], "temperature": 0.0, "avg_logprob": -0.2342161530429877, "compression_ratio": 1.6254681647940075, "no_speech_prob": 6.707695865770802e-05}, {"id": 301, "seek": 133004, "start": 1336.8799999999999, "end": 1342.76, "text": " And even the guy that wrote R, Ross Lotko, says he's not that fond nowadays of various", "tokens": [400, 754, 264, 2146, 300, 4114, 497, 11, 16140, 20131, 4093, 11, 1619, 415, 311, 406, 300, 9557, 13434, 295, 3683], "temperature": 0.0, "avg_logprob": -0.2342161530429877, "compression_ratio": 1.6254681647940075, "no_speech_prob": 6.707695865770802e-05}, {"id": 302, "seek": 133004, "start": 1342.76, "end": 1349.28, "text": " things of R as a kind of an underlying language.", "tokens": [721, 295, 497, 382, 257, 733, 295, 364, 14217, 2856, 13], "temperature": 0.0, "avg_logprob": -0.2342161530429877, "compression_ratio": 1.6254681647940075, "no_speech_prob": 6.707695865770802e-05}, {"id": 303, "seek": 133004, "start": 1349.28, "end": 1353.84, "text": " Whereas there are other languages which are just so powerful and so rich and so beautiful.", "tokens": [13813, 456, 366, 661, 8650, 597, 366, 445, 370, 4005, 293, 370, 4593, 293, 370, 2238, 13], "temperature": 0.0, "avg_logprob": -0.2342161530429877, "compression_ratio": 1.6254681647940075, "no_speech_prob": 6.707695865770802e-05}, {"id": 304, "seek": 133004, "start": 1353.84, "end": 1357.2, "text": " I should have actually included some of the functional languages in here too like Haskell", "tokens": [286, 820, 362, 767, 5556, 512, 295, 264, 11745, 8650, 294, 510, 886, 411, 8646, 43723], "temperature": 0.0, "avg_logprob": -0.2342161530429877, "compression_ratio": 1.6254681647940075, "no_speech_prob": 6.707695865770802e-05}, {"id": 305, "seek": 133004, "start": 1357.2, "end": 1359.28, "text": " would be another great choice.", "tokens": [576, 312, 1071, 869, 3922, 13], "temperature": 0.0, "avg_logprob": -0.2342161530429877, "compression_ratio": 1.6254681647940075, "no_speech_prob": 6.707695865770802e-05}, {"id": 306, "seek": 135928, "start": 1359.28, "end": 1365.24, "text": " But if you've got a good powerful language, a good powerful matrix library and a good", "tokens": [583, 498, 291, 600, 658, 257, 665, 4005, 2856, 11, 257, 665, 4005, 8141, 6405, 293, 257, 665], "temperature": 0.0, "avg_logprob": -0.2196963416205512, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.9769547836622223e-05}, {"id": 307, "seek": 135928, "start": 1365.24, "end": 1369.24, "text": " powerful machine learning toolkit, you're doing great.", "tokens": [4005, 3479, 2539, 40167, 11, 291, 434, 884, 869, 13], "temperature": 0.0, "avg_logprob": -0.2196963416205512, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.9769547836622223e-05}, {"id": 308, "seek": 135928, "start": 1369.24, "end": 1370.72, "text": " So Python is fantastic.", "tokens": [407, 15329, 307, 5456, 13], "temperature": 0.0, "avg_logprob": -0.2196963416205512, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.9769547836622223e-05}, {"id": 309, "seek": 135928, "start": 1370.72, "end": 1373.68, "text": " Python also has a really, really nice REPL.", "tokens": [15329, 611, 575, 257, 534, 11, 534, 1481, 31511, 43, 13], "temperature": 0.0, "avg_logprob": -0.2196963416205512, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.9769547836622223e-05}, {"id": 310, "seek": 135928, "start": 1373.68, "end": 1379.52, "text": " A REPL is like where you type in a line of code like an R and it immediately gives you", "tokens": [316, 31511, 43, 307, 411, 689, 291, 2010, 294, 257, 1622, 295, 3089, 411, 364, 497, 293, 309, 4258, 2709, 291], "temperature": 0.0, "avg_logprob": -0.2196963416205512, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.9769547836622223e-05}, {"id": 311, "seek": 135928, "start": 1379.52, "end": 1382.0, "text": " the results and you can keep looking through like that.", "tokens": [264, 3542, 293, 291, 393, 1066, 1237, 807, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.2196963416205512, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.9769547836622223e-05}, {"id": 312, "seek": 138200, "start": 1382.0, "end": 1390.36, "text": " You can use IPython, which is a really fantastic REPL for Python.", "tokens": [509, 393, 764, 8671, 88, 11943, 11, 597, 307, 257, 534, 5456, 31511, 43, 337, 15329, 13], "temperature": 0.0, "avg_logprob": -0.1485076395670573, "compression_ratio": 1.5193370165745856, "no_speech_prob": 7.182960871432442e-06}, {"id": 313, "seek": 138200, "start": 1390.36, "end": 1395.32, "text": " And in fact, the other really nice thing in Python is Matplotlib, which gives you a really", "tokens": [400, 294, 1186, 11, 264, 661, 534, 1481, 551, 294, 15329, 307, 6789, 564, 310, 38270, 11, 597, 2709, 291, 257, 534], "temperature": 0.0, "avg_logprob": -0.1485076395670573, "compression_ratio": 1.5193370165745856, "no_speech_prob": 7.182960871432442e-06}, {"id": 314, "seek": 138200, "start": 1395.32, "end": 1399.32, "text": " nice charting library.", "tokens": [1481, 6927, 278, 6405, 13], "temperature": 0.0, "avg_logprob": -0.1485076395670573, "compression_ratio": 1.5193370165745856, "no_speech_prob": 7.182960871432442e-06}, {"id": 315, "seek": 138200, "start": 1399.32, "end": 1408.72, "text": " Much less elegant, but just as effective for C sharp and just as free is the MS chart controls.", "tokens": [12313, 1570, 21117, 11, 457, 445, 382, 4942, 337, 383, 8199, 293, 445, 382, 1737, 307, 264, 7395, 6927, 9003, 13], "temperature": 0.0, "avg_logprob": -0.1485076395670573, "compression_ratio": 1.5193370165745856, "no_speech_prob": 7.182960871432442e-06}, {"id": 316, "seek": 140872, "start": 1408.72, "end": 1412.1200000000001, "text": " I've written a kind of a functional layer on top of those to make them easier to do", "tokens": [286, 600, 3720, 257, 733, 295, 257, 11745, 4583, 322, 1192, 295, 729, 281, 652, 552, 3571, 281, 360], "temperature": 0.0, "avg_logprob": -0.19555360443738043, "compression_ratio": 1.515267175572519, "no_speech_prob": 2.7966065317741595e-05}, {"id": 317, "seek": 140872, "start": 1412.1200000000001, "end": 1414.8, "text": " analysis with, but they're super fast and super powerful.", "tokens": [5215, 365, 11, 457, 436, 434, 1687, 2370, 293, 1687, 4005, 13], "temperature": 0.0, "avg_logprob": -0.19555360443738043, "compression_ratio": 1.515267175572519, "no_speech_prob": 2.7966065317741595e-05}, {"id": 318, "seek": 140872, "start": 1414.8, "end": 1418.0, "text": " So that only takes 10 minutes.", "tokens": [407, 300, 787, 2516, 1266, 2077, 13], "temperature": 0.0, "avg_logprob": -0.19555360443738043, "compression_ratio": 1.515267175572519, "no_speech_prob": 2.7966065317741595e-05}, {"id": 319, "seek": 140872, "start": 1418.0, "end": 1421.08, "text": " If you use C++, that also works great.", "tokens": [759, 291, 764, 383, 25472, 11, 300, 611, 1985, 869, 13], "temperature": 0.0, "avg_logprob": -0.19555360443738043, "compression_ratio": 1.515267175572519, "no_speech_prob": 2.7966065317741595e-05}, {"id": 320, "seek": 140872, "start": 1421.08, "end": 1425.44, "text": " There's a really brilliant thing, very, very underutilized called Eigen, which originally", "tokens": [821, 311, 257, 534, 10248, 551, 11, 588, 11, 588, 833, 20835, 1602, 1219, 30586, 11, 597, 7993], "temperature": 0.0, "avg_logprob": -0.19555360443738043, "compression_ratio": 1.515267175572519, "no_speech_prob": 2.7966065317741595e-05}, {"id": 321, "seek": 140872, "start": 1425.44, "end": 1432.76, "text": " came from the KDE project and just provides an amazingly powerful kind of vector and scientific", "tokens": [1361, 490, 264, 591, 22296, 1716, 293, 445, 6417, 364, 31762, 4005, 733, 295, 8062, 293, 8134], "temperature": 0.0, "avg_logprob": -0.19555360443738043, "compression_ratio": 1.515267175572519, "no_speech_prob": 2.7966065317741595e-05}, {"id": 322, "seek": 143276, "start": 1432.76, "end": 1439.08, "text": " programming kind of language on top of C++.", "tokens": [9410, 733, 295, 2856, 322, 1192, 295, 383, 25472, 13], "temperature": 0.0, "avg_logprob": -0.14241976928710937, "compression_ratio": 1.5729537366548043, "no_speech_prob": 3.704885966726579e-05}, {"id": 323, "seek": 143276, "start": 1439.08, "end": 1445.28, "text": " Java to me is something that used to be on a par with C sharp back in the 1.0, 1.1.1.", "tokens": [10745, 281, 385, 307, 746, 300, 1143, 281, 312, 322, 257, 971, 365, 383, 8199, 646, 294, 264, 502, 13, 15, 11, 502, 13, 16, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.14241976928710937, "compression_ratio": 1.5729537366548043, "no_speech_prob": 3.704885966726579e-05}, {"id": 324, "seek": 143276, "start": 1445.28, "end": 1446.28, "text": " days.", "tokens": [1708, 13], "temperature": 0.0, "avg_logprob": -0.14241976928710937, "compression_ratio": 1.5729537366548043, "no_speech_prob": 3.704885966726579e-05}, {"id": 325, "seek": 143276, "start": 1446.28, "end": 1451.56, "text": " It's looking a bit sad nowadays, but on the other hand, it has just about the most powerful", "tokens": [467, 311, 1237, 257, 857, 4227, 13434, 11, 457, 322, 264, 661, 1011, 11, 309, 575, 445, 466, 264, 881, 4005], "temperature": 0.0, "avg_logprob": -0.14241976928710937, "compression_ratio": 1.5729537366548043, "no_speech_prob": 3.704885966726579e-05}, {"id": 326, "seek": 143276, "start": 1451.56, "end": 1455.48, "text": " general purpose machine learning library on top of it, which is weaker.", "tokens": [2674, 4334, 3479, 2539, 6405, 322, 1192, 295, 309, 11, 597, 307, 24286, 13], "temperature": 0.0, "avg_logprob": -0.14241976928710937, "compression_ratio": 1.5729537366548043, "no_speech_prob": 3.704885966726579e-05}, {"id": 327, "seek": 143276, "start": 1455.48, "end": 1459.36, "text": " So there's a lot to be said for using that combination.", "tokens": [407, 456, 311, 257, 688, 281, 312, 848, 337, 1228, 300, 6562, 13], "temperature": 0.0, "avg_logprob": -0.14241976928710937, "compression_ratio": 1.5729537366548043, "no_speech_prob": 3.704885966726579e-05}, {"id": 328, "seek": 143276, "start": 1459.36, "end": 1462.32, "text": " In the end, if you're a data scientist who doesn't yet know how to program, my message", "tokens": [682, 264, 917, 11, 498, 291, 434, 257, 1412, 12662, 567, 1177, 380, 1939, 458, 577, 281, 1461, 11, 452, 3636], "temperature": 0.0, "avg_logprob": -0.14241976928710937, "compression_ratio": 1.5729537366548043, "no_speech_prob": 3.704885966726579e-05}, {"id": 329, "seek": 146232, "start": 1462.32, "end": 1464.28, "text": " is learn to program.", "tokens": [307, 1466, 281, 1461, 13], "temperature": 0.4, "avg_logprob": -0.27974448665495844, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.00010888529504882172}, {"id": 330, "seek": 146232, "start": 1464.28, "end": 1466.8999999999999, "text": " And I don't think it matters too much which one you pick.", "tokens": [400, 286, 500, 380, 519, 309, 7001, 886, 709, 597, 472, 291, 1888, 13], "temperature": 0.4, "avg_logprob": -0.27974448665495844, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.00010888529504882172}, {"id": 331, "seek": 146232, "start": 1466.8999999999999, "end": 1472.6599999999999, "text": " I would be picking one of these, but without it, you're going to be struggling to go beyond", "tokens": [286, 576, 312, 8867, 472, 295, 613, 11, 457, 1553, 309, 11, 291, 434, 516, 281, 312, 9314, 281, 352, 4399], "temperature": 0.4, "avg_logprob": -0.27974448665495844, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.00010888529504882172}, {"id": 332, "seek": 146232, "start": 1472.6599999999999, "end": 1474.08, "text": " what the tools provide.", "tokens": [437, 264, 3873, 2893, 13], "temperature": 0.4, "avg_logprob": -0.27974448665495844, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.00010888529504882172}, {"id": 333, "seek": 146232, "start": 1474.08, "end": 1477.08, "text": " Question at the back.", "tokens": [14464, 412, 264, 646, 13], "temperature": 0.4, "avg_logprob": -0.27974448665495844, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.00010888529504882172}, {"id": 334, "seek": 147708, "start": 1477.08, "end": 1494.3999999999999, "text": " Yeah, okay, so the question was about visualization tools and equivalent to SAS jump.", "tokens": [865, 11, 1392, 11, 370, 264, 1168, 390, 466, 25801, 3873, 293, 10344, 281, 33441, 3012, 13], "temperature": 0.0, "avg_logprob": -0.36857704783594886, "compression_ratio": 1.2222222222222223, "no_speech_prob": 5.3056668548379093e-05}, {"id": 335, "seek": 147708, "start": 1494.3999999999999, "end": 1495.3999999999999, "text": " Fairly available.", "tokens": [12157, 356, 2435, 13], "temperature": 0.0, "avg_logprob": -0.36857704783594886, "compression_ratio": 1.2222222222222223, "no_speech_prob": 5.3056668548379093e-05}, {"id": 336, "seek": 147708, "start": 1495.3999999999999, "end": 1503.08, "text": " Yeah, I would have a look at something like Ggobi.", "tokens": [865, 11, 286, 576, 362, 257, 574, 412, 746, 411, 460, 70, 19293, 13], "temperature": 0.0, "avg_logprob": -0.36857704783594886, "compression_ratio": 1.2222222222222223, "no_speech_prob": 5.3056668548379093e-05}, {"id": 337, "seek": 150308, "start": 1503.08, "end": 1511.6799999999998, "text": " Ggobi is a fascinating tool which kind of has and not free, but in the same kind of", "tokens": [460, 70, 19293, 307, 257, 10343, 2290, 597, 733, 295, 575, 293, 406, 1737, 11, 457, 294, 264, 912, 733, 295], "temperature": 0.0, "avg_logprob": -0.1705655058224996, "compression_ratio": 1.7336244541484715, "no_speech_prob": 5.22386580996681e-05}, {"id": 338, "seek": 150308, "start": 1511.6799999999998, "end": 1515.52, "text": " area if we talked about Tableau.", "tokens": [1859, 498, 321, 2825, 466, 25535, 1459, 13], "temperature": 0.0, "avg_logprob": -0.1705655058224996, "compression_ratio": 1.7336244541484715, "no_speech_prob": 5.22386580996681e-05}, {"id": 339, "seek": 150308, "start": 1515.52, "end": 1521.6399999999999, "text": " It supports this concept of brushing, which is this idea that you can create a whole bunch", "tokens": [467, 9346, 341, 3410, 295, 33130, 11, 597, 307, 341, 1558, 300, 291, 393, 1884, 257, 1379, 3840], "temperature": 0.0, "avg_logprob": -0.1705655058224996, "compression_ratio": 1.7336244541484715, "no_speech_prob": 5.22386580996681e-05}, {"id": 340, "seek": 150308, "start": 1521.6399999999999, "end": 1527.56, "text": " of plots and scatter plots and parallel coordinate plots and all kinds of plots and you can highlight", "tokens": [295, 28609, 293, 34951, 28609, 293, 8952, 15670, 28609, 293, 439, 3685, 295, 28609, 293, 291, 393, 5078], "temperature": 0.0, "avg_logprob": -0.1705655058224996, "compression_ratio": 1.7336244541484715, "no_speech_prob": 5.22386580996681e-05}, {"id": 341, "seek": 150308, "start": 1527.56, "end": 1532.56, "text": " one area of one plot and it will show you where those points are in all the other plots", "tokens": [472, 1859, 295, 472, 7542, 293, 309, 486, 855, 291, 689, 729, 2793, 366, 294, 439, 264, 661, 28609], "temperature": 0.0, "avg_logprob": -0.1705655058224996, "compression_ratio": 1.7336244541484715, "no_speech_prob": 5.22386580996681e-05}, {"id": 342, "seek": 153256, "start": 1532.56, "end": 1537.48, "text": " and so in terms of kind of really powerful visualization libraries, I think Ggobi would", "tokens": [293, 370, 294, 2115, 295, 733, 295, 534, 4005, 25801, 15148, 11, 286, 519, 460, 70, 19293, 576], "temperature": 0.0, "avg_logprob": -0.13425493876139322, "compression_ratio": 1.4352331606217616, "no_speech_prob": 3.5353030398255214e-05}, {"id": 343, "seek": 153256, "start": 1537.48, "end": 1541.44, "text": " be where I would go.", "tokens": [312, 689, 286, 576, 352, 13], "temperature": 0.0, "avg_logprob": -0.13425493876139322, "compression_ratio": 1.4352331606217616, "no_speech_prob": 3.5353030398255214e-05}, {"id": 344, "seek": 153256, "start": 1541.44, "end": 1548.04, "text": " Having said that, it's amazing how little I use it in real life because things like", "tokens": [10222, 848, 300, 11, 309, 311, 2243, 577, 707, 286, 764, 309, 294, 957, 993, 570, 721, 411], "temperature": 0.0, "avg_logprob": -0.13425493876139322, "compression_ratio": 1.4352331606217616, "no_speech_prob": 3.5353030398255214e-05}, {"id": 345, "seek": 153256, "start": 1548.04, "end": 1555.6, "text": " Excel and what I'm about to come to, which is ggplot2, although much less fancy than", "tokens": [19060, 293, 437, 286, 478, 466, 281, 808, 281, 11, 597, 307, 290, 70, 564, 310, 17, 11, 4878, 709, 1570, 10247, 813], "temperature": 0.0, "avg_logprob": -0.13425493876139322, "compression_ratio": 1.4352331606217616, "no_speech_prob": 3.5353030398255214e-05}, {"id": 346, "seek": 155560, "start": 1555.6, "end": 1562.6, "text": " things like jump and Tableau and Ggobi, support a kind of hypothesis-driven problem solving", "tokens": [721, 411, 3012, 293, 25535, 1459, 293, 460, 70, 19293, 11, 1406, 257, 733, 295, 17291, 12, 25456, 1154, 12606], "temperature": 0.0, "avg_logprob": -0.16790149636464577, "compression_ratio": 1.5549738219895288, "no_speech_prob": 7.527624802605715e-06}, {"id": 347, "seek": 155560, "start": 1562.6, "end": 1566.52, "text": " approach very well.", "tokens": [3109, 588, 731, 13], "temperature": 0.0, "avg_logprob": -0.16790149636464577, "compression_ratio": 1.5549738219895288, "no_speech_prob": 7.527624802605715e-06}, {"id": 348, "seek": 155560, "start": 1566.52, "end": 1574.04, "text": " Something else that I do is I tend to try to create visualizations which meet my particular", "tokens": [6595, 1646, 300, 286, 360, 307, 286, 3928, 281, 853, 281, 1884, 5056, 14455, 597, 1677, 452, 1729], "temperature": 0.0, "avg_logprob": -0.16790149636464577, "compression_ratio": 1.5549738219895288, "no_speech_prob": 7.527624802605715e-06}, {"id": 349, "seek": 155560, "start": 1574.04, "end": 1575.04, "text": " needs.", "tokens": [2203, 13], "temperature": 0.0, "avg_logprob": -0.16790149636464577, "compression_ratio": 1.5549738219895288, "no_speech_prob": 7.527624802605715e-06}, {"id": 350, "seek": 155560, "start": 1575.04, "end": 1584.0, "text": " So we talked about the time series problem and the time series problem is one in which", "tokens": [407, 321, 2825, 466, 264, 565, 2638, 1154, 293, 264, 565, 2638, 1154, 307, 472, 294, 597], "temperature": 0.0, "avg_logprob": -0.16790149636464577, "compression_ratio": 1.5549738219895288, "no_speech_prob": 7.527624802605715e-06}, {"id": 351, "seek": 158400, "start": 1584.0, "end": 1592.52, "text": " I used a very simple ten line JavaScript piece of code to plot every single time series in", "tokens": [286, 1143, 257, 588, 2199, 2064, 1622, 15778, 2522, 295, 3089, 281, 7542, 633, 2167, 565, 2638, 294], "temperature": 0.0, "avg_logprob": -0.15167065726386175, "compression_ratio": 1.6457399103139014, "no_speech_prob": 1.983229049073998e-05}, {"id": 352, "seek": 158400, "start": 1592.52, "end": 1595.08, "text": " a huge big mess like this.", "tokens": [257, 2603, 955, 2082, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.15167065726386175, "compression_ratio": 1.6457399103139014, "no_speech_prob": 1.983229049073998e-05}, {"id": 353, "seek": 158400, "start": 1595.08, "end": 1600.04, "text": " Now you kind of might think, well if you're plotting hundreds and hundreds of time series,", "tokens": [823, 291, 733, 295, 1062, 519, 11, 731, 498, 291, 434, 41178, 6779, 293, 6779, 295, 565, 2638, 11], "temperature": 0.0, "avg_logprob": -0.15167065726386175, "compression_ratio": 1.6457399103139014, "no_speech_prob": 1.983229049073998e-05}, {"id": 354, "seek": 158400, "start": 1600.04, "end": 1602.28, "text": " how much insight are you really getting from that?", "tokens": [577, 709, 11269, 366, 291, 534, 1242, 490, 300, 30], "temperature": 0.0, "avg_logprob": -0.15167065726386175, "compression_ratio": 1.6457399103139014, "no_speech_prob": 1.983229049073998e-05}, {"id": 355, "seek": 158400, "start": 1602.28, "end": 1608.04, "text": " But I found it was amazing how just scrolling through hundreds of time series, how much", "tokens": [583, 286, 1352, 309, 390, 2243, 577, 445, 29053, 807, 6779, 295, 565, 2638, 11, 577, 709], "temperature": 0.0, "avg_logprob": -0.15167065726386175, "compression_ratio": 1.6457399103139014, "no_speech_prob": 1.983229049073998e-05}, {"id": 356, "seek": 158400, "start": 1608.04, "end": 1609.72, "text": " my brain picked up.", "tokens": [452, 3567, 6183, 493, 13], "temperature": 0.0, "avg_logprob": -0.15167065726386175, "compression_ratio": 1.6457399103139014, "no_speech_prob": 1.983229049073998e-05}, {"id": 357, "seek": 160972, "start": 1609.72, "end": 1618.52, "text": " And what I then did was when I started modeling this was I then turned these into something", "tokens": [400, 437, 286, 550, 630, 390, 562, 286, 1409, 15983, 341, 390, 286, 550, 3574, 613, 666, 746], "temperature": 0.0, "avg_logprob": -0.21866387705649099, "compression_ratio": 1.5419354838709678, "no_speech_prob": 4.683510633185506e-05}, {"id": 358, "seek": 160972, "start": 1618.52, "end": 1621.28, "text": " a bit better which was to basically repeat it.", "tokens": [257, 857, 1101, 597, 390, 281, 1936, 7149, 309, 13], "temperature": 0.0, "avg_logprob": -0.21866387705649099, "compression_ratio": 1.5419354838709678, "no_speech_prob": 4.683510633185506e-05}, {"id": 359, "seek": 160972, "start": 1621.28, "end": 1633.0, "text": " At this time I showed both the orange, which is the actuals, and the blues, which is my", "tokens": [1711, 341, 565, 286, 4712, 1293, 264, 7671, 11, 597, 307, 264, 3539, 82, 11, 293, 264, 24244, 11, 597, 307, 452], "temperature": 0.0, "avg_logprob": -0.21866387705649099, "compression_ratio": 1.5419354838709678, "no_speech_prob": 4.683510633185506e-05}, {"id": 360, "seek": 160972, "start": 1633.0, "end": 1635.2, "text": " predictions.", "tokens": [21264, 13], "temperature": 0.0, "avg_logprob": -0.21866387705649099, "compression_ratio": 1.5419354838709678, "no_speech_prob": 4.683510633185506e-05}, {"id": 361, "seek": 163520, "start": 1635.2, "end": 1640.88, "text": " And then I put the metric of how successful this particular time series was.", "tokens": [400, 550, 286, 829, 264, 20678, 295, 577, 4406, 341, 1729, 565, 2638, 390, 13], "temperature": 0.0, "avg_logprob": -0.1772113045056661, "compression_ratio": 1.6965811965811965, "no_speech_prob": 1.4970539268688299e-05}, {"id": 362, "seek": 163520, "start": 1640.88, "end": 1650.52, "text": " So I kind of found that using more focused kind of visualization development, in this", "tokens": [407, 286, 733, 295, 1352, 300, 1228, 544, 5178, 733, 295, 25801, 3250, 11, 294, 341], "temperature": 0.0, "avg_logprob": -0.1772113045056661, "compression_ratio": 1.6965811965811965, "no_speech_prob": 1.4970539268688299e-05}, {"id": 363, "seek": 163520, "start": 1650.52, "end": 1656.52, "text": " case I could immediately see whereabouts were these, which numbers were high.", "tokens": [1389, 286, 727, 4258, 536, 689, 41620, 645, 613, 11, 597, 3547, 645, 1090, 13], "temperature": 0.0, "avg_logprob": -0.1772113045056661, "compression_ratio": 1.6965811965811965, "no_speech_prob": 1.4970539268688299e-05}, {"id": 364, "seek": 163520, "start": 1656.52, "end": 1659.48, "text": " So here's one here, point one, that's a bit higher than the others.", "tokens": [407, 510, 311, 472, 510, 11, 935, 472, 11, 300, 311, 257, 857, 2946, 813, 264, 2357, 13], "temperature": 0.0, "avg_logprob": -0.1772113045056661, "compression_ratio": 1.6965811965811965, "no_speech_prob": 1.4970539268688299e-05}, {"id": 365, "seek": 163520, "start": 1659.48, "end": 1663.68, "text": " And I could immediately kind of see what have I done wrong and I could get a feel of how", "tokens": [400, 286, 727, 4258, 733, 295, 536, 437, 362, 286, 1096, 2085, 293, 286, 727, 483, 257, 841, 295, 577], "temperature": 0.0, "avg_logprob": -0.1772113045056661, "compression_ratio": 1.6965811965811965, "no_speech_prob": 1.4970539268688299e-05}, {"id": 366, "seek": 166368, "start": 1663.68, "end": 1665.76, "text": " my modeling was going straight.", "tokens": [452, 15983, 390, 516, 2997, 13], "temperature": 0.0, "avg_logprob": -0.1896290827279139, "compression_ratio": 1.6489795918367347, "no_speech_prob": 4.9854599637910724e-05}, {"id": 367, "seek": 166368, "start": 1665.76, "end": 1672.24, "text": " So I tend to think you don't necessarily need particularly sophisticated visualization tools,", "tokens": [407, 286, 3928, 281, 519, 291, 500, 380, 4725, 643, 4098, 16950, 25801, 3873, 11], "temperature": 0.0, "avg_logprob": -0.1896290827279139, "compression_ratio": 1.6489795918367347, "no_speech_prob": 4.9854599637910724e-05}, {"id": 368, "seek": 166368, "start": 1672.24, "end": 1676.96, "text": " they just need to be fairly flexible and you need to know how to drive them to give you", "tokens": [436, 445, 643, 281, 312, 6457, 11358, 293, 291, 643, 281, 458, 577, 281, 3332, 552, 281, 976, 291], "temperature": 0.0, "avg_logprob": -0.1896290827279139, "compression_ratio": 1.6489795918367347, "no_speech_prob": 4.9854599637910724e-05}, {"id": 369, "seek": 166368, "start": 1676.96, "end": 1679.44, "text": " what you need.", "tokens": [437, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.1896290827279139, "compression_ratio": 1.6489795918367347, "no_speech_prob": 4.9854599637910724e-05}, {"id": 370, "seek": 166368, "start": 1679.44, "end": 1685.04, "text": " So through this kind of visualization I was able to make sure every single chart in this", "tokens": [407, 807, 341, 733, 295, 25801, 286, 390, 1075, 281, 652, 988, 633, 2167, 6927, 294, 341], "temperature": 0.0, "avg_logprob": -0.1896290827279139, "compression_ratio": 1.6489795918367347, "no_speech_prob": 4.9854599637910724e-05}, {"id": 371, "seek": 166368, "start": 1685.04, "end": 1690.8400000000001, "text": " competition, if it wasn't matching well, and I'd look at it and I'd say, yeah it's not", "tokens": [6211, 11, 498, 309, 2067, 380, 14324, 731, 11, 293, 286, 1116, 574, 412, 309, 293, 286, 1116, 584, 11, 1338, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.1896290827279139, "compression_ratio": 1.6489795918367347, "no_speech_prob": 4.9854599637910724e-05}, {"id": 372, "seek": 169084, "start": 1690.84, "end": 1695.08, "text": " matching well because there was just a shock in some period which couldn't possibly be", "tokens": [14324, 731, 570, 456, 390, 445, 257, 5588, 294, 512, 2896, 597, 2809, 380, 6264, 312], "temperature": 0.0, "avg_logprob": -0.16774928716965665, "compression_ratio": 1.5707317073170732, "no_speech_prob": 5.4741678468417376e-05}, {"id": 373, "seek": 169084, "start": 1695.08, "end": 1698.36, "text": " predicted so that's okay.", "tokens": [19147, 370, 300, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.16774928716965665, "compression_ratio": 1.5707317073170732, "no_speech_prob": 5.4741678468417376e-05}, {"id": 374, "seek": 169084, "start": 1698.36, "end": 1702.8799999999999, "text": " And so this was one of the competitions that I won and I really think that this visualization", "tokens": [400, 370, 341, 390, 472, 295, 264, 26185, 300, 286, 1582, 293, 286, 534, 519, 300, 341, 25801], "temperature": 0.0, "avg_logprob": -0.16774928716965665, "compression_ratio": 1.5707317073170732, "no_speech_prob": 5.4741678468417376e-05}, {"id": 375, "seek": 169084, "start": 1702.8799999999999, "end": 1709.8, "text": " approach was key.", "tokens": [3109, 390, 2141, 13], "temperature": 0.0, "avg_logprob": -0.16774928716965665, "compression_ratio": 1.5707317073170732, "no_speech_prob": 5.4741678468417376e-05}, {"id": 376, "seek": 169084, "start": 1709.8, "end": 1713.76, "text": " So I mentioned I was going to come back to one really interesting plotting tool which", "tokens": [407, 286, 2835, 286, 390, 516, 281, 808, 646, 281, 472, 534, 1880, 41178, 2290, 597], "temperature": 0.0, "avg_logprob": -0.16774928716965665, "compression_ratio": 1.5707317073170732, "no_speech_prob": 5.4741678468417376e-05}, {"id": 377, "seek": 169084, "start": 1713.76, "end": 1714.76, "text": " is ggplot2.", "tokens": [307, 290, 70, 564, 310, 17, 13], "temperature": 0.0, "avg_logprob": -0.16774928716965665, "compression_ratio": 1.5707317073170732, "no_speech_prob": 5.4741678468417376e-05}, {"id": 378, "seek": 171476, "start": 1714.76, "end": 1723.52, "text": " ggplot2 is created by a particularly amazing New Zealander who seems to have more time", "tokens": [290, 70, 564, 310, 17, 307, 2942, 538, 257, 4098, 2243, 1873, 4853, 304, 4483, 567, 2544, 281, 362, 544, 565], "temperature": 0.0, "avg_logprob": -0.1510883130525288, "compression_ratio": 1.5971731448763251, "no_speech_prob": 1.696252729743719e-05}, {"id": 379, "seek": 171476, "start": 1723.52, "end": 1728.16, "text": " than everybody else in the world combined and creates all these fantastic tools.", "tokens": [813, 2201, 1646, 294, 264, 1002, 9354, 293, 7829, 439, 613, 5456, 3873, 13], "temperature": 0.0, "avg_logprob": -0.1510883130525288, "compression_ratio": 1.5971731448763251, "no_speech_prob": 1.696252729743719e-05}, {"id": 380, "seek": 171476, "start": 1728.16, "end": 1729.16, "text": " Thank you Hadley.", "tokens": [1044, 291, 12298, 3420, 13], "temperature": 0.0, "avg_logprob": -0.1510883130525288, "compression_ratio": 1.5971731448763251, "no_speech_prob": 1.696252729743719e-05}, {"id": 381, "seek": 171476, "start": 1729.16, "end": 1734.56, "text": " I was going to show you what I meant by a really powerful but kind of simple plotting", "tokens": [286, 390, 516, 281, 855, 291, 437, 286, 4140, 538, 257, 534, 4005, 457, 733, 295, 2199, 41178], "temperature": 0.0, "avg_logprob": -0.1510883130525288, "compression_ratio": 1.5971731448763251, "no_speech_prob": 1.696252729743719e-05}, {"id": 382, "seek": 171476, "start": 1734.56, "end": 1735.56, "text": " tool.", "tokens": [2290, 13], "temperature": 0.0, "avg_logprob": -0.1510883130525288, "compression_ratio": 1.5971731448763251, "no_speech_prob": 1.696252729743719e-05}, {"id": 383, "seek": 171476, "start": 1735.56, "end": 1737.36, "text": " Here's something really fascinating, right?", "tokens": [1692, 311, 746, 534, 10343, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1510883130525288, "compression_ratio": 1.5971731448763251, "no_speech_prob": 1.696252729743719e-05}, {"id": 384, "seek": 171476, "start": 1737.36, "end": 1741.8, "text": " You know how creating scatter plots with lots and lots of data is really hard because you", "tokens": [509, 458, 577, 4084, 34951, 28609, 365, 3195, 293, 3195, 295, 1412, 307, 534, 1152, 570, 291], "temperature": 0.0, "avg_logprob": -0.1510883130525288, "compression_ratio": 1.5971731448763251, "no_speech_prob": 1.696252729743719e-05}, {"id": 385, "seek": 171476, "start": 1741.8, "end": 1744.48, "text": " end up with just big black blobs, right?", "tokens": [917, 493, 365, 445, 955, 2211, 1749, 929, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1510883130525288, "compression_ratio": 1.5971731448763251, "no_speech_prob": 1.696252729743719e-05}, {"id": 386, "seek": 174448, "start": 1744.48, "end": 1749.88, "text": " So here's a really simple idea which is why don't you give each point in the data a kind", "tokens": [407, 510, 311, 257, 534, 2199, 1558, 597, 307, 983, 500, 380, 291, 976, 1184, 935, 294, 264, 1412, 257, 733], "temperature": 0.0, "avg_logprob": -0.11206846767001682, "compression_ratio": 1.6979591836734693, "no_speech_prob": 2.318551742064301e-05}, {"id": 387, "seek": 174448, "start": 1749.88, "end": 1755.8, "text": " of a level of transparency so that the more they sit on top of each other, it's like transparent", "tokens": [295, 257, 1496, 295, 17131, 370, 300, 264, 544, 436, 1394, 322, 1192, 295, 1184, 661, 11, 309, 311, 411, 12737], "temperature": 0.0, "avg_logprob": -0.11206846767001682, "compression_ratio": 1.6979591836734693, "no_speech_prob": 2.318551742064301e-05}, {"id": 388, "seek": 174448, "start": 1755.8, "end": 1758.94, "text": " disks stacking up and getting darker and darker.", "tokens": [41617, 41376, 493, 293, 1242, 12741, 293, 12741, 13], "temperature": 0.0, "avg_logprob": -0.11206846767001682, "compression_ratio": 1.6979591836734693, "no_speech_prob": 2.318551742064301e-05}, {"id": 389, "seek": 174448, "start": 1758.94, "end": 1765.08, "text": " So in the amazing art package called ggplot2, you can add, so here's something that says", "tokens": [407, 294, 264, 2243, 1523, 7372, 1219, 290, 70, 564, 310, 17, 11, 291, 393, 909, 11, 370, 510, 311, 746, 300, 1619], "temperature": 0.0, "avg_logprob": -0.11206846767001682, "compression_ratio": 1.6979591836734693, "no_speech_prob": 2.318551742064301e-05}, {"id": 390, "seek": 174448, "start": 1765.08, "end": 1771.88, "text": " plot the carats of a diamond against its price and I want you to vary, it's called the alpha", "tokens": [7542, 264, 1032, 1720, 295, 257, 16059, 1970, 1080, 3218, 293, 286, 528, 291, 281, 10559, 11, 309, 311, 1219, 264, 8961], "temperature": 0.0, "avg_logprob": -0.11206846767001682, "compression_ratio": 1.6979591836734693, "no_speech_prob": 2.318551742064301e-05}, {"id": 391, "seek": 177188, "start": 1771.88, "end": 1777.0, "text": " channel to the graphics dex amongst you, you know that means kind of the level of transparency", "tokens": [2269, 281, 264, 11837, 368, 87, 12918, 291, 11, 291, 458, 300, 1355, 733, 295, 264, 1496, 295, 17131], "temperature": 0.0, "avg_logprob": -0.21004952502851726, "compression_ratio": 1.7191011235955056, "no_speech_prob": 5.2248098654672503e-05}, {"id": 392, "seek": 177188, "start": 1777.0, "end": 1781.3200000000002, "text": " and I want you to basically set the alpha channel for each point to be 1 over 10 or", "tokens": [293, 286, 528, 291, 281, 1936, 992, 264, 8961, 2269, 337, 1184, 935, 281, 312, 502, 670, 1266, 420], "temperature": 0.0, "avg_logprob": -0.21004952502851726, "compression_ratio": 1.7191011235955056, "no_speech_prob": 5.2248098654672503e-05}, {"id": 393, "seek": 177188, "start": 1781.3200000000002, "end": 1788.2800000000002, "text": " 1 over 100, 1 over 200 and you end up with these plots which actually show you kind of", "tokens": [502, 670, 2319, 11, 502, 670, 2331, 293, 291, 917, 493, 365, 613, 28609, 597, 767, 855, 291, 733, 295], "temperature": 0.0, "avg_logprob": -0.21004952502851726, "compression_ratio": 1.7191011235955056, "no_speech_prob": 5.2248098654672503e-05}, {"id": 394, "seek": 177188, "start": 1788.2800000000002, "end": 1793.6000000000001, "text": " the heat, you know, the amount of that area and it's just so much better than any other", "tokens": [264, 3738, 11, 291, 458, 11, 264, 2372, 295, 300, 1859, 293, 309, 311, 445, 370, 709, 1101, 813, 604, 661], "temperature": 0.0, "avg_logprob": -0.21004952502851726, "compression_ratio": 1.7191011235955056, "no_speech_prob": 5.2248098654672503e-05}, {"id": 395, "seek": 177188, "start": 1793.6000000000001, "end": 1795.92, "text": " approach to scatter plots that I've ever seen.", "tokens": [3109, 281, 34951, 28609, 300, 286, 600, 1562, 1612, 13], "temperature": 0.0, "avg_logprob": -0.21004952502851726, "compression_ratio": 1.7191011235955056, "no_speech_prob": 5.2248098654672503e-05}, {"id": 396, "seek": 177188, "start": 1795.92, "end": 1799.5600000000002, "text": " So simple and just one little line of code in your ggplot.", "tokens": [407, 2199, 293, 445, 472, 707, 1622, 295, 3089, 294, 428, 290, 70, 564, 310, 13], "temperature": 0.0, "avg_logprob": -0.21004952502851726, "compression_ratio": 1.7191011235955056, "no_speech_prob": 5.2248098654672503e-05}, {"id": 397, "seek": 179956, "start": 1799.56, "end": 1803.96, "text": " I'll show you another couple of examples and this by the way is in a completely free chapter", "tokens": [286, 603, 855, 291, 1071, 1916, 295, 5110, 293, 341, 538, 264, 636, 307, 294, 257, 2584, 1737, 7187], "temperature": 0.0, "avg_logprob": -0.16566667324159204, "compression_ratio": 1.7818181818181817, "no_speech_prob": 5.390743172029033e-05}, {"id": 398, "seek": 179956, "start": 1803.96, "end": 1805.8799999999999, "text": " of the book that he's got up on his website.", "tokens": [295, 264, 1446, 300, 415, 311, 658, 493, 322, 702, 3144, 13], "temperature": 0.0, "avg_logprob": -0.16566667324159204, "compression_ratio": 1.7818181818181817, "no_speech_prob": 5.390743172029033e-05}, {"id": 399, "seek": 179956, "start": 1805.8799999999999, "end": 1809.9199999999998, "text": " There's a fantastic book, you should definitely buy it by the author of the package about", "tokens": [821, 311, 257, 5456, 1446, 11, 291, 820, 2138, 2256, 309, 538, 264, 3793, 295, 264, 7372, 466], "temperature": 0.0, "avg_logprob": -0.16566667324159204, "compression_ratio": 1.7818181818181817, "no_speech_prob": 5.390743172029033e-05}, {"id": 400, "seek": 179956, "start": 1809.9199999999998, "end": 1816.72, "text": " ggplot2 but this one and most important chapter is available free on his website so check", "tokens": [290, 70, 564, 310, 17, 457, 341, 472, 293, 881, 1021, 7187, 307, 2435, 1737, 322, 702, 3144, 370, 1520], "temperature": 0.0, "avg_logprob": -0.16566667324159204, "compression_ratio": 1.7818181818181817, "no_speech_prob": 5.390743172029033e-05}, {"id": 401, "seek": 179956, "start": 1816.72, "end": 1817.72, "text": " it out.", "tokens": [309, 484, 13], "temperature": 0.0, "avg_logprob": -0.16566667324159204, "compression_ratio": 1.7818181818181817, "no_speech_prob": 5.390743172029033e-05}, {"id": 402, "seek": 179956, "start": 1817.72, "end": 1820.32, "text": " I'll show you another couple of examples.", "tokens": [286, 603, 855, 291, 1071, 1916, 295, 5110, 13], "temperature": 0.0, "avg_logprob": -0.16566667324159204, "compression_ratio": 1.7818181818181817, "no_speech_prob": 5.390743172029033e-05}, {"id": 403, "seek": 179956, "start": 1820.32, "end": 1822.0, "text": " Everything's done just right.", "tokens": [5471, 311, 1096, 445, 558, 13], "temperature": 0.0, "avg_logprob": -0.16566667324159204, "compression_ratio": 1.7818181818181817, "no_speech_prob": 5.390743172029033e-05}, {"id": 404, "seek": 179956, "start": 1822.0, "end": 1827.84, "text": " Here's a simple approach of plotting a lowest smoother through a bunch of data, always handy", "tokens": [1692, 311, 257, 2199, 3109, 295, 41178, 257, 12437, 28640, 807, 257, 3840, 295, 1412, 11, 1009, 13239], "temperature": 0.0, "avg_logprob": -0.16566667324159204, "compression_ratio": 1.7818181818181817, "no_speech_prob": 5.390743172029033e-05}, {"id": 405, "seek": 182784, "start": 1827.84, "end": 1832.36, "text": " but every time you plot something you should see the confidence intervals, no problem.", "tokens": [457, 633, 565, 291, 7542, 746, 291, 820, 536, 264, 6687, 26651, 11, 572, 1154, 13], "temperature": 0.0, "avg_logprob": -0.17841031153996786, "compression_ratio": 1.7746478873239437, "no_speech_prob": 5.3904353990219533e-05}, {"id": 406, "seek": 182784, "start": 1832.36, "end": 1834.6399999999999, "text": " This does it by default.", "tokens": [639, 775, 309, 538, 7576, 13], "temperature": 0.0, "avg_logprob": -0.17841031153996786, "compression_ratio": 1.7746478873239437, "no_speech_prob": 5.3904353990219533e-05}, {"id": 407, "seek": 182784, "start": 1834.6399999999999, "end": 1842.72, "text": " The best kind of plot kind of thing you want to see normally is a lower smoother so if", "tokens": [440, 1151, 733, 295, 7542, 733, 295, 551, 291, 528, 281, 536, 5646, 307, 257, 3126, 28640, 370, 498], "temperature": 0.0, "avg_logprob": -0.17841031153996786, "compression_ratio": 1.7746478873239437, "no_speech_prob": 5.3904353990219533e-05}, {"id": 408, "seek": 182784, "start": 1842.72, "end": 1846.84, "text": " you ask for a fit it gives you the lowest move by default, it gives you the confidence", "tokens": [291, 1029, 337, 257, 3318, 309, 2709, 291, 264, 12437, 1286, 538, 7576, 11, 309, 2709, 291, 264, 6687], "temperature": 0.0, "avg_logprob": -0.17841031153996786, "compression_ratio": 1.7746478873239437, "no_speech_prob": 5.3904353990219533e-05}, {"id": 409, "seek": 182784, "start": 1846.84, "end": 1854.36, "text": " interval by default so it's kind of, it makes it hard to create really bad graphs in ggplot2", "tokens": [15035, 538, 7576, 370, 309, 311, 733, 295, 11, 309, 1669, 309, 1152, 281, 1884, 534, 1578, 24877, 294, 290, 70, 564, 310, 17], "temperature": 0.0, "avg_logprob": -0.17841031153996786, "compression_ratio": 1.7746478873239437, "no_speech_prob": 5.3904353990219533e-05}, {"id": 410, "seek": 185436, "start": 1854.36, "end": 1859.8799999999999, "text": " although some people have managed I've noticed.", "tokens": [4878, 512, 561, 362, 6453, 286, 600, 5694, 13], "temperature": 0.0, "avg_logprob": -0.19516713995682566, "compression_ratio": 1.6074380165289257, "no_speech_prob": 2.5069681214517914e-05}, {"id": 411, "seek": 185436, "start": 1859.8799999999999, "end": 1864.1999999999998, "text": " Things like box plots all stacked up next to each other, such an easy way of seeing", "tokens": [9514, 411, 2424, 28609, 439, 28867, 493, 958, 281, 1184, 661, 11, 1270, 364, 1858, 636, 295, 2577], "temperature": 0.0, "avg_logprob": -0.19516713995682566, "compression_ratio": 1.6074380165289257, "no_speech_prob": 2.5069681214517914e-05}, {"id": 412, "seek": 185436, "start": 1864.1999999999998, "end": 1867.4799999999998, "text": " in this case how the color of diamonds varies.", "tokens": [294, 341, 1389, 577, 264, 2017, 295, 22612, 21716, 13], "temperature": 0.0, "avg_logprob": -0.19516713995682566, "compression_ratio": 1.6074380165289257, "no_speech_prob": 2.5069681214517914e-05}, {"id": 413, "seek": 185436, "start": 1867.4799999999998, "end": 1872.8, "text": " They've all got roughly the same median but some of them have really long tails in their", "tokens": [814, 600, 439, 658, 9810, 264, 912, 26779, 457, 512, 295, 552, 362, 534, 938, 28537, 294, 641], "temperature": 0.0, "avg_logprob": -0.19516713995682566, "compression_ratio": 1.6074380165289257, "no_speech_prob": 2.5069681214517914e-05}, {"id": 414, "seek": 185436, "start": 1872.8, "end": 1873.8, "text": " prices.", "tokens": [7901, 13], "temperature": 0.0, "avg_logprob": -0.19516713995682566, "compression_ratio": 1.6074380165289257, "no_speech_prob": 2.5069681214517914e-05}, {"id": 415, "seek": 185436, "start": 1873.8, "end": 1879.6399999999999, "text": " What a really powerful plotting device and so impressive that in this chapter of the", "tokens": [708, 257, 534, 4005, 41178, 4302, 293, 370, 8992, 300, 294, 341, 7187, 295, 264], "temperature": 0.0, "avg_logprob": -0.19516713995682566, "compression_ratio": 1.6074380165289257, "no_speech_prob": 2.5069681214517914e-05}, {"id": 416, "seek": 185436, "start": 1879.6399999999999, "end": 1881.4799999999998, "text": " book he shows a few options.", "tokens": [1446, 415, 3110, 257, 1326, 3956, 13], "temperature": 0.0, "avg_logprob": -0.19516713995682566, "compression_ratio": 1.6074380165289257, "no_speech_prob": 2.5069681214517914e-05}, {"id": 417, "seek": 188148, "start": 1881.48, "end": 1887.04, "text": " Here's what would happen if you used a jitter approach and he's got another one down here", "tokens": [1692, 311, 437, 576, 1051, 498, 291, 1143, 257, 361, 3904, 3109, 293, 415, 311, 658, 1071, 472, 760, 510], "temperature": 0.0, "avg_logprob": -0.18720891045742347, "compression_ratio": 1.8401486988847584, "no_speech_prob": 3.426667899475433e-05}, {"id": 418, "seek": 188148, "start": 1887.04, "end": 1891.4, "text": " which is here's what would happen if you use that alpha transparency approach and you can", "tokens": [597, 307, 510, 311, 437, 576, 1051, 498, 291, 764, 300, 8961, 17131, 3109, 293, 291, 393], "temperature": 0.0, "avg_logprob": -0.18720891045742347, "compression_ratio": 1.8401486988847584, "no_speech_prob": 3.426667899475433e-05}, {"id": 419, "seek": 188148, "start": 1891.4, "end": 1895.3600000000001, "text": " really compare the different approaches.", "tokens": [534, 6794, 264, 819, 11587, 13], "temperature": 0.0, "avg_logprob": -0.18720891045742347, "compression_ratio": 1.8401486988847584, "no_speech_prob": 3.426667899475433e-05}, {"id": 420, "seek": 188148, "start": 1895.3600000000001, "end": 1899.4, "text": " So ggplot2 is something which, and I'll scroll through these so you can see what kind of", "tokens": [407, 290, 70, 564, 310, 17, 307, 746, 597, 11, 293, 286, 603, 11369, 807, 613, 370, 291, 393, 536, 437, 733, 295], "temperature": 0.0, "avg_logprob": -0.18720891045742347, "compression_ratio": 1.8401486988847584, "no_speech_prob": 3.426667899475433e-05}, {"id": 421, "seek": 188148, "start": 1899.4, "end": 1904.3600000000001, "text": " stuff you can do, is a really important part of the toolbox.", "tokens": [1507, 291, 393, 360, 11, 307, 257, 534, 1021, 644, 295, 264, 44593, 13], "temperature": 0.0, "avg_logprob": -0.18720891045742347, "compression_ratio": 1.8401486988847584, "no_speech_prob": 3.426667899475433e-05}, {"id": 422, "seek": 188148, "start": 1904.3600000000001, "end": 1906.0, "text": " Here's another one I love, right?", "tokens": [1692, 311, 1071, 472, 286, 959, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18720891045742347, "compression_ratio": 1.8401486988847584, "no_speech_prob": 3.426667899475433e-05}, {"id": 423, "seek": 188148, "start": 1906.0, "end": 1911.44, "text": " Okay so we do lots of scatter plots and scatter plots are really powerful and sometimes we", "tokens": [1033, 370, 321, 360, 3195, 295, 34951, 28609, 293, 34951, 28609, 366, 534, 4005, 293, 2171, 321], "temperature": 0.0, "avg_logprob": -0.18720891045742347, "compression_ratio": 1.8401486988847584, "no_speech_prob": 3.426667899475433e-05}, {"id": 424, "seek": 191144, "start": 1911.44, "end": 1915.8, "text": " actually want to see how if the points are kind of ordered chronologically how did they", "tokens": [767, 528, 281, 536, 577, 498, 264, 2793, 366, 733, 295, 8866, 19393, 17157, 577, 630, 436], "temperature": 0.0, "avg_logprob": -0.1452251540289985, "compression_ratio": 1.7208333333333334, "no_speech_prob": 8.092242205748335e-05}, {"id": 425, "seek": 191144, "start": 1915.8, "end": 1917.24, "text": " change over time.", "tokens": [1319, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.1452251540289985, "compression_ratio": 1.7208333333333334, "no_speech_prob": 8.092242205748335e-05}, {"id": 426, "seek": 191144, "start": 1917.24, "end": 1920.52, "text": " So one way to do that is to connect them up with a line.", "tokens": [407, 472, 636, 281, 360, 300, 307, 281, 1745, 552, 493, 365, 257, 1622, 13], "temperature": 0.0, "avg_logprob": -0.1452251540289985, "compression_ratio": 1.7208333333333334, "no_speech_prob": 8.092242205748335e-05}, {"id": 427, "seek": 191144, "start": 1920.52, "end": 1921.8400000000001, "text": " Pretty bloody hard to read.", "tokens": [10693, 18938, 1152, 281, 1401, 13], "temperature": 0.0, "avg_logprob": -0.1452251540289985, "compression_ratio": 1.7208333333333334, "no_speech_prob": 8.092242205748335e-05}, {"id": 428, "seek": 191144, "start": 1921.8400000000001, "end": 1926.4, "text": " So if you take this exact thing but just add this simple thing.", "tokens": [407, 498, 291, 747, 341, 1900, 551, 457, 445, 909, 341, 2199, 551, 13], "temperature": 0.0, "avg_logprob": -0.1452251540289985, "compression_ratio": 1.7208333333333334, "no_speech_prob": 8.092242205748335e-05}, {"id": 429, "seek": 191144, "start": 1926.4, "end": 1931.24, "text": " Set the color to be related to the year of the date and then bang.", "tokens": [8928, 264, 2017, 281, 312, 4077, 281, 264, 1064, 295, 264, 4002, 293, 550, 8550, 13], "temperature": 0.0, "avg_logprob": -0.1452251540289985, "compression_ratio": 1.7208333333333334, "no_speech_prob": 8.092242205748335e-05}, {"id": 430, "seek": 191144, "start": 1931.24, "end": 1938.0800000000002, "text": " Now you can see by following the color exactly how this is ordered and so you can see we've", "tokens": [823, 291, 393, 536, 538, 3480, 264, 2017, 2293, 577, 341, 307, 8866, 293, 370, 291, 393, 536, 321, 600], "temperature": 0.0, "avg_logprob": -0.1452251540289985, "compression_ratio": 1.7208333333333334, "no_speech_prob": 8.092242205748335e-05}, {"id": 431, "seek": 193808, "start": 1938.08, "end": 1941.6399999999999, "text": " got his one end here and his one end here.", "tokens": [658, 702, 472, 917, 510, 293, 702, 472, 917, 510, 13], "temperature": 0.0, "avg_logprob": -0.1680591901143392, "compression_ratio": 1.565934065934066, "no_speech_prob": 2.9772545531159267e-05}, {"id": 432, "seek": 193808, "start": 1941.6399999999999, "end": 1952.04, "text": " So ggplot2 again has done fantastic things to make us understand this data more easily.", "tokens": [407, 290, 70, 564, 310, 17, 797, 575, 1096, 5456, 721, 281, 652, 505, 1223, 341, 1412, 544, 3612, 13], "temperature": 0.0, "avg_logprob": -0.1680591901143392, "compression_ratio": 1.565934065934066, "no_speech_prob": 2.9772545531159267e-05}, {"id": 433, "seek": 193808, "start": 1952.04, "end": 1954.8, "text": " One other thing I will mention is carrot.", "tokens": [1485, 661, 551, 286, 486, 2152, 307, 22767, 13], "temperature": 0.0, "avg_logprob": -0.1680591901143392, "compression_ratio": 1.565934065934066, "no_speech_prob": 2.9772545531159267e-05}, {"id": 434, "seek": 193808, "start": 1954.8, "end": 1958.78, "text": " How many people here have used the carrot package?", "tokens": [1012, 867, 561, 510, 362, 1143, 264, 22767, 7372, 30], "temperature": 0.0, "avg_logprob": -0.1680591901143392, "compression_ratio": 1.565934065934066, "no_speech_prob": 2.9772545531159267e-05}, {"id": 435, "seek": 193808, "start": 1958.78, "end": 1962.6399999999999, "text": " So I'm not going to show you carrot but I will tell you this.", "tokens": [407, 286, 478, 406, 516, 281, 855, 291, 22767, 457, 286, 486, 980, 291, 341, 13], "temperature": 0.0, "avg_logprob": -0.1680591901143392, "compression_ratio": 1.565934065934066, "no_speech_prob": 2.9772545531159267e-05}, {"id": 436, "seek": 196264, "start": 1962.64, "end": 1975.44, "text": " If you go into R and you type some model equals train on my data, create an svn, that's what", "tokens": [759, 291, 352, 666, 497, 293, 291, 2010, 512, 2316, 6915, 3847, 322, 452, 1412, 11, 1884, 364, 17342, 77, 11, 300, 311, 437], "temperature": 0.0, "avg_logprob": -0.28828887939453124, "compression_ratio": 1.5873015873015872, "no_speech_prob": 6.204852252267301e-05}, {"id": 437, "seek": 196264, "start": 1975.44, "end": 1978.2800000000002, "text": " a command carrot looks like.", "tokens": [257, 5622, 22767, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.28828887939453124, "compression_ratio": 1.5873015873015872, "no_speech_prob": 6.204852252267301e-05}, {"id": 438, "seek": 196264, "start": 1978.2800000000002, "end": 1984.8400000000001, "text": " It's got a command called train and you can pass in a string which is any of 300 different,", "tokens": [467, 311, 658, 257, 5622, 1219, 3847, 293, 291, 393, 1320, 294, 257, 6798, 597, 307, 604, 295, 6641, 819, 11], "temperature": 0.0, "avg_logprob": -0.28828887939453124, "compression_ratio": 1.5873015873015872, "no_speech_prob": 6.204852252267301e-05}, {"id": 439, "seek": 196264, "start": 1984.8400000000001, "end": 1991.72, "text": " I think it's about 300 different possible models, classification and regression models", "tokens": [286, 519, 309, 311, 466, 6641, 819, 1944, 5245, 11, 21538, 293, 24590, 5245], "temperature": 0.0, "avg_logprob": -0.28828887939453124, "compression_ratio": 1.5873015873015872, "no_speech_prob": 6.204852252267301e-05}, {"id": 440, "seek": 199172, "start": 1991.72, "end": 1995.16, "text": " and then you can add various things in here about saying I want you to center the data", "tokens": [293, 550, 291, 393, 909, 3683, 721, 294, 510, 466, 1566, 286, 528, 291, 281, 3056, 264, 1412], "temperature": 0.0, "avg_logprob": -0.21974002089455863, "compression_ratio": 1.794238683127572, "no_speech_prob": 5.306811726768501e-05}, {"id": 441, "seek": 199172, "start": 1995.16, "end": 2003.8, "text": " first please and I'll do a PCA on it first please and it just, you know, it kind of puts", "tokens": [700, 1767, 293, 286, 603, 360, 257, 6465, 32, 322, 309, 700, 1767, 293, 309, 445, 11, 291, 458, 11, 309, 733, 295, 8137], "temperature": 0.0, "avg_logprob": -0.21974002089455863, "compression_ratio": 1.794238683127572, "no_speech_prob": 5.306811726768501e-05}, {"id": 442, "seek": 199172, "start": 2003.8, "end": 2005.72, "text": " all of the pieces together.", "tokens": [439, 295, 264, 3755, 1214, 13], "temperature": 0.0, "avg_logprob": -0.21974002089455863, "compression_ratio": 1.794238683127572, "no_speech_prob": 5.306811726768501e-05}, {"id": 443, "seek": 199172, "start": 2005.72, "end": 2012.52, "text": " It can do things like remove columns from the data which hardly vary at all and therefore", "tokens": [467, 393, 360, 721, 411, 4159, 13766, 490, 264, 1412, 597, 13572, 10559, 412, 439, 293, 4412], "temperature": 0.0, "avg_logprob": -0.21974002089455863, "compression_ratio": 1.794238683127572, "no_speech_prob": 5.306811726768501e-05}, {"id": 444, "seek": 199172, "start": 2012.52, "end": 2013.52, "text": " use some modeling.", "tokens": [764, 512, 15983, 13], "temperature": 0.0, "avg_logprob": -0.21974002089455863, "compression_ratio": 1.794238683127572, "no_speech_prob": 5.306811726768501e-05}, {"id": 445, "seek": 199172, "start": 2013.52, "end": 2014.52, "text": " You can do that automatically.", "tokens": [509, 393, 360, 300, 6772, 13], "temperature": 0.0, "avg_logprob": -0.21974002089455863, "compression_ratio": 1.794238683127572, "no_speech_prob": 5.306811726768501e-05}, {"id": 446, "seek": 199172, "start": 2014.52, "end": 2019.96, "text": " It can automatically remove columns from the data that are hardly linear but most powerfully", "tokens": [467, 393, 6772, 4159, 13766, 490, 264, 1412, 300, 366, 13572, 8213, 457, 881, 1347, 2277], "temperature": 0.0, "avg_logprob": -0.21974002089455863, "compression_ratio": 1.794238683127572, "no_speech_prob": 5.306811726768501e-05}, {"id": 447, "seek": 201996, "start": 2019.96, "end": 2023.8, "text": " it's got this wrapper that basically lets you take any of hundreds and hundreds of R's", "tokens": [309, 311, 658, 341, 46906, 300, 1936, 6653, 291, 747, 604, 295, 6779, 293, 6779, 295, 497, 311], "temperature": 0.0, "avg_logprob": -0.178394285022703, "compression_ratio": 1.7282608695652173, "no_speech_prob": 4.006054223282263e-05}, {"id": 448, "seek": 201996, "start": 2023.8, "end": 2029.92, "text": " most powerful algorithms, really hard to use and they all now can be done through one command", "tokens": [881, 4005, 14642, 11, 534, 1152, 281, 764, 293, 436, 439, 586, 393, 312, 1096, 807, 472, 5622], "temperature": 0.0, "avg_logprob": -0.178394285022703, "compression_ratio": 1.7282608695652173, "no_speech_prob": 4.006054223282263e-05}, {"id": 449, "seek": 201996, "start": 2029.92, "end": 2031.92, "text": " and here's the cool bit, right?", "tokens": [293, 510, 311, 264, 1627, 857, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.178394285022703, "compression_ratio": 1.7282608695652173, "no_speech_prob": 4.006054223282263e-05}, {"id": 450, "seek": 201996, "start": 2031.92, "end": 2032.92, "text": " Imagine we're doing an svn.", "tokens": [11739, 321, 434, 884, 364, 17342, 77, 13], "temperature": 0.0, "avg_logprob": -0.178394285022703, "compression_ratio": 1.7282608695652173, "no_speech_prob": 4.006054223282263e-05}, {"id": 451, "seek": 201996, "start": 2032.92, "end": 2037.0, "text": " I don't know how many of you have tried to do svns but they're really hard to get a good", "tokens": [286, 500, 380, 458, 577, 867, 295, 291, 362, 3031, 281, 360, 17342, 3695, 457, 436, 434, 534, 1152, 281, 483, 257, 665], "temperature": 0.0, "avg_logprob": -0.178394285022703, "compression_ratio": 1.7282608695652173, "no_speech_prob": 4.006054223282263e-05}, {"id": 452, "seek": 201996, "start": 2037.0, "end": 2040.92, "text": " result because they depend so much on the parameters.", "tokens": [1874, 570, 436, 5672, 370, 709, 322, 264, 9834, 13], "temperature": 0.0, "avg_logprob": -0.178394285022703, "compression_ratio": 1.7282608695652173, "no_speech_prob": 4.006054223282263e-05}, {"id": 453, "seek": 201996, "start": 2040.92, "end": 2046.0, "text": " In this version it automatically does a grid search to automatically find the best parameters", "tokens": [682, 341, 3037, 309, 6772, 775, 257, 10748, 3164, 281, 6772, 915, 264, 1151, 9834], "temperature": 0.0, "avg_logprob": -0.178394285022703, "compression_ratio": 1.7282608695652173, "no_speech_prob": 4.006054223282263e-05}, {"id": 454, "seek": 204600, "start": 2046.0, "end": 2050.32, "text": " so you just create one command and it does svn for you.", "tokens": [370, 291, 445, 1884, 472, 5622, 293, 309, 775, 17342, 77, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.16700127758557284, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.1300568985461723e-05}, {"id": 455, "seek": 204600, "start": 2050.32, "end": 2056.72, "text": " So you definitely should be using a character.", "tokens": [407, 291, 2138, 820, 312, 1228, 257, 2517, 13], "temperature": 0.0, "avg_logprob": -0.16700127758557284, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.1300568985461723e-05}, {"id": 456, "seek": 204600, "start": 2056.72, "end": 2061.28, "text": " There's one more thing in the toolbox I wanted to mention which is you need to use some kind", "tokens": [821, 311, 472, 544, 551, 294, 264, 44593, 286, 1415, 281, 2152, 597, 307, 291, 643, 281, 764, 512, 733], "temperature": 0.0, "avg_logprob": -0.16700127758557284, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.1300568985461723e-05}, {"id": 457, "seek": 204600, "start": 2061.28, "end": 2064.16, "text": " of version control tool.", "tokens": [295, 3037, 1969, 2290, 13], "temperature": 0.0, "avg_logprob": -0.16700127758557284, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.1300568985461723e-05}, {"id": 458, "seek": 204600, "start": 2064.16, "end": 2069.96, "text": " How many people here have used a version control tool like git, cbs, svn?", "tokens": [1012, 867, 561, 510, 362, 1143, 257, 3037, 1969, 2290, 411, 18331, 11, 269, 929, 11, 17342, 77, 30], "temperature": 0.0, "avg_logprob": -0.16700127758557284, "compression_ratio": 1.5392670157068062, "no_speech_prob": 1.1300568985461723e-05}, {"id": 459, "seek": 206996, "start": 2069.96, "end": 2080.44, "text": " So let me give you an example from our terrific designer at Kaggle.", "tokens": [407, 718, 385, 976, 291, 364, 1365, 490, 527, 20899, 11795, 412, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.18309015909830728, "compression_ratio": 1.465, "no_speech_prob": 1.3211739315011073e-05}, {"id": 460, "seek": 206996, "start": 2080.44, "end": 2084.28, "text": " He's recently been changing some of the HTML on our site and he checked into this version", "tokens": [634, 311, 3938, 668, 4473, 512, 295, 264, 17995, 322, 527, 3621, 293, 415, 10033, 666, 341, 3037], "temperature": 0.0, "avg_logprob": -0.18309015909830728, "compression_ratio": 1.465, "no_speech_prob": 1.3211739315011073e-05}, {"id": 461, "seek": 206996, "start": 2084.28, "end": 2087.88, "text": " control tool that we use and it's so nice, right?", "tokens": [1969, 2290, 300, 321, 764, 293, 309, 311, 370, 1481, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18309015909830728, "compression_ratio": 1.465, "no_speech_prob": 1.3211739315011073e-05}, {"id": 462, "seek": 206996, "start": 2087.88, "end": 2094.64, "text": " Because I can go back to any file now and I can see exactly what was changed and when", "tokens": [1436, 286, 393, 352, 646, 281, 604, 3991, 586, 293, 286, 393, 536, 2293, 437, 390, 3105, 293, 562], "temperature": 0.0, "avg_logprob": -0.18309015909830728, "compression_ratio": 1.465, "no_speech_prob": 1.3211739315011073e-05}, {"id": 463, "seek": 209464, "start": 2094.64, "end": 2100.24, "text": " I can go through and I can say, okay, I remember that thing broke at about this time.", "tokens": [286, 393, 352, 807, 293, 286, 393, 584, 11, 1392, 11, 286, 1604, 300, 551, 6902, 412, 466, 341, 565, 13], "temperature": 0.0, "avg_logprob": -0.17438736863023652, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.1300568985461723e-05}, {"id": 464, "seek": 209464, "start": 2100.24, "end": 2101.24, "text": " What changed?", "tokens": [708, 3105, 30], "temperature": 0.0, "avg_logprob": -0.17438736863023652, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.1300568985461723e-05}, {"id": 465, "seek": 209464, "start": 2101.24, "end": 2104.8799999999997, "text": " Oh, I think it was this file here.", "tokens": [876, 11, 286, 519, 309, 390, 341, 3991, 510, 13], "temperature": 0.0, "avg_logprob": -0.17438736863023652, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.1300568985461723e-05}, {"id": 466, "seek": 209464, "start": 2104.8799999999997, "end": 2107.6, "text": " Okay, that line was deleted.", "tokens": [1033, 11, 300, 1622, 390, 22981, 13], "temperature": 0.0, "avg_logprob": -0.17438736863023652, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.1300568985461723e-05}, {"id": 467, "seek": 209464, "start": 2107.6, "end": 2109.48, "text": " This line was changed.", "tokens": [639, 1622, 390, 3105, 13], "temperature": 0.0, "avg_logprob": -0.17438736863023652, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.1300568985461723e-05}, {"id": 468, "seek": 209464, "start": 2109.48, "end": 2111.16, "text": " This section of this line was changed.", "tokens": [639, 3541, 295, 341, 1622, 390, 3105, 13], "temperature": 0.0, "avg_logprob": -0.17438736863023652, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.1300568985461723e-05}, {"id": 469, "seek": 209464, "start": 2111.16, "end": 2112.16, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.17438736863023652, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.1300568985461723e-05}, {"id": 470, "seek": 209464, "start": 2112.16, "end": 2116.52, "text": " And you can see with my version control tool it's keeping track of everything I can do.", "tokens": [400, 291, 393, 536, 365, 452, 3037, 1969, 2290, 309, 311, 5145, 2837, 295, 1203, 286, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.17438736863023652, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.1300568985461723e-05}, {"id": 471, "seek": 209464, "start": 2116.52, "end": 2119.56, "text": " Can you see how powerful this is for modeling?", "tokens": [1664, 291, 536, 577, 4005, 341, 307, 337, 15983, 30], "temperature": 0.0, "avg_logprob": -0.17438736863023652, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.1300568985461723e-05}, {"id": 472, "seek": 209464, "start": 2119.56, "end": 2124.12, "text": " Because you go back through your submission history at Kaggle and you say, oh, shit, I", "tokens": [1436, 291, 352, 646, 807, 428, 23689, 2503, 412, 48751, 22631, 293, 291, 584, 11, 1954, 11, 4611, 11, 286], "temperature": 0.0, "avg_logprob": -0.17438736863023652, "compression_ratio": 1.7224334600760456, "no_speech_prob": 1.1300568985461723e-05}, {"id": 473, "seek": 212412, "start": 2124.12, "end": 2126.24, "text": " used to be getting 0.97 AUC.", "tokens": [1143, 281, 312, 1242, 1958, 13, 23247, 7171, 34, 13], "temperature": 0.0, "avg_logprob": -0.15392316373667322, "compression_ratio": 1.7626459143968871, "no_speech_prob": 1.644157964619808e-05}, {"id": 474, "seek": 212412, "start": 2126.24, "end": 2127.24, "text": " Now I'm getting 0.93.", "tokens": [823, 286, 478, 1242, 1958, 13, 26372, 13], "temperature": 0.0, "avg_logprob": -0.15392316373667322, "compression_ratio": 1.7626459143968871, "no_speech_prob": 1.644157964619808e-05}, {"id": 475, "seek": 212412, "start": 2127.24, "end": 2130.0, "text": " I'm sure I'm doing everything the same.", "tokens": [286, 478, 988, 286, 478, 884, 1203, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.15392316373667322, "compression_ratio": 1.7626459143968871, "no_speech_prob": 1.644157964619808e-05}, {"id": 476, "seek": 212412, "start": 2130.0, "end": 2138.2, "text": " Go back into your version control tool and have a look at the history, so the commits", "tokens": [1037, 646, 666, 428, 3037, 1969, 2290, 293, 362, 257, 574, 412, 264, 2503, 11, 370, 264, 48311], "temperature": 0.0, "avg_logprob": -0.15392316373667322, "compression_ratio": 1.7626459143968871, "no_speech_prob": 1.644157964619808e-05}, {"id": 477, "seek": 212412, "start": 2138.2, "end": 2143.4, "text": " list and you can go back to the date where Kaggle shows you that you had a really shit", "tokens": [1329, 293, 291, 393, 352, 646, 281, 264, 4002, 689, 48751, 22631, 3110, 291, 300, 291, 632, 257, 534, 4611], "temperature": 0.0, "avg_logprob": -0.15392316373667322, "compression_ratio": 1.7626459143968871, "no_speech_prob": 1.644157964619808e-05}, {"id": 478, "seek": 212412, "start": 2143.4, "end": 2146.7999999999997, "text": " hot result and you can't now remember how the hell you did it.", "tokens": [2368, 1874, 293, 291, 393, 380, 586, 1604, 577, 264, 4921, 291, 630, 309, 13], "temperature": 0.0, "avg_logprob": -0.15392316373667322, "compression_ratio": 1.7626459143968871, "no_speech_prob": 1.644157964619808e-05}, {"id": 479, "seek": 212412, "start": 2146.7999999999997, "end": 2150.92, "text": " And you go back to that date and you go, oh yeah, it's this one here.", "tokens": [400, 291, 352, 646, 281, 300, 4002, 293, 291, 352, 11, 1954, 1338, 11, 309, 311, 341, 472, 510, 13], "temperature": 0.0, "avg_logprob": -0.15392316373667322, "compression_ratio": 1.7626459143968871, "no_speech_prob": 1.644157964619808e-05}, {"id": 480, "seek": 212412, "start": 2150.92, "end": 2153.88, "text": " And you go and you have a look and you see what changed.", "tokens": [400, 291, 352, 293, 291, 362, 257, 574, 293, 291, 536, 437, 3105, 13], "temperature": 0.0, "avg_logprob": -0.15392316373667322, "compression_ratio": 1.7626459143968871, "no_speech_prob": 1.644157964619808e-05}, {"id": 481, "seek": 215388, "start": 2153.88, "end": 2158.44, "text": " And it can do all kinds of cool stuff like it can merge back in results from earlier", "tokens": [400, 309, 393, 360, 439, 3685, 295, 1627, 1507, 411, 309, 393, 22183, 646, 294, 3542, 490, 3071], "temperature": 0.0, "avg_logprob": -0.1788418352111312, "compression_ratio": 1.6714285714285715, "no_speech_prob": 2.4681270588189363e-05}, {"id": 482, "seek": 215388, "start": 2158.44, "end": 2164.7200000000003, "text": " pushes or you can undo the change you made between these two dates, so on and so forth.", "tokens": [21020, 420, 291, 393, 23779, 264, 1319, 291, 1027, 1296, 613, 732, 11691, 11, 370, 322, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1788418352111312, "compression_ratio": 1.6714285714285715, "no_speech_prob": 2.4681270588189363e-05}, {"id": 483, "seek": 215388, "start": 2164.7200000000003, "end": 2169.52, "text": " And most importantly, at the end of the competition when you win and Anthony sends you an email,", "tokens": [400, 881, 8906, 11, 412, 264, 917, 295, 264, 6211, 562, 291, 1942, 293, 15853, 14790, 291, 364, 3796, 11], "temperature": 0.0, "avg_logprob": -0.1788418352111312, "compression_ratio": 1.6714285714285715, "no_speech_prob": 2.4681270588189363e-05}, {"id": 484, "seek": 215388, "start": 2169.52, "end": 2174.6, "text": " it's fantastic, send us your winning model and you go, oh, I don't have the winning model", "tokens": [309, 311, 5456, 11, 2845, 505, 428, 8224, 2316, 293, 291, 352, 11, 1954, 11, 286, 500, 380, 362, 264, 8224, 2316], "temperature": 0.0, "avg_logprob": -0.1788418352111312, "compression_ratio": 1.6714285714285715, "no_speech_prob": 2.4681270588189363e-05}, {"id": 485, "seek": 215388, "start": 2174.6, "end": 2175.6, "text": " anymore.", "tokens": [3602, 13], "temperature": 0.0, "avg_logprob": -0.1788418352111312, "compression_ratio": 1.6714285714285715, "no_speech_prob": 2.4681270588189363e-05}, {"id": 486, "seek": 215388, "start": 2175.6, "end": 2176.6, "text": " No problem.", "tokens": [883, 1154, 13], "temperature": 0.0, "avg_logprob": -0.1788418352111312, "compression_ratio": 1.6714285714285715, "no_speech_prob": 2.4681270588189363e-05}, {"id": 487, "seek": 215388, "start": 2176.6, "end": 2182.0, "text": " You can go back into your version control tool and ask for it as it was on the day that", "tokens": [509, 393, 352, 646, 666, 428, 3037, 1969, 2290, 293, 1029, 337, 309, 382, 309, 390, 322, 264, 786, 300], "temperature": 0.0, "avg_logprob": -0.1788418352111312, "compression_ratio": 1.6714285714285715, "no_speech_prob": 2.4681270588189363e-05}, {"id": 488, "seek": 218200, "start": 2182.0, "end": 2187.44, "text": " you had that fantastic answer.", "tokens": [291, 632, 300, 5456, 1867, 13], "temperature": 0.0, "avg_logprob": -0.11943682034810384, "compression_ratio": 1.5613207547169812, "no_speech_prob": 3.07135924231261e-05}, {"id": 489, "seek": 218200, "start": 2187.44, "end": 2191.68, "text": " So there's my toolkit.", "tokens": [407, 456, 311, 452, 40167, 13], "temperature": 0.0, "avg_logprob": -0.11943682034810384, "compression_ratio": 1.5613207547169812, "no_speech_prob": 3.07135924231261e-05}, {"id": 490, "seek": 218200, "start": 2191.68, "end": 2194.44, "text": " There's quite a lot of other things I wanted to show you, but I don't have time to do.", "tokens": [821, 311, 1596, 257, 688, 295, 661, 721, 286, 1415, 281, 855, 291, 11, 457, 286, 500, 380, 362, 565, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.11943682034810384, "compression_ratio": 1.5613207547169812, "no_speech_prob": 3.07135924231261e-05}, {"id": 491, "seek": 218200, "start": 2194.44, "end": 2203.76, "text": " So what I'm going to do is I'm going to jump to this interesting one, which was about predicting", "tokens": [407, 437, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 3012, 281, 341, 1880, 472, 11, 597, 390, 466, 32884], "temperature": 0.0, "avg_logprob": -0.11943682034810384, "compression_ratio": 1.5613207547169812, "no_speech_prob": 3.07135924231261e-05}, {"id": 492, "seek": 218200, "start": 2203.76, "end": 2209.4, "text": " which grants would be successful or unsuccessful at the University of Melbourne based on data", "tokens": [597, 16101, 576, 312, 4406, 420, 46258, 412, 264, 3535, 295, 27496, 2361, 322, 1412], "temperature": 0.0, "avg_logprob": -0.11943682034810384, "compression_ratio": 1.5613207547169812, "no_speech_prob": 3.07135924231261e-05}, {"id": 493, "seek": 220940, "start": 2209.4, "end": 2217.28, "text": " about the people involved in the grant and all kinds of metadata about the application.", "tokens": [466, 264, 561, 3288, 294, 264, 6386, 293, 439, 3685, 295, 26603, 466, 264, 3861, 13], "temperature": 0.0, "avg_logprob": -0.21910572052001953, "compression_ratio": 1.481283422459893, "no_speech_prob": 6.399358971975744e-05}, {"id": 494, "seek": 220940, "start": 2217.28, "end": 2222.8, "text": " This one's interesting because I won it by a fair margin.", "tokens": [639, 472, 311, 1880, 570, 286, 1582, 309, 538, 257, 3143, 10270, 13], "temperature": 0.0, "avg_logprob": -0.21910572052001953, "compression_ratio": 1.481283422459893, "no_speech_prob": 6.399358971975744e-05}, {"id": 495, "seek": 220940, "start": 2222.8, "end": 2227.36, "text": " The 0.967 is kind of 25% of the available error.", "tokens": [440, 1958, 13, 24, 22452, 307, 733, 295, 3552, 4, 295, 264, 2435, 6713, 13], "temperature": 0.0, "avg_logprob": -0.21910572052001953, "compression_ratio": 1.481283422459893, "no_speech_prob": 6.399358971975744e-05}, {"id": 496, "seek": 220940, "start": 2227.36, "end": 2237.12, "text": " It's interesting to think what did I do right this time and how did I set this up?", "tokens": [467, 311, 1880, 281, 519, 437, 630, 286, 360, 558, 341, 565, 293, 577, 630, 286, 992, 341, 493, 30], "temperature": 0.0, "avg_logprob": -0.21910572052001953, "compression_ratio": 1.481283422459893, "no_speech_prob": 6.399358971975744e-05}, {"id": 497, "seek": 223712, "start": 2237.12, "end": 2239.6, "text": " Basically what I did in this was I used a random forest.", "tokens": [8537, 437, 286, 630, 294, 341, 390, 286, 1143, 257, 4974, 6719, 13], "temperature": 0.0, "avg_logprob": -0.18763048607006408, "compression_ratio": 1.608, "no_speech_prob": 2.545991083025001e-05}, {"id": 498, "seek": 223712, "start": 2239.6, "end": 2242.52, "text": " So I'm going to show you guys a bit about random forests.", "tokens": [407, 286, 478, 516, 281, 855, 291, 1074, 257, 857, 466, 4974, 21700, 13], "temperature": 0.0, "avg_logprob": -0.18763048607006408, "compression_ratio": 1.608, "no_speech_prob": 2.545991083025001e-05}, {"id": 499, "seek": 223712, "start": 2242.52, "end": 2248.52, "text": " What's also interesting in this is I didn't use R at all.", "tokens": [708, 311, 611, 1880, 294, 341, 307, 286, 994, 380, 764, 497, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.18763048607006408, "compression_ratio": 1.608, "no_speech_prob": 2.545991083025001e-05}, {"id": 500, "seek": 223712, "start": 2248.52, "end": 2252.96, "text": " That's not to say that R couldn't have come up with a pretty interesting answer.", "tokens": [663, 311, 406, 281, 584, 300, 497, 2809, 380, 362, 808, 493, 365, 257, 1238, 1880, 1867, 13], "temperature": 0.0, "avg_logprob": -0.18763048607006408, "compression_ratio": 1.608, "no_speech_prob": 2.545991083025001e-05}, {"id": 501, "seek": 223712, "start": 2252.96, "end": 2259.12, "text": " The guy who came second in this comp used SAS, but I think he used like 12 gig of RAM,", "tokens": [440, 2146, 567, 1361, 1150, 294, 341, 715, 1143, 33441, 11, 457, 286, 519, 415, 1143, 411, 2272, 8741, 295, 14561, 11], "temperature": 0.0, "avg_logprob": -0.18763048607006408, "compression_ratio": 1.608, "no_speech_prob": 2.545991083025001e-05}, {"id": 502, "seek": 223712, "start": 2259.12, "end": 2261.7599999999998, "text": " multi-core, huge thing.", "tokens": [4825, 12, 12352, 11, 2603, 551, 13], "temperature": 0.0, "avg_logprob": -0.18763048607006408, "compression_ratio": 1.608, "no_speech_prob": 2.545991083025001e-05}, {"id": 503, "seek": 223712, "start": 2261.7599999999998, "end": 2264.7999999999997, "text": " Mine ran on my laptop in two seconds.", "tokens": [11620, 5872, 322, 452, 10732, 294, 732, 3949, 13], "temperature": 0.0, "avg_logprob": -0.18763048607006408, "compression_ratio": 1.608, "no_speech_prob": 2.545991083025001e-05}, {"id": 504, "seek": 226480, "start": 2264.8, "end": 2280.6800000000003, "text": " So I'll show you an approach which is very efficient as well as being very powerful.", "tokens": [407, 286, 603, 855, 291, 364, 3109, 597, 307, 588, 7148, 382, 731, 382, 885, 588, 4005, 13], "temperature": 0.0, "avg_logprob": -0.12734128447139964, "compression_ratio": 1.4624277456647399, "no_speech_prob": 1.805739157134667e-05}, {"id": 505, "seek": 226480, "start": 2280.6800000000003, "end": 2286.44, "text": " So I did this all in C sharp and the reason that I didn't use R for this is because the", "tokens": [407, 286, 630, 341, 439, 294, 383, 8199, 293, 264, 1778, 300, 286, 994, 380, 764, 497, 337, 341, 307, 570, 264], "temperature": 0.0, "avg_logprob": -0.12734128447139964, "compression_ratio": 1.4624277456647399, "no_speech_prob": 1.805739157134667e-05}, {"id": 506, "seek": 226480, "start": 2286.44, "end": 2288.2400000000002, "text": " data was kind of complex.", "tokens": [1412, 390, 733, 295, 3997, 13], "temperature": 0.0, "avg_logprob": -0.12734128447139964, "compression_ratio": 1.4624277456647399, "no_speech_prob": 1.805739157134667e-05}, {"id": 507, "seek": 226480, "start": 2288.2400000000002, "end": 2292.48, "text": " Each grant had a whole bunch of people attached to it.", "tokens": [6947, 6386, 632, 257, 1379, 3840, 295, 561, 8570, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.12734128447139964, "compression_ratio": 1.4624277456647399, "no_speech_prob": 1.805739157134667e-05}, {"id": 508, "seek": 229248, "start": 2292.48, "end": 2294.92, "text": " It was done in a denormalized form.", "tokens": [467, 390, 1096, 294, 257, 1441, 24440, 1602, 1254, 13], "temperature": 0.0, "avg_logprob": -0.19532352270081985, "compression_ratio": 1.7179487179487178, "no_speech_prob": 4.832172635360621e-05}, {"id": 509, "seek": 229248, "start": 2294.92, "end": 2304.4, "text": " I don't know how many of you guys are familiar with kind of normalization strategies, but", "tokens": [286, 500, 380, 458, 577, 867, 295, 291, 1074, 366, 4963, 365, 733, 295, 2710, 2144, 9029, 11, 457], "temperature": 0.0, "avg_logprob": -0.19532352270081985, "compression_ratio": 1.7179487179487178, "no_speech_prob": 4.832172635360621e-05}, {"id": 510, "seek": 229248, "start": 2304.4, "end": 2310.08, "text": " basically denormalized form basically means you had a whole bunch of information about", "tokens": [1936, 1441, 24440, 1602, 1254, 1936, 1355, 291, 632, 257, 1379, 3840, 295, 1589, 466], "temperature": 0.0, "avg_logprob": -0.19532352270081985, "compression_ratio": 1.7179487179487178, "no_speech_prob": 4.832172635360621e-05}, {"id": 511, "seek": 229248, "start": 2310.08, "end": 2314.72, "text": " the grant, you know, kind of the dates and blah, blah, blah.", "tokens": [264, 6386, 11, 291, 458, 11, 733, 295, 264, 11691, 293, 12288, 11, 12288, 11, 12288, 13], "temperature": 0.0, "avg_logprob": -0.19532352270081985, "compression_ratio": 1.7179487179487178, "no_speech_prob": 4.832172635360621e-05}, {"id": 512, "seek": 229248, "start": 2314.72, "end": 2318.72, "text": " And then there was a whole bunch of columns about person one.", "tokens": [400, 550, 456, 390, 257, 1379, 3840, 295, 13766, 466, 954, 472, 13], "temperature": 0.0, "avg_logprob": -0.19532352270081985, "compression_ratio": 1.7179487179487178, "no_speech_prob": 4.832172635360621e-05}, {"id": 513, "seek": 231872, "start": 2318.72, "end": 2323.48, "text": " Did they have a PhD?", "tokens": [2589, 436, 362, 257, 14476, 30], "temperature": 0.0, "avg_logprob": -0.1773867130279541, "compression_ratio": 1.4619047619047618, "no_speech_prob": 5.59423961021821e-06}, {"id": 514, "seek": 231872, "start": 2323.48, "end": 2327.2, "text": " And then there's a whole bunch of columns about person two, right?", "tokens": [400, 550, 456, 311, 257, 1379, 3840, 295, 13766, 466, 954, 732, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1773867130279541, "compression_ratio": 1.4619047619047618, "no_speech_prob": 5.59423961021821e-06}, {"id": 515, "seek": 231872, "start": 2327.2, "end": 2332.3599999999997, "text": " And so forth for I think it was about 13 people.", "tokens": [400, 370, 5220, 337, 286, 519, 309, 390, 466, 3705, 561, 13], "temperature": 0.0, "avg_logprob": -0.1773867130279541, "compression_ratio": 1.4619047619047618, "no_speech_prob": 5.59423961021821e-06}, {"id": 516, "seek": 231872, "start": 2332.3599999999997, "end": 2341.12, "text": " Very, very difficult to model this extremely wide and extremely, you know, messy data set.", "tokens": [4372, 11, 588, 2252, 281, 2316, 341, 4664, 4874, 293, 4664, 11, 291, 458, 11, 16191, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.1773867130279541, "compression_ratio": 1.4619047619047618, "no_speech_prob": 5.59423961021821e-06}, {"id": 517, "seek": 231872, "start": 2341.12, "end": 2344.7599999999998, "text": " It's the kind of thing that general purpose computing tools are pretty good at.", "tokens": [467, 311, 264, 733, 295, 551, 300, 2674, 4334, 15866, 3873, 366, 1238, 665, 412, 13], "temperature": 0.0, "avg_logprob": -0.1773867130279541, "compression_ratio": 1.4619047619047618, "no_speech_prob": 5.59423961021821e-06}, {"id": 518, "seek": 234476, "start": 2344.76, "end": 2352.28, "text": " So I pulled this into C sharp and created a grants data class where basically I went,", "tokens": [407, 286, 7373, 341, 666, 383, 8199, 293, 2942, 257, 16101, 1412, 1508, 689, 1936, 286, 1437, 11], "temperature": 0.0, "avg_logprob": -0.19883614398063498, "compression_ratio": 1.6816326530612244, "no_speech_prob": 2.3922204491100274e-05}, {"id": 519, "seek": 234476, "start": 2352.28, "end": 2358.1000000000004, "text": " okay, read through this file and I created this thing called grants data and for each", "tokens": [1392, 11, 1401, 807, 341, 3991, 293, 286, 2942, 341, 551, 1219, 16101, 1412, 293, 337, 1184], "temperature": 0.0, "avg_logprob": -0.19883614398063498, "compression_ratio": 1.6816326530612244, "no_speech_prob": 2.3922204491100274e-05}, {"id": 520, "seek": 234476, "start": 2358.1000000000004, "end": 2363.6800000000003, "text": " line I split it on a comma and I added that grant to this grants data.", "tokens": [1622, 286, 7472, 309, 322, 257, 22117, 293, 286, 3869, 300, 6386, 281, 341, 16101, 1412, 13], "temperature": 0.0, "avg_logprob": -0.19883614398063498, "compression_ratio": 1.6816326530612244, "no_speech_prob": 2.3922204491100274e-05}, {"id": 521, "seek": 234476, "start": 2363.6800000000003, "end": 2368.6000000000004, "text": " For those people who maybe aren't so familiar with general purpose computing, general purpose", "tokens": [1171, 729, 561, 567, 1310, 3212, 380, 370, 4963, 365, 2674, 4334, 15866, 11, 2674, 4334], "temperature": 0.0, "avg_logprob": -0.19883614398063498, "compression_ratio": 1.6816326530612244, "no_speech_prob": 2.3922204491100274e-05}, {"id": 522, "seek": 234476, "start": 2368.6000000000004, "end": 2372.2000000000003, "text": " programming languages, you might be surprised to see how readable they are.", "tokens": [9410, 8650, 11, 291, 1062, 312, 6100, 281, 536, 577, 49857, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.19883614398063498, "compression_ratio": 1.6816326530612244, "no_speech_prob": 2.3922204491100274e-05}, {"id": 523, "seek": 237220, "start": 2372.2, "end": 2378.6, "text": " You know, this idea I can say for each something in lines dot select the lines split by comma.", "tokens": [509, 458, 11, 341, 1558, 286, 393, 584, 337, 1184, 746, 294, 3876, 5893, 3048, 264, 3876, 7472, 538, 22117, 13], "temperature": 0.0, "avg_logprob": -0.21021836322286855, "compression_ratio": 1.6766917293233083, "no_speech_prob": 6.604207737836987e-05}, {"id": 524, "seek": 237220, "start": 2378.6, "end": 2383.9199999999996, "text": " You know, if you haven't used anything with portrait, you might be surprised that something", "tokens": [509, 458, 11, 498, 291, 2378, 380, 1143, 1340, 365, 17126, 11, 291, 1062, 312, 6100, 300, 746], "temperature": 0.0, "avg_logprob": -0.21021836322286855, "compression_ratio": 1.6766917293233083, "no_speech_prob": 6.604207737836987e-05}, {"id": 525, "seek": 237220, "start": 2383.9199999999996, "end": 2385.8799999999997, "text": " like C sharp looks so easy.", "tokens": [411, 383, 8199, 1542, 370, 1858, 13], "temperature": 0.0, "avg_logprob": -0.21021836322286855, "compression_ratio": 1.6766917293233083, "no_speech_prob": 6.604207737836987e-05}, {"id": 526, "seek": 237220, "start": 2385.8799999999997, "end": 2388.56, "text": " File dot read lines dot skip some lines.", "tokens": [26196, 5893, 1401, 3876, 5893, 10023, 512, 3876, 13], "temperature": 0.0, "avg_logprob": -0.21021836322286855, "compression_ratio": 1.6766917293233083, "no_speech_prob": 6.604207737836987e-05}, {"id": 527, "seek": 237220, "start": 2388.56, "end": 2391.08, "text": " This is just to skip the first line of the header.", "tokens": [639, 307, 445, 281, 10023, 264, 700, 1622, 295, 264, 23117, 13], "temperature": 0.0, "avg_logprob": -0.21021836322286855, "compression_ratio": 1.6766917293233083, "no_speech_prob": 6.604207737836987e-05}, {"id": 528, "seek": 237220, "start": 2391.08, "end": 2395.0, "text": " And in fact, later on I discovered the first couple of years of data were not very predictive", "tokens": [400, 294, 1186, 11, 1780, 322, 286, 6941, 264, 700, 1916, 295, 924, 295, 1412, 645, 406, 588, 35521], "temperature": 0.0, "avg_logprob": -0.21021836322286855, "compression_ratio": 1.6766917293233083, "no_speech_prob": 6.604207737836987e-05}, {"id": 529, "seek": 237220, "start": 2395.0, "end": 2396.2799999999997, "text": " of today.", "tokens": [295, 965, 13], "temperature": 0.0, "avg_logprob": -0.21021836322286855, "compression_ratio": 1.6766917293233083, "no_speech_prob": 6.604207737836987e-05}, {"id": 530, "seek": 237220, "start": 2396.2799999999997, "end": 2400.24, "text": " So I actually skipped all of those.", "tokens": [407, 286, 767, 30193, 439, 295, 729, 13], "temperature": 0.0, "avg_logprob": -0.21021836322286855, "compression_ratio": 1.6766917293233083, "no_speech_prob": 6.604207737836987e-05}, {"id": 531, "seek": 240024, "start": 2400.24, "end": 2403.64, "text": " And the other nice thing about these kind of tools is, okay, what does this dot add", "tokens": [400, 264, 661, 1481, 551, 466, 613, 733, 295, 3873, 307, 11, 1392, 11, 437, 775, 341, 5893, 909], "temperature": 0.0, "avg_logprob": -0.1615564437139602, "compression_ratio": 1.5783132530120483, "no_speech_prob": 3.76345997210592e-05}, {"id": 532, "seek": 240024, "start": 2403.64, "end": 2404.64, "text": " do?", "tokens": [360, 30], "temperature": 0.0, "avg_logprob": -0.1615564437139602, "compression_ratio": 1.5783132530120483, "no_speech_prob": 3.76345997210592e-05}, {"id": 533, "seek": 240024, "start": 2404.64, "end": 2408.12, "text": " I can whack one button and bang, I'm into the definition of dot add.", "tokens": [286, 393, 42877, 472, 2960, 293, 8550, 11, 286, 478, 666, 264, 7123, 295, 5893, 909, 13], "temperature": 0.0, "avg_logprob": -0.1615564437139602, "compression_ratio": 1.5783132530120483, "no_speech_prob": 3.76345997210592e-05}, {"id": 534, "seek": 240024, "start": 2408.12, "end": 2412.2799999999997, "text": " You know, these kind of IDE features are really helpful.", "tokens": [509, 458, 11, 613, 733, 295, 40930, 4122, 366, 534, 4961, 13], "temperature": 0.0, "avg_logprob": -0.1615564437139602, "compression_ratio": 1.5783132530120483, "no_speech_prob": 3.76345997210592e-05}, {"id": 535, "seek": 240024, "start": 2412.2799999999997, "end": 2419.2, "text": " And this is equally true of most Python and Java and C++ editors as well.", "tokens": [400, 341, 307, 12309, 2074, 295, 881, 15329, 293, 10745, 293, 383, 25472, 31446, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1615564437139602, "compression_ratio": 1.5783132530120483, "no_speech_prob": 3.76345997210592e-05}, {"id": 536, "seek": 240024, "start": 2419.2, "end": 2426.64, "text": " So the kind of stuff that I was able to do here was to create all kinds of interesting", "tokens": [407, 264, 733, 295, 1507, 300, 286, 390, 1075, 281, 360, 510, 390, 281, 1884, 439, 3685, 295, 1880], "temperature": 0.0, "avg_logprob": -0.1615564437139602, "compression_ratio": 1.5783132530120483, "no_speech_prob": 3.76345997210592e-05}, {"id": 537, "seek": 240024, "start": 2426.64, "end": 2428.3199999999997, "text": " derived variables.", "tokens": [18949, 9102, 13], "temperature": 0.0, "avg_logprob": -0.1615564437139602, "compression_ratio": 1.5783132530120483, "no_speech_prob": 3.76345997210592e-05}, {"id": 538, "seek": 242832, "start": 2428.32, "end": 2431.6000000000004, "text": " Like here's one called max year birth.", "tokens": [1743, 510, 311, 472, 1219, 11469, 1064, 3965, 13], "temperature": 0.0, "avg_logprob": -0.18671190620648978, "compression_ratio": 1.6074380165289257, "no_speech_prob": 4.539516885415651e-05}, {"id": 539, "seek": 242832, "start": 2431.6000000000004, "end": 2436.44, "text": " So this one is one that goes through all of the people on this application and finds the", "tokens": [407, 341, 472, 307, 472, 300, 1709, 807, 439, 295, 264, 561, 322, 341, 3861, 293, 10704, 264], "temperature": 0.0, "avg_logprob": -0.18671190620648978, "compression_ratio": 1.6074380165289257, "no_speech_prob": 4.539516885415651e-05}, {"id": 540, "seek": 242832, "start": 2436.44, "end": 2439.76, "text": " one with the largest year of birth.", "tokens": [472, 365, 264, 6443, 1064, 295, 3965, 13], "temperature": 0.0, "avg_logprob": -0.18671190620648978, "compression_ratio": 1.6074380165289257, "no_speech_prob": 4.539516885415651e-05}, {"id": 541, "seek": 242832, "start": 2439.76, "end": 2444.32, "text": " Okay, again, it's just a single line of code.", "tokens": [1033, 11, 797, 11, 309, 311, 445, 257, 2167, 1622, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.18671190620648978, "compression_ratio": 1.6074380165289257, "no_speech_prob": 4.539516885415651e-05}, {"id": 542, "seek": 242832, "start": 2444.32, "end": 2448.52, "text": " If you kind of get around the kind of curly brackets and other things like that, the actual", "tokens": [759, 291, 733, 295, 483, 926, 264, 733, 295, 32066, 26179, 293, 661, 721, 411, 300, 11, 264, 3539], "temperature": 0.0, "avg_logprob": -0.18671190620648978, "compression_ratio": 1.6074380165289257, "no_speech_prob": 4.539516885415651e-05}, {"id": 543, "seek": 242832, "start": 2448.52, "end": 2455.52, "text": " logic is extremely easy to understand, you know, things like do any of them have a PhD?", "tokens": [9952, 307, 4664, 1858, 281, 1223, 11, 291, 458, 11, 721, 411, 360, 604, 295, 552, 362, 257, 14476, 30], "temperature": 0.0, "avg_logprob": -0.18671190620648978, "compression_ratio": 1.6074380165289257, "no_speech_prob": 4.539516885415651e-05}, {"id": 544, "seek": 245552, "start": 2455.52, "end": 2458.8, "text": " Well, if there's no people in it, none of them do.", "tokens": [1042, 11, 498, 456, 311, 572, 561, 294, 309, 11, 6022, 295, 552, 360, 13], "temperature": 0.0, "avg_logprob": -0.3156959132144326, "compression_ratio": 1.5362903225806452, "no_speech_prob": 5.1439728849800304e-05}, {"id": 545, "seek": 245552, "start": 2458.8, "end": 2464.72, "text": " Otherwise, oh, this is just one person has a PhD down here somewhere I've got.", "tokens": [10328, 11, 1954, 11, 341, 307, 445, 472, 954, 575, 257, 14476, 760, 510, 4079, 286, 600, 658, 13], "temperature": 0.0, "avg_logprob": -0.3156959132144326, "compression_ratio": 1.5362903225806452, "no_speech_prob": 5.1439728849800304e-05}, {"id": 546, "seek": 245552, "start": 2464.72, "end": 2466.32, "text": " I can find it.", "tokens": [286, 393, 915, 309, 13], "temperature": 0.0, "avg_logprob": -0.3156959132144326, "compression_ratio": 1.5362903225806452, "no_speech_prob": 5.1439728849800304e-05}, {"id": 547, "seek": 245552, "start": 2466.32, "end": 2469.32, "text": " Any has a PhD.", "tokens": [2639, 575, 257, 14476, 13], "temperature": 0.0, "avg_logprob": -0.3156959132144326, "compression_ratio": 1.5362903225806452, "no_speech_prob": 5.1439728849800304e-05}, {"id": 548, "seek": 245552, "start": 2469.32, "end": 2471.32, "text": " Bang, straight to there.", "tokens": [11538, 11, 2997, 281, 456, 13], "temperature": 0.0, "avg_logprob": -0.3156959132144326, "compression_ratio": 1.5362903225806452, "no_speech_prob": 5.1439728849800304e-05}, {"id": 549, "seek": 245552, "start": 2471.32, "end": 2472.32, "text": " There you go.", "tokens": [821, 291, 352, 13], "temperature": 0.0, "avg_logprob": -0.3156959132144326, "compression_ratio": 1.5362903225806452, "no_speech_prob": 5.1439728849800304e-05}, {"id": 550, "seek": 245552, "start": 2472.32, "end": 2473.72, "text": " Does any person have a PhD?", "tokens": [4402, 604, 954, 362, 257, 14476, 30], "temperature": 0.0, "avg_logprob": -0.3156959132144326, "compression_ratio": 1.5362903225806452, "no_speech_prob": 5.1439728849800304e-05}, {"id": 551, "seek": 245552, "start": 2473.72, "end": 2476.04, "text": " So I create all these different derived fields.", "tokens": [407, 286, 1884, 439, 613, 819, 18949, 7909, 13], "temperature": 0.0, "avg_logprob": -0.3156959132144326, "compression_ratio": 1.5362903225806452, "no_speech_prob": 5.1439728849800304e-05}, {"id": 552, "seek": 245552, "start": 2476.04, "end": 2481.0, "text": " I use pivot tables to kind of work out which ones seem to be quite predictive before I", "tokens": [286, 764, 14538, 8020, 281, 733, 295, 589, 484, 597, 2306, 1643, 281, 312, 1596, 35521, 949, 286], "temperature": 0.0, "avg_logprob": -0.3156959132144326, "compression_ratio": 1.5362903225806452, "no_speech_prob": 5.1439728849800304e-05}, {"id": 553, "seek": 245552, "start": 2481.0, "end": 2484.88, "text": " put these together.", "tokens": [829, 613, 1214, 13], "temperature": 0.0, "avg_logprob": -0.3156959132144326, "compression_ratio": 1.5362903225806452, "no_speech_prob": 5.1439728849800304e-05}, {"id": 554, "seek": 248488, "start": 2484.88, "end": 2486.8, "text": " And so what did I do with this?", "tokens": [400, 370, 437, 630, 286, 360, 365, 341, 30], "temperature": 0.0, "avg_logprob": -0.2035789663141424, "compression_ratio": 1.3973509933774835, "no_speech_prob": 2.546189534768928e-05}, {"id": 555, "seek": 248488, "start": 2486.8, "end": 2490.1600000000003, "text": " Well, I wanted to create a random forest.", "tokens": [1042, 11, 286, 1415, 281, 1884, 257, 4974, 6719, 13], "temperature": 0.0, "avg_logprob": -0.2035789663141424, "compression_ratio": 1.3973509933774835, "no_speech_prob": 2.546189534768928e-05}, {"id": 556, "seek": 248488, "start": 2490.1600000000003, "end": 2498.52, "text": " Now random forests in are a very powerful, very general purpose tool.", "tokens": [823, 4974, 21700, 294, 366, 257, 588, 4005, 11, 588, 2674, 4334, 2290, 13], "temperature": 0.0, "avg_logprob": -0.2035789663141424, "compression_ratio": 1.3973509933774835, "no_speech_prob": 2.546189534768928e-05}, {"id": 557, "seek": 248488, "start": 2498.52, "end": 2505.6400000000003, "text": " But the R implementation of them has some pretty nasty limitations.", "tokens": [583, 264, 497, 11420, 295, 552, 575, 512, 1238, 17923, 15705, 13], "temperature": 0.0, "avg_logprob": -0.2035789663141424, "compression_ratio": 1.3973509933774835, "no_speech_prob": 2.546189534768928e-05}, {"id": 558, "seek": 250564, "start": 2505.64, "end": 2515.8399999999997, "text": " For example, if you have a categorical variable, in other words, a factor, it can't have any", "tokens": [1171, 1365, 11, 498, 291, 362, 257, 19250, 804, 7006, 11, 294, 661, 2283, 11, 257, 5952, 11, 309, 393, 380, 362, 604], "temperature": 0.0, "avg_logprob": -0.09264416164822048, "compression_ratio": 1.5390070921985815, "no_speech_prob": 8.013160368136596e-06}, {"id": 559, "seek": 250564, "start": 2515.8399999999997, "end": 2520.0, "text": " more than 32 levels.", "tokens": [544, 813, 8858, 4358, 13], "temperature": 0.0, "avg_logprob": -0.09264416164822048, "compression_ratio": 1.5390070921985815, "no_speech_prob": 8.013160368136596e-06}, {"id": 560, "seek": 250564, "start": 2520.0, "end": 2528.92, "text": " If you have a continuous variable, so like an integer or a double or whatever, it can't", "tokens": [759, 291, 362, 257, 10957, 7006, 11, 370, 411, 364, 24922, 420, 257, 3834, 420, 2035, 11, 309, 393, 380], "temperature": 0.0, "avg_logprob": -0.09264416164822048, "compression_ratio": 1.5390070921985815, "no_speech_prob": 8.013160368136596e-06}, {"id": 561, "seek": 250564, "start": 2528.92, "end": 2533.08, "text": " have any nulls.", "tokens": [362, 604, 18184, 82, 13], "temperature": 0.0, "avg_logprob": -0.09264416164822048, "compression_ratio": 1.5390070921985815, "no_speech_prob": 8.013160368136596e-06}, {"id": 562, "seek": 253308, "start": 2533.08, "end": 2535.7599999999998, "text": " So there are these nasty limitations that make it quite difficult.", "tokens": [407, 456, 366, 613, 17923, 15705, 300, 652, 309, 1596, 2252, 13], "temperature": 0.0, "avg_logprob": -0.2265134588018194, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.828701250720769e-05}, {"id": 563, "seek": 253308, "start": 2535.7599999999998, "end": 2539.7599999999998, "text": " It's particularly difficult to use in this case because things like the RFCD codes have", "tokens": [467, 311, 4098, 2252, 281, 764, 294, 341, 1389, 570, 721, 411, 264, 497, 18671, 35, 14211, 362], "temperature": 0.0, "avg_logprob": -0.2265134588018194, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.828701250720769e-05}, {"id": 564, "seek": 253308, "start": 2539.7599999999998, "end": 2541.64, "text": " hundreds and hundreds of levels.", "tokens": [6779, 293, 6779, 295, 4358, 13], "temperature": 0.0, "avg_logprob": -0.2265134588018194, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.828701250720769e-05}, {"id": 565, "seek": 253308, "start": 2541.64, "end": 2545.2799999999997, "text": " And all the continuous variables were full of nulls.", "tokens": [400, 439, 264, 10957, 9102, 645, 1577, 295, 18184, 82, 13], "temperature": 0.0, "avg_logprob": -0.2265134588018194, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.828701250720769e-05}, {"id": 566, "seek": 253308, "start": 2545.2799999999997, "end": 2551.7599999999998, "text": " And in fact, if I remember correctly, even the factors aren't allowed to have nulls,", "tokens": [400, 294, 1186, 11, 498, 286, 1604, 8944, 11, 754, 264, 6771, 3212, 380, 4350, 281, 362, 18184, 82, 11], "temperature": 0.0, "avg_logprob": -0.2265134588018194, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.828701250720769e-05}, {"id": 567, "seek": 253308, "start": 2551.7599999999998, "end": 2555.88, "text": " which I find a bit weird because to me, null is just another factor.", "tokens": [597, 286, 915, 257, 857, 3657, 570, 281, 385, 11, 18184, 307, 445, 1071, 5952, 13], "temperature": 0.0, "avg_logprob": -0.2265134588018194, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.828701250720769e-05}, {"id": 568, "seek": 253308, "start": 2555.88, "end": 2561.7599999999998, "text": " They're male or they're female or they're unknown.", "tokens": [814, 434, 7133, 420, 436, 434, 6556, 420, 436, 434, 9841, 13], "temperature": 0.0, "avg_logprob": -0.2265134588018194, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.828701250720769e-05}, {"id": 569, "seek": 256176, "start": 2561.76, "end": 2563.76, "text": " It's still something I should get a model on.", "tokens": [467, 311, 920, 746, 286, 820, 483, 257, 2316, 322, 13], "temperature": 0.0, "avg_logprob": -0.17455500987038683, "compression_ratio": 1.6209150326797386, "no_speech_prob": 3.1691299227532e-05}, {"id": 570, "seek": 256176, "start": 2563.76, "end": 2573.6800000000003, "text": " So I created a system that basically made it easy for me to create a data set up model", "tokens": [407, 286, 2942, 257, 1185, 300, 1936, 1027, 309, 1858, 337, 385, 281, 1884, 257, 1412, 992, 493, 2316], "temperature": 0.0, "avg_logprob": -0.17455500987038683, "compression_ratio": 1.6209150326797386, "no_speech_prob": 3.1691299227532e-05}, {"id": 571, "seek": 256176, "start": 2573.6800000000003, "end": 2574.6800000000003, "text": " on.", "tokens": [322, 13], "temperature": 0.0, "avg_logprob": -0.17455500987038683, "compression_ratio": 1.6209150326797386, "no_speech_prob": 3.1691299227532e-05}, {"id": 572, "seek": 256176, "start": 2574.6800000000003, "end": 2576.48, "text": " So I made this decision.", "tokens": [407, 286, 1027, 341, 3537, 13], "temperature": 0.0, "avg_logprob": -0.17455500987038683, "compression_ratio": 1.6209150326797386, "no_speech_prob": 3.1691299227532e-05}, {"id": 573, "seek": 256176, "start": 2576.48, "end": 2586.8, "text": " I decided that for doubles that had nulls in them, I created something which basically", "tokens": [286, 3047, 300, 337, 31634, 300, 632, 18184, 82, 294, 552, 11, 286, 2942, 746, 597, 1936], "temperature": 0.0, "avg_logprob": -0.17455500987038683, "compression_ratio": 1.6209150326797386, "no_speech_prob": 3.1691299227532e-05}, {"id": 574, "seek": 258680, "start": 2586.8, "end": 2594.44, "text": " simply added two columns.", "tokens": [2935, 3869, 732, 13766, 13], "temperature": 0.0, "avg_logprob": -0.28027150437638565, "compression_ratio": 1.5931034482758621, "no_speech_prob": 1.0129809197678696e-05}, {"id": 575, "seek": 258680, "start": 2594.44, "end": 2598.7200000000003, "text": " One column which was, is that column null or not?", "tokens": [1485, 7738, 597, 390, 11, 307, 300, 7738, 18184, 420, 406, 30], "temperature": 0.0, "avg_logprob": -0.28027150437638565, "compression_ratio": 1.5931034482758621, "no_speech_prob": 1.0129809197678696e-05}, {"id": 576, "seek": 258680, "start": 2598.7200000000003, "end": 2601.52, "text": " One or zero?", "tokens": [1485, 420, 4018, 30], "temperature": 0.0, "avg_logprob": -0.28027150437638565, "compression_ratio": 1.5931034482758621, "no_speech_prob": 1.0129809197678696e-05}, {"id": 577, "seek": 258680, "start": 2601.52, "end": 2606.04, "text": " And another column which is the actual data from that column, so whatever it was, 2.3,", "tokens": [400, 1071, 7738, 597, 307, 264, 3539, 1412, 490, 300, 7738, 11, 370, 2035, 309, 390, 11, 568, 13, 18, 11], "temperature": 0.0, "avg_logprob": -0.28027150437638565, "compression_ratio": 1.5931034482758621, "no_speech_prob": 1.0129809197678696e-05}, {"id": 578, "seek": 258680, "start": 2606.04, "end": 2612.5600000000004, "text": " that one's a null, sorry, 2.36, blah, blah, blah, blah.", "tokens": [300, 472, 311, 257, 18184, 11, 2597, 11, 568, 13, 11309, 11, 12288, 11, 12288, 11, 12288, 11, 12288, 13], "temperature": 0.0, "avg_logprob": -0.28027150437638565, "compression_ratio": 1.5931034482758621, "no_speech_prob": 1.0129809197678696e-05}, {"id": 579, "seek": 261256, "start": 2612.56, "end": 2619.52, "text": " And wherever there was a null, I just replaced it with the median.", "tokens": [400, 8660, 456, 390, 257, 18184, 11, 286, 445, 10772, 309, 365, 264, 26779, 13], "temperature": 0.0, "avg_logprob": -0.13246333157574688, "compression_ratio": 1.733644859813084, "no_speech_prob": 7.1831723289506044e-06}, {"id": 580, "seek": 261256, "start": 2619.52, "end": 2625.7999999999997, "text": " So I now had two columns where I used to have one and both of them are now modelable.", "tokens": [407, 286, 586, 632, 732, 13766, 689, 286, 1143, 281, 362, 472, 293, 1293, 295, 552, 366, 586, 2316, 712, 13], "temperature": 0.0, "avg_logprob": -0.13246333157574688, "compression_ratio": 1.733644859813084, "no_speech_prob": 7.1831723289506044e-06}, {"id": 581, "seek": 261256, "start": 2625.7999999999997, "end": 2629.0, "text": " Why is that, why the median?", "tokens": [1545, 307, 300, 11, 983, 264, 26779, 30], "temperature": 0.0, "avg_logprob": -0.13246333157574688, "compression_ratio": 1.733644859813084, "no_speech_prob": 7.1831723289506044e-06}, {"id": 582, "seek": 261256, "start": 2629.0, "end": 2634.2, "text": " Actually it doesn't matter because every place where this is the median, there's a one over", "tokens": [5135, 309, 1177, 380, 1871, 570, 633, 1081, 689, 341, 307, 264, 26779, 11, 456, 311, 257, 472, 670], "temperature": 0.0, "avg_logprob": -0.13246333157574688, "compression_ratio": 1.733644859813084, "no_speech_prob": 7.1831723289506044e-06}, {"id": 583, "seek": 261256, "start": 2634.2, "end": 2635.2, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.13246333157574688, "compression_ratio": 1.733644859813084, "no_speech_prob": 7.1831723289506044e-06}, {"id": 584, "seek": 261256, "start": 2635.2, "end": 2638.08, "text": " So in my model, I'm going to use this as a predictor.", "tokens": [407, 294, 452, 2316, 11, 286, 478, 516, 281, 764, 341, 382, 257, 6069, 284, 13], "temperature": 0.0, "avg_logprob": -0.13246333157574688, "compression_ratio": 1.733644859813084, "no_speech_prob": 7.1831723289506044e-06}, {"id": 585, "seek": 261256, "start": 2638.08, "end": 2640.0, "text": " I'm going to use this as a predictor.", "tokens": [286, 478, 516, 281, 764, 341, 382, 257, 6069, 284, 13], "temperature": 0.0, "avg_logprob": -0.13246333157574688, "compression_ratio": 1.733644859813084, "no_speech_prob": 7.1831723289506044e-06}, {"id": 586, "seek": 264000, "start": 2640.0, "end": 2647.8, "text": " So if all of the places that that data column was originally null all meant something interesting,", "tokens": [407, 498, 439, 295, 264, 3190, 300, 300, 1412, 7738, 390, 7993, 18184, 439, 4140, 746, 1880, 11], "temperature": 0.0, "avg_logprob": -0.10822869340578715, "compression_ratio": 1.6608695652173913, "no_speech_prob": 7.183181878644973e-06}, {"id": 587, "seek": 264000, "start": 2647.8, "end": 2652.72, "text": " then it'll be picked up by this is null version of the column.", "tokens": [550, 309, 603, 312, 6183, 493, 538, 341, 307, 18184, 3037, 295, 264, 7738, 13], "temperature": 0.0, "avg_logprob": -0.10822869340578715, "compression_ratio": 1.6608695652173913, "no_speech_prob": 7.183181878644973e-06}, {"id": 588, "seek": 264000, "start": 2652.72, "end": 2658.6, "text": " So to me, this is something which I do, which I did automatically because it's clearly the", "tokens": [407, 281, 385, 11, 341, 307, 746, 597, 286, 360, 11, 597, 286, 630, 6772, 570, 309, 311, 4448, 264], "temperature": 0.0, "avg_logprob": -0.10822869340578715, "compression_ratio": 1.6608695652173913, "no_speech_prob": 7.183181878644973e-06}, {"id": 589, "seek": 264000, "start": 2658.6, "end": 2663.2, "text": " obvious way to deal with null values.", "tokens": [6322, 636, 281, 2028, 365, 18184, 4190, 13], "temperature": 0.0, "avg_logprob": -0.10822869340578715, "compression_ratio": 1.6608695652173913, "no_speech_prob": 7.183181878644973e-06}, {"id": 590, "seek": 264000, "start": 2663.2, "end": 2668.68, "text": " And then as I said in the categorical variables, I just said, okay, the factors, if there's", "tokens": [400, 550, 382, 286, 848, 294, 264, 19250, 804, 9102, 11, 286, 445, 848, 11, 1392, 11, 264, 6771, 11, 498, 456, 311], "temperature": 0.0, "avg_logprob": -0.10822869340578715, "compression_ratio": 1.6608695652173913, "no_speech_prob": 7.183181878644973e-06}, {"id": 591, "seek": 266868, "start": 2668.68, "end": 2672.3999999999996, "text": " a null, just treat it as another level.", "tokens": [257, 18184, 11, 445, 2387, 309, 382, 1071, 1496, 13], "temperature": 0.0, "avg_logprob": -0.2023401351202102, "compression_ratio": 1.943298969072165, "no_speech_prob": 8.530237209924962e-06}, {"id": 592, "seek": 266868, "start": 2672.3999999999996, "end": 2679.94, "text": " And then finally in the categorical, in the factors, I said, okay, take all of the levels", "tokens": [400, 550, 2721, 294, 264, 19250, 804, 11, 294, 264, 6771, 11, 286, 848, 11, 1392, 11, 747, 439, 295, 264, 4358], "temperature": 0.0, "avg_logprob": -0.2023401351202102, "compression_ratio": 1.943298969072165, "no_speech_prob": 8.530237209924962e-06}, {"id": 593, "seek": 266868, "start": 2679.94, "end": 2687.6, "text": " and if there are more observations than, I think it was 25, then keep it.", "tokens": [293, 498, 456, 366, 544, 18163, 813, 11, 286, 519, 309, 390, 3552, 11, 550, 1066, 309, 13], "temperature": 0.0, "avg_logprob": -0.2023401351202102, "compression_ratio": 1.943298969072165, "no_speech_prob": 8.530237209924962e-06}, {"id": 594, "seek": 266868, "start": 2687.6, "end": 2688.68, "text": " Or maybe it's more than that.", "tokens": [1610, 1310, 309, 311, 544, 813, 300, 13], "temperature": 0.0, "avg_logprob": -0.2023401351202102, "compression_ratio": 1.943298969072165, "no_speech_prob": 8.530237209924962e-06}, {"id": 595, "seek": 266868, "start": 2688.68, "end": 2692.0, "text": " I think if there's more levels, maybe if there's more observations than 100, then keep it.", "tokens": [286, 519, 498, 456, 311, 544, 4358, 11, 1310, 498, 456, 311, 544, 18163, 813, 2319, 11, 550, 1066, 309, 13], "temperature": 0.0, "avg_logprob": -0.2023401351202102, "compression_ratio": 1.943298969072165, "no_speech_prob": 8.530237209924962e-06}, {"id": 596, "seek": 266868, "start": 2692.0, "end": 2696.3199999999997, "text": " If there's more observations than 25, less than 100.", "tokens": [759, 456, 311, 544, 18163, 813, 3552, 11, 1570, 813, 2319, 13], "temperature": 0.0, "avg_logprob": -0.2023401351202102, "compression_ratio": 1.943298969072165, "no_speech_prob": 8.530237209924962e-06}, {"id": 597, "seek": 269632, "start": 2696.32, "end": 2700.28, "text": " And it was quite predictive, in other words, that level was different to the others in", "tokens": [400, 309, 390, 1596, 35521, 11, 294, 661, 2283, 11, 300, 1496, 390, 819, 281, 264, 2357, 294], "temperature": 0.0, "avg_logprob": -0.19288369883661685, "compression_ratio": 1.5351351351351352, "no_speech_prob": 7.766702765366063e-06}, {"id": 598, "seek": 269632, "start": 2700.28, "end": 2703.0, "text": " terms of application success, then keep it.", "tokens": [2115, 295, 3861, 2245, 11, 550, 1066, 309, 13], "temperature": 0.0, "avg_logprob": -0.19288369883661685, "compression_ratio": 1.5351351351351352, "no_speech_prob": 7.766702765366063e-06}, {"id": 599, "seek": 269632, "start": 2703.0, "end": 2709.6800000000003, "text": " Otherwise, merge all the rest into one super level called the rest.", "tokens": [10328, 11, 22183, 439, 264, 1472, 666, 472, 1687, 1496, 1219, 264, 1472, 13], "temperature": 0.0, "avg_logprob": -0.19288369883661685, "compression_ratio": 1.5351351351351352, "no_speech_prob": 7.766702765366063e-06}, {"id": 600, "seek": 269632, "start": 2709.6800000000003, "end": 2717.6800000000003, "text": " So that way I basically was able to create a data set which actually I could then fit", "tokens": [407, 300, 636, 286, 1936, 390, 1075, 281, 1884, 257, 1412, 992, 597, 767, 286, 727, 550, 3318], "temperature": 0.0, "avg_logprob": -0.19288369883661685, "compression_ratio": 1.5351351351351352, "no_speech_prob": 7.766702765366063e-06}, {"id": 601, "seek": 271768, "start": 2717.68, "end": 2726.9199999999996, "text": " into R, although I think in this case I actually ended up using my own random forest implementation.", "tokens": [666, 497, 11, 4878, 286, 519, 294, 341, 1389, 286, 767, 4590, 493, 1228, 452, 1065, 4974, 6719, 11420, 13], "temperature": 0.0, "avg_logprob": -0.15358995809787657, "compression_ratio": 1.5566037735849056, "no_speech_prob": 2.9310587706277147e-05}, {"id": 602, "seek": 271768, "start": 2726.9199999999996, "end": 2735.7599999999998, "text": " So should we have a quick talk about random forests and how they work?", "tokens": [407, 820, 321, 362, 257, 1702, 751, 466, 4974, 21700, 293, 577, 436, 589, 30], "temperature": 0.0, "avg_logprob": -0.15358995809787657, "compression_ratio": 1.5566037735849056, "no_speech_prob": 2.9310587706277147e-05}, {"id": 603, "seek": 271768, "start": 2735.7599999999998, "end": 2740.48, "text": " So to me, there's kind of basically two main types of model, right?", "tokens": [407, 281, 385, 11, 456, 311, 733, 295, 1936, 732, 2135, 3467, 295, 2316, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15358995809787657, "compression_ratio": 1.5566037735849056, "no_speech_prob": 2.9310587706277147e-05}, {"id": 604, "seek": 271768, "start": 2740.48, "end": 2745.44, "text": " There's these kind of parametric models, models with parameters, things where you say, oh,", "tokens": [821, 311, 613, 733, 295, 6220, 17475, 5245, 11, 5245, 365, 9834, 11, 721, 689, 291, 584, 11, 1954, 11], "temperature": 0.0, "avg_logprob": -0.15358995809787657, "compression_ratio": 1.5566037735849056, "no_speech_prob": 2.9310587706277147e-05}, {"id": 605, "seek": 274544, "start": 2745.44, "end": 2749.84, "text": " this bit's linear and this bit's interactive with this bit and this bit's kind of logarithmic", "tokens": [341, 857, 311, 8213, 293, 341, 857, 311, 15141, 365, 341, 857, 293, 341, 857, 311, 733, 295, 41473, 355, 13195], "temperature": 0.0, "avg_logprob": -0.16694493813089806, "compression_ratio": 1.7826086956521738, "no_speech_prob": 4.331376840127632e-05}, {"id": 606, "seek": 274544, "start": 2749.84, "end": 2758.08, "text": " and I specify how I think this system looks.", "tokens": [293, 286, 16500, 577, 286, 519, 341, 1185, 1542, 13], "temperature": 0.0, "avg_logprob": -0.16694493813089806, "compression_ratio": 1.7826086956521738, "no_speech_prob": 4.331376840127632e-05}, {"id": 607, "seek": 274544, "start": 2758.08, "end": 2760.32, "text": " And all the modeling tool does is it fills in parameters.", "tokens": [400, 439, 264, 15983, 2290, 775, 307, 309, 22498, 294, 9834, 13], "temperature": 0.0, "avg_logprob": -0.16694493813089806, "compression_ratio": 1.7826086956521738, "no_speech_prob": 4.331376840127632e-05}, {"id": 608, "seek": 274544, "start": 2760.32, "end": 2762.56, "text": " Okay, this is the slope of that linear bit.", "tokens": [1033, 11, 341, 307, 264, 13525, 295, 300, 8213, 857, 13], "temperature": 0.0, "avg_logprob": -0.16694493813089806, "compression_ratio": 1.7826086956521738, "no_speech_prob": 4.331376840127632e-05}, {"id": 609, "seek": 274544, "start": 2762.56, "end": 2764.7200000000003, "text": " This is the slope of that logarithmic bit.", "tokens": [639, 307, 264, 13525, 295, 300, 41473, 355, 13195, 857, 13], "temperature": 0.0, "avg_logprob": -0.16694493813089806, "compression_ratio": 1.7826086956521738, "no_speech_prob": 4.331376840127632e-05}, {"id": 610, "seek": 274544, "start": 2764.7200000000003, "end": 2766.8, "text": " This is how these two interact.", "tokens": [639, 307, 577, 613, 732, 4648, 13], "temperature": 0.0, "avg_logprob": -0.16694493813089806, "compression_ratio": 1.7826086956521738, "no_speech_prob": 4.331376840127632e-05}, {"id": 611, "seek": 274544, "start": 2766.8, "end": 2773.6, "text": " So things like GLM, very well known parametric tools.", "tokens": [407, 721, 411, 16225, 44, 11, 588, 731, 2570, 6220, 17475, 3873, 13], "temperature": 0.0, "avg_logprob": -0.16694493813089806, "compression_ratio": 1.7826086956521738, "no_speech_prob": 4.331376840127632e-05}, {"id": 612, "seek": 277360, "start": 2773.6, "end": 2777.36, "text": " Then there are these kind of non-parametric or semi-parametric models which are things", "tokens": [1396, 456, 366, 613, 733, 295, 2107, 12, 2181, 335, 17475, 420, 12909, 12, 2181, 335, 17475, 5245, 597, 366, 721], "temperature": 0.0, "avg_logprob": -0.16603951794760569, "compression_ratio": 1.6945606694560669, "no_speech_prob": 7.646362973900978e-06}, {"id": 613, "seek": 277360, "start": 2777.36, "end": 2779.04, "text": " where I don't do any of that.", "tokens": [689, 286, 500, 380, 360, 604, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.16603951794760569, "compression_ratio": 1.6945606694560669, "no_speech_prob": 7.646362973900978e-06}, {"id": 614, "seek": 277360, "start": 2779.04, "end": 2781.24, "text": " I just say, here's my data.", "tokens": [286, 445, 584, 11, 510, 311, 452, 1412, 13], "temperature": 0.0, "avg_logprob": -0.16603951794760569, "compression_ratio": 1.6945606694560669, "no_speech_prob": 7.646362973900978e-06}, {"id": 615, "seek": 277360, "start": 2781.24, "end": 2783.3199999999997, "text": " I don't know how it's related to each other.", "tokens": [286, 500, 380, 458, 577, 309, 311, 4077, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.16603951794760569, "compression_ratio": 1.6945606694560669, "no_speech_prob": 7.646362973900978e-06}, {"id": 616, "seek": 277360, "start": 2783.3199999999997, "end": 2784.52, "text": " Just build a model.", "tokens": [1449, 1322, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16603951794760569, "compression_ratio": 1.6945606694560669, "no_speech_prob": 7.646362973900978e-06}, {"id": 617, "seek": 277360, "start": 2784.52, "end": 2792.88, "text": " And so things like support vector machines, neural nets, random forests, decision trees", "tokens": [400, 370, 721, 411, 1406, 8062, 8379, 11, 18161, 36170, 11, 4974, 21700, 11, 3537, 5852], "temperature": 0.0, "avg_logprob": -0.16603951794760569, "compression_ratio": 1.6945606694560669, "no_speech_prob": 7.646362973900978e-06}, {"id": 618, "seek": 277360, "start": 2792.88, "end": 2798.16, "text": " all have that kind of flexibility.", "tokens": [439, 362, 300, 733, 295, 12635, 13], "temperature": 0.0, "avg_logprob": -0.16603951794760569, "compression_ratio": 1.6945606694560669, "no_speech_prob": 7.646362973900978e-06}, {"id": 619, "seek": 277360, "start": 2798.16, "end": 2801.72, "text": " Non-parametric models are not necessarily better than parametric models.", "tokens": [8774, 12, 2181, 335, 17475, 5245, 366, 406, 4725, 1101, 813, 6220, 17475, 5245, 13], "temperature": 0.0, "avg_logprob": -0.16603951794760569, "compression_ratio": 1.6945606694560669, "no_speech_prob": 7.646362973900978e-06}, {"id": 620, "seek": 280172, "start": 2801.72, "end": 2806.2799999999997, "text": " Think back to that example of the R package competition where really all I wanted was", "tokens": [6557, 646, 281, 300, 1365, 295, 264, 497, 7372, 6211, 689, 534, 439, 286, 1415, 390], "temperature": 0.0, "avg_logprob": -0.2861679395039876, "compression_ratio": 1.5625, "no_speech_prob": 4.425437055033399e-06}, {"id": 621, "seek": 280172, "start": 2806.2799999999997, "end": 2810.56, "text": " some weights to say how does this kind of this max column relate?", "tokens": [512, 17443, 281, 584, 577, 775, 341, 733, 295, 341, 11469, 7738, 10961, 30], "temperature": 0.0, "avg_logprob": -0.2861679395039876, "compression_ratio": 1.5625, "no_speech_prob": 4.425437055033399e-06}, {"id": 622, "seek": 280172, "start": 2810.56, "end": 2816.8799999999997, "text": " And if all you really wanted some weights, all you wanted some parameters and so GLM's", "tokens": [400, 498, 439, 291, 534, 1415, 512, 17443, 11, 439, 291, 1415, 512, 9834, 293, 370, 16225, 44, 311], "temperature": 0.0, "avg_logprob": -0.2861679395039876, "compression_ratio": 1.5625, "no_speech_prob": 4.425437055033399e-06}, {"id": 623, "seek": 280172, "start": 2816.8799999999997, "end": 2819.0, "text": " perfect.", "tokens": [2176, 13], "temperature": 0.0, "avg_logprob": -0.2861679395039876, "compression_ratio": 1.5625, "no_speech_prob": 4.425437055033399e-06}, {"id": 624, "seek": 280172, "start": 2819.0, "end": 2825.64, "text": " GLM's certainly can overfit, but there are ways of creating GLM's that don't.", "tokens": [16225, 44, 311, 3297, 393, 670, 6845, 11, 457, 456, 366, 2098, 295, 4084, 16225, 44, 311, 300, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.2861679395039876, "compression_ratio": 1.5625, "no_speech_prob": 4.425437055033399e-06}, {"id": 625, "seek": 282564, "start": 2825.64, "end": 2832.7599999999998, "text": " For example, you can use stepwise regression or the much more fancy modern version, you", "tokens": [1171, 1365, 11, 291, 393, 764, 1823, 3711, 24590, 420, 264, 709, 544, 10247, 4363, 3037, 11, 291], "temperature": 0.0, "avg_logprob": -0.16887067819570567, "compression_ratio": 1.5135135135135136, "no_speech_prob": 4.93683228341979e-06}, {"id": 626, "seek": 282564, "start": 2832.7599999999998, "end": 2841.04, "text": " can use GLM net, which is basically another tool for doing GLM's which doesn't overfit.", "tokens": [393, 764, 16225, 44, 2533, 11, 597, 307, 1936, 1071, 2290, 337, 884, 16225, 44, 311, 597, 1177, 380, 670, 6845, 13], "temperature": 0.0, "avg_logprob": -0.16887067819570567, "compression_ratio": 1.5135135135135136, "no_speech_prob": 4.93683228341979e-06}, {"id": 627, "seek": 282564, "start": 2841.04, "end": 2846.7599999999998, "text": " But anytime you don't really know what the model form is, this is where you'd use a non-parametric", "tokens": [583, 13038, 291, 500, 380, 534, 458, 437, 264, 2316, 1254, 307, 11, 341, 307, 689, 291, 1116, 764, 257, 2107, 12, 2181, 335, 17475], "temperature": 0.0, "avg_logprob": -0.16887067819570567, "compression_ratio": 1.5135135135135136, "no_speech_prob": 4.93683228341979e-06}, {"id": 628, "seek": 282564, "start": 2846.7599999999998, "end": 2847.7599999999998, "text": " tool.", "tokens": [2290, 13], "temperature": 0.0, "avg_logprob": -0.16887067819570567, "compression_ratio": 1.5135135135135136, "no_speech_prob": 4.93683228341979e-06}, {"id": 629, "seek": 284776, "start": 2847.76, "end": 2856.28, "text": " Random forests are great because they're super, super fast and extremely flexible and they", "tokens": [37603, 21700, 366, 869, 570, 436, 434, 1687, 11, 1687, 2370, 293, 4664, 11358, 293, 436], "temperature": 0.0, "avg_logprob": -0.24468052096483184, "compression_ratio": 1.691542288557214, "no_speech_prob": 8.80083280208055e-06}, {"id": 630, "seek": 284776, "start": 2856.28, "end": 2861.28, "text": " don't really have any parameters in the attitude so they're pretty hard to get it wrong.", "tokens": [500, 380, 534, 362, 604, 9834, 294, 264, 10157, 370, 436, 434, 1238, 1152, 281, 483, 309, 2085, 13], "temperature": 0.0, "avg_logprob": -0.24468052096483184, "compression_ratio": 1.691542288557214, "no_speech_prob": 8.80083280208055e-06}, {"id": 631, "seek": 284776, "start": 2861.28, "end": 2864.28, "text": " So let me show you how that works.", "tokens": [407, 718, 385, 855, 291, 577, 300, 1985, 13], "temperature": 0.0, "avg_logprob": -0.24468052096483184, "compression_ratio": 1.691542288557214, "no_speech_prob": 8.80083280208055e-06}, {"id": 632, "seek": 284776, "start": 2864.28, "end": 2870.8, "text": " A random forest is simply, in fact we shouldn't even use this term random forest because a", "tokens": [316, 4974, 6719, 307, 2935, 11, 294, 1186, 321, 4659, 380, 754, 764, 341, 1433, 4974, 6719, 570, 257], "temperature": 0.0, "avg_logprob": -0.24468052096483184, "compression_ratio": 1.691542288557214, "no_speech_prob": 8.80083280208055e-06}, {"id": 633, "seek": 284776, "start": 2870.8, "end": 2873.6000000000004, "text": " random forest is a trademark term.", "tokens": [4974, 6719, 307, 257, 31361, 1433, 13], "temperature": 0.0, "avg_logprob": -0.24468052096483184, "compression_ratio": 1.691542288557214, "no_speech_prob": 8.80083280208055e-06}, {"id": 634, "seek": 287360, "start": 2873.6, "end": 2883.3199999999997, "text": " So we will call it a ensemble of decision trees.", "tokens": [407, 321, 486, 818, 309, 257, 19492, 295, 3537, 5852, 13], "temperature": 0.0, "avg_logprob": -0.24317491790394724, "compression_ratio": 1.5365853658536586, "no_speech_prob": 3.426369949011132e-05}, {"id": 635, "seek": 287360, "start": 2883.3199999999997, "end": 2890.56, "text": " And in fact the trademark term random forest, I think that was 2001, that wasn't where this", "tokens": [400, 294, 1186, 264, 31361, 1433, 4974, 6719, 11, 286, 519, 300, 390, 16382, 11, 300, 2067, 380, 689, 341], "temperature": 0.0, "avg_logprob": -0.24317491790394724, "compression_ratio": 1.5365853658536586, "no_speech_prob": 3.426369949011132e-05}, {"id": 636, "seek": 287360, "start": 2890.56, "end": 2892.44, "text": " ensemble of decision trees was invented.", "tokens": [19492, 295, 3537, 5852, 390, 14479, 13], "temperature": 0.0, "avg_logprob": -0.24317491790394724, "compression_ratio": 1.5365853658536586, "no_speech_prob": 3.426369949011132e-05}, {"id": 637, "seek": 287360, "start": 2892.44, "end": 2894.6, "text": " It goes all the way back to 1995.", "tokens": [467, 1709, 439, 264, 636, 646, 281, 22601, 13], "temperature": 0.0, "avg_logprob": -0.24317491790394724, "compression_ratio": 1.5365853658536586, "no_speech_prob": 3.426369949011132e-05}, {"id": 638, "seek": 287360, "start": 2894.6, "end": 2900.6, "text": " In fact it was actually kind of independently developed by three different people in 95,", "tokens": [682, 1186, 309, 390, 767, 733, 295, 21761, 4743, 538, 1045, 819, 561, 294, 13420, 11], "temperature": 0.0, "avg_logprob": -0.24317491790394724, "compression_ratio": 1.5365853658536586, "no_speech_prob": 3.426369949011132e-05}, {"id": 639, "seek": 287360, "start": 2900.6, "end": 2902.8399999999997, "text": " 96 and 97.", "tokens": [24124, 293, 23399, 13], "temperature": 0.0, "avg_logprob": -0.24317491790394724, "compression_ratio": 1.5365853658536586, "no_speech_prob": 3.426369949011132e-05}, {"id": 640, "seek": 290284, "start": 2902.84, "end": 2908.2400000000002, "text": " The random forest implementation is really just one way of doing it.", "tokens": [440, 4974, 6719, 11420, 307, 534, 445, 472, 636, 295, 884, 309, 13], "temperature": 0.0, "avg_logprob": -0.1035348823271602, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.165887533716159e-05}, {"id": 641, "seek": 290284, "start": 2908.2400000000002, "end": 2916.96, "text": " It all rests on a really fascinating observation, which is that if you have a model that is", "tokens": [467, 439, 39755, 322, 257, 534, 10343, 14816, 11, 597, 307, 300, 498, 291, 362, 257, 2316, 300, 307], "temperature": 0.0, "avg_logprob": -0.1035348823271602, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.165887533716159e-05}, {"id": 642, "seek": 290284, "start": 2916.96, "end": 2925.36, "text": " really, really, really shit but it's not quite random, it's slightly better than nothing.", "tokens": [534, 11, 534, 11, 534, 4611, 457, 309, 311, 406, 1596, 4974, 11, 309, 311, 4748, 1101, 813, 1825, 13], "temperature": 0.0, "avg_logprob": -0.1035348823271602, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.165887533716159e-05}, {"id": 643, "seek": 290284, "start": 2925.36, "end": 2930.88, "text": " And if you've got 10,000 of these models that are all different to each other and they're", "tokens": [400, 498, 291, 600, 658, 1266, 11, 1360, 295, 613, 5245, 300, 366, 439, 819, 281, 1184, 661, 293, 436, 434], "temperature": 0.0, "avg_logprob": -0.1035348823271602, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.165887533716159e-05}, {"id": 644, "seek": 293088, "start": 2930.88, "end": 2935.96, "text": " all shit in different ways but they're all better than nothing, the average of those", "tokens": [439, 4611, 294, 819, 2098, 457, 436, 434, 439, 1101, 813, 1825, 11, 264, 4274, 295, 729], "temperature": 0.0, "avg_logprob": -0.13839588690241542, "compression_ratio": 1.7820512820512822, "no_speech_prob": 1.0289193596690893e-05}, {"id": 645, "seek": 293088, "start": 2935.96, "end": 2943.08, "text": " 10,000 models will actually be fantastically powerful as a model of its own.", "tokens": [1266, 11, 1360, 5245, 486, 767, 312, 4115, 22808, 4005, 382, 257, 2316, 295, 1080, 1065, 13], "temperature": 0.0, "avg_logprob": -0.13839588690241542, "compression_ratio": 1.7820512820512822, "no_speech_prob": 1.0289193596690893e-05}, {"id": 646, "seek": 293088, "start": 2943.08, "end": 2948.36, "text": " So this is the wisdom of crowds or ensemble learning techniques.", "tokens": [407, 341, 307, 264, 10712, 295, 26070, 420, 19492, 2539, 7512, 13], "temperature": 0.0, "avg_logprob": -0.13839588690241542, "compression_ratio": 1.7820512820512822, "no_speech_prob": 1.0289193596690893e-05}, {"id": 647, "seek": 293088, "start": 2948.36, "end": 2950.32, "text": " You can kind of see why, right?", "tokens": [509, 393, 733, 295, 536, 983, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13839588690241542, "compression_ratio": 1.7820512820512822, "no_speech_prob": 1.0289193596690893e-05}, {"id": 648, "seek": 293088, "start": 2950.32, "end": 2954.6, "text": " Because if out of these 10,000 models they're all kind of crap in different ways, they're", "tokens": [1436, 498, 484, 295, 613, 1266, 11, 1360, 5245, 436, 434, 439, 733, 295, 12426, 294, 819, 2098, 11, 436, 434], "temperature": 0.0, "avg_logprob": -0.13839588690241542, "compression_ratio": 1.7820512820512822, "no_speech_prob": 1.0289193596690893e-05}, {"id": 649, "seek": 293088, "start": 2954.6, "end": 2955.6, "text": " all a bit random, right?", "tokens": [439, 257, 857, 4974, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13839588690241542, "compression_ratio": 1.7820512820512822, "no_speech_prob": 1.0289193596690893e-05}, {"id": 650, "seek": 293088, "start": 2955.6, "end": 2957.6, "text": " They're all a bit less better than nothing.", "tokens": [814, 434, 439, 257, 857, 1570, 1101, 813, 1825, 13], "temperature": 0.0, "avg_logprob": -0.13839588690241542, "compression_ratio": 1.7820512820512822, "no_speech_prob": 1.0289193596690893e-05}, {"id": 651, "seek": 295760, "start": 2957.6, "end": 2964.36, "text": " 9,999 of them might basically be useless but one of them just happened upon the true structure", "tokens": [1722, 11, 49017, 295, 552, 1062, 1936, 312, 14115, 457, 472, 295, 552, 445, 2011, 3564, 264, 2074, 3877], "temperature": 0.0, "avg_logprob": -0.1113421776715447, "compression_ratio": 1.7352941176470589, "no_speech_prob": 1.8631077182362787e-05}, {"id": 652, "seek": 295760, "start": 2964.36, "end": 2965.68, "text": " of the data.", "tokens": [295, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1113421776715447, "compression_ratio": 1.7352941176470589, "no_speech_prob": 1.8631077182362787e-05}, {"id": 653, "seek": 295760, "start": 2965.68, "end": 2971.7999999999997, "text": " So the other 9,999 will kind of average out if they're unbiased, not correlated with each", "tokens": [407, 264, 661, 1722, 11, 49017, 486, 733, 295, 4274, 484, 498, 436, 434, 517, 5614, 1937, 11, 406, 38574, 365, 1184], "temperature": 0.0, "avg_logprob": -0.1113421776715447, "compression_ratio": 1.7352941176470589, "no_speech_prob": 1.8631077182362787e-05}, {"id": 654, "seek": 295760, "start": 2971.7999999999997, "end": 2976.52, "text": " other, they'll all average out to whatever the average of the data is.", "tokens": [661, 11, 436, 603, 439, 4274, 484, 281, 2035, 264, 4274, 295, 264, 1412, 307, 13], "temperature": 0.0, "avg_logprob": -0.1113421776715447, "compression_ratio": 1.7352941176470589, "no_speech_prob": 1.8631077182362787e-05}, {"id": 655, "seek": 295760, "start": 2976.52, "end": 2982.64, "text": " So any difference in the predictions of this ensemble will all come down to that one model", "tokens": [407, 604, 2649, 294, 264, 21264, 295, 341, 19492, 486, 439, 808, 760, 281, 300, 472, 2316], "temperature": 0.0, "avg_logprob": -0.1113421776715447, "compression_ratio": 1.7352941176470589, "no_speech_prob": 1.8631077182362787e-05}, {"id": 656, "seek": 295760, "start": 2982.64, "end": 2985.48, "text": " which happened to have actually figured it out right.", "tokens": [597, 2011, 281, 362, 767, 8932, 309, 484, 558, 13], "temperature": 0.0, "avg_logprob": -0.1113421776715447, "compression_ratio": 1.7352941176470589, "no_speech_prob": 1.8631077182362787e-05}, {"id": 657, "seek": 298548, "start": 2985.48, "end": 2988.32, "text": " Now that's an extreme version, right?", "tokens": [823, 300, 311, 364, 8084, 3037, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13964803828749545, "compression_ratio": 1.591743119266055, "no_speech_prob": 4.289272510504816e-06}, {"id": 658, "seek": 298548, "start": 2988.32, "end": 2992.72, "text": " But that's basically the concept behind all these ensemble techniques.", "tokens": [583, 300, 311, 1936, 264, 3410, 2261, 439, 613, 19492, 7512, 13], "temperature": 0.0, "avg_logprob": -0.13964803828749545, "compression_ratio": 1.591743119266055, "no_speech_prob": 4.289272510504816e-06}, {"id": 659, "seek": 298548, "start": 2992.72, "end": 2997.3, "text": " And if you want to invent your own ensemble technique, all you have to do is come up with", "tokens": [400, 498, 291, 528, 281, 7962, 428, 1065, 19492, 6532, 11, 439, 291, 362, 281, 360, 307, 808, 493, 365], "temperature": 0.0, "avg_logprob": -0.13964803828749545, "compression_ratio": 1.591743119266055, "no_speech_prob": 4.289272510504816e-06}, {"id": 660, "seek": 298548, "start": 2997.3, "end": 3004.6, "text": " some learner, some underlying model which is, which you can randomize in some way and", "tokens": [512, 33347, 11, 512, 14217, 2316, 597, 307, 11, 597, 291, 393, 4974, 1125, 294, 512, 636, 293], "temperature": 0.0, "avg_logprob": -0.13964803828749545, "compression_ratio": 1.591743119266055, "no_speech_prob": 4.289272510504816e-06}, {"id": 661, "seek": 298548, "start": 3004.6, "end": 3008.56, "text": " each one will be a bit different and you run it lots of times.", "tokens": [1184, 472, 486, 312, 257, 857, 819, 293, 291, 1190, 309, 3195, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.13964803828749545, "compression_ratio": 1.591743119266055, "no_speech_prob": 4.289272510504816e-06}, {"id": 662, "seek": 300856, "start": 3008.56, "end": 3021.32, "text": " And generally speaking this whole approach we call random subspace.", "tokens": [400, 5101, 4124, 341, 1379, 3109, 321, 818, 4974, 2090, 17940, 13], "temperature": 0.0, "avg_logprob": -0.15468704889691065, "compression_ratio": 1.4634146341463414, "no_speech_prob": 3.288677135060425e-06}, {"id": 663, "seek": 300856, "start": 3021.32, "end": 3026.72, "text": " So random subspace techniques, let me show you how unbelievably easy this is.", "tokens": [407, 4974, 2090, 17940, 7512, 11, 718, 385, 855, 291, 577, 43593, 1858, 341, 307, 13], "temperature": 0.0, "avg_logprob": -0.15468704889691065, "compression_ratio": 1.4634146341463414, "no_speech_prob": 3.288677135060425e-06}, {"id": 664, "seek": 300856, "start": 3026.72, "end": 3031.72, "text": " Take any model, any kind of modeling algorithm you like.", "tokens": [3664, 604, 2316, 11, 604, 733, 295, 15983, 9284, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.15468704889691065, "compression_ratio": 1.4634146341463414, "no_speech_prob": 3.288677135060425e-06}, {"id": 665, "seek": 300856, "start": 3031.72, "end": 3034.72, "text": " Here's our data.", "tokens": [1692, 311, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15468704889691065, "compression_ratio": 1.4634146341463414, "no_speech_prob": 3.288677135060425e-06}, {"id": 666, "seek": 300856, "start": 3034.72, "end": 3036.92, "text": " Here's all the rows.", "tokens": [1692, 311, 439, 264, 13241, 13], "temperature": 0.0, "avg_logprob": -0.15468704889691065, "compression_ratio": 1.4634146341463414, "no_speech_prob": 3.288677135060425e-06}, {"id": 667, "seek": 303692, "start": 3036.92, "end": 3038.92, "text": " Here's all the columns.", "tokens": [1692, 311, 439, 264, 13766, 13], "temperature": 0.0, "avg_logprob": -0.1698999243267512, "compression_ratio": 1.546875, "no_speech_prob": 2.1782025214633904e-05}, {"id": 668, "seek": 303692, "start": 3038.92, "end": 3050.16, "text": " Okay, I'm now going to create a random subspace.", "tokens": [1033, 11, 286, 478, 586, 516, 281, 1884, 257, 4974, 2090, 17940, 13], "temperature": 0.0, "avg_logprob": -0.1698999243267512, "compression_ratio": 1.546875, "no_speech_prob": 2.1782025214633904e-05}, {"id": 669, "seek": 303692, "start": 3050.16, "end": 3052.56, "text": " Some of the columns, some of the rows.", "tokens": [2188, 295, 264, 13766, 11, 512, 295, 264, 13241, 13], "temperature": 0.0, "avg_logprob": -0.1698999243267512, "compression_ratio": 1.546875, "no_speech_prob": 2.1782025214633904e-05}, {"id": 670, "seek": 303692, "start": 3052.56, "end": 3059.76, "text": " Okay, so let's now build a model using that subset of rows and that subset of columns.", "tokens": [1033, 11, 370, 718, 311, 586, 1322, 257, 2316, 1228, 300, 25993, 295, 13241, 293, 300, 25993, 295, 13766, 13], "temperature": 0.0, "avg_logprob": -0.1698999243267512, "compression_ratio": 1.546875, "no_speech_prob": 2.1782025214633904e-05}, {"id": 671, "seek": 305976, "start": 3059.76, "end": 3067.84, "text": " It's not going to be as perfect at recognizing the training data as using the full lot, but", "tokens": [467, 311, 406, 516, 281, 312, 382, 2176, 412, 18538, 264, 3097, 1412, 382, 1228, 264, 1577, 688, 11, 457], "temperature": 0.0, "avg_logprob": -0.27809388296944754, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.586600385257043e-05}, {"id": 672, "seek": 305976, "start": 3067.84, "end": 3070.0800000000004, "text": " you know, it's one way of building a model.", "tokens": [291, 458, 11, 309, 311, 472, 636, 295, 2390, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.27809388296944754, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.586600385257043e-05}, {"id": 673, "seek": 305976, "start": 3070.0800000000004, "end": 3073.6000000000004, "text": " Let's say I build a second model.", "tokens": [961, 311, 584, 286, 1322, 257, 1150, 2316, 13], "temperature": 0.0, "avg_logprob": -0.27809388296944754, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.586600385257043e-05}, {"id": 674, "seek": 305976, "start": 3073.6000000000004, "end": 3079.2400000000002, "text": " This time I'll use this subspace, a different set of rows and a different set of columns.", "tokens": [639, 565, 286, 603, 764, 341, 2090, 17940, 11, 257, 819, 992, 295, 13241, 293, 257, 819, 992, 295, 13766, 13], "temperature": 0.0, "avg_logprob": -0.27809388296944754, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.586600385257043e-05}, {"id": 675, "seek": 305976, "start": 3079.2400000000002, "end": 3082.2400000000002, "text": " No, absolutely not.", "tokens": [883, 11, 3122, 406, 13], "temperature": 0.0, "avg_logprob": -0.27809388296944754, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.586600385257043e-05}, {"id": 676, "seek": 305976, "start": 3082.2400000000002, "end": 3087.32, "text": " But I didn't want to draw 4,000 lines, so let's pretend.", "tokens": [583, 286, 994, 380, 528, 281, 2642, 1017, 11, 1360, 3876, 11, 370, 718, 311, 11865, 13], "temperature": 0.0, "avg_logprob": -0.27809388296944754, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.586600385257043e-05}, {"id": 677, "seek": 308732, "start": 3087.32, "end": 3094.1200000000003, "text": " Yeah, so in fact what I'm really doing each time here is I'm pulling out a bunch of random", "tokens": [865, 11, 370, 294, 1186, 437, 286, 478, 534, 884, 1184, 565, 510, 307, 286, 478, 8407, 484, 257, 3840, 295, 4974], "temperature": 0.0, "avg_logprob": -0.20898637231790795, "compression_ratio": 1.6608695652173913, "no_speech_prob": 5.5618274927837774e-05}, {"id": 678, "seek": 308732, "start": 3094.1200000000003, "end": 3098.44, "text": " rows and a bunch of random columns.", "tokens": [13241, 293, 257, 3840, 295, 4974, 13766, 13], "temperature": 0.0, "avg_logprob": -0.20898637231790795, "compression_ratio": 1.6608695652173913, "no_speech_prob": 5.5618274927837774e-05}, {"id": 679, "seek": 308732, "start": 3098.44, "end": 3101.8, "text": " You have a random something that goes both ways.", "tokens": [509, 362, 257, 4974, 746, 300, 1709, 1293, 2098, 13], "temperature": 0.0, "avg_logprob": -0.20898637231790795, "compression_ratio": 1.6608695652173913, "no_speech_prob": 5.5618274927837774e-05}, {"id": 680, "seek": 308732, "start": 3101.8, "end": 3102.8, "text": " Correct.", "tokens": [12753, 13], "temperature": 0.0, "avg_logprob": -0.20898637231790795, "compression_ratio": 1.6608695652173913, "no_speech_prob": 5.5618274927837774e-05}, {"id": 681, "seek": 308732, "start": 3102.8, "end": 3105.0, "text": " And this is a random subspace.", "tokens": [400, 341, 307, 257, 4974, 2090, 17940, 13], "temperature": 0.0, "avg_logprob": -0.20898637231790795, "compression_ratio": 1.6608695652173913, "no_speech_prob": 5.5618274927837774e-05}, {"id": 682, "seek": 308732, "start": 3105.0, "end": 3108.5, "text": " It's just one way of creating a random subspace, but it's a nice easy one.", "tokens": [467, 311, 445, 472, 636, 295, 4084, 257, 4974, 2090, 17940, 11, 457, 309, 311, 257, 1481, 1858, 472, 13], "temperature": 0.0, "avg_logprob": -0.20898637231790795, "compression_ratio": 1.6608695652173913, "no_speech_prob": 5.5618274927837774e-05}, {"id": 683, "seek": 308732, "start": 3108.5, "end": 3114.32, "text": " And because I didn't do very well at linear algebra, in fact I'm just a philosophy graduate", "tokens": [400, 570, 286, 994, 380, 360, 588, 731, 412, 8213, 21989, 11, 294, 1186, 286, 478, 445, 257, 10675, 8080], "temperature": 0.0, "avg_logprob": -0.20898637231790795, "compression_ratio": 1.6608695652173913, "no_speech_prob": 5.5618274927837774e-05}, {"id": 684, "seek": 311432, "start": 3114.32, "end": 3119.04, "text": " and I don't know any linear algebra, I don't know what subspace means well enough to do", "tokens": [293, 286, 500, 380, 458, 604, 8213, 21989, 11, 286, 500, 380, 458, 437, 2090, 17940, 1355, 731, 1547, 281, 360], "temperature": 0.0, "avg_logprob": -0.18383517123685025, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.8856353310402483e-05}, {"id": 685, "seek": 311432, "start": 3119.04, "end": 3121.48, "text": " it properly, but this certainly works.", "tokens": [309, 6108, 11, 457, 341, 3297, 1985, 13], "temperature": 0.0, "avg_logprob": -0.18383517123685025, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.8856353310402483e-05}, {"id": 686, "seek": 311432, "start": 3121.48, "end": 3123.28, "text": " And this is all decision trees do.", "tokens": [400, 341, 307, 439, 3537, 5852, 360, 13], "temperature": 0.0, "avg_logprob": -0.18383517123685025, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.8856353310402483e-05}, {"id": 687, "seek": 311432, "start": 3123.28, "end": 3127.56, "text": " So now imagine that we're going to do this and for each one of these different random", "tokens": [407, 586, 3811, 300, 321, 434, 516, 281, 360, 341, 293, 337, 1184, 472, 295, 613, 819, 4974], "temperature": 0.0, "avg_logprob": -0.18383517123685025, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.8856353310402483e-05}, {"id": 688, "seek": 311432, "start": 3127.56, "end": 3130.6800000000003, "text": " subspaces we're going to build a decision tree.", "tokens": [2090, 79, 2116, 321, 434, 516, 281, 1322, 257, 3537, 4230, 13], "temperature": 0.0, "avg_logprob": -0.18383517123685025, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.8856353310402483e-05}, {"id": 689, "seek": 311432, "start": 3130.6800000000003, "end": 3132.8, "text": " How do we build a decision tree?", "tokens": [1012, 360, 321, 1322, 257, 3537, 4230, 30], "temperature": 0.0, "avg_logprob": -0.18383517123685025, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.8856353310402483e-05}, {"id": 690, "seek": 311432, "start": 3132.8, "end": 3133.8, "text": " Easy.", "tokens": [16002, 13], "temperature": 0.0, "avg_logprob": -0.18383517123685025, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.8856353310402483e-05}, {"id": 691, "seek": 311432, "start": 3133.8, "end": 3137.44, "text": " Let's create some data.", "tokens": [961, 311, 1884, 512, 1412, 13], "temperature": 0.0, "avg_logprob": -0.18383517123685025, "compression_ratio": 1.6886792452830188, "no_speech_prob": 2.8856353310402483e-05}, {"id": 692, "seek": 313744, "start": 3137.44, "end": 3153.68, "text": " So let's say we've got age, sex, is smoker, and lung capacity.", "tokens": [407, 718, 311, 584, 321, 600, 658, 3205, 11, 3260, 11, 307, 899, 16722, 11, 293, 16730, 6042, 13], "temperature": 0.0, "avg_logprob": -0.3026219050089518, "compression_ratio": 1.3834586466165413, "no_speech_prob": 4.985334453522228e-05}, {"id": 693, "seek": 313744, "start": 3153.68, "end": 3156.12, "text": " And we're trying to predict people's lung capacity.", "tokens": [400, 321, 434, 1382, 281, 6069, 561, 311, 16730, 6042, 13], "temperature": 0.0, "avg_logprob": -0.3026219050089518, "compression_ratio": 1.3834586466165413, "no_speech_prob": 4.985334453522228e-05}, {"id": 694, "seek": 313744, "start": 3156.12, "end": 3160.08, "text": " So we've got a whole bunch of data there.", "tokens": [407, 321, 600, 658, 257, 1379, 3840, 295, 1412, 456, 13], "temperature": 0.0, "avg_logprob": -0.3026219050089518, "compression_ratio": 1.3834586466165413, "no_speech_prob": 4.985334453522228e-05}, {"id": 695, "seek": 313744, "start": 3160.08, "end": 3164.48, "text": " Dirty, male, yes, whatever.", "tokens": [413, 9340, 11, 7133, 11, 2086, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.3026219050089518, "compression_ratio": 1.3834586466165413, "no_speech_prob": 4.985334453522228e-05}, {"id": 696, "seek": 316448, "start": 3164.48, "end": 3174.8, "text": " So to build a decision tree, let's assume that this is the particular subset of columns", "tokens": [407, 281, 1322, 257, 3537, 4230, 11, 718, 311, 6552, 300, 341, 307, 264, 1729, 25993, 295, 13766], "temperature": 0.0, "avg_logprob": -0.1684498411885808, "compression_ratio": 1.7880434782608696, "no_speech_prob": 2.1781801478937268e-05}, {"id": 697, "seek": 316448, "start": 3174.8, "end": 3176.96, "text": " and rows in a random subspace.", "tokens": [293, 13241, 294, 257, 4974, 2090, 17940, 13], "temperature": 0.0, "avg_logprob": -0.1684498411885808, "compression_ratio": 1.7880434782608696, "no_speech_prob": 2.1781801478937268e-05}, {"id": 698, "seek": 316448, "start": 3176.96, "end": 3178.46, "text": " So let's build a decision tree.", "tokens": [407, 718, 311, 1322, 257, 3537, 4230, 13], "temperature": 0.0, "avg_logprob": -0.1684498411885808, "compression_ratio": 1.7880434782608696, "no_speech_prob": 2.1781801478937268e-05}, {"id": 699, "seek": 316448, "start": 3178.46, "end": 3188.6, "text": " So to build a decision tree, what I do is I say, okay, on which variable, on which predictor,", "tokens": [407, 281, 1322, 257, 3537, 4230, 11, 437, 286, 360, 307, 286, 584, 11, 1392, 11, 322, 597, 7006, 11, 322, 597, 6069, 284, 11], "temperature": 0.0, "avg_logprob": -0.1684498411885808, "compression_ratio": 1.7880434782608696, "no_speech_prob": 2.1781801478937268e-05}, {"id": 700, "seek": 316448, "start": 3188.6, "end": 3193.96, "text": " and at which point of that predictor can I do a single split which makes the biggest", "tokens": [293, 412, 597, 935, 295, 300, 6069, 284, 393, 286, 360, 257, 2167, 7472, 597, 1669, 264, 3880], "temperature": 0.0, "avg_logprob": -0.1684498411885808, "compression_ratio": 1.7880434782608696, "no_speech_prob": 2.1781801478937268e-05}, {"id": 701, "seek": 319396, "start": 3193.96, "end": 3197.4, "text": " difference possible in my dependent variable?", "tokens": [2649, 1944, 294, 452, 12334, 7006, 30], "temperature": 0.0, "avg_logprob": -0.12187998781922044, "compression_ratio": 1.696078431372549, "no_speech_prob": 7.889128028182313e-06}, {"id": 702, "seek": 319396, "start": 3197.4, "end": 3209.36, "text": " So it might turn out that if I looked at his smoker, yes and no, that the average lung", "tokens": [407, 309, 1062, 1261, 484, 300, 498, 286, 2956, 412, 702, 899, 16722, 11, 2086, 293, 572, 11, 300, 264, 4274, 16730], "temperature": 0.0, "avg_logprob": -0.12187998781922044, "compression_ratio": 1.696078431372549, "no_speech_prob": 7.889128028182313e-06}, {"id": 703, "seek": 319396, "start": 3209.36, "end": 3215.2400000000002, "text": " capacity for all of the smokers might be 30 and the average for all of the non-smokers", "tokens": [6042, 337, 439, 295, 264, 32073, 433, 1062, 312, 2217, 293, 264, 4274, 337, 439, 295, 264, 2107, 12, 10817, 453, 433], "temperature": 0.0, "avg_logprob": -0.12187998781922044, "compression_ratio": 1.696078431372549, "no_speech_prob": 7.889128028182313e-06}, {"id": 704, "seek": 319396, "start": 3215.2400000000002, "end": 3217.12, "text": " might be 70.", "tokens": [1062, 312, 5285, 13], "temperature": 0.0, "avg_logprob": -0.12187998781922044, "compression_ratio": 1.696078431372549, "no_speech_prob": 7.889128028182313e-06}, {"id": 705, "seek": 319396, "start": 3217.12, "end": 3220.52, "text": " So literally all I've done is I've just gone through each of these and calculated the average", "tokens": [407, 3736, 439, 286, 600, 1096, 307, 286, 600, 445, 2780, 807, 1184, 295, 613, 293, 15598, 264, 4274], "temperature": 0.0, "avg_logprob": -0.12187998781922044, "compression_ratio": 1.696078431372549, "no_speech_prob": 7.889128028182313e-06}, {"id": 706, "seek": 319396, "start": 3220.52, "end": 3222.6, "text": " for the two groups.", "tokens": [337, 264, 732, 3935, 13], "temperature": 0.0, "avg_logprob": -0.12187998781922044, "compression_ratio": 1.696078431372549, "no_speech_prob": 7.889128028182313e-06}, {"id": 707, "seek": 322260, "start": 3222.6, "end": 3228.24, "text": " And I found the one split that makes that as big a difference as possible.", "tokens": [400, 286, 1352, 264, 472, 7472, 300, 1669, 300, 382, 955, 257, 2649, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.1070225219096034, "compression_ratio": 1.7481481481481482, "no_speech_prob": 8.80100742506329e-06}, {"id": 708, "seek": 322260, "start": 3228.24, "end": 3229.24, "text": " And then I keep doing that.", "tokens": [400, 550, 286, 1066, 884, 300, 13], "temperature": 0.0, "avg_logprob": -0.1070225219096034, "compression_ratio": 1.7481481481481482, "no_speech_prob": 8.80100742506329e-06}, {"id": 709, "seek": 322260, "start": 3229.24, "end": 3237.0, "text": " So in those people that are non-smokers, I now, interestingly, with the random forest", "tokens": [407, 294, 729, 561, 300, 366, 2107, 12, 10817, 453, 433, 11, 286, 586, 11, 25873, 11, 365, 264, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.1070225219096034, "compression_ratio": 1.7481481481481482, "no_speech_prob": 8.80100742506329e-06}, {"id": 710, "seek": 322260, "start": 3237.0, "end": 3242.56, "text": " or these decision tree ensemble algorithms, generally speaking, at each point, I select", "tokens": [420, 613, 3537, 4230, 19492, 14642, 11, 5101, 4124, 11, 412, 1184, 935, 11, 286, 3048], "temperature": 0.0, "avg_logprob": -0.1070225219096034, "compression_ratio": 1.7481481481481482, "no_speech_prob": 8.80100742506329e-06}, {"id": 711, "seek": 322260, "start": 3242.56, "end": 3244.48, "text": " a different group of columns.", "tokens": [257, 819, 1594, 295, 13766, 13], "temperature": 0.0, "avg_logprob": -0.1070225219096034, "compression_ratio": 1.7481481481481482, "no_speech_prob": 8.80100742506329e-06}, {"id": 712, "seek": 322260, "start": 3244.48, "end": 3247.56, "text": " So I randomly select a new group of columns, but I'm going to use the same rows.", "tokens": [407, 286, 16979, 3048, 257, 777, 1594, 295, 13766, 11, 457, 286, 478, 516, 281, 764, 264, 912, 13241, 13], "temperature": 0.0, "avg_logprob": -0.1070225219096034, "compression_ratio": 1.7481481481481482, "no_speech_prob": 8.80100742506329e-06}, {"id": 713, "seek": 322260, "start": 3247.56, "end": 3252.52, "text": " I obviously have to use the same rows because I'm kind of taking them down the tree.", "tokens": [286, 2745, 362, 281, 764, 264, 912, 13241, 570, 286, 478, 733, 295, 1940, 552, 760, 264, 4230, 13], "temperature": 0.0, "avg_logprob": -0.1070225219096034, "compression_ratio": 1.7481481481481482, "no_speech_prob": 8.80100742506329e-06}, {"id": 714, "seek": 325252, "start": 3252.52, "end": 3257.56, "text": " So now it turns out that if we look at age amongst the people that are non-smokers, if", "tokens": [407, 586, 309, 4523, 484, 300, 498, 321, 574, 412, 3205, 12918, 264, 561, 300, 366, 2107, 12, 10817, 453, 433, 11, 498], "temperature": 0.0, "avg_logprob": -0.1818205638996606, "compression_ratio": 1.611353711790393, "no_speech_prob": 1.2606264135683887e-05}, {"id": 715, "seek": 325252, "start": 3257.56, "end": 3263.36, "text": " you're less than 18 versus greater than 18 is the number one biggest thing in this random", "tokens": [291, 434, 1570, 813, 2443, 5717, 5044, 813, 2443, 307, 264, 1230, 472, 3880, 551, 294, 341, 4974], "temperature": 0.0, "avg_logprob": -0.1818205638996606, "compression_ratio": 1.611353711790393, "no_speech_prob": 1.2606264135683887e-05}, {"id": 716, "seek": 325252, "start": 3263.36, "end": 3264.84, "text": " subspace that makes the difference.", "tokens": [2090, 17940, 300, 1669, 264, 2649, 13], "temperature": 0.0, "avg_logprob": -0.1818205638996606, "compression_ratio": 1.611353711790393, "no_speech_prob": 1.2606264135683887e-05}, {"id": 717, "seek": 325252, "start": 3264.84, "end": 3268.32, "text": " And that's like 50 and that's like 80.", "tokens": [400, 300, 311, 411, 2625, 293, 300, 311, 411, 4688, 13], "temperature": 0.0, "avg_logprob": -0.1818205638996606, "compression_ratio": 1.611353711790393, "no_speech_prob": 1.2606264135683887e-05}, {"id": 718, "seek": 325252, "start": 3268.32, "end": 3271.2, "text": " And so this is how I create a decision tree.", "tokens": [400, 370, 341, 307, 577, 286, 1884, 257, 3537, 4230, 13], "temperature": 0.0, "avg_logprob": -0.1818205638996606, "compression_ratio": 1.611353711790393, "no_speech_prob": 1.2606264135683887e-05}, {"id": 719, "seek": 325252, "start": 3271.2, "end": 3272.2, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1818205638996606, "compression_ratio": 1.611353711790393, "no_speech_prob": 1.2606264135683887e-05}, {"id": 720, "seek": 325252, "start": 3272.2, "end": 3280.08, "text": " So at each point, I've taken a different random subset of columns.", "tokens": [407, 412, 1184, 935, 11, 286, 600, 2726, 257, 819, 4974, 25993, 295, 13766, 13], "temperature": 0.0, "avg_logprob": -0.1818205638996606, "compression_ratio": 1.611353711790393, "no_speech_prob": 1.2606264135683887e-05}, {"id": 721, "seek": 328008, "start": 3280.08, "end": 3283.72, "text": " For the whole tree, I've used the same random subset of rows.", "tokens": [1171, 264, 1379, 4230, 11, 286, 600, 1143, 264, 912, 4974, 25993, 295, 13241, 13], "temperature": 0.0, "avg_logprob": -0.154550855809992, "compression_ratio": 1.6359223300970873, "no_speech_prob": 6.14404689258663e-06}, {"id": 722, "seek": 328008, "start": 3283.72, "end": 3291.08, "text": " And at the end of that, I keep going until every one of my leaves either has only one", "tokens": [400, 412, 264, 917, 295, 300, 11, 286, 1066, 516, 1826, 633, 472, 295, 452, 5510, 2139, 575, 787, 472], "temperature": 0.0, "avg_logprob": -0.154550855809992, "compression_ratio": 1.6359223300970873, "no_speech_prob": 6.14404689258663e-06}, {"id": 723, "seek": 328008, "start": 3291.08, "end": 3296.2799999999997, "text": " or two data points left or all of the data points at that leaf all have exactly the same", "tokens": [420, 732, 1412, 2793, 1411, 420, 439, 295, 264, 1412, 2793, 412, 300, 10871, 439, 362, 2293, 264, 912], "temperature": 0.0, "avg_logprob": -0.154550855809992, "compression_ratio": 1.6359223300970873, "no_speech_prob": 6.14404689258663e-06}, {"id": 724, "seek": 328008, "start": 3296.2799999999997, "end": 3301.16, "text": " outcome, the same lung capacity, for example.", "tokens": [9700, 11, 264, 912, 16730, 6042, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.154550855809992, "compression_ratio": 1.6359223300970873, "no_speech_prob": 6.14404689258663e-06}, {"id": 725, "seek": 328008, "start": 3301.16, "end": 3303.88, "text": " And at that point, I finished making my decision tree.", "tokens": [400, 412, 300, 935, 11, 286, 4335, 1455, 452, 3537, 4230, 13], "temperature": 0.0, "avg_logprob": -0.154550855809992, "compression_ratio": 1.6359223300970873, "no_speech_prob": 6.14404689258663e-06}, {"id": 726, "seek": 330388, "start": 3303.88, "end": 3311.12, "text": " So now I put that aside and I say, okay, that is decision tree number one.", "tokens": [407, 586, 286, 829, 300, 7359, 293, 286, 584, 11, 1392, 11, 300, 307, 3537, 4230, 1230, 472, 13], "temperature": 0.0, "avg_logprob": -0.1367088881405917, "compression_ratio": 1.7966101694915255, "no_speech_prob": 9.080376003112178e-06}, {"id": 727, "seek": 330388, "start": 3311.12, "end": 3318.6400000000003, "text": " Put that aside and now go back and take a different set of rows and repeat the whole", "tokens": [4935, 300, 7359, 293, 586, 352, 646, 293, 747, 257, 819, 992, 295, 13241, 293, 7149, 264, 1379], "temperature": 0.0, "avg_logprob": -0.1367088881405917, "compression_ratio": 1.7966101694915255, "no_speech_prob": 9.080376003112178e-06}, {"id": 728, "seek": 330388, "start": 3318.6400000000003, "end": 3320.48, "text": " process.", "tokens": [1399, 13], "temperature": 0.0, "avg_logprob": -0.1367088881405917, "compression_ratio": 1.7966101694915255, "no_speech_prob": 9.080376003112178e-06}, {"id": 729, "seek": 330388, "start": 3320.48, "end": 3323.84, "text": " And that gives me decision tree number two.", "tokens": [400, 300, 2709, 385, 3537, 4230, 1230, 732, 13], "temperature": 0.0, "avg_logprob": -0.1367088881405917, "compression_ratio": 1.7966101694915255, "no_speech_prob": 9.080376003112178e-06}, {"id": 730, "seek": 330388, "start": 3323.84, "end": 3327.48, "text": " And I do that a thousand times, whatever.", "tokens": [400, 286, 360, 300, 257, 4714, 1413, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.1367088881405917, "compression_ratio": 1.7966101694915255, "no_speech_prob": 9.080376003112178e-06}, {"id": 731, "seek": 330388, "start": 3327.48, "end": 3331.36, "text": " And at the end of that, I've now got a thousand decision trees.", "tokens": [400, 412, 264, 917, 295, 300, 11, 286, 600, 586, 658, 257, 4714, 3537, 5852, 13], "temperature": 0.0, "avg_logprob": -0.1367088881405917, "compression_ratio": 1.7966101694915255, "no_speech_prob": 9.080376003112178e-06}, {"id": 732, "seek": 333136, "start": 3331.36, "end": 3336.04, "text": " And for each thing I want to predict, I then stick that thing I want to predict down every", "tokens": [400, 337, 1184, 551, 286, 528, 281, 6069, 11, 286, 550, 2897, 300, 551, 286, 528, 281, 6069, 760, 633], "temperature": 0.0, "avg_logprob": -0.14574849289075464, "compression_ratio": 1.7923728813559323, "no_speech_prob": 2.3922615582705475e-05}, {"id": 733, "seek": 333136, "start": 3336.04, "end": 3337.6800000000003, "text": " one of these decision trees.", "tokens": [472, 295, 613, 3537, 5852, 13], "temperature": 0.0, "avg_logprob": -0.14574849289075464, "compression_ratio": 1.7923728813559323, "no_speech_prob": 2.3922615582705475e-05}, {"id": 734, "seek": 333136, "start": 3337.6800000000003, "end": 3343.08, "text": " So the first thing I'm trying to predict might be a non-smoker who's 16 years old, blah,", "tokens": [407, 264, 700, 551, 286, 478, 1382, 281, 6069, 1062, 312, 257, 2107, 12, 10817, 16722, 567, 311, 3165, 924, 1331, 11, 12288, 11], "temperature": 0.0, "avg_logprob": -0.14574849289075464, "compression_ratio": 1.7923728813559323, "no_speech_prob": 2.3922615582705475e-05}, {"id": 735, "seek": 333136, "start": 3343.08, "end": 3344.08, "text": " blah, blah, blah, blah.", "tokens": [12288, 11, 12288, 11, 12288, 11, 12288, 13], "temperature": 0.0, "avg_logprob": -0.14574849289075464, "compression_ratio": 1.7923728813559323, "no_speech_prob": 2.3922615582705475e-05}, {"id": 736, "seek": 333136, "start": 3344.08, "end": 3345.48, "text": " And that gives me a prediction.", "tokens": [400, 300, 2709, 385, 257, 17630, 13], "temperature": 0.0, "avg_logprob": -0.14574849289075464, "compression_ratio": 1.7923728813559323, "no_speech_prob": 2.3922615582705475e-05}, {"id": 737, "seek": 333136, "start": 3345.48, "end": 3353.6, "text": " So the predictions for these things at the very bottom is simply what's the average of", "tokens": [407, 264, 21264, 337, 613, 721, 412, 264, 588, 2767, 307, 2935, 437, 311, 264, 4274, 295], "temperature": 0.0, "avg_logprob": -0.14574849289075464, "compression_ratio": 1.7923728813559323, "no_speech_prob": 2.3922615582705475e-05}, {"id": 738, "seek": 333136, "start": 3353.6, "end": 3357.28, "text": " the dependent variable, in this case, the lung capacity for that group.", "tokens": [264, 12334, 7006, 11, 294, 341, 1389, 11, 264, 16730, 6042, 337, 300, 1594, 13], "temperature": 0.0, "avg_logprob": -0.14574849289075464, "compression_ratio": 1.7923728813559323, "no_speech_prob": 2.3922615582705475e-05}, {"id": 739, "seek": 335728, "start": 3357.28, "end": 3362.2000000000003, "text": " So that gives me 50 in decision tree one and it might be 30 in decision tree two and 14", "tokens": [407, 300, 2709, 385, 2625, 294, 3537, 4230, 472, 293, 309, 1062, 312, 2217, 294, 3537, 4230, 732, 293, 3499], "temperature": 0.0, "avg_logprob": -0.154022216796875, "compression_ratio": 1.70703125, "no_speech_prob": 3.3405160593247274e-06}, {"id": 740, "seek": 335728, "start": 3362.2000000000003, "end": 3363.2000000000003, "text": " in decision tree three.", "tokens": [294, 3537, 4230, 1045, 13], "temperature": 0.0, "avg_logprob": -0.154022216796875, "compression_ratio": 1.70703125, "no_speech_prob": 3.3405160593247274e-06}, {"id": 741, "seek": 335728, "start": 3363.2000000000003, "end": 3365.48, "text": " I just take the average on all of those.", "tokens": [286, 445, 747, 264, 4274, 322, 439, 295, 729, 13], "temperature": 0.0, "avg_logprob": -0.154022216796875, "compression_ratio": 1.70703125, "no_speech_prob": 3.3405160593247274e-06}, {"id": 742, "seek": 335728, "start": 3365.48, "end": 3373.6800000000003, "text": " And that's given me what I wanted, which is a whole bunch of independent, unbiased, not", "tokens": [400, 300, 311, 2212, 385, 437, 286, 1415, 11, 597, 307, 257, 1379, 3840, 295, 6695, 11, 517, 5614, 1937, 11, 406], "temperature": 0.0, "avg_logprob": -0.154022216796875, "compression_ratio": 1.70703125, "no_speech_prob": 3.3405160593247274e-06}, {"id": 743, "seek": 335728, "start": 3373.6800000000003, "end": 3377.1600000000003, "text": " completely crap models.", "tokens": [2584, 12426, 5245, 13], "temperature": 0.0, "avg_logprob": -0.154022216796875, "compression_ratio": 1.70703125, "no_speech_prob": 3.3405160593247274e-06}, {"id": 744, "seek": 335728, "start": 3377.1600000000003, "end": 3378.8, "text": " How not completely crap are they?", "tokens": [1012, 406, 2584, 12426, 366, 436, 30], "temperature": 0.0, "avg_logprob": -0.154022216796875, "compression_ratio": 1.70703125, "no_speech_prob": 3.3405160593247274e-06}, {"id": 745, "seek": 335728, "start": 3378.8, "end": 3381.88, "text": " Well, the nice thing is we can pick, right?", "tokens": [1042, 11, 264, 1481, 551, 307, 321, 393, 1888, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.154022216796875, "compression_ratio": 1.70703125, "no_speech_prob": 3.3405160593247274e-06}, {"id": 746, "seek": 335728, "start": 3381.88, "end": 3387.2000000000003, "text": " If you want to be super cautious and you really need to make sure you're avoiding overfitting,", "tokens": [759, 291, 528, 281, 312, 1687, 25278, 293, 291, 534, 643, 281, 652, 988, 291, 434, 20220, 670, 69, 2414, 11], "temperature": 0.0, "avg_logprob": -0.154022216796875, "compression_ratio": 1.70703125, "no_speech_prob": 3.3405160593247274e-06}, {"id": 747, "seek": 338720, "start": 3387.2, "end": 3391.48, "text": " then what you do is you make sure your random subspaces are smaller.", "tokens": [550, 437, 291, 360, 307, 291, 652, 988, 428, 4974, 2090, 79, 2116, 366, 4356, 13], "temperature": 0.0, "avg_logprob": -0.13685493266328852, "compression_ratio": 1.8009478672985781, "no_speech_prob": 8.939531653595623e-06}, {"id": 748, "seek": 338720, "start": 3391.48, "end": 3400.6, "text": " You pick less rows and less columns so that then each tree is shitter than average.", "tokens": [509, 1888, 1570, 13241, 293, 1570, 13766, 370, 300, 550, 1184, 4230, 307, 402, 3904, 813, 4274, 13], "temperature": 0.0, "avg_logprob": -0.13685493266328852, "compression_ratio": 1.8009478672985781, "no_speech_prob": 8.939531653595623e-06}, {"id": 749, "seek": 338720, "start": 3400.6, "end": 3406.0, "text": " Where else if you want to be quick, you make each one have more rows and more columns so", "tokens": [2305, 1646, 498, 291, 528, 281, 312, 1702, 11, 291, 652, 1184, 472, 362, 544, 13241, 293, 544, 13766, 370], "temperature": 0.0, "avg_logprob": -0.13685493266328852, "compression_ratio": 1.8009478672985781, "no_speech_prob": 8.939531653595623e-06}, {"id": 750, "seek": 338720, "start": 3406.0, "end": 3410.2, "text": " it better reflects the true data that you've got.", "tokens": [309, 1101, 18926, 264, 2074, 1412, 300, 291, 600, 658, 13], "temperature": 0.0, "avg_logprob": -0.13685493266328852, "compression_ratio": 1.8009478672985781, "no_speech_prob": 8.939531653595623e-06}, {"id": 751, "seek": 338720, "start": 3410.2, "end": 3414.6, "text": " Obviously the less rows and less columns you have each time, the less powerful each tree", "tokens": [7580, 264, 1570, 13241, 293, 1570, 13766, 291, 362, 1184, 565, 11, 264, 1570, 4005, 1184, 4230], "temperature": 0.0, "avg_logprob": -0.13685493266328852, "compression_ratio": 1.8009478672985781, "no_speech_prob": 8.939531653595623e-06}, {"id": 752, "seek": 341460, "start": 3414.6, "end": 3418.48, "text": " is and therefore the more trees you need.", "tokens": [307, 293, 4412, 264, 544, 5852, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.2248317527770996, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.296251624211436e-06}, {"id": 753, "seek": 341460, "start": 3418.48, "end": 3424.4, "text": " And the nice thing about this is that building each of these trees takes like a 10,000th", "tokens": [400, 264, 1481, 551, 466, 341, 307, 300, 2390, 1184, 295, 613, 5852, 2516, 411, 257, 1266, 11, 1360, 392], "temperature": 0.0, "avg_logprob": -0.2248317527770996, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.296251624211436e-06}, {"id": 754, "seek": 341460, "start": 3424.4, "end": 3426.4, "text": " of a second or less.", "tokens": [295, 257, 1150, 420, 1570, 13], "temperature": 0.0, "avg_logprob": -0.2248317527770996, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.296251624211436e-06}, {"id": 755, "seek": 341460, "start": 3426.4, "end": 3430.08, "text": " It depends on how much data you've got, but you can build thousands of trees in a few", "tokens": [467, 5946, 322, 577, 709, 1412, 291, 600, 658, 11, 457, 291, 393, 1322, 5383, 295, 5852, 294, 257, 1326], "temperature": 0.0, "avg_logprob": -0.2248317527770996, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.296251624211436e-06}, {"id": 756, "seek": 341460, "start": 3430.08, "end": 3432.48, "text": " seconds are the kind of data sets I look at.", "tokens": [3949, 366, 264, 733, 295, 1412, 6352, 286, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.2248317527770996, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.296251624211436e-06}, {"id": 757, "seek": 341460, "start": 3432.48, "end": 3437.2799999999997, "text": " So generally speaking, this isn't an issue.", "tokens": [407, 5101, 4124, 11, 341, 1943, 380, 364, 2734, 13], "temperature": 0.0, "avg_logprob": -0.2248317527770996, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.296251624211436e-06}, {"id": 758, "seek": 341460, "start": 3437.2799999999997, "end": 3441.4, "text": " And here's the really cool thing.", "tokens": [400, 510, 311, 264, 534, 1627, 551, 13], "temperature": 0.0, "avg_logprob": -0.2248317527770996, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.296251624211436e-06}, {"id": 759, "seek": 344140, "start": 3441.4, "end": 3452.6800000000003, "text": " In this tree, I built it with these rows, which means that these rows I didn't use to", "tokens": [682, 341, 4230, 11, 286, 3094, 309, 365, 613, 13241, 11, 597, 1355, 300, 613, 13241, 286, 994, 380, 764, 281], "temperature": 0.0, "avg_logprob": -0.10004610239073287, "compression_ratio": 1.7459459459459459, "no_speech_prob": 6.6433353822503705e-06}, {"id": 760, "seek": 344140, "start": 3452.6800000000003, "end": 3458.64, "text": " build my tree, which means these rows are out of sample for that tree.", "tokens": [1322, 452, 4230, 11, 597, 1355, 613, 13241, 366, 484, 295, 6889, 337, 300, 4230, 13], "temperature": 0.0, "avg_logprob": -0.10004610239073287, "compression_ratio": 1.7459459459459459, "no_speech_prob": 6.6433353822503705e-06}, {"id": 761, "seek": 344140, "start": 3458.64, "end": 3463.8, "text": " And what that means is I don't need to have a separate cross validation data set.", "tokens": [400, 437, 300, 1355, 307, 286, 500, 380, 643, 281, 362, 257, 4994, 3278, 24071, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.10004610239073287, "compression_ratio": 1.7459459459459459, "no_speech_prob": 6.6433353822503705e-06}, {"id": 762, "seek": 344140, "start": 3463.8, "end": 3471.2000000000003, "text": " What it means is I can create a table now of my full data set and for each one I can", "tokens": [708, 309, 1355, 307, 286, 393, 1884, 257, 3199, 586, 295, 452, 1577, 1412, 992, 293, 337, 1184, 472, 286, 393], "temperature": 0.0, "avg_logprob": -0.10004610239073287, "compression_ratio": 1.7459459459459459, "no_speech_prob": 6.6433353822503705e-06}, {"id": 763, "seek": 347120, "start": 3471.2, "end": 3476.6, "text": " say okay row number one, how good am I at predicting row number one?", "tokens": [584, 1392, 5386, 1230, 472, 11, 577, 665, 669, 286, 412, 32884, 5386, 1230, 472, 30], "temperature": 0.0, "avg_logprob": -0.13996341740973642, "compression_ratio": 1.8578199052132702, "no_speech_prob": 6.108447269070894e-05}, {"id": 764, "seek": 347120, "start": 3476.6, "end": 3481.12, "text": " Well here's all of my trees from one to a thousand.", "tokens": [1042, 510, 311, 439, 295, 452, 5852, 490, 472, 281, 257, 4714, 13], "temperature": 0.0, "avg_logprob": -0.13996341740973642, "compression_ratio": 1.8578199052132702, "no_speech_prob": 6.108447269070894e-05}, {"id": 765, "seek": 347120, "start": 3481.12, "end": 3486.2799999999997, "text": " Row number one is in fact one of the things that was included when I created tree number", "tokens": [20309, 1230, 472, 307, 294, 1186, 472, 295, 264, 721, 300, 390, 5556, 562, 286, 2942, 4230, 1230], "temperature": 0.0, "avg_logprob": -0.13996341740973642, "compression_ratio": 1.8578199052132702, "no_speech_prob": 6.108447269070894e-05}, {"id": 766, "seek": 347120, "start": 3486.2799999999997, "end": 3487.2799999999997, "text": " one.", "tokens": [472, 13], "temperature": 0.0, "avg_logprob": -0.13996341740973642, "compression_ratio": 1.8578199052132702, "no_speech_prob": 6.108447269070894e-05}, {"id": 767, "seek": 347120, "start": 3487.2799999999997, "end": 3493.06, "text": " So I won't use it here, but row number one wasn't included when I built tree two.", "tokens": [407, 286, 1582, 380, 764, 309, 510, 11, 457, 5386, 1230, 472, 2067, 380, 5556, 562, 286, 3094, 4230, 732, 13], "temperature": 0.0, "avg_logprob": -0.13996341740973642, "compression_ratio": 1.8578199052132702, "no_speech_prob": 6.108447269070894e-05}, {"id": 768, "seek": 347120, "start": 3493.06, "end": 3496.7599999999998, "text": " It wasn't included in the random subspace for tree three and it was included in the", "tokens": [467, 2067, 380, 5556, 294, 264, 4974, 2090, 17940, 337, 4230, 1045, 293, 309, 390, 5556, 294, 264], "temperature": 0.0, "avg_logprob": -0.13996341740973642, "compression_ratio": 1.8578199052132702, "no_speech_prob": 6.108447269070894e-05}, {"id": 769, "seek": 347120, "start": 3496.7599999999998, "end": 3498.3199999999997, "text": " one before.", "tokens": [472, 949, 13], "temperature": 0.0, "avg_logprob": -0.13996341740973642, "compression_ratio": 1.8578199052132702, "no_speech_prob": 6.108447269070894e-05}, {"id": 770, "seek": 349832, "start": 3498.32, "end": 3506.76, "text": " So what I do is row number one, I send down trees two and three and I get predictions", "tokens": [407, 437, 286, 360, 307, 5386, 1230, 472, 11, 286, 2845, 760, 5852, 732, 293, 1045, 293, 286, 483, 21264], "temperature": 0.0, "avg_logprob": -0.11085776428678143, "compression_ratio": 1.5521472392638036, "no_speech_prob": 2.225242496933788e-06}, {"id": 771, "seek": 349832, "start": 3506.76, "end": 3515.28, "text": " for everything that it wasn't in and average them out and that gives me this fantastic", "tokens": [337, 1203, 300, 309, 2067, 380, 294, 293, 4274, 552, 484, 293, 300, 2709, 385, 341, 5456], "temperature": 0.0, "avg_logprob": -0.11085776428678143, "compression_ratio": 1.5521472392638036, "no_speech_prob": 2.225242496933788e-06}, {"id": 772, "seek": 349832, "start": 3515.28, "end": 3523.2200000000003, "text": " thing which is an out of band estimate for row one.", "tokens": [551, 597, 307, 364, 484, 295, 4116, 12539, 337, 5386, 472, 13], "temperature": 0.0, "avg_logprob": -0.11085776428678143, "compression_ratio": 1.5521472392638036, "no_speech_prob": 2.225242496933788e-06}, {"id": 773, "seek": 349832, "start": 3523.2200000000003, "end": 3525.38, "text": " And I do that for every row.", "tokens": [400, 286, 360, 300, 337, 633, 5386, 13], "temperature": 0.0, "avg_logprob": -0.11085776428678143, "compression_ratio": 1.5521472392638036, "no_speech_prob": 2.225242496933788e-06}, {"id": 774, "seek": 352538, "start": 3525.38, "end": 3530.1600000000003, "text": " So none of this, all of this stuff which is being predicted here is actually not using", "tokens": [407, 6022, 295, 341, 11, 439, 295, 341, 1507, 597, 307, 885, 19147, 510, 307, 767, 406, 1228], "temperature": 0.0, "avg_logprob": -0.1532040278116862, "compression_ratio": 1.6884057971014492, "no_speech_prob": 1.0289303645549808e-05}, {"id": 775, "seek": 352538, "start": 3530.1600000000003, "end": 3533.8, "text": " any of the data that was used to build the trees and therefore it is truly out of sample", "tokens": [604, 295, 264, 1412, 300, 390, 1143, 281, 1322, 264, 5852, 293, 4412, 309, 307, 4908, 484, 295, 6889], "temperature": 0.0, "avg_logprob": -0.1532040278116862, "compression_ratio": 1.6884057971014492, "no_speech_prob": 1.0289303645549808e-05}, {"id": 776, "seek": 352538, "start": 3533.8, "end": 3538.52, "text": " or out of band and therefore when I put this all together to create my final whatever it", "tokens": [420, 484, 295, 4116, 293, 4412, 562, 286, 829, 341, 439, 1214, 281, 1884, 452, 2572, 2035, 309], "temperature": 0.0, "avg_logprob": -0.1532040278116862, "compression_ratio": 1.6884057971014492, "no_speech_prob": 1.0289303645549808e-05}, {"id": 777, "seek": 352538, "start": 3538.52, "end": 3546.6400000000003, "text": " is AUC or log likelihood or R-square or SSE or whatever and then I send that off to Kaggle.", "tokens": [307, 7171, 34, 420, 3565, 22119, 420, 497, 12, 33292, 543, 420, 318, 5879, 420, 2035, 293, 550, 286, 2845, 300, 766, 281, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.1532040278116862, "compression_ratio": 1.6884057971014492, "no_speech_prob": 1.0289303645549808e-05}, {"id": 778, "seek": 352538, "start": 3546.6400000000003, "end": 3551.7200000000003, "text": " Kaggle should give you pretty much the same answer because you're by definition not over", "tokens": [48751, 22631, 820, 976, 291, 1238, 709, 264, 912, 1867, 570, 291, 434, 538, 7123, 406, 670], "temperature": 0.0, "avg_logprob": -0.1532040278116862, "compression_ratio": 1.6884057971014492, "no_speech_prob": 1.0289303645549808e-05}, {"id": 779, "seek": 352538, "start": 3551.7200000000003, "end": 3552.7200000000003, "text": " fitting.", "tokens": [15669, 13], "temperature": 0.0, "avg_logprob": -0.1532040278116862, "compression_ratio": 1.6884057971014492, "no_speech_prob": 1.0289303645549808e-05}, {"id": 780, "seek": 352538, "start": 3552.7200000000003, "end": 3553.7200000000003, "text": " Yes you do.", "tokens": [1079, 291, 360, 13], "temperature": 0.0, "avg_logprob": -0.1532040278116862, "compression_ratio": 1.6884057971014492, "no_speech_prob": 1.0289303645549808e-05}, {"id": 781, "seek": 355372, "start": 3553.72, "end": 3558.3999999999996, "text": " I was just wondering if it is possible to say just pick a tree that has the best, instead", "tokens": [286, 390, 445, 6359, 498, 309, 307, 1944, 281, 584, 445, 1888, 257, 4230, 300, 575, 264, 1151, 11, 2602], "temperature": 0.0, "avg_logprob": -0.23244422251783955, "compression_ratio": 1.9069767441860466, "no_speech_prob": 4.0061171603156254e-05}, {"id": 782, "seek": 355372, "start": 3558.3999999999996, "end": 3562.9199999999996, "text": " of averaging out in those cases, is it possible to just pick that one tree that actually has", "tokens": [295, 47308, 484, 294, 729, 3331, 11, 307, 309, 1944, 281, 445, 1888, 300, 472, 4230, 300, 767, 575], "temperature": 0.0, "avg_logprob": -0.23244422251783955, "compression_ratio": 1.9069767441860466, "no_speech_prob": 4.0061171603156254e-05}, {"id": 783, "seek": 355372, "start": 3562.9199999999996, "end": 3565.72, "text": " the best performance and would you recommend it?", "tokens": [264, 1151, 3389, 293, 576, 291, 2748, 309, 30], "temperature": 0.0, "avg_logprob": -0.23244422251783955, "compression_ratio": 1.9069767441860466, "no_speech_prob": 4.0061171603156254e-05}, {"id": 784, "seek": 355372, "start": 3565.72, "end": 3567.0, "text": " Yes you can, no I wouldn't.", "tokens": [1079, 291, 393, 11, 572, 286, 2759, 380, 13], "temperature": 0.0, "avg_logprob": -0.23244422251783955, "compression_ratio": 1.9069767441860466, "no_speech_prob": 4.0061171603156254e-05}, {"id": 785, "seek": 355372, "start": 3567.0, "end": 3571.8799999999997, "text": " So the question was can you just pick one tree and would that tree, picking that one", "tokens": [407, 264, 1168, 390, 393, 291, 445, 1888, 472, 4230, 293, 576, 300, 4230, 11, 8867, 300, 472], "temperature": 0.0, "avg_logprob": -0.23244422251783955, "compression_ratio": 1.9069767441860466, "no_speech_prob": 4.0061171603156254e-05}, {"id": 786, "seek": 355372, "start": 3571.8799999999997, "end": 3574.3199999999997, "text": " tree be better than what we've just done?", "tokens": [4230, 312, 1101, 813, 437, 321, 600, 445, 1096, 30], "temperature": 0.0, "avg_logprob": -0.23244422251783955, "compression_ratio": 1.9069767441860466, "no_speech_prob": 4.0061171603156254e-05}, {"id": 787, "seek": 355372, "start": 3574.3199999999997, "end": 3578.6, "text": " And let's think about what, that's a really important question and let's think about why", "tokens": [400, 718, 311, 519, 466, 437, 11, 300, 311, 257, 534, 1021, 1168, 293, 718, 311, 519, 466, 983], "temperature": 0.0, "avg_logprob": -0.23244422251783955, "compression_ratio": 1.9069767441860466, "no_speech_prob": 4.0061171603156254e-05}, {"id": 788, "seek": 355372, "start": 3578.6, "end": 3580.2, "text": " that won't work.", "tokens": [300, 1582, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.23244422251783955, "compression_ratio": 1.9069767441860466, "no_speech_prob": 4.0061171603156254e-05}, {"id": 789, "seek": 358020, "start": 3580.2, "end": 3583.72, "text": " The whole purpose of this was to not over fit.", "tokens": [440, 1379, 4334, 295, 341, 390, 281, 406, 670, 3318, 13], "temperature": 0.0, "avg_logprob": -0.11462292878524116, "compression_ratio": 1.7966804979253113, "no_speech_prob": 8.800809155218303e-06}, {"id": 790, "seek": 358020, "start": 3583.72, "end": 3590.04, "text": " So the whole purpose of this was to say each of these trees is pretty crap but it's better", "tokens": [407, 264, 1379, 4334, 295, 341, 390, 281, 584, 1184, 295, 613, 5852, 307, 1238, 12426, 457, 309, 311, 1101], "temperature": 0.0, "avg_logprob": -0.11462292878524116, "compression_ratio": 1.7966804979253113, "no_speech_prob": 8.800809155218303e-06}, {"id": 791, "seek": 358020, "start": 3590.04, "end": 3594.64, "text": " than nothing and so we average them all out, it tells us something about the true data,", "tokens": [813, 1825, 293, 370, 321, 4274, 552, 439, 484, 11, 309, 5112, 505, 746, 466, 264, 2074, 1412, 11], "temperature": 0.0, "avg_logprob": -0.11462292878524116, "compression_ratio": 1.7966804979253113, "no_speech_prob": 8.800809155218303e-06}, {"id": 792, "seek": 358020, "start": 3594.64, "end": 3596.98, "text": " each one can't over fit on its own.", "tokens": [1184, 472, 393, 380, 670, 3318, 322, 1080, 1065, 13], "temperature": 0.0, "avg_logprob": -0.11462292878524116, "compression_ratio": 1.7966804979253113, "no_speech_prob": 8.800809155218303e-06}, {"id": 793, "seek": 358020, "start": 3596.98, "end": 3601.72, "text": " If I now go back and do anything to those trees, if I try and prune them, which is in", "tokens": [759, 286, 586, 352, 646, 293, 360, 1340, 281, 729, 5852, 11, 498, 286, 853, 293, 582, 2613, 552, 11, 597, 307, 294], "temperature": 0.0, "avg_logprob": -0.11462292878524116, "compression_ratio": 1.7966804979253113, "no_speech_prob": 8.800809155218303e-06}, {"id": 794, "seek": 358020, "start": 3601.72, "end": 3607.02, "text": " the old fashioned decision tree algorithms, or if I weight them or if I pick a subset", "tokens": [264, 1331, 40646, 3537, 4230, 14642, 11, 420, 498, 286, 3364, 552, 420, 498, 286, 1888, 257, 25993], "temperature": 0.0, "avg_logprob": -0.11462292878524116, "compression_ratio": 1.7966804979253113, "no_speech_prob": 8.800809155218303e-06}, {"id": 795, "seek": 360702, "start": 3607.02, "end": 3613.88, "text": " of them, I'm now introducing bias based on the training set predictivity.", "tokens": [295, 552, 11, 286, 478, 586, 15424, 12577, 2361, 322, 264, 3097, 992, 6069, 4253, 13], "temperature": 0.0, "avg_logprob": -0.145877380893655, "compression_ratio": 1.530612244897959, "no_speech_prob": 1.983269612537697e-05}, {"id": 796, "seek": 360702, "start": 3613.88, "end": 3619.88, "text": " So anytime I introduce bias, I now break the laws of ensemble methods fundamentally.", "tokens": [407, 13038, 286, 5366, 12577, 11, 286, 586, 1821, 264, 6064, 295, 19492, 7150, 17879, 13], "temperature": 0.0, "avg_logprob": -0.145877380893655, "compression_ratio": 1.530612244897959, "no_speech_prob": 1.983269612537697e-05}, {"id": 797, "seek": 360702, "start": 3619.88, "end": 3625.3, "text": " So the other thing I'd say is there's no point, right?", "tokens": [407, 264, 661, 551, 286, 1116, 584, 307, 456, 311, 572, 935, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.145877380893655, "compression_ratio": 1.530612244897959, "no_speech_prob": 1.983269612537697e-05}, {"id": 798, "seek": 360702, "start": 3625.3, "end": 3633.88, "text": " Because if you have something where actually you've got so much training data that out", "tokens": [1436, 498, 291, 362, 746, 689, 767, 291, 600, 658, 370, 709, 3097, 1412, 300, 484], "temperature": 0.0, "avg_logprob": -0.145877380893655, "compression_ratio": 1.530612244897959, "no_speech_prob": 1.983269612537697e-05}, {"id": 799, "seek": 363388, "start": 3633.88, "end": 3640.92, "text": " of sample isn't a big problem or whatever, you just use bigger subspaces and less trees.", "tokens": [295, 6889, 1943, 380, 257, 955, 1154, 420, 2035, 11, 291, 445, 764, 3801, 2090, 79, 2116, 293, 1570, 5852, 13], "temperature": 0.0, "avg_logprob": -0.1460422238995952, "compression_ratio": 1.7191011235955056, "no_speech_prob": 5.7384495448786765e-05}, {"id": 800, "seek": 363388, "start": 3640.92, "end": 3644.6800000000003, "text": " And in fact, the only reason you do that is for time and because this approach is so fast", "tokens": [400, 294, 1186, 11, 264, 787, 1778, 291, 360, 300, 307, 337, 565, 293, 570, 341, 3109, 307, 370, 2370], "temperature": 0.0, "avg_logprob": -0.1460422238995952, "compression_ratio": 1.7191011235955056, "no_speech_prob": 5.7384495448786765e-05}, {"id": 801, "seek": 363388, "start": 3644.6800000000003, "end": 3647.92, "text": " anyway, I wouldn't even bother then, you see.", "tokens": [4033, 11, 286, 2759, 380, 754, 8677, 550, 11, 291, 536, 13], "temperature": 0.0, "avg_logprob": -0.1460422238995952, "compression_ratio": 1.7191011235955056, "no_speech_prob": 5.7384495448786765e-05}, {"id": 802, "seek": 363388, "start": 3647.92, "end": 3654.28, "text": " And the nice thing about this is, is that you can say, okay, I'm going to use kind of", "tokens": [400, 264, 1481, 551, 466, 341, 307, 11, 307, 300, 291, 393, 584, 11, 1392, 11, 286, 478, 516, 281, 764, 733, 295], "temperature": 0.0, "avg_logprob": -0.1460422238995952, "compression_ratio": 1.7191011235955056, "no_speech_prob": 5.7384495448786765e-05}, {"id": 803, "seek": 363388, "start": 3654.28, "end": 3658.2400000000002, "text": " this many columns and this many rows in each subspace, right?", "tokens": [341, 867, 13766, 293, 341, 867, 13241, 294, 1184, 2090, 17940, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1460422238995952, "compression_ratio": 1.7191011235955056, "no_speech_prob": 5.7384495448786765e-05}, {"id": 804, "seek": 363388, "start": 3658.2400000000002, "end": 3663.2000000000003, "text": " And I've got to start building my trees and I build tree number one and I get this out", "tokens": [400, 286, 600, 658, 281, 722, 2390, 452, 5852, 293, 286, 1322, 4230, 1230, 472, 293, 286, 483, 341, 484], "temperature": 0.0, "avg_logprob": -0.1460422238995952, "compression_ratio": 1.7191011235955056, "no_speech_prob": 5.7384495448786765e-05}, {"id": 805, "seek": 366320, "start": 3663.2, "end": 3664.52, "text": " of band error.", "tokens": [295, 4116, 6713, 13], "temperature": 0.0, "avg_logprob": -0.1549219258626302, "compression_ratio": 1.8007246376811594, "no_speech_prob": 8.80090010468848e-06}, {"id": 806, "seek": 366320, "start": 3664.52, "end": 3666.04, "text": " Tree number two, this out of band error.", "tokens": [22291, 1230, 732, 11, 341, 484, 295, 4116, 6713, 13], "temperature": 0.0, "avg_logprob": -0.1549219258626302, "compression_ratio": 1.8007246376811594, "no_speech_prob": 8.80090010468848e-06}, {"id": 807, "seek": 366320, "start": 3666.04, "end": 3667.7999999999997, "text": " Tree number three, this out of band error.", "tokens": [22291, 1230, 1045, 11, 341, 484, 295, 4116, 6713, 13], "temperature": 0.0, "avg_logprob": -0.1549219258626302, "compression_ratio": 1.8007246376811594, "no_speech_prob": 8.80090010468848e-06}, {"id": 808, "seek": 366320, "start": 3667.7999999999997, "end": 3673.0, "text": " And the nice thing is I can watch and see and it will be monotonic.", "tokens": [400, 264, 1481, 551, 307, 286, 393, 1159, 293, 536, 293, 309, 486, 312, 1108, 310, 11630, 13], "temperature": 0.0, "avg_logprob": -0.1549219258626302, "compression_ratio": 1.8007246376811594, "no_speech_prob": 8.80090010468848e-06}, {"id": 809, "seek": 366320, "start": 3673.0, "end": 3677.0, "text": " Well, not exactly monotonic, but kind of bumping monotonic.", "tokens": [1042, 11, 406, 2293, 1108, 310, 11630, 11, 457, 733, 295, 9961, 278, 1108, 310, 11630, 13], "temperature": 0.0, "avg_logprob": -0.1549219258626302, "compression_ratio": 1.8007246376811594, "no_speech_prob": 8.80090010468848e-06}, {"id": 810, "seek": 366320, "start": 3677.0, "end": 3678.7999999999997, "text": " It will keep getting better on average.", "tokens": [467, 486, 1066, 1242, 1101, 322, 4274, 13], "temperature": 0.0, "avg_logprob": -0.1549219258626302, "compression_ratio": 1.8007246376811594, "no_speech_prob": 8.80090010468848e-06}, {"id": 811, "seek": 366320, "start": 3678.7999999999997, "end": 3682.2, "text": " And I can get to a point where I say, okay, that's good enough, I'll stop.", "tokens": [400, 286, 393, 483, 281, 257, 935, 689, 286, 584, 11, 1392, 11, 300, 311, 665, 1547, 11, 286, 603, 1590, 13], "temperature": 0.0, "avg_logprob": -0.1549219258626302, "compression_ratio": 1.8007246376811594, "no_speech_prob": 8.80090010468848e-06}, {"id": 812, "seek": 366320, "start": 3682.2, "end": 3685.52, "text": " And as I say, normally it's talking four or five seconds.", "tokens": [400, 382, 286, 584, 11, 5646, 309, 311, 1417, 1451, 420, 1732, 3949, 13], "temperature": 0.0, "avg_logprob": -0.1549219258626302, "compression_ratio": 1.8007246376811594, "no_speech_prob": 8.80090010468848e-06}, {"id": 813, "seek": 366320, "start": 3685.52, "end": 3687.08, "text": " It's just time's not an issue.", "tokens": [467, 311, 445, 565, 311, 406, 364, 2734, 13], "temperature": 0.0, "avg_logprob": -0.1549219258626302, "compression_ratio": 1.8007246376811594, "no_speech_prob": 8.80090010468848e-06}, {"id": 814, "seek": 366320, "start": 3687.08, "end": 3690.8799999999997, "text": " But if you're talking about huge data sets, you can't sample them.", "tokens": [583, 498, 291, 434, 1417, 466, 2603, 1412, 6352, 11, 291, 393, 380, 6889, 552, 13], "temperature": 0.0, "avg_logprob": -0.1549219258626302, "compression_ratio": 1.8007246376811594, "no_speech_prob": 8.80090010468848e-06}, {"id": 815, "seek": 369088, "start": 3690.88, "end": 3695.44, "text": " This is a way you can watch it go.", "tokens": [639, 307, 257, 636, 291, 393, 1159, 309, 352, 13], "temperature": 0.0, "avg_logprob": -0.09865242453182445, "compression_ratio": 1.5863636363636364, "no_speech_prob": 1.0952787306450773e-05}, {"id": 816, "seek": 369088, "start": 3695.44, "end": 3699.88, "text": " So this is a technique that I used in the grants prediction competition.", "tokens": [407, 341, 307, 257, 6532, 300, 286, 1143, 294, 264, 16101, 17630, 6211, 13], "temperature": 0.0, "avg_logprob": -0.09865242453182445, "compression_ratio": 1.5863636363636364, "no_speech_prob": 1.0952787306450773e-05}, {"id": 817, "seek": 369088, "start": 3699.88, "end": 3706.1600000000003, "text": " I did a bunch of things to make it even more random than this.", "tokens": [286, 630, 257, 3840, 295, 721, 281, 652, 309, 754, 544, 4974, 813, 341, 13], "temperature": 0.0, "avg_logprob": -0.09865242453182445, "compression_ratio": 1.5863636363636364, "no_speech_prob": 1.0952787306450773e-05}, {"id": 818, "seek": 369088, "start": 3706.1600000000003, "end": 3710.32, "text": " One of the big problems here, both in terms of time and lack of randomness, is that all", "tokens": [1485, 295, 264, 955, 2740, 510, 11, 1293, 294, 2115, 295, 565, 293, 5011, 295, 4974, 1287, 11, 307, 300, 439], "temperature": 0.0, "avg_logprob": -0.09865242453182445, "compression_ratio": 1.5863636363636364, "no_speech_prob": 1.0952787306450773e-05}, {"id": 819, "seek": 369088, "start": 3710.32, "end": 3717.0, "text": " of these continuous variables, the official random forest algorithm searches through every", "tokens": [295, 613, 10957, 9102, 11, 264, 4783, 4974, 6719, 9284, 26701, 807, 633], "temperature": 0.0, "avg_logprob": -0.09865242453182445, "compression_ratio": 1.5863636363636364, "no_speech_prob": 1.0952787306450773e-05}, {"id": 820, "seek": 371700, "start": 3717.0, "end": 3722.4, "text": " possible breakpoint to find the very best, which means that every single time that you", "tokens": [1944, 1821, 6053, 281, 915, 264, 588, 1151, 11, 597, 1355, 300, 633, 2167, 565, 300, 291], "temperature": 0.0, "avg_logprob": -0.1351607186453683, "compression_ratio": 1.6944444444444444, "no_speech_prob": 2.318643055332359e-05}, {"id": 821, "seek": 371700, "start": 3722.4, "end": 3727.72, "text": " use that particular variable, particularly if it's in the same spot, like at the top", "tokens": [764, 300, 1729, 7006, 11, 4098, 498, 309, 311, 294, 264, 912, 4008, 11, 411, 412, 264, 1192], "temperature": 0.0, "avg_logprob": -0.1351607186453683, "compression_ratio": 1.6944444444444444, "no_speech_prob": 2.318643055332359e-05}, {"id": 822, "seek": 371700, "start": 3727.72, "end": 3731.12, "text": " of the tree, it's going to do the same split.", "tokens": [295, 264, 4230, 11, 309, 311, 516, 281, 360, 264, 912, 7472, 13], "temperature": 0.0, "avg_logprob": -0.1351607186453683, "compression_ratio": 1.6944444444444444, "no_speech_prob": 2.318643055332359e-05}, {"id": 823, "seek": 371700, "start": 3731.12, "end": 3735.84, "text": " In the version I wrote, actually all it does is every time it comes across a continuous", "tokens": [682, 264, 3037, 286, 4114, 11, 767, 439, 309, 775, 307, 633, 565, 309, 1487, 2108, 257, 10957], "temperature": 0.0, "avg_logprob": -0.1351607186453683, "compression_ratio": 1.6944444444444444, "no_speech_prob": 2.318643055332359e-05}, {"id": 824, "seek": 371700, "start": 3735.84, "end": 3740.32, "text": " variable, it randomly picks three breakpoints.", "tokens": [7006, 11, 309, 16979, 16137, 1045, 1821, 20552, 13], "temperature": 0.0, "avg_logprob": -0.1351607186453683, "compression_ratio": 1.6944444444444444, "no_speech_prob": 2.318643055332359e-05}, {"id": 825, "seek": 371700, "start": 3740.32, "end": 3744.08, "text": " So it might try 50, 70, and 19.", "tokens": [407, 309, 1062, 853, 2625, 11, 5285, 11, 293, 1294, 13], "temperature": 0.0, "avg_logprob": -0.1351607186453683, "compression_ratio": 1.6944444444444444, "no_speech_prob": 2.318643055332359e-05}, {"id": 826, "seek": 371700, "start": 3744.08, "end": 3746.32, "text": " And it just finds the best of those three.", "tokens": [400, 309, 445, 10704, 264, 1151, 295, 729, 1045, 13], "temperature": 0.0, "avg_logprob": -0.1351607186453683, "compression_ratio": 1.6944444444444444, "no_speech_prob": 2.318643055332359e-05}, {"id": 827, "seek": 374632, "start": 3746.32, "end": 3751.2000000000003, "text": " And to me, this is the secret of good ensemble algorithms, is to make every one as different", "tokens": [400, 281, 385, 11, 341, 307, 264, 4054, 295, 665, 19492, 14642, 11, 307, 281, 652, 633, 472, 382, 819], "temperature": 0.0, "avg_logprob": -0.1558694165162366, "compression_ratio": 1.7276785714285714, "no_speech_prob": 2.3552040147478692e-05}, {"id": 828, "seek": 374632, "start": 3751.2000000000003, "end": 3754.52, "text": " to every other tree as possible.", "tokens": [281, 633, 661, 4230, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.1558694165162366, "compression_ratio": 1.7276785714285714, "no_speech_prob": 2.3552040147478692e-05}, {"id": 829, "seek": 374632, "start": 3754.52, "end": 3759.48, "text": " Does the distribution of the population variable you're trying to predict matter?", "tokens": [4402, 264, 7316, 295, 264, 4415, 7006, 291, 434, 1382, 281, 6069, 1871, 30], "temperature": 0.0, "avg_logprob": -0.1558694165162366, "compression_ratio": 1.7276785714285714, "no_speech_prob": 2.3552040147478692e-05}, {"id": 830, "seek": 374632, "start": 3759.48, "end": 3760.92, "text": " No, not at all.", "tokens": [883, 11, 406, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.1558694165162366, "compression_ratio": 1.7276785714285714, "no_speech_prob": 2.3552040147478692e-05}, {"id": 831, "seek": 374632, "start": 3760.92, "end": 3765.92, "text": " So the question was, does the distribution of the dependent variable matter?", "tokens": [407, 264, 1168, 390, 11, 775, 264, 7316, 295, 264, 12334, 7006, 1871, 30], "temperature": 0.0, "avg_logprob": -0.1558694165162366, "compression_ratio": 1.7276785714285714, "no_speech_prob": 2.3552040147478692e-05}, {"id": 832, "seek": 374632, "start": 3765.92, "end": 3767.6400000000003, "text": " And the answer is it doesn't.", "tokens": [400, 264, 1867, 307, 309, 1177, 380, 13], "temperature": 0.0, "avg_logprob": -0.1558694165162366, "compression_ratio": 1.7276785714285714, "no_speech_prob": 2.3552040147478692e-05}, {"id": 833, "seek": 374632, "start": 3767.6400000000003, "end": 3770.4, "text": " And the reason it doesn't is because we're using a tree.", "tokens": [400, 264, 1778, 309, 1177, 380, 307, 570, 321, 434, 1228, 257, 4230, 13], "temperature": 0.0, "avg_logprob": -0.1558694165162366, "compression_ratio": 1.7276785714285714, "no_speech_prob": 2.3552040147478692e-05}, {"id": 834, "seek": 377040, "start": 3770.4, "end": 3780.58, "text": " So the nice thing about a tree is, let's imagine that the dependent variable was maybe very", "tokens": [407, 264, 1481, 551, 466, 257, 4230, 307, 11, 718, 311, 3811, 300, 264, 12334, 7006, 390, 1310, 588], "temperature": 0.0, "avg_logprob": -0.10945287280612521, "compression_ratio": 1.7746478873239437, "no_speech_prob": 1.1659153642540332e-05}, {"id": 835, "seek": 377040, "start": 3780.58, "end": 3784.0, "text": " long tail distribution, like so.", "tokens": [938, 6838, 7316, 11, 411, 370, 13], "temperature": 0.0, "avg_logprob": -0.10945287280612521, "compression_ratio": 1.7746478873239437, "no_speech_prob": 1.1659153642540332e-05}, {"id": 836, "seek": 377040, "start": 3784.0, "end": 3788.84, "text": " The nice thing is that as it looks at the independent variables, it's looking at the", "tokens": [440, 1481, 551, 307, 300, 382, 309, 1542, 412, 264, 6695, 9102, 11, 309, 311, 1237, 412, 264], "temperature": 0.0, "avg_logprob": -0.10945287280612521, "compression_ratio": 1.7746478873239437, "no_speech_prob": 1.1659153642540332e-05}, {"id": 837, "seek": 377040, "start": 3788.84, "end": 3793.64, "text": " difference in two groups and trying to find the biggest difference between those two groups.", "tokens": [2649, 294, 732, 3935, 293, 1382, 281, 915, 264, 3880, 2649, 1296, 729, 732, 3935, 13], "temperature": 0.0, "avg_logprob": -0.10945287280612521, "compression_ratio": 1.7746478873239437, "no_speech_prob": 1.1659153642540332e-05}, {"id": 838, "seek": 377040, "start": 3793.64, "end": 3798.8, "text": " So regardless of the distribution, it's more like a rank measure, isn't it?", "tokens": [407, 10060, 295, 264, 7316, 11, 309, 311, 544, 411, 257, 6181, 3481, 11, 1943, 380, 309, 30], "temperature": 0.0, "avg_logprob": -0.10945287280612521, "compression_ratio": 1.7746478873239437, "no_speech_prob": 1.1659153642540332e-05}, {"id": 839, "seek": 379880, "start": 3798.8, "end": 3803.1400000000003, "text": " It's picked a particular breakpoint, and it's saying which one finds the biggest difference", "tokens": [467, 311, 6183, 257, 1729, 1821, 6053, 11, 293, 309, 311, 1566, 597, 472, 10704, 264, 3880, 2649], "temperature": 0.0, "avg_logprob": -0.1396526537443462, "compression_ratio": 1.7179487179487178, "no_speech_prob": 2.8406817364157178e-05}, {"id": 840, "seek": 379880, "start": 3803.1400000000003, "end": 3804.7200000000003, "text": " between the two groups.", "tokens": [1296, 264, 732, 3935, 13], "temperature": 0.0, "avg_logprob": -0.1396526537443462, "compression_ratio": 1.7179487179487178, "no_speech_prob": 2.8406817364157178e-05}, {"id": 841, "seek": 379880, "start": 3804.7200000000003, "end": 3808.32, "text": " So regardless of the distribution of the dependent variable, it's still going to find the same", "tokens": [407, 10060, 295, 264, 7316, 295, 264, 12334, 7006, 11, 309, 311, 920, 516, 281, 915, 264, 912], "temperature": 0.0, "avg_logprob": -0.1396526537443462, "compression_ratio": 1.7179487179487178, "no_speech_prob": 2.8406817364157178e-05}, {"id": 842, "seek": 379880, "start": 3808.32, "end": 3813.32, "text": " breakpoints because it's really a non-parametric measure.", "tokens": [1821, 20552, 570, 309, 311, 534, 257, 2107, 12, 2181, 335, 17475, 3481, 13], "temperature": 0.0, "avg_logprob": -0.1396526537443462, "compression_ratio": 1.7179487179487178, "no_speech_prob": 2.8406817364157178e-05}, {"id": 843, "seek": 379880, "start": 3813.32, "end": 3820.44, "text": " We're using something like, for example, Gini or some kind of other measure of the information", "tokens": [492, 434, 1228, 746, 411, 11, 337, 1365, 11, 460, 3812, 420, 512, 733, 295, 661, 3481, 295, 264, 1589], "temperature": 0.0, "avg_logprob": -0.1396526537443462, "compression_ratio": 1.7179487179487178, "no_speech_prob": 2.8406817364157178e-05}, {"id": 844, "seek": 379880, "start": 3820.44, "end": 3822.84, "text": " gain of that to build the decision tree.", "tokens": [6052, 295, 300, 281, 1322, 264, 3537, 4230, 13], "temperature": 0.0, "avg_logprob": -0.1396526537443462, "compression_ratio": 1.7179487179487178, "no_speech_prob": 2.8406817364157178e-05}, {"id": 845, "seek": 379880, "start": 3822.84, "end": 3826.92, "text": " So this is true of really all decision tree approaches, in fact.", "tokens": [407, 341, 307, 2074, 295, 534, 439, 3537, 4230, 11587, 11, 294, 1186, 13], "temperature": 0.0, "avg_logprob": -0.1396526537443462, "compression_ratio": 1.7179487179487178, "no_speech_prob": 2.8406817364157178e-05}, {"id": 846, "seek": 382692, "start": 3826.92, "end": 3833.08, "text": " Does it work with the highly imbalanced data set?", "tokens": [4402, 309, 589, 365, 264, 5405, 566, 40251, 1412, 992, 30], "temperature": 0.0, "avg_logprob": -0.17490456321022727, "compression_ratio": 1.7914691943127963, "no_speech_prob": 3.7630143197020516e-05}, {"id": 847, "seek": 382692, "start": 3833.08, "end": 3834.08, "text": " Yes and no.", "tokens": [1079, 293, 572, 13], "temperature": 0.0, "avg_logprob": -0.17490456321022727, "compression_ratio": 1.7914691943127963, "no_speech_prob": 3.7630143197020516e-05}, {"id": 848, "seek": 382692, "start": 3834.08, "end": 3838.6800000000003, "text": " So the question is, does it work for a highly imbalanced data set?", "tokens": [407, 264, 1168, 307, 11, 775, 309, 589, 337, 257, 5405, 566, 40251, 1412, 992, 30], "temperature": 0.0, "avg_logprob": -0.17490456321022727, "compression_ratio": 1.7914691943127963, "no_speech_prob": 3.7630143197020516e-05}, {"id": 849, "seek": 382692, "start": 3838.6800000000003, "end": 3842.6, "text": " Sometimes some versions can, and some versions can't.", "tokens": [4803, 512, 9606, 393, 11, 293, 512, 9606, 393, 380, 13], "temperature": 0.0, "avg_logprob": -0.17490456321022727, "compression_ratio": 1.7914691943127963, "no_speech_prob": 3.7630143197020516e-05}, {"id": 850, "seek": 382692, "start": 3842.6, "end": 3846.32, "text": " The approaches which use more randomization are more likely to work okay.", "tokens": [440, 11587, 597, 764, 544, 4974, 2144, 366, 544, 3700, 281, 589, 1392, 13], "temperature": 0.0, "avg_logprob": -0.17490456321022727, "compression_ratio": 1.7914691943127963, "no_speech_prob": 3.7630143197020516e-05}, {"id": 851, "seek": 382692, "start": 3846.32, "end": 3852.12, "text": " But the problem is, in highly imbalanced data sets, you can quite quickly end up with nodes", "tokens": [583, 264, 1154, 307, 11, 294, 5405, 566, 40251, 1412, 6352, 11, 291, 393, 1596, 2661, 917, 493, 365, 13891], "temperature": 0.0, "avg_logprob": -0.17490456321022727, "compression_ratio": 1.7914691943127963, "no_speech_prob": 3.7630143197020516e-05}, {"id": 852, "seek": 382692, "start": 3852.12, "end": 3854.2400000000002, "text": " which are all the same value.", "tokens": [597, 366, 439, 264, 912, 2158, 13], "temperature": 0.0, "avg_logprob": -0.17490456321022727, "compression_ratio": 1.7914691943127963, "no_speech_prob": 3.7630143197020516e-05}, {"id": 853, "seek": 385424, "start": 3854.24, "end": 3860.16, "text": " So I actually have often found I get better results if I do some stratified sampling.", "tokens": [407, 286, 767, 362, 2049, 1352, 286, 483, 1101, 3542, 498, 286, 360, 512, 23674, 2587, 21179, 13], "temperature": 0.0, "avg_logprob": -0.15917181260514968, "compression_ratio": 1.5204918032786885, "no_speech_prob": 1.6441608750028536e-05}, {"id": 854, "seek": 385424, "start": 3860.16, "end": 3867.3199999999997, "text": " So that, for example, think about the R competition where most people don't have in store, other", "tokens": [407, 300, 11, 337, 1365, 11, 519, 466, 264, 497, 6211, 689, 881, 561, 500, 380, 362, 294, 3531, 11, 661], "temperature": 0.0, "avg_logprob": -0.15917181260514968, "compression_ratio": 1.5204918032786885, "no_speech_prob": 1.6441608750028536e-05}, {"id": 855, "seek": 385424, "start": 3867.3199999999997, "end": 3871.3799999999997, "text": " than user number five, 99% of packages.", "tokens": [813, 4195, 1230, 1732, 11, 11803, 4, 295, 17401, 13], "temperature": 0.0, "avg_logprob": -0.15917181260514968, "compression_ratio": 1.5204918032786885, "no_speech_prob": 1.6441608750028536e-05}, {"id": 856, "seek": 385424, "start": 3871.3799999999997, "end": 3876.64, "text": " So in that case, I tend to say, all right, at least half of that data set is so obviously", "tokens": [407, 294, 300, 1389, 11, 286, 3928, 281, 584, 11, 439, 558, 11, 412, 1935, 1922, 295, 300, 1412, 992, 307, 370, 2745], "temperature": 0.0, "avg_logprob": -0.15917181260514968, "compression_ratio": 1.5204918032786885, "no_speech_prob": 1.6441608750028536e-05}, {"id": 857, "seek": 385424, "start": 3876.64, "end": 3877.64, "text": " zero.", "tokens": [4018, 13], "temperature": 0.0, "avg_logprob": -0.15917181260514968, "compression_ratio": 1.5204918032786885, "no_speech_prob": 1.6441608750028536e-05}, {"id": 858, "seek": 385424, "start": 3877.64, "end": 3880.6, "text": " Let's just call it zero and just work with the rest.", "tokens": [961, 311, 445, 818, 309, 4018, 293, 445, 589, 365, 264, 1472, 13], "temperature": 0.0, "avg_logprob": -0.15917181260514968, "compression_ratio": 1.5204918032786885, "no_speech_prob": 1.6441608750028536e-05}, {"id": 859, "seek": 388060, "start": 3880.6, "end": 3884.52, "text": " I do find I often get better answers, but it does depend.", "tokens": [286, 360, 915, 286, 2049, 483, 1101, 6338, 11, 457, 309, 775, 5672, 13], "temperature": 0.0, "avg_logprob": -0.3672880664948494, "compression_ratio": 1.7203065134099618, "no_speech_prob": 2.977203439513687e-05}, {"id": 860, "seek": 388060, "start": 3884.52, "end": 3890.04, "text": " Would it be better to, instead of using random tree for the process, use another algorithm", "tokens": [6068, 309, 312, 1101, 281, 11, 2602, 295, 1228, 4974, 4230, 337, 264, 1399, 11, 764, 1071, 9284], "temperature": 0.0, "avg_logprob": -0.3672880664948494, "compression_ratio": 1.7203065134099618, "no_speech_prob": 2.977203439513687e-05}, {"id": 861, "seek": 388060, "start": 3890.04, "end": 3891.04, "text": " in the forest?", "tokens": [294, 264, 6719, 30], "temperature": 0.0, "avg_logprob": -0.3672880664948494, "compression_ratio": 1.7203065134099618, "no_speech_prob": 2.977203439513687e-05}, {"id": 862, "seek": 388060, "start": 3891.04, "end": 3892.04, "text": " Or the common balance?", "tokens": [1610, 264, 2689, 4772, 30], "temperature": 0.0, "avg_logprob": -0.3672880664948494, "compression_ratio": 1.7203065134099618, "no_speech_prob": 2.977203439513687e-05}, {"id": 863, "seek": 388060, "start": 3892.04, "end": 3896.44, "text": " Well, you can't call it forests if you use a different algorithm other than a tree.", "tokens": [1042, 11, 291, 393, 380, 818, 309, 21700, 498, 291, 764, 257, 819, 9284, 661, 813, 257, 4230, 13], "temperature": 0.0, "avg_logprob": -0.3672880664948494, "compression_ratio": 1.7203065134099618, "no_speech_prob": 2.977203439513687e-05}, {"id": 864, "seek": 388060, "start": 3896.44, "end": 3899.24, "text": " Yes, you can use other random subspace methods.", "tokens": [1079, 11, 291, 393, 764, 661, 4974, 2090, 17940, 7150, 13], "temperature": 0.0, "avg_logprob": -0.3672880664948494, "compression_ratio": 1.7203065134099618, "no_speech_prob": 2.977203439513687e-05}, {"id": 865, "seek": 388060, "start": 3899.24, "end": 3900.92, "text": " You can use another class five.", "tokens": [509, 393, 764, 1071, 1508, 1732, 13], "temperature": 0.0, "avg_logprob": -0.3672880664948494, "compression_ratio": 1.7203065134099618, "no_speech_prob": 2.977203439513687e-05}, {"id": 866, "seek": 388060, "start": 3900.92, "end": 3901.92, "text": " Yes, you absolutely can.", "tokens": [1079, 11, 291, 3122, 393, 13], "temperature": 0.0, "avg_logprob": -0.3672880664948494, "compression_ratio": 1.7203065134099618, "no_speech_prob": 2.977203439513687e-05}, {"id": 867, "seek": 388060, "start": 3901.92, "end": 3906.12, "text": " A lot of people have been going down that path.", "tokens": [316, 688, 295, 561, 362, 668, 516, 760, 300, 3100, 13], "temperature": 0.0, "avg_logprob": -0.3672880664948494, "compression_ratio": 1.7203065134099618, "no_speech_prob": 2.977203439513687e-05}, {"id": 868, "seek": 388060, "start": 3906.12, "end": 3907.72, "text": " It would have to be fast.", "tokens": [467, 576, 362, 281, 312, 2370, 13], "temperature": 0.0, "avg_logprob": -0.3672880664948494, "compression_ratio": 1.7203065134099618, "no_speech_prob": 2.977203439513687e-05}, {"id": 869, "seek": 390772, "start": 3907.72, "end": 3911.64, "text": " So GLMnet would be a good example because that's very fast.", "tokens": [407, 16225, 44, 7129, 576, 312, 257, 665, 1365, 570, 300, 311, 588, 2370, 13], "temperature": 0.0, "avg_logprob": -0.1557626630745682, "compression_ratio": 1.6022304832713754, "no_speech_prob": 5.33806269231718e-06}, {"id": 870, "seek": 390772, "start": 3911.64, "end": 3916.48, "text": " But GLMnet is parametric.", "tokens": [583, 16225, 44, 7129, 307, 6220, 17475, 13], "temperature": 0.0, "avg_logprob": -0.1557626630745682, "compression_ratio": 1.6022304832713754, "no_speech_prob": 5.33806269231718e-06}, {"id": 871, "seek": 390772, "start": 3916.48, "end": 3921.12, "text": " The nice thing about decision trees is that they're totally flexible.", "tokens": [440, 1481, 551, 466, 3537, 5852, 307, 300, 436, 434, 3879, 11358, 13], "temperature": 0.0, "avg_logprob": -0.1557626630745682, "compression_ratio": 1.6022304832713754, "no_speech_prob": 5.33806269231718e-06}, {"id": 872, "seek": 390772, "start": 3921.12, "end": 3923.56, "text": " They don't assume any particular data structure.", "tokens": [814, 500, 380, 6552, 604, 1729, 1412, 3877, 13], "temperature": 0.0, "avg_logprob": -0.1557626630745682, "compression_ratio": 1.6022304832713754, "no_speech_prob": 5.33806269231718e-06}, {"id": 873, "seek": 390772, "start": 3923.56, "end": 3929.3999999999996, "text": " They kind of are almost unlimited in the amount of interactions they can handle.", "tokens": [814, 733, 295, 366, 1920, 21950, 294, 264, 2372, 295, 13280, 436, 393, 4813, 13], "temperature": 0.0, "avg_logprob": -0.1557626630745682, "compression_ratio": 1.6022304832713754, "no_speech_prob": 5.33806269231718e-06}, {"id": 874, "seek": 390772, "start": 3929.3999999999996, "end": 3932.16, "text": " And you can build thousands of them very quickly.", "tokens": [400, 291, 393, 1322, 5383, 295, 552, 588, 2661, 13], "temperature": 0.0, "avg_logprob": -0.1557626630745682, "compression_ratio": 1.6022304832713754, "no_speech_prob": 5.33806269231718e-06}, {"id": 875, "seek": 390772, "start": 3932.16, "end": 3936.7599999999998, "text": " But there are certainly people who are creating other types of random subspace ensemble methods", "tokens": [583, 456, 366, 3297, 561, 567, 366, 4084, 661, 3467, 295, 4974, 2090, 17940, 19492, 7150], "temperature": 0.0, "avg_logprob": -0.1557626630745682, "compression_ratio": 1.6022304832713754, "no_speech_prob": 5.33806269231718e-06}, {"id": 876, "seek": 393676, "start": 3936.76, "end": 3941.0400000000004, "text": " and, I believe, some of them are quite effective.", "tokens": [293, 11, 286, 1697, 11, 512, 295, 552, 366, 1596, 4942, 13], "temperature": 0.0, "avg_logprob": -0.11137750413682726, "compression_ratio": 1.739463601532567, "no_speech_prob": 9.08024321688572e-06}, {"id": 877, "seek": 393676, "start": 3941.0400000000004, "end": 3946.44, "text": " Interestingly, I can't remember where I saw it, but I have seen some papers which show", "tokens": [30564, 11, 286, 393, 380, 1604, 689, 286, 1866, 309, 11, 457, 286, 362, 1612, 512, 10577, 597, 855], "temperature": 0.0, "avg_logprob": -0.11137750413682726, "compression_ratio": 1.739463601532567, "no_speech_prob": 9.08024321688572e-06}, {"id": 878, "seek": 393676, "start": 3946.44, "end": 3949.7200000000003, "text": " evidence that it doesn't really matter.", "tokens": [4467, 300, 309, 1177, 380, 534, 1871, 13], "temperature": 0.0, "avg_logprob": -0.11137750413682726, "compression_ratio": 1.739463601532567, "no_speech_prob": 9.08024321688572e-06}, {"id": 879, "seek": 393676, "start": 3949.7200000000003, "end": 3955.92, "text": " If you've got a truly flexible underlying model and you make it random enough and you", "tokens": [759, 291, 600, 658, 257, 4908, 11358, 14217, 2316, 293, 291, 652, 309, 4974, 1547, 293, 291], "temperature": 0.0, "avg_logprob": -0.11137750413682726, "compression_ratio": 1.739463601532567, "no_speech_prob": 9.08024321688572e-06}, {"id": 880, "seek": 393676, "start": 3955.92, "end": 3960.44, "text": " create enough of them, it doesn't really matter which one you use or how you do it, which", "tokens": [1884, 1547, 295, 552, 11, 309, 1177, 380, 534, 1871, 597, 472, 291, 764, 420, 577, 291, 360, 309, 11, 597], "temperature": 0.0, "avg_logprob": -0.11137750413682726, "compression_ratio": 1.739463601532567, "no_speech_prob": 9.08024321688572e-06}, {"id": 881, "seek": 393676, "start": 3960.44, "end": 3961.44, "text": " is a nice result.", "tokens": [307, 257, 1481, 1874, 13], "temperature": 0.0, "avg_logprob": -0.11137750413682726, "compression_ratio": 1.739463601532567, "no_speech_prob": 9.08024321688572e-06}, {"id": 882, "seek": 393676, "start": 3961.44, "end": 3965.7200000000003, "text": " It kind of suggests that we don't have to spend lots of time trying to come up with", "tokens": [467, 733, 295, 13409, 300, 321, 500, 380, 362, 281, 3496, 3195, 295, 565, 1382, 281, 808, 493, 365], "temperature": 0.0, "avg_logprob": -0.11137750413682726, "compression_ratio": 1.739463601532567, "no_speech_prob": 9.08024321688572e-06}, {"id": 883, "seek": 396572, "start": 3965.72, "end": 3971.9199999999996, "text": " better and better and better predictive modeling, generic predictive modeling tools.", "tokens": [1101, 293, 1101, 293, 1101, 35521, 15983, 11, 19577, 35521, 15983, 3873, 13], "temperature": 0.0, "avg_logprob": -0.1799616813659668, "compression_ratio": 1.7312252964426877, "no_speech_prob": 7.251476927194744e-05}, {"id": 884, "seek": 396572, "start": 3971.9199999999996, "end": 3976.12, "text": " If you think about things, there are better versions of this, in quotes, like a rotation", "tokens": [759, 291, 519, 466, 721, 11, 456, 366, 1101, 9606, 295, 341, 11, 294, 19963, 11, 411, 257, 12447], "temperature": 0.0, "avg_logprob": -0.1799616813659668, "compression_ratio": 1.7312252964426877, "no_speech_prob": 7.251476927194744e-05}, {"id": 885, "seek": 396572, "start": 3976.12, "end": 3980.48, "text": " forest and then there's things like GBMs, gradient boosting machines and so forth.", "tokens": [6719, 293, 550, 456, 311, 721, 411, 460, 18345, 82, 11, 16235, 43117, 8379, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1799616813659668, "compression_ratio": 1.7312252964426877, "no_speech_prob": 7.251476927194744e-05}, {"id": 886, "seek": 396572, "start": 3980.48, "end": 3985.72, "text": " In practice, they can be faster for certain types of situation.", "tokens": [682, 3124, 11, 436, 393, 312, 4663, 337, 1629, 3467, 295, 2590, 13], "temperature": 0.0, "avg_logprob": -0.1799616813659668, "compression_ratio": 1.7312252964426877, "no_speech_prob": 7.251476927194744e-05}, {"id": 887, "seek": 396572, "start": 3985.72, "end": 3990.6, "text": " But the general result here is that these ensemble methods are as flexible as you need", "tokens": [583, 264, 2674, 1874, 510, 307, 300, 613, 19492, 7150, 366, 382, 11358, 382, 291, 643], "temperature": 0.0, "avg_logprob": -0.1799616813659668, "compression_ratio": 1.7312252964426877, "no_speech_prob": 7.251476927194744e-05}, {"id": 888, "seek": 396572, "start": 3990.6, "end": 3991.6, "text": " them to be.", "tokens": [552, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.1799616813659668, "compression_ratio": 1.7312252964426877, "no_speech_prob": 7.251476927194744e-05}, {"id": 889, "seek": 396572, "start": 3991.6, "end": 3992.6, "text": " Right at the back.", "tokens": [1779, 412, 264, 646, 13], "temperature": 0.0, "avg_logprob": -0.1799616813659668, "compression_ratio": 1.7312252964426877, "no_speech_prob": 7.251476927194744e-05}, {"id": 890, "seek": 399260, "start": 3992.6, "end": 3997.72, "text": " How do you define the optimal size of the subspace?", "tokens": [1012, 360, 291, 6964, 264, 16252, 2744, 295, 264, 2090, 17940, 30], "temperature": 0.0, "avg_logprob": -0.1885351068833295, "compression_ratio": 1.7988826815642458, "no_speech_prob": 4.2640778701752424e-05}, {"id": 891, "seek": 399260, "start": 3997.72, "end": 4001.0, "text": " So the question is how do you define the optimal size of the subspace?", "tokens": [407, 264, 1168, 307, 577, 360, 291, 6964, 264, 16252, 2744, 295, 264, 2090, 17940, 30], "temperature": 0.0, "avg_logprob": -0.1885351068833295, "compression_ratio": 1.7988826815642458, "no_speech_prob": 4.2640778701752424e-05}, {"id": 892, "seek": 399260, "start": 4001.0, "end": 4003.2799999999997, "text": " And that's a really terrific question.", "tokens": [400, 300, 311, 257, 534, 20899, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1885351068833295, "compression_ratio": 1.7988826815642458, "no_speech_prob": 4.2640778701752424e-05}, {"id": 893, "seek": 399260, "start": 4003.2799999999997, "end": 4009.68, "text": " The answer to it is really nice and it's that you really don't have to.", "tokens": [440, 1867, 281, 309, 307, 534, 1481, 293, 309, 311, 300, 291, 534, 500, 380, 362, 281, 13], "temperature": 0.0, "avg_logprob": -0.1885351068833295, "compression_ratio": 1.7988826815642458, "no_speech_prob": 4.2640778701752424e-05}, {"id": 894, "seek": 399260, "start": 4009.68, "end": 4017.7999999999997, "text": " Generally speaking, the less rows and the less columns you use, the more trees you need,", "tokens": [21082, 4124, 11, 264, 1570, 13241, 293, 264, 1570, 13766, 291, 764, 11, 264, 544, 5852, 291, 643, 11], "temperature": 0.0, "avg_logprob": -0.1885351068833295, "compression_ratio": 1.7988826815642458, "no_speech_prob": 4.2640778701752424e-05}, {"id": 895, "seek": 401780, "start": 4017.8, "end": 4022.8, "text": " the less you'll overfit and the better results you'll get.", "tokens": [264, 1570, 291, 603, 670, 6845, 293, 264, 1101, 3542, 291, 603, 483, 13], "temperature": 0.0, "avg_logprob": -0.1426937324660165, "compression_ratio": 1.7153846153846153, "no_speech_prob": 1.406358387612272e-05}, {"id": 896, "seek": 401780, "start": 4022.8, "end": 4027.2400000000002, "text": " The nice thing normally is that for most data sets, because of the speed of random forest,", "tokens": [440, 1481, 551, 5646, 307, 300, 337, 881, 1412, 6352, 11, 570, 295, 264, 3073, 295, 4974, 6719, 11], "temperature": 0.0, "avg_logprob": -0.1426937324660165, "compression_ratio": 1.7153846153846153, "no_speech_prob": 1.406358387612272e-05}, {"id": 897, "seek": 401780, "start": 4027.2400000000002, "end": 4032.32, "text": " you can pretty much always pick a row count and a column count that's small enough that", "tokens": [291, 393, 1238, 709, 1009, 1888, 257, 5386, 1207, 293, 257, 7738, 1207, 300, 311, 1359, 1547, 300], "temperature": 0.0, "avg_logprob": -0.1426937324660165, "compression_ratio": 1.7153846153846153, "no_speech_prob": 1.406358387612272e-05}, {"id": 898, "seek": 401780, "start": 4032.32, "end": 4036.48, "text": " you're absolutely sure it's going to be fine.", "tokens": [291, 434, 3122, 988, 309, 311, 516, 281, 312, 2489, 13], "temperature": 0.0, "avg_logprob": -0.1426937324660165, "compression_ratio": 1.7153846153846153, "no_speech_prob": 1.406358387612272e-05}, {"id": 899, "seek": 401780, "start": 4036.48, "end": 4037.48, "text": " Sometimes it can become an issue.", "tokens": [4803, 309, 393, 1813, 364, 2734, 13], "temperature": 0.0, "avg_logprob": -0.1426937324660165, "compression_ratio": 1.7153846153846153, "no_speech_prob": 1.406358387612272e-05}, {"id": 900, "seek": 401780, "start": 4037.48, "end": 4042.6000000000004, "text": " Maybe you've got really huge data sets or maybe you've got really big problems with", "tokens": [2704, 291, 600, 658, 534, 2603, 1412, 6352, 420, 1310, 291, 600, 658, 534, 955, 2740, 365], "temperature": 0.0, "avg_logprob": -0.1426937324660165, "compression_ratio": 1.7153846153846153, "no_speech_prob": 1.406358387612272e-05}, {"id": 901, "seek": 401780, "start": 4042.6000000000004, "end": 4046.5600000000004, "text": " data imbalances or hardly any training data.", "tokens": [1412, 566, 2645, 2676, 420, 13572, 604, 3097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1426937324660165, "compression_ratio": 1.7153846153846153, "no_speech_prob": 1.406358387612272e-05}, {"id": 902, "seek": 404656, "start": 4046.56, "end": 4051.0, "text": " And in these cases, you can use the kind of approaches which would be familiar to most", "tokens": [400, 294, 613, 3331, 11, 291, 393, 764, 264, 733, 295, 11587, 597, 576, 312, 4963, 281, 881], "temperature": 0.0, "avg_logprob": -0.11080107528172183, "compression_ratio": 1.641255605381166, "no_speech_prob": 2.5864530471153557e-05}, {"id": 903, "seek": 404656, "start": 4051.0, "end": 4059.04, "text": " of us around creating a grid of a few different values of the column count and the row count", "tokens": [295, 505, 926, 4084, 257, 10748, 295, 257, 1326, 819, 4190, 295, 264, 7738, 1207, 293, 264, 5386, 1207], "temperature": 0.0, "avg_logprob": -0.11080107528172183, "compression_ratio": 1.641255605381166, "no_speech_prob": 2.5864530471153557e-05}, {"id": 904, "seek": 404656, "start": 4059.04, "end": 4066.16, "text": " and trying a few out and watching that graph of as you add more trees, how does it improve?", "tokens": [293, 1382, 257, 1326, 484, 293, 1976, 300, 4295, 295, 382, 291, 909, 544, 5852, 11, 577, 775, 309, 3470, 30], "temperature": 0.0, "avg_logprob": -0.11080107528172183, "compression_ratio": 1.641255605381166, "no_speech_prob": 2.5864530471153557e-05}, {"id": 905, "seek": 404656, "start": 4066.16, "end": 4076.54, "text": " But the truth is it's so unsensitive to this that if you pick a number of columns of somewhere", "tokens": [583, 264, 3494, 307, 309, 311, 370, 2693, 34465, 281, 341, 300, 498, 291, 1888, 257, 1230, 295, 13766, 295, 4079], "temperature": 0.0, "avg_logprob": -0.11080107528172183, "compression_ratio": 1.641255605381166, "no_speech_prob": 2.5864530471153557e-05}, {"id": 906, "seek": 407654, "start": 4076.54, "end": 4087.52, "text": " between 10% and 50% of the total and a number of rows of between 10% and 50% of the total,", "tokens": [1296, 1266, 4, 293, 2625, 4, 295, 264, 3217, 293, 257, 1230, 295, 13241, 295, 1296, 1266, 4, 293, 2625, 4, 295, 264, 3217, 11], "temperature": 0.0, "avg_logprob": -0.22581763977700092, "compression_ratio": 1.631578947368421, "no_speech_prob": 4.831839760299772e-05}, {"id": 907, "seek": 407654, "start": 4087.52, "end": 4088.52, "text": " and you'll be fine.", "tokens": [293, 291, 603, 312, 2489, 13], "temperature": 0.0, "avg_logprob": -0.22581763977700092, "compression_ratio": 1.631578947368421, "no_speech_prob": 4.831839760299772e-05}, {"id": 908, "seek": 407654, "start": 4088.52, "end": 4094.04, "text": " And then you just keep adding more trees until either you're sick of waiting or it's obviously", "tokens": [400, 550, 291, 445, 1066, 5127, 544, 5852, 1826, 2139, 291, 434, 4998, 295, 3806, 420, 309, 311, 2745], "temperature": 0.0, "avg_logprob": -0.22581763977700092, "compression_ratio": 1.631578947368421, "no_speech_prob": 4.831839760299772e-05}, {"id": 909, "seek": 407654, "start": 4094.04, "end": 4102.24, "text": " flat or if you do a thousand trees, again, these are all, it really doesn't matter.", "tokens": [4962, 420, 498, 291, 360, 257, 4714, 5852, 11, 797, 11, 613, 366, 439, 11, 309, 534, 1177, 380, 1871, 13], "temperature": 0.0, "avg_logprob": -0.22581763977700092, "compression_ratio": 1.631578947368421, "no_speech_prob": 4.831839760299772e-05}, {"id": 910, "seek": 407654, "start": 4102.24, "end": 4105.2, "text": " It's not sensitive to that assumption on the whole.", "tokens": [467, 311, 406, 9477, 281, 300, 15302, 322, 264, 1379, 13], "temperature": 0.0, "avg_logprob": -0.22581763977700092, "compression_ratio": 1.631578947368421, "no_speech_prob": 4.831839760299772e-05}, {"id": 911, "seek": 410520, "start": 4105.2, "end": 4108.2, "text": " The R routine samples with replacement.", "tokens": [440, 497, 9927, 10938, 365, 14419, 13], "temperature": 0.0, "avg_logprob": -0.26019351378731104, "compression_ratio": 1.7201834862385321, "no_speech_prob": 3.9440074033336714e-05}, {"id": 912, "seek": 410520, "start": 4108.2, "end": 4114.2, "text": " Yeah, the R routine actually, the R routine actually, so this idea of a random subspace,", "tokens": [865, 11, 264, 497, 9927, 767, 11, 264, 497, 9927, 767, 11, 370, 341, 1558, 295, 257, 4974, 2090, 17940, 11], "temperature": 0.0, "avg_logprob": -0.26019351378731104, "compression_ratio": 1.7201834862385321, "no_speech_prob": 3.9440074033336714e-05}, {"id": 913, "seek": 410520, "start": 4114.2, "end": 4118.24, "text": " there are different ways of creating this random subspace.", "tokens": [456, 366, 819, 2098, 295, 4084, 341, 4974, 2090, 17940, 13], "temperature": 0.0, "avg_logprob": -0.26019351378731104, "compression_ratio": 1.7201834862385321, "no_speech_prob": 3.9440074033336714e-05}, {"id": 914, "seek": 410520, "start": 4118.24, "end": 4126.2, "text": " And one key one is can I go out and pull out a row again that I've already pulled out before?", "tokens": [400, 472, 2141, 472, 307, 393, 286, 352, 484, 293, 2235, 484, 257, 5386, 797, 300, 286, 600, 1217, 7373, 484, 949, 30], "temperature": 0.0, "avg_logprob": -0.26019351378731104, "compression_ratio": 1.7201834862385321, "no_speech_prob": 3.9440074033336714e-05}, {"id": 915, "seek": 410520, "start": 4126.2, "end": 4131.96, "text": " The R random forest and the portrait encoder which is based by default let you pull something", "tokens": [440, 497, 4974, 6719, 293, 264, 17126, 2058, 19866, 597, 307, 2361, 538, 7576, 718, 291, 2235, 746], "temperature": 0.0, "avg_logprob": -0.26019351378731104, "compression_ratio": 1.7201834862385321, "no_speech_prob": 3.9440074033336714e-05}, {"id": 916, "seek": 413196, "start": 4131.96, "end": 4138.0, "text": " out multiple times and by default in fact pull out if you've got n rows, it will pull", "tokens": [484, 3866, 1413, 293, 538, 7576, 294, 1186, 2235, 484, 498, 291, 600, 658, 297, 13241, 11, 309, 486, 2235], "temperature": 0.0, "avg_logprob": -0.2733186967302077, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.5205772797344252e-05}, {"id": 917, "seek": 413196, "start": 4138.0, "end": 4139.0, "text": " out n rows.", "tokens": [484, 297, 13241, 13], "temperature": 0.0, "avg_logprob": -0.2733186967302077, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.5205772797344252e-05}, {"id": 918, "seek": 413196, "start": 4139.0, "end": 4144.84, "text": " But because it's pulled out some multiple times on average it will cover I think 63.2%", "tokens": [583, 570, 309, 311, 7373, 484, 512, 3866, 1413, 322, 4274, 309, 486, 2060, 286, 519, 25082, 13, 17, 4], "temperature": 0.0, "avg_logprob": -0.2733186967302077, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.5205772797344252e-05}, {"id": 919, "seek": 413196, "start": 4144.84, "end": 4148.4800000000005, "text": " of the rows.", "tokens": [295, 264, 13241, 13], "temperature": 0.0, "avg_logprob": -0.2733186967302077, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.5205772797344252e-05}, {"id": 920, "seek": 413196, "start": 4148.4800000000005, "end": 4154.32, "text": " I don't have the best results when I use that, but it doesn't matter because in R random", "tokens": [286, 500, 380, 362, 264, 1151, 3542, 562, 286, 764, 300, 11, 457, 309, 1177, 380, 1871, 570, 294, 497, 4974], "temperature": 0.0, "avg_logprob": -0.2733186967302077, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.5205772797344252e-05}, {"id": 921, "seek": 413196, "start": 4154.32, "end": 4159.88, "text": " forest options you can choose is it with or without sampling and how many tables is in.", "tokens": [6719, 3956, 291, 393, 2826, 307, 309, 365, 420, 1553, 21179, 293, 577, 867, 8020, 307, 294, 13], "temperature": 0.0, "avg_logprob": -0.2733186967302077, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.5205772797344252e-05}, {"id": 922, "seek": 415988, "start": 4159.88, "end": 4163.84, "text": " Yes, I absolutely think it makes a difference.", "tokens": [1079, 11, 286, 3122, 519, 309, 1669, 257, 2649, 13], "temperature": 0.0, "avg_logprob": -0.21302234209500825, "compression_ratio": 1.6171875, "no_speech_prob": 2.5863868359010667e-05}, {"id": 923, "seek": 415988, "start": 4163.84, "end": 4169.68, "text": " Yes to me, I'm sure it depends on the data set, but you know, I guess I always enter", "tokens": [1079, 281, 385, 11, 286, 478, 988, 309, 5946, 322, 264, 1412, 992, 11, 457, 291, 458, 11, 286, 2041, 286, 1009, 3242], "temperature": 0.0, "avg_logprob": -0.21302234209500825, "compression_ratio": 1.6171875, "no_speech_prob": 2.5863868359010667e-05}, {"id": 924, "seek": 415988, "start": 4169.68, "end": 4174.64, "text": " Kaggle competitions which are in areas that I've never entered before kind of domain wise", "tokens": [48751, 22631, 26185, 597, 366, 294, 3179, 300, 286, 600, 1128, 9065, 949, 733, 295, 9274, 10829], "temperature": 0.0, "avg_logprob": -0.21302234209500825, "compression_ratio": 1.6171875, "no_speech_prob": 2.5863868359010667e-05}, {"id": 925, "seek": 415988, "start": 4174.64, "end": 4176.72, "text": " or algorithm wise.", "tokens": [420, 9284, 10829, 13], "temperature": 0.0, "avg_logprob": -0.21302234209500825, "compression_ratio": 1.6171875, "no_speech_prob": 2.5863868359010667e-05}, {"id": 926, "seek": 415988, "start": 4176.72, "end": 4181.4800000000005, "text": " So I guess I'd be getting a good spread of different types of situation and in the ones", "tokens": [407, 286, 2041, 286, 1116, 312, 1242, 257, 665, 3974, 295, 819, 3467, 295, 2590, 293, 294, 264, 2306], "temperature": 0.0, "avg_logprob": -0.21302234209500825, "compression_ratio": 1.6171875, "no_speech_prob": 2.5863868359010667e-05}, {"id": 927, "seek": 415988, "start": 4181.4800000000005, "end": 4188.400000000001, "text": " I've looked at sampling without replacement is kind of more random and I also tend to", "tokens": [286, 600, 2956, 412, 21179, 1553, 14419, 307, 733, 295, 544, 4974, 293, 286, 611, 3928, 281], "temperature": 0.0, "avg_logprob": -0.21302234209500825, "compression_ratio": 1.6171875, "no_speech_prob": 2.5863868359010667e-05}, {"id": 928, "seek": 418840, "start": 4188.4, "end": 4191.2, "text": " pick much lower n than 63.2%.", "tokens": [1888, 709, 3126, 297, 813, 25082, 13, 17, 6856], "temperature": 0.0, "avg_logprob": -0.3446817789992241, "compression_ratio": 1.3837209302325582, "no_speech_prob": 7.25312638678588e-05}, {"id": 929, "seek": 418840, "start": 4191.2, "end": 4195.879999999999, "text": " I tend to use more like 10 or 20% of the data in my random subspecies.", "tokens": [286, 3928, 281, 764, 544, 411, 1266, 420, 945, 4, 295, 264, 1412, 294, 452, 4974, 2090, 494, 4629, 13], "temperature": 0.0, "avg_logprob": -0.3446817789992241, "compression_ratio": 1.3837209302325582, "no_speech_prob": 7.25312638678588e-05}, {"id": 930, "seek": 418840, "start": 4195.879999999999, "end": 4213.0, "text": " So I guess I can say that's my experience, but I'm sure it depends on the data set and", "tokens": [407, 286, 2041, 286, 393, 584, 300, 311, 452, 1752, 11, 457, 286, 478, 988, 309, 5946, 322, 264, 1412, 992, 293], "temperature": 0.0, "avg_logprob": -0.3446817789992241, "compression_ratio": 1.3837209302325582, "no_speech_prob": 7.25312638678588e-05}, {"id": 931, "seek": 418840, "start": 4213.0, "end": 4218.2, "text": " I'm not sure it's terribly sensitive to it anyway.", "tokens": [286, 478, 406, 988, 309, 311, 22903, 9477, 281, 309, 4033, 13], "temperature": 0.0, "avg_logprob": -0.3446817789992241, "compression_ratio": 1.3837209302325582, "no_speech_prob": 7.25312638678588e-05}, {"id": 932, "seek": 421820, "start": 4218.2, "end": 4224.88, "text": " I always put it into two branches.", "tokens": [286, 1009, 829, 309, 666, 732, 14770, 13], "temperature": 0.0, "avg_logprob": -0.2168304443359375, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.00035101064713671803}, {"id": 933, "seek": 421820, "start": 4224.88, "end": 4230.8, "text": " So there's a few possibilities here as you split in your decision tree.", "tokens": [407, 456, 311, 257, 1326, 12178, 510, 382, 291, 7472, 294, 428, 3537, 4230, 13], "temperature": 0.0, "avg_logprob": -0.2168304443359375, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.00035101064713671803}, {"id": 934, "seek": 421820, "start": 4230.8, "end": 4236.76, "text": " In this case, I've got something here which is a binary variable.", "tokens": [682, 341, 1389, 11, 286, 600, 658, 746, 510, 597, 307, 257, 17434, 7006, 13], "temperature": 0.0, "avg_logprob": -0.2168304443359375, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.00035101064713671803}, {"id": 935, "seek": 421820, "start": 4236.76, "end": 4238.96, "text": " So obviously that has to be fit into two.", "tokens": [407, 2745, 300, 575, 281, 312, 3318, 666, 732, 13], "temperature": 0.0, "avg_logprob": -0.2168304443359375, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.00035101064713671803}, {"id": 936, "seek": 421820, "start": 4238.96, "end": 4242.8, "text": " In this case, I've got something here which is a continuous variable.", "tokens": [682, 341, 1389, 11, 286, 600, 658, 746, 510, 597, 307, 257, 10957, 7006, 13], "temperature": 0.0, "avg_logprob": -0.2168304443359375, "compression_ratio": 1.7530864197530864, "no_speech_prob": 0.00035101064713671803}, {"id": 937, "seek": 424280, "start": 4242.8, "end": 4248.400000000001, "text": " Now I split it into two, but if actually it's going to be optimal to split it into three,", "tokens": [823, 286, 7472, 309, 666, 732, 11, 457, 498, 767, 309, 311, 516, 281, 312, 16252, 281, 7472, 309, 666, 1045, 11], "temperature": 0.0, "avg_logprob": -0.2394806510523746, "compression_ratio": 1.6574074074074074, "no_speech_prob": 2.429825508443173e-05}, {"id": 938, "seek": 424280, "start": 4248.400000000001, "end": 4252.4400000000005, "text": " then if the variable appears again at the next level, it can always split it into another", "tokens": [550, 498, 264, 7006, 7038, 797, 412, 264, 958, 1496, 11, 309, 393, 1009, 7472, 309, 666, 1071], "temperature": 0.0, "avg_logprob": -0.2394806510523746, "compression_ratio": 1.6574074074074074, "no_speech_prob": 2.429825508443173e-05}, {"id": 939, "seek": 424280, "start": 4252.4400000000005, "end": 4255.8, "text": " two at that other split point.", "tokens": [732, 412, 300, 661, 7472, 935, 13], "temperature": 0.0, "avg_logprob": -0.2394806510523746, "compression_ratio": 1.6574074074074074, "no_speech_prob": 2.429825508443173e-05}, {"id": 940, "seek": 424280, "start": 4255.8, "end": 4260.68, "text": " I can.", "tokens": [286, 393, 13], "temperature": 0.0, "avg_logprob": -0.2394806510523746, "compression_ratio": 1.6574074074074074, "no_speech_prob": 2.429825508443173e-05}, {"id": 941, "seek": 424280, "start": 4260.68, "end": 4261.68, "text": " Absolutely I can.", "tokens": [7021, 286, 393, 13], "temperature": 0.0, "avg_logprob": -0.2394806510523746, "compression_ratio": 1.6574074074074074, "no_speech_prob": 2.429825508443173e-05}, {"id": 942, "seek": 424280, "start": 4261.68, "end": 4267.92, "text": " So it just depends whether when I did that, remember at every level I repeat the sampling", "tokens": [407, 309, 445, 5946, 1968, 562, 286, 630, 300, 11, 1604, 412, 633, 1496, 286, 7149, 264, 21179], "temperature": 0.0, "avg_logprob": -0.2394806510523746, "compression_ratio": 1.6574074074074074, "no_speech_prob": 2.429825508443173e-05}, {"id": 943, "seek": 424280, "start": 4267.92, "end": 4269.92, "text": " of a different bunch of columns.", "tokens": [295, 257, 819, 3840, 295, 13766, 13], "temperature": 0.0, "avg_logprob": -0.2394806510523746, "compression_ratio": 1.6574074074074074, "no_speech_prob": 2.429825508443173e-05}, {"id": 944, "seek": 426992, "start": 4269.92, "end": 4274.4800000000005, "text": " I could absolutely have the same column again in that group and it could so happen that", "tokens": [286, 727, 3122, 362, 264, 912, 7738, 797, 294, 300, 1594, 293, 309, 727, 370, 1051, 300], "temperature": 0.0, "avg_logprob": -0.15521206458409628, "compression_ratio": 1.6781115879828326, "no_speech_prob": 1.834152135415934e-05}, {"id": 945, "seek": 426992, "start": 4274.4800000000005, "end": 4277.32, "text": " again I find the split point which is the best in that group.", "tokens": [797, 286, 915, 264, 7472, 935, 597, 307, 264, 1151, 294, 300, 1594, 13], "temperature": 0.0, "avg_logprob": -0.15521206458409628, "compression_ratio": 1.6781115879828326, "no_speech_prob": 1.834152135415934e-05}, {"id": 946, "seek": 426992, "start": 4277.32, "end": 4282.76, "text": " If you're doing 10,000 trees with 100 levels each, it's going to happen lots of times.", "tokens": [759, 291, 434, 884, 1266, 11, 1360, 5852, 365, 2319, 4358, 1184, 11, 309, 311, 516, 281, 1051, 3195, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.15521206458409628, "compression_ratio": 1.6781115879828326, "no_speech_prob": 1.834152135415934e-05}, {"id": 947, "seek": 426992, "start": 4282.76, "end": 4290.52, "text": " So the nice thing is that if the true underlying system is a single univariate logarithmic", "tokens": [407, 264, 1481, 551, 307, 300, 498, 264, 2074, 14217, 1185, 307, 257, 2167, 517, 592, 3504, 473, 41473, 355, 13195], "temperature": 0.0, "avg_logprob": -0.15521206458409628, "compression_ratio": 1.6781115879828326, "no_speech_prob": 1.834152135415934e-05}, {"id": 948, "seek": 426992, "start": 4290.52, "end": 4296.4800000000005, "text": " relationship, these trees will absolutely find that eventually.", "tokens": [2480, 11, 613, 5852, 486, 3122, 915, 300, 4728, 13], "temperature": 0.0, "avg_logprob": -0.15521206458409628, "compression_ratio": 1.6781115879828326, "no_speech_prob": 1.834152135415934e-05}, {"id": 949, "seek": 429648, "start": 4296.48, "end": 4300.5199999999995, "text": " Do you care about pruning the trees?", "tokens": [1144, 291, 1127, 466, 582, 37726, 264, 5852, 30], "temperature": 0.0, "avg_logprob": -0.46233909074650253, "compression_ratio": 1.6593406593406594, "no_speech_prob": 1.7230253433808684e-05}, {"id": 950, "seek": 429648, "start": 4300.5199999999995, "end": 4301.5199999999995, "text": " Definitely don't prune the trees.", "tokens": [12151, 500, 380, 582, 2613, 264, 5852, 13], "temperature": 0.0, "avg_logprob": -0.46233909074650253, "compression_ratio": 1.6593406593406594, "no_speech_prob": 1.7230253433808684e-05}, {"id": 951, "seek": 429648, "start": 4301.5199999999995, "end": 4304.599999999999, "text": " If you prune the trees, you introduce bias.", "tokens": [759, 291, 582, 2613, 264, 5852, 11, 291, 5366, 12577, 13], "temperature": 0.0, "avg_logprob": -0.46233909074650253, "compression_ratio": 1.6593406593406594, "no_speech_prob": 1.7230253433808684e-05}, {"id": 952, "seek": 429648, "start": 4304.599999999999, "end": 4309.679999999999, "text": " So the key thing here which makes this so fast and so easy but also so powerful is you", "tokens": [407, 264, 2141, 551, 510, 597, 1669, 341, 370, 2370, 293, 370, 1858, 457, 611, 370, 4005, 307, 291], "temperature": 0.0, "avg_logprob": -0.46233909074650253, "compression_ratio": 1.6593406593406594, "no_speech_prob": 1.7230253433808684e-05}, {"id": 953, "seek": 429648, "start": 4309.679999999999, "end": 4310.679999999999, "text": " don't prune the trees.", "tokens": [500, 380, 582, 2613, 264, 5852, 13], "temperature": 0.0, "avg_logprob": -0.46233909074650253, "compression_ratio": 1.6593406593406594, "no_speech_prob": 1.7230253433808684e-05}, {"id": 954, "seek": 429648, "start": 4310.679999999999, "end": 4317.879999999999, "text": " The other thing is that the serum is usually not going to be able to do that.", "tokens": [440, 661, 551, 307, 300, 264, 32755, 307, 2673, 406, 516, 281, 312, 1075, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.46233909074650253, "compression_ratio": 1.6593406593406594, "no_speech_prob": 1.7230253433808684e-05}, {"id": 955, "seek": 431788, "start": 4317.88, "end": 4327.04, "text": " So it doesn't necessarily because your split point will be such that the two halves will", "tokens": [407, 309, 1177, 380, 4725, 570, 428, 7472, 935, 486, 312, 1270, 300, 264, 732, 38490, 486], "temperature": 0.0, "avg_logprob": -0.28534268110226363, "compression_ratio": 1.643979057591623, "no_speech_prob": 6.240466973395087e-06}, {"id": 956, "seek": 431788, "start": 4327.04, "end": 4329.76, "text": " not necessarily be balanced in count.", "tokens": [406, 4725, 312, 13902, 294, 1207, 13], "temperature": 0.0, "avg_logprob": -0.28534268110226363, "compression_ratio": 1.643979057591623, "no_speech_prob": 6.240466973395087e-06}, {"id": 957, "seek": 431788, "start": 4329.76, "end": 4333.12, "text": " In 17 not in 15.", "tokens": [682, 3282, 406, 294, 2119, 13], "temperature": 0.0, "avg_logprob": -0.28534268110226363, "compression_ratio": 1.643979057591623, "no_speech_prob": 6.240466973395087e-06}, {"id": 958, "seek": 431788, "start": 4333.12, "end": 4341.2, "text": " Yeah that's right because in the under 18 group you could have not that many people", "tokens": [865, 300, 311, 558, 570, 294, 264, 833, 2443, 1594, 291, 727, 362, 406, 300, 867, 561], "temperature": 0.0, "avg_logprob": -0.28534268110226363, "compression_ratio": 1.643979057591623, "no_speech_prob": 6.240466973395087e-06}, {"id": 959, "seek": 431788, "start": 4341.2, "end": 4345.16, "text": " and in the over 18 group you can have quite a lot of people so the weighted average of", "tokens": [293, 294, 264, 670, 2443, 1594, 291, 393, 362, 1596, 257, 688, 295, 561, 370, 264, 32807, 4274, 295], "temperature": 0.0, "avg_logprob": -0.28534268110226363, "compression_ratio": 1.643979057591623, "no_speech_prob": 6.240466973395087e-06}, {"id": 960, "seek": 434516, "start": 4345.16, "end": 4348.16, "text": " the two will come to 17.", "tokens": [264, 732, 486, 808, 281, 3282, 13], "temperature": 0.0, "avg_logprob": -0.33244501460682263, "compression_ratio": 1.7569060773480663, "no_speech_prob": 2.840615707100369e-05}, {"id": 961, "seek": 434516, "start": 4348.16, "end": 4354.48, "text": " Have you ever compared this with Gradient Boosting Machines?", "tokens": [3560, 291, 1562, 5347, 341, 365, 16710, 1196, 43902, 278, 12089, 1652, 30], "temperature": 0.0, "avg_logprob": -0.33244501460682263, "compression_ratio": 1.7569060773480663, "no_speech_prob": 2.840615707100369e-05}, {"id": 962, "seek": 434516, "start": 4354.48, "end": 4355.96, "text": " With Gradient Boosting Machines?", "tokens": [2022, 16710, 1196, 43902, 278, 12089, 1652, 30], "temperature": 0.0, "avg_logprob": -0.33244501460682263, "compression_ratio": 1.7569060773480663, "no_speech_prob": 2.840615707100369e-05}, {"id": 963, "seek": 434516, "start": 4355.96, "end": 4356.96, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.33244501460682263, "compression_ratio": 1.7569060773480663, "no_speech_prob": 2.840615707100369e-05}, {"id": 964, "seek": 434516, "start": 4356.96, "end": 4358.92, "text": " Yeah absolutely I have.", "tokens": [865, 3122, 286, 362, 13], "temperature": 0.0, "avg_logprob": -0.33244501460682263, "compression_ratio": 1.7569060773480663, "no_speech_prob": 2.840615707100369e-05}, {"id": 965, "seek": 434516, "start": 4358.92, "end": 4361.76, "text": " Gradient Boosting Machines are interesting.", "tokens": [16710, 1196, 43902, 278, 12089, 1652, 366, 1880, 13], "temperature": 0.0, "avg_logprob": -0.33244501460682263, "compression_ratio": 1.7569060773480663, "no_speech_prob": 2.840615707100369e-05}, {"id": 966, "seek": 434516, "start": 4361.76, "end": 4365.36, "text": " They're a lot harder to understand.", "tokens": [814, 434, 257, 688, 6081, 281, 1223, 13], "temperature": 0.0, "avg_logprob": -0.33244501460682263, "compression_ratio": 1.7569060773480663, "no_speech_prob": 2.840615707100369e-05}, {"id": 967, "seek": 434516, "start": 4365.36, "end": 4370.04, "text": " Gradient Boosting Machines, I mean they're still basically ensemble technique and they're", "tokens": [16710, 1196, 43902, 278, 12089, 1652, 11, 286, 914, 436, 434, 920, 1936, 19492, 6532, 293, 436, 434], "temperature": 0.0, "avg_logprob": -0.33244501460682263, "compression_ratio": 1.7569060773480663, "no_speech_prob": 2.840615707100369e-05}, {"id": 968, "seek": 437004, "start": 4370.04, "end": 4375.76, "text": " more working with kind of the residuals of previous models.", "tokens": [544, 1364, 365, 733, 295, 264, 27980, 82, 295, 3894, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1565520513625372, "compression_ratio": 1.784, "no_speech_prob": 2.4679229682078585e-05}, {"id": 969, "seek": 437004, "start": 4375.76, "end": 4379.32, "text": " There's a few pieces of theory around Gradient Boosting Machines which are nicer than random", "tokens": [821, 311, 257, 1326, 3755, 295, 5261, 926, 16710, 1196, 43902, 278, 12089, 1652, 597, 366, 22842, 813, 4974], "temperature": 0.0, "avg_logprob": -0.1565520513625372, "compression_ratio": 1.784, "no_speech_prob": 2.4679229682078585e-05}, {"id": 970, "seek": 437004, "start": 4379.32, "end": 4380.32, "text": " forests.", "tokens": [21700, 13], "temperature": 0.0, "avg_logprob": -0.1565520513625372, "compression_ratio": 1.784, "no_speech_prob": 2.4679229682078585e-05}, {"id": 971, "seek": 437004, "start": 4380.32, "end": 4384.88, "text": " They ought to be faster and they ought to be more well directed and you can do things", "tokens": [814, 13416, 281, 312, 4663, 293, 436, 13416, 281, 312, 544, 731, 12898, 293, 291, 393, 360, 721], "temperature": 0.0, "avg_logprob": -0.1565520513625372, "compression_ratio": 1.784, "no_speech_prob": 2.4679229682078585e-05}, {"id": 972, "seek": 437004, "start": 4384.88, "end": 4389.76, "text": " like say with a Gradient Boosting Machine this particular column has a monotonic relationship", "tokens": [411, 584, 365, 257, 16710, 1196, 43902, 278, 22155, 341, 1729, 7738, 575, 257, 1108, 310, 11630, 2480], "temperature": 0.0, "avg_logprob": -0.1565520513625372, "compression_ratio": 1.784, "no_speech_prob": 2.4679229682078585e-05}, {"id": 973, "seek": 437004, "start": 4389.76, "end": 4393.76, "text": " with a dependent variable so you can actually add constraints in which you can't do with", "tokens": [365, 257, 12334, 7006, 370, 291, 393, 767, 909, 18491, 294, 597, 291, 393, 380, 360, 365], "temperature": 0.0, "avg_logprob": -0.1565520513625372, "compression_ratio": 1.784, "no_speech_prob": 2.4679229682078585e-05}, {"id": 974, "seek": 437004, "start": 4393.76, "end": 4396.16, "text": " random forests.", "tokens": [4974, 21700, 13], "temperature": 0.0, "avg_logprob": -0.1565520513625372, "compression_ratio": 1.784, "no_speech_prob": 2.4679229682078585e-05}, {"id": 975, "seek": 439616, "start": 4396.16, "end": 4402.84, "text": " In my experience I don't need the extra speed of GBMs because I just never have found it", "tokens": [682, 452, 1752, 286, 500, 380, 643, 264, 2857, 3073, 295, 460, 18345, 82, 570, 286, 445, 1128, 362, 1352, 309], "temperature": 0.0, "avg_logprob": -0.2178616170529966, "compression_ratio": 1.6236162361623616, "no_speech_prob": 2.2472511773230508e-05}, {"id": 976, "seek": 439616, "start": 4402.84, "end": 4403.84, "text": " necessary.", "tokens": [4818, 13], "temperature": 0.0, "avg_logprob": -0.2178616170529966, "compression_ratio": 1.6236162361623616, "no_speech_prob": 2.2472511773230508e-05}, {"id": 977, "seek": 439616, "start": 4403.84, "end": 4410.04, "text": " I find them harder to, they've got more parameters to deal with so I haven't found them useful", "tokens": [286, 915, 552, 6081, 281, 11, 436, 600, 658, 544, 9834, 281, 2028, 365, 370, 286, 2378, 380, 1352, 552, 4420], "temperature": 0.0, "avg_logprob": -0.2178616170529966, "compression_ratio": 1.6236162361623616, "no_speech_prob": 2.2472511773230508e-05}, {"id": 978, "seek": 439616, "start": 4410.04, "end": 4415.96, "text": " for me and I know a lot of data mining competitions and also a lot of real world predictive modeling", "tokens": [337, 385, 293, 286, 458, 257, 688, 295, 1412, 15512, 26185, 293, 611, 257, 688, 295, 957, 1002, 35521, 15983], "temperature": 0.0, "avg_logprob": -0.2178616170529966, "compression_ratio": 1.6236162361623616, "no_speech_prob": 2.2472511773230508e-05}, {"id": 979, "seek": 439616, "start": 4415.96, "end": 4419.84, "text": " problems people try both and end up with random forests.", "tokens": [2740, 561, 853, 1293, 293, 917, 493, 365, 4974, 21700, 13], "temperature": 0.0, "avg_logprob": -0.2178616170529966, "compression_ratio": 1.6236162361623616, "no_speech_prob": 2.2472511773230508e-05}, {"id": 980, "seek": 439616, "start": 4419.84, "end": 4425.04, "text": " Well we're probably just about out of time so maybe if there's any more questions I can", "tokens": [1042, 321, 434, 1391, 445, 466, 484, 295, 565, 370, 1310, 498, 456, 311, 604, 544, 1651, 286, 393], "temperature": 0.0, "avg_logprob": -0.2178616170529966, "compression_ratio": 1.6236162361623616, "no_speech_prob": 2.2472511773230508e-05}, {"id": 981, "seek": 442504, "start": 4425.04, "end": 4427.04, "text": " chat to you guys afterwards.", "tokens": [5081, 281, 291, 1074, 10543, 13], "temperature": 0.0, "avg_logprob": -0.4591124216715495, "compression_ratio": 0.8518518518518519, "no_speech_prob": 0.0005672852857969701}, {"id": 982, "seek": 442704, "start": 4427.04, "end": 4455.04, "text": " Thanks very much.", "tokens": [50364, 2561, 588, 709, 13, 51764], "temperature": 0.0, "avg_logprob": -0.6008682250976562, "compression_ratio": 0.68, "no_speech_prob": 0.000620582839474082}], "language": "en"}