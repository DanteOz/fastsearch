{"text": " So this week, obviously quite a bit just to get set up to get results from this week in terms of needing all of ImageNet and that kind of thing, and getting all that working. So I know that a lot of you are still working through that. I did want to mention a couple of reminders that I've noticed. One is that in general, we have that thing on the wiki about how to use the notebooks, and we really strongly advise that you don't open up the notebook we give you and click shift-enter through it again and again. You're not really going to learn much from that. But go back to that wiki page, it's like the first thing that's mentioned in the first paragraph of the home page or wiki is how to use the notebooks. Basically the idea is try to start with a fresh notebook, think about what you think you need to do first, try and do that thing if you have no idea. Then you can go to the existing notebook, take a peek, close it again, try and re-implement what you just saw. As much as possible, really don't just shift-enter through the notebooks. I know some of you are doing it because there are threads on the forum saying, I was shift-entering through the notebook and this thing didn't work. And somebody's like, well, that's because that thing's not defined yet. Consider yourself busted. The other thing to remind you about is that the goal of part 2 is to get you to a point where you can read papers. And the reason for that is because you kind of know the best practices now, so anytime you want to do something beyond what we've learned, you're going to be implementing things from papers or probably going beyond that and implementing new things. Reading a new paper in an area that you haven't looked at before is, at least to me, somewhat terrifying. On the other hand, reading the paper for the thing that we already studied last week hopefully isn't terrifying at all because you already know what the paper says. So I always have that in the assignments each week, read the paper for the thing you just learned about. Go back over it and please ask on the forums if there's a bit of notation or anything that you don't understand, or if there's something we heard in class that you can't see in the paper, or particularly interesting, if you see something in the paper that you don't think we mentioned in class. So that's the reason that I really encourage you to read the papers that we studied for the topics we studied in class. I think for those of you like me who don't have a technical academic background, it's a really great way to familiarize yourself with notation. And I'm actually really looking forward for some of you asking about notation on the forums so I can explain some of it to you. There's a few key things that keep coming up in notation like probability distributions and stuff like that. So please feel free, and if you're watching this later in the MOOC, again, feel free to ask on the forum anything that's not clear. I was kind of interested in following up on some of last week's experiments myself. And the thing that I think we all were a bit shocked about was putting this guy into the device model and getting out more pictures of similar-looking fish in nets. And I was kind of curious about how that was working and how well that was working. I then completely broke things by training it for a few more epochs. And after doing that, I then did an image similarity search again and I got these three guys who were no longer in nets. So I'm not quite sure what's going on here. The other thing I mentioned is when I trained it where my starting point was what we looked at in class, which was just before the final bottleneck layer, I didn't get very good results from this thing. But when I trained it from the starting point of just after the bottleneck layer, I got the good results that you saw. And again, I don't know why that is and I don't think this has been studied as far as I'm aware, so there's lots of open questions here. But I'll show you something I did then do. I thought, well, that's interesting, I think what's happened here is that when you train it for longer, it knows that the important thing is the fish and not the net, and it seems to be now focusing on giving us the same kind of fish. These are clearly the exact same type of fish. So I started wondering how could we force it to combine. So I tried the most obvious possible thing, I wanted to get more fish in nets. And I typed word2vec dict tench, plus word2vec dict net divided by 2, get the average of the two word vectors, and give me the nearest neighbor. And that's what I got. And then just to prove it wasn't a fluke, I tried the same on tench plus rod, and there's my nearest neighbor. Now do you know what's really freaky about this? If you Google for ImageNet categories, you'll get a list of the 1000 ImageNet categories. If you search through them, neither net nor rod appear at all. I can't begin to imagine why this works, but it does. So this device model is clearly doing some pretty deep magic in terms of the understanding of these objects and their relationships. Not only are we able to combine things like this, but we're able to combine it with categories that it's literally never seen before. It's never seen a rod, we've never told it what a rod looks like, and ditto for a net. And I tried quite a few of these combinations and they just kept working. Another one I tried was, I understand why this works, which is I tried searching for boat. Now boat doesn't appear in ImageNet, but there's lots of kinds of boats that appear in ImageNet. So not surprisingly, it figures out generally speaking how to find boats. I expected that. And then I tried boat plus engine, and I got back pictures of power boats. And then I tried boat plus paddle, and I got back pictures of rowing boats. So there's a whole lot going on here, and I think there's lots of opportunities for you to explore and experiment based on the explorations and experiments that I've done. And more to the point, perhaps to create some interesting and valuable tools. Like I would have thought a tool to do an image search to say, show me all the images that contain these kinds of objects. Or better still, maybe you could start training with things that aren't just nouns but also adjectives. So you could start to search for pictures of crying babies or flaming houses or whatever. I think there's all kinds of stuff you could do with this, which would be really interesting whether it be in a narrow organizational setting or to create some new startup or a new open source project or whatever. So anyway, lots of things to try. More stuff this week. I actually missed this, this wasn't this week, but I was thrilled to see that one of our students has written this fantastic medium post, linear algebra cheat sheet. I think I missed it because it was posted not to the part 2 forum, but maybe to the main forum. This is really cool. Brendan has gone through and really explained all the stuff that I would have wanted to have known about linear algebra before I got started. And particularly I really appreciate that he's taking a code-first approach, so how do you actually do this in NumPy and talking about broadcasting. So you guys will all be very familiar with this already, but for your friends who are wondering how to get started in deep learning, what's the minimal things you need to know, it's probably the chain rule and some linear algebra. I think this covers a lot of linear algebra pretty effectively. So thank you, Brendan. Other things from last week. Andrea Frome, who wrote that Debye's paper, I actually emailed her and asked her what she thought I should look at. And she suggested this paper, Zero-Shot Learning by a Convex Combination of Semantic Embeddings, which is only a later author on, but she says it's in some ways a more powerful version of Debye's. It's actually quite different, and I haven't implemented it myself, but it solves some similar problems, and anybody who's interested in exploring this multimodal images and text space might be interested in this. We'll put this on the lesson wiki of course. And then one more involving the same author in a similar area a little bit later was looking at attention for fine-grained categorization. So a lot of these things, at least the way I think Andrea Frome was casting it, was about fine-grained categorization, which is how do we build something that can find very specific kinds of birds or very specific kinds of dogs. But I think these kinds of models have very, very wide applicability. So I mentioned we'd kind of wrap up some final topics around computer vision-y stuff this week before we started looking at some more NLP-related stuff. One of the things I wanted to zip through was a paper which I think some of you might enjoy, Systematic Evaluation of CNN Advances on the ImageNet data set. And I've pulled out what I thought were some of the key insights, because some of these are things we haven't really looked at before. One key insight, which is very much the kind of thing I appreciate, is that they compared the difference between the kind of original CafeNet-AlexNet versus GoogleNet versus VGGNet on two different sized images, training on the original 227 or 128. And what this chart shows is that the relative difference between these different architectures is almost exactly the same regardless of what size image you're looking at. And this really reminds me of like in part 1 when we looked at data augmentation, and we said, Hey, you can understand which types of data augmentation to use and how much on a small sample of the data rather than on the whole data set. And what this paper is saying is something similar, which is you can look at different architectures on small sized images rather than on full sized images. And so they then used this insight to do all of their experiments from then on using a smaller 128x128 ImageNet model, which they said was 10 times faster. So I thought that was the kind of thing which not enough academic papers do, which is like what are the hacky shortcuts we can get away with. So they tried lots of different activation functions. It does look like max pooling is way better. So this is the gain compared to ReLU. But this one actually has twice the complexity, so it doesn't quite say that. What it really says is that something we haven't looked at, which is ReLU, which as you can see is very simple. If x is greater than or equal to 0, it's y equals x. Otherwise it's e to the x minus 1. So ReLU basically is just like ReLU except it's smooth. Whereas ReLU looks like that, ElU looks exactly the same here, then here it goes like that. So it's kind of a nice smooth version. So that's one thing you might want to try using. Another thing they tried which was interesting was using ElU for the convolutional layers and maxout for the fully connected layers. I guess nowadays we don't use fully connected layers very much, so maybe that's not as interesting. The main interesting thing here I think is the ElU activation function. 2 percentage points is quite a big difference. They looked at different learning rate annealing approaches. You can use Keras to automatically do learning rate annealing. What they showed is that linear annealing seems to work best. They tried something else which was what about different color transformations. They found that amongst the normal approaches to thinking about color, RGB actually seems to work the best. But then they tried something I haven't seen before, which is they added two 1x1 convolutions at the very start of the network. So each of those 1x1 convolutions is basically doing some kind of linear combination of the channels with a non-linearity then in between. And they found that that actually gave them quite a big improvement. And that should be pretty much zero cost. So there's another thing which I haven't really seen written about elsewhere, but it's a good trick. They looked at the impact of batch norm. So here is the impact of batch norm, positive or negative. Actually adding batch norm to Google net didn't help, it actually made it worse. So it seems these really complex, carefully tuned architectures, you've got to be pretty careful. Whereas on a simple network it helps a lot. And the amount it helps also depends on somewhat which activation function you use. So batch norm, I think we kind of know that now. Be careful when you use it, sometimes it's fantastically helpful, sometimes it's slightly unhelpful. Question, is there any advantage in using fully connected layers for cloud? I think there is, they're terribly out of fashion. But I think for transfer learning, they still seem to be the best in terms of the fully connected layers, super fast train and you seem to get a lot of flexibility there. So I don't think we know one way or another yet, but I do think that VGG still has a lot to give us in terms of the last carefully tuned thing with fully connected layers and that really seems to be great for transfer learning. Question, anytime you hear me say something slightly stupid, please feel free to jump in because otherwise it's on the video forever. So on the other hand, it does give you an improvement in accuracy if you remove the final max pooling layer, replace all the fully connected layers with convolutional layers and stick an average pooling at the end, which is basically what this is doing. So it does seem there's definitely an upside to fully convolutional networks in terms of accuracy, but there may be a downside in terms of flexibility around transfer learning. That's let-long clear still. I thought this was an interesting picture I hadn't quite seen before. Let me explain the picture. What this shows is these are different batch sizes along the bottom, and then we've got accuracy. What it's showing is with a learning rate of.01, this is what happens to accuracy. So as you go above 256 batch size, it plummets. On the other hand, if you use a learning rate of.01 times batch size over 256, it's pretty flat. So what this suggests to me is that any time you change the batch size, this basically is telling you to change the learning rate by a proportional amount, which I think a lot of us have realized through experiment, but I don't think I've seen it explicitly mentioned before. I think this is very helpful to understand as well is that removing data has a kind of non-linear effect on accuracy. So this green line here is what happens when you remove images. So with ImageNet, down to about half the size of ImageNet, there isn't a huge impact on accuracy. Or maybe if you want to really speed things up, you can go 128x128 sized images and use just 600,000 of them, or even maybe 400,000. But then beneath that, it starts to plummet. So I think that's an interesting insight. Another interesting insight, although I'm going to add something to this in a moment, is that rather than removing images, if you instead flip the labels to make them incorrect, that has a worse effect than not having the data at all. But there are things we can do to try and improve things there. And specifically I want to bring your attention to this paper, Training Deep Neural Networks on Noisy Labels with Bootstrapping. And what they show is a very simple approach, a very simple tweak you can add to any training method, which dramatically improves their ability to handle noisy labels. So this here is showing, if you add noise, varying from.3 up to.5 to MNIST, up to half of it, the baselines are doing nothing at all. It really collapses the accuracy. But if you use their approach to bootstrapping, you can actually go up to nearly half the images, intentionally changing their label, and it still works nearly as well. I think this is a really important paper to mention in this stuff that most of you will find important and useful area because most real-world datasets have noise in them. So maybe this is something you should consider adding to everything that you've trained, whether it be Kaggle datasets or your own datasets or whatever, particularly because you don't necessarily know how noisy the labels are. So noisy labels means incorrect. Noisy just means incorrect. This particular paper describes a particular technique, which you can read during the week if you're interested. Interestingly they find that if you take VGG and then add all of these things together and do them all at once, you can actually get a pretty big performance hike. It looks in fact like VGG becomes more accurate than GoogleNet to make all these changes. So that's an interesting point, although VGG is very, very slow and big. There's lots of stuff that I noticed they didn't look at. They didn't look at data augmentation, different approaches to zooming and cropping, adding skip connections like in ResNet or DenseNet or Hire Networks, different initialization methods, different amounts of depth. And to me, the most important is the impact on transfer learning. So these to me are all open questions as far as I know, and so maybe one of you would like to create the successor to this, more observations on training CNNs. There's another interesting paper, although the main interesting thing about this paper is this particular picture, so feel free to check it out, it's pretty short and simple. This paper is looking at the accuracy versus the size and the speed of different networks. So the size of a bubble is how big is the network, how many parameters does it have. So you can see VGG16 and VGG19 are by far the biggest of any of these networks. Interestingly the second biggest are the very old basic AlexNet. Interestingly newer networks tend to have a lot less parameters, which is a good sign. Then on this axis we have basically how long does it take to train. So again VGG is big and slow, and without at least some tweaks, not terribly accurate. So again there's definitely reasons not to use VGG even if it seems easier for transfer learning or we don't necessarily know how to do a great job of transfer learning on ResNet or Inception. But as you can see, the more recent ResNet and Inception-based approaches are significantly more accurate and faster and smaller. So this is I think why I was looking last week at trying to do transfer learning on top of ResNet. There's really good reasons to want to do that. I think this is a great picture. These two papers really show us that academic papers are not always just here's some highly theoretical wacky result. From time to time people write these great thorough analyses of best practices and everything that's going on. There's some really great stuff out there. One other paper to mention in this kind of broad ideas about things that you might find helpful is a paper by somebody named Leslie Smith, who I think has got to be just about the most overlooked researcher. Leslie Smith does a lot of really great papers, which I really like. This particular paper came up with a list of 14 design patterns which seem to be generally associated with better CNNs. This is a great paper to read. It's a really easy read. You guys won't have any trouble with it at all, I don't think. It's very short. But I looked through all these and I just kind of thought, yeah, these all make a lot of sense. If you're doing something a bit different and a bit new and you have to design a new architecture, this would be a great list of patterns to look through. One more Leslie Smith paper to mention, and it's crazy that this is not more well-known, something incredibly simple, which is a different approach to learning rates. Rather than just having your learning rate gradually decrease, I'm sure a lot of you have noticed that sometimes if you suddenly increase the learning rate for a bit and then suddenly decrease it again for a bit, it kind of goes into a better little area. What this paper suggests doing is try actually continually increasing your learning rate and then decreasing it, increasing it, decreasing it, increasing it, decreasing it. Something that they call cyclical learning rates. Check out the impact. Compared to non-cyclical approaches, it is way, way faster. At every point, it's much better. This is something which you could easily add. I haven't seen this added to any library. If you created the cyclical learning rate annealing class for Keras, many people would thank you. Actually, many people would have no idea what you're talking about, so they didn't have to write the blog post to explain why it's good and show them this picture, and they would thank you. Question asked. If I was doing this in Keras, what I would do would be I would start with the existing learning rate annealing code that's there and make small changes until it starts working. There's already code that does pretty much everything you want. The other cool thing about this paper is that they suggest a fairly automated approach to picking what the minimum and maximum bounds should be. This idea of roughly what should our learning rate be is something which we tend to use a lot of trial and error for. So check out this paper for a suggestion about how to do it somewhat automatically. So there's a whole bunch of things that I've zipped over. Normally I would have dug into each of those and explained it and shown examples in notebooks and stuff, but this is like a, you guys hopefully now have enough knowledge to take this information and play with it. And what I'm hoping is that different people will play with different parts and come back and tell us what you find and hopefully we'll get some good new contributions to Keras or PyTorch or some blog posts or some papers or so forth. Or maybe with that device stuff, even some new applications. The next thing I wanted to look at, and again somewhat briefly, is the data science bot. And the reason I particularly wanted to dig into the data science bot is, there's a couple of reasons. One of them, there's a million reasons. It's a million dollar price and there are 23 days to go. The second is it's an extension to everything you guys have learned so far about computer vision. It uses all the techniques you've learned, but then some. So rather than 2D images, they're going to be 3D volumes. Rather than being 300x300 or 500x500, they're going to be 512x512x200. So a couple of hundred times bigger than stuff you've dealt with before. The stuff we learned in Lesson 7 about where are the fish, you're going to be needing to use a lot of that. So I think it's a really interesting problem to solve. And I personally care a lot about this because my previous startup in Lidic was the first organization to use deep learning to tackle this exact problem, which is trying to find lung cancer in CT scans. The reason I made that in Lidic's first problem was mainly because I learned that if you can find lung cancer earlier, the probability of survival is 10 times higher. So here is something where you can have a real impact by doing this well. Which is not to say that a million dollars isn't a big impact as well. So let me tell you a little bit about this problem. Here is a lung, and this is in a DICOM viewer. DICOM is the standard that is used for sharing most kinds of medical imaging, certainly CT scans. It is a format which contains two main things. One is a stack of images and another is some metadata. So that metadata will be things like how much radiation were they zapped by, how far away from the chest with the machine, and what brand of machine was it, and so on and so forth. So you can, for most DICOM viewers, just use your scroll wheel to zip through them. So all this is doing is going from top to bottom or from bottom to top. Actually what I might do, I think is more interesting, is to say, let's actually focus on the bit that's going to matter to you, which is the inside of the lung is this dark area here, and these little white dots are what's called the vasculature, so the little vessels and stuff going through the lungs. As I scroll through, have a look at this little dot, you'll see that it seems to move. See how it's moving? The reason it's moving is because it's not a dot, it's actually a vessel going through space so it actually looks like this. And so if you take a slice through that, it looks like lots of dots. And so as you go through those slices, it looks like that. And then eventually we get to the top of the lung, and that's why you see eventually it all goes to white. So that's the edge basically of the organ. So you can see there are edges on each side. Then there's also bone. So some of you have been looking at this already over the last few weeks and have often asked me about how to deal with multiple images. And what I've said each time is don't think of it as multiple images. Think of it in the way your DICOM viewer can, if you have a 3D button like this one does. That's actually what we're just looking at. So it's not a bunch of flat images, it's a 3D volume. It just so happens that the default way that most DICOM viewers show things is by a bunch of flat images. But it's really important that you think of it as a 3D volume because you're looking in this space. Now what are you looking for in this space? What you're looking for is you're looking for somebody who has lung cancer. And what somebody who has lung cancer looks like is that somewhere in this space there is a blob. It could be roughly spherical blob, it could be pretty small, around 5mm is where people start to get particularly concerned about a blob. And so what that means is that for a radiologist, as they flick through a scan like this, is that they're looking for a dot which doesn't move, but which appears, gets bigger, and then disappears. That's what a blob looks like. So you can see why radiologists very, very, very often miss nodules, blobs, inlions. Because in all of this area, you've got to have extraordinary vision to be able to see every little blob appear and then disappear again. And remember, the sooner you catch it, you get a 10x improved chance of survival. And generally speaking, when a radiologist looks at one of these scans, they're not looking for nodules. They're looking for something else because lung cancer, at least in the earlier stages, is asymptomatic. It doesn't cause you to feel different. So it's like something that every radiologist has to be thinking about when they're looking for pneumonia or whatever else. So that's the basic idea, is that we're going to try and come up with in the next hour or so some idea about how would you find these blobs, how would you find these nodules. So each of these things generally is about 512x512x100. The equivalent of a pixel in 3D space is called a voxel. So a voxel simply means a pixel in 3D space. So this here is rendering a bunch of voxels. Each voxel in a CT scan is a 12-bit integer, if memory serves me correctly, and a computer screen can only show 8 bits of grayscale. And furthermore, your eyes can't necessarily distinguish between all those grayscale perfectly anyway. So what every DICOM viewer provides is something called a windowing adjustment. So a windowing adjustment, here is the default window which is designed to basically map some subset of that 12-bit space to the screen so that it highlights certain things. And so the units, CT scans use are called Houndsfield units, and certain ranges of Houndsfield units tell you that something is some particular part of the body. And so you can see here that the bone is being lit up. So we've selected an image window which is designed to allow us to see the bone clearly. So what I did when I opened this was I switched it to CT's chest, which is some kind person has already figured out what the best window is. For you working with deep learning, you don't have to care about that because of course the deep learning algorithm can see 12 bits perfectly well, so nothing really to worry about. So one of the challenges with dealing with this data science bold data is that there's a lot of preprocessing to do. But the good news is that there's a couple of fantastic tutorials. So hopefully you've found out by now that I'm caggle, if you click on the kernels button, you basically get to see people's IPython notebooks where they show you how to do certain things. In this case, this guy has got a full preprocessing tutorial showing how to load DICOM, convert the values to Houndsfield units, and so forth. I'll show you some of these pieces. So DICOM you will load with some library, probably with pydicom. Pydicom is a library that's a bit like a pillow or an image.open, it's more like a DICOM.open and end up with a 3D file and of course the metadata. You can see here, using the metadata, image position, lice location. So the metadata comes through with just attributes of the Python object. This person has very kindly provided to you a list of the Houndsfield units for each of the different substances. So he shows how to translate stuff into that range, and so it's great to draw lots of pictures. Here is a histogram for this particular picture. So you can see that most of it is air, and then you get some bone and some lung. So the next thing to think about is the voxel spacing, which is as you move across one bit of x-axis or one bit of y-axis or from slice to slice, how far in the real world are you moving. One of the annoying things about MetaPool imaging is that different kinds of scanners have different distances between those slices, it's called the slice thickness, and the different meanings of the x and y axes. Luckily that stuff is all in the DICOM metadata. So the resampling process means taking those lists of slices and turning it into something where every step in the x-direction or the y-direction or the z-direction equals 1 mm in the real world. And so it would be very annoying for your deep learning network if your different lung images were squished by different amounts, especially if you didn't give it the metadata about how much was being squished. So that's what resampling does. As you can see, it's using the slice thickness and the pixel spacing to make everything nice and even. So there are various ways to do 3D plots, and it's always a good idea to do that. And then something else that people tend to do is segmentation. Depending on time, we may or may not get around to looking more at segmentation in this part of the course, but effectively segmentation is just another generative model. It's a generative model where hopefully somebody's given you some things saying this is lung, this is air, and then you build a model that tries to predict for something else what's lung and what's air. Unfortunately for lung CT scans, we don't generally have that kind of ground truth of which bit's lung and which bit's air. So generally speaking in medical imaging, people use a whole lot of heuristic approaches, so kind of hacky, rule-based approaches. And in particular, applications of region-growing and morphological operations. I find this kind of the boring part of medical imaging because it's so clearly a dumb way to do things. But deep learning is far too new in this area yet to develop the data sets that we need to do this properly. But the good news is that there's a button which I don't think many people notice exists called tutorial on the main data science page where these folks from Booz Allen Hamilton actually show you a complete segmentation approach. Now it's interesting that they picked UNET segmentation. This is definitely the thing about segmentation I would be teaching you guys if we have time. UNET is one of these things that outside of the Kaggle world, I don't think that many people are familiar with. But inside the Kaggle world, we know that anytime segmentation crops up, UNET wins and it's the best. More recently there's actually been something called DenseNet for segmentation, which takes UNET even a little bit further, and maybe that would be the new winner for newer Kaggle competitions when they happen. But the basic idea here of things like UNET and DenseNet is that we have a model where when we do generative models, we think about doing style transfer. We generally start with this kind of large image and then we do some downsampling operations to make it a smaller image, and then we do some computation and then we make it bigger again with these upsampling operations. What happens in UNET is that there are additional neural network connections made directly from here to here, and directly from here to here, and here to here, and here to here. Those connections basically allow it to almost do like a residual learning approach. It can figure out the key semantic pieces at a really low resolution. But then as it upscales it, it can learn what was special about the difference between the downsampled image and the original image here. It can kind of learn to add that additional detail at each point. So UNET and DenseNet for segmentation are really interesting. I hope we find some time to get back to them in this part of the course. But if we don't, you can get started by looking at this tutorial in which these folks basically show you from scratch. What they try to do in this tutorial is something very specific, which is the detection part. So what happens in this kind of, think about the fisheries competition. We pretty much decided that in the fisheries competition, if you wanted to do really well, you would first of all find the fish, and then you would zoom into the fish, and then you would figure out what kind of fish it is. Certainly in the right whale competition earlier, that was how that was planned. For this competition, this is even more clearly going to be the approach, because these images are just far too big to do a normal convolutional neural network. We need one step that's going to find the nodule, and then a second step that's going to zoom into a possible nodule and figure out is this a malignant tumor or something else, a false positive. The bad news is that the data science bold dataset does not give you any information at all for the training set, where are the cancerous nodules. Which I actually wrote a post in the Kaggle forums about this, I just think this is a terrible, terrible idea. That information actually exists. The dataset they got this from is something called the National Lung Screening Trial, which actually has that information, or something close to it. So the fact they didn't provide it, I just think it's horrible for a competition which can save lives, and I can't begin to imagine. The good news though is that there is a dataset which does have this information. The original dataset was called LIDC-IDRI, but interestingly that dataset was recently used for another competition, a non-Kaggle competition called Luna. That competition is now finished. One of the tracks in that competition was actually specifically a false positive detection track and then the other track was a find-the-nodule track. So you can actually go back and look at the papers written by the winners. They're generally ridiculously short. Many of them are a single sentence saying for commercial confidentiality agreement, we can't do anything. But some of them, including the winner of the false positive track, they actually provide it. Not surprisingly, they all use deep learning. And so what you could do, in fact I think what you have to do to do well in this competition, is download the Luna dataset, use that to build a nodule detection algorithm. So the Luna dataset includes files saying this lung has nodules here, here, here and here. So do nodule detection based on that, and then run that nodule detection algorithm on the Kaggle dataset, find the nodules and then use that to do some classification. There are some tricky things with that. The biggest tricky thing is that most of the CT scans in the Luna dataset are what's called contrast studies. A contrast scan means that the patient had a radioactive dye injected into them so that the things that they're looking for are easier to see. For the National Lung Screening Trial, which is what they use in the Kaggle dataset, none of them use contrast. And the reason why is that what we really want to be able to do is to take anybody who's like over 65 and has been smoking more than a pack a day for more than 20 years and give them all a CT scan and find out which ones have cancer. But in the process, we don't want to be shooting them up with radioactive dye and giving them cancer. So that's why we try to make sure that when we're doing these kind of asymptomatic scans that they're as low radiation dose as possible. So that means that you're going to have to think about transfer learning issues, that the contrast in your image is going to be different between the thing you build on the Luna dataset, the nodule detection, and the Kaggle competition dataset. When I looked at it, I didn't find that that was a terribly difficult problem. I'm sure you won't find it impossible by any means. So to finalize this discussion, I wanted to refer to this paper, which I'm guessing not that many people have read yet. It's a medical imaging paper. And what it is, is a non-deep learning approach to trying to find nodules. Yes, Rachel. I have a correction from our radiologist saying that dye is not radioactive. It's just dense. Isobutyl 70 or isobutyl 70. But there's a reason we don't inject people with the contrast dye. I do know though that the NLST studies use a lower amount of radioactivity than I think the Luna ones do, so that's another difference. So this is an interesting idea of how can you find nodules using more of a heuristic approach. The heuristic approach they suggest here is to do clustering. We haven't really done any clustering in class yet, so we're going to dig into this in some time, because I think this is a great idea for the kind of heuristics you can add on top of deep learning to make deep learning work in different areas. The basic idea here is to, as you can say, they call it a 5-dimensional mean. They're going to try and find groups of voxels which are similar and they're going to cluster them together. And hopefully we're going to particularly cluster together things that look like nodules. So the idea is at the end of this segmentation, there will be one cluster for the whole lung boundary, one cluster for the whole vasculature, and then one cluster for every nodule. So the 5 dimensions are x, y and z, intensity, so the number of Houndsfield units. And then the fifth one is volumetric shape index, and this is the one tricky one. The basic idea here is it's going to be a combination of the different curvatures of a voxel based on the Gaussian and mean curvatures. Now what the paper goes on to explain is that you can use for these the first and second derivatives of the image. Now all that basically means is you subtract one voxel from its neighbor, and then you take that whole thing and subtract one voxel's version of that from its neighbor. You get the first and second derivatives. So it kind of tells you the direction of the change of image intensity at that point. So by getting these first and second derivatives of the image and then you put it into this formula, it comes out with something which basically tells you how sphere-like this voxel seems to be, how spheres like a construct. So that's great, if we can basically take all the voxels and combine the ones that are nearby, have a similar number of Houndsfield units, and seem to be of similar kinds of shapes, we're going to get what we want. So I'm not going to worry about this bit here because it's very specific to medical imaging. Anybody who's interested in doing this, feel free to talk on the forum about what this looks like in Python. But what I did want to talk about was the main-shift clustering, which is a particular approach to clustering which they talk about. Question asked. Clustering is something which for a long time I've been kind of an anti-fan of. It belongs to this group of unsupervised learning algorithms which always seem to be looking for a problem to solve. But I've realized recently there are some specific problems that can be solved well with them, and I'm going to be showing you a couple, one today and one in Lesson 14. Clustering algorithms are perhaps the easiest to describe by what they do by generating some data to show them. Here's some generated data. I'm going to create 6 clusters, and for each cluster I'll create 250 samples. So I'm going to basically say, okay, let's create a bunch of centroids by creating some random numbers. So 6 pairs of random numbers for my centroids. And then I'll grab a bunch of random numbers around each of those centroids and combine them all together and then plot them. And so here you can see each of these X's represents a centroid. So a centroid is just like the average point for a cluster of data. And each color represents one cluster. So imagine if this was showing you clusterings of different kinds of lung tissue. Maybe you'd have some voxels that were colored one thing for a nodule and a bunch of things that are colored a different color for vasculature and so forth. We can only show this easily in 2 dimensions, but there's no reason to not be able to imagine doing this in certainly 5 dimensions. So the goal of clustering will be to undo this. Imagine the data, but not the X's, how can you figure out where the X's were. And then it's pretty straightforward once you know where the X's are to then find the closest points to that to assign every data point to a cluster. The most popular approach to clustering is called k-means. K-means is an approach where you have to decide upfront how many clusters are there. And what it basically does is there's 2 steps. The first one is to guess as to where those clusters might be. And the really simple way to do that is just to randomize the clusters. The simple way to do that is basically to randomly pick a point and then start randomly picking points which are as far away as possible from all the previous ones I've picked. So if I start here, then probably the furthest away point would be down here. So this would be like our starting point for cluster 1. And you say, what point is furthest away from that? That's probably this one here. That's our starting point for cluster 2. What's the furthest point away from both of these? Probably this one over here, and so forth. So you keep doing that to get your initial points. And then you just iteratively move every point. So you basically then say, these are the clusters, let's assume these are the clusters. Which cluster does every point belong to? And then you just iteratively move the points to different clusters a bunch of times. Now k means, it's a shame it's so popular because it kind of sucks. Sucky thing number 1 is that you have to decide how many clusters there are. The whole point is we don't know how many nodules there are. And then sucky thing number 2 is without some changes, to do something called kernel k means, it only works if the things are the same shape. They're all kind of nicely Gaussian-shaped. So we're going to talk about something way cooler which I only kind of came across somewhat recently, much less well-known, which is called mean-shift clustering. Now mean-shift clustering is one of these things which seems to spend all of its time in serious mathematician land. Whenever I try to look up something about mean-shift clustering, I kind of started seeing this kind of thing. This is the first tutorial, not in a PDF, that I could find. So this is one way to think about mean-shift clustering. Another way is the code-first approach, which is that this is the entire algorithm. So let's talk about what's going on here. What are we doing? At a high level, we're going to do a bunch of loops. So we're going to do 5 steps. It would be better if I didn't do 5 steps, but I kept doing this until it was stable. For now I'm just going to do 5 steps. And each step, our data is x, I'm going to enumerate through our data. What I'm going to do is I'm going to find a small x is the current data point I'm looking at. Now what I want to do is find out how far away is this data point from every other data point. So I'm going to create a vector of distances. And I'm going to do that with the magic of broadcasting. So small x is a vector of size 2, this is 2 coordinates. And big X is a matrix of size n by 2, where n is the number of points. And thanks to what we've now learned about broadcasting, we know that we could subtract a matrix from a vector, and that vector will be broadcast across the axis of the matrix. And so this is going to subtract every element of big X from little x. And so if we then go ahead and square that and then sum it up and then take the square root, this is going to return a vector of distances of small x to every element of big X. And the sum here is just summing up the 2 coordinates. So that's step 1. So we now know for this particular data point how far away is it from all of the other data points. Now the next thing we want to do is to, let's go to the final step. The final step will be to take a weighted average. In the final step, we're going to say what cluster do you belong to. And we're currently looking at this one. What we've done is we've now got a list of how far it is away from all of the other data points. And the basic idea is now what we want to do is take the weighted average of all of those data points, weight it by the inverse of that distance. So the things that are a long way away, we want to weight very small. And the things that are very close, we want to weight very big. So I think this is probably the closest, this is about the second closest, and this is about the third closest. So assuming these have got most of the weight, the average is going to be somewhere about here. And so by doing that at every point, we're going to move every point closer to where its friends are, closer to where the nearby things are. And so if we keep doing this again and again, everything's going to move until it's right next to its friends. So how do we take something which initially is a distance and make it so that the larger distances have smaller weights? And the answer is we probably want a shape that looks something like that. In other words, Gaussian. This is by no means the only shape you could choose. It would be equally valid to choose this shape, which is a triangle, at least half of one. In general though, note that if we're going to multiply every point by one of these things and add them all together, it would be nice if all of our weights added to one, because then we're going to end up with something that's of the same scale that we start with. So when you create one of these curves where it all adds up to one, generally speaking we call that a kernel. And I mention this because you will see kernels everywhere, if you haven't already. Now that you've seen it, you'll see them everywhere. In fact, kernel methods is a whole area of machine learning that in the late 90s basically took over because it was so theoretically pure. And if you want to get published in conference proceedings, it's much more important to be theoretically pure than actually accurate. So for a long time kernel methods went out and neural networks in particular disappeared. Eventually people realized that accuracy was important as well, and in more recent times kernel methods are largely disappearing. But you still see the idea of a kernel coming up very often because they're super useful tools to have. They're basically something that lets you take a number, like in this case a distance, and turn it into some other number where you can weight everything by that other number and add them together to get a nice little weighted average. So in our case, we're going to use a Gaussian kernel. The particular formula for a Gaussian doesn't matter. I remember learning this formula in grade 10 and it was by far the most terrifying mathematical formula I've ever seen, but it doesn't really matter. For those of you that remember or have seen the Gaussian formula, you'll recognize it. For those of you that haven't, it doesn't matter. But this is the function that draws that curve. So if we take every one of our distances and put it through the Gaussian, we will then get back a bunch of weights that add to 1. So then in the final step, we can multiply every one of our data points by that weight, add them up and divide by the sum of the weights, in other words, take a weighted average. You'll notice that I had to be a bit careful about broadcasting here because I needed to add a unit axis at the end of my dimensions, not at the start, so by default it adds unit axes to the beginning when you do broadcasting. That's why I had to do an expandims. If you're not clear on why this is, then that's a sign you definitely need to do some more playing around with broadcasting. So have a fiddle with that during the week. Feel free to ask if you're not clear after you've experimented. But this is just a weighted sum. So this is just doing sum of weights times x divided by sum of weights. Importantly, there's a nice little thing that we can pass to a Gaussian, which is the thing that decides does it look like the thing I just drew, or does it look like this, or does it look like this. All of those things add up to one, they all have the same area underneath, but they're very different shapes. If we make it look like this, then what that's going to do is it's going to create a lot more clusters because things that are really close to it are going to have really high weights, and everything else is going to have a tiny weight and be meaningless. So if we use something like this, we're going to have much fewer clusters because even stuff that's further away is going to have a higher weight from the weighted sum. So the choice that you use for the kernel width, we actually here use BW being bandwidth, and there's actually some cool ways to choose it. One simple way to choose it is to find out which size of bandwidth covers 1 third of the data in your dataset. I think that's the approach that Scikit-learn uses. So anyway, there are some ways that you can automatically figure out the bandwidth. Just one of the very nice things about mean shift. So we just go through a bunch of times, 5 times, and each time we replace every point with its weighted average, weighted by this Gaussian kernel. And so when we run this 5 times, it takes a second, and here's the results. I've offset everything by 1 just so that we can see it, otherwise it would be right on top of the X. So you can see that for nearly all of them, it's in exactly the right spot. Whereas for this cluster, let's just remind ourselves what that cluster looked like. These two clusters, this particular bandwidth, it decided to create one cluster for them rather than two. So this is kind of an example, whereas if we decreased our bandwidth, it would create two clusters. There's no one right answer, that should be one or two. So one challenge with this is that it's kind of slow. So I thought, let's try and accelerate it for the GPU. Because mean shift's not very cool, nobody seems to have implemented it for the GPU yet, or maybe it's just not a good idea, so I thought I'd use PyTorch. The reason I used PyTorch is because it really feels like writing PyTorch just feels like writing NumPy, everything happens straight away. So I really hoped that I could take my original code and make it almost the same. And indeed, here is the entirety of mean shift in PyTorch. So that's pretty cool. You can see rather than anywhere that I used to have Np, it now says Torch, Np.array is now Torch.floatTensor, Np.squareRoot is Torch.squareRoot, everything else is almost the same. One issue is that Torch doesn't support broadcasting. So we'll talk more about this shortly in a couple of weeks, but basically I decided that's not okay, so I wrote my own broadcasting library for PyTorch. So rather than saying x, little x minus big X, I used sub for subtract, that's the subtract from my broadcasting library. If you're curious, check out TorchUtils and you can see my broadcasting operations there. But basically if you use those, you can see the same for multiplication, it'll do all the broadcasting for you. So as you can see, this looks basically identical to the previous code, but it takes longer. So that's not ideal. One problem here is that I'm not using CUDA. So I could easily fix that by adding.cuda to my x, but that made it slower still. The reason why is that all the work's being done in this for loop, and PyTorch doesn't accelerate for loops. Each run through a for loop in PyTorch is basically calling a new CUDA kernel each time you're going through. It takes a certain amount of time to even launch a CUDA kernel. When I'm saying CUDA kernel, this is a different usage of the word kernel. In CUDA, kernel refers to a little piece of code that runs on the GPU. So it's launching a little GPU process every time through the for loop, which takes quite a bit of time, and it's also having to copy data all over the place. So what I then tried to do was to make it faster. The trick is to do it by minibatch. So each time through the loop, we don't want to do just one piece of data, but a minibatch of data. So here are the changes I made. The main one was that my for i now jumps through one batch size at a time. So I'm going to go not 0.123, but 0.1632. So I now need to create a slice which is from i to i plus batch size, unless we've gone past the end of the data, which is just as far as n. So this is going to refer to the slice of data that we're interested in. So what we can now do is say x with that slice to grab back all of the data in this minibatch. And so then I have to create a special version of, I can't just say subtract anymore, I need to think carefully about the broadcasting operations here. I'm going to return a matrix, let's say batch size is 32, I'm going to have 32 rows, and then let's say n is 1000, it'll be 1000 columns. That shows me how far away each thing in my batch is from every piece of data. So when we do things a batch at a time, you're basically adding another axis to all of your tensors. Suddenly now you have a batch axis all the time. And when we've been doing deep learning, that's been something I think we've gotten pretty used to. The first axis in all of our tensors has always been a batch axis. So now we're writing our own GPU accelerated algorithm. Can you believe how crazy this is? Two years ago, if you Google for K means Buddha or K means GPU, you get back research studies where people write papers about how to put these algorithm GPUs, because it was hard. And here's a page of code that does it. This is crazy, this is possible, but here we are. We have built a batch-by-batch GPU accelerated main shift algorithm. So the basic distance formula is exactly the same. I just have to be careful about where I add it. Unsqueeze is the same as expandims. So I just have to be careful about where I add my unit axes. Add it to the first axis of one bit and the second axis of the other bit. So that's going to subtract every one of these from every one of these. Return a matrix. Again, this is like a really good time to look at this and think why does this broadcasting work because this is getting more and more complex broadcasting. And hopefully you can now see the value of broadcasting. Not only did I get to avoid writing a pair of nested for loops here, but I also got to do this all on the GPU in a single operation, so I've made this thousands of times faster. So here is a single operation which does that entire matrix subtraction. So that's our batch-wise distance function. We then chuck that into a Gaussian. And because this is just element-wise, the Gaussian function hasn't changed at all. And then I've got my weighted sum and then divide that by the sum of weights. So that's basically the algorithm. So previously for my NumPy version it took a second, now it's 48 milliseconds. So we've just sped that up by 20 seconds. Question. I get how batching helps with locality in cache, but I do not quite follow how it helps otherwise, especially with respect to accelerating the for loop. In PyTorch, the for loop is not run on the GPU. The for loop is run on your CPU. And your CPU goes through each step of the for loop and calls the GPU to say, do this thing, do this thing, do this thing. So this is not to say you can't accelerate this in TensorFlow in a similar way. Like in TensorFlow there's a tf.wile and stuff like that where you can actually do GPU-based loops. Even still, if you do it entirely in a loop in Python, it's going to be pretty difficult to get this performance. But particularly in PyTorch, it's important to remember in PyTorch, your loops are not optimized. It's what you do inside each loop that's optimized. Question. Some of the math functions are coming from Torch and others are coming from the Python library. What is the difference? When you use the Python math library, does that mean the GPU is not being used? Answer. You'll see that I use that math.py is a constant and then math.square root of 2 times pi is a constant. So you need to use the GPU to calculate a constant, obviously. So we only use Torch for things that are running on a vector or a matrix or a tensor of data. So let's have a break. We'll come back in 10 minutes, so that will be 2 past 8, and we'll talk about some ideas I have for improving mean shift, which maybe you guys will want to try out during the week. So basically the idea here is we figure that there are 2 steps we need to figure out where the nodules are in something like this, if any. Step number 1 is to find the things that may be kind of nodule-ish, zoom into them and create a little cropped version. And then step 2 would be where your deep learning particularly comes in, which is to figure out is that cancerous or not. Once you've found a nodule-ish thing, the cancerous ones are actually by far the biggest driver of whether or not something is an owing to cancer is how big it is. So it's actually pretty straightforward. The other thing particularly important is how kind of spidery it looks. If it looks like it's kind of evilly going out to capture more territory, that's probably a bad sign as well. So the size and the shape are the 2 things you're going to be wanting to try and find, and obviously that's a pretty good thing for a neural net to be able to do. You probably don't need that in the examples of it. When you get to that point, there's obviously a question about how to deal with the 3D aspect here. You can just create a 3D convolutional neural net. So if you had like a 10x10x10 space, that's obviously certainly not going to be too big, but if it's 20x20x20 you might be okay. Think about how big a volume you can create. There's plenty of papers around on 3D convolutions, although I'm not sure if you even need one because it's just a convolution in 3D. The other approach that you might find interesting to think about is something called triplanar. What triplanar means is that you take a slice through the X and the Y and the Z axes, and so you basically end up with 3 images. One is a slice through X, Y and Z, and then you can kind of treat those as different channels if you like even. They probably use pretty standard neural net libraries that expect 3 channels. So there's a couple of ideas for how you can deal with the 3D aspect of it. I think using the Luna dataset as much as possible is going to be a good idea because you really want something that's pretty good at detecting nodules before you start putting it onto the Kaggle dataset. The other problem with the Kaggle dataset is it's ridiculously small. Again, there's no reason for it. There are far more cases in NLST than they've provided to Kaggle, so I can't begin to imagine why they went into all this trouble with a million dollars of money for something which has not been set up to succeed. Anyway, that's not our problem. It makes it all a more interesting thing to play with. But after the competition is finished, if you get interested in it, you'll probably want to go and download the whole NLST dataset as much as possible and do it properly. Question from the audience. One is just for the audio stream, there are occasional max volume pops that are really hard on the ears for remote listeners. This might not be solvable right now, but something to look into. And then someone asked, last class you mentioned that you would explain when and why to use Keras versus PyTorch. If you only had brain space for one, in the same way some only have brain space for VI or Emacs, which would you pick? So I just reduced the volume a little bit, so let us know if that helps. I would pick PyTorch. It feels like it kind of does everything Keras does, but gives you the flexibility to really play around a lot more. I'm sure you've got brain space for both. So question, you mentioned there are other datasets of cancerous images that have labels and proper marks. Can you explain this thing on that dataset? That was my suggestion, and that's what the tutorial shows how to do. There's a whole thing, kernel on Kaggle called candidate generation and Luna 16 something something, which shows how to use Luna to build a logical finder. This is one of the highest rated Kaggle kernels. We've now used kernel in 3 totally different ways in this lesson. If we can come up with a fourth, Kaggle kernels, CUDA kernels, and kernel methods. So this looks very familiar, doesn't it? So here's a Keras approach to finding LungNodules based on Luna. I mentioned an opportunity to improve this mean shift algorithm. The opportunity for improvement, when you think about it, is pretty obvious. The actual amount of data is huge. You've got data points all over the place. The ones that are a long way away, the weight is going to be so close to zero that we may as well just ignore them. The question is how do we quickly find the ones which are a long way away. We know the answer to that. We learned it. It's approximate nearest neighbors. So what if we added an extra step here, which rather than using X to get the distance to every data point, we instead used approximate nearest neighbors to grab the closest ones, the ones that are actually going to matter. So that would basically turn this linear time piece into a logarithmic time piece, which would be pretty fantastic. So we learned very briefly about a particular approach, which is locality-sensitive hashing. I think I mentioned also there's another approach which I'm really fond of called spill trees. I really, really want us as a team to take this algorithm and add approximate nearest neighbors to it and release it to the community as the first ever super-fast GPU-accelerated approximate nearest neighbor accelerated in-ship clustering algorithm. I think that would be a really big deal. If anybody's interested in doing that, I believe you're going to have to implement something like LSH or spill trees in PyTorch. And once you've done that, it should be totally trivial to add the step that then uses that here. So if you do that, then if you're interested, I would invite you to team up with me and that we would then release this piece of software together and author a paper or a post together. So that's my hope, is that one of you or a group of you will make that happen. That would be super exciting because I think this would be great. We'd be showing people something pretty cool about the idea of writing GPU algorithms today. In fact, I found just during the break, here's a whole paper about how to write k-means with CUDA. It used to be so much work. This is without even including any kind of approximate nearest neighbor piece or whatever. So I think this would be great. So hopefully that will happen. Okay, and look, it's just the right answer. I guess to do it properly, we should also be replacing the Gaussian kernel bandwidth with something that we figure out dynamically rather than have it hard-coded. So big change, we're going to learn about chatbots. So we're going to start here with Slate. Facebook thinks it has found the secret to making bots less dumb. So this talks about a new thing called memory networks, which was demonstrated by Facebook. You can feed it sentences that convey key plot points and lord of the rings and then ask it various questions. Published a new paper on Archive that generalizes the approach. There was another long article about this on Popular Science in which they described its early progress towards a truly intelligent AI. Donald Kuhn is excited about working on a memory network, giving the ability to retain information. You can tell the network a story and have it answer questions. And so it even has this little gif. So in the article they've got this little example showing reading a story of Lord of the Rings and then asking various questions about Lord of the Rings, and it all looks pretty impressive. So we're going to implement this paper. And the paper is called End-to-End Memory Networks. The paper was actually not shown on Lord of the Rings, but was actually shown on something called Babby. I don't know, Babby or Baby, I'm never quite sure which one it is. It's a paper describing a synthetic dataset towards AI complete question answering, a set of prerequisite toy tasks. I saw a cute tweet last week explaining the meaning of various different types of titles of papers. It's basically saying towards means we've actually made no progress whatsoever. So we'll take this with a grain of salt. So these introduce the Babby tasks, and the Babby tasks are probably best described by showing an example. Here's an example. So each task is basically a story. A story contains a list of sentences. A sentence contains a list of words. At the end of the story is a query to which there is an answer. So the sentences are ordered in time. So where is Daniel? We have to go backwards. This says where John is. This says where Daniel is. Daniel is going to the bathroom. So Daniel is in the bathroom. So this is what the Babby tasks look like. There's a number of different structures. This is called a one-supporting-fact structure, which is to say you only have to go back and find one sentence in the story to figure out the answer. We're also going to look at two-supporting-fact stories, which is ones where you're going to have to look twice. So the reading in these datasets is not remotely interesting. They're just a text file, we can parse them out. There's various different text files for the various different tasks. If you're interested in the various different tasks, you can check out the paper. We're going to be looking at single-supporting-fact and two-supporting-fact. They have some with 10,000 examples and some with 1,000 examples. The goal is to be able to solve every one of their challenges with just 1,000 examples. And this paper is not successful at that goal, but it makes some movement towards it. So basically we're going to put that into a bunch of different lists of stories along with their queries. We can start off by having a look at some statistics about them. So the first is, for each story, what's the maximum number of sentences in a story? And the answer is 10. So Lord of the Rings it ain't. In fact, if you go back and you look at the GIF, when it says read story, Lord of the Rings, that's the whole Lord of the Rings. The total number of different words in this thing is 32. The maximum length of any sentence in a story is 8. The maximum number of words in any query is 4. So we're immediately thinking what the hell, because this was presented by the press as being the secret to making bots less dumb, and showed us that they took a story and summarized Lord of the Rings, made a plot point and asked various questions. And clearly that's not entirely true. If you look at even the stories, the first word is always somebody's name. The second word here is always some synonym for move. There's then a bunch of prepositions, and then the last word is always place. So these story tasks are very, very, very toy. So immediately we're thinking maybe this is not the step to making bots less dumb, or whatever they said here. A truly intelligent AI. Maybe it's towards a truly intelligent AI. So to get this into Keras, we need to turn it into a tensor in which everything is the same size. So we use pad sequences for that like we did in the last part of the course, which will add zeros to make sure that everything is the same size. So the other thing we'll do is we will create a dictionary from words to integers to turn every word into an index. So we're going to turn every word into an index and then pad them so that they're all the same length. And then that's going to give us inputs train, 10,000 stories, each one of 10 sentences, each one of 8 words, anything that's not 10 sentences long is going to get sentences of just zeros, any sentence that's not 8 words long will get some zeros appended to that. And you know for the test, except we just got 1000. So how do we do this? Not surprisingly, we're going to use embeddings. Now we've never done this before, we have to turn a sentence into an embedding, not just a word into an embedding. So there's lots of interesting ways of turning a sentence into an embedding. But when you're just doing towards intelligent AI, you don't do any of them, you instead just add the embeddings up. And that's what happened in this paper. And if you look at the way it was set up, you can see why. You can just add the embeddings up. Mary, John and Sandra, they only ever appear in one place, they're always the object of this. The verb is always the same thing, the prepositions are always meaningless, and the last word is always a place. So to figure out what a whole sentence says, you can just add up the word concepts. The order of them doesn't make any difference, there's no nots, there's nothing that makes language remotely complicated or interesting. So what we're going to do is we're going to create an input for our stories with the number of sentences and the length of each one. We're going to take each word and put it through an embedding. So that's what time distributed is doing here. It's putting each word through a separate embedding. And then we do a lambda layer to add them up. So here is our very sophisticated approach to creating sentence embeddings. So we do that for our story. So we end up with something which rather than being 10 by 8, i.e. 10 sentences by 8 words, is now 10 by 20, that is 10 sentences by length 20 embedding. So each one of our 10 sentences has been turned into a length 20 embedding. And we're just starting with a random embedding. We're not going to use Word2Vec or anything because we don't need the complexity of that vocabulary model. We're going to do exactly the same thing for the query. We don't need to use time distributed this time. We can just take the query because this time we have just one query. So we can do the embedding, sum it up, and then we use reshape to add a unit access to the front so that it's now the same basic rank. We now have one question of embedding to length 20. So we have 10 sentences of the story and one query. So what is the memory network, or more specifically the more advanced end-to-end memory network? And the answer is, it is this. As per usual, when you get down to it, it's less than a page of code to do these things. Let's draw this before we look at the code. So we have a bunch of sentences. Let's just use 4 sentences for now. So each sentence contained a bunch of words. We took each word and we turned them into an embedding. And then we summed all of those embeddings up to get an embedding for that sentence. So each sentence was turned into an embedding and they were of length 20. Then we took the query. This is my query. Same kind of idea, bunch of words, which we got embeddings for, and we added them up to get an embedding for our question. So to do a memory network, what we're going to do is we're going to take each of these embeddings and we're going to combine each one with a question or query. And we're just going to take a dot product. So we're going to end up with 4 dot products from each sentence of the story times the query. So what does the dot product do? It basically says how similar 2 things are. When one thing is big, the other thing is big, when one thing is small, the other thing is small. Those things both make the dot product bigger. So these are basically going to be 4 vectors describing how similar each of our 4 sentences to the query. So that's step 1. Step 2 is to stick them through a softmax. So remember dot product is written as a scalar. So we now have 4 scalars, and they add up to 1. And they each are basically related to how similar is the query to each of the 4 sentences. We're now going to create a totally separate embedding of each of the sentences in our story by creating a totally separate embedding for each word. So we're basically just going to create a new random embedding matrix for each word to start with, sum them all together, and that's going to give us a new embedding. This one they call C I believe. And all we're going to do is we're going to multiply each one of these embeddings by the equivalent softmax as a weighting and then just add them all together. So we're just going to have S1234, going to be C1 times S1 plus C2 times S2 and then divided by S1 plus. Actually you don't need to divide by because they add to 1. So that's going to be our final result, which is going to be of length 20. So this thing is a vector of length 20. And then we're going to take that and put it through a single dense layer, and we're going to get back the answer. And that whole thing is the memory network. It's incredibly simple. There's nothing deep in terms of deep learning, there's almost no non-linearities. It doesn't seem like it's likely to be able to do very much, but I guess we haven't given it very much to do. So let's take a look at the code version. So in that last step you said the answer, was that really like the embedding of the answer and then it has to get the reverse lookup? It's the softmax of the answer and then you have to do an argmax. So here it is. We've got the story times the query, the embedding of the story times the embedding of the query, the dot product. We do a softmax. Softmax works in the last dimension, so I just have to reshape to get rid of the unit axis and then I reshape again to put the unit axis back on again. But the reshapes aren't doing anything interesting. So it's just a dot product followed by softmax, and that gives us the weights. So now we're going to take each weight and multiply it by the second set of embeddings, embedding C. In order to do this, I just used the dot product again, but because of the fact that you've got a unit axis there, this is actually just doing a very simple weighted average. And again, reshape to get rid of the unit axis so that we can stick it through a dense layer with a softmax, and that gives us our final result. So what this is effectively doing is it's basically saying, okay, how similar is the query to each one of the sentences in the story? Use that to create a bunch of weights, and then these things here are basically the answers. If story number 1 was where the answer was, then we're going to use this one, story number 2, 3 and 4. Because there's a single linear layer at the very end, so it doesn't really get to do much computation. So it basically has to learn what the answer represented by each story is. And again, this is lucky because from the original dataset, the answer to every question is the last word of the sentence. Where is Frodo's ring? So that's why we have this incredibly simple final piece. So this is an interesting use of Keras. We've created a model which is in no possible way deep learning. But it's bunches of tensors and layers that are stuck together, so it has some inputs and it has an output, so we can call it a model. We can compile it with an optimizer and a loss, and then we can fit it. So it's kind of interesting how you can use Keras for things which don't really use any of the normal layers in any normal way. And as you can see, it works for what it's worth. We solved this problem. And the particular problem we solved here is the one supporting that problem. In fact, it worked in less than one epoch. More interesting is two supporting parts. Actually before I do that, I'll just point out something interesting. We could create another model now that this is already trained, which is to return not the final answer, but the value of the weights. And so we can now go back and say, for a particular story, what are the weights? So let's do F.predict rather than answer.predict. So for this story, where is Sandra? Daniel, Mary, Sandra. Sandra went to the bathroom. Where is Sandra? Bathroom. So for this particular story, the weights are here. You can see that the weight for sentence number 2 is 0.98. So we can actually look inside the model and find out what sentences it's using to answer this question. Question, Would it not make more sense to concat the embeddings rather than sum them? Not for this particular problem because of the way the vocabulary is structured when the sentences are structured. You would also have to deal with the variable length. We've used padding to make them the same length. If you wanted to use this in real life, you would need to come up with a better sentence embedding, which presumably might be an RN or something like that. You need to deal with things like not and location of subject and object and so forth. One thing to point out is that the order of the sentences matters. What I actually did when I preprocessed it was I added a 0 colon, 1 colon, whatever to the start of each sentence so that it would actually be able to learn the time order of sentences. So this is like another token that I added. Because you're wondering what that was, that was something that I added in the preprocessing. So one nice thing with memory networks is we can look and see if they're not working, in particular why they're not working. So, MultiHop. So let's now look at an example of a to-supporting-fats story. It's mildly more interesting. We still only have one type of verb with various synonyms and a small number of subjects and a small number of objects. So it's basically the same. But now, to answer your question, we have to go down through two hots. So where is the milk? Let's find the milk. Daniel left the milk there. Where is Daniel? Daniel traveled to the hallway. Where is the milk? Hallway. So that's what we have to be able to do this time. And so what we're going to do is exactly the same thing as we did before. We're going to take our whole little model, do the embedding, reshape.softmax, reshape.denselayer.sum, and we're going to call this one hop. So this whole picture is going to become one hop. And what we're then going to do is we're going to take this and go back and replace the query with our new output. So at each step, each hop, we're going to replace the query with the result of our memory network. And so that way, the memory network can learn to recognize that the first thing I need is the milk, search back, find milk. I now have the milk, now you need to update the query to where is Daniel. Now go back, find Daniel. So the memory network in multi-hop mode basically does this whole thing again and again and again, replacing the query each time. So that's why I just took the whole set of steps and chucked it into a single function. And so then I just go, okay, response, story is one hop, response, story is one hop on that, and you can keep repeating that again and again and again. And then at the end, get our output, that's our model, compile, fit. I had real trouble getting this to fit nicely. I had to play around a lot with learning rates and batch sizes and whatever else, but I did eventually get it up to.999 accuracy. So this is kind of an unusual class for me to be teaching, particularly compared to part 1 where it was best practices. Clearly this is anything but. I'm kind of showing you something which was maybe the most popular request, which was to teach us about chatbots. But let's be honest, who has ever used a chatbot that's not terrible? And the reason no one's used a chatbot that's not terrible is that the current state of the art is terrible. So chatbots have their place, and indeed one of the students in class has written a really interesting analysis of this, which hopefully she'll share on the forum. But that place has really lots of heuristics and carefully set up vocabularies and selecting from small sets of answers and so forth. It's not general purpose, here's a story, ask anything you like about it, here are some answers. It's not to say we won't get there, I sure hope we will, but the kind of incredible hype we had around neural-turing machines and memory networks and end-to-end memory networks is kind of, as you can see, even when you just look at the dataset, what they worked on is kind of crazy. So that is not quite the final conclusion of this though, because yesterday a paper came out which showed how to identify buffer overruns in computer source code using memory networks. And so it kind of spoiled my whole narrative that somebody seems to have actually used this technology for something effectively. And I guess when you think about it, it makes some sense. So in case you don't know what a buffer overrun is, that's like if you're writing in an unsafe language, properly C, you allocate some memory that's going to store some result or some input, and you try to put into that memory something bigger than the amount that you allocated, it basically spills out the end. And in the best case, it crashes. In the worst case, somebody figures out how to get exactly the right code to spill out into exactly the right place and ends up taking over your machine. So buffer overruns are horrible things. And the idea of being able to find them, I can actually see it does look a lot like this memory network. You kind of have to see where was that variable set, and then where was the thing that was set from set, and where was the original thing allocated. It's kind of like just going back through the source code. The vocabulary is pretty straightforward, it's just the variables that have been defined. So that's kind of interesting. I haven't had a chance to really study the paper yet, but it's no chat bot, but maybe there is a room for memory networks already after all. Question. Is there a way to visualize what the neural network has learned for the text? Answer. There is no neural network. If you mean the embeddings, you can look at the embeddings easily enough. The whole thing is so simple, it's very easy to look at every embedding. As I mentioned, we looked at visualizing the weights that came out of the softmax. We don't even need to look at it in order to figure out what it learned. Based on the fact that this is just a small number of simple linear steps, we know that it basically has to learn what each sentence answer can be. Sentence number 3's answer will always be milk, sentence number 4's answer will always be four-way or whatever. And then the embeddings of the weights are going to have to basically learn how to come up with what's going to be a similar embedding to the query. In fact, I think you can even make them the same embedding so that these dot products basically give you something that gives you similarity scores. So this is really a very simple, largely linear model. It doesn't require too much visualizing. So having said all that, none of this is to say that memory networks are useless. They were created by very smart people with impressive pedigree and deep learning. This is very early and this tends to happen in popular press. They kind of get over-excited about things. Although in this case, I don't think we can blame the press, I think we have to blame Facebook for creating a ridiculous demo like this. This is clearly created to give people the wrong idea, which I find very surprising from people like Jan LeCun who normally do the opposite of that kind of thing. So this is not really the press' fault in this case. But this may well turn out to be a critical component in chatbots and Q&A systems and whatever else. But we're not there yet. I had a good chat to Stephen Meridy the other day, who's a researcher I respect a lot and also somebody I like. I asked him what he thought was the most exciting research in this direction at the moment, and he mentioned something that I was also very excited about, which is called Recurrent Entity Networks. The Recurrent Entity Network paper is the first to solve all of the Babbi tasks with 100% accuracy. Now take of that what you will. I don't know how much that means. They're synthetic tasks. One of the things that Stephen Meridy actually pointed out in a blog post is that even the basic kind of coding of how they're created is pretty bad. They have lots of replicas and the whole thing is a bit of a mess. Nonetheless, this is an interesting approach, so if you're interested in memory networks, this is certainly something you can look at. And I do think this is likely to be an important direction. Having said all that, one of the key reasons I wanted to look at these memory networks is not only because it was the largest request I think from the forums for this part of the course, but also because it introduces something that's going to be critical for the next couple of lessons, which is the concept of attention. Attention or models are models where we have to do exactly what we just looked at, which is basically find out at each time which part of a story to look at next, or which part of an image to look at next, or which part of a sentence to look at next. And so the task that we're going to be trying to get at over the next lesson or two is going to be to translate French into English. So this is clearly not a toy task, this is a very challenging task. And one of the challenges is that in a particular French sentence which has got some bunch of words, it's likely to turn into an English sentence with some different bunch of words. And maybe these particular words here might be this translation here, and this one might be this one, and this one might be this one. And so as you go through, you need some way of saying which word do I look at next. So that's going to be the attentional model. And so what we're going to do is we're going to be trying to come up with a proper RNN, like an LSTM or a GRU or whatever, where we're going to change it so that inside the RNN it's going to actually have some way of figuring out which part of the input to look at next. So that's the basic idea of attentional models. And so interestingly, during this time that memory networks and neural-turing machines and stuff were getting all this huge amount of press attention very quietly in the background at exactly the same time, attentional models were appearing as well. And it's the attentional models for language that have really turned out to be critical. So you've probably seen all of the press about Google's new neural translation system, and that really is everything that it's claimed to be. It really is basically one giant neural network that can translate any pair of languages. The accuracy of those translations is far beyond anything that's happened before. And the basic structure of that neural net, as we're going to learn, is not that different to what we've already learned. It's just going to have this one extra step, which is attention. And kind of depending on how interested you guys are on the details of this neural translation system, it turns out that there are also lots of little tweaks. The tweaks are kind of around like, okay, you've got a really big vocabulary, some of the words appear very rarely, how do you build a system that can understand how to translate those really rare words, for example. And also just kind of things like how do you deal with the memory issues around having huge embedding matrices of 160,000 words and stuff like that. So there's lots of details, and the nice thing is that because Google has ended up putting this thing in production, all of these little details have answers now. Those answers are all really interesting. There aren't really, on the whole, great examples of all of those things put together. So one of the things interesting here will be that you'll have opportunities to do that. Generally speaking, the blog posts about these neural translation systems tend to be at a pretty high level. They describe roughly how these approaches work. But Google's complete neural translation system is not out there, you can't download it and see the code. So we'll see how we go, but we'll kind of do it piece by piece. I guess one other thing to mention about the memory network is that Keras actually comes with an end-to-end memory network example in the Keras GitHub, which weirdly enough, when I actually looked at it, it turns out doesn't implement this at all. And so even on the single supporting fact thing, it takes many, many generations and doesn't get to 100% accuracy. And I found this quite surprising to discover that once you start getting to some of these more recent advances, or not just a standard CNN or whatever, it's just less and less common that you actually find code that's correct and that works. And so this memory network example was one of them. So if you actually go into the Keras GitHub and look at examples, and go and have a look and download the memory network, you'll find that you don't get results anything like this. If you look at the code, you'll see that it really doesn't do this at all. So I just kind of wanted to mention that as a bit of a warning that you're kind of at the point now where you might want to take with a grain of salt blog posts you read, or even some papers that you read, well worth experimenting with them and assuming you should start with the assumption that you can do it better. And maybe even start with the assumption that you can't necessarily trust all of the conclusions that you've read, because the vast majority of the time, in my experience putting together this part of the course, the vast majority of the time the stuff out there is just wrong. And in cases like, I deeply respect the Keras authors and the Keras source code, but even in that case, this is wrong. I think that's an important point to be aware of. I think we're going to finish 5 minutes early for a change. I think that's never happened before. So thanks everybody. So this week, hopefully we can have a look at the data science bowl, make a million dollars, create a new PyTorch approximate nearest neighbors algorithm, and then when you're done maybe figure out the next stage for memory networks. Thanks everybody.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 14.88, "text": " So this week, obviously quite a bit just to get set up to get results from this week in", "tokens": [407, 341, 1243, 11, 2745, 1596, 257, 857, 445, 281, 483, 992, 493, 281, 483, 3542, 490, 341, 1243, 294], "temperature": 0.0, "avg_logprob": -0.18266496501985144, "compression_ratio": 1.5394736842105263, "no_speech_prob": 0.013423529453575611}, {"id": 1, "seek": 0, "start": 14.88, "end": 19.82, "text": " terms of needing all of ImageNet and that kind of thing, and getting all that working.", "tokens": [2115, 295, 18006, 439, 295, 29903, 31890, 293, 300, 733, 295, 551, 11, 293, 1242, 439, 300, 1364, 13], "temperature": 0.0, "avg_logprob": -0.18266496501985144, "compression_ratio": 1.5394736842105263, "no_speech_prob": 0.013423529453575611}, {"id": 2, "seek": 0, "start": 19.82, "end": 25.2, "text": " So I know that a lot of you are still working through that.", "tokens": [407, 286, 458, 300, 257, 688, 295, 291, 366, 920, 1364, 807, 300, 13], "temperature": 0.0, "avg_logprob": -0.18266496501985144, "compression_ratio": 1.5394736842105263, "no_speech_prob": 0.013423529453575611}, {"id": 3, "seek": 2520, "start": 25.2, "end": 30.88, "text": " I did want to mention a couple of reminders that I've noticed.", "tokens": [286, 630, 528, 281, 2152, 257, 1916, 295, 43458, 300, 286, 600, 5694, 13], "temperature": 0.0, "avg_logprob": -0.1645209347760236, "compression_ratio": 1.6720647773279351, "no_speech_prob": 0.00010070105781778693}, {"id": 4, "seek": 2520, "start": 30.88, "end": 37.519999999999996, "text": " One is that in general, we have that thing on the wiki about how to use the notebooks,", "tokens": [1485, 307, 300, 294, 2674, 11, 321, 362, 300, 551, 322, 264, 261, 9850, 466, 577, 281, 764, 264, 43782, 11], "temperature": 0.0, "avg_logprob": -0.1645209347760236, "compression_ratio": 1.6720647773279351, "no_speech_prob": 0.00010070105781778693}, {"id": 5, "seek": 2520, "start": 37.519999999999996, "end": 42.72, "text": " and we really strongly advise that you don't open up the notebook we give you and click", "tokens": [293, 321, 534, 10613, 18312, 300, 291, 500, 380, 1269, 493, 264, 21060, 321, 976, 291, 293, 2052], "temperature": 0.0, "avg_logprob": -0.1645209347760236, "compression_ratio": 1.6720647773279351, "no_speech_prob": 0.00010070105781778693}, {"id": 6, "seek": 2520, "start": 42.72, "end": 45.120000000000005, "text": " shift-enter through it again and again.", "tokens": [5513, 12, 14278, 807, 309, 797, 293, 797, 13], "temperature": 0.0, "avg_logprob": -0.1645209347760236, "compression_ratio": 1.6720647773279351, "no_speech_prob": 0.00010070105781778693}, {"id": 7, "seek": 2520, "start": 45.120000000000005, "end": 48.96, "text": " You're not really going to learn much from that.", "tokens": [509, 434, 406, 534, 516, 281, 1466, 709, 490, 300, 13], "temperature": 0.0, "avg_logprob": -0.1645209347760236, "compression_ratio": 1.6720647773279351, "no_speech_prob": 0.00010070105781778693}, {"id": 8, "seek": 2520, "start": 48.96, "end": 52.04, "text": " But go back to that wiki page, it's like the first thing that's mentioned in the first", "tokens": [583, 352, 646, 281, 300, 261, 9850, 3028, 11, 309, 311, 411, 264, 700, 551, 300, 311, 2835, 294, 264, 700], "temperature": 0.0, "avg_logprob": -0.1645209347760236, "compression_ratio": 1.6720647773279351, "no_speech_prob": 0.00010070105781778693}, {"id": 9, "seek": 5204, "start": 52.04, "end": 55.96, "text": " paragraph of the home page or wiki is how to use the notebooks.", "tokens": [18865, 295, 264, 1280, 3028, 420, 261, 9850, 307, 577, 281, 764, 264, 43782, 13], "temperature": 0.0, "avg_logprob": -0.18796338457049747, "compression_ratio": 1.7849462365591398, "no_speech_prob": 4.757428905577399e-05}, {"id": 10, "seek": 5204, "start": 55.96, "end": 60.4, "text": " Basically the idea is try to start with a fresh notebook, think about what you think", "tokens": [8537, 264, 1558, 307, 853, 281, 722, 365, 257, 4451, 21060, 11, 519, 466, 437, 291, 519], "temperature": 0.0, "avg_logprob": -0.18796338457049747, "compression_ratio": 1.7849462365591398, "no_speech_prob": 4.757428905577399e-05}, {"id": 11, "seek": 5204, "start": 60.4, "end": 63.48, "text": " you need to do first, try and do that thing if you have no idea.", "tokens": [291, 643, 281, 360, 700, 11, 853, 293, 360, 300, 551, 498, 291, 362, 572, 1558, 13], "temperature": 0.0, "avg_logprob": -0.18796338457049747, "compression_ratio": 1.7849462365591398, "no_speech_prob": 4.757428905577399e-05}, {"id": 12, "seek": 5204, "start": 63.48, "end": 69.44, "text": " Then you can go to the existing notebook, take a peek, close it again, try and re-implement", "tokens": [1396, 291, 393, 352, 281, 264, 6741, 21060, 11, 747, 257, 19604, 11, 1998, 309, 797, 11, 853, 293, 319, 12, 332, 43704], "temperature": 0.0, "avg_logprob": -0.18796338457049747, "compression_ratio": 1.7849462365591398, "no_speech_prob": 4.757428905577399e-05}, {"id": 13, "seek": 5204, "start": 69.44, "end": 70.44, "text": " what you just saw.", "tokens": [437, 291, 445, 1866, 13], "temperature": 0.0, "avg_logprob": -0.18796338457049747, "compression_ratio": 1.7849462365591398, "no_speech_prob": 4.757428905577399e-05}, {"id": 14, "seek": 5204, "start": 70.44, "end": 76.16, "text": " As much as possible, really don't just shift-enter through the notebooks.", "tokens": [1018, 709, 382, 1944, 11, 534, 500, 380, 445, 5513, 12, 14278, 807, 264, 43782, 13], "temperature": 0.0, "avg_logprob": -0.18796338457049747, "compression_ratio": 1.7849462365591398, "no_speech_prob": 4.757428905577399e-05}, {"id": 15, "seek": 5204, "start": 76.16, "end": 80.44, "text": " I know some of you are doing it because there are threads on the forum saying, I was shift-entering", "tokens": [286, 458, 512, 295, 291, 366, 884, 309, 570, 456, 366, 19314, 322, 264, 17542, 1566, 11, 286, 390, 5513, 12, 317, 1794], "temperature": 0.0, "avg_logprob": -0.18796338457049747, "compression_ratio": 1.7849462365591398, "no_speech_prob": 4.757428905577399e-05}, {"id": 16, "seek": 8044, "start": 80.44, "end": 83.28, "text": " through the notebook and this thing didn't work.", "tokens": [807, 264, 21060, 293, 341, 551, 994, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.22344871097140842, "compression_ratio": 1.5746606334841629, "no_speech_prob": 1.9832748876069672e-05}, {"id": 17, "seek": 8044, "start": 83.28, "end": 88.88, "text": " And somebody's like, well, that's because that thing's not defined yet.", "tokens": [400, 2618, 311, 411, 11, 731, 11, 300, 311, 570, 300, 551, 311, 406, 7642, 1939, 13], "temperature": 0.0, "avg_logprob": -0.22344871097140842, "compression_ratio": 1.5746606334841629, "no_speech_prob": 1.9832748876069672e-05}, {"id": 18, "seek": 8044, "start": 88.88, "end": 92.6, "text": " Consider yourself busted.", "tokens": [17416, 1803, 41074, 13], "temperature": 0.0, "avg_logprob": -0.22344871097140842, "compression_ratio": 1.5746606334841629, "no_speech_prob": 1.9832748876069672e-05}, {"id": 19, "seek": 8044, "start": 92.6, "end": 99.0, "text": " The other thing to remind you about is that the goal of part 2 is to get you to a point", "tokens": [440, 661, 551, 281, 4160, 291, 466, 307, 300, 264, 3387, 295, 644, 568, 307, 281, 483, 291, 281, 257, 935], "temperature": 0.0, "avg_logprob": -0.22344871097140842, "compression_ratio": 1.5746606334841629, "no_speech_prob": 1.9832748876069672e-05}, {"id": 20, "seek": 8044, "start": 99.0, "end": 101.47999999999999, "text": " where you can read papers.", "tokens": [689, 291, 393, 1401, 10577, 13], "temperature": 0.0, "avg_logprob": -0.22344871097140842, "compression_ratio": 1.5746606334841629, "no_speech_prob": 1.9832748876069672e-05}, {"id": 21, "seek": 8044, "start": 101.47999999999999, "end": 106.2, "text": " And the reason for that is because you kind of know the best practices now, so anytime", "tokens": [400, 264, 1778, 337, 300, 307, 570, 291, 733, 295, 458, 264, 1151, 7525, 586, 11, 370, 13038], "temperature": 0.0, "avg_logprob": -0.22344871097140842, "compression_ratio": 1.5746606334841629, "no_speech_prob": 1.9832748876069672e-05}, {"id": 22, "seek": 10620, "start": 106.2, "end": 111.64, "text": " you want to do something beyond what we've learned, you're going to be implementing things", "tokens": [291, 528, 281, 360, 746, 4399, 437, 321, 600, 3264, 11, 291, 434, 516, 281, 312, 18114, 721], "temperature": 0.0, "avg_logprob": -0.1201704216003418, "compression_ratio": 1.7925311203319503, "no_speech_prob": 7.411213118757587e-06}, {"id": 23, "seek": 10620, "start": 111.64, "end": 116.92, "text": " from papers or probably going beyond that and implementing new things.", "tokens": [490, 10577, 420, 1391, 516, 4399, 300, 293, 18114, 777, 721, 13], "temperature": 0.0, "avg_logprob": -0.1201704216003418, "compression_ratio": 1.7925311203319503, "no_speech_prob": 7.411213118757587e-06}, {"id": 24, "seek": 10620, "start": 116.92, "end": 122.16, "text": " Reading a new paper in an area that you haven't looked at before is, at least to me, somewhat", "tokens": [29766, 257, 777, 3035, 294, 364, 1859, 300, 291, 2378, 380, 2956, 412, 949, 307, 11, 412, 1935, 281, 385, 11, 8344], "temperature": 0.0, "avg_logprob": -0.1201704216003418, "compression_ratio": 1.7925311203319503, "no_speech_prob": 7.411213118757587e-06}, {"id": 25, "seek": 10620, "start": 122.16, "end": 124.68, "text": " terrifying.", "tokens": [18106, 13], "temperature": 0.0, "avg_logprob": -0.1201704216003418, "compression_ratio": 1.7925311203319503, "no_speech_prob": 7.411213118757587e-06}, {"id": 26, "seek": 10620, "start": 124.68, "end": 131.0, "text": " On the other hand, reading the paper for the thing that we already studied last week hopefully", "tokens": [1282, 264, 661, 1011, 11, 3760, 264, 3035, 337, 264, 551, 300, 321, 1217, 9454, 1036, 1243, 4696], "temperature": 0.0, "avg_logprob": -0.1201704216003418, "compression_ratio": 1.7925311203319503, "no_speech_prob": 7.411213118757587e-06}, {"id": 27, "seek": 10620, "start": 131.0, "end": 134.68, "text": " isn't terrifying at all because you already know what the paper says.", "tokens": [1943, 380, 18106, 412, 439, 570, 291, 1217, 458, 437, 264, 3035, 1619, 13], "temperature": 0.0, "avg_logprob": -0.1201704216003418, "compression_ratio": 1.7925311203319503, "no_speech_prob": 7.411213118757587e-06}, {"id": 28, "seek": 13468, "start": 134.68, "end": 139.64000000000001, "text": " So I always have that in the assignments each week, read the paper for the thing you just", "tokens": [407, 286, 1009, 362, 300, 294, 264, 22546, 1184, 1243, 11, 1401, 264, 3035, 337, 264, 551, 291, 445], "temperature": 0.0, "avg_logprob": -0.16344335873921711, "compression_ratio": 1.8587786259541985, "no_speech_prob": 9.972742191166617e-06}, {"id": 29, "seek": 13468, "start": 139.64000000000001, "end": 140.64000000000001, "text": " learned about.", "tokens": [3264, 466, 13], "temperature": 0.0, "avg_logprob": -0.16344335873921711, "compression_ratio": 1.8587786259541985, "no_speech_prob": 9.972742191166617e-06}, {"id": 30, "seek": 13468, "start": 140.64000000000001, "end": 145.48000000000002, "text": " Go back over it and please ask on the forums if there's a bit of notation or anything that", "tokens": [1037, 646, 670, 309, 293, 1767, 1029, 322, 264, 26998, 498, 456, 311, 257, 857, 295, 24657, 420, 1340, 300], "temperature": 0.0, "avg_logprob": -0.16344335873921711, "compression_ratio": 1.8587786259541985, "no_speech_prob": 9.972742191166617e-06}, {"id": 31, "seek": 13468, "start": 145.48000000000002, "end": 150.4, "text": " you don't understand, or if there's something we heard in class that you can't see in the", "tokens": [291, 500, 380, 1223, 11, 420, 498, 456, 311, 746, 321, 2198, 294, 1508, 300, 291, 393, 380, 536, 294, 264], "temperature": 0.0, "avg_logprob": -0.16344335873921711, "compression_ratio": 1.8587786259541985, "no_speech_prob": 9.972742191166617e-06}, {"id": 32, "seek": 13468, "start": 150.4, "end": 154.32, "text": " paper, or particularly interesting, if you see something in the paper that you don't", "tokens": [3035, 11, 420, 4098, 1880, 11, 498, 291, 536, 746, 294, 264, 3035, 300, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.16344335873921711, "compression_ratio": 1.8587786259541985, "no_speech_prob": 9.972742191166617e-06}, {"id": 33, "seek": 13468, "start": 154.32, "end": 157.34, "text": " think we mentioned in class.", "tokens": [519, 321, 2835, 294, 1508, 13], "temperature": 0.0, "avg_logprob": -0.16344335873921711, "compression_ratio": 1.8587786259541985, "no_speech_prob": 9.972742191166617e-06}, {"id": 34, "seek": 13468, "start": 157.34, "end": 164.4, "text": " So that's the reason that I really encourage you to read the papers that we studied for", "tokens": [407, 300, 311, 264, 1778, 300, 286, 534, 5373, 291, 281, 1401, 264, 10577, 300, 321, 9454, 337], "temperature": 0.0, "avg_logprob": -0.16344335873921711, "compression_ratio": 1.8587786259541985, "no_speech_prob": 9.972742191166617e-06}, {"id": 35, "seek": 16440, "start": 164.4, "end": 167.0, "text": " the topics we studied in class.", "tokens": [264, 8378, 321, 9454, 294, 1508, 13], "temperature": 0.0, "avg_logprob": -0.1630674934387207, "compression_ratio": 1.609375, "no_speech_prob": 3.944113268516958e-05}, {"id": 36, "seek": 16440, "start": 167.0, "end": 172.68, "text": " I think for those of you like me who don't have a technical academic background, it's", "tokens": [286, 519, 337, 729, 295, 291, 411, 385, 567, 500, 380, 362, 257, 6191, 7778, 3678, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.1630674934387207, "compression_ratio": 1.609375, "no_speech_prob": 3.944113268516958e-05}, {"id": 37, "seek": 16440, "start": 172.68, "end": 178.16, "text": " a really great way to familiarize yourself with notation.", "tokens": [257, 534, 869, 636, 281, 4963, 1125, 1803, 365, 24657, 13], "temperature": 0.0, "avg_logprob": -0.1630674934387207, "compression_ratio": 1.609375, "no_speech_prob": 3.944113268516958e-05}, {"id": 38, "seek": 16440, "start": 178.16, "end": 183.28, "text": " And I'm actually really looking forward for some of you asking about notation on the forums", "tokens": [400, 286, 478, 767, 534, 1237, 2128, 337, 512, 295, 291, 3365, 466, 24657, 322, 264, 26998], "temperature": 0.0, "avg_logprob": -0.1630674934387207, "compression_ratio": 1.609375, "no_speech_prob": 3.944113268516958e-05}, {"id": 39, "seek": 16440, "start": 183.28, "end": 185.28, "text": " so I can explain some of it to you.", "tokens": [370, 286, 393, 2903, 512, 295, 309, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.1630674934387207, "compression_ratio": 1.609375, "no_speech_prob": 3.944113268516958e-05}, {"id": 40, "seek": 16440, "start": 185.28, "end": 191.72, "text": " There's a few key things that keep coming up in notation like probability distributions", "tokens": [821, 311, 257, 1326, 2141, 721, 300, 1066, 1348, 493, 294, 24657, 411, 8482, 37870], "temperature": 0.0, "avg_logprob": -0.1630674934387207, "compression_ratio": 1.609375, "no_speech_prob": 3.944113268516958e-05}, {"id": 41, "seek": 16440, "start": 191.72, "end": 192.76, "text": " and stuff like that.", "tokens": [293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1630674934387207, "compression_ratio": 1.609375, "no_speech_prob": 3.944113268516958e-05}, {"id": 42, "seek": 19276, "start": 192.76, "end": 199.32, "text": " So please feel free, and if you're watching this later in the MOOC, again, feel free to", "tokens": [407, 1767, 841, 1737, 11, 293, 498, 291, 434, 1976, 341, 1780, 294, 264, 49197, 34, 11, 797, 11, 841, 1737, 281], "temperature": 0.0, "avg_logprob": -0.14076771987111944, "compression_ratio": 1.5899581589958158, "no_speech_prob": 5.2247960411477834e-05}, {"id": 43, "seek": 19276, "start": 199.32, "end": 203.07999999999998, "text": " ask on the forum anything that's not clear.", "tokens": [1029, 322, 264, 17542, 1340, 300, 311, 406, 1850, 13], "temperature": 0.0, "avg_logprob": -0.14076771987111944, "compression_ratio": 1.5899581589958158, "no_speech_prob": 5.2247960411477834e-05}, {"id": 44, "seek": 19276, "start": 203.07999999999998, "end": 208.84, "text": " I was kind of interested in following up on some of last week's experiments myself.", "tokens": [286, 390, 733, 295, 3102, 294, 3480, 493, 322, 512, 295, 1036, 1243, 311, 12050, 2059, 13], "temperature": 0.0, "avg_logprob": -0.14076771987111944, "compression_ratio": 1.5899581589958158, "no_speech_prob": 5.2247960411477834e-05}, {"id": 45, "seek": 19276, "start": 208.84, "end": 214.92, "text": " And the thing that I think we all were a bit shocked about was putting this guy into the", "tokens": [400, 264, 551, 300, 286, 519, 321, 439, 645, 257, 857, 12763, 466, 390, 3372, 341, 2146, 666, 264], "temperature": 0.0, "avg_logprob": -0.14076771987111944, "compression_ratio": 1.5899581589958158, "no_speech_prob": 5.2247960411477834e-05}, {"id": 46, "seek": 19276, "start": 214.92, "end": 221.0, "text": " device model and getting out more pictures of similar-looking fish in nets.", "tokens": [4302, 2316, 293, 1242, 484, 544, 5242, 295, 2531, 12, 16129, 3506, 294, 36170, 13], "temperature": 0.0, "avg_logprob": -0.14076771987111944, "compression_ratio": 1.5899581589958158, "no_speech_prob": 5.2247960411477834e-05}, {"id": 47, "seek": 22100, "start": 221.0, "end": 226.92, "text": " And I was kind of curious about how that was working and how well that was working.", "tokens": [400, 286, 390, 733, 295, 6369, 466, 577, 300, 390, 1364, 293, 577, 731, 300, 390, 1364, 13], "temperature": 0.0, "avg_logprob": -0.14187590589801086, "compression_ratio": 1.6491935483870968, "no_speech_prob": 2.2124786482891068e-05}, {"id": 48, "seek": 22100, "start": 226.92, "end": 232.08, "text": " I then completely broke things by training it for a few more epochs.", "tokens": [286, 550, 2584, 6902, 721, 538, 3097, 309, 337, 257, 1326, 544, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.14187590589801086, "compression_ratio": 1.6491935483870968, "no_speech_prob": 2.2124786482891068e-05}, {"id": 49, "seek": 22100, "start": 232.08, "end": 235.72, "text": " And after doing that, I then did an image similarity search again and I got these three", "tokens": [400, 934, 884, 300, 11, 286, 550, 630, 364, 3256, 32194, 3164, 797, 293, 286, 658, 613, 1045], "temperature": 0.0, "avg_logprob": -0.14187590589801086, "compression_ratio": 1.6491935483870968, "no_speech_prob": 2.2124786482891068e-05}, {"id": 50, "seek": 22100, "start": 235.72, "end": 240.92000000000002, "text": " guys who were no longer in nets.", "tokens": [1074, 567, 645, 572, 2854, 294, 36170, 13], "temperature": 0.0, "avg_logprob": -0.14187590589801086, "compression_ratio": 1.6491935483870968, "no_speech_prob": 2.2124786482891068e-05}, {"id": 51, "seek": 22100, "start": 240.92000000000002, "end": 245.72, "text": " So I'm not quite sure what's going on here.", "tokens": [407, 286, 478, 406, 1596, 988, 437, 311, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.14187590589801086, "compression_ratio": 1.6491935483870968, "no_speech_prob": 2.2124786482891068e-05}, {"id": 52, "seek": 22100, "start": 245.72, "end": 250.4, "text": " The other thing I mentioned is when I trained it where my starting point was what we looked", "tokens": [440, 661, 551, 286, 2835, 307, 562, 286, 8895, 309, 689, 452, 2891, 935, 390, 437, 321, 2956], "temperature": 0.0, "avg_logprob": -0.14187590589801086, "compression_ratio": 1.6491935483870968, "no_speech_prob": 2.2124786482891068e-05}, {"id": 53, "seek": 25040, "start": 250.4, "end": 256.2, "text": " at in class, which was just before the final bottleneck layer, I didn't get very good results", "tokens": [412, 294, 1508, 11, 597, 390, 445, 949, 264, 2572, 44641, 547, 4583, 11, 286, 994, 380, 483, 588, 665, 3542], "temperature": 0.0, "avg_logprob": -0.17643000172302786, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.922257069963962e-05}, {"id": 54, "seek": 25040, "start": 256.2, "end": 257.84000000000003, "text": " from this thing.", "tokens": [490, 341, 551, 13], "temperature": 0.0, "avg_logprob": -0.17643000172302786, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.922257069963962e-05}, {"id": 55, "seek": 25040, "start": 257.84000000000003, "end": 263.36, "text": " But when I trained it from the starting point of just after the bottleneck layer, I got", "tokens": [583, 562, 286, 8895, 309, 490, 264, 2891, 935, 295, 445, 934, 264, 44641, 547, 4583, 11, 286, 658], "temperature": 0.0, "avg_logprob": -0.17643000172302786, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.922257069963962e-05}, {"id": 56, "seek": 25040, "start": 263.36, "end": 267.4, "text": " the good results that you saw.", "tokens": [264, 665, 3542, 300, 291, 1866, 13], "temperature": 0.0, "avg_logprob": -0.17643000172302786, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.922257069963962e-05}, {"id": 57, "seek": 25040, "start": 267.4, "end": 271.36, "text": " And again, I don't know why that is and I don't think this has been studied as far as", "tokens": [400, 797, 11, 286, 500, 380, 458, 983, 300, 307, 293, 286, 500, 380, 519, 341, 575, 668, 9454, 382, 1400, 382], "temperature": 0.0, "avg_logprob": -0.17643000172302786, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.922257069963962e-05}, {"id": 58, "seek": 25040, "start": 271.36, "end": 273.48, "text": " I'm aware, so there's lots of open questions here.", "tokens": [286, 478, 3650, 11, 370, 456, 311, 3195, 295, 1269, 1651, 510, 13], "temperature": 0.0, "avg_logprob": -0.17643000172302786, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.922257069963962e-05}, {"id": 59, "seek": 25040, "start": 273.48, "end": 275.32, "text": " But I'll show you something I did then do.", "tokens": [583, 286, 603, 855, 291, 746, 286, 630, 550, 360, 13], "temperature": 0.0, "avg_logprob": -0.17643000172302786, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.922257069963962e-05}, {"id": 60, "seek": 27532, "start": 275.32, "end": 282.12, "text": " I thought, well, that's interesting, I think what's happened here is that when you train", "tokens": [286, 1194, 11, 731, 11, 300, 311, 1880, 11, 286, 519, 437, 311, 2011, 510, 307, 300, 562, 291, 3847], "temperature": 0.0, "avg_logprob": -0.19079967781349463, "compression_ratio": 1.6814516129032258, "no_speech_prob": 3.883021054207347e-05}, {"id": 61, "seek": 27532, "start": 282.12, "end": 286.28, "text": " it for longer, it knows that the important thing is the fish and not the net, and it", "tokens": [309, 337, 2854, 11, 309, 3255, 300, 264, 1021, 551, 307, 264, 3506, 293, 406, 264, 2533, 11, 293, 309], "temperature": 0.0, "avg_logprob": -0.19079967781349463, "compression_ratio": 1.6814516129032258, "no_speech_prob": 3.883021054207347e-05}, {"id": 62, "seek": 27532, "start": 286.28, "end": 289.12, "text": " seems to be now focusing on giving us the same kind of fish.", "tokens": [2544, 281, 312, 586, 8416, 322, 2902, 505, 264, 912, 733, 295, 3506, 13], "temperature": 0.0, "avg_logprob": -0.19079967781349463, "compression_ratio": 1.6814516129032258, "no_speech_prob": 3.883021054207347e-05}, {"id": 63, "seek": 27532, "start": 289.12, "end": 295.12, "text": " These are clearly the exact same type of fish.", "tokens": [1981, 366, 4448, 264, 1900, 912, 2010, 295, 3506, 13], "temperature": 0.0, "avg_logprob": -0.19079967781349463, "compression_ratio": 1.6814516129032258, "no_speech_prob": 3.883021054207347e-05}, {"id": 64, "seek": 27532, "start": 295.12, "end": 298.12, "text": " So I started wondering how could we force it to combine.", "tokens": [407, 286, 1409, 6359, 577, 727, 321, 3464, 309, 281, 10432, 13], "temperature": 0.0, "avg_logprob": -0.19079967781349463, "compression_ratio": 1.6814516129032258, "no_speech_prob": 3.883021054207347e-05}, {"id": 65, "seek": 27532, "start": 298.12, "end": 304.76, "text": " So I tried the most obvious possible thing, I wanted to get more fish in nets.", "tokens": [407, 286, 3031, 264, 881, 6322, 1944, 551, 11, 286, 1415, 281, 483, 544, 3506, 294, 36170, 13], "temperature": 0.0, "avg_logprob": -0.19079967781349463, "compression_ratio": 1.6814516129032258, "no_speech_prob": 3.883021054207347e-05}, {"id": 66, "seek": 30476, "start": 304.76, "end": 312.44, "text": " And I typed word2vec dict tench, plus word2vec dict net divided by 2, get the average of", "tokens": [400, 286, 33941, 1349, 17, 303, 66, 12569, 2064, 339, 11, 1804, 1349, 17, 303, 66, 12569, 2533, 6666, 538, 568, 11, 483, 264, 4274, 295], "temperature": 0.0, "avg_logprob": -0.1776062074254771, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.5936468116706237e-05}, {"id": 67, "seek": 30476, "start": 312.44, "end": 315.36, "text": " the two word vectors, and give me the nearest neighbor.", "tokens": [264, 732, 1349, 18875, 11, 293, 976, 385, 264, 23831, 5987, 13], "temperature": 0.0, "avg_logprob": -0.1776062074254771, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.5936468116706237e-05}, {"id": 68, "seek": 30476, "start": 315.36, "end": 318.52, "text": " And that's what I got.", "tokens": [400, 300, 311, 437, 286, 658, 13], "temperature": 0.0, "avg_logprob": -0.1776062074254771, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.5936468116706237e-05}, {"id": 69, "seek": 30476, "start": 318.52, "end": 323.56, "text": " And then just to prove it wasn't a fluke, I tried the same on tench plus rod, and there's", "tokens": [400, 550, 445, 281, 7081, 309, 2067, 380, 257, 5029, 330, 11, 286, 3031, 264, 912, 322, 2064, 339, 1804, 8685, 11, 293, 456, 311], "temperature": 0.0, "avg_logprob": -0.1776062074254771, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.5936468116706237e-05}, {"id": 70, "seek": 30476, "start": 323.56, "end": 324.56, "text": " my nearest neighbor.", "tokens": [452, 23831, 5987, 13], "temperature": 0.0, "avg_logprob": -0.1776062074254771, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.5936468116706237e-05}, {"id": 71, "seek": 30476, "start": 324.56, "end": 327.71999999999997, "text": " Now do you know what's really freaky about this?", "tokens": [823, 360, 291, 458, 437, 311, 534, 2130, 15681, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.1776062074254771, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.5936468116706237e-05}, {"id": 72, "seek": 30476, "start": 327.71999999999997, "end": 332.92, "text": " If you Google for ImageNet categories, you'll get a list of the 1000 ImageNet categories.", "tokens": [759, 291, 3329, 337, 29903, 31890, 10479, 11, 291, 603, 483, 257, 1329, 295, 264, 9714, 29903, 31890, 10479, 13], "temperature": 0.0, "avg_logprob": -0.1776062074254771, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.5936468116706237e-05}, {"id": 73, "seek": 33292, "start": 332.92, "end": 337.6, "text": " If you search through them, neither net nor rod appear at all.", "tokens": [759, 291, 3164, 807, 552, 11, 9662, 2533, 6051, 8685, 4204, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.12187515792026314, "compression_ratio": 1.6092436974789917, "no_speech_prob": 1.7502748960396275e-05}, {"id": 74, "seek": 33292, "start": 337.6, "end": 343.96000000000004, "text": " I can't begin to imagine why this works, but it does.", "tokens": [286, 393, 380, 1841, 281, 3811, 983, 341, 1985, 11, 457, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.12187515792026314, "compression_ratio": 1.6092436974789917, "no_speech_prob": 1.7502748960396275e-05}, {"id": 75, "seek": 33292, "start": 343.96000000000004, "end": 349.8, "text": " So this device model is clearly doing some pretty deep magic in terms of the understanding", "tokens": [407, 341, 4302, 2316, 307, 4448, 884, 512, 1238, 2452, 5585, 294, 2115, 295, 264, 3701], "temperature": 0.0, "avg_logprob": -0.12187515792026314, "compression_ratio": 1.6092436974789917, "no_speech_prob": 1.7502748960396275e-05}, {"id": 76, "seek": 33292, "start": 349.8, "end": 352.72, "text": " of these objects and their relationships.", "tokens": [295, 613, 6565, 293, 641, 6159, 13], "temperature": 0.0, "avg_logprob": -0.12187515792026314, "compression_ratio": 1.6092436974789917, "no_speech_prob": 1.7502748960396275e-05}, {"id": 77, "seek": 33292, "start": 352.72, "end": 357.56, "text": " Not only are we able to combine things like this, but we're able to combine it with categories", "tokens": [1726, 787, 366, 321, 1075, 281, 10432, 721, 411, 341, 11, 457, 321, 434, 1075, 281, 10432, 309, 365, 10479], "temperature": 0.0, "avg_logprob": -0.12187515792026314, "compression_ratio": 1.6092436974789917, "no_speech_prob": 1.7502748960396275e-05}, {"id": 78, "seek": 33292, "start": 357.56, "end": 360.48, "text": " that it's literally never seen before.", "tokens": [300, 309, 311, 3736, 1128, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.12187515792026314, "compression_ratio": 1.6092436974789917, "no_speech_prob": 1.7502748960396275e-05}, {"id": 79, "seek": 36048, "start": 360.48, "end": 365.92, "text": " It's never seen a rod, we've never told it what a rod looks like, and ditto for a net.", "tokens": [467, 311, 1128, 1612, 257, 8685, 11, 321, 600, 1128, 1907, 309, 437, 257, 8685, 1542, 411, 11, 293, 274, 34924, 337, 257, 2533, 13], "temperature": 0.0, "avg_logprob": -0.18525571904630742, "compression_ratio": 1.6692015209125475, "no_speech_prob": 1.1300723599561024e-05}, {"id": 80, "seek": 36048, "start": 365.92, "end": 369.32, "text": " And I tried quite a few of these combinations and they just kept working.", "tokens": [400, 286, 3031, 1596, 257, 1326, 295, 613, 21267, 293, 436, 445, 4305, 1364, 13], "temperature": 0.0, "avg_logprob": -0.18525571904630742, "compression_ratio": 1.6692015209125475, "no_speech_prob": 1.1300723599561024e-05}, {"id": 81, "seek": 36048, "start": 369.32, "end": 374.04, "text": " Another one I tried was, I understand why this works, which is I tried searching for", "tokens": [3996, 472, 286, 3031, 390, 11, 286, 1223, 983, 341, 1985, 11, 597, 307, 286, 3031, 10808, 337], "temperature": 0.0, "avg_logprob": -0.18525571904630742, "compression_ratio": 1.6692015209125475, "no_speech_prob": 1.1300723599561024e-05}, {"id": 82, "seek": 36048, "start": 374.04, "end": 375.04, "text": " boat.", "tokens": [6582, 13], "temperature": 0.0, "avg_logprob": -0.18525571904630742, "compression_ratio": 1.6692015209125475, "no_speech_prob": 1.1300723599561024e-05}, {"id": 83, "seek": 36048, "start": 375.04, "end": 380.16, "text": " Now boat doesn't appear in ImageNet, but there's lots of kinds of boats that appear in ImageNet.", "tokens": [823, 6582, 1177, 380, 4204, 294, 29903, 31890, 11, 457, 456, 311, 3195, 295, 3685, 295, 17772, 300, 4204, 294, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.18525571904630742, "compression_ratio": 1.6692015209125475, "no_speech_prob": 1.1300723599561024e-05}, {"id": 84, "seek": 36048, "start": 380.16, "end": 384.88, "text": " So not surprisingly, it figures out generally speaking how to find boats.", "tokens": [407, 406, 17600, 11, 309, 9624, 484, 5101, 4124, 577, 281, 915, 17772, 13], "temperature": 0.0, "avg_logprob": -0.18525571904630742, "compression_ratio": 1.6692015209125475, "no_speech_prob": 1.1300723599561024e-05}, {"id": 85, "seek": 36048, "start": 384.88, "end": 386.52000000000004, "text": " I expected that.", "tokens": [286, 5176, 300, 13], "temperature": 0.0, "avg_logprob": -0.18525571904630742, "compression_ratio": 1.6692015209125475, "no_speech_prob": 1.1300723599561024e-05}, {"id": 86, "seek": 38652, "start": 386.52, "end": 391.28, "text": " And then I tried boat plus engine, and I got back pictures of power boats.", "tokens": [400, 550, 286, 3031, 6582, 1804, 2848, 11, 293, 286, 658, 646, 5242, 295, 1347, 17772, 13], "temperature": 0.0, "avg_logprob": -0.18541495654047752, "compression_ratio": 1.8433179723502304, "no_speech_prob": 3.668845465654158e-06}, {"id": 87, "seek": 38652, "start": 391.28, "end": 396.68, "text": " And then I tried boat plus paddle, and I got back pictures of rowing boats.", "tokens": [400, 550, 286, 3031, 6582, 1804, 31834, 11, 293, 286, 658, 646, 5242, 295, 5386, 278, 17772, 13], "temperature": 0.0, "avg_logprob": -0.18541495654047752, "compression_ratio": 1.8433179723502304, "no_speech_prob": 3.668845465654158e-06}, {"id": 88, "seek": 38652, "start": 396.68, "end": 400.2, "text": " So there's a whole lot going on here, and I think there's lots of opportunities for", "tokens": [407, 456, 311, 257, 1379, 688, 516, 322, 510, 11, 293, 286, 519, 456, 311, 3195, 295, 4786, 337], "temperature": 0.0, "avg_logprob": -0.18541495654047752, "compression_ratio": 1.8433179723502304, "no_speech_prob": 3.668845465654158e-06}, {"id": 89, "seek": 38652, "start": 400.2, "end": 406.03999999999996, "text": " you to explore and experiment based on the explorations and experiments that I've done.", "tokens": [291, 281, 6839, 293, 5120, 2361, 322, 264, 24765, 763, 293, 12050, 300, 286, 600, 1096, 13], "temperature": 0.0, "avg_logprob": -0.18541495654047752, "compression_ratio": 1.8433179723502304, "no_speech_prob": 3.668845465654158e-06}, {"id": 90, "seek": 38652, "start": 406.03999999999996, "end": 412.91999999999996, "text": " And more to the point, perhaps to create some interesting and valuable tools.", "tokens": [400, 544, 281, 264, 935, 11, 4317, 281, 1884, 512, 1880, 293, 8263, 3873, 13], "temperature": 0.0, "avg_logprob": -0.18541495654047752, "compression_ratio": 1.8433179723502304, "no_speech_prob": 3.668845465654158e-06}, {"id": 91, "seek": 41292, "start": 412.92, "end": 418.08000000000004, "text": " Like I would have thought a tool to do an image search to say, show me all the images", "tokens": [1743, 286, 576, 362, 1194, 257, 2290, 281, 360, 364, 3256, 3164, 281, 584, 11, 855, 385, 439, 264, 5267], "temperature": 0.0, "avg_logprob": -0.18359853783432317, "compression_ratio": 1.6626016260162602, "no_speech_prob": 5.828998109791428e-05}, {"id": 92, "seek": 41292, "start": 418.08000000000004, "end": 420.76, "text": " that contain these kinds of objects.", "tokens": [300, 5304, 613, 3685, 295, 6565, 13], "temperature": 0.0, "avg_logprob": -0.18359853783432317, "compression_ratio": 1.6626016260162602, "no_speech_prob": 5.828998109791428e-05}, {"id": 93, "seek": 41292, "start": 420.76, "end": 425.52000000000004, "text": " Or better still, maybe you could start training with things that aren't just nouns but also", "tokens": [1610, 1101, 920, 11, 1310, 291, 727, 722, 3097, 365, 721, 300, 3212, 380, 445, 48184, 457, 611], "temperature": 0.0, "avg_logprob": -0.18359853783432317, "compression_ratio": 1.6626016260162602, "no_speech_prob": 5.828998109791428e-05}, {"id": 94, "seek": 41292, "start": 425.52000000000004, "end": 426.52000000000004, "text": " adjectives.", "tokens": [29378, 1539, 13], "temperature": 0.0, "avg_logprob": -0.18359853783432317, "compression_ratio": 1.6626016260162602, "no_speech_prob": 5.828998109791428e-05}, {"id": 95, "seek": 41292, "start": 426.52000000000004, "end": 438.44, "text": " So you could start to search for pictures of crying babies or flaming houses or whatever.", "tokens": [407, 291, 727, 722, 281, 3164, 337, 5242, 295, 8554, 10917, 420, 45718, 8078, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.18359853783432317, "compression_ratio": 1.6626016260162602, "no_speech_prob": 5.828998109791428e-05}, {"id": 96, "seek": 41292, "start": 438.44, "end": 440.92, "text": " I think there's all kinds of stuff you could do with this, which would be really interesting", "tokens": [286, 519, 456, 311, 439, 3685, 295, 1507, 291, 727, 360, 365, 341, 11, 597, 576, 312, 534, 1880], "temperature": 0.0, "avg_logprob": -0.18359853783432317, "compression_ratio": 1.6626016260162602, "no_speech_prob": 5.828998109791428e-05}, {"id": 97, "seek": 44092, "start": 440.92, "end": 448.32, "text": " whether it be in a narrow organizational setting or to create some new startup or a new open", "tokens": [1968, 309, 312, 294, 257, 9432, 24730, 3287, 420, 281, 1884, 512, 777, 18578, 420, 257, 777, 1269], "temperature": 0.0, "avg_logprob": -0.23631912680233227, "compression_ratio": 1.5545454545454545, "no_speech_prob": 2.6273988623870537e-05}, {"id": 98, "seek": 44092, "start": 448.32, "end": 450.84000000000003, "text": " source project or whatever.", "tokens": [4009, 1716, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.23631912680233227, "compression_ratio": 1.5545454545454545, "no_speech_prob": 2.6273988623870537e-05}, {"id": 99, "seek": 44092, "start": 450.84000000000003, "end": 455.68, "text": " So anyway, lots of things to try.", "tokens": [407, 4033, 11, 3195, 295, 721, 281, 853, 13], "temperature": 0.0, "avg_logprob": -0.23631912680233227, "compression_ratio": 1.5545454545454545, "no_speech_prob": 2.6273988623870537e-05}, {"id": 100, "seek": 44092, "start": 455.68, "end": 457.36, "text": " More stuff this week.", "tokens": [5048, 1507, 341, 1243, 13], "temperature": 0.0, "avg_logprob": -0.23631912680233227, "compression_ratio": 1.5545454545454545, "no_speech_prob": 2.6273988623870537e-05}, {"id": 101, "seek": 44092, "start": 457.36, "end": 463.04, "text": " I actually missed this, this wasn't this week, but I was thrilled to see that one of our", "tokens": [286, 767, 6721, 341, 11, 341, 2067, 380, 341, 1243, 11, 457, 286, 390, 18744, 281, 536, 300, 472, 295, 527], "temperature": 0.0, "avg_logprob": -0.23631912680233227, "compression_ratio": 1.5545454545454545, "no_speech_prob": 2.6273988623870537e-05}, {"id": 102, "seek": 44092, "start": 463.04, "end": 467.72, "text": " students has written this fantastic medium post, linear algebra cheat sheet.", "tokens": [1731, 575, 3720, 341, 5456, 6399, 2183, 11, 8213, 21989, 17470, 8193, 13], "temperature": 0.0, "avg_logprob": -0.23631912680233227, "compression_ratio": 1.5545454545454545, "no_speech_prob": 2.6273988623870537e-05}, {"id": 103, "seek": 46772, "start": 467.72, "end": 473.52000000000004, "text": " I think I missed it because it was posted not to the part 2 forum, but maybe to the", "tokens": [286, 519, 286, 6721, 309, 570, 309, 390, 9437, 406, 281, 264, 644, 568, 17542, 11, 457, 1310, 281, 264], "temperature": 0.0, "avg_logprob": -0.23817698790295289, "compression_ratio": 1.5653846153846154, "no_speech_prob": 1.892473483167123e-05}, {"id": 104, "seek": 46772, "start": 473.52000000000004, "end": 474.52000000000004, "text": " main forum.", "tokens": [2135, 17542, 13], "temperature": 0.0, "avg_logprob": -0.23817698790295289, "compression_ratio": 1.5653846153846154, "no_speech_prob": 1.892473483167123e-05}, {"id": 105, "seek": 46772, "start": 474.52000000000004, "end": 477.76000000000005, "text": " This is really cool.", "tokens": [639, 307, 534, 1627, 13], "temperature": 0.0, "avg_logprob": -0.23817698790295289, "compression_ratio": 1.5653846153846154, "no_speech_prob": 1.892473483167123e-05}, {"id": 106, "seek": 46772, "start": 477.76000000000005, "end": 481.88000000000005, "text": " Brendan has gone through and really explained all the stuff that I would have wanted to", "tokens": [48484, 575, 2780, 807, 293, 534, 8825, 439, 264, 1507, 300, 286, 576, 362, 1415, 281], "temperature": 0.0, "avg_logprob": -0.23817698790295289, "compression_ratio": 1.5653846153846154, "no_speech_prob": 1.892473483167123e-05}, {"id": 107, "seek": 46772, "start": 481.88000000000005, "end": 485.64000000000004, "text": " have known about linear algebra before I got started.", "tokens": [362, 2570, 466, 8213, 21989, 949, 286, 658, 1409, 13], "temperature": 0.0, "avg_logprob": -0.23817698790295289, "compression_ratio": 1.5653846153846154, "no_speech_prob": 1.892473483167123e-05}, {"id": 108, "seek": 46772, "start": 485.64000000000004, "end": 490.68, "text": " And particularly I really appreciate that he's taking a code-first approach, so how", "tokens": [400, 4098, 286, 534, 4449, 300, 415, 311, 1940, 257, 3089, 12, 29581, 3109, 11, 370, 577], "temperature": 0.0, "avg_logprob": -0.23817698790295289, "compression_ratio": 1.5653846153846154, "no_speech_prob": 1.892473483167123e-05}, {"id": 109, "seek": 46772, "start": 490.68, "end": 494.88000000000005, "text": " do you actually do this in NumPy and talking about broadcasting.", "tokens": [360, 291, 767, 360, 341, 294, 22592, 47, 88, 293, 1417, 466, 30024, 13], "temperature": 0.0, "avg_logprob": -0.23817698790295289, "compression_ratio": 1.5653846153846154, "no_speech_prob": 1.892473483167123e-05}, {"id": 110, "seek": 49488, "start": 494.88, "end": 498.96, "text": " So you guys will all be very familiar with this already, but for your friends who are", "tokens": [407, 291, 1074, 486, 439, 312, 588, 4963, 365, 341, 1217, 11, 457, 337, 428, 1855, 567, 366], "temperature": 0.0, "avg_logprob": -0.2056896245038068, "compression_ratio": 1.5824175824175823, "no_speech_prob": 1.1478603482828476e-05}, {"id": 111, "seek": 49488, "start": 498.96, "end": 504.71999999999997, "text": " wondering how to get started in deep learning, what's the minimal things you need to know,", "tokens": [6359, 577, 281, 483, 1409, 294, 2452, 2539, 11, 437, 311, 264, 13206, 721, 291, 643, 281, 458, 11], "temperature": 0.0, "avg_logprob": -0.2056896245038068, "compression_ratio": 1.5824175824175823, "no_speech_prob": 1.1478603482828476e-05}, {"id": 112, "seek": 49488, "start": 504.71999999999997, "end": 507.2, "text": " it's probably the chain rule and some linear algebra.", "tokens": [309, 311, 1391, 264, 5021, 4978, 293, 512, 8213, 21989, 13], "temperature": 0.0, "avg_logprob": -0.2056896245038068, "compression_ratio": 1.5824175824175823, "no_speech_prob": 1.1478603482828476e-05}, {"id": 113, "seek": 49488, "start": 507.2, "end": 511.34, "text": " I think this covers a lot of linear algebra pretty effectively.", "tokens": [286, 519, 341, 10538, 257, 688, 295, 8213, 21989, 1238, 8659, 13], "temperature": 0.0, "avg_logprob": -0.2056896245038068, "compression_ratio": 1.5824175824175823, "no_speech_prob": 1.1478603482828476e-05}, {"id": 114, "seek": 49488, "start": 511.34, "end": 515.96, "text": " So thank you, Brendan.", "tokens": [407, 1309, 291, 11, 48484, 13], "temperature": 0.0, "avg_logprob": -0.2056896245038068, "compression_ratio": 1.5824175824175823, "no_speech_prob": 1.1478603482828476e-05}, {"id": 115, "seek": 49488, "start": 515.96, "end": 519.24, "text": " Other things from last week.", "tokens": [5358, 721, 490, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.2056896245038068, "compression_ratio": 1.5824175824175823, "no_speech_prob": 1.1478603482828476e-05}, {"id": 116, "seek": 49488, "start": 519.24, "end": 524.36, "text": " Andrea Frome, who wrote that Debye's paper, I actually emailed her and asked her what", "tokens": [24215, 1526, 423, 11, 567, 4114, 300, 1346, 6650, 311, 3035, 11, 286, 767, 45460, 720, 293, 2351, 720, 437], "temperature": 0.0, "avg_logprob": -0.2056896245038068, "compression_ratio": 1.5824175824175823, "no_speech_prob": 1.1478603482828476e-05}, {"id": 117, "seek": 52436, "start": 524.36, "end": 527.04, "text": " she thought I should look at.", "tokens": [750, 1194, 286, 820, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.19530747950762167, "compression_ratio": 1.4292237442922375, "no_speech_prob": 3.4267472074134275e-05}, {"id": 118, "seek": 52436, "start": 527.04, "end": 532.72, "text": " And she suggested this paper, Zero-Shot Learning by a Convex Combination of Semantic Embeddings,", "tokens": [400, 750, 10945, 341, 3035, 11, 17182, 12, 7774, 310, 15205, 538, 257, 2656, 303, 87, 25939, 2486, 295, 14421, 7128, 24234, 292, 29432, 11], "temperature": 0.0, "avg_logprob": -0.19530747950762167, "compression_ratio": 1.4292237442922375, "no_speech_prob": 3.4267472074134275e-05}, {"id": 119, "seek": 52436, "start": 532.72, "end": 541.92, "text": " which is only a later author on, but she says it's in some ways a more powerful version", "tokens": [597, 307, 787, 257, 1780, 3793, 322, 11, 457, 750, 1619, 309, 311, 294, 512, 2098, 257, 544, 4005, 3037], "temperature": 0.0, "avg_logprob": -0.19530747950762167, "compression_ratio": 1.4292237442922375, "no_speech_prob": 3.4267472074134275e-05}, {"id": 120, "seek": 52436, "start": 541.92, "end": 542.92, "text": " of Debye's.", "tokens": [295, 1346, 6650, 311, 13], "temperature": 0.0, "avg_logprob": -0.19530747950762167, "compression_ratio": 1.4292237442922375, "no_speech_prob": 3.4267472074134275e-05}, {"id": 121, "seek": 52436, "start": 542.92, "end": 548.7, "text": " It's actually quite different, and I haven't implemented it myself, but it solves some", "tokens": [467, 311, 767, 1596, 819, 11, 293, 286, 2378, 380, 12270, 309, 2059, 11, 457, 309, 39890, 512], "temperature": 0.0, "avg_logprob": -0.19530747950762167, "compression_ratio": 1.4292237442922375, "no_speech_prob": 3.4267472074134275e-05}, {"id": 122, "seek": 54870, "start": 548.7, "end": 554.72, "text": " similar problems, and anybody who's interested in exploring this multimodal images and text", "tokens": [2531, 2740, 11, 293, 4472, 567, 311, 3102, 294, 12736, 341, 32972, 378, 304, 5267, 293, 2487], "temperature": 0.0, "avg_logprob": -0.1988280487060547, "compression_ratio": 1.6, "no_speech_prob": 1.568886000313796e-05}, {"id": 123, "seek": 54870, "start": 554.72, "end": 556.2, "text": " space might be interested in this.", "tokens": [1901, 1062, 312, 3102, 294, 341, 13], "temperature": 0.0, "avg_logprob": -0.1988280487060547, "compression_ratio": 1.6, "no_speech_prob": 1.568886000313796e-05}, {"id": 124, "seek": 54870, "start": 556.2, "end": 559.96, "text": " We'll put this on the lesson wiki of course.", "tokens": [492, 603, 829, 341, 322, 264, 6898, 261, 9850, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.1988280487060547, "compression_ratio": 1.6, "no_speech_prob": 1.568886000313796e-05}, {"id": 125, "seek": 54870, "start": 559.96, "end": 567.88, "text": " And then one more involving the same author in a similar area a little bit later was looking", "tokens": [400, 550, 472, 544, 17030, 264, 912, 3793, 294, 257, 2531, 1859, 257, 707, 857, 1780, 390, 1237], "temperature": 0.0, "avg_logprob": -0.1988280487060547, "compression_ratio": 1.6, "no_speech_prob": 1.568886000313796e-05}, {"id": 126, "seek": 54870, "start": 567.88, "end": 571.5600000000001, "text": " at attention for fine-grained categorization.", "tokens": [412, 3202, 337, 2489, 12, 20735, 2001, 19250, 2144, 13], "temperature": 0.0, "avg_logprob": -0.1988280487060547, "compression_ratio": 1.6, "no_speech_prob": 1.568886000313796e-05}, {"id": 127, "seek": 54870, "start": 571.5600000000001, "end": 577.2, "text": " So a lot of these things, at least the way I think Andrea Frome was casting it, was about", "tokens": [407, 257, 688, 295, 613, 721, 11, 412, 1935, 264, 636, 286, 519, 24215, 1526, 423, 390, 17301, 309, 11, 390, 466], "temperature": 0.0, "avg_logprob": -0.1988280487060547, "compression_ratio": 1.6, "no_speech_prob": 1.568886000313796e-05}, {"id": 128, "seek": 57720, "start": 577.2, "end": 583.88, "text": " fine-grained categorization, which is how do we build something that can find very specific", "tokens": [2489, 12, 20735, 2001, 19250, 2144, 11, 597, 307, 577, 360, 321, 1322, 746, 300, 393, 915, 588, 2685], "temperature": 0.0, "avg_logprob": -0.13850315303018648, "compression_ratio": 1.5520833333333333, "no_speech_prob": 1.3211418263381347e-05}, {"id": 129, "seek": 57720, "start": 583.88, "end": 586.36, "text": " kinds of birds or very specific kinds of dogs.", "tokens": [3685, 295, 9009, 420, 588, 2685, 3685, 295, 7197, 13], "temperature": 0.0, "avg_logprob": -0.13850315303018648, "compression_ratio": 1.5520833333333333, "no_speech_prob": 1.3211418263381347e-05}, {"id": 130, "seek": 57720, "start": 586.36, "end": 600.2800000000001, "text": " But I think these kinds of models have very, very wide applicability.", "tokens": [583, 286, 519, 613, 3685, 295, 5245, 362, 588, 11, 588, 4874, 2580, 2310, 13], "temperature": 0.0, "avg_logprob": -0.13850315303018648, "compression_ratio": 1.5520833333333333, "no_speech_prob": 1.3211418263381347e-05}, {"id": 131, "seek": 57720, "start": 600.2800000000001, "end": 606.6400000000001, "text": " So I mentioned we'd kind of wrap up some final topics around computer vision-y stuff this", "tokens": [407, 286, 2835, 321, 1116, 733, 295, 7019, 493, 512, 2572, 8378, 926, 3820, 5201, 12, 88, 1507, 341], "temperature": 0.0, "avg_logprob": -0.13850315303018648, "compression_ratio": 1.5520833333333333, "no_speech_prob": 1.3211418263381347e-05}, {"id": 132, "seek": 60664, "start": 606.64, "end": 611.6, "text": " week before we started looking at some more NLP-related stuff.", "tokens": [1243, 949, 321, 1409, 1237, 412, 512, 544, 426, 45196, 12, 12004, 1507, 13], "temperature": 0.0, "avg_logprob": -0.14180765833173478, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.8631148122949526e-05}, {"id": 133, "seek": 60664, "start": 611.6, "end": 615.48, "text": " One of the things I wanted to zip through was a paper which I think some of you might", "tokens": [1485, 295, 264, 721, 286, 1415, 281, 20730, 807, 390, 257, 3035, 597, 286, 519, 512, 295, 291, 1062], "temperature": 0.0, "avg_logprob": -0.14180765833173478, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.8631148122949526e-05}, {"id": 134, "seek": 60664, "start": 615.48, "end": 622.4399999999999, "text": " enjoy, Systematic Evaluation of CNN Advances on the ImageNet data set.", "tokens": [2103, 11, 8910, 2399, 462, 46504, 295, 24859, 13634, 2676, 322, 264, 29903, 31890, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.14180765833173478, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.8631148122949526e-05}, {"id": 135, "seek": 60664, "start": 622.4399999999999, "end": 625.96, "text": " And I've pulled out what I thought were some of the key insights, because some of these", "tokens": [400, 286, 600, 7373, 484, 437, 286, 1194, 645, 512, 295, 264, 2141, 14310, 11, 570, 512, 295, 613], "temperature": 0.0, "avg_logprob": -0.14180765833173478, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.8631148122949526e-05}, {"id": 136, "seek": 60664, "start": 625.96, "end": 629.4, "text": " are things we haven't really looked at before.", "tokens": [366, 721, 321, 2378, 380, 534, 2956, 412, 949, 13], "temperature": 0.0, "avg_logprob": -0.14180765833173478, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.8631148122949526e-05}, {"id": 137, "seek": 60664, "start": 629.4, "end": 636.12, "text": " One key insight, which is very much the kind of thing I appreciate, is that they compared", "tokens": [1485, 2141, 11269, 11, 597, 307, 588, 709, 264, 733, 295, 551, 286, 4449, 11, 307, 300, 436, 5347], "temperature": 0.0, "avg_logprob": -0.14180765833173478, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.8631148122949526e-05}, {"id": 138, "seek": 63612, "start": 636.12, "end": 643.52, "text": " the difference between the kind of original CafeNet-AlexNet versus GoogleNet versus VGGNet", "tokens": [264, 2649, 1296, 264, 733, 295, 3380, 35864, 31890, 12, 22993, 31890, 5717, 3329, 31890, 5717, 691, 27561, 31890], "temperature": 0.0, "avg_logprob": -0.20854237205103823, "compression_ratio": 1.6706827309236947, "no_speech_prob": 1.5689123756601475e-05}, {"id": 139, "seek": 63612, "start": 643.52, "end": 649.88, "text": " on two different sized images, training on the original 227 or 128.", "tokens": [322, 732, 819, 20004, 5267, 11, 3097, 322, 264, 3380, 5853, 22, 420, 29810, 13], "temperature": 0.0, "avg_logprob": -0.20854237205103823, "compression_ratio": 1.6706827309236947, "no_speech_prob": 1.5689123756601475e-05}, {"id": 140, "seek": 63612, "start": 649.88, "end": 655.76, "text": " And what this chart shows is that the relative difference between these different architectures", "tokens": [400, 437, 341, 6927, 3110, 307, 300, 264, 4972, 2649, 1296, 613, 819, 6331, 1303], "temperature": 0.0, "avg_logprob": -0.20854237205103823, "compression_ratio": 1.6706827309236947, "no_speech_prob": 1.5689123756601475e-05}, {"id": 141, "seek": 63612, "start": 655.76, "end": 660.36, "text": " is almost exactly the same regardless of what size image you're looking at.", "tokens": [307, 1920, 2293, 264, 912, 10060, 295, 437, 2744, 3256, 291, 434, 1237, 412, 13], "temperature": 0.0, "avg_logprob": -0.20854237205103823, "compression_ratio": 1.6706827309236947, "no_speech_prob": 1.5689123756601475e-05}, {"id": 142, "seek": 63612, "start": 660.36, "end": 664.52, "text": " And this really reminds me of like in part 1 when we looked at data augmentation, and", "tokens": [400, 341, 534, 12025, 385, 295, 411, 294, 644, 502, 562, 321, 2956, 412, 1412, 14501, 19631, 11, 293], "temperature": 0.0, "avg_logprob": -0.20854237205103823, "compression_ratio": 1.6706827309236947, "no_speech_prob": 1.5689123756601475e-05}, {"id": 143, "seek": 66452, "start": 664.52, "end": 669.28, "text": " we said, Hey, you can understand which types of data augmentation to use and how much on", "tokens": [321, 848, 11, 1911, 11, 291, 393, 1223, 597, 3467, 295, 1412, 14501, 19631, 281, 764, 293, 577, 709, 322], "temperature": 0.0, "avg_logprob": -0.17009308063878423, "compression_ratio": 1.753787878787879, "no_speech_prob": 4.289302523829974e-06}, {"id": 144, "seek": 66452, "start": 669.28, "end": 673.1999999999999, "text": " a small sample of the data rather than on the whole data set.", "tokens": [257, 1359, 6889, 295, 264, 1412, 2831, 813, 322, 264, 1379, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.17009308063878423, "compression_ratio": 1.753787878787879, "no_speech_prob": 4.289302523829974e-06}, {"id": 145, "seek": 66452, "start": 673.1999999999999, "end": 676.76, "text": " And what this paper is saying is something similar, which is you can look at different", "tokens": [400, 437, 341, 3035, 307, 1566, 307, 746, 2531, 11, 597, 307, 291, 393, 574, 412, 819], "temperature": 0.0, "avg_logprob": -0.17009308063878423, "compression_ratio": 1.753787878787879, "no_speech_prob": 4.289302523829974e-06}, {"id": 146, "seek": 66452, "start": 676.76, "end": 682.4, "text": " architectures on small sized images rather than on full sized images.", "tokens": [6331, 1303, 322, 1359, 20004, 5267, 2831, 813, 322, 1577, 20004, 5267, 13], "temperature": 0.0, "avg_logprob": -0.17009308063878423, "compression_ratio": 1.753787878787879, "no_speech_prob": 4.289302523829974e-06}, {"id": 147, "seek": 66452, "start": 682.4, "end": 686.92, "text": " And so they then used this insight to do all of their experiments from then on using a", "tokens": [400, 370, 436, 550, 1143, 341, 11269, 281, 360, 439, 295, 641, 12050, 490, 550, 322, 1228, 257], "temperature": 0.0, "avg_logprob": -0.17009308063878423, "compression_ratio": 1.753787878787879, "no_speech_prob": 4.289302523829974e-06}, {"id": 148, "seek": 66452, "start": 686.92, "end": 692.16, "text": " smaller 128x128 ImageNet model, which they said was 10 times faster.", "tokens": [4356, 29810, 87, 4762, 23, 29903, 31890, 2316, 11, 597, 436, 848, 390, 1266, 1413, 4663, 13], "temperature": 0.0, "avg_logprob": -0.17009308063878423, "compression_ratio": 1.753787878787879, "no_speech_prob": 4.289302523829974e-06}, {"id": 149, "seek": 69216, "start": 692.16, "end": 697.4399999999999, "text": " So I thought that was the kind of thing which not enough academic papers do, which is like", "tokens": [407, 286, 1194, 300, 390, 264, 733, 295, 551, 597, 406, 1547, 7778, 10577, 360, 11, 597, 307, 411], "temperature": 0.0, "avg_logprob": -0.21576934732416625, "compression_ratio": 1.554585152838428, "no_speech_prob": 4.356847966846544e-06}, {"id": 150, "seek": 69216, "start": 697.4399999999999, "end": 701.7199999999999, "text": " what are the hacky shortcuts we can get away with.", "tokens": [437, 366, 264, 10339, 88, 34620, 321, 393, 483, 1314, 365, 13], "temperature": 0.0, "avg_logprob": -0.21576934732416625, "compression_ratio": 1.554585152838428, "no_speech_prob": 4.356847966846544e-06}, {"id": 151, "seek": 69216, "start": 701.7199999999999, "end": 707.8, "text": " So they tried lots of different activation functions.", "tokens": [407, 436, 3031, 3195, 295, 819, 24433, 6828, 13], "temperature": 0.0, "avg_logprob": -0.21576934732416625, "compression_ratio": 1.554585152838428, "no_speech_prob": 4.356847966846544e-06}, {"id": 152, "seek": 69216, "start": 707.8, "end": 711.8, "text": " It does look like max pooling is way better.", "tokens": [467, 775, 574, 411, 11469, 7005, 278, 307, 636, 1101, 13], "temperature": 0.0, "avg_logprob": -0.21576934732416625, "compression_ratio": 1.554585152838428, "no_speech_prob": 4.356847966846544e-06}, {"id": 153, "seek": 69216, "start": 711.8, "end": 715.9599999999999, "text": " So this is the gain compared to ReLU.", "tokens": [407, 341, 307, 264, 6052, 5347, 281, 1300, 43, 52, 13], "temperature": 0.0, "avg_logprob": -0.21576934732416625, "compression_ratio": 1.554585152838428, "no_speech_prob": 4.356847966846544e-06}, {"id": 154, "seek": 69216, "start": 715.9599999999999, "end": 721.28, "text": " But this one actually has twice the complexity, so it doesn't quite say that.", "tokens": [583, 341, 472, 767, 575, 6091, 264, 14024, 11, 370, 309, 1177, 380, 1596, 584, 300, 13], "temperature": 0.0, "avg_logprob": -0.21576934732416625, "compression_ratio": 1.554585152838428, "no_speech_prob": 4.356847966846544e-06}, {"id": 155, "seek": 72128, "start": 721.28, "end": 727.48, "text": " What it really says is that something we haven't looked at, which is ReLU, which as you can", "tokens": [708, 309, 534, 1619, 307, 300, 746, 321, 2378, 380, 2956, 412, 11, 597, 307, 1300, 43, 52, 11, 597, 382, 291, 393], "temperature": 0.0, "avg_logprob": -0.23357826325951553, "compression_ratio": 1.4245810055865922, "no_speech_prob": 1.5936391719151288e-05}, {"id": 156, "seek": 72128, "start": 727.48, "end": 729.36, "text": " see is very simple.", "tokens": [536, 307, 588, 2199, 13], "temperature": 0.0, "avg_logprob": -0.23357826325951553, "compression_ratio": 1.4245810055865922, "no_speech_prob": 1.5936391719151288e-05}, {"id": 157, "seek": 72128, "start": 729.36, "end": 732.76, "text": " If x is greater than or equal to 0, it's y equals x.", "tokens": [759, 2031, 307, 5044, 813, 420, 2681, 281, 1958, 11, 309, 311, 288, 6915, 2031, 13], "temperature": 0.0, "avg_logprob": -0.23357826325951553, "compression_ratio": 1.4245810055865922, "no_speech_prob": 1.5936391719151288e-05}, {"id": 158, "seek": 72128, "start": 732.76, "end": 735.9399999999999, "text": " Otherwise it's e to the x minus 1.", "tokens": [10328, 309, 311, 308, 281, 264, 2031, 3175, 502, 13], "temperature": 0.0, "avg_logprob": -0.23357826325951553, "compression_ratio": 1.4245810055865922, "no_speech_prob": 1.5936391719151288e-05}, {"id": 159, "seek": 72128, "start": 735.9399999999999, "end": 745.0799999999999, "text": " So ReLU basically is just like ReLU except it's smooth.", "tokens": [407, 1300, 43, 52, 1936, 307, 445, 411, 1300, 43, 52, 3993, 309, 311, 5508, 13], "temperature": 0.0, "avg_logprob": -0.23357826325951553, "compression_ratio": 1.4245810055865922, "no_speech_prob": 1.5936391719151288e-05}, {"id": 160, "seek": 74508, "start": 745.08, "end": 761.6800000000001, "text": " Whereas ReLU looks like that, ElU looks exactly the same here, then here it goes like that.", "tokens": [13813, 1300, 43, 52, 1542, 411, 300, 11, 2699, 52, 1542, 2293, 264, 912, 510, 11, 550, 510, 309, 1709, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.19555704358597875, "compression_ratio": 1.5284090909090908, "no_speech_prob": 8.39796666696202e-06}, {"id": 161, "seek": 74508, "start": 761.6800000000001, "end": 765.44, "text": " So it's kind of a nice smooth version.", "tokens": [407, 309, 311, 733, 295, 257, 1481, 5508, 3037, 13], "temperature": 0.0, "avg_logprob": -0.19555704358597875, "compression_ratio": 1.5284090909090908, "no_speech_prob": 8.39796666696202e-06}, {"id": 162, "seek": 74508, "start": 765.44, "end": 768.4000000000001, "text": " So that's one thing you might want to try using.", "tokens": [407, 300, 311, 472, 551, 291, 1062, 528, 281, 853, 1228, 13], "temperature": 0.0, "avg_logprob": -0.19555704358597875, "compression_ratio": 1.5284090909090908, "no_speech_prob": 8.39796666696202e-06}, {"id": 163, "seek": 74508, "start": 768.4000000000001, "end": 773.36, "text": " Another thing they tried which was interesting was using ElU for the convolutional layers", "tokens": [3996, 551, 436, 3031, 597, 390, 1880, 390, 1228, 2699, 52, 337, 264, 45216, 304, 7914], "temperature": 0.0, "avg_logprob": -0.19555704358597875, "compression_ratio": 1.5284090909090908, "no_speech_prob": 8.39796666696202e-06}, {"id": 164, "seek": 77336, "start": 773.36, "end": 783.6, "text": " and maxout for the fully connected layers.", "tokens": [293, 11469, 346, 337, 264, 4498, 4582, 7914, 13], "temperature": 0.0, "avg_logprob": -0.27445096818227616, "compression_ratio": 1.5, "no_speech_prob": 4.2892975216091145e-06}, {"id": 165, "seek": 77336, "start": 783.6, "end": 789.4, "text": " I guess nowadays we don't use fully connected layers very much, so maybe that's not as interesting.", "tokens": [286, 2041, 13434, 321, 500, 380, 764, 4498, 4582, 7914, 588, 709, 11, 370, 1310, 300, 311, 406, 382, 1880, 13], "temperature": 0.0, "avg_logprob": -0.27445096818227616, "compression_ratio": 1.5, "no_speech_prob": 4.2892975216091145e-06}, {"id": 166, "seek": 77336, "start": 789.4, "end": 793.6, "text": " The main interesting thing here I think is the ElU activation function.", "tokens": [440, 2135, 1880, 551, 510, 286, 519, 307, 264, 2699, 52, 24433, 2445, 13], "temperature": 0.0, "avg_logprob": -0.27445096818227616, "compression_ratio": 1.5, "no_speech_prob": 4.2892975216091145e-06}, {"id": 167, "seek": 77336, "start": 793.6, "end": 797.88, "text": " 2 percentage points is quite a big difference.", "tokens": [568, 9668, 2793, 307, 1596, 257, 955, 2649, 13], "temperature": 0.0, "avg_logprob": -0.27445096818227616, "compression_ratio": 1.5, "no_speech_prob": 4.2892975216091145e-06}, {"id": 168, "seek": 79788, "start": 797.88, "end": 803.88, "text": " They looked at different learning rate annealing approaches.", "tokens": [814, 2956, 412, 819, 2539, 3314, 22256, 4270, 11587, 13], "temperature": 0.0, "avg_logprob": -0.20285736642232755, "compression_ratio": 1.7511737089201878, "no_speech_prob": 3.5559633033699356e-06}, {"id": 169, "seek": 79788, "start": 803.88, "end": 806.8, "text": " You can use Keras to automatically do learning rate annealing.", "tokens": [509, 393, 764, 591, 6985, 281, 6772, 360, 2539, 3314, 22256, 4270, 13], "temperature": 0.0, "avg_logprob": -0.20285736642232755, "compression_ratio": 1.7511737089201878, "no_speech_prob": 3.5559633033699356e-06}, {"id": 170, "seek": 79788, "start": 806.8, "end": 811.52, "text": " What they showed is that linear annealing seems to work best.", "tokens": [708, 436, 4712, 307, 300, 8213, 22256, 4270, 2544, 281, 589, 1151, 13], "temperature": 0.0, "avg_logprob": -0.20285736642232755, "compression_ratio": 1.7511737089201878, "no_speech_prob": 3.5559633033699356e-06}, {"id": 171, "seek": 79788, "start": 811.52, "end": 818.36, "text": " They tried something else which was what about different color transformations.", "tokens": [814, 3031, 746, 1646, 597, 390, 437, 466, 819, 2017, 34852, 13], "temperature": 0.0, "avg_logprob": -0.20285736642232755, "compression_ratio": 1.7511737089201878, "no_speech_prob": 3.5559633033699356e-06}, {"id": 172, "seek": 79788, "start": 818.36, "end": 822.52, "text": " They found that amongst the normal approaches to thinking about color, RGB actually seems", "tokens": [814, 1352, 300, 12918, 264, 2710, 11587, 281, 1953, 466, 2017, 11, 31231, 767, 2544], "temperature": 0.0, "avg_logprob": -0.20285736642232755, "compression_ratio": 1.7511737089201878, "no_speech_prob": 3.5559633033699356e-06}, {"id": 173, "seek": 79788, "start": 822.52, "end": 823.52, "text": " to work the best.", "tokens": [281, 589, 264, 1151, 13], "temperature": 0.0, "avg_logprob": -0.20285736642232755, "compression_ratio": 1.7511737089201878, "no_speech_prob": 3.5559633033699356e-06}, {"id": 174, "seek": 82352, "start": 823.52, "end": 829.76, "text": " But then they tried something I haven't seen before, which is they added two 1x1 convolutions", "tokens": [583, 550, 436, 3031, 746, 286, 2378, 380, 1612, 949, 11, 597, 307, 436, 3869, 732, 502, 87, 16, 3754, 15892], "temperature": 0.0, "avg_logprob": -0.1895140999241879, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.267786142823752e-06}, {"id": 175, "seek": 82352, "start": 829.76, "end": 831.76, "text": " at the very start of the network.", "tokens": [412, 264, 588, 722, 295, 264, 3209, 13], "temperature": 0.0, "avg_logprob": -0.1895140999241879, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.267786142823752e-06}, {"id": 176, "seek": 82352, "start": 831.76, "end": 839.76, "text": " So each of those 1x1 convolutions is basically doing some kind of linear combination of the", "tokens": [407, 1184, 295, 729, 502, 87, 16, 3754, 15892, 307, 1936, 884, 512, 733, 295, 8213, 6562, 295, 264], "temperature": 0.0, "avg_logprob": -0.1895140999241879, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.267786142823752e-06}, {"id": 177, "seek": 82352, "start": 839.76, "end": 844.88, "text": " channels with a non-linearity then in between.", "tokens": [9235, 365, 257, 2107, 12, 1889, 17409, 550, 294, 1296, 13], "temperature": 0.0, "avg_logprob": -0.1895140999241879, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.267786142823752e-06}, {"id": 178, "seek": 82352, "start": 844.88, "end": 849.76, "text": " And they found that that actually gave them quite a big improvement.", "tokens": [400, 436, 1352, 300, 300, 767, 2729, 552, 1596, 257, 955, 10444, 13], "temperature": 0.0, "avg_logprob": -0.1895140999241879, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.267786142823752e-06}, {"id": 179, "seek": 82352, "start": 849.76, "end": 852.56, "text": " And that should be pretty much zero cost.", "tokens": [400, 300, 820, 312, 1238, 709, 4018, 2063, 13], "temperature": 0.0, "avg_logprob": -0.1895140999241879, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.267786142823752e-06}, {"id": 180, "seek": 85256, "start": 852.56, "end": 858.8399999999999, "text": " So there's another thing which I haven't really seen written about elsewhere, but it's a good", "tokens": [407, 456, 311, 1071, 551, 597, 286, 2378, 380, 534, 1612, 3720, 466, 14517, 11, 457, 309, 311, 257, 665], "temperature": 0.0, "avg_logprob": -0.23215777603621335, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.4285385987022892e-05}, {"id": 181, "seek": 85256, "start": 858.8399999999999, "end": 859.8399999999999, "text": " trick.", "tokens": [4282, 13], "temperature": 0.0, "avg_logprob": -0.23215777603621335, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.4285385987022892e-05}, {"id": 182, "seek": 85256, "start": 859.8399999999999, "end": 862.3199999999999, "text": " They looked at the impact of batch norm.", "tokens": [814, 2956, 412, 264, 2712, 295, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.23215777603621335, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.4285385987022892e-05}, {"id": 183, "seek": 85256, "start": 862.3199999999999, "end": 868.56, "text": " So here is the impact of batch norm, positive or negative.", "tokens": [407, 510, 307, 264, 2712, 295, 15245, 2026, 11, 3353, 420, 3671, 13], "temperature": 0.0, "avg_logprob": -0.23215777603621335, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.4285385987022892e-05}, {"id": 184, "seek": 85256, "start": 868.56, "end": 873.28, "text": " Actually adding batch norm to Google net didn't help, it actually made it worse.", "tokens": [5135, 5127, 15245, 2026, 281, 3329, 2533, 994, 380, 854, 11, 309, 767, 1027, 309, 5324, 13], "temperature": 0.0, "avg_logprob": -0.23215777603621335, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.4285385987022892e-05}, {"id": 185, "seek": 85256, "start": 873.28, "end": 877.7199999999999, "text": " So it seems these really complex, carefully tuned architectures, you've got to be pretty", "tokens": [407, 309, 2544, 613, 534, 3997, 11, 7500, 10870, 6331, 1303, 11, 291, 600, 658, 281, 312, 1238], "temperature": 0.0, "avg_logprob": -0.23215777603621335, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.4285385987022892e-05}, {"id": 186, "seek": 85256, "start": 877.7199999999999, "end": 878.7199999999999, "text": " careful.", "tokens": [5026, 13], "temperature": 0.0, "avg_logprob": -0.23215777603621335, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.4285385987022892e-05}, {"id": 187, "seek": 87872, "start": 878.72, "end": 883.08, "text": " Whereas on a simple network it helps a lot.", "tokens": [13813, 322, 257, 2199, 3209, 309, 3665, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.2682412465413411, "compression_ratio": 1.5758928571428572, "no_speech_prob": 3.844890215987107e-06}, {"id": 188, "seek": 87872, "start": 883.08, "end": 888.6, "text": " And the amount it helps also depends on somewhat which activation function you use.", "tokens": [400, 264, 2372, 309, 3665, 611, 5946, 322, 8344, 597, 24433, 2445, 291, 764, 13], "temperature": 0.0, "avg_logprob": -0.2682412465413411, "compression_ratio": 1.5758928571428572, "no_speech_prob": 3.844890215987107e-06}, {"id": 189, "seek": 87872, "start": 888.6, "end": 892.6800000000001, "text": " So batch norm, I think we kind of know that now.", "tokens": [407, 15245, 2026, 11, 286, 519, 321, 733, 295, 458, 300, 586, 13], "temperature": 0.0, "avg_logprob": -0.2682412465413411, "compression_ratio": 1.5758928571428572, "no_speech_prob": 3.844890215987107e-06}, {"id": 190, "seek": 87872, "start": 892.6800000000001, "end": 897.32, "text": " Be careful when you use it, sometimes it's fantastically helpful, sometimes it's slightly", "tokens": [879, 5026, 562, 291, 764, 309, 11, 2171, 309, 311, 4115, 22808, 4961, 11, 2171, 309, 311, 4748], "temperature": 0.0, "avg_logprob": -0.2682412465413411, "compression_ratio": 1.5758928571428572, "no_speech_prob": 3.844890215987107e-06}, {"id": 191, "seek": 87872, "start": 897.32, "end": 898.32, "text": " unhelpful.", "tokens": [517, 37451, 906, 13], "temperature": 0.0, "avg_logprob": -0.2682412465413411, "compression_ratio": 1.5758928571428572, "no_speech_prob": 3.844890215987107e-06}, {"id": 192, "seek": 89832, "start": 898.32, "end": 909.72, "text": " Question, is there any advantage in using fully connected layers for cloud?", "tokens": [14464, 11, 307, 456, 604, 5002, 294, 1228, 4498, 4582, 7914, 337, 4588, 30], "temperature": 0.0, "avg_logprob": -0.22317933136562132, "compression_ratio": 1.4266666666666667, "no_speech_prob": 1.497079119872069e-05}, {"id": 193, "seek": 89832, "start": 909.72, "end": 915.32, "text": " I think there is, they're terribly out of fashion.", "tokens": [286, 519, 456, 307, 11, 436, 434, 22903, 484, 295, 6700, 13], "temperature": 0.0, "avg_logprob": -0.22317933136562132, "compression_ratio": 1.4266666666666667, "no_speech_prob": 1.497079119872069e-05}, {"id": 194, "seek": 89832, "start": 915.32, "end": 924.8800000000001, "text": " But I think for transfer learning, they still seem to be the best in terms of the fully", "tokens": [583, 286, 519, 337, 5003, 2539, 11, 436, 920, 1643, 281, 312, 264, 1151, 294, 2115, 295, 264, 4498], "temperature": 0.0, "avg_logprob": -0.22317933136562132, "compression_ratio": 1.4266666666666667, "no_speech_prob": 1.497079119872069e-05}, {"id": 195, "seek": 92488, "start": 924.88, "end": 931.18, "text": " connected layers, super fast train and you seem to get a lot of flexibility there.", "tokens": [4582, 7914, 11, 1687, 2370, 3847, 293, 291, 1643, 281, 483, 257, 688, 295, 12635, 456, 13], "temperature": 0.0, "avg_logprob": -0.17883529910793552, "compression_ratio": 1.5577889447236182, "no_speech_prob": 2.931068229372613e-05}, {"id": 196, "seek": 92488, "start": 931.18, "end": 938.08, "text": " So I don't think we know one way or another yet, but I do think that VGG still has a lot", "tokens": [407, 286, 500, 380, 519, 321, 458, 472, 636, 420, 1071, 1939, 11, 457, 286, 360, 519, 300, 691, 27561, 920, 575, 257, 688], "temperature": 0.0, "avg_logprob": -0.17883529910793552, "compression_ratio": 1.5577889447236182, "no_speech_prob": 2.931068229372613e-05}, {"id": 197, "seek": 92488, "start": 938.08, "end": 945.36, "text": " to give us in terms of the last carefully tuned thing with fully connected layers and", "tokens": [281, 976, 505, 294, 2115, 295, 264, 1036, 7500, 10870, 551, 365, 4498, 4582, 7914, 293], "temperature": 0.0, "avg_logprob": -0.17883529910793552, "compression_ratio": 1.5577889447236182, "no_speech_prob": 2.931068229372613e-05}, {"id": 198, "seek": 92488, "start": 945.36, "end": 947.72, "text": " that really seems to be great for transfer learning.", "tokens": [300, 534, 2544, 281, 312, 869, 337, 5003, 2539, 13], "temperature": 0.0, "avg_logprob": -0.17883529910793552, "compression_ratio": 1.5577889447236182, "no_speech_prob": 2.931068229372613e-05}, {"id": 199, "seek": 94772, "start": 947.72, "end": 967.84, "text": " Question, anytime you hear me say something slightly stupid, please feel free to jump", "tokens": [14464, 11, 13038, 291, 1568, 385, 584, 746, 4748, 6631, 11, 1767, 841, 1737, 281, 3012], "temperature": 0.0, "avg_logprob": -0.26683300733566284, "compression_ratio": 1.2314814814814814, "no_speech_prob": 1.8630869817570783e-05}, {"id": 200, "seek": 94772, "start": 967.84, "end": 975.24, "text": " in because otherwise it's on the video forever.", "tokens": [294, 570, 5911, 309, 311, 322, 264, 960, 5680, 13], "temperature": 0.0, "avg_logprob": -0.26683300733566284, "compression_ratio": 1.2314814814814814, "no_speech_prob": 1.8630869817570783e-05}, {"id": 201, "seek": 97524, "start": 975.24, "end": 982.12, "text": " So on the other hand, it does give you an improvement in accuracy if you remove the", "tokens": [407, 322, 264, 661, 1011, 11, 309, 775, 976, 291, 364, 10444, 294, 14170, 498, 291, 4159, 264], "temperature": 0.0, "avg_logprob": -0.10553292383121539, "compression_ratio": 1.6442307692307692, "no_speech_prob": 2.6687353965826333e-05}, {"id": 202, "seek": 97524, "start": 982.12, "end": 988.7, "text": " final max pooling layer, replace all the fully connected layers with convolutional layers", "tokens": [2572, 11469, 7005, 278, 4583, 11, 7406, 439, 264, 4498, 4582, 7914, 365, 45216, 304, 7914], "temperature": 0.0, "avg_logprob": -0.10553292383121539, "compression_ratio": 1.6442307692307692, "no_speech_prob": 2.6687353965826333e-05}, {"id": 203, "seek": 97524, "start": 988.7, "end": 993.16, "text": " and stick an average pooling at the end, which is basically what this is doing.", "tokens": [293, 2897, 364, 4274, 7005, 278, 412, 264, 917, 11, 597, 307, 1936, 437, 341, 307, 884, 13], "temperature": 0.0, "avg_logprob": -0.10553292383121539, "compression_ratio": 1.6442307692307692, "no_speech_prob": 2.6687353965826333e-05}, {"id": 204, "seek": 97524, "start": 993.16, "end": 998.44, "text": " So it does seem there's definitely an upside to fully convolutional networks in terms of", "tokens": [407, 309, 775, 1643, 456, 311, 2138, 364, 14119, 281, 4498, 45216, 304, 9590, 294, 2115, 295], "temperature": 0.0, "avg_logprob": -0.10553292383121539, "compression_ratio": 1.6442307692307692, "no_speech_prob": 2.6687353965826333e-05}, {"id": 205, "seek": 99844, "start": 998.44, "end": 1007.08, "text": " accuracy, but there may be a downside in terms of flexibility around transfer learning.", "tokens": [14170, 11, 457, 456, 815, 312, 257, 25060, 294, 2115, 295, 12635, 926, 5003, 2539, 13], "temperature": 0.0, "avg_logprob": -0.2384663200378418, "compression_ratio": 1.58, "no_speech_prob": 4.49515300715575e-06}, {"id": 206, "seek": 99844, "start": 1007.08, "end": 1009.7600000000001, "text": " That's let-long clear still.", "tokens": [663, 311, 718, 12, 13025, 1850, 920, 13], "temperature": 0.0, "avg_logprob": -0.2384663200378418, "compression_ratio": 1.58, "no_speech_prob": 4.49515300715575e-06}, {"id": 207, "seek": 99844, "start": 1009.7600000000001, "end": 1012.1600000000001, "text": " I thought this was an interesting picture I hadn't quite seen before.", "tokens": [286, 1194, 341, 390, 364, 1880, 3036, 286, 8782, 380, 1596, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.2384663200378418, "compression_ratio": 1.58, "no_speech_prob": 4.49515300715575e-06}, {"id": 208, "seek": 99844, "start": 1012.1600000000001, "end": 1013.7, "text": " Let me explain the picture.", "tokens": [961, 385, 2903, 264, 3036, 13], "temperature": 0.0, "avg_logprob": -0.2384663200378418, "compression_ratio": 1.58, "no_speech_prob": 4.49515300715575e-06}, {"id": 209, "seek": 99844, "start": 1013.7, "end": 1020.48, "text": " What this shows is these are different batch sizes along the bottom, and then we've got", "tokens": [708, 341, 3110, 307, 613, 366, 819, 15245, 11602, 2051, 264, 2767, 11, 293, 550, 321, 600, 658], "temperature": 0.0, "avg_logprob": -0.2384663200378418, "compression_ratio": 1.58, "no_speech_prob": 4.49515300715575e-06}, {"id": 210, "seek": 99844, "start": 1020.48, "end": 1021.48, "text": " accuracy.", "tokens": [14170, 13], "temperature": 0.0, "avg_logprob": -0.2384663200378418, "compression_ratio": 1.58, "no_speech_prob": 4.49515300715575e-06}, {"id": 211, "seek": 99844, "start": 1021.48, "end": 1027.56, "text": " What it's showing is with a learning rate of.01, this is what happens to accuracy.", "tokens": [708, 309, 311, 4099, 307, 365, 257, 2539, 3314, 295, 2411, 10607, 11, 341, 307, 437, 2314, 281, 14170, 13], "temperature": 0.0, "avg_logprob": -0.2384663200378418, "compression_ratio": 1.58, "no_speech_prob": 4.49515300715575e-06}, {"id": 212, "seek": 102756, "start": 1027.56, "end": 1033.12, "text": " So as you go above 256 batch size, it plummets.", "tokens": [407, 382, 291, 352, 3673, 38882, 15245, 2744, 11, 309, 25854, 76, 1385, 13], "temperature": 0.0, "avg_logprob": -0.11974391397440208, "compression_ratio": 1.6408163265306122, "no_speech_prob": 7.4112349466304295e-06}, {"id": 213, "seek": 102756, "start": 1033.12, "end": 1039.8799999999999, "text": " On the other hand, if you use a learning rate of.01 times batch size over 256, it's pretty", "tokens": [1282, 264, 661, 1011, 11, 498, 291, 764, 257, 2539, 3314, 295, 2411, 10607, 1413, 15245, 2744, 670, 38882, 11, 309, 311, 1238], "temperature": 0.0, "avg_logprob": -0.11974391397440208, "compression_ratio": 1.6408163265306122, "no_speech_prob": 7.4112349466304295e-06}, {"id": 214, "seek": 102756, "start": 1039.8799999999999, "end": 1040.8799999999999, "text": " flat.", "tokens": [4962, 13], "temperature": 0.0, "avg_logprob": -0.11974391397440208, "compression_ratio": 1.6408163265306122, "no_speech_prob": 7.4112349466304295e-06}, {"id": 215, "seek": 102756, "start": 1040.8799999999999, "end": 1044.6, "text": " So what this suggests to me is that any time you change the batch size, this basically", "tokens": [407, 437, 341, 13409, 281, 385, 307, 300, 604, 565, 291, 1319, 264, 15245, 2744, 11, 341, 1936], "temperature": 0.0, "avg_logprob": -0.11974391397440208, "compression_ratio": 1.6408163265306122, "no_speech_prob": 7.4112349466304295e-06}, {"id": 216, "seek": 102756, "start": 1044.6, "end": 1049.3999999999999, "text": " is telling you to change the learning rate by a proportional amount, which I think a", "tokens": [307, 3585, 291, 281, 1319, 264, 2539, 3314, 538, 257, 24969, 2372, 11, 597, 286, 519, 257], "temperature": 0.0, "avg_logprob": -0.11974391397440208, "compression_ratio": 1.6408163265306122, "no_speech_prob": 7.4112349466304295e-06}, {"id": 217, "seek": 102756, "start": 1049.3999999999999, "end": 1052.6399999999999, "text": " lot of us have realized through experiment, but I don't think I've seen it explicitly", "tokens": [688, 295, 505, 362, 5334, 807, 5120, 11, 457, 286, 500, 380, 519, 286, 600, 1612, 309, 20803], "temperature": 0.0, "avg_logprob": -0.11974391397440208, "compression_ratio": 1.6408163265306122, "no_speech_prob": 7.4112349466304295e-06}, {"id": 218, "seek": 105264, "start": 1052.64, "end": 1057.76, "text": " mentioned before.", "tokens": [2835, 949, 13], "temperature": 0.0, "avg_logprob": -0.14717763141520973, "compression_ratio": 1.5975103734439835, "no_speech_prob": 9.368598512082826e-06}, {"id": 219, "seek": 105264, "start": 1057.76, "end": 1064.0400000000002, "text": " I think this is very helpful to understand as well is that removing data has a kind of", "tokens": [286, 519, 341, 307, 588, 4961, 281, 1223, 382, 731, 307, 300, 12720, 1412, 575, 257, 733, 295], "temperature": 0.0, "avg_logprob": -0.14717763141520973, "compression_ratio": 1.5975103734439835, "no_speech_prob": 9.368598512082826e-06}, {"id": 220, "seek": 105264, "start": 1064.0400000000002, "end": 1066.24, "text": " non-linear effect on accuracy.", "tokens": [2107, 12, 28263, 1802, 322, 14170, 13], "temperature": 0.0, "avg_logprob": -0.14717763141520973, "compression_ratio": 1.5975103734439835, "no_speech_prob": 9.368598512082826e-06}, {"id": 221, "seek": 105264, "start": 1066.24, "end": 1069.7, "text": " So this green line here is what happens when you remove images.", "tokens": [407, 341, 3092, 1622, 510, 307, 437, 2314, 562, 291, 4159, 5267, 13], "temperature": 0.0, "avg_logprob": -0.14717763141520973, "compression_ratio": 1.5975103734439835, "no_speech_prob": 9.368598512082826e-06}, {"id": 222, "seek": 105264, "start": 1069.7, "end": 1076.3600000000001, "text": " So with ImageNet, down to about half the size of ImageNet, there isn't a huge impact on", "tokens": [407, 365, 29903, 31890, 11, 760, 281, 466, 1922, 264, 2744, 295, 29903, 31890, 11, 456, 1943, 380, 257, 2603, 2712, 322], "temperature": 0.0, "avg_logprob": -0.14717763141520973, "compression_ratio": 1.5975103734439835, "no_speech_prob": 9.368598512082826e-06}, {"id": 223, "seek": 105264, "start": 1076.3600000000001, "end": 1077.3600000000001, "text": " accuracy.", "tokens": [14170, 13], "temperature": 0.0, "avg_logprob": -0.14717763141520973, "compression_ratio": 1.5975103734439835, "no_speech_prob": 9.368598512082826e-06}, {"id": 224, "seek": 105264, "start": 1077.3600000000001, "end": 1082.24, "text": " Or maybe if you want to really speed things up, you can go 128x128 sized images and use", "tokens": [1610, 1310, 498, 291, 528, 281, 534, 3073, 721, 493, 11, 291, 393, 352, 29810, 87, 4762, 23, 20004, 5267, 293, 764], "temperature": 0.0, "avg_logprob": -0.14717763141520973, "compression_ratio": 1.5975103734439835, "no_speech_prob": 9.368598512082826e-06}, {"id": 225, "seek": 108224, "start": 1082.24, "end": 1087.08, "text": " just 600,000 of them, or even maybe 400,000.", "tokens": [445, 11849, 11, 1360, 295, 552, 11, 420, 754, 1310, 8423, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.12437822421391805, "compression_ratio": 1.6194690265486726, "no_speech_prob": 1.6442099877167493e-05}, {"id": 226, "seek": 108224, "start": 1087.08, "end": 1090.08, "text": " But then beneath that, it starts to plummet.", "tokens": [583, 550, 17149, 300, 11, 309, 3719, 281, 25854, 5537, 13], "temperature": 0.0, "avg_logprob": -0.12437822421391805, "compression_ratio": 1.6194690265486726, "no_speech_prob": 1.6442099877167493e-05}, {"id": 227, "seek": 108224, "start": 1090.08, "end": 1094.16, "text": " So I think that's an interesting insight.", "tokens": [407, 286, 519, 300, 311, 364, 1880, 11269, 13], "temperature": 0.0, "avg_logprob": -0.12437822421391805, "compression_ratio": 1.6194690265486726, "no_speech_prob": 1.6442099877167493e-05}, {"id": 228, "seek": 108224, "start": 1094.16, "end": 1097.52, "text": " Another interesting insight, although I'm going to add something to this in a moment,", "tokens": [3996, 1880, 11269, 11, 4878, 286, 478, 516, 281, 909, 746, 281, 341, 294, 257, 1623, 11], "temperature": 0.0, "avg_logprob": -0.12437822421391805, "compression_ratio": 1.6194690265486726, "no_speech_prob": 1.6442099877167493e-05}, {"id": 229, "seek": 108224, "start": 1097.52, "end": 1105.08, "text": " is that rather than removing images, if you instead flip the labels to make them incorrect,", "tokens": [307, 300, 2831, 813, 12720, 5267, 11, 498, 291, 2602, 7929, 264, 16949, 281, 652, 552, 18424, 11], "temperature": 0.0, "avg_logprob": -0.12437822421391805, "compression_ratio": 1.6194690265486726, "no_speech_prob": 1.6442099877167493e-05}, {"id": 230, "seek": 108224, "start": 1105.08, "end": 1111.1200000000001, "text": " that has a worse effect than not having the data at all.", "tokens": [300, 575, 257, 5324, 1802, 813, 406, 1419, 264, 1412, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.12437822421391805, "compression_ratio": 1.6194690265486726, "no_speech_prob": 1.6442099877167493e-05}, {"id": 231, "seek": 111112, "start": 1111.12, "end": 1114.52, "text": " But there are things we can do to try and improve things there.", "tokens": [583, 456, 366, 721, 321, 393, 360, 281, 853, 293, 3470, 721, 456, 13], "temperature": 0.0, "avg_logprob": -0.13646602630615234, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.647702033049427e-05}, {"id": 232, "seek": 111112, "start": 1114.52, "end": 1121.6399999999999, "text": " And specifically I want to bring your attention to this paper, Training Deep Neural Networks", "tokens": [400, 4682, 286, 528, 281, 1565, 428, 3202, 281, 341, 3035, 11, 20620, 14895, 1734, 1807, 12640, 82], "temperature": 0.0, "avg_logprob": -0.13646602630615234, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.647702033049427e-05}, {"id": 233, "seek": 111112, "start": 1121.6399999999999, "end": 1125.04, "text": " on Noisy Labels with Bootstrapping.", "tokens": [322, 883, 14169, 10137, 1625, 365, 37263, 19639, 3759, 13], "temperature": 0.0, "avg_logprob": -0.13646602630615234, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.647702033049427e-05}, {"id": 234, "seek": 111112, "start": 1125.04, "end": 1130.6399999999999, "text": " And what they show is a very simple approach, a very simple tweak you can add to any training", "tokens": [400, 437, 436, 855, 307, 257, 588, 2199, 3109, 11, 257, 588, 2199, 29879, 291, 393, 909, 281, 604, 3097], "temperature": 0.0, "avg_logprob": -0.13646602630615234, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.647702033049427e-05}, {"id": 235, "seek": 111112, "start": 1130.6399999999999, "end": 1137.28, "text": " method, which dramatically improves their ability to handle noisy labels.", "tokens": [3170, 11, 597, 17548, 24771, 641, 3485, 281, 4813, 24518, 16949, 13], "temperature": 0.0, "avg_logprob": -0.13646602630615234, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.647702033049427e-05}, {"id": 236, "seek": 113728, "start": 1137.28, "end": 1145.28, "text": " So this here is showing, if you add noise, varying from.3 up to.5 to MNIST, up to half", "tokens": [407, 341, 510, 307, 4099, 11, 498, 291, 909, 5658, 11, 22984, 490, 2411, 18, 493, 281, 2411, 20, 281, 376, 45, 19756, 11, 493, 281, 1922], "temperature": 0.0, "avg_logprob": -0.214943110148112, "compression_ratio": 1.4519774011299436, "no_speech_prob": 5.255298674455844e-06}, {"id": 237, "seek": 113728, "start": 1145.28, "end": 1151.76, "text": " of it, the baselines are doing nothing at all.", "tokens": [295, 309, 11, 264, 987, 9173, 366, 884, 1825, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.214943110148112, "compression_ratio": 1.4519774011299436, "no_speech_prob": 5.255298674455844e-06}, {"id": 238, "seek": 113728, "start": 1151.76, "end": 1156.56, "text": " It really collapses the accuracy.", "tokens": [467, 534, 48765, 264, 14170, 13], "temperature": 0.0, "avg_logprob": -0.214943110148112, "compression_ratio": 1.4519774011299436, "no_speech_prob": 5.255298674455844e-06}, {"id": 239, "seek": 113728, "start": 1156.56, "end": 1162.84, "text": " But if you use their approach to bootstrapping, you can actually go up to nearly half the", "tokens": [583, 498, 291, 764, 641, 3109, 281, 11450, 19639, 3759, 11, 291, 393, 767, 352, 493, 281, 6217, 1922, 264], "temperature": 0.0, "avg_logprob": -0.214943110148112, "compression_ratio": 1.4519774011299436, "no_speech_prob": 5.255298674455844e-06}, {"id": 240, "seek": 116284, "start": 1162.84, "end": 1170.1599999999999, "text": " images, intentionally changing their label, and it still works nearly as well.", "tokens": [5267, 11, 22062, 4473, 641, 7645, 11, 293, 309, 920, 1985, 6217, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.14659282435541568, "compression_ratio": 1.6825396825396826, "no_speech_prob": 3.0717601475771517e-05}, {"id": 241, "seek": 116284, "start": 1170.1599999999999, "end": 1173.8799999999999, "text": " I think this is a really important paper to mention in this stuff that most of you will", "tokens": [286, 519, 341, 307, 257, 534, 1021, 3035, 281, 2152, 294, 341, 1507, 300, 881, 295, 291, 486], "temperature": 0.0, "avg_logprob": -0.14659282435541568, "compression_ratio": 1.6825396825396826, "no_speech_prob": 3.0717601475771517e-05}, {"id": 242, "seek": 116284, "start": 1173.8799999999999, "end": 1179.56, "text": " find important and useful area because most real-world datasets have noise in them.", "tokens": [915, 1021, 293, 4420, 1859, 570, 881, 957, 12, 13217, 42856, 362, 5658, 294, 552, 13], "temperature": 0.0, "avg_logprob": -0.14659282435541568, "compression_ratio": 1.6825396825396826, "no_speech_prob": 3.0717601475771517e-05}, {"id": 243, "seek": 116284, "start": 1179.56, "end": 1185.0, "text": " So maybe this is something you should consider adding to everything that you've trained,", "tokens": [407, 1310, 341, 307, 746, 291, 820, 1949, 5127, 281, 1203, 300, 291, 600, 8895, 11], "temperature": 0.0, "avg_logprob": -0.14659282435541568, "compression_ratio": 1.6825396825396826, "no_speech_prob": 3.0717601475771517e-05}, {"id": 244, "seek": 116284, "start": 1185.0, "end": 1191.24, "text": " whether it be Kaggle datasets or your own datasets or whatever, particularly because", "tokens": [1968, 309, 312, 48751, 22631, 42856, 420, 428, 1065, 42856, 420, 2035, 11, 4098, 570], "temperature": 0.0, "avg_logprob": -0.14659282435541568, "compression_ratio": 1.6825396825396826, "no_speech_prob": 3.0717601475771517e-05}, {"id": 245, "seek": 119124, "start": 1191.24, "end": 1202.0, "text": " you don't necessarily know how noisy the labels are.", "tokens": [291, 500, 380, 4725, 458, 577, 24518, 264, 16949, 366, 13], "temperature": 0.0, "avg_logprob": -0.4168493444269354, "compression_ratio": 1.5066666666666666, "no_speech_prob": 1.7231262972927652e-05}, {"id": 246, "seek": 119124, "start": 1202.0, "end": 1204.44, "text": " So noisy labels means incorrect.", "tokens": [407, 24518, 16949, 1355, 18424, 13], "temperature": 0.0, "avg_logprob": -0.4168493444269354, "compression_ratio": 1.5066666666666666, "no_speech_prob": 1.7231262972927652e-05}, {"id": 247, "seek": 119124, "start": 1204.44, "end": 1205.44, "text": " Noisy just means incorrect.", "tokens": [883, 14169, 445, 1355, 18424, 13], "temperature": 0.0, "avg_logprob": -0.4168493444269354, "compression_ratio": 1.5066666666666666, "no_speech_prob": 1.7231262972927652e-05}, {"id": 248, "seek": 119124, "start": 1205.44, "end": 1213.08, "text": " This particular paper describes a particular technique, which you can read during the week", "tokens": [639, 1729, 3035, 15626, 257, 1729, 6532, 11, 597, 291, 393, 1401, 1830, 264, 1243], "temperature": 0.0, "avg_logprob": -0.4168493444269354, "compression_ratio": 1.5066666666666666, "no_speech_prob": 1.7231262972927652e-05}, {"id": 249, "seek": 119124, "start": 1213.08, "end": 1218.8, "text": " if you're interested.", "tokens": [498, 291, 434, 3102, 13], "temperature": 0.0, "avg_logprob": -0.4168493444269354, "compression_ratio": 1.5066666666666666, "no_speech_prob": 1.7231262972927652e-05}, {"id": 250, "seek": 121880, "start": 1218.8, "end": 1224.68, "text": " Interestingly they find that if you take VGG and then add all of these things together", "tokens": [30564, 436, 915, 300, 498, 291, 747, 691, 27561, 293, 550, 909, 439, 295, 613, 721, 1214], "temperature": 0.0, "avg_logprob": -0.14667240189917294, "compression_ratio": 1.5450236966824644, "no_speech_prob": 9.818122634897009e-06}, {"id": 251, "seek": 121880, "start": 1224.68, "end": 1229.6399999999999, "text": " and do them all at once, you can actually get a pretty big performance hike.", "tokens": [293, 360, 552, 439, 412, 1564, 11, 291, 393, 767, 483, 257, 1238, 955, 3389, 23282, 13], "temperature": 0.0, "avg_logprob": -0.14667240189917294, "compression_ratio": 1.5450236966824644, "no_speech_prob": 9.818122634897009e-06}, {"id": 252, "seek": 121880, "start": 1229.6399999999999, "end": 1237.2, "text": " It looks in fact like VGG becomes more accurate than GoogleNet to make all these changes.", "tokens": [467, 1542, 294, 1186, 411, 691, 27561, 3643, 544, 8559, 813, 3329, 31890, 281, 652, 439, 613, 2962, 13], "temperature": 0.0, "avg_logprob": -0.14667240189917294, "compression_ratio": 1.5450236966824644, "no_speech_prob": 9.818122634897009e-06}, {"id": 253, "seek": 121880, "start": 1237.2, "end": 1246.8, "text": " So that's an interesting point, although VGG is very, very slow and big.", "tokens": [407, 300, 311, 364, 1880, 935, 11, 4878, 691, 27561, 307, 588, 11, 588, 2964, 293, 955, 13], "temperature": 0.0, "avg_logprob": -0.14667240189917294, "compression_ratio": 1.5450236966824644, "no_speech_prob": 9.818122634897009e-06}, {"id": 254, "seek": 124680, "start": 1246.8, "end": 1248.9199999999998, "text": " There's lots of stuff that I noticed they didn't look at.", "tokens": [821, 311, 3195, 295, 1507, 300, 286, 5694, 436, 994, 380, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.21153040285463687, "compression_ratio": 1.6412213740458015, "no_speech_prob": 1.1842959793284535e-05}, {"id": 255, "seek": 124680, "start": 1248.9199999999998, "end": 1252.96, "text": " They didn't look at data augmentation, different approaches to zooming and cropping, adding", "tokens": [814, 994, 380, 574, 412, 1412, 14501, 19631, 11, 819, 11587, 281, 48226, 293, 4848, 3759, 11, 5127], "temperature": 0.0, "avg_logprob": -0.21153040285463687, "compression_ratio": 1.6412213740458015, "no_speech_prob": 1.1842959793284535e-05}, {"id": 256, "seek": 124680, "start": 1252.96, "end": 1260.56, "text": " skip connections like in ResNet or DenseNet or Hire Networks, different initialization", "tokens": [10023, 9271, 411, 294, 5015, 31890, 420, 413, 1288, 31890, 420, 389, 621, 12640, 82, 11, 819, 5883, 2144], "temperature": 0.0, "avg_logprob": -0.21153040285463687, "compression_ratio": 1.6412213740458015, "no_speech_prob": 1.1842959793284535e-05}, {"id": 257, "seek": 124680, "start": 1260.56, "end": 1264.6399999999999, "text": " methods, different amounts of depth.", "tokens": [7150, 11, 819, 11663, 295, 7161, 13], "temperature": 0.0, "avg_logprob": -0.21153040285463687, "compression_ratio": 1.6412213740458015, "no_speech_prob": 1.1842959793284535e-05}, {"id": 258, "seek": 124680, "start": 1264.6399999999999, "end": 1268.6399999999999, "text": " And to me, the most important is the impact on transfer learning.", "tokens": [400, 281, 385, 11, 264, 881, 1021, 307, 264, 2712, 322, 5003, 2539, 13], "temperature": 0.0, "avg_logprob": -0.21153040285463687, "compression_ratio": 1.6412213740458015, "no_speech_prob": 1.1842959793284535e-05}, {"id": 259, "seek": 124680, "start": 1268.6399999999999, "end": 1274.6, "text": " So these to me are all open questions as far as I know, and so maybe one of you would like", "tokens": [407, 613, 281, 385, 366, 439, 1269, 1651, 382, 1400, 382, 286, 458, 11, 293, 370, 1310, 472, 295, 291, 576, 411], "temperature": 0.0, "avg_logprob": -0.21153040285463687, "compression_ratio": 1.6412213740458015, "no_speech_prob": 1.1842959793284535e-05}, {"id": 260, "seek": 127460, "start": 1274.6, "end": 1285.04, "text": " to create the successor to this, more observations on training CNNs.", "tokens": [281, 1884, 264, 31864, 281, 341, 11, 544, 18163, 322, 3097, 24859, 82, 13], "temperature": 0.0, "avg_logprob": -0.17629853316715785, "compression_ratio": 1.5061728395061729, "no_speech_prob": 6.540365120599745e-06}, {"id": 261, "seek": 127460, "start": 1285.04, "end": 1288.1599999999999, "text": " There's another interesting paper, although the main interesting thing about this paper", "tokens": [821, 311, 1071, 1880, 3035, 11, 4878, 264, 2135, 1880, 551, 466, 341, 3035], "temperature": 0.0, "avg_logprob": -0.17629853316715785, "compression_ratio": 1.5061728395061729, "no_speech_prob": 6.540365120599745e-06}, {"id": 262, "seek": 127460, "start": 1288.1599999999999, "end": 1294.12, "text": " is this particular picture, so feel free to check it out, it's pretty short and simple.", "tokens": [307, 341, 1729, 3036, 11, 370, 841, 1737, 281, 1520, 309, 484, 11, 309, 311, 1238, 2099, 293, 2199, 13], "temperature": 0.0, "avg_logprob": -0.17629853316715785, "compression_ratio": 1.5061728395061729, "no_speech_prob": 6.540365120599745e-06}, {"id": 263, "seek": 129412, "start": 1294.12, "end": 1305.76, "text": " This paper is looking at the accuracy versus the size and the speed of different networks.", "tokens": [639, 3035, 307, 1237, 412, 264, 14170, 5717, 264, 2744, 293, 264, 3073, 295, 819, 9590, 13], "temperature": 0.0, "avg_logprob": -0.14756763515187732, "compression_ratio": 1.5088757396449703, "no_speech_prob": 6.2408767007582355e-06}, {"id": 264, "seek": 129412, "start": 1305.76, "end": 1311.4399999999998, "text": " So the size of a bubble is how big is the network, how many parameters does it have.", "tokens": [407, 264, 2744, 295, 257, 12212, 307, 577, 955, 307, 264, 3209, 11, 577, 867, 9834, 775, 309, 362, 13], "temperature": 0.0, "avg_logprob": -0.14756763515187732, "compression_ratio": 1.5088757396449703, "no_speech_prob": 6.2408767007582355e-06}, {"id": 265, "seek": 129412, "start": 1311.4399999999998, "end": 1318.8799999999999, "text": " So you can see VGG16 and VGG19 are by far the biggest of any of these networks.", "tokens": [407, 291, 393, 536, 691, 27561, 6866, 293, 691, 27561, 3405, 366, 538, 1400, 264, 3880, 295, 604, 295, 613, 9590, 13], "temperature": 0.0, "avg_logprob": -0.14756763515187732, "compression_ratio": 1.5088757396449703, "no_speech_prob": 6.2408767007582355e-06}, {"id": 266, "seek": 131888, "start": 1318.88, "end": 1324.3600000000001, "text": " Interestingly the second biggest are the very old basic AlexNet.", "tokens": [30564, 264, 1150, 3880, 366, 264, 588, 1331, 3875, 5202, 31890, 13], "temperature": 0.0, "avg_logprob": -0.1608934531340728, "compression_ratio": 1.5, "no_speech_prob": 9.818204489420168e-06}, {"id": 267, "seek": 131888, "start": 1324.3600000000001, "end": 1328.3600000000001, "text": " Interestingly newer networks tend to have a lot less parameters, which is a good sign.", "tokens": [30564, 17628, 9590, 3928, 281, 362, 257, 688, 1570, 9834, 11, 597, 307, 257, 665, 1465, 13], "temperature": 0.0, "avg_logprob": -0.1608934531340728, "compression_ratio": 1.5, "no_speech_prob": 9.818204489420168e-06}, {"id": 268, "seek": 131888, "start": 1328.3600000000001, "end": 1334.5800000000002, "text": " Then on this axis we have basically how long does it take to train.", "tokens": [1396, 322, 341, 10298, 321, 362, 1936, 577, 938, 775, 309, 747, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.1608934531340728, "compression_ratio": 1.5, "no_speech_prob": 9.818204489420168e-06}, {"id": 269, "seek": 131888, "start": 1334.5800000000002, "end": 1344.94, "text": " So again VGG is big and slow, and without at least some tweaks, not terribly accurate.", "tokens": [407, 797, 691, 27561, 307, 955, 293, 2964, 11, 293, 1553, 412, 1935, 512, 46664, 11, 406, 22903, 8559, 13], "temperature": 0.0, "avg_logprob": -0.1608934531340728, "compression_ratio": 1.5, "no_speech_prob": 9.818204489420168e-06}, {"id": 270, "seek": 134494, "start": 1344.94, "end": 1350.28, "text": " So again there's definitely reasons not to use VGG even if it seems easier for transfer", "tokens": [407, 797, 456, 311, 2138, 4112, 406, 281, 764, 691, 27561, 754, 498, 309, 2544, 3571, 337, 5003], "temperature": 0.0, "avg_logprob": -0.17287574151549676, "compression_ratio": 1.6504065040650406, "no_speech_prob": 3.6119442938797874e-06}, {"id": 271, "seek": 134494, "start": 1350.28, "end": 1355.68, "text": " learning or we don't necessarily know how to do a great job of transfer learning on", "tokens": [2539, 420, 321, 500, 380, 4725, 458, 577, 281, 360, 257, 869, 1691, 295, 5003, 2539, 322], "temperature": 0.0, "avg_logprob": -0.17287574151549676, "compression_ratio": 1.6504065040650406, "no_speech_prob": 3.6119442938797874e-06}, {"id": 272, "seek": 134494, "start": 1355.68, "end": 1357.76, "text": " ResNet or Inception.", "tokens": [5015, 31890, 420, 682, 7311, 13], "temperature": 0.0, "avg_logprob": -0.17287574151549676, "compression_ratio": 1.6504065040650406, "no_speech_prob": 3.6119442938797874e-06}, {"id": 273, "seek": 134494, "start": 1357.76, "end": 1364.28, "text": " But as you can see, the more recent ResNet and Inception-based approaches are significantly", "tokens": [583, 382, 291, 393, 536, 11, 264, 544, 5162, 5015, 31890, 293, 682, 7311, 12, 6032, 11587, 366, 10591], "temperature": 0.0, "avg_logprob": -0.17287574151549676, "compression_ratio": 1.6504065040650406, "no_speech_prob": 3.6119442938797874e-06}, {"id": 274, "seek": 134494, "start": 1364.28, "end": 1367.8, "text": " more accurate and faster and smaller.", "tokens": [544, 8559, 293, 4663, 293, 4356, 13], "temperature": 0.0, "avg_logprob": -0.17287574151549676, "compression_ratio": 1.6504065040650406, "no_speech_prob": 3.6119442938797874e-06}, {"id": 275, "seek": 134494, "start": 1367.8, "end": 1373.8, "text": " So this is I think why I was looking last week at trying to do transfer learning on", "tokens": [407, 341, 307, 286, 519, 983, 286, 390, 1237, 1036, 1243, 412, 1382, 281, 360, 5003, 2539, 322], "temperature": 0.0, "avg_logprob": -0.17287574151549676, "compression_ratio": 1.6504065040650406, "no_speech_prob": 3.6119442938797874e-06}, {"id": 276, "seek": 137380, "start": 1373.8, "end": 1375.24, "text": " top of ResNet.", "tokens": [1192, 295, 5015, 31890, 13], "temperature": 0.0, "avg_logprob": -0.24584728334008193, "compression_ratio": 1.5576923076923077, "no_speech_prob": 2.4299408323713578e-05}, {"id": 277, "seek": 137380, "start": 1375.24, "end": 1379.72, "text": " There's really good reasons to want to do that.", "tokens": [821, 311, 534, 665, 4112, 281, 528, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.24584728334008193, "compression_ratio": 1.5576923076923077, "no_speech_prob": 2.4299408323713578e-05}, {"id": 278, "seek": 137380, "start": 1379.72, "end": 1383.44, "text": " I think this is a great picture.", "tokens": [286, 519, 341, 307, 257, 869, 3036, 13], "temperature": 0.0, "avg_logprob": -0.24584728334008193, "compression_ratio": 1.5576923076923077, "no_speech_prob": 2.4299408323713578e-05}, {"id": 279, "seek": 137380, "start": 1383.44, "end": 1389.48, "text": " These two papers really show us that academic papers are not always just here's some highly", "tokens": [1981, 732, 10577, 534, 855, 505, 300, 7778, 10577, 366, 406, 1009, 445, 510, 311, 512, 5405], "temperature": 0.0, "avg_logprob": -0.24584728334008193, "compression_ratio": 1.5576923076923077, "no_speech_prob": 2.4299408323713578e-05}, {"id": 280, "seek": 137380, "start": 1389.48, "end": 1392.3999999999999, "text": " theoretical wacky result.", "tokens": [20864, 42138, 88, 1874, 13], "temperature": 0.0, "avg_logprob": -0.24584728334008193, "compression_ratio": 1.5576923076923077, "no_speech_prob": 2.4299408323713578e-05}, {"id": 281, "seek": 137380, "start": 1392.3999999999999, "end": 1398.24, "text": " From time to time people write these great thorough analyses of best practices and everything", "tokens": [3358, 565, 281, 565, 561, 2464, 613, 869, 12934, 37560, 295, 1151, 7525, 293, 1203], "temperature": 0.0, "avg_logprob": -0.24584728334008193, "compression_ratio": 1.5576923076923077, "no_speech_prob": 2.4299408323713578e-05}, {"id": 282, "seek": 137380, "start": 1398.24, "end": 1399.24, "text": " that's going on.", "tokens": [300, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.24584728334008193, "compression_ratio": 1.5576923076923077, "no_speech_prob": 2.4299408323713578e-05}, {"id": 283, "seek": 139924, "start": 1399.24, "end": 1406.32, "text": " There's some really great stuff out there.", "tokens": [821, 311, 512, 534, 869, 1507, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.18780682303688742, "compression_ratio": 1.5870646766169154, "no_speech_prob": 1.2218740266689565e-05}, {"id": 284, "seek": 139924, "start": 1406.32, "end": 1412.44, "text": " One other paper to mention in this kind of broad ideas about things that you might find", "tokens": [1485, 661, 3035, 281, 2152, 294, 341, 733, 295, 4152, 3487, 466, 721, 300, 291, 1062, 915], "temperature": 0.0, "avg_logprob": -0.18780682303688742, "compression_ratio": 1.5870646766169154, "no_speech_prob": 1.2218740266689565e-05}, {"id": 285, "seek": 139924, "start": 1412.44, "end": 1417.28, "text": " helpful is a paper by somebody named Leslie Smith, who I think has got to be just about", "tokens": [4961, 307, 257, 3035, 538, 2618, 4926, 28140, 8538, 11, 567, 286, 519, 575, 658, 281, 312, 445, 466], "temperature": 0.0, "avg_logprob": -0.18780682303688742, "compression_ratio": 1.5870646766169154, "no_speech_prob": 1.2218740266689565e-05}, {"id": 286, "seek": 139924, "start": 1417.28, "end": 1424.0, "text": " the most overlooked researcher.", "tokens": [264, 881, 32269, 21751, 13], "temperature": 0.0, "avg_logprob": -0.18780682303688742, "compression_ratio": 1.5870646766169154, "no_speech_prob": 1.2218740266689565e-05}, {"id": 287, "seek": 139924, "start": 1424.0, "end": 1428.48, "text": " Leslie Smith does a lot of really great papers, which I really like.", "tokens": [28140, 8538, 775, 257, 688, 295, 534, 869, 10577, 11, 597, 286, 534, 411, 13], "temperature": 0.0, "avg_logprob": -0.18780682303688742, "compression_ratio": 1.5870646766169154, "no_speech_prob": 1.2218740266689565e-05}, {"id": 288, "seek": 142848, "start": 1428.48, "end": 1436.92, "text": " This particular paper came up with a list of 14 design patterns which seem to be generally", "tokens": [639, 1729, 3035, 1361, 493, 365, 257, 1329, 295, 3499, 1715, 8294, 597, 1643, 281, 312, 5101], "temperature": 0.0, "avg_logprob": -0.18656530536589075, "compression_ratio": 1.614814814814815, "no_speech_prob": 2.4682121875230223e-05}, {"id": 289, "seek": 142848, "start": 1436.92, "end": 1440.16, "text": " associated with better CNNs.", "tokens": [6615, 365, 1101, 24859, 82, 13], "temperature": 0.0, "avg_logprob": -0.18656530536589075, "compression_ratio": 1.614814814814815, "no_speech_prob": 2.4682121875230223e-05}, {"id": 290, "seek": 142848, "start": 1440.16, "end": 1442.48, "text": " This is a great paper to read.", "tokens": [639, 307, 257, 869, 3035, 281, 1401, 13], "temperature": 0.0, "avg_logprob": -0.18656530536589075, "compression_ratio": 1.614814814814815, "no_speech_prob": 2.4682121875230223e-05}, {"id": 291, "seek": 142848, "start": 1442.48, "end": 1443.96, "text": " It's a really easy read.", "tokens": [467, 311, 257, 534, 1858, 1401, 13], "temperature": 0.0, "avg_logprob": -0.18656530536589075, "compression_ratio": 1.614814814814815, "no_speech_prob": 2.4682121875230223e-05}, {"id": 292, "seek": 142848, "start": 1443.96, "end": 1446.16, "text": " You guys won't have any trouble with it at all, I don't think.", "tokens": [509, 1074, 1582, 380, 362, 604, 5253, 365, 309, 412, 439, 11, 286, 500, 380, 519, 13], "temperature": 0.0, "avg_logprob": -0.18656530536589075, "compression_ratio": 1.614814814814815, "no_speech_prob": 2.4682121875230223e-05}, {"id": 293, "seek": 142848, "start": 1446.16, "end": 1447.16, "text": " It's very short.", "tokens": [467, 311, 588, 2099, 13], "temperature": 0.0, "avg_logprob": -0.18656530536589075, "compression_ratio": 1.614814814814815, "no_speech_prob": 2.4682121875230223e-05}, {"id": 294, "seek": 142848, "start": 1447.16, "end": 1451.04, "text": " But I looked through all these and I just kind of thought, yeah, these all make a lot", "tokens": [583, 286, 2956, 807, 439, 613, 293, 286, 445, 733, 295, 1194, 11, 1338, 11, 613, 439, 652, 257, 688], "temperature": 0.0, "avg_logprob": -0.18656530536589075, "compression_ratio": 1.614814814814815, "no_speech_prob": 2.4682121875230223e-05}, {"id": 295, "seek": 142848, "start": 1451.04, "end": 1453.0, "text": " of sense.", "tokens": [295, 2020, 13], "temperature": 0.0, "avg_logprob": -0.18656530536589075, "compression_ratio": 1.614814814814815, "no_speech_prob": 2.4682121875230223e-05}, {"id": 296, "seek": 142848, "start": 1453.0, "end": 1455.56, "text": " If you're doing something a bit different and a bit new and you have to design a new", "tokens": [759, 291, 434, 884, 746, 257, 857, 819, 293, 257, 857, 777, 293, 291, 362, 281, 1715, 257, 777], "temperature": 0.0, "avg_logprob": -0.18656530536589075, "compression_ratio": 1.614814814814815, "no_speech_prob": 2.4682121875230223e-05}, {"id": 297, "seek": 145556, "start": 1455.56, "end": 1462.1599999999999, "text": " architecture, this would be a great list of patterns to look through.", "tokens": [9482, 11, 341, 576, 312, 257, 869, 1329, 295, 8294, 281, 574, 807, 13], "temperature": 0.0, "avg_logprob": -0.1253077599310106, "compression_ratio": 1.644, "no_speech_prob": 1.5936364434310235e-05}, {"id": 298, "seek": 145556, "start": 1462.1599999999999, "end": 1468.3999999999999, "text": " One more Leslie Smith paper to mention, and it's crazy that this is not more well-known,", "tokens": [1485, 544, 28140, 8538, 3035, 281, 2152, 11, 293, 309, 311, 3219, 300, 341, 307, 406, 544, 731, 12, 6861, 11], "temperature": 0.0, "avg_logprob": -0.1253077599310106, "compression_ratio": 1.644, "no_speech_prob": 1.5936364434310235e-05}, {"id": 299, "seek": 145556, "start": 1468.3999999999999, "end": 1473.0, "text": " something incredibly simple, which is a different approach to learning rates.", "tokens": [746, 6252, 2199, 11, 597, 307, 257, 819, 3109, 281, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.1253077599310106, "compression_ratio": 1.644, "no_speech_prob": 1.5936364434310235e-05}, {"id": 300, "seek": 145556, "start": 1473.0, "end": 1477.3999999999999, "text": " Rather than just having your learning rate gradually decrease, I'm sure a lot of you", "tokens": [16571, 813, 445, 1419, 428, 2539, 3314, 13145, 11514, 11, 286, 478, 988, 257, 688, 295, 291], "temperature": 0.0, "avg_logprob": -0.1253077599310106, "compression_ratio": 1.644, "no_speech_prob": 1.5936364434310235e-05}, {"id": 301, "seek": 145556, "start": 1477.3999999999999, "end": 1482.2, "text": " have noticed that sometimes if you suddenly increase the learning rate for a bit and then", "tokens": [362, 5694, 300, 2171, 498, 291, 5800, 3488, 264, 2539, 3314, 337, 257, 857, 293, 550], "temperature": 0.0, "avg_logprob": -0.1253077599310106, "compression_ratio": 1.644, "no_speech_prob": 1.5936364434310235e-05}, {"id": 302, "seek": 148220, "start": 1482.2, "end": 1488.32, "text": " suddenly decrease it again for a bit, it kind of goes into a better little area.", "tokens": [5800, 11514, 309, 797, 337, 257, 857, 11, 309, 733, 295, 1709, 666, 257, 1101, 707, 1859, 13], "temperature": 0.0, "avg_logprob": -0.23087233466071053, "compression_ratio": 1.8361581920903955, "no_speech_prob": 5.422173217084492e-06}, {"id": 303, "seek": 148220, "start": 1488.32, "end": 1492.92, "text": " What this paper suggests doing is try actually continually increasing your learning rate", "tokens": [708, 341, 3035, 13409, 884, 307, 853, 767, 22277, 5662, 428, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.23087233466071053, "compression_ratio": 1.8361581920903955, "no_speech_prob": 5.422173217084492e-06}, {"id": 304, "seek": 148220, "start": 1492.92, "end": 1497.0800000000002, "text": " and then decreasing it, increasing it, decreasing it, increasing it, decreasing it.", "tokens": [293, 550, 23223, 309, 11, 5662, 309, 11, 23223, 309, 11, 5662, 309, 11, 23223, 309, 13], "temperature": 0.0, "avg_logprob": -0.23087233466071053, "compression_ratio": 1.8361581920903955, "no_speech_prob": 5.422173217084492e-06}, {"id": 305, "seek": 148220, "start": 1497.0800000000002, "end": 1501.48, "text": " Something that they call cyclical learning rates.", "tokens": [6595, 300, 436, 818, 19474, 804, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.23087233466071053, "compression_ratio": 1.8361581920903955, "no_speech_prob": 5.422173217084492e-06}, {"id": 306, "seek": 148220, "start": 1501.48, "end": 1504.48, "text": " Check out the impact.", "tokens": [6881, 484, 264, 2712, 13], "temperature": 0.0, "avg_logprob": -0.23087233466071053, "compression_ratio": 1.8361581920903955, "no_speech_prob": 5.422173217084492e-06}, {"id": 307, "seek": 150448, "start": 1504.48, "end": 1515.16, "text": " Compared to non-cyclical approaches, it is way, way faster.", "tokens": [30539, 281, 2107, 12, 34080, 804, 11587, 11, 309, 307, 636, 11, 636, 4663, 13], "temperature": 0.0, "avg_logprob": -0.172425172267816, "compression_ratio": 1.4607329842931938, "no_speech_prob": 4.637834990717238e-06}, {"id": 308, "seek": 150448, "start": 1515.16, "end": 1518.32, "text": " At every point, it's much better.", "tokens": [1711, 633, 935, 11, 309, 311, 709, 1101, 13], "temperature": 0.0, "avg_logprob": -0.172425172267816, "compression_ratio": 1.4607329842931938, "no_speech_prob": 4.637834990717238e-06}, {"id": 309, "seek": 150448, "start": 1518.32, "end": 1520.84, "text": " This is something which you could easily add.", "tokens": [639, 307, 746, 597, 291, 727, 3612, 909, 13], "temperature": 0.0, "avg_logprob": -0.172425172267816, "compression_ratio": 1.4607329842931938, "no_speech_prob": 4.637834990717238e-06}, {"id": 310, "seek": 150448, "start": 1520.84, "end": 1526.32, "text": " I haven't seen this added to any library.", "tokens": [286, 2378, 380, 1612, 341, 3869, 281, 604, 6405, 13], "temperature": 0.0, "avg_logprob": -0.172425172267816, "compression_ratio": 1.4607329842931938, "no_speech_prob": 4.637834990717238e-06}, {"id": 311, "seek": 150448, "start": 1526.32, "end": 1532.68, "text": " If you created the cyclical learning rate annealing class for Keras, many people would", "tokens": [759, 291, 2942, 264, 19474, 804, 2539, 3314, 22256, 4270, 1508, 337, 591, 6985, 11, 867, 561, 576], "temperature": 0.0, "avg_logprob": -0.172425172267816, "compression_ratio": 1.4607329842931938, "no_speech_prob": 4.637834990717238e-06}, {"id": 312, "seek": 150448, "start": 1532.68, "end": 1533.68, "text": " thank you.", "tokens": [1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.172425172267816, "compression_ratio": 1.4607329842931938, "no_speech_prob": 4.637834990717238e-06}, {"id": 313, "seek": 153368, "start": 1533.68, "end": 1536.3600000000001, "text": " Actually, many people would have no idea what you're talking about, so they didn't have", "tokens": [5135, 11, 867, 561, 576, 362, 572, 1558, 437, 291, 434, 1417, 466, 11, 370, 436, 994, 380, 362], "temperature": 0.0, "avg_logprob": -0.2953179188263722, "compression_ratio": 1.5129533678756477, "no_speech_prob": 4.611157055478543e-05}, {"id": 314, "seek": 153368, "start": 1536.3600000000001, "end": 1540.5600000000002, "text": " to write the blog post to explain why it's good and show them this picture, and they", "tokens": [281, 2464, 264, 6968, 2183, 281, 2903, 983, 309, 311, 665, 293, 855, 552, 341, 3036, 11, 293, 436], "temperature": 0.0, "avg_logprob": -0.2953179188263722, "compression_ratio": 1.5129533678756477, "no_speech_prob": 4.611157055478543e-05}, {"id": 315, "seek": 153368, "start": 1540.5600000000002, "end": 1541.5600000000002, "text": " would thank you.", "tokens": [576, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.2953179188263722, "compression_ratio": 1.5129533678756477, "no_speech_prob": 4.611157055478543e-05}, {"id": 316, "seek": 153368, "start": 1541.5600000000002, "end": 1542.5600000000002, "text": " Question asked.", "tokens": [14464, 2351, 13], "temperature": 0.0, "avg_logprob": -0.2953179188263722, "compression_ratio": 1.5129533678756477, "no_speech_prob": 4.611157055478543e-05}, {"id": 317, "seek": 153368, "start": 1542.5600000000002, "end": 1559.72, "text": " If I was doing this in Keras, what I would do would be I would start with the existing", "tokens": [759, 286, 390, 884, 341, 294, 591, 6985, 11, 437, 286, 576, 360, 576, 312, 286, 576, 722, 365, 264, 6741], "temperature": 0.0, "avg_logprob": -0.2953179188263722, "compression_ratio": 1.5129533678756477, "no_speech_prob": 4.611157055478543e-05}, {"id": 318, "seek": 155972, "start": 1559.72, "end": 1566.48, "text": " learning rate annealing code that's there and make small changes until it starts working.", "tokens": [2539, 3314, 22256, 4270, 3089, 300, 311, 456, 293, 652, 1359, 2962, 1826, 309, 3719, 1364, 13], "temperature": 0.0, "avg_logprob": -0.17673695341069648, "compression_ratio": 1.7119341563786008, "no_speech_prob": 1.5689072824898176e-05}, {"id": 319, "seek": 155972, "start": 1566.48, "end": 1572.08, "text": " There's already code that does pretty much everything you want.", "tokens": [821, 311, 1217, 3089, 300, 775, 1238, 709, 1203, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.17673695341069648, "compression_ratio": 1.7119341563786008, "no_speech_prob": 1.5689072824898176e-05}, {"id": 320, "seek": 155972, "start": 1572.08, "end": 1578.28, "text": " The other cool thing about this paper is that they suggest a fairly automated approach to", "tokens": [440, 661, 1627, 551, 466, 341, 3035, 307, 300, 436, 3402, 257, 6457, 18473, 3109, 281], "temperature": 0.0, "avg_logprob": -0.17673695341069648, "compression_ratio": 1.7119341563786008, "no_speech_prob": 1.5689072824898176e-05}, {"id": 321, "seek": 155972, "start": 1578.28, "end": 1582.92, "text": " picking what the minimum and maximum bounds should be.", "tokens": [8867, 437, 264, 7285, 293, 6674, 29905, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.17673695341069648, "compression_ratio": 1.7119341563786008, "no_speech_prob": 1.5689072824898176e-05}, {"id": 322, "seek": 155972, "start": 1582.92, "end": 1587.6000000000001, "text": " This idea of roughly what should our learning rate be is something which we tend to use", "tokens": [639, 1558, 295, 9810, 437, 820, 527, 2539, 3314, 312, 307, 746, 597, 321, 3928, 281, 764], "temperature": 0.0, "avg_logprob": -0.17673695341069648, "compression_ratio": 1.7119341563786008, "no_speech_prob": 1.5689072824898176e-05}, {"id": 323, "seek": 155972, "start": 1587.6000000000001, "end": 1588.92, "text": " a lot of trial and error for.", "tokens": [257, 688, 295, 7308, 293, 6713, 337, 13], "temperature": 0.0, "avg_logprob": -0.17673695341069648, "compression_ratio": 1.7119341563786008, "no_speech_prob": 1.5689072824898176e-05}, {"id": 324, "seek": 158892, "start": 1588.92, "end": 1597.0800000000002, "text": " So check out this paper for a suggestion about how to do it somewhat automatically.", "tokens": [407, 1520, 484, 341, 3035, 337, 257, 16541, 466, 577, 281, 360, 309, 8344, 6772, 13], "temperature": 0.0, "avg_logprob": -0.13562573836399958, "compression_ratio": 1.6204379562043796, "no_speech_prob": 9.818178114073817e-06}, {"id": 325, "seek": 158892, "start": 1597.0800000000002, "end": 1600.28, "text": " So there's a whole bunch of things that I've zipped over.", "tokens": [407, 456, 311, 257, 1379, 3840, 295, 721, 300, 286, 600, 710, 5529, 670, 13], "temperature": 0.0, "avg_logprob": -0.13562573836399958, "compression_ratio": 1.6204379562043796, "no_speech_prob": 9.818178114073817e-06}, {"id": 326, "seek": 158892, "start": 1600.28, "end": 1604.48, "text": " Normally I would have dug into each of those and explained it and shown examples in notebooks", "tokens": [17424, 286, 576, 362, 22954, 666, 1184, 295, 729, 293, 8825, 309, 293, 4898, 5110, 294, 43782], "temperature": 0.0, "avg_logprob": -0.13562573836399958, "compression_ratio": 1.6204379562043796, "no_speech_prob": 9.818178114073817e-06}, {"id": 327, "seek": 158892, "start": 1604.48, "end": 1612.64, "text": " and stuff, but this is like a, you guys hopefully now have enough knowledge to take this information", "tokens": [293, 1507, 11, 457, 341, 307, 411, 257, 11, 291, 1074, 4696, 586, 362, 1547, 3601, 281, 747, 341, 1589], "temperature": 0.0, "avg_logprob": -0.13562573836399958, "compression_ratio": 1.6204379562043796, "no_speech_prob": 9.818178114073817e-06}, {"id": 328, "seek": 158892, "start": 1612.64, "end": 1614.3200000000002, "text": " and play with it.", "tokens": [293, 862, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.13562573836399958, "compression_ratio": 1.6204379562043796, "no_speech_prob": 9.818178114073817e-06}, {"id": 329, "seek": 158892, "start": 1614.3200000000002, "end": 1617.24, "text": " And what I'm hoping is that different people will play with different parts and come back", "tokens": [400, 437, 286, 478, 7159, 307, 300, 819, 561, 486, 862, 365, 819, 3166, 293, 808, 646], "temperature": 0.0, "avg_logprob": -0.13562573836399958, "compression_ratio": 1.6204379562043796, "no_speech_prob": 9.818178114073817e-06}, {"id": 330, "seek": 161724, "start": 1617.24, "end": 1623.68, "text": " and tell us what you find and hopefully we'll get some good new contributions to Keras or", "tokens": [293, 980, 505, 437, 291, 915, 293, 4696, 321, 603, 483, 512, 665, 777, 15725, 281, 591, 6985, 420], "temperature": 0.0, "avg_logprob": -0.22298970975373922, "compression_ratio": 1.4923857868020305, "no_speech_prob": 1.3631152796733659e-05}, {"id": 331, "seek": 161724, "start": 1623.68, "end": 1627.48, "text": " PyTorch or some blog posts or some papers or so forth.", "tokens": [9953, 51, 284, 339, 420, 512, 6968, 12300, 420, 512, 10577, 420, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.22298970975373922, "compression_ratio": 1.4923857868020305, "no_speech_prob": 1.3631152796733659e-05}, {"id": 332, "seek": 161724, "start": 1627.48, "end": 1633.68, "text": " Or maybe with that device stuff, even some new applications.", "tokens": [1610, 1310, 365, 300, 4302, 1507, 11, 754, 512, 777, 5821, 13], "temperature": 0.0, "avg_logprob": -0.22298970975373922, "compression_ratio": 1.4923857868020305, "no_speech_prob": 1.3631152796733659e-05}, {"id": 333, "seek": 161724, "start": 1633.68, "end": 1646.0, "text": " The next thing I wanted to look at, and again somewhat briefly, is the data science bot.", "tokens": [440, 958, 551, 286, 1415, 281, 574, 412, 11, 293, 797, 8344, 10515, 11, 307, 264, 1412, 3497, 10592, 13], "temperature": 0.0, "avg_logprob": -0.22298970975373922, "compression_ratio": 1.4923857868020305, "no_speech_prob": 1.3631152796733659e-05}, {"id": 334, "seek": 164600, "start": 1646.0, "end": 1650.6, "text": " And the reason I particularly wanted to dig into the data science bot is, there's a couple", "tokens": [400, 264, 1778, 286, 4098, 1415, 281, 2528, 666, 264, 1412, 3497, 10592, 307, 11, 456, 311, 257, 1916], "temperature": 0.0, "avg_logprob": -0.23143800776055518, "compression_ratio": 1.5919282511210762, "no_speech_prob": 1.0289210877090227e-05}, {"id": 335, "seek": 164600, "start": 1650.6, "end": 1651.6, "text": " of reasons.", "tokens": [295, 4112, 13], "temperature": 0.0, "avg_logprob": -0.23143800776055518, "compression_ratio": 1.5919282511210762, "no_speech_prob": 1.0289210877090227e-05}, {"id": 336, "seek": 164600, "start": 1651.6, "end": 1654.6, "text": " One of them, there's a million reasons.", "tokens": [1485, 295, 552, 11, 456, 311, 257, 2459, 4112, 13], "temperature": 0.0, "avg_logprob": -0.23143800776055518, "compression_ratio": 1.5919282511210762, "no_speech_prob": 1.0289210877090227e-05}, {"id": 337, "seek": 164600, "start": 1654.6, "end": 1660.52, "text": " It's a million dollar price and there are 23 days to go.", "tokens": [467, 311, 257, 2459, 7241, 3218, 293, 456, 366, 6673, 1708, 281, 352, 13], "temperature": 0.0, "avg_logprob": -0.23143800776055518, "compression_ratio": 1.5919282511210762, "no_speech_prob": 1.0289210877090227e-05}, {"id": 338, "seek": 164600, "start": 1660.52, "end": 1665.76, "text": " The second is it's an extension to everything you guys have learned so far about computer", "tokens": [440, 1150, 307, 309, 311, 364, 10320, 281, 1203, 291, 1074, 362, 3264, 370, 1400, 466, 3820], "temperature": 0.0, "avg_logprob": -0.23143800776055518, "compression_ratio": 1.5919282511210762, "no_speech_prob": 1.0289210877090227e-05}, {"id": 339, "seek": 164600, "start": 1665.76, "end": 1666.76, "text": " vision.", "tokens": [5201, 13], "temperature": 0.0, "avg_logprob": -0.23143800776055518, "compression_ratio": 1.5919282511210762, "no_speech_prob": 1.0289210877090227e-05}, {"id": 340, "seek": 164600, "start": 1666.76, "end": 1670.72, "text": " It uses all the techniques you've learned, but then some.", "tokens": [467, 4960, 439, 264, 7512, 291, 600, 3264, 11, 457, 550, 512, 13], "temperature": 0.0, "avg_logprob": -0.23143800776055518, "compression_ratio": 1.5919282511210762, "no_speech_prob": 1.0289210877090227e-05}, {"id": 341, "seek": 167072, "start": 1670.72, "end": 1676.44, "text": " So rather than 2D images, they're going to be 3D volumes.", "tokens": [407, 2831, 813, 568, 35, 5267, 11, 436, 434, 516, 281, 312, 805, 35, 22219, 13], "temperature": 0.0, "avg_logprob": -0.17320675320095485, "compression_ratio": 1.5964912280701755, "no_speech_prob": 9.972727639251389e-06}, {"id": 342, "seek": 167072, "start": 1676.44, "end": 1685.08, "text": " Rather than being 300x300 or 500x500, they're going to be 512x512x200.", "tokens": [16571, 813, 885, 6641, 87, 12566, 420, 5923, 87, 7526, 11, 436, 434, 516, 281, 312, 1025, 4762, 87, 20, 4762, 87, 7629, 13], "temperature": 0.0, "avg_logprob": -0.17320675320095485, "compression_ratio": 1.5964912280701755, "no_speech_prob": 9.972727639251389e-06}, {"id": 343, "seek": 167072, "start": 1685.08, "end": 1689.84, "text": " So a couple of hundred times bigger than stuff you've dealt with before.", "tokens": [407, 257, 1916, 295, 3262, 1413, 3801, 813, 1507, 291, 600, 15991, 365, 949, 13], "temperature": 0.0, "avg_logprob": -0.17320675320095485, "compression_ratio": 1.5964912280701755, "no_speech_prob": 9.972727639251389e-06}, {"id": 344, "seek": 167072, "start": 1689.84, "end": 1694.48, "text": " The stuff we learned in Lesson 7 about where are the fish, you're going to be needing to", "tokens": [440, 1507, 321, 3264, 294, 18649, 266, 1614, 466, 689, 366, 264, 3506, 11, 291, 434, 516, 281, 312, 18006, 281], "temperature": 0.0, "avg_logprob": -0.17320675320095485, "compression_ratio": 1.5964912280701755, "no_speech_prob": 9.972727639251389e-06}, {"id": 345, "seek": 167072, "start": 1694.48, "end": 1697.3600000000001, "text": " use a lot of that.", "tokens": [764, 257, 688, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.17320675320095485, "compression_ratio": 1.5964912280701755, "no_speech_prob": 9.972727639251389e-06}, {"id": 346, "seek": 167072, "start": 1697.3600000000001, "end": 1700.32, "text": " So I think it's a really interesting problem to solve.", "tokens": [407, 286, 519, 309, 311, 257, 534, 1880, 1154, 281, 5039, 13], "temperature": 0.0, "avg_logprob": -0.17320675320095485, "compression_ratio": 1.5964912280701755, "no_speech_prob": 9.972727639251389e-06}, {"id": 347, "seek": 170032, "start": 1700.32, "end": 1706.08, "text": " And I personally care a lot about this because my previous startup in Lidic was the first", "tokens": [400, 286, 5665, 1127, 257, 688, 466, 341, 570, 452, 3894, 18578, 294, 441, 327, 299, 390, 264, 700], "temperature": 0.0, "avg_logprob": -0.12104522098194469, "compression_ratio": 1.611353711790393, "no_speech_prob": 9.665943252912257e-06}, {"id": 348, "seek": 170032, "start": 1706.08, "end": 1713.08, "text": " organization to use deep learning to tackle this exact problem, which is trying to find", "tokens": [4475, 281, 764, 2452, 2539, 281, 14896, 341, 1900, 1154, 11, 597, 307, 1382, 281, 915], "temperature": 0.0, "avg_logprob": -0.12104522098194469, "compression_ratio": 1.611353711790393, "no_speech_prob": 9.665943252912257e-06}, {"id": 349, "seek": 170032, "start": 1713.08, "end": 1716.8, "text": " lung cancer in CT scans.", "tokens": [16730, 5592, 294, 19529, 35116, 13], "temperature": 0.0, "avg_logprob": -0.12104522098194469, "compression_ratio": 1.611353711790393, "no_speech_prob": 9.665943252912257e-06}, {"id": 350, "seek": 170032, "start": 1716.8, "end": 1722.9199999999998, "text": " The reason I made that in Lidic's first problem was mainly because I learned that if you can", "tokens": [440, 1778, 286, 1027, 300, 294, 441, 327, 299, 311, 700, 1154, 390, 8704, 570, 286, 3264, 300, 498, 291, 393], "temperature": 0.0, "avg_logprob": -0.12104522098194469, "compression_ratio": 1.611353711790393, "no_speech_prob": 9.665943252912257e-06}, {"id": 351, "seek": 170032, "start": 1722.9199999999998, "end": 1728.08, "text": " find lung cancer earlier, the probability of survival is 10 times higher.", "tokens": [915, 16730, 5592, 3071, 11, 264, 8482, 295, 12559, 307, 1266, 1413, 2946, 13], "temperature": 0.0, "avg_logprob": -0.12104522098194469, "compression_ratio": 1.611353711790393, "no_speech_prob": 9.665943252912257e-06}, {"id": 352, "seek": 172808, "start": 1728.08, "end": 1735.6399999999999, "text": " So here is something where you can have a real impact by doing this well.", "tokens": [407, 510, 307, 746, 689, 291, 393, 362, 257, 957, 2712, 538, 884, 341, 731, 13], "temperature": 0.0, "avg_logprob": -0.16430332975567513, "compression_ratio": 1.462686567164179, "no_speech_prob": 8.80098195921164e-06}, {"id": 353, "seek": 172808, "start": 1735.6399999999999, "end": 1740.1599999999999, "text": " Which is not to say that a million dollars isn't a big impact as well.", "tokens": [3013, 307, 406, 281, 584, 300, 257, 2459, 3808, 1943, 380, 257, 955, 2712, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.16430332975567513, "compression_ratio": 1.462686567164179, "no_speech_prob": 8.80098195921164e-06}, {"id": 354, "seek": 172808, "start": 1740.1599999999999, "end": 1753.1999999999998, "text": " So let me tell you a little bit about this problem.", "tokens": [407, 718, 385, 980, 291, 257, 707, 857, 466, 341, 1154, 13], "temperature": 0.0, "avg_logprob": -0.16430332975567513, "compression_ratio": 1.462686567164179, "no_speech_prob": 8.80098195921164e-06}, {"id": 355, "seek": 175320, "start": 1753.2, "end": 1759.32, "text": " Here is a lung, and this is in a DICOM viewer.", "tokens": [1692, 307, 257, 16730, 11, 293, 341, 307, 294, 257, 413, 2532, 5251, 16767, 13], "temperature": 0.0, "avg_logprob": -0.2382565907069615, "compression_ratio": 1.430232558139535, "no_speech_prob": 6.2408807934843935e-06}, {"id": 356, "seek": 175320, "start": 1759.32, "end": 1769.3600000000001, "text": " DICOM is the standard that is used for sharing most kinds of medical imaging, certainly CT", "tokens": [413, 2532, 5251, 307, 264, 3832, 300, 307, 1143, 337, 5414, 881, 3685, 295, 4625, 25036, 11, 3297, 19529], "temperature": 0.0, "avg_logprob": -0.2382565907069615, "compression_ratio": 1.430232558139535, "no_speech_prob": 6.2408807934843935e-06}, {"id": 357, "seek": 175320, "start": 1769.3600000000001, "end": 1770.3600000000001, "text": " scans.", "tokens": [35116, 13], "temperature": 0.0, "avg_logprob": -0.2382565907069615, "compression_ratio": 1.430232558139535, "no_speech_prob": 6.2408807934843935e-06}, {"id": 358, "seek": 175320, "start": 1770.3600000000001, "end": 1775.6000000000001, "text": " It is a format which contains two main things.", "tokens": [467, 307, 257, 7877, 597, 8306, 732, 2135, 721, 13], "temperature": 0.0, "avg_logprob": -0.2382565907069615, "compression_ratio": 1.430232558139535, "no_speech_prob": 6.2408807934843935e-06}, {"id": 359, "seek": 175320, "start": 1775.6000000000001, "end": 1779.4, "text": " One is a stack of images and another is some metadata.", "tokens": [1485, 307, 257, 8630, 295, 5267, 293, 1071, 307, 512, 26603, 13], "temperature": 0.0, "avg_logprob": -0.2382565907069615, "compression_ratio": 1.430232558139535, "no_speech_prob": 6.2408807934843935e-06}, {"id": 360, "seek": 177940, "start": 1779.4, "end": 1784.5600000000002, "text": " So that metadata will be things like how much radiation were they zapped by, how far away", "tokens": [407, 300, 26603, 486, 312, 721, 411, 577, 709, 12420, 645, 436, 710, 20780, 538, 11, 577, 1400, 1314], "temperature": 0.0, "avg_logprob": -0.24531135978279534, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.00010229595500277355}, {"id": 361, "seek": 177940, "start": 1784.5600000000002, "end": 1791.3200000000002, "text": " from the chest with the machine, and what brand of machine was it, and so on and so", "tokens": [490, 264, 7443, 365, 264, 3479, 11, 293, 437, 3360, 295, 3479, 390, 309, 11, 293, 370, 322, 293, 370], "temperature": 0.0, "avg_logprob": -0.24531135978279534, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.00010229595500277355}, {"id": 362, "seek": 177940, "start": 1791.3200000000002, "end": 1792.3200000000002, "text": " forth.", "tokens": [5220, 13], "temperature": 0.0, "avg_logprob": -0.24531135978279534, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.00010229595500277355}, {"id": 363, "seek": 177940, "start": 1792.3200000000002, "end": 1797.8400000000001, "text": " So you can, for most DICOM viewers, just use your scroll wheel to zip through them.", "tokens": [407, 291, 393, 11, 337, 881, 413, 2532, 5251, 8499, 11, 445, 764, 428, 11369, 5589, 281, 20730, 807, 552, 13], "temperature": 0.0, "avg_logprob": -0.24531135978279534, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.00010229595500277355}, {"id": 364, "seek": 177940, "start": 1797.8400000000001, "end": 1803.2800000000002, "text": " So all this is doing is going from top to bottom or from bottom to top.", "tokens": [407, 439, 341, 307, 884, 307, 516, 490, 1192, 281, 2767, 420, 490, 2767, 281, 1192, 13], "temperature": 0.0, "avg_logprob": -0.24531135978279534, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.00010229595500277355}, {"id": 365, "seek": 180328, "start": 1803.28, "end": 1822.56, "text": " Actually what I might do, I think is more interesting, is to say, let's actually focus", "tokens": [5135, 437, 286, 1062, 360, 11, 286, 519, 307, 544, 1880, 11, 307, 281, 584, 11, 718, 311, 767, 1879], "temperature": 0.0, "avg_logprob": -0.1974262075221285, "compression_ratio": 1.384, "no_speech_prob": 1.2411366697051562e-05}, {"id": 366, "seek": 180328, "start": 1822.56, "end": 1827.5, "text": " on the bit that's going to matter to you, which is the inside of the lung is this dark", "tokens": [322, 264, 857, 300, 311, 516, 281, 1871, 281, 291, 11, 597, 307, 264, 1854, 295, 264, 16730, 307, 341, 2877], "temperature": 0.0, "avg_logprob": -0.1974262075221285, "compression_ratio": 1.384, "no_speech_prob": 1.2411366697051562e-05}, {"id": 367, "seek": 182750, "start": 1827.5, "end": 1833.64, "text": " area here, and these little white dots are what's called the vasculature, so the little", "tokens": [1859, 510, 11, 293, 613, 707, 2418, 15026, 366, 437, 311, 1219, 264, 11481, 2444, 1503, 11, 370, 264, 707], "temperature": 0.0, "avg_logprob": -0.2016502691774952, "compression_ratio": 1.7122641509433962, "no_speech_prob": 6.854289040347794e-06}, {"id": 368, "seek": 182750, "start": 1833.64, "end": 1836.0, "text": " vessels and stuff going through the lungs.", "tokens": [20117, 293, 1507, 516, 807, 264, 19467, 13], "temperature": 0.0, "avg_logprob": -0.2016502691774952, "compression_ratio": 1.7122641509433962, "no_speech_prob": 6.854289040347794e-06}, {"id": 369, "seek": 182750, "start": 1836.0, "end": 1840.76, "text": " As I scroll through, have a look at this little dot, you'll see that it seems to move.", "tokens": [1018, 286, 11369, 807, 11, 362, 257, 574, 412, 341, 707, 5893, 11, 291, 603, 536, 300, 309, 2544, 281, 1286, 13], "temperature": 0.0, "avg_logprob": -0.2016502691774952, "compression_ratio": 1.7122641509433962, "no_speech_prob": 6.854289040347794e-06}, {"id": 370, "seek": 182750, "start": 1840.76, "end": 1843.48, "text": " See how it's moving?", "tokens": [3008, 577, 309, 311, 2684, 30], "temperature": 0.0, "avg_logprob": -0.2016502691774952, "compression_ratio": 1.7122641509433962, "no_speech_prob": 6.854289040347794e-06}, {"id": 371, "seek": 182750, "start": 1843.48, "end": 1849.32, "text": " The reason it's moving is because it's not a dot, it's actually a vessel going through", "tokens": [440, 1778, 309, 311, 2684, 307, 570, 309, 311, 406, 257, 5893, 11, 309, 311, 767, 257, 18098, 516, 807], "temperature": 0.0, "avg_logprob": -0.2016502691774952, "compression_ratio": 1.7122641509433962, "no_speech_prob": 6.854289040347794e-06}, {"id": 372, "seek": 182750, "start": 1849.32, "end": 1855.84, "text": " space so it actually looks like this.", "tokens": [1901, 370, 309, 767, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.2016502691774952, "compression_ratio": 1.7122641509433962, "no_speech_prob": 6.854289040347794e-06}, {"id": 373, "seek": 185584, "start": 1855.84, "end": 1861.6399999999999, "text": " And so if you take a slice through that, it looks like lots of dots.", "tokens": [400, 370, 498, 291, 747, 257, 13153, 807, 300, 11, 309, 1542, 411, 3195, 295, 15026, 13], "temperature": 0.0, "avg_logprob": -0.21339672743672072, "compression_ratio": 1.7653061224489797, "no_speech_prob": 2.332069016119931e-06}, {"id": 374, "seek": 185584, "start": 1861.6399999999999, "end": 1868.1999999999998, "text": " And so as you go through those slices, it looks like that.", "tokens": [400, 370, 382, 291, 352, 807, 729, 19793, 11, 309, 1542, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.21339672743672072, "compression_ratio": 1.7653061224489797, "no_speech_prob": 2.332069016119931e-06}, {"id": 375, "seek": 185584, "start": 1868.1999999999998, "end": 1872.56, "text": " And then eventually we get to the top of the lung, and that's why you see eventually it", "tokens": [400, 550, 4728, 321, 483, 281, 264, 1192, 295, 264, 16730, 11, 293, 300, 311, 983, 291, 536, 4728, 309], "temperature": 0.0, "avg_logprob": -0.21339672743672072, "compression_ratio": 1.7653061224489797, "no_speech_prob": 2.332069016119931e-06}, {"id": 376, "seek": 185584, "start": 1872.56, "end": 1874.9199999999998, "text": " all goes to white.", "tokens": [439, 1709, 281, 2418, 13], "temperature": 0.0, "avg_logprob": -0.21339672743672072, "compression_ratio": 1.7653061224489797, "no_speech_prob": 2.332069016119931e-06}, {"id": 377, "seek": 185584, "start": 1874.9199999999998, "end": 1879.1599999999999, "text": " So that's the edge basically of the organ.", "tokens": [407, 300, 311, 264, 4691, 1936, 295, 264, 1798, 13], "temperature": 0.0, "avg_logprob": -0.21339672743672072, "compression_ratio": 1.7653061224489797, "no_speech_prob": 2.332069016119931e-06}, {"id": 378, "seek": 185584, "start": 1879.1599999999999, "end": 1881.4399999999998, "text": " So you can see there are edges on each side.", "tokens": [407, 291, 393, 536, 456, 366, 8819, 322, 1184, 1252, 13], "temperature": 0.0, "avg_logprob": -0.21339672743672072, "compression_ratio": 1.7653061224489797, "no_speech_prob": 2.332069016119931e-06}, {"id": 379, "seek": 185584, "start": 1881.4399999999998, "end": 1883.9199999999998, "text": " Then there's also bone.", "tokens": [1396, 456, 311, 611, 9026, 13], "temperature": 0.0, "avg_logprob": -0.21339672743672072, "compression_ratio": 1.7653061224489797, "no_speech_prob": 2.332069016119931e-06}, {"id": 380, "seek": 188392, "start": 1883.92, "end": 1889.64, "text": " So some of you have been looking at this already over the last few weeks and have often asked", "tokens": [407, 512, 295, 291, 362, 668, 1237, 412, 341, 1217, 670, 264, 1036, 1326, 3259, 293, 362, 2049, 2351], "temperature": 0.0, "avg_logprob": -0.12629625071649966, "compression_ratio": 1.5813953488372092, "no_speech_prob": 1.5056987194839166e-06}, {"id": 381, "seek": 188392, "start": 1889.64, "end": 1894.44, "text": " me about how to deal with multiple images.", "tokens": [385, 466, 577, 281, 2028, 365, 3866, 5267, 13], "temperature": 0.0, "avg_logprob": -0.12629625071649966, "compression_ratio": 1.5813953488372092, "no_speech_prob": 1.5056987194839166e-06}, {"id": 382, "seek": 188392, "start": 1894.44, "end": 1899.48, "text": " And what I've said each time is don't think of it as multiple images.", "tokens": [400, 437, 286, 600, 848, 1184, 565, 307, 500, 380, 519, 295, 309, 382, 3866, 5267, 13], "temperature": 0.0, "avg_logprob": -0.12629625071649966, "compression_ratio": 1.5813953488372092, "no_speech_prob": 1.5056987194839166e-06}, {"id": 383, "seek": 188392, "start": 1899.48, "end": 1907.64, "text": " Think of it in the way your DICOM viewer can, if you have a 3D button like this one does.", "tokens": [6557, 295, 309, 294, 264, 636, 428, 413, 2532, 5251, 16767, 393, 11, 498, 291, 362, 257, 805, 35, 2960, 411, 341, 472, 775, 13], "temperature": 0.0, "avg_logprob": -0.12629625071649966, "compression_ratio": 1.5813953488372092, "no_speech_prob": 1.5056987194839166e-06}, {"id": 384, "seek": 188392, "start": 1907.64, "end": 1911.3600000000001, "text": " That's actually what we're just looking at.", "tokens": [663, 311, 767, 437, 321, 434, 445, 1237, 412, 13], "temperature": 0.0, "avg_logprob": -0.12629625071649966, "compression_ratio": 1.5813953488372092, "no_speech_prob": 1.5056987194839166e-06}, {"id": 385, "seek": 191136, "start": 1911.36, "end": 1916.84, "text": " So it's not a bunch of flat images, it's a 3D volume.", "tokens": [407, 309, 311, 406, 257, 3840, 295, 4962, 5267, 11, 309, 311, 257, 805, 35, 5523, 13], "temperature": 0.0, "avg_logprob": -0.14108016355982367, "compression_ratio": 1.738532110091743, "no_speech_prob": 2.2603153411182575e-06}, {"id": 386, "seek": 191136, "start": 1916.84, "end": 1921.1599999999999, "text": " It just so happens that the default way that most DICOM viewers show things is by a bunch", "tokens": [467, 445, 370, 2314, 300, 264, 7576, 636, 300, 881, 413, 2532, 5251, 8499, 855, 721, 307, 538, 257, 3840], "temperature": 0.0, "avg_logprob": -0.14108016355982367, "compression_ratio": 1.738532110091743, "no_speech_prob": 2.2603153411182575e-06}, {"id": 387, "seek": 191136, "start": 1921.1599999999999, "end": 1924.84, "text": " of flat images.", "tokens": [295, 4962, 5267, 13], "temperature": 0.0, "avg_logprob": -0.14108016355982367, "compression_ratio": 1.738532110091743, "no_speech_prob": 2.2603153411182575e-06}, {"id": 388, "seek": 191136, "start": 1924.84, "end": 1930.24, "text": " But it's really important that you think of it as a 3D volume because you're looking in", "tokens": [583, 309, 311, 534, 1021, 300, 291, 519, 295, 309, 382, 257, 805, 35, 5523, 570, 291, 434, 1237, 294], "temperature": 0.0, "avg_logprob": -0.14108016355982367, "compression_ratio": 1.738532110091743, "no_speech_prob": 2.2603153411182575e-06}, {"id": 389, "seek": 191136, "start": 1930.24, "end": 1932.12, "text": " this space.", "tokens": [341, 1901, 13], "temperature": 0.0, "avg_logprob": -0.14108016355982367, "compression_ratio": 1.738532110091743, "no_speech_prob": 2.2603153411182575e-06}, {"id": 390, "seek": 191136, "start": 1932.12, "end": 1935.0, "text": " Now what are you looking for in this space?", "tokens": [823, 437, 366, 291, 1237, 337, 294, 341, 1901, 30], "temperature": 0.0, "avg_logprob": -0.14108016355982367, "compression_ratio": 1.738532110091743, "no_speech_prob": 2.2603153411182575e-06}, {"id": 391, "seek": 191136, "start": 1935.0, "end": 1939.76, "text": " What you're looking for is you're looking for somebody who has lung cancer.", "tokens": [708, 291, 434, 1237, 337, 307, 291, 434, 1237, 337, 2618, 567, 575, 16730, 5592, 13], "temperature": 0.0, "avg_logprob": -0.14108016355982367, "compression_ratio": 1.738532110091743, "no_speech_prob": 2.2603153411182575e-06}, {"id": 392, "seek": 193976, "start": 1939.76, "end": 1943.76, "text": " And what somebody who has lung cancer looks like is that somewhere in this space there", "tokens": [400, 437, 2618, 567, 575, 16730, 5592, 1542, 411, 307, 300, 4079, 294, 341, 1901, 456], "temperature": 0.0, "avg_logprob": -0.20518847209651295, "compression_ratio": 1.6287128712871286, "no_speech_prob": 6.048888280929532e-06}, {"id": 393, "seek": 193976, "start": 1943.76, "end": 1944.8799999999999, "text": " is a blob.", "tokens": [307, 257, 46115, 13], "temperature": 0.0, "avg_logprob": -0.20518847209651295, "compression_ratio": 1.6287128712871286, "no_speech_prob": 6.048888280929532e-06}, {"id": 394, "seek": 193976, "start": 1944.8799999999999, "end": 1953.8, "text": " It could be roughly spherical blob, it could be pretty small, around 5mm is where people", "tokens": [467, 727, 312, 9810, 37300, 46115, 11, 309, 727, 312, 1238, 1359, 11, 926, 1025, 2174, 307, 689, 561], "temperature": 0.0, "avg_logprob": -0.20518847209651295, "compression_ratio": 1.6287128712871286, "no_speech_prob": 6.048888280929532e-06}, {"id": 395, "seek": 193976, "start": 1953.8, "end": 1957.8799999999999, "text": " start to get particularly concerned about a blob.", "tokens": [722, 281, 483, 4098, 5922, 466, 257, 46115, 13], "temperature": 0.0, "avg_logprob": -0.20518847209651295, "compression_ratio": 1.6287128712871286, "no_speech_prob": 6.048888280929532e-06}, {"id": 396, "seek": 193976, "start": 1957.8799999999999, "end": 1963.44, "text": " And so what that means is that for a radiologist, as they flick through a scan like this, is", "tokens": [400, 370, 437, 300, 1355, 307, 300, 337, 257, 16335, 9201, 11, 382, 436, 22774, 807, 257, 11049, 411, 341, 11, 307], "temperature": 0.0, "avg_logprob": -0.20518847209651295, "compression_ratio": 1.6287128712871286, "no_speech_prob": 6.048888280929532e-06}, {"id": 397, "seek": 196344, "start": 1963.44, "end": 1970.3200000000002, "text": " that they're looking for a dot which doesn't move, but which appears, gets bigger, and", "tokens": [300, 436, 434, 1237, 337, 257, 5893, 597, 1177, 380, 1286, 11, 457, 597, 7038, 11, 2170, 3801, 11, 293], "temperature": 0.0, "avg_logprob": -0.16465056311223925, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.157342573307687e-06}, {"id": 398, "seek": 196344, "start": 1970.3200000000002, "end": 1971.3200000000002, "text": " then disappears.", "tokens": [550, 25527, 13], "temperature": 0.0, "avg_logprob": -0.16465056311223925, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.157342573307687e-06}, {"id": 399, "seek": 196344, "start": 1971.3200000000002, "end": 1974.92, "text": " That's what a blob looks like.", "tokens": [663, 311, 437, 257, 46115, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.16465056311223925, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.157342573307687e-06}, {"id": 400, "seek": 196344, "start": 1974.92, "end": 1983.76, "text": " So you can see why radiologists very, very, very often miss nodules, blobs, inlions.", "tokens": [407, 291, 393, 536, 983, 16335, 12256, 588, 11, 588, 11, 588, 2049, 1713, 15224, 3473, 11, 1749, 929, 11, 294, 75, 626, 13], "temperature": 0.0, "avg_logprob": -0.16465056311223925, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.157342573307687e-06}, {"id": 401, "seek": 196344, "start": 1983.76, "end": 1988.56, "text": " Because in all of this area, you've got to have extraordinary vision to be able to see", "tokens": [1436, 294, 439, 295, 341, 1859, 11, 291, 600, 658, 281, 362, 10581, 5201, 281, 312, 1075, 281, 536], "temperature": 0.0, "avg_logprob": -0.16465056311223925, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.157342573307687e-06}, {"id": 402, "seek": 196344, "start": 1988.56, "end": 1992.0, "text": " every little blob appear and then disappear again.", "tokens": [633, 707, 46115, 4204, 293, 550, 11596, 797, 13], "temperature": 0.0, "avg_logprob": -0.16465056311223925, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.157342573307687e-06}, {"id": 403, "seek": 199200, "start": 1992.0, "end": 2000.32, "text": " And remember, the sooner you catch it, you get a 10x improved chance of survival.", "tokens": [400, 1604, 11, 264, 15324, 291, 3745, 309, 11, 291, 483, 257, 1266, 87, 9689, 2931, 295, 12559, 13], "temperature": 0.0, "avg_logprob": -0.137239915020061, "compression_ratio": 1.6588235294117648, "no_speech_prob": 6.439006938308012e-06}, {"id": 404, "seek": 199200, "start": 2000.32, "end": 2005.72, "text": " And generally speaking, when a radiologist looks at one of these scans, they're not looking", "tokens": [400, 5101, 4124, 11, 562, 257, 16335, 9201, 1542, 412, 472, 295, 613, 35116, 11, 436, 434, 406, 1237], "temperature": 0.0, "avg_logprob": -0.137239915020061, "compression_ratio": 1.6588235294117648, "no_speech_prob": 6.439006938308012e-06}, {"id": 405, "seek": 199200, "start": 2005.72, "end": 2007.84, "text": " for nodules.", "tokens": [337, 15224, 3473, 13], "temperature": 0.0, "avg_logprob": -0.137239915020061, "compression_ratio": 1.6588235294117648, "no_speech_prob": 6.439006938308012e-06}, {"id": 406, "seek": 199200, "start": 2007.84, "end": 2012.6, "text": " They're looking for something else because lung cancer, at least in the earlier stages,", "tokens": [814, 434, 1237, 337, 746, 1646, 570, 16730, 5592, 11, 412, 1935, 294, 264, 3071, 10232, 11], "temperature": 0.0, "avg_logprob": -0.137239915020061, "compression_ratio": 1.6588235294117648, "no_speech_prob": 6.439006938308012e-06}, {"id": 407, "seek": 199200, "start": 2012.6, "end": 2013.6, "text": " is asymptomatic.", "tokens": [307, 35114, 13143, 13], "temperature": 0.0, "avg_logprob": -0.137239915020061, "compression_ratio": 1.6588235294117648, "no_speech_prob": 6.439006938308012e-06}, {"id": 408, "seek": 199200, "start": 2013.6, "end": 2016.6, "text": " It doesn't cause you to feel different.", "tokens": [467, 1177, 380, 3082, 291, 281, 841, 819, 13], "temperature": 0.0, "avg_logprob": -0.137239915020061, "compression_ratio": 1.6588235294117648, "no_speech_prob": 6.439006938308012e-06}, {"id": 409, "seek": 199200, "start": 2016.6, "end": 2019.68, "text": " So it's like something that every radiologist has to be thinking about when they're looking", "tokens": [407, 309, 311, 411, 746, 300, 633, 16335, 9201, 575, 281, 312, 1953, 466, 562, 436, 434, 1237], "temperature": 0.0, "avg_logprob": -0.137239915020061, "compression_ratio": 1.6588235294117648, "no_speech_prob": 6.439006938308012e-06}, {"id": 410, "seek": 201968, "start": 2019.68, "end": 2023.8400000000001, "text": " for pneumonia or whatever else.", "tokens": [337, 43097, 420, 2035, 1646, 13], "temperature": 0.0, "avg_logprob": -0.19070116678873697, "compression_ratio": 1.5595238095238095, "no_speech_prob": 8.397969395446125e-06}, {"id": 411, "seek": 201968, "start": 2023.8400000000001, "end": 2029.3600000000001, "text": " So that's the basic idea, is that we're going to try and come up with in the next hour or", "tokens": [407, 300, 311, 264, 3875, 1558, 11, 307, 300, 321, 434, 516, 281, 853, 293, 808, 493, 365, 294, 264, 958, 1773, 420], "temperature": 0.0, "avg_logprob": -0.19070116678873697, "compression_ratio": 1.5595238095238095, "no_speech_prob": 8.397969395446125e-06}, {"id": 412, "seek": 201968, "start": 2029.3600000000001, "end": 2035.72, "text": " so some idea about how would you find these blobs, how would you find these nodules.", "tokens": [370, 512, 1558, 466, 577, 576, 291, 915, 613, 1749, 929, 11, 577, 576, 291, 915, 613, 15224, 3473, 13], "temperature": 0.0, "avg_logprob": -0.19070116678873697, "compression_ratio": 1.5595238095238095, "no_speech_prob": 8.397969395446125e-06}, {"id": 413, "seek": 201968, "start": 2035.72, "end": 2048.4, "text": " So each of these things generally is about 512x512x100.", "tokens": [407, 1184, 295, 613, 721, 5101, 307, 466, 1025, 4762, 87, 20, 4762, 87, 6879, 13], "temperature": 0.0, "avg_logprob": -0.19070116678873697, "compression_ratio": 1.5595238095238095, "no_speech_prob": 8.397969395446125e-06}, {"id": 414, "seek": 204840, "start": 2048.4, "end": 2052.4, "text": " The equivalent of a pixel in 3D space is called a voxel.", "tokens": [440, 10344, 295, 257, 19261, 294, 805, 35, 1901, 307, 1219, 257, 1650, 87, 338, 13], "temperature": 0.0, "avg_logprob": -0.11633404932524029, "compression_ratio": 1.48125, "no_speech_prob": 2.5215647383447504e-06}, {"id": 415, "seek": 204840, "start": 2052.4, "end": 2056.8, "text": " So a voxel simply means a pixel in 3D space.", "tokens": [407, 257, 1650, 87, 338, 2935, 1355, 257, 19261, 294, 805, 35, 1901, 13], "temperature": 0.0, "avg_logprob": -0.11633404932524029, "compression_ratio": 1.48125, "no_speech_prob": 2.5215647383447504e-06}, {"id": 416, "seek": 204840, "start": 2056.8, "end": 2065.96, "text": " So this here is rendering a bunch of voxels.", "tokens": [407, 341, 510, 307, 22407, 257, 3840, 295, 1650, 87, 1625, 13], "temperature": 0.0, "avg_logprob": -0.11633404932524029, "compression_ratio": 1.48125, "no_speech_prob": 2.5215647383447504e-06}, {"id": 417, "seek": 204840, "start": 2065.96, "end": 2076.52, "text": " Each voxel in a CT scan is a 12-bit integer, if memory serves me correctly, and a computer", "tokens": [6947, 1650, 87, 338, 294, 257, 19529, 11049, 307, 257, 2272, 12, 5260, 24922, 11, 498, 4675, 13451, 385, 8944, 11, 293, 257, 3820], "temperature": 0.0, "avg_logprob": -0.11633404932524029, "compression_ratio": 1.48125, "no_speech_prob": 2.5215647383447504e-06}, {"id": 418, "seek": 207652, "start": 2076.52, "end": 2080.92, "text": " screen can only show 8 bits of grayscale.", "tokens": [2568, 393, 787, 855, 1649, 9239, 295, 677, 3772, 37088, 13], "temperature": 0.0, "avg_logprob": -0.1410486602783203, "compression_ratio": 1.5194174757281553, "no_speech_prob": 4.222805728204548e-06}, {"id": 419, "seek": 207652, "start": 2080.92, "end": 2086.0, "text": " And furthermore, your eyes can't necessarily distinguish between all those grayscale perfectly", "tokens": [400, 3052, 3138, 11, 428, 2575, 393, 380, 4725, 20206, 1296, 439, 729, 677, 3772, 37088, 6239], "temperature": 0.0, "avg_logprob": -0.1410486602783203, "compression_ratio": 1.5194174757281553, "no_speech_prob": 4.222805728204548e-06}, {"id": 420, "seek": 207652, "start": 2086.0, "end": 2087.0, "text": " anyway.", "tokens": [4033, 13], "temperature": 0.0, "avg_logprob": -0.1410486602783203, "compression_ratio": 1.5194174757281553, "no_speech_prob": 4.222805728204548e-06}, {"id": 421, "seek": 207652, "start": 2087.0, "end": 2093.86, "text": " So what every DICOM viewer provides is something called a windowing adjustment.", "tokens": [407, 437, 633, 413, 2532, 5251, 16767, 6417, 307, 746, 1219, 257, 4910, 278, 17132, 13], "temperature": 0.0, "avg_logprob": -0.1410486602783203, "compression_ratio": 1.5194174757281553, "no_speech_prob": 4.222805728204548e-06}, {"id": 422, "seek": 207652, "start": 2093.86, "end": 2101.92, "text": " So a windowing adjustment, here is the default window which is designed to basically map", "tokens": [407, 257, 4910, 278, 17132, 11, 510, 307, 264, 7576, 4910, 597, 307, 4761, 281, 1936, 4471], "temperature": 0.0, "avg_logprob": -0.1410486602783203, "compression_ratio": 1.5194174757281553, "no_speech_prob": 4.222805728204548e-06}, {"id": 423, "seek": 210192, "start": 2101.92, "end": 2108.84, "text": " some subset of that 12-bit space to the screen so that it highlights certain things.", "tokens": [512, 25993, 295, 300, 2272, 12, 5260, 1901, 281, 264, 2568, 370, 300, 309, 14254, 1629, 721, 13], "temperature": 0.0, "avg_logprob": -0.1151244795167601, "compression_ratio": 1.627027027027027, "no_speech_prob": 1.4144720807962585e-06}, {"id": 424, "seek": 210192, "start": 2108.84, "end": 2118.64, "text": " And so the units, CT scans use are called Houndsfield units, and certain ranges of Houndsfield", "tokens": [400, 370, 264, 6815, 11, 19529, 35116, 764, 366, 1219, 389, 4432, 7610, 6815, 11, 293, 1629, 22526, 295, 389, 4432, 7610], "temperature": 0.0, "avg_logprob": -0.1151244795167601, "compression_ratio": 1.627027027027027, "no_speech_prob": 1.4144720807962585e-06}, {"id": 425, "seek": 210192, "start": 2118.64, "end": 2123.64, "text": " units tell you that something is some particular part of the body.", "tokens": [6815, 980, 291, 300, 746, 307, 512, 1729, 644, 295, 264, 1772, 13], "temperature": 0.0, "avg_logprob": -0.1151244795167601, "compression_ratio": 1.627027027027027, "no_speech_prob": 1.4144720807962585e-06}, {"id": 426, "seek": 210192, "start": 2123.64, "end": 2126.88, "text": " And so you can see here that the bone is being lit up.", "tokens": [400, 370, 291, 393, 536, 510, 300, 264, 9026, 307, 885, 7997, 493, 13], "temperature": 0.0, "avg_logprob": -0.1151244795167601, "compression_ratio": 1.627027027027027, "no_speech_prob": 1.4144720807962585e-06}, {"id": 427, "seek": 212688, "start": 2126.88, "end": 2132.88, "text": " So we've selected an image window which is designed to allow us to see the bone clearly.", "tokens": [407, 321, 600, 8209, 364, 3256, 4910, 597, 307, 4761, 281, 2089, 505, 281, 536, 264, 9026, 4448, 13], "temperature": 0.0, "avg_logprob": -0.1782944050241024, "compression_ratio": 1.3432835820895523, "no_speech_prob": 8.93964352144394e-06}, {"id": 428, "seek": 212688, "start": 2132.88, "end": 2141.08, "text": " So what I did when I opened this was I switched it to CT's chest, which is some kind person", "tokens": [407, 437, 286, 630, 562, 286, 5625, 341, 390, 286, 16858, 309, 281, 19529, 311, 7443, 11, 597, 307, 512, 733, 954], "temperature": 0.0, "avg_logprob": -0.1782944050241024, "compression_ratio": 1.3432835820895523, "no_speech_prob": 8.93964352144394e-06}, {"id": 429, "seek": 214108, "start": 2141.08, "end": 2157.48, "text": " has already figured out what the best window is.", "tokens": [575, 1217, 8932, 484, 437, 264, 1151, 4910, 307, 13], "temperature": 0.0, "avg_logprob": -0.3016991895787856, "compression_ratio": 1.2477064220183487, "no_speech_prob": 5.7718821153684985e-06}, {"id": 430, "seek": 214108, "start": 2157.48, "end": 2168.12, "text": " For you working with deep learning, you don't have to care about that because of course", "tokens": [1171, 291, 1364, 365, 2452, 2539, 11, 291, 500, 380, 362, 281, 1127, 466, 300, 570, 295, 1164], "temperature": 0.0, "avg_logprob": -0.3016991895787856, "compression_ratio": 1.2477064220183487, "no_speech_prob": 5.7718821153684985e-06}, {"id": 431, "seek": 216812, "start": 2168.12, "end": 2173.68, "text": " the deep learning algorithm can see 12 bits perfectly well, so nothing really to worry", "tokens": [264, 2452, 2539, 9284, 393, 536, 2272, 9239, 6239, 731, 11, 370, 1825, 534, 281, 3292], "temperature": 0.0, "avg_logprob": -0.17822437555017606, "compression_ratio": 1.538888888888889, "no_speech_prob": 1.3419688002613839e-05}, {"id": 432, "seek": 216812, "start": 2173.68, "end": 2177.04, "text": " about.", "tokens": [466, 13], "temperature": 0.0, "avg_logprob": -0.17822437555017606, "compression_ratio": 1.538888888888889, "no_speech_prob": 1.3419688002613839e-05}, {"id": 433, "seek": 216812, "start": 2177.04, "end": 2184.72, "text": " So one of the challenges with dealing with this data science bold data is that there's", "tokens": [407, 472, 295, 264, 4759, 365, 6260, 365, 341, 1412, 3497, 11928, 1412, 307, 300, 456, 311], "temperature": 0.0, "avg_logprob": -0.17822437555017606, "compression_ratio": 1.538888888888889, "no_speech_prob": 1.3419688002613839e-05}, {"id": 434, "seek": 216812, "start": 2184.72, "end": 2187.64, "text": " a lot of preprocessing to do.", "tokens": [257, 688, 295, 2666, 340, 780, 278, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.17822437555017606, "compression_ratio": 1.538888888888889, "no_speech_prob": 1.3419688002613839e-05}, {"id": 435, "seek": 216812, "start": 2187.64, "end": 2194.16, "text": " But the good news is that there's a couple of fantastic tutorials.", "tokens": [583, 264, 665, 2583, 307, 300, 456, 311, 257, 1916, 295, 5456, 17616, 13], "temperature": 0.0, "avg_logprob": -0.17822437555017606, "compression_ratio": 1.538888888888889, "no_speech_prob": 1.3419688002613839e-05}, {"id": 436, "seek": 219416, "start": 2194.16, "end": 2200.2799999999997, "text": " So hopefully you've found out by now that I'm caggle, if you click on the kernels button,", "tokens": [407, 4696, 291, 600, 1352, 484, 538, 586, 300, 286, 478, 269, 559, 22631, 11, 498, 291, 2052, 322, 264, 23434, 1625, 2960, 11], "temperature": 0.0, "avg_logprob": -0.17879370542672965, "compression_ratio": 1.5378151260504203, "no_speech_prob": 8.2678106991807e-06}, {"id": 437, "seek": 219416, "start": 2200.2799999999997, "end": 2206.16, "text": " you basically get to see people's IPython notebooks where they show you how to do certain", "tokens": [291, 1936, 483, 281, 536, 561, 311, 8671, 88, 11943, 43782, 689, 436, 855, 291, 577, 281, 360, 1629], "temperature": 0.0, "avg_logprob": -0.17879370542672965, "compression_ratio": 1.5378151260504203, "no_speech_prob": 8.2678106991807e-06}, {"id": 438, "seek": 219416, "start": 2206.16, "end": 2207.16, "text": " things.", "tokens": [721, 13], "temperature": 0.0, "avg_logprob": -0.17879370542672965, "compression_ratio": 1.5378151260504203, "no_speech_prob": 8.2678106991807e-06}, {"id": 439, "seek": 219416, "start": 2207.16, "end": 2214.3599999999997, "text": " In this case, this guy has got a full preprocessing tutorial showing how to load DICOM, convert", "tokens": [682, 341, 1389, 11, 341, 2146, 575, 658, 257, 1577, 2666, 340, 780, 278, 7073, 4099, 577, 281, 3677, 413, 2532, 5251, 11, 7620], "temperature": 0.0, "avg_logprob": -0.17879370542672965, "compression_ratio": 1.5378151260504203, "no_speech_prob": 8.2678106991807e-06}, {"id": 440, "seek": 219416, "start": 2214.3599999999997, "end": 2217.68, "text": " the values to Houndsfield units, and so forth.", "tokens": [264, 4190, 281, 389, 4432, 7610, 6815, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.17879370542672965, "compression_ratio": 1.5378151260504203, "no_speech_prob": 8.2678106991807e-06}, {"id": 441, "seek": 219416, "start": 2217.68, "end": 2221.12, "text": " I'll show you some of these pieces.", "tokens": [286, 603, 855, 291, 512, 295, 613, 3755, 13], "temperature": 0.0, "avg_logprob": -0.17879370542672965, "compression_ratio": 1.5378151260504203, "no_speech_prob": 8.2678106991807e-06}, {"id": 442, "seek": 222112, "start": 2221.12, "end": 2229.56, "text": " So DICOM you will load with some library, probably with pydicom.", "tokens": [407, 413, 2532, 5251, 291, 486, 3677, 365, 512, 6405, 11, 1391, 365, 10664, 67, 299, 298, 13], "temperature": 0.0, "avg_logprob": -0.2990531921386719, "compression_ratio": 1.403973509933775, "no_speech_prob": 4.985831765225157e-05}, {"id": 443, "seek": 222112, "start": 2229.56, "end": 2237.48, "text": " Pydicom is a library that's a bit like a pillow or an image.open, it's more like a DICOM.open", "tokens": [9953, 67, 299, 298, 307, 257, 6405, 300, 311, 257, 857, 411, 257, 18581, 420, 364, 3256, 13, 15752, 11, 309, 311, 544, 411, 257, 413, 2532, 5251, 13, 15752], "temperature": 0.0, "avg_logprob": -0.2990531921386719, "compression_ratio": 1.403973509933775, "no_speech_prob": 4.985831765225157e-05}, {"id": 444, "seek": 222112, "start": 2237.48, "end": 2244.3199999999997, "text": " and end up with a 3D file and of course the metadata.", "tokens": [293, 917, 493, 365, 257, 805, 35, 3991, 293, 295, 1164, 264, 26603, 13], "temperature": 0.0, "avg_logprob": -0.2990531921386719, "compression_ratio": 1.403973509933775, "no_speech_prob": 4.985831765225157e-05}, {"id": 445, "seek": 224432, "start": 2244.32, "end": 2251.2400000000002, "text": " You can see here, using the metadata, image position, lice location.", "tokens": [509, 393, 536, 510, 11, 1228, 264, 26603, 11, 3256, 2535, 11, 287, 573, 4914, 13], "temperature": 0.0, "avg_logprob": -0.18810661377445345, "compression_ratio": 1.457142857142857, "no_speech_prob": 7.766884664306417e-06}, {"id": 446, "seek": 224432, "start": 2251.2400000000002, "end": 2257.6000000000004, "text": " So the metadata comes through with just attributes of the Python object.", "tokens": [407, 264, 26603, 1487, 807, 365, 445, 17212, 295, 264, 15329, 2657, 13], "temperature": 0.0, "avg_logprob": -0.18810661377445345, "compression_ratio": 1.457142857142857, "no_speech_prob": 7.766884664306417e-06}, {"id": 447, "seek": 224432, "start": 2257.6000000000004, "end": 2268.0800000000004, "text": " This person has very kindly provided to you a list of the Houndsfield units for each of", "tokens": [639, 954, 575, 588, 29736, 5649, 281, 291, 257, 1329, 295, 264, 389, 4432, 7610, 6815, 337, 1184, 295], "temperature": 0.0, "avg_logprob": -0.18810661377445345, "compression_ratio": 1.457142857142857, "no_speech_prob": 7.766884664306417e-06}, {"id": 448, "seek": 224432, "start": 2268.0800000000004, "end": 2272.4, "text": " the different substances.", "tokens": [264, 819, 25455, 13], "temperature": 0.0, "avg_logprob": -0.18810661377445345, "compression_ratio": 1.457142857142857, "no_speech_prob": 7.766884664306417e-06}, {"id": 449, "seek": 227240, "start": 2272.4, "end": 2282.56, "text": " So he shows how to translate stuff into that range, and so it's great to draw lots of pictures.", "tokens": [407, 415, 3110, 577, 281, 13799, 1507, 666, 300, 3613, 11, 293, 370, 309, 311, 869, 281, 2642, 3195, 295, 5242, 13], "temperature": 0.0, "avg_logprob": -0.23384949366251628, "compression_ratio": 1.480263157894737, "no_speech_prob": 9.080391464522108e-06}, {"id": 450, "seek": 227240, "start": 2282.56, "end": 2286.2400000000002, "text": " Here is a histogram for this particular picture.", "tokens": [1692, 307, 257, 49816, 337, 341, 1729, 3036, 13], "temperature": 0.0, "avg_logprob": -0.23384949366251628, "compression_ratio": 1.480263157894737, "no_speech_prob": 9.080391464522108e-06}, {"id": 451, "seek": 227240, "start": 2286.2400000000002, "end": 2300.64, "text": " So you can see that most of it is air, and then you get some bone and some lung.", "tokens": [407, 291, 393, 536, 300, 881, 295, 309, 307, 1988, 11, 293, 550, 291, 483, 512, 9026, 293, 512, 16730, 13], "temperature": 0.0, "avg_logprob": -0.23384949366251628, "compression_ratio": 1.480263157894737, "no_speech_prob": 9.080391464522108e-06}, {"id": 452, "seek": 230064, "start": 2300.64, "end": 2310.8799999999997, "text": " So the next thing to think about is the voxel spacing, which is as you move across one bit", "tokens": [407, 264, 958, 551, 281, 519, 466, 307, 264, 1650, 87, 338, 27739, 11, 597, 307, 382, 291, 1286, 2108, 472, 857], "temperature": 0.0, "avg_logprob": -0.18997314998081752, "compression_ratio": 1.5964912280701755, "no_speech_prob": 7.296316198335262e-06}, {"id": 453, "seek": 230064, "start": 2310.8799999999997, "end": 2318.2, "text": " of x-axis or one bit of y-axis or from slice to slice, how far in the real world are you", "tokens": [295, 2031, 12, 24633, 420, 472, 857, 295, 288, 12, 24633, 420, 490, 13153, 281, 13153, 11, 577, 1400, 294, 264, 957, 1002, 366, 291], "temperature": 0.0, "avg_logprob": -0.18997314998081752, "compression_ratio": 1.5964912280701755, "no_speech_prob": 7.296316198335262e-06}, {"id": 454, "seek": 230064, "start": 2318.2, "end": 2319.2, "text": " moving.", "tokens": [2684, 13], "temperature": 0.0, "avg_logprob": -0.18997314998081752, "compression_ratio": 1.5964912280701755, "no_speech_prob": 7.296316198335262e-06}, {"id": 455, "seek": 230064, "start": 2319.2, "end": 2324.04, "text": " One of the annoying things about MetaPool imaging is that different kinds of scanners", "tokens": [1485, 295, 264, 11304, 721, 466, 6377, 64, 47, 1092, 25036, 307, 300, 819, 3685, 295, 795, 25792], "temperature": 0.0, "avg_logprob": -0.18997314998081752, "compression_ratio": 1.5964912280701755, "no_speech_prob": 7.296316198335262e-06}, {"id": 456, "seek": 232404, "start": 2324.04, "end": 2330.72, "text": " have different distances between those slices, it's called the slice thickness, and the different", "tokens": [362, 819, 22182, 1296, 729, 19793, 11, 309, 311, 1219, 264, 13153, 14855, 11, 293, 264, 819], "temperature": 0.0, "avg_logprob": -0.1620559397431993, "compression_ratio": 1.6832579185520362, "no_speech_prob": 2.1907760583417257e-06}, {"id": 457, "seek": 232404, "start": 2330.72, "end": 2333.92, "text": " meanings of the x and y axes.", "tokens": [28138, 295, 264, 2031, 293, 288, 35387, 13], "temperature": 0.0, "avg_logprob": -0.1620559397431993, "compression_ratio": 1.6832579185520362, "no_speech_prob": 2.1907760583417257e-06}, {"id": 458, "seek": 232404, "start": 2333.92, "end": 2336.6, "text": " Luckily that stuff is all in the DICOM metadata.", "tokens": [19726, 300, 1507, 307, 439, 294, 264, 413, 2532, 5251, 26603, 13], "temperature": 0.0, "avg_logprob": -0.1620559397431993, "compression_ratio": 1.6832579185520362, "no_speech_prob": 2.1907760583417257e-06}, {"id": 459, "seek": 232404, "start": 2336.6, "end": 2344.68, "text": " So the resampling process means taking those lists of slices and turning it into something", "tokens": [407, 264, 725, 335, 11970, 1399, 1355, 1940, 729, 14511, 295, 19793, 293, 6246, 309, 666, 746], "temperature": 0.0, "avg_logprob": -0.1620559397431993, "compression_ratio": 1.6832579185520362, "no_speech_prob": 2.1907760583417257e-06}, {"id": 460, "seek": 232404, "start": 2344.68, "end": 2350.48, "text": " where every step in the x-direction or the y-direction or the z-direction equals 1 mm", "tokens": [689, 633, 1823, 294, 264, 2031, 12, 18267, 882, 420, 264, 288, 12, 18267, 882, 420, 264, 710, 12, 18267, 882, 6915, 502, 11169], "temperature": 0.0, "avg_logprob": -0.1620559397431993, "compression_ratio": 1.6832579185520362, "no_speech_prob": 2.1907760583417257e-06}, {"id": 461, "seek": 232404, "start": 2350.48, "end": 2352.4, "text": " in the real world.", "tokens": [294, 264, 957, 1002, 13], "temperature": 0.0, "avg_logprob": -0.1620559397431993, "compression_ratio": 1.6832579185520362, "no_speech_prob": 2.1907760583417257e-06}, {"id": 462, "seek": 235240, "start": 2352.4, "end": 2356.84, "text": " And so it would be very annoying for your deep learning network if your different lung", "tokens": [400, 370, 309, 576, 312, 588, 11304, 337, 428, 2452, 2539, 3209, 498, 428, 819, 16730], "temperature": 0.0, "avg_logprob": -0.17083647084790607, "compression_ratio": 1.5753424657534247, "no_speech_prob": 4.860430180997355e-06}, {"id": 463, "seek": 235240, "start": 2356.84, "end": 2361.84, "text": " images were squished by different amounts, especially if you didn't give it the metadata", "tokens": [5267, 645, 2339, 4729, 538, 819, 11663, 11, 2318, 498, 291, 994, 380, 976, 309, 264, 26603], "temperature": 0.0, "avg_logprob": -0.17083647084790607, "compression_ratio": 1.5753424657534247, "no_speech_prob": 4.860430180997355e-06}, {"id": 464, "seek": 235240, "start": 2361.84, "end": 2365.6600000000003, "text": " about how much was being squished.", "tokens": [466, 577, 709, 390, 885, 2339, 4729, 13], "temperature": 0.0, "avg_logprob": -0.17083647084790607, "compression_ratio": 1.5753424657534247, "no_speech_prob": 4.860430180997355e-06}, {"id": 465, "seek": 235240, "start": 2365.6600000000003, "end": 2366.92, "text": " So that's what resampling does.", "tokens": [407, 300, 311, 437, 725, 335, 11970, 775, 13], "temperature": 0.0, "avg_logprob": -0.17083647084790607, "compression_ratio": 1.5753424657534247, "no_speech_prob": 4.860430180997355e-06}, {"id": 466, "seek": 235240, "start": 2366.92, "end": 2372.2000000000003, "text": " As you can see, it's using the slice thickness and the pixel spacing to make everything nice", "tokens": [1018, 291, 393, 536, 11, 309, 311, 1228, 264, 13153, 14855, 293, 264, 19261, 27739, 281, 652, 1203, 1481], "temperature": 0.0, "avg_logprob": -0.17083647084790607, "compression_ratio": 1.5753424657534247, "no_speech_prob": 4.860430180997355e-06}, {"id": 467, "seek": 235240, "start": 2372.2000000000003, "end": 2375.94, "text": " and even.", "tokens": [293, 754, 13], "temperature": 0.0, "avg_logprob": -0.17083647084790607, "compression_ratio": 1.5753424657534247, "no_speech_prob": 4.860430180997355e-06}, {"id": 468, "seek": 237594, "start": 2375.94, "end": 2385.76, "text": " So there are various ways to do 3D plots, and it's always a good idea to do that.", "tokens": [407, 456, 366, 3683, 2098, 281, 360, 805, 35, 28609, 11, 293, 309, 311, 1009, 257, 665, 1558, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.16024504557694538, "compression_ratio": 1.7083333333333333, "no_speech_prob": 1.1015905556632788e-06}, {"id": 469, "seek": 237594, "start": 2385.76, "end": 2389.2400000000002, "text": " And then something else that people tend to do is segmentation.", "tokens": [400, 550, 746, 1646, 300, 561, 3928, 281, 360, 307, 9469, 399, 13], "temperature": 0.0, "avg_logprob": -0.16024504557694538, "compression_ratio": 1.7083333333333333, "no_speech_prob": 1.1015905556632788e-06}, {"id": 470, "seek": 237594, "start": 2389.2400000000002, "end": 2395.4, "text": " Depending on time, we may or may not get around to looking more at segmentation in this part", "tokens": [22539, 322, 565, 11, 321, 815, 420, 815, 406, 483, 926, 281, 1237, 544, 412, 9469, 399, 294, 341, 644], "temperature": 0.0, "avg_logprob": -0.16024504557694538, "compression_ratio": 1.7083333333333333, "no_speech_prob": 1.1015905556632788e-06}, {"id": 471, "seek": 237594, "start": 2395.4, "end": 2399.36, "text": " of the course, but effectively segmentation is just another generative model.", "tokens": [295, 264, 1164, 11, 457, 8659, 9469, 399, 307, 445, 1071, 1337, 1166, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16024504557694538, "compression_ratio": 1.7083333333333333, "no_speech_prob": 1.1015905556632788e-06}, {"id": 472, "seek": 237594, "start": 2399.36, "end": 2405.32, "text": " It's a generative model where hopefully somebody's given you some things saying this is lung,", "tokens": [467, 311, 257, 1337, 1166, 2316, 689, 4696, 2618, 311, 2212, 291, 512, 721, 1566, 341, 307, 16730, 11], "temperature": 0.0, "avg_logprob": -0.16024504557694538, "compression_ratio": 1.7083333333333333, "no_speech_prob": 1.1015905556632788e-06}, {"id": 473, "seek": 240532, "start": 2405.32, "end": 2410.28, "text": " this is air, and then you build a model that tries to predict for something else what's", "tokens": [341, 307, 1988, 11, 293, 550, 291, 1322, 257, 2316, 300, 9898, 281, 6069, 337, 746, 1646, 437, 311], "temperature": 0.0, "avg_logprob": -0.17187658611096834, "compression_ratio": 1.65, "no_speech_prob": 4.029402589367237e-06}, {"id": 474, "seek": 240532, "start": 2410.28, "end": 2413.36, "text": " lung and what's air.", "tokens": [16730, 293, 437, 311, 1988, 13], "temperature": 0.0, "avg_logprob": -0.17187658611096834, "compression_ratio": 1.65, "no_speech_prob": 4.029402589367237e-06}, {"id": 475, "seek": 240532, "start": 2413.36, "end": 2424.2400000000002, "text": " Unfortunately for lung CT scans, we don't generally have that kind of ground truth of", "tokens": [8590, 337, 16730, 19529, 35116, 11, 321, 500, 380, 5101, 362, 300, 733, 295, 2727, 3494, 295], "temperature": 0.0, "avg_logprob": -0.17187658611096834, "compression_ratio": 1.65, "no_speech_prob": 4.029402589367237e-06}, {"id": 476, "seek": 240532, "start": 2424.2400000000002, "end": 2426.0, "text": " which bit's lung and which bit's air.", "tokens": [597, 857, 311, 16730, 293, 597, 857, 311, 1988, 13], "temperature": 0.0, "avg_logprob": -0.17187658611096834, "compression_ratio": 1.65, "no_speech_prob": 4.029402589367237e-06}, {"id": 477, "seek": 240532, "start": 2426.0, "end": 2431.1600000000003, "text": " So generally speaking in medical imaging, people use a whole lot of heuristic approaches,", "tokens": [407, 5101, 4124, 294, 4625, 25036, 11, 561, 764, 257, 1379, 688, 295, 415, 374, 3142, 11587, 11], "temperature": 0.0, "avg_logprob": -0.17187658611096834, "compression_ratio": 1.65, "no_speech_prob": 4.029402589367237e-06}, {"id": 478, "seek": 240532, "start": 2431.1600000000003, "end": 2434.6000000000004, "text": " so kind of hacky, rule-based approaches.", "tokens": [370, 733, 295, 10339, 88, 11, 4978, 12, 6032, 11587, 13], "temperature": 0.0, "avg_logprob": -0.17187658611096834, "compression_ratio": 1.65, "no_speech_prob": 4.029402589367237e-06}, {"id": 479, "seek": 243460, "start": 2434.6, "end": 2441.88, "text": " And in particular, applications of region-growing and morphological operations.", "tokens": [400, 294, 1729, 11, 5821, 295, 4458, 12, 861, 9637, 293, 25778, 4383, 7705, 13], "temperature": 0.0, "avg_logprob": -0.15668587935598274, "compression_ratio": 1.6144067796610169, "no_speech_prob": 1.0451386515342165e-05}, {"id": 480, "seek": 243460, "start": 2441.88, "end": 2446.6, "text": " I find this kind of the boring part of medical imaging because it's so clearly a dumb way", "tokens": [286, 915, 341, 733, 295, 264, 9989, 644, 295, 4625, 25036, 570, 309, 311, 370, 4448, 257, 10316, 636], "temperature": 0.0, "avg_logprob": -0.15668587935598274, "compression_ratio": 1.6144067796610169, "no_speech_prob": 1.0451386515342165e-05}, {"id": 481, "seek": 243460, "start": 2446.6, "end": 2448.6, "text": " to do things.", "tokens": [281, 360, 721, 13], "temperature": 0.0, "avg_logprob": -0.15668587935598274, "compression_ratio": 1.6144067796610169, "no_speech_prob": 1.0451386515342165e-05}, {"id": 482, "seek": 243460, "start": 2448.6, "end": 2454.72, "text": " But deep learning is far too new in this area yet to develop the data sets that we need", "tokens": [583, 2452, 2539, 307, 1400, 886, 777, 294, 341, 1859, 1939, 281, 1499, 264, 1412, 6352, 300, 321, 643], "temperature": 0.0, "avg_logprob": -0.15668587935598274, "compression_ratio": 1.6144067796610169, "no_speech_prob": 1.0451386515342165e-05}, {"id": 483, "seek": 243460, "start": 2454.72, "end": 2457.68, "text": " to do this properly.", "tokens": [281, 360, 341, 6108, 13], "temperature": 0.0, "avg_logprob": -0.15668587935598274, "compression_ratio": 1.6144067796610169, "no_speech_prob": 1.0451386515342165e-05}, {"id": 484, "seek": 243460, "start": 2457.68, "end": 2464.0, "text": " But the good news is that there's a button which I don't think many people notice exists", "tokens": [583, 264, 665, 2583, 307, 300, 456, 311, 257, 2960, 597, 286, 500, 380, 519, 867, 561, 3449, 8198], "temperature": 0.0, "avg_logprob": -0.15668587935598274, "compression_ratio": 1.6144067796610169, "no_speech_prob": 1.0451386515342165e-05}, {"id": 485, "seek": 246400, "start": 2464.0, "end": 2471.28, "text": " called tutorial on the main data science page where these folks from Booz Allen Hamilton", "tokens": [1219, 7073, 322, 264, 2135, 1412, 3497, 3028, 689, 613, 4024, 490, 23351, 89, 17160, 18484], "temperature": 0.0, "avg_logprob": -0.20963603258132935, "compression_ratio": 1.592885375494071, "no_speech_prob": 4.539693327387795e-05}, {"id": 486, "seek": 246400, "start": 2471.28, "end": 2474.56, "text": " actually show you a complete segmentation approach.", "tokens": [767, 855, 291, 257, 3566, 9469, 399, 3109, 13], "temperature": 0.0, "avg_logprob": -0.20963603258132935, "compression_ratio": 1.592885375494071, "no_speech_prob": 4.539693327387795e-05}, {"id": 487, "seek": 246400, "start": 2474.56, "end": 2479.24, "text": " Now it's interesting that they picked UNET segmentation.", "tokens": [823, 309, 311, 1880, 300, 436, 6183, 8229, 4850, 9469, 399, 13], "temperature": 0.0, "avg_logprob": -0.20963603258132935, "compression_ratio": 1.592885375494071, "no_speech_prob": 4.539693327387795e-05}, {"id": 488, "seek": 246400, "start": 2479.24, "end": 2485.52, "text": " This is definitely the thing about segmentation I would be teaching you guys if we have time.", "tokens": [639, 307, 2138, 264, 551, 466, 9469, 399, 286, 576, 312, 4571, 291, 1074, 498, 321, 362, 565, 13], "temperature": 0.0, "avg_logprob": -0.20963603258132935, "compression_ratio": 1.592885375494071, "no_speech_prob": 4.539693327387795e-05}, {"id": 489, "seek": 246400, "start": 2485.52, "end": 2488.8, "text": " UNET is one of these things that outside of the Kaggle world, I don't think that many", "tokens": [8229, 4850, 307, 472, 295, 613, 721, 300, 2380, 295, 264, 48751, 22631, 1002, 11, 286, 500, 380, 519, 300, 867], "temperature": 0.0, "avg_logprob": -0.20963603258132935, "compression_ratio": 1.592885375494071, "no_speech_prob": 4.539693327387795e-05}, {"id": 490, "seek": 246400, "start": 2488.8, "end": 2491.4, "text": " people are familiar with.", "tokens": [561, 366, 4963, 365, 13], "temperature": 0.0, "avg_logprob": -0.20963603258132935, "compression_ratio": 1.592885375494071, "no_speech_prob": 4.539693327387795e-05}, {"id": 491, "seek": 249140, "start": 2491.4, "end": 2497.8, "text": " But inside the Kaggle world, we know that anytime segmentation crops up, UNET wins and", "tokens": [583, 1854, 264, 48751, 22631, 1002, 11, 321, 458, 300, 13038, 9469, 399, 16829, 493, 11, 8229, 4850, 10641, 293], "temperature": 0.0, "avg_logprob": -0.158841350410558, "compression_ratio": 1.5, "no_speech_prob": 1.451022399123758e-05}, {"id": 492, "seek": 249140, "start": 2497.8, "end": 2499.1600000000003, "text": " it's the best.", "tokens": [309, 311, 264, 1151, 13], "temperature": 0.0, "avg_logprob": -0.158841350410558, "compression_ratio": 1.5, "no_speech_prob": 1.451022399123758e-05}, {"id": 493, "seek": 249140, "start": 2499.1600000000003, "end": 2505.52, "text": " More recently there's actually been something called DenseNet for segmentation, which takes", "tokens": [5048, 3938, 456, 311, 767, 668, 746, 1219, 413, 1288, 31890, 337, 9469, 399, 11, 597, 2516], "temperature": 0.0, "avg_logprob": -0.158841350410558, "compression_ratio": 1.5, "no_speech_prob": 1.451022399123758e-05}, {"id": 494, "seek": 249140, "start": 2505.52, "end": 2509.7200000000003, "text": " UNET even a little bit further, and maybe that would be the new winner for newer Kaggle", "tokens": [8229, 4850, 754, 257, 707, 857, 3052, 11, 293, 1310, 300, 576, 312, 264, 777, 8507, 337, 17628, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.158841350410558, "compression_ratio": 1.5, "no_speech_prob": 1.451022399123758e-05}, {"id": 495, "seek": 249140, "start": 2509.7200000000003, "end": 2512.44, "text": " competitions when they happen.", "tokens": [26185, 562, 436, 1051, 13], "temperature": 0.0, "avg_logprob": -0.158841350410558, "compression_ratio": 1.5, "no_speech_prob": 1.451022399123758e-05}, {"id": 496, "seek": 251244, "start": 2512.44, "end": 2530.04, "text": " But the basic idea here of things like UNET and DenseNet is that we have a model where", "tokens": [583, 264, 3875, 1558, 510, 295, 721, 411, 8229, 4850, 293, 413, 1288, 31890, 307, 300, 321, 362, 257, 2316, 689], "temperature": 0.0, "avg_logprob": -0.16575774208444063, "compression_ratio": 1.473053892215569, "no_speech_prob": 8.801020157989115e-06}, {"id": 497, "seek": 251244, "start": 2530.04, "end": 2535.04, "text": " when we do generative models, we think about doing style transfer.", "tokens": [562, 321, 360, 1337, 1166, 5245, 11, 321, 519, 466, 884, 3758, 5003, 13], "temperature": 0.0, "avg_logprob": -0.16575774208444063, "compression_ratio": 1.473053892215569, "no_speech_prob": 8.801020157989115e-06}, {"id": 498, "seek": 251244, "start": 2535.04, "end": 2540.46, "text": " We generally start with this kind of large image and then we do some downsampling operations", "tokens": [492, 5101, 722, 365, 341, 733, 295, 2416, 3256, 293, 550, 321, 360, 512, 760, 19988, 11970, 7705], "temperature": 0.0, "avg_logprob": -0.16575774208444063, "compression_ratio": 1.473053892215569, "no_speech_prob": 8.801020157989115e-06}, {"id": 499, "seek": 254046, "start": 2540.46, "end": 2546.7, "text": " to make it a smaller image, and then we do some computation and then we make it bigger", "tokens": [281, 652, 309, 257, 4356, 3256, 11, 293, 550, 321, 360, 512, 24903, 293, 550, 321, 652, 309, 3801], "temperature": 0.0, "avg_logprob": -0.1383495050318101, "compression_ratio": 1.4509803921568627, "no_speech_prob": 5.173861609364394e-06}, {"id": 500, "seek": 254046, "start": 2546.7, "end": 2552.96, "text": " again with these upsampling operations.", "tokens": [797, 365, 613, 493, 19988, 11970, 7705, 13], "temperature": 0.0, "avg_logprob": -0.1383495050318101, "compression_ratio": 1.4509803921568627, "no_speech_prob": 5.173861609364394e-06}, {"id": 501, "seek": 254046, "start": 2552.96, "end": 2561.88, "text": " What happens in UNET is that there are additional neural network connections made directly from", "tokens": [708, 2314, 294, 8229, 4850, 307, 300, 456, 366, 4497, 18161, 3209, 9271, 1027, 3838, 490], "temperature": 0.0, "avg_logprob": -0.1383495050318101, "compression_ratio": 1.4509803921568627, "no_speech_prob": 5.173861609364394e-06}, {"id": 502, "seek": 256188, "start": 2561.88, "end": 2572.1600000000003, "text": " here to here, and directly from here to here, and here to here, and here to here.", "tokens": [510, 281, 510, 11, 293, 3838, 490, 510, 281, 510, 11, 293, 510, 281, 510, 11, 293, 510, 281, 510, 13], "temperature": 0.0, "avg_logprob": -0.18412550189827062, "compression_ratio": 1.6855670103092784, "no_speech_prob": 4.092887593287742e-06}, {"id": 503, "seek": 256188, "start": 2572.1600000000003, "end": 2580.44, "text": " Those connections basically allow it to almost do like a residual learning approach.", "tokens": [3950, 9271, 1936, 2089, 309, 281, 1920, 360, 411, 257, 27980, 2539, 3109, 13], "temperature": 0.0, "avg_logprob": -0.18412550189827062, "compression_ratio": 1.6855670103092784, "no_speech_prob": 4.092887593287742e-06}, {"id": 504, "seek": 256188, "start": 2580.44, "end": 2586.76, "text": " It can figure out the key semantic pieces at a really low resolution.", "tokens": [467, 393, 2573, 484, 264, 2141, 47982, 3755, 412, 257, 534, 2295, 8669, 13], "temperature": 0.0, "avg_logprob": -0.18412550189827062, "compression_ratio": 1.6855670103092784, "no_speech_prob": 4.092887593287742e-06}, {"id": 505, "seek": 256188, "start": 2586.76, "end": 2591.38, "text": " But then as it upscales it, it can learn what was special about the difference between the", "tokens": [583, 550, 382, 309, 493, 4417, 4229, 309, 11, 309, 393, 1466, 437, 390, 2121, 466, 264, 2649, 1296, 264], "temperature": 0.0, "avg_logprob": -0.18412550189827062, "compression_ratio": 1.6855670103092784, "no_speech_prob": 4.092887593287742e-06}, {"id": 506, "seek": 259138, "start": 2591.38, "end": 2593.48, "text": " downsampled image and the original image here.", "tokens": [760, 19988, 15551, 3256, 293, 264, 3380, 3256, 510, 13], "temperature": 0.0, "avg_logprob": -0.22526530063513553, "compression_ratio": 1.436046511627907, "no_speech_prob": 1.9833063561236486e-05}, {"id": 507, "seek": 259138, "start": 2593.48, "end": 2600.12, "text": " It can kind of learn to add that additional detail at each point.", "tokens": [467, 393, 733, 295, 1466, 281, 909, 300, 4497, 2607, 412, 1184, 935, 13], "temperature": 0.0, "avg_logprob": -0.22526530063513553, "compression_ratio": 1.436046511627907, "no_speech_prob": 1.9833063561236486e-05}, {"id": 508, "seek": 259138, "start": 2600.12, "end": 2613.2000000000003, "text": " So UNET and DenseNet for segmentation are really interesting.", "tokens": [407, 8229, 4850, 293, 413, 1288, 31890, 337, 9469, 399, 366, 534, 1880, 13], "temperature": 0.0, "avg_logprob": -0.22526530063513553, "compression_ratio": 1.436046511627907, "no_speech_prob": 1.9833063561236486e-05}, {"id": 509, "seek": 259138, "start": 2613.2000000000003, "end": 2617.2400000000002, "text": " I hope we find some time to get back to them in this part of the course.", "tokens": [286, 1454, 321, 915, 512, 565, 281, 483, 646, 281, 552, 294, 341, 644, 295, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.22526530063513553, "compression_ratio": 1.436046511627907, "no_speech_prob": 1.9833063561236486e-05}, {"id": 510, "seek": 261724, "start": 2617.24, "end": 2625.64, "text": " But if we don't, you can get started by looking at this tutorial in which these folks basically", "tokens": [583, 498, 321, 500, 380, 11, 291, 393, 483, 1409, 538, 1237, 412, 341, 7073, 294, 597, 613, 4024, 1936], "temperature": 0.0, "avg_logprob": -0.11934233771430122, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.5936415366013534e-05}, {"id": 511, "seek": 261724, "start": 2625.64, "end": 2627.6, "text": " show you from scratch.", "tokens": [855, 291, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.11934233771430122, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.5936415366013534e-05}, {"id": 512, "seek": 261724, "start": 2627.6, "end": 2634.56, "text": " What they try to do in this tutorial is something very specific, which is the detection part.", "tokens": [708, 436, 853, 281, 360, 294, 341, 7073, 307, 746, 588, 2685, 11, 597, 307, 264, 17784, 644, 13], "temperature": 0.0, "avg_logprob": -0.11934233771430122, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.5936415366013534e-05}, {"id": 513, "seek": 261724, "start": 2634.56, "end": 2642.2799999999997, "text": " So what happens in this kind of, think about the fisheries competition.", "tokens": [407, 437, 2314, 294, 341, 733, 295, 11, 519, 466, 264, 20698, 530, 6211, 13], "temperature": 0.0, "avg_logprob": -0.11934233771430122, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.5936415366013534e-05}, {"id": 514, "seek": 261724, "start": 2642.2799999999997, "end": 2646.04, "text": " We pretty much decided that in the fisheries competition, if you wanted to do really well,", "tokens": [492, 1238, 709, 3047, 300, 294, 264, 20698, 530, 6211, 11, 498, 291, 1415, 281, 360, 534, 731, 11], "temperature": 0.0, "avg_logprob": -0.11934233771430122, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.5936415366013534e-05}, {"id": 515, "seek": 264604, "start": 2646.04, "end": 2649.84, "text": " you would first of all find the fish, and then you would zoom into the fish, and then", "tokens": [291, 576, 700, 295, 439, 915, 264, 3506, 11, 293, 550, 291, 576, 8863, 666, 264, 3506, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.1544310139343802, "compression_ratio": 1.8508064516129032, "no_speech_prob": 1.618744499864988e-05}, {"id": 516, "seek": 264604, "start": 2649.84, "end": 2652.88, "text": " you would figure out what kind of fish it is.", "tokens": [291, 576, 2573, 484, 437, 733, 295, 3506, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1544310139343802, "compression_ratio": 1.8508064516129032, "no_speech_prob": 1.618744499864988e-05}, {"id": 517, "seek": 264604, "start": 2652.88, "end": 2656.6, "text": " Certainly in the right whale competition earlier, that was how that was planned.", "tokens": [16628, 294, 264, 558, 25370, 6211, 3071, 11, 300, 390, 577, 300, 390, 8589, 13], "temperature": 0.0, "avg_logprob": -0.1544310139343802, "compression_ratio": 1.8508064516129032, "no_speech_prob": 1.618744499864988e-05}, {"id": 518, "seek": 264604, "start": 2656.6, "end": 2661.44, "text": " For this competition, this is even more clearly going to be the approach, because these images", "tokens": [1171, 341, 6211, 11, 341, 307, 754, 544, 4448, 516, 281, 312, 264, 3109, 11, 570, 613, 5267], "temperature": 0.0, "avg_logprob": -0.1544310139343802, "compression_ratio": 1.8508064516129032, "no_speech_prob": 1.618744499864988e-05}, {"id": 519, "seek": 264604, "start": 2661.44, "end": 2664.72, "text": " are just far too big to do a normal convolutional neural network.", "tokens": [366, 445, 1400, 886, 955, 281, 360, 257, 2710, 45216, 304, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.1544310139343802, "compression_ratio": 1.8508064516129032, "no_speech_prob": 1.618744499864988e-05}, {"id": 520, "seek": 264604, "start": 2664.72, "end": 2669.16, "text": " We need one step that's going to find the nodule, and then a second step that's going", "tokens": [492, 643, 472, 1823, 300, 311, 516, 281, 915, 264, 15224, 2271, 11, 293, 550, 257, 1150, 1823, 300, 311, 516], "temperature": 0.0, "avg_logprob": -0.1544310139343802, "compression_ratio": 1.8508064516129032, "no_speech_prob": 1.618744499864988e-05}, {"id": 521, "seek": 266916, "start": 2669.16, "end": 2677.24, "text": " to zoom into a possible nodule and figure out is this a malignant tumor or something", "tokens": [281, 8863, 666, 257, 1944, 15224, 2271, 293, 2573, 484, 307, 341, 257, 2806, 36818, 22512, 420, 746], "temperature": 0.0, "avg_logprob": -0.20328838641826924, "compression_ratio": 1.5088757396449703, "no_speech_prob": 4.029443971376168e-06}, {"id": 522, "seek": 266916, "start": 2677.24, "end": 2682.3199999999997, "text": " else, a false positive.", "tokens": [1646, 11, 257, 7908, 3353, 13], "temperature": 0.0, "avg_logprob": -0.20328838641826924, "compression_ratio": 1.5088757396449703, "no_speech_prob": 4.029443971376168e-06}, {"id": 523, "seek": 266916, "start": 2682.3199999999997, "end": 2688.2799999999997, "text": " The bad news is that the data science bold dataset does not give you any information", "tokens": [440, 1578, 2583, 307, 300, 264, 1412, 3497, 11928, 28872, 775, 406, 976, 291, 604, 1589], "temperature": 0.0, "avg_logprob": -0.20328838641826924, "compression_ratio": 1.5088757396449703, "no_speech_prob": 4.029443971376168e-06}, {"id": 524, "seek": 266916, "start": 2688.2799999999997, "end": 2696.0, "text": " at all for the training set, where are the cancerous nodules.", "tokens": [412, 439, 337, 264, 3097, 992, 11, 689, 366, 264, 5592, 563, 15224, 3473, 13], "temperature": 0.0, "avg_logprob": -0.20328838641826924, "compression_ratio": 1.5088757396449703, "no_speech_prob": 4.029443971376168e-06}, {"id": 525, "seek": 269600, "start": 2696.0, "end": 2699.0, "text": " Which I actually wrote a post in the Kaggle forums about this, I just think this is a", "tokens": [3013, 286, 767, 4114, 257, 2183, 294, 264, 48751, 22631, 26998, 466, 341, 11, 286, 445, 519, 341, 307, 257], "temperature": 0.0, "avg_logprob": -0.19433345972934615, "compression_ratio": 1.717741935483871, "no_speech_prob": 1.7502732589491643e-05}, {"id": 526, "seek": 269600, "start": 2699.0, "end": 2703.8, "text": " terrible, terrible idea.", "tokens": [6237, 11, 6237, 1558, 13], "temperature": 0.0, "avg_logprob": -0.19433345972934615, "compression_ratio": 1.717741935483871, "no_speech_prob": 1.7502732589491643e-05}, {"id": 527, "seek": 269600, "start": 2703.8, "end": 2705.6, "text": " That information actually exists.", "tokens": [663, 1589, 767, 8198, 13], "temperature": 0.0, "avg_logprob": -0.19433345972934615, "compression_ratio": 1.717741935483871, "no_speech_prob": 1.7502732589491643e-05}, {"id": 528, "seek": 269600, "start": 2705.6, "end": 2709.32, "text": " The dataset they got this from is something called the National Lung Screening Trial,", "tokens": [440, 28872, 436, 658, 341, 490, 307, 746, 1219, 264, 4862, 441, 1063, 25823, 278, 314, 7111, 11], "temperature": 0.0, "avg_logprob": -0.19433345972934615, "compression_ratio": 1.717741935483871, "no_speech_prob": 1.7502732589491643e-05}, {"id": 529, "seek": 269600, "start": 2709.32, "end": 2713.0, "text": " which actually has that information, or something close to it.", "tokens": [597, 767, 575, 300, 1589, 11, 420, 746, 1998, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.19433345972934615, "compression_ratio": 1.717741935483871, "no_speech_prob": 1.7502732589491643e-05}, {"id": 530, "seek": 269600, "start": 2713.0, "end": 2718.76, "text": " So the fact they didn't provide it, I just think it's horrible for a competition which", "tokens": [407, 264, 1186, 436, 994, 380, 2893, 309, 11, 286, 445, 519, 309, 311, 9263, 337, 257, 6211, 597], "temperature": 0.0, "avg_logprob": -0.19433345972934615, "compression_ratio": 1.717741935483871, "no_speech_prob": 1.7502732589491643e-05}, {"id": 531, "seek": 269600, "start": 2718.76, "end": 2723.32, "text": " can save lives, and I can't begin to imagine.", "tokens": [393, 3155, 2909, 11, 293, 286, 393, 380, 1841, 281, 3811, 13], "temperature": 0.0, "avg_logprob": -0.19433345972934615, "compression_ratio": 1.717741935483871, "no_speech_prob": 1.7502732589491643e-05}, {"id": 532, "seek": 272332, "start": 2723.32, "end": 2729.76, "text": " The good news though is that there is a dataset which does have this information.", "tokens": [440, 665, 2583, 1673, 307, 300, 456, 307, 257, 28872, 597, 775, 362, 341, 1589, 13], "temperature": 0.0, "avg_logprob": -0.23230208879635658, "compression_ratio": 1.6559633027522935, "no_speech_prob": 6.643368578806985e-06}, {"id": 533, "seek": 272332, "start": 2729.76, "end": 2737.92, "text": " The original dataset was called LIDC-IDRI, but interestingly that dataset was recently", "tokens": [440, 3380, 28872, 390, 1219, 441, 2777, 34, 12, 2777, 5577, 11, 457, 25873, 300, 28872, 390, 3938], "temperature": 0.0, "avg_logprob": -0.23230208879635658, "compression_ratio": 1.6559633027522935, "no_speech_prob": 6.643368578806985e-06}, {"id": 534, "seek": 272332, "start": 2737.92, "end": 2743.7200000000003, "text": " used for another competition, a non-Kaggle competition called Luna.", "tokens": [1143, 337, 1071, 6211, 11, 257, 2107, 12, 42, 559, 22631, 6211, 1219, 27355, 13], "temperature": 0.0, "avg_logprob": -0.23230208879635658, "compression_ratio": 1.6559633027522935, "no_speech_prob": 6.643368578806985e-06}, {"id": 535, "seek": 272332, "start": 2743.7200000000003, "end": 2746.32, "text": " That competition is now finished.", "tokens": [663, 6211, 307, 586, 4335, 13], "temperature": 0.0, "avg_logprob": -0.23230208879635658, "compression_ratio": 1.6559633027522935, "no_speech_prob": 6.643368578806985e-06}, {"id": 536, "seek": 272332, "start": 2746.32, "end": 2751.28, "text": " One of the tracks in that competition was actually specifically a false positive detection", "tokens": [1485, 295, 264, 10218, 294, 300, 6211, 390, 767, 4682, 257, 7908, 3353, 17784], "temperature": 0.0, "avg_logprob": -0.23230208879635658, "compression_ratio": 1.6559633027522935, "no_speech_prob": 6.643368578806985e-06}, {"id": 537, "seek": 275128, "start": 2751.28, "end": 2757.44, "text": " track and then the other track was a find-the-nodule track.", "tokens": [2837, 293, 550, 264, 661, 2837, 390, 257, 915, 12, 3322, 12, 77, 378, 2271, 2837, 13], "temperature": 0.0, "avg_logprob": -0.19426500229608445, "compression_ratio": 1.6653386454183268, "no_speech_prob": 1.5689340216340497e-05}, {"id": 538, "seek": 275128, "start": 2757.44, "end": 2762.32, "text": " So you can actually go back and look at the papers written by the winners.", "tokens": [407, 291, 393, 767, 352, 646, 293, 574, 412, 264, 10577, 3720, 538, 264, 17193, 13], "temperature": 0.0, "avg_logprob": -0.19426500229608445, "compression_ratio": 1.6653386454183268, "no_speech_prob": 1.5689340216340497e-05}, {"id": 539, "seek": 275128, "start": 2762.32, "end": 2764.5600000000004, "text": " They're generally ridiculously short.", "tokens": [814, 434, 5101, 41358, 2099, 13], "temperature": 0.0, "avg_logprob": -0.19426500229608445, "compression_ratio": 1.6653386454183268, "no_speech_prob": 1.5689340216340497e-05}, {"id": 540, "seek": 275128, "start": 2764.5600000000004, "end": 2768.92, "text": " Many of them are a single sentence saying for commercial confidentiality agreement,", "tokens": [5126, 295, 552, 366, 257, 2167, 8174, 1566, 337, 6841, 27054, 507, 8106, 11], "temperature": 0.0, "avg_logprob": -0.19426500229608445, "compression_ratio": 1.6653386454183268, "no_speech_prob": 1.5689340216340497e-05}, {"id": 541, "seek": 275128, "start": 2768.92, "end": 2770.48, "text": " we can't do anything.", "tokens": [321, 393, 380, 360, 1340, 13], "temperature": 0.0, "avg_logprob": -0.19426500229608445, "compression_ratio": 1.6653386454183268, "no_speech_prob": 1.5689340216340497e-05}, {"id": 542, "seek": 275128, "start": 2770.48, "end": 2776.2400000000002, "text": " But some of them, including the winner of the false positive track, they actually provide", "tokens": [583, 512, 295, 552, 11, 3009, 264, 8507, 295, 264, 7908, 3353, 2837, 11, 436, 767, 2893], "temperature": 0.0, "avg_logprob": -0.19426500229608445, "compression_ratio": 1.6653386454183268, "no_speech_prob": 1.5689340216340497e-05}, {"id": 543, "seek": 275128, "start": 2776.2400000000002, "end": 2777.2400000000002, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.19426500229608445, "compression_ratio": 1.6653386454183268, "no_speech_prob": 1.5689340216340497e-05}, {"id": 544, "seek": 275128, "start": 2777.2400000000002, "end": 2781.1600000000003, "text": " Not surprisingly, they all use deep learning.", "tokens": [1726, 17600, 11, 436, 439, 764, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.19426500229608445, "compression_ratio": 1.6653386454183268, "no_speech_prob": 1.5689340216340497e-05}, {"id": 545, "seek": 278116, "start": 2781.16, "end": 2784.8399999999997, "text": " And so what you could do, in fact I think what you have to do to do well in this competition,", "tokens": [400, 370, 437, 291, 727, 360, 11, 294, 1186, 286, 519, 437, 291, 362, 281, 360, 281, 360, 731, 294, 341, 6211, 11], "temperature": 0.0, "avg_logprob": -0.16524183080437477, "compression_ratio": 1.8219895287958114, "no_speech_prob": 1.18429925350938e-05}, {"id": 546, "seek": 278116, "start": 2784.8399999999997, "end": 2790.72, "text": " is download the Luna dataset, use that to build a nodule detection algorithm.", "tokens": [307, 5484, 264, 27355, 28872, 11, 764, 300, 281, 1322, 257, 15224, 2271, 17784, 9284, 13], "temperature": 0.0, "avg_logprob": -0.16524183080437477, "compression_ratio": 1.8219895287958114, "no_speech_prob": 1.18429925350938e-05}, {"id": 547, "seek": 278116, "start": 2790.72, "end": 2797.12, "text": " So the Luna dataset includes files saying this lung has nodules here, here, here and", "tokens": [407, 264, 27355, 28872, 5974, 7098, 1566, 341, 16730, 575, 15224, 3473, 510, 11, 510, 11, 510, 293], "temperature": 0.0, "avg_logprob": -0.16524183080437477, "compression_ratio": 1.8219895287958114, "no_speech_prob": 1.18429925350938e-05}, {"id": 548, "seek": 278116, "start": 2797.12, "end": 2798.12, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.16524183080437477, "compression_ratio": 1.8219895287958114, "no_speech_prob": 1.18429925350938e-05}, {"id": 549, "seek": 278116, "start": 2798.12, "end": 2803.3199999999997, "text": " So do nodule detection based on that, and then run that nodule detection algorithm on", "tokens": [407, 360, 15224, 2271, 17784, 2361, 322, 300, 11, 293, 550, 1190, 300, 15224, 2271, 17784, 9284, 322], "temperature": 0.0, "avg_logprob": -0.16524183080437477, "compression_ratio": 1.8219895287958114, "no_speech_prob": 1.18429925350938e-05}, {"id": 550, "seek": 280332, "start": 2803.32, "end": 2812.6400000000003, "text": " the Kaggle dataset, find the nodules and then use that to do some classification.", "tokens": [264, 48751, 22631, 28872, 11, 915, 264, 15224, 3473, 293, 550, 764, 300, 281, 360, 512, 21538, 13], "temperature": 0.0, "avg_logprob": -0.133297402283241, "compression_ratio": 1.519736842105263, "no_speech_prob": 9.9728358691209e-06}, {"id": 551, "seek": 280332, "start": 2812.6400000000003, "end": 2815.4, "text": " There are some tricky things with that.", "tokens": [821, 366, 512, 12414, 721, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.133297402283241, "compression_ratio": 1.519736842105263, "no_speech_prob": 9.9728358691209e-06}, {"id": 552, "seek": 280332, "start": 2815.4, "end": 2824.8, "text": " The biggest tricky thing is that most of the CT scans in the Luna dataset are what's called", "tokens": [440, 3880, 12414, 551, 307, 300, 881, 295, 264, 19529, 35116, 294, 264, 27355, 28872, 366, 437, 311, 1219], "temperature": 0.0, "avg_logprob": -0.133297402283241, "compression_ratio": 1.519736842105263, "no_speech_prob": 9.9728358691209e-06}, {"id": 553, "seek": 280332, "start": 2824.8, "end": 2827.48, "text": " contrast studies.", "tokens": [8712, 5313, 13], "temperature": 0.0, "avg_logprob": -0.133297402283241, "compression_ratio": 1.519736842105263, "no_speech_prob": 9.9728358691209e-06}, {"id": 554, "seek": 282748, "start": 2827.48, "end": 2835.32, "text": " A contrast scan means that the patient had a radioactive dye injected into them so that", "tokens": [316, 8712, 11049, 1355, 300, 264, 4537, 632, 257, 35844, 20179, 36967, 666, 552, 370, 300], "temperature": 0.0, "avg_logprob": -0.13273983868685635, "compression_ratio": 1.6425855513307985, "no_speech_prob": 3.138126430712873e-06}, {"id": 555, "seek": 282748, "start": 2835.32, "end": 2841.56, "text": " the things that they're looking for are easier to see.", "tokens": [264, 721, 300, 436, 434, 1237, 337, 366, 3571, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.13273983868685635, "compression_ratio": 1.6425855513307985, "no_speech_prob": 3.138126430712873e-06}, {"id": 556, "seek": 282748, "start": 2841.56, "end": 2845.28, "text": " For the National Lung Screening Trial, which is what they use in the Kaggle dataset, none", "tokens": [1171, 264, 4862, 441, 1063, 25823, 278, 314, 7111, 11, 597, 307, 437, 436, 764, 294, 264, 48751, 22631, 28872, 11, 6022], "temperature": 0.0, "avg_logprob": -0.13273983868685635, "compression_ratio": 1.6425855513307985, "no_speech_prob": 3.138126430712873e-06}, {"id": 557, "seek": 282748, "start": 2845.28, "end": 2847.44, "text": " of them use contrast.", "tokens": [295, 552, 764, 8712, 13], "temperature": 0.0, "avg_logprob": -0.13273983868685635, "compression_ratio": 1.6425855513307985, "no_speech_prob": 3.138126430712873e-06}, {"id": 558, "seek": 282748, "start": 2847.44, "end": 2851.88, "text": " And the reason why is that what we really want to be able to do is to take anybody who's", "tokens": [400, 264, 1778, 983, 307, 300, 437, 321, 534, 528, 281, 312, 1075, 281, 360, 307, 281, 747, 4472, 567, 311], "temperature": 0.0, "avg_logprob": -0.13273983868685635, "compression_ratio": 1.6425855513307985, "no_speech_prob": 3.138126430712873e-06}, {"id": 559, "seek": 282748, "start": 2851.88, "end": 2856.44, "text": " like over 65 and has been smoking more than a pack a day for more than 20 years and give", "tokens": [411, 670, 11624, 293, 575, 668, 14055, 544, 813, 257, 2844, 257, 786, 337, 544, 813, 945, 924, 293, 976], "temperature": 0.0, "avg_logprob": -0.13273983868685635, "compression_ratio": 1.6425855513307985, "no_speech_prob": 3.138126430712873e-06}, {"id": 560, "seek": 285644, "start": 2856.44, "end": 2859.36, "text": " them all a CT scan and find out which ones have cancer.", "tokens": [552, 439, 257, 19529, 11049, 293, 915, 484, 597, 2306, 362, 5592, 13], "temperature": 0.0, "avg_logprob": -0.13460752367973328, "compression_ratio": 1.6740088105726871, "no_speech_prob": 4.710888788395096e-06}, {"id": 561, "seek": 285644, "start": 2859.36, "end": 2864.4, "text": " But in the process, we don't want to be shooting them up with radioactive dye and giving them", "tokens": [583, 294, 264, 1399, 11, 321, 500, 380, 528, 281, 312, 5942, 552, 493, 365, 35844, 20179, 293, 2902, 552], "temperature": 0.0, "avg_logprob": -0.13460752367973328, "compression_ratio": 1.6740088105726871, "no_speech_prob": 4.710888788395096e-06}, {"id": 562, "seek": 285644, "start": 2864.4, "end": 2865.4, "text": " cancer.", "tokens": [5592, 13], "temperature": 0.0, "avg_logprob": -0.13460752367973328, "compression_ratio": 1.6740088105726871, "no_speech_prob": 4.710888788395096e-06}, {"id": 563, "seek": 285644, "start": 2865.4, "end": 2872.6, "text": " So that's why we try to make sure that when we're doing these kind of asymptomatic scans", "tokens": [407, 300, 311, 983, 321, 853, 281, 652, 988, 300, 562, 321, 434, 884, 613, 733, 295, 35114, 13143, 35116], "temperature": 0.0, "avg_logprob": -0.13460752367973328, "compression_ratio": 1.6740088105726871, "no_speech_prob": 4.710888788395096e-06}, {"id": 564, "seek": 285644, "start": 2872.6, "end": 2877.12, "text": " that they're as low radiation dose as possible.", "tokens": [300, 436, 434, 382, 2295, 12420, 14041, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.13460752367973328, "compression_ratio": 1.6740088105726871, "no_speech_prob": 4.710888788395096e-06}, {"id": 565, "seek": 285644, "start": 2877.12, "end": 2882.84, "text": " So that means that you're going to have to think about transfer learning issues, that", "tokens": [407, 300, 1355, 300, 291, 434, 516, 281, 362, 281, 519, 466, 5003, 2539, 2663, 11, 300], "temperature": 0.0, "avg_logprob": -0.13460752367973328, "compression_ratio": 1.6740088105726871, "no_speech_prob": 4.710888788395096e-06}, {"id": 566, "seek": 288284, "start": 2882.84, "end": 2887.32, "text": " the contrast in your image is going to be different between the thing you build on the", "tokens": [264, 8712, 294, 428, 3256, 307, 516, 281, 312, 819, 1296, 264, 551, 291, 1322, 322, 264], "temperature": 0.0, "avg_logprob": -0.17241292457058005, "compression_ratio": 1.529100529100529, "no_speech_prob": 7.071855179674458e-06}, {"id": 567, "seek": 288284, "start": 2887.32, "end": 2898.2400000000002, "text": " Luna dataset, the nodule detection, and the Kaggle competition dataset.", "tokens": [27355, 28872, 11, 264, 15224, 2271, 17784, 11, 293, 264, 48751, 22631, 6211, 28872, 13], "temperature": 0.0, "avg_logprob": -0.17241292457058005, "compression_ratio": 1.529100529100529, "no_speech_prob": 7.071855179674458e-06}, {"id": 568, "seek": 288284, "start": 2898.2400000000002, "end": 2902.08, "text": " When I looked at it, I didn't find that that was a terribly difficult problem.", "tokens": [1133, 286, 2956, 412, 309, 11, 286, 994, 380, 915, 300, 300, 390, 257, 22903, 2252, 1154, 13], "temperature": 0.0, "avg_logprob": -0.17241292457058005, "compression_ratio": 1.529100529100529, "no_speech_prob": 7.071855179674458e-06}, {"id": 569, "seek": 288284, "start": 2902.08, "end": 2912.6000000000004, "text": " I'm sure you won't find it impossible by any means.", "tokens": [286, 478, 988, 291, 1582, 380, 915, 309, 6243, 538, 604, 1355, 13], "temperature": 0.0, "avg_logprob": -0.17241292457058005, "compression_ratio": 1.529100529100529, "no_speech_prob": 7.071855179674458e-06}, {"id": 570, "seek": 291260, "start": 2912.6, "end": 2921.16, "text": " So to finalize this discussion, I wanted to refer to this paper, which I'm guessing not", "tokens": [407, 281, 2572, 1125, 341, 5017, 11, 286, 1415, 281, 2864, 281, 341, 3035, 11, 597, 286, 478, 17939, 406], "temperature": 0.0, "avg_logprob": -0.18829681759788877, "compression_ratio": 1.4, "no_speech_prob": 3.535528594511561e-05}, {"id": 571, "seek": 291260, "start": 2921.16, "end": 2923.7999999999997, "text": " that many people have read yet.", "tokens": [300, 867, 561, 362, 1401, 1939, 13], "temperature": 0.0, "avg_logprob": -0.18829681759788877, "compression_ratio": 1.4, "no_speech_prob": 3.535528594511561e-05}, {"id": 572, "seek": 291260, "start": 2923.7999999999997, "end": 2926.16, "text": " It's a medical imaging paper.", "tokens": [467, 311, 257, 4625, 25036, 3035, 13], "temperature": 0.0, "avg_logprob": -0.18829681759788877, "compression_ratio": 1.4, "no_speech_prob": 3.535528594511561e-05}, {"id": 573, "seek": 291260, "start": 2926.16, "end": 2932.12, "text": " And what it is, is a non-deep learning approach to trying to find nodules.", "tokens": [400, 437, 309, 307, 11, 307, 257, 2107, 12, 38422, 2539, 3109, 281, 1382, 281, 915, 15224, 3473, 13], "temperature": 0.0, "avg_logprob": -0.18829681759788877, "compression_ratio": 1.4, "no_speech_prob": 3.535528594511561e-05}, {"id": 574, "seek": 293212, "start": 2932.12, "end": 2943.12, "text": " Yes, Rachel.", "tokens": [1079, 11, 14246, 13], "temperature": 0.2, "avg_logprob": -0.6184756652168606, "compression_ratio": 1.2072072072072073, "no_speech_prob": 9.665790457802359e-06}, {"id": 575, "seek": 293212, "start": 2943.12, "end": 2949.7599999999998, "text": " I have a correction from our radiologist saying that dye is not radioactive.", "tokens": [286, 362, 257, 19984, 490, 527, 16335, 9201, 1566, 300, 20179, 307, 406, 35844, 13], "temperature": 0.2, "avg_logprob": -0.6184756652168606, "compression_ratio": 1.2072072072072073, "no_speech_prob": 9.665790457802359e-06}, {"id": 576, "seek": 293212, "start": 2949.7599999999998, "end": 2950.7599999999998, "text": " It's just dense.", "tokens": [467, 311, 445, 18011, 13], "temperature": 0.2, "avg_logprob": -0.6184756652168606, "compression_ratio": 1.2072072072072073, "no_speech_prob": 9.665790457802359e-06}, {"id": 577, "seek": 293212, "start": 2950.7599999999998, "end": 2953.7599999999998, "text": " Isobutyl 70 or isobutyl 70.", "tokens": [1119, 996, 325, 5088, 5285, 420, 307, 996, 325, 5088, 5285, 13], "temperature": 0.2, "avg_logprob": -0.6184756652168606, "compression_ratio": 1.2072072072072073, "no_speech_prob": 9.665790457802359e-06}, {"id": 578, "seek": 295376, "start": 2953.76, "end": 2968.0400000000004, "text": " But there's a reason we don't inject people with the contrast dye.", "tokens": [583, 456, 311, 257, 1778, 321, 500, 380, 10711, 561, 365, 264, 8712, 20179, 13], "temperature": 0.0, "avg_logprob": -0.18471235320681617, "compression_ratio": 1.2419354838709677, "no_speech_prob": 2.0462106476770714e-05}, {"id": 579, "seek": 295376, "start": 2968.0400000000004, "end": 2980.4, "text": " I do know though that the NLST studies use a lower amount of radioactivity than I think", "tokens": [286, 360, 458, 1673, 300, 264, 426, 43, 6840, 5313, 764, 257, 3126, 2372, 295, 6477, 578, 4253, 813, 286, 519], "temperature": 0.0, "avg_logprob": -0.18471235320681617, "compression_ratio": 1.2419354838709677, "no_speech_prob": 2.0462106476770714e-05}, {"id": 580, "seek": 298040, "start": 2980.4, "end": 2986.36, "text": " the Luna ones do, so that's another difference.", "tokens": [264, 27355, 2306, 360, 11, 370, 300, 311, 1071, 2649, 13], "temperature": 0.0, "avg_logprob": -0.1699278442947953, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.4285122233559377e-05}, {"id": 581, "seek": 298040, "start": 2986.36, "end": 2995.84, "text": " So this is an interesting idea of how can you find nodules using more of a heuristic", "tokens": [407, 341, 307, 364, 1880, 1558, 295, 577, 393, 291, 915, 15224, 3473, 1228, 544, 295, 257, 415, 374, 3142], "temperature": 0.0, "avg_logprob": -0.1699278442947953, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.4285122233559377e-05}, {"id": 582, "seek": 298040, "start": 2995.84, "end": 2997.2000000000003, "text": " approach.", "tokens": [3109, 13], "temperature": 0.0, "avg_logprob": -0.1699278442947953, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.4285122233559377e-05}, {"id": 583, "seek": 298040, "start": 2997.2000000000003, "end": 3003.04, "text": " The heuristic approach they suggest here is to do clustering.", "tokens": [440, 415, 374, 3142, 3109, 436, 3402, 510, 307, 281, 360, 596, 48673, 13], "temperature": 0.0, "avg_logprob": -0.1699278442947953, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.4285122233559377e-05}, {"id": 584, "seek": 298040, "start": 3003.04, "end": 3007.04, "text": " We haven't really done any clustering in class yet, so we're going to dig into this in some", "tokens": [492, 2378, 380, 534, 1096, 604, 596, 48673, 294, 1508, 1939, 11, 370, 321, 434, 516, 281, 2528, 666, 341, 294, 512], "temperature": 0.0, "avg_logprob": -0.1699278442947953, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.4285122233559377e-05}, {"id": 585, "seek": 300704, "start": 3007.04, "end": 3011.64, "text": " time, because I think this is a great idea for the kind of heuristics you can add on", "tokens": [565, 11, 570, 286, 519, 341, 307, 257, 869, 1558, 337, 264, 733, 295, 415, 374, 6006, 291, 393, 909, 322], "temperature": 0.0, "avg_logprob": -0.15252528059373208, "compression_ratio": 1.7023809523809523, "no_speech_prob": 3.0241135391406715e-05}, {"id": 586, "seek": 300704, "start": 3011.64, "end": 3016.2, "text": " top of deep learning to make deep learning work in different areas.", "tokens": [1192, 295, 2452, 2539, 281, 652, 2452, 2539, 589, 294, 819, 3179, 13], "temperature": 0.0, "avg_logprob": -0.15252528059373208, "compression_ratio": 1.7023809523809523, "no_speech_prob": 3.0241135391406715e-05}, {"id": 587, "seek": 300704, "start": 3016.2, "end": 3023.16, "text": " The basic idea here is to, as you can say, they call it a 5-dimensional mean.", "tokens": [440, 3875, 1558, 510, 307, 281, 11, 382, 291, 393, 584, 11, 436, 818, 309, 257, 1025, 12, 18759, 914, 13], "temperature": 0.0, "avg_logprob": -0.15252528059373208, "compression_ratio": 1.7023809523809523, "no_speech_prob": 3.0241135391406715e-05}, {"id": 588, "seek": 300704, "start": 3023.16, "end": 3028.12, "text": " They're going to try and find groups of voxels which are similar and they're going to cluster", "tokens": [814, 434, 516, 281, 853, 293, 915, 3935, 295, 1650, 87, 1625, 597, 366, 2531, 293, 436, 434, 516, 281, 13630], "temperature": 0.0, "avg_logprob": -0.15252528059373208, "compression_ratio": 1.7023809523809523, "no_speech_prob": 3.0241135391406715e-05}, {"id": 589, "seek": 300704, "start": 3028.12, "end": 3029.52, "text": " them together.", "tokens": [552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.15252528059373208, "compression_ratio": 1.7023809523809523, "no_speech_prob": 3.0241135391406715e-05}, {"id": 590, "seek": 300704, "start": 3029.52, "end": 3034.32, "text": " And hopefully we're going to particularly cluster together things that look like nodules.", "tokens": [400, 4696, 321, 434, 516, 281, 4098, 13630, 1214, 721, 300, 574, 411, 15224, 3473, 13], "temperature": 0.0, "avg_logprob": -0.15252528059373208, "compression_ratio": 1.7023809523809523, "no_speech_prob": 3.0241135391406715e-05}, {"id": 591, "seek": 303432, "start": 3034.32, "end": 3040.2400000000002, "text": " So the idea is at the end of this segmentation, there will be one cluster for the whole lung", "tokens": [407, 264, 1558, 307, 412, 264, 917, 295, 341, 9469, 399, 11, 456, 486, 312, 472, 13630, 337, 264, 1379, 16730], "temperature": 0.0, "avg_logprob": -0.1939747307890205, "compression_ratio": 1.6829268292682926, "no_speech_prob": 4.8603719733364414e-06}, {"id": 592, "seek": 303432, "start": 3040.2400000000002, "end": 3045.88, "text": " boundary, one cluster for the whole vasculature, and then one cluster for every nodule.", "tokens": [12866, 11, 472, 13630, 337, 264, 1379, 11481, 2444, 1503, 11, 293, 550, 472, 13630, 337, 633, 15224, 2271, 13], "temperature": 0.0, "avg_logprob": -0.1939747307890205, "compression_ratio": 1.6829268292682926, "no_speech_prob": 4.8603719733364414e-06}, {"id": 593, "seek": 303432, "start": 3045.88, "end": 3054.36, "text": " So the 5 dimensions are x, y and z, intensity, so the number of Houndsfield units.", "tokens": [407, 264, 1025, 12819, 366, 2031, 11, 288, 293, 710, 11, 13749, 11, 370, 264, 1230, 295, 389, 4432, 7610, 6815, 13], "temperature": 0.0, "avg_logprob": -0.1939747307890205, "compression_ratio": 1.6829268292682926, "no_speech_prob": 4.8603719733364414e-06}, {"id": 594, "seek": 303432, "start": 3054.36, "end": 3061.4, "text": " And then the fifth one is volumetric shape index, and this is the one tricky one.", "tokens": [400, 550, 264, 9266, 472, 307, 1996, 449, 17475, 3909, 8186, 11, 293, 341, 307, 264, 472, 12414, 472, 13], "temperature": 0.0, "avg_logprob": -0.1939747307890205, "compression_ratio": 1.6829268292682926, "no_speech_prob": 4.8603719733364414e-06}, {"id": 595, "seek": 306140, "start": 3061.4, "end": 3066.0, "text": " The basic idea here is it's going to be a combination of the different curvatures of", "tokens": [440, 3875, 1558, 510, 307, 309, 311, 516, 281, 312, 257, 6562, 295, 264, 819, 33900, 3377, 295], "temperature": 0.0, "avg_logprob": -0.11173661249988484, "compression_ratio": 1.7754237288135593, "no_speech_prob": 6.439005574065959e-06}, {"id": 596, "seek": 306140, "start": 3066.0, "end": 3071.56, "text": " a voxel based on the Gaussian and mean curvatures.", "tokens": [257, 1650, 87, 338, 2361, 322, 264, 39148, 293, 914, 33900, 3377, 13], "temperature": 0.0, "avg_logprob": -0.11173661249988484, "compression_ratio": 1.7754237288135593, "no_speech_prob": 6.439005574065959e-06}, {"id": 597, "seek": 306140, "start": 3071.56, "end": 3078.84, "text": " Now what the paper goes on to explain is that you can use for these the first and second", "tokens": [823, 437, 264, 3035, 1709, 322, 281, 2903, 307, 300, 291, 393, 764, 337, 613, 264, 700, 293, 1150], "temperature": 0.0, "avg_logprob": -0.11173661249988484, "compression_ratio": 1.7754237288135593, "no_speech_prob": 6.439005574065959e-06}, {"id": 598, "seek": 306140, "start": 3078.84, "end": 3080.6800000000003, "text": " derivatives of the image.", "tokens": [33733, 295, 264, 3256, 13], "temperature": 0.0, "avg_logprob": -0.11173661249988484, "compression_ratio": 1.7754237288135593, "no_speech_prob": 6.439005574065959e-06}, {"id": 599, "seek": 306140, "start": 3080.6800000000003, "end": 3086.8, "text": " Now all that basically means is you subtract one voxel from its neighbor, and then you", "tokens": [823, 439, 300, 1936, 1355, 307, 291, 16390, 472, 1650, 87, 338, 490, 1080, 5987, 11, 293, 550, 291], "temperature": 0.0, "avg_logprob": -0.11173661249988484, "compression_ratio": 1.7754237288135593, "no_speech_prob": 6.439005574065959e-06}, {"id": 600, "seek": 306140, "start": 3086.8, "end": 3090.6800000000003, "text": " take that whole thing and subtract one voxel's version of that from its neighbor.", "tokens": [747, 300, 1379, 551, 293, 16390, 472, 1650, 87, 338, 311, 3037, 295, 300, 490, 1080, 5987, 13], "temperature": 0.0, "avg_logprob": -0.11173661249988484, "compression_ratio": 1.7754237288135593, "no_speech_prob": 6.439005574065959e-06}, {"id": 601, "seek": 309068, "start": 3090.68, "end": 3092.68, "text": " You get the first and second derivatives.", "tokens": [509, 483, 264, 700, 293, 1150, 33733, 13], "temperature": 0.0, "avg_logprob": -0.1569123725368552, "compression_ratio": 1.7166666666666666, "no_speech_prob": 2.33206856137258e-06}, {"id": 602, "seek": 309068, "start": 3092.68, "end": 3104.2, "text": " So it kind of tells you the direction of the change of image intensity at that point.", "tokens": [407, 309, 733, 295, 5112, 291, 264, 3513, 295, 264, 1319, 295, 3256, 13749, 412, 300, 935, 13], "temperature": 0.0, "avg_logprob": -0.1569123725368552, "compression_ratio": 1.7166666666666666, "no_speech_prob": 2.33206856137258e-06}, {"id": 603, "seek": 309068, "start": 3104.2, "end": 3107.96, "text": " So by getting these first and second derivatives of the image and then you put it into this", "tokens": [407, 538, 1242, 613, 700, 293, 1150, 33733, 295, 264, 3256, 293, 550, 291, 829, 309, 666, 341], "temperature": 0.0, "avg_logprob": -0.1569123725368552, "compression_ratio": 1.7166666666666666, "no_speech_prob": 2.33206856137258e-06}, {"id": 604, "seek": 309068, "start": 3107.96, "end": 3116.9199999999996, "text": " formula, it comes out with something which basically tells you how sphere-like this voxel", "tokens": [8513, 11, 309, 1487, 484, 365, 746, 597, 1936, 5112, 291, 577, 16687, 12, 4092, 341, 1650, 87, 338], "temperature": 0.0, "avg_logprob": -0.1569123725368552, "compression_ratio": 1.7166666666666666, "no_speech_prob": 2.33206856137258e-06}, {"id": 605, "seek": 311692, "start": 3116.92, "end": 3121.2400000000002, "text": " seems to be, how spheres like a construct.", "tokens": [2544, 281, 312, 11, 577, 41225, 411, 257, 7690, 13], "temperature": 0.0, "avg_logprob": -0.15584298184043482, "compression_ratio": 1.6305970149253732, "no_speech_prob": 2.123372269124957e-06}, {"id": 606, "seek": 311692, "start": 3121.2400000000002, "end": 3127.6, "text": " So that's great, if we can basically take all the voxels and combine the ones that are", "tokens": [407, 300, 311, 869, 11, 498, 321, 393, 1936, 747, 439, 264, 1650, 87, 1625, 293, 10432, 264, 2306, 300, 366], "temperature": 0.0, "avg_logprob": -0.15584298184043482, "compression_ratio": 1.6305970149253732, "no_speech_prob": 2.123372269124957e-06}, {"id": 607, "seek": 311692, "start": 3127.6, "end": 3132.2000000000003, "text": " nearby, have a similar number of Houndsfield units, and seem to be of similar kinds of", "tokens": [11184, 11, 362, 257, 2531, 1230, 295, 389, 4432, 7610, 6815, 11, 293, 1643, 281, 312, 295, 2531, 3685, 295], "temperature": 0.0, "avg_logprob": -0.15584298184043482, "compression_ratio": 1.6305970149253732, "no_speech_prob": 2.123372269124957e-06}, {"id": 608, "seek": 311692, "start": 3132.2000000000003, "end": 3136.4, "text": " shapes, we're going to get what we want.", "tokens": [10854, 11, 321, 434, 516, 281, 483, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.15584298184043482, "compression_ratio": 1.6305970149253732, "no_speech_prob": 2.123372269124957e-06}, {"id": 609, "seek": 311692, "start": 3136.4, "end": 3141.4, "text": " So I'm not going to worry about this bit here because it's very specific to medical imaging.", "tokens": [407, 286, 478, 406, 516, 281, 3292, 466, 341, 857, 510, 570, 309, 311, 588, 2685, 281, 4625, 25036, 13], "temperature": 0.0, "avg_logprob": -0.15584298184043482, "compression_ratio": 1.6305970149253732, "no_speech_prob": 2.123372269124957e-06}, {"id": 610, "seek": 311692, "start": 3141.4, "end": 3146.12, "text": " Anybody who's interested in doing this, feel free to talk on the forum about what this", "tokens": [19082, 567, 311, 3102, 294, 884, 341, 11, 841, 1737, 281, 751, 322, 264, 17542, 466, 437, 341], "temperature": 0.0, "avg_logprob": -0.15584298184043482, "compression_ratio": 1.6305970149253732, "no_speech_prob": 2.123372269124957e-06}, {"id": 611, "seek": 314612, "start": 3146.12, "end": 3150.04, "text": " looks like in Python.", "tokens": [1542, 411, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.3016943477448963, "compression_ratio": 1.3391304347826087, "no_speech_prob": 5.2251423767302185e-05}, {"id": 612, "seek": 314612, "start": 3150.04, "end": 3155.3199999999997, "text": " But what I did want to talk about was the main-shift clustering, which is a particular", "tokens": [583, 437, 286, 630, 528, 281, 751, 466, 390, 264, 2135, 12, 47445, 596, 48673, 11, 597, 307, 257, 1729], "temperature": 0.0, "avg_logprob": -0.3016943477448963, "compression_ratio": 1.3391304347826087, "no_speech_prob": 5.2251423767302185e-05}, {"id": 613, "seek": 314612, "start": 3155.3199999999997, "end": 3158.72, "text": " approach to clustering which they talk about.", "tokens": [3109, 281, 596, 48673, 597, 436, 751, 466, 13], "temperature": 0.0, "avg_logprob": -0.3016943477448963, "compression_ratio": 1.3391304347826087, "no_speech_prob": 5.2251423767302185e-05}, {"id": 614, "seek": 315872, "start": 3158.72, "end": 3181.3599999999997, "text": " Question asked.", "tokens": [14464, 2351, 13], "temperature": 0.0, "avg_logprob": -0.23758411407470703, "compression_ratio": 1.0795454545454546, "no_speech_prob": 1.9525166862877086e-05}, {"id": 615, "seek": 315872, "start": 3181.3599999999997, "end": 3187.48, "text": " Clustering is something which for a long time I've been kind of an anti-fan of.", "tokens": [2033, 48673, 307, 746, 597, 337, 257, 938, 565, 286, 600, 668, 733, 295, 364, 6061, 12, 20361, 295, 13], "temperature": 0.0, "avg_logprob": -0.23758411407470703, "compression_ratio": 1.0795454545454546, "no_speech_prob": 1.9525166862877086e-05}, {"id": 616, "seek": 318748, "start": 3187.48, "end": 3194.2400000000002, "text": " It belongs to this group of unsupervised learning algorithms which always seem to be looking", "tokens": [467, 12953, 281, 341, 1594, 295, 2693, 12879, 24420, 2539, 14642, 597, 1009, 1643, 281, 312, 1237], "temperature": 0.0, "avg_logprob": -0.1764063468346229, "compression_ratio": 1.6037735849056605, "no_speech_prob": 2.7968288122792728e-05}, {"id": 617, "seek": 318748, "start": 3194.2400000000002, "end": 3196.76, "text": " for a problem to solve.", "tokens": [337, 257, 1154, 281, 5039, 13], "temperature": 0.0, "avg_logprob": -0.1764063468346229, "compression_ratio": 1.6037735849056605, "no_speech_prob": 2.7968288122792728e-05}, {"id": 618, "seek": 318748, "start": 3196.76, "end": 3199.88, "text": " But I've realized recently there are some specific problems that can be solved well", "tokens": [583, 286, 600, 5334, 3938, 456, 366, 512, 2685, 2740, 300, 393, 312, 13041, 731], "temperature": 0.0, "avg_logprob": -0.1764063468346229, "compression_ratio": 1.6037735849056605, "no_speech_prob": 2.7968288122792728e-05}, {"id": 619, "seek": 318748, "start": 3199.88, "end": 3208.36, "text": " with them, and I'm going to be showing you a couple, one today and one in Lesson 14.", "tokens": [365, 552, 11, 293, 286, 478, 516, 281, 312, 4099, 291, 257, 1916, 11, 472, 965, 293, 472, 294, 18649, 266, 3499, 13], "temperature": 0.0, "avg_logprob": -0.1764063468346229, "compression_ratio": 1.6037735849056605, "no_speech_prob": 2.7968288122792728e-05}, {"id": 620, "seek": 318748, "start": 3208.36, "end": 3212.52, "text": " Clustering algorithms are perhaps the easiest to describe by what they do by generating", "tokens": [2033, 48673, 14642, 366, 4317, 264, 12889, 281, 6786, 538, 437, 436, 360, 538, 17746], "temperature": 0.0, "avg_logprob": -0.1764063468346229, "compression_ratio": 1.6037735849056605, "no_speech_prob": 2.7968288122792728e-05}, {"id": 621, "seek": 318748, "start": 3212.52, "end": 3214.44, "text": " some data to show them.", "tokens": [512, 1412, 281, 855, 552, 13], "temperature": 0.0, "avg_logprob": -0.1764063468346229, "compression_ratio": 1.6037735849056605, "no_speech_prob": 2.7968288122792728e-05}, {"id": 622, "seek": 318748, "start": 3214.44, "end": 3216.2400000000002, "text": " Here's some generated data.", "tokens": [1692, 311, 512, 10833, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1764063468346229, "compression_ratio": 1.6037735849056605, "no_speech_prob": 2.7968288122792728e-05}, {"id": 623, "seek": 321624, "start": 3216.24, "end": 3222.8799999999997, "text": " I'm going to create 6 clusters, and for each cluster I'll create 250 samples.", "tokens": [286, 478, 516, 281, 1884, 1386, 23313, 11, 293, 337, 1184, 13630, 286, 603, 1884, 11650, 10938, 13], "temperature": 0.0, "avg_logprob": -0.15657728461809056, "compression_ratio": 1.7878787878787878, "no_speech_prob": 2.4299955839524046e-05}, {"id": 624, "seek": 321624, "start": 3222.8799999999997, "end": 3228.0, "text": " So I'm going to basically say, okay, let's create a bunch of centroids by creating some", "tokens": [407, 286, 478, 516, 281, 1936, 584, 11, 1392, 11, 718, 311, 1884, 257, 3840, 295, 24607, 3742, 538, 4084, 512], "temperature": 0.0, "avg_logprob": -0.15657728461809056, "compression_ratio": 1.7878787878787878, "no_speech_prob": 2.4299955839524046e-05}, {"id": 625, "seek": 321624, "start": 3228.0, "end": 3229.0, "text": " random numbers.", "tokens": [4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.15657728461809056, "compression_ratio": 1.7878787878787878, "no_speech_prob": 2.4299955839524046e-05}, {"id": 626, "seek": 321624, "start": 3229.0, "end": 3236.0, "text": " So 6 pairs of random numbers for my centroids.", "tokens": [407, 1386, 15494, 295, 4974, 3547, 337, 452, 24607, 3742, 13], "temperature": 0.0, "avg_logprob": -0.15657728461809056, "compression_ratio": 1.7878787878787878, "no_speech_prob": 2.4299955839524046e-05}, {"id": 627, "seek": 321624, "start": 3236.0, "end": 3242.8799999999997, "text": " And then I'll grab a bunch of random numbers around each of those centroids and combine", "tokens": [400, 550, 286, 603, 4444, 257, 3840, 295, 4974, 3547, 926, 1184, 295, 729, 24607, 3742, 293, 10432], "temperature": 0.0, "avg_logprob": -0.15657728461809056, "compression_ratio": 1.7878787878787878, "no_speech_prob": 2.4299955839524046e-05}, {"id": 628, "seek": 321624, "start": 3242.8799999999997, "end": 3245.8799999999997, "text": " them all together and then plot them.", "tokens": [552, 439, 1214, 293, 550, 7542, 552, 13], "temperature": 0.0, "avg_logprob": -0.15657728461809056, "compression_ratio": 1.7878787878787878, "no_speech_prob": 2.4299955839524046e-05}, {"id": 629, "seek": 324588, "start": 3245.88, "end": 3250.8, "text": " And so here you can see each of these X's represents a centroid.", "tokens": [400, 370, 510, 291, 393, 536, 1184, 295, 613, 1783, 311, 8855, 257, 1489, 6490, 13], "temperature": 0.0, "avg_logprob": -0.17598719963660606, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.3007015695620794e-05}, {"id": 630, "seek": 324588, "start": 3250.8, "end": 3256.78, "text": " So a centroid is just like the average point for a cluster of data.", "tokens": [407, 257, 1489, 6490, 307, 445, 411, 264, 4274, 935, 337, 257, 13630, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.17598719963660606, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.3007015695620794e-05}, {"id": 631, "seek": 324588, "start": 3256.78, "end": 3260.76, "text": " And each color represents one cluster.", "tokens": [400, 1184, 2017, 8855, 472, 13630, 13], "temperature": 0.0, "avg_logprob": -0.17598719963660606, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.3007015695620794e-05}, {"id": 632, "seek": 324588, "start": 3260.76, "end": 3272.44, "text": " So imagine if this was showing you clusterings of different kinds of lung tissue.", "tokens": [407, 3811, 498, 341, 390, 4099, 291, 13630, 1109, 295, 819, 3685, 295, 16730, 12404, 13], "temperature": 0.0, "avg_logprob": -0.17598719963660606, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.3007015695620794e-05}, {"id": 633, "seek": 327244, "start": 3272.44, "end": 3277.88, "text": " Maybe you'd have some voxels that were colored one thing for a nodule and a bunch of things", "tokens": [2704, 291, 1116, 362, 512, 1650, 87, 1625, 300, 645, 14332, 472, 551, 337, 257, 15224, 2271, 293, 257, 3840, 295, 721], "temperature": 0.0, "avg_logprob": -0.19917184656316583, "compression_ratio": 1.6076555023923444, "no_speech_prob": 5.0936841944349e-06}, {"id": 634, "seek": 327244, "start": 3277.88, "end": 3283.56, "text": " that are colored a different color for vasculature and so forth.", "tokens": [300, 366, 14332, 257, 819, 2017, 337, 11481, 2444, 1503, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.19917184656316583, "compression_ratio": 1.6076555023923444, "no_speech_prob": 5.0936841944349e-06}, {"id": 635, "seek": 327244, "start": 3283.56, "end": 3289.0, "text": " We can only show this easily in 2 dimensions, but there's no reason to not be able to imagine", "tokens": [492, 393, 787, 855, 341, 3612, 294, 568, 12819, 11, 457, 456, 311, 572, 1778, 281, 406, 312, 1075, 281, 3811], "temperature": 0.0, "avg_logprob": -0.19917184656316583, "compression_ratio": 1.6076555023923444, "no_speech_prob": 5.0936841944349e-06}, {"id": 636, "seek": 327244, "start": 3289.0, "end": 3292.96, "text": " doing this in certainly 5 dimensions.", "tokens": [884, 341, 294, 3297, 1025, 12819, 13], "temperature": 0.0, "avg_logprob": -0.19917184656316583, "compression_ratio": 1.6076555023923444, "no_speech_prob": 5.0936841944349e-06}, {"id": 637, "seek": 327244, "start": 3292.96, "end": 3297.2400000000002, "text": " So the goal of clustering will be to undo this.", "tokens": [407, 264, 3387, 295, 596, 48673, 486, 312, 281, 23779, 341, 13], "temperature": 0.0, "avg_logprob": -0.19917184656316583, "compression_ratio": 1.6076555023923444, "no_speech_prob": 5.0936841944349e-06}, {"id": 638, "seek": 329724, "start": 3297.24, "end": 3304.4399999999996, "text": " Imagine the data, but not the X's, how can you figure out where the X's were.", "tokens": [11739, 264, 1412, 11, 457, 406, 264, 1783, 311, 11, 577, 393, 291, 2573, 484, 689, 264, 1783, 311, 645, 13], "temperature": 0.0, "avg_logprob": -0.1888885999980726, "compression_ratio": 1.580110497237569, "no_speech_prob": 3.611972260841867e-06}, {"id": 639, "seek": 329724, "start": 3304.4399999999996, "end": 3308.2799999999997, "text": " And then it's pretty straightforward once you know where the X's are to then find the", "tokens": [400, 550, 309, 311, 1238, 15325, 1564, 291, 458, 689, 264, 1783, 311, 366, 281, 550, 915, 264], "temperature": 0.0, "avg_logprob": -0.1888885999980726, "compression_ratio": 1.580110497237569, "no_speech_prob": 3.611972260841867e-06}, {"id": 640, "seek": 329724, "start": 3308.2799999999997, "end": 3315.72, "text": " closest points to that to assign every data point to a cluster.", "tokens": [13699, 2793, 281, 300, 281, 6269, 633, 1412, 935, 281, 257, 13630, 13], "temperature": 0.0, "avg_logprob": -0.1888885999980726, "compression_ratio": 1.580110497237569, "no_speech_prob": 3.611972260841867e-06}, {"id": 641, "seek": 329724, "start": 3315.72, "end": 3321.12, "text": " The most popular approach to clustering is called k-means.", "tokens": [440, 881, 3743, 3109, 281, 596, 48673, 307, 1219, 350, 12, 1398, 599, 13], "temperature": 0.0, "avg_logprob": -0.1888885999980726, "compression_ratio": 1.580110497237569, "no_speech_prob": 3.611972260841867e-06}, {"id": 642, "seek": 332112, "start": 3321.12, "end": 3332.2, "text": " K-means is an approach where you have to decide upfront how many clusters are there.", "tokens": [591, 12, 1398, 599, 307, 364, 3109, 689, 291, 362, 281, 4536, 30264, 577, 867, 23313, 366, 456, 13], "temperature": 0.0, "avg_logprob": -0.2434899193899972, "compression_ratio": 1.5375722543352601, "no_speech_prob": 5.014718681195518e-06}, {"id": 643, "seek": 332112, "start": 3332.2, "end": 3336.68, "text": " And what it basically does is there's 2 steps.", "tokens": [400, 437, 309, 1936, 775, 307, 456, 311, 568, 4439, 13], "temperature": 0.0, "avg_logprob": -0.2434899193899972, "compression_ratio": 1.5375722543352601, "no_speech_prob": 5.014718681195518e-06}, {"id": 644, "seek": 332112, "start": 3336.68, "end": 3342.04, "text": " The first one is to guess as to where those clusters might be.", "tokens": [440, 700, 472, 307, 281, 2041, 382, 281, 689, 729, 23313, 1062, 312, 13], "temperature": 0.0, "avg_logprob": -0.2434899193899972, "compression_ratio": 1.5375722543352601, "no_speech_prob": 5.014718681195518e-06}, {"id": 645, "seek": 332112, "start": 3342.04, "end": 3348.52, "text": " And the really simple way to do that is just to randomize the clusters.", "tokens": [400, 264, 534, 2199, 636, 281, 360, 300, 307, 445, 281, 4974, 1125, 264, 23313, 13], "temperature": 0.0, "avg_logprob": -0.2434899193899972, "compression_ratio": 1.5375722543352601, "no_speech_prob": 5.014718681195518e-06}, {"id": 646, "seek": 334852, "start": 3348.52, "end": 3354.44, "text": " The simple way to do that is basically to randomly pick a point and then start randomly", "tokens": [440, 2199, 636, 281, 360, 300, 307, 1936, 281, 16979, 1888, 257, 935, 293, 550, 722, 16979], "temperature": 0.0, "avg_logprob": -0.2909623541013159, "compression_ratio": 1.7312775330396475, "no_speech_prob": 1.451035222999053e-05}, {"id": 647, "seek": 334852, "start": 3354.44, "end": 3361.88, "text": " picking points which are as far away as possible from all the previous ones I've picked.", "tokens": [8867, 2793, 597, 366, 382, 1400, 1314, 382, 1944, 490, 439, 264, 3894, 2306, 286, 600, 6183, 13], "temperature": 0.0, "avg_logprob": -0.2909623541013159, "compression_ratio": 1.7312775330396475, "no_speech_prob": 1.451035222999053e-05}, {"id": 648, "seek": 334852, "start": 3361.88, "end": 3368.64, "text": " So if I start here, then probably the furthest away point would be down here.", "tokens": [407, 498, 286, 722, 510, 11, 550, 1391, 264, 2687, 36356, 1314, 935, 576, 312, 760, 510, 13], "temperature": 0.0, "avg_logprob": -0.2909623541013159, "compression_ratio": 1.7312775330396475, "no_speech_prob": 1.451035222999053e-05}, {"id": 649, "seek": 334852, "start": 3368.64, "end": 3371.6, "text": " So this would be like our starting point for cluster 1.", "tokens": [407, 341, 576, 312, 411, 527, 2891, 935, 337, 13630, 502, 13], "temperature": 0.0, "avg_logprob": -0.2909623541013159, "compression_ratio": 1.7312775330396475, "no_speech_prob": 1.451035222999053e-05}, {"id": 650, "seek": 334852, "start": 3371.6, "end": 3374.28, "text": " And you say, what point is furthest away from that?", "tokens": [400, 291, 584, 11, 437, 935, 307, 2687, 36356, 1314, 490, 300, 30], "temperature": 0.0, "avg_logprob": -0.2909623541013159, "compression_ratio": 1.7312775330396475, "no_speech_prob": 1.451035222999053e-05}, {"id": 651, "seek": 334852, "start": 3374.28, "end": 3375.72, "text": " That's probably this one here.", "tokens": [663, 311, 1391, 341, 472, 510, 13], "temperature": 0.0, "avg_logprob": -0.2909623541013159, "compression_ratio": 1.7312775330396475, "no_speech_prob": 1.451035222999053e-05}, {"id": 652, "seek": 337572, "start": 3375.72, "end": 3378.56, "text": " That's our starting point for cluster 2.", "tokens": [663, 311, 527, 2891, 935, 337, 13630, 568, 13], "temperature": 0.0, "avg_logprob": -0.21964544998972038, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.42218276677886e-06}, {"id": 653, "seek": 337572, "start": 3378.56, "end": 3380.4399999999996, "text": " What's the furthest point away from both of these?", "tokens": [708, 311, 264, 2687, 36356, 935, 1314, 490, 1293, 295, 613, 30], "temperature": 0.0, "avg_logprob": -0.21964544998972038, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.42218276677886e-06}, {"id": 654, "seek": 337572, "start": 3380.4399999999996, "end": 3382.7999999999997, "text": " Probably this one over here, and so forth.", "tokens": [9210, 341, 472, 670, 510, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.21964544998972038, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.42218276677886e-06}, {"id": 655, "seek": 337572, "start": 3382.7999999999997, "end": 3386.9599999999996, "text": " So you keep doing that to get your initial points.", "tokens": [407, 291, 1066, 884, 300, 281, 483, 428, 5883, 2793, 13], "temperature": 0.0, "avg_logprob": -0.21964544998972038, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.42218276677886e-06}, {"id": 656, "seek": 337572, "start": 3386.9599999999996, "end": 3392.2, "text": " And then you just iteratively move every point.", "tokens": [400, 550, 291, 445, 17138, 19020, 1286, 633, 935, 13], "temperature": 0.0, "avg_logprob": -0.21964544998972038, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.42218276677886e-06}, {"id": 657, "seek": 337572, "start": 3392.2, "end": 3397.48, "text": " So you basically then say, these are the clusters, let's assume these are the clusters.", "tokens": [407, 291, 1936, 550, 584, 11, 613, 366, 264, 23313, 11, 718, 311, 6552, 613, 366, 264, 23313, 13], "temperature": 0.0, "avg_logprob": -0.21964544998972038, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.42218276677886e-06}, {"id": 658, "seek": 337572, "start": 3397.48, "end": 3399.8799999999997, "text": " Which cluster does every point belong to?", "tokens": [3013, 13630, 775, 633, 935, 5784, 281, 30], "temperature": 0.0, "avg_logprob": -0.21964544998972038, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.42218276677886e-06}, {"id": 659, "seek": 339988, "start": 3399.88, "end": 3406.28, "text": " And then you just iteratively move the points to different clusters a bunch of times.", "tokens": [400, 550, 291, 445, 17138, 19020, 1286, 264, 2793, 281, 819, 23313, 257, 3840, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.20303568213877052, "compression_ratio": 1.6594827586206897, "no_speech_prob": 3.138138708891347e-06}, {"id": 660, "seek": 339988, "start": 3406.28, "end": 3414.48, "text": " Now k means, it's a shame it's so popular because it kind of sucks.", "tokens": [823, 350, 1355, 11, 309, 311, 257, 10069, 309, 311, 370, 3743, 570, 309, 733, 295, 15846, 13], "temperature": 0.0, "avg_logprob": -0.20303568213877052, "compression_ratio": 1.6594827586206897, "no_speech_prob": 3.138138708891347e-06}, {"id": 661, "seek": 339988, "start": 3414.48, "end": 3419.52, "text": " Sucky thing number 1 is that you have to decide how many clusters there are.", "tokens": [318, 10616, 551, 1230, 502, 307, 300, 291, 362, 281, 4536, 577, 867, 23313, 456, 366, 13], "temperature": 0.0, "avg_logprob": -0.20303568213877052, "compression_ratio": 1.6594827586206897, "no_speech_prob": 3.138138708891347e-06}, {"id": 662, "seek": 339988, "start": 3419.52, "end": 3424.1600000000003, "text": " The whole point is we don't know how many nodules there are.", "tokens": [440, 1379, 935, 307, 321, 500, 380, 458, 577, 867, 15224, 3473, 456, 366, 13], "temperature": 0.0, "avg_logprob": -0.20303568213877052, "compression_ratio": 1.6594827586206897, "no_speech_prob": 3.138138708891347e-06}, {"id": 663, "seek": 339988, "start": 3424.1600000000003, "end": 3428.7200000000003, "text": " And then sucky thing number 2 is without some changes, to do something called kernel k means,", "tokens": [400, 550, 9967, 88, 551, 1230, 568, 307, 1553, 512, 2962, 11, 281, 360, 746, 1219, 28256, 350, 1355, 11], "temperature": 0.0, "avg_logprob": -0.20303568213877052, "compression_ratio": 1.6594827586206897, "no_speech_prob": 3.138138708891347e-06}, {"id": 664, "seek": 342872, "start": 3428.72, "end": 3431.3999999999996, "text": " it only works if the things are the same shape.", "tokens": [309, 787, 1985, 498, 264, 721, 366, 264, 912, 3909, 13], "temperature": 0.0, "avg_logprob": -0.276989272146514, "compression_ratio": 1.4597701149425288, "no_speech_prob": 3.187545871696784e-06}, {"id": 665, "seek": 342872, "start": 3431.3999999999996, "end": 3433.6, "text": " They're all kind of nicely Gaussian-shaped.", "tokens": [814, 434, 439, 733, 295, 9594, 39148, 12, 23103, 13], "temperature": 0.0, "avg_logprob": -0.276989272146514, "compression_ratio": 1.4597701149425288, "no_speech_prob": 3.187545871696784e-06}, {"id": 666, "seek": 342872, "start": 3433.6, "end": 3440.7999999999997, "text": " So we're going to talk about something way cooler which I only kind of came across somewhat", "tokens": [407, 321, 434, 516, 281, 751, 466, 746, 636, 15566, 597, 286, 787, 733, 295, 1361, 2108, 8344], "temperature": 0.0, "avg_logprob": -0.276989272146514, "compression_ratio": 1.4597701149425288, "no_speech_prob": 3.187545871696784e-06}, {"id": 667, "seek": 342872, "start": 3440.7999999999997, "end": 3445.9199999999996, "text": " recently, much less well-known, which is called mean-shift clustering.", "tokens": [3938, 11, 709, 1570, 731, 12, 6861, 11, 597, 307, 1219, 914, 12, 47445, 596, 48673, 13], "temperature": 0.0, "avg_logprob": -0.276989272146514, "compression_ratio": 1.4597701149425288, "no_speech_prob": 3.187545871696784e-06}, {"id": 668, "seek": 344592, "start": 3445.92, "end": 3460.5, "text": " Now mean-shift clustering is one of these things which seems to spend all of its time in serious", "tokens": [823, 914, 12, 47445, 596, 48673, 307, 472, 295, 613, 721, 597, 2544, 281, 3496, 439, 295, 1080, 565, 294, 3156], "temperature": 0.0, "avg_logprob": -0.21837688747205233, "compression_ratio": 1.53475935828877, "no_speech_prob": 1.3007011148147285e-05}, {"id": 669, "seek": 344592, "start": 3460.5, "end": 3463.5, "text": " mathematician land.", "tokens": [48281, 2117, 13], "temperature": 0.0, "avg_logprob": -0.21837688747205233, "compression_ratio": 1.53475935828877, "no_speech_prob": 1.3007011148147285e-05}, {"id": 670, "seek": 344592, "start": 3463.5, "end": 3468.84, "text": " Whenever I try to look up something about mean-shift clustering, I kind of started seeing", "tokens": [14159, 286, 853, 281, 574, 493, 746, 466, 914, 12, 47445, 596, 48673, 11, 286, 733, 295, 1409, 2577], "temperature": 0.0, "avg_logprob": -0.21837688747205233, "compression_ratio": 1.53475935828877, "no_speech_prob": 1.3007011148147285e-05}, {"id": 671, "seek": 344592, "start": 3468.84, "end": 3470.56, "text": " this kind of thing.", "tokens": [341, 733, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.21837688747205233, "compression_ratio": 1.53475935828877, "no_speech_prob": 1.3007011148147285e-05}, {"id": 672, "seek": 344592, "start": 3470.56, "end": 3475.78, "text": " This is the first tutorial, not in a PDF, that I could find.", "tokens": [639, 307, 264, 700, 7073, 11, 406, 294, 257, 17752, 11, 300, 286, 727, 915, 13], "temperature": 0.0, "avg_logprob": -0.21837688747205233, "compression_ratio": 1.53475935828877, "no_speech_prob": 1.3007011148147285e-05}, {"id": 673, "seek": 347578, "start": 3475.78, "end": 3480.92, "text": " So this is one way to think about mean-shift clustering.", "tokens": [407, 341, 307, 472, 636, 281, 519, 466, 914, 12, 47445, 596, 48673, 13], "temperature": 0.0, "avg_logprob": -0.16157507462935014, "compression_ratio": 1.6517857142857142, "no_speech_prob": 1.568933657836169e-05}, {"id": 674, "seek": 347578, "start": 3480.92, "end": 3488.0800000000004, "text": " Another way is the code-first approach, which is that this is the entire algorithm.", "tokens": [3996, 636, 307, 264, 3089, 12, 29581, 3109, 11, 597, 307, 300, 341, 307, 264, 2302, 9284, 13], "temperature": 0.0, "avg_logprob": -0.16157507462935014, "compression_ratio": 1.6517857142857142, "no_speech_prob": 1.568933657836169e-05}, {"id": 675, "seek": 347578, "start": 3488.0800000000004, "end": 3490.84, "text": " So let's talk about what's going on here.", "tokens": [407, 718, 311, 751, 466, 437, 311, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.16157507462935014, "compression_ratio": 1.6517857142857142, "no_speech_prob": 1.568933657836169e-05}, {"id": 676, "seek": 347578, "start": 3490.84, "end": 3491.84, "text": " What are we doing?", "tokens": [708, 366, 321, 884, 30], "temperature": 0.0, "avg_logprob": -0.16157507462935014, "compression_ratio": 1.6517857142857142, "no_speech_prob": 1.568933657836169e-05}, {"id": 677, "seek": 347578, "start": 3491.84, "end": 3497.44, "text": " At a high level, we're going to do a bunch of loops.", "tokens": [1711, 257, 1090, 1496, 11, 321, 434, 516, 281, 360, 257, 3840, 295, 16121, 13], "temperature": 0.0, "avg_logprob": -0.16157507462935014, "compression_ratio": 1.6517857142857142, "no_speech_prob": 1.568933657836169e-05}, {"id": 678, "seek": 347578, "start": 3497.44, "end": 3499.6200000000003, "text": " So we're going to do 5 steps.", "tokens": [407, 321, 434, 516, 281, 360, 1025, 4439, 13], "temperature": 0.0, "avg_logprob": -0.16157507462935014, "compression_ratio": 1.6517857142857142, "no_speech_prob": 1.568933657836169e-05}, {"id": 679, "seek": 347578, "start": 3499.6200000000003, "end": 3503.5800000000004, "text": " It would be better if I didn't do 5 steps, but I kept doing this until it was stable.", "tokens": [467, 576, 312, 1101, 498, 286, 994, 380, 360, 1025, 4439, 11, 457, 286, 4305, 884, 341, 1826, 309, 390, 8351, 13], "temperature": 0.0, "avg_logprob": -0.16157507462935014, "compression_ratio": 1.6517857142857142, "no_speech_prob": 1.568933657836169e-05}, {"id": 680, "seek": 350358, "start": 3503.58, "end": 3506.52, "text": " For now I'm just going to do 5 steps.", "tokens": [1171, 586, 286, 478, 445, 516, 281, 360, 1025, 4439, 13], "temperature": 0.0, "avg_logprob": -0.2166992914109003, "compression_ratio": 1.7236180904522613, "no_speech_prob": 1.6187421351787634e-05}, {"id": 681, "seek": 350358, "start": 3506.52, "end": 3515.52, "text": " And each step, our data is x, I'm going to enumerate through our data.", "tokens": [400, 1184, 1823, 11, 527, 1412, 307, 2031, 11, 286, 478, 516, 281, 465, 15583, 473, 807, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.2166992914109003, "compression_ratio": 1.7236180904522613, "no_speech_prob": 1.6187421351787634e-05}, {"id": 682, "seek": 350358, "start": 3515.52, "end": 3520.68, "text": " What I'm going to do is I'm going to find a small x is the current data point I'm looking", "tokens": [708, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 915, 257, 1359, 2031, 307, 264, 2190, 1412, 935, 286, 478, 1237], "temperature": 0.0, "avg_logprob": -0.2166992914109003, "compression_ratio": 1.7236180904522613, "no_speech_prob": 1.6187421351787634e-05}, {"id": 683, "seek": 350358, "start": 3520.68, "end": 3521.68, "text": " at.", "tokens": [412, 13], "temperature": 0.0, "avg_logprob": -0.2166992914109003, "compression_ratio": 1.7236180904522613, "no_speech_prob": 1.6187421351787634e-05}, {"id": 684, "seek": 350358, "start": 3521.68, "end": 3526.7999999999997, "text": " Now what I want to do is find out how far away is this data point from every other data", "tokens": [823, 437, 286, 528, 281, 360, 307, 915, 484, 577, 1400, 1314, 307, 341, 1412, 935, 490, 633, 661, 1412], "temperature": 0.0, "avg_logprob": -0.2166992914109003, "compression_ratio": 1.7236180904522613, "no_speech_prob": 1.6187421351787634e-05}, {"id": 685, "seek": 350358, "start": 3526.7999999999997, "end": 3527.7999999999997, "text": " point.", "tokens": [935, 13], "temperature": 0.0, "avg_logprob": -0.2166992914109003, "compression_ratio": 1.7236180904522613, "no_speech_prob": 1.6187421351787634e-05}, {"id": 686, "seek": 350358, "start": 3527.7999999999997, "end": 3530.88, "text": " So I'm going to create a vector of distances.", "tokens": [407, 286, 478, 516, 281, 1884, 257, 8062, 295, 22182, 13], "temperature": 0.0, "avg_logprob": -0.2166992914109003, "compression_ratio": 1.7236180904522613, "no_speech_prob": 1.6187421351787634e-05}, {"id": 687, "seek": 353088, "start": 3530.88, "end": 3535.0, "text": " And I'm going to do that with the magic of broadcasting.", "tokens": [400, 286, 478, 516, 281, 360, 300, 365, 264, 5585, 295, 30024, 13], "temperature": 0.0, "avg_logprob": -0.11266169945398967, "compression_ratio": 1.663594470046083, "no_speech_prob": 2.9023046863585478e-06}, {"id": 688, "seek": 353088, "start": 3535.0, "end": 3541.12, "text": " So small x is a vector of size 2, this is 2 coordinates.", "tokens": [407, 1359, 2031, 307, 257, 8062, 295, 2744, 568, 11, 341, 307, 568, 21056, 13], "temperature": 0.0, "avg_logprob": -0.11266169945398967, "compression_ratio": 1.663594470046083, "no_speech_prob": 2.9023046863585478e-06}, {"id": 689, "seek": 353088, "start": 3541.12, "end": 3548.96, "text": " And big X is a matrix of size n by 2, where n is the number of points.", "tokens": [400, 955, 1783, 307, 257, 8141, 295, 2744, 297, 538, 568, 11, 689, 297, 307, 264, 1230, 295, 2793, 13], "temperature": 0.0, "avg_logprob": -0.11266169945398967, "compression_ratio": 1.663594470046083, "no_speech_prob": 2.9023046863585478e-06}, {"id": 690, "seek": 353088, "start": 3548.96, "end": 3552.92, "text": " And thanks to what we've now learned about broadcasting, we know that we could subtract", "tokens": [400, 3231, 281, 437, 321, 600, 586, 3264, 466, 30024, 11, 321, 458, 300, 321, 727, 16390], "temperature": 0.0, "avg_logprob": -0.11266169945398967, "compression_ratio": 1.663594470046083, "no_speech_prob": 2.9023046863585478e-06}, {"id": 691, "seek": 353088, "start": 3552.92, "end": 3559.44, "text": " a matrix from a vector, and that vector will be broadcast across the axis of the matrix.", "tokens": [257, 8141, 490, 257, 8062, 11, 293, 300, 8062, 486, 312, 9975, 2108, 264, 10298, 295, 264, 8141, 13], "temperature": 0.0, "avg_logprob": -0.11266169945398967, "compression_ratio": 1.663594470046083, "no_speech_prob": 2.9023046863585478e-06}, {"id": 692, "seek": 355944, "start": 3559.44, "end": 3565.2000000000003, "text": " And so this is going to subtract every element of big X from little x.", "tokens": [400, 370, 341, 307, 516, 281, 16390, 633, 4478, 295, 955, 1783, 490, 707, 2031, 13], "temperature": 0.0, "avg_logprob": -0.17812633514404297, "compression_ratio": 1.6802721088435375, "no_speech_prob": 3.72663225789438e-06}, {"id": 693, "seek": 355944, "start": 3565.2000000000003, "end": 3571.8, "text": " And so if we then go ahead and square that and then sum it up and then take the square", "tokens": [400, 370, 498, 321, 550, 352, 2286, 293, 3732, 300, 293, 550, 2408, 309, 493, 293, 550, 747, 264, 3732], "temperature": 0.0, "avg_logprob": -0.17812633514404297, "compression_ratio": 1.6802721088435375, "no_speech_prob": 3.72663225789438e-06}, {"id": 694, "seek": 355944, "start": 3571.8, "end": 3581.2000000000003, "text": " root, this is going to return a vector of distances of small x to every element of big", "tokens": [5593, 11, 341, 307, 516, 281, 2736, 257, 8062, 295, 22182, 295, 1359, 2031, 281, 633, 4478, 295, 955], "temperature": 0.0, "avg_logprob": -0.17812633514404297, "compression_ratio": 1.6802721088435375, "no_speech_prob": 3.72663225789438e-06}, {"id": 695, "seek": 355944, "start": 3581.2000000000003, "end": 3584.48, "text": " X.", "tokens": [1783, 13], "temperature": 0.0, "avg_logprob": -0.17812633514404297, "compression_ratio": 1.6802721088435375, "no_speech_prob": 3.72663225789438e-06}, {"id": 696, "seek": 358448, "start": 3584.48, "end": 3591.36, "text": " And the sum here is just summing up the 2 coordinates.", "tokens": [400, 264, 2408, 510, 307, 445, 2408, 2810, 493, 264, 568, 21056, 13], "temperature": 0.0, "avg_logprob": -0.19933348263011258, "compression_ratio": 1.572972972972973, "no_speech_prob": 1.1189404176548123e-06}, {"id": 697, "seek": 358448, "start": 3591.36, "end": 3592.36, "text": " So that's step 1.", "tokens": [407, 300, 311, 1823, 502, 13], "temperature": 0.0, "avg_logprob": -0.19933348263011258, "compression_ratio": 1.572972972972973, "no_speech_prob": 1.1189404176548123e-06}, {"id": 698, "seek": 358448, "start": 3592.36, "end": 3596.88, "text": " So we now know for this particular data point how far away is it from all of the other data", "tokens": [407, 321, 586, 458, 337, 341, 1729, 1412, 935, 577, 1400, 1314, 307, 309, 490, 439, 295, 264, 661, 1412], "temperature": 0.0, "avg_logprob": -0.19933348263011258, "compression_ratio": 1.572972972972973, "no_speech_prob": 1.1189404176548123e-06}, {"id": 699, "seek": 358448, "start": 3596.88, "end": 3597.88, "text": " points.", "tokens": [2793, 13], "temperature": 0.0, "avg_logprob": -0.19933348263011258, "compression_ratio": 1.572972972972973, "no_speech_prob": 1.1189404176548123e-06}, {"id": 700, "seek": 358448, "start": 3597.88, "end": 3604.36, "text": " Now the next thing we want to do is to, let's go to the final step.", "tokens": [823, 264, 958, 551, 321, 528, 281, 360, 307, 281, 11, 718, 311, 352, 281, 264, 2572, 1823, 13], "temperature": 0.0, "avg_logprob": -0.19933348263011258, "compression_ratio": 1.572972972972973, "no_speech_prob": 1.1189404176548123e-06}, {"id": 701, "seek": 358448, "start": 3604.36, "end": 3608.76, "text": " The final step will be to take a weighted average.", "tokens": [440, 2572, 1823, 486, 312, 281, 747, 257, 32807, 4274, 13], "temperature": 0.0, "avg_logprob": -0.19933348263011258, "compression_ratio": 1.572972972972973, "no_speech_prob": 1.1189404176548123e-06}, {"id": 702, "seek": 360876, "start": 3608.76, "end": 3627.0400000000004, "text": " In the final step, we're going to say what cluster do you belong to.", "tokens": [682, 264, 2572, 1823, 11, 321, 434, 516, 281, 584, 437, 13630, 360, 291, 5784, 281, 13], "temperature": 0.0, "avg_logprob": -0.2728237807750702, "compression_ratio": 1.1595744680851063, "no_speech_prob": 2.7693999982147943e-06}, {"id": 703, "seek": 360876, "start": 3627.0400000000004, "end": 3634.28, "text": " And we're currently looking at this one.", "tokens": [400, 321, 434, 4362, 1237, 412, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.2728237807750702, "compression_ratio": 1.1595744680851063, "no_speech_prob": 2.7693999982147943e-06}, {"id": 704, "seek": 363428, "start": 3634.28, "end": 3639.1200000000003, "text": " What we've done is we've now got a list of how far it is away from all of the other data", "tokens": [708, 321, 600, 1096, 307, 321, 600, 586, 658, 257, 1329, 295, 577, 1400, 309, 307, 1314, 490, 439, 295, 264, 661, 1412], "temperature": 0.0, "avg_logprob": -0.11657675030162033, "compression_ratio": 1.898989898989899, "no_speech_prob": 2.9480104331014445e-06}, {"id": 705, "seek": 363428, "start": 3639.1200000000003, "end": 3643.84, "text": " points.", "tokens": [2793, 13], "temperature": 0.0, "avg_logprob": -0.11657675030162033, "compression_ratio": 1.898989898989899, "no_speech_prob": 2.9480104331014445e-06}, {"id": 706, "seek": 363428, "start": 3643.84, "end": 3649.44, "text": " And the basic idea is now what we want to do is take the weighted average of all of", "tokens": [400, 264, 3875, 1558, 307, 586, 437, 321, 528, 281, 360, 307, 747, 264, 32807, 4274, 295, 439, 295], "temperature": 0.0, "avg_logprob": -0.11657675030162033, "compression_ratio": 1.898989898989899, "no_speech_prob": 2.9480104331014445e-06}, {"id": 707, "seek": 363428, "start": 3649.44, "end": 3654.0, "text": " those data points, weight it by the inverse of that distance.", "tokens": [729, 1412, 2793, 11, 3364, 309, 538, 264, 17340, 295, 300, 4560, 13], "temperature": 0.0, "avg_logprob": -0.11657675030162033, "compression_ratio": 1.898989898989899, "no_speech_prob": 2.9480104331014445e-06}, {"id": 708, "seek": 363428, "start": 3654.0, "end": 3658.6200000000003, "text": " So the things that are a long way away, we want to weight very small.", "tokens": [407, 264, 721, 300, 366, 257, 938, 636, 1314, 11, 321, 528, 281, 3364, 588, 1359, 13], "temperature": 0.0, "avg_logprob": -0.11657675030162033, "compression_ratio": 1.898989898989899, "no_speech_prob": 2.9480104331014445e-06}, {"id": 709, "seek": 363428, "start": 3658.6200000000003, "end": 3662.84, "text": " And the things that are very close, we want to weight very big.", "tokens": [400, 264, 721, 300, 366, 588, 1998, 11, 321, 528, 281, 3364, 588, 955, 13], "temperature": 0.0, "avg_logprob": -0.11657675030162033, "compression_ratio": 1.898989898989899, "no_speech_prob": 2.9480104331014445e-06}, {"id": 710, "seek": 366284, "start": 3662.84, "end": 3670.76, "text": " So I think this is probably the closest, this is about the second closest, and this is about", "tokens": [407, 286, 519, 341, 307, 1391, 264, 13699, 11, 341, 307, 466, 264, 1150, 13699, 11, 293, 341, 307, 466], "temperature": 0.0, "avg_logprob": -0.15451709875899755, "compression_ratio": 1.8412698412698412, "no_speech_prob": 1.1189412134626764e-06}, {"id": 711, "seek": 366284, "start": 3670.76, "end": 3671.76, "text": " the third closest.", "tokens": [264, 2636, 13699, 13], "temperature": 0.0, "avg_logprob": -0.15451709875899755, "compression_ratio": 1.8412698412698412, "no_speech_prob": 1.1189412134626764e-06}, {"id": 712, "seek": 366284, "start": 3671.76, "end": 3675.6800000000003, "text": " So assuming these have got most of the weight, the average is going to be somewhere about", "tokens": [407, 11926, 613, 362, 658, 881, 295, 264, 3364, 11, 264, 4274, 307, 516, 281, 312, 4079, 466], "temperature": 0.0, "avg_logprob": -0.15451709875899755, "compression_ratio": 1.8412698412698412, "no_speech_prob": 1.1189412134626764e-06}, {"id": 713, "seek": 366284, "start": 3675.6800000000003, "end": 3678.76, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.15451709875899755, "compression_ratio": 1.8412698412698412, "no_speech_prob": 1.1189412134626764e-06}, {"id": 714, "seek": 366284, "start": 3678.76, "end": 3685.56, "text": " And so by doing that at every point, we're going to move every point closer to where", "tokens": [400, 370, 538, 884, 300, 412, 633, 935, 11, 321, 434, 516, 281, 1286, 633, 935, 4966, 281, 689], "temperature": 0.0, "avg_logprob": -0.15451709875899755, "compression_ratio": 1.8412698412698412, "no_speech_prob": 1.1189412134626764e-06}, {"id": 715, "seek": 366284, "start": 3685.56, "end": 3689.2200000000003, "text": " its friends are, closer to where the nearby things are.", "tokens": [1080, 1855, 366, 11, 4966, 281, 689, 264, 11184, 721, 366, 13], "temperature": 0.0, "avg_logprob": -0.15451709875899755, "compression_ratio": 1.8412698412698412, "no_speech_prob": 1.1189412134626764e-06}, {"id": 716, "seek": 368922, "start": 3689.22, "end": 3694.04, "text": " And so if we keep doing this again and again, everything's going to move until it's right", "tokens": [400, 370, 498, 321, 1066, 884, 341, 797, 293, 797, 11, 1203, 311, 516, 281, 1286, 1826, 309, 311, 558], "temperature": 0.0, "avg_logprob": -0.13636399586995443, "compression_ratio": 1.577319587628866, "no_speech_prob": 5.989256806060439e-07}, {"id": 717, "seek": 368922, "start": 3694.04, "end": 3696.3999999999996, "text": " next to its friends.", "tokens": [958, 281, 1080, 1855, 13], "temperature": 0.0, "avg_logprob": -0.13636399586995443, "compression_ratio": 1.577319587628866, "no_speech_prob": 5.989256806060439e-07}, {"id": 718, "seek": 368922, "start": 3696.3999999999996, "end": 3704.64, "text": " So how do we take something which initially is a distance and make it so that the larger", "tokens": [407, 577, 360, 321, 747, 746, 597, 9105, 307, 257, 4560, 293, 652, 309, 370, 300, 264, 4833], "temperature": 0.0, "avg_logprob": -0.13636399586995443, "compression_ratio": 1.577319587628866, "no_speech_prob": 5.989256806060439e-07}, {"id": 719, "seek": 368922, "start": 3704.64, "end": 3707.3399999999997, "text": " distances have smaller weights?", "tokens": [22182, 362, 4356, 17443, 30], "temperature": 0.0, "avg_logprob": -0.13636399586995443, "compression_ratio": 1.577319587628866, "no_speech_prob": 5.989256806060439e-07}, {"id": 720, "seek": 368922, "start": 3707.3399999999997, "end": 3713.7599999999998, "text": " And the answer is we probably want a shape that looks something like that.", "tokens": [400, 264, 1867, 307, 321, 1391, 528, 257, 3909, 300, 1542, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.13636399586995443, "compression_ratio": 1.577319587628866, "no_speech_prob": 5.989256806060439e-07}, {"id": 721, "seek": 371376, "start": 3713.76, "end": 3720.2400000000002, "text": " In other words, Gaussian.", "tokens": [682, 661, 2283, 11, 39148, 13], "temperature": 0.0, "avg_logprob": -0.1538196421684103, "compression_ratio": 1.330708661417323, "no_speech_prob": 2.4824755655572517e-06}, {"id": 722, "seek": 371376, "start": 3720.2400000000002, "end": 3722.76, "text": " This is by no means the only shape you could choose.", "tokens": [639, 307, 538, 572, 1355, 264, 787, 3909, 291, 727, 2826, 13], "temperature": 0.0, "avg_logprob": -0.1538196421684103, "compression_ratio": 1.330708661417323, "no_speech_prob": 2.4824755655572517e-06}, {"id": 723, "seek": 371376, "start": 3722.76, "end": 3737.96, "text": " It would be equally valid to choose this shape, which is a triangle, at least half of one.", "tokens": [467, 576, 312, 12309, 7363, 281, 2826, 341, 3909, 11, 597, 307, 257, 13369, 11, 412, 1935, 1922, 295, 472, 13], "temperature": 0.0, "avg_logprob": -0.1538196421684103, "compression_ratio": 1.330708661417323, "no_speech_prob": 2.4824755655572517e-06}, {"id": 724, "seek": 373796, "start": 3737.96, "end": 3745.64, "text": " In general though, note that if we're going to multiply every point by one of these things", "tokens": [682, 2674, 1673, 11, 3637, 300, 498, 321, 434, 516, 281, 12972, 633, 935, 538, 472, 295, 613, 721], "temperature": 0.0, "avg_logprob": -0.12289667677605288, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.714317232261237e-07}, {"id": 725, "seek": 373796, "start": 3745.64, "end": 3751.52, "text": " and add them all together, it would be nice if all of our weights added to one, because", "tokens": [293, 909, 552, 439, 1214, 11, 309, 576, 312, 1481, 498, 439, 295, 527, 17443, 3869, 281, 472, 11, 570], "temperature": 0.0, "avg_logprob": -0.12289667677605288, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.714317232261237e-07}, {"id": 726, "seek": 373796, "start": 3751.52, "end": 3756.0, "text": " then we're going to end up with something that's of the same scale that we start with.", "tokens": [550, 321, 434, 516, 281, 917, 493, 365, 746, 300, 311, 295, 264, 912, 4373, 300, 321, 722, 365, 13], "temperature": 0.0, "avg_logprob": -0.12289667677605288, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.714317232261237e-07}, {"id": 727, "seek": 373796, "start": 3756.0, "end": 3765.48, "text": " So when you create one of these curves where it all adds up to one, generally speaking", "tokens": [407, 562, 291, 1884, 472, 295, 613, 19490, 689, 309, 439, 10860, 493, 281, 472, 11, 5101, 4124], "temperature": 0.0, "avg_logprob": -0.12289667677605288, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.714317232261237e-07}, {"id": 728, "seek": 376548, "start": 3765.48, "end": 3769.52, "text": " we call that a kernel.", "tokens": [321, 818, 300, 257, 28256, 13], "temperature": 0.0, "avg_logprob": -0.148350135567262, "compression_ratio": 1.6553191489361703, "no_speech_prob": 6.240891707420815e-06}, {"id": 729, "seek": 376548, "start": 3769.52, "end": 3775.6, "text": " And I mention this because you will see kernels everywhere, if you haven't already.", "tokens": [400, 286, 2152, 341, 570, 291, 486, 536, 23434, 1625, 5315, 11, 498, 291, 2378, 380, 1217, 13], "temperature": 0.0, "avg_logprob": -0.148350135567262, "compression_ratio": 1.6553191489361703, "no_speech_prob": 6.240891707420815e-06}, {"id": 730, "seek": 376548, "start": 3775.6, "end": 3778.12, "text": " Now that you've seen it, you'll see them everywhere.", "tokens": [823, 300, 291, 600, 1612, 309, 11, 291, 603, 536, 552, 5315, 13], "temperature": 0.0, "avg_logprob": -0.148350135567262, "compression_ratio": 1.6553191489361703, "no_speech_prob": 6.240891707420815e-06}, {"id": 731, "seek": 376548, "start": 3778.12, "end": 3784.76, "text": " In fact, kernel methods is a whole area of machine learning that in the late 90s basically", "tokens": [682, 1186, 11, 28256, 7150, 307, 257, 1379, 1859, 295, 3479, 2539, 300, 294, 264, 3469, 4289, 82, 1936], "temperature": 0.0, "avg_logprob": -0.148350135567262, "compression_ratio": 1.6553191489361703, "no_speech_prob": 6.240891707420815e-06}, {"id": 732, "seek": 376548, "start": 3784.76, "end": 3789.02, "text": " took over because it was so theoretically pure.", "tokens": [1890, 670, 570, 309, 390, 370, 29400, 6075, 13], "temperature": 0.0, "avg_logprob": -0.148350135567262, "compression_ratio": 1.6553191489361703, "no_speech_prob": 6.240891707420815e-06}, {"id": 733, "seek": 376548, "start": 3789.02, "end": 3792.2400000000002, "text": " And if you want to get published in conference proceedings, it's much more important to be", "tokens": [400, 498, 291, 528, 281, 483, 6572, 294, 7586, 37254, 11, 309, 311, 709, 544, 1021, 281, 312], "temperature": 0.0, "avg_logprob": -0.148350135567262, "compression_ratio": 1.6553191489361703, "no_speech_prob": 6.240891707420815e-06}, {"id": 734, "seek": 379224, "start": 3792.24, "end": 3796.4799999999996, "text": " theoretically pure than actually accurate.", "tokens": [29400, 6075, 813, 767, 8559, 13], "temperature": 0.0, "avg_logprob": -0.20751523367966276, "compression_ratio": 1.5982532751091703, "no_speech_prob": 4.495150278671645e-06}, {"id": 735, "seek": 379224, "start": 3796.4799999999996, "end": 3804.0, "text": " So for a long time kernel methods went out and neural networks in particular disappeared.", "tokens": [407, 337, 257, 938, 565, 28256, 7150, 1437, 484, 293, 18161, 9590, 294, 1729, 13954, 13], "temperature": 0.0, "avg_logprob": -0.20751523367966276, "compression_ratio": 1.5982532751091703, "no_speech_prob": 4.495150278671645e-06}, {"id": 736, "seek": 379224, "start": 3804.0, "end": 3808.68, "text": " Eventually people realized that accuracy was important as well, and in more recent times", "tokens": [17586, 561, 5334, 300, 14170, 390, 1021, 382, 731, 11, 293, 294, 544, 5162, 1413], "temperature": 0.0, "avg_logprob": -0.20751523367966276, "compression_ratio": 1.5982532751091703, "no_speech_prob": 4.495150278671645e-06}, {"id": 737, "seek": 379224, "start": 3808.68, "end": 3811.04, "text": " kernel methods are largely disappearing.", "tokens": [28256, 7150, 366, 11611, 34900, 13], "temperature": 0.0, "avg_logprob": -0.20751523367966276, "compression_ratio": 1.5982532751091703, "no_speech_prob": 4.495150278671645e-06}, {"id": 738, "seek": 379224, "start": 3811.04, "end": 3817.24, "text": " But you still see the idea of a kernel coming up very often because they're super useful", "tokens": [583, 291, 920, 536, 264, 1558, 295, 257, 28256, 1348, 493, 588, 2049, 570, 436, 434, 1687, 4420], "temperature": 0.0, "avg_logprob": -0.20751523367966276, "compression_ratio": 1.5982532751091703, "no_speech_prob": 4.495150278671645e-06}, {"id": 739, "seek": 379224, "start": 3817.24, "end": 3818.56, "text": " tools to have.", "tokens": [3873, 281, 362, 13], "temperature": 0.0, "avg_logprob": -0.20751523367966276, "compression_ratio": 1.5982532751091703, "no_speech_prob": 4.495150278671645e-06}, {"id": 740, "seek": 381856, "start": 3818.56, "end": 3823.16, "text": " They're basically something that lets you take a number, like in this case a distance,", "tokens": [814, 434, 1936, 746, 300, 6653, 291, 747, 257, 1230, 11, 411, 294, 341, 1389, 257, 4560, 11], "temperature": 0.0, "avg_logprob": -0.1292339268297252, "compression_ratio": 1.681992337164751, "no_speech_prob": 3.50083973899018e-06}, {"id": 741, "seek": 381856, "start": 3823.16, "end": 3828.68, "text": " and turn it into some other number where you can weight everything by that other number", "tokens": [293, 1261, 309, 666, 512, 661, 1230, 689, 291, 393, 3364, 1203, 538, 300, 661, 1230], "temperature": 0.0, "avg_logprob": -0.1292339268297252, "compression_ratio": 1.681992337164751, "no_speech_prob": 3.50083973899018e-06}, {"id": 742, "seek": 381856, "start": 3828.68, "end": 3832.56, "text": " and add them together to get a nice little weighted average.", "tokens": [293, 909, 552, 1214, 281, 483, 257, 1481, 707, 32807, 4274, 13], "temperature": 0.0, "avg_logprob": -0.1292339268297252, "compression_ratio": 1.681992337164751, "no_speech_prob": 3.50083973899018e-06}, {"id": 743, "seek": 381856, "start": 3832.56, "end": 3837.52, "text": " So in our case, we're going to use a Gaussian kernel.", "tokens": [407, 294, 527, 1389, 11, 321, 434, 516, 281, 764, 257, 39148, 28256, 13], "temperature": 0.0, "avg_logprob": -0.1292339268297252, "compression_ratio": 1.681992337164751, "no_speech_prob": 3.50083973899018e-06}, {"id": 744, "seek": 381856, "start": 3837.52, "end": 3841.44, "text": " The particular formula for a Gaussian doesn't matter.", "tokens": [440, 1729, 8513, 337, 257, 39148, 1177, 380, 1871, 13], "temperature": 0.0, "avg_logprob": -0.1292339268297252, "compression_ratio": 1.681992337164751, "no_speech_prob": 3.50083973899018e-06}, {"id": 745, "seek": 381856, "start": 3841.44, "end": 3846.44, "text": " I remember learning this formula in grade 10 and it was by far the most terrifying mathematical", "tokens": [286, 1604, 2539, 341, 8513, 294, 7204, 1266, 293, 309, 390, 538, 1400, 264, 881, 18106, 18894], "temperature": 0.0, "avg_logprob": -0.1292339268297252, "compression_ratio": 1.681992337164751, "no_speech_prob": 3.50083973899018e-06}, {"id": 746, "seek": 384644, "start": 3846.44, "end": 3849.7200000000003, "text": " formula I've ever seen, but it doesn't really matter.", "tokens": [8513, 286, 600, 1562, 1612, 11, 457, 309, 1177, 380, 534, 1871, 13], "temperature": 0.0, "avg_logprob": -0.16393219694799308, "compression_ratio": 1.6576576576576576, "no_speech_prob": 5.093683284940198e-06}, {"id": 747, "seek": 384644, "start": 3849.7200000000003, "end": 3854.56, "text": " For those of you that remember or have seen the Gaussian formula, you'll recognize it.", "tokens": [1171, 729, 295, 291, 300, 1604, 420, 362, 1612, 264, 39148, 8513, 11, 291, 603, 5521, 309, 13], "temperature": 0.0, "avg_logprob": -0.16393219694799308, "compression_ratio": 1.6576576576576576, "no_speech_prob": 5.093683284940198e-06}, {"id": 748, "seek": 384644, "start": 3854.56, "end": 3857.36, "text": " For those of you that haven't, it doesn't matter.", "tokens": [1171, 729, 295, 291, 300, 2378, 380, 11, 309, 1177, 380, 1871, 13], "temperature": 0.0, "avg_logprob": -0.16393219694799308, "compression_ratio": 1.6576576576576576, "no_speech_prob": 5.093683284940198e-06}, {"id": 749, "seek": 384644, "start": 3857.36, "end": 3863.48, "text": " But this is the function that draws that curve.", "tokens": [583, 341, 307, 264, 2445, 300, 20045, 300, 7605, 13], "temperature": 0.0, "avg_logprob": -0.16393219694799308, "compression_ratio": 1.6576576576576576, "no_speech_prob": 5.093683284940198e-06}, {"id": 750, "seek": 384644, "start": 3863.48, "end": 3871.14, "text": " So if we take every one of our distances and put it through the Gaussian, we will then", "tokens": [407, 498, 321, 747, 633, 472, 295, 527, 22182, 293, 829, 309, 807, 264, 39148, 11, 321, 486, 550], "temperature": 0.0, "avg_logprob": -0.16393219694799308, "compression_ratio": 1.6576576576576576, "no_speech_prob": 5.093683284940198e-06}, {"id": 751, "seek": 384644, "start": 3871.14, "end": 3875.32, "text": " get back a bunch of weights that add to 1.", "tokens": [483, 646, 257, 3840, 295, 17443, 300, 909, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.16393219694799308, "compression_ratio": 1.6576576576576576, "no_speech_prob": 5.093683284940198e-06}, {"id": 752, "seek": 387532, "start": 3875.32, "end": 3884.4, "text": " So then in the final step, we can multiply every one of our data points by that weight,", "tokens": [407, 550, 294, 264, 2572, 1823, 11, 321, 393, 12972, 633, 472, 295, 527, 1412, 2793, 538, 300, 3364, 11], "temperature": 0.0, "avg_logprob": -0.1262024552074831, "compression_ratio": 1.5084745762711864, "no_speech_prob": 4.029445790365571e-06}, {"id": 753, "seek": 387532, "start": 3884.4, "end": 3890.1200000000003, "text": " add them up and divide by the sum of the weights, in other words, take a weighted average.", "tokens": [909, 552, 493, 293, 9845, 538, 264, 2408, 295, 264, 17443, 11, 294, 661, 2283, 11, 747, 257, 32807, 4274, 13], "temperature": 0.0, "avg_logprob": -0.1262024552074831, "compression_ratio": 1.5084745762711864, "no_speech_prob": 4.029445790365571e-06}, {"id": 754, "seek": 387532, "start": 3890.1200000000003, "end": 3897.0, "text": " You'll notice that I had to be a bit careful about broadcasting here because I needed to", "tokens": [509, 603, 3449, 300, 286, 632, 281, 312, 257, 857, 5026, 466, 30024, 510, 570, 286, 2978, 281], "temperature": 0.0, "avg_logprob": -0.1262024552074831, "compression_ratio": 1.5084745762711864, "no_speech_prob": 4.029445790365571e-06}, {"id": 755, "seek": 389700, "start": 3897.0, "end": 3907.32, "text": " add a unit axis at the end of my dimensions, not at the start, so by default it adds unit", "tokens": [909, 257, 4985, 10298, 412, 264, 917, 295, 452, 12819, 11, 406, 412, 264, 722, 11, 370, 538, 7576, 309, 10860, 4985], "temperature": 0.0, "avg_logprob": -0.16807930520240297, "compression_ratio": 1.6570048309178744, "no_speech_prob": 3.785308535952936e-06}, {"id": 756, "seek": 389700, "start": 3907.32, "end": 3910.24, "text": " axes to the beginning when you do broadcasting.", "tokens": [35387, 281, 264, 2863, 562, 291, 360, 30024, 13], "temperature": 0.0, "avg_logprob": -0.16807930520240297, "compression_ratio": 1.6570048309178744, "no_speech_prob": 3.785308535952936e-06}, {"id": 757, "seek": 389700, "start": 3910.24, "end": 3913.28, "text": " That's why I had to do an expandims.", "tokens": [663, 311, 983, 286, 632, 281, 360, 364, 5268, 18857, 13], "temperature": 0.0, "avg_logprob": -0.16807930520240297, "compression_ratio": 1.6570048309178744, "no_speech_prob": 3.785308535952936e-06}, {"id": 758, "seek": 389700, "start": 3913.28, "end": 3918.48, "text": " If you're not clear on why this is, then that's a sign you definitely need to do some more", "tokens": [759, 291, 434, 406, 1850, 322, 983, 341, 307, 11, 550, 300, 311, 257, 1465, 291, 2138, 643, 281, 360, 512, 544], "temperature": 0.0, "avg_logprob": -0.16807930520240297, "compression_ratio": 1.6570048309178744, "no_speech_prob": 3.785308535952936e-06}, {"id": 759, "seek": 389700, "start": 3918.48, "end": 3921.08, "text": " playing around with broadcasting.", "tokens": [2433, 926, 365, 30024, 13], "temperature": 0.0, "avg_logprob": -0.16807930520240297, "compression_ratio": 1.6570048309178744, "no_speech_prob": 3.785308535952936e-06}, {"id": 760, "seek": 389700, "start": 3921.08, "end": 3923.52, "text": " So have a fiddle with that during the week.", "tokens": [407, 362, 257, 24553, 2285, 365, 300, 1830, 264, 1243, 13], "temperature": 0.0, "avg_logprob": -0.16807930520240297, "compression_ratio": 1.6570048309178744, "no_speech_prob": 3.785308535952936e-06}, {"id": 761, "seek": 392352, "start": 3923.52, "end": 3927.4, "text": " Feel free to ask if you're not clear after you've experimented.", "tokens": [14113, 1737, 281, 1029, 498, 291, 434, 406, 1850, 934, 291, 600, 5120, 292, 13], "temperature": 0.0, "avg_logprob": -0.1896481786455427, "compression_ratio": 1.4942528735632183, "no_speech_prob": 3.2377481602452463e-06}, {"id": 762, "seek": 392352, "start": 3927.4, "end": 3929.04, "text": " But this is just a weighted sum.", "tokens": [583, 341, 307, 445, 257, 32807, 2408, 13], "temperature": 0.0, "avg_logprob": -0.1896481786455427, "compression_ratio": 1.4942528735632183, "no_speech_prob": 3.2377481602452463e-06}, {"id": 763, "seek": 392352, "start": 3929.04, "end": 3942.32, "text": " So this is just doing sum of weights times x divided by sum of weights.", "tokens": [407, 341, 307, 445, 884, 2408, 295, 17443, 1413, 2031, 6666, 538, 2408, 295, 17443, 13], "temperature": 0.0, "avg_logprob": -0.1896481786455427, "compression_ratio": 1.4942528735632183, "no_speech_prob": 3.2377481602452463e-06}, {"id": 764, "seek": 392352, "start": 3942.32, "end": 3948.36, "text": " Importantly, there's a nice little thing that we can pass to a Gaussian, which is the thing", "tokens": [26391, 3627, 11, 456, 311, 257, 1481, 707, 551, 300, 321, 393, 1320, 281, 257, 39148, 11, 597, 307, 264, 551], "temperature": 0.0, "avg_logprob": -0.1896481786455427, "compression_ratio": 1.4942528735632183, "no_speech_prob": 3.2377481602452463e-06}, {"id": 765, "seek": 394836, "start": 3948.36, "end": 3956.56, "text": " that decides does it look like the thing I just drew, or does it look like this, or does", "tokens": [300, 14898, 775, 309, 574, 411, 264, 551, 286, 445, 12804, 11, 420, 775, 309, 574, 411, 341, 11, 420, 775], "temperature": 0.0, "avg_logprob": -0.14856102397140947, "compression_ratio": 1.8803827751196172, "no_speech_prob": 3.966971235058736e-06}, {"id": 766, "seek": 394836, "start": 3956.56, "end": 3959.88, "text": " it look like this.", "tokens": [309, 574, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.14856102397140947, "compression_ratio": 1.8803827751196172, "no_speech_prob": 3.966971235058736e-06}, {"id": 767, "seek": 394836, "start": 3959.88, "end": 3963.08, "text": " All of those things add up to one, they all have the same area underneath, but they're", "tokens": [1057, 295, 729, 721, 909, 493, 281, 472, 11, 436, 439, 362, 264, 912, 1859, 7223, 11, 457, 436, 434], "temperature": 0.0, "avg_logprob": -0.14856102397140947, "compression_ratio": 1.8803827751196172, "no_speech_prob": 3.966971235058736e-06}, {"id": 768, "seek": 394836, "start": 3963.08, "end": 3964.96, "text": " very different shapes.", "tokens": [588, 819, 10854, 13], "temperature": 0.0, "avg_logprob": -0.14856102397140947, "compression_ratio": 1.8803827751196172, "no_speech_prob": 3.966971235058736e-06}, {"id": 769, "seek": 394836, "start": 3964.96, "end": 3969.04, "text": " If we make it look like this, then what that's going to do is it's going to create a lot", "tokens": [759, 321, 652, 309, 574, 411, 341, 11, 550, 437, 300, 311, 516, 281, 360, 307, 309, 311, 516, 281, 1884, 257, 688], "temperature": 0.0, "avg_logprob": -0.14856102397140947, "compression_ratio": 1.8803827751196172, "no_speech_prob": 3.966971235058736e-06}, {"id": 770, "seek": 394836, "start": 3969.04, "end": 3973.6800000000003, "text": " more clusters because things that are really close to it are going to have really high", "tokens": [544, 23313, 570, 721, 300, 366, 534, 1998, 281, 309, 366, 516, 281, 362, 534, 1090], "temperature": 0.0, "avg_logprob": -0.14856102397140947, "compression_ratio": 1.8803827751196172, "no_speech_prob": 3.966971235058736e-06}, {"id": 771, "seek": 397368, "start": 3973.68, "end": 3978.68, "text": " weights, and everything else is going to have a tiny weight and be meaningless.", "tokens": [17443, 11, 293, 1203, 1646, 307, 516, 281, 362, 257, 5870, 3364, 293, 312, 33232, 13], "temperature": 0.0, "avg_logprob": -0.23117246745545186, "compression_ratio": 1.69, "no_speech_prob": 7.5279458542354405e-06}, {"id": 772, "seek": 397368, "start": 3978.68, "end": 3983.48, "text": " So if we use something like this, we're going to have much fewer clusters because even stuff", "tokens": [407, 498, 321, 764, 746, 411, 341, 11, 321, 434, 516, 281, 362, 709, 13366, 23313, 570, 754, 1507], "temperature": 0.0, "avg_logprob": -0.23117246745545186, "compression_ratio": 1.69, "no_speech_prob": 7.5279458542354405e-06}, {"id": 773, "seek": 397368, "start": 3983.48, "end": 3989.3999999999996, "text": " that's further away is going to have a higher weight from the weighted sum.", "tokens": [300, 311, 3052, 1314, 307, 516, 281, 362, 257, 2946, 3364, 490, 264, 32807, 2408, 13], "temperature": 0.0, "avg_logprob": -0.23117246745545186, "compression_ratio": 1.69, "no_speech_prob": 7.5279458542354405e-06}, {"id": 774, "seek": 397368, "start": 3989.3999999999996, "end": 4003.64, "text": " So the choice that you use for the kernel width, we actually here use BW being bandwidth,", "tokens": [407, 264, 3922, 300, 291, 764, 337, 264, 28256, 11402, 11, 321, 767, 510, 764, 363, 54, 885, 23647, 11], "temperature": 0.0, "avg_logprob": -0.23117246745545186, "compression_ratio": 1.69, "no_speech_prob": 7.5279458542354405e-06}, {"id": 775, "seek": 400364, "start": 4003.64, "end": 4006.92, "text": " and there's actually some cool ways to choose it.", "tokens": [293, 456, 311, 767, 512, 1627, 2098, 281, 2826, 309, 13], "temperature": 0.0, "avg_logprob": -0.24530779520670573, "compression_ratio": 1.5701357466063348, "no_speech_prob": 1.2606834388861898e-05}, {"id": 776, "seek": 400364, "start": 4006.92, "end": 4015.64, "text": " One simple way to choose it is to find out which size of bandwidth covers 1 third of", "tokens": [1485, 2199, 636, 281, 2826, 309, 307, 281, 915, 484, 597, 2744, 295, 23647, 10538, 502, 2636, 295], "temperature": 0.0, "avg_logprob": -0.24530779520670573, "compression_ratio": 1.5701357466063348, "no_speech_prob": 1.2606834388861898e-05}, {"id": 777, "seek": 400364, "start": 4015.64, "end": 4018.04, "text": " the data in your dataset.", "tokens": [264, 1412, 294, 428, 28872, 13], "temperature": 0.0, "avg_logprob": -0.24530779520670573, "compression_ratio": 1.5701357466063348, "no_speech_prob": 1.2606834388861898e-05}, {"id": 778, "seek": 400364, "start": 4018.04, "end": 4020.96, "text": " I think that's the approach that Scikit-learn uses.", "tokens": [286, 519, 300, 311, 264, 3109, 300, 16942, 22681, 12, 306, 1083, 4960, 13], "temperature": 0.0, "avg_logprob": -0.24530779520670573, "compression_ratio": 1.5701357466063348, "no_speech_prob": 1.2606834388861898e-05}, {"id": 779, "seek": 400364, "start": 4020.96, "end": 4029.16, "text": " So anyway, there are some ways that you can automatically figure out the bandwidth.", "tokens": [407, 4033, 11, 456, 366, 512, 2098, 300, 291, 393, 6772, 2573, 484, 264, 23647, 13], "temperature": 0.0, "avg_logprob": -0.24530779520670573, "compression_ratio": 1.5701357466063348, "no_speech_prob": 1.2606834388861898e-05}, {"id": 780, "seek": 400364, "start": 4029.16, "end": 4032.3599999999997, "text": " Just one of the very nice things about mean shift.", "tokens": [1449, 472, 295, 264, 588, 1481, 721, 466, 914, 5513, 13], "temperature": 0.0, "avg_logprob": -0.24530779520670573, "compression_ratio": 1.5701357466063348, "no_speech_prob": 1.2606834388861898e-05}, {"id": 781, "seek": 403236, "start": 4032.36, "end": 4037.6, "text": " So we just go through a bunch of times, 5 times, and each time we replace every point", "tokens": [407, 321, 445, 352, 807, 257, 3840, 295, 1413, 11, 1025, 1413, 11, 293, 1184, 565, 321, 7406, 633, 935], "temperature": 0.0, "avg_logprob": -0.15400925549593839, "compression_ratio": 1.5454545454545454, "no_speech_prob": 4.7108815124374814e-06}, {"id": 782, "seek": 403236, "start": 4037.6, "end": 4045.84, "text": " with its weighted average, weighted by this Gaussian kernel.", "tokens": [365, 1080, 32807, 4274, 11, 32807, 538, 341, 39148, 28256, 13], "temperature": 0.0, "avg_logprob": -0.15400925549593839, "compression_ratio": 1.5454545454545454, "no_speech_prob": 4.7108815124374814e-06}, {"id": 783, "seek": 403236, "start": 4045.84, "end": 4053.08, "text": " And so when we run this 5 times, it takes a second, and here's the results.", "tokens": [400, 370, 562, 321, 1190, 341, 1025, 1413, 11, 309, 2516, 257, 1150, 11, 293, 510, 311, 264, 3542, 13], "temperature": 0.0, "avg_logprob": -0.15400925549593839, "compression_ratio": 1.5454545454545454, "no_speech_prob": 4.7108815124374814e-06}, {"id": 784, "seek": 403236, "start": 4053.08, "end": 4057.28, "text": " I've offset everything by 1 just so that we can see it, otherwise it would be right on", "tokens": [286, 600, 18687, 1203, 538, 502, 445, 370, 300, 321, 393, 536, 309, 11, 5911, 309, 576, 312, 558, 322], "temperature": 0.0, "avg_logprob": -0.15400925549593839, "compression_ratio": 1.5454545454545454, "no_speech_prob": 4.7108815124374814e-06}, {"id": 785, "seek": 403236, "start": 4057.28, "end": 4058.28, "text": " top of the X.", "tokens": [1192, 295, 264, 1783, 13], "temperature": 0.0, "avg_logprob": -0.15400925549593839, "compression_ratio": 1.5454545454545454, "no_speech_prob": 4.7108815124374814e-06}, {"id": 786, "seek": 405828, "start": 4058.28, "end": 4062.96, "text": " So you can see that for nearly all of them, it's in exactly the right spot.", "tokens": [407, 291, 393, 536, 300, 337, 6217, 439, 295, 552, 11, 309, 311, 294, 2293, 264, 558, 4008, 13], "temperature": 0.0, "avg_logprob": -0.20051267411973742, "compression_ratio": 1.7023255813953488, "no_speech_prob": 3.6688384170702193e-06}, {"id": 787, "seek": 405828, "start": 4062.96, "end": 4066.5600000000004, "text": " Whereas for this cluster, let's just remind ourselves what that cluster looked like.", "tokens": [13813, 337, 341, 13630, 11, 718, 311, 445, 4160, 4175, 437, 300, 13630, 2956, 411, 13], "temperature": 0.0, "avg_logprob": -0.20051267411973742, "compression_ratio": 1.7023255813953488, "no_speech_prob": 3.6688384170702193e-06}, {"id": 788, "seek": 405828, "start": 4066.5600000000004, "end": 4073.6400000000003, "text": " These two clusters, this particular bandwidth, it decided to create one cluster for them", "tokens": [1981, 732, 23313, 11, 341, 1729, 23647, 11, 309, 3047, 281, 1884, 472, 13630, 337, 552], "temperature": 0.0, "avg_logprob": -0.20051267411973742, "compression_ratio": 1.7023255813953488, "no_speech_prob": 3.6688384170702193e-06}, {"id": 789, "seek": 405828, "start": 4073.6400000000003, "end": 4076.2400000000002, "text": " rather than two.", "tokens": [2831, 813, 732, 13], "temperature": 0.0, "avg_logprob": -0.20051267411973742, "compression_ratio": 1.7023255813953488, "no_speech_prob": 3.6688384170702193e-06}, {"id": 790, "seek": 405828, "start": 4076.2400000000002, "end": 4080.8, "text": " So this is kind of an example, whereas if we decreased our bandwidth, it would create", "tokens": [407, 341, 307, 733, 295, 364, 1365, 11, 9735, 498, 321, 24436, 527, 23647, 11, 309, 576, 1884], "temperature": 0.0, "avg_logprob": -0.20051267411973742, "compression_ratio": 1.7023255813953488, "no_speech_prob": 3.6688384170702193e-06}, {"id": 791, "seek": 405828, "start": 4080.8, "end": 4081.8, "text": " two clusters.", "tokens": [732, 23313, 13], "temperature": 0.0, "avg_logprob": -0.20051267411973742, "compression_ratio": 1.7023255813953488, "no_speech_prob": 3.6688384170702193e-06}, {"id": 792, "seek": 408180, "start": 4081.8, "end": 4088.52, "text": " There's no one right answer, that should be one or two.", "tokens": [821, 311, 572, 472, 558, 1867, 11, 300, 820, 312, 472, 420, 732, 13], "temperature": 0.0, "avg_logprob": -0.18243322064799647, "compression_ratio": 1.5336538461538463, "no_speech_prob": 4.936971890856512e-06}, {"id": 793, "seek": 408180, "start": 4088.52, "end": 4094.28, "text": " So one challenge with this is that it's kind of slow.", "tokens": [407, 472, 3430, 365, 341, 307, 300, 309, 311, 733, 295, 2964, 13], "temperature": 0.0, "avg_logprob": -0.18243322064799647, "compression_ratio": 1.5336538461538463, "no_speech_prob": 4.936971890856512e-06}, {"id": 794, "seek": 408180, "start": 4094.28, "end": 4101.0, "text": " So I thought, let's try and accelerate it for the GPU.", "tokens": [407, 286, 1194, 11, 718, 311, 853, 293, 21341, 309, 337, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.18243322064799647, "compression_ratio": 1.5336538461538463, "no_speech_prob": 4.936971890856512e-06}, {"id": 795, "seek": 408180, "start": 4101.0, "end": 4106.56, "text": " Because mean shift's not very cool, nobody seems to have implemented it for the GPU yet,", "tokens": [1436, 914, 5513, 311, 406, 588, 1627, 11, 5079, 2544, 281, 362, 12270, 309, 337, 264, 18407, 1939, 11], "temperature": 0.0, "avg_logprob": -0.18243322064799647, "compression_ratio": 1.5336538461538463, "no_speech_prob": 4.936971890856512e-06}, {"id": 796, "seek": 408180, "start": 4106.56, "end": 4110.88, "text": " or maybe it's just not a good idea, so I thought I'd use PyTorch.", "tokens": [420, 1310, 309, 311, 445, 406, 257, 665, 1558, 11, 370, 286, 1194, 286, 1116, 764, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.18243322064799647, "compression_ratio": 1.5336538461538463, "no_speech_prob": 4.936971890856512e-06}, {"id": 797, "seek": 411088, "start": 4110.88, "end": 4115.24, "text": " The reason I used PyTorch is because it really feels like writing PyTorch just feels like", "tokens": [440, 1778, 286, 1143, 9953, 51, 284, 339, 307, 570, 309, 534, 3417, 411, 3579, 9953, 51, 284, 339, 445, 3417, 411], "temperature": 0.0, "avg_logprob": -0.13742570650009883, "compression_ratio": 1.5408163265306123, "no_speech_prob": 1.2606865311681759e-05}, {"id": 798, "seek": 411088, "start": 4115.24, "end": 4117.76, "text": " writing NumPy, everything happens straight away.", "tokens": [3579, 22592, 47, 88, 11, 1203, 2314, 2997, 1314, 13], "temperature": 0.0, "avg_logprob": -0.13742570650009883, "compression_ratio": 1.5408163265306123, "no_speech_prob": 1.2606865311681759e-05}, {"id": 799, "seek": 411088, "start": 4117.76, "end": 4125.4800000000005, "text": " So I really hoped that I could take my original code and make it almost the same.", "tokens": [407, 286, 534, 19737, 300, 286, 727, 747, 452, 3380, 3089, 293, 652, 309, 1920, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.13742570650009883, "compression_ratio": 1.5408163265306123, "no_speech_prob": 1.2606865311681759e-05}, {"id": 800, "seek": 411088, "start": 4125.4800000000005, "end": 4133.56, "text": " And indeed, here is the entirety of mean shift in PyTorch.", "tokens": [400, 6451, 11, 510, 307, 264, 31557, 295, 914, 5513, 294, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.13742570650009883, "compression_ratio": 1.5408163265306123, "no_speech_prob": 1.2606865311681759e-05}, {"id": 801, "seek": 411088, "start": 4133.56, "end": 4134.56, "text": " So that's pretty cool.", "tokens": [407, 300, 311, 1238, 1627, 13], "temperature": 0.0, "avg_logprob": -0.13742570650009883, "compression_ratio": 1.5408163265306123, "no_speech_prob": 1.2606865311681759e-05}, {"id": 802, "seek": 413456, "start": 4134.56, "end": 4141.76, "text": " You can see rather than anywhere that I used to have Np, it now says Torch, Np.array is", "tokens": [509, 393, 536, 2831, 813, 4992, 300, 286, 1143, 281, 362, 426, 79, 11, 309, 586, 1619, 7160, 339, 11, 426, 79, 13, 2284, 320, 307], "temperature": 0.0, "avg_logprob": -0.1618798959134805, "compression_ratio": 1.5233644859813085, "no_speech_prob": 7.0718483584641945e-06}, {"id": 803, "seek": 413456, "start": 4141.76, "end": 4151.72, "text": " now Torch.floatTensor, Np.squareRoot is Torch.squareRoot, everything else is almost the same.", "tokens": [586, 7160, 339, 13, 43645, 267, 51, 23153, 11, 426, 79, 13, 33292, 543, 49, 6259, 307, 7160, 339, 13, 33292, 543, 49, 6259, 11, 1203, 1646, 307, 1920, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.1618798959134805, "compression_ratio": 1.5233644859813085, "no_speech_prob": 7.0718483584641945e-06}, {"id": 804, "seek": 413456, "start": 4151.72, "end": 4158.04, "text": " One issue is that Torch doesn't support broadcasting.", "tokens": [1485, 2734, 307, 300, 7160, 339, 1177, 380, 1406, 30024, 13], "temperature": 0.0, "avg_logprob": -0.1618798959134805, "compression_ratio": 1.5233644859813085, "no_speech_prob": 7.0718483584641945e-06}, {"id": 805, "seek": 413456, "start": 4158.04, "end": 4163.080000000001, "text": " So we'll talk more about this shortly in a couple of weeks, but basically I decided that's", "tokens": [407, 321, 603, 751, 544, 466, 341, 13392, 294, 257, 1916, 295, 3259, 11, 457, 1936, 286, 3047, 300, 311], "temperature": 0.0, "avg_logprob": -0.1618798959134805, "compression_ratio": 1.5233644859813085, "no_speech_prob": 7.0718483584641945e-06}, {"id": 806, "seek": 416308, "start": 4163.08, "end": 4166.88, "text": " not okay, so I wrote my own broadcasting library for PyTorch.", "tokens": [406, 1392, 11, 370, 286, 4114, 452, 1065, 30024, 6405, 337, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.20960590886134728, "compression_ratio": 1.711111111111111, "no_speech_prob": 2.078498073387891e-05}, {"id": 807, "seek": 416308, "start": 4166.88, "end": 4172.32, "text": " So rather than saying x, little x minus big X, I used sub for subtract, that's the subtract", "tokens": [407, 2831, 813, 1566, 2031, 11, 707, 2031, 3175, 955, 1783, 11, 286, 1143, 1422, 337, 16390, 11, 300, 311, 264, 16390], "temperature": 0.0, "avg_logprob": -0.20960590886134728, "compression_ratio": 1.711111111111111, "no_speech_prob": 2.078498073387891e-05}, {"id": 808, "seek": 416308, "start": 4172.32, "end": 4174.96, "text": " from my broadcasting library.", "tokens": [490, 452, 30024, 6405, 13], "temperature": 0.0, "avg_logprob": -0.20960590886134728, "compression_ratio": 1.711111111111111, "no_speech_prob": 2.078498073387891e-05}, {"id": 809, "seek": 416308, "start": 4174.96, "end": 4180.4, "text": " If you're curious, check out TorchUtils and you can see my broadcasting operations there.", "tokens": [759, 291, 434, 6369, 11, 1520, 484, 7160, 339, 52, 83, 4174, 293, 291, 393, 536, 452, 30024, 7705, 456, 13], "temperature": 0.0, "avg_logprob": -0.20960590886134728, "compression_ratio": 1.711111111111111, "no_speech_prob": 2.078498073387891e-05}, {"id": 810, "seek": 416308, "start": 4180.4, "end": 4185.24, "text": " But basically if you use those, you can see the same for multiplication, it'll do all", "tokens": [583, 1936, 498, 291, 764, 729, 11, 291, 393, 536, 264, 912, 337, 27290, 11, 309, 603, 360, 439], "temperature": 0.0, "avg_logprob": -0.20960590886134728, "compression_ratio": 1.711111111111111, "no_speech_prob": 2.078498073387891e-05}, {"id": 811, "seek": 416308, "start": 4185.24, "end": 4191.4, "text": " the broadcasting for you.", "tokens": [264, 30024, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.20960590886134728, "compression_ratio": 1.711111111111111, "no_speech_prob": 2.078498073387891e-05}, {"id": 812, "seek": 419140, "start": 4191.4, "end": 4202.16, "text": " So as you can see, this looks basically identical to the previous code, but it takes longer.", "tokens": [407, 382, 291, 393, 536, 11, 341, 1542, 1936, 14800, 281, 264, 3894, 3089, 11, 457, 309, 2516, 2854, 13], "temperature": 0.0, "avg_logprob": -0.13485289982386997, "compression_ratio": 1.4201183431952662, "no_speech_prob": 3.0894832434569253e-06}, {"id": 813, "seek": 419140, "start": 4202.16, "end": 4205.32, "text": " So that's not ideal.", "tokens": [407, 300, 311, 406, 7157, 13], "temperature": 0.0, "avg_logprob": -0.13485289982386997, "compression_ratio": 1.4201183431952662, "no_speech_prob": 3.0894832434569253e-06}, {"id": 814, "seek": 419140, "start": 4205.32, "end": 4208.08, "text": " One problem here is that I'm not using CUDA.", "tokens": [1485, 1154, 510, 307, 300, 286, 478, 406, 1228, 29777, 7509, 13], "temperature": 0.0, "avg_logprob": -0.13485289982386997, "compression_ratio": 1.4201183431952662, "no_speech_prob": 3.0894832434569253e-06}, {"id": 815, "seek": 419140, "start": 4208.08, "end": 4215.679999999999, "text": " So I could easily fix that by adding.cuda to my x, but that made it slower still.", "tokens": [407, 286, 727, 3612, 3191, 300, 538, 5127, 2411, 66, 11152, 281, 452, 2031, 11, 457, 300, 1027, 309, 14009, 920, 13], "temperature": 0.0, "avg_logprob": -0.13485289982386997, "compression_ratio": 1.4201183431952662, "no_speech_prob": 3.0894832434569253e-06}, {"id": 816, "seek": 421568, "start": 4215.68, "end": 4222.200000000001, "text": " The reason why is that all the work's being done in this for loop, and PyTorch doesn't", "tokens": [440, 1778, 983, 307, 300, 439, 264, 589, 311, 885, 1096, 294, 341, 337, 6367, 11, 293, 9953, 51, 284, 339, 1177, 380], "temperature": 0.0, "avg_logprob": -0.1640411949157715, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.123373405993334e-06}, {"id": 817, "seek": 421568, "start": 4222.200000000001, "end": 4224.76, "text": " accelerate for loops.", "tokens": [21341, 337, 16121, 13], "temperature": 0.0, "avg_logprob": -0.1640411949157715, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.123373405993334e-06}, {"id": 818, "seek": 421568, "start": 4224.76, "end": 4233.0, "text": " Each run through a for loop in PyTorch is basically calling a new CUDA kernel each time", "tokens": [6947, 1190, 807, 257, 337, 6367, 294, 9953, 51, 284, 339, 307, 1936, 5141, 257, 777, 29777, 7509, 28256, 1184, 565], "temperature": 0.0, "avg_logprob": -0.1640411949157715, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.123373405993334e-06}, {"id": 819, "seek": 421568, "start": 4233.0, "end": 4234.0, "text": " you're going through.", "tokens": [291, 434, 516, 807, 13], "temperature": 0.0, "avg_logprob": -0.1640411949157715, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.123373405993334e-06}, {"id": 820, "seek": 421568, "start": 4234.0, "end": 4241.360000000001, "text": " It takes a certain amount of time to even launch a CUDA kernel.", "tokens": [467, 2516, 257, 1629, 2372, 295, 565, 281, 754, 4025, 257, 29777, 7509, 28256, 13], "temperature": 0.0, "avg_logprob": -0.1640411949157715, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.123373405993334e-06}, {"id": 821, "seek": 421568, "start": 4241.360000000001, "end": 4244.84, "text": " When I'm saying CUDA kernel, this is a different usage of the word kernel.", "tokens": [1133, 286, 478, 1566, 29777, 7509, 28256, 11, 341, 307, 257, 819, 14924, 295, 264, 1349, 28256, 13], "temperature": 0.0, "avg_logprob": -0.1640411949157715, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.123373405993334e-06}, {"id": 822, "seek": 424484, "start": 4244.84, "end": 4250.84, "text": " In CUDA, kernel refers to a little piece of code that runs on the GPU.", "tokens": [682, 29777, 7509, 11, 28256, 14942, 281, 257, 707, 2522, 295, 3089, 300, 6676, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.1594948922434161, "compression_ratio": 1.3795180722891567, "no_speech_prob": 1.1478707165224478e-05}, {"id": 823, "seek": 424484, "start": 4250.84, "end": 4255.6, "text": " So it's launching a little GPU process every time through the for loop, which takes quite", "tokens": [407, 309, 311, 18354, 257, 707, 18407, 1399, 633, 565, 807, 264, 337, 6367, 11, 597, 2516, 1596], "temperature": 0.0, "avg_logprob": -0.1594948922434161, "compression_ratio": 1.3795180722891567, "no_speech_prob": 1.1478707165224478e-05}, {"id": 824, "seek": 424484, "start": 4255.6, "end": 4265.68, "text": " a bit of time, and it's also having to copy data all over the place.", "tokens": [257, 857, 295, 565, 11, 293, 309, 311, 611, 1419, 281, 5055, 1412, 439, 670, 264, 1081, 13], "temperature": 0.0, "avg_logprob": -0.1594948922434161, "compression_ratio": 1.3795180722891567, "no_speech_prob": 1.1478707165224478e-05}, {"id": 825, "seek": 426568, "start": 4265.68, "end": 4274.200000000001, "text": " So what I then tried to do was to make it faster.", "tokens": [407, 437, 286, 550, 3031, 281, 360, 390, 281, 652, 309, 4663, 13], "temperature": 0.0, "avg_logprob": -0.1282056754743549, "compression_ratio": 1.4662162162162162, "no_speech_prob": 1.5779573914187495e-06}, {"id": 826, "seek": 426568, "start": 4274.200000000001, "end": 4278.6, "text": " The trick is to do it by minibatch.", "tokens": [440, 4282, 307, 281, 360, 309, 538, 923, 897, 852, 13], "temperature": 0.0, "avg_logprob": -0.1282056754743549, "compression_ratio": 1.4662162162162162, "no_speech_prob": 1.5779573914187495e-06}, {"id": 827, "seek": 426568, "start": 4278.6, "end": 4286.56, "text": " So each time through the loop, we don't want to do just one piece of data, but a minibatch", "tokens": [407, 1184, 565, 807, 264, 6367, 11, 321, 500, 380, 528, 281, 360, 445, 472, 2522, 295, 1412, 11, 457, 257, 923, 897, 852], "temperature": 0.0, "avg_logprob": -0.1282056754743549, "compression_ratio": 1.4662162162162162, "no_speech_prob": 1.5779573914187495e-06}, {"id": 828, "seek": 426568, "start": 4286.56, "end": 4288.02, "text": " of data.", "tokens": [295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1282056754743549, "compression_ratio": 1.4662162162162162, "no_speech_prob": 1.5779573914187495e-06}, {"id": 829, "seek": 426568, "start": 4288.02, "end": 4291.240000000001, "text": " So here are the changes I made.", "tokens": [407, 510, 366, 264, 2962, 286, 1027, 13], "temperature": 0.0, "avg_logprob": -0.1282056754743549, "compression_ratio": 1.4662162162162162, "no_speech_prob": 1.5779573914187495e-06}, {"id": 830, "seek": 429124, "start": 4291.24, "end": 4299.62, "text": " The main one was that my for i now jumps through one batch size at a time.", "tokens": [440, 2135, 472, 390, 300, 452, 337, 741, 586, 16704, 807, 472, 15245, 2744, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.14987829959753787, "compression_ratio": 1.3509933774834437, "no_speech_prob": 1.2411444004101213e-05}, {"id": 831, "seek": 429124, "start": 4299.62, "end": 4307.28, "text": " So I'm going to go not 0.123, but 0.1632.", "tokens": [407, 286, 478, 516, 281, 352, 406, 1958, 13, 16, 9356, 11, 457, 1958, 13, 6866, 11440, 13], "temperature": 0.0, "avg_logprob": -0.14987829959753787, "compression_ratio": 1.3509933774834437, "no_speech_prob": 1.2411444004101213e-05}, {"id": 832, "seek": 429124, "start": 4307.28, "end": 4318.44, "text": " So I now need to create a slice which is from i to i plus batch size, unless we've gone", "tokens": [407, 286, 586, 643, 281, 1884, 257, 13153, 597, 307, 490, 741, 281, 741, 1804, 15245, 2744, 11, 5969, 321, 600, 2780], "temperature": 0.0, "avg_logprob": -0.14987829959753787, "compression_ratio": 1.3509933774834437, "no_speech_prob": 1.2411444004101213e-05}, {"id": 833, "seek": 431844, "start": 4318.44, "end": 4322.599999999999, "text": " past the end of the data, which is just as far as n.", "tokens": [1791, 264, 917, 295, 264, 1412, 11, 597, 307, 445, 382, 1400, 382, 297, 13], "temperature": 0.0, "avg_logprob": -0.1532018566131592, "compression_ratio": 1.617391304347826, "no_speech_prob": 4.565944436762948e-06}, {"id": 834, "seek": 431844, "start": 4322.599999999999, "end": 4328.16, "text": " So this is going to refer to the slice of data that we're interested in.", "tokens": [407, 341, 307, 516, 281, 2864, 281, 264, 13153, 295, 1412, 300, 321, 434, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.1532018566131592, "compression_ratio": 1.617391304347826, "no_speech_prob": 4.565944436762948e-06}, {"id": 835, "seek": 431844, "start": 4328.16, "end": 4335.839999999999, "text": " So what we can now do is say x with that slice to grab back all of the data in this minibatch.", "tokens": [407, 437, 321, 393, 586, 360, 307, 584, 2031, 365, 300, 13153, 281, 4444, 646, 439, 295, 264, 1412, 294, 341, 923, 897, 852, 13], "temperature": 0.0, "avg_logprob": -0.1532018566131592, "compression_ratio": 1.617391304347826, "no_speech_prob": 4.565944436762948e-06}, {"id": 836, "seek": 431844, "start": 4335.839999999999, "end": 4342.0, "text": " And so then I have to create a special version of, I can't just say subtract anymore, I need", "tokens": [400, 370, 550, 286, 362, 281, 1884, 257, 2121, 3037, 295, 11, 286, 393, 380, 445, 584, 16390, 3602, 11, 286, 643], "temperature": 0.0, "avg_logprob": -0.1532018566131592, "compression_ratio": 1.617391304347826, "no_speech_prob": 4.565944436762948e-06}, {"id": 837, "seek": 431844, "start": 4342.0, "end": 4344.639999999999, "text": " to think carefully about the broadcasting operations here.", "tokens": [281, 519, 7500, 466, 264, 30024, 7705, 510, 13], "temperature": 0.0, "avg_logprob": -0.1532018566131592, "compression_ratio": 1.617391304347826, "no_speech_prob": 4.565944436762948e-06}, {"id": 838, "seek": 434464, "start": 4344.64, "end": 4351.64, "text": " I'm going to return a matrix, let's say batch size is 32, I'm going to have 32 rows, and", "tokens": [286, 478, 516, 281, 2736, 257, 8141, 11, 718, 311, 584, 15245, 2744, 307, 8858, 11, 286, 478, 516, 281, 362, 8858, 13241, 11, 293], "temperature": 0.0, "avg_logprob": -0.16271209716796875, "compression_ratio": 1.605263157894737, "no_speech_prob": 4.425479346537031e-06}, {"id": 839, "seek": 434464, "start": 4351.64, "end": 4355.160000000001, "text": " then let's say n is 1000, it'll be 1000 columns.", "tokens": [550, 718, 311, 584, 297, 307, 9714, 11, 309, 603, 312, 9714, 13766, 13], "temperature": 0.0, "avg_logprob": -0.16271209716796875, "compression_ratio": 1.605263157894737, "no_speech_prob": 4.425479346537031e-06}, {"id": 840, "seek": 434464, "start": 4355.160000000001, "end": 4360.88, "text": " That shows me how far away each thing in my batch is from every piece of data.", "tokens": [663, 3110, 385, 577, 1400, 1314, 1184, 551, 294, 452, 15245, 307, 490, 633, 2522, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.16271209716796875, "compression_ratio": 1.605263157894737, "no_speech_prob": 4.425479346537031e-06}, {"id": 841, "seek": 434464, "start": 4360.88, "end": 4366.8, "text": " So when we do things a batch at a time, you're basically adding another axis to all of your", "tokens": [407, 562, 321, 360, 721, 257, 15245, 412, 257, 565, 11, 291, 434, 1936, 5127, 1071, 10298, 281, 439, 295, 428], "temperature": 0.0, "avg_logprob": -0.16271209716796875, "compression_ratio": 1.605263157894737, "no_speech_prob": 4.425479346537031e-06}, {"id": 842, "seek": 434464, "start": 4366.8, "end": 4367.8, "text": " tensors.", "tokens": [10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.16271209716796875, "compression_ratio": 1.605263157894737, "no_speech_prob": 4.425479346537031e-06}, {"id": 843, "seek": 434464, "start": 4367.8, "end": 4371.200000000001, "text": " Suddenly now you have a batch axis all the time.", "tokens": [21194, 586, 291, 362, 257, 15245, 10298, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.16271209716796875, "compression_ratio": 1.605263157894737, "no_speech_prob": 4.425479346537031e-06}, {"id": 844, "seek": 437120, "start": 4371.2, "end": 4374.5199999999995, "text": " And when we've been doing deep learning, that's been something I think we've gotten pretty", "tokens": [400, 562, 321, 600, 668, 884, 2452, 2539, 11, 300, 311, 668, 746, 286, 519, 321, 600, 5768, 1238], "temperature": 0.0, "avg_logprob": -0.22352374118307364, "compression_ratio": 1.4936170212765958, "no_speech_prob": 5.50758022654918e-06}, {"id": 845, "seek": 437120, "start": 4374.5199999999995, "end": 4375.5199999999995, "text": " used to.", "tokens": [1143, 281, 13], "temperature": 0.0, "avg_logprob": -0.22352374118307364, "compression_ratio": 1.4936170212765958, "no_speech_prob": 5.50758022654918e-06}, {"id": 846, "seek": 437120, "start": 4375.5199999999995, "end": 4379.84, "text": " The first axis in all of our tensors has always been a batch axis.", "tokens": [440, 700, 10298, 294, 439, 295, 527, 10688, 830, 575, 1009, 668, 257, 15245, 10298, 13], "temperature": 0.0, "avg_logprob": -0.22352374118307364, "compression_ratio": 1.4936170212765958, "no_speech_prob": 5.50758022654918e-06}, {"id": 847, "seek": 437120, "start": 4379.84, "end": 4383.0, "text": " So now we're writing our own GPU accelerated algorithm.", "tokens": [407, 586, 321, 434, 3579, 527, 1065, 18407, 29763, 9284, 13], "temperature": 0.0, "avg_logprob": -0.22352374118307364, "compression_ratio": 1.4936170212765958, "no_speech_prob": 5.50758022654918e-06}, {"id": 848, "seek": 437120, "start": 4383.0, "end": 4386.2, "text": " Can you believe how crazy this is?", "tokens": [1664, 291, 1697, 577, 3219, 341, 307, 30], "temperature": 0.0, "avg_logprob": -0.22352374118307364, "compression_ratio": 1.4936170212765958, "no_speech_prob": 5.50758022654918e-06}, {"id": 849, "seek": 437120, "start": 4386.2, "end": 4396.32, "text": " Two years ago, if you Google for K means Buddha or K means GPU, you get back research studies", "tokens": [4453, 924, 2057, 11, 498, 291, 3329, 337, 591, 1355, 16375, 420, 591, 1355, 18407, 11, 291, 483, 646, 2132, 5313], "temperature": 0.0, "avg_logprob": -0.22352374118307364, "compression_ratio": 1.4936170212765958, "no_speech_prob": 5.50758022654918e-06}, {"id": 850, "seek": 439632, "start": 4396.32, "end": 4404.04, "text": " where people write papers about how to put these algorithm GPUs, because it was hard.", "tokens": [689, 561, 2464, 10577, 466, 577, 281, 829, 613, 9284, 18407, 82, 11, 570, 309, 390, 1152, 13], "temperature": 0.0, "avg_logprob": -0.22589824199676514, "compression_ratio": 1.4603960396039604, "no_speech_prob": 2.123370904882904e-06}, {"id": 851, "seek": 439632, "start": 4404.04, "end": 4407.5599999999995, "text": " And here's a page of code that does it.", "tokens": [400, 510, 311, 257, 3028, 295, 3089, 300, 775, 309, 13], "temperature": 0.0, "avg_logprob": -0.22589824199676514, "compression_ratio": 1.4603960396039604, "no_speech_prob": 2.123370904882904e-06}, {"id": 852, "seek": 439632, "start": 4407.5599999999995, "end": 4410.32, "text": " This is crazy, this is possible, but here we are.", "tokens": [639, 307, 3219, 11, 341, 307, 1944, 11, 457, 510, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.22589824199676514, "compression_ratio": 1.4603960396039604, "no_speech_prob": 2.123370904882904e-06}, {"id": 853, "seek": 439632, "start": 4410.32, "end": 4417.0, "text": " We have built a batch-by-batch GPU accelerated main shift algorithm.", "tokens": [492, 362, 3094, 257, 15245, 12, 2322, 12, 65, 852, 18407, 29763, 2135, 5513, 9284, 13], "temperature": 0.0, "avg_logprob": -0.22589824199676514, "compression_ratio": 1.4603960396039604, "no_speech_prob": 2.123370904882904e-06}, {"id": 854, "seek": 439632, "start": 4417.0, "end": 4420.84, "text": " So the basic distance formula is exactly the same.", "tokens": [407, 264, 3875, 4560, 8513, 307, 2293, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.22589824199676514, "compression_ratio": 1.4603960396039604, "no_speech_prob": 2.123370904882904e-06}, {"id": 855, "seek": 442084, "start": 4420.84, "end": 4426.88, "text": " I just have to be careful about where I add it.", "tokens": [286, 445, 362, 281, 312, 5026, 466, 689, 286, 909, 309, 13], "temperature": 0.0, "avg_logprob": -0.3132657137784091, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.7502701666671783e-05}, {"id": 856, "seek": 442084, "start": 4426.88, "end": 4428.4800000000005, "text": " Unsqueeze is the same as expandims.", "tokens": [25017, 1077, 10670, 307, 264, 912, 382, 5268, 18857, 13], "temperature": 0.0, "avg_logprob": -0.3132657137784091, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.7502701666671783e-05}, {"id": 857, "seek": 442084, "start": 4428.4800000000005, "end": 4430.76, "text": " So I just have to be careful about where I add my unit axes.", "tokens": [407, 286, 445, 362, 281, 312, 5026, 466, 689, 286, 909, 452, 4985, 35387, 13], "temperature": 0.0, "avg_logprob": -0.3132657137784091, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.7502701666671783e-05}, {"id": 858, "seek": 442084, "start": 4430.76, "end": 4435.52, "text": " Add it to the first axis of one bit and the second axis of the other bit.", "tokens": [5349, 309, 281, 264, 700, 10298, 295, 472, 857, 293, 264, 1150, 10298, 295, 264, 661, 857, 13], "temperature": 0.0, "avg_logprob": -0.3132657137784091, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.7502701666671783e-05}, {"id": 859, "seek": 442084, "start": 4435.52, "end": 4440.12, "text": " So that's going to subtract every one of these from every one of these.", "tokens": [407, 300, 311, 516, 281, 16390, 633, 472, 295, 613, 490, 633, 472, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.3132657137784091, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.7502701666671783e-05}, {"id": 860, "seek": 442084, "start": 4440.12, "end": 4442.12, "text": " Return a matrix.", "tokens": [24350, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.3132657137784091, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.7502701666671783e-05}, {"id": 861, "seek": 442084, "start": 4442.12, "end": 4449.08, "text": " Again, this is like a really good time to look at this and think why does this broadcasting", "tokens": [3764, 11, 341, 307, 411, 257, 534, 665, 565, 281, 574, 412, 341, 293, 519, 983, 775, 341, 30024], "temperature": 0.0, "avg_logprob": -0.3132657137784091, "compression_ratio": 1.7733333333333334, "no_speech_prob": 1.7502701666671783e-05}, {"id": 862, "seek": 444908, "start": 4449.08, "end": 4455.6, "text": " work because this is getting more and more complex broadcasting.", "tokens": [589, 570, 341, 307, 1242, 544, 293, 544, 3997, 30024, 13], "temperature": 0.0, "avg_logprob": -0.09343535741170247, "compression_ratio": 1.4875621890547264, "no_speech_prob": 1.061600778484717e-05}, {"id": 863, "seek": 444908, "start": 4455.6, "end": 4459.92, "text": " And hopefully you can now see the value of broadcasting.", "tokens": [400, 4696, 291, 393, 586, 536, 264, 2158, 295, 30024, 13], "temperature": 0.0, "avg_logprob": -0.09343535741170247, "compression_ratio": 1.4875621890547264, "no_speech_prob": 1.061600778484717e-05}, {"id": 864, "seek": 444908, "start": 4459.92, "end": 4466.6, "text": " Not only did I get to avoid writing a pair of nested for loops here, but I also got to", "tokens": [1726, 787, 630, 286, 483, 281, 5042, 3579, 257, 6119, 295, 15646, 292, 337, 16121, 510, 11, 457, 286, 611, 658, 281], "temperature": 0.0, "avg_logprob": -0.09343535741170247, "compression_ratio": 1.4875621890547264, "no_speech_prob": 1.061600778484717e-05}, {"id": 865, "seek": 444908, "start": 4466.6, "end": 4475.64, "text": " do this all on the GPU in a single operation, so I've made this thousands of times faster.", "tokens": [360, 341, 439, 322, 264, 18407, 294, 257, 2167, 6916, 11, 370, 286, 600, 1027, 341, 5383, 295, 1413, 4663, 13], "temperature": 0.0, "avg_logprob": -0.09343535741170247, "compression_ratio": 1.4875621890547264, "no_speech_prob": 1.061600778484717e-05}, {"id": 866, "seek": 447564, "start": 4475.64, "end": 4491.160000000001, "text": " So here is a single operation which does that entire matrix subtraction.", "tokens": [407, 510, 307, 257, 2167, 6916, 597, 775, 300, 2302, 8141, 16390, 313, 13], "temperature": 0.0, "avg_logprob": -0.2166700522104899, "compression_ratio": 1.475, "no_speech_prob": 2.6688212528824806e-05}, {"id": 867, "seek": 447564, "start": 4491.160000000001, "end": 4494.6, "text": " So that's our batch-wise distance function.", "tokens": [407, 300, 311, 527, 15245, 12, 3711, 4560, 2445, 13], "temperature": 0.0, "avg_logprob": -0.2166700522104899, "compression_ratio": 1.475, "no_speech_prob": 2.6688212528824806e-05}, {"id": 868, "seek": 447564, "start": 4494.6, "end": 4496.56, "text": " We then chuck that into a Gaussian.", "tokens": [492, 550, 20870, 300, 666, 257, 39148, 13], "temperature": 0.0, "avg_logprob": -0.2166700522104899, "compression_ratio": 1.475, "no_speech_prob": 2.6688212528824806e-05}, {"id": 869, "seek": 447564, "start": 4496.56, "end": 4504.92, "text": " And because this is just element-wise, the Gaussian function hasn't changed at all.", "tokens": [400, 570, 341, 307, 445, 4478, 12, 3711, 11, 264, 39148, 2445, 6132, 380, 3105, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.2166700522104899, "compression_ratio": 1.475, "no_speech_prob": 2.6688212528824806e-05}, {"id": 870, "seek": 450492, "start": 4504.92, "end": 4513.52, "text": " And then I've got my weighted sum and then divide that by the sum of weights.", "tokens": [400, 550, 286, 600, 658, 452, 32807, 2408, 293, 550, 9845, 300, 538, 264, 2408, 295, 17443, 13], "temperature": 0.0, "avg_logprob": -0.21925091487105175, "compression_ratio": 1.5253456221198156, "no_speech_prob": 5.0147045840276405e-06}, {"id": 871, "seek": 450492, "start": 4513.52, "end": 4515.52, "text": " So that's basically the algorithm.", "tokens": [407, 300, 311, 1936, 264, 9284, 13], "temperature": 0.0, "avg_logprob": -0.21925091487105175, "compression_ratio": 1.5253456221198156, "no_speech_prob": 5.0147045840276405e-06}, {"id": 872, "seek": 450492, "start": 4515.52, "end": 4524.4400000000005, "text": " So previously for my NumPy version it took a second, now it's 48 milliseconds.", "tokens": [407, 8046, 337, 452, 22592, 47, 88, 3037, 309, 1890, 257, 1150, 11, 586, 309, 311, 11174, 34184, 13], "temperature": 0.0, "avg_logprob": -0.21925091487105175, "compression_ratio": 1.5253456221198156, "no_speech_prob": 5.0147045840276405e-06}, {"id": 873, "seek": 450492, "start": 4524.4400000000005, "end": 4528.4800000000005, "text": " So we've just sped that up by 20 seconds.", "tokens": [407, 321, 600, 445, 637, 292, 300, 493, 538, 945, 3949, 13], "temperature": 0.0, "avg_logprob": -0.21925091487105175, "compression_ratio": 1.5253456221198156, "no_speech_prob": 5.0147045840276405e-06}, {"id": 874, "seek": 450492, "start": 4528.4800000000005, "end": 4529.4800000000005, "text": " Question.", "tokens": [14464, 13], "temperature": 0.0, "avg_logprob": -0.21925091487105175, "compression_ratio": 1.5253456221198156, "no_speech_prob": 5.0147045840276405e-06}, {"id": 875, "seek": 450492, "start": 4529.4800000000005, "end": 4534.88, "text": " I get how batching helps with locality in cache, but I do not quite follow how it helps", "tokens": [286, 483, 577, 15245, 278, 3665, 365, 1628, 1860, 294, 19459, 11, 457, 286, 360, 406, 1596, 1524, 577, 309, 3665], "temperature": 0.0, "avg_logprob": -0.21925091487105175, "compression_ratio": 1.5253456221198156, "no_speech_prob": 5.0147045840276405e-06}, {"id": 876, "seek": 453488, "start": 4534.88, "end": 4542.0, "text": " otherwise, especially with respect to accelerating the for loop.", "tokens": [5911, 11, 2318, 365, 3104, 281, 34391, 264, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.1956020257411859, "compression_ratio": 1.7516339869281046, "no_speech_prob": 1.4510400433209725e-05}, {"id": 877, "seek": 453488, "start": 4542.0, "end": 4547.64, "text": " In PyTorch, the for loop is not run on the GPU.", "tokens": [682, 9953, 51, 284, 339, 11, 264, 337, 6367, 307, 406, 1190, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.1956020257411859, "compression_ratio": 1.7516339869281046, "no_speech_prob": 1.4510400433209725e-05}, {"id": 878, "seek": 453488, "start": 4547.64, "end": 4549.84, "text": " The for loop is run on your CPU.", "tokens": [440, 337, 6367, 307, 1190, 322, 428, 13199, 13], "temperature": 0.0, "avg_logprob": -0.1956020257411859, "compression_ratio": 1.7516339869281046, "no_speech_prob": 1.4510400433209725e-05}, {"id": 879, "seek": 453488, "start": 4549.84, "end": 4555.28, "text": " And your CPU goes through each step of the for loop and calls the GPU to say, do this", "tokens": [400, 428, 13199, 1709, 807, 1184, 1823, 295, 264, 337, 6367, 293, 5498, 264, 18407, 281, 584, 11, 360, 341], "temperature": 0.0, "avg_logprob": -0.1956020257411859, "compression_ratio": 1.7516339869281046, "no_speech_prob": 1.4510400433209725e-05}, {"id": 880, "seek": 453488, "start": 4555.28, "end": 4558.16, "text": " thing, do this thing, do this thing.", "tokens": [551, 11, 360, 341, 551, 11, 360, 341, 551, 13], "temperature": 0.0, "avg_logprob": -0.1956020257411859, "compression_ratio": 1.7516339869281046, "no_speech_prob": 1.4510400433209725e-05}, {"id": 881, "seek": 455816, "start": 4558.16, "end": 4565.76, "text": " So this is not to say you can't accelerate this in TensorFlow in a similar way.", "tokens": [407, 341, 307, 406, 281, 584, 291, 393, 380, 21341, 341, 294, 37624, 294, 257, 2531, 636, 13], "temperature": 0.0, "avg_logprob": -0.15259255495938387, "compression_ratio": 1.6024590163934427, "no_speech_prob": 9.874617035166011e-07}, {"id": 882, "seek": 455816, "start": 4565.76, "end": 4572.4, "text": " Like in TensorFlow there's a tf.wile and stuff like that where you can actually do GPU-based", "tokens": [1743, 294, 37624, 456, 311, 257, 256, 69, 13, 86, 794, 293, 1507, 411, 300, 689, 291, 393, 767, 360, 18407, 12, 6032], "temperature": 0.0, "avg_logprob": -0.15259255495938387, "compression_ratio": 1.6024590163934427, "no_speech_prob": 9.874617035166011e-07}, {"id": 883, "seek": 455816, "start": 4572.4, "end": 4573.4, "text": " loops.", "tokens": [16121, 13], "temperature": 0.0, "avg_logprob": -0.15259255495938387, "compression_ratio": 1.6024590163934427, "no_speech_prob": 9.874617035166011e-07}, {"id": 884, "seek": 455816, "start": 4573.4, "end": 4577.72, "text": " Even still, if you do it entirely in a loop in Python, it's going to be pretty difficult", "tokens": [2754, 920, 11, 498, 291, 360, 309, 7696, 294, 257, 6367, 294, 15329, 11, 309, 311, 516, 281, 312, 1238, 2252], "temperature": 0.0, "avg_logprob": -0.15259255495938387, "compression_ratio": 1.6024590163934427, "no_speech_prob": 9.874617035166011e-07}, {"id": 885, "seek": 455816, "start": 4577.72, "end": 4578.72, "text": " to get this performance.", "tokens": [281, 483, 341, 3389, 13], "temperature": 0.0, "avg_logprob": -0.15259255495938387, "compression_ratio": 1.6024590163934427, "no_speech_prob": 9.874617035166011e-07}, {"id": 886, "seek": 455816, "start": 4578.72, "end": 4583.76, "text": " But particularly in PyTorch, it's important to remember in PyTorch, your loops are not", "tokens": [583, 4098, 294, 9953, 51, 284, 339, 11, 309, 311, 1021, 281, 1604, 294, 9953, 51, 284, 339, 11, 428, 16121, 366, 406], "temperature": 0.0, "avg_logprob": -0.15259255495938387, "compression_ratio": 1.6024590163934427, "no_speech_prob": 9.874617035166011e-07}, {"id": 887, "seek": 455816, "start": 4583.76, "end": 4584.76, "text": " optimized.", "tokens": [26941, 13], "temperature": 0.0, "avg_logprob": -0.15259255495938387, "compression_ratio": 1.6024590163934427, "no_speech_prob": 9.874617035166011e-07}, {"id": 888, "seek": 458476, "start": 4584.76, "end": 4588.400000000001, "text": " It's what you do inside each loop that's optimized.", "tokens": [467, 311, 437, 291, 360, 1854, 1184, 6367, 300, 311, 26941, 13], "temperature": 0.0, "avg_logprob": -0.23058606993477299, "compression_ratio": 1.6359649122807018, "no_speech_prob": 1.8342707335250452e-05}, {"id": 889, "seek": 458476, "start": 4588.400000000001, "end": 4589.400000000001, "text": " Question.", "tokens": [14464, 13], "temperature": 0.0, "avg_logprob": -0.23058606993477299, "compression_ratio": 1.6359649122807018, "no_speech_prob": 1.8342707335250452e-05}, {"id": 890, "seek": 458476, "start": 4589.400000000001, "end": 4594.52, "text": " Some of the math functions are coming from Torch and others are coming from the Python", "tokens": [2188, 295, 264, 5221, 6828, 366, 1348, 490, 7160, 339, 293, 2357, 366, 1348, 490, 264, 15329], "temperature": 0.0, "avg_logprob": -0.23058606993477299, "compression_ratio": 1.6359649122807018, "no_speech_prob": 1.8342707335250452e-05}, {"id": 891, "seek": 458476, "start": 4594.52, "end": 4595.72, "text": " library.", "tokens": [6405, 13], "temperature": 0.0, "avg_logprob": -0.23058606993477299, "compression_ratio": 1.6359649122807018, "no_speech_prob": 1.8342707335250452e-05}, {"id": 892, "seek": 458476, "start": 4595.72, "end": 4596.84, "text": " What is the difference?", "tokens": [708, 307, 264, 2649, 30], "temperature": 0.0, "avg_logprob": -0.23058606993477299, "compression_ratio": 1.6359649122807018, "no_speech_prob": 1.8342707335250452e-05}, {"id": 893, "seek": 458476, "start": 4596.84, "end": 4601.4400000000005, "text": " When you use the Python math library, does that mean the GPU is not being used?", "tokens": [1133, 291, 764, 264, 15329, 5221, 6405, 11, 775, 300, 914, 264, 18407, 307, 406, 885, 1143, 30], "temperature": 0.0, "avg_logprob": -0.23058606993477299, "compression_ratio": 1.6359649122807018, "no_speech_prob": 1.8342707335250452e-05}, {"id": 894, "seek": 458476, "start": 4601.4400000000005, "end": 4602.4400000000005, "text": " Answer.", "tokens": [24545, 13], "temperature": 0.0, "avg_logprob": -0.23058606993477299, "compression_ratio": 1.6359649122807018, "no_speech_prob": 1.8342707335250452e-05}, {"id": 895, "seek": 458476, "start": 4602.4400000000005, "end": 4611.280000000001, "text": " You'll see that I use that math.py is a constant and then math.square root of 2 times pi is", "tokens": [509, 603, 536, 300, 286, 764, 300, 5221, 13, 8200, 307, 257, 5754, 293, 550, 5221, 13, 33292, 543, 5593, 295, 568, 1413, 3895, 307], "temperature": 0.0, "avg_logprob": -0.23058606993477299, "compression_ratio": 1.6359649122807018, "no_speech_prob": 1.8342707335250452e-05}, {"id": 896, "seek": 458476, "start": 4611.280000000001, "end": 4612.280000000001, "text": " a constant.", "tokens": [257, 5754, 13], "temperature": 0.0, "avg_logprob": -0.23058606993477299, "compression_ratio": 1.6359649122807018, "no_speech_prob": 1.8342707335250452e-05}, {"id": 897, "seek": 461228, "start": 4612.28, "end": 4616.4, "text": " So you need to use the GPU to calculate a constant, obviously.", "tokens": [407, 291, 643, 281, 764, 264, 18407, 281, 8873, 257, 5754, 11, 2745, 13], "temperature": 0.0, "avg_logprob": -0.18725579744809634, "compression_ratio": 1.4031413612565444, "no_speech_prob": 5.09366373080411e-06}, {"id": 898, "seek": 461228, "start": 4616.4, "end": 4630.08, "text": " So we only use Torch for things that are running on a vector or a matrix or a tensor of data.", "tokens": [407, 321, 787, 764, 7160, 339, 337, 721, 300, 366, 2614, 322, 257, 8062, 420, 257, 8141, 420, 257, 40863, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.18725579744809634, "compression_ratio": 1.4031413612565444, "no_speech_prob": 5.09366373080411e-06}, {"id": 899, "seek": 461228, "start": 4630.08, "end": 4631.8, "text": " So let's have a break.", "tokens": [407, 718, 311, 362, 257, 1821, 13], "temperature": 0.0, "avg_logprob": -0.18725579744809634, "compression_ratio": 1.4031413612565444, "no_speech_prob": 5.09366373080411e-06}, {"id": 900, "seek": 461228, "start": 4631.8, "end": 4636.36, "text": " We'll come back in 10 minutes, so that will be 2 past 8, and we'll talk about some ideas", "tokens": [492, 603, 808, 646, 294, 1266, 2077, 11, 370, 300, 486, 312, 568, 1791, 1649, 11, 293, 321, 603, 751, 466, 512, 3487], "temperature": 0.0, "avg_logprob": -0.18725579744809634, "compression_ratio": 1.4031413612565444, "no_speech_prob": 5.09366373080411e-06}, {"id": 901, "seek": 463636, "start": 4636.36, "end": 4643.88, "text": " I have for improving mean shift, which maybe you guys will want to try out during the week.", "tokens": [286, 362, 337, 11470, 914, 5513, 11, 597, 1310, 291, 1074, 486, 528, 281, 853, 484, 1830, 264, 1243, 13], "temperature": 0.0, "avg_logprob": -0.1578848402378923, "compression_ratio": 1.4556962025316456, "no_speech_prob": 1.544607584946789e-05}, {"id": 902, "seek": 463636, "start": 4643.88, "end": 4656.96, "text": " So basically the idea here is we figure that there are 2 steps we need to figure out where", "tokens": [407, 1936, 264, 1558, 510, 307, 321, 2573, 300, 456, 366, 568, 4439, 321, 643, 281, 2573, 484, 689], "temperature": 0.0, "avg_logprob": -0.1578848402378923, "compression_ratio": 1.4556962025316456, "no_speech_prob": 1.544607584946789e-05}, {"id": 903, "seek": 463636, "start": 4656.96, "end": 4661.5199999999995, "text": " the nodules are in something like this, if any.", "tokens": [264, 15224, 3473, 366, 294, 746, 411, 341, 11, 498, 604, 13], "temperature": 0.0, "avg_logprob": -0.1578848402378923, "compression_ratio": 1.4556962025316456, "no_speech_prob": 1.544607584946789e-05}, {"id": 904, "seek": 466152, "start": 4661.52, "end": 4667.080000000001, "text": " Step number 1 is to find the things that may be kind of nodule-ish, zoom into them and", "tokens": [5470, 1230, 502, 307, 281, 915, 264, 721, 300, 815, 312, 733, 295, 15224, 2271, 12, 742, 11, 8863, 666, 552, 293], "temperature": 0.0, "avg_logprob": -0.15556183728304776, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.4063935850572307e-05}, {"id": 905, "seek": 466152, "start": 4667.080000000001, "end": 4669.92, "text": " create a little cropped version.", "tokens": [1884, 257, 707, 4848, 3320, 3037, 13], "temperature": 0.0, "avg_logprob": -0.15556183728304776, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.4063935850572307e-05}, {"id": 906, "seek": 466152, "start": 4669.92, "end": 4674.88, "text": " And then step 2 would be where your deep learning particularly comes in, which is to figure", "tokens": [400, 550, 1823, 568, 576, 312, 689, 428, 2452, 2539, 4098, 1487, 294, 11, 597, 307, 281, 2573], "temperature": 0.0, "avg_logprob": -0.15556183728304776, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.4063935850572307e-05}, {"id": 907, "seek": 466152, "start": 4674.88, "end": 4680.080000000001, "text": " out is that cancerous or not.", "tokens": [484, 307, 300, 5592, 563, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.15556183728304776, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.4063935850572307e-05}, {"id": 908, "seek": 466152, "start": 4680.080000000001, "end": 4687.780000000001, "text": " Once you've found a nodule-ish thing, the cancerous ones are actually by far the biggest", "tokens": [3443, 291, 600, 1352, 257, 15224, 2271, 12, 742, 551, 11, 264, 5592, 563, 2306, 366, 767, 538, 1400, 264, 3880], "temperature": 0.0, "avg_logprob": -0.15556183728304776, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.4063935850572307e-05}, {"id": 909, "seek": 468778, "start": 4687.78, "end": 4694.24, "text": " driver of whether or not something is an owing to cancer is how big it is.", "tokens": [6787, 295, 1968, 420, 406, 746, 307, 364, 277, 7904, 281, 5592, 307, 577, 955, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.24695161819458009, "compression_ratio": 1.648068669527897, "no_speech_prob": 1.0783203833852895e-05}, {"id": 910, "seek": 468778, "start": 4694.24, "end": 4697.679999999999, "text": " So it's actually pretty straightforward.", "tokens": [407, 309, 311, 767, 1238, 15325, 13], "temperature": 0.0, "avg_logprob": -0.24695161819458009, "compression_ratio": 1.648068669527897, "no_speech_prob": 1.0783203833852895e-05}, {"id": 911, "seek": 468778, "start": 4697.679999999999, "end": 4704.719999999999, "text": " The other thing particularly important is how kind of spidery it looks.", "tokens": [440, 661, 551, 4098, 1021, 307, 577, 733, 295, 17614, 88, 309, 1542, 13], "temperature": 0.0, "avg_logprob": -0.24695161819458009, "compression_ratio": 1.648068669527897, "no_speech_prob": 1.0783203833852895e-05}, {"id": 912, "seek": 468778, "start": 4704.719999999999, "end": 4709.66, "text": " If it looks like it's kind of evilly going out to capture more territory, that's probably", "tokens": [759, 309, 1542, 411, 309, 311, 733, 295, 1073, 6917, 516, 484, 281, 7983, 544, 11360, 11, 300, 311, 1391], "temperature": 0.0, "avg_logprob": -0.24695161819458009, "compression_ratio": 1.648068669527897, "no_speech_prob": 1.0783203833852895e-05}, {"id": 913, "seek": 468778, "start": 4709.66, "end": 4712.78, "text": " a bad sign as well.", "tokens": [257, 1578, 1465, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.24695161819458009, "compression_ratio": 1.648068669527897, "no_speech_prob": 1.0783203833852895e-05}, {"id": 914, "seek": 468778, "start": 4712.78, "end": 4716.719999999999, "text": " So the size and the shape are the 2 things you're going to be wanting to try and find,", "tokens": [407, 264, 2744, 293, 264, 3909, 366, 264, 568, 721, 291, 434, 516, 281, 312, 7935, 281, 853, 293, 915, 11], "temperature": 0.0, "avg_logprob": -0.24695161819458009, "compression_ratio": 1.648068669527897, "no_speech_prob": 1.0783203833852895e-05}, {"id": 915, "seek": 471672, "start": 4716.72, "end": 4721.92, "text": " and obviously that's a pretty good thing for a neural net to be able to do.", "tokens": [293, 2745, 300, 311, 257, 1238, 665, 551, 337, 257, 18161, 2533, 281, 312, 1075, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.18392470276471481, "compression_ratio": 1.6096491228070176, "no_speech_prob": 9.516199497738853e-06}, {"id": 916, "seek": 471672, "start": 4721.92, "end": 4727.16, "text": " You probably don't need that in the examples of it.", "tokens": [509, 1391, 500, 380, 643, 300, 294, 264, 5110, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.18392470276471481, "compression_ratio": 1.6096491228070176, "no_speech_prob": 9.516199497738853e-06}, {"id": 917, "seek": 471672, "start": 4727.16, "end": 4731.4400000000005, "text": " When you get to that point, there's obviously a question about how to deal with the 3D aspect", "tokens": [1133, 291, 483, 281, 300, 935, 11, 456, 311, 2745, 257, 1168, 466, 577, 281, 2028, 365, 264, 805, 35, 4171], "temperature": 0.0, "avg_logprob": -0.18392470276471481, "compression_ratio": 1.6096491228070176, "no_speech_prob": 9.516199497738853e-06}, {"id": 918, "seek": 471672, "start": 4731.4400000000005, "end": 4732.4400000000005, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.18392470276471481, "compression_ratio": 1.6096491228070176, "no_speech_prob": 9.516199497738853e-06}, {"id": 919, "seek": 471672, "start": 4732.4400000000005, "end": 4736.88, "text": " You can just create a 3D convolutional neural net.", "tokens": [509, 393, 445, 1884, 257, 805, 35, 45216, 304, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.18392470276471481, "compression_ratio": 1.6096491228070176, "no_speech_prob": 9.516199497738853e-06}, {"id": 920, "seek": 471672, "start": 4736.88, "end": 4745.08, "text": " So if you had like a 10x10x10 space, that's obviously certainly not going to be too big,", "tokens": [407, 498, 291, 632, 411, 257, 1266, 87, 3279, 87, 3279, 1901, 11, 300, 311, 2745, 3297, 406, 516, 281, 312, 886, 955, 11], "temperature": 0.0, "avg_logprob": -0.18392470276471481, "compression_ratio": 1.6096491228070176, "no_speech_prob": 9.516199497738853e-06}, {"id": 921, "seek": 474508, "start": 4745.08, "end": 4747.6, "text": " but if it's 20x20x20 you might be okay.", "tokens": [457, 498, 309, 311, 945, 87, 2009, 87, 2009, 291, 1062, 312, 1392, 13], "temperature": 0.0, "avg_logprob": -0.22104183463163155, "compression_ratio": 1.5441176470588236, "no_speech_prob": 9.51618767430773e-06}, {"id": 922, "seek": 474508, "start": 4747.6, "end": 4750.96, "text": " Think about how big a volume you can create.", "tokens": [6557, 466, 577, 955, 257, 5523, 291, 393, 1884, 13], "temperature": 0.0, "avg_logprob": -0.22104183463163155, "compression_ratio": 1.5441176470588236, "no_speech_prob": 9.51618767430773e-06}, {"id": 923, "seek": 474508, "start": 4750.96, "end": 4756.4, "text": " There's plenty of papers around on 3D convolutions, although I'm not sure if you even need one", "tokens": [821, 311, 7140, 295, 10577, 926, 322, 805, 35, 3754, 15892, 11, 4878, 286, 478, 406, 988, 498, 291, 754, 643, 472], "temperature": 0.0, "avg_logprob": -0.22104183463163155, "compression_ratio": 1.5441176470588236, "no_speech_prob": 9.51618767430773e-06}, {"id": 924, "seek": 474508, "start": 4756.4, "end": 4761.5199999999995, "text": " because it's just a convolution in 3D.", "tokens": [570, 309, 311, 445, 257, 45216, 294, 805, 35, 13], "temperature": 0.0, "avg_logprob": -0.22104183463163155, "compression_ratio": 1.5441176470588236, "no_speech_prob": 9.51618767430773e-06}, {"id": 925, "seek": 474508, "start": 4761.5199999999995, "end": 4767.62, "text": " The other approach that you might find interesting to think about is something called triplanar.", "tokens": [440, 661, 3109, 300, 291, 1062, 915, 1880, 281, 519, 466, 307, 746, 1219, 1376, 16554, 289, 13], "temperature": 0.0, "avg_logprob": -0.22104183463163155, "compression_ratio": 1.5441176470588236, "no_speech_prob": 9.51618767430773e-06}, {"id": 926, "seek": 476762, "start": 4767.62, "end": 4776.16, "text": " What triplanar means is that you take a slice through the X and the Y and the Z axes, and", "tokens": [708, 1376, 16554, 289, 1355, 307, 300, 291, 747, 257, 13153, 807, 264, 1783, 293, 264, 398, 293, 264, 1176, 35387, 11, 293], "temperature": 0.0, "avg_logprob": -0.22181043170747303, "compression_ratio": 1.5792079207920793, "no_speech_prob": 9.132533591582614e-07}, {"id": 927, "seek": 476762, "start": 4776.16, "end": 4778.96, "text": " so you basically end up with 3 images.", "tokens": [370, 291, 1936, 917, 493, 365, 805, 5267, 13], "temperature": 0.0, "avg_logprob": -0.22181043170747303, "compression_ratio": 1.5792079207920793, "no_speech_prob": 9.132533591582614e-07}, {"id": 928, "seek": 476762, "start": 4778.96, "end": 4785.04, "text": " One is a slice through X, Y and Z, and then you can kind of treat those as different channels", "tokens": [1485, 307, 257, 13153, 807, 1783, 11, 398, 293, 1176, 11, 293, 550, 291, 393, 733, 295, 2387, 729, 382, 819, 9235], "temperature": 0.0, "avg_logprob": -0.22181043170747303, "compression_ratio": 1.5792079207920793, "no_speech_prob": 9.132533591582614e-07}, {"id": 929, "seek": 476762, "start": 4785.04, "end": 4786.04, "text": " if you like even.", "tokens": [498, 291, 411, 754, 13], "temperature": 0.0, "avg_logprob": -0.22181043170747303, "compression_ratio": 1.5792079207920793, "no_speech_prob": 9.132533591582614e-07}, {"id": 930, "seek": 476762, "start": 4786.04, "end": 4793.78, "text": " They probably use pretty standard neural net libraries that expect 3 channels.", "tokens": [814, 1391, 764, 1238, 3832, 18161, 2533, 15148, 300, 2066, 805, 9235, 13], "temperature": 0.0, "avg_logprob": -0.22181043170747303, "compression_ratio": 1.5792079207920793, "no_speech_prob": 9.132533591582614e-07}, {"id": 931, "seek": 479378, "start": 4793.78, "end": 4803.08, "text": " So there's a couple of ideas for how you can deal with the 3D aspect of it.", "tokens": [407, 456, 311, 257, 1916, 295, 3487, 337, 577, 291, 393, 2028, 365, 264, 805, 35, 4171, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.1781697565195512, "compression_ratio": 1.6297872340425532, "no_speech_prob": 3.446566324782907e-06}, {"id": 932, "seek": 479378, "start": 4803.08, "end": 4809.16, "text": " I think using the Luna dataset as much as possible is going to be a good idea because", "tokens": [286, 519, 1228, 264, 27355, 28872, 382, 709, 382, 1944, 307, 516, 281, 312, 257, 665, 1558, 570], "temperature": 0.0, "avg_logprob": -0.1781697565195512, "compression_ratio": 1.6297872340425532, "no_speech_prob": 3.446566324782907e-06}, {"id": 933, "seek": 479378, "start": 4809.16, "end": 4813.599999999999, "text": " you really want something that's pretty good at detecting nodules before you start putting", "tokens": [291, 534, 528, 746, 300, 311, 1238, 665, 412, 40237, 15224, 3473, 949, 291, 722, 3372], "temperature": 0.0, "avg_logprob": -0.1781697565195512, "compression_ratio": 1.6297872340425532, "no_speech_prob": 3.446566324782907e-06}, {"id": 934, "seek": 479378, "start": 4813.599999999999, "end": 4814.599999999999, "text": " it onto the Kaggle dataset.", "tokens": [309, 3911, 264, 48751, 22631, 28872, 13], "temperature": 0.0, "avg_logprob": -0.1781697565195512, "compression_ratio": 1.6297872340425532, "no_speech_prob": 3.446566324782907e-06}, {"id": 935, "seek": 479378, "start": 4814.599999999999, "end": 4818.4, "text": " The other problem with the Kaggle dataset is it's ridiculously small.", "tokens": [440, 661, 1154, 365, 264, 48751, 22631, 28872, 307, 309, 311, 41358, 1359, 13], "temperature": 0.0, "avg_logprob": -0.1781697565195512, "compression_ratio": 1.6297872340425532, "no_speech_prob": 3.446566324782907e-06}, {"id": 936, "seek": 479378, "start": 4818.4, "end": 4821.0, "text": " Again, there's no reason for it.", "tokens": [3764, 11, 456, 311, 572, 1778, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.1781697565195512, "compression_ratio": 1.6297872340425532, "no_speech_prob": 3.446566324782907e-06}, {"id": 937, "seek": 482100, "start": 4821.0, "end": 4828.32, "text": " There are far more cases in NLST than they've provided to Kaggle, so I can't begin to imagine", "tokens": [821, 366, 1400, 544, 3331, 294, 426, 43, 6840, 813, 436, 600, 5649, 281, 48751, 22631, 11, 370, 286, 393, 380, 1841, 281, 3811], "temperature": 0.0, "avg_logprob": -0.22764500704678622, "compression_ratio": 1.5421686746987953, "no_speech_prob": 5.86277428737958e-06}, {"id": 938, "seek": 482100, "start": 4828.32, "end": 4832.7, "text": " why they went into all this trouble with a million dollars of money for something which", "tokens": [983, 436, 1437, 666, 439, 341, 5253, 365, 257, 2459, 3808, 295, 1460, 337, 746, 597], "temperature": 0.0, "avg_logprob": -0.22764500704678622, "compression_ratio": 1.5421686746987953, "no_speech_prob": 5.86277428737958e-06}, {"id": 939, "seek": 482100, "start": 4832.7, "end": 4835.12, "text": " has not been set up to succeed.", "tokens": [575, 406, 668, 992, 493, 281, 7754, 13], "temperature": 0.0, "avg_logprob": -0.22764500704678622, "compression_ratio": 1.5421686746987953, "no_speech_prob": 5.86277428737958e-06}, {"id": 940, "seek": 482100, "start": 4835.12, "end": 4837.28, "text": " Anyway, that's not our problem.", "tokens": [5684, 11, 300, 311, 406, 527, 1154, 13], "temperature": 0.0, "avg_logprob": -0.22764500704678622, "compression_ratio": 1.5421686746987953, "no_speech_prob": 5.86277428737958e-06}, {"id": 941, "seek": 482100, "start": 4837.28, "end": 4841.8, "text": " It makes it all a more interesting thing to play with.", "tokens": [467, 1669, 309, 439, 257, 544, 1880, 551, 281, 862, 365, 13], "temperature": 0.0, "avg_logprob": -0.22764500704678622, "compression_ratio": 1.5421686746987953, "no_speech_prob": 5.86277428737958e-06}, {"id": 942, "seek": 482100, "start": 4841.8, "end": 4847.44, "text": " But after the competition is finished, if you get interested in it, you'll probably", "tokens": [583, 934, 264, 6211, 307, 4335, 11, 498, 291, 483, 3102, 294, 309, 11, 291, 603, 1391], "temperature": 0.0, "avg_logprob": -0.22764500704678622, "compression_ratio": 1.5421686746987953, "no_speech_prob": 5.86277428737958e-06}, {"id": 943, "seek": 484744, "start": 4847.44, "end": 4854.44, "text": " want to go and download the whole NLST dataset as much as possible and do it properly.", "tokens": [528, 281, 352, 293, 5484, 264, 1379, 426, 43, 6840, 28872, 382, 709, 382, 1944, 293, 360, 309, 6108, 13], "temperature": 0.0, "avg_logprob": -0.31830611786285, "compression_ratio": 1.5, "no_speech_prob": 5.561961006605998e-05}, {"id": 944, "seek": 484744, "start": 4854.44, "end": 4861.799999999999, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.31830611786285, "compression_ratio": 1.5, "no_speech_prob": 5.561961006605998e-05}, {"id": 945, "seek": 484744, "start": 4861.799999999999, "end": 4866.719999999999, "text": " One is just for the audio stream, there are occasional max volume pops that are really", "tokens": [1485, 307, 445, 337, 264, 6278, 4309, 11, 456, 366, 31644, 11469, 5523, 16795, 300, 366, 534], "temperature": 0.0, "avg_logprob": -0.31830611786285, "compression_ratio": 1.5, "no_speech_prob": 5.561961006605998e-05}, {"id": 946, "seek": 484744, "start": 4866.719999999999, "end": 4868.919999999999, "text": " hard on the ears for remote listeners.", "tokens": [1152, 322, 264, 8798, 337, 8607, 23274, 13], "temperature": 0.0, "avg_logprob": -0.31830611786285, "compression_ratio": 1.5, "no_speech_prob": 5.561961006605998e-05}, {"id": 947, "seek": 484744, "start": 4868.919999999999, "end": 4876.719999999999, "text": " This might not be solvable right now, but something to look into.", "tokens": [639, 1062, 406, 312, 1404, 17915, 558, 586, 11, 457, 746, 281, 574, 666, 13], "temperature": 0.0, "avg_logprob": -0.31830611786285, "compression_ratio": 1.5, "no_speech_prob": 5.561961006605998e-05}, {"id": 948, "seek": 487672, "start": 4876.72, "end": 4881.320000000001, "text": " And then someone asked, last class you mentioned that you would explain when and why to use", "tokens": [400, 550, 1580, 2351, 11, 1036, 1508, 291, 2835, 300, 291, 576, 2903, 562, 293, 983, 281, 764], "temperature": 0.0, "avg_logprob": -0.24323016895967373, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.7230904632015154e-05}, {"id": 949, "seek": 487672, "start": 4881.320000000001, "end": 4883.72, "text": " Keras versus PyTorch.", "tokens": [591, 6985, 5717, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.24323016895967373, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.7230904632015154e-05}, {"id": 950, "seek": 487672, "start": 4883.72, "end": 4890.08, "text": " If you only had brain space for one, in the same way some only have brain space for VI", "tokens": [759, 291, 787, 632, 3567, 1901, 337, 472, 11, 294, 264, 912, 636, 512, 787, 362, 3567, 1901, 337, 27619], "temperature": 0.0, "avg_logprob": -0.24323016895967373, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.7230904632015154e-05}, {"id": 951, "seek": 487672, "start": 4890.08, "end": 4895.360000000001, "text": " or Emacs, which would you pick?", "tokens": [420, 3968, 44937, 11, 597, 576, 291, 1888, 30], "temperature": 0.0, "avg_logprob": -0.24323016895967373, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.7230904632015154e-05}, {"id": 952, "seek": 487672, "start": 4895.360000000001, "end": 4905.52, "text": " So I just reduced the volume a little bit, so let us know if that helps.", "tokens": [407, 286, 445, 9212, 264, 5523, 257, 707, 857, 11, 370, 718, 505, 458, 498, 300, 3665, 13], "temperature": 0.0, "avg_logprob": -0.24323016895967373, "compression_ratio": 1.50990099009901, "no_speech_prob": 1.7230904632015154e-05}, {"id": 953, "seek": 490552, "start": 4905.52, "end": 4908.64, "text": " I would pick PyTorch.", "tokens": [286, 576, 1888, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.33143356408965724, "compression_ratio": 1.4646017699115044, "no_speech_prob": 4.908439586870372e-05}, {"id": 954, "seek": 490552, "start": 4908.64, "end": 4914.4400000000005, "text": " It feels like it kind of does everything Keras does, but gives you the flexibility to really", "tokens": [467, 3417, 411, 309, 733, 295, 775, 1203, 591, 6985, 775, 11, 457, 2709, 291, 264, 12635, 281, 534], "temperature": 0.0, "avg_logprob": -0.33143356408965724, "compression_ratio": 1.4646017699115044, "no_speech_prob": 4.908439586870372e-05}, {"id": 955, "seek": 490552, "start": 4914.4400000000005, "end": 4920.160000000001, "text": " play around a lot more.", "tokens": [862, 926, 257, 688, 544, 13], "temperature": 0.0, "avg_logprob": -0.33143356408965724, "compression_ratio": 1.4646017699115044, "no_speech_prob": 4.908439586870372e-05}, {"id": 956, "seek": 490552, "start": 4920.160000000001, "end": 4924.52, "text": " I'm sure you've got brain space for both.", "tokens": [286, 478, 988, 291, 600, 658, 3567, 1901, 337, 1293, 13], "temperature": 0.0, "avg_logprob": -0.33143356408965724, "compression_ratio": 1.4646017699115044, "no_speech_prob": 4.908439586870372e-05}, {"id": 957, "seek": 490552, "start": 4924.52, "end": 4929.56, "text": " So question, you mentioned there are other datasets of cancerous images that have labels", "tokens": [407, 1168, 11, 291, 2835, 456, 366, 661, 42856, 295, 5592, 563, 5267, 300, 362, 16949], "temperature": 0.0, "avg_logprob": -0.33143356408965724, "compression_ratio": 1.4646017699115044, "no_speech_prob": 4.908439586870372e-05}, {"id": 958, "seek": 490552, "start": 4929.56, "end": 4930.56, "text": " and proper marks.", "tokens": [293, 2296, 10640, 13], "temperature": 0.0, "avg_logprob": -0.33143356408965724, "compression_ratio": 1.4646017699115044, "no_speech_prob": 4.908439586870372e-05}, {"id": 959, "seek": 490552, "start": 4930.56, "end": 4933.56, "text": " Can you explain this thing on that dataset?", "tokens": [1664, 291, 2903, 341, 551, 322, 300, 28872, 30], "temperature": 0.0, "avg_logprob": -0.33143356408965724, "compression_ratio": 1.4646017699115044, "no_speech_prob": 4.908439586870372e-05}, {"id": 960, "seek": 493356, "start": 4933.56, "end": 4945.320000000001, "text": " That was my suggestion, and that's what the tutorial shows how to do.", "tokens": [663, 390, 452, 16541, 11, 293, 300, 311, 437, 264, 7073, 3110, 577, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.33118104934692383, "compression_ratio": 1.4516129032258065, "no_speech_prob": 3.120089240837842e-05}, {"id": 961, "seek": 493356, "start": 4945.320000000001, "end": 4951.240000000001, "text": " There's a whole thing, kernel on Kaggle called candidate generation and Luna 16 something", "tokens": [821, 311, 257, 1379, 551, 11, 28256, 322, 48751, 22631, 1219, 11532, 5125, 293, 27355, 3165, 746], "temperature": 0.0, "avg_logprob": -0.33118104934692383, "compression_ratio": 1.4516129032258065, "no_speech_prob": 3.120089240837842e-05}, {"id": 962, "seek": 493356, "start": 4951.240000000001, "end": 4960.56, "text": " something, which shows how to use Luna to build a logical finder.", "tokens": [746, 11, 597, 3110, 577, 281, 764, 27355, 281, 1322, 257, 14978, 915, 260, 13], "temperature": 0.0, "avg_logprob": -0.33118104934692383, "compression_ratio": 1.4516129032258065, "no_speech_prob": 3.120089240837842e-05}, {"id": 963, "seek": 496056, "start": 4960.56, "end": 4965.04, "text": " This is one of the highest rated Kaggle kernels.", "tokens": [639, 307, 472, 295, 264, 6343, 22103, 48751, 22631, 23434, 1625, 13], "temperature": 0.0, "avg_logprob": -0.25913954472196277, "compression_ratio": 1.4251497005988023, "no_speech_prob": 1.028942824632395e-05}, {"id": 964, "seek": 496056, "start": 4965.04, "end": 4969.0, "text": " We've now used kernel in 3 totally different ways in this lesson.", "tokens": [492, 600, 586, 1143, 28256, 294, 805, 3879, 819, 2098, 294, 341, 6898, 13], "temperature": 0.0, "avg_logprob": -0.25913954472196277, "compression_ratio": 1.4251497005988023, "no_speech_prob": 1.028942824632395e-05}, {"id": 965, "seek": 496056, "start": 4969.0, "end": 4978.240000000001, "text": " If we can come up with a fourth, Kaggle kernels, CUDA kernels, and kernel methods.", "tokens": [759, 321, 393, 808, 493, 365, 257, 6409, 11, 48751, 22631, 23434, 1625, 11, 29777, 7509, 23434, 1625, 11, 293, 28256, 7150, 13], "temperature": 0.0, "avg_logprob": -0.25913954472196277, "compression_ratio": 1.4251497005988023, "no_speech_prob": 1.028942824632395e-05}, {"id": 966, "seek": 496056, "start": 4978.240000000001, "end": 4982.68, "text": " So this looks very familiar, doesn't it?", "tokens": [407, 341, 1542, 588, 4963, 11, 1177, 380, 309, 30], "temperature": 0.0, "avg_logprob": -0.25913954472196277, "compression_ratio": 1.4251497005988023, "no_speech_prob": 1.028942824632395e-05}, {"id": 967, "seek": 498268, "start": 4982.68, "end": 4993.280000000001, "text": " So here's a Keras approach to finding LungNodules based on Luna.", "tokens": [407, 510, 311, 257, 591, 6985, 3109, 281, 5006, 441, 1063, 45, 378, 3473, 2361, 322, 27355, 13], "temperature": 0.0, "avg_logprob": -0.390410379929976, "compression_ratio": 0.9411764705882353, "no_speech_prob": 4.264562812750228e-05}, {"id": 968, "seek": 499328, "start": 4993.28, "end": 5018.639999999999, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.5795921802520752, "compression_ratio": 0.1111111111111111, "no_speech_prob": 2.7968144422629848e-05}, {"id": 969, "seek": 501864, "start": 5018.64, "end": 5024.64, "text": " mentioned an opportunity to improve this mean shift algorithm.", "tokens": [2835, 364, 2650, 281, 3470, 341, 914, 5513, 9284, 13], "temperature": 0.0, "avg_logprob": -0.19991616641773896, "compression_ratio": 1.5781990521327014, "no_speech_prob": 6.708880391670391e-05}, {"id": 970, "seek": 501864, "start": 5024.64, "end": 5028.88, "text": " The opportunity for improvement, when you think about it, is pretty obvious.", "tokens": [440, 2650, 337, 10444, 11, 562, 291, 519, 466, 309, 11, 307, 1238, 6322, 13], "temperature": 0.0, "avg_logprob": -0.19991616641773896, "compression_ratio": 1.5781990521327014, "no_speech_prob": 6.708880391670391e-05}, {"id": 971, "seek": 501864, "start": 5028.88, "end": 5031.88, "text": " The actual amount of data is huge.", "tokens": [440, 3539, 2372, 295, 1412, 307, 2603, 13], "temperature": 0.0, "avg_logprob": -0.19991616641773896, "compression_ratio": 1.5781990521327014, "no_speech_prob": 6.708880391670391e-05}, {"id": 972, "seek": 501864, "start": 5031.88, "end": 5037.6, "text": " You've got data points all over the place.", "tokens": [509, 600, 658, 1412, 2793, 439, 670, 264, 1081, 13], "temperature": 0.0, "avg_logprob": -0.19991616641773896, "compression_ratio": 1.5781990521327014, "no_speech_prob": 6.708880391670391e-05}, {"id": 973, "seek": 501864, "start": 5037.6, "end": 5042.56, "text": " The ones that are a long way away, the weight is going to be so close to zero that we may", "tokens": [440, 2306, 300, 366, 257, 938, 636, 1314, 11, 264, 3364, 307, 516, 281, 312, 370, 1998, 281, 4018, 300, 321, 815], "temperature": 0.0, "avg_logprob": -0.19991616641773896, "compression_ratio": 1.5781990521327014, "no_speech_prob": 6.708880391670391e-05}, {"id": 974, "seek": 501864, "start": 5042.56, "end": 5047.76, "text": " as well just ignore them.", "tokens": [382, 731, 445, 11200, 552, 13], "temperature": 0.0, "avg_logprob": -0.19991616641773896, "compression_ratio": 1.5781990521327014, "no_speech_prob": 6.708880391670391e-05}, {"id": 975, "seek": 504776, "start": 5047.76, "end": 5055.72, "text": " The question is how do we quickly find the ones which are a long way away.", "tokens": [440, 1168, 307, 577, 360, 321, 2661, 915, 264, 2306, 597, 366, 257, 938, 636, 1314, 13], "temperature": 0.0, "avg_logprob": -0.18244766466545337, "compression_ratio": 1.4491017964071857, "no_speech_prob": 1.2029405297653284e-05}, {"id": 976, "seek": 504776, "start": 5055.72, "end": 5057.92, "text": " We know the answer to that.", "tokens": [492, 458, 264, 1867, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.18244766466545337, "compression_ratio": 1.4491017964071857, "no_speech_prob": 1.2029405297653284e-05}, {"id": 977, "seek": 504776, "start": 5057.92, "end": 5059.08, "text": " We learned it.", "tokens": [492, 3264, 309, 13], "temperature": 0.0, "avg_logprob": -0.18244766466545337, "compression_ratio": 1.4491017964071857, "no_speech_prob": 1.2029405297653284e-05}, {"id": 978, "seek": 504776, "start": 5059.08, "end": 5062.320000000001, "text": " It's approximate nearest neighbors.", "tokens": [467, 311, 30874, 23831, 12512, 13], "temperature": 0.0, "avg_logprob": -0.18244766466545337, "compression_ratio": 1.4491017964071857, "no_speech_prob": 1.2029405297653284e-05}, {"id": 979, "seek": 504776, "start": 5062.320000000001, "end": 5073.8, "text": " So what if we added an extra step here, which rather than using X to get the distance to", "tokens": [407, 437, 498, 321, 3869, 364, 2857, 1823, 510, 11, 597, 2831, 813, 1228, 1783, 281, 483, 264, 4560, 281], "temperature": 0.0, "avg_logprob": -0.18244766466545337, "compression_ratio": 1.4491017964071857, "no_speech_prob": 1.2029405297653284e-05}, {"id": 980, "seek": 507380, "start": 5073.8, "end": 5082.6, "text": " every data point, we instead used approximate nearest neighbors to grab the closest ones,", "tokens": [633, 1412, 935, 11, 321, 2602, 1143, 30874, 23831, 12512, 281, 4444, 264, 13699, 2306, 11], "temperature": 0.0, "avg_logprob": -0.13446659877382475, "compression_ratio": 1.537037037037037, "no_speech_prob": 9.368607607029844e-06}, {"id": 981, "seek": 507380, "start": 5082.6, "end": 5086.28, "text": " the ones that are actually going to matter.", "tokens": [264, 2306, 300, 366, 767, 516, 281, 1871, 13], "temperature": 0.0, "avg_logprob": -0.13446659877382475, "compression_ratio": 1.537037037037037, "no_speech_prob": 9.368607607029844e-06}, {"id": 982, "seek": 507380, "start": 5086.28, "end": 5098.76, "text": " So that would basically turn this linear time piece into a logarithmic time piece, which", "tokens": [407, 300, 576, 1936, 1261, 341, 8213, 565, 2522, 666, 257, 41473, 355, 13195, 565, 2522, 11, 597], "temperature": 0.0, "avg_logprob": -0.13446659877382475, "compression_ratio": 1.537037037037037, "no_speech_prob": 9.368607607029844e-06}, {"id": 983, "seek": 507380, "start": 5098.76, "end": 5100.24, "text": " would be pretty fantastic.", "tokens": [576, 312, 1238, 5456, 13], "temperature": 0.0, "avg_logprob": -0.13446659877382475, "compression_ratio": 1.537037037037037, "no_speech_prob": 9.368607607029844e-06}, {"id": 984, "seek": 510024, "start": 5100.24, "end": 5107.599999999999, "text": " So we learned very briefly about a particular approach, which is locality-sensitive hashing.", "tokens": [407, 321, 3264, 588, 10515, 466, 257, 1729, 3109, 11, 597, 307, 1628, 1860, 12, 82, 34465, 575, 571, 13], "temperature": 0.0, "avg_logprob": -0.14102664589881897, "compression_ratio": 1.4918032786885247, "no_speech_prob": 1.0953000128210988e-05}, {"id": 985, "seek": 510024, "start": 5107.599999999999, "end": 5117.5199999999995, "text": " I think I mentioned also there's another approach which I'm really fond of called spill trees.", "tokens": [286, 519, 286, 2835, 611, 456, 311, 1071, 3109, 597, 286, 478, 534, 9557, 295, 1219, 22044, 5852, 13], "temperature": 0.0, "avg_logprob": -0.14102664589881897, "compression_ratio": 1.4918032786885247, "no_speech_prob": 1.0953000128210988e-05}, {"id": 986, "seek": 510024, "start": 5117.5199999999995, "end": 5124.719999999999, "text": " I really, really want us as a team to take this algorithm and add approximate nearest", "tokens": [286, 534, 11, 534, 528, 505, 382, 257, 1469, 281, 747, 341, 9284, 293, 909, 30874, 23831], "temperature": 0.0, "avg_logprob": -0.14102664589881897, "compression_ratio": 1.4918032786885247, "no_speech_prob": 1.0953000128210988e-05}, {"id": 987, "seek": 512472, "start": 5124.72, "end": 5135.0, "text": " neighbors to it and release it to the community as the first ever super-fast GPU-accelerated", "tokens": [12512, 281, 309, 293, 4374, 309, 281, 264, 1768, 382, 264, 700, 1562, 1687, 12, 7011, 18407, 12, 326, 4933, 260, 770], "temperature": 0.0, "avg_logprob": -0.22454194619622028, "compression_ratio": 1.485, "no_speech_prob": 3.844909315375844e-06}, {"id": 988, "seek": 512472, "start": 5135.0, "end": 5140.320000000001, "text": " approximate nearest neighbor accelerated in-ship clustering algorithm.", "tokens": [30874, 23831, 5987, 29763, 294, 12, 5278, 596, 48673, 9284, 13], "temperature": 0.0, "avg_logprob": -0.22454194619622028, "compression_ratio": 1.485, "no_speech_prob": 3.844909315375844e-06}, {"id": 989, "seek": 512472, "start": 5140.320000000001, "end": 5143.76, "text": " I think that would be a really big deal.", "tokens": [286, 519, 300, 576, 312, 257, 534, 955, 2028, 13], "temperature": 0.0, "avg_logprob": -0.22454194619622028, "compression_ratio": 1.485, "no_speech_prob": 3.844909315375844e-06}, {"id": 990, "seek": 512472, "start": 5143.76, "end": 5150.16, "text": " If anybody's interested in doing that, I believe you're going to have to implement something", "tokens": [759, 4472, 311, 3102, 294, 884, 300, 11, 286, 1697, 291, 434, 516, 281, 362, 281, 4445, 746], "temperature": 0.0, "avg_logprob": -0.22454194619622028, "compression_ratio": 1.485, "no_speech_prob": 3.844909315375844e-06}, {"id": 991, "seek": 515016, "start": 5150.16, "end": 5155.36, "text": " like LSH or spill trees in PyTorch.", "tokens": [411, 441, 17308, 420, 22044, 5852, 294, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.1340123976784191, "compression_ratio": 1.5637254901960784, "no_speech_prob": 1.5936448107822798e-05}, {"id": 992, "seek": 515016, "start": 5155.36, "end": 5160.16, "text": " And once you've done that, it should be totally trivial to add the step that then uses that", "tokens": [400, 1564, 291, 600, 1096, 300, 11, 309, 820, 312, 3879, 26703, 281, 909, 264, 1823, 300, 550, 4960, 300], "temperature": 0.0, "avg_logprob": -0.1340123976784191, "compression_ratio": 1.5637254901960784, "no_speech_prob": 1.5936448107822798e-05}, {"id": 993, "seek": 515016, "start": 5160.16, "end": 5161.16, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.1340123976784191, "compression_ratio": 1.5637254901960784, "no_speech_prob": 1.5936448107822798e-05}, {"id": 994, "seek": 515016, "start": 5161.16, "end": 5167.48, "text": " So if you do that, then if you're interested, I would invite you to team up with me and", "tokens": [407, 498, 291, 360, 300, 11, 550, 498, 291, 434, 3102, 11, 286, 576, 7980, 291, 281, 1469, 493, 365, 385, 293], "temperature": 0.0, "avg_logprob": -0.1340123976784191, "compression_ratio": 1.5637254901960784, "no_speech_prob": 1.5936448107822798e-05}, {"id": 995, "seek": 515016, "start": 5167.48, "end": 5174.84, "text": " that we would then release this piece of software together and author a paper or a post together.", "tokens": [300, 321, 576, 550, 4374, 341, 2522, 295, 4722, 1214, 293, 3793, 257, 3035, 420, 257, 2183, 1214, 13], "temperature": 0.0, "avg_logprob": -0.1340123976784191, "compression_ratio": 1.5637254901960784, "no_speech_prob": 1.5936448107822798e-05}, {"id": 996, "seek": 517484, "start": 5174.84, "end": 5180.64, "text": " So that's my hope, is that one of you or a group of you will make that happen.", "tokens": [407, 300, 311, 452, 1454, 11, 307, 300, 472, 295, 291, 420, 257, 1594, 295, 291, 486, 652, 300, 1051, 13], "temperature": 0.0, "avg_logprob": -0.24486047956678603, "compression_ratio": 1.5135135135135136, "no_speech_prob": 4.06926337745972e-05}, {"id": 997, "seek": 517484, "start": 5180.64, "end": 5185.0, "text": " That would be super exciting because I think this would be great.", "tokens": [663, 576, 312, 1687, 4670, 570, 286, 519, 341, 576, 312, 869, 13], "temperature": 0.0, "avg_logprob": -0.24486047956678603, "compression_ratio": 1.5135135135135136, "no_speech_prob": 4.06926337745972e-05}, {"id": 998, "seek": 517484, "start": 5185.0, "end": 5192.16, "text": " We'd be showing people something pretty cool about the idea of writing GPU algorithms today.", "tokens": [492, 1116, 312, 4099, 561, 746, 1238, 1627, 466, 264, 1558, 295, 3579, 18407, 14642, 965, 13], "temperature": 0.0, "avg_logprob": -0.24486047956678603, "compression_ratio": 1.5135135135135136, "no_speech_prob": 4.06926337745972e-05}, {"id": 999, "seek": 517484, "start": 5192.16, "end": 5200.8, "text": " In fact, I found just during the break, here's a whole paper about how to write k-means with", "tokens": [682, 1186, 11, 286, 1352, 445, 1830, 264, 1821, 11, 510, 311, 257, 1379, 3035, 466, 577, 281, 2464, 350, 12, 1398, 599, 365], "temperature": 0.0, "avg_logprob": -0.24486047956678603, "compression_ratio": 1.5135135135135136, "no_speech_prob": 4.06926337745972e-05}, {"id": 1000, "seek": 517484, "start": 5200.8, "end": 5201.8, "text": " CUDA.", "tokens": [29777, 7509, 13], "temperature": 0.0, "avg_logprob": -0.24486047956678603, "compression_ratio": 1.5135135135135136, "no_speech_prob": 4.06926337745972e-05}, {"id": 1001, "seek": 520180, "start": 5201.8, "end": 5207.2, "text": " It used to be so much work.", "tokens": [467, 1143, 281, 312, 370, 709, 589, 13], "temperature": 0.0, "avg_logprob": -0.3203017077868498, "compression_ratio": 1.441860465116279, "no_speech_prob": 3.647765697678551e-05}, {"id": 1002, "seek": 520180, "start": 5207.2, "end": 5211.92, "text": " This is without even including any kind of approximate nearest neighbor piece or whatever.", "tokens": [639, 307, 1553, 754, 3009, 604, 733, 295, 30874, 23831, 5987, 2522, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.3203017077868498, "compression_ratio": 1.441860465116279, "no_speech_prob": 3.647765697678551e-05}, {"id": 1003, "seek": 520180, "start": 5211.92, "end": 5214.68, "text": " So I think this would be great.", "tokens": [407, 286, 519, 341, 576, 312, 869, 13], "temperature": 0.0, "avg_logprob": -0.3203017077868498, "compression_ratio": 1.441860465116279, "no_speech_prob": 3.647765697678551e-05}, {"id": 1004, "seek": 520180, "start": 5214.68, "end": 5215.92, "text": " So hopefully that will happen.", "tokens": [407, 4696, 300, 486, 1051, 13], "temperature": 0.0, "avg_logprob": -0.3203017077868498, "compression_ratio": 1.441860465116279, "no_speech_prob": 3.647765697678551e-05}, {"id": 1005, "seek": 520180, "start": 5215.92, "end": 5220.04, "text": " Okay, and look, it's just the right answer.", "tokens": [1033, 11, 293, 574, 11, 309, 311, 445, 264, 558, 1867, 13], "temperature": 0.0, "avg_logprob": -0.3203017077868498, "compression_ratio": 1.441860465116279, "no_speech_prob": 3.647765697678551e-05}, {"id": 1006, "seek": 520180, "start": 5220.04, "end": 5227.64, "text": " I guess to do it properly, we should also be replacing the Gaussian kernel bandwidth", "tokens": [286, 2041, 281, 360, 309, 6108, 11, 321, 820, 611, 312, 19139, 264, 39148, 28256, 23647], "temperature": 0.0, "avg_logprob": -0.3203017077868498, "compression_ratio": 1.441860465116279, "no_speech_prob": 3.647765697678551e-05}, {"id": 1007, "seek": 522764, "start": 5227.64, "end": 5236.4400000000005, "text": " with something that we figure out dynamically rather than have it hard-coded.", "tokens": [365, 746, 300, 321, 2573, 484, 43492, 2831, 813, 362, 309, 1152, 12, 66, 12340, 13], "temperature": 0.0, "avg_logprob": -0.19092300534248352, "compression_ratio": 1.475, "no_speech_prob": 1.2805384358216543e-05}, {"id": 1008, "seek": 522764, "start": 5236.4400000000005, "end": 5244.0, "text": " So big change, we're going to learn about chatbots.", "tokens": [407, 955, 1319, 11, 321, 434, 516, 281, 1466, 466, 5081, 65, 1971, 13], "temperature": 0.0, "avg_logprob": -0.19092300534248352, "compression_ratio": 1.475, "no_speech_prob": 1.2805384358216543e-05}, {"id": 1009, "seek": 522764, "start": 5244.0, "end": 5247.96, "text": " So we're going to start here with Slate.", "tokens": [407, 321, 434, 516, 281, 722, 510, 365, 6187, 473, 13], "temperature": 0.0, "avg_logprob": -0.19092300534248352, "compression_ratio": 1.475, "no_speech_prob": 1.2805384358216543e-05}, {"id": 1010, "seek": 522764, "start": 5247.96, "end": 5255.84, "text": " Facebook thinks it has found the secret to making bots less dumb.", "tokens": [4384, 7309, 309, 575, 1352, 264, 4054, 281, 1455, 35410, 1570, 10316, 13], "temperature": 0.0, "avg_logprob": -0.19092300534248352, "compression_ratio": 1.475, "no_speech_prob": 1.2805384358216543e-05}, {"id": 1011, "seek": 525584, "start": 5255.84, "end": 5262.360000000001, "text": " So this talks about a new thing called memory networks, which was demonstrated by Facebook.", "tokens": [407, 341, 6686, 466, 257, 777, 551, 1219, 4675, 9590, 11, 597, 390, 18772, 538, 4384, 13], "temperature": 0.0, "avg_logprob": -0.21279446082779124, "compression_ratio": 1.5129310344827587, "no_speech_prob": 1.0952889169857372e-05}, {"id": 1012, "seek": 525584, "start": 5262.360000000001, "end": 5266.68, "text": " You can feed it sentences that convey key plot points and lord of the rings and then", "tokens": [509, 393, 3154, 309, 16579, 300, 16965, 2141, 7542, 2793, 293, 15448, 295, 264, 11136, 293, 550], "temperature": 0.0, "avg_logprob": -0.21279446082779124, "compression_ratio": 1.5129310344827587, "no_speech_prob": 1.0952889169857372e-05}, {"id": 1013, "seek": 525584, "start": 5266.68, "end": 5269.68, "text": " ask it various questions.", "tokens": [1029, 309, 3683, 1651, 13], "temperature": 0.0, "avg_logprob": -0.21279446082779124, "compression_ratio": 1.5129310344827587, "no_speech_prob": 1.0952889169857372e-05}, {"id": 1014, "seek": 525584, "start": 5269.68, "end": 5276.96, "text": " Published a new paper on Archive that generalizes the approach.", "tokens": [21808, 4173, 257, 777, 3035, 322, 10984, 488, 300, 2674, 5660, 264, 3109, 13], "temperature": 0.0, "avg_logprob": -0.21279446082779124, "compression_ratio": 1.5129310344827587, "no_speech_prob": 1.0952889169857372e-05}, {"id": 1015, "seek": 525584, "start": 5276.96, "end": 5280.76, "text": " There was another long article about this on Popular Science in which they described", "tokens": [821, 390, 1071, 938, 7222, 466, 341, 322, 37637, 8976, 294, 597, 436, 7619], "temperature": 0.0, "avg_logprob": -0.21279446082779124, "compression_ratio": 1.5129310344827587, "no_speech_prob": 1.0952889169857372e-05}, {"id": 1016, "seek": 528076, "start": 5280.76, "end": 5286.0, "text": " its early progress towards a truly intelligent AI.", "tokens": [1080, 2440, 4205, 3030, 257, 4908, 13232, 7318, 13], "temperature": 0.0, "avg_logprob": -0.30412393592926396, "compression_ratio": 1.5462962962962963, "no_speech_prob": 5.77187756789499e-06}, {"id": 1017, "seek": 528076, "start": 5286.0, "end": 5290.0, "text": " Donald Kuhn is excited about working on a memory network, giving the ability to retain", "tokens": [8632, 591, 3232, 77, 307, 2919, 466, 1364, 322, 257, 4675, 3209, 11, 2902, 264, 3485, 281, 18340], "temperature": 0.0, "avg_logprob": -0.30412393592926396, "compression_ratio": 1.5462962962962963, "no_speech_prob": 5.77187756789499e-06}, {"id": 1018, "seek": 528076, "start": 5290.0, "end": 5291.360000000001, "text": " information.", "tokens": [1589, 13], "temperature": 0.0, "avg_logprob": -0.30412393592926396, "compression_ratio": 1.5462962962962963, "no_speech_prob": 5.77187756789499e-06}, {"id": 1019, "seek": 528076, "start": 5291.360000000001, "end": 5295.4400000000005, "text": " You can tell the network a story and have it answer questions.", "tokens": [509, 393, 980, 264, 3209, 257, 1657, 293, 362, 309, 1867, 1651, 13], "temperature": 0.0, "avg_logprob": -0.30412393592926396, "compression_ratio": 1.5462962962962963, "no_speech_prob": 5.77187756789499e-06}, {"id": 1020, "seek": 528076, "start": 5295.4400000000005, "end": 5301.360000000001, "text": " And so it even has this little gif.", "tokens": [400, 370, 309, 754, 575, 341, 707, 290, 351, 13], "temperature": 0.0, "avg_logprob": -0.30412393592926396, "compression_ratio": 1.5462962962962963, "no_speech_prob": 5.77187756789499e-06}, {"id": 1021, "seek": 528076, "start": 5301.360000000001, "end": 5309.2, "text": " So in the article they've got this little example showing reading a story of Lord of", "tokens": [407, 294, 264, 7222, 436, 600, 658, 341, 707, 1365, 4099, 3760, 257, 1657, 295, 3257, 295], "temperature": 0.0, "avg_logprob": -0.30412393592926396, "compression_ratio": 1.5462962962962963, "no_speech_prob": 5.77187756789499e-06}, {"id": 1022, "seek": 530920, "start": 5309.2, "end": 5312.88, "text": " the Rings and then asking various questions about Lord of the Rings, and it all looks", "tokens": [264, 38543, 293, 550, 3365, 3683, 1651, 466, 3257, 295, 264, 38543, 11, 293, 309, 439, 1542], "temperature": 0.0, "avg_logprob": -0.20990109207606553, "compression_ratio": 1.6, "no_speech_prob": 2.225254320364911e-06}, {"id": 1023, "seek": 530920, "start": 5312.88, "end": 5314.28, "text": " pretty impressive.", "tokens": [1238, 8992, 13], "temperature": 0.0, "avg_logprob": -0.20990109207606553, "compression_ratio": 1.6, "no_speech_prob": 2.225254320364911e-06}, {"id": 1024, "seek": 530920, "start": 5314.28, "end": 5318.16, "text": " So we're going to implement this paper.", "tokens": [407, 321, 434, 516, 281, 4445, 341, 3035, 13], "temperature": 0.0, "avg_logprob": -0.20990109207606553, "compression_ratio": 1.6, "no_speech_prob": 2.225254320364911e-06}, {"id": 1025, "seek": 530920, "start": 5318.16, "end": 5323.96, "text": " And the paper is called End-to-End Memory Networks.", "tokens": [400, 264, 3035, 307, 1219, 6967, 12, 1353, 12, 36952, 38203, 12640, 82, 13], "temperature": 0.0, "avg_logprob": -0.20990109207606553, "compression_ratio": 1.6, "no_speech_prob": 2.225254320364911e-06}, {"id": 1026, "seek": 530920, "start": 5323.96, "end": 5328.9, "text": " The paper was actually not shown on Lord of the Rings, but was actually shown on something", "tokens": [440, 3035, 390, 767, 406, 4898, 322, 3257, 295, 264, 38543, 11, 457, 390, 767, 4898, 322, 746], "temperature": 0.0, "avg_logprob": -0.20990109207606553, "compression_ratio": 1.6, "no_speech_prob": 2.225254320364911e-06}, {"id": 1027, "seek": 530920, "start": 5328.9, "end": 5329.9, "text": " called Babby.", "tokens": [1219, 15820, 2322, 13], "temperature": 0.0, "avg_logprob": -0.20990109207606553, "compression_ratio": 1.6, "no_speech_prob": 2.225254320364911e-06}, {"id": 1028, "seek": 530920, "start": 5329.9, "end": 5335.04, "text": " I don't know, Babby or Baby, I'm never quite sure which one it is.", "tokens": [286, 500, 380, 458, 11, 15820, 2322, 420, 9425, 11, 286, 478, 1128, 1596, 988, 597, 472, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.20990109207606553, "compression_ratio": 1.6, "no_speech_prob": 2.225254320364911e-06}, {"id": 1029, "seek": 533504, "start": 5335.04, "end": 5342.72, "text": " It's a paper describing a synthetic dataset towards AI complete question answering, a", "tokens": [467, 311, 257, 3035, 16141, 257, 23420, 28872, 3030, 7318, 3566, 1168, 13430, 11, 257], "temperature": 0.0, "avg_logprob": -0.21834678649902345, "compression_ratio": 1.5458715596330275, "no_speech_prob": 3.882952296407893e-05}, {"id": 1030, "seek": 533504, "start": 5342.72, "end": 5344.92, "text": " set of prerequisite toy tasks.", "tokens": [992, 295, 38333, 34152, 12058, 9608, 13], "temperature": 0.0, "avg_logprob": -0.21834678649902345, "compression_ratio": 1.5458715596330275, "no_speech_prob": 3.882952296407893e-05}, {"id": 1031, "seek": 533504, "start": 5344.92, "end": 5350.72, "text": " I saw a cute tweet last week explaining the meaning of various different types of titles", "tokens": [286, 1866, 257, 4052, 15258, 1036, 1243, 13468, 264, 3620, 295, 3683, 819, 3467, 295, 12992], "temperature": 0.0, "avg_logprob": -0.21834678649902345, "compression_ratio": 1.5458715596330275, "no_speech_prob": 3.882952296407893e-05}, {"id": 1032, "seek": 533504, "start": 5350.72, "end": 5351.72, "text": " of papers.", "tokens": [295, 10577, 13], "temperature": 0.0, "avg_logprob": -0.21834678649902345, "compression_ratio": 1.5458715596330275, "no_speech_prob": 3.882952296407893e-05}, {"id": 1033, "seek": 533504, "start": 5351.72, "end": 5356.32, "text": " It's basically saying towards means we've actually made no progress whatsoever.", "tokens": [467, 311, 1936, 1566, 3030, 1355, 321, 600, 767, 1027, 572, 4205, 17076, 13], "temperature": 0.0, "avg_logprob": -0.21834678649902345, "compression_ratio": 1.5458715596330275, "no_speech_prob": 3.882952296407893e-05}, {"id": 1034, "seek": 533504, "start": 5356.32, "end": 5361.48, "text": " So we'll take this with a grain of salt.", "tokens": [407, 321, 603, 747, 341, 365, 257, 12837, 295, 5139, 13], "temperature": 0.0, "avg_logprob": -0.21834678649902345, "compression_ratio": 1.5458715596330275, "no_speech_prob": 3.882952296407893e-05}, {"id": 1035, "seek": 536148, "start": 5361.48, "end": 5367.839999999999, "text": " So these introduce the Babby tasks, and the Babby tasks are probably best described by", "tokens": [407, 613, 5366, 264, 15820, 2322, 9608, 11, 293, 264, 15820, 2322, 9608, 366, 1391, 1151, 7619, 538], "temperature": 0.0, "avg_logprob": -0.2003369111281175, "compression_ratio": 1.6095890410958904, "no_speech_prob": 5.338131359167164e-06}, {"id": 1036, "seek": 536148, "start": 5367.839999999999, "end": 5370.839999999999, "text": " showing an example.", "tokens": [4099, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.2003369111281175, "compression_ratio": 1.6095890410958904, "no_speech_prob": 5.338131359167164e-06}, {"id": 1037, "seek": 536148, "start": 5370.839999999999, "end": 5374.16, "text": " Here's an example.", "tokens": [1692, 311, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.2003369111281175, "compression_ratio": 1.6095890410958904, "no_speech_prob": 5.338131359167164e-06}, {"id": 1038, "seek": 536148, "start": 5374.16, "end": 5379.32, "text": " So each task is basically a story.", "tokens": [407, 1184, 5633, 307, 1936, 257, 1657, 13], "temperature": 0.0, "avg_logprob": -0.2003369111281175, "compression_ratio": 1.6095890410958904, "no_speech_prob": 5.338131359167164e-06}, {"id": 1039, "seek": 536148, "start": 5379.32, "end": 5382.08, "text": " A story contains a list of sentences.", "tokens": [316, 1657, 8306, 257, 1329, 295, 16579, 13], "temperature": 0.0, "avg_logprob": -0.2003369111281175, "compression_ratio": 1.6095890410958904, "no_speech_prob": 5.338131359167164e-06}, {"id": 1040, "seek": 536148, "start": 5382.08, "end": 5386.719999999999, "text": " A sentence contains a list of words.", "tokens": [316, 8174, 8306, 257, 1329, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.2003369111281175, "compression_ratio": 1.6095890410958904, "no_speech_prob": 5.338131359167164e-06}, {"id": 1041, "seek": 538672, "start": 5386.72, "end": 5392.68, "text": " At the end of the story is a query to which there is an answer.", "tokens": [1711, 264, 917, 295, 264, 1657, 307, 257, 14581, 281, 597, 456, 307, 364, 1867, 13], "temperature": 0.0, "avg_logprob": -0.19277600681080537, "compression_ratio": 1.7044334975369457, "no_speech_prob": 1.7330463606413105e-06}, {"id": 1042, "seek": 538672, "start": 5392.68, "end": 5395.64, "text": " So the sentences are ordered in time.", "tokens": [407, 264, 16579, 366, 8866, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.19277600681080537, "compression_ratio": 1.7044334975369457, "no_speech_prob": 1.7330463606413105e-06}, {"id": 1043, "seek": 538672, "start": 5395.64, "end": 5396.64, "text": " So where is Daniel?", "tokens": [407, 689, 307, 8033, 30], "temperature": 0.0, "avg_logprob": -0.19277600681080537, "compression_ratio": 1.7044334975369457, "no_speech_prob": 1.7330463606413105e-06}, {"id": 1044, "seek": 538672, "start": 5396.64, "end": 5398.68, "text": " We have to go backwards.", "tokens": [492, 362, 281, 352, 12204, 13], "temperature": 0.0, "avg_logprob": -0.19277600681080537, "compression_ratio": 1.7044334975369457, "no_speech_prob": 1.7330463606413105e-06}, {"id": 1045, "seek": 538672, "start": 5398.68, "end": 5399.68, "text": " This says where John is.", "tokens": [639, 1619, 689, 2619, 307, 13], "temperature": 0.0, "avg_logprob": -0.19277600681080537, "compression_ratio": 1.7044334975369457, "no_speech_prob": 1.7330463606413105e-06}, {"id": 1046, "seek": 538672, "start": 5399.68, "end": 5400.68, "text": " This says where Daniel is.", "tokens": [639, 1619, 689, 8033, 307, 13], "temperature": 0.0, "avg_logprob": -0.19277600681080537, "compression_ratio": 1.7044334975369457, "no_speech_prob": 1.7330463606413105e-06}, {"id": 1047, "seek": 538672, "start": 5400.68, "end": 5401.68, "text": " Daniel is going to the bathroom.", "tokens": [8033, 307, 516, 281, 264, 8687, 13], "temperature": 0.0, "avg_logprob": -0.19277600681080537, "compression_ratio": 1.7044334975369457, "no_speech_prob": 1.7330463606413105e-06}, {"id": 1048, "seek": 538672, "start": 5401.68, "end": 5405.96, "text": " So Daniel is in the bathroom.", "tokens": [407, 8033, 307, 294, 264, 8687, 13], "temperature": 0.0, "avg_logprob": -0.19277600681080537, "compression_ratio": 1.7044334975369457, "no_speech_prob": 1.7330463606413105e-06}, {"id": 1049, "seek": 538672, "start": 5405.96, "end": 5408.68, "text": " So this is what the Babby tasks look like.", "tokens": [407, 341, 307, 437, 264, 15820, 2322, 9608, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.19277600681080537, "compression_ratio": 1.7044334975369457, "no_speech_prob": 1.7330463606413105e-06}, {"id": 1050, "seek": 538672, "start": 5408.68, "end": 5412.2, "text": " There's a number of different structures.", "tokens": [821, 311, 257, 1230, 295, 819, 9227, 13], "temperature": 0.0, "avg_logprob": -0.19277600681080537, "compression_ratio": 1.7044334975369457, "no_speech_prob": 1.7330463606413105e-06}, {"id": 1051, "seek": 541220, "start": 5412.2, "end": 5417.72, "text": " This is called a one-supporting-fact structure, which is to say you only have to go back and", "tokens": [639, 307, 1219, 257, 472, 12, 36622, 477, 278, 12, 44919, 3877, 11, 597, 307, 281, 584, 291, 787, 362, 281, 352, 646, 293], "temperature": 0.0, "avg_logprob": -0.18513280695134943, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.9637952846096596e-06}, {"id": 1052, "seek": 541220, "start": 5417.72, "end": 5421.639999999999, "text": " find one sentence in the story to figure out the answer.", "tokens": [915, 472, 8174, 294, 264, 1657, 281, 2573, 484, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.18513280695134943, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.9637952846096596e-06}, {"id": 1053, "seek": 541220, "start": 5421.639999999999, "end": 5425.5599999999995, "text": " We're also going to look at two-supporting-fact stories, which is ones where you're going", "tokens": [492, 434, 611, 516, 281, 574, 412, 732, 12, 36622, 477, 278, 12, 44919, 3676, 11, 597, 307, 2306, 689, 291, 434, 516], "temperature": 0.0, "avg_logprob": -0.18513280695134943, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.9637952846096596e-06}, {"id": 1054, "seek": 541220, "start": 5425.5599999999995, "end": 5431.08, "text": " to have to look twice.", "tokens": [281, 362, 281, 574, 6091, 13], "temperature": 0.0, "avg_logprob": -0.18513280695134943, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.9637952846096596e-06}, {"id": 1055, "seek": 541220, "start": 5431.08, "end": 5437.72, "text": " So the reading in these datasets is not remotely interesting.", "tokens": [407, 264, 3760, 294, 613, 42856, 307, 406, 20824, 1880, 13], "temperature": 0.0, "avg_logprob": -0.18513280695134943, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.9637952846096596e-06}, {"id": 1056, "seek": 543772, "start": 5437.72, "end": 5443.8, "text": " They're just a text file, we can parse them out.", "tokens": [814, 434, 445, 257, 2487, 3991, 11, 321, 393, 48377, 552, 484, 13], "temperature": 0.0, "avg_logprob": -0.16948871202366325, "compression_ratio": 1.7772020725388602, "no_speech_prob": 7.766876478854101e-06}, {"id": 1057, "seek": 543772, "start": 5443.8, "end": 5446.240000000001, "text": " There's various different text files for the various different tasks.", "tokens": [821, 311, 3683, 819, 2487, 7098, 337, 264, 3683, 819, 9608, 13], "temperature": 0.0, "avg_logprob": -0.16948871202366325, "compression_ratio": 1.7772020725388602, "no_speech_prob": 7.766876478854101e-06}, {"id": 1058, "seek": 543772, "start": 5446.240000000001, "end": 5451.6, "text": " If you're interested in the various different tasks, you can check out the paper.", "tokens": [759, 291, 434, 3102, 294, 264, 3683, 819, 9608, 11, 291, 393, 1520, 484, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.16948871202366325, "compression_ratio": 1.7772020725388602, "no_speech_prob": 7.766876478854101e-06}, {"id": 1059, "seek": 543772, "start": 5451.6, "end": 5454.84, "text": " We're going to be looking at single-supporting-fact and two-supporting-fact.", "tokens": [492, 434, 516, 281, 312, 1237, 412, 2167, 12, 36622, 477, 278, 12, 44919, 293, 732, 12, 36622, 477, 278, 12, 44919, 13], "temperature": 0.0, "avg_logprob": -0.16948871202366325, "compression_ratio": 1.7772020725388602, "no_speech_prob": 7.766876478854101e-06}, {"id": 1060, "seek": 543772, "start": 5454.84, "end": 5461.6, "text": " They have some with 10,000 examples and some with 1,000 examples.", "tokens": [814, 362, 512, 365, 1266, 11, 1360, 5110, 293, 512, 365, 502, 11, 1360, 5110, 13], "temperature": 0.0, "avg_logprob": -0.16948871202366325, "compression_ratio": 1.7772020725388602, "no_speech_prob": 7.766876478854101e-06}, {"id": 1061, "seek": 546160, "start": 5461.6, "end": 5469.240000000001, "text": " The goal is to be able to solve every one of their challenges with just 1,000 examples.", "tokens": [440, 3387, 307, 281, 312, 1075, 281, 5039, 633, 472, 295, 641, 4759, 365, 445, 502, 11, 1360, 5110, 13], "temperature": 0.0, "avg_logprob": -0.13746503470600516, "compression_ratio": 1.4381443298969072, "no_speech_prob": 1.4144699207463418e-06}, {"id": 1062, "seek": 546160, "start": 5469.240000000001, "end": 5477.360000000001, "text": " And this paper is not successful at that goal, but it makes some movement towards it.", "tokens": [400, 341, 3035, 307, 406, 4406, 412, 300, 3387, 11, 457, 309, 1669, 512, 3963, 3030, 309, 13], "temperature": 0.0, "avg_logprob": -0.13746503470600516, "compression_ratio": 1.4381443298969072, "no_speech_prob": 1.4144699207463418e-06}, {"id": 1063, "seek": 546160, "start": 5477.360000000001, "end": 5487.240000000001, "text": " So basically we're going to put that into a bunch of different lists of stories along", "tokens": [407, 1936, 321, 434, 516, 281, 829, 300, 666, 257, 3840, 295, 819, 14511, 295, 3676, 2051], "temperature": 0.0, "avg_logprob": -0.13746503470600516, "compression_ratio": 1.4381443298969072, "no_speech_prob": 1.4144699207463418e-06}, {"id": 1064, "seek": 546160, "start": 5487.240000000001, "end": 5490.160000000001, "text": " with their queries.", "tokens": [365, 641, 24109, 13], "temperature": 0.0, "avg_logprob": -0.13746503470600516, "compression_ratio": 1.4381443298969072, "no_speech_prob": 1.4144699207463418e-06}, {"id": 1065, "seek": 549016, "start": 5490.16, "end": 5494.32, "text": " We can start off by having a look at some statistics about them.", "tokens": [492, 393, 722, 766, 538, 1419, 257, 574, 412, 512, 12523, 466, 552, 13], "temperature": 0.0, "avg_logprob": -0.2553829076338787, "compression_ratio": 1.6097560975609757, "no_speech_prob": 6.605023372685537e-05}, {"id": 1066, "seek": 549016, "start": 5494.32, "end": 5498.8, "text": " So the first is, for each story, what's the maximum number of sentences in a story?", "tokens": [407, 264, 700, 307, 11, 337, 1184, 1657, 11, 437, 311, 264, 6674, 1230, 295, 16579, 294, 257, 1657, 30], "temperature": 0.0, "avg_logprob": -0.2553829076338787, "compression_ratio": 1.6097560975609757, "no_speech_prob": 6.605023372685537e-05}, {"id": 1067, "seek": 549016, "start": 5498.8, "end": 5499.8, "text": " And the answer is 10.", "tokens": [400, 264, 1867, 307, 1266, 13], "temperature": 0.0, "avg_logprob": -0.2553829076338787, "compression_ratio": 1.6097560975609757, "no_speech_prob": 6.605023372685537e-05}, {"id": 1068, "seek": 549016, "start": 5499.8, "end": 5504.48, "text": " So Lord of the Rings it ain't.", "tokens": [407, 3257, 295, 264, 38543, 309, 7862, 380, 13], "temperature": 0.0, "avg_logprob": -0.2553829076338787, "compression_ratio": 1.6097560975609757, "no_speech_prob": 6.605023372685537e-05}, {"id": 1069, "seek": 549016, "start": 5504.48, "end": 5508.72, "text": " In fact, if you go back and you look at the GIF, when it says read story, Lord of the", "tokens": [682, 1186, 11, 498, 291, 352, 646, 293, 291, 574, 412, 264, 460, 12775, 11, 562, 309, 1619, 1401, 1657, 11, 3257, 295, 264], "temperature": 0.0, "avg_logprob": -0.2553829076338787, "compression_ratio": 1.6097560975609757, "no_speech_prob": 6.605023372685537e-05}, {"id": 1070, "seek": 549016, "start": 5508.72, "end": 5511.84, "text": " Rings, that's the whole Lord of the Rings.", "tokens": [38543, 11, 300, 311, 264, 1379, 3257, 295, 264, 38543, 13], "temperature": 0.0, "avg_logprob": -0.2553829076338787, "compression_ratio": 1.6097560975609757, "no_speech_prob": 6.605023372685537e-05}, {"id": 1071, "seek": 551184, "start": 5511.84, "end": 5527.96, "text": " The total number of different words in this thing is 32.", "tokens": [440, 3217, 1230, 295, 819, 2283, 294, 341, 551, 307, 8858, 13], "temperature": 0.0, "avg_logprob": -0.1150516576545183, "compression_ratio": 1.4220183486238531, "no_speech_prob": 3.9669694160693325e-06}, {"id": 1072, "seek": 551184, "start": 5527.96, "end": 5533.24, "text": " The maximum length of any sentence in a story is 8.", "tokens": [440, 6674, 4641, 295, 604, 8174, 294, 257, 1657, 307, 1649, 13], "temperature": 0.0, "avg_logprob": -0.1150516576545183, "compression_ratio": 1.4220183486238531, "no_speech_prob": 3.9669694160693325e-06}, {"id": 1073, "seek": 551184, "start": 5533.24, "end": 5537.92, "text": " The maximum number of words in any query is 4.", "tokens": [440, 6674, 1230, 295, 2283, 294, 604, 14581, 307, 1017, 13], "temperature": 0.0, "avg_logprob": -0.1150516576545183, "compression_ratio": 1.4220183486238531, "no_speech_prob": 3.9669694160693325e-06}, {"id": 1074, "seek": 553792, "start": 5537.92, "end": 5545.36, "text": " So we're immediately thinking what the hell, because this was presented by the press as", "tokens": [407, 321, 434, 4258, 1953, 437, 264, 4921, 11, 570, 341, 390, 8212, 538, 264, 1886, 382], "temperature": 0.0, "avg_logprob": -0.16706617311997848, "compression_ratio": 1.5627705627705628, "no_speech_prob": 5.338109076546971e-06}, {"id": 1075, "seek": 553792, "start": 5545.36, "end": 5551.28, "text": " being the secret to making bots less dumb, and showed us that they took a story and summarized", "tokens": [885, 264, 4054, 281, 1455, 35410, 1570, 10316, 11, 293, 4712, 505, 300, 436, 1890, 257, 1657, 293, 14611, 1602], "temperature": 0.0, "avg_logprob": -0.16706617311997848, "compression_ratio": 1.5627705627705628, "no_speech_prob": 5.338109076546971e-06}, {"id": 1076, "seek": 553792, "start": 5551.28, "end": 5557.6, "text": " Lord of the Rings, made a plot point and asked various questions.", "tokens": [3257, 295, 264, 38543, 11, 1027, 257, 7542, 935, 293, 2351, 3683, 1651, 13], "temperature": 0.0, "avg_logprob": -0.16706617311997848, "compression_ratio": 1.5627705627705628, "no_speech_prob": 5.338109076546971e-06}, {"id": 1077, "seek": 553792, "start": 5557.6, "end": 5561.4, "text": " And clearly that's not entirely true.", "tokens": [400, 4448, 300, 311, 406, 7696, 2074, 13], "temperature": 0.0, "avg_logprob": -0.16706617311997848, "compression_ratio": 1.5627705627705628, "no_speech_prob": 5.338109076546971e-06}, {"id": 1078, "seek": 553792, "start": 5561.4, "end": 5567.0, "text": " If you look at even the stories, the first word is always somebody's name.", "tokens": [759, 291, 574, 412, 754, 264, 3676, 11, 264, 700, 1349, 307, 1009, 2618, 311, 1315, 13], "temperature": 0.0, "avg_logprob": -0.16706617311997848, "compression_ratio": 1.5627705627705628, "no_speech_prob": 5.338109076546971e-06}, {"id": 1079, "seek": 556700, "start": 5567.0, "end": 5571.52, "text": " The second word here is always some synonym for move.", "tokens": [440, 1150, 1349, 510, 307, 1009, 512, 5451, 12732, 337, 1286, 13], "temperature": 0.0, "avg_logprob": -0.2818831275491154, "compression_ratio": 1.5495049504950495, "no_speech_prob": 2.668799970706459e-05}, {"id": 1080, "seek": 556700, "start": 5571.52, "end": 5577.72, "text": " There's then a bunch of prepositions, and then the last word is always place.", "tokens": [821, 311, 550, 257, 3840, 295, 2666, 329, 2451, 11, 293, 550, 264, 1036, 1349, 307, 1009, 1081, 13], "temperature": 0.0, "avg_logprob": -0.2818831275491154, "compression_ratio": 1.5495049504950495, "no_speech_prob": 2.668799970706459e-05}, {"id": 1081, "seek": 556700, "start": 5577.72, "end": 5580.62, "text": " So these story tasks are very, very, very toy.", "tokens": [407, 613, 1657, 9608, 366, 588, 11, 588, 11, 588, 12058, 13], "temperature": 0.0, "avg_logprob": -0.2818831275491154, "compression_ratio": 1.5495049504950495, "no_speech_prob": 2.668799970706459e-05}, {"id": 1082, "seek": 556700, "start": 5580.62, "end": 5588.92, "text": " So immediately we're thinking maybe this is not the step to making bots less dumb, or", "tokens": [407, 4258, 321, 434, 1953, 1310, 341, 307, 406, 264, 1823, 281, 1455, 35410, 1570, 10316, 11, 420], "temperature": 0.0, "avg_logprob": -0.2818831275491154, "compression_ratio": 1.5495049504950495, "no_speech_prob": 2.668799970706459e-05}, {"id": 1083, "seek": 556700, "start": 5588.92, "end": 5591.52, "text": " whatever they said here.", "tokens": [2035, 436, 848, 510, 13], "temperature": 0.0, "avg_logprob": -0.2818831275491154, "compression_ratio": 1.5495049504950495, "no_speech_prob": 2.668799970706459e-05}, {"id": 1084, "seek": 556700, "start": 5591.52, "end": 5594.52, "text": " A truly intelligent AI.", "tokens": [316, 4908, 13232, 7318, 13], "temperature": 0.0, "avg_logprob": -0.2818831275491154, "compression_ratio": 1.5495049504950495, "no_speech_prob": 2.668799970706459e-05}, {"id": 1085, "seek": 559452, "start": 5594.52, "end": 5599.72, "text": " Maybe it's towards a truly intelligent AI.", "tokens": [2704, 309, 311, 3030, 257, 4908, 13232, 7318, 13], "temperature": 0.0, "avg_logprob": -0.12535420950357015, "compression_ratio": 1.574585635359116, "no_speech_prob": 1.2805271580873523e-05}, {"id": 1086, "seek": 559452, "start": 5599.72, "end": 5607.0, "text": " So to get this into Keras, we need to turn it into a tensor in which everything is the", "tokens": [407, 281, 483, 341, 666, 591, 6985, 11, 321, 643, 281, 1261, 309, 666, 257, 40863, 294, 597, 1203, 307, 264], "temperature": 0.0, "avg_logprob": -0.12535420950357015, "compression_ratio": 1.574585635359116, "no_speech_prob": 1.2805271580873523e-05}, {"id": 1087, "seek": 559452, "start": 5607.0, "end": 5608.8, "text": " same size.", "tokens": [912, 2744, 13], "temperature": 0.0, "avg_logprob": -0.12535420950357015, "compression_ratio": 1.574585635359116, "no_speech_prob": 1.2805271580873523e-05}, {"id": 1088, "seek": 559452, "start": 5608.8, "end": 5615.400000000001, "text": " So we use pad sequences for that like we did in the last part of the course, which will", "tokens": [407, 321, 764, 6887, 22978, 337, 300, 411, 321, 630, 294, 264, 1036, 644, 295, 264, 1164, 11, 597, 486], "temperature": 0.0, "avg_logprob": -0.12535420950357015, "compression_ratio": 1.574585635359116, "no_speech_prob": 1.2805271580873523e-05}, {"id": 1089, "seek": 559452, "start": 5615.400000000001, "end": 5619.84, "text": " add zeros to make sure that everything is the same size.", "tokens": [909, 35193, 281, 652, 988, 300, 1203, 307, 264, 912, 2744, 13], "temperature": 0.0, "avg_logprob": -0.12535420950357015, "compression_ratio": 1.574585635359116, "no_speech_prob": 1.2805271580873523e-05}, {"id": 1090, "seek": 561984, "start": 5619.84, "end": 5627.2, "text": " So the other thing we'll do is we will create a dictionary from words to integers to turn", "tokens": [407, 264, 661, 551, 321, 603, 360, 307, 321, 486, 1884, 257, 25890, 490, 2283, 281, 41674, 281, 1261], "temperature": 0.0, "avg_logprob": -0.16657869499849987, "compression_ratio": 1.6594594594594594, "no_speech_prob": 8.800999239610974e-06}, {"id": 1091, "seek": 561984, "start": 5627.2, "end": 5628.52, "text": " every word into an index.", "tokens": [633, 1349, 666, 364, 8186, 13], "temperature": 0.0, "avg_logprob": -0.16657869499849987, "compression_ratio": 1.6594594594594594, "no_speech_prob": 8.800999239610974e-06}, {"id": 1092, "seek": 561984, "start": 5628.52, "end": 5633.8, "text": " So we're going to turn every word into an index and then pad them so that they're all", "tokens": [407, 321, 434, 516, 281, 1261, 633, 1349, 666, 364, 8186, 293, 550, 6887, 552, 370, 300, 436, 434, 439], "temperature": 0.0, "avg_logprob": -0.16657869499849987, "compression_ratio": 1.6594594594594594, "no_speech_prob": 8.800999239610974e-06}, {"id": 1093, "seek": 561984, "start": 5633.8, "end": 5636.92, "text": " the same length.", "tokens": [264, 912, 4641, 13], "temperature": 0.0, "avg_logprob": -0.16657869499849987, "compression_ratio": 1.6594594594594594, "no_speech_prob": 8.800999239610974e-06}, {"id": 1094, "seek": 561984, "start": 5636.92, "end": 5646.84, "text": " And then that's going to give us inputs train, 10,000 stories, each one of 10 sentences,", "tokens": [400, 550, 300, 311, 516, 281, 976, 505, 15743, 3847, 11, 1266, 11, 1360, 3676, 11, 1184, 472, 295, 1266, 16579, 11], "temperature": 0.0, "avg_logprob": -0.16657869499849987, "compression_ratio": 1.6594594594594594, "no_speech_prob": 8.800999239610974e-06}, {"id": 1095, "seek": 564684, "start": 5646.84, "end": 5652.64, "text": " each one of 8 words, anything that's not 10 sentences long is going to get sentences of", "tokens": [1184, 472, 295, 1649, 2283, 11, 1340, 300, 311, 406, 1266, 16579, 938, 307, 516, 281, 483, 16579, 295], "temperature": 0.0, "avg_logprob": -0.23806457984738233, "compression_ratio": 1.596774193548387, "no_speech_prob": 6.048860541341128e-06}, {"id": 1096, "seek": 564684, "start": 5652.64, "end": 5658.56, "text": " just zeros, any sentence that's not 8 words long will get some zeros appended to that.", "tokens": [445, 35193, 11, 604, 8174, 300, 311, 406, 1649, 2283, 938, 486, 483, 512, 35193, 724, 3502, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.23806457984738233, "compression_ratio": 1.596774193548387, "no_speech_prob": 6.048860541341128e-06}, {"id": 1097, "seek": 564684, "start": 5658.56, "end": 5663.92, "text": " And you know for the test, except we just got 1000.", "tokens": [400, 291, 458, 337, 264, 1500, 11, 3993, 321, 445, 658, 9714, 13], "temperature": 0.0, "avg_logprob": -0.23806457984738233, "compression_ratio": 1.596774193548387, "no_speech_prob": 6.048860541341128e-06}, {"id": 1098, "seek": 564684, "start": 5663.92, "end": 5668.400000000001, "text": " So how do we do this?", "tokens": [407, 577, 360, 321, 360, 341, 30], "temperature": 0.0, "avg_logprob": -0.23806457984738233, "compression_ratio": 1.596774193548387, "no_speech_prob": 6.048860541341128e-06}, {"id": 1099, "seek": 564684, "start": 5668.400000000001, "end": 5673.64, "text": " Not surprisingly, we're going to use embeddings.", "tokens": [1726, 17600, 11, 321, 434, 516, 281, 764, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.23806457984738233, "compression_ratio": 1.596774193548387, "no_speech_prob": 6.048860541341128e-06}, {"id": 1100, "seek": 567364, "start": 5673.64, "end": 5681.160000000001, "text": " Now we've never done this before, we have to turn a sentence into an embedding, not", "tokens": [823, 321, 600, 1128, 1096, 341, 949, 11, 321, 362, 281, 1261, 257, 8174, 666, 364, 12240, 3584, 11, 406], "temperature": 0.0, "avg_logprob": -0.14913494819033463, "compression_ratio": 1.7543103448275863, "no_speech_prob": 2.7264582058705855e-06}, {"id": 1101, "seek": 567364, "start": 5681.160000000001, "end": 5684.64, "text": " just a word into an embedding.", "tokens": [445, 257, 1349, 666, 364, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.14913494819033463, "compression_ratio": 1.7543103448275863, "no_speech_prob": 2.7264582058705855e-06}, {"id": 1102, "seek": 567364, "start": 5684.64, "end": 5690.4800000000005, "text": " So there's lots of interesting ways of turning a sentence into an embedding.", "tokens": [407, 456, 311, 3195, 295, 1880, 2098, 295, 6246, 257, 8174, 666, 364, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.14913494819033463, "compression_ratio": 1.7543103448275863, "no_speech_prob": 2.7264582058705855e-06}, {"id": 1103, "seek": 567364, "start": 5690.4800000000005, "end": 5695.200000000001, "text": " But when you're just doing towards intelligent AI, you don't do any of them, you instead", "tokens": [583, 562, 291, 434, 445, 884, 3030, 13232, 7318, 11, 291, 500, 380, 360, 604, 295, 552, 11, 291, 2602], "temperature": 0.0, "avg_logprob": -0.14913494819033463, "compression_ratio": 1.7543103448275863, "no_speech_prob": 2.7264582058705855e-06}, {"id": 1104, "seek": 567364, "start": 5695.200000000001, "end": 5697.0, "text": " just add the embeddings up.", "tokens": [445, 909, 264, 12240, 29432, 493, 13], "temperature": 0.0, "avg_logprob": -0.14913494819033463, "compression_ratio": 1.7543103448275863, "no_speech_prob": 2.7264582058705855e-06}, {"id": 1105, "seek": 567364, "start": 5697.0, "end": 5699.320000000001, "text": " And that's what happened in this paper.", "tokens": [400, 300, 311, 437, 2011, 294, 341, 3035, 13], "temperature": 0.0, "avg_logprob": -0.14913494819033463, "compression_ratio": 1.7543103448275863, "no_speech_prob": 2.7264582058705855e-06}, {"id": 1106, "seek": 567364, "start": 5699.320000000001, "end": 5702.240000000001, "text": " And if you look at the way it was set up, you can see why.", "tokens": [400, 498, 291, 574, 412, 264, 636, 309, 390, 992, 493, 11, 291, 393, 536, 983, 13], "temperature": 0.0, "avg_logprob": -0.14913494819033463, "compression_ratio": 1.7543103448275863, "no_speech_prob": 2.7264582058705855e-06}, {"id": 1107, "seek": 570224, "start": 5702.24, "end": 5704.48, "text": " You can just add the embeddings up.", "tokens": [509, 393, 445, 909, 264, 12240, 29432, 493, 13], "temperature": 0.0, "avg_logprob": -0.14527022043863932, "compression_ratio": 1.7340823970037453, "no_speech_prob": 3.446566324782907e-06}, {"id": 1108, "seek": 570224, "start": 5704.48, "end": 5710.0, "text": " Mary, John and Sandra, they only ever appear in one place, they're always the object of", "tokens": [6059, 11, 2619, 293, 28184, 11, 436, 787, 1562, 4204, 294, 472, 1081, 11, 436, 434, 1009, 264, 2657, 295], "temperature": 0.0, "avg_logprob": -0.14527022043863932, "compression_ratio": 1.7340823970037453, "no_speech_prob": 3.446566324782907e-06}, {"id": 1109, "seek": 570224, "start": 5710.0, "end": 5711.0, "text": " this.", "tokens": [341, 13], "temperature": 0.0, "avg_logprob": -0.14527022043863932, "compression_ratio": 1.7340823970037453, "no_speech_prob": 3.446566324782907e-06}, {"id": 1110, "seek": 570224, "start": 5711.0, "end": 5714.84, "text": " The verb is always the same thing, the prepositions are always meaningless, and the last word", "tokens": [440, 9595, 307, 1009, 264, 912, 551, 11, 264, 2666, 329, 2451, 366, 1009, 33232, 11, 293, 264, 1036, 1349], "temperature": 0.0, "avg_logprob": -0.14527022043863932, "compression_ratio": 1.7340823970037453, "no_speech_prob": 3.446566324782907e-06}, {"id": 1111, "seek": 570224, "start": 5714.84, "end": 5716.28, "text": " is always a place.", "tokens": [307, 1009, 257, 1081, 13], "temperature": 0.0, "avg_logprob": -0.14527022043863932, "compression_ratio": 1.7340823970037453, "no_speech_prob": 3.446566324782907e-06}, {"id": 1112, "seek": 570224, "start": 5716.28, "end": 5722.44, "text": " So to figure out what a whole sentence says, you can just add up the word concepts.", "tokens": [407, 281, 2573, 484, 437, 257, 1379, 8174, 1619, 11, 291, 393, 445, 909, 493, 264, 1349, 10392, 13], "temperature": 0.0, "avg_logprob": -0.14527022043863932, "compression_ratio": 1.7340823970037453, "no_speech_prob": 3.446566324782907e-06}, {"id": 1113, "seek": 570224, "start": 5722.44, "end": 5725.96, "text": " The order of them doesn't make any difference, there's no nots, there's nothing that makes", "tokens": [440, 1668, 295, 552, 1177, 380, 652, 604, 2649, 11, 456, 311, 572, 406, 82, 11, 456, 311, 1825, 300, 1669], "temperature": 0.0, "avg_logprob": -0.14527022043863932, "compression_ratio": 1.7340823970037453, "no_speech_prob": 3.446566324782907e-06}, {"id": 1114, "seek": 570224, "start": 5725.96, "end": 5728.48, "text": " language remotely complicated or interesting.", "tokens": [2856, 20824, 6179, 420, 1880, 13], "temperature": 0.0, "avg_logprob": -0.14527022043863932, "compression_ratio": 1.7340823970037453, "no_speech_prob": 3.446566324782907e-06}, {"id": 1115, "seek": 572848, "start": 5728.48, "end": 5734.32, "text": " So what we're going to do is we're going to create an input for our stories with the number", "tokens": [407, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 1884, 364, 4846, 337, 527, 3676, 365, 264, 1230], "temperature": 0.0, "avg_logprob": -0.15262698235912858, "compression_ratio": 1.8130434782608695, "no_speech_prob": 5.7718661992112175e-06}, {"id": 1116, "seek": 572848, "start": 5734.32, "end": 5737.679999999999, "text": " of sentences and the length of each one.", "tokens": [295, 16579, 293, 264, 4641, 295, 1184, 472, 13], "temperature": 0.0, "avg_logprob": -0.15262698235912858, "compression_ratio": 1.8130434782608695, "no_speech_prob": 5.7718661992112175e-06}, {"id": 1117, "seek": 572848, "start": 5737.679999999999, "end": 5740.799999999999, "text": " We're going to take each word and put it through an embedding.", "tokens": [492, 434, 516, 281, 747, 1184, 1349, 293, 829, 309, 807, 364, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.15262698235912858, "compression_ratio": 1.8130434782608695, "no_speech_prob": 5.7718661992112175e-06}, {"id": 1118, "seek": 572848, "start": 5740.799999999999, "end": 5743.5199999999995, "text": " So that's what time distributed is doing here.", "tokens": [407, 300, 311, 437, 565, 12631, 307, 884, 510, 13], "temperature": 0.0, "avg_logprob": -0.15262698235912858, "compression_ratio": 1.8130434782608695, "no_speech_prob": 5.7718661992112175e-06}, {"id": 1119, "seek": 572848, "start": 5743.5199999999995, "end": 5746.719999999999, "text": " It's putting each word through a separate embedding.", "tokens": [467, 311, 3372, 1184, 1349, 807, 257, 4994, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.15262698235912858, "compression_ratio": 1.8130434782608695, "no_speech_prob": 5.7718661992112175e-06}, {"id": 1120, "seek": 572848, "start": 5746.719999999999, "end": 5752.639999999999, "text": " And then we do a lambda layer to add them up.", "tokens": [400, 550, 321, 360, 257, 13607, 4583, 281, 909, 552, 493, 13], "temperature": 0.0, "avg_logprob": -0.15262698235912858, "compression_ratio": 1.8130434782608695, "no_speech_prob": 5.7718661992112175e-06}, {"id": 1121, "seek": 572848, "start": 5752.639999999999, "end": 5757.94, "text": " So here is our very sophisticated approach to creating sentence embeddings.", "tokens": [407, 510, 307, 527, 588, 16950, 3109, 281, 4084, 8174, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.15262698235912858, "compression_ratio": 1.8130434782608695, "no_speech_prob": 5.7718661992112175e-06}, {"id": 1122, "seek": 575794, "start": 5757.94, "end": 5760.679999999999, "text": " So we do that for our story.", "tokens": [407, 321, 360, 300, 337, 527, 1657, 13], "temperature": 0.0, "avg_logprob": -0.18174092522982893, "compression_ratio": 1.7136929460580912, "no_speech_prob": 4.495153916650452e-06}, {"id": 1123, "seek": 575794, "start": 5760.679999999999, "end": 5768.12, "text": " So we end up with something which rather than being 10 by 8, i.e. 10 sentences by 8 words,", "tokens": [407, 321, 917, 493, 365, 746, 597, 2831, 813, 885, 1266, 538, 1649, 11, 741, 13, 68, 13, 1266, 16579, 538, 1649, 2283, 11], "temperature": 0.0, "avg_logprob": -0.18174092522982893, "compression_ratio": 1.7136929460580912, "no_speech_prob": 4.495153916650452e-06}, {"id": 1124, "seek": 575794, "start": 5768.12, "end": 5775.12, "text": " is now 10 by 20, that is 10 sentences by length 20 embedding.", "tokens": [307, 586, 1266, 538, 945, 11, 300, 307, 1266, 16579, 538, 4641, 945, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.18174092522982893, "compression_ratio": 1.7136929460580912, "no_speech_prob": 4.495153916650452e-06}, {"id": 1125, "seek": 575794, "start": 5775.12, "end": 5778.719999999999, "text": " So each one of our 10 sentences has been turned into a length 20 embedding.", "tokens": [407, 1184, 472, 295, 527, 1266, 16579, 575, 668, 3574, 666, 257, 4641, 945, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.18174092522982893, "compression_ratio": 1.7136929460580912, "no_speech_prob": 4.495153916650452e-06}, {"id": 1126, "seek": 575794, "start": 5778.719999999999, "end": 5780.2, "text": " And we're just starting with a random embedding.", "tokens": [400, 321, 434, 445, 2891, 365, 257, 4974, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.18174092522982893, "compression_ratio": 1.7136929460580912, "no_speech_prob": 4.495153916650452e-06}, {"id": 1127, "seek": 575794, "start": 5780.2, "end": 5786.48, "text": " We're not going to use Word2Vec or anything because we don't need the complexity of that", "tokens": [492, 434, 406, 516, 281, 764, 8725, 17, 53, 3045, 420, 1340, 570, 321, 500, 380, 643, 264, 14024, 295, 300], "temperature": 0.0, "avg_logprob": -0.18174092522982893, "compression_ratio": 1.7136929460580912, "no_speech_prob": 4.495153916650452e-06}, {"id": 1128, "seek": 575794, "start": 5786.48, "end": 5787.48, "text": " vocabulary model.", "tokens": [19864, 2316, 13], "temperature": 0.0, "avg_logprob": -0.18174092522982893, "compression_ratio": 1.7136929460580912, "no_speech_prob": 4.495153916650452e-06}, {"id": 1129, "seek": 578748, "start": 5787.48, "end": 5792.28, "text": " We're going to do exactly the same thing for the query.", "tokens": [492, 434, 516, 281, 360, 2293, 264, 912, 551, 337, 264, 14581, 13], "temperature": 0.0, "avg_logprob": -0.14228898629374886, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.8448185983288568e-06}, {"id": 1130, "seek": 578748, "start": 5792.28, "end": 5797.2, "text": " We don't need to use time distributed this time.", "tokens": [492, 500, 380, 643, 281, 764, 565, 12631, 341, 565, 13], "temperature": 0.0, "avg_logprob": -0.14228898629374886, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.8448185983288568e-06}, {"id": 1131, "seek": 578748, "start": 5797.2, "end": 5804.48, "text": " We can just take the query because this time we have just one query.", "tokens": [492, 393, 445, 747, 264, 14581, 570, 341, 565, 321, 362, 445, 472, 14581, 13], "temperature": 0.0, "avg_logprob": -0.14228898629374886, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.8448185983288568e-06}, {"id": 1132, "seek": 578748, "start": 5804.48, "end": 5811.48, "text": " So we can do the embedding, sum it up, and then we use reshape to add a unit access to", "tokens": [407, 321, 393, 360, 264, 12240, 3584, 11, 2408, 309, 493, 11, 293, 550, 321, 764, 725, 42406, 281, 909, 257, 4985, 2105, 281], "temperature": 0.0, "avg_logprob": -0.14228898629374886, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.8448185983288568e-06}, {"id": 1133, "seek": 578748, "start": 5811.48, "end": 5816.32, "text": " the front so that it's now the same basic rank.", "tokens": [264, 1868, 370, 300, 309, 311, 586, 264, 912, 3875, 6181, 13], "temperature": 0.0, "avg_logprob": -0.14228898629374886, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.8448185983288568e-06}, {"id": 1134, "seek": 581632, "start": 5816.32, "end": 5822.16, "text": " We now have one question of embedding to length 20.", "tokens": [492, 586, 362, 472, 1168, 295, 12240, 3584, 281, 4641, 945, 13], "temperature": 0.0, "avg_logprob": -0.12824861738416884, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.9947228793171234e-06}, {"id": 1135, "seek": 581632, "start": 5822.16, "end": 5829.719999999999, "text": " So we have 10 sentences of the story and one query.", "tokens": [407, 321, 362, 1266, 16579, 295, 264, 1657, 293, 472, 14581, 13], "temperature": 0.0, "avg_logprob": -0.12824861738416884, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.9947228793171234e-06}, {"id": 1136, "seek": 581632, "start": 5829.719999999999, "end": 5835.5199999999995, "text": " So what is the memory network, or more specifically the more advanced end-to-end memory network?", "tokens": [407, 437, 307, 264, 4675, 3209, 11, 420, 544, 4682, 264, 544, 7339, 917, 12, 1353, 12, 521, 4675, 3209, 30], "temperature": 0.0, "avg_logprob": -0.12824861738416884, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.9947228793171234e-06}, {"id": 1137, "seek": 581632, "start": 5835.5199999999995, "end": 5839.32, "text": " And the answer is, it is this.", "tokens": [400, 264, 1867, 307, 11, 309, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.12824861738416884, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.9947228793171234e-06}, {"id": 1138, "seek": 581632, "start": 5839.32, "end": 5845.639999999999, "text": " As per usual, when you get down to it, it's less than a page of code to do these things.", "tokens": [1018, 680, 7713, 11, 562, 291, 483, 760, 281, 309, 11, 309, 311, 1570, 813, 257, 3028, 295, 3089, 281, 360, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.12824861738416884, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.9947228793171234e-06}, {"id": 1139, "seek": 584564, "start": 5845.64, "end": 5855.8, "text": " Let's draw this before we look at the code.", "tokens": [961, 311, 2642, 341, 949, 321, 574, 412, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.21832429601791056, "compression_ratio": 1.4144144144144144, "no_speech_prob": 3.966958502132911e-06}, {"id": 1140, "seek": 584564, "start": 5855.8, "end": 5859.76, "text": " So we have a bunch of sentences.", "tokens": [407, 321, 362, 257, 3840, 295, 16579, 13], "temperature": 0.0, "avg_logprob": -0.21832429601791056, "compression_ratio": 1.4144144144144144, "no_speech_prob": 3.966958502132911e-06}, {"id": 1141, "seek": 584564, "start": 5859.76, "end": 5865.8, "text": " Let's just use 4 sentences for now.", "tokens": [961, 311, 445, 764, 1017, 16579, 337, 586, 13], "temperature": 0.0, "avg_logprob": -0.21832429601791056, "compression_ratio": 1.4144144144144144, "no_speech_prob": 3.966958502132911e-06}, {"id": 1142, "seek": 584564, "start": 5865.8, "end": 5870.320000000001, "text": " So each sentence contained a bunch of words.", "tokens": [407, 1184, 8174, 16212, 257, 3840, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.21832429601791056, "compression_ratio": 1.4144144144144144, "no_speech_prob": 3.966958502132911e-06}, {"id": 1143, "seek": 587032, "start": 5870.32, "end": 5878.32, "text": " We took each word and we turned them into an embedding.", "tokens": [492, 1890, 1184, 1349, 293, 321, 3574, 552, 666, 364, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.2383164690251936, "compression_ratio": 1.5970149253731343, "no_speech_prob": 4.495165285334224e-06}, {"id": 1144, "seek": 587032, "start": 5878.32, "end": 5886.96, "text": " And then we summed all of those embeddings up to get an embedding for that sentence.", "tokens": [400, 550, 321, 2408, 1912, 439, 295, 729, 12240, 29432, 493, 281, 483, 364, 12240, 3584, 337, 300, 8174, 13], "temperature": 0.0, "avg_logprob": -0.2383164690251936, "compression_ratio": 1.5970149253731343, "no_speech_prob": 4.495165285334224e-06}, {"id": 1145, "seek": 587032, "start": 5886.96, "end": 5897.36, "text": " So each sentence was turned into an embedding and they were of length 20.", "tokens": [407, 1184, 8174, 390, 3574, 666, 364, 12240, 3584, 293, 436, 645, 295, 4641, 945, 13], "temperature": 0.0, "avg_logprob": -0.2383164690251936, "compression_ratio": 1.5970149253731343, "no_speech_prob": 4.495165285334224e-06}, {"id": 1146, "seek": 589736, "start": 5897.36, "end": 5910.16, "text": " Then we took the query.", "tokens": [1396, 321, 1890, 264, 14581, 13], "temperature": 0.0, "avg_logprob": -0.45102514871736854, "compression_ratio": 1.2403846153846154, "no_speech_prob": 4.63785136162187e-06}, {"id": 1147, "seek": 589736, "start": 5910.16, "end": 5913.16, "text": " This is my query.", "tokens": [639, 307, 452, 14581, 13], "temperature": 0.0, "avg_logprob": -0.45102514871736854, "compression_ratio": 1.2403846153846154, "no_speech_prob": 4.63785136162187e-06}, {"id": 1148, "seek": 589736, "start": 5913.16, "end": 5926.24, "text": " Same kind of idea, bunch of words, which we got embeddings for, and we added them up to", "tokens": [10635, 733, 295, 1558, 11, 3840, 295, 2283, 11, 597, 321, 658, 12240, 29432, 337, 11, 293, 321, 3869, 552, 493, 281], "temperature": 0.0, "avg_logprob": -0.45102514871736854, "compression_ratio": 1.2403846153846154, "no_speech_prob": 4.63785136162187e-06}, {"id": 1149, "seek": 592624, "start": 5926.24, "end": 5930.04, "text": " get an embedding for our question.", "tokens": [483, 364, 12240, 3584, 337, 527, 1168, 13], "temperature": 0.0, "avg_logprob": -0.19909895790947807, "compression_ratio": 1.603305785123967, "no_speech_prob": 2.1233690858935006e-06}, {"id": 1150, "seek": 592624, "start": 5930.04, "end": 5941.76, "text": " So to do a memory network, what we're going to do is we're going to take each of these", "tokens": [407, 281, 360, 257, 4675, 3209, 11, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 747, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.19909895790947807, "compression_ratio": 1.603305785123967, "no_speech_prob": 2.1233690858935006e-06}, {"id": 1151, "seek": 592624, "start": 5941.76, "end": 5953.08, "text": " embeddings and we're going to combine each one with a question or query.", "tokens": [12240, 29432, 293, 321, 434, 516, 281, 10432, 1184, 472, 365, 257, 1168, 420, 14581, 13], "temperature": 0.0, "avg_logprob": -0.19909895790947807, "compression_ratio": 1.603305785123967, "no_speech_prob": 2.1233690858935006e-06}, {"id": 1152, "seek": 595308, "start": 5953.08, "end": 5980.96, "text": " And we're just going to take a dot product.", "tokens": [400, 321, 434, 445, 516, 281, 747, 257, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.19000447591145833, "compression_ratio": 0.8431372549019608, "no_speech_prob": 4.6111290430417284e-05}, {"id": 1153, "seek": 598096, "start": 5980.96, "end": 5986.04, "text": " So we're going to end up with 4 dot products from each sentence of the story times the", "tokens": [407, 321, 434, 516, 281, 917, 493, 365, 1017, 5893, 3383, 490, 1184, 8174, 295, 264, 1657, 1413, 264], "temperature": 0.0, "avg_logprob": -0.22950544897115455, "compression_ratio": 1.8669724770642202, "no_speech_prob": 6.747973657184048e-06}, {"id": 1154, "seek": 598096, "start": 5986.04, "end": 5987.52, "text": " query.", "tokens": [14581, 13], "temperature": 0.0, "avg_logprob": -0.22950544897115455, "compression_ratio": 1.8669724770642202, "no_speech_prob": 6.747973657184048e-06}, {"id": 1155, "seek": 598096, "start": 5987.52, "end": 5990.04, "text": " So what does the dot product do?", "tokens": [407, 437, 775, 264, 5893, 1674, 360, 30], "temperature": 0.0, "avg_logprob": -0.22950544897115455, "compression_ratio": 1.8669724770642202, "no_speech_prob": 6.747973657184048e-06}, {"id": 1156, "seek": 598096, "start": 5990.04, "end": 5992.64, "text": " It basically says how similar 2 things are.", "tokens": [467, 1936, 1619, 577, 2531, 568, 721, 366, 13], "temperature": 0.0, "avg_logprob": -0.22950544897115455, "compression_ratio": 1.8669724770642202, "no_speech_prob": 6.747973657184048e-06}, {"id": 1157, "seek": 598096, "start": 5992.64, "end": 5995.96, "text": " When one thing is big, the other thing is big, when one thing is small, the other thing", "tokens": [1133, 472, 551, 307, 955, 11, 264, 661, 551, 307, 955, 11, 562, 472, 551, 307, 1359, 11, 264, 661, 551], "temperature": 0.0, "avg_logprob": -0.22950544897115455, "compression_ratio": 1.8669724770642202, "no_speech_prob": 6.747973657184048e-06}, {"id": 1158, "seek": 598096, "start": 5995.96, "end": 5996.96, "text": " is small.", "tokens": [307, 1359, 13], "temperature": 0.0, "avg_logprob": -0.22950544897115455, "compression_ratio": 1.8669724770642202, "no_speech_prob": 6.747973657184048e-06}, {"id": 1159, "seek": 598096, "start": 5996.96, "end": 6000.64, "text": " Those things both make the dot product bigger.", "tokens": [3950, 721, 1293, 652, 264, 5893, 1674, 3801, 13], "temperature": 0.0, "avg_logprob": -0.22950544897115455, "compression_ratio": 1.8669724770642202, "no_speech_prob": 6.747973657184048e-06}, {"id": 1160, "seek": 598096, "start": 6000.64, "end": 6006.8, "text": " So these are basically going to be 4 vectors describing how similar each of our 4 sentences", "tokens": [407, 613, 366, 1936, 516, 281, 312, 1017, 18875, 16141, 577, 2531, 1184, 295, 527, 1017, 16579], "temperature": 0.0, "avg_logprob": -0.22950544897115455, "compression_ratio": 1.8669724770642202, "no_speech_prob": 6.747973657184048e-06}, {"id": 1161, "seek": 600680, "start": 6006.8, "end": 6011.16, "text": " to the query.", "tokens": [281, 264, 14581, 13], "temperature": 0.0, "avg_logprob": -0.2961537896133051, "compression_ratio": 1.2079207920792079, "no_speech_prob": 2.9944237667223206e-06}, {"id": 1162, "seek": 600680, "start": 6011.16, "end": 6013.28, "text": " So that's step 1.", "tokens": [407, 300, 311, 1823, 502, 13], "temperature": 0.0, "avg_logprob": -0.2961537896133051, "compression_ratio": 1.2079207920792079, "no_speech_prob": 2.9944237667223206e-06}, {"id": 1163, "seek": 600680, "start": 6013.28, "end": 6029.360000000001, "text": " Step 2 is to stick them through a softmax.", "tokens": [5470, 568, 307, 281, 2897, 552, 807, 257, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.2961537896133051, "compression_ratio": 1.2079207920792079, "no_speech_prob": 2.9944237667223206e-06}, {"id": 1164, "seek": 600680, "start": 6029.360000000001, "end": 6032.360000000001, "text": " So remember dot product is written as a scalar.", "tokens": [407, 1604, 5893, 1674, 307, 3720, 382, 257, 39684, 13], "temperature": 0.0, "avg_logprob": -0.2961537896133051, "compression_ratio": 1.2079207920792079, "no_speech_prob": 2.9944237667223206e-06}, {"id": 1165, "seek": 603236, "start": 6032.36, "end": 6040.799999999999, "text": " So we now have 4 scalars, and they add up to 1.", "tokens": [407, 321, 586, 362, 1017, 15664, 685, 11, 293, 436, 909, 493, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.13510055541992189, "compression_ratio": 1.486842105263158, "no_speech_prob": 1.5779569366713986e-06}, {"id": 1166, "seek": 603236, "start": 6040.799999999999, "end": 6050.679999999999, "text": " And they each are basically related to how similar is the query to each of the 4 sentences.", "tokens": [400, 436, 1184, 366, 1936, 4077, 281, 577, 2531, 307, 264, 14581, 281, 1184, 295, 264, 1017, 16579, 13], "temperature": 0.0, "avg_logprob": -0.13510055541992189, "compression_ratio": 1.486842105263158, "no_speech_prob": 1.5779569366713986e-06}, {"id": 1167, "seek": 603236, "start": 6050.679999999999, "end": 6058.04, "text": " We're now going to create a totally separate embedding of each of the sentences in our", "tokens": [492, 434, 586, 516, 281, 1884, 257, 3879, 4994, 12240, 3584, 295, 1184, 295, 264, 16579, 294, 527], "temperature": 0.0, "avg_logprob": -0.13510055541992189, "compression_ratio": 1.486842105263158, "no_speech_prob": 1.5779569366713986e-06}, {"id": 1168, "seek": 605804, "start": 6058.04, "end": 6063.84, "text": " story by creating a totally separate embedding for each word.", "tokens": [1657, 538, 4084, 257, 3879, 4994, 12240, 3584, 337, 1184, 1349, 13], "temperature": 0.0, "avg_logprob": -0.14951358121984146, "compression_ratio": 1.5662650602409638, "no_speech_prob": 2.726454340518103e-06}, {"id": 1169, "seek": 605804, "start": 6063.84, "end": 6069.44, "text": " So we're basically just going to create a new random embedding matrix for each word", "tokens": [407, 321, 434, 1936, 445, 516, 281, 1884, 257, 777, 4974, 12240, 3584, 8141, 337, 1184, 1349], "temperature": 0.0, "avg_logprob": -0.14951358121984146, "compression_ratio": 1.5662650602409638, "no_speech_prob": 2.726454340518103e-06}, {"id": 1170, "seek": 605804, "start": 6069.44, "end": 6079.6, "text": " to start with, sum them all together, and that's going to give us a new embedding.", "tokens": [281, 722, 365, 11, 2408, 552, 439, 1214, 11, 293, 300, 311, 516, 281, 976, 505, 257, 777, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.14951358121984146, "compression_ratio": 1.5662650602409638, "no_speech_prob": 2.726454340518103e-06}, {"id": 1171, "seek": 605804, "start": 6079.6, "end": 6083.92, "text": " This one they call C I believe.", "tokens": [639, 472, 436, 818, 383, 286, 1697, 13], "temperature": 0.0, "avg_logprob": -0.14951358121984146, "compression_ratio": 1.5662650602409638, "no_speech_prob": 2.726454340518103e-06}, {"id": 1172, "seek": 608392, "start": 6083.92, "end": 6101.72, "text": " And all we're going to do is we're going to multiply each one of these embeddings by the", "tokens": [400, 439, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 12972, 1184, 472, 295, 613, 12240, 29432, 538, 264], "temperature": 0.0, "avg_logprob": -0.21545030957176572, "compression_ratio": 1.382608695652174, "no_speech_prob": 3.611971351347165e-06}, {"id": 1173, "seek": 608392, "start": 6101.72, "end": 6107.04, "text": " equivalent softmax as a weighting and then just add them all together.", "tokens": [10344, 2787, 41167, 382, 257, 3364, 278, 293, 550, 445, 909, 552, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.21545030957176572, "compression_ratio": 1.382608695652174, "no_speech_prob": 3.611971351347165e-06}, {"id": 1174, "seek": 610704, "start": 6107.04, "end": 6123.08, "text": " So we're just going to have S1234, going to be C1 times S1 plus C2 times S2 and then divided", "tokens": [407, 321, 434, 445, 516, 281, 362, 318, 4762, 12249, 11, 516, 281, 312, 383, 16, 1413, 318, 16, 1804, 383, 17, 1413, 318, 17, 293, 550, 6666], "temperature": 0.0, "avg_logprob": -0.3078901811079545, "compression_ratio": 1.3333333333333333, "no_speech_prob": 7.4112308539042715e-06}, {"id": 1175, "seek": 610704, "start": 6123.08, "end": 6126.08, "text": " by S1 plus.", "tokens": [538, 318, 16, 1804, 13], "temperature": 0.0, "avg_logprob": -0.3078901811079545, "compression_ratio": 1.3333333333333333, "no_speech_prob": 7.4112308539042715e-06}, {"id": 1176, "seek": 610704, "start": 6126.08, "end": 6131.64, "text": " Actually you don't need to divide by because they add to 1.", "tokens": [5135, 291, 500, 380, 643, 281, 9845, 538, 570, 436, 909, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.3078901811079545, "compression_ratio": 1.3333333333333333, "no_speech_prob": 7.4112308539042715e-06}, {"id": 1177, "seek": 613164, "start": 6131.64, "end": 6141.400000000001, "text": " So that's going to be our final result, which is going to be of length 20.", "tokens": [407, 300, 311, 516, 281, 312, 527, 2572, 1874, 11, 597, 307, 516, 281, 312, 295, 4641, 945, 13], "temperature": 0.0, "avg_logprob": -0.12623229171290543, "compression_ratio": 1.5902777777777777, "no_speech_prob": 2.948007022496313e-06}, {"id": 1178, "seek": 613164, "start": 6141.400000000001, "end": 6145.200000000001, "text": " So this thing is a vector of length 20.", "tokens": [407, 341, 551, 307, 257, 8062, 295, 4641, 945, 13], "temperature": 0.0, "avg_logprob": -0.12623229171290543, "compression_ratio": 1.5902777777777777, "no_speech_prob": 2.948007022496313e-06}, {"id": 1179, "seek": 613164, "start": 6145.200000000001, "end": 6151.0, "text": " And then we're going to take that and put it through a single dense layer, and we're", "tokens": [400, 550, 321, 434, 516, 281, 747, 300, 293, 829, 309, 807, 257, 2167, 18011, 4583, 11, 293, 321, 434], "temperature": 0.0, "avg_logprob": -0.12623229171290543, "compression_ratio": 1.5902777777777777, "no_speech_prob": 2.948007022496313e-06}, {"id": 1180, "seek": 613164, "start": 6151.0, "end": 6159.6, "text": " going to get back the answer.", "tokens": [516, 281, 483, 646, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.12623229171290543, "compression_ratio": 1.5902777777777777, "no_speech_prob": 2.948007022496313e-06}, {"id": 1181, "seek": 615960, "start": 6159.6, "end": 6163.200000000001, "text": " And that whole thing is the memory network.", "tokens": [400, 300, 1379, 551, 307, 264, 4675, 3209, 13], "temperature": 0.0, "avg_logprob": -0.21564434670113228, "compression_ratio": 1.4857142857142858, "no_speech_prob": 2.2473725039162673e-05}, {"id": 1182, "seek": 615960, "start": 6163.200000000001, "end": 6166.8, "text": " It's incredibly simple.", "tokens": [467, 311, 6252, 2199, 13], "temperature": 0.0, "avg_logprob": -0.21564434670113228, "compression_ratio": 1.4857142857142858, "no_speech_prob": 2.2473725039162673e-05}, {"id": 1183, "seek": 615960, "start": 6166.8, "end": 6173.84, "text": " There's nothing deep in terms of deep learning, there's almost no non-linearities.", "tokens": [821, 311, 1825, 2452, 294, 2115, 295, 2452, 2539, 11, 456, 311, 1920, 572, 2107, 12, 28263, 1088, 13], "temperature": 0.0, "avg_logprob": -0.21564434670113228, "compression_ratio": 1.4857142857142858, "no_speech_prob": 2.2473725039162673e-05}, {"id": 1184, "seek": 615960, "start": 6173.84, "end": 6181.76, "text": " It doesn't seem like it's likely to be able to do very much, but I guess we haven't given", "tokens": [467, 1177, 380, 1643, 411, 309, 311, 3700, 281, 312, 1075, 281, 360, 588, 709, 11, 457, 286, 2041, 321, 2378, 380, 2212], "temperature": 0.0, "avg_logprob": -0.21564434670113228, "compression_ratio": 1.4857142857142858, "no_speech_prob": 2.2473725039162673e-05}, {"id": 1185, "seek": 615960, "start": 6181.76, "end": 6183.88, "text": " it very much to do.", "tokens": [309, 588, 709, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.21564434670113228, "compression_ratio": 1.4857142857142858, "no_speech_prob": 2.2473725039162673e-05}, {"id": 1186, "seek": 618388, "start": 6183.88, "end": 6192.4800000000005, "text": " So let's take a look at the code version.", "tokens": [407, 718, 311, 747, 257, 574, 412, 264, 3089, 3037, 13], "temperature": 0.0, "avg_logprob": -0.2337754980310217, "compression_ratio": 1.610062893081761, "no_speech_prob": 2.1444689991767518e-05}, {"id": 1187, "seek": 618388, "start": 6192.4800000000005, "end": 6195.96, "text": " So in that last step you said the answer, was that really like the embedding of the", "tokens": [407, 294, 300, 1036, 1823, 291, 848, 264, 1867, 11, 390, 300, 534, 411, 264, 12240, 3584, 295, 264], "temperature": 0.0, "avg_logprob": -0.2337754980310217, "compression_ratio": 1.610062893081761, "no_speech_prob": 2.1444689991767518e-05}, {"id": 1188, "seek": 618388, "start": 6195.96, "end": 6199.36, "text": " answer and then it has to get the reverse lookup?", "tokens": [1867, 293, 550, 309, 575, 281, 483, 264, 9943, 574, 1010, 30], "temperature": 0.0, "avg_logprob": -0.2337754980310217, "compression_ratio": 1.610062893081761, "no_speech_prob": 2.1444689991767518e-05}, {"id": 1189, "seek": 618388, "start": 6199.36, "end": 6207.28, "text": " It's the softmax of the answer and then you have to do an argmax.", "tokens": [467, 311, 264, 2787, 41167, 295, 264, 1867, 293, 550, 291, 362, 281, 360, 364, 3882, 41167, 13], "temperature": 0.0, "avg_logprob": -0.2337754980310217, "compression_ratio": 1.610062893081761, "no_speech_prob": 2.1444689991767518e-05}, {"id": 1190, "seek": 618388, "start": 6207.28, "end": 6208.52, "text": " So here it is.", "tokens": [407, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.2337754980310217, "compression_ratio": 1.610062893081761, "no_speech_prob": 2.1444689991767518e-05}, {"id": 1191, "seek": 620852, "start": 6208.52, "end": 6217.4400000000005, "text": " We've got the story times the query, the embedding of the story times the embedding of the query,", "tokens": [492, 600, 658, 264, 1657, 1413, 264, 14581, 11, 264, 12240, 3584, 295, 264, 1657, 1413, 264, 12240, 3584, 295, 264, 14581, 11], "temperature": 0.0, "avg_logprob": -0.22591992219289145, "compression_ratio": 1.8108108108108107, "no_speech_prob": 3.041583113372326e-06}, {"id": 1192, "seek": 620852, "start": 6217.4400000000005, "end": 6221.040000000001, "text": " the dot product.", "tokens": [264, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.22591992219289145, "compression_ratio": 1.8108108108108107, "no_speech_prob": 3.041583113372326e-06}, {"id": 1193, "seek": 620852, "start": 6221.040000000001, "end": 6222.040000000001, "text": " We do a softmax.", "tokens": [492, 360, 257, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.22591992219289145, "compression_ratio": 1.8108108108108107, "no_speech_prob": 3.041583113372326e-06}, {"id": 1194, "seek": 620852, "start": 6222.040000000001, "end": 6227.040000000001, "text": " Softmax works in the last dimension, so I just have to reshape to get rid of the unit", "tokens": [16985, 41167, 1985, 294, 264, 1036, 10139, 11, 370, 286, 445, 362, 281, 725, 42406, 281, 483, 3973, 295, 264, 4985], "temperature": 0.0, "avg_logprob": -0.22591992219289145, "compression_ratio": 1.8108108108108107, "no_speech_prob": 3.041583113372326e-06}, {"id": 1195, "seek": 620852, "start": 6227.040000000001, "end": 6230.68, "text": " axis and then I reshape again to put the unit axis back on again.", "tokens": [10298, 293, 550, 286, 725, 42406, 797, 281, 829, 264, 4985, 10298, 646, 322, 797, 13], "temperature": 0.0, "avg_logprob": -0.22591992219289145, "compression_ratio": 1.8108108108108107, "no_speech_prob": 3.041583113372326e-06}, {"id": 1196, "seek": 620852, "start": 6230.68, "end": 6233.200000000001, "text": " But the reshapes aren't doing anything interesting.", "tokens": [583, 264, 725, 71, 569, 279, 3212, 380, 884, 1340, 1880, 13], "temperature": 0.0, "avg_logprob": -0.22591992219289145, "compression_ratio": 1.8108108108108107, "no_speech_prob": 3.041583113372326e-06}, {"id": 1197, "seek": 623320, "start": 6233.2, "end": 6241.32, "text": " So it's just a dot product followed by softmax, and that gives us the weights.", "tokens": [407, 309, 311, 445, 257, 5893, 1674, 6263, 538, 2787, 41167, 11, 293, 300, 2709, 505, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.22498730977376302, "compression_ratio": 1.5139664804469273, "no_speech_prob": 7.88921897765249e-06}, {"id": 1198, "seek": 623320, "start": 6241.32, "end": 6248.48, "text": " So now we're going to take each weight and multiply it by the second set of embeddings,", "tokens": [407, 586, 321, 434, 516, 281, 747, 1184, 3364, 293, 12972, 309, 538, 264, 1150, 992, 295, 12240, 29432, 11], "temperature": 0.0, "avg_logprob": -0.22498730977376302, "compression_ratio": 1.5139664804469273, "no_speech_prob": 7.88921897765249e-06}, {"id": 1199, "seek": 623320, "start": 6248.48, "end": 6252.48, "text": " embedding C.", "tokens": [12240, 3584, 383, 13], "temperature": 0.0, "avg_logprob": -0.22498730977376302, "compression_ratio": 1.5139664804469273, "no_speech_prob": 7.88921897765249e-06}, {"id": 1200, "seek": 623320, "start": 6252.48, "end": 6259.48, "text": " In order to do this, I just used the dot product again, but because of the fact that you've", "tokens": [682, 1668, 281, 360, 341, 11, 286, 445, 1143, 264, 5893, 1674, 797, 11, 457, 570, 295, 264, 1186, 300, 291, 600], "temperature": 0.0, "avg_logprob": -0.22498730977376302, "compression_ratio": 1.5139664804469273, "no_speech_prob": 7.88921897765249e-06}, {"id": 1201, "seek": 625948, "start": 6259.48, "end": 6267.639999999999, "text": " got a unit axis there, this is actually just doing a very simple weighted average.", "tokens": [658, 257, 4985, 10298, 456, 11, 341, 307, 767, 445, 884, 257, 588, 2199, 32807, 4274, 13], "temperature": 0.0, "avg_logprob": -0.12959781769783266, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.3287749425217044e-06}, {"id": 1202, "seek": 625948, "start": 6267.639999999999, "end": 6271.759999999999, "text": " And again, reshape to get rid of the unit axis so that we can stick it through a dense", "tokens": [400, 797, 11, 725, 42406, 281, 483, 3973, 295, 264, 4985, 10298, 370, 300, 321, 393, 2897, 309, 807, 257, 18011], "temperature": 0.0, "avg_logprob": -0.12959781769783266, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.3287749425217044e-06}, {"id": 1203, "seek": 625948, "start": 6271.759999999999, "end": 6277.12, "text": " layer with a softmax, and that gives us our final result.", "tokens": [4583, 365, 257, 2787, 41167, 11, 293, 300, 2709, 505, 527, 2572, 1874, 13], "temperature": 0.0, "avg_logprob": -0.12959781769783266, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.3287749425217044e-06}, {"id": 1204, "seek": 625948, "start": 6277.12, "end": 6283.679999999999, "text": " So what this is effectively doing is it's basically saying, okay, how similar is the", "tokens": [407, 437, 341, 307, 8659, 884, 307, 309, 311, 1936, 1566, 11, 1392, 11, 577, 2531, 307, 264], "temperature": 0.0, "avg_logprob": -0.12959781769783266, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.3287749425217044e-06}, {"id": 1205, "seek": 625948, "start": 6283.679999999999, "end": 6288.16, "text": " query to each one of the sentences in the story?", "tokens": [14581, 281, 1184, 472, 295, 264, 16579, 294, 264, 1657, 30], "temperature": 0.0, "avg_logprob": -0.12959781769783266, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.3287749425217044e-06}, {"id": 1206, "seek": 628816, "start": 6288.16, "end": 6295.2, "text": " Use that to create a bunch of weights, and then these things here are basically the answers.", "tokens": [8278, 300, 281, 1884, 257, 3840, 295, 17443, 11, 293, 550, 613, 721, 510, 366, 1936, 264, 6338, 13], "temperature": 0.0, "avg_logprob": -0.2075407651005959, "compression_ratio": 1.6331877729257642, "no_speech_prob": 6.748021405655891e-06}, {"id": 1207, "seek": 628816, "start": 6295.2, "end": 6299.92, "text": " If story number 1 was where the answer was, then we're going to use this one, story number", "tokens": [759, 1657, 1230, 502, 390, 689, 264, 1867, 390, 11, 550, 321, 434, 516, 281, 764, 341, 472, 11, 1657, 1230], "temperature": 0.0, "avg_logprob": -0.2075407651005959, "compression_ratio": 1.6331877729257642, "no_speech_prob": 6.748021405655891e-06}, {"id": 1208, "seek": 628816, "start": 6299.92, "end": 6301.639999999999, "text": " 2, 3 and 4.", "tokens": [568, 11, 805, 293, 1017, 13], "temperature": 0.0, "avg_logprob": -0.2075407651005959, "compression_ratio": 1.6331877729257642, "no_speech_prob": 6.748021405655891e-06}, {"id": 1209, "seek": 628816, "start": 6301.639999999999, "end": 6307.08, "text": " Because there's a single linear layer at the very end, so it doesn't really get to do much", "tokens": [1436, 456, 311, 257, 2167, 8213, 4583, 412, 264, 588, 917, 11, 370, 309, 1177, 380, 534, 483, 281, 360, 709], "temperature": 0.0, "avg_logprob": -0.2075407651005959, "compression_ratio": 1.6331877729257642, "no_speech_prob": 6.748021405655891e-06}, {"id": 1210, "seek": 628816, "start": 6307.08, "end": 6308.08, "text": " computation.", "tokens": [24903, 13], "temperature": 0.0, "avg_logprob": -0.2075407651005959, "compression_ratio": 1.6331877729257642, "no_speech_prob": 6.748021405655891e-06}, {"id": 1211, "seek": 628816, "start": 6308.08, "end": 6313.96, "text": " So it basically has to learn what the answer represented by each story is.", "tokens": [407, 309, 1936, 575, 281, 1466, 437, 264, 1867, 10379, 538, 1184, 1657, 307, 13], "temperature": 0.0, "avg_logprob": -0.2075407651005959, "compression_ratio": 1.6331877729257642, "no_speech_prob": 6.748021405655891e-06}, {"id": 1212, "seek": 631396, "start": 6313.96, "end": 6331.84, "text": " And again, this is lucky because from the original dataset, the answer to every question", "tokens": [400, 797, 11, 341, 307, 6356, 570, 490, 264, 3380, 28872, 11, 264, 1867, 281, 633, 1168], "temperature": 0.0, "avg_logprob": -0.19245020151138306, "compression_ratio": 1.2393162393162394, "no_speech_prob": 7.071874279063195e-06}, {"id": 1213, "seek": 631396, "start": 6331.84, "end": 6337.44, "text": " is the last word of the sentence.", "tokens": [307, 264, 1036, 1349, 295, 264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.19245020151138306, "compression_ratio": 1.2393162393162394, "no_speech_prob": 7.071874279063195e-06}, {"id": 1214, "seek": 631396, "start": 6337.44, "end": 6340.76, "text": " Where is Frodo's ring?", "tokens": [2305, 307, 25028, 2595, 311, 4875, 30], "temperature": 0.0, "avg_logprob": -0.19245020151138306, "compression_ratio": 1.2393162393162394, "no_speech_prob": 7.071874279063195e-06}, {"id": 1215, "seek": 634076, "start": 6340.76, "end": 6347.400000000001, "text": " So that's why we have this incredibly simple final piece.", "tokens": [407, 300, 311, 983, 321, 362, 341, 6252, 2199, 2572, 2522, 13], "temperature": 0.0, "avg_logprob": -0.20320987701416016, "compression_ratio": 1.5153061224489797, "no_speech_prob": 6.854282219137531e-06}, {"id": 1216, "seek": 634076, "start": 6347.400000000001, "end": 6351.8, "text": " So this is an interesting use of Keras.", "tokens": [407, 341, 307, 364, 1880, 764, 295, 591, 6985, 13], "temperature": 0.0, "avg_logprob": -0.20320987701416016, "compression_ratio": 1.5153061224489797, "no_speech_prob": 6.854282219137531e-06}, {"id": 1217, "seek": 634076, "start": 6351.8, "end": 6358.2, "text": " We've created a model which is in no possible way deep learning.", "tokens": [492, 600, 2942, 257, 2316, 597, 307, 294, 572, 1944, 636, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.20320987701416016, "compression_ratio": 1.5153061224489797, "no_speech_prob": 6.854282219137531e-06}, {"id": 1218, "seek": 634076, "start": 6358.2, "end": 6364.4800000000005, "text": " But it's bunches of tensors and layers that are stuck together, so it has some inputs", "tokens": [583, 309, 311, 3840, 279, 295, 10688, 830, 293, 7914, 300, 366, 5541, 1214, 11, 370, 309, 575, 512, 15743], "temperature": 0.0, "avg_logprob": -0.20320987701416016, "compression_ratio": 1.5153061224489797, "no_speech_prob": 6.854282219137531e-06}, {"id": 1219, "seek": 634076, "start": 6364.4800000000005, "end": 6367.360000000001, "text": " and it has an output, so we can call it a model.", "tokens": [293, 309, 575, 364, 5598, 11, 370, 321, 393, 818, 309, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.20320987701416016, "compression_ratio": 1.5153061224489797, "no_speech_prob": 6.854282219137531e-06}, {"id": 1220, "seek": 636736, "start": 6367.36, "end": 6373.62, "text": " We can compile it with an optimizer and a loss, and then we can fit it.", "tokens": [492, 393, 31413, 309, 365, 364, 5028, 6545, 293, 257, 4470, 11, 293, 550, 321, 393, 3318, 309, 13], "temperature": 0.0, "avg_logprob": -0.16188683760793585, "compression_ratio": 1.647887323943662, "no_speech_prob": 3.0415776564041153e-06}, {"id": 1221, "seek": 636736, "start": 6373.62, "end": 6379.799999999999, "text": " So it's kind of interesting how you can use Keras for things which don't really use any", "tokens": [407, 309, 311, 733, 295, 1880, 577, 291, 393, 764, 591, 6985, 337, 721, 597, 500, 380, 534, 764, 604], "temperature": 0.0, "avg_logprob": -0.16188683760793585, "compression_ratio": 1.647887323943662, "no_speech_prob": 3.0415776564041153e-06}, {"id": 1222, "seek": 636736, "start": 6379.799999999999, "end": 6383.24, "text": " of the normal layers in any normal way.", "tokens": [295, 264, 2710, 7914, 294, 604, 2710, 636, 13], "temperature": 0.0, "avg_logprob": -0.16188683760793585, "compression_ratio": 1.647887323943662, "no_speech_prob": 3.0415776564041153e-06}, {"id": 1223, "seek": 636736, "start": 6383.24, "end": 6386.4, "text": " And as you can see, it works for what it's worth.", "tokens": [400, 382, 291, 393, 536, 11, 309, 1985, 337, 437, 309, 311, 3163, 13], "temperature": 0.0, "avg_logprob": -0.16188683760793585, "compression_ratio": 1.647887323943662, "no_speech_prob": 3.0415776564041153e-06}, {"id": 1224, "seek": 636736, "start": 6386.4, "end": 6387.839999999999, "text": " We solved this problem.", "tokens": [492, 13041, 341, 1154, 13], "temperature": 0.0, "avg_logprob": -0.16188683760793585, "compression_ratio": 1.647887323943662, "no_speech_prob": 3.0415776564041153e-06}, {"id": 1225, "seek": 636736, "start": 6387.839999999999, "end": 6393.639999999999, "text": " And the particular problem we solved here is the one supporting that problem.", "tokens": [400, 264, 1729, 1154, 321, 13041, 510, 307, 264, 472, 7231, 300, 1154, 13], "temperature": 0.0, "avg_logprob": -0.16188683760793585, "compression_ratio": 1.647887323943662, "no_speech_prob": 3.0415776564041153e-06}, {"id": 1226, "seek": 639364, "start": 6393.64, "end": 6398.56, "text": " In fact, it worked in less than one epoch.", "tokens": [682, 1186, 11, 309, 2732, 294, 1570, 813, 472, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.17610292936626235, "compression_ratio": 1.574468085106383, "no_speech_prob": 5.255362339084968e-06}, {"id": 1227, "seek": 639364, "start": 6398.56, "end": 6401.400000000001, "text": " More interesting is two supporting parts.", "tokens": [5048, 1880, 307, 732, 7231, 3166, 13], "temperature": 0.0, "avg_logprob": -0.17610292936626235, "compression_ratio": 1.574468085106383, "no_speech_prob": 5.255362339084968e-06}, {"id": 1228, "seek": 639364, "start": 6401.400000000001, "end": 6405.240000000001, "text": " Actually before I do that, I'll just point out something interesting.", "tokens": [5135, 949, 286, 360, 300, 11, 286, 603, 445, 935, 484, 746, 1880, 13], "temperature": 0.0, "avg_logprob": -0.17610292936626235, "compression_ratio": 1.574468085106383, "no_speech_prob": 5.255362339084968e-06}, {"id": 1229, "seek": 639364, "start": 6405.240000000001, "end": 6409.9800000000005, "text": " We could create another model now that this is already trained, which is to return not", "tokens": [492, 727, 1884, 1071, 2316, 586, 300, 341, 307, 1217, 8895, 11, 597, 307, 281, 2736, 406], "temperature": 0.0, "avg_logprob": -0.17610292936626235, "compression_ratio": 1.574468085106383, "no_speech_prob": 5.255362339084968e-06}, {"id": 1230, "seek": 639364, "start": 6409.9800000000005, "end": 6414.92, "text": " the final answer, but the value of the weights.", "tokens": [264, 2572, 1867, 11, 457, 264, 2158, 295, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.17610292936626235, "compression_ratio": 1.574468085106383, "no_speech_prob": 5.255362339084968e-06}, {"id": 1231, "seek": 639364, "start": 6414.92, "end": 6422.72, "text": " And so we can now go back and say, for a particular story, what are the weights?", "tokens": [400, 370, 321, 393, 586, 352, 646, 293, 584, 11, 337, 257, 1729, 1657, 11, 437, 366, 264, 17443, 30], "temperature": 0.0, "avg_logprob": -0.17610292936626235, "compression_ratio": 1.574468085106383, "no_speech_prob": 5.255362339084968e-06}, {"id": 1232, "seek": 642272, "start": 6422.72, "end": 6426.64, "text": " So let's do F.predict rather than answer.predict.", "tokens": [407, 718, 311, 360, 479, 13, 79, 24945, 2831, 813, 1867, 13, 79, 24945, 13], "temperature": 0.0, "avg_logprob": -0.2857452041801365, "compression_ratio": 1.583815028901734, "no_speech_prob": 6.240891707420815e-06}, {"id": 1233, "seek": 642272, "start": 6426.64, "end": 6428.76, "text": " So for this story, where is Sandra?", "tokens": [407, 337, 341, 1657, 11, 689, 307, 28184, 30], "temperature": 0.0, "avg_logprob": -0.2857452041801365, "compression_ratio": 1.583815028901734, "no_speech_prob": 6.240891707420815e-06}, {"id": 1234, "seek": 642272, "start": 6428.76, "end": 6430.76, "text": " Daniel, Mary, Sandra.", "tokens": [8033, 11, 6059, 11, 28184, 13], "temperature": 0.0, "avg_logprob": -0.2857452041801365, "compression_ratio": 1.583815028901734, "no_speech_prob": 6.240891707420815e-06}, {"id": 1235, "seek": 642272, "start": 6430.76, "end": 6433.84, "text": " Sandra went to the bathroom.", "tokens": [28184, 1437, 281, 264, 8687, 13], "temperature": 0.0, "avg_logprob": -0.2857452041801365, "compression_ratio": 1.583815028901734, "no_speech_prob": 6.240891707420815e-06}, {"id": 1236, "seek": 642272, "start": 6433.84, "end": 6434.84, "text": " Where is Sandra?", "tokens": [2305, 307, 28184, 30], "temperature": 0.0, "avg_logprob": -0.2857452041801365, "compression_ratio": 1.583815028901734, "no_speech_prob": 6.240891707420815e-06}, {"id": 1237, "seek": 642272, "start": 6434.84, "end": 6435.84, "text": " Bathroom.", "tokens": [36167, 2861, 13], "temperature": 0.0, "avg_logprob": -0.2857452041801365, "compression_ratio": 1.583815028901734, "no_speech_prob": 6.240891707420815e-06}, {"id": 1238, "seek": 642272, "start": 6435.84, "end": 6442.360000000001, "text": " So for this particular story, the weights are here.", "tokens": [407, 337, 341, 1729, 1657, 11, 264, 17443, 366, 510, 13], "temperature": 0.0, "avg_logprob": -0.2857452041801365, "compression_ratio": 1.583815028901734, "no_speech_prob": 6.240891707420815e-06}, {"id": 1239, "seek": 642272, "start": 6442.360000000001, "end": 6448.42, "text": " You can see that the weight for sentence number 2 is 0.98.", "tokens": [509, 393, 536, 300, 264, 3364, 337, 8174, 1230, 568, 307, 1958, 13, 22516, 13], "temperature": 0.0, "avg_logprob": -0.2857452041801365, "compression_ratio": 1.583815028901734, "no_speech_prob": 6.240891707420815e-06}, {"id": 1240, "seek": 644842, "start": 6448.42, "end": 6455.8, "text": " So we can actually look inside the model and find out what sentences it's using to answer", "tokens": [407, 321, 393, 767, 574, 1854, 264, 2316, 293, 915, 484, 437, 16579, 309, 311, 1228, 281, 1867], "temperature": 0.0, "avg_logprob": -0.23648385598625934, "compression_ratio": 1.5326633165829147, "no_speech_prob": 4.7108815124374814e-06}, {"id": 1241, "seek": 644842, "start": 6455.8, "end": 6456.8, "text": " this question.", "tokens": [341, 1168, 13], "temperature": 0.0, "avg_logprob": -0.23648385598625934, "compression_ratio": 1.5326633165829147, "no_speech_prob": 4.7108815124374814e-06}, {"id": 1242, "seek": 644842, "start": 6456.8, "end": 6466.72, "text": " Question, Would it not make more sense to concat the embeddings rather than sum them?", "tokens": [14464, 11, 6068, 309, 406, 652, 544, 2020, 281, 1588, 267, 264, 12240, 29432, 2831, 813, 2408, 552, 30], "temperature": 0.0, "avg_logprob": -0.23648385598625934, "compression_ratio": 1.5326633165829147, "no_speech_prob": 4.7108815124374814e-06}, {"id": 1243, "seek": 644842, "start": 6466.72, "end": 6471.36, "text": " Not for this particular problem because of the way the vocabulary is structured when", "tokens": [1726, 337, 341, 1729, 1154, 570, 295, 264, 636, 264, 19864, 307, 18519, 562], "temperature": 0.0, "avg_logprob": -0.23648385598625934, "compression_ratio": 1.5326633165829147, "no_speech_prob": 4.7108815124374814e-06}, {"id": 1244, "seek": 644842, "start": 6471.36, "end": 6474.36, "text": " the sentences are structured.", "tokens": [264, 16579, 366, 18519, 13], "temperature": 0.0, "avg_logprob": -0.23648385598625934, "compression_ratio": 1.5326633165829147, "no_speech_prob": 4.7108815124374814e-06}, {"id": 1245, "seek": 647436, "start": 6474.36, "end": 6478.96, "text": " You would also have to deal with the variable length.", "tokens": [509, 576, 611, 362, 281, 2028, 365, 264, 7006, 4641, 13], "temperature": 0.0, "avg_logprob": -0.2091868422752203, "compression_ratio": 1.6350710900473933, "no_speech_prob": 1.805823740141932e-05}, {"id": 1246, "seek": 647436, "start": 6478.96, "end": 6485.04, "text": " We've used padding to make them the same length.", "tokens": [492, 600, 1143, 39562, 281, 652, 552, 264, 912, 4641, 13], "temperature": 0.0, "avg_logprob": -0.2091868422752203, "compression_ratio": 1.6350710900473933, "no_speech_prob": 1.805823740141932e-05}, {"id": 1247, "seek": 647436, "start": 6485.04, "end": 6490.5199999999995, "text": " If you wanted to use this in real life, you would need to come up with a better sentence", "tokens": [759, 291, 1415, 281, 764, 341, 294, 957, 993, 11, 291, 576, 643, 281, 808, 493, 365, 257, 1101, 8174], "temperature": 0.0, "avg_logprob": -0.2091868422752203, "compression_ratio": 1.6350710900473933, "no_speech_prob": 1.805823740141932e-05}, {"id": 1248, "seek": 647436, "start": 6490.5199999999995, "end": 6496.759999999999, "text": " embedding, which presumably might be an RN or something like that.", "tokens": [12240, 3584, 11, 597, 26742, 1062, 312, 364, 45702, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.2091868422752203, "compression_ratio": 1.6350710900473933, "no_speech_prob": 1.805823740141932e-05}, {"id": 1249, "seek": 647436, "start": 6496.759999999999, "end": 6503.839999999999, "text": " You need to deal with things like not and location of subject and object and so forth.", "tokens": [509, 643, 281, 2028, 365, 721, 411, 406, 293, 4914, 295, 3983, 293, 2657, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.2091868422752203, "compression_ratio": 1.6350710900473933, "no_speech_prob": 1.805823740141932e-05}, {"id": 1250, "seek": 650384, "start": 6503.84, "end": 6508.360000000001, "text": " One thing to point out is that the order of the sentences matters.", "tokens": [1485, 551, 281, 935, 484, 307, 300, 264, 1668, 295, 264, 16579, 7001, 13], "temperature": 0.0, "avg_logprob": -0.19791900410371668, "compression_ratio": 1.7567567567567568, "no_speech_prob": 1.3631180081574712e-05}, {"id": 1251, "seek": 650384, "start": 6508.360000000001, "end": 6514.08, "text": " What I actually did when I preprocessed it was I added a 0 colon, 1 colon, whatever to", "tokens": [708, 286, 767, 630, 562, 286, 2666, 340, 780, 292, 309, 390, 286, 3869, 257, 1958, 8255, 11, 502, 8255, 11, 2035, 281], "temperature": 0.0, "avg_logprob": -0.19791900410371668, "compression_ratio": 1.7567567567567568, "no_speech_prob": 1.3631180081574712e-05}, {"id": 1252, "seek": 650384, "start": 6514.08, "end": 6520.4400000000005, "text": " the start of each sentence so that it would actually be able to learn the time order of", "tokens": [264, 722, 295, 1184, 8174, 370, 300, 309, 576, 767, 312, 1075, 281, 1466, 264, 565, 1668, 295], "temperature": 0.0, "avg_logprob": -0.19791900410371668, "compression_ratio": 1.7567567567567568, "no_speech_prob": 1.3631180081574712e-05}, {"id": 1253, "seek": 650384, "start": 6520.4400000000005, "end": 6521.4400000000005, "text": " sentences.", "tokens": [16579, 13], "temperature": 0.0, "avg_logprob": -0.19791900410371668, "compression_ratio": 1.7567567567567568, "no_speech_prob": 1.3631180081574712e-05}, {"id": 1254, "seek": 650384, "start": 6521.4400000000005, "end": 6525.24, "text": " So this is like another token that I added.", "tokens": [407, 341, 307, 411, 1071, 14862, 300, 286, 3869, 13], "temperature": 0.0, "avg_logprob": -0.19791900410371668, "compression_ratio": 1.7567567567567568, "no_speech_prob": 1.3631180081574712e-05}, {"id": 1255, "seek": 650384, "start": 6525.24, "end": 6529.66, "text": " Because you're wondering what that was, that was something that I added in the preprocessing.", "tokens": [1436, 291, 434, 6359, 437, 300, 390, 11, 300, 390, 746, 300, 286, 3869, 294, 264, 2666, 340, 780, 278, 13], "temperature": 0.0, "avg_logprob": -0.19791900410371668, "compression_ratio": 1.7567567567567568, "no_speech_prob": 1.3631180081574712e-05}, {"id": 1256, "seek": 652966, "start": 6529.66, "end": 6533.92, "text": " So one nice thing with memory networks is we can look and see if they're not working,", "tokens": [407, 472, 1481, 551, 365, 4675, 9590, 307, 321, 393, 574, 293, 536, 498, 436, 434, 406, 1364, 11], "temperature": 0.0, "avg_logprob": -0.2643712705320066, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.5056957636261359e-06}, {"id": 1257, "seek": 652966, "start": 6533.92, "end": 6537.5199999999995, "text": " in particular why they're not working.", "tokens": [294, 1729, 983, 436, 434, 406, 1364, 13], "temperature": 0.0, "avg_logprob": -0.2643712705320066, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.5056957636261359e-06}, {"id": 1258, "seek": 652966, "start": 6537.5199999999995, "end": 6540.72, "text": " So, MultiHop.", "tokens": [407, 11, 29238, 39, 404, 13], "temperature": 0.0, "avg_logprob": -0.2643712705320066, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.5056957636261359e-06}, {"id": 1259, "seek": 652966, "start": 6540.72, "end": 6546.68, "text": " So let's now look at an example of a to-supporting-fats story.", "tokens": [407, 718, 311, 586, 574, 412, 364, 1365, 295, 257, 281, 12, 36622, 477, 278, 12, 69, 1720, 1657, 13], "temperature": 0.0, "avg_logprob": -0.2643712705320066, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.5056957636261359e-06}, {"id": 1260, "seek": 652966, "start": 6546.68, "end": 6548.48, "text": " It's mildly more interesting.", "tokens": [467, 311, 15154, 356, 544, 1880, 13], "temperature": 0.0, "avg_logprob": -0.2643712705320066, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.5056957636261359e-06}, {"id": 1261, "seek": 652966, "start": 6548.48, "end": 6552.5599999999995, "text": " We still only have one type of verb with various synonyms and a small number of subjects and", "tokens": [492, 920, 787, 362, 472, 2010, 295, 9595, 365, 3683, 5451, 2526, 2592, 293, 257, 1359, 1230, 295, 13066, 293], "temperature": 0.0, "avg_logprob": -0.2643712705320066, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.5056957636261359e-06}, {"id": 1262, "seek": 652966, "start": 6552.5599999999995, "end": 6554.28, "text": " a small number of objects.", "tokens": [257, 1359, 1230, 295, 6565, 13], "temperature": 0.0, "avg_logprob": -0.2643712705320066, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.5056957636261359e-06}, {"id": 1263, "seek": 652966, "start": 6554.28, "end": 6557.84, "text": " So it's basically the same.", "tokens": [407, 309, 311, 1936, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.2643712705320066, "compression_ratio": 1.6844444444444444, "no_speech_prob": 1.5056957636261359e-06}, {"id": 1264, "seek": 655784, "start": 6557.84, "end": 6562.400000000001, "text": " But now, to answer your question, we have to go down through two hots.", "tokens": [583, 586, 11, 281, 1867, 428, 1168, 11, 321, 362, 281, 352, 760, 807, 732, 36121, 13], "temperature": 0.0, "avg_logprob": -0.2145206594021521, "compression_ratio": 1.6896551724137931, "no_speech_prob": 6.439001936087152e-06}, {"id": 1265, "seek": 655784, "start": 6562.400000000001, "end": 6565.2, "text": " So where is the milk?", "tokens": [407, 689, 307, 264, 5392, 30], "temperature": 0.0, "avg_logprob": -0.2145206594021521, "compression_ratio": 1.6896551724137931, "no_speech_prob": 6.439001936087152e-06}, {"id": 1266, "seek": 655784, "start": 6565.2, "end": 6566.2, "text": " Let's find the milk.", "tokens": [961, 311, 915, 264, 5392, 13], "temperature": 0.0, "avg_logprob": -0.2145206594021521, "compression_ratio": 1.6896551724137931, "no_speech_prob": 6.439001936087152e-06}, {"id": 1267, "seek": 655784, "start": 6566.2, "end": 6569.56, "text": " Daniel left the milk there.", "tokens": [8033, 1411, 264, 5392, 456, 13], "temperature": 0.0, "avg_logprob": -0.2145206594021521, "compression_ratio": 1.6896551724137931, "no_speech_prob": 6.439001936087152e-06}, {"id": 1268, "seek": 655784, "start": 6569.56, "end": 6570.56, "text": " Where is Daniel?", "tokens": [2305, 307, 8033, 30], "temperature": 0.0, "avg_logprob": -0.2145206594021521, "compression_ratio": 1.6896551724137931, "no_speech_prob": 6.439001936087152e-06}, {"id": 1269, "seek": 655784, "start": 6570.56, "end": 6572.84, "text": " Daniel traveled to the hallway.", "tokens": [8033, 16147, 281, 264, 23903, 13], "temperature": 0.0, "avg_logprob": -0.2145206594021521, "compression_ratio": 1.6896551724137931, "no_speech_prob": 6.439001936087152e-06}, {"id": 1270, "seek": 655784, "start": 6572.84, "end": 6573.84, "text": " Where is the milk?", "tokens": [2305, 307, 264, 5392, 30], "temperature": 0.0, "avg_logprob": -0.2145206594021521, "compression_ratio": 1.6896551724137931, "no_speech_prob": 6.439001936087152e-06}, {"id": 1271, "seek": 655784, "start": 6573.84, "end": 6574.84, "text": " Hallway.", "tokens": [5434, 676, 13], "temperature": 0.0, "avg_logprob": -0.2145206594021521, "compression_ratio": 1.6896551724137931, "no_speech_prob": 6.439001936087152e-06}, {"id": 1272, "seek": 655784, "start": 6574.84, "end": 6580.4800000000005, "text": " So that's what we have to be able to do this time.", "tokens": [407, 300, 311, 437, 321, 362, 281, 312, 1075, 281, 360, 341, 565, 13], "temperature": 0.0, "avg_logprob": -0.2145206594021521, "compression_ratio": 1.6896551724137931, "no_speech_prob": 6.439001936087152e-06}, {"id": 1273, "seek": 655784, "start": 6580.4800000000005, "end": 6587.08, "text": " And so what we're going to do is exactly the same thing as we did before.", "tokens": [400, 370, 437, 321, 434, 516, 281, 360, 307, 2293, 264, 912, 551, 382, 321, 630, 949, 13], "temperature": 0.0, "avg_logprob": -0.2145206594021521, "compression_ratio": 1.6896551724137931, "no_speech_prob": 6.439001936087152e-06}, {"id": 1274, "seek": 658708, "start": 6587.08, "end": 6609.8, "text": " We're going to take our whole little model, do the embedding, reshape.softmax, reshape.denselayer.sum,", "tokens": [492, 434, 516, 281, 747, 527, 1379, 707, 2316, 11, 360, 264, 12240, 3584, 11, 725, 42406, 13, 13908, 41167, 11, 725, 42406, 13, 67, 1288, 8376, 260, 13, 82, 449, 11], "temperature": 0.0, "avg_logprob": -0.26344722509384155, "compression_ratio": 1.308411214953271, "no_speech_prob": 1.5689245628891513e-05}, {"id": 1275, "seek": 658708, "start": 6609.8, "end": 6612.2, "text": " and we're going to call this one hop.", "tokens": [293, 321, 434, 516, 281, 818, 341, 472, 3818, 13], "temperature": 0.0, "avg_logprob": -0.26344722509384155, "compression_ratio": 1.308411214953271, "no_speech_prob": 1.5689245628891513e-05}, {"id": 1276, "seek": 661220, "start": 6612.2, "end": 6617.76, "text": " So this whole picture is going to become one hop.", "tokens": [407, 341, 1379, 3036, 307, 516, 281, 1813, 472, 3818, 13], "temperature": 0.0, "avg_logprob": -0.1398976208412484, "compression_ratio": 1.696774193548387, "no_speech_prob": 1.2482680631364929e-06}, {"id": 1277, "seek": 661220, "start": 6617.76, "end": 6630.48, "text": " And what we're then going to do is we're going to take this and go back and replace the query", "tokens": [400, 437, 321, 434, 550, 516, 281, 360, 307, 321, 434, 516, 281, 747, 341, 293, 352, 646, 293, 7406, 264, 14581], "temperature": 0.0, "avg_logprob": -0.1398976208412484, "compression_ratio": 1.696774193548387, "no_speech_prob": 1.2482680631364929e-06}, {"id": 1278, "seek": 661220, "start": 6630.48, "end": 6631.92, "text": " with our new output.", "tokens": [365, 527, 777, 5598, 13], "temperature": 0.0, "avg_logprob": -0.1398976208412484, "compression_ratio": 1.696774193548387, "no_speech_prob": 1.2482680631364929e-06}, {"id": 1279, "seek": 661220, "start": 6631.92, "end": 6640.32, "text": " So at each step, each hop, we're going to replace the query with the result of our memory", "tokens": [407, 412, 1184, 1823, 11, 1184, 3818, 11, 321, 434, 516, 281, 7406, 264, 14581, 365, 264, 1874, 295, 527, 4675], "temperature": 0.0, "avg_logprob": -0.1398976208412484, "compression_ratio": 1.696774193548387, "no_speech_prob": 1.2482680631364929e-06}, {"id": 1280, "seek": 661220, "start": 6640.32, "end": 6641.48, "text": " network.", "tokens": [3209, 13], "temperature": 0.0, "avg_logprob": -0.1398976208412484, "compression_ratio": 1.696774193548387, "no_speech_prob": 1.2482680631364929e-06}, {"id": 1281, "seek": 664148, "start": 6641.48, "end": 6650.24, "text": " And so that way, the memory network can learn to recognize that the first thing I need is", "tokens": [400, 370, 300, 636, 11, 264, 4675, 3209, 393, 1466, 281, 5521, 300, 264, 700, 551, 286, 643, 307], "temperature": 0.0, "avg_logprob": -0.21843704344734313, "compression_ratio": 1.5170068027210883, "no_speech_prob": 1.9637966488517122e-06}, {"id": 1282, "seek": 664148, "start": 6650.24, "end": 6655.16, "text": " the milk, search back, find milk.", "tokens": [264, 5392, 11, 3164, 646, 11, 915, 5392, 13], "temperature": 0.0, "avg_logprob": -0.21843704344734313, "compression_ratio": 1.5170068027210883, "no_speech_prob": 1.9637966488517122e-06}, {"id": 1283, "seek": 664148, "start": 6655.16, "end": 6661.16, "text": " I now have the milk, now you need to update the query to where is Daniel.", "tokens": [286, 586, 362, 264, 5392, 11, 586, 291, 643, 281, 5623, 264, 14581, 281, 689, 307, 8033, 13], "temperature": 0.0, "avg_logprob": -0.21843704344734313, "compression_ratio": 1.5170068027210883, "no_speech_prob": 1.9637966488517122e-06}, {"id": 1284, "seek": 664148, "start": 6661.16, "end": 6663.879999999999, "text": " Now go back, find Daniel.", "tokens": [823, 352, 646, 11, 915, 8033, 13], "temperature": 0.0, "avg_logprob": -0.21843704344734313, "compression_ratio": 1.5170068027210883, "no_speech_prob": 1.9637966488517122e-06}, {"id": 1285, "seek": 666388, "start": 6663.88, "end": 6671.4400000000005, "text": " So the memory network in multi-hop mode basically does this whole thing again and again and", "tokens": [407, 264, 4675, 3209, 294, 4825, 12, 9050, 4391, 1936, 775, 341, 1379, 551, 797, 293, 797, 293], "temperature": 0.0, "avg_logprob": -0.17064415216445922, "compression_ratio": 1.6521739130434783, "no_speech_prob": 5.20355911248771e-07}, {"id": 1286, "seek": 666388, "start": 6671.4400000000005, "end": 6675.8, "text": " again, replacing the query each time.", "tokens": [797, 11, 19139, 264, 14581, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.17064415216445922, "compression_ratio": 1.6521739130434783, "no_speech_prob": 5.20355911248771e-07}, {"id": 1287, "seek": 666388, "start": 6675.8, "end": 6682.400000000001, "text": " So that's why I just took the whole set of steps and chucked it into a single function.", "tokens": [407, 300, 311, 983, 286, 445, 1890, 264, 1379, 992, 295, 4439, 293, 20870, 292, 309, 666, 257, 2167, 2445, 13], "temperature": 0.0, "avg_logprob": -0.17064415216445922, "compression_ratio": 1.6521739130434783, "no_speech_prob": 5.20355911248771e-07}, {"id": 1288, "seek": 666388, "start": 6682.400000000001, "end": 6690.16, "text": " And so then I just go, okay, response, story is one hop, response, story is one hop on", "tokens": [400, 370, 550, 286, 445, 352, 11, 1392, 11, 4134, 11, 1657, 307, 472, 3818, 11, 4134, 11, 1657, 307, 472, 3818, 322], "temperature": 0.0, "avg_logprob": -0.17064415216445922, "compression_ratio": 1.6521739130434783, "no_speech_prob": 5.20355911248771e-07}, {"id": 1289, "seek": 669016, "start": 6690.16, "end": 6694.88, "text": " that, and you can keep repeating that again and again and again.", "tokens": [300, 11, 293, 291, 393, 1066, 18617, 300, 797, 293, 797, 293, 797, 13], "temperature": 0.0, "avg_logprob": -0.127170837088807, "compression_ratio": 1.5813953488372092, "no_speech_prob": 3.9054707485775e-06}, {"id": 1290, "seek": 669016, "start": 6694.88, "end": 6705.08, "text": " And then at the end, get our output, that's our model, compile, fit.", "tokens": [400, 550, 412, 264, 917, 11, 483, 527, 5598, 11, 300, 311, 527, 2316, 11, 31413, 11, 3318, 13], "temperature": 0.0, "avg_logprob": -0.127170837088807, "compression_ratio": 1.5813953488372092, "no_speech_prob": 3.9054707485775e-06}, {"id": 1291, "seek": 669016, "start": 6705.08, "end": 6709.48, "text": " I had real trouble getting this to fit nicely.", "tokens": [286, 632, 957, 5253, 1242, 341, 281, 3318, 9594, 13], "temperature": 0.0, "avg_logprob": -0.127170837088807, "compression_ratio": 1.5813953488372092, "no_speech_prob": 3.9054707485775e-06}, {"id": 1292, "seek": 669016, "start": 6709.48, "end": 6716.04, "text": " I had to play around a lot with learning rates and batch sizes and whatever else, but I did", "tokens": [286, 632, 281, 862, 926, 257, 688, 365, 2539, 6846, 293, 15245, 11602, 293, 2035, 1646, 11, 457, 286, 630], "temperature": 0.0, "avg_logprob": -0.127170837088807, "compression_ratio": 1.5813953488372092, "no_speech_prob": 3.9054707485775e-06}, {"id": 1293, "seek": 671604, "start": 6716.04, "end": 6725.24, "text": " eventually get it up to.999 accuracy.", "tokens": [4728, 483, 309, 493, 281, 2411, 49017, 14170, 13], "temperature": 0.0, "avg_logprob": -0.27628942848979565, "compression_ratio": 1.483695652173913, "no_speech_prob": 3.905464382114587e-06}, {"id": 1294, "seek": 671604, "start": 6725.24, "end": 6732.28, "text": " So this is kind of an unusual class for me to be teaching, particularly compared to part", "tokens": [407, 341, 307, 733, 295, 364, 10901, 1508, 337, 385, 281, 312, 4571, 11, 4098, 5347, 281, 644], "temperature": 0.0, "avg_logprob": -0.27628942848979565, "compression_ratio": 1.483695652173913, "no_speech_prob": 3.905464382114587e-06}, {"id": 1295, "seek": 671604, "start": 6732.28, "end": 6734.28, "text": " 1 where it was best practices.", "tokens": [502, 689, 309, 390, 1151, 7525, 13], "temperature": 0.0, "avg_logprob": -0.27628942848979565, "compression_ratio": 1.483695652173913, "no_speech_prob": 3.905464382114587e-06}, {"id": 1296, "seek": 671604, "start": 6734.28, "end": 6736.76, "text": " Clearly this is anything but.", "tokens": [24120, 341, 307, 1340, 457, 13], "temperature": 0.0, "avg_logprob": -0.27628942848979565, "compression_ratio": 1.483695652173913, "no_speech_prob": 3.905464382114587e-06}, {"id": 1297, "seek": 671604, "start": 6736.76, "end": 6743.04, "text": " I'm kind of showing you something which was maybe the most popular request, which was", "tokens": [286, 478, 733, 295, 4099, 291, 746, 597, 390, 1310, 264, 881, 3743, 5308, 11, 597, 390], "temperature": 0.0, "avg_logprob": -0.27628942848979565, "compression_ratio": 1.483695652173913, "no_speech_prob": 3.905464382114587e-06}, {"id": 1298, "seek": 674304, "start": 6743.04, "end": 6746.48, "text": " to teach us about chatbots.", "tokens": [281, 2924, 505, 466, 5081, 65, 1971, 13], "temperature": 0.0, "avg_logprob": -0.17680227876913668, "compression_ratio": 1.7535545023696681, "no_speech_prob": 7.646485755685717e-06}, {"id": 1299, "seek": 674304, "start": 6746.48, "end": 6750.04, "text": " But let's be honest, who has ever used a chatbot that's not terrible?", "tokens": [583, 718, 311, 312, 3245, 11, 567, 575, 1562, 1143, 257, 5081, 18870, 300, 311, 406, 6237, 30], "temperature": 0.0, "avg_logprob": -0.17680227876913668, "compression_ratio": 1.7535545023696681, "no_speech_prob": 7.646485755685717e-06}, {"id": 1300, "seek": 674304, "start": 6750.04, "end": 6753.6, "text": " And the reason no one's used a chatbot that's not terrible is that the current state of", "tokens": [400, 264, 1778, 572, 472, 311, 1143, 257, 5081, 18870, 300, 311, 406, 6237, 307, 300, 264, 2190, 1785, 295], "temperature": 0.0, "avg_logprob": -0.17680227876913668, "compression_ratio": 1.7535545023696681, "no_speech_prob": 7.646485755685717e-06}, {"id": 1301, "seek": 674304, "start": 6753.6, "end": 6756.4, "text": " the art is terrible.", "tokens": [264, 1523, 307, 6237, 13], "temperature": 0.0, "avg_logprob": -0.17680227876913668, "compression_ratio": 1.7535545023696681, "no_speech_prob": 7.646485755685717e-06}, {"id": 1302, "seek": 674304, "start": 6756.4, "end": 6763.84, "text": " So chatbots have their place, and indeed one of the students in class has written a really", "tokens": [407, 5081, 65, 1971, 362, 641, 1081, 11, 293, 6451, 472, 295, 264, 1731, 294, 1508, 575, 3720, 257, 534], "temperature": 0.0, "avg_logprob": -0.17680227876913668, "compression_ratio": 1.7535545023696681, "no_speech_prob": 7.646485755685717e-06}, {"id": 1303, "seek": 674304, "start": 6763.84, "end": 6770.16, "text": " interesting analysis of this, which hopefully she'll share on the forum.", "tokens": [1880, 5215, 295, 341, 11, 597, 4696, 750, 603, 2073, 322, 264, 17542, 13], "temperature": 0.0, "avg_logprob": -0.17680227876913668, "compression_ratio": 1.7535545023696681, "no_speech_prob": 7.646485755685717e-06}, {"id": 1304, "seek": 677016, "start": 6770.16, "end": 6780.5199999999995, "text": " But that place has really lots of heuristics and carefully set up vocabularies and selecting", "tokens": [583, 300, 1081, 575, 534, 3195, 295, 415, 374, 6006, 293, 7500, 992, 493, 2329, 455, 1040, 530, 293, 18182], "temperature": 0.0, "avg_logprob": -0.20876866921611215, "compression_ratio": 1.5714285714285714, "no_speech_prob": 7.646449375897646e-06}, {"id": 1305, "seek": 677016, "start": 6780.5199999999995, "end": 6783.24, "text": " from small sets of answers and so forth.", "tokens": [490, 1359, 6352, 295, 6338, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.20876866921611215, "compression_ratio": 1.5714285714285714, "no_speech_prob": 7.646449375897646e-06}, {"id": 1306, "seek": 677016, "start": 6783.24, "end": 6791.2, "text": " It's not general purpose, here's a story, ask anything you like about it, here are some", "tokens": [467, 311, 406, 2674, 4334, 11, 510, 311, 257, 1657, 11, 1029, 1340, 291, 411, 466, 309, 11, 510, 366, 512], "temperature": 0.0, "avg_logprob": -0.20876866921611215, "compression_ratio": 1.5714285714285714, "no_speech_prob": 7.646449375897646e-06}, {"id": 1307, "seek": 677016, "start": 6791.2, "end": 6792.2, "text": " answers.", "tokens": [6338, 13], "temperature": 0.0, "avg_logprob": -0.20876866921611215, "compression_ratio": 1.5714285714285714, "no_speech_prob": 7.646449375897646e-06}, {"id": 1308, "seek": 677016, "start": 6792.2, "end": 6798.96, "text": " It's not to say we won't get there, I sure hope we will, but the kind of incredible hype", "tokens": [467, 311, 406, 281, 584, 321, 1582, 380, 483, 456, 11, 286, 988, 1454, 321, 486, 11, 457, 264, 733, 295, 4651, 24144], "temperature": 0.0, "avg_logprob": -0.20876866921611215, "compression_ratio": 1.5714285714285714, "no_speech_prob": 7.646449375897646e-06}, {"id": 1309, "seek": 679896, "start": 6798.96, "end": 6803.56, "text": " we had around neural-turing machines and memory networks and end-to-end memory networks is", "tokens": [321, 632, 926, 18161, 12, 83, 1345, 8379, 293, 4675, 9590, 293, 917, 12, 1353, 12, 521, 4675, 9590, 307], "temperature": 0.0, "avg_logprob": -0.26358842849731445, "compression_ratio": 1.582857142857143, "no_speech_prob": 5.093676463729935e-06}, {"id": 1310, "seek": 679896, "start": 6803.56, "end": 6809.16, "text": " kind of, as you can see, even when you just look at the dataset, what they worked on is", "tokens": [733, 295, 11, 382, 291, 393, 536, 11, 754, 562, 291, 445, 574, 412, 264, 28872, 11, 437, 436, 2732, 322, 307], "temperature": 0.0, "avg_logprob": -0.26358842849731445, "compression_ratio": 1.582857142857143, "no_speech_prob": 5.093676463729935e-06}, {"id": 1311, "seek": 679896, "start": 6809.16, "end": 6812.36, "text": " kind of crazy.", "tokens": [733, 295, 3219, 13], "temperature": 0.0, "avg_logprob": -0.26358842849731445, "compression_ratio": 1.582857142857143, "no_speech_prob": 5.093676463729935e-06}, {"id": 1312, "seek": 679896, "start": 6812.36, "end": 6821.68, "text": " So that is not quite the final conclusion of this though, because yesterday a paper", "tokens": [407, 300, 307, 406, 1596, 264, 2572, 10063, 295, 341, 1673, 11, 570, 5186, 257, 3035], "temperature": 0.0, "avg_logprob": -0.26358842849731445, "compression_ratio": 1.582857142857143, "no_speech_prob": 5.093676463729935e-06}, {"id": 1313, "seek": 682168, "start": 6821.68, "end": 6833.240000000001, "text": " came out which showed how to identify buffer overruns in computer source code using memory", "tokens": [1361, 484, 597, 4712, 577, 281, 5876, 21762, 670, 12997, 82, 294, 3820, 4009, 3089, 1228, 4675], "temperature": 0.0, "avg_logprob": -0.16973647786610163, "compression_ratio": 1.4845360824742269, "no_speech_prob": 3.966951226175297e-06}, {"id": 1314, "seek": 682168, "start": 6833.240000000001, "end": 6835.16, "text": " networks.", "tokens": [9590, 13], "temperature": 0.0, "avg_logprob": -0.16973647786610163, "compression_ratio": 1.4845360824742269, "no_speech_prob": 3.966951226175297e-06}, {"id": 1315, "seek": 682168, "start": 6835.16, "end": 6842.4800000000005, "text": " And so it kind of spoiled my whole narrative that somebody seems to have actually used", "tokens": [400, 370, 309, 733, 295, 32439, 452, 1379, 9977, 300, 2618, 2544, 281, 362, 767, 1143], "temperature": 0.0, "avg_logprob": -0.16973647786610163, "compression_ratio": 1.4845360824742269, "no_speech_prob": 3.966951226175297e-06}, {"id": 1316, "seek": 682168, "start": 6842.4800000000005, "end": 6846.280000000001, "text": " this technology for something effectively.", "tokens": [341, 2899, 337, 746, 8659, 13], "temperature": 0.0, "avg_logprob": -0.16973647786610163, "compression_ratio": 1.4845360824742269, "no_speech_prob": 3.966951226175297e-06}, {"id": 1317, "seek": 682168, "start": 6846.280000000001, "end": 6848.76, "text": " And I guess when you think about it, it makes some sense.", "tokens": [400, 286, 2041, 562, 291, 519, 466, 309, 11, 309, 1669, 512, 2020, 13], "temperature": 0.0, "avg_logprob": -0.16973647786610163, "compression_ratio": 1.4845360824742269, "no_speech_prob": 3.966951226175297e-06}, {"id": 1318, "seek": 684876, "start": 6848.76, "end": 6853.360000000001, "text": " So in case you don't know what a buffer overrun is, that's like if you're writing in an unsafe", "tokens": [407, 294, 1389, 291, 500, 380, 458, 437, 257, 21762, 670, 12997, 307, 11, 300, 311, 411, 498, 291, 434, 3579, 294, 364, 35948], "temperature": 0.0, "avg_logprob": -0.15014100508256392, "compression_ratio": 1.69140625, "no_speech_prob": 5.173882527742535e-06}, {"id": 1319, "seek": 684876, "start": 6853.360000000001, "end": 6860.0, "text": " language, properly C, you allocate some memory that's going to store some result or some", "tokens": [2856, 11, 6108, 383, 11, 291, 35713, 512, 4675, 300, 311, 516, 281, 3531, 512, 1874, 420, 512], "temperature": 0.0, "avg_logprob": -0.15014100508256392, "compression_ratio": 1.69140625, "no_speech_prob": 5.173882527742535e-06}, {"id": 1320, "seek": 684876, "start": 6860.0, "end": 6864.72, "text": " input, and you try to put into that memory something bigger than the amount that you", "tokens": [4846, 11, 293, 291, 853, 281, 829, 666, 300, 4675, 746, 3801, 813, 264, 2372, 300, 291], "temperature": 0.0, "avg_logprob": -0.15014100508256392, "compression_ratio": 1.69140625, "no_speech_prob": 5.173882527742535e-06}, {"id": 1321, "seek": 684876, "start": 6864.72, "end": 6867.88, "text": " allocated, it basically spills out the end.", "tokens": [29772, 11, 309, 1936, 637, 2565, 484, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.15014100508256392, "compression_ratio": 1.69140625, "no_speech_prob": 5.173882527742535e-06}, {"id": 1322, "seek": 684876, "start": 6867.88, "end": 6871.88, "text": " And in the best case, it crashes.", "tokens": [400, 294, 264, 1151, 1389, 11, 309, 28642, 13], "temperature": 0.0, "avg_logprob": -0.15014100508256392, "compression_ratio": 1.69140625, "no_speech_prob": 5.173882527742535e-06}, {"id": 1323, "seek": 684876, "start": 6871.88, "end": 6876.6, "text": " In the worst case, somebody figures out how to get exactly the right code to spill out", "tokens": [682, 264, 5855, 1389, 11, 2618, 9624, 484, 577, 281, 483, 2293, 264, 558, 3089, 281, 22044, 484], "temperature": 0.0, "avg_logprob": -0.15014100508256392, "compression_ratio": 1.69140625, "no_speech_prob": 5.173882527742535e-06}, {"id": 1324, "seek": 687660, "start": 6876.6, "end": 6880.68, "text": " into exactly the right place and ends up taking over your machine.", "tokens": [666, 2293, 264, 558, 1081, 293, 5314, 493, 1940, 670, 428, 3479, 13], "temperature": 0.0, "avg_logprob": -0.22317449623179214, "compression_ratio": 1.689516129032258, "no_speech_prob": 5.422196409199387e-06}, {"id": 1325, "seek": 687660, "start": 6880.68, "end": 6885.120000000001, "text": " So buffer overruns are horrible things.", "tokens": [407, 21762, 670, 12997, 82, 366, 9263, 721, 13], "temperature": 0.0, "avg_logprob": -0.22317449623179214, "compression_ratio": 1.689516129032258, "no_speech_prob": 5.422196409199387e-06}, {"id": 1326, "seek": 687660, "start": 6885.120000000001, "end": 6889.360000000001, "text": " And the idea of being able to find them, I can actually see it does look a lot like this", "tokens": [400, 264, 1558, 295, 885, 1075, 281, 915, 552, 11, 286, 393, 767, 536, 309, 775, 574, 257, 688, 411, 341], "temperature": 0.0, "avg_logprob": -0.22317449623179214, "compression_ratio": 1.689516129032258, "no_speech_prob": 5.422196409199387e-06}, {"id": 1327, "seek": 687660, "start": 6889.360000000001, "end": 6890.360000000001, "text": " memory network.", "tokens": [4675, 3209, 13], "temperature": 0.0, "avg_logprob": -0.22317449623179214, "compression_ratio": 1.689516129032258, "no_speech_prob": 5.422196409199387e-06}, {"id": 1328, "seek": 687660, "start": 6890.360000000001, "end": 6895.84, "text": " You kind of have to see where was that variable set, and then where was the thing that was", "tokens": [509, 733, 295, 362, 281, 536, 689, 390, 300, 7006, 992, 11, 293, 550, 689, 390, 264, 551, 300, 390], "temperature": 0.0, "avg_logprob": -0.22317449623179214, "compression_ratio": 1.689516129032258, "no_speech_prob": 5.422196409199387e-06}, {"id": 1329, "seek": 687660, "start": 6895.84, "end": 6899.120000000001, "text": " set from set, and where was the original thing allocated.", "tokens": [992, 490, 992, 11, 293, 689, 390, 264, 3380, 551, 29772, 13], "temperature": 0.0, "avg_logprob": -0.22317449623179214, "compression_ratio": 1.689516129032258, "no_speech_prob": 5.422196409199387e-06}, {"id": 1330, "seek": 687660, "start": 6899.120000000001, "end": 6902.76, "text": " It's kind of like just going back through the source code.", "tokens": [467, 311, 733, 295, 411, 445, 516, 646, 807, 264, 4009, 3089, 13], "temperature": 0.0, "avg_logprob": -0.22317449623179214, "compression_ratio": 1.689516129032258, "no_speech_prob": 5.422196409199387e-06}, {"id": 1331, "seek": 690276, "start": 6902.76, "end": 6909.56, "text": " The vocabulary is pretty straightforward, it's just the variables that have been defined.", "tokens": [440, 19864, 307, 1238, 15325, 11, 309, 311, 445, 264, 9102, 300, 362, 668, 7642, 13], "temperature": 0.0, "avg_logprob": -0.20507390975952147, "compression_ratio": 1.6041666666666667, "no_speech_prob": 4.56595580544672e-06}, {"id": 1332, "seek": 690276, "start": 6909.56, "end": 6911.72, "text": " So that's kind of interesting.", "tokens": [407, 300, 311, 733, 295, 1880, 13], "temperature": 0.0, "avg_logprob": -0.20507390975952147, "compression_ratio": 1.6041666666666667, "no_speech_prob": 4.56595580544672e-06}, {"id": 1333, "seek": 690276, "start": 6911.72, "end": 6918.56, "text": " I haven't had a chance to really study the paper yet, but it's no chat bot, but maybe", "tokens": [286, 2378, 380, 632, 257, 2931, 281, 534, 2979, 264, 3035, 1939, 11, 457, 309, 311, 572, 5081, 10592, 11, 457, 1310], "temperature": 0.0, "avg_logprob": -0.20507390975952147, "compression_ratio": 1.6041666666666667, "no_speech_prob": 4.56595580544672e-06}, {"id": 1334, "seek": 690276, "start": 6918.56, "end": 6921.92, "text": " there is a room for memory networks already after all.", "tokens": [456, 307, 257, 1808, 337, 4675, 9590, 1217, 934, 439, 13], "temperature": 0.0, "avg_logprob": -0.20507390975952147, "compression_ratio": 1.6041666666666667, "no_speech_prob": 4.56595580544672e-06}, {"id": 1335, "seek": 690276, "start": 6921.92, "end": 6922.92, "text": " Question.", "tokens": [14464, 13], "temperature": 0.0, "avg_logprob": -0.20507390975952147, "compression_ratio": 1.6041666666666667, "no_speech_prob": 4.56595580544672e-06}, {"id": 1336, "seek": 690276, "start": 6922.92, "end": 6926.52, "text": " Is there a way to visualize what the neural network has learned for the text?", "tokens": [1119, 456, 257, 636, 281, 23273, 437, 264, 18161, 3209, 575, 3264, 337, 264, 2487, 30], "temperature": 0.0, "avg_logprob": -0.20507390975952147, "compression_ratio": 1.6041666666666667, "no_speech_prob": 4.56595580544672e-06}, {"id": 1337, "seek": 690276, "start": 6926.52, "end": 6927.52, "text": " Answer.", "tokens": [24545, 13], "temperature": 0.0, "avg_logprob": -0.20507390975952147, "compression_ratio": 1.6041666666666667, "no_speech_prob": 4.56595580544672e-06}, {"id": 1338, "seek": 690276, "start": 6927.52, "end": 6930.64, "text": " There is no neural network.", "tokens": [821, 307, 572, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.20507390975952147, "compression_ratio": 1.6041666666666667, "no_speech_prob": 4.56595580544672e-06}, {"id": 1339, "seek": 693064, "start": 6930.64, "end": 6936.4400000000005, "text": " If you mean the embeddings, you can look at the embeddings easily enough.", "tokens": [759, 291, 914, 264, 12240, 29432, 11, 291, 393, 574, 412, 264, 12240, 29432, 3612, 1547, 13], "temperature": 0.0, "avg_logprob": -0.1454242224832183, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.66458321979735e-06}, {"id": 1340, "seek": 693064, "start": 6936.4400000000005, "end": 6940.08, "text": " The whole thing is so simple, it's very easy to look at every embedding.", "tokens": [440, 1379, 551, 307, 370, 2199, 11, 309, 311, 588, 1858, 281, 574, 412, 633, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.1454242224832183, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.66458321979735e-06}, {"id": 1341, "seek": 693064, "start": 6940.08, "end": 6948.160000000001, "text": " As I mentioned, we looked at visualizing the weights that came out of the softmax.", "tokens": [1018, 286, 2835, 11, 321, 2956, 412, 5056, 3319, 264, 17443, 300, 1361, 484, 295, 264, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.1454242224832183, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.66458321979735e-06}, {"id": 1342, "seek": 693064, "start": 6948.160000000001, "end": 6952.4800000000005, "text": " We don't even need to look at it in order to figure out what it learned.", "tokens": [492, 500, 380, 754, 643, 281, 574, 412, 309, 294, 1668, 281, 2573, 484, 437, 309, 3264, 13], "temperature": 0.0, "avg_logprob": -0.1454242224832183, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.66458321979735e-06}, {"id": 1343, "seek": 693064, "start": 6952.4800000000005, "end": 6958.280000000001, "text": " Based on the fact that this is just a small number of simple linear steps, we know that", "tokens": [18785, 322, 264, 1186, 300, 341, 307, 445, 257, 1359, 1230, 295, 2199, 8213, 4439, 11, 321, 458, 300], "temperature": 0.0, "avg_logprob": -0.1454242224832183, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.66458321979735e-06}, {"id": 1344, "seek": 695828, "start": 6958.28, "end": 6966.719999999999, "text": " it basically has to learn what each sentence answer can be.", "tokens": [309, 1936, 575, 281, 1466, 437, 1184, 8174, 1867, 393, 312, 13], "temperature": 0.0, "avg_logprob": -0.34900944049541766, "compression_ratio": 1.639240506329114, "no_speech_prob": 1.2411322131811175e-05}, {"id": 1345, "seek": 695828, "start": 6966.719999999999, "end": 6971.28, "text": " Sentence number 3's answer will always be milk, sentence number 4's answer will always", "tokens": [23652, 655, 1230, 805, 311, 1867, 486, 1009, 312, 5392, 11, 8174, 1230, 1017, 311, 1867, 486, 1009], "temperature": 0.0, "avg_logprob": -0.34900944049541766, "compression_ratio": 1.639240506329114, "no_speech_prob": 1.2411322131811175e-05}, {"id": 1346, "seek": 695828, "start": 6971.28, "end": 6975.04, "text": " be four-way or whatever.", "tokens": [312, 1451, 12, 676, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.34900944049541766, "compression_ratio": 1.639240506329114, "no_speech_prob": 1.2411322131811175e-05}, {"id": 1347, "seek": 695828, "start": 6975.04, "end": 6986.36, "text": " And then the embeddings of the weights are going to have to basically learn how to come", "tokens": [400, 550, 264, 12240, 29432, 295, 264, 17443, 366, 516, 281, 362, 281, 1936, 1466, 577, 281, 808], "temperature": 0.0, "avg_logprob": -0.34900944049541766, "compression_ratio": 1.639240506329114, "no_speech_prob": 1.2411322131811175e-05}, {"id": 1348, "seek": 698636, "start": 6986.36, "end": 6989.599999999999, "text": " up with what's going to be a similar embedding to the query.", "tokens": [493, 365, 437, 311, 516, 281, 312, 257, 2531, 12240, 3584, 281, 264, 14581, 13], "temperature": 0.0, "avg_logprob": -0.2145938086755497, "compression_ratio": 1.6260504201680672, "no_speech_prob": 4.936947334499564e-06}, {"id": 1349, "seek": 698636, "start": 6989.599999999999, "end": 6994.32, "text": " In fact, I think you can even make them the same embedding so that these dot products", "tokens": [682, 1186, 11, 286, 519, 291, 393, 754, 652, 552, 264, 912, 12240, 3584, 370, 300, 613, 5893, 3383], "temperature": 0.0, "avg_logprob": -0.2145938086755497, "compression_ratio": 1.6260504201680672, "no_speech_prob": 4.936947334499564e-06}, {"id": 1350, "seek": 698636, "start": 6994.32, "end": 6998.88, "text": " basically give you something that gives you similarity scores.", "tokens": [1936, 976, 291, 746, 300, 2709, 291, 32194, 13444, 13], "temperature": 0.0, "avg_logprob": -0.2145938086755497, "compression_ratio": 1.6260504201680672, "no_speech_prob": 4.936947334499564e-06}, {"id": 1351, "seek": 698636, "start": 6998.88, "end": 7002.0, "text": " So this is really a very simple, largely linear model.", "tokens": [407, 341, 307, 534, 257, 588, 2199, 11, 11611, 8213, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2145938086755497, "compression_ratio": 1.6260504201680672, "no_speech_prob": 4.936947334499564e-06}, {"id": 1352, "seek": 698636, "start": 7002.0, "end": 7008.96, "text": " It doesn't require too much visualizing.", "tokens": [467, 1177, 380, 3651, 886, 709, 5056, 3319, 13], "temperature": 0.0, "avg_logprob": -0.2145938086755497, "compression_ratio": 1.6260504201680672, "no_speech_prob": 4.936947334499564e-06}, {"id": 1353, "seek": 698636, "start": 7008.96, "end": 7015.48, "text": " So having said all that, none of this is to say that memory networks are useless.", "tokens": [407, 1419, 848, 439, 300, 11, 6022, 295, 341, 307, 281, 584, 300, 4675, 9590, 366, 14115, 13], "temperature": 0.0, "avg_logprob": -0.2145938086755497, "compression_ratio": 1.6260504201680672, "no_speech_prob": 4.936947334499564e-06}, {"id": 1354, "seek": 701548, "start": 7015.48, "end": 7021.04, "text": " They were created by very smart people with impressive pedigree and deep learning.", "tokens": [814, 645, 2942, 538, 588, 4069, 561, 365, 8992, 5670, 328, 701, 293, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.23095823986695543, "compression_ratio": 1.6693548387096775, "no_speech_prob": 2.507112549210433e-05}, {"id": 1355, "seek": 701548, "start": 7021.04, "end": 7026.599999999999, "text": " This is very early and this tends to happen in popular press.", "tokens": [639, 307, 588, 2440, 293, 341, 12258, 281, 1051, 294, 3743, 1886, 13], "temperature": 0.0, "avg_logprob": -0.23095823986695543, "compression_ratio": 1.6693548387096775, "no_speech_prob": 2.507112549210433e-05}, {"id": 1356, "seek": 701548, "start": 7026.599999999999, "end": 7030.36, "text": " They kind of get over-excited about things.", "tokens": [814, 733, 295, 483, 670, 12, 27855, 1226, 466, 721, 13], "temperature": 0.0, "avg_logprob": -0.23095823986695543, "compression_ratio": 1.6693548387096775, "no_speech_prob": 2.507112549210433e-05}, {"id": 1357, "seek": 701548, "start": 7030.36, "end": 7032.919999999999, "text": " Although in this case, I don't think we can blame the press, I think we have to blame", "tokens": [5780, 294, 341, 1389, 11, 286, 500, 380, 519, 321, 393, 10127, 264, 1886, 11, 286, 519, 321, 362, 281, 10127], "temperature": 0.0, "avg_logprob": -0.23095823986695543, "compression_ratio": 1.6693548387096775, "no_speech_prob": 2.507112549210433e-05}, {"id": 1358, "seek": 701548, "start": 7032.919999999999, "end": 7035.5599999999995, "text": " Facebook for creating a ridiculous demo like this.", "tokens": [4384, 337, 4084, 257, 11083, 10723, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.23095823986695543, "compression_ratio": 1.6693548387096775, "no_speech_prob": 2.507112549210433e-05}, {"id": 1359, "seek": 701548, "start": 7035.5599999999995, "end": 7041.5599999999995, "text": " This is clearly created to give people the wrong idea, which I find very surprising from", "tokens": [639, 307, 4448, 2942, 281, 976, 561, 264, 2085, 1558, 11, 597, 286, 915, 588, 8830, 490], "temperature": 0.0, "avg_logprob": -0.23095823986695543, "compression_ratio": 1.6693548387096775, "no_speech_prob": 2.507112549210433e-05}, {"id": 1360, "seek": 704156, "start": 7041.56, "end": 7047.400000000001, "text": " people like Jan LeCun who normally do the opposite of that kind of thing.", "tokens": [561, 411, 4956, 1456, 34, 409, 567, 5646, 360, 264, 6182, 295, 300, 733, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.21821943047928483, "compression_ratio": 1.4482758620689655, "no_speech_prob": 1.2805291589756962e-05}, {"id": 1361, "seek": 704156, "start": 7047.400000000001, "end": 7052.64, "text": " So this is not really the press' fault in this case.", "tokens": [407, 341, 307, 406, 534, 264, 1886, 6, 7441, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.21821943047928483, "compression_ratio": 1.4482758620689655, "no_speech_prob": 1.2805291589756962e-05}, {"id": 1362, "seek": 704156, "start": 7052.64, "end": 7059.0, "text": " But this may well turn out to be a critical component in chatbots and Q&A systems and", "tokens": [583, 341, 815, 731, 1261, 484, 281, 312, 257, 4924, 6542, 294, 5081, 65, 1971, 293, 1249, 5, 32, 3652, 293], "temperature": 0.0, "avg_logprob": -0.21821943047928483, "compression_ratio": 1.4482758620689655, "no_speech_prob": 1.2805291589756962e-05}, {"id": 1363, "seek": 704156, "start": 7059.0, "end": 7060.8, "text": " whatever else.", "tokens": [2035, 1646, 13], "temperature": 0.0, "avg_logprob": -0.21821943047928483, "compression_ratio": 1.4482758620689655, "no_speech_prob": 1.2805291589756962e-05}, {"id": 1364, "seek": 704156, "start": 7060.8, "end": 7065.120000000001, "text": " But we're not there yet.", "tokens": [583, 321, 434, 406, 456, 1939, 13], "temperature": 0.0, "avg_logprob": -0.21821943047928483, "compression_ratio": 1.4482758620689655, "no_speech_prob": 1.2805291589756962e-05}, {"id": 1365, "seek": 706512, "start": 7065.12, "end": 7072.48, "text": " I had a good chat to Stephen Meridy the other day, who's a researcher I respect a lot and", "tokens": [286, 632, 257, 665, 5081, 281, 13391, 6124, 38836, 264, 661, 786, 11, 567, 311, 257, 21751, 286, 3104, 257, 688, 293], "temperature": 0.0, "avg_logprob": -0.2668126344680786, "compression_ratio": 1.5297029702970297, "no_speech_prob": 1.4510185792460106e-05}, {"id": 1366, "seek": 706512, "start": 7072.48, "end": 7074.64, "text": " also somebody I like.", "tokens": [611, 2618, 286, 411, 13], "temperature": 0.0, "avg_logprob": -0.2668126344680786, "compression_ratio": 1.5297029702970297, "no_speech_prob": 1.4510185792460106e-05}, {"id": 1367, "seek": 706512, "start": 7074.64, "end": 7080.48, "text": " I asked him what he thought was the most exciting research in this direction at the moment,", "tokens": [286, 2351, 796, 437, 415, 1194, 390, 264, 881, 4670, 2132, 294, 341, 3513, 412, 264, 1623, 11], "temperature": 0.0, "avg_logprob": -0.2668126344680786, "compression_ratio": 1.5297029702970297, "no_speech_prob": 1.4510185792460106e-05}, {"id": 1368, "seek": 706512, "start": 7080.48, "end": 7086.12, "text": " and he mentioned something that I was also very excited about, which is called Recurrent", "tokens": [293, 415, 2835, 746, 300, 286, 390, 611, 588, 2919, 466, 11, 597, 307, 1219, 9647, 374, 1753], "temperature": 0.0, "avg_logprob": -0.2668126344680786, "compression_ratio": 1.5297029702970297, "no_speech_prob": 1.4510185792460106e-05}, {"id": 1369, "seek": 706512, "start": 7086.12, "end": 7088.96, "text": " Entity Networks.", "tokens": [3951, 507, 12640, 82, 13], "temperature": 0.0, "avg_logprob": -0.2668126344680786, "compression_ratio": 1.5297029702970297, "no_speech_prob": 1.4510185792460106e-05}, {"id": 1370, "seek": 708896, "start": 7088.96, "end": 7096.84, "text": " The Recurrent Entity Network paper is the first to solve all of the Babbi tasks with", "tokens": [440, 9647, 374, 1753, 3951, 507, 12640, 3035, 307, 264, 700, 281, 5039, 439, 295, 264, 15820, 5614, 9608, 365], "temperature": 0.0, "avg_logprob": -0.2359357184552132, "compression_ratio": 1.48471615720524, "no_speech_prob": 1.28052597574424e-05}, {"id": 1371, "seek": 708896, "start": 7096.84, "end": 7099.84, "text": " 100% accuracy.", "tokens": [2319, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.2359357184552132, "compression_ratio": 1.48471615720524, "no_speech_prob": 1.28052597574424e-05}, {"id": 1372, "seek": 708896, "start": 7099.84, "end": 7102.8, "text": " Now take of that what you will.", "tokens": [823, 747, 295, 300, 437, 291, 486, 13], "temperature": 0.0, "avg_logprob": -0.2359357184552132, "compression_ratio": 1.48471615720524, "no_speech_prob": 1.28052597574424e-05}, {"id": 1373, "seek": 708896, "start": 7102.8, "end": 7105.24, "text": " I don't know how much that means.", "tokens": [286, 500, 380, 458, 577, 709, 300, 1355, 13], "temperature": 0.0, "avg_logprob": -0.2359357184552132, "compression_ratio": 1.48471615720524, "no_speech_prob": 1.28052597574424e-05}, {"id": 1374, "seek": 708896, "start": 7105.24, "end": 7106.92, "text": " They're synthetic tasks.", "tokens": [814, 434, 23420, 9608, 13], "temperature": 0.0, "avg_logprob": -0.2359357184552132, "compression_ratio": 1.48471615720524, "no_speech_prob": 1.28052597574424e-05}, {"id": 1375, "seek": 708896, "start": 7106.92, "end": 7112.72, "text": " One of the things that Stephen Meridy actually pointed out in a blog post is that even the", "tokens": [1485, 295, 264, 721, 300, 13391, 6124, 38836, 767, 10932, 484, 294, 257, 6968, 2183, 307, 300, 754, 264], "temperature": 0.0, "avg_logprob": -0.2359357184552132, "compression_ratio": 1.48471615720524, "no_speech_prob": 1.28052597574424e-05}, {"id": 1376, "seek": 708896, "start": 7112.72, "end": 7115.2, "text": " basic kind of coding of how they're created is pretty bad.", "tokens": [3875, 733, 295, 17720, 295, 577, 436, 434, 2942, 307, 1238, 1578, 13], "temperature": 0.0, "avg_logprob": -0.2359357184552132, "compression_ratio": 1.48471615720524, "no_speech_prob": 1.28052597574424e-05}, {"id": 1377, "seek": 711520, "start": 7115.2, "end": 7120.92, "text": " They have lots of replicas and the whole thing is a bit of a mess.", "tokens": [814, 362, 3195, 295, 3248, 9150, 293, 264, 1379, 551, 307, 257, 857, 295, 257, 2082, 13], "temperature": 0.0, "avg_logprob": -0.2163808947411653, "compression_ratio": 1.7054263565891472, "no_speech_prob": 9.818075341172516e-06}, {"id": 1378, "seek": 711520, "start": 7120.92, "end": 7126.12, "text": " Nonetheless, this is an interesting approach, so if you're interested in memory networks,", "tokens": [45437, 11, 341, 307, 364, 1880, 3109, 11, 370, 498, 291, 434, 3102, 294, 4675, 9590, 11], "temperature": 0.0, "avg_logprob": -0.2163808947411653, "compression_ratio": 1.7054263565891472, "no_speech_prob": 9.818075341172516e-06}, {"id": 1379, "seek": 711520, "start": 7126.12, "end": 7129.36, "text": " this is certainly something you can look at.", "tokens": [341, 307, 3297, 746, 291, 393, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.2163808947411653, "compression_ratio": 1.7054263565891472, "no_speech_prob": 9.818075341172516e-06}, {"id": 1380, "seek": 711520, "start": 7129.36, "end": 7134.4, "text": " And I do think this is likely to be an important direction.", "tokens": [400, 286, 360, 519, 341, 307, 3700, 281, 312, 364, 1021, 3513, 13], "temperature": 0.0, "avg_logprob": -0.2163808947411653, "compression_ratio": 1.7054263565891472, "no_speech_prob": 9.818075341172516e-06}, {"id": 1381, "seek": 711520, "start": 7134.4, "end": 7138.96, "text": " Having said all that, one of the key reasons I wanted to look at these memory networks", "tokens": [10222, 848, 439, 300, 11, 472, 295, 264, 2141, 4112, 286, 1415, 281, 574, 412, 613, 4675, 9590], "temperature": 0.0, "avg_logprob": -0.2163808947411653, "compression_ratio": 1.7054263565891472, "no_speech_prob": 9.818075341172516e-06}, {"id": 1382, "seek": 711520, "start": 7138.96, "end": 7142.92, "text": " is not only because it was the largest request I think from the forums for this part of the", "tokens": [307, 406, 787, 570, 309, 390, 264, 6443, 5308, 286, 519, 490, 264, 26998, 337, 341, 644, 295, 264], "temperature": 0.0, "avg_logprob": -0.2163808947411653, "compression_ratio": 1.7054263565891472, "no_speech_prob": 9.818075341172516e-06}, {"id": 1383, "seek": 714292, "start": 7142.92, "end": 7149.96, "text": " course, but also because it introduces something that's going to be critical for the next couple", "tokens": [1164, 11, 457, 611, 570, 309, 31472, 746, 300, 311, 516, 281, 312, 4924, 337, 264, 958, 1916], "temperature": 0.0, "avg_logprob": -0.14176159434848362, "compression_ratio": 1.5064935064935066, "no_speech_prob": 6.240843504201621e-06}, {"id": 1384, "seek": 714292, "start": 7149.96, "end": 7157.88, "text": " of lessons, which is the concept of attention.", "tokens": [295, 8820, 11, 597, 307, 264, 3410, 295, 3202, 13], "temperature": 0.0, "avg_logprob": -0.14176159434848362, "compression_ratio": 1.5064935064935066, "no_speech_prob": 6.240843504201621e-06}, {"id": 1385, "seek": 714292, "start": 7157.88, "end": 7171.4, "text": " Attention or models are models where we have to do exactly what we just looked at, which", "tokens": [31858, 420, 5245, 366, 5245, 689, 321, 362, 281, 360, 2293, 437, 321, 445, 2956, 412, 11, 597], "temperature": 0.0, "avg_logprob": -0.14176159434848362, "compression_ratio": 1.5064935064935066, "no_speech_prob": 6.240843504201621e-06}, {"id": 1386, "seek": 717140, "start": 7171.4, "end": 7181.799999999999, "text": " is basically find out at each time which part of a story to look at next, or which part", "tokens": [307, 1936, 915, 484, 412, 1184, 565, 597, 644, 295, 257, 1657, 281, 574, 412, 958, 11, 420, 597, 644], "temperature": 0.0, "avg_logprob": -0.08266989390055339, "compression_ratio": 1.801418439716312, "no_speech_prob": 4.3138820160493196e-07}, {"id": 1387, "seek": 717140, "start": 7181.799999999999, "end": 7187.679999999999, "text": " of an image to look at next, or which part of a sentence to look at next.", "tokens": [295, 364, 3256, 281, 574, 412, 958, 11, 420, 597, 644, 295, 257, 8174, 281, 574, 412, 958, 13], "temperature": 0.0, "avg_logprob": -0.08266989390055339, "compression_ratio": 1.801418439716312, "no_speech_prob": 4.3138820160493196e-07}, {"id": 1388, "seek": 717140, "start": 7187.679999999999, "end": 7193.879999999999, "text": " And so the task that we're going to be trying to get at over the next lesson or two is going", "tokens": [400, 370, 264, 5633, 300, 321, 434, 516, 281, 312, 1382, 281, 483, 412, 670, 264, 958, 6898, 420, 732, 307, 516], "temperature": 0.0, "avg_logprob": -0.08266989390055339, "compression_ratio": 1.801418439716312, "no_speech_prob": 4.3138820160493196e-07}, {"id": 1389, "seek": 719388, "start": 7193.88, "end": 7202.2, "text": " to be to translate French into English.", "tokens": [281, 312, 281, 13799, 5522, 666, 3669, 13], "temperature": 0.0, "avg_logprob": -0.11616679755124179, "compression_ratio": 1.766355140186916, "no_speech_prob": 4.029394858662272e-06}, {"id": 1390, "seek": 719388, "start": 7202.2, "end": 7207.6, "text": " So this is clearly not a toy task, this is a very challenging task.", "tokens": [407, 341, 307, 4448, 406, 257, 12058, 5633, 11, 341, 307, 257, 588, 7595, 5633, 13], "temperature": 0.0, "avg_logprob": -0.11616679755124179, "compression_ratio": 1.766355140186916, "no_speech_prob": 4.029394858662272e-06}, {"id": 1391, "seek": 719388, "start": 7207.6, "end": 7212.88, "text": " And one of the challenges is that in a particular French sentence which has got some bunch of", "tokens": [400, 472, 295, 264, 4759, 307, 300, 294, 257, 1729, 5522, 8174, 597, 575, 658, 512, 3840, 295], "temperature": 0.0, "avg_logprob": -0.11616679755124179, "compression_ratio": 1.766355140186916, "no_speech_prob": 4.029394858662272e-06}, {"id": 1392, "seek": 719388, "start": 7212.88, "end": 7218.12, "text": " words, it's likely to turn into an English sentence with some different bunch of words.", "tokens": [2283, 11, 309, 311, 3700, 281, 1261, 666, 364, 3669, 8174, 365, 512, 819, 3840, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.11616679755124179, "compression_ratio": 1.766355140186916, "no_speech_prob": 4.029394858662272e-06}, {"id": 1393, "seek": 719388, "start": 7218.12, "end": 7223.04, "text": " And maybe these particular words here might be this translation here, and this one might", "tokens": [400, 1310, 613, 1729, 2283, 510, 1062, 312, 341, 12853, 510, 11, 293, 341, 472, 1062], "temperature": 0.0, "avg_logprob": -0.11616679755124179, "compression_ratio": 1.766355140186916, "no_speech_prob": 4.029394858662272e-06}, {"id": 1394, "seek": 722304, "start": 7223.04, "end": 7225.2, "text": " be this one, and this one might be this one.", "tokens": [312, 341, 472, 11, 293, 341, 472, 1062, 312, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.13401741754441035, "compression_ratio": 1.6587677725118484, "no_speech_prob": 1.520647947472753e-05}, {"id": 1395, "seek": 722304, "start": 7225.2, "end": 7231.96, "text": " And so as you go through, you need some way of saying which word do I look at next.", "tokens": [400, 370, 382, 291, 352, 807, 11, 291, 643, 512, 636, 295, 1566, 597, 1349, 360, 286, 574, 412, 958, 13], "temperature": 0.0, "avg_logprob": -0.13401741754441035, "compression_ratio": 1.6587677725118484, "no_speech_prob": 1.520647947472753e-05}, {"id": 1396, "seek": 722304, "start": 7231.96, "end": 7236.16, "text": " So that's going to be the attentional model.", "tokens": [407, 300, 311, 516, 281, 312, 264, 3202, 304, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13401741754441035, "compression_ratio": 1.6587677725118484, "no_speech_prob": 1.520647947472753e-05}, {"id": 1397, "seek": 722304, "start": 7236.16, "end": 7243.5199999999995, "text": " And so what we're going to do is we're going to be trying to come up with a proper RNN,", "tokens": [400, 370, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 312, 1382, 281, 808, 493, 365, 257, 2296, 45702, 45, 11], "temperature": 0.0, "avg_logprob": -0.13401741754441035, "compression_ratio": 1.6587677725118484, "no_speech_prob": 1.520647947472753e-05}, {"id": 1398, "seek": 722304, "start": 7243.5199999999995, "end": 7252.76, "text": " like an LSTM or a GRU or whatever, where we're going to change it so that inside the RNN", "tokens": [411, 364, 441, 6840, 44, 420, 257, 10903, 52, 420, 2035, 11, 689, 321, 434, 516, 281, 1319, 309, 370, 300, 1854, 264, 45702, 45], "temperature": 0.0, "avg_logprob": -0.13401741754441035, "compression_ratio": 1.6587677725118484, "no_speech_prob": 1.520647947472753e-05}, {"id": 1399, "seek": 725276, "start": 7252.76, "end": 7263.08, "text": " it's going to actually have some way of figuring out which part of the input to look at next.", "tokens": [309, 311, 516, 281, 767, 362, 512, 636, 295, 15213, 484, 597, 644, 295, 264, 4846, 281, 574, 412, 958, 13], "temperature": 0.0, "avg_logprob": -0.14180430850467166, "compression_ratio": 1.5631067961165048, "no_speech_prob": 8.139557394315489e-06}, {"id": 1400, "seek": 725276, "start": 7263.08, "end": 7266.8, "text": " So that's the basic idea of attentional models.", "tokens": [407, 300, 311, 264, 3875, 1558, 295, 3202, 304, 5245, 13], "temperature": 0.0, "avg_logprob": -0.14180430850467166, "compression_ratio": 1.5631067961165048, "no_speech_prob": 8.139557394315489e-06}, {"id": 1401, "seek": 725276, "start": 7266.8, "end": 7273.860000000001, "text": " And so interestingly, during this time that memory networks and neural-turing machines", "tokens": [400, 370, 25873, 11, 1830, 341, 565, 300, 4675, 9590, 293, 18161, 12, 83, 1345, 8379], "temperature": 0.0, "avg_logprob": -0.14180430850467166, "compression_ratio": 1.5631067961165048, "no_speech_prob": 8.139557394315489e-06}, {"id": 1402, "seek": 725276, "start": 7273.860000000001, "end": 7280.92, "text": " and stuff were getting all this huge amount of press attention very quietly in the background", "tokens": [293, 1507, 645, 1242, 439, 341, 2603, 2372, 295, 1886, 3202, 588, 19141, 294, 264, 3678], "temperature": 0.0, "avg_logprob": -0.14180430850467166, "compression_ratio": 1.5631067961165048, "no_speech_prob": 8.139557394315489e-06}, {"id": 1403, "seek": 728092, "start": 7280.92, "end": 7287.52, "text": " at exactly the same time, attentional models were appearing as well.", "tokens": [412, 2293, 264, 912, 565, 11, 3202, 304, 5245, 645, 19870, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.13000638720015406, "compression_ratio": 1.549222797927461, "no_speech_prob": 4.02943987865001e-06}, {"id": 1404, "seek": 728092, "start": 7287.52, "end": 7294.84, "text": " And it's the attentional models for language that have really turned out to be critical.", "tokens": [400, 309, 311, 264, 3202, 304, 5245, 337, 2856, 300, 362, 534, 3574, 484, 281, 312, 4924, 13], "temperature": 0.0, "avg_logprob": -0.13000638720015406, "compression_ratio": 1.549222797927461, "no_speech_prob": 4.02943987865001e-06}, {"id": 1405, "seek": 728092, "start": 7294.84, "end": 7302.8, "text": " So you've probably seen all of the press about Google's new neural translation system, and", "tokens": [407, 291, 600, 1391, 1612, 439, 295, 264, 1886, 466, 3329, 311, 777, 18161, 12853, 1185, 11, 293], "temperature": 0.0, "avg_logprob": -0.13000638720015406, "compression_ratio": 1.549222797927461, "no_speech_prob": 4.02943987865001e-06}, {"id": 1406, "seek": 728092, "start": 7302.8, "end": 7306.2, "text": " that really is everything that it's claimed to be.", "tokens": [300, 534, 307, 1203, 300, 309, 311, 12941, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.13000638720015406, "compression_ratio": 1.549222797927461, "no_speech_prob": 4.02943987865001e-06}, {"id": 1407, "seek": 730620, "start": 7306.2, "end": 7314.92, "text": " It really is basically one giant neural network that can translate any pair of languages.", "tokens": [467, 534, 307, 1936, 472, 7410, 18161, 3209, 300, 393, 13799, 604, 6119, 295, 8650, 13], "temperature": 0.0, "avg_logprob": -0.15817418552580334, "compression_ratio": 1.6126126126126126, "no_speech_prob": 1.3496977544491529e-06}, {"id": 1408, "seek": 730620, "start": 7314.92, "end": 7320.599999999999, "text": " The accuracy of those translations is far beyond anything that's happened before.", "tokens": [440, 14170, 295, 729, 37578, 307, 1400, 4399, 1340, 300, 311, 2011, 949, 13], "temperature": 0.0, "avg_logprob": -0.15817418552580334, "compression_ratio": 1.6126126126126126, "no_speech_prob": 1.3496977544491529e-06}, {"id": 1409, "seek": 730620, "start": 7320.599999999999, "end": 7329.8, "text": " And the basic structure of that neural net, as we're going to learn, is not that different", "tokens": [400, 264, 3875, 3877, 295, 300, 18161, 2533, 11, 382, 321, 434, 516, 281, 1466, 11, 307, 406, 300, 819], "temperature": 0.0, "avg_logprob": -0.15817418552580334, "compression_ratio": 1.6126126126126126, "no_speech_prob": 1.3496977544491529e-06}, {"id": 1410, "seek": 730620, "start": 7329.8, "end": 7331.08, "text": " to what we've already learned.", "tokens": [281, 437, 321, 600, 1217, 3264, 13], "temperature": 0.0, "avg_logprob": -0.15817418552580334, "compression_ratio": 1.6126126126126126, "no_speech_prob": 1.3496977544491529e-06}, {"id": 1411, "seek": 730620, "start": 7331.08, "end": 7335.84, "text": " It's just going to have this one extra step, which is attention.", "tokens": [467, 311, 445, 516, 281, 362, 341, 472, 2857, 1823, 11, 597, 307, 3202, 13], "temperature": 0.0, "avg_logprob": -0.15817418552580334, "compression_ratio": 1.6126126126126126, "no_speech_prob": 1.3496977544491529e-06}, {"id": 1412, "seek": 733584, "start": 7335.84, "end": 7342.04, "text": " And kind of depending on how interested you guys are on the details of this neural translation", "tokens": [400, 733, 295, 5413, 322, 577, 3102, 291, 1074, 366, 322, 264, 4365, 295, 341, 18161, 12853], "temperature": 0.0, "avg_logprob": -0.18556260526849983, "compression_ratio": 1.6519823788546255, "no_speech_prob": 1.5936448107822798e-05}, {"id": 1413, "seek": 733584, "start": 7342.04, "end": 7346.4400000000005, "text": " system, it turns out that there are also lots of little tweaks.", "tokens": [1185, 11, 309, 4523, 484, 300, 456, 366, 611, 3195, 295, 707, 46664, 13], "temperature": 0.0, "avg_logprob": -0.18556260526849983, "compression_ratio": 1.6519823788546255, "no_speech_prob": 1.5936448107822798e-05}, {"id": 1414, "seek": 733584, "start": 7346.4400000000005, "end": 7353.64, "text": " The tweaks are kind of around like, okay, you've got a really big vocabulary, some of", "tokens": [440, 46664, 366, 733, 295, 926, 411, 11, 1392, 11, 291, 600, 658, 257, 534, 955, 19864, 11, 512, 295], "temperature": 0.0, "avg_logprob": -0.18556260526849983, "compression_ratio": 1.6519823788546255, "no_speech_prob": 1.5936448107822798e-05}, {"id": 1415, "seek": 733584, "start": 7353.64, "end": 7359.96, "text": " the words appear very rarely, how do you build a system that can understand how to translate", "tokens": [264, 2283, 4204, 588, 13752, 11, 577, 360, 291, 1322, 257, 1185, 300, 393, 1223, 577, 281, 13799], "temperature": 0.0, "avg_logprob": -0.18556260526849983, "compression_ratio": 1.6519823788546255, "no_speech_prob": 1.5936448107822798e-05}, {"id": 1416, "seek": 733584, "start": 7359.96, "end": 7364.28, "text": " those really rare words, for example.", "tokens": [729, 534, 5892, 2283, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.18556260526849983, "compression_ratio": 1.6519823788546255, "no_speech_prob": 1.5936448107822798e-05}, {"id": 1417, "seek": 736428, "start": 7364.28, "end": 7368.88, "text": " And also just kind of things like how do you deal with the memory issues around having", "tokens": [400, 611, 445, 733, 295, 721, 411, 577, 360, 291, 2028, 365, 264, 4675, 2663, 926, 1419], "temperature": 0.0, "avg_logprob": -0.22143738528332077, "compression_ratio": 1.5619469026548674, "no_speech_prob": 7.766872840875294e-06}, {"id": 1418, "seek": 736428, "start": 7368.88, "end": 7374.759999999999, "text": " huge embedding matrices of 160,000 words and stuff like that.", "tokens": [2603, 12240, 3584, 32284, 295, 21243, 11, 1360, 2283, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.22143738528332077, "compression_ratio": 1.5619469026548674, "no_speech_prob": 7.766872840875294e-06}, {"id": 1419, "seek": 736428, "start": 7374.759999999999, "end": 7381.96, "text": " So there's lots of details, and the nice thing is that because Google has ended up putting", "tokens": [407, 456, 311, 3195, 295, 4365, 11, 293, 264, 1481, 551, 307, 300, 570, 3329, 575, 4590, 493, 3372], "temperature": 0.0, "avg_logprob": -0.22143738528332077, "compression_ratio": 1.5619469026548674, "no_speech_prob": 7.766872840875294e-06}, {"id": 1420, "seek": 736428, "start": 7381.96, "end": 7388.36, "text": " this thing in production, all of these little details have answers now.", "tokens": [341, 551, 294, 4265, 11, 439, 295, 613, 707, 4365, 362, 6338, 586, 13], "temperature": 0.0, "avg_logprob": -0.22143738528332077, "compression_ratio": 1.5619469026548674, "no_speech_prob": 7.766872840875294e-06}, {"id": 1421, "seek": 736428, "start": 7388.36, "end": 7391.32, "text": " Those answers are all really interesting.", "tokens": [3950, 6338, 366, 439, 534, 1880, 13], "temperature": 0.0, "avg_logprob": -0.22143738528332077, "compression_ratio": 1.5619469026548674, "no_speech_prob": 7.766872840875294e-06}, {"id": 1422, "seek": 739132, "start": 7391.32, "end": 7399.36, "text": " There aren't really, on the whole, great examples of all of those things put together.", "tokens": [821, 3212, 380, 534, 11, 322, 264, 1379, 11, 869, 5110, 295, 439, 295, 729, 721, 829, 1214, 13], "temperature": 0.0, "avg_logprob": -0.20756131952459161, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.4112026595685165e-06}, {"id": 1423, "seek": 739132, "start": 7399.36, "end": 7405.92, "text": " So one of the things interesting here will be that you'll have opportunities to do that.", "tokens": [407, 472, 295, 264, 721, 1880, 510, 486, 312, 300, 291, 603, 362, 4786, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.20756131952459161, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.4112026595685165e-06}, {"id": 1424, "seek": 739132, "start": 7405.92, "end": 7410.719999999999, "text": " Generally speaking, the blog posts about these neural translation systems tend to be at a", "tokens": [21082, 4124, 11, 264, 6968, 12300, 466, 613, 18161, 12853, 3652, 3928, 281, 312, 412, 257], "temperature": 0.0, "avg_logprob": -0.20756131952459161, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.4112026595685165e-06}, {"id": 1425, "seek": 739132, "start": 7410.719999999999, "end": 7411.719999999999, "text": " pretty high level.", "tokens": [1238, 1090, 1496, 13], "temperature": 0.0, "avg_logprob": -0.20756131952459161, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.4112026595685165e-06}, {"id": 1426, "seek": 739132, "start": 7411.719999999999, "end": 7415.36, "text": " They describe roughly how these approaches work.", "tokens": [814, 6786, 9810, 577, 613, 11587, 589, 13], "temperature": 0.0, "avg_logprob": -0.20756131952459161, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.4112026595685165e-06}, {"id": 1427, "seek": 741536, "start": 7415.36, "end": 7422.04, "text": " But Google's complete neural translation system is not out there, you can't download it and", "tokens": [583, 3329, 311, 3566, 18161, 12853, 1185, 307, 406, 484, 456, 11, 291, 393, 380, 5484, 309, 293], "temperature": 0.0, "avg_logprob": -0.22325710374481825, "compression_ratio": 1.330708661417323, "no_speech_prob": 9.66602692642482e-06}, {"id": 1428, "seek": 741536, "start": 7422.04, "end": 7424.24, "text": " see the code.", "tokens": [536, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.22325710374481825, "compression_ratio": 1.330708661417323, "no_speech_prob": 9.66602692642482e-06}, {"id": 1429, "seek": 741536, "start": 7424.24, "end": 7441.12, "text": " So we'll see how we go, but we'll kind of do it piece by piece.", "tokens": [407, 321, 603, 536, 577, 321, 352, 11, 457, 321, 603, 733, 295, 360, 309, 2522, 538, 2522, 13], "temperature": 0.0, "avg_logprob": -0.22325710374481825, "compression_ratio": 1.330708661417323, "no_speech_prob": 9.66602692642482e-06}, {"id": 1430, "seek": 744112, "start": 7441.12, "end": 7449.08, "text": " I guess one other thing to mention about the memory network is that Keras actually comes", "tokens": [286, 2041, 472, 661, 551, 281, 2152, 466, 264, 4675, 3209, 307, 300, 591, 6985, 767, 1487], "temperature": 0.0, "avg_logprob": -0.13808223347605011, "compression_ratio": 1.5809523809523809, "no_speech_prob": 1.3631070032715797e-05}, {"id": 1431, "seek": 744112, "start": 7449.08, "end": 7458.92, "text": " with an end-to-end memory network example in the Keras GitHub, which weirdly enough,", "tokens": [365, 364, 917, 12, 1353, 12, 521, 4675, 3209, 1365, 294, 264, 591, 6985, 23331, 11, 597, 48931, 1547, 11], "temperature": 0.0, "avg_logprob": -0.13808223347605011, "compression_ratio": 1.5809523809523809, "no_speech_prob": 1.3631070032715797e-05}, {"id": 1432, "seek": 744112, "start": 7458.92, "end": 7464.8, "text": " when I actually looked at it, it turns out doesn't implement this at all.", "tokens": [562, 286, 767, 2956, 412, 309, 11, 309, 4523, 484, 1177, 380, 4445, 341, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.13808223347605011, "compression_ratio": 1.5809523809523809, "no_speech_prob": 1.3631070032715797e-05}, {"id": 1433, "seek": 744112, "start": 7464.8, "end": 7470.64, "text": " And so even on the single supporting fact thing, it takes many, many generations and", "tokens": [400, 370, 754, 322, 264, 2167, 7231, 1186, 551, 11, 309, 2516, 867, 11, 867, 10593, 293], "temperature": 0.0, "avg_logprob": -0.13808223347605011, "compression_ratio": 1.5809523809523809, "no_speech_prob": 1.3631070032715797e-05}, {"id": 1434, "seek": 747064, "start": 7470.64, "end": 7474.160000000001, "text": " doesn't get to 100% accuracy.", "tokens": [1177, 380, 483, 281, 2319, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.16805075407028197, "compression_ratio": 1.542857142857143, "no_speech_prob": 9.818229045777116e-06}, {"id": 1435, "seek": 747064, "start": 7474.160000000001, "end": 7479.0, "text": " And I found this quite surprising to discover that once you start getting to some of these", "tokens": [400, 286, 1352, 341, 1596, 8830, 281, 4411, 300, 1564, 291, 722, 1242, 281, 512, 295, 613], "temperature": 0.0, "avg_logprob": -0.16805075407028197, "compression_ratio": 1.542857142857143, "no_speech_prob": 9.818229045777116e-06}, {"id": 1436, "seek": 747064, "start": 7479.0, "end": 7489.72, "text": " more recent advances, or not just a standard CNN or whatever, it's just less and less common", "tokens": [544, 5162, 25297, 11, 420, 406, 445, 257, 3832, 24859, 420, 2035, 11, 309, 311, 445, 1570, 293, 1570, 2689], "temperature": 0.0, "avg_logprob": -0.16805075407028197, "compression_ratio": 1.542857142857143, "no_speech_prob": 9.818229045777116e-06}, {"id": 1437, "seek": 747064, "start": 7489.72, "end": 7493.68, "text": " that you actually find code that's correct and that works.", "tokens": [300, 291, 767, 915, 3089, 300, 311, 3006, 293, 300, 1985, 13], "temperature": 0.0, "avg_logprob": -0.16805075407028197, "compression_ratio": 1.542857142857143, "no_speech_prob": 9.818229045777116e-06}, {"id": 1438, "seek": 747064, "start": 7493.68, "end": 7496.4800000000005, "text": " And so this memory network example was one of them.", "tokens": [400, 370, 341, 4675, 3209, 1365, 390, 472, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.16805075407028197, "compression_ratio": 1.542857142857143, "no_speech_prob": 9.818229045777116e-06}, {"id": 1439, "seek": 749648, "start": 7496.48, "end": 7501.44, "text": " So if you actually go into the Keras GitHub and look at examples, and go and have a look", "tokens": [407, 498, 291, 767, 352, 666, 264, 591, 6985, 23331, 293, 574, 412, 5110, 11, 293, 352, 293, 362, 257, 574], "temperature": 0.0, "avg_logprob": -0.18913189002445766, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.4593734931622748e-06}, {"id": 1440, "seek": 749648, "start": 7501.44, "end": 7506.5199999999995, "text": " and download the memory network, you'll find that you don't get results anything like this.", "tokens": [293, 5484, 264, 4675, 3209, 11, 291, 603, 915, 300, 291, 500, 380, 483, 3542, 1340, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.18913189002445766, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.4593734931622748e-06}, {"id": 1441, "seek": 749648, "start": 7506.5199999999995, "end": 7511.28, "text": " If you look at the code, you'll see that it really doesn't do this at all.", "tokens": [759, 291, 574, 412, 264, 3089, 11, 291, 603, 536, 300, 309, 534, 1177, 380, 360, 341, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.18913189002445766, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.4593734931622748e-06}, {"id": 1442, "seek": 749648, "start": 7511.28, "end": 7518.4, "text": " So I just kind of wanted to mention that as a bit of a warning that you're kind of at", "tokens": [407, 286, 445, 733, 295, 1415, 281, 2152, 300, 382, 257, 857, 295, 257, 9164, 300, 291, 434, 733, 295, 412], "temperature": 0.0, "avg_logprob": -0.18913189002445766, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.4593734931622748e-06}, {"id": 1443, "seek": 749648, "start": 7518.4, "end": 7524.0, "text": " the point now where you might want to take with a grain of salt blog posts you read,", "tokens": [264, 935, 586, 689, 291, 1062, 528, 281, 747, 365, 257, 12837, 295, 5139, 6968, 12300, 291, 1401, 11], "temperature": 0.0, "avg_logprob": -0.18913189002445766, "compression_ratio": 1.6904761904761905, "no_speech_prob": 1.4593734931622748e-06}, {"id": 1444, "seek": 752400, "start": 7524.0, "end": 7530.0, "text": " or even some papers that you read, well worth experimenting with them and assuming you should", "tokens": [420, 754, 512, 10577, 300, 291, 1401, 11, 731, 3163, 29070, 365, 552, 293, 11926, 291, 820], "temperature": 0.0, "avg_logprob": -0.13753093920255963, "compression_ratio": 1.9272727272727272, "no_speech_prob": 1.3211625628173351e-05}, {"id": 1445, "seek": 752400, "start": 7530.0, "end": 7533.4, "text": " start with the assumption that you can do it better.", "tokens": [722, 365, 264, 15302, 300, 291, 393, 360, 309, 1101, 13], "temperature": 0.0, "avg_logprob": -0.13753093920255963, "compression_ratio": 1.9272727272727272, "no_speech_prob": 1.3211625628173351e-05}, {"id": 1446, "seek": 752400, "start": 7533.4, "end": 7540.52, "text": " And maybe even start with the assumption that you can't necessarily trust all of the conclusions", "tokens": [400, 1310, 754, 722, 365, 264, 15302, 300, 291, 393, 380, 4725, 3361, 439, 295, 264, 22865], "temperature": 0.0, "avg_logprob": -0.13753093920255963, "compression_ratio": 1.9272727272727272, "no_speech_prob": 1.3211625628173351e-05}, {"id": 1447, "seek": 752400, "start": 7540.52, "end": 7546.4, "text": " that you've read, because the vast majority of the time, in my experience putting together", "tokens": [300, 291, 600, 1401, 11, 570, 264, 8369, 6286, 295, 264, 565, 11, 294, 452, 1752, 3372, 1214], "temperature": 0.0, "avg_logprob": -0.13753093920255963, "compression_ratio": 1.9272727272727272, "no_speech_prob": 1.3211625628173351e-05}, {"id": 1448, "seek": 752400, "start": 7546.4, "end": 7552.56, "text": " this part of the course, the vast majority of the time the stuff out there is just wrong.", "tokens": [341, 644, 295, 264, 1164, 11, 264, 8369, 6286, 295, 264, 565, 264, 1507, 484, 456, 307, 445, 2085, 13], "temperature": 0.0, "avg_logprob": -0.13753093920255963, "compression_ratio": 1.9272727272727272, "no_speech_prob": 1.3211625628173351e-05}, {"id": 1449, "seek": 755256, "start": 7552.56, "end": 7558.52, "text": " And in cases like, I deeply respect the Keras authors and the Keras source code, but even", "tokens": [400, 294, 3331, 411, 11, 286, 8760, 3104, 264, 591, 6985, 16552, 293, 264, 591, 6985, 4009, 3089, 11, 457, 754], "temperature": 0.0, "avg_logprob": -0.30458881236888746, "compression_ratio": 1.5265957446808511, "no_speech_prob": 3.024128636752721e-05}, {"id": 1450, "seek": 755256, "start": 7558.52, "end": 7562.360000000001, "text": " in that case, this is wrong.", "tokens": [294, 300, 1389, 11, 341, 307, 2085, 13], "temperature": 0.0, "avg_logprob": -0.30458881236888746, "compression_ratio": 1.5265957446808511, "no_speech_prob": 3.024128636752721e-05}, {"id": 1451, "seek": 755256, "start": 7562.360000000001, "end": 7569.0, "text": " I think that's an important point to be aware of.", "tokens": [286, 519, 300, 311, 364, 1021, 935, 281, 312, 3650, 295, 13], "temperature": 0.0, "avg_logprob": -0.30458881236888746, "compression_ratio": 1.5265957446808511, "no_speech_prob": 3.024128636752721e-05}, {"id": 1452, "seek": 755256, "start": 7569.0, "end": 7572.160000000001, "text": " I think we're going to finish 5 minutes early for a change.", "tokens": [286, 519, 321, 434, 516, 281, 2413, 1025, 2077, 2440, 337, 257, 1319, 13], "temperature": 0.0, "avg_logprob": -0.30458881236888746, "compression_ratio": 1.5265957446808511, "no_speech_prob": 3.024128636752721e-05}, {"id": 1453, "seek": 755256, "start": 7572.160000000001, "end": 7574.080000000001, "text": " I think that's never happened before.", "tokens": [286, 519, 300, 311, 1128, 2011, 949, 13], "temperature": 0.0, "avg_logprob": -0.30458881236888746, "compression_ratio": 1.5265957446808511, "no_speech_prob": 3.024128636752721e-05}, {"id": 1454, "seek": 755256, "start": 7574.080000000001, "end": 7575.64, "text": " So thanks everybody.", "tokens": [407, 3231, 2201, 13], "temperature": 0.0, "avg_logprob": -0.30458881236888746, "compression_ratio": 1.5265957446808511, "no_speech_prob": 3.024128636752721e-05}, {"id": 1455, "seek": 757564, "start": 7575.64, "end": 7584.64, "text": " So this week, hopefully we can have a look at the data science bowl, make a million dollars,", "tokens": [407, 341, 1243, 11, 4696, 321, 393, 362, 257, 574, 412, 264, 1412, 3497, 6571, 11, 652, 257, 2459, 3808, 11], "temperature": 0.0, "avg_logprob": -0.25618306144339137, "compression_ratio": 1.4022346368715084, "no_speech_prob": 6.81314486428164e-05}, {"id": 1456, "seek": 757564, "start": 7584.64, "end": 7589.4800000000005, "text": " create a new PyTorch approximate nearest neighbors algorithm, and then when you're done maybe", "tokens": [1884, 257, 777, 9953, 51, 284, 339, 30874, 23831, 12512, 9284, 11, 293, 550, 562, 291, 434, 1096, 1310], "temperature": 0.0, "avg_logprob": -0.25618306144339137, "compression_ratio": 1.4022346368715084, "no_speech_prob": 6.81314486428164e-05}, {"id": 1457, "seek": 757564, "start": 7589.4800000000005, "end": 7592.0, "text": " figure out the next stage for memory networks.", "tokens": [2573, 484, 264, 958, 3233, 337, 4675, 9590, 13], "temperature": 0.0, "avg_logprob": -0.25618306144339137, "compression_ratio": 1.4022346368715084, "no_speech_prob": 6.81314486428164e-05}, {"id": 1458, "seek": 759200, "start": 7592.0, "end": 7606.2, "text": " Thanks everybody.", "tokens": [50364, 2561, 2201, 13, 51074], "temperature": 0.0, "avg_logprob": -0.9896775881449381, "compression_ratio": 0.68, "no_speech_prob": 0.000649972993414849}], "language": "en"}