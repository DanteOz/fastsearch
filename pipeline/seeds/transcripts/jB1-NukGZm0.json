{"text": " Okay. So, yeah, this is our first time in the Bay Area, so it's nice to meet you all, and thanks for coming. I'm, you know, not so much noticed. So I'll start by just giving a quick introduction of us and, you know, some of the things that we're doing before I start with the sort of main content of the talk, which is about this open-source library that we developed, Spacey, for natural language processing. So the other things that we develop as well at Explosion AI is a machine learning library behind Spacey Think, which allows us to avoid depending on other libraries and kind of keep control of everything and make sure that everything's easy to install. We also have an annotation tool that we develop alongside Spacey Prodigy, which is what Innis will be talking about. And we're also preparing a data store of other pre-trained models for more specific languages and use cases and things that people will be able to use that basically will extend the capabilities of the software for more specific use cases. So to give you a quick introduction to Innis and I, which is basically all of Explosion AI. So I've been working on natural language processing for pretty much my whole career. I started doing this after doing a PhD in computer science. I started off in linguistics and then kind of moved across to computational linguistics. And then around 2014, I saw that these technologies were getting increasingly viable. And I was also at the point in my career where I was supposed to start writing grant proposals, which didn't really agree with me. So I decided to leave and I saw that there was a gap in the capabilities available for something that actually translated the research systems to something that was more sort of practically focused. And then, you know, soon after I moved to Berlin to do this, I met Innis and we've been working together since on these things. And I think, you know, we kind of have a nice complementarity of things and she is the lead developer of our annotation tool, Prodigy, and has also been working on Spacey pretty much since the first release. Okay, so I included this slide, which we normally actually give this when we talk to companies specifically, but I think that it's a good thing to include, to give you a bit of, you know, this is what we tell people about what we do and how we make money and how the company works. And I think that this is a very valid question that people would have about an open source library. It's like, well, why are you doing this? And, you know, how does it fit into the rest of your projects and plans? So the Explain it like I'm 5 version, which I guess is also the Explain it like I'm senior management version, is we give an analogy. It's kind of like a boutique kitchen. So the free recipes we publish online, you can see is kind of like the open source software. So that's Spacey, Think, et cetera. At the start of the company, especially, we were doing consulting, which I'm happy to say we've been able to wind down over the last six months and focus on our products. And then we also focus on a line of kitchen gadgets, which is things like Prodigy. These are these downloadable tools to use alongside the open source software. And soon we'll have this sort of premium ingredients, which are the pre-trained models. So the thing that we don't do here is enterprise support, which I guess is probably the most common way that people, you know, fund open source software or imagine that they'll fund open source software with a business model. And we really don't like this because we want our software to be as easy to use as possible and as transparent as possible and the documentation to be good. So I think it's kind of weird to have this thing where you have explicitly a plan that we're going to make our free stuff as good as possible, and then we're going to have this service that we hope nobody... We hope people pay us lots of money for, but we hope nobody uses. And that's kind of weird, right? It's kind of weird to have a company that, you know, you hope that your paid offering is really poor value to people. And so we don't think that that's a good way to do it. And so instead we have the downloadable tools, I think is a good way to, you know, we have something which works alongside spaCy and I think is useful to people who use spaCy as well. OK, so, you know, onto the sort of main content of the talk and, you know, the bit that I'll be talking about. So I'm going to talk to you about the syntactic parser within spaCy, the natural language processing library that we use. And so before I do it... So this is kind of what it looks like as, you know, sort of visualised as an output. So it's this sort of tree-based structure that gives you these syntactic relationships between words. So the way that you should read this here is that the arrow pointing from this word to this word means that Apple is a child of looking in the tree. And it's a child with this relationship and such. In other words, Apple is the subject of looking. And is is an auxiliary verb attached to looking. And then at is a prepositional phrase attached to looking. So these sorts of relationships tell you about the syntactic structure of the sentence and basically help you get at the who did what to whom sort of relationships in the sentence and also to extract phrases and things. So, for instance, here, to make the thing more easy to read, we've merged UK Startup, which is, you know, a sort of basic noun phrase into one unit. And you can find these sorts of phrases more easily from, given the syntactic structure. And just above here, we've got an example of, you know, what the code looks like to actually get the syntactic structure and navigate the tree. In spaCy, you just get this NLP object after loading the models. And you just use that as a function that you feed text or pipe text through if you've got a sequence of texts. And given that, you get a document object, which you can just use as an iterable. And from the tokens, you get attributes that you can use to navigate the tree. So, for instance, here, the dependency relationship is just a.dep. By default, that's an integer key, integer ID, because everything's kind of coded to an integer for easy and efficient processing. But then you can get the text value with an underscore as well. And then you can navigate up the tree with.head. And then you can look at the left and right children of the tree as well. So we try to have a root API that makes it easy to use these dependency relationships. You know, so that just getting dependency parses, you know, obviously, just the first step, you want to actually use it in some way. And that's why we have this API to make that easy. So the question that always comes up with this, and I think this is a very interesting thing for the field in general, is, you know, what's the point of parsing? What is this actually good for in terms of applications? So Joav Goldberg is a very prominent parsing researcher. And he's, you know, this is kind of the stuff that he's studied for most of his career. And he's, you know, one of the more well-known parsing people. And so it's interesting to see him and other people reflect on this and say that he finds it fascinating that even though we have so many best papers in NLP, so it's kind of a high prestige thing to study parsing. But it seems like syntax is hardly used in practice in, you know, most of these applications. So the question is, you know, why is this? Is it just that because parsing is based on trees and structured predictions kind of fun to study, and there's all these deep algorithmic questions, is it just kind of this catnip to researchers? And is it, does it have this kind of over-prominence in the field? Or is it that, you know, there is something deeper about this, and we should really be continue studying this? Well, I think that this is, there's kind of, I can go either way on this. And so this slide shows you the case for parsing, and then I'll, you know, kind of have a counterpoint in a second. So I think that the most important case for parsing is that there's a sort of deep truth to the fact that sentence is a tree structure. That's, they just are, right? Language, syntactic structure of sentences is recursive, and that means that you can have arbitrarily long gaps between two words which are related. So for instance, if you have a relationship between, say, a subject and a verb, like syntax is, whether the subject of that verb is plural or singular is going to change the form of the verb. And that dependency between them can be arbitrarily long because you can have this nested structure. But it will never, but it can't be arbitrarily long in tree space because, you know, there's only, if you, the relationship between them will always be the subject and the verb, like, sort of next to each other in the tree. So you can see how, for some of these things, it should be sort of more efficient to think about it or model it as a tree. And the tree should tell you things that you otherwise would have to infer from an enormous amount of data. It should be more efficient in this way. So we can say, okay, you know, in theory, this should be important. And it should be something that we study based on this knowledge about how sentences are structured. So then the sort of counterpoint to this is, all right, so sentence is a tree structure, and that's a truth about sentences. But it's also true that they're written and read in order. So, you know, if you read a sentence, you do read it from left to right, or in English anyway, or, like, basically from start to finish, or you hear a sentence from start to finish. And this really puts a sort of bounding on the linear complexity that you will empirically see, right? Because when somebody wrote this sentence, yes, they could have an arbitrarily long dependency, but they expect that, you know, that would mean that their audience listening to it will have to wait arbitrarily long between, you know, some word and the thing that it attaches to. And that's kind of not very nice, right? So empirically, it's not very surprising to see that most dependencies are in fact short. And, you know, there's a lot of arguments that the options that are kind of provided to grammars are sort of arranged so that you're able to keep your dependencies short. Like, that's what are some of the reasons you have options for how to move things around in sentences to make nice reading orders, because, you know, you want short dependencies. So this means that if most dependencies are short, then processing text as, say, chunks of words of one or two at a time kind of gives you a pretty similar view. Most of the time, you don't get something that's so dramatically different if you look at a tree instead of looking at chunks of three or four-word sentences. So, you know, this is kind of a counterpoint that says, you know, maybe even though the sentences are in fact tree-structured, maybe it's not that crucially useful. So I think that the part that makes this, you know, particularly rewarding to look at syntax or particularly useful to provide syntactic structures in a library like spaCy is that they're application independent. So there's the syntactic structure of the sentence doesn't depend on what you hope to do with the sentence or how you hope to process it. And that's something that's quite different from other labels or other information that we can attach to the sentence. If you're doing something like a sentiment analysis, there's no truth about the sentiment of a sentence that's independent of what you're hoping to process. Like, that's not a thing that's in the text itself. It's, you know, a lens that you want to take on it based on how you want to process it. So, you know, there's a... Whether you consider some review to be positive or negative depends on your application. It doesn't really... It's not necessarily in the text itself because, you know, what counts as positive or negative? What's the labeling scheme? What's the rating scheme? Or, you know, exactly what are they talking about? Well, the taxonomy that you have will depend on what you're hoping to process with. Those things aren't in the language, but details about the syntactic structure are in the language. They're things which are, you know, just part of the structure of the code. And that means that we can provide these things, sort of, learn it once and give it to many people. And I think that that's very valuable and useful and different from other types of annotations that we could calculate and attach. And that's why spaCy provides pre-trained models for syntax but doesn't provide pre-trained models for something like sentiment because we know how to give you a syntactic analysis that's, you know, as useful as it may be, or maybe not, depending on, you know, whether that actually solves your problems. But at least it's sort of true and generalizable, whereas we don't know how to give you... We don't know what categorization scheme you want to classify your text in, so we can't give you a pre-trained model that does that because that's your own problem. So we try to, you know, basically give you these things which are annotation layers which do generalize in this way, and that means that there has to be a sort of linguistic truth to them, and that means that looking at things like the semantic roles or sentence structure or sentence divisions are things that we can do, and that's why we, you know, are interested in this. So the other thing about syntactic structures and, you know, whether they're useful or not, is that in English, not using syntax is pretty powerful because English orthography happens to cut things up into pretty convenient units. They're not optimal units, but they're still, like, pretty nice, in a way that doesn't really hold true across a lot of other languages. So in the bottom right here, we have Japanese, which, you know, usually isn't segmented into words. Like, you can't just cut that up trivially with whitespace and get something that you can feed into a search engine or get something that you can feed forward into a topic model. You have to do some extra work, and the extra work that you do there really should consider syntactic structure. You can use a technology that only makes linear decisions, but the, you know, truth about what counts as a word or not is very entangled with the syntactic structure, and so there's real value in doing it jointly with syntactic parsing. For other languages, you have kind of the opposite problem. So we have here a German word, and this, you know, is the German word for income tax return. Now, whether or not you want that to be sort of one unit will depend on what you're looking for. For many applications, actually, the English phrase is too short, and the domain object, the thing that you want to be, you know, looking for and having a single node in your knowledge graph for would actually be income tax return. That's pretty awesome. But in other applications, maybe you just want to look for tax, and so in those cases, the German word will be too large and your data will be too sparse. So there's, you know, there's sort of different aspects of this. In the bottom left here, we have an example of Hebrew, and like Arabic and a couple of other languages like this, there's no vowels in the text, and the words tend to be kind of, have all sorts of attachments to them that are difficult to segment off. So there again, you have difficult, like, segmentation problems that are all tangled up with the syntactic process. Okay, so going forward to sort of an example of what we can do if we, you know, recognize non-white space looking words and feed them into some of the other processing stuff that we have. So this is a demo that we prepared a couple of years ago for an approach that we call, that is termed Sense2vec. So all this is is basically processing text using natural language processing tools, in this case specifically spaCy, in order to recognize these concepts that are longer than one word. So specifically here we look for base noun phrases and also named entities, and we just merge those into one token before feeding the text forward into a word2vec implementation, which gives you sort of these semantic relationships. And this lets you search for and find similarities between phrases which are much longer than one word. And as soon as you do this, you find, ah, the things which I'm searching for are much more specific in meaning. I'm not, you know, looking for, you know, one meaning of learning or one meaning of processing, which doesn't tend to be so useful or interesting. Instead, I'm looking, you can find things related to natural language processing, and then you see, ah, machine learning, computer vision, et cetera. These are, you know, real results that came out of the thing, as soon as you did this division. And so we can do this for other languages as well. So if we were doing, if we were hoping to use word2vec for a language like Chinese, you really want to be processing it into words before you do that. Or if you're going to do this for a language like Finnish, you really want to cut off the morphological suffixes before you do this. Okay. So, incidentally, Innis has cleaned up the sensitive recently, so you can actually use this as a handy component within spaCy. So you can load up a standard model and then add a component that gives you these sensitive excenses. So you can just say, all right, the token for 3 would be natural language processing because it would do the merging for you. And then you can also look up the similarity. So it's now much easier to actually use the pre-trained model and use that approach within spaCy. Incidentally, we have this concept of an extension attribute in spaCy so that you can kind of attach your own things to the tokens so that you can basically attach your own little markups or processing things. So this underscore object is kind of a free space that you can attach attributes to, which ends up being quite convenient. It's a lot more convenient than trying to subclass something or something. Okay. So for the rest of the talk, I'll give you a little bit of a pretty brief overview of the parsing algorithm and then explain how we're modifying the parsing algorithm to work with languages other than English so that we can basically broaden out the support of spaCy to these other languages. So what we see here is a completed parse. And I'm going to sort of talk you through the steps or the decision points that the parser is going to make to derive this structure. And so the kind of key thing to keep in mind or the key aspect of the solution is that it's going to read the sentence from left to right and maintain some state. And then it's going to have a sort of fixed inventory of actions that it has to choose between to manipulate the current parse state to build up the arcs. And this type of approach, which is called transition-based parsing, I find deeply satisfying because it's linear in time because you only make so many decisions per word. And I do think that it makes a lot of sense to take algorithms which process language incrementally. I think that that's sort of deeply satisfying and sort of correct in a way that a lot of other approaches to parsing aren't. And it's also a very flexible approach. So we can do joint modeling and have it output all sorts of other structures as well as the parse tree. And that's actually what we're going to do. So already in spaCy we have joint prediction of the sentence boundaries in the parse tree. And what we're going to do is extend this to this joint prediction of word boundaries as well. Okay, so here's how the sort of decision process of building the tree works. So we start off with in an initial state. And so for sort of ease of notation or ease of readability, we're notating the sort of first word of the buffer. And so the first word that's kind of being focused on as this kind of beam of highlighting. And then the other element of the state is a stack. And so when as the first action that we do, we have an action that can advance the buffer one and put the word that was previously at the start of the buffer onto the stack. So here's what that shift move is going to look like. So here we have Google on the stack, which we write up here. And the first word of the buffer is reader. And so then another action that we can take is to form a dependency arc between the word that's on top of the stack and the first word of the buffer. So in this case, we want to attach Google as a child of reader. So we have an action that does that. And because we're building a tree, when we make an arc to Google, we know that we can pop it from the stack because it's a tree, it only can have one head. It can only have sort of one attachment point. It's not a different type of graph. And so that means that we can kind of do that and keep moving forward. So here's what that looks like. We add an arc and pop Google from the stack. So now we make the next move. Clearly, we've got no words on the stack. So we should put reader on the stacks that we can continue. Now we're at was. And now we want to decide whether we should make an arc directly between was and reader. In this case, no, we want to attach was to canceled. So we're going to move was onto the stack and move forward onto canceled. So then here we do want this arc between cancel and was. So we do another left arc. And so we basically continue here. So sort of stepping back a bit and thinking about this, we've got a fixed inventory of actions. And as long as we can predict the right sequence of those actions, we can derive the correct paths. So that's how the machine learning model is going to work here. The machine learning model is going to be a classifier that predicts, given some state, what to do next. And you can sort of imagine that we can have other actions instead if we wanted to predict other aspects of the structure. So in the case of spaCy, we have an action that inserts a sentence boundary. So it just says, all right, given the words that are currently on the stack, you have to make actions that can clear the stack, but you're not allowed to push the next token until your stack is clear. And that means that there's going to be a sentence boundary there. And we could have other actions as well. There's been work to jointly predict part of speech tags at the same time as you're parsing. Or you can do semantics at the same time as you do syntax. And so you can code up all sorts of structures into this. And you're going to read the sentence left to right, and you're going to output some meaning structure attached to it. And as I said, I find this a satisfying way to do natural language understanding, because it does involve reading the sentence and adding an interpretation incrementally. OK, so that's what this looks like as we proceed through. So all right, so how are we going to do this splitting up or merging of other things? Well, it's actually not that complicated given this transition-based framework. So already you can kind of see that in order to merge tokens, all we really have to do is we've got those tokens, and if we want to Google Read it to be one token, we just have to have some special dependency label, which we are going to have in the tree. And so I've used the label sub-token, and then all we have to do is say, all right, at the end of parsing, we're going to consider that as one token. So the step from going through something like this and labeling a language like Chinese is actually super simple. We just have to prepare the training data so that the tokens are individual characters. And then we can say, all right, things which should be one word should have this sort of structure with this label. And then if the parser decides that those things are attached together, then at the end of it, you just merge them up. The splitting tokens is more complicated because you have to have some universe of actions that manipulates the strings. So I'm still sort of working on the implementation of this in a way that's sort of clean and tidy. But I actually think that this will be useful for a lot of English texts as well, because if you have English text that's sort of misspelled, a lot of the time, things which should be two tokens get merged into one. So its is a particularly common and frustrating one of this because the verb is, should be its own token. But if you have its as its, which is also a common word in English, you need to figure out that you have to have two parser actions, two parser states for that. And in general, you could have a statistical model that reads the sentence beforehand, but that statistical model that is going to read the sentence and process it is going to end up taking on work and doing jobs of figuring out the syntactic structure of the sentence in order to make those decisions. And that's why I think doing these things jointly is kind of satisfying because instead of learning that information in one level of representation and throwing it away only to build up the same information in the next parser to pipeline, you can do it all at once. And so I think that, you know, the joint incremental approaches, I think, are very satisfying and good. Okay, so where are we at the moment? So I've implemented the learning to merge, you know, side of things which involves figuring out better alignments between the gold standard tokenization and the output of the tokenizer. And that's allowed me to complete the experiments for Chinese, Vietnamese, and Japanese of the conference in natural language learning 2017 benchmark, which was a sort of bake-off of these parsing models which was conducted last year. Now, in that benchmark, the team from Stanford did extremely well compared to everybody else in the field. They were, you know, some two or three percentage points better. And so at the moment, we're ranking kind of at the top of what was the second place pack. So most of the languages were coming sort of underneath the Stanford system, but with significantly better efficiency and with sort of this end-to-end process. And in particular, we're doing better than Stanford on these languages like Chinese, Vietnamese, and Japanese because the Stanford system did have this disadvantage of using the sort of pre-processed text. They didn't do the whole task. They wanted to just use the provided pre-processed text so that they could focus on the parsing algorithm. And that meant that they did have this error propagation problem. If the inputs are incorrect because the pre-processed segmenter is incorrect, then they have a big disadvantage on these languages. So satisfyingly, we're sort of doing all at once and entangling all of these representations. It does have this advantage, and we're seeing that in the results that we have for those languages. And the other thing that's satisfying is this joint modeling approach of deciding the segmentation at the same time as deciding the parse structure is consistently better than the pipeline approach in our experiments. So basically, we're getting a sort of 1% to 3% improvement from this, which is about the same size as we're getting from using the neural network model instead of the linear model. So I've found this also quite satisfying that the sort of conceptually neat solution is also working well in practice. Okay, so where does this go and what do we hope to deliver from this? Yes, that would probably be it. How am I for time? Now, I don't know. Okay, well, this is the last slide, so wrapping up. Okay, so what we want to do is we want to deliver a sort of workflow or user experience where it's very easy to start with these pre-trained models for the different languages and broad application areas. And we want to make sure that they have the same representation across languages. So you get the same parse scheme, which the folks have been working hard on and basically now have a pretty satisfying solution from the universal dependencies. And so if you're processing text from different languages, it should be easy to find, say, subject-verb relationships or direct-object relationships. And that should work across basically any language so that you can use these parse trees and basically have a level of abstraction from which language the text is in. And then given this, you should be able to do pretty powerful rule-based matching from the parse tree and other annotations that are provided. So it should be pretty easy to find information even without knowing much about the language and reuse rules across language. And then if the syntactic model and the identity models that we provide aren't accurate enough, the library should support easy updating of those, including learning new vocabulary items without you taking particular effort from this. And overall, we sort of want to emphasize a workflow of rapid iteration and data annotation. And so the concept of this is that we should be able to provide things which give a sort of broad-based understanding of language, but that still ends up with a need for the knowledge specific to your domain and the training data and evaluation data specific to your problems. And we want to make sure that it's easy to connect the two up and go the extra, start from a basic understanding of language and move forward to building the specific applications, which is now Ines will be talking about that aspect of the sort of intended package. Oh, yeah. Yeah. Yeah. Right. So the... Yes, certainly. So delete parsed what the sort of overall difference or main most important difference between Spacey's parsing algorithm and Stanford's parsing algorithm. So amongst other things, the sort of most fundamental differences that Stanford's system is a graph-based parser. So this is O N, like O N squared or maybe O N cubed in length of the sentence. So you're unable to use this type of parsing algorithm for joint segmentation and parsing. You have to have a pre-segmented text, which is why it has this disadvantage on languages, which are more difficult or text, which is more difficult to segment into sentences. So in Spacey, we want to make sure that we basically only use linear time algorithms. And that's why we only take this transition-based approach. Other reasons sort of why they get such a good result. Other people have done graph-based models and they're not nearly as accurate. So I hope to meet the Stanford team in the next couple of days and shake out the details of why the system is so accurate, because actually it is quite surprising. I've read their papers several times. I can't get the sort of one key insight that means that their system performs so well. It's interesting. Something that could replace the near-percent taxing parsing or go through that or how do you think about that? So I think that... Right, yes, certainly, yes. So the question, which is a very good one that many people have been thinking about, to what extent can end-to-end systems, which maybe learn things about syntax, but learn them latently and don't have an explicit syntactic representation internally, replace the need for this type of syntactic processing. So I would say that for any application where there's sufficient text, currently the best approach or state-of-the-art approach doesn't use a parser. And actually this includes translation and other things where you would kind of expect that having an explicit syntactic layer would help. If there's enough text, it seems that going straight to the end-to-end representation tends to be better. However, that does involve having a lot of text. And for most applications, creating that much training data, especially initially when you're prototyping, tends not to be such a viable solution. So the way that I see it is that the parsing stuff is a great scaffolding, and it's a very practical thing to have in your toolbox, especially when you're trying to figure out how to model the problem. Because otherwise you end up in this chicken-and-egg situation of, well, we need lots of data to make our model work well, and otherwise it just doesn't really get off the ground, but then how do we even know that we're collecting the right data for the right model until we have that data collected and we can see the accuracy? So if you can take sort of smaller steps using these sorts of rule-based scaffolding and bootstrapping approaches, I think you have a much more powerful and practical set of tools. And then finally, once you have a system that you know you want to eke out every percent, maybe you end up collecting enough data that you don't need a parser in your solution explicitly. So Dalip has pointed to a paper that recently showed that BiLSTM models don't necessarily learn long-range dependencies. I think that that's probably true, but as somebody who's worked on parsing for a lot of my career, I try to remind myself not to cherry-pick results. And even if I do find a paper that shows that parsing works on something, well, the overall trend is that BiLSTM models which don't use parsing work well. And the fact is that long-range dependencies are kind of rare. So that's basically why it's important to be asking, well, what are these things good for, and not say, oh, everything should be using parsing, because it's true that not everything should. So the question is, if we look at other aspects of language variation instead of just the segmentation and things, how does the incremental model perform? So specifically, how does it perform in free word or languages, perhaps ones with longer range or crossing dependencies? So Stanford, actually, their paper had excellent analysis about a lot of these questions. And so they showed that their model, which is much less sensitive to whether the trees are projective, they do do relatively well in those languages. So for our preliminary results, we do fine on German and pretty well on Russian. We still suck at Finnish, and I think there's a bug in Korean. It's at like 50%. So it's a mixed bag. But I would say that there's some problems to solve about the projectivity. The way that I'm doing this is a little bit crude at the moment. But yeah, so in general, there is a disadvantage that we take from the incremental approach in this. And there's a lot of clever solutions that I'm looking into for this. So yeah. Any words, sentiment analysis in the library? So there's a pretty good extension package for our coreference resolution that has taken some of the pressure off us to support it internally. We do think that coreference resolution is something that does belong in the library because it's something that does have that property of being a language internal thing. So I think that there's a truth about whether that he or she belongs to that noun that doesn't depend on the application. It's just a truth fact about that sentence. So we're very interested in being able to give you that piece of annotation. I wouldn't quite say the same thing about the sentiment. I don't quite know. I haven't been convinced by any schema of sentiment that is sufficiently independent of what you're trying to do that we could provide it. Instead, what we do provide you is a text categorization library. And the text categorization model that we have is only one of many that you might build. And it's not best for every application. But it does do pretty well for short text. And I think that on many sentiment benchmarks, it performs quite well. It's a lot slower than some other sentiment ways that you could do sentiment. So it depends on what type of text you're trying to process and that sort of thing. Well, yes. So explicitly, the coreference resolution package that you should use is called neural coref. So neural coref. Yeah. It's built on PyTorch. It's overall pretty good. You can train it yourself. Well, PyTorch is the machine learning layer. Yes, it's built on space. So for German, I think it's pretty easy. I've been using the word vectors trained by fast text. And you can basically just plug that in. So there's sort of one command to convert that into a spacey vocab object and load it up. We're trying to provide pre-trained models which don't depend on pre-trained word vectors. So you can bring your own. Because otherwise, there's kind of this conflict of, you know, the model's been trained to expect some word vectors. And then if you sub your own in, it's going to get different input representations. So but yeah, training or bringing your own vectors is designed to be pretty easy. And if it's not, I apologize if there's bugs. And we'll try to fix them. So the question is, after parsing and interpreting, do we have an interlingual representation that can then be used to generate another language? The answer is probably not. I mean, I don't have we don't have generation capabilities in spacey. People have worked on this sort of thing. But in general, having an explicit interlingua tends to perform less well than more brute force statistical approaches to syntax. And I think the reason does sort of make sense that, you know, the languages are pretty different in the way that they phrase things and the way that they model the world in lots of ways. And so getting a translation that's remotely idiomatic out of that sort of interlingual representation is pretty tough. So and then there's another argument that you're solving a sub problem that's harder than the than the direct translation approach, which I'm not sure whether I buy that argument or not. But it's a common one that people use. OK, so should we move forward to the next talk? So, yeah, we've we started out by hearing a lot about the more theoretical side of things. And I'm actually going to talk about how we collect and build training data for all these great models we can now build. And the nice thing about machine learning is that, well, we can now train a system by just showing examples of what we want. And that's great. But the problem is, of course, we need those examples. And even if you're like, oh, I got this all figured out, are you using this amazing unsupervised method that where my system just infers the categories from the data and I never need to label any data? That's pretty nice. But you still need some way of evaluating your system. So we pretty much always need some form of annotations. And now the question is, well, why why do we even care about this? Why do we care about whether this is efficient, whether this works or not? The thing is, the big problem is that we actually with many things in data science and machine learning, we need to try out things before we know whether they work or we often don't know whether an idea is going to work before we try it. So we need to expect to do annotation lots of times and start off from scratch, start all over again. If we got if we fucked up our label scheme, try something else. So we we need to do this lots of times. So it needs to work. And similarly, especially if you're working in a company in a team where you really want to use your model to find something out. Ideally, the person building the model should be involved in that process. And also, we always say good annotation teams are small. A lot of people don't understand this. There's a lot of movement towards, oh, let's crowdsource this, get like hundreds of volunteers. And we always have to remind, especially companies that, well, look at the big corpora that we use to train models like those. The good ones were produced by very few people. And there's a reason for that. It like does not more people doesn't always mean better results. Actually, quite the opposite. So, you know, how how great would it be if actually the developer of the model could be involved in labeling the data? And of course, we also have the problem of the specialist knowledge, especially in, you know, in industries where this matters. You might want to have a medical professional give some feedback on the labels or actually really label your data or maybe a finance expert. And, yeah, those people usually have limited time. If you get an hour of their time, you want to use it more efficiently and you don't want to bore them to death or actually find the one person who has nothing else to do because they're probably their knowledge is probably not as valuable as other people, other experts knowledge. And yeah, and another big problem since you know, you want humans is that humans are actually humans kind of suck. Like we really we're not that efficient at a lot of a lot of things. So, for example, like we really have problems performing boring unstructured tasks, especially things that require multiple steps and multiple things we need to get right. We can't remember stuff. We yeah, we really we're bad at consistency and getting stuff right. So, yeah, fortunately, computers are really good at that stuff. And in fact, it's probably also the main reason we built computers. So there's really no need to waste the human's time by making them do stuff that they're going to do badly anyways. And instead, we want our annotation tooling to be as like automated as possible. We want to in general, we want to automate as much as possible and really have the human focus on the stuff that the human is good at and we really need that input. And that's usually context ambiguity like stuff like we can look at a sentence and most of us will be able to understand the figure of speech immediately without thinking twice about it. That's the stuff that's really, really hard for computer. Also, you know, put differently. Yeah, humans are good at precision. Computers are good at recall. So the thing is, yeah, what I'm saying here, it sounds a bit like floss and eat your veggies. Yeah, we probably all have had some experience with labeling data. And normally, yeah, we also gave this talk to a crowd of like more data science focused industry professionals. And actually, you'd be surprised how many companies we talk to, also very large companies, very actually technologically sophisticated companies that mostly use Excel spreadsheets for everything. And it's not inherently bad, but they are very obvious problems with Excel spreadsheets and there's definitely a lot of room for improvement. So once people figure this out and realize that maybe they could do something better or it's just terrible, like we don't want to do this. The next move is normally let's move this all out to Mechanical Turk or some other crowdsourced platform and Mechanical Turk, the Amazon cloud of human labor. And so, yeah, people do that. And often then I also surprised that the results are not very good. And the problem is, yeah, OK, so you have some some guy do it for five dollars an hour, get the data back, train your model doesn't work. And actually, it's very difficult to then retroactively find out what the problem was. Maybe your label scheme was bad. Maybe your idea was bad. Maybe the data was bad. Maybe you didn't write your annotation manual properly. Maybe actually, yeah, another another nice thing. Maybe you pay too much because, you know, if you pay too much on Mechanical Turk, you attract all the bad actors. So you kind of have to stick to the like half of half minimum wage. So that could have been a problem. You want maybe your model was bad. Your training code was bad. It's very, very difficult to find that out. And also you realize that, well, it's not really just a cheap click work like you. You know, you need to do it more. So then, yeah, what most people conclude from this is, fuck this labeling in general. I don't want to do this anymore. Let's just find some unsupervised method and like not bother with this. And that's actually also a conversation I had recently where we talked to a larger media company and they'd done exactly that. And now they have a few hundred clusters. And it's really great. They have really great clusters. But now their problem is that they have no idea what these clusters are. So they now need to label their clusters. And now they're kind of back in the beginning. And I think what we see from this is that the label data itself, the fact that we need label data, that's an opportunity. That's not the problem. The problem is how we do it. And yeah, so then there are a few like we've been thinking about this a lot. And there are at least, yeah, from our point of view, there are a lot of things we could do better. So one of the things really to work against this problem that we have caused by us being human is that we should we need to break down these very complex things we're asking the humans into smaller, simpler questions. And really, these should be binary decisions so we can have a much better annotation speed because we can move through the things faster and we can also measure the reliability much easier than if we ask people open questions because we can actually say, OK, do our annotators agree? Do they not agree? Because that's in the end very important to find out whether we've collected data the right way. The binary thing itself, it sounds very it sounds a bit radical, but actually if you think about it, most or pretty much any task can be broken down into a sequence of binary decisions like yes or no decisions. It might mean that we have to accept that, OK, and if we annotating a sentence or entities, we won't actually end up with a gold standard with gold standard data for this sentence. We might actually end up with only partially annotated data and have to deal with that. But as it but still we actually able to use our human's time more efficiently, which is often much more important. So a lot of that love the samples I'm going to show you now are from using our annotation tool prodigy, which we started building as an internal tool. But we very quickly realized that OK, this is really something pretty much every company we talk to most users we talk to. This was always something that kept coming up. So we thought, OK, what if we really combine all these ideas we already have and how to train a model actually use the technology we're working with within the tool and also use the insights we have from user experience and how to get how to get humans to do stuff most efficiently. How to get humans excited actually even how to the whole idea of gamification, how to get humans to really stick to doing something and put this all into one tool. And that's that's prodigy. And so here we see some some examples of those tasks and how to you know how we can present things in a more binary way. So in the top left we have a named entity task. So here this is this comes from Reddit and we're labeling whether something is a product or not. And what we did here is we load in a spacey model, ask the model to label the products and then we look at them and say yes or no. We could even we can also use a mode where we can where we can then actually click on this, remove this, label something else. But still you see, OK, we don't have to we don't have to do this in an Excel spreadsheet. We actually get one question. We look at this and pretty much immediately we can say yes or no. The same here on the right there we're using I think this is actually a real example using the YOLO2 model with the default categories and we have an image of a skateboard. We could say is this a skateboard. Yes or no. We immediately have our annotations here and even this one in the corner even if we if we're not able to really break it down into a true binary task we can still make it more efficient and easier for a human to answer because here with keyboard shortcuts you can still do maybe two, three seconds per annotation and you have an answer. Or we say hey it's actually so fast if we can get to one second we might as well label our entire corpus twice positive negative other labels we want to do and just move move through it quicker. And yet to give you some background on like what why did we do this what do we what do we think Prodigy should achieve. We really think that okay we we want to be able to make annotation so efficient that data scientists can do it themselves or hear what we call data scientists can also be researchers and people working with the data people training the models like it's it's still reading it like that it still doesn't sound like fun. But the idea is you know we could really make it a process that's efficient that you actually really want to do this because you don't have to depend on anyone else. You can just get the job done job done and see whether your idea works or not. And the same. Yeah. And this also means you can iterate faster. We're very used to okay you iterate on your code but you can actually iterate on your code and your data. You try something out doesn't work try something else. Maybe see okay is it going to work if I collect more annotations you can all try this out. And we also want to waste as little time as possible and use what the model already knows and have the human correct its predictions instead of just having a human do everything from scratch. And yeah as a library itself we really want prodigy to fit into the Python ecosystem. We want it to be customizable extensible in Python. You can write scripts for it. And we also it was a very conscious decision not to make it a source tool because we think data privacy is important. You don't you shouldn't have to send your text to our service for no reason. And we also think you should you shouldn't be locked in like you should get a JSON format out that you can use to train your models however you like and not our random format that you can then download from our service. So that's where we're going with prodigy and just here's this very simple. Illustration of how the app looks the center recipes which are very simple Python scripts that orchestrate the whole thing you have a rest API that communicates with web app naturally so you can see things on the screen. You have your data that's coming in with just text images. And you can have an optional model state that's updated in a loop. If you want that and then you have the model then communicates with the recipe. You can ask the user annotates it's updated in a loop and can suggest more annotations that are more compatible with the annotators recent decisions. And yeah we also there's a database and a command line interface so you can actually use it efficiently and don't have to worry about these aspects. So here can you see yeah in the corner we have a simple example of a recipe function which really is just a Python function. You load your data in and then you return this dictionary of components. For example an ID of the data set how to store your data a stream of examples you can pass in callbacks to update your model things to execute before the thing starts. So the idea is really okay if you can if you need to load something in if you can write that in Python you can do it in part. And you can also we provide a bunch of pre built in recipes for different tasks with some ideas of how we think it could you know it could work like named entity recognition. For example you can use the model correct its predictions you can use the model say yes or no to things you can use it for dependency parsing and look at an arc and annotate that. We have recipes that use word vectors to build terminology lists text classification so there's a lot also a lot that you can mix and match creatively like for example you have those the multiple choice example that's not really tied to any machine learning task. But it fits pretty much into any of these workflows that you might be doing and of course the evaluation is also something we think is very very important and is often neglected especially in more industry use cases. But we think there's actually a BL valuation is actually very powerful way of testing with it. So that you know your output is really what you want it to be. And yeah, and so here we see we see an example of okay the of how you can chain different workflows together, all using models work vectors things you already have in order to get where you want to get to faster so here is simple example we want, we want to label fruit. It's kind of a stupid example because it's that I can't think of many use cases we actually want to do that but it makes a great illustration here. So, yeah, we start off we say okay we want for what our fruit, we have some examples apple pear banana. That's what we can think of, and we also have word vectors that we can use that will easily give us more terms that are similar to these three fruit terms that we can use. So that's what we came up with. And then we can use this terminology list that we collected by just saying yes or no to what we've gotten out of the word vectors. Look at those in our data, and then say whether Apple, apples in this context is a fruit or not. So we still, you know, we're not just labeling all fruit terms as a fruit entity because it could be Apple the company, but we get to look at it, and it's much more efficient than if you ask the human to sit through and highlight every instance of fruit nouns in your text. So this this also leads to kind of one of our main. Yeah, main aspects of the tool workflows that we especially proud of and that we think really can make a difference, which is, we can actually start by telling the computer at more abstract rules of what we're looking for, and then annotating the exceptions instead of really starting from scratch or we can use, we can even use the technology we're working with to build these semi automatically using word vectors using other other cool things that we can now do. And then of course also specifically look at those examples that the model is the statistical model, we want to train is most uncertain about. So, we try to avoid the predictions, where we can be pretty sure that they correct and actually really ask the user, ask the human first about the stuff that's 5050 and we really the human feedback makes most of the difference. And so here's a quick example, let's say, okay, we want to label locations, we start off with one city San Francisco, and then we look at what else is similar to that term so these are actually these are actually real suggestions from that sense to make model that Matt showed earlier. And you can see we also will in the nice thing is we're using word vectors we're not using a dictionary so we're not going to, we're going to annotate California and maybe University of San Francisco, but we're not going to annotate California roles because we already you know we innovate the space and we know that what we're actually looking for is at least similar to the real meaning of the word and a lot of these are super trivial to answer so we can accept them, we can reject them or we can ignore them because we, because this is a bit too ambiguous and we don't actually want that in our list, because it can mean too many things and then from here. We can actually create a pattern that uses species attributes or in this case. Yeah, the, the token, the lower case form of the token and GPE that's stands for geopolitical entities so anything with the government. And that's what we're trying to label so we can easily build up these roles, very quickly they are like automated, and then we have a bunch of locations that we can then match in our text so here. Constructs we can we can really take advantage of the syntactic structure. So, here this was an, this was a finance example. So what we're trying to do is we want to extract information about executive compensation. So, yeah, some executive receive some amount of money in stock for example like this one. And it's, this is a pretty difficult task but also the idea is we have, we have this theory that maybe if we could train a model, a text classification model to predict whether a sentence is about executive compensation or not. We can then very easily use what we already know about the text to extract, let's say the first person entity, we extract the amount of money, put that in our database. And we've actually, yeah, we found a good solution for an otherwise very complex task so for this, this is just an idea. I haven't like we haven't tried this in detail but one possible pattern using token attributes we have available would be. Let's try look for an entity type person, followed by the lemma or a token with a lemma receive so received receives receiving and followed by an end by a token with the entity type money. And let's just look at what this pulls up. That's an idea. I mean, you can get plenty of other possible patterns, you can come up with. And the nice thing is we actually going to be looking at them again in context. So, they don't have to be perfect and even actually in fact, even if it pulls up random stuff that you realize is totally not what you want. It's, this is also very important because you won't only be collecting annotations for the things you know are definitely right you also collecting annotations for the things that are very similar or look very similar to what you're looking for but actually not what you're looking for that's probably just as important as the positive examples. So, yeah, the moral of the story is what we really what we're saying is, you know, you will be used to iterating on our code as programmers but but you should, you should really be doing both like the data is just as important so we, as we see here okay that's the normal type of programming, you have a runtime program, you work on the source code, you compile it get your runtime program you don't like something about your program, you go back change the source code compile it and so on that's a pretty standard workflow. And in machine learning. We don't have a runtime program in that sense we have a runtime model. So, the part we should really be thinking about and working on is the training data. Instead, most focus is currently on the training algorithm and that's if you use that analogy that's kind of, that's very similar to going and tweaking your compiler if you're not happy with your runtime program. You can do that but of course you probably go back and edit your source code and I think this is actually, this is actually a pretty good example is pretty accurate and you usually you know there are only so many training algorithms but what really makes a difference is your data so if you have a good way and a fast way of iterating on that data. You actually, and you know you're able to really master this part of the problem you, you'll also get to try more things quickly you really, you know, as we know most ideas don't actually work I think that's, it's always one of these things that's kind of misrepresented or a lot of people have this idea, you're doing all these amazing AI things and everything just works and it's like, kind of doesn't like nothing works. And sometimes sometimes things work and you know you really want to find the things that actually work and, you know, for that you need to try them and so it also means you know if you can actually figure out what works before you try it and invest in it, you know, you can actually be more successful overall because you don't, you're not going to waste your time on the things that might fail and more scale things up that actually weren't even supposed to work in the first place. And one thing that's also very important to us is you can really build custom solutions you can, you know, build solutions that fit exactly to your use case and you know you keep these, if you collect your own data you keep that forever and nobody can lock you in. You're not just consuming some API and if that API shuts down, you can start again from scratch. You actually, you know, you have your data, no matter what other cool things we can do at some point in the future you can always go back to your label data and really build your own systems and we believe that this is really something that's very important in the future of the technology. That's also a reason why we think AI development in general in companies should be done in house. And yeah we're hoping that we can keep providing useful tools that will make this easier. Okay. So the question is, yeah, Jeremy thinks we write very good software, even though we only two people and how we doing that. Yeah, that's a very good question I mean we do get this. I don't even know where this idea comes from that like, yeah, you can scale things up, like I don't know scaling things up, makes things better, because I do think, yeah, actually, the more people you get involved you sometimes actually can have a very negative impact on the quality of the software you're using. In our case, it's just okay, it just works. Like I'm also, I also don't like this idea of, oh, everyone can do exactly the same thing if they just work hard even though people like thinking of it that way. It's just okay. In our case, we have a good combination of like things that we like to do things that we happen to be good at, and it just works together so I guess we are lucky in that way but we also cut out a lot of bullshit like the amount of meetings we don't take, the amount of events we don't go to. I mean, yeah, it's kind of ironic saying that, speaking at an event, but like, I really don't normally go to many events. I don't, we don't take coffee dates with random people we barely know we don't. Yeah, we mostly we really just like to write software. And yeah, we've had some good ideas in the past. I mean, we've also, the question is, if we've done any experiments where we compare the binary decisions, and whether it influences the annotators versus really doing everything from scratch. So, we haven't done experiments specifically focusing on the bias because that's, in some sense, that's difficult because, you know, we're mostly looking, we're looking at the output, we're looking at does it improve accuracy. So, we've done experiments of manual annotation versus binary annotation, but also mostly focused on our own tooling, because we think it's kind of useless like yeah we can present you a study where we said oh we did stuff in an Excel spreadsheet and then we did stuff in Prodigy and it was much better. But it's really it's mostly focused around our own tooling and we did find that, well, it depends on the task you're doing. That's the other thing, it's sometimes I feel like giving these answers sounds unsatisfying because I'm always saying well, it depends on your data, but that's also kind of, that's also the whole point of it because, you know, we're doing this because your data is different and there's no one size fits all solution. But essentially, so we found what binary annotation works especially well if you already have a pre-trained model that predicts something, ideally also something that's not completely terrible. Otherwise, the pattern approach does work very well on kind of limited, very specific domains like we did one example of where we labeled drug names on Reddit, like on our opiates, which was a pretty good, this was a pretty good data source because it's a very specific topic and also it's a subreddit that's very on topic because people who, I mean, you know, people who discuss, who go on Reddit to discuss opiate use, you know, usually, you know, very dedicated to talking about this one topic. So it was a good interesting data source and so what we wanted to do is we labeled drug names, so drugs and pharmaceuticals, and in order to, for example, have a better, have a better tool set to analyze, really analyze the content of the subreddit and see how it develops over time. So there we found the pattern based approach worked very, very well because we have very specific terms. We can use word vectors to bootstrap these, especially also we can include spelling mistakes and stuff, which was very interesting, like we can really build up good word lists, find them in the text, confirm them and get to pretty decent accuracy. I would expect this work to work a little less well, the cold start problem on a much more ambiguous domain and there you're probably better off to say, okay, we're labeling by hand, but even there, that's something I haven't really shown in detail here, but we've also, we have a manual interface where you highlight, but what we do there is we use the tokenizer to pre-segment the text, so you don't have to sit there and pixel perfect like highlight and then, oh shit, now I've got the white space in, let's start again. So that's another thing we're doing. You can be much lazier in highlighting and also there get more efficiency out of it and still use a simpler interface. Yeah. Okay, so the question is, you gave an example of annotating patient data, which is obviously very problematic because doctors are not always very specific in what they fill in and then in the end, this was how did they enrich that with? Yeah. So basically, okay, the question is whether we have some experience with, yeah, in the medical field mixing is not like, the answer is, well, we haven't personally done this. We do have quite a few companies in that domain, also because the tool itself is quite appealing because you can run it in your own compliant environment, you know, that data privacy aspect, but it's definitely, it's interesting to explore. Maybe also where, okay, having the professionals, getting the medical professionals more involved might make sense, which normally is very difficult. You don't want the doctor to do all the work themselves, but if you can find some way to distill that and then ask the doctor, okay, you said you wrote this here, does that mean you wrote X, does that mean Y and the doctor says yep, or doctor says nah. So, that, if you can try this out and, you know, extract some information or that, that could be one idea to solve that, for example, yeah, I can definitely see that. And then, like right now, it's not, we don't have a built in logic for that, although we are working on, oh, sorry, I forgot to repeat the question, interannotator agreement, if you can calculate that and incorporate that into your model. So, we're actually working on an extension for Prodigy, which is much more specifically for managing multiple annotators, because we really, the tool here, we really designed specifically as a developer tool first, and then, you know, scaling it up a second, but since you have the binary feedback, and if you have an idea, if you have an algorithm you want to use and you kind of, you know, you know what you want, you can already do that fairly easily because you can download all the data as JSON, you have a key that's answer, which is either accept, reject, or ignore. You can attach your own arbitrary data like a user ID, and then it's fairly trivial to write your own function that really takes all of this, reads it in, computes something, and then uses this later on. So, that's definitely possible, but we also, this is also something we're really interested in, yeah, exploring and working on. Yeah, so if you binary, yeah, that's also, that's a big advantage of the binary interface is that, yeah, they only pretty much, yeah, there are two options, you filter out the ignored ones, and then, yeah, you can really answer that question. Yeah. Yeah, well you can, like you could design, so the question was, the one interface I showed, which was the sentiment one with the multiple selections, this is not binary, that's true, and actually it's also something we usually tell our users, avoid this as much as possible, if you can. Like that's, and in some cases you might still want that, or we say, look, if you need to, you know, a lot of people still think of surveys when they think of annotating data, and I get where this is coming from, but I think if you can leave that sort of mindset and really open up a bit and think of other creative ways, you can get more out of this. So if you want to re engineer a survey, maybe you want to use a survey tool. But, but so this, I would, so for example if I were doing this with those four options, I would say, okay, we have all texts, the annotator sees every text four times and says, is this happy or is this not happy. And because you can get to one second per annotation, that's very fast, like you can, even if you have thousands of examples, you can do this in a day yourself. And so that's how we would probably solve this, and it also means you get every example four times, and for each text you know, is it sad, is it happy, is it neutral, is it something else, you have much more data, but not everyone wants this, like some people really want to build that survey and we let them, but yeah. Yeah. So the question is, yeah, if you're doing the same example multiple times, whether it slows down the annotation or not. Well, actually, I mean, it's difficult to say because it depends. But I've actually found that even if you do the bare maths, it can easily be much faster because if you, you know, you say okay 1000 examples. And normally if you really have to think about five different concepts that are maybe not even fully related, that just every tiny bit of friction you put between a human and the interface or the decision can very significantly slow down the process. You think about oh is this happy or is this sad or is this about sports or is this about horses, and just this thing that can easily add like 10 seconds to each question. So, if you do, if you do the whole thing three times at one second, you're still faster than you would have been if you'd added this friction and also and the other part is the just a human error. So if you have to think too much, you're much more likely to fuck it up and do it badly and then that's also something you know that's also something you want to, you want to avoid. Yeah. The active learning also makes a difference here because you can actually, yeah, you could pre-select the ones that really make a difference to annotate and don't have to like really go through every single one that yeah it's not as important as some of the other ones that you really care about. Yeah, so the question is, yeah, what about tasks that need a lot of context like the whole medical history or just a whole document. So we basically, so we have, whether we have experience with that. So, in general, we do say if you can, if your tasks require so much context that you can't fit this into the prodigy interface, then it doesn't mean that you can't train a model on that but for most of the tasks that users most commonly want to do, this is often also an indicator that it's very difficult to actually teach your model that like if you're doing entity recognition or even text classification and you need a lot of context and every other context is equally as important. That's often an indicator that that might not work so well. So for example, text classification, we say, okay, we start off by selecting one sentence from the whole document. And then instead of you annotating the whole document, you say, okay, this is the most important sentence. Does this label apply or not? So there are some tricks we use to get around this problem because yeah, we also think that okay, it's important to get this across and frame it in that way because yeah, if you need two pages on your screen, it's not efficient at all. And also likely you can do all that work but your model won't learn that because your model needs local context as well, at least for the tasks that we are presenting. I don't know if you have anything to add to that. Yeah, okay. Yeah, so the suggestion was yeah, okay, having some tools, some process that goes along with the software that helps people break this down. Yeah, we've actually been thinking about this a lot because we do realize, you know, the tool is quite new and we're introducing a lot of new concepts at once and also some best practices where we think, ah, that's how you should do it or you could try this. And we are also realizing that there's no real satisfying one size fits all answer. That's another problem. Everyone's use case is different. So right now what we're doing is we have a support form for Prodigy where we answer people's questions and actually a lot of users share what they're working on, asking for tips. We kind of talk about it, other users come in and are like, oh, I actually tried to do this type of medical or this type of legal annotation and here's what worked for me and have this sort of exchange around it to figure out, okay, what works because yeah, it's just like I think machine learning, deep learning, a lot of the best practices are still evolving. And it's very, very specific. So it's definitely, yeah, we're open for suggestions as well, but like we're still in the process of really coming up with a good set of best practices and ideas. The question is whether we have any plans to sell models like medical models. Yes, as part of what Matt mentioned in the very introduction, we are definitely planning on having more of a models, like an online store for very, very specific models. Medical, that's a very, very interesting domain and if so, we really want to have it specific like medical texts in French or Chinese and really go in that direction because we believe that, okay, pre-trained models are very valuable and even if you do medical texts, you can start off with a pre-trained model, then you can use a tool like Prodigy or something else to really fine tune it on your very, very specific context, have word vectors in it that already fit to your domain, maybe up those as well. We think that this is a very future-proof way of working with these technologies. So currently, so question is the text classification model, we're using Prodigy, more details on that. So what we're using is Spacey's text classification model. That's what's built in, but I think actually this question is pretty good because what's important to note is that Prodigy itself comes with a few built-in recipes that are basically ideas for, okay, how you could train a text classifier. You could use Spacey, but it's definitely not tied to those. Like the idea, the tool itself is really the scaffolding around it. So if you say, hey, I wrote my own model using PyTorch and I would like to train this, all you need to do is you need to have one function that takes examples and updates them. And you need to have one function that takes raw text and outputs a score for each text. And then you provide that to Prodigy. And then you can use the same active learning mechanism as you would use with a built-in model. So the idea is really the models we ship are just a suggestion or an idea you can use to try it out. But ultimately, we also hope that people in the future will transition to just plugging in their own model and just using the scaffolding around it to do that. We definitely don't want to lock anyone in and say, oh, you have to use Spacey, especially for NER and stuff and other things. We think Spacey is pretty good. But if you don't want to do that or for other use cases, especially text classification, we think there are a lot of cases where you might want to use scikit-learn or what? Valko? Wabbit? Yeah. What a great name. Yeah. Basically something completely custom. So the question is active learning part, whether this is built on the underlying model. I mean, so the question is active learning versus no active learning, how well this works. First, also maybe as a general introduction. So what we're doing for most of the examples is we use a basic uncertainty sampling. That's what we found works best. But we also know there are lots of other ways you could be solving that. So, you know, in the end, how we implement this is we have a simple function that takes a stream and outputs a sorted stream based on the assigned scores and the model in the loop. So how you wire this up again is also up to you. Yeah. To answer the part about what works best in general, in our kind of framework where you really you see one sentence at a time and often you start off with a model not knowing very much. The active learning component, basically resorting the stream is actually very crucial because otherwise if you start from scratch, have very few examples, you'll be annotating for a very, very long time and all kinds of random predictions. You annotate your your stream in order. There's very little you need some kind of guidance that tells you okay what to work on next, especially if you feed in millions of texts, you need to sort them. You need to pre select them based on something. And this could be the models predictions. This could be something else. This could be the keywords or the patterns. But without that, yes, it's very, very difficult. And that's also that's kind of what we're trying to solve with the tool. Thank you. Thank you so much. I got to say, you know, anybody who's using fastai is anytime you've used fastai nlp or fastai.txt, you're called the spacey tokenize function. You're using spacey behind the scenes and the reason you're using spacey is because I tried every damn tokenizer I could find and spaceys was like so much better than everything else. And the kind of story of fastai development is that over time I get sick of all the shitty parts of every third party library I find and I gradually rewrite them myself. And the fact that I haven't rewritten spacey or attempted to is because I actually think it's one of those rare pieces of software that doesn't suck at all is actually really good. It's got good documentation and it's got a good install story and so forth. And I haven't used prodigy but just the fact that these guys are working on, I recognize the importance of active learning and the importance of combining human plus machine. It puts them in that rare category of people who, in my opinion, are actually working on what's one of the most important problems today. So thank you both so much for coming and for this fantastic talk and I look forward to seeing what you do next. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.0, "text": " Okay. So, yeah, this is our first time in the Bay Area,", "tokens": [1033, 13, 407, 11, 1338, 11, 341, 307, 527, 700, 565, 294, 264, 7840, 19405, 11], "temperature": 0.0, "avg_logprob": -0.18777718688502457, "compression_ratio": 1.6228373702422145, "no_speech_prob": 0.011486958712339401}, {"id": 1, "seek": 0, "start": 4.0, "end": 6.5, "text": " so it's nice to meet you all, and thanks for coming.", "tokens": [370, 309, 311, 1481, 281, 1677, 291, 439, 11, 293, 3231, 337, 1348, 13], "temperature": 0.0, "avg_logprob": -0.18777718688502457, "compression_ratio": 1.6228373702422145, "no_speech_prob": 0.011486958712339401}, {"id": 2, "seek": 0, "start": 6.5, "end": 9.200000000000001, "text": " I'm, you know, not so much noticed.", "tokens": [286, 478, 11, 291, 458, 11, 406, 370, 709, 5694, 13], "temperature": 0.0, "avg_logprob": -0.18777718688502457, "compression_ratio": 1.6228373702422145, "no_speech_prob": 0.011486958712339401}, {"id": 3, "seek": 0, "start": 9.200000000000001, "end": 11.6, "text": " So I'll start by just giving a quick introduction of us", "tokens": [407, 286, 603, 722, 538, 445, 2902, 257, 1702, 9339, 295, 505], "temperature": 0.0, "avg_logprob": -0.18777718688502457, "compression_ratio": 1.6228373702422145, "no_speech_prob": 0.011486958712339401}, {"id": 4, "seek": 0, "start": 11.6, "end": 13.8, "text": " and, you know, some of the things that we're doing", "tokens": [293, 11, 291, 458, 11, 512, 295, 264, 721, 300, 321, 434, 884], "temperature": 0.0, "avg_logprob": -0.18777718688502457, "compression_ratio": 1.6228373702422145, "no_speech_prob": 0.011486958712339401}, {"id": 5, "seek": 0, "start": 13.8, "end": 16.2, "text": " before I start with the sort of main content of the talk,", "tokens": [949, 286, 722, 365, 264, 1333, 295, 2135, 2701, 295, 264, 751, 11], "temperature": 0.0, "avg_logprob": -0.18777718688502457, "compression_ratio": 1.6228373702422145, "no_speech_prob": 0.011486958712339401}, {"id": 6, "seek": 0, "start": 16.2, "end": 18.2, "text": " which is about this open-source library", "tokens": [597, 307, 466, 341, 1269, 12, 41676, 6405], "temperature": 0.0, "avg_logprob": -0.18777718688502457, "compression_ratio": 1.6228373702422145, "no_speech_prob": 0.011486958712339401}, {"id": 7, "seek": 0, "start": 18.2, "end": 22.400000000000002, "text": " that we developed, Spacey, for natural language processing.", "tokens": [300, 321, 4743, 11, 8705, 88, 11, 337, 3303, 2856, 9007, 13], "temperature": 0.0, "avg_logprob": -0.18777718688502457, "compression_ratio": 1.6228373702422145, "no_speech_prob": 0.011486958712339401}, {"id": 8, "seek": 0, "start": 22.400000000000002, "end": 26.2, "text": " So the other things that we develop as well at Explosion AI", "tokens": [407, 264, 661, 721, 300, 321, 1499, 382, 731, 412, 12514, 19996, 7318], "temperature": 0.0, "avg_logprob": -0.18777718688502457, "compression_ratio": 1.6228373702422145, "no_speech_prob": 0.011486958712339401}, {"id": 9, "seek": 2620, "start": 26.2, "end": 30.8, "text": " is a machine learning library behind Spacey Think,", "tokens": [307, 257, 3479, 2539, 6405, 2261, 8705, 88, 6557, 11], "temperature": 0.0, "avg_logprob": -0.13359181777290677, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0004943863023072481}, {"id": 10, "seek": 2620, "start": 30.8, "end": 33.4, "text": " which allows us to avoid depending on other libraries", "tokens": [597, 4045, 505, 281, 5042, 5413, 322, 661, 15148], "temperature": 0.0, "avg_logprob": -0.13359181777290677, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0004943863023072481}, {"id": 11, "seek": 2620, "start": 33.4, "end": 35.0, "text": " and kind of keep control of everything", "tokens": [293, 733, 295, 1066, 1969, 295, 1203], "temperature": 0.0, "avg_logprob": -0.13359181777290677, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0004943863023072481}, {"id": 12, "seek": 2620, "start": 35.0, "end": 38.0, "text": " and make sure that everything's easy to install.", "tokens": [293, 652, 988, 300, 1203, 311, 1858, 281, 3625, 13], "temperature": 0.0, "avg_logprob": -0.13359181777290677, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0004943863023072481}, {"id": 13, "seek": 2620, "start": 38.0, "end": 40.4, "text": " We also have an annotation tool that we develop", "tokens": [492, 611, 362, 364, 48654, 2290, 300, 321, 1499], "temperature": 0.0, "avg_logprob": -0.13359181777290677, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0004943863023072481}, {"id": 14, "seek": 2620, "start": 40.4, "end": 41.8, "text": " alongside Spacey Prodigy,", "tokens": [12385, 8705, 88, 1705, 25259, 88, 11], "temperature": 0.0, "avg_logprob": -0.13359181777290677, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0004943863023072481}, {"id": 15, "seek": 2620, "start": 41.8, "end": 44.2, "text": " which is what Innis will be talking about.", "tokens": [597, 307, 437, 682, 10661, 486, 312, 1417, 466, 13], "temperature": 0.0, "avg_logprob": -0.13359181777290677, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0004943863023072481}, {"id": 16, "seek": 2620, "start": 44.2, "end": 47.2, "text": " And we're also preparing a data store of other pre-trained models", "tokens": [400, 321, 434, 611, 10075, 257, 1412, 3531, 295, 661, 659, 12, 17227, 2001, 5245], "temperature": 0.0, "avg_logprob": -0.13359181777290677, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0004943863023072481}, {"id": 17, "seek": 2620, "start": 47.2, "end": 50.2, "text": " for more specific languages and use cases and things", "tokens": [337, 544, 2685, 8650, 293, 764, 3331, 293, 721], "temperature": 0.0, "avg_logprob": -0.13359181777290677, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0004943863023072481}, {"id": 18, "seek": 2620, "start": 50.2, "end": 52.4, "text": " that people will be able to use", "tokens": [300, 561, 486, 312, 1075, 281, 764], "temperature": 0.0, "avg_logprob": -0.13359181777290677, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0004943863023072481}, {"id": 19, "seek": 5240, "start": 52.4, "end": 56.4, "text": " that basically will extend the capabilities of the software", "tokens": [300, 1936, 486, 10101, 264, 10862, 295, 264, 4722], "temperature": 0.0, "avg_logprob": -0.04303841841848273, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.00013332768867257982}, {"id": 20, "seek": 5240, "start": 56.4, "end": 59.6, "text": " for more specific use cases.", "tokens": [337, 544, 2685, 764, 3331, 13], "temperature": 0.0, "avg_logprob": -0.04303841841848273, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.00013332768867257982}, {"id": 21, "seek": 5240, "start": 59.6, "end": 62.8, "text": " So to give you a quick introduction to Innis and I,", "tokens": [407, 281, 976, 291, 257, 1702, 9339, 281, 682, 10661, 293, 286, 11], "temperature": 0.0, "avg_logprob": -0.04303841841848273, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.00013332768867257982}, {"id": 22, "seek": 5240, "start": 62.8, "end": 65.6, "text": " which is basically all of Explosion AI.", "tokens": [597, 307, 1936, 439, 295, 12514, 19996, 7318, 13], "temperature": 0.0, "avg_logprob": -0.04303841841848273, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.00013332768867257982}, {"id": 23, "seek": 5240, "start": 65.6, "end": 67.8, "text": " So I've been working on natural language processing", "tokens": [407, 286, 600, 668, 1364, 322, 3303, 2856, 9007], "temperature": 0.0, "avg_logprob": -0.04303841841848273, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.00013332768867257982}, {"id": 24, "seek": 5240, "start": 67.8, "end": 69.4, "text": " for pretty much my whole career.", "tokens": [337, 1238, 709, 452, 1379, 3988, 13], "temperature": 0.0, "avg_logprob": -0.04303841841848273, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.00013332768867257982}, {"id": 25, "seek": 5240, "start": 69.4, "end": 73.8, "text": " I started doing this after doing a PhD in computer science.", "tokens": [286, 1409, 884, 341, 934, 884, 257, 14476, 294, 3820, 3497, 13], "temperature": 0.0, "avg_logprob": -0.04303841841848273, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.00013332768867257982}, {"id": 26, "seek": 5240, "start": 73.8, "end": 75.2, "text": " I started off in linguistics", "tokens": [286, 1409, 766, 294, 21766, 6006], "temperature": 0.0, "avg_logprob": -0.04303841841848273, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.00013332768867257982}, {"id": 27, "seek": 5240, "start": 75.2, "end": 78.8, "text": " and then kind of moved across to computational linguistics.", "tokens": [293, 550, 733, 295, 4259, 2108, 281, 28270, 21766, 6006, 13], "temperature": 0.0, "avg_logprob": -0.04303841841848273, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.00013332768867257982}, {"id": 28, "seek": 5240, "start": 78.8, "end": 82.0, "text": " And then around 2014, I saw that these technologies", "tokens": [400, 550, 926, 8227, 11, 286, 1866, 300, 613, 7943], "temperature": 0.0, "avg_logprob": -0.04303841841848273, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.00013332768867257982}, {"id": 29, "seek": 8200, "start": 82.0, "end": 83.8, "text": " were getting increasingly viable.", "tokens": [645, 1242, 12980, 22024, 13], "temperature": 0.0, "avg_logprob": -0.08845697811671666, "compression_ratio": 1.728125, "no_speech_prob": 0.0001312701788265258}, {"id": 30, "seek": 8200, "start": 83.8, "end": 85.8, "text": " And I was also at the point in my career", "tokens": [400, 286, 390, 611, 412, 264, 935, 294, 452, 3988], "temperature": 0.0, "avg_logprob": -0.08845697811671666, "compression_ratio": 1.728125, "no_speech_prob": 0.0001312701788265258}, {"id": 31, "seek": 8200, "start": 85.8, "end": 88.4, "text": " where I was supposed to start writing grant proposals,", "tokens": [689, 286, 390, 3442, 281, 722, 3579, 6386, 20198, 11], "temperature": 0.0, "avg_logprob": -0.08845697811671666, "compression_ratio": 1.728125, "no_speech_prob": 0.0001312701788265258}, {"id": 32, "seek": 8200, "start": 88.4, "end": 90.0, "text": " which didn't really agree with me.", "tokens": [597, 994, 380, 534, 3986, 365, 385, 13], "temperature": 0.0, "avg_logprob": -0.08845697811671666, "compression_ratio": 1.728125, "no_speech_prob": 0.0001312701788265258}, {"id": 33, "seek": 8200, "start": 90.0, "end": 92.4, "text": " So I decided to leave and I saw that there was a gap", "tokens": [407, 286, 3047, 281, 1856, 293, 286, 1866, 300, 456, 390, 257, 7417], "temperature": 0.0, "avg_logprob": -0.08845697811671666, "compression_ratio": 1.728125, "no_speech_prob": 0.0001312701788265258}, {"id": 34, "seek": 8200, "start": 92.4, "end": 94.8, "text": " in the capabilities available for something", "tokens": [294, 264, 10862, 2435, 337, 746], "temperature": 0.0, "avg_logprob": -0.08845697811671666, "compression_ratio": 1.728125, "no_speech_prob": 0.0001312701788265258}, {"id": 35, "seek": 8200, "start": 94.8, "end": 97.0, "text": " that actually translated the research systems", "tokens": [300, 767, 16805, 264, 2132, 3652], "temperature": 0.0, "avg_logprob": -0.08845697811671666, "compression_ratio": 1.728125, "no_speech_prob": 0.0001312701788265258}, {"id": 36, "seek": 8200, "start": 97.0, "end": 99.8, "text": " to something that was more sort of practically focused.", "tokens": [281, 746, 300, 390, 544, 1333, 295, 15667, 5178, 13], "temperature": 0.0, "avg_logprob": -0.08845697811671666, "compression_ratio": 1.728125, "no_speech_prob": 0.0001312701788265258}, {"id": 37, "seek": 8200, "start": 99.8, "end": 102.6, "text": " And then, you know, soon after I moved to Berlin to do this,", "tokens": [400, 550, 11, 291, 458, 11, 2321, 934, 286, 4259, 281, 13848, 281, 360, 341, 11], "temperature": 0.0, "avg_logprob": -0.08845697811671666, "compression_ratio": 1.728125, "no_speech_prob": 0.0001312701788265258}, {"id": 38, "seek": 8200, "start": 102.6, "end": 105.4, "text": " I met Innis and we've been working together", "tokens": [286, 1131, 682, 10661, 293, 321, 600, 668, 1364, 1214], "temperature": 0.0, "avg_logprob": -0.08845697811671666, "compression_ratio": 1.728125, "no_speech_prob": 0.0001312701788265258}, {"id": 39, "seek": 8200, "start": 105.4, "end": 106.4, "text": " since on these things.", "tokens": [1670, 322, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.08845697811671666, "compression_ratio": 1.728125, "no_speech_prob": 0.0001312701788265258}, {"id": 40, "seek": 8200, "start": 106.4, "end": 109.0, "text": " And I think, you know, we kind of have a nice complementarity", "tokens": [400, 286, 519, 11, 291, 458, 11, 321, 733, 295, 362, 257, 1481, 17103, 17409], "temperature": 0.0, "avg_logprob": -0.08845697811671666, "compression_ratio": 1.728125, "no_speech_prob": 0.0001312701788265258}, {"id": 41, "seek": 10900, "start": 109.0, "end": 112.6, "text": " of things and she is the lead developer", "tokens": [295, 721, 293, 750, 307, 264, 1477, 10754], "temperature": 0.0, "avg_logprob": -0.09969146945808507, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.00021306145936250687}, {"id": 42, "seek": 10900, "start": 112.6, "end": 114.2, "text": " of our annotation tool, Prodigy,", "tokens": [295, 527, 48654, 2290, 11, 1705, 25259, 88, 11], "temperature": 0.0, "avg_logprob": -0.09969146945808507, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.00021306145936250687}, {"id": 43, "seek": 10900, "start": 114.2, "end": 116.4, "text": " and has also been working on Spacey", "tokens": [293, 575, 611, 668, 1364, 322, 8705, 88], "temperature": 0.0, "avg_logprob": -0.09969146945808507, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.00021306145936250687}, {"id": 44, "seek": 10900, "start": 116.4, "end": 119.0, "text": " pretty much since the first release.", "tokens": [1238, 709, 1670, 264, 700, 4374, 13], "temperature": 0.0, "avg_logprob": -0.09969146945808507, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.00021306145936250687}, {"id": 45, "seek": 10900, "start": 119.0, "end": 121.0, "text": " Okay, so I included this slide,", "tokens": [1033, 11, 370, 286, 5556, 341, 4137, 11], "temperature": 0.0, "avg_logprob": -0.09969146945808507, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.00021306145936250687}, {"id": 46, "seek": 10900, "start": 121.0, "end": 122.8, "text": " which we normally actually give this", "tokens": [597, 321, 5646, 767, 976, 341], "temperature": 0.0, "avg_logprob": -0.09969146945808507, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.00021306145936250687}, {"id": 47, "seek": 10900, "start": 122.8, "end": 124.4, "text": " when we talk to companies specifically,", "tokens": [562, 321, 751, 281, 3431, 4682, 11], "temperature": 0.0, "avg_logprob": -0.09969146945808507, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.00021306145936250687}, {"id": 48, "seek": 10900, "start": 124.4, "end": 126.4, "text": " but I think that it's a good thing to include,", "tokens": [457, 286, 519, 300, 309, 311, 257, 665, 551, 281, 4090, 11], "temperature": 0.0, "avg_logprob": -0.09969146945808507, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.00021306145936250687}, {"id": 49, "seek": 10900, "start": 126.4, "end": 128.4, "text": " to give you a bit of, you know,", "tokens": [281, 976, 291, 257, 857, 295, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.09969146945808507, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.00021306145936250687}, {"id": 50, "seek": 10900, "start": 128.4, "end": 130.6, "text": " this is what we tell people about what we do", "tokens": [341, 307, 437, 321, 980, 561, 466, 437, 321, 360], "temperature": 0.0, "avg_logprob": -0.09969146945808507, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.00021306145936250687}, {"id": 51, "seek": 10900, "start": 130.6, "end": 133.0, "text": " and how we make money and how the company works.", "tokens": [293, 577, 321, 652, 1460, 293, 577, 264, 2237, 1985, 13], "temperature": 0.0, "avg_logprob": -0.09969146945808507, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.00021306145936250687}, {"id": 52, "seek": 10900, "start": 133.0, "end": 134.8, "text": " And I think that this is a very valid question", "tokens": [400, 286, 519, 300, 341, 307, 257, 588, 7363, 1168], "temperature": 0.0, "avg_logprob": -0.09969146945808507, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.00021306145936250687}, {"id": 53, "seek": 10900, "start": 134.8, "end": 137.0, "text": " that people would have about an open source library.", "tokens": [300, 561, 576, 362, 466, 364, 1269, 4009, 6405, 13], "temperature": 0.0, "avg_logprob": -0.09969146945808507, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.00021306145936250687}, {"id": 54, "seek": 10900, "start": 137.0, "end": 138.4, "text": " It's like, well, why are you doing this?", "tokens": [467, 311, 411, 11, 731, 11, 983, 366, 291, 884, 341, 30], "temperature": 0.0, "avg_logprob": -0.09969146945808507, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.00021306145936250687}, {"id": 55, "seek": 13840, "start": 138.4, "end": 140.20000000000002, "text": " And, you know, how does it fit into the rest", "tokens": [400, 11, 291, 458, 11, 577, 775, 309, 3318, 666, 264, 1472], "temperature": 0.0, "avg_logprob": -0.12547004545057142, "compression_ratio": 1.6731391585760518, "no_speech_prob": 0.00010218915849691257}, {"id": 56, "seek": 13840, "start": 140.20000000000002, "end": 142.6, "text": " of your projects and plans?", "tokens": [295, 428, 4455, 293, 5482, 30], "temperature": 0.0, "avg_logprob": -0.12547004545057142, "compression_ratio": 1.6731391585760518, "no_speech_prob": 0.00010218915849691257}, {"id": 57, "seek": 13840, "start": 142.6, "end": 145.20000000000002, "text": " So the Explain it like I'm 5 version,", "tokens": [407, 264, 39574, 309, 411, 286, 478, 1025, 3037, 11], "temperature": 0.0, "avg_logprob": -0.12547004545057142, "compression_ratio": 1.6731391585760518, "no_speech_prob": 0.00010218915849691257}, {"id": 58, "seek": 13840, "start": 145.20000000000002, "end": 147.4, "text": " which I guess is also the Explain it like I'm", "tokens": [597, 286, 2041, 307, 611, 264, 39574, 309, 411, 286, 478], "temperature": 0.0, "avg_logprob": -0.12547004545057142, "compression_ratio": 1.6731391585760518, "no_speech_prob": 0.00010218915849691257}, {"id": 59, "seek": 13840, "start": 147.4, "end": 150.4, "text": " senior management version, is we give an analogy.", "tokens": [7965, 4592, 3037, 11, 307, 321, 976, 364, 21663, 13], "temperature": 0.0, "avg_logprob": -0.12547004545057142, "compression_ratio": 1.6731391585760518, "no_speech_prob": 0.00010218915849691257}, {"id": 60, "seek": 13840, "start": 150.4, "end": 152.0, "text": " It's kind of like a boutique kitchen.", "tokens": [467, 311, 733, 295, 411, 257, 15738, 1925, 6525, 13], "temperature": 0.0, "avg_logprob": -0.12547004545057142, "compression_ratio": 1.6731391585760518, "no_speech_prob": 0.00010218915849691257}, {"id": 61, "seek": 13840, "start": 152.0, "end": 154.8, "text": " So the free recipes we publish online,", "tokens": [407, 264, 1737, 13035, 321, 11374, 2950, 11], "temperature": 0.0, "avg_logprob": -0.12547004545057142, "compression_ratio": 1.6731391585760518, "no_speech_prob": 0.00010218915849691257}, {"id": 62, "seek": 13840, "start": 154.8, "end": 157.20000000000002, "text": " you can see is kind of like the open source software.", "tokens": [291, 393, 536, 307, 733, 295, 411, 264, 1269, 4009, 4722, 13], "temperature": 0.0, "avg_logprob": -0.12547004545057142, "compression_ratio": 1.6731391585760518, "no_speech_prob": 0.00010218915849691257}, {"id": 63, "seek": 13840, "start": 157.20000000000002, "end": 160.0, "text": " So that's Spacey, Think, et cetera.", "tokens": [407, 300, 311, 8705, 88, 11, 6557, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.12547004545057142, "compression_ratio": 1.6731391585760518, "no_speech_prob": 0.00010218915849691257}, {"id": 64, "seek": 13840, "start": 160.0, "end": 161.4, "text": " At the start of the company, especially,", "tokens": [1711, 264, 722, 295, 264, 2237, 11, 2318, 11], "temperature": 0.0, "avg_logprob": -0.12547004545057142, "compression_ratio": 1.6731391585760518, "no_speech_prob": 0.00010218915849691257}, {"id": 65, "seek": 13840, "start": 161.4, "end": 163.6, "text": " we were doing consulting, which I'm happy to say", "tokens": [321, 645, 884, 23682, 11, 597, 286, 478, 2055, 281, 584], "temperature": 0.0, "avg_logprob": -0.12547004545057142, "compression_ratio": 1.6731391585760518, "no_speech_prob": 0.00010218915849691257}, {"id": 66, "seek": 13840, "start": 163.6, "end": 166.8, "text": " we've been able to wind down over the last six months", "tokens": [321, 600, 668, 1075, 281, 2468, 760, 670, 264, 1036, 2309, 2493], "temperature": 0.0, "avg_logprob": -0.12547004545057142, "compression_ratio": 1.6731391585760518, "no_speech_prob": 0.00010218915849691257}, {"id": 67, "seek": 16680, "start": 166.8, "end": 168.60000000000002, "text": " and focus on our products.", "tokens": [293, 1879, 322, 527, 3383, 13], "temperature": 0.0, "avg_logprob": -0.0653712128939694, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00012141357728978619}, {"id": 68, "seek": 16680, "start": 168.60000000000002, "end": 171.8, "text": " And then we also focus on a line of kitchen gadgets,", "tokens": [400, 550, 321, 611, 1879, 322, 257, 1622, 295, 6525, 37635, 11], "temperature": 0.0, "avg_logprob": -0.0653712128939694, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00012141357728978619}, {"id": 69, "seek": 16680, "start": 171.8, "end": 173.4, "text": " which is things like Prodigy.", "tokens": [597, 307, 721, 411, 1705, 25259, 88, 13], "temperature": 0.0, "avg_logprob": -0.0653712128939694, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00012141357728978619}, {"id": 70, "seek": 16680, "start": 173.4, "end": 175.4, "text": " These are these downloadable tools to use", "tokens": [1981, 366, 613, 5484, 712, 3873, 281, 764], "temperature": 0.0, "avg_logprob": -0.0653712128939694, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00012141357728978619}, {"id": 71, "seek": 16680, "start": 175.4, "end": 177.20000000000002, "text": " alongside the open source software.", "tokens": [12385, 264, 1269, 4009, 4722, 13], "temperature": 0.0, "avg_logprob": -0.0653712128939694, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00012141357728978619}, {"id": 72, "seek": 16680, "start": 177.20000000000002, "end": 179.4, "text": " And soon we'll have this sort of premium ingredients,", "tokens": [400, 2321, 321, 603, 362, 341, 1333, 295, 12049, 6952, 11], "temperature": 0.0, "avg_logprob": -0.0653712128939694, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00012141357728978619}, {"id": 73, "seek": 16680, "start": 179.4, "end": 181.4, "text": " which are the pre-trained models.", "tokens": [597, 366, 264, 659, 12, 17227, 2001, 5245, 13], "temperature": 0.0, "avg_logprob": -0.0653712128939694, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00012141357728978619}, {"id": 74, "seek": 16680, "start": 181.4, "end": 184.4, "text": " So the thing that we don't do here is enterprise support,", "tokens": [407, 264, 551, 300, 321, 500, 380, 360, 510, 307, 14132, 1406, 11], "temperature": 0.0, "avg_logprob": -0.0653712128939694, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00012141357728978619}, {"id": 75, "seek": 16680, "start": 184.4, "end": 186.20000000000002, "text": " which I guess is probably the most common way", "tokens": [597, 286, 2041, 307, 1391, 264, 881, 2689, 636], "temperature": 0.0, "avg_logprob": -0.0653712128939694, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00012141357728978619}, {"id": 76, "seek": 16680, "start": 186.20000000000002, "end": 189.4, "text": " that people, you know, fund open source software", "tokens": [300, 561, 11, 291, 458, 11, 2374, 1269, 4009, 4722], "temperature": 0.0, "avg_logprob": -0.0653712128939694, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00012141357728978619}, {"id": 77, "seek": 16680, "start": 189.4, "end": 191.4, "text": " or imagine that they'll fund open source software", "tokens": [420, 3811, 300, 436, 603, 2374, 1269, 4009, 4722], "temperature": 0.0, "avg_logprob": -0.0653712128939694, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00012141357728978619}, {"id": 78, "seek": 16680, "start": 191.4, "end": 192.8, "text": " with a business model.", "tokens": [365, 257, 1606, 2316, 13], "temperature": 0.0, "avg_logprob": -0.0653712128939694, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00012141357728978619}, {"id": 79, "seek": 16680, "start": 192.8, "end": 196.20000000000002, "text": " And we really don't like this because we want our software", "tokens": [400, 321, 534, 500, 380, 411, 341, 570, 321, 528, 527, 4722], "temperature": 0.0, "avg_logprob": -0.0653712128939694, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00012141357728978619}, {"id": 80, "seek": 19620, "start": 196.2, "end": 199.0, "text": " to be as easy to use as possible and as transparent as possible", "tokens": [281, 312, 382, 1858, 281, 764, 382, 1944, 293, 382, 12737, 382, 1944], "temperature": 0.0, "avg_logprob": -0.0905210942397883, "compression_ratio": 1.9427609427609427, "no_speech_prob": 0.0001464532979298383}, {"id": 81, "seek": 19620, "start": 199.0, "end": 200.79999999999998, "text": " and the documentation to be good.", "tokens": [293, 264, 14333, 281, 312, 665, 13], "temperature": 0.0, "avg_logprob": -0.0905210942397883, "compression_ratio": 1.9427609427609427, "no_speech_prob": 0.0001464532979298383}, {"id": 82, "seek": 19620, "start": 200.79999999999998, "end": 203.2, "text": " So I think it's kind of weird to have this thing", "tokens": [407, 286, 519, 309, 311, 733, 295, 3657, 281, 362, 341, 551], "temperature": 0.0, "avg_logprob": -0.0905210942397883, "compression_ratio": 1.9427609427609427, "no_speech_prob": 0.0001464532979298383}, {"id": 83, "seek": 19620, "start": 203.2, "end": 206.39999999999998, "text": " where you have explicitly a plan that we're going to make", "tokens": [689, 291, 362, 20803, 257, 1393, 300, 321, 434, 516, 281, 652], "temperature": 0.0, "avg_logprob": -0.0905210942397883, "compression_ratio": 1.9427609427609427, "no_speech_prob": 0.0001464532979298383}, {"id": 84, "seek": 19620, "start": 206.39999999999998, "end": 208.39999999999998, "text": " our free stuff as good as possible,", "tokens": [527, 1737, 1507, 382, 665, 382, 1944, 11], "temperature": 0.0, "avg_logprob": -0.0905210942397883, "compression_ratio": 1.9427609427609427, "no_speech_prob": 0.0001464532979298383}, {"id": 85, "seek": 19620, "start": 208.39999999999998, "end": 210.6, "text": " and then we're going to have this service", "tokens": [293, 550, 321, 434, 516, 281, 362, 341, 2643], "temperature": 0.0, "avg_logprob": -0.0905210942397883, "compression_ratio": 1.9427609427609427, "no_speech_prob": 0.0001464532979298383}, {"id": 86, "seek": 19620, "start": 210.6, "end": 212.2, "text": " that we hope nobody...", "tokens": [300, 321, 1454, 5079, 485], "temperature": 0.0, "avg_logprob": -0.0905210942397883, "compression_ratio": 1.9427609427609427, "no_speech_prob": 0.0001464532979298383}, {"id": 87, "seek": 19620, "start": 212.2, "end": 213.6, "text": " We hope people pay us lots of money for,", "tokens": [492, 1454, 561, 1689, 505, 3195, 295, 1460, 337, 11], "temperature": 0.0, "avg_logprob": -0.0905210942397883, "compression_ratio": 1.9427609427609427, "no_speech_prob": 0.0001464532979298383}, {"id": 88, "seek": 19620, "start": 213.6, "end": 215.2, "text": " but we hope nobody uses.", "tokens": [457, 321, 1454, 5079, 4960, 13], "temperature": 0.0, "avg_logprob": -0.0905210942397883, "compression_ratio": 1.9427609427609427, "no_speech_prob": 0.0001464532979298383}, {"id": 89, "seek": 19620, "start": 215.2, "end": 216.6, "text": " And that's kind of weird, right?", "tokens": [400, 300, 311, 733, 295, 3657, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.0905210942397883, "compression_ratio": 1.9427609427609427, "no_speech_prob": 0.0001464532979298383}, {"id": 90, "seek": 19620, "start": 216.6, "end": 219.0, "text": " It's kind of weird to have a company that, you know,", "tokens": [467, 311, 733, 295, 3657, 281, 362, 257, 2237, 300, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.0905210942397883, "compression_ratio": 1.9427609427609427, "no_speech_prob": 0.0001464532979298383}, {"id": 91, "seek": 19620, "start": 219.0, "end": 221.79999999999998, "text": " you hope that your paid offering is really poor value to people.", "tokens": [291, 1454, 300, 428, 4835, 8745, 307, 534, 4716, 2158, 281, 561, 13], "temperature": 0.0, "avg_logprob": -0.0905210942397883, "compression_ratio": 1.9427609427609427, "no_speech_prob": 0.0001464532979298383}, {"id": 92, "seek": 19620, "start": 221.79999999999998, "end": 224.39999999999998, "text": " And so we don't think that that's a good way to do it.", "tokens": [400, 370, 321, 500, 380, 519, 300, 300, 311, 257, 665, 636, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.0905210942397883, "compression_ratio": 1.9427609427609427, "no_speech_prob": 0.0001464532979298383}, {"id": 93, "seek": 22440, "start": 224.4, "end": 227.6, "text": " And so instead we have the downloadable tools,", "tokens": [400, 370, 2602, 321, 362, 264, 5484, 712, 3873, 11], "temperature": 0.0, "avg_logprob": -0.10904966524946011, "compression_ratio": 1.7049180327868851, "no_speech_prob": 6.497055437648669e-05}, {"id": 94, "seek": 22440, "start": 227.6, "end": 229.8, "text": " I think is a good way to, you know,", "tokens": [286, 519, 307, 257, 665, 636, 281, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.10904966524946011, "compression_ratio": 1.7049180327868851, "no_speech_prob": 6.497055437648669e-05}, {"id": 95, "seek": 22440, "start": 229.8, "end": 231.8, "text": " we have something which works alongside spaCy", "tokens": [321, 362, 746, 597, 1985, 12385, 32543, 34, 88], "temperature": 0.0, "avg_logprob": -0.10904966524946011, "compression_ratio": 1.7049180327868851, "no_speech_prob": 6.497055437648669e-05}, {"id": 96, "seek": 22440, "start": 231.8, "end": 236.8, "text": " and I think is useful to people who use spaCy as well.", "tokens": [293, 286, 519, 307, 4420, 281, 561, 567, 764, 32543, 34, 88, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.10904966524946011, "compression_ratio": 1.7049180327868851, "no_speech_prob": 6.497055437648669e-05}, {"id": 97, "seek": 22440, "start": 236.8, "end": 241.6, "text": " OK, so, you know, onto the sort of main content of the talk", "tokens": [2264, 11, 370, 11, 291, 458, 11, 3911, 264, 1333, 295, 2135, 2701, 295, 264, 751], "temperature": 0.0, "avg_logprob": -0.10904966524946011, "compression_ratio": 1.7049180327868851, "no_speech_prob": 6.497055437648669e-05}, {"id": 98, "seek": 22440, "start": 241.6, "end": 244.6, "text": " and, you know, the bit that I'll be talking about.", "tokens": [293, 11, 291, 458, 11, 264, 857, 300, 286, 603, 312, 1417, 466, 13], "temperature": 0.0, "avg_logprob": -0.10904966524946011, "compression_ratio": 1.7049180327868851, "no_speech_prob": 6.497055437648669e-05}, {"id": 99, "seek": 22440, "start": 244.6, "end": 248.0, "text": " So I'm going to talk to you about the syntactic parser", "tokens": [407, 286, 478, 516, 281, 751, 281, 291, 466, 264, 23980, 19892, 21156, 260], "temperature": 0.0, "avg_logprob": -0.10904966524946011, "compression_ratio": 1.7049180327868851, "no_speech_prob": 6.497055437648669e-05}, {"id": 100, "seek": 22440, "start": 248.0, "end": 250.8, "text": " within spaCy, the natural language processing library", "tokens": [1951, 32543, 34, 88, 11, 264, 3303, 2856, 9007, 6405], "temperature": 0.0, "avg_logprob": -0.10904966524946011, "compression_ratio": 1.7049180327868851, "no_speech_prob": 6.497055437648669e-05}, {"id": 101, "seek": 22440, "start": 250.8, "end": 252.0, "text": " that we use.", "tokens": [300, 321, 764, 13], "temperature": 0.0, "avg_logprob": -0.10904966524946011, "compression_ratio": 1.7049180327868851, "no_speech_prob": 6.497055437648669e-05}, {"id": 102, "seek": 25200, "start": 252.0, "end": 255.0, "text": " And so before I do it...", "tokens": [400, 370, 949, 286, 360, 309, 485], "temperature": 0.0, "avg_logprob": -0.11656283480780465, "compression_ratio": 1.715481171548117, "no_speech_prob": 6.91816458129324e-05}, {"id": 103, "seek": 25200, "start": 255.0, "end": 258.0, "text": " So this is kind of what it looks like as, you know,", "tokens": [407, 341, 307, 733, 295, 437, 309, 1542, 411, 382, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.11656283480780465, "compression_ratio": 1.715481171548117, "no_speech_prob": 6.91816458129324e-05}, {"id": 104, "seek": 25200, "start": 258.0, "end": 259.8, "text": " sort of visualised as an output.", "tokens": [1333, 295, 5056, 2640, 382, 364, 5598, 13], "temperature": 0.0, "avg_logprob": -0.11656283480780465, "compression_ratio": 1.715481171548117, "no_speech_prob": 6.91816458129324e-05}, {"id": 105, "seek": 25200, "start": 259.8, "end": 262.2, "text": " So it's this sort of tree-based structure", "tokens": [407, 309, 311, 341, 1333, 295, 4230, 12, 6032, 3877], "temperature": 0.0, "avg_logprob": -0.11656283480780465, "compression_ratio": 1.715481171548117, "no_speech_prob": 6.91816458129324e-05}, {"id": 106, "seek": 25200, "start": 262.2, "end": 266.8, "text": " that gives you these syntactic relationships between words.", "tokens": [300, 2709, 291, 613, 23980, 19892, 6159, 1296, 2283, 13], "temperature": 0.0, "avg_logprob": -0.11656283480780465, "compression_ratio": 1.715481171548117, "no_speech_prob": 6.91816458129324e-05}, {"id": 107, "seek": 25200, "start": 266.8, "end": 271.0, "text": " So the way that you should read this here is that", "tokens": [407, 264, 636, 300, 291, 820, 1401, 341, 510, 307, 300], "temperature": 0.0, "avg_logprob": -0.11656283480780465, "compression_ratio": 1.715481171548117, "no_speech_prob": 6.91816458129324e-05}, {"id": 108, "seek": 25200, "start": 271.0, "end": 273.6, "text": " the arrow pointing from this word to this word", "tokens": [264, 11610, 12166, 490, 341, 1349, 281, 341, 1349], "temperature": 0.0, "avg_logprob": -0.11656283480780465, "compression_ratio": 1.715481171548117, "no_speech_prob": 6.91816458129324e-05}, {"id": 109, "seek": 25200, "start": 273.6, "end": 277.4, "text": " means that Apple is a child of looking in the tree.", "tokens": [1355, 300, 6373, 307, 257, 1440, 295, 1237, 294, 264, 4230, 13], "temperature": 0.0, "avg_logprob": -0.11656283480780465, "compression_ratio": 1.715481171548117, "no_speech_prob": 6.91816458129324e-05}, {"id": 110, "seek": 25200, "start": 277.4, "end": 280.0, "text": " And it's a child with this relationship and such.", "tokens": [400, 309, 311, 257, 1440, 365, 341, 2480, 293, 1270, 13], "temperature": 0.0, "avg_logprob": -0.11656283480780465, "compression_ratio": 1.715481171548117, "no_speech_prob": 6.91816458129324e-05}, {"id": 111, "seek": 28000, "start": 280.0, "end": 282.4, "text": " In other words, Apple is the subject of looking.", "tokens": [682, 661, 2283, 11, 6373, 307, 264, 3983, 295, 1237, 13], "temperature": 0.0, "avg_logprob": -0.11171819161677705, "compression_ratio": 1.8041237113402062, "no_speech_prob": 1.6436631995020434e-05}, {"id": 112, "seek": 28000, "start": 282.4, "end": 285.6, "text": " And is is an auxiliary verb attached to looking.", "tokens": [400, 307, 307, 364, 43741, 9595, 8570, 281, 1237, 13], "temperature": 0.0, "avg_logprob": -0.11171819161677705, "compression_ratio": 1.8041237113402062, "no_speech_prob": 1.6436631995020434e-05}, {"id": 113, "seek": 28000, "start": 285.6, "end": 289.4, "text": " And then at is a prepositional phrase attached to looking.", "tokens": [400, 550, 412, 307, 257, 2666, 329, 2628, 9535, 8570, 281, 1237, 13], "temperature": 0.0, "avg_logprob": -0.11171819161677705, "compression_ratio": 1.8041237113402062, "no_speech_prob": 1.6436631995020434e-05}, {"id": 114, "seek": 28000, "start": 289.4, "end": 291.6, "text": " So these sorts of relationships tell you about", "tokens": [407, 613, 7527, 295, 6159, 980, 291, 466], "temperature": 0.0, "avg_logprob": -0.11171819161677705, "compression_ratio": 1.8041237113402062, "no_speech_prob": 1.6436631995020434e-05}, {"id": 115, "seek": 28000, "start": 291.6, "end": 293.2, "text": " the syntactic structure of the sentence", "tokens": [264, 23980, 19892, 3877, 295, 264, 8174], "temperature": 0.0, "avg_logprob": -0.11171819161677705, "compression_ratio": 1.8041237113402062, "no_speech_prob": 1.6436631995020434e-05}, {"id": 116, "seek": 28000, "start": 293.2, "end": 296.0, "text": " and basically help you get at the who did what to whom", "tokens": [293, 1936, 854, 291, 483, 412, 264, 567, 630, 437, 281, 7101], "temperature": 0.0, "avg_logprob": -0.11171819161677705, "compression_ratio": 1.8041237113402062, "no_speech_prob": 1.6436631995020434e-05}, {"id": 117, "seek": 28000, "start": 296.0, "end": 297.8, "text": " sort of relationships in the sentence", "tokens": [1333, 295, 6159, 294, 264, 8174], "temperature": 0.0, "avg_logprob": -0.11171819161677705, "compression_ratio": 1.8041237113402062, "no_speech_prob": 1.6436631995020434e-05}, {"id": 118, "seek": 28000, "start": 297.8, "end": 299.8, "text": " and also to extract phrases and things.", "tokens": [293, 611, 281, 8947, 20312, 293, 721, 13], "temperature": 0.0, "avg_logprob": -0.11171819161677705, "compression_ratio": 1.8041237113402062, "no_speech_prob": 1.6436631995020434e-05}, {"id": 119, "seek": 28000, "start": 299.8, "end": 302.8, "text": " So, for instance, here, to make the thing more easy to read,", "tokens": [407, 11, 337, 5197, 11, 510, 11, 281, 652, 264, 551, 544, 1858, 281, 1401, 11], "temperature": 0.0, "avg_logprob": -0.11171819161677705, "compression_ratio": 1.8041237113402062, "no_speech_prob": 1.6436631995020434e-05}, {"id": 120, "seek": 28000, "start": 302.8, "end": 305.4, "text": " we've merged UK Startup, which is, you know,", "tokens": [321, 600, 36427, 7051, 6481, 1010, 11, 597, 307, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.11171819161677705, "compression_ratio": 1.8041237113402062, "no_speech_prob": 1.6436631995020434e-05}, {"id": 121, "seek": 28000, "start": 305.4, "end": 308.8, "text": " a sort of basic noun phrase into one unit.", "tokens": [257, 1333, 295, 3875, 23307, 9535, 666, 472, 4985, 13], "temperature": 0.0, "avg_logprob": -0.11171819161677705, "compression_ratio": 1.8041237113402062, "no_speech_prob": 1.6436631995020434e-05}, {"id": 122, "seek": 30880, "start": 308.8, "end": 311.8, "text": " And you can find these sorts of phrases more easily from,", "tokens": [400, 291, 393, 915, 613, 7527, 295, 20312, 544, 3612, 490, 11], "temperature": 0.0, "avg_logprob": -0.10554293020447689, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.00011405795521568507}, {"id": 123, "seek": 30880, "start": 311.8, "end": 313.6, "text": " given the syntactic structure.", "tokens": [2212, 264, 23980, 19892, 3877, 13], "temperature": 0.0, "avg_logprob": -0.10554293020447689, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.00011405795521568507}, {"id": 124, "seek": 30880, "start": 313.6, "end": 316.8, "text": " And just above here, we've got an example of, you know,", "tokens": [400, 445, 3673, 510, 11, 321, 600, 658, 364, 1365, 295, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.10554293020447689, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.00011405795521568507}, {"id": 125, "seek": 30880, "start": 316.8, "end": 319.8, "text": " what the code looks like to actually get the syntactic structure", "tokens": [437, 264, 3089, 1542, 411, 281, 767, 483, 264, 23980, 19892, 3877], "temperature": 0.0, "avg_logprob": -0.10554293020447689, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.00011405795521568507}, {"id": 126, "seek": 30880, "start": 319.8, "end": 321.6, "text": " and navigate the tree.", "tokens": [293, 12350, 264, 4230, 13], "temperature": 0.0, "avg_logprob": -0.10554293020447689, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.00011405795521568507}, {"id": 127, "seek": 30880, "start": 321.6, "end": 326.2, "text": " In spaCy, you just get this NLP object after loading the models.", "tokens": [682, 32543, 34, 88, 11, 291, 445, 483, 341, 426, 45196, 2657, 934, 15114, 264, 5245, 13], "temperature": 0.0, "avg_logprob": -0.10554293020447689, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.00011405795521568507}, {"id": 128, "seek": 30880, "start": 326.2, "end": 329.2, "text": " And you just use that as a function that you feed text", "tokens": [400, 291, 445, 764, 300, 382, 257, 2445, 300, 291, 3154, 2487], "temperature": 0.0, "avg_logprob": -0.10554293020447689, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.00011405795521568507}, {"id": 129, "seek": 30880, "start": 329.2, "end": 332.2, "text": " or pipe text through if you've got a sequence of texts.", "tokens": [420, 11240, 2487, 807, 498, 291, 600, 658, 257, 8310, 295, 15765, 13], "temperature": 0.0, "avg_logprob": -0.10554293020447689, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.00011405795521568507}, {"id": 130, "seek": 30880, "start": 332.2, "end": 335.40000000000003, "text": " And given that, you get a document object,", "tokens": [400, 2212, 300, 11, 291, 483, 257, 4166, 2657, 11], "temperature": 0.0, "avg_logprob": -0.10554293020447689, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.00011405795521568507}, {"id": 131, "seek": 30880, "start": 335.40000000000003, "end": 337.40000000000003, "text": " which you can just use as an iterable.", "tokens": [597, 291, 393, 445, 764, 382, 364, 17138, 712, 13], "temperature": 0.0, "avg_logprob": -0.10554293020447689, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.00011405795521568507}, {"id": 132, "seek": 33740, "start": 337.4, "end": 339.59999999999997, "text": " And from the tokens, you get attributes", "tokens": [400, 490, 264, 22667, 11, 291, 483, 17212], "temperature": 0.0, "avg_logprob": -0.10481292884666603, "compression_ratio": 1.75, "no_speech_prob": 7.600695971632376e-05}, {"id": 133, "seek": 33740, "start": 339.59999999999997, "end": 341.59999999999997, "text": " that you can use to navigate the tree.", "tokens": [300, 291, 393, 764, 281, 12350, 264, 4230, 13], "temperature": 0.0, "avg_logprob": -0.10481292884666603, "compression_ratio": 1.75, "no_speech_prob": 7.600695971632376e-05}, {"id": 134, "seek": 33740, "start": 341.59999999999997, "end": 344.59999999999997, "text": " So, for instance, here, the dependency relationship", "tokens": [407, 11, 337, 5197, 11, 510, 11, 264, 33621, 2480], "temperature": 0.0, "avg_logprob": -0.10481292884666603, "compression_ratio": 1.75, "no_speech_prob": 7.600695971632376e-05}, {"id": 135, "seek": 33740, "start": 344.59999999999997, "end": 346.59999999999997, "text": " is just a.dep.", "tokens": [307, 445, 257, 2411, 19929, 13], "temperature": 0.0, "avg_logprob": -0.10481292884666603, "compression_ratio": 1.75, "no_speech_prob": 7.600695971632376e-05}, {"id": 136, "seek": 33740, "start": 346.59999999999997, "end": 350.0, "text": " By default, that's an integer key, integer ID,", "tokens": [3146, 7576, 11, 300, 311, 364, 24922, 2141, 11, 24922, 7348, 11], "temperature": 0.0, "avg_logprob": -0.10481292884666603, "compression_ratio": 1.75, "no_speech_prob": 7.600695971632376e-05}, {"id": 137, "seek": 33740, "start": 350.0, "end": 352.0, "text": " because everything's kind of coded to an integer", "tokens": [570, 1203, 311, 733, 295, 34874, 281, 364, 24922], "temperature": 0.0, "avg_logprob": -0.10481292884666603, "compression_ratio": 1.75, "no_speech_prob": 7.600695971632376e-05}, {"id": 138, "seek": 33740, "start": 352.0, "end": 355.2, "text": " for easy and efficient processing.", "tokens": [337, 1858, 293, 7148, 9007, 13], "temperature": 0.0, "avg_logprob": -0.10481292884666603, "compression_ratio": 1.75, "no_speech_prob": 7.600695971632376e-05}, {"id": 139, "seek": 33740, "start": 355.2, "end": 358.4, "text": " But then you can get the text value with an underscore as well.", "tokens": [583, 550, 291, 393, 483, 264, 2487, 2158, 365, 364, 37556, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.10481292884666603, "compression_ratio": 1.75, "no_speech_prob": 7.600695971632376e-05}, {"id": 140, "seek": 33740, "start": 358.4, "end": 361.2, "text": " And then you can navigate up the tree with.head.", "tokens": [400, 550, 291, 393, 12350, 493, 264, 4230, 365, 2411, 1934, 13], "temperature": 0.0, "avg_logprob": -0.10481292884666603, "compression_ratio": 1.75, "no_speech_prob": 7.600695971632376e-05}, {"id": 141, "seek": 33740, "start": 361.2, "end": 363.2, "text": " And then you can look at the left and right children", "tokens": [400, 550, 291, 393, 574, 412, 264, 1411, 293, 558, 2227], "temperature": 0.0, "avg_logprob": -0.10481292884666603, "compression_ratio": 1.75, "no_speech_prob": 7.600695971632376e-05}, {"id": 142, "seek": 33740, "start": 363.2, "end": 364.2, "text": " of the tree as well.", "tokens": [295, 264, 4230, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.10481292884666603, "compression_ratio": 1.75, "no_speech_prob": 7.600695971632376e-05}, {"id": 143, "seek": 33740, "start": 364.2, "end": 366.59999999999997, "text": " So we try to have a root API that makes it easy to use", "tokens": [407, 321, 853, 281, 362, 257, 5593, 9362, 300, 1669, 309, 1858, 281, 764], "temperature": 0.0, "avg_logprob": -0.10481292884666603, "compression_ratio": 1.75, "no_speech_prob": 7.600695971632376e-05}, {"id": 144, "seek": 36660, "start": 366.6, "end": 369.0, "text": " these dependency relationships.", "tokens": [613, 33621, 6159, 13], "temperature": 0.0, "avg_logprob": -0.1484025698989185, "compression_ratio": 1.698961937716263, "no_speech_prob": 3.535396172082983e-05}, {"id": 145, "seek": 36660, "start": 369.0, "end": 373.0, "text": " You know, so that just getting dependency parses,", "tokens": [509, 458, 11, 370, 300, 445, 1242, 33621, 21156, 279, 11], "temperature": 0.0, "avg_logprob": -0.1484025698989185, "compression_ratio": 1.698961937716263, "no_speech_prob": 3.535396172082983e-05}, {"id": 146, "seek": 36660, "start": 373.0, "end": 375.0, "text": " you know, obviously, just the first step,", "tokens": [291, 458, 11, 2745, 11, 445, 264, 700, 1823, 11], "temperature": 0.0, "avg_logprob": -0.1484025698989185, "compression_ratio": 1.698961937716263, "no_speech_prob": 3.535396172082983e-05}, {"id": 147, "seek": 36660, "start": 375.0, "end": 376.40000000000003, "text": " you want to actually use it in some way.", "tokens": [291, 528, 281, 767, 764, 309, 294, 512, 636, 13], "temperature": 0.0, "avg_logprob": -0.1484025698989185, "compression_ratio": 1.698961937716263, "no_speech_prob": 3.535396172082983e-05}, {"id": 148, "seek": 36660, "start": 376.40000000000003, "end": 380.40000000000003, "text": " And that's why we have this API to make that easy.", "tokens": [400, 300, 311, 983, 321, 362, 341, 9362, 281, 652, 300, 1858, 13], "temperature": 0.0, "avg_logprob": -0.1484025698989185, "compression_ratio": 1.698961937716263, "no_speech_prob": 3.535396172082983e-05}, {"id": 149, "seek": 36660, "start": 380.40000000000003, "end": 383.40000000000003, "text": " So the question that always comes up with this,", "tokens": [407, 264, 1168, 300, 1009, 1487, 493, 365, 341, 11], "temperature": 0.0, "avg_logprob": -0.1484025698989185, "compression_ratio": 1.698961937716263, "no_speech_prob": 3.535396172082983e-05}, {"id": 150, "seek": 36660, "start": 383.40000000000003, "end": 385.6, "text": " and I think this is a very interesting thing", "tokens": [293, 286, 519, 341, 307, 257, 588, 1880, 551], "temperature": 0.0, "avg_logprob": -0.1484025698989185, "compression_ratio": 1.698961937716263, "no_speech_prob": 3.535396172082983e-05}, {"id": 151, "seek": 36660, "start": 385.6, "end": 387.6, "text": " for the field in general, is, you know,", "tokens": [337, 264, 2519, 294, 2674, 11, 307, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.1484025698989185, "compression_ratio": 1.698961937716263, "no_speech_prob": 3.535396172082983e-05}, {"id": 152, "seek": 36660, "start": 387.6, "end": 389.0, "text": " what's the point of parsing?", "tokens": [437, 311, 264, 935, 295, 21156, 278, 30], "temperature": 0.0, "avg_logprob": -0.1484025698989185, "compression_ratio": 1.698961937716263, "no_speech_prob": 3.535396172082983e-05}, {"id": 153, "seek": 36660, "start": 389.0, "end": 391.8, "text": " What is this actually good for in terms of applications?", "tokens": [708, 307, 341, 767, 665, 337, 294, 2115, 295, 5821, 30], "temperature": 0.0, "avg_logprob": -0.1484025698989185, "compression_ratio": 1.698961937716263, "no_speech_prob": 3.535396172082983e-05}, {"id": 154, "seek": 36660, "start": 391.8, "end": 395.40000000000003, "text": " So Joav Goldberg is a very prominent parsing researcher.", "tokens": [407, 3139, 706, 6731, 6873, 307, 257, 588, 17034, 21156, 278, 21751, 13], "temperature": 0.0, "avg_logprob": -0.1484025698989185, "compression_ratio": 1.698961937716263, "no_speech_prob": 3.535396172082983e-05}, {"id": 155, "seek": 39540, "start": 395.4, "end": 397.0, "text": " And he's, you know, this is kind of the stuff", "tokens": [400, 415, 311, 11, 291, 458, 11, 341, 307, 733, 295, 264, 1507], "temperature": 0.0, "avg_logprob": -0.07996131212283404, "compression_ratio": 1.794788273615635, "no_speech_prob": 4.005513619631529e-05}, {"id": 156, "seek": 39540, "start": 397.0, "end": 398.59999999999997, "text": " that he's studied for most of his career.", "tokens": [300, 415, 311, 9454, 337, 881, 295, 702, 3988, 13], "temperature": 0.0, "avg_logprob": -0.07996131212283404, "compression_ratio": 1.794788273615635, "no_speech_prob": 4.005513619631529e-05}, {"id": 157, "seek": 39540, "start": 398.59999999999997, "end": 402.0, "text": " And he's, you know, one of the more well-known parsing people.", "tokens": [400, 415, 311, 11, 291, 458, 11, 472, 295, 264, 544, 731, 12, 6861, 21156, 278, 561, 13], "temperature": 0.0, "avg_logprob": -0.07996131212283404, "compression_ratio": 1.794788273615635, "no_speech_prob": 4.005513619631529e-05}, {"id": 158, "seek": 39540, "start": 402.0, "end": 404.79999999999995, "text": " And so it's interesting to see him and other people reflect", "tokens": [400, 370, 309, 311, 1880, 281, 536, 796, 293, 661, 561, 5031], "temperature": 0.0, "avg_logprob": -0.07996131212283404, "compression_ratio": 1.794788273615635, "no_speech_prob": 4.005513619631529e-05}, {"id": 159, "seek": 39540, "start": 404.79999999999995, "end": 406.79999999999995, "text": " on this and say that he finds it fascinating", "tokens": [322, 341, 293, 584, 300, 415, 10704, 309, 10343], "temperature": 0.0, "avg_logprob": -0.07996131212283404, "compression_ratio": 1.794788273615635, "no_speech_prob": 4.005513619631529e-05}, {"id": 160, "seek": 39540, "start": 406.79999999999995, "end": 410.0, "text": " that even though we have so many best papers in NLP,", "tokens": [300, 754, 1673, 321, 362, 370, 867, 1151, 10577, 294, 426, 45196, 11], "temperature": 0.0, "avg_logprob": -0.07996131212283404, "compression_ratio": 1.794788273615635, "no_speech_prob": 4.005513619631529e-05}, {"id": 161, "seek": 39540, "start": 410.0, "end": 413.59999999999997, "text": " so it's kind of a high prestige thing to study parsing.", "tokens": [370, 309, 311, 733, 295, 257, 1090, 42531, 551, 281, 2979, 21156, 278, 13], "temperature": 0.0, "avg_logprob": -0.07996131212283404, "compression_ratio": 1.794788273615635, "no_speech_prob": 4.005513619631529e-05}, {"id": 162, "seek": 39540, "start": 413.59999999999997, "end": 416.79999999999995, "text": " But it seems like syntax is hardly used in practice", "tokens": [583, 309, 2544, 411, 28431, 307, 13572, 1143, 294, 3124], "temperature": 0.0, "avg_logprob": -0.07996131212283404, "compression_ratio": 1.794788273615635, "no_speech_prob": 4.005513619631529e-05}, {"id": 163, "seek": 39540, "start": 416.79999999999995, "end": 419.79999999999995, "text": " in, you know, most of these applications.", "tokens": [294, 11, 291, 458, 11, 881, 295, 613, 5821, 13], "temperature": 0.0, "avg_logprob": -0.07996131212283404, "compression_ratio": 1.794788273615635, "no_speech_prob": 4.005513619631529e-05}, {"id": 164, "seek": 39540, "start": 419.79999999999995, "end": 422.0, "text": " So the question is, you know, why is this?", "tokens": [407, 264, 1168, 307, 11, 291, 458, 11, 983, 307, 341, 30], "temperature": 0.0, "avg_logprob": -0.07996131212283404, "compression_ratio": 1.794788273615635, "no_speech_prob": 4.005513619631529e-05}, {"id": 165, "seek": 39540, "start": 422.0, "end": 424.79999999999995, "text": " Is it just that because parsing is based on trees", "tokens": [1119, 309, 445, 300, 570, 21156, 278, 307, 2361, 322, 5852], "temperature": 0.0, "avg_logprob": -0.07996131212283404, "compression_ratio": 1.794788273615635, "no_speech_prob": 4.005513619631529e-05}, {"id": 166, "seek": 42480, "start": 424.8, "end": 426.6, "text": " and structured predictions kind of fun to study,", "tokens": [293, 18519, 21264, 733, 295, 1019, 281, 2979, 11], "temperature": 0.0, "avg_logprob": -0.13578129553979681, "compression_ratio": 1.6996197718631179, "no_speech_prob": 9.911771485349163e-05}, {"id": 167, "seek": 42480, "start": 426.6, "end": 428.6, "text": " and there's all these deep algorithmic questions,", "tokens": [293, 456, 311, 439, 613, 2452, 9284, 299, 1651, 11], "temperature": 0.0, "avg_logprob": -0.13578129553979681, "compression_ratio": 1.6996197718631179, "no_speech_prob": 9.911771485349163e-05}, {"id": 168, "seek": 42480, "start": 428.6, "end": 431.8, "text": " is it just kind of this catnip to researchers?", "tokens": [307, 309, 445, 733, 295, 341, 3857, 77, 647, 281, 10309, 30], "temperature": 0.0, "avg_logprob": -0.13578129553979681, "compression_ratio": 1.6996197718631179, "no_speech_prob": 9.911771485349163e-05}, {"id": 169, "seek": 42480, "start": 431.8, "end": 436.40000000000003, "text": " And is it, does it have this kind of over-prominence", "tokens": [400, 307, 309, 11, 775, 309, 362, 341, 733, 295, 670, 12, 1424, 6981, 655], "temperature": 0.0, "avg_logprob": -0.13578129553979681, "compression_ratio": 1.6996197718631179, "no_speech_prob": 9.911771485349163e-05}, {"id": 170, "seek": 42480, "start": 436.40000000000003, "end": 437.40000000000003, "text": " in the field?", "tokens": [294, 264, 2519, 30], "temperature": 0.0, "avg_logprob": -0.13578129553979681, "compression_ratio": 1.6996197718631179, "no_speech_prob": 9.911771485349163e-05}, {"id": 171, "seek": 42480, "start": 437.40000000000003, "end": 442.40000000000003, "text": " Or is it that, you know, there is something deeper about this,", "tokens": [1610, 307, 309, 300, 11, 291, 458, 11, 456, 307, 746, 7731, 466, 341, 11], "temperature": 0.0, "avg_logprob": -0.13578129553979681, "compression_ratio": 1.6996197718631179, "no_speech_prob": 9.911771485349163e-05}, {"id": 172, "seek": 42480, "start": 442.40000000000003, "end": 445.40000000000003, "text": " and we should really be continue studying this?", "tokens": [293, 321, 820, 534, 312, 2354, 7601, 341, 30], "temperature": 0.0, "avg_logprob": -0.13578129553979681, "compression_ratio": 1.6996197718631179, "no_speech_prob": 9.911771485349163e-05}, {"id": 173, "seek": 42480, "start": 445.40000000000003, "end": 448.6, "text": " Well, I think that this is, there's kind of,", "tokens": [1042, 11, 286, 519, 300, 341, 307, 11, 456, 311, 733, 295, 11], "temperature": 0.0, "avg_logprob": -0.13578129553979681, "compression_ratio": 1.6996197718631179, "no_speech_prob": 9.911771485349163e-05}, {"id": 174, "seek": 42480, "start": 448.6, "end": 449.8, "text": " I can go either way on this.", "tokens": [286, 393, 352, 2139, 636, 322, 341, 13], "temperature": 0.0, "avg_logprob": -0.13578129553979681, "compression_ratio": 1.6996197718631179, "no_speech_prob": 9.911771485349163e-05}, {"id": 175, "seek": 42480, "start": 449.8, "end": 452.6, "text": " And so this slide shows you the case for parsing,", "tokens": [400, 370, 341, 4137, 3110, 291, 264, 1389, 337, 21156, 278, 11], "temperature": 0.0, "avg_logprob": -0.13578129553979681, "compression_ratio": 1.6996197718631179, "no_speech_prob": 9.911771485349163e-05}, {"id": 176, "seek": 45260, "start": 452.6, "end": 455.20000000000005, "text": " and then I'll, you know, kind of have a counterpoint in a second.", "tokens": [293, 550, 286, 603, 11, 291, 458, 11, 733, 295, 362, 257, 5682, 6053, 294, 257, 1150, 13], "temperature": 0.0, "avg_logprob": -0.0933492840744379, "compression_ratio": 1.660649819494585, "no_speech_prob": 7.719443965470418e-05}, {"id": 177, "seek": 45260, "start": 455.20000000000005, "end": 458.8, "text": " So I think that the most important case for parsing", "tokens": [407, 286, 519, 300, 264, 881, 1021, 1389, 337, 21156, 278], "temperature": 0.0, "avg_logprob": -0.0933492840744379, "compression_ratio": 1.660649819494585, "no_speech_prob": 7.719443965470418e-05}, {"id": 178, "seek": 45260, "start": 458.8, "end": 461.20000000000005, "text": " is that there's a sort of deep truth to the fact", "tokens": [307, 300, 456, 311, 257, 1333, 295, 2452, 3494, 281, 264, 1186], "temperature": 0.0, "avg_logprob": -0.0933492840744379, "compression_ratio": 1.660649819494585, "no_speech_prob": 7.719443965470418e-05}, {"id": 179, "seek": 45260, "start": 461.20000000000005, "end": 462.6, "text": " that sentence is a tree structure.", "tokens": [300, 8174, 307, 257, 4230, 3877, 13], "temperature": 0.0, "avg_logprob": -0.0933492840744379, "compression_ratio": 1.660649819494585, "no_speech_prob": 7.719443965470418e-05}, {"id": 180, "seek": 45260, "start": 462.6, "end": 465.20000000000005, "text": " That's, they just are, right?", "tokens": [663, 311, 11, 436, 445, 366, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.0933492840744379, "compression_ratio": 1.660649819494585, "no_speech_prob": 7.719443965470418e-05}, {"id": 181, "seek": 45260, "start": 465.20000000000005, "end": 469.20000000000005, "text": " Language, syntactic structure of sentences is recursive,", "tokens": [24445, 11, 23980, 19892, 3877, 295, 16579, 307, 20560, 488, 11], "temperature": 0.0, "avg_logprob": -0.0933492840744379, "compression_ratio": 1.660649819494585, "no_speech_prob": 7.719443965470418e-05}, {"id": 182, "seek": 45260, "start": 469.20000000000005, "end": 472.20000000000005, "text": " and that means that you can have arbitrarily long gaps", "tokens": [293, 300, 1355, 300, 291, 393, 362, 19071, 3289, 938, 15031], "temperature": 0.0, "avg_logprob": -0.0933492840744379, "compression_ratio": 1.660649819494585, "no_speech_prob": 7.719443965470418e-05}, {"id": 183, "seek": 45260, "start": 472.20000000000005, "end": 474.40000000000003, "text": " between two words which are related.", "tokens": [1296, 732, 2283, 597, 366, 4077, 13], "temperature": 0.0, "avg_logprob": -0.0933492840744379, "compression_ratio": 1.660649819494585, "no_speech_prob": 7.719443965470418e-05}, {"id": 184, "seek": 45260, "start": 474.40000000000003, "end": 479.40000000000003, "text": " So for instance, if you have a relationship", "tokens": [407, 337, 5197, 11, 498, 291, 362, 257, 2480], "temperature": 0.0, "avg_logprob": -0.0933492840744379, "compression_ratio": 1.660649819494585, "no_speech_prob": 7.719443965470418e-05}, {"id": 185, "seek": 45260, "start": 479.40000000000003, "end": 481.6, "text": " between, say, a subject and a verb,", "tokens": [1296, 11, 584, 11, 257, 3983, 293, 257, 9595, 11], "temperature": 0.0, "avg_logprob": -0.0933492840744379, "compression_ratio": 1.660649819494585, "no_speech_prob": 7.719443965470418e-05}, {"id": 186, "seek": 48160, "start": 481.6, "end": 486.6, "text": " like syntax is, whether the subject of that verb", "tokens": [411, 28431, 307, 11, 1968, 264, 3983, 295, 300, 9595], "temperature": 0.0, "avg_logprob": -0.12087774276733398, "compression_ratio": 1.7655677655677655, "no_speech_prob": 7.964930409798399e-05}, {"id": 187, "seek": 48160, "start": 486.6, "end": 491.0, "text": " is plural or singular is going to change the form of the verb.", "tokens": [307, 25377, 420, 20010, 307, 516, 281, 1319, 264, 1254, 295, 264, 9595, 13], "temperature": 0.0, "avg_logprob": -0.12087774276733398, "compression_ratio": 1.7655677655677655, "no_speech_prob": 7.964930409798399e-05}, {"id": 188, "seek": 48160, "start": 491.0, "end": 493.8, "text": " And that dependency between them can be arbitrarily long", "tokens": [400, 300, 33621, 1296, 552, 393, 312, 19071, 3289, 938], "temperature": 0.0, "avg_logprob": -0.12087774276733398, "compression_ratio": 1.7655677655677655, "no_speech_prob": 7.964930409798399e-05}, {"id": 189, "seek": 48160, "start": 493.8, "end": 496.0, "text": " because you can have this nested structure.", "tokens": [570, 291, 393, 362, 341, 15646, 292, 3877, 13], "temperature": 0.0, "avg_logprob": -0.12087774276733398, "compression_ratio": 1.7655677655677655, "no_speech_prob": 7.964930409798399e-05}, {"id": 190, "seek": 48160, "start": 496.0, "end": 498.0, "text": " But it will never, but it can't be arbitrarily long", "tokens": [583, 309, 486, 1128, 11, 457, 309, 393, 380, 312, 19071, 3289, 938], "temperature": 0.0, "avg_logprob": -0.12087774276733398, "compression_ratio": 1.7655677655677655, "no_speech_prob": 7.964930409798399e-05}, {"id": 191, "seek": 48160, "start": 498.0, "end": 501.20000000000005, "text": " in tree space because, you know, there's only,", "tokens": [294, 4230, 1901, 570, 11, 291, 458, 11, 456, 311, 787, 11], "temperature": 0.0, "avg_logprob": -0.12087774276733398, "compression_ratio": 1.7655677655677655, "no_speech_prob": 7.964930409798399e-05}, {"id": 192, "seek": 48160, "start": 501.20000000000005, "end": 503.8, "text": " if you, the relationship between them will always be", "tokens": [498, 291, 11, 264, 2480, 1296, 552, 486, 1009, 312], "temperature": 0.0, "avg_logprob": -0.12087774276733398, "compression_ratio": 1.7655677655677655, "no_speech_prob": 7.964930409798399e-05}, {"id": 193, "seek": 48160, "start": 503.8, "end": 508.0, "text": " the subject and the verb, like, sort of next to each other in the tree.", "tokens": [264, 3983, 293, 264, 9595, 11, 411, 11, 1333, 295, 958, 281, 1184, 661, 294, 264, 4230, 13], "temperature": 0.0, "avg_logprob": -0.12087774276733398, "compression_ratio": 1.7655677655677655, "no_speech_prob": 7.964930409798399e-05}, {"id": 194, "seek": 48160, "start": 508.0, "end": 510.20000000000005, "text": " So you can see how, for some of these things,", "tokens": [407, 291, 393, 536, 577, 11, 337, 512, 295, 613, 721, 11], "temperature": 0.0, "avg_logprob": -0.12087774276733398, "compression_ratio": 1.7655677655677655, "no_speech_prob": 7.964930409798399e-05}, {"id": 195, "seek": 51020, "start": 510.2, "end": 512.2, "text": " it should be sort of more efficient to think about it", "tokens": [309, 820, 312, 1333, 295, 544, 7148, 281, 519, 466, 309], "temperature": 0.0, "avg_logprob": -0.11728987517180266, "compression_ratio": 1.9080882352941178, "no_speech_prob": 7.367365469690412e-05}, {"id": 196, "seek": 51020, "start": 512.2, "end": 514.0, "text": " or model it as a tree.", "tokens": [420, 2316, 309, 382, 257, 4230, 13], "temperature": 0.0, "avg_logprob": -0.11728987517180266, "compression_ratio": 1.9080882352941178, "no_speech_prob": 7.367365469690412e-05}, {"id": 197, "seek": 51020, "start": 514.0, "end": 518.0, "text": " And the tree should tell you things that you otherwise", "tokens": [400, 264, 4230, 820, 980, 291, 721, 300, 291, 5911], "temperature": 0.0, "avg_logprob": -0.11728987517180266, "compression_ratio": 1.9080882352941178, "no_speech_prob": 7.367365469690412e-05}, {"id": 198, "seek": 51020, "start": 518.0, "end": 520.8, "text": " would have to infer from an enormous amount of data.", "tokens": [576, 362, 281, 13596, 490, 364, 11322, 2372, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.11728987517180266, "compression_ratio": 1.9080882352941178, "no_speech_prob": 7.367365469690412e-05}, {"id": 199, "seek": 51020, "start": 520.8, "end": 522.8, "text": " It should be more efficient in this way.", "tokens": [467, 820, 312, 544, 7148, 294, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.11728987517180266, "compression_ratio": 1.9080882352941178, "no_speech_prob": 7.367365469690412e-05}, {"id": 200, "seek": 51020, "start": 522.8, "end": 526.8, "text": " So we can say, okay, you know, in theory, this should be important.", "tokens": [407, 321, 393, 584, 11, 1392, 11, 291, 458, 11, 294, 5261, 11, 341, 820, 312, 1021, 13], "temperature": 0.0, "avg_logprob": -0.11728987517180266, "compression_ratio": 1.9080882352941178, "no_speech_prob": 7.367365469690412e-05}, {"id": 201, "seek": 51020, "start": 526.8, "end": 529.0, "text": " And it should be something that we study based on this knowledge", "tokens": [400, 309, 820, 312, 746, 300, 321, 2979, 2361, 322, 341, 3601], "temperature": 0.0, "avg_logprob": -0.11728987517180266, "compression_ratio": 1.9080882352941178, "no_speech_prob": 7.367365469690412e-05}, {"id": 202, "seek": 51020, "start": 529.0, "end": 533.2, "text": " about how sentences are structured.", "tokens": [466, 577, 16579, 366, 18519, 13], "temperature": 0.0, "avg_logprob": -0.11728987517180266, "compression_ratio": 1.9080882352941178, "no_speech_prob": 7.367365469690412e-05}, {"id": 203, "seek": 51020, "start": 533.2, "end": 536.0, "text": " So then the sort of counterpoint to this is, all right,", "tokens": [407, 550, 264, 1333, 295, 5682, 6053, 281, 341, 307, 11, 439, 558, 11], "temperature": 0.0, "avg_logprob": -0.11728987517180266, "compression_ratio": 1.9080882352941178, "no_speech_prob": 7.367365469690412e-05}, {"id": 204, "seek": 51020, "start": 536.0, "end": 539.4, "text": " so sentence is a tree structure, and that's a truth about sentences.", "tokens": [370, 8174, 307, 257, 4230, 3877, 11, 293, 300, 311, 257, 3494, 466, 16579, 13], "temperature": 0.0, "avg_logprob": -0.11728987517180266, "compression_ratio": 1.9080882352941178, "no_speech_prob": 7.367365469690412e-05}, {"id": 205, "seek": 53940, "start": 539.4, "end": 542.1999999999999, "text": " But it's also true that they're written and read in order.", "tokens": [583, 309, 311, 611, 2074, 300, 436, 434, 3720, 293, 1401, 294, 1668, 13], "temperature": 0.0, "avg_logprob": -0.09241746705153893, "compression_ratio": 1.8175895765472312, "no_speech_prob": 0.00013338556163944304}, {"id": 206, "seek": 53940, "start": 542.1999999999999, "end": 545.4, "text": " So, you know, if you read a sentence, you do read it from left to right,", "tokens": [407, 11, 291, 458, 11, 498, 291, 1401, 257, 8174, 11, 291, 360, 1401, 309, 490, 1411, 281, 558, 11], "temperature": 0.0, "avg_logprob": -0.09241746705153893, "compression_ratio": 1.8175895765472312, "no_speech_prob": 0.00013338556163944304}, {"id": 207, "seek": 53940, "start": 545.4, "end": 548.4, "text": " or in English anyway, or, like, basically from start to finish,", "tokens": [420, 294, 3669, 4033, 11, 420, 11, 411, 11, 1936, 490, 722, 281, 2413, 11], "temperature": 0.0, "avg_logprob": -0.09241746705153893, "compression_ratio": 1.8175895765472312, "no_speech_prob": 0.00013338556163944304}, {"id": 208, "seek": 53940, "start": 548.4, "end": 550.6, "text": " or you hear a sentence from start to finish.", "tokens": [420, 291, 1568, 257, 8174, 490, 722, 281, 2413, 13], "temperature": 0.0, "avg_logprob": -0.09241746705153893, "compression_ratio": 1.8175895765472312, "no_speech_prob": 0.00013338556163944304}, {"id": 209, "seek": 53940, "start": 550.6, "end": 554.6, "text": " And this really puts a sort of bounding on the linear complexity", "tokens": [400, 341, 534, 8137, 257, 1333, 295, 5472, 278, 322, 264, 8213, 14024], "temperature": 0.0, "avg_logprob": -0.09241746705153893, "compression_ratio": 1.8175895765472312, "no_speech_prob": 0.00013338556163944304}, {"id": 210, "seek": 53940, "start": 554.6, "end": 556.8, "text": " that you will empirically see, right?", "tokens": [300, 291, 486, 25790, 984, 536, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.09241746705153893, "compression_ratio": 1.8175895765472312, "no_speech_prob": 0.00013338556163944304}, {"id": 211, "seek": 53940, "start": 556.8, "end": 559.0, "text": " Because when somebody wrote this sentence,", "tokens": [1436, 562, 2618, 4114, 341, 8174, 11], "temperature": 0.0, "avg_logprob": -0.09241746705153893, "compression_ratio": 1.8175895765472312, "no_speech_prob": 0.00013338556163944304}, {"id": 212, "seek": 53940, "start": 559.0, "end": 561.8, "text": " yes, they could have an arbitrarily long dependency,", "tokens": [2086, 11, 436, 727, 362, 364, 19071, 3289, 938, 33621, 11], "temperature": 0.0, "avg_logprob": -0.09241746705153893, "compression_ratio": 1.8175895765472312, "no_speech_prob": 0.00013338556163944304}, {"id": 213, "seek": 53940, "start": 561.8, "end": 564.6, "text": " but they expect that, you know, that would mean that their audience", "tokens": [457, 436, 2066, 300, 11, 291, 458, 11, 300, 576, 914, 300, 641, 4034], "temperature": 0.0, "avg_logprob": -0.09241746705153893, "compression_ratio": 1.8175895765472312, "no_speech_prob": 0.00013338556163944304}, {"id": 214, "seek": 53940, "start": 564.6, "end": 567.0, "text": " listening to it will have to wait arbitrarily long", "tokens": [4764, 281, 309, 486, 362, 281, 1699, 19071, 3289, 938], "temperature": 0.0, "avg_logprob": -0.09241746705153893, "compression_ratio": 1.8175895765472312, "no_speech_prob": 0.00013338556163944304}, {"id": 215, "seek": 56700, "start": 567.0, "end": 570.4, "text": " between, you know, some word and the thing that it attaches to.", "tokens": [1296, 11, 291, 458, 11, 512, 1349, 293, 264, 551, 300, 309, 49404, 281, 13], "temperature": 0.0, "avg_logprob": -0.07897961223042095, "compression_ratio": 1.8503401360544218, "no_speech_prob": 8.612993406131864e-05}, {"id": 216, "seek": 56700, "start": 570.4, "end": 572.2, "text": " And that's kind of not very nice, right?", "tokens": [400, 300, 311, 733, 295, 406, 588, 1481, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.07897961223042095, "compression_ratio": 1.8503401360544218, "no_speech_prob": 8.612993406131864e-05}, {"id": 217, "seek": 56700, "start": 572.2, "end": 575.6, "text": " So empirically, it's not very surprising to see", "tokens": [407, 25790, 984, 11, 309, 311, 406, 588, 8830, 281, 536], "temperature": 0.0, "avg_logprob": -0.07897961223042095, "compression_ratio": 1.8503401360544218, "no_speech_prob": 8.612993406131864e-05}, {"id": 218, "seek": 56700, "start": 575.6, "end": 578.2, "text": " that most dependencies are in fact short.", "tokens": [300, 881, 36606, 366, 294, 1186, 2099, 13], "temperature": 0.0, "avg_logprob": -0.07897961223042095, "compression_ratio": 1.8503401360544218, "no_speech_prob": 8.612993406131864e-05}, {"id": 219, "seek": 56700, "start": 578.2, "end": 582.0, "text": " And, you know, there's a lot of arguments that the options", "tokens": [400, 11, 291, 458, 11, 456, 311, 257, 688, 295, 12869, 300, 264, 3956], "temperature": 0.0, "avg_logprob": -0.07897961223042095, "compression_ratio": 1.8503401360544218, "no_speech_prob": 8.612993406131864e-05}, {"id": 220, "seek": 56700, "start": 582.0, "end": 585.0, "text": " that are kind of provided to grammars are sort of arranged", "tokens": [300, 366, 733, 295, 5649, 281, 17570, 685, 366, 1333, 295, 18721], "temperature": 0.0, "avg_logprob": -0.07897961223042095, "compression_ratio": 1.8503401360544218, "no_speech_prob": 8.612993406131864e-05}, {"id": 221, "seek": 56700, "start": 585.0, "end": 588.0, "text": " so that you're able to keep your dependencies short.", "tokens": [370, 300, 291, 434, 1075, 281, 1066, 428, 36606, 2099, 13], "temperature": 0.0, "avg_logprob": -0.07897961223042095, "compression_ratio": 1.8503401360544218, "no_speech_prob": 8.612993406131864e-05}, {"id": 222, "seek": 56700, "start": 588.0, "end": 590.4, "text": " Like, that's what are some of the reasons you have options", "tokens": [1743, 11, 300, 311, 437, 366, 512, 295, 264, 4112, 291, 362, 3956], "temperature": 0.0, "avg_logprob": -0.07897961223042095, "compression_ratio": 1.8503401360544218, "no_speech_prob": 8.612993406131864e-05}, {"id": 223, "seek": 56700, "start": 590.4, "end": 592.2, "text": " for how to move things around in sentences", "tokens": [337, 577, 281, 1286, 721, 926, 294, 16579], "temperature": 0.0, "avg_logprob": -0.07897961223042095, "compression_ratio": 1.8503401360544218, "no_speech_prob": 8.612993406131864e-05}, {"id": 224, "seek": 56700, "start": 592.2, "end": 593.8, "text": " to make nice reading orders,", "tokens": [281, 652, 1481, 3760, 9470, 11], "temperature": 0.0, "avg_logprob": -0.07897961223042095, "compression_ratio": 1.8503401360544218, "no_speech_prob": 8.612993406131864e-05}, {"id": 225, "seek": 56700, "start": 593.8, "end": 596.0, "text": " because, you know, you want short dependencies.", "tokens": [570, 11, 291, 458, 11, 291, 528, 2099, 36606, 13], "temperature": 0.0, "avg_logprob": -0.07897961223042095, "compression_ratio": 1.8503401360544218, "no_speech_prob": 8.612993406131864e-05}, {"id": 226, "seek": 59600, "start": 596.0, "end": 598.8, "text": " So this means that if most dependencies are short,", "tokens": [407, 341, 1355, 300, 498, 881, 36606, 366, 2099, 11], "temperature": 0.0, "avg_logprob": -0.09041208120492789, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.3919910745462403e-05}, {"id": 227, "seek": 59600, "start": 598.8, "end": 603.4, "text": " then processing text as, say, chunks of words of one or two at a time", "tokens": [550, 9007, 2487, 382, 11, 584, 11, 24004, 295, 2283, 295, 472, 420, 732, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.09041208120492789, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.3919910745462403e-05}, {"id": 228, "seek": 59600, "start": 603.4, "end": 606.0, "text": " kind of gives you a pretty similar view.", "tokens": [733, 295, 2709, 291, 257, 1238, 2531, 1910, 13], "temperature": 0.0, "avg_logprob": -0.09041208120492789, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.3919910745462403e-05}, {"id": 229, "seek": 59600, "start": 606.0, "end": 607.6, "text": " Most of the time, you don't get something", "tokens": [4534, 295, 264, 565, 11, 291, 500, 380, 483, 746], "temperature": 0.0, "avg_logprob": -0.09041208120492789, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.3919910745462403e-05}, {"id": 230, "seek": 59600, "start": 607.6, "end": 611.2, "text": " that's so dramatically different if you look at a tree", "tokens": [300, 311, 370, 17548, 819, 498, 291, 574, 412, 257, 4230], "temperature": 0.0, "avg_logprob": -0.09041208120492789, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.3919910745462403e-05}, {"id": 231, "seek": 59600, "start": 611.2, "end": 614.4, "text": " instead of looking at chunks of three or four-word sentences.", "tokens": [2602, 295, 1237, 412, 24004, 295, 1045, 420, 1451, 12, 7462, 16579, 13], "temperature": 0.0, "avg_logprob": -0.09041208120492789, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.3919910745462403e-05}, {"id": 232, "seek": 59600, "start": 614.4, "end": 616.8, "text": " So, you know, this is kind of a counterpoint that says,", "tokens": [407, 11, 291, 458, 11, 341, 307, 733, 295, 257, 5682, 6053, 300, 1619, 11], "temperature": 0.0, "avg_logprob": -0.09041208120492789, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.3919910745462403e-05}, {"id": 233, "seek": 59600, "start": 616.8, "end": 620.8, "text": " you know, maybe even though the sentences are in fact tree-structured,", "tokens": [291, 458, 11, 1310, 754, 1673, 264, 16579, 366, 294, 1186, 4230, 12, 372, 46847, 11], "temperature": 0.0, "avg_logprob": -0.09041208120492789, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.3919910745462403e-05}, {"id": 234, "seek": 59600, "start": 620.8, "end": 623.6, "text": " maybe it's not that crucially useful.", "tokens": [1310, 309, 311, 406, 300, 5140, 1909, 4420, 13], "temperature": 0.0, "avg_logprob": -0.09041208120492789, "compression_ratio": 1.7259786476868328, "no_speech_prob": 2.3919910745462403e-05}, {"id": 235, "seek": 62360, "start": 623.6, "end": 626.6, "text": " So I think that the part that makes this, you know,", "tokens": [407, 286, 519, 300, 264, 644, 300, 1669, 341, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.07793790580582445, "compression_ratio": 1.9036544850498338, "no_speech_prob": 3.5351829865248874e-05}, {"id": 236, "seek": 62360, "start": 626.6, "end": 629.0, "text": " particularly rewarding to look at syntax", "tokens": [4098, 20063, 281, 574, 412, 28431], "temperature": 0.0, "avg_logprob": -0.07793790580582445, "compression_ratio": 1.9036544850498338, "no_speech_prob": 3.5351829865248874e-05}, {"id": 237, "seek": 62360, "start": 629.0, "end": 631.8000000000001, "text": " or particularly useful to provide syntactic structures", "tokens": [420, 4098, 4420, 281, 2893, 23980, 19892, 9227], "temperature": 0.0, "avg_logprob": -0.07793790580582445, "compression_ratio": 1.9036544850498338, "no_speech_prob": 3.5351829865248874e-05}, {"id": 238, "seek": 62360, "start": 631.8000000000001, "end": 635.4, "text": " in a library like spaCy is that they're application independent.", "tokens": [294, 257, 6405, 411, 32543, 34, 88, 307, 300, 436, 434, 3861, 6695, 13], "temperature": 0.0, "avg_logprob": -0.07793790580582445, "compression_ratio": 1.9036544850498338, "no_speech_prob": 3.5351829865248874e-05}, {"id": 239, "seek": 62360, "start": 635.4, "end": 638.2, "text": " So there's the syntactic structure of the sentence", "tokens": [407, 456, 311, 264, 23980, 19892, 3877, 295, 264, 8174], "temperature": 0.0, "avg_logprob": -0.07793790580582445, "compression_ratio": 1.9036544850498338, "no_speech_prob": 3.5351829865248874e-05}, {"id": 240, "seek": 62360, "start": 638.2, "end": 640.6, "text": " doesn't depend on what you hope to do with the sentence", "tokens": [1177, 380, 5672, 322, 437, 291, 1454, 281, 360, 365, 264, 8174], "temperature": 0.0, "avg_logprob": -0.07793790580582445, "compression_ratio": 1.9036544850498338, "no_speech_prob": 3.5351829865248874e-05}, {"id": 241, "seek": 62360, "start": 640.6, "end": 642.2, "text": " or how you hope to process it.", "tokens": [420, 577, 291, 1454, 281, 1399, 309, 13], "temperature": 0.0, "avg_logprob": -0.07793790580582445, "compression_ratio": 1.9036544850498338, "no_speech_prob": 3.5351829865248874e-05}, {"id": 242, "seek": 62360, "start": 642.2, "end": 644.6, "text": " And that's something that's quite different from other labels", "tokens": [400, 300, 311, 746, 300, 311, 1596, 819, 490, 661, 16949], "temperature": 0.0, "avg_logprob": -0.07793790580582445, "compression_ratio": 1.9036544850498338, "no_speech_prob": 3.5351829865248874e-05}, {"id": 243, "seek": 62360, "start": 644.6, "end": 647.4, "text": " or other information that we can attach to the sentence.", "tokens": [420, 661, 1589, 300, 321, 393, 5085, 281, 264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.07793790580582445, "compression_ratio": 1.9036544850498338, "no_speech_prob": 3.5351829865248874e-05}, {"id": 244, "seek": 62360, "start": 647.4, "end": 649.6, "text": " If you're doing something like a sentiment analysis,", "tokens": [759, 291, 434, 884, 746, 411, 257, 16149, 5215, 11], "temperature": 0.0, "avg_logprob": -0.07793790580582445, "compression_ratio": 1.9036544850498338, "no_speech_prob": 3.5351829865248874e-05}, {"id": 245, "seek": 62360, "start": 649.6, "end": 653.0, "text": " there's no truth about the sentiment of a sentence", "tokens": [456, 311, 572, 3494, 466, 264, 16149, 295, 257, 8174], "temperature": 0.0, "avg_logprob": -0.07793790580582445, "compression_ratio": 1.9036544850498338, "no_speech_prob": 3.5351829865248874e-05}, {"id": 246, "seek": 65300, "start": 653.0, "end": 655.4, "text": " that's independent of what you're hoping to process.", "tokens": [300, 311, 6695, 295, 437, 291, 434, 7159, 281, 1399, 13], "temperature": 0.0, "avg_logprob": -0.07750401625762114, "compression_ratio": 1.9064748201438848, "no_speech_prob": 4.197563248453662e-05}, {"id": 247, "seek": 65300, "start": 655.4, "end": 658.2, "text": " Like, that's not a thing that's in the text itself.", "tokens": [1743, 11, 300, 311, 406, 257, 551, 300, 311, 294, 264, 2487, 2564, 13], "temperature": 0.0, "avg_logprob": -0.07750401625762114, "compression_ratio": 1.9064748201438848, "no_speech_prob": 4.197563248453662e-05}, {"id": 248, "seek": 65300, "start": 658.2, "end": 660.8, "text": " It's, you know, a lens that you want to take on it", "tokens": [467, 311, 11, 291, 458, 11, 257, 6765, 300, 291, 528, 281, 747, 322, 309], "temperature": 0.0, "avg_logprob": -0.07750401625762114, "compression_ratio": 1.9064748201438848, "no_speech_prob": 4.197563248453662e-05}, {"id": 249, "seek": 65300, "start": 660.8, "end": 662.6, "text": " based on how you want to process it.", "tokens": [2361, 322, 577, 291, 528, 281, 1399, 309, 13], "temperature": 0.0, "avg_logprob": -0.07750401625762114, "compression_ratio": 1.9064748201438848, "no_speech_prob": 4.197563248453662e-05}, {"id": 250, "seek": 65300, "start": 662.6, "end": 664.4, "text": " So, you know, there's a...", "tokens": [407, 11, 291, 458, 11, 456, 311, 257, 485], "temperature": 0.0, "avg_logprob": -0.07750401625762114, "compression_ratio": 1.9064748201438848, "no_speech_prob": 4.197563248453662e-05}, {"id": 251, "seek": 65300, "start": 664.4, "end": 667.2, "text": " Whether you consider some review to be positive or negative", "tokens": [8503, 291, 1949, 512, 3131, 281, 312, 3353, 420, 3671], "temperature": 0.0, "avg_logprob": -0.07750401625762114, "compression_ratio": 1.9064748201438848, "no_speech_prob": 4.197563248453662e-05}, {"id": 252, "seek": 65300, "start": 667.2, "end": 669.4, "text": " depends on your application.", "tokens": [5946, 322, 428, 3861, 13], "temperature": 0.0, "avg_logprob": -0.07750401625762114, "compression_ratio": 1.9064748201438848, "no_speech_prob": 4.197563248453662e-05}, {"id": 253, "seek": 65300, "start": 669.4, "end": 670.8, "text": " It doesn't really...", "tokens": [467, 1177, 380, 534, 485], "temperature": 0.0, "avg_logprob": -0.07750401625762114, "compression_ratio": 1.9064748201438848, "no_speech_prob": 4.197563248453662e-05}, {"id": 254, "seek": 65300, "start": 670.8, "end": 672.6, "text": " It's not necessarily in the text itself", "tokens": [467, 311, 406, 4725, 294, 264, 2487, 2564], "temperature": 0.0, "avg_logprob": -0.07750401625762114, "compression_ratio": 1.9064748201438848, "no_speech_prob": 4.197563248453662e-05}, {"id": 255, "seek": 65300, "start": 672.6, "end": 675.0, "text": " because, you know, what counts as positive or negative?", "tokens": [570, 11, 291, 458, 11, 437, 14893, 382, 3353, 420, 3671, 30], "temperature": 0.0, "avg_logprob": -0.07750401625762114, "compression_ratio": 1.9064748201438848, "no_speech_prob": 4.197563248453662e-05}, {"id": 256, "seek": 65300, "start": 675.0, "end": 677.8, "text": " What's the labeling scheme? What's the rating scheme?", "tokens": [708, 311, 264, 40244, 12232, 30, 708, 311, 264, 10990, 12232, 30], "temperature": 0.0, "avg_logprob": -0.07750401625762114, "compression_ratio": 1.9064748201438848, "no_speech_prob": 4.197563248453662e-05}, {"id": 257, "seek": 65300, "start": 677.8, "end": 680.6, "text": " Or, you know, exactly what are they talking about?", "tokens": [1610, 11, 291, 458, 11, 2293, 437, 366, 436, 1417, 466, 30], "temperature": 0.0, "avg_logprob": -0.07750401625762114, "compression_ratio": 1.9064748201438848, "no_speech_prob": 4.197563248453662e-05}, {"id": 258, "seek": 68060, "start": 680.6, "end": 683.4, "text": " Well, the taxonomy that you have will depend on", "tokens": [1042, 11, 264, 3366, 23423, 300, 291, 362, 486, 5672, 322], "temperature": 0.0, "avg_logprob": -0.08064235399847161, "compression_ratio": 1.7741935483870968, "no_speech_prob": 5.2230734581826255e-05}, {"id": 259, "seek": 68060, "start": 683.4, "end": 686.0, "text": " what you're hoping to process with.", "tokens": [437, 291, 434, 7159, 281, 1399, 365, 13], "temperature": 0.0, "avg_logprob": -0.08064235399847161, "compression_ratio": 1.7741935483870968, "no_speech_prob": 5.2230734581826255e-05}, {"id": 260, "seek": 68060, "start": 686.0, "end": 687.6, "text": " Those things aren't in the language,", "tokens": [3950, 721, 3212, 380, 294, 264, 2856, 11], "temperature": 0.0, "avg_logprob": -0.08064235399847161, "compression_ratio": 1.7741935483870968, "no_speech_prob": 5.2230734581826255e-05}, {"id": 261, "seek": 68060, "start": 687.6, "end": 691.4, "text": " but details about the syntactic structure are in the language.", "tokens": [457, 4365, 466, 264, 23980, 19892, 3877, 366, 294, 264, 2856, 13], "temperature": 0.0, "avg_logprob": -0.08064235399847161, "compression_ratio": 1.7741935483870968, "no_speech_prob": 5.2230734581826255e-05}, {"id": 262, "seek": 68060, "start": 691.4, "end": 692.8000000000001, "text": " They're things which are, you know,", "tokens": [814, 434, 721, 597, 366, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.08064235399847161, "compression_ratio": 1.7741935483870968, "no_speech_prob": 5.2230734581826255e-05}, {"id": 263, "seek": 68060, "start": 692.8000000000001, "end": 695.8000000000001, "text": " just part of the structure of the code.", "tokens": [445, 644, 295, 264, 3877, 295, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.08064235399847161, "compression_ratio": 1.7741935483870968, "no_speech_prob": 5.2230734581826255e-05}, {"id": 264, "seek": 68060, "start": 695.8000000000001, "end": 698.0, "text": " And that means that we can provide these things,", "tokens": [400, 300, 1355, 300, 321, 393, 2893, 613, 721, 11], "temperature": 0.0, "avg_logprob": -0.08064235399847161, "compression_ratio": 1.7741935483870968, "no_speech_prob": 5.2230734581826255e-05}, {"id": 265, "seek": 68060, "start": 698.0, "end": 700.4, "text": " sort of, learn it once and give it to many people.", "tokens": [1333, 295, 11, 1466, 309, 1564, 293, 976, 309, 281, 867, 561, 13], "temperature": 0.0, "avg_logprob": -0.08064235399847161, "compression_ratio": 1.7741935483870968, "no_speech_prob": 5.2230734581826255e-05}, {"id": 266, "seek": 68060, "start": 700.4, "end": 702.6, "text": " And I think that that's very valuable and useful", "tokens": [400, 286, 519, 300, 300, 311, 588, 8263, 293, 4420], "temperature": 0.0, "avg_logprob": -0.08064235399847161, "compression_ratio": 1.7741935483870968, "no_speech_prob": 5.2230734581826255e-05}, {"id": 267, "seek": 68060, "start": 702.6, "end": 704.6, "text": " and different from other types of annotations", "tokens": [293, 819, 490, 661, 3467, 295, 25339, 763], "temperature": 0.0, "avg_logprob": -0.08064235399847161, "compression_ratio": 1.7741935483870968, "no_speech_prob": 5.2230734581826255e-05}, {"id": 268, "seek": 68060, "start": 704.6, "end": 706.4, "text": " that we could calculate and attach.", "tokens": [300, 321, 727, 8873, 293, 5085, 13], "temperature": 0.0, "avg_logprob": -0.08064235399847161, "compression_ratio": 1.7741935483870968, "no_speech_prob": 5.2230734581826255e-05}, {"id": 269, "seek": 68060, "start": 706.4, "end": 709.2, "text": " And that's why spaCy provides pre-trained models for syntax", "tokens": [400, 300, 311, 983, 32543, 34, 88, 6417, 659, 12, 17227, 2001, 5245, 337, 28431], "temperature": 0.0, "avg_logprob": -0.08064235399847161, "compression_ratio": 1.7741935483870968, "no_speech_prob": 5.2230734581826255e-05}, {"id": 270, "seek": 70920, "start": 709.2, "end": 712.0, "text": " but doesn't provide pre-trained models for something like sentiment", "tokens": [457, 1177, 380, 2893, 659, 12, 17227, 2001, 5245, 337, 746, 411, 16149], "temperature": 0.0, "avg_logprob": -0.06150599710302415, "compression_ratio": 1.859531772575251, "no_speech_prob": 7.719445420661941e-05}, {"id": 271, "seek": 70920, "start": 712.0, "end": 715.8000000000001, "text": " because we know how to give you a syntactic analysis", "tokens": [570, 321, 458, 577, 281, 976, 291, 257, 23980, 19892, 5215], "temperature": 0.0, "avg_logprob": -0.06150599710302415, "compression_ratio": 1.859531772575251, "no_speech_prob": 7.719445420661941e-05}, {"id": 272, "seek": 70920, "start": 715.8000000000001, "end": 718.0, "text": " that's, you know, as useful as it may be,", "tokens": [300, 311, 11, 291, 458, 11, 382, 4420, 382, 309, 815, 312, 11], "temperature": 0.0, "avg_logprob": -0.06150599710302415, "compression_ratio": 1.859531772575251, "no_speech_prob": 7.719445420661941e-05}, {"id": 273, "seek": 70920, "start": 718.0, "end": 720.4000000000001, "text": " or maybe not, depending on, you know,", "tokens": [420, 1310, 406, 11, 5413, 322, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.06150599710302415, "compression_ratio": 1.859531772575251, "no_speech_prob": 7.719445420661941e-05}, {"id": 274, "seek": 70920, "start": 720.4000000000001, "end": 722.2, "text": " whether that actually solves your problems.", "tokens": [1968, 300, 767, 39890, 428, 2740, 13], "temperature": 0.0, "avg_logprob": -0.06150599710302415, "compression_ratio": 1.859531772575251, "no_speech_prob": 7.719445420661941e-05}, {"id": 275, "seek": 70920, "start": 722.2, "end": 724.6, "text": " But at least it's sort of true and generalizable,", "tokens": [583, 412, 1935, 309, 311, 1333, 295, 2074, 293, 2674, 22395, 11], "temperature": 0.0, "avg_logprob": -0.06150599710302415, "compression_ratio": 1.859531772575251, "no_speech_prob": 7.719445420661941e-05}, {"id": 276, "seek": 70920, "start": 724.6, "end": 726.4000000000001, "text": " whereas we don't know how to give you...", "tokens": [9735, 321, 500, 380, 458, 577, 281, 976, 291, 485], "temperature": 0.0, "avg_logprob": -0.06150599710302415, "compression_ratio": 1.859531772575251, "no_speech_prob": 7.719445420661941e-05}, {"id": 277, "seek": 70920, "start": 726.4000000000001, "end": 728.4000000000001, "text": " We don't know what categorization scheme", "tokens": [492, 500, 380, 458, 437, 19250, 2144, 12232], "temperature": 0.0, "avg_logprob": -0.06150599710302415, "compression_ratio": 1.859531772575251, "no_speech_prob": 7.719445420661941e-05}, {"id": 278, "seek": 70920, "start": 728.4000000000001, "end": 729.8000000000001, "text": " you want to classify your text in,", "tokens": [291, 528, 281, 33872, 428, 2487, 294, 11], "temperature": 0.0, "avg_logprob": -0.06150599710302415, "compression_ratio": 1.859531772575251, "no_speech_prob": 7.719445420661941e-05}, {"id": 279, "seek": 70920, "start": 729.8000000000001, "end": 732.2, "text": " so we can't give you a pre-trained model that does that", "tokens": [370, 321, 393, 380, 976, 291, 257, 659, 12, 17227, 2001, 2316, 300, 775, 300], "temperature": 0.0, "avg_logprob": -0.06150599710302415, "compression_ratio": 1.859531772575251, "no_speech_prob": 7.719445420661941e-05}, {"id": 280, "seek": 70920, "start": 732.2, "end": 734.8000000000001, "text": " because that's your own problem.", "tokens": [570, 300, 311, 428, 1065, 1154, 13], "temperature": 0.0, "avg_logprob": -0.06150599710302415, "compression_ratio": 1.859531772575251, "no_speech_prob": 7.719445420661941e-05}, {"id": 281, "seek": 70920, "start": 734.8000000000001, "end": 737.4000000000001, "text": " So we try to, you know, basically give you these things", "tokens": [407, 321, 853, 281, 11, 291, 458, 11, 1936, 976, 291, 613, 721], "temperature": 0.0, "avg_logprob": -0.06150599710302415, "compression_ratio": 1.859531772575251, "no_speech_prob": 7.719445420661941e-05}, {"id": 282, "seek": 73740, "start": 737.4, "end": 740.0, "text": " which are annotation layers which do generalize in this way,", "tokens": [597, 366, 48654, 7914, 597, 360, 2674, 1125, 294, 341, 636, 11], "temperature": 0.0, "avg_logprob": -0.10234431177377701, "compression_ratio": 1.8535714285714286, "no_speech_prob": 2.9306514989002608e-05}, {"id": 283, "seek": 73740, "start": 740.0, "end": 743.1999999999999, "text": " and that means that there has to be a sort of linguistic truth to them,", "tokens": [293, 300, 1355, 300, 456, 575, 281, 312, 257, 1333, 295, 43002, 3494, 281, 552, 11], "temperature": 0.0, "avg_logprob": -0.10234431177377701, "compression_ratio": 1.8535714285714286, "no_speech_prob": 2.9306514989002608e-05}, {"id": 284, "seek": 73740, "start": 743.1999999999999, "end": 746.1999999999999, "text": " and that means that looking at things like the semantic roles", "tokens": [293, 300, 1355, 300, 1237, 412, 721, 411, 264, 47982, 9604], "temperature": 0.0, "avg_logprob": -0.10234431177377701, "compression_ratio": 1.8535714285714286, "no_speech_prob": 2.9306514989002608e-05}, {"id": 285, "seek": 73740, "start": 746.1999999999999, "end": 749.0, "text": " or sentence structure or sentence divisions", "tokens": [420, 8174, 3877, 420, 8174, 24328], "temperature": 0.0, "avg_logprob": -0.10234431177377701, "compression_ratio": 1.8535714285714286, "no_speech_prob": 2.9306514989002608e-05}, {"id": 286, "seek": 73740, "start": 749.0, "end": 750.4, "text": " are things that we can do,", "tokens": [366, 721, 300, 321, 393, 360, 11], "temperature": 0.0, "avg_logprob": -0.10234431177377701, "compression_ratio": 1.8535714285714286, "no_speech_prob": 2.9306514989002608e-05}, {"id": 287, "seek": 73740, "start": 750.4, "end": 753.6, "text": " and that's why we, you know, are interested in this.", "tokens": [293, 300, 311, 983, 321, 11, 291, 458, 11, 366, 3102, 294, 341, 13], "temperature": 0.0, "avg_logprob": -0.10234431177377701, "compression_ratio": 1.8535714285714286, "no_speech_prob": 2.9306514989002608e-05}, {"id": 288, "seek": 73740, "start": 753.6, "end": 756.8, "text": " So the other thing about syntactic structures", "tokens": [407, 264, 661, 551, 466, 23980, 19892, 9227], "temperature": 0.0, "avg_logprob": -0.10234431177377701, "compression_ratio": 1.8535714285714286, "no_speech_prob": 2.9306514989002608e-05}, {"id": 289, "seek": 73740, "start": 756.8, "end": 758.6, "text": " and, you know, whether they're useful or not,", "tokens": [293, 11, 291, 458, 11, 1968, 436, 434, 4420, 420, 406, 11], "temperature": 0.0, "avg_logprob": -0.10234431177377701, "compression_ratio": 1.8535714285714286, "no_speech_prob": 2.9306514989002608e-05}, {"id": 290, "seek": 73740, "start": 758.6, "end": 762.4, "text": " is that in English, not using syntax is pretty powerful", "tokens": [307, 300, 294, 3669, 11, 406, 1228, 28431, 307, 1238, 4005], "temperature": 0.0, "avg_logprob": -0.10234431177377701, "compression_ratio": 1.8535714285714286, "no_speech_prob": 2.9306514989002608e-05}, {"id": 291, "seek": 73740, "start": 762.4, "end": 766.1999999999999, "text": " because English orthography happens to cut things up", "tokens": [570, 3669, 19052, 5820, 2314, 281, 1723, 721, 493], "temperature": 0.0, "avg_logprob": -0.10234431177377701, "compression_ratio": 1.8535714285714286, "no_speech_prob": 2.9306514989002608e-05}, {"id": 292, "seek": 76620, "start": 766.2, "end": 768.4000000000001, "text": " into pretty convenient units.", "tokens": [666, 1238, 10851, 6815, 13], "temperature": 0.0, "avg_logprob": -0.06466883056017818, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.00013972906162962317}, {"id": 293, "seek": 76620, "start": 768.4000000000001, "end": 772.2, "text": " They're not optimal units, but they're still, like, pretty nice,", "tokens": [814, 434, 406, 16252, 6815, 11, 457, 436, 434, 920, 11, 411, 11, 1238, 1481, 11], "temperature": 0.0, "avg_logprob": -0.06466883056017818, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.00013972906162962317}, {"id": 294, "seek": 76620, "start": 772.2, "end": 775.8000000000001, "text": " in a way that doesn't really hold true across a lot of other languages.", "tokens": [294, 257, 636, 300, 1177, 380, 534, 1797, 2074, 2108, 257, 688, 295, 661, 8650, 13], "temperature": 0.0, "avg_logprob": -0.06466883056017818, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.00013972906162962317}, {"id": 295, "seek": 76620, "start": 775.8000000000001, "end": 778.2, "text": " So in the bottom right here, we have Japanese,", "tokens": [407, 294, 264, 2767, 558, 510, 11, 321, 362, 5433, 11], "temperature": 0.0, "avg_logprob": -0.06466883056017818, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.00013972906162962317}, {"id": 296, "seek": 76620, "start": 778.2, "end": 781.2, "text": " which, you know, usually isn't segmented into words.", "tokens": [597, 11, 291, 458, 11, 2673, 1943, 380, 9469, 292, 666, 2283, 13], "temperature": 0.0, "avg_logprob": -0.06466883056017818, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.00013972906162962317}, {"id": 297, "seek": 76620, "start": 781.2, "end": 784.6, "text": " Like, you can't just cut that up trivially with whitespace", "tokens": [1743, 11, 291, 393, 380, 445, 1723, 300, 493, 1376, 85, 2270, 365, 21909, 17940], "temperature": 0.0, "avg_logprob": -0.06466883056017818, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.00013972906162962317}, {"id": 298, "seek": 76620, "start": 784.6, "end": 787.0, "text": " and get something that you can feed into a search engine", "tokens": [293, 483, 746, 300, 291, 393, 3154, 666, 257, 3164, 2848], "temperature": 0.0, "avg_logprob": -0.06466883056017818, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.00013972906162962317}, {"id": 299, "seek": 76620, "start": 787.0, "end": 789.8000000000001, "text": " or get something that you can feed forward into a topic model.", "tokens": [420, 483, 746, 300, 291, 393, 3154, 2128, 666, 257, 4829, 2316, 13], "temperature": 0.0, "avg_logprob": -0.06466883056017818, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.00013972906162962317}, {"id": 300, "seek": 76620, "start": 789.8000000000001, "end": 791.4000000000001, "text": " You have to do some extra work,", "tokens": [509, 362, 281, 360, 512, 2857, 589, 11], "temperature": 0.0, "avg_logprob": -0.06466883056017818, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.00013972906162962317}, {"id": 301, "seek": 76620, "start": 791.4000000000001, "end": 793.0, "text": " and the extra work that you do there", "tokens": [293, 264, 2857, 589, 300, 291, 360, 456], "temperature": 0.0, "avg_logprob": -0.06466883056017818, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.00013972906162962317}, {"id": 302, "seek": 76620, "start": 793.0, "end": 795.2, "text": " really should consider syntactic structure.", "tokens": [534, 820, 1949, 23980, 19892, 3877, 13], "temperature": 0.0, "avg_logprob": -0.06466883056017818, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.00013972906162962317}, {"id": 303, "seek": 79520, "start": 795.2, "end": 798.8000000000001, "text": " You can use a technology that only makes linear decisions,", "tokens": [509, 393, 764, 257, 2899, 300, 787, 1669, 8213, 5327, 11], "temperature": 0.0, "avg_logprob": -0.06720073283219538, "compression_ratio": 1.6457564575645756, "no_speech_prob": 5.1408154831733555e-05}, {"id": 304, "seek": 79520, "start": 798.8000000000001, "end": 801.8000000000001, "text": " but the, you know, truth about what counts as a word or not", "tokens": [457, 264, 11, 291, 458, 11, 3494, 466, 437, 14893, 382, 257, 1349, 420, 406], "temperature": 0.0, "avg_logprob": -0.06720073283219538, "compression_ratio": 1.6457564575645756, "no_speech_prob": 5.1408154831733555e-05}, {"id": 305, "seek": 79520, "start": 801.8000000000001, "end": 803.8000000000001, "text": " is very entangled with the syntactic structure,", "tokens": [307, 588, 948, 39101, 365, 264, 23980, 19892, 3877, 11], "temperature": 0.0, "avg_logprob": -0.06720073283219538, "compression_ratio": 1.6457564575645756, "no_speech_prob": 5.1408154831733555e-05}, {"id": 306, "seek": 79520, "start": 803.8000000000001, "end": 807.8000000000001, "text": " and so there's real value in doing it jointly with syntactic parsing.", "tokens": [293, 370, 456, 311, 957, 2158, 294, 884, 309, 46557, 365, 23980, 19892, 21156, 278, 13], "temperature": 0.0, "avg_logprob": -0.06720073283219538, "compression_ratio": 1.6457564575645756, "no_speech_prob": 5.1408154831733555e-05}, {"id": 307, "seek": 79520, "start": 807.8000000000001, "end": 810.4000000000001, "text": " For other languages, you have kind of the opposite problem.", "tokens": [1171, 661, 8650, 11, 291, 362, 733, 295, 264, 6182, 1154, 13], "temperature": 0.0, "avg_logprob": -0.06720073283219538, "compression_ratio": 1.6457564575645756, "no_speech_prob": 5.1408154831733555e-05}, {"id": 308, "seek": 79520, "start": 810.4000000000001, "end": 815.2, "text": " So we have here a German word,", "tokens": [407, 321, 362, 510, 257, 6521, 1349, 11], "temperature": 0.0, "avg_logprob": -0.06720073283219538, "compression_ratio": 1.6457564575645756, "no_speech_prob": 5.1408154831733555e-05}, {"id": 309, "seek": 79520, "start": 815.2, "end": 819.2, "text": " and this, you know, is the German word for income tax return.", "tokens": [293, 341, 11, 291, 458, 11, 307, 264, 6521, 1349, 337, 5742, 3366, 2736, 13], "temperature": 0.0, "avg_logprob": -0.06720073283219538, "compression_ratio": 1.6457564575645756, "no_speech_prob": 5.1408154831733555e-05}, {"id": 310, "seek": 79520, "start": 819.2, "end": 823.8000000000001, "text": " Now, whether or not you want that to be sort of one unit", "tokens": [823, 11, 1968, 420, 406, 291, 528, 300, 281, 312, 1333, 295, 472, 4985], "temperature": 0.0, "avg_logprob": -0.06720073283219538, "compression_ratio": 1.6457564575645756, "no_speech_prob": 5.1408154831733555e-05}, {"id": 311, "seek": 82380, "start": 823.8, "end": 825.4, "text": " will depend on what you're looking for.", "tokens": [486, 5672, 322, 437, 291, 434, 1237, 337, 13], "temperature": 0.0, "avg_logprob": -0.08742440448087804, "compression_ratio": 1.7542662116040955, "no_speech_prob": 0.00010382378241047263}, {"id": 312, "seek": 82380, "start": 825.4, "end": 828.8, "text": " For many applications, actually, the English phrase is too short,", "tokens": [1171, 867, 5821, 11, 767, 11, 264, 3669, 9535, 307, 886, 2099, 11], "temperature": 0.0, "avg_logprob": -0.08742440448087804, "compression_ratio": 1.7542662116040955, "no_speech_prob": 0.00010382378241047263}, {"id": 313, "seek": 82380, "start": 828.8, "end": 832.8, "text": " and the domain object, the thing that you want to be, you know,", "tokens": [293, 264, 9274, 2657, 11, 264, 551, 300, 291, 528, 281, 312, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.08742440448087804, "compression_ratio": 1.7542662116040955, "no_speech_prob": 0.00010382378241047263}, {"id": 314, "seek": 82380, "start": 832.8, "end": 836.4, "text": " looking for and having a single node in your knowledge graph for", "tokens": [1237, 337, 293, 1419, 257, 2167, 9984, 294, 428, 3601, 4295, 337], "temperature": 0.0, "avg_logprob": -0.08742440448087804, "compression_ratio": 1.7542662116040955, "no_speech_prob": 0.00010382378241047263}, {"id": 315, "seek": 82380, "start": 836.4, "end": 838.0, "text": " would actually be income tax return.", "tokens": [576, 767, 312, 5742, 3366, 2736, 13], "temperature": 0.0, "avg_logprob": -0.08742440448087804, "compression_ratio": 1.7542662116040955, "no_speech_prob": 0.00010382378241047263}, {"id": 316, "seek": 82380, "start": 838.0, "end": 839.5999999999999, "text": " That's pretty awesome.", "tokens": [663, 311, 1238, 3476, 13], "temperature": 0.0, "avg_logprob": -0.08742440448087804, "compression_ratio": 1.7542662116040955, "no_speech_prob": 0.00010382378241047263}, {"id": 317, "seek": 82380, "start": 839.5999999999999, "end": 842.5999999999999, "text": " But in other applications, maybe you just want to look for tax,", "tokens": [583, 294, 661, 5821, 11, 1310, 291, 445, 528, 281, 574, 337, 3366, 11], "temperature": 0.0, "avg_logprob": -0.08742440448087804, "compression_ratio": 1.7542662116040955, "no_speech_prob": 0.00010382378241047263}, {"id": 318, "seek": 82380, "start": 842.5999999999999, "end": 845.8, "text": " and so in those cases, the German word will be too large", "tokens": [293, 370, 294, 729, 3331, 11, 264, 6521, 1349, 486, 312, 886, 2416], "temperature": 0.0, "avg_logprob": -0.08742440448087804, "compression_ratio": 1.7542662116040955, "no_speech_prob": 0.00010382378241047263}, {"id": 319, "seek": 82380, "start": 845.8, "end": 847.4, "text": " and your data will be too sparse.", "tokens": [293, 428, 1412, 486, 312, 886, 637, 11668, 13], "temperature": 0.0, "avg_logprob": -0.08742440448087804, "compression_ratio": 1.7542662116040955, "no_speech_prob": 0.00010382378241047263}, {"id": 320, "seek": 82380, "start": 847.4, "end": 851.5999999999999, "text": " So there's, you know, there's sort of different aspects of this.", "tokens": [407, 456, 311, 11, 291, 458, 11, 456, 311, 1333, 295, 819, 7270, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.08742440448087804, "compression_ratio": 1.7542662116040955, "no_speech_prob": 0.00010382378241047263}, {"id": 321, "seek": 85160, "start": 851.6, "end": 856.0, "text": " In the bottom left here, we have an example of Hebrew,", "tokens": [682, 264, 2767, 1411, 510, 11, 321, 362, 364, 1365, 295, 17895, 11], "temperature": 0.0, "avg_logprob": -0.14769503410826337, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.00012143214553361759}, {"id": 322, "seek": 85160, "start": 856.0, "end": 861.6, "text": " and like Arabic and a couple of other languages like this,", "tokens": [293, 411, 19938, 293, 257, 1916, 295, 661, 8650, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.14769503410826337, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.00012143214553361759}, {"id": 323, "seek": 85160, "start": 861.6, "end": 865.6, "text": " there's no vowels in the text, and the words tend to be kind of,", "tokens": [456, 311, 572, 44972, 294, 264, 2487, 11, 293, 264, 2283, 3928, 281, 312, 733, 295, 11], "temperature": 0.0, "avg_logprob": -0.14769503410826337, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.00012143214553361759}, {"id": 324, "seek": 85160, "start": 865.6, "end": 869.6, "text": " have all sorts of attachments to them that are difficult to segment off.", "tokens": [362, 439, 7527, 295, 37987, 281, 552, 300, 366, 2252, 281, 9469, 766, 13], "temperature": 0.0, "avg_logprob": -0.14769503410826337, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.00012143214553361759}, {"id": 325, "seek": 85160, "start": 869.6, "end": 872.4, "text": " So there again, you have difficult, like, segmentation problems", "tokens": [407, 456, 797, 11, 291, 362, 2252, 11, 411, 11, 9469, 399, 2740], "temperature": 0.0, "avg_logprob": -0.14769503410826337, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.00012143214553361759}, {"id": 326, "seek": 85160, "start": 872.4, "end": 875.8000000000001, "text": " that are all tangled up with the syntactic process.", "tokens": [300, 366, 439, 47192, 493, 365, 264, 23980, 19892, 1399, 13], "temperature": 0.0, "avg_logprob": -0.14769503410826337, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.00012143214553361759}, {"id": 327, "seek": 87580, "start": 875.8, "end": 882.1999999999999, "text": " Okay, so going forward to sort of an example of what we can do", "tokens": [1033, 11, 370, 516, 2128, 281, 1333, 295, 364, 1365, 295, 437, 321, 393, 360], "temperature": 0.0, "avg_logprob": -0.17062170198648283, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00016857149603310972}, {"id": 328, "seek": 87580, "start": 882.1999999999999, "end": 886.0, "text": " if we, you know, recognize non-white space looking words", "tokens": [498, 321, 11, 291, 458, 11, 5521, 2107, 12, 28865, 1901, 1237, 2283], "temperature": 0.0, "avg_logprob": -0.17062170198648283, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00016857149603310972}, {"id": 329, "seek": 87580, "start": 886.0, "end": 891.5999999999999, "text": " and feed them into some of the other processing stuff that we have.", "tokens": [293, 3154, 552, 666, 512, 295, 264, 661, 9007, 1507, 300, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.17062170198648283, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00016857149603310972}, {"id": 330, "seek": 87580, "start": 891.5999999999999, "end": 895.4, "text": " So this is a demo that we prepared a couple of years ago", "tokens": [407, 341, 307, 257, 10723, 300, 321, 4927, 257, 1916, 295, 924, 2057], "temperature": 0.0, "avg_logprob": -0.17062170198648283, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00016857149603310972}, {"id": 331, "seek": 87580, "start": 895.4, "end": 900.4, "text": " for an approach that we call, that is termed Sense2vec.", "tokens": [337, 364, 3109, 300, 321, 818, 11, 300, 307, 1433, 292, 33123, 17, 303, 66, 13], "temperature": 0.0, "avg_logprob": -0.17062170198648283, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00016857149603310972}, {"id": 332, "seek": 87580, "start": 900.4, "end": 902.8, "text": " So all this is is basically processing text", "tokens": [407, 439, 341, 307, 307, 1936, 9007, 2487], "temperature": 0.0, "avg_logprob": -0.17062170198648283, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00016857149603310972}, {"id": 333, "seek": 87580, "start": 902.8, "end": 905.1999999999999, "text": " using natural language processing tools,", "tokens": [1228, 3303, 2856, 9007, 3873, 11], "temperature": 0.0, "avg_logprob": -0.17062170198648283, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.00016857149603310972}, {"id": 334, "seek": 90520, "start": 905.2, "end": 907.0, "text": " in this case specifically spaCy,", "tokens": [294, 341, 1389, 4682, 32543, 34, 88, 11], "temperature": 0.0, "avg_logprob": -0.10648691887949027, "compression_ratio": 1.693798449612403, "no_speech_prob": 0.00012142776540713385}, {"id": 335, "seek": 90520, "start": 907.0, "end": 911.0, "text": " in order to recognize these concepts that are longer than one word.", "tokens": [294, 1668, 281, 5521, 613, 10392, 300, 366, 2854, 813, 472, 1349, 13], "temperature": 0.0, "avg_logprob": -0.10648691887949027, "compression_ratio": 1.693798449612403, "no_speech_prob": 0.00012142776540713385}, {"id": 336, "seek": 90520, "start": 911.0, "end": 914.2, "text": " So specifically here we look for base noun phrases", "tokens": [407, 4682, 510, 321, 574, 337, 3096, 23307, 20312], "temperature": 0.0, "avg_logprob": -0.10648691887949027, "compression_ratio": 1.693798449612403, "no_speech_prob": 0.00012142776540713385}, {"id": 337, "seek": 90520, "start": 914.2, "end": 917.6, "text": " and also named entities, and we just merge those into one token", "tokens": [293, 611, 4926, 16667, 11, 293, 321, 445, 22183, 729, 666, 472, 14862], "temperature": 0.0, "avg_logprob": -0.10648691887949027, "compression_ratio": 1.693798449612403, "no_speech_prob": 0.00012142776540713385}, {"id": 338, "seek": 90520, "start": 917.6, "end": 922.4000000000001, "text": " before feeding the text forward into a word2vec implementation,", "tokens": [949, 12919, 264, 2487, 2128, 666, 257, 1349, 17, 303, 66, 11420, 11], "temperature": 0.0, "avg_logprob": -0.10648691887949027, "compression_ratio": 1.693798449612403, "no_speech_prob": 0.00012142776540713385}, {"id": 339, "seek": 90520, "start": 922.4000000000001, "end": 925.2, "text": " which gives you sort of these semantic relationships.", "tokens": [597, 2709, 291, 1333, 295, 613, 47982, 6159, 13], "temperature": 0.0, "avg_logprob": -0.10648691887949027, "compression_ratio": 1.693798449612403, "no_speech_prob": 0.00012142776540713385}, {"id": 340, "seek": 90520, "start": 925.2, "end": 929.6, "text": " And this lets you search for and find similarities", "tokens": [400, 341, 6653, 291, 3164, 337, 293, 915, 24197], "temperature": 0.0, "avg_logprob": -0.10648691887949027, "compression_ratio": 1.693798449612403, "no_speech_prob": 0.00012142776540713385}, {"id": 341, "seek": 90520, "start": 929.6, "end": 932.8000000000001, "text": " between phrases which are much longer than one word.", "tokens": [1296, 20312, 597, 366, 709, 2854, 813, 472, 1349, 13], "temperature": 0.0, "avg_logprob": -0.10648691887949027, "compression_ratio": 1.693798449612403, "no_speech_prob": 0.00012142776540713385}, {"id": 342, "seek": 93280, "start": 932.8, "end": 937.0, "text": " And as soon as you do this, you find, ah, the things which I'm searching for", "tokens": [400, 382, 2321, 382, 291, 360, 341, 11, 291, 915, 11, 3716, 11, 264, 721, 597, 286, 478, 10808, 337], "temperature": 0.0, "avg_logprob": -0.09769183476765951, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.00013976215268485248}, {"id": 343, "seek": 93280, "start": 937.0, "end": 938.4, "text": " are much more specific in meaning.", "tokens": [366, 709, 544, 2685, 294, 3620, 13], "temperature": 0.0, "avg_logprob": -0.09769183476765951, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.00013976215268485248}, {"id": 344, "seek": 93280, "start": 938.4, "end": 941.8, "text": " I'm not, you know, looking for, you know, one meaning of learning", "tokens": [286, 478, 406, 11, 291, 458, 11, 1237, 337, 11, 291, 458, 11, 472, 3620, 295, 2539], "temperature": 0.0, "avg_logprob": -0.09769183476765951, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.00013976215268485248}, {"id": 345, "seek": 93280, "start": 941.8, "end": 943.1999999999999, "text": " or one meaning of processing,", "tokens": [420, 472, 3620, 295, 9007, 11], "temperature": 0.0, "avg_logprob": -0.09769183476765951, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.00013976215268485248}, {"id": 346, "seek": 93280, "start": 943.1999999999999, "end": 945.0, "text": " which doesn't tend to be so useful or interesting.", "tokens": [597, 1177, 380, 3928, 281, 312, 370, 4420, 420, 1880, 13], "temperature": 0.0, "avg_logprob": -0.09769183476765951, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.00013976215268485248}, {"id": 347, "seek": 93280, "start": 945.0, "end": 949.4, "text": " Instead, I'm looking, you can find things related to natural language processing,", "tokens": [7156, 11, 286, 478, 1237, 11, 291, 393, 915, 721, 4077, 281, 3303, 2856, 9007, 11], "temperature": 0.0, "avg_logprob": -0.09769183476765951, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.00013976215268485248}, {"id": 348, "seek": 93280, "start": 949.4, "end": 952.0, "text": " and then you see, ah, machine learning, computer vision, et cetera.", "tokens": [293, 550, 291, 536, 11, 3716, 11, 3479, 2539, 11, 3820, 5201, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.09769183476765951, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.00013976215268485248}, {"id": 349, "seek": 93280, "start": 952.0, "end": 954.4, "text": " These are, you know, real results that came out of the thing,", "tokens": [1981, 366, 11, 291, 458, 11, 957, 3542, 300, 1361, 484, 295, 264, 551, 11], "temperature": 0.0, "avg_logprob": -0.09769183476765951, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.00013976215268485248}, {"id": 350, "seek": 93280, "start": 954.4, "end": 958.4, "text": " as soon as you did this division.", "tokens": [382, 2321, 382, 291, 630, 341, 10044, 13], "temperature": 0.0, "avg_logprob": -0.09769183476765951, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.00013976215268485248}, {"id": 351, "seek": 93280, "start": 958.4, "end": 962.1999999999999, "text": " And so we can do this for other languages as well.", "tokens": [400, 370, 321, 393, 360, 341, 337, 661, 8650, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.09769183476765951, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.00013976215268485248}, {"id": 352, "seek": 96220, "start": 962.2, "end": 967.8000000000001, "text": " So if we were doing, if we were hoping to use word2vec for a language like Chinese,", "tokens": [407, 498, 321, 645, 884, 11, 498, 321, 645, 7159, 281, 764, 1349, 17, 303, 66, 337, 257, 2856, 411, 4649, 11], "temperature": 0.0, "avg_logprob": -0.11622204651703706, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.56069498998113e-05}, {"id": 353, "seek": 96220, "start": 967.8000000000001, "end": 971.2, "text": " you really want to be processing it into words before you do that.", "tokens": [291, 534, 528, 281, 312, 9007, 309, 666, 2283, 949, 291, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.11622204651703706, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.56069498998113e-05}, {"id": 354, "seek": 96220, "start": 971.2, "end": 974.2, "text": " Or if you're going to do this for a language like Finnish,", "tokens": [1610, 498, 291, 434, 516, 281, 360, 341, 337, 257, 2856, 411, 38429, 11], "temperature": 0.0, "avg_logprob": -0.11622204651703706, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.56069498998113e-05}, {"id": 355, "seek": 96220, "start": 974.2, "end": 979.4000000000001, "text": " you really want to cut off the morphological suffixes before you do this.", "tokens": [291, 534, 528, 281, 1723, 766, 264, 25778, 4383, 3889, 36005, 949, 291, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.11622204651703706, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.56069498998113e-05}, {"id": 356, "seek": 96220, "start": 979.4000000000001, "end": 986.0, "text": " Okay. So, incidentally, Innis has cleaned up the sensitive recently,", "tokens": [1033, 13, 407, 11, 9348, 379, 11, 682, 10661, 575, 16146, 493, 264, 9477, 3938, 11], "temperature": 0.0, "avg_logprob": -0.11622204651703706, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.56069498998113e-05}, {"id": 357, "seek": 96220, "start": 986.0, "end": 990.6, "text": " so you can actually use this as a handy component within spaCy.", "tokens": [370, 291, 393, 767, 764, 341, 382, 257, 13239, 6542, 1951, 32543, 34, 88, 13], "temperature": 0.0, "avg_logprob": -0.11622204651703706, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.56069498998113e-05}, {"id": 358, "seek": 99060, "start": 990.6, "end": 996.6, "text": " So you can load up a standard model and then add a component", "tokens": [407, 291, 393, 3677, 493, 257, 3832, 2316, 293, 550, 909, 257, 6542], "temperature": 0.0, "avg_logprob": -0.10899328148883322, "compression_ratio": 1.6455223880597014, "no_speech_prob": 5.9180852986173704e-05}, {"id": 359, "seek": 99060, "start": 996.6, "end": 998.4, "text": " that gives you these sensitive excenses.", "tokens": [300, 2709, 291, 613, 9477, 1624, 9085, 13], "temperature": 0.0, "avg_logprob": -0.10899328148883322, "compression_ratio": 1.6455223880597014, "no_speech_prob": 5.9180852986173704e-05}, {"id": 360, "seek": 99060, "start": 998.4, "end": 1004.2, "text": " So you can just say, all right, the token for 3 would be natural language processing", "tokens": [407, 291, 393, 445, 584, 11, 439, 558, 11, 264, 14862, 337, 805, 576, 312, 3303, 2856, 9007], "temperature": 0.0, "avg_logprob": -0.10899328148883322, "compression_ratio": 1.6455223880597014, "no_speech_prob": 5.9180852986173704e-05}, {"id": 361, "seek": 99060, "start": 1004.2, "end": 1005.8000000000001, "text": " because it would do the merging for you.", "tokens": [570, 309, 576, 360, 264, 44559, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.10899328148883322, "compression_ratio": 1.6455223880597014, "no_speech_prob": 5.9180852986173704e-05}, {"id": 362, "seek": 99060, "start": 1005.8000000000001, "end": 1007.8000000000001, "text": " And then you can also look up the similarity.", "tokens": [400, 550, 291, 393, 611, 574, 493, 264, 32194, 13], "temperature": 0.0, "avg_logprob": -0.10899328148883322, "compression_ratio": 1.6455223880597014, "no_speech_prob": 5.9180852986173704e-05}, {"id": 363, "seek": 99060, "start": 1007.8000000000001, "end": 1012.4, "text": " So it's now much easier to actually use the pre-trained model", "tokens": [407, 309, 311, 586, 709, 3571, 281, 767, 764, 264, 659, 12, 17227, 2001, 2316], "temperature": 0.0, "avg_logprob": -0.10899328148883322, "compression_ratio": 1.6455223880597014, "no_speech_prob": 5.9180852986173704e-05}, {"id": 364, "seek": 99060, "start": 1012.4, "end": 1016.4, "text": " and use that approach within spaCy.", "tokens": [293, 764, 300, 3109, 1951, 32543, 34, 88, 13], "temperature": 0.0, "avg_logprob": -0.10899328148883322, "compression_ratio": 1.6455223880597014, "no_speech_prob": 5.9180852986173704e-05}, {"id": 365, "seek": 99060, "start": 1016.4, "end": 1020.0, "text": " Incidentally, we have this concept of an extension attribute in spaCy", "tokens": [7779, 36578, 11, 321, 362, 341, 3410, 295, 364, 10320, 19667, 294, 32543, 34, 88], "temperature": 0.0, "avg_logprob": -0.10899328148883322, "compression_ratio": 1.6455223880597014, "no_speech_prob": 5.9180852986173704e-05}, {"id": 366, "seek": 102000, "start": 1020.0, "end": 1023.6, "text": " so that you can kind of attach your own things to the tokens", "tokens": [370, 300, 291, 393, 733, 295, 5085, 428, 1065, 721, 281, 264, 22667], "temperature": 0.0, "avg_logprob": -0.10193191779838813, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.0001583794510224834}, {"id": 367, "seek": 102000, "start": 1023.6, "end": 1029.0, "text": " so that you can basically attach your own little markups or processing things.", "tokens": [370, 300, 291, 393, 1936, 5085, 428, 1065, 707, 1491, 7528, 420, 9007, 721, 13], "temperature": 0.0, "avg_logprob": -0.10193191779838813, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.0001583794510224834}, {"id": 368, "seek": 102000, "start": 1029.0, "end": 1035.6, "text": " So this underscore object is kind of a free space that you can attach attributes to,", "tokens": [407, 341, 37556, 2657, 307, 733, 295, 257, 1737, 1901, 300, 291, 393, 5085, 17212, 281, 11], "temperature": 0.0, "avg_logprob": -0.10193191779838813, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.0001583794510224834}, {"id": 369, "seek": 102000, "start": 1035.6, "end": 1037.6, "text": " which ends up being quite convenient.", "tokens": [597, 5314, 493, 885, 1596, 10851, 13], "temperature": 0.0, "avg_logprob": -0.10193191779838813, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.0001583794510224834}, {"id": 370, "seek": 102000, "start": 1037.6, "end": 1042.4, "text": " It's a lot more convenient than trying to subclass something or something.", "tokens": [467, 311, 257, 688, 544, 10851, 813, 1382, 281, 1422, 11665, 746, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.10193191779838813, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.0001583794510224834}, {"id": 371, "seek": 102000, "start": 1042.4, "end": 1046.6, "text": " Okay. So for the rest of the talk,", "tokens": [1033, 13, 407, 337, 264, 1472, 295, 264, 751, 11], "temperature": 0.0, "avg_logprob": -0.10193191779838813, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.0001583794510224834}, {"id": 372, "seek": 104660, "start": 1046.6, "end": 1051.6, "text": " I'll give you a little bit of a pretty brief overview of the parsing algorithm", "tokens": [286, 603, 976, 291, 257, 707, 857, 295, 257, 1238, 5353, 12492, 295, 264, 21156, 278, 9284], "temperature": 0.0, "avg_logprob": -0.07774746284056246, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.3006061635678634e-05}, {"id": 373, "seek": 104660, "start": 1051.6, "end": 1055.6, "text": " and then explain how we're modifying the parsing algorithm", "tokens": [293, 550, 2903, 577, 321, 434, 42626, 264, 21156, 278, 9284], "temperature": 0.0, "avg_logprob": -0.07774746284056246, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.3006061635678634e-05}, {"id": 374, "seek": 104660, "start": 1055.6, "end": 1058.8, "text": " to work with languages other than English", "tokens": [281, 589, 365, 8650, 661, 813, 3669], "temperature": 0.0, "avg_logprob": -0.07774746284056246, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.3006061635678634e-05}, {"id": 375, "seek": 104660, "start": 1058.8, "end": 1063.6, "text": " so that we can basically broaden out the support of spaCy to these other languages.", "tokens": [370, 300, 321, 393, 1936, 47045, 484, 264, 1406, 295, 32543, 34, 88, 281, 613, 661, 8650, 13], "temperature": 0.0, "avg_logprob": -0.07774746284056246, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.3006061635678634e-05}, {"id": 376, "seek": 104660, "start": 1063.6, "end": 1070.0, "text": " So what we see here is a completed parse.", "tokens": [407, 437, 321, 536, 510, 307, 257, 7365, 48377, 13], "temperature": 0.0, "avg_logprob": -0.07774746284056246, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.3006061635678634e-05}, {"id": 377, "seek": 104660, "start": 1070.0, "end": 1073.6, "text": " And I'm going to sort of talk you through the steps", "tokens": [400, 286, 478, 516, 281, 1333, 295, 751, 291, 807, 264, 4439], "temperature": 0.0, "avg_logprob": -0.07774746284056246, "compression_ratio": 1.63013698630137, "no_speech_prob": 1.3006061635678634e-05}, {"id": 378, "seek": 107360, "start": 1073.6, "end": 1077.6, "text": " or the decision points that the parser is going to make to derive this structure.", "tokens": [420, 264, 3537, 2793, 300, 264, 21156, 260, 307, 516, 281, 652, 281, 28446, 341, 3877, 13], "temperature": 0.0, "avg_logprob": -0.09499975351186898, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00021308074065018445}, {"id": 379, "seek": 107360, "start": 1077.6, "end": 1082.3999999999999, "text": " And so the kind of key thing to keep in mind", "tokens": [400, 370, 264, 733, 295, 2141, 551, 281, 1066, 294, 1575], "temperature": 0.0, "avg_logprob": -0.09499975351186898, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00021308074065018445}, {"id": 380, "seek": 107360, "start": 1082.3999999999999, "end": 1087.3999999999999, "text": " or the key aspect of the solution is that", "tokens": [420, 264, 2141, 4171, 295, 264, 3827, 307, 300], "temperature": 0.0, "avg_logprob": -0.09499975351186898, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00021308074065018445}, {"id": 381, "seek": 107360, "start": 1087.3999999999999, "end": 1090.8, "text": " it's going to read the sentence from left to right and maintain some state.", "tokens": [309, 311, 516, 281, 1401, 264, 8174, 490, 1411, 281, 558, 293, 6909, 512, 1785, 13], "temperature": 0.0, "avg_logprob": -0.09499975351186898, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00021308074065018445}, {"id": 382, "seek": 107360, "start": 1090.8, "end": 1094.6, "text": " And then it's going to have a sort of fixed inventory of actions", "tokens": [400, 550, 309, 311, 516, 281, 362, 257, 1333, 295, 6806, 14228, 295, 5909], "temperature": 0.0, "avg_logprob": -0.09499975351186898, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00021308074065018445}, {"id": 383, "seek": 107360, "start": 1094.6, "end": 1099.8, "text": " that it has to choose between to manipulate the current parse state to build up the arcs.", "tokens": [300, 309, 575, 281, 2826, 1296, 281, 20459, 264, 2190, 48377, 1785, 281, 1322, 493, 264, 10346, 82, 13], "temperature": 0.0, "avg_logprob": -0.09499975351186898, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00021308074065018445}, {"id": 384, "seek": 107360, "start": 1099.8, "end": 1103.1999999999998, "text": " And this type of approach, which is called transition-based parsing,", "tokens": [400, 341, 2010, 295, 3109, 11, 597, 307, 1219, 6034, 12, 6032, 21156, 278, 11], "temperature": 0.0, "avg_logprob": -0.09499975351186898, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00021308074065018445}, {"id": 385, "seek": 110320, "start": 1103.2, "end": 1108.6000000000001, "text": " I find deeply satisfying because it's linear in time", "tokens": [286, 915, 8760, 18348, 570, 309, 311, 8213, 294, 565], "temperature": 0.0, "avg_logprob": -0.0654421613997772, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.00010383972403360531}, {"id": 386, "seek": 110320, "start": 1108.6000000000001, "end": 1112.4, "text": " because you only make so many decisions per word.", "tokens": [570, 291, 787, 652, 370, 867, 5327, 680, 1349, 13], "temperature": 0.0, "avg_logprob": -0.0654421613997772, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.00010383972403360531}, {"id": 387, "seek": 110320, "start": 1112.4, "end": 1116.4, "text": " And I do think that it makes a lot of sense to take algorithms", "tokens": [400, 286, 360, 519, 300, 309, 1669, 257, 688, 295, 2020, 281, 747, 14642], "temperature": 0.0, "avg_logprob": -0.0654421613997772, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.00010383972403360531}, {"id": 388, "seek": 110320, "start": 1116.4, "end": 1118.2, "text": " which process language incrementally.", "tokens": [597, 1399, 2856, 26200, 379, 13], "temperature": 0.0, "avg_logprob": -0.0654421613997772, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.00010383972403360531}, {"id": 389, "seek": 110320, "start": 1118.2, "end": 1121.6000000000001, "text": " I think that that's sort of deeply satisfying and sort of correct", "tokens": [286, 519, 300, 300, 311, 1333, 295, 8760, 18348, 293, 1333, 295, 3006], "temperature": 0.0, "avg_logprob": -0.0654421613997772, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.00010383972403360531}, {"id": 390, "seek": 110320, "start": 1121.6000000000001, "end": 1124.4, "text": " in a way that a lot of other approaches to parsing aren't.", "tokens": [294, 257, 636, 300, 257, 688, 295, 661, 11587, 281, 21156, 278, 3212, 380, 13], "temperature": 0.0, "avg_logprob": -0.0654421613997772, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.00010383972403360531}, {"id": 391, "seek": 110320, "start": 1124.4, "end": 1126.2, "text": " And it's also a very flexible approach.", "tokens": [400, 309, 311, 611, 257, 588, 11358, 3109, 13], "temperature": 0.0, "avg_logprob": -0.0654421613997772, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.00010383972403360531}, {"id": 392, "seek": 110320, "start": 1126.2, "end": 1130.4, "text": " So we can do joint modeling and have it output all sorts of other structures", "tokens": [407, 321, 393, 360, 7225, 15983, 293, 362, 309, 5598, 439, 7527, 295, 661, 9227], "temperature": 0.0, "avg_logprob": -0.0654421613997772, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.00010383972403360531}, {"id": 393, "seek": 110320, "start": 1130.4, "end": 1132.8, "text": " as well as the parse tree.", "tokens": [382, 731, 382, 264, 48377, 4230, 13], "temperature": 0.0, "avg_logprob": -0.0654421613997772, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.00010383972403360531}, {"id": 394, "seek": 113280, "start": 1132.8, "end": 1134.2, "text": " And that's actually what we're going to do.", "tokens": [400, 300, 311, 767, 437, 321, 434, 516, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.12054355939229329, "compression_ratio": 1.844, "no_speech_prob": 4.907671609544195e-05}, {"id": 395, "seek": 113280, "start": 1134.2, "end": 1139.6, "text": " So already in spaCy we have joint prediction of the sentence boundaries in the parse tree.", "tokens": [407, 1217, 294, 32543, 34, 88, 321, 362, 7225, 17630, 295, 264, 8174, 13180, 294, 264, 48377, 4230, 13], "temperature": 0.0, "avg_logprob": -0.12054355939229329, "compression_ratio": 1.844, "no_speech_prob": 4.907671609544195e-05}, {"id": 396, "seek": 113280, "start": 1139.6, "end": 1144.8, "text": " And what we're going to do is extend this to this joint prediction of word boundaries as well.", "tokens": [400, 437, 321, 434, 516, 281, 360, 307, 10101, 341, 281, 341, 7225, 17630, 295, 1349, 13180, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12054355939229329, "compression_ratio": 1.844, "no_speech_prob": 4.907671609544195e-05}, {"id": 397, "seek": 113280, "start": 1144.8, "end": 1150.6, "text": " Okay, so here's how the sort of decision process of building the tree works.", "tokens": [1033, 11, 370, 510, 311, 577, 264, 1333, 295, 3537, 1399, 295, 2390, 264, 4230, 1985, 13], "temperature": 0.0, "avg_logprob": -0.12054355939229329, "compression_ratio": 1.844, "no_speech_prob": 4.907671609544195e-05}, {"id": 398, "seek": 113280, "start": 1150.6, "end": 1153.0, "text": " So we start off with in an initial state.", "tokens": [407, 321, 722, 766, 365, 294, 364, 5883, 1785, 13], "temperature": 0.0, "avg_logprob": -0.12054355939229329, "compression_ratio": 1.844, "no_speech_prob": 4.907671609544195e-05}, {"id": 399, "seek": 113280, "start": 1153.0, "end": 1156.8, "text": " And so for sort of ease of notation or ease of readability,", "tokens": [400, 370, 337, 1333, 295, 12708, 295, 24657, 420, 12708, 295, 1401, 2310, 11], "temperature": 0.0, "avg_logprob": -0.12054355939229329, "compression_ratio": 1.844, "no_speech_prob": 4.907671609544195e-05}, {"id": 400, "seek": 113280, "start": 1156.8, "end": 1160.3999999999999, "text": " we're notating the sort of first word of the buffer.", "tokens": [321, 434, 406, 990, 264, 1333, 295, 700, 1349, 295, 264, 21762, 13], "temperature": 0.0, "avg_logprob": -0.12054355939229329, "compression_ratio": 1.844, "no_speech_prob": 4.907671609544195e-05}, {"id": 401, "seek": 116040, "start": 1160.4, "end": 1166.0, "text": " And so the first word that's kind of being focused on as this kind of beam of highlighting.", "tokens": [400, 370, 264, 700, 1349, 300, 311, 733, 295, 885, 5178, 322, 382, 341, 733, 295, 14269, 295, 26551, 13], "temperature": 0.0, "avg_logprob": -0.09510313837151778, "compression_ratio": 1.854077253218884, "no_speech_prob": 7.253264629980549e-05}, {"id": 402, "seek": 116040, "start": 1166.0, "end": 1170.4, "text": " And then the other element of the state is a stack.", "tokens": [400, 550, 264, 661, 4478, 295, 264, 1785, 307, 257, 8630, 13], "temperature": 0.0, "avg_logprob": -0.09510313837151778, "compression_ratio": 1.854077253218884, "no_speech_prob": 7.253264629980549e-05}, {"id": 403, "seek": 116040, "start": 1170.4, "end": 1173.8000000000002, "text": " And so when as the first action that we do,", "tokens": [400, 370, 562, 382, 264, 700, 3069, 300, 321, 360, 11], "temperature": 0.0, "avg_logprob": -0.09510313837151778, "compression_ratio": 1.854077253218884, "no_speech_prob": 7.253264629980549e-05}, {"id": 404, "seek": 116040, "start": 1173.8000000000002, "end": 1176.6000000000001, "text": " we have an action that can advance the buffer one", "tokens": [321, 362, 364, 3069, 300, 393, 7295, 264, 21762, 472], "temperature": 0.0, "avg_logprob": -0.09510313837151778, "compression_ratio": 1.854077253218884, "no_speech_prob": 7.253264629980549e-05}, {"id": 405, "seek": 116040, "start": 1176.6000000000001, "end": 1180.4, "text": " and put the word that was previously at the start of the buffer onto the stack.", "tokens": [293, 829, 264, 1349, 300, 390, 8046, 412, 264, 722, 295, 264, 21762, 3911, 264, 8630, 13], "temperature": 0.0, "avg_logprob": -0.09510313837151778, "compression_ratio": 1.854077253218884, "no_speech_prob": 7.253264629980549e-05}, {"id": 406, "seek": 116040, "start": 1180.4, "end": 1183.4, "text": " So here's what that shift move is going to look like.", "tokens": [407, 510, 311, 437, 300, 5513, 1286, 307, 516, 281, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.09510313837151778, "compression_ratio": 1.854077253218884, "no_speech_prob": 7.253264629980549e-05}, {"id": 407, "seek": 116040, "start": 1183.4, "end": 1188.2, "text": " So here we have Google on the stack, which we write up here.", "tokens": [407, 510, 321, 362, 3329, 322, 264, 8630, 11, 597, 321, 2464, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.09510313837151778, "compression_ratio": 1.854077253218884, "no_speech_prob": 7.253264629980549e-05}, {"id": 408, "seek": 118820, "start": 1188.2, "end": 1191.0, "text": " And the first word of the buffer is reader.", "tokens": [400, 264, 700, 1349, 295, 264, 21762, 307, 15149, 13], "temperature": 0.0, "avg_logprob": -0.09748854288240759, "compression_ratio": 1.875, "no_speech_prob": 8.939428880694322e-06}, {"id": 409, "seek": 118820, "start": 1191.0, "end": 1199.0, "text": " And so then another action that we can take is to form a dependency arc between the word that's on top of the stack and the first word of the buffer.", "tokens": [400, 370, 550, 1071, 3069, 300, 321, 393, 747, 307, 281, 1254, 257, 33621, 10346, 1296, 264, 1349, 300, 311, 322, 1192, 295, 264, 8630, 293, 264, 700, 1349, 295, 264, 21762, 13], "temperature": 0.0, "avg_logprob": -0.09748854288240759, "compression_ratio": 1.875, "no_speech_prob": 8.939428880694322e-06}, {"id": 410, "seek": 118820, "start": 1199.0, "end": 1204.3, "text": " So in this case, we want to attach Google as a child of reader.", "tokens": [407, 294, 341, 1389, 11, 321, 528, 281, 5085, 3329, 382, 257, 1440, 295, 15149, 13], "temperature": 0.0, "avg_logprob": -0.09748854288240759, "compression_ratio": 1.875, "no_speech_prob": 8.939428880694322e-06}, {"id": 411, "seek": 118820, "start": 1204.3, "end": 1205.8, "text": " So we have an action that does that.", "tokens": [407, 321, 362, 364, 3069, 300, 775, 300, 13], "temperature": 0.0, "avg_logprob": -0.09748854288240759, "compression_ratio": 1.875, "no_speech_prob": 8.939428880694322e-06}, {"id": 412, "seek": 118820, "start": 1205.8, "end": 1210.9, "text": " And because we're building a tree, when we make an arc to Google,", "tokens": [400, 570, 321, 434, 2390, 257, 4230, 11, 562, 321, 652, 364, 10346, 281, 3329, 11], "temperature": 0.0, "avg_logprob": -0.09748854288240759, "compression_ratio": 1.875, "no_speech_prob": 8.939428880694322e-06}, {"id": 413, "seek": 118820, "start": 1210.9, "end": 1215.4, "text": " we know that we can pop it from the stack because it's a tree,", "tokens": [321, 458, 300, 321, 393, 1665, 309, 490, 264, 8630, 570, 309, 311, 257, 4230, 11], "temperature": 0.0, "avg_logprob": -0.09748854288240759, "compression_ratio": 1.875, "no_speech_prob": 8.939428880694322e-06}, {"id": 414, "seek": 118820, "start": 1215.4, "end": 1217.4, "text": " it only can have one head.", "tokens": [309, 787, 393, 362, 472, 1378, 13], "temperature": 0.0, "avg_logprob": -0.09748854288240759, "compression_ratio": 1.875, "no_speech_prob": 8.939428880694322e-06}, {"id": 415, "seek": 121740, "start": 1217.4, "end": 1219.6000000000001, "text": " It can only have sort of one attachment point.", "tokens": [467, 393, 787, 362, 1333, 295, 472, 19431, 935, 13], "temperature": 0.0, "avg_logprob": -0.09355117335464015, "compression_ratio": 1.7306273062730628, "no_speech_prob": 4.331281888880767e-05}, {"id": 416, "seek": 121740, "start": 1219.6000000000001, "end": 1223.4, "text": " It's not a different type of graph.", "tokens": [467, 311, 406, 257, 819, 2010, 295, 4295, 13], "temperature": 0.0, "avg_logprob": -0.09355117335464015, "compression_ratio": 1.7306273062730628, "no_speech_prob": 4.331281888880767e-05}, {"id": 417, "seek": 121740, "start": 1223.4, "end": 1227.2, "text": " And so that means that we can kind of do that and keep moving forward.", "tokens": [400, 370, 300, 1355, 300, 321, 393, 733, 295, 360, 300, 293, 1066, 2684, 2128, 13], "temperature": 0.0, "avg_logprob": -0.09355117335464015, "compression_ratio": 1.7306273062730628, "no_speech_prob": 4.331281888880767e-05}, {"id": 418, "seek": 121740, "start": 1227.2, "end": 1228.4, "text": " So here's what that looks like.", "tokens": [407, 510, 311, 437, 300, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.09355117335464015, "compression_ratio": 1.7306273062730628, "no_speech_prob": 4.331281888880767e-05}, {"id": 419, "seek": 121740, "start": 1228.4, "end": 1232.0, "text": " We add an arc and pop Google from the stack.", "tokens": [492, 909, 364, 10346, 293, 1665, 3329, 490, 264, 8630, 13], "temperature": 0.0, "avg_logprob": -0.09355117335464015, "compression_ratio": 1.7306273062730628, "no_speech_prob": 4.331281888880767e-05}, {"id": 420, "seek": 121740, "start": 1232.0, "end": 1234.6000000000001, "text": " So now we make the next move.", "tokens": [407, 586, 321, 652, 264, 958, 1286, 13], "temperature": 0.0, "avg_logprob": -0.09355117335464015, "compression_ratio": 1.7306273062730628, "no_speech_prob": 4.331281888880767e-05}, {"id": 421, "seek": 121740, "start": 1234.6000000000001, "end": 1236.0, "text": " Clearly, we've got no words on the stack.", "tokens": [24120, 11, 321, 600, 658, 572, 2283, 322, 264, 8630, 13], "temperature": 0.0, "avg_logprob": -0.09355117335464015, "compression_ratio": 1.7306273062730628, "no_speech_prob": 4.331281888880767e-05}, {"id": 422, "seek": 121740, "start": 1236.0, "end": 1239.6000000000001, "text": " So we should put reader on the stacks that we can continue.", "tokens": [407, 321, 820, 829, 15149, 322, 264, 30792, 300, 321, 393, 2354, 13], "temperature": 0.0, "avg_logprob": -0.09355117335464015, "compression_ratio": 1.7306273062730628, "no_speech_prob": 4.331281888880767e-05}, {"id": 423, "seek": 121740, "start": 1239.6000000000001, "end": 1241.1000000000001, "text": " Now we're at was.", "tokens": [823, 321, 434, 412, 390, 13], "temperature": 0.0, "avg_logprob": -0.09355117335464015, "compression_ratio": 1.7306273062730628, "no_speech_prob": 4.331281888880767e-05}, {"id": 424, "seek": 121740, "start": 1241.1000000000001, "end": 1245.8000000000002, "text": " And now we want to decide whether we should make an arc directly between was and reader.", "tokens": [400, 586, 321, 528, 281, 4536, 1968, 321, 820, 652, 364, 10346, 3838, 1296, 390, 293, 15149, 13], "temperature": 0.0, "avg_logprob": -0.09355117335464015, "compression_ratio": 1.7306273062730628, "no_speech_prob": 4.331281888880767e-05}, {"id": 425, "seek": 124580, "start": 1245.8, "end": 1248.6, "text": " In this case, no, we want to attach was to canceled.", "tokens": [682, 341, 1389, 11, 572, 11, 321, 528, 281, 5085, 390, 281, 24839, 13], "temperature": 0.0, "avg_logprob": -0.09562195048612707, "compression_ratio": 1.75, "no_speech_prob": 1.8631157217896543e-05}, {"id": 426, "seek": 124580, "start": 1248.6, "end": 1252.6, "text": " So we're going to move was onto the stack and move forward onto canceled.", "tokens": [407, 321, 434, 516, 281, 1286, 390, 3911, 264, 8630, 293, 1286, 2128, 3911, 24839, 13], "temperature": 0.0, "avg_logprob": -0.09562195048612707, "compression_ratio": 1.75, "no_speech_prob": 1.8631157217896543e-05}, {"id": 427, "seek": 124580, "start": 1252.6, "end": 1258.0, "text": " So then here we do want this arc between cancel and was.", "tokens": [407, 550, 510, 321, 360, 528, 341, 10346, 1296, 10373, 293, 390, 13], "temperature": 0.0, "avg_logprob": -0.09562195048612707, "compression_ratio": 1.75, "no_speech_prob": 1.8631157217896543e-05}, {"id": 428, "seek": 124580, "start": 1258.0, "end": 1259.6, "text": " So we do another left arc.", "tokens": [407, 321, 360, 1071, 1411, 10346, 13], "temperature": 0.0, "avg_logprob": -0.09562195048612707, "compression_ratio": 1.75, "no_speech_prob": 1.8631157217896543e-05}, {"id": 429, "seek": 124580, "start": 1259.6, "end": 1261.6, "text": " And so we basically continue here.", "tokens": [400, 370, 321, 1936, 2354, 510, 13], "temperature": 0.0, "avg_logprob": -0.09562195048612707, "compression_ratio": 1.75, "no_speech_prob": 1.8631157217896543e-05}, {"id": 430, "seek": 124580, "start": 1261.6, "end": 1266.3999999999999, "text": " So sort of stepping back a bit and thinking about this,", "tokens": [407, 1333, 295, 16821, 646, 257, 857, 293, 1953, 466, 341, 11], "temperature": 0.0, "avg_logprob": -0.09562195048612707, "compression_ratio": 1.75, "no_speech_prob": 1.8631157217896543e-05}, {"id": 431, "seek": 124580, "start": 1266.3999999999999, "end": 1268.5, "text": " we've got a fixed inventory of actions.", "tokens": [321, 600, 658, 257, 6806, 14228, 295, 5909, 13], "temperature": 0.0, "avg_logprob": -0.09562195048612707, "compression_ratio": 1.75, "no_speech_prob": 1.8631157217896543e-05}, {"id": 432, "seek": 124580, "start": 1268.5, "end": 1272.8, "text": " And as long as we can predict the right sequence of those actions,", "tokens": [400, 382, 938, 382, 321, 393, 6069, 264, 558, 8310, 295, 729, 5909, 11], "temperature": 0.0, "avg_logprob": -0.09562195048612707, "compression_ratio": 1.75, "no_speech_prob": 1.8631157217896543e-05}, {"id": 433, "seek": 124580, "start": 1272.8, "end": 1274.5, "text": " we can derive the correct paths.", "tokens": [321, 393, 28446, 264, 3006, 14518, 13], "temperature": 0.0, "avg_logprob": -0.09562195048612707, "compression_ratio": 1.75, "no_speech_prob": 1.8631157217896543e-05}, {"id": 434, "seek": 127450, "start": 1274.5, "end": 1276.9, "text": " So that's how the machine learning model is going to work here.", "tokens": [407, 300, 311, 577, 264, 3479, 2539, 2316, 307, 516, 281, 589, 510, 13], "temperature": 0.0, "avg_logprob": -0.0626162633503953, "compression_ratio": 1.8655737704918032, "no_speech_prob": 2.930758273578249e-05}, {"id": 435, "seek": 127450, "start": 1276.9, "end": 1279.7, "text": " The machine learning model is going to be a classifier that predicts,", "tokens": [440, 3479, 2539, 2316, 307, 516, 281, 312, 257, 1508, 9902, 300, 6069, 82, 11], "temperature": 0.0, "avg_logprob": -0.0626162633503953, "compression_ratio": 1.8655737704918032, "no_speech_prob": 2.930758273578249e-05}, {"id": 436, "seek": 127450, "start": 1279.7, "end": 1282.8, "text": " given some state, what to do next.", "tokens": [2212, 512, 1785, 11, 437, 281, 360, 958, 13], "temperature": 0.0, "avg_logprob": -0.0626162633503953, "compression_ratio": 1.8655737704918032, "no_speech_prob": 2.930758273578249e-05}, {"id": 437, "seek": 127450, "start": 1282.8, "end": 1286.8, "text": " And you can sort of imagine that we can have other actions instead", "tokens": [400, 291, 393, 1333, 295, 3811, 300, 321, 393, 362, 661, 5909, 2602], "temperature": 0.0, "avg_logprob": -0.0626162633503953, "compression_ratio": 1.8655737704918032, "no_speech_prob": 2.930758273578249e-05}, {"id": 438, "seek": 127450, "start": 1286.8, "end": 1289.7, "text": " if we wanted to predict other aspects of the structure.", "tokens": [498, 321, 1415, 281, 6069, 661, 7270, 295, 264, 3877, 13], "temperature": 0.0, "avg_logprob": -0.0626162633503953, "compression_ratio": 1.8655737704918032, "no_speech_prob": 2.930758273578249e-05}, {"id": 439, "seek": 127450, "start": 1289.7, "end": 1293.8, "text": " So in the case of spaCy, we have an action that inserts a sentence boundary.", "tokens": [407, 294, 264, 1389, 295, 32543, 34, 88, 11, 321, 362, 364, 3069, 300, 49163, 257, 8174, 12866, 13], "temperature": 0.0, "avg_logprob": -0.0626162633503953, "compression_ratio": 1.8655737704918032, "no_speech_prob": 2.930758273578249e-05}, {"id": 440, "seek": 127450, "start": 1293.8, "end": 1297.3, "text": " So it just says, all right, given the words that are currently on the stack,", "tokens": [407, 309, 445, 1619, 11, 439, 558, 11, 2212, 264, 2283, 300, 366, 4362, 322, 264, 8630, 11], "temperature": 0.0, "avg_logprob": -0.0626162633503953, "compression_ratio": 1.8655737704918032, "no_speech_prob": 2.930758273578249e-05}, {"id": 441, "seek": 127450, "start": 1297.3, "end": 1300.4, "text": " you have to make actions that can clear the stack,", "tokens": [291, 362, 281, 652, 5909, 300, 393, 1850, 264, 8630, 11], "temperature": 0.0, "avg_logprob": -0.0626162633503953, "compression_ratio": 1.8655737704918032, "no_speech_prob": 2.930758273578249e-05}, {"id": 442, "seek": 127450, "start": 1300.4, "end": 1304.2, "text": " but you're not allowed to push the next token until your stack is clear.", "tokens": [457, 291, 434, 406, 4350, 281, 2944, 264, 958, 14862, 1826, 428, 8630, 307, 1850, 13], "temperature": 0.0, "avg_logprob": -0.0626162633503953, "compression_ratio": 1.8655737704918032, "no_speech_prob": 2.930758273578249e-05}, {"id": 443, "seek": 130420, "start": 1304.2, "end": 1308.3, "text": " And that means that there's going to be a sentence boundary there.", "tokens": [400, 300, 1355, 300, 456, 311, 516, 281, 312, 257, 8174, 12866, 456, 13], "temperature": 0.0, "avg_logprob": -0.10583518156364782, "compression_ratio": 1.823943661971831, "no_speech_prob": 2.5068669856409542e-05}, {"id": 444, "seek": 130420, "start": 1308.3, "end": 1310.8, "text": " And we could have other actions as well.", "tokens": [400, 321, 727, 362, 661, 5909, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.10583518156364782, "compression_ratio": 1.823943661971831, "no_speech_prob": 2.5068669856409542e-05}, {"id": 445, "seek": 130420, "start": 1310.8, "end": 1314.9, "text": " There's been work to jointly predict part of speech tags", "tokens": [821, 311, 668, 589, 281, 46557, 6069, 644, 295, 6218, 18632], "temperature": 0.0, "avg_logprob": -0.10583518156364782, "compression_ratio": 1.823943661971831, "no_speech_prob": 2.5068669856409542e-05}, {"id": 446, "seek": 130420, "start": 1314.9, "end": 1316.3, "text": " at the same time as you're parsing.", "tokens": [412, 264, 912, 565, 382, 291, 434, 21156, 278, 13], "temperature": 0.0, "avg_logprob": -0.10583518156364782, "compression_ratio": 1.823943661971831, "no_speech_prob": 2.5068669856409542e-05}, {"id": 447, "seek": 130420, "start": 1316.3, "end": 1319.9, "text": " Or you can do semantics at the same time as you do syntax.", "tokens": [1610, 291, 393, 360, 4361, 45298, 412, 264, 912, 565, 382, 291, 360, 28431, 13], "temperature": 0.0, "avg_logprob": -0.10583518156364782, "compression_ratio": 1.823943661971831, "no_speech_prob": 2.5068669856409542e-05}, {"id": 448, "seek": 130420, "start": 1319.9, "end": 1322.6000000000001, "text": " And so you can code up all sorts of structures into this.", "tokens": [400, 370, 291, 393, 3089, 493, 439, 7527, 295, 9227, 666, 341, 13], "temperature": 0.0, "avg_logprob": -0.10583518156364782, "compression_ratio": 1.823943661971831, "no_speech_prob": 2.5068669856409542e-05}, {"id": 449, "seek": 130420, "start": 1322.6000000000001, "end": 1325.0, "text": " And you're going to read the sentence left to right,", "tokens": [400, 291, 434, 516, 281, 1401, 264, 8174, 1411, 281, 558, 11], "temperature": 0.0, "avg_logprob": -0.10583518156364782, "compression_ratio": 1.823943661971831, "no_speech_prob": 2.5068669856409542e-05}, {"id": 450, "seek": 130420, "start": 1325.0, "end": 1328.3, "text": " and you're going to output some meaning structure attached to it.", "tokens": [293, 291, 434, 516, 281, 5598, 512, 3620, 3877, 8570, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.10583518156364782, "compression_ratio": 1.823943661971831, "no_speech_prob": 2.5068669856409542e-05}, {"id": 451, "seek": 130420, "start": 1328.3, "end": 1333.8, "text": " And as I said, I find this a satisfying way to do natural language understanding,", "tokens": [400, 382, 286, 848, 11, 286, 915, 341, 257, 18348, 636, 281, 360, 3303, 2856, 3701, 11], "temperature": 0.0, "avg_logprob": -0.10583518156364782, "compression_ratio": 1.823943661971831, "no_speech_prob": 2.5068669856409542e-05}, {"id": 452, "seek": 133380, "start": 1333.8, "end": 1336.3999999999999, "text": " because it does involve reading the sentence", "tokens": [570, 309, 775, 9494, 3760, 264, 8174], "temperature": 0.0, "avg_logprob": -0.10606295207761368, "compression_ratio": 1.6259541984732824, "no_speech_prob": 2.9310762329259887e-05}, {"id": 453, "seek": 133380, "start": 1336.3999999999999, "end": 1340.0, "text": " and adding an interpretation incrementally.", "tokens": [293, 5127, 364, 14174, 26200, 379, 13], "temperature": 0.0, "avg_logprob": -0.10606295207761368, "compression_ratio": 1.6259541984732824, "no_speech_prob": 2.9310762329259887e-05}, {"id": 454, "seek": 133380, "start": 1340.0, "end": 1345.3, "text": " OK, so that's what this looks like as we proceed through.", "tokens": [2264, 11, 370, 300, 311, 437, 341, 1542, 411, 382, 321, 8991, 807, 13], "temperature": 0.0, "avg_logprob": -0.10606295207761368, "compression_ratio": 1.6259541984732824, "no_speech_prob": 2.9310762329259887e-05}, {"id": 455, "seek": 133380, "start": 1345.3, "end": 1351.1, "text": " So all right, so how are we going to do this splitting up or merging of other things?", "tokens": [407, 439, 558, 11, 370, 577, 366, 321, 516, 281, 360, 341, 30348, 493, 420, 44559, 295, 661, 721, 30], "temperature": 0.0, "avg_logprob": -0.10606295207761368, "compression_ratio": 1.6259541984732824, "no_speech_prob": 2.9310762329259887e-05}, {"id": 456, "seek": 133380, "start": 1351.1, "end": 1355.0, "text": " Well, it's actually not that complicated given this transition-based framework.", "tokens": [1042, 11, 309, 311, 767, 406, 300, 6179, 2212, 341, 6034, 12, 6032, 8388, 13], "temperature": 0.0, "avg_logprob": -0.10606295207761368, "compression_ratio": 1.6259541984732824, "no_speech_prob": 2.9310762329259887e-05}, {"id": 457, "seek": 133380, "start": 1355.0, "end": 1359.6, "text": " So already you can kind of see that in order to merge tokens,", "tokens": [407, 1217, 291, 393, 733, 295, 536, 300, 294, 1668, 281, 22183, 22667, 11], "temperature": 0.0, "avg_logprob": -0.10606295207761368, "compression_ratio": 1.6259541984732824, "no_speech_prob": 2.9310762329259887e-05}, {"id": 458, "seek": 133380, "start": 1359.6, "end": 1362.3, "text": " all we really have to do is we've got those tokens,", "tokens": [439, 321, 534, 362, 281, 360, 307, 321, 600, 658, 729, 22667, 11], "temperature": 0.0, "avg_logprob": -0.10606295207761368, "compression_ratio": 1.6259541984732824, "no_speech_prob": 2.9310762329259887e-05}, {"id": 459, "seek": 136230, "start": 1362.3, "end": 1365.1, "text": " and if we want to Google Read it to be one token,", "tokens": [293, 498, 321, 528, 281, 3329, 17604, 309, 281, 312, 472, 14862, 11], "temperature": 0.0, "avg_logprob": -0.10937759894451113, "compression_ratio": 1.7607142857142857, "no_speech_prob": 5.919943214394152e-05}, {"id": 460, "seek": 136230, "start": 1365.1, "end": 1367.2, "text": " we just have to have some special dependency label,", "tokens": [321, 445, 362, 281, 362, 512, 2121, 33621, 7645, 11], "temperature": 0.0, "avg_logprob": -0.10937759894451113, "compression_ratio": 1.7607142857142857, "no_speech_prob": 5.919943214394152e-05}, {"id": 461, "seek": 136230, "start": 1367.2, "end": 1369.3999999999999, "text": " which we are going to have in the tree.", "tokens": [597, 321, 366, 516, 281, 362, 294, 264, 4230, 13], "temperature": 0.0, "avg_logprob": -0.10937759894451113, "compression_ratio": 1.7607142857142857, "no_speech_prob": 5.919943214394152e-05}, {"id": 462, "seek": 136230, "start": 1369.3999999999999, "end": 1371.8999999999999, "text": " And so I've used the label sub-token,", "tokens": [400, 370, 286, 600, 1143, 264, 7645, 1422, 12, 83, 8406, 11], "temperature": 0.0, "avg_logprob": -0.10937759894451113, "compression_ratio": 1.7607142857142857, "no_speech_prob": 5.919943214394152e-05}, {"id": 463, "seek": 136230, "start": 1371.8999999999999, "end": 1375.3, "text": " and then all we have to do is say, all right,", "tokens": [293, 550, 439, 321, 362, 281, 360, 307, 584, 11, 439, 558, 11], "temperature": 0.0, "avg_logprob": -0.10937759894451113, "compression_ratio": 1.7607142857142857, "no_speech_prob": 5.919943214394152e-05}, {"id": 464, "seek": 136230, "start": 1375.3, "end": 1378.6, "text": " at the end of parsing, we're going to consider that as one token.", "tokens": [412, 264, 917, 295, 21156, 278, 11, 321, 434, 516, 281, 1949, 300, 382, 472, 14862, 13], "temperature": 0.0, "avg_logprob": -0.10937759894451113, "compression_ratio": 1.7607142857142857, "no_speech_prob": 5.919943214394152e-05}, {"id": 465, "seek": 136230, "start": 1378.6, "end": 1383.1, "text": " So the step from going through something like this", "tokens": [407, 264, 1823, 490, 516, 807, 746, 411, 341], "temperature": 0.0, "avg_logprob": -0.10937759894451113, "compression_ratio": 1.7607142857142857, "no_speech_prob": 5.919943214394152e-05}, {"id": 466, "seek": 136230, "start": 1383.1, "end": 1386.3999999999999, "text": " and labeling a language like Chinese is actually super simple.", "tokens": [293, 40244, 257, 2856, 411, 4649, 307, 767, 1687, 2199, 13], "temperature": 0.0, "avg_logprob": -0.10937759894451113, "compression_ratio": 1.7607142857142857, "no_speech_prob": 5.919943214394152e-05}, {"id": 467, "seek": 136230, "start": 1386.3999999999999, "end": 1391.0, "text": " We just have to prepare the training data so that the tokens are individual characters.", "tokens": [492, 445, 362, 281, 5940, 264, 3097, 1412, 370, 300, 264, 22667, 366, 2609, 4342, 13], "temperature": 0.0, "avg_logprob": -0.10937759894451113, "compression_ratio": 1.7607142857142857, "no_speech_prob": 5.919943214394152e-05}, {"id": 468, "seek": 139100, "start": 1391.0, "end": 1396.6, "text": " And then we can say, all right, things which should be one word", "tokens": [400, 550, 321, 393, 584, 11, 439, 558, 11, 721, 597, 820, 312, 472, 1349], "temperature": 0.0, "avg_logprob": -0.07583906776026676, "compression_ratio": 1.6814814814814816, "no_speech_prob": 4.0059232560452074e-05}, {"id": 469, "seek": 139100, "start": 1396.6, "end": 1399.0, "text": " should have this sort of structure with this label.", "tokens": [820, 362, 341, 1333, 295, 3877, 365, 341, 7645, 13], "temperature": 0.0, "avg_logprob": -0.07583906776026676, "compression_ratio": 1.6814814814814816, "no_speech_prob": 4.0059232560452074e-05}, {"id": 470, "seek": 139100, "start": 1399.0, "end": 1403.2, "text": " And then if the parser decides that those things are attached together,", "tokens": [400, 550, 498, 264, 21156, 260, 14898, 300, 729, 721, 366, 8570, 1214, 11], "temperature": 0.0, "avg_logprob": -0.07583906776026676, "compression_ratio": 1.6814814814814816, "no_speech_prob": 4.0059232560452074e-05}, {"id": 471, "seek": 139100, "start": 1403.2, "end": 1406.6, "text": " then at the end of it, you just merge them up.", "tokens": [550, 412, 264, 917, 295, 309, 11, 291, 445, 22183, 552, 493, 13], "temperature": 0.0, "avg_logprob": -0.07583906776026676, "compression_ratio": 1.6814814814814816, "no_speech_prob": 4.0059232560452074e-05}, {"id": 472, "seek": 139100, "start": 1406.6, "end": 1409.1, "text": " The splitting tokens is more complicated", "tokens": [440, 30348, 22667, 307, 544, 6179], "temperature": 0.0, "avg_logprob": -0.07583906776026676, "compression_ratio": 1.6814814814814816, "no_speech_prob": 4.0059232560452074e-05}, {"id": 473, "seek": 139100, "start": 1409.1, "end": 1413.1, "text": " because you have to have some universe of actions that manipulates the strings.", "tokens": [570, 291, 362, 281, 362, 512, 6445, 295, 5909, 300, 9258, 26192, 264, 13985, 13], "temperature": 0.0, "avg_logprob": -0.07583906776026676, "compression_ratio": 1.6814814814814816, "no_speech_prob": 4.0059232560452074e-05}, {"id": 474, "seek": 139100, "start": 1413.1, "end": 1415.6, "text": " So I'm still sort of working on the implementation of this", "tokens": [407, 286, 478, 920, 1333, 295, 1364, 322, 264, 11420, 295, 341], "temperature": 0.0, "avg_logprob": -0.07583906776026676, "compression_ratio": 1.6814814814814816, "no_speech_prob": 4.0059232560452074e-05}, {"id": 475, "seek": 139100, "start": 1415.6, "end": 1419.7, "text": " in a way that's sort of clean and tidy.", "tokens": [294, 257, 636, 300, 311, 1333, 295, 2541, 293, 34646, 13], "temperature": 0.0, "avg_logprob": -0.07583906776026676, "compression_ratio": 1.6814814814814816, "no_speech_prob": 4.0059232560452074e-05}, {"id": 476, "seek": 141970, "start": 1419.7, "end": 1422.8, "text": " But I actually think that this will be useful for a lot of English texts as well,", "tokens": [583, 286, 767, 519, 300, 341, 486, 312, 4420, 337, 257, 688, 295, 3669, 15765, 382, 731, 11], "temperature": 0.0, "avg_logprob": -0.12013144005002, "compression_ratio": 1.79182156133829, "no_speech_prob": 9.311099711339921e-05}, {"id": 477, "seek": 141970, "start": 1422.8, "end": 1427.4, "text": " because if you have English text that's sort of misspelled,", "tokens": [570, 498, 291, 362, 3669, 2487, 300, 311, 1333, 295, 1713, 33000, 11], "temperature": 0.0, "avg_logprob": -0.12013144005002, "compression_ratio": 1.79182156133829, "no_speech_prob": 9.311099711339921e-05}, {"id": 478, "seek": 141970, "start": 1427.4, "end": 1430.7, "text": " a lot of the time, things which should be two tokens get merged into one.", "tokens": [257, 688, 295, 264, 565, 11, 721, 597, 820, 312, 732, 22667, 483, 36427, 666, 472, 13], "temperature": 0.0, "avg_logprob": -0.12013144005002, "compression_ratio": 1.79182156133829, "no_speech_prob": 9.311099711339921e-05}, {"id": 479, "seek": 141970, "start": 1430.7, "end": 1435.3, "text": " So its is a particularly common and frustrating one of this", "tokens": [407, 1080, 307, 257, 4098, 2689, 293, 16522, 472, 295, 341], "temperature": 0.0, "avg_logprob": -0.12013144005002, "compression_ratio": 1.79182156133829, "no_speech_prob": 9.311099711339921e-05}, {"id": 480, "seek": 141970, "start": 1435.3, "end": 1439.3, "text": " because the verb is, should be its own token.", "tokens": [570, 264, 9595, 307, 11, 820, 312, 1080, 1065, 14862, 13], "temperature": 0.0, "avg_logprob": -0.12013144005002, "compression_ratio": 1.79182156133829, "no_speech_prob": 9.311099711339921e-05}, {"id": 481, "seek": 141970, "start": 1439.3, "end": 1443.6000000000001, "text": " But if you have its as its, which is also a common word in English,", "tokens": [583, 498, 291, 362, 1080, 382, 1080, 11, 597, 307, 611, 257, 2689, 1349, 294, 3669, 11], "temperature": 0.0, "avg_logprob": -0.12013144005002, "compression_ratio": 1.79182156133829, "no_speech_prob": 9.311099711339921e-05}, {"id": 482, "seek": 141970, "start": 1443.6000000000001, "end": 1447.5, "text": " you need to figure out that you have to have two parser actions,", "tokens": [291, 643, 281, 2573, 484, 300, 291, 362, 281, 362, 732, 21156, 260, 5909, 11], "temperature": 0.0, "avg_logprob": -0.12013144005002, "compression_ratio": 1.79182156133829, "no_speech_prob": 9.311099711339921e-05}, {"id": 483, "seek": 141970, "start": 1447.5, "end": 1449.2, "text": " two parser states for that.", "tokens": [732, 21156, 260, 4368, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.12013144005002, "compression_ratio": 1.79182156133829, "no_speech_prob": 9.311099711339921e-05}, {"id": 484, "seek": 144920, "start": 1449.2, "end": 1452.0, "text": " And in general, you could have a statistical model", "tokens": [400, 294, 2674, 11, 291, 727, 362, 257, 22820, 2316], "temperature": 0.0, "avg_logprob": -0.08596003605769231, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0001711235527181998}, {"id": 485, "seek": 144920, "start": 1452.0, "end": 1453.8, "text": " that reads the sentence beforehand,", "tokens": [300, 15700, 264, 8174, 22893, 11], "temperature": 0.0, "avg_logprob": -0.08596003605769231, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0001711235527181998}, {"id": 486, "seek": 144920, "start": 1453.8, "end": 1457.5, "text": " but that statistical model that is going to read the sentence and process it", "tokens": [457, 300, 22820, 2316, 300, 307, 516, 281, 1401, 264, 8174, 293, 1399, 309], "temperature": 0.0, "avg_logprob": -0.08596003605769231, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0001711235527181998}, {"id": 487, "seek": 144920, "start": 1457.5, "end": 1461.0, "text": " is going to end up taking on work and doing jobs of figuring out", "tokens": [307, 516, 281, 917, 493, 1940, 322, 589, 293, 884, 4782, 295, 15213, 484], "temperature": 0.0, "avg_logprob": -0.08596003605769231, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0001711235527181998}, {"id": 488, "seek": 144920, "start": 1461.0, "end": 1464.1000000000001, "text": " the syntactic structure of the sentence in order to make those decisions.", "tokens": [264, 23980, 19892, 3877, 295, 264, 8174, 294, 1668, 281, 652, 729, 5327, 13], "temperature": 0.0, "avg_logprob": -0.08596003605769231, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0001711235527181998}, {"id": 489, "seek": 144920, "start": 1464.1000000000001, "end": 1467.1000000000001, "text": " And that's why I think doing these things jointly is kind of satisfying", "tokens": [400, 300, 311, 983, 286, 519, 884, 613, 721, 46557, 307, 733, 295, 18348], "temperature": 0.0, "avg_logprob": -0.08596003605769231, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0001711235527181998}, {"id": 490, "seek": 144920, "start": 1467.1000000000001, "end": 1471.5, "text": " because instead of learning that information in one level of representation", "tokens": [570, 2602, 295, 2539, 300, 1589, 294, 472, 1496, 295, 10290], "temperature": 0.0, "avg_logprob": -0.08596003605769231, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0001711235527181998}, {"id": 491, "seek": 144920, "start": 1471.5, "end": 1475.0, "text": " and throwing it away only to build up the same information", "tokens": [293, 10238, 309, 1314, 787, 281, 1322, 493, 264, 912, 1589], "temperature": 0.0, "avg_logprob": -0.08596003605769231, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0001711235527181998}, {"id": 492, "seek": 144920, "start": 1475.0, "end": 1478.7, "text": " in the next parser to pipeline, you can do it all at once.", "tokens": [294, 264, 958, 21156, 260, 281, 15517, 11, 291, 393, 360, 309, 439, 412, 1564, 13], "temperature": 0.0, "avg_logprob": -0.08596003605769231, "compression_ratio": 1.8870431893687707, "no_speech_prob": 0.0001711235527181998}, {"id": 493, "seek": 147870, "start": 1478.7, "end": 1481.8, "text": " And so I think that, you know, the joint incremental approaches,", "tokens": [400, 370, 286, 519, 300, 11, 291, 458, 11, 264, 7225, 35759, 11587, 11], "temperature": 0.0, "avg_logprob": -0.11248239536875303, "compression_ratio": 1.5877551020408163, "no_speech_prob": 6.703458348056301e-05}, {"id": 494, "seek": 147870, "start": 1481.8, "end": 1484.2, "text": " I think, are very satisfying and good.", "tokens": [286, 519, 11, 366, 588, 18348, 293, 665, 13], "temperature": 0.0, "avg_logprob": -0.11248239536875303, "compression_ratio": 1.5877551020408163, "no_speech_prob": 6.703458348056301e-05}, {"id": 495, "seek": 147870, "start": 1484.2, "end": 1486.9, "text": " Okay, so where are we at the moment?", "tokens": [1033, 11, 370, 689, 366, 321, 412, 264, 1623, 30], "temperature": 0.0, "avg_logprob": -0.11248239536875303, "compression_ratio": 1.5877551020408163, "no_speech_prob": 6.703458348056301e-05}, {"id": 496, "seek": 147870, "start": 1486.9, "end": 1492.8, "text": " So I've implemented the learning to merge, you know, side of things", "tokens": [407, 286, 600, 12270, 264, 2539, 281, 22183, 11, 291, 458, 11, 1252, 295, 721], "temperature": 0.0, "avg_logprob": -0.11248239536875303, "compression_ratio": 1.5877551020408163, "no_speech_prob": 6.703458348056301e-05}, {"id": 497, "seek": 147870, "start": 1492.8, "end": 1496.8, "text": " which involves figuring out better alignments", "tokens": [597, 11626, 15213, 484, 1101, 7975, 1117], "temperature": 0.0, "avg_logprob": -0.11248239536875303, "compression_ratio": 1.5877551020408163, "no_speech_prob": 6.703458348056301e-05}, {"id": 498, "seek": 147870, "start": 1496.8, "end": 1501.5, "text": " between the gold standard tokenization and the output of the tokenizer.", "tokens": [1296, 264, 3821, 3832, 14862, 2144, 293, 264, 5598, 295, 264, 14862, 6545, 13], "temperature": 0.0, "avg_logprob": -0.11248239536875303, "compression_ratio": 1.5877551020408163, "no_speech_prob": 6.703458348056301e-05}, {"id": 499, "seek": 147870, "start": 1501.5, "end": 1505.3, "text": " And that's allowed me to complete the experiments for Chinese,", "tokens": [400, 300, 311, 4350, 385, 281, 3566, 264, 12050, 337, 4649, 11], "temperature": 0.0, "avg_logprob": -0.11248239536875303, "compression_ratio": 1.5877551020408163, "no_speech_prob": 6.703458348056301e-05}, {"id": 500, "seek": 150530, "start": 1505.3, "end": 1509.1, "text": " Vietnamese, and Japanese of the conference", "tokens": [25934, 11, 293, 5433, 295, 264, 7586], "temperature": 0.0, "avg_logprob": -0.12616473978215997, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.00021627024398185313}, {"id": 501, "seek": 150530, "start": 1509.1, "end": 1511.7, "text": " in natural language learning 2017 benchmark,", "tokens": [294, 3303, 2856, 2539, 6591, 18927, 11], "temperature": 0.0, "avg_logprob": -0.12616473978215997, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.00021627024398185313}, {"id": 502, "seek": 150530, "start": 1511.7, "end": 1514.2, "text": " which was a sort of bake-off of these parsing models", "tokens": [597, 390, 257, 1333, 295, 16562, 12, 4506, 295, 613, 21156, 278, 5245], "temperature": 0.0, "avg_logprob": -0.12616473978215997, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.00021627024398185313}, {"id": 503, "seek": 150530, "start": 1514.2, "end": 1516.6, "text": " which was conducted last year.", "tokens": [597, 390, 13809, 1036, 1064, 13], "temperature": 0.0, "avg_logprob": -0.12616473978215997, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.00021627024398185313}, {"id": 504, "seek": 150530, "start": 1516.6, "end": 1522.5, "text": " Now, in that benchmark, the team from Stanford did extremely well", "tokens": [823, 11, 294, 300, 18927, 11, 264, 1469, 490, 20374, 630, 4664, 731], "temperature": 0.0, "avg_logprob": -0.12616473978215997, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.00021627024398185313}, {"id": 505, "seek": 150530, "start": 1522.5, "end": 1524.5, "text": " compared to everybody else in the field.", "tokens": [5347, 281, 2201, 1646, 294, 264, 2519, 13], "temperature": 0.0, "avg_logprob": -0.12616473978215997, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.00021627024398185313}, {"id": 506, "seek": 150530, "start": 1524.5, "end": 1528.5, "text": " They were, you know, some two or three percentage points better.", "tokens": [814, 645, 11, 291, 458, 11, 512, 732, 420, 1045, 9668, 2793, 1101, 13], "temperature": 0.0, "avg_logprob": -0.12616473978215997, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.00021627024398185313}, {"id": 507, "seek": 150530, "start": 1528.5, "end": 1532.0, "text": " And so at the moment, we're ranking kind of at the top of", "tokens": [400, 370, 412, 264, 1623, 11, 321, 434, 17833, 733, 295, 412, 264, 1192, 295], "temperature": 0.0, "avg_logprob": -0.12616473978215997, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.00021627024398185313}, {"id": 508, "seek": 150530, "start": 1532.0, "end": 1533.3, "text": " what was the second place pack.", "tokens": [437, 390, 264, 1150, 1081, 2844, 13], "temperature": 0.0, "avg_logprob": -0.12616473978215997, "compression_ratio": 1.586080586080586, "no_speech_prob": 0.00021627024398185313}, {"id": 509, "seek": 153330, "start": 1533.3, "end": 1537.1, "text": " So most of the languages were coming sort of underneath the Stanford system,", "tokens": [407, 881, 295, 264, 8650, 645, 1348, 1333, 295, 7223, 264, 20374, 1185, 11], "temperature": 0.0, "avg_logprob": -0.079157230282618, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.0002062729181488976}, {"id": 510, "seek": 153330, "start": 1537.1, "end": 1543.0, "text": " but with significantly better efficiency and with sort of this end-to-end process.", "tokens": [457, 365, 10591, 1101, 10493, 293, 365, 1333, 295, 341, 917, 12, 1353, 12, 521, 1399, 13], "temperature": 0.0, "avg_logprob": -0.079157230282618, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.0002062729181488976}, {"id": 511, "seek": 153330, "start": 1543.0, "end": 1545.8, "text": " And in particular, we're doing better than Stanford", "tokens": [400, 294, 1729, 11, 321, 434, 884, 1101, 813, 20374], "temperature": 0.0, "avg_logprob": -0.079157230282618, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.0002062729181488976}, {"id": 512, "seek": 153330, "start": 1545.8, "end": 1548.1, "text": " on these languages like Chinese, Vietnamese, and Japanese", "tokens": [322, 613, 8650, 411, 4649, 11, 25934, 11, 293, 5433], "temperature": 0.0, "avg_logprob": -0.079157230282618, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.0002062729181488976}, {"id": 513, "seek": 153330, "start": 1548.1, "end": 1550.3999999999999, "text": " because the Stanford system did have this disadvantage", "tokens": [570, 264, 20374, 1185, 630, 362, 341, 24292], "temperature": 0.0, "avg_logprob": -0.079157230282618, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.0002062729181488976}, {"id": 514, "seek": 153330, "start": 1550.3999999999999, "end": 1552.7, "text": " of using the sort of pre-processed text.", "tokens": [295, 1228, 264, 1333, 295, 659, 12, 41075, 292, 2487, 13], "temperature": 0.0, "avg_logprob": -0.079157230282618, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.0002062729181488976}, {"id": 515, "seek": 153330, "start": 1552.7, "end": 1553.8, "text": " They didn't do the whole task.", "tokens": [814, 994, 380, 360, 264, 1379, 5633, 13], "temperature": 0.0, "avg_logprob": -0.079157230282618, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.0002062729181488976}, {"id": 516, "seek": 153330, "start": 1553.8, "end": 1559.5, "text": " They wanted to just use the provided pre-processed text", "tokens": [814, 1415, 281, 445, 764, 264, 5649, 659, 12, 41075, 292, 2487], "temperature": 0.0, "avg_logprob": -0.079157230282618, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.0002062729181488976}, {"id": 517, "seek": 153330, "start": 1559.5, "end": 1561.7, "text": " so that they could focus on the parsing algorithm.", "tokens": [370, 300, 436, 727, 1879, 322, 264, 21156, 278, 9284, 13], "temperature": 0.0, "avg_logprob": -0.079157230282618, "compression_ratio": 1.7773851590106007, "no_speech_prob": 0.0002062729181488976}, {"id": 518, "seek": 156170, "start": 1561.7, "end": 1564.4, "text": " And that meant that they did have this error propagation problem.", "tokens": [400, 300, 4140, 300, 436, 630, 362, 341, 6713, 38377, 1154, 13], "temperature": 0.0, "avg_logprob": -0.1341885740106756, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.0001397321029799059}, {"id": 519, "seek": 156170, "start": 1564.4, "end": 1568.8, "text": " If the inputs are incorrect because the pre-processed segmenter is incorrect,", "tokens": [759, 264, 15743, 366, 18424, 570, 264, 659, 12, 41075, 292, 9469, 260, 307, 18424, 11], "temperature": 0.0, "avg_logprob": -0.1341885740106756, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.0001397321029799059}, {"id": 520, "seek": 156170, "start": 1568.8, "end": 1573.7, "text": " then they have a big disadvantage on these languages.", "tokens": [550, 436, 362, 257, 955, 24292, 322, 613, 8650, 13], "temperature": 0.0, "avg_logprob": -0.1341885740106756, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.0001397321029799059}, {"id": 521, "seek": 156170, "start": 1573.7, "end": 1578.0, "text": " So satisfyingly, we're sort of doing all at once", "tokens": [407, 18348, 356, 11, 321, 434, 1333, 295, 884, 439, 412, 1564], "temperature": 0.0, "avg_logprob": -0.1341885740106756, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.0001397321029799059}, {"id": 522, "seek": 156170, "start": 1578.0, "end": 1581.3, "text": " and entangling all of these representations.", "tokens": [293, 948, 656, 1688, 439, 295, 613, 33358, 13], "temperature": 0.0, "avg_logprob": -0.1341885740106756, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.0001397321029799059}, {"id": 523, "seek": 156170, "start": 1581.3, "end": 1584.1000000000001, "text": " It does have this advantage, and we're seeing that in the results", "tokens": [467, 775, 362, 341, 5002, 11, 293, 321, 434, 2577, 300, 294, 264, 3542], "temperature": 0.0, "avg_logprob": -0.1341885740106756, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.0001397321029799059}, {"id": 524, "seek": 156170, "start": 1584.1000000000001, "end": 1586.4, "text": " that we have for those languages.", "tokens": [300, 321, 362, 337, 729, 8650, 13], "temperature": 0.0, "avg_logprob": -0.1341885740106756, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.0001397321029799059}, {"id": 525, "seek": 156170, "start": 1586.4, "end": 1589.8, "text": " And the other thing that's satisfying is this joint modeling approach", "tokens": [400, 264, 661, 551, 300, 311, 18348, 307, 341, 7225, 15983, 3109], "temperature": 0.0, "avg_logprob": -0.1341885740106756, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.0001397321029799059}, {"id": 526, "seek": 158980, "start": 1589.8, "end": 1592.2, "text": " of deciding the segmentation at the same time", "tokens": [295, 17990, 264, 9469, 399, 412, 264, 912, 565], "temperature": 0.0, "avg_logprob": -0.13899925519835274, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.00022673893545288593}, {"id": 527, "seek": 158980, "start": 1592.2, "end": 1595.3, "text": " as deciding the parse structure is consistently better", "tokens": [382, 17990, 264, 48377, 3877, 307, 14961, 1101], "temperature": 0.0, "avg_logprob": -0.13899925519835274, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.00022673893545288593}, {"id": 528, "seek": 158980, "start": 1595.3, "end": 1597.5, "text": " than the pipeline approach in our experiments.", "tokens": [813, 264, 15517, 3109, 294, 527, 12050, 13], "temperature": 0.0, "avg_logprob": -0.13899925519835274, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.00022673893545288593}, {"id": 529, "seek": 158980, "start": 1597.5, "end": 1603.2, "text": " So basically, we're getting a sort of 1% to 3% improvement from this,", "tokens": [407, 1936, 11, 321, 434, 1242, 257, 1333, 295, 502, 4, 281, 805, 4, 10444, 490, 341, 11], "temperature": 0.0, "avg_logprob": -0.13899925519835274, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.00022673893545288593}, {"id": 530, "seek": 158980, "start": 1603.2, "end": 1605.7, "text": " which is about the same size as we're getting", "tokens": [597, 307, 466, 264, 912, 2744, 382, 321, 434, 1242], "temperature": 0.0, "avg_logprob": -0.13899925519835274, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.00022673893545288593}, {"id": 531, "seek": 158980, "start": 1605.7, "end": 1608.3, "text": " from using the neural network model instead of the linear model.", "tokens": [490, 1228, 264, 18161, 3209, 2316, 2602, 295, 264, 8213, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13899925519835274, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.00022673893545288593}, {"id": 532, "seek": 158980, "start": 1608.3, "end": 1613.5, "text": " So I've found this also quite satisfying that the sort of conceptually neat solution", "tokens": [407, 286, 600, 1352, 341, 611, 1596, 18348, 300, 264, 1333, 295, 3410, 671, 10654, 3827], "temperature": 0.0, "avg_logprob": -0.13899925519835274, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.00022673893545288593}, {"id": 533, "seek": 158980, "start": 1613.5, "end": 1616.2, "text": " is also working well in practice.", "tokens": [307, 611, 1364, 731, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.13899925519835274, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.00022673893545288593}, {"id": 534, "seek": 161620, "start": 1616.2, "end": 1621.8, "text": " Okay, so where does this go and what do we hope to deliver from this?", "tokens": [1033, 11, 370, 689, 775, 341, 352, 293, 437, 360, 321, 1454, 281, 4239, 490, 341, 30], "temperature": 0.0, "avg_logprob": -0.29732567656273934, "compression_ratio": 1.555045871559633, "no_speech_prob": 0.00013975582260172814}, {"id": 535, "seek": 161620, "start": 1621.8, "end": 1625.8, "text": " Yes, that would probably be it.", "tokens": [1079, 11, 300, 576, 1391, 312, 309, 13], "temperature": 0.0, "avg_logprob": -0.29732567656273934, "compression_ratio": 1.555045871559633, "no_speech_prob": 0.00013975582260172814}, {"id": 536, "seek": 161620, "start": 1625.8, "end": 1628.8, "text": " How am I for time?", "tokens": [1012, 669, 286, 337, 565, 30], "temperature": 0.0, "avg_logprob": -0.29732567656273934, "compression_ratio": 1.555045871559633, "no_speech_prob": 0.00013975582260172814}, {"id": 537, "seek": 161620, "start": 1628.8, "end": 1631.3, "text": " Now, I don't know.", "tokens": [823, 11, 286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.29732567656273934, "compression_ratio": 1.555045871559633, "no_speech_prob": 0.00013975582260172814}, {"id": 538, "seek": 161620, "start": 1631.3, "end": 1634.3, "text": " Okay, well, this is the last slide, so wrapping up.", "tokens": [1033, 11, 731, 11, 341, 307, 264, 1036, 4137, 11, 370, 21993, 493, 13], "temperature": 0.0, "avg_logprob": -0.29732567656273934, "compression_ratio": 1.555045871559633, "no_speech_prob": 0.00013975582260172814}, {"id": 539, "seek": 161620, "start": 1634.3, "end": 1640.7, "text": " Okay, so what we want to do is we want to deliver a sort of workflow", "tokens": [1033, 11, 370, 437, 321, 528, 281, 360, 307, 321, 528, 281, 4239, 257, 1333, 295, 20993], "temperature": 0.0, "avg_logprob": -0.29732567656273934, "compression_ratio": 1.555045871559633, "no_speech_prob": 0.00013975582260172814}, {"id": 540, "seek": 161620, "start": 1640.7, "end": 1644.3, "text": " or user experience where it's very easy to start with these pre-trained models", "tokens": [420, 4195, 1752, 689, 309, 311, 588, 1858, 281, 722, 365, 613, 659, 12, 17227, 2001, 5245], "temperature": 0.0, "avg_logprob": -0.29732567656273934, "compression_ratio": 1.555045871559633, "no_speech_prob": 0.00013975582260172814}, {"id": 541, "seek": 164430, "start": 1644.3, "end": 1648.3999999999999, "text": " for the different languages and broad application areas.", "tokens": [337, 264, 819, 8650, 293, 4152, 3861, 3179, 13], "temperature": 0.0, "avg_logprob": -0.13541817446367457, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00010881794150918722}, {"id": 542, "seek": 164430, "start": 1648.3999999999999, "end": 1651.5, "text": " And we want to make sure that they have the same representation across languages.", "tokens": [400, 321, 528, 281, 652, 988, 300, 436, 362, 264, 912, 10290, 2108, 8650, 13], "temperature": 0.0, "avg_logprob": -0.13541817446367457, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00010881794150918722}, {"id": 543, "seek": 164430, "start": 1651.5, "end": 1658.3999999999999, "text": " So you get the same parse scheme, which the folks have been working hard on", "tokens": [407, 291, 483, 264, 912, 48377, 12232, 11, 597, 264, 4024, 362, 668, 1364, 1152, 322], "temperature": 0.0, "avg_logprob": -0.13541817446367457, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00010881794150918722}, {"id": 544, "seek": 164430, "start": 1658.3999999999999, "end": 1662.3999999999999, "text": " and basically now have a pretty satisfying solution from the universal dependencies.", "tokens": [293, 1936, 586, 362, 257, 1238, 18348, 3827, 490, 264, 11455, 36606, 13], "temperature": 0.0, "avg_logprob": -0.13541817446367457, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00010881794150918722}, {"id": 545, "seek": 164430, "start": 1662.3999999999999, "end": 1664.6, "text": " And so if you're processing text from different languages,", "tokens": [400, 370, 498, 291, 434, 9007, 2487, 490, 819, 8650, 11], "temperature": 0.0, "avg_logprob": -0.13541817446367457, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00010881794150918722}, {"id": 546, "seek": 164430, "start": 1664.6, "end": 1667.1, "text": " it should be easy to find, say, subject-verb relationships", "tokens": [309, 820, 312, 1858, 281, 915, 11, 584, 11, 3983, 12, 25809, 6159], "temperature": 0.0, "avg_logprob": -0.13541817446367457, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00010881794150918722}, {"id": 547, "seek": 164430, "start": 1667.1, "end": 1668.8, "text": " or direct-object relationships.", "tokens": [420, 2047, 12, 41070, 6159, 13], "temperature": 0.0, "avg_logprob": -0.13541817446367457, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00010881794150918722}, {"id": 548, "seek": 164430, "start": 1668.8, "end": 1671.3, "text": " And that should work across basically any language", "tokens": [400, 300, 820, 589, 2108, 1936, 604, 2856], "temperature": 0.0, "avg_logprob": -0.13541817446367457, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00010881794150918722}, {"id": 549, "seek": 167130, "start": 1671.3, "end": 1675.5, "text": " so that you can use these parse trees and basically have a level of abstraction", "tokens": [370, 300, 291, 393, 764, 613, 48377, 5852, 293, 1936, 362, 257, 1496, 295, 37765], "temperature": 0.0, "avg_logprob": -0.11650051853873512, "compression_ratio": 1.6695652173913043, "no_speech_prob": 5.919507020735182e-05}, {"id": 550, "seek": 167130, "start": 1675.5, "end": 1679.5, "text": " from which language the text is in.", "tokens": [490, 597, 2856, 264, 2487, 307, 294, 13], "temperature": 0.0, "avg_logprob": -0.11650051853873512, "compression_ratio": 1.6695652173913043, "no_speech_prob": 5.919507020735182e-05}, {"id": 551, "seek": 167130, "start": 1679.5, "end": 1683.8, "text": " And then given this, you should be able to do pretty powerful rule-based matching", "tokens": [400, 550, 2212, 341, 11, 291, 820, 312, 1075, 281, 360, 1238, 4005, 4978, 12, 6032, 14324], "temperature": 0.0, "avg_logprob": -0.11650051853873512, "compression_ratio": 1.6695652173913043, "no_speech_prob": 5.919507020735182e-05}, {"id": 552, "seek": 167130, "start": 1683.8, "end": 1687.3999999999999, "text": " from the parse tree and other annotations that are provided.", "tokens": [490, 264, 48377, 4230, 293, 661, 25339, 763, 300, 366, 5649, 13], "temperature": 0.0, "avg_logprob": -0.11650051853873512, "compression_ratio": 1.6695652173913043, "no_speech_prob": 5.919507020735182e-05}, {"id": 553, "seek": 167130, "start": 1687.3999999999999, "end": 1691.2, "text": " So it should be pretty easy to find information", "tokens": [407, 309, 820, 312, 1238, 1858, 281, 915, 1589], "temperature": 0.0, "avg_logprob": -0.11650051853873512, "compression_ratio": 1.6695652173913043, "no_speech_prob": 5.919507020735182e-05}, {"id": 554, "seek": 167130, "start": 1691.2, "end": 1695.5, "text": " even without knowing much about the language and reuse rules across language.", "tokens": [754, 1553, 5276, 709, 466, 264, 2856, 293, 26225, 4474, 2108, 2856, 13], "temperature": 0.0, "avg_logprob": -0.11650051853873512, "compression_ratio": 1.6695652173913043, "no_speech_prob": 5.919507020735182e-05}, {"id": 555, "seek": 169550, "start": 1695.5, "end": 1703.7, "text": " And then if the syntactic model and the identity models that we provide aren't accurate enough,", "tokens": [400, 550, 498, 264, 23980, 19892, 2316, 293, 264, 6575, 5245, 300, 321, 2893, 3212, 380, 8559, 1547, 11], "temperature": 0.0, "avg_logprob": -0.12685760231905205, "compression_ratio": 1.6514522821576763, "no_speech_prob": 4.399104000185616e-05}, {"id": 556, "seek": 169550, "start": 1703.7, "end": 1706.3, "text": " the library should support easy updating of those,", "tokens": [264, 6405, 820, 1406, 1858, 25113, 295, 729, 11], "temperature": 0.0, "avg_logprob": -0.12685760231905205, "compression_ratio": 1.6514522821576763, "no_speech_prob": 4.399104000185616e-05}, {"id": 557, "seek": 169550, "start": 1706.3, "end": 1712.0, "text": " including learning new vocabulary items without you taking particular effort from this.", "tokens": [3009, 2539, 777, 19864, 4754, 1553, 291, 1940, 1729, 4630, 490, 341, 13], "temperature": 0.0, "avg_logprob": -0.12685760231905205, "compression_ratio": 1.6514522821576763, "no_speech_prob": 4.399104000185616e-05}, {"id": 558, "seek": 169550, "start": 1712.0, "end": 1717.6, "text": " And overall, we sort of want to emphasize a workflow of rapid iteration and data annotation.", "tokens": [400, 4787, 11, 321, 1333, 295, 528, 281, 16078, 257, 20993, 295, 7558, 24784, 293, 1412, 48654, 13], "temperature": 0.0, "avg_logprob": -0.12685760231905205, "compression_ratio": 1.6514522821576763, "no_speech_prob": 4.399104000185616e-05}, {"id": 559, "seek": 169550, "start": 1717.6, "end": 1721.5, "text": " And so the concept of this is that we should be able to provide things", "tokens": [400, 370, 264, 3410, 295, 341, 307, 300, 321, 820, 312, 1075, 281, 2893, 721], "temperature": 0.0, "avg_logprob": -0.12685760231905205, "compression_ratio": 1.6514522821576763, "no_speech_prob": 4.399104000185616e-05}, {"id": 560, "seek": 172150, "start": 1721.5, "end": 1725.8, "text": " which give a sort of broad-based understanding of language,", "tokens": [597, 976, 257, 1333, 295, 4152, 12, 6032, 3701, 295, 2856, 11], "temperature": 0.0, "avg_logprob": -0.1008245489570532, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.00010385633504483849}, {"id": 561, "seek": 172150, "start": 1725.8, "end": 1731.4, "text": " but that still ends up with a need for the knowledge specific to your domain", "tokens": [457, 300, 920, 5314, 493, 365, 257, 643, 337, 264, 3601, 2685, 281, 428, 9274], "temperature": 0.0, "avg_logprob": -0.1008245489570532, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.00010385633504483849}, {"id": 562, "seek": 172150, "start": 1731.4, "end": 1735.2, "text": " and the training data and evaluation data specific to your problems.", "tokens": [293, 264, 3097, 1412, 293, 13344, 1412, 2685, 281, 428, 2740, 13], "temperature": 0.0, "avg_logprob": -0.1008245489570532, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.00010385633504483849}, {"id": 563, "seek": 172150, "start": 1735.2, "end": 1740.4, "text": " And we want to make sure that it's easy to connect the two up and go the extra,", "tokens": [400, 321, 528, 281, 652, 988, 300, 309, 311, 1858, 281, 1745, 264, 732, 493, 293, 352, 264, 2857, 11], "temperature": 0.0, "avg_logprob": -0.1008245489570532, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.00010385633504483849}, {"id": 564, "seek": 172150, "start": 1740.4, "end": 1742.1, "text": " start from a basic understanding of language", "tokens": [722, 490, 257, 3875, 3701, 295, 2856], "temperature": 0.0, "avg_logprob": -0.1008245489570532, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.00010385633504483849}, {"id": 565, "seek": 172150, "start": 1742.1, "end": 1745.5, "text": " and move forward to building the specific applications,", "tokens": [293, 1286, 2128, 281, 2390, 264, 2685, 5821, 11], "temperature": 0.0, "avg_logprob": -0.1008245489570532, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.00010385633504483849}, {"id": 566, "seek": 174550, "start": 1745.5, "end": 1752.5, "text": " which is now Ines will be talking about that aspect of the sort of intended package.", "tokens": [597, 307, 586, 682, 279, 486, 312, 1417, 466, 300, 4171, 295, 264, 1333, 295, 10226, 7372, 13], "temperature": 0.0, "avg_logprob": -0.5522964832394622, "compression_ratio": 1.1826923076923077, "no_speech_prob": 4.462612923816778e-05}, {"id": 567, "seek": 174550, "start": 1752.5, "end": 1756.5, "text": " Oh, yeah.", "tokens": [876, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.5522964832394622, "compression_ratio": 1.1826923076923077, "no_speech_prob": 4.462612923816778e-05}, {"id": 568, "seek": 174550, "start": 1756.5, "end": 1759.5, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.5522964832394622, "compression_ratio": 1.1826923076923077, "no_speech_prob": 4.462612923816778e-05}, {"id": 569, "seek": 174550, "start": 1759.5, "end": 1768.5, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.5522964832394622, "compression_ratio": 1.1826923076923077, "no_speech_prob": 4.462612923816778e-05}, {"id": 570, "seek": 174550, "start": 1768.5, "end": 1771.5, "text": " Right. So the...", "tokens": [1779, 13, 407, 264, 485], "temperature": 0.0, "avg_logprob": -0.5522964832394622, "compression_ratio": 1.1826923076923077, "no_speech_prob": 4.462612923816778e-05}, {"id": 571, "seek": 177150, "start": 1771.5, "end": 1776.5, "text": " Yes, certainly. So delete parsed what the sort of overall difference", "tokens": [1079, 11, 3297, 13, 407, 12097, 21156, 292, 437, 264, 1333, 295, 4787, 2649], "temperature": 0.0, "avg_logprob": -0.22414097840758576, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00015835740487091243}, {"id": 572, "seek": 177150, "start": 1776.5, "end": 1781.5, "text": " or main most important difference between Spacey's parsing algorithm and Stanford's parsing algorithm.", "tokens": [420, 2135, 881, 1021, 2649, 1296, 8705, 88, 311, 21156, 278, 9284, 293, 20374, 311, 21156, 278, 9284, 13], "temperature": 0.0, "avg_logprob": -0.22414097840758576, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00015835740487091243}, {"id": 573, "seek": 177150, "start": 1781.5, "end": 1790.0, "text": " So amongst other things, the sort of most fundamental differences that Stanford's system is a graph-based parser.", "tokens": [407, 12918, 661, 721, 11, 264, 1333, 295, 881, 8088, 7300, 300, 20374, 311, 1185, 307, 257, 4295, 12, 6032, 21156, 260, 13], "temperature": 0.0, "avg_logprob": -0.22414097840758576, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00015835740487091243}, {"id": 574, "seek": 177150, "start": 1790.0, "end": 1796.5, "text": " So this is O N, like O N squared or maybe O N cubed in length of the sentence.", "tokens": [407, 341, 307, 422, 426, 11, 411, 422, 426, 8889, 420, 1310, 422, 426, 36510, 294, 4641, 295, 264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.22414097840758576, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00015835740487091243}, {"id": 575, "seek": 179650, "start": 1796.5, "end": 1802.5, "text": " So you're unable to use this type of parsing algorithm for joint segmentation and parsing.", "tokens": [407, 291, 434, 11299, 281, 764, 341, 2010, 295, 21156, 278, 9284, 337, 7225, 9469, 399, 293, 21156, 278, 13], "temperature": 0.0, "avg_logprob": -0.1285263603808833, "compression_ratio": 1.6954732510288066, "no_speech_prob": 5.648370643029921e-05}, {"id": 576, "seek": 179650, "start": 1802.5, "end": 1809.5, "text": " You have to have a pre-segmented text, which is why it has this disadvantage on languages,", "tokens": [509, 362, 281, 362, 257, 659, 12, 405, 10433, 292, 2487, 11, 597, 307, 983, 309, 575, 341, 24292, 322, 8650, 11], "temperature": 0.0, "avg_logprob": -0.1285263603808833, "compression_ratio": 1.6954732510288066, "no_speech_prob": 5.648370643029921e-05}, {"id": 577, "seek": 179650, "start": 1809.5, "end": 1813.5, "text": " which are more difficult or text, which is more difficult to segment into sentences.", "tokens": [597, 366, 544, 2252, 420, 2487, 11, 597, 307, 544, 2252, 281, 9469, 666, 16579, 13], "temperature": 0.0, "avg_logprob": -0.1285263603808833, "compression_ratio": 1.6954732510288066, "no_speech_prob": 5.648370643029921e-05}, {"id": 578, "seek": 179650, "start": 1813.5, "end": 1819.5, "text": " So in Spacey, we want to make sure that we basically only use linear time algorithms.", "tokens": [407, 294, 8705, 88, 11, 321, 528, 281, 652, 988, 300, 321, 1936, 787, 764, 8213, 565, 14642, 13], "temperature": 0.0, "avg_logprob": -0.1285263603808833, "compression_ratio": 1.6954732510288066, "no_speech_prob": 5.648370643029921e-05}, {"id": 579, "seek": 179650, "start": 1819.5, "end": 1823.5, "text": " And that's why we only take this transition-based approach.", "tokens": [400, 300, 311, 983, 321, 787, 747, 341, 6034, 12, 6032, 3109, 13], "temperature": 0.0, "avg_logprob": -0.1285263603808833, "compression_ratio": 1.6954732510288066, "no_speech_prob": 5.648370643029921e-05}, {"id": 580, "seek": 182350, "start": 1823.5, "end": 1829.5, "text": " Other reasons sort of why they get such a good result.", "tokens": [5358, 4112, 1333, 295, 983, 436, 483, 1270, 257, 665, 1874, 13], "temperature": 0.0, "avg_logprob": -0.12955234050750733, "compression_ratio": 1.5462962962962963, "no_speech_prob": 4.39969589933753e-05}, {"id": 581, "seek": 182350, "start": 1829.5, "end": 1834.5, "text": " Other people have done graph-based models and they're not nearly as accurate.", "tokens": [5358, 561, 362, 1096, 4295, 12, 6032, 5245, 293, 436, 434, 406, 6217, 382, 8559, 13], "temperature": 0.0, "avg_logprob": -0.12955234050750733, "compression_ratio": 1.5462962962962963, "no_speech_prob": 4.39969589933753e-05}, {"id": 582, "seek": 182350, "start": 1834.5, "end": 1842.5, "text": " So I hope to meet the Stanford team in the next couple of days and shake out the details of why the system is so accurate,", "tokens": [407, 286, 1454, 281, 1677, 264, 20374, 1469, 294, 264, 958, 1916, 295, 1708, 293, 10283, 484, 264, 4365, 295, 983, 264, 1185, 307, 370, 8559, 11], "temperature": 0.0, "avg_logprob": -0.12955234050750733, "compression_ratio": 1.5462962962962963, "no_speech_prob": 4.39969589933753e-05}, {"id": 583, "seek": 182350, "start": 1842.5, "end": 1846.5, "text": " because actually it is quite surprising. I've read their papers several times.", "tokens": [570, 767, 309, 307, 1596, 8830, 13, 286, 600, 1401, 641, 10577, 2940, 1413, 13], "temperature": 0.0, "avg_logprob": -0.12955234050750733, "compression_ratio": 1.5462962962962963, "no_speech_prob": 4.39969589933753e-05}, {"id": 584, "seek": 184650, "start": 1846.5, "end": 1854.5, "text": " I can't get the sort of one key insight that means that their system performs so well. It's interesting.", "tokens": [286, 393, 380, 483, 264, 1333, 295, 472, 2141, 11269, 300, 1355, 300, 641, 1185, 26213, 370, 731, 13, 467, 311, 1880, 13], "temperature": 0.0, "avg_logprob": -0.2550454729058769, "compression_ratio": 1.5818181818181818, "no_speech_prob": 8.879819506546482e-05}, {"id": 585, "seek": 184650, "start": 1854.5, "end": 1862.5, "text": " Something that could replace the near-percent taxing parsing or go through that or how do you think about that?", "tokens": [6595, 300, 727, 7406, 264, 2651, 12, 610, 2207, 3366, 278, 21156, 278, 420, 352, 807, 300, 420, 577, 360, 291, 519, 466, 300, 30], "temperature": 0.0, "avg_logprob": -0.2550454729058769, "compression_ratio": 1.5818181818181818, "no_speech_prob": 8.879819506546482e-05}, {"id": 586, "seek": 184650, "start": 1862.5, "end": 1867.5, "text": " So I think that... Right, yes, certainly, yes.", "tokens": [407, 286, 519, 300, 485, 1779, 11, 2086, 11, 3297, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.2550454729058769, "compression_ratio": 1.5818181818181818, "no_speech_prob": 8.879819506546482e-05}, {"id": 587, "seek": 184650, "start": 1867.5, "end": 1873.5, "text": " So the question, which is a very good one that many people have been thinking about,", "tokens": [407, 264, 1168, 11, 597, 307, 257, 588, 665, 472, 300, 867, 561, 362, 668, 1953, 466, 11], "temperature": 0.0, "avg_logprob": -0.2550454729058769, "compression_ratio": 1.5818181818181818, "no_speech_prob": 8.879819506546482e-05}, {"id": 588, "seek": 187350, "start": 1873.5, "end": 1878.5, "text": " to what extent can end-to-end systems, which maybe learn things about syntax,", "tokens": [281, 437, 8396, 393, 917, 12, 1353, 12, 521, 3652, 11, 597, 1310, 1466, 721, 466, 28431, 11], "temperature": 0.0, "avg_logprob": -0.08118618760153512, "compression_ratio": 1.6962962962962962, "no_speech_prob": 5.919399336562492e-05}, {"id": 589, "seek": 187350, "start": 1878.5, "end": 1883.5, "text": " but learn them latently and don't have an explicit syntactic representation internally,", "tokens": [457, 1466, 552, 4465, 2276, 293, 500, 380, 362, 364, 13691, 23980, 19892, 10290, 19501, 11], "temperature": 0.0, "avg_logprob": -0.08118618760153512, "compression_ratio": 1.6962962962962962, "no_speech_prob": 5.919399336562492e-05}, {"id": 590, "seek": 187350, "start": 1883.5, "end": 1887.5, "text": " replace the need for this type of syntactic processing.", "tokens": [7406, 264, 643, 337, 341, 2010, 295, 23980, 19892, 9007, 13], "temperature": 0.0, "avg_logprob": -0.08118618760153512, "compression_ratio": 1.6962962962962962, "no_speech_prob": 5.919399336562492e-05}, {"id": 591, "seek": 187350, "start": 1887.5, "end": 1892.5, "text": " So I would say that for any application where there's sufficient text,", "tokens": [407, 286, 576, 584, 300, 337, 604, 3861, 689, 456, 311, 11563, 2487, 11], "temperature": 0.0, "avg_logprob": -0.08118618760153512, "compression_ratio": 1.6962962962962962, "no_speech_prob": 5.919399336562492e-05}, {"id": 592, "seek": 187350, "start": 1892.5, "end": 1897.5, "text": " currently the best approach or state-of-the-art approach doesn't use a parser.", "tokens": [4362, 264, 1151, 3109, 420, 1785, 12, 2670, 12, 3322, 12, 446, 3109, 1177, 380, 764, 257, 21156, 260, 13], "temperature": 0.0, "avg_logprob": -0.08118618760153512, "compression_ratio": 1.6962962962962962, "no_speech_prob": 5.919399336562492e-05}, {"id": 593, "seek": 187350, "start": 1897.5, "end": 1901.5, "text": " And actually this includes translation and other things where you would kind of expect", "tokens": [400, 767, 341, 5974, 12853, 293, 661, 721, 689, 291, 576, 733, 295, 2066], "temperature": 0.0, "avg_logprob": -0.08118618760153512, "compression_ratio": 1.6962962962962962, "no_speech_prob": 5.919399336562492e-05}, {"id": 594, "seek": 190150, "start": 1901.5, "end": 1904.5, "text": " that having an explicit syntactic layer would help.", "tokens": [300, 1419, 364, 13691, 23980, 19892, 4583, 576, 854, 13], "temperature": 0.0, "avg_logprob": -0.05010855493466716, "compression_ratio": 1.6609589041095891, "no_speech_prob": 6.920877785887569e-05}, {"id": 595, "seek": 190150, "start": 1904.5, "end": 1908.5, "text": " If there's enough text, it seems that going straight to the end-to-end representation tends to be better.", "tokens": [759, 456, 311, 1547, 2487, 11, 309, 2544, 300, 516, 2997, 281, 264, 917, 12, 1353, 12, 521, 10290, 12258, 281, 312, 1101, 13], "temperature": 0.0, "avg_logprob": -0.05010855493466716, "compression_ratio": 1.6609589041095891, "no_speech_prob": 6.920877785887569e-05}, {"id": 596, "seek": 190150, "start": 1908.5, "end": 1911.5, "text": " However, that does involve having a lot of text.", "tokens": [2908, 11, 300, 775, 9494, 1419, 257, 688, 295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.05010855493466716, "compression_ratio": 1.6609589041095891, "no_speech_prob": 6.920877785887569e-05}, {"id": 597, "seek": 190150, "start": 1911.5, "end": 1915.5, "text": " And for most applications, creating that much training data,", "tokens": [400, 337, 881, 5821, 11, 4084, 300, 709, 3097, 1412, 11], "temperature": 0.0, "avg_logprob": -0.05010855493466716, "compression_ratio": 1.6609589041095891, "no_speech_prob": 6.920877785887569e-05}, {"id": 598, "seek": 190150, "start": 1915.5, "end": 1920.5, "text": " especially initially when you're prototyping, tends not to be such a viable solution.", "tokens": [2318, 9105, 562, 291, 434, 46219, 3381, 11, 12258, 406, 281, 312, 1270, 257, 22024, 3827, 13], "temperature": 0.0, "avg_logprob": -0.05010855493466716, "compression_ratio": 1.6609589041095891, "no_speech_prob": 6.920877785887569e-05}, {"id": 599, "seek": 190150, "start": 1920.5, "end": 1925.5, "text": " So the way that I see it is that the parsing stuff is a great scaffolding,", "tokens": [407, 264, 636, 300, 286, 536, 309, 307, 300, 264, 21156, 278, 1507, 307, 257, 869, 44094, 278, 11], "temperature": 0.0, "avg_logprob": -0.05010855493466716, "compression_ratio": 1.6609589041095891, "no_speech_prob": 6.920877785887569e-05}, {"id": 600, "seek": 190150, "start": 1925.5, "end": 1928.5, "text": " and it's a very practical thing to have in your toolbox,", "tokens": [293, 309, 311, 257, 588, 8496, 551, 281, 362, 294, 428, 44593, 11], "temperature": 0.0, "avg_logprob": -0.05010855493466716, "compression_ratio": 1.6609589041095891, "no_speech_prob": 6.920877785887569e-05}, {"id": 601, "seek": 192850, "start": 1928.5, "end": 1931.5, "text": " especially when you're trying to figure out how to model the problem.", "tokens": [2318, 562, 291, 434, 1382, 281, 2573, 484, 577, 281, 2316, 264, 1154, 13], "temperature": 0.0, "avg_logprob": -0.06154802629164049, "compression_ratio": 1.7439759036144578, "no_speech_prob": 9.026689076563343e-05}, {"id": 602, "seek": 192850, "start": 1931.5, "end": 1934.5, "text": " Because otherwise you end up in this chicken-and-egg situation of,", "tokens": [1436, 5911, 291, 917, 493, 294, 341, 4662, 12, 474, 12, 1146, 70, 2590, 295, 11], "temperature": 0.0, "avg_logprob": -0.06154802629164049, "compression_ratio": 1.7439759036144578, "no_speech_prob": 9.026689076563343e-05}, {"id": 603, "seek": 192850, "start": 1934.5, "end": 1937.5, "text": " well, we need lots of data to make our model work well,", "tokens": [731, 11, 321, 643, 3195, 295, 1412, 281, 652, 527, 2316, 589, 731, 11], "temperature": 0.0, "avg_logprob": -0.06154802629164049, "compression_ratio": 1.7439759036144578, "no_speech_prob": 9.026689076563343e-05}, {"id": 604, "seek": 192850, "start": 1937.5, "end": 1940.5, "text": " and otherwise it just doesn't really get off the ground,", "tokens": [293, 5911, 309, 445, 1177, 380, 534, 483, 766, 264, 2727, 11], "temperature": 0.0, "avg_logprob": -0.06154802629164049, "compression_ratio": 1.7439759036144578, "no_speech_prob": 9.026689076563343e-05}, {"id": 605, "seek": 192850, "start": 1940.5, "end": 1944.5, "text": " but then how do we even know that we're collecting the right data for the right model", "tokens": [457, 550, 577, 360, 321, 754, 458, 300, 321, 434, 12510, 264, 558, 1412, 337, 264, 558, 2316], "temperature": 0.0, "avg_logprob": -0.06154802629164049, "compression_ratio": 1.7439759036144578, "no_speech_prob": 9.026689076563343e-05}, {"id": 606, "seek": 192850, "start": 1944.5, "end": 1947.5, "text": " until we have that data collected and we can see the accuracy?", "tokens": [1826, 321, 362, 300, 1412, 11087, 293, 321, 393, 536, 264, 14170, 30], "temperature": 0.0, "avg_logprob": -0.06154802629164049, "compression_ratio": 1.7439759036144578, "no_speech_prob": 9.026689076563343e-05}, {"id": 607, "seek": 192850, "start": 1947.5, "end": 1951.5, "text": " So if you can take sort of smaller steps using these sorts of rule-based scaffolding", "tokens": [407, 498, 291, 393, 747, 1333, 295, 4356, 4439, 1228, 613, 7527, 295, 4978, 12, 6032, 44094, 278], "temperature": 0.0, "avg_logprob": -0.06154802629164049, "compression_ratio": 1.7439759036144578, "no_speech_prob": 9.026689076563343e-05}, {"id": 608, "seek": 192850, "start": 1951.5, "end": 1956.5, "text": " and bootstrapping approaches, I think you have a much more powerful and practical set of tools.", "tokens": [293, 11450, 19639, 3759, 11587, 11, 286, 519, 291, 362, 257, 709, 544, 4005, 293, 8496, 992, 295, 3873, 13], "temperature": 0.0, "avg_logprob": -0.06154802629164049, "compression_ratio": 1.7439759036144578, "no_speech_prob": 9.026689076563343e-05}, {"id": 609, "seek": 195650, "start": 1956.5, "end": 1961.5, "text": " And then finally, once you have a system that you know you want to eke out every percent,", "tokens": [400, 550, 2721, 11, 1564, 291, 362, 257, 1185, 300, 291, 458, 291, 528, 281, 308, 330, 484, 633, 3043, 11], "temperature": 0.0, "avg_logprob": -0.11840952836073838, "compression_ratio": 1.5098039215686274, "no_speech_prob": 2.1444235244416632e-05}, {"id": 610, "seek": 195650, "start": 1961.5, "end": 1966.5, "text": " maybe you end up collecting enough data that you don't need a parser in your solution explicitly.", "tokens": [1310, 291, 917, 493, 12510, 1547, 1412, 300, 291, 500, 380, 643, 257, 21156, 260, 294, 428, 3827, 20803, 13], "temperature": 0.0, "avg_logprob": -0.11840952836073838, "compression_ratio": 1.5098039215686274, "no_speech_prob": 2.1444235244416632e-05}, {"id": 611, "seek": 195650, "start": 1974.5, "end": 1979.5, "text": " So Dalip has pointed to a paper that recently showed that", "tokens": [407, 17357, 647, 575, 10932, 281, 257, 3035, 300, 3938, 4712, 300], "temperature": 0.0, "avg_logprob": -0.11840952836073838, "compression_ratio": 1.5098039215686274, "no_speech_prob": 2.1444235244416632e-05}, {"id": 612, "seek": 195650, "start": 1979.5, "end": 1983.5, "text": " BiLSTM models don't necessarily learn long-range dependencies.", "tokens": [13007, 43, 6840, 44, 5245, 500, 380, 4725, 1466, 938, 12, 14521, 36606, 13], "temperature": 0.0, "avg_logprob": -0.11840952836073838, "compression_ratio": 1.5098039215686274, "no_speech_prob": 2.1444235244416632e-05}, {"id": 613, "seek": 198350, "start": 1983.5, "end": 1991.5, "text": " I think that that's probably true, but as somebody who's worked on parsing for a lot of my career,", "tokens": [286, 519, 300, 300, 311, 1391, 2074, 11, 457, 382, 2618, 567, 311, 2732, 322, 21156, 278, 337, 257, 688, 295, 452, 3988, 11], "temperature": 0.0, "avg_logprob": -0.06493303091219156, "compression_ratio": 1.555084745762712, "no_speech_prob": 0.00010888075485127047}, {"id": 614, "seek": 198350, "start": 1991.5, "end": 1994.5, "text": " I try to remind myself not to cherry-pick results.", "tokens": [286, 853, 281, 4160, 2059, 406, 281, 20164, 12, 79, 618, 3542, 13], "temperature": 0.0, "avg_logprob": -0.06493303091219156, "compression_ratio": 1.555084745762712, "no_speech_prob": 0.00010888075485127047}, {"id": 615, "seek": 198350, "start": 1994.5, "end": 1998.5, "text": " And even if I do find a paper that shows that parsing works on something,", "tokens": [400, 754, 498, 286, 360, 915, 257, 3035, 300, 3110, 300, 21156, 278, 1985, 322, 746, 11], "temperature": 0.0, "avg_logprob": -0.06493303091219156, "compression_ratio": 1.555084745762712, "no_speech_prob": 0.00010888075485127047}, {"id": 616, "seek": 198350, "start": 1998.5, "end": 2004.5, "text": " well, the overall trend is that BiLSTM models which don't use parsing work well.", "tokens": [731, 11, 264, 4787, 6028, 307, 300, 13007, 43, 6840, 44, 5245, 597, 500, 380, 764, 21156, 278, 589, 731, 13], "temperature": 0.0, "avg_logprob": -0.06493303091219156, "compression_ratio": 1.555084745762712, "no_speech_prob": 0.00010888075485127047}, {"id": 617, "seek": 198350, "start": 2004.5, "end": 2007.5, "text": " And the fact is that long-range dependencies are kind of rare.", "tokens": [400, 264, 1186, 307, 300, 938, 12, 14521, 36606, 366, 733, 295, 5892, 13], "temperature": 0.0, "avg_logprob": -0.06493303091219156, "compression_ratio": 1.555084745762712, "no_speech_prob": 0.00010888075485127047}, {"id": 618, "seek": 200750, "start": 2007.5, "end": 2018.5, "text": " So that's basically why it's important to be asking, well, what are these things good for,", "tokens": [407, 300, 311, 1936, 983, 309, 311, 1021, 281, 312, 3365, 11, 731, 11, 437, 366, 613, 721, 665, 337, 11], "temperature": 0.0, "avg_logprob": -0.179449412287498, "compression_ratio": 1.4651162790697674, "no_speech_prob": 9.166103700408712e-05}, {"id": 619, "seek": 200750, "start": 2018.5, "end": 2023.5, "text": " and not say, oh, everything should be using parsing, because it's true that not everything should.", "tokens": [293, 406, 584, 11, 1954, 11, 1203, 820, 312, 1228, 21156, 278, 11, 570, 309, 311, 2074, 300, 406, 1203, 820, 13], "temperature": 0.0, "avg_logprob": -0.179449412287498, "compression_ratio": 1.4651162790697674, "no_speech_prob": 9.166103700408712e-05}, {"id": 620, "seek": 202350, "start": 2023.5, "end": 2040.5, "text": " So the question is, if we look at other aspects of language variation instead of just the segmentation and things,", "tokens": [407, 264, 1168, 307, 11, 498, 321, 574, 412, 661, 7270, 295, 2856, 12990, 2602, 295, 445, 264, 9469, 399, 293, 721, 11], "temperature": 0.0, "avg_logprob": -0.1594813100753292, "compression_ratio": 1.5714285714285714, "no_speech_prob": 9.023041639011353e-05}, {"id": 621, "seek": 202350, "start": 2040.5, "end": 2043.5, "text": " how does the incremental model perform?", "tokens": [577, 775, 264, 35759, 2316, 2042, 30], "temperature": 0.0, "avg_logprob": -0.1594813100753292, "compression_ratio": 1.5714285714285714, "no_speech_prob": 9.023041639011353e-05}, {"id": 622, "seek": 202350, "start": 2043.5, "end": 2046.5, "text": " So specifically, how does it perform in free word or languages,", "tokens": [407, 4682, 11, 577, 775, 309, 2042, 294, 1737, 1349, 420, 8650, 11], "temperature": 0.0, "avg_logprob": -0.1594813100753292, "compression_ratio": 1.5714285714285714, "no_speech_prob": 9.023041639011353e-05}, {"id": 623, "seek": 202350, "start": 2046.5, "end": 2051.5, "text": " perhaps ones with longer range or crossing dependencies?", "tokens": [4317, 2306, 365, 2854, 3613, 420, 14712, 36606, 30], "temperature": 0.0, "avg_logprob": -0.1594813100753292, "compression_ratio": 1.5714285714285714, "no_speech_prob": 9.023041639011353e-05}, {"id": 624, "seek": 205150, "start": 2051.5, "end": 2056.5, "text": " So Stanford, actually, their paper had excellent analysis about a lot of these questions.", "tokens": [407, 20374, 11, 767, 11, 641, 3035, 632, 7103, 5215, 466, 257, 688, 295, 613, 1651, 13], "temperature": 0.0, "avg_logprob": -0.06592717686214962, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.00011055728100473061}, {"id": 625, "seek": 205150, "start": 2056.5, "end": 2062.5, "text": " And so they showed that their model, which is much less sensitive to whether the trees are projective,", "tokens": [400, 370, 436, 4712, 300, 641, 2316, 11, 597, 307, 709, 1570, 9477, 281, 1968, 264, 5852, 366, 1716, 488, 11], "temperature": 0.0, "avg_logprob": -0.06592717686214962, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.00011055728100473061}, {"id": 626, "seek": 205150, "start": 2062.5, "end": 2065.5, "text": " they do do relatively well in those languages.", "tokens": [436, 360, 360, 7226, 731, 294, 729, 8650, 13], "temperature": 0.0, "avg_logprob": -0.06592717686214962, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.00011055728100473061}, {"id": 627, "seek": 205150, "start": 2065.5, "end": 2076.5, "text": " So for our preliminary results, we do fine on German and pretty well on Russian.", "tokens": [407, 337, 527, 28817, 3542, 11, 321, 360, 2489, 322, 6521, 293, 1238, 731, 322, 7220, 13], "temperature": 0.0, "avg_logprob": -0.06592717686214962, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.00011055728100473061}, {"id": 628, "seek": 207650, "start": 2076.5, "end": 2082.5, "text": " We still suck at Finnish, and I think there's a bug in Korean.", "tokens": [492, 920, 9967, 412, 38429, 11, 293, 286, 519, 456, 311, 257, 7426, 294, 6933, 13], "temperature": 0.0, "avg_logprob": -0.09447556734085083, "compression_ratio": 1.5110132158590308, "no_speech_prob": 0.0002095159434247762}, {"id": 629, "seek": 207650, "start": 2082.5, "end": 2085.5, "text": " It's at like 50%.", "tokens": [467, 311, 412, 411, 2625, 6856], "temperature": 0.0, "avg_logprob": -0.09447556734085083, "compression_ratio": 1.5110132158590308, "no_speech_prob": 0.0002095159434247762}, {"id": 630, "seek": 207650, "start": 2085.5, "end": 2087.5, "text": " So it's a mixed bag.", "tokens": [407, 309, 311, 257, 7467, 3411, 13], "temperature": 0.0, "avg_logprob": -0.09447556734085083, "compression_ratio": 1.5110132158590308, "no_speech_prob": 0.0002095159434247762}, {"id": 631, "seek": 207650, "start": 2087.5, "end": 2091.5, "text": " But I would say that there's some problems to solve about the projectivity.", "tokens": [583, 286, 576, 584, 300, 456, 311, 512, 2740, 281, 5039, 466, 264, 1716, 4253, 13], "temperature": 0.0, "avg_logprob": -0.09447556734085083, "compression_ratio": 1.5110132158590308, "no_speech_prob": 0.0002095159434247762}, {"id": 632, "seek": 207650, "start": 2091.5, "end": 2095.5, "text": " The way that I'm doing this is a little bit crude at the moment.", "tokens": [440, 636, 300, 286, 478, 884, 341, 307, 257, 707, 857, 30796, 412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.09447556734085083, "compression_ratio": 1.5110132158590308, "no_speech_prob": 0.0002095159434247762}, {"id": 633, "seek": 207650, "start": 2095.5, "end": 2102.5, "text": " But yeah, so in general, there is a disadvantage that we take from the incremental approach in this.", "tokens": [583, 1338, 11, 370, 294, 2674, 11, 456, 307, 257, 24292, 300, 321, 747, 490, 264, 35759, 3109, 294, 341, 13], "temperature": 0.0, "avg_logprob": -0.09447556734085083, "compression_ratio": 1.5110132158590308, "no_speech_prob": 0.0002095159434247762}, {"id": 634, "seek": 210250, "start": 2102.5, "end": 2106.5, "text": " And there's a lot of clever solutions that I'm looking into for this.", "tokens": [400, 456, 311, 257, 688, 295, 13494, 6547, 300, 286, 478, 1237, 666, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.11256881788665173, "compression_ratio": 1.76, "no_speech_prob": 0.00018499682482797652}, {"id": 635, "seek": 210250, "start": 2106.5, "end": 2108.5, "text": " So yeah.", "tokens": [407, 1338, 13], "temperature": 0.0, "avg_logprob": -0.11256881788665173, "compression_ratio": 1.76, "no_speech_prob": 0.00018499682482797652}, {"id": 636, "seek": 210250, "start": 2108.5, "end": 2112.5, "text": " Any words, sentiment analysis in the library?", "tokens": [2639, 2283, 11, 16149, 5215, 294, 264, 6405, 30], "temperature": 0.0, "avg_logprob": -0.11256881788665173, "compression_ratio": 1.76, "no_speech_prob": 0.00018499682482797652}, {"id": 637, "seek": 210250, "start": 2112.5, "end": 2117.5, "text": " So there's a pretty good extension package for our coreference resolution", "tokens": [407, 456, 311, 257, 1238, 665, 10320, 7372, 337, 527, 4965, 5158, 8669], "temperature": 0.0, "avg_logprob": -0.11256881788665173, "compression_ratio": 1.76, "no_speech_prob": 0.00018499682482797652}, {"id": 638, "seek": 210250, "start": 2117.5, "end": 2121.5, "text": " that has taken some of the pressure off us to support it internally.", "tokens": [300, 575, 2726, 512, 295, 264, 3321, 766, 505, 281, 1406, 309, 19501, 13], "temperature": 0.0, "avg_logprob": -0.11256881788665173, "compression_ratio": 1.76, "no_speech_prob": 0.00018499682482797652}, {"id": 639, "seek": 210250, "start": 2121.5, "end": 2124.5, "text": " We do think that coreference resolution is something that does belong in the library", "tokens": [492, 360, 519, 300, 4965, 5158, 8669, 307, 746, 300, 775, 5784, 294, 264, 6405], "temperature": 0.0, "avg_logprob": -0.11256881788665173, "compression_ratio": 1.76, "no_speech_prob": 0.00018499682482797652}, {"id": 640, "seek": 210250, "start": 2124.5, "end": 2129.5, "text": " because it's something that does have that property of being a language internal thing.", "tokens": [570, 309, 311, 746, 300, 775, 362, 300, 4707, 295, 885, 257, 2856, 6920, 551, 13], "temperature": 0.0, "avg_logprob": -0.11256881788665173, "compression_ratio": 1.76, "no_speech_prob": 0.00018499682482797652}, {"id": 641, "seek": 212950, "start": 2129.5, "end": 2133.5, "text": " So I think that there's a truth about whether that he or she belongs to that noun", "tokens": [407, 286, 519, 300, 456, 311, 257, 3494, 466, 1968, 300, 415, 420, 750, 12953, 281, 300, 23307], "temperature": 0.0, "avg_logprob": -0.06379701891018234, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.0003340294351801276}, {"id": 642, "seek": 212950, "start": 2133.5, "end": 2135.5, "text": " that doesn't depend on the application.", "tokens": [300, 1177, 380, 5672, 322, 264, 3861, 13], "temperature": 0.0, "avg_logprob": -0.06379701891018234, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.0003340294351801276}, {"id": 643, "seek": 212950, "start": 2135.5, "end": 2137.5, "text": " It's just a truth fact about that sentence.", "tokens": [467, 311, 445, 257, 3494, 1186, 466, 300, 8174, 13], "temperature": 0.0, "avg_logprob": -0.06379701891018234, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.0003340294351801276}, {"id": 644, "seek": 212950, "start": 2137.5, "end": 2140.5, "text": " So we're very interested in being able to give you that piece of annotation.", "tokens": [407, 321, 434, 588, 3102, 294, 885, 1075, 281, 976, 291, 300, 2522, 295, 48654, 13], "temperature": 0.0, "avg_logprob": -0.06379701891018234, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.0003340294351801276}, {"id": 645, "seek": 212950, "start": 2140.5, "end": 2143.5, "text": " I wouldn't quite say the same thing about the sentiment.", "tokens": [286, 2759, 380, 1596, 584, 264, 912, 551, 466, 264, 16149, 13], "temperature": 0.0, "avg_logprob": -0.06379701891018234, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.0003340294351801276}, {"id": 646, "seek": 212950, "start": 2143.5, "end": 2145.5, "text": " I don't quite know.", "tokens": [286, 500, 380, 1596, 458, 13], "temperature": 0.0, "avg_logprob": -0.06379701891018234, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.0003340294351801276}, {"id": 647, "seek": 212950, "start": 2145.5, "end": 2150.5, "text": " I haven't been convinced by any schema of sentiment that is sufficiently independent", "tokens": [286, 2378, 380, 668, 12561, 538, 604, 34078, 295, 16149, 300, 307, 31868, 6695], "temperature": 0.0, "avg_logprob": -0.06379701891018234, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.0003340294351801276}, {"id": 648, "seek": 212950, "start": 2150.5, "end": 2152.5, "text": " of what you're trying to do that we could provide it.", "tokens": [295, 437, 291, 434, 1382, 281, 360, 300, 321, 727, 2893, 309, 13], "temperature": 0.0, "avg_logprob": -0.06379701891018234, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.0003340294351801276}, {"id": 649, "seek": 212950, "start": 2152.5, "end": 2155.5, "text": " Instead, what we do provide you is a text categorization library.", "tokens": [7156, 11, 437, 321, 360, 2893, 291, 307, 257, 2487, 19250, 2144, 6405, 13], "temperature": 0.0, "avg_logprob": -0.06379701891018234, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.0003340294351801276}, {"id": 650, "seek": 215550, "start": 2155.5, "end": 2161.5, "text": " And the text categorization model that we have is only one of many that you might build.", "tokens": [400, 264, 2487, 19250, 2144, 2316, 300, 321, 362, 307, 787, 472, 295, 867, 300, 291, 1062, 1322, 13], "temperature": 0.0, "avg_logprob": -0.06177632643444703, "compression_ratio": 1.639676113360324, "no_speech_prob": 3.426281182328239e-05}, {"id": 651, "seek": 215550, "start": 2161.5, "end": 2163.5, "text": " And it's not best for every application.", "tokens": [400, 309, 311, 406, 1151, 337, 633, 3861, 13], "temperature": 0.0, "avg_logprob": -0.06177632643444703, "compression_ratio": 1.639676113360324, "no_speech_prob": 3.426281182328239e-05}, {"id": 652, "seek": 215550, "start": 2163.5, "end": 2167.5, "text": " But it does do pretty well for short text.", "tokens": [583, 309, 775, 360, 1238, 731, 337, 2099, 2487, 13], "temperature": 0.0, "avg_logprob": -0.06177632643444703, "compression_ratio": 1.639676113360324, "no_speech_prob": 3.426281182328239e-05}, {"id": 653, "seek": 215550, "start": 2167.5, "end": 2173.5, "text": " And I think that on many sentiment benchmarks, it performs quite well.", "tokens": [400, 286, 519, 300, 322, 867, 16149, 43751, 11, 309, 26213, 1596, 731, 13], "temperature": 0.0, "avg_logprob": -0.06177632643444703, "compression_ratio": 1.639676113360324, "no_speech_prob": 3.426281182328239e-05}, {"id": 654, "seek": 215550, "start": 2173.5, "end": 2177.5, "text": " It's a lot slower than some other sentiment ways that you could do sentiment.", "tokens": [467, 311, 257, 688, 14009, 813, 512, 661, 16149, 2098, 300, 291, 727, 360, 16149, 13], "temperature": 0.0, "avg_logprob": -0.06177632643444703, "compression_ratio": 1.639676113360324, "no_speech_prob": 3.426281182328239e-05}, {"id": 655, "seek": 215550, "start": 2177.5, "end": 2182.5, "text": " So it depends on what type of text you're trying to process and that sort of thing.", "tokens": [407, 309, 5946, 322, 437, 2010, 295, 2487, 291, 434, 1382, 281, 1399, 293, 300, 1333, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.06177632643444703, "compression_ratio": 1.639676113360324, "no_speech_prob": 3.426281182328239e-05}, {"id": 656, "seek": 218250, "start": 2182.5, "end": 2185.5, "text": " Well, yes.", "tokens": [1042, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.21833805508083767, "compression_ratio": 1.5307262569832403, "no_speech_prob": 0.0003513435076456517}, {"id": 657, "seek": 218250, "start": 2185.5, "end": 2193.5, "text": " So explicitly, the coreference resolution package that you should use is called neural coref.", "tokens": [407, 20803, 11, 264, 4965, 5158, 8669, 7372, 300, 291, 820, 764, 307, 1219, 18161, 4965, 69, 13], "temperature": 0.0, "avg_logprob": -0.21833805508083767, "compression_ratio": 1.5307262569832403, "no_speech_prob": 0.0003513435076456517}, {"id": 658, "seek": 218250, "start": 2193.5, "end": 2195.5, "text": " So neural coref.", "tokens": [407, 18161, 4965, 69, 13], "temperature": 0.0, "avg_logprob": -0.21833805508083767, "compression_ratio": 1.5307262569832403, "no_speech_prob": 0.0003513435076456517}, {"id": 659, "seek": 218250, "start": 2195.5, "end": 2197.5, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.21833805508083767, "compression_ratio": 1.5307262569832403, "no_speech_prob": 0.0003513435076456517}, {"id": 660, "seek": 218250, "start": 2197.5, "end": 2199.5, "text": " It's built on PyTorch.", "tokens": [467, 311, 3094, 322, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.21833805508083767, "compression_ratio": 1.5307262569832403, "no_speech_prob": 0.0003513435076456517}, {"id": 661, "seek": 218250, "start": 2199.5, "end": 2200.5, "text": " It's overall pretty good.", "tokens": [467, 311, 4787, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.21833805508083767, "compression_ratio": 1.5307262569832403, "no_speech_prob": 0.0003513435076456517}, {"id": 662, "seek": 218250, "start": 2200.5, "end": 2202.5, "text": " You can train it yourself.", "tokens": [509, 393, 3847, 309, 1803, 13], "temperature": 0.0, "avg_logprob": -0.21833805508083767, "compression_ratio": 1.5307262569832403, "no_speech_prob": 0.0003513435076456517}, {"id": 663, "seek": 218250, "start": 2202.5, "end": 2205.5, "text": " Well, PyTorch is the machine learning layer.", "tokens": [1042, 11, 9953, 51, 284, 339, 307, 264, 3479, 2539, 4583, 13], "temperature": 0.0, "avg_logprob": -0.21833805508083767, "compression_ratio": 1.5307262569832403, "no_speech_prob": 0.0003513435076456517}, {"id": 664, "seek": 218250, "start": 2205.5, "end": 2207.5, "text": " Yes, it's built on space.", "tokens": [1079, 11, 309, 311, 3094, 322, 1901, 13], "temperature": 0.0, "avg_logprob": -0.21833805508083767, "compression_ratio": 1.5307262569832403, "no_speech_prob": 0.0003513435076456517}, {"id": 665, "seek": 220750, "start": 2207.5, "end": 2219.5, "text": " So for German, I think it's pretty easy.", "tokens": [407, 337, 6521, 11, 286, 519, 309, 311, 1238, 1858, 13], "temperature": 0.0, "avg_logprob": -0.1031646877527237, "compression_ratio": 1.3614457831325302, "no_speech_prob": 3.37344863510225e-05}, {"id": 666, "seek": 220750, "start": 2219.5, "end": 2223.5, "text": " I've been using the word vectors trained by fast text.", "tokens": [286, 600, 668, 1228, 264, 1349, 18875, 8895, 538, 2370, 2487, 13], "temperature": 0.0, "avg_logprob": -0.1031646877527237, "compression_ratio": 1.3614457831325302, "no_speech_prob": 3.37344863510225e-05}, {"id": 667, "seek": 220750, "start": 2223.5, "end": 2226.5, "text": " And you can basically just plug that in.", "tokens": [400, 291, 393, 1936, 445, 5452, 300, 294, 13], "temperature": 0.0, "avg_logprob": -0.1031646877527237, "compression_ratio": 1.3614457831325302, "no_speech_prob": 3.37344863510225e-05}, {"id": 668, "seek": 220750, "start": 2226.5, "end": 2233.5, "text": " So there's sort of one command to convert that into a spacey vocab object and load it up.", "tokens": [407, 456, 311, 1333, 295, 472, 5622, 281, 7620, 300, 666, 257, 1901, 88, 2329, 455, 2657, 293, 3677, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.1031646877527237, "compression_ratio": 1.3614457831325302, "no_speech_prob": 3.37344863510225e-05}, {"id": 669, "seek": 223350, "start": 2233.5, "end": 2238.5, "text": " We're trying to provide pre-trained models which don't depend on pre-trained word vectors.", "tokens": [492, 434, 1382, 281, 2893, 659, 12, 17227, 2001, 5245, 597, 500, 380, 5672, 322, 659, 12, 17227, 2001, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.11564923154896703, "compression_ratio": 1.7490196078431373, "no_speech_prob": 8.218531729653478e-05}, {"id": 670, "seek": 223350, "start": 2238.5, "end": 2239.5, "text": " So you can bring your own.", "tokens": [407, 291, 393, 1565, 428, 1065, 13], "temperature": 0.0, "avg_logprob": -0.11564923154896703, "compression_ratio": 1.7490196078431373, "no_speech_prob": 8.218531729653478e-05}, {"id": 671, "seek": 223350, "start": 2239.5, "end": 2244.5, "text": " Because otherwise, there's kind of this conflict of, you know, the model's been trained to expect some word vectors.", "tokens": [1436, 5911, 11, 456, 311, 733, 295, 341, 6596, 295, 11, 291, 458, 11, 264, 2316, 311, 668, 8895, 281, 2066, 512, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.11564923154896703, "compression_ratio": 1.7490196078431373, "no_speech_prob": 8.218531729653478e-05}, {"id": 672, "seek": 223350, "start": 2244.5, "end": 2249.5, "text": " And then if you sub your own in, it's going to get different input representations.", "tokens": [400, 550, 498, 291, 1422, 428, 1065, 294, 11, 309, 311, 516, 281, 483, 819, 4846, 33358, 13], "temperature": 0.0, "avg_logprob": -0.11564923154896703, "compression_ratio": 1.7490196078431373, "no_speech_prob": 8.218531729653478e-05}, {"id": 673, "seek": 223350, "start": 2249.5, "end": 2255.5, "text": " So but yeah, training or bringing your own vectors is designed to be pretty easy.", "tokens": [407, 457, 1338, 11, 3097, 420, 5062, 428, 1065, 18875, 307, 4761, 281, 312, 1238, 1858, 13], "temperature": 0.0, "avg_logprob": -0.11564923154896703, "compression_ratio": 1.7490196078431373, "no_speech_prob": 8.218531729653478e-05}, {"id": 674, "seek": 223350, "start": 2255.5, "end": 2257.5, "text": " And if it's not, I apologize if there's bugs.", "tokens": [400, 498, 309, 311, 406, 11, 286, 12328, 498, 456, 311, 15120, 13], "temperature": 0.0, "avg_logprob": -0.11564923154896703, "compression_ratio": 1.7490196078431373, "no_speech_prob": 8.218531729653478e-05}, {"id": 675, "seek": 225750, "start": 2257.5, "end": 2266.5, "text": " And we'll try to fix them.", "tokens": [400, 321, 603, 853, 281, 3191, 552, 13], "temperature": 0.0, "avg_logprob": -0.09305505988038616, "compression_ratio": 1.5294117647058822, "no_speech_prob": 8.217986032832414e-05}, {"id": 676, "seek": 225750, "start": 2266.5, "end": 2274.5, "text": " So the question is, after parsing and interpreting, do we have an interlingual representation that can then be used to generate another language?", "tokens": [407, 264, 1168, 307, 11, 934, 21156, 278, 293, 37395, 11, 360, 321, 362, 364, 728, 1688, 901, 10290, 300, 393, 550, 312, 1143, 281, 8460, 1071, 2856, 30], "temperature": 0.0, "avg_logprob": -0.09305505988038616, "compression_ratio": 1.5294117647058822, "no_speech_prob": 8.217986032832414e-05}, {"id": 677, "seek": 225750, "start": 2274.5, "end": 2276.5, "text": " The answer is probably not.", "tokens": [440, 1867, 307, 1391, 406, 13], "temperature": 0.0, "avg_logprob": -0.09305505988038616, "compression_ratio": 1.5294117647058822, "no_speech_prob": 8.217986032832414e-05}, {"id": 678, "seek": 225750, "start": 2276.5, "end": 2279.5, "text": " I mean, I don't have we don't have generation capabilities in spacey.", "tokens": [286, 914, 11, 286, 500, 380, 362, 321, 500, 380, 362, 5125, 10862, 294, 1901, 88, 13], "temperature": 0.0, "avg_logprob": -0.09305505988038616, "compression_ratio": 1.5294117647058822, "no_speech_prob": 8.217986032832414e-05}, {"id": 679, "seek": 225750, "start": 2279.5, "end": 2282.5, "text": " People have worked on this sort of thing.", "tokens": [3432, 362, 2732, 322, 341, 1333, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.09305505988038616, "compression_ratio": 1.5294117647058822, "no_speech_prob": 8.217986032832414e-05}, {"id": 680, "seek": 228250, "start": 2282.5, "end": 2291.5, "text": " But in general, having an explicit interlingua tends to perform less well than more brute force statistical approaches to syntax.", "tokens": [583, 294, 2674, 11, 1419, 364, 13691, 728, 1688, 4398, 12258, 281, 2042, 1570, 731, 813, 544, 47909, 3464, 22820, 11587, 281, 28431, 13], "temperature": 0.0, "avg_logprob": -0.06853416342484324, "compression_ratio": 1.70703125, "no_speech_prob": 2.0780154954991303e-05}, {"id": 681, "seek": 228250, "start": 2291.5, "end": 2301.5, "text": " And I think the reason does sort of make sense that, you know, the languages are pretty different in the way that they phrase things and the way that they model the world in lots of ways.", "tokens": [400, 286, 519, 264, 1778, 775, 1333, 295, 652, 2020, 300, 11, 291, 458, 11, 264, 8650, 366, 1238, 819, 294, 264, 636, 300, 436, 9535, 721, 293, 264, 636, 300, 436, 2316, 264, 1002, 294, 3195, 295, 2098, 13], "temperature": 0.0, "avg_logprob": -0.06853416342484324, "compression_ratio": 1.70703125, "no_speech_prob": 2.0780154954991303e-05}, {"id": 682, "seek": 228250, "start": 2301.5, "end": 2307.5, "text": " And so getting a translation that's remotely idiomatic out of that sort of interlingual representation is pretty tough.", "tokens": [400, 370, 1242, 257, 12853, 300, 311, 20824, 18014, 13143, 484, 295, 300, 1333, 295, 728, 1688, 901, 10290, 307, 1238, 4930, 13], "temperature": 0.0, "avg_logprob": -0.06853416342484324, "compression_ratio": 1.70703125, "no_speech_prob": 2.0780154954991303e-05}, {"id": 683, "seek": 230750, "start": 2307.5, "end": 2318.5, "text": " So and then there's another argument that you're solving a sub problem that's harder than the than the direct translation approach, which I'm not sure whether I buy that argument or not.", "tokens": [407, 293, 550, 456, 311, 1071, 6770, 300, 291, 434, 12606, 257, 1422, 1154, 300, 311, 6081, 813, 264, 813, 264, 2047, 12853, 3109, 11, 597, 286, 478, 406, 988, 1968, 286, 2256, 300, 6770, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.11593983453862808, "compression_ratio": 1.483695652173913, "no_speech_prob": 3.3726082619978115e-05}, {"id": 684, "seek": 230750, "start": 2318.5, "end": 2324.5, "text": " But it's a common one that people use.", "tokens": [583, 309, 311, 257, 2689, 472, 300, 561, 764, 13], "temperature": 0.0, "avg_logprob": -0.11593983453862808, "compression_ratio": 1.483695652173913, "no_speech_prob": 3.3726082619978115e-05}, {"id": 685, "seek": 230750, "start": 2324.5, "end": 2335.5, "text": " OK, so should we move forward to the next talk?", "tokens": [2264, 11, 370, 820, 321, 1286, 2128, 281, 264, 958, 751, 30], "temperature": 0.0, "avg_logprob": -0.11593983453862808, "compression_ratio": 1.483695652173913, "no_speech_prob": 3.3726082619978115e-05}, {"id": 686, "seek": 233550, "start": 2335.5, "end": 2345.5, "text": " So, yeah, we've we started out by hearing a lot about the more theoretical side of things.", "tokens": [407, 11, 1338, 11, 321, 600, 321, 1409, 484, 538, 4763, 257, 688, 466, 264, 544, 20864, 1252, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.06437763365188448, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.00013686266902368516}, {"id": 687, "seek": 233550, "start": 2345.5, "end": 2352.5, "text": " And I'm actually going to talk about how we collect and build training data for all these great models we can now build.", "tokens": [400, 286, 478, 767, 516, 281, 751, 466, 577, 321, 2500, 293, 1322, 3097, 1412, 337, 439, 613, 869, 5245, 321, 393, 586, 1322, 13], "temperature": 0.0, "avg_logprob": -0.06437763365188448, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.00013686266902368516}, {"id": 688, "seek": 233550, "start": 2352.5, "end": 2359.5, "text": " And the nice thing about machine learning is that, well, we can now train a system by just showing examples of what we want.", "tokens": [400, 264, 1481, 551, 466, 3479, 2539, 307, 300, 11, 731, 11, 321, 393, 586, 3847, 257, 1185, 538, 445, 4099, 5110, 295, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.06437763365188448, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.00013686266902368516}, {"id": 689, "seek": 233550, "start": 2359.5, "end": 2363.5, "text": " And that's great. But the problem is, of course, we need those examples.", "tokens": [400, 300, 311, 869, 13, 583, 264, 1154, 307, 11, 295, 1164, 11, 321, 643, 729, 5110, 13], "temperature": 0.0, "avg_logprob": -0.06437763365188448, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.00013686266902368516}, {"id": 690, "seek": 236350, "start": 2363.5, "end": 2374.5, "text": " And even if you're like, oh, I got this all figured out, are you using this amazing unsupervised method that where my system just infers the categories from the data and I never need to label any data?", "tokens": [400, 754, 498, 291, 434, 411, 11, 1954, 11, 286, 658, 341, 439, 8932, 484, 11, 366, 291, 1228, 341, 2243, 2693, 12879, 24420, 3170, 300, 689, 452, 1185, 445, 1536, 433, 264, 10479, 490, 264, 1412, 293, 286, 1128, 643, 281, 7645, 604, 1412, 30], "temperature": 0.0, "avg_logprob": -0.08046294460777476, "compression_ratio": 1.6808510638297873, "no_speech_prob": 5.0912523875012994e-05}, {"id": 691, "seek": 236350, "start": 2374.5, "end": 2378.5, "text": " That's pretty nice. But you still need some way of evaluating your system.", "tokens": [663, 311, 1238, 1481, 13, 583, 291, 920, 643, 512, 636, 295, 27479, 428, 1185, 13], "temperature": 0.0, "avg_logprob": -0.08046294460777476, "compression_ratio": 1.6808510638297873, "no_speech_prob": 5.0912523875012994e-05}, {"id": 692, "seek": 236350, "start": 2378.5, "end": 2382.5, "text": " So we pretty much always need some form of annotations.", "tokens": [407, 321, 1238, 709, 1009, 643, 512, 1254, 295, 25339, 763, 13], "temperature": 0.0, "avg_logprob": -0.08046294460777476, "compression_ratio": 1.6808510638297873, "no_speech_prob": 5.0912523875012994e-05}, {"id": 693, "seek": 236350, "start": 2382.5, "end": 2387.5, "text": " And now the question is, well, why why do we even care about this?", "tokens": [400, 586, 264, 1168, 307, 11, 731, 11, 983, 983, 360, 321, 754, 1127, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.08046294460777476, "compression_ratio": 1.6808510638297873, "no_speech_prob": 5.0912523875012994e-05}, {"id": 694, "seek": 236350, "start": 2387.5, "end": 2392.5, "text": " Why do we care about whether this is efficient, whether this works or not?", "tokens": [1545, 360, 321, 1127, 466, 1968, 341, 307, 7148, 11, 1968, 341, 1985, 420, 406, 30], "temperature": 0.0, "avg_logprob": -0.08046294460777476, "compression_ratio": 1.6808510638297873, "no_speech_prob": 5.0912523875012994e-05}, {"id": 695, "seek": 239250, "start": 2392.5, "end": 2405.5, "text": " The thing is, the big problem is that we actually with many things in data science and machine learning, we need to try out things before we know whether they work or we often don't know whether an idea is going to work before we try it.", "tokens": [440, 551, 307, 11, 264, 955, 1154, 307, 300, 321, 767, 365, 867, 721, 294, 1412, 3497, 293, 3479, 2539, 11, 321, 643, 281, 853, 484, 721, 949, 321, 458, 1968, 436, 589, 420, 321, 2049, 500, 380, 458, 1968, 364, 1558, 307, 516, 281, 589, 949, 321, 853, 309, 13], "temperature": 0.0, "avg_logprob": -0.08745531828507133, "compression_ratio": 1.8054474708171206, "no_speech_prob": 0.00010296042455593124}, {"id": 696, "seek": 239250, "start": 2405.5, "end": 2412.5, "text": " So we need to expect to do annotation lots of times and start off from scratch, start all over again.", "tokens": [407, 321, 643, 281, 2066, 281, 360, 48654, 3195, 295, 1413, 293, 722, 766, 490, 8459, 11, 722, 439, 670, 797, 13], "temperature": 0.0, "avg_logprob": -0.08745531828507133, "compression_ratio": 1.8054474708171206, "no_speech_prob": 0.00010296042455593124}, {"id": 697, "seek": 239250, "start": 2412.5, "end": 2416.5, "text": " If we got if we fucked up our label scheme, try something else.", "tokens": [759, 321, 658, 498, 321, 22518, 493, 527, 7645, 12232, 11, 853, 746, 1646, 13], "temperature": 0.0, "avg_logprob": -0.08745531828507133, "compression_ratio": 1.8054474708171206, "no_speech_prob": 0.00010296042455593124}, {"id": 698, "seek": 239250, "start": 2416.5, "end": 2419.5, "text": " So we we need to do this lots of times. So it needs to work.", "tokens": [407, 321, 321, 643, 281, 360, 341, 3195, 295, 1413, 13, 407, 309, 2203, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.08745531828507133, "compression_ratio": 1.8054474708171206, "no_speech_prob": 0.00010296042455593124}, {"id": 699, "seek": 241950, "start": 2419.5, "end": 2429.5, "text": " And similarly, especially if you're working in a company in a team where you really want to use your model to find something out.", "tokens": [400, 14138, 11, 2318, 498, 291, 434, 1364, 294, 257, 2237, 294, 257, 1469, 689, 291, 534, 528, 281, 764, 428, 2316, 281, 915, 746, 484, 13], "temperature": 0.0, "avg_logprob": -0.11375183754778923, "compression_ratio": 1.5903614457831325, "no_speech_prob": 9.093482367461547e-05}, {"id": 700, "seek": 241950, "start": 2429.5, "end": 2433.5, "text": " Ideally, the person building the model should be involved in that process.", "tokens": [40817, 11, 264, 954, 2390, 264, 2316, 820, 312, 3288, 294, 300, 1399, 13], "temperature": 0.0, "avg_logprob": -0.11375183754778923, "compression_ratio": 1.5903614457831325, "no_speech_prob": 9.093482367461547e-05}, {"id": 701, "seek": 241950, "start": 2433.5, "end": 2438.5, "text": " And also, we always say good annotation teams are small. A lot of people don't understand this.", "tokens": [400, 611, 11, 321, 1009, 584, 665, 48654, 5491, 366, 1359, 13, 316, 688, 295, 561, 500, 380, 1223, 341, 13], "temperature": 0.0, "avg_logprob": -0.11375183754778923, "compression_ratio": 1.5903614457831325, "no_speech_prob": 9.093482367461547e-05}, {"id": 702, "seek": 241950, "start": 2438.5, "end": 2444.5, "text": " There's a lot of movement towards, oh, let's crowdsource this, get like hundreds of volunteers.", "tokens": [821, 311, 257, 688, 295, 3963, 3030, 11, 1954, 11, 718, 311, 26070, 2948, 341, 11, 483, 411, 6779, 295, 14352, 13], "temperature": 0.0, "avg_logprob": -0.11375183754778923, "compression_ratio": 1.5903614457831325, "no_speech_prob": 9.093482367461547e-05}, {"id": 703, "seek": 244450, "start": 2444.5, "end": 2451.5, "text": " And we always have to remind, especially companies that, well, look at the big corpora that we use to train models like those.", "tokens": [400, 321, 1009, 362, 281, 4160, 11, 2318, 3431, 300, 11, 731, 11, 574, 412, 264, 955, 6804, 64, 300, 321, 764, 281, 3847, 5245, 411, 729, 13], "temperature": 0.0, "avg_logprob": -0.1250078560102104, "compression_ratio": 1.6053639846743295, "no_speech_prob": 5.715339648304507e-05}, {"id": 704, "seek": 244450, "start": 2451.5, "end": 2455.5, "text": " The good ones were produced by very few people. And there's a reason for that.", "tokens": [440, 665, 2306, 645, 7126, 538, 588, 1326, 561, 13, 400, 456, 311, 257, 1778, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.1250078560102104, "compression_ratio": 1.6053639846743295, "no_speech_prob": 5.715339648304507e-05}, {"id": 705, "seek": 244450, "start": 2455.5, "end": 2460.5, "text": " It like does not more people doesn't always mean better results. Actually, quite the opposite.", "tokens": [467, 411, 775, 406, 544, 561, 1177, 380, 1009, 914, 1101, 3542, 13, 5135, 11, 1596, 264, 6182, 13], "temperature": 0.0, "avg_logprob": -0.1250078560102104, "compression_ratio": 1.6053639846743295, "no_speech_prob": 5.715339648304507e-05}, {"id": 706, "seek": 244450, "start": 2460.5, "end": 2468.5, "text": " So, you know, how how great would it be if actually the developer of the model could be involved in labeling the data?", "tokens": [407, 11, 291, 458, 11, 577, 577, 869, 576, 309, 312, 498, 767, 264, 10754, 295, 264, 2316, 727, 312, 3288, 294, 40244, 264, 1412, 30], "temperature": 0.0, "avg_logprob": -0.1250078560102104, "compression_ratio": 1.6053639846743295, "no_speech_prob": 5.715339648304507e-05}, {"id": 707, "seek": 246850, "start": 2468.5, "end": 2476.5, "text": " And of course, we also have the problem of the specialist knowledge, especially in, you know, in industries where this matters.", "tokens": [400, 295, 1164, 11, 321, 611, 362, 264, 1154, 295, 264, 17008, 3601, 11, 2318, 294, 11, 291, 458, 11, 294, 13284, 689, 341, 7001, 13], "temperature": 0.0, "avg_logprob": -0.0896597498470975, "compression_ratio": 1.6538461538461537, "no_speech_prob": 7.304637256311253e-05}, {"id": 708, "seek": 246850, "start": 2476.5, "end": 2485.5, "text": " You might want to have a medical professional give some feedback on the labels or actually really label your data or maybe a finance expert.", "tokens": [509, 1062, 528, 281, 362, 257, 4625, 4843, 976, 512, 5824, 322, 264, 16949, 420, 767, 534, 7645, 428, 1412, 420, 1310, 257, 10719, 5844, 13], "temperature": 0.0, "avg_logprob": -0.0896597498470975, "compression_ratio": 1.6538461538461537, "no_speech_prob": 7.304637256311253e-05}, {"id": 709, "seek": 246850, "start": 2485.5, "end": 2493.5, "text": " And, yeah, those people usually have limited time. If you get an hour of their time, you want to use it more efficiently and you don't want to bore them to death", "tokens": [400, 11, 1338, 11, 729, 561, 2673, 362, 5567, 565, 13, 759, 291, 483, 364, 1773, 295, 641, 565, 11, 291, 528, 281, 764, 309, 544, 19621, 293, 291, 500, 380, 528, 281, 26002, 552, 281, 2966], "temperature": 0.0, "avg_logprob": -0.0896597498470975, "compression_ratio": 1.6538461538461537, "no_speech_prob": 7.304637256311253e-05}, {"id": 710, "seek": 249350, "start": 2493.5, "end": 2505.5, "text": " or actually find the one person who has nothing else to do because they're probably their knowledge is probably not as valuable as other people, other experts knowledge.", "tokens": [420, 767, 915, 264, 472, 954, 567, 575, 1825, 1646, 281, 360, 570, 436, 434, 1391, 641, 3601, 307, 1391, 406, 382, 8263, 382, 661, 561, 11, 661, 8572, 3601, 13], "temperature": 0.0, "avg_logprob": -0.14114718437194823, "compression_ratio": 1.7053140096618358, "no_speech_prob": 4.2462885176064447e-05}, {"id": 711, "seek": 249350, "start": 2505.5, "end": 2512.5, "text": " And yeah, and another big problem since you know, you want humans is that humans are actually humans kind of suck.", "tokens": [400, 1338, 11, 293, 1071, 955, 1154, 1670, 291, 458, 11, 291, 528, 6255, 307, 300, 6255, 366, 767, 6255, 733, 295, 9967, 13], "temperature": 0.0, "avg_logprob": -0.14114718437194823, "compression_ratio": 1.7053140096618358, "no_speech_prob": 4.2462885176064447e-05}, {"id": 712, "seek": 249350, "start": 2512.5, "end": 2517.5, "text": " Like we really we're not that efficient at a lot of a lot of things.", "tokens": [1743, 321, 534, 321, 434, 406, 300, 7148, 412, 257, 688, 295, 257, 688, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.14114718437194823, "compression_ratio": 1.7053140096618358, "no_speech_prob": 4.2462885176064447e-05}, {"id": 713, "seek": 251750, "start": 2517.5, "end": 2525.5, "text": " So, for example, like we really have problems performing boring unstructured tasks, especially things that require multiple steps and multiple things we need to get right.", "tokens": [407, 11, 337, 1365, 11, 411, 321, 534, 362, 2740, 10205, 9989, 18799, 46847, 9608, 11, 2318, 721, 300, 3651, 3866, 4439, 293, 3866, 721, 321, 643, 281, 483, 558, 13], "temperature": 0.0, "avg_logprob": -0.09428600688557048, "compression_ratio": 1.6680672268907564, "no_speech_prob": 3.223275052732788e-05}, {"id": 714, "seek": 251750, "start": 2525.5, "end": 2532.5, "text": " We can't remember stuff. We yeah, we really we're bad at consistency and getting stuff right.", "tokens": [492, 393, 380, 1604, 1507, 13, 492, 1338, 11, 321, 534, 321, 434, 1578, 412, 14416, 293, 1242, 1507, 558, 13], "temperature": 0.0, "avg_logprob": -0.09428600688557048, "compression_ratio": 1.6680672268907564, "no_speech_prob": 3.223275052732788e-05}, {"id": 715, "seek": 251750, "start": 2532.5, "end": 2536.5, "text": " So, yeah, fortunately, computers are really good at that stuff.", "tokens": [407, 11, 1338, 11, 25511, 11, 10807, 366, 534, 665, 412, 300, 1507, 13], "temperature": 0.0, "avg_logprob": -0.09428600688557048, "compression_ratio": 1.6680672268907564, "no_speech_prob": 3.223275052732788e-05}, {"id": 716, "seek": 251750, "start": 2536.5, "end": 2540.5, "text": " And in fact, it's probably also the main reason we built computers.", "tokens": [400, 294, 1186, 11, 309, 311, 1391, 611, 264, 2135, 1778, 321, 3094, 10807, 13], "temperature": 0.0, "avg_logprob": -0.09428600688557048, "compression_ratio": 1.6680672268907564, "no_speech_prob": 3.223275052732788e-05}, {"id": 717, "seek": 254050, "start": 2540.5, "end": 2547.5, "text": " So there's really no need to waste the human's time by making them do stuff that they're going to do badly anyways.", "tokens": [407, 456, 311, 534, 572, 643, 281, 5964, 264, 1952, 311, 565, 538, 1455, 552, 360, 1507, 300, 436, 434, 516, 281, 360, 13425, 13448, 13], "temperature": 0.0, "avg_logprob": -0.12566223764807227, "compression_ratio": 1.8327759197324414, "no_speech_prob": 5.807369961985387e-05}, {"id": 718, "seek": 254050, "start": 2547.5, "end": 2553.5, "text": " And instead, we want our annotation tooling to be as like automated as possible.", "tokens": [400, 2602, 11, 321, 528, 527, 48654, 46593, 281, 312, 382, 411, 18473, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.12566223764807227, "compression_ratio": 1.8327759197324414, "no_speech_prob": 5.807369961985387e-05}, {"id": 719, "seek": 254050, "start": 2553.5, "end": 2560.5, "text": " We want to in general, we want to automate as much as possible and really have the human focus on the stuff that the human is good at and we really need that input.", "tokens": [492, 528, 281, 294, 2674, 11, 321, 528, 281, 31605, 382, 709, 382, 1944, 293, 534, 362, 264, 1952, 1879, 322, 264, 1507, 300, 264, 1952, 307, 665, 412, 293, 321, 534, 643, 300, 4846, 13], "temperature": 0.0, "avg_logprob": -0.12566223764807227, "compression_ratio": 1.8327759197324414, "no_speech_prob": 5.807369961985387e-05}, {"id": 720, "seek": 254050, "start": 2560.5, "end": 2569.5, "text": " And that's usually context ambiguity like stuff like we can look at a sentence and most of us will be able to understand the figure of speech immediately without thinking twice about it.", "tokens": [400, 300, 311, 2673, 4319, 46519, 411, 1507, 411, 321, 393, 574, 412, 257, 8174, 293, 881, 295, 505, 486, 312, 1075, 281, 1223, 264, 2573, 295, 6218, 4258, 1553, 1953, 6091, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.12566223764807227, "compression_ratio": 1.8327759197324414, "no_speech_prob": 5.807369961985387e-05}, {"id": 721, "seek": 256950, "start": 2569.5, "end": 2572.5, "text": " That's the stuff that's really, really hard for computer.", "tokens": [663, 311, 264, 1507, 300, 311, 534, 11, 534, 1152, 337, 3820, 13], "temperature": 0.0, "avg_logprob": -0.15296291443238774, "compression_ratio": 1.5095238095238095, "no_speech_prob": 3.491202005534433e-05}, {"id": 722, "seek": 256950, "start": 2572.5, "end": 2578.5, "text": " Also, you know, put differently. Yeah, humans are good at precision. Computers are good at recall.", "tokens": [2743, 11, 291, 458, 11, 829, 7614, 13, 865, 11, 6255, 366, 665, 412, 18356, 13, 37804, 433, 366, 665, 412, 9901, 13], "temperature": 0.0, "avg_logprob": -0.15296291443238774, "compression_ratio": 1.5095238095238095, "no_speech_prob": 3.491202005534433e-05}, {"id": 723, "seek": 256950, "start": 2578.5, "end": 2585.5, "text": " So the thing is, yeah, what I'm saying here, it sounds a bit like floss and eat your veggies.", "tokens": [407, 264, 551, 307, 11, 1338, 11, 437, 286, 478, 1566, 510, 11, 309, 3263, 257, 857, 411, 49697, 293, 1862, 428, 27889, 13], "temperature": 0.0, "avg_logprob": -0.15296291443238774, "compression_ratio": 1.5095238095238095, "no_speech_prob": 3.491202005534433e-05}, {"id": 724, "seek": 256950, "start": 2585.5, "end": 2589.5, "text": " Yeah, we probably all have had some experience with labeling data.", "tokens": [865, 11, 321, 1391, 439, 362, 632, 512, 1752, 365, 40244, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15296291443238774, "compression_ratio": 1.5095238095238095, "no_speech_prob": 3.491202005534433e-05}, {"id": 725, "seek": 258950, "start": 2589.5, "end": 2599.5, "text": " And normally, yeah, we also gave this talk to a crowd of like more data science focused industry professionals.", "tokens": [400, 5646, 11, 1338, 11, 321, 611, 2729, 341, 751, 281, 257, 6919, 295, 411, 544, 1412, 3497, 5178, 3518, 11954, 13], "temperature": 0.0, "avg_logprob": -0.15178052840694303, "compression_ratio": 1.5583756345177664, "no_speech_prob": 6.142211350379512e-05}, {"id": 726, "seek": 258950, "start": 2599.5, "end": 2611.5, "text": " And actually, you'd be surprised how many companies we talk to, also very large companies, very actually technologically sophisticated companies that mostly use Excel spreadsheets for everything.", "tokens": [400, 767, 11, 291, 1116, 312, 6100, 577, 867, 3431, 321, 751, 281, 11, 611, 588, 2416, 3431, 11, 588, 767, 1537, 17157, 16950, 3431, 300, 5240, 764, 19060, 23651, 1385, 337, 1203, 13], "temperature": 0.0, "avg_logprob": -0.15178052840694303, "compression_ratio": 1.5583756345177664, "no_speech_prob": 6.142211350379512e-05}, {"id": 727, "seek": 261150, "start": 2611.5, "end": 2619.5, "text": " And it's not inherently bad, but they are very obvious problems with Excel spreadsheets and there's definitely a lot of room for improvement.", "tokens": [400, 309, 311, 406, 27993, 1578, 11, 457, 436, 366, 588, 6322, 2740, 365, 19060, 23651, 1385, 293, 456, 311, 2138, 257, 688, 295, 1808, 337, 10444, 13], "temperature": 0.0, "avg_logprob": -0.08614179611206055, "compression_ratio": 1.5905797101449275, "no_speech_prob": 7.730781362624839e-05}, {"id": 728, "seek": 261150, "start": 2619.5, "end": 2626.5, "text": " So once people figure this out and realize that maybe they could do something better or it's just terrible, like we don't want to do this.", "tokens": [407, 1564, 561, 2573, 341, 484, 293, 4325, 300, 1310, 436, 727, 360, 746, 1101, 420, 309, 311, 445, 6237, 11, 411, 321, 500, 380, 528, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.08614179611206055, "compression_ratio": 1.5905797101449275, "no_speech_prob": 7.730781362624839e-05}, {"id": 729, "seek": 261150, "start": 2626.5, "end": 2640.5, "text": " The next move is normally let's move this all out to Mechanical Turk or some other crowdsourced platform and Mechanical Turk, the Amazon cloud of human labor.", "tokens": [440, 958, 1286, 307, 5646, 718, 311, 1286, 341, 439, 484, 281, 30175, 804, 15714, 420, 512, 661, 26070, 396, 1232, 3663, 293, 30175, 804, 15714, 11, 264, 6795, 4588, 295, 1952, 5938, 13], "temperature": 0.0, "avg_logprob": -0.08614179611206055, "compression_ratio": 1.5905797101449275, "no_speech_prob": 7.730781362624839e-05}, {"id": 730, "seek": 264050, "start": 2640.5, "end": 2645.5, "text": " And so, yeah, people do that. And often then I also surprised that the results are not very good.", "tokens": [400, 370, 11, 1338, 11, 561, 360, 300, 13, 400, 2049, 550, 286, 611, 6100, 300, 264, 3542, 366, 406, 588, 665, 13], "temperature": 0.0, "avg_logprob": -0.1108101637467094, "compression_ratio": 1.7692307692307692, "no_speech_prob": 6.58881472190842e-05}, {"id": 731, "seek": 264050, "start": 2645.5, "end": 2653.5, "text": " And the problem is, yeah, OK, so you have some some guy do it for five dollars an hour, get the data back, train your model doesn't work.", "tokens": [400, 264, 1154, 307, 11, 1338, 11, 2264, 11, 370, 291, 362, 512, 512, 2146, 360, 309, 337, 1732, 3808, 364, 1773, 11, 483, 264, 1412, 646, 11, 3847, 428, 2316, 1177, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.1108101637467094, "compression_ratio": 1.7692307692307692, "no_speech_prob": 6.58881472190842e-05}, {"id": 732, "seek": 264050, "start": 2653.5, "end": 2658.5, "text": " And actually, it's very difficult to then retroactively find out what the problem was.", "tokens": [400, 767, 11, 309, 311, 588, 2252, 281, 550, 18820, 45679, 915, 484, 437, 264, 1154, 390, 13], "temperature": 0.0, "avg_logprob": -0.1108101637467094, "compression_ratio": 1.7692307692307692, "no_speech_prob": 6.58881472190842e-05}, {"id": 733, "seek": 264050, "start": 2658.5, "end": 2668.5, "text": " Maybe your label scheme was bad. Maybe your idea was bad. Maybe the data was bad. Maybe you didn't write your annotation manual properly.", "tokens": [2704, 428, 7645, 12232, 390, 1578, 13, 2704, 428, 1558, 390, 1578, 13, 2704, 264, 1412, 390, 1578, 13, 2704, 291, 994, 380, 2464, 428, 48654, 9688, 6108, 13], "temperature": 0.0, "avg_logprob": -0.1108101637467094, "compression_ratio": 1.7692307692307692, "no_speech_prob": 6.58881472190842e-05}, {"id": 734, "seek": 266850, "start": 2668.5, "end": 2675.5, "text": " Maybe actually, yeah, another another nice thing. Maybe you pay too much because, you know, if you pay too much on Mechanical Turk, you attract all the bad actors.", "tokens": [2704, 767, 11, 1338, 11, 1071, 1071, 1481, 551, 13, 2704, 291, 1689, 886, 709, 570, 11, 291, 458, 11, 498, 291, 1689, 886, 709, 322, 30175, 804, 15714, 11, 291, 5049, 439, 264, 1578, 10037, 13], "temperature": 0.0, "avg_logprob": -0.14383080909992085, "compression_ratio": 1.6764705882352942, "no_speech_prob": 1.2198308468214236e-05}, {"id": 735, "seek": 266850, "start": 2675.5, "end": 2681.5, "text": " So you kind of have to stick to the like half of half minimum wage. So that could have been a problem.", "tokens": [407, 291, 733, 295, 362, 281, 2897, 281, 264, 411, 1922, 295, 1922, 7285, 15444, 13, 407, 300, 727, 362, 668, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.14383080909992085, "compression_ratio": 1.6764705882352942, "no_speech_prob": 1.2198308468214236e-05}, {"id": 736, "seek": 266850, "start": 2681.5, "end": 2686.5, "text": " You want maybe your model was bad. Your training code was bad. It's very, very difficult to find that out.", "tokens": [509, 528, 1310, 428, 2316, 390, 1578, 13, 2260, 3097, 3089, 390, 1578, 13, 467, 311, 588, 11, 588, 2252, 281, 915, 300, 484, 13], "temperature": 0.0, "avg_logprob": -0.14383080909992085, "compression_ratio": 1.6764705882352942, "no_speech_prob": 1.2198308468214236e-05}, {"id": 737, "seek": 266850, "start": 2686.5, "end": 2691.5, "text": " And also you realize that, well, it's not really just a cheap click work like you.", "tokens": [400, 611, 291, 4325, 300, 11, 731, 11, 309, 311, 406, 534, 445, 257, 7084, 2052, 589, 411, 291, 13], "temperature": 0.0, "avg_logprob": -0.14383080909992085, "compression_ratio": 1.6764705882352942, "no_speech_prob": 1.2198308468214236e-05}, {"id": 738, "seek": 269150, "start": 2691.5, "end": 2699.5, "text": " You know, you need to do it more. So then, yeah, what most people conclude from this is, fuck this labeling in general.", "tokens": [509, 458, 11, 291, 643, 281, 360, 309, 544, 13, 407, 550, 11, 1338, 11, 437, 881, 561, 16886, 490, 341, 307, 11, 3275, 341, 40244, 294, 2674, 13], "temperature": 0.0, "avg_logprob": -0.10263230583884499, "compression_ratio": 1.6446886446886446, "no_speech_prob": 0.00013958029740024358}, {"id": 739, "seek": 269150, "start": 2699.5, "end": 2705.5, "text": " I don't want to do this anymore. Let's just find some unsupervised method and like not bother with this.", "tokens": [286, 500, 380, 528, 281, 360, 341, 3602, 13, 961, 311, 445, 915, 512, 2693, 12879, 24420, 3170, 293, 411, 406, 8677, 365, 341, 13], "temperature": 0.0, "avg_logprob": -0.10263230583884499, "compression_ratio": 1.6446886446886446, "no_speech_prob": 0.00013958029740024358}, {"id": 740, "seek": 269150, "start": 2705.5, "end": 2712.5, "text": " And that's actually also a conversation I had recently where we talked to a larger media company and they'd done exactly that.", "tokens": [400, 300, 311, 767, 611, 257, 3761, 286, 632, 3938, 689, 321, 2825, 281, 257, 4833, 3021, 2237, 293, 436, 1116, 1096, 2293, 300, 13], "temperature": 0.0, "avg_logprob": -0.10263230583884499, "compression_ratio": 1.6446886446886446, "no_speech_prob": 0.00013958029740024358}, {"id": 741, "seek": 269150, "start": 2712.5, "end": 2717.5, "text": " And now they have a few hundred clusters. And it's really great. They have really great clusters.", "tokens": [400, 586, 436, 362, 257, 1326, 3262, 23313, 13, 400, 309, 311, 534, 869, 13, 814, 362, 534, 869, 23313, 13], "temperature": 0.0, "avg_logprob": -0.10263230583884499, "compression_ratio": 1.6446886446886446, "no_speech_prob": 0.00013958029740024358}, {"id": 742, "seek": 271750, "start": 2717.5, "end": 2723.5, "text": " But now their problem is that they have no idea what these clusters are. So they now need to label their clusters.", "tokens": [583, 586, 641, 1154, 307, 300, 436, 362, 572, 1558, 437, 613, 23313, 366, 13, 407, 436, 586, 643, 281, 7645, 641, 23313, 13], "temperature": 0.0, "avg_logprob": -0.11424625836885892, "compression_ratio": 1.8034934497816595, "no_speech_prob": 2.770517676253803e-05}, {"id": 743, "seek": 271750, "start": 2723.5, "end": 2732.5, "text": " And now they're kind of back in the beginning. And I think what we see from this is that the label data itself, the fact that we need label data, that's an opportunity.", "tokens": [400, 586, 436, 434, 733, 295, 646, 294, 264, 2863, 13, 400, 286, 519, 437, 321, 536, 490, 341, 307, 300, 264, 7645, 1412, 2564, 11, 264, 1186, 300, 321, 643, 7645, 1412, 11, 300, 311, 364, 2650, 13], "temperature": 0.0, "avg_logprob": -0.11424625836885892, "compression_ratio": 1.8034934497816595, "no_speech_prob": 2.770517676253803e-05}, {"id": 744, "seek": 271750, "start": 2732.5, "end": 2740.5, "text": " That's not the problem. The problem is how we do it. And yeah, so then there are a few like we've been thinking about this a lot.", "tokens": [663, 311, 406, 264, 1154, 13, 440, 1154, 307, 577, 321, 360, 309, 13, 400, 1338, 11, 370, 550, 456, 366, 257, 1326, 411, 321, 600, 668, 1953, 466, 341, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.11424625836885892, "compression_ratio": 1.8034934497816595, "no_speech_prob": 2.770517676253803e-05}, {"id": 745, "seek": 274050, "start": 2740.5, "end": 2747.5, "text": " And there are at least, yeah, from our point of view, there are a lot of things we could do better.", "tokens": [400, 456, 366, 412, 1935, 11, 1338, 11, 490, 527, 935, 295, 1910, 11, 456, 366, 257, 688, 295, 721, 321, 727, 360, 1101, 13], "temperature": 0.0, "avg_logprob": -0.12058794176256335, "compression_ratio": 1.585, "no_speech_prob": 5.639046139549464e-05}, {"id": 746, "seek": 274050, "start": 2747.5, "end": 2762.5, "text": " So one of the things really to work against this problem that we have caused by us being human is that we should we need to break down these very complex things we're asking the humans into smaller, simpler questions.", "tokens": [407, 472, 295, 264, 721, 534, 281, 589, 1970, 341, 1154, 300, 321, 362, 7008, 538, 505, 885, 1952, 307, 300, 321, 820, 321, 643, 281, 1821, 760, 613, 588, 3997, 721, 321, 434, 3365, 264, 6255, 666, 4356, 11, 18587, 1651, 13], "temperature": 0.0, "avg_logprob": -0.12058794176256335, "compression_ratio": 1.585, "no_speech_prob": 5.639046139549464e-05}, {"id": 747, "seek": 276250, "start": 2762.5, "end": 2777.5, "text": " And really, these should be binary decisions so we can have a much better annotation speed because we can move through the things faster and we can also measure the reliability much easier than if we ask people open questions because we can actually say, OK, do our annotators agree?", "tokens": [400, 534, 11, 613, 820, 312, 17434, 5327, 370, 321, 393, 362, 257, 709, 1101, 48654, 3073, 570, 321, 393, 1286, 807, 264, 721, 4663, 293, 321, 393, 611, 3481, 264, 24550, 709, 3571, 813, 498, 321, 1029, 561, 1269, 1651, 570, 321, 393, 767, 584, 11, 2264, 11, 360, 527, 25339, 3391, 3986, 30], "temperature": 0.0, "avg_logprob": -0.12555477231047873, "compression_ratio": 1.6352459016393444, "no_speech_prob": 3.758179082069546e-05}, {"id": 748, "seek": 276250, "start": 2777.5, "end": 2784.5, "text": " Do they not agree? Because that's in the end very important to find out whether we've collected data the right way.", "tokens": [1144, 436, 406, 3986, 30, 1436, 300, 311, 294, 264, 917, 588, 1021, 281, 915, 484, 1968, 321, 600, 11087, 1412, 264, 558, 636, 13], "temperature": 0.0, "avg_logprob": -0.12555477231047873, "compression_ratio": 1.6352459016393444, "no_speech_prob": 3.758179082069546e-05}, {"id": 749, "seek": 278450, "start": 2784.5, "end": 2798.5, "text": " The binary thing itself, it sounds very it sounds a bit radical, but actually if you think about it, most or pretty much any task can be broken down into a sequence of binary decisions like yes or no decisions.", "tokens": [440, 17434, 551, 2564, 11, 309, 3263, 588, 309, 3263, 257, 857, 12001, 11, 457, 767, 498, 291, 519, 466, 309, 11, 881, 420, 1238, 709, 604, 5633, 393, 312, 5463, 760, 666, 257, 8310, 295, 17434, 5327, 411, 2086, 420, 572, 5327, 13], "temperature": 0.0, "avg_logprob": -0.15409687889946833, "compression_ratio": 1.6638297872340426, "no_speech_prob": 3.3136358979390934e-05}, {"id": 750, "seek": 278450, "start": 2798.5, "end": 2807.5, "text": " It might mean that we have to accept that, OK, and if we annotating a sentence or entities, we won't actually end up with a gold standard with gold standard data for this sentence.", "tokens": [467, 1062, 914, 300, 321, 362, 281, 3241, 300, 11, 2264, 11, 293, 498, 321, 25339, 990, 257, 8174, 420, 16667, 11, 321, 1582, 380, 767, 917, 493, 365, 257, 3821, 3832, 365, 3821, 3832, 1412, 337, 341, 8174, 13], "temperature": 0.0, "avg_logprob": -0.15409687889946833, "compression_ratio": 1.6638297872340426, "no_speech_prob": 3.3136358979390934e-05}, {"id": 751, "seek": 280750, "start": 2807.5, "end": 2819.5, "text": " We might actually end up with only partially annotated data and have to deal with that. But as it but still we actually able to use our human's time more efficiently, which is often much more important.", "tokens": [492, 1062, 767, 917, 493, 365, 787, 18886, 25339, 770, 1412, 293, 362, 281, 2028, 365, 300, 13, 583, 382, 309, 457, 920, 321, 767, 1075, 281, 764, 527, 1952, 311, 565, 544, 19621, 11, 597, 307, 2049, 709, 544, 1021, 13], "temperature": 0.0, "avg_logprob": -0.1947995976703923, "compression_ratio": 1.5829596412556053, "no_speech_prob": 6.285474228207022e-05}, {"id": 752, "seek": 280750, "start": 2819.5, "end": 2831.5, "text": " So a lot of that love the samples I'm going to show you now are from using our annotation tool prodigy, which we started building as an internal tool.", "tokens": [407, 257, 688, 295, 300, 959, 264, 10938, 286, 478, 516, 281, 855, 291, 586, 366, 490, 1228, 527, 48654, 2290, 15792, 328, 88, 11, 597, 321, 1409, 2390, 382, 364, 6920, 2290, 13], "temperature": 0.0, "avg_logprob": -0.1947995976703923, "compression_ratio": 1.5829596412556053, "no_speech_prob": 6.285474228207022e-05}, {"id": 753, "seek": 283150, "start": 2831.5, "end": 2837.5, "text": " But we very quickly realized that OK, this is really something pretty much every company we talk to most users we talk to.", "tokens": [583, 321, 588, 2661, 5334, 300, 2264, 11, 341, 307, 534, 746, 1238, 709, 633, 2237, 321, 751, 281, 881, 5022, 321, 751, 281, 13], "temperature": 0.0, "avg_logprob": -0.13663501155619717, "compression_ratio": 1.7318007662835249, "no_speech_prob": 0.00016498677723575383}, {"id": 754, "seek": 283150, "start": 2837.5, "end": 2860.5, "text": " This was always something that kept coming up. So we thought, OK, what if we really combine all these ideas we already have and how to train a model actually use the technology we're working with within the tool and also use the insights we have from user experience and how to get how to get humans to do stuff most efficiently.", "tokens": [639, 390, 1009, 746, 300, 4305, 1348, 493, 13, 407, 321, 1194, 11, 2264, 11, 437, 498, 321, 534, 10432, 439, 613, 3487, 321, 1217, 362, 293, 577, 281, 3847, 257, 2316, 767, 764, 264, 2899, 321, 434, 1364, 365, 1951, 264, 2290, 293, 611, 764, 264, 14310, 321, 362, 490, 4195, 1752, 293, 577, 281, 483, 577, 281, 483, 6255, 281, 360, 1507, 881, 19621, 13], "temperature": 0.0, "avg_logprob": -0.13663501155619717, "compression_ratio": 1.7318007662835249, "no_speech_prob": 0.00016498677723575383}, {"id": 755, "seek": 286050, "start": 2860.5, "end": 2871.5, "text": " How to get humans excited actually even how to the whole idea of gamification, how to get humans to really stick to doing something and put this all into one tool.", "tokens": [1012, 281, 483, 6255, 2919, 767, 754, 577, 281, 264, 1379, 1558, 295, 8019, 3774, 11, 577, 281, 483, 6255, 281, 534, 2897, 281, 884, 746, 293, 829, 341, 439, 666, 472, 2290, 13], "temperature": 0.0, "avg_logprob": -0.12206484142102693, "compression_ratio": 1.61139896373057, "no_speech_prob": 9.574431169312447e-05}, {"id": 756, "seek": 286050, "start": 2871.5, "end": 2883.5, "text": " And that's that's prodigy. And so here we see some some examples of those tasks and how to you know how we can present things in a more binary way.", "tokens": [400, 300, 311, 300, 311, 15792, 328, 88, 13, 400, 370, 510, 321, 536, 512, 512, 5110, 295, 729, 9608, 293, 577, 281, 291, 458, 577, 321, 393, 1974, 721, 294, 257, 544, 17434, 636, 13], "temperature": 0.0, "avg_logprob": -0.12206484142102693, "compression_ratio": 1.61139896373057, "no_speech_prob": 9.574431169312447e-05}, {"id": 757, "seek": 288350, "start": 2883.5, "end": 2893.5, "text": " So in the top left we have a named entity task. So here this is this comes from Reddit and we're labeling whether something is a product or not.", "tokens": [407, 294, 264, 1192, 1411, 321, 362, 257, 4926, 13977, 5633, 13, 407, 510, 341, 307, 341, 1487, 490, 32210, 293, 321, 434, 40244, 1968, 746, 307, 257, 1674, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.14147610731527838, "compression_ratio": 1.5625, "no_speech_prob": 0.0002395882474957034}, {"id": 758, "seek": 288350, "start": 2893.5, "end": 2902.5, "text": " And what we did here is we load in a spacey model, ask the model to label the products and then we look at them and say yes or no.", "tokens": [400, 437, 321, 630, 510, 307, 321, 3677, 294, 257, 1901, 88, 2316, 11, 1029, 264, 2316, 281, 7645, 264, 3383, 293, 550, 321, 574, 412, 552, 293, 584, 2086, 420, 572, 13], "temperature": 0.0, "avg_logprob": -0.14147610731527838, "compression_ratio": 1.5625, "no_speech_prob": 0.0002395882474957034}, {"id": 759, "seek": 290250, "start": 2902.5, "end": 2914.5, "text": " We could even we can also use a mode where we can where we can then actually click on this, remove this, label something else. But still you see, OK, we don't have to we don't have to do this in an Excel spreadsheet.", "tokens": [492, 727, 754, 321, 393, 611, 764, 257, 4391, 689, 321, 393, 689, 321, 393, 550, 767, 2052, 322, 341, 11, 4159, 341, 11, 7645, 746, 1646, 13, 583, 920, 291, 536, 11, 2264, 11, 321, 500, 380, 362, 281, 321, 500, 380, 362, 281, 360, 341, 294, 364, 19060, 27733, 13], "temperature": 0.0, "avg_logprob": -0.1378129524520681, "compression_ratio": 1.6335078534031413, "no_speech_prob": 0.000231469253776595}, {"id": 760, "seek": 290250, "start": 2914.5, "end": 2922.5, "text": " We actually get one question. We look at this and pretty much immediately we can say yes or no.", "tokens": [492, 767, 483, 472, 1168, 13, 492, 574, 412, 341, 293, 1238, 709, 4258, 321, 393, 584, 2086, 420, 572, 13], "temperature": 0.0, "avg_logprob": -0.1378129524520681, "compression_ratio": 1.6335078534031413, "no_speech_prob": 0.000231469253776595}, {"id": 761, "seek": 292250, "start": 2922.5, "end": 2933.5, "text": " The same here on the right there we're using I think this is actually a real example using the YOLO2 model with the default categories and we have an image of a skateboard.", "tokens": [440, 912, 510, 322, 264, 558, 456, 321, 434, 1228, 286, 519, 341, 307, 767, 257, 957, 1365, 1228, 264, 398, 5046, 46, 17, 2316, 365, 264, 7576, 10479, 293, 321, 362, 364, 3256, 295, 257, 32204, 13], "temperature": 0.0, "avg_logprob": -0.16012418270111084, "compression_ratio": 1.3974358974358974, "no_speech_prob": 4.567455835058354e-05}, {"id": 762, "seek": 292250, "start": 2933.5, "end": 2937.5, "text": " We could say is this a skateboard. Yes or no.", "tokens": [492, 727, 584, 307, 341, 257, 32204, 13, 1079, 420, 572, 13], "temperature": 0.0, "avg_logprob": -0.16012418270111084, "compression_ratio": 1.3974358974358974, "no_speech_prob": 4.567455835058354e-05}, {"id": 763, "seek": 293750, "start": 2937.5, "end": 2960.5, "text": " We immediately have our annotations here and even this one in the corner even if we if we're not able to really break it down into a true binary task we can still make it more efficient and easier for a human to answer because here with keyboard shortcuts you can still do maybe two, three seconds per annotation and you have an answer.", "tokens": [492, 4258, 362, 527, 25339, 763, 510, 293, 754, 341, 472, 294, 264, 4538, 754, 498, 321, 498, 321, 434, 406, 1075, 281, 534, 1821, 309, 760, 666, 257, 2074, 17434, 5633, 321, 393, 920, 652, 309, 544, 7148, 293, 3571, 337, 257, 1952, 281, 1867, 570, 510, 365, 10186, 34620, 291, 393, 920, 360, 1310, 732, 11, 1045, 3949, 680, 48654, 293, 291, 362, 364, 1867, 13], "temperature": 0.0, "avg_logprob": -0.16174129645029703, "compression_ratio": 1.6390243902439023, "no_speech_prob": 7.343896868405864e-05}, {"id": 764, "seek": 296050, "start": 2960.5, "end": 2976.5, "text": " Or we say hey it's actually so fast if we can get to one second we might as well label our entire corpus twice positive negative other labels we want to do and just move move through it quicker.", "tokens": [1610, 321, 584, 4177, 309, 311, 767, 370, 2370, 498, 321, 393, 483, 281, 472, 1150, 321, 1062, 382, 731, 7645, 527, 2302, 1181, 31624, 6091, 3353, 3671, 661, 16949, 321, 528, 281, 360, 293, 445, 1286, 1286, 807, 309, 16255, 13], "temperature": 0.0, "avg_logprob": -0.15762100721660413, "compression_ratio": 1.5621890547263682, "no_speech_prob": 0.00014476804062724113}, {"id": 765, "seek": 296050, "start": 2976.5, "end": 2985.5, "text": " And yet to give you some background on like what why did we do this what do we what do we think Prodigy should achieve.", "tokens": [400, 1939, 281, 976, 291, 512, 3678, 322, 411, 437, 983, 630, 321, 360, 341, 437, 360, 321, 437, 360, 321, 519, 1705, 25259, 88, 820, 4584, 13], "temperature": 0.0, "avg_logprob": -0.15762100721660413, "compression_ratio": 1.5621890547263682, "no_speech_prob": 0.00014476804062724113}, {"id": 766, "seek": 298550, "start": 2985.5, "end": 3003.5, "text": " We really think that okay we we want to be able to make annotation so efficient that data scientists can do it themselves or hear what we call data scientists can also be researchers and people working with the data people training the models like it's it's still reading it like that it still doesn't sound like fun.", "tokens": [492, 534, 519, 300, 1392, 321, 321, 528, 281, 312, 1075, 281, 652, 48654, 370, 7148, 300, 1412, 7708, 393, 360, 309, 2969, 420, 1568, 437, 321, 818, 1412, 7708, 393, 611, 312, 10309, 293, 561, 1364, 365, 264, 1412, 561, 3097, 264, 5245, 411, 309, 311, 309, 311, 920, 3760, 309, 411, 300, 309, 920, 1177, 380, 1626, 411, 1019, 13], "temperature": 0.0, "avg_logprob": -0.1554459658536044, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.00018265267135575414}, {"id": 767, "seek": 300350, "start": 3003.5, "end": 3015.5, "text": " But the idea is you know we could really make it a process that's efficient that you actually really want to do this because you don't have to depend on anyone else. You can just get the job done job done and see whether your idea works or not.", "tokens": [583, 264, 1558, 307, 291, 458, 321, 727, 534, 652, 309, 257, 1399, 300, 311, 7148, 300, 291, 767, 534, 528, 281, 360, 341, 570, 291, 500, 380, 362, 281, 5672, 322, 2878, 1646, 13, 509, 393, 445, 483, 264, 1691, 1096, 1691, 1096, 293, 536, 1968, 428, 1558, 1985, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.12047600500362436, "compression_ratio": 1.7844827586206897, "no_speech_prob": 0.00016819857410155237}, {"id": 768, "seek": 300350, "start": 3015.5, "end": 3023.5, "text": " And the same. Yeah. And this also means you can iterate faster. We're very used to okay you iterate on your code but you can actually iterate on your code and your data.", "tokens": [400, 264, 912, 13, 865, 13, 400, 341, 611, 1355, 291, 393, 44497, 4663, 13, 492, 434, 588, 1143, 281, 1392, 291, 44497, 322, 428, 3089, 457, 291, 393, 767, 44497, 322, 428, 3089, 293, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12047600500362436, "compression_ratio": 1.7844827586206897, "no_speech_prob": 0.00016819857410155237}, {"id": 769, "seek": 302350, "start": 3023.5, "end": 3033.5, "text": " You try something out doesn't work try something else. Maybe see okay is it going to work if I collect more annotations you can all try this out.", "tokens": [509, 853, 746, 484, 1177, 380, 589, 853, 746, 1646, 13, 2704, 536, 1392, 307, 309, 516, 281, 589, 498, 286, 2500, 544, 25339, 763, 291, 393, 439, 853, 341, 484, 13], "temperature": 0.0, "avg_logprob": -0.10298576870480099, "compression_ratio": 1.5971563981042654, "no_speech_prob": 7.253461808431894e-05}, {"id": 770, "seek": 302350, "start": 3033.5, "end": 3045.5, "text": " And we also want to waste as little time as possible and use what the model already knows and have the human correct its predictions instead of just having a human do everything from scratch.", "tokens": [400, 321, 611, 528, 281, 5964, 382, 707, 565, 382, 1944, 293, 764, 437, 264, 2316, 1217, 3255, 293, 362, 264, 1952, 3006, 1080, 21264, 2602, 295, 445, 1419, 257, 1952, 360, 1203, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.10298576870480099, "compression_ratio": 1.5971563981042654, "no_speech_prob": 7.253461808431894e-05}, {"id": 771, "seek": 304550, "start": 3045.5, "end": 3057.5, "text": " And yeah as a library itself we really want prodigy to fit into the Python ecosystem. We want it to be customizable extensible in Python. You can write scripts for it.", "tokens": [400, 1338, 382, 257, 6405, 2564, 321, 534, 528, 15792, 328, 88, 281, 3318, 666, 264, 15329, 11311, 13, 492, 528, 309, 281, 312, 47922, 1279, 30633, 294, 15329, 13, 509, 393, 2464, 23294, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.12280793025575835, "compression_ratio": 1.5603448275862069, "no_speech_prob": 5.648086153087206e-05}, {"id": 772, "seek": 304550, "start": 3057.5, "end": 3064.5, "text": " And we also it was a very conscious decision not to make it a source tool because we think data privacy is important.", "tokens": [400, 321, 611, 309, 390, 257, 588, 6648, 3537, 406, 281, 652, 309, 257, 4009, 2290, 570, 321, 519, 1412, 11427, 307, 1021, 13], "temperature": 0.0, "avg_logprob": -0.12280793025575835, "compression_ratio": 1.5603448275862069, "no_speech_prob": 5.648086153087206e-05}, {"id": 773, "seek": 304550, "start": 3064.5, "end": 3068.5, "text": " You don't you shouldn't have to send your text to our service for no reason.", "tokens": [509, 500, 380, 291, 4659, 380, 362, 281, 2845, 428, 2487, 281, 527, 2643, 337, 572, 1778, 13], "temperature": 0.0, "avg_logprob": -0.12280793025575835, "compression_ratio": 1.5603448275862069, "no_speech_prob": 5.648086153087206e-05}, {"id": 774, "seek": 306850, "start": 3068.5, "end": 3080.5, "text": " And we also think you should you shouldn't be locked in like you should get a JSON format out that you can use to train your models however you like and not our random format that you can then download from our service.", "tokens": [400, 321, 611, 519, 291, 820, 291, 4659, 380, 312, 9376, 294, 411, 291, 820, 483, 257, 31828, 7877, 484, 300, 291, 393, 764, 281, 3847, 428, 5245, 4461, 291, 411, 293, 406, 527, 4974, 7877, 300, 291, 393, 550, 5484, 490, 527, 2643, 13], "temperature": 0.0, "avg_logprob": -0.11677701132638114, "compression_ratio": 1.5638297872340425, "no_speech_prob": 5.526756285689771e-05}, {"id": 775, "seek": 306850, "start": 3080.5, "end": 3086.5, "text": " So that's where we're going with prodigy and just here's this very simple.", "tokens": [407, 300, 311, 689, 321, 434, 516, 365, 15792, 328, 88, 293, 445, 510, 311, 341, 588, 2199, 13], "temperature": 0.0, "avg_logprob": -0.11677701132638114, "compression_ratio": 1.5638297872340425, "no_speech_prob": 5.526756285689771e-05}, {"id": 776, "seek": 308650, "start": 3086.5, "end": 3102.5, "text": " Illustration of how the app looks the center recipes which are very simple Python scripts that orchestrate the whole thing you have a rest API that communicates with web app naturally so you can see things on the screen.", "tokens": [37788, 2405, 295, 577, 264, 724, 1542, 264, 3056, 13035, 597, 366, 588, 2199, 15329, 23294, 300, 14161, 4404, 264, 1379, 551, 291, 362, 257, 1472, 9362, 300, 3363, 1024, 365, 3670, 724, 8195, 370, 291, 393, 536, 721, 322, 264, 2568, 13], "temperature": 0.0, "avg_logprob": -0.13972020918323147, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.00012999391765333712}, {"id": 777, "seek": 308650, "start": 3102.5, "end": 3106.5, "text": " You have your data that's coming in with just text images.", "tokens": [509, 362, 428, 1412, 300, 311, 1348, 294, 365, 445, 2487, 5267, 13], "temperature": 0.0, "avg_logprob": -0.13972020918323147, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.00012999391765333712}, {"id": 778, "seek": 310650, "start": 3106.5, "end": 3117.5, "text": " And you can have an optional model state that's updated in a loop. If you want that and then you have the model then communicates with the recipe.", "tokens": [400, 291, 393, 362, 364, 17312, 2316, 1785, 300, 311, 10588, 294, 257, 6367, 13, 759, 291, 528, 300, 293, 550, 291, 362, 264, 2316, 550, 3363, 1024, 365, 264, 6782, 13], "temperature": 0.0, "avg_logprob": -0.16002291791579304, "compression_ratio": 1.7411764705882353, "no_speech_prob": 5.4658343287883326e-05}, {"id": 779, "seek": 310650, "start": 3117.5, "end": 3129.5, "text": " You can ask the user annotates it's updated in a loop and can suggest more annotations that are more compatible with the annotators recent decisions.", "tokens": [509, 393, 1029, 264, 4195, 25339, 1024, 309, 311, 10588, 294, 257, 6367, 293, 393, 3402, 544, 25339, 763, 300, 366, 544, 18218, 365, 264, 25339, 3391, 5162, 5327, 13], "temperature": 0.0, "avg_logprob": -0.16002291791579304, "compression_ratio": 1.7411764705882353, "no_speech_prob": 5.4658343287883326e-05}, {"id": 780, "seek": 312950, "start": 3129.5, "end": 3138.5, "text": " And yeah we also there's a database and a command line interface so you can actually use it efficiently and don't have to worry about these aspects.", "tokens": [400, 1338, 321, 611, 456, 311, 257, 8149, 293, 257, 5622, 1622, 9226, 370, 291, 393, 767, 764, 309, 19621, 293, 500, 380, 362, 281, 3292, 466, 613, 7270, 13], "temperature": 0.0, "avg_logprob": -0.10795853688166691, "compression_ratio": 1.5871559633027523, "no_speech_prob": 5.4373816965380684e-05}, {"id": 781, "seek": 312950, "start": 3138.5, "end": 3147.5, "text": " So here can you see yeah in the corner we have a simple example of a recipe function which really is just a Python function.", "tokens": [407, 510, 393, 291, 536, 1338, 294, 264, 4538, 321, 362, 257, 2199, 1365, 295, 257, 6782, 2445, 597, 534, 307, 445, 257, 15329, 2445, 13], "temperature": 0.0, "avg_logprob": -0.10795853688166691, "compression_ratio": 1.5871559633027523, "no_speech_prob": 5.4373816965380684e-05}, {"id": 782, "seek": 312950, "start": 3147.5, "end": 3152.5, "text": " You load your data in and then you return this dictionary of components.", "tokens": [509, 3677, 428, 1412, 294, 293, 550, 291, 2736, 341, 25890, 295, 6677, 13], "temperature": 0.0, "avg_logprob": -0.10795853688166691, "compression_ratio": 1.5871559633027523, "no_speech_prob": 5.4373816965380684e-05}, {"id": 783, "seek": 315250, "start": 3152.5, "end": 3162.5, "text": " For example an ID of the data set how to store your data a stream of examples you can pass in callbacks to update your model things to execute before the thing starts.", "tokens": [1171, 1365, 364, 7348, 295, 264, 1412, 992, 577, 281, 3531, 428, 1412, 257, 4309, 295, 5110, 291, 393, 1320, 294, 818, 17758, 281, 5623, 428, 2316, 721, 281, 14483, 949, 264, 551, 3719, 13], "temperature": 0.0, "avg_logprob": -0.15092460202499175, "compression_ratio": 1.6839080459770115, "no_speech_prob": 0.00013789630611427128}, {"id": 784, "seek": 315250, "start": 3162.5, "end": 3172.5, "text": " So the idea is really okay if you can if you need to load something in if you can write that in Python you can do it in part.", "tokens": [407, 264, 1558, 307, 534, 1392, 498, 291, 393, 498, 291, 643, 281, 3677, 746, 294, 498, 291, 393, 2464, 300, 294, 15329, 291, 393, 360, 309, 294, 644, 13], "temperature": 0.0, "avg_logprob": -0.15092460202499175, "compression_ratio": 1.6839080459770115, "no_speech_prob": 0.00013789630611427128}, {"id": 785, "seek": 317250, "start": 3172.5, "end": 3184.5, "text": " And you can also we provide a bunch of pre built in recipes for different tasks with some ideas of how we think it could you know it could work like named entity recognition.", "tokens": [400, 291, 393, 611, 321, 2893, 257, 3840, 295, 659, 3094, 294, 13035, 337, 819, 9608, 365, 512, 3487, 295, 577, 321, 519, 309, 727, 291, 458, 309, 727, 589, 411, 4926, 13977, 11150, 13], "temperature": 0.0, "avg_logprob": -0.13527178764343262, "compression_ratio": 1.7019230769230769, "no_speech_prob": 2.595370460767299e-05}, {"id": 786, "seek": 317250, "start": 3184.5, "end": 3196.5, "text": " For example you can use the model correct its predictions you can use the model say yes or no to things you can use it for dependency parsing and look at an arc and annotate that.", "tokens": [1171, 1365, 291, 393, 764, 264, 2316, 3006, 1080, 21264, 291, 393, 764, 264, 2316, 584, 2086, 420, 572, 281, 721, 291, 393, 764, 309, 337, 33621, 21156, 278, 293, 574, 412, 364, 10346, 293, 25339, 473, 300, 13], "temperature": 0.0, "avg_logprob": -0.13527178764343262, "compression_ratio": 1.7019230769230769, "no_speech_prob": 2.595370460767299e-05}, {"id": 787, "seek": 319650, "start": 3196.5, "end": 3213.5, "text": " We have recipes that use word vectors to build terminology lists text classification so there's a lot also a lot that you can mix and match creatively like for example you have those the multiple choice example that's not really tied to any machine learning task.", "tokens": [492, 362, 13035, 300, 764, 1349, 18875, 281, 1322, 27575, 14511, 2487, 21538, 370, 456, 311, 257, 688, 611, 257, 688, 300, 291, 393, 2890, 293, 2995, 43750, 411, 337, 1365, 291, 362, 729, 264, 3866, 3922, 1365, 300, 311, 406, 534, 9601, 281, 604, 3479, 2539, 5633, 13], "temperature": 0.0, "avg_logprob": -0.13576914229482975, "compression_ratio": 1.5114942528735633, "no_speech_prob": 5.200316445552744e-05}, {"id": 788, "seek": 321350, "start": 3213.5, "end": 3228.5, "text": " But it fits pretty much into any of these workflows that you might be doing and of course the evaluation is also something we think is very very important and is often neglected especially in more industry use cases.", "tokens": [583, 309, 9001, 1238, 709, 666, 604, 295, 613, 43461, 300, 291, 1062, 312, 884, 293, 295, 1164, 264, 13344, 307, 611, 746, 321, 519, 307, 588, 588, 1021, 293, 307, 2049, 32701, 2318, 294, 544, 3518, 764, 3331, 13], "temperature": 0.0, "avg_logprob": -0.1795608667226938, "compression_ratio": 1.5786802030456852, "no_speech_prob": 6.988378299865872e-05}, {"id": 789, "seek": 321350, "start": 3228.5, "end": 3236.5, "text": " But we think there's actually a BL valuation is actually very powerful way of testing with it.", "tokens": [583, 321, 519, 456, 311, 767, 257, 15132, 38546, 307, 767, 588, 4005, 636, 295, 4997, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.1795608667226938, "compression_ratio": 1.5786802030456852, "no_speech_prob": 6.988378299865872e-05}, {"id": 790, "seek": 323650, "start": 3236.5, "end": 3243.5, "text": " So that you know your output is really what you want it to be.", "tokens": [407, 300, 291, 458, 428, 5598, 307, 534, 437, 291, 528, 309, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.2401939826675608, "compression_ratio": 1.6818181818181819, "no_speech_prob": 4.969702786183916e-05}, {"id": 791, "seek": 323650, "start": 3243.5, "end": 3260.5, "text": " And yeah, and so here we see we see an example of okay the of how you can chain different workflows together, all using models work vectors things you already have in order to get where you want to get to faster so here is simple example we want, we want to label fruit.", "tokens": [400, 1338, 11, 293, 370, 510, 321, 536, 321, 536, 364, 1365, 295, 1392, 264, 295, 577, 291, 393, 5021, 819, 43461, 1214, 11, 439, 1228, 5245, 589, 18875, 721, 291, 1217, 362, 294, 1668, 281, 483, 689, 291, 528, 281, 483, 281, 4663, 370, 510, 307, 2199, 1365, 321, 528, 11, 321, 528, 281, 7645, 6773, 13], "temperature": 0.0, "avg_logprob": -0.2401939826675608, "compression_ratio": 1.6818181818181819, "no_speech_prob": 4.969702786183916e-05}, {"id": 792, "seek": 326050, "start": 3260.5, "end": 3270.5, "text": " It's kind of a stupid example because it's that I can't think of many use cases we actually want to do that but it makes a great illustration here.", "tokens": [467, 311, 733, 295, 257, 6631, 1365, 570, 309, 311, 300, 286, 393, 380, 519, 295, 867, 764, 3331, 321, 767, 528, 281, 360, 300, 457, 309, 1669, 257, 869, 22645, 510, 13], "temperature": 0.0, "avg_logprob": -0.1604970359802246, "compression_ratio": 1.7131147540983607, "no_speech_prob": 0.00015384101425297558}, {"id": 793, "seek": 326050, "start": 3270.5, "end": 3287.5, "text": " So, yeah, we start off we say okay we want for what our fruit, we have some examples apple pear banana. That's what we can think of, and we also have word vectors that we can use that will easily give us more terms that are similar to these three fruit terms that we can", "tokens": [407, 11, 1338, 11, 321, 722, 766, 321, 584, 1392, 321, 528, 337, 437, 527, 6773, 11, 321, 362, 512, 5110, 10606, 37320, 14194, 13, 663, 311, 437, 321, 393, 519, 295, 11, 293, 321, 611, 362, 1349, 18875, 300, 321, 393, 764, 300, 486, 3612, 976, 505, 544, 2115, 300, 366, 2531, 281, 613, 1045, 6773, 2115, 300, 321, 393], "temperature": 0.0, "avg_logprob": -0.1604970359802246, "compression_ratio": 1.7131147540983607, "no_speech_prob": 0.00015384101425297558}, {"id": 794, "seek": 328750, "start": 3287.5, "end": 3297.5, "text": " use. So that's what we came up with. And then we can use this terminology list that we collected by just saying yes or no to what we've gotten out of the word vectors.", "tokens": [764, 13, 407, 300, 311, 437, 321, 1361, 493, 365, 13, 400, 550, 321, 393, 764, 341, 27575, 1329, 300, 321, 11087, 538, 445, 1566, 2086, 420, 572, 281, 437, 321, 600, 5768, 484, 295, 264, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.23235165371614344, "compression_ratio": 1.5260115606936415, "no_speech_prob": 0.00015205047384370118}, {"id": 795, "seek": 328750, "start": 3297.5, "end": 3304.5, "text": " Look at those in our data, and then say whether Apple, apples in this context is a fruit or not.", "tokens": [2053, 412, 729, 294, 527, 1412, 11, 293, 550, 584, 1968, 6373, 11, 16814, 294, 341, 4319, 307, 257, 6773, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.23235165371614344, "compression_ratio": 1.5260115606936415, "no_speech_prob": 0.00015205047384370118}, {"id": 796, "seek": 330450, "start": 3304.5, "end": 3327.5, "text": " So we still, you know, we're not just labeling all fruit terms as a fruit entity because it could be Apple the company, but we get to look at it, and it's much more efficient than if you ask the human to sit through and highlight every instance of fruit nouns in your text.", "tokens": [407, 321, 920, 11, 291, 458, 11, 321, 434, 406, 445, 40244, 439, 6773, 2115, 382, 257, 6773, 13977, 570, 309, 727, 312, 6373, 264, 2237, 11, 457, 321, 483, 281, 574, 412, 309, 11, 293, 309, 311, 709, 544, 7148, 813, 498, 291, 1029, 264, 1952, 281, 1394, 807, 293, 5078, 633, 5197, 295, 6773, 48184, 294, 428, 2487, 13], "temperature": 0.0, "avg_logprob": -0.1934415523822491, "compression_ratio": 1.4756756756756757, "no_speech_prob": 7.747227937215939e-05}, {"id": 797, "seek": 332750, "start": 3327.5, "end": 3334.5, "text": " So this this also leads to kind of one of our main.", "tokens": [407, 341, 341, 611, 6689, 281, 733, 295, 472, 295, 527, 2135, 13], "temperature": 0.0, "avg_logprob": -0.12732161415947807, "compression_ratio": 1.6080402010050252, "no_speech_prob": 5.8581394114298746e-05}, {"id": 798, "seek": 332750, "start": 3334.5, "end": 3347.5, "text": " Yeah, main aspects of the tool workflows that we especially proud of and that we think really can make a difference, which is, we can actually start by telling the computer at more abstract rules of what we're looking for, and then annotating the exceptions instead of", "tokens": [865, 11, 2135, 7270, 295, 264, 2290, 43461, 300, 321, 2318, 4570, 295, 293, 300, 321, 519, 534, 393, 652, 257, 2649, 11, 597, 307, 11, 321, 393, 767, 722, 538, 3585, 264, 3820, 412, 544, 12649, 4474, 295, 437, 321, 434, 1237, 337, 11, 293, 550, 25339, 990, 264, 22847, 2602, 295], "temperature": 0.0, "avg_logprob": -0.12732161415947807, "compression_ratio": 1.6080402010050252, "no_speech_prob": 5.8581394114298746e-05}, {"id": 799, "seek": 334750, "start": 3347.5, "end": 3357.5, "text": " really starting from scratch or we can use, we can even use the technology we're working with to build these semi automatically using word vectors using other other cool things that we can now do.", "tokens": [534, 2891, 490, 8459, 420, 321, 393, 764, 11, 321, 393, 754, 764, 264, 2899, 321, 434, 1364, 365, 281, 1322, 613, 12909, 6772, 1228, 1349, 18875, 1228, 661, 661, 1627, 721, 300, 321, 393, 586, 360, 13], "temperature": 0.0, "avg_logprob": -0.09489708215417997, "compression_ratio": 1.6699507389162562, "no_speech_prob": 8.342145883943886e-05}, {"id": 800, "seek": 334750, "start": 3357.5, "end": 3366.5, "text": " And then of course also specifically look at those examples that the model is the statistical model, we want to train is most uncertain about.", "tokens": [400, 550, 295, 1164, 611, 4682, 574, 412, 729, 5110, 300, 264, 2316, 307, 264, 22820, 2316, 11, 321, 528, 281, 3847, 307, 881, 11308, 466, 13], "temperature": 0.0, "avg_logprob": -0.09489708215417997, "compression_ratio": 1.6699507389162562, "no_speech_prob": 8.342145883943886e-05}, {"id": 801, "seek": 336650, "start": 3366.5, "end": 3383.5, "text": " So, we try to avoid the predictions, where we can be pretty sure that they correct and actually really ask the user, ask the human first about the stuff that's 5050 and we really the human feedback makes most of the difference.", "tokens": [407, 11, 321, 853, 281, 5042, 264, 21264, 11, 689, 321, 393, 312, 1238, 988, 300, 436, 3006, 293, 767, 534, 1029, 264, 4195, 11, 1029, 264, 1952, 700, 466, 264, 1507, 300, 311, 2625, 2803, 293, 321, 534, 264, 1952, 5824, 1669, 881, 295, 264, 2649, 13], "temperature": 0.0, "avg_logprob": -0.1255464370434101, "compression_ratio": 1.5033112582781456, "no_speech_prob": 7.816940342308953e-05}, {"id": 802, "seek": 338350, "start": 3383.5, "end": 3400.5, "text": " And so here's a quick example, let's say, okay, we want to label locations, we start off with one city San Francisco, and then we look at what else is similar to that term so these are actually these are actually real suggestions from that sense to make model that", "tokens": [400, 370, 510, 311, 257, 1702, 1365, 11, 718, 311, 584, 11, 1392, 11, 321, 528, 281, 7645, 9253, 11, 321, 722, 766, 365, 472, 2307, 5271, 12279, 11, 293, 550, 321, 574, 412, 437, 1646, 307, 2531, 281, 300, 1433, 370, 613, 366, 767, 613, 366, 767, 957, 13396, 490, 300, 2020, 281, 652, 2316, 300], "temperature": 0.0, "avg_logprob": -0.12731746767388016, "compression_ratio": 1.5529411764705883, "no_speech_prob": 0.00011792501754825935}, {"id": 803, "seek": 340050, "start": 3400.5, "end": 3413.5, "text": " Matt showed earlier. And you can see we also will in the nice thing is we're using word vectors we're not using a dictionary so we're not going to, we're going to annotate California and maybe University of San Francisco, but we're not going to annotate California roles", "tokens": [7397, 4712, 3071, 13, 400, 291, 393, 536, 321, 611, 486, 294, 264, 1481, 551, 307, 321, 434, 1228, 1349, 18875, 321, 434, 406, 1228, 257, 25890, 370, 321, 434, 406, 516, 281, 11, 321, 434, 516, 281, 25339, 473, 5384, 293, 1310, 3535, 295, 5271, 12279, 11, 457, 321, 434, 406, 516, 281, 25339, 473, 5384, 9604], "temperature": 0.0, "avg_logprob": -0.13588017322978035, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.00036204332718625665}, {"id": 804, "seek": 340050, "start": 3413.5, "end": 3426.5, "text": " because we already you know we innovate the space and we know that what we're actually looking for is at least similar to the real meaning of the word and a lot of these are super trivial to answer so we can accept them, we can reject them or we can ignore them because we,", "tokens": [570, 321, 1217, 291, 458, 321, 33444, 264, 1901, 293, 321, 458, 300, 437, 321, 434, 767, 1237, 337, 307, 412, 1935, 2531, 281, 264, 957, 3620, 295, 264, 1349, 293, 257, 688, 295, 613, 366, 1687, 26703, 281, 1867, 370, 321, 393, 3241, 552, 11, 321, 393, 8248, 552, 420, 321, 393, 11200, 552, 570, 321, 11], "temperature": 0.0, "avg_logprob": -0.13588017322978035, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.00036204332718625665}, {"id": 805, "seek": 342650, "start": 3426.5, "end": 3434.5, "text": " because this is a bit too ambiguous and we don't actually want that in our list, because it can mean too many things and then from here.", "tokens": [570, 341, 307, 257, 857, 886, 39465, 293, 321, 500, 380, 767, 528, 300, 294, 527, 1329, 11, 570, 309, 393, 914, 886, 867, 721, 293, 550, 490, 510, 13], "temperature": 0.0, "avg_logprob": -0.10287717744415882, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.00011151939543196931}, {"id": 806, "seek": 342650, "start": 3434.5, "end": 3441.5, "text": " We can actually create a pattern that uses species attributes or in this case.", "tokens": [492, 393, 767, 1884, 257, 5102, 300, 4960, 6172, 17212, 420, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.10287717744415882, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.00011151939543196931}, {"id": 807, "seek": 344150, "start": 3441.5, "end": 3457.5, "text": " Yeah, the, the token, the lower case form of the token and GPE that's stands for geopolitical entities so anything with the government. And that's what we're trying to label so we can easily build up these roles, very quickly they are like automated,", "tokens": [865, 11, 264, 11, 264, 14862, 11, 264, 3126, 1389, 1254, 295, 264, 14862, 293, 460, 5208, 300, 311, 7382, 337, 46615, 804, 16667, 370, 1340, 365, 264, 2463, 13, 400, 300, 311, 437, 321, 434, 1382, 281, 7645, 370, 321, 393, 3612, 1322, 493, 613, 9604, 11, 588, 2661, 436, 366, 411, 18473, 11], "temperature": 0.0, "avg_logprob": -0.16431512671001888, "compression_ratio": 1.4619883040935673, "no_speech_prob": 0.0001545220729894936}, {"id": 808, "seek": 345750, "start": 3457.5, "end": 3474.5, "text": " and then we have a bunch of locations that we can then match in our text so here.", "tokens": [293, 550, 321, 362, 257, 3840, 295, 9253, 300, 321, 393, 550, 2995, 294, 527, 2487, 370, 510, 13], "temperature": 0.0, "avg_logprob": -0.11194817916206691, "compression_ratio": 1.1408450704225352, "no_speech_prob": 8.64472531247884e-05}, {"id": 809, "seek": 347450, "start": 3474.5, "end": 3487.5, "text": " Constructs we can we can really take advantage of the syntactic structure. So, here this was an, this was a finance example. So what we're trying to do is we want to extract information about executive compensation.", "tokens": [8574, 1757, 82, 321, 393, 321, 393, 534, 747, 5002, 295, 264, 23980, 19892, 3877, 13, 407, 11, 510, 341, 390, 364, 11, 341, 390, 257, 10719, 1365, 13, 407, 437, 321, 434, 1382, 281, 360, 307, 321, 528, 281, 8947, 1589, 466, 10140, 19644, 13], "temperature": 0.0, "avg_logprob": -0.15415292739868164, "compression_ratio": 1.4429530201342282, "no_speech_prob": 5.462071203510277e-05}, {"id": 810, "seek": 348750, "start": 3487.5, "end": 3504.5, "text": " So, yeah, some executive receive some amount of money in stock for example like this one. And it's, this is a pretty difficult task but also the idea is we have, we have this theory that maybe if we could train a model, a text classification model to predict", "tokens": [407, 11, 1338, 11, 512, 10140, 4774, 512, 2372, 295, 1460, 294, 4127, 337, 1365, 411, 341, 472, 13, 400, 309, 311, 11, 341, 307, 257, 1238, 2252, 5633, 457, 611, 264, 1558, 307, 321, 362, 11, 321, 362, 341, 5261, 300, 1310, 498, 321, 727, 3847, 257, 2316, 11, 257, 2487, 21538, 2316, 281, 6069], "temperature": 0.0, "avg_logprob": -0.11411457061767578, "compression_ratio": 1.5, "no_speech_prob": 3.926522549591027e-05}, {"id": 811, "seek": 350450, "start": 3504.5, "end": 3519.5, "text": " whether a sentence is about executive compensation or not. We can then very easily use what we already know about the text to extract, let's say the first person entity, we extract the amount of money, put that in our database.", "tokens": [1968, 257, 8174, 307, 466, 10140, 19644, 420, 406, 13, 492, 393, 550, 588, 3612, 764, 437, 321, 1217, 458, 466, 264, 2487, 281, 8947, 11, 718, 311, 584, 264, 700, 954, 13977, 11, 321, 8947, 264, 2372, 295, 1460, 11, 829, 300, 294, 527, 8149, 13], "temperature": 0.0, "avg_logprob": -0.10060153287999771, "compression_ratio": 1.474025974025974, "no_speech_prob": 7.940947398310527e-05}, {"id": 812, "seek": 351950, "start": 3519.5, "end": 3537.5, "text": " And we've actually, yeah, we found a good solution for an otherwise very complex task so for this, this is just an idea. I haven't like we haven't tried this in detail but one possible pattern using token attributes we have available would be.", "tokens": [400, 321, 600, 767, 11, 1338, 11, 321, 1352, 257, 665, 3827, 337, 364, 5911, 588, 3997, 5633, 370, 337, 341, 11, 341, 307, 445, 364, 1558, 13, 286, 2378, 380, 411, 321, 2378, 380, 3031, 341, 294, 2607, 457, 472, 1944, 5102, 1228, 14862, 17212, 321, 362, 2435, 576, 312, 13], "temperature": 0.0, "avg_logprob": -0.11340223891394478, "compression_ratio": 1.4817073170731707, "no_speech_prob": 6.553942512255162e-05}, {"id": 813, "seek": 353750, "start": 3537.5, "end": 3551.5, "text": " Let's try look for an entity type person, followed by the lemma or a token with a lemma receive so received receives receiving and followed by an end by a token with the entity type money.", "tokens": [961, 311, 853, 574, 337, 364, 13977, 2010, 954, 11, 6263, 538, 264, 7495, 1696, 420, 257, 14862, 365, 257, 7495, 1696, 4774, 370, 4613, 20717, 10040, 293, 6263, 538, 364, 917, 538, 257, 14862, 365, 264, 13977, 2010, 1460, 13], "temperature": 0.0, "avg_logprob": -0.17727172618009607, "compression_ratio": 1.7413793103448276, "no_speech_prob": 6.437767297029495e-05}, {"id": 814, "seek": 353750, "start": 3551.5, "end": 3564.5, "text": " And let's just look at what this pulls up. That's an idea. I mean, you can get plenty of other possible patterns, you can come up with. And the nice thing is we actually going to be looking at them again in context.", "tokens": [400, 718, 311, 445, 574, 412, 437, 341, 16982, 493, 13, 663, 311, 364, 1558, 13, 286, 914, 11, 291, 393, 483, 7140, 295, 661, 1944, 8294, 11, 291, 393, 808, 493, 365, 13, 400, 264, 1481, 551, 307, 321, 767, 516, 281, 312, 1237, 412, 552, 797, 294, 4319, 13], "temperature": 0.0, "avg_logprob": -0.17727172618009607, "compression_ratio": 1.7413793103448276, "no_speech_prob": 6.437767297029495e-05}, {"id": 815, "seek": 356450, "start": 3564.5, "end": 3573.5, "text": " So, they don't have to be perfect and even actually in fact, even if it pulls up random stuff that you realize is totally not what you want.", "tokens": [407, 11, 436, 500, 380, 362, 281, 312, 2176, 293, 754, 767, 294, 1186, 11, 754, 498, 309, 16982, 493, 4974, 1507, 300, 291, 4325, 307, 3879, 406, 437, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.09811845389745569, "compression_ratio": 1.8849557522123894, "no_speech_prob": 9.387972386321053e-05}, {"id": 816, "seek": 356450, "start": 3573.5, "end": 3588.5, "text": " It's, this is also very important because you won't only be collecting annotations for the things you know are definitely right you also collecting annotations for the things that are very similar or look very similar to what you're looking for but actually not what you're looking for", "tokens": [467, 311, 11, 341, 307, 611, 588, 1021, 570, 291, 1582, 380, 787, 312, 12510, 25339, 763, 337, 264, 721, 291, 458, 366, 2138, 558, 291, 611, 12510, 25339, 763, 337, 264, 721, 300, 366, 588, 2531, 420, 574, 588, 2531, 281, 437, 291, 434, 1237, 337, 457, 767, 406, 437, 291, 434, 1237, 337], "temperature": 0.0, "avg_logprob": -0.09811845389745569, "compression_ratio": 1.8849557522123894, "no_speech_prob": 9.387972386321053e-05}, {"id": 817, "seek": 358850, "start": 3588.5, "end": 3594.5, "text": " that's probably just as important as the positive examples.", "tokens": [300, 311, 1391, 445, 382, 1021, 382, 264, 3353, 5110, 13], "temperature": 0.0, "avg_logprob": -0.13435091972351074, "compression_ratio": 1.6446700507614214, "no_speech_prob": 2.462893280608114e-05}, {"id": 818, "seek": 358850, "start": 3594.5, "end": 3609.5, "text": " So, yeah, the moral of the story is what we really what we're saying is, you know, you will be used to iterating on our code as programmers but but you should, you should really be doing both like the data is just as important so we, as we see here okay that's the", "tokens": [407, 11, 1338, 11, 264, 9723, 295, 264, 1657, 307, 437, 321, 534, 437, 321, 434, 1566, 307, 11, 291, 458, 11, 291, 486, 312, 1143, 281, 17138, 990, 322, 527, 3089, 382, 41504, 457, 457, 291, 820, 11, 291, 820, 534, 312, 884, 1293, 411, 264, 1412, 307, 445, 382, 1021, 370, 321, 11, 382, 321, 536, 510, 1392, 300, 311, 264], "temperature": 0.0, "avg_logprob": -0.13435091972351074, "compression_ratio": 1.6446700507614214, "no_speech_prob": 2.462893280608114e-05}, {"id": 819, "seek": 360950, "start": 3609.5, "end": 3622.5, "text": " normal type of programming, you have a runtime program, you work on the source code, you compile it get your runtime program you don't like something about your program, you go back change the source code compile it and so on that's a pretty standard", "tokens": [2710, 2010, 295, 9410, 11, 291, 362, 257, 34474, 1461, 11, 291, 589, 322, 264, 4009, 3089, 11, 291, 31413, 309, 483, 428, 34474, 1461, 291, 500, 380, 411, 746, 466, 428, 1461, 11, 291, 352, 646, 1319, 264, 4009, 3089, 31413, 309, 293, 370, 322, 300, 311, 257, 1238, 3832], "temperature": 0.0, "avg_logprob": -0.08095410236945519, "compression_ratio": 1.9173913043478261, "no_speech_prob": 2.166685226256959e-05}, {"id": 820, "seek": 360950, "start": 3622.5, "end": 3623.5, "text": " workflow.", "tokens": [20993, 13], "temperature": 0.0, "avg_logprob": -0.08095410236945519, "compression_ratio": 1.9173913043478261, "no_speech_prob": 2.166685226256959e-05}, {"id": 821, "seek": 360950, "start": 3623.5, "end": 3625.5, "text": " And in machine learning.", "tokens": [400, 294, 3479, 2539, 13], "temperature": 0.0, "avg_logprob": -0.08095410236945519, "compression_ratio": 1.9173913043478261, "no_speech_prob": 2.166685226256959e-05}, {"id": 822, "seek": 360950, "start": 3625.5, "end": 3629.5, "text": " We don't have a runtime program in that sense we have a runtime model.", "tokens": [492, 500, 380, 362, 257, 34474, 1461, 294, 300, 2020, 321, 362, 257, 34474, 2316, 13], "temperature": 0.0, "avg_logprob": -0.08095410236945519, "compression_ratio": 1.9173913043478261, "no_speech_prob": 2.166685226256959e-05}, {"id": 823, "seek": 360950, "start": 3629.5, "end": 3634.5, "text": " So, the part we should really be thinking about and working on is the training data.", "tokens": [407, 11, 264, 644, 321, 820, 534, 312, 1953, 466, 293, 1364, 322, 307, 264, 3097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.08095410236945519, "compression_ratio": 1.9173913043478261, "no_speech_prob": 2.166685226256959e-05}, {"id": 824, "seek": 363450, "start": 3634.5, "end": 3648.5, "text": " Instead, most focus is currently on the training algorithm and that's if you use that analogy that's kind of, that's very similar to going and tweaking your compiler if you're not happy with your runtime program.", "tokens": [7156, 11, 881, 1879, 307, 4362, 322, 264, 3097, 9284, 293, 300, 311, 498, 291, 764, 300, 21663, 300, 311, 733, 295, 11, 300, 311, 588, 2531, 281, 516, 293, 6986, 2456, 428, 31958, 498, 291, 434, 406, 2055, 365, 428, 34474, 1461, 13], "temperature": 0.0, "avg_logprob": -0.11557719230651856, "compression_ratio": 1.7900763358778626, "no_speech_prob": 8.788335253484547e-05}, {"id": 825, "seek": 363450, "start": 3648.5, "end": 3662.5, "text": " You can do that but of course you probably go back and edit your source code and I think this is actually, this is actually a pretty good example is pretty accurate and you usually you know there are only so many training algorithms but what really makes a", "tokens": [509, 393, 360, 300, 457, 295, 1164, 291, 1391, 352, 646, 293, 8129, 428, 4009, 3089, 293, 286, 519, 341, 307, 767, 11, 341, 307, 767, 257, 1238, 665, 1365, 307, 1238, 8559, 293, 291, 2673, 291, 458, 456, 366, 787, 370, 867, 3097, 14642, 457, 437, 534, 1669, 257], "temperature": 0.0, "avg_logprob": -0.11557719230651856, "compression_ratio": 1.7900763358778626, "no_speech_prob": 8.788335253484547e-05}, {"id": 826, "seek": 366250, "start": 3662.5, "end": 3668.5, "text": " difference is your data so if you have a good way and a fast way of iterating on that data.", "tokens": [2649, 307, 428, 1412, 370, 498, 291, 362, 257, 665, 636, 293, 257, 2370, 636, 295, 17138, 990, 322, 300, 1412, 13], "temperature": 0.0, "avg_logprob": -0.10642141454360064, "compression_ratio": 1.6699507389162562, "no_speech_prob": 0.00012733683979604393}, {"id": 827, "seek": 366250, "start": 3668.5, "end": 3681.5, "text": " You actually, and you know you're able to really master this part of the problem you, you'll also get to try more things quickly you really, you know, as we know most ideas don't actually work I think that's, it's always one of these things that's", "tokens": [509, 767, 11, 293, 291, 458, 291, 434, 1075, 281, 534, 4505, 341, 644, 295, 264, 1154, 291, 11, 291, 603, 611, 483, 281, 853, 544, 721, 2661, 291, 534, 11, 291, 458, 11, 382, 321, 458, 881, 3487, 500, 380, 767, 589, 286, 519, 300, 311, 11, 309, 311, 1009, 472, 295, 613, 721, 300, 311], "temperature": 0.0, "avg_logprob": -0.10642141454360064, "compression_ratio": 1.6699507389162562, "no_speech_prob": 0.00012733683979604393}, {"id": 828, "seek": 368150, "start": 3681.5, "end": 3699.5, "text": " kind of misrepresented or a lot of people have this idea, you're doing all these amazing AI things and everything just works and it's like, kind of doesn't like nothing works. And sometimes sometimes things work and you know you really want to find the things that actually work and, you know, for that you need to try them and so", "tokens": [733, 295, 3346, 38293, 420, 257, 688, 295, 561, 362, 341, 1558, 11, 291, 434, 884, 439, 613, 2243, 7318, 721, 293, 1203, 445, 1985, 293, 309, 311, 411, 11, 733, 295, 1177, 380, 411, 1825, 1985, 13, 400, 2171, 2171, 721, 589, 293, 291, 458, 291, 534, 528, 281, 915, 264, 721, 300, 767, 589, 293, 11, 291, 458, 11, 337, 300, 291, 643, 281, 853, 552, 293, 370], "temperature": 0.0, "avg_logprob": -0.2529649476747255, "compression_ratio": 1.7010309278350515, "no_speech_prob": 7.713937520748004e-05}, {"id": 829, "seek": 369950, "start": 3699.5, "end": 3714.5, "text": " it also means you know if you can actually figure out what works before you try it and invest in it, you know, you can actually be more successful overall because you don't, you're not going to waste your time on the things that might fail and more scale things up that actually", "tokens": [309, 611, 1355, 291, 458, 498, 291, 393, 767, 2573, 484, 437, 1985, 949, 291, 853, 309, 293, 1963, 294, 309, 11, 291, 458, 11, 291, 393, 767, 312, 544, 4406, 4787, 570, 291, 500, 380, 11, 291, 434, 406, 516, 281, 5964, 428, 565, 322, 264, 721, 300, 1062, 3061, 293, 544, 4373, 721, 493, 300, 767], "temperature": 0.0, "avg_logprob": -0.13471469571513514, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.00010428368841530755}, {"id": 830, "seek": 371450, "start": 3714.5, "end": 3731.5, "text": " weren't even supposed to work in the first place. And one thing that's also very important to us is you can really build custom solutions you can, you know, build solutions that fit exactly to your use case and you know you keep these, if you collect your own data you keep that forever and nobody can lock you in.", "tokens": [4999, 380, 754, 3442, 281, 589, 294, 264, 700, 1081, 13, 400, 472, 551, 300, 311, 611, 588, 1021, 281, 505, 307, 291, 393, 534, 1322, 2375, 6547, 291, 393, 11, 291, 458, 11, 1322, 6547, 300, 3318, 2293, 281, 428, 764, 1389, 293, 291, 458, 291, 1066, 613, 11, 498, 291, 2500, 428, 1065, 1412, 291, 1066, 300, 5680, 293, 5079, 393, 4017, 291, 294, 13], "temperature": 0.0, "avg_logprob": -0.1832154502331371, "compression_ratio": 1.643979057591623, "no_speech_prob": 0.0001711920922389254}, {"id": 831, "seek": 373150, "start": 3731.5, "end": 3746.5, "text": " You're not just consuming some API and if that API shuts down, you can start again from scratch. You actually, you know, you have your data, no matter what other cool things we can do at some point in the future you can always go back to your label data and really", "tokens": [509, 434, 406, 445, 19867, 512, 9362, 293, 498, 300, 9362, 48590, 760, 11, 291, 393, 722, 797, 490, 8459, 13, 509, 767, 11, 291, 458, 11, 291, 362, 428, 1412, 11, 572, 1871, 437, 661, 1627, 721, 321, 393, 360, 412, 512, 935, 294, 264, 2027, 291, 393, 1009, 352, 646, 281, 428, 7645, 1412, 293, 534], "temperature": 0.0, "avg_logprob": -0.10614603565585229, "compression_ratio": 1.4915254237288136, "no_speech_prob": 0.00013538243365474045}, {"id": 832, "seek": 374650, "start": 3746.5, "end": 3761.5, "text": " build your own systems and we believe that this is really something that's very important in the future of the technology. That's also a reason why we think AI development in general in companies should be done in house.", "tokens": [1322, 428, 1065, 3652, 293, 321, 1697, 300, 341, 307, 534, 746, 300, 311, 588, 1021, 294, 264, 2027, 295, 264, 2899, 13, 663, 311, 611, 257, 1778, 983, 321, 519, 7318, 3250, 294, 2674, 294, 3431, 820, 312, 1096, 294, 1782, 13], "temperature": 0.0, "avg_logprob": -0.12304676112843983, "compression_ratio": 1.5422885572139304, "no_speech_prob": 0.00018279165669810027}, {"id": 833, "seek": 374650, "start": 3761.5, "end": 3768.5, "text": " And yeah we're hoping that we can keep providing useful tools that will make this easier.", "tokens": [400, 1338, 321, 434, 7159, 300, 321, 393, 1066, 6530, 4420, 3873, 300, 486, 652, 341, 3571, 13], "temperature": 0.0, "avg_logprob": -0.12304676112843983, "compression_ratio": 1.5422885572139304, "no_speech_prob": 0.00018279165669810027}, {"id": 834, "seek": 376850, "start": 3768.5, "end": 3785.5, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5004932880401611, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.00010616139479679987}, {"id": 835, "seek": 378550, "start": 3785.5, "end": 3798.5, "text": " So the question is, yeah, Jeremy thinks we write very good software, even though we only two people and how we doing that. Yeah, that's a very good question I mean we do get this.", "tokens": [407, 264, 1168, 307, 11, 1338, 11, 17809, 7309, 321, 2464, 588, 665, 4722, 11, 754, 1673, 321, 787, 732, 561, 293, 577, 321, 884, 300, 13, 865, 11, 300, 311, 257, 588, 665, 1168, 286, 914, 321, 360, 483, 341, 13], "temperature": 0.0, "avg_logprob": -0.20691875789476477, "compression_ratio": 1.376923076923077, "no_speech_prob": 0.00011538645776454359}, {"id": 836, "seek": 379850, "start": 3798.5, "end": 3815.5, "text": " I don't even know where this idea comes from that like, yeah, you can scale things up, like I don't know scaling things up, makes things better, because I do think, yeah, actually, the more people you get involved you sometimes actually can have a very negative impact", "tokens": [286, 500, 380, 754, 458, 689, 341, 1558, 1487, 490, 300, 411, 11, 1338, 11, 291, 393, 4373, 721, 493, 11, 411, 286, 500, 380, 458, 21589, 721, 493, 11, 1669, 721, 1101, 11, 570, 286, 360, 519, 11, 1338, 11, 767, 11, 264, 544, 561, 291, 483, 3288, 291, 2171, 767, 393, 362, 257, 588, 3671, 2712], "temperature": 0.0, "avg_logprob": -0.1778849786327731, "compression_ratio": 1.6242424242424243, "no_speech_prob": 0.00018189576803706586}, {"id": 837, "seek": 381550, "start": 3815.5, "end": 3828.5, "text": " on the quality of the software you're using. In our case, it's just okay, it just works. Like I'm also, I also don't like this idea of, oh, everyone can do exactly the same thing if they just work hard even though people like thinking of it that way.", "tokens": [322, 264, 3125, 295, 264, 4722, 291, 434, 1228, 13, 682, 527, 1389, 11, 309, 311, 445, 1392, 11, 309, 445, 1985, 13, 1743, 286, 478, 611, 11, 286, 611, 500, 380, 411, 341, 1558, 295, 11, 1954, 11, 1518, 393, 360, 2293, 264, 912, 551, 498, 436, 445, 589, 1152, 754, 1673, 561, 411, 1953, 295, 309, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.18468093872070312, "compression_ratio": 1.874074074074074, "no_speech_prob": 0.0011077432427555323}, {"id": 838, "seek": 381550, "start": 3828.5, "end": 3844.5, "text": " It's just okay. In our case, we have a good combination of like things that we like to do things that we happen to be good at, and it just works together so I guess we are lucky in that way but we also cut out a lot of bullshit like the amount of meetings", "tokens": [467, 311, 445, 1392, 13, 682, 527, 1389, 11, 321, 362, 257, 665, 6562, 295, 411, 721, 300, 321, 411, 281, 360, 721, 300, 321, 1051, 281, 312, 665, 412, 11, 293, 309, 445, 1985, 1214, 370, 286, 2041, 321, 366, 6356, 294, 300, 636, 457, 321, 611, 1723, 484, 257, 688, 295, 22676, 411, 264, 2372, 295, 8410], "temperature": 0.0, "avg_logprob": -0.18468093872070312, "compression_ratio": 1.874074074074074, "no_speech_prob": 0.0011077432427555323}, {"id": 839, "seek": 384450, "start": 3844.5, "end": 3862.5, "text": " we don't take, the amount of events we don't go to. I mean, yeah, it's kind of ironic saying that, speaking at an event, but like, I really don't normally go to many events. I don't, we don't take coffee dates with random people we barely know we don't.", "tokens": [321, 500, 380, 747, 11, 264, 2372, 295, 3931, 321, 500, 380, 352, 281, 13, 286, 914, 11, 1338, 11, 309, 311, 733, 295, 33719, 1566, 300, 11, 4124, 412, 364, 2280, 11, 457, 411, 11, 286, 534, 500, 380, 5646, 352, 281, 867, 3931, 13, 286, 500, 380, 11, 321, 500, 380, 747, 4982, 11691, 365, 4974, 561, 321, 10268, 458, 321, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.1679124559674944, "compression_ratio": 1.5911949685534592, "no_speech_prob": 0.00029876973712816834}, {"id": 840, "seek": 386250, "start": 3862.5, "end": 3875.5, "text": " Yeah, we mostly we really just like to write software. And yeah, we've had some good ideas in the past.", "tokens": [50364, 865, 11, 321, 5240, 321, 534, 445, 411, 281, 2464, 4722, 13, 400, 1338, 11, 321, 600, 632, 512, 665, 3487, 294, 264, 1791, 13, 51014], "temperature": 0.0, "avg_logprob": -0.17296610559735978, "compression_ratio": 1.1318681318681318, "no_speech_prob": 0.0001881049101939425}, {"id": 841, "seek": 389250, "start": 3892.5, "end": 3921.5, "text": " I mean, we've also, the question is, if we've done any experiments where we compare the binary decisions, and whether it influences the annotators versus really doing everything from scratch. So, we haven't done experiments specifically focusing on the bias because that's, in some sense, that's difficult because, you know, we're mostly looking, we're looking at the output, we're looking at does it improve accuracy.", "tokens": [286, 914, 11, 321, 600, 611, 11, 264, 1168, 307, 11, 498, 321, 600, 1096, 604, 12050, 689, 321, 6794, 264, 17434, 5327, 11, 293, 1968, 309, 21222, 264, 25339, 3391, 5717, 534, 884, 1203, 490, 8459, 13, 407, 11, 321, 2378, 380, 1096, 12050, 4682, 8416, 322, 264, 12577, 570, 300, 311, 11, 294, 512, 2020, 11, 300, 311, 2252, 570, 11, 291, 458, 11, 321, 434, 5240, 1237, 11, 321, 434, 1237, 412, 264, 5598, 11, 321, 434, 1237, 412, 775, 309, 3470, 14170, 13], "temperature": 0.0, "avg_logprob": -0.20985311990255837, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.5736797451972961}, {"id": 842, "seek": 392150, "start": 3921.5, "end": 3938.5, "text": " So, we've done experiments of manual annotation versus binary annotation, but also mostly focused on our own tooling, because we think it's kind of useless like yeah we can present you a study where we said oh we did stuff in an Excel spreadsheet and then we did stuff in Prodigy and it was much better.", "tokens": [407, 11, 321, 600, 1096, 12050, 295, 9688, 48654, 5717, 17434, 48654, 11, 457, 611, 5240, 5178, 322, 527, 1065, 46593, 11, 570, 321, 519, 309, 311, 733, 295, 14115, 411, 1338, 321, 393, 1974, 291, 257, 2979, 689, 321, 848, 1954, 321, 630, 1507, 294, 364, 19060, 27733, 293, 550, 321, 630, 1507, 294, 1705, 25259, 88, 293, 309, 390, 709, 1101, 13], "temperature": 0.0, "avg_logprob": -0.1609310683082132, "compression_ratio": 1.5459183673469388, "no_speech_prob": 0.00016829869127832353}, {"id": 843, "seek": 393850, "start": 3938.5, "end": 3964.5, "text": " But it's really it's mostly focused around our own tooling and we did find that, well, it depends on the task you're doing. That's the other thing, it's sometimes I feel like giving these answers sounds unsatisfying because I'm always saying well, it depends on your data, but that's also kind of, that's also the whole point of it because, you know, we're doing this because your data is different and there's no one size fits all solution.", "tokens": [583, 309, 311, 534, 309, 311, 5240, 5178, 926, 527, 1065, 46593, 293, 321, 630, 915, 300, 11, 731, 11, 309, 5946, 322, 264, 5633, 291, 434, 884, 13, 663, 311, 264, 661, 551, 11, 309, 311, 2171, 286, 841, 411, 2902, 613, 6338, 3263, 2693, 25239, 1840, 570, 286, 478, 1009, 1566, 731, 11, 309, 5946, 322, 428, 1412, 11, 457, 300, 311, 611, 733, 295, 11, 300, 311, 611, 264, 1379, 935, 295, 309, 570, 11, 291, 458, 11, 321, 434, 884, 341, 570, 428, 1412, 307, 819, 293, 456, 311, 572, 472, 2744, 9001, 439, 3827, 13], "temperature": 0.0, "avg_logprob": -0.15495852323678824, "compression_ratio": 1.736220472440945, "no_speech_prob": 3.782139901886694e-05}, {"id": 844, "seek": 396450, "start": 3964.5, "end": 3975.5, "text": " But essentially, so we found what binary annotation works especially well if you already have a pre-trained model that predicts something, ideally also something that's not completely terrible.", "tokens": [583, 4476, 11, 370, 321, 1352, 437, 17434, 48654, 1985, 2318, 731, 498, 291, 1217, 362, 257, 659, 12, 17227, 2001, 2316, 300, 6069, 82, 746, 11, 22915, 611, 746, 300, 311, 406, 2584, 6237, 13], "temperature": 0.0, "avg_logprob": -0.1176134467124939, "compression_ratio": 1.3985507246376812, "no_speech_prob": 0.00012894262908957899}, {"id": 845, "seek": 397550, "start": 3975.5, "end": 3996.5, "text": " Otherwise, the pattern approach does work very well on kind of limited, very specific domains like we did one example of where we labeled drug names on Reddit, like on our opiates, which was a pretty good, this was a pretty good data source because it's a very specific topic and also it's a subreddit that's very on topic because people who,", "tokens": [10328, 11, 264, 5102, 3109, 775, 589, 588, 731, 322, 733, 295, 5567, 11, 588, 2685, 25514, 411, 321, 630, 472, 1365, 295, 689, 321, 21335, 4110, 5288, 322, 32210, 11, 411, 322, 527, 999, 72, 1024, 11, 597, 390, 257, 1238, 665, 11, 341, 390, 257, 1238, 665, 1412, 4009, 570, 309, 311, 257, 588, 2685, 4829, 293, 611, 309, 311, 257, 1422, 986, 17975, 300, 311, 588, 322, 4829, 570, 561, 567, 11], "temperature": 0.0, "avg_logprob": -0.1686701593519766, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.00017458655929658562}, {"id": 846, "seek": 399650, "start": 3996.5, "end": 4024.5, "text": " I mean, you know, people who discuss, who go on Reddit to discuss opiate use, you know, usually, you know, very dedicated to talking about this one topic. So it was a good interesting data source and so what we wanted to do is we labeled drug names, so drugs and pharmaceuticals, and in order to, for example, have a better, have a better tool set to analyze, really analyze the content of the subreddit and see how it develops over time.", "tokens": [286, 914, 11, 291, 458, 11, 561, 567, 2248, 11, 567, 352, 322, 32210, 281, 2248, 999, 13024, 764, 11, 291, 458, 11, 2673, 11, 291, 458, 11, 588, 8374, 281, 1417, 466, 341, 472, 4829, 13, 407, 309, 390, 257, 665, 1880, 1412, 4009, 293, 370, 437, 321, 1415, 281, 360, 307, 321, 21335, 4110, 5288, 11, 370, 7766, 293, 27130, 82, 11, 293, 294, 1668, 281, 11, 337, 1365, 11, 362, 257, 1101, 11, 362, 257, 1101, 2290, 992, 281, 12477, 11, 534, 12477, 264, 2701, 295, 264, 1422, 986, 17975, 293, 536, 577, 309, 25453, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.16367999485560825, "compression_ratio": 1.7244094488188977, "no_speech_prob": 0.0001864734513219446}, {"id": 847, "seek": 402450, "start": 4024.5, "end": 4044.5, "text": " So there we found the pattern based approach worked very, very well because we have very specific terms. We can use word vectors to bootstrap these, especially also we can include spelling mistakes and stuff, which was very interesting, like we can really build up good word lists, find them in the text, confirm them and get to pretty decent accuracy.", "tokens": [407, 456, 321, 1352, 264, 5102, 2361, 3109, 2732, 588, 11, 588, 731, 570, 321, 362, 588, 2685, 2115, 13, 492, 393, 764, 1349, 18875, 281, 11450, 372, 4007, 613, 11, 2318, 611, 321, 393, 4090, 22254, 8038, 293, 1507, 11, 597, 390, 588, 1880, 11, 411, 321, 393, 534, 1322, 493, 665, 1349, 14511, 11, 915, 552, 294, 264, 2487, 11, 9064, 552, 293, 483, 281, 1238, 8681, 14170, 13], "temperature": 0.0, "avg_logprob": -0.1199584706624349, "compression_ratio": 1.6073059360730593, "no_speech_prob": 5.8046047342941165e-05}, {"id": 848, "seek": 404450, "start": 4044.5, "end": 4073.5, "text": " I would expect this work to work a little less well, the cold start problem on a much more ambiguous domain and there you're probably better off to say, okay, we're labeling by hand, but even there, that's something I haven't really shown in detail here, but we've also, we have a manual interface where you highlight, but what we do there is we use the tokenizer to pre-segment the text, so you don't have to sit there and pixel perfect like highlight and then, oh shit, now I've got the white space in, let's start again.", "tokens": [286, 576, 2066, 341, 589, 281, 589, 257, 707, 1570, 731, 11, 264, 3554, 722, 1154, 322, 257, 709, 544, 39465, 9274, 293, 456, 291, 434, 1391, 1101, 766, 281, 584, 11, 1392, 11, 321, 434, 40244, 538, 1011, 11, 457, 754, 456, 11, 300, 311, 746, 286, 2378, 380, 534, 4898, 294, 2607, 510, 11, 457, 321, 600, 611, 11, 321, 362, 257, 9688, 9226, 689, 291, 5078, 11, 457, 437, 321, 360, 456, 307, 321, 764, 264, 14862, 6545, 281, 659, 12, 405, 10433, 264, 2487, 11, 370, 291, 500, 380, 362, 281, 1394, 456, 293, 19261, 2176, 411, 5078, 293, 550, 11, 1954, 4611, 11, 586, 286, 600, 658, 264, 2418, 1901, 294, 11, 718, 311, 722, 797, 13], "temperature": 0.0, "avg_logprob": -0.1373516567169674, "compression_ratio": 1.7491638795986622, "no_speech_prob": 0.00014231030945666134}, {"id": 849, "seek": 407350, "start": 4073.5, "end": 4088.5, "text": " So that's another thing we're doing. You can be much lazier in highlighting and also there get more efficiency out of it and still use a simpler interface.", "tokens": [407, 300, 311, 1071, 551, 321, 434, 884, 13, 509, 393, 312, 709, 19320, 811, 294, 26551, 293, 611, 456, 483, 544, 10493, 484, 295, 309, 293, 920, 764, 257, 18587, 9226, 13], "temperature": 0.0, "avg_logprob": -0.12763166427612305, "compression_ratio": 1.3025210084033614, "no_speech_prob": 7.450616249116138e-05}, {"id": 850, "seek": 408850, "start": 4088.5, "end": 4115.5, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.47346222400665283, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.0010320944711565971}, {"id": 851, "seek": 411550, "start": 4115.5, "end": 4138.5, "text": " Okay, so the question is, you gave an example of annotating patient data, which is obviously very problematic because doctors are not always very specific in what they fill in and then in the end, this was how did they enrich that with?", "tokens": [1033, 11, 370, 264, 1168, 307, 11, 291, 2729, 364, 1365, 295, 25339, 990, 4537, 1412, 11, 597, 307, 2745, 588, 19011, 570, 8778, 366, 406, 1009, 588, 2685, 294, 437, 436, 2836, 294, 293, 550, 294, 264, 917, 11, 341, 390, 577, 630, 436, 18849, 300, 365, 30], "temperature": 0.0, "avg_logprob": -0.1742452989544785, "compression_ratio": 1.4319526627218935, "no_speech_prob": 0.0002293458383064717}, {"id": 852, "seek": 411550, "start": 4138.5, "end": 4140.5, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.1742452989544785, "compression_ratio": 1.4319526627218935, "no_speech_prob": 0.0002293458383064717}, {"id": 853, "seek": 414050, "start": 4140.5, "end": 4167.5, "text": " So basically, okay, the question is whether we have some experience with, yeah, in the medical field mixing is not like, the answer is, well, we haven't personally done this. We do have quite a few companies in that domain, also because the tool itself is quite appealing because you can run it in your own compliant environment, you know, that data privacy aspect, but it's definitely, it's interesting to explore.", "tokens": [407, 1936, 11, 1392, 11, 264, 1168, 307, 1968, 321, 362, 512, 1752, 365, 11, 1338, 11, 294, 264, 4625, 2519, 11983, 307, 406, 411, 11, 264, 1867, 307, 11, 731, 11, 321, 2378, 380, 5665, 1096, 341, 13, 492, 360, 362, 1596, 257, 1326, 3431, 294, 300, 9274, 11, 611, 570, 264, 2290, 2564, 307, 1596, 23842, 570, 291, 393, 1190, 309, 294, 428, 1065, 36248, 2823, 11, 291, 458, 11, 300, 1412, 11427, 4171, 11, 457, 309, 311, 2138, 11, 309, 311, 1880, 281, 6839, 13], "temperature": 0.0, "avg_logprob": -0.17995162632154382, "compression_ratio": 1.6468253968253967, "no_speech_prob": 6.764454155927524e-05}, {"id": 854, "seek": 416750, "start": 4167.5, "end": 4188.5, "text": " Maybe also where, okay, having the professionals, getting the medical professionals more involved might make sense, which normally is very difficult. You don't want the doctor to do all the work themselves, but if you can find some way to distill that and then ask the doctor, okay, you said you wrote this here, does that mean you wrote X, does that mean Y and the doctor says yep, or doctor says nah.", "tokens": [2704, 611, 689, 11, 1392, 11, 1419, 264, 11954, 11, 1242, 264, 4625, 11954, 544, 3288, 1062, 652, 2020, 11, 597, 5646, 307, 588, 2252, 13, 509, 500, 380, 528, 264, 4631, 281, 360, 439, 264, 589, 2969, 11, 457, 498, 291, 393, 915, 512, 636, 281, 42923, 300, 293, 550, 1029, 264, 4631, 11, 1392, 11, 291, 848, 291, 4114, 341, 510, 11, 775, 300, 914, 291, 4114, 1783, 11, 775, 300, 914, 398, 293, 264, 4631, 1619, 18633, 11, 420, 4631, 1619, 17170, 13], "temperature": 0.0, "avg_logprob": -0.15301153394911024, "compression_ratio": 1.7866666666666666, "no_speech_prob": 0.00039653951535001397}, {"id": 855, "seek": 418850, "start": 4188.5, "end": 4208.5, "text": " So, that, if you can try this out and, you know, extract some information or that, that could be one idea to solve that, for example, yeah, I can definitely see that.", "tokens": [407, 11, 300, 11, 498, 291, 393, 853, 341, 484, 293, 11, 291, 458, 11, 8947, 512, 1589, 420, 300, 11, 300, 727, 312, 472, 1558, 281, 5039, 300, 11, 337, 1365, 11, 1338, 11, 286, 393, 2138, 536, 300, 13], "temperature": 0.0, "avg_logprob": -0.20623357560899522, "compression_ratio": 1.360655737704918, "no_speech_prob": 0.00020262307953089476}, {"id": 856, "seek": 420850, "start": 4208.5, "end": 4223.5, "text": " And then, like right now, it's not, we don't have a built in logic for that, although we are working on, oh, sorry, I forgot to repeat the question, interannotator agreement, if you can calculate that and incorporate that into your model.", "tokens": [400, 550, 11, 411, 558, 586, 11, 309, 311, 406, 11, 321, 500, 380, 362, 257, 3094, 294, 9952, 337, 300, 11, 4878, 321, 366, 1364, 322, 11, 1954, 11, 2597, 11, 286, 5298, 281, 7149, 264, 1168, 11, 728, 969, 310, 1639, 8106, 11, 498, 291, 393, 8873, 300, 293, 16091, 300, 666, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15572802746882203, "compression_ratio": 1.4424242424242424, "no_speech_prob": 6.625864625675604e-05}, {"id": 857, "seek": 422350, "start": 4223.5, "end": 4252.5, "text": " So, we're actually working on an extension for Prodigy, which is much more specifically for managing multiple annotators, because we really, the tool here, we really designed specifically as a developer tool first, and then, you know, scaling it up a second, but since you have the binary feedback, and if you have an idea, if you have an algorithm you want to use and you kind of, you know, you know what you want, you can already do that fairly easily because you can download all the data as JSON, you have a key that's answer, which is either accept, reject, or ignore.", "tokens": [407, 11, 321, 434, 767, 1364, 322, 364, 10320, 337, 1705, 25259, 88, 11, 597, 307, 709, 544, 4682, 337, 11642, 3866, 25339, 3391, 11, 570, 321, 534, 11, 264, 2290, 510, 11, 321, 534, 4761, 4682, 382, 257, 10754, 2290, 700, 11, 293, 550, 11, 291, 458, 11, 21589, 309, 493, 257, 1150, 11, 457, 1670, 291, 362, 264, 17434, 5824, 11, 293, 498, 291, 362, 364, 1558, 11, 498, 291, 362, 364, 9284, 291, 528, 281, 764, 293, 291, 733, 295, 11, 291, 458, 11, 291, 458, 437, 291, 528, 11, 291, 393, 1217, 360, 300, 6457, 3612, 570, 291, 393, 5484, 439, 264, 1412, 382, 31828, 11, 291, 362, 257, 2141, 300, 311, 1867, 11, 597, 307, 2139, 3241, 11, 8248, 11, 420, 11200, 13], "temperature": 0.0, "avg_logprob": -0.08751799121047511, "compression_ratio": 1.8306709265175718, "no_speech_prob": 0.00014806684339419007}, {"id": 858, "seek": 425250, "start": 4252.5, "end": 4265.5, "text": " You can attach your own arbitrary data like a user ID, and then it's fairly trivial to write your own function that really takes all of this, reads it in, computes something, and then uses this later on.", "tokens": [509, 393, 5085, 428, 1065, 23211, 1412, 411, 257, 4195, 7348, 11, 293, 550, 309, 311, 6457, 26703, 281, 2464, 428, 1065, 2445, 300, 534, 2516, 439, 295, 341, 11, 15700, 309, 294, 11, 715, 1819, 746, 11, 293, 550, 4960, 341, 1780, 322, 13], "temperature": 0.0, "avg_logprob": -0.13302463821217983, "compression_ratio": 1.5990338164251208, "no_speech_prob": 0.00013686672900803387}, {"id": 859, "seek": 425250, "start": 4265.5, "end": 4272.5, "text": " So, that's definitely possible, but we also, this is also something we're really interested in, yeah, exploring and working on.", "tokens": [407, 11, 300, 311, 2138, 1944, 11, 457, 321, 611, 11, 341, 307, 611, 746, 321, 434, 534, 3102, 294, 11, 1338, 11, 12736, 293, 1364, 322, 13], "temperature": 0.0, "avg_logprob": -0.13302463821217983, "compression_ratio": 1.5990338164251208, "no_speech_prob": 0.00013686672900803387}, {"id": 860, "seek": 427250, "start": 4272.5, "end": 4290.5, "text": " Yeah, so if you binary, yeah, that's also, that's a big advantage of the binary interface is that, yeah, they only pretty much, yeah, there are two options, you filter out the ignored ones, and then, yeah, you can really answer that question.", "tokens": [865, 11, 370, 498, 291, 17434, 11, 1338, 11, 300, 311, 611, 11, 300, 311, 257, 955, 5002, 295, 264, 17434, 9226, 307, 300, 11, 1338, 11, 436, 787, 1238, 709, 11, 1338, 11, 456, 366, 732, 3956, 11, 291, 6608, 484, 264, 19735, 2306, 11, 293, 550, 11, 1338, 11, 291, 393, 534, 1867, 300, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2353459560509884, "compression_ratio": 1.6, "no_speech_prob": 0.00025443450431339443}, {"id": 861, "seek": 427250, "start": 4290.5, "end": 4300.5, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2353459560509884, "compression_ratio": 1.6, "no_speech_prob": 0.00025443450431339443}, {"id": 862, "seek": 430050, "start": 4300.5, "end": 4318.5, "text": " Yeah, well you can, like you could design, so the question was, the one interface I showed, which was the sentiment one with the multiple selections, this is not binary, that's true, and actually it's also something we usually tell our users, avoid this as much as possible, if you can.", "tokens": [865, 11, 731, 291, 393, 11, 411, 291, 727, 1715, 11, 370, 264, 1168, 390, 11, 264, 472, 9226, 286, 4712, 11, 597, 390, 264, 16149, 472, 365, 264, 3866, 47829, 11, 341, 307, 406, 17434, 11, 300, 311, 2074, 11, 293, 767, 309, 311, 611, 746, 321, 2673, 980, 527, 5022, 11, 5042, 341, 382, 709, 382, 1944, 11, 498, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.14390857079449823, "compression_ratio": 1.5294117647058822, "no_speech_prob": 6.424656021408737e-05}, {"id": 863, "seek": 431850, "start": 4318.5, "end": 4336.5, "text": " Like that's, and in some cases you might still want that, or we say, look, if you need to, you know, a lot of people still think of surveys when they think of annotating data, and I get where this is coming from, but I think if you can leave that sort of mindset and really open up a bit and think of other creative ways, you can get more out of this.", "tokens": [1743, 300, 311, 11, 293, 294, 512, 3331, 291, 1062, 920, 528, 300, 11, 420, 321, 584, 11, 574, 11, 498, 291, 643, 281, 11, 291, 458, 11, 257, 688, 295, 561, 920, 519, 295, 22711, 562, 436, 519, 295, 25339, 990, 1412, 11, 293, 286, 483, 689, 341, 307, 1348, 490, 11, 457, 286, 519, 498, 291, 393, 1856, 300, 1333, 295, 12543, 293, 534, 1269, 493, 257, 857, 293, 519, 295, 661, 5880, 2098, 11, 291, 393, 483, 544, 484, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.10670836587970177, "compression_ratio": 1.6794258373205742, "no_speech_prob": 6.717369251418859e-05}, {"id": 864, "seek": 433650, "start": 4336.5, "end": 4352.5, "text": " So if you want to re engineer a survey, maybe you want to use a survey tool. But, but so this, I would, so for example if I were doing this with those four options, I would say, okay, we have all texts, the annotator sees every text four times and says, is this happy or is this not happy.", "tokens": [407, 498, 291, 528, 281, 319, 11403, 257, 8984, 11, 1310, 291, 528, 281, 764, 257, 8984, 2290, 13, 583, 11, 457, 370, 341, 11, 286, 576, 11, 370, 337, 1365, 498, 286, 645, 884, 341, 365, 729, 1451, 3956, 11, 286, 576, 584, 11, 1392, 11, 321, 362, 439, 15765, 11, 264, 25339, 1639, 8194, 633, 2487, 1451, 1413, 293, 1619, 11, 307, 341, 2055, 420, 307, 341, 406, 2055, 13], "temperature": 0.0, "avg_logprob": -0.17711848961679558, "compression_ratio": 1.6145251396648044, "no_speech_prob": 9.249107097275555e-05}, {"id": 865, "seek": 435250, "start": 4352.5, "end": 4380.5, "text": " And because you can get to one second per annotation, that's very fast, like you can, even if you have thousands of examples, you can do this in a day yourself. And so that's how we would probably solve this, and it also means you get every example four times, and for each text you know, is it sad, is it happy, is it neutral, is it something else, you have much more data, but not everyone wants this, like some people really want to build that survey and we let them, but yeah.", "tokens": [400, 570, 291, 393, 483, 281, 472, 1150, 680, 48654, 11, 300, 311, 588, 2370, 11, 411, 291, 393, 11, 754, 498, 291, 362, 5383, 295, 5110, 11, 291, 393, 360, 341, 294, 257, 786, 1803, 13, 400, 370, 300, 311, 577, 321, 576, 1391, 5039, 341, 11, 293, 309, 611, 1355, 291, 483, 633, 1365, 1451, 1413, 11, 293, 337, 1184, 2487, 291, 458, 11, 307, 309, 4227, 11, 307, 309, 2055, 11, 307, 309, 10598, 11, 307, 309, 746, 1646, 11, 291, 362, 709, 544, 1412, 11, 457, 406, 1518, 2738, 341, 11, 411, 512, 561, 534, 528, 281, 1322, 300, 8984, 293, 321, 718, 552, 11, 457, 1338, 13], "temperature": 0.0, "avg_logprob": -0.1120590588142132, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.000137688301037997}, {"id": 866, "seek": 438050, "start": 4380.5, "end": 4392.5, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.46198836962382, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.00014666539209429175}, {"id": 867, "seek": 439250, "start": 4392.5, "end": 4410.5, "text": " So the question is, yeah, if you're doing the same example multiple times, whether it slows down the annotation or not. Well, actually, I mean, it's difficult to say because it depends. But I've actually found that even if you do the bare maths, it can easily be much faster because if you, you know, you say okay 1000 examples.", "tokens": [407, 264, 1168, 307, 11, 1338, 11, 498, 291, 434, 884, 264, 912, 1365, 3866, 1413, 11, 1968, 309, 35789, 760, 264, 48654, 420, 406, 13, 1042, 11, 767, 11, 286, 914, 11, 309, 311, 2252, 281, 584, 570, 309, 5946, 13, 583, 286, 600, 767, 1352, 300, 754, 498, 291, 360, 264, 6949, 36287, 11, 309, 393, 3612, 312, 709, 4663, 570, 498, 291, 11, 291, 458, 11, 291, 584, 1392, 9714, 5110, 13], "temperature": 0.0, "avg_logprob": -0.11830531494526923, "compression_ratio": 1.539906103286385, "no_speech_prob": 9.107193909585476e-05}, {"id": 868, "seek": 441050, "start": 4410.5, "end": 4423.5, "text": " And normally if you really have to think about five different concepts that are maybe not even fully related, that just every tiny bit of friction you put between a human and the interface or the decision can very significantly slow down the process.", "tokens": [400, 5646, 498, 291, 534, 362, 281, 519, 466, 1732, 819, 10392, 300, 366, 1310, 406, 754, 4498, 4077, 11, 300, 445, 633, 5870, 857, 295, 17710, 291, 829, 1296, 257, 1952, 293, 264, 9226, 420, 264, 3537, 393, 588, 10591, 2964, 760, 264, 1399, 13], "temperature": 0.0, "avg_logprob": -0.11157177925109864, "compression_ratio": 1.4970059880239521, "no_speech_prob": 6.281989044509828e-05}, {"id": 869, "seek": 442350, "start": 4423.5, "end": 4447.5, "text": " You think about oh is this happy or is this sad or is this about sports or is this about horses, and just this thing that can easily add like 10 seconds to each question. So, if you do, if you do the whole thing three times at one second, you're still faster than you would have been if you'd added this friction and also and the other part is the just a human error.", "tokens": [509, 519, 466, 1954, 307, 341, 2055, 420, 307, 341, 4227, 420, 307, 341, 466, 6573, 420, 307, 341, 466, 13112, 11, 293, 445, 341, 551, 300, 393, 3612, 909, 411, 1266, 3949, 281, 1184, 1168, 13, 407, 11, 498, 291, 360, 11, 498, 291, 360, 264, 1379, 551, 1045, 1413, 412, 472, 1150, 11, 291, 434, 920, 4663, 813, 291, 576, 362, 668, 498, 291, 1116, 3869, 341, 17710, 293, 611, 293, 264, 661, 644, 307, 264, 445, 257, 1952, 6713, 13], "temperature": 0.0, "avg_logprob": -0.1664057326042789, "compression_ratio": 1.755980861244019, "no_speech_prob": 8.755199087318033e-05}, {"id": 870, "seek": 444750, "start": 4447.5, "end": 4456.5, "text": " So if you have to think too much, you're much more likely to fuck it up and do it badly and then that's also something you know that's also something you want to, you want to avoid.", "tokens": [407, 498, 291, 362, 281, 519, 886, 709, 11, 291, 434, 709, 544, 3700, 281, 3275, 309, 493, 293, 360, 309, 13425, 293, 550, 300, 311, 611, 746, 291, 458, 300, 311, 611, 746, 291, 528, 281, 11, 291, 528, 281, 5042, 13], "temperature": 0.0, "avg_logprob": -0.21678724475935393, "compression_ratio": 1.5327868852459017, "no_speech_prob": 0.00010796196147566661}, {"id": 871, "seek": 444750, "start": 4456.5, "end": 4466.5, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.21678724475935393, "compression_ratio": 1.5327868852459017, "no_speech_prob": 0.00010796196147566661}, {"id": 872, "seek": 446650, "start": 4466.5, "end": 4490.5, "text": " The active learning also makes a difference here because you can actually, yeah, you could pre-select the ones that really make a difference to annotate and don't have to like really go through every single one that yeah it's not as important as some of the other ones that you really care about.", "tokens": [440, 4967, 2539, 611, 1669, 257, 2649, 510, 570, 291, 393, 767, 11, 1338, 11, 291, 727, 659, 12, 405, 1809, 264, 2306, 300, 534, 652, 257, 2649, 281, 25339, 473, 293, 500, 380, 362, 281, 411, 534, 352, 807, 633, 2167, 472, 300, 1338, 309, 311, 406, 382, 1021, 382, 512, 295, 264, 661, 2306, 300, 291, 534, 1127, 466, 13], "temperature": 0.0, "avg_logprob": -0.1307165983951453, "compression_ratio": 1.6263736263736264, "no_speech_prob": 4.1430019336985424e-05}, {"id": 873, "seek": 449050, "start": 4490.5, "end": 4497.5, "text": " Yeah, so the question is, yeah, what about tasks that need a lot of context like the whole medical history or just a whole document.", "tokens": [865, 11, 370, 264, 1168, 307, 11, 1338, 11, 437, 466, 9608, 300, 643, 257, 688, 295, 4319, 411, 264, 1379, 4625, 2503, 420, 445, 257, 1379, 4166, 13], "temperature": 0.0, "avg_logprob": -0.1954181989034017, "compression_ratio": 1.8064516129032258, "no_speech_prob": 9.755713836057112e-05}, {"id": 874, "seek": 449050, "start": 4497.5, "end": 4519.5, "text": " So we basically, so we have, whether we have experience with that. So, in general, we do say if you can, if your tasks require so much context that you can't fit this into the prodigy interface, then it doesn't mean that you can't train a model on that but for most of the tasks that users most commonly want to do, this is often also an indicator that it's very difficult to actually teach your model that like if you're doing", "tokens": [407, 321, 1936, 11, 370, 321, 362, 11, 1968, 321, 362, 1752, 365, 300, 13, 407, 11, 294, 2674, 11, 321, 360, 584, 498, 291, 393, 11, 498, 428, 9608, 3651, 370, 709, 4319, 300, 291, 393, 380, 3318, 341, 666, 264, 15792, 328, 88, 9226, 11, 550, 309, 1177, 380, 914, 300, 291, 393, 380, 3847, 257, 2316, 322, 300, 457, 337, 881, 295, 264, 9608, 300, 5022, 881, 12719, 528, 281, 360, 11, 341, 307, 2049, 611, 364, 16961, 300, 309, 311, 588, 2252, 281, 767, 2924, 428, 2316, 300, 411, 498, 291, 434, 884], "temperature": 0.0, "avg_logprob": -0.1954181989034017, "compression_ratio": 1.8064516129032258, "no_speech_prob": 9.755713836057112e-05}, {"id": 875, "seek": 451950, "start": 4519.5, "end": 4536.5, "text": " entity recognition or even text classification and you need a lot of context and every other context is equally as important. That's often an indicator that that might not work so well. So for example, text classification, we say, okay, we start off by selecting one sentence from the whole document.", "tokens": [13977, 11150, 420, 754, 2487, 21538, 293, 291, 643, 257, 688, 295, 4319, 293, 633, 661, 4319, 307, 12309, 382, 1021, 13, 663, 311, 2049, 364, 16961, 300, 300, 1062, 406, 589, 370, 731, 13, 407, 337, 1365, 11, 2487, 21538, 11, 321, 584, 11, 1392, 11, 321, 722, 766, 538, 18182, 472, 8174, 490, 264, 1379, 4166, 13], "temperature": 0.0, "avg_logprob": -0.1507443624829489, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.0005824118852615356}, {"id": 876, "seek": 453650, "start": 4536.5, "end": 4565.5, "text": " And then instead of you annotating the whole document, you say, okay, this is the most important sentence. Does this label apply or not? So there are some tricks we use to get around this problem because yeah, we also think that okay, it's important to get this across and frame it in that way because yeah, if you need two pages on your screen, it's not efficient at all.", "tokens": [400, 550, 2602, 295, 291, 25339, 990, 264, 1379, 4166, 11, 291, 584, 11, 1392, 11, 341, 307, 264, 881, 1021, 8174, 13, 4402, 341, 7645, 3079, 420, 406, 30, 407, 456, 366, 512, 11733, 321, 764, 281, 483, 926, 341, 1154, 570, 1338, 11, 321, 611, 519, 300, 1392, 11, 309, 311, 1021, 281, 483, 341, 2108, 293, 3920, 309, 294, 300, 636, 570, 1338, 11, 498, 291, 643, 732, 7183, 322, 428, 2568, 11, 309, 311, 406, 7148, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.1480705327001111, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.0006532908300869167}, {"id": 877, "seek": 456550, "start": 4565.5, "end": 4590.5, "text": " And also likely you can do all that work but your model won't learn that because your model needs local context as well, at least for the tasks that we are presenting. I don't know if you have anything to add to that. Yeah, okay.", "tokens": [400, 611, 3700, 291, 393, 360, 439, 300, 589, 457, 428, 2316, 1582, 380, 1466, 300, 570, 428, 2316, 2203, 2654, 4319, 382, 731, 11, 412, 1935, 337, 264, 9608, 300, 321, 366, 15578, 13, 286, 500, 380, 458, 498, 291, 362, 1340, 281, 909, 281, 300, 13, 865, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.14207248176847184, "compression_ratio": 1.4585987261146496, "no_speech_prob": 0.0003108620294369757}, {"id": 878, "seek": 459050, "start": 4590.5, "end": 4609.5, "text": " Yeah, so the suggestion was yeah, okay, having some tools, some process that goes along with the software that helps people break this down. Yeah, we've actually been thinking about this a lot because we do realize, you know, the tool is quite new and we're introducing a lot of new concepts at once and also some best practices where we think, ah, that's how you should do it or you could try this.", "tokens": [865, 11, 370, 264, 16541, 390, 1338, 11, 1392, 11, 1419, 512, 3873, 11, 512, 1399, 300, 1709, 2051, 365, 264, 4722, 300, 3665, 561, 1821, 341, 760, 13, 865, 11, 321, 600, 767, 668, 1953, 466, 341, 257, 688, 570, 321, 360, 4325, 11, 291, 458, 11, 264, 2290, 307, 1596, 777, 293, 321, 434, 15424, 257, 688, 295, 777, 10392, 412, 1564, 293, 611, 512, 1151, 7525, 689, 321, 519, 11, 3716, 11, 300, 311, 577, 291, 820, 360, 309, 420, 291, 727, 853, 341, 13], "temperature": 0.0, "avg_logprob": -0.1255400802778161, "compression_ratio": 1.6694560669456067, "no_speech_prob": 0.00015380683180410415}, {"id": 879, "seek": 460950, "start": 4609.5, "end": 4627.5, "text": " And we are also realizing that there's no real satisfying one size fits all answer. That's another problem. Everyone's use case is different. So right now what we're doing is we have a support form for Prodigy where we answer people's questions and actually a lot of users share what they're working on, asking for tips.", "tokens": [400, 321, 366, 611, 16734, 300, 456, 311, 572, 957, 18348, 472, 2744, 9001, 439, 1867, 13, 663, 311, 1071, 1154, 13, 5198, 311, 764, 1389, 307, 819, 13, 407, 558, 586, 437, 321, 434, 884, 307, 321, 362, 257, 1406, 1254, 337, 1705, 25259, 88, 689, 321, 1867, 561, 311, 1651, 293, 767, 257, 688, 295, 5022, 2073, 437, 436, 434, 1364, 322, 11, 3365, 337, 6082, 13], "temperature": 0.0, "avg_logprob": -0.08319376592766749, "compression_ratio": 1.5165876777251184, "no_speech_prob": 3.165334783261642e-05}, {"id": 880, "seek": 462750, "start": 4627.5, "end": 4648.5, "text": " We kind of talk about it, other users come in and are like, oh, I actually tried to do this type of medical or this type of legal annotation and here's what worked for me and have this sort of exchange around it to figure out, okay, what works because yeah, it's just like I think machine learning, deep learning, a lot of the best practices are still evolving.", "tokens": [492, 733, 295, 751, 466, 309, 11, 661, 5022, 808, 294, 293, 366, 411, 11, 1954, 11, 286, 767, 3031, 281, 360, 341, 2010, 295, 4625, 420, 341, 2010, 295, 5089, 48654, 293, 510, 311, 437, 2732, 337, 385, 293, 362, 341, 1333, 295, 7742, 926, 309, 281, 2573, 484, 11, 1392, 11, 437, 1985, 570, 1338, 11, 309, 311, 445, 411, 286, 519, 3479, 2539, 11, 2452, 2539, 11, 257, 688, 295, 264, 1151, 7525, 366, 920, 21085, 13], "temperature": 0.0, "avg_logprob": -0.1815629346030099, "compression_ratio": 1.6188340807174888, "no_speech_prob": 0.00020007899729534984}, {"id": 881, "seek": 464850, "start": 4648.5, "end": 4664.5, "text": " And it's very, very specific. So it's definitely, yeah, we're open for suggestions as well, but like we're still in the process of really coming up with a good set of best practices and ideas.", "tokens": [400, 309, 311, 588, 11, 588, 2685, 13, 407, 309, 311, 2138, 11, 1338, 11, 321, 434, 1269, 337, 13396, 382, 731, 11, 457, 411, 321, 434, 920, 294, 264, 1399, 295, 534, 1348, 493, 365, 257, 665, 992, 295, 1151, 7525, 293, 3487, 13], "temperature": 0.0, "avg_logprob": -0.24768344723448463, "compression_ratio": 1.3617021276595744, "no_speech_prob": 0.000129219755763188}, {"id": 882, "seek": 466450, "start": 4664.5, "end": 4685.5, "text": " The question is whether we have any plans to sell models like medical models. Yes, as part of what Matt mentioned in the very introduction, we are definitely planning on having more of a models, like an online store for very, very specific models.", "tokens": [440, 1168, 307, 1968, 321, 362, 604, 5482, 281, 3607, 5245, 411, 4625, 5245, 13, 1079, 11, 382, 644, 295, 437, 7397, 2835, 294, 264, 588, 9339, 11, 321, 366, 2138, 5038, 322, 1419, 544, 295, 257, 5245, 11, 411, 364, 2950, 3531, 337, 588, 11, 588, 2685, 5245, 13], "temperature": 0.0, "avg_logprob": -0.16633595360649955, "compression_ratio": 1.5246913580246915, "no_speech_prob": 0.0005343818920664489}, {"id": 883, "seek": 468550, "start": 4685.5, "end": 4714.5, "text": " Medical, that's a very, very interesting domain and if so, we really want to have it specific like medical texts in French or Chinese and really go in that direction because we believe that, okay, pre-trained models are very valuable and even if you do medical texts, you can start off with a pre-trained model, then you can use a tool like Prodigy or something else to really fine tune it on your very, very specific context, have word vectors in it that already fit to your domain,", "tokens": [15896, 11, 300, 311, 257, 588, 11, 588, 1880, 9274, 293, 498, 370, 11, 321, 534, 528, 281, 362, 309, 2685, 411, 4625, 15765, 294, 5522, 420, 4649, 293, 534, 352, 294, 300, 3513, 570, 321, 1697, 300, 11, 1392, 11, 659, 12, 17227, 2001, 5245, 366, 588, 8263, 293, 754, 498, 291, 360, 4625, 15765, 11, 291, 393, 722, 766, 365, 257, 659, 12, 17227, 2001, 2316, 11, 550, 291, 393, 764, 257, 2290, 411, 1705, 25259, 88, 420, 746, 1646, 281, 534, 2489, 10864, 309, 322, 428, 588, 11, 588, 2685, 4319, 11, 362, 1349, 18875, 294, 309, 300, 1217, 3318, 281, 428, 9274, 11], "temperature": 0.0, "avg_logprob": -0.10569305248088665, "compression_ratio": 1.788888888888889, "no_speech_prob": 7.043223013170063e-05}, {"id": 884, "seek": 471450, "start": 4714.5, "end": 4726.5, "text": " maybe up those as well. We think that this is a very future-proof way of working with these technologies.", "tokens": [1310, 493, 729, 382, 731, 13, 492, 519, 300, 341, 307, 257, 588, 2027, 12, 15690, 636, 295, 1364, 365, 613, 7943, 13], "temperature": 0.0, "avg_logprob": -0.22172558124248798, "compression_ratio": 1.552325581395349, "no_speech_prob": 0.0003788429021369666}, {"id": 885, "seek": 471450, "start": 4726.5, "end": 4735.5, "text": " So currently, so question is the text classification model, we're using Prodigy, more details on that. So what we're using is Spacey's text classification model.", "tokens": [407, 4362, 11, 370, 1168, 307, 264, 2487, 21538, 2316, 11, 321, 434, 1228, 1705, 25259, 88, 11, 544, 4365, 322, 300, 13, 407, 437, 321, 434, 1228, 307, 8705, 88, 311, 2487, 21538, 2316, 13], "temperature": 0.0, "avg_logprob": -0.22172558124248798, "compression_ratio": 1.552325581395349, "no_speech_prob": 0.0003788429021369666}, {"id": 886, "seek": 473550, "start": 4735.5, "end": 4749.5, "text": " That's what's built in, but I think actually this question is pretty good because what's important to note is that Prodigy itself comes with a few built-in recipes that are basically ideas for, okay, how you could train a text classifier.", "tokens": [663, 311, 437, 311, 3094, 294, 11, 457, 286, 519, 767, 341, 1168, 307, 1238, 665, 570, 437, 311, 1021, 281, 3637, 307, 300, 1705, 25259, 88, 2564, 1487, 365, 257, 1326, 3094, 12, 259, 13035, 300, 366, 1936, 3487, 337, 11, 1392, 11, 577, 291, 727, 3847, 257, 2487, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.1077544993924019, "compression_ratio": 1.7, "no_speech_prob": 0.00020143180154263973}, {"id": 887, "seek": 473550, "start": 4749.5, "end": 4764.5, "text": " You could use Spacey, but it's definitely not tied to those. Like the idea, the tool itself is really the scaffolding around it. So if you say, hey, I wrote my own model using PyTorch and I would like to train this, all you need to do is you need to have one function that takes examples and updates them.", "tokens": [509, 727, 764, 8705, 88, 11, 457, 309, 311, 2138, 406, 9601, 281, 729, 13, 1743, 264, 1558, 11, 264, 2290, 2564, 307, 534, 264, 44094, 278, 926, 309, 13, 407, 498, 291, 584, 11, 4177, 11, 286, 4114, 452, 1065, 2316, 1228, 9953, 51, 284, 339, 293, 286, 576, 411, 281, 3847, 341, 11, 439, 291, 643, 281, 360, 307, 291, 643, 281, 362, 472, 2445, 300, 2516, 5110, 293, 9205, 552, 13], "temperature": 0.0, "avg_logprob": -0.1077544993924019, "compression_ratio": 1.7, "no_speech_prob": 0.00020143180154263973}, {"id": 888, "seek": 476450, "start": 4764.5, "end": 4774.5, "text": " And you need to have one function that takes raw text and outputs a score for each text. And then you provide that to Prodigy.", "tokens": [400, 291, 643, 281, 362, 472, 2445, 300, 2516, 8936, 2487, 293, 23930, 257, 6175, 337, 1184, 2487, 13, 400, 550, 291, 2893, 300, 281, 1705, 25259, 88, 13], "temperature": 0.0, "avg_logprob": -0.10308185815811158, "compression_ratio": 1.6231155778894473, "no_speech_prob": 0.00020973033679183573}, {"id": 889, "seek": 476450, "start": 4774.5, "end": 4788.5, "text": " And then you can use the same active learning mechanism as you would use with a built-in model. So the idea is really the models we ship are just a suggestion or an idea you can use to try it out.", "tokens": [400, 550, 291, 393, 764, 264, 912, 4967, 2539, 7513, 382, 291, 576, 764, 365, 257, 3094, 12, 259, 2316, 13, 407, 264, 1558, 307, 534, 264, 5245, 321, 5374, 366, 445, 257, 16541, 420, 364, 1558, 291, 393, 764, 281, 853, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.10308185815811158, "compression_ratio": 1.6231155778894473, "no_speech_prob": 0.00020973033679183573}, {"id": 890, "seek": 478850, "start": 4788.5, "end": 4799.5, "text": " But ultimately, we also hope that people in the future will transition to just plugging in their own model and just using the scaffolding around it to do that.", "tokens": [583, 6284, 11, 321, 611, 1454, 300, 561, 294, 264, 2027, 486, 6034, 281, 445, 42975, 294, 641, 1065, 2316, 293, 445, 1228, 264, 44094, 278, 926, 309, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.07573400844227184, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0001423084904672578}, {"id": 891, "seek": 478850, "start": 4799.5, "end": 4807.5, "text": " We definitely don't want to lock anyone in and say, oh, you have to use Spacey, especially for NER and stuff and other things. We think Spacey is pretty good.", "tokens": [492, 2138, 500, 380, 528, 281, 4017, 2878, 294, 293, 584, 11, 1954, 11, 291, 362, 281, 764, 8705, 88, 11, 2318, 337, 426, 1598, 293, 1507, 293, 661, 721, 13, 492, 519, 8705, 88, 307, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.07573400844227184, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0001423084904672578}, {"id": 892, "seek": 480750, "start": 4807.5, "end": 4819.5, "text": " But if you don't want to do that or for other use cases, especially text classification, we think there are a lot of cases where you might want to use scikit-learn or what? Valko? Wabbit? Yeah.", "tokens": [583, 498, 291, 500, 380, 528, 281, 360, 300, 420, 337, 661, 764, 3331, 11, 2318, 2487, 21538, 11, 321, 519, 456, 366, 257, 688, 295, 3331, 689, 291, 1062, 528, 281, 764, 2180, 22681, 12, 306, 1083, 420, 437, 30, 691, 667, 78, 30, 343, 455, 5260, 30, 865, 13], "temperature": 0.0, "avg_logprob": -0.270892682282821, "compression_ratio": 1.4602272727272727, "no_speech_prob": 6.778303213650361e-05}, {"id": 893, "seek": 480750, "start": 4819.5, "end": 4830.5, "text": " What a great name. Yeah. Basically something completely custom.", "tokens": [708, 257, 869, 1315, 13, 865, 13, 8537, 746, 2584, 2375, 13], "temperature": 0.0, "avg_logprob": -0.270892682282821, "compression_ratio": 1.4602272727272727, "no_speech_prob": 6.778303213650361e-05}, {"id": 894, "seek": 483050, "start": 4830.5, "end": 4841.5, "text": " So the question is active learning part, whether this is built on the underlying model.", "tokens": [407, 264, 1168, 307, 4967, 2539, 644, 11, 1968, 341, 307, 3094, 322, 264, 14217, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15174153484875644, "compression_ratio": 1.6715686274509804, "no_speech_prob": 0.00010186437430093065}, {"id": 895, "seek": 483050, "start": 4841.5, "end": 4849.5, "text": " I mean, so the question is active learning versus no active learning, how well this works. First, also maybe as a general introduction.", "tokens": [286, 914, 11, 370, 264, 1168, 307, 4967, 2539, 5717, 572, 4967, 2539, 11, 577, 731, 341, 1985, 13, 2386, 11, 611, 1310, 382, 257, 2674, 9339, 13], "temperature": 0.0, "avg_logprob": -0.15174153484875644, "compression_ratio": 1.6715686274509804, "no_speech_prob": 0.00010186437430093065}, {"id": 896, "seek": 483050, "start": 4849.5, "end": 4855.5, "text": " So what we're doing for most of the examples is we use a basic uncertainty sampling. That's what we found works best.", "tokens": [407, 437, 321, 434, 884, 337, 881, 295, 264, 5110, 307, 321, 764, 257, 3875, 15697, 21179, 13, 663, 311, 437, 321, 1352, 1985, 1151, 13], "temperature": 0.0, "avg_logprob": -0.15174153484875644, "compression_ratio": 1.6715686274509804, "no_speech_prob": 0.00010186437430093065}, {"id": 897, "seek": 485550, "start": 4855.5, "end": 4870.5, "text": " But we also know there are lots of other ways you could be solving that. So, you know, in the end, how we implement this is we have a simple function that takes a stream and outputs a sorted stream based on the assigned scores and the model in the loop.", "tokens": [583, 321, 611, 458, 456, 366, 3195, 295, 661, 2098, 291, 727, 312, 12606, 300, 13, 407, 11, 291, 458, 11, 294, 264, 917, 11, 577, 321, 4445, 341, 307, 321, 362, 257, 2199, 2445, 300, 2516, 257, 4309, 293, 23930, 257, 25462, 4309, 2361, 322, 264, 13279, 13444, 293, 264, 2316, 294, 264, 6367, 13], "temperature": 0.0, "avg_logprob": -0.11020862579345703, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.00022725018789060414}, {"id": 898, "seek": 485550, "start": 4870.5, "end": 4875.5, "text": " So how you wire this up again is also up to you.", "tokens": [407, 577, 291, 6234, 341, 493, 797, 307, 611, 493, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.11020862579345703, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.00022725018789060414}, {"id": 899, "seek": 487550, "start": 4875.5, "end": 4887.5, "text": " Yeah. To answer the part about what works best in general, in our kind of framework where you really you see one sentence at a time and often you start off with a model not knowing very much.", "tokens": [865, 13, 1407, 1867, 264, 644, 466, 437, 1985, 1151, 294, 2674, 11, 294, 527, 733, 295, 8388, 689, 291, 534, 291, 536, 472, 8174, 412, 257, 565, 293, 2049, 291, 722, 766, 365, 257, 2316, 406, 5276, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.1562505640009398, "compression_ratio": 1.6615384615384616, "no_speech_prob": 9.37082659220323e-05}, {"id": 900, "seek": 487550, "start": 4887.5, "end": 4901.5, "text": " The active learning component, basically resorting the stream is actually very crucial because otherwise if you start from scratch, have very few examples, you'll be annotating for a very, very long time and all kinds of random predictions.", "tokens": [440, 4967, 2539, 6542, 11, 1936, 19606, 278, 264, 4309, 307, 767, 588, 11462, 570, 5911, 498, 291, 722, 490, 8459, 11, 362, 588, 1326, 5110, 11, 291, 603, 312, 25339, 990, 337, 257, 588, 11, 588, 938, 565, 293, 439, 3685, 295, 4974, 21264, 13], "temperature": 0.0, "avg_logprob": -0.1562505640009398, "compression_ratio": 1.6615384615384616, "no_speech_prob": 9.37082659220323e-05}, {"id": 901, "seek": 490150, "start": 4901.5, "end": 4911.5, "text": " You annotate your your stream in order. There's very little you need some kind of guidance that tells you okay what to work on next, especially if you feed in millions of texts, you need to sort them.", "tokens": [509, 25339, 473, 428, 428, 4309, 294, 1668, 13, 821, 311, 588, 707, 291, 643, 512, 733, 295, 10056, 300, 5112, 291, 1392, 437, 281, 589, 322, 958, 11, 2318, 498, 291, 3154, 294, 6803, 295, 15765, 11, 291, 643, 281, 1333, 552, 13], "temperature": 0.0, "avg_logprob": -0.13482230542654014, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.00020182959269732237}, {"id": 902, "seek": 490150, "start": 4911.5, "end": 4919.5, "text": " You need to pre select them based on something. And this could be the models predictions. This could be something else. This could be the keywords or the patterns.", "tokens": [509, 643, 281, 659, 3048, 552, 2361, 322, 746, 13, 400, 341, 727, 312, 264, 5245, 21264, 13, 639, 727, 312, 746, 1646, 13, 639, 727, 312, 264, 21009, 420, 264, 8294, 13], "temperature": 0.0, "avg_logprob": -0.13482230542654014, "compression_ratio": 1.6930232558139535, "no_speech_prob": 0.00020182959269732237}, {"id": 903, "seek": 491950, "start": 4919.5, "end": 4933.5, "text": " But without that, yes, it's very, very difficult. And that's also that's kind of what we're trying to solve with the tool.", "tokens": [583, 1553, 300, 11, 2086, 11, 309, 311, 588, 11, 588, 2252, 13, 400, 300, 311, 611, 300, 311, 733, 295, 437, 321, 434, 1382, 281, 5039, 365, 264, 2290, 13], "temperature": 0.0, "avg_logprob": -0.19871762458314288, "compression_ratio": 1.3217391304347825, "no_speech_prob": 3.534239294822328e-05}, {"id": 904, "seek": 491950, "start": 4933.5, "end": 4935.5, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.19871762458314288, "compression_ratio": 1.3217391304347825, "no_speech_prob": 3.534239294822328e-05}, {"id": 905, "seek": 491950, "start": 4935.5, "end": 4938.5, "text": " Thank you so much.", "tokens": [1044, 291, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.19871762458314288, "compression_ratio": 1.3217391304347825, "no_speech_prob": 3.534239294822328e-05}, {"id": 906, "seek": 493850, "start": 4938.5, "end": 4950.5, "text": " I got to say, you know, anybody who's using fastai is anytime you've used fastai nlp or fastai.txt, you're called the spacey tokenize function.", "tokens": [286, 658, 281, 584, 11, 291, 458, 11, 4472, 567, 311, 1228, 2370, 1301, 307, 13038, 291, 600, 1143, 2370, 1301, 297, 75, 79, 420, 2370, 1301, 13, 83, 734, 11, 291, 434, 1219, 264, 1901, 88, 14862, 1125, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1787538306657658, "compression_ratio": 1.676923076923077, "no_speech_prob": 3.882345845340751e-05}, {"id": 907, "seek": 493850, "start": 4950.5, "end": 4962.5, "text": " You're using spacey behind the scenes and the reason you're using spacey is because I tried every damn tokenizer I could find and spaceys was like so much better than everything else.", "tokens": [509, 434, 1228, 1901, 88, 2261, 264, 8026, 293, 264, 1778, 291, 434, 1228, 1901, 88, 307, 570, 286, 3031, 633, 8151, 14862, 6545, 286, 727, 915, 293, 1901, 749, 390, 411, 370, 709, 1101, 813, 1203, 1646, 13], "temperature": 0.0, "avg_logprob": -0.1787538306657658, "compression_ratio": 1.676923076923077, "no_speech_prob": 3.882345845340751e-05}, {"id": 908, "seek": 496250, "start": 4962.5, "end": 4970.5, "text": " And the kind of story of fastai development is that over time I get sick of all the shitty parts of every third party library I find and I gradually rewrite them myself.", "tokens": [400, 264, 733, 295, 1657, 295, 2370, 1301, 3250, 307, 300, 670, 565, 286, 483, 4998, 295, 439, 264, 30748, 3166, 295, 633, 2636, 3595, 6405, 286, 915, 293, 286, 13145, 28132, 552, 2059, 13], "temperature": 0.0, "avg_logprob": -0.0975142777568162, "compression_ratio": 1.717741935483871, "no_speech_prob": 4.132703907089308e-05}, {"id": 909, "seek": 496250, "start": 4970.5, "end": 4980.5, "text": " And the fact that I haven't rewritten spacey or attempted to is because I actually think it's one of those rare pieces of software that doesn't suck at all is actually really good.", "tokens": [400, 264, 1186, 300, 286, 2378, 380, 319, 26859, 1901, 88, 420, 18997, 281, 307, 570, 286, 767, 519, 309, 311, 472, 295, 729, 5892, 3755, 295, 4722, 300, 1177, 380, 9967, 412, 439, 307, 767, 534, 665, 13], "temperature": 0.0, "avg_logprob": -0.0975142777568162, "compression_ratio": 1.717741935483871, "no_speech_prob": 4.132703907089308e-05}, {"id": 910, "seek": 496250, "start": 4980.5, "end": 4986.5, "text": " It's got good documentation and it's got a good install story and so forth.", "tokens": [467, 311, 658, 665, 14333, 293, 309, 311, 658, 257, 665, 3625, 1657, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.0975142777568162, "compression_ratio": 1.717741935483871, "no_speech_prob": 4.132703907089308e-05}, {"id": 911, "seek": 498650, "start": 4986.5, "end": 4996.5, "text": " And I haven't used prodigy but just the fact that these guys are working on, I recognize the importance of active learning and the importance of combining human plus machine.", "tokens": [400, 286, 2378, 380, 1143, 15792, 328, 88, 457, 445, 264, 1186, 300, 613, 1074, 366, 1364, 322, 11, 286, 5521, 264, 7379, 295, 4967, 2539, 293, 264, 7379, 295, 21928, 1952, 1804, 3479, 13], "temperature": 0.0, "avg_logprob": -0.13822446823120116, "compression_ratio": 1.6795366795366795, "no_speech_prob": 4.9065230996347964e-05}, {"id": 912, "seek": 498650, "start": 4996.5, "end": 5002.5, "text": " It puts them in that rare category of people who, in my opinion, are actually working on what's one of the most important problems today.", "tokens": [467, 8137, 552, 294, 300, 5892, 7719, 295, 561, 567, 11, 294, 452, 4800, 11, 366, 767, 1364, 322, 437, 311, 472, 295, 264, 881, 1021, 2740, 965, 13], "temperature": 0.0, "avg_logprob": -0.13822446823120116, "compression_ratio": 1.6795366795366795, "no_speech_prob": 4.9065230996347964e-05}, {"id": 913, "seek": 498650, "start": 5002.5, "end": 5009.5, "text": " So thank you both so much for coming and for this fantastic talk and I look forward to seeing what you do next.", "tokens": [407, 1309, 291, 1293, 370, 709, 337, 1348, 293, 337, 341, 5456, 751, 293, 286, 574, 2128, 281, 2577, 437, 291, 360, 958, 13], "temperature": 0.0, "avg_logprob": -0.13822446823120116, "compression_ratio": 1.6795366795366795, "no_speech_prob": 4.9065230996347964e-05}, {"id": 914, "seek": 500950, "start": 5009.5, "end": 5016.5, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50714], "temperature": 0.0, "avg_logprob": -0.29949456453323364, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.00068335747346282}], "language": "en"}