{"text": " I've got a question. Yeah. To do with, is there a way that machine learning can actually find the sort of conditional probabilistic segments that are say in sort of heterogeneous data? I am having trouble parsing that question. Can you give like an example or something? Yeah. Okay. All right. Well, I'm modeling with road surface friction with road risk rather. And kind of immediately there's a set of stereotypes in road analysis. And we all know that there's highways, freeways, urban turials. And they actually go through a series of stages. So they're almost like states. And each of the states has got a sort of conditional probabilistic relationship between the set of predictors and the actual response variable, the crash response variable. Is there anything that like that in deep learning? So how is that different to a normal predictive model? I mean, all predictive models are conditional probabilities, right? What's the difference here? Well, I mean, if you take say something like XGBoost, for example, and you want to predict a risk of a given road, say, it'll give you value. But then you've got no idea as to what's happened inside of the model. And now, I'm really, we're really interested in that because once you find the distributions, you can then start to do some quality testing on whether they actually follow the domain or whether your segmentation process that actually determines predictions is good or not. And so, in a way, rather than say predicting some sort of crash rate or risk or whatever, I'm really looking for those probabilistic distributions and working beneath the surface. So all deep learning models will return a set of probabilities. That's what their final layer returns. And then we decode them by taking the argmax across them. But there's nothing to stop you using those probabilities directly. I'm probably misunderstanding your question. It's a little abstract for me to understand. I mean, I know there's lots of things you can do with confidence intervals and whatnot, but really depends a great deal on the specific details of the application, what you're trying to do and how you're trying to do it. Good question, Daniel. I'm just, are you talking about the probability of an incident or risk related to the road surface? So you're going to need some sort of tabular data that has the occurrences with each road surface that you're trying to... And why wouldn't XGBoost give you that if you had a predictive model of incidents? Well, in my mind, at least one of the big disadvantages of XGBoost is the fact that it only gives you a single set of variable effects. Whereas in what we're dealing with, we've got some really high crash roads that have got a different conditional probability relationship between the predictors and the response to the accident. And so XGBoost does an excellent job in making the predictions, but you've got no idea as to group them, instances that are actually making the prediction or the actual variable effects. Okay. So I think I understand your question now. And I think the answer is actually it does. And what I suggest you do, if you haven't already, is read the chapter of the FASTA AI book on tabular modelling. And it will cover something very similar, which is random forests, which is another ensemble of decision trees. And it will show you how to get exactly the kind of insights that I think you're looking for. And all of the techniques there would work equally well for random forests and they also work equally well for deep learning. So maybe after you've done that, you can come back and let us know whether that helped. Yeah, sure. Yeah, well, I've sort of played with random forests, David, and it doesn't really give me what I'm looking for. I strongly suggest you read the chapter before you see that. I will, I will. Because I'm pretty sure it will. And if it doesn't, that would be very interesting to me. In fact, I mentioned to you last time that I'm really looking forward to the tabular data. Cool. Great. I'll show you guys what I've been working on, which has been fun. So the first thing I did, you know, after I got off our last call was I basically just threw together the kind of like most obvious basic steps one would do for a standard image recognition competition, just in order to show people that that can be quite good. And it was actually a little embarrassing because I didn't mean to do this. When I submitted it, it turned out to be a little bit of a mess. And I was like, well, I'm going to do this. And I was like, well, I'm going to do this. And I was like, well, I'm going to do this. When I submitted it, it turned out I got first on the leaderboard. So now I feel like I'm going to have to write down exactly what I did because, you know, during an active competition, everybody needs to share what they're doing if they share it with anybody semi publicly. So I thought I'd show you what I did here. But I think this is about to go up quite a lot because, you know, what we're doing here is where there are interesting images for a couple of reasons. One is that they're kind of like things that you see in ImageNet, like they're pictures of natural objects, they're photos. But I don't think ImageNet has any kind of like categories about diseases. You know, they have categories about like, what's the main object in this? So they might have a category about like, I don't know if they do, like some different kinds of grass or some different types of, even some different types of, you know, fields or something. But I'm pretty sure they don't have anything about different kinds of crop disease. So it's a bit different to ImageNet. And I think it's a bit different to ImageNet, which is what most of our pre-trained models are trained on. But it's not that different. And it's also interesting because nearly all of the images are the same shape and size. So we can kind of try to take advantage of that. And, you know, so when we fine tune a pre-trained model, there's, so let me pull up this Kaggle notebook I just created. So I just published this yesterday. Kind of look at what are the best vision models for fine tuning. And so I kind of realized that there are two key dimensions that really seem to impact how well a model can be fine tuned, you know, whether it works well or not, or how it's different. So one is what I just talked about, which is how similar is your data set to the data set used for the pre-trained model? If it's really similar, like pets to ImageNet, then like the critical factor is how well does the fine tuning of the model maintain the weights that are pre-trained, you know, because you're probably not going to be changing very, very much. And you're probably going to be able to take advantage of really big accurate models, because they've already learned to do almost the exact same thing. So that's a really good point. And then the other thing that I want to talk about is the exact thing you're trying to do. On the other hand, so that's the pets data set. On the other hand, there's a data set called the planet data set, which is satellite images. And these are not really at all like anything that ImageNet ever saw. You know, they're taken from above, they're taken from much further away. There's no single main object. So a lot of the weights of a pre-trained model are going to be useless for fine tuning this, because they've learned specific features like, you know, what does text look like? What do eyeballs look like? What does fur look like? You know, which none of which are going to be very useful. So that's the first dimension. The second dimension is just how big the data set is. So on a big data set, you've got time, you've got epochs to take advantage of having lots of parameters in the model to learn to use them effectively. And if you don't have much data, then you don't have much ability to do that. So you might imagine that deep learning practitioners already know these answers of what's the best models for fine tuning. But in fact, we don't. As far as I know, nobody's ever done an analysis before of which models are the best for fine tuning. So that's what I did over the weekend. And not just over the weekend, but really over the last couple of weeks. And I did this with Thomas Capell, who works at Weights at Biases, another Fast AI community member slash alumni. And so what we did was we tried fine tuning lots of models on two data sets, one which has 10 times over 10 times less images. And where those images are not at all like ImageNet, that being the Kaggle planet sample, and one which is a very, very small sample, and one which is a very small sample, and one which is a lot like ImageNet and has a lot more images that being IoT pets. And I kind of figured like if we get some insights from those two, perhaps there'll be something that we can leverage more generally. So Thomas wrote this script, which it's 86 lines, but really there's only like three or four lines and they're all lines you recognize, right? The lines are untar data, image data loaders dot from blah, and then vision learner, DLs, model, etc. So there's the normal like three or four lines of code we see three or four lines of code we see over and over again. And then you know the rest of it is basically lets you pass into the script different choices about batch size, epochs, and so forth. And that's about it. So this is like how simple the script was that we used. And then partly because Thomas works at weights and biases and partly because weights and biases is pretty cool, we used weights and biases then to feed in different values for each of those parameters. So this is a YAML file that weights and biases uses where you can say, okay, try each of these different learning rates, try each of these different models, try, let's see if I can find another one, try each of these different resize methods, each of these different pooling methods, this distribution of learning rates, you know, whatever, and it goes away and tries them. And then you can use their web GUI to look at like the training results. So then you basically say, okay, start training and it trains each of these models over each of these data sets with each of these poor values and each of these resize methods. And if you look at the data sets, methods and a few different selections from this distribution of learning rates and creates a web GUI that you can dive into. I personally hate web GUIs. I would much rather use Python. So, but they also thankfully have an API. So, yeah, so once we ran that script for a few hours, I then checked the results into a, into a gist. So a gist is just a place to chuck text files, basically, if you haven't used it before. So I can, I chucked my CSV file in here. As you can see, it kind of displays it in a nice way, or you can just click on raw to see the raw data. And then you can see the data in a nice way, or you can just click on raw to see the raw data. So I find that quite a nice place just to chuck things, which I'm just going to share publicly. And so then I can, because there's the URL to the gist. And maybe, let me show you how I did that. Right. So I just kind of like everything to be automated so I can always easily redo it, because I always assume my first effort is going to be crap, and it always is. And normally my second, third efforts are crap as well. So here's my little notebook I put together. So basically each time you do one of these sweeps on weights and biases, it generates a new ID. And so we ended up kind of doing five different ones as we realized we were able to add different models and change things a little bit. And so they have this API that you can use. And so they basically can go through and say, go through each of the sweep IDs and ask the API for that sweep and grab the runs from it. And then for each one create a dictionary containing a summary and the model name. So the details don't matter too much, but you kind of get the idea hopefully, and then turn that into a data frame. And so I kind of end up with this data frame that contains all the different configuration parameters along with their loss and their speed, their accuracy, GPU, maximum memory usage and so forth. So that's basically what I wanted to chuck into a gist. And so specifically I really wanted this subset of the columns. So these are the columns I wanted. So I can grab those columns and put them into a CSV. Now one thing you might not realize is I would say for most Python libraries or at least most well written ones, anyway you can put a file name. So normally when you say to CSV you put here a file name or a path. You can instead put something called a string IO object, which is something that behaves exactly like a file, but it actually just stores it into a string. Because I don't want this stored into a file. It's put into a string. So if you then call.getValue, I actually get the string. And so even things like creating the gist, I want to do that automatically. So there's a library I'm very fond of. I'm very biased because I made it called ghapi, which is an API for GitHub where we can do things like say create gist and you give it a name. And then you can do things like create gist and you give it a description. And here's the text, which is the contents of the CSV. And the file name, make it public. And then you can get the HTML URL of the gist. So that's how I used in this case a notebook as my kind of interactive read of our print loop for manipulating this data set, putting it together and then uploading it to GitHub. Jermey, I had a doubt in this Pandas data frame. Here you have like in your data, I just took a look at your gist and it had in the data set entries with planned and that other data set, the pet status. So how did you populate it? Sorry, what's your question? How did I populate this data set? Yeah, that Pandas data frame. Yeah, just here. So I passed it a list of dictionaries. And the list of dictionaries I created using a list comprehension containing a bunch of dictionaries. Okay, got it. And so that's going to make each key, so that means all the dictionaries. Should have, you know, roughly the same keys, anyone's that are missing, they're going to end up being NA. And then I just fiddled around with it slightly. So for example, so I made sure everything had an error rate that was equal to one minus the accuracy. On the planet data set is not called accuracy. So I copied accuracy multi into accuracy. Yeah, nothing very exciting. Thank you. No worries. Jeremy, what's the actual goal of this? Let me show you. So what we've now got is a CSV, which I can then Okay, a CSV, which I can then use Pandas pivot table functionality to group by the data set, the model family and name, and calculate the min of error rate fit to the data set. And I can then take the pets subset of that. Sort by score where score represents a combination of error and speed and take the top 15. And this now shows the error rate of the data set. Sort by score where score represents a combination of error and speed and take the top 15. And this now shows me the top 15 best models for fine tuning on pets. And this is this is gold, in my opinion, I don't think anybody's ever done anything like this before there's never been a list of like pure the best models for fine tuning. And yeah, sorry, I have a question. So you you you fine tune different models with pets and then collected this information, is that correct? That's correct. And then based on the information that you collected from the fine tune of five or whatever number of iterations. Three runs for each model. Yes. And and then you collected this information to find out which one is is the best behave model for this specific case. Correct, correct, correct, correct. Exactly. And the best is going to involve two things. It's going to be which ones have the lowest error rate and which ones are the fastest. Now, I created this kind of arbitrary scoring function where I multiplied the error rate times fit time plus 80. Just because I felt like that particular value of that constant gave me an ordering that I was reasonably comfortable with. But you can kind of look through here and see like, OK, well, the IT base has a much better error rate than conv next tiny. But it's also much slower, like you can decide for your needs where you want to trade off. So that's what I kind of the first thing I did was to create this kind of top 15. And it's interesting looking at the family, right. The family is like each of these different architectures, you know, is kind of from, you know, from different sizes of a smaller subset of families. Right. So there's conv next tiny, conv next base, conv next tiny and 22K and so forth. So you can kind of get a sense of like, if you want to learn more about architectures, which one seemed most interesting to you. And, you know, for fine tuning on pets, it looks like conv next, viet, swin, resnet are the main ones. So that's the first thing I did. The second thing I then did was to take those most interesting families. I actually also added this one called regnetx and created a scatter plot of them. Colored by family. And so you can kind of see, like, for example, conv next, which I'm rather fond of. Is these this kind of blue line, these blue ones, right. And so you can see that the very best error rate actually was a conv next. So they're pretty good. You can see this one here, which is regnetx seems to be have some pretty nice values. They're like super fast. Seems like these tiny swings seem to be pretty good. So it kind of gives you a sense of like, you know, depending on how much time you've got to run or how accurate you want to be. What families are likely to most useful. And then the last thing I did for pets was I grabbed a subset of the basically the ones which are in the top, basically smaller than the median and faster than the median, because these are the ones I generally care about most of the time, because most of the time I'm going to be, you know, training quick iterations. And so and then I just ordered those by error rate. And so conv next tiny has got the best error rate of those which are in the upper half of both speed and accuracy. What's the GPU memory in this context? That's the maximum amount of GPU memory that was used. I can't remember what the units of measure are, but they don't matter too much because it'll be different for your data set. All that matters is the relative usage. And so if you want something, you know, if you if you try to use this and it's actually uses too much GPU memory, you could try ResNet 50D, for example. Or, you know, it's interesting that like ResNet 26 is really good for memory and speed. Or if you want something really lightweight on memory RegNet Y004. But the error rates are getting much worse once you get out to here, as you can see. So then so then I looked at Planet and so, as I said, Planet's kind of as different a data set as you're going to get in one sense. Or it's very different. And so not surprisingly, its top 15 is also very different. And interestingly. All of the top six from the same family. So this VIT family, these are kind of model called Transformers models. And what this is basically showing is that these models are particularly good at rapidly identifying features of data types it hasn't seen before. So, you know, if you're doing something like medical imaging or satellite imagery or something like that, these would probably be a good thing to try. And Swin, by the way, is kind of another Transformers based model, which, as you can see, it's actually the most accurate of all. But it's also the slowest. This is Swin V2. So I thought that was pretty interesting. And, you know, these VIT models, there are ones with pretty good error rates that also have very little memory use and also run very quickly. So I did the same thing for Planet. And so perhaps not surprisingly, but interestingly for Planet, these these lines don't necessarily go down, which is to say that the really big models, the big slow models don't don't necessarily have better error rates. And that makes sense, right? Because if they've got heaps of parameters, but they're trying to learn something they've never seen before on very little data, it's unlikely we're going to be able to take advantage of those parameters. So when you're doing stuff that doesn't really look much like ImageNet, you might want to be down more towards this end. So here's the VIT, for example, and here's that really good Swin model. And there's ConvNext Tiny. So then we can do the same thing again of like, OK, let's take the top half both in terms of speed and memory use. ConvNext Tiny still looks good. These VIT models, this 224, yeah, this is because you can only run these models on images of size 224 by 224. They're not you can't use different sizes, whereas the ConvNext models you can use any size. So it's also interesting to see that classic ResNet still, again, they do pretty well. Yeah, so I'm pretty excited about this. It feels like exactly what we need to. Kick ass on this Patty Doctor competition or indeed. Any kind of. Computer vision classification task needs this and. I ran this week on. Three consumer RTX GPUs in 12 hours or something like this is not. Big institutional resources required. And one of the reasons why is because. I didn't. Try every possible level of everything right? I tried. A couple of you know, so. Thomas did a kind of a quick learning rate sweep to kind of get a sense of the broad range of learning rates that seem pretty good. And then we just tried a couple of learning rates and a couple of the best resize methods and a couple of the best polling types across a few broadly different kinds of models across the two different data sets. To kind of see if there was any common features and we found in every single case, the same learning rate, the same resource method and the same polling type was the best. So we didn't need to try every possible combination of everything, you know, and this is where, like a lot of the. Stuff you see from like Google and stuff, they tend to do hundreds of thousands of experiments, because I guess because they don't they have no need to do things efficiently right? Yeah, but you don't have to do it the Google way. You can do it the fast AI way. Quick quick question, Jeremy, which which cards did you use? And another question is why do you think? Yeah, the GPU cards. Oh, I think that you 90. Oh, OK, so they were all three different. They're all the same card, they're all RTX 3090s. OK, and you reset the index after the query. Why? Oh, just because otherwise it shows the numeric ID here will be the numeric ID from the original data set. And I wanted to be able to quickly kind of say what's number six, what's number 10, what's number three. That's all. Yeah. Jeremy, getting back getting back to the Earth satellite images, when you say, you know, like the classification, what is it trying to classify? In this case, the planet competition. We have some examples. Basically, they try to classify for each area of the satellite imagery. What's it a picture of? Where is it forest or farmland or town or whatever? And what weather conditions to observe, if I remember correctly? Question in this image space, is it just these two major data sets or how do you find other models that are trained on beside the planet and ImageNet? You mean beside planet and pets? Sorry, yeah, that's it. And so what is your question? How do you do what with them? How do you find other trained pre-trained models that have been worked on? Well, these all use pre-trained models, pre-trained on ImageNet. These are only using pre-trained models, pre-trained on ImageNet. So how do you find pre-trained models, pre-trained on other things? Mainly you don't. There aren't many. But, you know, just Google. Depends what you're interested in and academic papers. There is a, I don't know how it's doing. It's there was a model zoo. So there is a model zoo, which I've never had much success with, to be honest. So these are a range of pre-trained models that you can download. Yeah, but as I say, I haven't found it particularly successful, to be honest. You could also try papers with papers with code. And I think these. Yeah, they have a link to the paper and the code. That doesn't necessarily mean they've got a pre-trained model. But then you can just click on the code and see. And of course, for NLP models, there's the hugging face model hub, which we've seen before. And that is an easy answer for NLP. It's like lots of different pre-trained models are on that hub. Jeremy, since you touch on academic papers and papers with code. First question, this comparison, do you or Toma intend to publish it? If not, if you were to do that, what would you go for actually? What kind of journal would you look for? So I'm not a good person to ask that question because I very rarely publish anything, which is partly a philosophical thing. I find academia overly exclusive and I don't love PDFs as a publication form. And I don't love the writing style, which is kind of required if you're going to get published as being like rather difficult to follow. I have published a couple of papers, but only really one significant deep learning one. And that was because a guy named Sebastian Ruter was doing his PhD at the time. And he said it would be really helpful to him if we could co-publish something and that he would kind of take the lead on writing the paper. And so that was good because I'm always very happy to help students. And he did a good job and he was a terrific researcher to work with. The other time I've written a paper, the main time was when I wanted to get the message out about masks. And I felt like it probably not going to be taken seriously unless it's an exclusive academic paper because medical people are very into exclusive things. Yeah. So I don't know. Like I'd say like this kind of thing I suspect would be quite hard to publish because most deep learning academic venues are very focused on things with kind of reasonably strong theoretical pieces and this kind of field of like trying things and seeing what works is, you know, content-based is certainly a very important part of science in other areas. But in the deep learning world, it hasn't really yet been recognised as a valid source of research as far as I can tell. Oh, I could concur with all the domains and feel the same quandary to be honest here. Fair enough. What's your domain? Hydrology, but more the computational science part of it. Yeah. Okay. So then what I did was I, I mean, this is kind of a bit at the same time, but I went back to Patty. And I wanted to try out a few of these interesting looking models reasonably quickly. So what I did was I kind of took our standard, well, in this case, three lines of code because I've already untarget earlier. Took our three lines of code. So I could basically say train and pass in an architecture and pass in some per item pre-processing, in this case, resizing everything to the same square using Squish and some per batch pre-processing, which in this case is the standard FASTA data augmentation transforms targeting a final size of 224, which is what most models tend to be trained at. And so then train a model using those parameters. And then finally, it would use test time augmentation. So test time augmentation is where I think we briefly mentioned it last time. We on this case on the validation set, I basically run the model, the fine tuned model four times using random data augmentations each time. And then I run it one more time with no data augmentations at all and take an average of all of those five predictions basically. And that gives me some predictions and then I take an error rate for TTA, for the test time augmentation. So that basically spits out a number, which is an error rate for Patty. And I use a fixed random seed when picking out my validation set. So each time I run this, it's going to be with the same validation set. So I can compare. So I've got a few different next small models I've run. First of all, by squishing when I resize and then by cropping when I resize. So that was 235. This is also 235. And then instead of resizing to a square, I resize to a rectangle. In theory, this wouldn't have been necessary. I thought they were all 480 by 680, sorry, 480 by 640. But when I ran this, I got an error. And then I looked back at the results of that parallel image sizing thing we ran and I realized there was actually three or four images that were the opposite aspect ratio. So that's why. So the vast majority of the images, this resizing does nothing at all. But there's three or four that are the opposite aspect ratio. And then for the augmentation, yeah, pick a size based on 224 of a similar aspect ratio. But what I'm actually aiming for here is something that is a multiple of 32 on both edges. And the reason for that we'll kind of get into later when we learn about how convolutional networks really work. But it basically turns out that the kind of the final patch size in a conv net is 32 by 32 pixels. So you generally want both of your sides, normally you want them to be multiples of 32. So this one, you've got a pretty similar result again, 240. And then, you know, I wasn't sure about my contention that they need to be multiples of 32. I thought maybe it's better if they like a really crisp resizing by using an exact multiple. So I tried that as well. And that, as I suspected, was a bit worse. And oh, what's this? I've got some which which ones are the right way around. Now I'm confused. I think let's check. Some of these originally I had my aspect ratio backwards. That's why I've got both. It looks like I never got around to removing the ones that were unnecessary. To even things in the right order by adding these. Just. It's. As. method equals add. Oops. Pad mode. This makes it a bit easier to see what's going on if you do padding with black around them. It's a bit of a pain in the ass. There we go. Okay, yeah, so you can clearly see this is the wrong way around, right? I've tried to make them wide, but actually they were tall. So the best way around is actually 640 by 480. That's more like it. So 640 by 480 is best. So let's get rid of the ones that were the wrong way around. Okay. All right. Yeah, so that was all, you know, various different transforms, pre-processing for ConvNext Small, and then I did the same thing for one of the VITs, VIT Small. Now VIT, remember I mentioned it can only work on 224 by 224 images, so these rectangular approaches aren't going to be possible. So I've just got the Squish and the Crop versions. The Crop version doesn't look very good. The Squish version does look pretty good. And I also tried a Pad version, which looks pretty good. And then, yeah, I also tried Swin. So here's Swin V2. And this one is slow and memory intensive. So I had to go down to the 192 pixel version. But actually it seems to work very well. This is the first time we've had one that's better than 0.02. It is interesting. This one's also very good. So it's interesting that this slow memory intensive model works better even on smaller size, 192 pixel size, which I think is pretty interesting. And then there's one more Swin which seemed to do pretty well. So I included that, which I was able to do it 224. That one had pretty good results. So like I kind of did that for all these different small models. And as you can see, they run pretty quickly, right? Five or ten minutes. And so then I picked out the ones that look pretty fast, sorry, pretty fast, pretty accurate. And created just a copy of that, which I called Paddy Large. And this time I just replaced small with large. And actually I've made a mistake. I'm going to have to rerun this because there should be not, there should not be a C equals 42. I actually want to run this on a different subset each time. And the reason why is my plan is to train. So basically what I did is I deleted the ones that were less good in Paddy Small. And so now just running the large ones. Now some of these, particularly something like this one, which is 288 by 224. They ran out of memory. They were too big for my graphics card. And a lot of people at this point say, oh, I need to go buy a more expensive graphics card. But that's not true. You don't. So, if you guys remember our training loop. We get the gradients. We add the gradients times the learning rate to the weights. And then we zero the gradients. What you could do is half the batch size. So, for example, from 64 to 32. And then only zero the gradients every two iterations. Right. And so, and only do the update every two iterations. So basically you can calculate in two batches what you used to calculate in one batch, and it will be mathematically identical. And that's called gradient accumulation. And so for the ones which ran out of memory, I added this little accume equals true. Which is here in my function. And I said, yeah, I said if accume equals true, then set the batch size to 32. Because by default it's 64. And add this thing called a callback. Callbacks are basically things that change the behavior of the training. And there's a thing called gradient accumulation callback. Which gradient accumulation. And this is like just for people that are interested. This is not like massively complex stuff. The entire gradient accumulation callback is that many lines of code. These are not big things. And like literally all it does is it keeps account of how many iterations it's been. And it adds the, you know, keeps track of the count. And as long as we're not up to the point where we there's the number of accumulations we want, we skip the step and the zero gradient basically. So it's yeah, things like gradient accumulation, they sound like big complex things, but they turn out not to be, at least when you have a nice code base like fast AIs. Jeremy, can I get a question here? Of course. How exactly did the batch size mass nations work? So we will get into that in detail in the course. And it certainly we get into it in detail in the book. But basically all that happens is we randomly shuffle the data set and we grab the batch sizes 64. We grab the next 64 images. We resize them all to be the same size and we stack them on top of each other. So if it's black and white images, for example, we would have 64 whatever, 640 by 480 images. And so we would end up with a 64 by 640 by 480 tensor. And pretty much all the like functionality provided by Tytorch will work fine for a mini batch of things, just as it would for a single thing on the whole. So in the in the large scheme of things, you know, like some huge processes that's trying to characterize what roles the batch sort of claim. Well, it's just it's just about trying to get the most out of your GPU. Your GPU can do 10,000 things at once. So if you just give it one image at a time, you can use it. Yeah, I got you. So if you give it 64 things, it can do one thing on each image and then on each channel in that image. And then you don't have enough you other kind of degrees of paralyzation it can do. And so that's where you start. You know, we saw that Nvidia SMI daemon command that shows you the utilization of your symmetric multiprocessor. If you use a batch size of one, you'll see that SM will be like 1%, 2%, and everything will be slow. It's a bit tricky at inference time, you know, in production or whatever, because you know, most of the time you only get one thing to do at a time. And so often inference is done on CPU rather than GPU because we don't get to benefit from batching. Or, you know, people will queue a few of them up and stick them all in the GPU at once and, you know, stuff like that. But yeah, for training, it's pretty easy to take advantage of many batches. Thank you. No worries. Jeremy, you've trained so many models. Will you consider using a majority vote or something like that? No, I wouldn't because a majority vote throws away information. It throws away the probabilities. So I pretty much always find I get better results by averaging the probabilities. So each of them, each of the models after I've trained it, I'm exporting to a uniquely named model, which is going to be the name of the architecture and then an underscore and then some description, which is just the thing I pass in. And so that way, yeah, when I'm done training, I can just have a little loop which opens each of those models up, grabs the TTA predictions, sticks them into our list. And then at the end, I'll average those TTA predictions across the models. And that will be my on some board prediction. So that's my next step. I'm not up to that yet. All right. Well, I think that's it. So that's really more of a like little update on what I've been doing over my weekend. But hopefully, yeah, gives you some ideas for things to try. And hopefully you find the Kaggle notebook useful. So, Jeremy, so how many hours did you spend in all these experimentations because you spend a lot of experience here? So, you know, it's like a week or two of work to do the fine tuning experiments. But that was like a few hours here and a few hours there. The final sweep was probably maybe six hours of three GPUs. The patty competition stuff was maybe four hours a day over the last four days since I last saw you guys. And writing the notebook was maybe another four hours. Thanks. It helps. No worries. All right. Bye, everybody. Nice to see you all. Thanks, Jeremy. Bye, everyone.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 1.92, "text": " I've got a question. Yeah.", "tokens": [286, 600, 658, 257, 1168, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.19679774099321506, "compression_ratio": 1.4438502673796791, "no_speech_prob": 0.007335844449698925}, {"id": 1, "seek": 0, "start": 3.92, "end": 9.44, "text": " To do with, is there a way that machine learning can actually find the", "tokens": [1407, 360, 365, 11, 307, 456, 257, 636, 300, 3479, 2539, 393, 767, 915, 264], "temperature": 0.0, "avg_logprob": -0.19679774099321506, "compression_ratio": 1.4438502673796791, "no_speech_prob": 0.007335844449698925}, {"id": 2, "seek": 0, "start": 10.56, "end": 18.32, "text": " sort of conditional probabilistic segments that are say in sort of heterogeneous data?", "tokens": [1333, 295, 27708, 31959, 3142, 19904, 300, 366, 584, 294, 1333, 295, 20789, 31112, 1412, 30], "temperature": 0.0, "avg_logprob": -0.19679774099321506, "compression_ratio": 1.4438502673796791, "no_speech_prob": 0.007335844449698925}, {"id": 3, "seek": 0, "start": 20.8, "end": 24.96, "text": " I am having trouble parsing that question. Can you give like an example or something?", "tokens": [286, 669, 1419, 5253, 21156, 278, 300, 1168, 13, 1664, 291, 976, 411, 364, 1365, 420, 746, 30], "temperature": 0.0, "avg_logprob": -0.19679774099321506, "compression_ratio": 1.4438502673796791, "no_speech_prob": 0.007335844449698925}, {"id": 4, "seek": 2496, "start": 24.96, "end": 33.04, "text": " Yeah. Okay. All right. Well, I'm modeling with road surface friction with road risk rather.", "tokens": [865, 13, 1033, 13, 1057, 558, 13, 1042, 11, 286, 478, 15983, 365, 3060, 3753, 17710, 365, 3060, 3148, 2831, 13], "temperature": 0.0, "avg_logprob": -0.29682386094245355, "compression_ratio": 1.4945652173913044, "no_speech_prob": 0.00022961903596296906}, {"id": 5, "seek": 2496, "start": 34.480000000000004, "end": 42.400000000000006, "text": " And kind of immediately there's a set of stereotypes in road analysis. And we all know", "tokens": [400, 733, 295, 4258, 456, 311, 257, 992, 295, 30853, 294, 3060, 5215, 13, 400, 321, 439, 458], "temperature": 0.0, "avg_logprob": -0.29682386094245355, "compression_ratio": 1.4945652173913044, "no_speech_prob": 0.00022961903596296906}, {"id": 6, "seek": 2496, "start": 42.400000000000006, "end": 51.84, "text": " that there's highways, freeways, urban turials. And they actually go through a series of stages.", "tokens": [300, 456, 311, 43747, 11, 1737, 942, 11, 9681, 3243, 12356, 13, 400, 436, 767, 352, 807, 257, 2638, 295, 10232, 13], "temperature": 0.0, "avg_logprob": -0.29682386094245355, "compression_ratio": 1.4945652173913044, "no_speech_prob": 0.00022961903596296906}, {"id": 7, "seek": 5184, "start": 51.84, "end": 60.480000000000004, "text": " So they're almost like states. And each of the states has got a sort of conditional probabilistic", "tokens": [407, 436, 434, 1920, 411, 4368, 13, 400, 1184, 295, 264, 4368, 575, 658, 257, 1333, 295, 27708, 31959, 3142], "temperature": 0.0, "avg_logprob": -0.1729259831564767, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.00013967820268590003}, {"id": 8, "seek": 5184, "start": 60.480000000000004, "end": 67.76, "text": " relationship between the set of predictors and the actual response variable, the crash response", "tokens": [2480, 1296, 264, 992, 295, 6069, 830, 293, 264, 3539, 4134, 7006, 11, 264, 8252, 4134], "temperature": 0.0, "avg_logprob": -0.1729259831564767, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.00013967820268590003}, {"id": 9, "seek": 5184, "start": 67.76, "end": 74.80000000000001, "text": " variable. Is there anything that like that in deep learning?", "tokens": [7006, 13, 1119, 456, 1340, 300, 411, 300, 294, 2452, 2539, 30], "temperature": 0.0, "avg_logprob": -0.1729259831564767, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.00013967820268590003}, {"id": 10, "seek": 7480, "start": 74.8, "end": 83.28, "text": " So how is that different to a normal predictive model? I mean, all predictive models are conditional", "tokens": [407, 577, 307, 300, 819, 281, 257, 2710, 35521, 2316, 30, 286, 914, 11, 439, 35521, 5245, 366, 27708], "temperature": 0.0, "avg_logprob": -0.17654280132717556, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.00011576402903301641}, {"id": 11, "seek": 7480, "start": 83.28, "end": 88.24, "text": " probabilities, right? What's the difference here?", "tokens": [33783, 11, 558, 30, 708, 311, 264, 2649, 510, 30], "temperature": 0.0, "avg_logprob": -0.17654280132717556, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.00011576402903301641}, {"id": 12, "seek": 7480, "start": 88.24, "end": 93.2, "text": " Well, I mean, if you take say something like XGBoost, for example, and you want to predict a", "tokens": [1042, 11, 286, 914, 11, 498, 291, 747, 584, 746, 411, 1783, 8769, 78, 555, 11, 337, 1365, 11, 293, 291, 528, 281, 6069, 257], "temperature": 0.0, "avg_logprob": -0.17654280132717556, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.00011576402903301641}, {"id": 13, "seek": 7480, "start": 93.92, "end": 100.8, "text": " risk of a given road, say, it'll give you value. But then you've got no idea as to what's happened", "tokens": [3148, 295, 257, 2212, 3060, 11, 584, 11, 309, 603, 976, 291, 2158, 13, 583, 550, 291, 600, 658, 572, 1558, 382, 281, 437, 311, 2011], "temperature": 0.0, "avg_logprob": -0.17654280132717556, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.00011576402903301641}, {"id": 14, "seek": 10080, "start": 100.8, "end": 110.32, "text": " inside of the model. And now, I'm really, we're really interested in that because once you find", "tokens": [1854, 295, 264, 2316, 13, 400, 586, 11, 286, 478, 534, 11, 321, 434, 534, 3102, 294, 300, 570, 1564, 291, 915], "temperature": 0.0, "avg_logprob": -0.17283335328102112, "compression_ratio": 1.5585106382978724, "no_speech_prob": 7.712857041042298e-05}, {"id": 15, "seek": 10080, "start": 110.32, "end": 116.72, "text": " the distributions, you can then start to do some quality testing on whether they actually follow", "tokens": [264, 37870, 11, 291, 393, 550, 722, 281, 360, 512, 3125, 4997, 322, 1968, 436, 767, 1524], "temperature": 0.0, "avg_logprob": -0.17283335328102112, "compression_ratio": 1.5585106382978724, "no_speech_prob": 7.712857041042298e-05}, {"id": 16, "seek": 10080, "start": 116.72, "end": 124.4, "text": " the domain or whether your segmentation process that actually determines predictions is good or not.", "tokens": [264, 9274, 420, 1968, 428, 9469, 399, 1399, 300, 767, 24799, 21264, 307, 665, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.17283335328102112, "compression_ratio": 1.5585106382978724, "no_speech_prob": 7.712857041042298e-05}, {"id": 17, "seek": 12440, "start": 124.4, "end": 132.32, "text": " And so, in a way, rather than say predicting some sort of", "tokens": [400, 370, 11, 294, 257, 636, 11, 2831, 813, 584, 32884, 512, 1333, 295], "temperature": 0.0, "avg_logprob": -0.21999614888971503, "compression_ratio": 1.3161764705882353, "no_speech_prob": 2.8393222237355076e-05}, {"id": 18, "seek": 12440, "start": 134.96, "end": 142.08, "text": " crash rate or risk or whatever, I'm really looking for those probabilistic", "tokens": [8252, 3314, 420, 3148, 420, 2035, 11, 286, 478, 534, 1237, 337, 729, 31959, 3142], "temperature": 0.0, "avg_logprob": -0.21999614888971503, "compression_ratio": 1.3161764705882353, "no_speech_prob": 2.8393222237355076e-05}, {"id": 19, "seek": 12440, "start": 143.12, "end": 146.24, "text": " distributions and working beneath the surface.", "tokens": [37870, 293, 1364, 17149, 264, 3753, 13], "temperature": 0.0, "avg_logprob": -0.21999614888971503, "compression_ratio": 1.3161764705882353, "no_speech_prob": 2.8393222237355076e-05}, {"id": 20, "seek": 14624, "start": 146.24, "end": 154.4, "text": " So all deep learning models will return a set of probabilities. That's what their", "tokens": [407, 439, 2452, 2539, 5245, 486, 2736, 257, 992, 295, 33783, 13, 663, 311, 437, 641], "temperature": 0.0, "avg_logprob": -0.16130772829055787, "compression_ratio": 1.6009389671361502, "no_speech_prob": 5.255028099782066e-06}, {"id": 21, "seek": 14624, "start": 155.36, "end": 161.04000000000002, "text": " final layer returns. And then we decode them by taking the argmax across them. But there's", "tokens": [2572, 4583, 11247, 13, 400, 550, 321, 979, 1429, 552, 538, 1940, 264, 3882, 41167, 2108, 552, 13, 583, 456, 311], "temperature": 0.0, "avg_logprob": -0.16130772829055787, "compression_ratio": 1.6009389671361502, "no_speech_prob": 5.255028099782066e-06}, {"id": 22, "seek": 14624, "start": 161.04000000000002, "end": 168.72, "text": " nothing to stop you using those probabilities directly. I'm probably misunderstanding your", "tokens": [1825, 281, 1590, 291, 1228, 729, 33783, 3838, 13, 286, 478, 1391, 29227, 428], "temperature": 0.0, "avg_logprob": -0.16130772829055787, "compression_ratio": 1.6009389671361502, "no_speech_prob": 5.255028099782066e-06}, {"id": 23, "seek": 14624, "start": 169.52, "end": 174.72, "text": " question. It's a little abstract for me to understand. I mean, I know there's", "tokens": [1168, 13, 467, 311, 257, 707, 12649, 337, 385, 281, 1223, 13, 286, 914, 11, 286, 458, 456, 311], "temperature": 0.0, "avg_logprob": -0.16130772829055787, "compression_ratio": 1.6009389671361502, "no_speech_prob": 5.255028099782066e-06}, {"id": 24, "seek": 17472, "start": 174.72, "end": 184.16, "text": " lots of things you can do with confidence intervals and whatnot, but", "tokens": [3195, 295, 721, 291, 393, 360, 365, 6687, 26651, 293, 25882, 11, 457], "temperature": 0.0, "avg_logprob": -0.1292643345577616, "compression_ratio": 1.5396825396825398, "no_speech_prob": 3.37315650540404e-05}, {"id": 25, "seek": 17472, "start": 185.6, "end": 189.52, "text": " really depends a great deal on the specific details of the application,", "tokens": [534, 5946, 257, 869, 2028, 322, 264, 2685, 4365, 295, 264, 3861, 11], "temperature": 0.0, "avg_logprob": -0.1292643345577616, "compression_ratio": 1.5396825396825398, "no_speech_prob": 3.37315650540404e-05}, {"id": 26, "seek": 17472, "start": 189.52, "end": 192.32, "text": " what you're trying to do and how you're trying to do it.", "tokens": [437, 291, 434, 1382, 281, 360, 293, 577, 291, 434, 1382, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.1292643345577616, "compression_ratio": 1.5396825396825398, "no_speech_prob": 3.37315650540404e-05}, {"id": 27, "seek": 17472, "start": 193.68, "end": 200.56, "text": " Good question, Daniel. I'm just, are you talking about the probability of an incident or risk", "tokens": [2205, 1168, 11, 8033, 13, 286, 478, 445, 11, 366, 291, 1417, 466, 264, 8482, 295, 364, 9348, 420, 3148], "temperature": 0.0, "avg_logprob": -0.1292643345577616, "compression_ratio": 1.5396825396825398, "no_speech_prob": 3.37315650540404e-05}, {"id": 28, "seek": 20056, "start": 200.56, "end": 207.6, "text": " related to the road surface? So you're going to need some sort of tabular data that has", "tokens": [4077, 281, 264, 3060, 3753, 30, 407, 291, 434, 516, 281, 643, 512, 1333, 295, 4421, 1040, 1412, 300, 575], "temperature": 0.0, "avg_logprob": -0.22760510053790983, "compression_ratio": 1.4625, "no_speech_prob": 6.201467476785183e-05}, {"id": 29, "seek": 20056, "start": 208.48, "end": 213.52, "text": " the occurrences with each road surface that you're trying to...", "tokens": [264, 5160, 38983, 365, 1184, 3060, 3753, 300, 291, 434, 1382, 281, 485], "temperature": 0.0, "avg_logprob": -0.22760510053790983, "compression_ratio": 1.4625, "no_speech_prob": 6.201467476785183e-05}, {"id": 30, "seek": 20056, "start": 214.4, "end": 223.76, "text": " And why wouldn't XGBoost give you that if you had a predictive model of incidents?", "tokens": [400, 983, 2759, 380, 1783, 8769, 78, 555, 976, 291, 300, 498, 291, 632, 257, 35521, 2316, 295, 21139, 30], "temperature": 0.0, "avg_logprob": -0.22760510053790983, "compression_ratio": 1.4625, "no_speech_prob": 6.201467476785183e-05}, {"id": 31, "seek": 22376, "start": 223.76, "end": 228.95999999999998, "text": " Well, in my mind, at least one of the big disadvantages of XGBoost is the fact that", "tokens": [1042, 11, 294, 452, 1575, 11, 412, 1935, 472, 295, 264, 955, 37431, 295, 1783, 8769, 78, 555, 307, 264, 1186, 300], "temperature": 0.0, "avg_logprob": -0.35149058317526793, "compression_ratio": 1.5420560747663552, "no_speech_prob": 0.00012131537368986756}, {"id": 32, "seek": 22376, "start": 228.95999999999998, "end": 237.04, "text": " it only gives you a single set of variable effects. Whereas in what we're dealing with,", "tokens": [309, 787, 2709, 291, 257, 2167, 992, 295, 7006, 5065, 13, 13813, 294, 437, 321, 434, 6260, 365, 11], "temperature": 0.0, "avg_logprob": -0.35149058317526793, "compression_ratio": 1.5420560747663552, "no_speech_prob": 0.00012131537368986756}, {"id": 33, "seek": 22376, "start": 238.39999999999998, "end": 245.6, "text": " we've got some really high crash roads that have got a different conditional probability", "tokens": [321, 600, 658, 512, 534, 1090, 8252, 11344, 300, 362, 658, 257, 819, 27708, 8482], "temperature": 0.0, "avg_logprob": -0.35149058317526793, "compression_ratio": 1.5420560747663552, "no_speech_prob": 0.00012131537368986756}, {"id": 34, "seek": 22376, "start": 245.6, "end": 252.0, "text": " relationship between the predictors and the response to the accident.", "tokens": [2480, 1296, 264, 6069, 830, 293, 264, 4134, 281, 264, 6398, 13], "temperature": 0.0, "avg_logprob": -0.35149058317526793, "compression_ratio": 1.5420560747663552, "no_speech_prob": 0.00012131537368986756}, {"id": 35, "seek": 25200, "start": 252.0, "end": 263.04, "text": " And so XGBoost does an excellent job in making the predictions, but you've got no idea as to", "tokens": [400, 370, 1783, 8769, 78, 555, 775, 364, 7103, 1691, 294, 1455, 264, 21264, 11, 457, 291, 600, 658, 572, 1558, 382, 281], "temperature": 0.0, "avg_logprob": -0.29341700497795553, "compression_ratio": 1.516304347826087, "no_speech_prob": 5.1392406021477655e-05}, {"id": 36, "seek": 25200, "start": 263.04, "end": 272.72, "text": " group them, instances that are actually making the prediction or the actual variable effects.", "tokens": [1594, 552, 11, 14519, 300, 366, 767, 1455, 264, 17630, 420, 264, 3539, 7006, 5065, 13], "temperature": 0.0, "avg_logprob": -0.29341700497795553, "compression_ratio": 1.516304347826087, "no_speech_prob": 5.1392406021477655e-05}, {"id": 37, "seek": 25200, "start": 274.08, "end": 279.6, "text": " Okay. So I think I understand your question now. And I think the answer is actually it does.", "tokens": [1033, 13, 407, 286, 519, 286, 1223, 428, 1168, 586, 13, 400, 286, 519, 264, 1867, 307, 767, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.29341700497795553, "compression_ratio": 1.516304347826087, "no_speech_prob": 5.1392406021477655e-05}, {"id": 38, "seek": 27960, "start": 279.6, "end": 285.68, "text": " And what I suggest you do, if you haven't already, is read the chapter of the FASTA AI book on", "tokens": [400, 437, 286, 3402, 291, 360, 11, 498, 291, 2378, 380, 1217, 11, 307, 1401, 264, 7187, 295, 264, 479, 3160, 8241, 7318, 1446, 322], "temperature": 0.0, "avg_logprob": -0.2023316929849346, "compression_ratio": 1.5739130434782609, "no_speech_prob": 6.701840175082907e-05}, {"id": 39, "seek": 27960, "start": 285.68, "end": 292.88, "text": " tabular modelling. And it will cover something very similar, which is random forests, which is", "tokens": [4421, 1040, 42253, 13, 400, 309, 486, 2060, 746, 588, 2531, 11, 597, 307, 4974, 21700, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.2023316929849346, "compression_ratio": 1.5739130434782609, "no_speech_prob": 6.701840175082907e-05}, {"id": 40, "seek": 27960, "start": 292.88, "end": 299.28000000000003, "text": " another ensemble of decision trees. And it will show you how to get exactly the kind of", "tokens": [1071, 19492, 295, 3537, 5852, 13, 400, 309, 486, 855, 291, 577, 281, 483, 2293, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.2023316929849346, "compression_ratio": 1.5739130434782609, "no_speech_prob": 6.701840175082907e-05}, {"id": 41, "seek": 27960, "start": 300.8, "end": 305.44, "text": " insights that I think you're looking for. And all of the techniques there would work", "tokens": [14310, 300, 286, 519, 291, 434, 1237, 337, 13, 400, 439, 295, 264, 7512, 456, 576, 589], "temperature": 0.0, "avg_logprob": -0.2023316929849346, "compression_ratio": 1.5739130434782609, "no_speech_prob": 6.701840175082907e-05}, {"id": 42, "seek": 30544, "start": 305.44, "end": 309.84, "text": " equally well for random forests and they also work equally well for deep learning. So maybe after", "tokens": [12309, 731, 337, 4974, 21700, 293, 436, 611, 589, 12309, 731, 337, 2452, 2539, 13, 407, 1310, 934], "temperature": 0.0, "avg_logprob": -0.27803681536418634, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.0002512361388653517}, {"id": 43, "seek": 30544, "start": 309.84, "end": 313.04, "text": " you've done that, you can come back and let us know whether that helped.", "tokens": [291, 600, 1096, 300, 11, 291, 393, 808, 646, 293, 718, 505, 458, 1968, 300, 4254, 13], "temperature": 0.0, "avg_logprob": -0.27803681536418634, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.0002512361388653517}, {"id": 44, "seek": 30544, "start": 313.04, "end": 319.76, "text": " Yeah, sure. Yeah, well, I've sort of played with random forests, David, and it doesn't really", "tokens": [865, 11, 988, 13, 865, 11, 731, 11, 286, 600, 1333, 295, 3737, 365, 4974, 21700, 11, 4389, 11, 293, 309, 1177, 380, 534], "temperature": 0.0, "avg_logprob": -0.27803681536418634, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.0002512361388653517}, {"id": 45, "seek": 30544, "start": 321.76, "end": 322.8, "text": " give me what I'm looking for.", "tokens": [976, 385, 437, 286, 478, 1237, 337, 13], "temperature": 0.0, "avg_logprob": -0.27803681536418634, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.0002512361388653517}, {"id": 46, "seek": 30544, "start": 322.8, "end": 325.76, "text": " I strongly suggest you read the chapter before you see that.", "tokens": [286, 10613, 3402, 291, 1401, 264, 7187, 949, 291, 536, 300, 13], "temperature": 0.0, "avg_logprob": -0.27803681536418634, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.0002512361388653517}, {"id": 47, "seek": 30544, "start": 325.76, "end": 326.32, "text": " I will, I will.", "tokens": [286, 486, 11, 286, 486, 13], "temperature": 0.0, "avg_logprob": -0.27803681536418634, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.0002512361388653517}, {"id": 48, "seek": 30544, "start": 326.32, "end": 331.68, "text": " Because I'm pretty sure it will. And if it doesn't, that would be very interesting to me.", "tokens": [1436, 286, 478, 1238, 988, 309, 486, 13, 400, 498, 309, 1177, 380, 11, 300, 576, 312, 588, 1880, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.27803681536418634, "compression_ratio": 1.7074074074074075, "no_speech_prob": 0.0002512361388653517}, {"id": 49, "seek": 33168, "start": 331.68, "end": 339.76, "text": " In fact, I mentioned to you last time that I'm really looking forward to the tabular data.", "tokens": [682, 1186, 11, 286, 2835, 281, 291, 1036, 565, 300, 286, 478, 534, 1237, 2128, 281, 264, 4421, 1040, 1412, 13], "temperature": 0.0, "avg_logprob": -0.30460838476816815, "compression_ratio": 1.352, "no_speech_prob": 0.00017035743803717196}, {"id": 50, "seek": 33168, "start": 339.76, "end": 352.24, "text": " Cool. Great. I'll show you guys what I've been working on, which has been fun.", "tokens": [8561, 13, 3769, 13, 286, 603, 855, 291, 1074, 437, 286, 600, 668, 1364, 322, 11, 597, 575, 668, 1019, 13], "temperature": 0.0, "avg_logprob": -0.30460838476816815, "compression_ratio": 1.352, "no_speech_prob": 0.00017035743803717196}, {"id": 51, "seek": 35224, "start": 352.24, "end": 358.64, "text": " So the first thing I did, you know, after I got off our last call was I basically just threw together", "tokens": [407, 264, 700, 551, 286, 630, 11, 291, 458, 11, 934, 286, 658, 766, 527, 1036, 818, 390, 286, 1936, 445, 11918, 1214], "temperature": 0.0, "avg_logprob": -0.37139804188798115, "compression_ratio": 1.3559322033898304, "no_speech_prob": 1.3844154636899475e-05}, {"id": 52, "seek": 35224, "start": 358.64, "end": 367.28000000000003, "text": " the kind of like most obvious basic steps one would do for", "tokens": [264, 733, 295, 411, 881, 6322, 3875, 4439, 472, 576, 360, 337], "temperature": 0.0, "avg_logprob": -0.37139804188798115, "compression_ratio": 1.3559322033898304, "no_speech_prob": 1.3844154636899475e-05}, {"id": 53, "seek": 36728, "start": 367.28, "end": 379.03999999999996, "text": " a standard image recognition competition, just in order to show people that that can be quite good.", "tokens": [257, 3832, 3256, 11150, 6211, 11, 445, 294, 1668, 281, 855, 561, 300, 300, 393, 312, 1596, 665, 13], "temperature": 0.0, "avg_logprob": -0.573807532970722, "compression_ratio": 1.927461139896373, "no_speech_prob": 1.5443960364791565e-05}, {"id": 54, "seek": 36728, "start": 379.03999999999996, "end": 384.47999999999996, "text": " And it was actually a little embarrassing because I didn't mean to do this. When I submitted it,", "tokens": [400, 309, 390, 767, 257, 707, 17299, 570, 286, 994, 380, 914, 281, 360, 341, 13, 1133, 286, 14405, 309, 11], "temperature": 0.0, "avg_logprob": -0.573807532970722, "compression_ratio": 1.927461139896373, "no_speech_prob": 1.5443960364791565e-05}, {"id": 55, "seek": 36728, "start": 384.47999999999996, "end": 390.47999999999996, "text": " it turned out to be a little bit of a mess. And I was like, well, I'm going to do this.", "tokens": [309, 3574, 484, 281, 312, 257, 707, 857, 295, 257, 2082, 13, 400, 286, 390, 411, 11, 731, 11, 286, 478, 516, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.573807532970722, "compression_ratio": 1.927461139896373, "no_speech_prob": 1.5443960364791565e-05}, {"id": 56, "seek": 36728, "start": 390.47999999999996, "end": 395.67999999999995, "text": " And I was like, well, I'm going to do this. And I was like, well, I'm going to do this.", "tokens": [400, 286, 390, 411, 11, 731, 11, 286, 478, 516, 281, 360, 341, 13, 400, 286, 390, 411, 11, 731, 11, 286, 478, 516, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.573807532970722, "compression_ratio": 1.927461139896373, "no_speech_prob": 1.5443960364791565e-05}, {"id": 57, "seek": 39568, "start": 395.68, "end": 399.36, "text": " When I submitted it, it turned out I got first on the leaderboard.", "tokens": [1133, 286, 14405, 309, 11, 309, 3574, 484, 286, 658, 700, 322, 264, 5263, 3787, 13], "temperature": 0.0, "avg_logprob": -0.11741466736525633, "compression_ratio": 1.5377777777777777, "no_speech_prob": 2.93055363727035e-05}, {"id": 58, "seek": 39568, "start": 400.88, "end": 404.96, "text": " So now I feel like I'm going to have to write down exactly what I did because,", "tokens": [407, 586, 286, 841, 411, 286, 478, 516, 281, 362, 281, 2464, 760, 2293, 437, 286, 630, 570, 11], "temperature": 0.0, "avg_logprob": -0.11741466736525633, "compression_ratio": 1.5377777777777777, "no_speech_prob": 2.93055363727035e-05}, {"id": 59, "seek": 39568, "start": 407.2, "end": 414.48, "text": " you know, during an active competition, everybody needs to share what they're doing if they share it", "tokens": [291, 458, 11, 1830, 364, 4967, 6211, 11, 2201, 2203, 281, 2073, 437, 436, 434, 884, 498, 436, 2073, 309], "temperature": 0.0, "avg_logprob": -0.11741466736525633, "compression_ratio": 1.5377777777777777, "no_speech_prob": 2.93055363727035e-05}, {"id": 60, "seek": 39568, "start": 414.48, "end": 421.52, "text": " with anybody semi publicly. So I thought I'd show you what I did here. But I think this is about to", "tokens": [365, 4472, 12909, 14843, 13, 407, 286, 1194, 286, 1116, 855, 291, 437, 286, 630, 510, 13, 583, 286, 519, 341, 307, 466, 281], "temperature": 0.0, "avg_logprob": -0.11741466736525633, "compression_ratio": 1.5377777777777777, "no_speech_prob": 2.93055363727035e-05}, {"id": 61, "seek": 42152, "start": 421.52, "end": 429.91999999999996, "text": " go up quite a lot because, you know, what we're doing here is where", "tokens": [352, 493, 1596, 257, 688, 570, 11, 291, 458, 11, 437, 321, 434, 884, 510, 307, 689], "temperature": 0.0, "avg_logprob": -0.1345902094765315, "compression_ratio": 1.5408805031446542, "no_speech_prob": 2.42945461650379e-05}, {"id": 62, "seek": 42152, "start": 434.4, "end": 439.91999999999996, "text": " there are interesting images for a couple of reasons. One is that they're kind of like things", "tokens": [456, 366, 1880, 5267, 337, 257, 1916, 295, 4112, 13, 1485, 307, 300, 436, 434, 733, 295, 411, 721], "temperature": 0.0, "avg_logprob": -0.1345902094765315, "compression_ratio": 1.5408805031446542, "no_speech_prob": 2.42945461650379e-05}, {"id": 63, "seek": 42152, "start": 439.91999999999996, "end": 445.44, "text": " that you see in ImageNet, like they're pictures of natural objects, they're photos.", "tokens": [300, 291, 536, 294, 29903, 31890, 11, 411, 436, 434, 5242, 295, 3303, 6565, 11, 436, 434, 5787, 13], "temperature": 0.0, "avg_logprob": -0.1345902094765315, "compression_ratio": 1.5408805031446542, "no_speech_prob": 2.42945461650379e-05}, {"id": 64, "seek": 44544, "start": 445.44, "end": 452.0, "text": " But I don't think ImageNet has any kind of like categories about diseases. You know, they have", "tokens": [583, 286, 500, 380, 519, 29903, 31890, 575, 604, 733, 295, 411, 10479, 466, 11044, 13, 509, 458, 11, 436, 362], "temperature": 0.0, "avg_logprob": -0.1923791771261101, "compression_ratio": 1.9790794979079498, "no_speech_prob": 2.0143852452747524e-05}, {"id": 65, "seek": 44544, "start": 452.0, "end": 456.71999999999997, "text": " categories about like, what's the main object in this? So they might have a category about like,", "tokens": [10479, 466, 411, 11, 437, 311, 264, 2135, 2657, 294, 341, 30, 407, 436, 1062, 362, 257, 7719, 466, 411, 11], "temperature": 0.0, "avg_logprob": -0.1923791771261101, "compression_ratio": 1.9790794979079498, "no_speech_prob": 2.0143852452747524e-05}, {"id": 66, "seek": 44544, "start": 456.71999999999997, "end": 461.44, "text": " I don't know if they do, like some different kinds of grass or some different types of,", "tokens": [286, 500, 380, 458, 498, 436, 360, 11, 411, 512, 819, 3685, 295, 8054, 420, 512, 819, 3467, 295, 11], "temperature": 0.0, "avg_logprob": -0.1923791771261101, "compression_ratio": 1.9790794979079498, "no_speech_prob": 2.0143852452747524e-05}, {"id": 67, "seek": 44544, "start": 461.44, "end": 468.24, "text": " even some different types of, you know, fields or something. But I'm pretty sure they don't have", "tokens": [754, 512, 819, 3467, 295, 11, 291, 458, 11, 7909, 420, 746, 13, 583, 286, 478, 1238, 988, 436, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.1923791771261101, "compression_ratio": 1.9790794979079498, "no_speech_prob": 2.0143852452747524e-05}, {"id": 68, "seek": 44544, "start": 468.24, "end": 474.08, "text": " anything about different kinds of crop disease. So it's a bit different to ImageNet. And I think", "tokens": [1340, 466, 819, 3685, 295, 9086, 4752, 13, 407, 309, 311, 257, 857, 819, 281, 29903, 31890, 13, 400, 286, 519], "temperature": 0.0, "avg_logprob": -0.1923791771261101, "compression_ratio": 1.9790794979079498, "no_speech_prob": 2.0143852452747524e-05}, {"id": 69, "seek": 47408, "start": 474.08, "end": 479.35999999999996, "text": " it's a bit different to ImageNet, which is what most of our pre-trained models are trained on.", "tokens": [309, 311, 257, 857, 819, 281, 29903, 31890, 11, 597, 307, 437, 881, 295, 527, 659, 12, 17227, 2001, 5245, 366, 8895, 322, 13], "temperature": 0.0, "avg_logprob": -0.09792068730229916, "compression_ratio": 1.4857142857142858, "no_speech_prob": 4.356705176178366e-06}, {"id": 70, "seek": 47408, "start": 480.64, "end": 486.0, "text": " But it's not that different. And it's also interesting because nearly all of the images are", "tokens": [583, 309, 311, 406, 300, 819, 13, 400, 309, 311, 611, 1880, 570, 6217, 439, 295, 264, 5267, 366], "temperature": 0.0, "avg_logprob": -0.09792068730229916, "compression_ratio": 1.4857142857142858, "no_speech_prob": 4.356705176178366e-06}, {"id": 71, "seek": 47408, "start": 486.8, "end": 491.36, "text": " the same shape and size. So we can kind of try to take advantage of that.", "tokens": [264, 912, 3909, 293, 2744, 13, 407, 321, 393, 733, 295, 853, 281, 747, 5002, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.09792068730229916, "compression_ratio": 1.4857142857142858, "no_speech_prob": 4.356705176178366e-06}, {"id": 72, "seek": 49136, "start": 491.36, "end": 501.44, "text": " And, you know, so when we fine tune a pre-trained model, there's, so let me pull up this Kaggle", "tokens": [400, 11, 291, 458, 11, 370, 562, 321, 2489, 10864, 257, 659, 12, 17227, 2001, 2316, 11, 456, 311, 11, 370, 718, 385, 2235, 493, 341, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.3051288646200429, "compression_ratio": 1.2580645161290323, "no_speech_prob": 3.90535296901362e-06}, {"id": 73, "seek": 49136, "start": 501.44, "end": 517.28, "text": " notebook I just created. So I just published this yesterday.", "tokens": [21060, 286, 445, 2942, 13, 407, 286, 445, 6572, 341, 5186, 13], "temperature": 0.0, "avg_logprob": -0.3051288646200429, "compression_ratio": 1.2580645161290323, "no_speech_prob": 3.90535296901362e-06}, {"id": 74, "seek": 51728, "start": 517.28, "end": 521.6, "text": " Kind of look at what are the best vision models for fine tuning. And so I kind of realized that", "tokens": [9242, 295, 574, 412, 437, 366, 264, 1151, 5201, 5245, 337, 2489, 15164, 13, 400, 370, 286, 733, 295, 5334, 300], "temperature": 0.0, "avg_logprob": -0.1895893414815267, "compression_ratio": 1.603448275862069, "no_speech_prob": 1.7330296486761654e-06}, {"id": 75, "seek": 51728, "start": 521.6, "end": 527.36, "text": " there are two key dimensions that really seem to impact how well a model can be fine tuned,", "tokens": [456, 366, 732, 2141, 12819, 300, 534, 1643, 281, 2712, 577, 731, 257, 2316, 393, 312, 2489, 10870, 11], "temperature": 0.0, "avg_logprob": -0.1895893414815267, "compression_ratio": 1.603448275862069, "no_speech_prob": 1.7330296486761654e-06}, {"id": 76, "seek": 51728, "start": 527.36, "end": 532.24, "text": " you know, whether it works well or not, or how it's different. So one is what I just talked about,", "tokens": [291, 458, 11, 1968, 309, 1985, 731, 420, 406, 11, 420, 577, 309, 311, 819, 13, 407, 472, 307, 437, 286, 445, 2825, 466, 11], "temperature": 0.0, "avg_logprob": -0.1895893414815267, "compression_ratio": 1.603448275862069, "no_speech_prob": 1.7330296486761654e-06}, {"id": 77, "seek": 51728, "start": 532.24, "end": 539.4399999999999, "text": " which is how similar is your data set to the data set used for the pre-trained model?", "tokens": [597, 307, 577, 2531, 307, 428, 1412, 992, 281, 264, 1412, 992, 1143, 337, 264, 659, 12, 17227, 2001, 2316, 30], "temperature": 0.0, "avg_logprob": -0.1895893414815267, "compression_ratio": 1.603448275862069, "no_speech_prob": 1.7330296486761654e-06}, {"id": 78, "seek": 53944, "start": 539.44, "end": 548.5600000000001, "text": " If it's really similar, like pets to ImageNet, then like the critical factor is how well does", "tokens": [759, 309, 311, 534, 2531, 11, 411, 19897, 281, 29903, 31890, 11, 550, 411, 264, 4924, 5952, 307, 577, 731, 775], "temperature": 0.0, "avg_logprob": -0.3641646675441576, "compression_ratio": 1.7184115523465704, "no_speech_prob": 2.5612662284402177e-06}, {"id": 79, "seek": 53944, "start": 548.5600000000001, "end": 554.4000000000001, "text": " the fine tuning of the model maintain the weights that are pre-trained, you know, because you're", "tokens": [264, 2489, 15164, 295, 264, 2316, 6909, 264, 17443, 300, 366, 659, 12, 17227, 2001, 11, 291, 458, 11, 570, 291, 434], "temperature": 0.0, "avg_logprob": -0.3641646675441576, "compression_ratio": 1.7184115523465704, "no_speech_prob": 2.5612662284402177e-06}, {"id": 80, "seek": 53944, "start": 554.4000000000001, "end": 557.84, "text": " probably not going to be changing very, very much. And you're probably going to be able to take", "tokens": [1391, 406, 516, 281, 312, 4473, 588, 11, 588, 709, 13, 400, 291, 434, 1391, 516, 281, 312, 1075, 281, 747], "temperature": 0.0, "avg_logprob": -0.3641646675441576, "compression_ratio": 1.7184115523465704, "no_speech_prob": 2.5612662284402177e-06}, {"id": 81, "seek": 53944, "start": 557.84, "end": 563.2, "text": " advantage of really big accurate models, because they've already learned to do almost the exact", "tokens": [5002, 295, 534, 955, 8559, 5245, 11, 570, 436, 600, 1217, 3264, 281, 360, 1920, 264, 1900], "temperature": 0.0, "avg_logprob": -0.3641646675441576, "compression_ratio": 1.7184115523465704, "no_speech_prob": 2.5612662284402177e-06}, {"id": 82, "seek": 53944, "start": 563.2, "end": 569.0400000000001, "text": " same thing. So that's a really good point. And then the other thing that I want to talk about", "tokens": [912, 551, 13, 407, 300, 311, 257, 534, 665, 935, 13, 400, 550, 264, 661, 551, 300, 286, 528, 281, 751, 466], "temperature": 0.0, "avg_logprob": -0.3641646675441576, "compression_ratio": 1.7184115523465704, "no_speech_prob": 2.5612662284402177e-06}, {"id": 83, "seek": 56904, "start": 569.04, "end": 575.28, "text": " is the exact thing you're trying to do. On the other hand, so that's the pets data set. On the", "tokens": [307, 264, 1900, 551, 291, 434, 1382, 281, 360, 13, 1282, 264, 661, 1011, 11, 370, 300, 311, 264, 19897, 1412, 992, 13, 1282, 264], "temperature": 0.0, "avg_logprob": -0.16242627093666478, "compression_ratio": 1.625, "no_speech_prob": 1.0952695447485894e-05}, {"id": 84, "seek": 56904, "start": 575.28, "end": 587.52, "text": " other hand, there's a data set called the planet data set, which is satellite images. And these are", "tokens": [661, 1011, 11, 456, 311, 257, 1412, 992, 1219, 264, 5054, 1412, 992, 11, 597, 307, 16016, 5267, 13, 400, 613, 366], "temperature": 0.0, "avg_logprob": -0.16242627093666478, "compression_ratio": 1.625, "no_speech_prob": 1.0952695447485894e-05}, {"id": 85, "seek": 56904, "start": 587.52, "end": 593.4399999999999, "text": " not really at all like anything that ImageNet ever saw. You know, they're taken from above,", "tokens": [406, 534, 412, 439, 411, 1340, 300, 29903, 31890, 1562, 1866, 13, 509, 458, 11, 436, 434, 2726, 490, 3673, 11], "temperature": 0.0, "avg_logprob": -0.16242627093666478, "compression_ratio": 1.625, "no_speech_prob": 1.0952695447485894e-05}, {"id": 86, "seek": 59344, "start": 593.44, "end": 603.44, "text": " they're taken from much further away. There's no single main object. So a lot of the weights of a", "tokens": [436, 434, 2726, 490, 709, 3052, 1314, 13, 821, 311, 572, 2167, 2135, 2657, 13, 407, 257, 688, 295, 264, 17443, 295, 257], "temperature": 0.0, "avg_logprob": -0.14458506447928293, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.0615839528327342e-05}, {"id": 87, "seek": 59344, "start": 603.44, "end": 609.44, "text": " pre-trained model are going to be useless for fine tuning this, because they've learned specific", "tokens": [659, 12, 17227, 2001, 2316, 366, 516, 281, 312, 14115, 337, 2489, 15164, 341, 11, 570, 436, 600, 3264, 2685], "temperature": 0.0, "avg_logprob": -0.14458506447928293, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.0615839528327342e-05}, {"id": 88, "seek": 59344, "start": 609.9200000000001, "end": 616.32, "text": " features like, you know, what does text look like? What do eyeballs look like? What does fur look", "tokens": [4122, 411, 11, 291, 458, 11, 437, 775, 2487, 574, 411, 30, 708, 360, 43758, 574, 411, 30, 708, 775, 2687, 574], "temperature": 0.0, "avg_logprob": -0.14458506447928293, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.0615839528327342e-05}, {"id": 89, "seek": 59344, "start": 616.32, "end": 621.36, "text": " like? You know, which none of which are going to be very useful. So that's the first dimension.", "tokens": [411, 30, 509, 458, 11, 597, 6022, 295, 597, 366, 516, 281, 312, 588, 4420, 13, 407, 300, 311, 264, 700, 10139, 13], "temperature": 0.0, "avg_logprob": -0.14458506447928293, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.0615839528327342e-05}, {"id": 90, "seek": 62136, "start": 621.36, "end": 627.6, "text": " The second dimension is just how big the data set is. So on a big data set, you've got time,", "tokens": [440, 1150, 10139, 307, 445, 577, 955, 264, 1412, 992, 307, 13, 407, 322, 257, 955, 1412, 992, 11, 291, 600, 658, 565, 11], "temperature": 0.0, "avg_logprob": -0.083879025777181, "compression_ratio": 1.5932203389830508, "no_speech_prob": 4.78499259770615e-06}, {"id": 91, "seek": 62136, "start": 628.32, "end": 639.36, "text": " you've got epochs to take advantage of having lots of parameters in the model to learn to use", "tokens": [291, 600, 658, 30992, 28346, 281, 747, 5002, 295, 1419, 3195, 295, 9834, 294, 264, 2316, 281, 1466, 281, 764], "temperature": 0.0, "avg_logprob": -0.083879025777181, "compression_ratio": 1.5932203389830508, "no_speech_prob": 4.78499259770615e-06}, {"id": 92, "seek": 62136, "start": 639.36, "end": 644.48, "text": " them effectively. And if you don't have much data, then you don't have much ability to do that.", "tokens": [552, 8659, 13, 400, 498, 291, 500, 380, 362, 709, 1412, 11, 550, 291, 500, 380, 362, 709, 3485, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.083879025777181, "compression_ratio": 1.5932203389830508, "no_speech_prob": 4.78499259770615e-06}, {"id": 93, "seek": 64448, "start": 644.48, "end": 650.88, "text": " So you might imagine that deep learning practitioners already know these answers of", "tokens": [407, 291, 1062, 3811, 300, 2452, 2539, 25742, 1217, 458, 613, 6338, 295], "temperature": 0.0, "avg_logprob": -0.17926730466692636, "compression_ratio": 1.6822429906542056, "no_speech_prob": 8.397396413784008e-06}, {"id": 94, "seek": 64448, "start": 650.88, "end": 656.24, "text": " what's the best models for fine tuning. But in fact, we don't. As far as I know, nobody's", "tokens": [437, 311, 264, 1151, 5245, 337, 2489, 15164, 13, 583, 294, 1186, 11, 321, 500, 380, 13, 1018, 1400, 382, 286, 458, 11, 5079, 311], "temperature": 0.0, "avg_logprob": -0.17926730466692636, "compression_ratio": 1.6822429906542056, "no_speech_prob": 8.397396413784008e-06}, {"id": 95, "seek": 64448, "start": 656.24, "end": 661.28, "text": " ever done an analysis before of which models are the best for fine tuning. So that's what I did over", "tokens": [1562, 1096, 364, 5215, 949, 295, 597, 5245, 366, 264, 1151, 337, 2489, 15164, 13, 407, 300, 311, 437, 286, 630, 670], "temperature": 0.0, "avg_logprob": -0.17926730466692636, "compression_ratio": 1.6822429906542056, "no_speech_prob": 8.397396413784008e-06}, {"id": 96, "seek": 64448, "start": 661.28, "end": 666.4, "text": " the weekend. And not just over the weekend, but really over the last couple of weeks.", "tokens": [264, 6711, 13, 400, 406, 445, 670, 264, 6711, 11, 457, 534, 670, 264, 1036, 1916, 295, 3259, 13], "temperature": 0.0, "avg_logprob": -0.17926730466692636, "compression_ratio": 1.6822429906542056, "no_speech_prob": 8.397396413784008e-06}, {"id": 97, "seek": 66640, "start": 666.4, "end": 673.68, "text": " And I did this with Thomas Capell, who works at Weights at Biases, another Fast AI community member", "tokens": [400, 286, 630, 341, 365, 8500, 27517, 285, 11, 567, 1985, 412, 492, 5761, 412, 13007, 1957, 11, 1071, 15968, 7318, 1768, 4006], "temperature": 0.0, "avg_logprob": -0.4404958089192708, "compression_ratio": 1.6446280991735538, "no_speech_prob": 8.800067917036358e-06}, {"id": 98, "seek": 66640, "start": 673.68, "end": 682.4, "text": " slash alumni. And so what we did was we tried fine tuning lots of models on two data sets, one which", "tokens": [17330, 16347, 13, 400, 370, 437, 321, 630, 390, 321, 3031, 2489, 15164, 3195, 295, 5245, 322, 732, 1412, 6352, 11, 472, 597], "temperature": 0.0, "avg_logprob": -0.4404958089192708, "compression_ratio": 1.6446280991735538, "no_speech_prob": 8.800067917036358e-06}, {"id": 99, "seek": 66640, "start": 683.12, "end": 689.52, "text": " has 10 times over 10 times less images. And where those images are not at all like ImageNet, that", "tokens": [575, 1266, 1413, 670, 1266, 1413, 1570, 5267, 13, 400, 689, 729, 5267, 366, 406, 412, 439, 411, 29903, 31890, 11, 300], "temperature": 0.0, "avg_logprob": -0.4404958089192708, "compression_ratio": 1.6446280991735538, "no_speech_prob": 8.800067917036358e-06}, {"id": 100, "seek": 66640, "start": 689.52, "end": 696.3199999999999, "text": " being the Kaggle planet sample, and one which is a very, very small sample, and one which is a very", "tokens": [885, 264, 48751, 22631, 5054, 6889, 11, 293, 472, 597, 307, 257, 588, 11, 588, 1359, 6889, 11, 293, 472, 597, 307, 257, 588], "temperature": 0.0, "avg_logprob": -0.4404958089192708, "compression_ratio": 1.6446280991735538, "no_speech_prob": 8.800067917036358e-06}, {"id": 101, "seek": 69632, "start": 696.32, "end": 702.5600000000001, "text": " small sample, and one which is a lot like ImageNet and has a lot more images that being IoT pets.", "tokens": [1359, 6889, 11, 293, 472, 597, 307, 257, 688, 411, 29903, 31890, 293, 575, 257, 688, 544, 5267, 300, 885, 30112, 19897, 13], "temperature": 0.0, "avg_logprob": -0.3165707588195801, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.1910922239621868e-06}, {"id": 102, "seek": 69632, "start": 704.0, "end": 708.48, "text": " And I kind of figured like if we get some insights from those two, perhaps there'll", "tokens": [400, 286, 733, 295, 8932, 411, 498, 321, 483, 512, 14310, 490, 729, 732, 11, 4317, 456, 603], "temperature": 0.0, "avg_logprob": -0.3165707588195801, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.1910922239621868e-06}, {"id": 103, "seek": 69632, "start": 708.48, "end": 716.96, "text": " be something that we can leverage more generally. So Thomas wrote this script,", "tokens": [312, 746, 300, 321, 393, 13982, 544, 5101, 13, 407, 8500, 4114, 341, 5755, 11], "temperature": 0.0, "avg_logprob": -0.3165707588195801, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.1910922239621868e-06}, {"id": 104, "seek": 71696, "start": 716.96, "end": 726.32, "text": " which it's 86 lines, but really there's only like three or four lines and they're all lines you", "tokens": [597, 309, 311, 26687, 3876, 11, 457, 534, 456, 311, 787, 411, 1045, 420, 1451, 3876, 293, 436, 434, 439, 3876, 291], "temperature": 0.0, "avg_logprob": -0.2590943637647127, "compression_ratio": 1.5561797752808988, "no_speech_prob": 3.0894141218595905e-06}, {"id": 105, "seek": 71696, "start": 726.32, "end": 735.2800000000001, "text": " recognize, right? The lines are untar data, image data loaders dot from blah, and then", "tokens": [5521, 11, 558, 30, 440, 3876, 366, 1701, 289, 1412, 11, 3256, 1412, 3677, 433, 5893, 490, 12288, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.2590943637647127, "compression_ratio": 1.5561797752808988, "no_speech_prob": 3.0894141218595905e-06}, {"id": 106, "seek": 71696, "start": 736.32, "end": 743.0400000000001, "text": " vision learner, DLs, model, etc. So there's the normal like three or four lines of code we see", "tokens": [5201, 33347, 11, 413, 43, 82, 11, 2316, 11, 5183, 13, 407, 456, 311, 264, 2710, 411, 1045, 420, 1451, 3876, 295, 3089, 321, 536], "temperature": 0.0, "avg_logprob": -0.2590943637647127, "compression_ratio": 1.5561797752808988, "no_speech_prob": 3.0894141218595905e-06}, {"id": 107, "seek": 74304, "start": 743.04, "end": 748.48, "text": " three or four lines of code we see over and over again. And then you know the rest of it is", "tokens": [1045, 420, 1451, 3876, 295, 3089, 321, 536, 670, 293, 670, 797, 13, 400, 550, 291, 458, 264, 1472, 295, 309, 307], "temperature": 0.0, "avg_logprob": -0.09493014510248748, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.4060568648565095e-06}, {"id": 108, "seek": 74304, "start": 748.48, "end": 755.5999999999999, "text": " basically lets you pass into the script different choices about batch size, epochs, and so forth.", "tokens": [1936, 6653, 291, 1320, 666, 264, 5755, 819, 7994, 466, 15245, 2744, 11, 30992, 28346, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.09493014510248748, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.4060568648565095e-06}, {"id": 109, "seek": 74304, "start": 758.9599999999999, "end": 767.28, "text": " And that's about it. So this is like how simple the script was that we used. And then", "tokens": [400, 300, 311, 466, 309, 13, 407, 341, 307, 411, 577, 2199, 264, 5755, 390, 300, 321, 1143, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.09493014510248748, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.4060568648565095e-06}, {"id": 110, "seek": 76728, "start": 767.28, "end": 771.92, "text": " partly because Thomas works at weights and biases and partly because weights and biases is pretty", "tokens": [17031, 570, 8500, 1985, 412, 17443, 293, 32152, 293, 17031, 570, 17443, 293, 32152, 307, 1238], "temperature": 0.0, "avg_logprob": -0.15878627005587803, "compression_ratio": 1.9141414141414141, "no_speech_prob": 1.8057911802316085e-05}, {"id": 111, "seek": 76728, "start": 771.92, "end": 781.8399999999999, "text": " cool, we used weights and biases then to feed in different values for each of those parameters. So", "tokens": [1627, 11, 321, 1143, 17443, 293, 32152, 550, 281, 3154, 294, 819, 4190, 337, 1184, 295, 729, 9834, 13, 407], "temperature": 0.0, "avg_logprob": -0.15878627005587803, "compression_ratio": 1.9141414141414141, "no_speech_prob": 1.8057911802316085e-05}, {"id": 112, "seek": 76728, "start": 781.8399999999999, "end": 788.24, "text": " this is a YAML file that weights and biases uses where you can say, okay, try each of these", "tokens": [341, 307, 257, 398, 2865, 43, 3991, 300, 17443, 293, 32152, 4960, 689, 291, 393, 584, 11, 1392, 11, 853, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.15878627005587803, "compression_ratio": 1.9141414141414141, "no_speech_prob": 1.8057911802316085e-05}, {"id": 113, "seek": 76728, "start": 788.24, "end": 794.4, "text": " different learning rates, try each of these different models, try, let's see if I can find", "tokens": [819, 2539, 6846, 11, 853, 1184, 295, 613, 819, 5245, 11, 853, 11, 718, 311, 536, 498, 286, 393, 915], "temperature": 0.0, "avg_logprob": -0.15878627005587803, "compression_ratio": 1.9141414141414141, "no_speech_prob": 1.8057911802316085e-05}, {"id": 114, "seek": 79440, "start": 794.4, "end": 801.6, "text": " another one, try each of these different resize methods, each of these different pooling methods,", "tokens": [1071, 472, 11, 853, 1184, 295, 613, 819, 50069, 7150, 11, 1184, 295, 613, 819, 7005, 278, 7150, 11], "temperature": 0.0, "avg_logprob": -0.2444119623729161, "compression_ratio": 1.9625, "no_speech_prob": 1.1842639651149511e-05}, {"id": 115, "seek": 79440, "start": 801.6, "end": 807.28, "text": " this distribution of learning rates, you know, whatever, and it goes away and tries them.", "tokens": [341, 7316, 295, 2539, 6846, 11, 291, 458, 11, 2035, 11, 293, 309, 1709, 1314, 293, 9898, 552, 13], "temperature": 0.0, "avg_logprob": -0.2444119623729161, "compression_ratio": 1.9625, "no_speech_prob": 1.1842639651149511e-05}, {"id": 116, "seek": 79440, "start": 808.3199999999999, "end": 815.52, "text": " And then you can use their web GUI to look at like the training results. So then you basically say,", "tokens": [400, 550, 291, 393, 764, 641, 3670, 17917, 40, 281, 574, 412, 411, 264, 3097, 3542, 13, 407, 550, 291, 1936, 584, 11], "temperature": 0.0, "avg_logprob": -0.2444119623729161, "compression_ratio": 1.9625, "no_speech_prob": 1.1842639651149511e-05}, {"id": 117, "seek": 79440, "start": 815.52, "end": 820.24, "text": " okay, start training and it trains each of these models over each of these data sets with each of", "tokens": [1392, 11, 722, 3097, 293, 309, 16329, 1184, 295, 613, 5245, 670, 1184, 295, 613, 1412, 6352, 365, 1184, 295], "temperature": 0.0, "avg_logprob": -0.2444119623729161, "compression_ratio": 1.9625, "no_speech_prob": 1.1842639651149511e-05}, {"id": 118, "seek": 79440, "start": 820.24, "end": 823.68, "text": " these poor values and each of these resize methods. And if you look at the data sets,", "tokens": [613, 4716, 4190, 293, 1184, 295, 613, 50069, 7150, 13, 400, 498, 291, 574, 412, 264, 1412, 6352, 11], "temperature": 0.0, "avg_logprob": -0.2444119623729161, "compression_ratio": 1.9625, "no_speech_prob": 1.1842639651149511e-05}, {"id": 119, "seek": 82368, "start": 823.68, "end": 826.9599999999999, "text": " methods and a few different selections from this distribution of learning rates", "tokens": [7150, 293, 257, 1326, 819, 47829, 490, 341, 7316, 295, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.1653892384019009, "compression_ratio": 1.5446009389671362, "no_speech_prob": 5.594203230430139e-06}, {"id": 120, "seek": 82368, "start": 828.64, "end": 834.7199999999999, "text": " and creates a web GUI that you can dive into. I personally hate web GUIs. I would much rather", "tokens": [293, 7829, 257, 3670, 17917, 40, 300, 291, 393, 9192, 666, 13, 286, 5665, 4700, 3670, 17917, 6802, 13, 286, 576, 709, 2831], "temperature": 0.0, "avg_logprob": -0.1653892384019009, "compression_ratio": 1.5446009389671362, "no_speech_prob": 5.594203230430139e-06}, {"id": 121, "seek": 82368, "start": 834.7199999999999, "end": 841.92, "text": " use Python. So, but they also thankfully have an API. So, yeah, so once we ran that script for", "tokens": [764, 15329, 13, 407, 11, 457, 436, 611, 27352, 362, 364, 9362, 13, 407, 11, 1338, 11, 370, 1564, 321, 5872, 300, 5755, 337], "temperature": 0.0, "avg_logprob": -0.1653892384019009, "compression_ratio": 1.5446009389671362, "no_speech_prob": 5.594203230430139e-06}, {"id": 122, "seek": 82368, "start": 841.92, "end": 850.16, "text": " a few hours, I then checked the results into a, into a gist.", "tokens": [257, 1326, 2496, 11, 286, 550, 10033, 264, 3542, 666, 257, 11, 666, 257, 290, 468, 13], "temperature": 0.0, "avg_logprob": -0.1653892384019009, "compression_ratio": 1.5446009389671362, "no_speech_prob": 5.594203230430139e-06}, {"id": 123, "seek": 85016, "start": 850.16, "end": 858.56, "text": " So a gist is just a place to chuck text files, basically, if you haven't used it before.", "tokens": [407, 257, 290, 468, 307, 445, 257, 1081, 281, 20870, 2487, 7098, 11, 1936, 11, 498, 291, 2378, 380, 1143, 309, 949, 13], "temperature": 0.0, "avg_logprob": -0.3868416517208784, "compression_ratio": 1.5087719298245614, "no_speech_prob": 8.939447070588358e-06}, {"id": 124, "seek": 85016, "start": 863.28, "end": 869.1999999999999, "text": " So I can, I chucked my CSV file in here. As you can see, it kind of displays it in a nice way,", "tokens": [407, 286, 393, 11, 286, 20870, 292, 452, 48814, 3991, 294, 510, 13, 1018, 291, 393, 536, 11, 309, 733, 295, 20119, 309, 294, 257, 1481, 636, 11], "temperature": 0.0, "avg_logprob": -0.3868416517208784, "compression_ratio": 1.5087719298245614, "no_speech_prob": 8.939447070588358e-06}, {"id": 125, "seek": 85016, "start": 869.1999999999999, "end": 875.12, "text": " or you can just click on raw to see the raw data. And then you can see the", "tokens": [420, 291, 393, 445, 2052, 322, 8936, 281, 536, 264, 8936, 1412, 13, 400, 550, 291, 393, 536, 264], "temperature": 0.0, "avg_logprob": -0.3868416517208784, "compression_ratio": 1.5087719298245614, "no_speech_prob": 8.939447070588358e-06}, {"id": 126, "seek": 87512, "start": 875.12, "end": 881.44, "text": " data in a nice way, or you can just click on raw to see the raw data. So I find that quite a nice", "tokens": [1412, 294, 257, 1481, 636, 11, 420, 291, 393, 445, 2052, 322, 8936, 281, 536, 264, 8936, 1412, 13, 407, 286, 915, 300, 1596, 257, 1481], "temperature": 0.0, "avg_logprob": -0.2458513944576948, "compression_ratio": 1.4748603351955307, "no_speech_prob": 2.507051067368593e-05}, {"id": 127, "seek": 87512, "start": 881.44, "end": 886.88, "text": " place just to chuck things, which I'm just going to share publicly. And so then I can,", "tokens": [1081, 445, 281, 20870, 721, 11, 597, 286, 478, 445, 516, 281, 2073, 14843, 13, 400, 370, 550, 286, 393, 11], "temperature": 0.0, "avg_logprob": -0.2458513944576948, "compression_ratio": 1.4748603351955307, "no_speech_prob": 2.507051067368593e-05}, {"id": 128, "seek": 87512, "start": 886.88, "end": 889.12, "text": " because there's the URL to the gist.", "tokens": [570, 456, 311, 264, 12905, 281, 264, 290, 468, 13], "temperature": 0.0, "avg_logprob": -0.2458513944576948, "compression_ratio": 1.4748603351955307, "no_speech_prob": 2.507051067368593e-05}, {"id": 129, "seek": 88912, "start": 889.12, "end": 907.04, "text": " And maybe, let me show you how I did that.", "tokens": [50364, 400, 1310, 11, 718, 385, 855, 291, 577, 286, 630, 300, 13, 51260], "temperature": 0.0, "avg_logprob": -0.40077587763468425, "compression_ratio": 0.8936170212765957, "no_speech_prob": 4.751882079290226e-05}, {"id": 130, "seek": 91912, "start": 919.12, "end": 929.04, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.9023725191752116, "compression_ratio": 0.42857142857142855, "no_speech_prob": 0.03959345445036888}, {"id": 131, "seek": 92904, "start": 929.04, "end": 950.56, "text": " So I just kind of like everything to be automated so I can always easily redo it,", "tokens": [407, 286, 445, 733, 295, 411, 1203, 281, 312, 18473, 370, 286, 393, 1009, 3612, 29956, 309, 11], "temperature": 0.0, "avg_logprob": -0.17742313657488143, "compression_ratio": 1.4594594594594594, "no_speech_prob": 3.070706225116737e-05}, {"id": 132, "seek": 92904, "start": 950.56, "end": 953.8399999999999, "text": " because I always assume my first effort is going to be crap, and it always is.", "tokens": [570, 286, 1009, 6552, 452, 700, 4630, 307, 516, 281, 312, 12426, 11, 293, 309, 1009, 307, 13], "temperature": 0.0, "avg_logprob": -0.17742313657488143, "compression_ratio": 1.4594594594594594, "no_speech_prob": 3.070706225116737e-05}, {"id": 133, "seek": 92904, "start": 953.8399999999999, "end": 955.8399999999999, "text": " And normally my second, third efforts are crap as well.", "tokens": [400, 5646, 452, 1150, 11, 2636, 6484, 366, 12426, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.17742313657488143, "compression_ratio": 1.4594594594594594, "no_speech_prob": 3.070706225116737e-05}, {"id": 134, "seek": 95584, "start": 955.84, "end": 965.44, "text": " So here's my little notebook I put together. So basically each time you do one of these sweeps on", "tokens": [407, 510, 311, 452, 707, 21060, 286, 829, 1214, 13, 407, 1936, 1184, 565, 291, 360, 472, 295, 613, 2484, 10653, 322], "temperature": 0.0, "avg_logprob": -0.15358914931615195, "compression_ratio": 1.6738197424892705, "no_speech_prob": 8.939175131672528e-06}, {"id": 135, "seek": 95584, "start": 967.36, "end": 972.32, "text": " weights and biases, it generates a new ID. And so we ended up kind of doing five different ones as", "tokens": [17443, 293, 32152, 11, 309, 23815, 257, 777, 7348, 13, 400, 370, 321, 4590, 493, 733, 295, 884, 1732, 819, 2306, 382], "temperature": 0.0, "avg_logprob": -0.15358914931615195, "compression_ratio": 1.6738197424892705, "no_speech_prob": 8.939175131672528e-06}, {"id": 136, "seek": 95584, "start": 972.32, "end": 976.48, "text": " we realized we were able to add different models and change things a little bit. And so they have", "tokens": [321, 5334, 321, 645, 1075, 281, 909, 819, 5245, 293, 1319, 721, 257, 707, 857, 13, 400, 370, 436, 362], "temperature": 0.0, "avg_logprob": -0.15358914931615195, "compression_ratio": 1.6738197424892705, "no_speech_prob": 8.939175131672528e-06}, {"id": 137, "seek": 95584, "start": 976.48, "end": 985.44, "text": " this API that you can use. And so they basically can go through and say, go through each of the", "tokens": [341, 9362, 300, 291, 393, 764, 13, 400, 370, 436, 1936, 393, 352, 807, 293, 584, 11, 352, 807, 1184, 295, 264], "temperature": 0.0, "avg_logprob": -0.15358914931615195, "compression_ratio": 1.6738197424892705, "no_speech_prob": 8.939175131672528e-06}, {"id": 138, "seek": 98544, "start": 985.44, "end": 991.6800000000001, "text": " sweep IDs and ask the API for that sweep and grab the runs from it. And then for each one create a", "tokens": [22169, 48212, 293, 1029, 264, 9362, 337, 300, 22169, 293, 4444, 264, 6676, 490, 309, 13, 400, 550, 337, 1184, 472, 1884, 257], "temperature": 0.0, "avg_logprob": -0.10136100683319435, "compression_ratio": 1.6355555555555557, "no_speech_prob": 1.1300507139822003e-05}, {"id": 139, "seek": 98544, "start": 991.6800000000001, "end": 996.24, "text": " dictionary containing a summary and the model name. So the details don't matter too much,", "tokens": [25890, 19273, 257, 12691, 293, 264, 2316, 1315, 13, 407, 264, 4365, 500, 380, 1871, 886, 709, 11], "temperature": 0.0, "avg_logprob": -0.10136100683319435, "compression_ratio": 1.6355555555555557, "no_speech_prob": 1.1300507139822003e-05}, {"id": 140, "seek": 98544, "start": 996.24, "end": 999.36, "text": " but you kind of get the idea hopefully, and then turn that into a data frame.", "tokens": [457, 291, 733, 295, 483, 264, 1558, 4696, 11, 293, 550, 1261, 300, 666, 257, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.10136100683319435, "compression_ratio": 1.6355555555555557, "no_speech_prob": 1.1300507139822003e-05}, {"id": 141, "seek": 98544, "start": 1002.4000000000001, "end": 1006.8000000000001, "text": " And so I kind of end up with this data frame that contains all the different", "tokens": [400, 370, 286, 733, 295, 917, 493, 365, 341, 1412, 3920, 300, 8306, 439, 264, 819], "temperature": 0.0, "avg_logprob": -0.10136100683319435, "compression_ratio": 1.6355555555555557, "no_speech_prob": 1.1300507139822003e-05}, {"id": 142, "seek": 98544, "start": 1008.4000000000001, "end": 1009.84, "text": " configuration parameters", "tokens": [11694, 9834], "temperature": 0.0, "avg_logprob": -0.10136100683319435, "compression_ratio": 1.6355555555555557, "no_speech_prob": 1.1300507139822003e-05}, {"id": 143, "seek": 100984, "start": 1009.84, "end": 1017.52, "text": " along with their loss and their speed, their accuracy, GPU, maximum memory usage and so forth.", "tokens": [2051, 365, 641, 4470, 293, 641, 3073, 11, 641, 14170, 11, 18407, 11, 6674, 4675, 14924, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.2860105177935432, "compression_ratio": 1.6009389671361502, "no_speech_prob": 1.8447963157086633e-06}, {"id": 144, "seek": 100984, "start": 1018.64, "end": 1024.64, "text": " So that's basically what I wanted to chuck into a gist. And so specifically I really wanted", "tokens": [407, 300, 311, 1936, 437, 286, 1415, 281, 20870, 666, 257, 290, 468, 13, 400, 370, 4682, 286, 534, 1415], "temperature": 0.0, "avg_logprob": -0.2860105177935432, "compression_ratio": 1.6009389671361502, "no_speech_prob": 1.8447963157086633e-06}, {"id": 145, "seek": 100984, "start": 1024.64, "end": 1028.88, "text": " this subset of the columns. So these are the columns I wanted. So I can grab those columns", "tokens": [341, 25993, 295, 264, 13766, 13, 407, 613, 366, 264, 13766, 286, 1415, 13, 407, 286, 393, 4444, 729, 13766], "temperature": 0.0, "avg_logprob": -0.2860105177935432, "compression_ratio": 1.6009389671361502, "no_speech_prob": 1.8447963157086633e-06}, {"id": 146, "seek": 100984, "start": 1029.52, "end": 1034.32, "text": " and put them into a CSV. Now one thing you might not realize is", "tokens": [293, 829, 552, 666, 257, 48814, 13, 823, 472, 551, 291, 1062, 406, 4325, 307], "temperature": 0.0, "avg_logprob": -0.2860105177935432, "compression_ratio": 1.6009389671361502, "no_speech_prob": 1.8447963157086633e-06}, {"id": 147, "seek": 103432, "start": 1034.32, "end": 1040.1599999999999, "text": " I would say for most Python libraries or at least most well written ones, anyway you can put a file", "tokens": [286, 576, 584, 337, 881, 15329, 15148, 420, 412, 1935, 881, 731, 3720, 2306, 11, 4033, 291, 393, 829, 257, 3991], "temperature": 0.0, "avg_logprob": -0.25891532068667206, "compression_ratio": 1.6025641025641026, "no_speech_prob": 3.7852796594961546e-06}, {"id": 148, "seek": 103432, "start": 1040.1599999999999, "end": 1045.52, "text": " name. So normally when you say to CSV you put here a file name or a path. You can instead put", "tokens": [1315, 13, 407, 5646, 562, 291, 584, 281, 48814, 291, 829, 510, 257, 3991, 1315, 420, 257, 3100, 13, 509, 393, 2602, 829], "temperature": 0.0, "avg_logprob": -0.25891532068667206, "compression_ratio": 1.6025641025641026, "no_speech_prob": 3.7852796594961546e-06}, {"id": 149, "seek": 103432, "start": 1045.52, "end": 1049.76, "text": " something called a string IO object, which is something that behaves exactly like a file,", "tokens": [746, 1219, 257, 6798, 39839, 2657, 11, 597, 307, 746, 300, 36896, 2293, 411, 257, 3991, 11], "temperature": 0.0, "avg_logprob": -0.25891532068667206, "compression_ratio": 1.6025641025641026, "no_speech_prob": 3.7852796594961546e-06}, {"id": 150, "seek": 103432, "start": 1050.48, "end": 1058.1599999999999, "text": " but it actually just stores it into a string. Because I don't want this stored into a file.", "tokens": [457, 309, 767, 445, 9512, 309, 666, 257, 6798, 13, 1436, 286, 500, 380, 528, 341, 12187, 666, 257, 3991, 13], "temperature": 0.0, "avg_logprob": -0.25891532068667206, "compression_ratio": 1.6025641025641026, "no_speech_prob": 3.7852796594961546e-06}, {"id": 151, "seek": 105816, "start": 1058.16, "end": 1063.44, "text": " It's put into a string. So if you then call.getValue, I actually get the string.", "tokens": [467, 311, 829, 666, 257, 6798, 13, 407, 498, 291, 550, 818, 2411, 847, 53, 304, 622, 11, 286, 767, 483, 264, 6798, 13], "temperature": 0.0, "avg_logprob": -0.36575960645488664, "compression_ratio": 1.6126126126126126, "no_speech_prob": 4.56588941233349e-06}, {"id": 152, "seek": 105816, "start": 1064.48, "end": 1068.88, "text": " And so even things like creating the gist, I want to do that automatically. So there's a", "tokens": [400, 370, 754, 721, 411, 4084, 264, 290, 468, 11, 286, 528, 281, 360, 300, 6772, 13, 407, 456, 311, 257], "temperature": 0.0, "avg_logprob": -0.36575960645488664, "compression_ratio": 1.6126126126126126, "no_speech_prob": 4.56588941233349e-06}, {"id": 153, "seek": 105816, "start": 1071.44, "end": 1077.2, "text": " library I'm very fond of. I'm very biased because I made it called ghapi, which is an API for GitHub", "tokens": [6405, 286, 478, 588, 9557, 295, 13, 286, 478, 588, 28035, 570, 286, 1027, 309, 1219, 33937, 35891, 11, 597, 307, 364, 9362, 337, 23331], "temperature": 0.0, "avg_logprob": -0.36575960645488664, "compression_ratio": 1.6126126126126126, "no_speech_prob": 4.56588941233349e-06}, {"id": 154, "seek": 105816, "start": 1078.16, "end": 1083.6000000000001, "text": " where we can do things like say create gist and you give it a name. And then you can do", "tokens": [689, 321, 393, 360, 721, 411, 584, 1884, 290, 468, 293, 291, 976, 309, 257, 1315, 13, 400, 550, 291, 393, 360], "temperature": 0.0, "avg_logprob": -0.36575960645488664, "compression_ratio": 1.6126126126126126, "no_speech_prob": 4.56588941233349e-06}, {"id": 155, "seek": 108360, "start": 1083.6, "end": 1088.8799999999999, "text": " things like create gist and you give it a description. And here's the text, which is the", "tokens": [721, 411, 1884, 290, 468, 293, 291, 976, 309, 257, 3855, 13, 400, 510, 311, 264, 2487, 11, 597, 307, 264], "temperature": 0.0, "avg_logprob": -0.30769912401835126, "compression_ratio": 1.4636871508379887, "no_speech_prob": 2.4682165530975908e-05}, {"id": 156, "seek": 108360, "start": 1088.8799999999999, "end": 1096.32, "text": " contents of the CSV. And the file name, make it public. And then you can get the HTML URL of the", "tokens": [15768, 295, 264, 48814, 13, 400, 264, 3991, 1315, 11, 652, 309, 1908, 13, 400, 550, 291, 393, 483, 264, 17995, 12905, 295, 264], "temperature": 0.0, "avg_logprob": -0.30769912401835126, "compression_ratio": 1.4636871508379887, "no_speech_prob": 2.4682165530975908e-05}, {"id": 157, "seek": 108360, "start": 1096.32, "end": 1105.36, "text": " gist. So that's how I used in this case a notebook as my kind of interactive", "tokens": [290, 468, 13, 407, 300, 311, 577, 286, 1143, 294, 341, 1389, 257, 21060, 382, 452, 733, 295, 15141], "temperature": 0.0, "avg_logprob": -0.30769912401835126, "compression_ratio": 1.4636871508379887, "no_speech_prob": 2.4682165530975908e-05}, {"id": 158, "seek": 110536, "start": 1105.36, "end": 1114.0, "text": " read of our print loop for manipulating this data set, putting it together and then uploading it to GitHub.", "tokens": [1401, 295, 527, 4482, 6367, 337, 40805, 341, 1412, 992, 11, 3372, 309, 1214, 293, 550, 27301, 309, 281, 23331, 13], "temperature": 0.0, "avg_logprob": -0.4963645515861092, "compression_ratio": 1.6328502415458936, "no_speech_prob": 8.397722922381945e-06}, {"id": 159, "seek": 110536, "start": 1115.04, "end": 1121.52, "text": " Jermey, I had a doubt in this Pandas data frame. Here you have like in your data, I just took a look at your", "tokens": [508, 966, 2030, 11, 286, 632, 257, 6385, 294, 341, 16995, 296, 1412, 3920, 13, 1692, 291, 362, 411, 294, 428, 1412, 11, 286, 445, 1890, 257, 574, 412, 428], "temperature": 0.0, "avg_logprob": -0.4963645515861092, "compression_ratio": 1.6328502415458936, "no_speech_prob": 8.397722922381945e-06}, {"id": 160, "seek": 110536, "start": 1121.52, "end": 1128.7199999999998, "text": " gist and it had in the data set entries with planned and that other data set, the pet status.", "tokens": [290, 468, 293, 309, 632, 294, 264, 1412, 992, 23041, 365, 8589, 293, 300, 661, 1412, 992, 11, 264, 3817, 6558, 13], "temperature": 0.0, "avg_logprob": -0.4963645515861092, "compression_ratio": 1.6328502415458936, "no_speech_prob": 8.397722922381945e-06}, {"id": 161, "seek": 110536, "start": 1128.7199999999998, "end": 1130.8, "text": " So how did you populate it?", "tokens": [407, 577, 630, 291, 1665, 5256, 309, 30], "temperature": 0.0, "avg_logprob": -0.4963645515861092, "compression_ratio": 1.6328502415458936, "no_speech_prob": 8.397722922381945e-06}, {"id": 162, "seek": 113080, "start": 1130.8, "end": 1136.8799999999999, "text": " Sorry, what's your question? How did I populate this data set? Yeah, that Pandas data frame.", "tokens": [4919, 11, 437, 311, 428, 1168, 30, 1012, 630, 286, 1665, 5256, 341, 1412, 992, 30, 865, 11, 300, 16995, 296, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.34397659720955315, "compression_ratio": 1.6650246305418719, "no_speech_prob": 1.9522596630849876e-05}, {"id": 163, "seek": 113080, "start": 1136.8799999999999, "end": 1146.08, "text": " Yeah, just here. So I passed it a list of dictionaries. And the list of dictionaries I created using a list comprehension", "tokens": [865, 11, 445, 510, 13, 407, 286, 4678, 309, 257, 1329, 295, 22352, 4889, 13, 400, 264, 1329, 295, 22352, 4889, 286, 2942, 1228, 257, 1329, 44991], "temperature": 0.0, "avg_logprob": -0.34397659720955315, "compression_ratio": 1.6650246305418719, "no_speech_prob": 1.9522596630849876e-05}, {"id": 164, "seek": 113080, "start": 1147.12, "end": 1149.68, "text": " containing a bunch of dictionaries.", "tokens": [19273, 257, 3840, 295, 22352, 4889, 13], "temperature": 0.0, "avg_logprob": -0.34397659720955315, "compression_ratio": 1.6650246305418719, "no_speech_prob": 1.9522596630849876e-05}, {"id": 165, "seek": 113080, "start": 1151.12, "end": 1157.6, "text": " Okay, got it. And so that's going to make each key, so that means all the dictionaries.", "tokens": [1033, 11, 658, 309, 13, 400, 370, 300, 311, 516, 281, 652, 1184, 2141, 11, 370, 300, 1355, 439, 264, 22352, 4889, 13], "temperature": 0.0, "avg_logprob": -0.34397659720955315, "compression_ratio": 1.6650246305418719, "no_speech_prob": 1.9522596630849876e-05}, {"id": 166, "seek": 115760, "start": 1157.6, "end": 1163.12, "text": " Should have, you know, roughly the same keys, anyone's that are missing, they're going to end up being NA.", "tokens": [6454, 362, 11, 291, 458, 11, 9810, 264, 912, 9317, 11, 2878, 311, 300, 366, 5361, 11, 436, 434, 516, 281, 917, 493, 885, 16585, 13], "temperature": 0.0, "avg_logprob": -0.3591179941214767, "compression_ratio": 1.5669291338582678, "no_speech_prob": 5.823145329486579e-05}, {"id": 167, "seek": 115760, "start": 1165.12, "end": 1172.1599999999999, "text": " And then I just fiddled around with it slightly. So for example, so I made sure everything had an error rate that was equal to one minus the accuracy.", "tokens": [400, 550, 286, 445, 283, 14273, 1493, 926, 365, 309, 4748, 13, 407, 337, 1365, 11, 370, 286, 1027, 988, 1203, 632, 364, 6713, 3314, 300, 390, 2681, 281, 472, 3175, 264, 14170, 13], "temperature": 0.0, "avg_logprob": -0.3591179941214767, "compression_ratio": 1.5669291338582678, "no_speech_prob": 5.823145329486579e-05}, {"id": 168, "seek": 115760, "start": 1173.04, "end": 1179.4399999999998, "text": " On the planet data set is not called accuracy. So I copied accuracy multi into accuracy. Yeah, nothing very exciting.", "tokens": [1282, 264, 5054, 1412, 992, 307, 406, 1219, 14170, 13, 407, 286, 25365, 14170, 4825, 666, 14170, 13, 865, 11, 1825, 588, 4670, 13], "temperature": 0.0, "avg_logprob": -0.3591179941214767, "compression_ratio": 1.5669291338582678, "no_speech_prob": 5.823145329486579e-05}, {"id": 169, "seek": 115760, "start": 1181.4399999999998, "end": 1182.08, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.3591179941214767, "compression_ratio": 1.5669291338582678, "no_speech_prob": 5.823145329486579e-05}, {"id": 170, "seek": 115760, "start": 1182.08, "end": 1182.7199999999998, "text": " No worries.", "tokens": [883, 16340, 13], "temperature": 0.0, "avg_logprob": -0.3591179941214767, "compression_ratio": 1.5669291338582678, "no_speech_prob": 5.823145329486579e-05}, {"id": 171, "seek": 118272, "start": 1182.72, "end": 1187.76, "text": " Jeremy, what's the actual goal of this?", "tokens": [17809, 11, 437, 311, 264, 3539, 3387, 295, 341, 30], "temperature": 0.0, "avg_logprob": -0.37848758697509766, "compression_ratio": 1.09375, "no_speech_prob": 0.0001021927673718892}, {"id": 172, "seek": 118272, "start": 1187.76, "end": 1196.96, "text": " Let me show you. So what we've now got is a CSV, which I can then", "tokens": [961, 385, 855, 291, 13, 407, 437, 321, 600, 586, 658, 307, 257, 48814, 11, 597, 286, 393, 550], "temperature": 0.0, "avg_logprob": -0.37848758697509766, "compression_ratio": 1.09375, "no_speech_prob": 0.0001021927673718892}, {"id": 173, "seek": 119696, "start": 1196.96, "end": 1216.8, "text": " Okay, a CSV, which I can then use Pandas pivot table functionality to group by the data set, the model family and name, and calculate the min of error rate fit to the data set.", "tokens": [1033, 11, 257, 48814, 11, 597, 286, 393, 550, 764, 16995, 296, 14538, 3199, 14980, 281, 1594, 538, 264, 1412, 992, 11, 264, 2316, 1605, 293, 1315, 11, 293, 8873, 264, 923, 295, 6713, 3314, 3318, 281, 264, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.39417877197265627, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.2218713891343214e-05}, {"id": 174, "seek": 121680, "start": 1216.8, "end": 1226.96, "text": " And I can then take the pets subset of that.", "tokens": [400, 286, 393, 550, 747, 264, 19897, 25993, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.5747053185287787, "compression_ratio": 1.4198473282442747, "no_speech_prob": 1.0288926205248572e-05}, {"id": 175, "seek": 121680, "start": 1226.96, "end": 1233.76, "text": " Sort by score where score represents a combination of error and speed and take the top 15.", "tokens": [26149, 538, 6175, 689, 6175, 8855, 257, 6562, 295, 6713, 293, 3073, 293, 747, 264, 1192, 2119, 13], "temperature": 0.0, "avg_logprob": -0.5747053185287787, "compression_ratio": 1.4198473282442747, "no_speech_prob": 1.0288926205248572e-05}, {"id": 176, "seek": 121680, "start": 1233.76, "end": 1238.08, "text": " And this now shows the error rate of the data set.", "tokens": [400, 341, 586, 3110, 264, 6713, 3314, 295, 264, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.5747053185287787, "compression_ratio": 1.4198473282442747, "no_speech_prob": 1.0288926205248572e-05}, {"id": 177, "seek": 123808, "start": 1238.08, "end": 1253.84, "text": " Sort by score where score represents a combination of error and speed and take the top 15. And this now shows me the top 15 best models for fine tuning on pets.", "tokens": [26149, 538, 6175, 689, 6175, 8855, 257, 6562, 295, 6713, 293, 3073, 293, 747, 264, 1192, 2119, 13, 400, 341, 586, 3110, 385, 264, 1192, 2119, 1151, 5245, 337, 2489, 15164, 322, 19897, 13], "temperature": 0.0, "avg_logprob": -0.18486309051513672, "compression_ratio": 1.6733668341708543, "no_speech_prob": 1.4965715308790095e-05}, {"id": 178, "seek": 123808, "start": 1255.84, "end": 1264.96, "text": " And this is this is gold, in my opinion, I don't think anybody's ever done anything like this before there's never been a list of like pure the best models for fine tuning.", "tokens": [400, 341, 307, 341, 307, 3821, 11, 294, 452, 4800, 11, 286, 500, 380, 519, 4472, 311, 1562, 1096, 1340, 411, 341, 949, 456, 311, 1128, 668, 257, 1329, 295, 411, 6075, 264, 1151, 5245, 337, 2489, 15164, 13], "temperature": 0.0, "avg_logprob": -0.18486309051513672, "compression_ratio": 1.6733668341708543, "no_speech_prob": 1.4965715308790095e-05}, {"id": 179, "seek": 126496, "start": 1264.96, "end": 1277.76, "text": " And yeah, sorry, I have a question. So you you you fine tune different models with pets and then collected this information, is that correct?", "tokens": [400, 1338, 11, 2597, 11, 286, 362, 257, 1168, 13, 407, 291, 291, 291, 2489, 10864, 819, 5245, 365, 19897, 293, 550, 11087, 341, 1589, 11, 307, 300, 3006, 30], "temperature": 0.0, "avg_logprob": -0.27522941484843216, "compression_ratio": 1.6203208556149733, "no_speech_prob": 2.7522037271410227e-05}, {"id": 180, "seek": 126496, "start": 1277.76, "end": 1278.32, "text": " That's correct.", "tokens": [663, 311, 3006, 13], "temperature": 0.0, "avg_logprob": -0.27522941484843216, "compression_ratio": 1.6203208556149733, "no_speech_prob": 2.7522037271410227e-05}, {"id": 181, "seek": 126496, "start": 1279.2, "end": 1286.72, "text": " And then based on the information that you collected from the fine tune of five or whatever number of iterations.", "tokens": [400, 550, 2361, 322, 264, 1589, 300, 291, 11087, 490, 264, 2489, 10864, 295, 1732, 420, 2035, 1230, 295, 36540, 13], "temperature": 0.0, "avg_logprob": -0.27522941484843216, "compression_ratio": 1.6203208556149733, "no_speech_prob": 2.7522037271410227e-05}, {"id": 182, "seek": 126496, "start": 1286.72, "end": 1288.8, "text": " Three runs for each model. Yes.", "tokens": [6244, 6676, 337, 1184, 2316, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.27522941484843216, "compression_ratio": 1.6203208556149733, "no_speech_prob": 2.7522037271410227e-05}, {"id": 183, "seek": 128880, "start": 1288.8, "end": 1297.9199999999998, "text": " And and then you collected this information to find out which one is is the best behave model for this specific case.", "tokens": [400, 293, 550, 291, 11087, 341, 1589, 281, 915, 484, 597, 472, 307, 307, 264, 1151, 15158, 2316, 337, 341, 2685, 1389, 13], "temperature": 0.0, "avg_logprob": -0.23792370329511928, "compression_ratio": 1.7245762711864407, "no_speech_prob": 3.5007290080102393e-06}, {"id": 184, "seek": 128880, "start": 1297.9199999999998, "end": 1300.6399999999999, "text": " Correct, correct, correct, correct. Exactly.", "tokens": [12753, 11, 3006, 11, 3006, 11, 3006, 13, 7587, 13], "temperature": 0.0, "avg_logprob": -0.23792370329511928, "compression_ratio": 1.7245762711864407, "no_speech_prob": 3.5007290080102393e-06}, {"id": 185, "seek": 128880, "start": 1300.6399999999999, "end": 1306.72, "text": " And the best is going to involve two things. It's going to be which ones have the lowest error rate and which ones are the fastest.", "tokens": [400, 264, 1151, 307, 516, 281, 9494, 732, 721, 13, 467, 311, 516, 281, 312, 597, 2306, 362, 264, 12437, 6713, 3314, 293, 597, 2306, 366, 264, 14573, 13], "temperature": 0.0, "avg_logprob": -0.23792370329511928, "compression_ratio": 1.7245762711864407, "no_speech_prob": 3.5007290080102393e-06}, {"id": 186, "seek": 128880, "start": 1306.72, "end": 1314.3999999999999, "text": " Now, I created this kind of arbitrary scoring function where I multiplied the error rate times fit time plus 80.", "tokens": [823, 11, 286, 2942, 341, 733, 295, 23211, 22358, 2445, 689, 286, 17207, 264, 6713, 3314, 1413, 3318, 565, 1804, 4688, 13], "temperature": 0.0, "avg_logprob": -0.23792370329511928, "compression_ratio": 1.7245762711864407, "no_speech_prob": 3.5007290080102393e-06}, {"id": 187, "seek": 131440, "start": 1314.4, "end": 1322.88, "text": " Just because I felt like that particular value of that constant gave me an ordering that I was reasonably comfortable with.", "tokens": [1449, 570, 286, 2762, 411, 300, 1729, 2158, 295, 300, 5754, 2729, 385, 364, 21739, 300, 286, 390, 23551, 4619, 365, 13], "temperature": 0.0, "avg_logprob": -0.20301969551745755, "compression_ratio": 1.5294117647058822, "no_speech_prob": 6.643127562711015e-06}, {"id": 188, "seek": 131440, "start": 1322.88, "end": 1333.52, "text": " But you can kind of look through here and see like, OK, well, the IT base has a much better error rate than conv next tiny.", "tokens": [583, 291, 393, 733, 295, 574, 807, 510, 293, 536, 411, 11, 2264, 11, 731, 11, 264, 6783, 3096, 575, 257, 709, 1101, 6713, 3314, 813, 3754, 958, 5870, 13], "temperature": 0.0, "avg_logprob": -0.20301969551745755, "compression_ratio": 1.5294117647058822, "no_speech_prob": 6.643127562711015e-06}, {"id": 189, "seek": 131440, "start": 1333.52, "end": 1339.0400000000002, "text": " But it's also much slower, like you can decide for your needs where you want to trade off.", "tokens": [583, 309, 311, 611, 709, 14009, 11, 411, 291, 393, 4536, 337, 428, 2203, 689, 291, 528, 281, 4923, 766, 13], "temperature": 0.0, "avg_logprob": -0.20301969551745755, "compression_ratio": 1.5294117647058822, "no_speech_prob": 6.643127562711015e-06}, {"id": 190, "seek": 133904, "start": 1339.04, "end": 1342.3999999999999, "text": " So that's what I kind of the first thing I did was to create this kind of top 15.", "tokens": [407, 300, 311, 437, 286, 733, 295, 264, 700, 551, 286, 630, 390, 281, 1884, 341, 733, 295, 1192, 2119, 13], "temperature": 0.0, "avg_logprob": -0.2405676245689392, "compression_ratio": 1.8205128205128205, "no_speech_prob": 7.646114681847394e-06}, {"id": 191, "seek": 133904, "start": 1342.3999999999999, "end": 1344.96, "text": " And it's interesting looking at the family, right.", "tokens": [400, 309, 311, 1880, 1237, 412, 264, 1605, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.2405676245689392, "compression_ratio": 1.8205128205128205, "no_speech_prob": 7.646114681847394e-06}, {"id": 192, "seek": 133904, "start": 1344.96, "end": 1355.04, "text": " The family is like each of these different architectures, you know, is kind of from, you know, from different sizes of a smaller subset of families.", "tokens": [440, 1605, 307, 411, 1184, 295, 613, 819, 6331, 1303, 11, 291, 458, 11, 307, 733, 295, 490, 11, 291, 458, 11, 490, 819, 11602, 295, 257, 4356, 25993, 295, 4466, 13], "temperature": 0.0, "avg_logprob": -0.2405676245689392, "compression_ratio": 1.8205128205128205, "no_speech_prob": 7.646114681847394e-06}, {"id": 193, "seek": 133904, "start": 1355.04, "end": 1361.36, "text": " Right. So there's conv next tiny, conv next base, conv next tiny and 22K and so forth.", "tokens": [1779, 13, 407, 456, 311, 3754, 958, 5870, 11, 3754, 958, 3096, 11, 3754, 958, 5870, 293, 5853, 42, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.2405676245689392, "compression_ratio": 1.8205128205128205, "no_speech_prob": 7.646114681847394e-06}, {"id": 194, "seek": 133904, "start": 1361.36, "end": 1367.6, "text": " So you can kind of get a sense of like, if you want to learn more about architectures, which one seemed most interesting to you.", "tokens": [407, 291, 393, 733, 295, 483, 257, 2020, 295, 411, 11, 498, 291, 528, 281, 1466, 544, 466, 6331, 1303, 11, 597, 472, 6576, 881, 1880, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.2405676245689392, "compression_ratio": 1.8205128205128205, "no_speech_prob": 7.646114681847394e-06}, {"id": 195, "seek": 136760, "start": 1367.6, "end": 1378.0, "text": " And, you know, for fine tuning on pets, it looks like conv next, viet, swin, resnet are the main ones.", "tokens": [400, 11, 291, 458, 11, 337, 2489, 15164, 322, 19897, 11, 309, 1542, 411, 3754, 958, 11, 371, 1684, 11, 1693, 259, 11, 725, 7129, 366, 264, 2135, 2306, 13], "temperature": 0.0, "avg_logprob": -0.32759062851531595, "compression_ratio": 1.5025906735751295, "no_speech_prob": 1.2606455129571259e-05}, {"id": 196, "seek": 136760, "start": 1378.0, "end": 1387.12, "text": " So that's the first thing I did. The second thing I then did was to take those most interesting families.", "tokens": [407, 300, 311, 264, 700, 551, 286, 630, 13, 440, 1150, 551, 286, 550, 630, 390, 281, 747, 729, 881, 1880, 4466, 13], "temperature": 0.0, "avg_logprob": -0.32759062851531595, "compression_ratio": 1.5025906735751295, "no_speech_prob": 1.2606455129571259e-05}, {"id": 197, "seek": 136760, "start": 1387.12, "end": 1394.6399999999999, "text": " I actually also added this one called regnetx and created a scatter plot of them.", "tokens": [286, 767, 611, 3869, 341, 472, 1219, 1121, 7129, 87, 293, 2942, 257, 34951, 7542, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.32759062851531595, "compression_ratio": 1.5025906735751295, "no_speech_prob": 1.2606455129571259e-05}, {"id": 198, "seek": 139464, "start": 1394.64, "end": 1405.0400000000002, "text": " Colored by family. And so you can kind of see, like, for example, conv next, which I'm rather fond of.", "tokens": [4004, 2769, 538, 1605, 13, 400, 370, 291, 393, 733, 295, 536, 11, 411, 11, 337, 1365, 11, 3754, 958, 11, 597, 286, 478, 2831, 9557, 295, 13], "temperature": 0.0, "avg_logprob": -0.2690515645345052, "compression_ratio": 1.5176470588235293, "no_speech_prob": 9.080185918719508e-06}, {"id": 199, "seek": 139464, "start": 1405.0400000000002, "end": 1410.64, "text": " Is these this kind of blue line, these blue ones, right.", "tokens": [1119, 613, 341, 733, 295, 3344, 1622, 11, 613, 3344, 2306, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.2690515645345052, "compression_ratio": 1.5176470588235293, "no_speech_prob": 9.080185918719508e-06}, {"id": 200, "seek": 139464, "start": 1410.64, "end": 1417.0400000000002, "text": " And so you can see that the very best error rate actually was a conv next.", "tokens": [400, 370, 291, 393, 536, 300, 264, 588, 1151, 6713, 3314, 767, 390, 257, 3754, 958, 13], "temperature": 0.0, "avg_logprob": -0.2690515645345052, "compression_ratio": 1.5176470588235293, "no_speech_prob": 9.080185918719508e-06}, {"id": 201, "seek": 139464, "start": 1417.0400000000002, "end": 1421.0400000000002, "text": " So they're pretty good.", "tokens": [407, 436, 434, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.2690515645345052, "compression_ratio": 1.5176470588235293, "no_speech_prob": 9.080185918719508e-06}, {"id": 202, "seek": 142104, "start": 1421.04, "end": 1433.44, "text": " You can see this one here, which is regnetx seems to be have some pretty nice values.", "tokens": [509, 393, 536, 341, 472, 510, 11, 597, 307, 1121, 7129, 87, 2544, 281, 312, 362, 512, 1238, 1481, 4190, 13], "temperature": 0.0, "avg_logprob": -0.21881952041234726, "compression_ratio": 1.5315789473684212, "no_speech_prob": 4.425286988407606e-06}, {"id": 203, "seek": 142104, "start": 1433.44, "end": 1440.6399999999999, "text": " They're like super fast. Seems like these tiny swings seem to be pretty good.", "tokens": [814, 434, 411, 1687, 2370, 13, 22524, 411, 613, 5870, 32386, 1643, 281, 312, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.21881952041234726, "compression_ratio": 1.5315789473684212, "no_speech_prob": 4.425286988407606e-06}, {"id": 204, "seek": 142104, "start": 1440.6399999999999, "end": 1446.48, "text": " So it kind of gives you a sense of like, you know, depending on how much time you've got to run or how accurate you want to be.", "tokens": [407, 309, 733, 295, 2709, 291, 257, 2020, 295, 411, 11, 291, 458, 11, 5413, 322, 577, 709, 565, 291, 600, 658, 281, 1190, 420, 577, 8559, 291, 528, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.21881952041234726, "compression_ratio": 1.5315789473684212, "no_speech_prob": 4.425286988407606e-06}, {"id": 205, "seek": 144648, "start": 1446.48, "end": 1453.28, "text": " What families are likely to most useful.", "tokens": [708, 4466, 366, 3700, 281, 881, 4420, 13], "temperature": 0.0, "avg_logprob": -0.12873515417409498, "compression_ratio": 1.7177033492822966, "no_speech_prob": 8.800507202977315e-06}, {"id": 206, "seek": 144648, "start": 1453.28, "end": 1461.08, "text": " And then the last thing I did for pets was I grabbed a subset of the basically the ones which are in the top,", "tokens": [400, 550, 264, 1036, 551, 286, 630, 337, 19897, 390, 286, 18607, 257, 25993, 295, 264, 1936, 264, 2306, 597, 366, 294, 264, 1192, 11], "temperature": 0.0, "avg_logprob": -0.12873515417409498, "compression_ratio": 1.7177033492822966, "no_speech_prob": 8.800507202977315e-06}, {"id": 207, "seek": 144648, "start": 1461.08, "end": 1467.92, "text": " basically smaller than the median and faster than the median, because these are the ones I generally care about most of the time,", "tokens": [1936, 4356, 813, 264, 26779, 293, 4663, 813, 264, 26779, 11, 570, 613, 366, 264, 2306, 286, 5101, 1127, 466, 881, 295, 264, 565, 11], "temperature": 0.0, "avg_logprob": -0.12873515417409498, "compression_ratio": 1.7177033492822966, "no_speech_prob": 8.800507202977315e-06}, {"id": 208, "seek": 144648, "start": 1467.92, "end": 1472.2, "text": " because most of the time I'm going to be, you know, training quick iterations.", "tokens": [570, 881, 295, 264, 565, 286, 478, 516, 281, 312, 11, 291, 458, 11, 3097, 1702, 36540, 13], "temperature": 0.0, "avg_logprob": -0.12873515417409498, "compression_ratio": 1.7177033492822966, "no_speech_prob": 8.800507202977315e-06}, {"id": 209, "seek": 147220, "start": 1472.2, "end": 1487.96, "text": " And so and then I just ordered those by error rate. And so conv next tiny has got the best error rate of those which are in the upper half of both speed and accuracy.", "tokens": [400, 370, 293, 550, 286, 445, 8866, 729, 538, 6713, 3314, 13, 400, 370, 3754, 958, 5870, 575, 658, 264, 1151, 6713, 3314, 295, 729, 597, 366, 294, 264, 6597, 1922, 295, 1293, 3073, 293, 14170, 13], "temperature": 0.0, "avg_logprob": -0.19533140009099786, "compression_ratio": 1.5294117647058822, "no_speech_prob": 8.80014613358071e-06}, {"id": 210, "seek": 147220, "start": 1487.96, "end": 1491.64, "text": " What's the GPU memory in this context?", "tokens": [708, 311, 264, 18407, 4675, 294, 341, 4319, 30], "temperature": 0.0, "avg_logprob": -0.19533140009099786, "compression_ratio": 1.5294117647058822, "no_speech_prob": 8.80014613358071e-06}, {"id": 211, "seek": 147220, "start": 1491.64, "end": 1496.04, "text": " That's the maximum amount of GPU memory that was used.", "tokens": [663, 311, 264, 6674, 2372, 295, 18407, 4675, 300, 390, 1143, 13], "temperature": 0.0, "avg_logprob": -0.19533140009099786, "compression_ratio": 1.5294117647058822, "no_speech_prob": 8.80014613358071e-06}, {"id": 212, "seek": 149604, "start": 1496.04, "end": 1505.72, "text": " I can't remember what the units of measure are, but they don't matter too much because it'll be different for your data set.", "tokens": [286, 393, 380, 1604, 437, 264, 6815, 295, 3481, 366, 11, 457, 436, 500, 380, 1871, 886, 709, 570, 309, 603, 312, 819, 337, 428, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.1678251128598868, "compression_ratio": 1.488262910798122, "no_speech_prob": 3.905330686393427e-06}, {"id": 213, "seek": 149604, "start": 1505.72, "end": 1512.0, "text": " All that matters is the relative usage.", "tokens": [1057, 300, 7001, 307, 264, 4972, 14924, 13], "temperature": 0.0, "avg_logprob": -0.1678251128598868, "compression_ratio": 1.488262910798122, "no_speech_prob": 3.905330686393427e-06}, {"id": 214, "seek": 149604, "start": 1512.0, "end": 1523.8799999999999, "text": " And so if you want something, you know, if you if you try to use this and it's actually uses too much GPU memory, you could try ResNet 50D, for example.", "tokens": [400, 370, 498, 291, 528, 746, 11, 291, 458, 11, 498, 291, 498, 291, 853, 281, 764, 341, 293, 309, 311, 767, 4960, 886, 709, 18407, 4675, 11, 291, 727, 853, 5015, 31890, 2625, 35, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1678251128598868, "compression_ratio": 1.488262910798122, "no_speech_prob": 3.905330686393427e-06}, {"id": 215, "seek": 152388, "start": 1523.88, "end": 1534.44, "text": " Or, you know, it's interesting that like ResNet 26 is really good for memory and speed.", "tokens": [1610, 11, 291, 458, 11, 309, 311, 1880, 300, 411, 5015, 31890, 7551, 307, 534, 665, 337, 4675, 293, 3073, 13], "temperature": 0.0, "avg_logprob": -0.14031411707401276, "compression_ratio": 1.4058823529411764, "no_speech_prob": 1.7603158539714059e-06}, {"id": 216, "seek": 152388, "start": 1534.44, "end": 1539.72, "text": " Or if you want something really lightweight on memory RegNet Y004.", "tokens": [1610, 498, 291, 528, 746, 534, 22052, 322, 4675, 4791, 31890, 398, 628, 19, 13], "temperature": 0.0, "avg_logprob": -0.14031411707401276, "compression_ratio": 1.4058823529411764, "no_speech_prob": 1.7603158539714059e-06}, {"id": 217, "seek": 152388, "start": 1539.72, "end": 1544.2, "text": " But the error rates are getting much worse once you get out to here, as you can see.", "tokens": [583, 264, 6713, 6846, 366, 1242, 709, 5324, 1564, 291, 483, 484, 281, 510, 11, 382, 291, 393, 536, 13], "temperature": 0.0, "avg_logprob": -0.14031411707401276, "compression_ratio": 1.4058823529411764, "no_speech_prob": 1.7603158539714059e-06}, {"id": 218, "seek": 154420, "start": 1544.2, "end": 1556.32, "text": " So then so then I looked at Planet and so, as I said, Planet's kind of as different a data set as you're going to get in one sense.", "tokens": [407, 550, 370, 550, 286, 2956, 412, 22146, 293, 370, 11, 382, 286, 848, 11, 22146, 311, 733, 295, 382, 819, 257, 1412, 992, 382, 291, 434, 516, 281, 483, 294, 472, 2020, 13], "temperature": 0.0, "avg_logprob": -0.15366407342859217, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.123311787727289e-06}, {"id": 219, "seek": 154420, "start": 1556.32, "end": 1562.92, "text": " Or it's very different. And so not surprisingly, its top 15 is also very different.", "tokens": [1610, 309, 311, 588, 819, 13, 400, 370, 406, 17600, 11, 1080, 1192, 2119, 307, 611, 588, 819, 13], "temperature": 0.0, "avg_logprob": -0.15366407342859217, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.123311787727289e-06}, {"id": 220, "seek": 154420, "start": 1562.92, "end": 1569.16, "text": " And interestingly. All of the top six from the same family.", "tokens": [400, 25873, 13, 1057, 295, 264, 1192, 2309, 490, 264, 912, 1605, 13], "temperature": 0.0, "avg_logprob": -0.15366407342859217, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.123311787727289e-06}, {"id": 221, "seek": 156916, "start": 1569.16, "end": 1575.16, "text": " So this VIT family, these are kind of model called Transformers models.", "tokens": [407, 341, 691, 3927, 1605, 11, 613, 366, 733, 295, 2316, 1219, 27938, 433, 5245, 13], "temperature": 0.0, "avg_logprob": -0.07549222310384114, "compression_ratio": 1.618421052631579, "no_speech_prob": 3.844718321488472e-06}, {"id": 222, "seek": 156916, "start": 1575.16, "end": 1587.52, "text": " And what this is basically showing is that these models are particularly good at rapidly identifying features of data types it hasn't seen before.", "tokens": [400, 437, 341, 307, 1936, 4099, 307, 300, 613, 5245, 366, 4098, 665, 412, 12910, 16696, 4122, 295, 1412, 3467, 309, 6132, 380, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.07549222310384114, "compression_ratio": 1.618421052631579, "no_speech_prob": 3.844718321488472e-06}, {"id": 223, "seek": 156916, "start": 1587.52, "end": 1594.44, "text": " So, you know, if you're doing something like medical imaging or satellite imagery or something like that, these would probably be a good thing to try.", "tokens": [407, 11, 291, 458, 11, 498, 291, 434, 884, 746, 411, 4625, 25036, 420, 16016, 24340, 420, 746, 411, 300, 11, 613, 576, 1391, 312, 257, 665, 551, 281, 853, 13], "temperature": 0.0, "avg_logprob": -0.07549222310384114, "compression_ratio": 1.618421052631579, "no_speech_prob": 3.844718321488472e-06}, {"id": 224, "seek": 159444, "start": 1594.44, "end": 1605.04, "text": " And Swin, by the way, is kind of another Transformers based model, which, as you can see, it's actually the most accurate of all.", "tokens": [400, 3926, 259, 11, 538, 264, 636, 11, 307, 733, 295, 1071, 27938, 433, 2361, 2316, 11, 597, 11, 382, 291, 393, 536, 11, 309, 311, 767, 264, 881, 8559, 295, 439, 13], "temperature": 0.0, "avg_logprob": -0.13382655382156372, "compression_ratio": 1.34375, "no_speech_prob": 1.3709276345252874e-06}, {"id": 225, "seek": 159444, "start": 1605.04, "end": 1612.52, "text": " But it's also the slowest. This is Swin V2.", "tokens": [583, 309, 311, 611, 264, 2964, 377, 13, 639, 307, 3926, 259, 691, 17, 13], "temperature": 0.0, "avg_logprob": -0.13382655382156372, "compression_ratio": 1.34375, "no_speech_prob": 1.3709276345252874e-06}, {"id": 226, "seek": 159444, "start": 1612.52, "end": 1616.04, "text": " So I thought that was pretty interesting.", "tokens": [407, 286, 1194, 300, 390, 1238, 1880, 13], "temperature": 0.0, "avg_logprob": -0.13382655382156372, "compression_ratio": 1.34375, "no_speech_prob": 1.3709276345252874e-06}, {"id": 227, "seek": 161604, "start": 1616.04, "end": 1626.8799999999999, "text": " And, you know, these VIT models, there are ones with pretty good error rates that also have very little memory use and also run very quickly.", "tokens": [400, 11, 291, 458, 11, 613, 691, 3927, 5245, 11, 456, 366, 2306, 365, 1238, 665, 6713, 6846, 300, 611, 362, 588, 707, 4675, 764, 293, 611, 1190, 588, 2661, 13], "temperature": 0.0, "avg_logprob": -0.08474527752917746, "compression_ratio": 1.7008547008547008, "no_speech_prob": 2.5611943783587776e-06}, {"id": 228, "seek": 161604, "start": 1626.8799999999999, "end": 1637.6399999999999, "text": " So I did the same thing for Planet. And so perhaps not surprisingly, but interestingly for Planet, these these lines don't necessarily go down,", "tokens": [407, 286, 630, 264, 912, 551, 337, 22146, 13, 400, 370, 4317, 406, 17600, 11, 457, 25873, 337, 22146, 11, 613, 613, 3876, 500, 380, 4725, 352, 760, 11], "temperature": 0.0, "avg_logprob": -0.08474527752917746, "compression_ratio": 1.7008547008547008, "no_speech_prob": 2.5611943783587776e-06}, {"id": 229, "seek": 161604, "start": 1637.6399999999999, "end": 1644.56, "text": " which is to say that the really big models, the big slow models don't don't necessarily have better error rates.", "tokens": [597, 307, 281, 584, 300, 264, 534, 955, 5245, 11, 264, 955, 2964, 5245, 500, 380, 500, 380, 4725, 362, 1101, 6713, 6846, 13], "temperature": 0.0, "avg_logprob": -0.08474527752917746, "compression_ratio": 1.7008547008547008, "no_speech_prob": 2.5611943783587776e-06}, {"id": 230, "seek": 164456, "start": 1644.56, "end": 1651.6799999999998, "text": " And that makes sense, right? Because if they've got heaps of parameters, but they're trying to learn something they've never seen before on very little data,", "tokens": [400, 300, 1669, 2020, 11, 558, 30, 1436, 498, 436, 600, 658, 415, 2382, 295, 9834, 11, 457, 436, 434, 1382, 281, 1466, 746, 436, 600, 1128, 1612, 949, 322, 588, 707, 1412, 11], "temperature": 0.0, "avg_logprob": -0.09406632816090303, "compression_ratio": 1.5458515283842795, "no_speech_prob": 2.057991878245957e-06}, {"id": 231, "seek": 164456, "start": 1651.6799999999998, "end": 1658.12, "text": " it's unlikely we're going to be able to take advantage of those parameters.", "tokens": [309, 311, 17518, 321, 434, 516, 281, 312, 1075, 281, 747, 5002, 295, 729, 9834, 13], "temperature": 0.0, "avg_logprob": -0.09406632816090303, "compression_ratio": 1.5458515283842795, "no_speech_prob": 2.057991878245957e-06}, {"id": 232, "seek": 164456, "start": 1658.12, "end": 1668.24, "text": " So when you're doing stuff that doesn't really look much like ImageNet, you might want to be down more towards this end.", "tokens": [407, 562, 291, 434, 884, 1507, 300, 1177, 380, 534, 574, 709, 411, 29903, 31890, 11, 291, 1062, 528, 281, 312, 760, 544, 3030, 341, 917, 13], "temperature": 0.0, "avg_logprob": -0.09406632816090303, "compression_ratio": 1.5458515283842795, "no_speech_prob": 2.057991878245957e-06}, {"id": 233, "seek": 166824, "start": 1668.24, "end": 1678.56, "text": " So here's the VIT, for example, and here's that really good Swin model.", "tokens": [407, 510, 311, 264, 691, 3927, 11, 337, 1365, 11, 293, 510, 311, 300, 534, 665, 3926, 259, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1811506846179701, "compression_ratio": 1.432748538011696, "no_speech_prob": 1.6027975107135717e-06}, {"id": 234, "seek": 166824, "start": 1678.56, "end": 1688.16, "text": " And there's ConvNext Tiny. So then we can do the same thing again of like, OK, let's take the top half both in terms of speed and memory use.", "tokens": [400, 456, 311, 2656, 85, 31002, 39992, 13, 407, 550, 321, 393, 360, 264, 912, 551, 797, 295, 411, 11, 2264, 11, 718, 311, 747, 264, 1192, 1922, 1293, 294, 2115, 295, 3073, 293, 4675, 764, 13], "temperature": 0.0, "avg_logprob": -0.1811506846179701, "compression_ratio": 1.432748538011696, "no_speech_prob": 1.6027975107135717e-06}, {"id": 235, "seek": 166824, "start": 1688.16, "end": 1691.96, "text": " ConvNext Tiny still looks good.", "tokens": [2656, 85, 31002, 39992, 920, 1542, 665, 13], "temperature": 0.0, "avg_logprob": -0.1811506846179701, "compression_ratio": 1.432748538011696, "no_speech_prob": 1.6027975107135717e-06}, {"id": 236, "seek": 169196, "start": 1691.96, "end": 1702.2, "text": " These VIT models, this 224, yeah, this is because you can only run these models on images of size 224 by 224.", "tokens": [1981, 691, 3927, 5245, 11, 341, 5853, 19, 11, 1338, 11, 341, 307, 570, 291, 393, 787, 1190, 613, 5245, 322, 5267, 295, 2744, 5853, 19, 538, 5853, 19, 13], "temperature": 0.0, "avg_logprob": -0.12706344127655028, "compression_ratio": 1.4820512820512821, "no_speech_prob": 2.60157571574382e-06}, {"id": 237, "seek": 169196, "start": 1702.2, "end": 1707.8, "text": " They're not you can't use different sizes, whereas the ConvNext models you can use any size.", "tokens": [814, 434, 406, 291, 393, 380, 764, 819, 11602, 11, 9735, 264, 2656, 85, 31002, 5245, 291, 393, 764, 604, 2744, 13], "temperature": 0.0, "avg_logprob": -0.12706344127655028, "compression_ratio": 1.4820512820512821, "no_speech_prob": 2.60157571574382e-06}, {"id": 238, "seek": 169196, "start": 1707.8, "end": 1718.8400000000001, "text": " So it's also interesting to see that classic ResNet still, again, they do pretty well.", "tokens": [407, 309, 311, 611, 1880, 281, 536, 300, 7230, 5015, 31890, 920, 11, 797, 11, 436, 360, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.12706344127655028, "compression_ratio": 1.4820512820512821, "no_speech_prob": 2.60157571574382e-06}, {"id": 239, "seek": 171884, "start": 1718.84, "end": 1729.4399999999998, "text": " Yeah, so I'm pretty excited about this. It feels like exactly what we need to.", "tokens": [865, 11, 370, 286, 478, 1238, 2919, 466, 341, 13, 467, 3417, 411, 2293, 437, 321, 643, 281, 13], "temperature": 0.0, "avg_logprob": -0.21696345660151267, "compression_ratio": 1.3424657534246576, "no_speech_prob": 5.337691618478857e-06}, {"id": 240, "seek": 171884, "start": 1729.4399999999998, "end": 1734.12, "text": " Kick ass on this Patty Doctor competition or indeed.", "tokens": [20886, 1256, 322, 341, 44116, 10143, 6211, 420, 6451, 13], "temperature": 0.0, "avg_logprob": -0.21696345660151267, "compression_ratio": 1.3424657534246576, "no_speech_prob": 5.337691618478857e-06}, {"id": 241, "seek": 171884, "start": 1734.12, "end": 1743.9599999999998, "text": " Any kind of. Computer vision classification task needs this and.", "tokens": [2639, 733, 295, 13, 22289, 5201, 21538, 5633, 2203, 341, 293, 13], "temperature": 0.0, "avg_logprob": -0.21696345660151267, "compression_ratio": 1.3424657534246576, "no_speech_prob": 5.337691618478857e-06}, {"id": 242, "seek": 174396, "start": 1743.96, "end": 1757.16, "text": " I ran this week on. Three consumer RTX GPUs in 12 hours or something like this is not.", "tokens": [286, 5872, 341, 1243, 322, 13, 6244, 9711, 44573, 18407, 82, 294, 2272, 2496, 420, 746, 411, 341, 307, 406, 13], "temperature": 0.0, "avg_logprob": -0.15839722373268822, "compression_ratio": 1.311377245508982, "no_speech_prob": 2.7692501589626772e-06}, {"id": 243, "seek": 174396, "start": 1757.16, "end": 1766.24, "text": " Big institutional resources required. And one of the reasons why is because.", "tokens": [5429, 18391, 3593, 4739, 13, 400, 472, 295, 264, 4112, 983, 307, 570, 13], "temperature": 0.0, "avg_logprob": -0.15839722373268822, "compression_ratio": 1.311377245508982, "no_speech_prob": 2.7692501589626772e-06}, {"id": 244, "seek": 174396, "start": 1766.24, "end": 1771.44, "text": " I didn't. Try every possible level of everything right?", "tokens": [286, 994, 380, 13, 6526, 633, 1944, 1496, 295, 1203, 558, 30], "temperature": 0.0, "avg_logprob": -0.15839722373268822, "compression_ratio": 1.311377245508982, "no_speech_prob": 2.7692501589626772e-06}, {"id": 245, "seek": 177144, "start": 1771.44, "end": 1777.48, "text": " I tried. A couple of you know, so.", "tokens": [286, 3031, 13, 316, 1916, 295, 291, 458, 11, 370, 13], "temperature": 0.0, "avg_logprob": -0.11155275865034624, "compression_ratio": 1.8529411764705883, "no_speech_prob": 5.954598236712627e-06}, {"id": 246, "seek": 177144, "start": 1777.48, "end": 1783.52, "text": " Thomas did a kind of a quick learning rate sweep to kind of get a sense of the broad range of learning rates that seem pretty good.", "tokens": [8500, 630, 257, 733, 295, 257, 1702, 2539, 3314, 22169, 281, 733, 295, 483, 257, 2020, 295, 264, 4152, 3613, 295, 2539, 6846, 300, 1643, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.11155275865034624, "compression_ratio": 1.8529411764705883, "no_speech_prob": 5.954598236712627e-06}, {"id": 247, "seek": 177144, "start": 1783.52, "end": 1799.0, "text": " And then we just tried a couple of learning rates and a couple of the best resize methods and a couple of the best polling types across a few broadly different kinds of models across the two different data sets.", "tokens": [400, 550, 321, 445, 3031, 257, 1916, 295, 2539, 6846, 293, 257, 1916, 295, 264, 1151, 50069, 7150, 293, 257, 1916, 295, 264, 1151, 29518, 3467, 2108, 257, 1326, 19511, 819, 3685, 295, 5245, 2108, 264, 732, 819, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.11155275865034624, "compression_ratio": 1.8529411764705883, "no_speech_prob": 5.954598236712627e-06}, {"id": 248, "seek": 179900, "start": 1799.0, "end": 1807.84, "text": " To kind of see if there was any common features and we found in every single case, the same learning rate, the same resource method and the same polling type was the best.", "tokens": [1407, 733, 295, 536, 498, 456, 390, 604, 2689, 4122, 293, 321, 1352, 294, 633, 2167, 1389, 11, 264, 912, 2539, 3314, 11, 264, 912, 7684, 3170, 293, 264, 912, 29518, 2010, 390, 264, 1151, 13], "temperature": 0.0, "avg_logprob": -0.15044821979843567, "compression_ratio": 1.7296296296296296, "no_speech_prob": 3.5556784041546052e-06}, {"id": 249, "seek": 179900, "start": 1807.84, "end": 1813.84, "text": " So we didn't need to try every possible combination of everything, you know, and this is where, like a lot of the.", "tokens": [407, 321, 994, 380, 643, 281, 853, 633, 1944, 6562, 295, 1203, 11, 291, 458, 11, 293, 341, 307, 689, 11, 411, 257, 688, 295, 264, 13], "temperature": 0.0, "avg_logprob": -0.15044821979843567, "compression_ratio": 1.7296296296296296, "no_speech_prob": 3.5556784041546052e-06}, {"id": 250, "seek": 179900, "start": 1813.84, "end": 1828.2, "text": " Stuff you see from like Google and stuff, they tend to do hundreds of thousands of experiments, because I guess because they don't they have no need to do things efficiently right?", "tokens": [31347, 291, 536, 490, 411, 3329, 293, 1507, 11, 436, 3928, 281, 360, 6779, 295, 5383, 295, 12050, 11, 570, 286, 2041, 570, 436, 500, 380, 436, 362, 572, 643, 281, 360, 721, 19621, 558, 30], "temperature": 0.0, "avg_logprob": -0.15044821979843567, "compression_ratio": 1.7296296296296296, "no_speech_prob": 3.5556784041546052e-06}, {"id": 251, "seek": 182820, "start": 1828.2, "end": 1837.52, "text": " Yeah, but you don't have to do it the Google way. You can do it the fast AI way.", "tokens": [865, 11, 457, 291, 500, 380, 362, 281, 360, 309, 264, 3329, 636, 13, 509, 393, 360, 309, 264, 2370, 7318, 636, 13], "temperature": 0.0, "avg_logprob": -0.3687601566314697, "compression_ratio": 1.4972375690607735, "no_speech_prob": 1.2601270100276452e-05}, {"id": 252, "seek": 182820, "start": 1837.52, "end": 1842.0, "text": " Quick quick question, Jeremy, which which cards did you use?", "tokens": [12101, 1702, 1168, 11, 17809, 11, 597, 597, 5632, 630, 291, 764, 30], "temperature": 0.0, "avg_logprob": -0.3687601566314697, "compression_ratio": 1.4972375690607735, "no_speech_prob": 1.2601270100276452e-05}, {"id": 253, "seek": 182820, "start": 1842.0, "end": 1847.56, "text": " And another question is why do you think? Yeah, the GPU cards.", "tokens": [400, 1071, 1168, 307, 983, 360, 291, 519, 30, 865, 11, 264, 18407, 5632, 13], "temperature": 0.0, "avg_logprob": -0.3687601566314697, "compression_ratio": 1.4972375690607735, "no_speech_prob": 1.2601270100276452e-05}, {"id": 254, "seek": 182820, "start": 1847.56, "end": 1854.88, "text": " Oh, I think that you 90. Oh, OK, so they were all three different.", "tokens": [876, 11, 286, 519, 300, 291, 4289, 13, 876, 11, 2264, 11, 370, 436, 645, 439, 1045, 819, 13], "temperature": 0.0, "avg_logprob": -0.3687601566314697, "compression_ratio": 1.4972375690607735, "no_speech_prob": 1.2601270100276452e-05}, {"id": 255, "seek": 185488, "start": 1854.88, "end": 1858.4, "text": " They're all the same card, they're all RTX 3090s.", "tokens": [814, 434, 439, 264, 912, 2920, 11, 436, 434, 439, 44573, 2217, 7771, 82, 13], "temperature": 0.0, "avg_logprob": -0.21069949865341187, "compression_ratio": 1.5302325581395348, "no_speech_prob": 1.669728044362273e-05}, {"id": 256, "seek": 185488, "start": 1858.4, "end": 1862.7600000000002, "text": " OK, and you reset the index after the query.", "tokens": [2264, 11, 293, 291, 14322, 264, 8186, 934, 264, 14581, 13], "temperature": 0.0, "avg_logprob": -0.21069949865341187, "compression_ratio": 1.5302325581395348, "no_speech_prob": 1.669728044362273e-05}, {"id": 257, "seek": 185488, "start": 1862.7600000000002, "end": 1871.5200000000002, "text": " Why? Oh, just because otherwise it shows the numeric ID here will be the numeric ID from the original data set.", "tokens": [1545, 30, 876, 11, 445, 570, 5911, 309, 3110, 264, 7866, 299, 7348, 510, 486, 312, 264, 7866, 299, 7348, 490, 264, 3380, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.21069949865341187, "compression_ratio": 1.5302325581395348, "no_speech_prob": 1.669728044362273e-05}, {"id": 258, "seek": 185488, "start": 1871.5200000000002, "end": 1874.96, "text": " And I wanted to be able to quickly kind of say what's number six, what's number 10, what's number three.", "tokens": [400, 286, 1415, 281, 312, 1075, 281, 2661, 733, 295, 584, 437, 311, 1230, 2309, 11, 437, 311, 1230, 1266, 11, 437, 311, 1230, 1045, 13], "temperature": 0.0, "avg_logprob": -0.21069949865341187, "compression_ratio": 1.5302325581395348, "no_speech_prob": 1.669728044362273e-05}, {"id": 259, "seek": 185488, "start": 1874.96, "end": 1880.16, "text": " That's all. Yeah.", "tokens": [663, 311, 439, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.21069949865341187, "compression_ratio": 1.5302325581395348, "no_speech_prob": 1.669728044362273e-05}, {"id": 260, "seek": 188016, "start": 1880.16, "end": 1890.4, "text": " Jeremy, getting back getting back to the Earth satellite images, when you say, you know, like the classification, what is it trying to classify?", "tokens": [17809, 11, 1242, 646, 1242, 646, 281, 264, 4755, 16016, 5267, 11, 562, 291, 584, 11, 291, 458, 11, 411, 264, 21538, 11, 437, 307, 309, 1382, 281, 33872, 30], "temperature": 0.0, "avg_logprob": -0.22829794883728027, "compression_ratio": 1.4, "no_speech_prob": 1.1836039448098745e-05}, {"id": 261, "seek": 188016, "start": 1890.4, "end": 1909.64, "text": " In this case, the planet competition.", "tokens": [682, 341, 1389, 11, 264, 5054, 6211, 13], "temperature": 0.0, "avg_logprob": -0.22829794883728027, "compression_ratio": 1.4, "no_speech_prob": 1.1836039448098745e-05}, {"id": 262, "seek": 190964, "start": 1909.64, "end": 1912.3200000000002, "text": " We have some examples.", "tokens": [492, 362, 512, 5110, 13], "temperature": 0.0, "avg_logprob": -0.25075612529631586, "compression_ratio": 1.3988095238095237, "no_speech_prob": 4.3262625695206225e-05}, {"id": 263, "seek": 190964, "start": 1912.3200000000002, "end": 1919.76, "text": " Basically, they try to classify for each area of the satellite imagery.", "tokens": [8537, 11, 436, 853, 281, 33872, 337, 1184, 1859, 295, 264, 16016, 24340, 13], "temperature": 0.0, "avg_logprob": -0.25075612529631586, "compression_ratio": 1.3988095238095237, "no_speech_prob": 4.3262625695206225e-05}, {"id": 264, "seek": 190964, "start": 1919.76, "end": 1921.5200000000002, "text": " What's it a picture of?", "tokens": [708, 311, 309, 257, 3036, 295, 30], "temperature": 0.0, "avg_logprob": -0.25075612529631586, "compression_ratio": 1.3988095238095237, "no_speech_prob": 4.3262625695206225e-05}, {"id": 265, "seek": 190964, "start": 1921.5200000000002, "end": 1926.76, "text": " Where is it forest or farmland or town or whatever?", "tokens": [2305, 307, 309, 6719, 420, 5421, 1661, 420, 3954, 420, 2035, 30], "temperature": 0.0, "avg_logprob": -0.25075612529631586, "compression_ratio": 1.3988095238095237, "no_speech_prob": 4.3262625695206225e-05}, {"id": 266, "seek": 190964, "start": 1926.76, "end": 1932.68, "text": " And what weather conditions to observe, if I remember correctly?", "tokens": [400, 437, 5503, 4487, 281, 11441, 11, 498, 286, 1604, 8944, 30], "temperature": 0.0, "avg_logprob": -0.25075612529631586, "compression_ratio": 1.3988095238095237, "no_speech_prob": 4.3262625695206225e-05}, {"id": 267, "seek": 193268, "start": 1932.68, "end": 1949.6000000000001, "text": " Question in this image space, is it just these two major data sets or how do you find other models that are trained on beside the planet and ImageNet?", "tokens": [14464, 294, 341, 3256, 1901, 11, 307, 309, 445, 613, 732, 2563, 1412, 6352, 420, 577, 360, 291, 915, 661, 5245, 300, 366, 8895, 322, 15726, 264, 5054, 293, 29903, 31890, 30], "temperature": 0.0, "avg_logprob": -0.2635824182531336, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.663057057361584e-05}, {"id": 268, "seek": 193268, "start": 1949.6000000000001, "end": 1952.24, "text": " You mean beside planet and pets?", "tokens": [509, 914, 15726, 5054, 293, 19897, 30], "temperature": 0.0, "avg_logprob": -0.2635824182531336, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.663057057361584e-05}, {"id": 269, "seek": 193268, "start": 1952.24, "end": 1953.48, "text": " Sorry, yeah, that's it.", "tokens": [4919, 11, 1338, 11, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.2635824182531336, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.663057057361584e-05}, {"id": 270, "seek": 193268, "start": 1953.48, "end": 1956.24, "text": " And so what is your question? How do you do what with them?", "tokens": [400, 370, 437, 307, 428, 1168, 30, 1012, 360, 291, 360, 437, 365, 552, 30], "temperature": 0.0, "avg_logprob": -0.2635824182531336, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.663057057361584e-05}, {"id": 271, "seek": 193268, "start": 1956.24, "end": 1961.44, "text": " How do you find other trained pre-trained models that have been worked on?", "tokens": [1012, 360, 291, 915, 661, 8895, 659, 12, 17227, 2001, 5245, 300, 362, 668, 2732, 322, 30], "temperature": 0.0, "avg_logprob": -0.2635824182531336, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.663057057361584e-05}, {"id": 272, "seek": 196144, "start": 1961.44, "end": 1965.88, "text": " Well, these all use pre-trained models, pre-trained on ImageNet.", "tokens": [1042, 11, 613, 439, 764, 659, 12, 17227, 2001, 5245, 11, 659, 12, 17227, 2001, 322, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.14578004402689415, "compression_ratio": 1.8068181818181819, "no_speech_prob": 2.0460620362428017e-05}, {"id": 273, "seek": 196144, "start": 1965.88, "end": 1970.44, "text": " These are only using pre-trained models, pre-trained on ImageNet.", "tokens": [1981, 366, 787, 1228, 659, 12, 17227, 2001, 5245, 11, 659, 12, 17227, 2001, 322, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.14578004402689415, "compression_ratio": 1.8068181818181819, "no_speech_prob": 2.0460620362428017e-05}, {"id": 274, "seek": 196144, "start": 1970.44, "end": 1974.6000000000001, "text": " So how do you find pre-trained models, pre-trained on other things?", "tokens": [407, 577, 360, 291, 915, 659, 12, 17227, 2001, 5245, 11, 659, 12, 17227, 2001, 322, 661, 721, 30], "temperature": 0.0, "avg_logprob": -0.14578004402689415, "compression_ratio": 1.8068181818181819, "no_speech_prob": 2.0460620362428017e-05}, {"id": 275, "seek": 196144, "start": 1974.6000000000001, "end": 1975.76, "text": " Mainly you don't.", "tokens": [47468, 291, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.14578004402689415, "compression_ratio": 1.8068181818181819, "no_speech_prob": 2.0460620362428017e-05}, {"id": 276, "seek": 196144, "start": 1975.76, "end": 1978.88, "text": " There aren't many.", "tokens": [821, 3212, 380, 867, 13], "temperature": 0.0, "avg_logprob": -0.14578004402689415, "compression_ratio": 1.8068181818181819, "no_speech_prob": 2.0460620362428017e-05}, {"id": 277, "seek": 196144, "start": 1978.88, "end": 1983.04, "text": " But, you know, just Google.", "tokens": [583, 11, 291, 458, 11, 445, 3329, 13], "temperature": 0.0, "avg_logprob": -0.14578004402689415, "compression_ratio": 1.8068181818181819, "no_speech_prob": 2.0460620362428017e-05}, {"id": 278, "seek": 196144, "start": 1983.04, "end": 1991.0800000000002, "text": " Depends what you're interested in and academic papers.", "tokens": [4056, 2581, 437, 291, 434, 3102, 294, 293, 7778, 10577, 13], "temperature": 0.0, "avg_logprob": -0.14578004402689415, "compression_ratio": 1.8068181818181819, "no_speech_prob": 2.0460620362428017e-05}, {"id": 279, "seek": 199108, "start": 1991.08, "end": 1994.04, "text": " There is a, I don't know how it's doing.", "tokens": [821, 307, 257, 11, 286, 500, 380, 458, 577, 309, 311, 884, 13], "temperature": 0.0, "avg_logprob": -0.18569822453740817, "compression_ratio": 1.429530201342282, "no_speech_prob": 3.1195984774967656e-05}, {"id": 280, "seek": 199108, "start": 1994.04, "end": 1997.72, "text": " It's there was a model zoo.", "tokens": [467, 311, 456, 390, 257, 2316, 25347, 13], "temperature": 0.0, "avg_logprob": -0.18569822453740817, "compression_ratio": 1.429530201342282, "no_speech_prob": 3.1195984774967656e-05}, {"id": 281, "seek": 199108, "start": 1997.72, "end": 2008.8799999999999, "text": " So there is a model zoo, which I've never had much success with, to be honest.", "tokens": [407, 456, 307, 257, 2316, 25347, 11, 597, 286, 600, 1128, 632, 709, 2245, 365, 11, 281, 312, 3245, 13], "temperature": 0.0, "avg_logprob": -0.18569822453740817, "compression_ratio": 1.429530201342282, "no_speech_prob": 3.1195984774967656e-05}, {"id": 282, "seek": 199108, "start": 2008.8799999999999, "end": 2018.24, "text": " So these are a range of pre-trained models that you can download.", "tokens": [407, 613, 366, 257, 3613, 295, 659, 12, 17227, 2001, 5245, 300, 291, 393, 5484, 13], "temperature": 0.0, "avg_logprob": -0.18569822453740817, "compression_ratio": 1.429530201342282, "no_speech_prob": 3.1195984774967656e-05}, {"id": 283, "seek": 201824, "start": 2018.24, "end": 2023.1200000000001, "text": " Yeah, but as I say, I haven't found it particularly successful, to be honest.", "tokens": [865, 11, 457, 382, 286, 584, 11, 286, 2378, 380, 1352, 309, 4098, 4406, 11, 281, 312, 3245, 13], "temperature": 0.0, "avg_logprob": -0.11487875757990657, "compression_ratio": 1.4913294797687862, "no_speech_prob": 1.9220251488150097e-05}, {"id": 284, "seek": 201824, "start": 2023.1200000000001, "end": 2032.96, "text": " You could also try papers with papers with code.", "tokens": [509, 727, 611, 853, 10577, 365, 10577, 365, 3089, 13], "temperature": 0.0, "avg_logprob": -0.11487875757990657, "compression_ratio": 1.4913294797687862, "no_speech_prob": 1.9220251488150097e-05}, {"id": 285, "seek": 201824, "start": 2032.96, "end": 2036.92, "text": " And I think these.", "tokens": [400, 286, 519, 613, 13], "temperature": 0.0, "avg_logprob": -0.11487875757990657, "compression_ratio": 1.4913294797687862, "no_speech_prob": 1.9220251488150097e-05}, {"id": 286, "seek": 201824, "start": 2036.92, "end": 2038.76, "text": " Yeah, they have a link to the paper and the code.", "tokens": [865, 11, 436, 362, 257, 2113, 281, 264, 3035, 293, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.11487875757990657, "compression_ratio": 1.4913294797687862, "no_speech_prob": 1.9220251488150097e-05}, {"id": 287, "seek": 201824, "start": 2038.76, "end": 2042.72, "text": " That doesn't necessarily mean they've got a pre-trained model.", "tokens": [663, 1177, 380, 4725, 914, 436, 600, 658, 257, 659, 12, 17227, 2001, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11487875757990657, "compression_ratio": 1.4913294797687862, "no_speech_prob": 1.9220251488150097e-05}, {"id": 288, "seek": 204272, "start": 2042.72, "end": 2057.2400000000002, "text": " But then you can just click on the code and see.", "tokens": [583, 550, 291, 393, 445, 2052, 322, 264, 3089, 293, 536, 13], "temperature": 0.0, "avg_logprob": -0.18953420074892716, "compression_ratio": 1.4285714285714286, "no_speech_prob": 9.366836820845492e-06}, {"id": 289, "seek": 204272, "start": 2057.2400000000002, "end": 2062.2400000000002, "text": " And of course, for NLP models, there's the hugging face model hub, which we've seen before.", "tokens": [400, 295, 1164, 11, 337, 426, 45196, 5245, 11, 456, 311, 264, 41706, 1851, 2316, 11838, 11, 597, 321, 600, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.18953420074892716, "compression_ratio": 1.4285714285714286, "no_speech_prob": 9.366836820845492e-06}, {"id": 290, "seek": 204272, "start": 2062.2400000000002, "end": 2064.8, "text": " And that is an easy answer for NLP.", "tokens": [400, 300, 307, 364, 1858, 1867, 337, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.18953420074892716, "compression_ratio": 1.4285714285714286, "no_speech_prob": 9.366836820845492e-06}, {"id": 291, "seek": 204272, "start": 2064.8, "end": 2071.32, "text": " It's like lots of different pre-trained models are on that hub.", "tokens": [467, 311, 411, 3195, 295, 819, 659, 12, 17227, 2001, 5245, 366, 322, 300, 11838, 13], "temperature": 0.0, "avg_logprob": -0.18953420074892716, "compression_ratio": 1.4285714285714286, "no_speech_prob": 9.366836820845492e-06}, {"id": 292, "seek": 207132, "start": 2071.32, "end": 2076.0, "text": " Jeremy, since you touch on academic papers and papers with code.", "tokens": [17809, 11, 1670, 291, 2557, 322, 7778, 10577, 293, 10577, 365, 3089, 13], "temperature": 0.0, "avg_logprob": -0.2584104537963867, "compression_ratio": 1.5324074074074074, "no_speech_prob": 5.059740215074271e-05}, {"id": 293, "seek": 207132, "start": 2076.0, "end": 2084.56, "text": " First question, this comparison, do you or Toma intend to publish it?", "tokens": [2386, 1168, 11, 341, 9660, 11, 360, 291, 420, 5041, 64, 19759, 281, 11374, 309, 30], "temperature": 0.0, "avg_logprob": -0.2584104537963867, "compression_ratio": 1.5324074074074074, "no_speech_prob": 5.059740215074271e-05}, {"id": 294, "seek": 207132, "start": 2084.56, "end": 2090.88, "text": " If not, if you were to do that, what would you go for actually?", "tokens": [759, 406, 11, 498, 291, 645, 281, 360, 300, 11, 437, 576, 291, 352, 337, 767, 30], "temperature": 0.0, "avg_logprob": -0.2584104537963867, "compression_ratio": 1.5324074074074074, "no_speech_prob": 5.059740215074271e-05}, {"id": 295, "seek": 207132, "start": 2090.88, "end": 2092.96, "text": " What kind of journal would you look for?", "tokens": [708, 733, 295, 6708, 576, 291, 574, 337, 30], "temperature": 0.0, "avg_logprob": -0.2584104537963867, "compression_ratio": 1.5324074074074074, "no_speech_prob": 5.059740215074271e-05}, {"id": 296, "seek": 207132, "start": 2092.96, "end": 2099.48, "text": " So I'm not a good person to ask that question because I very rarely publish anything, which", "tokens": [407, 286, 478, 406, 257, 665, 954, 281, 1029, 300, 1168, 570, 286, 588, 13752, 11374, 1340, 11, 597], "temperature": 0.0, "avg_logprob": -0.2584104537963867, "compression_ratio": 1.5324074074074074, "no_speech_prob": 5.059740215074271e-05}, {"id": 297, "seek": 209948, "start": 2099.48, "end": 2101.56, "text": " is partly a philosophical thing.", "tokens": [307, 17031, 257, 25066, 551, 13], "temperature": 0.0, "avg_logprob": -0.1858003370223507, "compression_ratio": 1.436046511627907, "no_speech_prob": 2.177226633648388e-05}, {"id": 298, "seek": 209948, "start": 2101.56, "end": 2111.6, "text": " I find academia overly exclusive and I don't love PDFs as a publication form.", "tokens": [286, 915, 28937, 24324, 13005, 293, 286, 500, 380, 959, 17752, 82, 382, 257, 19953, 1254, 13], "temperature": 0.0, "avg_logprob": -0.1858003370223507, "compression_ratio": 1.436046511627907, "no_speech_prob": 2.177226633648388e-05}, {"id": 299, "seek": 209948, "start": 2111.6, "end": 2115.96, "text": " And I don't love the writing style, which is kind of required if you're going to get", "tokens": [400, 286, 500, 380, 959, 264, 3579, 3758, 11, 597, 307, 733, 295, 4739, 498, 291, 434, 516, 281, 483], "temperature": 0.0, "avg_logprob": -0.1858003370223507, "compression_ratio": 1.436046511627907, "no_speech_prob": 2.177226633648388e-05}, {"id": 300, "seek": 209948, "start": 2115.96, "end": 2122.92, "text": " published as being like rather difficult to follow.", "tokens": [6572, 382, 885, 411, 2831, 2252, 281, 1524, 13], "temperature": 0.0, "avg_logprob": -0.1858003370223507, "compression_ratio": 1.436046511627907, "no_speech_prob": 2.177226633648388e-05}, {"id": 301, "seek": 212292, "start": 2122.92, "end": 2132.12, "text": " I have published a couple of papers, but only really one significant deep learning one.", "tokens": [286, 362, 6572, 257, 1916, 295, 10577, 11, 457, 787, 534, 472, 4776, 2452, 2539, 472, 13], "temperature": 0.0, "avg_logprob": -0.21478096811394942, "compression_ratio": 1.597457627118644, "no_speech_prob": 9.366725862491876e-06}, {"id": 302, "seek": 212292, "start": 2132.12, "end": 2138.48, "text": " And that was because a guy named Sebastian Ruter was doing his PhD at the time.", "tokens": [400, 300, 390, 570, 257, 2146, 4926, 31102, 497, 20314, 390, 884, 702, 14476, 412, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.21478096811394942, "compression_ratio": 1.597457627118644, "no_speech_prob": 9.366725862491876e-06}, {"id": 303, "seek": 212292, "start": 2138.48, "end": 2143.04, "text": " And he said it would be really helpful to him if we could co-publish something and that", "tokens": [400, 415, 848, 309, 576, 312, 534, 4961, 281, 796, 498, 321, 727, 598, 12, 79, 836, 1933, 746, 293, 300], "temperature": 0.0, "avg_logprob": -0.21478096811394942, "compression_ratio": 1.597457627118644, "no_speech_prob": 9.366725862491876e-06}, {"id": 304, "seek": 212292, "start": 2143.04, "end": 2146.2000000000003, "text": " he would kind of take the lead on writing the paper.", "tokens": [415, 576, 733, 295, 747, 264, 1477, 322, 3579, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.21478096811394942, "compression_ratio": 1.597457627118644, "no_speech_prob": 9.366725862491876e-06}, {"id": 305, "seek": 212292, "start": 2146.2000000000003, "end": 2150.48, "text": " And so that was good because I'm always very happy to help students.", "tokens": [400, 370, 300, 390, 665, 570, 286, 478, 1009, 588, 2055, 281, 854, 1731, 13], "temperature": 0.0, "avg_logprob": -0.21478096811394942, "compression_ratio": 1.597457627118644, "no_speech_prob": 9.366725862491876e-06}, {"id": 306, "seek": 215048, "start": 2150.48, "end": 2158.44, "text": " And he did a good job and he was a terrific researcher to work with.", "tokens": [400, 415, 630, 257, 665, 1691, 293, 415, 390, 257, 20899, 21751, 281, 589, 365, 13], "temperature": 0.0, "avg_logprob": -0.1879392084868058, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.95128104678588e-05}, {"id": 307, "seek": 215048, "start": 2158.44, "end": 2163.12, "text": " The other time I've written a paper, the main time was when I wanted to get the message", "tokens": [440, 661, 565, 286, 600, 3720, 257, 3035, 11, 264, 2135, 565, 390, 562, 286, 1415, 281, 483, 264, 3636], "temperature": 0.0, "avg_logprob": -0.1879392084868058, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.95128104678588e-05}, {"id": 308, "seek": 215048, "start": 2163.12, "end": 2164.12, "text": " out about masks.", "tokens": [484, 466, 11830, 13], "temperature": 0.0, "avg_logprob": -0.1879392084868058, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.95128104678588e-05}, {"id": 309, "seek": 215048, "start": 2164.12, "end": 2168.96, "text": " And I felt like it probably not going to be taken seriously unless it's an exclusive academic", "tokens": [400, 286, 2762, 411, 309, 1391, 406, 516, 281, 312, 2726, 6638, 5969, 309, 311, 364, 13005, 7778], "temperature": 0.0, "avg_logprob": -0.1879392084868058, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.95128104678588e-05}, {"id": 310, "seek": 215048, "start": 2168.96, "end": 2172.56, "text": " paper because medical people are very into exclusive things.", "tokens": [3035, 570, 4625, 561, 366, 588, 666, 13005, 721, 13], "temperature": 0.0, "avg_logprob": -0.1879392084868058, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.95128104678588e-05}, {"id": 311, "seek": 215048, "start": 2172.56, "end": 2173.56, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.1879392084868058, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.95128104678588e-05}, {"id": 312, "seek": 215048, "start": 2173.56, "end": 2175.96, "text": " So I don't know.", "tokens": [407, 286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.1879392084868058, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.95128104678588e-05}, {"id": 313, "seek": 217596, "start": 2175.96, "end": 2183.56, "text": " Like I'd say like this kind of thing I suspect would be quite hard to publish because most", "tokens": [1743, 286, 1116, 584, 411, 341, 733, 295, 551, 286, 9091, 576, 312, 1596, 1152, 281, 11374, 570, 881], "temperature": 0.0, "avg_logprob": -0.15449422304747534, "compression_ratio": 1.5444444444444445, "no_speech_prob": 6.746545295754913e-06}, {"id": 314, "seek": 217596, "start": 2183.56, "end": 2191.44, "text": " deep learning academic venues are very focused on things with kind of reasonably strong theoretical", "tokens": [2452, 2539, 7778, 32882, 366, 588, 5178, 322, 721, 365, 733, 295, 23551, 2068, 20864], "temperature": 0.0, "avg_logprob": -0.15449422304747534, "compression_ratio": 1.5444444444444445, "no_speech_prob": 6.746545295754913e-06}, {"id": 315, "seek": 217596, "start": 2191.44, "end": 2202.32, "text": " pieces and this kind of field of like trying things and seeing what works is, you know,", "tokens": [3755, 293, 341, 733, 295, 2519, 295, 411, 1382, 721, 293, 2577, 437, 1985, 307, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.15449422304747534, "compression_ratio": 1.5444444444444445, "no_speech_prob": 6.746545295754913e-06}, {"id": 316, "seek": 220232, "start": 2202.32, "end": 2208.96, "text": " content-based is certainly a very important part of science in other areas.", "tokens": [2701, 12, 6032, 307, 3297, 257, 588, 1021, 644, 295, 3497, 294, 661, 3179, 13], "temperature": 0.0, "avg_logprob": -0.3013630105991556, "compression_ratio": 1.5413223140495869, "no_speech_prob": 8.883130067260936e-05}, {"id": 317, "seek": 220232, "start": 2208.96, "end": 2214.2000000000003, "text": " But in the deep learning world, it hasn't really yet been recognised as a valid source", "tokens": [583, 294, 264, 2452, 2539, 1002, 11, 309, 6132, 380, 534, 1939, 668, 36802, 382, 257, 7363, 4009], "temperature": 0.0, "avg_logprob": -0.3013630105991556, "compression_ratio": 1.5413223140495869, "no_speech_prob": 8.883130067260936e-05}, {"id": 318, "seek": 220232, "start": 2214.2000000000003, "end": 2216.6800000000003, "text": " of research as far as I can tell.", "tokens": [295, 2132, 382, 1400, 382, 286, 393, 980, 13], "temperature": 0.0, "avg_logprob": -0.3013630105991556, "compression_ratio": 1.5413223140495869, "no_speech_prob": 8.883130067260936e-05}, {"id": 319, "seek": 220232, "start": 2216.6800000000003, "end": 2222.0800000000004, "text": " Oh, I could concur with all the domains and feel the same quandary to be honest here.", "tokens": [876, 11, 286, 727, 23702, 365, 439, 264, 25514, 293, 841, 264, 912, 6932, 822, 281, 312, 3245, 510, 13], "temperature": 0.0, "avg_logprob": -0.3013630105991556, "compression_ratio": 1.5413223140495869, "no_speech_prob": 8.883130067260936e-05}, {"id": 320, "seek": 220232, "start": 2222.0800000000004, "end": 2223.0800000000004, "text": " Fair enough.", "tokens": [12157, 1547, 13], "temperature": 0.0, "avg_logprob": -0.3013630105991556, "compression_ratio": 1.5413223140495869, "no_speech_prob": 8.883130067260936e-05}, {"id": 321, "seek": 220232, "start": 2223.0800000000004, "end": 2225.0800000000004, "text": " What's your domain?", "tokens": [708, 311, 428, 9274, 30], "temperature": 0.0, "avg_logprob": -0.3013630105991556, "compression_ratio": 1.5413223140495869, "no_speech_prob": 8.883130067260936e-05}, {"id": 322, "seek": 220232, "start": 2225.0800000000004, "end": 2230.6800000000003, "text": " Hydrology, but more the computational science part of it.", "tokens": [24231, 20978, 88, 11, 457, 544, 264, 28270, 3497, 644, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.3013630105991556, "compression_ratio": 1.5413223140495869, "no_speech_prob": 8.883130067260936e-05}, {"id": 323, "seek": 223068, "start": 2230.68, "end": 2232.68, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.37588815910871637, "compression_ratio": 1.1326530612244898, "no_speech_prob": 5.7319062761962414e-05}, {"id": 324, "seek": 223068, "start": 2232.68, "end": 2235.3199999999997, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.37588815910871637, "compression_ratio": 1.1326530612244898, "no_speech_prob": 5.7319062761962414e-05}, {"id": 325, "seek": 223068, "start": 2235.3199999999997, "end": 2251.3199999999997, "text": " So then what I did was I, I mean, this is kind of a bit at the same time, but I went", "tokens": [407, 550, 437, 286, 630, 390, 286, 11, 286, 914, 11, 341, 307, 733, 295, 257, 857, 412, 264, 912, 565, 11, 457, 286, 1437], "temperature": 0.0, "avg_logprob": -0.37588815910871637, "compression_ratio": 1.1326530612244898, "no_speech_prob": 5.7319062761962414e-05}, {"id": 326, "seek": 223068, "start": 2251.3199999999997, "end": 2256.2, "text": " back to Patty.", "tokens": [646, 281, 44116, 13], "temperature": 0.0, "avg_logprob": -0.37588815910871637, "compression_ratio": 1.1326530612244898, "no_speech_prob": 5.7319062761962414e-05}, {"id": 327, "seek": 225620, "start": 2256.2, "end": 2268.4399999999996, "text": " And I wanted to try out a few of these interesting looking models reasonably quickly.", "tokens": [400, 286, 1415, 281, 853, 484, 257, 1326, 295, 613, 1880, 1237, 5245, 23551, 2661, 13], "temperature": 0.0, "avg_logprob": -0.28560848236083985, "compression_ratio": 1.3790849673202614, "no_speech_prob": 1.9517496184562333e-05}, {"id": 328, "seek": 225620, "start": 2268.4399999999996, "end": 2274.2, "text": " So what I did was I kind of took our standard, well, in this case, three lines of code because", "tokens": [407, 437, 286, 630, 390, 286, 733, 295, 1890, 527, 3832, 11, 731, 11, 294, 341, 1389, 11, 1045, 3876, 295, 3089, 570], "temperature": 0.0, "avg_logprob": -0.28560848236083985, "compression_ratio": 1.3790849673202614, "no_speech_prob": 1.9517496184562333e-05}, {"id": 329, "seek": 225620, "start": 2274.2, "end": 2284.3999999999996, "text": " I've already untarget earlier.", "tokens": [286, 600, 1217, 1701, 289, 847, 3071, 13], "temperature": 0.0, "avg_logprob": -0.28560848236083985, "compression_ratio": 1.3790849673202614, "no_speech_prob": 1.9517496184562333e-05}, {"id": 330, "seek": 228440, "start": 2284.4, "end": 2286.56, "text": " Took our three lines of code.", "tokens": [38288, 527, 1045, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.16538210919028834, "compression_ratio": 1.5842105263157895, "no_speech_prob": 8.938327482610475e-06}, {"id": 331, "seek": 228440, "start": 2286.56, "end": 2295.08, "text": " So I could basically say train and pass in an architecture and pass in some per item", "tokens": [407, 286, 727, 1936, 584, 3847, 293, 1320, 294, 364, 9482, 293, 1320, 294, 512, 680, 3174], "temperature": 0.0, "avg_logprob": -0.16538210919028834, "compression_ratio": 1.5842105263157895, "no_speech_prob": 8.938327482610475e-06}, {"id": 332, "seek": 228440, "start": 2295.08, "end": 2302.64, "text": " pre-processing, in this case, resizing everything to the same square using Squish and some per", "tokens": [659, 12, 41075, 278, 11, 294, 341, 1389, 11, 725, 3319, 1203, 281, 264, 912, 3732, 1228, 8683, 742, 293, 512, 680], "temperature": 0.0, "avg_logprob": -0.16538210919028834, "compression_ratio": 1.5842105263157895, "no_speech_prob": 8.938327482610475e-06}, {"id": 333, "seek": 228440, "start": 2302.64, "end": 2308.6, "text": " batch pre-processing, which in this case is the standard FASTA data augmentation transforms", "tokens": [15245, 659, 12, 41075, 278, 11, 597, 294, 341, 1389, 307, 264, 3832, 479, 3160, 8241, 1412, 14501, 19631, 35592], "temperature": 0.0, "avg_logprob": -0.16538210919028834, "compression_ratio": 1.5842105263157895, "no_speech_prob": 8.938327482610475e-06}, {"id": 334, "seek": 230860, "start": 2308.6, "end": 2316.24, "text": " targeting a final size of 224, which is what most models tend to be trained at.", "tokens": [17918, 257, 2572, 2744, 295, 5853, 19, 11, 597, 307, 437, 881, 5245, 3928, 281, 312, 8895, 412, 13], "temperature": 0.0, "avg_logprob": -0.10742904179131807, "compression_ratio": 1.5688622754491017, "no_speech_prob": 2.190727400375181e-06}, {"id": 335, "seek": 230860, "start": 2316.24, "end": 2320.92, "text": " And so then train a model using those parameters.", "tokens": [400, 370, 550, 3847, 257, 2316, 1228, 729, 9834, 13], "temperature": 0.0, "avg_logprob": -0.10742904179131807, "compression_ratio": 1.5688622754491017, "no_speech_prob": 2.190727400375181e-06}, {"id": 336, "seek": 230860, "start": 2320.92, "end": 2325.64, "text": " And then finally, it would use test time augmentation.", "tokens": [400, 550, 2721, 11, 309, 576, 764, 1500, 565, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.10742904179131807, "compression_ratio": 1.5688622754491017, "no_speech_prob": 2.190727400375181e-06}, {"id": 337, "seek": 230860, "start": 2325.64, "end": 2330.16, "text": " So test time augmentation is where I think we briefly mentioned it last time.", "tokens": [407, 1500, 565, 14501, 19631, 307, 689, 286, 519, 321, 10515, 2835, 309, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.10742904179131807, "compression_ratio": 1.5688622754491017, "no_speech_prob": 2.190727400375181e-06}, {"id": 338, "seek": 233016, "start": 2330.16, "end": 2343.7599999999998, "text": " We on this case on the validation set, I basically run the model, the fine tuned model four times", "tokens": [492, 322, 341, 1389, 322, 264, 24071, 992, 11, 286, 1936, 1190, 264, 2316, 11, 264, 2489, 10870, 2316, 1451, 1413], "temperature": 0.0, "avg_logprob": -0.14245375719937411, "compression_ratio": 1.6265060240963856, "no_speech_prob": 9.874499937723158e-07}, {"id": 339, "seek": 233016, "start": 2343.7599999999998, "end": 2347.6, "text": " using random data augmentations each time.", "tokens": [1228, 4974, 1412, 29919, 763, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.14245375719937411, "compression_ratio": 1.6265060240963856, "no_speech_prob": 9.874499937723158e-07}, {"id": 340, "seek": 233016, "start": 2347.6, "end": 2351.6, "text": " And then I run it one more time with no data augmentations at all and take an average of", "tokens": [400, 550, 286, 1190, 309, 472, 544, 565, 365, 572, 1412, 29919, 763, 412, 439, 293, 747, 364, 4274, 295], "temperature": 0.0, "avg_logprob": -0.14245375719937411, "compression_ratio": 1.6265060240963856, "no_speech_prob": 9.874499937723158e-07}, {"id": 341, "seek": 233016, "start": 2351.6, "end": 2354.08, "text": " all of those five predictions basically.", "tokens": [439, 295, 729, 1732, 21264, 1936, 13], "temperature": 0.0, "avg_logprob": -0.14245375719937411, "compression_ratio": 1.6265060240963856, "no_speech_prob": 9.874499937723158e-07}, {"id": 342, "seek": 235408, "start": 2354.08, "end": 2360.48, "text": " And that gives me some predictions and then I take an error rate for TTA, for the test", "tokens": [400, 300, 2709, 385, 512, 21264, 293, 550, 286, 747, 364, 6713, 3314, 337, 314, 8241, 11, 337, 264, 1500], "temperature": 0.0, "avg_logprob": -0.12537464888199515, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.059377023528214e-07}, {"id": 343, "seek": 235408, "start": 2360.48, "end": 2361.88, "text": " time augmentation.", "tokens": [565, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.12537464888199515, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.059377023528214e-07}, {"id": 344, "seek": 235408, "start": 2361.88, "end": 2370.48, "text": " So that basically spits out a number, which is an error rate for Patty.", "tokens": [407, 300, 1936, 637, 1208, 484, 257, 1230, 11, 597, 307, 364, 6713, 3314, 337, 44116, 13], "temperature": 0.0, "avg_logprob": -0.12537464888199515, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.059377023528214e-07}, {"id": 345, "seek": 235408, "start": 2370.48, "end": 2376.36, "text": " And I use a fixed random seed when picking out my validation set.", "tokens": [400, 286, 764, 257, 6806, 4974, 8871, 562, 8867, 484, 452, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.12537464888199515, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.059377023528214e-07}, {"id": 346, "seek": 235408, "start": 2376.36, "end": 2379.6, "text": " So each time I run this, it's going to be with the same validation set.", "tokens": [407, 1184, 565, 286, 1190, 341, 11, 309, 311, 516, 281, 312, 365, 264, 912, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.12537464888199515, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.059377023528214e-07}, {"id": 347, "seek": 235408, "start": 2379.6, "end": 2380.88, "text": " So I can compare.", "tokens": [407, 286, 393, 6794, 13], "temperature": 0.0, "avg_logprob": -0.12537464888199515, "compression_ratio": 1.616504854368932, "no_speech_prob": 8.059377023528214e-07}, {"id": 348, "seek": 238088, "start": 2380.88, "end": 2385.6800000000003, "text": " So I've got a few different next small models I've run.", "tokens": [407, 286, 600, 658, 257, 1326, 819, 958, 1359, 5245, 286, 600, 1190, 13], "temperature": 0.0, "avg_logprob": -0.16856106013467867, "compression_ratio": 1.453416149068323, "no_speech_prob": 6.240803941182094e-06}, {"id": 349, "seek": 238088, "start": 2385.6800000000003, "end": 2397.08, "text": " First of all, by squishing when I resize and then by cropping when I resize.", "tokens": [2386, 295, 439, 11, 538, 2339, 3807, 562, 286, 50069, 293, 550, 538, 4848, 3759, 562, 286, 50069, 13], "temperature": 0.0, "avg_logprob": -0.16856106013467867, "compression_ratio": 1.453416149068323, "no_speech_prob": 6.240803941182094e-06}, {"id": 350, "seek": 238088, "start": 2397.08, "end": 2398.08, "text": " So that was 235.", "tokens": [407, 300, 390, 6673, 20, 13], "temperature": 0.0, "avg_logprob": -0.16856106013467867, "compression_ratio": 1.453416149068323, "no_speech_prob": 6.240803941182094e-06}, {"id": 351, "seek": 238088, "start": 2398.08, "end": 2401.96, "text": " This is also 235.", "tokens": [639, 307, 611, 6673, 20, 13], "temperature": 0.0, "avg_logprob": -0.16856106013467867, "compression_ratio": 1.453416149068323, "no_speech_prob": 6.240803941182094e-06}, {"id": 352, "seek": 238088, "start": 2401.96, "end": 2410.36, "text": " And then instead of resizing to a square, I resize to a rectangle.", "tokens": [400, 550, 2602, 295, 725, 3319, 281, 257, 3732, 11, 286, 50069, 281, 257, 21930, 13], "temperature": 0.0, "avg_logprob": -0.16856106013467867, "compression_ratio": 1.453416149068323, "no_speech_prob": 6.240803941182094e-06}, {"id": 353, "seek": 241036, "start": 2410.36, "end": 2412.08, "text": " In theory, this wouldn't have been necessary.", "tokens": [682, 5261, 11, 341, 2759, 380, 362, 668, 4818, 13], "temperature": 0.0, "avg_logprob": -0.16016317749023437, "compression_ratio": 1.741444866920152, "no_speech_prob": 3.2886923690966796e-06}, {"id": 354, "seek": 241036, "start": 2412.08, "end": 2416.1600000000003, "text": " I thought they were all 480 by 680, sorry, 480 by 640.", "tokens": [286, 1194, 436, 645, 439, 1017, 4702, 538, 1386, 4702, 11, 2597, 11, 1017, 4702, 538, 1386, 5254, 13], "temperature": 0.0, "avg_logprob": -0.16016317749023437, "compression_ratio": 1.741444866920152, "no_speech_prob": 3.2886923690966796e-06}, {"id": 355, "seek": 241036, "start": 2416.1600000000003, "end": 2418.28, "text": " But when I ran this, I got an error.", "tokens": [583, 562, 286, 5872, 341, 11, 286, 658, 364, 6713, 13], "temperature": 0.0, "avg_logprob": -0.16016317749023437, "compression_ratio": 1.741444866920152, "no_speech_prob": 3.2886923690966796e-06}, {"id": 356, "seek": 241036, "start": 2418.28, "end": 2422.48, "text": " And then I looked back at the results of that parallel image sizing thing we ran and I realized", "tokens": [400, 550, 286, 2956, 646, 412, 264, 3542, 295, 300, 8952, 3256, 45435, 551, 321, 5872, 293, 286, 5334], "temperature": 0.0, "avg_logprob": -0.16016317749023437, "compression_ratio": 1.741444866920152, "no_speech_prob": 3.2886923690966796e-06}, {"id": 357, "seek": 241036, "start": 2422.48, "end": 2428.8, "text": " there was actually three or four images that were the opposite aspect ratio.", "tokens": [456, 390, 767, 1045, 420, 1451, 5267, 300, 645, 264, 6182, 4171, 8509, 13], "temperature": 0.0, "avg_logprob": -0.16016317749023437, "compression_ratio": 1.741444866920152, "no_speech_prob": 3.2886923690966796e-06}, {"id": 358, "seek": 241036, "start": 2428.8, "end": 2430.6400000000003, "text": " So that's why.", "tokens": [407, 300, 311, 983, 13], "temperature": 0.0, "avg_logprob": -0.16016317749023437, "compression_ratio": 1.741444866920152, "no_speech_prob": 3.2886923690966796e-06}, {"id": 359, "seek": 241036, "start": 2430.6400000000003, "end": 2434.28, "text": " So the vast majority of the images, this resizing does nothing at all.", "tokens": [407, 264, 8369, 6286, 295, 264, 5267, 11, 341, 725, 3319, 775, 1825, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.16016317749023437, "compression_ratio": 1.741444866920152, "no_speech_prob": 3.2886923690966796e-06}, {"id": 360, "seek": 241036, "start": 2434.28, "end": 2438.46, "text": " But there's three or four that are the opposite aspect ratio.", "tokens": [583, 456, 311, 1045, 420, 1451, 300, 366, 264, 6182, 4171, 8509, 13], "temperature": 0.0, "avg_logprob": -0.16016317749023437, "compression_ratio": 1.741444866920152, "no_speech_prob": 3.2886923690966796e-06}, {"id": 361, "seek": 243846, "start": 2438.46, "end": 2451.08, "text": " And then for the augmentation, yeah, pick a size based on 224 of a similar aspect ratio.", "tokens": [400, 550, 337, 264, 14501, 19631, 11, 1338, 11, 1888, 257, 2744, 2361, 322, 5853, 19, 295, 257, 2531, 4171, 8509, 13], "temperature": 0.0, "avg_logprob": -0.08867725372314453, "compression_ratio": 1.4673366834170853, "no_speech_prob": 7.071634172461927e-06}, {"id": 362, "seek": 243846, "start": 2451.08, "end": 2459.58, "text": " But what I'm actually aiming for here is something that is a multiple of 32 on both edges.", "tokens": [583, 437, 286, 478, 767, 20253, 337, 510, 307, 746, 300, 307, 257, 3866, 295, 8858, 322, 1293, 8819, 13], "temperature": 0.0, "avg_logprob": -0.08867725372314453, "compression_ratio": 1.4673366834170853, "no_speech_prob": 7.071634172461927e-06}, {"id": 363, "seek": 243846, "start": 2459.58, "end": 2463.36, "text": " And the reason for that we'll kind of get into later when we learn about how convolutional", "tokens": [400, 264, 1778, 337, 300, 321, 603, 733, 295, 483, 666, 1780, 562, 321, 1466, 466, 577, 45216, 304], "temperature": 0.0, "avg_logprob": -0.08867725372314453, "compression_ratio": 1.4673366834170853, "no_speech_prob": 7.071634172461927e-06}, {"id": 364, "seek": 243846, "start": 2463.36, "end": 2465.64, "text": " networks really work.", "tokens": [9590, 534, 589, 13], "temperature": 0.0, "avg_logprob": -0.08867725372314453, "compression_ratio": 1.4673366834170853, "no_speech_prob": 7.071634172461927e-06}, {"id": 365, "seek": 246564, "start": 2465.64, "end": 2470.8799999999997, "text": " But it basically turns out that the kind of the final patch size in a conv net is 32 by", "tokens": [583, 309, 1936, 4523, 484, 300, 264, 733, 295, 264, 2572, 9972, 2744, 294, 257, 3754, 2533, 307, 8858, 538], "temperature": 0.0, "avg_logprob": -0.13939503491935085, "compression_ratio": 1.6513409961685823, "no_speech_prob": 8.013176739041228e-06}, {"id": 366, "seek": 246564, "start": 2470.8799999999997, "end": 2471.8799999999997, "text": " 32 pixels.", "tokens": [8858, 18668, 13], "temperature": 0.0, "avg_logprob": -0.13939503491935085, "compression_ratio": 1.6513409961685823, "no_speech_prob": 8.013176739041228e-06}, {"id": 367, "seek": 246564, "start": 2471.8799999999997, "end": 2476.12, "text": " So you generally want both of your sides, normally you want them to be multiples of", "tokens": [407, 291, 5101, 528, 1293, 295, 428, 4881, 11, 5646, 291, 528, 552, 281, 312, 46099, 295], "temperature": 0.0, "avg_logprob": -0.13939503491935085, "compression_ratio": 1.6513409961685823, "no_speech_prob": 8.013176739041228e-06}, {"id": 368, "seek": 246564, "start": 2476.12, "end": 2478.8799999999997, "text": " 32.", "tokens": [8858, 13], "temperature": 0.0, "avg_logprob": -0.13939503491935085, "compression_ratio": 1.6513409961685823, "no_speech_prob": 8.013176739041228e-06}, {"id": 369, "seek": 246564, "start": 2478.8799999999997, "end": 2483.2799999999997, "text": " So this one, you've got a pretty similar result again, 240.", "tokens": [407, 341, 472, 11, 291, 600, 658, 257, 1238, 2531, 1874, 797, 11, 26837, 13], "temperature": 0.0, "avg_logprob": -0.13939503491935085, "compression_ratio": 1.6513409961685823, "no_speech_prob": 8.013176739041228e-06}, {"id": 370, "seek": 246564, "start": 2483.2799999999997, "end": 2486.72, "text": " And then, you know, I wasn't sure about my contention that they need to be multiples", "tokens": [400, 550, 11, 291, 458, 11, 286, 2067, 380, 988, 466, 452, 660, 1251, 300, 436, 643, 281, 312, 46099], "temperature": 0.0, "avg_logprob": -0.13939503491935085, "compression_ratio": 1.6513409961685823, "no_speech_prob": 8.013176739041228e-06}, {"id": 371, "seek": 246564, "start": 2486.72, "end": 2487.72, "text": " of 32.", "tokens": [295, 8858, 13], "temperature": 0.0, "avg_logprob": -0.13939503491935085, "compression_ratio": 1.6513409961685823, "no_speech_prob": 8.013176739041228e-06}, {"id": 372, "seek": 246564, "start": 2487.72, "end": 2493.8799999999997, "text": " I thought maybe it's better if they like a really crisp resizing by using an exact multiple.", "tokens": [286, 1194, 1310, 309, 311, 1101, 498, 436, 411, 257, 534, 22952, 725, 3319, 538, 1228, 364, 1900, 3866, 13], "temperature": 0.0, "avg_logprob": -0.13939503491935085, "compression_ratio": 1.6513409961685823, "no_speech_prob": 8.013176739041228e-06}, {"id": 373, "seek": 249388, "start": 2493.88, "end": 2497.52, "text": " So I tried that as well.", "tokens": [407, 286, 3031, 300, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.3285730906895229, "compression_ratio": 1.2936507936507937, "no_speech_prob": 1.5445044482476078e-05}, {"id": 374, "seek": 249388, "start": 2497.52, "end": 2504.88, "text": " And that, as I suspected, was a bit worse.", "tokens": [400, 300, 11, 382, 286, 26439, 11, 390, 257, 857, 5324, 13], "temperature": 0.0, "avg_logprob": -0.3285730906895229, "compression_ratio": 1.2936507936507937, "no_speech_prob": 1.5445044482476078e-05}, {"id": 375, "seek": 249388, "start": 2504.88, "end": 2512.4, "text": " And oh, what's this?", "tokens": [400, 1954, 11, 437, 311, 341, 30], "temperature": 0.0, "avg_logprob": -0.3285730906895229, "compression_ratio": 1.2936507936507937, "no_speech_prob": 1.5445044482476078e-05}, {"id": 376, "seek": 249388, "start": 2512.4, "end": 2515.28, "text": " I've got some which which ones are the right way around.", "tokens": [286, 600, 658, 512, 597, 597, 2306, 366, 264, 558, 636, 926, 13], "temperature": 0.0, "avg_logprob": -0.3285730906895229, "compression_ratio": 1.2936507936507937, "no_speech_prob": 1.5445044482476078e-05}, {"id": 377, "seek": 249388, "start": 2515.28, "end": 2517.32, "text": " Now I'm confused.", "tokens": [823, 286, 478, 9019, 13], "temperature": 0.0, "avg_logprob": -0.3285730906895229, "compression_ratio": 1.2936507936507937, "no_speech_prob": 1.5445044482476078e-05}, {"id": 378, "seek": 251732, "start": 2517.32, "end": 2528.8, "text": " I think let's check.", "tokens": [286, 519, 718, 311, 1520, 13], "temperature": 0.0, "avg_logprob": -0.2594480323791504, "compression_ratio": 1.3211678832116789, "no_speech_prob": 8.52915854920866e-06}, {"id": 379, "seek": 251732, "start": 2528.8, "end": 2533.96, "text": " Some of these originally I had my aspect ratio backwards.", "tokens": [2188, 295, 613, 7993, 286, 632, 452, 4171, 8509, 12204, 13], "temperature": 0.0, "avg_logprob": -0.2594480323791504, "compression_ratio": 1.3211678832116789, "no_speech_prob": 8.52915854920866e-06}, {"id": 380, "seek": 251732, "start": 2533.96, "end": 2534.96, "text": " That's why I've got both.", "tokens": [663, 311, 983, 286, 600, 658, 1293, 13], "temperature": 0.0, "avg_logprob": -0.2594480323791504, "compression_ratio": 1.3211678832116789, "no_speech_prob": 8.52915854920866e-06}, {"id": 381, "seek": 251732, "start": 2534.96, "end": 2541.2000000000003, "text": " It looks like I never got around to removing the ones that were unnecessary.", "tokens": [467, 1542, 411, 286, 1128, 658, 926, 281, 12720, 264, 2306, 300, 645, 19350, 13], "temperature": 0.0, "avg_logprob": -0.2594480323791504, "compression_ratio": 1.3211678832116789, "no_speech_prob": 8.52915854920866e-06}, {"id": 382, "seek": 254120, "start": 2541.2, "end": 2557.54, "text": " To even things in the right order by adding these.", "tokens": [1407, 754, 721, 294, 264, 558, 1668, 538, 5127, 613, 13], "temperature": 1.0, "avg_logprob": -2.873325892857143, "compression_ratio": 0.9705882352941176, "no_speech_prob": 0.00023396735196001828}, {"id": 383, "seek": 254120, "start": 2557.54, "end": 2559.3399999999997, "text": " Just.", "tokens": [1449, 13], "temperature": 1.0, "avg_logprob": -2.873325892857143, "compression_ratio": 0.9705882352941176, "no_speech_prob": 0.00023396735196001828}, {"id": 384, "seek": 254120, "start": 2559.3399999999997, "end": 2561.52, "text": " It's.", "tokens": [467, 311, 13], "temperature": 1.0, "avg_logprob": -2.873325892857143, "compression_ratio": 0.9705882352941176, "no_speech_prob": 0.00023396735196001828}, {"id": 385, "seek": 254120, "start": 2561.52, "end": 2562.96, "text": " As.", "tokens": [1018, 13], "temperature": 1.0, "avg_logprob": -2.873325892857143, "compression_ratio": 0.9705882352941176, "no_speech_prob": 0.00023396735196001828}, {"id": 386, "seek": 256296, "start": 2562.96, "end": 2570.96, "text": " method equals add. Oops. Pad mode. This makes it a bit easier to see what's going on if you do", "tokens": [3170, 6915, 909, 13, 21726, 13, 18691, 4391, 13, 639, 1669, 309, 257, 857, 3571, 281, 536, 437, 311, 516, 322, 498, 291, 360], "temperature": 0.0, "avg_logprob": -0.45038302285330634, "compression_ratio": 1.1886792452830188, "no_speech_prob": 0.045983608812093735}, {"id": 387, "seek": 257096, "start": 2570.96, "end": 2578.96, "text": " padding with black around them. It's a bit of a pain in the ass.", "tokens": [50364, 39562, 365, 2211, 926, 552, 13, 467, 311, 257, 857, 295, 257, 1822, 294, 264, 1256, 13, 50764], "temperature": 0.0, "avg_logprob": -0.9411792755126953, "compression_ratio": 0.9411764705882353, "no_speech_prob": 8.749001426622272e-05}, {"id": 388, "seek": 260096, "start": 2600.96, "end": 2614.96, "text": " There we go. Okay, yeah, so you can clearly see this is the wrong way around, right? I've tried to make them wide, but actually they were tall. So the best way around is actually", "tokens": [821, 321, 352, 13, 1033, 11, 1338, 11, 370, 291, 393, 4448, 536, 341, 307, 264, 2085, 636, 926, 11, 558, 30, 286, 600, 3031, 281, 652, 552, 4874, 11, 457, 767, 436, 645, 6764, 13, 407, 264, 1151, 636, 926, 307, 767], "temperature": 0.0, "avg_logprob": -0.2718805556601666, "compression_ratio": 1.3185185185185184, "no_speech_prob": 0.16010482609272003}, {"id": 389, "seek": 261496, "start": 2614.96, "end": 2634.96, "text": " 640 by 480. That's more like it. So 640 by 480 is best. So let's get rid of the ones that were the wrong way around.", "tokens": [1386, 5254, 538, 1017, 4702, 13, 663, 311, 544, 411, 309, 13, 407, 1386, 5254, 538, 1017, 4702, 307, 1151, 13, 407, 718, 311, 483, 3973, 295, 264, 2306, 300, 645, 264, 2085, 636, 926, 13], "temperature": 0.0, "avg_logprob": -0.08523850440979004, "compression_ratio": 1.1958762886597938, "no_speech_prob": 3.1691142794443294e-05}, {"id": 390, "seek": 263496, "start": 2634.96, "end": 2654.96, "text": " Okay. All right. Yeah, so that was all, you know, various different transforms, pre-processing for ConvNext Small, and then I did the same thing for one of the VITs, VIT Small.", "tokens": [1033, 13, 1057, 558, 13, 865, 11, 370, 300, 390, 439, 11, 291, 458, 11, 3683, 819, 35592, 11, 659, 12, 41075, 278, 337, 2656, 85, 31002, 15287, 11, 293, 550, 286, 630, 264, 912, 551, 337, 472, 295, 264, 691, 3927, 82, 11, 691, 3927, 15287, 13], "temperature": 0.0, "avg_logprob": -0.257216398532574, "compression_ratio": 1.2753623188405796, "no_speech_prob": 1.8630604245117866e-05}, {"id": 391, "seek": 265496, "start": 2654.96, "end": 2664.96, "text": " Now VIT, remember I mentioned it can only work on 224 by 224 images, so these rectangular approaches aren't going to be possible.", "tokens": [823, 691, 3927, 11, 1604, 286, 2835, 309, 393, 787, 589, 322, 5853, 19, 538, 5853, 19, 5267, 11, 370, 613, 31167, 11587, 3212, 380, 516, 281, 312, 1944, 13], "temperature": 0.0, "avg_logprob": -0.09495076011208926, "compression_ratio": 1.1415929203539823, "no_speech_prob": 7.182788067439105e-06}, {"id": 392, "seek": 266496, "start": 2664.96, "end": 2690.96, "text": " So I've just got the Squish and the Crop versions. The Crop version doesn't look very good. The Squish version does look pretty good.", "tokens": [407, 286, 600, 445, 658, 264, 8683, 742, 293, 264, 383, 1513, 9606, 13, 440, 383, 1513, 3037, 1177, 380, 574, 588, 665, 13, 440, 8683, 742, 3037, 775, 574, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.0727104496311497, "compression_ratio": 1.4301075268817205, "no_speech_prob": 7.721101428614929e-05}, {"id": 393, "seek": 269096, "start": 2690.96, "end": 2702.96, "text": " And I also tried a Pad version, which looks pretty good. And then, yeah, I also tried Swin. So here's Swin V2.", "tokens": [400, 286, 611, 3031, 257, 18691, 3037, 11, 597, 1542, 1238, 665, 13, 400, 550, 11, 1338, 11, 286, 611, 3031, 3926, 259, 13, 407, 510, 311, 3926, 259, 691, 17, 13], "temperature": 0.0, "avg_logprob": -0.1618745724360148, "compression_ratio": 1.1458333333333333, "no_speech_prob": 4.784970315085957e-06}, {"id": 394, "seek": 270296, "start": 2702.96, "end": 2722.96, "text": " And this one is slow and memory intensive. So I had to go down to the 192 pixel version. But actually it seems to work very well. This is the first time we've had one that's better than 0.02.", "tokens": [400, 341, 472, 307, 2964, 293, 4675, 18957, 13, 407, 286, 632, 281, 352, 760, 281, 264, 1294, 17, 19261, 3037, 13, 583, 767, 309, 2544, 281, 589, 588, 731, 13, 639, 307, 264, 700, 565, 321, 600, 632, 472, 300, 311, 1101, 813, 1958, 13, 12756, 13], "temperature": 0.0, "avg_logprob": -0.06626439094543457, "compression_ratio": 1.3082191780821917, "no_speech_prob": 1.6797063153717318e-06}, {"id": 395, "seek": 272296, "start": 2722.96, "end": 2732.96, "text": " It is interesting.", "tokens": [467, 307, 1880, 13], "temperature": 0.0, "avg_logprob": -0.09806585311889648, "compression_ratio": 1.3840579710144927, "no_speech_prob": 4.784923021361465e-06}, {"id": 396, "seek": 272296, "start": 2732.96, "end": 2750.96, "text": " This one's also very good. So it's interesting that this slow memory intensive model works better even on smaller size, 192 pixel size, which I think is pretty interesting.", "tokens": [639, 472, 311, 611, 588, 665, 13, 407, 309, 311, 1880, 300, 341, 2964, 4675, 18957, 2316, 1985, 1101, 754, 322, 4356, 2744, 11, 1294, 17, 19261, 2744, 11, 597, 286, 519, 307, 1238, 1880, 13], "temperature": 0.0, "avg_logprob": -0.09806585311889648, "compression_ratio": 1.3840579710144927, "no_speech_prob": 4.784923021361465e-06}, {"id": 397, "seek": 275096, "start": 2750.96, "end": 2763.96, "text": " And then there's one more Swin which seemed to do pretty well. So I included that, which I was able to do it 224. That one had", "tokens": [400, 550, 456, 311, 472, 544, 3926, 259, 597, 6576, 281, 360, 1238, 731, 13, 407, 286, 5556, 300, 11, 597, 286, 390, 1075, 281, 360, 309, 5853, 19, 13, 663, 472, 632], "temperature": 0.0, "avg_logprob": -0.18770412496618322, "compression_ratio": 1.1775700934579438, "no_speech_prob": 8.664261258672923e-06}, {"id": 398, "seek": 276396, "start": 2763.96, "end": 2782.96, "text": " pretty good results. So like I kind of did that for all these different small models. And as you can see, they run pretty quickly, right? Five or ten minutes. And so then I picked out the ones that look pretty fast, sorry, pretty fast, pretty accurate.", "tokens": [1238, 665, 3542, 13, 407, 411, 286, 733, 295, 630, 300, 337, 439, 613, 819, 1359, 5245, 13, 400, 382, 291, 393, 536, 11, 436, 1190, 1238, 2661, 11, 558, 30, 9436, 420, 2064, 2077, 13, 400, 370, 550, 286, 6183, 484, 264, 2306, 300, 574, 1238, 2370, 11, 2597, 11, 1238, 2370, 11, 1238, 8559, 13], "temperature": 0.0, "avg_logprob": -0.21832872609623144, "compression_ratio": 1.5180722891566265, "no_speech_prob": 8.446068591183575e-07}, {"id": 399, "seek": 278296, "start": 2782.96, "end": 2800.96, "text": " And created just a copy of that, which I called Paddy Large. And this time I just replaced small with large.", "tokens": [400, 2942, 445, 257, 5055, 295, 300, 11, 597, 286, 1219, 18691, 3173, 33092, 13, 400, 341, 565, 286, 445, 10772, 1359, 365, 2416, 13], "temperature": 0.0, "avg_logprob": -0.13923653803373637, "compression_ratio": 1.517766497461929, "no_speech_prob": 1.7330077071164851e-06}, {"id": 400, "seek": 278296, "start": 2800.96, "end": 2809.96, "text": " And actually I've made a mistake. I'm going to have to rerun this because there should be not, there should not be a C equals 42. I actually want to run this on a different subset each time.", "tokens": [400, 767, 286, 600, 1027, 257, 6146, 13, 286, 478, 516, 281, 362, 281, 43819, 409, 341, 570, 456, 820, 312, 406, 11, 456, 820, 406, 312, 257, 383, 6915, 14034, 13, 286, 767, 528, 281, 1190, 341, 322, 257, 819, 25993, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.13923653803373637, "compression_ratio": 1.517766497461929, "no_speech_prob": 1.7330077071164851e-06}, {"id": 401, "seek": 280996, "start": 2809.96, "end": 2825.96, "text": " And the reason why is my plan is to train. So basically what I did is I deleted the ones that were less good in Paddy Small. And so now just running the large ones.", "tokens": [400, 264, 1778, 983, 307, 452, 1393, 307, 281, 3847, 13, 407, 1936, 437, 286, 630, 307, 286, 22981, 264, 2306, 300, 645, 1570, 665, 294, 18691, 3173, 15287, 13, 400, 370, 586, 445, 2614, 264, 2416, 2306, 13], "temperature": 0.0, "avg_logprob": -0.11421946798052107, "compression_ratio": 1.4074074074074074, "no_speech_prob": 8.530003469786607e-06}, {"id": 402, "seek": 280996, "start": 2825.96, "end": 2833.96, "text": " Now some of these, particularly something like this one, which is 288 by 224. They ran out of memory.", "tokens": [823, 512, 295, 613, 11, 4098, 746, 411, 341, 472, 11, 597, 307, 7562, 23, 538, 5853, 19, 13, 814, 5872, 484, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.11421946798052107, "compression_ratio": 1.4074074074074074, "no_speech_prob": 8.530003469786607e-06}, {"id": 403, "seek": 283396, "start": 2833.96, "end": 2842.96, "text": " They were too big for my graphics card. And a lot of people at this point say, oh, I need to go buy a more expensive graphics card.", "tokens": [814, 645, 886, 955, 337, 452, 11837, 2920, 13, 400, 257, 688, 295, 561, 412, 341, 935, 584, 11, 1954, 11, 286, 643, 281, 352, 2256, 257, 544, 5124, 11837, 2920, 13], "temperature": 0.0, "avg_logprob": -0.08473099283425205, "compression_ratio": 1.5343915343915344, "no_speech_prob": 2.7263884021522244e-06}, {"id": 404, "seek": 283396, "start": 2842.96, "end": 2846.96, "text": " But that's not true. You don't.", "tokens": [583, 300, 311, 406, 2074, 13, 509, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.08473099283425205, "compression_ratio": 1.5343915343915344, "no_speech_prob": 2.7263884021522244e-06}, {"id": 405, "seek": 283396, "start": 2846.96, "end": 2853.96, "text": " So, if you guys remember our training loop.", "tokens": [407, 11, 498, 291, 1074, 1604, 527, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.08473099283425205, "compression_ratio": 1.5343915343915344, "no_speech_prob": 2.7263884021522244e-06}, {"id": 406, "seek": 283396, "start": 2853.96, "end": 2856.96, "text": " We get the gradients.", "tokens": [492, 483, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.08473099283425205, "compression_ratio": 1.5343915343915344, "no_speech_prob": 2.7263884021522244e-06}, {"id": 407, "seek": 283396, "start": 2856.96, "end": 2860.96, "text": " We add the gradients times the learning rate to the weights.", "tokens": [492, 909, 264, 2771, 2448, 1413, 264, 2539, 3314, 281, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.08473099283425205, "compression_ratio": 1.5343915343915344, "no_speech_prob": 2.7263884021522244e-06}, {"id": 408, "seek": 286096, "start": 2860.96, "end": 2863.96, "text": " And then we zero the gradients.", "tokens": [400, 550, 321, 4018, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.10434358384874132, "compression_ratio": 1.7227722772277227, "no_speech_prob": 2.7693586162058637e-06}, {"id": 409, "seek": 286096, "start": 2863.96, "end": 2867.96, "text": " What you could do is half the batch size.", "tokens": [708, 291, 727, 360, 307, 1922, 264, 15245, 2744, 13], "temperature": 0.0, "avg_logprob": -0.10434358384874132, "compression_ratio": 1.7227722772277227, "no_speech_prob": 2.7693586162058637e-06}, {"id": 410, "seek": 286096, "start": 2867.96, "end": 2870.96, "text": " So, for example, from 64 to 32.", "tokens": [407, 11, 337, 1365, 11, 490, 12145, 281, 8858, 13], "temperature": 0.0, "avg_logprob": -0.10434358384874132, "compression_ratio": 1.7227722772277227, "no_speech_prob": 2.7693586162058637e-06}, {"id": 411, "seek": 286096, "start": 2870.96, "end": 2875.96, "text": " And then only zero the gradients every two iterations.", "tokens": [400, 550, 787, 4018, 264, 2771, 2448, 633, 732, 36540, 13], "temperature": 0.0, "avg_logprob": -0.10434358384874132, "compression_ratio": 1.7227722772277227, "no_speech_prob": 2.7693586162058637e-06}, {"id": 412, "seek": 286096, "start": 2875.96, "end": 2887.96, "text": " Right. And so, and only do the update every two iterations. So basically you can calculate in two batches what you used to calculate in one batch, and it will be mathematically identical.", "tokens": [1779, 13, 400, 370, 11, 293, 787, 360, 264, 5623, 633, 732, 36540, 13, 407, 1936, 291, 393, 8873, 294, 732, 15245, 279, 437, 291, 1143, 281, 8873, 294, 472, 15245, 11, 293, 309, 486, 312, 44003, 14800, 13], "temperature": 0.0, "avg_logprob": -0.10434358384874132, "compression_ratio": 1.7227722772277227, "no_speech_prob": 2.7693586162058637e-06}, {"id": 413, "seek": 288796, "start": 2887.96, "end": 2890.96, "text": " And that's called gradient accumulation.", "tokens": [400, 300, 311, 1219, 16235, 35647, 13], "temperature": 0.0, "avg_logprob": -0.15005925758597777, "compression_ratio": 1.6359649122807018, "no_speech_prob": 7.071799245750299e-06}, {"id": 414, "seek": 288796, "start": 2890.96, "end": 2896.96, "text": " And so for the ones which ran out of memory, I added this little accume equals true.", "tokens": [400, 370, 337, 264, 2306, 597, 5872, 484, 295, 4675, 11, 286, 3869, 341, 707, 1317, 2540, 6915, 2074, 13], "temperature": 0.0, "avg_logprob": -0.15005925758597777, "compression_ratio": 1.6359649122807018, "no_speech_prob": 7.071799245750299e-06}, {"id": 415, "seek": 288796, "start": 2896.96, "end": 2903.96, "text": " Which is here in my function. And I said, yeah, I said if accume equals true, then set the batch size to 32.", "tokens": [3013, 307, 510, 294, 452, 2445, 13, 400, 286, 848, 11, 1338, 11, 286, 848, 498, 1317, 2540, 6915, 2074, 11, 550, 992, 264, 15245, 2744, 281, 8858, 13], "temperature": 0.0, "avg_logprob": -0.15005925758597777, "compression_ratio": 1.6359649122807018, "no_speech_prob": 7.071799245750299e-06}, {"id": 416, "seek": 288796, "start": 2903.96, "end": 2908.96, "text": " Because by default it's 64. And add", "tokens": [1436, 538, 7576, 309, 311, 12145, 13, 400, 909], "temperature": 0.0, "avg_logprob": -0.15005925758597777, "compression_ratio": 1.6359649122807018, "no_speech_prob": 7.071799245750299e-06}, {"id": 417, "seek": 288796, "start": 2908.96, "end": 2914.96, "text": " this thing called a callback. Callbacks are basically things that change the behavior of the training.", "tokens": [341, 551, 1219, 257, 818, 3207, 13, 7807, 17758, 366, 1936, 721, 300, 1319, 264, 5223, 295, 264, 3097, 13], "temperature": 0.0, "avg_logprob": -0.15005925758597777, "compression_ratio": 1.6359649122807018, "no_speech_prob": 7.071799245750299e-06}, {"id": 418, "seek": 291496, "start": 2914.96, "end": 2918.96, "text": " And there's a thing called gradient accumulation callback.", "tokens": [400, 456, 311, 257, 551, 1219, 16235, 35647, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.15858673166345666, "compression_ratio": 1.6304347826086956, "no_speech_prob": 5.093610980111407e-06}, {"id": 419, "seek": 291496, "start": 2918.96, "end": 2928.96, "text": " Which", "tokens": [3013], "temperature": 0.0, "avg_logprob": -0.15858673166345666, "compression_ratio": 1.6304347826086956, "no_speech_prob": 5.093610980111407e-06}, {"id": 420, "seek": 291496, "start": 2928.96, "end": 2931.96, "text": " gradient accumulation.", "tokens": [16235, 35647, 13], "temperature": 0.0, "avg_logprob": -0.15858673166345666, "compression_ratio": 1.6304347826086956, "no_speech_prob": 5.093610980111407e-06}, {"id": 421, "seek": 291496, "start": 2931.96, "end": 2935.96, "text": " And this is like just for people that are interested.", "tokens": [400, 341, 307, 411, 445, 337, 561, 300, 366, 3102, 13], "temperature": 0.0, "avg_logprob": -0.15858673166345666, "compression_ratio": 1.6304347826086956, "no_speech_prob": 5.093610980111407e-06}, {"id": 422, "seek": 291496, "start": 2935.96, "end": 2938.96, "text": " This is not like", "tokens": [639, 307, 406, 411], "temperature": 0.0, "avg_logprob": -0.15858673166345666, "compression_ratio": 1.6304347826086956, "no_speech_prob": 5.093610980111407e-06}, {"id": 423, "seek": 291496, "start": 2938.96, "end": 2942.96, "text": " massively complex stuff. The entire gradient accumulation callback", "tokens": [29379, 3997, 1507, 13, 440, 2302, 16235, 35647, 818, 3207], "temperature": 0.0, "avg_logprob": -0.15858673166345666, "compression_ratio": 1.6304347826086956, "no_speech_prob": 5.093610980111407e-06}, {"id": 424, "seek": 294296, "start": 2942.96, "end": 2945.96, "text": " is that many lines of code.", "tokens": [307, 300, 867, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.14654993329729352, "compression_ratio": 1.5157232704402517, "no_speech_prob": 8.664227607368957e-06}, {"id": 425, "seek": 294296, "start": 2945.96, "end": 2955.96, "text": " These are not big things. And like literally all it does is it keeps account of how many iterations it's been.", "tokens": [1981, 366, 406, 955, 721, 13, 400, 411, 3736, 439, 309, 775, 307, 309, 5965, 2696, 295, 577, 867, 36540, 309, 311, 668, 13], "temperature": 0.0, "avg_logprob": -0.14654993329729352, "compression_ratio": 1.5157232704402517, "no_speech_prob": 8.664227607368957e-06}, {"id": 426, "seek": 294296, "start": 2955.96, "end": 2960.96, "text": " And it", "tokens": [400, 309], "temperature": 0.0, "avg_logprob": -0.14654993329729352, "compression_ratio": 1.5157232704402517, "no_speech_prob": 8.664227607368957e-06}, {"id": 427, "seek": 294296, "start": 2960.96, "end": 2964.96, "text": " adds the, you know, keeps track of the count.", "tokens": [10860, 264, 11, 291, 458, 11, 5965, 2837, 295, 264, 1207, 13], "temperature": 0.0, "avg_logprob": -0.14654993329729352, "compression_ratio": 1.5157232704402517, "no_speech_prob": 8.664227607368957e-06}, {"id": 428, "seek": 294296, "start": 2964.96, "end": 2968.96, "text": " And as long as we're not up to the point where we", "tokens": [400, 382, 938, 382, 321, 434, 406, 493, 281, 264, 935, 689, 321], "temperature": 0.0, "avg_logprob": -0.14654993329729352, "compression_ratio": 1.5157232704402517, "no_speech_prob": 8.664227607368957e-06}, {"id": 429, "seek": 296896, "start": 2968.96, "end": 2977.96, "text": " there's the number of accumulations we want, we skip the step and the zero gradient basically.", "tokens": [456, 311, 264, 1230, 295, 12989, 4136, 321, 528, 11, 321, 10023, 264, 1823, 293, 264, 4018, 16235, 1936, 13], "temperature": 0.0, "avg_logprob": -0.18064454776137623, "compression_ratio": 1.5114942528735633, "no_speech_prob": 2.7964752007392235e-05}, {"id": 430, "seek": 296896, "start": 2977.96, "end": 2987.96, "text": " So it's yeah, things like gradient accumulation, they sound like big complex things, but they turn out", "tokens": [407, 309, 311, 1338, 11, 721, 411, 16235, 35647, 11, 436, 1626, 411, 955, 3997, 721, 11, 457, 436, 1261, 484], "temperature": 0.0, "avg_logprob": -0.18064454776137623, "compression_ratio": 1.5114942528735633, "no_speech_prob": 2.7964752007392235e-05}, {"id": 431, "seek": 296896, "start": 2987.96, "end": 2995.96, "text": " not to be, at least when you have a nice code base like fast AIs.", "tokens": [406, 281, 312, 11, 412, 1935, 562, 291, 362, 257, 1481, 3089, 3096, 411, 2370, 316, 6802, 13], "temperature": 0.0, "avg_logprob": -0.18064454776137623, "compression_ratio": 1.5114942528735633, "no_speech_prob": 2.7964752007392235e-05}, {"id": 432, "seek": 299596, "start": 2995.96, "end": 2999.96, "text": " Jeremy, can I get a question here? Of course.", "tokens": [17809, 11, 393, 286, 483, 257, 1168, 510, 30, 2720, 1164, 13], "temperature": 0.0, "avg_logprob": -0.26766180613684276, "compression_ratio": 1.471698113207547, "no_speech_prob": 6.919637962710112e-05}, {"id": 433, "seek": 299596, "start": 2999.96, "end": 3008.96, "text": " How exactly did the batch size mass nations work?", "tokens": [1012, 2293, 630, 264, 15245, 2744, 2758, 11035, 589, 30], "temperature": 0.0, "avg_logprob": -0.26766180613684276, "compression_ratio": 1.471698113207547, "no_speech_prob": 6.919637962710112e-05}, {"id": 434, "seek": 299596, "start": 3008.96, "end": 3015.96, "text": " So we will get into that in detail in the course. And it certainly we get into it in detail in the book.", "tokens": [407, 321, 486, 483, 666, 300, 294, 2607, 294, 264, 1164, 13, 400, 309, 3297, 321, 483, 666, 309, 294, 2607, 294, 264, 1446, 13], "temperature": 0.0, "avg_logprob": -0.26766180613684276, "compression_ratio": 1.471698113207547, "no_speech_prob": 6.919637962710112e-05}, {"id": 435, "seek": 299596, "start": 3015.96, "end": 3020.96, "text": " But basically all that happens is", "tokens": [583, 1936, 439, 300, 2314, 307], "temperature": 0.0, "avg_logprob": -0.26766180613684276, "compression_ratio": 1.471698113207547, "no_speech_prob": 6.919637962710112e-05}, {"id": 436, "seek": 302096, "start": 3020.96, "end": 3030.96, "text": " we randomly shuffle the data set and we grab the batch sizes 64. We grab the next 64 images.", "tokens": [321, 16979, 39426, 264, 1412, 992, 293, 321, 4444, 264, 15245, 11602, 12145, 13, 492, 4444, 264, 958, 12145, 5267, 13], "temperature": 0.0, "avg_logprob": -0.14989595115184784, "compression_ratio": 1.5128205128205128, "no_speech_prob": 3.446300070208963e-06}, {"id": 437, "seek": 302096, "start": 3030.96, "end": 3036.96, "text": " We resize them all to be the same size and we stack them on top of each other.", "tokens": [492, 50069, 552, 439, 281, 312, 264, 912, 2744, 293, 321, 8630, 552, 322, 1192, 295, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.14989595115184784, "compression_ratio": 1.5128205128205128, "no_speech_prob": 3.446300070208963e-06}, {"id": 438, "seek": 302096, "start": 3036.96, "end": 3043.96, "text": " So if it's black and white images, for example, we would have 64", "tokens": [407, 498, 309, 311, 2211, 293, 2418, 5267, 11, 337, 1365, 11, 321, 576, 362, 12145], "temperature": 0.0, "avg_logprob": -0.14989595115184784, "compression_ratio": 1.5128205128205128, "no_speech_prob": 3.446300070208963e-06}, {"id": 439, "seek": 304396, "start": 3043.96, "end": 3051.96, "text": " whatever, 640 by 480 images. And so we would end up with a 64 by 640 by 480", "tokens": [2035, 11, 1386, 5254, 538, 1017, 4702, 5267, 13, 400, 370, 321, 576, 917, 493, 365, 257, 12145, 538, 1386, 5254, 538, 1017, 4702], "temperature": 0.0, "avg_logprob": -0.16440849572839872, "compression_ratio": 1.4484848484848485, "no_speech_prob": 9.816937563300598e-06}, {"id": 440, "seek": 304396, "start": 3051.96, "end": 3058.96, "text": " tensor. And pretty much all the like", "tokens": [40863, 13, 400, 1238, 709, 439, 264, 411], "temperature": 0.0, "avg_logprob": -0.16440849572839872, "compression_ratio": 1.4484848484848485, "no_speech_prob": 9.816937563300598e-06}, {"id": 441, "seek": 304396, "start": 3058.96, "end": 3064.96, "text": " functionality provided by Tytorch will work fine", "tokens": [14980, 5649, 538, 314, 4328, 284, 339, 486, 589, 2489], "temperature": 0.0, "avg_logprob": -0.16440849572839872, "compression_ratio": 1.4484848484848485, "no_speech_prob": 9.816937563300598e-06}, {"id": 442, "seek": 304396, "start": 3064.96, "end": 3072.96, "text": " for a mini batch of things, just as it would for a single thing on the whole.", "tokens": [337, 257, 8382, 15245, 295, 721, 11, 445, 382, 309, 576, 337, 257, 2167, 551, 322, 264, 1379, 13], "temperature": 0.0, "avg_logprob": -0.16440849572839872, "compression_ratio": 1.4484848484848485, "no_speech_prob": 9.816937563300598e-06}, {"id": 443, "seek": 307296, "start": 3072.96, "end": 3077.96, "text": " So in the in the large scheme of things, you know, like", "tokens": [407, 294, 264, 294, 264, 2416, 12232, 295, 721, 11, 291, 458, 11, 411], "temperature": 0.0, "avg_logprob": -0.2202547119884956, "compression_ratio": 1.5336787564766838, "no_speech_prob": 3.216887853341177e-05}, {"id": 444, "seek": 307296, "start": 3077.96, "end": 3084.96, "text": " some huge processes that's trying to characterize", "tokens": [512, 2603, 7555, 300, 311, 1382, 281, 38463], "temperature": 0.0, "avg_logprob": -0.2202547119884956, "compression_ratio": 1.5336787564766838, "no_speech_prob": 3.216887853341177e-05}, {"id": 445, "seek": 307296, "start": 3084.96, "end": 3092.96, "text": " what roles the batch sort of claim. Well, it's just it's just about trying to get the most out of your GPU.", "tokens": [437, 9604, 264, 15245, 1333, 295, 3932, 13, 1042, 11, 309, 311, 445, 309, 311, 445, 466, 1382, 281, 483, 264, 881, 484, 295, 428, 18407, 13], "temperature": 0.0, "avg_logprob": -0.2202547119884956, "compression_ratio": 1.5336787564766838, "no_speech_prob": 3.216887853341177e-05}, {"id": 446, "seek": 307296, "start": 3092.96, "end": 3099.96, "text": " Your GPU can do 10,000 things at once. So if you just give it one image at a time,", "tokens": [2260, 18407, 393, 360, 1266, 11, 1360, 721, 412, 1564, 13, 407, 498, 291, 445, 976, 309, 472, 3256, 412, 257, 565, 11], "temperature": 0.0, "avg_logprob": -0.2202547119884956, "compression_ratio": 1.5336787564766838, "no_speech_prob": 3.216887853341177e-05}, {"id": 447, "seek": 309996, "start": 3099.96, "end": 3108.96, "text": " you can use it. Yeah, I got you. So if you give it 64 things, it can do one thing on each image and then on each channel in that image.", "tokens": [291, 393, 764, 309, 13, 865, 11, 286, 658, 291, 13, 407, 498, 291, 976, 309, 12145, 721, 11, 309, 393, 360, 472, 551, 322, 1184, 3256, 293, 550, 322, 1184, 2269, 294, 300, 3256, 13], "temperature": 0.0, "avg_logprob": -0.20489493939055115, "compression_ratio": 1.623134328358209, "no_speech_prob": 8.087090100161731e-05}, {"id": 448, "seek": 309996, "start": 3108.96, "end": 3113.96, "text": " And then you don't have enough you other kind of degrees of paralyzation it can do.", "tokens": [400, 550, 291, 500, 380, 362, 1547, 291, 661, 733, 295, 5310, 295, 32645, 89, 399, 309, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.20489493939055115, "compression_ratio": 1.623134328358209, "no_speech_prob": 8.087090100161731e-05}, {"id": 449, "seek": 309996, "start": 3113.96, "end": 3120.96, "text": " And so that's where you start. You know, we saw that Nvidia SMI daemon command that shows you the", "tokens": [400, 370, 300, 311, 689, 291, 722, 13, 509, 458, 11, 321, 1866, 300, 46284, 13115, 40, 1120, 36228, 5622, 300, 3110, 291, 264], "temperature": 0.0, "avg_logprob": -0.20489493939055115, "compression_ratio": 1.623134328358209, "no_speech_prob": 8.087090100161731e-05}, {"id": 450, "seek": 309996, "start": 3120.96, "end": 3128.96, "text": " utilization of your symmetric multiprocessor. If you use a batch size of one, you'll see that SM will be like 1%, 2%,", "tokens": [37074, 295, 428, 32330, 3311, 340, 25432, 13, 759, 291, 764, 257, 15245, 2744, 295, 472, 11, 291, 603, 536, 300, 13115, 486, 312, 411, 502, 8923, 568, 8923], "temperature": 0.0, "avg_logprob": -0.20489493939055115, "compression_ratio": 1.623134328358209, "no_speech_prob": 8.087090100161731e-05}, {"id": 451, "seek": 312896, "start": 3128.96, "end": 3137.96, "text": " and everything will be slow. It's a bit tricky at inference time, you know, in production or whatever, because", "tokens": [293, 1203, 486, 312, 2964, 13, 467, 311, 257, 857, 12414, 412, 38253, 565, 11, 291, 458, 11, 294, 4265, 420, 2035, 11, 570], "temperature": 0.0, "avg_logprob": -0.12790641519758436, "compression_ratio": 1.5191256830601092, "no_speech_prob": 7.181960882007843e-06}, {"id": 452, "seek": 312896, "start": 3137.96, "end": 3145.96, "text": " you know, most of the time you only get one thing to do at a time. And so often inference is done on CPU rather than GPU", "tokens": [291, 458, 11, 881, 295, 264, 565, 291, 787, 483, 472, 551, 281, 360, 412, 257, 565, 13, 400, 370, 2049, 38253, 307, 1096, 322, 13199, 2831, 813, 18407], "temperature": 0.0, "avg_logprob": -0.12790641519758436, "compression_ratio": 1.5191256830601092, "no_speech_prob": 7.181960882007843e-06}, {"id": 453, "seek": 312896, "start": 3145.96, "end": 3152.96, "text": " because we don't get to benefit from batching.", "tokens": [570, 321, 500, 380, 483, 281, 5121, 490, 15245, 278, 13], "temperature": 0.0, "avg_logprob": -0.12790641519758436, "compression_ratio": 1.5191256830601092, "no_speech_prob": 7.181960882007843e-06}, {"id": 454, "seek": 315296, "start": 3152.96, "end": 3158.96, "text": " Or, you know, people will queue a few of them up and stick them all in the GPU at once and, you know, stuff like that.", "tokens": [1610, 11, 291, 458, 11, 561, 486, 18639, 257, 1326, 295, 552, 493, 293, 2897, 552, 439, 294, 264, 18407, 412, 1564, 293, 11, 291, 458, 11, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.15301438265068587, "compression_ratio": 1.4746543778801844, "no_speech_prob": 1.777671423042193e-05}, {"id": 455, "seek": 315296, "start": 3158.96, "end": 3163.96, "text": " But yeah, for training, it's pretty easy to take advantage of many batches.", "tokens": [583, 1338, 11, 337, 3097, 11, 309, 311, 1238, 1858, 281, 747, 5002, 295, 867, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.15301438265068587, "compression_ratio": 1.4746543778801844, "no_speech_prob": 1.777671423042193e-05}, {"id": 456, "seek": 315296, "start": 3163.96, "end": 3168.96, "text": " Thank you. No worries.", "tokens": [1044, 291, 13, 883, 16340, 13], "temperature": 0.0, "avg_logprob": -0.15301438265068587, "compression_ratio": 1.4746543778801844, "no_speech_prob": 1.777671423042193e-05}, {"id": 457, "seek": 315296, "start": 3168.96, "end": 3177.96, "text": " Jeremy, you've trained so many models. Will you consider using a majority vote or something like that?", "tokens": [17809, 11, 291, 600, 8895, 370, 867, 5245, 13, 3099, 291, 1949, 1228, 257, 6286, 4740, 420, 746, 411, 300, 30], "temperature": 0.0, "avg_logprob": -0.15301438265068587, "compression_ratio": 1.4746543778801844, "no_speech_prob": 1.777671423042193e-05}, {"id": 458, "seek": 317796, "start": 3177.96, "end": 3186.96, "text": " No, I wouldn't because a majority vote throws away information. It throws away the probabilities.", "tokens": [883, 11, 286, 2759, 380, 570, 257, 6286, 4740, 19251, 1314, 1589, 13, 467, 19251, 1314, 264, 33783, 13], "temperature": 0.0, "avg_logprob": -0.09531188011169434, "compression_ratio": 1.558659217877095, "no_speech_prob": 3.0222159693948925e-05}, {"id": 459, "seek": 317796, "start": 3186.96, "end": 3193.96, "text": " So I pretty much always find I get better results by averaging the probabilities.", "tokens": [407, 286, 1238, 709, 1009, 915, 286, 483, 1101, 3542, 538, 47308, 264, 33783, 13], "temperature": 0.0, "avg_logprob": -0.09531188011169434, "compression_ratio": 1.558659217877095, "no_speech_prob": 3.0222159693948925e-05}, {"id": 460, "seek": 317796, "start": 3193.96, "end": 3201.96, "text": " So each of them, each of the models after I've trained it, I'm exporting to a uniquely named model,", "tokens": [407, 1184, 295, 552, 11, 1184, 295, 264, 5245, 934, 286, 600, 8895, 309, 11, 286, 478, 44686, 281, 257, 31474, 4926, 2316, 11], "temperature": 0.0, "avg_logprob": -0.09531188011169434, "compression_ratio": 1.558659217877095, "no_speech_prob": 3.0222159693948925e-05}, {"id": 461, "seek": 320196, "start": 3201.96, "end": 3207.96, "text": " which is going to be the name of the architecture and then an underscore and then some description, which is just the thing I pass in.", "tokens": [597, 307, 516, 281, 312, 264, 1315, 295, 264, 9482, 293, 550, 364, 37556, 293, 550, 512, 3855, 11, 597, 307, 445, 264, 551, 286, 1320, 294, 13], "temperature": 0.0, "avg_logprob": -0.09292666535628469, "compression_ratio": 1.5408163265306123, "no_speech_prob": 2.0143299479968846e-05}, {"id": 462, "seek": 320196, "start": 3207.96, "end": 3218.96, "text": " And so that way, yeah, when I'm done training, I can just have a little loop which opens each of those models up,", "tokens": [400, 370, 300, 636, 11, 1338, 11, 562, 286, 478, 1096, 3097, 11, 286, 393, 445, 362, 257, 707, 6367, 597, 9870, 1184, 295, 729, 5245, 493, 11], "temperature": 0.0, "avg_logprob": -0.09292666535628469, "compression_ratio": 1.5408163265306123, "no_speech_prob": 2.0143299479968846e-05}, {"id": 463, "seek": 320196, "start": 3218.96, "end": 3224.96, "text": " grabs the TTA predictions, sticks them into our list.", "tokens": [30028, 264, 314, 8241, 21264, 11, 12518, 552, 666, 527, 1329, 13], "temperature": 0.0, "avg_logprob": -0.09292666535628469, "compression_ratio": 1.5408163265306123, "no_speech_prob": 2.0143299479968846e-05}, {"id": 464, "seek": 322496, "start": 3224.96, "end": 3232.96, "text": " And then at the end, I'll average those TTA predictions across the models. And that will be my on some board prediction.", "tokens": [400, 550, 412, 264, 917, 11, 286, 603, 4274, 729, 314, 8241, 21264, 2108, 264, 5245, 13, 400, 300, 486, 312, 452, 322, 512, 3150, 17630, 13], "temperature": 0.0, "avg_logprob": -0.10355225315800419, "compression_ratio": 1.515625, "no_speech_prob": 6.745711743860738e-06}, {"id": 465, "seek": 322496, "start": 3232.96, "end": 3241.96, "text": " So that's my next step. I'm not up to that yet.", "tokens": [407, 300, 311, 452, 958, 1823, 13, 286, 478, 406, 493, 281, 300, 1939, 13], "temperature": 0.0, "avg_logprob": -0.10355225315800419, "compression_ratio": 1.515625, "no_speech_prob": 6.745711743860738e-06}, {"id": 466, "seek": 322496, "start": 3241.96, "end": 3248.96, "text": " All right. Well, I think that's it. So that's really more of a like little update on what I've been doing over my weekend.", "tokens": [1057, 558, 13, 1042, 11, 286, 519, 300, 311, 309, 13, 407, 300, 311, 534, 544, 295, 257, 411, 707, 5623, 322, 437, 286, 600, 668, 884, 670, 452, 6711, 13], "temperature": 0.0, "avg_logprob": -0.10355225315800419, "compression_ratio": 1.515625, "no_speech_prob": 6.745711743860738e-06}, {"id": 467, "seek": 324896, "start": 3248.96, "end": 3256.96, "text": " But hopefully, yeah, gives you some ideas for things to try.", "tokens": [583, 4696, 11, 1338, 11, 2709, 291, 512, 3487, 337, 721, 281, 853, 13], "temperature": 0.0, "avg_logprob": -0.16918804771021792, "compression_ratio": 1.4493670886075949, "no_speech_prob": 1.4062590707908385e-05}, {"id": 468, "seek": 324896, "start": 3256.96, "end": 3265.96, "text": " And hopefully you find the Kaggle notebook useful.", "tokens": [400, 4696, 291, 915, 264, 48751, 22631, 21060, 4420, 13], "temperature": 0.0, "avg_logprob": -0.16918804771021792, "compression_ratio": 1.4493670886075949, "no_speech_prob": 1.4062590707908385e-05}, {"id": 469, "seek": 324896, "start": 3265.96, "end": 3275.96, "text": " So, Jeremy, so how many hours did you spend in all these experimentations because you spend a lot of experience here?", "tokens": [407, 11, 17809, 11, 370, 577, 867, 2496, 630, 291, 3496, 294, 439, 613, 5120, 763, 570, 291, 3496, 257, 688, 295, 1752, 510, 30], "temperature": 0.0, "avg_logprob": -0.16918804771021792, "compression_ratio": 1.4493670886075949, "no_speech_prob": 1.4062590707908385e-05}, {"id": 470, "seek": 327596, "start": 3275.96, "end": 3282.96, "text": " So, you know, it's like a week or two of work to do the fine tuning experiments.", "tokens": [407, 11, 291, 458, 11, 309, 311, 411, 257, 1243, 420, 732, 295, 589, 281, 360, 264, 2489, 15164, 12050, 13], "temperature": 0.0, "avg_logprob": -0.09876959664481026, "compression_ratio": 1.3846153846153846, "no_speech_prob": 3.526824730215594e-05}, {"id": 471, "seek": 327596, "start": 3282.96, "end": 3288.96, "text": " But that was like a few hours here and a few hours there.", "tokens": [583, 300, 390, 411, 257, 1326, 2496, 510, 293, 257, 1326, 2496, 456, 13], "temperature": 0.0, "avg_logprob": -0.09876959664481026, "compression_ratio": 1.3846153846153846, "no_speech_prob": 3.526824730215594e-05}, {"id": 472, "seek": 327596, "start": 3288.96, "end": 3298.96, "text": " The final sweep was probably maybe six hours of three GPUs.", "tokens": [440, 2572, 22169, 390, 1391, 1310, 2309, 2496, 295, 1045, 18407, 82, 13], "temperature": 0.0, "avg_logprob": -0.09876959664481026, "compression_ratio": 1.3846153846153846, "no_speech_prob": 3.526824730215594e-05}, {"id": 473, "seek": 329896, "start": 3298.96, "end": 3311.96, "text": " The patty competition stuff was maybe four hours a day over the last four days since I last saw you guys.", "tokens": [440, 1947, 874, 6211, 1507, 390, 1310, 1451, 2496, 257, 786, 670, 264, 1036, 1451, 1708, 1670, 286, 1036, 1866, 291, 1074, 13], "temperature": 0.0, "avg_logprob": -0.1397707789551978, "compression_ratio": 1.4074074074074074, "no_speech_prob": 3.166580791003071e-05}, {"id": 474, "seek": 329896, "start": 3311.96, "end": 3319.96, "text": " And writing the notebook was maybe another four hours.", "tokens": [400, 3579, 264, 21060, 390, 1310, 1071, 1451, 2496, 13], "temperature": 0.0, "avg_logprob": -0.1397707789551978, "compression_ratio": 1.4074074074074074, "no_speech_prob": 3.166580791003071e-05}, {"id": 475, "seek": 329896, "start": 3319.96, "end": 3320.96, "text": " Thanks. It helps.", "tokens": [2561, 13, 467, 3665, 13], "temperature": 0.0, "avg_logprob": -0.1397707789551978, "compression_ratio": 1.4074074074074074, "no_speech_prob": 3.166580791003071e-05}, {"id": 476, "seek": 329896, "start": 3320.96, "end": 3323.96, "text": " No worries.", "tokens": [883, 16340, 13], "temperature": 0.0, "avg_logprob": -0.1397707789551978, "compression_ratio": 1.4074074074074074, "no_speech_prob": 3.166580791003071e-05}, {"id": 477, "seek": 332396, "start": 3323.96, "end": 3328.96, "text": " All right. Bye, everybody. Nice to see you all.", "tokens": [1057, 558, 13, 4621, 11, 2201, 13, 5490, 281, 536, 291, 439, 13], "temperature": 0.0, "avg_logprob": -0.23359428919278657, "compression_ratio": 1.0263157894736843, "no_speech_prob": 0.00023549077741336077}, {"id": 478, "seek": 332896, "start": 3328.96, "end": 3357.96, "text": " Thanks, Jeremy. Bye, everyone.", "tokens": [50364, 2561, 11, 17809, 13, 4621, 11, 1518, 13, 51814], "temperature": 0.0, "avg_logprob": -0.26581402258439496, "compression_ratio": 0.7894736842105263, "no_speech_prob": 7.451013516401872e-05}], "language": "en"}