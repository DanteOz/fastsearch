{"text": " All right, it's 11.05, so let's get started. Today, we're going to go over the transformer model, which we started last time, but we'll continue reviewing it. Then in the second half of class, we'll talk about disinformation and how it's related to NLP. So I wanted to start though by going back to the Notebook 7 and 7B when we were doing Seq2Seq. So this is when we first started working on the translation task. Translating some French questions into English questions. We got a spoiler at the start of what the accuracy and blue scores for these different approaches would be. The first three are all using Seq2Seq, and then the fourth is a transformer, which does not use an RNN, whereas the first three are using RNNs. Next, I'll ask some review questions. Can someone remind us what the blue score is? We saw a really great blog post from Rachel Tatman explaining it. Any recollection? Is that a hand? Can I throw you the microphone? Sure. Yeah, it's basically used to evaluate how well a generated sentence would be from a reference sentence. So the score of one is the best you could get. Yeah. So it's a way to evaluate how well a translation task is doing. Do you remember any details about how it's calculated? It's okay if no. Does anyone remember any of the components that go into it? Doesn't it look at like n-grams in addition to the exact word? Exactly. Yeah. So it looks at n-grams, and the reason it's doing that is to catch some notion of order because if you're just looking at the words in it, you could have a totally jumbled order that's not even proper grammar. So it considers that typically I think unigrams, bigrams, trigrams, and four grams. Then there's one other piece. Does anyone remember what that is? So this is to address if the correct answer was, he ate the apple, the sentence he ate has all correct, well, unigram and bigram in that case, but it's not a great translation because it's left some stuff out. So there's a brevity score that you multiply by if your answer is shorter. Good. Thank you. Rachel Totman covers in her post that the blue metric was designed for translation task, and so it's something to be aware of the limitations of it, no matter where you're using it. I think we saw for instance, you're translating something where the right answer was, he ate the apple, he ate an apple, and he ate the potato, both had the same blue score even though one of those is clearly a better translation than the other. So just to be aware of those limitations. So the first, well actually I'll start walking through this, seek to seek. So we're just going to briefly review this because some of this is going to be useful. Yeah, Jeremy? It is. I was going to say we should apologize because again, we're two days out of date. Last week, earlier we taught classification using ULM fit, and then two days before somebody created this thing called Excel net, which uses transformer plus tree training to break the status gap in classification. By the way, I just found out yesterday that training that model, to train at once cost $250,000, so maybe it's less accessible. Yeah. That's the Excel net variation for it. Yeah. So then two days ago, Microsoft announced that they've created something called mass, m-a-s-s, which allows you to use pre-training and transfer learning with transformers for seek to seek. So that's a new thing. Say that again, pre-training with transformers. We're going to show you seek to seek today, although they work great. They don't use any transfer learning. Yes. And you would expect to give them a little bit more about deep learning, but it would have been better if we did use transfer learning, but nobody knew how to until two days ago. So if you combine transfer learning and transformer to seek to seek, you get for translation, you get even better results. So the paper is m-a-s-s-m-s from Microsoft Research. Thank you, Jeremy. Yeah. And so we covered transfer learning for classification with ULMFID a few weeks ago. And remember that research is really just like a year old. So this kind of applying transfer learning to NLP is a fairly new area, and applying it to translation specifically is even newer as of this week. So it's exciting that you're seeing a lot of the concepts used in that. So we looked at initially, we had our basic seek to seek RNN, then we added teacher forcing. Does anyone remember what teacher forcing is? Hatch. Isn't that like feeding the correct answer to the decoder instead of what was put from the encoder just so it has a better time learning? Yeah. So it's feeding the correct answer instead of the last prediction. So particularly when you first start, your predictions are probably not that good. And so if you're predicting the previous word incorrectly, you don't have a good shot at getting the next word correctly. So initially, you want to feed in the right answers even when it's wrong, and you gradually reduce that as training continues and the model gets better. Thank you. So first we added teacher forcing and then we added attention, which is focusing on which part is most relevant. We saw this example from Jay Alomar's blog post, which was using a tensor to tensor notebook. And this is a self-attention on a sentence. But if you're interested in it, it's helpful to focus on the animal. And this is for the sentence, the animal didn't cross the street because it was too tired. And this is the model's learned that it's referring to the animal. It's not referring to the street or didn't or cross, but that kind of takes, it takes some knowledge of the sentence to figure out what's relevant and what's being referred to. And this is very relevant for translation because things aren't translated in a word for word. Different languages have different grammatical structure or different order that the noun, verb, subject will be in. So going back to this because we're going to use this function for transformer. We had seek to seek collate. And what we did here is taking in here, our X is, I think, the sentence in French, Y is the English version of the sentence. And we want to pick the max length of each. So the max length of the English questions, max length of the French questions, create, initialize a tensor. And we first start initializing it of zeros. And the length of the samples is the batch size. So this is basically batch size by the maximum sentence length. And when we add the padding token, broadcasting is happening. So that's initializing kind of this whole, it's probably about 64 by 30 since we limited our sentence length at 30, 64 by 30 tensor and initializing that to have the padding token. And this is kind of necessary to get, sentences can be all different lengths and we kind of need to get them into this rectangular format to feed them into our neural network. So we do that with Xs and with Ys. Here, you can choose padding first. And if you do that, it basically fills in from starting at the beginning to spot negative 30. Oops. Oh, sorry. No, it starts negative 30 to the end and fills that in with the sentence. So that's kind of putting your padding tokens at the beginning. However, if you do not padding first, that's going to put the padding at the end. And so that puts the sentence and this has an extra colon, remove those. Doesn't change the impact. It's just easier to read. This goes from say zero to 30 is where it fills in the sentence. Not all the sentences have the same length. So you might have one that's length 26. You fill in zero to 26 with the sentence and then you're putting padding tokens at the end to fill it out. Are there questions about? Oh, and then another option is you can do this backwards. And that's useful if you're going to do a bi-directional model. Any questions about this? So this is really just kind of like a dating data formatting issue to kind of get our data into this use format of the same length, 64 probably by 30 filling in padding either at the beginning or at the end depending on what the user's chosen. All right. And so that's what we're calling collating. Then we create a seek to seek data bunch. Here this uses our collate function that we've created. We use sortish sampler. Does anyone remember what sortish sampler does? This is a fast AI method. So it's like sorting, but not exactly. It's got some randomness to it. And the benefit of that is particularly if we had here just having sentences of length 30 is not too long, but particularly if we had longer sentences, you want to somewhat sort by length because it allows you to be more efficient, thus faster. So if you think if you had a sentence that was just five tokens long and you put that with a sentence that was 40 tokens long, you're going to have to fill out 35 tokens of padding, which is kind of adding this extra computation that it's not really necessary for the short sentence. But you're having to do that to fill in your data. Actually, let me do maybe a picture. I think I might just erase these. And so the idea with this filling in is say you had like he ate an apple. If these were your tokens, she ran. He jumped, making them pretty short. To get this into the kind of the right format, you would have to kind of put your padding token here because we want to fill this out to be like a full three by four array. To kind of get something that's more manageable. And so here the issue is if one of these sentences was much longer, you're kind of wasting. And it's not the space that's so much the issue, but the extra computation of having to make all these things longer. So there's this benefit to sorting by length, sentence length for your batches. However, if you sort perfectly, you'll get this downside that for every epoch as you're training, your batches will be identical. And this can cause your model to not generalize as well because it might be kind of learning these idiosyncrasies of how you have your data sorted if it's always the same. And so the benefit of sortish is that it's giving you some randomness and your batches will be different each time, but they'll still be close enough that you're not kind of losing time on performance from having to fill out really short sentences with a ton of padding. Are there questions about that? So that's what's going on with this sortish sampler. And then basically we want to get our data loaders together. So having our training data loader, validation data loaders added, kind of putting those together and having this collating method that will fill in our padding. Yeah, and then down here we had defined our loss in accuracy. These n-grams and getting n-grams are necessary for calculating the blue score as we talked about earlier. Is that a question? Can I throw you the box? What's the benefit of using the sorting methods rather than like bad sequences and like that? Like when you kind of put them one next to each other, so you only have like two leads, that's why you have like the sequences one continues and then like it kind of keeps driving one, one, start, and the other ends. Oh, you mean you're saying kind of having everything in one single long sequence? And then like keeping driving like one basically. So that's not letting you take advantage of GPUs and doing batches because you, let me go back to the picture. Oh, maybe I need to go this way to go back to the picture. So you're kind of saying, you know, stringing after this sentence, then string together another sentence. There, because you only have like a dimension of one on this side, that's not letting you take advantage of kind of like having a batch of 64? To kind of, it's kind of the parallelization that the GPU lets you do, taking advantage of that is kind of what makes training much faster. Yeah. Oh, Jeremy's going to get it. Oh yeah, any other questions? Okay, and we'll be using kind of all of this again in the transformer is why I'm going over this. So here we saw calculating the blue score, which involves getting the correct n-grams. We talked about this a little bit earlier, the correct n-grams. First you have to figure out what the n-grams that are in the sentence you've predicted as well as the correct answer are, and then counting them. And down here that comes into play with kind of the predictions, the length penalty, that's also called the brevity penalty, and that's the one that's kind of if what you predict is way shorter than the correct answer, or even if everything in it is, you know, a correct subset of the right answer, it's not as good a translation, so you want to penalize that. And so, yeah, this is how those are calculated. Teacher forcing, we can see here that the probability of teacher forcing is going to start out at one and then decrease as you get later and later into the epoch, and that's because your model's getting better, and so you don't want to keep feeding in the correct answer all the time, but want to have it to start using its own output since that's what will be necessary at inference time or in production. And then here we implement attention, and so kind of the key lines for this, so we have an encoder and a decoder, which are both embeddings from before. Since this is seek to seek, we're using a GRU. Does anyone remember what the problem that GRUs addresses? I think I hear people whispering it maybe louder. Yeah, so vanishing gradients is very much related, but it's short-term memory, which can be caused by vanishing gradients, and that's that as your sequence gets longer, it becomes harder for the model to kind of hold state from the beginning, and so GRUs or LSTMs are two different mini architectures that you can use to address that in your model. Down here, so we've got some dropout, linear layer. Then this is kind of the attention part comes into play here, which is basically just learning weights to take a weighted average, and so we can see that in play in the decoder. Here it's adding up the attention on, so you're kind of bringing in these inputs from your encoder and from the hidden state, adding those together, and then multiplying that by V, which are the weights you've learned, and you get the context is what it's called when you take the attention weights times the output from the encoder that's getting packed into the decoder, so this is just a weighted average telling you kind of how important each hidden state from all the previous time steps is. And so here we're using attention in combination with an RNN, but I guess let's go ahead and switch over to the transformer. So the transformer does not use an RNN. It comes from the paper. I have to get it. Let me go back up to the top. It comes from the paper, attention is all you need, but as we talked about last time, it's not just attention. It's also positional encodings, feed forward blocks, which are kind of traditional fully connected neural network components. There's also label smoothing. There are a few different things that go into this beyond just attention. And so let's start walking through this, and we'll go back to, we had some slides that I think were helpful. We'll go back to those in a moment. So here, this is the same seek to seek collate that we saw before, and this is again just kind of adding padding to make these into nice kind of batches, these 2D blocks that we can parallelize well in terms of putting on the GPU and using them to calculate our weights simultaneously. So exact same before, kind of finding the max length of x, the max length of y, so our English sentences and our French sentences. Here, we're doing some broadcasting, initializing these tensors to be 64 by the max length, and initializing everything to the padding ID, and then based on whether we're putting the padding first or second, filling in the sentences at the beginning or from the end. And that actually, probably should have showed that over here as well. Filling in padding from the end, what that would look like is, so say I was doing max length of 6, really that would just be putting, she ran at kind of the right hand side and all the padding before it. So this is padding first down here, and the ones at top are the padding at the end. Create our data bunch, the same as before. Here, this is nice to see some examples of the text, so we've got questions in French, and then the target is the English translation that we're trying to get. This is a picture of the transformer model. This comes from the attention is all you need paper. And actually, I skipped over this. I should just note at the top, I linked to a nice blog post from Chip Hewin, who's a researcher at NVIDIA, and she wrote a write up on top 8 trends from ICLR this year, which was just, I don't know, I think two months ago. And one of the trends she cited was that RNNs are losing luster with researchers. She saw kind of a real decrease in papers on RNNs. However, this is still very much an open question of research is kind of coming out all the time, and it's unclear kind of what's going to end up being best in terms of attention versus RNNs versus CNNs versus, you know, are there ways to combine aspects of those. And I think we mentioned last time, there was a paper that came out earlier this year, pay less attention with lightweight and dynamic convolutions. Kind of in their research, they showed convolutions outperforming attention, so this is still very much an area of active research. But so back down to kind of the original transformer model. So it involves our input embeddings, and this is true of I think probably any seek to seek is starting with input embeddings, ending with output embeddings. But here in the meantime, or kind of in between, you have multi-head attention showing up in three places, and it's, there's encoder to encoder attention, decoder to decoder attention, and then encoder to decoder attention of where to focus. And we have these feed forward blocks, which you'll see in a moment, which is kind of your traditional fully connected neural network. Adding and norming is kind of our non-linearities going between these. And then these blocks are repeated six times. That's that nx over here, n is six. So looking at the code, so there's kind of a, we have to shift the target over one just since we're predicting the next step. For the encoding, sorry, for the embeddings, so RNNs have a concept of time since you're passing things in sequentially at each time step, you get the next word in the sentence. And that's not the case with attention because you're kind of giving it everything at once and there's no notion of order there. And so positional encoding is a way to address this. And basically that's to, actually Jay Alomar had a great picture of this. So let me go to that later on. We'll come back to some of the earlier pieces. But basically we kind of use sine and cosine to come up with this matrix that captures position. Cuz there's kind of like a different value at each position and you've got these patterns across your two directions. And so this is added to the input to encode the position. And so this is, I'll show you the formula in a moment, but this is kind of what it looks like is adding this sine and cosine that is both varying here across the x and across the y. Whereas, I mean this would be like a batch of sequences, or not a batch of sequences, yeah, but like a batch of your sentences. And so what that looks like is this formula of, will you take one over a thousand raised to, and this can be rewritten as, do you know Lin space in NumPy? Where you're just getting these evenly spaced steps. So here you're taking steps of size, I guess, 2 over d, d is the dimension, and you're going from 0 to 1. And actually we, I refactored this, but that must be in a different notebook, but it's steps from, actually I think we can see this if we look at, Let me plug in. I'll do just d equals 30 so you can see, but this idea of kind of going up by evenly spaced steps to get between 0 and 1 is what this is outputting. And then we are applying sine to that and applying cosine to that. And so this is another way to picture it. Here we're plotting, so the result. Actually, I already have it. This is what the result looks like. So we've got zeros on the first row, and then it's kind of varying as you go along this dimension and along this dimension. But this is how we get around the fact that we haven't captured our sense of time by putting everything into the network at once. And so we use this to create a transformer embedding, which is basically taking our normal embedding and putting that into the positional encoder. We're also using dropout. And then here there's some sort of normalization going on where you're multiplying by the square root of the embedding size. And I think that's to give this embedding kind of more proportional weight compared to the positional encoding. So then the next piece of this to consider are the feedforward blocks. And this is something that should be relevant to the X, and this is something that should be relatively familiar, just from traditional fully connected neural networks. So we've got a linear layer and relU, we'll append some dropout. Sequential X is a fast AI way of doing sequential that lets you include concatenating with the identity. And have you learned about res blocks or resnet in your other classes at all? Okay, let me draw just a quick picture of this. Oops. Jeremy, how do I get a new piece of paper for the drawing? You have to press the trash can. Okay, thanks. Great. So the idea with a res block, and this is the idea behind resnet, which now for a while was kind of a top performer for computer vision and images. So we haven't covered it here since this is an NLP class. But it's basically that with kind of as you're going from layer to layer in your neural network, the idea behind it is like, wouldn't it be nice to learn the residuals of kind of what you're getting wrong? And so to do that, you need to have what's called a skip connection. But basically it means you're just bringing in the identity from the previous layer. And so kind of by having this new answer and the previous layer, that's enough information to try to learn the residuals. And these are called skip connections. And so in order to be able to do them, you have to be able to merge or concatenate this input in this one. And in a resnet, you have a lot of these skip connections, but we'll just be using a little bit. But that's the idea behind sequential X and using merge. So let me show kind of where that plays out. That's a merge layer is to be able to kind of concatenate two things. And these are also, oops. This is also a way to kind of let more information make it through the network. The fact that you're concatenating with the identity, so you're not necessarily kind of losing what was previously present when you go to a further layer. But you can kind of with the skip connection, okay. Should have made this smaller. I can think there's another layer down here, and that's got a skip connection from B going into layer D. Kind of to get this information, kind of preserve it and to give your network an opportunity to learn the residuals at each layer. Yes, Mikael? I believe so, yes. Wouldn't the network learn to adopt the embeddings to ignore the constant multiplication? Let me just repeat the question is wouldn't the network learn to adopt the embeddings to ignore this constant multiplication? Yeah. I think here the bigger thing is, my guess is that this is about having, kind of this having a different magnitude than the positional encodings. So that it can learn both. I mean, neither of these is learned, right? Like the embeddings, well, I guess we're starting with pre-trained embeddings as what's being input. This is what we were trying to figure out yesterday. Yeah, no, we're not certain about this. Yeah, so now that's a good question. I don't know exactly what's going on in this piece. Yeah. Any other questions? Okay, so what's, yeah, here coming back to our feed forward. So this is mostly just kind of a traditional feed forward with linear value dropout, another linear layer, more dropout. Then this merge layer is just letting you have a skip connection from the kind of two layers previous, not just the previous layer, and then a layer norm. I remember that we saw there are two feed forward blocks, one in the encoder, one in the decoder. Or rather, I should say in each, after each encoder to encoder, multi-head attention, and decoder to decoder, multi-head attention. Again, there'll be six of each of these kind of broader blocks. So looking at multi-head attention, we can start with single-head attention over here, also called scale.product attention. So we have a query, a key, and a value. And the query and key, we're gonna get a score from. So that's coming out with a scalar value from the query and key, and then using that to kind of take our weighted average across the values. And so typically, k and v are the same, like the same set of vectors. You're learning different weights for them. But basically, you can think of q is representing the words from sequence one, k and v are representing the words from sequence two. And you're using q and k together to come up with the weight of how much weight to give to each word in sequence two when you're taking this weighted average. And this is probably a good time to go back to Jay Alomar's pictures of this. I think this was a nice intro. So here, the word from sequence one is thinking. The word from sequence two is machines. Again, if you're using self-attention, then sequence one and sequence two are the same. If you're talking about the encoder to decoder attention, they're different. And so we have queries, keys, and values. So these are representations that correspond to each word in sequence one and sequence two, or rather, sorry, I should say, we learn these weights. And then we're multiplying x1 by wq to get q1. So it's coming from this word one, times this weight matrix gives us q1, x2 times wq gives us q2, x1 times wk gives us k1, x2 times wk gives us k2, and so on. And so this is how we're getting our queries, keys, and values. But you can see that kind of here, these are corresponding to this word thinking, cuz we used its embedding times different weights to get these vectors. And in the second column, q2, k2, v2 represent, or they're all derived from our embedding x2 of machines with these weight matrices. And then I think it's helpful, let me show the kind of the next step before we pause for questions. Key thing is kind of q and k1, we're dotting together to get this score. And that's also, there's a formula that goes into it, but kind of key thing is we're getting a scalar out of these. And then that scalar is telling us how to take a weighted average of the v's. Oops. And so we see that here, so the score is also, you're normalizing by the square root of the dimension, taking the soft max. And then multiply that by the value, or by v, and sum it up. So that's the weighted average, kind of how attention is calculated. There are questions. And these graphics come from J Alomar's post, the Illustrated Transformer, which I really like. All right, let's see, we can kind of look at what that looks like in the code. And the multi-head, so that was what you would do for kind of a single-head attention. And multi-head is just kind of increasing the dimensionality of this. I think it's, I can't remember if it's six or eight heads in the paper, but you're learning multiple sets of these weight matrices, basically. WQ, WK, WV. Alternately, you can think of that as kind of increasing an extra dimension in terms of the tensor, you've multiplied each of these by eight. So how that looks here, so we, for our multi-head attention, so we need to set the number of heads, the dimension of each head in the scale. We'll have our Q weights, our K weights, and our V weights. And it's kind of easier to, or you can potentially kind of put those all together, it's just a linear layer, you're learning these weights. We'll have dropout for our attention and on the result, layer norm. The forward step is just going to apply attention, so we'll look at that. It's taking Q and KV as input, and the reason we have it as KV is because K and V are the same thing. So the weight matrices WK and WV are different, but KV or K and V are the same, and that's because it's talking about the same word in sequence two that you're figuring out how much weight to give to. So we have this, well actually, let's go into apply attention. So we'll need the batch size and the sequence length. Then we're going to call this helper method create a attention matrix on Q. And the reason we're feeding KV in twice is once it'll be used as K and the other time as V with the weights for Q, K, and V. And these are being zipped together, so it's Q and weights for Q are being passed into this together. And this is just creating a layer. We want to view this as batch size times this first dimension from X, which I guess is probably the length of the sequence, times the number of heads by the dimension of heads. And there's some permuting that has to happen just for these to kind of line up correctly when you do the multiplication. But the attention score here is just WQ times WK. So that's going to give us our weights for V that we talked about. So we can take this weighted average of V. Then the attention probability is we're applying our dropout on the soft max of the attention score. Apply those probabilities to WV. So actually I guess here is kind of the attention vector is the true weighted average after we've calculated the weights from the scores. And so this is kind of the, I guess, code version of the picture. And I know it's a lot of pieces. I mean, I think it's kind of reassuring that these are primarily just matrix multiplications with, you know, it's kind of the same building blocks that we always see with neural nets of taking matrix multiplications. Some non-linearities of when we have soft max that I think it looks more complicated. And I think these are like complicated ideas of, you know, thinking about this key query and value. But that it is just this representation of taking weighted averages of knowing what to focus on. There are questions. All right. The next piece to look at is masking. And this is basically just that kind of later on when we're dealing with the decoder, it would be cheating to look at the words that come after the word we're dealing with since we shouldn't know those yet. So this is a way of dealing with the fact that since this is not an RNN, we've taken out this time sequential nature that you kind of automatically get when you're using an RNN. But here we don't want to peek or look ahead. And so basically you will just use this upper triangular matrix of ones to cover up everything in the future. So kind of at the first time step, you can't see anything that's coming ahead. Then, you know, at the second time step, you just know the first two things and then the first three things, the first four things. And so we apply this mask. Then kind of getting to the encoder and decoder block. So up here, again, this was just the definition of multi-head attention. But as the paper, and I took a quote from the paper, as the paper says, the transformer uses multi-head attention in three different ways, the encoder to decoder attention. And then there you can think about that of thinking kind of like what word from the English is now being translated to French because the syntax is different in English and French. This is not always going to be the first word being translated to the second word, or the first word of French being translated to the first word of English. There's a different order. And also a different number of words. French has, you know, gender pronouns that English doesn't. Secondly, the encoder has self-attention layers. We saw that can be useful in the sentence, the animal crossed the street because it was tired, knowing what it relates to. And then the decoder has self-attention layers. And again, that's the same idea of kind of knowing what other words in the sentence are relevant to the current concept. So this was defining kind of a multi-head attention class. We'll be using three of these in our transformer. So there's one in the encoder block. And it takes in the number of heads, the dimension of the model, dimension of the head, feed forward. And that's the feed forward block that comes after the multi-head attention block. And then the forward step for the encoder calls self.multi-headattention. And it's passing in xx since this is self-attention. It's giving the same thing for both arguments. And then as a reminder, kind of this first thing corresponds to the query. And the second will be used for both the key and the value because you need, you know, you calculate the score using the key and the query. And then that gives you a scaled version of the value. The decoder block has two multi-head attention classes. And you can see in the, kind of in the forward step, the first is the self-attention that you're calling on the same thing twice. And then you call multi-head attention on that to the encoder. So this is self-attention on the decoder and then get attention between that and the encoder. And here we want to be able to apply the output mask when we're dealing with the decoder because we don't want to look at the future. So we're now ready to put that all together into the whole model. We have a transformer embedding for the encoder, transformer embedding for the decoder. Kind of we'll have these args that we'll need about the number of heads, the dimension of the model, the dimension of the head. This is dealing with the fact that, so the original paper had six encoder blocks and six decoder blocks. So those are declared using a list comprehension. Then a linear layer embedding weights. We need to know what our padding ID is, our output mask, and then just kind of composing these together. The encoder, which then will be fed, the result of that will be fed into the decoder. And so this is something kind of now that we have the building blocks, the transformer can be defined kind of more concisely, but there were a lot of pieces that went into this. Questions? Jeremy, let me pass you the microphone. We did this more manually than we had to as well. High Torch has a multi-head attention built into it. In real life you would just use that. Because we just wanted to show all the steps. We went more kind of from the foundation. Thank you, Jeremy. That was a good point. In practice, use PyTorch's implementation. This is to kind of help you see what goes into it. Similar to how in the GRU lesson, in practice, please use PyTorch's GRU, but we wanted to show you how you would build it yourself. And it's also something I think is simultaneously encouraging and discouraging in that there are a lot of pieces that seems complicated, but it's also individually you know what each of the pieces are and they're things you recognize. So that's, yeah. I feel like both good and bad when I look at the low level implementation. Yeah, so now we're ready to train this one. We'll use the blue metric again since we're doing a translation task and we want to be able to compare to our performance before. So we declare our model, create a learner, choose a learning rate. And then here we're getting, and I'm not going to run it in real time, although it's not terrible, but it's more than I want to do in class. We're getting much, much higher accuracy than we were before and a much higher blue score than we were before. So this is working well. And we can also see some predictions of what it predicts for the output. And so again, I think the second sentence is the target and the third is what we predicted. While I go about maintaining this high degree of fitness, am I protected under an insurance or pension plan? While I do my physical, physical, physical, do I aware by the pension plan, service plan? So not great, but still better than some of the stuff we were seeing previously with our early seek to seeks. Let me look at another. Where do the U.S., Canada, and unknown stand? What is U.S. United and the unknown fit in? Yeah, so it can be interesting to look at. This one's pretty good. What are some of the long-term policy implications of this global knowledge revolution? What are the long-term policy implications of this global scientific? So I think that's pretty good. And then the one other thing that Transformer uses is label smoothing. And this is basically just the idea of since we're predicting predictions, often with softmax you're putting like 100% on the correct answer and 0 on everything else. And the idea of label smoothing is to put, say, 90% on the correct answer and then evenly divide up the other 10% among all the other words. And that can help improve the accuracy of your model. Because really it's kind of you want to get everything, you know, to a prediction in the 90s is better than kind of trying to, you know, overconfident, get to one on everything. There are questions about Transformer? Let me just check the kind of status of the slides. Here, this is just from the paper. They're explaining the motivation for positional encoding. Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. So describing the motivation and the formula for it. And we saw the code version of this earlier. But here, this is position and the dimension of the model. Yeah, and I think that's it. Yeah, so no, yes, question over there. What's the rationale for using triangular function to equal this position? Sorry, you said what's the rationale for using the triangle? Functions to equal positions. By triangle function, do you mean sine and cosine? Yeah. So I think it's just to get something that you can be unique and capture. And I think they even say in the paper that there are many different ways they could have done the positional encoding. But to have this unique way of capturing order. Of like, this is generating this unique sequence that's kind of varying through 2D that'll be the same across batches. So like adding this to kind of all your French sentences, you're adding the same thing and it's unique with the position and ordering. But yeah, I think that there are, I think they say this in the paper, that there are like many other ways they could have done positional encoding as well. Other questions? All right, let's take our five minute break, so let's meet back here at 12.05.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.32, "text": " All right, it's 11.05, so let's get started.", "tokens": [1057, 558, 11, 309, 311, 2975, 13, 13328, 11, 370, 718, 311, 483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.21473536682128908, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.01853821985423565}, {"id": 1, "seek": 0, "start": 4.32, "end": 7.8, "text": " Today, we're going to go over the transformer model,", "tokens": [2692, 11, 321, 434, 516, 281, 352, 670, 264, 31782, 2316, 11], "temperature": 0.0, "avg_logprob": -0.21473536682128908, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.01853821985423565}, {"id": 2, "seek": 0, "start": 7.8, "end": 9.32, "text": " which we started last time,", "tokens": [597, 321, 1409, 1036, 565, 11], "temperature": 0.0, "avg_logprob": -0.21473536682128908, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.01853821985423565}, {"id": 3, "seek": 0, "start": 9.32, "end": 12.0, "text": " but we'll continue reviewing it.", "tokens": [457, 321, 603, 2354, 19576, 309, 13], "temperature": 0.0, "avg_logprob": -0.21473536682128908, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.01853821985423565}, {"id": 4, "seek": 0, "start": 12.0, "end": 13.540000000000001, "text": " Then in the second half of class,", "tokens": [1396, 294, 264, 1150, 1922, 295, 1508, 11], "temperature": 0.0, "avg_logprob": -0.21473536682128908, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.01853821985423565}, {"id": 5, "seek": 0, "start": 13.540000000000001, "end": 17.400000000000002, "text": " we'll talk about disinformation and how it's related to NLP.", "tokens": [321, 603, 751, 466, 717, 20941, 293, 577, 309, 311, 4077, 281, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.21473536682128908, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.01853821985423565}, {"id": 6, "seek": 0, "start": 17.400000000000002, "end": 19.78, "text": " So I wanted to start though by going back to", "tokens": [407, 286, 1415, 281, 722, 1673, 538, 516, 646, 281], "temperature": 0.0, "avg_logprob": -0.21473536682128908, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.01853821985423565}, {"id": 7, "seek": 0, "start": 19.78, "end": 25.02, "text": " the Notebook 7 and 7B when we were doing Seq2Seq.", "tokens": [264, 11633, 2939, 1614, 293, 1614, 33, 562, 321, 645, 884, 1100, 80, 17, 10637, 80, 13], "temperature": 0.0, "avg_logprob": -0.21473536682128908, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.01853821985423565}, {"id": 8, "seek": 0, "start": 25.02, "end": 27.060000000000002, "text": " So this is when we first started", "tokens": [407, 341, 307, 562, 321, 700, 1409], "temperature": 0.0, "avg_logprob": -0.21473536682128908, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.01853821985423565}, {"id": 9, "seek": 0, "start": 27.060000000000002, "end": 29.5, "text": " working on the translation task.", "tokens": [1364, 322, 264, 12853, 5633, 13], "temperature": 0.0, "avg_logprob": -0.21473536682128908, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.01853821985423565}, {"id": 10, "seek": 2950, "start": 29.5, "end": 34.08, "text": " Translating some French questions into English questions.", "tokens": [6531, 75, 990, 512, 5522, 1651, 666, 3669, 1651, 13], "temperature": 0.0, "avg_logprob": -0.1578276572971169, "compression_ratio": 1.6375, "no_speech_prob": 3.269319495302625e-05}, {"id": 11, "seek": 2950, "start": 34.08, "end": 37.480000000000004, "text": " We got a spoiler at the start of what", "tokens": [492, 658, 257, 26927, 412, 264, 722, 295, 437], "temperature": 0.0, "avg_logprob": -0.1578276572971169, "compression_ratio": 1.6375, "no_speech_prob": 3.269319495302625e-05}, {"id": 12, "seek": 2950, "start": 37.480000000000004, "end": 39.34, "text": " the accuracy and blue scores", "tokens": [264, 14170, 293, 3344, 13444], "temperature": 0.0, "avg_logprob": -0.1578276572971169, "compression_ratio": 1.6375, "no_speech_prob": 3.269319495302625e-05}, {"id": 13, "seek": 2950, "start": 39.34, "end": 41.64, "text": " for these different approaches would be.", "tokens": [337, 613, 819, 11587, 576, 312, 13], "temperature": 0.0, "avg_logprob": -0.1578276572971169, "compression_ratio": 1.6375, "no_speech_prob": 3.269319495302625e-05}, {"id": 14, "seek": 2950, "start": 41.64, "end": 44.32, "text": " The first three are all using Seq2Seq,", "tokens": [440, 700, 1045, 366, 439, 1228, 1100, 80, 17, 10637, 80, 11], "temperature": 0.0, "avg_logprob": -0.1578276572971169, "compression_ratio": 1.6375, "no_speech_prob": 3.269319495302625e-05}, {"id": 15, "seek": 2950, "start": 44.32, "end": 46.22, "text": " and then the fourth is a transformer,", "tokens": [293, 550, 264, 6409, 307, 257, 31782, 11], "temperature": 0.0, "avg_logprob": -0.1578276572971169, "compression_ratio": 1.6375, "no_speech_prob": 3.269319495302625e-05}, {"id": 16, "seek": 2950, "start": 46.22, "end": 48.28, "text": " which does not use an RNN,", "tokens": [597, 775, 406, 764, 364, 45702, 45, 11], "temperature": 0.0, "avg_logprob": -0.1578276572971169, "compression_ratio": 1.6375, "no_speech_prob": 3.269319495302625e-05}, {"id": 17, "seek": 2950, "start": 48.28, "end": 51.760000000000005, "text": " whereas the first three are using RNNs.", "tokens": [9735, 264, 700, 1045, 366, 1228, 45702, 45, 82, 13], "temperature": 0.0, "avg_logprob": -0.1578276572971169, "compression_ratio": 1.6375, "no_speech_prob": 3.269319495302625e-05}, {"id": 18, "seek": 2950, "start": 51.760000000000005, "end": 54.760000000000005, "text": " Next, I'll ask some review questions.", "tokens": [3087, 11, 286, 603, 1029, 512, 3131, 1651, 13], "temperature": 0.0, "avg_logprob": -0.1578276572971169, "compression_ratio": 1.6375, "no_speech_prob": 3.269319495302625e-05}, {"id": 19, "seek": 5476, "start": 54.76, "end": 59.28, "text": " Can someone remind us what the blue score is?", "tokens": [1664, 1580, 4160, 505, 437, 264, 3344, 6175, 307, 30], "temperature": 0.0, "avg_logprob": -0.32638427734375, "compression_ratio": 1.2826086956521738, "no_speech_prob": 2.468085403961595e-05}, {"id": 20, "seek": 5476, "start": 63.28, "end": 68.8, "text": " We saw a really great blog post from Rachel Tatman explaining it.", "tokens": [492, 1866, 257, 534, 869, 6968, 2183, 490, 14246, 19645, 1601, 13468, 309, 13], "temperature": 0.0, "avg_logprob": -0.32638427734375, "compression_ratio": 1.2826086956521738, "no_speech_prob": 2.468085403961595e-05}, {"id": 21, "seek": 5476, "start": 73.92, "end": 78.47999999999999, "text": " Any recollection? Is that a hand?", "tokens": [2639, 39495, 10183, 30, 1119, 300, 257, 1011, 30], "temperature": 0.0, "avg_logprob": -0.32638427734375, "compression_ratio": 1.2826086956521738, "no_speech_prob": 2.468085403961595e-05}, {"id": 22, "seek": 5476, "start": 78.47999999999999, "end": 80.32, "text": " Can I throw you the microphone?", "tokens": [1664, 286, 3507, 291, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.32638427734375, "compression_ratio": 1.2826086956521738, "no_speech_prob": 2.468085403961595e-05}, {"id": 23, "seek": 8032, "start": 80.32, "end": 88.88, "text": " Sure. Yeah, it's basically used to evaluate how", "tokens": [4894, 13, 865, 11, 309, 311, 1936, 1143, 281, 13059, 577], "temperature": 0.0, "avg_logprob": -0.20827466986152562, "compression_ratio": 1.6717948717948719, "no_speech_prob": 2.6271567548974417e-05}, {"id": 24, "seek": 8032, "start": 88.88, "end": 92.83999999999999, "text": " well a generated sentence would be from a reference sentence.", "tokens": [731, 257, 10833, 8174, 576, 312, 490, 257, 6408, 8174, 13], "temperature": 0.0, "avg_logprob": -0.20827466986152562, "compression_ratio": 1.6717948717948719, "no_speech_prob": 2.6271567548974417e-05}, {"id": 25, "seek": 8032, "start": 92.83999999999999, "end": 96.72, "text": " So the score of one is the best you could get.", "tokens": [407, 264, 6175, 295, 472, 307, 264, 1151, 291, 727, 483, 13], "temperature": 0.0, "avg_logprob": -0.20827466986152562, "compression_ratio": 1.6717948717948719, "no_speech_prob": 2.6271567548974417e-05}, {"id": 26, "seek": 8032, "start": 96.72, "end": 99.39999999999999, "text": " Yeah. So it's a way to", "tokens": [865, 13, 407, 309, 311, 257, 636, 281], "temperature": 0.0, "avg_logprob": -0.20827466986152562, "compression_ratio": 1.6717948717948719, "no_speech_prob": 2.6271567548974417e-05}, {"id": 27, "seek": 8032, "start": 99.39999999999999, "end": 102.67999999999999, "text": " evaluate how well a translation task is doing.", "tokens": [13059, 577, 731, 257, 12853, 5633, 307, 884, 13], "temperature": 0.0, "avg_logprob": -0.20827466986152562, "compression_ratio": 1.6717948717948719, "no_speech_prob": 2.6271567548974417e-05}, {"id": 28, "seek": 8032, "start": 102.67999999999999, "end": 105.75999999999999, "text": " Do you remember any details about how it's calculated?", "tokens": [1144, 291, 1604, 604, 4365, 466, 577, 309, 311, 15598, 30], "temperature": 0.0, "avg_logprob": -0.20827466986152562, "compression_ratio": 1.6717948717948719, "no_speech_prob": 2.6271567548974417e-05}, {"id": 29, "seek": 8032, "start": 105.75999999999999, "end": 109.47999999999999, "text": " It's okay if no. Does anyone remember any of", "tokens": [467, 311, 1392, 498, 572, 13, 4402, 2878, 1604, 604, 295], "temperature": 0.0, "avg_logprob": -0.20827466986152562, "compression_ratio": 1.6717948717948719, "no_speech_prob": 2.6271567548974417e-05}, {"id": 30, "seek": 10948, "start": 109.48, "end": 111.84, "text": " the components that go into it?", "tokens": [264, 6677, 300, 352, 666, 309, 30], "temperature": 0.0, "avg_logprob": -0.2039571872427444, "compression_ratio": 1.6584362139917694, "no_speech_prob": 1.4285268662206363e-05}, {"id": 31, "seek": 10948, "start": 114.44, "end": 119.08, "text": " Doesn't it look at like n-grams in addition to the exact word?", "tokens": [12955, 380, 309, 574, 412, 411, 297, 12, 1342, 82, 294, 4500, 281, 264, 1900, 1349, 30], "temperature": 0.0, "avg_logprob": -0.2039571872427444, "compression_ratio": 1.6584362139917694, "no_speech_prob": 1.4285268662206363e-05}, {"id": 32, "seek": 10948, "start": 119.08, "end": 121.64, "text": " Exactly. Yeah. So it looks at n-grams,", "tokens": [7587, 13, 865, 13, 407, 309, 1542, 412, 297, 12, 1342, 82, 11], "temperature": 0.0, "avg_logprob": -0.2039571872427444, "compression_ratio": 1.6584362139917694, "no_speech_prob": 1.4285268662206363e-05}, {"id": 33, "seek": 10948, "start": 121.64, "end": 123.76, "text": " and the reason it's doing that is to catch some notion of", "tokens": [293, 264, 1778, 309, 311, 884, 300, 307, 281, 3745, 512, 10710, 295], "temperature": 0.0, "avg_logprob": -0.2039571872427444, "compression_ratio": 1.6584362139917694, "no_speech_prob": 1.4285268662206363e-05}, {"id": 34, "seek": 10948, "start": 123.76, "end": 127.60000000000001, "text": " order because if you're just looking at the words in it,", "tokens": [1668, 570, 498, 291, 434, 445, 1237, 412, 264, 2283, 294, 309, 11], "temperature": 0.0, "avg_logprob": -0.2039571872427444, "compression_ratio": 1.6584362139917694, "no_speech_prob": 1.4285268662206363e-05}, {"id": 35, "seek": 10948, "start": 127.60000000000001, "end": 129.34, "text": " you could have a totally jumbled order", "tokens": [291, 727, 362, 257, 3879, 361, 19928, 1668], "temperature": 0.0, "avg_logprob": -0.2039571872427444, "compression_ratio": 1.6584362139917694, "no_speech_prob": 1.4285268662206363e-05}, {"id": 36, "seek": 10948, "start": 129.34, "end": 131.16, "text": " that's not even proper grammar.", "tokens": [300, 311, 406, 754, 2296, 22317, 13], "temperature": 0.0, "avg_logprob": -0.2039571872427444, "compression_ratio": 1.6584362139917694, "no_speech_prob": 1.4285268662206363e-05}, {"id": 37, "seek": 10948, "start": 131.16, "end": 134.12, "text": " So it considers that typically I think", "tokens": [407, 309, 33095, 300, 5850, 286, 519], "temperature": 0.0, "avg_logprob": -0.2039571872427444, "compression_ratio": 1.6584362139917694, "no_speech_prob": 1.4285268662206363e-05}, {"id": 38, "seek": 10948, "start": 134.12, "end": 137.96, "text": " unigrams, bigrams, trigrams, and four grams.", "tokens": [517, 33737, 82, 11, 955, 2356, 82, 11, 504, 33737, 82, 11, 293, 1451, 11899, 13], "temperature": 0.0, "avg_logprob": -0.2039571872427444, "compression_ratio": 1.6584362139917694, "no_speech_prob": 1.4285268662206363e-05}, {"id": 39, "seek": 13796, "start": 137.96, "end": 140.6, "text": " Then there's one other piece.", "tokens": [1396, 456, 311, 472, 661, 2522, 13], "temperature": 0.0, "avg_logprob": -0.16164721120702158, "compression_ratio": 1.6157407407407407, "no_speech_prob": 1.3419788956525736e-05}, {"id": 40, "seek": 13796, "start": 140.6, "end": 143.04000000000002, "text": " Does anyone remember what that is?", "tokens": [4402, 2878, 1604, 437, 300, 307, 30], "temperature": 0.0, "avg_logprob": -0.16164721120702158, "compression_ratio": 1.6157407407407407, "no_speech_prob": 1.3419788956525736e-05}, {"id": 41, "seek": 13796, "start": 145.20000000000002, "end": 149.88, "text": " So this is to address if the correct answer was,", "tokens": [407, 341, 307, 281, 2985, 498, 264, 3006, 1867, 390, 11], "temperature": 0.0, "avg_logprob": -0.16164721120702158, "compression_ratio": 1.6157407407407407, "no_speech_prob": 1.3419788956525736e-05}, {"id": 42, "seek": 13796, "start": 149.88, "end": 154.52, "text": " he ate the apple, the sentence he ate has all correct,", "tokens": [415, 8468, 264, 10606, 11, 264, 8174, 415, 8468, 575, 439, 3006, 11], "temperature": 0.0, "avg_logprob": -0.16164721120702158, "compression_ratio": 1.6157407407407407, "no_speech_prob": 1.3419788956525736e-05}, {"id": 43, "seek": 13796, "start": 154.52, "end": 157.0, "text": " well, unigram and bigram in that case,", "tokens": [731, 11, 517, 33737, 293, 955, 2356, 294, 300, 1389, 11], "temperature": 0.0, "avg_logprob": -0.16164721120702158, "compression_ratio": 1.6157407407407407, "no_speech_prob": 1.3419788956525736e-05}, {"id": 44, "seek": 13796, "start": 157.0, "end": 159.68, "text": " but it's not a great translation", "tokens": [457, 309, 311, 406, 257, 869, 12853], "temperature": 0.0, "avg_logprob": -0.16164721120702158, "compression_ratio": 1.6157407407407407, "no_speech_prob": 1.3419788956525736e-05}, {"id": 45, "seek": 13796, "start": 159.68, "end": 161.44, "text": " because it's left some stuff out.", "tokens": [570, 309, 311, 1411, 512, 1507, 484, 13], "temperature": 0.0, "avg_logprob": -0.16164721120702158, "compression_ratio": 1.6157407407407407, "no_speech_prob": 1.3419788956525736e-05}, {"id": 46, "seek": 13796, "start": 161.44, "end": 163.04000000000002, "text": " So there's a brevity score that you", "tokens": [407, 456, 311, 257, 1403, 23110, 6175, 300, 291], "temperature": 0.0, "avg_logprob": -0.16164721120702158, "compression_ratio": 1.6157407407407407, "no_speech_prob": 1.3419788956525736e-05}, {"id": 47, "seek": 13796, "start": 163.04000000000002, "end": 165.60000000000002, "text": " multiply by if your answer is shorter.", "tokens": [12972, 538, 498, 428, 1867, 307, 11639, 13], "temperature": 0.0, "avg_logprob": -0.16164721120702158, "compression_ratio": 1.6157407407407407, "no_speech_prob": 1.3419788956525736e-05}, {"id": 48, "seek": 16560, "start": 165.6, "end": 168.12, "text": " Good. Thank you.", "tokens": [2205, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.20750350595634676, "compression_ratio": 1.6711711711711712, "no_speech_prob": 1.0783051948237699e-05}, {"id": 49, "seek": 16560, "start": 170.16, "end": 173.79999999999998, "text": " Rachel Totman covers in her post that", "tokens": [14246, 11236, 1601, 10538, 294, 720, 2183, 300], "temperature": 0.0, "avg_logprob": -0.20750350595634676, "compression_ratio": 1.6711711711711712, "no_speech_prob": 1.0783051948237699e-05}, {"id": 50, "seek": 16560, "start": 173.79999999999998, "end": 178.32, "text": " the blue metric was designed for translation task,", "tokens": [264, 3344, 20678, 390, 4761, 337, 12853, 5633, 11], "temperature": 0.0, "avg_logprob": -0.20750350595634676, "compression_ratio": 1.6711711711711712, "no_speech_prob": 1.0783051948237699e-05}, {"id": 51, "seek": 16560, "start": 178.32, "end": 181.68, "text": " and so it's something to be aware of the limitations of it,", "tokens": [293, 370, 309, 311, 746, 281, 312, 3650, 295, 264, 15705, 295, 309, 11], "temperature": 0.0, "avg_logprob": -0.20750350595634676, "compression_ratio": 1.6711711711711712, "no_speech_prob": 1.0783051948237699e-05}, {"id": 52, "seek": 16560, "start": 181.68, "end": 183.24, "text": " no matter where you're using it.", "tokens": [572, 1871, 689, 291, 434, 1228, 309, 13], "temperature": 0.0, "avg_logprob": -0.20750350595634676, "compression_ratio": 1.6711711711711712, "no_speech_prob": 1.0783051948237699e-05}, {"id": 53, "seek": 16560, "start": 183.24, "end": 185.04, "text": " I think we saw for instance,", "tokens": [286, 519, 321, 1866, 337, 5197, 11], "temperature": 0.0, "avg_logprob": -0.20750350595634676, "compression_ratio": 1.6711711711711712, "no_speech_prob": 1.0783051948237699e-05}, {"id": 54, "seek": 16560, "start": 185.04, "end": 187.24, "text": " you're translating something where the right answer was,", "tokens": [291, 434, 35030, 746, 689, 264, 558, 1867, 390, 11], "temperature": 0.0, "avg_logprob": -0.20750350595634676, "compression_ratio": 1.6711711711711712, "no_speech_prob": 1.0783051948237699e-05}, {"id": 55, "seek": 16560, "start": 187.24, "end": 188.56, "text": " he ate the apple,", "tokens": [415, 8468, 264, 10606, 11], "temperature": 0.0, "avg_logprob": -0.20750350595634676, "compression_ratio": 1.6711711711711712, "no_speech_prob": 1.0783051948237699e-05}, {"id": 56, "seek": 16560, "start": 188.56, "end": 191.44, "text": " he ate an apple,", "tokens": [415, 8468, 364, 10606, 11], "temperature": 0.0, "avg_logprob": -0.20750350595634676, "compression_ratio": 1.6711711711711712, "no_speech_prob": 1.0783051948237699e-05}, {"id": 57, "seek": 16560, "start": 191.44, "end": 193.2, "text": " and he ate the potato,", "tokens": [293, 415, 8468, 264, 7445, 11], "temperature": 0.0, "avg_logprob": -0.20750350595634676, "compression_ratio": 1.6711711711711712, "no_speech_prob": 1.0783051948237699e-05}, {"id": 58, "seek": 16560, "start": 193.2, "end": 194.76, "text": " both had the same blue score", "tokens": [1293, 632, 264, 912, 3344, 6175], "temperature": 0.0, "avg_logprob": -0.20750350595634676, "compression_ratio": 1.6711711711711712, "no_speech_prob": 1.0783051948237699e-05}, {"id": 59, "seek": 19476, "start": 194.76, "end": 196.04, "text": " even though one of those is clearly", "tokens": [754, 1673, 472, 295, 729, 307, 4448], "temperature": 0.0, "avg_logprob": -0.2809161474538404, "compression_ratio": 1.563157894736842, "no_speech_prob": 8.748711843509227e-05}, {"id": 60, "seek": 19476, "start": 196.04, "end": 198.12, "text": " a better translation than the other.", "tokens": [257, 1101, 12853, 813, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.2809161474538404, "compression_ratio": 1.563157894736842, "no_speech_prob": 8.748711843509227e-05}, {"id": 61, "seek": 19476, "start": 198.12, "end": 201.35999999999999, "text": " So just to be aware of those limitations.", "tokens": [407, 445, 281, 312, 3650, 295, 729, 15705, 13], "temperature": 0.0, "avg_logprob": -0.2809161474538404, "compression_ratio": 1.563157894736842, "no_speech_prob": 8.748711843509227e-05}, {"id": 62, "seek": 19476, "start": 203.79999999999998, "end": 206.67999999999998, "text": " So the first, well actually I'll", "tokens": [407, 264, 700, 11, 731, 767, 286, 603], "temperature": 0.0, "avg_logprob": -0.2809161474538404, "compression_ratio": 1.563157894736842, "no_speech_prob": 8.748711843509227e-05}, {"id": 63, "seek": 19476, "start": 206.67999999999998, "end": 211.32, "text": " start walking through this, seek to seek.", "tokens": [722, 4494, 807, 341, 11, 8075, 281, 8075, 13], "temperature": 0.0, "avg_logprob": -0.2809161474538404, "compression_ratio": 1.563157894736842, "no_speech_prob": 8.748711843509227e-05}, {"id": 64, "seek": 19476, "start": 211.32, "end": 212.72, "text": " So we're just going to briefly", "tokens": [407, 321, 434, 445, 516, 281, 10515], "temperature": 0.0, "avg_logprob": -0.2809161474538404, "compression_ratio": 1.563157894736842, "no_speech_prob": 8.748711843509227e-05}, {"id": 65, "seek": 19476, "start": 212.72, "end": 214.64, "text": " review this because some of this is going to be useful.", "tokens": [3131, 341, 570, 512, 295, 341, 307, 516, 281, 312, 4420, 13], "temperature": 0.0, "avg_logprob": -0.2809161474538404, "compression_ratio": 1.563157894736842, "no_speech_prob": 8.748711843509227e-05}, {"id": 66, "seek": 19476, "start": 214.64, "end": 216.28, "text": " Yeah, Jeremy?", "tokens": [865, 11, 17809, 30], "temperature": 0.0, "avg_logprob": -0.2809161474538404, "compression_ratio": 1.563157894736842, "no_speech_prob": 8.748711843509227e-05}, {"id": 67, "seek": 19476, "start": 219.28, "end": 221.2, "text": " It is.", "tokens": [467, 307, 13], "temperature": 0.0, "avg_logprob": -0.2809161474538404, "compression_ratio": 1.563157894736842, "no_speech_prob": 8.748711843509227e-05}, {"id": 68, "seek": 22120, "start": 221.2, "end": 224.88, "text": " I was going to say we should apologize because again,", "tokens": [286, 390, 516, 281, 584, 321, 820, 12328, 570, 797, 11], "temperature": 0.0, "avg_logprob": -0.2737758079271638, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0001140804888564162}, {"id": 69, "seek": 22120, "start": 224.88, "end": 226.92, "text": " we're two days out of date.", "tokens": [321, 434, 732, 1708, 484, 295, 4002, 13], "temperature": 0.0, "avg_logprob": -0.2737758079271638, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0001140804888564162}, {"id": 70, "seek": 22120, "start": 227.07999999999998, "end": 236.07999999999998, "text": " Last week, earlier we taught classification using ULM fit,", "tokens": [5264, 1243, 11, 3071, 321, 5928, 21538, 1228, 624, 43, 44, 3318, 11], "temperature": 0.0, "avg_logprob": -0.2737758079271638, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0001140804888564162}, {"id": 71, "seek": 22120, "start": 236.07999999999998, "end": 239.72, "text": " and then two days before somebody created this thing called Excel net,", "tokens": [293, 550, 732, 1708, 949, 2618, 2942, 341, 551, 1219, 19060, 2533, 11], "temperature": 0.0, "avg_logprob": -0.2737758079271638, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0001140804888564162}, {"id": 72, "seek": 22120, "start": 239.72, "end": 242.32, "text": " which uses transformer plus tree training", "tokens": [597, 4960, 31782, 1804, 4230, 3097], "temperature": 0.0, "avg_logprob": -0.2737758079271638, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0001140804888564162}, {"id": 73, "seek": 22120, "start": 242.32, "end": 246.23999999999998, "text": " to break the status gap in classification.", "tokens": [281, 1821, 264, 6558, 7417, 294, 21538, 13], "temperature": 0.0, "avg_logprob": -0.2737758079271638, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0001140804888564162}, {"id": 74, "seek": 22120, "start": 246.23999999999998, "end": 249.35999999999999, "text": " By the way, I just found out yesterday that training that model,", "tokens": [3146, 264, 636, 11, 286, 445, 1352, 484, 5186, 300, 3097, 300, 2316, 11], "temperature": 0.0, "avg_logprob": -0.2737758079271638, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0001140804888564162}, {"id": 75, "seek": 24936, "start": 249.36, "end": 251.92000000000002, "text": " to train at once cost $250,000,", "tokens": [281, 3847, 412, 1564, 2063, 1848, 23538, 11, 1360, 11], "temperature": 0.0, "avg_logprob": -0.290070997930206, "compression_ratio": 1.5982905982905984, "no_speech_prob": 3.943990304833278e-05}, {"id": 76, "seek": 24936, "start": 251.92000000000002, "end": 254.04000000000002, "text": " so maybe it's less accessible.", "tokens": [370, 1310, 309, 311, 1570, 9515, 13], "temperature": 0.0, "avg_logprob": -0.290070997930206, "compression_ratio": 1.5982905982905984, "no_speech_prob": 3.943990304833278e-05}, {"id": 77, "seek": 24936, "start": 254.04000000000002, "end": 258.08000000000004, "text": " Yeah. That's the Excel net variation for it.", "tokens": [865, 13, 663, 311, 264, 19060, 2533, 12990, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.290070997930206, "compression_ratio": 1.5982905982905984, "no_speech_prob": 3.943990304833278e-05}, {"id": 78, "seek": 24936, "start": 258.08000000000004, "end": 261.76, "text": " Yeah. So then two days ago,", "tokens": [865, 13, 407, 550, 732, 1708, 2057, 11], "temperature": 0.0, "avg_logprob": -0.290070997930206, "compression_ratio": 1.5982905982905984, "no_speech_prob": 3.943990304833278e-05}, {"id": 79, "seek": 24936, "start": 261.76, "end": 264.92, "text": " Microsoft announced that they've created something called mass,", "tokens": [8116, 7548, 300, 436, 600, 2942, 746, 1219, 2758, 11], "temperature": 0.0, "avg_logprob": -0.290070997930206, "compression_ratio": 1.5982905982905984, "no_speech_prob": 3.943990304833278e-05}, {"id": 80, "seek": 24936, "start": 264.92, "end": 268.56, "text": " m-a-s-s, which allows you to use pre-training and transfer", "tokens": [275, 12, 64, 12, 82, 12, 82, 11, 597, 4045, 291, 281, 764, 659, 12, 17227, 1760, 293, 5003], "temperature": 0.0, "avg_logprob": -0.290070997930206, "compression_ratio": 1.5982905982905984, "no_speech_prob": 3.943990304833278e-05}, {"id": 81, "seek": 24936, "start": 268.56, "end": 272.48, "text": " learning with transformers for seek to seek.", "tokens": [2539, 365, 4088, 433, 337, 8075, 281, 8075, 13], "temperature": 0.0, "avg_logprob": -0.290070997930206, "compression_ratio": 1.5982905982905984, "no_speech_prob": 3.943990304833278e-05}, {"id": 82, "seek": 24936, "start": 272.48, "end": 274.2, "text": " So that's a new thing.", "tokens": [407, 300, 311, 257, 777, 551, 13], "temperature": 0.0, "avg_logprob": -0.290070997930206, "compression_ratio": 1.5982905982905984, "no_speech_prob": 3.943990304833278e-05}, {"id": 83, "seek": 24936, "start": 274.2, "end": 276.92, "text": " Say that again, pre-training with transformers.", "tokens": [6463, 300, 797, 11, 659, 12, 17227, 1760, 365, 4088, 433, 13], "temperature": 0.0, "avg_logprob": -0.290070997930206, "compression_ratio": 1.5982905982905984, "no_speech_prob": 3.943990304833278e-05}, {"id": 84, "seek": 27692, "start": 276.92, "end": 280.48, "text": " We're going to show you seek to seek today, although they work great.", "tokens": [492, 434, 516, 281, 855, 291, 8075, 281, 8075, 965, 11, 4878, 436, 589, 869, 13], "temperature": 0.0, "avg_logprob": -0.33839711603128686, "compression_ratio": 1.7617021276595746, "no_speech_prob": 4.2641291656764224e-05}, {"id": 85, "seek": 27692, "start": 280.48, "end": 283.2, "text": " They don't use any transfer learning.", "tokens": [814, 500, 380, 764, 604, 5003, 2539, 13], "temperature": 0.0, "avg_logprob": -0.33839711603128686, "compression_ratio": 1.7617021276595746, "no_speech_prob": 4.2641291656764224e-05}, {"id": 86, "seek": 27692, "start": 283.2, "end": 284.36, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.33839711603128686, "compression_ratio": 1.7617021276595746, "no_speech_prob": 4.2641291656764224e-05}, {"id": 87, "seek": 27692, "start": 284.36, "end": 287.88, "text": " And you would expect to give them a little bit more about deep learning,", "tokens": [400, 291, 576, 2066, 281, 976, 552, 257, 707, 857, 544, 466, 2452, 2539, 11], "temperature": 0.0, "avg_logprob": -0.33839711603128686, "compression_ratio": 1.7617021276595746, "no_speech_prob": 4.2641291656764224e-05}, {"id": 88, "seek": 27692, "start": 287.88, "end": 290.44, "text": " but it would have been better if we did use transfer learning,", "tokens": [457, 309, 576, 362, 668, 1101, 498, 321, 630, 764, 5003, 2539, 11], "temperature": 0.0, "avg_logprob": -0.33839711603128686, "compression_ratio": 1.7617021276595746, "no_speech_prob": 4.2641291656764224e-05}, {"id": 89, "seek": 27692, "start": 290.44, "end": 293.44, "text": " but nobody knew how to until two days ago.", "tokens": [457, 5079, 2586, 577, 281, 1826, 732, 1708, 2057, 13], "temperature": 0.0, "avg_logprob": -0.33839711603128686, "compression_ratio": 1.7617021276595746, "no_speech_prob": 4.2641291656764224e-05}, {"id": 90, "seek": 27692, "start": 293.44, "end": 298.36, "text": " So if you combine transfer learning and transformer to seek to seek,", "tokens": [407, 498, 291, 10432, 5003, 2539, 293, 31782, 281, 8075, 281, 8075, 11], "temperature": 0.0, "avg_logprob": -0.33839711603128686, "compression_ratio": 1.7617021276595746, "no_speech_prob": 4.2641291656764224e-05}, {"id": 91, "seek": 27692, "start": 298.36, "end": 302.08000000000004, "text": " you get for translation, you get even better results.", "tokens": [291, 483, 337, 12853, 11, 291, 483, 754, 1101, 3542, 13], "temperature": 0.0, "avg_logprob": -0.33839711603128686, "compression_ratio": 1.7617021276595746, "no_speech_prob": 4.2641291656764224e-05}, {"id": 92, "seek": 30208, "start": 302.08, "end": 307.59999999999997, "text": " So the paper is m-a-s-s-m-s from Microsoft Research.", "tokens": [407, 264, 3035, 307, 275, 12, 64, 12, 82, 12, 82, 12, 76, 12, 82, 490, 8116, 10303, 13], "temperature": 0.0, "avg_logprob": -0.2485190140573602, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.282438799738884e-05}, {"id": 93, "seek": 30208, "start": 307.59999999999997, "end": 309.35999999999996, "text": " Thank you, Jeremy. Yeah.", "tokens": [1044, 291, 11, 17809, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.2485190140573602, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.282438799738884e-05}, {"id": 94, "seek": 30208, "start": 309.35999999999996, "end": 319.96, "text": " And so we covered transfer learning for classification with ULMFID a few weeks ago.", "tokens": [400, 370, 321, 5343, 5003, 2539, 337, 21538, 365, 624, 43, 44, 37, 2777, 257, 1326, 3259, 2057, 13], "temperature": 0.0, "avg_logprob": -0.2485190140573602, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.282438799738884e-05}, {"id": 95, "seek": 30208, "start": 319.96, "end": 322.79999999999995, "text": " And remember that research is really just like a year old.", "tokens": [400, 1604, 300, 2132, 307, 534, 445, 411, 257, 1064, 1331, 13], "temperature": 0.0, "avg_logprob": -0.2485190140573602, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.282438799738884e-05}, {"id": 96, "seek": 30208, "start": 322.79999999999995, "end": 327.15999999999997, "text": " So this kind of applying transfer learning to NLP is a fairly new area,", "tokens": [407, 341, 733, 295, 9275, 5003, 2539, 281, 426, 45196, 307, 257, 6457, 777, 1859, 11], "temperature": 0.0, "avg_logprob": -0.2485190140573602, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.282438799738884e-05}, {"id": 97, "seek": 30208, "start": 327.15999999999997, "end": 330.59999999999997, "text": " and applying it to translation specifically is even", "tokens": [293, 9275, 309, 281, 12853, 4682, 307, 754], "temperature": 0.0, "avg_logprob": -0.2485190140573602, "compression_ratio": 1.5087719298245614, "no_speech_prob": 2.282438799738884e-05}, {"id": 98, "seek": 33060, "start": 330.6, "end": 333.0, "text": " newer as of this week.", "tokens": [17628, 382, 295, 341, 1243, 13], "temperature": 0.0, "avg_logprob": -0.29799822906949625, "compression_ratio": 1.4654088050314464, "no_speech_prob": 5.389329089666717e-05}, {"id": 99, "seek": 33060, "start": 333.0, "end": 338.20000000000005, "text": " So it's exciting that you're seeing a lot of the concepts used in that.", "tokens": [407, 309, 311, 4670, 300, 291, 434, 2577, 257, 688, 295, 264, 10392, 1143, 294, 300, 13], "temperature": 0.0, "avg_logprob": -0.29799822906949625, "compression_ratio": 1.4654088050314464, "no_speech_prob": 5.389329089666717e-05}, {"id": 100, "seek": 33060, "start": 338.76000000000005, "end": 342.68, "text": " So we looked at initially,", "tokens": [407, 321, 2956, 412, 9105, 11], "temperature": 0.0, "avg_logprob": -0.29799822906949625, "compression_ratio": 1.4654088050314464, "no_speech_prob": 5.389329089666717e-05}, {"id": 101, "seek": 33060, "start": 342.68, "end": 346.0, "text": " we had our basic seek to seek RNN,", "tokens": [321, 632, 527, 3875, 8075, 281, 8075, 45702, 45, 11], "temperature": 0.0, "avg_logprob": -0.29799822906949625, "compression_ratio": 1.4654088050314464, "no_speech_prob": 5.389329089666717e-05}, {"id": 102, "seek": 33060, "start": 346.0, "end": 348.36, "text": " then we added teacher forcing.", "tokens": [550, 321, 3869, 5027, 19030, 13], "temperature": 0.0, "avg_logprob": -0.29799822906949625, "compression_ratio": 1.4654088050314464, "no_speech_prob": 5.389329089666717e-05}, {"id": 103, "seek": 33060, "start": 348.36, "end": 351.6, "text": " Does anyone remember what teacher forcing is?", "tokens": [4402, 2878, 1604, 437, 5027, 19030, 307, 30], "temperature": 0.0, "avg_logprob": -0.29799822906949625, "compression_ratio": 1.4654088050314464, "no_speech_prob": 5.389329089666717e-05}, {"id": 104, "seek": 35160, "start": 351.6, "end": 359.20000000000005, "text": " Hatch.", "tokens": [389, 852, 13], "temperature": 0.0, "avg_logprob": -0.20606548755199877, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.9520612113410607e-05}, {"id": 105, "seek": 35160, "start": 359.20000000000005, "end": 364.88, "text": " Isn't that like feeding the correct answer to the decoder instead of what was", "tokens": [6998, 380, 300, 411, 12919, 264, 3006, 1867, 281, 264, 979, 19866, 2602, 295, 437, 390], "temperature": 0.0, "avg_logprob": -0.20606548755199877, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.9520612113410607e-05}, {"id": 106, "seek": 35160, "start": 364.88, "end": 368.56, "text": " put from the encoder just so it has a better time learning?", "tokens": [829, 490, 264, 2058, 19866, 445, 370, 309, 575, 257, 1101, 565, 2539, 30], "temperature": 0.0, "avg_logprob": -0.20606548755199877, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.9520612113410607e-05}, {"id": 107, "seek": 35160, "start": 368.56, "end": 373.8, "text": " Yeah. So it's feeding the correct answer instead of the last prediction.", "tokens": [865, 13, 407, 309, 311, 12919, 264, 3006, 1867, 2602, 295, 264, 1036, 17630, 13], "temperature": 0.0, "avg_logprob": -0.20606548755199877, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.9520612113410607e-05}, {"id": 108, "seek": 35160, "start": 373.8, "end": 376.16, "text": " So particularly when you first start,", "tokens": [407, 4098, 562, 291, 700, 722, 11], "temperature": 0.0, "avg_logprob": -0.20606548755199877, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.9520612113410607e-05}, {"id": 109, "seek": 35160, "start": 376.16, "end": 378.16, "text": " your predictions are probably not that good.", "tokens": [428, 21264, 366, 1391, 406, 300, 665, 13], "temperature": 0.0, "avg_logprob": -0.20606548755199877, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.9520612113410607e-05}, {"id": 110, "seek": 37816, "start": 378.16, "end": 381.64000000000004, "text": " And so if you're predicting the previous word incorrectly,", "tokens": [400, 370, 498, 291, 434, 32884, 264, 3894, 1349, 42892, 11], "temperature": 0.0, "avg_logprob": -0.12148134925148704, "compression_ratio": 1.6555555555555554, "no_speech_prob": 2.9769589673378505e-05}, {"id": 111, "seek": 37816, "start": 381.64000000000004, "end": 384.04, "text": " you don't have a good shot at getting the next word correctly.", "tokens": [291, 500, 380, 362, 257, 665, 3347, 412, 1242, 264, 958, 1349, 8944, 13], "temperature": 0.0, "avg_logprob": -0.12148134925148704, "compression_ratio": 1.6555555555555554, "no_speech_prob": 2.9769589673378505e-05}, {"id": 112, "seek": 37816, "start": 384.04, "end": 388.04, "text": " So initially, you want to feed in the right answers even when it's wrong,", "tokens": [407, 9105, 11, 291, 528, 281, 3154, 294, 264, 558, 6338, 754, 562, 309, 311, 2085, 11], "temperature": 0.0, "avg_logprob": -0.12148134925148704, "compression_ratio": 1.6555555555555554, "no_speech_prob": 2.9769589673378505e-05}, {"id": 113, "seek": 37816, "start": 388.04, "end": 393.56, "text": " and you gradually reduce that as training continues and the model gets better.", "tokens": [293, 291, 13145, 5407, 300, 382, 3097, 6515, 293, 264, 2316, 2170, 1101, 13], "temperature": 0.0, "avg_logprob": -0.12148134925148704, "compression_ratio": 1.6555555555555554, "no_speech_prob": 2.9769589673378505e-05}, {"id": 114, "seek": 37816, "start": 393.56, "end": 398.8, "text": " Thank you. So first we added teacher forcing and then we added attention,", "tokens": [1044, 291, 13, 407, 700, 321, 3869, 5027, 19030, 293, 550, 321, 3869, 3202, 11], "temperature": 0.0, "avg_logprob": -0.12148134925148704, "compression_ratio": 1.6555555555555554, "no_speech_prob": 2.9769589673378505e-05}, {"id": 115, "seek": 37816, "start": 398.8, "end": 403.04, "text": " which is focusing on which part is most relevant.", "tokens": [597, 307, 8416, 322, 597, 644, 307, 881, 7340, 13], "temperature": 0.0, "avg_logprob": -0.12148134925148704, "compression_ratio": 1.6555555555555554, "no_speech_prob": 2.9769589673378505e-05}, {"id": 116, "seek": 37816, "start": 403.04, "end": 407.44000000000005, "text": " We saw this example from Jay Alomar's blog post,", "tokens": [492, 1866, 341, 1365, 490, 11146, 967, 298, 289, 311, 6968, 2183, 11], "temperature": 0.0, "avg_logprob": -0.12148134925148704, "compression_ratio": 1.6555555555555554, "no_speech_prob": 2.9769589673378505e-05}, {"id": 117, "seek": 40744, "start": 407.44, "end": 410.0, "text": " which was using a tensor to tensor notebook.", "tokens": [597, 390, 1228, 257, 40863, 281, 40863, 21060, 13], "temperature": 0.0, "avg_logprob": -0.1457752566183767, "compression_ratio": 1.8661087866108788, "no_speech_prob": 2.4297878553625196e-05}, {"id": 118, "seek": 40744, "start": 410.0, "end": 413.2, "text": " And this is a self-attention on a sentence.", "tokens": [400, 341, 307, 257, 2698, 12, 1591, 1251, 322, 257, 8174, 13], "temperature": 0.0, "avg_logprob": -0.1457752566183767, "compression_ratio": 1.8661087866108788, "no_speech_prob": 2.4297878553625196e-05}, {"id": 119, "seek": 40744, "start": 413.2, "end": 415.08, "text": " But if you're interested in it,", "tokens": [583, 498, 291, 434, 3102, 294, 309, 11], "temperature": 0.0, "avg_logprob": -0.1457752566183767, "compression_ratio": 1.8661087866108788, "no_speech_prob": 2.4297878553625196e-05}, {"id": 120, "seek": 40744, "start": 415.08, "end": 417.4, "text": " it's helpful to focus on the animal.", "tokens": [309, 311, 4961, 281, 1879, 322, 264, 5496, 13], "temperature": 0.0, "avg_logprob": -0.1457752566183767, "compression_ratio": 1.8661087866108788, "no_speech_prob": 2.4297878553625196e-05}, {"id": 121, "seek": 40744, "start": 417.4, "end": 419.16, "text": " And this is for the sentence, the animal didn't", "tokens": [400, 341, 307, 337, 264, 8174, 11, 264, 5496, 994, 380], "temperature": 0.0, "avg_logprob": -0.1457752566183767, "compression_ratio": 1.8661087866108788, "no_speech_prob": 2.4297878553625196e-05}, {"id": 122, "seek": 40744, "start": 419.16, "end": 421.64, "text": " cross the street because it was too tired.", "tokens": [3278, 264, 4838, 570, 309, 390, 886, 5868, 13], "temperature": 0.0, "avg_logprob": -0.1457752566183767, "compression_ratio": 1.8661087866108788, "no_speech_prob": 2.4297878553625196e-05}, {"id": 123, "seek": 40744, "start": 421.64, "end": 425.48, "text": " And this is the model's learned that it's referring to the animal.", "tokens": [400, 341, 307, 264, 2316, 311, 3264, 300, 309, 311, 13761, 281, 264, 5496, 13], "temperature": 0.0, "avg_logprob": -0.1457752566183767, "compression_ratio": 1.8661087866108788, "no_speech_prob": 2.4297878553625196e-05}, {"id": 124, "seek": 40744, "start": 425.48, "end": 429.6, "text": " It's not referring to the street or didn't or cross,", "tokens": [467, 311, 406, 13761, 281, 264, 4838, 420, 994, 380, 420, 3278, 11], "temperature": 0.0, "avg_logprob": -0.1457752566183767, "compression_ratio": 1.8661087866108788, "no_speech_prob": 2.4297878553625196e-05}, {"id": 125, "seek": 40744, "start": 429.6, "end": 431.36, "text": " but that kind of takes,", "tokens": [457, 300, 733, 295, 2516, 11], "temperature": 0.0, "avg_logprob": -0.1457752566183767, "compression_ratio": 1.8661087866108788, "no_speech_prob": 2.4297878553625196e-05}, {"id": 126, "seek": 40744, "start": 431.36, "end": 433.96, "text": " it takes some knowledge of the sentence to figure out", "tokens": [309, 2516, 512, 3601, 295, 264, 8174, 281, 2573, 484], "temperature": 0.0, "avg_logprob": -0.1457752566183767, "compression_ratio": 1.8661087866108788, "no_speech_prob": 2.4297878553625196e-05}, {"id": 127, "seek": 43396, "start": 433.96, "end": 438.03999999999996, "text": " what's relevant and what's being referred to.", "tokens": [437, 311, 7340, 293, 437, 311, 885, 10839, 281, 13], "temperature": 0.0, "avg_logprob": -0.1991522471110026, "compression_ratio": 1.6712328767123288, "no_speech_prob": 3.02373828162672e-05}, {"id": 128, "seek": 43396, "start": 438.03999999999996, "end": 442.76, "text": " And this is very relevant for translation because things", "tokens": [400, 341, 307, 588, 7340, 337, 12853, 570, 721], "temperature": 0.0, "avg_logprob": -0.1991522471110026, "compression_ratio": 1.6712328767123288, "no_speech_prob": 3.02373828162672e-05}, {"id": 129, "seek": 43396, "start": 442.76, "end": 445.56, "text": " aren't translated in a word for word.", "tokens": [3212, 380, 16805, 294, 257, 1349, 337, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1991522471110026, "compression_ratio": 1.6712328767123288, "no_speech_prob": 3.02373828162672e-05}, {"id": 130, "seek": 43396, "start": 445.56, "end": 448.08, "text": " Different languages have different grammatical structure or", "tokens": [20825, 8650, 362, 819, 17570, 267, 804, 3877, 420], "temperature": 0.0, "avg_logprob": -0.1991522471110026, "compression_ratio": 1.6712328767123288, "no_speech_prob": 3.02373828162672e-05}, {"id": 131, "seek": 43396, "start": 448.08, "end": 453.12, "text": " different order that the noun, verb, subject will be in.", "tokens": [819, 1668, 300, 264, 23307, 11, 9595, 11, 3983, 486, 312, 294, 13], "temperature": 0.0, "avg_logprob": -0.1991522471110026, "compression_ratio": 1.6712328767123288, "no_speech_prob": 3.02373828162672e-05}, {"id": 132, "seek": 43396, "start": 453.12, "end": 459.67999999999995, "text": " So going back to this because we're going to use this function for transformer.", "tokens": [407, 516, 646, 281, 341, 570, 321, 434, 516, 281, 764, 341, 2445, 337, 31782, 13], "temperature": 0.0, "avg_logprob": -0.1991522471110026, "compression_ratio": 1.6712328767123288, "no_speech_prob": 3.02373828162672e-05}, {"id": 133, "seek": 43396, "start": 459.67999999999995, "end": 462.03999999999996, "text": " We had seek to seek collate.", "tokens": [492, 632, 8075, 281, 8075, 1263, 473, 13], "temperature": 0.0, "avg_logprob": -0.1991522471110026, "compression_ratio": 1.6712328767123288, "no_speech_prob": 3.02373828162672e-05}, {"id": 134, "seek": 46204, "start": 462.04, "end": 466.24, "text": " And what we did here is taking in here,", "tokens": [400, 437, 321, 630, 510, 307, 1940, 294, 510, 11], "temperature": 0.0, "avg_logprob": -0.2083033695015856, "compression_ratio": 1.7595628415300546, "no_speech_prob": 4.2628907976904884e-05}, {"id": 135, "seek": 46204, "start": 466.24, "end": 468.72, "text": " our X is, I think, the sentence in French,", "tokens": [527, 1783, 307, 11, 286, 519, 11, 264, 8174, 294, 5522, 11], "temperature": 0.0, "avg_logprob": -0.2083033695015856, "compression_ratio": 1.7595628415300546, "no_speech_prob": 4.2628907976904884e-05}, {"id": 136, "seek": 46204, "start": 468.72, "end": 471.88, "text": " Y is the English version of the sentence.", "tokens": [398, 307, 264, 3669, 3037, 295, 264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.2083033695015856, "compression_ratio": 1.7595628415300546, "no_speech_prob": 4.2628907976904884e-05}, {"id": 137, "seek": 46204, "start": 471.88, "end": 477.6, "text": " And we want to pick the max length of each.", "tokens": [400, 321, 528, 281, 1888, 264, 11469, 4641, 295, 1184, 13], "temperature": 0.0, "avg_logprob": -0.2083033695015856, "compression_ratio": 1.7595628415300546, "no_speech_prob": 4.2628907976904884e-05}, {"id": 138, "seek": 46204, "start": 477.6, "end": 481.6, "text": " So the max length of the English questions,", "tokens": [407, 264, 11469, 4641, 295, 264, 3669, 1651, 11], "temperature": 0.0, "avg_logprob": -0.2083033695015856, "compression_ratio": 1.7595628415300546, "no_speech_prob": 4.2628907976904884e-05}, {"id": 139, "seek": 46204, "start": 481.6, "end": 485.08000000000004, "text": " max length of the French questions,", "tokens": [11469, 4641, 295, 264, 5522, 1651, 11], "temperature": 0.0, "avg_logprob": -0.2083033695015856, "compression_ratio": 1.7595628415300546, "no_speech_prob": 4.2628907976904884e-05}, {"id": 140, "seek": 46204, "start": 485.68, "end": 489.32000000000005, "text": " create, initialize a tensor.", "tokens": [1884, 11, 5883, 1125, 257, 40863, 13], "temperature": 0.0, "avg_logprob": -0.2083033695015856, "compression_ratio": 1.7595628415300546, "no_speech_prob": 4.2628907976904884e-05}, {"id": 141, "seek": 46204, "start": 489.32000000000005, "end": 491.68, "text": " And we first start initializing it of zeros.", "tokens": [400, 321, 700, 722, 5883, 3319, 309, 295, 35193, 13], "temperature": 0.0, "avg_logprob": -0.2083033695015856, "compression_ratio": 1.7595628415300546, "no_speech_prob": 4.2628907976904884e-05}, {"id": 142, "seek": 49168, "start": 491.68, "end": 493.84000000000003, "text": " And the length of the samples is the batch size.", "tokens": [400, 264, 4641, 295, 264, 10938, 307, 264, 15245, 2744, 13], "temperature": 0.0, "avg_logprob": -0.13970789018568103, "compression_ratio": 1.7233009708737863, "no_speech_prob": 8.3482576883398e-05}, {"id": 143, "seek": 49168, "start": 493.84000000000003, "end": 499.48, "text": " So this is basically batch size by the maximum sentence length.", "tokens": [407, 341, 307, 1936, 15245, 2744, 538, 264, 6674, 8174, 4641, 13], "temperature": 0.0, "avg_logprob": -0.13970789018568103, "compression_ratio": 1.7233009708737863, "no_speech_prob": 8.3482576883398e-05}, {"id": 144, "seek": 49168, "start": 499.48, "end": 503.64, "text": " And when we add the padding token,", "tokens": [400, 562, 321, 909, 264, 39562, 14862, 11], "temperature": 0.0, "avg_logprob": -0.13970789018568103, "compression_ratio": 1.7233009708737863, "no_speech_prob": 8.3482576883398e-05}, {"id": 145, "seek": 49168, "start": 503.64, "end": 505.16, "text": " broadcasting is happening.", "tokens": [30024, 307, 2737, 13], "temperature": 0.0, "avg_logprob": -0.13970789018568103, "compression_ratio": 1.7233009708737863, "no_speech_prob": 8.3482576883398e-05}, {"id": 146, "seek": 49168, "start": 505.16, "end": 507.64, "text": " So that's initializing kind of this whole,", "tokens": [407, 300, 311, 5883, 3319, 733, 295, 341, 1379, 11], "temperature": 0.0, "avg_logprob": -0.13970789018568103, "compression_ratio": 1.7233009708737863, "no_speech_prob": 8.3482576883398e-05}, {"id": 147, "seek": 49168, "start": 507.64, "end": 513.96, "text": " it's probably about 64 by 30 since we limited our sentence length at 30,", "tokens": [309, 311, 1391, 466, 12145, 538, 2217, 1670, 321, 5567, 527, 8174, 4641, 412, 2217, 11], "temperature": 0.0, "avg_logprob": -0.13970789018568103, "compression_ratio": 1.7233009708737863, "no_speech_prob": 8.3482576883398e-05}, {"id": 148, "seek": 49168, "start": 513.96, "end": 520.36, "text": " 64 by 30 tensor and initializing that to have the padding token.", "tokens": [12145, 538, 2217, 40863, 293, 5883, 3319, 300, 281, 362, 264, 39562, 14862, 13], "temperature": 0.0, "avg_logprob": -0.13970789018568103, "compression_ratio": 1.7233009708737863, "no_speech_prob": 8.3482576883398e-05}, {"id": 149, "seek": 52036, "start": 520.36, "end": 523.44, "text": " And this is kind of necessary to get,", "tokens": [400, 341, 307, 733, 295, 4818, 281, 483, 11], "temperature": 0.0, "avg_logprob": -0.1855114528111049, "compression_ratio": 1.5740740740740742, "no_speech_prob": 4.682798316935077e-05}, {"id": 150, "seek": 52036, "start": 523.44, "end": 526.64, "text": " sentences can be all different lengths and we kind of need to get them into", "tokens": [16579, 393, 312, 439, 819, 26329, 293, 321, 733, 295, 643, 281, 483, 552, 666], "temperature": 0.0, "avg_logprob": -0.1855114528111049, "compression_ratio": 1.5740740740740742, "no_speech_prob": 4.682798316935077e-05}, {"id": 151, "seek": 52036, "start": 526.64, "end": 531.48, "text": " this rectangular format to feed them into our neural network.", "tokens": [341, 31167, 7877, 281, 3154, 552, 666, 527, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.1855114528111049, "compression_ratio": 1.5740740740740742, "no_speech_prob": 4.682798316935077e-05}, {"id": 152, "seek": 52036, "start": 531.48, "end": 535.5600000000001, "text": " So we do that with Xs and with Ys.", "tokens": [407, 321, 360, 300, 365, 1783, 82, 293, 365, 398, 82, 13], "temperature": 0.0, "avg_logprob": -0.1855114528111049, "compression_ratio": 1.5740740740740742, "no_speech_prob": 4.682798316935077e-05}, {"id": 153, "seek": 52036, "start": 535.5600000000001, "end": 540.0, "text": " Here, you can choose padding first.", "tokens": [1692, 11, 291, 393, 2826, 39562, 700, 13], "temperature": 0.0, "avg_logprob": -0.1855114528111049, "compression_ratio": 1.5740740740740742, "no_speech_prob": 4.682798316935077e-05}, {"id": 154, "seek": 52036, "start": 540.0, "end": 541.92, "text": " And if you do that,", "tokens": [400, 498, 291, 360, 300, 11], "temperature": 0.0, "avg_logprob": -0.1855114528111049, "compression_ratio": 1.5740740740740742, "no_speech_prob": 4.682798316935077e-05}, {"id": 155, "seek": 52036, "start": 541.92, "end": 549.64, "text": " it basically fills in from starting at the beginning to spot negative 30.", "tokens": [309, 1936, 22498, 294, 490, 2891, 412, 264, 2863, 281, 4008, 3671, 2217, 13], "temperature": 0.0, "avg_logprob": -0.1855114528111049, "compression_ratio": 1.5740740740740742, "no_speech_prob": 4.682798316935077e-05}, {"id": 156, "seek": 54964, "start": 549.64, "end": 553.0, "text": " Oops. Oh, sorry.", "tokens": [21726, 13, 876, 11, 2597, 13], "temperature": 0.0, "avg_logprob": -0.14017481016881259, "compression_ratio": 1.5739130434782609, "no_speech_prob": 4.83197218272835e-05}, {"id": 157, "seek": 54964, "start": 553.0, "end": 557.4399999999999, "text": " No, it starts negative 30 to the end and fills that in with the sentence.", "tokens": [883, 11, 309, 3719, 3671, 2217, 281, 264, 917, 293, 22498, 300, 294, 365, 264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.14017481016881259, "compression_ratio": 1.5739130434782609, "no_speech_prob": 4.83197218272835e-05}, {"id": 158, "seek": 54964, "start": 557.4399999999999, "end": 561.3199999999999, "text": " So that's kind of putting your padding tokens at the beginning.", "tokens": [407, 300, 311, 733, 295, 3372, 428, 39562, 22667, 412, 264, 2863, 13], "temperature": 0.0, "avg_logprob": -0.14017481016881259, "compression_ratio": 1.5739130434782609, "no_speech_prob": 4.83197218272835e-05}, {"id": 159, "seek": 54964, "start": 561.3199999999999, "end": 564.76, "text": " However, if you do not padding first,", "tokens": [2908, 11, 498, 291, 360, 406, 39562, 700, 11], "temperature": 0.0, "avg_logprob": -0.14017481016881259, "compression_ratio": 1.5739130434782609, "no_speech_prob": 4.83197218272835e-05}, {"id": 160, "seek": 54964, "start": 564.76, "end": 566.56, "text": " that's going to put the padding at the end.", "tokens": [300, 311, 516, 281, 829, 264, 39562, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.14017481016881259, "compression_ratio": 1.5739130434782609, "no_speech_prob": 4.83197218272835e-05}, {"id": 161, "seek": 54964, "start": 566.56, "end": 574.56, "text": " And so that puts the sentence and this has an extra colon, remove those.", "tokens": [400, 370, 300, 8137, 264, 8174, 293, 341, 575, 364, 2857, 8255, 11, 4159, 729, 13], "temperature": 0.0, "avg_logprob": -0.14017481016881259, "compression_ratio": 1.5739130434782609, "no_speech_prob": 4.83197218272835e-05}, {"id": 162, "seek": 54964, "start": 574.56, "end": 576.36, "text": " Doesn't change the impact.", "tokens": [12955, 380, 1319, 264, 2712, 13], "temperature": 0.0, "avg_logprob": -0.14017481016881259, "compression_ratio": 1.5739130434782609, "no_speech_prob": 4.83197218272835e-05}, {"id": 163, "seek": 54964, "start": 576.36, "end": 577.64, "text": " It's just easier to read.", "tokens": [467, 311, 445, 3571, 281, 1401, 13], "temperature": 0.0, "avg_logprob": -0.14017481016881259, "compression_ratio": 1.5739130434782609, "no_speech_prob": 4.83197218272835e-05}, {"id": 164, "seek": 57764, "start": 577.64, "end": 582.56, "text": " This goes from say zero to 30 is where it fills in the sentence.", "tokens": [639, 1709, 490, 584, 4018, 281, 2217, 307, 689, 309, 22498, 294, 264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.2013951170033422, "compression_ratio": 1.7131474103585658, "no_speech_prob": 1.414465373272833e-06}, {"id": 165, "seek": 57764, "start": 582.56, "end": 584.92, "text": " Not all the sentences have the same length.", "tokens": [1726, 439, 264, 16579, 362, 264, 912, 4641, 13], "temperature": 0.0, "avg_logprob": -0.2013951170033422, "compression_ratio": 1.7131474103585658, "no_speech_prob": 1.414465373272833e-06}, {"id": 166, "seek": 57764, "start": 584.92, "end": 587.92, "text": " So you might have one that's length 26.", "tokens": [407, 291, 1062, 362, 472, 300, 311, 4641, 7551, 13], "temperature": 0.0, "avg_logprob": -0.2013951170033422, "compression_ratio": 1.7131474103585658, "no_speech_prob": 1.414465373272833e-06}, {"id": 167, "seek": 57764, "start": 587.92, "end": 591.04, "text": " You fill in zero to 26 with the sentence and then you're", "tokens": [509, 2836, 294, 4018, 281, 7551, 365, 264, 8174, 293, 550, 291, 434], "temperature": 0.0, "avg_logprob": -0.2013951170033422, "compression_ratio": 1.7131474103585658, "no_speech_prob": 1.414465373272833e-06}, {"id": 168, "seek": 57764, "start": 591.04, "end": 594.4, "text": " putting padding tokens at the end to fill it out.", "tokens": [3372, 39562, 22667, 412, 264, 917, 281, 2836, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.2013951170033422, "compression_ratio": 1.7131474103585658, "no_speech_prob": 1.414465373272833e-06}, {"id": 169, "seek": 57764, "start": 594.4, "end": 600.72, "text": " Are there questions about? Oh, and then another option is you can do this backwards.", "tokens": [2014, 456, 1651, 466, 30, 876, 11, 293, 550, 1071, 3614, 307, 291, 393, 360, 341, 12204, 13], "temperature": 0.0, "avg_logprob": -0.2013951170033422, "compression_ratio": 1.7131474103585658, "no_speech_prob": 1.414465373272833e-06}, {"id": 170, "seek": 57764, "start": 600.72, "end": 604.52, "text": " And that's useful if you're going to do a bi-directional model.", "tokens": [400, 300, 311, 4420, 498, 291, 434, 516, 281, 360, 257, 3228, 12, 18267, 41048, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2013951170033422, "compression_ratio": 1.7131474103585658, "no_speech_prob": 1.414465373272833e-06}, {"id": 171, "seek": 57764, "start": 604.52, "end": 606.4399999999999, "text": " Any questions about this?", "tokens": [2639, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.2013951170033422, "compression_ratio": 1.7131474103585658, "no_speech_prob": 1.414465373272833e-06}, {"id": 172, "seek": 60644, "start": 606.44, "end": 611.2, "text": " So this is really just kind of like a dating data formatting issue to kind of", "tokens": [407, 341, 307, 534, 445, 733, 295, 411, 257, 10689, 1412, 39366, 2734, 281, 733, 295], "temperature": 0.0, "avg_logprob": -0.23870594581861174, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.0129632755706552e-05}, {"id": 173, "seek": 60644, "start": 611.2, "end": 614.8800000000001, "text": " get our data into this use format of the same length,", "tokens": [483, 527, 1412, 666, 341, 764, 7877, 295, 264, 912, 4641, 11], "temperature": 0.0, "avg_logprob": -0.23870594581861174, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.0129632755706552e-05}, {"id": 174, "seek": 60644, "start": 614.8800000000001, "end": 619.4000000000001, "text": " 64 probably by 30 filling in padding either at the beginning or at", "tokens": [12145, 1391, 538, 2217, 10623, 294, 39562, 2139, 412, 264, 2863, 420, 412], "temperature": 0.0, "avg_logprob": -0.23870594581861174, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.0129632755706552e-05}, {"id": 175, "seek": 60644, "start": 619.4000000000001, "end": 623.0400000000001, "text": " the end depending on what the user's chosen.", "tokens": [264, 917, 5413, 322, 437, 264, 4195, 311, 8614, 13], "temperature": 0.0, "avg_logprob": -0.23870594581861174, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.0129632755706552e-05}, {"id": 176, "seek": 60644, "start": 626.0, "end": 630.0400000000001, "text": " All right. And so that's what we're calling collating.", "tokens": [1057, 558, 13, 400, 370, 300, 311, 437, 321, 434, 5141, 1263, 990, 13], "temperature": 0.0, "avg_logprob": -0.23870594581861174, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.0129632755706552e-05}, {"id": 177, "seek": 60644, "start": 630.0400000000001, "end": 634.12, "text": " Then we create a seek to seek data bunch.", "tokens": [1396, 321, 1884, 257, 8075, 281, 8075, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.23870594581861174, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.0129632755706552e-05}, {"id": 178, "seek": 63412, "start": 634.12, "end": 643.12, "text": " Here this uses our collate function that we've created.", "tokens": [1692, 341, 4960, 527, 1263, 473, 2445, 300, 321, 600, 2942, 13], "temperature": 0.0, "avg_logprob": -0.19184641770913569, "compression_ratio": 1.4088050314465408, "no_speech_prob": 1.4738657228008378e-05}, {"id": 179, "seek": 63412, "start": 643.12, "end": 645.48, "text": " We use sortish sampler.", "tokens": [492, 764, 1333, 742, 3247, 22732, 13], "temperature": 0.0, "avg_logprob": -0.19184641770913569, "compression_ratio": 1.4088050314465408, "no_speech_prob": 1.4738657228008378e-05}, {"id": 180, "seek": 63412, "start": 645.48, "end": 649.12, "text": " Does anyone remember what sortish sampler does?", "tokens": [4402, 2878, 1604, 437, 1333, 742, 3247, 22732, 775, 30], "temperature": 0.0, "avg_logprob": -0.19184641770913569, "compression_ratio": 1.4088050314465408, "no_speech_prob": 1.4738657228008378e-05}, {"id": 181, "seek": 63412, "start": 650.8, "end": 654.68, "text": " This is a fast AI method.", "tokens": [639, 307, 257, 2370, 7318, 3170, 13], "temperature": 0.0, "avg_logprob": -0.19184641770913569, "compression_ratio": 1.4088050314465408, "no_speech_prob": 1.4738657228008378e-05}, {"id": 182, "seek": 63412, "start": 655.52, "end": 658.32, "text": " So it's like sorting,", "tokens": [407, 309, 311, 411, 32411, 11], "temperature": 0.0, "avg_logprob": -0.19184641770913569, "compression_ratio": 1.4088050314465408, "no_speech_prob": 1.4738657228008378e-05}, {"id": 183, "seek": 63412, "start": 658.32, "end": 659.6, "text": " but not exactly.", "tokens": [457, 406, 2293, 13], "temperature": 0.0, "avg_logprob": -0.19184641770913569, "compression_ratio": 1.4088050314465408, "no_speech_prob": 1.4738657228008378e-05}, {"id": 184, "seek": 63412, "start": 659.6, "end": 661.72, "text": " It's got some randomness to it.", "tokens": [467, 311, 658, 512, 4974, 1287, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.19184641770913569, "compression_ratio": 1.4088050314465408, "no_speech_prob": 1.4738657228008378e-05}, {"id": 185, "seek": 66172, "start": 661.72, "end": 667.96, "text": " And the benefit of that is particularly if we had here just having sentences of length 30 is not too long,", "tokens": [400, 264, 5121, 295, 300, 307, 4098, 498, 321, 632, 510, 445, 1419, 16579, 295, 4641, 2217, 307, 406, 886, 938, 11], "temperature": 0.0, "avg_logprob": -0.16575112836114292, "compression_ratio": 1.8053435114503817, "no_speech_prob": 5.2248196880100295e-05}, {"id": 186, "seek": 66172, "start": 667.96, "end": 669.96, "text": " but particularly if we had longer sentences,", "tokens": [457, 4098, 498, 321, 632, 2854, 16579, 11], "temperature": 0.0, "avg_logprob": -0.16575112836114292, "compression_ratio": 1.8053435114503817, "no_speech_prob": 5.2248196880100295e-05}, {"id": 187, "seek": 66172, "start": 669.96, "end": 675.0400000000001, "text": " you want to somewhat sort by length because it allows you to be more efficient,", "tokens": [291, 528, 281, 8344, 1333, 538, 4641, 570, 309, 4045, 291, 281, 312, 544, 7148, 11], "temperature": 0.0, "avg_logprob": -0.16575112836114292, "compression_ratio": 1.8053435114503817, "no_speech_prob": 5.2248196880100295e-05}, {"id": 188, "seek": 66172, "start": 675.0400000000001, "end": 676.4, "text": " thus faster.", "tokens": [8807, 4663, 13], "temperature": 0.0, "avg_logprob": -0.16575112836114292, "compression_ratio": 1.8053435114503817, "no_speech_prob": 5.2248196880100295e-05}, {"id": 189, "seek": 66172, "start": 676.4, "end": 680.4, "text": " So if you think if you had a sentence that was just five tokens long and you put that with", "tokens": [407, 498, 291, 519, 498, 291, 632, 257, 8174, 300, 390, 445, 1732, 22667, 938, 293, 291, 829, 300, 365], "temperature": 0.0, "avg_logprob": -0.16575112836114292, "compression_ratio": 1.8053435114503817, "no_speech_prob": 5.2248196880100295e-05}, {"id": 190, "seek": 66172, "start": 680.4, "end": 682.64, "text": " a sentence that was 40 tokens long,", "tokens": [257, 8174, 300, 390, 3356, 22667, 938, 11], "temperature": 0.0, "avg_logprob": -0.16575112836114292, "compression_ratio": 1.8053435114503817, "no_speech_prob": 5.2248196880100295e-05}, {"id": 191, "seek": 66172, "start": 682.64, "end": 687.44, "text": " you're going to have to fill out 35 tokens of padding,", "tokens": [291, 434, 516, 281, 362, 281, 2836, 484, 6976, 22667, 295, 39562, 11], "temperature": 0.0, "avg_logprob": -0.16575112836114292, "compression_ratio": 1.8053435114503817, "no_speech_prob": 5.2248196880100295e-05}, {"id": 192, "seek": 66172, "start": 687.44, "end": 691.4, "text": " which is kind of adding this extra computation", "tokens": [597, 307, 733, 295, 5127, 341, 2857, 24903], "temperature": 0.0, "avg_logprob": -0.16575112836114292, "compression_ratio": 1.8053435114503817, "no_speech_prob": 5.2248196880100295e-05}, {"id": 193, "seek": 69140, "start": 691.4, "end": 693.88, "text": " that it's not really necessary for the short sentence.", "tokens": [300, 309, 311, 406, 534, 4818, 337, 264, 2099, 8174, 13], "temperature": 0.0, "avg_logprob": -0.24468210266857612, "compression_ratio": 1.2566371681415929, "no_speech_prob": 1.952490310941357e-05}, {"id": 194, "seek": 69140, "start": 693.88, "end": 695.92, "text": " But you're having to do that to fill in your data.", "tokens": [583, 291, 434, 1419, 281, 360, 300, 281, 2836, 294, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.24468210266857612, "compression_ratio": 1.2566371681415929, "no_speech_prob": 1.952490310941357e-05}, {"id": 195, "seek": 69140, "start": 695.92, "end": 698.88, "text": " Actually, let me do maybe a picture.", "tokens": [5135, 11, 718, 385, 360, 1310, 257, 3036, 13], "temperature": 0.0, "avg_logprob": -0.24468210266857612, "compression_ratio": 1.2566371681415929, "no_speech_prob": 1.952490310941357e-05}, {"id": 196, "seek": 69888, "start": 698.88, "end": 716.48, "text": " I think I might just erase these.", "tokens": [50364, 286, 519, 286, 1062, 445, 23525, 613, 13, 51244], "temperature": 0.2, "avg_logprob": -0.7425472086126154, "compression_ratio": 0.825, "no_speech_prob": 6.498828588519245e-05}, {"id": 197, "seek": 72888, "start": 729.88, "end": 739.24, "text": " And so the idea with this filling in is say you had like he ate an apple.", "tokens": [400, 370, 264, 1558, 365, 341, 10623, 294, 307, 584, 291, 632, 411, 415, 8468, 364, 10606, 13], "temperature": 0.0, "avg_logprob": -0.23561042463275747, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.019115081056952477}, {"id": 198, "seek": 72888, "start": 739.24, "end": 741.68, "text": " If these were your tokens,", "tokens": [759, 613, 645, 428, 22667, 11], "temperature": 0.0, "avg_logprob": -0.23561042463275747, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.019115081056952477}, {"id": 199, "seek": 72888, "start": 741.68, "end": 745.4399999999999, "text": " she ran.", "tokens": [750, 5872, 13], "temperature": 0.0, "avg_logprob": -0.23561042463275747, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.019115081056952477}, {"id": 200, "seek": 72888, "start": 748.76, "end": 752.56, "text": " He jumped, making them pretty short.", "tokens": [634, 13864, 11, 1455, 552, 1238, 2099, 13], "temperature": 0.0, "avg_logprob": -0.23561042463275747, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.019115081056952477}, {"id": 201, "seek": 72888, "start": 752.56, "end": 755.04, "text": " To get this into the kind of the right format,", "tokens": [1407, 483, 341, 666, 264, 733, 295, 264, 558, 7877, 11], "temperature": 0.0, "avg_logprob": -0.23561042463275747, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.019115081056952477}, {"id": 202, "seek": 72888, "start": 755.04, "end": 757.84, "text": " you would have to kind of put your padding token here", "tokens": [291, 576, 362, 281, 733, 295, 829, 428, 39562, 14862, 510], "temperature": 0.0, "avg_logprob": -0.23561042463275747, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.019115081056952477}, {"id": 203, "seek": 75784, "start": 757.84, "end": 763.2800000000001, "text": " because we want to fill this out to be like a full three by four array.", "tokens": [570, 321, 528, 281, 2836, 341, 484, 281, 312, 411, 257, 1577, 1045, 538, 1451, 10225, 13], "temperature": 0.0, "avg_logprob": -0.147947696370816, "compression_ratio": 1.715481171548117, "no_speech_prob": 4.2645242501748726e-05}, {"id": 204, "seek": 75784, "start": 763.5600000000001, "end": 767.44, "text": " To kind of get something that's more manageable.", "tokens": [1407, 733, 295, 483, 746, 300, 311, 544, 38798, 13], "temperature": 0.0, "avg_logprob": -0.147947696370816, "compression_ratio": 1.715481171548117, "no_speech_prob": 4.2645242501748726e-05}, {"id": 205, "seek": 75784, "start": 767.44, "end": 772.6, "text": " And so here the issue is if one of these sentences was much longer,", "tokens": [400, 370, 510, 264, 2734, 307, 498, 472, 295, 613, 16579, 390, 709, 2854, 11], "temperature": 0.0, "avg_logprob": -0.147947696370816, "compression_ratio": 1.715481171548117, "no_speech_prob": 4.2645242501748726e-05}, {"id": 206, "seek": 75784, "start": 772.6, "end": 775.2800000000001, "text": " you're kind of wasting.", "tokens": [291, 434, 733, 295, 20457, 13], "temperature": 0.0, "avg_logprob": -0.147947696370816, "compression_ratio": 1.715481171548117, "no_speech_prob": 4.2645242501748726e-05}, {"id": 207, "seek": 75784, "start": 775.2800000000001, "end": 777.88, "text": " And it's not the space that's so much the issue,", "tokens": [400, 309, 311, 406, 264, 1901, 300, 311, 370, 709, 264, 2734, 11], "temperature": 0.0, "avg_logprob": -0.147947696370816, "compression_ratio": 1.715481171548117, "no_speech_prob": 4.2645242501748726e-05}, {"id": 208, "seek": 75784, "start": 777.88, "end": 781.4, "text": " but the extra computation of having to make all these things longer.", "tokens": [457, 264, 2857, 24903, 295, 1419, 281, 652, 439, 613, 721, 2854, 13], "temperature": 0.0, "avg_logprob": -0.147947696370816, "compression_ratio": 1.715481171548117, "no_speech_prob": 4.2645242501748726e-05}, {"id": 209, "seek": 75784, "start": 781.4, "end": 784.08, "text": " So there's this benefit to sorting by length,", "tokens": [407, 456, 311, 341, 5121, 281, 32411, 538, 4641, 11], "temperature": 0.0, "avg_logprob": -0.147947696370816, "compression_ratio": 1.715481171548117, "no_speech_prob": 4.2645242501748726e-05}, {"id": 210, "seek": 75784, "start": 784.08, "end": 786.32, "text": " sentence length for your batches.", "tokens": [8174, 4641, 337, 428, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.147947696370816, "compression_ratio": 1.715481171548117, "no_speech_prob": 4.2645242501748726e-05}, {"id": 211, "seek": 78632, "start": 786.32, "end": 788.96, "text": " However, if you sort perfectly,", "tokens": [2908, 11, 498, 291, 1333, 6239, 11], "temperature": 0.0, "avg_logprob": -0.10878780909946986, "compression_ratio": 1.709016393442623, "no_speech_prob": 0.00011957165406784043}, {"id": 212, "seek": 78632, "start": 788.96, "end": 792.96, "text": " you'll get this downside that for every epoch as you're training,", "tokens": [291, 603, 483, 341, 25060, 300, 337, 633, 30992, 339, 382, 291, 434, 3097, 11], "temperature": 0.0, "avg_logprob": -0.10878780909946986, "compression_ratio": 1.709016393442623, "no_speech_prob": 0.00011957165406784043}, {"id": 213, "seek": 78632, "start": 792.96, "end": 794.96, "text": " your batches will be identical.", "tokens": [428, 15245, 279, 486, 312, 14800, 13], "temperature": 0.0, "avg_logprob": -0.10878780909946986, "compression_ratio": 1.709016393442623, "no_speech_prob": 0.00011957165406784043}, {"id": 214, "seek": 78632, "start": 794.96, "end": 799.12, "text": " And this can cause your model to not generalize as well", "tokens": [400, 341, 393, 3082, 428, 2316, 281, 406, 2674, 1125, 382, 731], "temperature": 0.0, "avg_logprob": -0.10878780909946986, "compression_ratio": 1.709016393442623, "no_speech_prob": 0.00011957165406784043}, {"id": 215, "seek": 78632, "start": 799.12, "end": 802.6400000000001, "text": " because it might be kind of learning these idiosyncrasies", "tokens": [570, 309, 1062, 312, 733, 295, 2539, 613, 4496, 2717, 34015, 3906, 530], "temperature": 0.0, "avg_logprob": -0.10878780909946986, "compression_ratio": 1.709016393442623, "no_speech_prob": 0.00011957165406784043}, {"id": 216, "seek": 78632, "start": 802.6400000000001, "end": 806.12, "text": " of how you have your data sorted if it's always the same.", "tokens": [295, 577, 291, 362, 428, 1412, 25462, 498, 309, 311, 1009, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.10878780909946986, "compression_ratio": 1.709016393442623, "no_speech_prob": 0.00011957165406784043}, {"id": 217, "seek": 78632, "start": 806.12, "end": 811.0, "text": " And so the benefit of sortish is that it's giving you some randomness", "tokens": [400, 370, 264, 5121, 295, 1333, 742, 307, 300, 309, 311, 2902, 291, 512, 4974, 1287], "temperature": 0.0, "avg_logprob": -0.10878780909946986, "compression_ratio": 1.709016393442623, "no_speech_prob": 0.00011957165406784043}, {"id": 218, "seek": 78632, "start": 811.0, "end": 814.84, "text": " and your batches will be different each time,", "tokens": [293, 428, 15245, 279, 486, 312, 819, 1184, 565, 11], "temperature": 0.0, "avg_logprob": -0.10878780909946986, "compression_ratio": 1.709016393442623, "no_speech_prob": 0.00011957165406784043}, {"id": 219, "seek": 81484, "start": 814.84, "end": 819.4, "text": " but they'll still be close enough that you're not kind of losing time", "tokens": [457, 436, 603, 920, 312, 1998, 1547, 300, 291, 434, 406, 733, 295, 7027, 565], "temperature": 0.0, "avg_logprob": -0.16125144349767806, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.221850652655121e-05}, {"id": 220, "seek": 81484, "start": 819.4, "end": 823.08, "text": " on performance from having to fill out really short sentences", "tokens": [322, 3389, 490, 1419, 281, 2836, 484, 534, 2099, 16579], "temperature": 0.0, "avg_logprob": -0.16125144349767806, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.221850652655121e-05}, {"id": 221, "seek": 81484, "start": 823.08, "end": 825.08, "text": " with a ton of padding.", "tokens": [365, 257, 2952, 295, 39562, 13], "temperature": 0.0, "avg_logprob": -0.16125144349767806, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.221850652655121e-05}, {"id": 222, "seek": 81484, "start": 825.08, "end": 827.08, "text": " Are there questions about that?", "tokens": [2014, 456, 1651, 466, 300, 30], "temperature": 0.0, "avg_logprob": -0.16125144349767806, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.221850652655121e-05}, {"id": 223, "seek": 81484, "start": 829.52, "end": 833.76, "text": " So that's what's going on with this sortish sampler.", "tokens": [407, 300, 311, 437, 311, 516, 322, 365, 341, 1333, 742, 3247, 22732, 13], "temperature": 0.0, "avg_logprob": -0.16125144349767806, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.221850652655121e-05}, {"id": 224, "seek": 81484, "start": 833.76, "end": 836.96, "text": " And then basically we want to get our data loaders together.", "tokens": [400, 550, 1936, 321, 528, 281, 483, 527, 1412, 3677, 433, 1214, 13], "temperature": 0.0, "avg_logprob": -0.16125144349767806, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.221850652655121e-05}, {"id": 225, "seek": 81484, "start": 836.96, "end": 843.1600000000001, "text": " So having our training data loader, validation data loaders added,", "tokens": [407, 1419, 527, 3097, 1412, 3677, 260, 11, 24071, 1412, 3677, 433, 3869, 11], "temperature": 0.0, "avg_logprob": -0.16125144349767806, "compression_ratio": 1.6311111111111112, "no_speech_prob": 1.221850652655121e-05}, {"id": 226, "seek": 84316, "start": 843.16, "end": 846.1999999999999, "text": " kind of putting those together and having this collating method", "tokens": [733, 295, 3372, 729, 1214, 293, 1419, 341, 1263, 990, 3170], "temperature": 0.0, "avg_logprob": -0.21707605180286227, "compression_ratio": 1.5247524752475248, "no_speech_prob": 4.831648402614519e-05}, {"id": 227, "seek": 84316, "start": 846.1999999999999, "end": 848.1999999999999, "text": " that will fill in our padding.", "tokens": [300, 486, 2836, 294, 527, 39562, 13], "temperature": 0.0, "avg_logprob": -0.21707605180286227, "compression_ratio": 1.5247524752475248, "no_speech_prob": 4.831648402614519e-05}, {"id": 228, "seek": 84316, "start": 850.3199999999999, "end": 855.6, "text": " Yeah, and then down here we had defined our loss in accuracy.", "tokens": [865, 11, 293, 550, 760, 510, 321, 632, 7642, 527, 4470, 294, 14170, 13], "temperature": 0.0, "avg_logprob": -0.21707605180286227, "compression_ratio": 1.5247524752475248, "no_speech_prob": 4.831648402614519e-05}, {"id": 229, "seek": 84316, "start": 855.6, "end": 859.3199999999999, "text": " These n-grams and getting n-grams are necessary", "tokens": [1981, 297, 12, 1342, 82, 293, 1242, 297, 12, 1342, 82, 366, 4818], "temperature": 0.0, "avg_logprob": -0.21707605180286227, "compression_ratio": 1.5247524752475248, "no_speech_prob": 4.831648402614519e-05}, {"id": 230, "seek": 84316, "start": 859.3199999999999, "end": 862.52, "text": " for calculating the blue score as we talked about earlier.", "tokens": [337, 28258, 264, 3344, 6175, 382, 321, 2825, 466, 3071, 13], "temperature": 0.0, "avg_logprob": -0.21707605180286227, "compression_ratio": 1.5247524752475248, "no_speech_prob": 4.831648402614519e-05}, {"id": 231, "seek": 84316, "start": 862.52, "end": 864.36, "text": " Is that a question?", "tokens": [1119, 300, 257, 1168, 30], "temperature": 0.0, "avg_logprob": -0.21707605180286227, "compression_ratio": 1.5247524752475248, "no_speech_prob": 4.831648402614519e-05}, {"id": 232, "seek": 84316, "start": 864.36, "end": 866.36, "text": " Can I throw you the box?", "tokens": [1664, 286, 3507, 291, 264, 2424, 30], "temperature": 0.0, "avg_logprob": -0.21707605180286227, "compression_ratio": 1.5247524752475248, "no_speech_prob": 4.831648402614519e-05}, {"id": 233, "seek": 86636, "start": 866.36, "end": 874.12, "text": " What's the benefit of using the sorting methods rather than", "tokens": [708, 311, 264, 5121, 295, 1228, 264, 32411, 7150, 2831, 813], "temperature": 0.0, "avg_logprob": -0.5551539611816406, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.00017239390581380576}, {"id": 234, "seek": 86636, "start": 874.12, "end": 877.24, "text": " like bad sequences and like that?", "tokens": [411, 1578, 22978, 293, 411, 300, 30], "temperature": 0.0, "avg_logprob": -0.5551539611816406, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.00017239390581380576}, {"id": 235, "seek": 86636, "start": 877.24, "end": 882.6, "text": " Like when you kind of put them one next to each other,", "tokens": [1743, 562, 291, 733, 295, 829, 552, 472, 958, 281, 1184, 661, 11], "temperature": 0.0, "avg_logprob": -0.5551539611816406, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.00017239390581380576}, {"id": 236, "seek": 86636, "start": 882.6, "end": 887.44, "text": " so you only have like two leads, that's why you have like the sequences", "tokens": [370, 291, 787, 362, 411, 732, 6689, 11, 300, 311, 983, 291, 362, 411, 264, 22978], "temperature": 0.0, "avg_logprob": -0.5551539611816406, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.00017239390581380576}, {"id": 237, "seek": 86636, "start": 887.44, "end": 893.44, "text": " one continues and then like it kind of keeps driving one, one, start,", "tokens": [472, 6515, 293, 550, 411, 309, 733, 295, 5965, 4840, 472, 11, 472, 11, 722, 11], "temperature": 0.0, "avg_logprob": -0.5551539611816406, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.00017239390581380576}, {"id": 238, "seek": 89344, "start": 893.44, "end": 897.44, "text": " and the other ends.", "tokens": [293, 264, 661, 5314, 13], "temperature": 0.0, "avg_logprob": -0.2706576260653409, "compression_ratio": 1.5658536585365854, "no_speech_prob": 1.696377330517862e-05}, {"id": 239, "seek": 89344, "start": 897.44, "end": 899.7600000000001, "text": " Oh, you mean you're saying kind of having everything", "tokens": [876, 11, 291, 914, 291, 434, 1566, 733, 295, 1419, 1203], "temperature": 0.0, "avg_logprob": -0.2706576260653409, "compression_ratio": 1.5658536585365854, "no_speech_prob": 1.696377330517862e-05}, {"id": 240, "seek": 89344, "start": 899.7600000000001, "end": 902.2, "text": " in one single long sequence?", "tokens": [294, 472, 2167, 938, 8310, 30], "temperature": 0.0, "avg_logprob": -0.2706576260653409, "compression_ratio": 1.5658536585365854, "no_speech_prob": 1.696377330517862e-05}, {"id": 241, "seek": 89344, "start": 902.2, "end": 904.9200000000001, "text": " And then like keeping driving like one basically.", "tokens": [400, 550, 411, 5145, 4840, 411, 472, 1936, 13], "temperature": 0.0, "avg_logprob": -0.2706576260653409, "compression_ratio": 1.5658536585365854, "no_speech_prob": 1.696377330517862e-05}, {"id": 242, "seek": 89344, "start": 904.9200000000001, "end": 910.6400000000001, "text": " So that's not letting you take advantage of GPUs and doing batches", "tokens": [407, 300, 311, 406, 8295, 291, 747, 5002, 295, 18407, 82, 293, 884, 15245, 279], "temperature": 0.0, "avg_logprob": -0.2706576260653409, "compression_ratio": 1.5658536585365854, "no_speech_prob": 1.696377330517862e-05}, {"id": 243, "seek": 89344, "start": 910.6400000000001, "end": 916.08, "text": " because you, let me go back to the picture.", "tokens": [570, 291, 11, 718, 385, 352, 646, 281, 264, 3036, 13], "temperature": 0.0, "avg_logprob": -0.2706576260653409, "compression_ratio": 1.5658536585365854, "no_speech_prob": 1.696377330517862e-05}, {"id": 244, "seek": 91608, "start": 916.08, "end": 927.72, "text": " Oh, maybe I need to go this way to go back to the picture.", "tokens": [876, 11, 1310, 286, 643, 281, 352, 341, 636, 281, 352, 646, 281, 264, 3036, 13], "temperature": 0.0, "avg_logprob": -0.17536557288396926, "compression_ratio": 1.5583756345177664, "no_speech_prob": 2.684135324670933e-06}, {"id": 245, "seek": 91608, "start": 927.72, "end": 930.5600000000001, "text": " So you're kind of saying, you know, stringing after this sentence,", "tokens": [407, 291, 434, 733, 295, 1566, 11, 291, 458, 11, 6798, 278, 934, 341, 8174, 11], "temperature": 0.0, "avg_logprob": -0.17536557288396926, "compression_ratio": 1.5583756345177664, "no_speech_prob": 2.684135324670933e-06}, {"id": 246, "seek": 91608, "start": 930.5600000000001, "end": 933.72, "text": " then string together another sentence.", "tokens": [550, 6798, 1214, 1071, 8174, 13], "temperature": 0.0, "avg_logprob": -0.17536557288396926, "compression_ratio": 1.5583756345177664, "no_speech_prob": 2.684135324670933e-06}, {"id": 247, "seek": 91608, "start": 933.72, "end": 938.0, "text": " There, because you only have like a dimension of one on this side,", "tokens": [821, 11, 570, 291, 787, 362, 411, 257, 10139, 295, 472, 322, 341, 1252, 11], "temperature": 0.0, "avg_logprob": -0.17536557288396926, "compression_ratio": 1.5583756345177664, "no_speech_prob": 2.684135324670933e-06}, {"id": 248, "seek": 91608, "start": 938.0, "end": 940.6400000000001, "text": " that's not letting you take advantage of kind", "tokens": [300, 311, 406, 8295, 291, 747, 5002, 295, 733], "temperature": 0.0, "avg_logprob": -0.17536557288396926, "compression_ratio": 1.5583756345177664, "no_speech_prob": 2.684135324670933e-06}, {"id": 249, "seek": 91608, "start": 940.6400000000001, "end": 943.9200000000001, "text": " of like having a batch of 64?", "tokens": [295, 411, 1419, 257, 15245, 295, 12145, 30], "temperature": 0.0, "avg_logprob": -0.17536557288396926, "compression_ratio": 1.5583756345177664, "no_speech_prob": 2.684135324670933e-06}, {"id": 250, "seek": 94392, "start": 943.92, "end": 952.0, "text": " To kind of, it's kind of the parallelization that the GPU lets you do,", "tokens": [1407, 733, 295, 11, 309, 311, 733, 295, 264, 8952, 2144, 300, 264, 18407, 6653, 291, 360, 11], "temperature": 0.0, "avg_logprob": -0.3372686104696305, "compression_ratio": 1.3666666666666667, "no_speech_prob": 4.26446640631184e-05}, {"id": 251, "seek": 94392, "start": 952.0, "end": 955.68, "text": " taking advantage of that is kind of what makes training much faster.", "tokens": [1940, 5002, 295, 300, 307, 733, 295, 437, 1669, 3097, 709, 4663, 13], "temperature": 0.0, "avg_logprob": -0.3372686104696305, "compression_ratio": 1.3666666666666667, "no_speech_prob": 4.26446640631184e-05}, {"id": 252, "seek": 94392, "start": 955.68, "end": 957.4399999999999, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3372686104696305, "compression_ratio": 1.3666666666666667, "no_speech_prob": 4.26446640631184e-05}, {"id": 253, "seek": 94392, "start": 957.4399999999999, "end": 967.12, "text": " Oh, Jeremy's going to get it.", "tokens": [876, 11, 17809, 311, 516, 281, 483, 309, 13], "temperature": 0.0, "avg_logprob": -0.3372686104696305, "compression_ratio": 1.3666666666666667, "no_speech_prob": 4.26446640631184e-05}, {"id": 254, "seek": 94392, "start": 967.12, "end": 971.28, "text": " Oh yeah, any other questions?", "tokens": [876, 1338, 11, 604, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.3372686104696305, "compression_ratio": 1.3666666666666667, "no_speech_prob": 4.26446640631184e-05}, {"id": 255, "seek": 97128, "start": 971.28, "end": 974.8399999999999, "text": " Okay, and we'll be using kind of all of this again in the transformer", "tokens": [1033, 11, 293, 321, 603, 312, 1228, 733, 295, 439, 295, 341, 797, 294, 264, 31782], "temperature": 0.0, "avg_logprob": -0.1602156639099121, "compression_ratio": 1.5870646766169154, "no_speech_prob": 4.0061760955723e-05}, {"id": 256, "seek": 97128, "start": 974.8399999999999, "end": 977.24, "text": " is why I'm going over this.", "tokens": [307, 983, 286, 478, 516, 670, 341, 13], "temperature": 0.0, "avg_logprob": -0.1602156639099121, "compression_ratio": 1.5870646766169154, "no_speech_prob": 4.0061760955723e-05}, {"id": 257, "seek": 97128, "start": 977.24, "end": 981.76, "text": " So here we saw calculating the blue score,", "tokens": [407, 510, 321, 1866, 28258, 264, 3344, 6175, 11], "temperature": 0.0, "avg_logprob": -0.1602156639099121, "compression_ratio": 1.5870646766169154, "no_speech_prob": 4.0061760955723e-05}, {"id": 258, "seek": 97128, "start": 981.76, "end": 987.0799999999999, "text": " which involves getting the correct n-grams.", "tokens": [597, 11626, 1242, 264, 3006, 297, 12, 1342, 82, 13], "temperature": 0.0, "avg_logprob": -0.1602156639099121, "compression_ratio": 1.5870646766169154, "no_speech_prob": 4.0061760955723e-05}, {"id": 259, "seek": 97128, "start": 987.0799999999999, "end": 991.24, "text": " We talked about this a little bit earlier, the correct n-grams.", "tokens": [492, 2825, 466, 341, 257, 707, 857, 3071, 11, 264, 3006, 297, 12, 1342, 82, 13], "temperature": 0.0, "avg_logprob": -0.1602156639099121, "compression_ratio": 1.5870646766169154, "no_speech_prob": 4.0061760955723e-05}, {"id": 260, "seek": 97128, "start": 991.24, "end": 999.16, "text": " First you have to figure out what the n-grams that are in the sentence", "tokens": [2386, 291, 362, 281, 2573, 484, 437, 264, 297, 12, 1342, 82, 300, 366, 294, 264, 8174], "temperature": 0.0, "avg_logprob": -0.1602156639099121, "compression_ratio": 1.5870646766169154, "no_speech_prob": 4.0061760955723e-05}, {"id": 261, "seek": 99916, "start": 999.16, "end": 1002.4, "text": " you've predicted as well as the correct answer are,", "tokens": [291, 600, 19147, 382, 731, 382, 264, 3006, 1867, 366, 11], "temperature": 0.0, "avg_logprob": -0.1473648452758789, "compression_ratio": 1.7980769230769231, "no_speech_prob": 2.4681708964635618e-05}, {"id": 262, "seek": 99916, "start": 1002.4, "end": 1011.4399999999999, "text": " and then counting them.", "tokens": [293, 550, 13251, 552, 13], "temperature": 0.0, "avg_logprob": -0.1473648452758789, "compression_ratio": 1.7980769230769231, "no_speech_prob": 2.4681708964635618e-05}, {"id": 263, "seek": 99916, "start": 1011.4399999999999, "end": 1016.1999999999999, "text": " And down here that comes into play with kind of the predictions,", "tokens": [400, 760, 510, 300, 1487, 666, 862, 365, 733, 295, 264, 21264, 11], "temperature": 0.0, "avg_logprob": -0.1473648452758789, "compression_ratio": 1.7980769230769231, "no_speech_prob": 2.4681708964635618e-05}, {"id": 264, "seek": 99916, "start": 1016.1999999999999, "end": 1019.48, "text": " the length penalty, that's also called the brevity penalty,", "tokens": [264, 4641, 16263, 11, 300, 311, 611, 1219, 264, 1403, 23110, 16263, 11], "temperature": 0.0, "avg_logprob": -0.1473648452758789, "compression_ratio": 1.7980769230769231, "no_speech_prob": 2.4681708964635618e-05}, {"id": 265, "seek": 99916, "start": 1019.48, "end": 1022.0, "text": " and that's the one that's kind of if what you predict", "tokens": [293, 300, 311, 264, 472, 300, 311, 733, 295, 498, 437, 291, 6069], "temperature": 0.0, "avg_logprob": -0.1473648452758789, "compression_ratio": 1.7980769230769231, "no_speech_prob": 2.4681708964635618e-05}, {"id": 266, "seek": 99916, "start": 1022.0, "end": 1023.8, "text": " is way shorter than the correct answer,", "tokens": [307, 636, 11639, 813, 264, 3006, 1867, 11], "temperature": 0.0, "avg_logprob": -0.1473648452758789, "compression_ratio": 1.7980769230769231, "no_speech_prob": 2.4681708964635618e-05}, {"id": 267, "seek": 99916, "start": 1023.8, "end": 1026.08, "text": " or even if everything in it is, you know,", "tokens": [420, 754, 498, 1203, 294, 309, 307, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.1473648452758789, "compression_ratio": 1.7980769230769231, "no_speech_prob": 2.4681708964635618e-05}, {"id": 268, "seek": 99916, "start": 1026.08, "end": 1027.76, "text": " a correct subset of the right answer,", "tokens": [257, 3006, 25993, 295, 264, 558, 1867, 11], "temperature": 0.0, "avg_logprob": -0.1473648452758789, "compression_ratio": 1.7980769230769231, "no_speech_prob": 2.4681708964635618e-05}, {"id": 269, "seek": 102776, "start": 1027.76, "end": 1032.2, "text": " it's not as good a translation, so you want to penalize that.", "tokens": [309, 311, 406, 382, 665, 257, 12853, 11, 370, 291, 528, 281, 13661, 1125, 300, 13], "temperature": 0.0, "avg_logprob": -0.11655435723773504, "compression_ratio": 1.7411764705882353, "no_speech_prob": 2.9310140234883875e-05}, {"id": 270, "seek": 102776, "start": 1032.2, "end": 1037.48, "text": " And so, yeah, this is how those are calculated.", "tokens": [400, 370, 11, 1338, 11, 341, 307, 577, 729, 366, 15598, 13], "temperature": 0.0, "avg_logprob": -0.11655435723773504, "compression_ratio": 1.7411764705882353, "no_speech_prob": 2.9310140234883875e-05}, {"id": 271, "seek": 102776, "start": 1037.48, "end": 1042.8799999999999, "text": " Teacher forcing, we can see here that the probability of teacher forcing", "tokens": [19745, 19030, 11, 321, 393, 536, 510, 300, 264, 8482, 295, 5027, 19030], "temperature": 0.0, "avg_logprob": -0.11655435723773504, "compression_ratio": 1.7411764705882353, "no_speech_prob": 2.9310140234883875e-05}, {"id": 272, "seek": 102776, "start": 1042.8799999999999, "end": 1046.12, "text": " is going to start out at one and then decrease", "tokens": [307, 516, 281, 722, 484, 412, 472, 293, 550, 11514], "temperature": 0.0, "avg_logprob": -0.11655435723773504, "compression_ratio": 1.7411764705882353, "no_speech_prob": 2.9310140234883875e-05}, {"id": 273, "seek": 102776, "start": 1046.12, "end": 1048.08, "text": " as you get later and later into the epoch,", "tokens": [382, 291, 483, 1780, 293, 1780, 666, 264, 30992, 339, 11], "temperature": 0.0, "avg_logprob": -0.11655435723773504, "compression_ratio": 1.7411764705882353, "no_speech_prob": 2.9310140234883875e-05}, {"id": 274, "seek": 102776, "start": 1048.08, "end": 1050.36, "text": " and that's because your model's getting better,", "tokens": [293, 300, 311, 570, 428, 2316, 311, 1242, 1101, 11], "temperature": 0.0, "avg_logprob": -0.11655435723773504, "compression_ratio": 1.7411764705882353, "no_speech_prob": 2.9310140234883875e-05}, {"id": 275, "seek": 102776, "start": 1050.36, "end": 1053.8799999999999, "text": " and so you don't want to keep feeding in the correct answer all the time,", "tokens": [293, 370, 291, 500, 380, 528, 281, 1066, 12919, 294, 264, 3006, 1867, 439, 264, 565, 11], "temperature": 0.0, "avg_logprob": -0.11655435723773504, "compression_ratio": 1.7411764705882353, "no_speech_prob": 2.9310140234883875e-05}, {"id": 276, "seek": 102776, "start": 1053.8799999999999, "end": 1056.08, "text": " but want to have it to start using its own output", "tokens": [457, 528, 281, 362, 309, 281, 722, 1228, 1080, 1065, 5598], "temperature": 0.0, "avg_logprob": -0.11655435723773504, "compression_ratio": 1.7411764705882353, "no_speech_prob": 2.9310140234883875e-05}, {"id": 277, "seek": 105608, "start": 1056.08, "end": 1064.32, "text": " since that's what will be necessary at inference time or in production.", "tokens": [1670, 300, 311, 437, 486, 312, 4818, 412, 38253, 565, 420, 294, 4265, 13], "temperature": 0.0, "avg_logprob": -0.16539993041600937, "compression_ratio": 1.4808743169398908, "no_speech_prob": 2.0783227228093892e-05}, {"id": 278, "seek": 105608, "start": 1064.32, "end": 1066.3999999999999, "text": " And then here we implement attention,", "tokens": [400, 550, 510, 321, 4445, 3202, 11], "temperature": 0.0, "avg_logprob": -0.16539993041600937, "compression_ratio": 1.4808743169398908, "no_speech_prob": 2.0783227228093892e-05}, {"id": 279, "seek": 105608, "start": 1066.3999999999999, "end": 1068.1599999999999, "text": " and so kind of the key lines for this,", "tokens": [293, 370, 733, 295, 264, 2141, 3876, 337, 341, 11], "temperature": 0.0, "avg_logprob": -0.16539993041600937, "compression_ratio": 1.4808743169398908, "no_speech_prob": 2.0783227228093892e-05}, {"id": 280, "seek": 105608, "start": 1068.1599999999999, "end": 1073.1999999999998, "text": " so we have an encoder and a decoder,", "tokens": [370, 321, 362, 364, 2058, 19866, 293, 257, 979, 19866, 11], "temperature": 0.0, "avg_logprob": -0.16539993041600937, "compression_ratio": 1.4808743169398908, "no_speech_prob": 2.0783227228093892e-05}, {"id": 281, "seek": 105608, "start": 1073.1999999999998, "end": 1075.8799999999999, "text": " which are both embeddings from before.", "tokens": [597, 366, 1293, 12240, 29432, 490, 949, 13], "temperature": 0.0, "avg_logprob": -0.16539993041600937, "compression_ratio": 1.4808743169398908, "no_speech_prob": 2.0783227228093892e-05}, {"id": 282, "seek": 105608, "start": 1075.8799999999999, "end": 1079.12, "text": " Since this is seek to seek, we're using a GRU.", "tokens": [4162, 341, 307, 8075, 281, 8075, 11, 321, 434, 1228, 257, 10903, 52, 13], "temperature": 0.0, "avg_logprob": -0.16539993041600937, "compression_ratio": 1.4808743169398908, "no_speech_prob": 2.0783227228093892e-05}, {"id": 283, "seek": 107912, "start": 1079.12, "end": 1087.7199999999998, "text": " Does anyone remember what the problem that GRUs addresses?", "tokens": [4402, 2878, 1604, 437, 264, 1154, 300, 10903, 29211, 16862, 30], "temperature": 0.0, "avg_logprob": -0.17705138524373373, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.9525161405908875e-05}, {"id": 284, "seek": 107912, "start": 1087.7199999999998, "end": 1094.32, "text": " I think I hear people whispering it maybe louder.", "tokens": [286, 519, 286, 1568, 561, 42445, 309, 1310, 22717, 13], "temperature": 0.0, "avg_logprob": -0.17705138524373373, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.9525161405908875e-05}, {"id": 285, "seek": 107912, "start": 1094.32, "end": 1097.9199999999998, "text": " Yeah, so vanishing gradients is very much related,", "tokens": [865, 11, 370, 3161, 3807, 2771, 2448, 307, 588, 709, 4077, 11], "temperature": 0.0, "avg_logprob": -0.17705138524373373, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.9525161405908875e-05}, {"id": 286, "seek": 107912, "start": 1097.9199999999998, "end": 1102.6399999999999, "text": " but it's short-term memory, which can be caused by vanishing gradients,", "tokens": [457, 309, 311, 2099, 12, 7039, 4675, 11, 597, 393, 312, 7008, 538, 3161, 3807, 2771, 2448, 11], "temperature": 0.0, "avg_logprob": -0.17705138524373373, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.9525161405908875e-05}, {"id": 287, "seek": 107912, "start": 1102.6399999999999, "end": 1104.6399999999999, "text": " and that's that as your sequence gets longer,", "tokens": [293, 300, 311, 300, 382, 428, 8310, 2170, 2854, 11], "temperature": 0.0, "avg_logprob": -0.17705138524373373, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.9525161405908875e-05}, {"id": 288, "seek": 107912, "start": 1104.6399999999999, "end": 1108.84, "text": " it becomes harder for the model to kind of hold state from the beginning,", "tokens": [309, 3643, 6081, 337, 264, 2316, 281, 733, 295, 1797, 1785, 490, 264, 2863, 11], "temperature": 0.0, "avg_logprob": -0.17705138524373373, "compression_ratio": 1.5194805194805194, "no_speech_prob": 1.9525161405908875e-05}, {"id": 289, "seek": 110884, "start": 1108.84, "end": 1116.12, "text": " and so GRUs or LSTMs are two different mini architectures", "tokens": [293, 370, 10903, 29211, 420, 441, 6840, 26386, 366, 732, 819, 8382, 6331, 1303], "temperature": 0.0, "avg_logprob": -0.16562033223581837, "compression_ratio": 1.5227272727272727, "no_speech_prob": 1.922211413329933e-05}, {"id": 290, "seek": 110884, "start": 1116.12, "end": 1120.9599999999998, "text": " that you can use to address that in your model.", "tokens": [300, 291, 393, 764, 281, 2985, 300, 294, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16562033223581837, "compression_ratio": 1.5227272727272727, "no_speech_prob": 1.922211413329933e-05}, {"id": 291, "seek": 110884, "start": 1120.9599999999998, "end": 1125.76, "text": " Down here, so we've got some dropout, linear layer.", "tokens": [9506, 510, 11, 370, 321, 600, 658, 512, 3270, 346, 11, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.16562033223581837, "compression_ratio": 1.5227272727272727, "no_speech_prob": 1.922211413329933e-05}, {"id": 292, "seek": 110884, "start": 1125.76, "end": 1130.0, "text": " Then this is kind of the attention part comes into play here,", "tokens": [1396, 341, 307, 733, 295, 264, 3202, 644, 1487, 666, 862, 510, 11], "temperature": 0.0, "avg_logprob": -0.16562033223581837, "compression_ratio": 1.5227272727272727, "no_speech_prob": 1.922211413329933e-05}, {"id": 293, "seek": 110884, "start": 1130.0, "end": 1134.56, "text": " which is basically just learning weights to take a weighted average,", "tokens": [597, 307, 1936, 445, 2539, 17443, 281, 747, 257, 32807, 4274, 11], "temperature": 0.0, "avg_logprob": -0.16562033223581837, "compression_ratio": 1.5227272727272727, "no_speech_prob": 1.922211413329933e-05}, {"id": 294, "seek": 110884, "start": 1134.56, "end": 1138.32, "text": " and so we can see that in play in the decoder.", "tokens": [293, 370, 321, 393, 536, 300, 294, 862, 294, 264, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.16562033223581837, "compression_ratio": 1.5227272727272727, "no_speech_prob": 1.922211413329933e-05}, {"id": 295, "seek": 113832, "start": 1138.32, "end": 1143.0, "text": " Here it's adding up the attention on,", "tokens": [1692, 309, 311, 5127, 493, 264, 3202, 322, 11], "temperature": 0.0, "avg_logprob": -0.1526554468515757, "compression_ratio": 1.5934065934065933, "no_speech_prob": 2.2124513634480536e-05}, {"id": 296, "seek": 113832, "start": 1143.0, "end": 1147.48, "text": " so you're kind of bringing in these inputs from your encoder", "tokens": [370, 291, 434, 733, 295, 5062, 294, 613, 15743, 490, 428, 2058, 19866], "temperature": 0.0, "avg_logprob": -0.1526554468515757, "compression_ratio": 1.5934065934065933, "no_speech_prob": 2.2124513634480536e-05}, {"id": 297, "seek": 113832, "start": 1147.48, "end": 1151.0, "text": " and from the hidden state, adding those together,", "tokens": [293, 490, 264, 7633, 1785, 11, 5127, 729, 1214, 11], "temperature": 0.0, "avg_logprob": -0.1526554468515757, "compression_ratio": 1.5934065934065933, "no_speech_prob": 2.2124513634480536e-05}, {"id": 298, "seek": 113832, "start": 1151.0, "end": 1160.6799999999998, "text": " and then multiplying that by V, which are the weights you've learned,", "tokens": [293, 550, 30955, 300, 538, 691, 11, 597, 366, 264, 17443, 291, 600, 3264, 11], "temperature": 0.0, "avg_logprob": -0.1526554468515757, "compression_ratio": 1.5934065934065933, "no_speech_prob": 2.2124513634480536e-05}, {"id": 299, "seek": 113832, "start": 1160.6799999999998, "end": 1164.3999999999999, "text": " and you get the context is what it's called when you take the attention", "tokens": [293, 291, 483, 264, 4319, 307, 437, 309, 311, 1219, 562, 291, 747, 264, 3202], "temperature": 0.0, "avg_logprob": -0.1526554468515757, "compression_ratio": 1.5934065934065933, "no_speech_prob": 2.2124513634480536e-05}, {"id": 300, "seek": 116440, "start": 1164.4, "end": 1169.24, "text": " weights times the output from the encoder that's getting packed into the decoder,", "tokens": [17443, 1413, 264, 5598, 490, 264, 2058, 19866, 300, 311, 1242, 13265, 666, 264, 979, 19866, 11], "temperature": 0.0, "avg_logprob": -0.17932875438403056, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.012980555969989e-05}, {"id": 301, "seek": 116440, "start": 1169.24, "end": 1173.0400000000002, "text": " so this is just a weighted average telling you kind of how important", "tokens": [370, 341, 307, 445, 257, 32807, 4274, 3585, 291, 733, 295, 577, 1021], "temperature": 0.0, "avg_logprob": -0.17932875438403056, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.012980555969989e-05}, {"id": 302, "seek": 116440, "start": 1173.0400000000002, "end": 1177.0, "text": " each hidden state from all the previous time steps is.", "tokens": [1184, 7633, 1785, 490, 439, 264, 3894, 565, 4439, 307, 13], "temperature": 0.0, "avg_logprob": -0.17932875438403056, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.012980555969989e-05}, {"id": 303, "seek": 116440, "start": 1180.0, "end": 1186.5600000000002, "text": " And so here we're using attention in combination with an RNN,", "tokens": [400, 370, 510, 321, 434, 1228, 3202, 294, 6562, 365, 364, 45702, 45, 11], "temperature": 0.0, "avg_logprob": -0.17932875438403056, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.012980555969989e-05}, {"id": 304, "seek": 116440, "start": 1186.5600000000002, "end": 1191.44, "text": " but I guess let's go ahead and switch over to the transformer.", "tokens": [457, 286, 2041, 718, 311, 352, 2286, 293, 3679, 670, 281, 264, 31782, 13], "temperature": 0.0, "avg_logprob": -0.17932875438403056, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.012980555969989e-05}, {"id": 305, "seek": 116440, "start": 1191.44, "end": 1194.0, "text": " So the transformer does not use an RNN.", "tokens": [407, 264, 31782, 775, 406, 764, 364, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.17932875438403056, "compression_ratio": 1.6017316017316017, "no_speech_prob": 1.012980555969989e-05}, {"id": 306, "seek": 119400, "start": 1194.0, "end": 1197.68, "text": " It comes from the paper.", "tokens": [467, 1487, 490, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.21665428424703664, "compression_ratio": 1.5765306122448979, "no_speech_prob": 1.618701935512945e-05}, {"id": 307, "seek": 119400, "start": 1197.68, "end": 1198.68, "text": " I have to get it.", "tokens": [286, 362, 281, 483, 309, 13], "temperature": 0.0, "avg_logprob": -0.21665428424703664, "compression_ratio": 1.5765306122448979, "no_speech_prob": 1.618701935512945e-05}, {"id": 308, "seek": 119400, "start": 1202.4, "end": 1204.76, "text": " Let me go back up to the top.", "tokens": [961, 385, 352, 646, 493, 281, 264, 1192, 13], "temperature": 0.0, "avg_logprob": -0.21665428424703664, "compression_ratio": 1.5765306122448979, "no_speech_prob": 1.618701935512945e-05}, {"id": 309, "seek": 119400, "start": 1204.76, "end": 1207.68, "text": " It comes from the paper, attention is all you need,", "tokens": [467, 1487, 490, 264, 3035, 11, 3202, 307, 439, 291, 643, 11], "temperature": 0.0, "avg_logprob": -0.21665428424703664, "compression_ratio": 1.5765306122448979, "no_speech_prob": 1.618701935512945e-05}, {"id": 310, "seek": 119400, "start": 1207.68, "end": 1210.76, "text": " but as we talked about last time, it's not just attention.", "tokens": [457, 382, 321, 2825, 466, 1036, 565, 11, 309, 311, 406, 445, 3202, 13], "temperature": 0.0, "avg_logprob": -0.21665428424703664, "compression_ratio": 1.5765306122448979, "no_speech_prob": 1.618701935512945e-05}, {"id": 311, "seek": 119400, "start": 1210.76, "end": 1216.76, "text": " It's also positional encodings, feed forward blocks,", "tokens": [467, 311, 611, 2535, 304, 2058, 378, 1109, 11, 3154, 2128, 8474, 11], "temperature": 0.0, "avg_logprob": -0.21665428424703664, "compression_ratio": 1.5765306122448979, "no_speech_prob": 1.618701935512945e-05}, {"id": 312, "seek": 119400, "start": 1216.76, "end": 1223.24, "text": " which are kind of traditional fully connected neural network components.", "tokens": [597, 366, 733, 295, 5164, 4498, 4582, 18161, 3209, 6677, 13], "temperature": 0.0, "avg_logprob": -0.21665428424703664, "compression_ratio": 1.5765306122448979, "no_speech_prob": 1.618701935512945e-05}, {"id": 313, "seek": 122324, "start": 1223.24, "end": 1225.44, "text": " There's also label smoothing.", "tokens": [821, 311, 611, 7645, 899, 6259, 571, 13], "temperature": 0.0, "avg_logprob": -0.13302035989432498, "compression_ratio": 1.6991869918699187, "no_speech_prob": 7.6462720244308e-06}, {"id": 314, "seek": 122324, "start": 1225.44, "end": 1230.4, "text": " There are a few different things that go into this beyond just attention.", "tokens": [821, 366, 257, 1326, 819, 721, 300, 352, 666, 341, 4399, 445, 3202, 13], "temperature": 0.0, "avg_logprob": -0.13302035989432498, "compression_ratio": 1.6991869918699187, "no_speech_prob": 7.6462720244308e-06}, {"id": 315, "seek": 122324, "start": 1230.4, "end": 1233.16, "text": " And so let's start walking through this, and we'll go back to,", "tokens": [400, 370, 718, 311, 722, 4494, 807, 341, 11, 293, 321, 603, 352, 646, 281, 11], "temperature": 0.0, "avg_logprob": -0.13302035989432498, "compression_ratio": 1.6991869918699187, "no_speech_prob": 7.6462720244308e-06}, {"id": 316, "seek": 122324, "start": 1233.16, "end": 1235.68, "text": " we had some slides that I think were helpful.", "tokens": [321, 632, 512, 9788, 300, 286, 519, 645, 4961, 13], "temperature": 0.0, "avg_logprob": -0.13302035989432498, "compression_ratio": 1.6991869918699187, "no_speech_prob": 7.6462720244308e-06}, {"id": 317, "seek": 122324, "start": 1235.68, "end": 1237.4, "text": " We'll go back to those in a moment.", "tokens": [492, 603, 352, 646, 281, 729, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.13302035989432498, "compression_ratio": 1.6991869918699187, "no_speech_prob": 7.6462720244308e-06}, {"id": 318, "seek": 122324, "start": 1237.4, "end": 1242.64, "text": " So here, this is the same seek to seek collate that we saw before,", "tokens": [407, 510, 11, 341, 307, 264, 912, 8075, 281, 8075, 1263, 473, 300, 321, 1866, 949, 11], "temperature": 0.0, "avg_logprob": -0.13302035989432498, "compression_ratio": 1.6991869918699187, "no_speech_prob": 7.6462720244308e-06}, {"id": 319, "seek": 122324, "start": 1242.64, "end": 1248.2, "text": " and this is again just kind of adding padding", "tokens": [293, 341, 307, 797, 445, 733, 295, 5127, 39562], "temperature": 0.0, "avg_logprob": -0.13302035989432498, "compression_ratio": 1.6991869918699187, "no_speech_prob": 7.6462720244308e-06}, {"id": 320, "seek": 122324, "start": 1248.2, "end": 1252.84, "text": " to make these into nice kind of batches, these 2D blocks", "tokens": [281, 652, 613, 666, 1481, 733, 295, 15245, 279, 11, 613, 568, 35, 8474], "temperature": 0.0, "avg_logprob": -0.13302035989432498, "compression_ratio": 1.6991869918699187, "no_speech_prob": 7.6462720244308e-06}, {"id": 321, "seek": 125284, "start": 1252.84, "end": 1258.56, "text": " that we can parallelize well in terms of putting on the GPU", "tokens": [300, 321, 393, 8952, 1125, 731, 294, 2115, 295, 3372, 322, 264, 18407], "temperature": 0.0, "avg_logprob": -0.09406564500596788, "compression_ratio": 1.641255605381166, "no_speech_prob": 2.8855127311544493e-05}, {"id": 322, "seek": 125284, "start": 1258.56, "end": 1263.12, "text": " and using them to calculate our weights simultaneously.", "tokens": [293, 1228, 552, 281, 8873, 527, 17443, 16561, 13], "temperature": 0.0, "avg_logprob": -0.09406564500596788, "compression_ratio": 1.641255605381166, "no_speech_prob": 2.8855127311544493e-05}, {"id": 323, "seek": 125284, "start": 1263.12, "end": 1267.8799999999999, "text": " So exact same before, kind of finding the max length of x,", "tokens": [407, 1900, 912, 949, 11, 733, 295, 5006, 264, 11469, 4641, 295, 2031, 11], "temperature": 0.0, "avg_logprob": -0.09406564500596788, "compression_ratio": 1.641255605381166, "no_speech_prob": 2.8855127311544493e-05}, {"id": 324, "seek": 125284, "start": 1267.8799999999999, "end": 1273.1599999999999, "text": " the max length of y, so our English sentences and our French sentences.", "tokens": [264, 11469, 4641, 295, 288, 11, 370, 527, 3669, 16579, 293, 527, 5522, 16579, 13], "temperature": 0.0, "avg_logprob": -0.09406564500596788, "compression_ratio": 1.641255605381166, "no_speech_prob": 2.8855127311544493e-05}, {"id": 325, "seek": 125284, "start": 1273.1599999999999, "end": 1277.04, "text": " Here, we're doing some broadcasting, initializing these tensors", "tokens": [1692, 11, 321, 434, 884, 512, 30024, 11, 5883, 3319, 613, 10688, 830], "temperature": 0.0, "avg_logprob": -0.09406564500596788, "compression_ratio": 1.641255605381166, "no_speech_prob": 2.8855127311544493e-05}, {"id": 326, "seek": 125284, "start": 1277.04, "end": 1281.08, "text": " to be 64 by the max length, and initializing everything", "tokens": [281, 312, 12145, 538, 264, 11469, 4641, 11, 293, 5883, 3319, 1203], "temperature": 0.0, "avg_logprob": -0.09406564500596788, "compression_ratio": 1.641255605381166, "no_speech_prob": 2.8855127311544493e-05}, {"id": 327, "seek": 128108, "start": 1281.08, "end": 1285.6399999999999, "text": " to the padding ID, and then based on whether we're putting the padding first", "tokens": [281, 264, 39562, 7348, 11, 293, 550, 2361, 322, 1968, 321, 434, 3372, 264, 39562, 700], "temperature": 0.0, "avg_logprob": -0.17669288763839208, "compression_ratio": 1.6714285714285715, "no_speech_prob": 1.544530641695019e-05}, {"id": 328, "seek": 128108, "start": 1285.6399999999999, "end": 1291.48, "text": " or second, filling in the sentences at the beginning or from the end.", "tokens": [420, 1150, 11, 10623, 294, 264, 16579, 412, 264, 2863, 420, 490, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.17669288763839208, "compression_ratio": 1.6714285714285715, "no_speech_prob": 1.544530641695019e-05}, {"id": 329, "seek": 128108, "start": 1291.48, "end": 1300.24, "text": " And that actually, probably should have showed that over here as well.", "tokens": [400, 300, 767, 11, 1391, 820, 362, 4712, 300, 670, 510, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.17669288763839208, "compression_ratio": 1.6714285714285715, "no_speech_prob": 1.544530641695019e-05}, {"id": 330, "seek": 128108, "start": 1300.24, "end": 1303.48, "text": " Filling in padding from the end, what that would look like is,", "tokens": [479, 7345, 294, 39562, 490, 264, 917, 11, 437, 300, 576, 574, 411, 307, 11], "temperature": 0.0, "avg_logprob": -0.17669288763839208, "compression_ratio": 1.6714285714285715, "no_speech_prob": 1.544530641695019e-05}, {"id": 331, "seek": 128108, "start": 1303.48, "end": 1309.4399999999998, "text": " so say I was doing max length of 6, really that would just be putting,", "tokens": [370, 584, 286, 390, 884, 11469, 4641, 295, 1386, 11, 534, 300, 576, 445, 312, 3372, 11], "temperature": 0.0, "avg_logprob": -0.17669288763839208, "compression_ratio": 1.6714285714285715, "no_speech_prob": 1.544530641695019e-05}, {"id": 332, "seek": 130944, "start": 1309.44, "end": 1316.76, "text": " she ran at kind of the right hand side and all the padding before it.", "tokens": [750, 5872, 412, 733, 295, 264, 558, 1011, 1252, 293, 439, 264, 39562, 949, 309, 13], "temperature": 0.0, "avg_logprob": -0.3248215415261009, "compression_ratio": 1.4961832061068703, "no_speech_prob": 4.8321155190933496e-05}, {"id": 333, "seek": 130944, "start": 1316.76, "end": 1327.16, "text": " So this is padding first down here, and the ones at top are the padding", "tokens": [407, 341, 307, 39562, 700, 760, 510, 11, 293, 264, 2306, 412, 1192, 366, 264, 39562], "temperature": 0.0, "avg_logprob": -0.3248215415261009, "compression_ratio": 1.4961832061068703, "no_speech_prob": 4.8321155190933496e-05}, {"id": 334, "seek": 130944, "start": 1327.16, "end": 1329.48, "text": " at the end.", "tokens": [412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.3248215415261009, "compression_ratio": 1.4961832061068703, "no_speech_prob": 4.8321155190933496e-05}, {"id": 335, "seek": 132948, "start": 1329.48, "end": 1340.64, "text": " Create our data bunch, the same as before.", "tokens": [20248, 527, 1412, 3840, 11, 264, 912, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.183326550384066, "compression_ratio": 1.4642857142857142, "no_speech_prob": 3.535326322889887e-05}, {"id": 336, "seek": 132948, "start": 1340.64, "end": 1343.64, "text": " Here, this is nice to see some examples of the text,", "tokens": [1692, 11, 341, 307, 1481, 281, 536, 512, 5110, 295, 264, 2487, 11], "temperature": 0.0, "avg_logprob": -0.183326550384066, "compression_ratio": 1.4642857142857142, "no_speech_prob": 3.535326322889887e-05}, {"id": 337, "seek": 132948, "start": 1343.64, "end": 1350.96, "text": " so we've got questions in French, and then the target is the English", "tokens": [370, 321, 600, 658, 1651, 294, 5522, 11, 293, 550, 264, 3779, 307, 264, 3669], "temperature": 0.0, "avg_logprob": -0.183326550384066, "compression_ratio": 1.4642857142857142, "no_speech_prob": 3.535326322889887e-05}, {"id": 338, "seek": 132948, "start": 1350.96, "end": 1354.0, "text": " translation that we're trying to get.", "tokens": [12853, 300, 321, 434, 1382, 281, 483, 13], "temperature": 0.0, "avg_logprob": -0.183326550384066, "compression_ratio": 1.4642857142857142, "no_speech_prob": 3.535326322889887e-05}, {"id": 339, "seek": 132948, "start": 1354.0, "end": 1359.08, "text": " This is a picture of the transformer model.", "tokens": [639, 307, 257, 3036, 295, 264, 31782, 2316, 13], "temperature": 0.0, "avg_logprob": -0.183326550384066, "compression_ratio": 1.4642857142857142, "no_speech_prob": 3.535326322889887e-05}, {"id": 340, "seek": 135908, "start": 1359.08, "end": 1363.0, "text": " This comes from the attention is all you need paper.", "tokens": [639, 1487, 490, 264, 3202, 307, 439, 291, 643, 3035, 13], "temperature": 0.0, "avg_logprob": -0.15925403041694, "compression_ratio": 1.582142857142857, "no_speech_prob": 6.811894854763523e-05}, {"id": 341, "seek": 135908, "start": 1363.0, "end": 1364.4399999999998, "text": " And actually, I skipped over this.", "tokens": [400, 767, 11, 286, 30193, 670, 341, 13], "temperature": 0.0, "avg_logprob": -0.15925403041694, "compression_ratio": 1.582142857142857, "no_speech_prob": 6.811894854763523e-05}, {"id": 342, "seek": 135908, "start": 1364.4399999999998, "end": 1369.96, "text": " I should just note at the top, I linked to a nice blog post from Chip Hewin,", "tokens": [286, 820, 445, 3637, 412, 264, 1192, 11, 286, 9408, 281, 257, 1481, 6968, 2183, 490, 29751, 634, 86, 259, 11], "temperature": 0.0, "avg_logprob": -0.15925403041694, "compression_ratio": 1.582142857142857, "no_speech_prob": 6.811894854763523e-05}, {"id": 343, "seek": 135908, "start": 1369.96, "end": 1374.6399999999999, "text": " who's a researcher at NVIDIA, and she wrote a write up on top 8 trends", "tokens": [567, 311, 257, 21751, 412, 426, 3958, 6914, 11, 293, 750, 4114, 257, 2464, 493, 322, 1192, 1649, 13892], "temperature": 0.0, "avg_logprob": -0.15925403041694, "compression_ratio": 1.582142857142857, "no_speech_prob": 6.811894854763523e-05}, {"id": 344, "seek": 135908, "start": 1374.6399999999999, "end": 1378.8799999999999, "text": " from ICLR this year, which was just, I don't know, I think two months ago.", "tokens": [490, 14360, 31722, 341, 1064, 11, 597, 390, 445, 11, 286, 500, 380, 458, 11, 286, 519, 732, 2493, 2057, 13], "temperature": 0.0, "avg_logprob": -0.15925403041694, "compression_ratio": 1.582142857142857, "no_speech_prob": 6.811894854763523e-05}, {"id": 345, "seek": 135908, "start": 1378.8799999999999, "end": 1382.4399999999998, "text": " And one of the trends she cited was that RNNs are losing luster", "tokens": [400, 472, 295, 264, 13892, 750, 30134, 390, 300, 45702, 45, 82, 366, 7027, 287, 8393], "temperature": 0.0, "avg_logprob": -0.15925403041694, "compression_ratio": 1.582142857142857, "no_speech_prob": 6.811894854763523e-05}, {"id": 346, "seek": 135908, "start": 1382.4399999999998, "end": 1383.36, "text": " with researchers.", "tokens": [365, 10309, 13], "temperature": 0.0, "avg_logprob": -0.15925403041694, "compression_ratio": 1.582142857142857, "no_speech_prob": 6.811894854763523e-05}, {"id": 347, "seek": 135908, "start": 1383.36, "end": 1387.32, "text": " She saw kind of a real decrease in papers on RNNs.", "tokens": [1240, 1866, 733, 295, 257, 957, 11514, 294, 10577, 322, 45702, 45, 82, 13], "temperature": 0.0, "avg_logprob": -0.15925403041694, "compression_ratio": 1.582142857142857, "no_speech_prob": 6.811894854763523e-05}, {"id": 348, "seek": 138732, "start": 1387.32, "end": 1392.12, "text": " However, this is still very much an open question of research is kind of coming", "tokens": [2908, 11, 341, 307, 920, 588, 709, 364, 1269, 1168, 295, 2132, 307, 733, 295, 1348], "temperature": 0.0, "avg_logprob": -0.13765668060819983, "compression_ratio": 1.7107142857142856, "no_speech_prob": 6.203595694387332e-05}, {"id": 349, "seek": 138732, "start": 1392.12, "end": 1395.8799999999999, "text": " out all the time, and it's unclear kind of what's going to end up being best", "tokens": [484, 439, 264, 565, 11, 293, 309, 311, 25636, 733, 295, 437, 311, 516, 281, 917, 493, 885, 1151], "temperature": 0.0, "avg_logprob": -0.13765668060819983, "compression_ratio": 1.7107142857142856, "no_speech_prob": 6.203595694387332e-05}, {"id": 350, "seek": 138732, "start": 1395.8799999999999, "end": 1401.08, "text": " in terms of attention versus RNNs versus CNNs versus, you know,", "tokens": [294, 2115, 295, 3202, 5717, 45702, 45, 82, 5717, 24859, 82, 5717, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.13765668060819983, "compression_ratio": 1.7107142857142856, "no_speech_prob": 6.203595694387332e-05}, {"id": 351, "seek": 138732, "start": 1401.08, "end": 1404.32, "text": " are there ways to combine aspects of those.", "tokens": [366, 456, 2098, 281, 10432, 7270, 295, 729, 13], "temperature": 0.0, "avg_logprob": -0.13765668060819983, "compression_ratio": 1.7107142857142856, "no_speech_prob": 6.203595694387332e-05}, {"id": 352, "seek": 138732, "start": 1404.32, "end": 1407.36, "text": " And I think we mentioned last time, there was a paper that came out earlier", "tokens": [400, 286, 519, 321, 2835, 1036, 565, 11, 456, 390, 257, 3035, 300, 1361, 484, 3071], "temperature": 0.0, "avg_logprob": -0.13765668060819983, "compression_ratio": 1.7107142857142856, "no_speech_prob": 6.203595694387332e-05}, {"id": 353, "seek": 138732, "start": 1407.36, "end": 1413.12, "text": " this year, pay less attention with lightweight and dynamic convolutions.", "tokens": [341, 1064, 11, 1689, 1570, 3202, 365, 22052, 293, 8546, 3754, 15892, 13], "temperature": 0.0, "avg_logprob": -0.13765668060819983, "compression_ratio": 1.7107142857142856, "no_speech_prob": 6.203595694387332e-05}, {"id": 354, "seek": 138732, "start": 1413.12, "end": 1416.04, "text": " Kind of in their research, they showed convolutions outperforming", "tokens": [9242, 295, 294, 641, 2132, 11, 436, 4712, 3754, 15892, 484, 26765, 278], "temperature": 0.0, "avg_logprob": -0.13765668060819983, "compression_ratio": 1.7107142857142856, "no_speech_prob": 6.203595694387332e-05}, {"id": 355, "seek": 141604, "start": 1416.04, "end": 1421.36, "text": " attention, so this is still very much an area of active research.", "tokens": [3202, 11, 370, 341, 307, 920, 588, 709, 364, 1859, 295, 4967, 2132, 13], "temperature": 0.0, "avg_logprob": -0.17503066201811857, "compression_ratio": 1.6835443037974684, "no_speech_prob": 8.939328836277127e-06}, {"id": 356, "seek": 141604, "start": 1421.36, "end": 1426.6, "text": " But so back down to kind of the original transformer model.", "tokens": [583, 370, 646, 760, 281, 733, 295, 264, 3380, 31782, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17503066201811857, "compression_ratio": 1.6835443037974684, "no_speech_prob": 8.939328836277127e-06}, {"id": 357, "seek": 141604, "start": 1426.6, "end": 1430.72, "text": " So it involves our input embeddings, and this is true of I think probably any", "tokens": [407, 309, 11626, 527, 4846, 12240, 29432, 11, 293, 341, 307, 2074, 295, 286, 519, 1391, 604], "temperature": 0.0, "avg_logprob": -0.17503066201811857, "compression_ratio": 1.6835443037974684, "no_speech_prob": 8.939328836277127e-06}, {"id": 358, "seek": 141604, "start": 1430.72, "end": 1433.48, "text": " seek to seek is starting with input embeddings,", "tokens": [8075, 281, 8075, 307, 2891, 365, 4846, 12240, 29432, 11], "temperature": 0.0, "avg_logprob": -0.17503066201811857, "compression_ratio": 1.6835443037974684, "no_speech_prob": 8.939328836277127e-06}, {"id": 359, "seek": 141604, "start": 1433.48, "end": 1435.3999999999999, "text": " ending with output embeddings.", "tokens": [8121, 365, 5598, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.17503066201811857, "compression_ratio": 1.6835443037974684, "no_speech_prob": 8.939328836277127e-06}, {"id": 360, "seek": 141604, "start": 1435.3999999999999, "end": 1439.68, "text": " But here in the meantime, or kind of in between,", "tokens": [583, 510, 294, 264, 14991, 11, 420, 733, 295, 294, 1296, 11], "temperature": 0.0, "avg_logprob": -0.17503066201811857, "compression_ratio": 1.6835443037974684, "no_speech_prob": 8.939328836277127e-06}, {"id": 361, "seek": 141604, "start": 1439.68, "end": 1444.3999999999999, "text": " you have multi-head attention showing up in three places, and it's,", "tokens": [291, 362, 4825, 12, 1934, 3202, 4099, 493, 294, 1045, 3190, 11, 293, 309, 311, 11], "temperature": 0.0, "avg_logprob": -0.17503066201811857, "compression_ratio": 1.6835443037974684, "no_speech_prob": 8.939328836277127e-06}, {"id": 362, "seek": 144440, "start": 1444.4, "end": 1450.2, "text": " there's encoder to encoder attention, decoder to decoder attention,", "tokens": [456, 311, 2058, 19866, 281, 2058, 19866, 3202, 11, 979, 19866, 281, 979, 19866, 3202, 11], "temperature": 0.0, "avg_logprob": -0.1374420065628855, "compression_ratio": 1.799043062200957, "no_speech_prob": 6.814231892349198e-05}, {"id": 363, "seek": 144440, "start": 1450.2, "end": 1455.52, "text": " and then encoder to decoder attention of where to focus.", "tokens": [293, 550, 2058, 19866, 281, 979, 19866, 3202, 295, 689, 281, 1879, 13], "temperature": 0.0, "avg_logprob": -0.1374420065628855, "compression_ratio": 1.799043062200957, "no_speech_prob": 6.814231892349198e-05}, {"id": 364, "seek": 144440, "start": 1455.52, "end": 1458.6000000000001, "text": " And we have these feed forward blocks, which you'll see in a moment,", "tokens": [400, 321, 362, 613, 3154, 2128, 8474, 11, 597, 291, 603, 536, 294, 257, 1623, 11], "temperature": 0.0, "avg_logprob": -0.1374420065628855, "compression_ratio": 1.799043062200957, "no_speech_prob": 6.814231892349198e-05}, {"id": 365, "seek": 144440, "start": 1458.6000000000001, "end": 1463.8000000000002, "text": " which is kind of your traditional fully connected neural network.", "tokens": [597, 307, 733, 295, 428, 5164, 4498, 4582, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.1374420065628855, "compression_ratio": 1.799043062200957, "no_speech_prob": 6.814231892349198e-05}, {"id": 366, "seek": 144440, "start": 1463.8000000000002, "end": 1469.76, "text": " Adding and norming is kind of our non-linearities going between these.", "tokens": [31204, 293, 2026, 278, 307, 733, 295, 527, 2107, 12, 28263, 1088, 516, 1296, 613, 13], "temperature": 0.0, "avg_logprob": -0.1374420065628855, "compression_ratio": 1.799043062200957, "no_speech_prob": 6.814231892349198e-05}, {"id": 367, "seek": 144440, "start": 1469.76, "end": 1473.4, "text": " And then these blocks are repeated six times.", "tokens": [400, 550, 613, 8474, 366, 10477, 2309, 1413, 13], "temperature": 0.0, "avg_logprob": -0.1374420065628855, "compression_ratio": 1.799043062200957, "no_speech_prob": 6.814231892349198e-05}, {"id": 368, "seek": 147340, "start": 1473.4, "end": 1476.64, "text": " That's that nx over here, n is six.", "tokens": [663, 311, 300, 297, 87, 670, 510, 11, 297, 307, 2309, 13], "temperature": 0.0, "avg_logprob": -0.2780141035715739, "compression_ratio": 1.4720496894409938, "no_speech_prob": 2.5070521587622352e-05}, {"id": 369, "seek": 147340, "start": 1483.16, "end": 1486.5600000000002, "text": " So looking at the code, so there's kind of a,", "tokens": [407, 1237, 412, 264, 3089, 11, 370, 456, 311, 733, 295, 257, 11], "temperature": 0.0, "avg_logprob": -0.2780141035715739, "compression_ratio": 1.4720496894409938, "no_speech_prob": 2.5070521587622352e-05}, {"id": 370, "seek": 147340, "start": 1486.5600000000002, "end": 1492.5600000000002, "text": " we have to shift the target over one just since we're predicting the next step.", "tokens": [321, 362, 281, 5513, 264, 3779, 670, 472, 445, 1670, 321, 434, 32884, 264, 958, 1823, 13], "temperature": 0.0, "avg_logprob": -0.2780141035715739, "compression_ratio": 1.4720496894409938, "no_speech_prob": 2.5070521587622352e-05}, {"id": 371, "seek": 147340, "start": 1495.3200000000002, "end": 1501.68, "text": " For the encoding, sorry, for the embeddings, so RNNs have a concept of time", "tokens": [1171, 264, 43430, 11, 2597, 11, 337, 264, 12240, 29432, 11, 370, 45702, 45, 82, 362, 257, 3410, 295, 565], "temperature": 0.0, "avg_logprob": -0.2780141035715739, "compression_ratio": 1.4720496894409938, "no_speech_prob": 2.5070521587622352e-05}, {"id": 372, "seek": 150168, "start": 1501.68, "end": 1506.28, "text": " since you're passing things in sequentially at each time step,", "tokens": [1670, 291, 434, 8437, 721, 294, 5123, 3137, 412, 1184, 565, 1823, 11], "temperature": 0.0, "avg_logprob": -0.1509223138132403, "compression_ratio": 1.6682242990654206, "no_speech_prob": 2.3922553737065755e-05}, {"id": 373, "seek": 150168, "start": 1506.28, "end": 1509.52, "text": " you get the next word in the sentence.", "tokens": [291, 483, 264, 958, 1349, 294, 264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.1509223138132403, "compression_ratio": 1.6682242990654206, "no_speech_prob": 2.3922553737065755e-05}, {"id": 374, "seek": 150168, "start": 1509.52, "end": 1513.64, "text": " And that's not the case with attention because you're kind of giving it", "tokens": [400, 300, 311, 406, 264, 1389, 365, 3202, 570, 291, 434, 733, 295, 2902, 309], "temperature": 0.0, "avg_logprob": -0.1509223138132403, "compression_ratio": 1.6682242990654206, "no_speech_prob": 2.3922553737065755e-05}, {"id": 375, "seek": 150168, "start": 1513.64, "end": 1518.64, "text": " everything at once and there's no notion of order there.", "tokens": [1203, 412, 1564, 293, 456, 311, 572, 10710, 295, 1668, 456, 13], "temperature": 0.0, "avg_logprob": -0.1509223138132403, "compression_ratio": 1.6682242990654206, "no_speech_prob": 2.3922553737065755e-05}, {"id": 376, "seek": 150168, "start": 1518.64, "end": 1524.6000000000001, "text": " And so positional encoding is a way to address this.", "tokens": [400, 370, 2535, 304, 43430, 307, 257, 636, 281, 2985, 341, 13], "temperature": 0.0, "avg_logprob": -0.1509223138132403, "compression_ratio": 1.6682242990654206, "no_speech_prob": 2.3922553737065755e-05}, {"id": 377, "seek": 150168, "start": 1524.6000000000001, "end": 1528.64, "text": " And basically that's to, actually Jay Alomar had a great picture of this.", "tokens": [400, 1936, 300, 311, 281, 11, 767, 11146, 967, 298, 289, 632, 257, 869, 3036, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.1509223138132403, "compression_ratio": 1.6682242990654206, "no_speech_prob": 2.3922553737065755e-05}, {"id": 378, "seek": 152864, "start": 1528.64, "end": 1532.5600000000002, "text": " So let me go to that later on.", "tokens": [407, 718, 385, 352, 281, 300, 1780, 322, 13], "temperature": 0.0, "avg_logprob": -0.21150408293071546, "compression_ratio": 1.5287958115183247, "no_speech_prob": 9.027432679431513e-05}, {"id": 379, "seek": 152864, "start": 1532.5600000000002, "end": 1535.5200000000002, "text": " We'll come back to some of the earlier pieces.", "tokens": [492, 603, 808, 646, 281, 512, 295, 264, 3071, 3755, 13], "temperature": 0.0, "avg_logprob": -0.21150408293071546, "compression_ratio": 1.5287958115183247, "no_speech_prob": 9.027432679431513e-05}, {"id": 380, "seek": 152864, "start": 1535.5200000000002, "end": 1538.0800000000002, "text": " But basically we kind of use sine and", "tokens": [583, 1936, 321, 733, 295, 764, 18609, 293], "temperature": 0.0, "avg_logprob": -0.21150408293071546, "compression_ratio": 1.5287958115183247, "no_speech_prob": 9.027432679431513e-05}, {"id": 381, "seek": 152864, "start": 1538.0800000000002, "end": 1546.16, "text": " cosine to come up with this matrix that captures position.", "tokens": [23565, 281, 808, 493, 365, 341, 8141, 300, 27986, 2535, 13], "temperature": 0.0, "avg_logprob": -0.21150408293071546, "compression_ratio": 1.5287958115183247, "no_speech_prob": 9.027432679431513e-05}, {"id": 382, "seek": 152864, "start": 1546.16, "end": 1550.68, "text": " Cuz there's kind of like a different value at each position and", "tokens": [27017, 456, 311, 733, 295, 411, 257, 819, 2158, 412, 1184, 2535, 293], "temperature": 0.0, "avg_logprob": -0.21150408293071546, "compression_ratio": 1.5287958115183247, "no_speech_prob": 9.027432679431513e-05}, {"id": 383, "seek": 152864, "start": 1550.68, "end": 1557.0800000000002, "text": " you've got these patterns across your two directions.", "tokens": [291, 600, 658, 613, 8294, 2108, 428, 732, 11095, 13], "temperature": 0.0, "avg_logprob": -0.21150408293071546, "compression_ratio": 1.5287958115183247, "no_speech_prob": 9.027432679431513e-05}, {"id": 384, "seek": 155708, "start": 1557.08, "end": 1562.36, "text": " And so this is added to the input to encode the position.", "tokens": [400, 370, 341, 307, 3869, 281, 264, 4846, 281, 2058, 1429, 264, 2535, 13], "temperature": 0.0, "avg_logprob": -0.25145038926457786, "compression_ratio": 1.6162162162162161, "no_speech_prob": 1.670104575168807e-05}, {"id": 385, "seek": 155708, "start": 1562.36, "end": 1566.72, "text": " And so this is, I'll show you the formula in a moment, but", "tokens": [400, 370, 341, 307, 11, 286, 603, 855, 291, 264, 8513, 294, 257, 1623, 11, 457], "temperature": 0.0, "avg_logprob": -0.25145038926457786, "compression_ratio": 1.6162162162162161, "no_speech_prob": 1.670104575168807e-05}, {"id": 386, "seek": 155708, "start": 1566.72, "end": 1569.8799999999999, "text": " this is kind of what it looks like is adding this sine and", "tokens": [341, 307, 733, 295, 437, 309, 1542, 411, 307, 5127, 341, 18609, 293], "temperature": 0.0, "avg_logprob": -0.25145038926457786, "compression_ratio": 1.6162162162162161, "no_speech_prob": 1.670104575168807e-05}, {"id": 387, "seek": 155708, "start": 1569.8799999999999, "end": 1576.1999999999998, "text": " cosine that is both varying here across the x and across the y.", "tokens": [23565, 300, 307, 1293, 22984, 510, 2108, 264, 2031, 293, 2108, 264, 288, 13], "temperature": 0.0, "avg_logprob": -0.25145038926457786, "compression_ratio": 1.6162162162162161, "no_speech_prob": 1.670104575168807e-05}, {"id": 388, "seek": 155708, "start": 1578.6799999999998, "end": 1583.08, "text": " Whereas, I mean this would be like a batch of sequences, or", "tokens": [13813, 11, 286, 914, 341, 576, 312, 411, 257, 15245, 295, 22978, 11, 420], "temperature": 0.0, "avg_logprob": -0.25145038926457786, "compression_ratio": 1.6162162162162161, "no_speech_prob": 1.670104575168807e-05}, {"id": 389, "seek": 158308, "start": 1583.08, "end": 1587.1999999999998, "text": " not a batch of sequences, yeah, but like a batch of your sentences.", "tokens": [406, 257, 15245, 295, 22978, 11, 1338, 11, 457, 411, 257, 15245, 295, 428, 16579, 13], "temperature": 0.0, "avg_logprob": -0.35175556331485897, "compression_ratio": 1.478494623655914, "no_speech_prob": 3.480606392258778e-05}, {"id": 390, "seek": 158308, "start": 1590.3999999999999, "end": 1594.96, "text": " And so what that looks like is this formula of,", "tokens": [400, 370, 437, 300, 1542, 411, 307, 341, 8513, 295, 11], "temperature": 0.0, "avg_logprob": -0.35175556331485897, "compression_ratio": 1.478494623655914, "no_speech_prob": 3.480606392258778e-05}, {"id": 391, "seek": 158308, "start": 1594.96, "end": 1599.52, "text": " will you take one over a thousand raised to, and", "tokens": [486, 291, 747, 472, 670, 257, 4714, 6005, 281, 11, 293], "temperature": 0.0, "avg_logprob": -0.35175556331485897, "compression_ratio": 1.478494623655914, "no_speech_prob": 3.480606392258778e-05}, {"id": 392, "seek": 158308, "start": 1599.52, "end": 1605.6799999999998, "text": " this can be rewritten as, do you know Lin space in NumPy?", "tokens": [341, 393, 312, 319, 26859, 382, 11, 360, 291, 458, 9355, 1901, 294, 22592, 47, 88, 30], "temperature": 0.0, "avg_logprob": -0.35175556331485897, "compression_ratio": 1.478494623655914, "no_speech_prob": 3.480606392258778e-05}, {"id": 393, "seek": 158308, "start": 1605.6799999999998, "end": 1610.48, "text": " Where you're just getting these evenly spaced steps.", "tokens": [2305, 291, 434, 445, 1242, 613, 17658, 43766, 4439, 13], "temperature": 0.0, "avg_logprob": -0.35175556331485897, "compression_ratio": 1.478494623655914, "no_speech_prob": 3.480606392258778e-05}, {"id": 394, "seek": 161048, "start": 1610.48, "end": 1615.16, "text": " So here you're taking steps of size, I guess, 2 over d,", "tokens": [407, 510, 291, 434, 1940, 4439, 295, 2744, 11, 286, 2041, 11, 568, 670, 274, 11], "temperature": 0.0, "avg_logprob": -0.2773466472384296, "compression_ratio": 1.4883720930232558, "no_speech_prob": 2.1111382011440583e-05}, {"id": 395, "seek": 161048, "start": 1615.16, "end": 1622.2, "text": " d is the dimension, and you're going from 0 to 1.", "tokens": [274, 307, 264, 10139, 11, 293, 291, 434, 516, 490, 1958, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.2773466472384296, "compression_ratio": 1.4883720930232558, "no_speech_prob": 2.1111382011440583e-05}, {"id": 396, "seek": 161048, "start": 1622.2, "end": 1627.68, "text": " And actually we, I refactored this, but", "tokens": [400, 767, 321, 11, 286, 1895, 578, 2769, 341, 11, 457], "temperature": 0.0, "avg_logprob": -0.2773466472384296, "compression_ratio": 1.4883720930232558, "no_speech_prob": 2.1111382011440583e-05}, {"id": 397, "seek": 161048, "start": 1627.68, "end": 1632.0, "text": " that must be in a different notebook, but it's steps from,", "tokens": [300, 1633, 312, 294, 257, 819, 21060, 11, 457, 309, 311, 4439, 490, 11], "temperature": 0.0, "avg_logprob": -0.2773466472384296, "compression_ratio": 1.4883720930232558, "no_speech_prob": 2.1111382011440583e-05}, {"id": 398, "seek": 163200, "start": 1632.0, "end": 1642.52, "text": " actually I think we can see this if we look at, Let me plug in.", "tokens": [767, 286, 519, 321, 393, 536, 341, 498, 321, 574, 412, 11, 961, 385, 5452, 294, 13], "temperature": 0.0, "avg_logprob": -0.2886917052730437, "compression_ratio": 1.403973509933775, "no_speech_prob": 4.399982208269648e-05}, {"id": 399, "seek": 163200, "start": 1646.52, "end": 1651.08, "text": " I'll do just d equals 30 so you can see, but this idea of kind of going up by", "tokens": [286, 603, 360, 445, 274, 6915, 2217, 370, 291, 393, 536, 11, 457, 341, 1558, 295, 733, 295, 516, 493, 538], "temperature": 0.0, "avg_logprob": -0.2886917052730437, "compression_ratio": 1.403973509933775, "no_speech_prob": 4.399982208269648e-05}, {"id": 400, "seek": 163200, "start": 1651.08, "end": 1656.6, "text": " evenly spaced steps to get between 0 and 1 is what this is outputting.", "tokens": [17658, 43766, 4439, 281, 483, 1296, 1958, 293, 502, 307, 437, 341, 307, 5598, 783, 13], "temperature": 0.0, "avg_logprob": -0.2886917052730437, "compression_ratio": 1.403973509933775, "no_speech_prob": 4.399982208269648e-05}, {"id": 401, "seek": 165660, "start": 1656.6, "end": 1662.08, "text": " And then we are applying sine to that and applying cosine to that.", "tokens": [400, 550, 321, 366, 9275, 18609, 281, 300, 293, 9275, 23565, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.2934952480037038, "compression_ratio": 1.5888888888888888, "no_speech_prob": 2.5070650735870004e-05}, {"id": 402, "seek": 165660, "start": 1662.08, "end": 1665.6799999999998, "text": " And so this is another way to picture it.", "tokens": [400, 370, 341, 307, 1071, 636, 281, 3036, 309, 13], "temperature": 0.0, "avg_logprob": -0.2934952480037038, "compression_ratio": 1.5888888888888888, "no_speech_prob": 2.5070650735870004e-05}, {"id": 403, "seek": 165660, "start": 1665.6799999999998, "end": 1668.08, "text": " Here we're plotting, so the result.", "tokens": [1692, 321, 434, 41178, 11, 370, 264, 1874, 13], "temperature": 0.0, "avg_logprob": -0.2934952480037038, "compression_ratio": 1.5888888888888888, "no_speech_prob": 2.5070650735870004e-05}, {"id": 404, "seek": 165660, "start": 1673.6799999999998, "end": 1675.08, "text": " Actually, I already have it.", "tokens": [5135, 11, 286, 1217, 362, 309, 13], "temperature": 0.0, "avg_logprob": -0.2934952480037038, "compression_ratio": 1.5888888888888888, "no_speech_prob": 2.5070650735870004e-05}, {"id": 405, "seek": 165660, "start": 1676.56, "end": 1678.9199999999998, "text": " This is what the result looks like.", "tokens": [639, 307, 437, 264, 1874, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.2934952480037038, "compression_ratio": 1.5888888888888888, "no_speech_prob": 2.5070650735870004e-05}, {"id": 406, "seek": 165660, "start": 1678.9199999999998, "end": 1685.52, "text": " So we've got zeros on the first row, and then it's kind of varying as you go", "tokens": [407, 321, 600, 658, 35193, 322, 264, 700, 5386, 11, 293, 550, 309, 311, 733, 295, 22984, 382, 291, 352], "temperature": 0.0, "avg_logprob": -0.2934952480037038, "compression_ratio": 1.5888888888888888, "no_speech_prob": 2.5070650735870004e-05}, {"id": 407, "seek": 168552, "start": 1685.52, "end": 1689.28, "text": " along this dimension and along this dimension.", "tokens": [2051, 341, 10139, 293, 2051, 341, 10139, 13], "temperature": 0.0, "avg_logprob": -0.2119096192446622, "compression_ratio": 1.6308411214953271, "no_speech_prob": 2.66871320491191e-05}, {"id": 408, "seek": 168552, "start": 1689.28, "end": 1693.44, "text": " But this is how we get around the fact that we haven't captured our sense of", "tokens": [583, 341, 307, 577, 321, 483, 926, 264, 1186, 300, 321, 2378, 380, 11828, 527, 2020, 295], "temperature": 0.0, "avg_logprob": -0.2119096192446622, "compression_ratio": 1.6308411214953271, "no_speech_prob": 2.66871320491191e-05}, {"id": 409, "seek": 168552, "start": 1693.44, "end": 1697.12, "text": " time by putting everything into the network at once.", "tokens": [565, 538, 3372, 1203, 666, 264, 3209, 412, 1564, 13], "temperature": 0.0, "avg_logprob": -0.2119096192446622, "compression_ratio": 1.6308411214953271, "no_speech_prob": 2.66871320491191e-05}, {"id": 410, "seek": 168552, "start": 1701.4, "end": 1704.84, "text": " And so we use this to create a transformer embedding,", "tokens": [400, 370, 321, 764, 341, 281, 1884, 257, 31782, 12240, 3584, 11], "temperature": 0.0, "avg_logprob": -0.2119096192446622, "compression_ratio": 1.6308411214953271, "no_speech_prob": 2.66871320491191e-05}, {"id": 411, "seek": 168552, "start": 1704.84, "end": 1707.72, "text": " which is basically taking our normal embedding and", "tokens": [597, 307, 1936, 1940, 527, 2710, 12240, 3584, 293], "temperature": 0.0, "avg_logprob": -0.2119096192446622, "compression_ratio": 1.6308411214953271, "no_speech_prob": 2.66871320491191e-05}, {"id": 412, "seek": 168552, "start": 1707.72, "end": 1710.2, "text": " putting that into the positional encoder.", "tokens": [3372, 300, 666, 264, 2535, 304, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.2119096192446622, "compression_ratio": 1.6308411214953271, "no_speech_prob": 2.66871320491191e-05}, {"id": 413, "seek": 168552, "start": 1710.2, "end": 1711.84, "text": " We're also using dropout.", "tokens": [492, 434, 611, 1228, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.2119096192446622, "compression_ratio": 1.6308411214953271, "no_speech_prob": 2.66871320491191e-05}, {"id": 414, "seek": 171184, "start": 1711.84, "end": 1716.52, "text": " And then here there's some sort of normalization going on where you're", "tokens": [400, 550, 510, 456, 311, 512, 1333, 295, 2710, 2144, 516, 322, 689, 291, 434], "temperature": 0.0, "avg_logprob": -0.36375374629579743, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.747840870957589e-06}, {"id": 415, "seek": 171184, "start": 1716.52, "end": 1720.52, "text": " multiplying by the square root of the embedding size.", "tokens": [30955, 538, 264, 3732, 5593, 295, 264, 12240, 3584, 2744, 13], "temperature": 0.0, "avg_logprob": -0.36375374629579743, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.747840870957589e-06}, {"id": 416, "seek": 171184, "start": 1720.52, "end": 1725.28, "text": " And I think that's to give this embedding kind of more proportional weight", "tokens": [400, 286, 519, 300, 311, 281, 976, 341, 12240, 3584, 733, 295, 544, 24969, 3364], "temperature": 0.0, "avg_logprob": -0.36375374629579743, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.747840870957589e-06}, {"id": 417, "seek": 171184, "start": 1725.28, "end": 1727.76, "text": " compared to the positional encoding.", "tokens": [5347, 281, 264, 2535, 304, 43430, 13], "temperature": 0.0, "avg_logprob": -0.36375374629579743, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.747840870957589e-06}, {"id": 418, "seek": 171184, "start": 1733.24, "end": 1737.32, "text": " So then the next piece of this to consider are the feedforward blocks.", "tokens": [407, 550, 264, 958, 2522, 295, 341, 281, 1949, 366, 264, 3154, 13305, 8474, 13], "temperature": 0.0, "avg_logprob": -0.36375374629579743, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.747840870957589e-06}, {"id": 419, "seek": 171184, "start": 1737.32, "end": 1739.8799999999999, "text": " And this is something that should be relevant to the", "tokens": [400, 341, 307, 746, 300, 820, 312, 7340, 281, 264], "temperature": 0.0, "avg_logprob": -0.36375374629579743, "compression_ratio": 1.6363636363636365, "no_speech_prob": 6.747840870957589e-06}, {"id": 420, "seek": 173988, "start": 1739.88, "end": 1743.2800000000002, "text": " X, and this is something that should be relatively familiar,", "tokens": [1783, 11, 293, 341, 307, 746, 300, 820, 312, 7226, 4963, 11], "temperature": 0.0, "avg_logprob": -0.2974355220794678, "compression_ratio": 1.4467005076142132, "no_speech_prob": 2.4682016373844817e-05}, {"id": 421, "seek": 173988, "start": 1743.2800000000002, "end": 1748.5600000000002, "text": " just from traditional fully connected neural networks.", "tokens": [445, 490, 5164, 4498, 4582, 18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.2974355220794678, "compression_ratio": 1.4467005076142132, "no_speech_prob": 2.4682016373844817e-05}, {"id": 422, "seek": 173988, "start": 1748.5600000000002, "end": 1753.2800000000002, "text": " So we've got a linear layer and relU, we'll append some dropout.", "tokens": [407, 321, 600, 658, 257, 8213, 4583, 293, 1039, 52, 11, 321, 603, 34116, 512, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.2974355220794678, "compression_ratio": 1.4467005076142132, "no_speech_prob": 2.4682016373844817e-05}, {"id": 423, "seek": 173988, "start": 1754.72, "end": 1761.68, "text": " Sequential X is a fast AI way of doing sequential", "tokens": [46859, 2549, 1783, 307, 257, 2370, 7318, 636, 295, 884, 42881], "temperature": 0.0, "avg_logprob": -0.2974355220794678, "compression_ratio": 1.4467005076142132, "no_speech_prob": 2.4682016373844817e-05}, {"id": 424, "seek": 173988, "start": 1761.68, "end": 1766.3600000000001, "text": " that lets you include concatenating with the identity.", "tokens": [300, 6653, 291, 4090, 1588, 7186, 990, 365, 264, 6575, 13], "temperature": 0.0, "avg_logprob": -0.2974355220794678, "compression_ratio": 1.4467005076142132, "no_speech_prob": 2.4682016373844817e-05}, {"id": 425, "seek": 176636, "start": 1766.36, "end": 1771.04, "text": " And have you learned about res blocks or resnet in your other classes at all?", "tokens": [400, 362, 291, 3264, 466, 725, 8474, 420, 725, 7129, 294, 428, 661, 5359, 412, 439, 30], "temperature": 0.0, "avg_logprob": -0.38820477021046174, "compression_ratio": 1.190909090909091, "no_speech_prob": 3.0894202609488275e-06}, {"id": 426, "seek": 176636, "start": 1773.6399999999999, "end": 1776.28, "text": " Okay, let me draw just a quick picture of this.", "tokens": [1033, 11, 718, 385, 2642, 445, 257, 1702, 3036, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.38820477021046174, "compression_ratio": 1.190909090909091, "no_speech_prob": 3.0894202609488275e-06}, {"id": 427, "seek": 176636, "start": 1787.6399999999999, "end": 1788.4399999999998, "text": " Oops.", "tokens": [21726, 13], "temperature": 0.0, "avg_logprob": -0.38820477021046174, "compression_ratio": 1.190909090909091, "no_speech_prob": 3.0894202609488275e-06}, {"id": 428, "seek": 178844, "start": 1788.44, "end": 1797.04, "text": " Jeremy, how do I get a new piece of paper for the drawing?", "tokens": [17809, 11, 577, 360, 286, 483, 257, 777, 2522, 295, 3035, 337, 264, 6316, 30], "temperature": 0.0, "avg_logprob": -0.30907943032004614, "compression_ratio": 1.4444444444444444, "no_speech_prob": 2.39207820413867e-05}, {"id": 429, "seek": 178844, "start": 1797.04, "end": 1802.52, "text": " You have to press the trash can.", "tokens": [509, 362, 281, 1886, 264, 11321, 393, 13], "temperature": 0.0, "avg_logprob": -0.30907943032004614, "compression_ratio": 1.4444444444444444, "no_speech_prob": 2.39207820413867e-05}, {"id": 430, "seek": 178844, "start": 1802.52, "end": 1803.96, "text": " Okay, thanks.", "tokens": [1033, 11, 3231, 13], "temperature": 0.0, "avg_logprob": -0.30907943032004614, "compression_ratio": 1.4444444444444444, "no_speech_prob": 2.39207820413867e-05}, {"id": 431, "seek": 178844, "start": 1803.96, "end": 1804.48, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.30907943032004614, "compression_ratio": 1.4444444444444444, "no_speech_prob": 2.39207820413867e-05}, {"id": 432, "seek": 178844, "start": 1806.1200000000001, "end": 1810.4, "text": " So the idea with a res block, and this is the idea behind resnet,", "tokens": [407, 264, 1558, 365, 257, 725, 3461, 11, 293, 341, 307, 264, 1558, 2261, 725, 7129, 11], "temperature": 0.0, "avg_logprob": -0.30907943032004614, "compression_ratio": 1.4444444444444444, "no_speech_prob": 2.39207820413867e-05}, {"id": 433, "seek": 178844, "start": 1810.4, "end": 1816.96, "text": " which now for a while was kind of a top performer for computer vision and images.", "tokens": [597, 586, 337, 257, 1339, 390, 733, 295, 257, 1192, 30248, 337, 3820, 5201, 293, 5267, 13], "temperature": 0.0, "avg_logprob": -0.30907943032004614, "compression_ratio": 1.4444444444444444, "no_speech_prob": 2.39207820413867e-05}, {"id": 434, "seek": 181696, "start": 1816.96, "end": 1820.44, "text": " So we haven't covered it here since this is an NLP class.", "tokens": [407, 321, 2378, 380, 5343, 309, 510, 1670, 341, 307, 364, 426, 45196, 1508, 13], "temperature": 0.0, "avg_logprob": -0.14392431941601114, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.647375706350431e-05}, {"id": 435, "seek": 181696, "start": 1820.44, "end": 1826.24, "text": " But it's basically that with kind of as you're going from layer to layer in your", "tokens": [583, 309, 311, 1936, 300, 365, 733, 295, 382, 291, 434, 516, 490, 4583, 281, 4583, 294, 428], "temperature": 0.0, "avg_logprob": -0.14392431941601114, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.647375706350431e-05}, {"id": 436, "seek": 181696, "start": 1826.24, "end": 1830.1200000000001, "text": " neural network, the idea behind it is like,", "tokens": [18161, 3209, 11, 264, 1558, 2261, 309, 307, 411, 11], "temperature": 0.0, "avg_logprob": -0.14392431941601114, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.647375706350431e-05}, {"id": 437, "seek": 181696, "start": 1830.1200000000001, "end": 1835.24, "text": " wouldn't it be nice to learn the residuals of kind of what you're getting wrong?", "tokens": [2759, 380, 309, 312, 1481, 281, 1466, 264, 27980, 82, 295, 733, 295, 437, 291, 434, 1242, 2085, 30], "temperature": 0.0, "avg_logprob": -0.14392431941601114, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.647375706350431e-05}, {"id": 438, "seek": 181696, "start": 1835.24, "end": 1841.52, "text": " And so to do that, you need to have what's called a skip connection.", "tokens": [400, 370, 281, 360, 300, 11, 291, 643, 281, 362, 437, 311, 1219, 257, 10023, 4984, 13], "temperature": 0.0, "avg_logprob": -0.14392431941601114, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.647375706350431e-05}, {"id": 439, "seek": 181696, "start": 1841.52, "end": 1846.72, "text": " But basically it means you're just bringing in the identity from the previous layer.", "tokens": [583, 1936, 309, 1355, 291, 434, 445, 5062, 294, 264, 6575, 490, 264, 3894, 4583, 13], "temperature": 0.0, "avg_logprob": -0.14392431941601114, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.647375706350431e-05}, {"id": 440, "seek": 184672, "start": 1846.72, "end": 1852.4, "text": " And so kind of by having this new answer and the previous layer,", "tokens": [400, 370, 733, 295, 538, 1419, 341, 777, 1867, 293, 264, 3894, 4583, 11], "temperature": 0.0, "avg_logprob": -0.12299810671338848, "compression_ratio": 1.7089201877934272, "no_speech_prob": 6.013692109263502e-05}, {"id": 441, "seek": 184672, "start": 1852.4, "end": 1855.28, "text": " that's enough information to try to learn the residuals.", "tokens": [300, 311, 1547, 1589, 281, 853, 281, 1466, 264, 27980, 82, 13], "temperature": 0.0, "avg_logprob": -0.12299810671338848, "compression_ratio": 1.7089201877934272, "no_speech_prob": 6.013692109263502e-05}, {"id": 442, "seek": 184672, "start": 1855.28, "end": 1857.72, "text": " And these are called skip connections.", "tokens": [400, 613, 366, 1219, 10023, 9271, 13], "temperature": 0.0, "avg_logprob": -0.12299810671338848, "compression_ratio": 1.7089201877934272, "no_speech_prob": 6.013692109263502e-05}, {"id": 443, "seek": 184672, "start": 1857.72, "end": 1861.64, "text": " And so in order to be able to do them, you have to be able to merge or", "tokens": [400, 370, 294, 1668, 281, 312, 1075, 281, 360, 552, 11, 291, 362, 281, 312, 1075, 281, 22183, 420], "temperature": 0.0, "avg_logprob": -0.12299810671338848, "compression_ratio": 1.7089201877934272, "no_speech_prob": 6.013692109263502e-05}, {"id": 444, "seek": 184672, "start": 1861.64, "end": 1867.04, "text": " concatenate this input in this one.", "tokens": [1588, 7186, 473, 341, 4846, 294, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.12299810671338848, "compression_ratio": 1.7089201877934272, "no_speech_prob": 6.013692109263502e-05}, {"id": 445, "seek": 184672, "start": 1867.04, "end": 1869.76, "text": " And in a resnet, you have a lot of these skip connections, but", "tokens": [400, 294, 257, 725, 7129, 11, 291, 362, 257, 688, 295, 613, 10023, 9271, 11, 457], "temperature": 0.0, "avg_logprob": -0.12299810671338848, "compression_ratio": 1.7089201877934272, "no_speech_prob": 6.013692109263502e-05}, {"id": 446, "seek": 184672, "start": 1869.76, "end": 1871.8, "text": " we'll just be using a little bit.", "tokens": [321, 603, 445, 312, 1228, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.12299810671338848, "compression_ratio": 1.7089201877934272, "no_speech_prob": 6.013692109263502e-05}, {"id": 447, "seek": 187180, "start": 1871.8, "end": 1878.52, "text": " But that's the idea behind sequential X and using merge.", "tokens": [583, 300, 311, 264, 1558, 2261, 42881, 1783, 293, 1228, 22183, 13], "temperature": 0.0, "avg_logprob": -0.22247757849755226, "compression_ratio": 1.521978021978022, "no_speech_prob": 1.184270058729453e-05}, {"id": 448, "seek": 187180, "start": 1878.52, "end": 1880.44, "text": " So let me show kind of where that plays out.", "tokens": [407, 718, 385, 855, 733, 295, 689, 300, 5749, 484, 13], "temperature": 0.0, "avg_logprob": -0.22247757849755226, "compression_ratio": 1.521978021978022, "no_speech_prob": 1.184270058729453e-05}, {"id": 449, "seek": 187180, "start": 1880.44, "end": 1885.9199999999998, "text": " That's a merge layer is to be able to kind of concatenate two things.", "tokens": [663, 311, 257, 22183, 4583, 307, 281, 312, 1075, 281, 733, 295, 1588, 7186, 473, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.22247757849755226, "compression_ratio": 1.521978021978022, "no_speech_prob": 1.184270058729453e-05}, {"id": 450, "seek": 187180, "start": 1885.9199999999998, "end": 1889.08, "text": " And these are also, oops.", "tokens": [400, 613, 366, 611, 11, 34166, 13], "temperature": 0.0, "avg_logprob": -0.22247757849755226, "compression_ratio": 1.521978021978022, "no_speech_prob": 1.184270058729453e-05}, {"id": 451, "seek": 187180, "start": 1895.24, "end": 1900.48, "text": " This is also a way to kind of let more information make it through the network.", "tokens": [639, 307, 611, 257, 636, 281, 733, 295, 718, 544, 1589, 652, 309, 807, 264, 3209, 13], "temperature": 0.0, "avg_logprob": -0.22247757849755226, "compression_ratio": 1.521978021978022, "no_speech_prob": 1.184270058729453e-05}, {"id": 452, "seek": 190048, "start": 1900.48, "end": 1902.72, "text": " The fact that you're concatenating with the identity, so", "tokens": [440, 1186, 300, 291, 434, 1588, 7186, 990, 365, 264, 6575, 11, 370], "temperature": 0.0, "avg_logprob": -0.23825626373291015, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.8342199837206863e-05}, {"id": 453, "seek": 190048, "start": 1902.72, "end": 1907.76, "text": " you're not necessarily kind of losing what was previously present", "tokens": [291, 434, 406, 4725, 733, 295, 7027, 437, 390, 8046, 1974], "temperature": 0.0, "avg_logprob": -0.23825626373291015, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.8342199837206863e-05}, {"id": 454, "seek": 190048, "start": 1907.76, "end": 1911.28, "text": " when you go to a further layer.", "tokens": [562, 291, 352, 281, 257, 3052, 4583, 13], "temperature": 0.0, "avg_logprob": -0.23825626373291015, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.8342199837206863e-05}, {"id": 455, "seek": 190048, "start": 1911.28, "end": 1917.0, "text": " But you can kind of with the skip connection, okay.", "tokens": [583, 291, 393, 733, 295, 365, 264, 10023, 4984, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.23825626373291015, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.8342199837206863e-05}, {"id": 456, "seek": 190048, "start": 1918.88, "end": 1920.32, "text": " Should have made this smaller.", "tokens": [6454, 362, 1027, 341, 4356, 13], "temperature": 0.0, "avg_logprob": -0.23825626373291015, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.8342199837206863e-05}, {"id": 457, "seek": 190048, "start": 1920.32, "end": 1925.84, "text": " I can think there's another layer down here, and", "tokens": [286, 393, 519, 456, 311, 1071, 4583, 760, 510, 11, 293], "temperature": 0.0, "avg_logprob": -0.23825626373291015, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.8342199837206863e-05}, {"id": 458, "seek": 190048, "start": 1925.84, "end": 1929.84, "text": " that's got a skip connection from B going into layer D.", "tokens": [300, 311, 658, 257, 10023, 4984, 490, 363, 516, 666, 4583, 413, 13], "temperature": 0.0, "avg_logprob": -0.23825626373291015, "compression_ratio": 1.6285714285714286, "no_speech_prob": 1.8342199837206863e-05}, {"id": 459, "seek": 192984, "start": 1929.84, "end": 1932.9199999999998, "text": " Kind of to get this information, kind of preserve it and", "tokens": [9242, 295, 281, 483, 341, 1589, 11, 733, 295, 15665, 309, 293], "temperature": 0.0, "avg_logprob": -0.38860078652699787, "compression_ratio": 1.2755905511811023, "no_speech_prob": 3.822416692855768e-05}, {"id": 460, "seek": 192984, "start": 1932.9199999999998, "end": 1936.9199999999998, "text": " to give your network an opportunity to learn the residuals at each layer.", "tokens": [281, 976, 428, 3209, 364, 2650, 281, 1466, 264, 27980, 82, 412, 1184, 4583, 13], "temperature": 0.0, "avg_logprob": -0.38860078652699787, "compression_ratio": 1.2755905511811023, "no_speech_prob": 3.822416692855768e-05}, {"id": 461, "seek": 192984, "start": 1938.9199999999998, "end": 1939.56, "text": " Yes, Mikael?", "tokens": [1079, 11, 16380, 4300, 30], "temperature": 0.0, "avg_logprob": -0.38860078652699787, "compression_ratio": 1.2755905511811023, "no_speech_prob": 3.822416692855768e-05}, {"id": 462, "seek": 192984, "start": 1941.36, "end": 1953.76, "text": " I believe so, yes.", "tokens": [286, 1697, 370, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.38860078652699787, "compression_ratio": 1.2755905511811023, "no_speech_prob": 3.822416692855768e-05}, {"id": 463, "seek": 195376, "start": 1953.76, "end": 1961.48, "text": " Wouldn't the network learn to adopt the embeddings to ignore the constant", "tokens": [26291, 380, 264, 3209, 1466, 281, 6878, 264, 12240, 29432, 281, 11200, 264, 5754], "temperature": 0.0, "avg_logprob": -0.43465802404615617, "compression_ratio": 1.896774193548387, "no_speech_prob": 3.704778282553889e-05}, {"id": 464, "seek": 195376, "start": 1961.48, "end": 1963.04, "text": " multiplication?", "tokens": [27290, 30], "temperature": 0.0, "avg_logprob": -0.43465802404615617, "compression_ratio": 1.896774193548387, "no_speech_prob": 3.704778282553889e-05}, {"id": 465, "seek": 195376, "start": 1963.04, "end": 1967.8799999999999, "text": " Let me just repeat the question is wouldn't the network learn to adopt", "tokens": [961, 385, 445, 7149, 264, 1168, 307, 2759, 380, 264, 3209, 1466, 281, 6878], "temperature": 0.0, "avg_logprob": -0.43465802404615617, "compression_ratio": 1.896774193548387, "no_speech_prob": 3.704778282553889e-05}, {"id": 466, "seek": 195376, "start": 1967.8799999999999, "end": 1973.24, "text": " the embeddings to ignore this constant multiplication?", "tokens": [264, 12240, 29432, 281, 11200, 341, 5754, 27290, 30], "temperature": 0.0, "avg_logprob": -0.43465802404615617, "compression_ratio": 1.896774193548387, "no_speech_prob": 3.704778282553889e-05}, {"id": 467, "seek": 195376, "start": 1973.24, "end": 1973.76, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.43465802404615617, "compression_ratio": 1.896774193548387, "no_speech_prob": 3.704778282553889e-05}, {"id": 468, "seek": 195376, "start": 1975.76, "end": 1982.68, "text": " I think here the bigger thing is, my guess is that this is about having,", "tokens": [286, 519, 510, 264, 3801, 551, 307, 11, 452, 2041, 307, 300, 341, 307, 466, 1419, 11], "temperature": 0.0, "avg_logprob": -0.43465802404615617, "compression_ratio": 1.896774193548387, "no_speech_prob": 3.704778282553889e-05}, {"id": 469, "seek": 198268, "start": 1982.68, "end": 1988.3600000000001, "text": " kind of this having a different magnitude than the positional encodings.", "tokens": [733, 295, 341, 1419, 257, 819, 15668, 813, 264, 2535, 304, 2058, 378, 1109, 13], "temperature": 0.0, "avg_logprob": -0.2883199691772461, "compression_ratio": 1.4610778443113772, "no_speech_prob": 6.204285455169156e-05}, {"id": 470, "seek": 198268, "start": 1988.3600000000001, "end": 1990.5600000000002, "text": " So that it can learn both.", "tokens": [407, 300, 309, 393, 1466, 1293, 13], "temperature": 0.0, "avg_logprob": -0.2883199691772461, "compression_ratio": 1.4610778443113772, "no_speech_prob": 6.204285455169156e-05}, {"id": 471, "seek": 198268, "start": 1992.5600000000002, "end": 1998.6000000000001, "text": " I mean, neither of these is learned, right?", "tokens": [286, 914, 11, 9662, 295, 613, 307, 3264, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2883199691772461, "compression_ratio": 1.4610778443113772, "no_speech_prob": 6.204285455169156e-05}, {"id": 472, "seek": 198268, "start": 1998.6000000000001, "end": 2004.76, "text": " Like the embeddings, well, I guess we're starting with pre-trained embeddings", "tokens": [1743, 264, 12240, 29432, 11, 731, 11, 286, 2041, 321, 434, 2891, 365, 659, 12, 17227, 2001, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.2883199691772461, "compression_ratio": 1.4610778443113772, "no_speech_prob": 6.204285455169156e-05}, {"id": 473, "seek": 198268, "start": 2004.76, "end": 2008.24, "text": " as what's being input.", "tokens": [382, 437, 311, 885, 4846, 13], "temperature": 0.0, "avg_logprob": -0.2883199691772461, "compression_ratio": 1.4610778443113772, "no_speech_prob": 6.204285455169156e-05}, {"id": 474, "seek": 200824, "start": 2008.24, "end": 2014.04, "text": " This is what we were trying to figure out yesterday.", "tokens": [639, 307, 437, 321, 645, 1382, 281, 2573, 484, 5186, 13], "temperature": 0.0, "avg_logprob": -0.3844369053840637, "compression_ratio": 1.4246575342465753, "no_speech_prob": 3.70504567399621e-05}, {"id": 475, "seek": 200824, "start": 2014.04, "end": 2016.72, "text": " Yeah, no, we're not certain about this.", "tokens": [865, 11, 572, 11, 321, 434, 406, 1629, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.3844369053840637, "compression_ratio": 1.4246575342465753, "no_speech_prob": 3.70504567399621e-05}, {"id": 476, "seek": 200824, "start": 2017.92, "end": 2019.28, "text": " Yeah, so now that's a good question.", "tokens": [865, 11, 370, 586, 300, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.3844369053840637, "compression_ratio": 1.4246575342465753, "no_speech_prob": 3.70504567399621e-05}, {"id": 477, "seek": 200824, "start": 2019.28, "end": 2022.64, "text": " I don't know exactly what's going on in this piece.", "tokens": [286, 500, 380, 458, 2293, 437, 311, 516, 322, 294, 341, 2522, 13], "temperature": 0.0, "avg_logprob": -0.3844369053840637, "compression_ratio": 1.4246575342465753, "no_speech_prob": 3.70504567399621e-05}, {"id": 478, "seek": 200824, "start": 2027.64, "end": 2028.14, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3844369053840637, "compression_ratio": 1.4246575342465753, "no_speech_prob": 3.70504567399621e-05}, {"id": 479, "seek": 202814, "start": 2028.14, "end": 2039.5400000000002, "text": " Any other questions?", "tokens": [2639, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.2237717092853703, "compression_ratio": 1.5106382978723405, "no_speech_prob": 5.2247083658585325e-05}, {"id": 480, "seek": 202814, "start": 2039.5400000000002, "end": 2043.42, "text": " Okay, so what's, yeah, here coming back to our feed forward.", "tokens": [1033, 11, 370, 437, 311, 11, 1338, 11, 510, 1348, 646, 281, 527, 3154, 2128, 13], "temperature": 0.0, "avg_logprob": -0.2237717092853703, "compression_ratio": 1.5106382978723405, "no_speech_prob": 5.2247083658585325e-05}, {"id": 481, "seek": 202814, "start": 2043.42, "end": 2047.5400000000002, "text": " So this is mostly just kind of a traditional feed forward with linear", "tokens": [407, 341, 307, 5240, 445, 733, 295, 257, 5164, 3154, 2128, 365, 8213], "temperature": 0.0, "avg_logprob": -0.2237717092853703, "compression_ratio": 1.5106382978723405, "no_speech_prob": 5.2247083658585325e-05}, {"id": 482, "seek": 202814, "start": 2047.5400000000002, "end": 2051.6600000000003, "text": " value dropout, another linear layer, more dropout.", "tokens": [2158, 3270, 346, 11, 1071, 8213, 4583, 11, 544, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.2237717092853703, "compression_ratio": 1.5106382978723405, "no_speech_prob": 5.2247083658585325e-05}, {"id": 483, "seek": 202814, "start": 2051.6600000000003, "end": 2056.58, "text": " Then this merge layer is just letting you have a skip connection from the kind of", "tokens": [1396, 341, 22183, 4583, 307, 445, 8295, 291, 362, 257, 10023, 4984, 490, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.2237717092853703, "compression_ratio": 1.5106382978723405, "no_speech_prob": 5.2247083658585325e-05}, {"id": 484, "seek": 205658, "start": 2056.58, "end": 2060.74, "text": " two layers previous, not just the previous layer, and then a layer norm.", "tokens": [732, 7914, 3894, 11, 406, 445, 264, 3894, 4583, 11, 293, 550, 257, 4583, 2026, 13], "temperature": 0.0, "avg_logprob": -0.1808315912882487, "compression_ratio": 1.7681159420289856, "no_speech_prob": 1.2029221579723526e-05}, {"id": 485, "seek": 205658, "start": 2063.14, "end": 2068.14, "text": " I remember that we saw there are two feed forward blocks, one in the encoder,", "tokens": [286, 1604, 300, 321, 1866, 456, 366, 732, 3154, 2128, 8474, 11, 472, 294, 264, 2058, 19866, 11], "temperature": 0.0, "avg_logprob": -0.1808315912882487, "compression_ratio": 1.7681159420289856, "no_speech_prob": 1.2029221579723526e-05}, {"id": 486, "seek": 205658, "start": 2068.14, "end": 2069.86, "text": " one in the decoder.", "tokens": [472, 294, 264, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.1808315912882487, "compression_ratio": 1.7681159420289856, "no_speech_prob": 1.2029221579723526e-05}, {"id": 487, "seek": 205658, "start": 2069.86, "end": 2075.7799999999997, "text": " Or rather, I should say in each, after each encoder to encoder,", "tokens": [1610, 2831, 11, 286, 820, 584, 294, 1184, 11, 934, 1184, 2058, 19866, 281, 2058, 19866, 11], "temperature": 0.0, "avg_logprob": -0.1808315912882487, "compression_ratio": 1.7681159420289856, "no_speech_prob": 1.2029221579723526e-05}, {"id": 488, "seek": 205658, "start": 2075.7799999999997, "end": 2079.34, "text": " multi-head attention, and decoder to decoder, multi-head attention.", "tokens": [4825, 12, 1934, 3202, 11, 293, 979, 19866, 281, 979, 19866, 11, 4825, 12, 1934, 3202, 13], "temperature": 0.0, "avg_logprob": -0.1808315912882487, "compression_ratio": 1.7681159420289856, "no_speech_prob": 1.2029221579723526e-05}, {"id": 489, "seek": 207934, "start": 2079.34, "end": 2087.58, "text": " Again, there'll be six of each of these kind of broader blocks.", "tokens": [3764, 11, 456, 603, 312, 2309, 295, 1184, 295, 613, 733, 295, 13227, 8474, 13], "temperature": 0.0, "avg_logprob": -0.3072938052090732, "compression_ratio": 1.493421052631579, "no_speech_prob": 5.86265150559484e-06}, {"id": 490, "seek": 207934, "start": 2087.58, "end": 2095.1800000000003, "text": " So looking at multi-head attention,", "tokens": [407, 1237, 412, 4825, 12, 1934, 3202, 11], "temperature": 0.0, "avg_logprob": -0.3072938052090732, "compression_ratio": 1.493421052631579, "no_speech_prob": 5.86265150559484e-06}, {"id": 491, "seek": 207934, "start": 2095.1800000000003, "end": 2098.34, "text": " we can start with single-head attention over here,", "tokens": [321, 393, 722, 365, 2167, 12, 1934, 3202, 670, 510, 11], "temperature": 0.0, "avg_logprob": -0.3072938052090732, "compression_ratio": 1.493421052631579, "no_speech_prob": 5.86265150559484e-06}, {"id": 492, "seek": 207934, "start": 2098.34, "end": 2101.7000000000003, "text": " also called scale.product attention.", "tokens": [611, 1219, 4373, 13, 33244, 3202, 13], "temperature": 0.0, "avg_logprob": -0.3072938052090732, "compression_ratio": 1.493421052631579, "no_speech_prob": 5.86265150559484e-06}, {"id": 493, "seek": 207934, "start": 2101.7000000000003, "end": 2105.7000000000003, "text": " So we have a query, a key, and a value.", "tokens": [407, 321, 362, 257, 14581, 11, 257, 2141, 11, 293, 257, 2158, 13], "temperature": 0.0, "avg_logprob": -0.3072938052090732, "compression_ratio": 1.493421052631579, "no_speech_prob": 5.86265150559484e-06}, {"id": 494, "seek": 210570, "start": 2105.7, "end": 2110.3399999999997, "text": " And the query and key, we're gonna get a score from.", "tokens": [400, 264, 14581, 293, 2141, 11, 321, 434, 799, 483, 257, 6175, 490, 13], "temperature": 0.0, "avg_logprob": -0.18001105388005575, "compression_ratio": 1.6531531531531531, "no_speech_prob": 1.6964042515610345e-05}, {"id": 495, "seek": 210570, "start": 2110.3399999999997, "end": 2114.4199999999996, "text": " So that's coming out with a scalar value from the query and key, and", "tokens": [407, 300, 311, 1348, 484, 365, 257, 39684, 2158, 490, 264, 14581, 293, 2141, 11, 293], "temperature": 0.0, "avg_logprob": -0.18001105388005575, "compression_ratio": 1.6531531531531531, "no_speech_prob": 1.6964042515610345e-05}, {"id": 496, "seek": 210570, "start": 2114.4199999999996, "end": 2121.2599999999998, "text": " then using that to kind of take our weighted average across the values.", "tokens": [550, 1228, 300, 281, 733, 295, 747, 527, 32807, 4274, 2108, 264, 4190, 13], "temperature": 0.0, "avg_logprob": -0.18001105388005575, "compression_ratio": 1.6531531531531531, "no_speech_prob": 1.6964042515610345e-05}, {"id": 497, "seek": 210570, "start": 2122.3399999999997, "end": 2127.8199999999997, "text": " And so typically, k and v are the same, like the same set of vectors.", "tokens": [400, 370, 5850, 11, 350, 293, 371, 366, 264, 912, 11, 411, 264, 912, 992, 295, 18875, 13], "temperature": 0.0, "avg_logprob": -0.18001105388005575, "compression_ratio": 1.6531531531531531, "no_speech_prob": 1.6964042515610345e-05}, {"id": 498, "seek": 210570, "start": 2127.8199999999997, "end": 2129.8199999999997, "text": " You're learning different weights for them.", "tokens": [509, 434, 2539, 819, 17443, 337, 552, 13], "temperature": 0.0, "avg_logprob": -0.18001105388005575, "compression_ratio": 1.6531531531531531, "no_speech_prob": 1.6964042515610345e-05}, {"id": 499, "seek": 210570, "start": 2129.8199999999997, "end": 2135.06, "text": " But basically, you can think of q is representing the words", "tokens": [583, 1936, 11, 291, 393, 519, 295, 9505, 307, 13460, 264, 2283], "temperature": 0.0, "avg_logprob": -0.18001105388005575, "compression_ratio": 1.6531531531531531, "no_speech_prob": 1.6964042515610345e-05}, {"id": 500, "seek": 213506, "start": 2135.06, "end": 2140.5, "text": " from sequence one, k and v are representing the words from sequence two.", "tokens": [490, 8310, 472, 11, 350, 293, 371, 366, 13460, 264, 2283, 490, 8310, 732, 13], "temperature": 0.0, "avg_logprob": -0.15088395322306772, "compression_ratio": 1.6748768472906403, "no_speech_prob": 9.314209455624223e-05}, {"id": 501, "seek": 213506, "start": 2140.5, "end": 2146.18, "text": " And you're using q and k together to come up with the weight of how much weight to", "tokens": [400, 291, 434, 1228, 9505, 293, 350, 1214, 281, 808, 493, 365, 264, 3364, 295, 577, 709, 3364, 281], "temperature": 0.0, "avg_logprob": -0.15088395322306772, "compression_ratio": 1.6748768472906403, "no_speech_prob": 9.314209455624223e-05}, {"id": 502, "seek": 213506, "start": 2146.18, "end": 2150.94, "text": " give to each word in sequence two when you're taking this weighted average.", "tokens": [976, 281, 1184, 1349, 294, 8310, 732, 562, 291, 434, 1940, 341, 32807, 4274, 13], "temperature": 0.0, "avg_logprob": -0.15088395322306772, "compression_ratio": 1.6748768472906403, "no_speech_prob": 9.314209455624223e-05}, {"id": 503, "seek": 213506, "start": 2150.94, "end": 2155.62, "text": " And this is probably a good time to go back to Jay Alomar's pictures of this.", "tokens": [400, 341, 307, 1391, 257, 665, 565, 281, 352, 646, 281, 11146, 967, 298, 289, 311, 5242, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.15088395322306772, "compression_ratio": 1.6748768472906403, "no_speech_prob": 9.314209455624223e-05}, {"id": 504, "seek": 213506, "start": 2157.18, "end": 2159.86, "text": " I think this was a nice intro.", "tokens": [286, 519, 341, 390, 257, 1481, 12897, 13], "temperature": 0.0, "avg_logprob": -0.15088395322306772, "compression_ratio": 1.6748768472906403, "no_speech_prob": 9.314209455624223e-05}, {"id": 505, "seek": 215986, "start": 2159.86, "end": 2166.34, "text": " So here, the word from sequence one is thinking.", "tokens": [407, 510, 11, 264, 1349, 490, 8310, 472, 307, 1953, 13], "temperature": 0.0, "avg_logprob": -0.1314774141079042, "compression_ratio": 1.6857142857142857, "no_speech_prob": 9.223076631315053e-06}, {"id": 506, "seek": 215986, "start": 2166.34, "end": 2168.34, "text": " The word from sequence two is machines.", "tokens": [440, 1349, 490, 8310, 732, 307, 8379, 13], "temperature": 0.0, "avg_logprob": -0.1314774141079042, "compression_ratio": 1.6857142857142857, "no_speech_prob": 9.223076631315053e-06}, {"id": 507, "seek": 215986, "start": 2168.34, "end": 2170.7000000000003, "text": " Again, if you're using self-attention, then sequence one and", "tokens": [3764, 11, 498, 291, 434, 1228, 2698, 12, 1591, 1251, 11, 550, 8310, 472, 293], "temperature": 0.0, "avg_logprob": -0.1314774141079042, "compression_ratio": 1.6857142857142857, "no_speech_prob": 9.223076631315053e-06}, {"id": 508, "seek": 215986, "start": 2170.7000000000003, "end": 2172.38, "text": " sequence two are the same.", "tokens": [8310, 732, 366, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.1314774141079042, "compression_ratio": 1.6857142857142857, "no_speech_prob": 9.223076631315053e-06}, {"id": 509, "seek": 215986, "start": 2172.38, "end": 2176.42, "text": " If you're talking about the encoder to decoder attention, they're different.", "tokens": [759, 291, 434, 1417, 466, 264, 2058, 19866, 281, 979, 19866, 3202, 11, 436, 434, 819, 13], "temperature": 0.0, "avg_logprob": -0.1314774141079042, "compression_ratio": 1.6857142857142857, "no_speech_prob": 9.223076631315053e-06}, {"id": 510, "seek": 215986, "start": 2177.82, "end": 2184.1800000000003, "text": " And so we have queries, keys, and values.", "tokens": [400, 370, 321, 362, 24109, 11, 9317, 11, 293, 4190, 13], "temperature": 0.0, "avg_logprob": -0.1314774141079042, "compression_ratio": 1.6857142857142857, "no_speech_prob": 9.223076631315053e-06}, {"id": 511, "seek": 218418, "start": 2184.18, "end": 2193.46, "text": " So these are representations that correspond to each word in sequence one and", "tokens": [407, 613, 366, 33358, 300, 6805, 281, 1184, 1349, 294, 8310, 472, 293], "temperature": 0.0, "avg_logprob": -0.23966637702837382, "compression_ratio": 1.467032967032967, "no_speech_prob": 2.3922399122966453e-05}, {"id": 512, "seek": 218418, "start": 2193.46, "end": 2198.02, "text": " sequence two, or rather, sorry, I should say, we learn these weights.", "tokens": [8310, 732, 11, 420, 2831, 11, 2597, 11, 286, 820, 584, 11, 321, 1466, 613, 17443, 13], "temperature": 0.0, "avg_logprob": -0.23966637702837382, "compression_ratio": 1.467032967032967, "no_speech_prob": 2.3922399122966453e-05}, {"id": 513, "seek": 218418, "start": 2198.02, "end": 2202.94, "text": " And then we're multiplying x1 by wq to get q1.", "tokens": [400, 550, 321, 434, 30955, 2031, 16, 538, 261, 80, 281, 483, 9505, 16, 13], "temperature": 0.0, "avg_logprob": -0.23966637702837382, "compression_ratio": 1.467032967032967, "no_speech_prob": 2.3922399122966453e-05}, {"id": 514, "seek": 218418, "start": 2202.94, "end": 2209.94, "text": " So it's coming from this word one, times this weight matrix gives us q1,", "tokens": [407, 309, 311, 1348, 490, 341, 1349, 472, 11, 1413, 341, 3364, 8141, 2709, 505, 9505, 16, 11], "temperature": 0.0, "avg_logprob": -0.23966637702837382, "compression_ratio": 1.467032967032967, "no_speech_prob": 2.3922399122966453e-05}, {"id": 515, "seek": 220994, "start": 2209.94, "end": 2217.1, "text": " x2 times wq gives us q2, x1 times wk gives us k1,", "tokens": [2031, 17, 1413, 261, 80, 2709, 505, 9505, 17, 11, 2031, 16, 1413, 261, 74, 2709, 505, 350, 16, 11], "temperature": 0.0, "avg_logprob": -0.16757130366499706, "compression_ratio": 1.5947368421052632, "no_speech_prob": 3.426716648391448e-05}, {"id": 516, "seek": 220994, "start": 2217.1, "end": 2222.86, "text": " x2 times wk gives us k2, and so on.", "tokens": [2031, 17, 1413, 261, 74, 2709, 505, 350, 17, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.16757130366499706, "compression_ratio": 1.5947368421052632, "no_speech_prob": 3.426716648391448e-05}, {"id": 517, "seek": 220994, "start": 2222.86, "end": 2226.54, "text": " And so this is how we're getting our queries, keys, and values.", "tokens": [400, 370, 341, 307, 577, 321, 434, 1242, 527, 24109, 11, 9317, 11, 293, 4190, 13], "temperature": 0.0, "avg_logprob": -0.16757130366499706, "compression_ratio": 1.5947368421052632, "no_speech_prob": 3.426716648391448e-05}, {"id": 518, "seek": 220994, "start": 2226.54, "end": 2233.5, "text": " But you can see that kind of here, these are corresponding to this word thinking,", "tokens": [583, 291, 393, 536, 300, 733, 295, 510, 11, 613, 366, 11760, 281, 341, 1349, 1953, 11], "temperature": 0.0, "avg_logprob": -0.16757130366499706, "compression_ratio": 1.5947368421052632, "no_speech_prob": 3.426716648391448e-05}, {"id": 519, "seek": 220994, "start": 2233.5, "end": 2237.86, "text": " cuz we used its embedding times different weights to get these vectors.", "tokens": [11910, 321, 1143, 1080, 12240, 3584, 1413, 819, 17443, 281, 483, 613, 18875, 13], "temperature": 0.0, "avg_logprob": -0.16757130366499706, "compression_ratio": 1.5947368421052632, "no_speech_prob": 3.426716648391448e-05}, {"id": 520, "seek": 223786, "start": 2237.86, "end": 2244.58, "text": " And in the second column, q2, k2, v2 represent, or", "tokens": [400, 294, 264, 1150, 7738, 11, 9505, 17, 11, 350, 17, 11, 371, 17, 2906, 11, 420], "temperature": 0.0, "avg_logprob": -0.23429023163228097, "compression_ratio": 1.4216216216216215, "no_speech_prob": 1.7230911907972768e-05}, {"id": 521, "seek": 223786, "start": 2244.58, "end": 2251.6600000000003, "text": " they're all derived from our embedding x2 of machines with these weight matrices.", "tokens": [436, 434, 439, 18949, 490, 527, 12240, 3584, 2031, 17, 295, 8379, 365, 613, 3364, 32284, 13], "temperature": 0.0, "avg_logprob": -0.23429023163228097, "compression_ratio": 1.4216216216216215, "no_speech_prob": 1.7230911907972768e-05}, {"id": 522, "seek": 223786, "start": 2253.54, "end": 2254.46, "text": " And then I think it's helpful,", "tokens": [400, 550, 286, 519, 309, 311, 4961, 11], "temperature": 0.0, "avg_logprob": -0.23429023163228097, "compression_ratio": 1.4216216216216215, "no_speech_prob": 1.7230911907972768e-05}, {"id": 523, "seek": 223786, "start": 2254.46, "end": 2259.78, "text": " let me show the kind of the next step before we pause for questions.", "tokens": [718, 385, 855, 264, 733, 295, 264, 958, 1823, 949, 321, 10465, 337, 1651, 13], "temperature": 0.0, "avg_logprob": -0.23429023163228097, "compression_ratio": 1.4216216216216215, "no_speech_prob": 1.7230911907972768e-05}, {"id": 524, "seek": 223786, "start": 2260.94, "end": 2263.94, "text": " Key thing is kind of q and k1,", "tokens": [12759, 551, 307, 733, 295, 9505, 293, 350, 16, 11], "temperature": 0.0, "avg_logprob": -0.23429023163228097, "compression_ratio": 1.4216216216216215, "no_speech_prob": 1.7230911907972768e-05}, {"id": 525, "seek": 226394, "start": 2263.94, "end": 2268.42, "text": " we're dotting together to get this score.", "tokens": [321, 434, 5893, 783, 1214, 281, 483, 341, 6175, 13], "temperature": 0.0, "avg_logprob": -0.2978204461031182, "compression_ratio": 1.6271186440677967, "no_speech_prob": 4.198463284410536e-05}, {"id": 526, "seek": 226394, "start": 2268.42, "end": 2272.54, "text": " And that's also, there's a formula that goes into it, but", "tokens": [400, 300, 311, 611, 11, 456, 311, 257, 8513, 300, 1709, 666, 309, 11, 457], "temperature": 0.0, "avg_logprob": -0.2978204461031182, "compression_ratio": 1.6271186440677967, "no_speech_prob": 4.198463284410536e-05}, {"id": 527, "seek": 226394, "start": 2272.54, "end": 2276.06, "text": " kind of key thing is we're getting a scalar out of these.", "tokens": [733, 295, 2141, 551, 307, 321, 434, 1242, 257, 39684, 484, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.2978204461031182, "compression_ratio": 1.6271186440677967, "no_speech_prob": 4.198463284410536e-05}, {"id": 528, "seek": 226394, "start": 2276.06, "end": 2282.02, "text": " And then that scalar is telling us how to take a weighted average of the v's.", "tokens": [400, 550, 300, 39684, 307, 3585, 505, 577, 281, 747, 257, 32807, 4274, 295, 264, 371, 311, 13], "temperature": 0.0, "avg_logprob": -0.2978204461031182, "compression_ratio": 1.6271186440677967, "no_speech_prob": 4.198463284410536e-05}, {"id": 529, "seek": 226394, "start": 2282.02, "end": 2283.54, "text": " Oops.", "tokens": [21726, 13], "temperature": 0.0, "avg_logprob": -0.2978204461031182, "compression_ratio": 1.6271186440677967, "no_speech_prob": 4.198463284410536e-05}, {"id": 530, "seek": 226394, "start": 2286.7400000000002, "end": 2290.54, "text": " And so we see that here, so the score is also,", "tokens": [400, 370, 321, 536, 300, 510, 11, 370, 264, 6175, 307, 611, 11], "temperature": 0.0, "avg_logprob": -0.2978204461031182, "compression_ratio": 1.6271186440677967, "no_speech_prob": 4.198463284410536e-05}, {"id": 531, "seek": 229054, "start": 2290.54, "end": 2297.34, "text": " you're normalizing by the square root of the dimension, taking the soft max.", "tokens": [291, 434, 2710, 3319, 538, 264, 3732, 5593, 295, 264, 10139, 11, 1940, 264, 2787, 11469, 13], "temperature": 0.0, "avg_logprob": -0.25191613247520045, "compression_ratio": 1.4756756756756757, "no_speech_prob": 9.818211765377782e-06}, {"id": 532, "seek": 229054, "start": 2297.34, "end": 2303.66, "text": " And then multiply that by the value, or by v, and sum it up.", "tokens": [400, 550, 12972, 300, 538, 264, 2158, 11, 420, 538, 371, 11, 293, 2408, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.25191613247520045, "compression_ratio": 1.4756756756756757, "no_speech_prob": 9.818211765377782e-06}, {"id": 533, "seek": 229054, "start": 2303.66, "end": 2308.14, "text": " So that's the weighted average, kind of how attention is calculated.", "tokens": [407, 300, 311, 264, 32807, 4274, 11, 733, 295, 577, 3202, 307, 15598, 13], "temperature": 0.0, "avg_logprob": -0.25191613247520045, "compression_ratio": 1.4756756756756757, "no_speech_prob": 9.818211765377782e-06}, {"id": 534, "seek": 229054, "start": 2310.62, "end": 2312.02, "text": " There are questions.", "tokens": [821, 366, 1651, 13], "temperature": 0.0, "avg_logprob": -0.25191613247520045, "compression_ratio": 1.4756756756756757, "no_speech_prob": 9.818211765377782e-06}, {"id": 535, "seek": 229054, "start": 2316.66, "end": 2319.02, "text": " And these graphics come from J Alomar's post,", "tokens": [400, 613, 11837, 808, 490, 508, 967, 298, 289, 311, 2183, 11], "temperature": 0.0, "avg_logprob": -0.25191613247520045, "compression_ratio": 1.4756756756756757, "no_speech_prob": 9.818211765377782e-06}, {"id": 536, "seek": 231902, "start": 2319.02, "end": 2321.22, "text": " the Illustrated Transformer, which I really like.", "tokens": [264, 37788, 5468, 27938, 260, 11, 597, 286, 534, 411, 13], "temperature": 0.0, "avg_logprob": -0.26635845796561536, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9772041671094485e-05}, {"id": 537, "seek": 231902, "start": 2326.62, "end": 2330.02, "text": " All right, let's see, we can kind of look at what that looks like in the code.", "tokens": [1057, 558, 11, 718, 311, 536, 11, 321, 393, 733, 295, 574, 412, 437, 300, 1542, 411, 294, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.26635845796561536, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9772041671094485e-05}, {"id": 538, "seek": 231902, "start": 2333.02, "end": 2336.54, "text": " And the multi-head, so that was what you would do for", "tokens": [400, 264, 4825, 12, 1934, 11, 370, 300, 390, 437, 291, 576, 360, 337], "temperature": 0.0, "avg_logprob": -0.26635845796561536, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9772041671094485e-05}, {"id": 539, "seek": 231902, "start": 2339.14, "end": 2341.18, "text": " kind of a single-head attention.", "tokens": [733, 295, 257, 2167, 12, 1934, 3202, 13], "temperature": 0.0, "avg_logprob": -0.26635845796561536, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9772041671094485e-05}, {"id": 540, "seek": 231902, "start": 2341.18, "end": 2345.34, "text": " And multi-head is just kind of increasing the dimensionality of this.", "tokens": [400, 4825, 12, 1934, 307, 445, 733, 295, 5662, 264, 10139, 1860, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.26635845796561536, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9772041671094485e-05}, {"id": 541, "seek": 234534, "start": 2345.34, "end": 2350.6600000000003, "text": " I think it's, I can't remember if it's six or eight heads in the paper, but", "tokens": [286, 519, 309, 311, 11, 286, 393, 380, 1604, 498, 309, 311, 2309, 420, 3180, 8050, 294, 264, 3035, 11, 457], "temperature": 0.0, "avg_logprob": -0.2152207156261766, "compression_ratio": 1.5179487179487179, "no_speech_prob": 1.8058020941680297e-05}, {"id": 542, "seek": 234534, "start": 2350.6600000000003, "end": 2356.5, "text": " you're learning multiple sets of these weight matrices, basically.", "tokens": [291, 434, 2539, 3866, 6352, 295, 613, 3364, 32284, 11, 1936, 13], "temperature": 0.0, "avg_logprob": -0.2152207156261766, "compression_ratio": 1.5179487179487179, "no_speech_prob": 1.8058020941680297e-05}, {"id": 543, "seek": 234534, "start": 2356.5, "end": 2359.1800000000003, "text": " WQ, WK, WV.", "tokens": [343, 48, 11, 343, 42, 11, 343, 53, 13], "temperature": 0.0, "avg_logprob": -0.2152207156261766, "compression_ratio": 1.5179487179487179, "no_speech_prob": 1.8058020941680297e-05}, {"id": 544, "seek": 234534, "start": 2359.1800000000003, "end": 2363.5, "text": " Alternately, you can think of that as kind of increasing an extra dimension in", "tokens": [23830, 1592, 11, 291, 393, 519, 295, 300, 382, 733, 295, 5662, 364, 2857, 10139, 294], "temperature": 0.0, "avg_logprob": -0.2152207156261766, "compression_ratio": 1.5179487179487179, "no_speech_prob": 1.8058020941680297e-05}, {"id": 545, "seek": 236350, "start": 2363.5, "end": 2376.22, "text": " terms of the tensor, you've multiplied each of these by eight.", "tokens": [2115, 295, 264, 40863, 11, 291, 600, 17207, 1184, 295, 613, 538, 3180, 13], "temperature": 0.0, "avg_logprob": -0.19319140116373698, "compression_ratio": 1.5562130177514792, "no_speech_prob": 2.6841523776965914e-06}, {"id": 546, "seek": 236350, "start": 2376.22, "end": 2381.58, "text": " So how that looks here, so we, for our multi-head attention, so", "tokens": [407, 577, 300, 1542, 510, 11, 370, 321, 11, 337, 527, 4825, 12, 1934, 3202, 11, 370], "temperature": 0.0, "avg_logprob": -0.19319140116373698, "compression_ratio": 1.5562130177514792, "no_speech_prob": 2.6841523776965914e-06}, {"id": 547, "seek": 236350, "start": 2381.58, "end": 2387.66, "text": " we need to set the number of heads, the dimension of each head in the scale.", "tokens": [321, 643, 281, 992, 264, 1230, 295, 8050, 11, 264, 10139, 295, 1184, 1378, 294, 264, 4373, 13], "temperature": 0.0, "avg_logprob": -0.19319140116373698, "compression_ratio": 1.5562130177514792, "no_speech_prob": 2.6841523776965914e-06}, {"id": 548, "seek": 236350, "start": 2387.66, "end": 2390.78, "text": " We'll have our Q weights, our K weights, and our V weights.", "tokens": [492, 603, 362, 527, 1249, 17443, 11, 527, 591, 17443, 11, 293, 527, 691, 17443, 13], "temperature": 0.0, "avg_logprob": -0.19319140116373698, "compression_ratio": 1.5562130177514792, "no_speech_prob": 2.6841523776965914e-06}, {"id": 549, "seek": 239078, "start": 2390.78, "end": 2395.78, "text": " And it's kind of easier to, or you can potentially kind of put those all", "tokens": [400, 309, 311, 733, 295, 3571, 281, 11, 420, 291, 393, 7263, 733, 295, 829, 729, 439], "temperature": 0.0, "avg_logprob": -0.23179674784342447, "compression_ratio": 1.5524861878453038, "no_speech_prob": 1.4285034012573306e-05}, {"id": 550, "seek": 239078, "start": 2395.78, "end": 2400.78, "text": " together, it's just a linear layer, you're learning these weights.", "tokens": [1214, 11, 309, 311, 445, 257, 8213, 4583, 11, 291, 434, 2539, 613, 17443, 13], "temperature": 0.0, "avg_logprob": -0.23179674784342447, "compression_ratio": 1.5524861878453038, "no_speech_prob": 1.4285034012573306e-05}, {"id": 551, "seek": 239078, "start": 2404.78, "end": 2409.78, "text": " We'll have dropout for our attention and on the result, layer norm.", "tokens": [492, 603, 362, 3270, 346, 337, 527, 3202, 293, 322, 264, 1874, 11, 4583, 2026, 13], "temperature": 0.0, "avg_logprob": -0.23179674784342447, "compression_ratio": 1.5524861878453038, "no_speech_prob": 1.4285034012573306e-05}, {"id": 552, "seek": 239078, "start": 2414.78, "end": 2418.78, "text": " The forward step is just going to apply attention, so we'll look at that.", "tokens": [440, 2128, 1823, 307, 445, 516, 281, 3079, 3202, 11, 370, 321, 603, 574, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.23179674784342447, "compression_ratio": 1.5524861878453038, "no_speech_prob": 1.4285034012573306e-05}, {"id": 553, "seek": 241878, "start": 2418.78, "end": 2424.78, "text": " It's taking Q and KV as input, and the reason we have it as KV is because K and", "tokens": [467, 311, 1940, 1249, 293, 591, 53, 382, 4846, 11, 293, 264, 1778, 321, 362, 309, 382, 591, 53, 307, 570, 591, 293], "temperature": 0.0, "avg_logprob": -0.15723426319728387, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.0001088877470465377}, {"id": 554, "seek": 241878, "start": 2424.78, "end": 2426.78, "text": " V are the same thing.", "tokens": [691, 366, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.15723426319728387, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.0001088877470465377}, {"id": 555, "seek": 241878, "start": 2426.78, "end": 2431.78, "text": " So the weight matrices WK and WV are different, but KV or", "tokens": [407, 264, 3364, 32284, 343, 42, 293, 343, 53, 366, 819, 11, 457, 591, 53, 420], "temperature": 0.0, "avg_logprob": -0.15723426319728387, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.0001088877470465377}, {"id": 556, "seek": 241878, "start": 2431.78, "end": 2436.78, "text": " K and V are the same, and that's because it's talking about the same word in", "tokens": [591, 293, 691, 366, 264, 912, 11, 293, 300, 311, 570, 309, 311, 1417, 466, 264, 912, 1349, 294], "temperature": 0.0, "avg_logprob": -0.15723426319728387, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.0001088877470465377}, {"id": 557, "seek": 241878, "start": 2436.78, "end": 2439.78, "text": " sequence two that you're figuring out how much weight to give to.", "tokens": [8310, 732, 300, 291, 434, 15213, 484, 577, 709, 3364, 281, 976, 281, 13], "temperature": 0.0, "avg_logprob": -0.15723426319728387, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.0001088877470465377}, {"id": 558, "seek": 241878, "start": 2441.78, "end": 2445.78, "text": " So we have this, well actually, let's go into apply attention.", "tokens": [407, 321, 362, 341, 11, 731, 767, 11, 718, 311, 352, 666, 3079, 3202, 13], "temperature": 0.0, "avg_logprob": -0.15723426319728387, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.0001088877470465377}, {"id": 559, "seek": 244578, "start": 2445.78, "end": 2448.78, "text": " So we'll need the batch size and the sequence length.", "tokens": [407, 321, 603, 643, 264, 15245, 2744, 293, 264, 8310, 4641, 13], "temperature": 0.0, "avg_logprob": -0.146072058244185, "compression_ratio": 1.696969696969697, "no_speech_prob": 5.649509694194421e-05}, {"id": 560, "seek": 244578, "start": 2448.78, "end": 2454.78, "text": " Then we're going to call this helper method create a attention matrix on Q.", "tokens": [1396, 321, 434, 516, 281, 818, 341, 36133, 3170, 1884, 257, 3202, 8141, 322, 1249, 13], "temperature": 0.0, "avg_logprob": -0.146072058244185, "compression_ratio": 1.696969696969697, "no_speech_prob": 5.649509694194421e-05}, {"id": 561, "seek": 244578, "start": 2454.78, "end": 2459.78, "text": " And the reason we're feeding KV in twice is once it'll be used as K and", "tokens": [400, 264, 1778, 321, 434, 12919, 591, 53, 294, 6091, 307, 1564, 309, 603, 312, 1143, 382, 591, 293], "temperature": 0.0, "avg_logprob": -0.146072058244185, "compression_ratio": 1.696969696969697, "no_speech_prob": 5.649509694194421e-05}, {"id": 562, "seek": 244578, "start": 2459.78, "end": 2463.78, "text": " the other time as V with the weights for Q, K, and V.", "tokens": [264, 661, 565, 382, 691, 365, 264, 17443, 337, 1249, 11, 591, 11, 293, 691, 13], "temperature": 0.0, "avg_logprob": -0.146072058244185, "compression_ratio": 1.696969696969697, "no_speech_prob": 5.649509694194421e-05}, {"id": 563, "seek": 244578, "start": 2463.78, "end": 2467.78, "text": " And these are being zipped together, so it's Q and", "tokens": [400, 613, 366, 885, 710, 5529, 1214, 11, 370, 309, 311, 1249, 293], "temperature": 0.0, "avg_logprob": -0.146072058244185, "compression_ratio": 1.696969696969697, "no_speech_prob": 5.649509694194421e-05}, {"id": 564, "seek": 244578, "start": 2467.78, "end": 2470.78, "text": " weights for Q are being passed into this together.", "tokens": [17443, 337, 1249, 366, 885, 4678, 666, 341, 1214, 13], "temperature": 0.0, "avg_logprob": -0.146072058244185, "compression_ratio": 1.696969696969697, "no_speech_prob": 5.649509694194421e-05}, {"id": 565, "seek": 244578, "start": 2470.78, "end": 2473.78, "text": " And this is just creating a layer.", "tokens": [400, 341, 307, 445, 4084, 257, 4583, 13], "temperature": 0.0, "avg_logprob": -0.146072058244185, "compression_ratio": 1.696969696969697, "no_speech_prob": 5.649509694194421e-05}, {"id": 566, "seek": 247378, "start": 2473.78, "end": 2482.78, "text": " We want to view this as batch size times this first dimension from X,", "tokens": [492, 528, 281, 1910, 341, 382, 15245, 2744, 1413, 341, 700, 10139, 490, 1783, 11], "temperature": 0.0, "avg_logprob": -0.08223452439179292, "compression_ratio": 1.544041450777202, "no_speech_prob": 4.264475865056738e-05}, {"id": 567, "seek": 247378, "start": 2482.78, "end": 2487.78, "text": " which I guess is probably the length of the sequence,", "tokens": [597, 286, 2041, 307, 1391, 264, 4641, 295, 264, 8310, 11], "temperature": 0.0, "avg_logprob": -0.08223452439179292, "compression_ratio": 1.544041450777202, "no_speech_prob": 4.264475865056738e-05}, {"id": 568, "seek": 247378, "start": 2487.78, "end": 2491.78, "text": " times the number of heads by the dimension of heads.", "tokens": [1413, 264, 1230, 295, 8050, 538, 264, 10139, 295, 8050, 13], "temperature": 0.0, "avg_logprob": -0.08223452439179292, "compression_ratio": 1.544041450777202, "no_speech_prob": 4.264475865056738e-05}, {"id": 569, "seek": 247378, "start": 2491.78, "end": 2494.78, "text": " And there's some permuting that has to happen just for", "tokens": [400, 456, 311, 512, 4784, 10861, 300, 575, 281, 1051, 445, 337], "temperature": 0.0, "avg_logprob": -0.08223452439179292, "compression_ratio": 1.544041450777202, "no_speech_prob": 4.264475865056738e-05}, {"id": 570, "seek": 247378, "start": 2494.78, "end": 2498.78, "text": " these to kind of line up correctly when you do the multiplication.", "tokens": [613, 281, 733, 295, 1622, 493, 8944, 562, 291, 360, 264, 27290, 13], "temperature": 0.0, "avg_logprob": -0.08223452439179292, "compression_ratio": 1.544041450777202, "no_speech_prob": 4.264475865056738e-05}, {"id": 571, "seek": 249878, "start": 2498.78, "end": 2504.78, "text": " But the attention score here is just WQ times WK.", "tokens": [583, 264, 3202, 6175, 510, 307, 445, 343, 48, 1413, 343, 42, 13], "temperature": 0.0, "avg_logprob": -0.11616466084464652, "compression_ratio": 1.4294871794871795, "no_speech_prob": 1.5689072824898176e-05}, {"id": 572, "seek": 249878, "start": 2504.78, "end": 2509.78, "text": " So that's going to give us our weights for V that we talked about.", "tokens": [407, 300, 311, 516, 281, 976, 505, 527, 17443, 337, 691, 300, 321, 2825, 466, 13], "temperature": 0.0, "avg_logprob": -0.11616466084464652, "compression_ratio": 1.4294871794871795, "no_speech_prob": 1.5689072824898176e-05}, {"id": 573, "seek": 249878, "start": 2509.78, "end": 2511.78, "text": " So we can take this weighted average of V.", "tokens": [407, 321, 393, 747, 341, 32807, 4274, 295, 691, 13], "temperature": 0.0, "avg_logprob": -0.11616466084464652, "compression_ratio": 1.4294871794871795, "no_speech_prob": 1.5689072824898176e-05}, {"id": 574, "seek": 249878, "start": 2519.78, "end": 2524.78, "text": " Then the attention probability is we're applying our dropout on", "tokens": [1396, 264, 3202, 8482, 307, 321, 434, 9275, 527, 3270, 346, 322], "temperature": 0.0, "avg_logprob": -0.11616466084464652, "compression_ratio": 1.4294871794871795, "no_speech_prob": 1.5689072824898176e-05}, {"id": 575, "seek": 252478, "start": 2524.78, "end": 2529.78, "text": " the soft max of the attention score.", "tokens": [264, 2787, 11469, 295, 264, 3202, 6175, 13], "temperature": 0.0, "avg_logprob": -0.12743213211280713, "compression_ratio": 1.5872093023255813, "no_speech_prob": 1.1125433047709521e-05}, {"id": 576, "seek": 252478, "start": 2529.78, "end": 2531.78, "text": " Apply those probabilities to WV.", "tokens": [25264, 729, 33783, 281, 343, 53, 13], "temperature": 0.0, "avg_logprob": -0.12743213211280713, "compression_ratio": 1.5872093023255813, "no_speech_prob": 1.1125433047709521e-05}, {"id": 577, "seek": 252478, "start": 2531.78, "end": 2536.78, "text": " So actually I guess here is kind of the attention vector is the true", "tokens": [407, 767, 286, 2041, 510, 307, 733, 295, 264, 3202, 8062, 307, 264, 2074], "temperature": 0.0, "avg_logprob": -0.12743213211280713, "compression_ratio": 1.5872093023255813, "no_speech_prob": 1.1125433047709521e-05}, {"id": 578, "seek": 252478, "start": 2536.78, "end": 2542.78, "text": " weighted average after we've calculated the weights from the scores.", "tokens": [32807, 4274, 934, 321, 600, 15598, 264, 17443, 490, 264, 13444, 13], "temperature": 0.0, "avg_logprob": -0.12743213211280713, "compression_ratio": 1.5872093023255813, "no_speech_prob": 1.1125433047709521e-05}, {"id": 579, "seek": 254278, "start": 2542.78, "end": 2555.78, "text": " And so this is kind of the, I guess, code version of the picture.", "tokens": [400, 370, 341, 307, 733, 295, 264, 11, 286, 2041, 11, 3089, 3037, 295, 264, 3036, 13], "temperature": 0.0, "avg_logprob": -0.0868222224406707, "compression_ratio": 1.53551912568306, "no_speech_prob": 1.4285224096965976e-05}, {"id": 580, "seek": 254278, "start": 2555.78, "end": 2558.78, "text": " And I know it's a lot of pieces.", "tokens": [400, 286, 458, 309, 311, 257, 688, 295, 3755, 13], "temperature": 0.0, "avg_logprob": -0.0868222224406707, "compression_ratio": 1.53551912568306, "no_speech_prob": 1.4285224096965976e-05}, {"id": 581, "seek": 254278, "start": 2558.78, "end": 2564.78, "text": " I mean, I think it's kind of reassuring that these are primarily just", "tokens": [286, 914, 11, 286, 519, 309, 311, 733, 295, 19486, 1345, 300, 613, 366, 10029, 445], "temperature": 0.0, "avg_logprob": -0.0868222224406707, "compression_ratio": 1.53551912568306, "no_speech_prob": 1.4285224096965976e-05}, {"id": 582, "seek": 254278, "start": 2564.78, "end": 2566.78, "text": " matrix multiplications with, you know,", "tokens": [8141, 17596, 763, 365, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.0868222224406707, "compression_ratio": 1.53551912568306, "no_speech_prob": 1.4285224096965976e-05}, {"id": 583, "seek": 254278, "start": 2566.78, "end": 2569.78, "text": " it's kind of the same building blocks that we always see with neural nets", "tokens": [309, 311, 733, 295, 264, 912, 2390, 8474, 300, 321, 1009, 536, 365, 18161, 36170], "temperature": 0.0, "avg_logprob": -0.0868222224406707, "compression_ratio": 1.53551912568306, "no_speech_prob": 1.4285224096965976e-05}, {"id": 584, "seek": 256978, "start": 2569.78, "end": 2572.78, "text": " of taking matrix multiplications.", "tokens": [295, 1940, 8141, 17596, 763, 13], "temperature": 0.0, "avg_logprob": -0.1321540117263794, "compression_ratio": 1.572139303482587, "no_speech_prob": 1.8342594557907432e-05}, {"id": 585, "seek": 256978, "start": 2572.78, "end": 2580.78, "text": " Some non-linearities of when we have soft max that I think it looks more", "tokens": [2188, 2107, 12, 28263, 1088, 295, 562, 321, 362, 2787, 11469, 300, 286, 519, 309, 1542, 544], "temperature": 0.0, "avg_logprob": -0.1321540117263794, "compression_ratio": 1.572139303482587, "no_speech_prob": 1.8342594557907432e-05}, {"id": 586, "seek": 256978, "start": 2580.78, "end": 2581.78, "text": " complicated.", "tokens": [6179, 13], "temperature": 0.0, "avg_logprob": -0.1321540117263794, "compression_ratio": 1.572139303482587, "no_speech_prob": 1.8342594557907432e-05}, {"id": 587, "seek": 256978, "start": 2581.78, "end": 2583.78, "text": " And I think these are like complicated ideas of, you know,", "tokens": [400, 286, 519, 613, 366, 411, 6179, 3487, 295, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.1321540117263794, "compression_ratio": 1.572139303482587, "no_speech_prob": 1.8342594557907432e-05}, {"id": 588, "seek": 256978, "start": 2583.78, "end": 2587.78, "text": " thinking about this key query and value.", "tokens": [1953, 466, 341, 2141, 14581, 293, 2158, 13], "temperature": 0.0, "avg_logprob": -0.1321540117263794, "compression_ratio": 1.572139303482587, "no_speech_prob": 1.8342594557907432e-05}, {"id": 589, "seek": 256978, "start": 2587.78, "end": 2593.78, "text": " But that it is just this representation of taking weighted averages of", "tokens": [583, 300, 309, 307, 445, 341, 10290, 295, 1940, 32807, 42257, 295], "temperature": 0.0, "avg_logprob": -0.1321540117263794, "compression_ratio": 1.572139303482587, "no_speech_prob": 1.8342594557907432e-05}, {"id": 590, "seek": 256978, "start": 2593.78, "end": 2596.78, "text": " knowing what to focus on.", "tokens": [5276, 437, 281, 1879, 322, 13], "temperature": 0.0, "avg_logprob": -0.1321540117263794, "compression_ratio": 1.572139303482587, "no_speech_prob": 1.8342594557907432e-05}, {"id": 591, "seek": 259678, "start": 2596.78, "end": 2607.78, "text": " There are questions.", "tokens": [821, 366, 1651, 13], "temperature": 0.0, "avg_logprob": -0.13010034155338368, "compression_ratio": 1.2704918032786885, "no_speech_prob": 0.00014882822870276868}, {"id": 592, "seek": 259678, "start": 2607.78, "end": 2608.78, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.13010034155338368, "compression_ratio": 1.2704918032786885, "no_speech_prob": 0.00014882822870276868}, {"id": 593, "seek": 259678, "start": 2608.78, "end": 2612.78, "text": " The next piece to look at is masking.", "tokens": [440, 958, 2522, 281, 574, 412, 307, 31226, 13], "temperature": 0.0, "avg_logprob": -0.13010034155338368, "compression_ratio": 1.2704918032786885, "no_speech_prob": 0.00014882822870276868}, {"id": 594, "seek": 259678, "start": 2612.78, "end": 2621.78, "text": " And this is basically just that kind of later on when we're dealing with the", "tokens": [400, 341, 307, 1936, 445, 300, 733, 295, 1780, 322, 562, 321, 434, 6260, 365, 264], "temperature": 0.0, "avg_logprob": -0.13010034155338368, "compression_ratio": 1.2704918032786885, "no_speech_prob": 0.00014882822870276868}, {"id": 595, "seek": 259678, "start": 2621.78, "end": 2622.78, "text": " decoder,", "tokens": [979, 19866, 11], "temperature": 0.0, "avg_logprob": -0.13010034155338368, "compression_ratio": 1.2704918032786885, "no_speech_prob": 0.00014882822870276868}, {"id": 596, "seek": 262278, "start": 2622.78, "end": 2626.78, "text": " it would be cheating to look at the words that come after the word we're", "tokens": [309, 576, 312, 18309, 281, 574, 412, 264, 2283, 300, 808, 934, 264, 1349, 321, 434], "temperature": 0.0, "avg_logprob": -0.05586881152654098, "compression_ratio": 1.6814814814814816, "no_speech_prob": 9.027387568494305e-05}, {"id": 597, "seek": 262278, "start": 2626.78, "end": 2628.78, "text": " dealing with since we shouldn't know those yet.", "tokens": [6260, 365, 1670, 321, 4659, 380, 458, 729, 1939, 13], "temperature": 0.0, "avg_logprob": -0.05586881152654098, "compression_ratio": 1.6814814814814816, "no_speech_prob": 9.027387568494305e-05}, {"id": 598, "seek": 262278, "start": 2628.78, "end": 2632.78, "text": " So this is a way of dealing with the fact that since this is not an RNN,", "tokens": [407, 341, 307, 257, 636, 295, 6260, 365, 264, 1186, 300, 1670, 341, 307, 406, 364, 45702, 45, 11], "temperature": 0.0, "avg_logprob": -0.05586881152654098, "compression_ratio": 1.6814814814814816, "no_speech_prob": 9.027387568494305e-05}, {"id": 599, "seek": 262278, "start": 2632.78, "end": 2637.78, "text": " we've taken out this time sequential nature that you kind of automatically", "tokens": [321, 600, 2726, 484, 341, 565, 42881, 3687, 300, 291, 733, 295, 6772], "temperature": 0.0, "avg_logprob": -0.05586881152654098, "compression_ratio": 1.6814814814814816, "no_speech_prob": 9.027387568494305e-05}, {"id": 600, "seek": 262278, "start": 2637.78, "end": 2639.78, "text": " get when you're using an RNN.", "tokens": [483, 562, 291, 434, 1228, 364, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.05586881152654098, "compression_ratio": 1.6814814814814816, "no_speech_prob": 9.027387568494305e-05}, {"id": 601, "seek": 262278, "start": 2639.78, "end": 2642.78, "text": " But here we don't want to peek or look ahead.", "tokens": [583, 510, 321, 500, 380, 528, 281, 19604, 420, 574, 2286, 13], "temperature": 0.0, "avg_logprob": -0.05586881152654098, "compression_ratio": 1.6814814814814816, "no_speech_prob": 9.027387568494305e-05}, {"id": 602, "seek": 262278, "start": 2642.78, "end": 2647.78, "text": " And so basically you will just use this upper triangular matrix of ones to", "tokens": [400, 370, 1936, 291, 486, 445, 764, 341, 6597, 38190, 8141, 295, 2306, 281], "temperature": 0.0, "avg_logprob": -0.05586881152654098, "compression_ratio": 1.6814814814814816, "no_speech_prob": 9.027387568494305e-05}, {"id": 603, "seek": 262278, "start": 2647.78, "end": 2649.78, "text": " cover up everything in the future.", "tokens": [2060, 493, 1203, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.05586881152654098, "compression_ratio": 1.6814814814814816, "no_speech_prob": 9.027387568494305e-05}, {"id": 604, "seek": 264978, "start": 2649.78, "end": 2654.78, "text": " So kind of at the first time step, you can't see anything that's coming ahead.", "tokens": [407, 733, 295, 412, 264, 700, 565, 1823, 11, 291, 393, 380, 536, 1340, 300, 311, 1348, 2286, 13], "temperature": 0.0, "avg_logprob": -0.1102990117566339, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.0001195758450194262}, {"id": 605, "seek": 264978, "start": 2654.78, "end": 2657.78, "text": " Then, you know, at the second time step, you just know the first two things and", "tokens": [1396, 11, 291, 458, 11, 412, 264, 1150, 565, 1823, 11, 291, 445, 458, 264, 700, 732, 721, 293], "temperature": 0.0, "avg_logprob": -0.1102990117566339, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.0001195758450194262}, {"id": 606, "seek": 264978, "start": 2657.78, "end": 2660.78, "text": " then the first three things, the first four things.", "tokens": [550, 264, 700, 1045, 721, 11, 264, 700, 1451, 721, 13], "temperature": 0.0, "avg_logprob": -0.1102990117566339, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.0001195758450194262}, {"id": 607, "seek": 264978, "start": 2660.78, "end": 2665.78, "text": " And so we apply this mask.", "tokens": [400, 370, 321, 3079, 341, 6094, 13], "temperature": 0.0, "avg_logprob": -0.1102990117566339, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.0001195758450194262}, {"id": 608, "seek": 264978, "start": 2665.78, "end": 2668.78, "text": " Then kind of getting to the encoder and decoder block.", "tokens": [1396, 733, 295, 1242, 281, 264, 2058, 19866, 293, 979, 19866, 3461, 13], "temperature": 0.0, "avg_logprob": -0.1102990117566339, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.0001195758450194262}, {"id": 609, "seek": 264978, "start": 2668.78, "end": 2673.78, "text": " So up here, again, this was just the definition of multi-head attention.", "tokens": [407, 493, 510, 11, 797, 11, 341, 390, 445, 264, 7123, 295, 4825, 12, 1934, 3202, 13], "temperature": 0.0, "avg_logprob": -0.1102990117566339, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.0001195758450194262}, {"id": 610, "seek": 264978, "start": 2673.78, "end": 2677.78, "text": " But as the paper, and I took a quote from the paper,", "tokens": [583, 382, 264, 3035, 11, 293, 286, 1890, 257, 6513, 490, 264, 3035, 11], "temperature": 0.0, "avg_logprob": -0.1102990117566339, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.0001195758450194262}, {"id": 611, "seek": 267778, "start": 2677.78, "end": 2681.78, "text": " as the paper says, the transformer uses multi-head attention in three different", "tokens": [382, 264, 3035, 1619, 11, 264, 31782, 4960, 4825, 12, 1934, 3202, 294, 1045, 819], "temperature": 0.0, "avg_logprob": -0.10359577510667883, "compression_ratio": 1.9958847736625513, "no_speech_prob": 5.390737351262942e-05}, {"id": 612, "seek": 267778, "start": 2681.78, "end": 2685.78, "text": " ways, the encoder to decoder attention.", "tokens": [2098, 11, 264, 2058, 19866, 281, 979, 19866, 3202, 13], "temperature": 0.0, "avg_logprob": -0.10359577510667883, "compression_ratio": 1.9958847736625513, "no_speech_prob": 5.390737351262942e-05}, {"id": 613, "seek": 267778, "start": 2685.78, "end": 2690.78, "text": " And then there you can think about that of thinking kind of like what word from", "tokens": [400, 550, 456, 291, 393, 519, 466, 300, 295, 1953, 733, 295, 411, 437, 1349, 490], "temperature": 0.0, "avg_logprob": -0.10359577510667883, "compression_ratio": 1.9958847736625513, "no_speech_prob": 5.390737351262942e-05}, {"id": 614, "seek": 267778, "start": 2690.78, "end": 2695.78, "text": " the English is now being translated to French because the syntax is different", "tokens": [264, 3669, 307, 586, 885, 16805, 281, 5522, 570, 264, 28431, 307, 819], "temperature": 0.0, "avg_logprob": -0.10359577510667883, "compression_ratio": 1.9958847736625513, "no_speech_prob": 5.390737351262942e-05}, {"id": 615, "seek": 267778, "start": 2695.78, "end": 2696.78, "text": " in English and French.", "tokens": [294, 3669, 293, 5522, 13], "temperature": 0.0, "avg_logprob": -0.10359577510667883, "compression_ratio": 1.9958847736625513, "no_speech_prob": 5.390737351262942e-05}, {"id": 616, "seek": 267778, "start": 2696.78, "end": 2700.78, "text": " This is not always going to be the first word being translated to the second", "tokens": [639, 307, 406, 1009, 516, 281, 312, 264, 700, 1349, 885, 16805, 281, 264, 1150], "temperature": 0.0, "avg_logprob": -0.10359577510667883, "compression_ratio": 1.9958847736625513, "no_speech_prob": 5.390737351262942e-05}, {"id": 617, "seek": 267778, "start": 2700.78, "end": 2704.78, "text": " word, or the first word of French being translated to the first word of English.", "tokens": [1349, 11, 420, 264, 700, 1349, 295, 5522, 885, 16805, 281, 264, 700, 1349, 295, 3669, 13], "temperature": 0.0, "avg_logprob": -0.10359577510667883, "compression_ratio": 1.9958847736625513, "no_speech_prob": 5.390737351262942e-05}, {"id": 618, "seek": 267778, "start": 2704.78, "end": 2705.78, "text": " There's a different order.", "tokens": [821, 311, 257, 819, 1668, 13], "temperature": 0.0, "avg_logprob": -0.10359577510667883, "compression_ratio": 1.9958847736625513, "no_speech_prob": 5.390737351262942e-05}, {"id": 619, "seek": 270578, "start": 2705.78, "end": 2707.78, "text": " And also a different number of words.", "tokens": [400, 611, 257, 819, 1230, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.08963520386639763, "compression_ratio": 1.634020618556701, "no_speech_prob": 3.647691119113006e-05}, {"id": 620, "seek": 270578, "start": 2707.78, "end": 2714.78, "text": " French has, you know, gender pronouns that English doesn't.", "tokens": [5522, 575, 11, 291, 458, 11, 7898, 35883, 300, 3669, 1177, 380, 13], "temperature": 0.0, "avg_logprob": -0.08963520386639763, "compression_ratio": 1.634020618556701, "no_speech_prob": 3.647691119113006e-05}, {"id": 621, "seek": 270578, "start": 2714.78, "end": 2718.78, "text": " Secondly, the encoder has self-attention layers.", "tokens": [19483, 11, 264, 2058, 19866, 575, 2698, 12, 1591, 1251, 7914, 13], "temperature": 0.0, "avg_logprob": -0.08963520386639763, "compression_ratio": 1.634020618556701, "no_speech_prob": 3.647691119113006e-05}, {"id": 622, "seek": 270578, "start": 2718.78, "end": 2724.78, "text": " We saw that can be useful in the sentence, the animal crossed the street because", "tokens": [492, 1866, 300, 393, 312, 4420, 294, 264, 8174, 11, 264, 5496, 14622, 264, 4838, 570], "temperature": 0.0, "avg_logprob": -0.08963520386639763, "compression_ratio": 1.634020618556701, "no_speech_prob": 3.647691119113006e-05}, {"id": 623, "seek": 270578, "start": 2724.78, "end": 2728.78, "text": " it was tired, knowing what it relates to.", "tokens": [309, 390, 5868, 11, 5276, 437, 309, 16155, 281, 13], "temperature": 0.0, "avg_logprob": -0.08963520386639763, "compression_ratio": 1.634020618556701, "no_speech_prob": 3.647691119113006e-05}, {"id": 624, "seek": 270578, "start": 2728.78, "end": 2733.78, "text": " And then the decoder has self-attention layers.", "tokens": [400, 550, 264, 979, 19866, 575, 2698, 12, 1591, 1251, 7914, 13], "temperature": 0.0, "avg_logprob": -0.08963520386639763, "compression_ratio": 1.634020618556701, "no_speech_prob": 3.647691119113006e-05}, {"id": 625, "seek": 273378, "start": 2733.78, "end": 2738.78, "text": " And again, that's the same idea of kind of knowing what other words in the", "tokens": [400, 797, 11, 300, 311, 264, 912, 1558, 295, 733, 295, 5276, 437, 661, 2283, 294, 264], "temperature": 0.0, "avg_logprob": -0.06581217134502572, "compression_ratio": 1.528735632183908, "no_speech_prob": 2.5866380383376963e-05}, {"id": 626, "seek": 273378, "start": 2738.78, "end": 2746.78, "text": " sentence are relevant to the current concept.", "tokens": [8174, 366, 7340, 281, 264, 2190, 3410, 13], "temperature": 0.0, "avg_logprob": -0.06581217134502572, "compression_ratio": 1.528735632183908, "no_speech_prob": 2.5866380383376963e-05}, {"id": 627, "seek": 273378, "start": 2746.78, "end": 2750.78, "text": " So this was defining kind of a multi-head attention class.", "tokens": [407, 341, 390, 17827, 733, 295, 257, 4825, 12, 1934, 3202, 1508, 13], "temperature": 0.0, "avg_logprob": -0.06581217134502572, "compression_ratio": 1.528735632183908, "no_speech_prob": 2.5866380383376963e-05}, {"id": 628, "seek": 273378, "start": 2750.78, "end": 2758.78, "text": " We'll be using three of these in our transformer.", "tokens": [492, 603, 312, 1228, 1045, 295, 613, 294, 527, 31782, 13], "temperature": 0.0, "avg_logprob": -0.06581217134502572, "compression_ratio": 1.528735632183908, "no_speech_prob": 2.5866380383376963e-05}, {"id": 629, "seek": 273378, "start": 2758.78, "end": 2761.78, "text": " So there's one in the encoder block.", "tokens": [407, 456, 311, 472, 294, 264, 2058, 19866, 3461, 13], "temperature": 0.0, "avg_logprob": -0.06581217134502572, "compression_ratio": 1.528735632183908, "no_speech_prob": 2.5866380383376963e-05}, {"id": 630, "seek": 276178, "start": 2761.78, "end": 2766.78, "text": " And it takes in the number of heads, the dimension of the model, dimension of", "tokens": [400, 309, 2516, 294, 264, 1230, 295, 8050, 11, 264, 10139, 295, 264, 2316, 11, 10139, 295], "temperature": 0.0, "avg_logprob": -0.09708175230562018, "compression_ratio": 1.808139534883721, "no_speech_prob": 9.313265036325902e-05}, {"id": 631, "seek": 276178, "start": 2766.78, "end": 2771.78, "text": " the head, feed forward.", "tokens": [264, 1378, 11, 3154, 2128, 13], "temperature": 0.0, "avg_logprob": -0.09708175230562018, "compression_ratio": 1.808139534883721, "no_speech_prob": 9.313265036325902e-05}, {"id": 632, "seek": 276178, "start": 2771.78, "end": 2776.78, "text": " And that's the feed forward block that comes after the multi-head attention", "tokens": [400, 300, 311, 264, 3154, 2128, 3461, 300, 1487, 934, 264, 4825, 12, 1934, 3202], "temperature": 0.0, "avg_logprob": -0.09708175230562018, "compression_ratio": 1.808139534883721, "no_speech_prob": 9.313265036325902e-05}, {"id": 633, "seek": 276178, "start": 2776.78, "end": 2777.78, "text": " block.", "tokens": [3461, 13], "temperature": 0.0, "avg_logprob": -0.09708175230562018, "compression_ratio": 1.808139534883721, "no_speech_prob": 9.313265036325902e-05}, {"id": 634, "seek": 276178, "start": 2777.78, "end": 2784.78, "text": " And then the forward step for the encoder calls self.multi-headattention.", "tokens": [400, 550, 264, 2128, 1823, 337, 264, 2058, 19866, 5498, 2698, 13, 76, 723, 72, 12, 1934, 1591, 1251, 13], "temperature": 0.0, "avg_logprob": -0.09708175230562018, "compression_ratio": 1.808139534883721, "no_speech_prob": 9.313265036325902e-05}, {"id": 635, "seek": 276178, "start": 2784.78, "end": 2787.78, "text": " And it's passing in xx since this is self-attention.", "tokens": [400, 309, 311, 8437, 294, 2031, 87, 1670, 341, 307, 2698, 12, 1591, 1251, 13], "temperature": 0.0, "avg_logprob": -0.09708175230562018, "compression_ratio": 1.808139534883721, "no_speech_prob": 9.313265036325902e-05}, {"id": 636, "seek": 278778, "start": 2787.78, "end": 2792.78, "text": " It's giving the same thing for both arguments.", "tokens": [467, 311, 2902, 264, 912, 551, 337, 1293, 12869, 13], "temperature": 0.0, "avg_logprob": -0.07974862463680314, "compression_ratio": 1.7173913043478262, "no_speech_prob": 6.814616790506989e-05}, {"id": 637, "seek": 278778, "start": 2792.78, "end": 2799.78, "text": " And then as a reminder, kind of this first thing corresponds to the query.", "tokens": [400, 550, 382, 257, 13548, 11, 733, 295, 341, 700, 551, 23249, 281, 264, 14581, 13], "temperature": 0.0, "avg_logprob": -0.07974862463680314, "compression_ratio": 1.7173913043478262, "no_speech_prob": 6.814616790506989e-05}, {"id": 638, "seek": 278778, "start": 2799.78, "end": 2804.78, "text": " And the second will be used for both the key and the value because you need,", "tokens": [400, 264, 1150, 486, 312, 1143, 337, 1293, 264, 2141, 293, 264, 2158, 570, 291, 643, 11], "temperature": 0.0, "avg_logprob": -0.07974862463680314, "compression_ratio": 1.7173913043478262, "no_speech_prob": 6.814616790506989e-05}, {"id": 639, "seek": 278778, "start": 2804.78, "end": 2808.78, "text": " you know, you calculate the score using the key and the query.", "tokens": [291, 458, 11, 291, 8873, 264, 6175, 1228, 264, 2141, 293, 264, 14581, 13], "temperature": 0.0, "avg_logprob": -0.07974862463680314, "compression_ratio": 1.7173913043478262, "no_speech_prob": 6.814616790506989e-05}, {"id": 640, "seek": 278778, "start": 2808.78, "end": 2812.78, "text": " And then that gives you a scaled version of the value.", "tokens": [400, 550, 300, 2709, 291, 257, 36039, 3037, 295, 264, 2158, 13], "temperature": 0.0, "avg_logprob": -0.07974862463680314, "compression_ratio": 1.7173913043478262, "no_speech_prob": 6.814616790506989e-05}, {"id": 641, "seek": 281278, "start": 2812.78, "end": 2818.78, "text": " The decoder block has two multi-head attention classes.", "tokens": [440, 979, 19866, 3461, 575, 732, 4825, 12, 1934, 3202, 5359, 13], "temperature": 0.0, "avg_logprob": -0.0672377586364746, "compression_ratio": 1.8688524590163935, "no_speech_prob": 4.264607559889555e-05}, {"id": 642, "seek": 281278, "start": 2818.78, "end": 2825.78, "text": " And you can see in the, kind of in the forward step, the first is the", "tokens": [400, 291, 393, 536, 294, 264, 11, 733, 295, 294, 264, 2128, 1823, 11, 264, 700, 307, 264], "temperature": 0.0, "avg_logprob": -0.0672377586364746, "compression_ratio": 1.8688524590163935, "no_speech_prob": 4.264607559889555e-05}, {"id": 643, "seek": 281278, "start": 2825.78, "end": 2829.78, "text": " self-attention that you're calling on the same thing twice.", "tokens": [2698, 12, 1591, 1251, 300, 291, 434, 5141, 322, 264, 912, 551, 6091, 13], "temperature": 0.0, "avg_logprob": -0.0672377586364746, "compression_ratio": 1.8688524590163935, "no_speech_prob": 4.264607559889555e-05}, {"id": 644, "seek": 281278, "start": 2829.78, "end": 2833.78, "text": " And then you call multi-head attention on that to the encoder.", "tokens": [400, 550, 291, 818, 4825, 12, 1934, 3202, 322, 300, 281, 264, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.0672377586364746, "compression_ratio": 1.8688524590163935, "no_speech_prob": 4.264607559889555e-05}, {"id": 645, "seek": 281278, "start": 2833.78, "end": 2838.78, "text": " So this is self-attention on the decoder and then get attention between that and", "tokens": [407, 341, 307, 2698, 12, 1591, 1251, 322, 264, 979, 19866, 293, 550, 483, 3202, 1296, 300, 293], "temperature": 0.0, "avg_logprob": -0.0672377586364746, "compression_ratio": 1.8688524590163935, "no_speech_prob": 4.264607559889555e-05}, {"id": 646, "seek": 281278, "start": 2838.78, "end": 2841.78, "text": " the encoder.", "tokens": [264, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.0672377586364746, "compression_ratio": 1.8688524590163935, "no_speech_prob": 4.264607559889555e-05}, {"id": 647, "seek": 284178, "start": 2841.78, "end": 2845.78, "text": " And here we want to be able to apply the output mask when we're dealing with", "tokens": [400, 510, 321, 528, 281, 312, 1075, 281, 3079, 264, 5598, 6094, 562, 321, 434, 6260, 365], "temperature": 0.0, "avg_logprob": -0.05931869818239796, "compression_ratio": 1.7089201877934272, "no_speech_prob": 4.5396260247798637e-05}, {"id": 648, "seek": 284178, "start": 2845.78, "end": 2849.78, "text": " the decoder because we don't want to look at the future.", "tokens": [264, 979, 19866, 570, 321, 500, 380, 528, 281, 574, 412, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.05931869818239796, "compression_ratio": 1.7089201877934272, "no_speech_prob": 4.5396260247798637e-05}, {"id": 649, "seek": 284178, "start": 2849.78, "end": 2857.78, "text": " So we're now ready to put that all together into the whole model.", "tokens": [407, 321, 434, 586, 1919, 281, 829, 300, 439, 1214, 666, 264, 1379, 2316, 13], "temperature": 0.0, "avg_logprob": -0.05931869818239796, "compression_ratio": 1.7089201877934272, "no_speech_prob": 4.5396260247798637e-05}, {"id": 650, "seek": 284178, "start": 2857.78, "end": 2861.78, "text": " We have a transformer embedding for the encoder, transformer embedding for the", "tokens": [492, 362, 257, 31782, 12240, 3584, 337, 264, 2058, 19866, 11, 31782, 12240, 3584, 337, 264], "temperature": 0.0, "avg_logprob": -0.05931869818239796, "compression_ratio": 1.7089201877934272, "no_speech_prob": 4.5396260247798637e-05}, {"id": 651, "seek": 284178, "start": 2861.78, "end": 2863.78, "text": " decoder.", "tokens": [979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.05931869818239796, "compression_ratio": 1.7089201877934272, "no_speech_prob": 4.5396260247798637e-05}, {"id": 652, "seek": 284178, "start": 2863.78, "end": 2867.78, "text": " Kind of we'll have these args that we'll need about the number of heads, the", "tokens": [9242, 295, 321, 603, 362, 613, 3882, 82, 300, 321, 603, 643, 466, 264, 1230, 295, 8050, 11, 264], "temperature": 0.0, "avg_logprob": -0.05931869818239796, "compression_ratio": 1.7089201877934272, "no_speech_prob": 4.5396260247798637e-05}, {"id": 653, "seek": 286778, "start": 2867.78, "end": 2875.78, "text": " dimension of the model, the dimension of the head.", "tokens": [10139, 295, 264, 2316, 11, 264, 10139, 295, 264, 1378, 13], "temperature": 0.0, "avg_logprob": -0.09586586271013532, "compression_ratio": 1.605263157894737, "no_speech_prob": 4.539503424894065e-05}, {"id": 654, "seek": 286778, "start": 2875.78, "end": 2880.78, "text": " This is dealing with the fact that, so the original paper had six encoder", "tokens": [639, 307, 6260, 365, 264, 1186, 300, 11, 370, 264, 3380, 3035, 632, 2309, 2058, 19866], "temperature": 0.0, "avg_logprob": -0.09586586271013532, "compression_ratio": 1.605263157894737, "no_speech_prob": 4.539503424894065e-05}, {"id": 655, "seek": 286778, "start": 2880.78, "end": 2882.78, "text": " blocks and six decoder blocks.", "tokens": [8474, 293, 2309, 979, 19866, 8474, 13], "temperature": 0.0, "avg_logprob": -0.09586586271013532, "compression_ratio": 1.605263157894737, "no_speech_prob": 4.539503424894065e-05}, {"id": 656, "seek": 286778, "start": 2882.78, "end": 2888.78, "text": " So those are declared using a list comprehension.", "tokens": [407, 729, 366, 15489, 1228, 257, 1329, 44991, 13], "temperature": 0.0, "avg_logprob": -0.09586586271013532, "compression_ratio": 1.605263157894737, "no_speech_prob": 4.539503424894065e-05}, {"id": 657, "seek": 286778, "start": 2888.78, "end": 2893.78, "text": " Then a linear layer embedding weights.", "tokens": [1396, 257, 8213, 4583, 12240, 3584, 17443, 13], "temperature": 0.0, "avg_logprob": -0.09586586271013532, "compression_ratio": 1.605263157894737, "no_speech_prob": 4.539503424894065e-05}, {"id": 658, "seek": 289378, "start": 2893.78, "end": 2899.78, "text": " We need to know what our padding ID is, our output mask, and then just kind of", "tokens": [492, 643, 281, 458, 437, 527, 39562, 7348, 307, 11, 527, 5598, 6094, 11, 293, 550, 445, 733, 295], "temperature": 0.0, "avg_logprob": -0.11187896161976427, "compression_ratio": 1.6576576576576576, "no_speech_prob": 5.594287813437404e-06}, {"id": 659, "seek": 289378, "start": 2899.78, "end": 2901.78, "text": " composing these together.", "tokens": [715, 6110, 613, 1214, 13], "temperature": 0.0, "avg_logprob": -0.11187896161976427, "compression_ratio": 1.6576576576576576, "no_speech_prob": 5.594287813437404e-06}, {"id": 660, "seek": 289378, "start": 2901.78, "end": 2907.78, "text": " The encoder, which then will be fed, the result of that will be fed into the", "tokens": [440, 2058, 19866, 11, 597, 550, 486, 312, 4636, 11, 264, 1874, 295, 300, 486, 312, 4636, 666, 264], "temperature": 0.0, "avg_logprob": -0.11187896161976427, "compression_ratio": 1.6576576576576576, "no_speech_prob": 5.594287813437404e-06}, {"id": 661, "seek": 289378, "start": 2907.78, "end": 2908.78, "text": " decoder.", "tokens": [979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.11187896161976427, "compression_ratio": 1.6576576576576576, "no_speech_prob": 5.594287813437404e-06}, {"id": 662, "seek": 289378, "start": 2908.78, "end": 2912.78, "text": " And so this is something kind of now that we have the building blocks, the", "tokens": [400, 370, 341, 307, 746, 733, 295, 586, 300, 321, 362, 264, 2390, 8474, 11, 264], "temperature": 0.0, "avg_logprob": -0.11187896161976427, "compression_ratio": 1.6576576576576576, "no_speech_prob": 5.594287813437404e-06}, {"id": 663, "seek": 289378, "start": 2912.78, "end": 2916.78, "text": " transformer can be defined kind of more concisely, but there were a lot of", "tokens": [31782, 393, 312, 7642, 733, 295, 544, 1588, 271, 736, 11, 457, 456, 645, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.11187896161976427, "compression_ratio": 1.6576576576576576, "no_speech_prob": 5.594287813437404e-06}, {"id": 664, "seek": 289378, "start": 2916.78, "end": 2922.78, "text": " pieces that went into this.", "tokens": [3755, 300, 1437, 666, 341, 13], "temperature": 0.0, "avg_logprob": -0.11187896161976427, "compression_ratio": 1.6576576576576576, "no_speech_prob": 5.594287813437404e-06}, {"id": 665, "seek": 292278, "start": 2922.78, "end": 2927.78, "text": " Questions?", "tokens": [27738, 30], "temperature": 0.0, "avg_logprob": -0.18662193365264357, "compression_ratio": 1.3172413793103448, "no_speech_prob": 7.721851579844952e-05}, {"id": 666, "seek": 292278, "start": 2927.78, "end": 2940.78, "text": " Jeremy, let me pass you the microphone.", "tokens": [17809, 11, 718, 385, 1320, 291, 264, 10952, 13], "temperature": 0.0, "avg_logprob": -0.18662193365264357, "compression_ratio": 1.3172413793103448, "no_speech_prob": 7.721851579844952e-05}, {"id": 667, "seek": 292278, "start": 2940.78, "end": 2943.78, "text": " We did this more manually than we had to as well.", "tokens": [492, 630, 341, 544, 16945, 813, 321, 632, 281, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.18662193365264357, "compression_ratio": 1.3172413793103448, "no_speech_prob": 7.721851579844952e-05}, {"id": 668, "seek": 292278, "start": 2943.78, "end": 2947.78, "text": " High Torch has a multi-head attention built into it.", "tokens": [5229, 7160, 339, 575, 257, 4825, 12, 1934, 3202, 3094, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.18662193365264357, "compression_ratio": 1.3172413793103448, "no_speech_prob": 7.721851579844952e-05}, {"id": 669, "seek": 292278, "start": 2947.78, "end": 2950.78, "text": " In real life you would just use that.", "tokens": [682, 957, 993, 291, 576, 445, 764, 300, 13], "temperature": 0.0, "avg_logprob": -0.18662193365264357, "compression_ratio": 1.3172413793103448, "no_speech_prob": 7.721851579844952e-05}, {"id": 670, "seek": 295078, "start": 2950.78, "end": 2953.78, "text": " Because we just wanted to show all the steps.", "tokens": [1436, 321, 445, 1415, 281, 855, 439, 264, 4439, 13], "temperature": 0.0, "avg_logprob": -0.2175745122572955, "compression_ratio": 1.4113924050632911, "no_speech_prob": 0.0001488238194724545}, {"id": 671, "seek": 295078, "start": 2953.78, "end": 2959.78, "text": " We went more kind of from the foundation.", "tokens": [492, 1437, 544, 733, 295, 490, 264, 7030, 13], "temperature": 0.0, "avg_logprob": -0.2175745122572955, "compression_ratio": 1.4113924050632911, "no_speech_prob": 0.0001488238194724545}, {"id": 672, "seek": 295078, "start": 2959.78, "end": 2967.78, "text": " Thank you, Jeremy.", "tokens": [1044, 291, 11, 17809, 13], "temperature": 0.0, "avg_logprob": -0.2175745122572955, "compression_ratio": 1.4113924050632911, "no_speech_prob": 0.0001488238194724545}, {"id": 673, "seek": 295078, "start": 2967.78, "end": 2969.78, "text": " That was a good point.", "tokens": [663, 390, 257, 665, 935, 13], "temperature": 0.0, "avg_logprob": -0.2175745122572955, "compression_ratio": 1.4113924050632911, "no_speech_prob": 0.0001488238194724545}, {"id": 674, "seek": 295078, "start": 2969.78, "end": 2972.78, "text": " In practice, use PyTorch's implementation.", "tokens": [682, 3124, 11, 764, 9953, 51, 284, 339, 311, 11420, 13], "temperature": 0.0, "avg_logprob": -0.2175745122572955, "compression_ratio": 1.4113924050632911, "no_speech_prob": 0.0001488238194724545}, {"id": 675, "seek": 295078, "start": 2972.78, "end": 2975.78, "text": " This is to kind of help you see what goes into it.", "tokens": [639, 307, 281, 733, 295, 854, 291, 536, 437, 1709, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.2175745122572955, "compression_ratio": 1.4113924050632911, "no_speech_prob": 0.0001488238194724545}, {"id": 676, "seek": 297578, "start": 2975.78, "end": 2980.78, "text": " Similar to how in the GRU lesson, in practice, please use PyTorch's GRU, but we", "tokens": [10905, 281, 577, 294, 264, 10903, 52, 6898, 11, 294, 3124, 11, 1767, 764, 9953, 51, 284, 339, 311, 10903, 52, 11, 457, 321], "temperature": 0.0, "avg_logprob": -0.16837058347814224, "compression_ratio": 1.6092436974789917, "no_speech_prob": 8.480686665279791e-05}, {"id": 677, "seek": 297578, "start": 2980.78, "end": 2984.78, "text": " wanted to show you how you would build it yourself.", "tokens": [1415, 281, 855, 291, 577, 291, 576, 1322, 309, 1803, 13], "temperature": 0.0, "avg_logprob": -0.16837058347814224, "compression_ratio": 1.6092436974789917, "no_speech_prob": 8.480686665279791e-05}, {"id": 678, "seek": 297578, "start": 2984.78, "end": 2988.78, "text": " And it's also something I think is simultaneously encouraging and", "tokens": [400, 309, 311, 611, 746, 286, 519, 307, 16561, 14580, 293], "temperature": 0.0, "avg_logprob": -0.16837058347814224, "compression_ratio": 1.6092436974789917, "no_speech_prob": 8.480686665279791e-05}, {"id": 679, "seek": 297578, "start": 2988.78, "end": 2993.78, "text": " discouraging in that there are a lot of pieces that seems complicated, but it's", "tokens": [21497, 3568, 294, 300, 456, 366, 257, 688, 295, 3755, 300, 2544, 6179, 11, 457, 309, 311], "temperature": 0.0, "avg_logprob": -0.16837058347814224, "compression_ratio": 1.6092436974789917, "no_speech_prob": 8.480686665279791e-05}, {"id": 680, "seek": 297578, "start": 2993.78, "end": 2998.78, "text": " also individually you know what each of the pieces are and they're things you", "tokens": [611, 16652, 291, 458, 437, 1184, 295, 264, 3755, 366, 293, 436, 434, 721, 291], "temperature": 0.0, "avg_logprob": -0.16837058347814224, "compression_ratio": 1.6092436974789917, "no_speech_prob": 8.480686665279791e-05}, {"id": 681, "seek": 297578, "start": 2998.78, "end": 2999.78, "text": " recognize.", "tokens": [5521, 13], "temperature": 0.0, "avg_logprob": -0.16837058347814224, "compression_ratio": 1.6092436974789917, "no_speech_prob": 8.480686665279791e-05}, {"id": 682, "seek": 297578, "start": 2999.78, "end": 3001.78, "text": " So that's, yeah.", "tokens": [407, 300, 311, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.16837058347814224, "compression_ratio": 1.6092436974789917, "no_speech_prob": 8.480686665279791e-05}, {"id": 683, "seek": 300178, "start": 3001.78, "end": 3008.78, "text": " I feel like both good and bad when I look at the low level implementation.", "tokens": [286, 841, 411, 1293, 665, 293, 1578, 562, 286, 574, 412, 264, 2295, 1496, 11420, 13], "temperature": 0.0, "avg_logprob": -0.11101943690602373, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.00010553593165241182}, {"id": 684, "seek": 300178, "start": 3008.78, "end": 3011.78, "text": " Yeah, so now we're ready to train this one.", "tokens": [865, 11, 370, 586, 321, 434, 1919, 281, 3847, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.11101943690602373, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.00010553593165241182}, {"id": 685, "seek": 300178, "start": 3011.78, "end": 3015.78, "text": " We'll use the blue metric again since we're doing a translation task and we want", "tokens": [492, 603, 764, 264, 3344, 20678, 797, 1670, 321, 434, 884, 257, 12853, 5633, 293, 321, 528], "temperature": 0.0, "avg_logprob": -0.11101943690602373, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.00010553593165241182}, {"id": 686, "seek": 300178, "start": 3015.78, "end": 3019.78, "text": " to be able to compare to our performance before.", "tokens": [281, 312, 1075, 281, 6794, 281, 527, 3389, 949, 13], "temperature": 0.0, "avg_logprob": -0.11101943690602373, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.00010553593165241182}, {"id": 687, "seek": 300178, "start": 3019.78, "end": 3028.78, "text": " So we declare our model, create a learner, choose a learning rate.", "tokens": [407, 321, 19710, 527, 2316, 11, 1884, 257, 33347, 11, 2826, 257, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.11101943690602373, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.00010553593165241182}, {"id": 688, "seek": 302878, "start": 3028.78, "end": 3032.78, "text": " And then here we're getting, and I'm not going to run it in real time, although", "tokens": [400, 550, 510, 321, 434, 1242, 11, 293, 286, 478, 406, 516, 281, 1190, 309, 294, 957, 565, 11, 4878], "temperature": 0.0, "avg_logprob": -0.07115534444650014, "compression_ratio": 1.661904761904762, "no_speech_prob": 8.480556425638497e-05}, {"id": 689, "seek": 302878, "start": 3032.78, "end": 3038.78, "text": " it's not terrible, but it's more than I want to do in class.", "tokens": [309, 311, 406, 6237, 11, 457, 309, 311, 544, 813, 286, 528, 281, 360, 294, 1508, 13], "temperature": 0.0, "avg_logprob": -0.07115534444650014, "compression_ratio": 1.661904761904762, "no_speech_prob": 8.480556425638497e-05}, {"id": 690, "seek": 302878, "start": 3038.78, "end": 3042.78, "text": " We're getting much, much higher accuracy than we were before and a much higher", "tokens": [492, 434, 1242, 709, 11, 709, 2946, 14170, 813, 321, 645, 949, 293, 257, 709, 2946], "temperature": 0.0, "avg_logprob": -0.07115534444650014, "compression_ratio": 1.661904761904762, "no_speech_prob": 8.480556425638497e-05}, {"id": 691, "seek": 302878, "start": 3042.78, "end": 3044.78, "text": " blue score than we were before.", "tokens": [3344, 6175, 813, 321, 645, 949, 13], "temperature": 0.0, "avg_logprob": -0.07115534444650014, "compression_ratio": 1.661904761904762, "no_speech_prob": 8.480556425638497e-05}, {"id": 692, "seek": 302878, "start": 3044.78, "end": 3047.78, "text": " So this is working well.", "tokens": [407, 341, 307, 1364, 731, 13], "temperature": 0.0, "avg_logprob": -0.07115534444650014, "compression_ratio": 1.661904761904762, "no_speech_prob": 8.480556425638497e-05}, {"id": 693, "seek": 302878, "start": 3047.78, "end": 3052.78, "text": " And we can also see some predictions of what it predicts for the output.", "tokens": [400, 321, 393, 611, 536, 512, 21264, 295, 437, 309, 6069, 82, 337, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.07115534444650014, "compression_ratio": 1.661904761904762, "no_speech_prob": 8.480556425638497e-05}, {"id": 694, "seek": 305278, "start": 3052.78, "end": 3059.78, "text": " And so again, I think the second sentence is the target and the third is what we", "tokens": [400, 370, 797, 11, 286, 519, 264, 1150, 8174, 307, 264, 3779, 293, 264, 2636, 307, 437, 321], "temperature": 0.0, "avg_logprob": -0.07621268515891218, "compression_ratio": 1.6299559471365639, "no_speech_prob": 1.8924502001027577e-05}, {"id": 695, "seek": 305278, "start": 3059.78, "end": 3063.78, "text": " predicted.", "tokens": [19147, 13], "temperature": 0.0, "avg_logprob": -0.07621268515891218, "compression_ratio": 1.6299559471365639, "no_speech_prob": 1.8924502001027577e-05}, {"id": 696, "seek": 305278, "start": 3063.78, "end": 3067.78, "text": " While I go about maintaining this high degree of fitness, am I protected under", "tokens": [3987, 286, 352, 466, 14916, 341, 1090, 4314, 295, 15303, 11, 669, 286, 10594, 833], "temperature": 0.0, "avg_logprob": -0.07621268515891218, "compression_ratio": 1.6299559471365639, "no_speech_prob": 1.8924502001027577e-05}, {"id": 697, "seek": 305278, "start": 3067.78, "end": 3069.78, "text": " an insurance or pension plan?", "tokens": [364, 7214, 420, 21927, 1393, 30], "temperature": 0.0, "avg_logprob": -0.07621268515891218, "compression_ratio": 1.6299559471365639, "no_speech_prob": 1.8924502001027577e-05}, {"id": 698, "seek": 305278, "start": 3069.78, "end": 3073.78, "text": " While I do my physical, physical, physical, do I aware by the pension plan,", "tokens": [3987, 286, 360, 452, 4001, 11, 4001, 11, 4001, 11, 360, 286, 3650, 538, 264, 21927, 1393, 11], "temperature": 0.0, "avg_logprob": -0.07621268515891218, "compression_ratio": 1.6299559471365639, "no_speech_prob": 1.8924502001027577e-05}, {"id": 699, "seek": 305278, "start": 3073.78, "end": 3074.78, "text": " service plan?", "tokens": [2643, 1393, 30], "temperature": 0.0, "avg_logprob": -0.07621268515891218, "compression_ratio": 1.6299559471365639, "no_speech_prob": 1.8924502001027577e-05}, {"id": 700, "seek": 305278, "start": 3074.78, "end": 3079.78, "text": " So not great, but still better than some of the stuff we were seeing previously", "tokens": [407, 406, 869, 11, 457, 920, 1101, 813, 512, 295, 264, 1507, 321, 645, 2577, 8046], "temperature": 0.0, "avg_logprob": -0.07621268515891218, "compression_ratio": 1.6299559471365639, "no_speech_prob": 1.8924502001027577e-05}, {"id": 701, "seek": 307978, "start": 3079.78, "end": 3085.78, "text": " with our early seek to seeks.", "tokens": [365, 527, 2440, 8075, 281, 28840, 13], "temperature": 0.0, "avg_logprob": -0.11623833900274233, "compression_ratio": 1.4895833333333333, "no_speech_prob": 7.03064797562547e-05}, {"id": 702, "seek": 307978, "start": 3085.78, "end": 3088.78, "text": " Let me look at another.", "tokens": [961, 385, 574, 412, 1071, 13], "temperature": 0.0, "avg_logprob": -0.11623833900274233, "compression_ratio": 1.4895833333333333, "no_speech_prob": 7.03064797562547e-05}, {"id": 703, "seek": 307978, "start": 3088.78, "end": 3093.78, "text": " Where do the U.S., Canada, and unknown stand?", "tokens": [2305, 360, 264, 624, 13, 50, 7933, 6309, 11, 293, 9841, 1463, 30], "temperature": 0.0, "avg_logprob": -0.11623833900274233, "compression_ratio": 1.4895833333333333, "no_speech_prob": 7.03064797562547e-05}, {"id": 704, "seek": 307978, "start": 3093.78, "end": 3099.78, "text": " What is U.S. United and the unknown fit in?", "tokens": [708, 307, 624, 13, 50, 13, 2824, 293, 264, 9841, 3318, 294, 30], "temperature": 0.0, "avg_logprob": -0.11623833900274233, "compression_ratio": 1.4895833333333333, "no_speech_prob": 7.03064797562547e-05}, {"id": 705, "seek": 307978, "start": 3099.78, "end": 3102.78, "text": " Yeah, so it can be interesting to look at.", "tokens": [865, 11, 370, 309, 393, 312, 1880, 281, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.11623833900274233, "compression_ratio": 1.4895833333333333, "no_speech_prob": 7.03064797562547e-05}, {"id": 706, "seek": 307978, "start": 3102.78, "end": 3104.78, "text": " This one's pretty good.", "tokens": [639, 472, 311, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.11623833900274233, "compression_ratio": 1.4895833333333333, "no_speech_prob": 7.03064797562547e-05}, {"id": 707, "seek": 307978, "start": 3104.78, "end": 3107.78, "text": " What are some of the long-term policy implications of this global knowledge", "tokens": [708, 366, 512, 295, 264, 938, 12, 7039, 3897, 16602, 295, 341, 4338, 3601], "temperature": 0.0, "avg_logprob": -0.11623833900274233, "compression_ratio": 1.4895833333333333, "no_speech_prob": 7.03064797562547e-05}, {"id": 708, "seek": 310778, "start": 3107.78, "end": 3109.78, "text": " revolution?", "tokens": [8894, 30], "temperature": 0.0, "avg_logprob": -0.10112890330227939, "compression_ratio": 1.4666666666666666, "no_speech_prob": 4.683719453169033e-05}, {"id": 709, "seek": 310778, "start": 3109.78, "end": 3114.78, "text": " What are the long-term policy implications of this global scientific?", "tokens": [708, 366, 264, 938, 12, 7039, 3897, 16602, 295, 341, 4338, 8134, 30], "temperature": 0.0, "avg_logprob": -0.10112890330227939, "compression_ratio": 1.4666666666666666, "no_speech_prob": 4.683719453169033e-05}, {"id": 710, "seek": 310778, "start": 3114.78, "end": 3118.78, "text": " So I think that's pretty good.", "tokens": [407, 286, 519, 300, 311, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.10112890330227939, "compression_ratio": 1.4666666666666666, "no_speech_prob": 4.683719453169033e-05}, {"id": 711, "seek": 310778, "start": 3118.78, "end": 3125.78, "text": " And then the one other thing that Transformer uses is label smoothing.", "tokens": [400, 550, 264, 472, 661, 551, 300, 27938, 260, 4960, 307, 7645, 899, 6259, 571, 13], "temperature": 0.0, "avg_logprob": -0.10112890330227939, "compression_ratio": 1.4666666666666666, "no_speech_prob": 4.683719453169033e-05}, {"id": 712, "seek": 310778, "start": 3125.78, "end": 3130.78, "text": " And this is basically just the idea of since we're predicting predictions, often", "tokens": [400, 341, 307, 1936, 445, 264, 1558, 295, 1670, 321, 434, 32884, 21264, 11, 2049], "temperature": 0.0, "avg_logprob": -0.10112890330227939, "compression_ratio": 1.4666666666666666, "no_speech_prob": 4.683719453169033e-05}, {"id": 713, "seek": 313078, "start": 3130.78, "end": 3137.78, "text": " with softmax you're putting like 100% on the correct answer and 0 on everything", "tokens": [365, 2787, 41167, 291, 434, 3372, 411, 2319, 4, 322, 264, 3006, 1867, 293, 1958, 322, 1203], "temperature": 0.0, "avg_logprob": -0.10814761132309117, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.0001535534975118935}, {"id": 714, "seek": 313078, "start": 3137.78, "end": 3138.78, "text": " else.", "tokens": [1646, 13], "temperature": 0.0, "avg_logprob": -0.10814761132309117, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.0001535534975118935}, {"id": 715, "seek": 313078, "start": 3138.78, "end": 3143.78, "text": " And the idea of label smoothing is to put, say, 90% on the correct answer and", "tokens": [400, 264, 1558, 295, 7645, 899, 6259, 571, 307, 281, 829, 11, 584, 11, 4289, 4, 322, 264, 3006, 1867, 293], "temperature": 0.0, "avg_logprob": -0.10814761132309117, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.0001535534975118935}, {"id": 716, "seek": 313078, "start": 3143.78, "end": 3147.78, "text": " then evenly divide up the other 10% among all the other words.", "tokens": [550, 17658, 9845, 493, 264, 661, 1266, 4, 3654, 439, 264, 661, 2283, 13], "temperature": 0.0, "avg_logprob": -0.10814761132309117, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.0001535534975118935}, {"id": 717, "seek": 313078, "start": 3147.78, "end": 3151.78, "text": " And that can help improve the accuracy of your model.", "tokens": [400, 300, 393, 854, 3470, 264, 14170, 295, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10814761132309117, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.0001535534975118935}, {"id": 718, "seek": 313078, "start": 3151.78, "end": 3157.78, "text": " Because really it's kind of you want to get everything, you know, to a prediction", "tokens": [1436, 534, 309, 311, 733, 295, 291, 528, 281, 483, 1203, 11, 291, 458, 11, 281, 257, 17630], "temperature": 0.0, "avg_logprob": -0.10814761132309117, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.0001535534975118935}, {"id": 719, "seek": 315778, "start": 3157.78, "end": 3164.78, "text": " in the 90s is better than kind of trying to, you know, overconfident, get to one", "tokens": [294, 264, 4289, 82, 307, 1101, 813, 733, 295, 1382, 281, 11, 291, 458, 11, 670, 24697, 1078, 11, 483, 281, 472], "temperature": 0.0, "avg_logprob": -0.11446541625183898, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.00011959032417507842}, {"id": 720, "seek": 315778, "start": 3164.78, "end": 3167.78, "text": " on everything.", "tokens": [322, 1203, 13], "temperature": 0.0, "avg_logprob": -0.11446541625183898, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.00011959032417507842}, {"id": 721, "seek": 315778, "start": 3167.78, "end": 3174.78, "text": " There are questions about Transformer?", "tokens": [821, 366, 1651, 466, 27938, 260, 30], "temperature": 0.0, "avg_logprob": -0.11446541625183898, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.00011959032417507842}, {"id": 722, "seek": 315778, "start": 3174.78, "end": 3178.78, "text": " Let me just check the kind of status of the slides.", "tokens": [961, 385, 445, 1520, 264, 733, 295, 6558, 295, 264, 9788, 13], "temperature": 0.0, "avg_logprob": -0.11446541625183898, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.00011959032417507842}, {"id": 723, "seek": 315778, "start": 3178.78, "end": 3181.78, "text": " Here, this is just from the paper.", "tokens": [1692, 11, 341, 307, 445, 490, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.11446541625183898, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.00011959032417507842}, {"id": 724, "seek": 315778, "start": 3181.78, "end": 3185.78, "text": " They're explaining the motivation for positional encoding.", "tokens": [814, 434, 13468, 264, 12335, 337, 2535, 304, 43430, 13], "temperature": 0.0, "avg_logprob": -0.11446541625183898, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.00011959032417507842}, {"id": 725, "seek": 318578, "start": 3185.78, "end": 3189.78, "text": " Since our model contains no recurrence and no convolution, in order for the model", "tokens": [4162, 527, 2316, 8306, 572, 18680, 10760, 293, 572, 45216, 11, 294, 1668, 337, 264, 2316], "temperature": 0.0, "avg_logprob": -0.07379042971265184, "compression_ratio": 1.7981220657276995, "no_speech_prob": 3.763433051062748e-05}, {"id": 726, "seek": 318578, "start": 3189.78, "end": 3193.78, "text": " to make use of the order of the sequence, we must inject some information about", "tokens": [281, 652, 764, 295, 264, 1668, 295, 264, 8310, 11, 321, 1633, 10711, 512, 1589, 466], "temperature": 0.0, "avg_logprob": -0.07379042971265184, "compression_ratio": 1.7981220657276995, "no_speech_prob": 3.763433051062748e-05}, {"id": 727, "seek": 318578, "start": 3193.78, "end": 3197.78, "text": " the relative or absolute position of the tokens in the sequence.", "tokens": [264, 4972, 420, 8236, 2535, 295, 264, 22667, 294, 264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.07379042971265184, "compression_ratio": 1.7981220657276995, "no_speech_prob": 3.763433051062748e-05}, {"id": 728, "seek": 318578, "start": 3197.78, "end": 3200.78, "text": " So describing the motivation and the formula for it.", "tokens": [407, 16141, 264, 12335, 293, 264, 8513, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.07379042971265184, "compression_ratio": 1.7981220657276995, "no_speech_prob": 3.763433051062748e-05}, {"id": 729, "seek": 318578, "start": 3200.78, "end": 3205.78, "text": " And we saw the code version of this earlier.", "tokens": [400, 321, 1866, 264, 3089, 3037, 295, 341, 3071, 13], "temperature": 0.0, "avg_logprob": -0.07379042971265184, "compression_ratio": 1.7981220657276995, "no_speech_prob": 3.763433051062748e-05}, {"id": 730, "seek": 318578, "start": 3205.78, "end": 3214.78, "text": " But here, this is position and the dimension of the model.", "tokens": [583, 510, 11, 341, 307, 2535, 293, 264, 10139, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.07379042971265184, "compression_ratio": 1.7981220657276995, "no_speech_prob": 3.763433051062748e-05}, {"id": 731, "seek": 321478, "start": 3214.78, "end": 3218.78, "text": " Yeah, and I think that's it.", "tokens": [865, 11, 293, 286, 519, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.2443415323893229, "compression_ratio": 1.695906432748538, "no_speech_prob": 0.0002304632362211123}, {"id": 732, "seek": 321478, "start": 3218.78, "end": 3221.78, "text": " Yeah, so no, yes, question over there.", "tokens": [865, 11, 370, 572, 11, 2086, 11, 1168, 670, 456, 13], "temperature": 0.0, "avg_logprob": -0.2443415323893229, "compression_ratio": 1.695906432748538, "no_speech_prob": 0.0002304632362211123}, {"id": 733, "seek": 321478, "start": 3221.78, "end": 3228.78, "text": " What's the rationale for using triangular function to equal this position?", "tokens": [708, 311, 264, 41989, 337, 1228, 38190, 2445, 281, 2681, 341, 2535, 30], "temperature": 0.0, "avg_logprob": -0.2443415323893229, "compression_ratio": 1.695906432748538, "no_speech_prob": 0.0002304632362211123}, {"id": 734, "seek": 321478, "start": 3228.78, "end": 3232.78, "text": " Sorry, you said what's the rationale for using the triangle?", "tokens": [4919, 11, 291, 848, 437, 311, 264, 41989, 337, 1228, 264, 13369, 30], "temperature": 0.0, "avg_logprob": -0.2443415323893229, "compression_ratio": 1.695906432748538, "no_speech_prob": 0.0002304632362211123}, {"id": 735, "seek": 321478, "start": 3232.78, "end": 3236.78, "text": " Functions to equal positions.", "tokens": [11166, 3916, 281, 2681, 8432, 13], "temperature": 0.0, "avg_logprob": -0.2443415323893229, "compression_ratio": 1.695906432748538, "no_speech_prob": 0.0002304632362211123}, {"id": 736, "seek": 321478, "start": 3236.78, "end": 3239.78, "text": " By triangle function, do you mean sine and cosine?", "tokens": [3146, 13369, 2445, 11, 360, 291, 914, 18609, 293, 23565, 30], "temperature": 0.0, "avg_logprob": -0.2443415323893229, "compression_ratio": 1.695906432748538, "no_speech_prob": 0.0002304632362211123}, {"id": 737, "seek": 321478, "start": 3239.78, "end": 3242.78, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2443415323893229, "compression_ratio": 1.695906432748538, "no_speech_prob": 0.0002304632362211123}, {"id": 738, "seek": 324278, "start": 3242.78, "end": 3253.78, "text": " So I think it's just to get something that you can be unique and capture.", "tokens": [407, 286, 519, 309, 311, 445, 281, 483, 746, 300, 291, 393, 312, 3845, 293, 7983, 13], "temperature": 0.0, "avg_logprob": -0.11897624906946401, "compression_ratio": 1.5157232704402517, "no_speech_prob": 7.721402653260157e-05}, {"id": 739, "seek": 324278, "start": 3253.78, "end": 3257.78, "text": " And I think they even say in the paper that there are many different ways they", "tokens": [400, 286, 519, 436, 754, 584, 294, 264, 3035, 300, 456, 366, 867, 819, 2098, 436], "temperature": 0.0, "avg_logprob": -0.11897624906946401, "compression_ratio": 1.5157232704402517, "no_speech_prob": 7.721402653260157e-05}, {"id": 740, "seek": 324278, "start": 3257.78, "end": 3259.78, "text": " could have done the positional encoding.", "tokens": [727, 362, 1096, 264, 2535, 304, 43430, 13], "temperature": 0.0, "avg_logprob": -0.11897624906946401, "compression_ratio": 1.5157232704402517, "no_speech_prob": 7.721402653260157e-05}, {"id": 741, "seek": 324278, "start": 3259.78, "end": 3266.78, "text": " But to have this unique way of capturing order.", "tokens": [583, 281, 362, 341, 3845, 636, 295, 23384, 1668, 13], "temperature": 0.0, "avg_logprob": -0.11897624906946401, "compression_ratio": 1.5157232704402517, "no_speech_prob": 7.721402653260157e-05}, {"id": 742, "seek": 326678, "start": 3266.78, "end": 3272.78, "text": " Of like, this is generating this unique sequence that's kind of varying through 2D", "tokens": [2720, 411, 11, 341, 307, 17746, 341, 3845, 8310, 300, 311, 733, 295, 22984, 807, 568, 35], "temperature": 0.0, "avg_logprob": -0.1362515538930893, "compression_ratio": 1.5521472392638036, "no_speech_prob": 3.3735814213287085e-05}, {"id": 743, "seek": 326678, "start": 3272.78, "end": 3277.78, "text": " that'll be the same across batches.", "tokens": [300, 603, 312, 264, 912, 2108, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.1362515538930893, "compression_ratio": 1.5521472392638036, "no_speech_prob": 3.3735814213287085e-05}, {"id": 744, "seek": 326678, "start": 3277.78, "end": 3285.78, "text": " So like adding this to kind of all your French sentences, you're adding the same", "tokens": [407, 411, 5127, 341, 281, 733, 295, 439, 428, 5522, 16579, 11, 291, 434, 5127, 264, 912], "temperature": 0.0, "avg_logprob": -0.1362515538930893, "compression_ratio": 1.5521472392638036, "no_speech_prob": 3.3735814213287085e-05}, {"id": 745, "seek": 326678, "start": 3285.78, "end": 3295.78, "text": " thing and it's unique with the position and ordering.", "tokens": [551, 293, 309, 311, 3845, 365, 264, 2535, 293, 21739, 13], "temperature": 0.0, "avg_logprob": -0.1362515538930893, "compression_ratio": 1.5521472392638036, "no_speech_prob": 3.3735814213287085e-05}, {"id": 746, "seek": 329578, "start": 3295.78, "end": 3298.78, "text": " But yeah, I think that there are, I think they say this in the paper, that there", "tokens": [583, 1338, 11, 286, 519, 300, 456, 366, 11, 286, 519, 436, 584, 341, 294, 264, 3035, 11, 300, 456], "temperature": 0.0, "avg_logprob": -0.11565457219662874, "compression_ratio": 1.4333333333333333, "no_speech_prob": 0.00016596107161603868}, {"id": 747, "seek": 329578, "start": 3298.78, "end": 3304.78, "text": " are like many other ways they could have done positional encoding as well.", "tokens": [366, 411, 867, 661, 2098, 436, 727, 362, 1096, 2535, 304, 43430, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.11565457219662874, "compression_ratio": 1.4333333333333333, "no_speech_prob": 0.00016596107161603868}, {"id": 748, "seek": 329578, "start": 3304.78, "end": 3310.78, "text": " Other questions?", "tokens": [5358, 1651, 30], "temperature": 0.0, "avg_logprob": -0.11565457219662874, "compression_ratio": 1.4333333333333333, "no_speech_prob": 0.00016596107161603868}, {"id": 749, "seek": 331078, "start": 3310.78, "end": 3326.78, "text": " All right, let's take our five minute break, so let's meet back here at 12.05.", "tokens": [50364, 1057, 558, 11, 718, 311, 747, 527, 1732, 3456, 1821, 11, 370, 718, 311, 1677, 646, 510, 412, 2272, 13, 13328, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2073919105529785, "compression_ratio": 1.0129870129870129, "no_speech_prob": 0.0006331325857900083}], "language": "en"}