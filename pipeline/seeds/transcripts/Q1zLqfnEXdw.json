{"text": " Yeah, so I was going to start by revisiting Naive Bayes one more time in a spreadsheet. I realized last time I was like, oh, maybe this could give us a different perspective. And this is purely kind of a to get another perspective. I don't actually recommend that you use a spreadsheet to do this. And here I did some of the work behind the scenes in Python and a Jupyter notebook, but I just wanted to have a visual way of looking at it. And so what I did was I picked out some of the shortest reviews just because they would be easier to look at. And then I just picked out some of the words in them. So here we've got a review, this movie's so bad, I knew how it ends right after. We come over here, the class positive or negative, and I've sorted them so all the negative reviews are first. And then just some of the words and the word bad appears four times in this negative review. I guess if we can go up here, we can see more very bad acting, very bad plot, very bad movie. So a lot of very bads in here. Yeah, let me look through some of the other words I've got good and great. And for all of these, this is a term document matrix of how many times the word is showing up in each review. So we can scroll down to the bottom. And this should be available in the GitHub repo. I've calculated P0 and P1 here. And here P0 is just the sum of how often the word shows up in the negative reviews. P1 is how often it shows up in the positive reviews. So not surprisingly, bad shows up eight times in the negative reviews and zero times in the positive reviews. And again, I'm just looking at 40 particularly short reviews here. So this is not a ton of data. In practice, you wouldn't actually be using so few reviews to build your model. But I wanted something that was kind of easy to visualize in one place. Something that surprised me just with this small subset, I don't know if I would hold on a bigger one, is that good and great, I expected to kind of be much more common in the positive reviews. But here they're pretty similar, four goods in negative reviews and only three goods in positive reviews and so on. So you can look at those. It was kind of interesting, movie shows up six times in positive reviews but only once in negative reviews. So we get these raw counts and that was what was called P0 and P1 in the previous notebook. And then we calculate, these were the PR0 and PR1 and here that's just taking the count from before and dividing it by the total number of, in this case, negative reviews. And we've added one to both the numerator and to the denominator. And why do we add one? For numerical stability, that's right, to avoid dividing by zero or something very small. And then down here we get the, so we've done that with the negative reviews, with the positive reviews, and so in this case I had 17 negative reviews and 23 positive, so there's a slight difference. And you can think of this as kind of an approximate average of how often does SO show up on a per review basis for negative reviews. So 0.3 times you'll have SO on average for each one negative review. So we've got these averages and then we kind of put them together in the log ratio. So it's taking the log of this divided by that and get these values. And I've color coded them here just to kind of illustrate anything that's bigger than one, or sorry, anything bigger than zero really leans positive, anything below zero leans negative. And so you can see that with, that is definitely below zero, it leans negative. Let's look at what's, and some of this, this is a tiny data set, so it may not be accurate. I don't know that from really would lean positive a lot if you had a bigger data set, although that could be something to look into. But I just wanted to give you this as a way to visualize it. Any questions about this setup? Okay, so what we do is to use this as a model, so this row is what we are calling R in the Python notebook. We calculate B, which is the log of 23 divided by 17. So here we need to take into account that we had more positive reviews than negative reviews in our data set. And then I evaluated this just on our training data. And this is actually just part of our training data because I'm only using like 45 of the vocabulary words. But just to see how the model works, so this movie is so bad, very bad acting, very bad plot, very bad movie. We know the answer. And I just took the dot product of R with the word counts of how often each word showed up in the review. Added B, and then if that is less than zero, that means it predicted negative and it's correct since this was a negative review. So that's how you kind of apply it to make a prediction. Although, again, in practice, you wouldn't be making predictions on your training set, except if you wanted to look at your training error. It was interesting. I thought this was worked fairly well given what a tiny, tiny selection of words I was using. So I got 34 out of a 40 correct on the training set. Any questions about this, this visual approach? Why did I, so I said that things less than zero indicated that the word swung negative, greater than zero positive. Why was that the case? Why was zero the cutoff? What would be the significance of it being exactly zero? Do you have a, can I throw you the catch box? So the reason we, the cutoff is at zeros because since we're dealing with the log of the ratios, the log of one is zero. Exactly. Yeah. So we're looking at a log of ratios. So if the ratio was one, that would mean that it had showed up, was equally likely to show up in positive or negative reviews. Then we're taking the log of that, that maps to zero. Okay. And if you're interested in the, and I really just wanted you to see this visual, but if you were interested in the code of how I created it, I put that at the bottom of notebook 3B. Just what I ran to get those. And as I said, I just chose the 40 shortest reviews to look at. And then what I did is I chose vocabulary words that were used at least six times, but less than 30 because I thought that would be interesting to get words that were not super rare, but also not super common. Any more, any more questions about naive Bayes before we go, go on to regex, which we kind of started last time briefly. All right. So let's, let's go to regex. So I kind of open last time motivating with this phone number problem and showing kind of you could start trying to do it without regex, but it was quickly creating kind of this whole system of branches with a lot of if else's in different cases to consider. And so regex will give us a cleaner and more effective way to deal with this because even with my branching ifs in different cases, I still was not capturing capturing that match of the phone number and just I guess to highlight the phone number problem. What it is is that we're getting phone numbers with a lot of different formats. They might have hyphens, they might have parentheses or spaces, but there is a general pattern that we recognize with our eyes of what makes a phone number. We can tell when data is wrong, you know that this is actually an address. We don't want to don't want to pick it out. So how can we kind of generalize those patterns and pick them out. So we're going to in a moment we're going to take some time to practice because I think regex is really just something you have to kind of practice to get otherwise it seems like a lot of pretty obtruse rules. So I showed here that backslash lowercase d is a digit zero to nine phone number with hyphens you could write backslash d embraces three that signifying you want three of those dash three more dash four more in a moment will kind of build up to how to take into account. What if you have a space instead of a dash what if you have parentheses regex can deal with those as well. Just useful useful things to know some unexact quantifiers question mark means zero or one star means zero or more plus sign is one or more. So then I would say the only way to do this is to learn through practice I want us to take 15 minutes to start on your own going through regex one. But first I want to just show you and actually let me check who here has used regex before raise your hand. Okay fair number who's used it a lot. Just Jeremy. Okay. So some of this some of this might be review review for you. So this is the site regex one. It's nice it gives you a few patterns that you're trying to match and in the later ones it'll also have ones that you explicitly don't want to match something to be careful of just with this. The site is notice here if I just put a it's giving me three check marks I have matched these because they all have an A in them but you with regex you want to be as specific as possible. And here it seems like everything I want has ABC and it would be better to write out that full ABC because later on you know I could be fed different different examples and you want to be as specific as possible. So that's kind of my one caution as you're practicing here. You don't necessarily just want to do the first thing you get you kind of want to see have I have I gotten like the fullest answer I can. Yeah, so this is nice and it explains you can read kind of it introduces one or two roles at a time to practice. So I'm going to take 15 minutes to work through this and see see how far you get. This is the website regex one spelled out o ne.com. If you, if you're more advanced with regex and want something a little bit harder. I think it's regex Tuesdays. Regex Tuesdays are a little bit more challenging ones. And here's neat they have test cases, kind of a greater number of test cases of what you want to hit or not. And the only tricky thing with this is that see I want to, I think you have to put slashes before and after. Alright, so and feel free to kind of raise your hand as you get started if you have questions but we'll, we'll discuss in 15 minutes kind of how far you got and what you saw. And then one more regex resource that Jeremy recommended and I've tried this out is the regex crossword. And it's, it's pretty tricky. Something that's different about it is that you're. It's kind of in reverse for most regex so instead of writing the regex you're seeing to regex expressions and figuring out what characters would meet them both. But if you're wanting to think about regex in a different way this could be a resource as well. So if you're shaky at all with regex I would recommend starting with this regex one. Okay, so let's let's pause and discuss actually out of curiosity, how far did you get on regex one typically. Oh great. Awesome. Anyone else want to share how far they got. Any any favorite. Any favorite meta characters. I just wanted to highlight, make sure you particularly saw capture groups are really useful so this is when you don't want to just match something but you want to be able to extract part of part of what you found and so here. And I love that this is a very kind of real world example, given a list of file names, you're wanting to pick out the PDFs and what comes directly before PDF. And so here, using using parentheses will capture the group, so you're not just kind of identifying this is a PDF file but you can get hey this is what came before the dot PDF the file name. So I wanted to showcase that. Any any questions on regex or what you saw. Let's go back to notebook for in that case. So pros and cons. Some of the advantages are that it's very, very concise and powerful. It's supported by many different programming languages have regex in them it's not, not just a Python thing. The disadvantages can be brittle, and it can be hard to write and hard to read, and I think that's kind of the flip side for many things that are concise part of concision is that yeah it can be harder harder to read later. So the next step kind of real world use case I wanted to look at was revisiting tokenization. So here we're going to pick out a few different things and so this is still we're still looking at text processing and actually just don't remind me what what is a token. Yeah, so each word is a token it's kind of the the fundamental units that you want to look at so kind of breaking this into the units which for our purpose are generally words and doing that in a standard way. And typically when we tokenize something will also be coming up with IDs, so that we can map between, you know, inner jerk IDs and our vocabulary of tokens for the data we're looking at. And so, something to note about tokenization is there's not just one way to do it really depends on the implementation and depends on kind of what what you want. What your data set to be like, and so this is just kind of an illustration of one approach you could take, but here, say we want to add spaces around punctuation, so we can kind of pick out all the all the different forms of punctuation. We also want to pick out. Didn't don't wasn't wouldn't and kind of has a specific meaning right it's a shortened version of not. So we might want to pick that out apostrophe s has a particular meaning, showing the possessive of something. So, Rachel's class. Sarah's office, you know that apostrophe s has meaning so that's something else we might want to pick out. And then say we want to replace multiple spaces with just one, because ultimately. Thank you typically don't use white space as a token. So here we're going to be using from the pythons re library read dot compile to create these red boxes, and then dot sub to substitute in what we want. So here, writing a method that will take a sentence for punctuation. So, let's see what we can do with this. Any ideas. So, we're going to use the same method that we used to capture the first group so here for. This is for the punctuation one. We have, we have parentheses around the outside. So we're going to. And that indicates. Capture group. So, we're saying, you know, if you found, found a dot or colon, put in that call, put back in space, what you captured, which would be the colon and then another space. Then we're going to make an apostrophe T. Previously, since we've inserted all these spaces we had inserted spaces on either side, we're going to condense that back to the apostrophe T. So we'll substitute that in ditto with apostrophe S. We've inserted a space when we did this original add space around the tokens. Put that back to apostrophe S. So looking, it's probably easiest to see this on an example. Looking at, I don't know who Kara's new friend is, is it quote Mr. Toad. Sorry for the, I was just trying to think of something that had some various apostrophe S and an apostrophe T in it and quote marks so we could see how these things work. We can see, well, this is the final output we'll end up with. I, in this case, do. This is different than not. I mean, you also have not in your set, but an apostrophe T. We've added spaces around the double hyphen, spaces around the quote mark. Kara's, we've got the apostrophe S and then we can also run through this kind of line by line to see what each line is doing. So here that first substitution when you find the punctuation has added spaces around it. Now we've condensed an apostrophe T back from here where it had space. We've condensed apostrophe S back and then we've replaced multiple spaces with a single one. Any questions about, about this process? Yes. Hi. Okay. So yeah, that's a good question. When would you want to create your own rather than using a library like Space C? I think in most scenarios you put would probably want to use Space C. I think this would only be if you wanted to test something specific. I mean, I guess like with fast AI. Jeremy did want to look at, you know, some of the things I mentioned in. This was lesson three. Find this tokenization. So some of these ideas of indicating that the next word is capital indicating repetition. I don't know that this is necessarily implemented in Space C. And so here it was something like, oh, we want to test this with fast AI. So we would need a way to implement this ourselves. So if you had some particular hypothesis, you might. But yeah, I feel like in most purposes use Space C's tokenizer. And this is more kind of just to understand a way that you could do it, but you probably won't need to in practice. That's a good question. I'm not sure. Do you know, Jeremy? A combination. And just to repeat that for the microphone, Jeremy was saying that libraries such as Space C typically use a combination. Maybe using regex to parse URLs, but to have parsers in C for many other things. Welcome. Thanks. Other questions about this? Yeah, so this is something this is more kind of to know a way that you could do this, but you probably won't need to, for instance, like in a workplace project. Although you might be using regex for other things. Like I think the some of the examples earlier of cleaning up data, like, you know, if you have this messy column of phone numbers, like that's definitely an application. You could see the cleaning up the file names where they were trying to pick off the what came before the ending. And you might have something where, you know, you're trying to find all the JPEGs and PNGs and need to filter out other file types. So regex could definitely be useful for that. So here I wanted to show the second step as well. So kind of we have this first step of generating our tokens. And then I mentioned pretty much always once you have tokens, you want to use those to come up with your vocabulary and to come up with this mapping from tokens to IDs and translating the texture working at into a sequence of IDs. So that's that's what you'll actually be processing. So here I have a list of six sentences. I can apply. So here I'm mapping my simple tokens method from right here to the sentences and creating a list with that. So here I have a list of tokens and then I have a tokens to ID method that can take the sentences and create a vocabulary. And so kind of typical things you'll often want to get back from this would be the IDs, which here is the kind of the format of rewriting my sentences as a sequence of tokens. So here you can check this first sentence has one, two, three, four, five, six, seven, eight. Does that fit with? Oh, it's returning them in reverse order, I think. Yeah. Actually, maybe, maybe that is not not in order at all. We could convert these convert these back to two words if we wanted to check kind of what do you what do you get back? We'll want to know what our vocabulary is. So here's a list of the vocab. And what's what could be another name of this vocab variable? This is something we've talked about previously. I heard someone say IDs. Do they want to say it louder? Yeah, ID to word or ID to token. So this list is giving us a map of how we get from a number to a token. And we could look it up if you chose four, zero, one, two, three, four, vocab four is going to give you I. That's a way to get from your IDs to your tokens. And the reason this is a list is the the integers kind of implicit in how you store a list. Whereas when we go words to ID or tokens to ID, we'll use a dictionary. And you can see, yes, I I was mapped to four and having that token to ID and ID to token are kind of two useful mappings you'll typically want. So you can get back and forth. Actually, let me just add. Let's go through one of these sentences. So to check out the first sentence, we've got this. If we wanted to make that back into a sentence, we would want to use vocab I for I and IDs. All this happened, more or less. So that's given us back our sentence. So that's how we can can get between them. Any questions about that? OK, so that's a few more things to know about. Redjacks, it's very useful finding and searching, finding and replacing. I should mention that in in Python with the re library, there's a redot search, which is kind of a useful method. Above, we were using redot compile and redot substitute. But redot search is another one you'll see a lot of. And cleaning data, I think, is definitely a big one, particularly when you have something that's supposed to have a somewhat generalizable format such as phone numbers. Python does have some string methods which you can use. To find the first occurrence of something, string methods might be a little bit clearer to read. However, Redjacks can handle broader use cases. Redjacks can be language independent. You'll see versions of it kind of in so many different languages. And it can also be faster at scale. This was just for fun. This is, I guess, not NLP, but I did Redjacks on Unicode. And let me zoom in a little bit so you can see this. So here I have a message of frowny face about the movie and nauseated about the pizza. And I wanted to try substituting smiley faces in for the frowning and nauseated faces. And did that and got back the message, smiling about the movie and smiling about the pizza. What is the pipe doing here in my regular expression? That's right, or. So here this is looking for either of those two upset faces. And then I use substitute to get a smiley face in. Any question about this? And there was a question. The reason I thought of this was someone asked last time about Redjacks and UTF-8. And it should work fine. Python 3 handles Unicode. All right. So Redjacks errors. So you have to worry about both false positives and false negatives. And that's something I like that Redjacks 1 and Redjacks Tuesdays give you in terms of having like these are the cases you do want to match. And then these are the cases you do not want to match. And it's important to look at both. Trying to have tests for both to minimize your false positives and minimize your false negatives. Here are a few more useful websites. Redjacks R is kind of neat too. When you. Oh, and I meant to copy it. You can put in so earlier when we were talking about the phone numbers. And maybe we want to have either a space or a hyphen here and then digits four. This is kind of neat how it lets you. You can kind of look at the different groups. Also, I think if you have a. So what you can do is enter the things you're trying to match. And you'll see, see if it's worked. And then you can add one more. Oh, I want to have. So what am I doing wrong on this that it's not. Ah, thank you. Yes, that is a crucial part of phone numbers. Three digits, three, four. So it's kind of neat. It'll highlight what it's matching so you can see. Also, if you pick off character groups. So again, that's where you use the parentheses to pick a group. I think it even then shows you. Yeah, the capture group is one, two, three of what it's capturing. Any questions about this? I think these tools are kind of can be nice visualizations. If you're and I would particularly recommend if you were trying to code a redjacks, I would do it. And one of these editors with some positive and negative test cases and then copy it into your code potentially. But I think this is good when you're experimenting. This is a tweet that Brian Spearing had found saying when I meet someone with super advanced redjacks skills, I can't help but assume they've had a hard, a hard life. It's the coder equivalent of facial tats. I thought that was funny. And then this person has hopefully drawn some redjacks tattoos on their face. There are a number of X KCDs involving involving redjacks redjacks golf. You try to match one group, but not the other meta redjacks golf. Try to play redjacks golf with arbitrary list. But I lost my code, so I'm grepping for files that look like redjacks golf solvers. That's meta, meta redjacks golf. So lots of lots of jokes about redjacks. And then to review some of the just the terms and the target string is what you'll be what you're trying to match or find. A literal is any character used in a search or matching expression where you're kind of literally trying to find that string as opposed to the meta characters, which are characters that have special meaning and are not used as literals in the search expression. For instance, period is kind of wild card that means any character. So how do you indicate if you actually want to period? Yes, put a backslash before it, which is the next one here. Escape sequence and escape sequence is a way of indicating that we want to use a meta character as a literal. Yeah. And so when working with redjacks, you'll typically want to create create a pattern in plain English, mapped the redjacks language. You definitely want to check your false positives and your false negatives, making sure that you're not picking up things you don't want to pick up. I think that can happen more with redjacks than with other types of programming. And it's like this tip of it's OK to filter before and after as well. Like redjacks is kind of very much a tool. You don't have to try to use it end to end, kind of use it where it's useful, but use it in conjunction with other things. And you'll saw I think above there was a point where we were still using Python split to split our tokens apart. You don't have to try to do everything with the redjacks. Any questions about redjacks? I will and I'm going to try to post the next homework soon and I'll email you when I do. And you'll always have at least a week from when I post the homework to when it's due, but I'll try to include some redjacks in that. I think this is a good point maybe to stop for our break, so let's meet back at 12 o'clock and we'll start Notebook 5.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.8, "text": " Yeah, so I was going to start by revisiting Naive Bayes one more time in a spreadsheet.", "tokens": [865, 11, 370, 286, 390, 516, 281, 722, 538, 20767, 1748, 6056, 488, 7840, 279, 472, 544, 565, 294, 257, 27733, 13], "temperature": 0.0, "avg_logprob": -0.2035643536111583, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.007118525914847851}, {"id": 1, "seek": 0, "start": 7.8, "end": 12.0, "text": " I realized last time I was like, oh, maybe this could give us a different perspective.", "tokens": [286, 5334, 1036, 565, 286, 390, 411, 11, 1954, 11, 1310, 341, 727, 976, 505, 257, 819, 4585, 13], "temperature": 0.0, "avg_logprob": -0.2035643536111583, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.007118525914847851}, {"id": 2, "seek": 0, "start": 12.0, "end": 15.9, "text": " And this is purely kind of a to get another perspective.", "tokens": [400, 341, 307, 17491, 733, 295, 257, 281, 483, 1071, 4585, 13], "temperature": 0.0, "avg_logprob": -0.2035643536111583, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.007118525914847851}, {"id": 3, "seek": 0, "start": 15.9, "end": 18.8, "text": " I don't actually recommend that you use a spreadsheet to do this.", "tokens": [286, 500, 380, 767, 2748, 300, 291, 764, 257, 27733, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.2035643536111583, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.007118525914847851}, {"id": 4, "seek": 0, "start": 18.8, "end": 23.5, "text": " And here I did some of the work behind the scenes in Python and a Jupyter notebook,", "tokens": [400, 510, 286, 630, 512, 295, 264, 589, 2261, 264, 8026, 294, 15329, 293, 257, 22125, 88, 391, 21060, 11], "temperature": 0.0, "avg_logprob": -0.2035643536111583, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.007118525914847851}, {"id": 5, "seek": 0, "start": 23.5, "end": 26.0, "text": " but I just wanted to have a visual way of looking at it.", "tokens": [457, 286, 445, 1415, 281, 362, 257, 5056, 636, 295, 1237, 412, 309, 13], "temperature": 0.0, "avg_logprob": -0.2035643536111583, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.007118525914847851}, {"id": 6, "seek": 2600, "start": 26.0, "end": 32.5, "text": " And so what I did was I picked out some of the shortest reviews just because they would be easier to look at.", "tokens": [400, 370, 437, 286, 630, 390, 286, 6183, 484, 512, 295, 264, 31875, 10229, 445, 570, 436, 576, 312, 3571, 281, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.14728626128165953, "compression_ratio": 1.6435185185185186, "no_speech_prob": 8.80060724739451e-06}, {"id": 7, "seek": 2600, "start": 32.5, "end": 38.0, "text": " And then I just picked out some of the words in them.", "tokens": [400, 550, 286, 445, 6183, 484, 512, 295, 264, 2283, 294, 552, 13], "temperature": 0.0, "avg_logprob": -0.14728626128165953, "compression_ratio": 1.6435185185185186, "no_speech_prob": 8.80060724739451e-06}, {"id": 8, "seek": 2600, "start": 38.0, "end": 42.5, "text": " So here we've got a review, this movie's so bad, I knew how it ends right after.", "tokens": [407, 510, 321, 600, 658, 257, 3131, 11, 341, 3169, 311, 370, 1578, 11, 286, 2586, 577, 309, 5314, 558, 934, 13], "temperature": 0.0, "avg_logprob": -0.14728626128165953, "compression_ratio": 1.6435185185185186, "no_speech_prob": 8.80060724739451e-06}, {"id": 9, "seek": 2600, "start": 42.5, "end": 50.5, "text": " We come over here, the class positive or negative, and I've sorted them so all the negative reviews are first.", "tokens": [492, 808, 670, 510, 11, 264, 1508, 3353, 420, 3671, 11, 293, 286, 600, 25462, 552, 370, 439, 264, 3671, 10229, 366, 700, 13], "temperature": 0.0, "avg_logprob": -0.14728626128165953, "compression_ratio": 1.6435185185185186, "no_speech_prob": 8.80060724739451e-06}, {"id": 10, "seek": 5050, "start": 50.5, "end": 59.0, "text": " And then just some of the words and the word bad appears four times in this negative review.", "tokens": [400, 550, 445, 512, 295, 264, 2283, 293, 264, 1349, 1578, 7038, 1451, 1413, 294, 341, 3671, 3131, 13], "temperature": 0.0, "avg_logprob": -0.12045412528805616, "compression_ratio": 1.6132596685082874, "no_speech_prob": 1.428478481102502e-05}, {"id": 11, "seek": 5050, "start": 59.0, "end": 65.0, "text": " I guess if we can go up here, we can see more very bad acting, very bad plot, very bad movie.", "tokens": [286, 2041, 498, 321, 393, 352, 493, 510, 11, 321, 393, 536, 544, 588, 1578, 6577, 11, 588, 1578, 7542, 11, 588, 1578, 3169, 13], "temperature": 0.0, "avg_logprob": -0.12045412528805616, "compression_ratio": 1.6132596685082874, "no_speech_prob": 1.428478481102502e-05}, {"id": 12, "seek": 5050, "start": 65.0, "end": 70.5, "text": " So a lot of very bads in here.", "tokens": [407, 257, 688, 295, 588, 1578, 82, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.12045412528805616, "compression_ratio": 1.6132596685082874, "no_speech_prob": 1.428478481102502e-05}, {"id": 13, "seek": 5050, "start": 70.5, "end": 76.0, "text": " Yeah, let me look through some of the other words I've got good and great.", "tokens": [865, 11, 718, 385, 574, 807, 512, 295, 264, 661, 2283, 286, 600, 658, 665, 293, 869, 13], "temperature": 0.0, "avg_logprob": -0.12045412528805616, "compression_ratio": 1.6132596685082874, "no_speech_prob": 1.428478481102502e-05}, {"id": 14, "seek": 7600, "start": 76.0, "end": 84.0, "text": " And for all of these, this is a term document matrix of how many times the word is showing up in each review.", "tokens": [400, 337, 439, 295, 613, 11, 341, 307, 257, 1433, 4166, 8141, 295, 577, 867, 1413, 264, 1349, 307, 4099, 493, 294, 1184, 3131, 13], "temperature": 0.0, "avg_logprob": -0.11987719684839249, "compression_ratio": 1.3511904761904763, "no_speech_prob": 4.157259354542475e-06}, {"id": 15, "seek": 7600, "start": 84.0, "end": 88.0, "text": " So we can scroll down to the bottom.", "tokens": [407, 321, 393, 11369, 760, 281, 264, 2767, 13], "temperature": 0.0, "avg_logprob": -0.11987719684839249, "compression_ratio": 1.3511904761904763, "no_speech_prob": 4.157259354542475e-06}, {"id": 16, "seek": 7600, "start": 88.0, "end": 92.5, "text": " And this should be available in the GitHub repo.", "tokens": [400, 341, 820, 312, 2435, 294, 264, 23331, 49040, 13], "temperature": 0.0, "avg_logprob": -0.11987719684839249, "compression_ratio": 1.3511904761904763, "no_speech_prob": 4.157259354542475e-06}, {"id": 17, "seek": 7600, "start": 92.5, "end": 96.5, "text": " I've calculated P0 and P1 here.", "tokens": [286, 600, 15598, 430, 15, 293, 430, 16, 510, 13], "temperature": 0.0, "avg_logprob": -0.11987719684839249, "compression_ratio": 1.3511904761904763, "no_speech_prob": 4.157259354542475e-06}, {"id": 18, "seek": 9650, "start": 96.5, "end": 106.0, "text": " And here P0 is just the sum of how often the word shows up in the negative reviews.", "tokens": [400, 510, 430, 15, 307, 445, 264, 2408, 295, 577, 2049, 264, 1349, 3110, 493, 294, 264, 3671, 10229, 13], "temperature": 0.0, "avg_logprob": -0.0588529843550462, "compression_ratio": 1.8092485549132948, "no_speech_prob": 1.0676892543415306e-06}, {"id": 19, "seek": 9650, "start": 106.0, "end": 115.0, "text": " P1 is how often it shows up in the positive reviews.", "tokens": [430, 16, 307, 577, 2049, 309, 3110, 493, 294, 264, 3353, 10229, 13], "temperature": 0.0, "avg_logprob": -0.0588529843550462, "compression_ratio": 1.8092485549132948, "no_speech_prob": 1.0676892543415306e-06}, {"id": 20, "seek": 9650, "start": 115.0, "end": 121.5, "text": " So not surprisingly, bad shows up eight times in the negative reviews and zero times in the positive reviews.", "tokens": [407, 406, 17600, 11, 1578, 3110, 493, 3180, 1413, 294, 264, 3671, 10229, 293, 4018, 1413, 294, 264, 3353, 10229, 13], "temperature": 0.0, "avg_logprob": -0.0588529843550462, "compression_ratio": 1.8092485549132948, "no_speech_prob": 1.0676892543415306e-06}, {"id": 21, "seek": 9650, "start": 121.5, "end": 125.0, "text": " And again, I'm just looking at 40 particularly short reviews here.", "tokens": [400, 797, 11, 286, 478, 445, 1237, 412, 3356, 4098, 2099, 10229, 510, 13], "temperature": 0.0, "avg_logprob": -0.0588529843550462, "compression_ratio": 1.8092485549132948, "no_speech_prob": 1.0676892543415306e-06}, {"id": 22, "seek": 12500, "start": 125.0, "end": 127.0, "text": " So this is not a ton of data.", "tokens": [407, 341, 307, 406, 257, 2952, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.061875149607658386, "compression_ratio": 1.5560165975103735, "no_speech_prob": 1.544524639029987e-05}, {"id": 23, "seek": 12500, "start": 127.0, "end": 131.5, "text": " In practice, you wouldn't actually be using so few reviews to build your model.", "tokens": [682, 3124, 11, 291, 2759, 380, 767, 312, 1228, 370, 1326, 10229, 281, 1322, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.061875149607658386, "compression_ratio": 1.5560165975103735, "no_speech_prob": 1.544524639029987e-05}, {"id": 24, "seek": 12500, "start": 131.5, "end": 136.0, "text": " But I wanted something that was kind of easy to visualize in one place.", "tokens": [583, 286, 1415, 746, 300, 390, 733, 295, 1858, 281, 23273, 294, 472, 1081, 13], "temperature": 0.0, "avg_logprob": -0.061875149607658386, "compression_ratio": 1.5560165975103735, "no_speech_prob": 1.544524639029987e-05}, {"id": 25, "seek": 12500, "start": 136.0, "end": 141.0, "text": " Something that surprised me just with this small subset, I don't know if I would hold on a bigger one,", "tokens": [6595, 300, 6100, 385, 445, 365, 341, 1359, 25993, 11, 286, 500, 380, 458, 498, 286, 576, 1797, 322, 257, 3801, 472, 11], "temperature": 0.0, "avg_logprob": -0.061875149607658386, "compression_ratio": 1.5560165975103735, "no_speech_prob": 1.544524639029987e-05}, {"id": 26, "seek": 12500, "start": 141.0, "end": 148.5, "text": " is that good and great, I expected to kind of be much more common in the positive reviews.", "tokens": [307, 300, 665, 293, 869, 11, 286, 5176, 281, 733, 295, 312, 709, 544, 2689, 294, 264, 3353, 10229, 13], "temperature": 0.0, "avg_logprob": -0.061875149607658386, "compression_ratio": 1.5560165975103735, "no_speech_prob": 1.544524639029987e-05}, {"id": 27, "seek": 14850, "start": 148.5, "end": 160.0, "text": " But here they're pretty similar, four goods in negative reviews and only three goods in positive reviews and so on.", "tokens": [583, 510, 436, 434, 1238, 2531, 11, 1451, 10179, 294, 3671, 10229, 293, 787, 1045, 10179, 294, 3353, 10229, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.12240444321230233, "compression_ratio": 1.6965174129353233, "no_speech_prob": 4.71080875286134e-06}, {"id": 28, "seek": 14850, "start": 160.0, "end": 162.0, "text": " So you can look at those.", "tokens": [407, 291, 393, 574, 412, 729, 13], "temperature": 0.0, "avg_logprob": -0.12240444321230233, "compression_ratio": 1.6965174129353233, "no_speech_prob": 4.71080875286134e-06}, {"id": 29, "seek": 14850, "start": 162.0, "end": 168.0, "text": " It was kind of interesting, movie shows up six times in positive reviews but only once in negative reviews.", "tokens": [467, 390, 733, 295, 1880, 11, 3169, 3110, 493, 2309, 1413, 294, 3353, 10229, 457, 787, 1564, 294, 3671, 10229, 13], "temperature": 0.0, "avg_logprob": -0.12240444321230233, "compression_ratio": 1.6965174129353233, "no_speech_prob": 4.71080875286134e-06}, {"id": 30, "seek": 14850, "start": 168.0, "end": 176.5, "text": " So we get these raw counts and that was what was called P0 and P1 in the previous notebook.", "tokens": [407, 321, 483, 613, 8936, 14893, 293, 300, 390, 437, 390, 1219, 430, 15, 293, 430, 16, 294, 264, 3894, 21060, 13], "temperature": 0.0, "avg_logprob": -0.12240444321230233, "compression_ratio": 1.6965174129353233, "no_speech_prob": 4.71080875286134e-06}, {"id": 31, "seek": 17650, "start": 176.5, "end": 185.0, "text": " And then we calculate, these were the PR0 and PR1 and here that's just taking the count from before", "tokens": [400, 550, 321, 8873, 11, 613, 645, 264, 11568, 15, 293, 11568, 16, 293, 510, 300, 311, 445, 1940, 264, 1207, 490, 949], "temperature": 0.0, "avg_logprob": -0.09717602729797363, "compression_ratio": 1.4772727272727273, "no_speech_prob": 5.337902166502317e-06}, {"id": 32, "seek": 17650, "start": 185.0, "end": 190.5, "text": " and dividing it by the total number of, in this case, negative reviews.", "tokens": [293, 26764, 309, 538, 264, 3217, 1230, 295, 11, 294, 341, 1389, 11, 3671, 10229, 13], "temperature": 0.0, "avg_logprob": -0.09717602729797363, "compression_ratio": 1.4772727272727273, "no_speech_prob": 5.337902166502317e-06}, {"id": 33, "seek": 17650, "start": 190.5, "end": 195.0, "text": " And we've added one to both the numerator and to the denominator.", "tokens": [400, 321, 600, 3869, 472, 281, 1293, 264, 30380, 293, 281, 264, 20687, 13], "temperature": 0.0, "avg_logprob": -0.09717602729797363, "compression_ratio": 1.4772727272727273, "no_speech_prob": 5.337902166502317e-06}, {"id": 34, "seek": 17650, "start": 195.0, "end": 199.5, "text": " And why do we add one?", "tokens": [400, 983, 360, 321, 909, 472, 30], "temperature": 0.0, "avg_logprob": -0.09717602729797363, "compression_ratio": 1.4772727272727273, "no_speech_prob": 5.337902166502317e-06}, {"id": 35, "seek": 19950, "start": 199.5, "end": 209.5, "text": " For numerical stability, that's right, to avoid dividing by zero or something very small.", "tokens": [1171, 29054, 11826, 11, 300, 311, 558, 11, 281, 5042, 26764, 538, 4018, 420, 746, 588, 1359, 13], "temperature": 0.0, "avg_logprob": -0.0985133375440325, "compression_ratio": 1.565217391304348, "no_speech_prob": 9.368496648676228e-06}, {"id": 36, "seek": 19950, "start": 209.5, "end": 214.5, "text": " And then down here we get the, so we've done that with the negative reviews, with the positive reviews,", "tokens": [400, 550, 760, 510, 321, 483, 264, 11, 370, 321, 600, 1096, 300, 365, 264, 3671, 10229, 11, 365, 264, 3353, 10229, 11], "temperature": 0.0, "avg_logprob": -0.0985133375440325, "compression_ratio": 1.565217391304348, "no_speech_prob": 9.368496648676228e-06}, {"id": 37, "seek": 19950, "start": 214.5, "end": 220.0, "text": " and so in this case I had 17 negative reviews and 23 positive, so there's a slight difference.", "tokens": [293, 370, 294, 341, 1389, 286, 632, 3282, 3671, 10229, 293, 6673, 3353, 11, 370, 456, 311, 257, 4036, 2649, 13], "temperature": 0.0, "avg_logprob": -0.0985133375440325, "compression_ratio": 1.565217391304348, "no_speech_prob": 9.368496648676228e-06}, {"id": 38, "seek": 22000, "start": 220.0, "end": 232.5, "text": " And you can think of this as kind of an approximate average of how often does SO show up on a per review basis for negative reviews.", "tokens": [400, 291, 393, 519, 295, 341, 382, 733, 295, 364, 30874, 4274, 295, 577, 2049, 775, 10621, 855, 493, 322, 257, 680, 3131, 5143, 337, 3671, 10229, 13], "temperature": 0.0, "avg_logprob": -0.12005894713931614, "compression_ratio": 1.5833333333333333, "no_speech_prob": 4.222763436700916e-06}, {"id": 39, "seek": 22000, "start": 232.5, "end": 241.0, "text": " So 0.3 times you'll have SO on average for each one negative review.", "tokens": [407, 1958, 13, 18, 1413, 291, 603, 362, 10621, 322, 4274, 337, 1184, 472, 3671, 3131, 13], "temperature": 0.0, "avg_logprob": -0.12005894713931614, "compression_ratio": 1.5833333333333333, "no_speech_prob": 4.222763436700916e-06}, {"id": 40, "seek": 22000, "start": 241.0, "end": 247.0, "text": " So we've got these averages and then we kind of put them together in the log ratio.", "tokens": [407, 321, 600, 658, 613, 42257, 293, 550, 321, 733, 295, 829, 552, 1214, 294, 264, 3565, 8509, 13], "temperature": 0.0, "avg_logprob": -0.12005894713931614, "compression_ratio": 1.5833333333333333, "no_speech_prob": 4.222763436700916e-06}, {"id": 41, "seek": 24700, "start": 247.0, "end": 252.5, "text": " So it's taking the log of this divided by that and get these values.", "tokens": [407, 309, 311, 1940, 264, 3565, 295, 341, 6666, 538, 300, 293, 483, 613, 4190, 13], "temperature": 0.0, "avg_logprob": -0.1047612155776426, "compression_ratio": 1.7128205128205127, "no_speech_prob": 7.2962584454216994e-06}, {"id": 42, "seek": 24700, "start": 252.5, "end": 260.0, "text": " And I've color coded them here just to kind of illustrate anything that's bigger than one,", "tokens": [400, 286, 600, 2017, 34874, 552, 510, 445, 281, 733, 295, 23221, 1340, 300, 311, 3801, 813, 472, 11], "temperature": 0.0, "avg_logprob": -0.1047612155776426, "compression_ratio": 1.7128205128205127, "no_speech_prob": 7.2962584454216994e-06}, {"id": 43, "seek": 24700, "start": 260.0, "end": 267.5, "text": " or sorry, anything bigger than zero really leans positive, anything below zero leans negative.", "tokens": [420, 2597, 11, 1340, 3801, 813, 4018, 534, 476, 599, 3353, 11, 1340, 2507, 4018, 476, 599, 3671, 13], "temperature": 0.0, "avg_logprob": -0.1047612155776426, "compression_ratio": 1.7128205128205127, "no_speech_prob": 7.2962584454216994e-06}, {"id": 44, "seek": 24700, "start": 267.5, "end": 275.0, "text": " And so you can see that with, that is definitely below zero, it leans negative.", "tokens": [400, 370, 291, 393, 536, 300, 365, 11, 300, 307, 2138, 2507, 4018, 11, 309, 476, 599, 3671, 13], "temperature": 0.0, "avg_logprob": -0.1047612155776426, "compression_ratio": 1.7128205128205127, "no_speech_prob": 7.2962584454216994e-06}, {"id": 45, "seek": 27500, "start": 275.0, "end": 282.5, "text": " Let's look at what's, and some of this, this is a tiny data set, so it may not be accurate.", "tokens": [961, 311, 574, 412, 437, 311, 11, 293, 512, 295, 341, 11, 341, 307, 257, 5870, 1412, 992, 11, 370, 309, 815, 406, 312, 8559, 13], "temperature": 0.0, "avg_logprob": -0.11780558932911266, "compression_ratio": 1.4952830188679245, "no_speech_prob": 8.13948645372875e-06}, {"id": 46, "seek": 27500, "start": 282.5, "end": 287.0, "text": " I don't know that from really would lean positive a lot if you had a bigger data set,", "tokens": [286, 500, 380, 458, 300, 490, 534, 576, 11659, 3353, 257, 688, 498, 291, 632, 257, 3801, 1412, 992, 11], "temperature": 0.0, "avg_logprob": -0.11780558932911266, "compression_ratio": 1.4952830188679245, "no_speech_prob": 8.13948645372875e-06}, {"id": 47, "seek": 27500, "start": 287.0, "end": 290.0, "text": " although that could be something to look into.", "tokens": [4878, 300, 727, 312, 746, 281, 574, 666, 13], "temperature": 0.0, "avg_logprob": -0.11780558932911266, "compression_ratio": 1.4952830188679245, "no_speech_prob": 8.13948645372875e-06}, {"id": 48, "seek": 27500, "start": 290.0, "end": 293.5, "text": " But I just wanted to give you this as a way to visualize it.", "tokens": [583, 286, 445, 1415, 281, 976, 291, 341, 382, 257, 636, 281, 23273, 309, 13], "temperature": 0.0, "avg_logprob": -0.11780558932911266, "compression_ratio": 1.4952830188679245, "no_speech_prob": 8.13948645372875e-06}, {"id": 49, "seek": 27500, "start": 293.5, "end": 300.0, "text": " Any questions about this setup?", "tokens": [2639, 1651, 466, 341, 8657, 30], "temperature": 0.0, "avg_logprob": -0.11780558932911266, "compression_ratio": 1.4952830188679245, "no_speech_prob": 8.13948645372875e-06}, {"id": 50, "seek": 30000, "start": 300.0, "end": 306.5, "text": " Okay, so what we do is to use this as a model, so this row is what we are calling R in the Python notebook.", "tokens": [1033, 11, 370, 437, 321, 360, 307, 281, 764, 341, 382, 257, 2316, 11, 370, 341, 5386, 307, 437, 321, 366, 5141, 497, 294, 264, 15329, 21060, 13], "temperature": 0.0, "avg_logprob": -0.08178855110617245, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.2029366189381108e-05}, {"id": 51, "seek": 30000, "start": 306.5, "end": 311.5, "text": " We calculate B, which is the log of 23 divided by 17.", "tokens": [492, 8873, 363, 11, 597, 307, 264, 3565, 295, 6673, 6666, 538, 3282, 13], "temperature": 0.0, "avg_logprob": -0.08178855110617245, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.2029366189381108e-05}, {"id": 52, "seek": 30000, "start": 311.5, "end": 320.0, "text": " So here we need to take into account that we had more positive reviews than negative reviews in our data set.", "tokens": [407, 510, 321, 643, 281, 747, 666, 2696, 300, 321, 632, 544, 3353, 10229, 813, 3671, 10229, 294, 527, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.08178855110617245, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.2029366189381108e-05}, {"id": 53, "seek": 30000, "start": 320.0, "end": 326.0, "text": " And then I evaluated this just on our training data.", "tokens": [400, 550, 286, 25509, 341, 445, 322, 527, 3097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.08178855110617245, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.2029366189381108e-05}, {"id": 54, "seek": 32600, "start": 326.0, "end": 332.5, "text": " And this is actually just part of our training data because I'm only using like 45 of the vocabulary words.", "tokens": [400, 341, 307, 767, 445, 644, 295, 527, 3097, 1412, 570, 286, 478, 787, 1228, 411, 6905, 295, 264, 19864, 2283, 13], "temperature": 0.0, "avg_logprob": -0.07882101616162933, "compression_ratio": 1.5953488372093023, "no_speech_prob": 8.939429790189024e-06}, {"id": 55, "seek": 32600, "start": 332.5, "end": 341.5, "text": " But just to see how the model works, so this movie is so bad, very bad acting, very bad plot, very bad movie.", "tokens": [583, 445, 281, 536, 577, 264, 2316, 1985, 11, 370, 341, 3169, 307, 370, 1578, 11, 588, 1578, 6577, 11, 588, 1578, 7542, 11, 588, 1578, 3169, 13], "temperature": 0.0, "avg_logprob": -0.07882101616162933, "compression_ratio": 1.5953488372093023, "no_speech_prob": 8.939429790189024e-06}, {"id": 56, "seek": 32600, "start": 341.5, "end": 343.5, "text": " We know the answer.", "tokens": [492, 458, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.07882101616162933, "compression_ratio": 1.5953488372093023, "no_speech_prob": 8.939429790189024e-06}, {"id": 57, "seek": 32600, "start": 343.5, "end": 354.0, "text": " And I just took the dot product of R with the word counts of how often each word showed up in the review.", "tokens": [400, 286, 445, 1890, 264, 5893, 1674, 295, 497, 365, 264, 1349, 14893, 295, 577, 2049, 1184, 1349, 4712, 493, 294, 264, 3131, 13], "temperature": 0.0, "avg_logprob": -0.07882101616162933, "compression_ratio": 1.5953488372093023, "no_speech_prob": 8.939429790189024e-06}, {"id": 58, "seek": 35400, "start": 354.0, "end": 365.5, "text": " Added B, and then if that is less than zero, that means it predicted negative and it's correct since this was a negative review.", "tokens": [5349, 292, 363, 11, 293, 550, 498, 300, 307, 1570, 813, 4018, 11, 300, 1355, 309, 19147, 3671, 293, 309, 311, 3006, 1670, 341, 390, 257, 3671, 3131, 13], "temperature": 0.0, "avg_logprob": -0.10514878034591675, "compression_ratio": 1.6009852216748768, "no_speech_prob": 4.908397022518329e-05}, {"id": 59, "seek": 35400, "start": 365.5, "end": 369.0, "text": " So that's how you kind of apply it to make a prediction.", "tokens": [407, 300, 311, 577, 291, 733, 295, 3079, 309, 281, 652, 257, 17630, 13], "temperature": 0.0, "avg_logprob": -0.10514878034591675, "compression_ratio": 1.6009852216748768, "no_speech_prob": 4.908397022518329e-05}, {"id": 60, "seek": 35400, "start": 369.0, "end": 376.5, "text": " Although, again, in practice, you wouldn't be making predictions on your training set, except if you wanted to look at your training error.", "tokens": [5780, 11, 797, 11, 294, 3124, 11, 291, 2759, 380, 312, 1455, 21264, 322, 428, 3097, 992, 11, 3993, 498, 291, 1415, 281, 574, 412, 428, 3097, 6713, 13], "temperature": 0.0, "avg_logprob": -0.10514878034591675, "compression_ratio": 1.6009852216748768, "no_speech_prob": 4.908397022518329e-05}, {"id": 61, "seek": 37650, "start": 376.5, "end": 385.5, "text": " It was interesting. I thought this was worked fairly well given what a tiny, tiny selection of words I was using.", "tokens": [467, 390, 1880, 13, 286, 1194, 341, 390, 2732, 6457, 731, 2212, 437, 257, 5870, 11, 5870, 9450, 295, 2283, 286, 390, 1228, 13], "temperature": 0.0, "avg_logprob": -0.14995576685125178, "compression_ratio": 1.3806451612903226, "no_speech_prob": 2.026118181674974e-06}, {"id": 62, "seek": 37650, "start": 385.5, "end": 391.0, "text": " So I got 34 out of a 40 correct on the training set.", "tokens": [407, 286, 658, 12790, 484, 295, 257, 3356, 3006, 322, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.14995576685125178, "compression_ratio": 1.3806451612903226, "no_speech_prob": 2.026118181674974e-06}, {"id": 63, "seek": 37650, "start": 391.0, "end": 397.0, "text": " Any questions about this, this visual approach?", "tokens": [2639, 1651, 466, 341, 11, 341, 5056, 3109, 30], "temperature": 0.0, "avg_logprob": -0.14995576685125178, "compression_ratio": 1.3806451612903226, "no_speech_prob": 2.026118181674974e-06}, {"id": 64, "seek": 39700, "start": 397.0, "end": 408.0, "text": " Why did I, so I said that things less than zero indicated that the word swung negative, greater than zero positive.", "tokens": [1545, 630, 286, 11, 370, 286, 848, 300, 721, 1570, 813, 4018, 16176, 300, 264, 1349, 1693, 1063, 3671, 11, 5044, 813, 4018, 3353, 13], "temperature": 0.0, "avg_logprob": -0.16001784801483154, "compression_ratio": 1.4298245614035088, "no_speech_prob": 6.143843165773433e-06}, {"id": 65, "seek": 39700, "start": 408.0, "end": 425.0, "text": " Why was that the case? Why was zero the cutoff?", "tokens": [1545, 390, 300, 264, 1389, 30, 1545, 390, 4018, 264, 1723, 4506, 30], "temperature": 0.0, "avg_logprob": -0.16001784801483154, "compression_ratio": 1.4298245614035088, "no_speech_prob": 6.143843165773433e-06}, {"id": 66, "seek": 42500, "start": 425.0, "end": 428.0, "text": " What would be the significance of it being exactly zero?", "tokens": [708, 576, 312, 264, 17687, 295, 309, 885, 2293, 4018, 30], "temperature": 0.0, "avg_logprob": -0.1868558771470014, "compression_ratio": 1.460122699386503, "no_speech_prob": 1.1841188097605482e-05}, {"id": 67, "seek": 42500, "start": 428.0, "end": 438.0, "text": " Do you have a, can I throw you the catch box?", "tokens": [1144, 291, 362, 257, 11, 393, 286, 3507, 291, 264, 3745, 2424, 30], "temperature": 0.0, "avg_logprob": -0.1868558771470014, "compression_ratio": 1.460122699386503, "no_speech_prob": 1.1841188097605482e-05}, {"id": 68, "seek": 42500, "start": 438.0, "end": 449.0, "text": " So the reason we, the cutoff is at zeros because since we're dealing with the log of the ratios, the log of one is zero.", "tokens": [407, 264, 1778, 321, 11, 264, 1723, 4506, 307, 412, 35193, 570, 1670, 321, 434, 6260, 365, 264, 3565, 295, 264, 32435, 11, 264, 3565, 295, 472, 307, 4018, 13], "temperature": 0.0, "avg_logprob": -0.1868558771470014, "compression_ratio": 1.460122699386503, "no_speech_prob": 1.1841188097605482e-05}, {"id": 69, "seek": 42500, "start": 449.0, "end": 450.0, "text": " Exactly. Yeah.", "tokens": [7587, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.1868558771470014, "compression_ratio": 1.460122699386503, "no_speech_prob": 1.1841188097605482e-05}, {"id": 70, "seek": 45000, "start": 450.0, "end": 458.0, "text": " So we're looking at a log of ratios.", "tokens": [407, 321, 434, 1237, 412, 257, 3565, 295, 32435, 13], "temperature": 0.0, "avg_logprob": -0.07319035530090331, "compression_ratio": 1.5069444444444444, "no_speech_prob": 4.029322553833481e-06}, {"id": 71, "seek": 45000, "start": 458.0, "end": 466.0, "text": " So if the ratio was one, that would mean that it had showed up, was equally likely to show up in positive or negative reviews.", "tokens": [407, 498, 264, 8509, 390, 472, 11, 300, 576, 914, 300, 309, 632, 4712, 493, 11, 390, 12309, 3700, 281, 855, 493, 294, 3353, 420, 3671, 10229, 13], "temperature": 0.0, "avg_logprob": -0.07319035530090331, "compression_ratio": 1.5069444444444444, "no_speech_prob": 4.029322553833481e-06}, {"id": 72, "seek": 45000, "start": 466.0, "end": 473.0, "text": " Then we're taking the log of that, that maps to zero.", "tokens": [1396, 321, 434, 1940, 264, 3565, 295, 300, 11, 300, 11317, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.07319035530090331, "compression_ratio": 1.5069444444444444, "no_speech_prob": 4.029322553833481e-06}, {"id": 73, "seek": 47300, "start": 473.0, "end": 485.0, "text": " Okay. And if you're interested in the, and I really just wanted you to see this visual, but if you were interested in the code of how I created it, I put that at the bottom of notebook 3B.", "tokens": [1033, 13, 400, 498, 291, 434, 3102, 294, 264, 11, 293, 286, 534, 445, 1415, 291, 281, 536, 341, 5056, 11, 457, 498, 291, 645, 3102, 294, 264, 3089, 295, 577, 286, 2942, 309, 11, 286, 829, 300, 412, 264, 2767, 295, 21060, 805, 33, 13], "temperature": 0.0, "avg_logprob": -0.14171541531880696, "compression_ratio": 1.472972972972973, "no_speech_prob": 3.1200775993056595e-05}, {"id": 74, "seek": 47300, "start": 485.0, "end": 489.0, "text": " Just what I ran to get those.", "tokens": [1449, 437, 286, 5872, 281, 483, 729, 13], "temperature": 0.0, "avg_logprob": -0.14171541531880696, "compression_ratio": 1.472972972972973, "no_speech_prob": 3.1200775993056595e-05}, {"id": 75, "seek": 48900, "start": 489.0, "end": 510.0, "text": " And as I said, I just chose the 40 shortest reviews to look at. And then what I did is I chose vocabulary words that were used at least six times, but less than 30 because I thought that would be interesting to get words that were not super rare, but also not super common.", "tokens": [400, 382, 286, 848, 11, 286, 445, 5111, 264, 3356, 31875, 10229, 281, 574, 412, 13, 400, 550, 437, 286, 630, 307, 286, 5111, 19864, 2283, 300, 645, 1143, 412, 1935, 2309, 1413, 11, 457, 1570, 813, 2217, 570, 286, 1194, 300, 576, 312, 1880, 281, 483, 2283, 300, 645, 406, 1687, 5892, 11, 457, 611, 406, 1687, 2689, 13], "temperature": 0.0, "avg_logprob": -0.054272789508104324, "compression_ratio": 1.5511363636363635, "no_speech_prob": 5.594191861746367e-06}, {"id": 76, "seek": 51000, "start": 510.0, "end": 522.0, "text": " Any more, any more questions about naive Bayes before we go, go on to regex, which we kind of started last time briefly.", "tokens": [2639, 544, 11, 604, 544, 1651, 466, 29052, 7840, 279, 949, 321, 352, 11, 352, 322, 281, 319, 432, 87, 11, 597, 321, 733, 295, 1409, 1036, 565, 10515, 13], "temperature": 0.0, "avg_logprob": -0.15934454226026348, "compression_ratio": 1.3008130081300813, "no_speech_prob": 3.844824732368579e-06}, {"id": 77, "seek": 51000, "start": 522.0, "end": 526.0, "text": " All right. So let's, let's go to regex.", "tokens": [1057, 558, 13, 407, 718, 311, 11, 718, 311, 352, 281, 319, 432, 87, 13], "temperature": 0.0, "avg_logprob": -0.15934454226026348, "compression_ratio": 1.3008130081300813, "no_speech_prob": 3.844824732368579e-06}, {"id": 78, "seek": 52600, "start": 526.0, "end": 546.0, "text": " So I kind of open last time motivating with this phone number problem and showing kind of you could start trying to do it without regex, but it was quickly creating kind of this whole system of branches with a lot of if else's in different cases to consider.", "tokens": [407, 286, 733, 295, 1269, 1036, 565, 41066, 365, 341, 2593, 1230, 1154, 293, 4099, 733, 295, 291, 727, 722, 1382, 281, 360, 309, 1553, 319, 432, 87, 11, 457, 309, 390, 2661, 4084, 733, 295, 341, 1379, 1185, 295, 14770, 365, 257, 688, 295, 498, 1646, 311, 294, 819, 3331, 281, 1949, 13], "temperature": 0.0, "avg_logprob": -0.0847914301115891, "compression_ratio": 1.5, "no_speech_prob": 1.202938801725395e-05}, {"id": 79, "seek": 54600, "start": 546.0, "end": 565.0, "text": " And so regex will give us a cleaner and more effective way to deal with this because even with my branching ifs in different cases, I still was not capturing capturing that match of the phone number and just I guess to highlight the phone number problem.", "tokens": [400, 370, 319, 432, 87, 486, 976, 505, 257, 16532, 293, 544, 4942, 636, 281, 2028, 365, 341, 570, 754, 365, 452, 9819, 278, 498, 82, 294, 819, 3331, 11, 286, 920, 390, 406, 23384, 23384, 300, 2995, 295, 264, 2593, 1230, 293, 445, 286, 2041, 281, 5078, 264, 2593, 1230, 1154, 13], "temperature": 0.0, "avg_logprob": -0.16217307040565893, "compression_ratio": 1.5393939393939393, "no_speech_prob": 5.594183676294051e-06}, {"id": 80, "seek": 56500, "start": 565.0, "end": 578.0, "text": " What it is is that we're getting phone numbers with a lot of different formats. They might have hyphens, they might have parentheses or spaces, but there is a general pattern that we recognize with our eyes of what makes a phone number.", "tokens": [708, 309, 307, 307, 300, 321, 434, 1242, 2593, 3547, 365, 257, 688, 295, 819, 25879, 13, 814, 1062, 362, 2477, 950, 694, 11, 436, 1062, 362, 34153, 420, 7673, 11, 457, 456, 307, 257, 2674, 5102, 300, 321, 5521, 365, 527, 2575, 295, 437, 1669, 257, 2593, 1230, 13], "temperature": 0.0, "avg_logprob": -0.08955143493356056, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.202891053253552e-05}, {"id": 81, "seek": 56500, "start": 578.0, "end": 584.0, "text": " We can tell when data is wrong, you know that this is actually an address. We don't want to don't want to pick it out.", "tokens": [492, 393, 980, 562, 1412, 307, 2085, 11, 291, 458, 300, 341, 307, 767, 364, 2985, 13, 492, 500, 380, 528, 281, 500, 380, 528, 281, 1888, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.08955143493356056, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.202891053253552e-05}, {"id": 82, "seek": 56500, "start": 584.0, "end": 592.0, "text": " So how can we kind of generalize those patterns and pick them out.", "tokens": [407, 577, 393, 321, 733, 295, 2674, 1125, 729, 8294, 293, 1888, 552, 484, 13], "temperature": 0.0, "avg_logprob": -0.08955143493356056, "compression_ratio": 1.7154471544715446, "no_speech_prob": 1.202891053253552e-05}, {"id": 83, "seek": 59200, "start": 592.0, "end": 608.0, "text": " So we're going to in a moment we're going to take some time to practice because I think regex is really just something you have to kind of practice to get otherwise it seems like a lot of pretty obtruse rules.", "tokens": [407, 321, 434, 516, 281, 294, 257, 1623, 321, 434, 516, 281, 747, 512, 565, 281, 3124, 570, 286, 519, 319, 432, 87, 307, 534, 445, 746, 291, 362, 281, 733, 295, 3124, 281, 483, 5911, 309, 2544, 411, 257, 688, 295, 1238, 1111, 6903, 438, 4474, 13], "temperature": 0.0, "avg_logprob": -0.08217041309063251, "compression_ratio": 1.4928571428571429, "no_speech_prob": 1.644199073780328e-05}, {"id": 84, "seek": 60800, "start": 608.0, "end": 633.0, "text": " So I showed here that backslash lowercase d is a digit zero to nine phone number with hyphens you could write backslash d embraces three that signifying you want three of those dash three more dash four more in a moment will kind of build up to how to take into account.", "tokens": [407, 286, 4712, 510, 300, 646, 10418, 1299, 3126, 9765, 274, 307, 257, 14293, 4018, 281, 4949, 2593, 1230, 365, 2477, 950, 694, 291, 727, 2464, 646, 10418, 1299, 274, 9392, 2116, 1045, 300, 1465, 5489, 291, 528, 1045, 295, 729, 8240, 1045, 544, 8240, 1451, 544, 294, 257, 1623, 486, 733, 295, 1322, 493, 281, 577, 281, 747, 666, 2696, 13], "temperature": 0.0, "avg_logprob": -0.15077615506721265, "compression_ratio": 1.560693641618497, "no_speech_prob": 4.7107728278206196e-06}, {"id": 85, "seek": 63300, "start": 633.0, "end": 643.0, "text": " What if you have a space instead of a dash what if you have parentheses regex can deal with those as well.", "tokens": [708, 498, 291, 362, 257, 1901, 2602, 295, 257, 8240, 437, 498, 291, 362, 34153, 319, 432, 87, 393, 2028, 365, 729, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.08244187037150065, "compression_ratio": 1.5732484076433122, "no_speech_prob": 2.7534264518180862e-05}, {"id": 86, "seek": 63300, "start": 643.0, "end": 654.0, "text": " Just useful useful things to know some unexact quantifiers question mark means zero or one star means zero or more plus sign is one or more.", "tokens": [1449, 4420, 4420, 721, 281, 458, 512, 11572, 578, 4426, 23463, 1168, 1491, 1355, 4018, 420, 472, 3543, 1355, 4018, 420, 544, 1804, 1465, 307, 472, 420, 544, 13], "temperature": 0.0, "avg_logprob": -0.08244187037150065, "compression_ratio": 1.5732484076433122, "no_speech_prob": 2.7534264518180862e-05}, {"id": 87, "seek": 65400, "start": 654.0, "end": 663.0, "text": " So then I would say the only way to do this is to learn through practice I want us to take 15 minutes to start on your own going through regex one.", "tokens": [407, 550, 286, 576, 584, 264, 787, 636, 281, 360, 341, 307, 281, 1466, 807, 3124, 286, 528, 505, 281, 747, 2119, 2077, 281, 722, 322, 428, 1065, 516, 807, 319, 432, 87, 472, 13], "temperature": 0.0, "avg_logprob": -0.14297297905231343, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.644133953959681e-05}, {"id": 88, "seek": 65400, "start": 663.0, "end": 670.0, "text": " But first I want to just show you and actually let me check who here has used regex before raise your hand.", "tokens": [583, 700, 286, 528, 281, 445, 855, 291, 293, 767, 718, 385, 1520, 567, 510, 575, 1143, 319, 432, 87, 949, 5300, 428, 1011, 13], "temperature": 0.0, "avg_logprob": -0.14297297905231343, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.644133953959681e-05}, {"id": 89, "seek": 65400, "start": 670.0, "end": 675.0, "text": " Okay fair number who's used it a lot.", "tokens": [1033, 3143, 1230, 567, 311, 1143, 309, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.14297297905231343, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.644133953959681e-05}, {"id": 90, "seek": 65400, "start": 675.0, "end": 676.0, "text": " Just Jeremy.", "tokens": [1449, 17809, 13], "temperature": 0.0, "avg_logprob": -0.14297297905231343, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.644133953959681e-05}, {"id": 91, "seek": 65400, "start": 676.0, "end": 678.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.14297297905231343, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.644133953959681e-05}, {"id": 92, "seek": 67800, "start": 678.0, "end": 684.0, "text": " So some of this some of this might be review review for you. So this is the site regex one.", "tokens": [407, 512, 295, 341, 512, 295, 341, 1062, 312, 3131, 3131, 337, 291, 13, 407, 341, 307, 264, 3621, 319, 432, 87, 472, 13], "temperature": 0.0, "avg_logprob": -0.08900092707739936, "compression_ratio": 1.6171428571428572, "no_speech_prob": 1.7230968296644278e-05}, {"id": 93, "seek": 67800, "start": 684.0, "end": 695.0, "text": " It's nice it gives you a few patterns that you're trying to match and in the later ones it'll also have ones that you explicitly don't want to match something to be careful of just with this.", "tokens": [467, 311, 1481, 309, 2709, 291, 257, 1326, 8294, 300, 291, 434, 1382, 281, 2995, 293, 294, 264, 1780, 2306, 309, 603, 611, 362, 2306, 300, 291, 20803, 500, 380, 528, 281, 2995, 746, 281, 312, 5026, 295, 445, 365, 341, 13], "temperature": 0.0, "avg_logprob": -0.08900092707739936, "compression_ratio": 1.6171428571428572, "no_speech_prob": 1.7230968296644278e-05}, {"id": 94, "seek": 69500, "start": 695.0, "end": 708.0, "text": " The site is notice here if I just put a it's giving me three check marks I have matched these because they all have an A in them but you with regex you want to be as specific as possible.", "tokens": [440, 3621, 307, 3449, 510, 498, 286, 445, 829, 257, 309, 311, 2902, 385, 1045, 1520, 10640, 286, 362, 21447, 613, 570, 436, 439, 362, 364, 316, 294, 552, 457, 291, 365, 319, 432, 87, 291, 528, 281, 312, 382, 2685, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.10891043755315966, "compression_ratio": 1.7543859649122806, "no_speech_prob": 1.9832443285849877e-05}, {"id": 95, "seek": 69500, "start": 708.0, "end": 721.0, "text": " And here it seems like everything I want has ABC and it would be better to write out that full ABC because later on you know I could be fed different different examples and you want to be as specific as possible.", "tokens": [400, 510, 309, 2544, 411, 1203, 286, 528, 575, 22342, 293, 309, 576, 312, 1101, 281, 2464, 484, 300, 1577, 22342, 570, 1780, 322, 291, 458, 286, 727, 312, 4636, 819, 819, 5110, 293, 291, 528, 281, 312, 382, 2685, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.10891043755315966, "compression_ratio": 1.7543859649122806, "no_speech_prob": 1.9832443285849877e-05}, {"id": 96, "seek": 72100, "start": 721.0, "end": 734.0, "text": " So that's kind of my one caution as you're practicing here. You don't necessarily just want to do the first thing you get you kind of want to see have I have I gotten like the fullest answer I can.", "tokens": [407, 300, 311, 733, 295, 452, 472, 23585, 382, 291, 434, 11350, 510, 13, 509, 500, 380, 4725, 445, 528, 281, 360, 264, 700, 551, 291, 483, 291, 733, 295, 528, 281, 536, 362, 286, 362, 286, 5768, 411, 264, 45154, 1867, 286, 393, 13], "temperature": 0.0, "avg_logprob": -0.11330431157892401, "compression_ratio": 1.5816326530612246, "no_speech_prob": 3.535407449817285e-05}, {"id": 97, "seek": 72100, "start": 734.0, "end": 743.0, "text": " Yeah, so this is nice and it explains you can read kind of it introduces one or two roles at a time to practice.", "tokens": [865, 11, 370, 341, 307, 1481, 293, 309, 13948, 291, 393, 1401, 733, 295, 309, 31472, 472, 420, 732, 9604, 412, 257, 565, 281, 3124, 13], "temperature": 0.0, "avg_logprob": -0.11330431157892401, "compression_ratio": 1.5816326530612246, "no_speech_prob": 3.535407449817285e-05}, {"id": 98, "seek": 74300, "start": 743.0, "end": 754.0, "text": " So I'm going to take 15 minutes to work through this and see see how far you get. This is the website regex one spelled out o ne.com.", "tokens": [407, 286, 478, 516, 281, 747, 2119, 2077, 281, 589, 807, 341, 293, 536, 536, 577, 1400, 291, 483, 13, 639, 307, 264, 3144, 319, 432, 87, 472, 34388, 484, 277, 408, 13, 1112, 13], "temperature": 0.0, "avg_logprob": -0.17207994595379897, "compression_ratio": 1.4244186046511629, "no_speech_prob": 1.9832232283079065e-05}, {"id": 99, "seek": 74300, "start": 754.0, "end": 764.0, "text": " If you, if you're more advanced with regex and want something a little bit harder. I think it's regex Tuesdays.", "tokens": [759, 291, 11, 498, 291, 434, 544, 7339, 365, 319, 432, 87, 293, 528, 746, 257, 707, 857, 6081, 13, 286, 519, 309, 311, 319, 432, 87, 10017, 82, 13], "temperature": 0.0, "avg_logprob": -0.17207994595379897, "compression_ratio": 1.4244186046511629, "no_speech_prob": 1.9832232283079065e-05}, {"id": 100, "seek": 76400, "start": 764.0, "end": 777.0, "text": " Regex Tuesdays are a little bit more challenging ones. And here's neat they have test cases, kind of a greater number of test cases of what you want to hit or not.", "tokens": [1300, 432, 87, 10017, 82, 366, 257, 707, 857, 544, 7595, 2306, 13, 400, 510, 311, 10654, 436, 362, 1500, 3331, 11, 733, 295, 257, 5044, 1230, 295, 1500, 3331, 295, 437, 291, 528, 281, 2045, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.1169616883261162, "compression_ratio": 1.4161073825503356, "no_speech_prob": 6.240751190489391e-06}, {"id": 101, "seek": 76400, "start": 777.0, "end": 783.0, "text": " And the only tricky thing with this is that", "tokens": [400, 264, 787, 12414, 551, 365, 341, 307, 300], "temperature": 0.0, "avg_logprob": -0.1169616883261162, "compression_ratio": 1.4161073825503356, "no_speech_prob": 6.240751190489391e-06}, {"id": 102, "seek": 76400, "start": 783.0, "end": 789.0, "text": " see", "tokens": [536], "temperature": 0.0, "avg_logprob": -0.1169616883261162, "compression_ratio": 1.4161073825503356, "no_speech_prob": 6.240751190489391e-06}, {"id": 103, "seek": 78900, "start": 789.0, "end": 795.0, "text": " I want to, I think you have to put slashes before and after.", "tokens": [286, 528, 281, 11, 286, 519, 291, 362, 281, 829, 1061, 12808, 949, 293, 934, 13], "temperature": 0.0, "avg_logprob": -0.11474155849880642, "compression_ratio": 1.566820276497696, "no_speech_prob": 1.1659005394903943e-05}, {"id": 104, "seek": 78900, "start": 795.0, "end": 807.0, "text": " Alright, so and feel free to kind of raise your hand as you get started if you have questions but we'll, we'll discuss in 15 minutes kind of how far you got and what you saw.", "tokens": [2798, 11, 370, 293, 841, 1737, 281, 733, 295, 5300, 428, 1011, 382, 291, 483, 1409, 498, 291, 362, 1651, 457, 321, 603, 11, 321, 603, 2248, 294, 2119, 2077, 733, 295, 577, 1400, 291, 658, 293, 437, 291, 1866, 13], "temperature": 0.0, "avg_logprob": -0.11474155849880642, "compression_ratio": 1.566820276497696, "no_speech_prob": 1.1659005394903943e-05}, {"id": 105, "seek": 78900, "start": 807.0, "end": 815.0, "text": " And then one more regex resource that Jeremy recommended and I've tried this out is the regex crossword.", "tokens": [400, 550, 472, 544, 319, 432, 87, 7684, 300, 17809, 9628, 293, 286, 600, 3031, 341, 484, 307, 264, 319, 432, 87, 3278, 7462, 13], "temperature": 0.0, "avg_logprob": -0.11474155849880642, "compression_ratio": 1.566820276497696, "no_speech_prob": 1.1659005394903943e-05}, {"id": 106, "seek": 81500, "start": 815.0, "end": 819.0, "text": " And it's, it's pretty tricky.", "tokens": [400, 309, 311, 11, 309, 311, 1238, 12414, 13], "temperature": 0.0, "avg_logprob": -0.09125778410169813, "compression_ratio": 1.6407766990291262, "no_speech_prob": 1.0952017873933073e-05}, {"id": 107, "seek": 81500, "start": 819.0, "end": 822.0, "text": " Something that's different about it is that you're.", "tokens": [6595, 300, 311, 819, 466, 309, 307, 300, 291, 434, 13], "temperature": 0.0, "avg_logprob": -0.09125778410169813, "compression_ratio": 1.6407766990291262, "no_speech_prob": 1.0952017873933073e-05}, {"id": 108, "seek": 81500, "start": 822.0, "end": 832.0, "text": " It's kind of in reverse for most regex so instead of writing the regex you're seeing to regex expressions and figuring out what characters would meet them both.", "tokens": [467, 311, 733, 295, 294, 9943, 337, 881, 319, 432, 87, 370, 2602, 295, 3579, 264, 319, 432, 87, 291, 434, 2577, 281, 319, 432, 87, 15277, 293, 15213, 484, 437, 4342, 576, 1677, 552, 1293, 13], "temperature": 0.0, "avg_logprob": -0.09125778410169813, "compression_ratio": 1.6407766990291262, "no_speech_prob": 1.0952017873933073e-05}, {"id": 109, "seek": 81500, "start": 832.0, "end": 839.0, "text": " But if you're wanting to think about regex in a different way this could be a resource as well.", "tokens": [583, 498, 291, 434, 7935, 281, 519, 466, 319, 432, 87, 294, 257, 819, 636, 341, 727, 312, 257, 7684, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.09125778410169813, "compression_ratio": 1.6407766990291262, "no_speech_prob": 1.0952017873933073e-05}, {"id": 110, "seek": 83900, "start": 839.0, "end": 851.0, "text": " So if you're shaky at all with regex I would recommend starting with this regex one.", "tokens": [407, 498, 291, 434, 44785, 412, 439, 365, 319, 432, 87, 286, 576, 2748, 2891, 365, 341, 319, 432, 87, 472, 13], "temperature": 0.0, "avg_logprob": -0.16427167769401305, "compression_ratio": 1.4013157894736843, "no_speech_prob": 3.66870017387555e-06}, {"id": 111, "seek": 83900, "start": 851.0, "end": 860.0, "text": " Okay, so let's let's pause and discuss actually out of curiosity, how far did you get on regex one typically.", "tokens": [1033, 11, 370, 718, 311, 718, 311, 10465, 293, 2248, 767, 484, 295, 18769, 11, 577, 1400, 630, 291, 483, 322, 319, 432, 87, 472, 5850, 13], "temperature": 0.0, "avg_logprob": -0.16427167769401305, "compression_ratio": 1.4013157894736843, "no_speech_prob": 3.66870017387555e-06}, {"id": 112, "seek": 83900, "start": 860.0, "end": 864.0, "text": " Oh great. Awesome.", "tokens": [876, 869, 13, 10391, 13], "temperature": 0.0, "avg_logprob": -0.16427167769401305, "compression_ratio": 1.4013157894736843, "no_speech_prob": 3.66870017387555e-06}, {"id": 113, "seek": 86400, "start": 864.0, "end": 869.0, "text": " Anyone else want to share how far they got.", "tokens": [14643, 1646, 528, 281, 2073, 577, 1400, 436, 658, 13], "temperature": 0.0, "avg_logprob": -0.10494230295482435, "compression_ratio": 1.601010101010101, "no_speech_prob": 2.902246933444985e-06}, {"id": 114, "seek": 86400, "start": 869.0, "end": 872.0, "text": " Any any favorite.", "tokens": [2639, 604, 2954, 13], "temperature": 0.0, "avg_logprob": -0.10494230295482435, "compression_ratio": 1.601010101010101, "no_speech_prob": 2.902246933444985e-06}, {"id": 115, "seek": 86400, "start": 872.0, "end": 879.0, "text": " Any favorite meta characters.", "tokens": [2639, 2954, 19616, 4342, 13], "temperature": 0.0, "avg_logprob": -0.10494230295482435, "compression_ratio": 1.601010101010101, "no_speech_prob": 2.902246933444985e-06}, {"id": 116, "seek": 86400, "start": 879.0, "end": 893.0, "text": " I just wanted to highlight, make sure you particularly saw capture groups are really useful so this is when you don't want to just match something but you want to be able to extract part of part of what you found and so here.", "tokens": [286, 445, 1415, 281, 5078, 11, 652, 988, 291, 4098, 1866, 7983, 3935, 366, 534, 4420, 370, 341, 307, 562, 291, 500, 380, 528, 281, 445, 2995, 746, 457, 291, 528, 281, 312, 1075, 281, 8947, 644, 295, 644, 295, 437, 291, 1352, 293, 370, 510, 13], "temperature": 0.0, "avg_logprob": -0.10494230295482435, "compression_ratio": 1.601010101010101, "no_speech_prob": 2.902246933444985e-06}, {"id": 117, "seek": 89300, "start": 893.0, "end": 907.0, "text": " And I love that this is a very kind of real world example, given a list of file names, you're wanting to pick out the PDFs and what comes directly before PDF.", "tokens": [400, 286, 959, 300, 341, 307, 257, 588, 733, 295, 957, 1002, 1365, 11, 2212, 257, 1329, 295, 3991, 5288, 11, 291, 434, 7935, 281, 1888, 484, 264, 17752, 82, 293, 437, 1487, 3838, 949, 17752, 13], "temperature": 0.0, "avg_logprob": -0.11451965425072647, "compression_ratio": 1.2248062015503876, "no_speech_prob": 1.3630484318127856e-05}, {"id": 118, "seek": 90700, "start": 907.0, "end": 923.0, "text": " And so here, using using parentheses will capture the group, so you're not just kind of identifying this is a PDF file but you can get hey this is what came before the dot PDF the file name.", "tokens": [400, 370, 510, 11, 1228, 1228, 34153, 486, 7983, 264, 1594, 11, 370, 291, 434, 406, 445, 733, 295, 16696, 341, 307, 257, 17752, 3991, 457, 291, 393, 483, 4177, 341, 307, 437, 1361, 949, 264, 5893, 17752, 264, 3991, 1315, 13], "temperature": 0.0, "avg_logprob": -0.11460457677426546, "compression_ratio": 1.4585635359116023, "no_speech_prob": 8.939256076700985e-06}, {"id": 119, "seek": 90700, "start": 923.0, "end": 928.0, "text": " So I wanted to showcase that.", "tokens": [407, 286, 1415, 281, 20388, 300, 13], "temperature": 0.0, "avg_logprob": -0.11460457677426546, "compression_ratio": 1.4585635359116023, "no_speech_prob": 8.939256076700985e-06}, {"id": 120, "seek": 90700, "start": 928.0, "end": 934.0, "text": " Any any questions on regex or what you saw.", "tokens": [2639, 604, 1651, 322, 319, 432, 87, 420, 437, 291, 1866, 13], "temperature": 0.0, "avg_logprob": -0.11460457677426546, "compression_ratio": 1.4585635359116023, "no_speech_prob": 8.939256076700985e-06}, {"id": 121, "seek": 93400, "start": 934.0, "end": 942.0, "text": " Let's go back to notebook for in that case.", "tokens": [961, 311, 352, 646, 281, 21060, 337, 294, 300, 1389, 13], "temperature": 0.0, "avg_logprob": -0.12929869431715746, "compression_ratio": 1.4285714285714286, "no_speech_prob": 3.07152840832714e-05}, {"id": 122, "seek": 93400, "start": 942.0, "end": 944.0, "text": " So pros and cons.", "tokens": [407, 6267, 293, 1014, 13], "temperature": 0.0, "avg_logprob": -0.12929869431715746, "compression_ratio": 1.4285714285714286, "no_speech_prob": 3.07152840832714e-05}, {"id": 123, "seek": 93400, "start": 944.0, "end": 950.0, "text": " Some of the advantages are that it's very, very concise and powerful.", "tokens": [2188, 295, 264, 14906, 366, 300, 309, 311, 588, 11, 588, 44882, 293, 4005, 13], "temperature": 0.0, "avg_logprob": -0.12929869431715746, "compression_ratio": 1.4285714285714286, "no_speech_prob": 3.07152840832714e-05}, {"id": 124, "seek": 93400, "start": 950.0, "end": 959.0, "text": " It's supported by many different programming languages have regex in them it's not, not just a Python thing.", "tokens": [467, 311, 8104, 538, 867, 819, 9410, 8650, 362, 319, 432, 87, 294, 552, 309, 311, 406, 11, 406, 445, 257, 15329, 551, 13], "temperature": 0.0, "avg_logprob": -0.12929869431715746, "compression_ratio": 1.4285714285714286, "no_speech_prob": 3.07152840832714e-05}, {"id": 125, "seek": 95900, "start": 959.0, "end": 975.0, "text": " The disadvantages can be brittle, and it can be hard to write and hard to read, and I think that's kind of the flip side for many things that are concise part of concision is that yeah it can be harder harder to read later.", "tokens": [440, 37431, 393, 312, 49325, 11, 293, 309, 393, 312, 1152, 281, 2464, 293, 1152, 281, 1401, 11, 293, 286, 519, 300, 311, 733, 295, 264, 7929, 1252, 337, 867, 721, 300, 366, 44882, 644, 295, 1588, 1991, 307, 300, 1338, 309, 393, 312, 6081, 6081, 281, 1401, 1780, 13], "temperature": 0.0, "avg_logprob": -0.12164892973723235, "compression_ratio": 1.6159420289855073, "no_speech_prob": 8.664042979944497e-06}, {"id": 126, "seek": 97500, "start": 975.0, "end": 993.0, "text": " So the next step kind of real world use case I wanted to look at was revisiting tokenization.", "tokens": [407, 264, 958, 1823, 733, 295, 957, 1002, 764, 1389, 286, 1415, 281, 574, 412, 390, 20767, 1748, 14862, 2144, 13], "temperature": 0.0, "avg_logprob": -0.1498721694946289, "compression_ratio": 1.1071428571428572, "no_speech_prob": 3.269519584137015e-05}, {"id": 127, "seek": 99300, "start": 993.0, "end": 1016.0, "text": " So here we're going to pick out a few different things and so this is still we're still looking at text processing and actually just don't remind me what what is a token.", "tokens": [407, 510, 321, 434, 516, 281, 1888, 484, 257, 1326, 819, 721, 293, 370, 341, 307, 920, 321, 434, 920, 1237, 412, 2487, 9007, 293, 767, 445, 500, 380, 4160, 385, 437, 437, 307, 257, 14862, 13], "temperature": 0.0, "avg_logprob": -0.1797798901069455, "compression_ratio": 1.3709677419354838, "no_speech_prob": 2.0461076928768307e-05}, {"id": 128, "seek": 101600, "start": 1016.0, "end": 1029.0, "text": " Yeah, so each word is a token it's kind of the the fundamental units that you want to look at so kind of breaking this into the units which for our purpose are generally words and doing that in a standard way.", "tokens": [865, 11, 370, 1184, 1349, 307, 257, 14862, 309, 311, 733, 295, 264, 264, 8088, 6815, 300, 291, 528, 281, 574, 412, 370, 733, 295, 7697, 341, 666, 264, 6815, 597, 337, 527, 4334, 366, 5101, 2283, 293, 884, 300, 294, 257, 3832, 636, 13], "temperature": 0.0, "avg_logprob": -0.14743873347406802, "compression_ratio": 1.596774193548387, "no_speech_prob": 2.7965264962404035e-05}, {"id": 129, "seek": 101600, "start": 1029.0, "end": 1044.0, "text": " And typically when we tokenize something will also be coming up with IDs, so that we can map between, you know, inner jerk IDs and our vocabulary of tokens for the data we're looking at.", "tokens": [400, 5850, 562, 321, 14862, 1125, 746, 486, 611, 312, 1348, 493, 365, 48212, 11, 370, 300, 321, 393, 4471, 1296, 11, 291, 458, 11, 7284, 25197, 48212, 293, 527, 19864, 295, 22667, 337, 264, 1412, 321, 434, 1237, 412, 13], "temperature": 0.0, "avg_logprob": -0.14743873347406802, "compression_ratio": 1.596774193548387, "no_speech_prob": 2.7965264962404035e-05}, {"id": 130, "seek": 104400, "start": 1044.0, "end": 1057.0, "text": " And so, something to note about tokenization is there's not just one way to do it really depends on the implementation and depends on kind of what what you want.", "tokens": [400, 370, 11, 746, 281, 3637, 466, 14862, 2144, 307, 456, 311, 406, 445, 472, 636, 281, 360, 309, 534, 5946, 322, 264, 11420, 293, 5946, 322, 733, 295, 437, 437, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.07236991430583753, "compression_ratio": 1.3644067796610169, "no_speech_prob": 9.972145562642254e-06}, {"id": 131, "seek": 105700, "start": 1057.0, "end": 1076.0, "text": " What your data set to be like, and so this is just kind of an illustration of one approach you could take, but here, say we want to add spaces around punctuation, so we can kind of pick out all the all the different forms of punctuation.", "tokens": [708, 428, 1412, 992, 281, 312, 411, 11, 293, 370, 341, 307, 445, 733, 295, 364, 22645, 295, 472, 3109, 291, 727, 747, 11, 457, 510, 11, 584, 321, 528, 281, 909, 7673, 926, 27006, 16073, 11, 370, 321, 393, 733, 295, 1888, 484, 439, 264, 439, 264, 819, 6422, 295, 27006, 16073, 13], "temperature": 0.0, "avg_logprob": -0.0940557677170326, "compression_ratio": 1.5490196078431373, "no_speech_prob": 2.1443642253871076e-05}, {"id": 132, "seek": 107600, "start": 1076.0, "end": 1091.0, "text": " We also want to pick out.", "tokens": [492, 611, 528, 281, 1888, 484, 13], "temperature": 0.0, "avg_logprob": -0.2645756331357089, "compression_ratio": 0.7575757575757576, "no_speech_prob": 6.961789495107951e-06}, {"id": 133, "seek": 109100, "start": 1091.0, "end": 1109.0, "text": " Didn't don't wasn't wouldn't and kind of has a specific meaning right it's a shortened version of not. So we might want to pick that out apostrophe s has a particular meaning, showing the possessive of something.", "tokens": [11151, 380, 500, 380, 2067, 380, 2759, 380, 293, 733, 295, 575, 257, 2685, 3620, 558, 309, 311, 257, 45183, 3037, 295, 406, 13, 407, 321, 1062, 528, 281, 1888, 300, 484, 19484, 27194, 262, 575, 257, 1729, 3620, 11, 4099, 264, 17490, 488, 295, 746, 13], "temperature": 0.0, "avg_logprob": -0.1549527684196097, "compression_ratio": 1.4777070063694266, "no_speech_prob": 4.494939730648184e-06}, {"id": 134, "seek": 109100, "start": 1109.0, "end": 1112.0, "text": " So,", "tokens": [407, 11], "temperature": 0.0, "avg_logprob": -0.1549527684196097, "compression_ratio": 1.4777070063694266, "no_speech_prob": 4.494939730648184e-06}, {"id": 135, "seek": 109100, "start": 1112.0, "end": 1115.0, "text": " Rachel's class.", "tokens": [14246, 311, 1508, 13], "temperature": 0.0, "avg_logprob": -0.1549527684196097, "compression_ratio": 1.4777070063694266, "no_speech_prob": 4.494939730648184e-06}, {"id": 136, "seek": 111500, "start": 1115.0, "end": 1121.0, "text": " Sarah's office, you know that apostrophe s has meaning so that's something else we might want to pick out.", "tokens": [9519, 311, 3398, 11, 291, 458, 300, 19484, 27194, 262, 575, 3620, 370, 300, 311, 746, 1646, 321, 1062, 528, 281, 1888, 484, 13], "temperature": 0.0, "avg_logprob": -0.1312894344329834, "compression_ratio": 1.4378698224852071, "no_speech_prob": 9.665689503890462e-06}, {"id": 137, "seek": 111500, "start": 1121.0, "end": 1128.0, "text": " And then say we want to replace multiple spaces with just one, because ultimately.", "tokens": [400, 550, 584, 321, 528, 281, 7406, 3866, 7673, 365, 445, 472, 11, 570, 6284, 13], "temperature": 0.0, "avg_logprob": -0.1312894344329834, "compression_ratio": 1.4378698224852071, "no_speech_prob": 9.665689503890462e-06}, {"id": 138, "seek": 111500, "start": 1128.0, "end": 1133.0, "text": " Thank you typically don't use white space as a token.", "tokens": [1044, 291, 5850, 500, 380, 764, 2418, 1901, 382, 257, 14862, 13], "temperature": 0.0, "avg_logprob": -0.1312894344329834, "compression_ratio": 1.4378698224852071, "no_speech_prob": 9.665689503890462e-06}, {"id": 139, "seek": 113300, "start": 1133.0, "end": 1147.0, "text": " So here we're going to be using from the pythons re library read dot compile to create these red boxes, and then dot sub to substitute in what we want.", "tokens": [407, 510, 321, 434, 516, 281, 312, 1228, 490, 264, 10664, 392, 892, 319, 6405, 1401, 5893, 31413, 281, 1884, 613, 2182, 9002, 11, 293, 550, 5893, 1422, 281, 15802, 294, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.20572434938870943, "compression_ratio": 1.3017241379310345, "no_speech_prob": 3.785199851336074e-06}, {"id": 140, "seek": 114700, "start": 1147.0, "end": 1164.0, "text": " So here, writing a method that will take a sentence for punctuation.", "tokens": [407, 510, 11, 3579, 257, 3170, 300, 486, 747, 257, 8174, 337, 27006, 16073, 13], "temperature": 0.0, "avg_logprob": -0.20072714905989797, "compression_ratio": 0.9855072463768116, "no_speech_prob": 2.332000804017298e-06}, {"id": 141, "seek": 116400, "start": 1164.0, "end": 1178.0, "text": " So, let's see what we can do with this.", "tokens": [407, 11, 718, 311, 536, 437, 321, 393, 360, 365, 341, 13], "temperature": 0.0, "avg_logprob": -0.9665364764985585, "compression_ratio": 0.8620689655172413, "no_speech_prob": 7.527524303441169e-06}, {"id": 142, "seek": 116400, "start": 1178.0, "end": 1181.0, "text": " Any ideas.", "tokens": [2639, 3487, 13], "temperature": 0.0, "avg_logprob": -0.9665364764985585, "compression_ratio": 0.8620689655172413, "no_speech_prob": 7.527524303441169e-06}, {"id": 143, "seek": 118100, "start": 1181.0, "end": 1195.0, "text": " So, we're going to use the same method that we used to capture the first group so here for. This is for the punctuation one. We have, we have parentheses around the outside. So we're going to.", "tokens": [407, 11, 321, 434, 516, 281, 764, 264, 912, 3170, 300, 321, 1143, 281, 7983, 264, 700, 1594, 370, 510, 337, 13, 639, 307, 337, 264, 27006, 16073, 472, 13, 492, 362, 11, 321, 362, 34153, 926, 264, 2380, 13, 407, 321, 434, 516, 281, 13], "temperature": 0.0, "avg_logprob": -0.46185284276162425, "compression_ratio": 1.523489932885906, "no_speech_prob": 8.267319572041743e-06}, {"id": 144, "seek": 118100, "start": 1195.0, "end": 1198.0, "text": " And that indicates.", "tokens": [400, 300, 16203, 13], "temperature": 0.0, "avg_logprob": -0.46185284276162425, "compression_ratio": 1.523489932885906, "no_speech_prob": 8.267319572041743e-06}, {"id": 145, "seek": 118100, "start": 1198.0, "end": 1200.0, "text": " Capture group.", "tokens": [9480, 540, 1594, 13], "temperature": 0.0, "avg_logprob": -0.46185284276162425, "compression_ratio": 1.523489932885906, "no_speech_prob": 8.267319572041743e-06}, {"id": 146, "seek": 120000, "start": 1200.0, "end": 1223.0, "text": " So, we're saying, you know, if you found, found a dot or colon, put in that call, put back in space, what you captured, which would be the colon and then another space.", "tokens": [407, 11, 321, 434, 1566, 11, 291, 458, 11, 498, 291, 1352, 11, 1352, 257, 5893, 420, 8255, 11, 829, 294, 300, 818, 11, 829, 646, 294, 1901, 11, 437, 291, 11828, 11, 597, 576, 312, 264, 8255, 293, 550, 1071, 1901, 13], "temperature": 0.0, "avg_logprob": -0.3289778486211249, "compression_ratio": 1.411764705882353, "no_speech_prob": 6.048774139344459e-06}, {"id": 147, "seek": 122300, "start": 1223.0, "end": 1239.0, "text": " Then we're going to make an apostrophe T. Previously, since we've inserted all these spaces we had inserted spaces on either side, we're going to condense that back to the apostrophe T. So we'll substitute that in ditto with apostrophe S.", "tokens": [1396, 321, 434, 516, 281, 652, 364, 19484, 27194, 314, 13, 33606, 11, 1670, 321, 600, 27992, 439, 613, 7673, 321, 632, 27992, 7673, 322, 2139, 1252, 11, 321, 434, 516, 281, 2224, 1288, 300, 646, 281, 264, 19484, 27194, 314, 13, 407, 321, 603, 15802, 300, 294, 274, 34924, 365, 19484, 27194, 318, 13], "temperature": 0.0, "avg_logprob": -0.16851291408786526, "compression_ratio": 1.76536312849162, "no_speech_prob": 2.5865520001389086e-05}, {"id": 148, "seek": 122300, "start": 1239.0, "end": 1245.0, "text": " We've inserted a space when we did this original add space around the tokens.", "tokens": [492, 600, 27992, 257, 1901, 562, 321, 630, 341, 3380, 909, 1901, 926, 264, 22667, 13], "temperature": 0.0, "avg_logprob": -0.16851291408786526, "compression_ratio": 1.76536312849162, "no_speech_prob": 2.5865520001389086e-05}, {"id": 149, "seek": 124500, "start": 1245.0, "end": 1253.0, "text": " Put that back to apostrophe S. So looking, it's probably easiest to see this on an example.", "tokens": [4935, 300, 646, 281, 19484, 27194, 318, 13, 407, 1237, 11, 309, 311, 1391, 12889, 281, 536, 341, 322, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.12618469662136503, "compression_ratio": 1.558139534883721, "no_speech_prob": 5.594233243755298e-06}, {"id": 150, "seek": 124500, "start": 1253.0, "end": 1259.0, "text": " Looking at, I don't know who Kara's new friend is, is it quote Mr. Toad.", "tokens": [11053, 412, 11, 286, 500, 380, 458, 567, 34838, 311, 777, 1277, 307, 11, 307, 309, 6513, 2221, 13, 1407, 345, 13], "temperature": 0.0, "avg_logprob": -0.12618469662136503, "compression_ratio": 1.558139534883721, "no_speech_prob": 5.594233243755298e-06}, {"id": 151, "seek": 124500, "start": 1259.0, "end": 1272.0, "text": " Sorry for the, I was just trying to think of something that had some various apostrophe S and an apostrophe T in it and quote marks so we could see how these things work.", "tokens": [4919, 337, 264, 11, 286, 390, 445, 1382, 281, 519, 295, 746, 300, 632, 512, 3683, 19484, 27194, 318, 293, 364, 19484, 27194, 314, 294, 309, 293, 6513, 10640, 370, 321, 727, 536, 577, 613, 721, 589, 13], "temperature": 0.0, "avg_logprob": -0.12618469662136503, "compression_ratio": 1.558139534883721, "no_speech_prob": 5.594233243755298e-06}, {"id": 152, "seek": 127200, "start": 1272.0, "end": 1282.0, "text": " We can see, well, this is the final output we'll end up with. I, in this case, do.", "tokens": [492, 393, 536, 11, 731, 11, 341, 307, 264, 2572, 5598, 321, 603, 917, 493, 365, 13, 286, 11, 294, 341, 1389, 11, 360, 13], "temperature": 0.0, "avg_logprob": -0.17498770025041369, "compression_ratio": 1.4848484848484849, "no_speech_prob": 8.013431397557724e-06}, {"id": 153, "seek": 127200, "start": 1282.0, "end": 1287.0, "text": " This is different than not. I mean, you also have not in your set, but an apostrophe T.", "tokens": [639, 307, 819, 813, 406, 13, 286, 914, 11, 291, 611, 362, 406, 294, 428, 992, 11, 457, 364, 19484, 27194, 314, 13], "temperature": 0.0, "avg_logprob": -0.17498770025041369, "compression_ratio": 1.4848484848484849, "no_speech_prob": 8.013431397557724e-06}, {"id": 154, "seek": 127200, "start": 1287.0, "end": 1295.0, "text": " We've added spaces around the double hyphen, spaces around the quote mark.", "tokens": [492, 600, 3869, 7673, 926, 264, 3834, 2477, 47059, 11, 7673, 926, 264, 6513, 1491, 13], "temperature": 0.0, "avg_logprob": -0.17498770025041369, "compression_ratio": 1.4848484848484849, "no_speech_prob": 8.013431397557724e-06}, {"id": 155, "seek": 129500, "start": 1295.0, "end": 1303.0, "text": " Kara's, we've got the apostrophe S and then we can also run through this kind of line by line to see what each line is doing.", "tokens": [34838, 311, 11, 321, 600, 658, 264, 19484, 27194, 318, 293, 550, 321, 393, 611, 1190, 807, 341, 733, 295, 1622, 538, 1622, 281, 536, 437, 1184, 1622, 307, 884, 13], "temperature": 0.0, "avg_logprob": -0.10834582646687825, "compression_ratio": 1.545945945945946, "no_speech_prob": 8.26773884909926e-06}, {"id": 156, "seek": 129500, "start": 1303.0, "end": 1313.0, "text": " So here that first substitution when you find the punctuation has added spaces around it.", "tokens": [407, 510, 300, 700, 35827, 562, 291, 915, 264, 27006, 16073, 575, 3869, 7673, 926, 309, 13], "temperature": 0.0, "avg_logprob": -0.10834582646687825, "compression_ratio": 1.545945945945946, "no_speech_prob": 8.26773884909926e-06}, {"id": 157, "seek": 129500, "start": 1313.0, "end": 1319.0, "text": " Now we've condensed an apostrophe T back from here where it had space.", "tokens": [823, 321, 600, 36398, 364, 19484, 27194, 314, 646, 490, 510, 689, 309, 632, 1901, 13], "temperature": 0.0, "avg_logprob": -0.10834582646687825, "compression_ratio": 1.545945945945946, "no_speech_prob": 8.26773884909926e-06}, {"id": 158, "seek": 131900, "start": 1319.0, "end": 1326.0, "text": " We've condensed apostrophe S back and then we've replaced multiple spaces with a single one.", "tokens": [492, 600, 36398, 19484, 27194, 318, 646, 293, 550, 321, 600, 10772, 3866, 7673, 365, 257, 2167, 472, 13], "temperature": 0.0, "avg_logprob": -0.16006661750174858, "compression_ratio": 1.1896551724137931, "no_speech_prob": 1.9832479665637948e-05}, {"id": 159, "seek": 131900, "start": 1326.0, "end": 1337.0, "text": " Any questions about, about this process?", "tokens": [2639, 1651, 466, 11, 466, 341, 1399, 30], "temperature": 0.0, "avg_logprob": -0.16006661750174858, "compression_ratio": 1.1896551724137931, "no_speech_prob": 1.9832479665637948e-05}, {"id": 160, "seek": 131900, "start": 1337.0, "end": 1343.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.16006661750174858, "compression_ratio": 1.1896551724137931, "no_speech_prob": 1.9832479665637948e-05}, {"id": 161, "seek": 134300, "start": 1343.0, "end": 1353.0, "text": " Hi. Okay. So yeah, that's a good question. When would you want to create your own rather than using a library like Space C? I think in most scenarios you put would probably want to use Space C.", "tokens": [2421, 13, 1033, 13, 407, 1338, 11, 300, 311, 257, 665, 1168, 13, 1133, 576, 291, 528, 281, 1884, 428, 1065, 2831, 813, 1228, 257, 6405, 411, 8705, 383, 30, 286, 519, 294, 881, 15077, 291, 829, 576, 1391, 528, 281, 764, 8705, 383, 13], "temperature": 0.0, "avg_logprob": -0.15979975650185033, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.983227048185654e-05}, {"id": 162, "seek": 134300, "start": 1353.0, "end": 1361.0, "text": " I think this would only be if you wanted to test something specific. I mean, I guess like with fast AI.", "tokens": [286, 519, 341, 576, 787, 312, 498, 291, 1415, 281, 1500, 746, 2685, 13, 286, 914, 11, 286, 2041, 411, 365, 2370, 7318, 13], "temperature": 0.0, "avg_logprob": -0.15979975650185033, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.983227048185654e-05}, {"id": 163, "seek": 134300, "start": 1361.0, "end": 1366.0, "text": " Jeremy did want to look at, you know, some of the things I mentioned in.", "tokens": [17809, 630, 528, 281, 574, 412, 11, 291, 458, 11, 512, 295, 264, 721, 286, 2835, 294, 13], "temperature": 0.0, "avg_logprob": -0.15979975650185033, "compression_ratio": 1.608695652173913, "no_speech_prob": 1.983227048185654e-05}, {"id": 164, "seek": 136600, "start": 1366.0, "end": 1376.0, "text": " This was lesson three. Find this tokenization.", "tokens": [639, 390, 6898, 1045, 13, 11809, 341, 14862, 2144, 13], "temperature": 0.0, "avg_logprob": -0.11840637992410098, "compression_ratio": 1.4782608695652173, "no_speech_prob": 1.1125088349217549e-05}, {"id": 165, "seek": 136600, "start": 1376.0, "end": 1385.0, "text": " So some of these ideas of indicating that the next word is capital indicating repetition.", "tokens": [407, 512, 295, 613, 3487, 295, 25604, 300, 264, 958, 1349, 307, 4238, 25604, 30432, 13], "temperature": 0.0, "avg_logprob": -0.11840637992410098, "compression_ratio": 1.4782608695652173, "no_speech_prob": 1.1125088349217549e-05}, {"id": 166, "seek": 136600, "start": 1385.0, "end": 1388.0, "text": " I don't know that this is necessarily implemented in Space C.", "tokens": [286, 500, 380, 458, 300, 341, 307, 4725, 12270, 294, 8705, 383, 13], "temperature": 0.0, "avg_logprob": -0.11840637992410098, "compression_ratio": 1.4782608695652173, "no_speech_prob": 1.1125088349217549e-05}, {"id": 167, "seek": 136600, "start": 1388.0, "end": 1392.0, "text": " And so here it was something like, oh, we want to test this with fast AI.", "tokens": [400, 370, 510, 309, 390, 746, 411, 11, 1954, 11, 321, 528, 281, 1500, 341, 365, 2370, 7318, 13], "temperature": 0.0, "avg_logprob": -0.11840637992410098, "compression_ratio": 1.4782608695652173, "no_speech_prob": 1.1125088349217549e-05}, {"id": 168, "seek": 139200, "start": 1392.0, "end": 1398.0, "text": " So we would need a way to implement this ourselves. So if you had some particular hypothesis, you might.", "tokens": [407, 321, 576, 643, 257, 636, 281, 4445, 341, 4175, 13, 407, 498, 291, 632, 512, 1729, 17291, 11, 291, 1062, 13], "temperature": 0.0, "avg_logprob": -0.11664582335430643, "compression_ratio": 1.4721030042918455, "no_speech_prob": 1.0450434274389409e-05}, {"id": 169, "seek": 139200, "start": 1398.0, "end": 1402.0, "text": " But yeah, I feel like in most purposes use Space C's tokenizer.", "tokens": [583, 1338, 11, 286, 841, 411, 294, 881, 9932, 764, 8705, 383, 311, 14862, 6545, 13], "temperature": 0.0, "avg_logprob": -0.11664582335430643, "compression_ratio": 1.4721030042918455, "no_speech_prob": 1.0450434274389409e-05}, {"id": 170, "seek": 139200, "start": 1402.0, "end": 1413.0, "text": " And this is more kind of just to understand a way that you could do it, but you probably won't need to in practice.", "tokens": [400, 341, 307, 544, 733, 295, 445, 281, 1223, 257, 636, 300, 291, 727, 360, 309, 11, 457, 291, 1391, 1582, 380, 643, 281, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.11664582335430643, "compression_ratio": 1.4721030042918455, "no_speech_prob": 1.0450434274389409e-05}, {"id": 171, "seek": 139200, "start": 1413.0, "end": 1419.0, "text": " That's a good question. I'm not sure. Do you know, Jeremy?", "tokens": [663, 311, 257, 665, 1168, 13, 286, 478, 406, 988, 13, 1144, 291, 458, 11, 17809, 30], "temperature": 0.0, "avg_logprob": -0.11664582335430643, "compression_ratio": 1.4721030042918455, "no_speech_prob": 1.0450434274389409e-05}, {"id": 172, "seek": 141900, "start": 1419.0, "end": 1432.0, "text": " A combination.", "tokens": [316, 6562, 13], "temperature": 0.0, "avg_logprob": -0.17713494734330612, "compression_ratio": 1.391025641025641, "no_speech_prob": 7.596713840030134e-05}, {"id": 173, "seek": 141900, "start": 1432.0, "end": 1438.0, "text": " And just to repeat that for the microphone, Jeremy was saying that libraries such as Space C typically use a combination.", "tokens": [400, 445, 281, 7149, 300, 337, 264, 10952, 11, 17809, 390, 1566, 300, 15148, 1270, 382, 8705, 383, 5850, 764, 257, 6562, 13], "temperature": 0.0, "avg_logprob": -0.17713494734330612, "compression_ratio": 1.391025641025641, "no_speech_prob": 7.596713840030134e-05}, {"id": 174, "seek": 141900, "start": 1438.0, "end": 1447.0, "text": " Maybe using regex to parse URLs, but to have parsers in C for many other things.", "tokens": [2704, 1228, 319, 432, 87, 281, 48377, 43267, 11, 457, 281, 362, 21156, 433, 294, 383, 337, 867, 661, 721, 13], "temperature": 0.0, "avg_logprob": -0.17713494734330612, "compression_ratio": 1.391025641025641, "no_speech_prob": 7.596713840030134e-05}, {"id": 175, "seek": 144700, "start": 1447.0, "end": 1453.0, "text": " Welcome. Thanks.", "tokens": [4027, 13, 2561, 13], "temperature": 0.0, "avg_logprob": -0.13288904272991678, "compression_ratio": 1.4120879120879122, "no_speech_prob": 3.269631270086393e-05}, {"id": 176, "seek": 144700, "start": 1453.0, "end": 1457.0, "text": " Other questions about this?", "tokens": [5358, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.13288904272991678, "compression_ratio": 1.4120879120879122, "no_speech_prob": 3.269631270086393e-05}, {"id": 177, "seek": 144700, "start": 1457.0, "end": 1467.0, "text": " Yeah, so this is something this is more kind of to know a way that you could do this, but you probably won't need to, for instance, like in a workplace project.", "tokens": [865, 11, 370, 341, 307, 746, 341, 307, 544, 733, 295, 281, 458, 257, 636, 300, 291, 727, 360, 341, 11, 457, 291, 1391, 1582, 380, 643, 281, 11, 337, 5197, 11, 411, 294, 257, 15328, 1716, 13], "temperature": 0.0, "avg_logprob": -0.13288904272991678, "compression_ratio": 1.4120879120879122, "no_speech_prob": 3.269631270086393e-05}, {"id": 178, "seek": 144700, "start": 1467.0, "end": 1470.0, "text": " Although you might be using regex for other things.", "tokens": [5780, 291, 1062, 312, 1228, 319, 432, 87, 337, 661, 721, 13], "temperature": 0.0, "avg_logprob": -0.13288904272991678, "compression_ratio": 1.4120879120879122, "no_speech_prob": 3.269631270086393e-05}, {"id": 179, "seek": 147000, "start": 1470.0, "end": 1479.0, "text": " Like I think the some of the examples earlier of cleaning up data, like, you know, if you have this messy column of phone numbers, like that's definitely an application.", "tokens": [1743, 286, 519, 264, 512, 295, 264, 5110, 3071, 295, 8924, 493, 1412, 11, 411, 11, 291, 458, 11, 498, 291, 362, 341, 16191, 7738, 295, 2593, 3547, 11, 411, 300, 311, 2138, 364, 3861, 13], "temperature": 0.0, "avg_logprob": -0.11172433853149415, "compression_ratio": 1.6761133603238867, "no_speech_prob": 1.1841959349112585e-05}, {"id": 180, "seek": 147000, "start": 1479.0, "end": 1486.0, "text": " You could see the cleaning up the file names where they were trying to pick off the what came before the ending.", "tokens": [509, 727, 536, 264, 8924, 493, 264, 3991, 5288, 689, 436, 645, 1382, 281, 1888, 766, 264, 437, 1361, 949, 264, 8121, 13], "temperature": 0.0, "avg_logprob": -0.11172433853149415, "compression_ratio": 1.6761133603238867, "no_speech_prob": 1.1841959349112585e-05}, {"id": 181, "seek": 147000, "start": 1486.0, "end": 1494.0, "text": " And you might have something where, you know, you're trying to find all the JPEGs and PNGs and need to filter out other file types.", "tokens": [400, 291, 1062, 362, 746, 689, 11, 291, 458, 11, 291, 434, 1382, 281, 915, 439, 264, 508, 5208, 33715, 293, 430, 30237, 82, 293, 643, 281, 6608, 484, 661, 3991, 3467, 13], "temperature": 0.0, "avg_logprob": -0.11172433853149415, "compression_ratio": 1.6761133603238867, "no_speech_prob": 1.1841959349112585e-05}, {"id": 182, "seek": 149400, "start": 1494.0, "end": 1500.0, "text": " So regex could definitely be useful for that.", "tokens": [407, 319, 432, 87, 727, 2138, 312, 4420, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.09275513109953507, "compression_ratio": 1.6260504201680672, "no_speech_prob": 1.4737959645572118e-05}, {"id": 183, "seek": 149400, "start": 1500.0, "end": 1502.0, "text": " So here I wanted to show the second step as well.", "tokens": [407, 510, 286, 1415, 281, 855, 264, 1150, 1823, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.09275513109953507, "compression_ratio": 1.6260504201680672, "no_speech_prob": 1.4737959645572118e-05}, {"id": 184, "seek": 149400, "start": 1502.0, "end": 1505.0, "text": " So kind of we have this first step of generating our tokens.", "tokens": [407, 733, 295, 321, 362, 341, 700, 1823, 295, 17746, 527, 22667, 13], "temperature": 0.0, "avg_logprob": -0.09275513109953507, "compression_ratio": 1.6260504201680672, "no_speech_prob": 1.4737959645572118e-05}, {"id": 185, "seek": 149400, "start": 1505.0, "end": 1520.0, "text": " And then I mentioned pretty much always once you have tokens, you want to use those to come up with your vocabulary and to come up with this mapping from tokens to IDs and translating the texture working at into a sequence of IDs.", "tokens": [400, 550, 286, 2835, 1238, 709, 1009, 1564, 291, 362, 22667, 11, 291, 528, 281, 764, 729, 281, 808, 493, 365, 428, 19864, 293, 281, 808, 493, 365, 341, 18350, 490, 22667, 281, 48212, 293, 35030, 264, 8091, 1364, 412, 666, 257, 8310, 295, 48212, 13], "temperature": 0.0, "avg_logprob": -0.09275513109953507, "compression_ratio": 1.6260504201680672, "no_speech_prob": 1.4737959645572118e-05}, {"id": 186, "seek": 152000, "start": 1520.0, "end": 1524.0, "text": " So that's that's what you'll actually be processing.", "tokens": [407, 300, 311, 300, 311, 437, 291, 603, 767, 312, 9007, 13], "temperature": 0.0, "avg_logprob": -0.10068263678715147, "compression_ratio": 1.4791666666666667, "no_speech_prob": 2.5464954887866043e-05}, {"id": 187, "seek": 152000, "start": 1524.0, "end": 1530.0, "text": " So here I have a list of six sentences.", "tokens": [407, 510, 286, 362, 257, 1329, 295, 2309, 16579, 13], "temperature": 0.0, "avg_logprob": -0.10068263678715147, "compression_ratio": 1.4791666666666667, "no_speech_prob": 2.5464954887866043e-05}, {"id": 188, "seek": 152000, "start": 1530.0, "end": 1532.0, "text": " I can apply.", "tokens": [286, 393, 3079, 13], "temperature": 0.0, "avg_logprob": -0.10068263678715147, "compression_ratio": 1.4791666666666667, "no_speech_prob": 2.5464954887866043e-05}, {"id": 189, "seek": 152000, "start": 1532.0, "end": 1545.0, "text": " So here I'm mapping my simple tokens method from right here to the sentences and creating a list with that.", "tokens": [407, 510, 286, 478, 18350, 452, 2199, 22667, 3170, 490, 558, 510, 281, 264, 16579, 293, 4084, 257, 1329, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.10068263678715147, "compression_ratio": 1.4791666666666667, "no_speech_prob": 2.5464954887866043e-05}, {"id": 190, "seek": 154500, "start": 1545.0, "end": 1558.0, "text": " So here I have a list of tokens and then I have a tokens to ID method that can take the sentences and create a vocabulary.", "tokens": [407, 510, 286, 362, 257, 1329, 295, 22667, 293, 550, 286, 362, 257, 22667, 281, 7348, 3170, 300, 393, 747, 264, 16579, 293, 1884, 257, 19864, 13], "temperature": 0.0, "avg_logprob": -0.12263348367479113, "compression_ratio": 1.5935828877005347, "no_speech_prob": 7.7667737059528e-06}, {"id": 191, "seek": 154500, "start": 1558.0, "end": 1570.0, "text": " And so kind of typical things you'll often want to get back from this would be the IDs, which here is the kind of the format of rewriting my sentences as a sequence of tokens.", "tokens": [400, 370, 733, 295, 7476, 721, 291, 603, 2049, 528, 281, 483, 646, 490, 341, 576, 312, 264, 48212, 11, 597, 510, 307, 264, 733, 295, 264, 7877, 295, 319, 19868, 452, 16579, 382, 257, 8310, 295, 22667, 13], "temperature": 0.0, "avg_logprob": -0.12263348367479113, "compression_ratio": 1.5935828877005347, "no_speech_prob": 7.7667737059528e-06}, {"id": 192, "seek": 157000, "start": 1570.0, "end": 1576.0, "text": " So here you can check this first sentence has one, two, three, four, five, six, seven, eight.", "tokens": [407, 510, 291, 393, 1520, 341, 700, 8174, 575, 472, 11, 732, 11, 1045, 11, 1451, 11, 1732, 11, 2309, 11, 3407, 11, 3180, 13], "temperature": 0.0, "avg_logprob": -0.1365237571823765, "compression_ratio": 1.3950617283950617, "no_speech_prob": 1.6964007954811677e-05}, {"id": 193, "seek": 157000, "start": 1576.0, "end": 1579.0, "text": " Does that fit with?", "tokens": [4402, 300, 3318, 365, 30], "temperature": 0.0, "avg_logprob": -0.1365237571823765, "compression_ratio": 1.3950617283950617, "no_speech_prob": 1.6964007954811677e-05}, {"id": 194, "seek": 157000, "start": 1579.0, "end": 1585.0, "text": " Oh, it's returning them in reverse order, I think.", "tokens": [876, 11, 309, 311, 12678, 552, 294, 9943, 1668, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.1365237571823765, "compression_ratio": 1.3950617283950617, "no_speech_prob": 1.6964007954811677e-05}, {"id": 195, "seek": 157000, "start": 1585.0, "end": 1587.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.1365237571823765, "compression_ratio": 1.3950617283950617, "no_speech_prob": 1.6964007954811677e-05}, {"id": 196, "seek": 157000, "start": 1587.0, "end": 1593.0, "text": " Actually, maybe, maybe that is not not in order at all.", "tokens": [5135, 11, 1310, 11, 1310, 300, 307, 406, 406, 294, 1668, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.1365237571823765, "compression_ratio": 1.3950617283950617, "no_speech_prob": 1.6964007954811677e-05}, {"id": 197, "seek": 159300, "start": 1593.0, "end": 1601.0, "text": " We could convert these convert these back to two words if we wanted to check kind of what do you what do you get back?", "tokens": [492, 727, 7620, 613, 7620, 613, 646, 281, 732, 2283, 498, 321, 1415, 281, 1520, 733, 295, 437, 360, 291, 437, 360, 291, 483, 646, 30], "temperature": 0.0, "avg_logprob": -0.10545032364981514, "compression_ratio": 1.5974842767295598, "no_speech_prob": 8.939507097238675e-06}, {"id": 198, "seek": 159300, "start": 1601.0, "end": 1603.0, "text": " We'll want to know what our vocabulary is.", "tokens": [492, 603, 528, 281, 458, 437, 527, 19864, 307, 13], "temperature": 0.0, "avg_logprob": -0.10545032364981514, "compression_ratio": 1.5974842767295598, "no_speech_prob": 8.939507097238675e-06}, {"id": 199, "seek": 159300, "start": 1603.0, "end": 1606.0, "text": " So here's a list of the vocab.", "tokens": [407, 510, 311, 257, 1329, 295, 264, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.10545032364981514, "compression_ratio": 1.5974842767295598, "no_speech_prob": 8.939507097238675e-06}, {"id": 200, "seek": 159300, "start": 1606.0, "end": 1614.0, "text": " And what's what could be another name of this vocab variable?", "tokens": [400, 437, 311, 437, 727, 312, 1071, 1315, 295, 341, 2329, 455, 7006, 30], "temperature": 0.0, "avg_logprob": -0.10545032364981514, "compression_ratio": 1.5974842767295598, "no_speech_prob": 8.939507097238675e-06}, {"id": 201, "seek": 161400, "start": 1614.0, "end": 1623.0, "text": " This is something we've talked about previously.", "tokens": [639, 307, 746, 321, 600, 2825, 466, 8046, 13], "temperature": 0.0, "avg_logprob": -0.10241252928972244, "compression_ratio": 1.3660130718954249, "no_speech_prob": 4.637797701434465e-06}, {"id": 202, "seek": 161400, "start": 1623.0, "end": 1624.0, "text": " I heard someone say IDs.", "tokens": [286, 2198, 1580, 584, 48212, 13], "temperature": 0.0, "avg_logprob": -0.10241252928972244, "compression_ratio": 1.3660130718954249, "no_speech_prob": 4.637797701434465e-06}, {"id": 203, "seek": 161400, "start": 1624.0, "end": 1627.0, "text": " Do they want to say it louder?", "tokens": [1144, 436, 528, 281, 584, 309, 22717, 30], "temperature": 0.0, "avg_logprob": -0.10241252928972244, "compression_ratio": 1.3660130718954249, "no_speech_prob": 4.637797701434465e-06}, {"id": 204, "seek": 161400, "start": 1627.0, "end": 1631.0, "text": " Yeah, ID to word or ID to token.", "tokens": [865, 11, 7348, 281, 1349, 420, 7348, 281, 14862, 13], "temperature": 0.0, "avg_logprob": -0.10241252928972244, "compression_ratio": 1.3660130718954249, "no_speech_prob": 4.637797701434465e-06}, {"id": 205, "seek": 161400, "start": 1631.0, "end": 1637.0, "text": " So this list is giving us a map of how we get from a number to a token.", "tokens": [407, 341, 1329, 307, 2902, 505, 257, 4471, 295, 577, 321, 483, 490, 257, 1230, 281, 257, 14862, 13], "temperature": 0.0, "avg_logprob": -0.10241252928972244, "compression_ratio": 1.3660130718954249, "no_speech_prob": 4.637797701434465e-06}, {"id": 206, "seek": 163700, "start": 1637.0, "end": 1647.0, "text": " And we could look it up if you chose four, zero, one, two, three, four, vocab four is going to give you I.", "tokens": [400, 321, 727, 574, 309, 493, 498, 291, 5111, 1451, 11, 4018, 11, 472, 11, 732, 11, 1045, 11, 1451, 11, 2329, 455, 1451, 307, 516, 281, 976, 291, 286, 13], "temperature": 0.0, "avg_logprob": -0.09344882349814138, "compression_ratio": 1.5458937198067633, "no_speech_prob": 4.860345143242739e-06}, {"id": 207, "seek": 163700, "start": 1647.0, "end": 1650.0, "text": " That's a way to get from your IDs to your tokens.", "tokens": [663, 311, 257, 636, 281, 483, 490, 428, 48212, 281, 428, 22667, 13], "temperature": 0.0, "avg_logprob": -0.09344882349814138, "compression_ratio": 1.5458937198067633, "no_speech_prob": 4.860345143242739e-06}, {"id": 208, "seek": 163700, "start": 1650.0, "end": 1657.0, "text": " And the reason this is a list is the the integers kind of implicit in how you store a list.", "tokens": [400, 264, 1778, 341, 307, 257, 1329, 307, 264, 264, 41674, 733, 295, 26947, 294, 577, 291, 3531, 257, 1329, 13], "temperature": 0.0, "avg_logprob": -0.09344882349814138, "compression_ratio": 1.5458937198067633, "no_speech_prob": 4.860345143242739e-06}, {"id": 209, "seek": 163700, "start": 1657.0, "end": 1664.0, "text": " Whereas when we go words to ID or tokens to ID, we'll use a dictionary.", "tokens": [13813, 562, 321, 352, 2283, 281, 7348, 420, 22667, 281, 7348, 11, 321, 603, 764, 257, 25890, 13], "temperature": 0.0, "avg_logprob": -0.09344882349814138, "compression_ratio": 1.5458937198067633, "no_speech_prob": 4.860345143242739e-06}, {"id": 210, "seek": 166400, "start": 1664.0, "end": 1676.0, "text": " And you can see, yes, I I was mapped to four and having that token to ID and ID to token are kind of two useful mappings you'll typically want.", "tokens": [400, 291, 393, 536, 11, 2086, 11, 286, 286, 390, 33318, 281, 1451, 293, 1419, 300, 14862, 281, 7348, 293, 7348, 281, 14862, 366, 733, 295, 732, 4420, 463, 28968, 291, 603, 5850, 528, 13], "temperature": 0.0, "avg_logprob": -0.1353011615034463, "compression_ratio": 1.4152046783625731, "no_speech_prob": 1.1125392120447941e-05}, {"id": 211, "seek": 166400, "start": 1676.0, "end": 1679.0, "text": " So you can get back and forth.", "tokens": [407, 291, 393, 483, 646, 293, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1353011615034463, "compression_ratio": 1.4152046783625731, "no_speech_prob": 1.1125392120447941e-05}, {"id": 212, "seek": 166400, "start": 1679.0, "end": 1684.0, "text": " Actually, let me just add.", "tokens": [5135, 11, 718, 385, 445, 909, 13], "temperature": 0.0, "avg_logprob": -0.1353011615034463, "compression_ratio": 1.4152046783625731, "no_speech_prob": 1.1125392120447941e-05}, {"id": 213, "seek": 166400, "start": 1684.0, "end": 1687.0, "text": " Let's go through one of these sentences.", "tokens": [961, 311, 352, 807, 472, 295, 613, 16579, 13], "temperature": 0.0, "avg_logprob": -0.1353011615034463, "compression_ratio": 1.4152046783625731, "no_speech_prob": 1.1125392120447941e-05}, {"id": 214, "seek": 168700, "start": 1687.0, "end": 1694.0, "text": " So to check out the first sentence, we've got this.", "tokens": [407, 281, 1520, 484, 264, 700, 8174, 11, 321, 600, 658, 341, 13], "temperature": 0.0, "avg_logprob": -0.0944565302365786, "compression_ratio": 1.5389221556886228, "no_speech_prob": 1.8738446669885889e-06}, {"id": 215, "seek": 168700, "start": 1694.0, "end": 1705.0, "text": " If we wanted to make that back into a sentence, we would want to use vocab I for I and IDs.", "tokens": [759, 321, 1415, 281, 652, 300, 646, 666, 257, 8174, 11, 321, 576, 528, 281, 764, 2329, 455, 286, 337, 286, 293, 48212, 13], "temperature": 0.0, "avg_logprob": -0.0944565302365786, "compression_ratio": 1.5389221556886228, "no_speech_prob": 1.8738446669885889e-06}, {"id": 216, "seek": 168700, "start": 1705.0, "end": 1707.0, "text": " All this happened, more or less.", "tokens": [1057, 341, 2011, 11, 544, 420, 1570, 13], "temperature": 0.0, "avg_logprob": -0.0944565302365786, "compression_ratio": 1.5389221556886228, "no_speech_prob": 1.8738446669885889e-06}, {"id": 217, "seek": 168700, "start": 1707.0, "end": 1709.0, "text": " So that's given us back our sentence.", "tokens": [407, 300, 311, 2212, 505, 646, 527, 8174, 13], "temperature": 0.0, "avg_logprob": -0.0944565302365786, "compression_ratio": 1.5389221556886228, "no_speech_prob": 1.8738446669885889e-06}, {"id": 218, "seek": 168700, "start": 1709.0, "end": 1713.0, "text": " So that's how we can can get between them.", "tokens": [407, 300, 311, 577, 321, 393, 393, 483, 1296, 552, 13], "temperature": 0.0, "avg_logprob": -0.0944565302365786, "compression_ratio": 1.5389221556886228, "no_speech_prob": 1.8738446669885889e-06}, {"id": 219, "seek": 171300, "start": 1713.0, "end": 1720.0, "text": " Any questions about that?", "tokens": [2639, 1651, 466, 300, 30], "temperature": 0.0, "avg_logprob": -0.20692089241994938, "compression_ratio": 1.48, "no_speech_prob": 1.2029260687995702e-05}, {"id": 220, "seek": 171300, "start": 1720.0, "end": 1725.0, "text": " OK, so that's a few more things to know about.", "tokens": [2264, 11, 370, 300, 311, 257, 1326, 544, 721, 281, 458, 466, 13], "temperature": 0.0, "avg_logprob": -0.20692089241994938, "compression_ratio": 1.48, "no_speech_prob": 1.2029260687995702e-05}, {"id": 221, "seek": 171300, "start": 1725.0, "end": 1731.0, "text": " Redjacks, it's very useful finding and searching, finding and replacing.", "tokens": [4477, 73, 7424, 11, 309, 311, 588, 4420, 5006, 293, 10808, 11, 5006, 293, 19139, 13], "temperature": 0.0, "avg_logprob": -0.20692089241994938, "compression_ratio": 1.48, "no_speech_prob": 1.2029260687995702e-05}, {"id": 222, "seek": 171300, "start": 1731.0, "end": 1739.0, "text": " I should mention that in in Python with the re library, there's a redot search, which is kind of a useful method.", "tokens": [286, 820, 2152, 300, 294, 294, 15329, 365, 264, 319, 6405, 11, 456, 311, 257, 2182, 310, 3164, 11, 597, 307, 733, 295, 257, 4420, 3170, 13], "temperature": 0.0, "avg_logprob": -0.20692089241994938, "compression_ratio": 1.48, "no_speech_prob": 1.2029260687995702e-05}, {"id": 223, "seek": 173900, "start": 1739.0, "end": 1743.0, "text": " Above, we were using redot compile and redot substitute.", "tokens": [32691, 11, 321, 645, 1228, 2182, 310, 31413, 293, 2182, 310, 15802, 13], "temperature": 0.0, "avg_logprob": -0.1093517682131599, "compression_ratio": 1.455497382198953, "no_speech_prob": 3.6687640658783494e-06}, {"id": 224, "seek": 173900, "start": 1743.0, "end": 1746.0, "text": " But redot search is another one you'll see a lot of.", "tokens": [583, 2182, 310, 3164, 307, 1071, 472, 291, 603, 536, 257, 688, 295, 13], "temperature": 0.0, "avg_logprob": -0.1093517682131599, "compression_ratio": 1.455497382198953, "no_speech_prob": 3.6687640658783494e-06}, {"id": 225, "seek": 173900, "start": 1746.0, "end": 1759.0, "text": " And cleaning data, I think, is definitely a big one, particularly when you have something that's supposed to have a somewhat generalizable format such as phone numbers.", "tokens": [400, 8924, 1412, 11, 286, 519, 11, 307, 2138, 257, 955, 472, 11, 4098, 562, 291, 362, 746, 300, 311, 3442, 281, 362, 257, 8344, 2674, 22395, 7877, 1270, 382, 2593, 3547, 13], "temperature": 0.0, "avg_logprob": -0.1093517682131599, "compression_ratio": 1.455497382198953, "no_speech_prob": 3.6687640658783494e-06}, {"id": 226, "seek": 175900, "start": 1759.0, "end": 1773.0, "text": " Python does have some string methods which you can use.", "tokens": [15329, 775, 362, 512, 6798, 7150, 597, 291, 393, 764, 13], "temperature": 0.0, "avg_logprob": -0.1423066489550532, "compression_ratio": 1.3888888888888888, "no_speech_prob": 9.516029422229622e-06}, {"id": 227, "seek": 175900, "start": 1773.0, "end": 1780.0, "text": " To find the first occurrence of something, string methods might be a little bit clearer to read.", "tokens": [1407, 915, 264, 700, 36122, 295, 746, 11, 6798, 7150, 1062, 312, 257, 707, 857, 26131, 281, 1401, 13], "temperature": 0.0, "avg_logprob": -0.1423066489550532, "compression_ratio": 1.3888888888888888, "no_speech_prob": 9.516029422229622e-06}, {"id": 228, "seek": 175900, "start": 1780.0, "end": 1788.0, "text": " However, Redjacks can handle broader use cases.", "tokens": [2908, 11, 4477, 73, 7424, 393, 4813, 13227, 764, 3331, 13], "temperature": 0.0, "avg_logprob": -0.1423066489550532, "compression_ratio": 1.3888888888888888, "no_speech_prob": 9.516029422229622e-06}, {"id": 229, "seek": 178800, "start": 1788.0, "end": 1790.0, "text": " Redjacks can be language independent.", "tokens": [4477, 73, 7424, 393, 312, 2856, 6695, 13], "temperature": 0.0, "avg_logprob": -0.0762258757145033, "compression_ratio": 1.5126050420168067, "no_speech_prob": 3.534993447829038e-05}, {"id": 230, "seek": 178800, "start": 1790.0, "end": 1794.0, "text": " You'll see versions of it kind of in so many different languages.", "tokens": [509, 603, 536, 9606, 295, 309, 733, 295, 294, 370, 867, 819, 8650, 13], "temperature": 0.0, "avg_logprob": -0.0762258757145033, "compression_ratio": 1.5126050420168067, "no_speech_prob": 3.534993447829038e-05}, {"id": 231, "seek": 178800, "start": 1794.0, "end": 1797.0, "text": " And it can also be faster at scale.", "tokens": [400, 309, 393, 611, 312, 4663, 412, 4373, 13], "temperature": 0.0, "avg_logprob": -0.0762258757145033, "compression_ratio": 1.5126050420168067, "no_speech_prob": 3.534993447829038e-05}, {"id": 232, "seek": 178800, "start": 1797.0, "end": 1799.0, "text": " This was just for fun.", "tokens": [639, 390, 445, 337, 1019, 13], "temperature": 0.0, "avg_logprob": -0.0762258757145033, "compression_ratio": 1.5126050420168067, "no_speech_prob": 3.534993447829038e-05}, {"id": 233, "seek": 178800, "start": 1799.0, "end": 1804.0, "text": " This is, I guess, not NLP, but I did Redjacks on Unicode.", "tokens": [639, 307, 11, 286, 2041, 11, 406, 426, 45196, 11, 457, 286, 630, 4477, 73, 7424, 322, 1156, 299, 1429, 13], "temperature": 0.0, "avg_logprob": -0.0762258757145033, "compression_ratio": 1.5126050420168067, "no_speech_prob": 3.534993447829038e-05}, {"id": 234, "seek": 178800, "start": 1804.0, "end": 1806.0, "text": " And let me zoom in a little bit so you can see this.", "tokens": [400, 718, 385, 8863, 294, 257, 707, 857, 370, 291, 393, 536, 341, 13], "temperature": 0.0, "avg_logprob": -0.0762258757145033, "compression_ratio": 1.5126050420168067, "no_speech_prob": 3.534993447829038e-05}, {"id": 235, "seek": 178800, "start": 1806.0, "end": 1814.0, "text": " So here I have a message of frowny face about the movie and nauseated about the pizza.", "tokens": [407, 510, 286, 362, 257, 3636, 295, 431, 648, 88, 1851, 466, 264, 3169, 293, 34735, 770, 466, 264, 8298, 13], "temperature": 0.0, "avg_logprob": -0.0762258757145033, "compression_ratio": 1.5126050420168067, "no_speech_prob": 3.534993447829038e-05}, {"id": 236, "seek": 181400, "start": 1814.0, "end": 1823.0, "text": " And I wanted to try substituting smiley faces in for the frowning and nauseated faces.", "tokens": [400, 286, 1415, 281, 853, 26441, 10861, 7563, 88, 8475, 294, 337, 264, 431, 648, 278, 293, 34735, 770, 8475, 13], "temperature": 0.0, "avg_logprob": -0.10014338991535243, "compression_ratio": 1.5060240963855422, "no_speech_prob": 2.3550406694994308e-05}, {"id": 237, "seek": 181400, "start": 1823.0, "end": 1830.0, "text": " And did that and got back the message, smiling about the movie and smiling about the pizza.", "tokens": [400, 630, 300, 293, 658, 646, 264, 3636, 11, 16005, 466, 264, 3169, 293, 16005, 466, 264, 8298, 13], "temperature": 0.0, "avg_logprob": -0.10014338991535243, "compression_ratio": 1.5060240963855422, "no_speech_prob": 2.3550406694994308e-05}, {"id": 238, "seek": 181400, "start": 1830.0, "end": 1837.0, "text": " What is the pipe doing here in my regular expression?", "tokens": [708, 307, 264, 11240, 884, 510, 294, 452, 3890, 6114, 30], "temperature": 0.0, "avg_logprob": -0.10014338991535243, "compression_ratio": 1.5060240963855422, "no_speech_prob": 2.3550406694994308e-05}, {"id": 239, "seek": 181400, "start": 1837.0, "end": 1838.0, "text": " That's right, or.", "tokens": [663, 311, 558, 11, 420, 13], "temperature": 0.0, "avg_logprob": -0.10014338991535243, "compression_ratio": 1.5060240963855422, "no_speech_prob": 2.3550406694994308e-05}, {"id": 240, "seek": 183800, "start": 1838.0, "end": 1844.0, "text": " So here this is looking for either of those two upset faces.", "tokens": [407, 510, 341, 307, 1237, 337, 2139, 295, 729, 732, 8340, 8475, 13], "temperature": 0.0, "avg_logprob": -0.09852174612192008, "compression_ratio": 1.4917127071823204, "no_speech_prob": 3.7852278182981536e-06}, {"id": 241, "seek": 183800, "start": 1844.0, "end": 1849.0, "text": " And then I use substitute to get a smiley face in.", "tokens": [400, 550, 286, 764, 15802, 281, 483, 257, 7563, 88, 1851, 294, 13], "temperature": 0.0, "avg_logprob": -0.09852174612192008, "compression_ratio": 1.4917127071823204, "no_speech_prob": 3.7852278182981536e-06}, {"id": 242, "seek": 183800, "start": 1849.0, "end": 1853.0, "text": " Any question about this?", "tokens": [2639, 1168, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.09852174612192008, "compression_ratio": 1.4917127071823204, "no_speech_prob": 3.7852278182981536e-06}, {"id": 243, "seek": 183800, "start": 1853.0, "end": 1855.0, "text": " And there was a question.", "tokens": [400, 456, 390, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.09852174612192008, "compression_ratio": 1.4917127071823204, "no_speech_prob": 3.7852278182981536e-06}, {"id": 244, "seek": 183800, "start": 1855.0, "end": 1859.0, "text": " The reason I thought of this was someone asked last time about Redjacks and UTF-8.", "tokens": [440, 1778, 286, 1194, 295, 341, 390, 1580, 2351, 1036, 565, 466, 4477, 73, 7424, 293, 624, 20527, 12, 23, 13], "temperature": 0.0, "avg_logprob": -0.09852174612192008, "compression_ratio": 1.4917127071823204, "no_speech_prob": 3.7852278182981536e-06}, {"id": 245, "seek": 183800, "start": 1859.0, "end": 1861.0, "text": " And it should work fine.", "tokens": [400, 309, 820, 589, 2489, 13], "temperature": 0.0, "avg_logprob": -0.09852174612192008, "compression_ratio": 1.4917127071823204, "no_speech_prob": 3.7852278182981536e-06}, {"id": 246, "seek": 186100, "start": 1861.0, "end": 1869.0, "text": " Python 3 handles Unicode.", "tokens": [15329, 805, 18722, 1156, 299, 1429, 13], "temperature": 0.0, "avg_logprob": -0.12624216079711914, "compression_ratio": 1.6984126984126984, "no_speech_prob": 2.247311749670189e-05}, {"id": 247, "seek": 186100, "start": 1869.0, "end": 1870.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.12624216079711914, "compression_ratio": 1.6984126984126984, "no_speech_prob": 2.247311749670189e-05}, {"id": 248, "seek": 186100, "start": 1870.0, "end": 1873.0, "text": " So Redjacks errors.", "tokens": [407, 4477, 73, 7424, 13603, 13], "temperature": 0.0, "avg_logprob": -0.12624216079711914, "compression_ratio": 1.6984126984126984, "no_speech_prob": 2.247311749670189e-05}, {"id": 249, "seek": 186100, "start": 1873.0, "end": 1877.0, "text": " So you have to worry about both false positives and false negatives.", "tokens": [407, 291, 362, 281, 3292, 466, 1293, 7908, 35127, 293, 7908, 40019, 13], "temperature": 0.0, "avg_logprob": -0.12624216079711914, "compression_ratio": 1.6984126984126984, "no_speech_prob": 2.247311749670189e-05}, {"id": 250, "seek": 186100, "start": 1877.0, "end": 1886.0, "text": " And that's something I like that Redjacks 1 and Redjacks Tuesdays give you in terms of having like these are the cases you do want to match.", "tokens": [400, 300, 311, 746, 286, 411, 300, 4477, 73, 7424, 502, 293, 4477, 73, 7424, 10017, 82, 976, 291, 294, 2115, 295, 1419, 411, 613, 366, 264, 3331, 291, 360, 528, 281, 2995, 13], "temperature": 0.0, "avg_logprob": -0.12624216079711914, "compression_ratio": 1.6984126984126984, "no_speech_prob": 2.247311749670189e-05}, {"id": 251, "seek": 186100, "start": 1886.0, "end": 1889.0, "text": " And then these are the cases you do not want to match.", "tokens": [400, 550, 613, 366, 264, 3331, 291, 360, 406, 528, 281, 2995, 13], "temperature": 0.0, "avg_logprob": -0.12624216079711914, "compression_ratio": 1.6984126984126984, "no_speech_prob": 2.247311749670189e-05}, {"id": 252, "seek": 188900, "start": 1889.0, "end": 1892.0, "text": " And it's important to look at both.", "tokens": [400, 309, 311, 1021, 281, 574, 412, 1293, 13], "temperature": 0.0, "avg_logprob": -0.13955110822405134, "compression_ratio": 1.4457831325301205, "no_speech_prob": 7.5278535405232105e-06}, {"id": 253, "seek": 188900, "start": 1892.0, "end": 1900.0, "text": " Trying to have tests for both to minimize your false positives and minimize your false negatives.", "tokens": [20180, 281, 362, 6921, 337, 1293, 281, 17522, 428, 7908, 35127, 293, 17522, 428, 7908, 40019, 13], "temperature": 0.0, "avg_logprob": -0.13955110822405134, "compression_ratio": 1.4457831325301205, "no_speech_prob": 7.5278535405232105e-06}, {"id": 254, "seek": 188900, "start": 1900.0, "end": 1903.0, "text": " Here are a few more useful websites.", "tokens": [1692, 366, 257, 1326, 544, 4420, 12891, 13], "temperature": 0.0, "avg_logprob": -0.13955110822405134, "compression_ratio": 1.4457831325301205, "no_speech_prob": 7.5278535405232105e-06}, {"id": 255, "seek": 188900, "start": 1903.0, "end": 1906.0, "text": " Redjacks R is kind of neat too.", "tokens": [4477, 73, 7424, 497, 307, 733, 295, 10654, 886, 13], "temperature": 0.0, "avg_logprob": -0.13955110822405134, "compression_ratio": 1.4457831325301205, "no_speech_prob": 7.5278535405232105e-06}, {"id": 256, "seek": 188900, "start": 1906.0, "end": 1909.0, "text": " When you.", "tokens": [1133, 291, 13], "temperature": 0.0, "avg_logprob": -0.13955110822405134, "compression_ratio": 1.4457831325301205, "no_speech_prob": 7.5278535405232105e-06}, {"id": 257, "seek": 188900, "start": 1909.0, "end": 1912.0, "text": " Oh, and I meant to copy it.", "tokens": [876, 11, 293, 286, 4140, 281, 5055, 309, 13], "temperature": 0.0, "avg_logprob": -0.13955110822405134, "compression_ratio": 1.4457831325301205, "no_speech_prob": 7.5278535405232105e-06}, {"id": 258, "seek": 191200, "start": 1912.0, "end": 1919.0, "text": " You can put in so earlier when we were talking about the phone numbers.", "tokens": [509, 393, 829, 294, 370, 3071, 562, 321, 645, 1417, 466, 264, 2593, 3547, 13], "temperature": 0.0, "avg_logprob": -0.09353670477867126, "compression_ratio": 1.4596273291925466, "no_speech_prob": 6.14409827903728e-06}, {"id": 259, "seek": 191200, "start": 1919.0, "end": 1931.0, "text": " And maybe we want to have either a space or a hyphen here and then digits four.", "tokens": [400, 1310, 321, 528, 281, 362, 2139, 257, 1901, 420, 257, 2477, 47059, 510, 293, 550, 27011, 1451, 13], "temperature": 0.0, "avg_logprob": -0.09353670477867126, "compression_ratio": 1.4596273291925466, "no_speech_prob": 6.14409827903728e-06}, {"id": 260, "seek": 191200, "start": 1931.0, "end": 1934.0, "text": " This is kind of neat how it lets you.", "tokens": [639, 307, 733, 295, 10654, 577, 309, 6653, 291, 13], "temperature": 0.0, "avg_logprob": -0.09353670477867126, "compression_ratio": 1.4596273291925466, "no_speech_prob": 6.14409827903728e-06}, {"id": 261, "seek": 191200, "start": 1934.0, "end": 1937.0, "text": " You can kind of look at the different groups.", "tokens": [509, 393, 733, 295, 574, 412, 264, 819, 3935, 13], "temperature": 0.0, "avg_logprob": -0.09353670477867126, "compression_ratio": 1.4596273291925466, "no_speech_prob": 6.14409827903728e-06}, {"id": 262, "seek": 193700, "start": 1937.0, "end": 1942.0, "text": " Also, I think if you have a.", "tokens": [2743, 11, 286, 519, 498, 291, 362, 257, 13], "temperature": 0.0, "avg_logprob": -0.12249284566834916, "compression_ratio": 1.2211538461538463, "no_speech_prob": 5.2553400564647745e-06}, {"id": 263, "seek": 193700, "start": 1942.0, "end": 1950.0, "text": " So what you can do is enter the things you're trying to match.", "tokens": [407, 437, 291, 393, 360, 307, 3242, 264, 721, 291, 434, 1382, 281, 2995, 13], "temperature": 0.0, "avg_logprob": -0.12249284566834916, "compression_ratio": 1.2211538461538463, "no_speech_prob": 5.2553400564647745e-06}, {"id": 264, "seek": 193700, "start": 1950.0, "end": 1958.0, "text": " And you'll see, see if it's worked.", "tokens": [400, 291, 603, 536, 11, 536, 498, 309, 311, 2732, 13], "temperature": 0.0, "avg_logprob": -0.12249284566834916, "compression_ratio": 1.2211538461538463, "no_speech_prob": 5.2553400564647745e-06}, {"id": 265, "seek": 195800, "start": 1958.0, "end": 1973.0, "text": " And then you can add one more.", "tokens": [400, 550, 291, 393, 909, 472, 544, 13], "temperature": 0.0, "avg_logprob": -0.5281133651733398, "compression_ratio": 0.8620689655172413, "no_speech_prob": 1.2805151527572889e-05}, {"id": 266, "seek": 195800, "start": 1973.0, "end": 1976.0, "text": " Oh, I want to have.", "tokens": [876, 11, 286, 528, 281, 362, 13], "temperature": 0.0, "avg_logprob": -0.5281133651733398, "compression_ratio": 0.8620689655172413, "no_speech_prob": 1.2805151527572889e-05}, {"id": 267, "seek": 197600, "start": 1976.0, "end": 1988.0, "text": " So what am I doing wrong on this that it's not.", "tokens": [407, 437, 669, 286, 884, 2085, 322, 341, 300, 309, 311, 406, 13], "temperature": 0.0, "avg_logprob": -0.17358264923095704, "compression_ratio": 1.4335260115606936, "no_speech_prob": 2.5465573344263248e-05}, {"id": 268, "seek": 197600, "start": 1988.0, "end": 1990.0, "text": " Ah, thank you.", "tokens": [2438, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.17358264923095704, "compression_ratio": 1.4335260115606936, "no_speech_prob": 2.5465573344263248e-05}, {"id": 269, "seek": 197600, "start": 1990.0, "end": 1994.0, "text": " Yes, that is a crucial part of phone numbers.", "tokens": [1079, 11, 300, 307, 257, 11462, 644, 295, 2593, 3547, 13], "temperature": 0.0, "avg_logprob": -0.17358264923095704, "compression_ratio": 1.4335260115606936, "no_speech_prob": 2.5465573344263248e-05}, {"id": 270, "seek": 197600, "start": 1994.0, "end": 1996.0, "text": " Three digits, three, four.", "tokens": [6244, 27011, 11, 1045, 11, 1451, 13], "temperature": 0.0, "avg_logprob": -0.17358264923095704, "compression_ratio": 1.4335260115606936, "no_speech_prob": 2.5465573344263248e-05}, {"id": 271, "seek": 197600, "start": 1996.0, "end": 1997.0, "text": " So it's kind of neat.", "tokens": [407, 309, 311, 733, 295, 10654, 13], "temperature": 0.0, "avg_logprob": -0.17358264923095704, "compression_ratio": 1.4335260115606936, "no_speech_prob": 2.5465573344263248e-05}, {"id": 272, "seek": 197600, "start": 1997.0, "end": 2000.0, "text": " It'll highlight what it's matching so you can see.", "tokens": [467, 603, 5078, 437, 309, 311, 14324, 370, 291, 393, 536, 13], "temperature": 0.0, "avg_logprob": -0.17358264923095704, "compression_ratio": 1.4335260115606936, "no_speech_prob": 2.5465573344263248e-05}, {"id": 273, "seek": 197600, "start": 2000.0, "end": 2003.0, "text": " Also, if you pick off character groups.", "tokens": [2743, 11, 498, 291, 1888, 766, 2517, 3935, 13], "temperature": 0.0, "avg_logprob": -0.17358264923095704, "compression_ratio": 1.4335260115606936, "no_speech_prob": 2.5465573344263248e-05}, {"id": 274, "seek": 200300, "start": 2003.0, "end": 2009.0, "text": " So again, that's where you use the parentheses to pick a group.", "tokens": [407, 797, 11, 300, 311, 689, 291, 764, 264, 34153, 281, 1888, 257, 1594, 13], "temperature": 0.0, "avg_logprob": -0.14922493696212769, "compression_ratio": 1.5422222222222222, "no_speech_prob": 3.591020140447654e-05}, {"id": 275, "seek": 200300, "start": 2009.0, "end": 2010.0, "text": " I think it even then shows you.", "tokens": [286, 519, 309, 754, 550, 3110, 291, 13], "temperature": 0.0, "avg_logprob": -0.14922493696212769, "compression_ratio": 1.5422222222222222, "no_speech_prob": 3.591020140447654e-05}, {"id": 276, "seek": 200300, "start": 2010.0, "end": 2017.0, "text": " Yeah, the capture group is one, two, three of what it's capturing.", "tokens": [865, 11, 264, 7983, 1594, 307, 472, 11, 732, 11, 1045, 295, 437, 309, 311, 23384, 13], "temperature": 0.0, "avg_logprob": -0.14922493696212769, "compression_ratio": 1.5422222222222222, "no_speech_prob": 3.591020140447654e-05}, {"id": 277, "seek": 200300, "start": 2017.0, "end": 2020.0, "text": " Any questions about this?", "tokens": [2639, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.14922493696212769, "compression_ratio": 1.5422222222222222, "no_speech_prob": 3.591020140447654e-05}, {"id": 278, "seek": 200300, "start": 2020.0, "end": 2023.0, "text": " I think these tools are kind of can be nice visualizations.", "tokens": [286, 519, 613, 3873, 366, 733, 295, 393, 312, 1481, 5056, 14455, 13], "temperature": 0.0, "avg_logprob": -0.14922493696212769, "compression_ratio": 1.5422222222222222, "no_speech_prob": 3.591020140447654e-05}, {"id": 279, "seek": 200300, "start": 2023.0, "end": 2027.0, "text": " If you're and I would particularly recommend if you were trying to code a redjacks, I would do it.", "tokens": [759, 291, 434, 293, 286, 576, 4098, 2748, 498, 291, 645, 1382, 281, 3089, 257, 2182, 73, 7424, 11, 286, 576, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.14922493696212769, "compression_ratio": 1.5422222222222222, "no_speech_prob": 3.591020140447654e-05}, {"id": 280, "seek": 202700, "start": 2027.0, "end": 2034.0, "text": " And one of these editors with some positive and negative test cases and then copy it into your code potentially.", "tokens": [400, 472, 295, 613, 31446, 365, 512, 3353, 293, 3671, 1500, 3331, 293, 550, 5055, 309, 666, 428, 3089, 7263, 13], "temperature": 0.0, "avg_logprob": -0.047767959142986094, "compression_ratio": 1.312, "no_speech_prob": 2.0144723748671822e-05}, {"id": 281, "seek": 202700, "start": 2034.0, "end": 2051.0, "text": " But I think this is good when you're experimenting.", "tokens": [583, 286, 519, 341, 307, 665, 562, 291, 434, 29070, 13], "temperature": 0.0, "avg_logprob": -0.047767959142986094, "compression_ratio": 1.312, "no_speech_prob": 2.0144723748671822e-05}, {"id": 282, "seek": 205100, "start": 2051.0, "end": 2062.0, "text": " This is a tweet that Brian Spearing had found saying when I meet someone with super advanced redjacks skills, I can't help but assume they've had a hard, a hard life.", "tokens": [639, 307, 257, 15258, 300, 10765, 3550, 1921, 632, 1352, 1566, 562, 286, 1677, 1580, 365, 1687, 7339, 2182, 73, 7424, 3942, 11, 286, 393, 380, 854, 457, 6552, 436, 600, 632, 257, 1152, 11, 257, 1152, 993, 13], "temperature": 0.0, "avg_logprob": -0.14397667675483516, "compression_ratio": 1.4857142857142858, "no_speech_prob": 2.684099854377564e-06}, {"id": 283, "seek": 205100, "start": 2062.0, "end": 2065.0, "text": " It's the coder equivalent of facial tats.", "tokens": [467, 311, 264, 17656, 260, 10344, 295, 15642, 256, 1720, 13], "temperature": 0.0, "avg_logprob": -0.14397667675483516, "compression_ratio": 1.4857142857142858, "no_speech_prob": 2.684099854377564e-06}, {"id": 284, "seek": 205100, "start": 2065.0, "end": 2067.0, "text": " I thought that was funny.", "tokens": [286, 1194, 300, 390, 4074, 13], "temperature": 0.0, "avg_logprob": -0.14397667675483516, "compression_ratio": 1.4857142857142858, "no_speech_prob": 2.684099854377564e-06}, {"id": 285, "seek": 205100, "start": 2067.0, "end": 2077.0, "text": " And then this person has hopefully drawn some redjacks tattoos on their face.", "tokens": [400, 550, 341, 954, 575, 4696, 10117, 512, 2182, 73, 7424, 28662, 322, 641, 1851, 13], "temperature": 0.0, "avg_logprob": -0.14397667675483516, "compression_ratio": 1.4857142857142858, "no_speech_prob": 2.684099854377564e-06}, {"id": 286, "seek": 207700, "start": 2077.0, "end": 2086.0, "text": " There are a number of X KCDs involving involving redjacks redjacks golf.", "tokens": [821, 366, 257, 1230, 295, 1783, 591, 16508, 82, 17030, 17030, 2182, 73, 7424, 2182, 73, 7424, 12880, 13], "temperature": 0.0, "avg_logprob": -0.19783501003099524, "compression_ratio": 1.6685082872928176, "no_speech_prob": 1.0952663615171332e-05}, {"id": 287, "seek": 207700, "start": 2086.0, "end": 2091.0, "text": " You try to match one group, but not the other meta redjacks golf.", "tokens": [509, 853, 281, 2995, 472, 1594, 11, 457, 406, 264, 661, 19616, 2182, 73, 7424, 12880, 13], "temperature": 0.0, "avg_logprob": -0.19783501003099524, "compression_ratio": 1.6685082872928176, "no_speech_prob": 1.0952663615171332e-05}, {"id": 288, "seek": 207700, "start": 2091.0, "end": 2096.0, "text": " Try to play redjacks golf with arbitrary list.", "tokens": [6526, 281, 862, 2182, 73, 7424, 12880, 365, 23211, 1329, 13], "temperature": 0.0, "avg_logprob": -0.19783501003099524, "compression_ratio": 1.6685082872928176, "no_speech_prob": 1.0952663615171332e-05}, {"id": 289, "seek": 207700, "start": 2096.0, "end": 2101.0, "text": " But I lost my code, so I'm grepping for files that look like redjacks golf solvers.", "tokens": [583, 286, 2731, 452, 3089, 11, 370, 286, 478, 6066, 3759, 337, 7098, 300, 574, 411, 2182, 73, 7424, 12880, 1404, 840, 13], "temperature": 0.0, "avg_logprob": -0.19783501003099524, "compression_ratio": 1.6685082872928176, "no_speech_prob": 1.0952663615171332e-05}, {"id": 290, "seek": 207700, "start": 2101.0, "end": 2104.0, "text": " That's meta, meta redjacks golf.", "tokens": [663, 311, 19616, 11, 19616, 2182, 73, 7424, 12880, 13], "temperature": 0.0, "avg_logprob": -0.19783501003099524, "compression_ratio": 1.6685082872928176, "no_speech_prob": 1.0952663615171332e-05}, {"id": 291, "seek": 210400, "start": 2104.0, "end": 2111.0, "text": " So lots of lots of jokes about redjacks.", "tokens": [407, 3195, 295, 3195, 295, 14439, 466, 2182, 73, 7424, 13], "temperature": 0.0, "avg_logprob": -0.10516794625814858, "compression_ratio": 1.6822916666666667, "no_speech_prob": 1.0952840966638178e-05}, {"id": 292, "seek": 210400, "start": 2111.0, "end": 2121.0, "text": " And then to review some of the just the terms and the target string is what you'll be what you're trying to match or find.", "tokens": [400, 550, 281, 3131, 512, 295, 264, 445, 264, 2115, 293, 264, 3779, 6798, 307, 437, 291, 603, 312, 437, 291, 434, 1382, 281, 2995, 420, 915, 13], "temperature": 0.0, "avg_logprob": -0.10516794625814858, "compression_ratio": 1.6822916666666667, "no_speech_prob": 1.0952840966638178e-05}, {"id": 293, "seek": 210400, "start": 2121.0, "end": 2132.0, "text": " A literal is any character used in a search or matching expression where you're kind of literally trying to find that string as opposed to the meta characters,", "tokens": [316, 20411, 307, 604, 2517, 1143, 294, 257, 3164, 420, 14324, 6114, 689, 291, 434, 733, 295, 3736, 1382, 281, 915, 300, 6798, 382, 8851, 281, 264, 19616, 4342, 11], "temperature": 0.0, "avg_logprob": -0.10516794625814858, "compression_ratio": 1.6822916666666667, "no_speech_prob": 1.0952840966638178e-05}, {"id": 294, "seek": 213200, "start": 2132.0, "end": 2138.0, "text": " which are characters that have special meaning and are not used as literals in the search expression.", "tokens": [597, 366, 4342, 300, 362, 2121, 3620, 293, 366, 406, 1143, 382, 2733, 1124, 294, 264, 3164, 6114, 13], "temperature": 0.0, "avg_logprob": -0.09439975685543484, "compression_ratio": 1.4947368421052631, "no_speech_prob": 1.9524552044458687e-05}, {"id": 295, "seek": 213200, "start": 2138.0, "end": 2143.0, "text": " For instance, period is kind of wild card that means any character.", "tokens": [1171, 5197, 11, 2896, 307, 733, 295, 4868, 2920, 300, 1355, 604, 2517, 13], "temperature": 0.0, "avg_logprob": -0.09439975685543484, "compression_ratio": 1.4947368421052631, "no_speech_prob": 1.9524552044458687e-05}, {"id": 296, "seek": 213200, "start": 2143.0, "end": 2152.0, "text": " So how do you indicate if you actually want to period?", "tokens": [407, 577, 360, 291, 13330, 498, 291, 767, 528, 281, 2896, 30], "temperature": 0.0, "avg_logprob": -0.09439975685543484, "compression_ratio": 1.4947368421052631, "no_speech_prob": 1.9524552044458687e-05}, {"id": 297, "seek": 213200, "start": 2152.0, "end": 2158.0, "text": " Yes, put a backslash before it, which is the next one here.", "tokens": [1079, 11, 829, 257, 646, 10418, 1299, 949, 309, 11, 597, 307, 264, 958, 472, 510, 13], "temperature": 0.0, "avg_logprob": -0.09439975685543484, "compression_ratio": 1.4947368421052631, "no_speech_prob": 1.9524552044458687e-05}, {"id": 298, "seek": 215800, "start": 2158.0, "end": 2169.0, "text": " Escape sequence and escape sequence is a way of indicating that we want to use a meta character as a literal.", "tokens": [42960, 8310, 293, 7615, 8310, 307, 257, 636, 295, 25604, 300, 321, 528, 281, 764, 257, 19616, 2517, 382, 257, 20411, 13], "temperature": 0.0, "avg_logprob": -0.0861776372888586, "compression_ratio": 1.7217391304347827, "no_speech_prob": 1.1842773346870672e-05}, {"id": 299, "seek": 215800, "start": 2169.0, "end": 2179.0, "text": " Yeah. And so when working with redjacks, you'll typically want to create create a pattern in plain English, mapped the redjacks language.", "tokens": [865, 13, 400, 370, 562, 1364, 365, 2182, 73, 7424, 11, 291, 603, 5850, 528, 281, 1884, 1884, 257, 5102, 294, 11121, 3669, 11, 33318, 264, 2182, 73, 7424, 2856, 13], "temperature": 0.0, "avg_logprob": -0.0861776372888586, "compression_ratio": 1.7217391304347827, "no_speech_prob": 1.1842773346870672e-05}, {"id": 300, "seek": 215800, "start": 2179.0, "end": 2187.0, "text": " You definitely want to check your false positives and your false negatives, making sure that you're not picking up things you don't want to pick up.", "tokens": [509, 2138, 528, 281, 1520, 428, 7908, 35127, 293, 428, 7908, 40019, 11, 1455, 988, 300, 291, 434, 406, 8867, 493, 721, 291, 500, 380, 528, 281, 1888, 493, 13], "temperature": 0.0, "avg_logprob": -0.0861776372888586, "compression_ratio": 1.7217391304347827, "no_speech_prob": 1.1842773346870672e-05}, {"id": 301, "seek": 218700, "start": 2187.0, "end": 2195.0, "text": " I think that can happen more with redjacks than with other types of programming.", "tokens": [286, 519, 300, 393, 1051, 544, 365, 2182, 73, 7424, 813, 365, 661, 3467, 295, 9410, 13], "temperature": 0.0, "avg_logprob": -0.09062906828793613, "compression_ratio": 1.5643564356435644, "no_speech_prob": 2.0143897927482612e-05}, {"id": 302, "seek": 218700, "start": 2195.0, "end": 2200.0, "text": " And it's like this tip of it's OK to filter before and after as well.", "tokens": [400, 309, 311, 411, 341, 4125, 295, 309, 311, 2264, 281, 6608, 949, 293, 934, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.09062906828793613, "compression_ratio": 1.5643564356435644, "no_speech_prob": 2.0143897927482612e-05}, {"id": 303, "seek": 218700, "start": 2200.0, "end": 2202.0, "text": " Like redjacks is kind of very much a tool.", "tokens": [1743, 2182, 73, 7424, 307, 733, 295, 588, 709, 257, 2290, 13], "temperature": 0.0, "avg_logprob": -0.09062906828793613, "compression_ratio": 1.5643564356435644, "no_speech_prob": 2.0143897927482612e-05}, {"id": 304, "seek": 218700, "start": 2202.0, "end": 2209.0, "text": " You don't have to try to use it end to end, kind of use it where it's useful, but use it in conjunction with other things.", "tokens": [509, 500, 380, 362, 281, 853, 281, 764, 309, 917, 281, 917, 11, 733, 295, 764, 309, 689, 309, 311, 4420, 11, 457, 764, 309, 294, 27482, 365, 661, 721, 13], "temperature": 0.0, "avg_logprob": -0.09062906828793613, "compression_ratio": 1.5643564356435644, "no_speech_prob": 2.0143897927482612e-05}, {"id": 305, "seek": 220900, "start": 2209.0, "end": 2217.0, "text": " And you'll saw I think above there was a point where we were still using Python split to split our tokens apart.", "tokens": [400, 291, 603, 1866, 286, 519, 3673, 456, 390, 257, 935, 689, 321, 645, 920, 1228, 15329, 7472, 281, 7472, 527, 22667, 4936, 13], "temperature": 0.0, "avg_logprob": -0.10103389281260816, "compression_ratio": 1.5052083333333333, "no_speech_prob": 8.267265911854338e-06}, {"id": 306, "seek": 220900, "start": 2217.0, "end": 2222.0, "text": " You don't have to try to do everything with the redjacks.", "tokens": [509, 500, 380, 362, 281, 853, 281, 360, 1203, 365, 264, 2182, 73, 7424, 13], "temperature": 0.0, "avg_logprob": -0.10103389281260816, "compression_ratio": 1.5052083333333333, "no_speech_prob": 8.267265911854338e-06}, {"id": 307, "seek": 220900, "start": 2222.0, "end": 2227.0, "text": " Any questions about redjacks?", "tokens": [2639, 1651, 466, 2182, 73, 7424, 30], "temperature": 0.0, "avg_logprob": -0.10103389281260816, "compression_ratio": 1.5052083333333333, "no_speech_prob": 8.267265911854338e-06}, {"id": 308, "seek": 220900, "start": 2227.0, "end": 2233.0, "text": " I will and I'm going to try to post the next homework soon and I'll email you when I do.", "tokens": [286, 486, 293, 286, 478, 516, 281, 853, 281, 2183, 264, 958, 14578, 2321, 293, 286, 603, 3796, 291, 562, 286, 360, 13], "temperature": 0.0, "avg_logprob": -0.10103389281260816, "compression_ratio": 1.5052083333333333, "no_speech_prob": 8.267265911854338e-06}, {"id": 309, "seek": 223300, "start": 2233.0, "end": 2243.0, "text": " And you'll always have at least a week from when I post the homework to when it's due, but I'll try to include some redjacks in that.", "tokens": [400, 291, 603, 1009, 362, 412, 1935, 257, 1243, 490, 562, 286, 2183, 264, 14578, 281, 562, 309, 311, 3462, 11, 457, 286, 603, 853, 281, 4090, 512, 2182, 73, 7424, 294, 300, 13], "temperature": 0.0, "avg_logprob": -0.1044343396237022, "compression_ratio": 1.2201834862385321, "no_speech_prob": 2.0118855900363997e-05}, {"id": 310, "seek": 224300, "start": 2243.0, "end": 2264.0, "text": " I think this is a good point maybe to stop for our break, so let's meet back at 12 o'clock and we'll start Notebook 5.", "tokens": [50364, 286, 519, 341, 307, 257, 665, 935, 1310, 281, 1590, 337, 527, 1821, 11, 370, 718, 311, 1677, 646, 412, 2272, 277, 6, 9023, 293, 321, 603, 722, 11633, 2939, 1025, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18104790278843472, "compression_ratio": 1.145631067961165, "no_speech_prob": 0.00017320032930001616}], "language": "en"}