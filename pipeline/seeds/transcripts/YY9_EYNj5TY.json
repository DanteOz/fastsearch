{"text": " All right, I'm going to go ahead and get started. I wanted to announce first, I put up the second homework on GitHub, and that'll be due next Thursday, the 15th. And that's also the day that the draft for your writing project is due, so one week from today. Yeah, and so I wanted to start by kind of following up with some questions that came up in previous classes. One of those was kind of what's going on with the randomized projections, like why does this work? And so another way to think about it is, so here we have our matrix representing the video, where each column is a single point in time. And what we do with the random projection is basically take a random row vector. And so it's like taking a linear combination kind of of all these columns where you've got random values for how much you're taking of each column. And so in the case where you take one of those, what would you expect it to look like? So this is kind of remembering what we saw. Kind of I wrote it down in the note of like, you can think of a matrix times a vector as like taking a linear combination of the columns of that matrix. So what would this matrix times a column vector look like? Do you want the, I think Tim has something to say, Jeremy? And keep in mind, this random vector you're using to take your linear combination of the columns, that's appropriately normalized. So assume that everything in there is maybe positive and sums up to one. Sam? Well, linear combination of all the columns, it would look a lot like one of those columns. Exactly. Yeah, so Sam said a linear combination of these columns. Notice these columns are actually very similar to each other. That's why we have these horizontal lines. There's a little bit of feedback, Jeremy. Do you know what's causing that? Or like the microphone or speakers humming. OK, I'll keep going. So these columns that make up this matrix all look very similar. And that's why we have horizontal lines. Because here at the bottom, it looks like there's a black line and then a dark gray line and then a black line. That's the space where every column in that matrix has the same value. And so if you were to take this linear combination of them that's appropriately normalized, you're going to get the same thing at the bottom. There's no way that you're going to get a very white pixel for your result if you're taking basically this weighted average of all these black and dark gray pixels. It's going to be pretty close to the main column? Yes, yeah. So this would be very close to taking an average. Because the average of the whites is just going to be the number of columns. Yeah. Yeah, so if we were perfectly taking an average, what would happen would be that we would kind of have smears, and actually I should be pointing here, kind of smears where the black lines are. And so we won't get that because it's important that this is a randomly weighted average. But the horizontal lines are going to have to show up. And what we get is just a single column. But it would match up with these horizontal lines. There are questions about that, kind of with this idea of, so for right now, we're just talking about taking one, kind of multiplying by one vector. I see a lot of puzzled expressions. Linda? Mm-hm. So you're talking about using one factor times the matrix, like one single value, like factor as like one value? OK, that's a good question. Let me grab my stylus in my bag. OK, so kind of going back here, what I'm thinking about is kind of this is our matrix M. This is made up of columns. And I'll call that C1, C2, C3, and so on. And so I'm thinking about multiplying it by a single vector, but that'll have a lot of values. So I'll call these like maybe R1, R2. And these are random, but they are normalized. I can't remember exactly how. But the idea is, so kind of these are normalized. And we're taking this multiplication. And then just to, actually I should show, kind of to tie it in with the picture on the other page, the horizontal lines are still here. But you can think of a horizontal line as just meaning, you know, maybe these all have the value like 120 kind of in this particular row. But it's kind of several separate columns like that. And so when we multiply across, what we get is, I should zoom out a little bit, kind of R1 times C1 plus R2 times C2, and so on. So it's like a linear combination of those. But for this one, so kind of in this spot down here, actually I should make, so this is going to add up to some kind of final sum, which is going to be a column vector. What's going to be in this bottom spot where the 120s were? Exactly, 120. And so the idea is kind of no matter how we do the random weights, since this is normalized, if this was 120 in every single one, you would have to get a 120 there. Matthew? What's the advantage of doing a random vector versus just averaging it? That's a great question. And that kind of leads into what happens. I just want to go back to the notebook. Goes back into what happens when we take multiple of these. So when we take multiple ones, if we were just taking an average, we would get the same thing each time. But now, so now we're picking out two different vectors of coefficients, taking two linear combinations to get two columns. Because they're random, they're going to be orthonormal to each other. So because we're taking two different ones, they're random, we end up with something orthonormal, roughly. And basically, the idea is just because we have so many values that, let me go back maybe to the notebook or to the note, that it's just so unlikely that we would end up getting something that was a multiple of something else, given that we have all these random values. And this is nice because it lets us get this new space that's capturing more information about m. If we just took the average, we would just get the average. But here, we're going to be able to pick out different pieces of it. So I think with the background video, you could think of if you had two different backgrounds that show up in the video, you would be able to get enough information. And you would probably have to take more than two vectors. But you could probably reconstruct both of those backgrounds. This is, in some ways, a very simple example that we just have this one background we're catching. Or maybe if there's a person that is sitting in one place for a large part of the video, you would end up catching a lot of that person. But you would also catch maybe what was behind her when she moved. So you're able to get different information because these are random. Do you want to hand it back to you? It looks like his a phone. Do you want to hand it back to you? It looks like his a follow up question. Yeah, I guess I'm just not sure what your random vector is going to be affecting the direction of time. Is that correct? So the tough thing is it's kind of capturing different points in time. It's randomly. Yeah, you're right. It's random in time in that it's kind of saying, let's take, I don't know, 0.15 from second two and 0.3 from second three. And then what the interpretation, is there any way to interpret the information added by that vector? I mean, I'm just definitely still. So let me remind you the overarching goal with this randomized projection is to get something where the columns span the same space as the columns here. So kind of if we did several of these, we would get a matrix. Because this is a very wide matrix. And we want to get something that's narrower but has all the same information. Or Sam has his hand up. So if we had a picture that had, where we had many pictures, there was a lot of background. And in maybe a third of them, there was a person standing in the middle. Then we took two random vectors and multiplied to find the matrix and got two columns. Then we could represent the person. Let's say in one of the vectors, it multiplied 0 by some change. Yes, yeah. Sometimes all of the images that have a person, but there was some positive value multiplied by the ones with the person, then you could reconstruct just the person by some positive number times the second column minus the first column. So the information of the person is captured. Because now we can map anything else in any of the pictures by some linear combination of these random vectors. Yes, yeah. Yeah. And so that, and since it's, yeah. Yeah, that's a good example. Thank you, Sam. Yeah, and because of this is randomness and it's not going to work out perfectly, you would probably want more vectors in practice. But yeah, that's the idea. Can you still go over why they're orthonormal? That's a pretty strong condition. It is. So this is kind of approximately. And it really just comes down to dimensionality. So the idea that if, I can't remember, this is 4,800 tall. So if you had two vectors of length 4,800, for them to be pointing in the same direction is just statistically unlikely. Kind of if you have this random. Could be pointing with slight deviation away from each other. It could just be a slight perturbation where they're not this. Yeah, I guess it's more that. They're not multiples? Yeah, they're definitely not multiples. It's also really unlikely that they're exactly. Yeah, right. Yeah, and I may be, yeah, I want to emphasize that it's kind of the key thing is that they're not exact multiples of each other. OK. So is the idea that we want them to be orthonormal or the idea that we don't want them to be dependent? Yeah, it's more that we don't want them to be linearly dependent. Yeah, I should change that. Yeah, sorry about that. Well, the average correlation of two random projections will, on average, be 0. That could be surprising if they were highly correlated in a high dimension space. So I still think. I'll say maybe not highly correlated. I was thinking like in America, if you take two random vectors and you dot product them, it'll be close to 0. See what I mean? What if they're all like, they're not all, like everything is slightly positive, like every interest has to be positive? But these are random Gaussians, so 0 and random Gaussians. So what's the probability of having 4800 positive numbers? It's not high. It's 2 to the 4800. So you're picking the center at 0. Right. Yes? So my question is, so say that we randomly pick these colors. So how do we decide which colors to pick? I know we kind of randomize it, but then just for our own purposes, do we just see, oh, we pick these five out of 100 randomly, see what kind of background we get, and then try another one? See if we get something different? So really what we're doing, and let me open up this one, we had this randomized range finder method, and so we're kind of doing this in the context of trying to find a matrix that's like our original matrix, but has far fewer columns. Go back up to it. And so remember, this is kind of like one step in getting the randomized SVD, although in some ways this is, I think, like the hardest or kind of most non-intuitive step. But we're taking kind of random normal, yeah, of shape A, and then we're multiplying A and Q together. So this is kind of, I guess, going to the question of like, how do we know which ones to take? Like it really is just random, but we do have, remember, that number of oversamples. So like we are taking kind of more than we'll end up needing. You know, this is towards the goal of getting a randomized SVD, which is a good way to kind of get the truncated SVD without having to calculate the full SVD, which would be really slow for a large matrix. And so here, you know, we'll have like a number of components we're going for, and then we take this number of oversamples to kind of give us extra information. So I think for, so number of components would come from whatever problem you are working on. You would need to have either some intuition or something specifically that you're looking for. I mean, you can, sometimes you can take like, take a number, and then we kind of did this here where we looked at what the singular values are and see where it suddenly dips. And so that can be, this is looking at our error from the reconstruction. You could use that to be like, okay, it seems like, I don't know, really getting 50 components, like my error is already kind of like leveled off a lot. So that could be one technique. But if this was, if you were doing this for data compression, you might have particular constraints about, I don't know, this is what acceptable error is, or, you know, this is how much space I have available that I'm trying to compress into. So kind of your problem will give you the number of components. And then the number of oversamples was recommended to be 10 in the paper. That's what the authors kind of found to be a good number. Yeah, yeah, great. So I guess the randomized range finder is an algorithm that is in place that is doing the optimization for us. Yeah, so the randomized range finder, so I wrote it out so we could see what was going on in it, but it's actually implemented in SKLearn. And so, you know, in most cases you would probably be using, really if you're using SKLearn's randomized SVD, that calls SKLearn's randomized range finder, which is here. Oh. Yeah, that's true. It's not specifically optimizing anything, but it is doing this calculation for you, although it is random, and that it just, actually I can pull this up on here. Yeah, so here, randomized range finder, and this is very similar to what I did. I just kind of left out a lot of the kind of like fancier conditions or checks, but it, you know, it is just taking a bunch of random normals. Yeah, so we do iterate, and here we are using math to calculate the best stats. No, we're not getting the best of anything here. Yeah, okay, I'll go to my code because that's shorter. So this iteration, and this I kind of just wanted to share like an intuition about. The idea is that, so this is kind of going back to our goal is to find a matrix that has the same column space as A, our original matrix. So we're trying to find something with fewer columns, but a similar column space. And here, kind of taking powers of A kind of gets stuff that's like really in the column space of A. You know, if you did A times a vector, and then, you know, multiply that by A again and again, you're like kind of getting stuff like, okay, this is definitely in the column space of A. The issue with that is that unless A kind of has exactly the right size, your answer is either going to be getting like bigger and bigger, or it's going to be getting smaller and smaller, and you're going to run into numerical issues. So you can't just take a bunch of powers of A. And so what's going on here is it's basically taking a power of A, you know, A times Q, and then it's doing the LU decomposition to kind of normalize it. And a lot of, it's much more complex when you have this loop there, and a lot of the limitations don't have that loop at all. So like this really, the simple version is only has the two lines of code, create an old random matrix, and then take the QRT composition of that random matrix times the matrix. So you really want to know what this is doing, because the first line of code and the last line of code is actually all you need. Yeah, thank you, Jeremy. This is, yeah, you could have the number of iterations set to zero, and you could just be getting a random matrix and doing A times your random matrix, and then the QRT composition. And in this context, the QRT composition, actually this gets back to Tim's case. This is going to kind of like normalize what you're getting and make sure it's worth a normal for Q. Well, any other questions? And we may even revisit this again, because I know this is weird. Random matrix theory, like the idea that you can, a lot of times I say random matrix and that's it. Yeah. Weird, but yeah, that's all it does. I said random matrix theory is dropping the counter and sure it is. But we just multiply by a matrix and it happens to work. Great. Yeah, and so on that note, I also wanted to just kind of remind you of the Johnson-Linden-Strauss lemma that we talked about, which is that a small set of points in a high dimensional space can be embedded into a space of much lower dimension in a way that preserves kind of the important structure of the space. And so that's what we're doing here with taking this really wide matrix and trying to put it into a narrow matrix that's going to have kind of the important same structures. Then for something a little bit wider, this was really fun. Matthew had asked last time about kind of like the history of Gaussian elimination, and it's a lot more fascinating than I realized. So the first written record of Gaussian elimination is from 200 BC. I'm going to pull this up. Let me hide this. Oh, I see. Thank you. Anyways, I found this history of Gaussian elimination. And so 200 BC in the Chinese Book of Arithmetic, there's a problem. Three sheafs of a good crop, two sheafs of a mediocre crop, and one sheaf of a bad crop are sold for 39 Dao. Two sheafs of good, three mediocre, and one bad are sold for 34 Dao, and so on. And so this is kind of like a classic Gaussian elimination problem, so I thought that was really neat. And then they even apparently had a system where they used different colored bamboo rods on a counting board to do Gaussian elimination. And so I linked to this page because I think it's really interesting. So even in those days they solved it with a computer. Yeah, a much earlier computer. Anyway, so then it goes on that Gaussian elimination, kind of the next record of it was in Japan in the 1600s. Saki Kawa actually invented the determinant, although did not get credit for it. And around the same time Leibniz independently invented the determinant and also did not get credit for it because kind of the ideas didn't take off. And then, okay, I thought this was neat. So then Gauss described the elimination method as being commonly known. So he was not even the first European to discover it and never claimed to have invented it. But he did possibly invent the Cholesky decomposition, which Cholesky did not come up with until later on, but gets credit for. So it's just a kind of fun bit of math history. But this, yeah, this writing kind of emphasizes like how important Gaussian elimination is and just that it's been like a problem that people care about for over 2,000 years and is still widely used. I thought that was fun. Any questions or comments about the history of Gaussian elimination? Okay. So then another question that came up last time was about ways to speed up Gaussian elimination. And I did not have time to dive into this as much as I want. This is on my to-do list after this course ends. But LUD composition can be fully parallelized. And it looks like there are actually kind of multiple ways to do this. So I found this slide show. So kind of here they've shown what the different dependencies are and then talk about ways to parallelize that, including one approach is kind of to break it down into conglomeration, you know, of kind of what depends on what and like grouping those together and other tasks. And then also this is a kind of 2016 paper that you can do a randomized LUD composition, which lets you run it on a GPU without having to do GPU to CPU data transfer. So that would be much quicker. So I just want to kind of let you know we're not going to go into these, but let you know that they're out there, because I think that's exciting. And what's also interesting is this is still an area of research that people are working on. So another question that came up last time was, Valentin discovered that using SciPy.linouge.solve, so we had this kind of pathological matrix that we looked at. So let me do it for a lower dimension just so you can kind of remember what it looks like. So this matrix that was ones along the diagonal, ones in the last column, negative ones in the lower triangle. And you may remember that when we did Gaussian elimination by hand on that, basically what ends up happening is that you're kind of doubling at every row. So you end up with this final column of ones, and so you end up with these powers of two that's getting very large. And so we get the wrong answer. We get a wrong answer when we do use lusolve in SciPy for that. However, using.solve was getting the right answer. So we're getting into that more. And this is not a significant difference, but lusolve was quicker. This is SciPy's implementation of lusolve. And so I can imagine if you're doing a ton that there may be advantages there, particularly because if you were doing this a lot of times, you only have to do the lu factorization once of A. If you have like a lot of different columns B you're solving for, you can kind of save that decomposition. But looking at SciPy's lin-out solve, I looked at the Fortran source code for what it's calling in LawPack. And in the comments, actually I'll pull this up. Let's get to the right place. I mentioned several times that it's looking at the reciprocal pivot growth factor. And so if you'll remember, we defined this term row back in the notebook. Row was the growth factor. And that's kind of the greatest ratio of the AI over UI. And in this case, it's 2 to the 59th. This is where n equals 60. So it's 2 to the n minus 1. So the LawPack's method is actually taking that into account, whereas straight lusolve is not. And I think it can be, like you probably don't want to go too far down this rabbit hole, but I think it can be interesting to kind of look up the LawPack methods that are being called by a particular kind of SciPy method just to get an idea of what it's doing. Any questions about that? Kelsey? I'm just about the row pivoting. I still don't think I've heard any intuition for what makes the decorations or what property of the row or makes it explode. What property of, so when you say row pivoting, you mean when we're shuffling the rows, but not the columns? Yeah, not the path of the pathological case of the columns. That one I can see because it's sort of simple in a way. Yeah. So we saw when we weren't shuffling the rows, let me go back to this, we had that example with 10 to the negative 20th. Find it. Where we looked at this matrix, 10 to the negative 20th, and here we had problems because we hadn't hadn't shuffled the row order. And what's happening here is that you're I guess having to multiply by 10 to the 20th in order to, no, you have to multiply by 10 to the negative 20th to zero out this one on the next row. And kind of multiplying by this really tiny number is setting you up for these kind of like potential underflow errors, which is kind of what happens here. That like in general, yeah, you don't want to have to be multiplying by tiny numbers. And so what switching did is so we had did I pass it? Yeah. OK, so then we had this other matrix where we had switched them. And here when we calculate the LUD composition, actually I can change it. I guess like the fact that we multiply by 10 to the negative 20th and cancel those out. If you're multiplying by something large. So. So the good question, so like part of the issue, I don't think this is the heart of it, is that like over here you end up with one minus 10 to the negative 20th, which rounds to one. And that's that's an OK way to round. Let me let me do this one out by hand. I think might be helpful. Kind of like why the negative. Why the one case breaks down. So in this a we had 10 to the negative 20th is the first one. 20th, one, one, one. Oh, you know what? The other issue is going to be that we're like leaving this as a pivot. So later on, we're going to have to come back and multiply through by 10 to the 20th, which doesn't seem like. Seem like a good idea. And actually, even down, so that would mean L would be. It's negative. 10 to the negative 20th. One, one, zero. Well, aren't we? We're multiplying this one by. Oh, I see. Right. We're multiplying. Yeah. Thank you. Thanks, Tim. Oh, because the subtraction is implicit. Yes. Thank you. That's 10 to the 20th. So this was a in the case with a hat. We're getting or starting with one, one. 10 to the negative 20th one. Is this entry correct? The one minus 10 to the negative 20th. That should be. Right. So we're multiplying by 10 to the 20th. Yeah. Plus one. One minus. Thank you. And then this one will be one minus 10 to the negative. I guess. OK. So in this. Oh, it's OK. In the first day, the unstable one, we're ending up with pivots that are like we have one pivot that's like tiny, like dangerously tiny. And then our other pivot is dangerously huge, which is kind of like the worst of both worlds. Whereas when we've switched them, actually, both these pivots are good because they're kind of like one to one kind of much more reasonable numbers. So having having this 10 to the negative 20th as a pivot, I think, is the issue, because that's what then kind of led to us getting, yeah, huge and tiny pivot columns. The other thing that's going to happen is like when we're solving this system, you know, the UX part, like we're going to have to divide by 10 to the 20th and then on the next row, we've got these coefficients, I guess, that we're now like dividing by that are huge and tiny, which is bad. So it's only a problem because we do it like the composition going down. So it's only like the ratio. Yes. Yes. Yeah. The ratio is kind of what's creating these, but it's also like the ratio is kind of artificial because you could put the rows in any order and it would be smaller. So what it is is you would actually when you do this, if you had like a matrix that was more than two by two, is that on each iteration, you want to choose like the largest value that you have. Yeah. And then swap those two rows. But kind of once you have more rows, you can't do it all in advance. Like you kind of have to go, you know, like see how it changes your rows to do one of the outer loop where you zero something out and then for the next one to, you know, pivot, which is, you know, swapping two rows. But kind of each time you're going to like find what's the best row to swap with. You're welcome. Any other questions about this? OK, let me go back down. OK. And so then that was it for kind of following up on LU factorization. I wanted to return to a question from last week about block matrices. And this is something that I wanted to teach at some point and kind of wasn't sure where to stick it. So it's a talk about it here. First, I wanted to talk about ordinary matrix multiplication to contrast. And so I want you to take a moment to think about what is the computational complexity or big O of matrix multiplication if you have n by n matrices? I want to raise their hand if they know the answer. Yeah, Roger. Exactly. It's n cubed. No, that's good. I was going to ask you how you got that. So to calculate one entry, you need to multiply a row by the column. Yes. Yeah. Exactly. Yeah. Thank you. Yeah. So matrix multiplication and n cubed is slow. Like it's not a good computational complexity. And that's what matrix multiplication is, though. But a different or kind of an additional perspective to think about is, you know, we've talked about this memory hierarchy and in general, what can we say about slower types of memory? Or how much slower are they? I'm just looking for like a phrase, not a number. Yes, a lot. So slower. Yeah, I was thinking order of magnitude. Yeah, like typically going to a slower, the next slowest type of memory is going to be like an order of magnitude slower. And much cheaper. Yeah, they are also much cheaper, which is a benefit why we have slower memory. But if we think about matrix multiplication and kind of take into account memory, what's happening is, say we're reading row I of A into fast memory in an outer loop, and we'll read column J of B into fast memory, then we'll multiply that row times that column. So this inner loop is kind of like the end to get a single entry in C, our product. We're doing A times B. And here it's written. Oh, here it's written with a mistake. This was supposed to be plus equals. With the idea that you've, you know, you're kind of, you know, multiplying pairwise the elements of this row of A, this column of B, and adding those to your kind of running total to get this element C. And it takes N of those just to get one entry of C. And then kind of go write that element back to slow memory once you've got it. Are there questions first just about kind of what this algorithm is doing? So this is just standard matrix multiplication, only kind of like assuming that you can only fit one row of A and one column of B into your faster memory. OK, so I want you to think about how many reads and writes are made in doing this. So if you're multiplying two matrices together. Hsuan? So how is fast memory and slow memory separated in the actual process? So, yeah, this is kind of hard because this is hidden for you. So you're not explicitly telling the computer to do these things. And by slow memory here, I probably mean cache, not sorry, fast memory. I mean cache, slow memory, like main memory. Yeah, you're not explicitly controlling this, though. It's kind of like a lower level of the language is handling this. I mean, often when you write these, the people that write the libraries we use do do this. Yeah, that's true. Either it's like really big parallel stuff they read and write to the network from time to time. For the fast stuff on computers, they usually want to manage the cache. Particularly on GPUs, you have to manage it manually. And I guess this also comes up with like, so LOPAC is like specifically optimized for each type of computer. You know, unfortunately, it's the standard, so it's out there. But like LOPAC and BLAS are being kind of optimized to the specific architecture of your computer. In fact, as probably a lot of people in the past will end up having to deal with this directly, because they'll find themselves at jobs as data scientists where they're working on clusters. And so you basically pull stuff off the network. So when you multiply matrices, you're probably using these kind of techniques to do it on multiple computers. Yeah, that's a good point. This is like a helpful way of thinking because it applies to so many different levels of the memory hierarchy as well. So yeah, you'll see this kind of if you're doing a large amount of data on a network. Yeah, so take a moment. Write down how many reads and writes are being made to do this matrix multiplication. And sorry, I want you to make a distinction between. Actually, yeah, never mind. Just write down how many reads and writes. Okay. Okay. Okay. Okay. Okay. Raise your hand if you want more time. Does anyone have an answer they want to share? Reads is n squared plus n cubed. Yes, I would actually before you throw it back and how many how does that break down for kind of a versus B? Sure. Yes, exactly. Right, right. Yeah, thank you. That's great. So yeah, Sam said kind of this outer loop. We just read each row of a into memory once, which will end up taking n squared total. It's an each time and we go through the loop n times. However, with B, we're doing for each row. We're going to read in each column. And so that ends up being n cubed reads for B and then C. Because so for the. So kind of for each day, we're making n reads to read column J. And we're having to do that for J equals one to n. So that's n squared. And we're doing that then for each row of each row of a. The issue is like each column. Oh, because it's a column. So it has n length. Yes. Yeah. Yeah, sorry. So this is a number of element wise reads, but it's it's important to note that it's a n more reads for B than for A because you've got this redundancy of your you're pulling in each column of B. Multiple times because you have to do it for every single row of a. And then in the K loop, you're assuming that doesn't count because it's fast memory. So for the K loop. Well, they've already been pulled in. Yeah. So you're not having to. Yeah. Right. Right. Thanks, Jeremy. So questions about how I got. And then you're doing n squared writes to kind of write C back out to slow memory. Questions about that calculation. OK, so now we're going to contrast it with block matrix multiplication. And the idea behind block matrix multiplication is basically that you're just kind of subdividing A and B into several smaller matrices. So we could say you're making big n by big n blocks. Each one will be of size little n over big n times little n over big n. Here, big n is just two. So we're just getting four blocks, two by two. And it turns out like kind of the matrix multiplication works exactly the same. So the kind of top left quadrant of C is going to be A one one times B one one plus A one two times B two one, which you get from just thinking about rows of A times columns of B are giving you this result. Similarly, over here, you're taking the the top. And here, when I say row, I mean the top row of blocks of A times the second column of blocks of B to get this value. And so on. And so you kind of are just doing these smaller matrix multiplications and then putting them together to get get your result. So what this looks like in pseudocode. And here I'm just referring to them as a block IK. It's kind of talking which one of these, you know, four smaller matrices of A are you getting. So you have three nested loops and kind of an inner one. You're taking block IK of A block KJ of B and then saying block IJ of C plus equals block of A times the block of B. And you have to kind of go through each of your K's. Keep adding those to your running total and you get block IJ of C. First, are there questions kind of about just how this matrix multiplication is working? OK, so what is that? What is the big O of this? Here, say it past the microphone, Jeremy. Looks like we should have the same model of small operation. Yes, it should be magic, but we could have a little bit more operations because of moving those blocks. But it's negligible. It's so in terms of the like actually doing like additions and multiplications, it's the same. So it'll be N cubed again. And we'll talk about kind of the movement as the next question. But we've not changed our computational complexity. So, yeah, next question. Now, how many reads and writes are made? And here, like before, what the answer to kind of be element wise. And this will be in terms of both little n and big N. And I should note that here. Your loops are just you're just looping through big N because you're kind of just going through the blocks. And you can talk to the person next to you about this as you think about it. Oh, yes, Jeremy, you passed it. Before we answer this question, would you briefly explain why this is considered a very different method from the previous one? Because it seems to me that we didn't seem to be obviously that this has done something very different. Yeah, so it's not very different. It's different in that just the previous method, you're kind of thinking of A and B as a whole and dealing, you know, like row by row, column by column. Here, you've broken it into several small matrices. And so kind of inside these loops, you're now thinking a small matrix at a time. And I've even kind of cheated here because I say block of A times block of B. Really doing a block of A times a block of B would be like calling the previous method. I guess if the answer to question two turns out to be different, then that's the answer. Oh, that's true. Yeah. So we are. Spoiler alert, we are going to see that there's a different number of reads and writes being made. And then we'll talk about some other benefits of doing this approach. Yeah. I was just saying from an algorithm perspective. Okay. Okay. Who wants more time? Does anyone have an answer they want to share? Vincent. Yeah. So there's three for loops with big N each. So if you're just doing like an operation on your linear time within those three for loops, it would be a big N cubed. But since you're reading a block in each of those and the block sort of says little n over big N, then it winds up being big N squared times the one for the number of reads. Yes, exactly. Yes. Let me write that out. Okay. So let me I guess I'll use my own ID. I was trying to pull up the thing I could write on the screen. So let me delete what I have. Yeah. So as Vincent said, it's important to remember our blocks are of size little n over big N by little n over big N. So that's little n over big N squared is the amount of work. And then we've got big N cubed worth of loops since we have those three nested loops. And this reduces to big N times little n squared. And you'll have. Yeah. And it's two times as Tim indicated. So you're doing that for a and then also for B. So I'll just multiply by two. And then C is still going to be little n squared writes. So this is an improvement in the amount of reads from going between slow and fast memory that we have to do, because we're assuming that big N is substantially less than little n. It's the number of blocks that we've made. So this kind of illustrates. Yeah. One big benefit of block matrix multiplication is you've really reduced how much you have to kind of read from slower memory. And you're kind of taking better or the reason behind this is you're taking better advantage of locality of kind of when you're pulling things in a block. You're getting to kind of more fully utilize them than before with B. It was like we're pulling a column in of B only using it once and then like throwing it back and pulling in a different column of B. Other questions about this? Can anyone think of what another benefit of using block matrix multiplication might be? So this shows better locality. Kelsey? That's a good point. So that's actually possible with ordinary matrix multiplication as well. Oh, that's true. Yeah. Yeah. You could ignore blocks that are zero. Yeah, I like that you're thinking about sparse matrices. Tim? You could parallelize this? Yes. Yeah. So I was thinking you could parallelize this more easily. Because this is kind of already clearly separated. Memory? What about memory? You never actually read the whole matrix into memory. So Tim's point is basically another scalability thing. Yeah, if you had a giant matrix where even reading in a single row is excessive. Just another thing, I'm thinking this in terms of Python, because you can take advantage of vectorization I think in this respect. Of vectorization. Because we have block i, j, f, c plus block of a. Instead of in the original iteration, you're adding each entry one by one. Because you're calling it all at once. Oh, yes. Yes. Yeah. It's faster because you're calling the underlying. Yeah, that's a great point. Yeah. Thank you. I think the vector is the original one, just to say so. Like this section of a row times this section of a column. We'll have to think about it. Yeah, we'll have to think more about this. Thank you. I was going to say we're a little bit late for our break, so let's go ahead and take a break now for seven minutes. I'll be back at 12 13. I'm going to start back up. Any final questions about block matrix multiplication? And I also wanted to remind you again, I forgot to say this at the beginning of class, but if you haven't filled out the mid course feedback survey, please do that today. And the link for that is in the Slack channel. Oh, yes. Valentine and I have come up with a question. Okay. Just in terms of difference between vectorization and parallelization. So my understanding is parallelization is more like divide the course into computing tasks, while vectorization is more like computing some number in kind of matrix kind of form. Yes, vectorization often refers to SIMD, which is single instruction multiple data. But so yeah, it's kind of like you have a few. Yeah, like a little vector that you're doing kind of like the same operation to, but yeah, it's kind of next to each other. It turns out that modern processes have these process level instructions that can operate on four or eight things at a time on a single call. And that's vectorization. So normally processes can just take a number plus a number in one cycle with SIMD, modern processes in the last few years, we might take out a four or eight numbers and add them all at the same time in a single cycle, a single instruction. And that's SIMD. Right. Yeah. And then as you said, and SIMD is often described as vectorization. And then as you said, parallelization, you have kind of different cores handling, handling the processes, or even different computers. So is it correct to say that like for vectorization, we write it in a vectorized form, and then we hope then that the compiler or whatever will parallelize it on the hardware level. But for parallelization, we make it parallelized. Yeah, so NumPy handles vectorization for you. Yeah, so we don't have to usually explicitly vectorize it because NumPy is doing that for us. Whereas you have parallelization, you are having to write out more explicitly. But it would kind of vectorization, it depends what libraries you're using, like if they automatically do that. Thanks. Welcome. Good questions. All right. Now we're going to be starting a new lesson, Notebook 4. So compressed sensing of CT scans with robust regression. And we're going to start kind of before we get into the CT scans or compressed sensing with just a few foundational concepts that show up a lot of places. And the first one of these is broadcasting. And the term broadcasting actually originated with NumPy, although it's now used by a bunch of other libraries. And it describes how arrays with different shapes are treated during arithmetic operations. And so some of this, I think you've already seen and maybe haven't thought about because a lot of it feels very intuitive. So here we have an array A that's one, two, three. It's got three values. B is just a scalar two and we can do A times B. And what happens is really it did two times one, two times two and two times three. So in a sense, it's almost it's almost like A was treated as being two, two, two so that the dimensions matched. Another example, if we take a matrix where we can define a matrix, then kind of using this to be V, V times two, V times three. And what we get out is one, two, three, two, four, six, three, six, nine. So the first row is one, two, three, our original matrix three, our original matrix V. Then we've got two times each entry and then three times each entry. We have a three by three matrix. And then we can say M plus V and remember M is a three by three matrix and V is a vector one, two, three. And so typically kind of in written math, you would not want to do a matrix plus a vector. Like what does that even mean? But here NumPy kind of handles it for us and says, OK, you probably wanted to get back two, four, six, three, six, nine, four, eight, twelve. And so what was happening is that it just added one, two, three to each row. There are questions about these these two examples so far. That's a good question. And that actually leads kind of directly into the next example. So we transpose V, make it a column. So you'll notice up here the shape of V was three comma, which is like saying three by one. We've made V one, which is oh, sorry. This is like one by three. V one is three by one because we've transposed it. And now when we do M plus V one, we get two, three, four, four, six, eight, six, nine, twelve. So this highlights that it's important to kind of keep track of these dimensions. So if you were doing M plus V and you had a specific answer in mind, either adding it by rows or adding it by columns, you would want to make sure that you you did what you were hoping to do. So NumPy has a set of rules that kind of govern how this is happening. And those are that NumPy compares the shapes element wise. And it always starts with the trailing dimensions. So those are the dimensions kind of on the far right hand side. And it's looking for them to either match perfectly or one of them to be equal to one. And I think this is helpful. NumPy documentation is pretty good for this. They have a lot of examples, and so I want to walk through maybe a few of these. So here, and this is one that comes up a lot. If you were dealing with RGB values, so a picture, that channel is three. And maybe you had a 256 by 256 picture or number of pixels. And then if you wanted to scale it by three, kind of, you know, or sorry, scale, if you're scaling red, green and blue all by different values, you would have three values. So it's kind of dimension three. And these line up. So the last two dimensions are the same. You can kind of think of, since this is shorter, you can think of it as being like one by one by three. And so this works. Down here, if you had A being eight by one by six by one, B being seven by one by five, you can add those and get something that's eight by seven by six by five. So that's pretty amazing because A and B don't seem to have any dimensions in common starting out. But fortunately, their ones line up properly. So A has one in the last dimension, B has five, it goes with five. For the second to last dimension, we've got a six and a one, so it goes with six. Third to last dimension, a one and a seven, goes with seven. Final dimension, eight. And so it goes with eight. Are there questions about this? Matthew? In number B, we line up the dimension starting from left to right. Yeah, starting with the right to left. And these, I think it's really good to try, to kind of play around with trying a few different examples. There will be, in the homework, there's some of just kind of, I give you different sets of dimensions, and you have to say, you know, yes, this would work and this is what the dimensions would be, or no, I'm going to get an error if these don't line up properly. Yes, Richan? Yeah, so we just start by looking at the end. And so they've kind of written these out very nicely that you can see, kind of like, okay, like, let's look at the far right and see if these line up. Because you'll notice, like, here, A was a 4D array. It was eight by one by six by one, and B was seven by one by five. We don't start comparing, like, eight and seven. Those don't match. We're not going to do this. We start with the far right side. Yes, yeah, and so the two things that have to be true is they either need to be exactly equal, or one of them has to be one. It makes intuitive sense because if there were anything else, it wouldn't be obvious how to broadcast it. Yeah, yeah. Let me even, let me try making a matrix that's larger. I don't know, we can make a new matrix, N, that is one by one. I'll also say that I think.shape is just a really helpful, why is that making it, that's an array..shape is a really helpful attribute just to be able to see kind of what you're dealing with. And, like, I think it's a good thing to check on your data in general, like, okay, do I have what I think I have here? And so, like, here with N, and we can even kind of confirm, okay, what was the shape of it? M, M was three by three. Would I be able to add N and M? If one is two by three by three and the other is three by three? So I heard a yes, thumbs up. Yeah, and that's correct, we could. And that's because we would start with, okay, the trailing dimension is three for M and three for N. So those match. Then the second to last is three and three, those match. And then we have a two, and this is like having a one, so we're okay. Any other questions? Jeremy? I was just going to mention that I do broadcasting, and I find when I read other people's code, they don't normally use it, even in, like, popular open source libraries, and I'll rewrite it with broadcasting, and it'll be like four times less code and ten times faster. So it seems to be something that's kind of underappreciated. I actually like it so much, and in my favorite GP library called PyTorch, I wrote the broadcasting library and, like, make all this code much easier. In two weeks' time, we'll be adding a page for PyTorch broadcasting. That's awesome. Yeah, thank you. Yeah, and this is something that's nice about broadcasting is that Numpy is vectorizing the array operations kind of for you. So it's very efficient. We also found that thing the other day. So if you mentioned it, it said in the last couple of weeks they've changed it so it doesn't even have to reallocate memory. Temporaries, yeah, that they're reusing. Yeah, I think that might have been a month ago now, but yeah, that's a recent improvement to Numpy, which was already a great library. Cool, and then our next kind of general or foundational topic I want to talk about is sparse matrices and specific ways of dealing with them. So just to remind you, a matrix with lots of zeros is sparse. Here's an example. So all the light gray values are zeros. And then we've got some other values, but not a ton. And so it's more memory efficient to just save the non-zero values and not allocate a memory position for everything. And sparse matrices show up a lot in various engineering problems, and they often have kind of interesting patterns. And you'll get them from kind of like multigrid methods and also from differential equations. And so this is just an example of one kind of with a pattern. And here the black entries are all non-zero, the white entries are zero, and you can see most of the matrix is zeros. So I wanted to go into more detail about, OK, I've said we're just storing the entries, but what does that actually look like? So there are three really common sparse storage formats, and those are coordinate wise, which SciPy calls COO, compressed sparse row, which is CSR, and compressed space column, CSC. And I found a page that has, I think, some nice examples. And actually, yeah, I found two pages that we're going to look at. All right, so this is the coordinate wise storage method we'll talk about first. And I think in some ways, to me, it kind of seems like the most natural one. Yeah, so here we've got a matrix. And are you guys able to see this? OK. Kind of the matrix on yellow. And everywhere there's no number, that's a zero. So they've only written the non-zero entries in this matrix. You can see this is a 8 by 8 matrix. And then they've color coded some of the entries. And that's just nice because then you can kind of see. So down here on green is the representation. Yellow is kind of how we would think about the matrix. Green is what the computer's storing. And so in spot 0, 0, there's an 11. And we see down here they're storing value 11 is in row 0, column 0. So on there's a 22 in spot 1, 1. And so it stores value 22, row 1, column 1. So just storing the coordinates for each one. There's a 75 in row 6, column 4. And so it stores 75, 6, 4. So all kind of the computer needs is these three vectors, one of the values, one of the rows, and one of the columns to have all the information from this matrix. And so in this example, there were 20 entries in the matrix. And so you are having to note that for each entry, you are having to store the row and columns. So this is taking, I guess, like 60 spots in memory to have this information, which you can imagine many cases where that's less than, you know, here this is a 7 by 7 matrix. That would have been 49 spots in memory. Yeah, are there questions about coordinate wise storage? So this is kind of slightly more per entry, but overall if you have a lot of zeros, you're saving memory. And you're also, this is going to change how you access points. I mean, here you would, I don't know, if you wanted to see what the value at a particular coordinate was, you would have to check like is the row column paired even in this matrix? If not, it must be 0. OK, so some properties of coordinate wise method, the rows or columns don't need to be ordered in any way. You can put them in kind of in any order. And then you have to store NNZ stands for number of non-zeros, but yeah, the number of non-zero entries plus 2 times NNZ integers for the indices. I should have noted that above, like if these if these were decimals, I don't know, we're not really getting into memory types, but row and column are integers so they can take up less memory. OK, so then next, oh, actually, and first it talks about matrix vector multiplication. And so with that, what you can do is just basically loop through all your entries. So here it's just looping through values. And then for those values, you know, looking up the column value and the row value, but you don't actually have to go through every spot in the matrix. You're just going through the number of non-zeros because you kind of loop through that value vector. Questions? OK, so then it go on to the compressed sparse row data structure. So this is actually kind of even more consolidated. And the idea is we can store things in row, sorry, in order by row. And then we're only having to keep track of like when we go to a new row. So we have a row pointer that does that. So here you'll see in row zero, we've got 11, 12, and 14. So we store the values 11, 12, and 14. And then we're saying those are in row zero. Or here, let me come back to the row pointer a bit in a bit. So we're saying, OK, those were in columns 0, 1, and 3. And then we don't enter anything into this row pointer vector until we go to the next row. And so basically we're saying, OK, entries 0, 1, and 2 are in row zero. But then entry 3 is in row 1. So for this row pointer, you actually if you look back to the index, that's kind of telling you, OK, here row 3 is the first, sorry, the third value, which is 22, is the first thing in row 1. Then a new row starts with the sixth value. The sixth value we can look up here is 31. And so row 2 must start with a 31. And it does. So this takes a little bit of getting used to how it works. Let me do another one. The third row starts with a 9, no, the ninth entry. Yeah. So we go over here. That's 42 is the ninth value. And that is the first the first one in row 3 since we started indexing at zero. Then we can go to the 12th value is 55. 55 is the first thing in the fourth row. So everyone see kind of what this is doing. So what what do you think a benefit of this is? This is on the surface harder to I think figure out at first. You see Jeremy can pass the microphone. I think this present a way we can actually put the position of the nonzero entries in some kind of table matrix. And for the computer, it's actually not very easy. It's not very hard to actually look up these values. So yes, that's why it's beneficial. Yeah. So kind of the the coordinate method also give us a way to put them in a table. What's what's the benefit of this over kind of that previous coordinate method? I think the previous coordinate, unless we can't really put every single value right. This is a lot more compact into one fixed length fixed width table. But previously, if we expand it out, it's no longer valuable. So we have to compact it into this. OK, so this is this is more compact for the for the row pointer. You're just keeping track of kind of when you switch rows. So you've got less redundant information. Can anyone think of any other potential benefits, Matthew? I'm going to grab the microphone. Seems like it provides a linear format. The whole thing just transpose into a vector. I don't know if that would be really useful for operation, but we're fairly efficient. So I think this may be similar to what you're saying that this makes it really easy to look at particular rows, like if you needed to access your data by row. And so like if you were interested in getting row with index three, you know, you can just look up, OK, that starts at nine and then it's stopping right before twelve. So then you can come over here and say, OK, let me get the values that start at nine and stop right before twelve. And I've picked out this row in a fast way. So I'd say this is. Row access is easy, I guess is how they say it. But notice it's difficult to look up columns this way. In fact, it's going to be like I mean, so you can look up, you know, like go through here and say like, OK, I want to know everything. Oops, didn't mean to enlarge that. You know, you might say, OK, I want everything that's in column four, which is a lot of things. But notice going to OK, here's here's something column four, twenty five. If I want to find what row twenty five is in, that's going to be a little bit more of a pain. Like I mean, you're going to have to kind of like go through your your row pointers. You know, here there are a bunch of like even yeah, you could say like, OK, forty five is also in column four. Now I have to figure out which row it's in. And that's less obvious. So kind of knowing if you're going to be accessing your data by rows or columns would be really important here. Allenton, can you grab the microphone? Thanks. So what if we just transpose the matrix, then it will be easy to find the culprit. So when you say transpose, so there's also a compressed sparse column format, which is the same thing only by column. OK, and then just to the question. So if we just traverse through this representation to find like row I, the complexity is the whole of N where N is the number of rows. So in the worst case scenario, we'll have to make N steps until we go to the row number N. Say that again if you're trying to find what? It'll. Yeah, I mean, I do think you'll have to walk through the number of row pointers, but that's going to be when you're saying I is like in this case, eight by eight, like the number of potential rows there are. I think that's true that you. So. So you kind of want to you're suggesting almost like combining the coordinate storage where you do have all the rows. And so I'll say like these have different benefits, like so if you really were going to have to be looking things up by row and column a lot, you might want just to go with the coordinate wise method of storage. Since that does look like you look things up by row or column. You know, like kind of if you start having aspects of both, you are taking up a lot more storage space. So then there's a trade off of like, you know, if you have both like coordinate wise storage and the compressed row storage, you've now really increased kind of how much memory you're using. Sam. So it's. The issue with that pulling a column from CSR is that you are having to do more work though to find the row coordinates that correspond, you know, like you can iterate through and you're right, you have to go through everything to find all the fours since this isn't in order and COO is not in order either. But with COO when you find the fours, you're also automatically getting what the row index is. Whereas here you have to kind of do this like bookkeeping of like, okay, you know, here was a four at value 10, which row corresponds to value 10. So that is more work to figure out the rows. Yeah, these are these are good questions though. Yeah, this is good to think about. Any other questions on this? And I'm not going to go through. Actually, I think they don't have compressed column on this one because it's so similar to compressed row only now it's easy to look up or easy to get a column harder to get a row of data because you would have to like work towards to find the column. So here the amount of memory accesses is reduced. It's really. Or they even kind of say advantage of CSR over coordinate wise method. Yeah, fewer memory accesses. But yeah, it will. It will be difficult to look things up by column. And then actually this other web page, let me just do one more example. And are you able to see I just like here they have a single matrix and they show it in. This also I thought was fun. Just at the top it lists a ton of different sparse matrix compression formats. I had never heard of most of these, but I was like, wow, there are a lot of methods out there. If you had a very jagged diagonal format, non symmetric skyline format. So depending on, you know, if you had a very. Color coded this matrix and then it has it in compressed row storage, also in compressed column storage. And then, yeah, it's even got additional modified ones. But here, yeah, you can compare and they kind of represent the row start index a little bit differently. Showing that 11, 12, 13 and 14 this is like light red. Those are in row one. You're just storing a one once, but you have to store the column indexes all these times. And then next step for rows, you just store a five because you're up to value five. And like that is that is something that people do. It's like so, you know, when we saw with the kind of the coordinate wise multiplication that you do, you know, it is possible to only look at the entries you have. But yeah, there are. Yeah, like you can store your kind of block matrix is all being a zero. Yeah. Any any more questions on these sparse formats? And so then I took this from the SciPy documentation. So because I was seeing this in like code that I was finding online, the COO format can be a way to efficiently construct matrices. And so sometimes they were constructing them with COO format and then transferring them to CSC or CSR because that. And I guess this is like it would be it would be a pain to figure out how to manually enter CSC or CSR. So you probably want to use a different way. So I just want to note that multiplication, multiplication or inversion are better done with CSC or CSR format and all conversions among the different types of formats are pretty efficient linear time operations. So that's nice that you can go between them and that kind of gets it. I guess the earlier question of, yeah, like what if you want to be able to access rows at some points and columns at another. Okay, so we'll be we'll be seeing some sparse sparse matrices in a little bit. But yeah, now I want to kind of start our next application, which is CT scans. And I found this article that I thought was really nice. And it starts with me pull it up. Saving lives, the mathematics of tomography. There it goes. Whoops. And it starts with a quote. Can maths really save your life? Of course it can. And this is this is written in a pretty accessible way. It felt like it kind of talks a bit about makes this this analogy with milk bottles kind of stored in those kind of stacking crates. So they give actually let me just go up to their problem. So they're saying milk and fruit juice is delivered in bottles that are placed in trays, you know, three by three grid. And so maybe you're wondering which type of bottle is in which compartment. And you can't see because there's some in the middle, you know, that like it's been stacked up and we've got these grids. And the idea is that different amounts of light passing through milk and juice, you could get a total like from the lights. And this is a bit like a Sudoku puzzle where you just know you know some of the the totals going across a grid, and you're trying to figure out the individual entries. Are there questions and this is just kind of like as a analogy. So here they're saying okay maybe like five units of light have made it through here. Six, four, six, three, six. Let's work backwards to see which is in each of these. I don't know if you're amplifying. Yeah, the lights are on. It was it's off now. Try again. Just went off again. Batteries might be dead. Yeah. Is the red light on? Go ahead and just ask your question perhaps. I was going to say CT scans are used for example for lung cancer diagnostics which I spent a couple of years doing and better CT scans that you can diagnose earlier. Wow. Really, yeah. Yeah, so just to emphasize Jeremy was just saying that earlier diagnosis can increase survival rates tenfold. So doing CT scans well is really valuable. Saving lives. Let me go back to the notebook. So kind of what's going on with a CT scan is you've got this source of x-rays going through, you know, semi-dense object kind of being the person and then we've got a detector picking up kind of the strength of the signal to get information about the density that it's passed through. And I should say all this is coming from a scikit-learn example but I've kind of added more explanation and visualizations of it. So in our last lesson we used robust PCA and we saw that that was an optimization problem where we're trying to minimize the nuclear norm of L in order to get a low rank matrix and the L1 norm of S. And what's special about the L1 norm? What does it give us? Sparsity. Yeah, exactly. And so that'll be useful today. And let me show you. Let me show you what the. Okay, so these are what our pictures are going to look like. And actually I'm going to show you where we're going and then I'll come back and kind of show you more about how to get there. Basically where we're going is that we'll have this data where, you know, this is kind of an artificial data set that we're going to create. And the idea is that we're going to pass lines through it at different angles from all different directions and we're just going to get a single number from this line going through this complicated picture basically about what it's hitting. So here you can see it. This kind of intersects these circle globs at several points. And we'll get a slightly larger number, 6.4. And I'll talk about how this is boiled down in a moment. But thinking about a CT scan, you know, it's kind of sending an X-ray along this line and then just measuring something on the other side. And then here we've got a line going through, you know, different direction, different angle. This is not intersecting with much. Like it's just kind of catching the tail of this blob. And we get a much smaller number, 6, or sorry, 2. So before we had 6.4 or something here, we've just got 2. And so it's kind of amazing that you're just, you know, for each like X-ray shoot, you're just getting a single number. But kind of knowing that these are coming from, you know, different angles in different directions. The idea is that we want to work backwards to being able to reconstruct our original picture. Oh, no, that's a good. So Jeremy was just pointing out that this is an analogy in real life. They're doing 2D projections onto 3D. Sorry. Well, person is 3D. And you're coming up with these 2D pictures here to keep it simple. We kind of just have this 2D data set and we're just getting one dimensional data to reconstruct the two dimensional data set. Yeah. And then it's a it's 1255. So we'll stop now. But I just wanted to at least kind of introduce the problem that we'll be looking at next time. Thanks, Matthew. Oh, that was a six. Let me go back up. So there is it's kind of just a measure of how much it's intersected with these other lines. What? I mean, so I think in a person, it's more about kind of like the density or like the kind of what the material it's traveling traveling through is. In real life, it's not as direct as this. Transforms. Real life cities can pass. Okay. Basically, yeah, I mean, in this case, it's just a product. Yeah. Welcome.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.16, "text": " All right, I'm going to go ahead and get started.", "tokens": [1057, 558, 11, 286, 478, 516, 281, 352, 2286, 293, 483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.16457768848964147, "compression_ratio": 1.578125, "no_speech_prob": 0.008843235671520233}, {"id": 1, "seek": 0, "start": 3.16, "end": 6.08, "text": " I wanted to announce first, I put up the second homework", "tokens": [286, 1415, 281, 7478, 700, 11, 286, 829, 493, 264, 1150, 14578], "temperature": 0.0, "avg_logprob": -0.16457768848964147, "compression_ratio": 1.578125, "no_speech_prob": 0.008843235671520233}, {"id": 2, "seek": 0, "start": 6.08, "end": 10.48, "text": " on GitHub, and that'll be due next Thursday, the 15th.", "tokens": [322, 23331, 11, 293, 300, 603, 312, 3462, 958, 10383, 11, 264, 2119, 392, 13], "temperature": 0.0, "avg_logprob": -0.16457768848964147, "compression_ratio": 1.578125, "no_speech_prob": 0.008843235671520233}, {"id": 3, "seek": 0, "start": 10.48, "end": 14.280000000000001, "text": " And that's also the day that the draft for your writing project", "tokens": [400, 300, 311, 611, 264, 786, 300, 264, 11206, 337, 428, 3579, 1716], "temperature": 0.0, "avg_logprob": -0.16457768848964147, "compression_ratio": 1.578125, "no_speech_prob": 0.008843235671520233}, {"id": 4, "seek": 0, "start": 14.280000000000001, "end": 16.04, "text": " is due, so one week from today.", "tokens": [307, 3462, 11, 370, 472, 1243, 490, 965, 13], "temperature": 0.0, "avg_logprob": -0.16457768848964147, "compression_ratio": 1.578125, "no_speech_prob": 0.008843235671520233}, {"id": 5, "seek": 0, "start": 18.88, "end": 22.04, "text": " Yeah, and so I wanted to start by kind of following up", "tokens": [865, 11, 293, 370, 286, 1415, 281, 722, 538, 733, 295, 3480, 493], "temperature": 0.0, "avg_logprob": -0.16457768848964147, "compression_ratio": 1.578125, "no_speech_prob": 0.008843235671520233}, {"id": 6, "seek": 0, "start": 22.04, "end": 26.04, "text": " with some questions that came up in previous classes.", "tokens": [365, 512, 1651, 300, 1361, 493, 294, 3894, 5359, 13], "temperature": 0.0, "avg_logprob": -0.16457768848964147, "compression_ratio": 1.578125, "no_speech_prob": 0.008843235671520233}, {"id": 7, "seek": 0, "start": 26.04, "end": 28.8, "text": " One of those was kind of what's going", "tokens": [1485, 295, 729, 390, 733, 295, 437, 311, 516], "temperature": 0.0, "avg_logprob": -0.16457768848964147, "compression_ratio": 1.578125, "no_speech_prob": 0.008843235671520233}, {"id": 8, "seek": 2880, "start": 28.8, "end": 30.76, "text": " on with the randomized projections,", "tokens": [322, 365, 264, 38513, 32371, 11], "temperature": 0.0, "avg_logprob": -0.16756052129408894, "compression_ratio": 1.615, "no_speech_prob": 0.00016340163710992783}, {"id": 9, "seek": 2880, "start": 30.76, "end": 33.24, "text": " like why does this work?", "tokens": [411, 983, 775, 341, 589, 30], "temperature": 0.0, "avg_logprob": -0.16756052129408894, "compression_ratio": 1.615, "no_speech_prob": 0.00016340163710992783}, {"id": 10, "seek": 2880, "start": 33.24, "end": 36.72, "text": " And so another way to think about it is,", "tokens": [400, 370, 1071, 636, 281, 519, 466, 309, 307, 11], "temperature": 0.0, "avg_logprob": -0.16756052129408894, "compression_ratio": 1.615, "no_speech_prob": 0.00016340163710992783}, {"id": 11, "seek": 2880, "start": 36.72, "end": 42.96, "text": " so here we have our matrix representing the video,", "tokens": [370, 510, 321, 362, 527, 8141, 13460, 264, 960, 11], "temperature": 0.0, "avg_logprob": -0.16756052129408894, "compression_ratio": 1.615, "no_speech_prob": 0.00016340163710992783}, {"id": 12, "seek": 2880, "start": 42.96, "end": 45.760000000000005, "text": " where each column is a single point in time.", "tokens": [689, 1184, 7738, 307, 257, 2167, 935, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.16756052129408894, "compression_ratio": 1.615, "no_speech_prob": 0.00016340163710992783}, {"id": 13, "seek": 2880, "start": 45.760000000000005, "end": 48.24, "text": " And what we do with the random projection", "tokens": [400, 437, 321, 360, 365, 264, 4974, 22743], "temperature": 0.0, "avg_logprob": -0.16756052129408894, "compression_ratio": 1.615, "no_speech_prob": 0.00016340163710992783}, {"id": 14, "seek": 2880, "start": 48.24, "end": 52.6, "text": " is basically take a random row vector.", "tokens": [307, 1936, 747, 257, 4974, 5386, 8062, 13], "temperature": 0.0, "avg_logprob": -0.16756052129408894, "compression_ratio": 1.615, "no_speech_prob": 0.00016340163710992783}, {"id": 15, "seek": 2880, "start": 52.6, "end": 56.040000000000006, "text": " And so it's like taking a linear combination", "tokens": [400, 370, 309, 311, 411, 1940, 257, 8213, 6562], "temperature": 0.0, "avg_logprob": -0.16756052129408894, "compression_ratio": 1.615, "no_speech_prob": 0.00016340163710992783}, {"id": 16, "seek": 5604, "start": 56.04, "end": 60.16, "text": " kind of of all these columns where you've got random values", "tokens": [733, 295, 295, 439, 613, 13766, 689, 291, 600, 658, 4974, 4190], "temperature": 0.0, "avg_logprob": -0.17800363504661704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.665736797614954e-06}, {"id": 17, "seek": 5604, "start": 60.16, "end": 63.56, "text": " for how much you're taking of each column.", "tokens": [337, 577, 709, 291, 434, 1940, 295, 1184, 7738, 13], "temperature": 0.0, "avg_logprob": -0.17800363504661704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.665736797614954e-06}, {"id": 18, "seek": 5604, "start": 63.56, "end": 65.92, "text": " And so in the case where you take one of those,", "tokens": [400, 370, 294, 264, 1389, 689, 291, 747, 472, 295, 729, 11], "temperature": 0.0, "avg_logprob": -0.17800363504661704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.665736797614954e-06}, {"id": 19, "seek": 5604, "start": 65.92, "end": 67.84, "text": " what would you expect it to look like?", "tokens": [437, 576, 291, 2066, 309, 281, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.17800363504661704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.665736797614954e-06}, {"id": 20, "seek": 5604, "start": 73.68, "end": 76.52, "text": " So this is kind of remembering what we saw.", "tokens": [407, 341, 307, 733, 295, 20719, 437, 321, 1866, 13], "temperature": 0.0, "avg_logprob": -0.17800363504661704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.665736797614954e-06}, {"id": 21, "seek": 5604, "start": 76.52, "end": 78.16, "text": " Kind of I wrote it down in the note of like,", "tokens": [9242, 295, 286, 4114, 309, 760, 294, 264, 3637, 295, 411, 11], "temperature": 0.0, "avg_logprob": -0.17800363504661704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.665736797614954e-06}, {"id": 22, "seek": 5604, "start": 78.16, "end": 80.56, "text": " you can think of a matrix times a vector", "tokens": [291, 393, 519, 295, 257, 8141, 1413, 257, 8062], "temperature": 0.0, "avg_logprob": -0.17800363504661704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.665736797614954e-06}, {"id": 23, "seek": 5604, "start": 80.56, "end": 83.03999999999999, "text": " as like taking a linear combination", "tokens": [382, 411, 1940, 257, 8213, 6562], "temperature": 0.0, "avg_logprob": -0.17800363504661704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.665736797614954e-06}, {"id": 24, "seek": 8304, "start": 83.04, "end": 93.4, "text": " of the columns of that matrix.", "tokens": [295, 264, 13766, 295, 300, 8141, 13], "temperature": 0.0, "avg_logprob": -0.28824071186344796, "compression_ratio": 1.2758620689655173, "no_speech_prob": 2.947968596345163e-06}, {"id": 25, "seek": 8304, "start": 93.4, "end": 99.52000000000001, "text": " So what would this matrix times a column vector look like?", "tokens": [407, 437, 576, 341, 8141, 1413, 257, 7738, 8062, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.28824071186344796, "compression_ratio": 1.2758620689655173, "no_speech_prob": 2.947968596345163e-06}, {"id": 26, "seek": 9952, "start": 99.52, "end": 111.6, "text": " Do you want the, I think Tim has something to say, Jeremy?", "tokens": [1144, 291, 528, 264, 11, 286, 519, 7172, 575, 746, 281, 584, 11, 17809, 30], "temperature": 0.0, "avg_logprob": -0.37718772888183594, "compression_ratio": 0.9354838709677419, "no_speech_prob": 1.5205218005576171e-05}, {"id": 27, "seek": 11160, "start": 111.6, "end": 130.35999999999999, "text": " And keep in mind, this random vector you're using", "tokens": [400, 1066, 294, 1575, 11, 341, 4974, 8062, 291, 434, 1228], "temperature": 0.0, "avg_logprob": -0.17518746852874756, "compression_ratio": 1.326086956521739, "no_speech_prob": 5.09343271914986e-06}, {"id": 28, "seek": 11160, "start": 130.35999999999999, "end": 133.48, "text": " to take your linear combination of the columns,", "tokens": [281, 747, 428, 8213, 6562, 295, 264, 13766, 11], "temperature": 0.0, "avg_logprob": -0.17518746852874756, "compression_ratio": 1.326086956521739, "no_speech_prob": 5.09343271914986e-06}, {"id": 29, "seek": 11160, "start": 133.48, "end": 136.04, "text": " that's appropriately normalized.", "tokens": [300, 311, 23505, 48704, 13], "temperature": 0.0, "avg_logprob": -0.17518746852874756, "compression_ratio": 1.326086956521739, "no_speech_prob": 5.09343271914986e-06}, {"id": 30, "seek": 11160, "start": 136.04, "end": 140.07999999999998, "text": " So assume that everything in there is maybe positive", "tokens": [407, 6552, 300, 1203, 294, 456, 307, 1310, 3353], "temperature": 0.0, "avg_logprob": -0.17518746852874756, "compression_ratio": 1.326086956521739, "no_speech_prob": 5.09343271914986e-06}, {"id": 31, "seek": 14008, "start": 140.08, "end": 141.64000000000001, "text": " and sums up to one.", "tokens": [293, 34499, 493, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.44885685330345515, "compression_ratio": 1.179245283018868, "no_speech_prob": 2.586625123512931e-05}, {"id": 32, "seek": 14008, "start": 153.24, "end": 153.74, "text": " Sam?", "tokens": [4832, 30], "temperature": 0.0, "avg_logprob": -0.44885685330345515, "compression_ratio": 1.179245283018868, "no_speech_prob": 2.586625123512931e-05}, {"id": 33, "seek": 14008, "start": 156.92000000000002, "end": 160.84, "text": " Well, linear combination of all the columns,", "tokens": [1042, 11, 8213, 6562, 295, 439, 264, 13766, 11], "temperature": 0.0, "avg_logprob": -0.44885685330345515, "compression_ratio": 1.179245283018868, "no_speech_prob": 2.586625123512931e-05}, {"id": 34, "seek": 14008, "start": 160.84, "end": 166.84, "text": " it would look a lot like one of those columns.", "tokens": [309, 576, 574, 257, 688, 411, 472, 295, 729, 13766, 13], "temperature": 0.0, "avg_logprob": -0.44885685330345515, "compression_ratio": 1.179245283018868, "no_speech_prob": 2.586625123512931e-05}, {"id": 35, "seek": 14008, "start": 166.84, "end": 168.56, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.44885685330345515, "compression_ratio": 1.179245283018868, "no_speech_prob": 2.586625123512931e-05}, {"id": 36, "seek": 16856, "start": 168.56, "end": 171.36, "text": " Yeah, so Sam said a linear combination of these columns.", "tokens": [865, 11, 370, 4832, 848, 257, 8213, 6562, 295, 613, 13766, 13], "temperature": 0.0, "avg_logprob": -0.24034194946289061, "compression_ratio": 1.564102564102564, "no_speech_prob": 3.3208660170203075e-05}, {"id": 37, "seek": 16856, "start": 171.36, "end": 172.68, "text": " Notice these columns are actually", "tokens": [13428, 613, 13766, 366, 767], "temperature": 0.0, "avg_logprob": -0.24034194946289061, "compression_ratio": 1.564102564102564, "no_speech_prob": 3.3208660170203075e-05}, {"id": 38, "seek": 16856, "start": 172.68, "end": 174.12, "text": " very similar to each other.", "tokens": [588, 2531, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.24034194946289061, "compression_ratio": 1.564102564102564, "no_speech_prob": 3.3208660170203075e-05}, {"id": 39, "seek": 16856, "start": 174.12, "end": 177.2, "text": " That's why we have these horizontal lines.", "tokens": [663, 311, 983, 321, 362, 613, 12750, 3876, 13], "temperature": 0.0, "avg_logprob": -0.24034194946289061, "compression_ratio": 1.564102564102564, "no_speech_prob": 3.3208660170203075e-05}, {"id": 40, "seek": 16856, "start": 177.2, "end": 178.96, "text": " There's a little bit of feedback, Jeremy.", "tokens": [821, 311, 257, 707, 857, 295, 5824, 11, 17809, 13], "temperature": 0.0, "avg_logprob": -0.24034194946289061, "compression_ratio": 1.564102564102564, "no_speech_prob": 3.3208660170203075e-05}, {"id": 41, "seek": 16856, "start": 178.96, "end": 180.96, "text": " Do you know what's causing that?", "tokens": [1144, 291, 458, 437, 311, 9853, 300, 30], "temperature": 0.0, "avg_logprob": -0.24034194946289061, "compression_ratio": 1.564102564102564, "no_speech_prob": 3.3208660170203075e-05}, {"id": 42, "seek": 16856, "start": 180.96, "end": 184.12, "text": " Or like the microphone or speakers humming.", "tokens": [1610, 411, 264, 10952, 420, 9518, 34965, 13], "temperature": 0.0, "avg_logprob": -0.24034194946289061, "compression_ratio": 1.564102564102564, "no_speech_prob": 3.3208660170203075e-05}, {"id": 43, "seek": 16856, "start": 192.92000000000002, "end": 193.72, "text": " OK, I'll keep going.", "tokens": [2264, 11, 286, 603, 1066, 516, 13], "temperature": 0.0, "avg_logprob": -0.24034194946289061, "compression_ratio": 1.564102564102564, "no_speech_prob": 3.3208660170203075e-05}, {"id": 44, "seek": 16856, "start": 193.72, "end": 196.32, "text": " So these columns that make up this matrix", "tokens": [407, 613, 13766, 300, 652, 493, 341, 8141], "temperature": 0.0, "avg_logprob": -0.24034194946289061, "compression_ratio": 1.564102564102564, "no_speech_prob": 3.3208660170203075e-05}, {"id": 45, "seek": 16856, "start": 196.32, "end": 197.84, "text": " all look very similar.", "tokens": [439, 574, 588, 2531, 13], "temperature": 0.0, "avg_logprob": -0.24034194946289061, "compression_ratio": 1.564102564102564, "no_speech_prob": 3.3208660170203075e-05}, {"id": 46, "seek": 19784, "start": 197.84, "end": 199.76, "text": " And that's why we have horizontal lines.", "tokens": [400, 300, 311, 983, 321, 362, 12750, 3876, 13], "temperature": 0.0, "avg_logprob": -0.09900556176395739, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.1123653166578151e-05}, {"id": 47, "seek": 19784, "start": 199.76, "end": 204.20000000000002, "text": " Because here at the bottom, it looks like there's a black line", "tokens": [1436, 510, 412, 264, 2767, 11, 309, 1542, 411, 456, 311, 257, 2211, 1622], "temperature": 0.0, "avg_logprob": -0.09900556176395739, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.1123653166578151e-05}, {"id": 48, "seek": 19784, "start": 204.20000000000002, "end": 206.84, "text": " and then a dark gray line and then a black line.", "tokens": [293, 550, 257, 2877, 10855, 1622, 293, 550, 257, 2211, 1622, 13], "temperature": 0.0, "avg_logprob": -0.09900556176395739, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.1123653166578151e-05}, {"id": 49, "seek": 19784, "start": 206.84, "end": 210.12, "text": " That's the space where every column in that matrix", "tokens": [663, 311, 264, 1901, 689, 633, 7738, 294, 300, 8141], "temperature": 0.0, "avg_logprob": -0.09900556176395739, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.1123653166578151e-05}, {"id": 50, "seek": 19784, "start": 210.12, "end": 212.0, "text": " has the same value.", "tokens": [575, 264, 912, 2158, 13], "temperature": 0.0, "avg_logprob": -0.09900556176395739, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.1123653166578151e-05}, {"id": 51, "seek": 19784, "start": 212.0, "end": 216.64000000000001, "text": " And so if you were to take this linear combination of them", "tokens": [400, 370, 498, 291, 645, 281, 747, 341, 8213, 6562, 295, 552], "temperature": 0.0, "avg_logprob": -0.09900556176395739, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.1123653166578151e-05}, {"id": 52, "seek": 19784, "start": 216.64000000000001, "end": 218.24, "text": " that's appropriately normalized, you're", "tokens": [300, 311, 23505, 48704, 11, 291, 434], "temperature": 0.0, "avg_logprob": -0.09900556176395739, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.1123653166578151e-05}, {"id": 53, "seek": 19784, "start": 218.24, "end": 222.32, "text": " going to get the same thing at the bottom.", "tokens": [516, 281, 483, 264, 912, 551, 412, 264, 2767, 13], "temperature": 0.0, "avg_logprob": -0.09900556176395739, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.1123653166578151e-05}, {"id": 54, "seek": 19784, "start": 222.32, "end": 224.68, "text": " There's no way that you're going to get", "tokens": [821, 311, 572, 636, 300, 291, 434, 516, 281, 483], "temperature": 0.0, "avg_logprob": -0.09900556176395739, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.1123653166578151e-05}, {"id": 55, "seek": 19784, "start": 224.68, "end": 227.32, "text": " a very white pixel for your result", "tokens": [257, 588, 2418, 19261, 337, 428, 1874], "temperature": 0.0, "avg_logprob": -0.09900556176395739, "compression_ratio": 1.7741935483870968, "no_speech_prob": 1.1123653166578151e-05}, {"id": 56, "seek": 22732, "start": 227.32, "end": 230.79999999999998, "text": " if you're taking basically this weighted average of all", "tokens": [498, 291, 434, 1940, 1936, 341, 32807, 4274, 295, 439], "temperature": 0.0, "avg_logprob": -0.24561520342556936, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.6429224716557655e-06}, {"id": 57, "seek": 22732, "start": 230.79999999999998, "end": 232.76, "text": " these black and dark gray pixels.", "tokens": [613, 2211, 293, 2877, 10855, 18668, 13], "temperature": 0.0, "avg_logprob": -0.24561520342556936, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.6429224716557655e-06}, {"id": 58, "seek": 22732, "start": 235.44, "end": 238.32, "text": " It's going to be pretty close to the main column?", "tokens": [467, 311, 516, 281, 312, 1238, 1998, 281, 264, 2135, 7738, 30], "temperature": 0.0, "avg_logprob": -0.24561520342556936, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.6429224716557655e-06}, {"id": 59, "seek": 22732, "start": 238.32, "end": 239.12, "text": " Yes, yeah.", "tokens": [1079, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.24561520342556936, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.6429224716557655e-06}, {"id": 60, "seek": 22732, "start": 239.12, "end": 243.51999999999998, "text": " So this would be very close to taking an average.", "tokens": [407, 341, 576, 312, 588, 1998, 281, 1940, 364, 4274, 13], "temperature": 0.0, "avg_logprob": -0.24561520342556936, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.6429224716557655e-06}, {"id": 61, "seek": 22732, "start": 243.51999999999998, "end": 244.92, "text": " Because the average of the whites", "tokens": [1436, 264, 4274, 295, 264, 21909], "temperature": 0.0, "avg_logprob": -0.24561520342556936, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.6429224716557655e-06}, {"id": 62, "seek": 22732, "start": 244.92, "end": 249.07999999999998, "text": " is just going to be the number of columns.", "tokens": [307, 445, 516, 281, 312, 264, 1230, 295, 13766, 13], "temperature": 0.0, "avg_logprob": -0.24561520342556936, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.6429224716557655e-06}, {"id": 63, "seek": 22732, "start": 249.07999999999998, "end": 250.4, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.24561520342556936, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.6429224716557655e-06}, {"id": 64, "seek": 22732, "start": 250.4, "end": 253.6, "text": " Yeah, so if we were perfectly taking an average,", "tokens": [865, 11, 370, 498, 321, 645, 6239, 1940, 364, 4274, 11], "temperature": 0.0, "avg_logprob": -0.24561520342556936, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.6429224716557655e-06}, {"id": 65, "seek": 22732, "start": 253.6, "end": 256.2, "text": " what would happen would be that we would kind of have", "tokens": [437, 576, 1051, 576, 312, 300, 321, 576, 733, 295, 362], "temperature": 0.0, "avg_logprob": -0.24561520342556936, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.6429224716557655e-06}, {"id": 66, "seek": 25620, "start": 256.2, "end": 261.12, "text": " smears, and actually I should be pointing here, kind of smears", "tokens": [41818, 685, 11, 293, 767, 286, 820, 312, 12166, 510, 11, 733, 295, 41818, 685], "temperature": 0.0, "avg_logprob": -0.19659894184001442, "compression_ratio": 1.6594827586206897, "no_speech_prob": 1.760322788868507e-06}, {"id": 67, "seek": 25620, "start": 261.12, "end": 263.68, "text": " where the black lines are.", "tokens": [689, 264, 2211, 3876, 366, 13], "temperature": 0.0, "avg_logprob": -0.19659894184001442, "compression_ratio": 1.6594827586206897, "no_speech_prob": 1.760322788868507e-06}, {"id": 68, "seek": 25620, "start": 263.68, "end": 265.68, "text": " And so we won't get that because it's", "tokens": [400, 370, 321, 1582, 380, 483, 300, 570, 309, 311], "temperature": 0.0, "avg_logprob": -0.19659894184001442, "compression_ratio": 1.6594827586206897, "no_speech_prob": 1.760322788868507e-06}, {"id": 69, "seek": 25620, "start": 265.68, "end": 269.2, "text": " important that this is a randomly weighted average.", "tokens": [1021, 300, 341, 307, 257, 16979, 32807, 4274, 13], "temperature": 0.0, "avg_logprob": -0.19659894184001442, "compression_ratio": 1.6594827586206897, "no_speech_prob": 1.760322788868507e-06}, {"id": 70, "seek": 25620, "start": 269.2, "end": 273.71999999999997, "text": " But the horizontal lines are going to have to show up.", "tokens": [583, 264, 12750, 3876, 366, 516, 281, 362, 281, 855, 493, 13], "temperature": 0.0, "avg_logprob": -0.19659894184001442, "compression_ratio": 1.6594827586206897, "no_speech_prob": 1.760322788868507e-06}, {"id": 71, "seek": 25620, "start": 273.71999999999997, "end": 275.52, "text": " And what we get is just a single column.", "tokens": [400, 437, 321, 483, 307, 445, 257, 2167, 7738, 13], "temperature": 0.0, "avg_logprob": -0.19659894184001442, "compression_ratio": 1.6594827586206897, "no_speech_prob": 1.760322788868507e-06}, {"id": 72, "seek": 25620, "start": 275.52, "end": 280.2, "text": " But it would match up with these horizontal lines.", "tokens": [583, 309, 576, 2995, 493, 365, 613, 12750, 3876, 13], "temperature": 0.0, "avg_logprob": -0.19659894184001442, "compression_ratio": 1.6594827586206897, "no_speech_prob": 1.760322788868507e-06}, {"id": 73, "seek": 25620, "start": 280.2, "end": 282.8, "text": " There are questions about that, kind of with this idea of,", "tokens": [821, 366, 1651, 466, 300, 11, 733, 295, 365, 341, 1558, 295, 11], "temperature": 0.0, "avg_logprob": -0.19659894184001442, "compression_ratio": 1.6594827586206897, "no_speech_prob": 1.760322788868507e-06}, {"id": 74, "seek": 28280, "start": 282.8, "end": 287.48, "text": " so for right now, we're just talking about taking one,", "tokens": [370, 337, 558, 586, 11, 321, 434, 445, 1417, 466, 1940, 472, 11], "temperature": 0.0, "avg_logprob": -0.3639126289181593, "compression_ratio": 1.1891891891891893, "no_speech_prob": 4.2892211240541656e-06}, {"id": 75, "seek": 28280, "start": 287.48, "end": 288.68, "text": " kind of multiplying by one vector.", "tokens": [733, 295, 30955, 538, 472, 8062, 13], "temperature": 0.0, "avg_logprob": -0.3639126289181593, "compression_ratio": 1.1891891891891893, "no_speech_prob": 4.2892211240541656e-06}, {"id": 76, "seek": 28280, "start": 299.24, "end": 301.24, "text": " I see a lot of puzzled expressions.", "tokens": [286, 536, 257, 688, 295, 18741, 1493, 15277, 13], "temperature": 0.0, "avg_logprob": -0.3639126289181593, "compression_ratio": 1.1891891891891893, "no_speech_prob": 4.2892211240541656e-06}, {"id": 77, "seek": 28280, "start": 308.08000000000004, "end": 308.58000000000004, "text": " Linda?", "tokens": [20324, 30], "temperature": 0.0, "avg_logprob": -0.3639126289181593, "compression_ratio": 1.1891891891891893, "no_speech_prob": 4.2892211240541656e-06}, {"id": 78, "seek": 30858, "start": 308.58, "end": 309.08, "text": " Mm-hm.", "tokens": [8266, 12, 8587, 13], "temperature": 0.0, "avg_logprob": -0.2720409861782141, "compression_ratio": 1.3533834586466165, "no_speech_prob": 2.9760236429865472e-05}, {"id": 79, "seek": 30858, "start": 312.38, "end": 317.34, "text": " So you're talking about using one factor times the matrix,", "tokens": [407, 291, 434, 1417, 466, 1228, 472, 5952, 1413, 264, 8141, 11], "temperature": 0.0, "avg_logprob": -0.2720409861782141, "compression_ratio": 1.3533834586466165, "no_speech_prob": 2.9760236429865472e-05}, {"id": 80, "seek": 30858, "start": 317.34, "end": 322.74, "text": " like one single value, like factor as like one value?", "tokens": [411, 472, 2167, 2158, 11, 411, 5952, 382, 411, 472, 2158, 30], "temperature": 0.0, "avg_logprob": -0.2720409861782141, "compression_ratio": 1.3533834586466165, "no_speech_prob": 2.9760236429865472e-05}, {"id": 81, "seek": 30858, "start": 322.74, "end": 324.34, "text": " OK, that's a good question.", "tokens": [2264, 11, 300, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2720409861782141, "compression_ratio": 1.3533834586466165, "no_speech_prob": 2.9760236429865472e-05}, {"id": 82, "seek": 32434, "start": 324.34, "end": 345.78, "text": " Let me grab my stylus in my bag.", "tokens": [961, 385, 4444, 452, 7952, 3063, 294, 452, 3411, 13], "temperature": 0.0, "avg_logprob": -0.18097819884618124, "compression_ratio": 0.9411764705882353, "no_speech_prob": 5.862510988663416e-06}, {"id": 83, "seek": 34578, "start": 345.78, "end": 355.73999999999995, "text": " OK, so kind of going back here, what I'm thinking about", "tokens": [2264, 11, 370, 733, 295, 516, 646, 510, 11, 437, 286, 478, 1953, 466], "temperature": 0.0, "avg_logprob": -0.12079811096191406, "compression_ratio": 1.4066666666666667, "no_speech_prob": 6.962061434023781e-06}, {"id": 84, "seek": 34578, "start": 355.73999999999995, "end": 361.41999999999996, "text": " is kind of this is our matrix M. This is made up of columns.", "tokens": [307, 733, 295, 341, 307, 527, 8141, 376, 13, 639, 307, 1027, 493, 295, 13766, 13], "temperature": 0.0, "avg_logprob": -0.12079811096191406, "compression_ratio": 1.4066666666666667, "no_speech_prob": 6.962061434023781e-06}, {"id": 85, "seek": 34578, "start": 361.41999999999996, "end": 370.26, "text": " And I'll call that C1, C2, C3, and so on.", "tokens": [400, 286, 603, 818, 300, 383, 16, 11, 383, 17, 11, 383, 18, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.12079811096191406, "compression_ratio": 1.4066666666666667, "no_speech_prob": 6.962061434023781e-06}, {"id": 86, "seek": 34578, "start": 370.26, "end": 373.34, "text": " And so I'm thinking about multiplying it by a single", "tokens": [400, 370, 286, 478, 1953, 466, 30955, 309, 538, 257, 2167], "temperature": 0.0, "avg_logprob": -0.12079811096191406, "compression_ratio": 1.4066666666666667, "no_speech_prob": 6.962061434023781e-06}, {"id": 87, "seek": 37334, "start": 373.34, "end": 375.97999999999996, "text": " vector, but that'll have a lot of values.", "tokens": [8062, 11, 457, 300, 603, 362, 257, 688, 295, 4190, 13], "temperature": 0.0, "avg_logprob": -0.1658720927195506, "compression_ratio": 1.6033755274261603, "no_speech_prob": 1.6187088476726785e-05}, {"id": 88, "seek": 37334, "start": 375.97999999999996, "end": 380.65999999999997, "text": " So I'll call these like maybe R1, R2.", "tokens": [407, 286, 603, 818, 613, 411, 1310, 497, 16, 11, 497, 17, 13], "temperature": 0.0, "avg_logprob": -0.1658720927195506, "compression_ratio": 1.6033755274261603, "no_speech_prob": 1.6187088476726785e-05}, {"id": 89, "seek": 37334, "start": 380.65999999999997, "end": 384.5, "text": " And these are random, but they are normalized.", "tokens": [400, 613, 366, 4974, 11, 457, 436, 366, 48704, 13], "temperature": 0.0, "avg_logprob": -0.1658720927195506, "compression_ratio": 1.6033755274261603, "no_speech_prob": 1.6187088476726785e-05}, {"id": 90, "seek": 37334, "start": 384.5, "end": 388.97999999999996, "text": " I can't remember exactly how.", "tokens": [286, 393, 380, 1604, 2293, 577, 13], "temperature": 0.0, "avg_logprob": -0.1658720927195506, "compression_ratio": 1.6033755274261603, "no_speech_prob": 1.6187088476726785e-05}, {"id": 91, "seek": 37334, "start": 388.97999999999996, "end": 391.21999999999997, "text": " But the idea is, so kind of these are normalized.", "tokens": [583, 264, 1558, 307, 11, 370, 733, 295, 613, 366, 48704, 13], "temperature": 0.0, "avg_logprob": -0.1658720927195506, "compression_ratio": 1.6033755274261603, "no_speech_prob": 1.6187088476726785e-05}, {"id": 92, "seek": 37334, "start": 391.21999999999997, "end": 393.09999999999997, "text": " And we're taking this multiplication.", "tokens": [400, 321, 434, 1940, 341, 27290, 13], "temperature": 0.0, "avg_logprob": -0.1658720927195506, "compression_ratio": 1.6033755274261603, "no_speech_prob": 1.6187088476726785e-05}, {"id": 93, "seek": 37334, "start": 393.09999999999997, "end": 395.17999999999995, "text": " And then just to, actually I should show,", "tokens": [400, 550, 445, 281, 11, 767, 286, 820, 855, 11], "temperature": 0.0, "avg_logprob": -0.1658720927195506, "compression_ratio": 1.6033755274261603, "no_speech_prob": 1.6187088476726785e-05}, {"id": 94, "seek": 37334, "start": 395.17999999999995, "end": 399.97999999999996, "text": " kind of to tie it in with the picture on the other page,", "tokens": [733, 295, 281, 7582, 309, 294, 365, 264, 3036, 322, 264, 661, 3028, 11], "temperature": 0.0, "avg_logprob": -0.1658720927195506, "compression_ratio": 1.6033755274261603, "no_speech_prob": 1.6187088476726785e-05}, {"id": 95, "seek": 37334, "start": 399.97999999999996, "end": 403.02, "text": " the horizontal lines are still here.", "tokens": [264, 12750, 3876, 366, 920, 510, 13], "temperature": 0.0, "avg_logprob": -0.1658720927195506, "compression_ratio": 1.6033755274261603, "no_speech_prob": 1.6187088476726785e-05}, {"id": 96, "seek": 40302, "start": 403.02, "end": 405.97999999999996, "text": " But you can think of a horizontal line as just meaning,", "tokens": [583, 291, 393, 519, 295, 257, 12750, 1622, 382, 445, 3620, 11], "temperature": 0.0, "avg_logprob": -0.1999470114707947, "compression_ratio": 1.4642857142857142, "no_speech_prob": 9.08037691260688e-06}, {"id": 97, "seek": 40302, "start": 405.97999999999996, "end": 410.29999999999995, "text": " you know, maybe these all have the value like 120", "tokens": [291, 458, 11, 1310, 613, 439, 362, 264, 2158, 411, 10411], "temperature": 0.0, "avg_logprob": -0.1999470114707947, "compression_ratio": 1.4642857142857142, "no_speech_prob": 9.08037691260688e-06}, {"id": 98, "seek": 40302, "start": 410.29999999999995, "end": 412.38, "text": " kind of in this particular row.", "tokens": [733, 295, 294, 341, 1729, 5386, 13], "temperature": 0.0, "avg_logprob": -0.1999470114707947, "compression_ratio": 1.4642857142857142, "no_speech_prob": 9.08037691260688e-06}, {"id": 99, "seek": 40302, "start": 412.38, "end": 414.78, "text": " But it's kind of several separate columns like that.", "tokens": [583, 309, 311, 733, 295, 2940, 4994, 13766, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1999470114707947, "compression_ratio": 1.4642857142857142, "no_speech_prob": 9.08037691260688e-06}, {"id": 100, "seek": 40302, "start": 417.85999999999996, "end": 425.38, "text": " And so when we multiply across, what we get is,", "tokens": [400, 370, 562, 321, 12972, 2108, 11, 437, 321, 483, 307, 11], "temperature": 0.0, "avg_logprob": -0.1999470114707947, "compression_ratio": 1.4642857142857142, "no_speech_prob": 9.08037691260688e-06}, {"id": 101, "seek": 40302, "start": 425.38, "end": 432.18, "text": " I should zoom out a little bit, kind of R1 times", "tokens": [286, 820, 8863, 484, 257, 707, 857, 11, 733, 295, 497, 16, 1413], "temperature": 0.0, "avg_logprob": -0.1999470114707947, "compression_ratio": 1.4642857142857142, "no_speech_prob": 9.08037691260688e-06}, {"id": 102, "seek": 43218, "start": 432.18, "end": 440.86, "text": " C1 plus R2 times C2, and so on.", "tokens": [383, 16, 1804, 497, 17, 1413, 383, 17, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.10602308909098307, "compression_ratio": 1.5053191489361701, "no_speech_prob": 2.429980850138236e-05}, {"id": 103, "seek": 43218, "start": 440.86, "end": 443.74, "text": " So it's like a linear combination of those.", "tokens": [407, 309, 311, 411, 257, 8213, 6562, 295, 729, 13], "temperature": 0.0, "avg_logprob": -0.10602308909098307, "compression_ratio": 1.5053191489361701, "no_speech_prob": 2.429980850138236e-05}, {"id": 104, "seek": 43218, "start": 443.74, "end": 451.62, "text": " But for this one, so kind of in this spot down here,", "tokens": [583, 337, 341, 472, 11, 370, 733, 295, 294, 341, 4008, 760, 510, 11], "temperature": 0.0, "avg_logprob": -0.10602308909098307, "compression_ratio": 1.5053191489361701, "no_speech_prob": 2.429980850138236e-05}, {"id": 105, "seek": 43218, "start": 451.62, "end": 453.26, "text": " actually I should make, so this is", "tokens": [767, 286, 820, 652, 11, 370, 341, 307], "temperature": 0.0, "avg_logprob": -0.10602308909098307, "compression_ratio": 1.5053191489361701, "no_speech_prob": 2.429980850138236e-05}, {"id": 106, "seek": 43218, "start": 453.26, "end": 456.54, "text": " going to add up to some kind of final sum, which", "tokens": [516, 281, 909, 493, 281, 512, 733, 295, 2572, 2408, 11, 597], "temperature": 0.0, "avg_logprob": -0.10602308909098307, "compression_ratio": 1.5053191489361701, "no_speech_prob": 2.429980850138236e-05}, {"id": 107, "seek": 43218, "start": 456.54, "end": 458.38, "text": " is going to be a column vector.", "tokens": [307, 516, 281, 312, 257, 7738, 8062, 13], "temperature": 0.0, "avg_logprob": -0.10602308909098307, "compression_ratio": 1.5053191489361701, "no_speech_prob": 2.429980850138236e-05}, {"id": 108, "seek": 43218, "start": 458.38, "end": 460.74, "text": " What's going to be in this bottom spot", "tokens": [708, 311, 516, 281, 312, 294, 341, 2767, 4008], "temperature": 0.0, "avg_logprob": -0.10602308909098307, "compression_ratio": 1.5053191489361701, "no_speech_prob": 2.429980850138236e-05}, {"id": 109, "seek": 46074, "start": 460.74, "end": 462.02, "text": " where the 120s were?", "tokens": [689, 264, 10411, 82, 645, 30], "temperature": 0.0, "avg_logprob": -0.2045093297958374, "compression_ratio": 1.4378378378378378, "no_speech_prob": 4.289152911951533e-06}, {"id": 110, "seek": 46074, "start": 467.90000000000003, "end": 469.54, "text": " Exactly, 120.", "tokens": [7587, 11, 10411, 13], "temperature": 0.0, "avg_logprob": -0.2045093297958374, "compression_ratio": 1.4378378378378378, "no_speech_prob": 4.289152911951533e-06}, {"id": 111, "seek": 46074, "start": 469.54, "end": 473.46000000000004, "text": " And so the idea is kind of no matter", "tokens": [400, 370, 264, 1558, 307, 733, 295, 572, 1871], "temperature": 0.0, "avg_logprob": -0.2045093297958374, "compression_ratio": 1.4378378378378378, "no_speech_prob": 4.289152911951533e-06}, {"id": 112, "seek": 46074, "start": 473.46000000000004, "end": 477.34000000000003, "text": " how we do the random weights, since this is normalized,", "tokens": [577, 321, 360, 264, 4974, 17443, 11, 1670, 341, 307, 48704, 11], "temperature": 0.0, "avg_logprob": -0.2045093297958374, "compression_ratio": 1.4378378378378378, "no_speech_prob": 4.289152911951533e-06}, {"id": 113, "seek": 46074, "start": 477.34000000000003, "end": 479.90000000000003, "text": " if this was 120 in every single one,", "tokens": [498, 341, 390, 10411, 294, 633, 2167, 472, 11], "temperature": 0.0, "avg_logprob": -0.2045093297958374, "compression_ratio": 1.4378378378378378, "no_speech_prob": 4.289152911951533e-06}, {"id": 114, "seek": 46074, "start": 479.90000000000003, "end": 482.78000000000003, "text": " you would have to get a 120 there.", "tokens": [291, 576, 362, 281, 483, 257, 10411, 456, 13], "temperature": 0.0, "avg_logprob": -0.2045093297958374, "compression_ratio": 1.4378378378378378, "no_speech_prob": 4.289152911951533e-06}, {"id": 115, "seek": 46074, "start": 482.78000000000003, "end": 483.28000000000003, "text": " Matthew?", "tokens": [12434, 30], "temperature": 0.0, "avg_logprob": -0.2045093297958374, "compression_ratio": 1.4378378378378378, "no_speech_prob": 4.289152911951533e-06}, {"id": 116, "seek": 46074, "start": 486.38, "end": 489.54, "text": " What's the advantage of doing a random vector versus just", "tokens": [708, 311, 264, 5002, 295, 884, 257, 4974, 8062, 5717, 445], "temperature": 0.0, "avg_logprob": -0.2045093297958374, "compression_ratio": 1.4378378378378378, "no_speech_prob": 4.289152911951533e-06}, {"id": 117, "seek": 48954, "start": 489.54, "end": 491.14000000000004, "text": " averaging it?", "tokens": [47308, 309, 30], "temperature": 0.0, "avg_logprob": -0.21539192970352944, "compression_ratio": 1.6574074074074074, "no_speech_prob": 6.144049166323384e-06}, {"id": 118, "seek": 48954, "start": 491.14000000000004, "end": 492.26000000000005, "text": " That's a great question.", "tokens": [663, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.21539192970352944, "compression_ratio": 1.6574074074074074, "no_speech_prob": 6.144049166323384e-06}, {"id": 119, "seek": 48954, "start": 492.26000000000005, "end": 494.98, "text": " And that kind of leads into what happens.", "tokens": [400, 300, 733, 295, 6689, 666, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.21539192970352944, "compression_ratio": 1.6574074074074074, "no_speech_prob": 6.144049166323384e-06}, {"id": 120, "seek": 48954, "start": 494.98, "end": 496.54, "text": " I just want to go back to the notebook.", "tokens": [286, 445, 528, 281, 352, 646, 281, 264, 21060, 13], "temperature": 0.0, "avg_logprob": -0.21539192970352944, "compression_ratio": 1.6574074074074074, "no_speech_prob": 6.144049166323384e-06}, {"id": 121, "seek": 48954, "start": 503.42, "end": 507.38, "text": " Goes back into what happens when we take multiple of these.", "tokens": [44471, 646, 666, 437, 2314, 562, 321, 747, 3866, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.21539192970352944, "compression_ratio": 1.6574074074074074, "no_speech_prob": 6.144049166323384e-06}, {"id": 122, "seek": 48954, "start": 507.38, "end": 509.46000000000004, "text": " So when we take multiple ones, if we were just", "tokens": [407, 562, 321, 747, 3866, 2306, 11, 498, 321, 645, 445], "temperature": 0.0, "avg_logprob": -0.21539192970352944, "compression_ratio": 1.6574074074074074, "no_speech_prob": 6.144049166323384e-06}, {"id": 123, "seek": 48954, "start": 509.46000000000004, "end": 513.38, "text": " taking an average, we would get the same thing each time.", "tokens": [1940, 364, 4274, 11, 321, 576, 483, 264, 912, 551, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.21539192970352944, "compression_ratio": 1.6574074074074074, "no_speech_prob": 6.144049166323384e-06}, {"id": 124, "seek": 48954, "start": 513.38, "end": 516.02, "text": " But now, so now we're picking out", "tokens": [583, 586, 11, 370, 586, 321, 434, 8867, 484], "temperature": 0.0, "avg_logprob": -0.21539192970352944, "compression_ratio": 1.6574074074074074, "no_speech_prob": 6.144049166323384e-06}, {"id": 125, "seek": 48954, "start": 516.02, "end": 517.94, "text": " two different vectors of coefficients,", "tokens": [732, 819, 18875, 295, 31994, 11], "temperature": 0.0, "avg_logprob": -0.21539192970352944, "compression_ratio": 1.6574074074074074, "no_speech_prob": 6.144049166323384e-06}, {"id": 126, "seek": 51794, "start": 517.94, "end": 521.4200000000001, "text": " taking two linear combinations to get two columns.", "tokens": [1940, 732, 8213, 21267, 281, 483, 732, 13766, 13], "temperature": 0.0, "avg_logprob": -0.16961780290925102, "compression_ratio": 1.6292682926829267, "no_speech_prob": 2.5612398530938663e-06}, {"id": 127, "seek": 51794, "start": 521.4200000000001, "end": 523.0200000000001, "text": " Because they're random, they're going to be", "tokens": [1436, 436, 434, 4974, 11, 436, 434, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.16961780290925102, "compression_ratio": 1.6292682926829267, "no_speech_prob": 2.5612398530938663e-06}, {"id": 128, "seek": 51794, "start": 523.0200000000001, "end": 524.46, "text": " orthonormal to each other.", "tokens": [420, 11943, 24440, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.16961780290925102, "compression_ratio": 1.6292682926829267, "no_speech_prob": 2.5612398530938663e-06}, {"id": 129, "seek": 51794, "start": 529.1400000000001, "end": 533.0200000000001, "text": " So because we're taking two different ones, they're random,", "tokens": [407, 570, 321, 434, 1940, 732, 819, 2306, 11, 436, 434, 4974, 11], "temperature": 0.0, "avg_logprob": -0.16961780290925102, "compression_ratio": 1.6292682926829267, "no_speech_prob": 2.5612398530938663e-06}, {"id": 130, "seek": 51794, "start": 533.0200000000001, "end": 537.2600000000001, "text": " we end up with something orthonormal, roughly.", "tokens": [321, 917, 493, 365, 746, 420, 11943, 24440, 11, 9810, 13], "temperature": 0.0, "avg_logprob": -0.16961780290925102, "compression_ratio": 1.6292682926829267, "no_speech_prob": 2.5612398530938663e-06}, {"id": 131, "seek": 51794, "start": 537.2600000000001, "end": 539.82, "text": " And basically, the idea is just because we", "tokens": [400, 1936, 11, 264, 1558, 307, 445, 570, 321], "temperature": 0.0, "avg_logprob": -0.16961780290925102, "compression_ratio": 1.6292682926829267, "no_speech_prob": 2.5612398530938663e-06}, {"id": 132, "seek": 51794, "start": 539.82, "end": 545.3800000000001, "text": " have so many values that, let me go back maybe to the notebook", "tokens": [362, 370, 867, 4190, 300, 11, 718, 385, 352, 646, 1310, 281, 264, 21060], "temperature": 0.0, "avg_logprob": -0.16961780290925102, "compression_ratio": 1.6292682926829267, "no_speech_prob": 2.5612398530938663e-06}, {"id": 133, "seek": 54538, "start": 545.38, "end": 550.02, "text": " or to the note, that it's just so unlikely that we would end up", "tokens": [420, 281, 264, 3637, 11, 300, 309, 311, 445, 370, 17518, 300, 321, 576, 917, 493], "temperature": 0.0, "avg_logprob": -0.12507874232072097, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.3419390597846359e-05}, {"id": 134, "seek": 54538, "start": 550.02, "end": 555.14, "text": " getting something that was a multiple of something else,", "tokens": [1242, 746, 300, 390, 257, 3866, 295, 746, 1646, 11], "temperature": 0.0, "avg_logprob": -0.12507874232072097, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.3419390597846359e-05}, {"id": 135, "seek": 54538, "start": 555.14, "end": 558.14, "text": " given that we have all these random values.", "tokens": [2212, 300, 321, 362, 439, 613, 4974, 4190, 13], "temperature": 0.0, "avg_logprob": -0.12507874232072097, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.3419390597846359e-05}, {"id": 136, "seek": 54538, "start": 558.14, "end": 563.62, "text": " And this is nice because it lets us get this new space that's", "tokens": [400, 341, 307, 1481, 570, 309, 6653, 505, 483, 341, 777, 1901, 300, 311], "temperature": 0.0, "avg_logprob": -0.12507874232072097, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.3419390597846359e-05}, {"id": 137, "seek": 54538, "start": 563.62, "end": 566.86, "text": " capturing more information about m.", "tokens": [23384, 544, 1589, 466, 275, 13], "temperature": 0.0, "avg_logprob": -0.12507874232072097, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.3419390597846359e-05}, {"id": 138, "seek": 54538, "start": 566.86, "end": 570.1, "text": " If we just took the average, we would just get the average.", "tokens": [759, 321, 445, 1890, 264, 4274, 11, 321, 576, 445, 483, 264, 4274, 13], "temperature": 0.0, "avg_logprob": -0.12507874232072097, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.3419390597846359e-05}, {"id": 139, "seek": 54538, "start": 570.1, "end": 572.3, "text": " But here, we're going to be able to pick out", "tokens": [583, 510, 11, 321, 434, 516, 281, 312, 1075, 281, 1888, 484], "temperature": 0.0, "avg_logprob": -0.12507874232072097, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.3419390597846359e-05}, {"id": 140, "seek": 54538, "start": 572.3, "end": 575.3, "text": " different pieces of it.", "tokens": [819, 3755, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.12507874232072097, "compression_ratio": 1.670940170940171, "no_speech_prob": 1.3419390597846359e-05}, {"id": 141, "seek": 57530, "start": 575.3, "end": 578.9, "text": " So I think with the background video,", "tokens": [407, 286, 519, 365, 264, 3678, 960, 11], "temperature": 0.0, "avg_logprob": -0.17611787757095027, "compression_ratio": 1.71875, "no_speech_prob": 4.425363385962555e-06}, {"id": 142, "seek": 57530, "start": 578.9, "end": 584.0999999999999, "text": " you could think of if you had two different backgrounds that", "tokens": [291, 727, 519, 295, 498, 291, 632, 732, 819, 17336, 300], "temperature": 0.0, "avg_logprob": -0.17611787757095027, "compression_ratio": 1.71875, "no_speech_prob": 4.425363385962555e-06}, {"id": 143, "seek": 57530, "start": 584.0999999999999, "end": 587.38, "text": " show up in the video, you would be able to get enough", "tokens": [855, 493, 294, 264, 960, 11, 291, 576, 312, 1075, 281, 483, 1547], "temperature": 0.0, "avg_logprob": -0.17611787757095027, "compression_ratio": 1.71875, "no_speech_prob": 4.425363385962555e-06}, {"id": 144, "seek": 57530, "start": 587.38, "end": 588.9799999999999, "text": " information.", "tokens": [1589, 13], "temperature": 0.0, "avg_logprob": -0.17611787757095027, "compression_ratio": 1.71875, "no_speech_prob": 4.425363385962555e-06}, {"id": 145, "seek": 57530, "start": 588.9799999999999, "end": 591.5799999999999, "text": " And you would probably have to take more than two vectors.", "tokens": [400, 291, 576, 1391, 362, 281, 747, 544, 813, 732, 18875, 13], "temperature": 0.0, "avg_logprob": -0.17611787757095027, "compression_ratio": 1.71875, "no_speech_prob": 4.425363385962555e-06}, {"id": 146, "seek": 57530, "start": 591.5799999999999, "end": 593.02, "text": " But you could probably reconstruct", "tokens": [583, 291, 727, 1391, 31499], "temperature": 0.0, "avg_logprob": -0.17611787757095027, "compression_ratio": 1.71875, "no_speech_prob": 4.425363385962555e-06}, {"id": 147, "seek": 57530, "start": 593.02, "end": 595.5799999999999, "text": " both of those backgrounds.", "tokens": [1293, 295, 729, 17336, 13], "temperature": 0.0, "avg_logprob": -0.17611787757095027, "compression_ratio": 1.71875, "no_speech_prob": 4.425363385962555e-06}, {"id": 148, "seek": 57530, "start": 595.5799999999999, "end": 598.4599999999999, "text": " This is, in some ways, a very simple example", "tokens": [639, 307, 11, 294, 512, 2098, 11, 257, 588, 2199, 1365], "temperature": 0.0, "avg_logprob": -0.17611787757095027, "compression_ratio": 1.71875, "no_speech_prob": 4.425363385962555e-06}, {"id": 149, "seek": 57530, "start": 598.4599999999999, "end": 600.6999999999999, "text": " that we just have this one background we're catching.", "tokens": [300, 321, 445, 362, 341, 472, 3678, 321, 434, 16124, 13], "temperature": 0.0, "avg_logprob": -0.17611787757095027, "compression_ratio": 1.71875, "no_speech_prob": 4.425363385962555e-06}, {"id": 150, "seek": 60070, "start": 600.7, "end": 604.34, "text": " Or maybe if there's a person that is sitting in one place", "tokens": [1610, 1310, 498, 456, 311, 257, 954, 300, 307, 3798, 294, 472, 1081], "temperature": 0.0, "avg_logprob": -0.467611829439799, "compression_ratio": 1.5412844036697249, "no_speech_prob": 1.5534516251136665e-06}, {"id": 151, "seek": 60070, "start": 604.34, "end": 606.6600000000001, "text": " for a large part of the video, you", "tokens": [337, 257, 2416, 644, 295, 264, 960, 11, 291], "temperature": 0.0, "avg_logprob": -0.467611829439799, "compression_ratio": 1.5412844036697249, "no_speech_prob": 1.5534516251136665e-06}, {"id": 152, "seek": 60070, "start": 606.6600000000001, "end": 608.86, "text": " would end up catching a lot of that person.", "tokens": [576, 917, 493, 16124, 257, 688, 295, 300, 954, 13], "temperature": 0.0, "avg_logprob": -0.467611829439799, "compression_ratio": 1.5412844036697249, "no_speech_prob": 1.5534516251136665e-06}, {"id": 153, "seek": 60070, "start": 608.86, "end": 612.1, "text": " But you would also catch maybe what was behind her", "tokens": [583, 291, 576, 611, 3745, 1310, 437, 390, 2261, 720], "temperature": 0.0, "avg_logprob": -0.467611829439799, "compression_ratio": 1.5412844036697249, "no_speech_prob": 1.5534516251136665e-06}, {"id": 154, "seek": 60070, "start": 612.1, "end": 612.9000000000001, "text": " when she moved.", "tokens": [562, 750, 4259, 13], "temperature": 0.0, "avg_logprob": -0.467611829439799, "compression_ratio": 1.5412844036697249, "no_speech_prob": 1.5534516251136665e-06}, {"id": 155, "seek": 60070, "start": 616.9000000000001, "end": 619.1800000000001, "text": " So you're able to get different information", "tokens": [407, 291, 434, 1075, 281, 483, 819, 1589], "temperature": 0.0, "avg_logprob": -0.467611829439799, "compression_ratio": 1.5412844036697249, "no_speech_prob": 1.5534516251136665e-06}, {"id": 156, "seek": 60070, "start": 619.1800000000001, "end": 620.5, "text": " because these are random.", "tokens": [570, 613, 366, 4974, 13], "temperature": 0.0, "avg_logprob": -0.467611829439799, "compression_ratio": 1.5412844036697249, "no_speech_prob": 1.5534516251136665e-06}, {"id": 157, "seek": 60070, "start": 626.1800000000001, "end": 627.5, "text": " Do you want to hand it back to you?", "tokens": [1144, 291, 528, 281, 1011, 309, 646, 281, 291, 30], "temperature": 0.0, "avg_logprob": -0.467611829439799, "compression_ratio": 1.5412844036697249, "no_speech_prob": 1.5534516251136665e-06}, {"id": 158, "seek": 60070, "start": 627.5, "end": 628.62, "text": " It looks like his a phone.", "tokens": [467, 1542, 411, 702, 257, 2593, 13], "temperature": 0.0, "avg_logprob": -0.467611829439799, "compression_ratio": 1.5412844036697249, "no_speech_prob": 1.5534516251136665e-06}, {"id": 159, "seek": 62862, "start": 628.62, "end": 630.94, "text": " Do you want to hand it back to you?", "tokens": [1144, 291, 528, 281, 1011, 309, 646, 281, 291, 30], "temperature": 0.0, "avg_logprob": -0.4536077764969838, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.341857023362536e-05}, {"id": 160, "seek": 62862, "start": 630.94, "end": 632.66, "text": " It looks like his a follow up question.", "tokens": [467, 1542, 411, 702, 257, 1524, 493, 1168, 13], "temperature": 0.0, "avg_logprob": -0.4536077764969838, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.341857023362536e-05}, {"id": 161, "seek": 62862, "start": 636.66, "end": 639.66, "text": " Yeah, I guess I'm just not sure what", "tokens": [865, 11, 286, 2041, 286, 478, 445, 406, 988, 437], "temperature": 0.0, "avg_logprob": -0.4536077764969838, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.341857023362536e-05}, {"id": 162, "seek": 62862, "start": 639.66, "end": 643.58, "text": " your random vector is going to be", "tokens": [428, 4974, 8062, 307, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.4536077764969838, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.341857023362536e-05}, {"id": 163, "seek": 62862, "start": 643.58, "end": 646.0600000000001, "text": " affecting the direction of time.", "tokens": [17476, 264, 3513, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.4536077764969838, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.341857023362536e-05}, {"id": 164, "seek": 62862, "start": 646.0600000000001, "end": 647.54, "text": " Is that correct?", "tokens": [1119, 300, 3006, 30], "temperature": 0.0, "avg_logprob": -0.4536077764969838, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.341857023362536e-05}, {"id": 165, "seek": 62862, "start": 654.38, "end": 656.38, "text": " So the tough thing is it's kind of capturing", "tokens": [407, 264, 4930, 551, 307, 309, 311, 733, 295, 23384], "temperature": 0.0, "avg_logprob": -0.4536077764969838, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.341857023362536e-05}, {"id": 166, "seek": 62862, "start": 656.38, "end": 657.78, "text": " different points in time.", "tokens": [819, 2793, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.4536077764969838, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.341857023362536e-05}, {"id": 167, "seek": 65778, "start": 657.78, "end": 659.18, "text": " It's randomly.", "tokens": [467, 311, 16979, 13], "temperature": 0.0, "avg_logprob": -0.30764031410217285, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.2123904273030348e-05}, {"id": 168, "seek": 65778, "start": 659.18, "end": 660.22, "text": " Yeah, you're right.", "tokens": [865, 11, 291, 434, 558, 13], "temperature": 0.0, "avg_logprob": -0.30764031410217285, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.2123904273030348e-05}, {"id": 169, "seek": 65778, "start": 660.22, "end": 662.42, "text": " It's random in time in that it's kind of saying,", "tokens": [467, 311, 4974, 294, 565, 294, 300, 309, 311, 733, 295, 1566, 11], "temperature": 0.0, "avg_logprob": -0.30764031410217285, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.2123904273030348e-05}, {"id": 170, "seek": 65778, "start": 662.42, "end": 669.3, "text": " let's take, I don't know, 0.15 from second two and 0.3", "tokens": [718, 311, 747, 11, 286, 500, 380, 458, 11, 1958, 13, 5211, 490, 1150, 732, 293, 1958, 13, 18], "temperature": 0.0, "avg_logprob": -0.30764031410217285, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.2123904273030348e-05}, {"id": 171, "seek": 65778, "start": 669.3, "end": 670.38, "text": " from second three.", "tokens": [490, 1150, 1045, 13], "temperature": 0.0, "avg_logprob": -0.30764031410217285, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.2123904273030348e-05}, {"id": 172, "seek": 65778, "start": 673.42, "end": 677.26, "text": " And then what the interpretation,", "tokens": [400, 550, 437, 264, 14174, 11], "temperature": 0.0, "avg_logprob": -0.30764031410217285, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.2123904273030348e-05}, {"id": 173, "seek": 65778, "start": 677.26, "end": 679.42, "text": " is there any way to interpret the information added", "tokens": [307, 456, 604, 636, 281, 7302, 264, 1589, 3869], "temperature": 0.0, "avg_logprob": -0.30764031410217285, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.2123904273030348e-05}, {"id": 174, "seek": 65778, "start": 679.42, "end": 682.06, "text": " by that vector?", "tokens": [538, 300, 8062, 30], "temperature": 0.0, "avg_logprob": -0.30764031410217285, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.2123904273030348e-05}, {"id": 175, "seek": 65778, "start": 682.06, "end": 685.4599999999999, "text": " I mean, I'm just definitely still.", "tokens": [286, 914, 11, 286, 478, 445, 2138, 920, 13], "temperature": 0.0, "avg_logprob": -0.30764031410217285, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.2123904273030348e-05}, {"id": 176, "seek": 65778, "start": 685.4599999999999, "end": 687.54, "text": " So let me remind you the overarching goal", "tokens": [407, 718, 385, 4160, 291, 264, 45501, 3387], "temperature": 0.0, "avg_logprob": -0.30764031410217285, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.2123904273030348e-05}, {"id": 177, "seek": 68754, "start": 687.54, "end": 689.6999999999999, "text": " with this randomized projection is", "tokens": [365, 341, 38513, 22743, 307], "temperature": 0.0, "avg_logprob": -0.13171798881443067, "compression_ratio": 1.594871794871795, "no_speech_prob": 3.071461105719209e-05}, {"id": 178, "seek": 68754, "start": 689.6999999999999, "end": 694.14, "text": " to get something where the columns span the same space", "tokens": [281, 483, 746, 689, 264, 13766, 16174, 264, 912, 1901], "temperature": 0.0, "avg_logprob": -0.13171798881443067, "compression_ratio": 1.594871794871795, "no_speech_prob": 3.071461105719209e-05}, {"id": 179, "seek": 68754, "start": 694.14, "end": 697.18, "text": " as the columns here.", "tokens": [382, 264, 13766, 510, 13], "temperature": 0.0, "avg_logprob": -0.13171798881443067, "compression_ratio": 1.594871794871795, "no_speech_prob": 3.071461105719209e-05}, {"id": 180, "seek": 68754, "start": 697.18, "end": 701.38, "text": " So kind of if we did several of these,", "tokens": [407, 733, 295, 498, 321, 630, 2940, 295, 613, 11], "temperature": 0.0, "avg_logprob": -0.13171798881443067, "compression_ratio": 1.594871794871795, "no_speech_prob": 3.071461105719209e-05}, {"id": 181, "seek": 68754, "start": 701.38, "end": 702.54, "text": " we would get a matrix.", "tokens": [321, 576, 483, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.13171798881443067, "compression_ratio": 1.594871794871795, "no_speech_prob": 3.071461105719209e-05}, {"id": 182, "seek": 68754, "start": 702.54, "end": 704.9399999999999, "text": " Because this is a very wide matrix.", "tokens": [1436, 341, 307, 257, 588, 4874, 8141, 13], "temperature": 0.0, "avg_logprob": -0.13171798881443067, "compression_ratio": 1.594871794871795, "no_speech_prob": 3.071461105719209e-05}, {"id": 183, "seek": 68754, "start": 704.9399999999999, "end": 706.9399999999999, "text": " And we want to get something that's narrower", "tokens": [400, 321, 528, 281, 483, 746, 300, 311, 46751], "temperature": 0.0, "avg_logprob": -0.13171798881443067, "compression_ratio": 1.594871794871795, "no_speech_prob": 3.071461105719209e-05}, {"id": 184, "seek": 68754, "start": 706.9399999999999, "end": 709.86, "text": " but has all the same information.", "tokens": [457, 575, 439, 264, 912, 1589, 13], "temperature": 0.0, "avg_logprob": -0.13171798881443067, "compression_ratio": 1.594871794871795, "no_speech_prob": 3.071461105719209e-05}, {"id": 185, "seek": 68754, "start": 709.86, "end": 714.4599999999999, "text": " Or Sam has his hand up.", "tokens": [1610, 4832, 575, 702, 1011, 493, 13], "temperature": 0.0, "avg_logprob": -0.13171798881443067, "compression_ratio": 1.594871794871795, "no_speech_prob": 3.071461105719209e-05}, {"id": 186, "seek": 71446, "start": 714.46, "end": 719.7, "text": " So if we had a picture that had, where we had many pictures,", "tokens": [407, 498, 321, 632, 257, 3036, 300, 632, 11, 689, 321, 632, 867, 5242, 11], "temperature": 0.0, "avg_logprob": -0.2269176404500745, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00013753346865996718}, {"id": 187, "seek": 71446, "start": 719.7, "end": 720.94, "text": " there was a lot of background.", "tokens": [456, 390, 257, 688, 295, 3678, 13], "temperature": 0.0, "avg_logprob": -0.2269176404500745, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00013753346865996718}, {"id": 188, "seek": 71446, "start": 720.94, "end": 722.7, "text": " And in maybe a third of them, there", "tokens": [400, 294, 1310, 257, 2636, 295, 552, 11, 456], "temperature": 0.0, "avg_logprob": -0.2269176404500745, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00013753346865996718}, {"id": 189, "seek": 71446, "start": 722.7, "end": 725.7800000000001, "text": " was a person standing in the middle.", "tokens": [390, 257, 954, 4877, 294, 264, 2808, 13], "temperature": 0.0, "avg_logprob": -0.2269176404500745, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00013753346865996718}, {"id": 190, "seek": 71446, "start": 725.7800000000001, "end": 730.5400000000001, "text": " Then we took two random vectors and multiplied", "tokens": [1396, 321, 1890, 732, 4974, 18875, 293, 17207], "temperature": 0.0, "avg_logprob": -0.2269176404500745, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00013753346865996718}, {"id": 191, "seek": 71446, "start": 730.5400000000001, "end": 733.46, "text": " to find the matrix and got two columns.", "tokens": [281, 915, 264, 8141, 293, 658, 732, 13766, 13], "temperature": 0.0, "avg_logprob": -0.2269176404500745, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00013753346865996718}, {"id": 192, "seek": 71446, "start": 733.46, "end": 738.6600000000001, "text": " Then we could represent the person.", "tokens": [1396, 321, 727, 2906, 264, 954, 13], "temperature": 0.0, "avg_logprob": -0.2269176404500745, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00013753346865996718}, {"id": 193, "seek": 71446, "start": 738.6600000000001, "end": 743.98, "text": " Let's say in one of the vectors, it multiplied 0 by some change.", "tokens": [961, 311, 584, 294, 472, 295, 264, 18875, 11, 309, 17207, 1958, 538, 512, 1319, 13], "temperature": 0.0, "avg_logprob": -0.2269176404500745, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00013753346865996718}, {"id": 194, "seek": 74398, "start": 743.98, "end": 744.66, "text": " Yes, yeah.", "tokens": [1079, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2722328700376361, "compression_ratio": 1.6889952153110048, "no_speech_prob": 0.0001487130211899057}, {"id": 195, "seek": 74398, "start": 744.66, "end": 747.54, "text": " Sometimes all of the images that have a person,", "tokens": [4803, 439, 295, 264, 5267, 300, 362, 257, 954, 11], "temperature": 0.0, "avg_logprob": -0.2722328700376361, "compression_ratio": 1.6889952153110048, "no_speech_prob": 0.0001487130211899057}, {"id": 196, "seek": 74398, "start": 747.54, "end": 751.4200000000001, "text": " but there was some positive value multiplied", "tokens": [457, 456, 390, 512, 3353, 2158, 17207], "temperature": 0.0, "avg_logprob": -0.2722328700376361, "compression_ratio": 1.6889952153110048, "no_speech_prob": 0.0001487130211899057}, {"id": 197, "seek": 74398, "start": 751.4200000000001, "end": 754.38, "text": " by the ones with the person, then you", "tokens": [538, 264, 2306, 365, 264, 954, 11, 550, 291], "temperature": 0.0, "avg_logprob": -0.2722328700376361, "compression_ratio": 1.6889952153110048, "no_speech_prob": 0.0001487130211899057}, {"id": 198, "seek": 74398, "start": 754.38, "end": 757.1, "text": " could reconstruct just the person", "tokens": [727, 31499, 445, 264, 954], "temperature": 0.0, "avg_logprob": -0.2722328700376361, "compression_ratio": 1.6889952153110048, "no_speech_prob": 0.0001487130211899057}, {"id": 199, "seek": 74398, "start": 757.1, "end": 763.3000000000001, "text": " by some positive number times the second column", "tokens": [538, 512, 3353, 1230, 1413, 264, 1150, 7738], "temperature": 0.0, "avg_logprob": -0.2722328700376361, "compression_ratio": 1.6889952153110048, "no_speech_prob": 0.0001487130211899057}, {"id": 200, "seek": 74398, "start": 763.3000000000001, "end": 765.78, "text": " minus the first column.", "tokens": [3175, 264, 700, 7738, 13], "temperature": 0.0, "avg_logprob": -0.2722328700376361, "compression_ratio": 1.6889952153110048, "no_speech_prob": 0.0001487130211899057}, {"id": 201, "seek": 74398, "start": 765.78, "end": 769.1, "text": " So the information of the person is captured.", "tokens": [407, 264, 1589, 295, 264, 954, 307, 11828, 13], "temperature": 0.0, "avg_logprob": -0.2722328700376361, "compression_ratio": 1.6889952153110048, "no_speech_prob": 0.0001487130211899057}, {"id": 202, "seek": 74398, "start": 769.1, "end": 773.46, "text": " Because now we can map anything else in any of the pictures", "tokens": [1436, 586, 321, 393, 4471, 1340, 1646, 294, 604, 295, 264, 5242], "temperature": 0.0, "avg_logprob": -0.2722328700376361, "compression_ratio": 1.6889952153110048, "no_speech_prob": 0.0001487130211899057}, {"id": 203, "seek": 77346, "start": 773.46, "end": 777.1800000000001, "text": " by some linear combination of these random vectors.", "tokens": [538, 512, 8213, 6562, 295, 613, 4974, 18875, 13], "temperature": 0.0, "avg_logprob": -0.2175528564453125, "compression_ratio": 1.592741935483871, "no_speech_prob": 1.165870162367355e-05}, {"id": 204, "seek": 77346, "start": 777.1800000000001, "end": 779.62, "text": " Yes, yeah.", "tokens": [1079, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2175528564453125, "compression_ratio": 1.592741935483871, "no_speech_prob": 1.165870162367355e-05}, {"id": 205, "seek": 77346, "start": 779.62, "end": 781.22, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2175528564453125, "compression_ratio": 1.592741935483871, "no_speech_prob": 1.165870162367355e-05}, {"id": 206, "seek": 77346, "start": 781.22, "end": 784.74, "text": " And so that, and since it's, yeah.", "tokens": [400, 370, 300, 11, 293, 1670, 309, 311, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2175528564453125, "compression_ratio": 1.592741935483871, "no_speech_prob": 1.165870162367355e-05}, {"id": 207, "seek": 77346, "start": 784.74, "end": 785.94, "text": " Yeah, that's a good example.", "tokens": [865, 11, 300, 311, 257, 665, 1365, 13], "temperature": 0.0, "avg_logprob": -0.2175528564453125, "compression_ratio": 1.592741935483871, "no_speech_prob": 1.165870162367355e-05}, {"id": 208, "seek": 77346, "start": 785.94, "end": 787.34, "text": " Thank you, Sam.", "tokens": [1044, 291, 11, 4832, 13], "temperature": 0.0, "avg_logprob": -0.2175528564453125, "compression_ratio": 1.592741935483871, "no_speech_prob": 1.165870162367355e-05}, {"id": 209, "seek": 77346, "start": 787.34, "end": 789.3000000000001, "text": " Yeah, and because of this is randomness", "tokens": [865, 11, 293, 570, 295, 341, 307, 4974, 1287], "temperature": 0.0, "avg_logprob": -0.2175528564453125, "compression_ratio": 1.592741935483871, "no_speech_prob": 1.165870162367355e-05}, {"id": 210, "seek": 77346, "start": 789.3000000000001, "end": 790.94, "text": " and it's not going to work out perfectly,", "tokens": [293, 309, 311, 406, 516, 281, 589, 484, 6239, 11], "temperature": 0.0, "avg_logprob": -0.2175528564453125, "compression_ratio": 1.592741935483871, "no_speech_prob": 1.165870162367355e-05}, {"id": 211, "seek": 77346, "start": 790.94, "end": 794.34, "text": " you would probably want more vectors in practice.", "tokens": [291, 576, 1391, 528, 544, 18875, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.2175528564453125, "compression_ratio": 1.592741935483871, "no_speech_prob": 1.165870162367355e-05}, {"id": 212, "seek": 77346, "start": 794.34, "end": 796.1800000000001, "text": " But yeah, that's the idea.", "tokens": [583, 1338, 11, 300, 311, 264, 1558, 13], "temperature": 0.0, "avg_logprob": -0.2175528564453125, "compression_ratio": 1.592741935483871, "no_speech_prob": 1.165870162367355e-05}, {"id": 213, "seek": 77346, "start": 796.1800000000001, "end": 798.9000000000001, "text": " Can you still go over why they're orthonormal?", "tokens": [1664, 291, 920, 352, 670, 983, 436, 434, 420, 11943, 24440, 30], "temperature": 0.0, "avg_logprob": -0.2175528564453125, "compression_ratio": 1.592741935483871, "no_speech_prob": 1.165870162367355e-05}, {"id": 214, "seek": 77346, "start": 798.9000000000001, "end": 800.46, "text": " That's a pretty strong condition.", "tokens": [663, 311, 257, 1238, 2068, 4188, 13], "temperature": 0.0, "avg_logprob": -0.2175528564453125, "compression_ratio": 1.592741935483871, "no_speech_prob": 1.165870162367355e-05}, {"id": 215, "seek": 77346, "start": 800.46, "end": 800.98, "text": " It is.", "tokens": [467, 307, 13], "temperature": 0.0, "avg_logprob": -0.2175528564453125, "compression_ratio": 1.592741935483871, "no_speech_prob": 1.165870162367355e-05}, {"id": 216, "seek": 80098, "start": 800.98, "end": 804.62, "text": " So this is kind of approximately.", "tokens": [407, 341, 307, 733, 295, 10447, 13], "temperature": 0.0, "avg_logprob": -0.1837280497831457, "compression_ratio": 1.49746192893401, "no_speech_prob": 5.421818968898151e-06}, {"id": 217, "seek": 80098, "start": 804.62, "end": 807.22, "text": " And it really just comes down to dimensionality.", "tokens": [400, 309, 534, 445, 1487, 760, 281, 10139, 1860, 13], "temperature": 0.0, "avg_logprob": -0.1837280497831457, "compression_ratio": 1.49746192893401, "no_speech_prob": 5.421818968898151e-06}, {"id": 218, "seek": 80098, "start": 807.22, "end": 812.22, "text": " So the idea that if, I can't remember, this is 4,800 tall.", "tokens": [407, 264, 1558, 300, 498, 11, 286, 393, 380, 1604, 11, 341, 307, 1017, 11, 14423, 6764, 13], "temperature": 0.0, "avg_logprob": -0.1837280497831457, "compression_ratio": 1.49746192893401, "no_speech_prob": 5.421818968898151e-06}, {"id": 219, "seek": 80098, "start": 812.22, "end": 816.9, "text": " So if you had two vectors of length 4,800,", "tokens": [407, 498, 291, 632, 732, 18875, 295, 4641, 1017, 11, 14423, 11], "temperature": 0.0, "avg_logprob": -0.1837280497831457, "compression_ratio": 1.49746192893401, "no_speech_prob": 5.421818968898151e-06}, {"id": 220, "seek": 80098, "start": 816.9, "end": 823.34, "text": " for them to be pointing in the same direction", "tokens": [337, 552, 281, 312, 12166, 294, 264, 912, 3513], "temperature": 0.0, "avg_logprob": -0.1837280497831457, "compression_ratio": 1.49746192893401, "no_speech_prob": 5.421818968898151e-06}, {"id": 221, "seek": 80098, "start": 823.34, "end": 826.14, "text": " is just statistically unlikely.", "tokens": [307, 445, 36478, 17518, 13], "temperature": 0.0, "avg_logprob": -0.1837280497831457, "compression_ratio": 1.49746192893401, "no_speech_prob": 5.421818968898151e-06}, {"id": 222, "seek": 80098, "start": 826.14, "end": 828.58, "text": " Kind of if you have this random.", "tokens": [9242, 295, 498, 291, 362, 341, 4974, 13], "temperature": 0.0, "avg_logprob": -0.1837280497831457, "compression_ratio": 1.49746192893401, "no_speech_prob": 5.421818968898151e-06}, {"id": 223, "seek": 82858, "start": 828.58, "end": 832.3000000000001, "text": " Could be pointing with slight deviation away from each other.", "tokens": [7497, 312, 12166, 365, 4036, 25163, 1314, 490, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.24988259559820505, "compression_ratio": 1.771551724137931, "no_speech_prob": 0.00014199122961144894}, {"id": 224, "seek": 82858, "start": 832.3000000000001, "end": 834.0600000000001, "text": " It could just be a slight perturbation", "tokens": [467, 727, 445, 312, 257, 4036, 40468, 399], "temperature": 0.0, "avg_logprob": -0.24988259559820505, "compression_ratio": 1.771551724137931, "no_speech_prob": 0.00014199122961144894}, {"id": 225, "seek": 82858, "start": 834.0600000000001, "end": 836.46, "text": " where they're not this.", "tokens": [689, 436, 434, 406, 341, 13], "temperature": 0.0, "avg_logprob": -0.24988259559820505, "compression_ratio": 1.771551724137931, "no_speech_prob": 0.00014199122961144894}, {"id": 226, "seek": 82858, "start": 836.46, "end": 839.74, "text": " Yeah, I guess it's more that.", "tokens": [865, 11, 286, 2041, 309, 311, 544, 300, 13], "temperature": 0.0, "avg_logprob": -0.24988259559820505, "compression_ratio": 1.771551724137931, "no_speech_prob": 0.00014199122961144894}, {"id": 227, "seek": 82858, "start": 839.74, "end": 841.22, "text": " They're not multiples?", "tokens": [814, 434, 406, 46099, 30], "temperature": 0.0, "avg_logprob": -0.24988259559820505, "compression_ratio": 1.771551724137931, "no_speech_prob": 0.00014199122961144894}, {"id": 228, "seek": 82858, "start": 841.22, "end": 844.0200000000001, "text": " Yeah, they're definitely not multiples.", "tokens": [865, 11, 436, 434, 2138, 406, 46099, 13], "temperature": 0.0, "avg_logprob": -0.24988259559820505, "compression_ratio": 1.771551724137931, "no_speech_prob": 0.00014199122961144894}, {"id": 229, "seek": 82858, "start": 844.0200000000001, "end": 846.94, "text": " It's also really unlikely that they're exactly.", "tokens": [467, 311, 611, 534, 17518, 300, 436, 434, 2293, 13], "temperature": 0.0, "avg_logprob": -0.24988259559820505, "compression_ratio": 1.771551724137931, "no_speech_prob": 0.00014199122961144894}, {"id": 230, "seek": 82858, "start": 846.94, "end": 850.0600000000001, "text": " Yeah, right.", "tokens": [865, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.24988259559820505, "compression_ratio": 1.771551724137931, "no_speech_prob": 0.00014199122961144894}, {"id": 231, "seek": 82858, "start": 850.0600000000001, "end": 852.6600000000001, "text": " Yeah, and I may be, yeah, I want to emphasize", "tokens": [865, 11, 293, 286, 815, 312, 11, 1338, 11, 286, 528, 281, 16078], "temperature": 0.0, "avg_logprob": -0.24988259559820505, "compression_ratio": 1.771551724137931, "no_speech_prob": 0.00014199122961144894}, {"id": 232, "seek": 82858, "start": 852.6600000000001, "end": 854.6600000000001, "text": " that it's kind of the key thing is that they're not", "tokens": [300, 309, 311, 733, 295, 264, 2141, 551, 307, 300, 436, 434, 406], "temperature": 0.0, "avg_logprob": -0.24988259559820505, "compression_ratio": 1.771551724137931, "no_speech_prob": 0.00014199122961144894}, {"id": 233, "seek": 82858, "start": 854.6600000000001, "end": 856.9000000000001, "text": " exact multiples of each other.", "tokens": [1900, 46099, 295, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.24988259559820505, "compression_ratio": 1.771551724137931, "no_speech_prob": 0.00014199122961144894}, {"id": 234, "seek": 82858, "start": 856.9000000000001, "end": 858.0200000000001, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.24988259559820505, "compression_ratio": 1.771551724137931, "no_speech_prob": 0.00014199122961144894}, {"id": 235, "seek": 85802, "start": 858.02, "end": 861.14, "text": " So is the idea that we want them to be orthonormal", "tokens": [407, 307, 264, 1558, 300, 321, 528, 552, 281, 312, 420, 11943, 24440], "temperature": 0.0, "avg_logprob": -0.21421160016741073, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.586472146504093e-05}, {"id": 236, "seek": 85802, "start": 861.14, "end": 863.1, "text": " or the idea that we don't want them to be dependent?", "tokens": [420, 264, 1558, 300, 321, 500, 380, 528, 552, 281, 312, 12334, 30], "temperature": 0.0, "avg_logprob": -0.21421160016741073, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.586472146504093e-05}, {"id": 237, "seek": 85802, "start": 863.1, "end": 864.1, "text": " Yeah, it's more that we don't want", "tokens": [865, 11, 309, 311, 544, 300, 321, 500, 380, 528], "temperature": 0.0, "avg_logprob": -0.21421160016741073, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.586472146504093e-05}, {"id": 238, "seek": 85802, "start": 864.1, "end": 865.6999999999999, "text": " them to be linearly dependent.", "tokens": [552, 281, 312, 43586, 12334, 13], "temperature": 0.0, "avg_logprob": -0.21421160016741073, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.586472146504093e-05}, {"id": 239, "seek": 85802, "start": 865.6999999999999, "end": 866.98, "text": " Yeah, I should change that.", "tokens": [865, 11, 286, 820, 1319, 300, 13], "temperature": 0.0, "avg_logprob": -0.21421160016741073, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.586472146504093e-05}, {"id": 240, "seek": 85802, "start": 866.98, "end": 868.86, "text": " Yeah, sorry about that.", "tokens": [865, 11, 2597, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.21421160016741073, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.586472146504093e-05}, {"id": 241, "seek": 85802, "start": 868.86, "end": 873.6999999999999, "text": " Well, the average correlation of two random projections", "tokens": [1042, 11, 264, 4274, 20009, 295, 732, 4974, 32371], "temperature": 0.0, "avg_logprob": -0.21421160016741073, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.586472146504093e-05}, {"id": 242, "seek": 85802, "start": 873.6999999999999, "end": 876.66, "text": " will, on average, be 0.", "tokens": [486, 11, 322, 4274, 11, 312, 1958, 13], "temperature": 0.0, "avg_logprob": -0.21421160016741073, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.586472146504093e-05}, {"id": 243, "seek": 85802, "start": 876.66, "end": 879.5, "text": " That could be surprising if they were highly correlated", "tokens": [663, 727, 312, 8830, 498, 436, 645, 5405, 38574], "temperature": 0.0, "avg_logprob": -0.21421160016741073, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.586472146504093e-05}, {"id": 244, "seek": 85802, "start": 879.5, "end": 881.5, "text": " in a high dimension space.", "tokens": [294, 257, 1090, 10139, 1901, 13], "temperature": 0.0, "avg_logprob": -0.21421160016741073, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.586472146504093e-05}, {"id": 245, "seek": 85802, "start": 881.5, "end": 884.3, "text": " So I still think.", "tokens": [407, 286, 920, 519, 13], "temperature": 0.0, "avg_logprob": -0.21421160016741073, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.586472146504093e-05}, {"id": 246, "seek": 85802, "start": 884.3, "end": 886.9399999999999, "text": " I'll say maybe not highly correlated.", "tokens": [286, 603, 584, 1310, 406, 5405, 38574, 13], "temperature": 0.0, "avg_logprob": -0.21421160016741073, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.586472146504093e-05}, {"id": 247, "seek": 88694, "start": 886.94, "end": 889.4200000000001, "text": " I was thinking like in America, if you take two random vectors", "tokens": [286, 390, 1953, 411, 294, 3374, 11, 498, 291, 747, 732, 4974, 18875], "temperature": 0.0, "avg_logprob": -0.36405549225983797, "compression_ratio": 1.6616541353383458, "no_speech_prob": 5.3886440582573414e-05}, {"id": 248, "seek": 88694, "start": 889.4200000000001, "end": 892.4200000000001, "text": " and you dot product them, it'll be close to 0.", "tokens": [293, 291, 5893, 1674, 552, 11, 309, 603, 312, 1998, 281, 1958, 13], "temperature": 0.0, "avg_logprob": -0.36405549225983797, "compression_ratio": 1.6616541353383458, "no_speech_prob": 5.3886440582573414e-05}, {"id": 249, "seek": 88694, "start": 895.4200000000001, "end": 896.4200000000001, "text": " See what I mean?", "tokens": [3008, 437, 286, 914, 30], "temperature": 0.0, "avg_logprob": -0.36405549225983797, "compression_ratio": 1.6616541353383458, "no_speech_prob": 5.3886440582573414e-05}, {"id": 250, "seek": 88694, "start": 896.4200000000001, "end": 898.4200000000001, "text": " What if they're all like, they're not all,", "tokens": [708, 498, 436, 434, 439, 411, 11, 436, 434, 406, 439, 11], "temperature": 0.0, "avg_logprob": -0.36405549225983797, "compression_ratio": 1.6616541353383458, "no_speech_prob": 5.3886440582573414e-05}, {"id": 251, "seek": 88694, "start": 898.4200000000001, "end": 900.4200000000001, "text": " like everything is slightly positive,", "tokens": [411, 1203, 307, 4748, 3353, 11], "temperature": 0.0, "avg_logprob": -0.36405549225983797, "compression_ratio": 1.6616541353383458, "no_speech_prob": 5.3886440582573414e-05}, {"id": 252, "seek": 88694, "start": 900.4200000000001, "end": 902.4200000000001, "text": " like every interest has to be positive?", "tokens": [411, 633, 1179, 575, 281, 312, 3353, 30], "temperature": 0.0, "avg_logprob": -0.36405549225983797, "compression_ratio": 1.6616541353383458, "no_speech_prob": 5.3886440582573414e-05}, {"id": 253, "seek": 88694, "start": 902.4200000000001, "end": 904.4200000000001, "text": " But these are random Gaussians, so 0 and random Gaussians.", "tokens": [583, 613, 366, 4974, 10384, 2023, 2567, 11, 370, 1958, 293, 4974, 10384, 2023, 2567, 13], "temperature": 0.0, "avg_logprob": -0.36405549225983797, "compression_ratio": 1.6616541353383458, "no_speech_prob": 5.3886440582573414e-05}, {"id": 254, "seek": 88694, "start": 904.4200000000001, "end": 907.4200000000001, "text": " So what's the probability of having 4800 positive numbers?", "tokens": [407, 437, 311, 264, 8482, 295, 1419, 11174, 628, 3353, 3547, 30], "temperature": 0.0, "avg_logprob": -0.36405549225983797, "compression_ratio": 1.6616541353383458, "no_speech_prob": 5.3886440582573414e-05}, {"id": 255, "seek": 88694, "start": 907.4200000000001, "end": 909.4200000000001, "text": " It's not high.", "tokens": [467, 311, 406, 1090, 13], "temperature": 0.0, "avg_logprob": -0.36405549225983797, "compression_ratio": 1.6616541353383458, "no_speech_prob": 5.3886440582573414e-05}, {"id": 256, "seek": 88694, "start": 909.4200000000001, "end": 911.4200000000001, "text": " It's 2 to the 4800.", "tokens": [467, 311, 568, 281, 264, 11174, 628, 13], "temperature": 0.0, "avg_logprob": -0.36405549225983797, "compression_ratio": 1.6616541353383458, "no_speech_prob": 5.3886440582573414e-05}, {"id": 257, "seek": 88694, "start": 911.4200000000001, "end": 914.4200000000001, "text": " So you're picking the center at 0.", "tokens": [407, 291, 434, 8867, 264, 3056, 412, 1958, 13], "temperature": 0.0, "avg_logprob": -0.36405549225983797, "compression_ratio": 1.6616541353383458, "no_speech_prob": 5.3886440582573414e-05}, {"id": 258, "seek": 88694, "start": 914.4200000000001, "end": 915.4200000000001, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.36405549225983797, "compression_ratio": 1.6616541353383458, "no_speech_prob": 5.3886440582573414e-05}, {"id": 259, "seek": 91542, "start": 915.42, "end": 918.42, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.21124392747879028, "compression_ratio": 1.570754716981132, "no_speech_prob": 8.47526389406994e-05}, {"id": 260, "seek": 91542, "start": 918.42, "end": 924.42, "text": " So my question is, so say that we randomly pick these colors.", "tokens": [407, 452, 1168, 307, 11, 370, 584, 300, 321, 16979, 1888, 613, 4577, 13], "temperature": 0.0, "avg_logprob": -0.21124392747879028, "compression_ratio": 1.570754716981132, "no_speech_prob": 8.47526389406994e-05}, {"id": 261, "seek": 91542, "start": 924.42, "end": 927.42, "text": " So how do we decide which colors to pick?", "tokens": [407, 577, 360, 321, 4536, 597, 4577, 281, 1888, 30], "temperature": 0.0, "avg_logprob": -0.21124392747879028, "compression_ratio": 1.570754716981132, "no_speech_prob": 8.47526389406994e-05}, {"id": 262, "seek": 91542, "start": 927.42, "end": 932.42, "text": " I know we kind of randomize it, but then just for our own purposes,", "tokens": [286, 458, 321, 733, 295, 4974, 1125, 309, 11, 457, 550, 445, 337, 527, 1065, 9932, 11], "temperature": 0.0, "avg_logprob": -0.21124392747879028, "compression_ratio": 1.570754716981132, "no_speech_prob": 8.47526389406994e-05}, {"id": 263, "seek": 91542, "start": 932.42, "end": 936.42, "text": " do we just see, oh, we pick these five out of 100 randomly,", "tokens": [360, 321, 445, 536, 11, 1954, 11, 321, 1888, 613, 1732, 484, 295, 2319, 16979, 11], "temperature": 0.0, "avg_logprob": -0.21124392747879028, "compression_ratio": 1.570754716981132, "no_speech_prob": 8.47526389406994e-05}, {"id": 264, "seek": 91542, "start": 936.42, "end": 939.42, "text": " see what kind of background we get, and then try another one?", "tokens": [536, 437, 733, 295, 3678, 321, 483, 11, 293, 550, 853, 1071, 472, 30], "temperature": 0.0, "avg_logprob": -0.21124392747879028, "compression_ratio": 1.570754716981132, "no_speech_prob": 8.47526389406994e-05}, {"id": 265, "seek": 91542, "start": 939.42, "end": 941.42, "text": " See if we get something different?", "tokens": [3008, 498, 321, 483, 746, 819, 30], "temperature": 0.0, "avg_logprob": -0.21124392747879028, "compression_ratio": 1.570754716981132, "no_speech_prob": 8.47526389406994e-05}, {"id": 266, "seek": 94142, "start": 941.42, "end": 945.42, "text": " So really what we're doing, and let me open up this one,", "tokens": [407, 534, 437, 321, 434, 884, 11, 293, 718, 385, 1269, 493, 341, 472, 11], "temperature": 0.0, "avg_logprob": -0.11985735826089348, "compression_ratio": 1.5089820359281436, "no_speech_prob": 1.4970325537433382e-05}, {"id": 267, "seek": 94142, "start": 945.42, "end": 949.42, "text": " we had this randomized range finder method,", "tokens": [321, 632, 341, 38513, 3613, 915, 260, 3170, 11], "temperature": 0.0, "avg_logprob": -0.11985735826089348, "compression_ratio": 1.5089820359281436, "no_speech_prob": 1.4970325537433382e-05}, {"id": 268, "seek": 94142, "start": 949.42, "end": 953.42, "text": " and so we're kind of doing this in the context of trying to find a matrix", "tokens": [293, 370, 321, 434, 733, 295, 884, 341, 294, 264, 4319, 295, 1382, 281, 915, 257, 8141], "temperature": 0.0, "avg_logprob": -0.11985735826089348, "compression_ratio": 1.5089820359281436, "no_speech_prob": 1.4970325537433382e-05}, {"id": 269, "seek": 94142, "start": 953.42, "end": 958.42, "text": " that's like our original matrix, but has far fewer columns.", "tokens": [300, 311, 411, 527, 3380, 8141, 11, 457, 575, 1400, 13366, 13766, 13], "temperature": 0.0, "avg_logprob": -0.11985735826089348, "compression_ratio": 1.5089820359281436, "no_speech_prob": 1.4970325537433382e-05}, {"id": 270, "seek": 95842, "start": 958.42, "end": 965.42, "text": " Go back up to it.", "tokens": [1037, 646, 493, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.18255403462578268, "compression_ratio": 1.425, "no_speech_prob": 6.9621346483472735e-06}, {"id": 271, "seek": 95842, "start": 965.42, "end": 971.42, "text": " And so remember, this is kind of like one step in getting the randomized SVD,", "tokens": [400, 370, 1604, 11, 341, 307, 733, 295, 411, 472, 1823, 294, 1242, 264, 38513, 31910, 35, 11], "temperature": 0.0, "avg_logprob": -0.18255403462578268, "compression_ratio": 1.425, "no_speech_prob": 6.9621346483472735e-06}, {"id": 272, "seek": 95842, "start": 971.42, "end": 974.42, "text": " although in some ways this is, I think, like the hardest", "tokens": [4878, 294, 512, 2098, 341, 307, 11, 286, 519, 11, 411, 264, 13158], "temperature": 0.0, "avg_logprob": -0.18255403462578268, "compression_ratio": 1.425, "no_speech_prob": 6.9621346483472735e-06}, {"id": 273, "seek": 95842, "start": 974.42, "end": 977.42, "text": " or kind of most non-intuitive step.", "tokens": [420, 733, 295, 881, 2107, 12, 686, 48314, 1823, 13], "temperature": 0.0, "avg_logprob": -0.18255403462578268, "compression_ratio": 1.425, "no_speech_prob": 6.9621346483472735e-06}, {"id": 274, "seek": 95842, "start": 977.42, "end": 983.42, "text": " But we're taking kind of random normal,", "tokens": [583, 321, 434, 1940, 733, 295, 4974, 2710, 11], "temperature": 0.0, "avg_logprob": -0.18255403462578268, "compression_ratio": 1.425, "no_speech_prob": 6.9621346483472735e-06}, {"id": 275, "seek": 98342, "start": 983.42, "end": 991.42, "text": " yeah, of shape A, and then we're multiplying A and Q together.", "tokens": [1338, 11, 295, 3909, 316, 11, 293, 550, 321, 434, 30955, 316, 293, 1249, 1214, 13], "temperature": 0.0, "avg_logprob": -0.14808107305456092, "compression_ratio": 1.546218487394958, "no_speech_prob": 1.1478608939796686e-05}, {"id": 276, "seek": 98342, "start": 991.42, "end": 995.42, "text": " So this is kind of, I guess, going to the question of like,", "tokens": [407, 341, 307, 733, 295, 11, 286, 2041, 11, 516, 281, 264, 1168, 295, 411, 11], "temperature": 0.0, "avg_logprob": -0.14808107305456092, "compression_ratio": 1.546218487394958, "no_speech_prob": 1.1478608939796686e-05}, {"id": 277, "seek": 98342, "start": 995.42, "end": 997.42, "text": " how do we know which ones to take?", "tokens": [577, 360, 321, 458, 597, 2306, 281, 747, 30], "temperature": 0.0, "avg_logprob": -0.14808107305456092, "compression_ratio": 1.546218487394958, "no_speech_prob": 1.1478608939796686e-05}, {"id": 278, "seek": 98342, "start": 997.42, "end": 1000.42, "text": " Like it really is just random, but we do have, remember,", "tokens": [1743, 309, 534, 307, 445, 4974, 11, 457, 321, 360, 362, 11, 1604, 11], "temperature": 0.0, "avg_logprob": -0.14808107305456092, "compression_ratio": 1.546218487394958, "no_speech_prob": 1.1478608939796686e-05}, {"id": 279, "seek": 98342, "start": 1000.42, "end": 1003.42, "text": " that number of oversamples.", "tokens": [300, 1230, 295, 15488, 335, 2622, 13], "temperature": 0.0, "avg_logprob": -0.14808107305456092, "compression_ratio": 1.546218487394958, "no_speech_prob": 1.1478608939796686e-05}, {"id": 280, "seek": 98342, "start": 1003.42, "end": 1006.42, "text": " So like we are taking kind of more than we'll end up needing.", "tokens": [407, 411, 321, 366, 1940, 733, 295, 544, 813, 321, 603, 917, 493, 18006, 13], "temperature": 0.0, "avg_logprob": -0.14808107305456092, "compression_ratio": 1.546218487394958, "no_speech_prob": 1.1478608939796686e-05}, {"id": 281, "seek": 98342, "start": 1006.42, "end": 1009.42, "text": " You know, this is towards the goal of getting a randomized SVD,", "tokens": [509, 458, 11, 341, 307, 3030, 264, 3387, 295, 1242, 257, 38513, 31910, 35, 11], "temperature": 0.0, "avg_logprob": -0.14808107305456092, "compression_ratio": 1.546218487394958, "no_speech_prob": 1.1478608939796686e-05}, {"id": 282, "seek": 100942, "start": 1009.42, "end": 1013.42, "text": " which is a good way to kind of get the truncated SVD", "tokens": [597, 307, 257, 665, 636, 281, 733, 295, 483, 264, 504, 409, 66, 770, 31910, 35], "temperature": 0.0, "avg_logprob": -0.07815017256625863, "compression_ratio": 1.51, "no_speech_prob": 6.204758392414078e-05}, {"id": 283, "seek": 100942, "start": 1013.42, "end": 1016.42, "text": " without having to calculate the full SVD,", "tokens": [1553, 1419, 281, 8873, 264, 1577, 31910, 35, 11], "temperature": 0.0, "avg_logprob": -0.07815017256625863, "compression_ratio": 1.51, "no_speech_prob": 6.204758392414078e-05}, {"id": 284, "seek": 100942, "start": 1016.42, "end": 1018.42, "text": " which would be really slow for a large matrix.", "tokens": [597, 576, 312, 534, 2964, 337, 257, 2416, 8141, 13], "temperature": 0.0, "avg_logprob": -0.07815017256625863, "compression_ratio": 1.51, "no_speech_prob": 6.204758392414078e-05}, {"id": 285, "seek": 100942, "start": 1018.42, "end": 1022.42, "text": " And so here, you know, we'll have like a number of components", "tokens": [400, 370, 510, 11, 291, 458, 11, 321, 603, 362, 411, 257, 1230, 295, 6677], "temperature": 0.0, "avg_logprob": -0.07815017256625863, "compression_ratio": 1.51, "no_speech_prob": 6.204758392414078e-05}, {"id": 286, "seek": 100942, "start": 1022.42, "end": 1025.42, "text": " we're going for, and then we take this number of oversamples", "tokens": [321, 434, 516, 337, 11, 293, 550, 321, 747, 341, 1230, 295, 15488, 335, 2622], "temperature": 0.0, "avg_logprob": -0.07815017256625863, "compression_ratio": 1.51, "no_speech_prob": 6.204758392414078e-05}, {"id": 287, "seek": 102542, "start": 1025.42, "end": 1042.42, "text": " to kind of give us extra information.", "tokens": [281, 733, 295, 976, 505, 2857, 1589, 13], "temperature": 0.0, "avg_logprob": -0.09369101694652013, "compression_ratio": 1.4394904458598725, "no_speech_prob": 7.2537521191407e-05}, {"id": 288, "seek": 102542, "start": 1042.42, "end": 1045.42, "text": " So I think for, so number of components would come from", "tokens": [407, 286, 519, 337, 11, 370, 1230, 295, 6677, 576, 808, 490], "temperature": 0.0, "avg_logprob": -0.09369101694652013, "compression_ratio": 1.4394904458598725, "no_speech_prob": 7.2537521191407e-05}, {"id": 289, "seek": 102542, "start": 1045.42, "end": 1047.42, "text": " whatever problem you are working on.", "tokens": [2035, 1154, 291, 366, 1364, 322, 13], "temperature": 0.0, "avg_logprob": -0.09369101694652013, "compression_ratio": 1.4394904458598725, "no_speech_prob": 7.2537521191407e-05}, {"id": 290, "seek": 102542, "start": 1047.42, "end": 1050.42, "text": " You would need to have either some intuition", "tokens": [509, 576, 643, 281, 362, 2139, 512, 24002], "temperature": 0.0, "avg_logprob": -0.09369101694652013, "compression_ratio": 1.4394904458598725, "no_speech_prob": 7.2537521191407e-05}, {"id": 291, "seek": 102542, "start": 1050.42, "end": 1053.42, "text": " or something specifically that you're looking for.", "tokens": [420, 746, 4682, 300, 291, 434, 1237, 337, 13], "temperature": 0.0, "avg_logprob": -0.09369101694652013, "compression_ratio": 1.4394904458598725, "no_speech_prob": 7.2537521191407e-05}, {"id": 292, "seek": 105342, "start": 1053.42, "end": 1057.42, "text": " I mean, you can, sometimes you can take like, take a number,", "tokens": [286, 914, 11, 291, 393, 11, 2171, 291, 393, 747, 411, 11, 747, 257, 1230, 11], "temperature": 0.0, "avg_logprob": -0.1087766929909035, "compression_ratio": 1.71280276816609, "no_speech_prob": 3.8227735785767436e-05}, {"id": 293, "seek": 105342, "start": 1057.42, "end": 1060.42, "text": " and then we kind of did this here where we looked at what the singular values are", "tokens": [293, 550, 321, 733, 295, 630, 341, 510, 689, 321, 2956, 412, 437, 264, 20010, 4190, 366], "temperature": 0.0, "avg_logprob": -0.1087766929909035, "compression_ratio": 1.71280276816609, "no_speech_prob": 3.8227735785767436e-05}, {"id": 294, "seek": 105342, "start": 1060.42, "end": 1062.42, "text": " and see where it suddenly dips.", "tokens": [293, 536, 689, 309, 5800, 47814, 13], "temperature": 0.0, "avg_logprob": -0.1087766929909035, "compression_ratio": 1.71280276816609, "no_speech_prob": 3.8227735785767436e-05}, {"id": 295, "seek": 105342, "start": 1062.42, "end": 1066.42, "text": " And so that can be, this is looking at our error from the reconstruction.", "tokens": [400, 370, 300, 393, 312, 11, 341, 307, 1237, 412, 527, 6713, 490, 264, 31565, 13], "temperature": 0.0, "avg_logprob": -0.1087766929909035, "compression_ratio": 1.71280276816609, "no_speech_prob": 3.8227735785767436e-05}, {"id": 296, "seek": 105342, "start": 1066.42, "end": 1070.42, "text": " You could use that to be like, okay, it seems like, I don't know,", "tokens": [509, 727, 764, 300, 281, 312, 411, 11, 1392, 11, 309, 2544, 411, 11, 286, 500, 380, 458, 11], "temperature": 0.0, "avg_logprob": -0.1087766929909035, "compression_ratio": 1.71280276816609, "no_speech_prob": 3.8227735785767436e-05}, {"id": 297, "seek": 105342, "start": 1070.42, "end": 1073.42, "text": " really getting 50 components, like my error is already kind of like", "tokens": [534, 1242, 2625, 6677, 11, 411, 452, 6713, 307, 1217, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.1087766929909035, "compression_ratio": 1.71280276816609, "no_speech_prob": 3.8227735785767436e-05}, {"id": 298, "seek": 105342, "start": 1073.42, "end": 1075.42, "text": " leveled off a lot.", "tokens": [1496, 292, 766, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.1087766929909035, "compression_ratio": 1.71280276816609, "no_speech_prob": 3.8227735785767436e-05}, {"id": 299, "seek": 105342, "start": 1075.42, "end": 1077.42, "text": " So that could be one technique.", "tokens": [407, 300, 727, 312, 472, 6532, 13], "temperature": 0.0, "avg_logprob": -0.1087766929909035, "compression_ratio": 1.71280276816609, "no_speech_prob": 3.8227735785767436e-05}, {"id": 300, "seek": 105342, "start": 1077.42, "end": 1079.42, "text": " But if this was, if you were doing this for data compression,", "tokens": [583, 498, 341, 390, 11, 498, 291, 645, 884, 341, 337, 1412, 19355, 11], "temperature": 0.0, "avg_logprob": -0.1087766929909035, "compression_ratio": 1.71280276816609, "no_speech_prob": 3.8227735785767436e-05}, {"id": 301, "seek": 107942, "start": 1079.42, "end": 1083.42, "text": " you might have particular constraints about, I don't know,", "tokens": [291, 1062, 362, 1729, 18491, 466, 11, 286, 500, 380, 458, 11], "temperature": 0.0, "avg_logprob": -0.07562098697740205, "compression_ratio": 1.6111111111111112, "no_speech_prob": 4.0061557228909805e-05}, {"id": 302, "seek": 107942, "start": 1083.42, "end": 1085.42, "text": " this is what acceptable error is, or, you know,", "tokens": [341, 307, 437, 15513, 6713, 307, 11, 420, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.07562098697740205, "compression_ratio": 1.6111111111111112, "no_speech_prob": 4.0061557228909805e-05}, {"id": 303, "seek": 107942, "start": 1085.42, "end": 1089.42, "text": " this is how much space I have available that I'm trying to compress into.", "tokens": [341, 307, 577, 709, 1901, 286, 362, 2435, 300, 286, 478, 1382, 281, 14778, 666, 13], "temperature": 0.0, "avg_logprob": -0.07562098697740205, "compression_ratio": 1.6111111111111112, "no_speech_prob": 4.0061557228909805e-05}, {"id": 304, "seek": 107942, "start": 1089.42, "end": 1093.42, "text": " So kind of your problem will give you the number of components.", "tokens": [407, 733, 295, 428, 1154, 486, 976, 291, 264, 1230, 295, 6677, 13], "temperature": 0.0, "avg_logprob": -0.07562098697740205, "compression_ratio": 1.6111111111111112, "no_speech_prob": 4.0061557228909805e-05}, {"id": 305, "seek": 107942, "start": 1093.42, "end": 1099.42, "text": " And then the number of oversamples was recommended to be 10 in the paper.", "tokens": [400, 550, 264, 1230, 295, 15488, 335, 2622, 390, 9628, 281, 312, 1266, 294, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.07562098697740205, "compression_ratio": 1.6111111111111112, "no_speech_prob": 4.0061557228909805e-05}, {"id": 306, "seek": 107942, "start": 1099.42, "end": 1105.42, "text": " That's what the authors kind of found to be a good number.", "tokens": [663, 311, 437, 264, 16552, 733, 295, 1352, 281, 312, 257, 665, 1230, 13], "temperature": 0.0, "avg_logprob": -0.07562098697740205, "compression_ratio": 1.6111111111111112, "no_speech_prob": 4.0061557228909805e-05}, {"id": 307, "seek": 110542, "start": 1105.42, "end": 1110.42, "text": " Yeah, yeah, great. So I guess the randomized range finder is an algorithm", "tokens": [865, 11, 1338, 11, 869, 13, 407, 286, 2041, 264, 38513, 3613, 915, 260, 307, 364, 9284], "temperature": 0.0, "avg_logprob": -0.1360215699231183, "compression_ratio": 1.6506550218340612, "no_speech_prob": 5.4756754252593964e-05}, {"id": 308, "seek": 110542, "start": 1110.42, "end": 1114.42, "text": " that is in place that is doing the optimization for us.", "tokens": [300, 307, 294, 1081, 300, 307, 884, 264, 19618, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.1360215699231183, "compression_ratio": 1.6506550218340612, "no_speech_prob": 5.4756754252593964e-05}, {"id": 309, "seek": 110542, "start": 1114.42, "end": 1116.42, "text": " Yeah, so the randomized range finder,", "tokens": [865, 11, 370, 264, 38513, 3613, 915, 260, 11], "temperature": 0.0, "avg_logprob": -0.1360215699231183, "compression_ratio": 1.6506550218340612, "no_speech_prob": 5.4756754252593964e-05}, {"id": 310, "seek": 110542, "start": 1116.42, "end": 1118.42, "text": " so I wrote it out so we could see what was going on in it,", "tokens": [370, 286, 4114, 309, 484, 370, 321, 727, 536, 437, 390, 516, 322, 294, 309, 11], "temperature": 0.0, "avg_logprob": -0.1360215699231183, "compression_ratio": 1.6506550218340612, "no_speech_prob": 5.4756754252593964e-05}, {"id": 311, "seek": 110542, "start": 1118.42, "end": 1122.42, "text": " but it's actually implemented in SKLearn.", "tokens": [457, 309, 311, 767, 12270, 294, 21483, 11020, 1083, 13], "temperature": 0.0, "avg_logprob": -0.1360215699231183, "compression_ratio": 1.6506550218340612, "no_speech_prob": 5.4756754252593964e-05}, {"id": 312, "seek": 110542, "start": 1122.42, "end": 1127.42, "text": " And so, you know, in most cases you would probably be using,", "tokens": [400, 370, 11, 291, 458, 11, 294, 881, 3331, 291, 576, 1391, 312, 1228, 11], "temperature": 0.0, "avg_logprob": -0.1360215699231183, "compression_ratio": 1.6506550218340612, "no_speech_prob": 5.4756754252593964e-05}, {"id": 313, "seek": 110542, "start": 1127.42, "end": 1129.42, "text": " really if you're using SKLearn's randomized SVD,", "tokens": [534, 498, 291, 434, 1228, 21483, 11020, 1083, 311, 38513, 31910, 35, 11], "temperature": 0.0, "avg_logprob": -0.1360215699231183, "compression_ratio": 1.6506550218340612, "no_speech_prob": 5.4756754252593964e-05}, {"id": 314, "seek": 112942, "start": 1129.42, "end": 1137.42, "text": " that calls SKLearn's randomized range finder, which is here.", "tokens": [300, 5498, 21483, 11020, 1083, 311, 38513, 3613, 915, 260, 11, 597, 307, 510, 13], "temperature": 0.0, "avg_logprob": -0.13640792029244558, "compression_ratio": 1.455621301775148, "no_speech_prob": 1.8342385374126025e-05}, {"id": 315, "seek": 112942, "start": 1137.42, "end": 1142.42, "text": " Oh. Yeah, that's true.", "tokens": [876, 13, 865, 11, 300, 311, 2074, 13], "temperature": 0.0, "avg_logprob": -0.13640792029244558, "compression_ratio": 1.455621301775148, "no_speech_prob": 1.8342385374126025e-05}, {"id": 316, "seek": 112942, "start": 1142.42, "end": 1145.42, "text": " It's not specifically optimizing anything,", "tokens": [467, 311, 406, 4682, 40425, 1340, 11], "temperature": 0.0, "avg_logprob": -0.13640792029244558, "compression_ratio": 1.455621301775148, "no_speech_prob": 1.8342385374126025e-05}, {"id": 317, "seek": 112942, "start": 1145.42, "end": 1150.42, "text": " but it is doing this calculation for you, although it is random,", "tokens": [457, 309, 307, 884, 341, 17108, 337, 291, 11, 4878, 309, 307, 4974, 11], "temperature": 0.0, "avg_logprob": -0.13640792029244558, "compression_ratio": 1.455621301775148, "no_speech_prob": 1.8342385374126025e-05}, {"id": 318, "seek": 112942, "start": 1150.42, "end": 1155.42, "text": " and that it just, actually I can pull this up on here.", "tokens": [293, 300, 309, 445, 11, 767, 286, 393, 2235, 341, 493, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.13640792029244558, "compression_ratio": 1.455621301775148, "no_speech_prob": 1.8342385374126025e-05}, {"id": 319, "seek": 115542, "start": 1155.42, "end": 1160.42, "text": " Yeah, so here, randomized range finder,", "tokens": [865, 11, 370, 510, 11, 38513, 3613, 915, 260, 11], "temperature": 0.0, "avg_logprob": -0.10312986373901367, "compression_ratio": 1.4322580645161291, "no_speech_prob": 8.939496183302253e-06}, {"id": 320, "seek": 115542, "start": 1160.42, "end": 1163.42, "text": " and this is very similar to what I did.", "tokens": [293, 341, 307, 588, 2531, 281, 437, 286, 630, 13], "temperature": 0.0, "avg_logprob": -0.10312986373901367, "compression_ratio": 1.4322580645161291, "no_speech_prob": 8.939496183302253e-06}, {"id": 321, "seek": 115542, "start": 1163.42, "end": 1168.42, "text": " I just kind of left out a lot of the kind of like fancier conditions or checks,", "tokens": [286, 445, 733, 295, 1411, 484, 257, 688, 295, 264, 733, 295, 411, 3429, 27674, 4487, 420, 13834, 11], "temperature": 0.0, "avg_logprob": -0.10312986373901367, "compression_ratio": 1.4322580645161291, "no_speech_prob": 8.939496183302253e-06}, {"id": 322, "seek": 115542, "start": 1168.42, "end": 1180.42, "text": " but it, you know, it is just taking a bunch of random normals.", "tokens": [457, 309, 11, 291, 458, 11, 309, 307, 445, 1940, 257, 3840, 295, 4974, 2026, 1124, 13], "temperature": 0.0, "avg_logprob": -0.10312986373901367, "compression_ratio": 1.4322580645161291, "no_speech_prob": 8.939496183302253e-06}, {"id": 323, "seek": 118042, "start": 1180.42, "end": 1189.42, "text": " Yeah, so we do iterate, and here we are using math to calculate the best stats.", "tokens": [865, 11, 370, 321, 360, 44497, 11, 293, 510, 321, 366, 1228, 5221, 281, 8873, 264, 1151, 18152, 13], "temperature": 0.0, "avg_logprob": -0.15020869229290937, "compression_ratio": 1.5, "no_speech_prob": 3.2697567803552374e-05}, {"id": 324, "seek": 118042, "start": 1189.42, "end": 1194.42, "text": " No, we're not getting the best of anything here.", "tokens": [883, 11, 321, 434, 406, 1242, 264, 1151, 295, 1340, 510, 13], "temperature": 0.0, "avg_logprob": -0.15020869229290937, "compression_ratio": 1.5, "no_speech_prob": 3.2697567803552374e-05}, {"id": 325, "seek": 118042, "start": 1194.42, "end": 1198.42, "text": " Yeah, okay, I'll go to my code because that's shorter.", "tokens": [865, 11, 1392, 11, 286, 603, 352, 281, 452, 3089, 570, 300, 311, 11639, 13], "temperature": 0.0, "avg_logprob": -0.15020869229290937, "compression_ratio": 1.5, "no_speech_prob": 3.2697567803552374e-05}, {"id": 326, "seek": 118042, "start": 1198.42, "end": 1205.42, "text": " So this iteration, and this I kind of just wanted to share like an intuition about.", "tokens": [407, 341, 24784, 11, 293, 341, 286, 733, 295, 445, 1415, 281, 2073, 411, 364, 24002, 466, 13], "temperature": 0.0, "avg_logprob": -0.15020869229290937, "compression_ratio": 1.5, "no_speech_prob": 3.2697567803552374e-05}, {"id": 327, "seek": 120542, "start": 1205.42, "end": 1210.42, "text": " The idea is that, so this is kind of going back to our goal is to find a matrix", "tokens": [440, 1558, 307, 300, 11, 370, 341, 307, 733, 295, 516, 646, 281, 527, 3387, 307, 281, 915, 257, 8141], "temperature": 0.0, "avg_logprob": -0.09154511357212926, "compression_ratio": 1.7008196721311475, "no_speech_prob": 3.3213502319995314e-05}, {"id": 328, "seek": 120542, "start": 1210.42, "end": 1214.42, "text": " that has the same column space as A, our original matrix.", "tokens": [300, 575, 264, 912, 7738, 1901, 382, 316, 11, 527, 3380, 8141, 13], "temperature": 0.0, "avg_logprob": -0.09154511357212926, "compression_ratio": 1.7008196721311475, "no_speech_prob": 3.3213502319995314e-05}, {"id": 329, "seek": 120542, "start": 1214.42, "end": 1220.42, "text": " So we're trying to find something with fewer columns, but a similar column space.", "tokens": [407, 321, 434, 1382, 281, 915, 746, 365, 13366, 13766, 11, 457, 257, 2531, 7738, 1901, 13], "temperature": 0.0, "avg_logprob": -0.09154511357212926, "compression_ratio": 1.7008196721311475, "no_speech_prob": 3.3213502319995314e-05}, {"id": 330, "seek": 120542, "start": 1220.42, "end": 1228.42, "text": " And here, kind of taking powers of A kind of gets stuff that's like really in the column space of A.", "tokens": [400, 510, 11, 733, 295, 1940, 8674, 295, 316, 733, 295, 2170, 1507, 300, 311, 411, 534, 294, 264, 7738, 1901, 295, 316, 13], "temperature": 0.0, "avg_logprob": -0.09154511357212926, "compression_ratio": 1.7008196721311475, "no_speech_prob": 3.3213502319995314e-05}, {"id": 331, "seek": 120542, "start": 1228.42, "end": 1234.42, "text": " You know, if you did A times a vector, and then, you know, multiply that by A again and again,", "tokens": [509, 458, 11, 498, 291, 630, 316, 1413, 257, 8062, 11, 293, 550, 11, 291, 458, 11, 12972, 300, 538, 316, 797, 293, 797, 11], "temperature": 0.0, "avg_logprob": -0.09154511357212926, "compression_ratio": 1.7008196721311475, "no_speech_prob": 3.3213502319995314e-05}, {"id": 332, "seek": 123442, "start": 1234.42, "end": 1239.42, "text": " you're like kind of getting stuff like, okay, this is definitely in the column space of A.", "tokens": [291, 434, 411, 733, 295, 1242, 1507, 411, 11, 1392, 11, 341, 307, 2138, 294, 264, 7738, 1901, 295, 316, 13], "temperature": 0.0, "avg_logprob": -0.06188581315733546, "compression_ratio": 1.8075601374570447, "no_speech_prob": 2.885514186345972e-05}, {"id": 333, "seek": 123442, "start": 1239.42, "end": 1245.42, "text": " The issue with that is that unless A kind of has exactly the right size,", "tokens": [440, 2734, 365, 300, 307, 300, 5969, 316, 733, 295, 575, 2293, 264, 558, 2744, 11], "temperature": 0.0, "avg_logprob": -0.06188581315733546, "compression_ratio": 1.8075601374570447, "no_speech_prob": 2.885514186345972e-05}, {"id": 334, "seek": 123442, "start": 1245.42, "end": 1247.42, "text": " your answer is either going to be getting like bigger and bigger,", "tokens": [428, 1867, 307, 2139, 516, 281, 312, 1242, 411, 3801, 293, 3801, 11], "temperature": 0.0, "avg_logprob": -0.06188581315733546, "compression_ratio": 1.8075601374570447, "no_speech_prob": 2.885514186345972e-05}, {"id": 335, "seek": 123442, "start": 1247.42, "end": 1251.42, "text": " or it's going to be getting smaller and smaller, and you're going to run into numerical issues.", "tokens": [420, 309, 311, 516, 281, 312, 1242, 4356, 293, 4356, 11, 293, 291, 434, 516, 281, 1190, 666, 29054, 2663, 13], "temperature": 0.0, "avg_logprob": -0.06188581315733546, "compression_ratio": 1.8075601374570447, "no_speech_prob": 2.885514186345972e-05}, {"id": 336, "seek": 123442, "start": 1251.42, "end": 1254.42, "text": " So you can't just take a bunch of powers of A.", "tokens": [407, 291, 393, 380, 445, 747, 257, 3840, 295, 8674, 295, 316, 13], "temperature": 0.0, "avg_logprob": -0.06188581315733546, "compression_ratio": 1.8075601374570447, "no_speech_prob": 2.885514186345972e-05}, {"id": 337, "seek": 123442, "start": 1254.42, "end": 1259.42, "text": " And so what's going on here is it's basically taking a power of A, you know, A times Q,", "tokens": [400, 370, 437, 311, 516, 322, 510, 307, 309, 311, 1936, 1940, 257, 1347, 295, 316, 11, 291, 458, 11, 316, 1413, 1249, 11], "temperature": 0.0, "avg_logprob": -0.06188581315733546, "compression_ratio": 1.8075601374570447, "no_speech_prob": 2.885514186345972e-05}, {"id": 338, "seek": 123442, "start": 1259.42, "end": 1263.42, "text": " and then it's doing the LU decomposition to kind of normalize it.", "tokens": [293, 550, 309, 311, 884, 264, 31851, 48356, 281, 733, 295, 2710, 1125, 309, 13], "temperature": 0.0, "avg_logprob": -0.06188581315733546, "compression_ratio": 1.8075601374570447, "no_speech_prob": 2.885514186345972e-05}, {"id": 339, "seek": 126342, "start": 1263.42, "end": 1268.42, "text": " And a lot of, it's much more complex when you have this loop there,", "tokens": [400, 257, 688, 295, 11, 309, 311, 709, 544, 3997, 562, 291, 362, 341, 6367, 456, 11], "temperature": 0.0, "avg_logprob": -0.18275210287718646, "compression_ratio": 1.8117154811715481, "no_speech_prob": 9.313936607213691e-05}, {"id": 340, "seek": 126342, "start": 1268.42, "end": 1271.42, "text": " and a lot of the limitations don't have that loop at all.", "tokens": [293, 257, 688, 295, 264, 15705, 500, 380, 362, 300, 6367, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.18275210287718646, "compression_ratio": 1.8117154811715481, "no_speech_prob": 9.313936607213691e-05}, {"id": 341, "seek": 126342, "start": 1271.42, "end": 1275.42, "text": " So like this really, the simple version is only has the two lines of code,", "tokens": [407, 411, 341, 534, 11, 264, 2199, 3037, 307, 787, 575, 264, 732, 3876, 295, 3089, 11], "temperature": 0.0, "avg_logprob": -0.18275210287718646, "compression_ratio": 1.8117154811715481, "no_speech_prob": 9.313936607213691e-05}, {"id": 342, "seek": 126342, "start": 1275.42, "end": 1284.42, "text": " create an old random matrix, and then take the QRT composition of that random matrix times the matrix.", "tokens": [1884, 364, 1331, 4974, 8141, 11, 293, 550, 747, 264, 1249, 49, 51, 12686, 295, 300, 4974, 8141, 1413, 264, 8141, 13], "temperature": 0.0, "avg_logprob": -0.18275210287718646, "compression_ratio": 1.8117154811715481, "no_speech_prob": 9.313936607213691e-05}, {"id": 343, "seek": 126342, "start": 1284.42, "end": 1287.42, "text": " So you really want to know what this is doing,", "tokens": [407, 291, 534, 528, 281, 458, 437, 341, 307, 884, 11], "temperature": 0.0, "avg_logprob": -0.18275210287718646, "compression_ratio": 1.8117154811715481, "no_speech_prob": 9.313936607213691e-05}, {"id": 344, "seek": 126342, "start": 1287.42, "end": 1290.42, "text": " because the first line of code and the last line of code is actually all you need.", "tokens": [570, 264, 700, 1622, 295, 3089, 293, 264, 1036, 1622, 295, 3089, 307, 767, 439, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.18275210287718646, "compression_ratio": 1.8117154811715481, "no_speech_prob": 9.313936607213691e-05}, {"id": 345, "seek": 129042, "start": 1290.42, "end": 1294.42, "text": " Yeah, thank you, Jeremy. This is, yeah, you could have the number of iterations set to zero,", "tokens": [865, 11, 1309, 291, 11, 17809, 13, 639, 307, 11, 1338, 11, 291, 727, 362, 264, 1230, 295, 36540, 992, 281, 4018, 11], "temperature": 0.0, "avg_logprob": -0.11969291246854342, "compression_ratio": 1.6796536796536796, "no_speech_prob": 1.2411139323376119e-05}, {"id": 346, "seek": 129042, "start": 1294.42, "end": 1299.42, "text": " and you could just be getting a random matrix and doing A times your random matrix,", "tokens": [293, 291, 727, 445, 312, 1242, 257, 4974, 8141, 293, 884, 316, 1413, 428, 4974, 8141, 11], "temperature": 0.0, "avg_logprob": -0.11969291246854342, "compression_ratio": 1.6796536796536796, "no_speech_prob": 1.2411139323376119e-05}, {"id": 347, "seek": 129042, "start": 1299.42, "end": 1303.42, "text": " and then the QRT composition. And in this context, the QRT composition,", "tokens": [293, 550, 264, 1249, 49, 51, 12686, 13, 400, 294, 341, 4319, 11, 264, 1249, 49, 51, 12686, 11], "temperature": 0.0, "avg_logprob": -0.11969291246854342, "compression_ratio": 1.6796536796536796, "no_speech_prob": 1.2411139323376119e-05}, {"id": 348, "seek": 129042, "start": 1303.42, "end": 1305.42, "text": " actually this gets back to Tim's case.", "tokens": [767, 341, 2170, 646, 281, 7172, 311, 1389, 13], "temperature": 0.0, "avg_logprob": -0.11969291246854342, "compression_ratio": 1.6796536796536796, "no_speech_prob": 1.2411139323376119e-05}, {"id": 349, "seek": 129042, "start": 1305.42, "end": 1319.42, "text": " This is going to kind of like normalize what you're getting and make sure it's worth a normal for Q.", "tokens": [639, 307, 516, 281, 733, 295, 411, 2710, 1125, 437, 291, 434, 1242, 293, 652, 988, 309, 311, 3163, 257, 2710, 337, 1249, 13], "temperature": 0.0, "avg_logprob": -0.11969291246854342, "compression_ratio": 1.6796536796536796, "no_speech_prob": 1.2411139323376119e-05}, {"id": 350, "seek": 131942, "start": 1319.42, "end": 1322.42, "text": " Well, any other questions?", "tokens": [1042, 11, 604, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.293704468863351, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.00011234086559852585}, {"id": 351, "seek": 131942, "start": 1322.42, "end": 1328.42, "text": " And we may even revisit this again, because I know this is weird.", "tokens": [400, 321, 815, 754, 32676, 341, 797, 11, 570, 286, 458, 341, 307, 3657, 13], "temperature": 0.0, "avg_logprob": -0.293704468863351, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.00011234086559852585}, {"id": 352, "seek": 131942, "start": 1328.42, "end": 1334.42, "text": " Random matrix theory, like the idea that you can, a lot of times I say random matrix and that's it.", "tokens": [37603, 8141, 5261, 11, 411, 264, 1558, 300, 291, 393, 11, 257, 688, 295, 1413, 286, 584, 4974, 8141, 293, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.293704468863351, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.00011234086559852585}, {"id": 353, "seek": 131942, "start": 1334.42, "end": 1335.42, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.293704468863351, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.00011234086559852585}, {"id": 354, "seek": 131942, "start": 1335.42, "end": 1348.42, "text": " Weird, but yeah, that's all it does.", "tokens": [32033, 11, 457, 1338, 11, 300, 311, 439, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.293704468863351, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.00011234086559852585}, {"id": 355, "seek": 134842, "start": 1348.42, "end": 1357.42, "text": " I said random matrix theory is dropping the counter and sure it is.", "tokens": [286, 848, 4974, 8141, 5261, 307, 13601, 264, 5682, 293, 988, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.20770146630027078, "compression_ratio": 1.407185628742515, "no_speech_prob": 7.720351277384907e-05}, {"id": 356, "seek": 134842, "start": 1357.42, "end": 1366.42, "text": " But we just multiply by a matrix and it happens to work.", "tokens": [583, 321, 445, 12972, 538, 257, 8141, 293, 309, 2314, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.20770146630027078, "compression_ratio": 1.407185628742515, "no_speech_prob": 7.720351277384907e-05}, {"id": 357, "seek": 134842, "start": 1366.42, "end": 1373.42, "text": " Great. Yeah, and so on that note, I also wanted to just kind of remind you of the Johnson-Linden-Strauss lemma", "tokens": [3769, 13, 865, 11, 293, 370, 322, 300, 3637, 11, 286, 611, 1415, 281, 445, 733, 295, 4160, 291, 295, 264, 9779, 12, 43, 10291, 12, 4520, 424, 2023, 7495, 1696], "temperature": 0.0, "avg_logprob": -0.20770146630027078, "compression_ratio": 1.407185628742515, "no_speech_prob": 7.720351277384907e-05}, {"id": 358, "seek": 137342, "start": 1373.42, "end": 1379.42, "text": " that we talked about, which is that a small set of points in a high dimensional space can be embedded", "tokens": [300, 321, 2825, 466, 11, 597, 307, 300, 257, 1359, 992, 295, 2793, 294, 257, 1090, 18795, 1901, 393, 312, 16741], "temperature": 0.0, "avg_logprob": -0.046204475874311465, "compression_ratio": 1.7767441860465116, "no_speech_prob": 1.165915819001384e-05}, {"id": 359, "seek": 137342, "start": 1379.42, "end": 1385.42, "text": " into a space of much lower dimension in a way that preserves kind of the important structure of the space.", "tokens": [666, 257, 1901, 295, 709, 3126, 10139, 294, 257, 636, 300, 1183, 9054, 733, 295, 264, 1021, 3877, 295, 264, 1901, 13], "temperature": 0.0, "avg_logprob": -0.046204475874311465, "compression_ratio": 1.7767441860465116, "no_speech_prob": 1.165915819001384e-05}, {"id": 360, "seek": 137342, "start": 1385.42, "end": 1391.42, "text": " And so that's what we're doing here with taking this really wide matrix and trying to put it into a narrow", "tokens": [400, 370, 300, 311, 437, 321, 434, 884, 510, 365, 1940, 341, 534, 4874, 8141, 293, 1382, 281, 829, 309, 666, 257, 9432], "temperature": 0.0, "avg_logprob": -0.046204475874311465, "compression_ratio": 1.7767441860465116, "no_speech_prob": 1.165915819001384e-05}, {"id": 361, "seek": 137342, "start": 1391.42, "end": 1401.42, "text": " matrix that's going to have kind of the important same structures.", "tokens": [8141, 300, 311, 516, 281, 362, 733, 295, 264, 1021, 912, 9227, 13], "temperature": 0.0, "avg_logprob": -0.046204475874311465, "compression_ratio": 1.7767441860465116, "no_speech_prob": 1.165915819001384e-05}, {"id": 362, "seek": 140142, "start": 1401.42, "end": 1405.42, "text": " Then for something a little bit wider, this was really fun.", "tokens": [1396, 337, 746, 257, 707, 857, 11842, 11, 341, 390, 534, 1019, 13], "temperature": 0.0, "avg_logprob": -0.10277483540196572, "compression_ratio": 1.452513966480447, "no_speech_prob": 2.1442388970172033e-05}, {"id": 363, "seek": 140142, "start": 1405.42, "end": 1410.42, "text": " Matthew had asked last time about kind of like the history of Gaussian elimination,", "tokens": [12434, 632, 2351, 1036, 565, 466, 733, 295, 411, 264, 2503, 295, 39148, 29224, 11], "temperature": 0.0, "avg_logprob": -0.10277483540196572, "compression_ratio": 1.452513966480447, "no_speech_prob": 2.1442388970172033e-05}, {"id": 364, "seek": 140142, "start": 1410.42, "end": 1414.42, "text": " and it's a lot more fascinating than I realized.", "tokens": [293, 309, 311, 257, 688, 544, 10343, 813, 286, 5334, 13], "temperature": 0.0, "avg_logprob": -0.10277483540196572, "compression_ratio": 1.452513966480447, "no_speech_prob": 2.1442388970172033e-05}, {"id": 365, "seek": 140142, "start": 1414.42, "end": 1420.42, "text": " So the first written record of Gaussian elimination is from 200 BC.", "tokens": [407, 264, 700, 3720, 2136, 295, 39148, 29224, 307, 490, 2331, 14359, 13], "temperature": 0.0, "avg_logprob": -0.10277483540196572, "compression_ratio": 1.452513966480447, "no_speech_prob": 2.1442388970172033e-05}, {"id": 366, "seek": 142042, "start": 1420.42, "end": 1435.42, "text": " I'm going to pull this up.", "tokens": [286, 478, 516, 281, 2235, 341, 493, 13], "temperature": 0.0, "avg_logprob": -0.3263923494439376, "compression_ratio": 0.8979591836734694, "no_speech_prob": 1.045054250425892e-05}, {"id": 367, "seek": 142042, "start": 1435.42, "end": 1445.42, "text": " Let me hide this.", "tokens": [961, 385, 6479, 341, 13], "temperature": 0.0, "avg_logprob": -0.3263923494439376, "compression_ratio": 0.8979591836734694, "no_speech_prob": 1.045054250425892e-05}, {"id": 368, "seek": 144542, "start": 1445.42, "end": 1452.42, "text": " Oh, I see. Thank you. Anyways, I found this history of Gaussian elimination.", "tokens": [876, 11, 286, 536, 13, 1044, 291, 13, 15585, 11, 286, 1352, 341, 2503, 295, 39148, 29224, 13], "temperature": 0.0, "avg_logprob": -0.12658329570994659, "compression_ratio": 1.6323529411764706, "no_speech_prob": 1.0450824447616469e-05}, {"id": 369, "seek": 144542, "start": 1452.42, "end": 1457.42, "text": " And so 200 BC in the Chinese Book of Arithmetic, there's a problem.", "tokens": [400, 370, 2331, 14359, 294, 264, 4649, 9476, 295, 1587, 41179, 11, 456, 311, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.12658329570994659, "compression_ratio": 1.6323529411764706, "no_speech_prob": 1.0450824447616469e-05}, {"id": 370, "seek": 144542, "start": 1457.42, "end": 1464.42, "text": " Three sheafs of a good crop, two sheafs of a mediocre crop, and one sheaf of a bad crop are sold for 39 Dao.", "tokens": [6244, 750, 2792, 82, 295, 257, 665, 9086, 11, 732, 750, 2792, 82, 295, 257, 45415, 9086, 11, 293, 472, 750, 2792, 295, 257, 1578, 9086, 366, 3718, 337, 15238, 3933, 78, 13], "temperature": 0.0, "avg_logprob": -0.12658329570994659, "compression_ratio": 1.6323529411764706, "no_speech_prob": 1.0450824447616469e-05}, {"id": 371, "seek": 144542, "start": 1464.42, "end": 1469.42, "text": " Two sheafs of good, three mediocre, and one bad are sold for 34 Dao, and so on.", "tokens": [4453, 750, 2792, 82, 295, 665, 11, 1045, 45415, 11, 293, 472, 1578, 366, 3718, 337, 12790, 3933, 78, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.12658329570994659, "compression_ratio": 1.6323529411764706, "no_speech_prob": 1.0450824447616469e-05}, {"id": 372, "seek": 146942, "start": 1469.42, "end": 1475.42, "text": " And so this is kind of like a classic Gaussian elimination problem, so I thought that was really neat.", "tokens": [400, 370, 341, 307, 733, 295, 411, 257, 7230, 39148, 29224, 1154, 11, 370, 286, 1194, 300, 390, 534, 10654, 13], "temperature": 0.0, "avg_logprob": -0.10003898257300967, "compression_ratio": 1.6205357142857142, "no_speech_prob": 1.3210561519372277e-05}, {"id": 373, "seek": 146942, "start": 1475.42, "end": 1483.42, "text": " And then they even apparently had a system where they used different colored bamboo rods on a counting board", "tokens": [400, 550, 436, 754, 7970, 632, 257, 1185, 689, 436, 1143, 819, 14332, 26156, 32761, 322, 257, 13251, 3150], "temperature": 0.0, "avg_logprob": -0.10003898257300967, "compression_ratio": 1.6205357142857142, "no_speech_prob": 1.3210561519372277e-05}, {"id": 374, "seek": 146942, "start": 1483.42, "end": 1487.42, "text": " to do Gaussian elimination.", "tokens": [281, 360, 39148, 29224, 13], "temperature": 0.0, "avg_logprob": -0.10003898257300967, "compression_ratio": 1.6205357142857142, "no_speech_prob": 1.3210561519372277e-05}, {"id": 375, "seek": 146942, "start": 1487.42, "end": 1491.42, "text": " And so I linked to this page because I think it's really interesting.", "tokens": [400, 370, 286, 9408, 281, 341, 3028, 570, 286, 519, 309, 311, 534, 1880, 13], "temperature": 0.0, "avg_logprob": -0.10003898257300967, "compression_ratio": 1.6205357142857142, "no_speech_prob": 1.3210561519372277e-05}, {"id": 376, "seek": 146942, "start": 1491.42, "end": 1495.42, "text": " So even in those days they solved it with a computer.", "tokens": [407, 754, 294, 729, 1708, 436, 13041, 309, 365, 257, 3820, 13], "temperature": 0.0, "avg_logprob": -0.10003898257300967, "compression_ratio": 1.6205357142857142, "no_speech_prob": 1.3210561519372277e-05}, {"id": 377, "seek": 149542, "start": 1495.42, "end": 1499.42, "text": " Yeah, a much earlier computer.", "tokens": [865, 11, 257, 709, 3071, 3820, 13], "temperature": 0.0, "avg_logprob": -0.1130510730507933, "compression_ratio": 1.6127450980392157, "no_speech_prob": 3.269603257649578e-05}, {"id": 378, "seek": 149542, "start": 1499.42, "end": 1510.42, "text": " Anyway, so then it goes on that Gaussian elimination, kind of the next record of it was in Japan in the 1600s.", "tokens": [5684, 11, 370, 550, 309, 1709, 322, 300, 39148, 29224, 11, 733, 295, 264, 958, 2136, 295, 309, 390, 294, 3367, 294, 264, 36885, 82, 13], "temperature": 0.0, "avg_logprob": -0.1130510730507933, "compression_ratio": 1.6127450980392157, "no_speech_prob": 3.269603257649578e-05}, {"id": 379, "seek": 149542, "start": 1510.42, "end": 1516.42, "text": " Saki Kawa actually invented the determinant, although did not get credit for it.", "tokens": [318, 7421, 591, 10449, 767, 14479, 264, 41296, 11, 4878, 630, 406, 483, 5397, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.1130510730507933, "compression_ratio": 1.6127450980392157, "no_speech_prob": 3.269603257649578e-05}, {"id": 380, "seek": 149542, "start": 1516.42, "end": 1522.42, "text": " And around the same time Leibniz independently invented the determinant and also did not get credit for it", "tokens": [400, 926, 264, 912, 565, 1456, 897, 77, 590, 21761, 14479, 264, 41296, 293, 611, 630, 406, 483, 5397, 337, 309], "temperature": 0.0, "avg_logprob": -0.1130510730507933, "compression_ratio": 1.6127450980392157, "no_speech_prob": 3.269603257649578e-05}, {"id": 381, "seek": 152242, "start": 1522.42, "end": 1528.42, "text": " because kind of the ideas didn't take off.", "tokens": [570, 733, 295, 264, 3487, 994, 380, 747, 766, 13], "temperature": 0.0, "avg_logprob": -0.07841206224341142, "compression_ratio": 1.495049504950495, "no_speech_prob": 2.3920987587189302e-05}, {"id": 382, "seek": 152242, "start": 1528.42, "end": 1530.42, "text": " And then, okay, I thought this was neat.", "tokens": [400, 550, 11, 1392, 11, 286, 1194, 341, 390, 10654, 13], "temperature": 0.0, "avg_logprob": -0.07841206224341142, "compression_ratio": 1.495049504950495, "no_speech_prob": 2.3920987587189302e-05}, {"id": 383, "seek": 152242, "start": 1530.42, "end": 1537.42, "text": " So then Gauss described the elimination method as being commonly known.", "tokens": [407, 550, 10384, 2023, 7619, 264, 29224, 3170, 382, 885, 12719, 2570, 13], "temperature": 0.0, "avg_logprob": -0.07841206224341142, "compression_ratio": 1.495049504950495, "no_speech_prob": 2.3920987587189302e-05}, {"id": 384, "seek": 152242, "start": 1537.42, "end": 1544.42, "text": " So he was not even the first European to discover it and never claimed to have invented it.", "tokens": [407, 415, 390, 406, 754, 264, 700, 6473, 281, 4411, 309, 293, 1128, 12941, 281, 362, 14479, 309, 13], "temperature": 0.0, "avg_logprob": -0.07841206224341142, "compression_ratio": 1.495049504950495, "no_speech_prob": 2.3920987587189302e-05}, {"id": 385, "seek": 152242, "start": 1544.42, "end": 1548.42, "text": " But he did possibly invent the Cholesky decomposition,", "tokens": [583, 415, 630, 6264, 7962, 264, 761, 7456, 4133, 48356, 11], "temperature": 0.0, "avg_logprob": -0.07841206224341142, "compression_ratio": 1.495049504950495, "no_speech_prob": 2.3920987587189302e-05}, {"id": 386, "seek": 154842, "start": 1548.42, "end": 1552.42, "text": " which Cholesky did not come up with until later on, but gets credit for.", "tokens": [597, 761, 7456, 4133, 630, 406, 808, 493, 365, 1826, 1780, 322, 11, 457, 2170, 5397, 337, 13], "temperature": 0.0, "avg_logprob": -0.08001979721917046, "compression_ratio": 1.502183406113537, "no_speech_prob": 6.143696282379096e-06}, {"id": 387, "seek": 154842, "start": 1552.42, "end": 1555.42, "text": " So it's just a kind of fun bit of math history.", "tokens": [407, 309, 311, 445, 257, 733, 295, 1019, 857, 295, 5221, 2503, 13], "temperature": 0.0, "avg_logprob": -0.08001979721917046, "compression_ratio": 1.502183406113537, "no_speech_prob": 6.143696282379096e-06}, {"id": 388, "seek": 154842, "start": 1555.42, "end": 1562.42, "text": " But this, yeah, this writing kind of emphasizes like how important Gaussian elimination is", "tokens": [583, 341, 11, 1338, 11, 341, 3579, 733, 295, 48856, 411, 577, 1021, 39148, 29224, 307], "temperature": 0.0, "avg_logprob": -0.08001979721917046, "compression_ratio": 1.502183406113537, "no_speech_prob": 6.143696282379096e-06}, {"id": 389, "seek": 154842, "start": 1562.42, "end": 1568.42, "text": " and just that it's been like a problem that people care about for over 2,000 years and is still widely used.", "tokens": [293, 445, 300, 309, 311, 668, 411, 257, 1154, 300, 561, 1127, 466, 337, 670, 568, 11, 1360, 924, 293, 307, 920, 13371, 1143, 13], "temperature": 0.0, "avg_logprob": -0.08001979721917046, "compression_ratio": 1.502183406113537, "no_speech_prob": 6.143696282379096e-06}, {"id": 390, "seek": 154842, "start": 1568.42, "end": 1574.42, "text": " I thought that was fun.", "tokens": [286, 1194, 300, 390, 1019, 13], "temperature": 0.0, "avg_logprob": -0.08001979721917046, "compression_ratio": 1.502183406113537, "no_speech_prob": 6.143696282379096e-06}, {"id": 391, "seek": 157442, "start": 1574.42, "end": 1581.42, "text": " Any questions or comments about the history of Gaussian elimination?", "tokens": [2639, 1651, 420, 3053, 466, 264, 2503, 295, 39148, 29224, 30], "temperature": 0.0, "avg_logprob": -0.07124433866361292, "compression_ratio": 1.5255813953488373, "no_speech_prob": 5.4747946705901995e-05}, {"id": 392, "seek": 157442, "start": 1581.42, "end": 1588.42, "text": " Okay. So then another question that came up last time was about ways to speed up Gaussian elimination.", "tokens": [1033, 13, 407, 550, 1071, 1168, 300, 1361, 493, 1036, 565, 390, 466, 2098, 281, 3073, 493, 39148, 29224, 13], "temperature": 0.0, "avg_logprob": -0.07124433866361292, "compression_ratio": 1.5255813953488373, "no_speech_prob": 5.4747946705901995e-05}, {"id": 393, "seek": 157442, "start": 1588.42, "end": 1591.42, "text": " And I did not have time to dive into this as much as I want.", "tokens": [400, 286, 630, 406, 362, 565, 281, 9192, 666, 341, 382, 709, 382, 286, 528, 13], "temperature": 0.0, "avg_logprob": -0.07124433866361292, "compression_ratio": 1.5255813953488373, "no_speech_prob": 5.4747946705901995e-05}, {"id": 394, "seek": 157442, "start": 1591.42, "end": 1594.42, "text": " This is on my to-do list after this course ends.", "tokens": [639, 307, 322, 452, 281, 12, 2595, 1329, 934, 341, 1164, 5314, 13], "temperature": 0.0, "avg_logprob": -0.07124433866361292, "compression_ratio": 1.5255813953488373, "no_speech_prob": 5.4747946705901995e-05}, {"id": 395, "seek": 157442, "start": 1594.42, "end": 1599.42, "text": " But LUD composition can be fully parallelized.", "tokens": [583, 441, 9438, 12686, 393, 312, 4498, 8952, 1602, 13], "temperature": 0.0, "avg_logprob": -0.07124433866361292, "compression_ratio": 1.5255813953488373, "no_speech_prob": 5.4747946705901995e-05}, {"id": 396, "seek": 159942, "start": 1599.42, "end": 1606.42, "text": " And it looks like there are actually kind of multiple ways to do this.", "tokens": [400, 309, 1542, 411, 456, 366, 767, 733, 295, 3866, 2098, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.1066521303994315, "compression_ratio": 1.5243243243243243, "no_speech_prob": 3.763422500924207e-05}, {"id": 397, "seek": 159942, "start": 1606.42, "end": 1612.42, "text": " So I found this slide show.", "tokens": [407, 286, 1352, 341, 4137, 855, 13], "temperature": 0.0, "avg_logprob": -0.1066521303994315, "compression_ratio": 1.5243243243243243, "no_speech_prob": 3.763422500924207e-05}, {"id": 398, "seek": 159942, "start": 1612.42, "end": 1619.42, "text": " So kind of here they've shown what the different dependencies are and then talk about ways to parallelize that,", "tokens": [407, 733, 295, 510, 436, 600, 4898, 437, 264, 819, 36606, 366, 293, 550, 751, 466, 2098, 281, 8952, 1125, 300, 11], "temperature": 0.0, "avg_logprob": -0.1066521303994315, "compression_ratio": 1.5243243243243243, "no_speech_prob": 3.763422500924207e-05}, {"id": 399, "seek": 159942, "start": 1619.42, "end": 1627.42, "text": " including one approach is kind of to break it down into conglomeration,", "tokens": [3009, 472, 3109, 307, 733, 295, 281, 1821, 309, 760, 666, 416, 7191, 298, 5053, 11], "temperature": 0.0, "avg_logprob": -0.1066521303994315, "compression_ratio": 1.5243243243243243, "no_speech_prob": 3.763422500924207e-05}, {"id": 400, "seek": 162742, "start": 1627.42, "end": 1634.42, "text": " you know, of kind of what depends on what and like grouping those together and other tasks.", "tokens": [291, 458, 11, 295, 733, 295, 437, 5946, 322, 437, 293, 411, 40149, 729, 1214, 293, 661, 9608, 13], "temperature": 0.0, "avg_logprob": -0.12345985780682481, "compression_ratio": 1.5970695970695972, "no_speech_prob": 3.7852512377867242e-06}, {"id": 401, "seek": 162742, "start": 1634.42, "end": 1642.42, "text": " And then also this is a kind of 2016 paper that you can do a randomized LUD composition,", "tokens": [400, 550, 611, 341, 307, 257, 733, 295, 6549, 3035, 300, 291, 393, 360, 257, 38513, 441, 9438, 12686, 11], "temperature": 0.0, "avg_logprob": -0.12345985780682481, "compression_ratio": 1.5970695970695972, "no_speech_prob": 3.7852512377867242e-06}, {"id": 402, "seek": 162742, "start": 1642.42, "end": 1648.42, "text": " which lets you run it on a GPU without having to do GPU to CPU data transfer.", "tokens": [597, 6653, 291, 1190, 309, 322, 257, 18407, 1553, 1419, 281, 360, 18407, 281, 13199, 1412, 5003, 13], "temperature": 0.0, "avg_logprob": -0.12345985780682481, "compression_ratio": 1.5970695970695972, "no_speech_prob": 3.7852512377867242e-06}, {"id": 403, "seek": 162742, "start": 1648.42, "end": 1651.42, "text": " So that would be much quicker.", "tokens": [407, 300, 576, 312, 709, 16255, 13], "temperature": 0.0, "avg_logprob": -0.12345985780682481, "compression_ratio": 1.5970695970695972, "no_speech_prob": 3.7852512377867242e-06}, {"id": 404, "seek": 162742, "start": 1651.42, "end": 1653.42, "text": " So I just want to kind of let you know we're not going to go into these,", "tokens": [407, 286, 445, 528, 281, 733, 295, 718, 291, 458, 321, 434, 406, 516, 281, 352, 666, 613, 11], "temperature": 0.0, "avg_logprob": -0.12345985780682481, "compression_ratio": 1.5970695970695972, "no_speech_prob": 3.7852512377867242e-06}, {"id": 405, "seek": 162742, "start": 1653.42, "end": 1656.42, "text": " but let you know that they're out there, because I think that's exciting.", "tokens": [457, 718, 291, 458, 300, 436, 434, 484, 456, 11, 570, 286, 519, 300, 311, 4670, 13], "temperature": 0.0, "avg_logprob": -0.12345985780682481, "compression_ratio": 1.5970695970695972, "no_speech_prob": 3.7852512377867242e-06}, {"id": 406, "seek": 165642, "start": 1656.42, "end": 1665.42, "text": " And what's also interesting is this is still an area of research that people are working on.", "tokens": [400, 437, 311, 611, 1880, 307, 341, 307, 920, 364, 1859, 295, 2132, 300, 561, 366, 1364, 322, 13], "temperature": 0.0, "avg_logprob": -0.1628608565399612, "compression_ratio": 1.4431818181818181, "no_speech_prob": 8.397639248869382e-06}, {"id": 407, "seek": 165642, "start": 1665.42, "end": 1669.42, "text": " So another question that came up last time was,", "tokens": [407, 1071, 1168, 300, 1361, 493, 1036, 565, 390, 11], "temperature": 0.0, "avg_logprob": -0.1628608565399612, "compression_ratio": 1.4431818181818181, "no_speech_prob": 8.397639248869382e-06}, {"id": 408, "seek": 165642, "start": 1669.42, "end": 1675.42, "text": " Valentin discovered that using SciPy.linouge.solve,", "tokens": [17961, 259, 6941, 300, 1228, 16942, 47, 88, 13, 5045, 263, 432, 13, 30926, 303, 11], "temperature": 0.0, "avg_logprob": -0.1628608565399612, "compression_ratio": 1.4431818181818181, "no_speech_prob": 8.397639248869382e-06}, {"id": 409, "seek": 165642, "start": 1675.42, "end": 1680.42, "text": " so we had this kind of pathological matrix that we looked at.", "tokens": [370, 321, 632, 341, 733, 295, 3100, 4383, 8141, 300, 321, 2956, 412, 13], "temperature": 0.0, "avg_logprob": -0.1628608565399612, "compression_ratio": 1.4431818181818181, "no_speech_prob": 8.397639248869382e-06}, {"id": 410, "seek": 168042, "start": 1680.42, "end": 1690.42, "text": " So let me do it for a lower dimension just so you can kind of remember what it looks like.", "tokens": [407, 718, 385, 360, 309, 337, 257, 3126, 10139, 445, 370, 291, 393, 733, 295, 1604, 437, 309, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.07940833870021777, "compression_ratio": 1.6036036036036037, "no_speech_prob": 1.0783014658954926e-05}, {"id": 411, "seek": 168042, "start": 1690.42, "end": 1695.42, "text": " So this matrix that was ones along the diagonal, ones in the last column,", "tokens": [407, 341, 8141, 300, 390, 2306, 2051, 264, 21539, 11, 2306, 294, 264, 1036, 7738, 11], "temperature": 0.0, "avg_logprob": -0.07940833870021777, "compression_ratio": 1.6036036036036037, "no_speech_prob": 1.0783014658954926e-05}, {"id": 412, "seek": 168042, "start": 1695.42, "end": 1700.42, "text": " negative ones in the lower triangle.", "tokens": [3671, 2306, 294, 264, 3126, 13369, 13], "temperature": 0.0, "avg_logprob": -0.07940833870021777, "compression_ratio": 1.6036036036036037, "no_speech_prob": 1.0783014658954926e-05}, {"id": 413, "seek": 168042, "start": 1700.42, "end": 1704.42, "text": " And you may remember that when we did Gaussian elimination by hand on that,", "tokens": [400, 291, 815, 1604, 300, 562, 321, 630, 39148, 29224, 538, 1011, 322, 300, 11], "temperature": 0.0, "avg_logprob": -0.07940833870021777, "compression_ratio": 1.6036036036036037, "no_speech_prob": 1.0783014658954926e-05}, {"id": 414, "seek": 168042, "start": 1704.42, "end": 1709.42, "text": " basically what ends up happening is that you're kind of doubling at every row.", "tokens": [1936, 437, 5314, 493, 2737, 307, 300, 291, 434, 733, 295, 33651, 412, 633, 5386, 13], "temperature": 0.0, "avg_logprob": -0.07940833870021777, "compression_ratio": 1.6036036036036037, "no_speech_prob": 1.0783014658954926e-05}, {"id": 415, "seek": 170942, "start": 1709.42, "end": 1712.42, "text": " So you end up with this final column of ones,", "tokens": [407, 291, 917, 493, 365, 341, 2572, 7738, 295, 2306, 11], "temperature": 0.0, "avg_logprob": -0.17000836592454177, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.705152994371019e-05}, {"id": 416, "seek": 170942, "start": 1712.42, "end": 1716.42, "text": " and so you end up with these powers of two that's getting very large.", "tokens": [293, 370, 291, 917, 493, 365, 613, 8674, 295, 732, 300, 311, 1242, 588, 2416, 13], "temperature": 0.0, "avg_logprob": -0.17000836592454177, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.705152994371019e-05}, {"id": 417, "seek": 170942, "start": 1716.42, "end": 1721.42, "text": " And so we get the wrong answer.", "tokens": [400, 370, 321, 483, 264, 2085, 1867, 13], "temperature": 0.0, "avg_logprob": -0.17000836592454177, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.705152994371019e-05}, {"id": 418, "seek": 170942, "start": 1721.42, "end": 1730.42, "text": " We get a wrong answer when we do use lusolve in SciPy for that.", "tokens": [492, 483, 257, 2085, 1867, 562, 321, 360, 764, 287, 301, 37361, 294, 16942, 47, 88, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.17000836592454177, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.705152994371019e-05}, {"id": 419, "seek": 170942, "start": 1730.42, "end": 1733.42, "text": " However, using.solve was getting the right answer.", "tokens": [2908, 11, 1228, 2411, 30926, 303, 390, 1242, 264, 558, 1867, 13], "temperature": 0.0, "avg_logprob": -0.17000836592454177, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.705152994371019e-05}, {"id": 420, "seek": 173342, "start": 1733.42, "end": 1739.42, "text": " So we're getting into that more.", "tokens": [407, 321, 434, 1242, 666, 300, 544, 13], "temperature": 0.0, "avg_logprob": -0.1348708947499593, "compression_ratio": 1.6793893129770991, "no_speech_prob": 1.3419846254691947e-05}, {"id": 421, "seek": 173342, "start": 1739.42, "end": 1743.42, "text": " And this is not a significant difference, but lusolve was quicker.", "tokens": [400, 341, 307, 406, 257, 4776, 2649, 11, 457, 287, 301, 37361, 390, 16255, 13], "temperature": 0.0, "avg_logprob": -0.1348708947499593, "compression_ratio": 1.6793893129770991, "no_speech_prob": 1.3419846254691947e-05}, {"id": 422, "seek": 173342, "start": 1743.42, "end": 1746.42, "text": " This is SciPy's implementation of lusolve.", "tokens": [639, 307, 16942, 47, 88, 311, 11420, 295, 287, 301, 37361, 13], "temperature": 0.0, "avg_logprob": -0.1348708947499593, "compression_ratio": 1.6793893129770991, "no_speech_prob": 1.3419846254691947e-05}, {"id": 423, "seek": 173342, "start": 1746.42, "end": 1751.42, "text": " And so I can imagine if you're doing a ton that there may be advantages there,", "tokens": [400, 370, 286, 393, 3811, 498, 291, 434, 884, 257, 2952, 300, 456, 815, 312, 14906, 456, 11], "temperature": 0.0, "avg_logprob": -0.1348708947499593, "compression_ratio": 1.6793893129770991, "no_speech_prob": 1.3419846254691947e-05}, {"id": 424, "seek": 173342, "start": 1751.42, "end": 1753.42, "text": " particularly because if you were doing this a lot of times,", "tokens": [4098, 570, 498, 291, 645, 884, 341, 257, 688, 295, 1413, 11], "temperature": 0.0, "avg_logprob": -0.1348708947499593, "compression_ratio": 1.6793893129770991, "no_speech_prob": 1.3419846254691947e-05}, {"id": 425, "seek": 173342, "start": 1753.42, "end": 1756.42, "text": " you only have to do the lu factorization once of A.", "tokens": [291, 787, 362, 281, 360, 264, 287, 84, 5952, 2144, 1564, 295, 316, 13], "temperature": 0.0, "avg_logprob": -0.1348708947499593, "compression_ratio": 1.6793893129770991, "no_speech_prob": 1.3419846254691947e-05}, {"id": 426, "seek": 173342, "start": 1756.42, "end": 1759.42, "text": " If you have like a lot of different columns B you're solving for,", "tokens": [759, 291, 362, 411, 257, 688, 295, 819, 13766, 363, 291, 434, 12606, 337, 11], "temperature": 0.0, "avg_logprob": -0.1348708947499593, "compression_ratio": 1.6793893129770991, "no_speech_prob": 1.3419846254691947e-05}, {"id": 427, "seek": 173342, "start": 1759.42, "end": 1762.42, "text": " you can kind of save that decomposition.", "tokens": [291, 393, 733, 295, 3155, 300, 48356, 13], "temperature": 0.0, "avg_logprob": -0.1348708947499593, "compression_ratio": 1.6793893129770991, "no_speech_prob": 1.3419846254691947e-05}, {"id": 428, "seek": 176242, "start": 1762.42, "end": 1765.42, "text": " But looking at SciPy's lin-out solve,", "tokens": [583, 1237, 412, 16942, 47, 88, 311, 22896, 12, 346, 5039, 11], "temperature": 0.0, "avg_logprob": -0.21413267453511556, "compression_ratio": 1.273972602739726, "no_speech_prob": 1.8631450075190514e-05}, {"id": 429, "seek": 176242, "start": 1765.42, "end": 1770.42, "text": " I looked at the Fortran source code for what it's calling in LawPack.", "tokens": [286, 2956, 412, 264, 11002, 4257, 4009, 3089, 337, 437, 309, 311, 5141, 294, 7744, 47, 501, 13], "temperature": 0.0, "avg_logprob": -0.21413267453511556, "compression_ratio": 1.273972602739726, "no_speech_prob": 1.8631450075190514e-05}, {"id": 430, "seek": 176242, "start": 1770.42, "end": 1783.42, "text": " And in the comments, actually I'll pull this up.", "tokens": [400, 294, 264, 3053, 11, 767, 286, 603, 2235, 341, 493, 13], "temperature": 0.0, "avg_logprob": -0.21413267453511556, "compression_ratio": 1.273972602739726, "no_speech_prob": 1.8631450075190514e-05}, {"id": 431, "seek": 176242, "start": 1783.42, "end": 1791.42, "text": " Let's get to the right place.", "tokens": [961, 311, 483, 281, 264, 558, 1081, 13], "temperature": 0.0, "avg_logprob": -0.21413267453511556, "compression_ratio": 1.273972602739726, "no_speech_prob": 1.8631450075190514e-05}, {"id": 432, "seek": 179142, "start": 1791.42, "end": 1798.42, "text": " I mentioned several times that it's looking at the reciprocal pivot growth factor.", "tokens": [286, 2835, 2940, 1413, 300, 309, 311, 1237, 412, 264, 46948, 14538, 4599, 5952, 13], "temperature": 0.0, "avg_logprob": -0.1608028659572849, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.0001022905416903086}, {"id": 433, "seek": 179142, "start": 1798.42, "end": 1807.42, "text": " And so if you'll remember, we defined this term row back in the notebook.", "tokens": [400, 370, 498, 291, 603, 1604, 11, 321, 7642, 341, 1433, 5386, 646, 294, 264, 21060, 13], "temperature": 0.0, "avg_logprob": -0.1608028659572849, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.0001022905416903086}, {"id": 434, "seek": 179142, "start": 1807.42, "end": 1810.42, "text": " Row was the growth factor.", "tokens": [20309, 390, 264, 4599, 5952, 13], "temperature": 0.0, "avg_logprob": -0.1608028659572849, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.0001022905416903086}, {"id": 435, "seek": 179142, "start": 1810.42, "end": 1815.42, "text": " And that's kind of the greatest ratio of the AI over UI.", "tokens": [400, 300, 311, 733, 295, 264, 6636, 8509, 295, 264, 7318, 670, 15682, 13], "temperature": 0.0, "avg_logprob": -0.1608028659572849, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.0001022905416903086}, {"id": 436, "seek": 179142, "start": 1815.42, "end": 1819.42, "text": " And in this case, it's 2 to the 59th.", "tokens": [400, 294, 341, 1389, 11, 309, 311, 568, 281, 264, 24624, 392, 13], "temperature": 0.0, "avg_logprob": -0.1608028659572849, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.0001022905416903086}, {"id": 437, "seek": 181942, "start": 1819.42, "end": 1824.42, "text": " This is where n equals 60. So it's 2 to the n minus 1.", "tokens": [639, 307, 689, 297, 6915, 4060, 13, 407, 309, 311, 568, 281, 264, 297, 3175, 502, 13], "temperature": 0.0, "avg_logprob": -0.1031137251518142, "compression_ratio": 1.36, "no_speech_prob": 1.5689256542827934e-05}, {"id": 438, "seek": 181942, "start": 1824.42, "end": 1829.42, "text": " So the LawPack's method is actually taking that into account,", "tokens": [407, 264, 7744, 47, 501, 311, 3170, 307, 767, 1940, 300, 666, 2696, 11], "temperature": 0.0, "avg_logprob": -0.1031137251518142, "compression_ratio": 1.36, "no_speech_prob": 1.5689256542827934e-05}, {"id": 439, "seek": 181942, "start": 1829.42, "end": 1838.42, "text": " whereas straight lusolve is not.", "tokens": [9735, 2997, 287, 301, 37361, 307, 406, 13], "temperature": 0.0, "avg_logprob": -0.1031137251518142, "compression_ratio": 1.36, "no_speech_prob": 1.5689256542827934e-05}, {"id": 440, "seek": 181942, "start": 1838.42, "end": 1844.42, "text": " And I think it can be, like you probably don't want to go too far down this rabbit hole,", "tokens": [400, 286, 519, 309, 393, 312, 11, 411, 291, 1391, 500, 380, 528, 281, 352, 886, 1400, 760, 341, 19509, 5458, 11], "temperature": 0.0, "avg_logprob": -0.1031137251518142, "compression_ratio": 1.36, "no_speech_prob": 1.5689256542827934e-05}, {"id": 441, "seek": 184442, "start": 1844.42, "end": 1849.42, "text": " but I think it can be interesting to kind of look up the LawPack methods that are being called", "tokens": [457, 286, 519, 309, 393, 312, 1880, 281, 733, 295, 574, 493, 264, 7744, 47, 501, 7150, 300, 366, 885, 1219], "temperature": 0.0, "avg_logprob": -0.08600146429879325, "compression_ratio": 1.3576158940397351, "no_speech_prob": 6.33898844171199e-06}, {"id": 442, "seek": 184442, "start": 1849.42, "end": 1856.42, "text": " by a particular kind of SciPy method just to get an idea of what it's doing.", "tokens": [538, 257, 1729, 733, 295, 16942, 47, 88, 3170, 445, 281, 483, 364, 1558, 295, 437, 309, 311, 884, 13], "temperature": 0.0, "avg_logprob": -0.08600146429879325, "compression_ratio": 1.3576158940397351, "no_speech_prob": 6.33898844171199e-06}, {"id": 443, "seek": 184442, "start": 1856.42, "end": 1866.42, "text": " Any questions about that? Kelsey?", "tokens": [2639, 1651, 466, 300, 30, 44714, 30], "temperature": 0.0, "avg_logprob": -0.08600146429879325, "compression_ratio": 1.3576158940397351, "no_speech_prob": 6.33898844171199e-06}, {"id": 444, "seek": 186642, "start": 1866.42, "end": 1875.42, "text": " I'm just about the row pivoting. I still don't think I've heard any intuition for what makes the decorations", "tokens": [286, 478, 445, 466, 264, 5386, 14538, 278, 13, 286, 920, 500, 380, 519, 286, 600, 2198, 604, 24002, 337, 437, 1669, 264, 32367], "temperature": 0.0, "avg_logprob": -0.3744955997841031, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.4969422409194522e-05}, {"id": 445, "seek": 186642, "start": 1875.42, "end": 1881.42, "text": " or what property of the row or makes it explode.", "tokens": [420, 437, 4707, 295, 264, 5386, 420, 1669, 309, 21411, 13], "temperature": 0.0, "avg_logprob": -0.3744955997841031, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.4969422409194522e-05}, {"id": 446, "seek": 186642, "start": 1881.42, "end": 1887.42, "text": " What property of, so when you say row pivoting, you mean when we're shuffling the rows, but not the columns?", "tokens": [708, 4707, 295, 11, 370, 562, 291, 584, 5386, 14538, 278, 11, 291, 914, 562, 321, 434, 402, 1245, 1688, 264, 13241, 11, 457, 406, 264, 13766, 30], "temperature": 0.0, "avg_logprob": -0.3744955997841031, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.4969422409194522e-05}, {"id": 447, "seek": 186642, "start": 1887.42, "end": 1893.42, "text": " Yeah, not the path of the pathological case of the columns. That one I can see because it's sort of simple in a way.", "tokens": [865, 11, 406, 264, 3100, 295, 264, 3100, 4383, 1389, 295, 264, 13766, 13, 663, 472, 286, 393, 536, 570, 309, 311, 1333, 295, 2199, 294, 257, 636, 13], "temperature": 0.0, "avg_logprob": -0.3744955997841031, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.4969422409194522e-05}, {"id": 448, "seek": 189342, "start": 1893.42, "end": 1899.42, "text": " Yeah. So we saw when we weren't shuffling the rows, let me go back to this,", "tokens": [865, 13, 407, 321, 1866, 562, 321, 4999, 380, 402, 1245, 1688, 264, 13241, 11, 718, 385, 352, 646, 281, 341, 11], "temperature": 0.0, "avg_logprob": -0.12130045209612166, "compression_ratio": 1.4904458598726114, "no_speech_prob": 6.339074843708659e-06}, {"id": 449, "seek": 189342, "start": 1899.42, "end": 1904.42, "text": " we had that example with 10 to the negative 20th.", "tokens": [321, 632, 300, 1365, 365, 1266, 281, 264, 3671, 945, 392, 13], "temperature": 0.0, "avg_logprob": -0.12130045209612166, "compression_ratio": 1.4904458598726114, "no_speech_prob": 6.339074843708659e-06}, {"id": 450, "seek": 189342, "start": 1904.42, "end": 1909.42, "text": " Find it.", "tokens": [11809, 309, 13], "temperature": 0.0, "avg_logprob": -0.12130045209612166, "compression_ratio": 1.4904458598726114, "no_speech_prob": 6.339074843708659e-06}, {"id": 451, "seek": 189342, "start": 1909.42, "end": 1917.42, "text": " Where we looked at this matrix, 10 to the negative 20th, and here we had problems because we hadn't", "tokens": [2305, 321, 2956, 412, 341, 8141, 11, 1266, 281, 264, 3671, 945, 392, 11, 293, 510, 321, 632, 2740, 570, 321, 8782, 380], "temperature": 0.0, "avg_logprob": -0.12130045209612166, "compression_ratio": 1.4904458598726114, "no_speech_prob": 6.339074843708659e-06}, {"id": 452, "seek": 191742, "start": 1917.42, "end": 1924.42, "text": " hadn't shuffled the row order. And what's happening here is that you're", "tokens": [8782, 380, 402, 33974, 264, 5386, 1668, 13, 400, 437, 311, 2737, 510, 307, 300, 291, 434], "temperature": 0.0, "avg_logprob": -0.09316547195632736, "compression_ratio": 1.6222222222222222, "no_speech_prob": 1.6797193893580697e-06}, {"id": 453, "seek": 191742, "start": 1924.42, "end": 1932.42, "text": " I guess having to multiply by 10 to the 20th in order to, no, you have to multiply by 10 to the negative 20th to", "tokens": [286, 2041, 1419, 281, 12972, 538, 1266, 281, 264, 945, 392, 294, 1668, 281, 11, 572, 11, 291, 362, 281, 12972, 538, 1266, 281, 264, 3671, 945, 392, 281], "temperature": 0.0, "avg_logprob": -0.09316547195632736, "compression_ratio": 1.6222222222222222, "no_speech_prob": 1.6797193893580697e-06}, {"id": 454, "seek": 191742, "start": 1932.42, "end": 1941.42, "text": " zero out this one on the next row. And kind of multiplying by this really tiny number is setting you up for", "tokens": [4018, 484, 341, 472, 322, 264, 958, 5386, 13, 400, 733, 295, 30955, 538, 341, 534, 5870, 1230, 307, 3287, 291, 493, 337], "temperature": 0.0, "avg_logprob": -0.09316547195632736, "compression_ratio": 1.6222222222222222, "no_speech_prob": 1.6797193893580697e-06}, {"id": 455, "seek": 194142, "start": 1941.42, "end": 1947.42, "text": " these kind of like potential underflow errors, which is kind of what happens here.", "tokens": [613, 733, 295, 411, 3995, 833, 10565, 13603, 11, 597, 307, 733, 295, 437, 2314, 510, 13], "temperature": 0.0, "avg_logprob": -0.15167654885186088, "compression_ratio": 1.389261744966443, "no_speech_prob": 3.2887107863643905e-06}, {"id": 456, "seek": 194142, "start": 1947.42, "end": 1956.42, "text": " That like in general, yeah, you don't want to have to be multiplying by tiny numbers.", "tokens": [663, 411, 294, 2674, 11, 1338, 11, 291, 500, 380, 528, 281, 362, 281, 312, 30955, 538, 5870, 3547, 13], "temperature": 0.0, "avg_logprob": -0.15167654885186088, "compression_ratio": 1.389261744966443, "no_speech_prob": 3.2887107863643905e-06}, {"id": 457, "seek": 194142, "start": 1956.42, "end": 1962.42, "text": " And so what switching did is so we had", "tokens": [400, 370, 437, 16493, 630, 307, 370, 321, 632], "temperature": 0.0, "avg_logprob": -0.15167654885186088, "compression_ratio": 1.389261744966443, "no_speech_prob": 3.2887107863643905e-06}, {"id": 458, "seek": 196242, "start": 1962.42, "end": 1977.42, "text": " did I pass it? Yeah.", "tokens": [630, 286, 1320, 309, 30, 865, 13], "temperature": 0.0, "avg_logprob": -0.15033350672040666, "compression_ratio": 1.0625, "no_speech_prob": 2.7693799893313553e-06}, {"id": 459, "seek": 196242, "start": 1977.42, "end": 1980.42, "text": " OK, so then we had this other matrix where we had switched them.", "tokens": [2264, 11, 370, 550, 321, 632, 341, 661, 8141, 689, 321, 632, 16858, 552, 13], "temperature": 0.0, "avg_logprob": -0.15033350672040666, "compression_ratio": 1.0625, "no_speech_prob": 2.7693799893313553e-06}, {"id": 460, "seek": 198042, "start": 1980.42, "end": 1995.42, "text": " And here when we calculate the LUD composition, actually I can change it.", "tokens": [400, 510, 562, 321, 8873, 264, 441, 9438, 12686, 11, 767, 286, 393, 1319, 309, 13], "temperature": 0.0, "avg_logprob": -0.17950918674468994, "compression_ratio": 0.948051948051948, "no_speech_prob": 5.954920197837055e-06}, {"id": 461, "seek": 199542, "start": 1995.42, "end": 2011.42, "text": " I guess like the fact that we multiply by 10 to the negative 20th and cancel those out.", "tokens": [286, 2041, 411, 264, 1186, 300, 321, 12972, 538, 1266, 281, 264, 3671, 945, 392, 293, 10373, 729, 484, 13], "temperature": 0.0, "avg_logprob": -0.11211638701589484, "compression_ratio": 1.209090909090909, "no_speech_prob": 9.222879270964768e-06}, {"id": 462, "seek": 199542, "start": 2011.42, "end": 2016.42, "text": " If you're multiplying by something large.", "tokens": [759, 291, 434, 30955, 538, 746, 2416, 13], "temperature": 0.0, "avg_logprob": -0.11211638701589484, "compression_ratio": 1.209090909090909, "no_speech_prob": 9.222879270964768e-06}, {"id": 463, "seek": 199542, "start": 2016.42, "end": 2019.42, "text": " So.", "tokens": [407, 13], "temperature": 0.0, "avg_logprob": -0.11211638701589484, "compression_ratio": 1.209090909090909, "no_speech_prob": 9.222879270964768e-06}, {"id": 464, "seek": 201942, "start": 2019.42, "end": 2025.42, "text": " So the good question, so like part of the issue, I don't think this is the heart of it, is that like over here you end up with", "tokens": [407, 264, 665, 1168, 11, 370, 411, 644, 295, 264, 2734, 11, 286, 500, 380, 519, 341, 307, 264, 1917, 295, 309, 11, 307, 300, 411, 670, 510, 291, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.13398968605768113, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.5206267562462017e-05}, {"id": 465, "seek": 201942, "start": 2025.42, "end": 2030.42, "text": " one minus 10 to the negative 20th, which rounds to one.", "tokens": [472, 3175, 1266, 281, 264, 3671, 945, 392, 11, 597, 13757, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.13398968605768113, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.5206267562462017e-05}, {"id": 466, "seek": 201942, "start": 2030.42, "end": 2036.42, "text": " And that's that's an OK way to round.", "tokens": [400, 300, 311, 300, 311, 364, 2264, 636, 281, 3098, 13], "temperature": 0.0, "avg_logprob": -0.13398968605768113, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.5206267562462017e-05}, {"id": 467, "seek": 201942, "start": 2036.42, "end": 2043.42, "text": " Let me let me do this one out by hand. I think might be helpful.", "tokens": [961, 385, 718, 385, 360, 341, 472, 484, 538, 1011, 13, 286, 519, 1062, 312, 4961, 13], "temperature": 0.0, "avg_logprob": -0.13398968605768113, "compression_ratio": 1.532258064516129, "no_speech_prob": 1.5206267562462017e-05}, {"id": 468, "seek": 204342, "start": 2043.42, "end": 2058.42, "text": " Kind of like why the negative.", "tokens": [9242, 295, 411, 983, 264, 3671, 13], "temperature": 0.0, "avg_logprob": -0.18967056274414062, "compression_ratio": 0.7894736842105263, "no_speech_prob": 1.0952848242595792e-05}, {"id": 469, "seek": 205842, "start": 2058.42, "end": 2077.42, "text": " Why the one case breaks down. So in this a we had 10 to the negative 20th is the first one.", "tokens": [1545, 264, 472, 1389, 9857, 760, 13, 407, 294, 341, 257, 321, 632, 1266, 281, 264, 3671, 945, 392, 307, 264, 700, 472, 13], "temperature": 0.0, "avg_logprob": -0.15243000643593924, "compression_ratio": 1.0963855421686748, "no_speech_prob": 4.2892534111160785e-06}, {"id": 470, "seek": 207742, "start": 2077.42, "end": 2090.42, "text": " 20th, one, one, one.", "tokens": [945, 392, 11, 472, 11, 472, 11, 472, 13], "temperature": 0.0, "avg_logprob": -0.12785367042787613, "compression_ratio": 1.3680555555555556, "no_speech_prob": 1.7603078958927654e-06}, {"id": 471, "seek": 207742, "start": 2090.42, "end": 2096.42, "text": " Oh, you know what? The other issue is going to be that we're like leaving this as a pivot.", "tokens": [876, 11, 291, 458, 437, 30, 440, 661, 2734, 307, 516, 281, 312, 300, 321, 434, 411, 5012, 341, 382, 257, 14538, 13], "temperature": 0.0, "avg_logprob": -0.12785367042787613, "compression_ratio": 1.3680555555555556, "no_speech_prob": 1.7603078958927654e-06}, {"id": 472, "seek": 207742, "start": 2096.42, "end": 2101.42, "text": " So later on, we're going to have to come back and multiply through by 10 to the 20th,", "tokens": [407, 1780, 322, 11, 321, 434, 516, 281, 362, 281, 808, 646, 293, 12972, 807, 538, 1266, 281, 264, 945, 392, 11], "temperature": 0.0, "avg_logprob": -0.12785367042787613, "compression_ratio": 1.3680555555555556, "no_speech_prob": 1.7603078958927654e-06}, {"id": 473, "seek": 210142, "start": 2101.42, "end": 2120.42, "text": " which doesn't seem like. Seem like a good idea.", "tokens": [597, 1177, 380, 1643, 411, 13, 1100, 443, 411, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.15930408589980183, "compression_ratio": 0.9791666666666666, "no_speech_prob": 3.237683358747745e-06}, {"id": 474, "seek": 212042, "start": 2120.42, "end": 2132.42, "text": " And actually, even down, so that would mean L would be. It's negative. 10 to the negative 20th.", "tokens": [400, 767, 11, 754, 760, 11, 370, 300, 576, 914, 441, 576, 312, 13, 467, 311, 3671, 13, 1266, 281, 264, 3671, 945, 392, 13], "temperature": 0.0, "avg_logprob": -0.2436690330505371, "compression_ratio": 1.2325581395348837, "no_speech_prob": 2.4439339085802203e-06}, {"id": 475, "seek": 212042, "start": 2132.42, "end": 2142.42, "text": " One, one, zero.", "tokens": [1485, 11, 472, 11, 4018, 13], "temperature": 0.0, "avg_logprob": -0.2436690330505371, "compression_ratio": 1.2325581395348837, "no_speech_prob": 2.4439339085802203e-06}, {"id": 476, "seek": 212042, "start": 2142.42, "end": 2148.42, "text": " Well, aren't we? We're multiplying this one by.", "tokens": [1042, 11, 3212, 380, 321, 30, 492, 434, 30955, 341, 472, 538, 13], "temperature": 0.0, "avg_logprob": -0.2436690330505371, "compression_ratio": 1.2325581395348837, "no_speech_prob": 2.4439339085802203e-06}, {"id": 477, "seek": 214842, "start": 2148.42, "end": 2156.42, "text": " Oh, I see. Right. We're multiplying. Yeah. Thank you. Thanks, Tim.", "tokens": [876, 11, 286, 536, 13, 1779, 13, 492, 434, 30955, 13, 865, 13, 1044, 291, 13, 2561, 11, 7172, 13], "temperature": 0.0, "avg_logprob": -0.11936422189076741, "compression_ratio": 1.2478632478632479, "no_speech_prob": 5.594180947809946e-06}, {"id": 478, "seek": 214842, "start": 2156.42, "end": 2162.42, "text": " Oh, because the subtraction is implicit. Yes. Thank you. That's 10 to the 20th.", "tokens": [876, 11, 570, 264, 16390, 313, 307, 26947, 13, 1079, 13, 1044, 291, 13, 663, 311, 1266, 281, 264, 945, 392, 13], "temperature": 0.0, "avg_logprob": -0.11936422189076741, "compression_ratio": 1.2478632478632479, "no_speech_prob": 5.594180947809946e-06}, {"id": 479, "seek": 216242, "start": 2162.42, "end": 2191.42, "text": " So this was a in the case with a hat. We're getting or starting with one, one. 10 to the negative 20th one.", "tokens": [407, 341, 390, 257, 294, 264, 1389, 365, 257, 2385, 13, 492, 434, 1242, 420, 2891, 365, 472, 11, 472, 13, 1266, 281, 264, 3671, 945, 392, 472, 13], "temperature": 0.0, "avg_logprob": -0.27839178027528705, "compression_ratio": 1.202247191011236, "no_speech_prob": 1.1124536285933573e-05}, {"id": 480, "seek": 219142, "start": 2191.42, "end": 2203.42, "text": " Is this entry correct? The one minus 10 to the negative 20th. That should be.", "tokens": [1119, 341, 8729, 3006, 30, 440, 472, 3175, 1266, 281, 264, 3671, 945, 392, 13, 663, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.2560901227204696, "compression_ratio": 0.9871794871794872, "no_speech_prob": 7.76652723288862e-06}, {"id": 481, "seek": 220342, "start": 2203.42, "end": 2226.42, "text": " Right. So we're multiplying by 10 to the 20th. Yeah. Plus one.", "tokens": [1779, 13, 407, 321, 434, 30955, 538, 1266, 281, 264, 945, 392, 13, 865, 13, 7721, 472, 13], "temperature": 0.0, "avg_logprob": -0.14497971534729004, "compression_ratio": 0.9117647058823529, "no_speech_prob": 1.451006482966477e-05}, {"id": 482, "seek": 222642, "start": 2226.42, "end": 2241.42, "text": " One minus. Thank you. And then this one will be one minus 10 to the negative.", "tokens": [1485, 3175, 13, 1044, 291, 13, 400, 550, 341, 472, 486, 312, 472, 3175, 1266, 281, 264, 3671, 13], "temperature": 0.0, "avg_logprob": -0.18383421216692244, "compression_ratio": 1.2479338842975207, "no_speech_prob": 1.4063125490793027e-05}, {"id": 483, "seek": 222642, "start": 2241.42, "end": 2249.42, "text": " I guess. OK. So in this. Oh, it's OK. In the first day, the unstable one,", "tokens": [286, 2041, 13, 2264, 13, 407, 294, 341, 13, 876, 11, 309, 311, 2264, 13, 682, 264, 700, 786, 11, 264, 23742, 472, 11], "temperature": 0.0, "avg_logprob": -0.18383421216692244, "compression_ratio": 1.2479338842975207, "no_speech_prob": 1.4063125490793027e-05}, {"id": 484, "seek": 224942, "start": 2249.42, "end": 2256.42, "text": " we're ending up with pivots that are like we have one pivot that's like tiny, like dangerously tiny.", "tokens": [321, 434, 8121, 493, 365, 280, 592, 1971, 300, 366, 411, 321, 362, 472, 14538, 300, 311, 411, 5870, 11, 411, 4330, 5098, 5870, 13], "temperature": 0.0, "avg_logprob": -0.05627784337082954, "compression_ratio": 1.5965909090909092, "no_speech_prob": 1.0129449037776794e-05}, {"id": 485, "seek": 224942, "start": 2256.42, "end": 2263.42, "text": " And then our other pivot is dangerously huge, which is kind of like the worst of both worlds.", "tokens": [400, 550, 527, 661, 14538, 307, 4330, 5098, 2603, 11, 597, 307, 733, 295, 411, 264, 5855, 295, 1293, 13401, 13], "temperature": 0.0, "avg_logprob": -0.05627784337082954, "compression_ratio": 1.5965909090909092, "no_speech_prob": 1.0129449037776794e-05}, {"id": 486, "seek": 224942, "start": 2263.42, "end": 2270.42, "text": " Whereas when we've switched them, actually, both these pivots are good because they're", "tokens": [13813, 562, 321, 600, 16858, 552, 11, 767, 11, 1293, 613, 280, 592, 1971, 366, 665, 570, 436, 434], "temperature": 0.0, "avg_logprob": -0.05627784337082954, "compression_ratio": 1.5965909090909092, "no_speech_prob": 1.0129449037776794e-05}, {"id": 487, "seek": 227042, "start": 2270.42, "end": 2279.42, "text": " kind of like one to one kind of much more reasonable numbers. So having having this 10 to the negative 20th as a pivot,", "tokens": [733, 295, 411, 472, 281, 472, 733, 295, 709, 544, 10585, 3547, 13, 407, 1419, 1419, 341, 1266, 281, 264, 3671, 945, 392, 382, 257, 14538, 11], "temperature": 0.0, "avg_logprob": -0.20341368419368092, "compression_ratio": 1.5665024630541873, "no_speech_prob": 2.090410134769627e-06}, {"id": 488, "seek": 227042, "start": 2279.42, "end": 2288.42, "text": " I think, is the issue, because that's what then kind of led to us getting, yeah, huge and tiny pivot columns.", "tokens": [286, 519, 11, 307, 264, 2734, 11, 570, 300, 311, 437, 550, 733, 295, 4684, 281, 505, 1242, 11, 1338, 11, 2603, 293, 5870, 14538, 13766, 13], "temperature": 0.0, "avg_logprob": -0.20341368419368092, "compression_ratio": 1.5665024630541873, "no_speech_prob": 2.090410134769627e-06}, {"id": 489, "seek": 227042, "start": 2288.42, "end": 2294.42, "text": " The other thing that's going to happen is like when we're solving this system, you know,", "tokens": [440, 661, 551, 300, 311, 516, 281, 1051, 307, 411, 562, 321, 434, 12606, 341, 1185, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.20341368419368092, "compression_ratio": 1.5665024630541873, "no_speech_prob": 2.090410134769627e-06}, {"id": 490, "seek": 229442, "start": 2294.42, "end": 2302.42, "text": " the UX part, like we're going to have to divide by 10 to the 20th and then on the next row,", "tokens": [264, 40176, 644, 11, 411, 321, 434, 516, 281, 362, 281, 9845, 538, 1266, 281, 264, 945, 392, 293, 550, 322, 264, 958, 5386, 11], "temperature": 0.0, "avg_logprob": -0.1885413204330996, "compression_ratio": 1.556701030927835, "no_speech_prob": 2.260252585983835e-06}, {"id": 491, "seek": 229442, "start": 2302.42, "end": 2313.42, "text": " we've got these coefficients, I guess, that we're now like dividing by that are huge and tiny, which is bad.", "tokens": [321, 600, 658, 613, 31994, 11, 286, 2041, 11, 300, 321, 434, 586, 411, 26764, 538, 300, 366, 2603, 293, 5870, 11, 597, 307, 1578, 13], "temperature": 0.0, "avg_logprob": -0.1885413204330996, "compression_ratio": 1.556701030927835, "no_speech_prob": 2.260252585983835e-06}, {"id": 492, "seek": 229442, "start": 2313.42, "end": 2321.42, "text": " So it's only a problem because we do it like the composition going down. So it's only like the ratio.", "tokens": [407, 309, 311, 787, 257, 1154, 570, 321, 360, 309, 411, 264, 12686, 516, 760, 13, 407, 309, 311, 787, 411, 264, 8509, 13], "temperature": 0.0, "avg_logprob": -0.1885413204330996, "compression_ratio": 1.556701030927835, "no_speech_prob": 2.260252585983835e-06}, {"id": 493, "seek": 232142, "start": 2321.42, "end": 2327.42, "text": " Yes. Yes. Yeah. The ratio is kind of what's creating these, but it's also like the ratio is kind of artificial", "tokens": [1079, 13, 1079, 13, 865, 13, 440, 8509, 307, 733, 295, 437, 311, 4084, 613, 11, 457, 309, 311, 611, 411, 264, 8509, 307, 733, 295, 11677], "temperature": 0.0, "avg_logprob": -0.16220490344159014, "compression_ratio": 1.5966850828729282, "no_speech_prob": 2.3185772079159506e-05}, {"id": 494, "seek": 232142, "start": 2327.42, "end": 2338.42, "text": " because you could put the rows in any order and it would be smaller.", "tokens": [570, 291, 727, 829, 264, 13241, 294, 604, 1668, 293, 309, 576, 312, 4356, 13], "temperature": 0.0, "avg_logprob": -0.16220490344159014, "compression_ratio": 1.5966850828729282, "no_speech_prob": 2.3185772079159506e-05}, {"id": 495, "seek": 232142, "start": 2338.42, "end": 2345.42, "text": " So what it is is you would actually when you do this, if you had like a matrix that was more than two by two,", "tokens": [407, 437, 309, 307, 307, 291, 576, 767, 562, 291, 360, 341, 11, 498, 291, 632, 411, 257, 8141, 300, 390, 544, 813, 732, 538, 732, 11], "temperature": 0.0, "avg_logprob": -0.16220490344159014, "compression_ratio": 1.5966850828729282, "no_speech_prob": 2.3185772079159506e-05}, {"id": 496, "seek": 234542, "start": 2345.42, "end": 2352.42, "text": " is that on each iteration, you want to choose like the largest value that you have.", "tokens": [307, 300, 322, 1184, 24784, 11, 291, 528, 281, 2826, 411, 264, 6443, 2158, 300, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.09071523333908221, "compression_ratio": 1.7565217391304349, "no_speech_prob": 5.093539584777318e-06}, {"id": 497, "seek": 234542, "start": 2352.42, "end": 2358.42, "text": " Yeah. And then swap those two rows. But kind of once you have more rows, you can't do it all in advance.", "tokens": [865, 13, 400, 550, 18135, 729, 732, 13241, 13, 583, 733, 295, 1564, 291, 362, 544, 13241, 11, 291, 393, 380, 360, 309, 439, 294, 7295, 13], "temperature": 0.0, "avg_logprob": -0.09071523333908221, "compression_ratio": 1.7565217391304349, "no_speech_prob": 5.093539584777318e-06}, {"id": 498, "seek": 234542, "start": 2358.42, "end": 2365.42, "text": " Like you kind of have to go, you know, like see how it changes your rows to do one of the outer loop", "tokens": [1743, 291, 733, 295, 362, 281, 352, 11, 291, 458, 11, 411, 536, 577, 309, 2962, 428, 13241, 281, 360, 472, 295, 264, 10847, 6367], "temperature": 0.0, "avg_logprob": -0.09071523333908221, "compression_ratio": 1.7565217391304349, "no_speech_prob": 5.093539584777318e-06}, {"id": 499, "seek": 234542, "start": 2365.42, "end": 2372.42, "text": " where you zero something out and then for the next one to, you know, pivot, which is, you know, swapping two rows.", "tokens": [689, 291, 4018, 746, 484, 293, 550, 337, 264, 958, 472, 281, 11, 291, 458, 11, 14538, 11, 597, 307, 11, 291, 458, 11, 1693, 10534, 732, 13241, 13], "temperature": 0.0, "avg_logprob": -0.09071523333908221, "compression_ratio": 1.7565217391304349, "no_speech_prob": 5.093539584777318e-06}, {"id": 500, "seek": 237242, "start": 2372.42, "end": 2378.42, "text": " But kind of each time you're going to like find what's the best row to swap with.", "tokens": [583, 733, 295, 1184, 565, 291, 434, 516, 281, 411, 915, 437, 311, 264, 1151, 5386, 281, 18135, 365, 13], "temperature": 0.0, "avg_logprob": -0.08379185199737549, "compression_ratio": 1.1944444444444444, "no_speech_prob": 1.5689211068092845e-05}, {"id": 501, "seek": 237242, "start": 2378.42, "end": 2393.42, "text": " You're welcome. Any other questions about this?", "tokens": [509, 434, 2928, 13, 2639, 661, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.08379185199737549, "compression_ratio": 1.1944444444444444, "no_speech_prob": 1.5689211068092845e-05}, {"id": 502, "seek": 239342, "start": 2393.42, "end": 2405.42, "text": " OK, let me go back down. OK. And so then that was it for kind of following up on LU factorization.", "tokens": [2264, 11, 718, 385, 352, 646, 760, 13, 2264, 13, 400, 370, 550, 300, 390, 309, 337, 733, 295, 3480, 493, 322, 31851, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.10729919309201448, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.785252602028777e-06}, {"id": 503, "seek": 239342, "start": 2405.42, "end": 2410.42, "text": " I wanted to return to a question from last week about block matrices.", "tokens": [286, 1415, 281, 2736, 281, 257, 1168, 490, 1036, 1243, 466, 3461, 32284, 13], "temperature": 0.0, "avg_logprob": -0.10729919309201448, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.785252602028777e-06}, {"id": 504, "seek": 239342, "start": 2410.42, "end": 2415.42, "text": " And this is something that I wanted to teach at some point and kind of wasn't sure where to stick it.", "tokens": [400, 341, 307, 746, 300, 286, 1415, 281, 2924, 412, 512, 935, 293, 733, 295, 2067, 380, 988, 689, 281, 2897, 309, 13], "temperature": 0.0, "avg_logprob": -0.10729919309201448, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.785252602028777e-06}, {"id": 505, "seek": 239342, "start": 2415.42, "end": 2421.42, "text": " So it's a talk about it here. First, I wanted to talk about ordinary matrix multiplication", "tokens": [407, 309, 311, 257, 751, 466, 309, 510, 13, 2386, 11, 286, 1415, 281, 751, 466, 10547, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.10729919309201448, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.785252602028777e-06}, {"id": 506, "seek": 242142, "start": 2421.42, "end": 2429.42, "text": " to contrast. And so I want you to take a moment to think about what is the computational complexity", "tokens": [281, 8712, 13, 400, 370, 286, 528, 291, 281, 747, 257, 1623, 281, 519, 466, 437, 307, 264, 28270, 14024], "temperature": 0.0, "avg_logprob": -0.13922096181798865, "compression_ratio": 1.4266666666666667, "no_speech_prob": 5.3072864830028266e-05}, {"id": 507, "seek": 242142, "start": 2429.42, "end": 2437.42, "text": " or big O of matrix multiplication if you have n by n matrices?", "tokens": [420, 955, 422, 295, 8141, 27290, 498, 291, 362, 297, 538, 297, 32284, 30], "temperature": 0.0, "avg_logprob": -0.13922096181798865, "compression_ratio": 1.4266666666666667, "no_speech_prob": 5.3072864830028266e-05}, {"id": 508, "seek": 242142, "start": 2437.42, "end": 2443.42, "text": " I want to raise their hand if they know the answer.", "tokens": [286, 528, 281, 5300, 641, 1011, 498, 436, 458, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.13922096181798865, "compression_ratio": 1.4266666666666667, "no_speech_prob": 5.3072864830028266e-05}, {"id": 509, "seek": 244342, "start": 2443.42, "end": 2454.42, "text": " Yeah, Roger. Exactly. It's n cubed.", "tokens": [865, 11, 17666, 13, 7587, 13, 467, 311, 297, 36510, 13], "temperature": 0.0, "avg_logprob": -0.18503596232487604, "compression_ratio": 1.265625, "no_speech_prob": 5.828068606206216e-05}, {"id": 510, "seek": 244342, "start": 2454.42, "end": 2459.42, "text": " No, that's good. I was going to ask you how you got that.", "tokens": [883, 11, 300, 311, 665, 13, 286, 390, 516, 281, 1029, 291, 577, 291, 658, 300, 13], "temperature": 0.0, "avg_logprob": -0.18503596232487604, "compression_ratio": 1.265625, "no_speech_prob": 5.828068606206216e-05}, {"id": 511, "seek": 244342, "start": 2459.42, "end": 2467.42, "text": " So to calculate one entry, you need to multiply a row by the column.", "tokens": [407, 281, 8873, 472, 8729, 11, 291, 643, 281, 12972, 257, 5386, 538, 264, 7738, 13], "temperature": 0.0, "avg_logprob": -0.18503596232487604, "compression_ratio": 1.265625, "no_speech_prob": 5.828068606206216e-05}, {"id": 512, "seek": 246742, "start": 2467.42, "end": 2476.42, "text": " Yes. Yeah. Exactly. Yeah. Thank you.", "tokens": [1079, 13, 865, 13, 7587, 13, 865, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.14908996309552874, "compression_ratio": 1.5257142857142858, "no_speech_prob": 7.41093754186295e-06}, {"id": 513, "seek": 246742, "start": 2476.42, "end": 2480.42, "text": " Yeah. So matrix multiplication and n cubed is slow.", "tokens": [865, 13, 407, 8141, 27290, 293, 297, 36510, 307, 2964, 13], "temperature": 0.0, "avg_logprob": -0.14908996309552874, "compression_ratio": 1.5257142857142858, "no_speech_prob": 7.41093754186295e-06}, {"id": 514, "seek": 246742, "start": 2480.42, "end": 2483.42, "text": " Like it's not a good computational complexity.", "tokens": [1743, 309, 311, 406, 257, 665, 28270, 14024, 13], "temperature": 0.0, "avg_logprob": -0.14908996309552874, "compression_ratio": 1.5257142857142858, "no_speech_prob": 7.41093754186295e-06}, {"id": 515, "seek": 246742, "start": 2483.42, "end": 2486.42, "text": " And that's what matrix multiplication is, though.", "tokens": [400, 300, 311, 437, 8141, 27290, 307, 11, 1673, 13], "temperature": 0.0, "avg_logprob": -0.14908996309552874, "compression_ratio": 1.5257142857142858, "no_speech_prob": 7.41093754186295e-06}, {"id": 516, "seek": 246742, "start": 2486.42, "end": 2494.42, "text": " But a different or kind of an additional perspective to think about is, you know,", "tokens": [583, 257, 819, 420, 733, 295, 364, 4497, 4585, 281, 519, 466, 307, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.14908996309552874, "compression_ratio": 1.5257142857142858, "no_speech_prob": 7.41093754186295e-06}, {"id": 517, "seek": 249442, "start": 2494.42, "end": 2504.42, "text": " we've talked about this memory hierarchy and in general, what can we say about slower types of memory?", "tokens": [321, 600, 2825, 466, 341, 4675, 22333, 293, 294, 2674, 11, 437, 393, 321, 584, 466, 14009, 3467, 295, 4675, 30], "temperature": 0.0, "avg_logprob": -0.12348254989175235, "compression_ratio": 1.416184971098266, "no_speech_prob": 8.664263987157028e-06}, {"id": 518, "seek": 249442, "start": 2504.42, "end": 2511.42, "text": " Or how much slower are they?", "tokens": [1610, 577, 709, 14009, 366, 436, 30], "temperature": 0.0, "avg_logprob": -0.12348254989175235, "compression_ratio": 1.416184971098266, "no_speech_prob": 8.664263987157028e-06}, {"id": 519, "seek": 249442, "start": 2511.42, "end": 2517.42, "text": " I'm just looking for like a phrase, not a number. Yes, a lot.", "tokens": [286, 478, 445, 1237, 337, 411, 257, 9535, 11, 406, 257, 1230, 13, 1079, 11, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.12348254989175235, "compression_ratio": 1.416184971098266, "no_speech_prob": 8.664263987157028e-06}, {"id": 520, "seek": 249442, "start": 2517.42, "end": 2520.42, "text": " So slower. Yeah, I was thinking order of magnitude.", "tokens": [407, 14009, 13, 865, 11, 286, 390, 1953, 1668, 295, 15668, 13], "temperature": 0.0, "avg_logprob": -0.12348254989175235, "compression_ratio": 1.416184971098266, "no_speech_prob": 8.664263987157028e-06}, {"id": 521, "seek": 252042, "start": 2520.42, "end": 2527.42, "text": " Yeah, like typically going to a slower, the next slowest type of memory is going to be like an order of magnitude slower.", "tokens": [865, 11, 411, 5850, 516, 281, 257, 14009, 11, 264, 958, 2964, 377, 2010, 295, 4675, 307, 516, 281, 312, 411, 364, 1668, 295, 15668, 14009, 13], "temperature": 0.0, "avg_logprob": -0.13085162514134457, "compression_ratio": 1.6495726495726495, "no_speech_prob": 1.568905463500414e-05}, {"id": 522, "seek": 252042, "start": 2527.42, "end": 2536.42, "text": " And much cheaper. Yeah, they are also much cheaper, which is a benefit why we have slower memory.", "tokens": [400, 709, 12284, 13, 865, 11, 436, 366, 611, 709, 12284, 11, 597, 307, 257, 5121, 983, 321, 362, 14009, 4675, 13], "temperature": 0.0, "avg_logprob": -0.13085162514134457, "compression_ratio": 1.6495726495726495, "no_speech_prob": 1.568905463500414e-05}, {"id": 523, "seek": 252042, "start": 2536.42, "end": 2541.42, "text": " But if we think about matrix multiplication and kind of take into account memory,", "tokens": [583, 498, 321, 519, 466, 8141, 27290, 293, 733, 295, 747, 666, 2696, 4675, 11], "temperature": 0.0, "avg_logprob": -0.13085162514134457, "compression_ratio": 1.6495726495726495, "no_speech_prob": 1.568905463500414e-05}, {"id": 524, "seek": 252042, "start": 2541.42, "end": 2549.42, "text": " what's happening is, say we're reading row I of A into fast memory in an outer loop,", "tokens": [437, 311, 2737, 307, 11, 584, 321, 434, 3760, 5386, 286, 295, 316, 666, 2370, 4675, 294, 364, 10847, 6367, 11], "temperature": 0.0, "avg_logprob": -0.13085162514134457, "compression_ratio": 1.6495726495726495, "no_speech_prob": 1.568905463500414e-05}, {"id": 525, "seek": 254942, "start": 2549.42, "end": 2556.42, "text": " and we'll read column J of B into fast memory, then we'll multiply that row times that column.", "tokens": [293, 321, 603, 1401, 7738, 508, 295, 363, 666, 2370, 4675, 11, 550, 321, 603, 12972, 300, 5386, 1413, 300, 7738, 13], "temperature": 0.0, "avg_logprob": -0.09994138673294423, "compression_ratio": 1.517766497461929, "no_speech_prob": 3.591018685256131e-05}, {"id": 526, "seek": 254942, "start": 2556.42, "end": 2560.42, "text": " So this inner loop is kind of like the end to get a single entry in C, our product.", "tokens": [407, 341, 7284, 6367, 307, 733, 295, 411, 264, 917, 281, 483, 257, 2167, 8729, 294, 383, 11, 527, 1674, 13], "temperature": 0.0, "avg_logprob": -0.09994138673294423, "compression_ratio": 1.517766497461929, "no_speech_prob": 3.591018685256131e-05}, {"id": 527, "seek": 254942, "start": 2560.42, "end": 2565.42, "text": " We're doing A times B. And here it's written.", "tokens": [492, 434, 884, 316, 1413, 363, 13, 400, 510, 309, 311, 3720, 13], "temperature": 0.0, "avg_logprob": -0.09994138673294423, "compression_ratio": 1.517766497461929, "no_speech_prob": 3.591018685256131e-05}, {"id": 528, "seek": 254942, "start": 2565.42, "end": 2568.42, "text": " Oh, here it's written with a mistake.", "tokens": [876, 11, 510, 309, 311, 3720, 365, 257, 6146, 13], "temperature": 0.0, "avg_logprob": -0.09994138673294423, "compression_ratio": 1.517766497461929, "no_speech_prob": 3.591018685256131e-05}, {"id": 529, "seek": 254942, "start": 2568.42, "end": 2573.42, "text": " This was supposed to be plus equals.", "tokens": [639, 390, 3442, 281, 312, 1804, 6915, 13], "temperature": 0.0, "avg_logprob": -0.09994138673294423, "compression_ratio": 1.517766497461929, "no_speech_prob": 3.591018685256131e-05}, {"id": 530, "seek": 257342, "start": 2573.42, "end": 2580.42, "text": " With the idea that you've, you know, you're kind of, you know, multiplying pairwise the elements of this row of A,", "tokens": [2022, 264, 1558, 300, 291, 600, 11, 291, 458, 11, 291, 434, 733, 295, 11, 291, 458, 11, 30955, 6119, 3711, 264, 4959, 295, 341, 5386, 295, 316, 11], "temperature": 0.0, "avg_logprob": -0.09180285551837672, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.6187164874281734e-05}, {"id": 531, "seek": 257342, "start": 2580.42, "end": 2587.42, "text": " this column of B, and adding those to your kind of running total to get this element C.", "tokens": [341, 7738, 295, 363, 11, 293, 5127, 729, 281, 428, 733, 295, 2614, 3217, 281, 483, 341, 4478, 383, 13], "temperature": 0.0, "avg_logprob": -0.09180285551837672, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.6187164874281734e-05}, {"id": 532, "seek": 257342, "start": 2587.42, "end": 2591.42, "text": " And it takes N of those just to get one entry of C.", "tokens": [400, 309, 2516, 426, 295, 729, 445, 281, 483, 472, 8729, 295, 383, 13], "temperature": 0.0, "avg_logprob": -0.09180285551837672, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.6187164874281734e-05}, {"id": 533, "seek": 257342, "start": 2591.42, "end": 2597.42, "text": " And then kind of go write that element back to slow memory once you've got it.", "tokens": [400, 550, 733, 295, 352, 2464, 300, 4478, 646, 281, 2964, 4675, 1564, 291, 600, 658, 309, 13], "temperature": 0.0, "avg_logprob": -0.09180285551837672, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.6187164874281734e-05}, {"id": 534, "seek": 257342, "start": 2597.42, "end": 2602.42, "text": " Are there questions first just about kind of what this algorithm is doing?", "tokens": [2014, 456, 1651, 700, 445, 466, 733, 295, 437, 341, 9284, 307, 884, 30], "temperature": 0.0, "avg_logprob": -0.09180285551837672, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.6187164874281734e-05}, {"id": 535, "seek": 260242, "start": 2602.42, "end": 2608.42, "text": " So this is just standard matrix multiplication, only kind of like assuming that you can only fit one row of A", "tokens": [407, 341, 307, 445, 3832, 8141, 27290, 11, 787, 733, 295, 411, 11926, 300, 291, 393, 787, 3318, 472, 5386, 295, 316], "temperature": 0.0, "avg_logprob": -0.12436344254184777, "compression_ratio": 1.484375, "no_speech_prob": 2.0580191630870104e-06}, {"id": 536, "seek": 260242, "start": 2608.42, "end": 2617.42, "text": " and one column of B into your faster memory.", "tokens": [293, 472, 7738, 295, 363, 666, 428, 4663, 4675, 13], "temperature": 0.0, "avg_logprob": -0.12436344254184777, "compression_ratio": 1.484375, "no_speech_prob": 2.0580191630870104e-06}, {"id": 537, "seek": 260242, "start": 2617.42, "end": 2624.42, "text": " OK, so I want you to think about how many reads and writes are made in doing this.", "tokens": [2264, 11, 370, 286, 528, 291, 281, 519, 466, 577, 867, 15700, 293, 13657, 366, 1027, 294, 884, 341, 13], "temperature": 0.0, "avg_logprob": -0.12436344254184777, "compression_ratio": 1.484375, "no_speech_prob": 2.0580191630870104e-06}, {"id": 538, "seek": 260242, "start": 2624.42, "end": 2628.42, "text": " So if you're multiplying two matrices together.", "tokens": [407, 498, 291, 434, 30955, 732, 32284, 1214, 13], "temperature": 0.0, "avg_logprob": -0.12436344254184777, "compression_ratio": 1.484375, "no_speech_prob": 2.0580191630870104e-06}, {"id": 539, "seek": 262842, "start": 2628.42, "end": 2632.42, "text": " Hsuan?", "tokens": [389, 82, 6139, 30], "temperature": 0.0, "avg_logprob": -0.20101720408389442, "compression_ratio": 1.5108695652173914, "no_speech_prob": 3.120068504358642e-05}, {"id": 540, "seek": 262842, "start": 2632.42, "end": 2640.42, "text": " So how is fast memory and slow memory separated in the actual process?", "tokens": [407, 577, 307, 2370, 4675, 293, 2964, 4675, 12005, 294, 264, 3539, 1399, 30], "temperature": 0.0, "avg_logprob": -0.20101720408389442, "compression_ratio": 1.5108695652173914, "no_speech_prob": 3.120068504358642e-05}, {"id": 541, "seek": 262842, "start": 2640.42, "end": 2643.42, "text": " So, yeah, this is kind of hard because this is hidden for you.", "tokens": [407, 11, 1338, 11, 341, 307, 733, 295, 1152, 570, 341, 307, 7633, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.20101720408389442, "compression_ratio": 1.5108695652173914, "no_speech_prob": 3.120068504358642e-05}, {"id": 542, "seek": 262842, "start": 2643.42, "end": 2647.42, "text": " So you're not explicitly telling the computer to do these things.", "tokens": [407, 291, 434, 406, 20803, 3585, 264, 3820, 281, 360, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.20101720408389442, "compression_ratio": 1.5108695652173914, "no_speech_prob": 3.120068504358642e-05}, {"id": 543, "seek": 262842, "start": 2647.42, "end": 2652.42, "text": " And by slow memory here, I probably mean cache, not sorry, fast memory.", "tokens": [400, 538, 2964, 4675, 510, 11, 286, 1391, 914, 19459, 11, 406, 2597, 11, 2370, 4675, 13], "temperature": 0.0, "avg_logprob": -0.20101720408389442, "compression_ratio": 1.5108695652173914, "no_speech_prob": 3.120068504358642e-05}, {"id": 544, "seek": 265242, "start": 2652.42, "end": 2660.42, "text": " I mean cache, slow memory, like main memory.", "tokens": [286, 914, 19459, 11, 2964, 4675, 11, 411, 2135, 4675, 13], "temperature": 0.0, "avg_logprob": -0.14259336432632136, "compression_ratio": 1.6343612334801763, "no_speech_prob": 1.9525408788467757e-05}, {"id": 545, "seek": 265242, "start": 2660.42, "end": 2662.42, "text": " Yeah, you're not explicitly controlling this, though.", "tokens": [865, 11, 291, 434, 406, 20803, 14905, 341, 11, 1673, 13], "temperature": 0.0, "avg_logprob": -0.14259336432632136, "compression_ratio": 1.6343612334801763, "no_speech_prob": 1.9525408788467757e-05}, {"id": 546, "seek": 265242, "start": 2662.42, "end": 2667.42, "text": " It's kind of like a lower level of the language is handling this.", "tokens": [467, 311, 733, 295, 411, 257, 3126, 1496, 295, 264, 2856, 307, 13175, 341, 13], "temperature": 0.0, "avg_logprob": -0.14259336432632136, "compression_ratio": 1.6343612334801763, "no_speech_prob": 1.9525408788467757e-05}, {"id": 547, "seek": 265242, "start": 2667.42, "end": 2672.42, "text": " I mean, often when you write these, the people that write the libraries we use do do this.", "tokens": [286, 914, 11, 2049, 562, 291, 2464, 613, 11, 264, 561, 300, 2464, 264, 15148, 321, 764, 360, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.14259336432632136, "compression_ratio": 1.6343612334801763, "no_speech_prob": 1.9525408788467757e-05}, {"id": 548, "seek": 265242, "start": 2672.42, "end": 2674.42, "text": " Yeah, that's true.", "tokens": [865, 11, 300, 311, 2074, 13], "temperature": 0.0, "avg_logprob": -0.14259336432632136, "compression_ratio": 1.6343612334801763, "no_speech_prob": 1.9525408788467757e-05}, {"id": 549, "seek": 265242, "start": 2674.42, "end": 2679.42, "text": " Either it's like really big parallel stuff they read and write to the network from time to time.", "tokens": [13746, 309, 311, 411, 534, 955, 8952, 1507, 436, 1401, 293, 2464, 281, 264, 3209, 490, 565, 281, 565, 13], "temperature": 0.0, "avg_logprob": -0.14259336432632136, "compression_ratio": 1.6343612334801763, "no_speech_prob": 1.9525408788467757e-05}, {"id": 550, "seek": 267942, "start": 2679.42, "end": 2685.42, "text": " For the fast stuff on computers, they usually want to manage the cache.", "tokens": [1171, 264, 2370, 1507, 322, 10807, 11, 436, 2673, 528, 281, 3067, 264, 19459, 13], "temperature": 0.0, "avg_logprob": -0.18319087982177734, "compression_ratio": 1.584, "no_speech_prob": 2.753025728452485e-05}, {"id": 551, "seek": 267942, "start": 2685.42, "end": 2689.42, "text": " Particularly on GPUs, you have to manage it manually.", "tokens": [32281, 322, 18407, 82, 11, 291, 362, 281, 3067, 309, 16945, 13], "temperature": 0.0, "avg_logprob": -0.18319087982177734, "compression_ratio": 1.584, "no_speech_prob": 2.753025728452485e-05}, {"id": 552, "seek": 267942, "start": 2689.42, "end": 2696.42, "text": " And I guess this also comes up with like, so LOPAC is like specifically optimized for each type of computer.", "tokens": [400, 286, 2041, 341, 611, 1487, 493, 365, 411, 11, 370, 441, 12059, 4378, 307, 411, 4682, 26941, 337, 1184, 2010, 295, 3820, 13], "temperature": 0.0, "avg_logprob": -0.18319087982177734, "compression_ratio": 1.584, "no_speech_prob": 2.753025728452485e-05}, {"id": 553, "seek": 267942, "start": 2696.42, "end": 2700.42, "text": " You know, unfortunately, it's the standard, so it's out there.", "tokens": [509, 458, 11, 7015, 11, 309, 311, 264, 3832, 11, 370, 309, 311, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.18319087982177734, "compression_ratio": 1.584, "no_speech_prob": 2.753025728452485e-05}, {"id": 554, "seek": 267942, "start": 2700.42, "end": 2707.42, "text": " But like LOPAC and BLAS are being kind of optimized to the specific architecture of your computer.", "tokens": [583, 411, 441, 12059, 4378, 293, 15132, 3160, 366, 885, 733, 295, 26941, 281, 264, 2685, 9482, 295, 428, 3820, 13], "temperature": 0.0, "avg_logprob": -0.18319087982177734, "compression_ratio": 1.584, "no_speech_prob": 2.753025728452485e-05}, {"id": 555, "seek": 270742, "start": 2707.42, "end": 2711.42, "text": " In fact, as probably a lot of people in the past will end up having to deal with this directly,", "tokens": [682, 1186, 11, 382, 1391, 257, 688, 295, 561, 294, 264, 1791, 486, 917, 493, 1419, 281, 2028, 365, 341, 3838, 11], "temperature": 0.0, "avg_logprob": -0.1579004536504331, "compression_ratio": 1.625, "no_speech_prob": 5.306907769409008e-05}, {"id": 556, "seek": 270742, "start": 2711.42, "end": 2717.42, "text": " because they'll find themselves at jobs as data scientists where they're working on clusters.", "tokens": [570, 436, 603, 915, 2969, 412, 4782, 382, 1412, 7708, 689, 436, 434, 1364, 322, 23313, 13], "temperature": 0.0, "avg_logprob": -0.1579004536504331, "compression_ratio": 1.625, "no_speech_prob": 5.306907769409008e-05}, {"id": 557, "seek": 270742, "start": 2717.42, "end": 2720.42, "text": " And so you basically pull stuff off the network.", "tokens": [400, 370, 291, 1936, 2235, 1507, 766, 264, 3209, 13], "temperature": 0.0, "avg_logprob": -0.1579004536504331, "compression_ratio": 1.625, "no_speech_prob": 5.306907769409008e-05}, {"id": 558, "seek": 270742, "start": 2720.42, "end": 2725.42, "text": " So when you multiply matrices, you're probably using these kind of techniques to do it on multiple computers.", "tokens": [407, 562, 291, 12972, 32284, 11, 291, 434, 1391, 1228, 613, 733, 295, 7512, 281, 360, 309, 322, 3866, 10807, 13], "temperature": 0.0, "avg_logprob": -0.1579004536504331, "compression_ratio": 1.625, "no_speech_prob": 5.306907769409008e-05}, {"id": 559, "seek": 270742, "start": 2725.42, "end": 2728.42, "text": " Yeah, that's a good point.", "tokens": [865, 11, 300, 311, 257, 665, 935, 13], "temperature": 0.0, "avg_logprob": -0.1579004536504331, "compression_ratio": 1.625, "no_speech_prob": 5.306907769409008e-05}, {"id": 560, "seek": 270742, "start": 2728.42, "end": 2733.42, "text": " This is like a helpful way of thinking because it applies to so many different levels of the memory hierarchy as well.", "tokens": [639, 307, 411, 257, 4961, 636, 295, 1953, 570, 309, 13165, 281, 370, 867, 819, 4358, 295, 264, 4675, 22333, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1579004536504331, "compression_ratio": 1.625, "no_speech_prob": 5.306907769409008e-05}, {"id": 561, "seek": 273342, "start": 2733.42, "end": 2744.42, "text": " So yeah, you'll see this kind of if you're doing a large amount of data on a network.", "tokens": [407, 1338, 11, 291, 603, 536, 341, 733, 295, 498, 291, 434, 884, 257, 2416, 2372, 295, 1412, 322, 257, 3209, 13], "temperature": 0.0, "avg_logprob": -0.11924424261417028, "compression_ratio": 1.3732394366197183, "no_speech_prob": 1.2218602932989597e-05}, {"id": 562, "seek": 273342, "start": 2744.42, "end": 2745.42, "text": " Yeah, so take a moment.", "tokens": [865, 11, 370, 747, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.11924424261417028, "compression_ratio": 1.3732394366197183, "no_speech_prob": 1.2218602932989597e-05}, {"id": 563, "seek": 273342, "start": 2745.42, "end": 2753.42, "text": " Write down how many reads and writes are being made to do this matrix multiplication.", "tokens": [23499, 760, 577, 867, 15700, 293, 13657, 366, 885, 1027, 281, 360, 341, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.11924424261417028, "compression_ratio": 1.3732394366197183, "no_speech_prob": 1.2218602932989597e-05}, {"id": 564, "seek": 275342, "start": 2753.42, "end": 2767.42, "text": " And sorry, I want you to make a distinction between.", "tokens": [400, 2597, 11, 286, 528, 291, 281, 652, 257, 16844, 1296, 13], "temperature": 0.0, "avg_logprob": -0.1573239266872406, "compression_ratio": 0.8666666666666667, "no_speech_prob": 1.5440897186635993e-05}, {"id": 565, "seek": 276742, "start": 2767.42, "end": 2796.42, "text": " Actually, yeah, never mind. Just write down how many reads and writes.", "tokens": [5135, 11, 1338, 11, 1128, 1575, 13, 1449, 2464, 760, 577, 867, 15700, 293, 13657, 13], "temperature": 0.0, "avg_logprob": -0.2696779727935791, "compression_ratio": 0.9722222222222222, "no_speech_prob": 5.38804306415841e-05}, {"id": 566, "seek": 279642, "start": 2796.42, "end": 2823.42, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.6987557411193848, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.02208314836025238}, {"id": 567, "seek": 282342, "start": 2823.42, "end": 2840.42, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.6528462568918864, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.009132228791713715}, {"id": 568, "seek": 284042, "start": 2840.42, "end": 2854.42, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5119669437408447, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.0146193727850914}, {"id": 569, "seek": 285442, "start": 2854.42, "end": 2871.42, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5044885476430258, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.015813063830137253}, {"id": 570, "seek": 287142, "start": 2871.42, "end": 2899.42, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1722534497578939, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.00010218643001280725}, {"id": 571, "seek": 289942, "start": 2899.42, "end": 2906.42, "text": " Raise your hand if you want more time.", "tokens": [30062, 428, 1011, 498, 291, 528, 544, 565, 13], "temperature": 0.0, "avg_logprob": -0.2832537757025825, "compression_ratio": 1.3841463414634145, "no_speech_prob": 4.3299325625412166e-05}, {"id": 572, "seek": 289942, "start": 2906.42, "end": 2915.42, "text": " Does anyone have an answer they want to share?", "tokens": [4402, 2878, 362, 364, 1867, 436, 528, 281, 2073, 30], "temperature": 0.0, "avg_logprob": -0.2832537757025825, "compression_ratio": 1.3841463414634145, "no_speech_prob": 4.3299325625412166e-05}, {"id": 573, "seek": 289942, "start": 2915.42, "end": 2919.42, "text": " Reads is n squared plus n cubed.", "tokens": [1300, 5834, 307, 297, 8889, 1804, 297, 36510, 13], "temperature": 0.0, "avg_logprob": -0.2832537757025825, "compression_ratio": 1.3841463414634145, "no_speech_prob": 4.3299325625412166e-05}, {"id": 574, "seek": 289942, "start": 2919.42, "end": 2926.42, "text": " Yes, I would actually before you throw it back and how many how does that break down for kind of a versus B?", "tokens": [1079, 11, 286, 576, 767, 949, 291, 3507, 309, 646, 293, 577, 867, 577, 775, 300, 1821, 760, 337, 733, 295, 257, 5717, 363, 30], "temperature": 0.0, "avg_logprob": -0.2832537757025825, "compression_ratio": 1.3841463414634145, "no_speech_prob": 4.3299325625412166e-05}, {"id": 575, "seek": 292642, "start": 2926.42, "end": 2936.42, "text": " Sure.", "tokens": [4894, 13], "temperature": 0.0, "avg_logprob": -0.29351691404978436, "compression_ratio": 0.7037037037037037, "no_speech_prob": 0.00011769411503337324}, {"id": 576, "seek": 292642, "start": 2936.42, "end": 2947.42, "text": " Yes, exactly.", "tokens": [1079, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.29351691404978436, "compression_ratio": 0.7037037037037037, "no_speech_prob": 0.00011769411503337324}, {"id": 577, "seek": 294742, "start": 2947.42, "end": 2959.42, "text": " Right, right. Yeah, thank you. That's great. So yeah, Sam said kind of this outer loop. We just read each row of a into memory once, which will end up taking n squared total.", "tokens": [1779, 11, 558, 13, 865, 11, 1309, 291, 13, 663, 311, 869, 13, 407, 1338, 11, 4832, 848, 733, 295, 341, 10847, 6367, 13, 492, 445, 1401, 1184, 5386, 295, 257, 666, 4675, 1564, 11, 597, 486, 917, 493, 1940, 297, 8889, 3217, 13], "temperature": 0.0, "avg_logprob": -0.19991556803385416, "compression_ratio": 1.5123152709359606, "no_speech_prob": 9.515576493868139e-06}, {"id": 578, "seek": 294742, "start": 2959.42, "end": 2963.42, "text": " It's an each time and we go through the loop n times.", "tokens": [467, 311, 364, 1184, 565, 293, 321, 352, 807, 264, 6367, 297, 1413, 13], "temperature": 0.0, "avg_logprob": -0.19991556803385416, "compression_ratio": 1.5123152709359606, "no_speech_prob": 9.515576493868139e-06}, {"id": 579, "seek": 294742, "start": 2963.42, "end": 2968.42, "text": " However, with B, we're doing for each row. We're going to read in each column.", "tokens": [2908, 11, 365, 363, 11, 321, 434, 884, 337, 1184, 5386, 13, 492, 434, 516, 281, 1401, 294, 1184, 7738, 13], "temperature": 0.0, "avg_logprob": -0.19991556803385416, "compression_ratio": 1.5123152709359606, "no_speech_prob": 9.515576493868139e-06}, {"id": 580, "seek": 296842, "start": 2968.42, "end": 2978.42, "text": " And so that ends up being n cubed reads for B and then C.", "tokens": [400, 370, 300, 5314, 493, 885, 297, 36510, 15700, 337, 363, 293, 550, 383, 13], "temperature": 0.0, "avg_logprob": -0.216635619269477, "compression_ratio": 1.2589285714285714, "no_speech_prob": 2.5462872144998983e-05}, {"id": 581, "seek": 296842, "start": 2978.42, "end": 2981.42, "text": " Because so for the.", "tokens": [1436, 370, 337, 264, 13], "temperature": 0.0, "avg_logprob": -0.216635619269477, "compression_ratio": 1.2589285714285714, "no_speech_prob": 2.5462872144998983e-05}, {"id": 582, "seek": 296842, "start": 2981.42, "end": 2989.42, "text": " So kind of for each day, we're making n reads to read column J.", "tokens": [407, 733, 295, 337, 1184, 786, 11, 321, 434, 1455, 297, 15700, 281, 1401, 7738, 508, 13], "temperature": 0.0, "avg_logprob": -0.216635619269477, "compression_ratio": 1.2589285714285714, "no_speech_prob": 2.5462872144998983e-05}, {"id": 583, "seek": 298942, "start": 2989.42, "end": 2999.42, "text": " And we're having to do that for J equals one to n. So that's n squared. And we're doing that then for each row of each row of a.", "tokens": [400, 321, 434, 1419, 281, 360, 300, 337, 508, 6915, 472, 281, 297, 13, 407, 300, 311, 297, 8889, 13, 400, 321, 434, 884, 300, 550, 337, 1184, 5386, 295, 1184, 5386, 295, 257, 13], "temperature": 0.0, "avg_logprob": -0.20005555834089006, "compression_ratio": 1.4863013698630136, "no_speech_prob": 2.318557926628273e-05}, {"id": 584, "seek": 298942, "start": 2999.42, "end": 3005.42, "text": " The issue is like each column.", "tokens": [440, 2734, 307, 411, 1184, 7738, 13], "temperature": 0.0, "avg_logprob": -0.20005555834089006, "compression_ratio": 1.4863013698630136, "no_speech_prob": 2.318557926628273e-05}, {"id": 585, "seek": 298942, "start": 3005.42, "end": 3011.42, "text": " Oh, because it's a column. So it has n length.", "tokens": [876, 11, 570, 309, 311, 257, 7738, 13, 407, 309, 575, 297, 4641, 13], "temperature": 0.0, "avg_logprob": -0.20005555834089006, "compression_ratio": 1.4863013698630136, "no_speech_prob": 2.318557926628273e-05}, {"id": 586, "seek": 298942, "start": 3011.42, "end": 3015.42, "text": " Yes. Yeah.", "tokens": [1079, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.20005555834089006, "compression_ratio": 1.4863013698630136, "no_speech_prob": 2.318557926628273e-05}, {"id": 587, "seek": 301542, "start": 3015.42, "end": 3032.42, "text": " Yeah, sorry. So this is a number of element wise reads, but it's it's important to note that it's a n more reads for B than for A because you've got this redundancy of your you're pulling in each column of B.", "tokens": [865, 11, 2597, 13, 407, 341, 307, 257, 1230, 295, 4478, 10829, 15700, 11, 457, 309, 311, 309, 311, 1021, 281, 3637, 300, 309, 311, 257, 297, 544, 15700, 337, 363, 813, 337, 316, 570, 291, 600, 658, 341, 27830, 6717, 295, 428, 291, 434, 8407, 294, 1184, 7738, 295, 363, 13], "temperature": 0.0, "avg_logprob": -0.15848432003873067, "compression_ratio": 1.6379310344827587, "no_speech_prob": 6.6433271967980545e-06}, {"id": 588, "seek": 301542, "start": 3032.42, "end": 3035.42, "text": " Multiple times because you have to do it for every single row of a.", "tokens": [40056, 1413, 570, 291, 362, 281, 360, 309, 337, 633, 2167, 5386, 295, 257, 13], "temperature": 0.0, "avg_logprob": -0.15848432003873067, "compression_ratio": 1.6379310344827587, "no_speech_prob": 6.6433271967980545e-06}, {"id": 589, "seek": 301542, "start": 3035.42, "end": 3039.42, "text": " And then in the K loop, you're assuming that doesn't count because it's fast memory.", "tokens": [400, 550, 294, 264, 591, 6367, 11, 291, 434, 11926, 300, 1177, 380, 1207, 570, 309, 311, 2370, 4675, 13], "temperature": 0.0, "avg_logprob": -0.15848432003873067, "compression_ratio": 1.6379310344827587, "no_speech_prob": 6.6433271967980545e-06}, {"id": 590, "seek": 301542, "start": 3039.42, "end": 3043.42, "text": " So for the K loop.", "tokens": [407, 337, 264, 591, 6367, 13], "temperature": 0.0, "avg_logprob": -0.15848432003873067, "compression_ratio": 1.6379310344827587, "no_speech_prob": 6.6433271967980545e-06}, {"id": 591, "seek": 304342, "start": 3043.42, "end": 3049.42, "text": " Well, they've already been pulled in. Yeah. So you're not having to. Yeah. Right. Right.", "tokens": [1042, 11, 436, 600, 1217, 668, 7373, 294, 13, 865, 13, 407, 291, 434, 406, 1419, 281, 13, 865, 13, 1779, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.12954165330573694, "compression_ratio": 1.4508670520231215, "no_speech_prob": 5.422104550234508e-06}, {"id": 592, "seek": 304342, "start": 3049.42, "end": 3060.42, "text": " Thanks, Jeremy. So questions about how I got. And then you're doing n squared writes to kind of write C back out to slow memory.", "tokens": [2561, 11, 17809, 13, 407, 1651, 466, 577, 286, 658, 13, 400, 550, 291, 434, 884, 297, 8889, 13657, 281, 733, 295, 2464, 383, 646, 484, 281, 2964, 4675, 13], "temperature": 0.0, "avg_logprob": -0.12954165330573694, "compression_ratio": 1.4508670520231215, "no_speech_prob": 5.422104550234508e-06}, {"id": 593, "seek": 304342, "start": 3060.42, "end": 3069.42, "text": " Questions about that calculation.", "tokens": [27738, 466, 300, 17108, 13], "temperature": 0.0, "avg_logprob": -0.12954165330573694, "compression_ratio": 1.4508670520231215, "no_speech_prob": 5.422104550234508e-06}, {"id": 594, "seek": 306942, "start": 3069.42, "end": 3074.42, "text": " OK, so now we're going to contrast it with block matrix multiplication.", "tokens": [2264, 11, 370, 586, 321, 434, 516, 281, 8712, 309, 365, 3461, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.10251170468617635, "compression_ratio": 1.6485148514851484, "no_speech_prob": 5.50745699001709e-06}, {"id": 595, "seek": 306942, "start": 3074.42, "end": 3085.42, "text": " And the idea behind block matrix multiplication is basically that you're just kind of subdividing A and B into several smaller matrices.", "tokens": [400, 264, 1558, 2261, 3461, 8141, 27290, 307, 1936, 300, 291, 434, 445, 733, 295, 31662, 1843, 278, 316, 293, 363, 666, 2940, 4356, 32284, 13], "temperature": 0.0, "avg_logprob": -0.10251170468617635, "compression_ratio": 1.6485148514851484, "no_speech_prob": 5.50745699001709e-06}, {"id": 596, "seek": 306942, "start": 3085.42, "end": 3089.42, "text": " So we could say you're making big n by big n blocks.", "tokens": [407, 321, 727, 584, 291, 434, 1455, 955, 297, 538, 955, 297, 8474, 13], "temperature": 0.0, "avg_logprob": -0.10251170468617635, "compression_ratio": 1.6485148514851484, "no_speech_prob": 5.50745699001709e-06}, {"id": 597, "seek": 306942, "start": 3089.42, "end": 3095.42, "text": " Each one will be of size little n over big n times little n over big n.", "tokens": [6947, 472, 486, 312, 295, 2744, 707, 297, 670, 955, 297, 1413, 707, 297, 670, 955, 297, 13], "temperature": 0.0, "avg_logprob": -0.10251170468617635, "compression_ratio": 1.6485148514851484, "no_speech_prob": 5.50745699001709e-06}, {"id": 598, "seek": 309542, "start": 3095.42, "end": 3099.42, "text": " Here, big n is just two.", "tokens": [1692, 11, 955, 297, 307, 445, 732, 13], "temperature": 0.0, "avg_logprob": -0.10190949568877349, "compression_ratio": 1.5321637426900585, "no_speech_prob": 6.854229468444828e-06}, {"id": 599, "seek": 309542, "start": 3099.42, "end": 3103.42, "text": " So we're just getting four blocks, two by two.", "tokens": [407, 321, 434, 445, 1242, 1451, 8474, 11, 732, 538, 732, 13], "temperature": 0.0, "avg_logprob": -0.10190949568877349, "compression_ratio": 1.5321637426900585, "no_speech_prob": 6.854229468444828e-06}, {"id": 600, "seek": 309542, "start": 3103.42, "end": 3108.42, "text": " And it turns out like kind of the matrix multiplication works exactly the same.", "tokens": [400, 309, 4523, 484, 411, 733, 295, 264, 8141, 27290, 1985, 2293, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.10190949568877349, "compression_ratio": 1.5321637426900585, "no_speech_prob": 6.854229468444828e-06}, {"id": 601, "seek": 309542, "start": 3108.42, "end": 3117.42, "text": " So the kind of top left quadrant of C is going to be A one one times B one one plus A one two times B two one,", "tokens": [407, 264, 733, 295, 1192, 1411, 46856, 295, 383, 307, 516, 281, 312, 316, 472, 472, 1413, 363, 472, 472, 1804, 316, 472, 732, 1413, 363, 732, 472, 11], "temperature": 0.0, "avg_logprob": -0.10190949568877349, "compression_ratio": 1.5321637426900585, "no_speech_prob": 6.854229468444828e-06}, {"id": 602, "seek": 311742, "start": 3117.42, "end": 3129.42, "text": " which you get from just thinking about rows of A times columns of B are giving you this result.", "tokens": [597, 291, 483, 490, 445, 1953, 466, 13241, 295, 316, 1413, 13766, 295, 363, 366, 2902, 291, 341, 1874, 13], "temperature": 0.0, "avg_logprob": -0.08510737419128418, "compression_ratio": 1.5688622754491017, "no_speech_prob": 1.5779536397531047e-06}, {"id": 603, "seek": 311742, "start": 3129.42, "end": 3135.42, "text": " Similarly, over here, you're taking the the top.", "tokens": [13157, 11, 670, 510, 11, 291, 434, 1940, 264, 264, 1192, 13], "temperature": 0.0, "avg_logprob": -0.08510737419128418, "compression_ratio": 1.5688622754491017, "no_speech_prob": 1.5779536397531047e-06}, {"id": 604, "seek": 311742, "start": 3135.42, "end": 3146.42, "text": " And here, when I say row, I mean the top row of blocks of A times the second column of blocks of B to get this value.", "tokens": [400, 510, 11, 562, 286, 584, 5386, 11, 286, 914, 264, 1192, 5386, 295, 8474, 295, 316, 1413, 264, 1150, 7738, 295, 8474, 295, 363, 281, 483, 341, 2158, 13], "temperature": 0.0, "avg_logprob": -0.08510737419128418, "compression_ratio": 1.5688622754491017, "no_speech_prob": 1.5779536397531047e-06}, {"id": 605, "seek": 314642, "start": 3146.42, "end": 3157.42, "text": " And so on. And so you kind of are just doing these smaller matrix multiplications and then putting them together to get get your result.", "tokens": [400, 370, 322, 13, 400, 370, 291, 733, 295, 366, 445, 884, 613, 4356, 8141, 17596, 763, 293, 550, 3372, 552, 1214, 281, 483, 483, 428, 1874, 13], "temperature": 0.0, "avg_logprob": -0.13431062137379365, "compression_ratio": 1.5891089108910892, "no_speech_prob": 3.905406174453674e-06}, {"id": 606, "seek": 314642, "start": 3157.42, "end": 3162.42, "text": " So what this looks like in pseudocode.", "tokens": [407, 437, 341, 1542, 411, 294, 25505, 532, 905, 1429, 13], "temperature": 0.0, "avg_logprob": -0.13431062137379365, "compression_ratio": 1.5891089108910892, "no_speech_prob": 3.905406174453674e-06}, {"id": 607, "seek": 314642, "start": 3162.42, "end": 3167.42, "text": " And here I'm just referring to them as a block IK.", "tokens": [400, 510, 286, 478, 445, 13761, 281, 552, 382, 257, 3461, 286, 42, 13], "temperature": 0.0, "avg_logprob": -0.13431062137379365, "compression_ratio": 1.5891089108910892, "no_speech_prob": 3.905406174453674e-06}, {"id": 608, "seek": 314642, "start": 3167.42, "end": 3172.42, "text": " It's kind of talking which one of these, you know, four smaller matrices of A are you getting.", "tokens": [467, 311, 733, 295, 1417, 597, 472, 295, 613, 11, 291, 458, 11, 1451, 4356, 32284, 295, 316, 366, 291, 1242, 13], "temperature": 0.0, "avg_logprob": -0.13431062137379365, "compression_ratio": 1.5891089108910892, "no_speech_prob": 3.905406174453674e-06}, {"id": 609, "seek": 317242, "start": 3172.42, "end": 3177.42, "text": " So you have three nested loops and kind of an inner one.", "tokens": [407, 291, 362, 1045, 15646, 292, 16121, 293, 733, 295, 364, 7284, 472, 13], "temperature": 0.0, "avg_logprob": -0.10238494429477425, "compression_ratio": 1.7034883720930232, "no_speech_prob": 5.173850695427973e-06}, {"id": 610, "seek": 317242, "start": 3177.42, "end": 3188.42, "text": " You're taking block IK of A block KJ of B and then saying block IJ of C plus equals block of A times the block of B.", "tokens": [509, 434, 1940, 3461, 286, 42, 295, 316, 3461, 591, 41, 295, 363, 293, 550, 1566, 3461, 286, 41, 295, 383, 1804, 6915, 3461, 295, 316, 1413, 264, 3461, 295, 363, 13], "temperature": 0.0, "avg_logprob": -0.10238494429477425, "compression_ratio": 1.7034883720930232, "no_speech_prob": 5.173850695427973e-06}, {"id": 611, "seek": 317242, "start": 3188.42, "end": 3192.42, "text": " And you have to kind of go through each of your K's.", "tokens": [400, 291, 362, 281, 733, 295, 352, 807, 1184, 295, 428, 591, 311, 13], "temperature": 0.0, "avg_logprob": -0.10238494429477425, "compression_ratio": 1.7034883720930232, "no_speech_prob": 5.173850695427973e-06}, {"id": 612, "seek": 317242, "start": 3192.42, "end": 3198.42, "text": " Keep adding those to your running total and you get block IJ of C.", "tokens": [5527, 5127, 729, 281, 428, 2614, 3217, 293, 291, 483, 3461, 286, 41, 295, 383, 13], "temperature": 0.0, "avg_logprob": -0.10238494429477425, "compression_ratio": 1.7034883720930232, "no_speech_prob": 5.173850695427973e-06}, {"id": 613, "seek": 319842, "start": 3198.42, "end": 3208.42, "text": " First, are there questions kind of about just how this matrix multiplication is working?", "tokens": [2386, 11, 366, 456, 1651, 733, 295, 466, 445, 577, 341, 8141, 27290, 307, 1364, 30], "temperature": 0.0, "avg_logprob": -0.2195583265654895, "compression_ratio": 1.2992700729927007, "no_speech_prob": 5.306875391397625e-05}, {"id": 614, "seek": 319842, "start": 3208.42, "end": 3221.42, "text": " OK, so what is that? What is the big O of this?", "tokens": [2264, 11, 370, 437, 307, 300, 30, 708, 307, 264, 955, 422, 295, 341, 30], "temperature": 0.0, "avg_logprob": -0.2195583265654895, "compression_ratio": 1.2992700729927007, "no_speech_prob": 5.306875391397625e-05}, {"id": 615, "seek": 319842, "start": 3221.42, "end": 3226.42, "text": " Here, say it past the microphone, Jeremy.", "tokens": [1692, 11, 584, 309, 1791, 264, 10952, 11, 17809, 13], "temperature": 0.0, "avg_logprob": -0.2195583265654895, "compression_ratio": 1.2992700729927007, "no_speech_prob": 5.306875391397625e-05}, {"id": 616, "seek": 322642, "start": 3226.42, "end": 3230.42, "text": " Looks like we should have the same model of small operation.", "tokens": [10027, 411, 321, 820, 362, 264, 912, 2316, 295, 1359, 6916, 13], "temperature": 0.0, "avg_logprob": -0.1350127855936686, "compression_ratio": 1.6588235294117648, "no_speech_prob": 4.0689657907933e-05}, {"id": 617, "seek": 322642, "start": 3230.42, "end": 3235.42, "text": " Yes, it should be magic, but we could have a little bit more operations because of moving those blocks.", "tokens": [1079, 11, 309, 820, 312, 5585, 11, 457, 321, 727, 362, 257, 707, 857, 544, 7705, 570, 295, 2684, 729, 8474, 13], "temperature": 0.0, "avg_logprob": -0.1350127855936686, "compression_ratio": 1.6588235294117648, "no_speech_prob": 4.0689657907933e-05}, {"id": 618, "seek": 322642, "start": 3235.42, "end": 3237.42, "text": " But it's negligible.", "tokens": [583, 309, 311, 32570, 964, 13], "temperature": 0.0, "avg_logprob": -0.1350127855936686, "compression_ratio": 1.6588235294117648, "no_speech_prob": 4.0689657907933e-05}, {"id": 619, "seek": 322642, "start": 3237.42, "end": 3242.42, "text": " It's so in terms of the like actually doing like additions and multiplications, it's the same.", "tokens": [467, 311, 370, 294, 2115, 295, 264, 411, 767, 884, 411, 35113, 293, 17596, 763, 11, 309, 311, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.1350127855936686, "compression_ratio": 1.6588235294117648, "no_speech_prob": 4.0689657907933e-05}, {"id": 620, "seek": 322642, "start": 3242.42, "end": 3244.42, "text": " So it'll be N cubed again.", "tokens": [407, 309, 603, 312, 426, 36510, 797, 13], "temperature": 0.0, "avg_logprob": -0.1350127855936686, "compression_ratio": 1.6588235294117648, "no_speech_prob": 4.0689657907933e-05}, {"id": 621, "seek": 322642, "start": 3244.42, "end": 3247.42, "text": " And we'll talk about kind of the movement as the next question.", "tokens": [400, 321, 603, 751, 466, 733, 295, 264, 3963, 382, 264, 958, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1350127855936686, "compression_ratio": 1.6588235294117648, "no_speech_prob": 4.0689657907933e-05}, {"id": 622, "seek": 322642, "start": 3247.42, "end": 3254.42, "text": " But we've not changed our computational complexity.", "tokens": [583, 321, 600, 406, 3105, 527, 28270, 14024, 13], "temperature": 0.0, "avg_logprob": -0.1350127855936686, "compression_ratio": 1.6588235294117648, "no_speech_prob": 4.0689657907933e-05}, {"id": 623, "seek": 325442, "start": 3254.42, "end": 3262.42, "text": " So, yeah, next question. Now, how many reads and writes are made?", "tokens": [407, 11, 1338, 11, 958, 1168, 13, 823, 11, 577, 867, 15700, 293, 13657, 366, 1027, 30], "temperature": 0.0, "avg_logprob": -0.14766528529505576, "compression_ratio": 1.4144736842105263, "no_speech_prob": 3.668779299914604e-06}, {"id": 624, "seek": 325442, "start": 3262.42, "end": 3269.42, "text": " And here, like before, what the answer to kind of be element wise.", "tokens": [400, 510, 11, 411, 949, 11, 437, 264, 1867, 281, 733, 295, 312, 4478, 10829, 13], "temperature": 0.0, "avg_logprob": -0.14766528529505576, "compression_ratio": 1.4144736842105263, "no_speech_prob": 3.668779299914604e-06}, {"id": 625, "seek": 325442, "start": 3269.42, "end": 3275.42, "text": " And this will be in terms of both little n and big N. And I should note that here.", "tokens": [400, 341, 486, 312, 294, 2115, 295, 1293, 707, 297, 293, 955, 426, 13, 400, 286, 820, 3637, 300, 510, 13], "temperature": 0.0, "avg_logprob": -0.14766528529505576, "compression_ratio": 1.4144736842105263, "no_speech_prob": 3.668779299914604e-06}, {"id": 626, "seek": 327542, "start": 3275.42, "end": 3287.42, "text": " Your loops are just you're just looping through big N because you're kind of just going through the blocks.", "tokens": [2260, 16121, 366, 445, 291, 434, 445, 6367, 278, 807, 955, 426, 570, 291, 434, 733, 295, 445, 516, 807, 264, 8474, 13], "temperature": 0.0, "avg_logprob": -0.18018774258888373, "compression_ratio": 1.4896551724137932, "no_speech_prob": 1.644135954848025e-05}, {"id": 627, "seek": 327542, "start": 3287.42, "end": 3292.42, "text": " And you can talk to the person next to you about this as you think about it.", "tokens": [400, 291, 393, 751, 281, 264, 954, 958, 281, 291, 466, 341, 382, 291, 519, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.18018774258888373, "compression_ratio": 1.4896551724137932, "no_speech_prob": 1.644135954848025e-05}, {"id": 628, "seek": 327542, "start": 3292.42, "end": 3297.42, "text": " Oh, yes, Jeremy, you passed it.", "tokens": [876, 11, 2086, 11, 17809, 11, 291, 4678, 309, 13], "temperature": 0.0, "avg_logprob": -0.18018774258888373, "compression_ratio": 1.4896551724137932, "no_speech_prob": 1.644135954848025e-05}, {"id": 629, "seek": 329742, "start": 3297.42, "end": 3306.42, "text": " Before we answer this question, would you briefly explain why this is considered a very different method from the previous one?", "tokens": [4546, 321, 1867, 341, 1168, 11, 576, 291, 10515, 2903, 983, 341, 307, 4888, 257, 588, 819, 3170, 490, 264, 3894, 472, 30], "temperature": 0.0, "avg_logprob": -0.15037556648254394, "compression_ratio": 1.7131147540983607, "no_speech_prob": 1.7229911463800818e-05}, {"id": 630, "seek": 329742, "start": 3306.42, "end": 3314.42, "text": " Because it seems to me that we didn't seem to be obviously that this has done something very different.", "tokens": [1436, 309, 2544, 281, 385, 300, 321, 994, 380, 1643, 281, 312, 2745, 300, 341, 575, 1096, 746, 588, 819, 13], "temperature": 0.0, "avg_logprob": -0.15037556648254394, "compression_ratio": 1.7131147540983607, "no_speech_prob": 1.7229911463800818e-05}, {"id": 631, "seek": 329742, "start": 3314.42, "end": 3317.42, "text": " Yeah, so it's not very different.", "tokens": [865, 11, 370, 309, 311, 406, 588, 819, 13], "temperature": 0.0, "avg_logprob": -0.15037556648254394, "compression_ratio": 1.7131147540983607, "no_speech_prob": 1.7229911463800818e-05}, {"id": 632, "seek": 329742, "start": 3317.42, "end": 3326.42, "text": " It's different in that just the previous method, you're kind of thinking of A and B as a whole and dealing, you know, like row by row, column by column.", "tokens": [467, 311, 819, 294, 300, 445, 264, 3894, 3170, 11, 291, 434, 733, 295, 1953, 295, 316, 293, 363, 382, 257, 1379, 293, 6260, 11, 291, 458, 11, 411, 5386, 538, 5386, 11, 7738, 538, 7738, 13], "temperature": 0.0, "avg_logprob": -0.15037556648254394, "compression_ratio": 1.7131147540983607, "no_speech_prob": 1.7229911463800818e-05}, {"id": 633, "seek": 332642, "start": 3326.42, "end": 3330.42, "text": " Here, you've broken it into several small matrices.", "tokens": [1692, 11, 291, 600, 5463, 309, 666, 2940, 1359, 32284, 13], "temperature": 0.0, "avg_logprob": -0.059987434974083535, "compression_ratio": 1.6195652173913044, "no_speech_prob": 6.853981631138595e-06}, {"id": 634, "seek": 332642, "start": 3330.42, "end": 3338.42, "text": " And so kind of inside these loops, you're now thinking a small matrix at a time.", "tokens": [400, 370, 733, 295, 1854, 613, 16121, 11, 291, 434, 586, 1953, 257, 1359, 8141, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.059987434974083535, "compression_ratio": 1.6195652173913044, "no_speech_prob": 6.853981631138595e-06}, {"id": 635, "seek": 332642, "start": 3338.42, "end": 3345.42, "text": " And I've even kind of cheated here because I say block of A times block of B.", "tokens": [400, 286, 600, 754, 733, 295, 28079, 510, 570, 286, 584, 3461, 295, 316, 1413, 3461, 295, 363, 13], "temperature": 0.0, "avg_logprob": -0.059987434974083535, "compression_ratio": 1.6195652173913044, "no_speech_prob": 6.853981631138595e-06}, {"id": 636, "seek": 332642, "start": 3345.42, "end": 3353.42, "text": " Really doing a block of A times a block of B would be like calling the previous method.", "tokens": [4083, 884, 257, 3461, 295, 316, 1413, 257, 3461, 295, 363, 576, 312, 411, 5141, 264, 3894, 3170, 13], "temperature": 0.0, "avg_logprob": -0.059987434974083535, "compression_ratio": 1.6195652173913044, "no_speech_prob": 6.853981631138595e-06}, {"id": 637, "seek": 335342, "start": 3353.42, "end": 3357.42, "text": " I guess if the answer to question two turns out to be different, then that's the answer.", "tokens": [286, 2041, 498, 264, 1867, 281, 1168, 732, 4523, 484, 281, 312, 819, 11, 550, 300, 311, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.11653429422623073, "compression_ratio": 1.5561497326203209, "no_speech_prob": 2.466961814207025e-05}, {"id": 638, "seek": 335342, "start": 3357.42, "end": 3360.42, "text": " Oh, that's true. Yeah. So we are.", "tokens": [876, 11, 300, 311, 2074, 13, 865, 13, 407, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.11653429422623073, "compression_ratio": 1.5561497326203209, "no_speech_prob": 2.466961814207025e-05}, {"id": 639, "seek": 335342, "start": 3360.42, "end": 3364.42, "text": " Spoiler alert, we are going to see that there's a different number of reads and writes being made.", "tokens": [45011, 5441, 9615, 11, 321, 366, 516, 281, 536, 300, 456, 311, 257, 819, 1230, 295, 15700, 293, 13657, 885, 1027, 13], "temperature": 0.0, "avg_logprob": -0.11653429422623073, "compression_ratio": 1.5561497326203209, "no_speech_prob": 2.466961814207025e-05}, {"id": 640, "seek": 335342, "start": 3364.42, "end": 3367.42, "text": " And then we'll talk about some other benefits of doing this approach.", "tokens": [400, 550, 321, 603, 751, 466, 512, 661, 5311, 295, 884, 341, 3109, 13], "temperature": 0.0, "avg_logprob": -0.11653429422623073, "compression_ratio": 1.5561497326203209, "no_speech_prob": 2.466961814207025e-05}, {"id": 641, "seek": 336742, "start": 3367.42, "end": 3396.42, "text": " Yeah. I was just saying from an algorithm perspective.", "tokens": [865, 13, 286, 390, 445, 1566, 490, 364, 9284, 4585, 13], "temperature": 0.0, "avg_logprob": -0.18786994616190592, "compression_ratio": 0.8709677419354839, "no_speech_prob": 7.718816050328314e-05}, {"id": 642, "seek": 339642, "start": 3396.42, "end": 3423.42, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.6763106187184652, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.010255927219986916}, {"id": 643, "seek": 342342, "start": 3423.42, "end": 3450.42, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5691272417704264, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.010400104336440563}, {"id": 644, "seek": 345042, "start": 3450.42, "end": 3468.42, "text": " Who wants more time?", "tokens": [2102, 2738, 544, 565, 30], "temperature": 0.0, "avg_logprob": -0.17234005246843612, "compression_ratio": 0.9852941176470589, "no_speech_prob": 1.2482122428991715e-06}, {"id": 645, "seek": 345042, "start": 3468.42, "end": 3477.42, "text": " Does anyone have an answer they want to share?", "tokens": [4402, 2878, 362, 364, 1867, 436, 528, 281, 2073, 30], "temperature": 0.0, "avg_logprob": -0.17234005246843612, "compression_ratio": 0.9852941176470589, "no_speech_prob": 1.2482122428991715e-06}, {"id": 646, "seek": 347742, "start": 3477.42, "end": 3481.42, "text": " Vincent.", "tokens": [28003, 13], "temperature": 0.0, "avg_logprob": -0.18566083272298178, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.00014198124699760228}, {"id": 647, "seek": 347742, "start": 3481.42, "end": 3485.42, "text": " Yeah. So there's three for loops with big N each.", "tokens": [865, 13, 407, 456, 311, 1045, 337, 16121, 365, 955, 426, 1184, 13], "temperature": 0.0, "avg_logprob": -0.18566083272298178, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.00014198124699760228}, {"id": 648, "seek": 347742, "start": 3485.42, "end": 3494.42, "text": " So if you're just doing like an operation on your linear time within those three for loops, it would be a big N cubed.", "tokens": [407, 498, 291, 434, 445, 884, 411, 364, 6916, 322, 428, 8213, 565, 1951, 729, 1045, 337, 16121, 11, 309, 576, 312, 257, 955, 426, 36510, 13], "temperature": 0.0, "avg_logprob": -0.18566083272298178, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.00014198124699760228}, {"id": 649, "seek": 347742, "start": 3494.42, "end": 3502.42, "text": " But since you're reading a block in each of those and the block sort of says little n over big N,", "tokens": [583, 1670, 291, 434, 3760, 257, 3461, 294, 1184, 295, 729, 293, 264, 3461, 1333, 295, 1619, 707, 297, 670, 955, 426, 11], "temperature": 0.0, "avg_logprob": -0.18566083272298178, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.00014198124699760228}, {"id": 650, "seek": 350242, "start": 3502.42, "end": 3507.42, "text": " then it winds up being big N squared times the one for the number of reads.", "tokens": [550, 309, 17765, 493, 885, 955, 426, 8889, 1413, 264, 472, 337, 264, 1230, 295, 15700, 13], "temperature": 0.0, "avg_logprob": -0.16758650371006556, "compression_ratio": 1.17, "no_speech_prob": 4.9854097596835345e-05}, {"id": 651, "seek": 350242, "start": 3507.42, "end": 3524.42, "text": " Yes, exactly. Yes. Let me write that out.", "tokens": [1079, 11, 2293, 13, 1079, 13, 961, 385, 2464, 300, 484, 13], "temperature": 0.0, "avg_logprob": -0.16758650371006556, "compression_ratio": 1.17, "no_speech_prob": 4.9854097596835345e-05}, {"id": 652, "seek": 352442, "start": 3524.42, "end": 3535.42, "text": " Okay. So let me I guess I'll use my own ID. I was trying to pull up the thing I could write on the screen.", "tokens": [1033, 13, 407, 718, 385, 286, 2041, 286, 603, 764, 452, 1065, 7348, 13, 286, 390, 1382, 281, 2235, 493, 264, 551, 286, 727, 2464, 322, 264, 2568, 13], "temperature": 0.0, "avg_logprob": -0.27670782665873683, "compression_ratio": 1.2363636363636363, "no_speech_prob": 1.3211608347774018e-05}, {"id": 653, "seek": 352442, "start": 3535.42, "end": 3543.42, "text": " So let me delete what I have.", "tokens": [407, 718, 385, 12097, 437, 286, 362, 13], "temperature": 0.0, "avg_logprob": -0.27670782665873683, "compression_ratio": 1.2363636363636363, "no_speech_prob": 1.3211608347774018e-05}, {"id": 654, "seek": 354342, "start": 3543.42, "end": 3556.42, "text": " Yeah. So as Vincent said, it's important to remember our blocks are of size little n over big N by little n over big N.", "tokens": [865, 13, 407, 382, 28003, 848, 11, 309, 311, 1021, 281, 1604, 527, 8474, 366, 295, 2744, 707, 297, 670, 955, 426, 538, 707, 297, 670, 955, 426, 13], "temperature": 0.0, "avg_logprob": -0.05697724554273817, "compression_ratio": 1.5739644970414202, "no_speech_prob": 7.646262929483783e-06}, {"id": 655, "seek": 354342, "start": 3556.42, "end": 3563.42, "text": " So that's little n over big N squared is the amount of work.", "tokens": [407, 300, 311, 707, 297, 670, 955, 426, 8889, 307, 264, 2372, 295, 589, 13], "temperature": 0.0, "avg_logprob": -0.05697724554273817, "compression_ratio": 1.5739644970414202, "no_speech_prob": 7.646262929483783e-06}, {"id": 656, "seek": 354342, "start": 3563.42, "end": 3571.42, "text": " And then we've got big N cubed worth of loops since we have those three nested loops.", "tokens": [400, 550, 321, 600, 658, 955, 426, 36510, 3163, 295, 16121, 1670, 321, 362, 729, 1045, 15646, 292, 16121, 13], "temperature": 0.0, "avg_logprob": -0.05697724554273817, "compression_ratio": 1.5739644970414202, "no_speech_prob": 7.646262929483783e-06}, {"id": 657, "seek": 357142, "start": 3571.42, "end": 3580.42, "text": " And this reduces to big N times little n squared.", "tokens": [400, 341, 18081, 281, 955, 426, 1413, 707, 297, 8889, 13], "temperature": 0.0, "avg_logprob": -0.15218072542002503, "compression_ratio": 1.515527950310559, "no_speech_prob": 4.356839781394228e-06}, {"id": 658, "seek": 357142, "start": 3580.42, "end": 3587.42, "text": " And you'll have. Yeah. And it's two times as Tim indicated. So you're doing that for a and then also for B.", "tokens": [400, 291, 603, 362, 13, 865, 13, 400, 309, 311, 732, 1413, 382, 7172, 16176, 13, 407, 291, 434, 884, 300, 337, 257, 293, 550, 611, 337, 363, 13], "temperature": 0.0, "avg_logprob": -0.15218072542002503, "compression_ratio": 1.515527950310559, "no_speech_prob": 4.356839781394228e-06}, {"id": 659, "seek": 357142, "start": 3587.42, "end": 3592.42, "text": " So I'll just multiply by two.", "tokens": [407, 286, 603, 445, 12972, 538, 732, 13], "temperature": 0.0, "avg_logprob": -0.15218072542002503, "compression_ratio": 1.515527950310559, "no_speech_prob": 4.356839781394228e-06}, {"id": 660, "seek": 357142, "start": 3592.42, "end": 3597.42, "text": " And then C is still going to be little n squared writes.", "tokens": [400, 550, 383, 307, 920, 516, 281, 312, 707, 297, 8889, 13657, 13], "temperature": 0.0, "avg_logprob": -0.15218072542002503, "compression_ratio": 1.515527950310559, "no_speech_prob": 4.356839781394228e-06}, {"id": 661, "seek": 359742, "start": 3597.42, "end": 3603.42, "text": " So this is an improvement in the amount of reads from going between slow and fast memory that we have to do,", "tokens": [407, 341, 307, 364, 10444, 294, 264, 2372, 295, 15700, 490, 516, 1296, 2964, 293, 2370, 4675, 300, 321, 362, 281, 360, 11], "temperature": 0.0, "avg_logprob": -0.06830694364464801, "compression_ratio": 1.5892116182572613, "no_speech_prob": 4.029377578262938e-06}, {"id": 662, "seek": 359742, "start": 3603.42, "end": 3608.42, "text": " because we're assuming that big N is substantially less than little n.", "tokens": [570, 321, 434, 11926, 300, 955, 426, 307, 30797, 1570, 813, 707, 297, 13], "temperature": 0.0, "avg_logprob": -0.06830694364464801, "compression_ratio": 1.5892116182572613, "no_speech_prob": 4.029377578262938e-06}, {"id": 663, "seek": 359742, "start": 3608.42, "end": 3612.42, "text": " It's the number of blocks that we've made.", "tokens": [467, 311, 264, 1230, 295, 8474, 300, 321, 600, 1027, 13], "temperature": 0.0, "avg_logprob": -0.06830694364464801, "compression_ratio": 1.5892116182572613, "no_speech_prob": 4.029377578262938e-06}, {"id": 664, "seek": 359742, "start": 3612.42, "end": 3615.42, "text": " So this kind of illustrates. Yeah.", "tokens": [407, 341, 733, 295, 41718, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.06830694364464801, "compression_ratio": 1.5892116182572613, "no_speech_prob": 4.029377578262938e-06}, {"id": 665, "seek": 359742, "start": 3615.42, "end": 3623.42, "text": " One big benefit of block matrix multiplication is you've really reduced how much you have to kind of read from slower memory.", "tokens": [1485, 955, 5121, 295, 3461, 8141, 27290, 307, 291, 600, 534, 9212, 577, 709, 291, 362, 281, 733, 295, 1401, 490, 14009, 4675, 13], "temperature": 0.0, "avg_logprob": -0.06830694364464801, "compression_ratio": 1.5892116182572613, "no_speech_prob": 4.029377578262938e-06}, {"id": 666, "seek": 362342, "start": 3623.42, "end": 3631.42, "text": " And you're kind of taking better or the reason behind this is you're taking better advantage of locality of kind of when you're pulling things in a block.", "tokens": [400, 291, 434, 733, 295, 1940, 1101, 420, 264, 1778, 2261, 341, 307, 291, 434, 1940, 1101, 5002, 295, 1628, 1860, 295, 733, 295, 562, 291, 434, 8407, 721, 294, 257, 3461, 13], "temperature": 0.0, "avg_logprob": -0.1071095910183219, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.9033678881896776e-06}, {"id": 667, "seek": 362342, "start": 3631.42, "end": 3636.42, "text": " You're getting to kind of more fully utilize them than before with B.", "tokens": [509, 434, 1242, 281, 733, 295, 544, 4498, 16117, 552, 813, 949, 365, 363, 13], "temperature": 0.0, "avg_logprob": -0.1071095910183219, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.9033678881896776e-06}, {"id": 668, "seek": 362342, "start": 3636.42, "end": 3645.42, "text": " It was like we're pulling a column in of B only using it once and then like throwing it back and pulling in a different column of B.", "tokens": [467, 390, 411, 321, 434, 8407, 257, 7738, 294, 295, 363, 787, 1228, 309, 1564, 293, 550, 411, 10238, 309, 646, 293, 8407, 294, 257, 819, 7738, 295, 363, 13], "temperature": 0.0, "avg_logprob": -0.1071095910183219, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.9033678881896776e-06}, {"id": 669, "seek": 364542, "start": 3645.42, "end": 3655.42, "text": " Other questions about this?", "tokens": [5358, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.15195350646972655, "compression_ratio": 1.2478632478632479, "no_speech_prob": 3.1380743621411966e-06}, {"id": 670, "seek": 364542, "start": 3655.42, "end": 3662.42, "text": " Can anyone think of what another benefit of using block matrix multiplication might be?", "tokens": [1664, 2878, 519, 295, 437, 1071, 5121, 295, 1228, 3461, 8141, 27290, 1062, 312, 30], "temperature": 0.0, "avg_logprob": -0.15195350646972655, "compression_ratio": 1.2478632478632479, "no_speech_prob": 3.1380743621411966e-06}, {"id": 671, "seek": 364542, "start": 3662.42, "end": 3670.42, "text": " So this shows better locality.", "tokens": [407, 341, 3110, 1101, 1628, 1860, 13], "temperature": 0.0, "avg_logprob": -0.15195350646972655, "compression_ratio": 1.2478632478632479, "no_speech_prob": 3.1380743621411966e-06}, {"id": 672, "seek": 367042, "start": 3670.42, "end": 3684.42, "text": " Kelsey?", "tokens": [44714, 30], "temperature": 0.0, "avg_logprob": -0.28064991746629986, "compression_ratio": 0.7777777777777778, "no_speech_prob": 5.2249710279284045e-05}, {"id": 673, "seek": 367042, "start": 3684.42, "end": 3685.42, "text": " That's a good point.", "tokens": [663, 311, 257, 665, 935, 13], "temperature": 0.0, "avg_logprob": -0.28064991746629986, "compression_ratio": 0.7777777777777778, "no_speech_prob": 5.2249710279284045e-05}, {"id": 674, "seek": 368542, "start": 3685.42, "end": 3700.42, "text": " So that's actually possible with ordinary matrix multiplication as well.", "tokens": [407, 300, 311, 767, 1944, 365, 10547, 8141, 27290, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.16885981012563236, "compression_ratio": 1.4465408805031446, "no_speech_prob": 7.88907527748961e-06}, {"id": 675, "seek": 368542, "start": 3700.42, "end": 3706.42, "text": " Oh, that's true. Yeah. Yeah. You could ignore blocks that are zero.", "tokens": [876, 11, 300, 311, 2074, 13, 865, 13, 865, 13, 509, 727, 11200, 8474, 300, 366, 4018, 13], "temperature": 0.0, "avg_logprob": -0.16885981012563236, "compression_ratio": 1.4465408805031446, "no_speech_prob": 7.88907527748961e-06}, {"id": 676, "seek": 368542, "start": 3706.42, "end": 3710.42, "text": " Yeah, I like that you're thinking about sparse matrices. Tim?", "tokens": [865, 11, 286, 411, 300, 291, 434, 1953, 466, 637, 11668, 32284, 13, 7172, 30], "temperature": 0.0, "avg_logprob": -0.16885981012563236, "compression_ratio": 1.4465408805031446, "no_speech_prob": 7.88907527748961e-06}, {"id": 677, "seek": 368542, "start": 3710.42, "end": 3711.42, "text": " You could parallelize this?", "tokens": [509, 727, 8952, 1125, 341, 30], "temperature": 0.0, "avg_logprob": -0.16885981012563236, "compression_ratio": 1.4465408805031446, "no_speech_prob": 7.88907527748961e-06}, {"id": 678, "seek": 371142, "start": 3711.42, "end": 3717.42, "text": " Yes. Yeah. So I was thinking you could parallelize this more easily.", "tokens": [1079, 13, 865, 13, 407, 286, 390, 1953, 291, 727, 8952, 1125, 341, 544, 3612, 13], "temperature": 0.0, "avg_logprob": -0.128822254412102, "compression_ratio": 1.4166666666666667, "no_speech_prob": 3.6477689718594775e-05}, {"id": 679, "seek": 371142, "start": 3717.42, "end": 3722.42, "text": " Because this is kind of already clearly separated.", "tokens": [1436, 341, 307, 733, 295, 1217, 4448, 12005, 13], "temperature": 0.0, "avg_logprob": -0.128822254412102, "compression_ratio": 1.4166666666666667, "no_speech_prob": 3.6477689718594775e-05}, {"id": 680, "seek": 371142, "start": 3722.42, "end": 3724.42, "text": " Memory?", "tokens": [38203, 30], "temperature": 0.0, "avg_logprob": -0.128822254412102, "compression_ratio": 1.4166666666666667, "no_speech_prob": 3.6477689718594775e-05}, {"id": 681, "seek": 371142, "start": 3724.42, "end": 3727.42, "text": " What about memory?", "tokens": [708, 466, 4675, 30], "temperature": 0.0, "avg_logprob": -0.128822254412102, "compression_ratio": 1.4166666666666667, "no_speech_prob": 3.6477689718594775e-05}, {"id": 682, "seek": 371142, "start": 3727.42, "end": 3732.42, "text": " You never actually read the whole matrix into memory.", "tokens": [509, 1128, 767, 1401, 264, 1379, 8141, 666, 4675, 13], "temperature": 0.0, "avg_logprob": -0.128822254412102, "compression_ratio": 1.4166666666666667, "no_speech_prob": 3.6477689718594775e-05}, {"id": 683, "seek": 371142, "start": 3732.42, "end": 3738.42, "text": " So Tim's point is basically another scalability thing.", "tokens": [407, 7172, 311, 935, 307, 1936, 1071, 15664, 2310, 551, 13], "temperature": 0.0, "avg_logprob": -0.128822254412102, "compression_ratio": 1.4166666666666667, "no_speech_prob": 3.6477689718594775e-05}, {"id": 684, "seek": 373842, "start": 3738.42, "end": 3744.42, "text": " Yeah, if you had a giant matrix where even reading in a single row is excessive.", "tokens": [865, 11, 498, 291, 632, 257, 7410, 8141, 689, 754, 3760, 294, 257, 2167, 5386, 307, 22704, 13], "temperature": 0.0, "avg_logprob": -0.22927518476519668, "compression_ratio": 1.4679487179487178, "no_speech_prob": 3.6476612876867875e-05}, {"id": 685, "seek": 373842, "start": 3744.42, "end": 3754.42, "text": " Just another thing, I'm thinking this in terms of Python, because you can take advantage of vectorization I think in this respect.", "tokens": [1449, 1071, 551, 11, 286, 478, 1953, 341, 294, 2115, 295, 15329, 11, 570, 291, 393, 747, 5002, 295, 8062, 2144, 286, 519, 294, 341, 3104, 13], "temperature": 0.0, "avg_logprob": -0.22927518476519668, "compression_ratio": 1.4679487179487178, "no_speech_prob": 3.6476612876867875e-05}, {"id": 686, "seek": 373842, "start": 3754.42, "end": 3759.42, "text": " Of vectorization.", "tokens": [2720, 8062, 2144, 13], "temperature": 0.0, "avg_logprob": -0.22927518476519668, "compression_ratio": 1.4679487179487178, "no_speech_prob": 3.6476612876867875e-05}, {"id": 687, "seek": 375942, "start": 3759.42, "end": 3769.42, "text": " Because we have block i, j, f, c plus block of a. Instead of in the original iteration, you're adding each entry one by one.", "tokens": [1436, 321, 362, 3461, 741, 11, 361, 11, 283, 11, 269, 1804, 3461, 295, 257, 13, 7156, 295, 294, 264, 3380, 24784, 11, 291, 434, 5127, 1184, 8729, 472, 538, 472, 13], "temperature": 0.0, "avg_logprob": -0.2962323143368676, "compression_ratio": 1.603448275862069, "no_speech_prob": 6.500390009023249e-05}, {"id": 688, "seek": 375942, "start": 3769.42, "end": 3771.42, "text": " Because you're calling it all at once.", "tokens": [1436, 291, 434, 5141, 309, 439, 412, 1564, 13], "temperature": 0.0, "avg_logprob": -0.2962323143368676, "compression_ratio": 1.603448275862069, "no_speech_prob": 6.500390009023249e-05}, {"id": 689, "seek": 375942, "start": 3771.42, "end": 3772.42, "text": " Oh, yes. Yes. Yeah.", "tokens": [876, 11, 2086, 13, 1079, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.2962323143368676, "compression_ratio": 1.603448275862069, "no_speech_prob": 6.500390009023249e-05}, {"id": 690, "seek": 375942, "start": 3772.42, "end": 3774.42, "text": " It's faster because you're calling the underlying.", "tokens": [467, 311, 4663, 570, 291, 434, 5141, 264, 14217, 13], "temperature": 0.0, "avg_logprob": -0.2962323143368676, "compression_ratio": 1.603448275862069, "no_speech_prob": 6.500390009023249e-05}, {"id": 691, "seek": 375942, "start": 3774.42, "end": 3778.42, "text": " Yeah, that's a great point. Yeah. Thank you.", "tokens": [865, 11, 300, 311, 257, 869, 935, 13, 865, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.2962323143368676, "compression_ratio": 1.603448275862069, "no_speech_prob": 6.500390009023249e-05}, {"id": 692, "seek": 377842, "start": 3778.42, "end": 3792.42, "text": " I think the vector is the original one, just to say so. Like this section of a row times this section of a column.", "tokens": [286, 519, 264, 8062, 307, 264, 3380, 472, 11, 445, 281, 584, 370, 13, 1743, 341, 3541, 295, 257, 5386, 1413, 341, 3541, 295, 257, 7738, 13], "temperature": 0.0, "avg_logprob": -0.23865141691984953, "compression_ratio": 1.5203252032520325, "no_speech_prob": 4.331125819589943e-05}, {"id": 693, "seek": 377842, "start": 3792.42, "end": 3793.42, "text": " We'll have to think about it.", "tokens": [492, 603, 362, 281, 519, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.23865141691984953, "compression_ratio": 1.5203252032520325, "no_speech_prob": 4.331125819589943e-05}, {"id": 694, "seek": 377842, "start": 3793.42, "end": 3798.42, "text": " Yeah, we'll have to think more about this.", "tokens": [865, 11, 321, 603, 362, 281, 519, 544, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.23865141691984953, "compression_ratio": 1.5203252032520325, "no_speech_prob": 4.331125819589943e-05}, {"id": 695, "seek": 379842, "start": 3798.42, "end": 3811.42, "text": " Thank you. I was going to say we're a little bit late for our break, so let's go ahead and take a break now for seven minutes. I'll be back at 12 13.", "tokens": [1044, 291, 13, 286, 390, 516, 281, 584, 321, 434, 257, 707, 857, 3469, 337, 527, 1821, 11, 370, 718, 311, 352, 2286, 293, 747, 257, 1821, 586, 337, 3407, 2077, 13, 286, 603, 312, 646, 412, 2272, 3705, 13], "temperature": 0.0, "avg_logprob": -0.16718420386314392, "compression_ratio": 1.3892215568862276, "no_speech_prob": 2.429821870464366e-05}, {"id": 696, "seek": 379842, "start": 3811.42, "end": 3813.42, "text": " I'm going to start back up.", "tokens": [286, 478, 516, 281, 722, 646, 493, 13], "temperature": 0.0, "avg_logprob": -0.16718420386314392, "compression_ratio": 1.3892215568862276, "no_speech_prob": 2.429821870464366e-05}, {"id": 697, "seek": 379842, "start": 3813.42, "end": 3823.42, "text": " Any final questions about block matrix multiplication?", "tokens": [2639, 2572, 1651, 466, 3461, 8141, 27290, 30], "temperature": 0.0, "avg_logprob": -0.16718420386314392, "compression_ratio": 1.3892215568862276, "no_speech_prob": 2.429821870464366e-05}, {"id": 698, "seek": 382342, "start": 3823.42, "end": 3831.42, "text": " And I also wanted to remind you again, I forgot to say this at the beginning of class, but if you haven't filled out the mid course feedback survey, please do that today.", "tokens": [400, 286, 611, 1415, 281, 4160, 291, 797, 11, 286, 5298, 281, 584, 341, 412, 264, 2863, 295, 1508, 11, 457, 498, 291, 2378, 380, 6412, 484, 264, 2062, 1164, 5824, 8984, 11, 1767, 360, 300, 965, 13], "temperature": 0.0, "avg_logprob": -0.14186581698330966, "compression_ratio": 1.4631578947368422, "no_speech_prob": 0.000171207488165237}, {"id": 699, "seek": 382342, "start": 3831.42, "end": 3838.42, "text": " And the link for that is in the Slack channel.", "tokens": [400, 264, 2113, 337, 300, 307, 294, 264, 37211, 2269, 13], "temperature": 0.0, "avg_logprob": -0.14186581698330966, "compression_ratio": 1.4631578947368422, "no_speech_prob": 0.000171207488165237}, {"id": 700, "seek": 382342, "start": 3838.42, "end": 3842.42, "text": " Oh, yes.", "tokens": [876, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.14186581698330966, "compression_ratio": 1.4631578947368422, "no_speech_prob": 0.000171207488165237}, {"id": 701, "seek": 382342, "start": 3842.42, "end": 3844.42, "text": " Valentine and I have come up with a question.", "tokens": [24359, 293, 286, 362, 808, 493, 365, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.14186581698330966, "compression_ratio": 1.4631578947368422, "no_speech_prob": 0.000171207488165237}, {"id": 702, "seek": 382342, "start": 3844.42, "end": 3845.42, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.14186581698330966, "compression_ratio": 1.4631578947368422, "no_speech_prob": 0.000171207488165237}, {"id": 703, "seek": 384542, "start": 3845.42, "end": 3867.42, "text": " Just in terms of difference between vectorization and parallelization. So my understanding is parallelization is more like divide the course into computing tasks, while vectorization is more like computing some number in kind of matrix kind of form.", "tokens": [1449, 294, 2115, 295, 2649, 1296, 8062, 2144, 293, 8952, 2144, 13, 407, 452, 3701, 307, 8952, 2144, 307, 544, 411, 9845, 264, 1164, 666, 15866, 9608, 11, 1339, 8062, 2144, 307, 544, 411, 15866, 512, 1230, 294, 733, 295, 8141, 733, 295, 1254, 13], "temperature": 0.0, "avg_logprob": -0.16350755886155732, "compression_ratio": 1.6274509803921569, "no_speech_prob": 4.3986045056954026e-05}, {"id": 704, "seek": 386742, "start": 3867.42, "end": 3877.42, "text": " Yes, vectorization often refers to SIMD, which is single instruction multiple data. But so yeah, it's kind of like you have a few.", "tokens": [1079, 11, 8062, 2144, 2049, 14942, 281, 24738, 35, 11, 597, 307, 2167, 10951, 3866, 1412, 13, 583, 370, 1338, 11, 309, 311, 733, 295, 411, 291, 362, 257, 1326, 13], "temperature": 0.0, "avg_logprob": -0.16428355316617596, "compression_ratio": 1.5029585798816567, "no_speech_prob": 3.943568663089536e-05}, {"id": 705, "seek": 386742, "start": 3877.42, "end": 3885.42, "text": " Yeah, like a little vector that you're doing kind of like the same operation to, but yeah, it's kind of next to each other.", "tokens": [865, 11, 411, 257, 707, 8062, 300, 291, 434, 884, 733, 295, 411, 264, 912, 6916, 281, 11, 457, 1338, 11, 309, 311, 733, 295, 958, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.16428355316617596, "compression_ratio": 1.5029585798816567, "no_speech_prob": 3.943568663089536e-05}, {"id": 706, "seek": 388542, "start": 3885.42, "end": 3900.42, "text": " It turns out that modern processes have these process level instructions that can operate on four or eight things at a time on a single call. And that's vectorization.", "tokens": [467, 4523, 484, 300, 4363, 7555, 362, 613, 1399, 1496, 9415, 300, 393, 9651, 322, 1451, 420, 3180, 721, 412, 257, 565, 322, 257, 2167, 818, 13, 400, 300, 311, 8062, 2144, 13], "temperature": 0.0, "avg_logprob": -0.23674993257264834, "compression_ratio": 1.3688524590163935, "no_speech_prob": 1.2217778930789791e-05}, {"id": 707, "seek": 390042, "start": 3900.42, "end": 3917.42, "text": " So normally processes can just take a number plus a number in one cycle with SIMD, modern processes in the last few years, we might take out a four or eight numbers and add them all at the same time in a single cycle, a single instruction.", "tokens": [407, 5646, 7555, 393, 445, 747, 257, 1230, 1804, 257, 1230, 294, 472, 6586, 365, 24738, 35, 11, 4363, 7555, 294, 264, 1036, 1326, 924, 11, 321, 1062, 747, 484, 257, 1451, 420, 3180, 3547, 293, 909, 552, 439, 412, 264, 912, 565, 294, 257, 2167, 6586, 11, 257, 2167, 10951, 13], "temperature": 0.0, "avg_logprob": -0.2835597097873688, "compression_ratio": 1.5238095238095237, "no_speech_prob": 1.3843919077771716e-05}, {"id": 708, "seek": 390042, "start": 3917.42, "end": 3919.42, "text": " And that's SIMD.", "tokens": [400, 300, 311, 24738, 35, 13], "temperature": 0.0, "avg_logprob": -0.2835597097873688, "compression_ratio": 1.5238095238095237, "no_speech_prob": 1.3843919077771716e-05}, {"id": 709, "seek": 391942, "start": 3919.42, "end": 3937.42, "text": " Right. Yeah. And then as you said, and SIMD is often described as vectorization. And then as you said, parallelization, you have kind of different cores handling, handling the processes, or even different computers.", "tokens": [1779, 13, 865, 13, 400, 550, 382, 291, 848, 11, 293, 24738, 35, 307, 2049, 7619, 382, 8062, 2144, 13, 400, 550, 382, 291, 848, 11, 8952, 2144, 11, 291, 362, 733, 295, 819, 24826, 13175, 11, 13175, 264, 7555, 11, 420, 754, 819, 10807, 13], "temperature": 0.0, "avg_logprob": -0.1536012840270996, "compression_ratio": 1.4726027397260273, "no_speech_prob": 4.936639470543014e-06}, {"id": 710, "seek": 393742, "start": 3937.42, "end": 3952.42, "text": " So is it correct to say that like for vectorization, we write it in a vectorized form, and then we hope then that the compiler or whatever will parallelize it on the hardware level.", "tokens": [407, 307, 309, 3006, 281, 584, 300, 411, 337, 8062, 2144, 11, 321, 2464, 309, 294, 257, 8062, 1602, 1254, 11, 293, 550, 321, 1454, 550, 300, 264, 31958, 420, 2035, 486, 8952, 1125, 309, 322, 264, 8837, 1496, 13], "temperature": 0.0, "avg_logprob": -0.16015201702452542, "compression_ratio": 1.6153846153846154, "no_speech_prob": 9.368103746965062e-06}, {"id": 711, "seek": 393742, "start": 3952.42, "end": 3957.42, "text": " But for parallelization, we make it parallelized.", "tokens": [583, 337, 8952, 2144, 11, 321, 652, 309, 8952, 1602, 13], "temperature": 0.0, "avg_logprob": -0.16015201702452542, "compression_ratio": 1.6153846153846154, "no_speech_prob": 9.368103746965062e-06}, {"id": 712, "seek": 395742, "start": 3957.42, "end": 3968.42, "text": " Yeah, so NumPy handles vectorization for you. Yeah, so we don't have to usually explicitly vectorize it because NumPy is doing that for us.", "tokens": [865, 11, 370, 22592, 47, 88, 18722, 8062, 2144, 337, 291, 13, 865, 11, 370, 321, 500, 380, 362, 281, 2673, 20803, 8062, 1125, 309, 570, 22592, 47, 88, 307, 884, 300, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.10592883566151494, "compression_ratio": 1.5851528384279476, "no_speech_prob": 8.664371307531837e-06}, {"id": 713, "seek": 395742, "start": 3968.42, "end": 3972.42, "text": " Whereas you have parallelization, you are having to write out more explicitly.", "tokens": [13813, 291, 362, 8952, 2144, 11, 291, 366, 1419, 281, 2464, 484, 544, 20803, 13], "temperature": 0.0, "avg_logprob": -0.10592883566151494, "compression_ratio": 1.5851528384279476, "no_speech_prob": 8.664371307531837e-06}, {"id": 714, "seek": 395742, "start": 3972.42, "end": 3978.42, "text": " But it would kind of vectorization, it depends what libraries you're using, like if they automatically do that.", "tokens": [583, 309, 576, 733, 295, 8062, 2144, 11, 309, 5946, 437, 15148, 291, 434, 1228, 11, 411, 498, 436, 6772, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.10592883566151494, "compression_ratio": 1.5851528384279476, "no_speech_prob": 8.664371307531837e-06}, {"id": 715, "seek": 395742, "start": 3978.42, "end": 3979.42, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.10592883566151494, "compression_ratio": 1.5851528384279476, "no_speech_prob": 8.664371307531837e-06}, {"id": 716, "seek": 395742, "start": 3979.42, "end": 3985.42, "text": " Welcome. Good questions.", "tokens": [4027, 13, 2205, 1651, 13], "temperature": 0.0, "avg_logprob": -0.10592883566151494, "compression_ratio": 1.5851528384279476, "no_speech_prob": 8.664371307531837e-06}, {"id": 717, "seek": 398542, "start": 3985.42, "end": 3987.42, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.09847219080864629, "compression_ratio": 1.6281407035175879, "no_speech_prob": 4.399733734317124e-05}, {"id": 718, "seek": 398542, "start": 3987.42, "end": 3997.42, "text": " Now we're going to be starting a new lesson, Notebook 4. So compressed sensing of CT scans with robust regression.", "tokens": [823, 321, 434, 516, 281, 312, 2891, 257, 777, 6898, 11, 11633, 2939, 1017, 13, 407, 30353, 30654, 295, 19529, 35116, 365, 13956, 24590, 13], "temperature": 0.0, "avg_logprob": -0.09847219080864629, "compression_ratio": 1.6281407035175879, "no_speech_prob": 4.399733734317124e-05}, {"id": 719, "seek": 398542, "start": 3997.42, "end": 4007.42, "text": " And we're going to start kind of before we get into the CT scans or compressed sensing with just a few foundational concepts that show up a lot of places.", "tokens": [400, 321, 434, 516, 281, 722, 733, 295, 949, 321, 483, 666, 264, 19529, 35116, 420, 30353, 30654, 365, 445, 257, 1326, 32195, 10392, 300, 855, 493, 257, 688, 295, 3190, 13], "temperature": 0.0, "avg_logprob": -0.09847219080864629, "compression_ratio": 1.6281407035175879, "no_speech_prob": 4.399733734317124e-05}, {"id": 720, "seek": 398542, "start": 4007.42, "end": 4010.42, "text": " And the first one of these is broadcasting.", "tokens": [400, 264, 700, 472, 295, 613, 307, 30024, 13], "temperature": 0.0, "avg_logprob": -0.09847219080864629, "compression_ratio": 1.6281407035175879, "no_speech_prob": 4.399733734317124e-05}, {"id": 721, "seek": 401042, "start": 4010.42, "end": 4016.42, "text": " And the term broadcasting actually originated with NumPy, although it's now used by a bunch of other libraries.", "tokens": [400, 264, 1433, 30024, 767, 31129, 365, 22592, 47, 88, 11, 4878, 309, 311, 586, 1143, 538, 257, 3840, 295, 661, 15148, 13], "temperature": 0.0, "avg_logprob": -0.06467754311031765, "compression_ratio": 1.4885844748858448, "no_speech_prob": 2.3922193577163853e-05}, {"id": 722, "seek": 401042, "start": 4016.42, "end": 4023.42, "text": " And it describes how arrays with different shapes are treated during arithmetic operations.", "tokens": [400, 309, 15626, 577, 41011, 365, 819, 10854, 366, 8668, 1830, 42973, 7705, 13], "temperature": 0.0, "avg_logprob": -0.06467754311031765, "compression_ratio": 1.4885844748858448, "no_speech_prob": 2.3922193577163853e-05}, {"id": 723, "seek": 401042, "start": 4023.42, "end": 4029.42, "text": " And so some of this, I think you've already seen and maybe haven't thought about because a lot of it feels very intuitive.", "tokens": [400, 370, 512, 295, 341, 11, 286, 519, 291, 600, 1217, 1612, 293, 1310, 2378, 380, 1194, 466, 570, 257, 688, 295, 309, 3417, 588, 21769, 13], "temperature": 0.0, "avg_logprob": -0.06467754311031765, "compression_ratio": 1.4885844748858448, "no_speech_prob": 2.3922193577163853e-05}, {"id": 724, "seek": 402942, "start": 4029.42, "end": 4040.42, "text": " So here we have an array A that's one, two, three. It's got three values. B is just a scalar two and we can do A times B.", "tokens": [407, 510, 321, 362, 364, 10225, 316, 300, 311, 472, 11, 732, 11, 1045, 13, 467, 311, 658, 1045, 4190, 13, 363, 307, 445, 257, 39684, 732, 293, 321, 393, 360, 316, 1413, 363, 13], "temperature": 0.0, "avg_logprob": -0.13182966709136962, "compression_ratio": 1.4963503649635037, "no_speech_prob": 6.143945029180031e-06}, {"id": 725, "seek": 402942, "start": 4040.42, "end": 4047.42, "text": " And what happens is really it did two times one, two times two and two times three.", "tokens": [400, 437, 2314, 307, 534, 309, 630, 732, 1413, 472, 11, 732, 1413, 732, 293, 732, 1413, 1045, 13], "temperature": 0.0, "avg_logprob": -0.13182966709136962, "compression_ratio": 1.4963503649635037, "no_speech_prob": 6.143945029180031e-06}, {"id": 726, "seek": 404742, "start": 4047.42, "end": 4060.42, "text": " So in a sense, it's almost it's almost like A was treated as being two, two, two so that the dimensions matched.", "tokens": [407, 294, 257, 2020, 11, 309, 311, 1920, 309, 311, 1920, 411, 316, 390, 8668, 382, 885, 732, 11, 732, 11, 732, 370, 300, 264, 12819, 21447, 13], "temperature": 0.0, "avg_logprob": -0.10215137944077, "compression_ratio": 1.5686274509803921, "no_speech_prob": 3.2376945000578417e-06}, {"id": 727, "seek": 404742, "start": 4060.42, "end": 4075.42, "text": " Another example, if we take a matrix where we can define a matrix, then kind of using this to be V, V times two, V times three.", "tokens": [3996, 1365, 11, 498, 321, 747, 257, 8141, 689, 321, 393, 6964, 257, 8141, 11, 550, 733, 295, 1228, 341, 281, 312, 691, 11, 691, 1413, 732, 11, 691, 1413, 1045, 13], "temperature": 0.0, "avg_logprob": -0.10215137944077, "compression_ratio": 1.5686274509803921, "no_speech_prob": 3.2376945000578417e-06}, {"id": 728, "seek": 407542, "start": 4075.42, "end": 4081.42, "text": " And what we get out is one, two, three, two, four, six, three, six, nine.", "tokens": [400, 437, 321, 483, 484, 307, 472, 11, 732, 11, 1045, 11, 732, 11, 1451, 11, 2309, 11, 1045, 11, 2309, 11, 4949, 13], "temperature": 0.0, "avg_logprob": -0.057222482485648915, "compression_ratio": 1.806896551724138, "no_speech_prob": 6.8541671680577565e-06}, {"id": 729, "seek": 407542, "start": 4081.42, "end": 4088.42, "text": " So the first row is one, two, three, our original matrix three, our original matrix V.", "tokens": [407, 264, 700, 5386, 307, 472, 11, 732, 11, 1045, 11, 527, 3380, 8141, 1045, 11, 527, 3380, 8141, 691, 13], "temperature": 0.0, "avg_logprob": -0.057222482485648915, "compression_ratio": 1.806896551724138, "no_speech_prob": 6.8541671680577565e-06}, {"id": 730, "seek": 407542, "start": 4088.42, "end": 4093.42, "text": " Then we've got two times each entry and then three times each entry.", "tokens": [1396, 321, 600, 658, 732, 1413, 1184, 8729, 293, 550, 1045, 1413, 1184, 8729, 13], "temperature": 0.0, "avg_logprob": -0.057222482485648915, "compression_ratio": 1.806896551724138, "no_speech_prob": 6.8541671680577565e-06}, {"id": 731, "seek": 407542, "start": 4093.42, "end": 4098.42, "text": " We have a three by three matrix.", "tokens": [492, 362, 257, 1045, 538, 1045, 8141, 13], "temperature": 0.0, "avg_logprob": -0.057222482485648915, "compression_ratio": 1.806896551724138, "no_speech_prob": 6.8541671680577565e-06}, {"id": 732, "seek": 409842, "start": 4098.42, "end": 4107.42, "text": " And then we can say M plus V and remember M is a three by three matrix and V is a vector one, two, three.", "tokens": [400, 550, 321, 393, 584, 376, 1804, 691, 293, 1604, 376, 307, 257, 1045, 538, 1045, 8141, 293, 691, 307, 257, 8062, 472, 11, 732, 11, 1045, 13], "temperature": 0.0, "avg_logprob": -0.0947198959497305, "compression_ratio": 1.6043478260869566, "no_speech_prob": 3.0240453270380385e-05}, {"id": 733, "seek": 409842, "start": 4107.42, "end": 4112.42, "text": " And so typically kind of in written math, you would not want to do a matrix plus a vector.", "tokens": [400, 370, 5850, 733, 295, 294, 3720, 5221, 11, 291, 576, 406, 528, 281, 360, 257, 8141, 1804, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.0947198959497305, "compression_ratio": 1.6043478260869566, "no_speech_prob": 3.0240453270380385e-05}, {"id": 734, "seek": 409842, "start": 4112.42, "end": 4125.42, "text": " Like what does that even mean? But here NumPy kind of handles it for us and says, OK, you probably wanted to get back two, four, six, three, six, nine, four, eight, twelve.", "tokens": [1743, 437, 775, 300, 754, 914, 30, 583, 510, 22592, 47, 88, 733, 295, 18722, 309, 337, 505, 293, 1619, 11, 2264, 11, 291, 1391, 1415, 281, 483, 646, 732, 11, 1451, 11, 2309, 11, 1045, 11, 2309, 11, 4949, 11, 1451, 11, 3180, 11, 14390, 13], "temperature": 0.0, "avg_logprob": -0.0947198959497305, "compression_ratio": 1.6043478260869566, "no_speech_prob": 3.0240453270380385e-05}, {"id": 735, "seek": 412542, "start": 4125.42, "end": 4134.42, "text": " And so what was happening is that it just added one, two, three to each row.", "tokens": [400, 370, 437, 390, 2737, 307, 300, 309, 445, 3869, 472, 11, 732, 11, 1045, 281, 1184, 5386, 13], "temperature": 0.0, "avg_logprob": -0.07678334091020667, "compression_ratio": 1.0555555555555556, "no_speech_prob": 1.3006590961595066e-05}, {"id": 736, "seek": 413442, "start": 4134.42, "end": 4156.42, "text": " There are questions about these these two examples so far.", "tokens": [821, 366, 1651, 466, 613, 613, 732, 5110, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.20525328318277994, "compression_ratio": 1.0, "no_speech_prob": 1.0615894098009448e-05}, {"id": 737, "seek": 415642, "start": 4156.42, "end": 4164.42, "text": " That's a good question. And that actually leads kind of directly into the next example. So we transpose V, make it a column.", "tokens": [663, 311, 257, 665, 1168, 13, 400, 300, 767, 6689, 733, 295, 3838, 666, 264, 958, 1365, 13, 407, 321, 25167, 691, 11, 652, 309, 257, 7738, 13], "temperature": 0.0, "avg_logprob": -0.11514570402062457, "compression_ratio": 1.5885167464114833, "no_speech_prob": 3.844859747914597e-06}, {"id": 738, "seek": 415642, "start": 4164.42, "end": 4172.42, "text": " So you'll notice up here the shape of V was three comma, which is like saying three by one.", "tokens": [407, 291, 603, 3449, 493, 510, 264, 3909, 295, 691, 390, 1045, 22117, 11, 597, 307, 411, 1566, 1045, 538, 472, 13], "temperature": 0.0, "avg_logprob": -0.11514570402062457, "compression_ratio": 1.5885167464114833, "no_speech_prob": 3.844859747914597e-06}, {"id": 739, "seek": 415642, "start": 4172.42, "end": 4179.42, "text": " We've made V one, which is oh, sorry. This is like one by three.", "tokens": [492, 600, 1027, 691, 472, 11, 597, 307, 1954, 11, 2597, 13, 639, 307, 411, 472, 538, 1045, 13], "temperature": 0.0, "avg_logprob": -0.11514570402062457, "compression_ratio": 1.5885167464114833, "no_speech_prob": 3.844859747914597e-06}, {"id": 740, "seek": 415642, "start": 4179.42, "end": 4184.42, "text": " V one is three by one because we've transposed it.", "tokens": [691, 472, 307, 1045, 538, 472, 570, 321, 600, 7132, 1744, 309, 13], "temperature": 0.0, "avg_logprob": -0.11514570402062457, "compression_ratio": 1.5885167464114833, "no_speech_prob": 3.844859747914597e-06}, {"id": 741, "seek": 418442, "start": 4184.42, "end": 4193.42, "text": " And now when we do M plus V one, we get two, three, four, four, six, eight, six, nine, twelve.", "tokens": [400, 586, 562, 321, 360, 376, 1804, 691, 472, 11, 321, 483, 732, 11, 1045, 11, 1451, 11, 1451, 11, 2309, 11, 3180, 11, 2309, 11, 4949, 11, 14390, 13], "temperature": 0.0, "avg_logprob": -0.04345069571239192, "compression_ratio": 1.544502617801047, "no_speech_prob": 2.090429234158364e-06}, {"id": 742, "seek": 418442, "start": 4193.42, "end": 4197.42, "text": " So this highlights that it's important to kind of keep track of these dimensions.", "tokens": [407, 341, 14254, 300, 309, 311, 1021, 281, 733, 295, 1066, 2837, 295, 613, 12819, 13], "temperature": 0.0, "avg_logprob": -0.04345069571239192, "compression_ratio": 1.544502617801047, "no_speech_prob": 2.090429234158364e-06}, {"id": 743, "seek": 418442, "start": 4197.42, "end": 4204.42, "text": " So if you were doing M plus V and you had a specific answer in mind, either adding it by rows or adding it by columns,", "tokens": [407, 498, 291, 645, 884, 376, 1804, 691, 293, 291, 632, 257, 2685, 1867, 294, 1575, 11, 2139, 5127, 309, 538, 13241, 420, 5127, 309, 538, 13766, 11], "temperature": 0.0, "avg_logprob": -0.04345069571239192, "compression_ratio": 1.544502617801047, "no_speech_prob": 2.090429234158364e-06}, {"id": 744, "seek": 420442, "start": 4204.42, "end": 4217.42, "text": " you would want to make sure that you you did what you were hoping to do.", "tokens": [291, 576, 528, 281, 652, 988, 300, 291, 291, 630, 437, 291, 645, 7159, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.04563707680929275, "compression_ratio": 1.6861702127659575, "no_speech_prob": 4.8603596951579675e-06}, {"id": 745, "seek": 420442, "start": 4217.42, "end": 4221.42, "text": " So NumPy has a set of rules that kind of govern how this is happening.", "tokens": [407, 22592, 47, 88, 575, 257, 992, 295, 4474, 300, 733, 295, 1980, 577, 341, 307, 2737, 13], "temperature": 0.0, "avg_logprob": -0.04563707680929275, "compression_ratio": 1.6861702127659575, "no_speech_prob": 4.8603596951579675e-06}, {"id": 746, "seek": 420442, "start": 4221.42, "end": 4227.42, "text": " And those are that NumPy compares the shapes element wise.", "tokens": [400, 729, 366, 300, 22592, 47, 88, 38334, 264, 10854, 4478, 10829, 13], "temperature": 0.0, "avg_logprob": -0.04563707680929275, "compression_ratio": 1.6861702127659575, "no_speech_prob": 4.8603596951579675e-06}, {"id": 747, "seek": 420442, "start": 4227.42, "end": 4229.42, "text": " And it always starts with the trailing dimensions.", "tokens": [400, 309, 1009, 3719, 365, 264, 944, 4883, 12819, 13], "temperature": 0.0, "avg_logprob": -0.04563707680929275, "compression_ratio": 1.6861702127659575, "no_speech_prob": 4.8603596951579675e-06}, {"id": 748, "seek": 420442, "start": 4229.42, "end": 4232.42, "text": " So those are the dimensions kind of on the far right hand side.", "tokens": [407, 729, 366, 264, 12819, 733, 295, 322, 264, 1400, 558, 1011, 1252, 13], "temperature": 0.0, "avg_logprob": -0.04563707680929275, "compression_ratio": 1.6861702127659575, "no_speech_prob": 4.8603596951579675e-06}, {"id": 749, "seek": 423242, "start": 4232.42, "end": 4239.42, "text": " And it's looking for them to either match perfectly or one of them to be equal to one.", "tokens": [400, 309, 311, 1237, 337, 552, 281, 2139, 2995, 6239, 420, 472, 295, 552, 281, 312, 2681, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.07375021576881409, "compression_ratio": 1.4870466321243523, "no_speech_prob": 4.222774805384688e-06}, {"id": 750, "seek": 423242, "start": 4239.42, "end": 4245.42, "text": " And I think this is helpful. NumPy documentation is pretty good for this.", "tokens": [400, 286, 519, 341, 307, 4961, 13, 22592, 47, 88, 14333, 307, 1238, 665, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.07375021576881409, "compression_ratio": 1.4870466321243523, "no_speech_prob": 4.222774805384688e-06}, {"id": 751, "seek": 423242, "start": 4245.42, "end": 4253.42, "text": " They have a lot of examples, and so I want to walk through maybe a few of these.", "tokens": [814, 362, 257, 688, 295, 5110, 11, 293, 370, 286, 528, 281, 1792, 807, 1310, 257, 1326, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.07375021576881409, "compression_ratio": 1.4870466321243523, "no_speech_prob": 4.222774805384688e-06}, {"id": 752, "seek": 423242, "start": 4253.42, "end": 4256.42, "text": " So here, and this is one that comes up a lot.", "tokens": [407, 510, 11, 293, 341, 307, 472, 300, 1487, 493, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.07375021576881409, "compression_ratio": 1.4870466321243523, "no_speech_prob": 4.222774805384688e-06}, {"id": 753, "seek": 425642, "start": 4256.42, "end": 4263.42, "text": " If you were dealing with RGB values, so a picture, that channel is three.", "tokens": [759, 291, 645, 6260, 365, 31231, 4190, 11, 370, 257, 3036, 11, 300, 2269, 307, 1045, 13], "temperature": 0.0, "avg_logprob": -0.1416470663888114, "compression_ratio": 1.6199095022624435, "no_speech_prob": 2.2125026589492336e-05}, {"id": 754, "seek": 425642, "start": 4263.42, "end": 4269.42, "text": " And maybe you had a 256 by 256 picture or number of pixels.", "tokens": [400, 1310, 291, 632, 257, 38882, 538, 38882, 3036, 420, 1230, 295, 18668, 13], "temperature": 0.0, "avg_logprob": -0.1416470663888114, "compression_ratio": 1.6199095022624435, "no_speech_prob": 2.2125026589492336e-05}, {"id": 755, "seek": 425642, "start": 4269.42, "end": 4275.42, "text": " And then if you wanted to scale it by three, kind of, you know, or sorry, scale,", "tokens": [400, 550, 498, 291, 1415, 281, 4373, 309, 538, 1045, 11, 733, 295, 11, 291, 458, 11, 420, 2597, 11, 4373, 11], "temperature": 0.0, "avg_logprob": -0.1416470663888114, "compression_ratio": 1.6199095022624435, "no_speech_prob": 2.2125026589492336e-05}, {"id": 756, "seek": 425642, "start": 4275.42, "end": 4280.42, "text": " if you're scaling red, green and blue all by different values, you would have three values.", "tokens": [498, 291, 434, 21589, 2182, 11, 3092, 293, 3344, 439, 538, 819, 4190, 11, 291, 576, 362, 1045, 4190, 13], "temperature": 0.0, "avg_logprob": -0.1416470663888114, "compression_ratio": 1.6199095022624435, "no_speech_prob": 2.2125026589492336e-05}, {"id": 757, "seek": 425642, "start": 4280.42, "end": 4283.42, "text": " So it's kind of dimension three. And these line up.", "tokens": [407, 309, 311, 733, 295, 10139, 1045, 13, 400, 613, 1622, 493, 13], "temperature": 0.0, "avg_logprob": -0.1416470663888114, "compression_ratio": 1.6199095022624435, "no_speech_prob": 2.2125026589492336e-05}, {"id": 758, "seek": 428342, "start": 4283.42, "end": 4286.42, "text": " So the last two dimensions are the same.", "tokens": [407, 264, 1036, 732, 12819, 366, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.08342518677582612, "compression_ratio": 1.6193548387096774, "no_speech_prob": 7.646407539141364e-06}, {"id": 759, "seek": 428342, "start": 4286.42, "end": 4293.42, "text": " You can kind of think of, since this is shorter, you can think of it as being like one by one by three.", "tokens": [509, 393, 733, 295, 519, 295, 11, 1670, 341, 307, 11639, 11, 291, 393, 519, 295, 309, 382, 885, 411, 472, 538, 472, 538, 1045, 13], "temperature": 0.0, "avg_logprob": -0.08342518677582612, "compression_ratio": 1.6193548387096774, "no_speech_prob": 7.646407539141364e-06}, {"id": 760, "seek": 428342, "start": 4293.42, "end": 4299.42, "text": " And so this works.", "tokens": [400, 370, 341, 1985, 13], "temperature": 0.0, "avg_logprob": -0.08342518677582612, "compression_ratio": 1.6193548387096774, "no_speech_prob": 7.646407539141364e-06}, {"id": 761, "seek": 428342, "start": 4299.42, "end": 4305.42, "text": " Down here, if you had A being eight by one by six by one, B being seven by one by five,", "tokens": [9506, 510, 11, 498, 291, 632, 316, 885, 3180, 538, 472, 538, 2309, 538, 472, 11, 363, 885, 3407, 538, 472, 538, 1732, 11], "temperature": 0.0, "avg_logprob": -0.08342518677582612, "compression_ratio": 1.6193548387096774, "no_speech_prob": 7.646407539141364e-06}, {"id": 762, "seek": 430542, "start": 4305.42, "end": 4314.42, "text": " you can add those and get something that's eight by seven by six by five.", "tokens": [291, 393, 909, 729, 293, 483, 746, 300, 311, 3180, 538, 3407, 538, 2309, 538, 1732, 13], "temperature": 0.0, "avg_logprob": -0.08977010726928711, "compression_ratio": 1.507936507936508, "no_speech_prob": 5.014660928281955e-06}, {"id": 763, "seek": 430542, "start": 4314.42, "end": 4322.42, "text": " So that's pretty amazing because A and B don't seem to have any dimensions in common starting out.", "tokens": [407, 300, 311, 1238, 2243, 570, 316, 293, 363, 500, 380, 1643, 281, 362, 604, 12819, 294, 2689, 2891, 484, 13], "temperature": 0.0, "avg_logprob": -0.08977010726928711, "compression_ratio": 1.507936507936508, "no_speech_prob": 5.014660928281955e-06}, {"id": 764, "seek": 430542, "start": 4322.42, "end": 4325.42, "text": " But fortunately, their ones line up properly.", "tokens": [583, 25511, 11, 641, 2306, 1622, 493, 6108, 13], "temperature": 0.0, "avg_logprob": -0.08977010726928711, "compression_ratio": 1.507936507936508, "no_speech_prob": 5.014660928281955e-06}, {"id": 765, "seek": 430542, "start": 4325.42, "end": 4331.42, "text": " So A has one in the last dimension, B has five, it goes with five.", "tokens": [407, 316, 575, 472, 294, 264, 1036, 10139, 11, 363, 575, 1732, 11, 309, 1709, 365, 1732, 13], "temperature": 0.0, "avg_logprob": -0.08977010726928711, "compression_ratio": 1.507936507936508, "no_speech_prob": 5.014660928281955e-06}, {"id": 766, "seek": 433142, "start": 4331.42, "end": 4335.42, "text": " For the second to last dimension, we've got a six and a one, so it goes with six.", "tokens": [1171, 264, 1150, 281, 1036, 10139, 11, 321, 600, 658, 257, 2309, 293, 257, 472, 11, 370, 309, 1709, 365, 2309, 13], "temperature": 0.0, "avg_logprob": -0.10368678189706111, "compression_ratio": 1.6595744680851063, "no_speech_prob": 3.321307303849608e-05}, {"id": 767, "seek": 433142, "start": 4335.42, "end": 4340.42, "text": " Third to last dimension, a one and a seven, goes with seven.", "tokens": [12548, 281, 1036, 10139, 11, 257, 472, 293, 257, 3407, 11, 1709, 365, 3407, 13], "temperature": 0.0, "avg_logprob": -0.10368678189706111, "compression_ratio": 1.6595744680851063, "no_speech_prob": 3.321307303849608e-05}, {"id": 768, "seek": 433142, "start": 4340.42, "end": 4348.42, "text": " Final dimension, eight. And so it goes with eight.", "tokens": [13443, 10139, 11, 3180, 13, 400, 370, 309, 1709, 365, 3180, 13], "temperature": 0.0, "avg_logprob": -0.10368678189706111, "compression_ratio": 1.6595744680851063, "no_speech_prob": 3.321307303849608e-05}, {"id": 769, "seek": 433142, "start": 4348.42, "end": 4352.42, "text": " Are there questions about this?", "tokens": [2014, 456, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.10368678189706111, "compression_ratio": 1.6595744680851063, "no_speech_prob": 3.321307303849608e-05}, {"id": 770, "seek": 433142, "start": 4352.42, "end": 4356.42, "text": " Matthew?", "tokens": [12434, 30], "temperature": 0.0, "avg_logprob": -0.10368678189706111, "compression_ratio": 1.6595744680851063, "no_speech_prob": 3.321307303849608e-05}, {"id": 771, "seek": 435642, "start": 4356.42, "end": 4361.42, "text": " In number B, we line up the dimension starting from left to right.", "tokens": [682, 1230, 363, 11, 321, 1622, 493, 264, 10139, 2891, 490, 1411, 281, 558, 13], "temperature": 0.0, "avg_logprob": -0.13932447898678663, "compression_ratio": 1.565, "no_speech_prob": 2.2472182536148466e-05}, {"id": 772, "seek": 435642, "start": 4361.42, "end": 4370.42, "text": " Yeah, starting with the right to left.", "tokens": [865, 11, 2891, 365, 264, 558, 281, 1411, 13], "temperature": 0.0, "avg_logprob": -0.13932447898678663, "compression_ratio": 1.565, "no_speech_prob": 2.2472182536148466e-05}, {"id": 773, "seek": 435642, "start": 4370.42, "end": 4376.42, "text": " And these, I think it's really good to try, to kind of play around with trying a few different examples.", "tokens": [400, 613, 11, 286, 519, 309, 311, 534, 665, 281, 853, 11, 281, 733, 295, 862, 926, 365, 1382, 257, 1326, 819, 5110, 13], "temperature": 0.0, "avg_logprob": -0.13932447898678663, "compression_ratio": 1.565, "no_speech_prob": 2.2472182536148466e-05}, {"id": 774, "seek": 435642, "start": 4376.42, "end": 4381.42, "text": " There will be, in the homework, there's some of just kind of, I give you different sets of dimensions,", "tokens": [821, 486, 312, 11, 294, 264, 14578, 11, 456, 311, 512, 295, 445, 733, 295, 11, 286, 976, 291, 819, 6352, 295, 12819, 11], "temperature": 0.0, "avg_logprob": -0.13932447898678663, "compression_ratio": 1.565, "no_speech_prob": 2.2472182536148466e-05}, {"id": 775, "seek": 438142, "start": 4381.42, "end": 4386.42, "text": " and you have to say, you know, yes, this would work and this is what the dimensions would be,", "tokens": [293, 291, 362, 281, 584, 11, 291, 458, 11, 2086, 11, 341, 576, 589, 293, 341, 307, 437, 264, 12819, 576, 312, 11], "temperature": 0.0, "avg_logprob": -0.13005172941419813, "compression_ratio": 1.515, "no_speech_prob": 1.2411186617100611e-05}, {"id": 776, "seek": 438142, "start": 4386.42, "end": 4394.42, "text": " or no, I'm going to get an error if these don't line up properly.", "tokens": [420, 572, 11, 286, 478, 516, 281, 483, 364, 6713, 498, 613, 500, 380, 1622, 493, 6108, 13], "temperature": 0.0, "avg_logprob": -0.13005172941419813, "compression_ratio": 1.515, "no_speech_prob": 1.2411186617100611e-05}, {"id": 777, "seek": 438142, "start": 4394.42, "end": 4401.42, "text": " Yes, Richan?", "tokens": [1079, 11, 6781, 282, 30], "temperature": 0.0, "avg_logprob": -0.13005172941419813, "compression_ratio": 1.515, "no_speech_prob": 1.2411186617100611e-05}, {"id": 778, "seek": 438142, "start": 4401.42, "end": 4404.42, "text": " Yeah, so we just start by looking at the end.", "tokens": [865, 11, 370, 321, 445, 722, 538, 1237, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.13005172941419813, "compression_ratio": 1.515, "no_speech_prob": 1.2411186617100611e-05}, {"id": 779, "seek": 438142, "start": 4404.42, "end": 4409.42, "text": " And so they've kind of written these out very nicely that you can see, kind of like,", "tokens": [400, 370, 436, 600, 733, 295, 3720, 613, 484, 588, 9594, 300, 291, 393, 536, 11, 733, 295, 411, 11], "temperature": 0.0, "avg_logprob": -0.13005172941419813, "compression_ratio": 1.515, "no_speech_prob": 1.2411186617100611e-05}, {"id": 780, "seek": 440942, "start": 4409.42, "end": 4414.42, "text": " okay, like, let's look at the far right and see if these line up.", "tokens": [1392, 11, 411, 11, 718, 311, 574, 412, 264, 1400, 558, 293, 536, 498, 613, 1622, 493, 13], "temperature": 0.0, "avg_logprob": -0.10395542392885782, "compression_ratio": 1.680327868852459, "no_speech_prob": 5.1440150855341926e-05}, {"id": 781, "seek": 440942, "start": 4414.42, "end": 4417.42, "text": " Because you'll notice, like, here, A was a 4D array.", "tokens": [1436, 291, 603, 3449, 11, 411, 11, 510, 11, 316, 390, 257, 1017, 35, 10225, 13], "temperature": 0.0, "avg_logprob": -0.10395542392885782, "compression_ratio": 1.680327868852459, "no_speech_prob": 5.1440150855341926e-05}, {"id": 782, "seek": 440942, "start": 4417.42, "end": 4422.42, "text": " It was eight by one by six by one, and B was seven by one by five.", "tokens": [467, 390, 3180, 538, 472, 538, 2309, 538, 472, 11, 293, 363, 390, 3407, 538, 472, 538, 1732, 13], "temperature": 0.0, "avg_logprob": -0.10395542392885782, "compression_ratio": 1.680327868852459, "no_speech_prob": 5.1440150855341926e-05}, {"id": 783, "seek": 440942, "start": 4422.42, "end": 4425.42, "text": " We don't start comparing, like, eight and seven. Those don't match.", "tokens": [492, 500, 380, 722, 15763, 11, 411, 11, 3180, 293, 3407, 13, 3950, 500, 380, 2995, 13], "temperature": 0.0, "avg_logprob": -0.10395542392885782, "compression_ratio": 1.680327868852459, "no_speech_prob": 5.1440150855341926e-05}, {"id": 784, "seek": 440942, "start": 4425.42, "end": 4432.42, "text": " We're not going to do this. We start with the far right side.", "tokens": [492, 434, 406, 516, 281, 360, 341, 13, 492, 722, 365, 264, 1400, 558, 1252, 13], "temperature": 0.0, "avg_logprob": -0.10395542392885782, "compression_ratio": 1.680327868852459, "no_speech_prob": 5.1440150855341926e-05}, {"id": 785, "seek": 440942, "start": 4432.42, "end": 4436.42, "text": " Yes, yeah, and so the two things that have to be true is they either need to be exactly equal,", "tokens": [1079, 11, 1338, 11, 293, 370, 264, 732, 721, 300, 362, 281, 312, 2074, 307, 436, 2139, 643, 281, 312, 2293, 2681, 11], "temperature": 0.0, "avg_logprob": -0.10395542392885782, "compression_ratio": 1.680327868852459, "no_speech_prob": 5.1440150855341926e-05}, {"id": 786, "seek": 443642, "start": 4436.42, "end": 4447.42, "text": " or one of them has to be one.", "tokens": [420, 472, 295, 552, 575, 281, 312, 472, 13], "temperature": 0.0, "avg_logprob": -0.14995770984225804, "compression_ratio": 1.25, "no_speech_prob": 2.6687437639338896e-05}, {"id": 787, "seek": 443642, "start": 4447.42, "end": 4454.42, "text": " It makes intuitive sense because if there were anything else, it wouldn't be obvious how to broadcast it.", "tokens": [467, 1669, 21769, 2020, 570, 498, 456, 645, 1340, 1646, 11, 309, 2759, 380, 312, 6322, 577, 281, 9975, 309, 13], "temperature": 0.0, "avg_logprob": -0.14995770984225804, "compression_ratio": 1.25, "no_speech_prob": 2.6687437639338896e-05}, {"id": 788, "seek": 445442, "start": 4454.42, "end": 4471.42, "text": " Yeah, yeah. Let me even, let me try making a matrix that's larger.", "tokens": [865, 11, 1338, 13, 961, 385, 754, 11, 718, 385, 853, 1455, 257, 8141, 300, 311, 4833, 13], "temperature": 0.0, "avg_logprob": -0.18539857864379883, "compression_ratio": 0.9850746268656716, "no_speech_prob": 3.500788807286881e-06}, {"id": 789, "seek": 447142, "start": 4471.42, "end": 4492.42, "text": " I don't know, we can make a new matrix, N, that is one by one.", "tokens": [286, 500, 380, 458, 11, 321, 393, 652, 257, 777, 8141, 11, 426, 11, 300, 307, 472, 538, 472, 13], "temperature": 0.0, "avg_logprob": -0.2230570912361145, "compression_ratio": 0.9393939393939394, "no_speech_prob": 2.0145424059592187e-05}, {"id": 790, "seek": 449242, "start": 4492.42, "end": 4505.42, "text": " I'll also say that I think.shape is just a really helpful, why is that making it, that's an array.", "tokens": [286, 603, 611, 584, 300, 286, 519, 2411, 82, 42406, 307, 445, 257, 534, 4961, 11, 983, 307, 300, 1455, 309, 11, 300, 311, 364, 10225, 13], "temperature": 0.0, "avg_logprob": -0.15585549672444662, "compression_ratio": 1.6402116402116402, "no_speech_prob": 1.3006750123167876e-05}, {"id": 791, "seek": 449242, "start": 4505.42, "end": 4513.42, "text": ".shape is a really helpful attribute just to be able to see kind of what you're dealing with.", "tokens": [2411, 82, 42406, 307, 257, 534, 4961, 19667, 445, 281, 312, 1075, 281, 536, 733, 295, 437, 291, 434, 6260, 365, 13], "temperature": 0.0, "avg_logprob": -0.15585549672444662, "compression_ratio": 1.6402116402116402, "no_speech_prob": 1.3006750123167876e-05}, {"id": 792, "seek": 449242, "start": 4513.42, "end": 4520.42, "text": " And, like, I think it's a good thing to check on your data in general, like, okay, do I have what I think I have here?", "tokens": [400, 11, 411, 11, 286, 519, 309, 311, 257, 665, 551, 281, 1520, 322, 428, 1412, 294, 2674, 11, 411, 11, 1392, 11, 360, 286, 362, 437, 286, 519, 286, 362, 510, 30], "temperature": 0.0, "avg_logprob": -0.15585549672444662, "compression_ratio": 1.6402116402116402, "no_speech_prob": 1.3006750123167876e-05}, {"id": 793, "seek": 452042, "start": 4520.42, "end": 4528.42, "text": " And so, like, here with N, and we can even kind of confirm, okay, what was the shape of it?", "tokens": [400, 370, 11, 411, 11, 510, 365, 426, 11, 293, 321, 393, 754, 733, 295, 9064, 11, 1392, 11, 437, 390, 264, 3909, 295, 309, 30], "temperature": 0.0, "avg_logprob": -0.12237209651781165, "compression_ratio": 1.676991150442478, "no_speech_prob": 2.1111716705490835e-05}, {"id": 794, "seek": 452042, "start": 4528.42, "end": 4533.42, "text": " M, M was three by three. Would I be able to add N and M?", "tokens": [376, 11, 376, 390, 1045, 538, 1045, 13, 6068, 286, 312, 1075, 281, 909, 426, 293, 376, 30], "temperature": 0.0, "avg_logprob": -0.12237209651781165, "compression_ratio": 1.676991150442478, "no_speech_prob": 2.1111716705490835e-05}, {"id": 795, "seek": 452042, "start": 4533.42, "end": 4538.42, "text": " If one is two by three by three and the other is three by three?", "tokens": [759, 472, 307, 732, 538, 1045, 538, 1045, 293, 264, 661, 307, 1045, 538, 1045, 30], "temperature": 0.0, "avg_logprob": -0.12237209651781165, "compression_ratio": 1.676991150442478, "no_speech_prob": 2.1111716705490835e-05}, {"id": 796, "seek": 452042, "start": 4538.42, "end": 4542.42, "text": " So I heard a yes, thumbs up. Yeah, and that's correct, we could.", "tokens": [407, 286, 2198, 257, 2086, 11, 8838, 493, 13, 865, 11, 293, 300, 311, 3006, 11, 321, 727, 13], "temperature": 0.0, "avg_logprob": -0.12237209651781165, "compression_ratio": 1.676991150442478, "no_speech_prob": 2.1111716705490835e-05}, {"id": 797, "seek": 452042, "start": 4542.42, "end": 4548.42, "text": " And that's because we would start with, okay, the trailing dimension is three for M and three for N.", "tokens": [400, 300, 311, 570, 321, 576, 722, 365, 11, 1392, 11, 264, 944, 4883, 10139, 307, 1045, 337, 376, 293, 1045, 337, 426, 13], "temperature": 0.0, "avg_logprob": -0.12237209651781165, "compression_ratio": 1.676991150442478, "no_speech_prob": 2.1111716705490835e-05}, {"id": 798, "seek": 454842, "start": 4548.42, "end": 4553.42, "text": " So those match. Then the second to last is three and three, those match.", "tokens": [407, 729, 2995, 13, 1396, 264, 1150, 281, 1036, 307, 1045, 293, 1045, 11, 729, 2995, 13], "temperature": 0.0, "avg_logprob": -0.19710619906161694, "compression_ratio": 1.5642201834862386, "no_speech_prob": 3.21914121741429e-05}, {"id": 799, "seek": 454842, "start": 4553.42, "end": 4559.42, "text": " And then we have a two, and this is like having a one, so we're okay.", "tokens": [400, 550, 321, 362, 257, 732, 11, 293, 341, 307, 411, 1419, 257, 472, 11, 370, 321, 434, 1392, 13], "temperature": 0.0, "avg_logprob": -0.19710619906161694, "compression_ratio": 1.5642201834862386, "no_speech_prob": 3.21914121741429e-05}, {"id": 800, "seek": 454842, "start": 4559.42, "end": 4564.42, "text": " Any other questions? Jeremy?", "tokens": [2639, 661, 1651, 30, 17809, 30], "temperature": 0.0, "avg_logprob": -0.19710619906161694, "compression_ratio": 1.5642201834862386, "no_speech_prob": 3.21914121741429e-05}, {"id": 801, "seek": 454842, "start": 4564.42, "end": 4570.42, "text": " I was just going to mention that I do broadcasting, and I find when I read other people's code,", "tokens": [286, 390, 445, 516, 281, 2152, 300, 286, 360, 30024, 11, 293, 286, 915, 562, 286, 1401, 661, 561, 311, 3089, 11], "temperature": 0.0, "avg_logprob": -0.19710619906161694, "compression_ratio": 1.5642201834862386, "no_speech_prob": 3.21914121741429e-05}, {"id": 802, "seek": 454842, "start": 4570.42, "end": 4576.42, "text": " they don't normally use it, even in, like, popular open source libraries,", "tokens": [436, 500, 380, 5646, 764, 309, 11, 754, 294, 11, 411, 11, 3743, 1269, 4009, 15148, 11], "temperature": 0.0, "avg_logprob": -0.19710619906161694, "compression_ratio": 1.5642201834862386, "no_speech_prob": 3.21914121741429e-05}, {"id": 803, "seek": 457642, "start": 4576.42, "end": 4584.42, "text": " and I'll rewrite it with broadcasting, and it'll be like four times less code and ten times faster.", "tokens": [293, 286, 603, 28132, 309, 365, 30024, 11, 293, 309, 603, 312, 411, 1451, 1413, 1570, 3089, 293, 2064, 1413, 4663, 13], "temperature": 0.0, "avg_logprob": -0.21301957180625514, "compression_ratio": 1.6747967479674797, "no_speech_prob": 5.3067298722453415e-05}, {"id": 804, "seek": 457642, "start": 4584.42, "end": 4587.42, "text": " So it seems to be something that's kind of underappreciated.", "tokens": [407, 309, 2544, 281, 312, 746, 300, 311, 733, 295, 833, 1746, 3326, 770, 13], "temperature": 0.0, "avg_logprob": -0.21301957180625514, "compression_ratio": 1.6747967479674797, "no_speech_prob": 5.3067298722453415e-05}, {"id": 805, "seek": 457642, "start": 4587.42, "end": 4591.42, "text": " I actually like it so much, and in my favorite GP library called PyTorch,", "tokens": [286, 767, 411, 309, 370, 709, 11, 293, 294, 452, 2954, 26039, 6405, 1219, 9953, 51, 284, 339, 11], "temperature": 0.0, "avg_logprob": -0.21301957180625514, "compression_ratio": 1.6747967479674797, "no_speech_prob": 5.3067298722453415e-05}, {"id": 806, "seek": 457642, "start": 4591.42, "end": 4596.42, "text": " I wrote the broadcasting library and, like, make all this code much easier.", "tokens": [286, 4114, 264, 30024, 6405, 293, 11, 411, 11, 652, 439, 341, 3089, 709, 3571, 13], "temperature": 0.0, "avg_logprob": -0.21301957180625514, "compression_ratio": 1.6747967479674797, "no_speech_prob": 5.3067298722453415e-05}, {"id": 807, "seek": 457642, "start": 4596.42, "end": 4600.42, "text": " In two weeks' time, we'll be adding a page for PyTorch broadcasting.", "tokens": [682, 732, 3259, 6, 565, 11, 321, 603, 312, 5127, 257, 3028, 337, 9953, 51, 284, 339, 30024, 13], "temperature": 0.0, "avg_logprob": -0.21301957180625514, "compression_ratio": 1.6747967479674797, "no_speech_prob": 5.3067298722453415e-05}, {"id": 808, "seek": 457642, "start": 4600.42, "end": 4603.42, "text": " That's awesome. Yeah, thank you.", "tokens": [663, 311, 3476, 13, 865, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.21301957180625514, "compression_ratio": 1.6747967479674797, "no_speech_prob": 5.3067298722453415e-05}, {"id": 809, "seek": 460342, "start": 4603.42, "end": 4613.42, "text": " Yeah, and this is something that's nice about broadcasting is that Numpy is vectorizing the array operations kind of for you.", "tokens": [865, 11, 293, 341, 307, 746, 300, 311, 1481, 466, 30024, 307, 300, 426, 36142, 307, 8062, 3319, 264, 10225, 7705, 733, 295, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.20859489911868248, "compression_ratio": 1.5242718446601942, "no_speech_prob": 6.204601231729612e-05}, {"id": 810, "seek": 460342, "start": 4613.42, "end": 4622.42, "text": " So it's very efficient.", "tokens": [407, 309, 311, 588, 7148, 13], "temperature": 0.0, "avg_logprob": -0.20859489911868248, "compression_ratio": 1.5242718446601942, "no_speech_prob": 6.204601231729612e-05}, {"id": 811, "seek": 460342, "start": 4622.42, "end": 4624.42, "text": " We also found that thing the other day.", "tokens": [492, 611, 1352, 300, 551, 264, 661, 786, 13], "temperature": 0.0, "avg_logprob": -0.20859489911868248, "compression_ratio": 1.5242718446601942, "no_speech_prob": 6.204601231729612e-05}, {"id": 812, "seek": 460342, "start": 4624.42, "end": 4631.42, "text": " So if you mentioned it, it said in the last couple of weeks they've changed it so it doesn't even have to reallocate memory.", "tokens": [407, 498, 291, 2835, 309, 11, 309, 848, 294, 264, 1036, 1916, 295, 3259, 436, 600, 3105, 309, 370, 309, 1177, 380, 754, 362, 281, 319, 336, 42869, 4675, 13], "temperature": 0.0, "avg_logprob": -0.20859489911868248, "compression_ratio": 1.5242718446601942, "no_speech_prob": 6.204601231729612e-05}, {"id": 813, "seek": 463142, "start": 4631.42, "end": 4637.42, "text": " Temporaries, yeah, that they're reusing.", "tokens": [8095, 2816, 4889, 11, 1338, 11, 300, 436, 434, 319, 7981, 13], "temperature": 0.0, "avg_logprob": -0.11895555331383223, "compression_ratio": 1.4675925925925926, "no_speech_prob": 1.8057618945022114e-05}, {"id": 814, "seek": 463142, "start": 4637.42, "end": 4648.42, "text": " Yeah, I think that might have been a month ago now, but yeah, that's a recent improvement to Numpy, which was already a great library.", "tokens": [865, 11, 286, 519, 300, 1062, 362, 668, 257, 1618, 2057, 586, 11, 457, 1338, 11, 300, 311, 257, 5162, 10444, 281, 426, 36142, 11, 597, 390, 1217, 257, 869, 6405, 13], "temperature": 0.0, "avg_logprob": -0.11895555331383223, "compression_ratio": 1.4675925925925926, "no_speech_prob": 1.8057618945022114e-05}, {"id": 815, "seek": 463142, "start": 4648.42, "end": 4658.42, "text": " Cool, and then our next kind of general or foundational topic I want to talk about is sparse matrices and specific ways of dealing with them.", "tokens": [8561, 11, 293, 550, 527, 958, 733, 295, 2674, 420, 32195, 4829, 286, 528, 281, 751, 466, 307, 637, 11668, 32284, 293, 2685, 2098, 295, 6260, 365, 552, 13], "temperature": 0.0, "avg_logprob": -0.11895555331383223, "compression_ratio": 1.4675925925925926, "no_speech_prob": 1.8057618945022114e-05}, {"id": 816, "seek": 465842, "start": 4658.42, "end": 4662.42, "text": " So just to remind you, a matrix with lots of zeros is sparse.", "tokens": [407, 445, 281, 4160, 291, 11, 257, 8141, 365, 3195, 295, 35193, 307, 637, 11668, 13], "temperature": 0.0, "avg_logprob": -0.08521627462827243, "compression_ratio": 1.6254980079681276, "no_speech_prob": 1.8630513295647688e-05}, {"id": 817, "seek": 465842, "start": 4662.42, "end": 4664.42, "text": " Here's an example.", "tokens": [1692, 311, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.08521627462827243, "compression_ratio": 1.6254980079681276, "no_speech_prob": 1.8630513295647688e-05}, {"id": 818, "seek": 465842, "start": 4664.42, "end": 4667.42, "text": " So all the light gray values are zeros.", "tokens": [407, 439, 264, 1442, 10855, 4190, 366, 35193, 13], "temperature": 0.0, "avg_logprob": -0.08521627462827243, "compression_ratio": 1.6254980079681276, "no_speech_prob": 1.8630513295647688e-05}, {"id": 819, "seek": 465842, "start": 4667.42, "end": 4670.42, "text": " And then we've got some other values, but not a ton.", "tokens": [400, 550, 321, 600, 658, 512, 661, 4190, 11, 457, 406, 257, 2952, 13], "temperature": 0.0, "avg_logprob": -0.08521627462827243, "compression_ratio": 1.6254980079681276, "no_speech_prob": 1.8630513295647688e-05}, {"id": 820, "seek": 465842, "start": 4670.42, "end": 4678.42, "text": " And so it's more memory efficient to just save the non-zero values and not allocate a memory position for everything.", "tokens": [400, 370, 309, 311, 544, 4675, 7148, 281, 445, 3155, 264, 2107, 12, 32226, 4190, 293, 406, 35713, 257, 4675, 2535, 337, 1203, 13], "temperature": 0.0, "avg_logprob": -0.08521627462827243, "compression_ratio": 1.6254980079681276, "no_speech_prob": 1.8630513295647688e-05}, {"id": 821, "seek": 465842, "start": 4678.42, "end": 4687.42, "text": " And sparse matrices show up a lot in various engineering problems, and they often have kind of interesting patterns.", "tokens": [400, 637, 11668, 32284, 855, 493, 257, 688, 294, 3683, 7043, 2740, 11, 293, 436, 2049, 362, 733, 295, 1880, 8294, 13], "temperature": 0.0, "avg_logprob": -0.08521627462827243, "compression_ratio": 1.6254980079681276, "no_speech_prob": 1.8630513295647688e-05}, {"id": 822, "seek": 468742, "start": 4687.42, "end": 4693.42, "text": " And you'll get them from kind of like multigrid methods and also from differential equations.", "tokens": [400, 291, 603, 483, 552, 490, 733, 295, 411, 2120, 328, 8558, 7150, 293, 611, 490, 15756, 11787, 13], "temperature": 0.0, "avg_logprob": -0.067595594069537, "compression_ratio": 1.5810276679841897, "no_speech_prob": 5.338008577382425e-06}, {"id": 823, "seek": 468742, "start": 4693.42, "end": 4697.42, "text": " And so this is just an example of one kind of with a pattern.", "tokens": [400, 370, 341, 307, 445, 364, 1365, 295, 472, 733, 295, 365, 257, 5102, 13], "temperature": 0.0, "avg_logprob": -0.067595594069537, "compression_ratio": 1.5810276679841897, "no_speech_prob": 5.338008577382425e-06}, {"id": 824, "seek": 468742, "start": 4697.42, "end": 4706.42, "text": " And here the black entries are all non-zero, the white entries are zero, and you can see most of the matrix is zeros.", "tokens": [400, 510, 264, 2211, 23041, 366, 439, 2107, 12, 32226, 11, 264, 2418, 23041, 366, 4018, 11, 293, 291, 393, 536, 881, 295, 264, 8141, 307, 35193, 13], "temperature": 0.0, "avg_logprob": -0.067595594069537, "compression_ratio": 1.5810276679841897, "no_speech_prob": 5.338008577382425e-06}, {"id": 825, "seek": 468742, "start": 4706.42, "end": 4712.42, "text": " So I wanted to go into more detail about, OK, I've said we're just storing the entries, but what does that actually look like?", "tokens": [407, 286, 1415, 281, 352, 666, 544, 2607, 466, 11, 2264, 11, 286, 600, 848, 321, 434, 445, 26085, 264, 23041, 11, 457, 437, 775, 300, 767, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.067595594069537, "compression_ratio": 1.5810276679841897, "no_speech_prob": 5.338008577382425e-06}, {"id": 826, "seek": 471242, "start": 4712.42, "end": 4728.42, "text": " So there are three really common sparse storage formats, and those are coordinate wise, which SciPy calls COO, compressed sparse row, which is CSR, and compressed space column, CSC.", "tokens": [407, 456, 366, 1045, 534, 2689, 637, 11668, 6725, 25879, 11, 293, 729, 366, 15670, 10829, 11, 597, 16942, 47, 88, 5498, 3002, 46, 11, 30353, 637, 11668, 5386, 11, 597, 307, 9460, 49, 11, 293, 30353, 1901, 7738, 11, 9460, 34, 13], "temperature": 0.0, "avg_logprob": -0.11563844233751297, "compression_ratio": 1.4662576687116564, "no_speech_prob": 7.296202966244891e-06}, {"id": 827, "seek": 471242, "start": 4728.42, "end": 4733.42, "text": " And I found a page that has, I think, some nice examples.", "tokens": [400, 286, 1352, 257, 3028, 300, 575, 11, 286, 519, 11, 512, 1481, 5110, 13], "temperature": 0.0, "avg_logprob": -0.11563844233751297, "compression_ratio": 1.4662576687116564, "no_speech_prob": 7.296202966244891e-06}, {"id": 828, "seek": 473342, "start": 4733.42, "end": 4751.42, "text": " And actually, yeah, I found two pages that we're going to look at.", "tokens": [400, 767, 11, 1338, 11, 286, 1352, 732, 7183, 300, 321, 434, 516, 281, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.10152630578903925, "compression_ratio": 0.9295774647887324, "no_speech_prob": 4.860301487497054e-06}, {"id": 829, "seek": 475142, "start": 4751.42, "end": 4763.42, "text": " All right, so this is the coordinate wise storage method we'll talk about first. And I think in some ways, to me, it kind of seems like the most natural one.", "tokens": [1057, 558, 11, 370, 341, 307, 264, 15670, 10829, 6725, 3170, 321, 603, 751, 466, 700, 13, 400, 286, 519, 294, 512, 2098, 11, 281, 385, 11, 309, 733, 295, 2544, 411, 264, 881, 3303, 472, 13], "temperature": 0.0, "avg_logprob": -0.11581716633806324, "compression_ratio": 1.5611814345991561, "no_speech_prob": 1.4509651919070166e-05}, {"id": 830, "seek": 475142, "start": 4763.42, "end": 4767.42, "text": " Yeah, so here we've got a matrix. And are you guys able to see this? OK.", "tokens": [865, 11, 370, 510, 321, 600, 658, 257, 8141, 13, 400, 366, 291, 1074, 1075, 281, 536, 341, 30, 2264, 13], "temperature": 0.0, "avg_logprob": -0.11581716633806324, "compression_ratio": 1.5611814345991561, "no_speech_prob": 1.4509651919070166e-05}, {"id": 831, "seek": 475142, "start": 4767.42, "end": 4775.42, "text": " Kind of the matrix on yellow. And everywhere there's no number, that's a zero. So they've only written the non-zero entries in this matrix.", "tokens": [9242, 295, 264, 8141, 322, 5566, 13, 400, 5315, 456, 311, 572, 1230, 11, 300, 311, 257, 4018, 13, 407, 436, 600, 787, 3720, 264, 2107, 12, 32226, 23041, 294, 341, 8141, 13], "temperature": 0.0, "avg_logprob": -0.11581716633806324, "compression_ratio": 1.5611814345991561, "no_speech_prob": 1.4509651919070166e-05}, {"id": 832, "seek": 477542, "start": 4775.42, "end": 4785.42, "text": " You can see this is a 8 by 8 matrix. And then they've color coded some of the entries.", "tokens": [509, 393, 536, 341, 307, 257, 1649, 538, 1649, 8141, 13, 400, 550, 436, 600, 2017, 34874, 512, 295, 264, 23041, 13], "temperature": 0.0, "avg_logprob": -0.12198375520252046, "compression_ratio": 1.648068669527897, "no_speech_prob": 8.139521014527418e-06}, {"id": 833, "seek": 477542, "start": 4785.42, "end": 4790.42, "text": " And that's just nice because then you can kind of see. So down here on green is the representation.", "tokens": [400, 300, 311, 445, 1481, 570, 550, 291, 393, 733, 295, 536, 13, 407, 760, 510, 322, 3092, 307, 264, 10290, 13], "temperature": 0.0, "avg_logprob": -0.12198375520252046, "compression_ratio": 1.648068669527897, "no_speech_prob": 8.139521014527418e-06}, {"id": 834, "seek": 477542, "start": 4790.42, "end": 4797.42, "text": " Yellow is kind of how we would think about the matrix. Green is what the computer's storing. And so in spot 0, 0, there's an 11.", "tokens": [17550, 307, 733, 295, 577, 321, 576, 519, 466, 264, 8141, 13, 6969, 307, 437, 264, 3820, 311, 26085, 13, 400, 370, 294, 4008, 1958, 11, 1958, 11, 456, 311, 364, 2975, 13], "temperature": 0.0, "avg_logprob": -0.12198375520252046, "compression_ratio": 1.648068669527897, "no_speech_prob": 8.139521014527418e-06}, {"id": 835, "seek": 477542, "start": 4797.42, "end": 4804.42, "text": " And we see down here they're storing value 11 is in row 0, column 0.", "tokens": [400, 321, 536, 760, 510, 436, 434, 26085, 2158, 2975, 307, 294, 5386, 1958, 11, 7738, 1958, 13], "temperature": 0.0, "avg_logprob": -0.12198375520252046, "compression_ratio": 1.648068669527897, "no_speech_prob": 8.139521014527418e-06}, {"id": 836, "seek": 480442, "start": 4804.42, "end": 4813.42, "text": " So on there's a 22 in spot 1, 1. And so it stores value 22, row 1, column 1.", "tokens": [407, 322, 456, 311, 257, 5853, 294, 4008, 502, 11, 502, 13, 400, 370, 309, 9512, 2158, 5853, 11, 5386, 502, 11, 7738, 502, 13], "temperature": 0.0, "avg_logprob": -0.08213485655237417, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.788053509699239e-06}, {"id": 837, "seek": 480442, "start": 4813.42, "end": 4823.42, "text": " So just storing the coordinates for each one. There's a 75 in row 6, column 4. And so it stores 75, 6, 4.", "tokens": [407, 445, 26085, 264, 21056, 337, 1184, 472, 13, 821, 311, 257, 9562, 294, 5386, 1386, 11, 7738, 1017, 13, 400, 370, 309, 9512, 9562, 11, 1386, 11, 1017, 13], "temperature": 0.0, "avg_logprob": -0.08213485655237417, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.788053509699239e-06}, {"id": 838, "seek": 482342, "start": 4823.42, "end": 4836.42, "text": " So all kind of the computer needs is these three vectors, one of the values, one of the rows, and one of the columns to have all the information from this matrix.", "tokens": [407, 439, 733, 295, 264, 3820, 2203, 307, 613, 1045, 18875, 11, 472, 295, 264, 4190, 11, 472, 295, 264, 13241, 11, 293, 472, 295, 264, 13766, 281, 362, 439, 264, 1589, 490, 341, 8141, 13], "temperature": 0.0, "avg_logprob": -0.08710842254834297, "compression_ratio": 1.7921348314606742, "no_speech_prob": 1.2606662494363263e-05}, {"id": 839, "seek": 482342, "start": 4836.42, "end": 4847.42, "text": " And so in this example, there were 20 entries in the matrix. And so you are having to note that for each entry, you are having to store the row and columns.", "tokens": [400, 370, 294, 341, 1365, 11, 456, 645, 945, 23041, 294, 264, 8141, 13, 400, 370, 291, 366, 1419, 281, 3637, 300, 337, 1184, 8729, 11, 291, 366, 1419, 281, 3531, 264, 5386, 293, 13766, 13], "temperature": 0.0, "avg_logprob": -0.08710842254834297, "compression_ratio": 1.7921348314606742, "no_speech_prob": 1.2606662494363263e-05}, {"id": 840, "seek": 484742, "start": 4847.42, "end": 4861.42, "text": " So this is taking, I guess, like 60 spots in memory to have this information, which you can imagine many cases where that's less than, you know, here this is a 7 by 7 matrix.", "tokens": [407, 341, 307, 1940, 11, 286, 2041, 11, 411, 4060, 10681, 294, 4675, 281, 362, 341, 1589, 11, 597, 291, 393, 3811, 867, 3331, 689, 300, 311, 1570, 813, 11, 291, 458, 11, 510, 341, 307, 257, 1614, 538, 1614, 8141, 13], "temperature": 0.0, "avg_logprob": -0.14021523319073578, "compression_ratio": 1.4545454545454546, "no_speech_prob": 1.3211586519901175e-05}, {"id": 841, "seek": 484742, "start": 4861.42, "end": 4873.42, "text": " That would have been 49 spots in memory. Yeah, are there questions about coordinate wise storage?", "tokens": [663, 576, 362, 668, 16513, 10681, 294, 4675, 13, 865, 11, 366, 456, 1651, 466, 15670, 10829, 6725, 30], "temperature": 0.0, "avg_logprob": -0.14021523319073578, "compression_ratio": 1.4545454545454546, "no_speech_prob": 1.3211586519901175e-05}, {"id": 842, "seek": 487342, "start": 4873.42, "end": 4879.42, "text": " So this is kind of slightly more per entry, but overall if you have a lot of zeros, you're saving memory.", "tokens": [407, 341, 307, 733, 295, 4748, 544, 680, 8729, 11, 457, 4787, 498, 291, 362, 257, 688, 295, 35193, 11, 291, 434, 6816, 4675, 13], "temperature": 0.0, "avg_logprob": -0.12316623026010942, "compression_ratio": 1.5630252100840336, "no_speech_prob": 5.594210961135104e-06}, {"id": 843, "seek": 487342, "start": 4879.42, "end": 4890.42, "text": " And you're also, this is going to change how you access points. I mean, here you would, I don't know, if you wanted to see what the value at a particular coordinate was,", "tokens": [400, 291, 434, 611, 11, 341, 307, 516, 281, 1319, 577, 291, 2105, 2793, 13, 286, 914, 11, 510, 291, 576, 11, 286, 500, 380, 458, 11, 498, 291, 1415, 281, 536, 437, 264, 2158, 412, 257, 1729, 15670, 390, 11], "temperature": 0.0, "avg_logprob": -0.12316623026010942, "compression_ratio": 1.5630252100840336, "no_speech_prob": 5.594210961135104e-06}, {"id": 844, "seek": 487342, "start": 4890.42, "end": 4901.42, "text": " you would have to check like is the row column paired even in this matrix? If not, it must be 0.", "tokens": [291, 576, 362, 281, 1520, 411, 307, 264, 5386, 7738, 25699, 754, 294, 341, 8141, 30, 759, 406, 11, 309, 1633, 312, 1958, 13], "temperature": 0.0, "avg_logprob": -0.12316623026010942, "compression_ratio": 1.5630252100840336, "no_speech_prob": 5.594210961135104e-06}, {"id": 845, "seek": 490142, "start": 4901.42, "end": 4914.42, "text": " OK, so some properties of coordinate wise method, the rows or columns don't need to be ordered in any way. You can put them in kind of in any order.", "tokens": [2264, 11, 370, 512, 7221, 295, 15670, 10829, 3170, 11, 264, 13241, 420, 13766, 500, 380, 643, 281, 312, 8866, 294, 604, 636, 13, 509, 393, 829, 552, 294, 733, 295, 294, 604, 1668, 13], "temperature": 0.0, "avg_logprob": -0.113723886013031, "compression_ratio": 1.5128205128205128, "no_speech_prob": 1.045129647536669e-05}, {"id": 846, "seek": 490142, "start": 4914.42, "end": 4926.42, "text": " And then you have to store NNZ stands for number of non-zeros, but yeah, the number of non-zero entries plus 2 times NNZ integers for the indices.", "tokens": [400, 550, 291, 362, 281, 3531, 426, 45, 57, 7382, 337, 1230, 295, 2107, 12, 4527, 329, 11, 457, 1338, 11, 264, 1230, 295, 2107, 12, 32226, 23041, 1804, 568, 1413, 426, 45, 57, 41674, 337, 264, 43840, 13], "temperature": 0.0, "avg_logprob": -0.113723886013031, "compression_ratio": 1.5128205128205128, "no_speech_prob": 1.045129647536669e-05}, {"id": 847, "seek": 492642, "start": 4926.42, "end": 4942.42, "text": " I should have noted that above, like if these if these were decimals, I don't know, we're not really getting into memory types, but row and column are integers so they can take up less memory.", "tokens": [286, 820, 362, 12964, 300, 3673, 11, 411, 498, 613, 498, 613, 645, 979, 332, 1124, 11, 286, 500, 380, 458, 11, 321, 434, 406, 534, 1242, 666, 4675, 3467, 11, 457, 5386, 293, 7738, 366, 41674, 370, 436, 393, 747, 493, 1570, 4675, 13], "temperature": 0.0, "avg_logprob": -0.12828940761332608, "compression_ratio": 1.4014598540145986, "no_speech_prob": 6.438929631258361e-06}, {"id": 848, "seek": 494242, "start": 4942.42, "end": 4957.42, "text": " OK, so then next, oh, actually, and first it talks about matrix vector multiplication. And so with that, what you can do is just basically loop through all your entries.", "tokens": [2264, 11, 370, 550, 958, 11, 1954, 11, 767, 11, 293, 700, 309, 6686, 466, 8141, 8062, 27290, 13, 400, 370, 365, 300, 11, 437, 291, 393, 360, 307, 445, 1936, 6367, 807, 439, 428, 23041, 13], "temperature": 0.0, "avg_logprob": -0.12453651428222656, "compression_ratio": 1.3612903225806452, "no_speech_prob": 1.834220347518567e-05}, {"id": 849, "seek": 494242, "start": 4957.42, "end": 4962.42, "text": " So here it's just looping through values.", "tokens": [407, 510, 309, 311, 445, 6367, 278, 807, 4190, 13], "temperature": 0.0, "avg_logprob": -0.12453651428222656, "compression_ratio": 1.3612903225806452, "no_speech_prob": 1.834220347518567e-05}, {"id": 850, "seek": 496242, "start": 4962.42, "end": 4980.42, "text": " And then for those values, you know, looking up the column value and the row value, but you don't actually have to go through every spot in the matrix. You're just going through the number of non-zeros because you kind of loop through that value vector.", "tokens": [400, 550, 337, 729, 4190, 11, 291, 458, 11, 1237, 493, 264, 7738, 2158, 293, 264, 5386, 2158, 11, 457, 291, 500, 380, 767, 362, 281, 352, 807, 633, 4008, 294, 264, 8141, 13, 509, 434, 445, 516, 807, 264, 1230, 295, 2107, 12, 4527, 329, 570, 291, 733, 295, 6367, 807, 300, 2158, 8062, 13], "temperature": 0.0, "avg_logprob": -0.07255083322525024, "compression_ratio": 1.4831460674157304, "no_speech_prob": 1.0783121069835033e-05}, {"id": 851, "seek": 496242, "start": 4980.42, "end": 4986.42, "text": " Questions?", "tokens": [27738, 30], "temperature": 0.0, "avg_logprob": -0.07255083322525024, "compression_ratio": 1.4831460674157304, "no_speech_prob": 1.0783121069835033e-05}, {"id": 852, "seek": 498642, "start": 4986.42, "end": 4994.42, "text": " OK, so then it go on to the compressed sparse row data structure.", "tokens": [2264, 11, 370, 550, 309, 352, 322, 281, 264, 30353, 637, 11668, 5386, 1412, 3877, 13], "temperature": 0.0, "avg_logprob": -0.10840396421501436, "compression_ratio": 1.5463917525773196, "no_speech_prob": 1.184276243293425e-05}, {"id": 853, "seek": 498642, "start": 4994.42, "end": 4998.42, "text": " So this is actually kind of even more consolidated.", "tokens": [407, 341, 307, 767, 733, 295, 754, 544, 49008, 13], "temperature": 0.0, "avg_logprob": -0.10840396421501436, "compression_ratio": 1.5463917525773196, "no_speech_prob": 1.184276243293425e-05}, {"id": 854, "seek": 498642, "start": 4998.42, "end": 5006.42, "text": " And the idea is we can store things in row, sorry, in order by row.", "tokens": [400, 264, 1558, 307, 321, 393, 3531, 721, 294, 5386, 11, 2597, 11, 294, 1668, 538, 5386, 13], "temperature": 0.0, "avg_logprob": -0.10840396421501436, "compression_ratio": 1.5463917525773196, "no_speech_prob": 1.184276243293425e-05}, {"id": 855, "seek": 498642, "start": 5006.42, "end": 5012.42, "text": " And then we're only having to keep track of like when we go to a new row. So we have a row pointer that does that.", "tokens": [400, 550, 321, 434, 787, 1419, 281, 1066, 2837, 295, 411, 562, 321, 352, 281, 257, 777, 5386, 13, 407, 321, 362, 257, 5386, 23918, 300, 775, 300, 13], "temperature": 0.0, "avg_logprob": -0.10840396421501436, "compression_ratio": 1.5463917525773196, "no_speech_prob": 1.184276243293425e-05}, {"id": 856, "seek": 501242, "start": 5012.42, "end": 5020.42, "text": " So here you'll see in row zero, we've got 11, 12, and 14. So we store the values 11, 12, and 14.", "tokens": [407, 510, 291, 603, 536, 294, 5386, 4018, 11, 321, 600, 658, 2975, 11, 2272, 11, 293, 3499, 13, 407, 321, 3531, 264, 4190, 2975, 11, 2272, 11, 293, 3499, 13], "temperature": 0.0, "avg_logprob": -0.12048511505126953, "compression_ratio": 1.5449101796407185, "no_speech_prob": 7.5278499025444034e-06}, {"id": 857, "seek": 501242, "start": 5020.42, "end": 5026.42, "text": " And then we're saying those are in row zero.", "tokens": [400, 550, 321, 434, 1566, 729, 366, 294, 5386, 4018, 13], "temperature": 0.0, "avg_logprob": -0.12048511505126953, "compression_ratio": 1.5449101796407185, "no_speech_prob": 7.5278499025444034e-06}, {"id": 858, "seek": 501242, "start": 5026.42, "end": 5033.42, "text": " Or here, let me come back to the row pointer a bit in a bit. So we're saying, OK, those were in columns 0, 1, and 3.", "tokens": [1610, 510, 11, 718, 385, 808, 646, 281, 264, 5386, 23918, 257, 857, 294, 257, 857, 13, 407, 321, 434, 1566, 11, 2264, 11, 729, 645, 294, 13766, 1958, 11, 502, 11, 293, 805, 13], "temperature": 0.0, "avg_logprob": -0.12048511505126953, "compression_ratio": 1.5449101796407185, "no_speech_prob": 7.5278499025444034e-06}, {"id": 859, "seek": 503342, "start": 5033.42, "end": 5045.42, "text": " And then we don't enter anything into this row pointer vector until we go to the next row. And so basically we're saying, OK, entries 0, 1, and 2 are in row zero.", "tokens": [400, 550, 321, 500, 380, 3242, 1340, 666, 341, 5386, 23918, 8062, 1826, 321, 352, 281, 264, 958, 5386, 13, 400, 370, 1936, 321, 434, 1566, 11, 2264, 11, 23041, 1958, 11, 502, 11, 293, 568, 366, 294, 5386, 4018, 13], "temperature": 0.0, "avg_logprob": -0.10982794111425226, "compression_ratio": 1.5588235294117647, "no_speech_prob": 6.438937816710677e-06}, {"id": 860, "seek": 503342, "start": 5045.42, "end": 5059.42, "text": " But then entry 3 is in row 1. So for this row pointer, you actually if you look back to the index, that's kind of telling you, OK, here row 3 is the first,", "tokens": [583, 550, 8729, 805, 307, 294, 5386, 502, 13, 407, 337, 341, 5386, 23918, 11, 291, 767, 498, 291, 574, 646, 281, 264, 8186, 11, 300, 311, 733, 295, 3585, 291, 11, 2264, 11, 510, 5386, 805, 307, 264, 700, 11], "temperature": 0.0, "avg_logprob": -0.10982794111425226, "compression_ratio": 1.5588235294117647, "no_speech_prob": 6.438937816710677e-06}, {"id": 861, "seek": 505942, "start": 5059.42, "end": 5070.42, "text": " sorry, the third value, which is 22, is the first thing in row 1. Then a new row starts with the sixth value.", "tokens": [2597, 11, 264, 2636, 2158, 11, 597, 307, 5853, 11, 307, 264, 700, 551, 294, 5386, 502, 13, 1396, 257, 777, 5386, 3719, 365, 264, 15102, 2158, 13], "temperature": 0.0, "avg_logprob": -0.0885247537645243, "compression_ratio": 1.4814814814814814, "no_speech_prob": 7.296308922377648e-06}, {"id": 862, "seek": 505942, "start": 5070.42, "end": 5081.42, "text": " The sixth value we can look up here is 31. And so row 2 must start with a 31. And it does.", "tokens": [440, 15102, 2158, 321, 393, 574, 493, 510, 307, 10353, 13, 400, 370, 5386, 568, 1633, 722, 365, 257, 10353, 13, 400, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.0885247537645243, "compression_ratio": 1.4814814814814814, "no_speech_prob": 7.296308922377648e-06}, {"id": 863, "seek": 508142, "start": 5081.42, "end": 5089.42, "text": " So this takes a little bit of getting used to how it works. Let me do another one.", "tokens": [407, 341, 2516, 257, 707, 857, 295, 1242, 1143, 281, 577, 309, 1985, 13, 961, 385, 360, 1071, 472, 13], "temperature": 0.0, "avg_logprob": -0.1386441503252302, "compression_ratio": 1.3380281690140845, "no_speech_prob": 1.679722572589526e-06}, {"id": 864, "seek": 508142, "start": 5089.42, "end": 5101.42, "text": " The third row starts with a 9, no, the ninth entry. Yeah. So we go over here. That's 42 is the ninth value.", "tokens": [440, 2636, 5386, 3719, 365, 257, 1722, 11, 572, 11, 264, 28207, 8729, 13, 865, 13, 407, 321, 352, 670, 510, 13, 663, 311, 14034, 307, 264, 28207, 2158, 13], "temperature": 0.0, "avg_logprob": -0.1386441503252302, "compression_ratio": 1.3380281690140845, "no_speech_prob": 1.679722572589526e-06}, {"id": 865, "seek": 510142, "start": 5101.42, "end": 5111.42, "text": " And that is the first the first one in row 3 since we started indexing at zero.", "tokens": [400, 300, 307, 264, 700, 264, 700, 472, 294, 5386, 805, 1670, 321, 1409, 8186, 278, 412, 4018, 13], "temperature": 0.0, "avg_logprob": -0.09878898071030438, "compression_ratio": 1.457142857142857, "no_speech_prob": 2.52155177804525e-06}, {"id": 866, "seek": 510142, "start": 5111.42, "end": 5122.42, "text": " Then we can go to the 12th value is 55. 55 is the first thing in the fourth row.", "tokens": [1396, 321, 393, 352, 281, 264, 2272, 392, 2158, 307, 12330, 13, 12330, 307, 264, 700, 551, 294, 264, 6409, 5386, 13], "temperature": 0.0, "avg_logprob": -0.09878898071030438, "compression_ratio": 1.457142857142857, "no_speech_prob": 2.52155177804525e-06}, {"id": 867, "seek": 510142, "start": 5122.42, "end": 5128.42, "text": " So everyone see kind of what this is doing.", "tokens": [407, 1518, 536, 733, 295, 437, 341, 307, 884, 13], "temperature": 0.0, "avg_logprob": -0.09878898071030438, "compression_ratio": 1.457142857142857, "no_speech_prob": 2.52155177804525e-06}, {"id": 868, "seek": 512842, "start": 5128.42, "end": 5142.42, "text": " So what what do you think a benefit of this is? This is on the surface harder to I think figure out at first.", "tokens": [407, 437, 437, 360, 291, 519, 257, 5121, 295, 341, 307, 30, 639, 307, 322, 264, 3753, 6081, 281, 286, 519, 2573, 484, 412, 700, 13], "temperature": 0.0, "avg_logprob": -0.2105970064798991, "compression_ratio": 1.211111111111111, "no_speech_prob": 4.757383794640191e-05}, {"id": 869, "seek": 514242, "start": 5142.42, "end": 5158.42, "text": " You see Jeremy can pass the microphone. I think this present a way we can actually put the position of the nonzero entries in some kind of table matrix. And for the computer, it's actually not very easy.", "tokens": [509, 536, 17809, 393, 1320, 264, 10952, 13, 286, 519, 341, 1974, 257, 636, 321, 393, 767, 829, 264, 2535, 295, 264, 2107, 32226, 23041, 294, 512, 733, 295, 3199, 8141, 13, 400, 337, 264, 3820, 11, 309, 311, 767, 406, 588, 1858, 13], "temperature": 0.0, "avg_logprob": -0.36703284581502277, "compression_ratio": 1.4097222222222223, "no_speech_prob": 0.00011590501526370645}, {"id": 870, "seek": 515842, "start": 5158.42, "end": 5173.42, "text": " It's not very hard to actually look up these values. So yes, that's why it's beneficial. Yeah. So kind of the the coordinate method also give us a way to put them in a table.", "tokens": [467, 311, 406, 588, 1152, 281, 767, 574, 493, 613, 4190, 13, 407, 2086, 11, 300, 311, 983, 309, 311, 14072, 13, 865, 13, 407, 733, 295, 264, 264, 15670, 3170, 611, 976, 505, 257, 636, 281, 829, 552, 294, 257, 3199, 13], "temperature": 0.0, "avg_logprob": -0.158976880515494, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.763286076718941e-05}, {"id": 871, "seek": 515842, "start": 5173.42, "end": 5184.42, "text": " What's what's the benefit of this over kind of that previous coordinate method? I think the previous coordinate, unless we can't really put every single value right.", "tokens": [708, 311, 437, 311, 264, 5121, 295, 341, 670, 733, 295, 300, 3894, 15670, 3170, 30, 286, 519, 264, 3894, 15670, 11, 5969, 321, 393, 380, 534, 829, 633, 2167, 2158, 558, 13], "temperature": 0.0, "avg_logprob": -0.158976880515494, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.763286076718941e-05}, {"id": 872, "seek": 518442, "start": 5184.42, "end": 5191.42, "text": " This is a lot more compact into one fixed length fixed width table.", "tokens": [639, 307, 257, 688, 544, 14679, 666, 472, 6806, 4641, 6806, 11402, 3199, 13], "temperature": 0.0, "avg_logprob": -0.17931982676188152, "compression_ratio": 1.553763440860215, "no_speech_prob": 1.5205758245429024e-05}, {"id": 873, "seek": 518442, "start": 5191.42, "end": 5200.42, "text": " But previously, if we expand it out, it's no longer valuable. So we have to compact it into this.", "tokens": [583, 8046, 11, 498, 321, 5268, 309, 484, 11, 309, 311, 572, 2854, 8263, 13, 407, 321, 362, 281, 14679, 309, 666, 341, 13], "temperature": 0.0, "avg_logprob": -0.17931982676188152, "compression_ratio": 1.553763440860215, "no_speech_prob": 1.5205758245429024e-05}, {"id": 874, "seek": 518442, "start": 5200.42, "end": 5207.42, "text": " OK, so this is this is more compact for the for the row pointer. You're just keeping track of kind of when you switch rows.", "tokens": [2264, 11, 370, 341, 307, 341, 307, 544, 14679, 337, 264, 337, 264, 5386, 23918, 13, 509, 434, 445, 5145, 2837, 295, 733, 295, 562, 291, 3679, 13241, 13], "temperature": 0.0, "avg_logprob": -0.17931982676188152, "compression_ratio": 1.553763440860215, "no_speech_prob": 1.5205758245429024e-05}, {"id": 875, "seek": 520742, "start": 5207.42, "end": 5214.42, "text": " So you've got less redundant information. Can anyone think of any other potential benefits, Matthew?", "tokens": [407, 291, 600, 658, 1570, 40997, 1589, 13, 1664, 2878, 519, 295, 604, 661, 3995, 5311, 11, 12434, 30], "temperature": 0.0, "avg_logprob": -0.3494965235392253, "compression_ratio": 1.3496932515337423, "no_speech_prob": 4.784872544405516e-06}, {"id": 876, "seek": 520742, "start": 5214.42, "end": 5219.42, "text": " I'm going to grab the microphone.", "tokens": [286, 478, 516, 281, 4444, 264, 10952, 13], "temperature": 0.0, "avg_logprob": -0.3494965235392253, "compression_ratio": 1.3496932515337423, "no_speech_prob": 4.784872544405516e-06}, {"id": 877, "seek": 520742, "start": 5219.42, "end": 5225.42, "text": " Seems like it provides a linear format.", "tokens": [22524, 411, 309, 6417, 257, 8213, 7877, 13], "temperature": 0.0, "avg_logprob": -0.3494965235392253, "compression_ratio": 1.3496932515337423, "no_speech_prob": 4.784872544405516e-06}, {"id": 878, "seek": 520742, "start": 5225.42, "end": 5229.42, "text": " The whole thing just transpose into a vector.", "tokens": [440, 1379, 551, 445, 25167, 666, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.3494965235392253, "compression_ratio": 1.3496932515337423, "no_speech_prob": 4.784872544405516e-06}, {"id": 879, "seek": 522942, "start": 5229.42, "end": 5236.42, "text": " I don't know if that would be really useful for operation, but we're fairly efficient.", "tokens": [286, 500, 380, 458, 498, 300, 576, 312, 534, 4420, 337, 6916, 11, 457, 321, 434, 6457, 7148, 13], "temperature": 0.0, "avg_logprob": -0.2067702293395996, "compression_ratio": 1.6007751937984496, "no_speech_prob": 2.318570841453038e-05}, {"id": 880, "seek": 522942, "start": 5236.42, "end": 5245.42, "text": " So I think this may be similar to what you're saying that this makes it really easy to look at particular rows, like if you needed to access your data by row.", "tokens": [407, 286, 519, 341, 815, 312, 2531, 281, 437, 291, 434, 1566, 300, 341, 1669, 309, 534, 1858, 281, 574, 412, 1729, 13241, 11, 411, 498, 291, 2978, 281, 2105, 428, 1412, 538, 5386, 13], "temperature": 0.0, "avg_logprob": -0.2067702293395996, "compression_ratio": 1.6007751937984496, "no_speech_prob": 2.318570841453038e-05}, {"id": 881, "seek": 522942, "start": 5245.42, "end": 5256.42, "text": " And so like if you were interested in getting row with index three, you know, you can just look up, OK, that starts at nine and then it's stopping right before twelve.", "tokens": [400, 370, 411, 498, 291, 645, 3102, 294, 1242, 5386, 365, 8186, 1045, 11, 291, 458, 11, 291, 393, 445, 574, 493, 11, 2264, 11, 300, 3719, 412, 4949, 293, 550, 309, 311, 12767, 558, 949, 14390, 13], "temperature": 0.0, "avg_logprob": -0.2067702293395996, "compression_ratio": 1.6007751937984496, "no_speech_prob": 2.318570841453038e-05}, {"id": 882, "seek": 525642, "start": 5256.42, "end": 5262.42, "text": " So then you can come over here and say, OK, let me get the values that start at nine and stop right before twelve.", "tokens": [407, 550, 291, 393, 808, 670, 510, 293, 584, 11, 2264, 11, 718, 385, 483, 264, 4190, 300, 722, 412, 4949, 293, 1590, 558, 949, 14390, 13], "temperature": 0.0, "avg_logprob": -0.08498205654862998, "compression_ratio": 1.421383647798742, "no_speech_prob": 3.7265731407387648e-06}, {"id": 883, "seek": 525642, "start": 5262.42, "end": 5269.42, "text": " And I've picked out this row in a fast way.", "tokens": [400, 286, 600, 6183, 484, 341, 5386, 294, 257, 2370, 636, 13], "temperature": 0.0, "avg_logprob": -0.08498205654862998, "compression_ratio": 1.421383647798742, "no_speech_prob": 3.7265731407387648e-06}, {"id": 884, "seek": 525642, "start": 5269.42, "end": 5277.42, "text": " So I'd say this is.", "tokens": [407, 286, 1116, 584, 341, 307, 13], "temperature": 0.0, "avg_logprob": -0.08498205654862998, "compression_ratio": 1.421383647798742, "no_speech_prob": 3.7265731407387648e-06}, {"id": 885, "seek": 525642, "start": 5277.42, "end": 5281.42, "text": " Row access is easy, I guess is how they say it.", "tokens": [20309, 2105, 307, 1858, 11, 286, 2041, 307, 577, 436, 584, 309, 13], "temperature": 0.0, "avg_logprob": -0.08498205654862998, "compression_ratio": 1.421383647798742, "no_speech_prob": 3.7265731407387648e-06}, {"id": 886, "seek": 528142, "start": 5281.42, "end": 5292.42, "text": " But notice it's difficult to look up columns this way. In fact, it's going to be like I mean, so you can look up, you know, like go through here and say like, OK, I want to know everything.", "tokens": [583, 3449, 309, 311, 2252, 281, 574, 493, 13766, 341, 636, 13, 682, 1186, 11, 309, 311, 516, 281, 312, 411, 286, 914, 11, 370, 291, 393, 574, 493, 11, 291, 458, 11, 411, 352, 807, 510, 293, 584, 411, 11, 2264, 11, 286, 528, 281, 458, 1203, 13], "temperature": 0.0, "avg_logprob": -0.10111926949542502, "compression_ratio": 1.6, "no_speech_prob": 7.690284746786347e-07}, {"id": 887, "seek": 528142, "start": 5292.42, "end": 5296.42, "text": " Oops, didn't mean to enlarge that.", "tokens": [21726, 11, 994, 380, 914, 281, 31976, 432, 300, 13], "temperature": 0.0, "avg_logprob": -0.10111926949542502, "compression_ratio": 1.6, "no_speech_prob": 7.690284746786347e-07}, {"id": 888, "seek": 528142, "start": 5296.42, "end": 5304.42, "text": " You know, you might say, OK, I want everything that's in column four, which is a lot of things.", "tokens": [509, 458, 11, 291, 1062, 584, 11, 2264, 11, 286, 528, 1203, 300, 311, 294, 7738, 1451, 11, 597, 307, 257, 688, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.10111926949542502, "compression_ratio": 1.6, "no_speech_prob": 7.690284746786347e-07}, {"id": 889, "seek": 530442, "start": 5304.42, "end": 5314.42, "text": " But notice going to OK, here's here's something column four, twenty five. If I want to find what row twenty five is in,", "tokens": [583, 3449, 516, 281, 2264, 11, 510, 311, 510, 311, 746, 7738, 1451, 11, 7699, 1732, 13, 759, 286, 528, 281, 915, 437, 5386, 7699, 1732, 307, 294, 11], "temperature": 0.0, "avg_logprob": -0.13176674745520767, "compression_ratio": 1.6883720930232557, "no_speech_prob": 1.228896849170269e-06}, {"id": 890, "seek": 530442, "start": 5314.42, "end": 5321.42, "text": " that's going to be a little bit more of a pain. Like I mean, you're going to have to kind of like go through your your row pointers.", "tokens": [300, 311, 516, 281, 312, 257, 707, 857, 544, 295, 257, 1822, 13, 1743, 286, 914, 11, 291, 434, 516, 281, 362, 281, 733, 295, 411, 352, 807, 428, 428, 5386, 44548, 13], "temperature": 0.0, "avg_logprob": -0.13176674745520767, "compression_ratio": 1.6883720930232557, "no_speech_prob": 1.228896849170269e-06}, {"id": 891, "seek": 530442, "start": 5321.42, "end": 5326.42, "text": " You know, here there are a bunch of like even yeah, you could say like, OK, forty five is also in column four.", "tokens": [509, 458, 11, 510, 456, 366, 257, 3840, 295, 411, 754, 1338, 11, 291, 727, 584, 411, 11, 2264, 11, 15815, 1732, 307, 611, 294, 7738, 1451, 13], "temperature": 0.0, "avg_logprob": -0.13176674745520767, "compression_ratio": 1.6883720930232557, "no_speech_prob": 1.228896849170269e-06}, {"id": 892, "seek": 532642, "start": 5326.42, "end": 5337.42, "text": " Now I have to figure out which row it's in. And that's less obvious. So kind of knowing if you're going to be accessing your data by rows or columns would be really important here.", "tokens": [823, 286, 362, 281, 2573, 484, 597, 5386, 309, 311, 294, 13, 400, 300, 311, 1570, 6322, 13, 407, 733, 295, 5276, 498, 291, 434, 516, 281, 312, 26440, 428, 1412, 538, 13241, 420, 13766, 576, 312, 534, 1021, 510, 13], "temperature": 0.0, "avg_logprob": -0.2112422105742664, "compression_ratio": 1.4691943127962086, "no_speech_prob": 6.5401623032812495e-06}, {"id": 893, "seek": 532642, "start": 5337.42, "end": 5341.42, "text": " Allenton, can you grab the microphone?", "tokens": [1057, 317, 266, 11, 393, 291, 4444, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.2112422105742664, "compression_ratio": 1.4691943127962086, "no_speech_prob": 6.5401623032812495e-06}, {"id": 894, "seek": 532642, "start": 5341.42, "end": 5344.42, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.2112422105742664, "compression_ratio": 1.4691943127962086, "no_speech_prob": 6.5401623032812495e-06}, {"id": 895, "seek": 532642, "start": 5344.42, "end": 5353.42, "text": " So what if we just transpose the matrix, then it will be easy to find the culprit.", "tokens": [407, 437, 498, 321, 445, 25167, 264, 8141, 11, 550, 309, 486, 312, 1858, 281, 915, 264, 39220, 13], "temperature": 0.0, "avg_logprob": -0.2112422105742664, "compression_ratio": 1.4691943127962086, "no_speech_prob": 6.5401623032812495e-06}, {"id": 896, "seek": 535342, "start": 5353.42, "end": 5362.42, "text": " So when you say transpose, so there's also a compressed sparse column format, which is the same thing only by column.", "tokens": [407, 562, 291, 584, 25167, 11, 370, 456, 311, 611, 257, 30353, 637, 11668, 7738, 7877, 11, 597, 307, 264, 912, 551, 787, 538, 7738, 13], "temperature": 0.0, "avg_logprob": -0.19141301241787997, "compression_ratio": 1.4113924050632911, "no_speech_prob": 5.9198649978498e-05}, {"id": 897, "seek": 535342, "start": 5362.42, "end": 5370.42, "text": " OK, and then just to the question. So if we just traverse through this representation to find like row I,", "tokens": [2264, 11, 293, 550, 445, 281, 264, 1168, 13, 407, 498, 321, 445, 45674, 807, 341, 10290, 281, 915, 411, 5386, 286, 11], "temperature": 0.0, "avg_logprob": -0.19141301241787997, "compression_ratio": 1.4113924050632911, "no_speech_prob": 5.9198649978498e-05}, {"id": 898, "seek": 537042, "start": 5370.42, "end": 5384.42, "text": " the complexity is the whole of N where N is the number of rows. So in the worst case scenario, we'll have to make N steps until we go to the row number N.", "tokens": [264, 14024, 307, 264, 1379, 295, 426, 689, 426, 307, 264, 1230, 295, 13241, 13, 407, 294, 264, 5855, 1389, 9005, 11, 321, 603, 362, 281, 652, 426, 4439, 1826, 321, 352, 281, 264, 5386, 1230, 426, 13], "temperature": 0.0, "avg_logprob": -0.31575041725521996, "compression_ratio": 1.3162393162393162, "no_speech_prob": 1.3626646250486374e-05}, {"id": 899, "seek": 538442, "start": 5384.42, "end": 5400.42, "text": " Say that again if you're trying to find what?", "tokens": [6463, 300, 797, 498, 291, 434, 1382, 281, 915, 437, 30], "temperature": 0.0, "avg_logprob": -0.27887245814005535, "compression_ratio": 0.8653846153846154, "no_speech_prob": 2.144194695574697e-05}, {"id": 900, "seek": 540042, "start": 5400.42, "end": 5415.42, "text": " It'll. Yeah, I mean, I do think you'll have to walk through the number of row pointers, but that's going to be when you're saying I is like in this case, eight by eight, like the number of potential rows there are.", "tokens": [467, 603, 13, 865, 11, 286, 914, 11, 286, 360, 519, 291, 603, 362, 281, 1792, 807, 264, 1230, 295, 5386, 44548, 11, 457, 300, 311, 516, 281, 312, 562, 291, 434, 1566, 286, 307, 411, 294, 341, 1389, 11, 3180, 538, 3180, 11, 411, 264, 1230, 295, 3995, 13241, 456, 366, 13], "temperature": 0.0, "avg_logprob": -0.11459467703836006, "compression_ratio": 1.4657534246575343, "no_speech_prob": 6.20412320131436e-05}, {"id": 901, "seek": 541542, "start": 5415.42, "end": 5432.42, "text": " I think that's true that you.", "tokens": [286, 519, 300, 311, 2074, 300, 291, 13], "temperature": 0.0, "avg_logprob": -0.3030169407526652, "compression_ratio": 0.90625, "no_speech_prob": 1.5778790611875593e-06}, {"id": 902, "seek": 543242, "start": 5432.42, "end": 5449.42, "text": " So. So you kind of want to you're suggesting almost like combining the coordinate storage where you do have all the rows.", "tokens": [407, 13, 407, 291, 733, 295, 528, 281, 291, 434, 18094, 1920, 411, 21928, 264, 15670, 6725, 689, 291, 360, 362, 439, 264, 13241, 13], "temperature": 0.0, "avg_logprob": -0.1291841227432777, "compression_ratio": 1.2222222222222223, "no_speech_prob": 4.494983386393869e-06}, {"id": 903, "seek": 544942, "start": 5449.42, "end": 5462.42, "text": " And so I'll say like these have different benefits, like so if you really were going to have to be looking things up by row and column a lot, you might want just to go with the coordinate wise method of storage.", "tokens": [400, 370, 286, 603, 584, 411, 613, 362, 819, 5311, 11, 411, 370, 498, 291, 534, 645, 516, 281, 362, 281, 312, 1237, 721, 493, 538, 5386, 293, 7738, 257, 688, 11, 291, 1062, 528, 445, 281, 352, 365, 264, 15670, 10829, 3170, 295, 6725, 13], "temperature": 0.0, "avg_logprob": -0.11248856602293072, "compression_ratio": 1.5480225988700564, "no_speech_prob": 1.69639679370448e-05}, {"id": 904, "seek": 544942, "start": 5462.42, "end": 5467.42, "text": " Since that does look like you look things up by row or column.", "tokens": [4162, 300, 775, 574, 411, 291, 574, 721, 493, 538, 5386, 420, 7738, 13], "temperature": 0.0, "avg_logprob": -0.11248856602293072, "compression_ratio": 1.5480225988700564, "no_speech_prob": 1.69639679370448e-05}, {"id": 905, "seek": 546742, "start": 5467.42, "end": 5488.42, "text": " You know, like kind of if you start having aspects of both, you are taking up a lot more storage space. So then there's a trade off of like, you know, if you have both like coordinate wise storage and the compressed row storage, you've now really increased kind of how much memory you're using.", "tokens": [509, 458, 11, 411, 733, 295, 498, 291, 722, 1419, 7270, 295, 1293, 11, 291, 366, 1940, 493, 257, 688, 544, 6725, 1901, 13, 407, 550, 456, 311, 257, 4923, 766, 295, 411, 11, 291, 458, 11, 498, 291, 362, 1293, 411, 15670, 10829, 6725, 293, 264, 30353, 5386, 6725, 11, 291, 600, 586, 534, 6505, 733, 295, 577, 709, 4675, 291, 434, 1228, 13], "temperature": 0.0, "avg_logprob": -0.09025558526965155, "compression_ratio": 1.6153846153846154, "no_speech_prob": 5.17351600137772e-06}, {"id": 906, "seek": 548842, "start": 5488.42, "end": 5512.42, "text": " Sam.", "tokens": [4832, 13], "temperature": 0.0, "avg_logprob": -0.3117167552312215, "compression_ratio": 0.3333333333333333, "no_speech_prob": 6.539125479321228e-06}, {"id": 907, "seek": 551242, "start": 5512.42, "end": 5528.42, "text": " So it's.", "tokens": [407, 309, 311, 13], "temperature": 0.0, "avg_logprob": -0.32282915711402893, "compression_ratio": 0.5, "no_speech_prob": 7.410601028823294e-06}, {"id": 908, "seek": 552842, "start": 5528.42, "end": 5545.42, "text": " The issue with that pulling a column from CSR is that you are having to do more work though to find the row coordinates that correspond, you know, like you can iterate through and you're right, you have to go through everything to find all the fours since this isn't in order and COO is not in order either.", "tokens": [440, 2734, 365, 300, 8407, 257, 7738, 490, 9460, 49, 307, 300, 291, 366, 1419, 281, 360, 544, 589, 1673, 281, 915, 264, 5386, 21056, 300, 6805, 11, 291, 458, 11, 411, 291, 393, 44497, 807, 293, 291, 434, 558, 11, 291, 362, 281, 352, 807, 1203, 281, 915, 439, 264, 1451, 82, 1670, 341, 1943, 380, 294, 1668, 293, 3002, 46, 307, 406, 294, 1668, 2139, 13], "temperature": 0.0, "avg_logprob": -0.1560056209564209, "compression_ratio": 1.566326530612245, "no_speech_prob": 4.637554411601741e-06}, {"id": 909, "seek": 554542, "start": 5545.42, "end": 5563.42, "text": " But with COO when you find the fours, you're also automatically getting what the row index is. Whereas here you have to kind of do this like bookkeeping of like, okay, you know, here was a four at value 10, which row corresponds to value 10.", "tokens": [583, 365, 3002, 46, 562, 291, 915, 264, 1451, 82, 11, 291, 434, 611, 6772, 1242, 437, 264, 5386, 8186, 307, 13, 13813, 510, 291, 362, 281, 733, 295, 360, 341, 411, 1446, 25769, 295, 411, 11, 1392, 11, 291, 458, 11, 510, 390, 257, 1451, 412, 2158, 1266, 11, 597, 5386, 23249, 281, 2158, 1266, 13], "temperature": 0.0, "avg_logprob": -0.11463834143973686, "compression_ratio": 1.4818652849740932, "no_speech_prob": 2.6839913971343776e-06}, {"id": 910, "seek": 554542, "start": 5563.42, "end": 5574.42, "text": " So that is more work to figure out the rows.", "tokens": [407, 300, 307, 544, 589, 281, 2573, 484, 264, 13241, 13], "temperature": 0.0, "avg_logprob": -0.11463834143973686, "compression_ratio": 1.4818652849740932, "no_speech_prob": 2.6839913971343776e-06}, {"id": 911, "seek": 557442, "start": 5574.42, "end": 5583.42, "text": " Yeah, these are these are good questions though. Yeah, this is good to think about. Any other questions on this? And I'm not going to go through.", "tokens": [865, 11, 613, 366, 613, 366, 665, 1651, 1673, 13, 865, 11, 341, 307, 665, 281, 519, 466, 13, 2639, 661, 1651, 322, 341, 30, 400, 286, 478, 406, 516, 281, 352, 807, 13], "temperature": 0.0, "avg_logprob": -0.15570132606907894, "compression_ratio": 1.7566371681415929, "no_speech_prob": 1.4509634638670832e-05}, {"id": 912, "seek": 557442, "start": 5583.42, "end": 5601.42, "text": " Actually, I think they don't have compressed column on this one because it's so similar to compressed row only now it's easy to look up or easy to get a column harder to get a row of data because you would have to like work towards to find the column.", "tokens": [5135, 11, 286, 519, 436, 500, 380, 362, 30353, 7738, 322, 341, 472, 570, 309, 311, 370, 2531, 281, 30353, 5386, 787, 586, 309, 311, 1858, 281, 574, 493, 420, 1858, 281, 483, 257, 7738, 6081, 281, 483, 257, 5386, 295, 1412, 570, 291, 576, 362, 281, 411, 589, 3030, 281, 915, 264, 7738, 13], "temperature": 0.0, "avg_logprob": -0.15570132606907894, "compression_ratio": 1.7566371681415929, "no_speech_prob": 1.4509634638670832e-05}, {"id": 913, "seek": 560142, "start": 5601.42, "end": 5609.42, "text": " So here the amount of memory accesses is reduced. It's really.", "tokens": [407, 510, 264, 2372, 295, 4675, 2105, 279, 307, 9212, 13, 467, 311, 534, 13], "temperature": 0.0, "avg_logprob": -0.1346549218700778, "compression_ratio": 1.4, "no_speech_prob": 6.962088718864834e-06}, {"id": 914, "seek": 560142, "start": 5609.42, "end": 5621.42, "text": " Or they even kind of say advantage of CSR over coordinate wise method. Yeah, fewer memory accesses.", "tokens": [1610, 436, 754, 733, 295, 584, 5002, 295, 9460, 49, 670, 15670, 10829, 3170, 13, 865, 11, 13366, 4675, 2105, 279, 13], "temperature": 0.0, "avg_logprob": -0.1346549218700778, "compression_ratio": 1.4, "no_speech_prob": 6.962088718864834e-06}, {"id": 915, "seek": 560142, "start": 5621.42, "end": 5626.42, "text": " But yeah, it will. It will be difficult to look things up by column.", "tokens": [583, 1338, 11, 309, 486, 13, 467, 486, 312, 2252, 281, 574, 721, 493, 538, 7738, 13], "temperature": 0.0, "avg_logprob": -0.1346549218700778, "compression_ratio": 1.4, "no_speech_prob": 6.962088718864834e-06}, {"id": 916, "seek": 562642, "start": 5626.42, "end": 5641.42, "text": " And then actually this other web page, let me just do one more example.", "tokens": [400, 550, 767, 341, 661, 3670, 3028, 11, 718, 385, 445, 360, 472, 544, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1839602470397949, "compression_ratio": 1.0, "no_speech_prob": 5.594253252638737e-06}, {"id": 917, "seek": 564142, "start": 5641.42, "end": 5660.42, "text": " And are you able to see I just like here they have a single matrix and they show it in.", "tokens": [400, 366, 291, 1075, 281, 536, 286, 445, 411, 510, 436, 362, 257, 2167, 8141, 293, 436, 855, 309, 294, 13], "temperature": 0.0, "avg_logprob": -0.139021053314209, "compression_ratio": 1.0740740740740742, "no_speech_prob": 4.425423867360223e-06}, {"id": 918, "seek": 566042, "start": 5660.42, "end": 5672.42, "text": " This also I thought was fun. Just at the top it lists a ton of different sparse matrix compression formats. I had never heard of most of these, but I was like, wow, there are a lot of methods out there.", "tokens": [639, 611, 286, 1194, 390, 1019, 13, 1449, 412, 264, 1192, 309, 14511, 257, 2952, 295, 819, 637, 11668, 8141, 19355, 25879, 13, 286, 632, 1128, 2198, 295, 881, 295, 613, 11, 457, 286, 390, 411, 11, 6076, 11, 456, 366, 257, 688, 295, 7150, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.11553875299600455, "compression_ratio": 1.4125874125874125, "no_speech_prob": 7.646224730706308e-06}, {"id": 919, "seek": 567242, "start": 5672.42, "end": 5694.42, "text": " If you had a very jagged diagonal format, non symmetric skyline format. So depending on, you know, if you had a very.", "tokens": [759, 291, 632, 257, 588, 6368, 3004, 21539, 7877, 11, 2107, 32330, 5443, 1889, 7877, 13, 407, 5413, 322, 11, 291, 458, 11, 498, 291, 632, 257, 588, 13], "temperature": 0.4, "avg_logprob": -0.3695286548498905, "compression_ratio": 1.2446808510638299, "no_speech_prob": 1.696326035016682e-05}, {"id": 920, "seek": 569442, "start": 5694.42, "end": 5703.42, "text": " Color coded this matrix and then it has it in compressed row storage, also in compressed column storage.", "tokens": [10458, 34874, 341, 8141, 293, 550, 309, 575, 309, 294, 30353, 5386, 6725, 11, 611, 294, 30353, 7738, 6725, 13], "temperature": 0.0, "avg_logprob": -0.14208926529180807, "compression_ratio": 1.5773809523809523, "no_speech_prob": 1.0782995559566189e-05}, {"id": 921, "seek": 569442, "start": 5703.42, "end": 5715.42, "text": " And then, yeah, it's even got additional modified ones. But here, yeah, you can compare and they kind of represent the row start index a little bit differently.", "tokens": [400, 550, 11, 1338, 11, 309, 311, 754, 658, 4497, 15873, 2306, 13, 583, 510, 11, 1338, 11, 291, 393, 6794, 293, 436, 733, 295, 2906, 264, 5386, 722, 8186, 257, 707, 857, 7614, 13], "temperature": 0.0, "avg_logprob": -0.14208926529180807, "compression_ratio": 1.5773809523809523, "no_speech_prob": 1.0782995559566189e-05}, {"id": 922, "seek": 571542, "start": 5715.42, "end": 5730.42, "text": " Showing that 11, 12, 13 and 14 this is like light red. Those are in row one. You're just storing a one once, but you have to store the column indexes all these times.", "tokens": [6895, 278, 300, 2975, 11, 2272, 11, 3705, 293, 3499, 341, 307, 411, 1442, 2182, 13, 3950, 366, 294, 5386, 472, 13, 509, 434, 445, 26085, 257, 472, 1564, 11, 457, 291, 362, 281, 3531, 264, 7738, 8186, 279, 439, 613, 1413, 13], "temperature": 0.0, "avg_logprob": -0.14911633349479514, "compression_ratio": 1.2671755725190839, "no_speech_prob": 9.515479177935049e-06}, {"id": 923, "seek": 573042, "start": 5730.42, "end": 5752.42, "text": " And then next step for rows, you just store a five because you're up to value five.", "tokens": [400, 550, 958, 1823, 337, 13241, 11, 291, 445, 3531, 257, 1732, 570, 291, 434, 493, 281, 2158, 1732, 13], "temperature": 0.0, "avg_logprob": -0.12518930435180664, "compression_ratio": 1.077922077922078, "no_speech_prob": 4.936437107971869e-06}, {"id": 924, "seek": 575242, "start": 5752.42, "end": 5764.42, "text": " And like that is that is something that people do. It's like so, you know, when we saw with the kind of the coordinate wise multiplication that you do, you know, it is possible to only look at the entries you have.", "tokens": [400, 411, 300, 307, 300, 307, 746, 300, 561, 360, 13, 467, 311, 411, 370, 11, 291, 458, 11, 562, 321, 1866, 365, 264, 733, 295, 264, 15670, 10829, 27290, 300, 291, 360, 11, 291, 458, 11, 309, 307, 1944, 281, 787, 574, 412, 264, 23041, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.1542501694116837, "compression_ratio": 1.5824742268041236, "no_speech_prob": 2.9478951546479948e-06}, {"id": 925, "seek": 575242, "start": 5764.42, "end": 5773.42, "text": " But yeah, there are. Yeah, like you can store your kind of block matrix is all being a zero.", "tokens": [583, 1338, 11, 456, 366, 13, 865, 11, 411, 291, 393, 3531, 428, 733, 295, 3461, 8141, 307, 439, 885, 257, 4018, 13], "temperature": 0.0, "avg_logprob": -0.1542501694116837, "compression_ratio": 1.5824742268041236, "no_speech_prob": 2.9478951546479948e-06}, {"id": 926, "seek": 577342, "start": 5773.42, "end": 5790.42, "text": " Yeah. Any any more questions on these sparse formats? And so then I took this from the SciPy documentation.", "tokens": [865, 13, 2639, 604, 544, 1651, 322, 613, 637, 11668, 25879, 30, 400, 370, 550, 286, 1890, 341, 490, 264, 16942, 47, 88, 14333, 13], "temperature": 0.0, "avg_logprob": -0.21318955257021147, "compression_ratio": 1.1145833333333333, "no_speech_prob": 3.1200652301777154e-05}, {"id": 927, "seek": 579042, "start": 5790.42, "end": 5807.42, "text": " So because I was seeing this in like code that I was finding online, the COO format can be a way to efficiently construct matrices. And so sometimes they were constructing them with COO format and then transferring them to CSC or CSR because that.", "tokens": [407, 570, 286, 390, 2577, 341, 294, 411, 3089, 300, 286, 390, 5006, 2950, 11, 264, 3002, 46, 7877, 393, 312, 257, 636, 281, 19621, 7690, 32284, 13, 400, 370, 2171, 436, 645, 39969, 552, 365, 3002, 46, 7877, 293, 550, 31437, 552, 281, 9460, 34, 420, 9460, 49, 570, 300, 13], "temperature": 0.0, "avg_logprob": -0.07924769050196598, "compression_ratio": 1.6995708154506437, "no_speech_prob": 1.473837619414553e-05}, {"id": 928, "seek": 579042, "start": 5807.42, "end": 5817.42, "text": " And I guess this is like it would be it would be a pain to figure out how to manually enter CSC or CSR. So you probably want to use a different way.", "tokens": [400, 286, 2041, 341, 307, 411, 309, 576, 312, 309, 576, 312, 257, 1822, 281, 2573, 484, 577, 281, 16945, 3242, 9460, 34, 420, 9460, 49, 13, 407, 291, 1391, 528, 281, 764, 257, 819, 636, 13], "temperature": 0.0, "avg_logprob": -0.07924769050196598, "compression_ratio": 1.6995708154506437, "no_speech_prob": 1.473837619414553e-05}, {"id": 929, "seek": 581742, "start": 5817.42, "end": 5833.42, "text": " So I just want to note that multiplication, multiplication or inversion are better done with CSC or CSR format and all conversions among the different types of formats are pretty efficient linear time operations.", "tokens": [407, 286, 445, 528, 281, 3637, 300, 27290, 11, 27290, 420, 43576, 366, 1101, 1096, 365, 9460, 34, 420, 9460, 49, 7877, 293, 439, 42256, 3654, 264, 819, 3467, 295, 25879, 366, 1238, 7148, 8213, 565, 7705, 13], "temperature": 0.0, "avg_logprob": -0.10452893802097865, "compression_ratio": 1.4620689655172414, "no_speech_prob": 1.0451195521454792e-05}, {"id": 930, "seek": 583342, "start": 5833.42, "end": 5848.42, "text": " So that's nice that you can go between them and that kind of gets it. I guess the earlier question of, yeah, like what if you want to be able to access rows at some points and columns at another.", "tokens": [407, 300, 311, 1481, 300, 291, 393, 352, 1296, 552, 293, 300, 733, 295, 2170, 309, 13, 286, 2041, 264, 3071, 1168, 295, 11, 1338, 11, 411, 437, 498, 291, 528, 281, 312, 1075, 281, 2105, 13241, 412, 512, 2793, 293, 13766, 412, 1071, 13], "temperature": 0.0, "avg_logprob": -0.11103342017348931, "compression_ratio": 1.4233576642335766, "no_speech_prob": 1.4970664778957143e-05}, {"id": 931, "seek": 584842, "start": 5848.42, "end": 5866.42, "text": " Okay, so we'll be we'll be seeing some sparse sparse matrices in a little bit. But yeah, now I want to kind of start our next application, which is CT scans. And I found this article that I thought was really nice.", "tokens": [1033, 11, 370, 321, 603, 312, 321, 603, 312, 2577, 512, 637, 11668, 637, 11668, 32284, 294, 257, 707, 857, 13, 583, 1338, 11, 586, 286, 528, 281, 733, 295, 722, 527, 958, 3861, 11, 597, 307, 19529, 35116, 13, 400, 286, 1352, 341, 7222, 300, 286, 1194, 390, 534, 1481, 13], "temperature": 0.0, "avg_logprob": -0.1107956085886274, "compression_ratio": 1.3896103896103895, "no_speech_prob": 8.939465260482393e-06}, {"id": 932, "seek": 586642, "start": 5866.42, "end": 5878.42, "text": " And it starts with me pull it up. Saving lives, the mathematics of tomography.", "tokens": [400, 309, 3719, 365, 385, 2235, 309, 493, 13, 318, 6152, 2909, 11, 264, 18666, 295, 2916, 5820, 13], "temperature": 0.0, "avg_logprob": -0.18994900488084362, "compression_ratio": 1.086021505376344, "no_speech_prob": 3.340480361657683e-06}, {"id": 933, "seek": 586642, "start": 5878.42, "end": 5883.42, "text": " There it goes. Whoops.", "tokens": [821, 309, 1709, 13, 45263, 13], "temperature": 0.0, "avg_logprob": -0.18994900488084362, "compression_ratio": 1.086021505376344, "no_speech_prob": 3.340480361657683e-06}, {"id": 934, "seek": 588342, "start": 5883.42, "end": 5899.42, "text": " And it starts with a quote. Can maths really save your life? Of course it can. And this is this is written in a pretty accessible way. It felt like it kind of talks a bit about makes this", "tokens": [400, 309, 3719, 365, 257, 6513, 13, 1664, 36287, 534, 3155, 428, 993, 30, 2720, 1164, 309, 393, 13, 400, 341, 307, 341, 307, 3720, 294, 257, 1238, 9515, 636, 13, 467, 2762, 411, 309, 733, 295, 6686, 257, 857, 466, 1669, 341], "temperature": 0.0, "avg_logprob": -0.1326991243565336, "compression_ratio": 1.3453237410071943, "no_speech_prob": 2.0580157524818787e-06}, {"id": 935, "seek": 589942, "start": 5899.42, "end": 5911.42, "text": " this analogy with milk bottles kind of stored in those kind of stacking crates. So they give actually let me just go up to their problem.", "tokens": [341, 21663, 365, 5392, 15923, 733, 295, 12187, 294, 729, 733, 295, 41376, 941, 1024, 13, 407, 436, 976, 767, 718, 385, 445, 352, 493, 281, 641, 1154, 13], "temperature": 0.0, "avg_logprob": -0.09735944622852763, "compression_ratio": 1.5575757575757576, "no_speech_prob": 1.2805136066162959e-05}, {"id": 936, "seek": 589942, "start": 5911.42, "end": 5919.42, "text": " So they're saying milk and fruit juice is delivered in bottles that are placed in trays, you know, three by three grid.", "tokens": [407, 436, 434, 1566, 5392, 293, 6773, 8544, 307, 10144, 294, 15923, 300, 366, 7074, 294, 47496, 11, 291, 458, 11, 1045, 538, 1045, 10748, 13], "temperature": 0.0, "avg_logprob": -0.09735944622852763, "compression_ratio": 1.5575757575757576, "no_speech_prob": 1.2805136066162959e-05}, {"id": 937, "seek": 591942, "start": 5919.42, "end": 5929.42, "text": " And so maybe you're wondering which type of bottle is in which compartment. And you can't see because there's some in the middle, you know, that like it's been stacked up and we've got these grids.", "tokens": [400, 370, 1310, 291, 434, 6359, 597, 2010, 295, 7817, 307, 294, 597, 26505, 13, 400, 291, 393, 380, 536, 570, 456, 311, 512, 294, 264, 2808, 11, 291, 458, 11, 300, 411, 309, 311, 668, 28867, 493, 293, 321, 600, 658, 613, 677, 3742, 13], "temperature": 0.0, "avg_logprob": -0.09920199021049168, "compression_ratio": 1.7097902097902098, "no_speech_prob": 5.093590971227968e-06}, {"id": 938, "seek": 591942, "start": 5929.42, "end": 5947.42, "text": " And the idea is that different amounts of light passing through milk and juice, you could get a total like from the lights. And this is a bit like a Sudoku puzzle where you just know you know some of the the totals going across a grid, and you're trying to figure out the individual entries.", "tokens": [400, 264, 1558, 307, 300, 819, 11663, 295, 1442, 8437, 807, 5392, 293, 8544, 11, 291, 727, 483, 257, 3217, 411, 490, 264, 5811, 13, 400, 341, 307, 257, 857, 411, 257, 12323, 13275, 12805, 689, 291, 445, 458, 291, 458, 512, 295, 264, 264, 1993, 1124, 516, 2108, 257, 10748, 11, 293, 291, 434, 1382, 281, 2573, 484, 264, 2609, 23041, 13], "temperature": 0.0, "avg_logprob": -0.09920199021049168, "compression_ratio": 1.7097902097902098, "no_speech_prob": 5.093590971227968e-06}, {"id": 939, "seek": 594742, "start": 5947.42, "end": 5962.42, "text": " Are there questions and this is just kind of like as a analogy. So here they're saying okay maybe like five units of light have made it through here. Six, four, six, three, six. Let's work backwards to see which is in each of these.", "tokens": [2014, 456, 1651, 293, 341, 307, 445, 733, 295, 411, 382, 257, 21663, 13, 407, 510, 436, 434, 1566, 1392, 1310, 411, 1732, 6815, 295, 1442, 362, 1027, 309, 807, 510, 13, 11678, 11, 1451, 11, 2309, 11, 1045, 11, 2309, 13, 961, 311, 589, 12204, 281, 536, 597, 307, 294, 1184, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.1705600040059694, "compression_ratio": 1.451086956521739, "no_speech_prob": 2.4298498829011805e-05}, {"id": 940, "seek": 594742, "start": 5962.42, "end": 5974.42, "text": " I don't know if you're amplifying.", "tokens": [286, 500, 380, 458, 498, 291, 434, 9731, 5489, 13], "temperature": 0.0, "avg_logprob": -0.1705600040059694, "compression_ratio": 1.451086956521739, "no_speech_prob": 2.4298498829011805e-05}, {"id": 941, "seek": 597442, "start": 5974.42, "end": 5978.42, "text": " Yeah, the lights are on.", "tokens": [865, 11, 264, 5811, 366, 322, 13], "temperature": 0.0, "avg_logprob": -0.1830622066151012, "compression_ratio": 1.1368421052631579, "no_speech_prob": 7.60069306124933e-05}, {"id": 942, "seek": 597442, "start": 5978.42, "end": 5983.42, "text": " It was it's off now.", "tokens": [467, 390, 309, 311, 766, 586, 13], "temperature": 0.0, "avg_logprob": -0.1830622066151012, "compression_ratio": 1.1368421052631579, "no_speech_prob": 7.60069306124933e-05}, {"id": 943, "seek": 597442, "start": 5983.42, "end": 5985.42, "text": " Try again.", "tokens": [6526, 797, 13], "temperature": 0.0, "avg_logprob": -0.1830622066151012, "compression_ratio": 1.1368421052631579, "no_speech_prob": 7.60069306124933e-05}, {"id": 944, "seek": 597442, "start": 5985.42, "end": 5990.42, "text": " Just went off again.", "tokens": [1449, 1437, 766, 797, 13], "temperature": 0.0, "avg_logprob": -0.1830622066151012, "compression_ratio": 1.1368421052631579, "no_speech_prob": 7.60069306124933e-05}, {"id": 945, "seek": 597442, "start": 5990.42, "end": 5994.42, "text": " Batteries might be dead.", "tokens": [33066, 530, 1062, 312, 3116, 13], "temperature": 0.0, "avg_logprob": -0.1830622066151012, "compression_ratio": 1.1368421052631579, "no_speech_prob": 7.60069306124933e-05}, {"id": 946, "seek": 597442, "start": 5994.42, "end": 5995.42, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.1830622066151012, "compression_ratio": 1.1368421052631579, "no_speech_prob": 7.60069306124933e-05}, {"id": 947, "seek": 599542, "start": 5995.42, "end": 6004.42, "text": " Is the red light on?", "tokens": [1119, 264, 2182, 1442, 322, 30], "temperature": 0.0, "avg_logprob": -0.31259323301769437, "compression_ratio": 0.9558823529411765, "no_speech_prob": 2.282528657815419e-05}, {"id": 948, "seek": 599542, "start": 6004.42, "end": 6007.42, "text": " Go ahead and just ask your question perhaps.", "tokens": [1037, 2286, 293, 445, 1029, 428, 1168, 4317, 13], "temperature": 0.0, "avg_logprob": -0.31259323301769437, "compression_ratio": 0.9558823529411765, "no_speech_prob": 2.282528657815419e-05}, {"id": 949, "seek": 600742, "start": 6007.42, "end": 6026.42, "text": " I was going to say CT scans are used for example for lung cancer diagnostics which I spent a couple of years doing and better CT scans that you can diagnose earlier.", "tokens": [286, 390, 516, 281, 584, 19529, 35116, 366, 1143, 337, 1365, 337, 16730, 5592, 43215, 1167, 597, 286, 4418, 257, 1916, 295, 924, 884, 293, 1101, 19529, 35116, 300, 291, 393, 36238, 3071, 13], "temperature": 0.0, "avg_logprob": -0.3921268383661906, "compression_ratio": 1.362962962962963, "no_speech_prob": 4.682408325606957e-05}, {"id": 950, "seek": 600742, "start": 6026.42, "end": 6027.42, "text": " Wow.", "tokens": [3153, 13], "temperature": 0.0, "avg_logprob": -0.3921268383661906, "compression_ratio": 1.362962962962963, "no_speech_prob": 4.682408325606957e-05}, {"id": 951, "seek": 600742, "start": 6027.42, "end": 6028.42, "text": " Really, yeah.", "tokens": [4083, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.3921268383661906, "compression_ratio": 1.362962962962963, "no_speech_prob": 4.682408325606957e-05}, {"id": 952, "seek": 602842, "start": 6028.42, "end": 6043.42, "text": " Yeah, so just to emphasize Jeremy was just saying that earlier diagnosis can increase survival rates tenfold.", "tokens": [865, 11, 370, 445, 281, 16078, 17809, 390, 445, 1566, 300, 3071, 15217, 393, 3488, 12559, 6846, 2064, 18353, 13], "temperature": 0.0, "avg_logprob": -0.18530835174932714, "compression_ratio": 1.2868217054263567, "no_speech_prob": 3.966045369452331e-06}, {"id": 953, "seek": 602842, "start": 6043.42, "end": 6048.42, "text": " So doing CT scans well is really valuable.", "tokens": [407, 884, 19529, 35116, 731, 307, 534, 8263, 13], "temperature": 0.0, "avg_logprob": -0.18530835174932714, "compression_ratio": 1.2868217054263567, "no_speech_prob": 3.966045369452331e-06}, {"id": 954, "seek": 602842, "start": 6048.42, "end": 6051.42, "text": " Saving lives.", "tokens": [318, 6152, 2909, 13], "temperature": 0.0, "avg_logprob": -0.18530835174932714, "compression_ratio": 1.2868217054263567, "no_speech_prob": 3.966045369452331e-06}, {"id": 955, "seek": 605142, "start": 6051.42, "end": 6076.42, "text": " Let me go back to the notebook. So kind of what's going on with a CT scan is you've got this source of x-rays going through, you know, semi-dense object kind of being the person and then we've got a detector picking up kind of the strength of the signal to get information about the density that it's passed through.", "tokens": [961, 385, 352, 646, 281, 264, 21060, 13, 407, 733, 295, 437, 311, 516, 322, 365, 257, 19529, 11049, 307, 291, 600, 658, 341, 4009, 295, 2031, 12, 36212, 516, 807, 11, 291, 458, 11, 12909, 12, 67, 1288, 2657, 733, 295, 885, 264, 954, 293, 550, 321, 600, 658, 257, 25712, 8867, 493, 733, 295, 264, 3800, 295, 264, 6358, 281, 483, 1589, 466, 264, 10305, 300, 309, 311, 4678, 807, 13], "temperature": 0.0, "avg_logprob": -0.10179089880608894, "compression_ratio": 1.5879396984924623, "no_speech_prob": 5.5940499805728905e-06}, {"id": 956, "seek": 607642, "start": 6076.42, "end": 6086.42, "text": " And I should say all this is coming from a scikit-learn example but I've kind of added more explanation and visualizations of it.", "tokens": [400, 286, 820, 584, 439, 341, 307, 1348, 490, 257, 2180, 22681, 12, 306, 1083, 1365, 457, 286, 600, 733, 295, 3869, 544, 10835, 293, 5056, 14455, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.1057060536216287, "compression_ratio": 1.18348623853211, "no_speech_prob": 1.0615309292916209e-05}, {"id": 957, "seek": 608642, "start": 6086.42, "end": 6109.42, "text": " So in our last lesson we used robust PCA and we saw that that was an optimization problem where we're trying to minimize the nuclear norm of L in order to get a low rank matrix and the L1 norm of S. And what's special about the L1 norm?", "tokens": [407, 294, 527, 1036, 6898, 321, 1143, 13956, 6465, 32, 293, 321, 1866, 300, 300, 390, 364, 19618, 1154, 689, 321, 434, 1382, 281, 17522, 264, 8179, 2026, 295, 441, 294, 1668, 281, 483, 257, 2295, 6181, 8141, 293, 264, 441, 16, 2026, 295, 318, 13, 400, 437, 311, 2121, 466, 264, 441, 16, 2026, 30], "temperature": 0.0, "avg_logprob": -0.09228095030173278, "compression_ratio": 1.4438775510204083, "no_speech_prob": 1.3419327842711937e-05}, {"id": 958, "seek": 608642, "start": 6109.42, "end": 6111.42, "text": " What does it give us?", "tokens": [708, 775, 309, 976, 505, 30], "temperature": 0.0, "avg_logprob": -0.09228095030173278, "compression_ratio": 1.4438775510204083, "no_speech_prob": 1.3419327842711937e-05}, {"id": 959, "seek": 608642, "start": 6111.42, "end": 6115.42, "text": " Sparsity. Yeah, exactly.", "tokens": [1738, 685, 507, 13, 865, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.09228095030173278, "compression_ratio": 1.4438775510204083, "no_speech_prob": 1.3419327842711937e-05}, {"id": 960, "seek": 611542, "start": 6115.42, "end": 6122.42, "text": " And so that'll be useful today. And let me show you.", "tokens": [400, 370, 300, 603, 312, 4420, 965, 13, 400, 718, 385, 855, 291, 13], "temperature": 0.0, "avg_logprob": -0.22011039875171803, "compression_ratio": 1.0985915492957747, "no_speech_prob": 3.0716240871697664e-05}, {"id": 961, "seek": 611542, "start": 6122.42, "end": 6131.42, "text": " Let me show you what the.", "tokens": [961, 385, 855, 291, 437, 264, 13], "temperature": 0.0, "avg_logprob": -0.22011039875171803, "compression_ratio": 1.0985915492957747, "no_speech_prob": 3.0716240871697664e-05}, {"id": 962, "seek": 613142, "start": 6131.42, "end": 6153.42, "text": " Okay, so these are what our pictures are going to look like. And actually I'm going to show you where we're going and then I'll come back and kind of show you more about how to get there.", "tokens": [1033, 11, 370, 613, 366, 437, 527, 5242, 366, 516, 281, 574, 411, 13, 400, 767, 286, 478, 516, 281, 855, 291, 689, 321, 434, 516, 293, 550, 286, 603, 808, 646, 293, 733, 295, 855, 291, 544, 466, 577, 281, 483, 456, 13], "temperature": 0.0, "avg_logprob": -0.06024068593978882, "compression_ratio": 1.4384615384615385, "no_speech_prob": 2.3921917090774514e-05}, {"id": 963, "seek": 615342, "start": 6153.42, "end": 6166.42, "text": " Basically where we're going is that we'll have this data where, you know, this is kind of an artificial data set that we're going to create.", "tokens": [8537, 689, 321, 434, 516, 307, 300, 321, 603, 362, 341, 1412, 689, 11, 291, 458, 11, 341, 307, 733, 295, 364, 11677, 1412, 992, 300, 321, 434, 516, 281, 1884, 13], "temperature": 0.0, "avg_logprob": -0.09150042591324772, "compression_ratio": 1.7887323943661972, "no_speech_prob": 3.535381256369874e-05}, {"id": 964, "seek": 615342, "start": 6166.42, "end": 6182.42, "text": " And the idea is that we're going to pass lines through it at different angles from all different directions and we're just going to get a single number from this line going through this complicated picture basically about what it's hitting.", "tokens": [400, 264, 1558, 307, 300, 321, 434, 516, 281, 1320, 3876, 807, 309, 412, 819, 14708, 490, 439, 819, 11095, 293, 321, 434, 445, 516, 281, 483, 257, 2167, 1230, 490, 341, 1622, 516, 807, 341, 6179, 3036, 1936, 466, 437, 309, 311, 8850, 13], "temperature": 0.0, "avg_logprob": -0.09150042591324772, "compression_ratio": 1.7887323943661972, "no_speech_prob": 3.535381256369874e-05}, {"id": 965, "seek": 618242, "start": 6182.42, "end": 6189.42, "text": " So here you can see it. This kind of intersects these circle globs at several points.", "tokens": [407, 510, 291, 393, 536, 309, 13, 639, 733, 295, 27815, 82, 613, 6329, 3114, 929, 412, 2940, 2793, 13], "temperature": 0.0, "avg_logprob": -0.0893560606857826, "compression_ratio": 1.481818181818182, "no_speech_prob": 2.9310007448657416e-05}, {"id": 966, "seek": 618242, "start": 6189.42, "end": 6196.42, "text": " And we'll get a slightly larger number, 6.4. And I'll talk about how this is boiled down in a moment.", "tokens": [400, 321, 603, 483, 257, 4748, 4833, 1230, 11, 1386, 13, 19, 13, 400, 286, 603, 751, 466, 577, 341, 307, 21058, 760, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.0893560606857826, "compression_ratio": 1.481818181818182, "no_speech_prob": 2.9310007448657416e-05}, {"id": 967, "seek": 618242, "start": 6196.42, "end": 6205.42, "text": " But thinking about a CT scan, you know, it's kind of sending an X-ray along this line and then just measuring something on the other side.", "tokens": [583, 1953, 466, 257, 19529, 11049, 11, 291, 458, 11, 309, 311, 733, 295, 7750, 364, 1783, 12, 3458, 2051, 341, 1622, 293, 550, 445, 13389, 746, 322, 264, 661, 1252, 13], "temperature": 0.0, "avg_logprob": -0.0893560606857826, "compression_ratio": 1.481818181818182, "no_speech_prob": 2.9310007448657416e-05}, {"id": 968, "seek": 620542, "start": 6205.42, "end": 6218.42, "text": " And then here we've got a line going through, you know, different direction, different angle. This is not intersecting with much. Like it's just kind of catching the tail of this blob.", "tokens": [400, 550, 510, 321, 600, 658, 257, 1622, 516, 807, 11, 291, 458, 11, 819, 3513, 11, 819, 5802, 13, 639, 307, 406, 27815, 278, 365, 709, 13, 1743, 309, 311, 445, 733, 295, 16124, 264, 6838, 295, 341, 46115, 13], "temperature": 0.0, "avg_logprob": -0.1529922968224634, "compression_ratio": 1.4747474747474747, "no_speech_prob": 8.39778749650577e-06}, {"id": 969, "seek": 620542, "start": 6218.42, "end": 6225.42, "text": " And we get a much smaller number, 6, or sorry, 2. So before we had 6.4 or something here, we've just got 2.", "tokens": [400, 321, 483, 257, 709, 4356, 1230, 11, 1386, 11, 420, 2597, 11, 568, 13, 407, 949, 321, 632, 1386, 13, 19, 420, 746, 510, 11, 321, 600, 445, 658, 568, 13], "temperature": 0.0, "avg_logprob": -0.1529922968224634, "compression_ratio": 1.4747474747474747, "no_speech_prob": 8.39778749650577e-06}, {"id": 970, "seek": 622542, "start": 6225.42, "end": 6238.42, "text": " And so it's kind of amazing that you're just, you know, for each like X-ray shoot, you're just getting a single number. But kind of knowing that these are coming from, you know, different angles in different directions.", "tokens": [400, 370, 309, 311, 733, 295, 2243, 300, 291, 434, 445, 11, 291, 458, 11, 337, 1184, 411, 1783, 12, 3458, 3076, 11, 291, 434, 445, 1242, 257, 2167, 1230, 13, 583, 733, 295, 5276, 300, 613, 366, 1348, 490, 11, 291, 458, 11, 819, 14708, 294, 819, 11095, 13], "temperature": 0.0, "avg_logprob": -0.08471336880245724, "compression_ratio": 1.596938775510204, "no_speech_prob": 3.555836883606389e-06}, {"id": 971, "seek": 622542, "start": 6238.42, "end": 6248.42, "text": " The idea is that we want to work backwards to being able to reconstruct our original picture.", "tokens": [440, 1558, 307, 300, 321, 528, 281, 589, 12204, 281, 885, 1075, 281, 31499, 527, 3380, 3036, 13], "temperature": 0.0, "avg_logprob": -0.08471336880245724, "compression_ratio": 1.596938775510204, "no_speech_prob": 3.555836883606389e-06}, {"id": 972, "seek": 624842, "start": 6248.42, "end": 6257.42, "text": " Oh, no, that's a good. So Jeremy was just pointing out that this is an analogy in real life. They're doing 2D projections onto 3D.", "tokens": [876, 11, 572, 11, 300, 311, 257, 665, 13, 407, 17809, 390, 445, 12166, 484, 300, 341, 307, 364, 21663, 294, 957, 993, 13, 814, 434, 884, 568, 35, 32371, 3911, 805, 35, 13], "temperature": 0.0, "avg_logprob": -0.14489713815542368, "compression_ratio": 1.347305389221557, "no_speech_prob": 1.670049095991999e-05}, {"id": 973, "seek": 624842, "start": 6257.42, "end": 6267.42, "text": " Sorry. Well, person is 3D. And you're coming up with these 2D pictures here to keep it simple.", "tokens": [4919, 13, 1042, 11, 954, 307, 805, 35, 13, 400, 291, 434, 1348, 493, 365, 613, 568, 35, 5242, 510, 281, 1066, 309, 2199, 13], "temperature": 0.0, "avg_logprob": -0.14489713815542368, "compression_ratio": 1.347305389221557, "no_speech_prob": 1.670049095991999e-05}, {"id": 974, "seek": 626742, "start": 6267.42, "end": 6278.42, "text": " We kind of just have this 2D data set and we're just getting one dimensional data to reconstruct the two dimensional data set.", "tokens": [492, 733, 295, 445, 362, 341, 568, 35, 1412, 992, 293, 321, 434, 445, 1242, 472, 18795, 1412, 281, 31499, 264, 732, 18795, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.11423923658288043, "compression_ratio": 1.5, "no_speech_prob": 7.295800969586708e-06}, {"id": 975, "seek": 626742, "start": 6278.42, "end": 6286.42, "text": " Yeah. And then it's a it's 1255. So we'll stop now. But I just wanted to at least kind of introduce the problem that we'll be looking at next time.", "tokens": [865, 13, 400, 550, 309, 311, 257, 309, 311, 2272, 13622, 13, 407, 321, 603, 1590, 586, 13, 583, 286, 445, 1415, 281, 412, 1935, 733, 295, 5366, 264, 1154, 300, 321, 603, 312, 1237, 412, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.11423923658288043, "compression_ratio": 1.5, "no_speech_prob": 7.295800969586708e-06}, {"id": 976, "seek": 626742, "start": 6286.42, "end": 6290.42, "text": " Thanks, Matthew.", "tokens": [2561, 11, 12434, 13], "temperature": 0.0, "avg_logprob": -0.11423923658288043, "compression_ratio": 1.5, "no_speech_prob": 7.295800969586708e-06}, {"id": 977, "seek": 626742, "start": 6290.42, "end": 6295.42, "text": " Oh, that was a six. Let me go back up.", "tokens": [876, 11, 300, 390, 257, 2309, 13, 961, 385, 352, 646, 493, 13], "temperature": 0.0, "avg_logprob": -0.11423923658288043, "compression_ratio": 1.5, "no_speech_prob": 7.295800969586708e-06}, {"id": 978, "seek": 629542, "start": 6295.42, "end": 6302.42, "text": " So there is it's kind of just a measure of how much it's intersected with these other lines.", "tokens": [407, 456, 307, 309, 311, 733, 295, 445, 257, 3481, 295, 577, 709, 309, 311, 27815, 292, 365, 613, 661, 3876, 13], "temperature": 0.0, "avg_logprob": -0.15084136233610265, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.3005102118768264e-05}, {"id": 979, "seek": 629542, "start": 6302.42, "end": 6306.42, "text": " What?", "tokens": [708, 30], "temperature": 0.0, "avg_logprob": -0.15084136233610265, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.3005102118768264e-05}, {"id": 980, "seek": 629542, "start": 6306.42, "end": 6314.42, "text": " I mean, so I think in a person, it's more about kind of like the density or like the", "tokens": [286, 914, 11, 370, 286, 519, 294, 257, 954, 11, 309, 311, 544, 466, 733, 295, 411, 264, 10305, 420, 411, 264], "temperature": 0.0, "avg_logprob": -0.15084136233610265, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.3005102118768264e-05}, {"id": 981, "seek": 629542, "start": 6314.42, "end": 6319.42, "text": " kind of what the material it's traveling traveling through is.", "tokens": [733, 295, 437, 264, 2527, 309, 311, 9712, 9712, 807, 307, 13], "temperature": 0.0, "avg_logprob": -0.15084136233610265, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.3005102118768264e-05}, {"id": 982, "seek": 631942, "start": 6319.42, "end": 6326.42, "text": " In real life, it's not as direct as this.", "tokens": [682, 957, 993, 11, 309, 311, 406, 382, 2047, 382, 341, 13], "temperature": 0.0, "avg_logprob": -0.4869333047133226, "compression_ratio": 1.2586206896551724, "no_speech_prob": 7.479294436052442e-05}, {"id": 983, "seek": 631942, "start": 6326.42, "end": 6328.42, "text": " Transforms.", "tokens": [27938, 82, 13], "temperature": 0.0, "avg_logprob": -0.4869333047133226, "compression_ratio": 1.2586206896551724, "no_speech_prob": 7.479294436052442e-05}, {"id": 984, "seek": 631942, "start": 6328.42, "end": 6333.42, "text": " Real life cities can pass.", "tokens": [8467, 993, 6486, 393, 1320, 13], "temperature": 0.0, "avg_logprob": -0.4869333047133226, "compression_ratio": 1.2586206896551724, "no_speech_prob": 7.479294436052442e-05}, {"id": 985, "seek": 631942, "start": 6333.42, "end": 6336.42, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.4869333047133226, "compression_ratio": 1.2586206896551724, "no_speech_prob": 7.479294436052442e-05}, {"id": 986, "seek": 631942, "start": 6336.42, "end": 6346.42, "text": " Basically, yeah, I mean, in this case, it's just a product.", "tokens": [8537, 11, 1338, 11, 286, 914, 11, 294, 341, 1389, 11, 309, 311, 445, 257, 1674, 13], "temperature": 0.0, "avg_logprob": -0.4869333047133226, "compression_ratio": 1.2586206896551724, "no_speech_prob": 7.479294436052442e-05}, {"id": 987, "seek": 634642, "start": 6346.42, "end": 6349.42, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.36962244245741105, "compression_ratio": 0.6363636363636364, "no_speech_prob": 0.0003134283469989896}, {"id": 988, "seek": 634942, "start": 6349.42, "end": 6378.42, "text": " Welcome.", "tokens": [50364, 4027, 13, 51814], "temperature": 0.0, "avg_logprob": -0.40122456550598146, "compression_ratio": 0.5, "no_speech_prob": 0.00027604412753134966}], "language": "en"}