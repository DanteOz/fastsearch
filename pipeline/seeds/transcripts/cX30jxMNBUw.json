{"text": " Hi everybody and welcome to lesson 6, where we're going to continue looking at training convolutional neural networks for computer vision. And so we last looked at this the lesson before last and specifically we were looking at how to train an image classifier to pick out breeds of pet, one of 37 breeds of pet. And we've gotten as far as training a model, but we also had to look and figure out what loss function was actually being used in this model. And so we talked about cross entropy loss, which is actually a really important concept and some of the things we're talking about today depend a bit on you understanding this concept. So if you were at all unsure about where we got to with that, go back and have another look, have a look at the questionnaire in particular and make sure that you're comfortable with cross entropy loss. If you're not, you may want to go back to the 04 MNIST basics notebook and remind yourself about MNIST loss because it's very, very similar. That's what we built on to build up cross entropy loss. So having trained our model, the next thing we're going to do is look at model interpretation. There's not much point having a model if you don't see what it's doing. And one thing we can do is use a confusion matrix, which in this case is not terribly helpful. There's kind of a few too many. I mean, it's not too bad. We can kind of see some colored areas. And so this diagonal here are all the ones that are classified correctly. So for Persians, there were 31 classified as Persians, but we can see there's some bigger numbers here like a Siamese, six were misclassified. They're actually considered a Berman. But for when you've got a lot of classes like this, it might be better instead to use the most confused method. And that tells you the combinations, which it got wrong the most often. In other words, which numbers are the biggest? So actually here's the biggest one, 10. And that's confusing an American pit bull terrier or a Staffordshire bull terrier that's happened 10 times. And a ragdoll is getting confused with a Berman eight times. And so I'm not a dog or cat expert. And so I don't know what this stuff means. So I looked it up on the internet and I found that American pit bull terriers and Staffordshire bull terriers are almost identical, that I think they sometimes have a slightly different colored nose, if I remember correctly. And ragdolls and Bermans are types of cats that are so similar to each other that there's whole long threads on cat lover forums about is this a ragdoll or is this a Berman and experts disagreeing with each other. So no surprise that these things are getting confused. So when you see your model making sensible mistakes, the kind of mistakes that humans make, that's a pretty good sign that it's picking up the right kind of stuff and that the kinds of errors you're getting also might be pretty tricky to fix. But you know, let's see if we can make it better. And one way to try and make it better is to improve our learning rate. Why would we want to improve the learning rate? Well, one thing we'd like to do is to try to train it faster, get more done in less epochs. And so one way to do that would be to call our fine-tune method with a higher learning rate. So last time we used the default, which I think is, there you go, one e neg two. And so if we pump that up to 0.1, it's going to jump further each time. So remember the learning rate, if you've forgotten this, have a look again at notebook four. That's the thing we multiply the gradients by to decide how far to step. And unfortunately when we use this higher learning rate, the error rate goes from 0.083 epochs to 0.83. So we're getting the vast majority of them wrong now. So that's not a good sign. So why did that happen? Well, what happened is rather than this gradual move towards the minimum, we had this thing where we stepped too far and we get further, further away. So when you see this happening, which looks in practice like this, your error rate getting worse right from the start, that's a sign your learning rate is too high. So we need to find something just right, not too small that we take tiny jumps and it takes forever and not too big that we, you know, either get worse and worse or we just jump backwards and forwards quite slowly. So to find a good learning rate, we can use something that the researcher Leslie Smith came up with called the learning rate finder and the learning rate finder is pretty simple. All we do, remember when we do stochastic gradient descent, we look at one mini batch at a time, a few images in this case at a time, find the gradient for that set of images for the mini batch and jump your step-out weights based on the learning rate and the gradient. Well, what Leslie Smith said was, okay, let's do the very first mini batch at a really, really low learning rate, like 10 to the minus 7 and then let's increase by a little bit, so like maybe 25% higher and do another step and then 25% higher and do another step. So these are not epochs, these are just a single, a simple mini batch and then we can plot on this chart here. Okay, at 10 to the minus 7, what was the loss and at 25% higher than that, what was the loss and the 25% higher than that, what was the loss and so not surprisingly, if you do that at the low learning rates, the loss doesn't really come down because the learning rate is so small that these steps are tiny, tiny, tiny. And then gradually we get to the point where they're big enough to make a difference and the loss starts coming down because we've plotted here the learning rate against the loss, right. So here the loss is coming down as we continue to increase the learning rate, the loss comes down until we get to a point where our learning rates too high and so it flattens out and then, oh it's getting worse again. So here's the point above like 0.1 where we're in this territory. So what we really want is somewhere around here where it's kind of nice and steep. So you can actually ask it the learning rate finders, we used LR find to get this plot, we can we can get back from at the minimum and steep and so steep is where was it steepest, so the steepest point was 5 e-neg-3 and the minimum point divided by 10, that's quite a good rule of thumb, is 1 e-neg-2. So somewhere around this range might be pretty good. So each time you run it you'll get different values, a different time we ran it we thought that maybe 3 e-neg-3 would be good so we picked that and you'll notice the learning rate finder is a logarithmic scale, be careful of interpreting that. So we can now rerun the learning rate finder, setting the learning rate to a number we picked from the learning rate finder which in this case was 3 e-neg-3 and we can see now that's looking good, right? We've got an 8.3% error rate after three epochs. So this idea of the learning rate finder is very straightforward, I can describe it to you in a couple of sentences, it doesn't require any complex math and yet it was only invented in 2015 which is super interesting right? It just shows that there's so many interesting things for us to learn and discover. I think part of the reason perhaps for this it took a while is that you know engineers kind of love using lots and lots of computers, so before the learning rate finder came along people would like run lots of experiments on big clusters to find out which learning rate was the best rather than just doing a batch at a time. And I think partly also the idea of having a thing where a human is in the loop where we look at something and make a decision is also kind of unfashionable. A lot of folks in research and industry love things which are fully automated, but anyway it's great we now have this tool because it makes our life easier and fast AI certainly the first library to have this and I don't know if it's still the only one to have it built in at least to the basic the base library. So now we've got a good learning rate, how do we fine-tune the weights? So so far we've just been running this fine-tune method without thinking much about what it's actually doing. But we did mention in chapter one, lesson one briefly basically what's happening with the fine-tune, what is transfer learning doing? And before we look at that let's take a question. Is the learning rate plot in LR find plotted against one single mini-batch? No, it's not it's just it's actually just the standard kind of walking through the data loader, so just getting the usual mini-batches of the shuffled data and so it's kind of just normal training and the only thing that's been different is that we're increasing the learning rate a little bit after each mini-batch and and keeping track of it. Along with that is is the network reset to the initial status after each trial? No, certainly not we actually want to see how it learns we want to see it improving so we don't reset it to its initial state until we're done. So at the end of it we go back to the random weights we started with or whatever the weights were at the time we ran this. So what we're seeing here is is something that's actually the the actual learning that's happening as we at the same time increase the learning rate. Why would an ideal learning rate found with a single mini-batch at the start of training keep being a good learning rate even after several epochs and further loss reductions? Great question, it absolutely wouldn't. So let's look at that too shall we? And oh yeah can I ask one more? Of course, this is an important point so ask us. It is, this is very important. For the learning rate finder why use the steepest and not the minimum? We certainly don't want the minimum because the minimum is the point at which it's not learning anymore. Right so this flat section at the bottom here means in this mini-batch it didn't get better. So we want the steepest because that's the mini-batch where it got the most improved and that's what we want. We want the weights to be moving as fast as possible. As a rule of thumb though we do find that the minimum divided by 10 works pretty well that's Sylvain's favorite approach and he's generally pretty spot-on with that. So that's why we actually print out those two things. Lrmin is actually the minimum divided by 10 and steepest point is suggest the steepest point. Great good questions all. So remind ourselves what transfer learning does. So with transfer learning remember what our neural network is. It's a bunch of linear models basically with with activation functions between them and our activation functions are generally ReLU's rectified linear units. Any of this is fuzzy have a look at the 04 notebook again to remind yourself. And so each of those linear layers has a bunch of parameters to the whole neural network has a bunch of parameters and so after we train a neural network on something like ImageNet we have a whole bunch of parameters that aren't random anymore they're actually useful for something and we've also seen that the early layers seem to learn about fairly general ideas like gradients and edges and the later layers learn about more sophisticated ideas like what our eyes look like or what does fur look like or what does text look like. So with transfer learning we take a model so in other words a set of parameters which has already been trained something like ImageNet we throw away the very last layer because the very last layer is the bit that specifically says which one of those in the case of ImageNet 1000 categories is this an image in so we throw that away and we replace it with random weights sometimes with more than one layer of random weights and then we train that. Now yes. Oh I just wanted to make a comment and that's that I think the learning rate finder I think after you learn about it the idea almost seems kind of so simple or approximate that it's like wait this shouldn't work like or you know shouldn't you have to do something more more complicated or more precise that it's like I just want to highlight that this is a very surprising result that some kind of a such a simple approximate method would be so helpful. Yeah I would particularly say it's surprising to people who are not practitioners or who have not been practitioners for long. I've noticed that a lot of my students at USF have a tendency to kind of jump in to try to doing something very complex where they account for every possible imperfection from the start and it's very rare that that's necessary so one of the cool things about this is it's good example of trying the easiest thing first and seeing how well it works. And this was a very big innovation when it came out that I think it's kind of easy to take for granted now but this was super super helpful when it was kind of a new one. It was super helpful and it was also nearly entirely ignored. None of the research community cared about it and it wasn't until fast.ai I think in our first course talked about it that people started noticing and we had quite a few years in fact it's still a bit the case where super fancy researchers still don't know about the learning rate finder and you know get get beaten by you know first lesson fast.ai students on practical problems because they can pick learning rates better and they can do it without a cluster of thousands of computers. Okay so transfer learning. So we've got our pre-trained network and so it's really important every time you hear the word pre-trained network you're thinking a bunch of parameters which have particular numeric values and go with a particular architecture like ResNet34. We've thrown away the final layer and replaced them with random numbers and so now we want to train to fine-tune this set of parameters for a new set of images in this case pets. So fine-tune is the method we call to do that and to see what it does we can go burn dot fine-tune question mark and we can see the source code and here is the signature of the function and so the first thing that happens is we call freeze. So freeze is actually the method which makes it so only the last layers weights will get stepped by the optimizer. So the gradients are calculated just for those last layers of parameters and the step is done just for those last layer of parameters. So then we call fit and we fit for some number of epochs which by default is one. We don't change that very often and what that fit is doing is it's just fitting those randomly added weights which makes sense right they're the ones that are going to need the most work because at the time which we add them they're doing nothing at all they're just random. So that's why we spend one epoch trying to make them better. After you've done that you now have a model which is much better than we started with it's not random anymore. All the layers except the last are the same as the pre-trained network the last layer has been tuned for this new data set. So the closer you get to the right answer as you can kind of see in this picture the smaller the steps you want to create sorry the smaller the steps you want to take generally speaking. The next thing we do is we divide our learning rate by two and then we unfreeze so that means we make it so that all the parameters can now be stepped and all of them will have gradients calculated and then we fit for some more epochs and this is something we have to pass to the method. And so that's now got to train the whole network so if we want to we can kind of do this by hand right and actually CNN learner will by default freeze the model for us freeze the parameters for us so we actually don't have to call freeze. So if we just create our learner and then fit for a while this is three epochs of training just the last layer and so then we can just manually do it ourselves unfreeze and so now at this point as the question earlier suggested maybe this is not the right learning rate anymore so we can run LR find again and this time you don't see the same shape you don't see this rapid drop because it's much harder to train a model that's already pretty good. Instead you just see a very gentle little gradient so generally here what we do is we kind of try to find the bit where it starts to get worse again and go about which is about here and go about 10 let you know multiple of 10 less than that so about 1.8 neg 5 I would guess which yep that's what we picked. So then after unfreezing finding our new learning rate and then we can do a bunch more and so here we are we're getting down to 5.9 percent error which is okay but there's there's better we can do and the reason we can do better is that at this point here we're training the whole model at a 1 a neg 5 so 10 to the minus 5 learning rate which doesn't really make sense because we know that the last layer is still not that great it's only had three epochs of training from random so it probably needs more work. We know that the second last layer was probably pretty specialized to image net and less specialized to pet breeds so that probably needs a lot of work. Whereas the early layers the kind of gradients and edges probably don't need to be changed much at all. So what we'd really like is to have a small learning rate for the early layers and a bigger learning rate for the later layers and this is something that we developed at fast AI and we call it discriminative learning rates and Jason Yosinski actually is a guy who wrote a great paper that some of these ideas are based on which is he actually showed that different layers of the network really want to be trained at different rates although he didn't kind of go as far as trying that out and seeing how it goes it was more of a theoretical thing. So in fast AI if we want to do that we can pass to our learning rate rather than just passing a single number we can pass a slice. Now a slice is a special built-in feature of Python it's just an object which basically can have a few different numbers in it in this case it's repassing at two numbers and the way we read those basically what this means in fast AI is a learning rate is the very first layer will have this learning rate 10 to the minus 6 the very last layer will be 10 to the minus 4 and the layers between the two will be kind of equal multiples so they'll kind of be equally spaced learning rates from the start to the end. So here we can see basically doing our kind of own version of fine-tune we create the learner we fit with that automatically frozen version we unfreeze we fit some more and so when we do that you can see this works a lot better we're getting down to 5.3 5.1 5.4 error so that's pretty great. One thing we'll notice here is that we did kind of overshoot a bit it seemed like more like epoch number 8 was better so kind of back before you know well actually let me explain something about fit one cycle so fit one cycle is a bit different to just fit. So what fit one cycle does is it actually starts at a low learning rate it increases it gradually for the first one third or so of the batches until it gets to a high learning rate the highest this is why they're called LR max it's the highest learning rate we get to and then for the remaining two thirds or so of the batches it gradually decreases the learning rate and the reason for that is just that well actually it's kind of like empirically researchers have found that works the best in fact this was developed again by Leslie Smith the same guy that did the learning rate finder again it was a huge step you know it really dramatically accelerated the speed at which we can train neural networks and also made them much more accurate and again the academic community basically ignored it in fact the key publication that developed this idea was not even did not even pass peer review and so the reason I mentioned this now is to say that we can't we don't really just want to go back and pick the model that was trained back here because we could probably do better because we really want to pick a model that's got a low learning rate so what I would generally do here is I change this 12 to an 8 because this is this is looking good and then I would re train it from scratch I normally would find a better result you can plot the loss and you can see how the training and validation loss moved along and you can see here that you know the the error rate was starting to get worse here and what you'll often see is often the validation loss will get worse a bit before the error rate gets worse we're not really seeing it so much in this case but the error rate and the validation loss don't always or they're not always kind of in lockstep so what we're plotting here is the loss but you actually kind of want to look to see mainly what's happening with the error rate because that's actually the thing we care about remember the loss is just like an approximation of what we care about that just happens to have a gradient that works out nicely so how do you make it better now we're already down to just 5.4 or if we'd stopped a bit earlier maybe we could get down to 5.1 or less error on 37 categories that's pretty remarkable that's a very very good pet breed predictor if you want to do something even better you could try creating a deeper architecture so a deeper architecture is just literally putting more pairs of non-linear activation function also known as a non-linearity followed by these little linear models put more pairs onto the end and that basically the number of these sets of layers you have is the number that you'll see at the end of an architecture so there's ResNet 18, ResNet 34, ResNet 50, so forth. Having said that you can't really pick ResNet 19 or ResNet 38 I mean you can make one but nobody's created a pre-trained version of that for you so you won't be able to do any fine-tuning so like you can theoretically create any number of layers you like but in practice most of the time you'll want to pick a model that has a pre-trained version so you kind of have to select from the sizes people have pre-trained and there's nothing special about these sizes they're just ones that people happen to have picked out. For the bigger models there's more parameters and more gradients that are going to be stored on your GPU and you will get used to the idea of seeing this this error unfortunately out of memory so that's not out of memory in your RAM that's out of memory in your GPU. Udr is referring to the language the system used for your GPU. So if that happens unfortunately you actually have to restart your notebook so that's kernel restart and try again and that's a really annoying thing but that's just life. One thing you can do if you get an out of memory error is after you've your CNN learner call add this magic incantation to FP16. What that does is it uses for most of the operations numbers that use half as many bits as usual so they're less accurate this half precision floating point or FP16 and that will use less memory and on pretty much any Nvidia card created in 2020 or later and some more expensive cards even created in 2019 that's often going to result in a two to three times speed up in terms of how long it takes as well. So here if I add in to FP16 and I will be seeing often much faster training and in this case what I actually did is I switched to a ResNet 50 which would normally take about twice as long and my per epoch time has gone from 25 seconds to 26 seconds. So the fact that we used a much bigger network and it was no slower is thanks to 2FP16 but you'll see our error rate hasn't improved it's pretty similar to what it was and so it's important to realize that just because we increase the number of layers it doesn't always get better. So it tends to require a bit of experimentation to find what's going to work for you and of course don't forget the trick is use small models for as long as possible to do all of your cleaning up and testing and so forth and wait until you're all done to try some bigger models because they're going to take a lot longer. A question Jeremy, how do you know or suspect when you can quote do better? You have to always assume you can do better because you never know. So you just have to I mean part of it though is do you need to do better? So do you already have a good enough result to handle the actual task you're trying to do? Often people do spend too much time fiddling around with their models rather than actually trying to see whether it's already going to be super helpful. So as soon as you can actually try to use your model to do something practical the better. But yeah how much can you improve it? Who knows? I you know go through the techniques that we're teaching in this course and try them and see which ones help and unless it's a problem that somebody has already tried before and written down their results in a paper or a Kaggle competition or something there's no way to know how good can. So don't forget after you do the questionnaire to check out the further research section and one of the things we've asked you to do here is to read a paper. So find the learning rate find a paper and read it and see if you can kind of connect what you read up to the things that we've learned in this lesson and see if you can maybe even implement your own learning rate finder you know as manually as you need to and see if you can get something that you know based on reading the paper get to work yourself. You can even look at the source code of fastai learning rate finder of course and then can you make this classifier better? And so this is further research right so maybe you can start doing some reading to see what else could you do have a look on the forums see what people are trying have a look on the book website or the course website to see what other people have achieved and what they did and play around. So we've got some tools in our toolbox now for you to experiment with. So that is that is pet breeds that is a you know a pretty tricky computer vision classification problem and we kind of have seen most of the pieces of what goes into the training of it we haven't seen how to build the actual architecture but other than that we've kind of worked our way up to understanding what's going on. So let's build from there into another kind of data set one that involves multi-label classification. So what's multi-label classification? Well maybe so maybe let's look at an example here is a multi-label data set where you can see that it's not just one label on each image but sometimes is three bicycle car person. I don't actually see the car here at best it's being dropped out. So a multi-label data set is one where you still got one image per row but you can have 0, 1, 2 or more labels per row. So we're going to have a think about and look at how we handle that but first of all let's take another question. Does dropping floating point number precision switching from FP32 to FP16 have an impact on final? Yes it does. Often it makes it better believe it or not. It seems like you know the kind of it's doing a little bit of rounding off is one way to give it drop some of that precision and so that creates a bit more bumpiness a bit more uncertainty it was you know of a stochastic nature and you know when you introduce more slightly random stuff into training it very often makes it a bit better and so yeah FP16 training often gives us a slightly better result but I you know I wouldn't say it's generally a big deal either way and certainly it's not always better. Would you say this is a bit of a pattern in learning less less exact stochastic way? For sure not just in deep learning but machine learning more generally you know there's been some interesting research looking at like matrix factorization techniques which if you want them to go super fast you can lots of machines you can randomization and you often when you then use the results you often find you actually get better better outcomes. Just a brief plug for the fast AI computational linear algebra course which talks a little bit about about random. Does it really? Well that sounds like a fascinating course and look at that it's number one hit here on Google so easy to find brought by somebody called Rachel Thomas hey that's the same name as you Rachel Thomas. All right so how are we going to do multi-label classification so let's look at a data set called Pascal which is a pretty famous data set we'll look at the version that goes back to 2007 been around for a long time and it comes with a CSV file which we will read in CSV is comma separated values and let's take a look each row has a file name one or more labels and something telling you whether it's in the validation set or not. So the list of categories in each image is a space delimited string this doesn't have a horse person it has a horse and a person. PD here stands for pandas pandas is a really important library for any kind of data processing and you'll use it all the time in machine learning and deep learning so let's have a quick chat about it not a real panda it's a name of a library and it creates things called data frames that's what the DF here stands for and a data frame is a table containing rows and columns pandas can also do some slightly more sophisticated things than that but we'll treat it that way for now. So you can read in a data frame by saying PD for pandas pandas read CSV given a file name you've now got a data frame you can call head to see the first few rows of it for instance a data frame as a I lock integer location property which you can index into as if it was an array that looks just like numpy so colon means every row remember it's row comma column and zero means zero column and so here is the first column of the data frame you can do the exact opposite so the zeroth row and every column is going to give us the first row and you can see the row has column headers and values so it's a little bit different to numpy and remember if there's a comma colon or a bunch of comma colons at the end of an indexing in numpy or pytorch or pandas whatever you can get rid of it and these two are exactly the same you could do the same thing here by grabbing the column by name the first column is F name so a DFF name you get that first column you can create new columns so here's a tiny little data frame I've created from a dictionary and I could create a new column by for example adding two columns and you can see there it is so it's like a lot like numpy or pytorch except you have this idea of kind of rows and and column named columns and so it's all about kind of tabular data I find its API pretty unintuitive a lot of people do but it's fast and powerful so it takes a while to get familiar with it but it's worth taking a while and the creator of pandas wrote a fantastic book called Python for data analysis which I've read both versions and I found it fantastic it doesn't just cover pandas it covers other stuff as well like IPython and numpy and matplotlib so highly recommend this book this is our table so what we want to do now is construct data loaders that we can train with and we've talked about the data block API as being a great way to create data loaders so let's use this as an opportunity to create a data loaders or a process or create a data block and then data loaders for this and let's try to do it like right from square one so let's see exactly what's going on with a data block so first of all let's remind ourselves about what a data set and a data loader is a data set is an abstract idea of a class you can create a data set a data set is anything which you can index into it like so or and you can take the length of it like so so for example the list of the lowercase letters along with a number saying which lowercase letter it is I can index into it to get 0 comma a I can get the length of it to get 26 and so therefore this qualifies as a data set and in particular data sets normally you would expect that when you index into it you would get back a tuple because you've got the independent and dependent variables not necessarily always just two things there could be more there could be less but two is the most common so once we have a data set we can pass it to a data loader we can request we can request a particular batch size we can shuffle or not and so there's our data loader from a we could grab the first value from that iterator and here is the shuffled 7 is h 4 is a 20 is u and so forth and so I remember a mini batch has a bunch of a mini batch of the independent variable and a mini batch of the dependent variable if you want to see how the two correspond to each other you can use zip so if I zip passing in this list and then this list so be not and be one you can see what zip does in Python is that grabs one element from each of those in turn and gives you back the tuples of the corresponding elements since we're just passing in all of the elements of B to this function Python has a convenient shortcut for that which is just say star B and so star means insert into this parameter list each element of B just like we did here so these are the same thing so this is a very handy idiom that we use a lot in Python zip star something is kind of a way of like transposing something from one orientation to another. All right so we've got a data set we've got a data loader and then what about data sets what data sets is an object which has a training data set and a validation set data set so let's look at one but normally you don't start with kind of an enumeration like this like with with an independent variable and a dependent variable normally you start with like a file name for example and then you you kind of calculate or compute or transform your file name into an image by opening it and a label by for example looking at the file name and grabbing something out of it so for example we could do something similar here this is what data sets does so we could start with just the lowercase letters so this is still a data set right because we can index into it and we can get the length of it although it's not giving us tuples yet so if we now pass that list to the data sets class and index into it we get back the tuple and it's actually a tuple with just one item this is how Python shows a tuple with one item as it puts it in parentheses and a comma and then nothing okay so in practice what we'd really want to do is to say like okay we'll take this and do something to compute an independent variable and do something to compute the dependent variable so here's a function we could use to compute an independent variable which is to stick an a on the end and our dependent variable might just be the same thing with a B on the end so here's two functions so for example now we can call data sets passing in a and then we can pass in a list of transformations to do and so in this case I've just got one which is this function add an a on the end so now if I index into it I don't get a anymore I get a a if you pass multiple functions then it's going to do multiple things so here I've got f1 then f2 a a b that's this one then that's this one and you'll see this is a list of lists and the reason for that is that you can also pass something like this a list containing f1 a list containing f2 and this will actually take each element of a pass it through this list of functions and there's just one of them to give you a a and then start again and separately pass it through this list of functions there's just one to get a b and so this is actually kind of the main way we build up independent variables and dependent variables in fast AI is we start with something like a file name and we pass it through two lists of functions one of them will generally kind of open up the image for example and the other one will kind of pass the file name example and give you a independent variable and a dependent variable so you can then create a data loaders object from data sets by passing in the data sets and a batch size and so here you can see I've got shuffled o a i a etc o b i b etc so this is worth studying to make sure you understand what data sets and data loaders are we don't often have to create them from scratch we can create a data block to do it for us but now we can see what the data block has to do so let's see how it does it so we can start by creating an empty data block so an empty data block is going to take our data frame so we're going to go back to looking at data frame which remember was this guy and so if we pass in our data frame we can now we'll now find that this data block has created data sets a training and a validation data set for us and if we look at the training set it'll give us back an independent variable and a dependent variable and we'll see that they are both the same thing so this is the first row of the table that's actually shuffled so it's a random row of the table repeated twice and the reason for that is by default the data block assumes that we have two things the independent variable and the dependent or the input and the target and by default it just copies it just keeps exactly whatever you gave it to create the training set and the validation set by default it just randomly splits the data with a 20% validation set so that's what's happened here so this is not much use and what we actually want to do if we look at X for example is grab the F name the file name field because we want to open this image that's going to be our independent variable and then for the label we're going to want this here person cat so we can actually pass these as parameters get X and get Y functions that return the bit of data that we want and so you can create and use a function in the same line of code in Python by saying lambda so lambda R means create a function doesn't have a name it's going to take a parameter called R we don't even have to say return it's going to return the F name column in this case and get Y is something which is a function that takes an R and returns the labels column so now we can do the same thing called D block dot data sets we can grab a row from that from the training set and you can see look here it is there is the image file name and there is the space delimited list of labels so here's exactly the same thing again but done with functions okay so now the one line of code above has become three lines of code but it does exactly the same thing okay we don't get back the same result because the training set well wait why don't we get the same result oh I know why because it's randomly shuffle it's randomly picking a different validation set because the random split is done differently each time so that's why we don't get the same result. One thing to note be careful of lambdas if you want to save this data block for use later you won't be able to Python doesn't like saving things that contain lambdas so most of the time in the book and the course we normally use avoid lambdas for that reason because it's often very convenient to be able to save things we use the word here serialization that just means basically it means saving something. This is not enough to open an image because we don't have the path so to turn this into so rather than just using this function to grab the F name column we should actually use pathlib to go path train and then column and then for the y again the labels is not quite enough we actually have to split on space but this is Python we can use any function we like and so then we use the same three lines of code is here and now we've got a path and a list of labels so that's looking good. So we want this path to be opened as an image so the data block API lets you pass a blocks argument where you tell it for each of the things in your tuple so there's two of them what kind of block do you need so we need an image block to open an image and then in the past we've used a category block or categorical variables but this time we don't have a single category we've got multiple categories so we have to use a multi category block so once we do that and have a look we now have an 500 by 375 image as our independent variable and as a dependent variable we have a long list of zeros and ones. The long list of zeros and ones is the labels as a one hot encoded vector a rank one tensor and specifically there will be a zero in every location where in the vocab where there is not that kind of object in this image and a one in every location where there is so for this one there's just a person so this must be the location in the vocab where there's a person. Do you have any questions? So one hot encoding is a very important concept and we didn't have to use it before right we could just have a single integer saying which one thing is it but when we've got lots of things lots of potential labels it's it's convenient to use this one hot encoding and it's kind of what it's actually what's going to happen with them with the actual matrices anyway when we actually compare the activations of our neural network to the target it's actually going to be comparing each one of these. Okay so the categories as I mentioned is based on the vocab where we can grab the vocab from our data set subject and then we can say okay let's look at the first row and let's look at the dependent variable and let's look for where the dependent variable is one okay and then we can have a look past those indexes with the vocab and get back a list of what it actually was there and again each time I run this I'm going to get different results so each time we run this we're going to get different results because they called dot data sets again here so it's going to give me a different train test split and so this time it turns out that this is actually a chair and we have a question shouldn't the tensor be of integers why is it a tensor of floats yeah conceptually this is a tensor of integers they can only be 0 or 1 but we we're going to be using a cross entropy style loss function but we're going to actually need to do floating point calculations on them that's going to be faster to just store them as float in the first place rather than converting backwards and forwards even though they're conceptually an int we're not going to be doing kind of int style calculations with them good question I mentioned that by default the data block uses a random split you might have noticed in the data frame though it said here's a column saying what validation set to use and if the data set you're given tells you what validation set to use you should generally use it because that way you can compare your validation set results to somebody else's so you can pass a splitter argument which again is a function and so we're going to pass it a function that's also called splitter and the function is going to return the indexes where it's not valid and that's going to be the training set and the indexes where it is valid that's going to be the validation set and so the splitter argument is expected to return two lists of integers and so if we do that we get again the same thing but now we're using the correct train and validation sets another question sure any particular reason we don't use floating point 8 is it just that the precision is too low yeah trying to train with 8-bit precision is super difficult it's it's so flat and bumpy it's pretty difficult to get decent gradients there but you know it's an area of research the main thing people do with 8-bit or even 1-bit data types is they take a model that's already been trained with 16-bit or 32-bit floating point and then they kind of round it off it's called discretizing to create a kind of purely integer or even binary network which can do inference much faster figuring out how to train with such low precision data is an area of active research I suspect it's possible and I suspect I mean people have fiddled around it and had some success I think you know it could turn out to be super interesting particularly for stuff that's been done on like low-powered devices that might not even have a floating point unit right so the last thing we need to do is to add our item transforms random resource crop we've talked about that enough so I won't go into it but basically that means we now are going to ensure that everything has the same shape so that we can collate it into a data loader but now rather than going dot data sets or dot data loaders and display our data and remember if something goes wrong as we saw last week you can call summary to find out exactly what's happening in your data block so now you know this is something really worth studying this section because data blocks are super handy and if you haven't used fast AI before they won't be familiar to you because no other library uses them and so like this has really shown you how to go right back to the start and gradually build them up so hopefully that'll make a whole lot of sense now we're going to need a loss function again and to do that let's start by just creating a learner it's credit resident 18 from the data loaders object that we just created and let's grab one batch of data and then let's put that into our mini batch of independent independent variables and then learn dot model is the thing that actually contains the the the model itself in this case of CNN and you can treat it as a function and so therefore we can just pass something to it and so if we pass a mini batch of the independent variable to learn dot model it will return the activations from the final layer and that is shape 64 by 20 so anytime you get a tensor back look at its shape and in fact before you look at its shape predict what the shape should be and then make sure that you're right if you're not I think you guessed wrong so try to understand what way you made a mistake or there's a problem with your code in this case 64 by 20 makes sense because we have a mini batch size of 64 and for each of those we're going to make predictions about what probability is each of these 20 possible categories and we have a question two questions two questions all right is the data block API compatible with out-of-core data sets like Dask yeah the data block API can do anything you wanted to do so you're passing it if we go back to the start so you can create an empty one and then you can pass it anything that is indexable and yeah so that can be anything you you like and pretty much anything can be made indexable in Python and that's something like Dask is certainly indexable so that works perfectly fine if it's not indexable like it's a it's a network stream or something like that then the data loaders data sets API is directly which we'll learn about either in this course or the next one but yeah anything that you can index into it certainly includes Dask you can use with data blocks next question where do you put images for multi-label with that CSV table should they be in the same directory they can be anywhere you like so in this case we used a path lip object like so and in this case the the by default it's going to be using let's think about this so what's happening here is the path is oh it's saying dot okay the reason for that is that path dot base path is currently set to path and so that displays things relative so let's rid of that okay so the path we set is here right and so then when we said get X it's saying path slash chain slash whatever right so this is an absolute path and so here is the exact path so you can put them anywhere you like you just have to say what the path is and then if you want to not get confused by having this big long prefix that we can don't want to see all the time just set base path to the path you want everything to be relative to and then it'll just print things out in this more can more convenient manner right so this is really important that you can do this that you can create a learner you can grab a batch of data that you can pass it to the model is this is just plain PyTorch this line here right no fast AI you can see the shape right you can recognize why it has this shape and so now if you have a look here are the 20 activations now this is not a trained model it's a pre-trained model with a random set of final layer weights so these specific numbers don't mean anything right but it's just worth remembering this is what activations look like and most importantly they're not between 0 and 1 and if you remember from the MNIST notebook we know how to scale things between 0 and 1 we can pop them into the sigmoid function so the sigmoid function is something that scales everything to be between 0 and 1 so let's use that you'll also hopefully remember from the MNIST notebook that the MNIST loss the MNIST loss function first did sigmoid and then it did torch dot where so and then it did dot mean so we're going to use exactly the same thing as the MNIST loss function and we're just going to do one thing which is going to add dot log for the same reason that we talked about when we were looking at softmax we talked about why log is a good idea as a transformation we saw in the MNIST notebook we didn't need it but we're going to train faster and more accurately if we use it because it's just more it's going to be better behaved as we've seen so this particular function which is identical to MNIST loss plus dot log as a specific name and it's called binary cross entropy and we used it for the threes versus sevens problem to decide whether that column is it a three or not but because we can use broadcasting in high torch and element wise arithmetic this function when we pass it a whole matrix is going to be applied to every column so is the first column you know so it'll it'll basically do a torch dot where on on every column separately and every item separately so that's great it basically means that this binary cross entropy function is going to be just like MNIST loss but rather than just being is this the number three it'll be is this a dog is this a cat is this a car is this a person is this the bicycle and so forth so this is where it's so cool in pytorch we can kind of run write one thing and then kind of have it expand to handle higher dimensional tensors without doing any extra work we don't have to write this error ourselves of course because pytorch has one and it's called F dot binary cross entropy so we can just use pytorches as we've talked about there's always a equivalent module version so this is exactly the same thing as a module and end up BCE loss and these ones don't include the initial sigmoid actually if you want to include the initial sigmoid you need F dot binary cross entropy with logits or the equivalent and end up BCE with logits loss so BCE is binary cross entropy and so those are two functions last two equivalent classes or multi-label or binary problems and then the equivalent for single label like MNIST and PETS is NLL loss and cross entropy so that's the equivalent of binary cross entropy and binary cross entropy with logits so these are pretty awful names I think we can all agree but it is what it is so in our case we have a one hot encoded target and we want the one with the sigmoid in so the equivalent built-in is called BCE with logits loss so that we can make that our loss function we can compare the activations to our targets and we can get back a loss and then that's what we can use to train and then finally before we take our break we also need a metric now previously we've been using as a metric accuracy or actually error rate error rate is 1 minus accuracy accuracy only works or single label data sets like MNIST and PETS because what it does is it takes the input which is the final layer activations and it does arg max what arg max does is it says what is the index of the largest number in those activations so for example for MNIST you know maybe the largest the highest probability is 7 so this arg max would return 7 and then it says okay there's those are my predictions and then it says okay is the prediction equal to the target or not and then take the floating point mean so that's what accuracy is so arg max only makes sense when there's a single maximum thing you're looking for in this case we've got multi-label so instead we have to compare each activation to some threshold by default it's 0.5 and so we basically say if the sigmoid of the activation is greater than 0.5 let's assume that means that category is there and if it's not let's assume it means it's not there and so this is going to go give us a list of trues and falses for the ones that the based on the activations it thinks are there and we can compare that to the target and then again take the floating point mean so we can use the default threshold of 0.5 but we don't necessarily want to use 0.5 we might want to use a different threshold and remember we have to pass when we create our learner we have to pass to the metric the metrics argument a function so what if we want to use a threshold other than 0.5 well we'd like to create a special function which is accuracy multi with some different threshold and the way we do that is we use a special built-in in Python called partial let me show you how partial works here's a function called say hello say hello to somebody with something so say hello Jeremy or the default is hello so it says hello Jeremy say hello Jeremy comma ahoy ahoy Jeremy let's create a special version of this function that will be more suitable for Sylvain it's going to use French so we can say partial create a new function that's based on the say hello function but it's always going to set say what to Bonjour and we'll call that F but now F Jeremy is Bonjour Jeremy and F Sylvain is Bonjour Sylvain so you see we've created a new function from an existing function by fixing one of its parameters so we can do the same thing for accuracy multi say let's use a threshold of 0.2 and we can pass that to metrics and so let's create a CNN learner and you'll notice here we don't actually pass a loss function and that's because fast AI is not enough to realize hey you're doing a classification model with a a multi label dependent variable so I know what loss function you probably want so it does it for us or we can call fine-tune and here we have an accuracy of 94 and a half after the first few and eventually 95.1 that's pretty good we've got an accuracy of over 95 percent was point to a good threshold to pick who knows let's try point one well that's a worse accuracy so I guess in this case we could buy a higher threshold 94 hmm also not good so what's the best threshold well what we could do is call get preds to get all of the predictions and all of the targets and then we could calculate the accuracy at some threshold and then we could say okay let's grab lots of numbers between 0.05 and 0.95 and you with the list comprehension calculate the accuracy for all of those different thresholds and plot them ah looks like we want threshold somewhere a bit above 0.5 so cool we can just use that and it's going to give us 96 in a bit which is going to give us a better accuracy this is a you know something that a lot of theoreticians would be uncomfortable about I've used the validation set to pick a hyper parameter threshold right and so people might be like oh you're overfitting using the validation set to pick a hyper parameter but if you think about it this is a very smooth curve right it's not some bumpy thing where we've accidentally kind of randomly grabbed some unexpectedly good value when you're picking a single number from a smooth curve you know this is where the theory of like don't use a validation set for for hyper parameter tuning doesn't really apply so it's always good to be practical right don't treat these things as rules but as rules of thumb. Okay so let's take a break for five minutes and we'll see you back here in five minutes time. Hey welcome back so I want to show you something really cool image regression so we are not going to learn how to use a fast AI image regression application because we don't need one now that we know how to build stuff up with boss functions and the data block API ourselves we can invent our own applications so there is no image regression application per se but we can do image regression really easily what do we mean by image regression well remember back to lesson I think it's lesson one we talked about the two basic types of machine learning or supervised machine learning regression and classification classification is when our dependent variable is a discrete category or set of categories and regression is when our dependent variable is a continuous number like an age or XY coordinate or something like that so image regression means our independent variable is an image and our dependent variable is a continue one of all continuous value values and so here's what that can look like which is the b we head pose data set it has a number of things in it but one of the things we can do is find the midpoint of a person's face see so the b we had posed data set so the b we had posed data set comes from this paper random forest real-time 3d face analysis so thank you to those authors and we can grab it in the usual way and our data and we can have a look at what's in there and we can see there's 24 directories numbered from not from 1 to 24 is 1 2 3 and each one also has a dot-op file we're not going to be using the dot-op file I'm just the directories so let's look at one of the directories and as you can see there's a thousand things in the first directory so each one of these 24 directories is one different person that they photographed and you can see for each person there's frame 3 pose frame 3 RGB frame 4 pose frame 4 RGB and so forth so in each case we've got the image which is the RGB and we've got the pose is the pose dot text so as we've seen we can grab use get image files to get a list of all of the files image files recursively in a path and so once we have an image file name like this one sorry like this one we can turn it into a pose file name by removing the last one two three five six seven letters and adding back on pose dot text and so here is a function that does that and so you can hear see I can pass in an image file to image to pose and get back a pose file right so PIO image dot create is the faster I way to create an image at least a PIO image it has a shape in computer vision they're normally backwards they normally do columns by rows so that's why it's sway around where else pie torch numpy tensors and arrays are rows by columns so that's confusing but that's just how things are I'm afraid so here's an example of an image when you look at the readme from the data set website they tell you how to get the center point from from one of the text files and it's just this function so it doesn't matter it just it is what it is we call it get center and it will return the XY coordinate of the center of the person's head face so we can pass this as get why because get why remember is a thing that gives us back the label okay so so here's the thing right we can create a data block and we can pass in as the independent variables block image block as usual and then the dependent variables block we can say point block which is a tensor with two values in and now by combining these two things this says we want to do image regression with a dependent variable with two continuous values to get the items you call get image files to get the why we'll call the get center function to split it so this is important we should make sure that the validation set contains one or more people that don't appear in the training set so I'm just going to grab person number 13 just grabbed it randomly and I'll use all of those images as the validation set because I think they did this with a Xbox connect you know video thing so there's a lot of images that look almost identical so if you randomly assigned them then you would be massively overestimating how effective you are you want to make sure that you're actually doing a good job with a random with a new set of people not just a new set of frames that's why we use this and so funksplitter is a splitter that takes a function and in this case we're using lambda to create the function we will use data augmentation and we will also normalize so this is actually done automatically now but in this case we're doing it manually so this is going to subtract the mean and divide by the standard deviation of the original data set that the pre-trained model used which is image net so that's our data block and so we can call data loaders to get our data loaders passing in the path and show batch and we can see that looks good right here's our faces and the points and so let's like particularly for as a student don't just look at the pictures look at the actual data so grab a batch put it into an xb and a yb x patch and y batch and have a look at the shapes and make sure that makes sense the y is 64 by 1 by 2 so it's 64 in the mini batch 64 rows and then a the coordinates is a 1 by 2 tensor so there's a single point with two things in it it's like you could have like hands face and armpits or whatever or nose and ears and mouth so in this case we're just using one point and the point is represented by two values the x and the y and then why is this 64 by 3 by 240 by 320 well there's 240 rows by 320 columns that's the pixels that's the size of the images that we're using many batches 64 items and what's the three the three is the number of channels which in this case means the number of colors if we open up some random grizzly bear image and then we go through each of the elements of the first axis and through a show image you can see that it's got the red the green and the blue as the three channels so that's how we store a three channel image is it stored as a three by number of rows by number of columns rank three tensor and so a mini batch of those is a rank four tensor so that's why this is that shape so here's a row from the dependent variable okay there's that x y low location we talked about so we can now go ahead and create a learner passing in our data loaders as usual passing in a pre-trained architecture as usual and if you think back you may just remember in lesson one we learned about y range y range is where we tell fast AI what range of data we expect to see in the dependent variable so we want to use this generally when we're doing regression so the range of our coordinates is between minus one and one that's how fast AI and pytorch treats coordinates the left-hand side is minus one or the top is minus one and the bottom and the right one so there's no point predicting something that's smaller than minus one or bigger than one because that is not in the area that we use for our coordinates. I have a question. Sure just a moment. So how is y range work? Well it actually uses this function called sigmoid range which takes the sigmoid of x multiplies by high minus low and adds low and here is what sigmoid range looks like for minus one to one it's just a sigmoid where the bottom is the low and the top is the high and so that way all of our activations are going to be mapped to the range from minus one to one. Yes Rachel. Can you provide images with an arbitrary number of channels as inputs specifically more than three channels? Yeah you can have as many channels as you like. We've certainly seen images with less than three because we've been grayscale. More than three is common as well you could have like an infrared band or like satellite images often have multispectral as some kinds of medical images where there are bands that are kind of outside the visible range. Your pre-trained model will generally have three channels. The fast AI does some tricks to use three channel pre-trained models for non three channel data but that's the only tricky bit other than that it's just just a you know it's just an axis that happens to have four things or two things or one thing instead of three things there's nothing special about it. Okay we didn't specify a loss function here so we get whatever it gave us which is a MSE loss so MSE loss is mean squared error and that makes perfect sense right you would expect mean squared error to be a reasonable thing to use for regression we're just testing how close we are through the target and then taking the square taking them. We didn't specify any metrics and that's because mean squared error is already a good metric like it's not it's it's it's it has nice gradients it behaves well but and it's also the thing that we care about so we don't need a separate metric to track. So let's go ahead and use LR find and we can pick our learning rate so maybe about 10 to the minus 2 we can call fine tune and we get a valid loss of 0.0001 and so that's the mean squared error so we should take the square root on average we're about 0.01 off in a coordinate space that goes between minus 1 and 1 so that sounds super accurate took about three in a bit minutes to run so we can always call in fast AI and we always should their results see what our results look like and as you can see fast AI has automatically figured out how to display the combination of an image independent variable and a point dependent variable on the left is that is the target and on the right is the prediction and as you can see it is pretty close to perfect. You know one of the really interesting things here is we used fine tune even although think about it the thing we're fine-tuning image net isn't even an image regression model so we're actually fine-tuning an image classification model to become something totally different an image regression model why does that work so well well because an image net classification model must have learned a lot about kind of how images look what things look like and where the pieces of them are they kind of know how to figure out what breed of animal something is even if it's partly obscured by a bush or it's in the shade or it's turned in different angles you know these are pre-trained image models are incredibly powerful computers you know computing algorithms so built into every image net pre-trained model is all this capability that it had to learn for itself so asking it to use that capability to figure out where something is just actually not that hard for it and so that's why we can actually fine-tune an image net classification model to create something completely different which is a point image regression model so I find that incredibly cool I gotta say so again look at the further research after you've done the questionnaire and particularly if you haven't used data frames before please play with them because we're going to be using them more and more. Good question. I'll just do the last one and also go back and look at the bear classifier from notebook 2 or whatever hopefully you created some other classifier for your own data because remember we talked about how it would be better if the bear classifier could also recognize that there's no bear at all or maybe there's both a grizzly bear and a black bear or a grizzly bear and a teddy bear so if you retrain it using multi-label classification see what happens see how well it works when there's no bears and see whether it changes the accuracy of the single label model when you turn it into a multi-label problem so have a fiddle around and tell us on the forum what you find and we've got a question Rachel. Is there a tutorial showing how to use pre-trained models on four-channel images also how can you add a channel to a normal image? What's the last bit how do you add a channel to an image? I don't know what that means okay like I don't know you can't like an image is an image you can't add a channel to an image is what it is. I don't know if there's a tutorial but we can certainly make sure somebody on the forum has known how to do it it's um it's super straightforward it should be pretty much automatic. Okay we're going to talk about collaborative filtering. What is collaborative filtering? Well think about on Netflix or whatever you might have watched a lot of movies that are sci-fi and have a lot of action and were made in the 70s and Netflix might not know anything about the properties of movies you watched it might just know that they're movies with titles and IDs but what it could absolutely see without any manual work is find other people that watched the same movies that you watched and it could see what other movies those people watched that you haven't and it would probably find they were also you would probably find they're also science fiction and full of action and made in the 70s. So we can use an approach where we recommend things even if we don't know anything about what those things are as long as we know who else has used or recommended things that are similar you know the same kind you know many of the same things that that you've liked or or used. This doesn't necessarily mean users and products in fact in collaborative filtering sort of saying products we normally say items and items could be links you click on diagnoses for a patient and so forth. So there's a key idea here which is that in the underlying items and we're going to be using movies in this example there are some there are some features they may not be labeled but there's some underlying concept of features of of those movies like the fact that there's a action concept and a sci-fi concept in the 1970s concept. Now you would never actually told Netflix you like these kinds of movies and maybe Netflix never actually added columns to their movies saying what movies are those types but as long as like you know in the real world there's this concept of sci-fi and action and movie age and that those concepts are relevant for at least some people's movie watching decisions. As long as this is true then we can actually uncover these they're called late latent factors these things that kind of decide what kind of movies you want to watch and they're latent because nobody necessarily ever wrote them down or labeled them or communicated them in any way. So let me show you what this looks like. So there's a great data set we can use called Movie Lens which contains tens of millions of movie rankings and so a movie ranking looks like this it has a user number a movie number a rating and a time step. So we don't know anything about who user number 196 is I don't know if that is Rachel or John Vowell or somebody else. I don't know what movie number 242 is I don't know if that's Casablanca or Lord of the Rings or the Mask and then rating is a number between I think it was one and five. A question. Sure. In traditional machine learning we perform cross validations and k-fold training to check for variance and bias trade-off. Is this common in training deep learning models as well? So cross validation is a technique where you don't just split your data set into one training set and one validation set but you basically do it five or so times like five training sets and like five validation sets representing different overlapping subsets. And basically this was this used to be done a lot because people often used to have not enough data get a good result and so this way rather than kind of having 20% that you would leave out each time you could just leave out like 10% each time. Nowadays it's less common that we have so little data that we need to worry about the complexity and extra time of lots of models. It's done on Kaggle a lot. It's on Kaggle every little fraction of percent matters but it's not it's not a deep learning thing or a machine learning thing or whatever it's just a you know lots of data or not very much data thing and do you care about the last decimal place of them or not. It's not something we're going to talk about certainly in this part of the course if ever because it's not something that comes up in practice that often as being that important. There are two more questions. What would be some good applications of collaborative filtering outside of recommender systems? Well I mean depends how you define recommender system. If you're trying to figure out what kind of other diagnoses might be applicable to a patient I guess that's kind of a recommender system or you're trying to figure out where somebody is going to click next or whatever it's kind of a recommender system. But you know really conceptually it's anything where you're trying to learn from past behavior where that behavior is kind of like a thing happened to an entity. What is an approach to training using video streams i.e. from drone footage instead of images. Would you need to break up the footage into image frames? In practice quite often you would because images just tend to be pretty big. Sorry videos tend to be pretty big. There's a lot of so I mean theoretically the time could be the the fourth channel. Yeah or a fifth channel so if it's a full color movie you can absolutely have well I guess fourth because you can have it would be a five rank five tensor being batch by time by color by row by column. But often that's too computationally and too memory intensive. Sometimes people just look at one frame at a time. Sometimes people use a few frames around kind of the key frame like three or five frames at a time and sometimes people use something called a recurrent neural network which we'll be seeing in the next week or two treated as a sequence data. Yeah there's all kinds of tricks you can do to try and work with that conceptually though there's no reason you can't just add an additional access to your tensors and everything to work. It's just a practical issue around time and memory. And someone else noted that it's pretty fitting that you mentioned the movie The Mask. Yes it was not an accident. I guess I got masks on the brain. I'm not sure if we're allowed to like that movie anymore though. I kind of liked it when it came out. I don't know what I think nowadays. It's wild. Okay so let's take a look. So we can untar data ml100k. So ml100k is a small subset of the full set. There's another one that we can grab which is about the whole lot 25 million but 100k is good enough for messing around. So if you look at the readme you'll find the main table is in a file called u.data. So let's open it up with read.csv again. This one is actually not comma separated values it's tab separated. Rather confusingly we still use csv and to say delimiter is a tab. Backslash t is tab. There's no row at the top saying what the columns are called. So we say header is none and then pass in a list of what the columns are called..head will give us the first five rows and we mentioned this before what it looks like. It's not a particularly friendly way to look at it. So what I'm going to do is I'm going to cross tab it and so what I've done here is I've grabbed the top. I remember how many it was well I guess one two three four fifteen or twenty movies based on the most popular movies and the top bunch of users who watched the most movies and so I've basically kind of reoriented this. So for each user I have all the movies they've watched and the rating they gave them. So empty spots represent users that have not seen that movie. So this is just another way of looking at this same data. So basically what we want to do is guess what movies we should tell people they might want to watch and so it's basically filling in these gaps to tell user 212 do you think we would they might like movie 49 or 79 or 99 best to watch next. So let's assume that we actually had columns for every movie that represented say how much sci-fi they are how much action they are and how old they are and maybe they're between minus one and one and so like the lot the last Skywalker is very sci-fi barely action and definitely not old and then we could do the same thing for users so we could say user one really like sci-fi quite likes action and really doesn't like old and so now if you multiply those together and remember in PyTorch and NumPy you have element-wise calculations so this is going to multiply each corresponding item. It's not matrix multiplication if you're a mathematician don't go there this is element-wise multiplication if we want matrix multiplication it'd be an at sign. So if we multiply each element together next to with the equivalent element in the other one and then sum them up that's going to give us a number which will basically tell us how much do these two correspond because remember two negatives multiply together to get a positive so user one likes exactly the kind of stuff that last guy was that the last Skywalker has in it and so we get 2.1. Multiplying things together element-wise and adding them up is called the dot product and we use it a lot and it's the basis of matrix didn't say modification matrix multiplication. So make sure you know what a dot product is it's this so Casablanca is not at all sci-fi not much action and is certainly old so if we do user one times Casablanca we get a negative number so we might think okay user one more like won't like this movie. Problem is we don't know what the latent factors are and even if we did we don't know how to label a particular user or a particular movie with them so we have to learn them. How do we learn them? Well we can actually look at a spreadsheet so I've got a spreadsheet version so we have a spreadsheet version which is basically what I did was I popped this table into Excel and then I randomly created a let's count this now 1 2 3 4 5 6 7 8 9 10 11 12 13 14. I randomly created a 15 by 5 table here so these are just random numbers and I randomly created a 5 by 15 table here and I basically said okay well let's just pretend let's just assume that every movie and every user has five latent factors I don't know what they are and let's then do a matrix multiply of this set of factors by this set of factors and a matrix multiply of a row by a column is identical to a dot product of two vectors so that's why I could just use matrix multiply. So this is just what this first cell contains so they then copied it to the whole thing so all these numbers there are being calculated from the row latent factors dot product with or matrix model play with a column latent factors so in other words I'm doing exactly this calculation but I'm doing them with random numbers and so that gives us a whole bunch of values right and then what I could do is I could calculate a loss by comparing every one of these numbers here to every one of these numbers here and then I could do mean squared error and then I could use stochastic gradient descent to find the best set of numbers in each of these two locations and that is what collaborative filtering is. So that's actually all we need so rather than doing an Excel and very the Excel version later if you're interested because we can actually do this whole thing and it works in Excel let's jump and do it into pytorch. Now one thing that might just make this more fun is actually to know what the movies are and movie lens tells us in u.item what the movies are called and that uses the telemetry of the pipe sign weirdly enough so here are them here are the names of each movie and so one of the nice things about pandas is it can do joins just like SQL and so you can use the merge method to combine the ratings table and the movies table and since they both have a column called movie by default it will join on those and so now here we have the ratings table with actual movie names that's going to be a bit more fun we don't need it for modeling but it's just going to be better for looking at stuff. So we could use data blocks API at this point or we could just use the built-in application factory method since it's there we may as well use it so we can create a collaborative filtering data loaders object from a data frame by passing in the ratings table by default the user column is called user and ours is so fine by default the item column is called item and ours is not it's called title so let's pick title and choose a batch size and so if we now say show batch here is some of that data and the rating is called rating by default so that worked fine too. So here's some data. So we need to now create our let's assume we're going to use that five numbers of factors so the number of users is however many classes there are for user and the number of movies is however many classes there are the title and so these are so we don't just have a vocab now right we've actually got a list of classes for each categorical variable for each set of discrete choices so we've got a whole bunch of users at 944 and a whole bunch of titles 1635 so for our randomized latent factor parameters we're going to need to create those matrices that we can just create them with random numbers so this is normally distributed random numbers that's what random is and that will be n users okay so 944 by n factors which is 5 that's exactly the same as this except this is just 15 so let's do exactly the same thing for movies random numbers and movies by 5 okay and so to calculate the result for some movie and some user we have to look up the index of the movie in our movie latent factors the index of the user in our user latent factors and then do a cross product so in other words we would say like oh okay for this particular combination we would have to look up that numbered user over here and that numbered movie over here to get the two appropriate sets of latent factors but this is a problem because look up in an index is not a linear model like remember our deep learning models really only know how to just multiply matrices together and do simple element-wise nonlinearities like ReLU there isn't a thing called look up at an index okay I'll just finish this bit um here's a cool thing though the look up in an index is actually can be represented as a matrix product believe it or not so if you replace our indices with one hot encoded vectors then a one hot encoded vector times something is identical to looking up in an index let me show you so if we grab if we call the one hot function that creates a as it says here one hot encoding and we're going to one hot encode the value three with n users classes and so n users as we've discussed is 944 right then so if we go one hot one hot encoding the number three into n users one hot three we get this big array big tensor and as you can see in index three zero one two three we have a one and the size of that is 944 so if we then multiply that by user factors or user factors remember is that random matrix of this size so what's going to happen so we're going to go zero by the first row and so that's going to be all zeros and then we're going to go zero again they're going to zero again and then we're going to find it finally go one right on the index three row and so it's going to return each of them and then we'll go back to zero again so if we do that remember at sign is matrix multiply and compare that to user factors three same thing isn't that crazy so it's a kind of weird inefficient way to do it right but matrix multiplication is a way to index into an array and this is the thing that we know how to do SGD with and we know how to build models with so it turns out that anything that we can do with indexing to array we now have a way to optimize and we have a question are there two questions one how different in practice is collaborative filtering with sparse data compared to dense data we are not doing sparse data in this course but there's an excellent course I hear called computational linear algebra for coders it has a lot of information about sparse the fast AI course and second question in practice do we tune the number of latent factors absolutely we do yes it's just a it's just a number of filters like we have in much any kind of deep learning model all right so now that we know that the procedure of finding out which latent set of latent factors is the right thing looking something up at an index is the same as matrix multiplication with a one-hot vector or we had it over here we can go ahead and build a model with that so basically if we do this for a whole for a few more indices at once then we have a matrix of one hot encoded vectors so the whole thing is just one big matrix multiplication now the thing is as I said this is a pretty inefficient way to do an index lookup so there is a computational shortcut which is called an embedding an embedding is a layer that has the computational speed of an array lookup and the same gradients as a matrix multiplication how does it do that well just internally it uses an index lookup to actually grab the values and it also knows what the gradient of a matrix multiplication by a one-hot encoded vector is or matrix is without having to go to all this trouble and so an embedding is a matrix multiplication with a one-hot encoded vector where you never actually have to create the one-hot encoded vector you just need the indexes this is important to remember because a lot of people have heard about embeddings and they think there's something special and magical and and they're absolutely not you can do exactly the same thing by creating a one-hot encoded matrix and doing a matrix multiply it is just a computational shortcut nothing else I often find when I talk to people about this in person I have to tell them this six or seven times before they believe me because they think embeddings are something more clever and they're not it's just a computational shortcut to do a matrix multiplication more quickly with a one-hot encoded matrix by instead doing an array lookup okay so let's try and create a collaborative filtering model in pytorch a model or an architecture or really an nn.module is a class so to use pytorch through its fullest you need to understand object-oriented programming because we have to create classes there's a lot of tutorials about this so I won't go into detail about it but I'll give you a quick overview a class could be something like dog or resnet or circle and it's something that has some data attached to it and it has some functionality attached to it is a class called example the data it has attached to it is a and the functionality attached to it is say and so we can for example create an instance of this class an object of this type example we pass in Sylvain so Sylvain will now be in ex.a and we can then say ex.say and it will call say and it will say passing in nice to meet you so that will be ex and so it'll say hello self.a so that's Sylvain nice to meet you here it is okay so in Python the way you create a class is to say class and its name then to say what is passed to it when you create that object it's a special method called dunder in it as we've briefly mentioned before in Python there are all kinds of special method names that have special behavior they start with two underscores they end with two underscores and we pronounce that dunder so dunder in it all methods in all regular methods instance methods in Python always get past the actual object itself first that we normally call that self and then optionally anything else and so you can then change the contents of the current object by just setting self dot whatever to whatever you like so after this self.a is now equal to Sylvain so we call a method same thing that's past self optionally anything you pass to it and then you can access the contents of self which you stashed away back here when we initialized it so that's basically how object or you know the basics of object oriented programming works in Python in Python there's something else you can do when you create a new class which is you can pop something in parentheses after its name and that means we're going to use something called inheritance and what inheritance means is I want to have all the functionality of this class plus I want to add some additional functionality so a module is a pytorch class which fastai has customized so it's kind of a fastai version of a pytorch class and probably in the next course we'll see exactly how it works and but it looks a lot like a it acts almost exactly like a just a regular Python class we have an init and we can set attributes to whatever we like and one of the things we can use is an embedding and so an embedding is just this class that does what I just described a it's the same as an as a linear layer with one hot encoded matrix but it does it with this computational shortcut so you can say how many in this case users are there and how many factors will they have now there is one very special thing about things that inherit from module which is that when you call them it will actually call a method called forward so forward is a special pytorch method name it's the most important pytorch method name this is where you put the actual computation so to grab the factors from an embedding we just call it like a function right so this is going to get passed here the user IDs and the movie IDs as two columns so let's grab the zero index column and grab the embeddings by passing them to user factors and then we'll do the same thing for the index one column that's the movie IDs pass them to the movie factors and then here is our element-wise multiplication and then sum and now remember we've got another dimension time the first axis is the mini batch dimension so we want to sum over the other dimension the index one dimension so that's going to give us a dot product for each user sorry for each rating for each user movie combination so this is the dot product class so you can see if we look at one batch of our data it's of size of size shape 64 by 2 because there are 64 items in the mini batch and each one has this is the independent variables so it's got the user ID and the movie ID and to deep neural network based models for collaborative filtering work better than more traditional approaches like SVD or other matrix let's wait until we get there so is X right so here is one user ID movie ID combination okay and then for each one of those 64 here are the ratings so now we've created a dot product module from scratch so we can instantiate it passing in the number of users the number of movies and let's use 50 factors and now we can create a learner now this time we're not creating a CNN learner or a specific application learner it's just a totally generic learner so this is a learner that doesn't really know how to do anything clever it just stores away the data you give it and the model you give it and so when we're not using an application specific learner it doesn't know what loss function to use so we'll tell it to use MSE and fit and that's it right so we've just fitted our own collaborative filtering model where we literally created the entire architecture it's a pretty simple one from scratch so that's pretty amazing now the results aren't great if you look at the movie lens data set benchmarks online you'll see this is not actually a great result so one of the things we should do is take advantage of the tip we just mentioned earlier in this lesson which is when you're doing regression which we are here right the number between one and five is like a continuous value we're trying to get as close to it as possible we should tell fast AI what the range is so we can use y range as before so here's exactly the same thing we've got a y range we've stored it away and then at the end we use as we discussed sigmoid range passing in and look here we pass in star self dot y range that's going to pass in by default zero comma five point five and so we can see not really any better it's worth a try normally this is a little bit better but it always depends on when you run it I just run it a second time while it's we're looking um now there is something else we can do though which is that if we look back at our little excel version the thing is here when we multiply you know these latent factors by these latent factors and add them up it's not really taking account of the fact that this user may just rate movies really badly in general regardless of what kind of movie they are and this movie might be just a great movie in general just everybody likes it regardless of what kind of stuff they like and so it'd be nice to be able to represent this directly and we can do that using something we've already learned about which is bias we could have another single number for each movie which we just add and it's another single number for each user which we just add right and we've already seen this for linear models you know this idea that it's nice to be able to add a bias value so let's do that so that means that we're going to need another embedding for each user which is a size one it's just a single number we're going to add so in other words it's just an array lookup but remember to do an array lookup that we can kind of take a gradient of we have to say embedding so we do the same thing for movie bias and so then all of this is identical as before and we just add this one extra line which is to add the user and movie bias values and so let's train that see how it goes well that was a shame it got worse so we used to have that finished here 0.87 0.88 0.89 so it's a little bit worse why is that well if you look earlier on it was quite better it was 0.86 so it's it's overfitting very quickly and so what we need to do is we need to find a way that we can train more epochs without overfitting now we've already learned about data augmentation right like rotating images and changing their brightness and color and stuff but it's not obvious how we would do data augmentation for collaborative filtering right so how are we going to make it so that we can train lots of epochs without overfitting and to do that we're going to have to use something called regularization and regularization is a set of techniques which basically allow us to use models with lots of parameters and train them for a long period of time but penalize them effectively for overfitting or in some way cause them to try to stop overfitting and so that is what we will look at next week okay well thanks everybody so there's a lot to take in there so please remember to practice to experiment to listen to the lessons again because you know for the next couple of lessons things are going to really quickly build on top of all the stuff that we've learned so please be as comfortable as it with it as you can feel free to go back and re-listen and go through and follow through the notebooks and then try to recreate as much of them yourself thanks everybody and I will see you next week or see you in the next lesson whenever you watch it.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.44, "text": " Hi everybody and welcome to lesson 6, where we're going to continue looking at training", "tokens": [2421, 2201, 293, 2928, 281, 6898, 1386, 11, 689, 321, 434, 516, 281, 2354, 1237, 412, 3097], "temperature": 0.0, "avg_logprob": -0.14677379749439382, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.005553459748625755}, {"id": 1, "seek": 0, "start": 8.44, "end": 13.68, "text": " convolutional neural networks for computer vision. And so we last looked at this the", "tokens": [45216, 304, 18161, 9590, 337, 3820, 5201, 13, 400, 370, 321, 1036, 2956, 412, 341, 264], "temperature": 0.0, "avg_logprob": -0.14677379749439382, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.005553459748625755}, {"id": 2, "seek": 0, "start": 13.68, "end": 19.36, "text": " lesson before last and specifically we were looking at how to train an image classifier", "tokens": [6898, 949, 1036, 293, 4682, 321, 645, 1237, 412, 577, 281, 3847, 364, 3256, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.14677379749439382, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.005553459748625755}, {"id": 3, "seek": 0, "start": 19.36, "end": 26.68, "text": " to pick out breeds of pet, one of 37 breeds of pet. And we've gotten as far as training", "tokens": [281, 1888, 484, 41609, 295, 3817, 11, 472, 295, 13435, 41609, 295, 3817, 13, 400, 321, 600, 5768, 382, 1400, 382, 3097], "temperature": 0.0, "avg_logprob": -0.14677379749439382, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.005553459748625755}, {"id": 4, "seek": 2668, "start": 26.68, "end": 32.5, "text": " a model, but we also had to look and figure out what loss function was actually being", "tokens": [257, 2316, 11, 457, 321, 611, 632, 281, 574, 293, 2573, 484, 437, 4470, 2445, 390, 767, 885], "temperature": 0.0, "avg_logprob": -0.10794818735568323, "compression_ratio": 1.7269372693726937, "no_speech_prob": 3.120039400528185e-05}, {"id": 5, "seek": 2668, "start": 32.5, "end": 37.68, "text": " used in this model. And so we talked about cross entropy loss, which is actually a really", "tokens": [1143, 294, 341, 2316, 13, 400, 370, 321, 2825, 466, 3278, 30867, 4470, 11, 597, 307, 767, 257, 534], "temperature": 0.0, "avg_logprob": -0.10794818735568323, "compression_ratio": 1.7269372693726937, "no_speech_prob": 3.120039400528185e-05}, {"id": 6, "seek": 2668, "start": 37.68, "end": 42.6, "text": " important concept and some of the things we're talking about today depend a bit on you understanding", "tokens": [1021, 3410, 293, 512, 295, 264, 721, 321, 434, 1417, 466, 965, 5672, 257, 857, 322, 291, 3701], "temperature": 0.0, "avg_logprob": -0.10794818735568323, "compression_ratio": 1.7269372693726937, "no_speech_prob": 3.120039400528185e-05}, {"id": 7, "seek": 2668, "start": 42.6, "end": 48.519999999999996, "text": " this concept. So if you were at all unsure about where we got to with that, go back and", "tokens": [341, 3410, 13, 407, 498, 291, 645, 412, 439, 32486, 466, 689, 321, 658, 281, 365, 300, 11, 352, 646, 293], "temperature": 0.0, "avg_logprob": -0.10794818735568323, "compression_ratio": 1.7269372693726937, "no_speech_prob": 3.120039400528185e-05}, {"id": 8, "seek": 2668, "start": 48.519999999999996, "end": 53.879999999999995, "text": " have another look, have a look at the questionnaire in particular and make sure that you're comfortable", "tokens": [362, 1071, 574, 11, 362, 257, 574, 412, 264, 44702, 294, 1729, 293, 652, 988, 300, 291, 434, 4619], "temperature": 0.0, "avg_logprob": -0.10794818735568323, "compression_ratio": 1.7269372693726937, "no_speech_prob": 3.120039400528185e-05}, {"id": 9, "seek": 5388, "start": 53.88, "end": 60.28, "text": " with cross entropy loss. If you're not, you may want to go back to the 04 MNIST basics", "tokens": [365, 3278, 30867, 4470, 13, 759, 291, 434, 406, 11, 291, 815, 528, 281, 352, 646, 281, 264, 50022, 376, 45, 19756, 14688], "temperature": 0.0, "avg_logprob": -0.11322508576095745, "compression_ratio": 1.6116071428571428, "no_speech_prob": 1.9637936929939315e-06}, {"id": 10, "seek": 5388, "start": 60.28, "end": 64.72, "text": " notebook and remind yourself about MNIST loss because it's very, very similar. That's what", "tokens": [21060, 293, 4160, 1803, 466, 376, 45, 19756, 4470, 570, 309, 311, 588, 11, 588, 2531, 13, 663, 311, 437], "temperature": 0.0, "avg_logprob": -0.11322508576095745, "compression_ratio": 1.6116071428571428, "no_speech_prob": 1.9637936929939315e-06}, {"id": 11, "seek": 5388, "start": 64.72, "end": 71.88, "text": " we built on to build up cross entropy loss. So having trained our model, the next thing", "tokens": [321, 3094, 322, 281, 1322, 493, 3278, 30867, 4470, 13, 407, 1419, 8895, 527, 2316, 11, 264, 958, 551], "temperature": 0.0, "avg_logprob": -0.11322508576095745, "compression_ratio": 1.6116071428571428, "no_speech_prob": 1.9637936929939315e-06}, {"id": 12, "seek": 5388, "start": 71.88, "end": 77.24000000000001, "text": " we're going to do is look at model interpretation. There's not much point having a model if you", "tokens": [321, 434, 516, 281, 360, 307, 574, 412, 2316, 14174, 13, 821, 311, 406, 709, 935, 1419, 257, 2316, 498, 291], "temperature": 0.0, "avg_logprob": -0.11322508576095745, "compression_ratio": 1.6116071428571428, "no_speech_prob": 1.9637936929939315e-06}, {"id": 13, "seek": 7724, "start": 77.24, "end": 85.19999999999999, "text": " don't see what it's doing. And one thing we can do is use a confusion matrix, which in", "tokens": [500, 380, 536, 437, 309, 311, 884, 13, 400, 472, 551, 321, 393, 360, 307, 764, 257, 15075, 8141, 11, 597, 294], "temperature": 0.0, "avg_logprob": -0.14114463229139312, "compression_ratio": 1.678030303030303, "no_speech_prob": 1.3925432540418115e-06}, {"id": 14, "seek": 7724, "start": 85.19999999999999, "end": 88.96, "text": " this case is not terribly helpful. There's kind of a few too many. I mean, it's not too", "tokens": [341, 1389, 307, 406, 22903, 4961, 13, 821, 311, 733, 295, 257, 1326, 886, 867, 13, 286, 914, 11, 309, 311, 406, 886], "temperature": 0.0, "avg_logprob": -0.14114463229139312, "compression_ratio": 1.678030303030303, "no_speech_prob": 1.3925432540418115e-06}, {"id": 15, "seek": 7724, "start": 88.96, "end": 93.19999999999999, "text": " bad. We can kind of see some colored areas. And so this diagonal here are all the ones", "tokens": [1578, 13, 492, 393, 733, 295, 536, 512, 14332, 3179, 13, 400, 370, 341, 21539, 510, 366, 439, 264, 2306], "temperature": 0.0, "avg_logprob": -0.14114463229139312, "compression_ratio": 1.678030303030303, "no_speech_prob": 1.3925432540418115e-06}, {"id": 16, "seek": 7724, "start": 93.19999999999999, "end": 100.44, "text": " that are classified correctly. So for Persians, there were 31 classified as Persians, but", "tokens": [300, 366, 20627, 8944, 13, 407, 337, 14006, 2567, 11, 456, 645, 10353, 20627, 382, 14006, 2567, 11, 457], "temperature": 0.0, "avg_logprob": -0.14114463229139312, "compression_ratio": 1.678030303030303, "no_speech_prob": 1.3925432540418115e-06}, {"id": 17, "seek": 7724, "start": 100.44, "end": 105.19999999999999, "text": " we can see there's some bigger numbers here like a Siamese, six were misclassified. They're", "tokens": [321, 393, 536, 456, 311, 512, 3801, 3547, 510, 411, 257, 318, 2918, 1130, 11, 2309, 645, 3346, 11665, 2587, 13, 814, 434], "temperature": 0.0, "avg_logprob": -0.14114463229139312, "compression_ratio": 1.678030303030303, "no_speech_prob": 1.3925432540418115e-06}, {"id": 18, "seek": 10520, "start": 105.2, "end": 113.32000000000001, "text": " actually considered a Berman. But for when you've got a lot of classes like this, it", "tokens": [767, 4888, 257, 363, 11821, 13, 583, 337, 562, 291, 600, 658, 257, 688, 295, 5359, 411, 341, 11, 309], "temperature": 0.0, "avg_logprob": -0.1489746806385753, "compression_ratio": 1.5974025974025974, "no_speech_prob": 1.760336203915358e-06}, {"id": 19, "seek": 10520, "start": 113.32000000000001, "end": 122.64, "text": " might be better instead to use the most confused method. And that tells you the combinations,", "tokens": [1062, 312, 1101, 2602, 281, 764, 264, 881, 9019, 3170, 13, 400, 300, 5112, 291, 264, 21267, 11], "temperature": 0.0, "avg_logprob": -0.1489746806385753, "compression_ratio": 1.5974025974025974, "no_speech_prob": 1.760336203915358e-06}, {"id": 20, "seek": 10520, "start": 122.64, "end": 127.0, "text": " which it got wrong the most often. In other words, which numbers are the biggest? So actually", "tokens": [597, 309, 658, 2085, 264, 881, 2049, 13, 682, 661, 2283, 11, 597, 3547, 366, 264, 3880, 30, 407, 767], "temperature": 0.0, "avg_logprob": -0.1489746806385753, "compression_ratio": 1.5974025974025974, "no_speech_prob": 1.760336203915358e-06}, {"id": 21, "seek": 10520, "start": 127.0, "end": 134.0, "text": " here's the biggest one, 10. And that's confusing an American pit bull terrier or a Staffordshire", "tokens": [510, 311, 264, 3880, 472, 11, 1266, 13, 400, 300, 311, 13181, 364, 2665, 10147, 4693, 1796, 7326, 420, 257, 16440, 765, 22294], "temperature": 0.0, "avg_logprob": -0.1489746806385753, "compression_ratio": 1.5974025974025974, "no_speech_prob": 1.760336203915358e-06}, {"id": 22, "seek": 13400, "start": 134.0, "end": 139.92, "text": " bull terrier that's happened 10 times. And a ragdoll is getting confused with a Berman", "tokens": [4693, 1796, 7326, 300, 311, 2011, 1266, 1413, 13, 400, 257, 17539, 67, 1833, 307, 1242, 9019, 365, 257, 363, 11821], "temperature": 0.0, "avg_logprob": -0.1432148697029831, "compression_ratio": 1.6765799256505576, "no_speech_prob": 1.3709542372453143e-06}, {"id": 23, "seek": 13400, "start": 139.92, "end": 148.0, "text": " eight times. And so I'm not a dog or cat expert. And so I don't know what this stuff means.", "tokens": [3180, 1413, 13, 400, 370, 286, 478, 406, 257, 3000, 420, 3857, 5844, 13, 400, 370, 286, 500, 380, 458, 437, 341, 1507, 1355, 13], "temperature": 0.0, "avg_logprob": -0.1432148697029831, "compression_ratio": 1.6765799256505576, "no_speech_prob": 1.3709542372453143e-06}, {"id": 24, "seek": 13400, "start": 148.0, "end": 152.36, "text": " So I looked it up on the internet and I found that American pit bull terriers and Staffordshire", "tokens": [407, 286, 2956, 309, 493, 322, 264, 4705, 293, 286, 1352, 300, 2665, 10147, 4693, 1796, 10525, 293, 16440, 765, 22294], "temperature": 0.0, "avg_logprob": -0.1432148697029831, "compression_ratio": 1.6765799256505576, "no_speech_prob": 1.3709542372453143e-06}, {"id": 25, "seek": 13400, "start": 152.36, "end": 156.64, "text": " bull terriers are almost identical, that I think they sometimes have a slightly different", "tokens": [4693, 1796, 10525, 366, 1920, 14800, 11, 300, 286, 519, 436, 2171, 362, 257, 4748, 819], "temperature": 0.0, "avg_logprob": -0.1432148697029831, "compression_ratio": 1.6765799256505576, "no_speech_prob": 1.3709542372453143e-06}, {"id": 26, "seek": 13400, "start": 156.64, "end": 161.72, "text": " colored nose, if I remember correctly. And ragdolls and Bermans are types of cats that", "tokens": [14332, 6690, 11, 498, 286, 1604, 8944, 13, 400, 17539, 67, 1833, 82, 293, 363, 966, 599, 366, 3467, 295, 11111, 300], "temperature": 0.0, "avg_logprob": -0.1432148697029831, "compression_ratio": 1.6765799256505576, "no_speech_prob": 1.3709542372453143e-06}, {"id": 27, "seek": 16172, "start": 161.72, "end": 166.84, "text": " are so similar to each other that there's whole long threads on cat lover forums about", "tokens": [366, 370, 2531, 281, 1184, 661, 300, 456, 311, 1379, 938, 19314, 322, 3857, 18009, 26998, 466], "temperature": 0.0, "avg_logprob": -0.12047341184796027, "compression_ratio": 1.7777777777777777, "no_speech_prob": 4.495097527978942e-06}, {"id": 28, "seek": 16172, "start": 166.84, "end": 172.6, "text": " is this a ragdoll or is this a Berman and experts disagreeing with each other. So no surprise", "tokens": [307, 341, 257, 17539, 67, 1833, 420, 307, 341, 257, 363, 11821, 293, 8572, 14091, 278, 365, 1184, 661, 13, 407, 572, 6365], "temperature": 0.0, "avg_logprob": -0.12047341184796027, "compression_ratio": 1.7777777777777777, "no_speech_prob": 4.495097527978942e-06}, {"id": 29, "seek": 16172, "start": 172.6, "end": 179.76, "text": " that these things are getting confused. So when you see your model making sensible mistakes,", "tokens": [300, 613, 721, 366, 1242, 9019, 13, 407, 562, 291, 536, 428, 2316, 1455, 25380, 8038, 11], "temperature": 0.0, "avg_logprob": -0.12047341184796027, "compression_ratio": 1.7777777777777777, "no_speech_prob": 4.495097527978942e-06}, {"id": 30, "seek": 16172, "start": 179.76, "end": 183.48, "text": " the kind of mistakes that humans make, that's a pretty good sign that it's picking up the", "tokens": [264, 733, 295, 8038, 300, 6255, 652, 11, 300, 311, 257, 1238, 665, 1465, 300, 309, 311, 8867, 493, 264], "temperature": 0.0, "avg_logprob": -0.12047341184796027, "compression_ratio": 1.7777777777777777, "no_speech_prob": 4.495097527978942e-06}, {"id": 31, "seek": 16172, "start": 183.48, "end": 187.64, "text": " right kind of stuff and that the kinds of errors you're getting also might be pretty", "tokens": [558, 733, 295, 1507, 293, 300, 264, 3685, 295, 13603, 291, 434, 1242, 611, 1062, 312, 1238], "temperature": 0.0, "avg_logprob": -0.12047341184796027, "compression_ratio": 1.7777777777777777, "no_speech_prob": 4.495097527978942e-06}, {"id": 32, "seek": 18764, "start": 187.64, "end": 195.83999999999997, "text": " tricky to fix. But you know, let's see if we can make it better. And one way to try", "tokens": [12414, 281, 3191, 13, 583, 291, 458, 11, 718, 311, 536, 498, 321, 393, 652, 309, 1101, 13, 400, 472, 636, 281, 853], "temperature": 0.0, "avg_logprob": -0.09525540862420592, "compression_ratio": 1.6729857819905214, "no_speech_prob": 2.9022876333328895e-06}, {"id": 33, "seek": 18764, "start": 195.83999999999997, "end": 202.0, "text": " and make it better is to improve our learning rate. Why would we want to improve the learning", "tokens": [293, 652, 309, 1101, 307, 281, 3470, 527, 2539, 3314, 13, 1545, 576, 321, 528, 281, 3470, 264, 2539], "temperature": 0.0, "avg_logprob": -0.09525540862420592, "compression_ratio": 1.6729857819905214, "no_speech_prob": 2.9022876333328895e-06}, {"id": 34, "seek": 18764, "start": 202.0, "end": 208.16, "text": " rate? Well, one thing we'd like to do is to try to train it faster, get more done in less", "tokens": [3314, 30, 1042, 11, 472, 551, 321, 1116, 411, 281, 360, 307, 281, 853, 281, 3847, 309, 4663, 11, 483, 544, 1096, 294, 1570], "temperature": 0.0, "avg_logprob": -0.09525540862420592, "compression_ratio": 1.6729857819905214, "no_speech_prob": 2.9022876333328895e-06}, {"id": 35, "seek": 18764, "start": 208.16, "end": 215.44, "text": " epochs. And so one way to do that would be to call our fine-tune method with a higher", "tokens": [30992, 28346, 13, 400, 370, 472, 636, 281, 360, 300, 576, 312, 281, 818, 527, 2489, 12, 83, 2613, 3170, 365, 257, 2946], "temperature": 0.0, "avg_logprob": -0.09525540862420592, "compression_ratio": 1.6729857819905214, "no_speech_prob": 2.9022876333328895e-06}, {"id": 36, "seek": 21544, "start": 215.44, "end": 227.32, "text": " learning rate. So last time we used the default, which I think is, there you go, one e neg", "tokens": [2539, 3314, 13, 407, 1036, 565, 321, 1143, 264, 7576, 11, 597, 286, 519, 307, 11, 456, 291, 352, 11, 472, 308, 2485], "temperature": 0.0, "avg_logprob": -0.19671317150718287, "compression_ratio": 1.467032967032967, "no_speech_prob": 1.1911066621905775e-06}, {"id": 37, "seek": 21544, "start": 227.32, "end": 234.64, "text": " two. And so if we pump that up to 0.1, it's going to jump further each time. So remember", "tokens": [732, 13, 400, 370, 498, 321, 5889, 300, 493, 281, 1958, 13, 16, 11, 309, 311, 516, 281, 3012, 3052, 1184, 565, 13, 407, 1604], "temperature": 0.0, "avg_logprob": -0.19671317150718287, "compression_ratio": 1.467032967032967, "no_speech_prob": 1.1911066621905775e-06}, {"id": 38, "seek": 21544, "start": 234.64, "end": 239.52, "text": " the learning rate, if you've forgotten this, have a look again at notebook four. That's", "tokens": [264, 2539, 3314, 11, 498, 291, 600, 11832, 341, 11, 362, 257, 574, 797, 412, 21060, 1451, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.19671317150718287, "compression_ratio": 1.467032967032967, "no_speech_prob": 1.1911066621905775e-06}, {"id": 39, "seek": 23952, "start": 239.52, "end": 245.56, "text": " the thing we multiply the gradients by to decide how far to step. And unfortunately", "tokens": [264, 551, 321, 12972, 264, 2771, 2448, 538, 281, 4536, 577, 1400, 281, 1823, 13, 400, 7015], "temperature": 0.0, "avg_logprob": -0.12709896381084734, "compression_ratio": 1.5330396475770924, "no_speech_prob": 2.6577222911328136e-07}, {"id": 40, "seek": 23952, "start": 245.56, "end": 256.48, "text": " when we use this higher learning rate, the error rate goes from 0.083 epochs to 0.83.", "tokens": [562, 321, 764, 341, 2946, 2539, 3314, 11, 264, 6713, 3314, 1709, 490, 1958, 13, 16133, 18, 30992, 28346, 281, 1958, 13, 31849, 13], "temperature": 0.0, "avg_logprob": -0.12709896381084734, "compression_ratio": 1.5330396475770924, "no_speech_prob": 2.6577222911328136e-07}, {"id": 41, "seek": 23952, "start": 256.48, "end": 260.96000000000004, "text": " So we're getting the vast majority of them wrong now. So that's not a good sign. So why", "tokens": [407, 321, 434, 1242, 264, 8369, 6286, 295, 552, 2085, 586, 13, 407, 300, 311, 406, 257, 665, 1465, 13, 407, 983], "temperature": 0.0, "avg_logprob": -0.12709896381084734, "compression_ratio": 1.5330396475770924, "no_speech_prob": 2.6577222911328136e-07}, {"id": 42, "seek": 23952, "start": 260.96000000000004, "end": 269.44, "text": " did that happen? Well, what happened is rather than this gradual move towards the minimum,", "tokens": [630, 300, 1051, 30, 1042, 11, 437, 2011, 307, 2831, 813, 341, 32890, 1286, 3030, 264, 7285, 11], "temperature": 0.0, "avg_logprob": -0.12709896381084734, "compression_ratio": 1.5330396475770924, "no_speech_prob": 2.6577222911328136e-07}, {"id": 43, "seek": 26944, "start": 269.44, "end": 277.04, "text": " we had this thing where we stepped too far and we get further, further away. So when", "tokens": [321, 632, 341, 551, 689, 321, 15251, 886, 1400, 293, 321, 483, 3052, 11, 3052, 1314, 13, 407, 562], "temperature": 0.0, "avg_logprob": -0.12648669056508732, "compression_ratio": 1.7246376811594204, "no_speech_prob": 1.3287746014611912e-06}, {"id": 44, "seek": 26944, "start": 277.04, "end": 282.44, "text": " you see this happening, which looks in practice like this, your error rate getting worse right", "tokens": [291, 536, 341, 2737, 11, 597, 1542, 294, 3124, 411, 341, 11, 428, 6713, 3314, 1242, 5324, 558], "temperature": 0.0, "avg_logprob": -0.12648669056508732, "compression_ratio": 1.7246376811594204, "no_speech_prob": 1.3287746014611912e-06}, {"id": 45, "seek": 26944, "start": 282.44, "end": 287.64, "text": " from the start, that's a sign your learning rate is too high. So we need to find something", "tokens": [490, 264, 722, 11, 300, 311, 257, 1465, 428, 2539, 3314, 307, 886, 1090, 13, 407, 321, 643, 281, 915, 746], "temperature": 0.0, "avg_logprob": -0.12648669056508732, "compression_ratio": 1.7246376811594204, "no_speech_prob": 1.3287746014611912e-06}, {"id": 46, "seek": 26944, "start": 287.64, "end": 293.36, "text": " just right, not too small that we take tiny jumps and it takes forever and not too big", "tokens": [445, 558, 11, 406, 886, 1359, 300, 321, 747, 5870, 16704, 293, 309, 2516, 5680, 293, 406, 886, 955], "temperature": 0.0, "avg_logprob": -0.12648669056508732, "compression_ratio": 1.7246376811594204, "no_speech_prob": 1.3287746014611912e-06}, {"id": 47, "seek": 29336, "start": 293.36, "end": 299.40000000000003, "text": " that we, you know, either get worse and worse or we just jump backwards and forwards quite", "tokens": [300, 321, 11, 291, 458, 11, 2139, 483, 5324, 293, 5324, 420, 321, 445, 3012, 12204, 293, 30126, 1596], "temperature": 0.0, "avg_logprob": -0.1599380558934705, "compression_ratio": 1.672811059907834, "no_speech_prob": 6.681509603367886e-07}, {"id": 48, "seek": 29336, "start": 299.40000000000003, "end": 305.5, "text": " slowly. So to find a good learning rate, we can use something that the researcher Leslie", "tokens": [5692, 13, 407, 281, 915, 257, 665, 2539, 3314, 11, 321, 393, 764, 746, 300, 264, 21751, 28140], "temperature": 0.0, "avg_logprob": -0.1599380558934705, "compression_ratio": 1.672811059907834, "no_speech_prob": 6.681509603367886e-07}, {"id": 49, "seek": 29336, "start": 305.5, "end": 311.40000000000003, "text": " Smith came up with called the learning rate finder and the learning rate finder is pretty", "tokens": [8538, 1361, 493, 365, 1219, 264, 2539, 3314, 915, 260, 293, 264, 2539, 3314, 915, 260, 307, 1238], "temperature": 0.0, "avg_logprob": -0.1599380558934705, "compression_ratio": 1.672811059907834, "no_speech_prob": 6.681509603367886e-07}, {"id": 50, "seek": 29336, "start": 311.40000000000003, "end": 317.38, "text": " simple. All we do, remember when we do stochastic gradient descent, we look at one mini batch", "tokens": [2199, 13, 1057, 321, 360, 11, 1604, 562, 321, 360, 342, 8997, 2750, 16235, 23475, 11, 321, 574, 412, 472, 8382, 15245], "temperature": 0.0, "avg_logprob": -0.1599380558934705, "compression_ratio": 1.672811059907834, "no_speech_prob": 6.681509603367886e-07}, {"id": 51, "seek": 31738, "start": 317.38, "end": 323.56, "text": " at a time, a few images in this case at a time, find the gradient for that set of images", "tokens": [412, 257, 565, 11, 257, 1326, 5267, 294, 341, 1389, 412, 257, 565, 11, 915, 264, 16235, 337, 300, 992, 295, 5267], "temperature": 0.0, "avg_logprob": -0.15411741654951494, "compression_ratio": 1.6926829268292682, "no_speech_prob": 2.964903274005337e-07}, {"id": 52, "seek": 31738, "start": 323.56, "end": 328.76, "text": " for the mini batch and jump your step-out weights based on the learning rate and the", "tokens": [337, 264, 8382, 15245, 293, 3012, 428, 1823, 12, 346, 17443, 2361, 322, 264, 2539, 3314, 293, 264], "temperature": 0.0, "avg_logprob": -0.15411741654951494, "compression_ratio": 1.6926829268292682, "no_speech_prob": 2.964903274005337e-07}, {"id": 53, "seek": 31738, "start": 328.76, "end": 335.56, "text": " gradient. Well, what Leslie Smith said was, okay, let's do the very first mini batch at", "tokens": [16235, 13, 1042, 11, 437, 28140, 8538, 848, 390, 11, 1392, 11, 718, 311, 360, 264, 588, 700, 8382, 15245, 412], "temperature": 0.0, "avg_logprob": -0.15411741654951494, "compression_ratio": 1.6926829268292682, "no_speech_prob": 2.964903274005337e-07}, {"id": 54, "seek": 31738, "start": 335.56, "end": 343.68, "text": " a really, really low learning rate, like 10 to the minus 7 and then let's increase by", "tokens": [257, 534, 11, 534, 2295, 2539, 3314, 11, 411, 1266, 281, 264, 3175, 1614, 293, 550, 718, 311, 3488, 538], "temperature": 0.0, "avg_logprob": -0.15411741654951494, "compression_ratio": 1.6926829268292682, "no_speech_prob": 2.964903274005337e-07}, {"id": 55, "seek": 34368, "start": 343.68, "end": 350.36, "text": " a little bit, so like maybe 25% higher and do another step and then 25% higher and do", "tokens": [257, 707, 857, 11, 370, 411, 1310, 3552, 4, 2946, 293, 360, 1071, 1823, 293, 550, 3552, 4, 2946, 293, 360], "temperature": 0.0, "avg_logprob": -0.15509390630641906, "compression_ratio": 1.9954128440366972, "no_speech_prob": 1.392544277223351e-06}, {"id": 56, "seek": 34368, "start": 350.36, "end": 355.84000000000003, "text": " another step. So these are not epochs, these are just a single, a simple mini batch and", "tokens": [1071, 1823, 13, 407, 613, 366, 406, 30992, 28346, 11, 613, 366, 445, 257, 2167, 11, 257, 2199, 8382, 15245, 293], "temperature": 0.0, "avg_logprob": -0.15509390630641906, "compression_ratio": 1.9954128440366972, "no_speech_prob": 1.392544277223351e-06}, {"id": 57, "seek": 34368, "start": 355.84000000000003, "end": 361.8, "text": " then we can plot on this chart here. Okay, at 10 to the minus 7, what was the loss and", "tokens": [550, 321, 393, 7542, 322, 341, 6927, 510, 13, 1033, 11, 412, 1266, 281, 264, 3175, 1614, 11, 437, 390, 264, 4470, 293], "temperature": 0.0, "avg_logprob": -0.15509390630641906, "compression_ratio": 1.9954128440366972, "no_speech_prob": 1.392544277223351e-06}, {"id": 58, "seek": 34368, "start": 361.8, "end": 365.6, "text": " at 25% higher than that, what was the loss and the 25% higher than that, what was the", "tokens": [412, 3552, 4, 2946, 813, 300, 11, 437, 390, 264, 4470, 293, 264, 3552, 4, 2946, 813, 300, 11, 437, 390, 264], "temperature": 0.0, "avg_logprob": -0.15509390630641906, "compression_ratio": 1.9954128440366972, "no_speech_prob": 1.392544277223351e-06}, {"id": 59, "seek": 34368, "start": 365.6, "end": 370.16, "text": " loss and so not surprisingly, if you do that at the low learning rates, the loss doesn't", "tokens": [4470, 293, 370, 406, 17600, 11, 498, 291, 360, 300, 412, 264, 2295, 2539, 6846, 11, 264, 4470, 1177, 380], "temperature": 0.0, "avg_logprob": -0.15509390630641906, "compression_ratio": 1.9954128440366972, "no_speech_prob": 1.392544277223351e-06}, {"id": 60, "seek": 37016, "start": 370.16, "end": 377.92, "text": " really come down because the learning rate is so small that these steps are tiny, tiny,", "tokens": [534, 808, 760, 570, 264, 2539, 3314, 307, 370, 1359, 300, 613, 4439, 366, 5870, 11, 5870, 11], "temperature": 0.0, "avg_logprob": -0.1625796097975511, "compression_ratio": 1.9910714285714286, "no_speech_prob": 9.132538707490312e-07}, {"id": 61, "seek": 37016, "start": 377.92, "end": 382.96000000000004, "text": " tiny. And then gradually we get to the point where they're big enough to make a difference", "tokens": [5870, 13, 400, 550, 13145, 321, 483, 281, 264, 935, 689, 436, 434, 955, 1547, 281, 652, 257, 2649], "temperature": 0.0, "avg_logprob": -0.1625796097975511, "compression_ratio": 1.9910714285714286, "no_speech_prob": 9.132538707490312e-07}, {"id": 62, "seek": 37016, "start": 382.96000000000004, "end": 388.12, "text": " and the loss starts coming down because we've plotted here the learning rate against the", "tokens": [293, 264, 4470, 3719, 1348, 760, 570, 321, 600, 43288, 510, 264, 2539, 3314, 1970, 264], "temperature": 0.0, "avg_logprob": -0.1625796097975511, "compression_ratio": 1.9910714285714286, "no_speech_prob": 9.132538707490312e-07}, {"id": 63, "seek": 37016, "start": 388.12, "end": 393.92, "text": " loss, right. So here the loss is coming down as we continue to increase the learning rate,", "tokens": [4470, 11, 558, 13, 407, 510, 264, 4470, 307, 1348, 760, 382, 321, 2354, 281, 3488, 264, 2539, 3314, 11], "temperature": 0.0, "avg_logprob": -0.1625796097975511, "compression_ratio": 1.9910714285714286, "no_speech_prob": 9.132538707490312e-07}, {"id": 64, "seek": 37016, "start": 393.92, "end": 398.72, "text": " the loss comes down until we get to a point where our learning rates too high and so it", "tokens": [264, 4470, 1487, 760, 1826, 321, 483, 281, 257, 935, 689, 527, 2539, 6846, 886, 1090, 293, 370, 309], "temperature": 0.0, "avg_logprob": -0.1625796097975511, "compression_ratio": 1.9910714285714286, "no_speech_prob": 9.132538707490312e-07}, {"id": 65, "seek": 39872, "start": 398.72, "end": 406.48, "text": " flattens out and then, oh it's getting worse again. So here's the point above like 0.1", "tokens": [932, 1591, 694, 484, 293, 550, 11, 1954, 309, 311, 1242, 5324, 797, 13, 407, 510, 311, 264, 935, 3673, 411, 1958, 13, 16], "temperature": 0.0, "avg_logprob": -0.18216271651418586, "compression_ratio": 1.6415094339622642, "no_speech_prob": 1.5534950534856762e-06}, {"id": 66, "seek": 39872, "start": 406.48, "end": 413.72, "text": " where we're in this territory. So what we really want is somewhere around here where", "tokens": [689, 321, 434, 294, 341, 11360, 13, 407, 437, 321, 534, 528, 307, 4079, 926, 510, 689], "temperature": 0.0, "avg_logprob": -0.18216271651418586, "compression_ratio": 1.6415094339622642, "no_speech_prob": 1.5534950534856762e-06}, {"id": 67, "seek": 39872, "start": 413.72, "end": 420.08000000000004, "text": " it's kind of nice and steep. So you can actually ask it the learning rate finders, we used", "tokens": [309, 311, 733, 295, 1481, 293, 16841, 13, 407, 291, 393, 767, 1029, 309, 264, 2539, 3314, 915, 433, 11, 321, 1143], "temperature": 0.0, "avg_logprob": -0.18216271651418586, "compression_ratio": 1.6415094339622642, "no_speech_prob": 1.5534950534856762e-06}, {"id": 68, "seek": 39872, "start": 420.08000000000004, "end": 426.12, "text": " LR find to get this plot, we can we can get back from at the minimum and steep and so", "tokens": [441, 49, 915, 281, 483, 341, 7542, 11, 321, 393, 321, 393, 483, 646, 490, 412, 264, 7285, 293, 16841, 293, 370], "temperature": 0.0, "avg_logprob": -0.18216271651418586, "compression_ratio": 1.6415094339622642, "no_speech_prob": 1.5534950534856762e-06}, {"id": 69, "seek": 42612, "start": 426.12, "end": 434.44, "text": " steep is where was it steepest, so the steepest point was 5 e-neg-3 and the minimum point", "tokens": [16841, 307, 689, 390, 309, 16841, 377, 11, 370, 264, 16841, 377, 935, 390, 1025, 308, 12, 28561, 12, 18, 293, 264, 7285, 935], "temperature": 0.0, "avg_logprob": -0.1693912802390682, "compression_ratio": 1.6409090909090909, "no_speech_prob": 1.2878921324954717e-06}, {"id": 70, "seek": 42612, "start": 434.44, "end": 439.72, "text": " divided by 10, that's quite a good rule of thumb, is 1 e-neg-2. So somewhere around this", "tokens": [6666, 538, 1266, 11, 300, 311, 1596, 257, 665, 4978, 295, 9298, 11, 307, 502, 308, 12, 28561, 12, 17, 13, 407, 4079, 926, 341], "temperature": 0.0, "avg_logprob": -0.1693912802390682, "compression_ratio": 1.6409090909090909, "no_speech_prob": 1.2878921324954717e-06}, {"id": 71, "seek": 42612, "start": 439.72, "end": 447.32, "text": " range might be pretty good. So each time you run it you'll get different values, a different", "tokens": [3613, 1062, 312, 1238, 665, 13, 407, 1184, 565, 291, 1190, 309, 291, 603, 483, 819, 4190, 11, 257, 819], "temperature": 0.0, "avg_logprob": -0.1693912802390682, "compression_ratio": 1.6409090909090909, "no_speech_prob": 1.2878921324954717e-06}, {"id": 72, "seek": 42612, "start": 447.32, "end": 452.68, "text": " time we ran it we thought that maybe 3 e-neg-3 would be good so we picked that and you'll", "tokens": [565, 321, 5872, 309, 321, 1194, 300, 1310, 805, 308, 12, 28561, 12, 18, 576, 312, 665, 370, 321, 6183, 300, 293, 291, 603], "temperature": 0.0, "avg_logprob": -0.1693912802390682, "compression_ratio": 1.6409090909090909, "no_speech_prob": 1.2878921324954717e-06}, {"id": 73, "seek": 45268, "start": 452.68, "end": 458.8, "text": " notice the learning rate finder is a logarithmic scale, be careful of interpreting that. So", "tokens": [3449, 264, 2539, 3314, 915, 260, 307, 257, 41473, 355, 13195, 4373, 11, 312, 5026, 295, 37395, 300, 13, 407], "temperature": 0.0, "avg_logprob": -0.10429216022333823, "compression_ratio": 1.8247011952191234, "no_speech_prob": 7.22443473932799e-07}, {"id": 74, "seek": 45268, "start": 458.8, "end": 464.24, "text": " we can now rerun the learning rate finder, setting the learning rate to a number we picked", "tokens": [321, 393, 586, 43819, 409, 264, 2539, 3314, 915, 260, 11, 3287, 264, 2539, 3314, 281, 257, 1230, 321, 6183], "temperature": 0.0, "avg_logprob": -0.10429216022333823, "compression_ratio": 1.8247011952191234, "no_speech_prob": 7.22443473932799e-07}, {"id": 75, "seek": 45268, "start": 464.24, "end": 469.4, "text": " from the learning rate finder which in this case was 3 e-neg-3 and we can see now that's", "tokens": [490, 264, 2539, 3314, 915, 260, 597, 294, 341, 1389, 390, 805, 308, 12, 28561, 12, 18, 293, 321, 393, 536, 586, 300, 311], "temperature": 0.0, "avg_logprob": -0.10429216022333823, "compression_ratio": 1.8247011952191234, "no_speech_prob": 7.22443473932799e-07}, {"id": 76, "seek": 45268, "start": 469.4, "end": 477.76, "text": " looking good, right? We've got an 8.3% error rate after three epochs. So this idea of the", "tokens": [1237, 665, 11, 558, 30, 492, 600, 658, 364, 1649, 13, 18, 4, 6713, 3314, 934, 1045, 30992, 28346, 13, 407, 341, 1558, 295, 264], "temperature": 0.0, "avg_logprob": -0.10429216022333823, "compression_ratio": 1.8247011952191234, "no_speech_prob": 7.22443473932799e-07}, {"id": 77, "seek": 45268, "start": 477.76, "end": 482.12, "text": " learning rate finder is very straightforward, I can describe it to you in a couple of sentences,", "tokens": [2539, 3314, 915, 260, 307, 588, 15325, 11, 286, 393, 6786, 309, 281, 291, 294, 257, 1916, 295, 16579, 11], "temperature": 0.0, "avg_logprob": -0.10429216022333823, "compression_ratio": 1.8247011952191234, "no_speech_prob": 7.22443473932799e-07}, {"id": 78, "seek": 48212, "start": 482.12, "end": 489.48, "text": " it doesn't require any complex math and yet it was only invented in 2015 which is super", "tokens": [309, 1177, 380, 3651, 604, 3997, 5221, 293, 1939, 309, 390, 787, 14479, 294, 7546, 597, 307, 1687], "temperature": 0.0, "avg_logprob": -0.12815153121948242, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.902293545048451e-06}, {"id": 79, "seek": 48212, "start": 489.48, "end": 495.48, "text": " interesting right? It just shows that there's so many interesting things for us to learn", "tokens": [1880, 558, 30, 467, 445, 3110, 300, 456, 311, 370, 867, 1880, 721, 337, 505, 281, 1466], "temperature": 0.0, "avg_logprob": -0.12815153121948242, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.902293545048451e-06}, {"id": 80, "seek": 48212, "start": 495.48, "end": 501.92, "text": " and discover. I think part of the reason perhaps for this it took a while is that you know", "tokens": [293, 4411, 13, 286, 519, 644, 295, 264, 1778, 4317, 337, 341, 309, 1890, 257, 1339, 307, 300, 291, 458], "temperature": 0.0, "avg_logprob": -0.12815153121948242, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.902293545048451e-06}, {"id": 81, "seek": 48212, "start": 501.92, "end": 506.44, "text": " engineers kind of love using lots and lots of computers, so before the learning rate", "tokens": [11955, 733, 295, 959, 1228, 3195, 293, 3195, 295, 10807, 11, 370, 949, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.12815153121948242, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.902293545048451e-06}, {"id": 82, "seek": 48212, "start": 506.44, "end": 510.76, "text": " finder came along people would like run lots of experiments on big clusters to find out", "tokens": [915, 260, 1361, 2051, 561, 576, 411, 1190, 3195, 295, 12050, 322, 955, 23313, 281, 915, 484], "temperature": 0.0, "avg_logprob": -0.12815153121948242, "compression_ratio": 1.6296296296296295, "no_speech_prob": 2.902293545048451e-06}, {"id": 83, "seek": 51076, "start": 510.76, "end": 516.28, "text": " which learning rate was the best rather than just doing a batch at a time. And I think", "tokens": [597, 2539, 3314, 390, 264, 1151, 2831, 813, 445, 884, 257, 15245, 412, 257, 565, 13, 400, 286, 519], "temperature": 0.0, "avg_logprob": -0.12150160471598308, "compression_ratio": 1.696629213483146, "no_speech_prob": 4.4254384192754515e-06}, {"id": 84, "seek": 51076, "start": 516.28, "end": 520.56, "text": " partly also the idea of having a thing where a human is in the loop where we look at something", "tokens": [17031, 611, 264, 1558, 295, 1419, 257, 551, 689, 257, 1952, 307, 294, 264, 6367, 689, 321, 574, 412, 746], "temperature": 0.0, "avg_logprob": -0.12150160471598308, "compression_ratio": 1.696629213483146, "no_speech_prob": 4.4254384192754515e-06}, {"id": 85, "seek": 51076, "start": 520.56, "end": 525.8199999999999, "text": " and make a decision is also kind of unfashionable. A lot of folks in research and industry love", "tokens": [293, 652, 257, 3537, 307, 611, 733, 295, 3971, 5894, 712, 13, 316, 688, 295, 4024, 294, 2132, 293, 3518, 959], "temperature": 0.0, "avg_logprob": -0.12150160471598308, "compression_ratio": 1.696629213483146, "no_speech_prob": 4.4254384192754515e-06}, {"id": 86, "seek": 51076, "start": 525.8199999999999, "end": 530.18, "text": " things which are fully automated, but anyway it's great we now have this tool because it", "tokens": [721, 597, 366, 4498, 18473, 11, 457, 4033, 309, 311, 869, 321, 586, 362, 341, 2290, 570, 309], "temperature": 0.0, "avg_logprob": -0.12150160471598308, "compression_ratio": 1.696629213483146, "no_speech_prob": 4.4254384192754515e-06}, {"id": 87, "seek": 51076, "start": 530.18, "end": 537.16, "text": " makes our life easier and fast AI certainly the first library to have this and I don't", "tokens": [1669, 527, 993, 3571, 293, 2370, 7318, 3297, 264, 700, 6405, 281, 362, 341, 293, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.12150160471598308, "compression_ratio": 1.696629213483146, "no_speech_prob": 4.4254384192754515e-06}, {"id": 88, "seek": 53716, "start": 537.16, "end": 542.88, "text": " know if it's still the only one to have it built in at least to the basic the base library.", "tokens": [458, 498, 309, 311, 920, 264, 787, 472, 281, 362, 309, 3094, 294, 412, 1935, 281, 264, 3875, 264, 3096, 6405, 13], "temperature": 0.0, "avg_logprob": -0.15406877061595087, "compression_ratio": 1.609865470852018, "no_speech_prob": 6.08354753239837e-07}, {"id": 89, "seek": 53716, "start": 542.88, "end": 548.88, "text": " So now we've got a good learning rate, how do we fine-tune the weights? So so far we've", "tokens": [407, 586, 321, 600, 658, 257, 665, 2539, 3314, 11, 577, 360, 321, 2489, 12, 83, 2613, 264, 17443, 30, 407, 370, 1400, 321, 600], "temperature": 0.0, "avg_logprob": -0.15406877061595087, "compression_ratio": 1.609865470852018, "no_speech_prob": 6.08354753239837e-07}, {"id": 90, "seek": 53716, "start": 548.88, "end": 555.6, "text": " just been running this fine-tune method without thinking much about what it's actually doing.", "tokens": [445, 668, 2614, 341, 2489, 12, 83, 2613, 3170, 1553, 1953, 709, 466, 437, 309, 311, 767, 884, 13], "temperature": 0.0, "avg_logprob": -0.15406877061595087, "compression_ratio": 1.609865470852018, "no_speech_prob": 6.08354753239837e-07}, {"id": 91, "seek": 53716, "start": 555.6, "end": 563.3199999999999, "text": " But we did mention in chapter one, lesson one briefly basically what's happening with", "tokens": [583, 321, 630, 2152, 294, 7187, 472, 11, 6898, 472, 10515, 1936, 437, 311, 2737, 365], "temperature": 0.0, "avg_logprob": -0.15406877061595087, "compression_ratio": 1.609865470852018, "no_speech_prob": 6.08354753239837e-07}, {"id": 92, "seek": 56332, "start": 563.32, "end": 574.6800000000001, "text": " the fine-tune, what is transfer learning doing? And before we look at that let's take a question.", "tokens": [264, 2489, 12, 83, 2613, 11, 437, 307, 5003, 2539, 884, 30, 400, 949, 321, 574, 412, 300, 718, 311, 747, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.22925740480422974, "compression_ratio": 1.300751879699248, "no_speech_prob": 2.1907712834945414e-06}, {"id": 93, "seek": 56332, "start": 574.6800000000001, "end": 579.6800000000001, "text": " Is the learning rate plot in LR find plotted against one single mini-batch?", "tokens": [1119, 264, 2539, 3314, 7542, 294, 441, 49, 915, 43288, 1970, 472, 2167, 8382, 12, 65, 852, 30], "temperature": 0.0, "avg_logprob": -0.22925740480422974, "compression_ratio": 1.300751879699248, "no_speech_prob": 2.1907712834945414e-06}, {"id": 94, "seek": 57968, "start": 579.68, "end": 594.3599999999999, "text": " No, it's not it's just it's actually just the standard kind of walking through the data", "tokens": [883, 11, 309, 311, 406, 309, 311, 445, 309, 311, 767, 445, 264, 3832, 733, 295, 4494, 807, 264, 1412], "temperature": 0.0, "avg_logprob": -0.1647158221922059, "compression_ratio": 1.6419753086419753, "no_speech_prob": 1.7330416994809639e-06}, {"id": 95, "seek": 57968, "start": 594.3599999999999, "end": 599.8, "text": " loader, so just getting the usual mini-batches of the shuffled data and so it's kind of just", "tokens": [3677, 260, 11, 370, 445, 1242, 264, 7713, 8382, 12, 65, 852, 279, 295, 264, 402, 33974, 1412, 293, 370, 309, 311, 733, 295, 445], "temperature": 0.0, "avg_logprob": -0.1647158221922059, "compression_ratio": 1.6419753086419753, "no_speech_prob": 1.7330416994809639e-06}, {"id": 96, "seek": 57968, "start": 599.8, "end": 604.12, "text": " normal training and the only thing that's been different is that we're increasing the", "tokens": [2710, 3097, 293, 264, 787, 551, 300, 311, 668, 819, 307, 300, 321, 434, 5662, 264], "temperature": 0.0, "avg_logprob": -0.1647158221922059, "compression_ratio": 1.6419753086419753, "no_speech_prob": 1.7330416994809639e-06}, {"id": 97, "seek": 60412, "start": 604.12, "end": 614.04, "text": " learning rate a little bit after each mini-batch and and keeping track of it.", "tokens": [2539, 3314, 257, 707, 857, 934, 1184, 8382, 12, 65, 852, 293, 293, 5145, 2837, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.1757579803466797, "compression_ratio": 1.535031847133758, "no_speech_prob": 6.475926852544944e-07}, {"id": 98, "seek": 60412, "start": 614.04, "end": 621.72, "text": " Along with that is is the network reset to the initial status after each trial?", "tokens": [17457, 365, 300, 307, 307, 264, 3209, 14322, 281, 264, 5883, 6558, 934, 1184, 7308, 30], "temperature": 0.0, "avg_logprob": -0.1757579803466797, "compression_ratio": 1.535031847133758, "no_speech_prob": 6.475926852544944e-07}, {"id": 99, "seek": 60412, "start": 621.72, "end": 629.16, "text": " No, certainly not we actually want to see how it learns we want to see it improving", "tokens": [883, 11, 3297, 406, 321, 767, 528, 281, 536, 577, 309, 27152, 321, 528, 281, 536, 309, 11470], "temperature": 0.0, "avg_logprob": -0.1757579803466797, "compression_ratio": 1.535031847133758, "no_speech_prob": 6.475926852544944e-07}, {"id": 100, "seek": 62916, "start": 629.16, "end": 634.48, "text": " so we don't reset it to its initial state until we're done. So at the end of it we go", "tokens": [370, 321, 500, 380, 14322, 309, 281, 1080, 5883, 1785, 1826, 321, 434, 1096, 13, 407, 412, 264, 917, 295, 309, 321, 352], "temperature": 0.0, "avg_logprob": -0.09941950211158165, "compression_ratio": 1.8304347826086957, "no_speech_prob": 5.804981242363283e-07}, {"id": 101, "seek": 62916, "start": 634.48, "end": 638.04, "text": " back to the random weights we started with or whatever the weights were at the time we", "tokens": [646, 281, 264, 4974, 17443, 321, 1409, 365, 420, 2035, 264, 17443, 645, 412, 264, 565, 321], "temperature": 0.0, "avg_logprob": -0.09941950211158165, "compression_ratio": 1.8304347826086957, "no_speech_prob": 5.804981242363283e-07}, {"id": 102, "seek": 62916, "start": 638.04, "end": 646.4399999999999, "text": " ran this. So what we're seeing here is is something that's actually the the actual learning", "tokens": [5872, 341, 13, 407, 437, 321, 434, 2577, 510, 307, 307, 746, 300, 311, 767, 264, 264, 3539, 2539], "temperature": 0.0, "avg_logprob": -0.09941950211158165, "compression_ratio": 1.8304347826086957, "no_speech_prob": 5.804981242363283e-07}, {"id": 103, "seek": 62916, "start": 646.4399999999999, "end": 653.4399999999999, "text": " that's happening as we at the same time increase the learning rate.", "tokens": [300, 311, 2737, 382, 321, 412, 264, 912, 565, 3488, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.09941950211158165, "compression_ratio": 1.8304347826086957, "no_speech_prob": 5.804981242363283e-07}, {"id": 104, "seek": 62916, "start": 653.4399999999999, "end": 657.56, "text": " Why would an ideal learning rate found with a single mini-batch at the start of training", "tokens": [1545, 576, 364, 7157, 2539, 3314, 1352, 365, 257, 2167, 8382, 12, 65, 852, 412, 264, 722, 295, 3097], "temperature": 0.0, "avg_logprob": -0.09941950211158165, "compression_ratio": 1.8304347826086957, "no_speech_prob": 5.804981242363283e-07}, {"id": 105, "seek": 65756, "start": 657.56, "end": 662.4399999999999, "text": " keep being a good learning rate even after several epochs and further loss reductions?", "tokens": [1066, 885, 257, 665, 2539, 3314, 754, 934, 2940, 30992, 28346, 293, 3052, 4470, 40296, 30], "temperature": 0.0, "avg_logprob": -0.2902734469523472, "compression_ratio": 1.6425855513307985, "no_speech_prob": 1.0129854672413785e-05}, {"id": 106, "seek": 65756, "start": 662.4399999999999, "end": 671.56, "text": " Great question, it absolutely wouldn't. So let's look at that too shall we?", "tokens": [3769, 1168, 11, 309, 3122, 2759, 380, 13, 407, 718, 311, 574, 412, 300, 886, 4393, 321, 30], "temperature": 0.0, "avg_logprob": -0.2902734469523472, "compression_ratio": 1.6425855513307985, "no_speech_prob": 1.0129854672413785e-05}, {"id": 107, "seek": 65756, "start": 671.56, "end": 673.7199999999999, "text": " And oh yeah can I ask one more?", "tokens": [400, 1954, 1338, 393, 286, 1029, 472, 544, 30], "temperature": 0.0, "avg_logprob": -0.2902734469523472, "compression_ratio": 1.6425855513307985, "no_speech_prob": 1.0129854672413785e-05}, {"id": 108, "seek": 65756, "start": 673.7199999999999, "end": 676.04, "text": " Of course, this is an important point so ask us.", "tokens": [2720, 1164, 11, 341, 307, 364, 1021, 935, 370, 1029, 505, 13], "temperature": 0.0, "avg_logprob": -0.2902734469523472, "compression_ratio": 1.6425855513307985, "no_speech_prob": 1.0129854672413785e-05}, {"id": 109, "seek": 65756, "start": 676.04, "end": 680.92, "text": " It is, this is very important. For the learning rate finder why use the steepest and not the", "tokens": [467, 307, 11, 341, 307, 588, 1021, 13, 1171, 264, 2539, 3314, 915, 260, 983, 764, 264, 16841, 377, 293, 406, 264], "temperature": 0.0, "avg_logprob": -0.2902734469523472, "compression_ratio": 1.6425855513307985, "no_speech_prob": 1.0129854672413785e-05}, {"id": 110, "seek": 65756, "start": 680.92, "end": 683.5999999999999, "text": " minimum?", "tokens": [7285, 30], "temperature": 0.0, "avg_logprob": -0.2902734469523472, "compression_ratio": 1.6425855513307985, "no_speech_prob": 1.0129854672413785e-05}, {"id": 111, "seek": 65756, "start": 683.5999999999999, "end": 687.28, "text": " We certainly don't want the minimum because the minimum is the point at which it's not", "tokens": [492, 3297, 500, 380, 528, 264, 7285, 570, 264, 7285, 307, 264, 935, 412, 597, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.2902734469523472, "compression_ratio": 1.6425855513307985, "no_speech_prob": 1.0129854672413785e-05}, {"id": 112, "seek": 68728, "start": 687.28, "end": 692.76, "text": " learning anymore. Right so this flat section at the bottom here means in this mini-batch", "tokens": [2539, 3602, 13, 1779, 370, 341, 4962, 3541, 412, 264, 2767, 510, 1355, 294, 341, 8382, 12, 65, 852], "temperature": 0.0, "avg_logprob": -0.1158523728362227, "compression_ratio": 1.673003802281369, "no_speech_prob": 4.222803909215145e-06}, {"id": 113, "seek": 68728, "start": 692.76, "end": 697.4399999999999, "text": " it didn't get better. So we want the steepest because that's the mini-batch where it got", "tokens": [309, 994, 380, 483, 1101, 13, 407, 321, 528, 264, 16841, 377, 570, 300, 311, 264, 8382, 12, 65, 852, 689, 309, 658], "temperature": 0.0, "avg_logprob": -0.1158523728362227, "compression_ratio": 1.673003802281369, "no_speech_prob": 4.222803909215145e-06}, {"id": 114, "seek": 68728, "start": 697.4399999999999, "end": 701.4399999999999, "text": " the most improved and that's what we want. We want the weights to be moving as fast as", "tokens": [264, 881, 9689, 293, 300, 311, 437, 321, 528, 13, 492, 528, 264, 17443, 281, 312, 2684, 382, 2370, 382], "temperature": 0.0, "avg_logprob": -0.1158523728362227, "compression_ratio": 1.673003802281369, "no_speech_prob": 4.222803909215145e-06}, {"id": 115, "seek": 68728, "start": 701.4399999999999, "end": 708.16, "text": " possible. As a rule of thumb though we do find that the minimum divided by 10 works", "tokens": [1944, 13, 1018, 257, 4978, 295, 9298, 1673, 321, 360, 915, 300, 264, 7285, 6666, 538, 1266, 1985], "temperature": 0.0, "avg_logprob": -0.1158523728362227, "compression_ratio": 1.673003802281369, "no_speech_prob": 4.222803909215145e-06}, {"id": 116, "seek": 68728, "start": 708.16, "end": 713.68, "text": " pretty well that's Sylvain's favorite approach and he's generally pretty spot-on with that.", "tokens": [1238, 731, 300, 311, 3902, 14574, 491, 311, 2954, 3109, 293, 415, 311, 5101, 1238, 4008, 12, 266, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.1158523728362227, "compression_ratio": 1.673003802281369, "no_speech_prob": 4.222803909215145e-06}, {"id": 117, "seek": 71368, "start": 713.68, "end": 719.4799999999999, "text": " So that's why we actually print out those two things. Lrmin is actually the minimum", "tokens": [407, 300, 311, 983, 321, 767, 4482, 484, 729, 732, 721, 13, 441, 81, 2367, 307, 767, 264, 7285], "temperature": 0.0, "avg_logprob": -0.17685022354125976, "compression_ratio": 1.5783132530120483, "no_speech_prob": 1.7061749986169161e-06}, {"id": 118, "seek": 71368, "start": 719.4799999999999, "end": 732.8, "text": " divided by 10 and steepest point is suggest the steepest point. Great good questions all.", "tokens": [6666, 538, 1266, 293, 16841, 377, 935, 307, 3402, 264, 16841, 377, 935, 13, 3769, 665, 1651, 439, 13], "temperature": 0.0, "avg_logprob": -0.17685022354125976, "compression_ratio": 1.5783132530120483, "no_speech_prob": 1.7061749986169161e-06}, {"id": 119, "seek": 71368, "start": 732.8, "end": 737.2399999999999, "text": " So remind ourselves what transfer learning does. So with transfer learning remember what", "tokens": [407, 4160, 4175, 437, 5003, 2539, 775, 13, 407, 365, 5003, 2539, 1604, 437], "temperature": 0.0, "avg_logprob": -0.17685022354125976, "compression_ratio": 1.5783132530120483, "no_speech_prob": 1.7061749986169161e-06}, {"id": 120, "seek": 73724, "start": 737.24, "end": 746.88, "text": " our neural network is. It's a bunch of linear models basically with with activation functions", "tokens": [527, 18161, 3209, 307, 13, 467, 311, 257, 3840, 295, 8213, 5245, 1936, 365, 365, 24433, 6828], "temperature": 0.0, "avg_logprob": -0.18589870265272798, "compression_ratio": 1.4887640449438202, "no_speech_prob": 2.443979383315309e-06}, {"id": 121, "seek": 73724, "start": 746.88, "end": 753.36, "text": " between them and our activation functions are generally ReLU's rectified linear units.", "tokens": [1296, 552, 293, 527, 24433, 6828, 366, 5101, 1300, 43, 52, 311, 11048, 2587, 8213, 6815, 13], "temperature": 0.0, "avg_logprob": -0.18589870265272798, "compression_ratio": 1.4887640449438202, "no_speech_prob": 2.443979383315309e-06}, {"id": 122, "seek": 73724, "start": 753.36, "end": 763.5600000000001, "text": " Any of this is fuzzy have a look at the 04 notebook again to remind yourself. And so", "tokens": [2639, 295, 341, 307, 34710, 362, 257, 574, 412, 264, 50022, 21060, 797, 281, 4160, 1803, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.18589870265272798, "compression_ratio": 1.4887640449438202, "no_speech_prob": 2.443979383315309e-06}, {"id": 123, "seek": 76356, "start": 763.56, "end": 767.4, "text": " each of those linear layers has a bunch of parameters to the whole neural network has", "tokens": [1184, 295, 729, 8213, 7914, 575, 257, 3840, 295, 9834, 281, 264, 1379, 18161, 3209, 575], "temperature": 0.0, "avg_logprob": -0.0813616321932885, "compression_ratio": 1.9298245614035088, "no_speech_prob": 5.453277367450937e-07}, {"id": 124, "seek": 76356, "start": 767.4, "end": 776.16, "text": " a bunch of parameters and so after we train a neural network on something like ImageNet", "tokens": [257, 3840, 295, 9834, 293, 370, 934, 321, 3847, 257, 18161, 3209, 322, 746, 411, 29903, 31890], "temperature": 0.0, "avg_logprob": -0.0813616321932885, "compression_ratio": 1.9298245614035088, "no_speech_prob": 5.453277367450937e-07}, {"id": 125, "seek": 76356, "start": 776.16, "end": 779.9599999999999, "text": " we have a whole bunch of parameters that aren't random anymore they're actually useful for", "tokens": [321, 362, 257, 1379, 3840, 295, 9834, 300, 3212, 380, 4974, 3602, 436, 434, 767, 4420, 337], "temperature": 0.0, "avg_logprob": -0.0813616321932885, "compression_ratio": 1.9298245614035088, "no_speech_prob": 5.453277367450937e-07}, {"id": 126, "seek": 76356, "start": 779.9599999999999, "end": 786.68, "text": " something and we've also seen that the early layers seem to learn about fairly general", "tokens": [746, 293, 321, 600, 611, 1612, 300, 264, 2440, 7914, 1643, 281, 1466, 466, 6457, 2674], "temperature": 0.0, "avg_logprob": -0.0813616321932885, "compression_ratio": 1.9298245614035088, "no_speech_prob": 5.453277367450937e-07}, {"id": 127, "seek": 76356, "start": 786.68, "end": 791.52, "text": " ideas like gradients and edges and the later layers learn about more sophisticated ideas", "tokens": [3487, 411, 2771, 2448, 293, 8819, 293, 264, 1780, 7914, 1466, 466, 544, 16950, 3487], "temperature": 0.0, "avg_logprob": -0.0813616321932885, "compression_ratio": 1.9298245614035088, "no_speech_prob": 5.453277367450937e-07}, {"id": 128, "seek": 79152, "start": 791.52, "end": 797.3199999999999, "text": " like what our eyes look like or what does fur look like or what does text look like.", "tokens": [411, 437, 527, 2575, 574, 411, 420, 437, 775, 2687, 574, 411, 420, 437, 775, 2487, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.14894952026067995, "compression_ratio": 1.8130081300813008, "no_speech_prob": 4.710876055469271e-06}, {"id": 129, "seek": 79152, "start": 797.3199999999999, "end": 803.0799999999999, "text": " So with transfer learning we take a model so in other words a set of parameters which", "tokens": [407, 365, 5003, 2539, 321, 747, 257, 2316, 370, 294, 661, 2283, 257, 992, 295, 9834, 597], "temperature": 0.0, "avg_logprob": -0.14894952026067995, "compression_ratio": 1.8130081300813008, "no_speech_prob": 4.710876055469271e-06}, {"id": 130, "seek": 79152, "start": 803.0799999999999, "end": 808.76, "text": " has already been trained something like ImageNet we throw away the very last layer because", "tokens": [575, 1217, 668, 8895, 746, 411, 29903, 31890, 321, 3507, 1314, 264, 588, 1036, 4583, 570], "temperature": 0.0, "avg_logprob": -0.14894952026067995, "compression_ratio": 1.8130081300813008, "no_speech_prob": 4.710876055469271e-06}, {"id": 131, "seek": 79152, "start": 808.76, "end": 813.84, "text": " the very last layer is the bit that specifically says which one of those in the case of ImageNet", "tokens": [264, 588, 1036, 4583, 307, 264, 857, 300, 4682, 1619, 597, 472, 295, 729, 294, 264, 1389, 295, 29903, 31890], "temperature": 0.0, "avg_logprob": -0.14894952026067995, "compression_ratio": 1.8130081300813008, "no_speech_prob": 4.710876055469271e-06}, {"id": 132, "seek": 79152, "start": 813.84, "end": 819.92, "text": " 1000 categories is this an image in so we throw that away and we replace it with random", "tokens": [9714, 10479, 307, 341, 364, 3256, 294, 370, 321, 3507, 300, 1314, 293, 321, 7406, 309, 365, 4974], "temperature": 0.0, "avg_logprob": -0.14894952026067995, "compression_ratio": 1.8130081300813008, "no_speech_prob": 4.710876055469271e-06}, {"id": 133, "seek": 81992, "start": 819.92, "end": 826.64, "text": " weights sometimes with more than one layer of random weights and then we train that.", "tokens": [17443, 2171, 365, 544, 813, 472, 4583, 295, 4974, 17443, 293, 550, 321, 3847, 300, 13], "temperature": 0.0, "avg_logprob": -0.10237564760095932, "compression_ratio": 1.6604651162790698, "no_speech_prob": 5.093620529805776e-06}, {"id": 134, "seek": 81992, "start": 826.64, "end": 835.56, "text": " Now yes. Oh I just wanted to make a comment and that's that I think the learning rate", "tokens": [823, 2086, 13, 876, 286, 445, 1415, 281, 652, 257, 2871, 293, 300, 311, 300, 286, 519, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.10237564760095932, "compression_ratio": 1.6604651162790698, "no_speech_prob": 5.093620529805776e-06}, {"id": 135, "seek": 81992, "start": 835.56, "end": 843.9599999999999, "text": " finder I think after you learn about it the idea almost seems kind of so simple or approximate", "tokens": [915, 260, 286, 519, 934, 291, 1466, 466, 309, 264, 1558, 1920, 2544, 733, 295, 370, 2199, 420, 30874], "temperature": 0.0, "avg_logprob": -0.10237564760095932, "compression_ratio": 1.6604651162790698, "no_speech_prob": 5.093620529805776e-06}, {"id": 136, "seek": 81992, "start": 843.9599999999999, "end": 848.3399999999999, "text": " that it's like wait this shouldn't work like or you know shouldn't you have to do something", "tokens": [300, 309, 311, 411, 1699, 341, 4659, 380, 589, 411, 420, 291, 458, 4659, 380, 291, 362, 281, 360, 746], "temperature": 0.0, "avg_logprob": -0.10237564760095932, "compression_ratio": 1.6604651162790698, "no_speech_prob": 5.093620529805776e-06}, {"id": 137, "seek": 84834, "start": 848.34, "end": 853.0, "text": " more more complicated or more precise that it's like I just want to highlight that this", "tokens": [544, 544, 6179, 420, 544, 13600, 300, 309, 311, 411, 286, 445, 528, 281, 5078, 300, 341], "temperature": 0.0, "avg_logprob": -0.11366090774536133, "compression_ratio": 1.634703196347032, "no_speech_prob": 1.7603257447262877e-06}, {"id": 138, "seek": 84834, "start": 853.0, "end": 859.32, "text": " is a very surprising result that some kind of a such a simple approximate method would", "tokens": [307, 257, 588, 8830, 1874, 300, 512, 733, 295, 257, 1270, 257, 2199, 30874, 3170, 576], "temperature": 0.0, "avg_logprob": -0.11366090774536133, "compression_ratio": 1.634703196347032, "no_speech_prob": 1.7603257447262877e-06}, {"id": 139, "seek": 84834, "start": 859.32, "end": 865.74, "text": " be so helpful. Yeah I would particularly say it's surprising to people who are not practitioners", "tokens": [312, 370, 4961, 13, 865, 286, 576, 4098, 584, 309, 311, 8830, 281, 561, 567, 366, 406, 25742], "temperature": 0.0, "avg_logprob": -0.11366090774536133, "compression_ratio": 1.634703196347032, "no_speech_prob": 1.7603257447262877e-06}, {"id": 140, "seek": 84834, "start": 865.74, "end": 874.48, "text": " or who have not been practitioners for long. I've noticed that a lot of my students at", "tokens": [420, 567, 362, 406, 668, 25742, 337, 938, 13, 286, 600, 5694, 300, 257, 688, 295, 452, 1731, 412], "temperature": 0.0, "avg_logprob": -0.11366090774536133, "compression_ratio": 1.634703196347032, "no_speech_prob": 1.7603257447262877e-06}, {"id": 141, "seek": 87448, "start": 874.48, "end": 879.84, "text": " USF have a tendency to kind of jump in to try to doing something very complex where", "tokens": [2546, 37, 362, 257, 18187, 281, 733, 295, 3012, 294, 281, 853, 281, 884, 746, 588, 3997, 689], "temperature": 0.0, "avg_logprob": -0.08035116929274339, "compression_ratio": 1.7109375, "no_speech_prob": 1.9525263269315474e-05}, {"id": 142, "seek": 87448, "start": 879.84, "end": 884.6, "text": " they account for every possible imperfection from the start and it's very rare that that's", "tokens": [436, 2696, 337, 633, 1944, 26714, 313, 490, 264, 722, 293, 309, 311, 588, 5892, 300, 300, 311], "temperature": 0.0, "avg_logprob": -0.08035116929274339, "compression_ratio": 1.7109375, "no_speech_prob": 1.9525263269315474e-05}, {"id": 143, "seek": 87448, "start": 884.6, "end": 889.9200000000001, "text": " necessary so one of the cool things about this is it's good example of trying the easiest", "tokens": [4818, 370, 472, 295, 264, 1627, 721, 466, 341, 307, 309, 311, 665, 1365, 295, 1382, 264, 12889], "temperature": 0.0, "avg_logprob": -0.08035116929274339, "compression_ratio": 1.7109375, "no_speech_prob": 1.9525263269315474e-05}, {"id": 144, "seek": 87448, "start": 889.9200000000001, "end": 894.48, "text": " thing first and seeing how well it works. And this was a very big innovation when it", "tokens": [551, 700, 293, 2577, 577, 731, 309, 1985, 13, 400, 341, 390, 257, 588, 955, 8504, 562, 309], "temperature": 0.0, "avg_logprob": -0.08035116929274339, "compression_ratio": 1.7109375, "no_speech_prob": 1.9525263269315474e-05}, {"id": 145, "seek": 87448, "start": 894.48, "end": 899.94, "text": " came out that I think it's kind of easy to take for granted now but this was super super", "tokens": [1361, 484, 300, 286, 519, 309, 311, 733, 295, 1858, 281, 747, 337, 12344, 586, 457, 341, 390, 1687, 1687], "temperature": 0.0, "avg_logprob": -0.08035116929274339, "compression_ratio": 1.7109375, "no_speech_prob": 1.9525263269315474e-05}, {"id": 146, "seek": 89994, "start": 899.94, "end": 905.36, "text": " helpful when it was kind of a new one. It was super helpful and it was also nearly entirely", "tokens": [4961, 562, 309, 390, 733, 295, 257, 777, 472, 13, 467, 390, 1687, 4961, 293, 309, 390, 611, 6217, 7696], "temperature": 0.0, "avg_logprob": -0.1615396428991247, "compression_ratio": 1.7683397683397684, "no_speech_prob": 2.8572901555889985e-06}, {"id": 147, "seek": 89994, "start": 905.36, "end": 910.0400000000001, "text": " ignored. None of the research community cared about it and it wasn't until fast.ai I think", "tokens": [19735, 13, 14492, 295, 264, 2132, 1768, 19779, 466, 309, 293, 309, 2067, 380, 1826, 2370, 13, 1301, 286, 519], "temperature": 0.0, "avg_logprob": -0.1615396428991247, "compression_ratio": 1.7683397683397684, "no_speech_prob": 2.8572901555889985e-06}, {"id": 148, "seek": 89994, "start": 910.0400000000001, "end": 916.24, "text": " in our first course talked about it that people started noticing and we had quite a few years", "tokens": [294, 527, 700, 1164, 2825, 466, 309, 300, 561, 1409, 21814, 293, 321, 632, 1596, 257, 1326, 924], "temperature": 0.0, "avg_logprob": -0.1615396428991247, "compression_ratio": 1.7683397683397684, "no_speech_prob": 2.8572901555889985e-06}, {"id": 149, "seek": 89994, "start": 916.24, "end": 921.08, "text": " in fact it's still a bit the case where super fancy researchers still don't know about the", "tokens": [294, 1186, 309, 311, 920, 257, 857, 264, 1389, 689, 1687, 10247, 10309, 920, 500, 380, 458, 466, 264], "temperature": 0.0, "avg_logprob": -0.1615396428991247, "compression_ratio": 1.7683397683397684, "no_speech_prob": 2.8572901555889985e-06}, {"id": 150, "seek": 89994, "start": 921.08, "end": 929.1600000000001, "text": " learning rate finder and you know get get beaten by you know first lesson fast.ai students", "tokens": [2539, 3314, 915, 260, 293, 291, 458, 483, 483, 17909, 538, 291, 458, 700, 6898, 2370, 13, 1301, 1731], "temperature": 0.0, "avg_logprob": -0.1615396428991247, "compression_ratio": 1.7683397683397684, "no_speech_prob": 2.8572901555889985e-06}, {"id": 151, "seek": 92916, "start": 929.16, "end": 934.4, "text": " on practical problems because they can pick learning rates better and they can do it without", "tokens": [322, 8496, 2740, 570, 436, 393, 1888, 2539, 6846, 1101, 293, 436, 393, 360, 309, 1553], "temperature": 0.0, "avg_logprob": -0.11618577698130667, "compression_ratio": 1.6199095022624435, "no_speech_prob": 3.6688104501081398e-06}, {"id": 152, "seek": 92916, "start": 934.4, "end": 943.24, "text": " a cluster of thousands of computers. Okay so transfer learning. So we've got our pre-trained", "tokens": [257, 13630, 295, 5383, 295, 10807, 13, 1033, 370, 5003, 2539, 13, 407, 321, 600, 658, 527, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.11618577698130667, "compression_ratio": 1.6199095022624435, "no_speech_prob": 3.6688104501081398e-06}, {"id": 153, "seek": 92916, "start": 943.24, "end": 946.8, "text": " network and so it's really important every time you hear the word pre-trained network", "tokens": [3209, 293, 370, 309, 311, 534, 1021, 633, 565, 291, 1568, 264, 1349, 659, 12, 17227, 2001, 3209], "temperature": 0.0, "avg_logprob": -0.11618577698130667, "compression_ratio": 1.6199095022624435, "no_speech_prob": 3.6688104501081398e-06}, {"id": 154, "seek": 92916, "start": 946.8, "end": 952.78, "text": " you're thinking a bunch of parameters which have particular numeric values and go with", "tokens": [291, 434, 1953, 257, 3840, 295, 9834, 597, 362, 1729, 7866, 299, 4190, 293, 352, 365], "temperature": 0.0, "avg_logprob": -0.11618577698130667, "compression_ratio": 1.6199095022624435, "no_speech_prob": 3.6688104501081398e-06}, {"id": 155, "seek": 95278, "start": 952.78, "end": 959.4, "text": " a particular architecture like ResNet34. We've thrown away the final layer and replaced them", "tokens": [257, 1729, 9482, 411, 5015, 31890, 12249, 13, 492, 600, 11732, 1314, 264, 2572, 4583, 293, 10772, 552], "temperature": 0.0, "avg_logprob": -0.12591291510540506, "compression_ratio": 1.4887640449438202, "no_speech_prob": 6.37551977433759e-07}, {"id": 156, "seek": 95278, "start": 959.4, "end": 967.6, "text": " with random numbers and so now we want to train to fine-tune this set of parameters", "tokens": [365, 4974, 3547, 293, 370, 586, 321, 528, 281, 3847, 281, 2489, 12, 83, 2613, 341, 992, 295, 9834], "temperature": 0.0, "avg_logprob": -0.12591291510540506, "compression_ratio": 1.4887640449438202, "no_speech_prob": 6.37551977433759e-07}, {"id": 157, "seek": 95278, "start": 967.6, "end": 977.5799999999999, "text": " for a new set of images in this case pets. So fine-tune is the method we call to do that", "tokens": [337, 257, 777, 992, 295, 5267, 294, 341, 1389, 19897, 13, 407, 2489, 12, 83, 2613, 307, 264, 3170, 321, 818, 281, 360, 300], "temperature": 0.0, "avg_logprob": -0.12591291510540506, "compression_ratio": 1.4887640449438202, "no_speech_prob": 6.37551977433759e-07}, {"id": 158, "seek": 97758, "start": 977.58, "end": 983.08, "text": " and to see what it does we can go burn dot fine-tune question mark and we can see the", "tokens": [293, 281, 536, 437, 309, 775, 321, 393, 352, 5064, 5893, 2489, 12, 83, 2613, 1168, 1491, 293, 321, 393, 536, 264], "temperature": 0.0, "avg_logprob": -0.09573978366273822, "compression_ratio": 1.592814371257485, "no_speech_prob": 5.896409334127384e-07}, {"id": 159, "seek": 97758, "start": 983.08, "end": 989.5400000000001, "text": " source code and here is the signature of the function and so the first thing that happens", "tokens": [4009, 3089, 293, 510, 307, 264, 13397, 295, 264, 2445, 293, 370, 264, 700, 551, 300, 2314], "temperature": 0.0, "avg_logprob": -0.09573978366273822, "compression_ratio": 1.592814371257485, "no_speech_prob": 5.896409334127384e-07}, {"id": 160, "seek": 97758, "start": 989.5400000000001, "end": 1001.6600000000001, "text": " is we call freeze. So freeze is actually the method which makes it so only the last layers", "tokens": [307, 321, 818, 15959, 13, 407, 15959, 307, 767, 264, 3170, 597, 1669, 309, 370, 787, 264, 1036, 7914], "temperature": 0.0, "avg_logprob": -0.09573978366273822, "compression_ratio": 1.592814371257485, "no_speech_prob": 5.896409334127384e-07}, {"id": 161, "seek": 100166, "start": 1001.66, "end": 1007.88, "text": " weights will get stepped by the optimizer. So the gradients are calculated just for those", "tokens": [17443, 486, 483, 15251, 538, 264, 5028, 6545, 13, 407, 264, 2771, 2448, 366, 15598, 445, 337, 729], "temperature": 0.0, "avg_logprob": -0.09757969800163718, "compression_ratio": 1.7611940298507462, "no_speech_prob": 3.689878269597102e-07}, {"id": 162, "seek": 100166, "start": 1007.88, "end": 1012.48, "text": " last layers of parameters and the step is done just for those last layer of parameters.", "tokens": [1036, 7914, 295, 9834, 293, 264, 1823, 307, 1096, 445, 337, 729, 1036, 4583, 295, 9834, 13], "temperature": 0.0, "avg_logprob": -0.09757969800163718, "compression_ratio": 1.7611940298507462, "no_speech_prob": 3.689878269597102e-07}, {"id": 163, "seek": 100166, "start": 1012.48, "end": 1021.06, "text": " So then we call fit and we fit for some number of epochs which by default is one. We don't", "tokens": [407, 550, 321, 818, 3318, 293, 321, 3318, 337, 512, 1230, 295, 30992, 28346, 597, 538, 7576, 307, 472, 13, 492, 500, 380], "temperature": 0.0, "avg_logprob": -0.09757969800163718, "compression_ratio": 1.7611940298507462, "no_speech_prob": 3.689878269597102e-07}, {"id": 164, "seek": 100166, "start": 1021.06, "end": 1029.72, "text": " change that very often and what that fit is doing is it's just fitting those randomly", "tokens": [1319, 300, 588, 2049, 293, 437, 300, 3318, 307, 884, 307, 309, 311, 445, 15669, 729, 16979], "temperature": 0.0, "avg_logprob": -0.09757969800163718, "compression_ratio": 1.7611940298507462, "no_speech_prob": 3.689878269597102e-07}, {"id": 165, "seek": 102972, "start": 1029.72, "end": 1033.82, "text": " added weights which makes sense right they're the ones that are going to need the most work", "tokens": [3869, 17443, 597, 1669, 2020, 558, 436, 434, 264, 2306, 300, 366, 516, 281, 643, 264, 881, 589], "temperature": 0.0, "avg_logprob": -0.1472301306547942, "compression_ratio": 1.8148148148148149, "no_speech_prob": 2.4060959731286857e-06}, {"id": 166, "seek": 102972, "start": 1033.82, "end": 1040.1200000000001, "text": " because at the time which we add them they're doing nothing at all they're just random.", "tokens": [570, 412, 264, 565, 597, 321, 909, 552, 436, 434, 884, 1825, 412, 439, 436, 434, 445, 4974, 13], "temperature": 0.0, "avg_logprob": -0.1472301306547942, "compression_ratio": 1.8148148148148149, "no_speech_prob": 2.4060959731286857e-06}, {"id": 167, "seek": 102972, "start": 1040.1200000000001, "end": 1046.3600000000001, "text": " So that's why we spend one epoch trying to make them better. After you've done that you", "tokens": [407, 300, 311, 983, 321, 3496, 472, 30992, 339, 1382, 281, 652, 552, 1101, 13, 2381, 291, 600, 1096, 300, 291], "temperature": 0.0, "avg_logprob": -0.1472301306547942, "compression_ratio": 1.8148148148148149, "no_speech_prob": 2.4060959731286857e-06}, {"id": 168, "seek": 102972, "start": 1046.3600000000001, "end": 1053.56, "text": " now have a model which is much better than we started with it's not random anymore. All", "tokens": [586, 362, 257, 2316, 597, 307, 709, 1101, 813, 321, 1409, 365, 309, 311, 406, 4974, 3602, 13, 1057], "temperature": 0.0, "avg_logprob": -0.1472301306547942, "compression_ratio": 1.8148148148148149, "no_speech_prob": 2.4060959731286857e-06}, {"id": 169, "seek": 102972, "start": 1053.56, "end": 1057.06, "text": " the layers except the last are the same as the pre-trained network the last layer has", "tokens": [264, 7914, 3993, 264, 1036, 366, 264, 912, 382, 264, 659, 12, 17227, 2001, 3209, 264, 1036, 4583, 575], "temperature": 0.0, "avg_logprob": -0.1472301306547942, "compression_ratio": 1.8148148148148149, "no_speech_prob": 2.4060959731286857e-06}, {"id": 170, "seek": 105706, "start": 1057.06, "end": 1063.96, "text": " been tuned for this new data set. So the closer you get to the right answer as you can kind", "tokens": [668, 10870, 337, 341, 777, 1412, 992, 13, 407, 264, 4966, 291, 483, 281, 264, 558, 1867, 382, 291, 393, 733], "temperature": 0.0, "avg_logprob": -0.07618653557517312, "compression_ratio": 1.82, "no_speech_prob": 4.888291300630954e-07}, {"id": 171, "seek": 105706, "start": 1063.96, "end": 1068.52, "text": " of see in this picture the smaller the steps you want to create sorry the smaller the steps", "tokens": [295, 536, 294, 341, 3036, 264, 4356, 264, 4439, 291, 528, 281, 1884, 2597, 264, 4356, 264, 4439], "temperature": 0.0, "avg_logprob": -0.07618653557517312, "compression_ratio": 1.82, "no_speech_prob": 4.888291300630954e-07}, {"id": 172, "seek": 105706, "start": 1068.52, "end": 1073.28, "text": " you want to take generally speaking. The next thing we do is we divide our learning rate", "tokens": [291, 528, 281, 747, 5101, 4124, 13, 440, 958, 551, 321, 360, 307, 321, 9845, 527, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.07618653557517312, "compression_ratio": 1.82, "no_speech_prob": 4.888291300630954e-07}, {"id": 173, "seek": 105706, "start": 1073.28, "end": 1078.52, "text": " by two and then we unfreeze so that means we make it so that all the parameters can", "tokens": [538, 732, 293, 550, 321, 3971, 701, 1381, 370, 300, 1355, 321, 652, 309, 370, 300, 439, 264, 9834, 393], "temperature": 0.0, "avg_logprob": -0.07618653557517312, "compression_ratio": 1.82, "no_speech_prob": 4.888291300630954e-07}, {"id": 174, "seek": 105706, "start": 1078.52, "end": 1085.2, "text": " now be stepped and all of them will have gradients calculated and then we fit for some more epochs", "tokens": [586, 312, 15251, 293, 439, 295, 552, 486, 362, 2771, 2448, 15598, 293, 550, 321, 3318, 337, 512, 544, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.07618653557517312, "compression_ratio": 1.82, "no_speech_prob": 4.888291300630954e-07}, {"id": 175, "seek": 108520, "start": 1085.2, "end": 1093.8, "text": " and this is something we have to pass to the method. And so that's now got to train the", "tokens": [293, 341, 307, 746, 321, 362, 281, 1320, 281, 264, 3170, 13, 400, 370, 300, 311, 586, 658, 281, 3847, 264], "temperature": 0.0, "avg_logprob": -0.1235198241013747, "compression_ratio": 1.5808383233532934, "no_speech_prob": 1.4823536957919714e-06}, {"id": 176, "seek": 108520, "start": 1093.8, "end": 1100.0800000000002, "text": " whole network so if we want to we can kind of do this by hand right and actually CNN", "tokens": [1379, 3209, 370, 498, 321, 528, 281, 321, 393, 733, 295, 360, 341, 538, 1011, 558, 293, 767, 24859], "temperature": 0.0, "avg_logprob": -0.1235198241013747, "compression_ratio": 1.5808383233532934, "no_speech_prob": 1.4823536957919714e-06}, {"id": 177, "seek": 108520, "start": 1100.0800000000002, "end": 1108.56, "text": " learner will by default freeze the model for us freeze the parameters for us so we actually", "tokens": [33347, 486, 538, 7576, 15959, 264, 2316, 337, 505, 15959, 264, 9834, 337, 505, 370, 321, 767], "temperature": 0.0, "avg_logprob": -0.1235198241013747, "compression_ratio": 1.5808383233532934, "no_speech_prob": 1.4823536957919714e-06}, {"id": 178, "seek": 110856, "start": 1108.56, "end": 1115.44, "text": " don't have to call freeze. So if we just create our learner and then fit for a while this", "tokens": [500, 380, 362, 281, 818, 15959, 13, 407, 498, 321, 445, 1884, 527, 33347, 293, 550, 3318, 337, 257, 1339, 341], "temperature": 0.0, "avg_logprob": -0.11191383627958076, "compression_ratio": 1.650943396226415, "no_speech_prob": 6.083578796278744e-07}, {"id": 179, "seek": 110856, "start": 1115.44, "end": 1121.6, "text": " is three epochs of training just the last layer and so then we can just manually do", "tokens": [307, 1045, 30992, 28346, 295, 3097, 445, 264, 1036, 4583, 293, 370, 550, 321, 393, 445, 16945, 360], "temperature": 0.0, "avg_logprob": -0.11191383627958076, "compression_ratio": 1.650943396226415, "no_speech_prob": 6.083578796278744e-07}, {"id": 180, "seek": 110856, "start": 1121.6, "end": 1128.84, "text": " it ourselves unfreeze and so now at this point as the question earlier suggested maybe this", "tokens": [309, 4175, 3971, 701, 1381, 293, 370, 586, 412, 341, 935, 382, 264, 1168, 3071, 10945, 1310, 341], "temperature": 0.0, "avg_logprob": -0.11191383627958076, "compression_ratio": 1.650943396226415, "no_speech_prob": 6.083578796278744e-07}, {"id": 181, "seek": 110856, "start": 1128.84, "end": 1134.72, "text": " is not the right learning rate anymore so we can run LR find again and this time you", "tokens": [307, 406, 264, 558, 2539, 3314, 3602, 370, 321, 393, 1190, 441, 49, 915, 797, 293, 341, 565, 291], "temperature": 0.0, "avg_logprob": -0.11191383627958076, "compression_ratio": 1.650943396226415, "no_speech_prob": 6.083578796278744e-07}, {"id": 182, "seek": 113472, "start": 1134.72, "end": 1140.56, "text": " don't see the same shape you don't see this rapid drop because it's much harder to train", "tokens": [500, 380, 536, 264, 912, 3909, 291, 500, 380, 536, 341, 7558, 3270, 570, 309, 311, 709, 6081, 281, 3847], "temperature": 0.0, "avg_logprob": -0.14976593364368784, "compression_ratio": 1.688715953307393, "no_speech_prob": 7.69036091696762e-07}, {"id": 183, "seek": 113472, "start": 1140.56, "end": 1146.72, "text": " a model that's already pretty good. Instead you just see a very gentle little gradient", "tokens": [257, 2316, 300, 311, 1217, 1238, 665, 13, 7156, 291, 445, 536, 257, 588, 6424, 707, 16235], "temperature": 0.0, "avg_logprob": -0.14976593364368784, "compression_ratio": 1.688715953307393, "no_speech_prob": 7.69036091696762e-07}, {"id": 184, "seek": 113472, "start": 1146.72, "end": 1151.3, "text": " so generally here what we do is we kind of try to find the bit where it starts to get", "tokens": [370, 5101, 510, 437, 321, 360, 307, 321, 733, 295, 853, 281, 915, 264, 857, 689, 309, 3719, 281, 483], "temperature": 0.0, "avg_logprob": -0.14976593364368784, "compression_ratio": 1.688715953307393, "no_speech_prob": 7.69036091696762e-07}, {"id": 185, "seek": 113472, "start": 1151.3, "end": 1156.8, "text": " worse again and go about which is about here and go about 10 let you know multiple of 10", "tokens": [5324, 797, 293, 352, 466, 597, 307, 466, 510, 293, 352, 466, 1266, 718, 291, 458, 3866, 295, 1266], "temperature": 0.0, "avg_logprob": -0.14976593364368784, "compression_ratio": 1.688715953307393, "no_speech_prob": 7.69036091696762e-07}, {"id": 186, "seek": 113472, "start": 1156.8, "end": 1161.6000000000001, "text": " less than that so about 1.8 neg 5 I would guess which yep that's what we picked. So", "tokens": [1570, 813, 300, 370, 466, 502, 13, 23, 2485, 1025, 286, 576, 2041, 597, 18633, 300, 311, 437, 321, 6183, 13, 407], "temperature": 0.0, "avg_logprob": -0.14976593364368784, "compression_ratio": 1.688715953307393, "no_speech_prob": 7.69036091696762e-07}, {"id": 187, "seek": 116160, "start": 1161.6, "end": 1168.08, "text": " then after unfreezing finding our new learning rate and then we can do a bunch more and so", "tokens": [550, 934, 3971, 701, 8781, 5006, 527, 777, 2539, 3314, 293, 550, 321, 393, 360, 257, 3840, 544, 293, 370], "temperature": 0.0, "avg_logprob": -0.10899245220681895, "compression_ratio": 1.726829268292683, "no_speech_prob": 3.3075886562983214e-07}, {"id": 188, "seek": 116160, "start": 1168.08, "end": 1175.6, "text": " here we are we're getting down to 5.9 percent error which is okay but there's there's better", "tokens": [510, 321, 366, 321, 434, 1242, 760, 281, 1025, 13, 24, 3043, 6713, 597, 307, 1392, 457, 456, 311, 456, 311, 1101], "temperature": 0.0, "avg_logprob": -0.10899245220681895, "compression_ratio": 1.726829268292683, "no_speech_prob": 3.3075886562983214e-07}, {"id": 189, "seek": 116160, "start": 1175.6, "end": 1181.24, "text": " we can do and the reason we can do better is that at this point here we're training", "tokens": [321, 393, 360, 293, 264, 1778, 321, 393, 360, 1101, 307, 300, 412, 341, 935, 510, 321, 434, 3097], "temperature": 0.0, "avg_logprob": -0.10899245220681895, "compression_ratio": 1.726829268292683, "no_speech_prob": 3.3075886562983214e-07}, {"id": 190, "seek": 116160, "start": 1181.24, "end": 1188.6, "text": " the whole model at a 1 a neg 5 so 10 to the minus 5 learning rate which doesn't really", "tokens": [264, 1379, 2316, 412, 257, 502, 257, 2485, 1025, 370, 1266, 281, 264, 3175, 1025, 2539, 3314, 597, 1177, 380, 534], "temperature": 0.0, "avg_logprob": -0.10899245220681895, "compression_ratio": 1.726829268292683, "no_speech_prob": 3.3075886562983214e-07}, {"id": 191, "seek": 118860, "start": 1188.6, "end": 1193.9199999999998, "text": " make sense because we know that the last layer is still not that great it's only had three", "tokens": [652, 2020, 570, 321, 458, 300, 264, 1036, 4583, 307, 920, 406, 300, 869, 309, 311, 787, 632, 1045], "temperature": 0.0, "avg_logprob": -0.12060632155491756, "compression_ratio": 1.8130081300813008, "no_speech_prob": 1.1189398492206237e-06}, {"id": 192, "seek": 118860, "start": 1193.9199999999998, "end": 1199.3999999999999, "text": " epochs of training from random so it probably needs more work. We know that the second last", "tokens": [30992, 28346, 295, 3097, 490, 4974, 370, 309, 1391, 2203, 544, 589, 13, 492, 458, 300, 264, 1150, 1036], "temperature": 0.0, "avg_logprob": -0.12060632155491756, "compression_ratio": 1.8130081300813008, "no_speech_prob": 1.1189398492206237e-06}, {"id": 193, "seek": 118860, "start": 1199.3999999999999, "end": 1205.1999999999998, "text": " layer was probably pretty specialized to image net and less specialized to pet breeds so", "tokens": [4583, 390, 1391, 1238, 19813, 281, 3256, 2533, 293, 1570, 19813, 281, 3817, 41609, 370], "temperature": 0.0, "avg_logprob": -0.12060632155491756, "compression_ratio": 1.8130081300813008, "no_speech_prob": 1.1189398492206237e-06}, {"id": 194, "seek": 118860, "start": 1205.1999999999998, "end": 1210.02, "text": " that probably needs a lot of work. Whereas the early layers the kind of gradients and", "tokens": [300, 1391, 2203, 257, 688, 295, 589, 13, 13813, 264, 2440, 7914, 264, 733, 295, 2771, 2448, 293], "temperature": 0.0, "avg_logprob": -0.12060632155491756, "compression_ratio": 1.8130081300813008, "no_speech_prob": 1.1189398492206237e-06}, {"id": 195, "seek": 118860, "start": 1210.02, "end": 1214.08, "text": " edges probably don't need to be changed much at all. So what we'd really like is to have", "tokens": [8819, 1391, 500, 380, 643, 281, 312, 3105, 709, 412, 439, 13, 407, 437, 321, 1116, 534, 411, 307, 281, 362], "temperature": 0.0, "avg_logprob": -0.12060632155491756, "compression_ratio": 1.8130081300813008, "no_speech_prob": 1.1189398492206237e-06}, {"id": 196, "seek": 121408, "start": 1214.08, "end": 1219.84, "text": " a small learning rate for the early layers and a bigger learning rate for the later layers", "tokens": [257, 1359, 2539, 3314, 337, 264, 2440, 7914, 293, 257, 3801, 2539, 3314, 337, 264, 1780, 7914], "temperature": 0.0, "avg_logprob": -0.102756929397583, "compression_ratio": 1.8319672131147542, "no_speech_prob": 3.256308787058515e-07}, {"id": 197, "seek": 121408, "start": 1219.84, "end": 1224.46, "text": " and this is something that we developed at fast AI and we call it discriminative learning", "tokens": [293, 341, 307, 746, 300, 321, 4743, 412, 2370, 7318, 293, 321, 818, 309, 20828, 1166, 2539], "temperature": 0.0, "avg_logprob": -0.102756929397583, "compression_ratio": 1.8319672131147542, "no_speech_prob": 3.256308787058515e-07}, {"id": 198, "seek": 121408, "start": 1224.46, "end": 1232.98, "text": " rates and Jason Yosinski actually is a guy who wrote a great paper that some of these", "tokens": [6846, 293, 11181, 398, 329, 38984, 767, 307, 257, 2146, 567, 4114, 257, 869, 3035, 300, 512, 295, 613], "temperature": 0.0, "avg_logprob": -0.102756929397583, "compression_ratio": 1.8319672131147542, "no_speech_prob": 3.256308787058515e-07}, {"id": 199, "seek": 121408, "start": 1232.98, "end": 1237.36, "text": " ideas are based on which is he actually showed that different layers of the network really", "tokens": [3487, 366, 2361, 322, 597, 307, 415, 767, 4712, 300, 819, 7914, 295, 264, 3209, 534], "temperature": 0.0, "avg_logprob": -0.102756929397583, "compression_ratio": 1.8319672131147542, "no_speech_prob": 3.256308787058515e-07}, {"id": 200, "seek": 121408, "start": 1237.36, "end": 1242.24, "text": " want to be trained at different rates although he didn't kind of go as far as trying that", "tokens": [528, 281, 312, 8895, 412, 819, 6846, 4878, 415, 994, 380, 733, 295, 352, 382, 1400, 382, 1382, 300], "temperature": 0.0, "avg_logprob": -0.102756929397583, "compression_ratio": 1.8319672131147542, "no_speech_prob": 3.256308787058515e-07}, {"id": 201, "seek": 124224, "start": 1242.24, "end": 1248.2, "text": " out and seeing how it goes it was more of a theoretical thing. So in fast AI if we want", "tokens": [484, 293, 2577, 577, 309, 1709, 309, 390, 544, 295, 257, 20864, 551, 13, 407, 294, 2370, 7318, 498, 321, 528], "temperature": 0.0, "avg_logprob": -0.11672888745318402, "compression_ratio": 1.65, "no_speech_prob": 1.4823541505393223e-06}, {"id": 202, "seek": 124224, "start": 1248.2, "end": 1254.0, "text": " to do that we can pass to our learning rate rather than just passing a single number we", "tokens": [281, 360, 300, 321, 393, 1320, 281, 527, 2539, 3314, 2831, 813, 445, 8437, 257, 2167, 1230, 321], "temperature": 0.0, "avg_logprob": -0.11672888745318402, "compression_ratio": 1.65, "no_speech_prob": 1.4823541505393223e-06}, {"id": 203, "seek": 124224, "start": 1254.0, "end": 1263.1200000000001, "text": " can pass a slice. Now a slice is a special built-in feature of Python it's just an object", "tokens": [393, 1320, 257, 13153, 13, 823, 257, 13153, 307, 257, 2121, 3094, 12, 259, 4111, 295, 15329, 309, 311, 445, 364, 2657], "temperature": 0.0, "avg_logprob": -0.11672888745318402, "compression_ratio": 1.65, "no_speech_prob": 1.4823541505393223e-06}, {"id": 204, "seek": 124224, "start": 1263.1200000000001, "end": 1268.08, "text": " which basically can have a few different numbers in it in this case it's repassing at two numbers", "tokens": [597, 1936, 393, 362, 257, 1326, 819, 3547, 294, 309, 294, 341, 1389, 309, 311, 1085, 42705, 412, 732, 3547], "temperature": 0.0, "avg_logprob": -0.11672888745318402, "compression_ratio": 1.65, "no_speech_prob": 1.4823541505393223e-06}, {"id": 205, "seek": 126808, "start": 1268.08, "end": 1274.1599999999999, "text": " and the way we read those basically what this means in fast AI is a learning rate is the", "tokens": [293, 264, 636, 321, 1401, 729, 1936, 437, 341, 1355, 294, 2370, 7318, 307, 257, 2539, 3314, 307, 264], "temperature": 0.0, "avg_logprob": -0.1086875691133387, "compression_ratio": 1.809278350515464, "no_speech_prob": 8.714318369129614e-07}, {"id": 206, "seek": 126808, "start": 1274.1599999999999, "end": 1280.76, "text": " very first layer will have this learning rate 10 to the minus 6 the very last layer will", "tokens": [588, 700, 4583, 486, 362, 341, 2539, 3314, 1266, 281, 264, 3175, 1386, 264, 588, 1036, 4583, 486], "temperature": 0.0, "avg_logprob": -0.1086875691133387, "compression_ratio": 1.809278350515464, "no_speech_prob": 8.714318369129614e-07}, {"id": 207, "seek": 126808, "start": 1280.76, "end": 1286.8799999999999, "text": " be 10 to the minus 4 and the layers between the two will be kind of equal multiples so", "tokens": [312, 1266, 281, 264, 3175, 1017, 293, 264, 7914, 1296, 264, 732, 486, 312, 733, 295, 2681, 46099, 370], "temperature": 0.0, "avg_logprob": -0.1086875691133387, "compression_ratio": 1.809278350515464, "no_speech_prob": 8.714318369129614e-07}, {"id": 208, "seek": 126808, "start": 1286.8799999999999, "end": 1294.8, "text": " they'll kind of be equally spaced learning rates from the start to the end. So here we", "tokens": [436, 603, 733, 295, 312, 12309, 43766, 2539, 6846, 490, 264, 722, 281, 264, 917, 13, 407, 510, 321], "temperature": 0.0, "avg_logprob": -0.1086875691133387, "compression_ratio": 1.809278350515464, "no_speech_prob": 8.714318369129614e-07}, {"id": 209, "seek": 129480, "start": 1294.8, "end": 1301.84, "text": " can see basically doing our kind of own version of fine-tune we create the learner we fit", "tokens": [393, 536, 1936, 884, 527, 733, 295, 1065, 3037, 295, 2489, 12, 83, 2613, 321, 1884, 264, 33347, 321, 3318], "temperature": 0.0, "avg_logprob": -0.12405694660387541, "compression_ratio": 1.6203703703703705, "no_speech_prob": 9.570807151249028e-07}, {"id": 210, "seek": 129480, "start": 1301.84, "end": 1309.76, "text": " with that automatically frozen version we unfreeze we fit some more and so when we do", "tokens": [365, 300, 6772, 12496, 3037, 321, 3971, 701, 1381, 321, 3318, 512, 544, 293, 370, 562, 321, 360], "temperature": 0.0, "avg_logprob": -0.12405694660387541, "compression_ratio": 1.6203703703703705, "no_speech_prob": 9.570807151249028e-07}, {"id": 211, "seek": 129480, "start": 1309.76, "end": 1317.24, "text": " that you can see this works a lot better we're getting down to 5.3 5.1 5.4 error so that's", "tokens": [300, 291, 393, 536, 341, 1985, 257, 688, 1101, 321, 434, 1242, 760, 281, 1025, 13, 18, 1025, 13, 16, 1025, 13, 19, 6713, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.12405694660387541, "compression_ratio": 1.6203703703703705, "no_speech_prob": 9.570807151249028e-07}, {"id": 212, "seek": 129480, "start": 1317.24, "end": 1322.8799999999999, "text": " pretty great. One thing we'll notice here is that we did kind of overshoot a bit it", "tokens": [1238, 869, 13, 1485, 551, 321, 603, 3449, 510, 307, 300, 321, 630, 733, 295, 15488, 24467, 257, 857, 309], "temperature": 0.0, "avg_logprob": -0.12405694660387541, "compression_ratio": 1.6203703703703705, "no_speech_prob": 9.570807151249028e-07}, {"id": 213, "seek": 132288, "start": 1322.88, "end": 1330.6000000000001, "text": " seemed like more like epoch number 8 was better so kind of back before you know well actually", "tokens": [6576, 411, 544, 411, 30992, 339, 1230, 1649, 390, 1101, 370, 733, 295, 646, 949, 291, 458, 731, 767], "temperature": 0.0, "avg_logprob": -0.11824876815080643, "compression_ratio": 1.6, "no_speech_prob": 1.0348513796998304e-06}, {"id": 214, "seek": 132288, "start": 1330.6000000000001, "end": 1336.5600000000002, "text": " let me explain something about fit one cycle so fit one cycle is a bit different to just", "tokens": [718, 385, 2903, 746, 466, 3318, 472, 6586, 370, 3318, 472, 6586, 307, 257, 857, 819, 281, 445], "temperature": 0.0, "avg_logprob": -0.11824876815080643, "compression_ratio": 1.6, "no_speech_prob": 1.0348513796998304e-06}, {"id": 215, "seek": 132288, "start": 1336.5600000000002, "end": 1346.3600000000001, "text": " fit. So what fit one cycle does is it actually starts at a low learning rate it increases", "tokens": [3318, 13, 407, 437, 3318, 472, 6586, 775, 307, 309, 767, 3719, 412, 257, 2295, 2539, 3314, 309, 8637], "temperature": 0.0, "avg_logprob": -0.11824876815080643, "compression_ratio": 1.6, "no_speech_prob": 1.0348513796998304e-06}, {"id": 216, "seek": 134636, "start": 1346.36, "end": 1353.08, "text": " it gradually for the first one third or so of the batches until it gets to a high learning", "tokens": [309, 13145, 337, 264, 700, 472, 2636, 420, 370, 295, 264, 15245, 279, 1826, 309, 2170, 281, 257, 1090, 2539], "temperature": 0.0, "avg_logprob": -0.13866576781639686, "compression_ratio": 1.9106382978723404, "no_speech_prob": 1.1911057526958757e-06}, {"id": 217, "seek": 134636, "start": 1353.08, "end": 1357.8799999999999, "text": " rate the highest this is why they're called LR max it's the highest learning rate we get", "tokens": [3314, 264, 6343, 341, 307, 983, 436, 434, 1219, 441, 49, 11469, 309, 311, 264, 6343, 2539, 3314, 321, 483], "temperature": 0.0, "avg_logprob": -0.13866576781639686, "compression_ratio": 1.9106382978723404, "no_speech_prob": 1.1911057526958757e-06}, {"id": 218, "seek": 134636, "start": 1357.8799999999999, "end": 1364.3999999999999, "text": " to and then for the remaining two thirds or so of the batches it gradually decreases the", "tokens": [281, 293, 550, 337, 264, 8877, 732, 34552, 420, 370, 295, 264, 15245, 279, 309, 13145, 24108, 264], "temperature": 0.0, "avg_logprob": -0.13866576781639686, "compression_ratio": 1.9106382978723404, "no_speech_prob": 1.1911057526958757e-06}, {"id": 219, "seek": 134636, "start": 1364.3999999999999, "end": 1370.08, "text": " learning rate and the reason for that is just that well actually it's kind of like empirically", "tokens": [2539, 3314, 293, 264, 1778, 337, 300, 307, 445, 300, 731, 767, 309, 311, 733, 295, 411, 25790, 984], "temperature": 0.0, "avg_logprob": -0.13866576781639686, "compression_ratio": 1.9106382978723404, "no_speech_prob": 1.1911057526958757e-06}, {"id": 220, "seek": 134636, "start": 1370.08, "end": 1374.02, "text": " researchers have found that works the best in fact this was developed again by Leslie", "tokens": [10309, 362, 1352, 300, 1985, 264, 1151, 294, 1186, 341, 390, 4743, 797, 538, 28140], "temperature": 0.0, "avg_logprob": -0.13866576781639686, "compression_ratio": 1.9106382978723404, "no_speech_prob": 1.1911057526958757e-06}, {"id": 221, "seek": 137402, "start": 1374.02, "end": 1380.24, "text": " Smith the same guy that did the learning rate finder again it was a huge step you know it", "tokens": [8538, 264, 912, 2146, 300, 630, 264, 2539, 3314, 915, 260, 797, 309, 390, 257, 2603, 1823, 291, 458, 309], "temperature": 0.0, "avg_logprob": -0.10227644443511963, "compression_ratio": 1.7580645161290323, "no_speech_prob": 8.714318937563803e-07}, {"id": 222, "seek": 137402, "start": 1380.24, "end": 1384.48, "text": " really dramatically accelerated the speed at which we can train neural networks and", "tokens": [534, 17548, 29763, 264, 3073, 412, 597, 321, 393, 3847, 18161, 9590, 293], "temperature": 0.0, "avg_logprob": -0.10227644443511963, "compression_ratio": 1.7580645161290323, "no_speech_prob": 8.714318937563803e-07}, {"id": 223, "seek": 137402, "start": 1384.48, "end": 1389.6399999999999, "text": " also made them much more accurate and again the academic community basically ignored it", "tokens": [611, 1027, 552, 709, 544, 8559, 293, 797, 264, 7778, 1768, 1936, 19735, 309], "temperature": 0.0, "avg_logprob": -0.10227644443511963, "compression_ratio": 1.7580645161290323, "no_speech_prob": 8.714318937563803e-07}, {"id": 224, "seek": 137402, "start": 1389.6399999999999, "end": 1396.32, "text": " in fact the key publication that developed this idea was not even did not even pass peer", "tokens": [294, 1186, 264, 2141, 19953, 300, 4743, 341, 1558, 390, 406, 754, 630, 406, 754, 1320, 15108], "temperature": 0.0, "avg_logprob": -0.10227644443511963, "compression_ratio": 1.7580645161290323, "no_speech_prob": 8.714318937563803e-07}, {"id": 225, "seek": 137402, "start": 1396.32, "end": 1403.32, "text": " review and so the reason I mentioned this now is to say that we can't we don't really", "tokens": [3131, 293, 370, 264, 1778, 286, 2835, 341, 586, 307, 281, 584, 300, 321, 393, 380, 321, 500, 380, 534], "temperature": 0.0, "avg_logprob": -0.10227644443511963, "compression_ratio": 1.7580645161290323, "no_speech_prob": 8.714318937563803e-07}, {"id": 226, "seek": 140332, "start": 1403.32, "end": 1408.6399999999999, "text": " just want to go back and pick the model that was trained back here because we could probably", "tokens": [445, 528, 281, 352, 646, 293, 1888, 264, 2316, 300, 390, 8895, 646, 510, 570, 321, 727, 1391], "temperature": 0.0, "avg_logprob": -0.11519324211847215, "compression_ratio": 1.748768472906404, "no_speech_prob": 5.368737561184389e-07}, {"id": 227, "seek": 140332, "start": 1408.6399999999999, "end": 1413.8, "text": " do better because we really want to pick a model that's got a low learning rate so what", "tokens": [360, 1101, 570, 321, 534, 528, 281, 1888, 257, 2316, 300, 311, 658, 257, 2295, 2539, 3314, 370, 437], "temperature": 0.0, "avg_logprob": -0.11519324211847215, "compression_ratio": 1.748768472906404, "no_speech_prob": 5.368737561184389e-07}, {"id": 228, "seek": 140332, "start": 1413.8, "end": 1419.56, "text": " I would generally do here is I change this 12 to an 8 because this is this is looking", "tokens": [286, 576, 5101, 360, 510, 307, 286, 1319, 341, 2272, 281, 364, 1649, 570, 341, 307, 341, 307, 1237], "temperature": 0.0, "avg_logprob": -0.11519324211847215, "compression_ratio": 1.748768472906404, "no_speech_prob": 5.368737561184389e-07}, {"id": 229, "seek": 140332, "start": 1419.56, "end": 1427.6, "text": " good and then I would re train it from scratch I normally would find a better result you", "tokens": [665, 293, 550, 286, 576, 319, 3847, 309, 490, 8459, 286, 5646, 576, 915, 257, 1101, 1874, 291], "temperature": 0.0, "avg_logprob": -0.11519324211847215, "compression_ratio": 1.748768472906404, "no_speech_prob": 5.368737561184389e-07}, {"id": 230, "seek": 142760, "start": 1427.6, "end": 1433.6799999999998, "text": " can plot the loss and you can see how the training and validation loss moved along and", "tokens": [393, 7542, 264, 4470, 293, 291, 393, 536, 577, 264, 3097, 293, 24071, 4470, 4259, 2051, 293], "temperature": 0.0, "avg_logprob": -0.08426807040259951, "compression_ratio": 2.022346368715084, "no_speech_prob": 9.874592024061712e-07}, {"id": 231, "seek": 142760, "start": 1433.6799999999998, "end": 1442.8799999999999, "text": " you can see here that you know the the error rate was starting to get worse here and what", "tokens": [291, 393, 536, 510, 300, 291, 458, 264, 264, 6713, 3314, 390, 2891, 281, 483, 5324, 510, 293, 437], "temperature": 0.0, "avg_logprob": -0.08426807040259951, "compression_ratio": 2.022346368715084, "no_speech_prob": 9.874592024061712e-07}, {"id": 232, "seek": 142760, "start": 1442.8799999999999, "end": 1451.7199999999998, "text": " you'll often see is often the validation loss will get worse a bit before the error rate", "tokens": [291, 603, 2049, 536, 307, 2049, 264, 24071, 4470, 486, 483, 5324, 257, 857, 949, 264, 6713, 3314], "temperature": 0.0, "avg_logprob": -0.08426807040259951, "compression_ratio": 2.022346368715084, "no_speech_prob": 9.874592024061712e-07}, {"id": 233, "seek": 142760, "start": 1451.7199999999998, "end": 1455.3999999999999, "text": " gets worse we're not really seeing it so much in this case but the error rate and the validation", "tokens": [2170, 5324, 321, 434, 406, 534, 2577, 309, 370, 709, 294, 341, 1389, 457, 264, 6713, 3314, 293, 264, 24071], "temperature": 0.0, "avg_logprob": -0.08426807040259951, "compression_ratio": 2.022346368715084, "no_speech_prob": 9.874592024061712e-07}, {"id": 234, "seek": 145540, "start": 1455.4, "end": 1462.3600000000001, "text": " loss don't always or they're not always kind of in lockstep so what we're plotting here", "tokens": [4470, 500, 380, 1009, 420, 436, 434, 406, 1009, 733, 295, 294, 4017, 16792, 370, 437, 321, 434, 41178, 510], "temperature": 0.0, "avg_logprob": -0.07091762401439526, "compression_ratio": 1.766497461928934, "no_speech_prob": 9.422417974747077e-07}, {"id": 235, "seek": 145540, "start": 1462.3600000000001, "end": 1465.44, "text": " is the loss but you actually kind of want to look to see mainly what's happening with", "tokens": [307, 264, 4470, 457, 291, 767, 733, 295, 528, 281, 574, 281, 536, 8704, 437, 311, 2737, 365], "temperature": 0.0, "avg_logprob": -0.07091762401439526, "compression_ratio": 1.766497461928934, "no_speech_prob": 9.422417974747077e-07}, {"id": 236, "seek": 145540, "start": 1465.44, "end": 1469.3200000000002, "text": " the error rate because that's actually the thing we care about remember the loss is just", "tokens": [264, 6713, 3314, 570, 300, 311, 767, 264, 551, 321, 1127, 466, 1604, 264, 4470, 307, 445], "temperature": 0.0, "avg_logprob": -0.07091762401439526, "compression_ratio": 1.766497461928934, "no_speech_prob": 9.422417974747077e-07}, {"id": 237, "seek": 145540, "start": 1469.3200000000002, "end": 1475.64, "text": " like an approximation of what we care about that just happens to have a gradient that", "tokens": [411, 364, 28023, 295, 437, 321, 1127, 466, 300, 445, 2314, 281, 362, 257, 16235, 300], "temperature": 0.0, "avg_logprob": -0.07091762401439526, "compression_ratio": 1.766497461928934, "no_speech_prob": 9.422417974747077e-07}, {"id": 238, "seek": 147564, "start": 1475.64, "end": 1488.72, "text": " works out nicely so how do you make it better now we're already down to just 5.4 or if we'd", "tokens": [1985, 484, 9594, 370, 577, 360, 291, 652, 309, 1101, 586, 321, 434, 1217, 760, 281, 445, 1025, 13, 19, 420, 498, 321, 1116], "temperature": 0.0, "avg_logprob": -0.09131966174488336, "compression_ratio": 1.4782608695652173, "no_speech_prob": 7.811463547113817e-07}, {"id": 239, "seek": 147564, "start": 1488.72, "end": 1494.88, "text": " stopped a bit earlier maybe we could get down to 5.1 or less error on 37 categories that's", "tokens": [5936, 257, 857, 3071, 1310, 321, 727, 483, 760, 281, 1025, 13, 16, 420, 1570, 6713, 322, 13435, 10479, 300, 311], "temperature": 0.0, "avg_logprob": -0.09131966174488336, "compression_ratio": 1.4782608695652173, "no_speech_prob": 7.811463547113817e-07}, {"id": 240, "seek": 147564, "start": 1494.88, "end": 1501.5200000000002, "text": " pretty remarkable that's a very very good pet breed predictor if you want to do something", "tokens": [1238, 12802, 300, 311, 257, 588, 588, 665, 3817, 18971, 6069, 284, 498, 291, 528, 281, 360, 746], "temperature": 0.0, "avg_logprob": -0.09131966174488336, "compression_ratio": 1.4782608695652173, "no_speech_prob": 7.811463547113817e-07}, {"id": 241, "seek": 150152, "start": 1501.52, "end": 1506.44, "text": " even better you could try creating a deeper architecture so a deeper architecture is just", "tokens": [754, 1101, 291, 727, 853, 4084, 257, 7731, 9482, 370, 257, 7731, 9482, 307, 445], "temperature": 0.0, "avg_logprob": -0.12181724646152595, "compression_ratio": 1.78, "no_speech_prob": 1.0676990314095747e-06}, {"id": 242, "seek": 150152, "start": 1506.44, "end": 1515.16, "text": " literally putting more pairs of non-linear activation function also known as a non-linearity", "tokens": [3736, 3372, 544, 15494, 295, 2107, 12, 28263, 24433, 2445, 611, 2570, 382, 257, 2107, 12, 1889, 17409], "temperature": 0.0, "avg_logprob": -0.12181724646152595, "compression_ratio": 1.78, "no_speech_prob": 1.0676990314095747e-06}, {"id": 243, "seek": 150152, "start": 1515.16, "end": 1519.56, "text": " followed by these little linear models put more pairs onto the end and that basically", "tokens": [6263, 538, 613, 707, 8213, 5245, 829, 544, 15494, 3911, 264, 917, 293, 300, 1936], "temperature": 0.0, "avg_logprob": -0.12181724646152595, "compression_ratio": 1.78, "no_speech_prob": 1.0676990314095747e-06}, {"id": 244, "seek": 150152, "start": 1519.56, "end": 1525.96, "text": " the number of these sets of layers you have is the number that you'll see at the end of", "tokens": [264, 1230, 295, 613, 6352, 295, 7914, 291, 362, 307, 264, 1230, 300, 291, 603, 536, 412, 264, 917, 295], "temperature": 0.0, "avg_logprob": -0.12181724646152595, "compression_ratio": 1.78, "no_speech_prob": 1.0676990314095747e-06}, {"id": 245, "seek": 152596, "start": 1525.96, "end": 1533.96, "text": " an architecture so there's ResNet 18, ResNet 34, ResNet 50, so forth.", "tokens": [364, 9482, 370, 456, 311, 5015, 31890, 2443, 11, 5015, 31890, 12790, 11, 5015, 31890, 2625, 11, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.13265983954719876, "compression_ratio": 1.5913461538461537, "no_speech_prob": 7.81146638928476e-07}, {"id": 246, "seek": 152596, "start": 1533.96, "end": 1542.24, "text": " Having said that you can't really pick ResNet 19 or ResNet 38 I mean you can make one but", "tokens": [10222, 848, 300, 291, 393, 380, 534, 1888, 5015, 31890, 1294, 420, 5015, 31890, 12843, 286, 914, 291, 393, 652, 472, 457], "temperature": 0.0, "avg_logprob": -0.13265983954719876, "compression_ratio": 1.5913461538461537, "no_speech_prob": 7.81146638928476e-07}, {"id": 247, "seek": 152596, "start": 1542.24, "end": 1547.68, "text": " nobody's created a pre-trained version of that for you so you won't be able to do any", "tokens": [5079, 311, 2942, 257, 659, 12, 17227, 2001, 3037, 295, 300, 337, 291, 370, 291, 1582, 380, 312, 1075, 281, 360, 604], "temperature": 0.0, "avg_logprob": -0.13265983954719876, "compression_ratio": 1.5913461538461537, "no_speech_prob": 7.81146638928476e-07}, {"id": 248, "seek": 152596, "start": 1547.68, "end": 1554.28, "text": " fine-tuning so like you can theoretically create any number of layers you like but in", "tokens": [2489, 12, 83, 37726, 370, 411, 291, 393, 29400, 1884, 604, 1230, 295, 7914, 291, 411, 457, 294], "temperature": 0.0, "avg_logprob": -0.13265983954719876, "compression_ratio": 1.5913461538461537, "no_speech_prob": 7.81146638928476e-07}, {"id": 249, "seek": 155428, "start": 1554.28, "end": 1560.52, "text": " practice most of the time you'll want to pick a model that has a pre-trained version so", "tokens": [3124, 881, 295, 264, 565, 291, 603, 528, 281, 1888, 257, 2316, 300, 575, 257, 659, 12, 17227, 2001, 3037, 370], "temperature": 0.0, "avg_logprob": -0.07953543846423809, "compression_ratio": 1.71484375, "no_speech_prob": 1.11894053134165e-06}, {"id": 250, "seek": 155428, "start": 1560.52, "end": 1564.48, "text": " you kind of have to select from the sizes people have pre-trained and there's nothing", "tokens": [291, 733, 295, 362, 281, 3048, 490, 264, 11602, 561, 362, 659, 12, 17227, 2001, 293, 456, 311, 1825], "temperature": 0.0, "avg_logprob": -0.07953543846423809, "compression_ratio": 1.71484375, "no_speech_prob": 1.11894053134165e-06}, {"id": 251, "seek": 155428, "start": 1564.48, "end": 1570.08, "text": " special about these sizes they're just ones that people happen to have picked out.", "tokens": [2121, 466, 613, 11602, 436, 434, 445, 2306, 300, 561, 1051, 281, 362, 6183, 484, 13], "temperature": 0.0, "avg_logprob": -0.07953543846423809, "compression_ratio": 1.71484375, "no_speech_prob": 1.11894053134165e-06}, {"id": 252, "seek": 155428, "start": 1570.08, "end": 1576.76, "text": " For the bigger models there's more parameters and more gradients that are going to be stored", "tokens": [1171, 264, 3801, 5245, 456, 311, 544, 9834, 293, 544, 2771, 2448, 300, 366, 516, 281, 312, 12187], "temperature": 0.0, "avg_logprob": -0.07953543846423809, "compression_ratio": 1.71484375, "no_speech_prob": 1.11894053134165e-06}, {"id": 253, "seek": 155428, "start": 1576.76, "end": 1583.68, "text": " on your GPU and you will get used to the idea of seeing this this error unfortunately out", "tokens": [322, 428, 18407, 293, 291, 486, 483, 1143, 281, 264, 1558, 295, 2577, 341, 341, 6713, 7015, 484], "temperature": 0.0, "avg_logprob": -0.07953543846423809, "compression_ratio": 1.71484375, "no_speech_prob": 1.11894053134165e-06}, {"id": 254, "seek": 158368, "start": 1583.68, "end": 1589.2, "text": " of memory so that's not out of memory in your RAM that's out of memory in your GPU.", "tokens": [295, 4675, 370, 300, 311, 406, 484, 295, 4675, 294, 428, 14561, 300, 311, 484, 295, 4675, 294, 428, 18407, 13], "temperature": 0.0, "avg_logprob": -0.18396201936325224, "compression_ratio": 1.7885462555066078, "no_speech_prob": 1.0511481605135486e-06}, {"id": 255, "seek": 158368, "start": 1589.2, "end": 1594.5600000000002, "text": " Udr is referring to the language the system used for your GPU.", "tokens": [624, 67, 81, 307, 13761, 281, 264, 2856, 264, 1185, 1143, 337, 428, 18407, 13], "temperature": 0.0, "avg_logprob": -0.18396201936325224, "compression_ratio": 1.7885462555066078, "no_speech_prob": 1.0511481605135486e-06}, {"id": 256, "seek": 158368, "start": 1594.5600000000002, "end": 1600.68, "text": " So if that happens unfortunately you actually have to restart your notebook so that's kernel", "tokens": [407, 498, 300, 2314, 7015, 291, 767, 362, 281, 21022, 428, 21060, 370, 300, 311, 28256], "temperature": 0.0, "avg_logprob": -0.18396201936325224, "compression_ratio": 1.7885462555066078, "no_speech_prob": 1.0511481605135486e-06}, {"id": 257, "seek": 158368, "start": 1600.68, "end": 1608.1000000000001, "text": " restart and try again and that's a really annoying thing but that's just life.", "tokens": [21022, 293, 853, 797, 293, 300, 311, 257, 534, 11304, 551, 457, 300, 311, 445, 993, 13], "temperature": 0.0, "avg_logprob": -0.18396201936325224, "compression_ratio": 1.7885462555066078, "no_speech_prob": 1.0511481605135486e-06}, {"id": 258, "seek": 158368, "start": 1608.1000000000001, "end": 1612.3400000000001, "text": " One thing you can do if you get an out of memory error is after you've your CNN learner", "tokens": [1485, 551, 291, 393, 360, 498, 291, 483, 364, 484, 295, 4675, 6713, 307, 934, 291, 600, 428, 24859, 33347], "temperature": 0.0, "avg_logprob": -0.18396201936325224, "compression_ratio": 1.7885462555066078, "no_speech_prob": 1.0511481605135486e-06}, {"id": 259, "seek": 161234, "start": 1612.34, "end": 1617.4599999999998, "text": " call add this magic incantation to FP16.", "tokens": [818, 909, 341, 5585, 834, 394, 399, 281, 36655, 6866, 13], "temperature": 0.0, "avg_logprob": -0.14685432999222367, "compression_ratio": 1.4266666666666667, "no_speech_prob": 6.786720518903167e-07}, {"id": 260, "seek": 161234, "start": 1617.4599999999998, "end": 1624.26, "text": " What that does is it uses for most of the operations numbers that use half as many bits", "tokens": [708, 300, 775, 307, 309, 4960, 337, 881, 295, 264, 7705, 3547, 300, 764, 1922, 382, 867, 9239], "temperature": 0.0, "avg_logprob": -0.14685432999222367, "compression_ratio": 1.4266666666666667, "no_speech_prob": 6.786720518903167e-07}, {"id": 261, "seek": 161234, "start": 1624.26, "end": 1633.12, "text": " as usual so they're less accurate this half precision floating point or FP16 and that", "tokens": [382, 7713, 370, 436, 434, 1570, 8559, 341, 1922, 18356, 12607, 935, 420, 36655, 6866, 293, 300], "temperature": 0.0, "avg_logprob": -0.14685432999222367, "compression_ratio": 1.4266666666666667, "no_speech_prob": 6.786720518903167e-07}, {"id": 262, "seek": 163312, "start": 1633.12, "end": 1643.52, "text": " will use less memory and on pretty much any Nvidia card created in 2020 or later and some", "tokens": [486, 764, 1570, 4675, 293, 322, 1238, 709, 604, 46284, 2920, 2942, 294, 4808, 420, 1780, 293, 512], "temperature": 0.0, "avg_logprob": -0.10115584135055541, "compression_ratio": 1.5165876777251184, "no_speech_prob": 6.276696353779698e-07}, {"id": 263, "seek": 163312, "start": 1643.52, "end": 1648.6, "text": " more expensive cards even created in 2019 that's often going to result in a two to three", "tokens": [544, 5124, 5632, 754, 2942, 294, 6071, 300, 311, 2049, 516, 281, 1874, 294, 257, 732, 281, 1045], "temperature": 0.0, "avg_logprob": -0.10115584135055541, "compression_ratio": 1.5165876777251184, "no_speech_prob": 6.276696353779698e-07}, {"id": 264, "seek": 163312, "start": 1648.6, "end": 1653.04, "text": " times speed up in terms of how long it takes as well.", "tokens": [1413, 3073, 493, 294, 2115, 295, 577, 938, 309, 2516, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.10115584135055541, "compression_ratio": 1.5165876777251184, "no_speech_prob": 6.276696353779698e-07}, {"id": 265, "seek": 163312, "start": 1653.04, "end": 1661.82, "text": " So here if I add in to FP16 and I will be seeing often much faster training and in this", "tokens": [407, 510, 498, 286, 909, 294, 281, 36655, 6866, 293, 286, 486, 312, 2577, 2049, 709, 4663, 3097, 293, 294, 341], "temperature": 0.0, "avg_logprob": -0.10115584135055541, "compression_ratio": 1.5165876777251184, "no_speech_prob": 6.276696353779698e-07}, {"id": 266, "seek": 166182, "start": 1661.82, "end": 1665.6799999999998, "text": " case what I actually did is I switched to a ResNet 50 which would normally take about", "tokens": [1389, 437, 286, 767, 630, 307, 286, 16858, 281, 257, 5015, 31890, 2625, 597, 576, 5646, 747, 466], "temperature": 0.0, "avg_logprob": -0.1458870329946842, "compression_ratio": 1.5588235294117647, "no_speech_prob": 1.191104388453823e-06}, {"id": 267, "seek": 166182, "start": 1665.6799999999998, "end": 1674.0, "text": " twice as long and my per epoch time has gone from 25 seconds to 26 seconds.", "tokens": [6091, 382, 938, 293, 452, 680, 30992, 339, 565, 575, 2780, 490, 3552, 3949, 281, 7551, 3949, 13], "temperature": 0.0, "avg_logprob": -0.1458870329946842, "compression_ratio": 1.5588235294117647, "no_speech_prob": 1.191104388453823e-06}, {"id": 268, "seek": 166182, "start": 1674.0, "end": 1680.24, "text": " So the fact that we used a much bigger network and it was no slower is thanks to 2FP16 but", "tokens": [407, 264, 1186, 300, 321, 1143, 257, 709, 3801, 3209, 293, 309, 390, 572, 14009, 307, 3231, 281, 568, 45882, 6866, 457], "temperature": 0.0, "avg_logprob": -0.1458870329946842, "compression_ratio": 1.5588235294117647, "no_speech_prob": 1.191104388453823e-06}, {"id": 269, "seek": 166182, "start": 1680.24, "end": 1685.9199999999998, "text": " you'll see our error rate hasn't improved it's pretty similar to what it was and so", "tokens": [291, 603, 536, 527, 6713, 3314, 6132, 380, 9689, 309, 311, 1238, 2531, 281, 437, 309, 390, 293, 370], "temperature": 0.0, "avg_logprob": -0.1458870329946842, "compression_ratio": 1.5588235294117647, "no_speech_prob": 1.191104388453823e-06}, {"id": 270, "seek": 166182, "start": 1685.9199999999998, "end": 1690.56, "text": " it's important to realize that just because we increase the number of layers it doesn't", "tokens": [309, 311, 1021, 281, 4325, 300, 445, 570, 321, 3488, 264, 1230, 295, 7914, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.1458870329946842, "compression_ratio": 1.5588235294117647, "no_speech_prob": 1.191104388453823e-06}, {"id": 271, "seek": 169056, "start": 1690.56, "end": 1692.62, "text": " always get better.", "tokens": [1009, 483, 1101, 13], "temperature": 0.0, "avg_logprob": -0.13004075218649472, "compression_ratio": 1.6346153846153846, "no_speech_prob": 3.785271019296488e-06}, {"id": 272, "seek": 169056, "start": 1692.62, "end": 1698.56, "text": " So it tends to require a bit of experimentation to find what's going to work for you and of", "tokens": [407, 309, 12258, 281, 3651, 257, 857, 295, 37142, 281, 915, 437, 311, 516, 281, 589, 337, 291, 293, 295], "temperature": 0.0, "avg_logprob": -0.13004075218649472, "compression_ratio": 1.6346153846153846, "no_speech_prob": 3.785271019296488e-06}, {"id": 273, "seek": 169056, "start": 1698.56, "end": 1705.24, "text": " course don't forget the trick is use small models for as long as possible to do all of", "tokens": [1164, 500, 380, 2870, 264, 4282, 307, 764, 1359, 5245, 337, 382, 938, 382, 1944, 281, 360, 439, 295], "temperature": 0.0, "avg_logprob": -0.13004075218649472, "compression_ratio": 1.6346153846153846, "no_speech_prob": 3.785271019296488e-06}, {"id": 274, "seek": 169056, "start": 1705.24, "end": 1710.2, "text": " your cleaning up and testing and so forth and wait until you're all done to try some", "tokens": [428, 8924, 493, 293, 4997, 293, 370, 5220, 293, 1699, 1826, 291, 434, 439, 1096, 281, 853, 512], "temperature": 0.0, "avg_logprob": -0.13004075218649472, "compression_ratio": 1.6346153846153846, "no_speech_prob": 3.785271019296488e-06}, {"id": 275, "seek": 169056, "start": 1710.2, "end": 1715.12, "text": " bigger models because they're going to take a lot longer.", "tokens": [3801, 5245, 570, 436, 434, 516, 281, 747, 257, 688, 2854, 13], "temperature": 0.0, "avg_logprob": -0.13004075218649472, "compression_ratio": 1.6346153846153846, "no_speech_prob": 3.785271019296488e-06}, {"id": 276, "seek": 171512, "start": 1715.12, "end": 1725.2399999999998, "text": " A question Jeremy, how do you know or suspect when you can quote do better?", "tokens": [316, 1168, 17809, 11, 577, 360, 291, 458, 420, 9091, 562, 291, 393, 6513, 360, 1101, 30], "temperature": 0.0, "avg_logprob": -0.17461049674761178, "compression_ratio": 1.6694560669456067, "no_speech_prob": 9.36860578804044e-06}, {"id": 277, "seek": 171512, "start": 1725.2399999999998, "end": 1729.78, "text": " You have to always assume you can do better because you never know.", "tokens": [509, 362, 281, 1009, 6552, 291, 393, 360, 1101, 570, 291, 1128, 458, 13], "temperature": 0.0, "avg_logprob": -0.17461049674761178, "compression_ratio": 1.6694560669456067, "no_speech_prob": 9.36860578804044e-06}, {"id": 278, "seek": 171512, "start": 1729.78, "end": 1734.28, "text": " So you just have to I mean part of it though is do you need to do better?", "tokens": [407, 291, 445, 362, 281, 286, 914, 644, 295, 309, 1673, 307, 360, 291, 643, 281, 360, 1101, 30], "temperature": 0.0, "avg_logprob": -0.17461049674761178, "compression_ratio": 1.6694560669456067, "no_speech_prob": 9.36860578804044e-06}, {"id": 279, "seek": 171512, "start": 1734.28, "end": 1739.36, "text": " So do you already have a good enough result to handle the actual task you're trying to", "tokens": [407, 360, 291, 1217, 362, 257, 665, 1547, 1874, 281, 4813, 264, 3539, 5633, 291, 434, 1382, 281], "temperature": 0.0, "avg_logprob": -0.17461049674761178, "compression_ratio": 1.6694560669456067, "no_speech_prob": 9.36860578804044e-06}, {"id": 280, "seek": 171512, "start": 1739.36, "end": 1740.52, "text": " do?", "tokens": [360, 30], "temperature": 0.0, "avg_logprob": -0.17461049674761178, "compression_ratio": 1.6694560669456067, "no_speech_prob": 9.36860578804044e-06}, {"id": 281, "seek": 171512, "start": 1740.52, "end": 1744.4799999999998, "text": " Often people do spend too much time fiddling around with their models rather than actually", "tokens": [20043, 561, 360, 3496, 886, 709, 565, 283, 14273, 1688, 926, 365, 641, 5245, 2831, 813, 767], "temperature": 0.0, "avg_logprob": -0.17461049674761178, "compression_ratio": 1.6694560669456067, "no_speech_prob": 9.36860578804044e-06}, {"id": 282, "seek": 174448, "start": 1744.48, "end": 1748.4, "text": " trying to see whether it's already going to be super helpful.", "tokens": [1382, 281, 536, 1968, 309, 311, 1217, 516, 281, 312, 1687, 4961, 13], "temperature": 0.0, "avg_logprob": -0.19748105173525604, "compression_ratio": 1.609442060085837, "no_speech_prob": 5.255375072010793e-06}, {"id": 283, "seek": 174448, "start": 1748.4, "end": 1755.52, "text": " So as soon as you can actually try to use your model to do something practical the better.", "tokens": [407, 382, 2321, 382, 291, 393, 767, 853, 281, 764, 428, 2316, 281, 360, 746, 8496, 264, 1101, 13], "temperature": 0.0, "avg_logprob": -0.19748105173525604, "compression_ratio": 1.609442060085837, "no_speech_prob": 5.255375072010793e-06}, {"id": 284, "seek": 174448, "start": 1755.52, "end": 1757.6, "text": " But yeah how much can you improve it?", "tokens": [583, 1338, 577, 709, 393, 291, 3470, 309, 30], "temperature": 0.0, "avg_logprob": -0.19748105173525604, "compression_ratio": 1.609442060085837, "no_speech_prob": 5.255375072010793e-06}, {"id": 285, "seek": 174448, "start": 1757.6, "end": 1760.6, "text": " Who knows?", "tokens": [2102, 3255, 30], "temperature": 0.0, "avg_logprob": -0.19748105173525604, "compression_ratio": 1.609442060085837, "no_speech_prob": 5.255375072010793e-06}, {"id": 286, "seek": 174448, "start": 1760.6, "end": 1763.56, "text": " I you know go through the techniques that we're teaching in this course and try them", "tokens": [286, 291, 458, 352, 807, 264, 7512, 300, 321, 434, 4571, 294, 341, 1164, 293, 853, 552], "temperature": 0.0, "avg_logprob": -0.19748105173525604, "compression_ratio": 1.609442060085837, "no_speech_prob": 5.255375072010793e-06}, {"id": 287, "seek": 174448, "start": 1763.56, "end": 1770.84, "text": " and see which ones help and unless it's a problem that somebody has already tried before", "tokens": [293, 536, 597, 2306, 854, 293, 5969, 309, 311, 257, 1154, 300, 2618, 575, 1217, 3031, 949], "temperature": 0.0, "avg_logprob": -0.19748105173525604, "compression_ratio": 1.609442060085837, "no_speech_prob": 5.255375072010793e-06}, {"id": 288, "seek": 177084, "start": 1770.84, "end": 1774.84, "text": " and written down their results in a paper or a Kaggle competition or something there's", "tokens": [293, 3720, 760, 641, 3542, 294, 257, 3035, 420, 257, 48751, 22631, 6211, 420, 746, 456, 311], "temperature": 0.0, "avg_logprob": -0.1378462791442871, "compression_ratio": 1.662037037037037, "no_speech_prob": 7.112402613529412e-07}, {"id": 289, "seek": 177084, "start": 1774.84, "end": 1781.04, "text": " no way to know how good can.", "tokens": [572, 636, 281, 458, 577, 665, 393, 13], "temperature": 0.0, "avg_logprob": -0.1378462791442871, "compression_ratio": 1.662037037037037, "no_speech_prob": 7.112402613529412e-07}, {"id": 290, "seek": 177084, "start": 1781.04, "end": 1787.84, "text": " So don't forget after you do the questionnaire to check out the further research section", "tokens": [407, 500, 380, 2870, 934, 291, 360, 264, 44702, 281, 1520, 484, 264, 3052, 2132, 3541], "temperature": 0.0, "avg_logprob": -0.1378462791442871, "compression_ratio": 1.662037037037037, "no_speech_prob": 7.112402613529412e-07}, {"id": 291, "seek": 177084, "start": 1787.84, "end": 1791.3799999999999, "text": " and one of the things we've asked you to do here is to read a paper.", "tokens": [293, 472, 295, 264, 721, 321, 600, 2351, 291, 281, 360, 510, 307, 281, 1401, 257, 3035, 13], "temperature": 0.0, "avg_logprob": -0.1378462791442871, "compression_ratio": 1.662037037037037, "no_speech_prob": 7.112402613529412e-07}, {"id": 292, "seek": 177084, "start": 1791.3799999999999, "end": 1797.6399999999999, "text": " So find the learning rate find a paper and read it and see if you can kind of connect", "tokens": [407, 915, 264, 2539, 3314, 915, 257, 3035, 293, 1401, 309, 293, 536, 498, 291, 393, 733, 295, 1745], "temperature": 0.0, "avg_logprob": -0.1378462791442871, "compression_ratio": 1.662037037037037, "no_speech_prob": 7.112402613529412e-07}, {"id": 293, "seek": 179764, "start": 1797.64, "end": 1803.3200000000002, "text": " what you read up to the things that we've learned in this lesson and see if you can", "tokens": [437, 291, 1401, 493, 281, 264, 721, 300, 321, 600, 3264, 294, 341, 6898, 293, 536, 498, 291, 393], "temperature": 0.0, "avg_logprob": -0.15670945472324016, "compression_ratio": 1.8055555555555556, "no_speech_prob": 1.3709512813875335e-06}, {"id": 294, "seek": 179764, "start": 1803.3200000000002, "end": 1811.0, "text": " maybe even implement your own learning rate finder you know as manually as you need to", "tokens": [1310, 754, 4445, 428, 1065, 2539, 3314, 915, 260, 291, 458, 382, 16945, 382, 291, 643, 281], "temperature": 0.0, "avg_logprob": -0.15670945472324016, "compression_ratio": 1.8055555555555556, "no_speech_prob": 1.3709512813875335e-06}, {"id": 295, "seek": 179764, "start": 1811.0, "end": 1815.26, "text": " and see if you can get something that you know based on reading the paper get to work", "tokens": [293, 536, 498, 291, 393, 483, 746, 300, 291, 458, 2361, 322, 3760, 264, 3035, 483, 281, 589], "temperature": 0.0, "avg_logprob": -0.15670945472324016, "compression_ratio": 1.8055555555555556, "no_speech_prob": 1.3709512813875335e-06}, {"id": 296, "seek": 179764, "start": 1815.26, "end": 1816.98, "text": " yourself.", "tokens": [1803, 13], "temperature": 0.0, "avg_logprob": -0.15670945472324016, "compression_ratio": 1.8055555555555556, "no_speech_prob": 1.3709512813875335e-06}, {"id": 297, "seek": 179764, "start": 1816.98, "end": 1823.5600000000002, "text": " You can even look at the source code of fastai learning rate finder of course and then can", "tokens": [509, 393, 754, 574, 412, 264, 4009, 3089, 295, 2370, 1301, 2539, 3314, 915, 260, 295, 1164, 293, 550, 393], "temperature": 0.0, "avg_logprob": -0.15670945472324016, "compression_ratio": 1.8055555555555556, "no_speech_prob": 1.3709512813875335e-06}, {"id": 298, "seek": 179764, "start": 1823.5600000000002, "end": 1826.1200000000001, "text": " you make this classifier better?", "tokens": [291, 652, 341, 1508, 9902, 1101, 30], "temperature": 0.0, "avg_logprob": -0.15670945472324016, "compression_ratio": 1.8055555555555556, "no_speech_prob": 1.3709512813875335e-06}, {"id": 299, "seek": 182612, "start": 1826.12, "end": 1829.78, "text": " And so this is further research right so maybe you can start doing some reading to see what", "tokens": [400, 370, 341, 307, 3052, 2132, 558, 370, 1310, 291, 393, 722, 884, 512, 3760, 281, 536, 437], "temperature": 0.0, "avg_logprob": -0.1273604754743905, "compression_ratio": 1.7190476190476192, "no_speech_prob": 1.43674299124541e-06}, {"id": 300, "seek": 182612, "start": 1829.78, "end": 1834.36, "text": " else could you do have a look on the forums see what people are trying have a look on", "tokens": [1646, 727, 291, 360, 362, 257, 574, 322, 264, 26998, 536, 437, 561, 366, 1382, 362, 257, 574, 322], "temperature": 0.0, "avg_logprob": -0.1273604754743905, "compression_ratio": 1.7190476190476192, "no_speech_prob": 1.43674299124541e-06}, {"id": 301, "seek": 182612, "start": 1834.36, "end": 1839.7199999999998, "text": " the book website or the course website to see what other people have achieved and what", "tokens": [264, 1446, 3144, 420, 264, 1164, 3144, 281, 536, 437, 661, 561, 362, 11042, 293, 437], "temperature": 0.0, "avg_logprob": -0.1273604754743905, "compression_ratio": 1.7190476190476192, "no_speech_prob": 1.43674299124541e-06}, {"id": 302, "seek": 182612, "start": 1839.7199999999998, "end": 1842.0, "text": " they did and play around.", "tokens": [436, 630, 293, 862, 926, 13], "temperature": 0.0, "avg_logprob": -0.1273604754743905, "compression_ratio": 1.7190476190476192, "no_speech_prob": 1.43674299124541e-06}, {"id": 303, "seek": 182612, "start": 1842.0, "end": 1848.9599999999998, "text": " So we've got some tools in our toolbox now for you to experiment with.", "tokens": [407, 321, 600, 658, 512, 3873, 294, 527, 44593, 586, 337, 291, 281, 5120, 365, 13], "temperature": 0.0, "avg_logprob": -0.1273604754743905, "compression_ratio": 1.7190476190476192, "no_speech_prob": 1.43674299124541e-06}, {"id": 304, "seek": 184896, "start": 1848.96, "end": 1858.3400000000001, "text": " So that is that is pet breeds that is a you know a pretty tricky computer vision classification", "tokens": [407, 300, 307, 300, 307, 3817, 41609, 300, 307, 257, 291, 458, 257, 1238, 12414, 3820, 5201, 21538], "temperature": 0.0, "avg_logprob": -0.10421630583311382, "compression_ratio": 1.6564102564102565, "no_speech_prob": 1.172636189039622e-06}, {"id": 305, "seek": 184896, "start": 1858.3400000000001, "end": 1864.04, "text": " problem and we kind of have seen most of the pieces of what goes into the training of it", "tokens": [1154, 293, 321, 733, 295, 362, 1612, 881, 295, 264, 3755, 295, 437, 1709, 666, 264, 3097, 295, 309], "temperature": 0.0, "avg_logprob": -0.10421630583311382, "compression_ratio": 1.6564102564102565, "no_speech_prob": 1.172636189039622e-06}, {"id": 306, "seek": 184896, "start": 1864.04, "end": 1868.7, "text": " we haven't seen how to build the actual architecture but other than that we've kind of worked our", "tokens": [321, 2378, 380, 1612, 577, 281, 1322, 264, 3539, 9482, 457, 661, 813, 300, 321, 600, 733, 295, 2732, 527], "temperature": 0.0, "avg_logprob": -0.10421630583311382, "compression_ratio": 1.6564102564102565, "no_speech_prob": 1.172636189039622e-06}, {"id": 307, "seek": 184896, "start": 1868.7, "end": 1870.8400000000001, "text": " way up to understanding what's going on.", "tokens": [636, 493, 281, 3701, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.10421630583311382, "compression_ratio": 1.6564102564102565, "no_speech_prob": 1.172636189039622e-06}, {"id": 308, "seek": 187084, "start": 1870.84, "end": 1879.84, "text": " So let's build from there into another kind of data set one that involves multi-label", "tokens": [407, 718, 311, 1322, 490, 456, 666, 1071, 733, 295, 1412, 992, 472, 300, 11626, 4825, 12, 75, 18657], "temperature": 0.0, "avg_logprob": -0.19185055882097726, "compression_ratio": 1.6701570680628273, "no_speech_prob": 2.6841862563742325e-06}, {"id": 309, "seek": 187084, "start": 1879.84, "end": 1880.84, "text": " classification.", "tokens": [21538, 13], "temperature": 0.0, "avg_logprob": -0.19185055882097726, "compression_ratio": 1.6701570680628273, "no_speech_prob": 2.6841862563742325e-06}, {"id": 310, "seek": 187084, "start": 1880.84, "end": 1883.56, "text": " So what's multi-label classification?", "tokens": [407, 437, 311, 4825, 12, 75, 18657, 21538, 30], "temperature": 0.0, "avg_logprob": -0.19185055882097726, "compression_ratio": 1.6701570680628273, "no_speech_prob": 2.6841862563742325e-06}, {"id": 311, "seek": 187084, "start": 1883.56, "end": 1893.6399999999999, "text": " Well maybe so maybe let's look at an example here is a multi-label data set where you can", "tokens": [1042, 1310, 370, 1310, 718, 311, 574, 412, 364, 1365, 510, 307, 257, 4825, 12, 75, 18657, 1412, 992, 689, 291, 393], "temperature": 0.0, "avg_logprob": -0.19185055882097726, "compression_ratio": 1.6701570680628273, "no_speech_prob": 2.6841862563742325e-06}, {"id": 312, "seek": 187084, "start": 1893.6399999999999, "end": 1900.3999999999999, "text": " see that it's not just one label on each image but sometimes is three bicycle car person.", "tokens": [536, 300, 309, 311, 406, 445, 472, 7645, 322, 1184, 3256, 457, 2171, 307, 1045, 20888, 1032, 954, 13], "temperature": 0.0, "avg_logprob": -0.19185055882097726, "compression_ratio": 1.6701570680628273, "no_speech_prob": 2.6841862563742325e-06}, {"id": 313, "seek": 190040, "start": 1900.4, "end": 1903.24, "text": " I don't actually see the car here at best it's being dropped out.", "tokens": [286, 500, 380, 767, 536, 264, 1032, 510, 412, 1151, 309, 311, 885, 8119, 484, 13], "temperature": 0.0, "avg_logprob": -0.171198976450953, "compression_ratio": 1.495, "no_speech_prob": 1.963793920367607e-06}, {"id": 314, "seek": 190040, "start": 1903.24, "end": 1909.1200000000001, "text": " So a multi-label data set is one where you still got one image per row but you can have", "tokens": [407, 257, 4825, 12, 75, 18657, 1412, 992, 307, 472, 689, 291, 920, 658, 472, 3256, 680, 5386, 457, 291, 393, 362], "temperature": 0.0, "avg_logprob": -0.171198976450953, "compression_ratio": 1.495, "no_speech_prob": 1.963793920367607e-06}, {"id": 315, "seek": 190040, "start": 1909.1200000000001, "end": 1914.52, "text": " 0, 1, 2 or more labels per row.", "tokens": [1958, 11, 502, 11, 568, 420, 544, 16949, 680, 5386, 13], "temperature": 0.0, "avg_logprob": -0.171198976450953, "compression_ratio": 1.495, "no_speech_prob": 1.963793920367607e-06}, {"id": 316, "seek": 190040, "start": 1914.52, "end": 1918.24, "text": " So we're going to have a think about and look at how we handle that but first of all let's", "tokens": [407, 321, 434, 516, 281, 362, 257, 519, 466, 293, 574, 412, 577, 321, 4813, 300, 457, 700, 295, 439, 718, 311], "temperature": 0.0, "avg_logprob": -0.171198976450953, "compression_ratio": 1.495, "no_speech_prob": 1.963793920367607e-06}, {"id": 317, "seek": 190040, "start": 1918.24, "end": 1923.4, "text": " take another question.", "tokens": [747, 1071, 1168, 13], "temperature": 0.0, "avg_logprob": -0.171198976450953, "compression_ratio": 1.495, "no_speech_prob": 1.963793920367607e-06}, {"id": 318, "seek": 192340, "start": 1923.4, "end": 1930.68, "text": " Does dropping floating point number precision switching from FP32 to FP16 have an impact", "tokens": [4402, 13601, 12607, 935, 1230, 18356, 16493, 490, 36655, 11440, 281, 36655, 6866, 362, 364, 2712], "temperature": 0.0, "avg_logprob": -0.21538777784867721, "compression_ratio": 1.3707865168539326, "no_speech_prob": 3.6326639474282274e-07}, {"id": 319, "seek": 192340, "start": 1930.68, "end": 1934.3200000000002, "text": " on final?", "tokens": [322, 2572, 30], "temperature": 0.0, "avg_logprob": -0.21538777784867721, "compression_ratio": 1.3707865168539326, "no_speech_prob": 3.6326639474282274e-07}, {"id": 320, "seek": 192340, "start": 1934.3200000000002, "end": 1937.2800000000002, "text": " Yes it does.", "tokens": [1079, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.21538777784867721, "compression_ratio": 1.3707865168539326, "no_speech_prob": 3.6326639474282274e-07}, {"id": 321, "seek": 192340, "start": 1937.2800000000002, "end": 1942.1200000000001, "text": " Often it makes it better believe it or not.", "tokens": [20043, 309, 1669, 309, 1101, 1697, 309, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.21538777784867721, "compression_ratio": 1.3707865168539326, "no_speech_prob": 3.6326639474282274e-07}, {"id": 322, "seek": 192340, "start": 1942.1200000000001, "end": 1947.1200000000001, "text": " It seems like you know the kind of it's doing a little bit of rounding off is one way to", "tokens": [467, 2544, 411, 291, 458, 264, 733, 295, 309, 311, 884, 257, 707, 857, 295, 48237, 766, 307, 472, 636, 281], "temperature": 0.0, "avg_logprob": -0.21538777784867721, "compression_ratio": 1.3707865168539326, "no_speech_prob": 3.6326639474282274e-07}, {"id": 323, "seek": 194712, "start": 1947.12, "end": 1953.6399999999999, "text": " give it drop some of that precision and so that creates a bit more bumpiness a bit more", "tokens": [976, 309, 3270, 512, 295, 300, 18356, 293, 370, 300, 7829, 257, 857, 544, 9961, 1324, 257, 857, 544], "temperature": 0.0, "avg_logprob": -0.140421019660102, "compression_ratio": 1.7344398340248963, "no_speech_prob": 5.682373284798814e-06}, {"id": 324, "seek": 194712, "start": 1953.6399999999999, "end": 1960.08, "text": " uncertainty it was you know of a stochastic nature and you know when you introduce more", "tokens": [15697, 309, 390, 291, 458, 295, 257, 342, 8997, 2750, 3687, 293, 291, 458, 562, 291, 5366, 544], "temperature": 0.0, "avg_logprob": -0.140421019660102, "compression_ratio": 1.7344398340248963, "no_speech_prob": 5.682373284798814e-06}, {"id": 325, "seek": 194712, "start": 1960.08, "end": 1966.8, "text": " slightly random stuff into training it very often makes it a bit better and so yeah FP16", "tokens": [4748, 4974, 1507, 666, 3097, 309, 588, 2049, 1669, 309, 257, 857, 1101, 293, 370, 1338, 36655, 6866], "temperature": 0.0, "avg_logprob": -0.140421019660102, "compression_ratio": 1.7344398340248963, "no_speech_prob": 5.682373284798814e-06}, {"id": 326, "seek": 194712, "start": 1966.8, "end": 1972.08, "text": " training often gives us a slightly better result but I you know I wouldn't say it's", "tokens": [3097, 2049, 2709, 505, 257, 4748, 1101, 1874, 457, 286, 291, 458, 286, 2759, 380, 584, 309, 311], "temperature": 0.0, "avg_logprob": -0.140421019660102, "compression_ratio": 1.7344398340248963, "no_speech_prob": 5.682373284798814e-06}, {"id": 327, "seek": 194712, "start": 1972.08, "end": 1975.4399999999998, "text": " generally a big deal either way and certainly it's not always better.", "tokens": [5101, 257, 955, 2028, 2139, 636, 293, 3297, 309, 311, 406, 1009, 1101, 13], "temperature": 0.0, "avg_logprob": -0.140421019660102, "compression_ratio": 1.7344398340248963, "no_speech_prob": 5.682373284798814e-06}, {"id": 328, "seek": 197544, "start": 1975.44, "end": 1985.76, "text": " Would you say this is a bit of a pattern in learning less less exact stochastic way?", "tokens": [6068, 291, 584, 341, 307, 257, 857, 295, 257, 5102, 294, 2539, 1570, 1570, 1900, 342, 8997, 2750, 636, 30], "temperature": 0.0, "avg_logprob": -0.14276796732193384, "compression_ratio": 1.6221198156682028, "no_speech_prob": 1.9637764125945978e-06}, {"id": 329, "seek": 197544, "start": 1985.76, "end": 1992.48, "text": " For sure not just in deep learning but machine learning more generally you know there's been", "tokens": [1171, 988, 406, 445, 294, 2452, 2539, 457, 3479, 2539, 544, 5101, 291, 458, 456, 311, 668], "temperature": 0.0, "avg_logprob": -0.14276796732193384, "compression_ratio": 1.6221198156682028, "no_speech_prob": 1.9637764125945978e-06}, {"id": 330, "seek": 197544, "start": 1992.48, "end": 1996.3600000000001, "text": " some interesting research looking at like matrix factorization techniques which if you", "tokens": [512, 1880, 2132, 1237, 412, 411, 8141, 5952, 2144, 7512, 597, 498, 291], "temperature": 0.0, "avg_logprob": -0.14276796732193384, "compression_ratio": 1.6221198156682028, "no_speech_prob": 1.9637764125945978e-06}, {"id": 331, "seek": 197544, "start": 1996.3600000000001, "end": 2002.5800000000002, "text": " want them to go super fast you can lots of machines you can randomization and you often", "tokens": [528, 552, 281, 352, 1687, 2370, 291, 393, 3195, 295, 8379, 291, 393, 4974, 2144, 293, 291, 2049], "temperature": 0.0, "avg_logprob": -0.14276796732193384, "compression_ratio": 1.6221198156682028, "no_speech_prob": 1.9637764125945978e-06}, {"id": 332, "seek": 200258, "start": 2002.58, "end": 2006.96, "text": " when you then use the results you often find you actually get better better outcomes.", "tokens": [562, 291, 550, 764, 264, 3542, 291, 2049, 915, 291, 767, 483, 1101, 1101, 10070, 13], "temperature": 0.0, "avg_logprob": -0.2095807605319553, "compression_ratio": 1.572, "no_speech_prob": 4.157300736551406e-06}, {"id": 333, "seek": 200258, "start": 2006.96, "end": 2012.32, "text": " Just a brief plug for the fast AI computational linear algebra course which talks a little", "tokens": [1449, 257, 5353, 5452, 337, 264, 2370, 7318, 28270, 8213, 21989, 1164, 597, 6686, 257, 707], "temperature": 0.0, "avg_logprob": -0.2095807605319553, "compression_ratio": 1.572, "no_speech_prob": 4.157300736551406e-06}, {"id": 334, "seek": 200258, "start": 2012.32, "end": 2016.4399999999998, "text": " bit about about random.", "tokens": [857, 466, 466, 4974, 13], "temperature": 0.0, "avg_logprob": -0.2095807605319553, "compression_ratio": 1.572, "no_speech_prob": 4.157300736551406e-06}, {"id": 335, "seek": 200258, "start": 2016.4399999999998, "end": 2017.4399999999998, "text": " Does it really?", "tokens": [4402, 309, 534, 30], "temperature": 0.0, "avg_logprob": -0.2095807605319553, "compression_ratio": 1.572, "no_speech_prob": 4.157300736551406e-06}, {"id": 336, "seek": 200258, "start": 2017.4399999999998, "end": 2021.28, "text": " Well that sounds like a fascinating course and look at that it's number one hit here", "tokens": [1042, 300, 3263, 411, 257, 10343, 1164, 293, 574, 412, 300, 309, 311, 1230, 472, 2045, 510], "temperature": 0.0, "avg_logprob": -0.2095807605319553, "compression_ratio": 1.572, "no_speech_prob": 4.157300736551406e-06}, {"id": 337, "seek": 200258, "start": 2021.28, "end": 2030.0, "text": " on Google so easy to find brought by somebody called Rachel Thomas hey that's the same name", "tokens": [322, 3329, 370, 1858, 281, 915, 3038, 538, 2618, 1219, 14246, 8500, 4177, 300, 311, 264, 912, 1315], "temperature": 0.0, "avg_logprob": -0.2095807605319553, "compression_ratio": 1.572, "no_speech_prob": 4.157300736551406e-06}, {"id": 338, "seek": 203000, "start": 2030.0, "end": 2035.2, "text": " as you Rachel Thomas.", "tokens": [382, 291, 14246, 8500, 13], "temperature": 0.0, "avg_logprob": -0.1491545352739157, "compression_ratio": 1.5387755102040817, "no_speech_prob": 2.6425680061947787e-06}, {"id": 339, "seek": 203000, "start": 2035.2, "end": 2039.8, "text": " All right so how are we going to do multi-label classification so let's look at a data set", "tokens": [1057, 558, 370, 577, 366, 321, 516, 281, 360, 4825, 12, 75, 18657, 21538, 370, 718, 311, 574, 412, 257, 1412, 992], "temperature": 0.0, "avg_logprob": -0.1491545352739157, "compression_ratio": 1.5387755102040817, "no_speech_prob": 2.6425680061947787e-06}, {"id": 340, "seek": 203000, "start": 2039.8, "end": 2043.68, "text": " called Pascal which is a pretty famous data set we'll look at the version that goes back", "tokens": [1219, 41723, 597, 307, 257, 1238, 4618, 1412, 992, 321, 603, 574, 412, 264, 3037, 300, 1709, 646], "temperature": 0.0, "avg_logprob": -0.1491545352739157, "compression_ratio": 1.5387755102040817, "no_speech_prob": 2.6425680061947787e-06}, {"id": 341, "seek": 203000, "start": 2043.68, "end": 2050.52, "text": " to 2007 been around for a long time and it comes with a CSV file which we will read in", "tokens": [281, 12656, 668, 926, 337, 257, 938, 565, 293, 309, 1487, 365, 257, 48814, 3991, 597, 321, 486, 1401, 294], "temperature": 0.0, "avg_logprob": -0.1491545352739157, "compression_ratio": 1.5387755102040817, "no_speech_prob": 2.6425680061947787e-06}, {"id": 342, "seek": 203000, "start": 2050.52, "end": 2058.96, "text": " CSV is comma separated values and let's take a look each row has a file name one or more", "tokens": [48814, 307, 22117, 12005, 4190, 293, 718, 311, 747, 257, 574, 1184, 5386, 575, 257, 3991, 1315, 472, 420, 544], "temperature": 0.0, "avg_logprob": -0.1491545352739157, "compression_ratio": 1.5387755102040817, "no_speech_prob": 2.6425680061947787e-06}, {"id": 343, "seek": 205896, "start": 2058.96, "end": 2064.4, "text": " labels and something telling you whether it's in the validation set or not.", "tokens": [16949, 293, 746, 3585, 291, 1968, 309, 311, 294, 264, 24071, 992, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.1699681934557463, "compression_ratio": 1.6812227074235808, "no_speech_prob": 8.990934361463587e-07}, {"id": 344, "seek": 205896, "start": 2064.4, "end": 2068.6, "text": " So the list of categories in each image is a space delimited string this doesn't have", "tokens": [407, 264, 1329, 295, 10479, 294, 1184, 3256, 307, 257, 1901, 1103, 332, 1226, 6798, 341, 1177, 380, 362], "temperature": 0.0, "avg_logprob": -0.1699681934557463, "compression_ratio": 1.6812227074235808, "no_speech_prob": 8.990934361463587e-07}, {"id": 345, "seek": 205896, "start": 2068.6, "end": 2071.88, "text": " a horse person it has a horse and a person.", "tokens": [257, 6832, 954, 309, 575, 257, 6832, 293, 257, 954, 13], "temperature": 0.0, "avg_logprob": -0.1699681934557463, "compression_ratio": 1.6812227074235808, "no_speech_prob": 8.990934361463587e-07}, {"id": 346, "seek": 205896, "start": 2071.88, "end": 2082.9, "text": " PD here stands for pandas pandas is a really important library for any kind of data processing", "tokens": [10464, 510, 7382, 337, 4565, 296, 4565, 296, 307, 257, 534, 1021, 6405, 337, 604, 733, 295, 1412, 9007], "temperature": 0.0, "avg_logprob": -0.1699681934557463, "compression_ratio": 1.6812227074235808, "no_speech_prob": 8.990934361463587e-07}, {"id": 347, "seek": 205896, "start": 2082.9, "end": 2085.6, "text": " and you'll use it all the time in machine learning and deep learning so let's have a", "tokens": [293, 291, 603, 764, 309, 439, 264, 565, 294, 3479, 2539, 293, 2452, 2539, 370, 718, 311, 362, 257], "temperature": 0.0, "avg_logprob": -0.1699681934557463, "compression_ratio": 1.6812227074235808, "no_speech_prob": 8.990934361463587e-07}, {"id": 348, "seek": 208560, "start": 2085.6, "end": 2091.58, "text": " quick chat about it not a real panda it's a name of a library and it creates things", "tokens": [1702, 5081, 466, 309, 406, 257, 957, 46685, 309, 311, 257, 1315, 295, 257, 6405, 293, 309, 7829, 721], "temperature": 0.0, "avg_logprob": -0.0953133742014567, "compression_ratio": 1.7712177121771218, "no_speech_prob": 1.933351541083539e-06}, {"id": 349, "seek": 208560, "start": 2091.58, "end": 2096.64, "text": " called data frames that's what the DF here stands for and a data frame is a table containing", "tokens": [1219, 1412, 12083, 300, 311, 437, 264, 48336, 510, 7382, 337, 293, 257, 1412, 3920, 307, 257, 3199, 19273], "temperature": 0.0, "avg_logprob": -0.0953133742014567, "compression_ratio": 1.7712177121771218, "no_speech_prob": 1.933351541083539e-06}, {"id": 350, "seek": 208560, "start": 2096.64, "end": 2101.74, "text": " rows and columns pandas can also do some slightly more sophisticated things than that but we'll", "tokens": [13241, 293, 13766, 4565, 296, 393, 611, 360, 512, 4748, 544, 16950, 721, 813, 300, 457, 321, 603], "temperature": 0.0, "avg_logprob": -0.0953133742014567, "compression_ratio": 1.7712177121771218, "no_speech_prob": 1.933351541083539e-06}, {"id": 351, "seek": 208560, "start": 2101.74, "end": 2103.88, "text": " treat it that way for now.", "tokens": [2387, 309, 300, 636, 337, 586, 13], "temperature": 0.0, "avg_logprob": -0.0953133742014567, "compression_ratio": 1.7712177121771218, "no_speech_prob": 1.933351541083539e-06}, {"id": 352, "seek": 208560, "start": 2103.88, "end": 2108.8399999999997, "text": " So you can read in a data frame by saying PD for pandas pandas read CSV given a file", "tokens": [407, 291, 393, 1401, 294, 257, 1412, 3920, 538, 1566, 10464, 337, 4565, 296, 4565, 296, 1401, 48814, 2212, 257, 3991], "temperature": 0.0, "avg_logprob": -0.0953133742014567, "compression_ratio": 1.7712177121771218, "no_speech_prob": 1.933351541083539e-06}, {"id": 353, "seek": 208560, "start": 2108.8399999999997, "end": 2115.08, "text": " name you've now got a data frame you can call head to see the first few rows of it for instance", "tokens": [1315, 291, 600, 586, 658, 257, 1412, 3920, 291, 393, 818, 1378, 281, 536, 264, 700, 1326, 13241, 295, 309, 337, 5197], "temperature": 0.0, "avg_logprob": -0.0953133742014567, "compression_ratio": 1.7712177121771218, "no_speech_prob": 1.933351541083539e-06}, {"id": 354, "seek": 211508, "start": 2115.08, "end": 2123.12, "text": " a data frame as a I lock integer location property which you can index into as if it", "tokens": [257, 1412, 3920, 382, 257, 286, 4017, 24922, 4914, 4707, 597, 291, 393, 8186, 666, 382, 498, 309], "temperature": 0.0, "avg_logprob": -0.1754240260404699, "compression_ratio": 1.7425742574257426, "no_speech_prob": 8.579214068049623e-07}, {"id": 355, "seek": 211508, "start": 2123.12, "end": 2131.0, "text": " was an array that looks just like numpy so colon means every row remember it's row comma", "tokens": [390, 364, 10225, 300, 1542, 445, 411, 1031, 8200, 370, 8255, 1355, 633, 5386, 1604, 309, 311, 5386, 22117], "temperature": 0.0, "avg_logprob": -0.1754240260404699, "compression_ratio": 1.7425742574257426, "no_speech_prob": 8.579214068049623e-07}, {"id": 356, "seek": 211508, "start": 2131.0, "end": 2137.6, "text": " column and zero means zero column and so here is the first column of the data frame you", "tokens": [7738, 293, 4018, 1355, 4018, 7738, 293, 370, 510, 307, 264, 700, 7738, 295, 264, 1412, 3920, 291], "temperature": 0.0, "avg_logprob": -0.1754240260404699, "compression_ratio": 1.7425742574257426, "no_speech_prob": 8.579214068049623e-07}, {"id": 357, "seek": 211508, "start": 2137.6, "end": 2143.7999999999997, "text": " can do the exact opposite so the zeroth row and every column is going to give us the first", "tokens": [393, 360, 264, 1900, 6182, 370, 264, 44746, 900, 5386, 293, 633, 7738, 307, 516, 281, 976, 505, 264, 700], "temperature": 0.0, "avg_logprob": -0.1754240260404699, "compression_ratio": 1.7425742574257426, "no_speech_prob": 8.579214068049623e-07}, {"id": 358, "seek": 214380, "start": 2143.8, "end": 2150.1200000000003, "text": " row and you can see the row has column headers and values so it's a little bit different", "tokens": [5386, 293, 291, 393, 536, 264, 5386, 575, 7738, 45101, 293, 4190, 370, 309, 311, 257, 707, 857, 819], "temperature": 0.0, "avg_logprob": -0.08842959770789513, "compression_ratio": 1.7227722772277227, "no_speech_prob": 8.990938340502908e-07}, {"id": 359, "seek": 214380, "start": 2150.1200000000003, "end": 2155.7200000000003, "text": " to numpy and remember if there's a comma colon or a bunch of comma colons at the end", "tokens": [281, 1031, 8200, 293, 1604, 498, 456, 311, 257, 22117, 8255, 420, 257, 3840, 295, 22117, 1173, 892, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.08842959770789513, "compression_ratio": 1.7227722772277227, "no_speech_prob": 8.990938340502908e-07}, {"id": 360, "seek": 214380, "start": 2155.7200000000003, "end": 2162.28, "text": " of an indexing in numpy or pytorch or pandas whatever you can get rid of it and these two", "tokens": [295, 364, 8186, 278, 294, 1031, 8200, 420, 25878, 284, 339, 420, 4565, 296, 2035, 291, 393, 483, 3973, 295, 309, 293, 613, 732], "temperature": 0.0, "avg_logprob": -0.08842959770789513, "compression_ratio": 1.7227722772277227, "no_speech_prob": 8.990938340502908e-07}, {"id": 361, "seek": 214380, "start": 2162.28, "end": 2169.02, "text": " are exactly the same you could do the same thing here by grabbing the column by name", "tokens": [366, 2293, 264, 912, 291, 727, 360, 264, 912, 551, 510, 538, 23771, 264, 7738, 538, 1315], "temperature": 0.0, "avg_logprob": -0.08842959770789513, "compression_ratio": 1.7227722772277227, "no_speech_prob": 8.990938340502908e-07}, {"id": 362, "seek": 216902, "start": 2169.02, "end": 2176.04, "text": " the first column is F name so a DFF name you get that first column you can create new columns", "tokens": [264, 700, 7738, 307, 479, 1315, 370, 257, 413, 6345, 1315, 291, 483, 300, 700, 7738, 291, 393, 1884, 777, 13766], "temperature": 0.0, "avg_logprob": -0.10300700219122919, "compression_ratio": 1.7246376811594204, "no_speech_prob": 9.721528613226837e-07}, {"id": 363, "seek": 216902, "start": 2176.04, "end": 2181.72, "text": " so here's a tiny little data frame I've created from a dictionary and I could create a new", "tokens": [370, 510, 311, 257, 5870, 707, 1412, 3920, 286, 600, 2942, 490, 257, 25890, 293, 286, 727, 1884, 257, 777], "temperature": 0.0, "avg_logprob": -0.10300700219122919, "compression_ratio": 1.7246376811594204, "no_speech_prob": 9.721528613226837e-07}, {"id": 364, "seek": 216902, "start": 2181.72, "end": 2187.84, "text": " column by for example adding two columns and you can see there it is so it's like a lot", "tokens": [7738, 538, 337, 1365, 5127, 732, 13766, 293, 291, 393, 536, 456, 309, 307, 370, 309, 311, 411, 257, 688], "temperature": 0.0, "avg_logprob": -0.10300700219122919, "compression_ratio": 1.7246376811594204, "no_speech_prob": 9.721528613226837e-07}, {"id": 365, "seek": 216902, "start": 2187.84, "end": 2194.68, "text": " like numpy or pytorch except you have this idea of kind of rows and and column named", "tokens": [411, 1031, 8200, 420, 25878, 284, 339, 3993, 291, 362, 341, 1558, 295, 733, 295, 13241, 293, 293, 7738, 4926], "temperature": 0.0, "avg_logprob": -0.10300700219122919, "compression_ratio": 1.7246376811594204, "no_speech_prob": 9.721528613226837e-07}, {"id": 366, "seek": 219468, "start": 2194.68, "end": 2203.0, "text": " columns and so it's all about kind of tabular data I find its API pretty unintuitive a lot", "tokens": [13766, 293, 370, 309, 311, 439, 466, 733, 295, 4421, 1040, 1412, 286, 915, 1080, 9362, 1238, 29466, 48314, 257, 688], "temperature": 0.0, "avg_logprob": -0.08550293337215077, "compression_ratio": 1.6380090497737556, "no_speech_prob": 9.874617035166011e-07}, {"id": 367, "seek": 219468, "start": 2203.0, "end": 2208.2799999999997, "text": " of people do but it's fast and powerful so it takes a while to get familiar with it but", "tokens": [295, 561, 360, 457, 309, 311, 2370, 293, 4005, 370, 309, 2516, 257, 1339, 281, 483, 4963, 365, 309, 457], "temperature": 0.0, "avg_logprob": -0.08550293337215077, "compression_ratio": 1.6380090497737556, "no_speech_prob": 9.874617035166011e-07}, {"id": 368, "seek": 219468, "start": 2208.2799999999997, "end": 2213.48, "text": " it's worth taking a while and the creator of pandas wrote a fantastic book called Python", "tokens": [309, 311, 3163, 1940, 257, 1339, 293, 264, 14181, 295, 4565, 296, 4114, 257, 5456, 1446, 1219, 15329], "temperature": 0.0, "avg_logprob": -0.08550293337215077, "compression_ratio": 1.6380090497737556, "no_speech_prob": 9.874617035166011e-07}, {"id": 369, "seek": 219468, "start": 2213.48, "end": 2220.02, "text": " for data analysis which I've read both versions and I found it fantastic it doesn't just cover", "tokens": [337, 1412, 5215, 597, 286, 600, 1401, 1293, 9606, 293, 286, 1352, 309, 5456, 309, 1177, 380, 445, 2060], "temperature": 0.0, "avg_logprob": -0.08550293337215077, "compression_ratio": 1.6380090497737556, "no_speech_prob": 9.874617035166011e-07}, {"id": 370, "seek": 222002, "start": 2220.02, "end": 2227.58, "text": " pandas it covers other stuff as well like IPython and numpy and matplotlib so highly", "tokens": [4565, 296, 309, 10538, 661, 1507, 382, 731, 411, 8671, 88, 11943, 293, 1031, 8200, 293, 3803, 564, 310, 38270, 370, 5405], "temperature": 0.0, "avg_logprob": -0.13498448801564647, "compression_ratio": 1.6698564593301435, "no_speech_prob": 3.689879974899668e-07}, {"id": 371, "seek": 222002, "start": 2227.58, "end": 2238.48, "text": " recommend this book this is our table so what we want to do now is construct data loaders", "tokens": [2748, 341, 1446, 341, 307, 527, 3199, 370, 437, 321, 528, 281, 360, 586, 307, 7690, 1412, 3677, 433], "temperature": 0.0, "avg_logprob": -0.13498448801564647, "compression_ratio": 1.6698564593301435, "no_speech_prob": 3.689879974899668e-07}, {"id": 372, "seek": 222002, "start": 2238.48, "end": 2244.12, "text": " that we can train with and we've talked about the data block API as being a great way to", "tokens": [300, 321, 393, 3847, 365, 293, 321, 600, 2825, 466, 264, 1412, 3461, 9362, 382, 885, 257, 869, 636, 281], "temperature": 0.0, "avg_logprob": -0.13498448801564647, "compression_ratio": 1.6698564593301435, "no_speech_prob": 3.689879974899668e-07}, {"id": 373, "seek": 222002, "start": 2244.12, "end": 2249.64, "text": " create data loaders so let's use this as an opportunity to create a data loaders or a", "tokens": [1884, 1412, 3677, 433, 370, 718, 311, 764, 341, 382, 364, 2650, 281, 1884, 257, 1412, 3677, 433, 420, 257], "temperature": 0.0, "avg_logprob": -0.13498448801564647, "compression_ratio": 1.6698564593301435, "no_speech_prob": 3.689879974899668e-07}, {"id": 374, "seek": 224964, "start": 2249.64, "end": 2257.0, "text": " process or create a data block and then data loaders for this and let's try to do it like", "tokens": [1399, 420, 1884, 257, 1412, 3461, 293, 550, 1412, 3677, 433, 337, 341, 293, 718, 311, 853, 281, 360, 309, 411], "temperature": 0.0, "avg_logprob": -0.10438356399536133, "compression_ratio": 1.8465608465608465, "no_speech_prob": 1.760337454470573e-06}, {"id": 375, "seek": 224964, "start": 2257.0, "end": 2262.7999999999997, "text": " right from square one so let's see exactly what's going on with a data block so first", "tokens": [558, 490, 3732, 472, 370, 718, 311, 536, 2293, 437, 311, 516, 322, 365, 257, 1412, 3461, 370, 700], "temperature": 0.0, "avg_logprob": -0.10438356399536133, "compression_ratio": 1.8465608465608465, "no_speech_prob": 1.760337454470573e-06}, {"id": 376, "seek": 224964, "start": 2262.7999999999997, "end": 2269.12, "text": " of all let's remind ourselves about what a data set and a data loader is a data set is", "tokens": [295, 439, 718, 311, 4160, 4175, 466, 437, 257, 1412, 992, 293, 257, 1412, 3677, 260, 307, 257, 1412, 992, 307], "temperature": 0.0, "avg_logprob": -0.10438356399536133, "compression_ratio": 1.8465608465608465, "no_speech_prob": 1.760337454470573e-06}, {"id": 377, "seek": 224964, "start": 2269.12, "end": 2275.2, "text": " an abstract idea of a class you can create a data set a data set is anything which you", "tokens": [364, 12649, 1558, 295, 257, 1508, 291, 393, 1884, 257, 1412, 992, 257, 1412, 992, 307, 1340, 597, 291], "temperature": 0.0, "avg_logprob": -0.10438356399536133, "compression_ratio": 1.8465608465608465, "no_speech_prob": 1.760337454470573e-06}, {"id": 378, "seek": 227520, "start": 2275.2, "end": 2282.3599999999997, "text": " can index into it like so or and you can take the length of it like so so for example the", "tokens": [393, 8186, 666, 309, 411, 370, 420, 293, 291, 393, 747, 264, 4641, 295, 309, 411, 370, 370, 337, 1365, 264], "temperature": 0.0, "avg_logprob": -0.0771845110346762, "compression_ratio": 1.7860696517412935, "no_speech_prob": 1.018807097352692e-06}, {"id": 379, "seek": 227520, "start": 2282.3599999999997, "end": 2288.7999999999997, "text": " list of the lowercase letters along with a number saying which lowercase letter it is", "tokens": [1329, 295, 264, 3126, 9765, 7825, 2051, 365, 257, 1230, 1566, 597, 3126, 9765, 5063, 309, 307], "temperature": 0.0, "avg_logprob": -0.0771845110346762, "compression_ratio": 1.7860696517412935, "no_speech_prob": 1.018807097352692e-06}, {"id": 380, "seek": 227520, "start": 2288.7999999999997, "end": 2295.2, "text": " I can index into it to get 0 comma a I can get the length of it to get 26 and so therefore", "tokens": [286, 393, 8186, 666, 309, 281, 483, 1958, 22117, 257, 286, 393, 483, 264, 4641, 295, 309, 281, 483, 7551, 293, 370, 4412], "temperature": 0.0, "avg_logprob": -0.0771845110346762, "compression_ratio": 1.7860696517412935, "no_speech_prob": 1.018807097352692e-06}, {"id": 381, "seek": 227520, "start": 2295.2, "end": 2301.3999999999996, "text": " this qualifies as a data set and in particular data sets normally you would expect that when", "tokens": [341, 4101, 11221, 382, 257, 1412, 992, 293, 294, 1729, 1412, 6352, 5646, 291, 576, 2066, 300, 562], "temperature": 0.0, "avg_logprob": -0.0771845110346762, "compression_ratio": 1.7860696517412935, "no_speech_prob": 1.018807097352692e-06}, {"id": 382, "seek": 230140, "start": 2301.4, "end": 2308.2400000000002, "text": " you index into it you would get back a tuple because you've got the independent and dependent", "tokens": [291, 8186, 666, 309, 291, 576, 483, 646, 257, 2604, 781, 570, 291, 600, 658, 264, 6695, 293, 12334], "temperature": 0.0, "avg_logprob": -0.0835933573105756, "compression_ratio": 1.7598039215686274, "no_speech_prob": 3.412583282624837e-07}, {"id": 383, "seek": 230140, "start": 2308.2400000000002, "end": 2312.56, "text": " variables not necessarily always just two things there could be more there could be", "tokens": [9102, 406, 4725, 1009, 445, 732, 721, 456, 727, 312, 544, 456, 727, 312], "temperature": 0.0, "avg_logprob": -0.0835933573105756, "compression_ratio": 1.7598039215686274, "no_speech_prob": 3.412583282624837e-07}, {"id": 384, "seek": 230140, "start": 2312.56, "end": 2323.2400000000002, "text": " less but two is the most common so once we have a data set we can pass it to a data loader", "tokens": [1570, 457, 732, 307, 264, 881, 2689, 370, 1564, 321, 362, 257, 1412, 992, 321, 393, 1320, 309, 281, 257, 1412, 3677, 260], "temperature": 0.0, "avg_logprob": -0.0835933573105756, "compression_ratio": 1.7598039215686274, "no_speech_prob": 3.412583282624837e-07}, {"id": 385, "seek": 230140, "start": 2323.2400000000002, "end": 2329.12, "text": " we can request we can request a particular batch size we can shuffle or not and so there's", "tokens": [321, 393, 5308, 321, 393, 5308, 257, 1729, 15245, 2744, 321, 393, 39426, 420, 406, 293, 370, 456, 311], "temperature": 0.0, "avg_logprob": -0.0835933573105756, "compression_ratio": 1.7598039215686274, "no_speech_prob": 3.412583282624837e-07}, {"id": 386, "seek": 232912, "start": 2329.12, "end": 2335.92, "text": " our data loader from a we could grab the first value from that iterator and here is the shuffled", "tokens": [527, 1412, 3677, 260, 490, 257, 321, 727, 4444, 264, 700, 2158, 490, 300, 17138, 1639, 293, 510, 307, 264, 402, 33974], "temperature": 0.0, "avg_logprob": -0.10953595561365928, "compression_ratio": 1.7524271844660195, "no_speech_prob": 2.1691704432669212e-07}, {"id": 387, "seek": 232912, "start": 2335.92, "end": 2344.56, "text": " 7 is h 4 is a 20 is u and so forth and so I remember a mini batch has a bunch of a mini", "tokens": [1614, 307, 276, 1017, 307, 257, 945, 307, 344, 293, 370, 5220, 293, 370, 286, 1604, 257, 8382, 15245, 575, 257, 3840, 295, 257, 8382], "temperature": 0.0, "avg_logprob": -0.10953595561365928, "compression_ratio": 1.7524271844660195, "no_speech_prob": 2.1691704432669212e-07}, {"id": 388, "seek": 232912, "start": 2344.56, "end": 2349.72, "text": " batch of the independent variable and a mini batch of the dependent variable if you want", "tokens": [15245, 295, 264, 6695, 7006, 293, 257, 8382, 15245, 295, 264, 12334, 7006, 498, 291, 528], "temperature": 0.0, "avg_logprob": -0.10953595561365928, "compression_ratio": 1.7524271844660195, "no_speech_prob": 2.1691704432669212e-07}, {"id": 389, "seek": 232912, "start": 2349.72, "end": 2355.7999999999997, "text": " to see how the two correspond to each other you can use zip so if I zip passing in this", "tokens": [281, 536, 577, 264, 732, 6805, 281, 1184, 661, 291, 393, 764, 20730, 370, 498, 286, 20730, 8437, 294, 341], "temperature": 0.0, "avg_logprob": -0.10953595561365928, "compression_ratio": 1.7524271844660195, "no_speech_prob": 2.1691704432669212e-07}, {"id": 390, "seek": 235580, "start": 2355.8, "end": 2363.52, "text": " list and then this list so be not and be one you can see what zip does in Python is that", "tokens": [1329, 293, 550, 341, 1329, 370, 312, 406, 293, 312, 472, 291, 393, 536, 437, 20730, 775, 294, 15329, 307, 300], "temperature": 0.0, "avg_logprob": -0.11216092951157514, "compression_ratio": 1.6901408450704225, "no_speech_prob": 4.1811830442384235e-07}, {"id": 391, "seek": 235580, "start": 2363.52, "end": 2369.96, "text": " grabs one element from each of those in turn and gives you back the tuples of the corresponding", "tokens": [30028, 472, 4478, 490, 1184, 295, 729, 294, 1261, 293, 2709, 291, 646, 264, 2604, 2622, 295, 264, 11760], "temperature": 0.0, "avg_logprob": -0.11216092951157514, "compression_ratio": 1.6901408450704225, "no_speech_prob": 4.1811830442384235e-07}, {"id": 392, "seek": 235580, "start": 2369.96, "end": 2377.84, "text": " elements since we're just passing in all of the elements of B to this function Python", "tokens": [4959, 1670, 321, 434, 445, 8437, 294, 439, 295, 264, 4959, 295, 363, 281, 341, 2445, 15329], "temperature": 0.0, "avg_logprob": -0.11216092951157514, "compression_ratio": 1.6901408450704225, "no_speech_prob": 4.1811830442384235e-07}, {"id": 393, "seek": 235580, "start": 2377.84, "end": 2385.1200000000003, "text": " has a convenient shortcut for that which is just say star B and so star means insert into", "tokens": [575, 257, 10851, 24822, 337, 300, 597, 307, 445, 584, 3543, 363, 293, 370, 3543, 1355, 8969, 666], "temperature": 0.0, "avg_logprob": -0.11216092951157514, "compression_ratio": 1.6901408450704225, "no_speech_prob": 4.1811830442384235e-07}, {"id": 394, "seek": 238512, "start": 2385.12, "end": 2391.56, "text": " this parameter list each element of B just like we did here so these are the same thing", "tokens": [341, 13075, 1329, 1184, 4478, 295, 363, 445, 411, 321, 630, 510, 370, 613, 366, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.0952153035572597, "compression_ratio": 1.6195121951219513, "no_speech_prob": 3.866961151288706e-07}, {"id": 395, "seek": 238512, "start": 2391.56, "end": 2397.68, "text": " so this is a very handy idiom that we use a lot in Python zip star something is kind", "tokens": [370, 341, 307, 257, 588, 13239, 18014, 298, 300, 321, 764, 257, 688, 294, 15329, 20730, 3543, 746, 307, 733], "temperature": 0.0, "avg_logprob": -0.0952153035572597, "compression_ratio": 1.6195121951219513, "no_speech_prob": 3.866961151288706e-07}, {"id": 396, "seek": 238512, "start": 2397.68, "end": 2407.08, "text": " of a way of like transposing something from one orientation to another.", "tokens": [295, 257, 636, 295, 411, 7132, 6110, 746, 490, 472, 14764, 281, 1071, 13], "temperature": 0.0, "avg_logprob": -0.0952153035572597, "compression_ratio": 1.6195121951219513, "no_speech_prob": 3.866961151288706e-07}, {"id": 397, "seek": 238512, "start": 2407.08, "end": 2412.44, "text": " All right so we've got a data set we've got a data loader and then what about data sets", "tokens": [1057, 558, 370, 321, 600, 658, 257, 1412, 992, 321, 600, 658, 257, 1412, 3677, 260, 293, 550, 437, 466, 1412, 6352], "temperature": 0.0, "avg_logprob": -0.0952153035572597, "compression_ratio": 1.6195121951219513, "no_speech_prob": 3.866961151288706e-07}, {"id": 398, "seek": 241244, "start": 2412.44, "end": 2417.76, "text": " what data sets is an object which has a training data set and a validation set data set so", "tokens": [437, 1412, 6352, 307, 364, 2657, 597, 575, 257, 3097, 1412, 992, 293, 257, 24071, 992, 1412, 992, 370], "temperature": 0.0, "avg_logprob": -0.10089863800420994, "compression_ratio": 1.8512820512820514, "no_speech_prob": 1.1365616501279874e-06}, {"id": 399, "seek": 241244, "start": 2417.76, "end": 2426.3, "text": " let's look at one but normally you don't start with kind of an enumeration like this like", "tokens": [718, 311, 574, 412, 472, 457, 5646, 291, 500, 380, 722, 365, 733, 295, 364, 465, 449, 5053, 411, 341, 411], "temperature": 0.0, "avg_logprob": -0.10089863800420994, "compression_ratio": 1.8512820512820514, "no_speech_prob": 1.1365616501279874e-06}, {"id": 400, "seek": 241244, "start": 2426.3, "end": 2432.44, "text": " with with an independent variable and a dependent variable normally you start with like a file", "tokens": [365, 365, 364, 6695, 7006, 293, 257, 12334, 7006, 5646, 291, 722, 365, 411, 257, 3991], "temperature": 0.0, "avg_logprob": -0.10089863800420994, "compression_ratio": 1.8512820512820514, "no_speech_prob": 1.1365616501279874e-06}, {"id": 401, "seek": 241244, "start": 2432.44, "end": 2440.7200000000003, "text": " name for example and then you you kind of calculate or compute or transform your file", "tokens": [1315, 337, 1365, 293, 550, 291, 291, 733, 295, 8873, 420, 14722, 420, 4088, 428, 3991], "temperature": 0.0, "avg_logprob": -0.10089863800420994, "compression_ratio": 1.8512820512820514, "no_speech_prob": 1.1365616501279874e-06}, {"id": 402, "seek": 244072, "start": 2440.72, "end": 2446.56, "text": " name into an image by opening it and a label by for example looking at the file name and", "tokens": [1315, 666, 364, 3256, 538, 5193, 309, 293, 257, 7645, 538, 337, 1365, 1237, 412, 264, 3991, 1315, 293], "temperature": 0.0, "avg_logprob": -0.07498660314650762, "compression_ratio": 1.8389830508474576, "no_speech_prob": 2.2033302116142295e-07}, {"id": 403, "seek": 244072, "start": 2446.56, "end": 2450.3599999999997, "text": " grabbing something out of it so for example we could do something similar here this is", "tokens": [23771, 746, 484, 295, 309, 370, 337, 1365, 321, 727, 360, 746, 2531, 510, 341, 307], "temperature": 0.0, "avg_logprob": -0.07498660314650762, "compression_ratio": 1.8389830508474576, "no_speech_prob": 2.2033302116142295e-07}, {"id": 404, "seek": 244072, "start": 2450.3599999999997, "end": 2456.24, "text": " what data sets does so we could start with just the lowercase letters so this is still", "tokens": [437, 1412, 6352, 775, 370, 321, 727, 722, 365, 445, 264, 3126, 9765, 7825, 370, 341, 307, 920], "temperature": 0.0, "avg_logprob": -0.07498660314650762, "compression_ratio": 1.8389830508474576, "no_speech_prob": 2.2033302116142295e-07}, {"id": 405, "seek": 244072, "start": 2456.24, "end": 2460.3199999999997, "text": " a data set right because we can index into it and we can get the length of it although", "tokens": [257, 1412, 992, 558, 570, 321, 393, 8186, 666, 309, 293, 321, 393, 483, 264, 4641, 295, 309, 4878], "temperature": 0.0, "avg_logprob": -0.07498660314650762, "compression_ratio": 1.8389830508474576, "no_speech_prob": 2.2033302116142295e-07}, {"id": 406, "seek": 244072, "start": 2460.3199999999997, "end": 2468.68, "text": " it's not giving us tuples yet so if we now pass that list to the data sets class and", "tokens": [309, 311, 406, 2902, 505, 2604, 2622, 1939, 370, 498, 321, 586, 1320, 300, 1329, 281, 264, 1412, 6352, 1508, 293], "temperature": 0.0, "avg_logprob": -0.07498660314650762, "compression_ratio": 1.8389830508474576, "no_speech_prob": 2.2033302116142295e-07}, {"id": 407, "seek": 246868, "start": 2468.68, "end": 2474.64, "text": " index into it we get back the tuple and it's actually a tuple with just one item this is", "tokens": [8186, 666, 309, 321, 483, 646, 264, 2604, 781, 293, 309, 311, 767, 257, 2604, 781, 365, 445, 472, 3174, 341, 307], "temperature": 0.0, "avg_logprob": -0.0906538206433493, "compression_ratio": 2.053639846743295, "no_speech_prob": 7.571140940854093e-07}, {"id": 408, "seek": 246868, "start": 2474.64, "end": 2478.52, "text": " how Python shows a tuple with one item as it puts it in parentheses and a comma and", "tokens": [577, 15329, 3110, 257, 2604, 781, 365, 472, 3174, 382, 309, 8137, 309, 294, 34153, 293, 257, 22117, 293], "temperature": 0.0, "avg_logprob": -0.0906538206433493, "compression_ratio": 2.053639846743295, "no_speech_prob": 7.571140940854093e-07}, {"id": 409, "seek": 246868, "start": 2478.52, "end": 2484.56, "text": " then nothing okay so in practice what we'd really want to do is to say like okay we'll", "tokens": [550, 1825, 1392, 370, 294, 3124, 437, 321, 1116, 534, 528, 281, 360, 307, 281, 584, 411, 1392, 321, 603], "temperature": 0.0, "avg_logprob": -0.0906538206433493, "compression_ratio": 2.053639846743295, "no_speech_prob": 7.571140940854093e-07}, {"id": 410, "seek": 246868, "start": 2484.56, "end": 2489.7999999999997, "text": " take this and do something to compute an independent variable and do something to compute the dependent", "tokens": [747, 341, 293, 360, 746, 281, 14722, 364, 6695, 7006, 293, 360, 746, 281, 14722, 264, 12334], "temperature": 0.0, "avg_logprob": -0.0906538206433493, "compression_ratio": 2.053639846743295, "no_speech_prob": 7.571140940854093e-07}, {"id": 411, "seek": 246868, "start": 2489.7999999999997, "end": 2494.12, "text": " variable so here's a function we could use to compute an independent variable which is", "tokens": [7006, 370, 510, 311, 257, 2445, 321, 727, 764, 281, 14722, 364, 6695, 7006, 597, 307], "temperature": 0.0, "avg_logprob": -0.0906538206433493, "compression_ratio": 2.053639846743295, "no_speech_prob": 7.571140940854093e-07}, {"id": 412, "seek": 246868, "start": 2494.12, "end": 2498.0, "text": " to stick an a on the end and our dependent variable might just be the same thing with", "tokens": [281, 2897, 364, 257, 322, 264, 917, 293, 527, 12334, 7006, 1062, 445, 312, 264, 912, 551, 365], "temperature": 0.0, "avg_logprob": -0.0906538206433493, "compression_ratio": 2.053639846743295, "no_speech_prob": 7.571140940854093e-07}, {"id": 413, "seek": 249800, "start": 2498.0, "end": 2504.94, "text": " a B on the end so here's two functions so for example now we can call data sets passing", "tokens": [257, 363, 322, 264, 917, 370, 510, 311, 732, 6828, 370, 337, 1365, 586, 321, 393, 818, 1412, 6352, 8437], "temperature": 0.0, "avg_logprob": -0.10251916885375977, "compression_ratio": 1.6198830409356726, "no_speech_prob": 1.0511482742003864e-06}, {"id": 414, "seek": 249800, "start": 2504.94, "end": 2514.08, "text": " in a and then we can pass in a list of transformations to do and so in this case I've just got one", "tokens": [294, 257, 293, 550, 321, 393, 1320, 294, 257, 1329, 295, 34852, 281, 360, 293, 370, 294, 341, 1389, 286, 600, 445, 658, 472], "temperature": 0.0, "avg_logprob": -0.10251916885375977, "compression_ratio": 1.6198830409356726, "no_speech_prob": 1.0511482742003864e-06}, {"id": 415, "seek": 249800, "start": 2514.08, "end": 2518.84, "text": " which is this function add an a on the end so now if I index into it I don't get a anymore", "tokens": [597, 307, 341, 2445, 909, 364, 257, 322, 264, 917, 370, 586, 498, 286, 8186, 666, 309, 286, 500, 380, 483, 257, 3602], "temperature": 0.0, "avg_logprob": -0.10251916885375977, "compression_ratio": 1.6198830409356726, "no_speech_prob": 1.0511482742003864e-06}, {"id": 416, "seek": 251884, "start": 2518.84, "end": 2528.6800000000003, "text": " I get a a if you pass multiple functions then it's going to do multiple things so here I've", "tokens": [286, 483, 257, 257, 498, 291, 1320, 3866, 6828, 550, 309, 311, 516, 281, 360, 3866, 721, 370, 510, 286, 600], "temperature": 0.0, "avg_logprob": -0.11888866623242696, "compression_ratio": 1.8442211055276383, "no_speech_prob": 1.0030130397353787e-06}, {"id": 417, "seek": 251884, "start": 2528.6800000000003, "end": 2535.48, "text": " got f1 then f2 a a b that's this one then that's this one and you'll see this is a list", "tokens": [658, 283, 16, 550, 283, 17, 257, 257, 272, 300, 311, 341, 472, 550, 300, 311, 341, 472, 293, 291, 603, 536, 341, 307, 257, 1329], "temperature": 0.0, "avg_logprob": -0.11888866623242696, "compression_ratio": 1.8442211055276383, "no_speech_prob": 1.0030130397353787e-06}, {"id": 418, "seek": 251884, "start": 2535.48, "end": 2541.36, "text": " of lists and the reason for that is that you can also pass something like this a list containing", "tokens": [295, 14511, 293, 264, 1778, 337, 300, 307, 300, 291, 393, 611, 1320, 746, 411, 341, 257, 1329, 19273], "temperature": 0.0, "avg_logprob": -0.11888866623242696, "compression_ratio": 1.8442211055276383, "no_speech_prob": 1.0030130397353787e-06}, {"id": 419, "seek": 251884, "start": 2541.36, "end": 2548.0, "text": " f1 a list containing f2 and this will actually take each element of a pass it through this", "tokens": [283, 16, 257, 1329, 19273, 283, 17, 293, 341, 486, 767, 747, 1184, 4478, 295, 257, 1320, 309, 807, 341], "temperature": 0.0, "avg_logprob": -0.11888866623242696, "compression_ratio": 1.8442211055276383, "no_speech_prob": 1.0030130397353787e-06}, {"id": 420, "seek": 254800, "start": 2548.0, "end": 2554.2, "text": " list of functions and there's just one of them to give you a a and then start again", "tokens": [1329, 295, 6828, 293, 456, 311, 445, 472, 295, 552, 281, 976, 291, 257, 257, 293, 550, 722, 797], "temperature": 0.0, "avg_logprob": -0.09555190624576984, "compression_ratio": 1.9819004524886878, "no_speech_prob": 2.0261388726794394e-06}, {"id": 421, "seek": 254800, "start": 2554.2, "end": 2559.76, "text": " and separately pass it through this list of functions there's just one to get a b and", "tokens": [293, 14759, 1320, 309, 807, 341, 1329, 295, 6828, 456, 311, 445, 472, 281, 483, 257, 272, 293], "temperature": 0.0, "avg_logprob": -0.09555190624576984, "compression_ratio": 1.9819004524886878, "no_speech_prob": 2.0261388726794394e-06}, {"id": 422, "seek": 254800, "start": 2559.76, "end": 2566.48, "text": " so this is actually kind of the main way we build up independent variables and dependent", "tokens": [370, 341, 307, 767, 733, 295, 264, 2135, 636, 321, 1322, 493, 6695, 9102, 293, 12334], "temperature": 0.0, "avg_logprob": -0.09555190624576984, "compression_ratio": 1.9819004524886878, "no_speech_prob": 2.0261388726794394e-06}, {"id": 423, "seek": 254800, "start": 2566.48, "end": 2570.9, "text": " variables in fast AI is we start with something like a file name and we pass it through two", "tokens": [9102, 294, 2370, 7318, 307, 321, 722, 365, 746, 411, 257, 3991, 1315, 293, 321, 1320, 309, 807, 732], "temperature": 0.0, "avg_logprob": -0.09555190624576984, "compression_ratio": 1.9819004524886878, "no_speech_prob": 2.0261388726794394e-06}, {"id": 424, "seek": 254800, "start": 2570.9, "end": 2575.36, "text": " lists of functions one of them will generally kind of open up the image for example and", "tokens": [14511, 295, 6828, 472, 295, 552, 486, 5101, 733, 295, 1269, 493, 264, 3256, 337, 1365, 293], "temperature": 0.0, "avg_logprob": -0.09555190624576984, "compression_ratio": 1.9819004524886878, "no_speech_prob": 2.0261388726794394e-06}, {"id": 425, "seek": 257536, "start": 2575.36, "end": 2580.1600000000003, "text": " the other one will kind of pass the file name example and give you a independent variable", "tokens": [264, 661, 472, 486, 733, 295, 1320, 264, 3991, 1315, 1365, 293, 976, 291, 257, 6695, 7006], "temperature": 0.0, "avg_logprob": -0.15388717651367187, "compression_ratio": 1.7794117647058822, "no_speech_prob": 3.412583282624837e-07}, {"id": 426, "seek": 257536, "start": 2580.1600000000003, "end": 2587.88, "text": " and a dependent variable so you can then create a data loaders object from data sets by passing", "tokens": [293, 257, 12334, 7006, 370, 291, 393, 550, 1884, 257, 1412, 3677, 433, 2657, 490, 1412, 6352, 538, 8437], "temperature": 0.0, "avg_logprob": -0.15388717651367187, "compression_ratio": 1.7794117647058822, "no_speech_prob": 3.412583282624837e-07}, {"id": 427, "seek": 257536, "start": 2587.88, "end": 2596.36, "text": " in the data sets and a batch size and so here you can see I've got shuffled o a i a etc", "tokens": [294, 264, 1412, 6352, 293, 257, 15245, 2744, 293, 370, 510, 291, 393, 536, 286, 600, 658, 402, 33974, 277, 257, 741, 257, 5183], "temperature": 0.0, "avg_logprob": -0.15388717651367187, "compression_ratio": 1.7794117647058822, "no_speech_prob": 3.412583282624837e-07}, {"id": 428, "seek": 257536, "start": 2596.36, "end": 2602.96, "text": " o b i b etc so this is worth studying to make sure you understand what data sets and data", "tokens": [277, 272, 741, 272, 5183, 370, 341, 307, 3163, 7601, 281, 652, 988, 291, 1223, 437, 1412, 6352, 293, 1412], "temperature": 0.0, "avg_logprob": -0.15388717651367187, "compression_ratio": 1.7794117647058822, "no_speech_prob": 3.412583282624837e-07}, {"id": 429, "seek": 260296, "start": 2602.96, "end": 2608.2, "text": " loaders are we don't often have to create them from scratch we can create a data block", "tokens": [3677, 433, 366, 321, 500, 380, 2049, 362, 281, 1884, 552, 490, 8459, 321, 393, 1884, 257, 1412, 3461], "temperature": 0.0, "avg_logprob": -0.08738618311674698, "compression_ratio": 1.898936170212766, "no_speech_prob": 1.3081737506581703e-06}, {"id": 430, "seek": 260296, "start": 2608.2, "end": 2614.02, "text": " to do it for us but now we can see what the data block has to do so let's see how it does", "tokens": [281, 360, 309, 337, 505, 457, 586, 321, 393, 536, 437, 264, 1412, 3461, 575, 281, 360, 370, 718, 311, 536, 577, 309, 775], "temperature": 0.0, "avg_logprob": -0.08738618311674698, "compression_ratio": 1.898936170212766, "no_speech_prob": 1.3081737506581703e-06}, {"id": 431, "seek": 260296, "start": 2614.02, "end": 2620.36, "text": " it so we can start by creating an empty data block so an empty data block is going to take", "tokens": [309, 370, 321, 393, 722, 538, 4084, 364, 6707, 1412, 3461, 370, 364, 6707, 1412, 3461, 307, 516, 281, 747], "temperature": 0.0, "avg_logprob": -0.08738618311674698, "compression_ratio": 1.898936170212766, "no_speech_prob": 1.3081737506581703e-06}, {"id": 432, "seek": 260296, "start": 2620.36, "end": 2629.08, "text": " our data frame so we're going to go back to looking at data frame which remember was this", "tokens": [527, 1412, 3920, 370, 321, 434, 516, 281, 352, 646, 281, 1237, 412, 1412, 3920, 597, 1604, 390, 341], "temperature": 0.0, "avg_logprob": -0.08738618311674698, "compression_ratio": 1.898936170212766, "no_speech_prob": 1.3081737506581703e-06}, {"id": 433, "seek": 262908, "start": 2629.08, "end": 2641.12, "text": " guy and so if we pass in our data frame we can now we'll now find that this data block", "tokens": [2146, 293, 370, 498, 321, 1320, 294, 527, 1412, 3920, 321, 393, 586, 321, 603, 586, 915, 300, 341, 1412, 3461], "temperature": 0.0, "avg_logprob": -0.07693381096000101, "compression_ratio": 1.7908496732026145, "no_speech_prob": 3.412581577322271e-07}, {"id": 434, "seek": 262908, "start": 2641.12, "end": 2648.52, "text": " has created data sets a training and a validation data set for us and if we look at the training", "tokens": [575, 2942, 1412, 6352, 257, 3097, 293, 257, 24071, 1412, 992, 337, 505, 293, 498, 321, 574, 412, 264, 3097], "temperature": 0.0, "avg_logprob": -0.07693381096000101, "compression_ratio": 1.7908496732026145, "no_speech_prob": 3.412581577322271e-07}, {"id": 435, "seek": 262908, "start": 2648.52, "end": 2653.96, "text": " set it'll give us back an independent variable and a dependent variable and we'll see that", "tokens": [992, 309, 603, 976, 505, 646, 364, 6695, 7006, 293, 257, 12334, 7006, 293, 321, 603, 536, 300], "temperature": 0.0, "avg_logprob": -0.07693381096000101, "compression_ratio": 1.7908496732026145, "no_speech_prob": 3.412581577322271e-07}, {"id": 436, "seek": 265396, "start": 2653.96, "end": 2661.0, "text": " they are both the same thing so this is the first row of the table that's actually shuffled", "tokens": [436, 366, 1293, 264, 912, 551, 370, 341, 307, 264, 700, 5386, 295, 264, 3199, 300, 311, 767, 402, 33974], "temperature": 0.0, "avg_logprob": -0.07213520526885986, "compression_ratio": 1.9350649350649352, "no_speech_prob": 2.7852684070239775e-07}, {"id": 437, "seek": 265396, "start": 2661.0, "end": 2666.7200000000003, "text": " so it's a random row of the table repeated twice and the reason for that is by default", "tokens": [370, 309, 311, 257, 4974, 5386, 295, 264, 3199, 10477, 6091, 293, 264, 1778, 337, 300, 307, 538, 7576], "temperature": 0.0, "avg_logprob": -0.07213520526885986, "compression_ratio": 1.9350649350649352, "no_speech_prob": 2.7852684070239775e-07}, {"id": 438, "seek": 265396, "start": 2666.7200000000003, "end": 2671.12, "text": " the data block assumes that we have two things the independent variable and the dependent", "tokens": [264, 1412, 3461, 37808, 300, 321, 362, 732, 721, 264, 6695, 7006, 293, 264, 12334], "temperature": 0.0, "avg_logprob": -0.07213520526885986, "compression_ratio": 1.9350649350649352, "no_speech_prob": 2.7852684070239775e-07}, {"id": 439, "seek": 265396, "start": 2671.12, "end": 2676.8, "text": " or the input and the target and by default it just copies it just keeps exactly whatever", "tokens": [420, 264, 4846, 293, 264, 3779, 293, 538, 7576, 309, 445, 14341, 309, 445, 5965, 2293, 2035], "temperature": 0.0, "avg_logprob": -0.07213520526885986, "compression_ratio": 1.9350649350649352, "no_speech_prob": 2.7852684070239775e-07}, {"id": 440, "seek": 265396, "start": 2676.8, "end": 2683.08, "text": " you gave it to create the training set and the validation set by default it just randomly", "tokens": [291, 2729, 309, 281, 1884, 264, 3097, 992, 293, 264, 24071, 992, 538, 7576, 309, 445, 16979], "temperature": 0.0, "avg_logprob": -0.07213520526885986, "compression_ratio": 1.9350649350649352, "no_speech_prob": 2.7852684070239775e-07}, {"id": 441, "seek": 268308, "start": 2683.08, "end": 2690.2, "text": " splits the data with a 20% validation set so that's what's happened here so this is", "tokens": [37741, 264, 1412, 365, 257, 945, 4, 24071, 992, 370, 300, 311, 437, 311, 2011, 510, 370, 341, 307], "temperature": 0.0, "avg_logprob": -0.1307318743537454, "compression_ratio": 1.6394230769230769, "no_speech_prob": 1.5056963320603245e-06}, {"id": 442, "seek": 268308, "start": 2690.2, "end": 2696.6, "text": " not much use and what we actually want to do if we look at X for example is grab the", "tokens": [406, 709, 764, 293, 437, 321, 767, 528, 281, 360, 498, 321, 574, 412, 1783, 337, 1365, 307, 4444, 264], "temperature": 0.0, "avg_logprob": -0.1307318743537454, "compression_ratio": 1.6394230769230769, "no_speech_prob": 1.5056963320603245e-06}, {"id": 443, "seek": 268308, "start": 2696.6, "end": 2701.16, "text": " F name the file name field because we want to open this image that's going to be our", "tokens": [479, 1315, 264, 3991, 1315, 2519, 570, 321, 528, 281, 1269, 341, 3256, 300, 311, 516, 281, 312, 527], "temperature": 0.0, "avg_logprob": -0.1307318743537454, "compression_ratio": 1.6394230769230769, "no_speech_prob": 1.5056963320603245e-06}, {"id": 444, "seek": 268308, "start": 2701.16, "end": 2712.56, "text": " independent variable and then for the label we're going to want this here person cat so", "tokens": [6695, 7006, 293, 550, 337, 264, 7645, 321, 434, 516, 281, 528, 341, 510, 954, 3857, 370], "temperature": 0.0, "avg_logprob": -0.1307318743537454, "compression_ratio": 1.6394230769230769, "no_speech_prob": 1.5056963320603245e-06}, {"id": 445, "seek": 271256, "start": 2712.56, "end": 2720.72, "text": " we can actually pass these as parameters get X and get Y functions that return the bit", "tokens": [321, 393, 767, 1320, 613, 382, 9834, 483, 1783, 293, 483, 398, 6828, 300, 2736, 264, 857], "temperature": 0.0, "avg_logprob": -0.08631929131441338, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.1911065485037398e-06}, {"id": 446, "seek": 271256, "start": 2720.72, "end": 2727.16, "text": " of data that we want and so you can create and use a function in the same line of code", "tokens": [295, 1412, 300, 321, 528, 293, 370, 291, 393, 1884, 293, 764, 257, 2445, 294, 264, 912, 1622, 295, 3089], "temperature": 0.0, "avg_logprob": -0.08631929131441338, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.1911065485037398e-06}, {"id": 447, "seek": 271256, "start": 2727.16, "end": 2734.12, "text": " in Python by saying lambda so lambda R means create a function doesn't have a name it's", "tokens": [294, 15329, 538, 1566, 13607, 370, 13607, 497, 1355, 1884, 257, 2445, 1177, 380, 362, 257, 1315, 309, 311], "temperature": 0.0, "avg_logprob": -0.08631929131441338, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.1911065485037398e-06}, {"id": 448, "seek": 271256, "start": 2734.12, "end": 2739.24, "text": " going to take a parameter called R we don't even have to say return it's going to return", "tokens": [516, 281, 747, 257, 13075, 1219, 497, 321, 500, 380, 754, 362, 281, 584, 2736, 309, 311, 516, 281, 2736], "temperature": 0.0, "avg_logprob": -0.08631929131441338, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.1911065485037398e-06}, {"id": 449, "seek": 273924, "start": 2739.24, "end": 2747.08, "text": " the F name column in this case and get Y is something which is a function that takes an", "tokens": [264, 479, 1315, 7738, 294, 341, 1389, 293, 483, 398, 307, 746, 597, 307, 257, 2445, 300, 2516, 364], "temperature": 0.0, "avg_logprob": -0.11134588718414307, "compression_ratio": 1.7024390243902439, "no_speech_prob": 7.453755870301393e-07}, {"id": 450, "seek": 273924, "start": 2747.08, "end": 2754.3199999999997, "text": " R and returns the labels column so now we can do the same thing called D block dot data", "tokens": [497, 293, 11247, 264, 16949, 7738, 370, 586, 321, 393, 360, 264, 912, 551, 1219, 413, 3461, 5893, 1412], "temperature": 0.0, "avg_logprob": -0.11134588718414307, "compression_ratio": 1.7024390243902439, "no_speech_prob": 7.453755870301393e-07}, {"id": 451, "seek": 273924, "start": 2754.3199999999997, "end": 2758.7599999999998, "text": " sets we can grab a row from that from the training set and you can see look here it", "tokens": [6352, 321, 393, 4444, 257, 5386, 490, 300, 490, 264, 3097, 992, 293, 291, 393, 536, 574, 510, 309], "temperature": 0.0, "avg_logprob": -0.11134588718414307, "compression_ratio": 1.7024390243902439, "no_speech_prob": 7.453755870301393e-07}, {"id": 452, "seek": 273924, "start": 2758.7599999999998, "end": 2768.52, "text": " is there is the image file name and there is the space delimited list of labels so here's", "tokens": [307, 456, 307, 264, 3256, 3991, 1315, 293, 456, 307, 264, 1901, 1103, 332, 1226, 1329, 295, 16949, 370, 510, 311], "temperature": 0.0, "avg_logprob": -0.11134588718414307, "compression_ratio": 1.7024390243902439, "no_speech_prob": 7.453755870301393e-07}, {"id": 453, "seek": 276852, "start": 2768.52, "end": 2774.56, "text": " exactly the same thing again but done with functions okay so now the one line of code", "tokens": [2293, 264, 912, 551, 797, 457, 1096, 365, 6828, 1392, 370, 586, 264, 472, 1622, 295, 3089], "temperature": 0.0, "avg_logprob": -0.11604796648025513, "compression_ratio": 1.8385416666666667, "no_speech_prob": 1.4823557421550504e-06}, {"id": 454, "seek": 276852, "start": 2774.56, "end": 2781.96, "text": " above has become three lines of code but it does exactly the same thing okay we don't", "tokens": [3673, 575, 1813, 1045, 3876, 295, 3089, 457, 309, 775, 2293, 264, 912, 551, 1392, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.11604796648025513, "compression_ratio": 1.8385416666666667, "no_speech_prob": 1.4823557421550504e-06}, {"id": 455, "seek": 276852, "start": 2781.96, "end": 2787.24, "text": " get back the same result because the training set well wait why don't we get the same result", "tokens": [483, 646, 264, 912, 1874, 570, 264, 3097, 992, 731, 1699, 983, 500, 380, 321, 483, 264, 912, 1874], "temperature": 0.0, "avg_logprob": -0.11604796648025513, "compression_ratio": 1.8385416666666667, "no_speech_prob": 1.4823557421550504e-06}, {"id": 456, "seek": 276852, "start": 2787.24, "end": 2796.84, "text": " oh I know why because it's randomly shuffle it's randomly picking a different validation", "tokens": [1954, 286, 458, 983, 570, 309, 311, 16979, 39426, 309, 311, 16979, 8867, 257, 819, 24071], "temperature": 0.0, "avg_logprob": -0.11604796648025513, "compression_ratio": 1.8385416666666667, "no_speech_prob": 1.4823557421550504e-06}, {"id": 457, "seek": 279684, "start": 2796.84, "end": 2802.1400000000003, "text": " set because the random split is done differently each time so that's why we don't get the same", "tokens": [992, 570, 264, 4974, 7472, 307, 1096, 7614, 1184, 565, 370, 300, 311, 983, 321, 500, 380, 483, 264, 912], "temperature": 0.0, "avg_logprob": -0.10152726560025602, "compression_ratio": 1.7509578544061302, "no_speech_prob": 9.874619308902766e-07}, {"id": 458, "seek": 279684, "start": 2802.1400000000003, "end": 2804.4, "text": " result.", "tokens": [1874, 13], "temperature": 0.0, "avg_logprob": -0.10152726560025602, "compression_ratio": 1.7509578544061302, "no_speech_prob": 9.874619308902766e-07}, {"id": 459, "seek": 279684, "start": 2804.4, "end": 2811.44, "text": " One thing to note be careful of lambdas if you want to save this data block for use later", "tokens": [1485, 551, 281, 3637, 312, 5026, 295, 10097, 27476, 498, 291, 528, 281, 3155, 341, 1412, 3461, 337, 764, 1780], "temperature": 0.0, "avg_logprob": -0.10152726560025602, "compression_ratio": 1.7509578544061302, "no_speech_prob": 9.874619308902766e-07}, {"id": 460, "seek": 279684, "start": 2811.44, "end": 2816.8, "text": " you won't be able to Python doesn't like saving things that contain lambdas so most of the", "tokens": [291, 1582, 380, 312, 1075, 281, 15329, 1177, 380, 411, 6816, 721, 300, 5304, 10097, 27476, 370, 881, 295, 264], "temperature": 0.0, "avg_logprob": -0.10152726560025602, "compression_ratio": 1.7509578544061302, "no_speech_prob": 9.874619308902766e-07}, {"id": 461, "seek": 279684, "start": 2816.8, "end": 2821.52, "text": " time in the book and the course we normally use avoid lambdas for that reason because", "tokens": [565, 294, 264, 1446, 293, 264, 1164, 321, 5646, 764, 5042, 10097, 27476, 337, 300, 1778, 570], "temperature": 0.0, "avg_logprob": -0.10152726560025602, "compression_ratio": 1.7509578544061302, "no_speech_prob": 9.874619308902766e-07}, {"id": 462, "seek": 279684, "start": 2821.52, "end": 2826.32, "text": " it's often very convenient to be able to save things we use the word here serialization", "tokens": [309, 311, 2049, 588, 10851, 281, 312, 1075, 281, 3155, 721, 321, 764, 264, 1349, 510, 17436, 2144], "temperature": 0.0, "avg_logprob": -0.10152726560025602, "compression_ratio": 1.7509578544061302, "no_speech_prob": 9.874619308902766e-07}, {"id": 463, "seek": 282632, "start": 2826.32, "end": 2832.1800000000003, "text": " that just means basically it means saving something.", "tokens": [300, 445, 1355, 1936, 309, 1355, 6816, 746, 13], "temperature": 0.0, "avg_logprob": -0.11939050038655599, "compression_ratio": 1.656084656084656, "no_speech_prob": 4.22281163992011e-06}, {"id": 464, "seek": 282632, "start": 2832.1800000000003, "end": 2838.96, "text": " This is not enough to open an image because we don't have the path so to turn this into", "tokens": [639, 307, 406, 1547, 281, 1269, 364, 3256, 570, 321, 500, 380, 362, 264, 3100, 370, 281, 1261, 341, 666], "temperature": 0.0, "avg_logprob": -0.11939050038655599, "compression_ratio": 1.656084656084656, "no_speech_prob": 4.22281163992011e-06}, {"id": 465, "seek": 282632, "start": 2838.96, "end": 2844.6600000000003, "text": " so rather than just using this function to grab the F name column we should actually", "tokens": [370, 2831, 813, 445, 1228, 341, 2445, 281, 4444, 264, 479, 1315, 7738, 321, 820, 767], "temperature": 0.0, "avg_logprob": -0.11939050038655599, "compression_ratio": 1.656084656084656, "no_speech_prob": 4.22281163992011e-06}, {"id": 466, "seek": 282632, "start": 2844.6600000000003, "end": 2854.1000000000004, "text": " use pathlib to go path train and then column and then for the y again the labels is not", "tokens": [764, 3100, 38270, 281, 352, 3100, 3847, 293, 550, 7738, 293, 550, 337, 264, 288, 797, 264, 16949, 307, 406], "temperature": 0.0, "avg_logprob": -0.11939050038655599, "compression_ratio": 1.656084656084656, "no_speech_prob": 4.22281163992011e-06}, {"id": 467, "seek": 285410, "start": 2854.1, "end": 2858.04, "text": " quite enough we actually have to split on space but this is Python we can use any function", "tokens": [1596, 1547, 321, 767, 362, 281, 7472, 322, 1901, 457, 341, 307, 15329, 321, 393, 764, 604, 2445], "temperature": 0.0, "avg_logprob": -0.10382258019796232, "compression_ratio": 1.5829145728643217, "no_speech_prob": 6.179377010084863e-07}, {"id": 468, "seek": 285410, "start": 2858.04, "end": 2863.08, "text": " we like and so then we use the same three lines of code is here and now we've got a", "tokens": [321, 411, 293, 370, 550, 321, 764, 264, 912, 1045, 3876, 295, 3089, 307, 510, 293, 586, 321, 600, 658, 257], "temperature": 0.0, "avg_logprob": -0.10382258019796232, "compression_ratio": 1.5829145728643217, "no_speech_prob": 6.179377010084863e-07}, {"id": 469, "seek": 285410, "start": 2863.08, "end": 2870.44, "text": " path and a list of labels so that's looking good.", "tokens": [3100, 293, 257, 1329, 295, 16949, 370, 300, 311, 1237, 665, 13], "temperature": 0.0, "avg_logprob": -0.10382258019796232, "compression_ratio": 1.5829145728643217, "no_speech_prob": 6.179377010084863e-07}, {"id": 470, "seek": 285410, "start": 2870.44, "end": 2877.68, "text": " So we want this path to be opened as an image so the data block API lets you pass a blocks", "tokens": [407, 321, 528, 341, 3100, 281, 312, 5625, 382, 364, 3256, 370, 264, 1412, 3461, 9362, 6653, 291, 1320, 257, 8474], "temperature": 0.0, "avg_logprob": -0.10382258019796232, "compression_ratio": 1.5829145728643217, "no_speech_prob": 6.179377010084863e-07}, {"id": 471, "seek": 287768, "start": 2877.68, "end": 2884.24, "text": " argument where you tell it for each of the things in your tuple so there's two of them", "tokens": [6770, 689, 291, 980, 309, 337, 1184, 295, 264, 721, 294, 428, 2604, 781, 370, 456, 311, 732, 295, 552], "temperature": 0.0, "avg_logprob": -0.08300498940727928, "compression_ratio": 1.8131313131313131, "no_speech_prob": 1.1726385764632141e-06}, {"id": 472, "seek": 287768, "start": 2884.24, "end": 2890.2799999999997, "text": " what kind of block do you need so we need an image block to open an image and then in", "tokens": [437, 733, 295, 3461, 360, 291, 643, 370, 321, 643, 364, 3256, 3461, 281, 1269, 364, 3256, 293, 550, 294], "temperature": 0.0, "avg_logprob": -0.08300498940727928, "compression_ratio": 1.8131313131313131, "no_speech_prob": 1.1726385764632141e-06}, {"id": 473, "seek": 287768, "start": 2890.2799999999997, "end": 2895.96, "text": " the past we've used a category block or categorical variables but this time we don't have a single", "tokens": [264, 1791, 321, 600, 1143, 257, 7719, 3461, 420, 19250, 804, 9102, 457, 341, 565, 321, 500, 380, 362, 257, 2167], "temperature": 0.0, "avg_logprob": -0.08300498940727928, "compression_ratio": 1.8131313131313131, "no_speech_prob": 1.1726385764632141e-06}, {"id": 474, "seek": 287768, "start": 2895.96, "end": 2901.6, "text": " category we've got multiple categories so we have to use a multi category block so once", "tokens": [7719, 321, 600, 658, 3866, 10479, 370, 321, 362, 281, 764, 257, 4825, 7719, 3461, 370, 1564], "temperature": 0.0, "avg_logprob": -0.08300498940727928, "compression_ratio": 1.8131313131313131, "no_speech_prob": 1.1726385764632141e-06}, {"id": 475, "seek": 290160, "start": 2901.6, "end": 2909.36, "text": " we do that and have a look we now have an 500 by 375 image as our independent variable", "tokens": [321, 360, 300, 293, 362, 257, 574, 321, 586, 362, 364, 5923, 538, 805, 11901, 3256, 382, 527, 6695, 7006], "temperature": 0.0, "avg_logprob": -0.11192619055509567, "compression_ratio": 1.678082191780822, "no_speech_prob": 4.0525412714487175e-07}, {"id": 476, "seek": 290160, "start": 2909.36, "end": 2917.96, "text": " and as a dependent variable we have a long list of zeros and ones.", "tokens": [293, 382, 257, 12334, 7006, 321, 362, 257, 938, 1329, 295, 35193, 293, 2306, 13], "temperature": 0.0, "avg_logprob": -0.11192619055509567, "compression_ratio": 1.678082191780822, "no_speech_prob": 4.0525412714487175e-07}, {"id": 477, "seek": 290160, "start": 2917.96, "end": 2929.2799999999997, "text": " The long list of zeros and ones is the labels as a one hot encoded vector a rank one tensor", "tokens": [440, 938, 1329, 295, 35193, 293, 2306, 307, 264, 16949, 382, 257, 472, 2368, 2058, 12340, 8062, 257, 6181, 472, 40863], "temperature": 0.0, "avg_logprob": -0.11192619055509567, "compression_ratio": 1.678082191780822, "no_speech_prob": 4.0525412714487175e-07}, {"id": 478, "seek": 292928, "start": 2929.28, "end": 2938.88, "text": " and specifically there will be a zero in every location where in the vocab where there is", "tokens": [293, 4682, 456, 486, 312, 257, 4018, 294, 633, 4914, 689, 294, 264, 2329, 455, 689, 456, 307], "temperature": 0.0, "avg_logprob": -0.13555212509937775, "compression_ratio": 1.8695652173913044, "no_speech_prob": 5.539162657441921e-07}, {"id": 479, "seek": 292928, "start": 2938.88, "end": 2944.76, "text": " not that kind of object in this image and a one in every location where there is so", "tokens": [406, 300, 733, 295, 2657, 294, 341, 3256, 293, 257, 472, 294, 633, 4914, 689, 456, 307, 370], "temperature": 0.0, "avg_logprob": -0.13555212509937775, "compression_ratio": 1.8695652173913044, "no_speech_prob": 5.539162657441921e-07}, {"id": 480, "seek": 292928, "start": 2944.76, "end": 2949.92, "text": " for this one there's just a person so this must be the location in the vocab where there's", "tokens": [337, 341, 472, 456, 311, 445, 257, 954, 370, 341, 1633, 312, 264, 4914, 294, 264, 2329, 455, 689, 456, 311], "temperature": 0.0, "avg_logprob": -0.13555212509937775, "compression_ratio": 1.8695652173913044, "no_speech_prob": 5.539162657441921e-07}, {"id": 481, "seek": 292928, "start": 2949.92, "end": 2950.92, "text": " a person.", "tokens": [257, 954, 13], "temperature": 0.0, "avg_logprob": -0.13555212509937775, "compression_ratio": 1.8695652173913044, "no_speech_prob": 5.539162657441921e-07}, {"id": 482, "seek": 292928, "start": 2950.92, "end": 2955.44, "text": " Do you have any questions?", "tokens": [1144, 291, 362, 604, 1651, 30], "temperature": 0.0, "avg_logprob": -0.13555212509937775, "compression_ratio": 1.8695652173913044, "no_speech_prob": 5.539162657441921e-07}, {"id": 483, "seek": 295544, "start": 2955.44, "end": 2961.12, "text": " So one hot encoding is a very important concept and we didn't have to use it before right", "tokens": [407, 472, 2368, 43430, 307, 257, 588, 1021, 3410, 293, 321, 994, 380, 362, 281, 764, 309, 949, 558], "temperature": 0.0, "avg_logprob": -0.09628456340116613, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.903376301015669e-06}, {"id": 484, "seek": 295544, "start": 2961.12, "end": 2968.88, "text": " we could just have a single integer saying which one thing is it but when we've got lots", "tokens": [321, 727, 445, 362, 257, 2167, 24922, 1566, 597, 472, 551, 307, 309, 457, 562, 321, 600, 658, 3195], "temperature": 0.0, "avg_logprob": -0.09628456340116613, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.903376301015669e-06}, {"id": 485, "seek": 295544, "start": 2968.88, "end": 2975.2000000000003, "text": " of things lots of potential labels it's it's convenient to use this one hot encoding and", "tokens": [295, 721, 3195, 295, 3995, 16949, 309, 311, 309, 311, 10851, 281, 764, 341, 472, 2368, 43430, 293], "temperature": 0.0, "avg_logprob": -0.09628456340116613, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.903376301015669e-06}, {"id": 486, "seek": 295544, "start": 2975.2000000000003, "end": 2980.12, "text": " it's kind of what it's actually what's going to happen with them with the actual matrices", "tokens": [309, 311, 733, 295, 437, 309, 311, 767, 437, 311, 516, 281, 1051, 365, 552, 365, 264, 3539, 32284], "temperature": 0.0, "avg_logprob": -0.09628456340116613, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.903376301015669e-06}, {"id": 487, "seek": 298012, "start": 2980.12, "end": 2991.3599999999997, "text": " anyway when we actually compare the activations of our neural network to the target it's actually", "tokens": [4033, 562, 321, 767, 6794, 264, 2430, 763, 295, 527, 18161, 3209, 281, 264, 3779, 309, 311, 767], "temperature": 0.0, "avg_logprob": -0.1002061196735927, "compression_ratio": 1.5298013245033113, "no_speech_prob": 9.874604529613862e-07}, {"id": 488, "seek": 298012, "start": 2991.3599999999997, "end": 2997.24, "text": " going to be comparing each one of these.", "tokens": [516, 281, 312, 15763, 1184, 472, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.1002061196735927, "compression_ratio": 1.5298013245033113, "no_speech_prob": 9.874604529613862e-07}, {"id": 489, "seek": 298012, "start": 2997.24, "end": 3004.24, "text": " Okay so the categories as I mentioned is based on the vocab where we can grab the vocab from", "tokens": [1033, 370, 264, 10479, 382, 286, 2835, 307, 2361, 322, 264, 2329, 455, 689, 321, 393, 4444, 264, 2329, 455, 490], "temperature": 0.0, "avg_logprob": -0.1002061196735927, "compression_ratio": 1.5298013245033113, "no_speech_prob": 9.874604529613862e-07}, {"id": 490, "seek": 300424, "start": 3004.24, "end": 3011.3999999999996, "text": " our data set subject and then we can say okay let's look at the first row and let's look", "tokens": [527, 1412, 992, 3983, 293, 550, 321, 393, 584, 1392, 718, 311, 574, 412, 264, 700, 5386, 293, 718, 311, 574], "temperature": 0.0, "avg_logprob": -0.08482273145653736, "compression_ratio": 1.8333333333333333, "no_speech_prob": 5.539163225876109e-07}, {"id": 491, "seek": 300424, "start": 3011.3999999999996, "end": 3022.24, "text": " at the dependent variable and let's look for where the dependent variable is one okay and", "tokens": [412, 264, 12334, 7006, 293, 718, 311, 574, 337, 689, 264, 12334, 7006, 307, 472, 1392, 293], "temperature": 0.0, "avg_logprob": -0.08482273145653736, "compression_ratio": 1.8333333333333333, "no_speech_prob": 5.539163225876109e-07}, {"id": 492, "seek": 300424, "start": 3022.24, "end": 3026.56, "text": " then we can have a look past those indexes with the vocab and get back a list of what", "tokens": [550, 321, 393, 362, 257, 574, 1791, 729, 8186, 279, 365, 264, 2329, 455, 293, 483, 646, 257, 1329, 295, 437], "temperature": 0.0, "avg_logprob": -0.08482273145653736, "compression_ratio": 1.8333333333333333, "no_speech_prob": 5.539163225876109e-07}, {"id": 493, "seek": 300424, "start": 3026.56, "end": 3033.56, "text": " it actually was there and again each time I run this I'm going to get different results", "tokens": [309, 767, 390, 456, 293, 797, 1184, 565, 286, 1190, 341, 286, 478, 516, 281, 483, 819, 3542], "temperature": 0.0, "avg_logprob": -0.08482273145653736, "compression_ratio": 1.8333333333333333, "no_speech_prob": 5.539163225876109e-07}, {"id": 494, "seek": 303356, "start": 3033.56, "end": 3036.64, "text": " so each time we run this we're going to get different results because they called dot", "tokens": [370, 1184, 565, 321, 1190, 341, 321, 434, 516, 281, 483, 819, 3542, 570, 436, 1219, 5893], "temperature": 0.0, "avg_logprob": -0.12368288272764624, "compression_ratio": 1.7236180904522613, "no_speech_prob": 1.7603306332603097e-06}, {"id": 495, "seek": 303356, "start": 3036.64, "end": 3041.24, "text": " data sets again here so it's going to give me a different train test split and so this", "tokens": [1412, 6352, 797, 510, 370, 309, 311, 516, 281, 976, 385, 257, 819, 3847, 1500, 7472, 293, 370, 341], "temperature": 0.0, "avg_logprob": -0.12368288272764624, "compression_ratio": 1.7236180904522613, "no_speech_prob": 1.7603306332603097e-06}, {"id": 496, "seek": 303356, "start": 3041.24, "end": 3048.48, "text": " time it turns out that this is actually a chair and we have a question shouldn't the", "tokens": [565, 309, 4523, 484, 300, 341, 307, 767, 257, 6090, 293, 321, 362, 257, 1168, 4659, 380, 264], "temperature": 0.0, "avg_logprob": -0.12368288272764624, "compression_ratio": 1.7236180904522613, "no_speech_prob": 1.7603306332603097e-06}, {"id": 497, "seek": 303356, "start": 3048.48, "end": 3056.64, "text": " tensor be of integers why is it a tensor of floats yeah conceptually this is a tensor", "tokens": [40863, 312, 295, 41674, 983, 307, 309, 257, 40863, 295, 37878, 1338, 3410, 671, 341, 307, 257, 40863], "temperature": 0.0, "avg_logprob": -0.12368288272764624, "compression_ratio": 1.7236180904522613, "no_speech_prob": 1.7603306332603097e-06}, {"id": 498, "seek": 305664, "start": 3056.64, "end": 3068.6, "text": " of integers they can only be 0 or 1 but we we're going to be using a cross entropy style", "tokens": [295, 41674, 436, 393, 787, 312, 1958, 420, 502, 457, 321, 321, 434, 516, 281, 312, 1228, 257, 3278, 30867, 3758], "temperature": 0.0, "avg_logprob": -0.09661711586846246, "compression_ratio": 1.7339901477832513, "no_speech_prob": 8.315259378832707e-07}, {"id": 499, "seek": 305664, "start": 3068.6, "end": 3074.44, "text": " loss function but we're going to actually need to do floating point calculations on", "tokens": [4470, 2445, 457, 321, 434, 516, 281, 767, 643, 281, 360, 12607, 935, 20448, 322], "temperature": 0.0, "avg_logprob": -0.09661711586846246, "compression_ratio": 1.7339901477832513, "no_speech_prob": 8.315259378832707e-07}, {"id": 500, "seek": 305664, "start": 3074.44, "end": 3080.02, "text": " them that's going to be faster to just store them as float in the first place rather than", "tokens": [552, 300, 311, 516, 281, 312, 4663, 281, 445, 3531, 552, 382, 15706, 294, 264, 700, 1081, 2831, 813], "temperature": 0.0, "avg_logprob": -0.09661711586846246, "compression_ratio": 1.7339901477832513, "no_speech_prob": 8.315259378832707e-07}, {"id": 501, "seek": 305664, "start": 3080.02, "end": 3083.52, "text": " converting backwards and forwards even though they're conceptually an int we're not going", "tokens": [29942, 12204, 293, 30126, 754, 1673, 436, 434, 3410, 671, 364, 560, 321, 434, 406, 516], "temperature": 0.0, "avg_logprob": -0.09661711586846246, "compression_ratio": 1.7339901477832513, "no_speech_prob": 8.315259378832707e-07}, {"id": 502, "seek": 308352, "start": 3083.52, "end": 3093.08, "text": " to be doing kind of int style calculations with them good question I mentioned that by", "tokens": [281, 312, 884, 733, 295, 560, 3758, 20448, 365, 552, 665, 1168, 286, 2835, 300, 538], "temperature": 0.0, "avg_logprob": -0.08198653674516522, "compression_ratio": 1.576470588235294, "no_speech_prob": 1.444979460529794e-07}, {"id": 503, "seek": 308352, "start": 3093.08, "end": 3106.92, "text": " default the data block uses a random split you might have noticed in the data frame though", "tokens": [7576, 264, 1412, 3461, 4960, 257, 4974, 7472, 291, 1062, 362, 5694, 294, 264, 1412, 3920, 1673], "temperature": 0.0, "avg_logprob": -0.08198653674516522, "compression_ratio": 1.576470588235294, "no_speech_prob": 1.444979460529794e-07}, {"id": 504, "seek": 308352, "start": 3106.92, "end": 3112.24, "text": " it said here's a column saying what validation set to use and if the data set you're given", "tokens": [309, 848, 510, 311, 257, 7738, 1566, 437, 24071, 992, 281, 764, 293, 498, 264, 1412, 992, 291, 434, 2212], "temperature": 0.0, "avg_logprob": -0.08198653674516522, "compression_ratio": 1.576470588235294, "no_speech_prob": 1.444979460529794e-07}, {"id": 505, "seek": 311224, "start": 3112.24, "end": 3116.64, "text": " tells you what validation set to use you should generally use it because that way you can", "tokens": [5112, 291, 437, 24071, 992, 281, 764, 291, 820, 5101, 764, 309, 570, 300, 636, 291, 393], "temperature": 0.0, "avg_logprob": -0.06186896000268324, "compression_ratio": 2.0829493087557602, "no_speech_prob": 3.596839448505307e-08}, {"id": 506, "seek": 311224, "start": 3116.64, "end": 3124.04, "text": " compare your validation set results to somebody else's so you can pass a splitter argument", "tokens": [6794, 428, 24071, 992, 3542, 281, 2618, 1646, 311, 370, 291, 393, 1320, 257, 4732, 3904, 6770], "temperature": 0.0, "avg_logprob": -0.06186896000268324, "compression_ratio": 2.0829493087557602, "no_speech_prob": 3.596839448505307e-08}, {"id": 507, "seek": 311224, "start": 3124.04, "end": 3128.8399999999997, "text": " which again is a function and so we're going to pass it a function that's also called splitter", "tokens": [597, 797, 307, 257, 2445, 293, 370, 321, 434, 516, 281, 1320, 309, 257, 2445, 300, 311, 611, 1219, 4732, 3904], "temperature": 0.0, "avg_logprob": -0.06186896000268324, "compression_ratio": 2.0829493087557602, "no_speech_prob": 3.596839448505307e-08}, {"id": 508, "seek": 311224, "start": 3128.8399999999997, "end": 3136.56, "text": " and the function is going to return the indexes where it's not valid and that's going to be", "tokens": [293, 264, 2445, 307, 516, 281, 2736, 264, 8186, 279, 689, 309, 311, 406, 7363, 293, 300, 311, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.06186896000268324, "compression_ratio": 2.0829493087557602, "no_speech_prob": 3.596839448505307e-08}, {"id": 509, "seek": 311224, "start": 3136.56, "end": 3140.62, "text": " the training set and the indexes where it is valid that's going to be the validation", "tokens": [264, 3097, 992, 293, 264, 8186, 279, 689, 309, 307, 7363, 300, 311, 516, 281, 312, 264, 24071], "temperature": 0.0, "avg_logprob": -0.06186896000268324, "compression_ratio": 2.0829493087557602, "no_speech_prob": 3.596839448505307e-08}, {"id": 510, "seek": 314062, "start": 3140.62, "end": 3146.92, "text": " set and so the splitter argument is expected to return two lists of integers and so if", "tokens": [992, 293, 370, 264, 4732, 3904, 6770, 307, 5176, 281, 2736, 732, 14511, 295, 41674, 293, 370, 498], "temperature": 0.0, "avg_logprob": -0.10198031097161965, "compression_ratio": 1.5654761904761905, "no_speech_prob": 3.359673428349197e-07}, {"id": 511, "seek": 314062, "start": 3146.92, "end": 3152.24, "text": " we do that we get again the same thing but now we're using the correct train and validation", "tokens": [321, 360, 300, 321, 483, 797, 264, 912, 551, 457, 586, 321, 434, 1228, 264, 3006, 3847, 293, 24071], "temperature": 0.0, "avg_logprob": -0.10198031097161965, "compression_ratio": 1.5654761904761905, "no_speech_prob": 3.359673428349197e-07}, {"id": 512, "seek": 314062, "start": 3152.24, "end": 3162.8399999999997, "text": " sets another question sure any particular reason we don't use floating point 8 is it", "tokens": [6352, 1071, 1168, 988, 604, 1729, 1778, 321, 500, 380, 764, 12607, 935, 1649, 307, 309], "temperature": 0.0, "avg_logprob": -0.10198031097161965, "compression_ratio": 1.5654761904761905, "no_speech_prob": 3.359673428349197e-07}, {"id": 513, "seek": 316284, "start": 3162.84, "end": 3172.8, "text": " just that the precision is too low yeah trying to train with 8-bit precision is super difficult", "tokens": [445, 300, 264, 18356, 307, 886, 2295, 1338, 1382, 281, 3847, 365, 1649, 12, 5260, 18356, 307, 1687, 2252], "temperature": 0.0, "avg_logprob": -0.14265701132760922, "compression_ratio": 1.6104651162790697, "no_speech_prob": 4.313901058594638e-07}, {"id": 514, "seek": 316284, "start": 3172.8, "end": 3182.4, "text": " it's it's so flat and bumpy it's pretty difficult to get decent gradients there but you know", "tokens": [309, 311, 309, 311, 370, 4962, 293, 49400, 309, 311, 1238, 2252, 281, 483, 8681, 2771, 2448, 456, 457, 291, 458], "temperature": 0.0, "avg_logprob": -0.14265701132760922, "compression_ratio": 1.6104651162790697, "no_speech_prob": 4.313901058594638e-07}, {"id": 515, "seek": 316284, "start": 3182.4, "end": 3189.08, "text": " it's an area of research the main thing people do with 8-bit or even 1-bit data types is", "tokens": [309, 311, 364, 1859, 295, 2132, 264, 2135, 551, 561, 360, 365, 1649, 12, 5260, 420, 754, 502, 12, 5260, 1412, 3467, 307], "temperature": 0.0, "avg_logprob": -0.14265701132760922, "compression_ratio": 1.6104651162790697, "no_speech_prob": 4.313901058594638e-07}, {"id": 516, "seek": 318908, "start": 3189.08, "end": 3194.08, "text": " they take a model that's already been trained with 16-bit or 32-bit floating point and then", "tokens": [436, 747, 257, 2316, 300, 311, 1217, 668, 8895, 365, 3165, 12, 5260, 420, 8858, 12, 5260, 12607, 935, 293, 550], "temperature": 0.0, "avg_logprob": -0.06647370755672455, "compression_ratio": 1.489010989010989, "no_speech_prob": 3.011587921264436e-07}, {"id": 517, "seek": 318908, "start": 3194.08, "end": 3200.92, "text": " they kind of round it off it's called discretizing to create a kind of purely integer or even", "tokens": [436, 733, 295, 3098, 309, 766, 309, 311, 1219, 25656, 3319, 281, 1884, 257, 733, 295, 17491, 24922, 420, 754], "temperature": 0.0, "avg_logprob": -0.06647370755672455, "compression_ratio": 1.489010989010989, "no_speech_prob": 3.011587921264436e-07}, {"id": 518, "seek": 318908, "start": 3200.92, "end": 3209.3199999999997, "text": " binary network which can do inference much faster figuring out how to train with such", "tokens": [17434, 3209, 597, 393, 360, 38253, 709, 4663, 15213, 484, 577, 281, 3847, 365, 1270], "temperature": 0.0, "avg_logprob": -0.06647370755672455, "compression_ratio": 1.489010989010989, "no_speech_prob": 3.011587921264436e-07}, {"id": 519, "seek": 320932, "start": 3209.32, "end": 3221.44, "text": " low precision data is an area of active research I suspect it's possible and I suspect I mean", "tokens": [2295, 18356, 1412, 307, 364, 1859, 295, 4967, 2132, 286, 9091, 309, 311, 1944, 293, 286, 9091, 286, 914], "temperature": 0.0, "avg_logprob": -0.09305096821612623, "compression_ratio": 1.6108597285067874, "no_speech_prob": 1.8448141645421856e-06}, {"id": 520, "seek": 320932, "start": 3221.44, "end": 3225.0, "text": " people have fiddled around it and had some success I think you know it could turn out", "tokens": [561, 362, 283, 14273, 1493, 926, 309, 293, 632, 512, 2245, 286, 519, 291, 458, 309, 727, 1261, 484], "temperature": 0.0, "avg_logprob": -0.09305096821612623, "compression_ratio": 1.6108597285067874, "no_speech_prob": 1.8448141645421856e-06}, {"id": 521, "seek": 320932, "start": 3225.0, "end": 3229.8, "text": " to be super interesting particularly for stuff that's been done on like low-powered devices", "tokens": [281, 312, 1687, 1880, 4098, 337, 1507, 300, 311, 668, 1096, 322, 411, 2295, 12, 27178, 5759], "temperature": 0.0, "avg_logprob": -0.09305096821612623, "compression_ratio": 1.6108597285067874, "no_speech_prob": 1.8448141645421856e-06}, {"id": 522, "seek": 320932, "start": 3229.8, "end": 3238.28, "text": " that might not even have a floating point unit right so the last thing we need to do", "tokens": [300, 1062, 406, 754, 362, 257, 12607, 935, 4985, 558, 370, 264, 1036, 551, 321, 643, 281, 360], "temperature": 0.0, "avg_logprob": -0.09305096821612623, "compression_ratio": 1.6108597285067874, "no_speech_prob": 1.8448141645421856e-06}, {"id": 523, "seek": 323828, "start": 3238.28, "end": 3243.2000000000003, "text": " is to add our item transforms random resource crop we've talked about that enough so I won't", "tokens": [307, 281, 909, 527, 3174, 35592, 4974, 7684, 9086, 321, 600, 2825, 466, 300, 1547, 370, 286, 1582, 380], "temperature": 0.0, "avg_logprob": -0.10281086893914973, "compression_ratio": 1.71484375, "no_speech_prob": 1.0129854672413785e-05}, {"id": 524, "seek": 323828, "start": 3243.2000000000003, "end": 3247.32, "text": " go into it but basically that means we now are going to ensure that everything has the", "tokens": [352, 666, 309, 457, 1936, 300, 1355, 321, 586, 366, 516, 281, 5586, 300, 1203, 575, 264], "temperature": 0.0, "avg_logprob": -0.10281086893914973, "compression_ratio": 1.71484375, "no_speech_prob": 1.0129854672413785e-05}, {"id": 525, "seek": 323828, "start": 3247.32, "end": 3252.1800000000003, "text": " same shape so that we can collate it into a data loader but now rather than going dot", "tokens": [912, 3909, 370, 300, 321, 393, 1263, 473, 309, 666, 257, 1412, 3677, 260, 457, 586, 2831, 813, 516, 5893], "temperature": 0.0, "avg_logprob": -0.10281086893914973, "compression_ratio": 1.71484375, "no_speech_prob": 1.0129854672413785e-05}, {"id": 526, "seek": 323828, "start": 3252.1800000000003, "end": 3259.8, "text": " data sets or dot data loaders and display our data and remember if something goes wrong", "tokens": [1412, 6352, 420, 5893, 1412, 3677, 433, 293, 4674, 527, 1412, 293, 1604, 498, 746, 1709, 2085], "temperature": 0.0, "avg_logprob": -0.10281086893914973, "compression_ratio": 1.71484375, "no_speech_prob": 1.0129854672413785e-05}, {"id": 527, "seek": 323828, "start": 3259.8, "end": 3264.76, "text": " as we saw last week you can call summary to find out exactly what's happening in your", "tokens": [382, 321, 1866, 1036, 1243, 291, 393, 818, 12691, 281, 915, 484, 2293, 437, 311, 2737, 294, 428], "temperature": 0.0, "avg_logprob": -0.10281086893914973, "compression_ratio": 1.71484375, "no_speech_prob": 1.0129854672413785e-05}, {"id": 528, "seek": 326476, "start": 3264.76, "end": 3269.0, "text": " data block so now you know this is something really worth studying this section because", "tokens": [1412, 3461, 370, 586, 291, 458, 341, 307, 746, 534, 3163, 7601, 341, 3541, 570], "temperature": 0.0, "avg_logprob": -0.09060002317523012, "compression_ratio": 1.7250996015936255, "no_speech_prob": 4.592124014379806e-07}, {"id": 529, "seek": 326476, "start": 3269.0, "end": 3275.32, "text": " data blocks are super handy and if you haven't used fast AI before they won't be familiar", "tokens": [1412, 8474, 366, 1687, 13239, 293, 498, 291, 2378, 380, 1143, 2370, 7318, 949, 436, 1582, 380, 312, 4963], "temperature": 0.0, "avg_logprob": -0.09060002317523012, "compression_ratio": 1.7250996015936255, "no_speech_prob": 4.592124014379806e-07}, {"id": 530, "seek": 326476, "start": 3275.32, "end": 3280.92, "text": " to you because no other library uses them and so like this has really shown you how", "tokens": [281, 291, 570, 572, 661, 6405, 4960, 552, 293, 370, 411, 341, 575, 534, 4898, 291, 577], "temperature": 0.0, "avg_logprob": -0.09060002317523012, "compression_ratio": 1.7250996015936255, "no_speech_prob": 4.592124014379806e-07}, {"id": 531, "seek": 326476, "start": 3280.92, "end": 3285.5600000000004, "text": " to go right back to the start and gradually build them up so hopefully that'll make a", "tokens": [281, 352, 558, 646, 281, 264, 722, 293, 13145, 1322, 552, 493, 370, 4696, 300, 603, 652, 257], "temperature": 0.0, "avg_logprob": -0.09060002317523012, "compression_ratio": 1.7250996015936255, "no_speech_prob": 4.592124014379806e-07}, {"id": 532, "seek": 326476, "start": 3285.5600000000004, "end": 3293.5200000000004, "text": " whole lot of sense now we're going to need a loss function again and to do that let's", "tokens": [1379, 688, 295, 2020, 586, 321, 434, 516, 281, 643, 257, 4470, 2445, 797, 293, 281, 360, 300, 718, 311], "temperature": 0.0, "avg_logprob": -0.09060002317523012, "compression_ratio": 1.7250996015936255, "no_speech_prob": 4.592124014379806e-07}, {"id": 533, "seek": 329352, "start": 3293.52, "end": 3300.36, "text": " start by just creating a learner it's credit resident 18 from the data loaders object that", "tokens": [722, 538, 445, 4084, 257, 33347, 309, 311, 5397, 10832, 2443, 490, 264, 1412, 3677, 433, 2657, 300], "temperature": 0.0, "avg_logprob": -0.1176199197769165, "compression_ratio": 1.7412935323383085, "no_speech_prob": 1.1015932841473841e-06}, {"id": 534, "seek": 329352, "start": 3300.36, "end": 3309.12, "text": " we just created and let's grab one batch of data and then let's put that into our mini", "tokens": [321, 445, 2942, 293, 718, 311, 4444, 472, 15245, 295, 1412, 293, 550, 718, 311, 829, 300, 666, 527, 8382], "temperature": 0.0, "avg_logprob": -0.1176199197769165, "compression_ratio": 1.7412935323383085, "no_speech_prob": 1.1015932841473841e-06}, {"id": 535, "seek": 329352, "start": 3309.12, "end": 3315.4, "text": " batch of independent independent variables and then learn dot model is the thing that", "tokens": [15245, 295, 6695, 6695, 9102, 293, 550, 1466, 5893, 2316, 307, 264, 551, 300], "temperature": 0.0, "avg_logprob": -0.1176199197769165, "compression_ratio": 1.7412935323383085, "no_speech_prob": 1.1015932841473841e-06}, {"id": 536, "seek": 329352, "start": 3315.4, "end": 3321.24, "text": " actually contains the the the model itself in this case of CNN and you can treat it as", "tokens": [767, 8306, 264, 264, 264, 2316, 2564, 294, 341, 1389, 295, 24859, 293, 291, 393, 2387, 309, 382], "temperature": 0.0, "avg_logprob": -0.1176199197769165, "compression_ratio": 1.7412935323383085, "no_speech_prob": 1.1015932841473841e-06}, {"id": 537, "seek": 332124, "start": 3321.24, "end": 3328.2799999999997, "text": " a function and so therefore we can just pass something to it and so if we pass a mini batch", "tokens": [257, 2445, 293, 370, 4412, 321, 393, 445, 1320, 746, 281, 309, 293, 370, 498, 321, 1320, 257, 8382, 15245], "temperature": 0.0, "avg_logprob": -0.09291327981387867, "compression_ratio": 1.6948356807511737, "no_speech_prob": 5.203561386224465e-07}, {"id": 538, "seek": 332124, "start": 3328.2799999999997, "end": 3334.3199999999997, "text": " of the independent variable to learn dot model it will return the activations from the final", "tokens": [295, 264, 6695, 7006, 281, 1466, 5893, 2316, 309, 486, 2736, 264, 2430, 763, 490, 264, 2572], "temperature": 0.0, "avg_logprob": -0.09291327981387867, "compression_ratio": 1.6948356807511737, "no_speech_prob": 5.203561386224465e-07}, {"id": 539, "seek": 332124, "start": 3334.3199999999997, "end": 3343.9599999999996, "text": " layer and that is shape 64 by 20 so anytime you get a tensor back look at its shape and", "tokens": [4583, 293, 300, 307, 3909, 12145, 538, 945, 370, 13038, 291, 483, 257, 40863, 646, 574, 412, 1080, 3909, 293], "temperature": 0.0, "avg_logprob": -0.09291327981387867, "compression_ratio": 1.6948356807511737, "no_speech_prob": 5.203561386224465e-07}, {"id": 540, "seek": 332124, "start": 3343.9599999999996, "end": 3348.4399999999996, "text": " in fact before you look at its shape predict what the shape should be and then make sure", "tokens": [294, 1186, 949, 291, 574, 412, 1080, 3909, 6069, 437, 264, 3909, 820, 312, 293, 550, 652, 988], "temperature": 0.0, "avg_logprob": -0.09291327981387867, "compression_ratio": 1.6948356807511737, "no_speech_prob": 5.203561386224465e-07}, {"id": 541, "seek": 334844, "start": 3348.44, "end": 3353.32, "text": " that you're right if you're not I think you guessed wrong so try to understand what way", "tokens": [300, 291, 434, 558, 498, 291, 434, 406, 286, 519, 291, 21852, 2085, 370, 853, 281, 1223, 437, 636], "temperature": 0.0, "avg_logprob": -0.1160535698845273, "compression_ratio": 1.6036036036036037, "no_speech_prob": 7.57113411964383e-07}, {"id": 542, "seek": 334844, "start": 3353.32, "end": 3361.6, "text": " you made a mistake or there's a problem with your code in this case 64 by 20 makes sense", "tokens": [291, 1027, 257, 6146, 420, 456, 311, 257, 1154, 365, 428, 3089, 294, 341, 1389, 12145, 538, 945, 1669, 2020], "temperature": 0.0, "avg_logprob": -0.1160535698845273, "compression_ratio": 1.6036036036036037, "no_speech_prob": 7.57113411964383e-07}, {"id": 543, "seek": 334844, "start": 3361.6, "end": 3367.04, "text": " because we have a mini batch size of 64 and for each of those we're going to make predictions", "tokens": [570, 321, 362, 257, 8382, 15245, 2744, 295, 12145, 293, 337, 1184, 295, 729, 321, 434, 516, 281, 652, 21264], "temperature": 0.0, "avg_logprob": -0.1160535698845273, "compression_ratio": 1.6036036036036037, "no_speech_prob": 7.57113411964383e-07}, {"id": 544, "seek": 334844, "start": 3367.04, "end": 3373.36, "text": " about what probability is each of these 20 possible categories and we have a question", "tokens": [466, 437, 8482, 307, 1184, 295, 613, 945, 1944, 10479, 293, 321, 362, 257, 1168], "temperature": 0.0, "avg_logprob": -0.1160535698845273, "compression_ratio": 1.6036036036036037, "no_speech_prob": 7.57113411964383e-07}, {"id": 545, "seek": 337336, "start": 3373.36, "end": 3379.6400000000003, "text": " two questions two questions all right is the data block API compatible with out-of-core", "tokens": [732, 1651, 732, 1651, 439, 558, 307, 264, 1412, 3461, 9362, 18218, 365, 484, 12, 2670, 12, 12352], "temperature": 0.0, "avg_logprob": -0.18270923842245074, "compression_ratio": 1.6257668711656441, "no_speech_prob": 1.15445993742469e-06}, {"id": 546, "seek": 337336, "start": 3379.6400000000003, "end": 3388.4, "text": " data sets like Dask yeah the data block API can do anything you wanted to do so you're", "tokens": [1412, 6352, 411, 2846, 74, 1338, 264, 1412, 3461, 9362, 393, 360, 1340, 291, 1415, 281, 360, 370, 291, 434], "temperature": 0.0, "avg_logprob": -0.18270923842245074, "compression_ratio": 1.6257668711656441, "no_speech_prob": 1.15445993742469e-06}, {"id": 547, "seek": 337336, "start": 3388.4, "end": 3399.52, "text": " passing it if we go back to the start so you can create an empty one and then you can pass", "tokens": [8437, 309, 498, 321, 352, 646, 281, 264, 722, 370, 291, 393, 1884, 364, 6707, 472, 293, 550, 291, 393, 1320], "temperature": 0.0, "avg_logprob": -0.18270923842245074, "compression_ratio": 1.6257668711656441, "no_speech_prob": 1.15445993742469e-06}, {"id": 548, "seek": 339952, "start": 3399.52, "end": 3410.0, "text": " it anything that is indexable and yeah so that can be anything you you like and pretty", "tokens": [309, 1340, 300, 307, 8186, 712, 293, 1338, 370, 300, 393, 312, 1340, 291, 291, 411, 293, 1238], "temperature": 0.0, "avg_logprob": -0.13017319899338942, "compression_ratio": 1.8082191780821917, "no_speech_prob": 1.0188073247263674e-06}, {"id": 549, "seek": 339952, "start": 3410.0, "end": 3416.24, "text": " much anything can be made indexable in Python and that's something like Dask is certainly", "tokens": [709, 1340, 393, 312, 1027, 8186, 712, 294, 15329, 293, 300, 311, 746, 411, 2846, 74, 307, 3297], "temperature": 0.0, "avg_logprob": -0.13017319899338942, "compression_ratio": 1.8082191780821917, "no_speech_prob": 1.0188073247263674e-06}, {"id": 550, "seek": 339952, "start": 3416.24, "end": 3425.28, "text": " indexable so that works perfectly fine if it's not indexable like it's a it's a network", "tokens": [8186, 712, 370, 300, 1985, 6239, 2489, 498, 309, 311, 406, 8186, 712, 411, 309, 311, 257, 309, 311, 257, 3209], "temperature": 0.0, "avg_logprob": -0.13017319899338942, "compression_ratio": 1.8082191780821917, "no_speech_prob": 1.0188073247263674e-06}, {"id": 551, "seek": 342528, "start": 3425.28, "end": 3432.28, "text": " stream or something like that then the data loaders data sets API is directly which we'll", "tokens": [4309, 420, 746, 411, 300, 550, 264, 1412, 3677, 433, 1412, 6352, 9362, 307, 3838, 597, 321, 603], "temperature": 0.0, "avg_logprob": -0.11850556032157239, "compression_ratio": 1.5758928571428572, "no_speech_prob": 6.962196039239643e-06}, {"id": 552, "seek": 342528, "start": 3432.28, "end": 3437.88, "text": " learn about either in this course or the next one but yeah anything that you can index into", "tokens": [1466, 466, 2139, 294, 341, 1164, 420, 264, 958, 472, 457, 1338, 1340, 300, 291, 393, 8186, 666], "temperature": 0.0, "avg_logprob": -0.11850556032157239, "compression_ratio": 1.5758928571428572, "no_speech_prob": 6.962196039239643e-06}, {"id": 553, "seek": 342528, "start": 3437.88, "end": 3443.84, "text": " it certainly includes Dask you can use with data blocks next question where do you put", "tokens": [309, 3297, 5974, 2846, 74, 291, 393, 764, 365, 1412, 8474, 958, 1168, 689, 360, 291, 829], "temperature": 0.0, "avg_logprob": -0.11850556032157239, "compression_ratio": 1.5758928571428572, "no_speech_prob": 6.962196039239643e-06}, {"id": 554, "seek": 342528, "start": 3443.84, "end": 3450.44, "text": " images for multi-label with that CSV table should they be in the same directory they", "tokens": [5267, 337, 4825, 12, 75, 18657, 365, 300, 48814, 3199, 820, 436, 312, 294, 264, 912, 21120, 436], "temperature": 0.0, "avg_logprob": -0.11850556032157239, "compression_ratio": 1.5758928571428572, "no_speech_prob": 6.962196039239643e-06}, {"id": 555, "seek": 345044, "start": 3450.44, "end": 3461.36, "text": " can be anywhere you like so in this case we used a path lip object like so and in this", "tokens": [393, 312, 4992, 291, 411, 370, 294, 341, 1389, 321, 1143, 257, 3100, 8280, 2657, 411, 370, 293, 294, 341], "temperature": 0.0, "avg_logprob": -0.1484050750732422, "compression_ratio": 1.4915254237288136, "no_speech_prob": 4.936956429446582e-06}, {"id": 556, "seek": 345044, "start": 3461.36, "end": 3477.56, "text": " case the the by default it's going to be using let's think about this so what's happening", "tokens": [1389, 264, 264, 538, 7576, 309, 311, 516, 281, 312, 1228, 718, 311, 519, 466, 341, 370, 437, 311, 2737], "temperature": 0.0, "avg_logprob": -0.1484050750732422, "compression_ratio": 1.4915254237288136, "no_speech_prob": 4.936956429446582e-06}, {"id": 557, "seek": 347756, "start": 3477.56, "end": 3485.2799999999997, "text": " here is the path is oh it's saying dot okay the reason for that is that path dot base", "tokens": [510, 307, 264, 3100, 307, 1954, 309, 311, 1566, 5893, 1392, 264, 1778, 337, 300, 307, 300, 3100, 5893, 3096], "temperature": 0.0, "avg_logprob": -0.11455277240637576, "compression_ratio": 1.7517241379310344, "no_speech_prob": 1.2482645388445235e-06}, {"id": 558, "seek": 347756, "start": 3485.2799999999997, "end": 3491.44, "text": " path is currently set to path and so that displays things relative so let's rid of", "tokens": [3100, 307, 4362, 992, 281, 3100, 293, 370, 300, 20119, 721, 4972, 370, 718, 311, 3973, 295], "temperature": 0.0, "avg_logprob": -0.11455277240637576, "compression_ratio": 1.7517241379310344, "no_speech_prob": 1.2482645388445235e-06}, {"id": 559, "seek": 347756, "start": 3491.44, "end": 3500.24, "text": " that okay so the path we set is here right and so then when we said get X it's saying", "tokens": [300, 1392, 370, 264, 3100, 321, 992, 307, 510, 558, 293, 370, 550, 562, 321, 848, 483, 1783, 309, 311, 1566], "temperature": 0.0, "avg_logprob": -0.11455277240637576, "compression_ratio": 1.7517241379310344, "no_speech_prob": 1.2482645388445235e-06}, {"id": 560, "seek": 350024, "start": 3500.24, "end": 3507.9599999999996, "text": " path slash chain slash whatever right so this is an absolute path and so here is the exact", "tokens": [3100, 17330, 5021, 17330, 2035, 558, 370, 341, 307, 364, 8236, 3100, 293, 370, 510, 307, 264, 1900], "temperature": 0.0, "avg_logprob": -0.0926759775947122, "compression_ratio": 1.7688442211055277, "no_speech_prob": 1.2359556933461135e-07}, {"id": 561, "seek": 350024, "start": 3507.9599999999996, "end": 3512.3199999999997, "text": " path so you can put them anywhere you like you just have to say what the path is and", "tokens": [3100, 370, 291, 393, 829, 552, 4992, 291, 411, 291, 445, 362, 281, 584, 437, 264, 3100, 307, 293], "temperature": 0.0, "avg_logprob": -0.0926759775947122, "compression_ratio": 1.7688442211055277, "no_speech_prob": 1.2359556933461135e-07}, {"id": 562, "seek": 350024, "start": 3512.3199999999997, "end": 3519.3199999999997, "text": " then if you want to not get confused by having this big long prefix that we can don't want", "tokens": [550, 498, 291, 528, 281, 406, 483, 9019, 538, 1419, 341, 955, 938, 46969, 300, 321, 393, 500, 380, 528], "temperature": 0.0, "avg_logprob": -0.0926759775947122, "compression_ratio": 1.7688442211055277, "no_speech_prob": 1.2359556933461135e-07}, {"id": 563, "seek": 350024, "start": 3519.3199999999997, "end": 3523.7999999999997, "text": " to see all the time just set base path to the path you want everything to be relative", "tokens": [281, 536, 439, 264, 565, 445, 992, 3096, 3100, 281, 264, 3100, 291, 528, 1203, 281, 312, 4972], "temperature": 0.0, "avg_logprob": -0.0926759775947122, "compression_ratio": 1.7688442211055277, "no_speech_prob": 1.2359556933461135e-07}, {"id": 564, "seek": 352380, "start": 3523.8, "end": 3532.8, "text": " to and then it'll just print things out in this more can more convenient manner right", "tokens": [281, 293, 550, 309, 603, 445, 4482, 721, 484, 294, 341, 544, 393, 544, 10851, 9060, 558], "temperature": 0.0, "avg_logprob": -0.14870781122252, "compression_ratio": 1.7563451776649746, "no_speech_prob": 9.570817383064423e-07}, {"id": 565, "seek": 352380, "start": 3532.8, "end": 3538.4, "text": " so this is really important that you can do this that you can create a learner you can", "tokens": [370, 341, 307, 534, 1021, 300, 291, 393, 360, 341, 300, 291, 393, 1884, 257, 33347, 291, 393], "temperature": 0.0, "avg_logprob": -0.14870781122252, "compression_ratio": 1.7563451776649746, "no_speech_prob": 9.570817383064423e-07}, {"id": 566, "seek": 352380, "start": 3538.4, "end": 3543.04, "text": " grab a batch of data that you can pass it to the model is this is just plain PyTorch", "tokens": [4444, 257, 15245, 295, 1412, 300, 291, 393, 1320, 309, 281, 264, 2316, 307, 341, 307, 445, 11121, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.14870781122252, "compression_ratio": 1.7563451776649746, "no_speech_prob": 9.570817383064423e-07}, {"id": 567, "seek": 352380, "start": 3543.04, "end": 3549.32, "text": " this line here right no fast AI you can see the shape right you can recognize why it has", "tokens": [341, 1622, 510, 558, 572, 2370, 7318, 291, 393, 536, 264, 3909, 558, 291, 393, 5521, 983, 309, 575], "temperature": 0.0, "avg_logprob": -0.14870781122252, "compression_ratio": 1.7563451776649746, "no_speech_prob": 9.570817383064423e-07}, {"id": 568, "seek": 354932, "start": 3549.32, "end": 3559.1600000000003, "text": " this shape and so now if you have a look here are the 20 activations now this is not a trained", "tokens": [341, 3909, 293, 370, 586, 498, 291, 362, 257, 574, 510, 366, 264, 945, 2430, 763, 586, 341, 307, 406, 257, 8895], "temperature": 0.0, "avg_logprob": -0.09705259608126235, "compression_ratio": 1.672811059907834, "no_speech_prob": 1.3156690670257376e-07}, {"id": 569, "seek": 354932, "start": 3559.1600000000003, "end": 3566.0, "text": " model it's a pre-trained model with a random set of final layer weights so these specific", "tokens": [2316, 309, 311, 257, 659, 12, 17227, 2001, 2316, 365, 257, 4974, 992, 295, 2572, 4583, 17443, 370, 613, 2685], "temperature": 0.0, "avg_logprob": -0.09705259608126235, "compression_ratio": 1.672811059907834, "no_speech_prob": 1.3156690670257376e-07}, {"id": 570, "seek": 354932, "start": 3566.0, "end": 3570.6600000000003, "text": " numbers don't mean anything right but it's just worth remembering this is what activations", "tokens": [3547, 500, 380, 914, 1340, 558, 457, 309, 311, 445, 3163, 20719, 341, 307, 437, 2430, 763], "temperature": 0.0, "avg_logprob": -0.09705259608126235, "compression_ratio": 1.672811059907834, "no_speech_prob": 1.3156690670257376e-07}, {"id": 571, "seek": 354932, "start": 3570.6600000000003, "end": 3577.06, "text": " look like and most importantly they're not between 0 and 1 and if you remember from the", "tokens": [574, 411, 293, 881, 8906, 436, 434, 406, 1296, 1958, 293, 502, 293, 498, 291, 1604, 490, 264], "temperature": 0.0, "avg_logprob": -0.09705259608126235, "compression_ratio": 1.672811059907834, "no_speech_prob": 1.3156690670257376e-07}, {"id": 572, "seek": 357706, "start": 3577.06, "end": 3582.92, "text": " MNIST notebook we know how to scale things between 0 and 1 we can pop them into the sigmoid", "tokens": [376, 45, 19756, 21060, 321, 458, 577, 281, 4373, 721, 1296, 1958, 293, 502, 321, 393, 1665, 552, 666, 264, 4556, 3280, 327], "temperature": 0.0, "avg_logprob": -0.1259759923686152, "compression_ratio": 1.8051282051282052, "no_speech_prob": 1.2878921324954717e-06}, {"id": 573, "seek": 357706, "start": 3582.92, "end": 3587.92, "text": " function so the sigmoid function is something that scales everything to be between 0 and", "tokens": [2445, 370, 264, 4556, 3280, 327, 2445, 307, 746, 300, 17408, 1203, 281, 312, 1296, 1958, 293], "temperature": 0.0, "avg_logprob": -0.1259759923686152, "compression_ratio": 1.8051282051282052, "no_speech_prob": 1.2878921324954717e-06}, {"id": 574, "seek": 357706, "start": 3587.92, "end": 3595.24, "text": " 1 so let's use that you'll also hopefully remember from the MNIST notebook that the", "tokens": [502, 370, 718, 311, 764, 300, 291, 603, 611, 4696, 1604, 490, 264, 376, 45, 19756, 21060, 300, 264], "temperature": 0.0, "avg_logprob": -0.1259759923686152, "compression_ratio": 1.8051282051282052, "no_speech_prob": 1.2878921324954717e-06}, {"id": 575, "seek": 357706, "start": 3595.24, "end": 3606.6, "text": " MNIST loss the MNIST loss function first did sigmoid and then it did torch dot where so", "tokens": [376, 45, 19756, 4470, 264, 376, 45, 19756, 4470, 2445, 700, 630, 4556, 3280, 327, 293, 550, 309, 630, 27822, 5893, 689, 370], "temperature": 0.0, "avg_logprob": -0.1259759923686152, "compression_ratio": 1.8051282051282052, "no_speech_prob": 1.2878921324954717e-06}, {"id": 576, "seek": 360660, "start": 3606.6, "end": 3610.8399999999997, "text": " and then it did dot mean so we're going to use exactly the same thing as the MNIST loss", "tokens": [293, 550, 309, 630, 5893, 914, 370, 321, 434, 516, 281, 764, 2293, 264, 912, 551, 382, 264, 376, 45, 19756, 4470], "temperature": 0.0, "avg_logprob": -0.06130000596405358, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.1189410997758387e-06}, {"id": 577, "seek": 360660, "start": 3610.8399999999997, "end": 3615.44, "text": " function and we're just going to do one thing which is going to add dot log for the same", "tokens": [2445, 293, 321, 434, 445, 516, 281, 360, 472, 551, 597, 307, 516, 281, 909, 5893, 3565, 337, 264, 912], "temperature": 0.0, "avg_logprob": -0.06130000596405358, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.1189410997758387e-06}, {"id": 578, "seek": 360660, "start": 3615.44, "end": 3625.12, "text": " reason that we talked about when we were looking at softmax we talked about why log is a good", "tokens": [1778, 300, 321, 2825, 466, 562, 321, 645, 1237, 412, 2787, 41167, 321, 2825, 466, 983, 3565, 307, 257, 665], "temperature": 0.0, "avg_logprob": -0.06130000596405358, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.1189410997758387e-06}, {"id": 579, "seek": 360660, "start": 3625.12, "end": 3632.62, "text": " idea as a transformation we saw in the MNIST notebook we didn't need it but we're going", "tokens": [1558, 382, 257, 9887, 321, 1866, 294, 264, 376, 45, 19756, 21060, 321, 994, 380, 643, 309, 457, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.06130000596405358, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.1189410997758387e-06}, {"id": 580, "seek": 363262, "start": 3632.62, "end": 3636.7599999999998, "text": " to train faster and more accurately if we use it because it's just more it's going to", "tokens": [281, 3847, 4663, 293, 544, 20095, 498, 321, 764, 309, 570, 309, 311, 445, 544, 309, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.12030259494123788, "compression_ratio": 1.6017699115044248, "no_speech_prob": 3.9897133774502436e-07}, {"id": 581, "seek": 363262, "start": 3636.7599999999998, "end": 3643.12, "text": " be better behaved as we've seen so this particular function which is identical to MNIST loss", "tokens": [312, 1101, 48249, 382, 321, 600, 1612, 370, 341, 1729, 2445, 597, 307, 14800, 281, 376, 45, 19756, 4470], "temperature": 0.0, "avg_logprob": -0.12030259494123788, "compression_ratio": 1.6017699115044248, "no_speech_prob": 3.9897133774502436e-07}, {"id": 582, "seek": 363262, "start": 3643.12, "end": 3651.56, "text": " plus dot log as a specific name and it's called binary cross entropy and we used it for the", "tokens": [1804, 5893, 3565, 382, 257, 2685, 1315, 293, 309, 311, 1219, 17434, 3278, 30867, 293, 321, 1143, 309, 337, 264], "temperature": 0.0, "avg_logprob": -0.12030259494123788, "compression_ratio": 1.6017699115044248, "no_speech_prob": 3.9897133774502436e-07}, {"id": 583, "seek": 363262, "start": 3651.56, "end": 3661.52, "text": " threes versus sevens problem to decide whether that column is it a three or not but because", "tokens": [258, 4856, 5717, 3407, 82, 1154, 281, 4536, 1968, 300, 7738, 307, 309, 257, 1045, 420, 406, 457, 570], "temperature": 0.0, "avg_logprob": -0.12030259494123788, "compression_ratio": 1.6017699115044248, "no_speech_prob": 3.9897133774502436e-07}, {"id": 584, "seek": 366152, "start": 3661.52, "end": 3669.08, "text": " we can use broadcasting in high torch and element wise arithmetic this function when", "tokens": [321, 393, 764, 30024, 294, 1090, 27822, 293, 4478, 10829, 42973, 341, 2445, 562], "temperature": 0.0, "avg_logprob": -0.14259662383641952, "compression_ratio": 1.7549019607843137, "no_speech_prob": 1.2482685178838437e-06}, {"id": 585, "seek": 366152, "start": 3669.08, "end": 3677.16, "text": " we pass it a whole matrix is going to be applied to every column so is the first column you", "tokens": [321, 1320, 309, 257, 1379, 8141, 307, 516, 281, 312, 6456, 281, 633, 7738, 370, 307, 264, 700, 7738, 291], "temperature": 0.0, "avg_logprob": -0.14259662383641952, "compression_ratio": 1.7549019607843137, "no_speech_prob": 1.2482685178838437e-06}, {"id": 586, "seek": 366152, "start": 3677.16, "end": 3683.96, "text": " know so it'll it'll basically do a torch dot where on on every column separately and every", "tokens": [458, 370, 309, 603, 309, 603, 1936, 360, 257, 27822, 5893, 689, 322, 322, 633, 7738, 14759, 293, 633], "temperature": 0.0, "avg_logprob": -0.14259662383641952, "compression_ratio": 1.7549019607843137, "no_speech_prob": 1.2482685178838437e-06}, {"id": 587, "seek": 366152, "start": 3683.96, "end": 3690.92, "text": " item separately so that's great it basically means that this binary cross entropy function", "tokens": [3174, 14759, 370, 300, 311, 869, 309, 1936, 1355, 300, 341, 17434, 3278, 30867, 2445], "temperature": 0.0, "avg_logprob": -0.14259662383641952, "compression_ratio": 1.7549019607843137, "no_speech_prob": 1.2482685178838437e-06}, {"id": 588, "seek": 369092, "start": 3690.92, "end": 3696.92, "text": " is going to be just like MNIST loss but rather than just being is this the number three it'll", "tokens": [307, 516, 281, 312, 445, 411, 376, 45, 19756, 4470, 457, 2831, 813, 445, 885, 307, 341, 264, 1230, 1045, 309, 603], "temperature": 0.0, "avg_logprob": -0.10064375769231737, "compression_ratio": 1.733644859813084, "no_speech_prob": 2.948006567748962e-06}, {"id": 589, "seek": 369092, "start": 3696.92, "end": 3702.28, "text": " be is this a dog is this a cat is this a car is this a person is this the bicycle and so", "tokens": [312, 307, 341, 257, 3000, 307, 341, 257, 3857, 307, 341, 257, 1032, 307, 341, 257, 954, 307, 341, 264, 20888, 293, 370], "temperature": 0.0, "avg_logprob": -0.10064375769231737, "compression_ratio": 1.733644859813084, "no_speech_prob": 2.948006567748962e-06}, {"id": 590, "seek": 369092, "start": 3702.28, "end": 3707.8, "text": " forth so this is where it's so cool in pytorch we can kind of run write one thing and then", "tokens": [5220, 370, 341, 307, 689, 309, 311, 370, 1627, 294, 25878, 284, 339, 321, 393, 733, 295, 1190, 2464, 472, 551, 293, 550], "temperature": 0.0, "avg_logprob": -0.10064375769231737, "compression_ratio": 1.733644859813084, "no_speech_prob": 2.948006567748962e-06}, {"id": 591, "seek": 369092, "start": 3707.8, "end": 3715.88, "text": " kind of have it expand to handle higher dimensional tensors without doing any extra work we don't", "tokens": [733, 295, 362, 309, 5268, 281, 4813, 2946, 18795, 10688, 830, 1553, 884, 604, 2857, 589, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.10064375769231737, "compression_ratio": 1.733644859813084, "no_speech_prob": 2.948006567748962e-06}, {"id": 592, "seek": 371588, "start": 3715.88, "end": 3723.56, "text": " have to write this error ourselves of course because pytorch has one and it's called F", "tokens": [362, 281, 2464, 341, 6713, 4175, 295, 1164, 570, 25878, 284, 339, 575, 472, 293, 309, 311, 1219, 479], "temperature": 0.0, "avg_logprob": -0.18033419098964956, "compression_ratio": 1.6036036036036037, "no_speech_prob": 1.3709538961848011e-06}, {"id": 593, "seek": 371588, "start": 3723.56, "end": 3729.2400000000002, "text": " dot binary cross entropy so we can just use pytorches as we've talked about there's always", "tokens": [5893, 17434, 3278, 30867, 370, 321, 393, 445, 764, 25878, 284, 3781, 382, 321, 600, 2825, 466, 456, 311, 1009], "temperature": 0.0, "avg_logprob": -0.18033419098964956, "compression_ratio": 1.6036036036036037, "no_speech_prob": 1.3709538961848011e-06}, {"id": 594, "seek": 371588, "start": 3729.2400000000002, "end": 3736.52, "text": " a equivalent module version so this is exactly the same thing as a module and end up BCE", "tokens": [257, 10344, 10088, 3037, 370, 341, 307, 2293, 264, 912, 551, 382, 257, 10088, 293, 917, 493, 49369], "temperature": 0.0, "avg_logprob": -0.18033419098964956, "compression_ratio": 1.6036036036036037, "no_speech_prob": 1.3709538961848011e-06}, {"id": 595, "seek": 371588, "start": 3736.52, "end": 3745.8, "text": " loss and these ones don't include the initial sigmoid actually if you want to include the", "tokens": [4470, 293, 613, 2306, 500, 380, 4090, 264, 5883, 4556, 3280, 327, 767, 498, 291, 528, 281, 4090, 264], "temperature": 0.0, "avg_logprob": -0.18033419098964956, "compression_ratio": 1.6036036036036037, "no_speech_prob": 1.3709538961848011e-06}, {"id": 596, "seek": 374580, "start": 3745.8, "end": 3750.88, "text": " initial sigmoid you need F dot binary cross entropy with logits or the equivalent and", "tokens": [5883, 4556, 3280, 327, 291, 643, 479, 5893, 17434, 3278, 30867, 365, 3565, 1208, 420, 264, 10344, 293], "temperature": 0.0, "avg_logprob": -0.15206271900850185, "compression_ratio": 1.7738693467336684, "no_speech_prob": 1.2679248584390734e-06}, {"id": 597, "seek": 374580, "start": 3750.88, "end": 3760.6400000000003, "text": " end up BCE with logits loss so BCE is binary cross entropy and so those are two functions", "tokens": [917, 493, 49369, 365, 3565, 1208, 4470, 370, 49369, 307, 17434, 3278, 30867, 293, 370, 729, 366, 732, 6828], "temperature": 0.0, "avg_logprob": -0.15206271900850185, "compression_ratio": 1.7738693467336684, "no_speech_prob": 1.2679248584390734e-06}, {"id": 598, "seek": 374580, "start": 3760.6400000000003, "end": 3767.32, "text": " last two equivalent classes or multi-label or binary problems and then the equivalent", "tokens": [1036, 732, 10344, 5359, 420, 4825, 12, 75, 18657, 420, 17434, 2740, 293, 550, 264, 10344], "temperature": 0.0, "avg_logprob": -0.15206271900850185, "compression_ratio": 1.7738693467336684, "no_speech_prob": 1.2679248584390734e-06}, {"id": 599, "seek": 374580, "start": 3767.32, "end": 3774.5600000000004, "text": " for single label like MNIST and PETS is NLL loss and cross entropy so that's the equivalent", "tokens": [337, 2167, 7645, 411, 376, 45, 19756, 293, 21968, 50, 307, 426, 24010, 4470, 293, 3278, 30867, 370, 300, 311, 264, 10344], "temperature": 0.0, "avg_logprob": -0.15206271900850185, "compression_ratio": 1.7738693467336684, "no_speech_prob": 1.2679248584390734e-06}, {"id": 600, "seek": 377456, "start": 3774.56, "end": 3779.44, "text": " of binary cross entropy and binary cross entropy with logits so these are pretty awful names", "tokens": [295, 17434, 3278, 30867, 293, 17434, 3278, 30867, 365, 3565, 1208, 370, 613, 366, 1238, 11232, 5288], "temperature": 0.0, "avg_logprob": -0.09596338272094726, "compression_ratio": 1.7047619047619047, "no_speech_prob": 4.247022786785237e-07}, {"id": 601, "seek": 377456, "start": 3779.44, "end": 3787.84, "text": " I think we can all agree but it is what it is so in our case we have a one hot encoded", "tokens": [286, 519, 321, 393, 439, 3986, 457, 309, 307, 437, 309, 307, 370, 294, 527, 1389, 321, 362, 257, 472, 2368, 2058, 12340], "temperature": 0.0, "avg_logprob": -0.09596338272094726, "compression_ratio": 1.7047619047619047, "no_speech_prob": 4.247022786785237e-07}, {"id": 602, "seek": 377456, "start": 3787.84, "end": 3795.84, "text": " target and we want the one with the sigmoid in so the equivalent built-in is called BCE", "tokens": [3779, 293, 321, 528, 264, 472, 365, 264, 4556, 3280, 327, 294, 370, 264, 10344, 3094, 12, 259, 307, 1219, 49369], "temperature": 0.0, "avg_logprob": -0.09596338272094726, "compression_ratio": 1.7047619047619047, "no_speech_prob": 4.247022786785237e-07}, {"id": 603, "seek": 377456, "start": 3795.84, "end": 3801.24, "text": " with logits loss so that we can make that our loss function we can compare the activations", "tokens": [365, 3565, 1208, 4470, 370, 300, 321, 393, 652, 300, 527, 4470, 2445, 321, 393, 6794, 264, 2430, 763], "temperature": 0.0, "avg_logprob": -0.09596338272094726, "compression_ratio": 1.7047619047619047, "no_speech_prob": 4.247022786785237e-07}, {"id": 604, "seek": 380124, "start": 3801.24, "end": 3810.56, "text": " to our targets and we can get back a loss and then that's what we can use to train and", "tokens": [281, 527, 12911, 293, 321, 393, 483, 646, 257, 4470, 293, 550, 300, 311, 437, 321, 393, 764, 281, 3847, 293], "temperature": 0.0, "avg_logprob": -0.10898326692127046, "compression_ratio": 1.7225806451612904, "no_speech_prob": 5.626395136459905e-07}, {"id": 605, "seek": 380124, "start": 3810.56, "end": 3815.3199999999997, "text": " then finally before we take our break we also need a metric now previously we've been using", "tokens": [550, 2721, 949, 321, 747, 527, 1821, 321, 611, 643, 257, 20678, 586, 8046, 321, 600, 668, 1228], "temperature": 0.0, "avg_logprob": -0.10898326692127046, "compression_ratio": 1.7225806451612904, "no_speech_prob": 5.626395136459905e-07}, {"id": 606, "seek": 380124, "start": 3815.3199999999997, "end": 3821.6, "text": " as a metric accuracy or actually error rate error rate is 1 minus accuracy accuracy only", "tokens": [382, 257, 20678, 14170, 420, 767, 6713, 3314, 6713, 3314, 307, 502, 3175, 14170, 14170, 787], "temperature": 0.0, "avg_logprob": -0.10898326692127046, "compression_ratio": 1.7225806451612904, "no_speech_prob": 5.626395136459905e-07}, {"id": 607, "seek": 382160, "start": 3821.6, "end": 3831.48, "text": " works or single label data sets like MNIST and PETS because what it does is it takes", "tokens": [1985, 420, 2167, 7645, 1412, 6352, 411, 376, 45, 19756, 293, 21968, 50, 570, 437, 309, 775, 307, 309, 2516], "temperature": 0.0, "avg_logprob": -0.12417837207236987, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.9033749367736164e-06}, {"id": 608, "seek": 382160, "start": 3831.48, "end": 3838.6, "text": " the input which is the final layer activations and it does arg max what arg max does is it", "tokens": [264, 4846, 597, 307, 264, 2572, 4583, 2430, 763, 293, 309, 775, 3882, 11469, 437, 3882, 11469, 775, 307, 309], "temperature": 0.0, "avg_logprob": -0.12417837207236987, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.9033749367736164e-06}, {"id": 609, "seek": 382160, "start": 3838.6, "end": 3844.24, "text": " says what is the index of the largest number in those activations so for example for MNIST", "tokens": [1619, 437, 307, 264, 8186, 295, 264, 6443, 1230, 294, 729, 2430, 763, 370, 337, 1365, 337, 376, 45, 19756], "temperature": 0.0, "avg_logprob": -0.12417837207236987, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.9033749367736164e-06}, {"id": 610, "seek": 382160, "start": 3844.24, "end": 3851.2, "text": " you know maybe the largest the highest probability is 7 so this arg max would return 7 and then", "tokens": [291, 458, 1310, 264, 6443, 264, 6343, 8482, 307, 1614, 370, 341, 3882, 11469, 576, 2736, 1614, 293, 550], "temperature": 0.0, "avg_logprob": -0.12417837207236987, "compression_ratio": 1.699530516431925, "no_speech_prob": 1.9033749367736164e-06}, {"id": 611, "seek": 385120, "start": 3851.2, "end": 3856.7999999999997, "text": " it says okay there's those are my predictions and then it says okay is the prediction equal", "tokens": [309, 1619, 1392, 456, 311, 729, 366, 452, 21264, 293, 550, 309, 1619, 1392, 307, 264, 17630, 2681], "temperature": 0.0, "avg_logprob": -0.09619105429876418, "compression_ratio": 1.7014218009478672, "no_speech_prob": 1.0348508112656418e-06}, {"id": 612, "seek": 385120, "start": 3856.7999999999997, "end": 3863.7599999999998, "text": " to the target or not and then take the floating point mean so that's what accuracy is so arg", "tokens": [281, 264, 3779, 420, 406, 293, 550, 747, 264, 12607, 935, 914, 370, 300, 311, 437, 14170, 307, 370, 3882], "temperature": 0.0, "avg_logprob": -0.09619105429876418, "compression_ratio": 1.7014218009478672, "no_speech_prob": 1.0348508112656418e-06}, {"id": 613, "seek": 385120, "start": 3863.7599999999998, "end": 3870.4399999999996, "text": " max only makes sense when there's a single maximum thing you're looking for in this case", "tokens": [11469, 787, 1669, 2020, 562, 456, 311, 257, 2167, 6674, 551, 291, 434, 1237, 337, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.09619105429876418, "compression_ratio": 1.7014218009478672, "no_speech_prob": 1.0348508112656418e-06}, {"id": 614, "seek": 385120, "start": 3870.4399999999996, "end": 3879.3999999999996, "text": " we've got multi-label so instead we have to compare each activation to some threshold", "tokens": [321, 600, 658, 4825, 12, 75, 18657, 370, 2602, 321, 362, 281, 6794, 1184, 24433, 281, 512, 14678], "temperature": 0.0, "avg_logprob": -0.09619105429876418, "compression_ratio": 1.7014218009478672, "no_speech_prob": 1.0348508112656418e-06}, {"id": 615, "seek": 387940, "start": 3879.4, "end": 3886.04, "text": " by default it's 0.5 and so we basically say if the sigmoid of the activation is greater", "tokens": [538, 7576, 309, 311, 1958, 13, 20, 293, 370, 321, 1936, 584, 498, 264, 4556, 3280, 327, 295, 264, 24433, 307, 5044], "temperature": 0.0, "avg_logprob": -0.08680236339569092, "compression_ratio": 1.8051282051282052, "no_speech_prob": 2.0904526536469348e-06}, {"id": 616, "seek": 387940, "start": 3886.04, "end": 3893.08, "text": " than 0.5 let's assume that means that category is there and if it's not let's assume it means", "tokens": [813, 1958, 13, 20, 718, 311, 6552, 300, 1355, 300, 7719, 307, 456, 293, 498, 309, 311, 406, 718, 311, 6552, 309, 1355], "temperature": 0.0, "avg_logprob": -0.08680236339569092, "compression_ratio": 1.8051282051282052, "no_speech_prob": 2.0904526536469348e-06}, {"id": 617, "seek": 387940, "start": 3893.08, "end": 3898.08, "text": " it's not there and so this is going to go give us a list of trues and falses for the", "tokens": [309, 311, 406, 456, 293, 370, 341, 307, 516, 281, 352, 976, 505, 257, 1329, 295, 504, 1247, 293, 16720, 279, 337, 264], "temperature": 0.0, "avg_logprob": -0.08680236339569092, "compression_ratio": 1.8051282051282052, "no_speech_prob": 2.0904526536469348e-06}, {"id": 618, "seek": 387940, "start": 3898.08, "end": 3903.32, "text": " ones that the based on the activations it thinks are there and we can compare that to", "tokens": [2306, 300, 264, 2361, 322, 264, 2430, 763, 309, 7309, 366, 456, 293, 321, 393, 6794, 300, 281], "temperature": 0.0, "avg_logprob": -0.08680236339569092, "compression_ratio": 1.8051282051282052, "no_speech_prob": 2.0904526536469348e-06}, {"id": 619, "seek": 390332, "start": 3903.32, "end": 3911.1600000000003, "text": " the target and then again take the floating point mean so we can use the default threshold", "tokens": [264, 3779, 293, 550, 797, 747, 264, 12607, 935, 914, 370, 321, 393, 764, 264, 7576, 14678], "temperature": 0.0, "avg_logprob": -0.0960370206284797, "compression_ratio": 1.8144329896907216, "no_speech_prob": 1.6280484942399198e-06}, {"id": 620, "seek": 390332, "start": 3911.1600000000003, "end": 3918.36, "text": " of 0.5 but we don't necessarily want to use 0.5 we might want to use a different threshold", "tokens": [295, 1958, 13, 20, 457, 321, 500, 380, 4725, 528, 281, 764, 1958, 13, 20, 321, 1062, 528, 281, 764, 257, 819, 14678], "temperature": 0.0, "avg_logprob": -0.0960370206284797, "compression_ratio": 1.8144329896907216, "no_speech_prob": 1.6280484942399198e-06}, {"id": 621, "seek": 390332, "start": 3918.36, "end": 3923.5, "text": " and remember we have to pass when we create our learner we have to pass to the metric", "tokens": [293, 1604, 321, 362, 281, 1320, 562, 321, 1884, 527, 33347, 321, 362, 281, 1320, 281, 264, 20678], "temperature": 0.0, "avg_logprob": -0.0960370206284797, "compression_ratio": 1.8144329896907216, "no_speech_prob": 1.6280484942399198e-06}, {"id": 622, "seek": 390332, "start": 3923.5, "end": 3929.96, "text": " the metrics argument a function so what if we want to use a threshold other than 0.5", "tokens": [264, 16367, 6770, 257, 2445, 370, 437, 498, 321, 528, 281, 764, 257, 14678, 661, 813, 1958, 13, 20], "temperature": 0.0, "avg_logprob": -0.0960370206284797, "compression_ratio": 1.8144329896907216, "no_speech_prob": 1.6280484942399198e-06}, {"id": 623, "seek": 392996, "start": 3929.96, "end": 3934.84, "text": " well we'd like to create a special function which is accuracy multi with some different", "tokens": [731, 321, 1116, 411, 281, 1884, 257, 2121, 2445, 597, 307, 14170, 4825, 365, 512, 819], "temperature": 0.0, "avg_logprob": -0.10432599466058272, "compression_ratio": 1.7487437185929648, "no_speech_prob": 1.7603396145204897e-06}, {"id": 624, "seek": 392996, "start": 3934.84, "end": 3943.8, "text": " threshold and the way we do that is we use a special built-in in Python called partial", "tokens": [14678, 293, 264, 636, 321, 360, 300, 307, 321, 764, 257, 2121, 3094, 12, 259, 294, 15329, 1219, 14641], "temperature": 0.0, "avg_logprob": -0.10432599466058272, "compression_ratio": 1.7487437185929648, "no_speech_prob": 1.7603396145204897e-06}, {"id": 625, "seek": 392996, "start": 3943.8, "end": 3951.88, "text": " let me show you how partial works here's a function called say hello say hello to somebody", "tokens": [718, 385, 855, 291, 577, 14641, 1985, 510, 311, 257, 2445, 1219, 584, 7751, 584, 7751, 281, 2618], "temperature": 0.0, "avg_logprob": -0.10432599466058272, "compression_ratio": 1.7487437185929648, "no_speech_prob": 1.7603396145204897e-06}, {"id": 626, "seek": 392996, "start": 3951.88, "end": 3958.84, "text": " with something so say hello Jeremy or the default is hello so it says hello Jeremy", "tokens": [365, 746, 370, 584, 7751, 17809, 420, 264, 7576, 307, 7751, 370, 309, 1619, 7751, 17809], "temperature": 0.0, "avg_logprob": -0.10432599466058272, "compression_ratio": 1.7487437185929648, "no_speech_prob": 1.7603396145204897e-06}, {"id": 627, "seek": 395884, "start": 3958.84, "end": 3965.84, "text": " say hello Jeremy comma ahoy ahoy Jeremy let's create a special version of this function", "tokens": [584, 7751, 17809, 22117, 3716, 939, 3716, 939, 17809, 718, 311, 1884, 257, 2121, 3037, 295, 341, 2445], "temperature": 0.0, "avg_logprob": -0.14223494640616482, "compression_ratio": 1.73, "no_speech_prob": 3.237696773794596e-06}, {"id": 628, "seek": 395884, "start": 3965.84, "end": 3972.1600000000003, "text": " that will be more suitable for Sylvain it's going to use French so we can say partial", "tokens": [300, 486, 312, 544, 12873, 337, 3902, 14574, 491, 309, 311, 516, 281, 764, 5522, 370, 321, 393, 584, 14641], "temperature": 0.0, "avg_logprob": -0.14223494640616482, "compression_ratio": 1.73, "no_speech_prob": 3.237696773794596e-06}, {"id": 629, "seek": 395884, "start": 3972.1600000000003, "end": 3977.32, "text": " create a new function that's based on the say hello function but it's always going to", "tokens": [1884, 257, 777, 2445, 300, 311, 2361, 322, 264, 584, 7751, 2445, 457, 309, 311, 1009, 516, 281], "temperature": 0.0, "avg_logprob": -0.14223494640616482, "compression_ratio": 1.73, "no_speech_prob": 3.237696773794596e-06}, {"id": 630, "seek": 395884, "start": 3977.32, "end": 3985.8, "text": " set say what to Bonjour and we'll call that F but now F Jeremy is Bonjour Jeremy and F", "tokens": [992, 584, 437, 281, 25431, 293, 321, 603, 818, 300, 479, 457, 586, 479, 17809, 307, 25431, 17809, 293, 479], "temperature": 0.0, "avg_logprob": -0.14223494640616482, "compression_ratio": 1.73, "no_speech_prob": 3.237696773794596e-06}, {"id": 631, "seek": 398580, "start": 3985.8, "end": 3992.52, "text": " Sylvain is Bonjour Sylvain so you see we've created a new function from an existing function", "tokens": [3902, 14574, 491, 307, 25431, 3902, 14574, 491, 370, 291, 536, 321, 600, 2942, 257, 777, 2445, 490, 364, 6741, 2445], "temperature": 0.0, "avg_logprob": -0.10288761474274016, "compression_ratio": 1.5814977973568283, "no_speech_prob": 1.2410941053531133e-05}, {"id": 632, "seek": 398580, "start": 3992.52, "end": 3999.8, "text": " by fixing one of its parameters so we can do the same thing for accuracy multi say let's", "tokens": [538, 19442, 472, 295, 1080, 9834, 370, 321, 393, 360, 264, 912, 551, 337, 14170, 4825, 584, 718, 311], "temperature": 0.0, "avg_logprob": -0.10288761474274016, "compression_ratio": 1.5814977973568283, "no_speech_prob": 1.2410941053531133e-05}, {"id": 633, "seek": 398580, "start": 3999.8, "end": 4007.36, "text": " use a threshold of 0.2 and we can pass that to metrics and so let's create a CNN learner", "tokens": [764, 257, 14678, 295, 1958, 13, 17, 293, 321, 393, 1320, 300, 281, 16367, 293, 370, 718, 311, 1884, 257, 24859, 33347], "temperature": 0.0, "avg_logprob": -0.10288761474274016, "compression_ratio": 1.5814977973568283, "no_speech_prob": 1.2410941053531133e-05}, {"id": 634, "seek": 398580, "start": 4007.36, "end": 4011.6400000000003, "text": " and you'll notice here we don't actually pass a loss function and that's because fast AI", "tokens": [293, 291, 603, 3449, 510, 321, 500, 380, 767, 1320, 257, 4470, 2445, 293, 300, 311, 570, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.10288761474274016, "compression_ratio": 1.5814977973568283, "no_speech_prob": 1.2410941053531133e-05}, {"id": 635, "seek": 401164, "start": 4011.64, "end": 4020.92, "text": " is not enough to realize hey you're doing a classification model with a a multi label", "tokens": [307, 406, 1547, 281, 4325, 4177, 291, 434, 884, 257, 21538, 2316, 365, 257, 257, 4825, 7645], "temperature": 0.0, "avg_logprob": -0.14488148134808207, "compression_ratio": 1.5219298245614035, "no_speech_prob": 1.1911049568880117e-06}, {"id": 636, "seek": 401164, "start": 4020.92, "end": 4025.44, "text": " dependent variable so I know what loss function you probably want so it does it for us or", "tokens": [12334, 7006, 370, 286, 458, 437, 4470, 2445, 291, 1391, 528, 370, 309, 775, 309, 337, 505, 420], "temperature": 0.0, "avg_logprob": -0.14488148134808207, "compression_ratio": 1.5219298245614035, "no_speech_prob": 1.1911049568880117e-06}, {"id": 637, "seek": 401164, "start": 4025.44, "end": 4030.6, "text": " we can call fine-tune and here we have an accuracy of 94 and a half after the first", "tokens": [321, 393, 818, 2489, 12, 83, 2613, 293, 510, 321, 362, 364, 14170, 295, 30849, 293, 257, 1922, 934, 264, 700], "temperature": 0.0, "avg_logprob": -0.14488148134808207, "compression_ratio": 1.5219298245614035, "no_speech_prob": 1.1911049568880117e-06}, {"id": 638, "seek": 401164, "start": 4030.6, "end": 4038.24, "text": " few and eventually 95.1 that's pretty good we've got an accuracy of over 95 percent was", "tokens": [1326, 293, 4728, 13420, 13, 16, 300, 311, 1238, 665, 321, 600, 658, 364, 14170, 295, 670, 13420, 3043, 390], "temperature": 0.0, "avg_logprob": -0.14488148134808207, "compression_ratio": 1.5219298245614035, "no_speech_prob": 1.1911049568880117e-06}, {"id": 639, "seek": 403824, "start": 4038.24, "end": 4044.3999999999996, "text": " point to a good threshold to pick who knows let's try point one well that's a worse accuracy", "tokens": [935, 281, 257, 665, 14678, 281, 1888, 567, 3255, 718, 311, 853, 935, 472, 731, 300, 311, 257, 5324, 14170], "temperature": 0.0, "avg_logprob": -0.13204698453004332, "compression_ratio": 1.8756476683937824, "no_speech_prob": 3.747988159830129e-07}, {"id": 640, "seek": 403824, "start": 4044.3999999999996, "end": 4052.72, "text": " so I guess in this case we could buy a higher threshold 94 hmm also not good so what's the", "tokens": [370, 286, 2041, 294, 341, 1389, 321, 727, 2256, 257, 2946, 14678, 30849, 16478, 611, 406, 665, 370, 437, 311, 264], "temperature": 0.0, "avg_logprob": -0.13204698453004332, "compression_ratio": 1.8756476683937824, "no_speech_prob": 3.747988159830129e-07}, {"id": 641, "seek": 403824, "start": 4052.72, "end": 4058.16, "text": " best threshold well what we could do is call get preds to get all of the predictions and", "tokens": [1151, 14678, 731, 437, 321, 727, 360, 307, 818, 483, 3852, 82, 281, 483, 439, 295, 264, 21264, 293], "temperature": 0.0, "avg_logprob": -0.13204698453004332, "compression_ratio": 1.8756476683937824, "no_speech_prob": 3.747988159830129e-07}, {"id": 642, "seek": 403824, "start": 4058.16, "end": 4065.52, "text": " all of the targets and then we could calculate the accuracy at some threshold and then we", "tokens": [439, 295, 264, 12911, 293, 550, 321, 727, 8873, 264, 14170, 412, 512, 14678, 293, 550, 321], "temperature": 0.0, "avg_logprob": -0.13204698453004332, "compression_ratio": 1.8756476683937824, "no_speech_prob": 3.747988159830129e-07}, {"id": 643, "seek": 406552, "start": 4065.52, "end": 4073.24, "text": " could say okay let's grab lots of numbers between 0.05 and 0.95 and you with the list", "tokens": [727, 584, 1392, 718, 311, 4444, 3195, 295, 3547, 1296, 1958, 13, 13328, 293, 1958, 13, 15718, 293, 291, 365, 264, 1329], "temperature": 0.0, "avg_logprob": -0.13680642226646686, "compression_ratio": 1.6186046511627907, "no_speech_prob": 1.2679257679337752e-06}, {"id": 644, "seek": 406552, "start": 4073.24, "end": 4079.08, "text": " comprehension calculate the accuracy for all of those different thresholds and plot them", "tokens": [44991, 8873, 264, 14170, 337, 439, 295, 729, 819, 14678, 82, 293, 7542, 552], "temperature": 0.0, "avg_logprob": -0.13680642226646686, "compression_ratio": 1.6186046511627907, "no_speech_prob": 1.2679257679337752e-06}, {"id": 645, "seek": 406552, "start": 4079.08, "end": 4087.84, "text": " ah looks like we want threshold somewhere a bit above 0.5 so cool we can just use that", "tokens": [3716, 1542, 411, 321, 528, 14678, 4079, 257, 857, 3673, 1958, 13, 20, 370, 1627, 321, 393, 445, 764, 300], "temperature": 0.0, "avg_logprob": -0.13680642226646686, "compression_ratio": 1.6186046511627907, "no_speech_prob": 1.2679257679337752e-06}, {"id": 646, "seek": 406552, "start": 4087.84, "end": 4095.16, "text": " and it's going to give us 96 in a bit which is going to give us a better accuracy this", "tokens": [293, 309, 311, 516, 281, 976, 505, 24124, 294, 257, 857, 597, 307, 516, 281, 976, 505, 257, 1101, 14170, 341], "temperature": 0.0, "avg_logprob": -0.13680642226646686, "compression_ratio": 1.6186046511627907, "no_speech_prob": 1.2679257679337752e-06}, {"id": 647, "seek": 409516, "start": 4095.16, "end": 4101.48, "text": " is a you know something that a lot of theoreticians would be uncomfortable about I've used the", "tokens": [307, 257, 291, 458, 746, 300, 257, 688, 295, 14308, 8455, 576, 312, 10532, 466, 286, 600, 1143, 264], "temperature": 0.0, "avg_logprob": -0.08325081642227944, "compression_ratio": 1.7976190476190477, "no_speech_prob": 3.7853067169635324e-06}, {"id": 648, "seek": 409516, "start": 4101.48, "end": 4107.68, "text": " validation set to pick a hyper parameter threshold right and so people might be like oh you're", "tokens": [24071, 992, 281, 1888, 257, 9848, 13075, 14678, 558, 293, 370, 561, 1062, 312, 411, 1954, 291, 434], "temperature": 0.0, "avg_logprob": -0.08325081642227944, "compression_ratio": 1.7976190476190477, "no_speech_prob": 3.7853067169635324e-06}, {"id": 649, "seek": 409516, "start": 4107.68, "end": 4112.76, "text": " overfitting using the validation set to pick a hyper parameter but if you think about it", "tokens": [670, 69, 2414, 1228, 264, 24071, 992, 281, 1888, 257, 9848, 13075, 457, 498, 291, 519, 466, 309], "temperature": 0.0, "avg_logprob": -0.08325081642227944, "compression_ratio": 1.7976190476190477, "no_speech_prob": 3.7853067169635324e-06}, {"id": 650, "seek": 409516, "start": 4112.76, "end": 4116.84, "text": " this is a very smooth curve right it's not some bumpy thing where we've accidentally", "tokens": [341, 307, 257, 588, 5508, 7605, 558, 309, 311, 406, 512, 49400, 551, 689, 321, 600, 15715], "temperature": 0.0, "avg_logprob": -0.08325081642227944, "compression_ratio": 1.7976190476190477, "no_speech_prob": 3.7853067169635324e-06}, {"id": 651, "seek": 409516, "start": 4116.84, "end": 4123.76, "text": " kind of randomly grabbed some unexpectedly good value when you're picking a single number", "tokens": [733, 295, 16979, 18607, 512, 40452, 665, 2158, 562, 291, 434, 8867, 257, 2167, 1230], "temperature": 0.0, "avg_logprob": -0.08325081642227944, "compression_ratio": 1.7976190476190477, "no_speech_prob": 3.7853067169635324e-06}, {"id": 652, "seek": 412376, "start": 4123.76, "end": 4128.2, "text": " from a smooth curve you know this is where the theory of like don't use a validation", "tokens": [490, 257, 5508, 7605, 291, 458, 341, 307, 689, 264, 5261, 295, 411, 500, 380, 764, 257, 24071], "temperature": 0.0, "avg_logprob": -0.11824319362640381, "compression_ratio": 1.6029411764705883, "no_speech_prob": 6.083582775318064e-07}, {"id": 653, "seek": 412376, "start": 4128.2, "end": 4134.84, "text": " set for for hyper parameter tuning doesn't really apply so it's always good to be practical", "tokens": [992, 337, 337, 9848, 13075, 15164, 1177, 380, 534, 3079, 370, 309, 311, 1009, 665, 281, 312, 8496], "temperature": 0.0, "avg_logprob": -0.11824319362640381, "compression_ratio": 1.6029411764705883, "no_speech_prob": 6.083582775318064e-07}, {"id": 654, "seek": 412376, "start": 4134.84, "end": 4142.360000000001, "text": " right don't treat these things as rules but as rules of thumb.", "tokens": [558, 500, 380, 2387, 613, 721, 382, 4474, 457, 382, 4474, 295, 9298, 13], "temperature": 0.0, "avg_logprob": -0.11824319362640381, "compression_ratio": 1.6029411764705883, "no_speech_prob": 6.083582775318064e-07}, {"id": 655, "seek": 412376, "start": 4142.360000000001, "end": 4146.6, "text": " Okay so let's take a break for five minutes and we'll see you back here in five minutes", "tokens": [1033, 370, 718, 311, 747, 257, 1821, 337, 1732, 2077, 293, 321, 603, 536, 291, 646, 510, 294, 1732, 2077], "temperature": 0.0, "avg_logprob": -0.11824319362640381, "compression_ratio": 1.6029411764705883, "no_speech_prob": 6.083582775318064e-07}, {"id": 656, "seek": 414660, "start": 4146.6, "end": 4158.160000000001, "text": " time. Hey welcome back so I want to show you something really cool image regression so", "tokens": [565, 13, 1911, 2928, 646, 370, 286, 528, 281, 855, 291, 746, 534, 1627, 3256, 24590, 370], "temperature": 0.0, "avg_logprob": -0.15153489347364082, "compression_ratio": 1.5568862275449102, "no_speech_prob": 1.6797239368315786e-06}, {"id": 657, "seek": 414660, "start": 4158.160000000001, "end": 4164.04, "text": " we are not going to learn how to use a fast AI image regression application because we", "tokens": [321, 366, 406, 516, 281, 1466, 577, 281, 764, 257, 2370, 7318, 3256, 24590, 3861, 570, 321], "temperature": 0.0, "avg_logprob": -0.15153489347364082, "compression_ratio": 1.5568862275449102, "no_speech_prob": 1.6797239368315786e-06}, {"id": 658, "seek": 414660, "start": 4164.04, "end": 4171.4400000000005, "text": " don't need one now that we know how to build stuff up with boss functions and the data", "tokens": [500, 380, 643, 472, 586, 300, 321, 458, 577, 281, 1322, 1507, 493, 365, 5741, 6828, 293, 264, 1412], "temperature": 0.0, "avg_logprob": -0.15153489347364082, "compression_ratio": 1.5568862275449102, "no_speech_prob": 1.6797239368315786e-06}, {"id": 659, "seek": 417144, "start": 4171.44, "end": 4179.719999999999, "text": " block API ourselves we can invent our own applications so there is no image regression", "tokens": [3461, 9362, 4175, 321, 393, 7962, 527, 1065, 5821, 370, 456, 307, 572, 3256, 24590], "temperature": 0.0, "avg_logprob": -0.09022513890670518, "compression_ratio": 1.6566265060240963, "no_speech_prob": 1.3925402981840307e-06}, {"id": 660, "seek": 417144, "start": 4179.719999999999, "end": 4187.96, "text": " application per se but we can do image regression really easily what do we mean by image regression", "tokens": [3861, 680, 369, 457, 321, 393, 360, 3256, 24590, 534, 3612, 437, 360, 321, 914, 538, 3256, 24590], "temperature": 0.0, "avg_logprob": -0.09022513890670518, "compression_ratio": 1.6566265060240963, "no_speech_prob": 1.3925402981840307e-06}, {"id": 661, "seek": 417144, "start": 4187.96, "end": 4193.919999999999, "text": " well remember back to lesson I think it's lesson one we talked about the two basic types", "tokens": [731, 1604, 646, 281, 6898, 286, 519, 309, 311, 6898, 472, 321, 2825, 466, 264, 732, 3875, 3467], "temperature": 0.0, "avg_logprob": -0.09022513890670518, "compression_ratio": 1.6566265060240963, "no_speech_prob": 1.3925402981840307e-06}, {"id": 662, "seek": 419392, "start": 4193.92, "end": 4201.72, "text": " of machine learning or supervised machine learning regression and classification classification", "tokens": [295, 3479, 2539, 420, 46533, 3479, 2539, 24590, 293, 21538, 21538], "temperature": 0.0, "avg_logprob": -0.10104508330856544, "compression_ratio": 2.152542372881356, "no_speech_prob": 4.313891679430526e-07}, {"id": 663, "seek": 419392, "start": 4201.72, "end": 4209.4800000000005, "text": " is when our dependent variable is a discrete category or set of categories and regression", "tokens": [307, 562, 527, 12334, 7006, 307, 257, 27706, 7719, 420, 992, 295, 10479, 293, 24590], "temperature": 0.0, "avg_logprob": -0.10104508330856544, "compression_ratio": 2.152542372881356, "no_speech_prob": 4.313891679430526e-07}, {"id": 664, "seek": 419392, "start": 4209.4800000000005, "end": 4217.0, "text": " is when our dependent variable is a continuous number like an age or XY coordinate or something", "tokens": [307, 562, 527, 12334, 7006, 307, 257, 10957, 1230, 411, 364, 3205, 420, 48826, 15670, 420, 746], "temperature": 0.0, "avg_logprob": -0.10104508330856544, "compression_ratio": 2.152542372881356, "no_speech_prob": 4.313891679430526e-07}, {"id": 665, "seek": 419392, "start": 4217.0, "end": 4223.24, "text": " like that so image regression means our independent variable is an image and our dependent variable", "tokens": [411, 300, 370, 3256, 24590, 1355, 527, 6695, 7006, 307, 364, 3256, 293, 527, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.10104508330856544, "compression_ratio": 2.152542372881356, "no_speech_prob": 4.313891679430526e-07}, {"id": 666, "seek": 422324, "start": 4223.24, "end": 4234.48, "text": " is a continue one of all continuous value values and so here's what that can look like", "tokens": [307, 257, 2354, 472, 295, 439, 10957, 2158, 4190, 293, 370, 510, 311, 437, 300, 393, 574, 411], "temperature": 0.0, "avg_logprob": -0.1908002521680749, "compression_ratio": 1.5128205128205128, "no_speech_prob": 2.4439771095785545e-06}, {"id": 667, "seek": 422324, "start": 4234.48, "end": 4239.44, "text": " which is the b we head pose data set it has a number of things in it but one of the things", "tokens": [597, 307, 264, 272, 321, 1378, 10774, 1412, 992, 309, 575, 257, 1230, 295, 721, 294, 309, 457, 472, 295, 264, 721], "temperature": 0.0, "avg_logprob": -0.1908002521680749, "compression_ratio": 1.5128205128205128, "no_speech_prob": 2.4439771095785545e-06}, {"id": 668, "seek": 423944, "start": 4239.44, "end": 4257.44, "text": " we can do is find the midpoint of a person's face see so the b we had posed data set so", "tokens": [321, 393, 360, 307, 915, 264, 2062, 6053, 295, 257, 954, 311, 1851, 536, 370, 264, 272, 321, 632, 31399, 1412, 992, 370], "temperature": 0.0, "avg_logprob": -0.16945175248749403, "compression_ratio": 1.5614035087719298, "no_speech_prob": 1.538170977255504e-07}, {"id": 669, "seek": 423944, "start": 4257.44, "end": 4265.96, "text": " the b we had posed data set comes from this paper random forest real-time 3d face analysis", "tokens": [264, 272, 321, 632, 31399, 1412, 992, 1487, 490, 341, 3035, 4974, 6719, 957, 12, 3766, 805, 67, 1851, 5215], "temperature": 0.0, "avg_logprob": -0.16945175248749403, "compression_ratio": 1.5614035087719298, "no_speech_prob": 1.538170977255504e-07}, {"id": 670, "seek": 426596, "start": 4265.96, "end": 4274.76, "text": " so thank you to those authors and we can grab it in the usual way and our data and we can", "tokens": [370, 1309, 291, 281, 729, 16552, 293, 321, 393, 4444, 309, 294, 264, 7713, 636, 293, 527, 1412, 293, 321, 393], "temperature": 0.0, "avg_logprob": -0.11798645839218266, "compression_ratio": 1.9605263157894737, "no_speech_prob": 5.2553241403074935e-06}, {"id": 671, "seek": 426596, "start": 4274.76, "end": 4279.2, "text": " have a look at what's in there and we can see there's 24 directories numbered from not", "tokens": [362, 257, 574, 412, 437, 311, 294, 456, 293, 321, 393, 536, 456, 311, 4022, 5391, 530, 40936, 490, 406], "temperature": 0.0, "avg_logprob": -0.11798645839218266, "compression_ratio": 1.9605263157894737, "no_speech_prob": 5.2553241403074935e-06}, {"id": 672, "seek": 426596, "start": 4279.2, "end": 4285.12, "text": " from 1 to 24 is 1 2 3 and each one also has a dot-op file we're not going to be using", "tokens": [490, 502, 281, 4022, 307, 502, 568, 805, 293, 1184, 472, 611, 575, 257, 5893, 12, 404, 3991, 321, 434, 406, 516, 281, 312, 1228], "temperature": 0.0, "avg_logprob": -0.11798645839218266, "compression_ratio": 1.9605263157894737, "no_speech_prob": 5.2553241403074935e-06}, {"id": 673, "seek": 426596, "start": 4285.12, "end": 4289.96, "text": " the dot-op file I'm just the directories so let's look at one of the directories and as", "tokens": [264, 5893, 12, 404, 3991, 286, 478, 445, 264, 5391, 530, 370, 718, 311, 574, 412, 472, 295, 264, 5391, 530, 293, 382], "temperature": 0.0, "avg_logprob": -0.11798645839218266, "compression_ratio": 1.9605263157894737, "no_speech_prob": 5.2553241403074935e-06}, {"id": 674, "seek": 426596, "start": 4289.96, "end": 4294.68, "text": " you can see there's a thousand things in the first directory so each one of these 24 directories", "tokens": [291, 393, 536, 456, 311, 257, 4714, 721, 294, 264, 700, 21120, 370, 1184, 472, 295, 613, 4022, 5391, 530], "temperature": 0.0, "avg_logprob": -0.11798645839218266, "compression_ratio": 1.9605263157894737, "no_speech_prob": 5.2553241403074935e-06}, {"id": 675, "seek": 429468, "start": 4294.68, "end": 4302.4400000000005, "text": " is one different person that they photographed and you can see for each person there's frame", "tokens": [307, 472, 819, 954, 300, 436, 45067, 293, 291, 393, 536, 337, 1184, 954, 456, 311, 3920], "temperature": 0.0, "avg_logprob": -0.11289509574135581, "compression_ratio": 1.816326530612245, "no_speech_prob": 2.2732668014668889e-07}, {"id": 676, "seek": 429468, "start": 4302.4400000000005, "end": 4310.9800000000005, "text": " 3 pose frame 3 RGB frame 4 pose frame 4 RGB and so forth so in each case we've got the", "tokens": [805, 10774, 3920, 805, 31231, 3920, 1017, 10774, 3920, 1017, 31231, 293, 370, 5220, 370, 294, 1184, 1389, 321, 600, 658, 264], "temperature": 0.0, "avg_logprob": -0.11289509574135581, "compression_ratio": 1.816326530612245, "no_speech_prob": 2.2732668014668889e-07}, {"id": 677, "seek": 429468, "start": 4310.9800000000005, "end": 4318.400000000001, "text": " image which is the RGB and we've got the pose is the pose dot text so as we've seen we can", "tokens": [3256, 597, 307, 264, 31231, 293, 321, 600, 658, 264, 10774, 307, 264, 10774, 5893, 2487, 370, 382, 321, 600, 1612, 321, 393], "temperature": 0.0, "avg_logprob": -0.11289509574135581, "compression_ratio": 1.816326530612245, "no_speech_prob": 2.2732668014668889e-07}, {"id": 678, "seek": 429468, "start": 4318.400000000001, "end": 4323.280000000001, "text": " grab use get image files to get a list of all of the files image files recursively in", "tokens": [4444, 764, 483, 3256, 7098, 281, 483, 257, 1329, 295, 439, 295, 264, 7098, 3256, 7098, 20560, 3413, 294], "temperature": 0.0, "avg_logprob": -0.11289509574135581, "compression_ratio": 1.816326530612245, "no_speech_prob": 2.2732668014668889e-07}, {"id": 679, "seek": 432328, "start": 4323.28, "end": 4332.96, "text": " a path and so once we have an image file name like this one sorry like this one we can turn", "tokens": [257, 3100, 293, 370, 1564, 321, 362, 364, 3256, 3991, 1315, 411, 341, 472, 2597, 411, 341, 472, 321, 393, 1261], "temperature": 0.0, "avg_logprob": -0.08495955467224121, "compression_ratio": 1.6951219512195121, "no_speech_prob": 3.8669597302032344e-07}, {"id": 680, "seek": 432328, "start": 4332.96, "end": 4339.96, "text": " it into a pose file name by removing the last one two three five six seven letters and adding", "tokens": [309, 666, 257, 10774, 3991, 1315, 538, 12720, 264, 1036, 472, 732, 1045, 1732, 2309, 3407, 7825, 293, 5127], "temperature": 0.0, "avg_logprob": -0.08495955467224121, "compression_ratio": 1.6951219512195121, "no_speech_prob": 3.8669597302032344e-07}, {"id": 681, "seek": 432328, "start": 4339.96, "end": 4347.639999999999, "text": " back on pose dot text and so here is a function that does that and so you can hear see I can", "tokens": [646, 322, 10774, 5893, 2487, 293, 370, 510, 307, 257, 2445, 300, 775, 300, 293, 370, 291, 393, 1568, 536, 286, 393], "temperature": 0.0, "avg_logprob": -0.08495955467224121, "compression_ratio": 1.6951219512195121, "no_speech_prob": 3.8669597302032344e-07}, {"id": 682, "seek": 434764, "start": 4347.64, "end": 4356.4400000000005, "text": " pass in an image file to image to pose and get back a pose file right so PIO image dot", "tokens": [1320, 294, 364, 3256, 3991, 281, 3256, 281, 10774, 293, 483, 646, 257, 10774, 3991, 558, 370, 430, 15167, 3256, 5893], "temperature": 0.0, "avg_logprob": -0.16206384741741678, "compression_ratio": 1.6424242424242423, "no_speech_prob": 1.1189378028575447e-06}, {"id": 683, "seek": 434764, "start": 4356.4400000000005, "end": 4365.860000000001, "text": " create is the faster I way to create an image at least a PIO image it has a shape in computer", "tokens": [1884, 307, 264, 4663, 286, 636, 281, 1884, 364, 3256, 412, 1935, 257, 430, 15167, 3256, 309, 575, 257, 3909, 294, 3820], "temperature": 0.0, "avg_logprob": -0.16206384741741678, "compression_ratio": 1.6424242424242423, "no_speech_prob": 1.1189378028575447e-06}, {"id": 684, "seek": 434764, "start": 4365.860000000001, "end": 4371.34, "text": " vision they're normally backwards they normally do columns by rows so that's why it's sway", "tokens": [5201, 436, 434, 5646, 12204, 436, 5646, 360, 13766, 538, 13241, 370, 300, 311, 983, 309, 311, 27555], "temperature": 0.0, "avg_logprob": -0.16206384741741678, "compression_ratio": 1.6424242424242423, "no_speech_prob": 1.1189378028575447e-06}, {"id": 685, "seek": 437134, "start": 4371.34, "end": 4377.96, "text": " around where else pie torch numpy tensors and arrays are rows by columns so that's confusing", "tokens": [926, 689, 1646, 1730, 27822, 1031, 8200, 10688, 830, 293, 41011, 366, 13241, 538, 13766, 370, 300, 311, 13181], "temperature": 0.0, "avg_logprob": -0.1820444410497492, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.737895267226122e-07}, {"id": 686, "seek": 437134, "start": 4377.96, "end": 4386.12, "text": " but that's just how things are I'm afraid so here's an example of an image when you", "tokens": [457, 300, 311, 445, 577, 721, 366, 286, 478, 4638, 370, 510, 311, 364, 1365, 295, 364, 3256, 562, 291], "temperature": 0.0, "avg_logprob": -0.1820444410497492, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.737895267226122e-07}, {"id": 687, "seek": 437134, "start": 4386.12, "end": 4391.12, "text": " look at the readme from the data set website they tell you how to get the center point", "tokens": [574, 412, 264, 1401, 1398, 490, 264, 1412, 992, 3144, 436, 980, 291, 577, 281, 483, 264, 3056, 935], "temperature": 0.0, "avg_logprob": -0.1820444410497492, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.737895267226122e-07}, {"id": 688, "seek": 437134, "start": 4391.12, "end": 4397.12, "text": " from from one of the text files and it's just this function so it doesn't matter it just", "tokens": [490, 490, 472, 295, 264, 2487, 7098, 293, 309, 311, 445, 341, 2445, 370, 309, 1177, 380, 1871, 309, 445], "temperature": 0.0, "avg_logprob": -0.1820444410497492, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.737895267226122e-07}, {"id": 689, "seek": 439712, "start": 4397.12, "end": 4403.32, "text": " it is what it is we call it get center and it will return the XY coordinate of the center", "tokens": [309, 307, 437, 309, 307, 321, 818, 309, 483, 3056, 293, 309, 486, 2736, 264, 48826, 15670, 295, 264, 3056], "temperature": 0.0, "avg_logprob": -0.12306829811870187, "compression_ratio": 1.6484848484848484, "no_speech_prob": 2.7420873038863647e-07}, {"id": 690, "seek": 439712, "start": 4403.32, "end": 4410.48, "text": " of the person's head face so we can pass this as get why because get why remember is a thing", "tokens": [295, 264, 954, 311, 1378, 1851, 370, 321, 393, 1320, 341, 382, 483, 983, 570, 483, 983, 1604, 307, 257, 551], "temperature": 0.0, "avg_logprob": -0.12306829811870187, "compression_ratio": 1.6484848484848484, "no_speech_prob": 2.7420873038863647e-07}, {"id": 691, "seek": 439712, "start": 4410.48, "end": 4422.0199999999995, "text": " that gives us back the label okay so so here's the thing right we can create a data block", "tokens": [300, 2709, 505, 646, 264, 7645, 1392, 370, 370, 510, 311, 264, 551, 558, 321, 393, 1884, 257, 1412, 3461], "temperature": 0.0, "avg_logprob": -0.12306829811870187, "compression_ratio": 1.6484848484848484, "no_speech_prob": 2.7420873038863647e-07}, {"id": 692, "seek": 442202, "start": 4422.02, "end": 4427.68, "text": " and we can pass in as the independent variables block image block as usual and then the dependent", "tokens": [293, 321, 393, 1320, 294, 382, 264, 6695, 9102, 3461, 3256, 3461, 382, 7713, 293, 550, 264, 12334], "temperature": 0.0, "avg_logprob": -0.08381541570027669, "compression_ratio": 1.9267015706806283, "no_speech_prob": 2.785269828109449e-07}, {"id": 693, "seek": 442202, "start": 4427.68, "end": 4433.3, "text": " variables block we can say point block which is a tensor with two values in and now by", "tokens": [9102, 3461, 321, 393, 584, 935, 3461, 597, 307, 257, 40863, 365, 732, 4190, 294, 293, 586, 538], "temperature": 0.0, "avg_logprob": -0.08381541570027669, "compression_ratio": 1.9267015706806283, "no_speech_prob": 2.785269828109449e-07}, {"id": 694, "seek": 442202, "start": 4433.3, "end": 4440.22, "text": " combining these two things this says we want to do image regression with a dependent variable", "tokens": [21928, 613, 732, 721, 341, 1619, 321, 528, 281, 360, 3256, 24590, 365, 257, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.08381541570027669, "compression_ratio": 1.9267015706806283, "no_speech_prob": 2.785269828109449e-07}, {"id": 695, "seek": 442202, "start": 4440.22, "end": 4446.68, "text": " with two continuous values to get the items you call get image files to get the why we'll", "tokens": [365, 732, 10957, 4190, 281, 483, 264, 4754, 291, 818, 483, 3256, 7098, 281, 483, 264, 983, 321, 603], "temperature": 0.0, "avg_logprob": -0.08381541570027669, "compression_ratio": 1.9267015706806283, "no_speech_prob": 2.785269828109449e-07}, {"id": 696, "seek": 444668, "start": 4446.68, "end": 4454.360000000001, "text": " call the get center function to split it so this is important we should make sure that", "tokens": [818, 264, 483, 3056, 2445, 281, 7472, 309, 370, 341, 307, 1021, 321, 820, 652, 988, 300], "temperature": 0.0, "avg_logprob": -0.08161754724456043, "compression_ratio": 1.617117117117117, "no_speech_prob": 3.7479895809156005e-07}, {"id": 697, "seek": 444668, "start": 4454.360000000001, "end": 4463.6, "text": " the validation set contains one or more people that don't appear in the training set so I'm", "tokens": [264, 24071, 992, 8306, 472, 420, 544, 561, 300, 500, 380, 4204, 294, 264, 3097, 992, 370, 286, 478], "temperature": 0.0, "avg_logprob": -0.08161754724456043, "compression_ratio": 1.617117117117117, "no_speech_prob": 3.7479895809156005e-07}, {"id": 698, "seek": 444668, "start": 4463.6, "end": 4469.64, "text": " just going to grab person number 13 just grabbed it randomly and I'll use all of those images", "tokens": [445, 516, 281, 4444, 954, 1230, 3705, 445, 18607, 309, 16979, 293, 286, 603, 764, 439, 295, 729, 5267], "temperature": 0.0, "avg_logprob": -0.08161754724456043, "compression_ratio": 1.617117117117117, "no_speech_prob": 3.7479895809156005e-07}, {"id": 699, "seek": 444668, "start": 4469.64, "end": 4476.14, "text": " as the validation set because I think they did this with a Xbox connect you know video", "tokens": [382, 264, 24071, 992, 570, 286, 519, 436, 630, 341, 365, 257, 14544, 1745, 291, 458, 960], "temperature": 0.0, "avg_logprob": -0.08161754724456043, "compression_ratio": 1.617117117117117, "no_speech_prob": 3.7479895809156005e-07}, {"id": 700, "seek": 447614, "start": 4476.14, "end": 4481.84, "text": " thing so there's a lot of images that look almost identical so if you randomly assigned", "tokens": [551, 370, 456, 311, 257, 688, 295, 5267, 300, 574, 1920, 14800, 370, 498, 291, 16979, 13279], "temperature": 0.0, "avg_logprob": -0.09022202058271929, "compression_ratio": 1.790513833992095, "no_speech_prob": 1.2482670399549534e-06}, {"id": 701, "seek": 447614, "start": 4481.84, "end": 4487.08, "text": " them then you would be massively overestimating how effective you are you want to make sure", "tokens": [552, 550, 291, 576, 312, 29379, 670, 377, 332, 990, 577, 4942, 291, 366, 291, 528, 281, 652, 988], "temperature": 0.0, "avg_logprob": -0.09022202058271929, "compression_ratio": 1.790513833992095, "no_speech_prob": 1.2482670399549534e-06}, {"id": 702, "seek": 447614, "start": 4487.08, "end": 4492.58, "text": " that you're actually doing a good job with a random with a new set of people not just", "tokens": [300, 291, 434, 767, 884, 257, 665, 1691, 365, 257, 4974, 365, 257, 777, 992, 295, 561, 406, 445], "temperature": 0.0, "avg_logprob": -0.09022202058271929, "compression_ratio": 1.790513833992095, "no_speech_prob": 1.2482670399549534e-06}, {"id": 703, "seek": 447614, "start": 4492.58, "end": 4498.4800000000005, "text": " a new set of frames that's why we use this and so funksplitter is a splitter that takes", "tokens": [257, 777, 992, 295, 12083, 300, 311, 983, 321, 764, 341, 293, 370, 1019, 1694, 564, 3904, 307, 257, 4732, 3904, 300, 2516], "temperature": 0.0, "avg_logprob": -0.09022202058271929, "compression_ratio": 1.790513833992095, "no_speech_prob": 1.2482670399549534e-06}, {"id": 704, "seek": 447614, "start": 4498.4800000000005, "end": 4505.08, "text": " a function and in this case we're using lambda to create the function we will use data augmentation", "tokens": [257, 2445, 293, 294, 341, 1389, 321, 434, 1228, 13607, 281, 1884, 264, 2445, 321, 486, 764, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.09022202058271929, "compression_ratio": 1.790513833992095, "no_speech_prob": 1.2482670399549534e-06}, {"id": 705, "seek": 450508, "start": 4505.08, "end": 4513.64, "text": " and we will also normalize so this is actually done automatically now but in this case we're", "tokens": [293, 321, 486, 611, 2710, 1125, 370, 341, 307, 767, 1096, 6772, 586, 457, 294, 341, 1389, 321, 434], "temperature": 0.0, "avg_logprob": -0.11014784299410306, "compression_ratio": 1.5895953757225434, "no_speech_prob": 1.4367467429110548e-06}, {"id": 706, "seek": 450508, "start": 4513.64, "end": 4521.8, "text": " doing it manually so this is going to subtract the mean and divide by the standard deviation", "tokens": [884, 309, 16945, 370, 341, 307, 516, 281, 16390, 264, 914, 293, 9845, 538, 264, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.11014784299410306, "compression_ratio": 1.5895953757225434, "no_speech_prob": 1.4367467429110548e-06}, {"id": 707, "seek": 450508, "start": 4521.8, "end": 4528.64, "text": " of the original data set that the pre-trained model used which is image net so that's our", "tokens": [295, 264, 3380, 1412, 992, 300, 264, 659, 12, 17227, 2001, 2316, 1143, 597, 307, 3256, 2533, 370, 300, 311, 527], "temperature": 0.0, "avg_logprob": -0.11014784299410306, "compression_ratio": 1.5895953757225434, "no_speech_prob": 1.4367467429110548e-06}, {"id": 708, "seek": 452864, "start": 4528.64, "end": 4537.0, "text": " data block and so we can call data loaders to get our data loaders passing in the path", "tokens": [1412, 3461, 293, 370, 321, 393, 818, 1412, 3677, 433, 281, 483, 527, 1412, 3677, 433, 8437, 294, 264, 3100], "temperature": 0.0, "avg_logprob": -0.11902879632037619, "compression_ratio": 1.7846153846153847, "no_speech_prob": 7.811472073626646e-07}, {"id": 709, "seek": 452864, "start": 4537.0, "end": 4543.56, "text": " and show batch and we can see that looks good right here's our faces and the points and", "tokens": [293, 855, 15245, 293, 321, 393, 536, 300, 1542, 665, 558, 510, 311, 527, 8475, 293, 264, 2793, 293], "temperature": 0.0, "avg_logprob": -0.11902879632037619, "compression_ratio": 1.7846153846153847, "no_speech_prob": 7.811472073626646e-07}, {"id": 710, "seek": 452864, "start": 4543.56, "end": 4548.0, "text": " so let's like particularly for as a student don't just look at the pictures look at the", "tokens": [370, 718, 311, 411, 4098, 337, 382, 257, 3107, 500, 380, 445, 574, 412, 264, 5242, 574, 412, 264], "temperature": 0.0, "avg_logprob": -0.11902879632037619, "compression_ratio": 1.7846153846153847, "no_speech_prob": 7.811472073626646e-07}, {"id": 711, "seek": 452864, "start": 4548.0, "end": 4553.88, "text": " actual data so grab a batch put it into an xb and a yb x patch and y batch and have a", "tokens": [3539, 1412, 370, 4444, 257, 15245, 829, 309, 666, 364, 2031, 65, 293, 257, 288, 65, 2031, 9972, 293, 288, 15245, 293, 362, 257], "temperature": 0.0, "avg_logprob": -0.11902879632037619, "compression_ratio": 1.7846153846153847, "no_speech_prob": 7.811472073626646e-07}, {"id": 712, "seek": 455388, "start": 4553.88, "end": 4565.0, "text": " look at the shapes and make sure that makes sense the y is 64 by 1 by 2 so it's 64 in", "tokens": [574, 412, 264, 10854, 293, 652, 988, 300, 1669, 2020, 264, 288, 307, 12145, 538, 502, 538, 568, 370, 309, 311, 12145, 294], "temperature": 0.0, "avg_logprob": -0.20106074389289408, "compression_ratio": 1.4634146341463414, "no_speech_prob": 2.4824712454574183e-06}, {"id": 713, "seek": 455388, "start": 4565.0, "end": 4578.68, "text": " the mini batch 64 rows and then a the coordinates is a 1 by 2 tensor so there's a single point", "tokens": [264, 8382, 15245, 12145, 13241, 293, 550, 257, 264, 21056, 307, 257, 502, 538, 568, 40863, 370, 456, 311, 257, 2167, 935], "temperature": 0.0, "avg_logprob": -0.20106074389289408, "compression_ratio": 1.4634146341463414, "no_speech_prob": 2.4824712454574183e-06}, {"id": 714, "seek": 457868, "start": 4578.68, "end": 4584.900000000001, "text": " with two things in it it's like you could have like hands face and armpits or whatever", "tokens": [365, 732, 721, 294, 309, 309, 311, 411, 291, 727, 362, 411, 2377, 1851, 293, 44541, 1208, 420, 2035], "temperature": 0.0, "avg_logprob": -0.11032453113132053, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.9947244709328515e-06}, {"id": 715, "seek": 457868, "start": 4584.900000000001, "end": 4590.04, "text": " or nose and ears and mouth so in this case we're just using one point and the point is", "tokens": [420, 6690, 293, 8798, 293, 4525, 370, 294, 341, 1389, 321, 434, 445, 1228, 472, 935, 293, 264, 935, 307], "temperature": 0.0, "avg_logprob": -0.11032453113132053, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.9947244709328515e-06}, {"id": 716, "seek": 457868, "start": 4590.04, "end": 4598.360000000001, "text": " represented by two values the x and the y and then why is this 64 by 3 by 240 by 320", "tokens": [10379, 538, 732, 4190, 264, 2031, 293, 264, 288, 293, 550, 983, 307, 341, 12145, 538, 805, 538, 26837, 538, 42429], "temperature": 0.0, "avg_logprob": -0.11032453113132053, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.9947244709328515e-06}, {"id": 717, "seek": 457868, "start": 4598.360000000001, "end": 4603.240000000001, "text": " well there's 240 rows by 320 columns that's the pixels that's the size of the images that", "tokens": [731, 456, 311, 26837, 13241, 538, 42429, 13766, 300, 311, 264, 18668, 300, 311, 264, 2744, 295, 264, 5267, 300], "temperature": 0.0, "avg_logprob": -0.11032453113132053, "compression_ratio": 1.7058823529411764, "no_speech_prob": 1.9947244709328515e-06}, {"id": 718, "seek": 460324, "start": 4603.24, "end": 4611.679999999999, "text": " we're using many batches 64 items and what's the three the three is the number of channels", "tokens": [321, 434, 1228, 867, 15245, 279, 12145, 4754, 293, 437, 311, 264, 1045, 264, 1045, 307, 264, 1230, 295, 9235], "temperature": 0.0, "avg_logprob": -0.09099403294649991, "compression_ratio": 1.6144578313253013, "no_speech_prob": 1.5294091326722992e-06}, {"id": 719, "seek": 460324, "start": 4611.679999999999, "end": 4619.44, "text": " which in this case means the number of colors if we open up some random grizzly bear image", "tokens": [597, 294, 341, 1389, 1355, 264, 1230, 295, 4577, 498, 321, 1269, 493, 512, 4974, 17865, 4313, 356, 6155, 3256], "temperature": 0.0, "avg_logprob": -0.09099403294649991, "compression_ratio": 1.6144578313253013, "no_speech_prob": 1.5294091326722992e-06}, {"id": 720, "seek": 460324, "start": 4619.44, "end": 4632.4, "text": " and then we go through each of the elements of the first axis and through a show image", "tokens": [293, 550, 321, 352, 807, 1184, 295, 264, 4959, 295, 264, 700, 10298, 293, 807, 257, 855, 3256], "temperature": 0.0, "avg_logprob": -0.09099403294649991, "compression_ratio": 1.6144578313253013, "no_speech_prob": 1.5294091326722992e-06}, {"id": 721, "seek": 463240, "start": 4632.4, "end": 4640.32, "text": " you can see that it's got the red the green and the blue as the three channels so that's", "tokens": [291, 393, 536, 300, 309, 311, 658, 264, 2182, 264, 3092, 293, 264, 3344, 382, 264, 1045, 9235, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.130591182918339, "compression_ratio": 1.8358974358974358, "no_speech_prob": 1.5534943713646499e-06}, {"id": 722, "seek": 463240, "start": 4640.32, "end": 4648.04, "text": " how we store a three channel image is it stored as a three by number of rows by number of", "tokens": [577, 321, 3531, 257, 1045, 2269, 3256, 307, 309, 12187, 382, 257, 1045, 538, 1230, 295, 13241, 538, 1230, 295], "temperature": 0.0, "avg_logprob": -0.130591182918339, "compression_ratio": 1.8358974358974358, "no_speech_prob": 1.5534943713646499e-06}, {"id": 723, "seek": 463240, "start": 4648.04, "end": 4654.719999999999, "text": " columns rank three tensor and so a mini batch of those is a rank four tensor so that's why", "tokens": [13766, 6181, 1045, 40863, 293, 370, 257, 8382, 15245, 295, 729, 307, 257, 6181, 1451, 40863, 370, 300, 311, 983], "temperature": 0.0, "avg_logprob": -0.130591182918339, "compression_ratio": 1.8358974358974358, "no_speech_prob": 1.5534943713646499e-06}, {"id": 724, "seek": 463240, "start": 4654.719999999999, "end": 4662.379999999999, "text": " this is that shape so here's a row from the dependent variable okay there's that x y low", "tokens": [341, 307, 300, 3909, 370, 510, 311, 257, 5386, 490, 264, 12334, 7006, 1392, 456, 311, 300, 2031, 288, 2295], "temperature": 0.0, "avg_logprob": -0.130591182918339, "compression_ratio": 1.8358974358974358, "no_speech_prob": 1.5534943713646499e-06}, {"id": 725, "seek": 466238, "start": 4662.38, "end": 4670.08, "text": " location we talked about so we can now go ahead and create a learner passing in our", "tokens": [4914, 321, 2825, 466, 370, 321, 393, 586, 352, 2286, 293, 1884, 257, 33347, 8437, 294, 527], "temperature": 0.0, "avg_logprob": -0.10755323522231158, "compression_ratio": 1.680952380952381, "no_speech_prob": 3.5559735351853305e-06}, {"id": 726, "seek": 466238, "start": 4670.08, "end": 4675.84, "text": " data loaders as usual passing in a pre-trained architecture as usual and if you think back", "tokens": [1412, 3677, 433, 382, 7713, 8437, 294, 257, 659, 12, 17227, 2001, 9482, 382, 7713, 293, 498, 291, 519, 646], "temperature": 0.0, "avg_logprob": -0.10755323522231158, "compression_ratio": 1.680952380952381, "no_speech_prob": 3.5559735351853305e-06}, {"id": 727, "seek": 466238, "start": 4675.84, "end": 4685.72, "text": " you may just remember in lesson one we learned about y range y range is where we tell fast", "tokens": [291, 815, 445, 1604, 294, 6898, 472, 321, 3264, 466, 288, 3613, 288, 3613, 307, 689, 321, 980, 2370], "temperature": 0.0, "avg_logprob": -0.10755323522231158, "compression_ratio": 1.680952380952381, "no_speech_prob": 3.5559735351853305e-06}, {"id": 728, "seek": 466238, "start": 4685.72, "end": 4691.76, "text": " AI what range of data we expect to see in the dependent variable so we want to use this", "tokens": [7318, 437, 3613, 295, 1412, 321, 2066, 281, 536, 294, 264, 12334, 7006, 370, 321, 528, 281, 764, 341], "temperature": 0.0, "avg_logprob": -0.10755323522231158, "compression_ratio": 1.680952380952381, "no_speech_prob": 3.5559735351853305e-06}, {"id": 729, "seek": 469176, "start": 4691.76, "end": 4698.46, "text": " generally when we're doing regression so the range of our coordinates is between minus", "tokens": [5101, 562, 321, 434, 884, 24590, 370, 264, 3613, 295, 527, 21056, 307, 1296, 3175], "temperature": 0.0, "avg_logprob": -0.12651627038114815, "compression_ratio": 1.8578199052132702, "no_speech_prob": 2.6016034553322243e-06}, {"id": 730, "seek": 469176, "start": 4698.46, "end": 4705.2, "text": " one and one that's how fast AI and pytorch treats coordinates the left-hand side is minus", "tokens": [472, 293, 472, 300, 311, 577, 2370, 7318, 293, 25878, 284, 339, 19566, 21056, 264, 1411, 12, 5543, 1252, 307, 3175], "temperature": 0.0, "avg_logprob": -0.12651627038114815, "compression_ratio": 1.8578199052132702, "no_speech_prob": 2.6016034553322243e-06}, {"id": 731, "seek": 469176, "start": 4705.2, "end": 4711.64, "text": " one or the top is minus one and the bottom and the right one so there's no point predicting", "tokens": [472, 420, 264, 1192, 307, 3175, 472, 293, 264, 2767, 293, 264, 558, 472, 370, 456, 311, 572, 935, 32884], "temperature": 0.0, "avg_logprob": -0.12651627038114815, "compression_ratio": 1.8578199052132702, "no_speech_prob": 2.6016034553322243e-06}, {"id": 732, "seek": 469176, "start": 4711.64, "end": 4715.12, "text": " something that's smaller than minus one or bigger than one because that is not in the", "tokens": [746, 300, 311, 4356, 813, 3175, 472, 420, 3801, 813, 472, 570, 300, 307, 406, 294, 264], "temperature": 0.0, "avg_logprob": -0.12651627038114815, "compression_ratio": 1.8578199052132702, "no_speech_prob": 2.6016034553322243e-06}, {"id": 733, "seek": 469176, "start": 4715.12, "end": 4717.320000000001, "text": " area that we use for our coordinates.", "tokens": [1859, 300, 321, 764, 337, 527, 21056, 13], "temperature": 0.0, "avg_logprob": -0.12651627038114815, "compression_ratio": 1.8578199052132702, "no_speech_prob": 2.6016034553322243e-06}, {"id": 734, "seek": 471732, "start": 4717.32, "end": 4726.2, "text": " I have a question. Sure just a moment. So how is y range work? Well it actually uses", "tokens": [286, 362, 257, 1168, 13, 4894, 445, 257, 1623, 13, 407, 577, 307, 288, 3613, 589, 30, 1042, 309, 767, 4960], "temperature": 0.0, "avg_logprob": -0.14233440671648298, "compression_ratio": 1.5449101796407185, "no_speech_prob": 1.228916403306357e-06}, {"id": 735, "seek": 471732, "start": 4726.2, "end": 4731.799999999999, "text": " this function called sigmoid range which takes the sigmoid of x multiplies by high minus", "tokens": [341, 2445, 1219, 4556, 3280, 327, 3613, 597, 2516, 264, 4556, 3280, 327, 295, 2031, 12788, 530, 538, 1090, 3175], "temperature": 0.0, "avg_logprob": -0.14233440671648298, "compression_ratio": 1.5449101796407185, "no_speech_prob": 1.228916403306357e-06}, {"id": 736, "seek": 471732, "start": 4731.799999999999, "end": 4740.16, "text": " low and adds low and here is what sigmoid range looks like for minus one to one it's", "tokens": [2295, 293, 10860, 2295, 293, 510, 307, 437, 4556, 3280, 327, 3613, 1542, 411, 337, 3175, 472, 281, 472, 309, 311], "temperature": 0.0, "avg_logprob": -0.14233440671648298, "compression_ratio": 1.5449101796407185, "no_speech_prob": 1.228916403306357e-06}, {"id": 737, "seek": 474016, "start": 4740.16, "end": 4747.76, "text": " just a sigmoid where the bottom is the low and the top is the high and so that way all", "tokens": [445, 257, 4556, 3280, 327, 689, 264, 2767, 307, 264, 2295, 293, 264, 1192, 307, 264, 1090, 293, 370, 300, 636, 439], "temperature": 0.0, "avg_logprob": -0.11698365765948628, "compression_ratio": 1.5803571428571428, "no_speech_prob": 1.422577042831108e-07}, {"id": 738, "seek": 474016, "start": 4747.76, "end": 4755.84, "text": " of our activations are going to be mapped to the range from minus one to one. Yes Rachel.", "tokens": [295, 527, 2430, 763, 366, 516, 281, 312, 33318, 281, 264, 3613, 490, 3175, 472, 281, 472, 13, 1079, 14246, 13], "temperature": 0.0, "avg_logprob": -0.11698365765948628, "compression_ratio": 1.5803571428571428, "no_speech_prob": 1.422577042831108e-07}, {"id": 739, "seek": 474016, "start": 4755.84, "end": 4761.4, "text": " Can you provide images with an arbitrary number of channels as inputs specifically more than", "tokens": [1664, 291, 2893, 5267, 365, 364, 23211, 1230, 295, 9235, 382, 15743, 4682, 544, 813], "temperature": 0.0, "avg_logprob": -0.11698365765948628, "compression_ratio": 1.5803571428571428, "no_speech_prob": 1.422577042831108e-07}, {"id": 740, "seek": 474016, "start": 4761.4, "end": 4766.639999999999, "text": " three channels? Yeah you can have as many channels as you like. We've certainly seen", "tokens": [1045, 9235, 30, 865, 291, 393, 362, 382, 867, 9235, 382, 291, 411, 13, 492, 600, 3297, 1612], "temperature": 0.0, "avg_logprob": -0.11698365765948628, "compression_ratio": 1.5803571428571428, "no_speech_prob": 1.422577042831108e-07}, {"id": 741, "seek": 476664, "start": 4766.64, "end": 4772.76, "text": " images with less than three because we've been grayscale. More than three is common", "tokens": [5267, 365, 1570, 813, 1045, 570, 321, 600, 668, 677, 3772, 37088, 13, 5048, 813, 1045, 307, 2689], "temperature": 0.0, "avg_logprob": -0.14685842215296735, "compression_ratio": 1.6153846153846154, "no_speech_prob": 6.893596378176881e-07}, {"id": 742, "seek": 476664, "start": 4772.76, "end": 4779.160000000001, "text": " as well you could have like an infrared band or like satellite images often have multispectral", "tokens": [382, 731, 291, 727, 362, 411, 364, 30361, 4116, 420, 411, 16016, 5267, 2049, 362, 2120, 271, 1043, 2155], "temperature": 0.0, "avg_logprob": -0.14685842215296735, "compression_ratio": 1.6153846153846154, "no_speech_prob": 6.893596378176881e-07}, {"id": 743, "seek": 476664, "start": 4779.160000000001, "end": 4783.360000000001, "text": " as some kinds of medical images where there are bands that are kind of outside the visible", "tokens": [382, 512, 3685, 295, 4625, 5267, 689, 456, 366, 13543, 300, 366, 733, 295, 2380, 264, 8974], "temperature": 0.0, "avg_logprob": -0.14685842215296735, "compression_ratio": 1.6153846153846154, "no_speech_prob": 6.893596378176881e-07}, {"id": 744, "seek": 476664, "start": 4783.360000000001, "end": 4792.240000000001, "text": " range. Your pre-trained model will generally have three channels. The fast AI does some", "tokens": [3613, 13, 2260, 659, 12, 17227, 2001, 2316, 486, 5101, 362, 1045, 9235, 13, 440, 2370, 7318, 775, 512], "temperature": 0.0, "avg_logprob": -0.14685842215296735, "compression_ratio": 1.6153846153846154, "no_speech_prob": 6.893596378176881e-07}, {"id": 745, "seek": 479224, "start": 4792.24, "end": 4800.16, "text": " tricks to use three channel pre-trained models for non three channel data but that's the", "tokens": [11733, 281, 764, 1045, 2269, 659, 12, 17227, 2001, 5245, 337, 2107, 1045, 2269, 1412, 457, 300, 311, 264], "temperature": 0.0, "avg_logprob": -0.12283848632465709, "compression_ratio": 1.721698113207547, "no_speech_prob": 3.288732159489882e-06}, {"id": 746, "seek": 479224, "start": 4800.16, "end": 4806.04, "text": " only tricky bit other than that it's just just a you know it's just an axis that happens", "tokens": [787, 12414, 857, 661, 813, 300, 309, 311, 445, 445, 257, 291, 458, 309, 311, 445, 364, 10298, 300, 2314], "temperature": 0.0, "avg_logprob": -0.12283848632465709, "compression_ratio": 1.721698113207547, "no_speech_prob": 3.288732159489882e-06}, {"id": 747, "seek": 479224, "start": 4806.04, "end": 4811.84, "text": " to have four things or two things or one thing instead of three things there's nothing special", "tokens": [281, 362, 1451, 721, 420, 732, 721, 420, 472, 551, 2602, 295, 1045, 721, 456, 311, 1825, 2121], "temperature": 0.0, "avg_logprob": -0.12283848632465709, "compression_ratio": 1.721698113207547, "no_speech_prob": 3.288732159489882e-06}, {"id": 748, "seek": 479224, "start": 4811.84, "end": 4821.32, "text": " about it. Okay we didn't specify a loss function here so we get whatever it gave us which is", "tokens": [466, 309, 13, 1033, 321, 994, 380, 16500, 257, 4470, 2445, 510, 370, 321, 483, 2035, 309, 2729, 505, 597, 307], "temperature": 0.0, "avg_logprob": -0.12283848632465709, "compression_ratio": 1.721698113207547, "no_speech_prob": 3.288732159489882e-06}, {"id": 749, "seek": 482132, "start": 4821.32, "end": 4826.4, "text": " a MSE loss so MSE loss is mean squared error and that makes perfect sense right you would", "tokens": [257, 376, 5879, 4470, 370, 376, 5879, 4470, 307, 914, 8889, 6713, 293, 300, 1669, 2176, 2020, 558, 291, 576], "temperature": 0.0, "avg_logprob": -0.11533192225864955, "compression_ratio": 1.7251184834123223, "no_speech_prob": 1.7061776134141837e-06}, {"id": 750, "seek": 482132, "start": 4826.4, "end": 4832.12, "text": " expect mean squared error to be a reasonable thing to use for regression we're just testing", "tokens": [2066, 914, 8889, 6713, 281, 312, 257, 10585, 551, 281, 764, 337, 24590, 321, 434, 445, 4997], "temperature": 0.0, "avg_logprob": -0.11533192225864955, "compression_ratio": 1.7251184834123223, "no_speech_prob": 1.7061776134141837e-06}, {"id": 751, "seek": 482132, "start": 4832.12, "end": 4841.04, "text": " how close we are through the target and then taking the square taking them. We didn't specify", "tokens": [577, 1998, 321, 366, 807, 264, 3779, 293, 550, 1940, 264, 3732, 1940, 552, 13, 492, 994, 380, 16500], "temperature": 0.0, "avg_logprob": -0.11533192225864955, "compression_ratio": 1.7251184834123223, "no_speech_prob": 1.7061776134141837e-06}, {"id": 752, "seek": 482132, "start": 4841.04, "end": 4847.92, "text": " any metrics and that's because mean squared error is already a good metric like it's not", "tokens": [604, 16367, 293, 300, 311, 570, 914, 8889, 6713, 307, 1217, 257, 665, 20678, 411, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.11533192225864955, "compression_ratio": 1.7251184834123223, "no_speech_prob": 1.7061776134141837e-06}, {"id": 753, "seek": 484792, "start": 4847.92, "end": 4853.36, "text": " it's it's it's it has nice gradients it behaves well but and it's also the thing that we care", "tokens": [309, 311, 309, 311, 309, 311, 309, 575, 1481, 2771, 2448, 309, 36896, 731, 457, 293, 309, 311, 611, 264, 551, 300, 321, 1127], "temperature": 0.0, "avg_logprob": -0.1278727626800537, "compression_ratio": 1.6289592760180995, "no_speech_prob": 4.4508453811431536e-07}, {"id": 754, "seek": 484792, "start": 4853.36, "end": 4861.36, "text": " about so we don't need a separate metric to track. So let's go ahead and use LR find and", "tokens": [466, 370, 321, 500, 380, 643, 257, 4994, 20678, 281, 2837, 13, 407, 718, 311, 352, 2286, 293, 764, 441, 49, 915, 293], "temperature": 0.0, "avg_logprob": -0.1278727626800537, "compression_ratio": 1.6289592760180995, "no_speech_prob": 4.4508453811431536e-07}, {"id": 755, "seek": 484792, "start": 4861.36, "end": 4868.64, "text": " we can pick our learning rate so maybe about 10 to the minus 2 we can call fine tune and", "tokens": [321, 393, 1888, 527, 2539, 3314, 370, 1310, 466, 1266, 281, 264, 3175, 568, 321, 393, 818, 2489, 10864, 293], "temperature": 0.0, "avg_logprob": -0.1278727626800537, "compression_ratio": 1.6289592760180995, "no_speech_prob": 4.4508453811431536e-07}, {"id": 756, "seek": 484792, "start": 4868.64, "end": 4873.84, "text": " we get a valid loss of 0.0001 and so that's the mean squared error so we should take the", "tokens": [321, 483, 257, 7363, 4470, 295, 1958, 13, 1360, 16, 293, 370, 300, 311, 264, 914, 8889, 6713, 370, 321, 820, 747, 264], "temperature": 0.0, "avg_logprob": -0.1278727626800537, "compression_ratio": 1.6289592760180995, "no_speech_prob": 4.4508453811431536e-07}, {"id": 757, "seek": 487384, "start": 4873.84, "end": 4879.96, "text": " square root on average we're about 0.01 off in a coordinate space that goes between minus", "tokens": [3732, 5593, 322, 4274, 321, 434, 466, 1958, 13, 10607, 766, 294, 257, 15670, 1901, 300, 1709, 1296, 3175], "temperature": 0.0, "avg_logprob": -0.17694849014282227, "compression_ratio": 1.7265625, "no_speech_prob": 1.4367481071531074e-06}, {"id": 758, "seek": 487384, "start": 4879.96, "end": 4886.88, "text": " 1 and 1 so that sounds super accurate took about three in a bit minutes to run so we", "tokens": [502, 293, 502, 370, 300, 3263, 1687, 8559, 1890, 466, 1045, 294, 257, 857, 2077, 281, 1190, 370, 321], "temperature": 0.0, "avg_logprob": -0.17694849014282227, "compression_ratio": 1.7265625, "no_speech_prob": 1.4367481071531074e-06}, {"id": 759, "seek": 487384, "start": 4886.88, "end": 4892.32, "text": " can always call in fast AI and we always should their results see what our results look like", "tokens": [393, 1009, 818, 294, 2370, 7318, 293, 321, 1009, 820, 641, 3542, 536, 437, 527, 3542, 574, 411], "temperature": 0.0, "avg_logprob": -0.17694849014282227, "compression_ratio": 1.7265625, "no_speech_prob": 1.4367481071531074e-06}, {"id": 760, "seek": 487384, "start": 4892.32, "end": 4897.360000000001, "text": " and as you can see fast AI has automatically figured out how to display the combination", "tokens": [293, 382, 291, 393, 536, 2370, 7318, 575, 6772, 8932, 484, 577, 281, 4674, 264, 6562], "temperature": 0.0, "avg_logprob": -0.17694849014282227, "compression_ratio": 1.7265625, "no_speech_prob": 1.4367481071531074e-06}, {"id": 761, "seek": 487384, "start": 4897.360000000001, "end": 4903.16, "text": " of an image independent variable and a point dependent variable on the left is that is", "tokens": [295, 364, 3256, 6695, 7006, 293, 257, 935, 12334, 7006, 322, 264, 1411, 307, 300, 307], "temperature": 0.0, "avg_logprob": -0.17694849014282227, "compression_ratio": 1.7265625, "no_speech_prob": 1.4367481071531074e-06}, {"id": 762, "seek": 490316, "start": 4903.16, "end": 4909.84, "text": " the target and on the right is the prediction and as you can see it is pretty close to perfect.", "tokens": [264, 3779, 293, 322, 264, 558, 307, 264, 17630, 293, 382, 291, 393, 536, 309, 307, 1238, 1998, 281, 2176, 13], "temperature": 0.0, "avg_logprob": -0.1486950261252267, "compression_ratio": 1.7488151658767772, "no_speech_prob": 1.2878922461823095e-06}, {"id": 763, "seek": 490316, "start": 4909.84, "end": 4916.8, "text": " You know one of the really interesting things here is we used fine tune even although think", "tokens": [509, 458, 472, 295, 264, 534, 1880, 721, 510, 307, 321, 1143, 2489, 10864, 754, 4878, 519], "temperature": 0.0, "avg_logprob": -0.1486950261252267, "compression_ratio": 1.7488151658767772, "no_speech_prob": 1.2878922461823095e-06}, {"id": 764, "seek": 490316, "start": 4916.8, "end": 4922.599999999999, "text": " about it the thing we're fine-tuning image net isn't even an image regression model so", "tokens": [466, 309, 264, 551, 321, 434, 2489, 12, 83, 37726, 3256, 2533, 1943, 380, 754, 364, 3256, 24590, 2316, 370], "temperature": 0.0, "avg_logprob": -0.1486950261252267, "compression_ratio": 1.7488151658767772, "no_speech_prob": 1.2878922461823095e-06}, {"id": 765, "seek": 490316, "start": 4922.599999999999, "end": 4929.82, "text": " we're actually fine-tuning an image classification model to become something totally different", "tokens": [321, 434, 767, 2489, 12, 83, 37726, 364, 3256, 21538, 2316, 281, 1813, 746, 3879, 819], "temperature": 0.0, "avg_logprob": -0.1486950261252267, "compression_ratio": 1.7488151658767772, "no_speech_prob": 1.2878922461823095e-06}, {"id": 766, "seek": 492982, "start": 4929.82, "end": 4941.259999999999, "text": " an image regression model why does that work so well well because an image net classification", "tokens": [364, 3256, 24590, 2316, 983, 775, 300, 589, 370, 731, 731, 570, 364, 3256, 2533, 21538], "temperature": 0.0, "avg_logprob": -0.08656354122851269, "compression_ratio": 1.7303921568627452, "no_speech_prob": 6.276691237872001e-07}, {"id": 767, "seek": 492982, "start": 4941.259999999999, "end": 4947.4, "text": " model must have learned a lot about kind of how images look what things look like and", "tokens": [2316, 1633, 362, 3264, 257, 688, 466, 733, 295, 577, 5267, 574, 437, 721, 574, 411, 293], "temperature": 0.0, "avg_logprob": -0.08656354122851269, "compression_ratio": 1.7303921568627452, "no_speech_prob": 6.276691237872001e-07}, {"id": 768, "seek": 492982, "start": 4947.4, "end": 4952.28, "text": " where the pieces of them are they kind of know how to figure out what breed of animal", "tokens": [689, 264, 3755, 295, 552, 366, 436, 733, 295, 458, 577, 281, 2573, 484, 437, 18971, 295, 5496], "temperature": 0.0, "avg_logprob": -0.08656354122851269, "compression_ratio": 1.7303921568627452, "no_speech_prob": 6.276691237872001e-07}, {"id": 769, "seek": 492982, "start": 4952.28, "end": 4957.7, "text": " something is even if it's partly obscured by a bush or it's in the shade or it's turned", "tokens": [746, 307, 754, 498, 309, 311, 17031, 22082, 3831, 538, 257, 19910, 420, 309, 311, 294, 264, 11466, 420, 309, 311, 3574], "temperature": 0.0, "avg_logprob": -0.08656354122851269, "compression_ratio": 1.7303921568627452, "no_speech_prob": 6.276691237872001e-07}, {"id": 770, "seek": 495770, "start": 4957.7, "end": 4965.4, "text": " in different angles you know these are pre-trained image models are incredibly powerful computers", "tokens": [294, 819, 14708, 291, 458, 613, 366, 659, 12, 17227, 2001, 3256, 5245, 366, 6252, 4005, 10807], "temperature": 0.0, "avg_logprob": -0.10175183378619912, "compression_ratio": 1.7116279069767442, "no_speech_prob": 1.0188060741711524e-06}, {"id": 771, "seek": 495770, "start": 4965.4, "end": 4972.36, "text": " you know computing algorithms so built into every image net pre-trained model is all this", "tokens": [291, 458, 15866, 14642, 370, 3094, 666, 633, 3256, 2533, 659, 12, 17227, 2001, 2316, 307, 439, 341], "temperature": 0.0, "avg_logprob": -0.10175183378619912, "compression_ratio": 1.7116279069767442, "no_speech_prob": 1.0188060741711524e-06}, {"id": 772, "seek": 495770, "start": 4972.36, "end": 4978.12, "text": " capability that it had to learn for itself so asking it to use that capability to figure", "tokens": [13759, 300, 309, 632, 281, 1466, 337, 2564, 370, 3365, 309, 281, 764, 300, 13759, 281, 2573], "temperature": 0.0, "avg_logprob": -0.10175183378619912, "compression_ratio": 1.7116279069767442, "no_speech_prob": 1.0188060741711524e-06}, {"id": 773, "seek": 495770, "start": 4978.12, "end": 4983.96, "text": " out where something is just actually not that hard for it and so that's why we can actually", "tokens": [484, 689, 746, 307, 445, 767, 406, 300, 1152, 337, 309, 293, 370, 300, 311, 983, 321, 393, 767], "temperature": 0.0, "avg_logprob": -0.10175183378619912, "compression_ratio": 1.7116279069767442, "no_speech_prob": 1.0188060741711524e-06}, {"id": 774, "seek": 498396, "start": 4983.96, "end": 4990.36, "text": " fine-tune an image net classification model to create something completely different which", "tokens": [2489, 12, 83, 2613, 364, 3256, 2533, 21538, 2316, 281, 1884, 746, 2584, 819, 597], "temperature": 0.0, "avg_logprob": -0.09339746913394413, "compression_ratio": 1.5739910313901346, "no_speech_prob": 3.8070083974162117e-07}, {"id": 775, "seek": 498396, "start": 4990.36, "end": 5003.72, "text": " is a point image regression model so I find that incredibly cool I gotta say so again", "tokens": [307, 257, 935, 3256, 24590, 2316, 370, 286, 915, 300, 6252, 1627, 286, 3428, 584, 370, 797], "temperature": 0.0, "avg_logprob": -0.09339746913394413, "compression_ratio": 1.5739910313901346, "no_speech_prob": 3.8070083974162117e-07}, {"id": 776, "seek": 498396, "start": 5003.72, "end": 5007.84, "text": " look at the further research after you've done the questionnaire and particularly if", "tokens": [574, 412, 264, 3052, 2132, 934, 291, 600, 1096, 264, 44702, 293, 4098, 498], "temperature": 0.0, "avg_logprob": -0.09339746913394413, "compression_ratio": 1.5739910313901346, "no_speech_prob": 3.8070083974162117e-07}, {"id": 777, "seek": 498396, "start": 5007.84, "end": 5011.04, "text": " you haven't used data frames before please play with them because we're going to be using", "tokens": [291, 2378, 380, 1143, 1412, 12083, 949, 1767, 862, 365, 552, 570, 321, 434, 516, 281, 312, 1228], "temperature": 0.0, "avg_logprob": -0.09339746913394413, "compression_ratio": 1.5739910313901346, "no_speech_prob": 3.8070083974162117e-07}, {"id": 778, "seek": 501104, "start": 5011.04, "end": 5020.48, "text": " them more and more. Good question. I'll just do the last one and also go back and look", "tokens": [552, 544, 293, 544, 13, 2205, 1168, 13, 286, 603, 445, 360, 264, 1036, 472, 293, 611, 352, 646, 293, 574], "temperature": 0.0, "avg_logprob": -0.12201002167492378, "compression_ratio": 1.6540284360189574, "no_speech_prob": 1.1726360753527842e-06}, {"id": 779, "seek": 501104, "start": 5020.48, "end": 5027.32, "text": " at the bear classifier from notebook 2 or whatever hopefully you created some other", "tokens": [412, 264, 6155, 1508, 9902, 490, 21060, 568, 420, 2035, 4696, 291, 2942, 512, 661], "temperature": 0.0, "avg_logprob": -0.12201002167492378, "compression_ratio": 1.6540284360189574, "no_speech_prob": 1.1726360753527842e-06}, {"id": 780, "seek": 501104, "start": 5027.32, "end": 5032.6, "text": " classifier for your own data because remember we talked about how it would be better if", "tokens": [1508, 9902, 337, 428, 1065, 1412, 570, 1604, 321, 2825, 466, 577, 309, 576, 312, 1101, 498], "temperature": 0.0, "avg_logprob": -0.12201002167492378, "compression_ratio": 1.6540284360189574, "no_speech_prob": 1.1726360753527842e-06}, {"id": 781, "seek": 501104, "start": 5032.6, "end": 5038.94, "text": " the bear classifier could also recognize that there's no bear at all or maybe there's both", "tokens": [264, 6155, 1508, 9902, 727, 611, 5521, 300, 456, 311, 572, 6155, 412, 439, 420, 1310, 456, 311, 1293], "temperature": 0.0, "avg_logprob": -0.12201002167492378, "compression_ratio": 1.6540284360189574, "no_speech_prob": 1.1726360753527842e-06}, {"id": 782, "seek": 503894, "start": 5038.94, "end": 5044.4, "text": " a grizzly bear and a black bear or a grizzly bear and a teddy bear so if you retrain it", "tokens": [257, 17865, 4313, 356, 6155, 293, 257, 2211, 6155, 420, 257, 17865, 4313, 356, 6155, 293, 257, 45116, 6155, 370, 498, 291, 1533, 7146, 309], "temperature": 0.0, "avg_logprob": -0.1092011384796678, "compression_ratio": 1.7380952380952381, "no_speech_prob": 4.637837719201343e-06}, {"id": 783, "seek": 503894, "start": 5044.4, "end": 5048.74, "text": " using multi-label classification see what happens see how well it works when there's", "tokens": [1228, 4825, 12, 75, 18657, 21538, 536, 437, 2314, 536, 577, 731, 309, 1985, 562, 456, 311], "temperature": 0.0, "avg_logprob": -0.1092011384796678, "compression_ratio": 1.7380952380952381, "no_speech_prob": 4.637837719201343e-06}, {"id": 784, "seek": 503894, "start": 5048.74, "end": 5056.48, "text": " no bears and see whether it changes the accuracy of the single label model when you turn it", "tokens": [572, 17276, 293, 536, 1968, 309, 2962, 264, 14170, 295, 264, 2167, 7645, 2316, 562, 291, 1261, 309], "temperature": 0.0, "avg_logprob": -0.1092011384796678, "compression_ratio": 1.7380952380952381, "no_speech_prob": 4.637837719201343e-06}, {"id": 785, "seek": 503894, "start": 5056.48, "end": 5061.719999999999, "text": " into a multi-label problem so have a fiddle around and tell us on the forum what you find", "tokens": [666, 257, 4825, 12, 75, 18657, 1154, 370, 362, 257, 24553, 2285, 926, 293, 980, 505, 322, 264, 17542, 437, 291, 915], "temperature": 0.0, "avg_logprob": -0.1092011384796678, "compression_ratio": 1.7380952380952381, "no_speech_prob": 4.637837719201343e-06}, {"id": 786, "seek": 503894, "start": 5061.719999999999, "end": 5066.5199999999995, "text": " and we've got a question Rachel. Is there a tutorial showing how to use pre-trained", "tokens": [293, 321, 600, 658, 257, 1168, 14246, 13, 1119, 456, 257, 7073, 4099, 577, 281, 764, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.1092011384796678, "compression_ratio": 1.7380952380952381, "no_speech_prob": 4.637837719201343e-06}, {"id": 787, "seek": 506652, "start": 5066.52, "end": 5073.040000000001, "text": " models on four-channel images also how can you add a channel to a normal image?", "tokens": [5245, 322, 1451, 12, 339, 11444, 5267, 611, 577, 393, 291, 909, 257, 2269, 281, 257, 2710, 3256, 30], "temperature": 0.0, "avg_logprob": -0.24334992302788627, "compression_ratio": 1.8478260869565217, "no_speech_prob": 2.1444380763568915e-05}, {"id": 788, "seek": 506652, "start": 5073.040000000001, "end": 5080.280000000001, "text": " What's the last bit how do you add a channel to an image? I don't know what that means", "tokens": [708, 311, 264, 1036, 857, 577, 360, 291, 909, 257, 2269, 281, 364, 3256, 30, 286, 500, 380, 458, 437, 300, 1355], "temperature": 0.0, "avg_logprob": -0.24334992302788627, "compression_ratio": 1.8478260869565217, "no_speech_prob": 2.1444380763568915e-05}, {"id": 789, "seek": 506652, "start": 5080.280000000001, "end": 5087.52, "text": " okay like I don't know you can't like an image is an image you can't add a channel to an", "tokens": [1392, 411, 286, 500, 380, 458, 291, 393, 380, 411, 364, 3256, 307, 364, 3256, 291, 393, 380, 909, 257, 2269, 281, 364], "temperature": 0.0, "avg_logprob": -0.24334992302788627, "compression_ratio": 1.8478260869565217, "no_speech_prob": 2.1444380763568915e-05}, {"id": 790, "seek": 508752, "start": 5087.52, "end": 5098.72, "text": " image is what it is. I don't know if there's a tutorial but we can certainly make sure", "tokens": [3256, 307, 437, 309, 307, 13, 286, 500, 380, 458, 498, 456, 311, 257, 7073, 457, 321, 393, 3297, 652, 988], "temperature": 0.0, "avg_logprob": -0.18567570396091626, "compression_ratio": 1.3643410852713178, "no_speech_prob": 2.0904428765788907e-06}, {"id": 791, "seek": 508752, "start": 5098.72, "end": 5105.52, "text": " somebody on the forum has known how to do it it's um it's super straightforward it should", "tokens": [2618, 322, 264, 17542, 575, 2570, 577, 281, 360, 309, 309, 311, 1105, 309, 311, 1687, 15325, 309, 820], "temperature": 0.0, "avg_logprob": -0.18567570396091626, "compression_ratio": 1.3643410852713178, "no_speech_prob": 2.0904428765788907e-06}, {"id": 792, "seek": 510552, "start": 5105.52, "end": 5120.6, "text": " be pretty much automatic. Okay we're going to talk about collaborative filtering. What", "tokens": [312, 1238, 709, 12509, 13, 1033, 321, 434, 516, 281, 751, 466, 16555, 30822, 13, 708], "temperature": 0.0, "avg_logprob": -0.12729108655774915, "compression_ratio": 1.416, "no_speech_prob": 1.7983003886001825e-07}, {"id": 793, "seek": 510552, "start": 5120.6, "end": 5128.4400000000005, "text": " is collaborative filtering? Well think about on Netflix or whatever you might have watched", "tokens": [307, 16555, 30822, 30, 1042, 519, 466, 322, 12778, 420, 2035, 291, 1062, 362, 6337], "temperature": 0.0, "avg_logprob": -0.12729108655774915, "compression_ratio": 1.416, "no_speech_prob": 1.7983003886001825e-07}, {"id": 794, "seek": 512844, "start": 5128.44, "end": 5136.879999999999, "text": " a lot of movies that are sci-fi and have a lot of action and were made in the 70s and", "tokens": [257, 688, 295, 6233, 300, 366, 2180, 12, 13325, 293, 362, 257, 688, 295, 3069, 293, 645, 1027, 294, 264, 5285, 82, 293], "temperature": 0.0, "avg_logprob": -0.11172369218641712, "compression_ratio": 1.5476190476190477, "no_speech_prob": 1.392538138134114e-06}, {"id": 795, "seek": 512844, "start": 5136.879999999999, "end": 5142.599999999999, "text": " Netflix might not know anything about the properties of movies you watched it might", "tokens": [12778, 1062, 406, 458, 1340, 466, 264, 7221, 295, 6233, 291, 6337, 309, 1062], "temperature": 0.0, "avg_logprob": -0.11172369218641712, "compression_ratio": 1.5476190476190477, "no_speech_prob": 1.392538138134114e-06}, {"id": 796, "seek": 512844, "start": 5142.599999999999, "end": 5149.96, "text": " just know that they're movies with titles and IDs but what it could absolutely see without", "tokens": [445, 458, 300, 436, 434, 6233, 365, 12992, 293, 48212, 457, 437, 309, 727, 3122, 536, 1553], "temperature": 0.0, "avg_logprob": -0.11172369218641712, "compression_ratio": 1.5476190476190477, "no_speech_prob": 1.392538138134114e-06}, {"id": 797, "seek": 514996, "start": 5149.96, "end": 5159.44, "text": " any manual work is find other people that watched the same movies that you watched and", "tokens": [604, 9688, 589, 307, 915, 661, 561, 300, 6337, 264, 912, 6233, 300, 291, 6337, 293], "temperature": 0.0, "avg_logprob": -0.07870864868164062, "compression_ratio": 1.8040201005025125, "no_speech_prob": 6.375533985192305e-07}, {"id": 798, "seek": 514996, "start": 5159.44, "end": 5164.84, "text": " it could see what other movies those people watched that you haven't and it would probably", "tokens": [309, 727, 536, 437, 661, 6233, 729, 561, 6337, 300, 291, 2378, 380, 293, 309, 576, 1391], "temperature": 0.0, "avg_logprob": -0.07870864868164062, "compression_ratio": 1.8040201005025125, "no_speech_prob": 6.375533985192305e-07}, {"id": 799, "seek": 514996, "start": 5164.84, "end": 5169.12, "text": " find they were also you would probably find they're also science fiction and full of action", "tokens": [915, 436, 645, 611, 291, 576, 1391, 915, 436, 434, 611, 3497, 13266, 293, 1577, 295, 3069], "temperature": 0.0, "avg_logprob": -0.07870864868164062, "compression_ratio": 1.8040201005025125, "no_speech_prob": 6.375533985192305e-07}, {"id": 800, "seek": 514996, "start": 5169.12, "end": 5179.8, "text": " and made in the 70s. So we can use an approach where we recommend things even if we don't", "tokens": [293, 1027, 294, 264, 5285, 82, 13, 407, 321, 393, 764, 364, 3109, 689, 321, 2748, 721, 754, 498, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.07870864868164062, "compression_ratio": 1.8040201005025125, "no_speech_prob": 6.375533985192305e-07}, {"id": 801, "seek": 517980, "start": 5179.8, "end": 5189.6, "text": " know anything about what those things are as long as we know who else has used or recommended", "tokens": [458, 1340, 466, 437, 729, 721, 366, 382, 938, 382, 321, 458, 567, 1646, 575, 1143, 420, 9628], "temperature": 0.0, "avg_logprob": -0.11462336701232118, "compression_ratio": 1.7184466019417475, "no_speech_prob": 3.2887296583794523e-06}, {"id": 802, "seek": 517980, "start": 5189.6, "end": 5193.4400000000005, "text": " things that are similar you know the same kind you know many of the same things that", "tokens": [721, 300, 366, 2531, 291, 458, 264, 912, 733, 291, 458, 867, 295, 264, 912, 721, 300], "temperature": 0.0, "avg_logprob": -0.11462336701232118, "compression_ratio": 1.7184466019417475, "no_speech_prob": 3.2887296583794523e-06}, {"id": 803, "seek": 517980, "start": 5193.4400000000005, "end": 5202.96, "text": " that you've liked or or used. This doesn't necessarily mean users and products in fact", "tokens": [300, 291, 600, 4501, 420, 420, 1143, 13, 639, 1177, 380, 4725, 914, 5022, 293, 3383, 294, 1186], "temperature": 0.0, "avg_logprob": -0.11462336701232118, "compression_ratio": 1.7184466019417475, "no_speech_prob": 3.2887296583794523e-06}, {"id": 804, "seek": 517980, "start": 5202.96, "end": 5207.400000000001, "text": " in collaborative filtering sort of saying products we normally say items and items could", "tokens": [294, 16555, 30822, 1333, 295, 1566, 3383, 321, 5646, 584, 4754, 293, 4754, 727], "temperature": 0.0, "avg_logprob": -0.11462336701232118, "compression_ratio": 1.7184466019417475, "no_speech_prob": 3.2887296583794523e-06}, {"id": 805, "seek": 520740, "start": 5207.4, "end": 5218.0, "text": " be links you click on diagnoses for a patient and so forth. So there's a key idea here which", "tokens": [312, 6123, 291, 2052, 322, 7234, 4201, 337, 257, 4537, 293, 370, 5220, 13, 407, 456, 311, 257, 2141, 1558, 510, 597], "temperature": 0.0, "avg_logprob": -0.10239606350660324, "compression_ratio": 1.6024096385542168, "no_speech_prob": 8.851541224430548e-07}, {"id": 806, "seek": 520740, "start": 5218.0, "end": 5223.5199999999995, "text": " is that in the underlying items and we're going to be using movies in this example there", "tokens": [307, 300, 294, 264, 14217, 4754, 293, 321, 434, 516, 281, 312, 1228, 6233, 294, 341, 1365, 456], "temperature": 0.0, "avg_logprob": -0.10239606350660324, "compression_ratio": 1.6024096385542168, "no_speech_prob": 8.851541224430548e-07}, {"id": 807, "seek": 520740, "start": 5223.5199999999995, "end": 5229.0, "text": " are some there are some features they may not be labeled but there's some underlying", "tokens": [366, 512, 456, 366, 512, 4122, 436, 815, 406, 312, 21335, 457, 456, 311, 512, 14217], "temperature": 0.0, "avg_logprob": -0.10239606350660324, "compression_ratio": 1.6024096385542168, "no_speech_prob": 8.851541224430548e-07}, {"id": 808, "seek": 522900, "start": 5229.0, "end": 5237.48, "text": " concept of features of of those movies like the fact that there's a action concept and", "tokens": [3410, 295, 4122, 295, 295, 729, 6233, 411, 264, 1186, 300, 456, 311, 257, 3069, 3410, 293], "temperature": 0.0, "avg_logprob": -0.11831974983215332, "compression_ratio": 1.755, "no_speech_prob": 3.555948069333681e-06}, {"id": 809, "seek": 522900, "start": 5237.48, "end": 5243.08, "text": " a sci-fi concept in the 1970s concept. Now you would never actually told Netflix you", "tokens": [257, 2180, 12, 13325, 3410, 294, 264, 14577, 82, 3410, 13, 823, 291, 576, 1128, 767, 1907, 12778, 291], "temperature": 0.0, "avg_logprob": -0.11831974983215332, "compression_ratio": 1.755, "no_speech_prob": 3.555948069333681e-06}, {"id": 810, "seek": 522900, "start": 5243.08, "end": 5247.28, "text": " like these kinds of movies and maybe Netflix never actually added columns to their movies", "tokens": [411, 613, 3685, 295, 6233, 293, 1310, 12778, 1128, 767, 3869, 13766, 281, 641, 6233], "temperature": 0.0, "avg_logprob": -0.11831974983215332, "compression_ratio": 1.755, "no_speech_prob": 3.555948069333681e-06}, {"id": 811, "seek": 522900, "start": 5247.28, "end": 5253.96, "text": " saying what movies are those types but as long as like you know in the real world there's", "tokens": [1566, 437, 6233, 366, 729, 3467, 457, 382, 938, 382, 411, 291, 458, 294, 264, 957, 1002, 456, 311], "temperature": 0.0, "avg_logprob": -0.11831974983215332, "compression_ratio": 1.755, "no_speech_prob": 3.555948069333681e-06}, {"id": 812, "seek": 525396, "start": 5253.96, "end": 5260.44, "text": " this concept of sci-fi and action and movie age and that those concepts are relevant for", "tokens": [341, 3410, 295, 2180, 12, 13325, 293, 3069, 293, 3169, 3205, 293, 300, 729, 10392, 366, 7340, 337], "temperature": 0.0, "avg_logprob": -0.06778203988377052, "compression_ratio": 1.7089201877934272, "no_speech_prob": 9.42242479595734e-07}, {"id": 813, "seek": 525396, "start": 5260.44, "end": 5268.28, "text": " at least some people's movie watching decisions. As long as this is true then we can actually", "tokens": [412, 1935, 512, 561, 311, 3169, 1976, 5327, 13, 1018, 938, 382, 341, 307, 2074, 550, 321, 393, 767], "temperature": 0.0, "avg_logprob": -0.06778203988377052, "compression_ratio": 1.7089201877934272, "no_speech_prob": 9.42242479595734e-07}, {"id": 814, "seek": 525396, "start": 5268.28, "end": 5275.88, "text": " uncover these they're called late latent factors these things that kind of decide what kind", "tokens": [21694, 613, 436, 434, 1219, 3469, 48994, 6771, 613, 721, 300, 733, 295, 4536, 437, 733], "temperature": 0.0, "avg_logprob": -0.06778203988377052, "compression_ratio": 1.7089201877934272, "no_speech_prob": 9.42242479595734e-07}, {"id": 815, "seek": 525396, "start": 5275.88, "end": 5280.92, "text": " of movies you want to watch and they're latent because nobody necessarily ever wrote them", "tokens": [295, 6233, 291, 528, 281, 1159, 293, 436, 434, 48994, 570, 5079, 4725, 1562, 4114, 552], "temperature": 0.0, "avg_logprob": -0.06778203988377052, "compression_ratio": 1.7089201877934272, "no_speech_prob": 9.42242479595734e-07}, {"id": 816, "seek": 528092, "start": 5280.92, "end": 5287.32, "text": " down or labeled them or communicated them in any way. So let me show you what this looks", "tokens": [760, 420, 21335, 552, 420, 34989, 552, 294, 604, 636, 13, 407, 718, 385, 855, 291, 437, 341, 1542], "temperature": 0.0, "avg_logprob": -0.10578372585239695, "compression_ratio": 1.652694610778443, "no_speech_prob": 1.018807438413205e-06}, {"id": 817, "seek": 528092, "start": 5287.32, "end": 5295.88, "text": " like. So there's a great data set we can use called Movie Lens which contains tens of millions", "tokens": [411, 13, 407, 456, 311, 257, 869, 1412, 992, 321, 393, 764, 1219, 28766, 441, 694, 597, 8306, 10688, 295, 6803], "temperature": 0.0, "avg_logprob": -0.10578372585239695, "compression_ratio": 1.652694610778443, "no_speech_prob": 1.018807438413205e-06}, {"id": 818, "seek": 528092, "start": 5295.88, "end": 5304.64, "text": " of movie rankings and so a movie ranking looks like this it has a user number a movie number", "tokens": [295, 3169, 36550, 293, 370, 257, 3169, 17833, 1542, 411, 341, 309, 575, 257, 4195, 1230, 257, 3169, 1230], "temperature": 0.0, "avg_logprob": -0.10578372585239695, "compression_ratio": 1.652694610778443, "no_speech_prob": 1.018807438413205e-06}, {"id": 819, "seek": 530464, "start": 5304.64, "end": 5314.160000000001, "text": " a rating and a time step. So we don't know anything about who user number 196 is I don't", "tokens": [257, 10990, 293, 257, 565, 1823, 13, 407, 321, 500, 380, 458, 1340, 466, 567, 4195, 1230, 7998, 307, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.20186840404163708, "compression_ratio": 1.576470588235294, "no_speech_prob": 1.0030098565039225e-06}, {"id": 820, "seek": 530464, "start": 5314.160000000001, "end": 5321.200000000001, "text": " know if that is Rachel or John Vowell or somebody else. I don't know what movie number 242 is", "tokens": [458, 498, 300, 307, 14246, 420, 2619, 691, 305, 898, 420, 2618, 1646, 13, 286, 500, 380, 458, 437, 3169, 1230, 4022, 17, 307], "temperature": 0.0, "avg_logprob": -0.20186840404163708, "compression_ratio": 1.576470588235294, "no_speech_prob": 1.0030098565039225e-06}, {"id": 821, "seek": 530464, "start": 5321.200000000001, "end": 5328.4400000000005, "text": " I don't know if that's Casablanca or Lord of the Rings or the Mask and then rating is", "tokens": [286, 500, 380, 458, 498, 300, 311, 16100, 455, 8658, 496, 420, 3257, 295, 264, 38543, 420, 264, 25414, 293, 550, 10990, 307], "temperature": 0.0, "avg_logprob": -0.20186840404163708, "compression_ratio": 1.576470588235294, "no_speech_prob": 1.0030098565039225e-06}, {"id": 822, "seek": 532844, "start": 5328.44, "end": 5335.799999999999, "text": " a number between I think it was one and five. A question. Sure. In traditional machine learning", "tokens": [257, 1230, 1296, 286, 519, 309, 390, 472, 293, 1732, 13, 316, 1168, 13, 4894, 13, 682, 5164, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.12041529487161075, "compression_ratio": 1.665158371040724, "no_speech_prob": 2.4824691990943393e-06}, {"id": 823, "seek": 532844, "start": 5335.799999999999, "end": 5342.44, "text": " we perform cross validations and k-fold training to check for variance and bias trade-off.", "tokens": [321, 2042, 3278, 7363, 763, 293, 350, 12, 18353, 3097, 281, 1520, 337, 21977, 293, 12577, 4923, 12, 4506, 13], "temperature": 0.0, "avg_logprob": -0.12041529487161075, "compression_ratio": 1.665158371040724, "no_speech_prob": 2.4824691990943393e-06}, {"id": 824, "seek": 532844, "start": 5342.44, "end": 5351.5599999999995, "text": " Is this common in training deep learning models as well? So cross validation is a technique", "tokens": [1119, 341, 2689, 294, 3097, 2452, 2539, 5245, 382, 731, 30, 407, 3278, 24071, 307, 257, 6532], "temperature": 0.0, "avg_logprob": -0.12041529487161075, "compression_ratio": 1.665158371040724, "no_speech_prob": 2.4824691990943393e-06}, {"id": 825, "seek": 532844, "start": 5351.5599999999995, "end": 5357.5599999999995, "text": " where you don't just split your data set into one training set and one validation set but", "tokens": [689, 291, 500, 380, 445, 7472, 428, 1412, 992, 666, 472, 3097, 992, 293, 472, 24071, 992, 457], "temperature": 0.0, "avg_logprob": -0.12041529487161075, "compression_ratio": 1.665158371040724, "no_speech_prob": 2.4824691990943393e-06}, {"id": 826, "seek": 535756, "start": 5357.56, "end": 5363.400000000001, "text": " you basically do it five or so times like five training sets and like five validation", "tokens": [291, 1936, 360, 309, 1732, 420, 370, 1413, 411, 1732, 3097, 6352, 293, 411, 1732, 24071], "temperature": 0.0, "avg_logprob": -0.10406558121306987, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.611956344684586e-06}, {"id": 827, "seek": 535756, "start": 5363.400000000001, "end": 5371.400000000001, "text": " sets representing different overlapping subsets. And basically this was this used to be done", "tokens": [6352, 13460, 819, 33535, 2090, 1385, 13, 400, 1936, 341, 390, 341, 1143, 281, 312, 1096], "temperature": 0.0, "avg_logprob": -0.10406558121306987, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.611956344684586e-06}, {"id": 828, "seek": 535756, "start": 5371.400000000001, "end": 5378.92, "text": " a lot because people often used to have not enough data get a good result and so this", "tokens": [257, 688, 570, 561, 2049, 1143, 281, 362, 406, 1547, 1412, 483, 257, 665, 1874, 293, 370, 341], "temperature": 0.0, "avg_logprob": -0.10406558121306987, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.611956344684586e-06}, {"id": 829, "seek": 535756, "start": 5378.92, "end": 5384.68, "text": " way rather than kind of having 20% that you would leave out each time you could just leave", "tokens": [636, 2831, 813, 733, 295, 1419, 945, 4, 300, 291, 576, 1856, 484, 1184, 565, 291, 727, 445, 1856], "temperature": 0.0, "avg_logprob": -0.10406558121306987, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.611956344684586e-06}, {"id": 830, "seek": 538468, "start": 5384.68, "end": 5391.8, "text": " out like 10% each time. Nowadays it's less common that we have so little data that we", "tokens": [484, 411, 1266, 4, 1184, 565, 13, 28908, 309, 311, 1570, 2689, 300, 321, 362, 370, 707, 1412, 300, 321], "temperature": 0.0, "avg_logprob": -0.10256992297226124, "compression_ratio": 1.6445497630331753, "no_speech_prob": 7.338191494454804e-07}, {"id": 831, "seek": 538468, "start": 5391.8, "end": 5397.6, "text": " need to worry about the complexity and extra time of lots of models. It's done on Kaggle", "tokens": [643, 281, 3292, 466, 264, 14024, 293, 2857, 565, 295, 3195, 295, 5245, 13, 467, 311, 1096, 322, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.10256992297226124, "compression_ratio": 1.6445497630331753, "no_speech_prob": 7.338191494454804e-07}, {"id": 832, "seek": 538468, "start": 5397.6, "end": 5406.08, "text": " a lot. It's on Kaggle every little fraction of percent matters but it's not it's not a", "tokens": [257, 688, 13, 467, 311, 322, 48751, 22631, 633, 707, 14135, 295, 3043, 7001, 457, 309, 311, 406, 309, 311, 406, 257], "temperature": 0.0, "avg_logprob": -0.10256992297226124, "compression_ratio": 1.6445497630331753, "no_speech_prob": 7.338191494454804e-07}, {"id": 833, "seek": 538468, "start": 5406.08, "end": 5411.16, "text": " deep learning thing or a machine learning thing or whatever it's just a you know lots", "tokens": [2452, 2539, 551, 420, 257, 3479, 2539, 551, 420, 2035, 309, 311, 445, 257, 291, 458, 3195], "temperature": 0.0, "avg_logprob": -0.10256992297226124, "compression_ratio": 1.6445497630331753, "no_speech_prob": 7.338191494454804e-07}, {"id": 834, "seek": 541116, "start": 5411.16, "end": 5417.16, "text": " of data or not very much data thing and do you care about the last decimal place of them", "tokens": [295, 1412, 420, 406, 588, 709, 1412, 551, 293, 360, 291, 1127, 466, 264, 1036, 26601, 1081, 295, 552], "temperature": 0.0, "avg_logprob": -0.11448088036962302, "compression_ratio": 1.6772727272727272, "no_speech_prob": 4.181165991212765e-07}, {"id": 835, "seek": 541116, "start": 5417.16, "end": 5422.2, "text": " or not. It's not something we're going to talk about certainly in this part of the course", "tokens": [420, 406, 13, 467, 311, 406, 746, 321, 434, 516, 281, 751, 466, 3297, 294, 341, 644, 295, 264, 1164], "temperature": 0.0, "avg_logprob": -0.11448088036962302, "compression_ratio": 1.6772727272727272, "no_speech_prob": 4.181165991212765e-07}, {"id": 836, "seek": 541116, "start": 5422.2, "end": 5429.96, "text": " if ever because it's not something that comes up in practice that often as being that important.", "tokens": [498, 1562, 570, 309, 311, 406, 746, 300, 1487, 493, 294, 3124, 300, 2049, 382, 885, 300, 1021, 13], "temperature": 0.0, "avg_logprob": -0.11448088036962302, "compression_ratio": 1.6772727272727272, "no_speech_prob": 4.181165991212765e-07}, {"id": 837, "seek": 541116, "start": 5429.96, "end": 5437.8, "text": " There are two more questions. What would be some good applications of collaborative filtering", "tokens": [821, 366, 732, 544, 1651, 13, 708, 576, 312, 512, 665, 5821, 295, 16555, 30822], "temperature": 0.0, "avg_logprob": -0.11448088036962302, "compression_ratio": 1.6772727272727272, "no_speech_prob": 4.181165991212765e-07}, {"id": 838, "seek": 543780, "start": 5437.8, "end": 5448.04, "text": " outside of recommender systems? Well I mean depends how you define recommender system.", "tokens": [2380, 295, 2748, 260, 3652, 30, 1042, 286, 914, 5946, 577, 291, 6964, 2748, 260, 1185, 13], "temperature": 0.0, "avg_logprob": -0.0893004810108858, "compression_ratio": 1.8601036269430051, "no_speech_prob": 5.093655545351794e-06}, {"id": 839, "seek": 543780, "start": 5448.04, "end": 5454.84, "text": " If you're trying to figure out what kind of other diagnoses might be applicable to a patient", "tokens": [759, 291, 434, 1382, 281, 2573, 484, 437, 733, 295, 661, 7234, 4201, 1062, 312, 21142, 281, 257, 4537], "temperature": 0.0, "avg_logprob": -0.0893004810108858, "compression_ratio": 1.8601036269430051, "no_speech_prob": 5.093655545351794e-06}, {"id": 840, "seek": 543780, "start": 5454.84, "end": 5460.12, "text": " I guess that's kind of a recommender system or you're trying to figure out where somebody", "tokens": [286, 2041, 300, 311, 733, 295, 257, 2748, 260, 1185, 420, 291, 434, 1382, 281, 2573, 484, 689, 2618], "temperature": 0.0, "avg_logprob": -0.0893004810108858, "compression_ratio": 1.8601036269430051, "no_speech_prob": 5.093655545351794e-06}, {"id": 841, "seek": 543780, "start": 5460.12, "end": 5467.16, "text": " is going to click next or whatever it's kind of a recommender system. But you know really", "tokens": [307, 516, 281, 2052, 958, 420, 2035, 309, 311, 733, 295, 257, 2748, 260, 1185, 13, 583, 291, 458, 534], "temperature": 0.0, "avg_logprob": -0.0893004810108858, "compression_ratio": 1.8601036269430051, "no_speech_prob": 5.093655545351794e-06}, {"id": 842, "seek": 546716, "start": 5467.16, "end": 5475.2, "text": " conceptually it's anything where you're trying to learn from past behavior where that behavior", "tokens": [3410, 671, 309, 311, 1340, 689, 291, 434, 1382, 281, 1466, 490, 1791, 5223, 689, 300, 5223], "temperature": 0.0, "avg_logprob": -0.1039327943181417, "compression_ratio": 1.6561085972850678, "no_speech_prob": 1.0030090606960584e-06}, {"id": 843, "seek": 546716, "start": 5475.2, "end": 5483.16, "text": " is kind of like a thing happened to an entity. What is an approach to training using video", "tokens": [307, 733, 295, 411, 257, 551, 2011, 281, 364, 13977, 13, 708, 307, 364, 3109, 281, 3097, 1228, 960], "temperature": 0.0, "avg_logprob": -0.1039327943181417, "compression_ratio": 1.6561085972850678, "no_speech_prob": 1.0030090606960584e-06}, {"id": 844, "seek": 546716, "start": 5483.16, "end": 5489.22, "text": " streams i.e. from drone footage instead of images. Would you need to break up the footage", "tokens": [15842, 741, 13, 68, 13, 490, 13852, 9556, 2602, 295, 5267, 13, 6068, 291, 643, 281, 1821, 493, 264, 9556], "temperature": 0.0, "avg_logprob": -0.1039327943181417, "compression_ratio": 1.6561085972850678, "no_speech_prob": 1.0030090606960584e-06}, {"id": 845, "seek": 546716, "start": 5489.22, "end": 5494.72, "text": " into image frames? In practice quite often you would because images just tend to be pretty", "tokens": [666, 3256, 12083, 30, 682, 3124, 1596, 2049, 291, 576, 570, 5267, 445, 3928, 281, 312, 1238], "temperature": 0.0, "avg_logprob": -0.1039327943181417, "compression_ratio": 1.6561085972850678, "no_speech_prob": 1.0030090606960584e-06}, {"id": 846, "seek": 549472, "start": 5494.72, "end": 5505.68, "text": " big. Sorry videos tend to be pretty big. There's a lot of so I mean theoretically the time", "tokens": [955, 13, 4919, 2145, 3928, 281, 312, 1238, 955, 13, 821, 311, 257, 688, 295, 370, 286, 914, 29400, 264, 565], "temperature": 0.0, "avg_logprob": -0.19760813641904004, "compression_ratio": 1.5465116279069768, "no_speech_prob": 2.561267365308595e-06}, {"id": 847, "seek": 549472, "start": 5505.68, "end": 5512.56, "text": " could be the the fourth channel. Yeah or a fifth channel so if it's a full color movie", "tokens": [727, 312, 264, 264, 6409, 2269, 13, 865, 420, 257, 9266, 2269, 370, 498, 309, 311, 257, 1577, 2017, 3169], "temperature": 0.0, "avg_logprob": -0.19760813641904004, "compression_ratio": 1.5465116279069768, "no_speech_prob": 2.561267365308595e-06}, {"id": 848, "seek": 549472, "start": 5512.56, "end": 5520.68, "text": " you can absolutely have well I guess fourth because you can have it would be a five rank", "tokens": [291, 393, 3122, 362, 731, 286, 2041, 6409, 570, 291, 393, 362, 309, 576, 312, 257, 1732, 6181], "temperature": 0.0, "avg_logprob": -0.19760813641904004, "compression_ratio": 1.5465116279069768, "no_speech_prob": 2.561267365308595e-06}, {"id": 849, "seek": 552068, "start": 5520.68, "end": 5533.16, "text": " five tensor being batch by time by color by row by column. But often that's too computationally", "tokens": [1732, 40863, 885, 15245, 538, 565, 538, 2017, 538, 5386, 538, 7738, 13, 583, 2049, 300, 311, 886, 24903, 379], "temperature": 0.0, "avg_logprob": -0.12187872376552848, "compression_ratio": 1.3893129770992367, "no_speech_prob": 4.5209358745523787e-07}, {"id": 850, "seek": 552068, "start": 5533.16, "end": 5545.56, "text": " and too memory intensive. Sometimes people just look at one frame at a time. Sometimes", "tokens": [293, 886, 4675, 18957, 13, 4803, 561, 445, 574, 412, 472, 3920, 412, 257, 565, 13, 4803], "temperature": 0.0, "avg_logprob": -0.12187872376552848, "compression_ratio": 1.3893129770992367, "no_speech_prob": 4.5209358745523787e-07}, {"id": 851, "seek": 554556, "start": 5545.56, "end": 5553.92, "text": " people use a few frames around kind of the key frame like three or five frames at a time", "tokens": [561, 764, 257, 1326, 12083, 926, 733, 295, 264, 2141, 3920, 411, 1045, 420, 1732, 12083, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.10196269727220722, "compression_ratio": 1.7091633466135459, "no_speech_prob": 1.4144713986752322e-06}, {"id": 852, "seek": 554556, "start": 5553.92, "end": 5557.04, "text": " and sometimes people use something called a recurrent neural network which we'll be", "tokens": [293, 2171, 561, 764, 746, 1219, 257, 18680, 1753, 18161, 3209, 597, 321, 603, 312], "temperature": 0.0, "avg_logprob": -0.10196269727220722, "compression_ratio": 1.7091633466135459, "no_speech_prob": 1.4144713986752322e-06}, {"id": 853, "seek": 554556, "start": 5557.04, "end": 5562.84, "text": " seeing in the next week or two treated as a sequence data. Yeah there's all kinds of", "tokens": [2577, 294, 264, 958, 1243, 420, 732, 8668, 382, 257, 8310, 1412, 13, 865, 456, 311, 439, 3685, 295], "temperature": 0.0, "avg_logprob": -0.10196269727220722, "compression_ratio": 1.7091633466135459, "no_speech_prob": 1.4144713986752322e-06}, {"id": 854, "seek": 554556, "start": 5562.84, "end": 5569.96, "text": " tricks you can do to try and work with that conceptually though there's no reason you", "tokens": [11733, 291, 393, 360, 281, 853, 293, 589, 365, 300, 3410, 671, 1673, 456, 311, 572, 1778, 291], "temperature": 0.0, "avg_logprob": -0.10196269727220722, "compression_ratio": 1.7091633466135459, "no_speech_prob": 1.4144713986752322e-06}, {"id": 855, "seek": 554556, "start": 5569.96, "end": 5574.4800000000005, "text": " can't just add an additional access to your tensors and everything to work. It's just", "tokens": [393, 380, 445, 909, 364, 4497, 2105, 281, 428, 10688, 830, 293, 1203, 281, 589, 13, 467, 311, 445], "temperature": 0.0, "avg_logprob": -0.10196269727220722, "compression_ratio": 1.7091633466135459, "no_speech_prob": 1.4144713986752322e-06}, {"id": 856, "seek": 557448, "start": 5574.48, "end": 5581.24, "text": " a practical issue around time and memory. And someone else noted that it's pretty fitting", "tokens": [257, 8496, 2734, 926, 565, 293, 4675, 13, 400, 1580, 1646, 12964, 300, 309, 311, 1238, 15669], "temperature": 0.0, "avg_logprob": -0.21205893429842862, "compression_ratio": 1.4886363636363635, "no_speech_prob": 4.356854788056808e-06}, {"id": 857, "seek": 557448, "start": 5581.24, "end": 5588.5599999999995, "text": " that you mentioned the movie The Mask. Yes it was not an accident. I guess I got masks", "tokens": [300, 291, 2835, 264, 3169, 440, 25414, 13, 1079, 309, 390, 406, 364, 6398, 13, 286, 2041, 286, 658, 11830], "temperature": 0.0, "avg_logprob": -0.21205893429842862, "compression_ratio": 1.4886363636363635, "no_speech_prob": 4.356854788056808e-06}, {"id": 858, "seek": 557448, "start": 5588.5599999999995, "end": 5596.16, "text": " on the brain. I'm not sure if we're allowed to like that movie anymore though. I kind", "tokens": [322, 264, 3567, 13, 286, 478, 406, 988, 498, 321, 434, 4350, 281, 411, 300, 3169, 3602, 1673, 13, 286, 733], "temperature": 0.0, "avg_logprob": -0.21205893429842862, "compression_ratio": 1.4886363636363635, "no_speech_prob": 4.356854788056808e-06}, {"id": 859, "seek": 559616, "start": 5596.16, "end": 5607.92, "text": " of liked it when it came out. I don't know what I think nowadays. It's wild. Okay so", "tokens": [295, 4501, 309, 562, 309, 1361, 484, 13, 286, 500, 380, 458, 437, 286, 519, 13434, 13, 467, 311, 4868, 13, 1033, 370], "temperature": 0.0, "avg_logprob": -0.1609765969070734, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.0845142242033035e-06}, {"id": 860, "seek": 559616, "start": 5607.92, "end": 5614.72, "text": " let's take a look. So we can untar data ml100k. So ml100k is a small subset of the full set.", "tokens": [718, 311, 747, 257, 574, 13, 407, 321, 393, 1701, 289, 1412, 23271, 6879, 74, 13, 407, 23271, 6879, 74, 307, 257, 1359, 25993, 295, 264, 1577, 992, 13], "temperature": 0.0, "avg_logprob": -0.1609765969070734, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.0845142242033035e-06}, {"id": 861, "seek": 559616, "start": 5614.72, "end": 5619.639999999999, "text": " There's another one that we can grab which is about the whole lot 25 million but 100k", "tokens": [821, 311, 1071, 472, 300, 321, 393, 4444, 597, 307, 466, 264, 1379, 688, 3552, 2459, 457, 2319, 74], "temperature": 0.0, "avg_logprob": -0.1609765969070734, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.0845142242033035e-06}, {"id": 862, "seek": 559616, "start": 5619.639999999999, "end": 5626.08, "text": " is good enough for messing around. So if you look at the readme you'll find the main table", "tokens": [307, 665, 1547, 337, 23258, 926, 13, 407, 498, 291, 574, 412, 264, 1401, 1398, 291, 603, 915, 264, 2135, 3199], "temperature": 0.0, "avg_logprob": -0.1609765969070734, "compression_ratio": 1.5526315789473684, "no_speech_prob": 1.0845142242033035e-06}, {"id": 863, "seek": 562608, "start": 5626.08, "end": 5631.84, "text": " is in a file called u.data. So let's open it up with read.csv again. This one is actually", "tokens": [307, 294, 257, 3991, 1219, 344, 13, 67, 3274, 13, 407, 718, 311, 1269, 309, 493, 365, 1401, 13, 14368, 85, 797, 13, 639, 472, 307, 767], "temperature": 0.0, "avg_logprob": -0.1587034131659836, "compression_ratio": 1.75, "no_speech_prob": 2.6425755095260683e-06}, {"id": 864, "seek": 562608, "start": 5631.84, "end": 5637.04, "text": " not comma separated values it's tab separated. Rather confusingly we still use csv and to", "tokens": [406, 22117, 12005, 4190, 309, 311, 4421, 12005, 13, 16571, 13181, 356, 321, 920, 764, 28277, 85, 293, 281], "temperature": 0.0, "avg_logprob": -0.1587034131659836, "compression_ratio": 1.75, "no_speech_prob": 2.6425755095260683e-06}, {"id": 865, "seek": 562608, "start": 5637.04, "end": 5644.12, "text": " say delimiter is a tab. Backslash t is tab. There's no row at the top saying what the", "tokens": [584, 1103, 332, 1681, 307, 257, 4421, 13, 5833, 10418, 1299, 256, 307, 4421, 13, 821, 311, 572, 5386, 412, 264, 1192, 1566, 437, 264], "temperature": 0.0, "avg_logprob": -0.1587034131659836, "compression_ratio": 1.75, "no_speech_prob": 2.6425755095260683e-06}, {"id": 866, "seek": 562608, "start": 5644.12, "end": 5649.12, "text": " columns are called. So we say header is none and then pass in a list of what the columns", "tokens": [13766, 366, 1219, 13, 407, 321, 584, 23117, 307, 6022, 293, 550, 1320, 294, 257, 1329, 295, 437, 264, 13766], "temperature": 0.0, "avg_logprob": -0.1587034131659836, "compression_ratio": 1.75, "no_speech_prob": 2.6425755095260683e-06}, {"id": 867, "seek": 562608, "start": 5649.12, "end": 5654.5599999999995, "text": " are called..head will give us the first five rows and we mentioned this before what it", "tokens": [366, 1219, 13, 2411, 1934, 486, 976, 505, 264, 700, 1732, 13241, 293, 321, 2835, 341, 949, 437, 309], "temperature": 0.0, "avg_logprob": -0.1587034131659836, "compression_ratio": 1.75, "no_speech_prob": 2.6425755095260683e-06}, {"id": 868, "seek": 565456, "start": 5654.56, "end": 5663.84, "text": " looks like. It's not a particularly friendly way to look at it. So what I'm going to do", "tokens": [1542, 411, 13, 467, 311, 406, 257, 4098, 9208, 636, 281, 574, 412, 309, 13, 407, 437, 286, 478, 516, 281, 360], "temperature": 0.0, "avg_logprob": -0.21733335653940836, "compression_ratio": 1.5229885057471264, "no_speech_prob": 9.721520655148197e-07}, {"id": 869, "seek": 565456, "start": 5663.84, "end": 5672.8, "text": " is I'm going to cross tab it and so what I've done here is I've grabbed the top. I remember", "tokens": [307, 286, 478, 516, 281, 3278, 4421, 309, 293, 370, 437, 286, 600, 1096, 510, 307, 286, 600, 18607, 264, 1192, 13, 286, 1604], "temperature": 0.0, "avg_logprob": -0.21733335653940836, "compression_ratio": 1.5229885057471264, "no_speech_prob": 9.721520655148197e-07}, {"id": 870, "seek": 565456, "start": 5672.8, "end": 5680.64, "text": " how many it was well I guess one two three four fifteen or twenty movies based on the", "tokens": [577, 867, 309, 390, 731, 286, 2041, 472, 732, 1045, 1451, 18126, 420, 7699, 6233, 2361, 322, 264], "temperature": 0.0, "avg_logprob": -0.21733335653940836, "compression_ratio": 1.5229885057471264, "no_speech_prob": 9.721520655148197e-07}, {"id": 871, "seek": 568064, "start": 5680.64, "end": 5686.88, "text": " most popular movies and the top bunch of users who watched the most movies and so I've basically", "tokens": [881, 3743, 6233, 293, 264, 1192, 3840, 295, 5022, 567, 6337, 264, 881, 6233, 293, 370, 286, 600, 1936], "temperature": 0.0, "avg_logprob": -0.11425933383759998, "compression_ratio": 1.7475247524752475, "no_speech_prob": 1.6536832845304161e-06}, {"id": 872, "seek": 568064, "start": 5686.88, "end": 5692.76, "text": " kind of reoriented this. So for each user I have all the movies they've watched and", "tokens": [733, 295, 319, 27414, 341, 13, 407, 337, 1184, 4195, 286, 362, 439, 264, 6233, 436, 600, 6337, 293], "temperature": 0.0, "avg_logprob": -0.11425933383759998, "compression_ratio": 1.7475247524752475, "no_speech_prob": 1.6536832845304161e-06}, {"id": 873, "seek": 568064, "start": 5692.76, "end": 5698.96, "text": " the rating they gave them. So empty spots represent users that have not seen that movie.", "tokens": [264, 10990, 436, 2729, 552, 13, 407, 6707, 10681, 2906, 5022, 300, 362, 406, 1612, 300, 3169, 13], "temperature": 0.0, "avg_logprob": -0.11425933383759998, "compression_ratio": 1.7475247524752475, "no_speech_prob": 1.6536832845304161e-06}, {"id": 874, "seek": 568064, "start": 5698.96, "end": 5710.360000000001, "text": " So this is just another way of looking at this same data. So basically what we want", "tokens": [407, 341, 307, 445, 1071, 636, 295, 1237, 412, 341, 912, 1412, 13, 407, 1936, 437, 321, 528], "temperature": 0.0, "avg_logprob": -0.11425933383759998, "compression_ratio": 1.7475247524752475, "no_speech_prob": 1.6536832845304161e-06}, {"id": 875, "seek": 571036, "start": 5710.36, "end": 5717.12, "text": " to do is guess what movies we should tell people they might want to watch and so it's", "tokens": [281, 360, 307, 2041, 437, 6233, 321, 820, 980, 561, 436, 1062, 528, 281, 1159, 293, 370, 309, 311], "temperature": 0.0, "avg_logprob": -0.10395697080172026, "compression_ratio": 1.5, "no_speech_prob": 1.436747425032081e-06}, {"id": 876, "seek": 571036, "start": 5717.12, "end": 5722.719999999999, "text": " basically filling in these gaps to tell user 212 do you think we would they might like", "tokens": [1936, 10623, 294, 613, 15031, 281, 980, 4195, 568, 4762, 360, 291, 519, 321, 576, 436, 1062, 411], "temperature": 0.0, "avg_logprob": -0.10395697080172026, "compression_ratio": 1.5, "no_speech_prob": 1.436747425032081e-06}, {"id": 877, "seek": 571036, "start": 5722.719999999999, "end": 5739.92, "text": " movie 49 or 79 or 99 best to watch next. So let's assume that we actually had columns", "tokens": [3169, 16513, 420, 32803, 420, 11803, 1151, 281, 1159, 958, 13, 407, 718, 311, 6552, 300, 321, 767, 632, 13766], "temperature": 0.0, "avg_logprob": -0.10395697080172026, "compression_ratio": 1.5, "no_speech_prob": 1.436747425032081e-06}, {"id": 878, "seek": 573992, "start": 5739.92, "end": 5746.96, "text": " for every movie that represented say how much sci-fi they are how much action they are and", "tokens": [337, 633, 3169, 300, 10379, 584, 577, 709, 2180, 12, 13325, 436, 366, 577, 709, 3069, 436, 366, 293], "temperature": 0.0, "avg_logprob": -0.14889949560165405, "compression_ratio": 1.696774193548387, "no_speech_prob": 8.315264494740404e-07}, {"id": 879, "seek": 573992, "start": 5746.96, "end": 5752.32, "text": " how old they are and maybe they're between minus one and one and so like the lot the", "tokens": [577, 1331, 436, 366, 293, 1310, 436, 434, 1296, 3175, 472, 293, 472, 293, 370, 411, 264, 688, 264], "temperature": 0.0, "avg_logprob": -0.14889949560165405, "compression_ratio": 1.696774193548387, "no_speech_prob": 8.315264494740404e-07}, {"id": 880, "seek": 573992, "start": 5752.32, "end": 5761.4800000000005, "text": " last Skywalker is very sci-fi barely action and definitely not old and then we could do", "tokens": [1036, 49220, 307, 588, 2180, 12, 13325, 10268, 3069, 293, 2138, 406, 1331, 293, 550, 321, 727, 360], "temperature": 0.0, "avg_logprob": -0.14889949560165405, "compression_ratio": 1.696774193548387, "no_speech_prob": 8.315264494740404e-07}, {"id": 881, "seek": 576148, "start": 5761.48, "end": 5769.959999999999, "text": " the same thing for users so we could say user one really like sci-fi quite likes action", "tokens": [264, 912, 551, 337, 5022, 370, 321, 727, 584, 4195, 472, 534, 411, 2180, 12, 13325, 1596, 5902, 3069], "temperature": 0.0, "avg_logprob": -0.12439811363648832, "compression_ratio": 1.6726457399103138, "no_speech_prob": 4.1811830442384235e-07}, {"id": 882, "seek": 576148, "start": 5769.959999999999, "end": 5775.12, "text": " and really doesn't like old and so now if you multiply those together and remember in", "tokens": [293, 534, 1177, 380, 411, 1331, 293, 370, 586, 498, 291, 12972, 729, 1214, 293, 1604, 294], "temperature": 0.0, "avg_logprob": -0.12439811363648832, "compression_ratio": 1.6726457399103138, "no_speech_prob": 4.1811830442384235e-07}, {"id": 883, "seek": 576148, "start": 5775.12, "end": 5781.959999999999, "text": " PyTorch and NumPy you have element-wise calculations so this is going to multiply each corresponding", "tokens": [9953, 51, 284, 339, 293, 22592, 47, 88, 291, 362, 4478, 12, 3711, 20448, 370, 341, 307, 516, 281, 12972, 1184, 11760], "temperature": 0.0, "avg_logprob": -0.12439811363648832, "compression_ratio": 1.6726457399103138, "no_speech_prob": 4.1811830442384235e-07}, {"id": 884, "seek": 576148, "start": 5781.959999999999, "end": 5787.24, "text": " item. It's not matrix multiplication if you're a mathematician don't go there this is element-wise", "tokens": [3174, 13, 467, 311, 406, 8141, 27290, 498, 291, 434, 257, 48281, 500, 380, 352, 456, 341, 307, 4478, 12, 3711], "temperature": 0.0, "avg_logprob": -0.12439811363648832, "compression_ratio": 1.6726457399103138, "no_speech_prob": 4.1811830442384235e-07}, {"id": 885, "seek": 578724, "start": 5787.24, "end": 5794.5199999999995, "text": " multiplication if we want matrix multiplication it'd be an at sign. So if we multiply each", "tokens": [27290, 498, 321, 528, 8141, 27290, 309, 1116, 312, 364, 412, 1465, 13, 407, 498, 321, 12972, 1184], "temperature": 0.0, "avg_logprob": -0.12891963124275208, "compression_ratio": 1.742063492063492, "no_speech_prob": 2.0261384179320885e-06}, {"id": 886, "seek": 578724, "start": 5794.5199999999995, "end": 5799.679999999999, "text": " element together next to with the equivalent element in the other one and then sum them", "tokens": [4478, 1214, 958, 281, 365, 264, 10344, 4478, 294, 264, 661, 472, 293, 550, 2408, 552], "temperature": 0.0, "avg_logprob": -0.12891963124275208, "compression_ratio": 1.742063492063492, "no_speech_prob": 2.0261384179320885e-06}, {"id": 887, "seek": 578724, "start": 5799.679999999999, "end": 5804.639999999999, "text": " up that's going to give us a number which will basically tell us how much do these two", "tokens": [493, 300, 311, 516, 281, 976, 505, 257, 1230, 597, 486, 1936, 980, 505, 577, 709, 360, 613, 732], "temperature": 0.0, "avg_logprob": -0.12891963124275208, "compression_ratio": 1.742063492063492, "no_speech_prob": 2.0261384179320885e-06}, {"id": 888, "seek": 578724, "start": 5804.639999999999, "end": 5809.8, "text": " correspond because remember two negatives multiply together to get a positive so user", "tokens": [6805, 570, 1604, 732, 40019, 12972, 1214, 281, 483, 257, 3353, 370, 4195], "temperature": 0.0, "avg_logprob": -0.12891963124275208, "compression_ratio": 1.742063492063492, "no_speech_prob": 2.0261384179320885e-06}, {"id": 889, "seek": 578724, "start": 5809.8, "end": 5816.76, "text": " one likes exactly the kind of stuff that last guy was that the last Skywalker has in it", "tokens": [472, 5902, 2293, 264, 733, 295, 1507, 300, 1036, 2146, 390, 300, 264, 1036, 49220, 575, 294, 309], "temperature": 0.0, "avg_logprob": -0.12891963124275208, "compression_ratio": 1.742063492063492, "no_speech_prob": 2.0261384179320885e-06}, {"id": 890, "seek": 581676, "start": 5816.76, "end": 5822.72, "text": " and so we get 2.1. Multiplying things together element-wise and adding them up is called", "tokens": [293, 370, 321, 483, 568, 13, 16, 13, 31150, 7310, 721, 1214, 4478, 12, 3711, 293, 5127, 552, 493, 307, 1219], "temperature": 0.0, "avg_logprob": -0.1343639646257673, "compression_ratio": 1.5314285714285714, "no_speech_prob": 7.224433602459612e-07}, {"id": 891, "seek": 581676, "start": 5822.72, "end": 5828.76, "text": " the dot product and we use it a lot and it's the basis of matrix didn't say modification", "tokens": [264, 5893, 1674, 293, 321, 764, 309, 257, 688, 293, 309, 311, 264, 5143, 295, 8141, 994, 380, 584, 26747], "temperature": 0.0, "avg_logprob": -0.1343639646257673, "compression_ratio": 1.5314285714285714, "no_speech_prob": 7.224433602459612e-07}, {"id": 892, "seek": 581676, "start": 5828.76, "end": 5843.8, "text": " matrix multiplication. So make sure you know what a dot product is it's this so Casablanca", "tokens": [8141, 27290, 13, 407, 652, 988, 291, 458, 437, 257, 5893, 1674, 307, 309, 311, 341, 370, 16100, 455, 8658, 496], "temperature": 0.0, "avg_logprob": -0.1343639646257673, "compression_ratio": 1.5314285714285714, "no_speech_prob": 7.224433602459612e-07}, {"id": 893, "seek": 584380, "start": 5843.8, "end": 5852.08, "text": " is not at all sci-fi not much action and is certainly old so if we do user one times Casablanca", "tokens": [307, 406, 412, 439, 2180, 12, 13325, 406, 709, 3069, 293, 307, 3297, 1331, 370, 498, 321, 360, 4195, 472, 1413, 16100, 455, 8658, 496], "temperature": 0.0, "avg_logprob": -0.1033038596312205, "compression_ratio": 1.6757990867579908, "no_speech_prob": 9.132529044109106e-07}, {"id": 894, "seek": 584380, "start": 5852.08, "end": 5859.72, "text": " we get a negative number so we might think okay user one more like won't like this movie.", "tokens": [321, 483, 257, 3671, 1230, 370, 321, 1062, 519, 1392, 4195, 472, 544, 411, 1582, 380, 411, 341, 3169, 13], "temperature": 0.0, "avg_logprob": -0.1033038596312205, "compression_ratio": 1.6757990867579908, "no_speech_prob": 9.132529044109106e-07}, {"id": 895, "seek": 584380, "start": 5859.72, "end": 5864.88, "text": " Problem is we don't know what the latent factors are and even if we did we don't know how to", "tokens": [11676, 307, 321, 500, 380, 458, 437, 264, 48994, 6771, 366, 293, 754, 498, 321, 630, 321, 500, 380, 458, 577, 281], "temperature": 0.0, "avg_logprob": -0.1033038596312205, "compression_ratio": 1.6757990867579908, "no_speech_prob": 9.132529044109106e-07}, {"id": 896, "seek": 584380, "start": 5864.88, "end": 5873.4400000000005, "text": " label a particular user or a particular movie with them so we have to learn them. How do", "tokens": [7645, 257, 1729, 4195, 420, 257, 1729, 3169, 365, 552, 370, 321, 362, 281, 1466, 552, 13, 1012, 360], "temperature": 0.0, "avg_logprob": -0.1033038596312205, "compression_ratio": 1.6757990867579908, "no_speech_prob": 9.132529044109106e-07}, {"id": 897, "seek": 587344, "start": 5873.44, "end": 5886.96, "text": " we learn them? Well we can actually look at a spreadsheet so I've got a spreadsheet version", "tokens": [321, 1466, 552, 30, 1042, 321, 393, 767, 574, 412, 257, 27733, 370, 286, 600, 658, 257, 27733, 3037], "temperature": 0.0, "avg_logprob": -0.10788349878220331, "compression_ratio": 1.495798319327731, "no_speech_prob": 1.6536854445803328e-06}, {"id": 898, "seek": 587344, "start": 5886.96, "end": 5895.799999999999, "text": " so we have a spreadsheet version which is basically what I did was I popped this table", "tokens": [370, 321, 362, 257, 27733, 3037, 597, 307, 1936, 437, 286, 630, 390, 286, 21545, 341, 3199], "temperature": 0.0, "avg_logprob": -0.10788349878220331, "compression_ratio": 1.495798319327731, "no_speech_prob": 1.6536854445803328e-06}, {"id": 899, "seek": 589580, "start": 5895.8, "end": 5907.92, "text": " into Excel and then I randomly created a let's count this now 1 2 3 4 5 6 7 8 9 10 11 12", "tokens": [666, 19060, 293, 550, 286, 16979, 2942, 257, 718, 311, 1207, 341, 586, 502, 568, 805, 1017, 1025, 1386, 1614, 1649, 1722, 1266, 2975, 2272], "temperature": 0.0, "avg_logprob": -0.1417440226380254, "compression_ratio": 1.6375, "no_speech_prob": 9.721528613226837e-07}, {"id": 900, "seek": 589580, "start": 5907.92, "end": 5914.360000000001, "text": " 13 14. I randomly created a 15 by 5 table here so these are just random numbers and", "tokens": [3705, 3499, 13, 286, 16979, 2942, 257, 2119, 538, 1025, 3199, 510, 370, 613, 366, 445, 4974, 3547, 293], "temperature": 0.0, "avg_logprob": -0.1417440226380254, "compression_ratio": 1.6375, "no_speech_prob": 9.721528613226837e-07}, {"id": 901, "seek": 589580, "start": 5914.360000000001, "end": 5920.76, "text": " I randomly created a 5 by 15 table here and I basically said okay well let's just pretend", "tokens": [286, 16979, 2942, 257, 1025, 538, 2119, 3199, 510, 293, 286, 1936, 848, 1392, 731, 718, 311, 445, 11865], "temperature": 0.0, "avg_logprob": -0.1417440226380254, "compression_ratio": 1.6375, "no_speech_prob": 9.721528613226837e-07}, {"id": 902, "seek": 592076, "start": 5920.76, "end": 5926.320000000001, "text": " let's just assume that every movie and every user has five latent factors I don't know", "tokens": [718, 311, 445, 6552, 300, 633, 3169, 293, 633, 4195, 575, 1732, 48994, 6771, 286, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.10294345292178067, "compression_ratio": 1.82, "no_speech_prob": 1.2554198747238843e-07}, {"id": 903, "seek": 592076, "start": 5926.320000000001, "end": 5935.68, "text": " what they are and let's then do a matrix multiply of this set of factors by this set of factors", "tokens": [437, 436, 366, 293, 718, 311, 550, 360, 257, 8141, 12972, 295, 341, 992, 295, 6771, 538, 341, 992, 295, 6771], "temperature": 0.0, "avg_logprob": -0.10294345292178067, "compression_ratio": 1.82, "no_speech_prob": 1.2554198747238843e-07}, {"id": 904, "seek": 592076, "start": 5935.68, "end": 5942.780000000001, "text": " and a matrix multiply of a row by a column is identical to a dot product of two vectors", "tokens": [293, 257, 8141, 12972, 295, 257, 5386, 538, 257, 7738, 307, 14800, 281, 257, 5893, 1674, 295, 732, 18875], "temperature": 0.0, "avg_logprob": -0.10294345292178067, "compression_ratio": 1.82, "no_speech_prob": 1.2554198747238843e-07}, {"id": 905, "seek": 592076, "start": 5942.780000000001, "end": 5946.8, "text": " so that's why I could just use matrix multiply. So this is just what this first cell contains", "tokens": [370, 300, 311, 983, 286, 727, 445, 764, 8141, 12972, 13, 407, 341, 307, 445, 437, 341, 700, 2815, 8306], "temperature": 0.0, "avg_logprob": -0.10294345292178067, "compression_ratio": 1.82, "no_speech_prob": 1.2554198747238843e-07}, {"id": 906, "seek": 594680, "start": 5946.8, "end": 5953.28, "text": " so they then copied it to the whole thing so all these numbers there are being calculated", "tokens": [370, 436, 550, 25365, 309, 281, 264, 1379, 551, 370, 439, 613, 3547, 456, 366, 885, 15598], "temperature": 0.0, "avg_logprob": -0.13809242895094015, "compression_ratio": 1.6341463414634145, "no_speech_prob": 2.169170159049827e-07}, {"id": 907, "seek": 594680, "start": 5953.28, "end": 5963.04, "text": " from the row latent factors dot product with or matrix model play with a column latent", "tokens": [490, 264, 5386, 48994, 6771, 5893, 1674, 365, 420, 8141, 2316, 862, 365, 257, 7738, 48994], "temperature": 0.0, "avg_logprob": -0.13809242895094015, "compression_ratio": 1.6341463414634145, "no_speech_prob": 2.169170159049827e-07}, {"id": 908, "seek": 594680, "start": 5963.04, "end": 5969.68, "text": " factors so in other words I'm doing exactly this calculation but I'm doing them with random", "tokens": [6771, 370, 294, 661, 2283, 286, 478, 884, 2293, 341, 17108, 457, 286, 478, 884, 552, 365, 4974], "temperature": 0.0, "avg_logprob": -0.13809242895094015, "compression_ratio": 1.6341463414634145, "no_speech_prob": 2.169170159049827e-07}, {"id": 909, "seek": 596968, "start": 5969.68, "end": 5980.280000000001, "text": " numbers and so that gives us a whole bunch of values right and then what I could do is", "tokens": [3547, 293, 370, 300, 2709, 505, 257, 1379, 3840, 295, 4190, 558, 293, 550, 437, 286, 727, 360, 307], "temperature": 0.0, "avg_logprob": -0.06806680134364537, "compression_ratio": 1.8194444444444444, "no_speech_prob": 2.2732719173745863e-07}, {"id": 910, "seek": 596968, "start": 5980.280000000001, "end": 5988.56, "text": " I could calculate a loss by comparing every one of these numbers here to every one of", "tokens": [286, 727, 8873, 257, 4470, 538, 15763, 633, 472, 295, 613, 3547, 510, 281, 633, 472, 295], "temperature": 0.0, "avg_logprob": -0.06806680134364537, "compression_ratio": 1.8194444444444444, "no_speech_prob": 2.2732719173745863e-07}, {"id": 911, "seek": 596968, "start": 5988.56, "end": 5996.04, "text": " these numbers here and then I could do mean squared error and then I could use stochastic", "tokens": [613, 3547, 510, 293, 550, 286, 727, 360, 914, 8889, 6713, 293, 550, 286, 727, 764, 342, 8997, 2750], "temperature": 0.0, "avg_logprob": -0.06806680134364537, "compression_ratio": 1.8194444444444444, "no_speech_prob": 2.2732719173745863e-07}, {"id": 912, "seek": 599604, "start": 5996.04, "end": 6004.44, "text": " gradient descent to find the best set of numbers in each of these two locations and that is", "tokens": [16235, 23475, 281, 915, 264, 1151, 992, 295, 3547, 294, 1184, 295, 613, 732, 9253, 293, 300, 307], "temperature": 0.0, "avg_logprob": -0.13391809627927584, "compression_ratio": 1.5380116959064327, "no_speech_prob": 9.721534297568724e-07}, {"id": 913, "seek": 599604, "start": 6004.44, "end": 6013.28, "text": " what collaborative filtering is. So that's actually all we need so rather than doing", "tokens": [437, 16555, 30822, 307, 13, 407, 300, 311, 767, 439, 321, 643, 370, 2831, 813, 884], "temperature": 0.0, "avg_logprob": -0.13391809627927584, "compression_ratio": 1.5380116959064327, "no_speech_prob": 9.721534297568724e-07}, {"id": 914, "seek": 599604, "start": 6013.28, "end": 6018.76, "text": " an Excel and very the Excel version later if you're interested because we can actually", "tokens": [364, 19060, 293, 588, 264, 19060, 3037, 1780, 498, 291, 434, 3102, 570, 321, 393, 767], "temperature": 0.0, "avg_logprob": -0.13391809627927584, "compression_ratio": 1.5380116959064327, "no_speech_prob": 9.721534297568724e-07}, {"id": 915, "seek": 601876, "start": 6018.76, "end": 6027.6, "text": " do this whole thing and it works in Excel let's jump and do it into pytorch. Now one", "tokens": [360, 341, 1379, 551, 293, 309, 1985, 294, 19060, 718, 311, 3012, 293, 360, 309, 666, 25878, 284, 339, 13, 823, 472], "temperature": 0.0, "avg_logprob": -0.11373765733506945, "compression_ratio": 1.70935960591133, "no_speech_prob": 1.2878930419901735e-06}, {"id": 916, "seek": 601876, "start": 6027.6, "end": 6031.68, "text": " thing that might just make this more fun is actually to know what the movies are and movie", "tokens": [551, 300, 1062, 445, 652, 341, 544, 1019, 307, 767, 281, 458, 437, 264, 6233, 366, 293, 3169], "temperature": 0.0, "avg_logprob": -0.11373765733506945, "compression_ratio": 1.70935960591133, "no_speech_prob": 1.2878930419901735e-06}, {"id": 917, "seek": 601876, "start": 6031.68, "end": 6037.360000000001, "text": " lens tells us in u.item what the movies are called and that uses the telemetry of the", "tokens": [6765, 5112, 505, 294, 344, 13, 270, 443, 437, 264, 6233, 366, 1219, 293, 300, 4960, 264, 4304, 5537, 627, 295, 264], "temperature": 0.0, "avg_logprob": -0.11373765733506945, "compression_ratio": 1.70935960591133, "no_speech_prob": 1.2878930419901735e-06}, {"id": 918, "seek": 601876, "start": 6037.360000000001, "end": 6043.74, "text": " pipe sign weirdly enough so here are them here are the names of each movie and so one", "tokens": [11240, 1465, 48931, 1547, 370, 510, 366, 552, 510, 366, 264, 5288, 295, 1184, 3169, 293, 370, 472], "temperature": 0.0, "avg_logprob": -0.11373765733506945, "compression_ratio": 1.70935960591133, "no_speech_prob": 1.2878930419901735e-06}, {"id": 919, "seek": 604374, "start": 6043.74, "end": 6051.76, "text": " of the nice things about pandas is it can do joins just like SQL and so you can use", "tokens": [295, 264, 1481, 721, 466, 4565, 296, 307, 309, 393, 360, 24397, 445, 411, 19200, 293, 370, 291, 393, 764], "temperature": 0.0, "avg_logprob": -0.061535319616628247, "compression_ratio": 1.6956521739130435, "no_speech_prob": 4.1163573882840865e-07}, {"id": 920, "seek": 604374, "start": 6051.76, "end": 6057.2, "text": " the merge method to combine the ratings table and the movies table and since they both have", "tokens": [264, 22183, 3170, 281, 10432, 264, 24603, 3199, 293, 264, 6233, 3199, 293, 1670, 436, 1293, 362], "temperature": 0.0, "avg_logprob": -0.061535319616628247, "compression_ratio": 1.6956521739130435, "no_speech_prob": 4.1163573882840865e-07}, {"id": 921, "seek": 604374, "start": 6057.2, "end": 6063.5199999999995, "text": " a column called movie by default it will join on those and so now here we have the ratings", "tokens": [257, 7738, 1219, 3169, 538, 7576, 309, 486, 3917, 322, 729, 293, 370, 586, 510, 321, 362, 264, 24603], "temperature": 0.0, "avg_logprob": -0.061535319616628247, "compression_ratio": 1.6956521739130435, "no_speech_prob": 4.1163573882840865e-07}, {"id": 922, "seek": 604374, "start": 6063.5199999999995, "end": 6067.62, "text": " table with actual movie names that's going to be a bit more fun we don't need it for", "tokens": [3199, 365, 3539, 3169, 5288, 300, 311, 516, 281, 312, 257, 857, 544, 1019, 321, 500, 380, 643, 309, 337], "temperature": 0.0, "avg_logprob": -0.061535319616628247, "compression_ratio": 1.6956521739130435, "no_speech_prob": 4.1163573882840865e-07}, {"id": 923, "seek": 606762, "start": 6067.62, "end": 6074.599999999999, "text": " modeling but it's just going to be better for looking at stuff. So we could use data", "tokens": [15983, 457, 309, 311, 445, 516, 281, 312, 1101, 337, 1237, 412, 1507, 13, 407, 321, 727, 764, 1412], "temperature": 0.0, "avg_logprob": -0.09521996102681975, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.102431153616635e-07}, {"id": 924, "seek": 606762, "start": 6074.599999999999, "end": 6079.5, "text": " blocks API at this point or we could just use the built-in application factory method", "tokens": [8474, 9362, 412, 341, 935, 420, 321, 727, 445, 764, 264, 3094, 12, 259, 3861, 9265, 3170], "temperature": 0.0, "avg_logprob": -0.09521996102681975, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.102431153616635e-07}, {"id": 925, "seek": 606762, "start": 6079.5, "end": 6083.64, "text": " since it's there we may as well use it so we can create a collaborative filtering data", "tokens": [1670, 309, 311, 456, 321, 815, 382, 731, 764, 309, 370, 321, 393, 1884, 257, 16555, 30822, 1412], "temperature": 0.0, "avg_logprob": -0.09521996102681975, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.102431153616635e-07}, {"id": 926, "seek": 606762, "start": 6083.64, "end": 6092.84, "text": " loaders object from a data frame by passing in the ratings table by default the user column", "tokens": [3677, 433, 2657, 490, 257, 1412, 3920, 538, 8437, 294, 264, 24603, 3199, 538, 7576, 264, 4195, 7738], "temperature": 0.0, "avg_logprob": -0.09521996102681975, "compression_ratio": 1.579185520361991, "no_speech_prob": 2.102431153616635e-07}, {"id": 927, "seek": 609284, "start": 6092.84, "end": 6099.32, "text": " is called user and ours is so fine by default the item column is called item and ours is", "tokens": [307, 1219, 4195, 293, 11896, 307, 370, 2489, 538, 7576, 264, 3174, 7738, 307, 1219, 3174, 293, 11896, 307], "temperature": 0.0, "avg_logprob": -0.07017558993715228, "compression_ratio": 1.7891156462585034, "no_speech_prob": 5.626393999591528e-07}, {"id": 928, "seek": 609284, "start": 6099.32, "end": 6106.14, "text": " not it's called title so let's pick title and choose a batch size and so if we now say", "tokens": [406, 309, 311, 1219, 4876, 370, 718, 311, 1888, 4876, 293, 2826, 257, 15245, 2744, 293, 370, 498, 321, 586, 584], "temperature": 0.0, "avg_logprob": -0.07017558993715228, "compression_ratio": 1.7891156462585034, "no_speech_prob": 5.626393999591528e-07}, {"id": 929, "seek": 609284, "start": 6106.14, "end": 6115.24, "text": " show batch here is some of that data and the rating is called rating by default so that", "tokens": [855, 15245, 510, 307, 512, 295, 300, 1412, 293, 264, 10990, 307, 1219, 10990, 538, 7576, 370, 300], "temperature": 0.0, "avg_logprob": -0.07017558993715228, "compression_ratio": 1.7891156462585034, "no_speech_prob": 5.626393999591528e-07}, {"id": 930, "seek": 611524, "start": 6115.24, "end": 6127.2, "text": " worked fine too. So here's some data. So we need to now create our let's assume we're", "tokens": [2732, 2489, 886, 13, 407, 510, 311, 512, 1412, 13, 407, 321, 643, 281, 586, 1884, 527, 718, 311, 6552, 321, 434], "temperature": 0.0, "avg_logprob": -0.14313241839408875, "compression_ratio": 1.7466666666666666, "no_speech_prob": 2.6577214384815306e-07}, {"id": 931, "seek": 611524, "start": 6127.2, "end": 6134.099999999999, "text": " going to use that five numbers of factors so the number of users is however many classes", "tokens": [516, 281, 764, 300, 1732, 3547, 295, 6771, 370, 264, 1230, 295, 5022, 307, 4461, 867, 5359], "temperature": 0.0, "avg_logprob": -0.14313241839408875, "compression_ratio": 1.7466666666666666, "no_speech_prob": 2.6577214384815306e-07}, {"id": 932, "seek": 611524, "start": 6134.099999999999, "end": 6140.679999999999, "text": " there are for user and the number of movies is however many classes there are the title", "tokens": [456, 366, 337, 4195, 293, 264, 1230, 295, 6233, 307, 4461, 867, 5359, 456, 366, 264, 4876], "temperature": 0.0, "avg_logprob": -0.14313241839408875, "compression_ratio": 1.7466666666666666, "no_speech_prob": 2.6577214384815306e-07}, {"id": 933, "seek": 614068, "start": 6140.68, "end": 6152.8, "text": " and so these are so we don't just have a vocab now right we've actually got a list of classes", "tokens": [293, 370, 613, 366, 370, 321, 500, 380, 445, 362, 257, 2329, 455, 586, 558, 321, 600, 767, 658, 257, 1329, 295, 5359], "temperature": 0.0, "avg_logprob": -0.06989624145183157, "compression_ratio": 1.4523809523809523, "no_speech_prob": 7.224429623420292e-07}, {"id": 934, "seek": 614068, "start": 6152.8, "end": 6159.0, "text": " for each categorical variable for each set of discrete choices so we've got a whole bunch", "tokens": [337, 1184, 19250, 804, 7006, 337, 1184, 992, 295, 27706, 7994, 370, 321, 600, 658, 257, 1379, 3840], "temperature": 0.0, "avg_logprob": -0.06989624145183157, "compression_ratio": 1.4523809523809523, "no_speech_prob": 7.224429623420292e-07}, {"id": 935, "seek": 615900, "start": 6159.0, "end": 6175.28, "text": " of users at 944 and a whole bunch of titles 1635 so for our randomized latent factor parameters", "tokens": [295, 5022, 412, 1722, 13912, 293, 257, 1379, 3840, 295, 12992, 3165, 8794, 370, 337, 527, 38513, 48994, 5952, 9834], "temperature": 0.0, "avg_logprob": -0.17588555812835693, "compression_ratio": 1.5635359116022098, "no_speech_prob": 1.6280481531794067e-06}, {"id": 936, "seek": 615900, "start": 6175.28, "end": 6179.36, "text": " we're going to need to create those matrices that we can just create them with random numbers", "tokens": [321, 434, 516, 281, 643, 281, 1884, 729, 32284, 300, 321, 393, 445, 1884, 552, 365, 4974, 3547], "temperature": 0.0, "avg_logprob": -0.17588555812835693, "compression_ratio": 1.5635359116022098, "no_speech_prob": 1.6280481531794067e-06}, {"id": 937, "seek": 615900, "start": 6179.36, "end": 6185.76, "text": " so this is normally distributed random numbers that's what random is and that will be n users", "tokens": [370, 341, 307, 5646, 12631, 4974, 3547, 300, 311, 437, 4974, 307, 293, 300, 486, 312, 297, 5022], "temperature": 0.0, "avg_logprob": -0.17588555812835693, "compression_ratio": 1.5635359116022098, "no_speech_prob": 1.6280481531794067e-06}, {"id": 938, "seek": 618576, "start": 6185.76, "end": 6192.72, "text": " okay so 944 by n factors which is 5 that's exactly the same as this except this is just", "tokens": [1392, 370, 1722, 13912, 538, 297, 6771, 597, 307, 1025, 300, 311, 2293, 264, 912, 382, 341, 3993, 341, 307, 445], "temperature": 0.0, "avg_logprob": -0.10011499981547511, "compression_ratio": 1.8571428571428572, "no_speech_prob": 2.657722575349908e-07}, {"id": 939, "seek": 618576, "start": 6192.72, "end": 6203.08, "text": " 15 so let's do exactly the same thing for movies random numbers and movies by 5 okay", "tokens": [2119, 370, 718, 311, 360, 2293, 264, 912, 551, 337, 6233, 4974, 3547, 293, 6233, 538, 1025, 1392], "temperature": 0.0, "avg_logprob": -0.10011499981547511, "compression_ratio": 1.8571428571428572, "no_speech_prob": 2.657722575349908e-07}, {"id": 940, "seek": 618576, "start": 6203.08, "end": 6209.2, "text": " and so to calculate the result for some movie and some user we have to look up the index", "tokens": [293, 370, 281, 8873, 264, 1874, 337, 512, 3169, 293, 512, 4195, 321, 362, 281, 574, 493, 264, 8186], "temperature": 0.0, "avg_logprob": -0.10011499981547511, "compression_ratio": 1.8571428571428572, "no_speech_prob": 2.657722575349908e-07}, {"id": 941, "seek": 618576, "start": 6209.2, "end": 6215.08, "text": " of the movie in our movie latent factors the index of the user in our user latent factors", "tokens": [295, 264, 3169, 294, 527, 3169, 48994, 6771, 264, 8186, 295, 264, 4195, 294, 527, 4195, 48994, 6771], "temperature": 0.0, "avg_logprob": -0.10011499981547511, "compression_ratio": 1.8571428571428572, "no_speech_prob": 2.657722575349908e-07}, {"id": 942, "seek": 621508, "start": 6215.08, "end": 6222.22, "text": " and then do a cross product so in other words we would say like oh okay for this particular", "tokens": [293, 550, 360, 257, 3278, 1674, 370, 294, 661, 2283, 321, 576, 584, 411, 1954, 1392, 337, 341, 1729], "temperature": 0.0, "avg_logprob": -0.06661114854327703, "compression_ratio": 1.64375, "no_speech_prob": 6.083581638449687e-07}, {"id": 943, "seek": 621508, "start": 6222.22, "end": 6229.2, "text": " combination we would have to look up that numbered user over here and that numbered", "tokens": [6562, 321, 576, 362, 281, 574, 493, 300, 40936, 4195, 670, 510, 293, 300, 40936], "temperature": 0.0, "avg_logprob": -0.06661114854327703, "compression_ratio": 1.64375, "no_speech_prob": 6.083581638449687e-07}, {"id": 944, "seek": 621508, "start": 6229.2, "end": 6238.0599999999995, "text": " movie over here to get the two appropriate sets of latent factors but this is a problem", "tokens": [3169, 670, 510, 281, 483, 264, 732, 6854, 6352, 295, 48994, 6771, 457, 341, 307, 257, 1154], "temperature": 0.0, "avg_logprob": -0.06661114854327703, "compression_ratio": 1.64375, "no_speech_prob": 6.083581638449687e-07}, {"id": 945, "seek": 623806, "start": 6238.06, "end": 6248.52, "text": " because look up in an index is not a linear model like remember our deep learning models", "tokens": [570, 574, 493, 294, 364, 8186, 307, 406, 257, 8213, 2316, 411, 1604, 527, 2452, 2539, 5245], "temperature": 0.0, "avg_logprob": -0.13339953124523163, "compression_ratio": 1.4917127071823204, "no_speech_prob": 1.1253511189579513e-07}, {"id": 946, "seek": 623806, "start": 6248.52, "end": 6256.280000000001, "text": " really only know how to just multiply matrices together and do simple element-wise nonlinearities", "tokens": [534, 787, 458, 577, 281, 445, 12972, 32284, 1214, 293, 360, 2199, 4478, 12, 3711, 2107, 28263, 1088], "temperature": 0.0, "avg_logprob": -0.13339953124523163, "compression_ratio": 1.4917127071823204, "no_speech_prob": 1.1253511189579513e-07}, {"id": 947, "seek": 623806, "start": 6256.280000000001, "end": 6264.14, "text": " like ReLU there isn't a thing called look up at an index okay I'll just finish this", "tokens": [411, 1300, 43, 52, 456, 1943, 380, 257, 551, 1219, 574, 493, 412, 364, 8186, 1392, 286, 603, 445, 2413, 341], "temperature": 0.0, "avg_logprob": -0.13339953124523163, "compression_ratio": 1.4917127071823204, "no_speech_prob": 1.1253511189579513e-07}, {"id": 948, "seek": 626414, "start": 6264.14, "end": 6274.360000000001, "text": " bit um here's a cool thing though the look up in an index is actually can be represented", "tokens": [857, 1105, 510, 311, 257, 1627, 551, 1673, 264, 574, 493, 294, 364, 8186, 307, 767, 393, 312, 10379], "temperature": 0.0, "avg_logprob": -0.12989063868446957, "compression_ratio": 1.639751552795031, "no_speech_prob": 3.927858926999761e-07}, {"id": 949, "seek": 626414, "start": 6274.360000000001, "end": 6284.04, "text": " as a matrix product believe it or not so if you replace our indices with one hot encoded", "tokens": [382, 257, 8141, 1674, 1697, 309, 420, 406, 370, 498, 291, 7406, 527, 43840, 365, 472, 2368, 2058, 12340], "temperature": 0.0, "avg_logprob": -0.12989063868446957, "compression_ratio": 1.639751552795031, "no_speech_prob": 3.927858926999761e-07}, {"id": 950, "seek": 626414, "start": 6284.04, "end": 6293.72, "text": " vectors then a one hot encoded vector times something is identical to looking up in an", "tokens": [18875, 550, 257, 472, 2368, 2058, 12340, 8062, 1413, 746, 307, 14800, 281, 1237, 493, 294, 364], "temperature": 0.0, "avg_logprob": -0.12989063868446957, "compression_ratio": 1.639751552795031, "no_speech_prob": 3.927858926999761e-07}, {"id": 951, "seek": 629372, "start": 6293.72, "end": 6308.84, "text": " index let me show you so if we grab if we call the one hot function that creates a as", "tokens": [8186, 718, 385, 855, 291, 370, 498, 321, 4444, 498, 321, 818, 264, 472, 2368, 2445, 300, 7829, 257, 382], "temperature": 0.0, "avg_logprob": -0.167992835349225, "compression_ratio": 1.5213675213675213, "no_speech_prob": 5.626396841762471e-07}, {"id": 952, "seek": 629372, "start": 6308.84, "end": 6316.88, "text": " it says here one hot encoding and we're going to one hot encode the value three with n users", "tokens": [309, 1619, 510, 472, 2368, 43430, 293, 321, 434, 516, 281, 472, 2368, 2058, 1429, 264, 2158, 1045, 365, 297, 5022], "temperature": 0.0, "avg_logprob": -0.167992835349225, "compression_ratio": 1.5213675213675213, "no_speech_prob": 5.626396841762471e-07}, {"id": 953, "seek": 631688, "start": 6316.88, "end": 6334.82, "text": " classes and so n users as we've discussed is 944 right then so if we go one hot one", "tokens": [5359, 293, 370, 297, 5022, 382, 321, 600, 7152, 307, 1722, 13912, 558, 550, 370, 498, 321, 352, 472, 2368, 472], "temperature": 0.0, "avg_logprob": -0.13249994277954102, "compression_ratio": 1.0921052631578947, "no_speech_prob": 3.2058241572485713e-07}, {"id": 954, "seek": 633482, "start": 6334.82, "end": 6353.0, "text": " hot encoding the number three into n users one hot three we get this big array big tensor", "tokens": [2368, 43430, 264, 1230, 1045, 666, 297, 5022, 472, 2368, 1045, 321, 483, 341, 955, 10225, 955, 40863], "temperature": 0.0, "avg_logprob": -0.22455514561046253, "compression_ratio": 1.2361111111111112, "no_speech_prob": 3.520908933296596e-07}, {"id": 955, "seek": 635300, "start": 6353.0, "end": 6366.6, "text": " and as you can see in index three zero one two three we have a one and the size of that", "tokens": [293, 382, 291, 393, 536, 294, 8186, 1045, 4018, 472, 732, 1045, 321, 362, 257, 472, 293, 264, 2744, 295, 300], "temperature": 0.0, "avg_logprob": -0.11794124478879182, "compression_ratio": 1.459016393442623, "no_speech_prob": 5.896412176298327e-07}, {"id": 956, "seek": 635300, "start": 6366.6, "end": 6377.44, "text": " is 944 so if we then multiply that by user factors or user factors remember is that random", "tokens": [307, 1722, 13912, 370, 498, 321, 550, 12972, 300, 538, 4195, 6771, 420, 4195, 6771, 1604, 307, 300, 4974], "temperature": 0.0, "avg_logprob": -0.11794124478879182, "compression_ratio": 1.459016393442623, "no_speech_prob": 5.896412176298327e-07}, {"id": 957, "seek": 637744, "start": 6377.44, "end": 6393.839999999999, "text": " matrix of this size so what's going to happen so we're going to go zero by the first row", "tokens": [8141, 295, 341, 2744, 370, 437, 311, 516, 281, 1051, 370, 321, 434, 516, 281, 352, 4018, 538, 264, 700, 5386], "temperature": 0.0, "avg_logprob": -0.13273767111957938, "compression_ratio": 2.0, "no_speech_prob": 1.6280454246953013e-06}, {"id": 958, "seek": 637744, "start": 6393.839999999999, "end": 6397.04, "text": " and so that's going to be all zeros and then we're going to go zero again they're going", "tokens": [293, 370, 300, 311, 516, 281, 312, 439, 35193, 293, 550, 321, 434, 516, 281, 352, 4018, 797, 436, 434, 516], "temperature": 0.0, "avg_logprob": -0.13273767111957938, "compression_ratio": 2.0, "no_speech_prob": 1.6280454246953013e-06}, {"id": 959, "seek": 637744, "start": 6397.04, "end": 6404.32, "text": " to zero again and then we're going to find it finally go one right on the index three", "tokens": [281, 4018, 797, 293, 550, 321, 434, 516, 281, 915, 309, 2721, 352, 472, 558, 322, 264, 8186, 1045], "temperature": 0.0, "avg_logprob": -0.13273767111957938, "compression_ratio": 2.0, "no_speech_prob": 1.6280454246953013e-06}, {"id": 960, "seek": 640432, "start": 6404.32, "end": 6409.44, "text": " row and so it's going to return each of them and then we'll go back to zero again so if", "tokens": [5386, 293, 370, 309, 311, 516, 281, 2736, 1184, 295, 552, 293, 550, 321, 603, 352, 646, 281, 4018, 797, 370, 498], "temperature": 0.0, "avg_logprob": -0.14826074176364476, "compression_ratio": 1.435483870967742, "no_speech_prob": 1.6280487216135953e-06}, {"id": 961, "seek": 640432, "start": 6409.44, "end": 6426.639999999999, "text": " we do that remember at sign is matrix multiply and compare that to user factors three same", "tokens": [321, 360, 300, 1604, 412, 1465, 307, 8141, 12972, 293, 6794, 300, 281, 4195, 6771, 1045, 912], "temperature": 0.0, "avg_logprob": -0.14826074176364476, "compression_ratio": 1.435483870967742, "no_speech_prob": 1.6280487216135953e-06}, {"id": 962, "seek": 642664, "start": 6426.64, "end": 6435.72, "text": " thing isn't that crazy so it's a kind of weird inefficient way to do it right but matrix", "tokens": [551, 1943, 380, 300, 3219, 370, 309, 311, 257, 733, 295, 3657, 43495, 636, 281, 360, 309, 558, 457, 8141], "temperature": 0.0, "avg_logprob": -0.100645515356171, "compression_ratio": 1.755, "no_speech_prob": 7.571133551209641e-07}, {"id": 963, "seek": 642664, "start": 6435.72, "end": 6443.4800000000005, "text": " multiplication is a way to index into an array and this is the thing that we know how to", "tokens": [27290, 307, 257, 636, 281, 8186, 666, 364, 10225, 293, 341, 307, 264, 551, 300, 321, 458, 577, 281], "temperature": 0.0, "avg_logprob": -0.100645515356171, "compression_ratio": 1.755, "no_speech_prob": 7.571133551209641e-07}, {"id": 964, "seek": 642664, "start": 6443.4800000000005, "end": 6449.280000000001, "text": " do SGD with and we know how to build models with so it turns out that anything that we", "tokens": [360, 34520, 35, 365, 293, 321, 458, 577, 281, 1322, 5245, 365, 370, 309, 4523, 484, 300, 1340, 300, 321], "temperature": 0.0, "avg_logprob": -0.100645515356171, "compression_ratio": 1.755, "no_speech_prob": 7.571133551209641e-07}, {"id": 965, "seek": 642664, "start": 6449.280000000001, "end": 6455.8, "text": " can do with indexing to array we now have a way to optimize and we have a question are", "tokens": [393, 360, 365, 8186, 278, 281, 10225, 321, 586, 362, 257, 636, 281, 19719, 293, 321, 362, 257, 1168, 366], "temperature": 0.0, "avg_logprob": -0.100645515356171, "compression_ratio": 1.755, "no_speech_prob": 7.571133551209641e-07}, {"id": 966, "seek": 645580, "start": 6455.8, "end": 6462.28, "text": " there two questions one how different in practice is collaborative filtering with sparse data", "tokens": [456, 732, 1651, 472, 577, 819, 294, 3124, 307, 16555, 30822, 365, 637, 11668, 1412], "temperature": 0.0, "avg_logprob": -0.12070274353027344, "compression_ratio": 1.7307692307692308, "no_speech_prob": 8.267796147265472e-06}, {"id": 967, "seek": 645580, "start": 6462.28, "end": 6469.6, "text": " compared to dense data we are not doing sparse data in this course but there's an excellent", "tokens": [5347, 281, 18011, 1412, 321, 366, 406, 884, 637, 11668, 1412, 294, 341, 1164, 457, 456, 311, 364, 7103], "temperature": 0.0, "avg_logprob": -0.12070274353027344, "compression_ratio": 1.7307692307692308, "no_speech_prob": 8.267796147265472e-06}, {"id": 968, "seek": 645580, "start": 6469.6, "end": 6474.84, "text": " course I hear called computational linear algebra for coders it has a lot of information", "tokens": [1164, 286, 1568, 1219, 28270, 8213, 21989, 337, 17656, 433, 309, 575, 257, 688, 295, 1589], "temperature": 0.0, "avg_logprob": -0.12070274353027344, "compression_ratio": 1.7307692307692308, "no_speech_prob": 8.267796147265472e-06}, {"id": 969, "seek": 645580, "start": 6474.84, "end": 6482.12, "text": " about sparse the fast AI course and second question in practice do we tune the number", "tokens": [466, 637, 11668, 264, 2370, 7318, 1164, 293, 1150, 1168, 294, 3124, 360, 321, 10864, 264, 1230], "temperature": 0.0, "avg_logprob": -0.12070274353027344, "compression_ratio": 1.7307692307692308, "no_speech_prob": 8.267796147265472e-06}, {"id": 970, "seek": 648212, "start": 6482.12, "end": 6490.48, "text": " of latent factors absolutely we do yes it's just a it's just a number of filters like", "tokens": [295, 48994, 6771, 3122, 321, 360, 2086, 309, 311, 445, 257, 309, 311, 445, 257, 1230, 295, 15995, 411], "temperature": 0.0, "avg_logprob": -0.11752325589539575, "compression_ratio": 1.6496815286624205, "no_speech_prob": 7.934455084068759e-07}, {"id": 971, "seek": 648212, "start": 6490.48, "end": 6502.48, "text": " we have in much any kind of deep learning model all right so now that we know that the", "tokens": [321, 362, 294, 709, 604, 733, 295, 2452, 2539, 2316, 439, 558, 370, 586, 300, 321, 458, 300, 264], "temperature": 0.0, "avg_logprob": -0.11752325589539575, "compression_ratio": 1.6496815286624205, "no_speech_prob": 7.934455084068759e-07}, {"id": 972, "seek": 648212, "start": 6502.48, "end": 6508.04, "text": " procedure of finding out which latent set of latent factors is the right thing looking", "tokens": [10747, 295, 5006, 484, 597, 48994, 992, 295, 48994, 6771, 307, 264, 558, 551, 1237], "temperature": 0.0, "avg_logprob": -0.11752325589539575, "compression_ratio": 1.6496815286624205, "no_speech_prob": 7.934455084068759e-07}, {"id": 973, "seek": 650804, "start": 6508.04, "end": 6515.48, "text": " something up at an index is the same as matrix multiplication with a one-hot vector or we", "tokens": [746, 493, 412, 364, 8186, 307, 264, 912, 382, 8141, 27290, 365, 257, 472, 12, 12194, 8062, 420, 321], "temperature": 0.0, "avg_logprob": -0.16398320478551529, "compression_ratio": 1.5823529411764705, "no_speech_prob": 5.626390588986396e-07}, {"id": 974, "seek": 650804, "start": 6515.48, "end": 6525.4, "text": " had it over here we can go ahead and build a model with that so basically if we do this", "tokens": [632, 309, 670, 510, 321, 393, 352, 2286, 293, 1322, 257, 2316, 365, 300, 370, 1936, 498, 321, 360, 341], "temperature": 0.0, "avg_logprob": -0.16398320478551529, "compression_ratio": 1.5823529411764705, "no_speech_prob": 5.626390588986396e-07}, {"id": 975, "seek": 650804, "start": 6525.4, "end": 6529.88, "text": " for a whole for a few more indices at once then we have a matrix of one hot encoded vectors", "tokens": [337, 257, 1379, 337, 257, 1326, 544, 43840, 412, 1564, 550, 321, 362, 257, 8141, 295, 472, 2368, 2058, 12340, 18875], "temperature": 0.0, "avg_logprob": -0.16398320478551529, "compression_ratio": 1.5823529411764705, "no_speech_prob": 5.626390588986396e-07}, {"id": 976, "seek": 652988, "start": 6529.88, "end": 6538.64, "text": " so the whole thing is just one big matrix multiplication now the thing is as I said", "tokens": [370, 264, 1379, 551, 307, 445, 472, 955, 8141, 27290, 586, 264, 551, 307, 382, 286, 848], "temperature": 0.0, "avg_logprob": -0.09221624533335368, "compression_ratio": 1.6624203821656052, "no_speech_prob": 2.0377467535581673e-07}, {"id": 977, "seek": 652988, "start": 6538.64, "end": 6549.36, "text": " this is a pretty inefficient way to do an index lookup so there is a computational shortcut", "tokens": [341, 307, 257, 1238, 43495, 636, 281, 360, 364, 8186, 574, 1010, 370, 456, 307, 257, 28270, 24822], "temperature": 0.0, "avg_logprob": -0.09221624533335368, "compression_ratio": 1.6624203821656052, "no_speech_prob": 2.0377467535581673e-07}, {"id": 978, "seek": 652988, "start": 6549.36, "end": 6558.4400000000005, "text": " which is called an embedding an embedding is a layer that has the computational speed", "tokens": [597, 307, 1219, 364, 12240, 3584, 364, 12240, 3584, 307, 257, 4583, 300, 575, 264, 28270, 3073], "temperature": 0.0, "avg_logprob": -0.09221624533335368, "compression_ratio": 1.6624203821656052, "no_speech_prob": 2.0377467535581673e-07}, {"id": 979, "seek": 655844, "start": 6558.44, "end": 6567.599999999999, "text": " of an array lookup and the same gradients as a matrix multiplication how does it do", "tokens": [295, 364, 10225, 574, 1010, 293, 264, 912, 2771, 2448, 382, 257, 8141, 27290, 577, 775, 309, 360], "temperature": 0.0, "avg_logprob": -0.10532073676586151, "compression_ratio": 1.6855345911949686, "no_speech_prob": 6.083579364712932e-07}, {"id": 980, "seek": 655844, "start": 6567.599999999999, "end": 6575.12, "text": " that well just internally it uses an index lookup to actually grab the values and it", "tokens": [300, 731, 445, 19501, 309, 4960, 364, 8186, 574, 1010, 281, 767, 4444, 264, 4190, 293, 309], "temperature": 0.0, "avg_logprob": -0.10532073676586151, "compression_ratio": 1.6855345911949686, "no_speech_prob": 6.083579364712932e-07}, {"id": 981, "seek": 655844, "start": 6575.12, "end": 6584.719999999999, "text": " also knows what the gradient of a matrix multiplication by a one-hot encoded vector is or matrix is", "tokens": [611, 3255, 437, 264, 16235, 295, 257, 8141, 27290, 538, 257, 472, 12, 12194, 2058, 12340, 8062, 307, 420, 8141, 307], "temperature": 0.0, "avg_logprob": -0.10532073676586151, "compression_ratio": 1.6855345911949686, "no_speech_prob": 6.083579364712932e-07}, {"id": 982, "seek": 658472, "start": 6584.72, "end": 6590.400000000001, "text": " without having to go to all this trouble and so an embedding is a matrix multiplication", "tokens": [1553, 1419, 281, 352, 281, 439, 341, 5253, 293, 370, 364, 12240, 3584, 307, 257, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.07638614868449274, "compression_ratio": 1.8492063492063493, "no_speech_prob": 1.1911052979485248e-06}, {"id": 983, "seek": 658472, "start": 6590.400000000001, "end": 6594.360000000001, "text": " with a one-hot encoded vector where you never actually have to create the one-hot encoded", "tokens": [365, 257, 472, 12, 12194, 2058, 12340, 8062, 689, 291, 1128, 767, 362, 281, 1884, 264, 472, 12, 12194, 2058, 12340], "temperature": 0.0, "avg_logprob": -0.07638614868449274, "compression_ratio": 1.8492063492063493, "no_speech_prob": 1.1911052979485248e-06}, {"id": 984, "seek": 658472, "start": 6594.360000000001, "end": 6600.08, "text": " vector you just need the indexes this is important to remember because a lot of people have heard", "tokens": [8062, 291, 445, 643, 264, 8186, 279, 341, 307, 1021, 281, 1604, 570, 257, 688, 295, 561, 362, 2198], "temperature": 0.0, "avg_logprob": -0.07638614868449274, "compression_ratio": 1.8492063492063493, "no_speech_prob": 1.1911052979485248e-06}, {"id": 985, "seek": 658472, "start": 6600.08, "end": 6606.320000000001, "text": " about embeddings and they think there's something special and magical and and they're absolutely", "tokens": [466, 12240, 29432, 293, 436, 519, 456, 311, 746, 2121, 293, 12066, 293, 293, 436, 434, 3122], "temperature": 0.0, "avg_logprob": -0.07638614868449274, "compression_ratio": 1.8492063492063493, "no_speech_prob": 1.1911052979485248e-06}, {"id": 986, "seek": 658472, "start": 6606.320000000001, "end": 6611.400000000001, "text": " not you can do exactly the same thing by creating a one-hot encoded matrix and doing a matrix", "tokens": [406, 291, 393, 360, 2293, 264, 912, 551, 538, 4084, 257, 472, 12, 12194, 2058, 12340, 8141, 293, 884, 257, 8141], "temperature": 0.0, "avg_logprob": -0.07638614868449274, "compression_ratio": 1.8492063492063493, "no_speech_prob": 1.1911052979485248e-06}, {"id": 987, "seek": 661140, "start": 6611.4, "end": 6618.679999999999, "text": " multiply it is just a computational shortcut nothing else I often find when I talk to people", "tokens": [12972, 309, 307, 445, 257, 28270, 24822, 1825, 1646, 286, 2049, 915, 562, 286, 751, 281, 561], "temperature": 0.0, "avg_logprob": -0.06184688026522413, "compression_ratio": 1.7255813953488373, "no_speech_prob": 1.5056959909998113e-06}, {"id": 988, "seek": 661140, "start": 6618.679999999999, "end": 6624.679999999999, "text": " about this in person I have to tell them this six or seven times before they believe me", "tokens": [466, 341, 294, 954, 286, 362, 281, 980, 552, 341, 2309, 420, 3407, 1413, 949, 436, 1697, 385], "temperature": 0.0, "avg_logprob": -0.06184688026522413, "compression_ratio": 1.7255813953488373, "no_speech_prob": 1.5056959909998113e-06}, {"id": 989, "seek": 661140, "start": 6624.679999999999, "end": 6629.48, "text": " because they think embeddings are something more clever and they're not it's just a computational", "tokens": [570, 436, 519, 12240, 29432, 366, 746, 544, 13494, 293, 436, 434, 406, 309, 311, 445, 257, 28270], "temperature": 0.0, "avg_logprob": -0.06184688026522413, "compression_ratio": 1.7255813953488373, "no_speech_prob": 1.5056959909998113e-06}, {"id": 990, "seek": 661140, "start": 6629.48, "end": 6634.879999999999, "text": " shortcut to do a matrix multiplication more quickly with a one-hot encoded matrix by instead", "tokens": [24822, 281, 360, 257, 8141, 27290, 544, 2661, 365, 257, 472, 12, 12194, 2058, 12340, 8141, 538, 2602], "temperature": 0.0, "avg_logprob": -0.06184688026522413, "compression_ratio": 1.7255813953488373, "no_speech_prob": 1.5056959909998113e-06}, {"id": 991, "seek": 663488, "start": 6634.88, "end": 6646.36, "text": " doing an array lookup okay so let's try and create a collaborative filtering model in", "tokens": [884, 364, 10225, 574, 1010, 1392, 370, 718, 311, 853, 293, 1884, 257, 16555, 30822, 2316, 294], "temperature": 0.0, "avg_logprob": -0.1689022332429886, "compression_ratio": 1.5235294117647058, "no_speech_prob": 8.579220889259886e-07}, {"id": 992, "seek": 663488, "start": 6646.36, "end": 6657.24, "text": " pytorch a model or an architecture or really an nn.module is a class so to use pytorch", "tokens": [25878, 284, 339, 257, 2316, 420, 364, 9482, 420, 534, 364, 297, 77, 13, 8014, 2271, 307, 257, 1508, 370, 281, 764, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.1689022332429886, "compression_ratio": 1.5235294117647058, "no_speech_prob": 8.579220889259886e-07}, {"id": 993, "seek": 663488, "start": 6657.24, "end": 6660.64, "text": " through its fullest you need to understand object-oriented programming because we have", "tokens": [807, 1080, 45154, 291, 643, 281, 1223, 2657, 12, 27414, 9410, 570, 321, 362], "temperature": 0.0, "avg_logprob": -0.1689022332429886, "compression_ratio": 1.5235294117647058, "no_speech_prob": 8.579220889259886e-07}, {"id": 994, "seek": 666064, "start": 6660.64, "end": 6665.4800000000005, "text": " to create classes there's a lot of tutorials about this so I won't go into detail about", "tokens": [281, 1884, 5359, 456, 311, 257, 688, 295, 17616, 466, 341, 370, 286, 1582, 380, 352, 666, 2607, 466], "temperature": 0.0, "avg_logprob": -0.05782966835554256, "compression_ratio": 1.8426395939086295, "no_speech_prob": 1.101593738894735e-06}, {"id": 995, "seek": 666064, "start": 6665.4800000000005, "end": 6676.0, "text": " it but I'll give you a quick overview a class could be something like dog or resnet or circle", "tokens": [309, 457, 286, 603, 976, 291, 257, 1702, 12492, 257, 1508, 727, 312, 746, 411, 3000, 420, 725, 7129, 420, 6329], "temperature": 0.0, "avg_logprob": -0.05782966835554256, "compression_ratio": 1.8426395939086295, "no_speech_prob": 1.101593738894735e-06}, {"id": 996, "seek": 666064, "start": 6676.0, "end": 6680.4800000000005, "text": " and it's something that has some data attached to it and it has some functionality attached", "tokens": [293, 309, 311, 746, 300, 575, 512, 1412, 8570, 281, 309, 293, 309, 575, 512, 14980, 8570], "temperature": 0.0, "avg_logprob": -0.05782966835554256, "compression_ratio": 1.8426395939086295, "no_speech_prob": 1.101593738894735e-06}, {"id": 997, "seek": 666064, "start": 6680.4800000000005, "end": 6688.400000000001, "text": " to it is a class called example the data it has attached to it is a and the functionality", "tokens": [281, 309, 307, 257, 1508, 1219, 1365, 264, 1412, 309, 575, 8570, 281, 309, 307, 257, 293, 264, 14980], "temperature": 0.0, "avg_logprob": -0.05782966835554256, "compression_ratio": 1.8426395939086295, "no_speech_prob": 1.101593738894735e-06}, {"id": 998, "seek": 668840, "start": 6688.4, "end": 6696.08, "text": " attached to it is say and so we can for example create an instance of this class an object", "tokens": [8570, 281, 309, 307, 584, 293, 370, 321, 393, 337, 1365, 1884, 364, 5197, 295, 341, 1508, 364, 2657], "temperature": 0.0, "avg_logprob": -0.10790480794133367, "compression_ratio": 1.705128205128205, "no_speech_prob": 2.457986738590989e-07}, {"id": 999, "seek": 668840, "start": 6696.08, "end": 6704.82, "text": " of this type example we pass in Sylvain so Sylvain will now be in ex.a and we can then", "tokens": [295, 341, 2010, 1365, 321, 1320, 294, 3902, 14574, 491, 370, 3902, 14574, 491, 486, 586, 312, 294, 454, 13, 64, 293, 321, 393, 550], "temperature": 0.0, "avg_logprob": -0.10790480794133367, "compression_ratio": 1.705128205128205, "no_speech_prob": 2.457986738590989e-07}, {"id": 1000, "seek": 668840, "start": 6704.82, "end": 6710.719999999999, "text": " say ex.say and it will call say and it will say passing in nice to meet you so that will", "tokens": [584, 454, 13, 21664, 293, 309, 486, 818, 584, 293, 309, 486, 584, 8437, 294, 1481, 281, 1677, 291, 370, 300, 486], "temperature": 0.0, "avg_logprob": -0.10790480794133367, "compression_ratio": 1.705128205128205, "no_speech_prob": 2.457986738590989e-07}, {"id": 1001, "seek": 671072, "start": 6710.72, "end": 6722.84, "text": " be ex and so it'll say hello self.a so that's Sylvain nice to meet you here it is okay so", "tokens": [312, 454, 293, 370, 309, 603, 584, 7751, 2698, 13, 64, 370, 300, 311, 3902, 14574, 491, 1481, 281, 1677, 291, 510, 309, 307, 1392, 370], "temperature": 0.0, "avg_logprob": -0.17902798970540365, "compression_ratio": 1.5798816568047338, "no_speech_prob": 4.965273205925769e-07}, {"id": 1002, "seek": 671072, "start": 6722.84, "end": 6729.52, "text": " in Python the way you create a class is to say class and its name then to say what is", "tokens": [294, 15329, 264, 636, 291, 1884, 257, 1508, 307, 281, 584, 1508, 293, 1080, 1315, 550, 281, 584, 437, 307], "temperature": 0.0, "avg_logprob": -0.17902798970540365, "compression_ratio": 1.5798816568047338, "no_speech_prob": 4.965273205925769e-07}, {"id": 1003, "seek": 671072, "start": 6729.52, "end": 6735.56, "text": " passed to it when you create that object it's a special method called dunder in it as we've", "tokens": [4678, 281, 309, 562, 291, 1884, 300, 2657, 309, 311, 257, 2121, 3170, 1219, 274, 6617, 294, 309, 382, 321, 600], "temperature": 0.0, "avg_logprob": -0.17902798970540365, "compression_ratio": 1.5798816568047338, "no_speech_prob": 4.965273205925769e-07}, {"id": 1004, "seek": 673556, "start": 6735.56, "end": 6741.76, "text": " briefly mentioned before in Python there are all kinds of special method names that have", "tokens": [10515, 2835, 949, 294, 15329, 456, 366, 439, 3685, 295, 2121, 3170, 5288, 300, 362], "temperature": 0.0, "avg_logprob": -0.11766637563705444, "compression_ratio": 1.8284313725490196, "no_speech_prob": 9.874598845271976e-07}, {"id": 1005, "seek": 673556, "start": 6741.76, "end": 6747.200000000001, "text": " special behavior they start with two underscores they end with two underscores and we pronounce", "tokens": [2121, 5223, 436, 722, 365, 732, 16692, 66, 2706, 436, 917, 365, 732, 16692, 66, 2706, 293, 321, 19567], "temperature": 0.0, "avg_logprob": -0.11766637563705444, "compression_ratio": 1.8284313725490196, "no_speech_prob": 9.874598845271976e-07}, {"id": 1006, "seek": 673556, "start": 6747.200000000001, "end": 6758.280000000001, "text": " that dunder so dunder in it all methods in all regular methods instance methods in Python", "tokens": [300, 274, 6617, 370, 274, 6617, 294, 309, 439, 7150, 294, 439, 3890, 7150, 5197, 7150, 294, 15329], "temperature": 0.0, "avg_logprob": -0.11766637563705444, "compression_ratio": 1.8284313725490196, "no_speech_prob": 9.874598845271976e-07}, {"id": 1007, "seek": 673556, "start": 6758.280000000001, "end": 6765.06, "text": " always get past the actual object itself first that we normally call that self and then optionally", "tokens": [1009, 483, 1791, 264, 3539, 2657, 2564, 700, 300, 321, 5646, 818, 300, 2698, 293, 550, 3614, 379], "temperature": 0.0, "avg_logprob": -0.11766637563705444, "compression_ratio": 1.8284313725490196, "no_speech_prob": 9.874598845271976e-07}, {"id": 1008, "seek": 676506, "start": 6765.06, "end": 6770.52, "text": " anything else and so you can then change the contents of the current object by just setting", "tokens": [1340, 1646, 293, 370, 291, 393, 550, 1319, 264, 15768, 295, 264, 2190, 2657, 538, 445, 3287], "temperature": 0.0, "avg_logprob": -0.10967951381907744, "compression_ratio": 1.6872037914691944, "no_speech_prob": 1.7330449963992578e-06}, {"id": 1009, "seek": 676506, "start": 6770.52, "end": 6778.64, "text": " self dot whatever to whatever you like so after this self.a is now equal to Sylvain", "tokens": [2698, 5893, 2035, 281, 2035, 291, 411, 370, 934, 341, 2698, 13, 64, 307, 586, 2681, 281, 3902, 14574, 491], "temperature": 0.0, "avg_logprob": -0.10967951381907744, "compression_ratio": 1.6872037914691944, "no_speech_prob": 1.7330449963992578e-06}, {"id": 1010, "seek": 676506, "start": 6778.64, "end": 6784.52, "text": " so we call a method same thing that's past self optionally anything you pass to it and", "tokens": [370, 321, 818, 257, 3170, 912, 551, 300, 311, 1791, 2698, 3614, 379, 1340, 291, 1320, 281, 309, 293], "temperature": 0.0, "avg_logprob": -0.10967951381907744, "compression_ratio": 1.6872037914691944, "no_speech_prob": 1.7330449963992578e-06}, {"id": 1011, "seek": 676506, "start": 6784.52, "end": 6789.84, "text": " then you can access the contents of self which you stashed away back here when we initialized", "tokens": [550, 291, 393, 2105, 264, 15768, 295, 2698, 597, 291, 342, 12219, 1314, 646, 510, 562, 321, 5883, 1602], "temperature": 0.0, "avg_logprob": -0.10967951381907744, "compression_ratio": 1.6872037914691944, "no_speech_prob": 1.7330449963992578e-06}, {"id": 1012, "seek": 678984, "start": 6789.84, "end": 6795.72, "text": " it so that's basically how object or you know the basics of object oriented programming", "tokens": [309, 370, 300, 311, 1936, 577, 2657, 420, 291, 458, 264, 14688, 295, 2657, 21841, 9410], "temperature": 0.0, "avg_logprob": -0.08112935896043653, "compression_ratio": 1.7512437810945274, "no_speech_prob": 2.30906806564235e-07}, {"id": 1013, "seek": 678984, "start": 6795.72, "end": 6804.68, "text": " works in Python in Python there's something else you can do when you create a new class", "tokens": [1985, 294, 15329, 294, 15329, 456, 311, 746, 1646, 291, 393, 360, 562, 291, 1884, 257, 777, 1508], "temperature": 0.0, "avg_logprob": -0.08112935896043653, "compression_ratio": 1.7512437810945274, "no_speech_prob": 2.30906806564235e-07}, {"id": 1014, "seek": 678984, "start": 6804.68, "end": 6809.16, "text": " which is you can pop something in parentheses after its name and that means we're going", "tokens": [597, 307, 291, 393, 1665, 746, 294, 34153, 934, 1080, 1315, 293, 300, 1355, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.08112935896043653, "compression_ratio": 1.7512437810945274, "no_speech_prob": 2.30906806564235e-07}, {"id": 1015, "seek": 678984, "start": 6809.16, "end": 6814.2, "text": " to use something called inheritance and what inheritance means is I want to have all the", "tokens": [281, 764, 746, 1219, 32122, 293, 437, 32122, 1355, 307, 286, 528, 281, 362, 439, 264], "temperature": 0.0, "avg_logprob": -0.08112935896043653, "compression_ratio": 1.7512437810945274, "no_speech_prob": 2.30906806564235e-07}, {"id": 1016, "seek": 681420, "start": 6814.2, "end": 6821.04, "text": " functionality of this class plus I want to add some additional functionality so a module", "tokens": [14980, 295, 341, 1508, 1804, 286, 528, 281, 909, 512, 4497, 14980, 370, 257, 10088], "temperature": 0.0, "avg_logprob": -0.09343299721226547, "compression_ratio": 1.6280487804878048, "no_speech_prob": 1.0511473647056846e-06}, {"id": 1017, "seek": 681420, "start": 6821.04, "end": 6830.0, "text": " is a pytorch class which fastai has customized so it's kind of a fastai version of a pytorch", "tokens": [307, 257, 25878, 284, 339, 1508, 597, 2370, 1301, 575, 30581, 370, 309, 311, 733, 295, 257, 2370, 1301, 3037, 295, 257, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.09343299721226547, "compression_ratio": 1.6280487804878048, "no_speech_prob": 1.0511473647056846e-06}, {"id": 1018, "seek": 681420, "start": 6830.0, "end": 6840.28, "text": " class and probably in the next course we'll see exactly how it works and but it looks", "tokens": [1508, 293, 1391, 294, 264, 958, 1164, 321, 603, 536, 2293, 577, 309, 1985, 293, 457, 309, 1542], "temperature": 0.0, "avg_logprob": -0.09343299721226547, "compression_ratio": 1.6280487804878048, "no_speech_prob": 1.0511473647056846e-06}, {"id": 1019, "seek": 684028, "start": 6840.28, "end": 6847.48, "text": " a lot like a it acts almost exactly like a just a regular Python class we have an init", "tokens": [257, 688, 411, 257, 309, 10672, 1920, 2293, 411, 257, 445, 257, 3890, 15329, 1508, 321, 362, 364, 3157], "temperature": 0.0, "avg_logprob": -0.12241916442185305, "compression_ratio": 1.7342995169082125, "no_speech_prob": 3.689877985380008e-07}, {"id": 1020, "seek": 684028, "start": 6847.48, "end": 6855.2, "text": " and we can set attributes to whatever we like and one of the things we can use is an embedding", "tokens": [293, 321, 393, 992, 17212, 281, 2035, 321, 411, 293, 472, 295, 264, 721, 321, 393, 764, 307, 364, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.12241916442185305, "compression_ratio": 1.7342995169082125, "no_speech_prob": 3.689877985380008e-07}, {"id": 1021, "seek": 684028, "start": 6855.2, "end": 6859.96, "text": " and so an embedding is just this class that does what I just described a it's the same", "tokens": [293, 370, 364, 12240, 3584, 307, 445, 341, 1508, 300, 775, 437, 286, 445, 7619, 257, 309, 311, 264, 912], "temperature": 0.0, "avg_logprob": -0.12241916442185305, "compression_ratio": 1.7342995169082125, "no_speech_prob": 3.689877985380008e-07}, {"id": 1022, "seek": 684028, "start": 6859.96, "end": 6865.98, "text": " as an as a linear layer with one hot encoded matrix but it does it with this computational", "tokens": [382, 364, 382, 257, 8213, 4583, 365, 472, 2368, 2058, 12340, 8141, 457, 309, 775, 309, 365, 341, 28270], "temperature": 0.0, "avg_logprob": -0.12241916442185305, "compression_ratio": 1.7342995169082125, "no_speech_prob": 3.689877985380008e-07}, {"id": 1023, "seek": 686598, "start": 6865.98, "end": 6871.08, "text": " shortcut so you can say how many in this case users are there and how many factors will", "tokens": [24822, 370, 291, 393, 584, 577, 867, 294, 341, 1389, 5022, 366, 456, 293, 577, 867, 6771, 486], "temperature": 0.0, "avg_logprob": -0.08350350533002689, "compression_ratio": 1.8051282051282052, "no_speech_prob": 6.681504487460188e-07}, {"id": 1024, "seek": 686598, "start": 6871.08, "end": 6878.5599999999995, "text": " they have now there is one very special thing about things that inherit from module which", "tokens": [436, 362, 586, 456, 307, 472, 588, 2121, 551, 466, 721, 300, 21389, 490, 10088, 597], "temperature": 0.0, "avg_logprob": -0.08350350533002689, "compression_ratio": 1.8051282051282052, "no_speech_prob": 6.681504487460188e-07}, {"id": 1025, "seek": 686598, "start": 6878.5599999999995, "end": 6884.28, "text": " is that when you call them it will actually call a method called forward so forward is", "tokens": [307, 300, 562, 291, 818, 552, 309, 486, 767, 818, 257, 3170, 1219, 2128, 370, 2128, 307], "temperature": 0.0, "avg_logprob": -0.08350350533002689, "compression_ratio": 1.8051282051282052, "no_speech_prob": 6.681504487460188e-07}, {"id": 1026, "seek": 686598, "start": 6884.28, "end": 6889.879999999999, "text": " a special pytorch method name it's the most important pytorch method name this is where", "tokens": [257, 2121, 25878, 284, 339, 3170, 1315, 309, 311, 264, 881, 1021, 25878, 284, 339, 3170, 1315, 341, 307, 689], "temperature": 0.0, "avg_logprob": -0.08350350533002689, "compression_ratio": 1.8051282051282052, "no_speech_prob": 6.681504487460188e-07}, {"id": 1027, "seek": 688988, "start": 6889.88, "end": 6898.52, "text": " you put the actual computation so to grab the factors from an embedding we just call", "tokens": [291, 829, 264, 3539, 24903, 370, 281, 4444, 264, 6771, 490, 364, 12240, 3584, 321, 445, 818], "temperature": 0.0, "avg_logprob": -0.08147685868399483, "compression_ratio": 1.766497461928934, "no_speech_prob": 2.9649007160514884e-07}, {"id": 1028, "seek": 688988, "start": 6898.52, "end": 6905.72, "text": " it like a function right so this is going to get passed here the user IDs and the movie", "tokens": [309, 411, 257, 2445, 558, 370, 341, 307, 516, 281, 483, 4678, 510, 264, 4195, 48212, 293, 264, 3169], "temperature": 0.0, "avg_logprob": -0.08147685868399483, "compression_ratio": 1.766497461928934, "no_speech_prob": 2.9649007160514884e-07}, {"id": 1029, "seek": 688988, "start": 6905.72, "end": 6912.6, "text": " IDs as two columns so let's grab the zero index column and grab the embeddings by passing", "tokens": [48212, 382, 732, 13766, 370, 718, 311, 4444, 264, 4018, 8186, 7738, 293, 4444, 264, 12240, 29432, 538, 8437], "temperature": 0.0, "avg_logprob": -0.08147685868399483, "compression_ratio": 1.766497461928934, "no_speech_prob": 2.9649007160514884e-07}, {"id": 1030, "seek": 688988, "start": 6912.6, "end": 6917.7, "text": " them to user factors and then we'll do the same thing for the index one column that's", "tokens": [552, 281, 4195, 6771, 293, 550, 321, 603, 360, 264, 912, 551, 337, 264, 8186, 472, 7738, 300, 311], "temperature": 0.0, "avg_logprob": -0.08147685868399483, "compression_ratio": 1.766497461928934, "no_speech_prob": 2.9649007160514884e-07}, {"id": 1031, "seek": 691770, "start": 6917.7, "end": 6925.36, "text": " the movie IDs pass them to the movie factors and then here is our element-wise multiplication", "tokens": [264, 3169, 48212, 1320, 552, 281, 264, 3169, 6771, 293, 550, 510, 307, 527, 4478, 12, 3711, 27290], "temperature": 0.0, "avg_logprob": -0.11347935994466146, "compression_ratio": 1.6918238993710693, "no_speech_prob": 9.422428206562472e-07}, {"id": 1032, "seek": 691770, "start": 6925.36, "end": 6932.72, "text": " and then sum and now remember we've got another dimension time the first axis is the mini", "tokens": [293, 550, 2408, 293, 586, 1604, 321, 600, 658, 1071, 10139, 565, 264, 700, 10298, 307, 264, 8382], "temperature": 0.0, "avg_logprob": -0.11347935994466146, "compression_ratio": 1.6918238993710693, "no_speech_prob": 9.422428206562472e-07}, {"id": 1033, "seek": 691770, "start": 6932.72, "end": 6940.0, "text": " batch dimension so we want to sum over the other dimension the index one dimension so", "tokens": [15245, 10139, 370, 321, 528, 281, 2408, 670, 264, 661, 10139, 264, 8186, 472, 10139, 370], "temperature": 0.0, "avg_logprob": -0.11347935994466146, "compression_ratio": 1.6918238993710693, "no_speech_prob": 9.422428206562472e-07}, {"id": 1034, "seek": 694000, "start": 6940.0, "end": 6947.72, "text": " that's going to give us a dot product for each user sorry for each rating for each user", "tokens": [300, 311, 516, 281, 976, 505, 257, 5893, 1674, 337, 1184, 4195, 2597, 337, 1184, 10990, 337, 1184, 4195], "temperature": 0.0, "avg_logprob": -0.13027824572662808, "compression_ratio": 1.623456790123457, "no_speech_prob": 5.4845244790158176e-08}, {"id": 1035, "seek": 694000, "start": 6947.72, "end": 6958.32, "text": " movie combination so this is the dot product class so you can see if we look at one batch", "tokens": [3169, 6562, 370, 341, 307, 264, 5893, 1674, 1508, 370, 291, 393, 536, 498, 321, 574, 412, 472, 15245], "temperature": 0.0, "avg_logprob": -0.13027824572662808, "compression_ratio": 1.623456790123457, "no_speech_prob": 5.4845244790158176e-08}, {"id": 1036, "seek": 694000, "start": 6958.32, "end": 6964.6, "text": " of our data it's of size of size shape 64 by 2 because there are 64 items in the mini", "tokens": [295, 527, 1412, 309, 311, 295, 2744, 295, 2744, 3909, 12145, 538, 568, 570, 456, 366, 12145, 4754, 294, 264, 8382], "temperature": 0.0, "avg_logprob": -0.13027824572662808, "compression_ratio": 1.623456790123457, "no_speech_prob": 5.4845244790158176e-08}, {"id": 1037, "seek": 696460, "start": 6964.6, "end": 6972.360000000001, "text": " batch and each one has this is the independent variables so it's got the user ID and the", "tokens": [15245, 293, 1184, 472, 575, 341, 307, 264, 6695, 9102, 370, 309, 311, 658, 264, 4195, 7348, 293, 264], "temperature": 0.0, "avg_logprob": -0.1139625516431085, "compression_ratio": 1.5056818181818181, "no_speech_prob": 6.78671483456128e-07}, {"id": 1038, "seek": 696460, "start": 6972.360000000001, "end": 6984.400000000001, "text": " movie ID and to deep neural network based models for collaborative filtering work better", "tokens": [3169, 7348, 293, 281, 2452, 18161, 3209, 2361, 5245, 337, 16555, 30822, 589, 1101], "temperature": 0.0, "avg_logprob": -0.1139625516431085, "compression_ratio": 1.5056818181818181, "no_speech_prob": 6.78671483456128e-07}, {"id": 1039, "seek": 696460, "start": 6984.400000000001, "end": 6994.320000000001, "text": " than more traditional approaches like SVD or other matrix let's wait until we get there", "tokens": [813, 544, 5164, 11587, 411, 31910, 35, 420, 661, 8141, 718, 311, 1699, 1826, 321, 483, 456], "temperature": 0.0, "avg_logprob": -0.1139625516431085, "compression_ratio": 1.5056818181818181, "no_speech_prob": 6.78671483456128e-07}, {"id": 1040, "seek": 699432, "start": 6994.32, "end": 7007.24, "text": " so is X right so here is one user ID movie ID combination okay and then for each one", "tokens": [370, 307, 1783, 558, 370, 510, 307, 472, 4195, 7348, 3169, 7348, 6562, 1392, 293, 550, 337, 1184, 472], "temperature": 0.0, "avg_logprob": -0.13373649952023528, "compression_ratio": 1.3129770992366412, "no_speech_prob": 6.375542511705135e-07}, {"id": 1041, "seek": 699432, "start": 7007.24, "end": 7020.4, "text": " of those 64 here are the ratings so now we've created a dot product module from scratch", "tokens": [295, 729, 12145, 510, 366, 264, 24603, 370, 586, 321, 600, 2942, 257, 5893, 1674, 10088, 490, 8459], "temperature": 0.0, "avg_logprob": -0.13373649952023528, "compression_ratio": 1.3129770992366412, "no_speech_prob": 6.375542511705135e-07}, {"id": 1042, "seek": 702040, "start": 7020.4, "end": 7025.96, "text": " so we can instantiate it passing in the number of users the number of movies and let's use", "tokens": [370, 321, 393, 9836, 13024, 309, 8437, 294, 264, 1230, 295, 5022, 264, 1230, 295, 6233, 293, 718, 311, 764], "temperature": 0.0, "avg_logprob": -0.1019438046675462, "compression_ratio": 1.8865546218487395, "no_speech_prob": 1.248265107278712e-06}, {"id": 1043, "seek": 702040, "start": 7025.96, "end": 7031.16, "text": " 50 factors and now we can create a learner now this time we're not creating a CNN learner", "tokens": [2625, 6771, 293, 586, 321, 393, 1884, 257, 33347, 586, 341, 565, 321, 434, 406, 4084, 257, 24859, 33347], "temperature": 0.0, "avg_logprob": -0.1019438046675462, "compression_ratio": 1.8865546218487395, "no_speech_prob": 1.248265107278712e-06}, {"id": 1044, "seek": 702040, "start": 7031.16, "end": 7036.2, "text": " or a specific application learner it's just a totally generic learner so this is a learner", "tokens": [420, 257, 2685, 3861, 33347, 309, 311, 445, 257, 3879, 19577, 33347, 370, 341, 307, 257, 33347], "temperature": 0.0, "avg_logprob": -0.1019438046675462, "compression_ratio": 1.8865546218487395, "no_speech_prob": 1.248265107278712e-06}, {"id": 1045, "seek": 702040, "start": 7036.2, "end": 7040.08, "text": " that doesn't really know how to do anything clever it just stores away the data you give", "tokens": [300, 1177, 380, 534, 458, 577, 281, 360, 1340, 13494, 309, 445, 9512, 1314, 264, 1412, 291, 976], "temperature": 0.0, "avg_logprob": -0.1019438046675462, "compression_ratio": 1.8865546218487395, "no_speech_prob": 1.248265107278712e-06}, {"id": 1046, "seek": 702040, "start": 7040.08, "end": 7045.32, "text": " it and the model you give it and so when we're not using an application specific learner", "tokens": [309, 293, 264, 2316, 291, 976, 309, 293, 370, 562, 321, 434, 406, 1228, 364, 3861, 2685, 33347], "temperature": 0.0, "avg_logprob": -0.1019438046675462, "compression_ratio": 1.8865546218487395, "no_speech_prob": 1.248265107278712e-06}, {"id": 1047, "seek": 704532, "start": 7045.32, "end": 7053.639999999999, "text": " it doesn't know what loss function to use so we'll tell it to use MSE and fit and that's", "tokens": [309, 1177, 380, 458, 437, 4470, 2445, 281, 764, 370, 321, 603, 980, 309, 281, 764, 376, 5879, 293, 3318, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.07567946873013935, "compression_ratio": 1.5340909090909092, "no_speech_prob": 1.770425086533578e-07}, {"id": 1048, "seek": 704532, "start": 7053.639999999999, "end": 7058.88, "text": " it right so we've just fitted our own collaborative filtering model where we literally created", "tokens": [309, 558, 370, 321, 600, 445, 26321, 527, 1065, 16555, 30822, 2316, 689, 321, 3736, 2942], "temperature": 0.0, "avg_logprob": -0.07567946873013935, "compression_ratio": 1.5340909090909092, "no_speech_prob": 1.770425086533578e-07}, {"id": 1049, "seek": 704532, "start": 7058.88, "end": 7069.679999999999, "text": " the entire architecture it's a pretty simple one from scratch so that's pretty amazing", "tokens": [264, 2302, 9482, 309, 311, 257, 1238, 2199, 472, 490, 8459, 370, 300, 311, 1238, 2243], "temperature": 0.0, "avg_logprob": -0.07567946873013935, "compression_ratio": 1.5340909090909092, "no_speech_prob": 1.770425086533578e-07}, {"id": 1050, "seek": 706968, "start": 7069.68, "end": 7075.8, "text": " now the results aren't great if you look at the movie lens data set benchmarks online", "tokens": [586, 264, 3542, 3212, 380, 869, 498, 291, 574, 412, 264, 3169, 6765, 1412, 992, 43751, 2950], "temperature": 0.0, "avg_logprob": -0.07537432745391247, "compression_ratio": 1.7, "no_speech_prob": 2.260308519907994e-06}, {"id": 1051, "seek": 706968, "start": 7075.8, "end": 7079.88, "text": " you'll see this is not actually a great result so one of the things we should do is take", "tokens": [291, 603, 536, 341, 307, 406, 767, 257, 869, 1874, 370, 472, 295, 264, 721, 321, 820, 360, 307, 747], "temperature": 0.0, "avg_logprob": -0.07537432745391247, "compression_ratio": 1.7, "no_speech_prob": 2.260308519907994e-06}, {"id": 1052, "seek": 706968, "start": 7079.88, "end": 7084.92, "text": " advantage of the tip we just mentioned earlier in this lesson which is when you're doing", "tokens": [5002, 295, 264, 4125, 321, 445, 2835, 3071, 294, 341, 6898, 597, 307, 562, 291, 434, 884], "temperature": 0.0, "avg_logprob": -0.07537432745391247, "compression_ratio": 1.7, "no_speech_prob": 2.260308519907994e-06}, {"id": 1053, "seek": 706968, "start": 7084.92, "end": 7089.64, "text": " regression which we are here right the number between one and five is like a continuous", "tokens": [24590, 597, 321, 366, 510, 558, 264, 1230, 1296, 472, 293, 1732, 307, 411, 257, 10957], "temperature": 0.0, "avg_logprob": -0.07537432745391247, "compression_ratio": 1.7, "no_speech_prob": 2.260308519907994e-06}, {"id": 1054, "seek": 706968, "start": 7089.64, "end": 7096.16, "text": " value we're trying to get as close to it as possible we should tell fast AI what the range", "tokens": [2158, 321, 434, 1382, 281, 483, 382, 1998, 281, 309, 382, 1944, 321, 820, 980, 2370, 7318, 437, 264, 3613], "temperature": 0.0, "avg_logprob": -0.07537432745391247, "compression_ratio": 1.7, "no_speech_prob": 2.260308519907994e-06}, {"id": 1055, "seek": 709616, "start": 7096.16, "end": 7104.96, "text": " is so we can use y range as before so here's exactly the same thing we've got a y range", "tokens": [307, 370, 321, 393, 764, 288, 3613, 382, 949, 370, 510, 311, 2293, 264, 912, 551, 321, 600, 658, 257, 288, 3613], "temperature": 0.0, "avg_logprob": -0.14349775314331054, "compression_ratio": 1.664516129032258, "no_speech_prob": 1.505698151049728e-06}, {"id": 1056, "seek": 709616, "start": 7104.96, "end": 7112.62, "text": " we've stored it away and then at the end we use as we discussed sigmoid range passing", "tokens": [321, 600, 12187, 309, 1314, 293, 550, 412, 264, 917, 321, 764, 382, 321, 7152, 4556, 3280, 327, 3613, 8437], "temperature": 0.0, "avg_logprob": -0.14349775314331054, "compression_ratio": 1.664516129032258, "no_speech_prob": 1.505698151049728e-06}, {"id": 1057, "seek": 709616, "start": 7112.62, "end": 7117.96, "text": " in and look here we pass in star self dot y range that's going to pass in by default", "tokens": [294, 293, 574, 510, 321, 1320, 294, 3543, 2698, 5893, 288, 3613, 300, 311, 516, 281, 1320, 294, 538, 7576], "temperature": 0.0, "avg_logprob": -0.14349775314331054, "compression_ratio": 1.664516129032258, "no_speech_prob": 1.505698151049728e-06}, {"id": 1058, "seek": 711796, "start": 7117.96, "end": 7133.12, "text": " zero comma five point five and so we can see not really any better it's worth a try normally", "tokens": [4018, 22117, 1732, 935, 1732, 293, 370, 321, 393, 536, 406, 534, 604, 1101, 309, 311, 3163, 257, 853, 5646], "temperature": 0.0, "avg_logprob": -0.1269771946010305, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.6536854445803328e-06}, {"id": 1059, "seek": 711796, "start": 7133.12, "end": 7140.12, "text": " this is a little bit better but it always depends on when you run it I just run it a", "tokens": [341, 307, 257, 707, 857, 1101, 457, 309, 1009, 5946, 322, 562, 291, 1190, 309, 286, 445, 1190, 309, 257], "temperature": 0.0, "avg_logprob": -0.1269771946010305, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.6536854445803328e-06}, {"id": 1060, "seek": 711796, "start": 7140.12, "end": 7147.24, "text": " second time while it's we're looking um now there is something else we can do though which", "tokens": [1150, 565, 1339, 309, 311, 321, 434, 1237, 1105, 586, 456, 307, 746, 1646, 321, 393, 360, 1673, 597], "temperature": 0.0, "avg_logprob": -0.1269771946010305, "compression_ratio": 1.558139534883721, "no_speech_prob": 1.6536854445803328e-06}, {"id": 1061, "seek": 714724, "start": 7147.24, "end": 7158.5599999999995, "text": " is that if we look back at our little excel version the thing is here when we multiply", "tokens": [307, 300, 498, 321, 574, 646, 412, 527, 707, 24015, 3037, 264, 551, 307, 510, 562, 321, 12972], "temperature": 0.0, "avg_logprob": -0.07286932468414306, "compression_ratio": 1.6071428571428572, "no_speech_prob": 2.4198834580602124e-07}, {"id": 1062, "seek": 714724, "start": 7158.5599999999995, "end": 7165.2, "text": " you know these latent factors by these latent factors and add them up it's not really taking", "tokens": [291, 458, 613, 48994, 6771, 538, 613, 48994, 6771, 293, 909, 552, 493, 309, 311, 406, 534, 1940], "temperature": 0.0, "avg_logprob": -0.07286932468414306, "compression_ratio": 1.6071428571428572, "no_speech_prob": 2.4198834580602124e-07}, {"id": 1063, "seek": 714724, "start": 7165.2, "end": 7173.82, "text": " account of the fact that this user may just rate movies really badly in general regardless", "tokens": [2696, 295, 264, 1186, 300, 341, 4195, 815, 445, 3314, 6233, 534, 13425, 294, 2674, 10060], "temperature": 0.0, "avg_logprob": -0.07286932468414306, "compression_ratio": 1.6071428571428572, "no_speech_prob": 2.4198834580602124e-07}, {"id": 1064, "seek": 717382, "start": 7173.82, "end": 7180.4, "text": " of what kind of movie they are and this movie might be just a great movie in general just", "tokens": [295, 437, 733, 295, 3169, 436, 366, 293, 341, 3169, 1062, 312, 445, 257, 869, 3169, 294, 2674, 445], "temperature": 0.0, "avg_logprob": -0.0877054856747997, "compression_ratio": 1.9022222222222223, "no_speech_prob": 1.826624327350146e-07}, {"id": 1065, "seek": 717382, "start": 7180.4, "end": 7185.24, "text": " everybody likes it regardless of what kind of stuff they like and so it'd be nice to", "tokens": [2201, 5902, 309, 10060, 295, 437, 733, 295, 1507, 436, 411, 293, 370, 309, 1116, 312, 1481, 281], "temperature": 0.0, "avg_logprob": -0.0877054856747997, "compression_ratio": 1.9022222222222223, "no_speech_prob": 1.826624327350146e-07}, {"id": 1066, "seek": 717382, "start": 7185.24, "end": 7189.28, "text": " be able to represent this directly and we can do that using something we've already", "tokens": [312, 1075, 281, 2906, 341, 3838, 293, 321, 393, 360, 300, 1228, 746, 321, 600, 1217], "temperature": 0.0, "avg_logprob": -0.0877054856747997, "compression_ratio": 1.9022222222222223, "no_speech_prob": 1.826624327350146e-07}, {"id": 1067, "seek": 717382, "start": 7189.28, "end": 7195.08, "text": " learned about which is bias we could have another single number for each movie which", "tokens": [3264, 466, 597, 307, 12577, 321, 727, 362, 1071, 2167, 1230, 337, 1184, 3169, 597], "temperature": 0.0, "avg_logprob": -0.0877054856747997, "compression_ratio": 1.9022222222222223, "no_speech_prob": 1.826624327350146e-07}, {"id": 1068, "seek": 717382, "start": 7195.08, "end": 7201.5599999999995, "text": " we just add and it's another single number for each user which we just add right and", "tokens": [321, 445, 909, 293, 309, 311, 1071, 2167, 1230, 337, 1184, 4195, 597, 321, 445, 909, 558, 293], "temperature": 0.0, "avg_logprob": -0.0877054856747997, "compression_ratio": 1.9022222222222223, "no_speech_prob": 1.826624327350146e-07}, {"id": 1069, "seek": 720156, "start": 7201.56, "end": 7204.96, "text": " we've already seen this for linear models you know this idea that it's nice to be able", "tokens": [321, 600, 1217, 1612, 341, 337, 8213, 5245, 291, 458, 341, 1558, 300, 309, 311, 1481, 281, 312, 1075], "temperature": 0.0, "avg_logprob": -0.07868564539942248, "compression_ratio": 1.8619246861924685, "no_speech_prob": 2.0061541761151602e-07}, {"id": 1070, "seek": 720156, "start": 7204.96, "end": 7214.52, "text": " to add a bias value so let's do that so that means that we're going to need another embedding", "tokens": [281, 909, 257, 12577, 2158, 370, 718, 311, 360, 300, 370, 300, 1355, 300, 321, 434, 516, 281, 643, 1071, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.07868564539942248, "compression_ratio": 1.8619246861924685, "no_speech_prob": 2.0061541761151602e-07}, {"id": 1071, "seek": 720156, "start": 7214.52, "end": 7220.280000000001, "text": " for each user which is a size one it's just a single number we're going to add so in other", "tokens": [337, 1184, 4195, 597, 307, 257, 2744, 472, 309, 311, 445, 257, 2167, 1230, 321, 434, 516, 281, 909, 370, 294, 661], "temperature": 0.0, "avg_logprob": -0.07868564539942248, "compression_ratio": 1.8619246861924685, "no_speech_prob": 2.0061541761151602e-07}, {"id": 1072, "seek": 720156, "start": 7220.280000000001, "end": 7225.64, "text": " words it's just an array lookup but remember to do an array lookup that we can kind of", "tokens": [2283, 309, 311, 445, 364, 10225, 574, 1010, 457, 1604, 281, 360, 364, 10225, 574, 1010, 300, 321, 393, 733, 295], "temperature": 0.0, "avg_logprob": -0.07868564539942248, "compression_ratio": 1.8619246861924685, "no_speech_prob": 2.0061541761151602e-07}, {"id": 1073, "seek": 720156, "start": 7225.64, "end": 7231.52, "text": " take a gradient of we have to say embedding so we do the same thing for movie bias and", "tokens": [747, 257, 16235, 295, 321, 362, 281, 584, 12240, 3584, 370, 321, 360, 264, 912, 551, 337, 3169, 12577, 293], "temperature": 0.0, "avg_logprob": -0.07868564539942248, "compression_ratio": 1.8619246861924685, "no_speech_prob": 2.0061541761151602e-07}, {"id": 1074, "seek": 723152, "start": 7231.52, "end": 7238.120000000001, "text": " so then all of this is identical as before and we just add this one extra line which", "tokens": [370, 550, 439, 295, 341, 307, 14800, 382, 949, 293, 321, 445, 909, 341, 472, 2857, 1622, 597], "temperature": 0.0, "avg_logprob": -0.12384939193725586, "compression_ratio": 1.4583333333333333, "no_speech_prob": 3.747989012481412e-07}, {"id": 1075, "seek": 723152, "start": 7238.120000000001, "end": 7249.92, "text": " is to add the user and movie bias values and so let's train that see how it goes well that", "tokens": [307, 281, 909, 264, 4195, 293, 3169, 12577, 4190, 293, 370, 718, 311, 3847, 300, 536, 577, 309, 1709, 731, 300], "temperature": 0.0, "avg_logprob": -0.12384939193725586, "compression_ratio": 1.4583333333333333, "no_speech_prob": 3.747989012481412e-07}, {"id": 1076, "seek": 724992, "start": 7249.92, "end": 7263.8, "text": " was a shame it got worse so we used to have that finished here 0.87 0.88 0.89 so it's a", "tokens": [390, 257, 10069, 309, 658, 5324, 370, 321, 1143, 281, 362, 300, 4335, 510, 1958, 13, 23853, 1958, 13, 16919, 1958, 13, 21115, 370, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.16972163871482568, "compression_ratio": 1.3968253968253967, "no_speech_prob": 6.681502782157622e-07}, {"id": 1077, "seek": 724992, "start": 7263.8, "end": 7272.2, "text": " little bit worse why is that well if you look earlier on it was quite better it was 0.86", "tokens": [707, 857, 5324, 983, 307, 300, 731, 498, 291, 574, 3071, 322, 309, 390, 1596, 1101, 309, 390, 1958, 13, 22193], "temperature": 0.0, "avg_logprob": -0.16972163871482568, "compression_ratio": 1.3968253968253967, "no_speech_prob": 6.681502782157622e-07}, {"id": 1078, "seek": 727220, "start": 7272.2, "end": 7280.36, "text": " so it's it's overfitting very quickly and so what we need to do is we need to find a", "tokens": [370, 309, 311, 309, 311, 670, 69, 2414, 588, 2661, 293, 370, 437, 321, 643, 281, 360, 307, 321, 643, 281, 915, 257], "temperature": 0.0, "avg_logprob": -0.08122996342034987, "compression_ratio": 1.7227722772277227, "no_speech_prob": 2.7420836090641387e-07}, {"id": 1079, "seek": 727220, "start": 7280.36, "end": 7287.0, "text": " way that we can train more epochs without overfitting now we've already learned about", "tokens": [636, 300, 321, 393, 3847, 544, 30992, 28346, 1553, 670, 69, 2414, 586, 321, 600, 1217, 3264, 466], "temperature": 0.0, "avg_logprob": -0.08122996342034987, "compression_ratio": 1.7227722772277227, "no_speech_prob": 2.7420836090641387e-07}, {"id": 1080, "seek": 727220, "start": 7287.0, "end": 7291.84, "text": " data augmentation right like rotating images and changing their brightness and color and", "tokens": [1412, 14501, 19631, 558, 411, 19627, 5267, 293, 4473, 641, 21367, 293, 2017, 293], "temperature": 0.0, "avg_logprob": -0.08122996342034987, "compression_ratio": 1.7227722772277227, "no_speech_prob": 2.7420836090641387e-07}, {"id": 1081, "seek": 727220, "start": 7291.84, "end": 7297.76, "text": " stuff but it's not obvious how we would do data augmentation for collaborative filtering", "tokens": [1507, 457, 309, 311, 406, 6322, 577, 321, 576, 360, 1412, 14501, 19631, 337, 16555, 30822], "temperature": 0.0, "avg_logprob": -0.08122996342034987, "compression_ratio": 1.7227722772277227, "no_speech_prob": 2.7420836090641387e-07}, {"id": 1082, "seek": 729776, "start": 7297.76, "end": 7306.52, "text": " right so how are we going to make it so that we can train lots of epochs without overfitting", "tokens": [558, 370, 577, 366, 321, 516, 281, 652, 309, 370, 300, 321, 393, 3847, 3195, 295, 30992, 28346, 1553, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.0650096926195868, "compression_ratio": 1.7740384615384615, "no_speech_prob": 3.1381287044496275e-06}, {"id": 1083, "seek": 729776, "start": 7306.52, "end": 7311.38, "text": " and to do that we're going to have to use something called regularization and regularization", "tokens": [293, 281, 360, 300, 321, 434, 516, 281, 362, 281, 764, 746, 1219, 3890, 2144, 293, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.0650096926195868, "compression_ratio": 1.7740384615384615, "no_speech_prob": 3.1381287044496275e-06}, {"id": 1084, "seek": 729776, "start": 7311.38, "end": 7317.56, "text": " is a set of techniques which basically allow us to use models with lots of parameters and", "tokens": [307, 257, 992, 295, 7512, 597, 1936, 2089, 505, 281, 764, 5245, 365, 3195, 295, 9834, 293], "temperature": 0.0, "avg_logprob": -0.0650096926195868, "compression_ratio": 1.7740384615384615, "no_speech_prob": 3.1381287044496275e-06}, {"id": 1085, "seek": 729776, "start": 7317.56, "end": 7324.6, "text": " train them for a long period of time but penalize them effectively for overfitting or in some", "tokens": [3847, 552, 337, 257, 938, 2896, 295, 565, 457, 13661, 1125, 552, 8659, 337, 670, 69, 2414, 420, 294, 512], "temperature": 0.0, "avg_logprob": -0.0650096926195868, "compression_ratio": 1.7740384615384615, "no_speech_prob": 3.1381287044496275e-06}, {"id": 1086, "seek": 732460, "start": 7324.6, "end": 7333.240000000001, "text": " way cause them to try to stop overfitting and so that is what we will look at next week", "tokens": [636, 3082, 552, 281, 853, 281, 1590, 670, 69, 2414, 293, 370, 300, 307, 437, 321, 486, 574, 412, 958, 1243], "temperature": 0.0, "avg_logprob": -0.0969509964897519, "compression_ratio": 1.6775700934579438, "no_speech_prob": 4.092882136319531e-06}, {"id": 1087, "seek": 732460, "start": 7333.240000000001, "end": 7338.76, "text": " okay well thanks everybody so there's a lot to take in there so please remember to practice", "tokens": [1392, 731, 3231, 2201, 370, 456, 311, 257, 688, 281, 747, 294, 456, 370, 1767, 1604, 281, 3124], "temperature": 0.0, "avg_logprob": -0.0969509964897519, "compression_ratio": 1.6775700934579438, "no_speech_prob": 4.092882136319531e-06}, {"id": 1088, "seek": 732460, "start": 7338.76, "end": 7346.14, "text": " to experiment to listen to the lessons again because you know for the next couple of lessons", "tokens": [281, 5120, 281, 2140, 281, 264, 8820, 797, 570, 291, 458, 337, 264, 958, 1916, 295, 8820], "temperature": 0.0, "avg_logprob": -0.0969509964897519, "compression_ratio": 1.6775700934579438, "no_speech_prob": 4.092882136319531e-06}, {"id": 1089, "seek": 732460, "start": 7346.14, "end": 7350.52, "text": " things are going to really quickly build on top of all the stuff that we've learned so", "tokens": [721, 366, 516, 281, 534, 2661, 1322, 322, 1192, 295, 439, 264, 1507, 300, 321, 600, 3264, 370], "temperature": 0.0, "avg_logprob": -0.0969509964897519, "compression_ratio": 1.6775700934579438, "no_speech_prob": 4.092882136319531e-06}, {"id": 1090, "seek": 735052, "start": 7350.52, "end": 7355.88, "text": " please be as comfortable as it with it as you can feel free to go back and re-listen", "tokens": [1767, 312, 382, 4619, 382, 309, 365, 309, 382, 291, 393, 841, 1737, 281, 352, 646, 293, 319, 12, 75, 4821], "temperature": 0.0, "avg_logprob": -0.11491013264310533, "compression_ratio": 1.6149425287356323, "no_speech_prob": 1.1842013918794692e-05}, {"id": 1091, "seek": 735052, "start": 7355.88, "end": 7361.0, "text": " and go through and follow through the notebooks and then try to recreate as much of them yourself", "tokens": [293, 352, 807, 293, 1524, 807, 264, 43782, 293, 550, 853, 281, 25833, 382, 709, 295, 552, 1803], "temperature": 0.0, "avg_logprob": -0.11491013264310533, "compression_ratio": 1.6149425287356323, "no_speech_prob": 1.1842013918794692e-05}, {"id": 1092, "seek": 735052, "start": 7361.0, "end": 7365.280000000001, "text": " thanks everybody and I will see you next week or see you in the next lesson whenever you", "tokens": [3231, 2201, 293, 286, 486, 536, 291, 958, 1243, 420, 536, 291, 294, 264, 958, 6898, 5699, 291], "temperature": 0.0, "avg_logprob": -0.11491013264310533, "compression_ratio": 1.6149425287356323, "no_speech_prob": 1.1842013918794692e-05}, {"id": 1093, "seek": 736528, "start": 7365.28, "end": 7381.28, "text": " watch it.", "tokens": [50364, 1159, 309, 13, 51164], "temperature": 1.0, "avg_logprob": -1.1383249759674072, "compression_ratio": 0.5294117647058824, "no_speech_prob": 0.00048235038411803544}], "language": "en"}