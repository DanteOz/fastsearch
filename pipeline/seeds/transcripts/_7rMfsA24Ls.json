{"text": " Hi, everybody, and welcome to Deep Learning Foundations to Stable Diffusion. Hopefully, it's not too confusing that this is described here as lesson nine. That's because, strictly speaking, we treat this as part two of the Practical Deep Learning for Coders series. So that part one had eight lessons, so this is lesson nine. But don't worry, you didn't miss anything. It's the first lesson of part two, which is called Deep Learning Foundations to Stable Diffusion. And maybe rather than calling it Practical Deep Learning for Coders, we should call this Impractical Deep Learning for Coders in the sense that we are certainly not going to be spending all of our time seeing exactly how to do important things with deep learning. But we'll be doing a whole lot of fun things, generative model-y fun things, and also a whole lot of understanding, lots of details, which you won't necessarily need to know to use this stuff. But if you want to become a researcher, or if you want to put something in production, which has got some kind of complex customization requirements, stuff like that, then it is going to be very helpful to learn the details we'll be talking about. So here in lesson nine, there's kind of going to be two parts to it. One is just a quick run-through, quick-ish run-through of using stable diffusion, because we're all dying to play with it. And then the other thing that I'll be doing is describing in some detail what's going on. How is it working? There will be a whole lot of hand-waving either way, because it's going to take us a few lessons to describe everything from scratch. But hopefully you'll feel like you'll come away with this lesson with a reasonable intuitive understanding at least of how this is all working. Assumptions. Well, I'm going to try to explain everything like everything. I'm going to try to explain everything. So if you haven't done deep learning before, this is going to be very hard. But I will at least be trying to say this is roughly what's going on and where you can find out more. Having said that, I would strongly suggest doing part one before doing this course, unless you really want to throw yourself in the deep end and give yourself quite the test. If you haven't done part one of Practical Deep Learning for Coders, but you're reasonably comfortable with deep learning basics, you could write a basic SGD loop in Python. And you know how to use, ideally, PyTorch, but TensorFlow is probably OK as well. And you kind of know the basic ideas of how to create, you know, what an embedding is, and you could create one of those in Scratch, stuff like that. You know, you'll probably be fine. Generally speaking for these courses, I find most people tend to watch the videos a few times and often the second time through folks will like pause and look things up they don't know and check things out. Generally speaking, you know, we expect people to be spending about 10 hours of work on each video. Having said that, some people spend a hell of a lot more and go very deep. Some people will spend a whole year, you know, sabbatical studying Practical Deep Learning for Coders in order to really fully understand everything. So really it's up to you as to how deep you go. OK. So with that said, let's jump into it. And as I said, the first part, we're going to be playing around with stable diffusion. And I tried to prepare this as late as possible so it wouldn't be out of date. Unfortunately, as of 12 hours ago, it is now out of date. And this is one of the big issues with the bit I'm about to describe, which is like how to play with stable diffusion and exactly how do the details work, which is it's moving so quickly that all of the details I'm going to describe to you today and all of the software I'm going to show you today, by the time you watch this, if you're watching it, so what is it we're up to now? It's 11th of October. So if you're watching this in like December of 2022 or watching this in 2023, the details will have changed. So what's happened today in the last 24 hours is two papers have come out. So what I was going to be telling you today is, for example, to do a stable diffusion generative model, the number of steps required has gone down from a thousand to about 40 or 50. But then as of last night, papers just come out that saying it's now down to four and it's 256 times faster. And another paper has come out with a separate, I think, orthogonal approach, which makes it another, let's see, 10 to 20 times faster. So things are very exciting. Things are moving very quickly. Now having said that, don't worry, because after this lesson, we're going to be going from the foundations, which means we're going to be learning all the things of how these are built up. And those don't change much at all. And in fact, a lot of what we'll be seeing is extremely similar to another course we did in 2019, because the foundations don't change. And once you know the foundations, these kinds of details that you'll find in these papers, you'll be like, oh, I see, they did all these things the same way as usual and they made this little change. So that's why we do things from the foundations so that you can keep up with the research, do your own research by taking advantage of this foundational knowledge, which all these papers are building on top of. So anyway, I guess I should apologize that even as I record this, the notebook is now one day out of date. So in part one, you might remember we saw this stuff of Dali 2 illustrations of Twitter bios, which really are pretty cool. So the cool thing is that we're now at a point we can build this stuff ourselves and run this stuff ourselves. We won't actually be using this particular model. Dali 2 will be using a different model, stable diffusion, but has very similar outputs. But we can go even further now. So one of our wonderful alumni, Alon, actually recently started a new company called, I don't know who said it, Strummer, where you can use something that we'll be learning about today, called Dreambooth, to put any object, person, whatever, into an image. And so he was kind enough to do a quick Dreambooth run for me and added these various pictures of me using his service. So here's a fun service you can try. One crazy one he tried was me as a dwarf, which I've got to say actually worked pretty well. This half looks like me, I reckon. And then the bottom bit is the dwarf version. So thank you, Alon, and congratulations on your great progress since completing the FastAI course. I love it. So something that's a bit different about this compared to previous courses, a lot of previous courses, is this is no longer just a me thing. Because this is moving so quickly, I've needed to get a lot of help to even vaguely get up to date and stay up to date. So everything I'll be showing you today is very heavily influenced by extremely high levels of input from these amazing folks, all of whom are FastAI alumni. So Jonathan Whitaker, who I saw in our chat, was basically the first guy to create detailed educational material about stable diffusion and has been in the generative model space, well, for a long time by stable diffusion standards, I guess. So Waseem has been an extraordinary contributor to all things FastAI. Pedro came to San Francisco for the last time we did a part two course in 2019 and took what he learned there and made his amazing Camera Plus software dramatically better and had it highlighted by Apple for the extraordinary machine learning stuff added. And he's now at Hacking Face working on the software that we'll be using a lot, Diffuses. And then Tanishq, everybody in the FastAI community probably already knows, now at StabilityAI working on stable diffusion models, his expertise particularly is in medical applications. So really folks from all the key groups pretty much around stable diffusion and stuff are working on this together. And you'll also find some of these folks have recorded additional videos going into more detail about some of the areas which you will find on the course website. So make sure you go to course.fast.ai to get all the information about all the materials that you need to take full advantage of this. So every lesson has links to notebooks and to details and so forth. If you want to go even deeper, head over to forums.fast.ai into the part two 2022 category, hit on the About the Course button and you'll find that every lesson there's a chat with even more stuff. So look at this carefully to see all the things that me and the community have provided to you to help you understand this video. And also check out the questions underneath and answers underneath to see what people have talked about. They can get a bit overwhelming. So once they get big enough, you'll see that there's a summarize button that you can click to kind of see just the most liked parts. So that can be very helpful. So they're all important resources, I think, to get the most out of this course. Now, compute. So completing part two requires quite a bit more compute than part one. Compute options are changing rapidly. And to be honest, the main reason for that is because of the huge popularity of stable diffusion. Everybody is taken to using Colab for stable diffusion. And Colab's response has been to start charging by the hour for most usage. So you may well find if you're a Colab user, we still love Colab, you may find that you run out of... They start not giving you decent GPUs anymore. And if you want to then upgrade, they limit quite a lot how many hours you can use. So at the moment, yeah, still try Colab. They're pretty good. I mean, for free, you get some decent stuff. But I would strongly suggest trying out also Paper Space Gradient. You can pay like $9 a month to actually get some pretty good GPUs there at the moment, or pay a bit more to get even better ones. Again, but the thing is, this is all going to change a lot. I don't know, maybe people will make Paper Space Gradient and have to change their pricing too. I don't know. So check course.fast.ai to find out what our current recommendations are. Now Lambda Labs and Jarvis Labs are also both good options. Jarvis was created by an alum of the course and has some just really fantastic options at a very reasonable price. And a lot of Fast.ai students use them and love them. And also check out Lambda Labs, who are the most recent provider on this page. And they are rapidly adding new features. But the reason I particularly wanted to mention them is at least as I say this, which they say is early October 2022, they're the cheapest provider of kind of big GPUs that you might want to use to run like serious models. So they're absolutely well worth checking out. But as I say, this could all have changed by the time you watch this. So go and check out course.fast.ai. Also at the moment, late 2022, GPU prices have come down a lot and you may well want to consider buying your own machine at this point. Okay, so what we're now going to do is jump into the notebooks. And so there's a repo that we've linked to called Diffusion NBs, which isn't kind of the main course notebooks. It's not the From the Foundations notebooks. It's just a couple of notebooks that you might want to play with. A bit of fun stuff to try out. One of the interesting things here is Jonathan Wittaker, who I tend to call Jono. So if I say Jono, that's who I'm referring to, has created this really interesting thing called SuggestedTools.md, which hopefully he'll keep up to date. So even if you come here later, this will still be up to date. Because he knows so much about this area, he's been able to pull out some of the best stuff out there for just starting to play. And I think it's actually important to play, because that way you can really understand what the capabilities are and what the constraints are. So then you can think about, well, what could you do with that? And also, what kind of research opportunities might there be? So I'd strongly suggest trying out these things. The community on the whole has moved towards making things available as Colab notebooks. So if I click, for example, on this one, deforum, and they often have this kind of hacker aesthetic around them, which is kind of fun. So what happens is they add lots and lots of features, and you can basically just fill in this stuff to try things. And they often have a few examples. And so you can hit up the runtime and say change runtime type to make sure it says GPU. And you can say what kind of GPU. And start running things. Now, a lot of the folks who use this stuff honestly have no idea what any of these things mean. Now, by the end of the course, you'll know what all of these things mean, pretty much. And that will help you to make great outputs from stuff like this. But you can create great outputs just using more of an artisanal approach. There's lots of information online about what kinds of things could you try. So anyway, check out this stuff from Jono. And then he also links to this fantastic resource from Pharma Psychotic, which is a rather overwhelming list of things to play with. Now, again, maybe by the time you watch this, this is all changed. But I just wanted to know these kind of things are out there. And they're basically ready to go applications that you can start playing with. So play a lot. What you'll find is that most of them, at least at the moment, expect you to input some text to say what you want to create a picture of. It turns out that as we'll learn in detail why, the text you pick, it's not very easy to know what to write. And that gives kind of interesting results. At the moment, it's quite an artisanal thing to understand what to write. And the best way to learn what to write is called the prompt. The best way to learn about prompts is to look at other people's prompts and their outputs. So at the moment, perhaps the best way to do that is Lexica, which has lots and lots of really interesting artworks. And so AI artworks. And so you can click on one and see what prompt was used. And so you'll see here that generally you start with what do you want to make a picture of? What's the style? And then the trick is to add a bunch of like artists' names or places that they put art so that the algorithm will tend to create a piece which matches art, you know, that tends to have these kinds of words in their captions. So there's a really useful trick to kind of get good at this. And so you can even search for things. So I don't know if they have teddy bears. Let's try. There we go. So if there's a kind of like a, probably not that one. That's a pretty good teddy bear image. So you can kind of get some sense of how to create nice teddy bear images. That's so cute. I know what I'm going to be showing my daughter tomorrow. You can see they often tend to have similar kinds of stuff to try to encourage the algorithm to give good outputs. Okay, so by the end of this course, you'll understand why this is happening, why these kinds of out, you know, prompts create these kind of outputs and also how you can go beyond just creating prompts to actually building really innovative new things with new data types. Okay. So let's take a look at the diffusion NBs repo. The first thing we'll look at is stable diffusion. So a couple of options here. You can clone this repo, which is linked from both the course.fast.ai and from the forum and run it on like paper space gradient or your own machine or whatever. Or you can head over to Colab and you can just say GitHub, right? And then you can paste in the link to it directly from GitHub. Okay. So I'm running it on my own machine. And this notebook is largely been built thanks to the wonderful folks at Hugging Face. And Hugging Face have a library called Diffusers. So any of you that have done part one of the course will be very familiar with Hugging Face. We used a lot of their libraries in part one. Diffusers is their library for doing stable diffusion and stuff like stable diffusion. At the moment, these things are changing a lot, but at the moment, this is our recommended library for doing this stuff and it's what we'll be using in this course. Maybe by the time you watch this, there'll be lots of other options. So again, keep an eye on course.fast.ai. In general, Hugging Face have done a really good job of being at and staying at the kind of head of the pack around models in general for deep learning. So it would be not surprising if they continue to be the best option for quite a while. But the basic idea of any library is going to look pretty similar. So to get started playing with this, you will need to log in to Hugging Face. So if you've got a Hugging Face, you can create a username there and a password and then log in. Once you've done it once, it'll save it on your computer so you won't have to log in again. And the thing we're going to be working with is pipelines and in particular the stable diffusion pipeline. Again, you know, they might be using different pipelines by the time that you watch this. But the basic idea of pipeline is quite similar to what we call a learner in fast AI, which is it's got a whole bunch of things in it, you know, a bunch of kind of processing and models and inference all happening automatically. And just like you can save a pipeline in fast AI, sorry, save a learner in fast AI, you can save a pipeline in diffusers. Now something that you can do in all pretty much all Hugging Face libraries that you can't do in fast AI is you can then save a pipeline or whatever back up into the cloud onto Hugging Face, they call it the hub. And so then if we say from pre-trained, it's a lot like how we create pre-trained learners in fast AI. But the thing you put here is actually, if it's not a local path, it's a Hugging Face repo. So if we search Hugging Face for this, then you can see this is what it's going to download. And you can actually save your own pipelines up to the hub for other people to use. So I think this is a very nice feature that helps, you know, the community build stuff. So this is actually going to, the first time you run this, it's going to download many gigabytes of data from the internet. This is one of the slight challenges with using this on Colab is every time you use Colab, everything gets thrown away and start from scratch. So it'll all have to be downloaded every time you use Colab. If you use something like Paperspace or particularly actually Lambda Labs, it's all going to be saved for you. So once you've downloaded all this, it's going to save a whole bunch of stuff into your.cache in your home directory. So that's where Hugging Face puts things. So now that we have a pipeline called pipe, we can now treat it as if it's a function. It's pretty common for like PyTorch-y stuff and fast AI-y stuff. You should be very familiar with this, hopefully. And you can pass it a prompt. And so this is just some text. And that's going to return some images. Since we're only passing one prompt, it's going to return one image. So we'll just index into.images. And when we run it, it takes maybe 30 seconds or so and returns a photograph of an astronaut riding a horse. Every time you call a pipeline using the same random seed, you'll get the same image. You can set the random seed manually. And so you could send to somebody else and say, oh, this is a really cool astronaut riding a horse I found. Try manual seed 1024. And you'll get back this particular astronaut riding a horse. So that's the most basic way to get started running on Colab or on your own machine. You can start creating images. It takes, as I said, it takes 30 seconds or so. And in this case, it took 51 steps. What it's doing, this is a bit very different to what we're used to with inference in fast AI, where it's one step to classify something, for example. What it's doing in these 51 steps is it's starting with like, so this is actually an example that we're going to create ourselves in the course of creating handwritten digits. This is actually an image from a later notebook we'll be building. Well, it basically starts with random noise. And each step, it tries to make it slightly less noisy and slightly more like the thing we want. And so going down here is showing all the steps to create the first four, for example, or here to create the first one. And if you look closely, you can kind of see in this noise, there is something that looks a bit like a one, and so kind of decides to focus on that. And so that's how these diffusion models basically work. So remember, if you're having any trouble finding the materials we're looking at, to go to course.fast.ai or go to the forum topic to see all the links. And this one is called diffusion-nvs. And the notebook is called, you can see it at the top, stable diffusion. Now a question might be, well, why don't we just do it in one go? And we can do it in one go. But if we try to do it in one go, it doesn't do a very good job. These models aren't, as I speak now in October 2022, smart enough to do it in one go. Now, as I mentioned at the start, the fact that I'm doing it in 51 steps here is hopelessly out of date, because as of yesterday, apparently we can now do it in three to four steps. I'm not sure if that code's available yet. So by the time you see this, yeah, this might all be dramatically faster. But as I'll be describing, understanding this basic concept, I'm pretty confident it's going to be very important forever. So we'll talk about that. So if we do 16 steps instead of 51 steps, it looks a bit more like it, but it's still not amazing. OK. So that's how you can kind of get started. And I'll show you a few things that you can tune. And I should remind you that most of the stuff I'm showing you in this was built by Pedro Cuenca and the other folks at Hugging Face. So huge thanks to them. There's no way I could have been as up to speed with all this detail without their help. They built this library, Diffusers, and have done a fantastic job of helping display what you can do with it. So let's look at an example of what you can do with it. We're just going to quickly define a little function here to create a grid of images. The details don't matter. But what we do want to show here is you can take your prompt, which was an astronaut riding a horse, and just create four copies of it. OK. So times when applied to a list simply copies the list that many times. So here's a list of the exact same prompt four times. And then what we're going to do is we're going to pass to the pipeline the prompts. And we're going to use a different parameter now called guidance scale. We're going to be learning about guidance scale in detail later in the course. But basically what this does is it says to what degree should we be kind of focusing on the specific caption versus just creating an image. So we're going to try a few different guidance scales, about one, three, seven, 14, generally seven and a half, I believe, at this stage is the default. That might have changed by the time you watch this. And so each row here is a different guidance scale. So you can see in the first row, it hasn't really listened to us very much at all. These are very weird looking things, and none of them really look like astronauts riding a horse. At guidance scale of three, they look more like things riding horses that they might be astronautish. And at 7.5, they certainly on the whole look like astronauts riding a horse. And at 14 or 15, they certainly look like that, but getting a little bit too abstract sometimes. I have a pretty strong feeling there are some slight problems with actually how this is coded or actually how the algorithm works, which I will be looking at during this course. So maybe by the time you see this, some of these will be looking a bit better. I think basically something that's happening here is it's actually kind of over jumping a bit too far during these high ones. Anyway, so the basic idea of what it's doing here is this guidance is it's basically actually for every single prompt, it's creating two versions. One version of the image with the prompt, an astronaut riding a horse, and one version of the image with no prompt. So it's just some random thing. And then it takes the average basically of those two things. And that's what guidance scale does. And you can kind of think of the guidance scale as being a bit like a number that's used to weight the average. There's something very similar you can do where again you get the model to create two images, but rather than taking the average, you can ask it to effectively subtract one from the other. So here's something that Pedro did of using the prompt Elabrador in the style of a mirror. And then he said, well, what if we then subtract something which is just the model for the caption blue? And you can pass in this thing negative prompt to diffusers. And what that will do is it will take the prompt, which in this case is Elabrador in the style of Vermeer, and create a second image effectively which is just responding to the prompt blue, and effectively subtract one from the other. The details are slightly different to that, but that's the basic idea. And that way we get a non-blue Elabrador in the style of Vermeer. So yeah, this is the basic idea of how to use negative prompt, and you can play with that. Good fun. Here's something else you can play with, is you don't have to just pass in text. You can actually pass in images. So for this you'll need a different pipeline. You'll need an image to image pipeline. And with the image to image pipeline, you can grab a rather sketchy looking sketch. And you can then pass to this eye to eye, image to image pipeline, the initial image to start with. And basically what this is going to do is rather than starting diffusion process with random noise, it's going to basically start it with a noisy version of this drawing. And so then it's going to try to create something that matches this caption, and also like, follows this kind of guiding starting point. And so as a result, you get things that look quite a lot better than the original drawing. But you can see that the composition is the same. And so using this approach, you can construct things that match the particular kind of composition you're looking for. So I think that's quite a nifty approach. And so here this parameter strength is saying, to what degree do you want to really create something that looks like this? Or to what degree do you want the model to be able to try out different things a bit? Now here's where things get interesting. And this is the kind of stuff you're not going to be able to do at the moment with just the basic GUIs and stuff. But you can if you really know what you're doing. What we could do now is we could take these output images, and we could say, oh, this one's nice. Sorry, this one. This one's nice. Let's make this the initial image. And now we'll say, let's do an oil painting of by Van Gogh. And pass in the same thing here and a strength of one. And actually, that pretty much worked. And I think that's absolutely fascinating, right? Because this is something I haven't seen before, which Pedro put together this week. And it's combining simple Python code together. And so you can play with that. Something else you can do, which this one's actually example came from the folks at Lambda Labs is, and we won't be going into this in detail right now, because this is like basically exactly like what we've done a thousand times in fast AI, is you can take the models in that pipeline and you can pass it your own images and your own captions. And so what happened here is... Oh, I hate these things. Go away. Never mind. Oh, here we are. So what these folks did, I think this was Justin, if I remember correctly. So what Justin at Lambda did was he created a really cool dataset by going to grab a Pokemon dataset of images, which had almost a thousand images of Pokemon. And then this is really neat. He then used an image captioning model to automatically generate captions for each of those images. And then he fine-tuned the stable diffusion model using those image and caption pairs. So here's an example of one of the captions and one of the images. And then took that fine-tuned model and passed it prompts like girl with a pearl earring and cute Obama creature and got back these Totoro, these... Oopsie-daisy. And got back these super nifty images that now are reflecting the fine-tuning dataset that he used and also responding to these prompts. Here's another example of something you can do. Fine-tuning can take quite a bit of data and quite a bit of time, but you can actually do some special kinds of fine-tuning. One that you can do is called textual inversion, which is where we actually fine-tune just a single embedding. So for example, we can create a new embedding where we're trying to make things that look like this. So what we can do is we can give this concept a name. So here we're going to call it... Oh, I just lost it now. Watercolor. There we are. We're going to call it watercolor portrait. And so that's what the embedding name we're going to use is. And we can then basically add that token to the text model, and then we can train the embeddings for this so that they match the example pictures that we've seen. And this is going to be much faster because we're just training a single token for just in this case, four pictures. And so when we do that, we can then say, for example, woman reading in the style of, and then passing that token we just trained. And as you see, we'll get back a kind of novel image, which I think is pretty interesting. Another example, very similar to textual inversion, is something called Dream Booth, which as mentioned here, what it does is it takes an existing token, but one that isn't used much, like say SKS, nothing, almost nothing has SKS, and fine tunes a model to bring that token, as it says here, close to the images we provide. And so what Pedro did here was he grabbed some pictures of me and said, painting of SKS, so in this case, he's fine tuned this token to be Jeremy Howard photos in the style of Paul Siniaq. And there they are. And so the example I showed earlier of the draw off Jeremy Howard, that server, Streamr, is actually using this, Dream Booth. So here's how you can try that yourself. Okay. So that is part one of this lesson, which is the how to get started playing around with stable diffusion. In part two, we're going to talk about what's actually going on here from a machine learning point of view. So we'll come back in about seven minutes to talk about that. All right. See you guys in about seven minutes. Okay. Welcome back, folks. I just thought I'd share with you one more example, actually, of textual inversion training. This is my daughter's Teddy Tiny, who, as you can see, is grossly misnamed. And Petro and I tried to create a textual inversion version of Tiny. And I was trying to get Tiny riding a horse. And it's interesting that when I tried to do that, this top row here, this is actually Petro's example when he ran it. This is showing the kind of steps as he was training of trying to use the caption Tiny riding a horse. And as you can see, it never actually ended up generating Tiny riding a horse. Instead it ended up generating a horse that looks a little bit like Tiny. And then we're trying to get Tiny sitting on a pink rug. And actually, after a while, it did make some progress there. It doesn't quite look like Tiny. One thing Petro did that was different to me was he started with the embedding for person. In my one, I actually started with the embedding for Teddy, and it worked a bit better. But as you see, there are problems. And we'll understand where those problems come from as we talk more about how this is trained in the rest of this lesson. So I'm going to be relying on some understanding of the basic idea of how machine learning models are trained here. So if you start getting a bit lost at any point, you might want to go back to part one and then come back to this once you're unlost. The way we're going to start... Okay, so I need to explain. The way stable diffusion is normally explained is focused very much on a particular mathematical derivation. We've been developing a totally new way of thinking about stable diffusion. And I'm going to be teaching you that. It's mathematically equivalent to the approach which you'll see in other places, but what you'll realize and discover is that it's actually conceptually much simpler. And also later in this course, we'll be showing you some really innovative directions that this can take you when you think of it in this brand new way. So all of which is to say, when you listen to this and then you go and look at some blog post and it looks like I'm saying something different, just keep that in mind. I'm not saying something different. I'm expressing it in a different way, but it's equally mathematically valid. What I'm going to do is I'm going to start by saying, let's imagine that we were trying to get something to generate something much simpler, which is to generate handwritten digits. Okay, so it's like the stable diffusion for handwritten digits. And we're going to start by assuming there's some API, some web service or whatever out there. Who knows how it was made? But what it does is something pretty nifty, which is that you can get an image of a handwritten digit and you can pass it over into this web API, into this rest endpoint or whatever. It's just a black box as far as we're concerned. And it's going to spit out the probability that this thing you passed in is a handwritten digit. So for this one, so let's say this image is called x1. The probability that x1 is a handwritten digit, it might say is 0.98. And so then you pass something else into this magic API endpoint, which looks like this. You pass that in and that looks a little bit like an 8, I guess, but it might not be. You pass it into this API and you see what happens. This is x2. And it says the probability that x2 is a digit is 0.4. Okay, now we pass in our image x3 into our magic API and it returns the probability that x3 is a handwritten digit, pretty small. Okay, so why is this interesting? Well, it turns out that if you have a function, you know, let's not call this an API, let's call this, let's call this, it's called f, some function, but it's like behind some web API, REST endpoint, whatever. If you have this function, we can actually use it to generate handwritten digits. So that's something pretty magical. And we're going to see how on earth would you do that? If you have this function, which can take an image and tell you the probability that that is a handwritten digit, how could you use it to generate new images? Well, imagine you wanted to turn this mess into something that did look like an image. Here's something you could do. Let's say it's a 28 by 28 image, which is what, 786? Oopsie-dozy. 28 times 28, 784. So 794 pixels. And we could pick one of these pixels and say, what if I increase this pixel to be a little bit darker? And then we could pass that image through f and we could see what happens to the probability that it's a handwritten digit. So for a specific example, handwritten digits don't normally have any pixels that are black in the very bottom corners. So if we took this here and we said, what would happen if we made this a little bit lighter? And then we passed that exact image through here, the probability would probably go up a tiny bit, for example. So now we've got an image which is slightly more like a handwritten digit than before. And also in digits, generally there are straight lines. So this pixel here, it probably makes sense for it to be darker. So if we made a slightly darker version of this pixel and sent it through here, that would also increase the probability a little bit. And so we could do that for every single pixel of the 28 by 28, one at a time, finding out which ones, if we make them a little bit lighter, make it more like a handwritten digit, which ones if we make it a little bit darker, make it more like a handwritten digit. What we've just done is we've calculated the gradient of the probability that x3 is a handwritten digit with respect to the pixels of x3. Now notice that I didn't say dpx3 dx3, which you might be familiar with from high school. And the reason for that is that we've calculated this for every single pixel. And so when you do it for lots of different inputs, you have to turn the d into a, this is called a del or a nabla. And it just means there's lots of values here. So this here contains lots of values, which is the, how much does the probability that x3 is a digit increase as we increase this pixel value and as we increase this pixel value, as we increase this pixel value. So there's going to be, for 28 by 28 inputs, there's going to be 784 pixels, which means that this thing here has 784 values. Okay, I totally messed up the notation there. I did think about going back and re-recording it, but then I thought, well, maybe instead as penance for my failure to get the notation right, I should instead record a little section describing the notation in more detail, both for myself, so I don't make it a mistake again and for the rest of you, so you understand exactly what's going on. I think it's actually pretty worthwhile because this notation does come up a lot and I've been regularly butchering it in talks and notes for years now, so it's about time I got it right. So I should mention I have absolutely no excuse for butchering the notation like I have. On the basis that actually my friend Terrence and I wrote a, what is it, like 30, 40 page tutorial on matrix calculus, and in that paper, he actually described everything I'm going to show you here. Having said that, you certainly don't need to read this rather lengthy tutorial. I'm going to explain the key stuff that actually I think it's worth knowing, and then you'll understand the mistake that I made during the lesson. Maybe let's start with some reminders from stuff that hopefully you did at high school. So let's create maybe a 2D version here and maybe a 3D version as well. Okay, so let's say for example we've got a quadratic that looks something like this, and we can say this is an equation such as y equals x squared, and we might endeavor to identify the slope kind of at some exact moment like here. This slope at this exact moment here. So this thing is called the tangent, and the slope at a tangent is the derivative of a function. And so the derivative, let me just try to make this look a bit more like a y than an x. Maybe I'll write it like this. Okay, and so the derivative of a function we can write in a few ways, but one way we can write it is like this dy dx, and there are some rules we can use to calculate it analytically, and for squared the rule is that we it's 2x. You basically move the index out to the front to calculate its derivative. Another way of writing y equals x squared is we could write f of x equals x squared, and another way of writing the derivative is we could use this for example. Okay, so this is all stuff that hopefully you at least vaguely remember from high school. Now functions are not necessarily just of one variable, so here we've got x and y, but they could be of two variables, x and y and z, see. And so we could have functions which for example could be like a 3D parabola with this kind of curvature for instance. And so you can still find the derivative, but if you think about it there's one way to kind of get a derivative would be exactly what we did before, which would we say as we change x how does it change z, so that would be this slope again. But you could also have something that says as we change y how would we change z, that would be like rotating this whole thing around by 90 degrees and then kind of doing the same thing. So it's a little bit trickier now because we've got a function of x and y. And so we could calculate the derivative of that with respect to just one of these things or both of these things or whatever. And so what we do here is we write this little thing. And so we can then say this is how our output z changes as we change one thing at a time, in this case just x. There's another value which would be how does it change as we change just y one thing at a time. And so those are two separate numbers we could calculate at some particular point on this surface. And so these things are called partial derivatives, or just partials. So in our case we've got a 28 by 28 pixel image, which might be for example the number 7, made of 28 by 28 pixels. So the pixels would be something like this. And then down here. And so in our case we've got a situation where we're saying we've got some loss. And our loss is calculated at some function of both some weights in a neural network as well as some pixel values, such as the pixels in this number 7. And I guess you know actually the way it would work with these would be shaded. This is more like what MNIST looks like, right? All right. So there's my pixels, aren't they terrible. So the loss would be calculated more specifically as the MSE mean squared error of the actual answer of like which digit should it be, so let's call that y, minus the predicted y, which is some neural network with some weights and our pixels. So that would be like delving in one layer deeply. But none of these details really matter too much of like what's the loss function or the neural network or whatever. It's just some function that's calculating loss. So let's get rid of all that. And we can now say what happens as we change x. As we change x what happens to the loss? Because we want to change x in a way that the loss goes down. But there isn't just one x. There's, oh I wrote this as seven by seven. It's actually meant to be 28 by 28. But let's just do a simple seven by seven version here. So we've got seven pixels by seven pixels. This is a super low resolution 49 pixel image. So there's 49 different things we could change. We could make each of these pixels darker or lighter. All right so let's take for example pixel, maybe we can write it like this. We'll say pixel one comma one. We could write it like that for example. And so we could say what happens to the loss as we change pixel one comma one. So we can calculate a derivative right. And it's going to be a partial. Partial derivative. How does the loss change as we change pixel one comma one? So that would be a very useful thing to know because that would then tell us do we need to make pixel one comma one a bit brighter or a bit darker in order to improve the loss. And we could also calculate that for pixel one comma two, one comma three and so forth for all 49 pixels in this super low resolution digit. So okay so there's the first thing we can calculate. And then the second thing I mentioned could be loss over pixel one comma two. Okay so that's the slope as we change pixel one comma two. And I'm not going to write all of them but there could be, well there will be, 49 of these for each seven by seven. So rather than writing out all 49 of those it's nice to instead write them all at once and say and you can do that like so. You can say upside down triangle x loss and what that means is it's a vector of all of these derivatives. And this upside down triangle here is called either the del or the nabla and it's just a little convenient notational shortcut to avoid writing all these out. And the x here is telling you about the thing that we're basically putting on the bottom. What's it with respect to? What's the direction that we're trying to go? So that's actually what I should have written in the notes that I was doing in the lesson. I wrote something else which is basically the equivalent of writing this. And that's not a thing at all. So in my notes when you see me write this I actually mean this. So why does my brain get confused when I write this weird thing that doesn't even exist? As far as I know. Well the reason is that this thing does exist if you turn the triangles upside down and hence my brain always gets confused. And if I turn the triangles upside down these triangles are now totally different. These triangles now mean a small change. So this is a small change in loss divided by a small change in, let's for example one particular pixel. And that's a totally valid thing to say. And in fact if you make that small change small enough then you're going to end up with the derivative. That's what the derivative is. So the derivative is just our classic rise over run slope that we did in what is that grade 8, grade 9, rise over run. Once we make our step in x small enough. And so then we can then do that for changing just one variable in a multivariable function. So for example when I say variable I guess in this case I should give an example. For example changing one pixel value in an image to see how that impacts our loss. Or we could do the same thing for the weights. We could change one weight. And if you just change one thing at a time and calculate the derivative of the loss against that one thing then we get these things called partials. And if you then do it for all the different things that you could change such as every pixel you get this whole gradient vector which we use the upside down triangle to represent. And then finally the non-upside down triangle simply refers to a small change. So this would be a small change in loss which is caused by changing pixel 1,1 by a small bit. And if you use a small enough bit, an infinitely small bit, we call that the derivative. Okay, so with all that said net result every time you see me, well I think I only did it once but in the notes where I say this, please throw that away in your head and replace it in your head with this. And that is the moral of the story. Okay, so thank you very much for bearing with me as I do my penance to actually get this notation correct this time. And I will endeavor not to make the same mistake again during this course but no promises. All right, back to the lesson. Okay, so with those 784 values they tell us how can we change x3 to make it look more like a digit. And so what we can then do is we can now change the pixels according to this gradient. And so we can do something a lot like what we do when we train neural networks except instead of changing the weights in a model we're changing the inputs to the model. And so we're going to take every pixel and we're going to modify it, subtract its gradient a little bit times its gradient. So we multiply this by some constant, let's call it c, and then we're going to subtract it to get some new image. So with the new image it's probably going to get rid of some of these bits at the bottom, right, and it's probably going to add a few more bits between some of these here, right. Okay, and we've now got something that looks slightly more like a handwritten digit than before. And this is the basic idea. We can now do that again. We can now take this, we can run it through f, and so we've now got something, let's say we call it x3 prime, for example. So this new version x3 prime or whatever is now the probability that's a handwritten digit, it's quite a bit higher, I'd say it's probably like 0.2 maybe. And we can now do the same thing. We can say for every pixel if I increase its value a little bit or decrease its value a little bit, how does it change the probability that this new x3 whatever prime prime is a digit? And so we'll now get a new gradient here, 794 values, and we can use that to change every pixel to make it look a little bit more like a handwritten digit. So as you can see, if we have this magic function, we can use it to turn any arbitrary noisy input into something that looks like a valid input, something that has a high p-value from that function by using this derivative. So a key thing to remember here is this thing is saying as I change the input pixels, how does it change the probability that this is a digit? And that tells me which pixels to make darker and which pixels to make lighter. Now those of you who remember your high school calculus may recall that when you do this by changing x pixel one at a time to calculate a derivative, this is called the finite differencing method of calculating derivatives. And it's very slow because we have to call finite differ, sorry I can't spell, differencing. It's very slow because we have to call it seven this function 784 times for every single one. We don't have to use finite differencing. Assuming the folks running this magic API endpoint use Python, we can just call f dot backward and then we can get x3 dot grad. And that will tell us the same thing in one go by using the analytic derivatives. So we'll learn exactly about what this dot backward does. We'll write our own everything from scratch including our own calculus things from scratch later but for now, just like we did in part one of the course, we're just going to assume these things exist. So maybe then the nice folks that provide this endpoint could actually provide a new endpoint that calls dot backward for us and gives us dot grad. And then we don't really have to use f at all. We can instead just directly call this endpoint that gives us the gradient directly. We'll multiply it by this smaller constant c, we'll subtract it from the pixels and we'll do it a few times, making the input get larger and larger p-values, larger and larger probabilities that this is actually a digit. So we don't particularly need this thing at all. We don't particularly need the thing that calculates these probabilities. We only need the thing that tells us which pixels we should change to calculate the probabilities. Okay, so that's great. The problem is nobody's provided this for us. So we're going to have to write it. So how are we going to do that? Well, no problem. Simply speaking, in this course, when there's some magic black box that we want to exist and it doesn't exist, we create a neural net and we train it. So we want to train a neural net that tells us which pixels to change to make a digit look, well to make an image look more like a handwritten digit. Okay, so here's how we can do that. We could create some training data and use that training data to get the information we want. We could pass in something that looks a lot like a handwritten digit. We could pass something that looks a bit like a handwritten digit. We could pass something in that doesn't look very much like a handwritten digit. And we could pass in something which doesn't really look like a handwritten digit at all. Now you'll notice it was very easy for me to create these. I created real handwritten digits and then I just chucked random noise on top of it. It's a little bit awkward for us to come up with an exact score saying how much is that like a handwritten digit, how much is that like a handwritten digit, how much is that and how much is that. It seems a bit arbitrary. So let's not do that. Let's use something which is kind of like the opposite. And instead let's say oh why don't we predict how much noise I added, right, because this number 7 is actually equal to this number 7 plus this noise. And this number 3 is actually equal to this number 3 plus this noise. And this number 6 is actually equal to this number 6 plus this noise. And that one's got a lot. And of course the very first one is equal to this number 9 plus this noise. So why don't we generate this data and then rather than trying to come up with some arbitrary number of like how much like a digit is it, let's say the amount of noise tells us how much like a digit it is. So something with no noise is very much like a digit and something with lots of noise isn't much like a digit at all. So let's feed in, let's create a neural net. Who cares what the architecture is, right, it's just a neural net of some kind. And this is critical to your understanding of this course at this point. We're going to go beyond the idea of like worrying all the time about architectures and details and we're going to be spending quite often, I mean we're going to get to all those details, but the important thing to using this stuff well is to think about neural nets as being something that has some inputs, some outputs. Oopsie-daisy. Some outputs and some loss function which takes those two and then the derivative is used to update the weights. That's really what we care about, those four things. Now the inputs to our model is this. Okay that's the inputs to our model. The outputs to our model is a measure of how much noise there is. So maybe we could just say oh well what's the, these are all basically normally distributed random variables with a mean of zero and a variance in this case of zero. In this case they're normally distributed random variables with a mean of zero and a variance of like 0.1. This one's normally distributed random variables pixels I guess with a mean of zero and like 0.3. This one's super noisy. That's the mean and variance so that's the mean for each one and the variance for each one. So why don't we as the output use the variance? So predict how much noise or better still why don't we predict the actual noise itself? So why don't we actually use that? Now we're not just predicting how much noise but we predict the actual noise. That's our outputs. Now if we do that our loss is going to be very simple. It's going to be we took the input, we passed it through our neural net, we tried to predict what the noise was and so the prediction of the noise is n hat and the actual noise is n and so we can do something we've done a thousand times which is we can divide it by the count squared and then we can sum all that up and this here is the mean squared error which we use all the time. So the mean squared error means that we've now got inputs which is noisy digits, we've got outputs which is noise and so this neural network is trying to predict this noise. So we're basically jumping straight to the step that we had here. Remember this is what we really wanted. We wanted some ability to know how much do we have to change a pixel by to make it more digit like. Well to turn this number seven into this number seven, that's our goal, we have to remove all of that. So if we can predict the noise then we've got exactly what we want which is this. We can then do this process. We can take multiply it by a constant and subtract it from our input and so if you subtract this noise from this input you get this handwritten digit. So we're doing exactly what we wanted. Well that seems easy enough. We already know from part one how to do this. So we just have any old neural network, so some kind of conv net or something that takes as input numbers where we've just randomly added different amounts of noise, lots of noise to some, not much noise to others. It predicts what the noise was that we added. We take the loss between the predicted output and the actual noise, mean squared error, and we use that to update the weights. And so if we train this for a while, then if we pass this into our model it will return that. And we're done. We now have something that can generate images. How? Because now we can take this trained neural network, so I'm going to copy it down here, and we can pass it something very, very, very noisy, which is pure noise. We pass it to the neural net and it's going to spit out information saying which part of that does it think is noise. And it's going to leave behind the bits that look the most like a digit, just like we did back here. So it might say, oh, you know what, if you left behind just that bit, that bit, that bit, that bit, that bit, that bit, that bit, and that bit, it's going to look a little bit more like a digit. And then maybe you could increase the values of that bit, that bit, that bit, that bit, that bit, and that bit. And so after you do that, and so then everything else is noise, so we subtract those bits, subtract it, times some constant, we're now going to have something that looks more like a digit, which is what we hoped for. And so then we can just do it again. And you can see now why we are doing this multiple times. Somebody on the chat is saying they don't see me drawing? Oh, you can see. Thanks Jimmy. Don't know, Michelangelo, what's happening for you. Okay. And to answer your earlier question about how am I drawing, I'm using a graphics tablet, which I'm not very expert at because on Windows you can just draw directly on the screen, which is why this is particularly messy. Alright, in practice, at the moment, this might change by the time you've watched this, we use a particular type of neural net for this. The particular kind of neural net we use is something that was developed for medical imaging called the U-net. If you've done previous versions of the course, you'll have seen this, and don't worry, this course will see exactly how a U-net works and we'll build them ourselves from scratch. And this is the first component of stable diffusion. It's the U-net. Okay, so there's going to be a few pieces and the details of why they're called these things don't matter too much just yet. Just take my word for it, this is their names. And the thing that you do need to know for each thing is like, what's the input and what's the output? So the input to a U-net, well what does it do? The input to the U-net is a somewhat noisy image. When I say somewhat, it could be not noisy at all, or it could be all noise. That's the input. And the output is the noise. Such that if we subtract the output from the input, we end up with the un-noisy image, or at least an approximation of it. So that's the U-net. Now here's the problem, well here's our problem. We have, oh why do I keep forgetting this, we have 28 times 28, 784, I should write that down. We have in these things 784 pixels. That's quite a lot. And it gets worse because in practice we don't want to draw handwritten digits. The thing that we'd be passing in here is beautiful high definition photos or images of like great paintings. And at the moment the thing we tend to use for that is a 512 by 512 by 3 channel RGB. Nice big image. 512 by 512 by 3. Red green blue. These are the pixels. So that is 512 by 512 by 3, 786 432. So we've got 786 432 pixels in here. And so this is, I don't know, some beautiful picture. This is my amazing portrait Van Gogh style in a dainty little hat. There we go. So this is the beautiful painting or an image of it. That's a lot of pixels. And so training this model where we put noisy versions of millions of these beautiful images is going to take us an awful long time. And if you're Google with a huge cloud of TPUs or something, maybe that's okay. But for the rest of us, we would like to do this as efficiently as possible. How could we do this more efficiently? Well, when you think about it, in this beautiful picture I drew, storing the exact pixel value of every single pixel is probably not the most efficient way to store it. What if instead we said, oh, let's say this is like green rushes or something. It might say like, oh, over here is green and everything kind of underneath, it's pretty much the same. Or, you know, maybe I'm wearing a blue top in this beautiful portrait and it could kind of say like, oh, all the pixels in here are blue. You know, you don't really have to do everyone individually. There are faster, more concise ways of storing what an image is. We know this is true because, for example, a JPEG picture is far fewer bytes than the number of bytes you would get if you multiplied its height by its width by its channels. So we know that it's possible to compress pictures. So let me show you a really interesting way to compress pictures. Let's take this image and let's put it through a convolutional layer of stride2. Now if we put it through a convolutional layer of stride2 with six features, with six channels, we would get back a 256 by 256. Gosh, that was a terrible attempt at drawing a square, wasn't it? By 256, actually do it here. By, okay, let's double the number of channels to six. By six. And then let's put it through another stride2 convolution. And remember, we're going to be seeing exactly how to do all these things and building them all from scratch. So don't worry if you're not sure what a stride2 convolution exactly is. And just do it again to get 128 by 128. And again, let's double the number of channels. And then let's do it again, another stride2 convolution. So we're just building a neural network here. So now we're down to 64 by 64 by 24. Okay and then now let's put that through a few like resnet blocks to kind of squish down the number of channels as much as we can. So it'll be now down to let's say 64 by 64 by 4. Okay, so here's a neural network. And so the number of pixels in this version is now 64 times 64 times 4, 16384. So there's 16384 pixels here. Okay, so we've compressed it from 786, 432 to 16384, which is a 48 times decrease. Now that's no use if we've lost our image. So can we get the image back again? Sure, why not? What if we now create a kind of an inverse convolution which does the exact opposite? So actually let's put it over here. So we're going to take our 64 by 64 by 4 image, put it through an inverse convolution. So let's put it, let's keep moving this over further. Back to 128 by 128 by 12 and put it through another inverse convolution. So these are all just basically they're just neural network layers. 256 by 256 by 6. And then finally, wrap you out, all the way back to 512 by 512 by 3. Okay, we could put this whole thing inside a neural net. Here's our single neural network. And what we could do is we can start feeding it images. It goes all the way through this neural network and out of the other end comes back, well initially it's random. So initially comes out of this is random noise. 512 by 512, because I draw it inside here. So inside here initially it's going to give us random noise. And so now we need a loss function, right? So the loss function we can create could be to say let's take this output and this input and compare them and create and do an MSE, mean squared error, directly on those two pieces. So what would that do if we train this model? This model is going to try to put an image through and going to try to make it so that what comes out the other end is the exact same thing that went into it. That's what it's going to try to do. Because if it does that successfully, then the mean squared error would be zero. So I see some people in the chat saying that this is a UNET. This is not a UNET. We'll get to that later. There's no cross connections. It's just a bunch of convolutions that decrease inside followed by a bunch of convolutions that increase inside. And so we're going to try to train this model to spit out exactly what it received in. And that seems really boring. What's the point of a model that only learns to give you back exactly what came in? Well, this is actually extremely interesting. This kind of model is called an autoencoder. It's something that gives you back what you gave it. And the reason an autoencoder is interesting is because we can split it in half. Let's grab just this bit. Okay, let's cut it up. Let's grab just that bit. And then we'll get a second half. Okay, they're not quite halves, but you know what I mean, which is just this bit. And so let's say I take this image and I put it through just this first half, this green half, which is called the encoder. Okay, I can take this thing that comes out of it and I could save it. And the thing that I'm going to save is going to be 16,384 bytes. I started with something that was 48 times bigger than that, 786,432 bytes. And I've turned it into something that's 16,384 bytes. I could now attach that to an email, say, or whatever, and I've now got something that's 48 times more than my original picture. So what's going to happen? The person who receives these 16,384 bytes, well, as long as they have a copy of the decoder on their computer, they can feed those bytes into the decoder and get back the original image. So what we've just done is we've created a compression algorithm. That's pretty amazing, isn't it? And in fact, these compression algorithms work extremely, extremely well. And notice that we didn't train this on just this one image. We've trained it on, say, millions and millions of images. And then so you and I both need to have a copy of these two neural nets, but now we can share thousands of pictures that we send each other by sending just the 16,384 byte version. So we've created a very powerful compression algorithm. And so maybe you can see where this is going. If this here is something which contains all of the interesting and useful information of the image in 16,384 bytes, why on earth would we train our UNet with 786,432 pixels of information? And the answer is we wouldn't. That would be stupid. Instead, we're going to do this entire thing using our encoded version of each picture. So if we want to train this UNet on 10 million pictures, we put all 10 million pictures through the autoencoder's encoder. So we've now got 10 million of these smaller things. And then we feed it into the UNet training hundreds or thousands of times to train our UNet. And so what will that UNet now do? Something slightly different to what we described. It does not anymore take a somewhat noisy image. Instead it takes a somewhat noisy one of these. So it'd probably help to give this thing a name. And so the name we give this thing is Latents. These are called the Latents. Okay, so instead the input is somewhat noisy Latents. The output is still the noise. And so we can now subtract the noise from the slightly somewhat noisy Latents. And that would give us the actual Latents. And so we can then take the output of the UNet and pass it into our autoencoder's decoder. Because that's something which takes Latents and turns it into a picture. So the input to this is SmallLatentsTensor. And the output is a large image. Okay. Now this thing here is not going to be called an encoder. It's going to have the name the VAE. And we'll learn about why later. Those details aren't too important, but let's put its correct name here. The VAE's decoder. So you're only going to need the encoder for the VAE if you're training a unit. If you want to just do inference like we did today, you're only going to need the decoder of the unit. So this whole thing of Latents is entirely optional. This thing we described before works fine. But generally speaking, we would rather not use more compute than necessary. So unless you're trying to sell the world a room full of TPUs, you would probably rather everybody was doing stuff on the thing that's 48 times smaller. So the VAE is optional, but it saves us a whole lot of time and a whole lot of money. So that's good. Okay. What's next? Well, there's something else, which is we have not just been in this morning's, sorry, in the first half of today's lesson, we weren't just saying, produce me an image. We were saying produce me an image of Tiny the Teddy Bear riding a horse. So how does that bit work? So the way that bit works is actually on the whole pretty straightforward. Let's think about how we could do exactly that for our MNIST example. How could we get this so that rather than just feeding in noise and getting back some digit, how do we get it to give us a particular digit? What if we wanted to pass in the literal number three plus some noise and have it attempt to generate a handwritten three for us? How would we do that? Well, what we could do is way back here for the input to this model, why don't in addition to passing in the noisy input, let's also pass in a one hot encoded version of what digit it is. So we're now passing two things into this model. So previously, this neural net took as inputs just the pixels, but now it's going to take in the pixels and what digit is it as like a one hot encoded vector. So now it's going to learn how to predict what is the noise, right? And it's going to predict what is the noise and it's going to have some extra information, which is it's going to know what the original image was. So we would expect this model to be better at predicting noise than the previous one because we're giving it more information. This was a three, this was a six, this was a seven. So this neural net is going to learn to estimate noise better by taking advantage of the fact that it knows what actual the input was. And why is that useful? Well, the reason that's useful is because now when we feed in the number three, like the actual digit three is a one hot encoded vector plus noise after this has been trained, then our model is going to say the noise is everything that doesn't represent the number three because that's what it's learned to do. Right? So that's a pretty straightforward way to give it the word we use is guidance about what it is that we're actually trying to remove the noise from. And so then we can use that guidance to guide it as to what image we're trying to create. So that's the basic idea. Now the problem is if we want to create a picture of a cute teddy bear, we've got a problem. It was easy enough to pass the digit eight, the literal number eight into our neural net because we can just create a one hot encoded vector in which position number eight is a one and everything else is a zero. But how do we do that for a cute teddy? We can't. We can't create every possible sentence that can be uttered in the whole world and then create a one hot encoded version of every sentence in the world because that's going to take a vector that is too long to say the least. So we have to do something else to turn this into an embedding, something other than grabbing a one hot encoded version of this. So what do we do? So what we're going to do is we're going to try to create a model that can take a sentence like a cute teddy and can return a vector of numbers that in some way represents what cute teddies look like. And the way we're going to do that is we're first going to surf the internet and download images. So here are four examples of images that I found on the internet. And so for each of these images, they had an image tag next to them, right? And if people are being good, then they also added an alt tag to help with accessibility and maybe for SEO purposes. And they probably said things like a graceful swan. And the alt tag for this might have been a scene from Hitchcock's The Birds. And the alt tag for this might have been Jeremy Howard. And the alt tag for this might have been Fast.ai's logo. And we could do that for millions and millions and millions of images that we find on the internet. So what we can now do with these is we can create two models. One model which is a text encoder and one model which is an image encoder. Okay, so again, these are neural nets. We don't care about what their architectures are or whatever. We know that they're just black boxes which contain weights, which means they need inputs and outputs and a loss function. And then they'll do something. Once we've defined inputs and outputs and a loss function, the neural nets will then do something. So here's a really interesting idea. What if we take this image and what if we then also take the text, a graceful swan, and we're going to feed these into their respective models, which initially they of course have random weights. And that means that they're going to spit out random features, a vector of stuff, random crap. Because we haven't trained them yet, okay. And we can do the same thing with a scene from Hitchcock. We pass the scene from Hitchcock in and we'll pass in the word scene from Hitchcock and then it'll give us two other vectors. And so we can do something really interesting now. We can line these up. I guess we'll just move them. We can line these up. Okay, here's all of our images. And then we can have, whoopsie-daisy. And then we can have our text. So we've got graceful swan. We've got Hitchcock. We've got Jeremy Howard. And we've got FastAI logo. Now ideally when we pass the graceful swan through our model, what we'd like is that it creates a set of embeddings that are a good match for the text graceful swan. When we pass the scene from Hitchcock through our image model, we would like it to return embeddings which are similar to the embeddings for the text scene from Hitchcock. And ditto for the picture of Jeremy Howard versus the name Jeremy Howard and ditto for the image FastAI and, sorry, FastAI logo and the word FastAI logo. So in other words, for this particular combination here, we would like this one's features and this one's features to be similar. So how do we tell if two sets of things are similar? Two vectors. Well, what we can do is we can simply multiply them together element-wise and add them up. And this thing here is called the dot product. And so we could take the dot product of the features from the image model for this one and the dot product of the features from the text model for the word graceful swan and take their dot product. And we want that number to be nice and big. And the scene from Hitchcock's features should be very similar to the text scene from Hitchcock's features. So we want their product to be nice and big. And ditto for everything on this diagonal. Now, on the other hand, a graceful swan picture should not have embeddings that are similar to the text as seen from Hitchcock. So that should be nice and small. And ditto for everything else off diagonal. And so perhaps you can see where this is going. If we add up all of these, add those all together and then subtract all of these, we have a loss function. And so if we want this loss function to be good, then we're going to need the weights of our model for the text encoder to spit out embeddings that are very similar to the images that they're paired with. And we need them to spit out embed features for things that they are not paired with, which are not similar. And so if we can do that, then we're going to end up with a text encoder that we can feed in things like a graceful swan, some beautiful swan, such a lovely swan. And these should all give very similar embeddings, because these would all represent very similar pictures. And so what we've now done is we've successfully created two models that together put text and images into the same space. So we've got this multimodal set of models, which is exactly what we wanted. So now we can take our cute teddy bear, feed it in here, get out some features. And that is what we will use instead of these one hot encoded vectors when we train our photo or painting or whatever unit. And then we can do exactly the same thing with guidance. We can now pass in the text encoder's feature vector for a cute teddy, and it is going to turn the noise into something that is similar to things that it's previously seen that are cute teddies. So the model that's used, or the pair of models that's used here, is called PLEP. This thing where we want these to be bigger and these to be smaller is called a contrastive loss. And now you know where the CL comes from. So here we have a CLP text encoder. Its input is some text. Its output is, we call it an embedding. It's just some features. Oops, embedding. Where similar sets of text with similar meanings will give us similar embeddings. Okay. We need a bit more space, we're nearly done. So we've got the unit that can denoise latency into unnoisy latency, including pure noise. We've got a decoder that can take latency and create an image. We've got a text encoder which can allow us to train a unit which is guided by captions. So the last thing we need is the question about how exactly do we do this inference process here? So how exactly, once we've got something that gives us the gradients we want, and by the way these gradients are often called the score function, just in case you come across that. That's all that's referring to. Yeah, how exactly do we go about this process? And unfortunately the language used around this is weird and confusing. And so ideally you will learn to ignore the fact that the language is weird and confusing. And in particular the language you'll see a lot talks about time steps. And you'll notice that during our training process we never used any concept of time steps. This is basically an overhang from the particular way in which the math was formulated in the first papers. There are lots of other ways we can formulate it. And during the course on the whole we will avoid using the term time steps. But to see what time steps are, even though it's got nothing to do with time in real life, consider the fact that we used varying levels of noise. Some things were very noisy, some things were not noisy at all, some things had no noise, and some I haven't drawn here would have been pure noise. You could basically create a kind of a noising schedule where along here you could put say the numbers from 1 to 1000. And you could then say oh you know and we'll call this t. And maybe we randomly pick a number from 1 to 1000 and then we look up on this noise schedule which would be some monotonically decreasing function. And we'd look up, let's say we happen to pick randomly a number 4, we would look up here to find where that is. We'd look over here and this would return to us some sigma which is the amount of noise to use if you happen to get a 4. So if you happen to get a 1, you're going to get a whole lot of noise and if you happen to get a 1000, you're going to have hardly any noise. So this is one way of picking, so remember when we were training we were going to pick for every image a random amount of noise. So this would be one way to do that is to pick a random number from 1 to 1000, look it up on this function and that tells us how much noise to use. So this t is what people refer to as the time step. Nowadays you don't really have to do that that much and a lot of people are starting to get rid of this idea altogether and some people instead will simply say how much noise was there. Usually we would think of using sigma for standard deviations of Gaussians or normal distributions but actually much more common is to use the Greek letter beta. And so if you see something talking about beta, they're just saying oh for that particular image when it was being trained, what standard deviation of noise was being used basically. It's slightly hand wavy but close enough. And so what you do each time you're going to create a mini batch to pass into your model, you randomly pick an image from your training set, you randomly pick either an amount of noise or some models you randomly pick a t and then look up an amount of noise and then you use that amount of noise for each one and then you pass that mini batch into your model to train it and that trains the weights in your model so it can learn to predict noise. And so then when you come to inference time, so inference is when you're generating a picture from pure noise, you want your model, basically your model is now starting here, right, which is as much noise as possible. And so you want it to learn to remove noise. But what it does in practice, as we saw in our notebook, is it actually creates some hideous and rather random kind of thing. So in fact let's remind ourselves what that looked like. This is what it created when we tried to do it in one step. So remember what we then do is we say okay, what's the prediction of the noise and then we multiply the prediction of the noise I said by some constant, which is kind of like a learning rate, but we're not updating weights now, we're updating pixels. And we subtract it from the pixels. So it didn't actually predict the image, what it actually did was it predicted what the noise is so that it could then subtract that from the image, from the noisy image, to give us the denoised image. And so what we do is we don't actually subtract all of it, we multiply that by a constant and we get a somewhat noisy image. The reason we don't jump all the way to the best image we can find is because things that look like this never appeared in our training set. And so since it never appeared in our training set, our model has no idea what to do with it. Our model only knows how to deal with things that look like somewhat noisy latents. And so that's why we subtract just a bit of the noise so that we still have a somewhat noisy latent. So this process repeats a bunch of times and questions like what do we use for C, right, and how do we go from the prediction of noise to the thing that we subtract. These are all of the things that are the kind of the things that you decide in the actual sampler. And that's used both to think about like how do I add the noise and how do I subtract the noise. And there's a few things that might be jumping into your head at this point if you're anything like me. And one is that, gosh, this looks an awful lot like deep learning optimizers. So in a deep learning optimizer, this constant is called the learning rate. And we have some neat tricks where we say, for example, oh, if you change the same parameters by a similar amount multiple times in multiple steps, maybe you should increase the amount you change them. This concept is something we call momentum. And we'll be doing all this from scratch during the course, don't worry. And in fact, we even got better ways of doing that where we kind of say, well, what about what happens as the variance changes? Maybe we can look at that as well. And that gives us something called Adam. And these are types of optimizer. And so maybe you might be wondering, could we use these kinds of tricks? And the answer, based on our very early research, is yes, yes, we can. The whole world of like where stable diffusion and all these diffusion based models came from a very different world of maths, which is the world of differential equations. And there's a whole lot of very parallel concepts in the world of differential equations, which is really all about taking these like little steps, little steps, little steps, and trying to figure out how to take bigger steps. And so different differential equation solvers use a lot of the same kind of ideas, if you use Gwent, as optimizers. One thing that differential equations solvers do, which is kind of interesting though, is that they tend to take t as an input. And in fact, pretty much all diffusion models, I've actually lied, pretty much all diffusion models don't just take the input pixels and the digit or the caption or the prompt, they also take t. And the idea is that the model will be better at removing the noise if you tell it how much noise there is. And remember, this is related to how much noise there is. I very strongly suspect that this premise is incorrect. Because if you think about it, for a complicated fancy neural net, figuring out how noisy something is, is very, very straightforward. So I very much doubt we actually need to pass in t. And as soon as you stop doing that, things stop looking like differential equations, and they start looking more like optimizers. And so actually, Jono's started playing with this and experimenting a bit. And early results suggest that, yeah, actually, when we rethink about the whole thing as being about learning rates and optimizers, maybe it actually works a bit better. In fact, there's all kinds of things we could do. Once we stop thinking about them as differential equations, and worry about the math, don't worry about the math so much about Gaussians and whatever, we can really switch things around. So for example, we decided, for no particular obvious reason, to use MSE. Well the truth is, in statistics and machine learning, almost every time you see somebody use MSE, it's because the math worked out better that way. Not as in, it's a better thing to do, but as in, you know, it was kind of easier. Now MSE does fall out quite nicely as being a good thing to do with some particular premises. You know, like it's not totally arbitrary. But what if we instead used more sophisticated loss functions, where we actually said, well, you know, after we subtract the outputs, how good is this really? Does it look like a digit, or does it have the similar qualities to a digit? So we'll learn about this stuff, but there's things called, for example, perceptual loss. Or another question is, do we really need to do this thing where we actually put noise back at all? Could we instead use this directly? These are all things that suddenly become possible when we start thinking of this as an optimization problem, rather than a differential equation solving problem. So for those of you who are interested in kind of doing novel research, this is some of the kind of stuff that we are starting to research at the moment, and the early results are extremely positive, both in terms of how quickly we can do things and what kind of outputs we seem to be getting. Okay, so I think that's probably a good place to stop it. So what we're going to do in the next lesson is we're going to finish our journey into this notebook to see some of the code behind the scenes of what's in a pipeline. When I get there. So we'll do looking inside the pipeline and see exactly what's going on behind the scenes a bit more in terms of the code. And then we're going to do a huge rewind from the foundations, and we're going to build up from some very tricky ground rules. The ground rules will be we're only allowed to use pure Python, the Python standard library, and nothing else, and build up from there until we have recreated all of this and possibly some new research directions at the same time. So that's our goal. So strap in and see you all next time. See you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.0, "text": " Hi, everybody, and welcome to Deep Learning Foundations to Stable Diffusion.", "tokens": [2421, 11, 2201, 11, 293, 2928, 281, 14895, 15205, 8207, 763, 281, 745, 712, 413, 3661, 5704, 13], "temperature": 0.0, "avg_logprob": -0.20966549308932558, "compression_ratio": 1.5659574468085107, "no_speech_prob": 0.020319422706961632}, {"id": 1, "seek": 0, "start": 7.0, "end": 12.56, "text": " Hopefully, it's not too confusing that this is described here as lesson nine.", "tokens": [10429, 11, 309, 311, 406, 886, 13181, 300, 341, 307, 7619, 510, 382, 6898, 4949, 13], "temperature": 0.0, "avg_logprob": -0.20966549308932558, "compression_ratio": 1.5659574468085107, "no_speech_prob": 0.020319422706961632}, {"id": 2, "seek": 0, "start": 12.56, "end": 17.12, "text": " That's because, strictly speaking, we treat this as part two of the Practical Deep Learning", "tokens": [663, 311, 570, 11, 20792, 4124, 11, 321, 2387, 341, 382, 644, 732, 295, 264, 19170, 804, 14895, 15205], "temperature": 0.0, "avg_logprob": -0.20966549308932558, "compression_ratio": 1.5659574468085107, "no_speech_prob": 0.020319422706961632}, {"id": 3, "seek": 0, "start": 17.12, "end": 21.72, "text": " for Coders series.", "tokens": [337, 383, 378, 433, 2638, 13], "temperature": 0.0, "avg_logprob": -0.20966549308932558, "compression_ratio": 1.5659574468085107, "no_speech_prob": 0.020319422706961632}, {"id": 4, "seek": 0, "start": 21.72, "end": 26.44, "text": " So that part one had eight lessons, so this is lesson nine.", "tokens": [407, 300, 644, 472, 632, 3180, 8820, 11, 370, 341, 307, 6898, 4949, 13], "temperature": 0.0, "avg_logprob": -0.20966549308932558, "compression_ratio": 1.5659574468085107, "no_speech_prob": 0.020319422706961632}, {"id": 5, "seek": 0, "start": 26.44, "end": 29.080000000000002, "text": " But don't worry, you didn't miss anything.", "tokens": [583, 500, 380, 3292, 11, 291, 994, 380, 1713, 1340, 13], "temperature": 0.0, "avg_logprob": -0.20966549308932558, "compression_ratio": 1.5659574468085107, "no_speech_prob": 0.020319422706961632}, {"id": 6, "seek": 2908, "start": 29.08, "end": 36.36, "text": " It's the first lesson of part two, which is called Deep Learning Foundations to Stable", "tokens": [467, 311, 264, 700, 6898, 295, 644, 732, 11, 597, 307, 1219, 14895, 15205, 8207, 763, 281, 745, 712], "temperature": 0.0, "avg_logprob": -0.11159224302872367, "compression_ratio": 1.713740458015267, "no_speech_prob": 6.920506712049246e-05}, {"id": 7, "seek": 2908, "start": 36.36, "end": 37.76, "text": " Diffusion.", "tokens": [413, 3661, 5704, 13], "temperature": 0.0, "avg_logprob": -0.11159224302872367, "compression_ratio": 1.713740458015267, "no_speech_prob": 6.920506712049246e-05}, {"id": 8, "seek": 2908, "start": 37.76, "end": 41.16, "text": " And maybe rather than calling it Practical Deep Learning for Coders, we should call this", "tokens": [400, 1310, 2831, 813, 5141, 309, 19170, 804, 14895, 15205, 337, 383, 378, 433, 11, 321, 820, 818, 341], "temperature": 0.0, "avg_logprob": -0.11159224302872367, "compression_ratio": 1.713740458015267, "no_speech_prob": 6.920506712049246e-05}, {"id": 9, "seek": 2908, "start": 41.16, "end": 45.4, "text": " Impractical Deep Learning for Coders in the sense that we are certainly not going to be", "tokens": [4331, 42559, 804, 14895, 15205, 337, 383, 378, 433, 294, 264, 2020, 300, 321, 366, 3297, 406, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.11159224302872367, "compression_ratio": 1.713740458015267, "no_speech_prob": 6.920506712049246e-05}, {"id": 10, "seek": 2908, "start": 45.4, "end": 52.4, "text": " spending all of our time seeing exactly how to do important things with deep learning.", "tokens": [6434, 439, 295, 527, 565, 2577, 2293, 577, 281, 360, 1021, 721, 365, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.11159224302872367, "compression_ratio": 1.713740458015267, "no_speech_prob": 6.920506712049246e-05}, {"id": 11, "seek": 2908, "start": 52.4, "end": 57.32, "text": " But we'll be doing a whole lot of fun things, generative model-y fun things, and also a", "tokens": [583, 321, 603, 312, 884, 257, 1379, 688, 295, 1019, 721, 11, 1337, 1166, 2316, 12, 88, 1019, 721, 11, 293, 611, 257], "temperature": 0.0, "avg_logprob": -0.11159224302872367, "compression_ratio": 1.713740458015267, "no_speech_prob": 6.920506712049246e-05}, {"id": 12, "seek": 5732, "start": 57.32, "end": 65.76, "text": " whole lot of understanding, lots of details, which you won't necessarily need to know to", "tokens": [1379, 688, 295, 3701, 11, 3195, 295, 4365, 11, 597, 291, 1582, 380, 4725, 643, 281, 458, 281], "temperature": 0.0, "avg_logprob": -0.10665890188778147, "compression_ratio": 1.614678899082569, "no_speech_prob": 2.6424884254083736e-06}, {"id": 13, "seek": 5732, "start": 65.76, "end": 67.24, "text": " use this stuff.", "tokens": [764, 341, 1507, 13], "temperature": 0.0, "avg_logprob": -0.10665890188778147, "compression_ratio": 1.614678899082569, "no_speech_prob": 2.6424884254083736e-06}, {"id": 14, "seek": 5732, "start": 67.24, "end": 72.48, "text": " But if you want to become a researcher, or if you want to put something in production,", "tokens": [583, 498, 291, 528, 281, 1813, 257, 21751, 11, 420, 498, 291, 528, 281, 829, 746, 294, 4265, 11], "temperature": 0.0, "avg_logprob": -0.10665890188778147, "compression_ratio": 1.614678899082569, "no_speech_prob": 2.6424884254083736e-06}, {"id": 15, "seek": 5732, "start": 72.48, "end": 77.52, "text": " which has got some kind of complex customization requirements, stuff like that, then it is", "tokens": [597, 575, 658, 512, 733, 295, 3997, 39387, 7728, 11, 1507, 411, 300, 11, 550, 309, 307], "temperature": 0.0, "avg_logprob": -0.10665890188778147, "compression_ratio": 1.614678899082569, "no_speech_prob": 2.6424884254083736e-06}, {"id": 16, "seek": 5732, "start": 77.52, "end": 82.44, "text": " going to be very helpful to learn the details we'll be talking about.", "tokens": [516, 281, 312, 588, 4961, 281, 1466, 264, 4365, 321, 603, 312, 1417, 466, 13], "temperature": 0.0, "avg_logprob": -0.10665890188778147, "compression_ratio": 1.614678899082569, "no_speech_prob": 2.6424884254083736e-06}, {"id": 17, "seek": 8244, "start": 82.44, "end": 87.67999999999999, "text": " So here in lesson nine, there's kind of going to be two parts to it.", "tokens": [407, 510, 294, 6898, 4949, 11, 456, 311, 733, 295, 516, 281, 312, 732, 3166, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.16152456437034168, "compression_ratio": 1.5, "no_speech_prob": 1.136551190938917e-06}, {"id": 18, "seek": 8244, "start": 87.67999999999999, "end": 97.24, "text": " One is just a quick run-through, quick-ish run-through of using stable diffusion, because", "tokens": [1485, 307, 445, 257, 1702, 1190, 12, 11529, 11, 1702, 12, 742, 1190, 12, 11529, 295, 1228, 8351, 25242, 11, 570], "temperature": 0.0, "avg_logprob": -0.16152456437034168, "compression_ratio": 1.5, "no_speech_prob": 1.136551190938917e-06}, {"id": 19, "seek": 8244, "start": 97.24, "end": 99.84, "text": " we're all dying to play with it.", "tokens": [321, 434, 439, 8639, 281, 862, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.16152456437034168, "compression_ratio": 1.5, "no_speech_prob": 1.136551190938917e-06}, {"id": 20, "seek": 8244, "start": 99.84, "end": 108.36, "text": " And then the other thing that I'll be doing is describing in some detail what's going", "tokens": [400, 550, 264, 661, 551, 300, 286, 603, 312, 884, 307, 16141, 294, 512, 2607, 437, 311, 516], "temperature": 0.0, "avg_logprob": -0.16152456437034168, "compression_ratio": 1.5, "no_speech_prob": 1.136551190938917e-06}, {"id": 21, "seek": 8244, "start": 108.36, "end": 109.36, "text": " on.", "tokens": [322, 13], "temperature": 0.0, "avg_logprob": -0.16152456437034168, "compression_ratio": 1.5, "no_speech_prob": 1.136551190938917e-06}, {"id": 22, "seek": 8244, "start": 109.36, "end": 110.36, "text": " How is it working?", "tokens": [1012, 307, 309, 1364, 30], "temperature": 0.0, "avg_logprob": -0.16152456437034168, "compression_ratio": 1.5, "no_speech_prob": 1.136551190938917e-06}, {"id": 23, "seek": 11036, "start": 110.36, "end": 117.56, "text": " There will be a whole lot of hand-waving either way, because it's going to take us a few lessons", "tokens": [821, 486, 312, 257, 1379, 688, 295, 1011, 12, 86, 6152, 2139, 636, 11, 570, 309, 311, 516, 281, 747, 505, 257, 1326, 8820], "temperature": 0.0, "avg_logprob": -0.2426431995548614, "compression_ratio": 1.5025906735751295, "no_speech_prob": 5.0936337174789514e-06}, {"id": 24, "seek": 11036, "start": 117.56, "end": 122.24, "text": " to describe everything from scratch.", "tokens": [281, 6786, 1203, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.2426431995548614, "compression_ratio": 1.5025906735751295, "no_speech_prob": 5.0936337174789514e-06}, {"id": 25, "seek": 11036, "start": 122.24, "end": 127.64, "text": " But hopefully you'll feel like you'll come away with this lesson with a reasonable intuitive", "tokens": [583, 4696, 291, 603, 841, 411, 291, 603, 808, 1314, 365, 341, 6898, 365, 257, 10585, 21769], "temperature": 0.0, "avg_logprob": -0.2426431995548614, "compression_ratio": 1.5025906735751295, "no_speech_prob": 5.0936337174789514e-06}, {"id": 26, "seek": 11036, "start": 127.64, "end": 132.36, "text": " understanding at least of how this is all working.", "tokens": [3701, 412, 1935, 295, 577, 341, 307, 439, 1364, 13], "temperature": 0.0, "avg_logprob": -0.2426431995548614, "compression_ratio": 1.5025906735751295, "no_speech_prob": 5.0936337174789514e-06}, {"id": 27, "seek": 11036, "start": 132.36, "end": 134.4, "text": " Assumptions.", "tokens": [6281, 449, 9799, 13], "temperature": 0.0, "avg_logprob": -0.2426431995548614, "compression_ratio": 1.5025906735751295, "no_speech_prob": 5.0936337174789514e-06}, {"id": 28, "seek": 13440, "start": 134.4, "end": 141.84, "text": " Well, I'm going to try to explain everything like everything.", "tokens": [1042, 11, 286, 478, 516, 281, 853, 281, 2903, 1203, 411, 1203, 13], "temperature": 0.0, "avg_logprob": -0.1375665263125771, "compression_ratio": 1.7452830188679245, "no_speech_prob": 4.936855020787334e-06}, {"id": 29, "seek": 13440, "start": 141.84, "end": 143.84, "text": " I'm going to try to explain everything.", "tokens": [286, 478, 516, 281, 853, 281, 2903, 1203, 13], "temperature": 0.0, "avg_logprob": -0.1375665263125771, "compression_ratio": 1.7452830188679245, "no_speech_prob": 4.936855020787334e-06}, {"id": 30, "seek": 13440, "start": 143.84, "end": 152.32, "text": " So if you haven't done deep learning before, this is going to be very hard.", "tokens": [407, 498, 291, 2378, 380, 1096, 2452, 2539, 949, 11, 341, 307, 516, 281, 312, 588, 1152, 13], "temperature": 0.0, "avg_logprob": -0.1375665263125771, "compression_ratio": 1.7452830188679245, "no_speech_prob": 4.936855020787334e-06}, {"id": 31, "seek": 13440, "start": 152.32, "end": 155.76, "text": " But I will at least be trying to say this is roughly what's going on and where you can", "tokens": [583, 286, 486, 412, 1935, 312, 1382, 281, 584, 341, 307, 9810, 437, 311, 516, 322, 293, 689, 291, 393], "temperature": 0.0, "avg_logprob": -0.1375665263125771, "compression_ratio": 1.7452830188679245, "no_speech_prob": 4.936855020787334e-06}, {"id": 32, "seek": 13440, "start": 155.76, "end": 156.96, "text": " find out more.", "tokens": [915, 484, 544, 13], "temperature": 0.0, "avg_logprob": -0.1375665263125771, "compression_ratio": 1.7452830188679245, "no_speech_prob": 4.936855020787334e-06}, {"id": 33, "seek": 13440, "start": 156.96, "end": 162.88, "text": " Having said that, I would strongly suggest doing part one before doing this course, unless", "tokens": [10222, 848, 300, 11, 286, 576, 10613, 3402, 884, 644, 472, 949, 884, 341, 1164, 11, 5969], "temperature": 0.0, "avg_logprob": -0.1375665263125771, "compression_ratio": 1.7452830188679245, "no_speech_prob": 4.936855020787334e-06}, {"id": 34, "seek": 16288, "start": 162.88, "end": 168.35999999999999, "text": " you really want to throw yourself in the deep end and give yourself quite the test.", "tokens": [291, 534, 528, 281, 3507, 1803, 294, 264, 2452, 917, 293, 976, 1803, 1596, 264, 1500, 13], "temperature": 0.0, "avg_logprob": -0.11631211467172908, "compression_ratio": 1.4824561403508771, "no_speech_prob": 5.17376702191541e-06}, {"id": 35, "seek": 16288, "start": 168.35999999999999, "end": 172.12, "text": " If you haven't done part one of Practical Deep Learning for Coders, but you're reasonably", "tokens": [759, 291, 2378, 380, 1096, 644, 472, 295, 19170, 804, 14895, 15205, 337, 383, 378, 433, 11, 457, 291, 434, 23551], "temperature": 0.0, "avg_logprob": -0.11631211467172908, "compression_ratio": 1.4824561403508771, "no_speech_prob": 5.17376702191541e-06}, {"id": 36, "seek": 16288, "start": 172.12, "end": 180.96, "text": " comfortable with deep learning basics, you could write a basic SGD loop in Python.", "tokens": [4619, 365, 2452, 2539, 14688, 11, 291, 727, 2464, 257, 3875, 34520, 35, 6367, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.11631211467172908, "compression_ratio": 1.4824561403508771, "no_speech_prob": 5.17376702191541e-06}, {"id": 37, "seek": 16288, "start": 180.96, "end": 187.78, "text": " And you know how to use, ideally, PyTorch, but TensorFlow is probably OK as well.", "tokens": [400, 291, 458, 577, 281, 764, 11, 22915, 11, 9953, 51, 284, 339, 11, 457, 37624, 307, 1391, 2264, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.11631211467172908, "compression_ratio": 1.4824561403508771, "no_speech_prob": 5.17376702191541e-06}, {"id": 38, "seek": 18778, "start": 187.78, "end": 193.72, "text": " And you kind of know the basic ideas of how to create, you know, what an embedding is,", "tokens": [400, 291, 733, 295, 458, 264, 3875, 3487, 295, 577, 281, 1884, 11, 291, 458, 11, 437, 364, 12240, 3584, 307, 11], "temperature": 0.0, "avg_logprob": -0.15602366650690797, "compression_ratio": 1.7285714285714286, "no_speech_prob": 5.954898824711563e-06}, {"id": 39, "seek": 18778, "start": 193.72, "end": 196.56, "text": " and you could create one of those in Scratch, stuff like that.", "tokens": [293, 291, 727, 1884, 472, 295, 729, 294, 34944, 852, 11, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.15602366650690797, "compression_ratio": 1.7285714285714286, "no_speech_prob": 5.954898824711563e-06}, {"id": 40, "seek": 18778, "start": 196.56, "end": 199.12, "text": " You know, you'll probably be fine.", "tokens": [509, 458, 11, 291, 603, 1391, 312, 2489, 13], "temperature": 0.0, "avg_logprob": -0.15602366650690797, "compression_ratio": 1.7285714285714286, "no_speech_prob": 5.954898824711563e-06}, {"id": 41, "seek": 18778, "start": 199.12, "end": 203.84, "text": " Generally speaking for these courses, I find most people tend to watch the videos a few", "tokens": [21082, 4124, 337, 613, 7712, 11, 286, 915, 881, 561, 3928, 281, 1159, 264, 2145, 257, 1326], "temperature": 0.0, "avg_logprob": -0.15602366650690797, "compression_ratio": 1.7285714285714286, "no_speech_prob": 5.954898824711563e-06}, {"id": 42, "seek": 18778, "start": 203.84, "end": 208.2, "text": " times and often the second time through folks will like pause and look things up they don't", "tokens": [1413, 293, 2049, 264, 1150, 565, 807, 4024, 486, 411, 10465, 293, 574, 721, 493, 436, 500, 380], "temperature": 0.0, "avg_logprob": -0.15602366650690797, "compression_ratio": 1.7285714285714286, "no_speech_prob": 5.954898824711563e-06}, {"id": 43, "seek": 18778, "start": 208.2, "end": 211.6, "text": " know and check things out.", "tokens": [458, 293, 1520, 721, 484, 13], "temperature": 0.0, "avg_logprob": -0.15602366650690797, "compression_ratio": 1.7285714285714286, "no_speech_prob": 5.954898824711563e-06}, {"id": 44, "seek": 18778, "start": 211.6, "end": 217.68, "text": " Generally speaking, you know, we expect people to be spending about 10 hours of work on each", "tokens": [21082, 4124, 11, 291, 458, 11, 321, 2066, 561, 281, 312, 6434, 466, 1266, 2496, 295, 589, 322, 1184], "temperature": 0.0, "avg_logprob": -0.15602366650690797, "compression_ratio": 1.7285714285714286, "no_speech_prob": 5.954898824711563e-06}, {"id": 45, "seek": 21768, "start": 217.68, "end": 219.28, "text": " video.", "tokens": [960, 13], "temperature": 0.0, "avg_logprob": -0.13518917149510876, "compression_ratio": 1.59765625, "no_speech_prob": 4.092754807061283e-06}, {"id": 46, "seek": 21768, "start": 219.28, "end": 221.82, "text": " Having said that, some people spend a hell of a lot more and go very deep.", "tokens": [10222, 848, 300, 11, 512, 561, 3496, 257, 4921, 295, 257, 688, 544, 293, 352, 588, 2452, 13], "temperature": 0.0, "avg_logprob": -0.13518917149510876, "compression_ratio": 1.59765625, "no_speech_prob": 4.092754807061283e-06}, {"id": 47, "seek": 21768, "start": 221.82, "end": 226.16, "text": " Some people will spend a whole year, you know, sabbatical studying Practical Deep Learning", "tokens": [2188, 561, 486, 3496, 257, 1379, 1064, 11, 291, 458, 11, 5560, 11980, 804, 7601, 19170, 804, 14895, 15205], "temperature": 0.0, "avg_logprob": -0.13518917149510876, "compression_ratio": 1.59765625, "no_speech_prob": 4.092754807061283e-06}, {"id": 48, "seek": 21768, "start": 226.16, "end": 230.62, "text": " for Coders in order to really fully understand everything.", "tokens": [337, 383, 378, 433, 294, 1668, 281, 534, 4498, 1223, 1203, 13], "temperature": 0.0, "avg_logprob": -0.13518917149510876, "compression_ratio": 1.59765625, "no_speech_prob": 4.092754807061283e-06}, {"id": 49, "seek": 21768, "start": 230.62, "end": 233.68, "text": " So really it's up to you as to how deep you go.", "tokens": [407, 534, 309, 311, 493, 281, 291, 382, 281, 577, 2452, 291, 352, 13], "temperature": 0.0, "avg_logprob": -0.13518917149510876, "compression_ratio": 1.59765625, "no_speech_prob": 4.092754807061283e-06}, {"id": 50, "seek": 21768, "start": 233.68, "end": 235.34, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.13518917149510876, "compression_ratio": 1.59765625, "no_speech_prob": 4.092754807061283e-06}, {"id": 51, "seek": 21768, "start": 235.34, "end": 239.54000000000002, "text": " So with that said, let's jump into it.", "tokens": [407, 365, 300, 848, 11, 718, 311, 3012, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.13518917149510876, "compression_ratio": 1.59765625, "no_speech_prob": 4.092754807061283e-06}, {"id": 52, "seek": 21768, "start": 239.54000000000002, "end": 245.36, "text": " And as I said, the first part, we're going to be playing around with stable diffusion.", "tokens": [400, 382, 286, 848, 11, 264, 700, 644, 11, 321, 434, 516, 281, 312, 2433, 926, 365, 8351, 25242, 13], "temperature": 0.0, "avg_logprob": -0.13518917149510876, "compression_ratio": 1.59765625, "no_speech_prob": 4.092754807061283e-06}, {"id": 53, "seek": 24536, "start": 245.36, "end": 254.32000000000002, "text": " And I tried to prepare this as late as possible so it wouldn't be out of date.", "tokens": [400, 286, 3031, 281, 5940, 341, 382, 3469, 382, 1944, 370, 309, 2759, 380, 312, 484, 295, 4002, 13], "temperature": 0.0, "avg_logprob": -0.11313417525518508, "compression_ratio": 1.6846473029045643, "no_speech_prob": 4.565398739941884e-06}, {"id": 54, "seek": 24536, "start": 254.32000000000002, "end": 260.36, "text": " Unfortunately, as of 12 hours ago, it is now out of date.", "tokens": [8590, 11, 382, 295, 2272, 2496, 2057, 11, 309, 307, 586, 484, 295, 4002, 13], "temperature": 0.0, "avg_logprob": -0.11313417525518508, "compression_ratio": 1.6846473029045643, "no_speech_prob": 4.565398739941884e-06}, {"id": 55, "seek": 24536, "start": 260.36, "end": 264.04, "text": " And this is one of the big issues with the bit I'm about to describe, which is like how", "tokens": [400, 341, 307, 472, 295, 264, 955, 2663, 365, 264, 857, 286, 478, 466, 281, 6786, 11, 597, 307, 411, 577], "temperature": 0.0, "avg_logprob": -0.11313417525518508, "compression_ratio": 1.6846473029045643, "no_speech_prob": 4.565398739941884e-06}, {"id": 56, "seek": 24536, "start": 264.04, "end": 269.16, "text": " to play with stable diffusion and exactly how do the details work, which is it's moving", "tokens": [281, 862, 365, 8351, 25242, 293, 2293, 577, 360, 264, 4365, 589, 11, 597, 307, 309, 311, 2684], "temperature": 0.0, "avg_logprob": -0.11313417525518508, "compression_ratio": 1.6846473029045643, "no_speech_prob": 4.565398739941884e-06}, {"id": 57, "seek": 24536, "start": 269.16, "end": 274.8, "text": " so quickly that all of the details I'm going to describe to you today and all of the software", "tokens": [370, 2661, 300, 439, 295, 264, 4365, 286, 478, 516, 281, 6786, 281, 291, 965, 293, 439, 295, 264, 4722], "temperature": 0.0, "avg_logprob": -0.11313417525518508, "compression_ratio": 1.6846473029045643, "no_speech_prob": 4.565398739941884e-06}, {"id": 58, "seek": 27480, "start": 274.8, "end": 277.92, "text": " I'm going to show you today, by the time you watch this, if you're watching it, so what", "tokens": [286, 478, 516, 281, 855, 291, 965, 11, 538, 264, 565, 291, 1159, 341, 11, 498, 291, 434, 1976, 309, 11, 370, 437], "temperature": 0.0, "avg_logprob": -0.13402781570166872, "compression_ratio": 1.6820083682008369, "no_speech_prob": 8.26702034828486e-06}, {"id": 59, "seek": 27480, "start": 277.92, "end": 279.08, "text": " is it we're up to now?", "tokens": [307, 309, 321, 434, 493, 281, 586, 30], "temperature": 0.0, "avg_logprob": -0.13402781570166872, "compression_ratio": 1.6820083682008369, "no_speech_prob": 8.26702034828486e-06}, {"id": 60, "seek": 27480, "start": 279.08, "end": 281.64, "text": " It's 11th of October.", "tokens": [467, 311, 2975, 392, 295, 7617, 13], "temperature": 0.0, "avg_logprob": -0.13402781570166872, "compression_ratio": 1.6820083682008369, "no_speech_prob": 8.26702034828486e-06}, {"id": 61, "seek": 27480, "start": 281.64, "end": 288.28000000000003, "text": " So if you're watching this in like December of 2022 or watching this in 2023, the details", "tokens": [407, 498, 291, 434, 1976, 341, 294, 411, 7687, 295, 20229, 420, 1976, 341, 294, 44377, 11, 264, 4365], "temperature": 0.0, "avg_logprob": -0.13402781570166872, "compression_ratio": 1.6820083682008369, "no_speech_prob": 8.26702034828486e-06}, {"id": 62, "seek": 27480, "start": 288.28000000000003, "end": 290.5, "text": " will have changed.", "tokens": [486, 362, 3105, 13], "temperature": 0.0, "avg_logprob": -0.13402781570166872, "compression_ratio": 1.6820083682008369, "no_speech_prob": 8.26702034828486e-06}, {"id": 63, "seek": 27480, "start": 290.5, "end": 297.04, "text": " So what's happened today in the last 24 hours is two papers have come out.", "tokens": [407, 437, 311, 2011, 965, 294, 264, 1036, 4022, 2496, 307, 732, 10577, 362, 808, 484, 13], "temperature": 0.0, "avg_logprob": -0.13402781570166872, "compression_ratio": 1.6820083682008369, "no_speech_prob": 8.26702034828486e-06}, {"id": 64, "seek": 27480, "start": 297.04, "end": 301.24, "text": " So what I was going to be telling you today is, for example, to do a stable diffusion", "tokens": [407, 437, 286, 390, 516, 281, 312, 3585, 291, 965, 307, 11, 337, 1365, 11, 281, 360, 257, 8351, 25242], "temperature": 0.0, "avg_logprob": -0.13402781570166872, "compression_ratio": 1.6820083682008369, "no_speech_prob": 8.26702034828486e-06}, {"id": 65, "seek": 30124, "start": 301.24, "end": 308.28000000000003, "text": " generative model, the number of steps required has gone down from a thousand to about 40", "tokens": [1337, 1166, 2316, 11, 264, 1230, 295, 4439, 4739, 575, 2780, 760, 490, 257, 4714, 281, 466, 3356], "temperature": 0.0, "avg_logprob": -0.19529748891855214, "compression_ratio": 1.4554455445544554, "no_speech_prob": 6.338970706565306e-06}, {"id": 66, "seek": 30124, "start": 308.28000000000003, "end": 309.56, "text": " or 50.", "tokens": [420, 2625, 13], "temperature": 0.0, "avg_logprob": -0.19529748891855214, "compression_ratio": 1.4554455445544554, "no_speech_prob": 6.338970706565306e-06}, {"id": 67, "seek": 30124, "start": 309.56, "end": 316.76, "text": " But then as of last night, papers just come out that saying it's now down to four and", "tokens": [583, 550, 382, 295, 1036, 1818, 11, 10577, 445, 808, 484, 300, 1566, 309, 311, 586, 760, 281, 1451, 293], "temperature": 0.0, "avg_logprob": -0.19529748891855214, "compression_ratio": 1.4554455445544554, "no_speech_prob": 6.338970706565306e-06}, {"id": 68, "seek": 30124, "start": 316.76, "end": 319.12, "text": " it's 256 times faster.", "tokens": [309, 311, 38882, 1413, 4663, 13], "temperature": 0.0, "avg_logprob": -0.19529748891855214, "compression_ratio": 1.4554455445544554, "no_speech_prob": 6.338970706565306e-06}, {"id": 69, "seek": 30124, "start": 319.12, "end": 325.72, "text": " And another paper has come out with a separate, I think, orthogonal approach, which makes", "tokens": [400, 1071, 3035, 575, 808, 484, 365, 257, 4994, 11, 286, 519, 11, 41488, 3109, 11, 597, 1669], "temperature": 0.0, "avg_logprob": -0.19529748891855214, "compression_ratio": 1.4554455445544554, "no_speech_prob": 6.338970706565306e-06}, {"id": 70, "seek": 32572, "start": 325.72, "end": 332.56, "text": " it another, let's see, 10 to 20 times faster.", "tokens": [309, 1071, 11, 718, 311, 536, 11, 1266, 281, 945, 1413, 4663, 13], "temperature": 0.0, "avg_logprob": -0.10404248859571374, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.7263924948783824e-06}, {"id": 71, "seek": 32572, "start": 332.56, "end": 336.56, "text": " So things are very exciting.", "tokens": [407, 721, 366, 588, 4670, 13], "temperature": 0.0, "avg_logprob": -0.10404248859571374, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.7263924948783824e-06}, {"id": 72, "seek": 32572, "start": 336.56, "end": 338.36, "text": " Things are moving very quickly.", "tokens": [9514, 366, 2684, 588, 2661, 13], "temperature": 0.0, "avg_logprob": -0.10404248859571374, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.7263924948783824e-06}, {"id": 73, "seek": 32572, "start": 338.36, "end": 345.68, "text": " Now having said that, don't worry, because after this lesson, we're going to be going", "tokens": [823, 1419, 848, 300, 11, 500, 380, 3292, 11, 570, 934, 341, 6898, 11, 321, 434, 516, 281, 312, 516], "temperature": 0.0, "avg_logprob": -0.10404248859571374, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.7263924948783824e-06}, {"id": 74, "seek": 32572, "start": 345.68, "end": 349.68, "text": " from the foundations, which means we're going to be learning all the things of how these", "tokens": [490, 264, 22467, 11, 597, 1355, 321, 434, 516, 281, 312, 2539, 439, 264, 721, 295, 577, 613], "temperature": 0.0, "avg_logprob": -0.10404248859571374, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.7263924948783824e-06}, {"id": 75, "seek": 32572, "start": 349.68, "end": 351.44000000000005, "text": " are built up.", "tokens": [366, 3094, 493, 13], "temperature": 0.0, "avg_logprob": -0.10404248859571374, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.7263924948783824e-06}, {"id": 76, "seek": 32572, "start": 351.44000000000005, "end": 354.68, "text": " And those don't change much at all.", "tokens": [400, 729, 500, 380, 1319, 709, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.10404248859571374, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.7263924948783824e-06}, {"id": 77, "seek": 35468, "start": 354.68, "end": 358.92, "text": " And in fact, a lot of what we'll be seeing is extremely similar to another course we", "tokens": [400, 294, 1186, 11, 257, 688, 295, 437, 321, 603, 312, 2577, 307, 4664, 2531, 281, 1071, 1164, 321], "temperature": 0.0, "avg_logprob": -0.11800206791270863, "compression_ratio": 1.721774193548387, "no_speech_prob": 1.2028724086121656e-05}, {"id": 78, "seek": 35468, "start": 358.92, "end": 362.36, "text": " did in 2019, because the foundations don't change.", "tokens": [630, 294, 6071, 11, 570, 264, 22467, 500, 380, 1319, 13], "temperature": 0.0, "avg_logprob": -0.11800206791270863, "compression_ratio": 1.721774193548387, "no_speech_prob": 1.2028724086121656e-05}, {"id": 79, "seek": 35468, "start": 362.36, "end": 370.2, "text": " And once you know the foundations, these kinds of details that you'll find in these papers,", "tokens": [400, 1564, 291, 458, 264, 22467, 11, 613, 3685, 295, 4365, 300, 291, 603, 915, 294, 613, 10577, 11], "temperature": 0.0, "avg_logprob": -0.11800206791270863, "compression_ratio": 1.721774193548387, "no_speech_prob": 1.2028724086121656e-05}, {"id": 80, "seek": 35468, "start": 370.2, "end": 373.44, "text": " you'll be like, oh, I see, they did all these things the same way as usual and they made", "tokens": [291, 603, 312, 411, 11, 1954, 11, 286, 536, 11, 436, 630, 439, 613, 721, 264, 912, 636, 382, 7713, 293, 436, 1027], "temperature": 0.0, "avg_logprob": -0.11800206791270863, "compression_ratio": 1.721774193548387, "no_speech_prob": 1.2028724086121656e-05}, {"id": 81, "seek": 35468, "start": 373.44, "end": 375.6, "text": " this little change.", "tokens": [341, 707, 1319, 13], "temperature": 0.0, "avg_logprob": -0.11800206791270863, "compression_ratio": 1.721774193548387, "no_speech_prob": 1.2028724086121656e-05}, {"id": 82, "seek": 35468, "start": 375.6, "end": 380.22, "text": " So that's why we do things from the foundations so that you can keep up with the research,", "tokens": [407, 300, 311, 983, 321, 360, 721, 490, 264, 22467, 370, 300, 291, 393, 1066, 493, 365, 264, 2132, 11], "temperature": 0.0, "avg_logprob": -0.11800206791270863, "compression_ratio": 1.721774193548387, "no_speech_prob": 1.2028724086121656e-05}, {"id": 83, "seek": 38022, "start": 380.22, "end": 387.84000000000003, "text": " do your own research by taking advantage of this foundational knowledge, which all these", "tokens": [360, 428, 1065, 2132, 538, 1940, 5002, 295, 341, 32195, 3601, 11, 597, 439, 613], "temperature": 0.0, "avg_logprob": -0.14503740653013572, "compression_ratio": 1.4811320754716981, "no_speech_prob": 2.0579434476530878e-06}, {"id": 84, "seek": 38022, "start": 387.84000000000003, "end": 391.02000000000004, "text": " papers are building on top of.", "tokens": [10577, 366, 2390, 322, 1192, 295, 13], "temperature": 0.0, "avg_logprob": -0.14503740653013572, "compression_ratio": 1.4811320754716981, "no_speech_prob": 2.0579434476530878e-06}, {"id": 85, "seek": 38022, "start": 391.02000000000004, "end": 397.1, "text": " So anyway, I guess I should apologize that even as I record this, the notebook is now", "tokens": [407, 4033, 11, 286, 2041, 286, 820, 12328, 300, 754, 382, 286, 2136, 341, 11, 264, 21060, 307, 586], "temperature": 0.0, "avg_logprob": -0.14503740653013572, "compression_ratio": 1.4811320754716981, "no_speech_prob": 2.0579434476530878e-06}, {"id": 86, "seek": 38022, "start": 397.1, "end": 400.26000000000005, "text": " one day out of date.", "tokens": [472, 786, 484, 295, 4002, 13], "temperature": 0.0, "avg_logprob": -0.14503740653013572, "compression_ratio": 1.4811320754716981, "no_speech_prob": 2.0579434476530878e-06}, {"id": 87, "seek": 38022, "start": 400.26000000000005, "end": 408.0, "text": " So in part one, you might remember we saw this stuff of Dali 2 illustrations of Twitter", "tokens": [407, 294, 644, 472, 11, 291, 1062, 1604, 321, 1866, 341, 1507, 295, 413, 5103, 568, 34540, 295, 5794], "temperature": 0.0, "avg_logprob": -0.14503740653013572, "compression_ratio": 1.4811320754716981, "no_speech_prob": 2.0579434476530878e-06}, {"id": 88, "seek": 40800, "start": 408.0, "end": 415.92, "text": " bios, which really are pretty cool.", "tokens": [36997, 11, 597, 534, 366, 1238, 1627, 13], "temperature": 0.0, "avg_logprob": -0.14317266625094127, "compression_ratio": 1.572139303482587, "no_speech_prob": 9.079481060325634e-06}, {"id": 89, "seek": 40800, "start": 415.92, "end": 421.96, "text": " So the cool thing is that we're now at a point we can build this stuff ourselves and run", "tokens": [407, 264, 1627, 551, 307, 300, 321, 434, 586, 412, 257, 935, 321, 393, 1322, 341, 1507, 4175, 293, 1190], "temperature": 0.0, "avg_logprob": -0.14317266625094127, "compression_ratio": 1.572139303482587, "no_speech_prob": 9.079481060325634e-06}, {"id": 90, "seek": 40800, "start": 421.96, "end": 422.96, "text": " this stuff ourselves.", "tokens": [341, 1507, 4175, 13], "temperature": 0.0, "avg_logprob": -0.14317266625094127, "compression_ratio": 1.572139303482587, "no_speech_prob": 9.079481060325634e-06}, {"id": 91, "seek": 40800, "start": 422.96, "end": 424.84, "text": " We won't actually be using this particular model.", "tokens": [492, 1582, 380, 767, 312, 1228, 341, 1729, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14317266625094127, "compression_ratio": 1.572139303482587, "no_speech_prob": 9.079481060325634e-06}, {"id": 92, "seek": 40800, "start": 424.84, "end": 431.84, "text": " Dali 2 will be using a different model, stable diffusion, but has very similar outputs.", "tokens": [413, 5103, 568, 486, 312, 1228, 257, 819, 2316, 11, 8351, 25242, 11, 457, 575, 588, 2531, 23930, 13], "temperature": 0.0, "avg_logprob": -0.14317266625094127, "compression_ratio": 1.572139303482587, "no_speech_prob": 9.079481060325634e-06}, {"id": 93, "seek": 40800, "start": 431.84, "end": 435.32, "text": " But we can go even further now.", "tokens": [583, 321, 393, 352, 754, 3052, 586, 13], "temperature": 0.0, "avg_logprob": -0.14317266625094127, "compression_ratio": 1.572139303482587, "no_speech_prob": 9.079481060325634e-06}, {"id": 94, "seek": 43532, "start": 435.32, "end": 440.84, "text": " So one of our wonderful alumni, Alon, actually recently started a new company called, I don't", "tokens": [407, 472, 295, 527, 3715, 16347, 11, 967, 266, 11, 767, 3938, 1409, 257, 777, 2237, 1219, 11, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.16518222767373789, "compression_ratio": 1.4936170212765958, "no_speech_prob": 2.930185291916132e-05}, {"id": 95, "seek": 43532, "start": 440.84, "end": 445.8, "text": " know who said it, Strummer, where you can use something that we'll be learning about", "tokens": [458, 567, 848, 309, 11, 745, 6247, 936, 11, 689, 291, 393, 764, 746, 300, 321, 603, 312, 2539, 466], "temperature": 0.0, "avg_logprob": -0.16518222767373789, "compression_ratio": 1.4936170212765958, "no_speech_prob": 2.930185291916132e-05}, {"id": 96, "seek": 43532, "start": 445.8, "end": 454.64, "text": " today, called Dreambooth, to put any object, person, whatever, into an image.", "tokens": [965, 11, 1219, 12105, 1763, 900, 11, 281, 829, 604, 2657, 11, 954, 11, 2035, 11, 666, 364, 3256, 13], "temperature": 0.0, "avg_logprob": -0.16518222767373789, "compression_ratio": 1.4936170212765958, "no_speech_prob": 2.930185291916132e-05}, {"id": 97, "seek": 43532, "start": 454.64, "end": 462.12, "text": " And so he was kind enough to do a quick Dreambooth run for me and added these various pictures", "tokens": [400, 370, 415, 390, 733, 1547, 281, 360, 257, 1702, 12105, 1763, 900, 1190, 337, 385, 293, 3869, 613, 3683, 5242], "temperature": 0.0, "avg_logprob": -0.16518222767373789, "compression_ratio": 1.4936170212765958, "no_speech_prob": 2.930185291916132e-05}, {"id": 98, "seek": 46212, "start": 462.12, "end": 465.44, "text": " of me using his service.", "tokens": [295, 385, 1228, 702, 2643, 13], "temperature": 0.0, "avg_logprob": -0.18424766942074425, "compression_ratio": 1.47136563876652, "no_speech_prob": 9.663850505603477e-06}, {"id": 99, "seek": 46212, "start": 465.44, "end": 470.12, "text": " So here's a fun service you can try.", "tokens": [407, 510, 311, 257, 1019, 2643, 291, 393, 853, 13], "temperature": 0.0, "avg_logprob": -0.18424766942074425, "compression_ratio": 1.47136563876652, "no_speech_prob": 9.663850505603477e-06}, {"id": 100, "seek": 46212, "start": 470.12, "end": 475.2, "text": " One crazy one he tried was me as a dwarf, which I've got to say actually worked pretty", "tokens": [1485, 3219, 472, 415, 3031, 390, 385, 382, 257, 35527, 11, 597, 286, 600, 658, 281, 584, 767, 2732, 1238], "temperature": 0.0, "avg_logprob": -0.18424766942074425, "compression_ratio": 1.47136563876652, "no_speech_prob": 9.663850505603477e-06}, {"id": 101, "seek": 46212, "start": 475.2, "end": 476.98, "text": " well.", "tokens": [731, 13], "temperature": 0.0, "avg_logprob": -0.18424766942074425, "compression_ratio": 1.47136563876652, "no_speech_prob": 9.663850505603477e-06}, {"id": 102, "seek": 46212, "start": 476.98, "end": 479.32, "text": " This half looks like me, I reckon.", "tokens": [639, 1922, 1542, 411, 385, 11, 286, 29548, 13], "temperature": 0.0, "avg_logprob": -0.18424766942074425, "compression_ratio": 1.47136563876652, "no_speech_prob": 9.663850505603477e-06}, {"id": 103, "seek": 46212, "start": 479.32, "end": 482.88, "text": " And then the bottom bit is the dwarf version.", "tokens": [400, 550, 264, 2767, 857, 307, 264, 35527, 3037, 13], "temperature": 0.0, "avg_logprob": -0.18424766942074425, "compression_ratio": 1.47136563876652, "no_speech_prob": 9.663850505603477e-06}, {"id": 104, "seek": 46212, "start": 482.88, "end": 487.4, "text": " So thank you, Alon, and congratulations on your great progress since completing the FastAI", "tokens": [407, 1309, 291, 11, 967, 266, 11, 293, 13568, 322, 428, 869, 4205, 1670, 19472, 264, 15968, 48698], "temperature": 0.0, "avg_logprob": -0.18424766942074425, "compression_ratio": 1.47136563876652, "no_speech_prob": 9.663850505603477e-06}, {"id": 105, "seek": 46212, "start": 487.4, "end": 488.4, "text": " course.", "tokens": [1164, 13], "temperature": 0.0, "avg_logprob": -0.18424766942074425, "compression_ratio": 1.47136563876652, "no_speech_prob": 9.663850505603477e-06}, {"id": 106, "seek": 48840, "start": 488.4, "end": 495.76, "text": " I love it.", "tokens": [286, 959, 309, 13], "temperature": 0.0, "avg_logprob": -0.15727932112557547, "compression_ratio": 1.3693693693693694, "no_speech_prob": 6.9611223807442e-06}, {"id": 107, "seek": 48840, "start": 495.76, "end": 503.67999999999995, "text": " So something that's a bit different about this compared to previous courses, a lot of", "tokens": [407, 746, 300, 311, 257, 857, 819, 466, 341, 5347, 281, 3894, 7712, 11, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.15727932112557547, "compression_ratio": 1.3693693693693694, "no_speech_prob": 6.9611223807442e-06}, {"id": 108, "seek": 48840, "start": 503.67999999999995, "end": 511.23999999999995, "text": " previous courses, is this is no longer just a me thing.", "tokens": [3894, 7712, 11, 307, 341, 307, 572, 2854, 445, 257, 385, 551, 13], "temperature": 0.0, "avg_logprob": -0.15727932112557547, "compression_ratio": 1.3693693693693694, "no_speech_prob": 6.9611223807442e-06}, {"id": 109, "seek": 51124, "start": 511.24, "end": 519.08, "text": " Because this is moving so quickly, I've needed to get a lot of help to even vaguely get up", "tokens": [1436, 341, 307, 2684, 370, 2661, 11, 286, 600, 2978, 281, 483, 257, 688, 295, 854, 281, 754, 13501, 48863, 483, 493], "temperature": 0.0, "avg_logprob": -0.05520486160063408, "compression_ratio": 1.4278350515463918, "no_speech_prob": 6.539403784699971e-06}, {"id": 110, "seek": 51124, "start": 519.08, "end": 522.24, "text": " to date and stay up to date.", "tokens": [281, 4002, 293, 1754, 493, 281, 4002, 13], "temperature": 0.0, "avg_logprob": -0.05520486160063408, "compression_ratio": 1.4278350515463918, "no_speech_prob": 6.539403784699971e-06}, {"id": 111, "seek": 51124, "start": 522.24, "end": 528.08, "text": " So everything I'll be showing you today is very heavily influenced by extremely high", "tokens": [407, 1203, 286, 603, 312, 4099, 291, 965, 307, 588, 10950, 15269, 538, 4664, 1090], "temperature": 0.0, "avg_logprob": -0.05520486160063408, "compression_ratio": 1.4278350515463918, "no_speech_prob": 6.539403784699971e-06}, {"id": 112, "seek": 51124, "start": 528.08, "end": 535.36, "text": " levels of input from these amazing folks, all of whom are FastAI alumni.", "tokens": [4358, 295, 4846, 490, 613, 2243, 4024, 11, 439, 295, 7101, 366, 15968, 48698, 16347, 13], "temperature": 0.0, "avg_logprob": -0.05520486160063408, "compression_ratio": 1.4278350515463918, "no_speech_prob": 6.539403784699971e-06}, {"id": 113, "seek": 53536, "start": 535.36, "end": 546.36, "text": " So Jonathan Whitaker, who I saw in our chat, was basically the first guy to create detailed", "tokens": [407, 15471, 21693, 4003, 11, 567, 286, 1866, 294, 527, 5081, 11, 390, 1936, 264, 700, 2146, 281, 1884, 9942], "temperature": 0.0, "avg_logprob": -0.13529624437030993, "compression_ratio": 1.4695121951219512, "no_speech_prob": 1.1299263860564679e-05}, {"id": 114, "seek": 53536, "start": 546.36, "end": 552.32, "text": " educational material about stable diffusion and has been in the generative model space,", "tokens": [10189, 2527, 466, 8351, 25242, 293, 575, 668, 294, 264, 1337, 1166, 2316, 1901, 11], "temperature": 0.0, "avg_logprob": -0.13529624437030993, "compression_ratio": 1.4695121951219512, "no_speech_prob": 1.1299263860564679e-05}, {"id": 115, "seek": 53536, "start": 552.32, "end": 560.36, "text": " well, for a long time by stable diffusion standards, I guess.", "tokens": [731, 11, 337, 257, 938, 565, 538, 8351, 25242, 7787, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.13529624437030993, "compression_ratio": 1.4695121951219512, "no_speech_prob": 1.1299263860564679e-05}, {"id": 116, "seek": 56036, "start": 560.36, "end": 569.36, "text": " So Waseem has been an extraordinary contributor to all things FastAI.", "tokens": [407, 343, 651, 443, 575, 668, 364, 10581, 42859, 281, 439, 721, 15968, 48698, 13], "temperature": 0.0, "avg_logprob": -0.14461431368975572, "compression_ratio": 1.4840182648401827, "no_speech_prob": 4.1568855522200465e-06}, {"id": 117, "seek": 56036, "start": 569.36, "end": 577.88, "text": " Pedro came to San Francisco for the last time we did a part two course in 2019 and took", "tokens": [26662, 1361, 281, 5271, 12279, 337, 264, 1036, 565, 321, 630, 257, 644, 732, 1164, 294, 6071, 293, 1890], "temperature": 0.0, "avg_logprob": -0.14461431368975572, "compression_ratio": 1.4840182648401827, "no_speech_prob": 4.1568855522200465e-06}, {"id": 118, "seek": 56036, "start": 577.88, "end": 582.88, "text": " what he learned there and made his amazing Camera Plus software dramatically better and", "tokens": [437, 415, 3264, 456, 293, 1027, 702, 2243, 23734, 7721, 4722, 17548, 1101, 293], "temperature": 0.0, "avg_logprob": -0.14461431368975572, "compression_ratio": 1.4840182648401827, "no_speech_prob": 4.1568855522200465e-06}, {"id": 119, "seek": 56036, "start": 582.88, "end": 587.12, "text": " had it highlighted by Apple for the extraordinary machine learning stuff added.", "tokens": [632, 309, 17173, 538, 6373, 337, 264, 10581, 3479, 2539, 1507, 3869, 13], "temperature": 0.0, "avg_logprob": -0.14461431368975572, "compression_ratio": 1.4840182648401827, "no_speech_prob": 4.1568855522200465e-06}, {"id": 120, "seek": 58712, "start": 587.12, "end": 592.76, "text": " And he's now at Hacking Face working on the software that we'll be using a lot, Diffuses.", "tokens": [400, 415, 311, 586, 412, 389, 14134, 4047, 1364, 322, 264, 4722, 300, 321, 603, 312, 1228, 257, 688, 11, 413, 3661, 8355, 13], "temperature": 0.0, "avg_logprob": -0.17547372561782154, "compression_ratio": 1.4397905759162304, "no_speech_prob": 1.384096140100155e-05}, {"id": 121, "seek": 58712, "start": 592.76, "end": 601.2, "text": " And then Tanishq, everybody in the FastAI community probably already knows, now at StabilityAI", "tokens": [400, 550, 314, 7524, 80, 11, 2201, 294, 264, 15968, 48698, 1768, 1391, 1217, 3255, 11, 586, 412, 745, 2310, 48698], "temperature": 0.0, "avg_logprob": -0.17547372561782154, "compression_ratio": 1.4397905759162304, "no_speech_prob": 1.384096140100155e-05}, {"id": 122, "seek": 58712, "start": 601.2, "end": 608.8, "text": " working on stable diffusion models, his expertise particularly is in medical applications.", "tokens": [1364, 322, 8351, 25242, 5245, 11, 702, 11769, 4098, 307, 294, 4625, 5821, 13], "temperature": 0.0, "avg_logprob": -0.17547372561782154, "compression_ratio": 1.4397905759162304, "no_speech_prob": 1.384096140100155e-05}, {"id": 123, "seek": 60880, "start": 608.8, "end": 619.64, "text": " So really folks from all the key groups pretty much around stable diffusion and stuff are", "tokens": [407, 534, 4024, 490, 439, 264, 2141, 3935, 1238, 709, 926, 8351, 25242, 293, 1507, 366], "temperature": 0.0, "avg_logprob": -0.1157899538675944, "compression_ratio": 1.3877551020408163, "no_speech_prob": 2.8127940367994597e-06}, {"id": 124, "seek": 60880, "start": 619.64, "end": 621.4, "text": " working on this together.", "tokens": [1364, 322, 341, 1214, 13], "temperature": 0.0, "avg_logprob": -0.1157899538675944, "compression_ratio": 1.3877551020408163, "no_speech_prob": 2.8127940367994597e-06}, {"id": 125, "seek": 60880, "start": 621.4, "end": 628.92, "text": " And you'll also find some of these folks have recorded additional videos going into more", "tokens": [400, 291, 603, 611, 915, 512, 295, 613, 4024, 362, 8287, 4497, 2145, 516, 666, 544], "temperature": 0.0, "avg_logprob": -0.1157899538675944, "compression_ratio": 1.3877551020408163, "no_speech_prob": 2.8127940367994597e-06}, {"id": 126, "seek": 62892, "start": 628.92, "end": 639.4799999999999, "text": " detail about some of the areas which you will find on the course website.", "tokens": [2607, 466, 512, 295, 264, 3179, 597, 291, 486, 915, 322, 264, 1164, 3144, 13], "temperature": 0.0, "avg_logprob": -0.11565256118774414, "compression_ratio": 1.5182481751824817, "no_speech_prob": 9.367711754748598e-06}, {"id": 127, "seek": 62892, "start": 639.4799999999999, "end": 651.88, "text": " So make sure you go to course.fast.ai to get all the information about all the materials", "tokens": [407, 652, 988, 291, 352, 281, 1164, 13, 7011, 13, 1301, 281, 483, 439, 264, 1589, 466, 439, 264, 5319], "temperature": 0.0, "avg_logprob": -0.11565256118774414, "compression_ratio": 1.5182481751824817, "no_speech_prob": 9.367711754748598e-06}, {"id": 128, "seek": 62892, "start": 651.88, "end": 655.92, "text": " that you need to take full advantage of this.", "tokens": [300, 291, 643, 281, 747, 1577, 5002, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.11565256118774414, "compression_ratio": 1.5182481751824817, "no_speech_prob": 9.367711754748598e-06}, {"id": 129, "seek": 65592, "start": 655.92, "end": 666.68, "text": " So every lesson has links to notebooks and to details and so forth.", "tokens": [407, 633, 6898, 575, 6123, 281, 43782, 293, 281, 4365, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.15238297262857126, "compression_ratio": 1.282258064516129, "no_speech_prob": 5.954697826382471e-06}, {"id": 130, "seek": 65592, "start": 666.68, "end": 676.0799999999999, "text": " If you want to go even deeper, head over to forums.fast.ai into the part two 2022 category,", "tokens": [759, 291, 528, 281, 352, 754, 7731, 11, 1378, 670, 281, 26998, 13, 7011, 13, 1301, 666, 264, 644, 732, 20229, 7719, 11], "temperature": 0.0, "avg_logprob": -0.15238297262857126, "compression_ratio": 1.282258064516129, "no_speech_prob": 5.954697826382471e-06}, {"id": 131, "seek": 67608, "start": 676.08, "end": 687.6, "text": " hit on the About the Course button and you'll find that every lesson there's a chat with", "tokens": [2045, 322, 264, 7769, 264, 27327, 2960, 293, 291, 603, 915, 300, 633, 6898, 456, 311, 257, 5081, 365], "temperature": 0.0, "avg_logprob": -0.10125468571980795, "compression_ratio": 1.6130653266331658, "no_speech_prob": 1.7775833839550614e-05}, {"id": 132, "seek": 67608, "start": 687.6, "end": 689.88, "text": " even more stuff.", "tokens": [754, 544, 1507, 13], "temperature": 0.0, "avg_logprob": -0.10125468571980795, "compression_ratio": 1.6130653266331658, "no_speech_prob": 1.7775833839550614e-05}, {"id": 133, "seek": 67608, "start": 689.88, "end": 695.6800000000001, "text": " So look at this carefully to see all the things that me and the community have provided to", "tokens": [407, 574, 412, 341, 7500, 281, 536, 439, 264, 721, 300, 385, 293, 264, 1768, 362, 5649, 281], "temperature": 0.0, "avg_logprob": -0.10125468571980795, "compression_ratio": 1.6130653266331658, "no_speech_prob": 1.7775833839550614e-05}, {"id": 134, "seek": 67608, "start": 695.6800000000001, "end": 699.9200000000001, "text": " you to help you understand this video.", "tokens": [291, 281, 854, 291, 1223, 341, 960, 13], "temperature": 0.0, "avg_logprob": -0.10125468571980795, "compression_ratio": 1.6130653266331658, "no_speech_prob": 1.7775833839550614e-05}, {"id": 135, "seek": 67608, "start": 699.9200000000001, "end": 705.22, "text": " And also check out the questions underneath and answers underneath to see what people", "tokens": [400, 611, 1520, 484, 264, 1651, 7223, 293, 6338, 7223, 281, 536, 437, 561], "temperature": 0.0, "avg_logprob": -0.10125468571980795, "compression_ratio": 1.6130653266331658, "no_speech_prob": 1.7775833839550614e-05}, {"id": 136, "seek": 70522, "start": 705.22, "end": 706.22, "text": " have talked about.", "tokens": [362, 2825, 466, 13], "temperature": 0.0, "avg_logprob": -0.19476443926493328, "compression_ratio": 1.429530201342282, "no_speech_prob": 1.4508856111206114e-05}, {"id": 137, "seek": 70522, "start": 706.22, "end": 708.88, "text": " They can get a bit overwhelming.", "tokens": [814, 393, 483, 257, 857, 13373, 13], "temperature": 0.0, "avg_logprob": -0.19476443926493328, "compression_ratio": 1.429530201342282, "no_speech_prob": 1.4508856111206114e-05}, {"id": 138, "seek": 70522, "start": 708.88, "end": 721.72, "text": " So once they get big enough, you'll see that there's a summarize button that you can click", "tokens": [407, 1564, 436, 483, 955, 1547, 11, 291, 603, 536, 300, 456, 311, 257, 20858, 2960, 300, 291, 393, 2052], "temperature": 0.0, "avg_logprob": -0.19476443926493328, "compression_ratio": 1.429530201342282, "no_speech_prob": 1.4508856111206114e-05}, {"id": 139, "seek": 70522, "start": 721.72, "end": 724.2, "text": " to kind of see just the most liked parts.", "tokens": [281, 733, 295, 536, 445, 264, 881, 4501, 3166, 13], "temperature": 0.0, "avg_logprob": -0.19476443926493328, "compression_ratio": 1.429530201342282, "no_speech_prob": 1.4508856111206114e-05}, {"id": 140, "seek": 70522, "start": 724.2, "end": 728.2, "text": " So that can be very helpful.", "tokens": [407, 300, 393, 312, 588, 4961, 13], "temperature": 0.0, "avg_logprob": -0.19476443926493328, "compression_ratio": 1.429530201342282, "no_speech_prob": 1.4508856111206114e-05}, {"id": 141, "seek": 72820, "start": 728.2, "end": 735.88, "text": " So they're all important resources, I think, to get the most out of this course.", "tokens": [407, 436, 434, 439, 1021, 3593, 11, 286, 519, 11, 281, 483, 264, 881, 484, 295, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.13747072219848633, "compression_ratio": 1.5024875621890548, "no_speech_prob": 7.645742698514368e-06}, {"id": 142, "seek": 72820, "start": 735.88, "end": 738.96, "text": " Now, compute.", "tokens": [823, 11, 14722, 13], "temperature": 0.0, "avg_logprob": -0.13747072219848633, "compression_ratio": 1.5024875621890548, "no_speech_prob": 7.645742698514368e-06}, {"id": 143, "seek": 72820, "start": 738.96, "end": 747.84, "text": " So completing part two requires quite a bit more compute than part one.", "tokens": [407, 19472, 644, 732, 7029, 1596, 257, 857, 544, 14722, 813, 644, 472, 13], "temperature": 0.0, "avg_logprob": -0.13747072219848633, "compression_ratio": 1.5024875621890548, "no_speech_prob": 7.645742698514368e-06}, {"id": 144, "seek": 72820, "start": 747.84, "end": 749.76, "text": " Compute options are changing rapidly.", "tokens": [6620, 1169, 3956, 366, 4473, 12910, 13], "temperature": 0.0, "avg_logprob": -0.13747072219848633, "compression_ratio": 1.5024875621890548, "no_speech_prob": 7.645742698514368e-06}, {"id": 145, "seek": 72820, "start": 749.76, "end": 753.6, "text": " And to be honest, the main reason for that is because of the huge popularity of stable", "tokens": [400, 281, 312, 3245, 11, 264, 2135, 1778, 337, 300, 307, 570, 295, 264, 2603, 19301, 295, 8351], "temperature": 0.0, "avg_logprob": -0.13747072219848633, "compression_ratio": 1.5024875621890548, "no_speech_prob": 7.645742698514368e-06}, {"id": 146, "seek": 72820, "start": 753.6, "end": 755.32, "text": " diffusion.", "tokens": [25242, 13], "temperature": 0.0, "avg_logprob": -0.13747072219848633, "compression_ratio": 1.5024875621890548, "no_speech_prob": 7.645742698514368e-06}, {"id": 147, "seek": 75532, "start": 755.32, "end": 759.24, "text": " Everybody is taken to using Colab for stable diffusion.", "tokens": [7646, 307, 2726, 281, 1228, 4004, 455, 337, 8351, 25242, 13], "temperature": 0.0, "avg_logprob": -0.12304161071777343, "compression_ratio": 1.5757575757575757, "no_speech_prob": 2.84059569821693e-05}, {"id": 148, "seek": 75532, "start": 759.24, "end": 765.48, "text": " And Colab's response has been to start charging by the hour for most usage.", "tokens": [400, 4004, 455, 311, 4134, 575, 668, 281, 722, 11379, 538, 264, 1773, 337, 881, 14924, 13], "temperature": 0.0, "avg_logprob": -0.12304161071777343, "compression_ratio": 1.5757575757575757, "no_speech_prob": 2.84059569821693e-05}, {"id": 149, "seek": 75532, "start": 765.48, "end": 772.96, "text": " So you may well find if you're a Colab user, we still love Colab, you may find that you", "tokens": [407, 291, 815, 731, 915, 498, 291, 434, 257, 4004, 455, 4195, 11, 321, 920, 959, 4004, 455, 11, 291, 815, 915, 300, 291], "temperature": 0.0, "avg_logprob": -0.12304161071777343, "compression_ratio": 1.5757575757575757, "no_speech_prob": 2.84059569821693e-05}, {"id": 150, "seek": 75532, "start": 772.96, "end": 773.96, "text": " run out of...", "tokens": [1190, 484, 295, 485], "temperature": 0.0, "avg_logprob": -0.12304161071777343, "compression_ratio": 1.5757575757575757, "no_speech_prob": 2.84059569821693e-05}, {"id": 151, "seek": 75532, "start": 773.96, "end": 777.36, "text": " They start not giving you decent GPUs anymore.", "tokens": [814, 722, 406, 2902, 291, 8681, 18407, 82, 3602, 13], "temperature": 0.0, "avg_logprob": -0.12304161071777343, "compression_ratio": 1.5757575757575757, "no_speech_prob": 2.84059569821693e-05}, {"id": 152, "seek": 75532, "start": 777.36, "end": 784.2, "text": " And if you want to then upgrade, they limit quite a lot how many hours you can use.", "tokens": [400, 498, 291, 528, 281, 550, 11484, 11, 436, 4948, 1596, 257, 688, 577, 867, 2496, 291, 393, 764, 13], "temperature": 0.0, "avg_logprob": -0.12304161071777343, "compression_ratio": 1.5757575757575757, "no_speech_prob": 2.84059569821693e-05}, {"id": 153, "seek": 78420, "start": 784.2, "end": 787.32, "text": " So at the moment, yeah, still try Colab.", "tokens": [407, 412, 264, 1623, 11, 1338, 11, 920, 853, 4004, 455, 13], "temperature": 0.0, "avg_logprob": -0.2085798551451485, "compression_ratio": 1.5381355932203389, "no_speech_prob": 6.747876341250958e-06}, {"id": 154, "seek": 78420, "start": 787.32, "end": 788.32, "text": " They're pretty good.", "tokens": [814, 434, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.2085798551451485, "compression_ratio": 1.5381355932203389, "no_speech_prob": 6.747876341250958e-06}, {"id": 155, "seek": 78420, "start": 788.32, "end": 791.48, "text": " I mean, for free, you get some decent stuff.", "tokens": [286, 914, 11, 337, 1737, 11, 291, 483, 512, 8681, 1507, 13], "temperature": 0.0, "avg_logprob": -0.2085798551451485, "compression_ratio": 1.5381355932203389, "no_speech_prob": 6.747876341250958e-06}, {"id": 156, "seek": 78420, "start": 791.48, "end": 796.5, "text": " But I would strongly suggest trying out also Paper Space Gradient.", "tokens": [583, 286, 576, 10613, 3402, 1382, 484, 611, 24990, 8705, 16710, 1196, 13], "temperature": 0.0, "avg_logprob": -0.2085798551451485, "compression_ratio": 1.5381355932203389, "no_speech_prob": 6.747876341250958e-06}, {"id": 157, "seek": 78420, "start": 796.5, "end": 804.4000000000001, "text": " You can pay like $9 a month to actually get some pretty good GPUs there at the moment,", "tokens": [509, 393, 1689, 411, 1848, 24, 257, 1618, 281, 767, 483, 512, 1238, 665, 18407, 82, 456, 412, 264, 1623, 11], "temperature": 0.0, "avg_logprob": -0.2085798551451485, "compression_ratio": 1.5381355932203389, "no_speech_prob": 6.747876341250958e-06}, {"id": 158, "seek": 78420, "start": 804.4000000000001, "end": 807.0, "text": " or pay a bit more to get even better ones.", "tokens": [420, 1689, 257, 857, 544, 281, 483, 754, 1101, 2306, 13], "temperature": 0.0, "avg_logprob": -0.2085798551451485, "compression_ratio": 1.5381355932203389, "no_speech_prob": 6.747876341250958e-06}, {"id": 159, "seek": 78420, "start": 807.0, "end": 810.2, "text": " Again, but the thing is, this is all going to change a lot.", "tokens": [3764, 11, 457, 264, 551, 307, 11, 341, 307, 439, 516, 281, 1319, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.2085798551451485, "compression_ratio": 1.5381355932203389, "no_speech_prob": 6.747876341250958e-06}, {"id": 160, "seek": 81020, "start": 810.2, "end": 815.12, "text": " I don't know, maybe people will make Paper Space Gradient and have to change their pricing", "tokens": [286, 500, 380, 458, 11, 1310, 561, 486, 652, 24990, 8705, 16710, 1196, 293, 362, 281, 1319, 641, 17621], "temperature": 0.0, "avg_logprob": -0.14309131338241252, "compression_ratio": 1.5497835497835497, "no_speech_prob": 4.222680672683055e-06}, {"id": 161, "seek": 81020, "start": 815.12, "end": 816.12, "text": " too.", "tokens": [886, 13], "temperature": 0.0, "avg_logprob": -0.14309131338241252, "compression_ratio": 1.5497835497835497, "no_speech_prob": 4.222680672683055e-06}, {"id": 162, "seek": 81020, "start": 816.12, "end": 817.12, "text": " I don't know.", "tokens": [286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.14309131338241252, "compression_ratio": 1.5497835497835497, "no_speech_prob": 4.222680672683055e-06}, {"id": 163, "seek": 81020, "start": 817.12, "end": 822.6800000000001, "text": " So check course.fast.ai to find out what our current recommendations are.", "tokens": [407, 1520, 1164, 13, 7011, 13, 1301, 281, 915, 484, 437, 527, 2190, 10434, 366, 13], "temperature": 0.0, "avg_logprob": -0.14309131338241252, "compression_ratio": 1.5497835497835497, "no_speech_prob": 4.222680672683055e-06}, {"id": 164, "seek": 81020, "start": 822.6800000000001, "end": 826.12, "text": " Now Lambda Labs and Jarvis Labs are also both good options.", "tokens": [823, 45691, 40047, 293, 23941, 4938, 40047, 366, 611, 1293, 665, 3956, 13], "temperature": 0.0, "avg_logprob": -0.14309131338241252, "compression_ratio": 1.5497835497835497, "no_speech_prob": 4.222680672683055e-06}, {"id": 165, "seek": 81020, "start": 826.12, "end": 832.5600000000001, "text": " Jarvis was created by an alum of the course and has some just really fantastic options", "tokens": [23941, 4938, 390, 2942, 538, 364, 12064, 295, 264, 1164, 293, 575, 512, 445, 534, 5456, 3956], "temperature": 0.0, "avg_logprob": -0.14309131338241252, "compression_ratio": 1.5497835497835497, "no_speech_prob": 4.222680672683055e-06}, {"id": 166, "seek": 81020, "start": 832.5600000000001, "end": 835.24, "text": " at a very reasonable price.", "tokens": [412, 257, 588, 10585, 3218, 13], "temperature": 0.0, "avg_logprob": -0.14309131338241252, "compression_ratio": 1.5497835497835497, "no_speech_prob": 4.222680672683055e-06}, {"id": 167, "seek": 83524, "start": 835.24, "end": 840.5600000000001, "text": " And a lot of Fast.ai students use them and love them.", "tokens": [400, 257, 688, 295, 15968, 13, 1301, 1731, 764, 552, 293, 959, 552, 13], "temperature": 0.0, "avg_logprob": -0.13043085734049478, "compression_ratio": 1.5256410256410255, "no_speech_prob": 6.643162578257034e-06}, {"id": 168, "seek": 83524, "start": 840.5600000000001, "end": 848.8, "text": " And also check out Lambda Labs, who are the most recent provider on this page.", "tokens": [400, 611, 1520, 484, 45691, 40047, 11, 567, 366, 264, 881, 5162, 12398, 322, 341, 3028, 13], "temperature": 0.0, "avg_logprob": -0.13043085734049478, "compression_ratio": 1.5256410256410255, "no_speech_prob": 6.643162578257034e-06}, {"id": 169, "seek": 83524, "start": 848.8, "end": 852.6, "text": " And they are rapidly adding new features.", "tokens": [400, 436, 366, 12910, 5127, 777, 4122, 13], "temperature": 0.0, "avg_logprob": -0.13043085734049478, "compression_ratio": 1.5256410256410255, "no_speech_prob": 6.643162578257034e-06}, {"id": 170, "seek": 83524, "start": 852.6, "end": 856.44, "text": " But the reason I particularly wanted to mention them is at least as I say this, which they", "tokens": [583, 264, 1778, 286, 4098, 1415, 281, 2152, 552, 307, 412, 1935, 382, 286, 584, 341, 11, 597, 436], "temperature": 0.0, "avg_logprob": -0.13043085734049478, "compression_ratio": 1.5256410256410255, "no_speech_prob": 6.643162578257034e-06}, {"id": 171, "seek": 83524, "start": 856.44, "end": 864.72, "text": " say is early October 2022, they're the cheapest provider of kind of big GPUs that you might", "tokens": [584, 307, 2440, 7617, 20229, 11, 436, 434, 264, 29167, 12398, 295, 733, 295, 955, 18407, 82, 300, 291, 1062], "temperature": 0.0, "avg_logprob": -0.13043085734049478, "compression_ratio": 1.5256410256410255, "no_speech_prob": 6.643162578257034e-06}, {"id": 172, "seek": 86472, "start": 864.72, "end": 870.4, "text": " want to use to run like serious models.", "tokens": [528, 281, 764, 281, 1190, 411, 3156, 5245, 13], "temperature": 0.0, "avg_logprob": -0.18172677357991537, "compression_ratio": 1.5045871559633028, "no_speech_prob": 3.905316589225549e-06}, {"id": 173, "seek": 86472, "start": 870.4, "end": 874.1600000000001, "text": " So they're absolutely well worth checking out.", "tokens": [407, 436, 434, 3122, 731, 3163, 8568, 484, 13], "temperature": 0.0, "avg_logprob": -0.18172677357991537, "compression_ratio": 1.5045871559633028, "no_speech_prob": 3.905316589225549e-06}, {"id": 174, "seek": 86472, "start": 874.1600000000001, "end": 876.72, "text": " But as I say, this could all have changed by the time you watch this.", "tokens": [583, 382, 286, 584, 11, 341, 727, 439, 362, 3105, 538, 264, 565, 291, 1159, 341, 13], "temperature": 0.0, "avg_logprob": -0.18172677357991537, "compression_ratio": 1.5045871559633028, "no_speech_prob": 3.905316589225549e-06}, {"id": 175, "seek": 86472, "start": 876.72, "end": 879.2, "text": " So go and check out course.fast.ai.", "tokens": [407, 352, 293, 1520, 484, 1164, 13, 7011, 13, 1301, 13], "temperature": 0.0, "avg_logprob": -0.18172677357991537, "compression_ratio": 1.5045871559633028, "no_speech_prob": 3.905316589225549e-06}, {"id": 176, "seek": 86472, "start": 879.2, "end": 887.6800000000001, "text": " Also at the moment, late 2022, GPU prices have come down a lot and you may well want", "tokens": [2743, 412, 264, 1623, 11, 3469, 20229, 11, 18407, 7901, 362, 808, 760, 257, 688, 293, 291, 815, 731, 528], "temperature": 0.0, "avg_logprob": -0.18172677357991537, "compression_ratio": 1.5045871559633028, "no_speech_prob": 3.905316589225549e-06}, {"id": 177, "seek": 86472, "start": 887.6800000000001, "end": 893.48, "text": " to consider buying your own machine at this point.", "tokens": [281, 1949, 6382, 428, 1065, 3479, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.18172677357991537, "compression_ratio": 1.5045871559633028, "no_speech_prob": 3.905316589225549e-06}, {"id": 178, "seek": 89348, "start": 893.48, "end": 904.12, "text": " Okay, so what we're now going to do is jump into the notebooks.", "tokens": [1033, 11, 370, 437, 321, 434, 586, 516, 281, 360, 307, 3012, 666, 264, 43782, 13], "temperature": 0.0, "avg_logprob": -0.19262883996450772, "compression_ratio": 1.5693069306930694, "no_speech_prob": 1.3845499779563397e-05}, {"id": 179, "seek": 89348, "start": 904.12, "end": 914.4, "text": " And so there's a repo that we've linked to called Diffusion NBs, which isn't kind of", "tokens": [400, 370, 456, 311, 257, 49040, 300, 321, 600, 9408, 281, 1219, 413, 3661, 5704, 426, 33, 82, 11, 597, 1943, 380, 733, 295], "temperature": 0.0, "avg_logprob": -0.19262883996450772, "compression_ratio": 1.5693069306930694, "no_speech_prob": 1.3845499779563397e-05}, {"id": 180, "seek": 89348, "start": 914.4, "end": 915.64, "text": " the main course notebooks.", "tokens": [264, 2135, 1164, 43782, 13], "temperature": 0.0, "avg_logprob": -0.19262883996450772, "compression_ratio": 1.5693069306930694, "no_speech_prob": 1.3845499779563397e-05}, {"id": 181, "seek": 89348, "start": 915.64, "end": 917.32, "text": " It's not the From the Foundations notebooks.", "tokens": [467, 311, 406, 264, 3358, 264, 8207, 763, 43782, 13], "temperature": 0.0, "avg_logprob": -0.19262883996450772, "compression_ratio": 1.5693069306930694, "no_speech_prob": 1.3845499779563397e-05}, {"id": 182, "seek": 89348, "start": 917.32, "end": 920.2, "text": " It's just a couple of notebooks that you might want to play with.", "tokens": [467, 311, 445, 257, 1916, 295, 43782, 300, 291, 1062, 528, 281, 862, 365, 13], "temperature": 0.0, "avg_logprob": -0.19262883996450772, "compression_ratio": 1.5693069306930694, "no_speech_prob": 1.3845499779563397e-05}, {"id": 183, "seek": 89348, "start": 920.2, "end": 923.14, "text": " A bit of fun stuff to try out.", "tokens": [316, 857, 295, 1019, 1507, 281, 853, 484, 13], "temperature": 0.0, "avg_logprob": -0.19262883996450772, "compression_ratio": 1.5693069306930694, "no_speech_prob": 1.3845499779563397e-05}, {"id": 184, "seek": 92314, "start": 923.14, "end": 929.08, "text": " One of the interesting things here is Jonathan Wittaker, who I tend to call Jono.", "tokens": [1485, 295, 264, 1880, 721, 510, 307, 15471, 343, 593, 4003, 11, 567, 286, 3928, 281, 818, 7745, 78, 13], "temperature": 0.0, "avg_logprob": -0.10994212366953618, "compression_ratio": 1.6877470355731226, "no_speech_prob": 1.544540464237798e-05}, {"id": 185, "seek": 92314, "start": 929.08, "end": 932.4, "text": " So if I say Jono, that's who I'm referring to, has created this really interesting thing", "tokens": [407, 498, 286, 584, 7745, 78, 11, 300, 311, 567, 286, 478, 13761, 281, 11, 575, 2942, 341, 534, 1880, 551], "temperature": 0.0, "avg_logprob": -0.10994212366953618, "compression_ratio": 1.6877470355731226, "no_speech_prob": 1.544540464237798e-05}, {"id": 186, "seek": 92314, "start": 932.4, "end": 936.3199999999999, "text": " called SuggestedTools.md, which hopefully he'll keep up to date.", "tokens": [1219, 39131, 2629, 292, 51, 29298, 13, 76, 67, 11, 597, 4696, 415, 603, 1066, 493, 281, 4002, 13], "temperature": 0.0, "avg_logprob": -0.10994212366953618, "compression_ratio": 1.6877470355731226, "no_speech_prob": 1.544540464237798e-05}, {"id": 187, "seek": 92314, "start": 936.3199999999999, "end": 940.8, "text": " So even if you come here later, this will still be up to date.", "tokens": [407, 754, 498, 291, 808, 510, 1780, 11, 341, 486, 920, 312, 493, 281, 4002, 13], "temperature": 0.0, "avg_logprob": -0.10994212366953618, "compression_ratio": 1.6877470355731226, "no_speech_prob": 1.544540464237798e-05}, {"id": 188, "seek": 92314, "start": 940.8, "end": 946.88, "text": " Because he knows so much about this area, he's been able to pull out some of the best", "tokens": [1436, 415, 3255, 370, 709, 466, 341, 1859, 11, 415, 311, 668, 1075, 281, 2235, 484, 512, 295, 264, 1151], "temperature": 0.0, "avg_logprob": -0.10994212366953618, "compression_ratio": 1.6877470355731226, "no_speech_prob": 1.544540464237798e-05}, {"id": 189, "seek": 92314, "start": 946.88, "end": 951.5, "text": " stuff out there for just starting to play.", "tokens": [1507, 484, 456, 337, 445, 2891, 281, 862, 13], "temperature": 0.0, "avg_logprob": -0.10994212366953618, "compression_ratio": 1.6877470355731226, "no_speech_prob": 1.544540464237798e-05}, {"id": 190, "seek": 95150, "start": 951.5, "end": 956.52, "text": " And I think it's actually important to play, because that way you can really understand", "tokens": [400, 286, 519, 309, 311, 767, 1021, 281, 862, 11, 570, 300, 636, 291, 393, 534, 1223], "temperature": 0.0, "avg_logprob": -0.14025710465072036, "compression_ratio": 1.5742574257425743, "no_speech_prob": 4.495047960517695e-06}, {"id": 191, "seek": 95150, "start": 956.52, "end": 959.68, "text": " what the capabilities are and what the constraints are.", "tokens": [437, 264, 10862, 366, 293, 437, 264, 18491, 366, 13], "temperature": 0.0, "avg_logprob": -0.14025710465072036, "compression_ratio": 1.5742574257425743, "no_speech_prob": 4.495047960517695e-06}, {"id": 192, "seek": 95150, "start": 959.68, "end": 964.08, "text": " So then you can think about, well, what could you do with that?", "tokens": [407, 550, 291, 393, 519, 466, 11, 731, 11, 437, 727, 291, 360, 365, 300, 30], "temperature": 0.0, "avg_logprob": -0.14025710465072036, "compression_ratio": 1.5742574257425743, "no_speech_prob": 4.495047960517695e-06}, {"id": 193, "seek": 95150, "start": 964.08, "end": 967.8, "text": " And also, what kind of research opportunities might there be?", "tokens": [400, 611, 11, 437, 733, 295, 2132, 4786, 1062, 456, 312, 30], "temperature": 0.0, "avg_logprob": -0.14025710465072036, "compression_ratio": 1.5742574257425743, "no_speech_prob": 4.495047960517695e-06}, {"id": 194, "seek": 95150, "start": 967.8, "end": 970.88, "text": " So I'd strongly suggest trying out these things.", "tokens": [407, 286, 1116, 10613, 3402, 1382, 484, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.14025710465072036, "compression_ratio": 1.5742574257425743, "no_speech_prob": 4.495047960517695e-06}, {"id": 195, "seek": 97088, "start": 970.88, "end": 981.92, "text": " The community on the whole has moved towards making things available as Colab notebooks.", "tokens": [440, 1768, 322, 264, 1379, 575, 4259, 3030, 1455, 721, 2435, 382, 4004, 455, 43782, 13], "temperature": 0.0, "avg_logprob": -0.15171616652916217, "compression_ratio": 1.4387096774193548, "no_speech_prob": 2.769389539025724e-06}, {"id": 196, "seek": 97088, "start": 981.92, "end": 992.76, "text": " So if I click, for example, on this one, deforum, and they often have this kind of hacker aesthetic", "tokens": [407, 498, 286, 2052, 11, 337, 1365, 11, 322, 341, 472, 11, 368, 2994, 449, 11, 293, 436, 2049, 362, 341, 733, 295, 38155, 20092], "temperature": 0.0, "avg_logprob": -0.15171616652916217, "compression_ratio": 1.4387096774193548, "no_speech_prob": 2.769389539025724e-06}, {"id": 197, "seek": 97088, "start": 992.76, "end": 999.72, "text": " around them, which is kind of fun.", "tokens": [926, 552, 11, 597, 307, 733, 295, 1019, 13], "temperature": 0.0, "avg_logprob": -0.15171616652916217, "compression_ratio": 1.4387096774193548, "no_speech_prob": 2.769389539025724e-06}, {"id": 198, "seek": 99972, "start": 999.72, "end": 1006.1600000000001, "text": " So what happens is they add lots and lots of features, and you can basically just fill", "tokens": [407, 437, 2314, 307, 436, 909, 3195, 293, 3195, 295, 4122, 11, 293, 291, 393, 1936, 445, 2836], "temperature": 0.0, "avg_logprob": -0.16640132150532286, "compression_ratio": 1.6162162162162161, "no_speech_prob": 1.191088472296542e-06}, {"id": 199, "seek": 99972, "start": 1006.1600000000001, "end": 1011.64, "text": " in this stuff to try things.", "tokens": [294, 341, 1507, 281, 853, 721, 13], "temperature": 0.0, "avg_logprob": -0.16640132150532286, "compression_ratio": 1.6162162162162161, "no_speech_prob": 1.191088472296542e-06}, {"id": 200, "seek": 99972, "start": 1011.64, "end": 1014.6800000000001, "text": " And they often have a few examples.", "tokens": [400, 436, 2049, 362, 257, 1326, 5110, 13], "temperature": 0.0, "avg_logprob": -0.16640132150532286, "compression_ratio": 1.6162162162162161, "no_speech_prob": 1.191088472296542e-06}, {"id": 201, "seek": 99972, "start": 1014.6800000000001, "end": 1020.9200000000001, "text": " And so you can hit up the runtime and say change runtime type to make sure it says GPU.", "tokens": [400, 370, 291, 393, 2045, 493, 264, 34474, 293, 584, 1319, 34474, 2010, 281, 652, 988, 309, 1619, 18407, 13], "temperature": 0.0, "avg_logprob": -0.16640132150532286, "compression_ratio": 1.6162162162162161, "no_speech_prob": 1.191088472296542e-06}, {"id": 202, "seek": 99972, "start": 1020.9200000000001, "end": 1027.32, "text": " And you can say what kind of GPU.", "tokens": [400, 291, 393, 584, 437, 733, 295, 18407, 13], "temperature": 0.0, "avg_logprob": -0.16640132150532286, "compression_ratio": 1.6162162162162161, "no_speech_prob": 1.191088472296542e-06}, {"id": 203, "seek": 99972, "start": 1027.32, "end": 1028.64, "text": " And start running things.", "tokens": [400, 722, 2614, 721, 13], "temperature": 0.0, "avg_logprob": -0.16640132150532286, "compression_ratio": 1.6162162162162161, "no_speech_prob": 1.191088472296542e-06}, {"id": 204, "seek": 102864, "start": 1028.64, "end": 1034.48, "text": " Now, a lot of the folks who use this stuff honestly have no idea what any of these things", "tokens": [823, 11, 257, 688, 295, 264, 4024, 567, 764, 341, 1507, 6095, 362, 572, 1558, 437, 604, 295, 613, 721], "temperature": 0.0, "avg_logprob": -0.09924817316740461, "compression_ratio": 1.6991525423728813, "no_speech_prob": 7.411056685668882e-06}, {"id": 205, "seek": 102864, "start": 1034.48, "end": 1035.48, "text": " mean.", "tokens": [914, 13], "temperature": 0.0, "avg_logprob": -0.09924817316740461, "compression_ratio": 1.6991525423728813, "no_speech_prob": 7.411056685668882e-06}, {"id": 206, "seek": 102864, "start": 1035.48, "end": 1041.2800000000002, "text": " Now, by the end of the course, you'll know what all of these things mean, pretty much.", "tokens": [823, 11, 538, 264, 917, 295, 264, 1164, 11, 291, 603, 458, 437, 439, 295, 613, 721, 914, 11, 1238, 709, 13], "temperature": 0.0, "avg_logprob": -0.09924817316740461, "compression_ratio": 1.6991525423728813, "no_speech_prob": 7.411056685668882e-06}, {"id": 207, "seek": 102864, "start": 1041.2800000000002, "end": 1046.76, "text": " And that will help you to make great outputs from stuff like this.", "tokens": [400, 300, 486, 854, 291, 281, 652, 869, 23930, 490, 1507, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.09924817316740461, "compression_ratio": 1.6991525423728813, "no_speech_prob": 7.411056685668882e-06}, {"id": 208, "seek": 102864, "start": 1046.76, "end": 1051.24, "text": " But you can create great outputs just using more of an artisanal approach.", "tokens": [583, 291, 393, 1884, 869, 23930, 445, 1228, 544, 295, 364, 1523, 14804, 304, 3109, 13], "temperature": 0.0, "avg_logprob": -0.09924817316740461, "compression_ratio": 1.6991525423728813, "no_speech_prob": 7.411056685668882e-06}, {"id": 209, "seek": 102864, "start": 1051.24, "end": 1058.6000000000001, "text": " There's lots of information online about what kinds of things could you try.", "tokens": [821, 311, 3195, 295, 1589, 2950, 466, 437, 3685, 295, 721, 727, 291, 853, 13], "temperature": 0.0, "avg_logprob": -0.09924817316740461, "compression_ratio": 1.6991525423728813, "no_speech_prob": 7.411056685668882e-06}, {"id": 210, "seek": 105860, "start": 1058.6, "end": 1062.84, "text": " So anyway, check out this stuff from Jono.", "tokens": [407, 4033, 11, 1520, 484, 341, 1507, 490, 7745, 78, 13], "temperature": 0.0, "avg_logprob": -0.16913665070825692, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.4509836546494626e-05}, {"id": 211, "seek": 105860, "start": 1062.84, "end": 1072.6, "text": " And then he also links to this fantastic resource from Pharma Psychotic, which is a rather overwhelming", "tokens": [400, 550, 415, 611, 6123, 281, 341, 5456, 7684, 490, 2623, 36159, 17303, 9411, 11, 597, 307, 257, 2831, 13373], "temperature": 0.0, "avg_logprob": -0.16913665070825692, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.4509836546494626e-05}, {"id": 212, "seek": 105860, "start": 1072.6, "end": 1074.7199999999998, "text": " list of things to play with.", "tokens": [1329, 295, 721, 281, 862, 365, 13], "temperature": 0.0, "avg_logprob": -0.16913665070825692, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.4509836546494626e-05}, {"id": 213, "seek": 105860, "start": 1074.7199999999998, "end": 1079.32, "text": " Now, again, maybe by the time you watch this, this is all changed.", "tokens": [823, 11, 797, 11, 1310, 538, 264, 565, 291, 1159, 341, 11, 341, 307, 439, 3105, 13], "temperature": 0.0, "avg_logprob": -0.16913665070825692, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.4509836546494626e-05}, {"id": 214, "seek": 105860, "start": 1079.32, "end": 1083.0, "text": " But I just wanted to know these kind of things are out there.", "tokens": [583, 286, 445, 1415, 281, 458, 613, 733, 295, 721, 366, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.16913665070825692, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.4509836546494626e-05}, {"id": 215, "seek": 105860, "start": 1083.0, "end": 1088.24, "text": " And they're basically ready to go applications that you can start playing with.", "tokens": [400, 436, 434, 1936, 1919, 281, 352, 5821, 300, 291, 393, 722, 2433, 365, 13], "temperature": 0.0, "avg_logprob": -0.16913665070825692, "compression_ratio": 1.5609756097560976, "no_speech_prob": 1.4509836546494626e-05}, {"id": 216, "seek": 108824, "start": 1088.24, "end": 1093.8, "text": " So play a lot.", "tokens": [407, 862, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.09802031517028809, "compression_ratio": 1.5482233502538072, "no_speech_prob": 5.681966285919771e-06}, {"id": 217, "seek": 108824, "start": 1093.8, "end": 1098.44, "text": " What you'll find is that most of them, at least at the moment, expect you to input some", "tokens": [708, 291, 603, 915, 307, 300, 881, 295, 552, 11, 412, 1935, 412, 264, 1623, 11, 2066, 291, 281, 4846, 512], "temperature": 0.0, "avg_logprob": -0.09802031517028809, "compression_ratio": 1.5482233502538072, "no_speech_prob": 5.681966285919771e-06}, {"id": 218, "seek": 108824, "start": 1098.44, "end": 1102.68, "text": " text to say what you want to create a picture of.", "tokens": [2487, 281, 584, 437, 291, 528, 281, 1884, 257, 3036, 295, 13], "temperature": 0.0, "avg_logprob": -0.09802031517028809, "compression_ratio": 1.5482233502538072, "no_speech_prob": 5.681966285919771e-06}, {"id": 219, "seek": 108824, "start": 1102.68, "end": 1110.2, "text": " It turns out that as we'll learn in detail why, the text you pick, it's not very easy", "tokens": [467, 4523, 484, 300, 382, 321, 603, 1466, 294, 2607, 983, 11, 264, 2487, 291, 1888, 11, 309, 311, 406, 588, 1858], "temperature": 0.0, "avg_logprob": -0.09802031517028809, "compression_ratio": 1.5482233502538072, "no_speech_prob": 5.681966285919771e-06}, {"id": 220, "seek": 108824, "start": 1110.2, "end": 1112.7, "text": " to know what to write.", "tokens": [281, 458, 437, 281, 2464, 13], "temperature": 0.0, "avg_logprob": -0.09802031517028809, "compression_ratio": 1.5482233502538072, "no_speech_prob": 5.681966285919771e-06}, {"id": 221, "seek": 108824, "start": 1112.7, "end": 1116.36, "text": " And that gives kind of interesting results.", "tokens": [400, 300, 2709, 733, 295, 1880, 3542, 13], "temperature": 0.0, "avg_logprob": -0.09802031517028809, "compression_ratio": 1.5482233502538072, "no_speech_prob": 5.681966285919771e-06}, {"id": 222, "seek": 111636, "start": 1116.36, "end": 1121.76, "text": " At the moment, it's quite an artisanal thing to understand what to write.", "tokens": [1711, 264, 1623, 11, 309, 311, 1596, 364, 1523, 14804, 304, 551, 281, 1223, 437, 281, 2464, 13], "temperature": 0.0, "avg_logprob": -0.09395276283731266, "compression_ratio": 1.766990291262136, "no_speech_prob": 2.090414227495785e-06}, {"id": 223, "seek": 111636, "start": 1121.76, "end": 1125.08, "text": " And the best way to learn what to write is called the prompt.", "tokens": [400, 264, 1151, 636, 281, 1466, 437, 281, 2464, 307, 1219, 264, 12391, 13], "temperature": 0.0, "avg_logprob": -0.09395276283731266, "compression_ratio": 1.766990291262136, "no_speech_prob": 2.090414227495785e-06}, {"id": 224, "seek": 111636, "start": 1125.08, "end": 1129.32, "text": " The best way to learn about prompts is to look at other people's prompts and their outputs.", "tokens": [440, 1151, 636, 281, 1466, 466, 41095, 307, 281, 574, 412, 661, 561, 311, 41095, 293, 641, 23930, 13], "temperature": 0.0, "avg_logprob": -0.09395276283731266, "compression_ratio": 1.766990291262136, "no_speech_prob": 2.090414227495785e-06}, {"id": 225, "seek": 111636, "start": 1129.32, "end": 1136.1999999999998, "text": " So at the moment, perhaps the best way to do that is Lexica, which has lots and lots", "tokens": [407, 412, 264, 1623, 11, 4317, 264, 1151, 636, 281, 360, 300, 307, 24086, 2262, 11, 597, 575, 3195, 293, 3195], "temperature": 0.0, "avg_logprob": -0.09395276283731266, "compression_ratio": 1.766990291262136, "no_speech_prob": 2.090414227495785e-06}, {"id": 226, "seek": 111636, "start": 1136.1999999999998, "end": 1141.9799999999998, "text": " of really interesting artworks.", "tokens": [295, 534, 1880, 15829, 82, 13], "temperature": 0.0, "avg_logprob": -0.09395276283731266, "compression_ratio": 1.766990291262136, "no_speech_prob": 2.090414227495785e-06}, {"id": 227, "seek": 111636, "start": 1141.9799999999998, "end": 1143.78, "text": " And so AI artworks.", "tokens": [400, 370, 7318, 15829, 82, 13], "temperature": 0.0, "avg_logprob": -0.09395276283731266, "compression_ratio": 1.766990291262136, "no_speech_prob": 2.090414227495785e-06}, {"id": 228, "seek": 114378, "start": 1143.78, "end": 1147.68, "text": " And so you can click on one and see what prompt was used.", "tokens": [400, 370, 291, 393, 2052, 322, 472, 293, 536, 437, 12391, 390, 1143, 13], "temperature": 0.0, "avg_logprob": -0.13084211614396837, "compression_ratio": 1.536144578313253, "no_speech_prob": 2.726389766394277e-06}, {"id": 229, "seek": 114378, "start": 1147.68, "end": 1154.92, "text": " And so you'll see here that generally you start with what do you want to make a picture", "tokens": [400, 370, 291, 603, 536, 510, 300, 5101, 291, 722, 365, 437, 360, 291, 528, 281, 652, 257, 3036], "temperature": 0.0, "avg_logprob": -0.13084211614396837, "compression_ratio": 1.536144578313253, "no_speech_prob": 2.726389766394277e-06}, {"id": 230, "seek": 114378, "start": 1154.92, "end": 1156.3999999999999, "text": " of?", "tokens": [295, 30], "temperature": 0.0, "avg_logprob": -0.13084211614396837, "compression_ratio": 1.536144578313253, "no_speech_prob": 2.726389766394277e-06}, {"id": 231, "seek": 114378, "start": 1156.3999999999999, "end": 1159.36, "text": " What's the style?", "tokens": [708, 311, 264, 3758, 30], "temperature": 0.0, "avg_logprob": -0.13084211614396837, "compression_ratio": 1.536144578313253, "no_speech_prob": 2.726389766394277e-06}, {"id": 232, "seek": 114378, "start": 1159.36, "end": 1167.08, "text": " And then the trick is to add a bunch of like artists' names or places that they put art", "tokens": [400, 550, 264, 4282, 307, 281, 909, 257, 3840, 295, 411, 6910, 6, 5288, 420, 3190, 300, 436, 829, 1523], "temperature": 0.0, "avg_logprob": -0.13084211614396837, "compression_ratio": 1.536144578313253, "no_speech_prob": 2.726389766394277e-06}, {"id": 233, "seek": 116708, "start": 1167.08, "end": 1175.1599999999999, "text": " so that the algorithm will tend to create a piece which matches art, you know, that", "tokens": [370, 300, 264, 9284, 486, 3928, 281, 1884, 257, 2522, 597, 10676, 1523, 11, 291, 458, 11, 300], "temperature": 0.0, "avg_logprob": -0.12097472494298761, "compression_ratio": 1.52, "no_speech_prob": 2.902227834056248e-06}, {"id": 234, "seek": 116708, "start": 1175.1599999999999, "end": 1181.08, "text": " tends to have these kinds of words in their captions.", "tokens": [12258, 281, 362, 613, 3685, 295, 2283, 294, 641, 44832, 13], "temperature": 0.0, "avg_logprob": -0.12097472494298761, "compression_ratio": 1.52, "no_speech_prob": 2.902227834056248e-06}, {"id": 235, "seek": 116708, "start": 1181.08, "end": 1186.6799999999998, "text": " So there's a really useful trick to kind of get good at this.", "tokens": [407, 456, 311, 257, 534, 4420, 4282, 281, 733, 295, 483, 665, 412, 341, 13], "temperature": 0.0, "avg_logprob": -0.12097472494298761, "compression_ratio": 1.52, "no_speech_prob": 2.902227834056248e-06}, {"id": 236, "seek": 116708, "start": 1186.6799999999998, "end": 1189.0, "text": " And so you can even search for things.", "tokens": [400, 370, 291, 393, 754, 3164, 337, 721, 13], "temperature": 0.0, "avg_logprob": -0.12097472494298761, "compression_ratio": 1.52, "no_speech_prob": 2.902227834056248e-06}, {"id": 237, "seek": 116708, "start": 1189.0, "end": 1191.0, "text": " So I don't know if they have teddy bears.", "tokens": [407, 286, 500, 380, 458, 498, 436, 362, 45116, 17276, 13], "temperature": 0.0, "avg_logprob": -0.12097472494298761, "compression_ratio": 1.52, "no_speech_prob": 2.902227834056248e-06}, {"id": 238, "seek": 116708, "start": 1191.0, "end": 1192.0, "text": " Let's try.", "tokens": [961, 311, 853, 13], "temperature": 0.0, "avg_logprob": -0.12097472494298761, "compression_ratio": 1.52, "no_speech_prob": 2.902227834056248e-06}, {"id": 239, "seek": 116708, "start": 1192.0, "end": 1194.74, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.12097472494298761, "compression_ratio": 1.52, "no_speech_prob": 2.902227834056248e-06}, {"id": 240, "seek": 119474, "start": 1194.74, "end": 1199.84, "text": " So if there's a kind of like a, probably not that one.", "tokens": [407, 498, 456, 311, 257, 733, 295, 411, 257, 11, 1391, 406, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.18773688102255062, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3687839529884513e-06}, {"id": 241, "seek": 119474, "start": 1199.84, "end": 1202.96, "text": " That's a pretty good teddy bear image.", "tokens": [663, 311, 257, 1238, 665, 45116, 6155, 3256, 13], "temperature": 0.0, "avg_logprob": -0.18773688102255062, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3687839529884513e-06}, {"id": 242, "seek": 119474, "start": 1202.96, "end": 1208.0, "text": " So you can kind of get some sense of how to create nice teddy bear images.", "tokens": [407, 291, 393, 733, 295, 483, 512, 2020, 295, 577, 281, 1884, 1481, 45116, 6155, 5267, 13], "temperature": 0.0, "avg_logprob": -0.18773688102255062, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3687839529884513e-06}, {"id": 243, "seek": 119474, "start": 1208.0, "end": 1210.04, "text": " That's so cute.", "tokens": [663, 311, 370, 4052, 13], "temperature": 0.0, "avg_logprob": -0.18773688102255062, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3687839529884513e-06}, {"id": 244, "seek": 119474, "start": 1210.04, "end": 1216.16, "text": " I know what I'm going to be showing my daughter tomorrow.", "tokens": [286, 458, 437, 286, 478, 516, 281, 312, 4099, 452, 4653, 4153, 13], "temperature": 0.0, "avg_logprob": -0.18773688102255062, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3687839529884513e-06}, {"id": 245, "seek": 119474, "start": 1216.16, "end": 1220.88, "text": " You can see they often tend to have similar kinds of stuff to try to encourage the algorithm", "tokens": [509, 393, 536, 436, 2049, 3928, 281, 362, 2531, 3685, 295, 1507, 281, 853, 281, 5373, 264, 9284], "temperature": 0.0, "avg_logprob": -0.18773688102255062, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3687839529884513e-06}, {"id": 246, "seek": 119474, "start": 1220.88, "end": 1222.04, "text": " to give good outputs.", "tokens": [281, 976, 665, 23930, 13], "temperature": 0.0, "avg_logprob": -0.18773688102255062, "compression_ratio": 1.6081081081081081, "no_speech_prob": 2.3687839529884513e-06}, {"id": 247, "seek": 122204, "start": 1222.04, "end": 1227.48, "text": " Okay, so by the end of this course, you'll understand why this is happening, why these", "tokens": [1033, 11, 370, 538, 264, 917, 295, 341, 1164, 11, 291, 603, 1223, 983, 341, 307, 2737, 11, 983, 613], "temperature": 0.0, "avg_logprob": -0.20661164712214816, "compression_ratio": 1.547486033519553, "no_speech_prob": 2.332046960873413e-06}, {"id": 248, "seek": 122204, "start": 1227.48, "end": 1232.32, "text": " kinds of out, you know, prompts create these kind of outputs and also how you can go beyond", "tokens": [3685, 295, 484, 11, 291, 458, 11, 41095, 1884, 613, 733, 295, 23930, 293, 611, 577, 291, 393, 352, 4399], "temperature": 0.0, "avg_logprob": -0.20661164712214816, "compression_ratio": 1.547486033519553, "no_speech_prob": 2.332046960873413e-06}, {"id": 249, "seek": 122204, "start": 1232.32, "end": 1243.28, "text": " just creating prompts to actually building really innovative new things with new data", "tokens": [445, 4084, 41095, 281, 767, 2390, 534, 12999, 777, 721, 365, 777, 1412], "temperature": 0.0, "avg_logprob": -0.20661164712214816, "compression_ratio": 1.547486033519553, "no_speech_prob": 2.332046960873413e-06}, {"id": 250, "seek": 122204, "start": 1243.28, "end": 1246.52, "text": " types.", "tokens": [3467, 13], "temperature": 0.0, "avg_logprob": -0.20661164712214816, "compression_ratio": 1.547486033519553, "no_speech_prob": 2.332046960873413e-06}, {"id": 251, "seek": 122204, "start": 1246.52, "end": 1251.6, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.20661164712214816, "compression_ratio": 1.547486033519553, "no_speech_prob": 2.332046960873413e-06}, {"id": 252, "seek": 125160, "start": 1251.6, "end": 1254.9399999999998, "text": " So let's take a look at the diffusion NBs repo.", "tokens": [407, 718, 311, 747, 257, 574, 412, 264, 25242, 426, 33, 82, 49040, 13], "temperature": 0.0, "avg_logprob": -0.15955413579940797, "compression_ratio": 1.5052083333333333, "no_speech_prob": 1.2804871403204743e-05}, {"id": 253, "seek": 125160, "start": 1254.9399999999998, "end": 1257.6999999999998, "text": " The first thing we'll look at is stable diffusion.", "tokens": [440, 700, 551, 321, 603, 574, 412, 307, 8351, 25242, 13], "temperature": 0.0, "avg_logprob": -0.15955413579940797, "compression_ratio": 1.5052083333333333, "no_speech_prob": 1.2804871403204743e-05}, {"id": 254, "seek": 125160, "start": 1257.6999999999998, "end": 1259.4399999999998, "text": " So a couple of options here.", "tokens": [407, 257, 1916, 295, 3956, 510, 13], "temperature": 0.0, "avg_logprob": -0.15955413579940797, "compression_ratio": 1.5052083333333333, "no_speech_prob": 1.2804871403204743e-05}, {"id": 255, "seek": 125160, "start": 1259.4399999999998, "end": 1270.0, "text": " You can clone this repo, which is linked from both the course.fast.ai and from the forum", "tokens": [509, 393, 26506, 341, 49040, 11, 597, 307, 9408, 490, 1293, 264, 1164, 13, 7011, 13, 1301, 293, 490, 264, 17542], "temperature": 0.0, "avg_logprob": -0.15955413579940797, "compression_ratio": 1.5052083333333333, "no_speech_prob": 1.2804871403204743e-05}, {"id": 256, "seek": 125160, "start": 1270.0, "end": 1274.8, "text": " and run it on like paper space gradient or your own machine or whatever.", "tokens": [293, 1190, 309, 322, 411, 3035, 1901, 16235, 420, 428, 1065, 3479, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.15955413579940797, "compression_ratio": 1.5052083333333333, "no_speech_prob": 1.2804871403204743e-05}, {"id": 257, "seek": 127480, "start": 1274.8, "end": 1283.12, "text": " Or you can head over to Colab and you can just say GitHub, right?", "tokens": [1610, 291, 393, 1378, 670, 281, 4004, 455, 293, 291, 393, 445, 584, 23331, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1925900657221956, "compression_ratio": 1.2954545454545454, "no_speech_prob": 8.059402034632512e-07}, {"id": 258, "seek": 127480, "start": 1283.12, "end": 1290.8, "text": " And then you can paste in the link to it directly from GitHub.", "tokens": [400, 550, 291, 393, 9163, 294, 264, 2113, 281, 309, 3838, 490, 23331, 13], "temperature": 0.0, "avg_logprob": -0.1925900657221956, "compression_ratio": 1.2954545454545454, "no_speech_prob": 8.059402034632512e-07}, {"id": 259, "seek": 127480, "start": 1290.8, "end": 1293.56, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1925900657221956, "compression_ratio": 1.2954545454545454, "no_speech_prob": 8.059402034632512e-07}, {"id": 260, "seek": 127480, "start": 1293.56, "end": 1299.5, "text": " So I'm running it on my own machine.", "tokens": [407, 286, 478, 2614, 309, 322, 452, 1065, 3479, 13], "temperature": 0.0, "avg_logprob": -0.1925900657221956, "compression_ratio": 1.2954545454545454, "no_speech_prob": 8.059402034632512e-07}, {"id": 261, "seek": 129950, "start": 1299.5, "end": 1307.4, "text": " And this notebook is largely been built thanks to the wonderful folks at Hugging Face.", "tokens": [400, 341, 21060, 307, 11611, 668, 3094, 3231, 281, 264, 3715, 4024, 412, 46892, 3249, 4047, 13], "temperature": 0.0, "avg_logprob": -0.15606819686069284, "compression_ratio": 1.7403846153846154, "no_speech_prob": 2.3687744032940827e-06}, {"id": 262, "seek": 129950, "start": 1307.4, "end": 1313.02, "text": " And Hugging Face have a library called Diffusers.", "tokens": [400, 46892, 3249, 4047, 362, 257, 6405, 1219, 413, 3661, 301, 433, 13], "temperature": 0.0, "avg_logprob": -0.15606819686069284, "compression_ratio": 1.7403846153846154, "no_speech_prob": 2.3687744032940827e-06}, {"id": 263, "seek": 129950, "start": 1313.02, "end": 1316.38, "text": " So any of you that have done part one of the course will be very familiar with Hugging", "tokens": [407, 604, 295, 291, 300, 362, 1096, 644, 472, 295, 264, 1164, 486, 312, 588, 4963, 365, 46892, 3249], "temperature": 0.0, "avg_logprob": -0.15606819686069284, "compression_ratio": 1.7403846153846154, "no_speech_prob": 2.3687744032940827e-06}, {"id": 264, "seek": 129950, "start": 1316.38, "end": 1317.38, "text": " Face.", "tokens": [4047, 13], "temperature": 0.0, "avg_logprob": -0.15606819686069284, "compression_ratio": 1.7403846153846154, "no_speech_prob": 2.3687744032940827e-06}, {"id": 265, "seek": 129950, "start": 1317.38, "end": 1320.52, "text": " We used a lot of their libraries in part one.", "tokens": [492, 1143, 257, 688, 295, 641, 15148, 294, 644, 472, 13], "temperature": 0.0, "avg_logprob": -0.15606819686069284, "compression_ratio": 1.7403846153846154, "no_speech_prob": 2.3687744032940827e-06}, {"id": 266, "seek": 129950, "start": 1320.52, "end": 1328.16, "text": " Diffusers is their library for doing stable diffusion and stuff like stable diffusion.", "tokens": [413, 3661, 301, 433, 307, 641, 6405, 337, 884, 8351, 25242, 293, 1507, 411, 8351, 25242, 13], "temperature": 0.0, "avg_logprob": -0.15606819686069284, "compression_ratio": 1.7403846153846154, "no_speech_prob": 2.3687744032940827e-06}, {"id": 267, "seek": 132816, "start": 1328.16, "end": 1339.3600000000001, "text": " At the moment, these things are changing a lot, but at the moment, this is our recommended", "tokens": [1711, 264, 1623, 11, 613, 721, 366, 4473, 257, 688, 11, 457, 412, 264, 1623, 11, 341, 307, 527, 9628], "temperature": 0.0, "avg_logprob": -0.12339032426172374, "compression_ratio": 1.5991189427312775, "no_speech_prob": 4.494983386393869e-06}, {"id": 268, "seek": 132816, "start": 1339.3600000000001, "end": 1343.8400000000001, "text": " library for doing this stuff and it's what we'll be using in this course.", "tokens": [6405, 337, 884, 341, 1507, 293, 309, 311, 437, 321, 603, 312, 1228, 294, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.12339032426172374, "compression_ratio": 1.5991189427312775, "no_speech_prob": 4.494983386393869e-06}, {"id": 269, "seek": 132816, "start": 1343.8400000000001, "end": 1346.0400000000002, "text": " Maybe by the time you watch this, there'll be lots of other options.", "tokens": [2704, 538, 264, 565, 291, 1159, 341, 11, 456, 603, 312, 3195, 295, 661, 3956, 13], "temperature": 0.0, "avg_logprob": -0.12339032426172374, "compression_ratio": 1.5991189427312775, "no_speech_prob": 4.494983386393869e-06}, {"id": 270, "seek": 132816, "start": 1346.0400000000002, "end": 1349.1200000000001, "text": " So again, keep an eye on course.fast.ai.", "tokens": [407, 797, 11, 1066, 364, 3313, 322, 1164, 13, 7011, 13, 1301, 13], "temperature": 0.0, "avg_logprob": -0.12339032426172374, "compression_ratio": 1.5991189427312775, "no_speech_prob": 4.494983386393869e-06}, {"id": 271, "seek": 132816, "start": 1349.1200000000001, "end": 1354.14, "text": " In general, Hugging Face have done a really good job of being at and staying at the kind", "tokens": [682, 2674, 11, 46892, 3249, 4047, 362, 1096, 257, 534, 665, 1691, 295, 885, 412, 293, 7939, 412, 264, 733], "temperature": 0.0, "avg_logprob": -0.12339032426172374, "compression_ratio": 1.5991189427312775, "no_speech_prob": 4.494983386393869e-06}, {"id": 272, "seek": 135414, "start": 1354.14, "end": 1358.68, "text": " of head of the pack around models in general for deep learning.", "tokens": [295, 1378, 295, 264, 2844, 926, 5245, 294, 2674, 337, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.09334285100301107, "compression_ratio": 1.510204081632653, "no_speech_prob": 8.13902988738846e-06}, {"id": 273, "seek": 135414, "start": 1358.68, "end": 1367.64, "text": " So it would be not surprising if they continue to be the best option for quite a while.", "tokens": [407, 309, 576, 312, 406, 8830, 498, 436, 2354, 281, 312, 264, 1151, 3614, 337, 1596, 257, 1339, 13], "temperature": 0.0, "avg_logprob": -0.09334285100301107, "compression_ratio": 1.510204081632653, "no_speech_prob": 8.13902988738846e-06}, {"id": 274, "seek": 135414, "start": 1367.64, "end": 1371.94, "text": " But the basic idea of any library is going to look pretty similar.", "tokens": [583, 264, 3875, 1558, 295, 604, 6405, 307, 516, 281, 574, 1238, 2531, 13], "temperature": 0.0, "avg_logprob": -0.09334285100301107, "compression_ratio": 1.510204081632653, "no_speech_prob": 8.13902988738846e-06}, {"id": 275, "seek": 135414, "start": 1371.94, "end": 1382.7800000000002, "text": " So to get started playing with this, you will need to log in to Hugging Face.", "tokens": [407, 281, 483, 1409, 2433, 365, 341, 11, 291, 486, 643, 281, 3565, 294, 281, 46892, 3249, 4047, 13], "temperature": 0.0, "avg_logprob": -0.09334285100301107, "compression_ratio": 1.510204081632653, "no_speech_prob": 8.13902988738846e-06}, {"id": 276, "seek": 138278, "start": 1382.78, "end": 1389.18, "text": " So if you've got a Hugging Face, you can create a username there and a password and then log", "tokens": [407, 498, 291, 600, 658, 257, 46892, 3249, 4047, 11, 291, 393, 1884, 257, 30351, 456, 293, 257, 11524, 293, 550, 3565], "temperature": 0.0, "avg_logprob": -0.17034726929896085, "compression_ratio": 1.6594827586206897, "no_speech_prob": 4.784844804817112e-06}, {"id": 277, "seek": 138278, "start": 1389.18, "end": 1390.18, "text": " in.", "tokens": [294, 13], "temperature": 0.0, "avg_logprob": -0.17034726929896085, "compression_ratio": 1.6594827586206897, "no_speech_prob": 4.784844804817112e-06}, {"id": 278, "seek": 138278, "start": 1390.18, "end": 1392.62, "text": " Once you've done it once, it'll save it on your computer so you won't have to log in", "tokens": [3443, 291, 600, 1096, 309, 1564, 11, 309, 603, 3155, 309, 322, 428, 3820, 370, 291, 1582, 380, 362, 281, 3565, 294], "temperature": 0.0, "avg_logprob": -0.17034726929896085, "compression_ratio": 1.6594827586206897, "no_speech_prob": 4.784844804817112e-06}, {"id": 279, "seek": 138278, "start": 1392.62, "end": 1397.2, "text": " again.", "tokens": [797, 13], "temperature": 0.0, "avg_logprob": -0.17034726929896085, "compression_ratio": 1.6594827586206897, "no_speech_prob": 4.784844804817112e-06}, {"id": 280, "seek": 138278, "start": 1397.2, "end": 1403.78, "text": " And the thing we're going to be working with is pipelines and in particular the stable", "tokens": [400, 264, 551, 321, 434, 516, 281, 312, 1364, 365, 307, 40168, 293, 294, 1729, 264, 8351], "temperature": 0.0, "avg_logprob": -0.17034726929896085, "compression_ratio": 1.6594827586206897, "no_speech_prob": 4.784844804817112e-06}, {"id": 281, "seek": 138278, "start": 1403.78, "end": 1404.78, "text": " diffusion pipeline.", "tokens": [25242, 15517, 13], "temperature": 0.0, "avg_logprob": -0.17034726929896085, "compression_ratio": 1.6594827586206897, "no_speech_prob": 4.784844804817112e-06}, {"id": 282, "seek": 138278, "start": 1404.78, "end": 1411.6, "text": " Again, you know, they might be using different pipelines by the time that you watch this.", "tokens": [3764, 11, 291, 458, 11, 436, 1062, 312, 1228, 819, 40168, 538, 264, 565, 300, 291, 1159, 341, 13], "temperature": 0.0, "avg_logprob": -0.17034726929896085, "compression_ratio": 1.6594827586206897, "no_speech_prob": 4.784844804817112e-06}, {"id": 283, "seek": 141160, "start": 1411.6, "end": 1420.1999999999998, "text": " But the basic idea of pipeline is quite similar to what we call a learner in fast AI, which", "tokens": [583, 264, 3875, 1558, 295, 15517, 307, 1596, 2531, 281, 437, 321, 818, 257, 33347, 294, 2370, 7318, 11, 597], "temperature": 0.0, "avg_logprob": -0.1773815155029297, "compression_ratio": 1.7412935323383085, "no_speech_prob": 8.315175250572793e-07}, {"id": 284, "seek": 141160, "start": 1420.1999999999998, "end": 1424.28, "text": " is it's got a whole bunch of things in it, you know, a bunch of kind of processing and", "tokens": [307, 309, 311, 658, 257, 1379, 3840, 295, 721, 294, 309, 11, 291, 458, 11, 257, 3840, 295, 733, 295, 9007, 293], "temperature": 0.0, "avg_logprob": -0.1773815155029297, "compression_ratio": 1.7412935323383085, "no_speech_prob": 8.315175250572793e-07}, {"id": 285, "seek": 141160, "start": 1424.28, "end": 1429.32, "text": " models and inference all happening automatically.", "tokens": [5245, 293, 38253, 439, 2737, 6772, 13], "temperature": 0.0, "avg_logprob": -0.1773815155029297, "compression_ratio": 1.7412935323383085, "no_speech_prob": 8.315175250572793e-07}, {"id": 286, "seek": 141160, "start": 1429.32, "end": 1434.8, "text": " And just like you can save a pipeline in fast AI, sorry, save a learner in fast AI, you", "tokens": [400, 445, 411, 291, 393, 3155, 257, 15517, 294, 2370, 7318, 11, 2597, 11, 3155, 257, 33347, 294, 2370, 7318, 11, 291], "temperature": 0.0, "avg_logprob": -0.1773815155029297, "compression_ratio": 1.7412935323383085, "no_speech_prob": 8.315175250572793e-07}, {"id": 287, "seek": 141160, "start": 1434.8, "end": 1438.9199999999998, "text": " can save a pipeline in diffusers.", "tokens": [393, 3155, 257, 15517, 294, 7593, 301, 433, 13], "temperature": 0.0, "avg_logprob": -0.1773815155029297, "compression_ratio": 1.7412935323383085, "no_speech_prob": 8.315175250572793e-07}, {"id": 288, "seek": 143892, "start": 1438.92, "end": 1443.92, "text": " Now something that you can do in all pretty much all Hugging Face libraries that you can't", "tokens": [823, 746, 300, 291, 393, 360, 294, 439, 1238, 709, 439, 46892, 3249, 4047, 15148, 300, 291, 393, 380], "temperature": 0.0, "avg_logprob": -0.11942226603879767, "compression_ratio": 1.7393162393162394, "no_speech_prob": 2.090401039822609e-06}, {"id": 289, "seek": 143892, "start": 1443.92, "end": 1451.0, "text": " do in fast AI is you can then save a pipeline or whatever back up into the cloud onto Hugging", "tokens": [360, 294, 2370, 7318, 307, 291, 393, 550, 3155, 257, 15517, 420, 2035, 646, 493, 666, 264, 4588, 3911, 46892, 3249], "temperature": 0.0, "avg_logprob": -0.11942226603879767, "compression_ratio": 1.7393162393162394, "no_speech_prob": 2.090401039822609e-06}, {"id": 290, "seek": 143892, "start": 1451.0, "end": 1452.8600000000001, "text": " Face, they call it the hub.", "tokens": [4047, 11, 436, 818, 309, 264, 11838, 13], "temperature": 0.0, "avg_logprob": -0.11942226603879767, "compression_ratio": 1.7393162393162394, "no_speech_prob": 2.090401039822609e-06}, {"id": 291, "seek": 143892, "start": 1452.8600000000001, "end": 1459.3400000000001, "text": " And so then if we say from pre-trained, it's a lot like how we create pre-trained learners", "tokens": [400, 370, 550, 498, 321, 584, 490, 659, 12, 17227, 2001, 11, 309, 311, 257, 688, 411, 577, 321, 1884, 659, 12, 17227, 2001, 23655], "temperature": 0.0, "avg_logprob": -0.11942226603879767, "compression_ratio": 1.7393162393162394, "no_speech_prob": 2.090401039822609e-06}, {"id": 292, "seek": 143892, "start": 1459.3400000000001, "end": 1460.8600000000001, "text": " in fast AI.", "tokens": [294, 2370, 7318, 13], "temperature": 0.0, "avg_logprob": -0.11942226603879767, "compression_ratio": 1.7393162393162394, "no_speech_prob": 2.090401039822609e-06}, {"id": 293, "seek": 143892, "start": 1460.8600000000001, "end": 1466.4, "text": " But the thing you put here is actually, if it's not a local path, it's a Hugging Face", "tokens": [583, 264, 551, 291, 829, 510, 307, 767, 11, 498, 309, 311, 406, 257, 2654, 3100, 11, 309, 311, 257, 46892, 3249, 4047], "temperature": 0.0, "avg_logprob": -0.11942226603879767, "compression_ratio": 1.7393162393162394, "no_speech_prob": 2.090401039822609e-06}, {"id": 294, "seek": 143892, "start": 1466.4, "end": 1468.64, "text": " repo.", "tokens": [49040, 13], "temperature": 0.0, "avg_logprob": -0.11942226603879767, "compression_ratio": 1.7393162393162394, "no_speech_prob": 2.090401039822609e-06}, {"id": 295, "seek": 146864, "start": 1468.64, "end": 1482.88, "text": " So if we search Hugging Face for this, then you can see this is what it's going to download.", "tokens": [407, 498, 321, 3164, 46892, 3249, 4047, 337, 341, 11, 550, 291, 393, 536, 341, 307, 437, 309, 311, 516, 281, 5484, 13], "temperature": 0.0, "avg_logprob": -0.06172824251478997, "compression_ratio": 1.4666666666666666, "no_speech_prob": 4.09287622460397e-06}, {"id": 296, "seek": 146864, "start": 1482.88, "end": 1488.44, "text": " And you can actually save your own pipelines up to the hub for other people to use.", "tokens": [400, 291, 393, 767, 3155, 428, 1065, 40168, 493, 281, 264, 11838, 337, 661, 561, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.06172824251478997, "compression_ratio": 1.4666666666666666, "no_speech_prob": 4.09287622460397e-06}, {"id": 297, "seek": 146864, "start": 1488.44, "end": 1495.14, "text": " So I think this is a very nice feature that helps, you know, the community build stuff.", "tokens": [407, 286, 519, 341, 307, 257, 588, 1481, 4111, 300, 3665, 11, 291, 458, 11, 264, 1768, 1322, 1507, 13], "temperature": 0.0, "avg_logprob": -0.06172824251478997, "compression_ratio": 1.4666666666666666, "no_speech_prob": 4.09287622460397e-06}, {"id": 298, "seek": 149514, "start": 1495.14, "end": 1498.64, "text": " So this is actually going to, the first time you run this, it's going to download many", "tokens": [407, 341, 307, 767, 516, 281, 11, 264, 700, 565, 291, 1190, 341, 11, 309, 311, 516, 281, 5484, 867], "temperature": 0.0, "avg_logprob": -0.14891133989606584, "compression_ratio": 1.7661290322580645, "no_speech_prob": 1.7230751836905256e-05}, {"id": 299, "seek": 149514, "start": 1498.64, "end": 1501.24, "text": " gigabytes of data from the internet.", "tokens": [42741, 295, 1412, 490, 264, 4705, 13], "temperature": 0.0, "avg_logprob": -0.14891133989606584, "compression_ratio": 1.7661290322580645, "no_speech_prob": 1.7230751836905256e-05}, {"id": 300, "seek": 149514, "start": 1501.24, "end": 1505.8400000000001, "text": " This is one of the slight challenges with using this on Colab is every time you use", "tokens": [639, 307, 472, 295, 264, 4036, 4759, 365, 1228, 341, 322, 4004, 455, 307, 633, 565, 291, 764], "temperature": 0.0, "avg_logprob": -0.14891133989606584, "compression_ratio": 1.7661290322580645, "no_speech_prob": 1.7230751836905256e-05}, {"id": 301, "seek": 149514, "start": 1505.8400000000001, "end": 1508.3200000000002, "text": " Colab, everything gets thrown away and start from scratch.", "tokens": [4004, 455, 11, 1203, 2170, 11732, 1314, 293, 722, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.14891133989606584, "compression_ratio": 1.7661290322580645, "no_speech_prob": 1.7230751836905256e-05}, {"id": 302, "seek": 149514, "start": 1508.3200000000002, "end": 1512.3000000000002, "text": " So it'll all have to be downloaded every time you use Colab.", "tokens": [407, 309, 603, 439, 362, 281, 312, 21748, 633, 565, 291, 764, 4004, 455, 13], "temperature": 0.0, "avg_logprob": -0.14891133989606584, "compression_ratio": 1.7661290322580645, "no_speech_prob": 1.7230751836905256e-05}, {"id": 303, "seek": 149514, "start": 1512.3000000000002, "end": 1518.6000000000001, "text": " If you use something like Paperspace or particularly actually Lambda Labs, it's all going to be", "tokens": [759, 291, 764, 746, 411, 430, 14441, 17940, 420, 4098, 767, 45691, 40047, 11, 309, 311, 439, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.14891133989606584, "compression_ratio": 1.7661290322580645, "no_speech_prob": 1.7230751836905256e-05}, {"id": 304, "seek": 149514, "start": 1518.6000000000001, "end": 1521.88, "text": " saved for you.", "tokens": [6624, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.14891133989606584, "compression_ratio": 1.7661290322580645, "no_speech_prob": 1.7230751836905256e-05}, {"id": 305, "seek": 152188, "start": 1521.88, "end": 1530.3200000000002, "text": " So once you've downloaded all this, it's going to save a whole bunch of stuff into your.cache", "tokens": [407, 1564, 291, 600, 21748, 439, 341, 11, 309, 311, 516, 281, 3155, 257, 1379, 3840, 295, 1507, 666, 428, 2411, 66, 6000], "temperature": 0.0, "avg_logprob": -0.1653017746774774, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.8447888123773737e-06}, {"id": 306, "seek": 152188, "start": 1530.3200000000002, "end": 1531.7800000000002, "text": " in your home directory.", "tokens": [294, 428, 1280, 21120, 13], "temperature": 0.0, "avg_logprob": -0.1653017746774774, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.8447888123773737e-06}, {"id": 307, "seek": 152188, "start": 1531.7800000000002, "end": 1534.2, "text": " So that's where Hugging Face puts things.", "tokens": [407, 300, 311, 689, 46892, 3249, 4047, 8137, 721, 13], "temperature": 0.0, "avg_logprob": -0.1653017746774774, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.8447888123773737e-06}, {"id": 308, "seek": 152188, "start": 1534.2, "end": 1542.88, "text": " So now that we have a pipeline called pipe, we can now treat it as if it's a function.", "tokens": [407, 586, 300, 321, 362, 257, 15517, 1219, 11240, 11, 321, 393, 586, 2387, 309, 382, 498, 309, 311, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1653017746774774, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.8447888123773737e-06}, {"id": 309, "seek": 152188, "start": 1542.88, "end": 1547.0, "text": " It's pretty common for like PyTorch-y stuff and fast AI-y stuff.", "tokens": [467, 311, 1238, 2689, 337, 411, 9953, 51, 284, 339, 12, 88, 1507, 293, 2370, 7318, 12, 88, 1507, 13], "temperature": 0.0, "avg_logprob": -0.1653017746774774, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.8447888123773737e-06}, {"id": 310, "seek": 152188, "start": 1547.0, "end": 1549.2, "text": " You should be very familiar with this, hopefully.", "tokens": [509, 820, 312, 588, 4963, 365, 341, 11, 4696, 13], "temperature": 0.0, "avg_logprob": -0.1653017746774774, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.8447888123773737e-06}, {"id": 311, "seek": 152188, "start": 1549.2, "end": 1551.7800000000002, "text": " And you can pass it a prompt.", "tokens": [400, 291, 393, 1320, 309, 257, 12391, 13], "temperature": 0.0, "avg_logprob": -0.1653017746774774, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.8447888123773737e-06}, {"id": 312, "seek": 155178, "start": 1551.78, "end": 1554.48, "text": " And so this is just some text.", "tokens": [400, 370, 341, 307, 445, 512, 2487, 13], "temperature": 0.0, "avg_logprob": -0.10637939123459804, "compression_ratio": 1.5698324022346368, "no_speech_prob": 4.565937160805333e-06}, {"id": 313, "seek": 155178, "start": 1554.48, "end": 1556.28, "text": " And that's going to return some images.", "tokens": [400, 300, 311, 516, 281, 2736, 512, 5267, 13], "temperature": 0.0, "avg_logprob": -0.10637939123459804, "compression_ratio": 1.5698324022346368, "no_speech_prob": 4.565937160805333e-06}, {"id": 314, "seek": 155178, "start": 1556.28, "end": 1558.68, "text": " Since we're only passing one prompt, it's going to return one image.", "tokens": [4162, 321, 434, 787, 8437, 472, 12391, 11, 309, 311, 516, 281, 2736, 472, 3256, 13], "temperature": 0.0, "avg_logprob": -0.10637939123459804, "compression_ratio": 1.5698324022346368, "no_speech_prob": 4.565937160805333e-06}, {"id": 315, "seek": 155178, "start": 1558.68, "end": 1561.74, "text": " So we'll just index into.images.", "tokens": [407, 321, 603, 445, 8186, 666, 2411, 332, 1660, 13], "temperature": 0.0, "avg_logprob": -0.10637939123459804, "compression_ratio": 1.5698324022346368, "no_speech_prob": 4.565937160805333e-06}, {"id": 316, "seek": 155178, "start": 1561.74, "end": 1568.84, "text": " And when we run it, it takes maybe 30 seconds or so and returns a photograph of an astronaut", "tokens": [400, 562, 321, 1190, 309, 11, 309, 2516, 1310, 2217, 3949, 420, 370, 293, 11247, 257, 8348, 295, 364, 18516], "temperature": 0.0, "avg_logprob": -0.10637939123459804, "compression_ratio": 1.5698324022346368, "no_speech_prob": 4.565937160805333e-06}, {"id": 317, "seek": 155178, "start": 1568.84, "end": 1573.2, "text": " riding a horse.", "tokens": [9546, 257, 6832, 13], "temperature": 0.0, "avg_logprob": -0.10637939123459804, "compression_ratio": 1.5698324022346368, "no_speech_prob": 4.565937160805333e-06}, {"id": 318, "seek": 157320, "start": 1573.2, "end": 1584.28, "text": " Every time you call a pipeline using the same random seed, you'll get the same image.", "tokens": [2048, 565, 291, 818, 257, 15517, 1228, 264, 912, 4974, 8871, 11, 291, 603, 483, 264, 912, 3256, 13], "temperature": 0.0, "avg_logprob": -0.08730240662892659, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.13809914587182e-06}, {"id": 319, "seek": 157320, "start": 1584.28, "end": 1587.24, "text": " You can set the random seed manually.", "tokens": [509, 393, 992, 264, 4974, 8871, 16945, 13], "temperature": 0.0, "avg_logprob": -0.08730240662892659, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.13809914587182e-06}, {"id": 320, "seek": 157320, "start": 1587.24, "end": 1590.66, "text": " And so you could send to somebody else and say, oh, this is a really cool astronaut riding", "tokens": [400, 370, 291, 727, 2845, 281, 2618, 1646, 293, 584, 11, 1954, 11, 341, 307, 257, 534, 1627, 18516, 9546], "temperature": 0.0, "avg_logprob": -0.08730240662892659, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.13809914587182e-06}, {"id": 321, "seek": 157320, "start": 1590.66, "end": 1592.92, "text": " a horse I found.", "tokens": [257, 6832, 286, 1352, 13], "temperature": 0.0, "avg_logprob": -0.08730240662892659, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.13809914587182e-06}, {"id": 322, "seek": 157320, "start": 1592.92, "end": 1596.04, "text": " Try manual seed 1024.", "tokens": [6526, 9688, 8871, 1266, 7911, 13], "temperature": 0.0, "avg_logprob": -0.08730240662892659, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.13809914587182e-06}, {"id": 323, "seek": 157320, "start": 1596.04, "end": 1602.16, "text": " And you'll get back this particular astronaut riding a horse.", "tokens": [400, 291, 603, 483, 646, 341, 1729, 18516, 9546, 257, 6832, 13], "temperature": 0.0, "avg_logprob": -0.08730240662892659, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.13809914587182e-06}, {"id": 324, "seek": 160216, "start": 1602.16, "end": 1609.2, "text": " So that's the most basic way to get started running on Colab or on your own machine.", "tokens": [407, 300, 311, 264, 881, 3875, 636, 281, 483, 1409, 2614, 322, 4004, 455, 420, 322, 428, 1065, 3479, 13], "temperature": 0.0, "avg_logprob": -0.09810991961546618, "compression_ratio": 1.5086206896551724, "no_speech_prob": 3.1380882319353987e-06}, {"id": 325, "seek": 160216, "start": 1609.2, "end": 1613.6000000000001, "text": " You can start creating images.", "tokens": [509, 393, 722, 4084, 5267, 13], "temperature": 0.0, "avg_logprob": -0.09810991961546618, "compression_ratio": 1.5086206896551724, "no_speech_prob": 3.1380882319353987e-06}, {"id": 326, "seek": 160216, "start": 1613.6000000000001, "end": 1616.1200000000001, "text": " It takes, as I said, it takes 30 seconds or so.", "tokens": [467, 2516, 11, 382, 286, 848, 11, 309, 2516, 2217, 3949, 420, 370, 13], "temperature": 0.0, "avg_logprob": -0.09810991961546618, "compression_ratio": 1.5086206896551724, "no_speech_prob": 3.1380882319353987e-06}, {"id": 327, "seek": 160216, "start": 1616.1200000000001, "end": 1620.0400000000002, "text": " And in this case, it took 51 steps.", "tokens": [400, 294, 341, 1389, 11, 309, 1890, 18485, 4439, 13], "temperature": 0.0, "avg_logprob": -0.09810991961546618, "compression_ratio": 1.5086206896551724, "no_speech_prob": 3.1380882319353987e-06}, {"id": 328, "seek": 160216, "start": 1620.0400000000002, "end": 1624.6000000000001, "text": " What it's doing, this is a bit very different to what we're used to with inference in fast", "tokens": [708, 309, 311, 884, 11, 341, 307, 257, 857, 588, 819, 281, 437, 321, 434, 1143, 281, 365, 38253, 294, 2370], "temperature": 0.0, "avg_logprob": -0.09810991961546618, "compression_ratio": 1.5086206896551724, "no_speech_prob": 3.1380882319353987e-06}, {"id": 329, "seek": 160216, "start": 1624.6000000000001, "end": 1630.96, "text": " AI, where it's one step to classify something, for example.", "tokens": [7318, 11, 689, 309, 311, 472, 1823, 281, 33872, 746, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.09810991961546618, "compression_ratio": 1.5086206896551724, "no_speech_prob": 3.1380882319353987e-06}, {"id": 330, "seek": 163096, "start": 1630.96, "end": 1636.28, "text": " What it's doing in these 51 steps is it's starting with like, so this is actually an", "tokens": [708, 309, 311, 884, 294, 613, 18485, 4439, 307, 309, 311, 2891, 365, 411, 11, 370, 341, 307, 767, 364], "temperature": 0.0, "avg_logprob": -0.12046275891755756, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.8448007494953345e-06}, {"id": 331, "seek": 163096, "start": 1636.28, "end": 1644.1200000000001, "text": " example that we're going to create ourselves in the course of creating handwritten digits.", "tokens": [1365, 300, 321, 434, 516, 281, 1884, 4175, 294, 264, 1164, 295, 4084, 1011, 26859, 27011, 13], "temperature": 0.0, "avg_logprob": -0.12046275891755756, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.8448007494953345e-06}, {"id": 332, "seek": 163096, "start": 1644.1200000000001, "end": 1646.96, "text": " This is actually an image from a later notebook we'll be building.", "tokens": [639, 307, 767, 364, 3256, 490, 257, 1780, 21060, 321, 603, 312, 2390, 13], "temperature": 0.0, "avg_logprob": -0.12046275891755756, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.8448007494953345e-06}, {"id": 333, "seek": 163096, "start": 1646.96, "end": 1650.14, "text": " Well, it basically starts with random noise.", "tokens": [1042, 11, 309, 1936, 3719, 365, 4974, 5658, 13], "temperature": 0.0, "avg_logprob": -0.12046275891755756, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.8448007494953345e-06}, {"id": 334, "seek": 163096, "start": 1650.14, "end": 1654.88, "text": " And each step, it tries to make it slightly less noisy and slightly more like the thing", "tokens": [400, 1184, 1823, 11, 309, 9898, 281, 652, 309, 4748, 1570, 24518, 293, 4748, 544, 411, 264, 551], "temperature": 0.0, "avg_logprob": -0.12046275891755756, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.8448007494953345e-06}, {"id": 335, "seek": 163096, "start": 1654.88, "end": 1656.0, "text": " we want.", "tokens": [321, 528, 13], "temperature": 0.0, "avg_logprob": -0.12046275891755756, "compression_ratio": 1.6551724137931034, "no_speech_prob": 1.8448007494953345e-06}, {"id": 336, "seek": 165600, "start": 1656.0, "end": 1662.04, "text": " And so going down here is showing all the steps to create the first four, for example,", "tokens": [400, 370, 516, 760, 510, 307, 4099, 439, 264, 4439, 281, 1884, 264, 700, 1451, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.1348914434743482, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.1381298413180048e-06}, {"id": 337, "seek": 165600, "start": 1662.04, "end": 1664.92, "text": " or here to create the first one.", "tokens": [420, 510, 281, 1884, 264, 700, 472, 13], "temperature": 0.0, "avg_logprob": -0.1348914434743482, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.1381298413180048e-06}, {"id": 338, "seek": 165600, "start": 1664.92, "end": 1669.32, "text": " And if you look closely, you can kind of see in this noise, there is something that looks", "tokens": [400, 498, 291, 574, 8185, 11, 291, 393, 733, 295, 536, 294, 341, 5658, 11, 456, 307, 746, 300, 1542], "temperature": 0.0, "avg_logprob": -0.1348914434743482, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.1381298413180048e-06}, {"id": 339, "seek": 165600, "start": 1669.32, "end": 1674.12, "text": " a bit like a one, and so kind of decides to focus on that.", "tokens": [257, 857, 411, 257, 472, 11, 293, 370, 733, 295, 14898, 281, 1879, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.1348914434743482, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.1381298413180048e-06}, {"id": 340, "seek": 165600, "start": 1674.12, "end": 1680.92, "text": " And so that's how these diffusion models basically work.", "tokens": [400, 370, 300, 311, 577, 613, 25242, 5245, 1936, 589, 13], "temperature": 0.0, "avg_logprob": -0.1348914434743482, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.1381298413180048e-06}, {"id": 341, "seek": 168092, "start": 1680.92, "end": 1691.3200000000002, "text": " So remember, if you're having any trouble finding the materials we're looking at, to", "tokens": [407, 1604, 11, 498, 291, 434, 1419, 604, 5253, 5006, 264, 5319, 321, 434, 1237, 412, 11, 281], "temperature": 0.0, "avg_logprob": -0.17229548016109983, "compression_ratio": 1.5470588235294118, "no_speech_prob": 3.1875135846348712e-06}, {"id": 342, "seek": 168092, "start": 1691.3200000000002, "end": 1698.72, "text": " go to course.fast.ai or go to the forum topic to see all the links.", "tokens": [352, 281, 1164, 13, 7011, 13, 1301, 420, 352, 281, 264, 17542, 4829, 281, 536, 439, 264, 6123, 13], "temperature": 0.0, "avg_logprob": -0.17229548016109983, "compression_ratio": 1.5470588235294118, "no_speech_prob": 3.1875135846348712e-06}, {"id": 343, "seek": 168092, "start": 1698.72, "end": 1702.44, "text": " And this one is called diffusion-nvs.", "tokens": [400, 341, 472, 307, 1219, 25242, 12, 77, 36959, 13], "temperature": 0.0, "avg_logprob": -0.17229548016109983, "compression_ratio": 1.5470588235294118, "no_speech_prob": 3.1875135846348712e-06}, {"id": 344, "seek": 168092, "start": 1702.44, "end": 1708.76, "text": " And the notebook is called, you can see it at the top, stable diffusion.", "tokens": [400, 264, 21060, 307, 1219, 11, 291, 393, 536, 309, 412, 264, 1192, 11, 8351, 25242, 13], "temperature": 0.0, "avg_logprob": -0.17229548016109983, "compression_ratio": 1.5470588235294118, "no_speech_prob": 3.1875135846348712e-06}, {"id": 345, "seek": 170876, "start": 1708.76, "end": 1714.5, "text": " Now a question might be, well, why don't we just do it in one go?", "tokens": [823, 257, 1168, 1062, 312, 11, 731, 11, 983, 500, 380, 321, 445, 360, 309, 294, 472, 352, 30], "temperature": 0.0, "avg_logprob": -0.11342006921768188, "compression_ratio": 1.5345911949685536, "no_speech_prob": 5.4221259233599994e-06}, {"id": 346, "seek": 170876, "start": 1714.5, "end": 1716.28, "text": " And we can do it in one go.", "tokens": [400, 321, 393, 360, 309, 294, 472, 352, 13], "temperature": 0.0, "avg_logprob": -0.11342006921768188, "compression_ratio": 1.5345911949685536, "no_speech_prob": 5.4221259233599994e-06}, {"id": 347, "seek": 170876, "start": 1716.28, "end": 1723.12, "text": " But if we try to do it in one go, it doesn't do a very good job.", "tokens": [583, 498, 321, 853, 281, 360, 309, 294, 472, 352, 11, 309, 1177, 380, 360, 257, 588, 665, 1691, 13], "temperature": 0.0, "avg_logprob": -0.11342006921768188, "compression_ratio": 1.5345911949685536, "no_speech_prob": 5.4221259233599994e-06}, {"id": 348, "seek": 170876, "start": 1723.12, "end": 1729.8799999999999, "text": " These models aren't, as I speak now in October 2022, smart enough to do it in one go.", "tokens": [1981, 5245, 3212, 380, 11, 382, 286, 1710, 586, 294, 7617, 20229, 11, 4069, 1547, 281, 360, 309, 294, 472, 352, 13], "temperature": 0.0, "avg_logprob": -0.11342006921768188, "compression_ratio": 1.5345911949685536, "no_speech_prob": 5.4221259233599994e-06}, {"id": 349, "seek": 172988, "start": 1729.88, "end": 1740.1200000000001, "text": " Now, as I mentioned at the start, the fact that I'm doing it in 51 steps here is hopelessly", "tokens": [823, 11, 382, 286, 2835, 412, 264, 722, 11, 264, 1186, 300, 286, 478, 884, 309, 294, 18485, 4439, 510, 307, 27317, 356], "temperature": 0.0, "avg_logprob": -0.11736177324174761, "compression_ratio": 1.5827067669172932, "no_speech_prob": 3.5007624319405295e-06}, {"id": 350, "seek": 172988, "start": 1740.1200000000001, "end": 1743.88, "text": " out of date, because as of yesterday, apparently we can now do it in three to four steps.", "tokens": [484, 295, 4002, 11, 570, 382, 295, 5186, 11, 7970, 321, 393, 586, 360, 309, 294, 1045, 281, 1451, 4439, 13], "temperature": 0.0, "avg_logprob": -0.11736177324174761, "compression_ratio": 1.5827067669172932, "no_speech_prob": 3.5007624319405295e-06}, {"id": 351, "seek": 172988, "start": 1743.88, "end": 1746.16, "text": " I'm not sure if that code's available yet.", "tokens": [286, 478, 406, 988, 498, 300, 3089, 311, 2435, 1939, 13], "temperature": 0.0, "avg_logprob": -0.11736177324174761, "compression_ratio": 1.5827067669172932, "no_speech_prob": 3.5007624319405295e-06}, {"id": 352, "seek": 172988, "start": 1746.16, "end": 1750.7600000000002, "text": " So by the time you see this, yeah, this might all be dramatically faster.", "tokens": [407, 538, 264, 565, 291, 536, 341, 11, 1338, 11, 341, 1062, 439, 312, 17548, 4663, 13], "temperature": 0.0, "avg_logprob": -0.11736177324174761, "compression_ratio": 1.5827067669172932, "no_speech_prob": 3.5007624319405295e-06}, {"id": 353, "seek": 172988, "start": 1750.7600000000002, "end": 1754.5800000000002, "text": " But as I'll be describing, understanding this basic concept, I'm pretty confident it's going", "tokens": [583, 382, 286, 603, 312, 16141, 11, 3701, 341, 3875, 3410, 11, 286, 478, 1238, 6679, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.11736177324174761, "compression_ratio": 1.5827067669172932, "no_speech_prob": 3.5007624319405295e-06}, {"id": 354, "seek": 172988, "start": 1754.5800000000002, "end": 1759.4, "text": " to be very important forever.", "tokens": [281, 312, 588, 1021, 5680, 13], "temperature": 0.0, "avg_logprob": -0.11736177324174761, "compression_ratio": 1.5827067669172932, "no_speech_prob": 3.5007624319405295e-06}, {"id": 355, "seek": 175940, "start": 1759.4, "end": 1761.0, "text": " So we'll talk about that.", "tokens": [407, 321, 603, 751, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.12932519410785875, "compression_ratio": 1.5245098039215685, "no_speech_prob": 8.39748463477008e-06}, {"id": 356, "seek": 175940, "start": 1761.0, "end": 1767.8000000000002, "text": " So if we do 16 steps instead of 51 steps, it looks a bit more like it, but it's still", "tokens": [407, 498, 321, 360, 3165, 4439, 2602, 295, 18485, 4439, 11, 309, 1542, 257, 857, 544, 411, 309, 11, 457, 309, 311, 920], "temperature": 0.0, "avg_logprob": -0.12932519410785875, "compression_ratio": 1.5245098039215685, "no_speech_prob": 8.39748463477008e-06}, {"id": 357, "seek": 175940, "start": 1767.8000000000002, "end": 1771.0400000000002, "text": " not amazing.", "tokens": [406, 2243, 13], "temperature": 0.0, "avg_logprob": -0.12932519410785875, "compression_ratio": 1.5245098039215685, "no_speech_prob": 8.39748463477008e-06}, {"id": 358, "seek": 175940, "start": 1771.0400000000002, "end": 1773.76, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.12932519410785875, "compression_ratio": 1.5245098039215685, "no_speech_prob": 8.39748463477008e-06}, {"id": 359, "seek": 175940, "start": 1773.76, "end": 1777.24, "text": " So that's how you can kind of get started.", "tokens": [407, 300, 311, 577, 291, 393, 733, 295, 483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.12932519410785875, "compression_ratio": 1.5245098039215685, "no_speech_prob": 8.39748463477008e-06}, {"id": 360, "seek": 175940, "start": 1777.24, "end": 1780.52, "text": " And I'll show you a few things that you can tune.", "tokens": [400, 286, 603, 855, 291, 257, 1326, 721, 300, 291, 393, 10864, 13], "temperature": 0.0, "avg_logprob": -0.12932519410785875, "compression_ratio": 1.5245098039215685, "no_speech_prob": 8.39748463477008e-06}, {"id": 361, "seek": 175940, "start": 1780.52, "end": 1786.0800000000002, "text": " And I should remind you that most of the stuff I'm showing you in this was built by Pedro", "tokens": [400, 286, 820, 4160, 291, 300, 881, 295, 264, 1507, 286, 478, 4099, 291, 294, 341, 390, 3094, 538, 26662], "temperature": 0.0, "avg_logprob": -0.12932519410785875, "compression_ratio": 1.5245098039215685, "no_speech_prob": 8.39748463477008e-06}, {"id": 362, "seek": 178608, "start": 1786.08, "end": 1789.84, "text": " Cuenca and the other folks at Hugging Face.", "tokens": [383, 7801, 496, 293, 264, 661, 4024, 412, 46892, 3249, 4047, 13], "temperature": 0.0, "avg_logprob": -0.11200654211123127, "compression_ratio": 1.6377358490566039, "no_speech_prob": 9.36788728722604e-06}, {"id": 363, "seek": 178608, "start": 1789.84, "end": 1791.52, "text": " So huge thanks to them.", "tokens": [407, 2603, 3231, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.11200654211123127, "compression_ratio": 1.6377358490566039, "no_speech_prob": 9.36788728722604e-06}, {"id": 364, "seek": 178608, "start": 1791.52, "end": 1799.1, "text": " There's no way I could have been as up to speed with all this detail without their help.", "tokens": [821, 311, 572, 636, 286, 727, 362, 668, 382, 493, 281, 3073, 365, 439, 341, 2607, 1553, 641, 854, 13], "temperature": 0.0, "avg_logprob": -0.11200654211123127, "compression_ratio": 1.6377358490566039, "no_speech_prob": 9.36788728722604e-06}, {"id": 365, "seek": 178608, "start": 1799.1, "end": 1805.32, "text": " They built this library, Diffusers, and have done a fantastic job of helping display what", "tokens": [814, 3094, 341, 6405, 11, 413, 3661, 301, 433, 11, 293, 362, 1096, 257, 5456, 1691, 295, 4315, 4674, 437], "temperature": 0.0, "avg_logprob": -0.11200654211123127, "compression_ratio": 1.6377358490566039, "no_speech_prob": 9.36788728722604e-06}, {"id": 366, "seek": 178608, "start": 1805.32, "end": 1806.8, "text": " you can do with it.", "tokens": [291, 393, 360, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.11200654211123127, "compression_ratio": 1.6377358490566039, "no_speech_prob": 9.36788728722604e-06}, {"id": 367, "seek": 178608, "start": 1806.8, "end": 1808.48, "text": " So let's look at an example of what you can do with it.", "tokens": [407, 718, 311, 574, 412, 364, 1365, 295, 437, 291, 393, 360, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.11200654211123127, "compression_ratio": 1.6377358490566039, "no_speech_prob": 9.36788728722604e-06}, {"id": 368, "seek": 178608, "start": 1808.48, "end": 1813.28, "text": " We're just going to quickly define a little function here to create a grid of images.", "tokens": [492, 434, 445, 516, 281, 2661, 6964, 257, 707, 2445, 510, 281, 1884, 257, 10748, 295, 5267, 13], "temperature": 0.0, "avg_logprob": -0.11200654211123127, "compression_ratio": 1.6377358490566039, "no_speech_prob": 9.36788728722604e-06}, {"id": 369, "seek": 178608, "start": 1813.28, "end": 1815.32, "text": " The details don't matter.", "tokens": [440, 4365, 500, 380, 1871, 13], "temperature": 0.0, "avg_logprob": -0.11200654211123127, "compression_ratio": 1.6377358490566039, "no_speech_prob": 9.36788728722604e-06}, {"id": 370, "seek": 181532, "start": 1815.32, "end": 1822.24, "text": " But what we do want to show here is you can take your prompt, which was an astronaut riding", "tokens": [583, 437, 321, 360, 528, 281, 855, 510, 307, 291, 393, 747, 428, 12391, 11, 597, 390, 364, 18516, 9546], "temperature": 0.0, "avg_logprob": -0.11956783093904194, "compression_ratio": 1.6540284360189574, "no_speech_prob": 2.601556161607732e-06}, {"id": 371, "seek": 181532, "start": 1822.24, "end": 1826.04, "text": " a horse, and just create four copies of it.", "tokens": [257, 6832, 11, 293, 445, 1884, 1451, 14341, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.11956783093904194, "compression_ratio": 1.6540284360189574, "no_speech_prob": 2.601556161607732e-06}, {"id": 372, "seek": 181532, "start": 1826.04, "end": 1827.04, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.11956783093904194, "compression_ratio": 1.6540284360189574, "no_speech_prob": 2.601556161607732e-06}, {"id": 373, "seek": 181532, "start": 1827.04, "end": 1831.52, "text": " So times when applied to a list simply copies the list that many times.", "tokens": [407, 1413, 562, 6456, 281, 257, 1329, 2935, 14341, 264, 1329, 300, 867, 1413, 13], "temperature": 0.0, "avg_logprob": -0.11956783093904194, "compression_ratio": 1.6540284360189574, "no_speech_prob": 2.601556161607732e-06}, {"id": 374, "seek": 181532, "start": 1831.52, "end": 1839.24, "text": " So here's a list of the exact same prompt four times.", "tokens": [407, 510, 311, 257, 1329, 295, 264, 1900, 912, 12391, 1451, 1413, 13], "temperature": 0.0, "avg_logprob": -0.11956783093904194, "compression_ratio": 1.6540284360189574, "no_speech_prob": 2.601556161607732e-06}, {"id": 375, "seek": 181532, "start": 1839.24, "end": 1842.6, "text": " And then what we're going to do is we're going to pass to the pipeline the prompts.", "tokens": [400, 550, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 1320, 281, 264, 15517, 264, 41095, 13], "temperature": 0.0, "avg_logprob": -0.11956783093904194, "compression_ratio": 1.6540284360189574, "no_speech_prob": 2.601556161607732e-06}, {"id": 376, "seek": 184260, "start": 1842.6, "end": 1846.32, "text": " And we're going to use a different parameter now called guidance scale.", "tokens": [400, 321, 434, 516, 281, 764, 257, 819, 13075, 586, 1219, 10056, 4373, 13], "temperature": 0.0, "avg_logprob": -0.12860889225215702, "compression_ratio": 1.6916299559471366, "no_speech_prob": 2.1233543066045968e-06}, {"id": 377, "seek": 184260, "start": 1846.32, "end": 1852.56, "text": " We're going to be learning about guidance scale in detail later in the course.", "tokens": [492, 434, 516, 281, 312, 2539, 466, 10056, 4373, 294, 2607, 1780, 294, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.12860889225215702, "compression_ratio": 1.6916299559471366, "no_speech_prob": 2.1233543066045968e-06}, {"id": 378, "seek": 184260, "start": 1852.56, "end": 1861.04, "text": " But basically what this does is it says to what degree should we be kind of focusing", "tokens": [583, 1936, 437, 341, 775, 307, 309, 1619, 281, 437, 4314, 820, 321, 312, 733, 295, 8416], "temperature": 0.0, "avg_logprob": -0.12860889225215702, "compression_ratio": 1.6916299559471366, "no_speech_prob": 2.1233543066045968e-06}, {"id": 379, "seek": 184260, "start": 1861.04, "end": 1867.3999999999999, "text": " on the specific caption versus just creating an image.", "tokens": [322, 264, 2685, 31974, 5717, 445, 4084, 364, 3256, 13], "temperature": 0.0, "avg_logprob": -0.12860889225215702, "compression_ratio": 1.6916299559471366, "no_speech_prob": 2.1233543066045968e-06}, {"id": 380, "seek": 184260, "start": 1867.3999999999999, "end": 1871.9199999999998, "text": " So we're going to try a few different guidance scales, about one, three, seven, 14, generally", "tokens": [407, 321, 434, 516, 281, 853, 257, 1326, 819, 10056, 17408, 11, 466, 472, 11, 1045, 11, 3407, 11, 3499, 11, 5101], "temperature": 0.0, "avg_logprob": -0.12860889225215702, "compression_ratio": 1.6916299559471366, "no_speech_prob": 2.1233543066045968e-06}, {"id": 381, "seek": 187192, "start": 1871.92, "end": 1875.04, "text": " seven and a half, I believe, at this stage is the default.", "tokens": [3407, 293, 257, 1922, 11, 286, 1697, 11, 412, 341, 3233, 307, 264, 7576, 13], "temperature": 0.0, "avg_logprob": -0.11102702639518527, "compression_ratio": 1.6984126984126984, "no_speech_prob": 2.225266143796034e-06}, {"id": 382, "seek": 187192, "start": 1875.04, "end": 1878.0800000000002, "text": " That might have changed by the time you watch this.", "tokens": [663, 1062, 362, 3105, 538, 264, 565, 291, 1159, 341, 13], "temperature": 0.0, "avg_logprob": -0.11102702639518527, "compression_ratio": 1.6984126984126984, "no_speech_prob": 2.225266143796034e-06}, {"id": 383, "seek": 187192, "start": 1878.0800000000002, "end": 1881.48, "text": " And so each row here is a different guidance scale.", "tokens": [400, 370, 1184, 5386, 510, 307, 257, 819, 10056, 4373, 13], "temperature": 0.0, "avg_logprob": -0.11102702639518527, "compression_ratio": 1.6984126984126984, "no_speech_prob": 2.225266143796034e-06}, {"id": 384, "seek": 187192, "start": 1881.48, "end": 1886.5600000000002, "text": " So you can see in the first row, it hasn't really listened to us very much at all.", "tokens": [407, 291, 393, 536, 294, 264, 700, 5386, 11, 309, 6132, 380, 534, 13207, 281, 505, 588, 709, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.11102702639518527, "compression_ratio": 1.6984126984126984, "no_speech_prob": 2.225266143796034e-06}, {"id": 385, "seek": 187192, "start": 1886.5600000000002, "end": 1890.3600000000001, "text": " These are very weird looking things, and none of them really look like astronauts riding", "tokens": [1981, 366, 588, 3657, 1237, 721, 11, 293, 6022, 295, 552, 534, 574, 411, 28273, 9546], "temperature": 0.0, "avg_logprob": -0.11102702639518527, "compression_ratio": 1.6984126984126984, "no_speech_prob": 2.225266143796034e-06}, {"id": 386, "seek": 187192, "start": 1890.3600000000001, "end": 1892.68, "text": " a horse.", "tokens": [257, 6832, 13], "temperature": 0.0, "avg_logprob": -0.11102702639518527, "compression_ratio": 1.6984126984126984, "no_speech_prob": 2.225266143796034e-06}, {"id": 387, "seek": 187192, "start": 1892.68, "end": 1900.5800000000002, "text": " At guidance scale of three, they look more like things riding horses that they might", "tokens": [1711, 10056, 4373, 295, 1045, 11, 436, 574, 544, 411, 721, 9546, 13112, 300, 436, 1062], "temperature": 0.0, "avg_logprob": -0.11102702639518527, "compression_ratio": 1.6984126984126984, "no_speech_prob": 2.225266143796034e-06}, {"id": 388, "seek": 190058, "start": 1900.58, "end": 1902.12, "text": " be astronautish.", "tokens": [312, 18516, 742, 13], "temperature": 0.0, "avg_logprob": -0.11843410560062953, "compression_ratio": 1.7808764940239044, "no_speech_prob": 4.22278617406846e-06}, {"id": 389, "seek": 190058, "start": 1902.12, "end": 1908.46, "text": " And at 7.5, they certainly on the whole look like astronauts riding a horse.", "tokens": [400, 412, 1614, 13, 20, 11, 436, 3297, 322, 264, 1379, 574, 411, 28273, 9546, 257, 6832, 13], "temperature": 0.0, "avg_logprob": -0.11843410560062953, "compression_ratio": 1.7808764940239044, "no_speech_prob": 4.22278617406846e-06}, {"id": 390, "seek": 190058, "start": 1908.46, "end": 1913.76, "text": " And at 14 or 15, they certainly look like that, but getting a little bit too abstract", "tokens": [400, 412, 3499, 420, 2119, 11, 436, 3297, 574, 411, 300, 11, 457, 1242, 257, 707, 857, 886, 12649], "temperature": 0.0, "avg_logprob": -0.11843410560062953, "compression_ratio": 1.7808764940239044, "no_speech_prob": 4.22278617406846e-06}, {"id": 391, "seek": 190058, "start": 1913.76, "end": 1914.76, "text": " sometimes.", "tokens": [2171, 13], "temperature": 0.0, "avg_logprob": -0.11843410560062953, "compression_ratio": 1.7808764940239044, "no_speech_prob": 4.22278617406846e-06}, {"id": 392, "seek": 190058, "start": 1914.76, "end": 1919.4399999999998, "text": " I have a pretty strong feeling there are some slight problems with actually how this is", "tokens": [286, 362, 257, 1238, 2068, 2633, 456, 366, 512, 4036, 2740, 365, 767, 577, 341, 307], "temperature": 0.0, "avg_logprob": -0.11843410560062953, "compression_ratio": 1.7808764940239044, "no_speech_prob": 4.22278617406846e-06}, {"id": 393, "seek": 190058, "start": 1919.4399999999998, "end": 1924.4399999999998, "text": " coded or actually how the algorithm works, which I will be looking at during this course.", "tokens": [34874, 420, 767, 577, 264, 9284, 1985, 11, 597, 286, 486, 312, 1237, 412, 1830, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.11843410560062953, "compression_ratio": 1.7808764940239044, "no_speech_prob": 4.22278617406846e-06}, {"id": 394, "seek": 190058, "start": 1924.4399999999998, "end": 1930.0, "text": " So maybe by the time you see this, some of these will be looking a bit better.", "tokens": [407, 1310, 538, 264, 565, 291, 536, 341, 11, 512, 295, 613, 486, 312, 1237, 257, 857, 1101, 13], "temperature": 0.0, "avg_logprob": -0.11843410560062953, "compression_ratio": 1.7808764940239044, "no_speech_prob": 4.22278617406846e-06}, {"id": 395, "seek": 193000, "start": 1930.0, "end": 1936.16, "text": " I think basically something that's happening here is it's actually kind of over jumping", "tokens": [286, 519, 1936, 746, 300, 311, 2737, 510, 307, 309, 311, 767, 733, 295, 670, 11233], "temperature": 0.0, "avg_logprob": -0.11622386754945267, "compression_ratio": 1.7342995169082125, "no_speech_prob": 7.2961297519213986e-06}, {"id": 396, "seek": 193000, "start": 1936.16, "end": 1938.76, "text": " a bit too far during these high ones.", "tokens": [257, 857, 886, 1400, 1830, 613, 1090, 2306, 13], "temperature": 0.0, "avg_logprob": -0.11622386754945267, "compression_ratio": 1.7342995169082125, "no_speech_prob": 7.2961297519213986e-06}, {"id": 397, "seek": 193000, "start": 1938.76, "end": 1945.84, "text": " Anyway, so the basic idea of what it's doing here is this guidance is it's basically actually", "tokens": [5684, 11, 370, 264, 3875, 1558, 295, 437, 309, 311, 884, 510, 307, 341, 10056, 307, 309, 311, 1936, 767], "temperature": 0.0, "avg_logprob": -0.11622386754945267, "compression_ratio": 1.7342995169082125, "no_speech_prob": 7.2961297519213986e-06}, {"id": 398, "seek": 193000, "start": 1945.84, "end": 1952.24, "text": " for every single prompt, it's creating two versions.", "tokens": [337, 633, 2167, 12391, 11, 309, 311, 4084, 732, 9606, 13], "temperature": 0.0, "avg_logprob": -0.11622386754945267, "compression_ratio": 1.7342995169082125, "no_speech_prob": 7.2961297519213986e-06}, {"id": 399, "seek": 193000, "start": 1952.24, "end": 1958.96, "text": " One version of the image with the prompt, an astronaut riding a horse, and one version", "tokens": [1485, 3037, 295, 264, 3256, 365, 264, 12391, 11, 364, 18516, 9546, 257, 6832, 11, 293, 472, 3037], "temperature": 0.0, "avg_logprob": -0.11622386754945267, "compression_ratio": 1.7342995169082125, "no_speech_prob": 7.2961297519213986e-06}, {"id": 400, "seek": 195896, "start": 1958.96, "end": 1961.66, "text": " of the image with no prompt.", "tokens": [295, 264, 3256, 365, 572, 12391, 13], "temperature": 0.0, "avg_logprob": -0.1462009491459016, "compression_ratio": 1.7281553398058251, "no_speech_prob": 3.187488800904248e-06}, {"id": 401, "seek": 195896, "start": 1961.66, "end": 1963.68, "text": " So it's just some random thing.", "tokens": [407, 309, 311, 445, 512, 4974, 551, 13], "temperature": 0.0, "avg_logprob": -0.1462009491459016, "compression_ratio": 1.7281553398058251, "no_speech_prob": 3.187488800904248e-06}, {"id": 402, "seek": 195896, "start": 1963.68, "end": 1968.4, "text": " And then it takes the average basically of those two things.", "tokens": [400, 550, 309, 2516, 264, 4274, 1936, 295, 729, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.1462009491459016, "compression_ratio": 1.7281553398058251, "no_speech_prob": 3.187488800904248e-06}, {"id": 403, "seek": 195896, "start": 1968.4, "end": 1971.44, "text": " And that's what guidance scale does.", "tokens": [400, 300, 311, 437, 10056, 4373, 775, 13], "temperature": 0.0, "avg_logprob": -0.1462009491459016, "compression_ratio": 1.7281553398058251, "no_speech_prob": 3.187488800904248e-06}, {"id": 404, "seek": 195896, "start": 1971.44, "end": 1975.52, "text": " And you can kind of think of the guidance scale as being a bit like a number that's", "tokens": [400, 291, 393, 733, 295, 519, 295, 264, 10056, 4373, 382, 885, 257, 857, 411, 257, 1230, 300, 311], "temperature": 0.0, "avg_logprob": -0.1462009491459016, "compression_ratio": 1.7281553398058251, "no_speech_prob": 3.187488800904248e-06}, {"id": 405, "seek": 195896, "start": 1975.52, "end": 1979.44, "text": " used to weight the average.", "tokens": [1143, 281, 3364, 264, 4274, 13], "temperature": 0.0, "avg_logprob": -0.1462009491459016, "compression_ratio": 1.7281553398058251, "no_speech_prob": 3.187488800904248e-06}, {"id": 406, "seek": 195896, "start": 1979.44, "end": 1985.94, "text": " There's something very similar you can do where again you get the model to create two", "tokens": [821, 311, 746, 588, 2531, 291, 393, 360, 689, 797, 291, 483, 264, 2316, 281, 1884, 732], "temperature": 0.0, "avg_logprob": -0.1462009491459016, "compression_ratio": 1.7281553398058251, "no_speech_prob": 3.187488800904248e-06}, {"id": 407, "seek": 198594, "start": 1985.94, "end": 1991.8400000000001, "text": " images, but rather than taking the average, you can ask it to effectively subtract one", "tokens": [5267, 11, 457, 2831, 813, 1940, 264, 4274, 11, 291, 393, 1029, 309, 281, 8659, 16390, 472], "temperature": 0.0, "avg_logprob": -0.16258901204818335, "compression_ratio": 1.5360824742268042, "no_speech_prob": 6.240867151063867e-06}, {"id": 408, "seek": 198594, "start": 1991.8400000000001, "end": 1993.64, "text": " from the other.", "tokens": [490, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.16258901204818335, "compression_ratio": 1.5360824742268042, "no_speech_prob": 6.240867151063867e-06}, {"id": 409, "seek": 198594, "start": 1993.64, "end": 2002.28, "text": " So here's something that Pedro did of using the prompt Elabrador in the style of a mirror.", "tokens": [407, 510, 311, 746, 300, 26662, 630, 295, 1228, 264, 12391, 2699, 455, 81, 5409, 294, 264, 3758, 295, 257, 8013, 13], "temperature": 0.0, "avg_logprob": -0.16258901204818335, "compression_ratio": 1.5360824742268042, "no_speech_prob": 6.240867151063867e-06}, {"id": 410, "seek": 198594, "start": 2002.28, "end": 2010.4, "text": " And then he said, well, what if we then subtract something which is just the model for the", "tokens": [400, 550, 415, 848, 11, 731, 11, 437, 498, 321, 550, 16390, 746, 597, 307, 445, 264, 2316, 337, 264], "temperature": 0.0, "avg_logprob": -0.16258901204818335, "compression_ratio": 1.5360824742268042, "no_speech_prob": 6.240867151063867e-06}, {"id": 411, "seek": 198594, "start": 2010.4, "end": 2012.64, "text": " caption blue?", "tokens": [31974, 3344, 30], "temperature": 0.0, "avg_logprob": -0.16258901204818335, "compression_ratio": 1.5360824742268042, "no_speech_prob": 6.240867151063867e-06}, {"id": 412, "seek": 201264, "start": 2012.64, "end": 2017.0400000000002, "text": " And you can pass in this thing negative prompt to diffusers.", "tokens": [400, 291, 393, 1320, 294, 341, 551, 3671, 12391, 281, 7593, 301, 433, 13], "temperature": 0.0, "avg_logprob": -0.09436095994094322, "compression_ratio": 1.8075313807531381, "no_speech_prob": 8.013328624656424e-06}, {"id": 413, "seek": 201264, "start": 2017.0400000000002, "end": 2020.8400000000001, "text": " And what that will do is it will take the prompt, which in this case is Elabrador in", "tokens": [400, 437, 300, 486, 360, 307, 309, 486, 747, 264, 12391, 11, 597, 294, 341, 1389, 307, 2699, 455, 81, 5409, 294], "temperature": 0.0, "avg_logprob": -0.09436095994094322, "compression_ratio": 1.8075313807531381, "no_speech_prob": 8.013328624656424e-06}, {"id": 414, "seek": 201264, "start": 2020.8400000000001, "end": 2026.48, "text": " the style of Vermeer, and create a second image effectively which is just responding", "tokens": [264, 3758, 295, 20185, 68, 260, 11, 293, 1884, 257, 1150, 3256, 8659, 597, 307, 445, 16670], "temperature": 0.0, "avg_logprob": -0.09436095994094322, "compression_ratio": 1.8075313807531381, "no_speech_prob": 8.013328624656424e-06}, {"id": 415, "seek": 201264, "start": 2026.48, "end": 2029.8400000000001, "text": " to the prompt blue, and effectively subtract one from the other.", "tokens": [281, 264, 12391, 3344, 11, 293, 8659, 16390, 472, 490, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.09436095994094322, "compression_ratio": 1.8075313807531381, "no_speech_prob": 8.013328624656424e-06}, {"id": 416, "seek": 201264, "start": 2029.8400000000001, "end": 2033.68, "text": " The details are slightly different to that, but that's the basic idea.", "tokens": [440, 4365, 366, 4748, 819, 281, 300, 11, 457, 300, 311, 264, 3875, 1558, 13], "temperature": 0.0, "avg_logprob": -0.09436095994094322, "compression_ratio": 1.8075313807531381, "no_speech_prob": 8.013328624656424e-06}, {"id": 417, "seek": 201264, "start": 2033.68, "end": 2040.72, "text": " And that way we get a non-blue Elabrador in the style of Vermeer.", "tokens": [400, 300, 636, 321, 483, 257, 2107, 12, 42113, 2699, 455, 81, 5409, 294, 264, 3758, 295, 20185, 68, 260, 13], "temperature": 0.0, "avg_logprob": -0.09436095994094322, "compression_ratio": 1.8075313807531381, "no_speech_prob": 8.013328624656424e-06}, {"id": 418, "seek": 204072, "start": 2040.72, "end": 2046.92, "text": " So yeah, this is the basic idea of how to use negative prompt, and you can play with", "tokens": [407, 1338, 11, 341, 307, 264, 3875, 1558, 295, 577, 281, 764, 3671, 12391, 11, 293, 291, 393, 862, 365], "temperature": 0.0, "avg_logprob": -0.16122324713345232, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.9944042125862325e-06}, {"id": 419, "seek": 204072, "start": 2046.92, "end": 2047.92, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.16122324713345232, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.9944042125862325e-06}, {"id": 420, "seek": 204072, "start": 2047.92, "end": 2051.76, "text": " Good fun.", "tokens": [2205, 1019, 13], "temperature": 0.0, "avg_logprob": -0.16122324713345232, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.9944042125862325e-06}, {"id": 421, "seek": 204072, "start": 2051.76, "end": 2059.2400000000002, "text": " Here's something else you can play with, is you don't have to just pass in text.", "tokens": [1692, 311, 746, 1646, 291, 393, 862, 365, 11, 307, 291, 500, 380, 362, 281, 445, 1320, 294, 2487, 13], "temperature": 0.0, "avg_logprob": -0.16122324713345232, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.9944042125862325e-06}, {"id": 422, "seek": 204072, "start": 2059.2400000000002, "end": 2061.38, "text": " You can actually pass in images.", "tokens": [509, 393, 767, 1320, 294, 5267, 13], "temperature": 0.0, "avg_logprob": -0.16122324713345232, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.9944042125862325e-06}, {"id": 423, "seek": 204072, "start": 2061.38, "end": 2063.68, "text": " So for this you'll need a different pipeline.", "tokens": [407, 337, 341, 291, 603, 643, 257, 819, 15517, 13], "temperature": 0.0, "avg_logprob": -0.16122324713345232, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.9944042125862325e-06}, {"id": 424, "seek": 204072, "start": 2063.68, "end": 2066.1, "text": " You'll need an image to image pipeline.", "tokens": [509, 603, 643, 364, 3256, 281, 3256, 15517, 13], "temperature": 0.0, "avg_logprob": -0.16122324713345232, "compression_ratio": 1.5957446808510638, "no_speech_prob": 2.9944042125862325e-06}, {"id": 425, "seek": 206610, "start": 2066.1, "end": 2077.2, "text": " And with the image to image pipeline, you can grab a rather sketchy looking sketch.", "tokens": [400, 365, 264, 3256, 281, 3256, 15517, 11, 291, 393, 4444, 257, 2831, 12325, 88, 1237, 12325, 13], "temperature": 0.0, "avg_logprob": -0.1135473445970185, "compression_ratio": 1.5913043478260869, "no_speech_prob": 3.7265954233589582e-06}, {"id": 426, "seek": 206610, "start": 2077.2, "end": 2091.74, "text": " And you can then pass to this eye to eye, image to image pipeline, the initial image", "tokens": [400, 291, 393, 550, 1320, 281, 341, 3313, 281, 3313, 11, 3256, 281, 3256, 15517, 11, 264, 5883, 3256], "temperature": 0.0, "avg_logprob": -0.1135473445970185, "compression_ratio": 1.5913043478260869, "no_speech_prob": 3.7265954233589582e-06}, {"id": 427, "seek": 206610, "start": 2091.74, "end": 2094.68, "text": " to start with.", "tokens": [281, 722, 365, 13], "temperature": 0.0, "avg_logprob": -0.1135473445970185, "compression_ratio": 1.5913043478260869, "no_speech_prob": 3.7265954233589582e-06}, {"id": 428, "seek": 209468, "start": 2094.68, "end": 2102.3199999999997, "text": " And basically what this is going to do is rather than starting diffusion process with", "tokens": [400, 1936, 437, 341, 307, 516, 281, 360, 307, 2831, 813, 2891, 25242, 1399, 365], "temperature": 0.0, "avg_logprob": -0.15146214844750577, "compression_ratio": 1.607361963190184, "no_speech_prob": 7.766836461087223e-06}, {"id": 429, "seek": 209468, "start": 2102.3199999999997, "end": 2114.22, "text": " random noise, it's going to basically start it with a noisy version of this drawing.", "tokens": [4974, 5658, 11, 309, 311, 516, 281, 1936, 722, 309, 365, 257, 24518, 3037, 295, 341, 6316, 13], "temperature": 0.0, "avg_logprob": -0.15146214844750577, "compression_ratio": 1.607361963190184, "no_speech_prob": 7.766836461087223e-06}, {"id": 430, "seek": 209468, "start": 2114.22, "end": 2124.16, "text": " And so then it's going to try to create something that matches this caption, and also like,", "tokens": [400, 370, 550, 309, 311, 516, 281, 853, 281, 1884, 746, 300, 10676, 341, 31974, 11, 293, 611, 411, 11], "temperature": 0.0, "avg_logprob": -0.15146214844750577, "compression_ratio": 1.607361963190184, "no_speech_prob": 7.766836461087223e-06}, {"id": 431, "seek": 212416, "start": 2124.16, "end": 2127.92, "text": " follows this kind of guiding starting point.", "tokens": [10002, 341, 733, 295, 25061, 2891, 935, 13], "temperature": 0.0, "avg_logprob": -0.08082251216090003, "compression_ratio": 1.7128712871287128, "no_speech_prob": 1.6536703242309159e-06}, {"id": 432, "seek": 212416, "start": 2127.92, "end": 2135.68, "text": " And so as a result, you get things that look quite a lot better than the original drawing.", "tokens": [400, 370, 382, 257, 1874, 11, 291, 483, 721, 300, 574, 1596, 257, 688, 1101, 813, 264, 3380, 6316, 13], "temperature": 0.0, "avg_logprob": -0.08082251216090003, "compression_ratio": 1.7128712871287128, "no_speech_prob": 1.6536703242309159e-06}, {"id": 433, "seek": 212416, "start": 2135.68, "end": 2138.2799999999997, "text": " But you can see that the composition is the same.", "tokens": [583, 291, 393, 536, 300, 264, 12686, 307, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.08082251216090003, "compression_ratio": 1.7128712871287128, "no_speech_prob": 1.6536703242309159e-06}, {"id": 434, "seek": 212416, "start": 2138.2799999999997, "end": 2146.6, "text": " And so using this approach, you can construct things that match the particular kind of composition", "tokens": [400, 370, 1228, 341, 3109, 11, 291, 393, 7690, 721, 300, 2995, 264, 1729, 733, 295, 12686], "temperature": 0.0, "avg_logprob": -0.08082251216090003, "compression_ratio": 1.7128712871287128, "no_speech_prob": 1.6536703242309159e-06}, {"id": 435, "seek": 212416, "start": 2146.6, "end": 2150.3399999999997, "text": " you're looking for.", "tokens": [291, 434, 1237, 337, 13], "temperature": 0.0, "avg_logprob": -0.08082251216090003, "compression_ratio": 1.7128712871287128, "no_speech_prob": 1.6536703242309159e-06}, {"id": 436, "seek": 212416, "start": 2150.3399999999997, "end": 2153.3199999999997, "text": " So I think that's quite a nifty approach.", "tokens": [407, 286, 519, 300, 311, 1596, 257, 297, 37177, 3109, 13], "temperature": 0.0, "avg_logprob": -0.08082251216090003, "compression_ratio": 1.7128712871287128, "no_speech_prob": 1.6536703242309159e-06}, {"id": 437, "seek": 215332, "start": 2153.32, "end": 2160.2200000000003, "text": " And so here this parameter strength is saying, to what degree do you want to really create", "tokens": [400, 370, 510, 341, 13075, 3800, 307, 1566, 11, 281, 437, 4314, 360, 291, 528, 281, 534, 1884], "temperature": 0.0, "avg_logprob": -0.14131780023928042, "compression_ratio": 1.699588477366255, "no_speech_prob": 1.2606700693140738e-05}, {"id": 438, "seek": 215332, "start": 2160.2200000000003, "end": 2163.56, "text": " something that looks like this?", "tokens": [746, 300, 1542, 411, 341, 30], "temperature": 0.0, "avg_logprob": -0.14131780023928042, "compression_ratio": 1.699588477366255, "no_speech_prob": 1.2606700693140738e-05}, {"id": 439, "seek": 215332, "start": 2163.56, "end": 2170.88, "text": " Or to what degree do you want the model to be able to try out different things a bit?", "tokens": [1610, 281, 437, 4314, 360, 291, 528, 264, 2316, 281, 312, 1075, 281, 853, 484, 819, 721, 257, 857, 30], "temperature": 0.0, "avg_logprob": -0.14131780023928042, "compression_ratio": 1.699588477366255, "no_speech_prob": 1.2606700693140738e-05}, {"id": 440, "seek": 215332, "start": 2170.88, "end": 2172.0800000000004, "text": " Now here's where things get interesting.", "tokens": [823, 510, 311, 689, 721, 483, 1880, 13], "temperature": 0.0, "avg_logprob": -0.14131780023928042, "compression_ratio": 1.699588477366255, "no_speech_prob": 1.2606700693140738e-05}, {"id": 441, "seek": 215332, "start": 2172.0800000000004, "end": 2174.88, "text": " And this is the kind of stuff you're not going to be able to do at the moment with just the", "tokens": [400, 341, 307, 264, 733, 295, 1507, 291, 434, 406, 516, 281, 312, 1075, 281, 360, 412, 264, 1623, 365, 445, 264], "temperature": 0.0, "avg_logprob": -0.14131780023928042, "compression_ratio": 1.699588477366255, "no_speech_prob": 1.2606700693140738e-05}, {"id": 442, "seek": 215332, "start": 2174.88, "end": 2177.1600000000003, "text": " basic GUIs and stuff.", "tokens": [3875, 17917, 6802, 293, 1507, 13], "temperature": 0.0, "avg_logprob": -0.14131780023928042, "compression_ratio": 1.699588477366255, "no_speech_prob": 1.2606700693140738e-05}, {"id": 443, "seek": 215332, "start": 2177.1600000000003, "end": 2179.3, "text": " But you can if you really know what you're doing.", "tokens": [583, 291, 393, 498, 291, 534, 458, 437, 291, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.14131780023928042, "compression_ratio": 1.699588477366255, "no_speech_prob": 1.2606700693140738e-05}, {"id": 444, "seek": 217930, "start": 2179.3, "end": 2183.52, "text": " What we could do now is we could take these output images, and we could say, oh, this", "tokens": [708, 321, 727, 360, 586, 307, 321, 727, 747, 613, 5598, 5267, 11, 293, 321, 727, 584, 11, 1954, 11, 341], "temperature": 0.0, "avg_logprob": -0.13453450202941894, "compression_ratio": 1.6243654822335025, "no_speech_prob": 7.296310741367051e-06}, {"id": 445, "seek": 217930, "start": 2183.52, "end": 2184.52, "text": " one's nice.", "tokens": [472, 311, 1481, 13], "temperature": 0.0, "avg_logprob": -0.13453450202941894, "compression_ratio": 1.6243654822335025, "no_speech_prob": 7.296310741367051e-06}, {"id": 446, "seek": 217930, "start": 2184.52, "end": 2186.26, "text": " Sorry, this one.", "tokens": [4919, 11, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.13453450202941894, "compression_ratio": 1.6243654822335025, "no_speech_prob": 7.296310741367051e-06}, {"id": 447, "seek": 217930, "start": 2186.26, "end": 2188.1200000000003, "text": " This one's nice.", "tokens": [639, 472, 311, 1481, 13], "temperature": 0.0, "avg_logprob": -0.13453450202941894, "compression_ratio": 1.6243654822335025, "no_speech_prob": 7.296310741367051e-06}, {"id": 448, "seek": 217930, "start": 2188.1200000000003, "end": 2191.5600000000004, "text": " Let's make this the initial image.", "tokens": [961, 311, 652, 341, 264, 5883, 3256, 13], "temperature": 0.0, "avg_logprob": -0.13453450202941894, "compression_ratio": 1.6243654822335025, "no_speech_prob": 7.296310741367051e-06}, {"id": 449, "seek": 217930, "start": 2191.5600000000004, "end": 2198.0, "text": " And now we'll say, let's do an oil painting of by Van Gogh.", "tokens": [400, 586, 321, 603, 584, 11, 718, 311, 360, 364, 3184, 5370, 295, 538, 8979, 39690, 71, 13], "temperature": 0.0, "avg_logprob": -0.13453450202941894, "compression_ratio": 1.6243654822335025, "no_speech_prob": 7.296310741367051e-06}, {"id": 450, "seek": 217930, "start": 2198.0, "end": 2203.32, "text": " And pass in the same thing here and a strength of one.", "tokens": [400, 1320, 294, 264, 912, 551, 510, 293, 257, 3800, 295, 472, 13], "temperature": 0.0, "avg_logprob": -0.13453450202941894, "compression_ratio": 1.6243654822335025, "no_speech_prob": 7.296310741367051e-06}, {"id": 451, "seek": 217930, "start": 2203.32, "end": 2207.38, "text": " And actually, that pretty much worked.", "tokens": [400, 767, 11, 300, 1238, 709, 2732, 13], "temperature": 0.0, "avg_logprob": -0.13453450202941894, "compression_ratio": 1.6243654822335025, "no_speech_prob": 7.296310741367051e-06}, {"id": 452, "seek": 220738, "start": 2207.38, "end": 2211.12, "text": " And I think that's absolutely fascinating, right?", "tokens": [400, 286, 519, 300, 311, 3122, 10343, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.0726681915489403, "compression_ratio": 1.5124378109452736, "no_speech_prob": 7.934465884318342e-07}, {"id": 453, "seek": 220738, "start": 2211.12, "end": 2220.8, "text": " Because this is something I haven't seen before, which Pedro put together this week.", "tokens": [1436, 341, 307, 746, 286, 2378, 380, 1612, 949, 11, 597, 26662, 829, 1214, 341, 1243, 13], "temperature": 0.0, "avg_logprob": -0.0726681915489403, "compression_ratio": 1.5124378109452736, "no_speech_prob": 7.934465884318342e-07}, {"id": 454, "seek": 220738, "start": 2220.8, "end": 2228.04, "text": " And it's combining simple Python code together.", "tokens": [400, 309, 311, 21928, 2199, 15329, 3089, 1214, 13], "temperature": 0.0, "avg_logprob": -0.0726681915489403, "compression_ratio": 1.5124378109452736, "no_speech_prob": 7.934465884318342e-07}, {"id": 455, "seek": 220738, "start": 2228.04, "end": 2231.88, "text": " And so you can play with that.", "tokens": [400, 370, 291, 393, 862, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.0726681915489403, "compression_ratio": 1.5124378109452736, "no_speech_prob": 7.934465884318342e-07}, {"id": 456, "seek": 220738, "start": 2231.88, "end": 2235.46, "text": " Something else you can do, which this one's actually example came from the folks at Lambda", "tokens": [6595, 1646, 291, 393, 360, 11, 597, 341, 472, 311, 767, 1365, 1361, 490, 264, 4024, 412, 45691], "temperature": 0.0, "avg_logprob": -0.0726681915489403, "compression_ratio": 1.5124378109452736, "no_speech_prob": 7.934465884318342e-07}, {"id": 457, "seek": 223546, "start": 2235.46, "end": 2241.2, "text": " Labs is, and we won't be going into this in detail right now, because this is like basically", "tokens": [40047, 307, 11, 293, 321, 1582, 380, 312, 516, 666, 341, 294, 2607, 558, 586, 11, 570, 341, 307, 411, 1936], "temperature": 0.0, "avg_logprob": -0.22396410965337987, "compression_ratio": 1.5422885572139304, "no_speech_prob": 5.626353640764137e-07}, {"id": 458, "seek": 223546, "start": 2241.2, "end": 2245.92, "text": " exactly like what we've done a thousand times in fast AI, is you can take the models in", "tokens": [2293, 411, 437, 321, 600, 1096, 257, 4714, 1413, 294, 2370, 7318, 11, 307, 291, 393, 747, 264, 5245, 294], "temperature": 0.0, "avg_logprob": -0.22396410965337987, "compression_ratio": 1.5422885572139304, "no_speech_prob": 5.626353640764137e-07}, {"id": 459, "seek": 223546, "start": 2245.92, "end": 2255.14, "text": " that pipeline and you can pass it your own images and your own captions.", "tokens": [300, 15517, 293, 291, 393, 1320, 309, 428, 1065, 5267, 293, 428, 1065, 44832, 13], "temperature": 0.0, "avg_logprob": -0.22396410965337987, "compression_ratio": 1.5422885572139304, "no_speech_prob": 5.626353640764137e-07}, {"id": 460, "seek": 223546, "start": 2255.14, "end": 2257.76, "text": " And so what happened here is...", "tokens": [400, 370, 437, 2011, 510, 307, 485], "temperature": 0.0, "avg_logprob": -0.22396410965337987, "compression_ratio": 1.5422885572139304, "no_speech_prob": 5.626353640764137e-07}, {"id": 461, "seek": 223546, "start": 2257.76, "end": 2264.32, "text": " Oh, I hate these things.", "tokens": [876, 11, 286, 4700, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.22396410965337987, "compression_ratio": 1.5422885572139304, "no_speech_prob": 5.626353640764137e-07}, {"id": 462, "seek": 226432, "start": 2264.32, "end": 2266.36, "text": " Go away.", "tokens": [1037, 1314, 13], "temperature": 0.0, "avg_logprob": -0.3043268839518229, "compression_ratio": 1.3576158940397351, "no_speech_prob": 6.339013452816289e-06}, {"id": 463, "seek": 226432, "start": 2266.36, "end": 2268.36, "text": " Never mind.", "tokens": [7344, 1575, 13], "temperature": 0.0, "avg_logprob": -0.3043268839518229, "compression_ratio": 1.3576158940397351, "no_speech_prob": 6.339013452816289e-06}, {"id": 464, "seek": 226432, "start": 2268.36, "end": 2274.0, "text": " Oh, here we are.", "tokens": [876, 11, 510, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.3043268839518229, "compression_ratio": 1.3576158940397351, "no_speech_prob": 6.339013452816289e-06}, {"id": 465, "seek": 226432, "start": 2274.0, "end": 2279.84, "text": " So what these folks did, I think this was Justin, if I remember correctly.", "tokens": [407, 437, 613, 4024, 630, 11, 286, 519, 341, 390, 11320, 11, 498, 286, 1604, 8944, 13], "temperature": 0.0, "avg_logprob": -0.3043268839518229, "compression_ratio": 1.3576158940397351, "no_speech_prob": 6.339013452816289e-06}, {"id": 466, "seek": 226432, "start": 2279.84, "end": 2289.6800000000003, "text": " So what Justin at Lambda did was he created a really cool dataset by going to grab a Pokemon", "tokens": [407, 437, 11320, 412, 45691, 630, 390, 415, 2942, 257, 534, 1627, 28872, 538, 516, 281, 4444, 257, 13796], "temperature": 0.0, "avg_logprob": -0.3043268839518229, "compression_ratio": 1.3576158940397351, "no_speech_prob": 6.339013452816289e-06}, {"id": 467, "seek": 228968, "start": 2289.68, "end": 2295.64, "text": " dataset of images, which had almost a thousand images of Pokemon.", "tokens": [28872, 295, 5267, 11, 597, 632, 1920, 257, 4714, 5267, 295, 13796, 13], "temperature": 0.0, "avg_logprob": -0.10974645614624023, "compression_ratio": 1.736318407960199, "no_speech_prob": 3.1875454169494333e-06}, {"id": 468, "seek": 228968, "start": 2295.64, "end": 2297.2599999999998, "text": " And then this is really neat.", "tokens": [400, 550, 341, 307, 534, 10654, 13], "temperature": 0.0, "avg_logprob": -0.10974645614624023, "compression_ratio": 1.736318407960199, "no_speech_prob": 3.1875454169494333e-06}, {"id": 469, "seek": 228968, "start": 2297.2599999999998, "end": 2302.8799999999997, "text": " He then used an image captioning model to automatically generate captions for each of", "tokens": [634, 550, 1143, 364, 3256, 31974, 278, 2316, 281, 6772, 8460, 44832, 337, 1184, 295], "temperature": 0.0, "avg_logprob": -0.10974645614624023, "compression_ratio": 1.736318407960199, "no_speech_prob": 3.1875454169494333e-06}, {"id": 470, "seek": 228968, "start": 2302.8799999999997, "end": 2305.1, "text": " those images.", "tokens": [729, 5267, 13], "temperature": 0.0, "avg_logprob": -0.10974645614624023, "compression_ratio": 1.736318407960199, "no_speech_prob": 3.1875454169494333e-06}, {"id": 471, "seek": 228968, "start": 2305.1, "end": 2313.72, "text": " And then he fine-tuned the stable diffusion model using those image and caption pairs.", "tokens": [400, 550, 415, 2489, 12, 83, 43703, 264, 8351, 25242, 2316, 1228, 729, 3256, 293, 31974, 15494, 13], "temperature": 0.0, "avg_logprob": -0.10974645614624023, "compression_ratio": 1.736318407960199, "no_speech_prob": 3.1875454169494333e-06}, {"id": 472, "seek": 228968, "start": 2313.72, "end": 2318.0, "text": " So here's an example of one of the captions and one of the images.", "tokens": [407, 510, 311, 364, 1365, 295, 472, 295, 264, 44832, 293, 472, 295, 264, 5267, 13], "temperature": 0.0, "avg_logprob": -0.10974645614624023, "compression_ratio": 1.736318407960199, "no_speech_prob": 3.1875454169494333e-06}, {"id": 473, "seek": 231800, "start": 2318.0, "end": 2327.72, "text": " And then took that fine-tuned model and passed it prompts like girl with a pearl earring", "tokens": [400, 550, 1890, 300, 2489, 12, 83, 43703, 2316, 293, 4678, 309, 41095, 411, 2013, 365, 257, 20287, 1273, 2937], "temperature": 0.0, "avg_logprob": -0.27351226409276325, "compression_ratio": 1.2857142857142858, "no_speech_prob": 1.81621339834237e-06}, {"id": 474, "seek": 231800, "start": 2327.72, "end": 2333.96, "text": " and cute Obama creature and got back these Totoro, these...", "tokens": [293, 4052, 9560, 12797, 293, 658, 646, 613, 11236, 10780, 11, 613, 485], "temperature": 0.0, "avg_logprob": -0.27351226409276325, "compression_ratio": 1.2857142857142858, "no_speech_prob": 1.81621339834237e-06}, {"id": 475, "seek": 231800, "start": 2333.96, "end": 2338.36, "text": " Oopsie-daisy.", "tokens": [21726, 414, 12, 67, 1527, 88, 13], "temperature": 0.0, "avg_logprob": -0.27351226409276325, "compression_ratio": 1.2857142857142858, "no_speech_prob": 1.81621339834237e-06}, {"id": 476, "seek": 233836, "start": 2338.36, "end": 2348.32, "text": " And got back these super nifty images that now are reflecting the fine-tuning dataset", "tokens": [400, 658, 646, 613, 1687, 297, 37177, 5267, 300, 586, 366, 23543, 264, 2489, 12, 83, 37726, 28872], "temperature": 0.0, "avg_logprob": -0.08829629711988496, "compression_ratio": 1.6349206349206349, "no_speech_prob": 5.368739266486955e-07}, {"id": 477, "seek": 233836, "start": 2348.32, "end": 2357.96, "text": " that he used and also responding to these prompts.", "tokens": [300, 415, 1143, 293, 611, 16670, 281, 613, 41095, 13], "temperature": 0.0, "avg_logprob": -0.08829629711988496, "compression_ratio": 1.6349206349206349, "no_speech_prob": 5.368739266486955e-07}, {"id": 478, "seek": 233836, "start": 2357.96, "end": 2360.36, "text": " Here's another example of something you can do.", "tokens": [1692, 311, 1071, 1365, 295, 746, 291, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.08829629711988496, "compression_ratio": 1.6349206349206349, "no_speech_prob": 5.368739266486955e-07}, {"id": 479, "seek": 233836, "start": 2360.36, "end": 2364.54, "text": " Fine-tuning can take quite a bit of data and quite a bit of time, but you can actually", "tokens": [12024, 12, 83, 37726, 393, 747, 1596, 257, 857, 295, 1412, 293, 1596, 257, 857, 295, 565, 11, 457, 291, 393, 767], "temperature": 0.0, "avg_logprob": -0.08829629711988496, "compression_ratio": 1.6349206349206349, "no_speech_prob": 5.368739266486955e-07}, {"id": 480, "seek": 233836, "start": 2364.54, "end": 2367.36, "text": " do some special kinds of fine-tuning.", "tokens": [360, 512, 2121, 3685, 295, 2489, 12, 83, 37726, 13], "temperature": 0.0, "avg_logprob": -0.08829629711988496, "compression_ratio": 1.6349206349206349, "no_speech_prob": 5.368739266486955e-07}, {"id": 481, "seek": 236736, "start": 2367.36, "end": 2374.52, "text": " One that you can do is called textual inversion, which is where we actually fine-tune just", "tokens": [1485, 300, 291, 393, 360, 307, 1219, 2487, 901, 43576, 11, 597, 307, 689, 321, 767, 2489, 12, 83, 2613, 445], "temperature": 0.0, "avg_logprob": -0.09457867024308544, "compression_ratio": 1.4452054794520548, "no_speech_prob": 1.5056962183734868e-06}, {"id": 482, "seek": 236736, "start": 2374.52, "end": 2377.76, "text": " a single embedding.", "tokens": [257, 2167, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.09457867024308544, "compression_ratio": 1.4452054794520548, "no_speech_prob": 1.5056962183734868e-06}, {"id": 483, "seek": 236736, "start": 2377.76, "end": 2392.6400000000003, "text": " So for example, we can create a new embedding where we're trying to make things that look", "tokens": [407, 337, 1365, 11, 321, 393, 1884, 257, 777, 12240, 3584, 689, 321, 434, 1382, 281, 652, 721, 300, 574], "temperature": 0.0, "avg_logprob": -0.09457867024308544, "compression_ratio": 1.4452054794520548, "no_speech_prob": 1.5056962183734868e-06}, {"id": 484, "seek": 236736, "start": 2392.6400000000003, "end": 2395.7400000000002, "text": " like this.", "tokens": [411, 341, 13], "temperature": 0.0, "avg_logprob": -0.09457867024308544, "compression_ratio": 1.4452054794520548, "no_speech_prob": 1.5056962183734868e-06}, {"id": 485, "seek": 239574, "start": 2395.74, "end": 2400.7799999999997, "text": " So what we can do is we can give this concept a name.", "tokens": [407, 437, 321, 393, 360, 307, 321, 393, 976, 341, 3410, 257, 1315, 13], "temperature": 0.0, "avg_logprob": -0.2548436032065862, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.406088469797396e-06}, {"id": 486, "seek": 239574, "start": 2400.7799999999997, "end": 2402.7599999999998, "text": " So here we're going to call it...", "tokens": [407, 510, 321, 434, 516, 281, 818, 309, 485], "temperature": 0.0, "avg_logprob": -0.2548436032065862, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.406088469797396e-06}, {"id": 487, "seek": 239574, "start": 2402.7599999999998, "end": 2407.72, "text": " Oh, I just lost it now.", "tokens": [876, 11, 286, 445, 2731, 309, 586, 13], "temperature": 0.0, "avg_logprob": -0.2548436032065862, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.406088469797396e-06}, {"id": 488, "seek": 239574, "start": 2407.72, "end": 2409.72, "text": " Watercolor.", "tokens": [8772, 23851, 13], "temperature": 0.0, "avg_logprob": -0.2548436032065862, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.406088469797396e-06}, {"id": 489, "seek": 239574, "start": 2409.72, "end": 2414.3999999999996, "text": " There we are.", "tokens": [821, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.2548436032065862, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.406088469797396e-06}, {"id": 490, "seek": 239574, "start": 2414.3999999999996, "end": 2419.2, "text": " We're going to call it watercolor portrait.", "tokens": [492, 434, 516, 281, 818, 309, 31727, 17126, 13], "temperature": 0.0, "avg_logprob": -0.2548436032065862, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.406088469797396e-06}, {"id": 491, "seek": 239574, "start": 2419.2, "end": 2422.66, "text": " And so that's what the embedding name we're going to use is.", "tokens": [400, 370, 300, 311, 437, 264, 12240, 3584, 1315, 321, 434, 516, 281, 764, 307, 13], "temperature": 0.0, "avg_logprob": -0.2548436032065862, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.406088469797396e-06}, {"id": 492, "seek": 242266, "start": 2422.66, "end": 2430.3199999999997, "text": " And we can then basically add that token to the text model, and then we can train the", "tokens": [400, 321, 393, 550, 1936, 909, 300, 14862, 281, 264, 2487, 2316, 11, 293, 550, 321, 393, 3847, 264], "temperature": 0.0, "avg_logprob": -0.07235957554408483, "compression_ratio": 1.603448275862069, "no_speech_prob": 9.132523928201408e-07}, {"id": 493, "seek": 242266, "start": 2430.3199999999997, "end": 2438.3199999999997, "text": " embeddings for this so that they match the example pictures that we've seen.", "tokens": [12240, 29432, 337, 341, 370, 300, 436, 2995, 264, 1365, 5242, 300, 321, 600, 1612, 13], "temperature": 0.0, "avg_logprob": -0.07235957554408483, "compression_ratio": 1.603448275862069, "no_speech_prob": 9.132523928201408e-07}, {"id": 494, "seek": 242266, "start": 2438.3199999999997, "end": 2442.92, "text": " And this is going to be much faster because we're just training a single token for just", "tokens": [400, 341, 307, 516, 281, 312, 709, 4663, 570, 321, 434, 445, 3097, 257, 2167, 14862, 337, 445], "temperature": 0.0, "avg_logprob": -0.07235957554408483, "compression_ratio": 1.603448275862069, "no_speech_prob": 9.132523928201408e-07}, {"id": 495, "seek": 242266, "start": 2442.92, "end": 2448.3599999999997, "text": " in this case, four pictures.", "tokens": [294, 341, 1389, 11, 1451, 5242, 13], "temperature": 0.0, "avg_logprob": -0.07235957554408483, "compression_ratio": 1.603448275862069, "no_speech_prob": 9.132523928201408e-07}, {"id": 496, "seek": 244836, "start": 2448.36, "end": 2456.6400000000003, "text": " And so when we do that, we can then say, for example, woman reading in the style of, and", "tokens": [400, 370, 562, 321, 360, 300, 11, 321, 393, 550, 584, 11, 337, 1365, 11, 3059, 3760, 294, 264, 3758, 295, 11, 293], "temperature": 0.0, "avg_logprob": -0.14389019325131275, "compression_ratio": 1.4193548387096775, "no_speech_prob": 2.1568073407252086e-06}, {"id": 497, "seek": 244836, "start": 2456.6400000000003, "end": 2458.44, "text": " then passing that token we just trained.", "tokens": [550, 8437, 300, 14862, 321, 445, 8895, 13], "temperature": 0.0, "avg_logprob": -0.14389019325131275, "compression_ratio": 1.4193548387096775, "no_speech_prob": 2.1568073407252086e-06}, {"id": 498, "seek": 244836, "start": 2458.44, "end": 2476.92, "text": " And as you see, we'll get back a kind of novel image, which I think is pretty interesting.", "tokens": [400, 382, 291, 536, 11, 321, 603, 483, 646, 257, 733, 295, 7613, 3256, 11, 597, 286, 519, 307, 1238, 1880, 13], "temperature": 0.0, "avg_logprob": -0.14389019325131275, "compression_ratio": 1.4193548387096775, "no_speech_prob": 2.1568073407252086e-06}, {"id": 499, "seek": 247692, "start": 2476.92, "end": 2490.04, "text": " Another example, very similar to textual inversion, is something called Dream Booth, which as", "tokens": [3996, 1365, 11, 588, 2531, 281, 2487, 901, 43576, 11, 307, 746, 1219, 12105, 3286, 900, 11, 597, 382], "temperature": 0.0, "avg_logprob": -0.15092391967773439, "compression_ratio": 1.5528846153846154, "no_speech_prob": 2.994425585711724e-06}, {"id": 500, "seek": 247692, "start": 2490.04, "end": 2495.04, "text": " mentioned here, what it does is it takes an existing token, but one that isn't used much,", "tokens": [2835, 510, 11, 437, 309, 775, 307, 309, 2516, 364, 6741, 14862, 11, 457, 472, 300, 1943, 380, 1143, 709, 11], "temperature": 0.0, "avg_logprob": -0.15092391967773439, "compression_ratio": 1.5528846153846154, "no_speech_prob": 2.994425585711724e-06}, {"id": 501, "seek": 247692, "start": 2495.04, "end": 2500.04, "text": " like say SKS, nothing, almost nothing has SKS, and fine tunes a model to bring that", "tokens": [411, 584, 21483, 50, 11, 1825, 11, 1920, 1825, 575, 21483, 50, 11, 293, 2489, 38498, 257, 2316, 281, 1565, 300], "temperature": 0.0, "avg_logprob": -0.15092391967773439, "compression_ratio": 1.5528846153846154, "no_speech_prob": 2.994425585711724e-06}, {"id": 502, "seek": 247692, "start": 2500.04, "end": 2503.16, "text": " token, as it says here, close to the images we provide.", "tokens": [14862, 11, 382, 309, 1619, 510, 11, 1998, 281, 264, 5267, 321, 2893, 13], "temperature": 0.0, "avg_logprob": -0.15092391967773439, "compression_ratio": 1.5528846153846154, "no_speech_prob": 2.994425585711724e-06}, {"id": 503, "seek": 250316, "start": 2503.16, "end": 2509.3999999999996, "text": " And so what Pedro did here was he grabbed some pictures of me and said, painting of", "tokens": [400, 370, 437, 26662, 630, 510, 390, 415, 18607, 512, 5242, 295, 385, 293, 848, 11, 5370, 295], "temperature": 0.0, "avg_logprob": -0.17963238979907745, "compression_ratio": 1.51131221719457, "no_speech_prob": 1.1015895324817393e-06}, {"id": 504, "seek": 250316, "start": 2509.3999999999996, "end": 2515.8399999999997, "text": " SKS, so in this case, he's fine tuned this token to be Jeremy Howard photos in the style", "tokens": [21483, 50, 11, 370, 294, 341, 1389, 11, 415, 311, 2489, 10870, 341, 14862, 281, 312, 17809, 17626, 5787, 294, 264, 3758], "temperature": 0.0, "avg_logprob": -0.17963238979907745, "compression_ratio": 1.51131221719457, "no_speech_prob": 1.1015895324817393e-06}, {"id": 505, "seek": 250316, "start": 2515.8399999999997, "end": 2517.52, "text": " of Paul Siniaq.", "tokens": [295, 4552, 11187, 654, 80, 13], "temperature": 0.0, "avg_logprob": -0.17963238979907745, "compression_ratio": 1.51131221719457, "no_speech_prob": 1.1015895324817393e-06}, {"id": 506, "seek": 250316, "start": 2517.52, "end": 2518.7999999999997, "text": " And there they are.", "tokens": [400, 456, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.17963238979907745, "compression_ratio": 1.51131221719457, "no_speech_prob": 1.1015895324817393e-06}, {"id": 507, "seek": 250316, "start": 2518.7999999999997, "end": 2524.64, "text": " And so the example I showed earlier of the draw off Jeremy Howard, that server, Streamr,", "tokens": [400, 370, 264, 1365, 286, 4712, 3071, 295, 264, 2642, 766, 17809, 17626, 11, 300, 7154, 11, 24904, 81, 11], "temperature": 0.0, "avg_logprob": -0.17963238979907745, "compression_ratio": 1.51131221719457, "no_speech_prob": 1.1015895324817393e-06}, {"id": 508, "seek": 250316, "start": 2524.64, "end": 2528.3599999999997, "text": " is actually using this, Dream Booth.", "tokens": [307, 767, 1228, 341, 11, 12105, 3286, 900, 13], "temperature": 0.0, "avg_logprob": -0.17963238979907745, "compression_ratio": 1.51131221719457, "no_speech_prob": 1.1015895324817393e-06}, {"id": 509, "seek": 252836, "start": 2528.36, "end": 2533.76, "text": " So here's how you can try that yourself.", "tokens": [407, 510, 311, 577, 291, 393, 853, 300, 1803, 13], "temperature": 0.0, "avg_logprob": -0.1512234608332316, "compression_ratio": 1.4772727272727273, "no_speech_prob": 6.143814516690327e-06}, {"id": 510, "seek": 252836, "start": 2533.76, "end": 2536.2000000000003, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1512234608332316, "compression_ratio": 1.4772727272727273, "no_speech_prob": 6.143814516690327e-06}, {"id": 511, "seek": 252836, "start": 2536.2000000000003, "end": 2547.8, "text": " So that is part one of this lesson, which is the how to get started playing around with", "tokens": [407, 300, 307, 644, 472, 295, 341, 6898, 11, 597, 307, 264, 577, 281, 483, 1409, 2433, 926, 365], "temperature": 0.0, "avg_logprob": -0.1512234608332316, "compression_ratio": 1.4772727272727273, "no_speech_prob": 6.143814516690327e-06}, {"id": 512, "seek": 252836, "start": 2547.8, "end": 2550.4, "text": " stable diffusion.", "tokens": [8351, 25242, 13], "temperature": 0.0, "avg_logprob": -0.1512234608332316, "compression_ratio": 1.4772727272727273, "no_speech_prob": 6.143814516690327e-06}, {"id": 513, "seek": 252836, "start": 2550.4, "end": 2556.6, "text": " In part two, we're going to talk about what's actually going on here from a machine learning", "tokens": [682, 644, 732, 11, 321, 434, 516, 281, 751, 466, 437, 311, 767, 516, 322, 510, 490, 257, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.1512234608332316, "compression_ratio": 1.4772727272727273, "no_speech_prob": 6.143814516690327e-06}, {"id": 514, "seek": 252836, "start": 2556.6, "end": 2558.2200000000003, "text": " point of view.", "tokens": [935, 295, 1910, 13], "temperature": 0.0, "avg_logprob": -0.1512234608332316, "compression_ratio": 1.4772727272727273, "no_speech_prob": 6.143814516690327e-06}, {"id": 515, "seek": 255822, "start": 2558.22, "end": 2564.52, "text": " So we'll come back in about seven minutes to talk about that.", "tokens": [407, 321, 603, 808, 646, 294, 466, 3407, 2077, 281, 751, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.22468199874415543, "compression_ratio": 1.4585987261146496, "no_speech_prob": 1.5688989151385613e-05}, {"id": 516, "seek": 255822, "start": 2564.52, "end": 2565.6, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.22468199874415543, "compression_ratio": 1.4585987261146496, "no_speech_prob": 1.5688989151385613e-05}, {"id": 517, "seek": 255822, "start": 2565.6, "end": 2569.0, "text": " See you guys in about seven minutes.", "tokens": [3008, 291, 1074, 294, 466, 3407, 2077, 13], "temperature": 0.0, "avg_logprob": -0.22468199874415543, "compression_ratio": 1.4585987261146496, "no_speech_prob": 1.5688989151385613e-05}, {"id": 518, "seek": 255822, "start": 2569.0, "end": 2572.2799999999997, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.22468199874415543, "compression_ratio": 1.4585987261146496, "no_speech_prob": 1.5688989151385613e-05}, {"id": 519, "seek": 255822, "start": 2572.2799999999997, "end": 2573.7599999999998, "text": " Welcome back, folks.", "tokens": [4027, 646, 11, 4024, 13], "temperature": 0.0, "avg_logprob": -0.22468199874415543, "compression_ratio": 1.4585987261146496, "no_speech_prob": 1.5688989151385613e-05}, {"id": 520, "seek": 255822, "start": 2573.7599999999998, "end": 2582.16, "text": " I just thought I'd share with you one more example, actually, of textual inversion training.", "tokens": [286, 445, 1194, 286, 1116, 2073, 365, 291, 472, 544, 1365, 11, 767, 11, 295, 2487, 901, 43576, 3097, 13], "temperature": 0.0, "avg_logprob": -0.22468199874415543, "compression_ratio": 1.4585987261146496, "no_speech_prob": 1.5688989151385613e-05}, {"id": 521, "seek": 258216, "start": 2582.16, "end": 2590.3199999999997, "text": " This is my daughter's Teddy Tiny, who, as you can see, is grossly misnamed.", "tokens": [639, 307, 452, 4653, 311, 34330, 39992, 11, 567, 11, 382, 291, 393, 536, 11, 307, 11367, 356, 3346, 33465, 13], "temperature": 0.0, "avg_logprob": -0.11279118366730519, "compression_ratio": 1.550561797752809, "no_speech_prob": 3.6687640658783494e-06}, {"id": 522, "seek": 258216, "start": 2590.3199999999997, "end": 2600.52, "text": " And Petro and I tried to create a textual inversion version of Tiny.", "tokens": [400, 10472, 340, 293, 286, 3031, 281, 1884, 257, 2487, 901, 43576, 3037, 295, 39992, 13], "temperature": 0.0, "avg_logprob": -0.11279118366730519, "compression_ratio": 1.550561797752809, "no_speech_prob": 3.6687640658783494e-06}, {"id": 523, "seek": 258216, "start": 2600.52, "end": 2606.2599999999998, "text": " And I was trying to get Tiny riding a horse.", "tokens": [400, 286, 390, 1382, 281, 483, 39992, 9546, 257, 6832, 13], "temperature": 0.0, "avg_logprob": -0.11279118366730519, "compression_ratio": 1.550561797752809, "no_speech_prob": 3.6687640658783494e-06}, {"id": 524, "seek": 258216, "start": 2606.2599999999998, "end": 2611.3999999999996, "text": " And it's interesting that when I tried to do that, this top row here, this is actually", "tokens": [400, 309, 311, 1880, 300, 562, 286, 3031, 281, 360, 300, 11, 341, 1192, 5386, 510, 11, 341, 307, 767], "temperature": 0.0, "avg_logprob": -0.11279118366730519, "compression_ratio": 1.550561797752809, "no_speech_prob": 3.6687640658783494e-06}, {"id": 525, "seek": 261140, "start": 2611.4, "end": 2614.5, "text": " Petro's example when he ran it.", "tokens": [10472, 340, 311, 1365, 562, 415, 5872, 309, 13], "temperature": 0.0, "avg_logprob": -0.1351966501396393, "compression_ratio": 1.75, "no_speech_prob": 2.5612662284402177e-06}, {"id": 526, "seek": 261140, "start": 2614.5, "end": 2620.36, "text": " This is showing the kind of steps as he was training of trying to use the caption Tiny", "tokens": [639, 307, 4099, 264, 733, 295, 4439, 382, 415, 390, 3097, 295, 1382, 281, 764, 264, 31974, 39992], "temperature": 0.0, "avg_logprob": -0.1351966501396393, "compression_ratio": 1.75, "no_speech_prob": 2.5612662284402177e-06}, {"id": 527, "seek": 261140, "start": 2620.36, "end": 2622.08, "text": " riding a horse.", "tokens": [9546, 257, 6832, 13], "temperature": 0.0, "avg_logprob": -0.1351966501396393, "compression_ratio": 1.75, "no_speech_prob": 2.5612662284402177e-06}, {"id": 528, "seek": 261140, "start": 2622.08, "end": 2626.1600000000003, "text": " And as you can see, it never actually ended up generating Tiny riding a horse.", "tokens": [400, 382, 291, 393, 536, 11, 309, 1128, 767, 4590, 493, 17746, 39992, 9546, 257, 6832, 13], "temperature": 0.0, "avg_logprob": -0.1351966501396393, "compression_ratio": 1.75, "no_speech_prob": 2.5612662284402177e-06}, {"id": 529, "seek": 261140, "start": 2626.1600000000003, "end": 2631.64, "text": " Instead it ended up generating a horse that looks a little bit like Tiny.", "tokens": [7156, 309, 4590, 493, 17746, 257, 6832, 300, 1542, 257, 707, 857, 411, 39992, 13], "temperature": 0.0, "avg_logprob": -0.1351966501396393, "compression_ratio": 1.75, "no_speech_prob": 2.5612662284402177e-06}, {"id": 530, "seek": 261140, "start": 2631.64, "end": 2636.84, "text": " And then we're trying to get Tiny sitting on a pink rug.", "tokens": [400, 550, 321, 434, 1382, 281, 483, 39992, 3798, 322, 257, 7022, 18329, 13], "temperature": 0.0, "avg_logprob": -0.1351966501396393, "compression_ratio": 1.75, "no_speech_prob": 2.5612662284402177e-06}, {"id": 531, "seek": 261140, "start": 2636.84, "end": 2639.7200000000003, "text": " And actually, after a while, it did make some progress there.", "tokens": [400, 767, 11, 934, 257, 1339, 11, 309, 630, 652, 512, 4205, 456, 13], "temperature": 0.0, "avg_logprob": -0.1351966501396393, "compression_ratio": 1.75, "no_speech_prob": 2.5612662284402177e-06}, {"id": 532, "seek": 263972, "start": 2639.72, "end": 2643.8399999999997, "text": " It doesn't quite look like Tiny.", "tokens": [467, 1177, 380, 1596, 574, 411, 39992, 13], "temperature": 0.0, "avg_logprob": -0.12753795755320582, "compression_ratio": 1.5857142857142856, "no_speech_prob": 5.681984930561157e-06}, {"id": 533, "seek": 263972, "start": 2643.8399999999997, "end": 2648.8399999999997, "text": " One thing Petro did that was different to me was he started with the embedding for person.", "tokens": [1485, 551, 10472, 340, 630, 300, 390, 819, 281, 385, 390, 415, 1409, 365, 264, 12240, 3584, 337, 954, 13], "temperature": 0.0, "avg_logprob": -0.12753795755320582, "compression_ratio": 1.5857142857142856, "no_speech_prob": 5.681984930561157e-06}, {"id": 534, "seek": 263972, "start": 2648.8399999999997, "end": 2652.8399999999997, "text": " In my one, I actually started with the embedding for Teddy, and it worked a bit better.", "tokens": [682, 452, 472, 11, 286, 767, 1409, 365, 264, 12240, 3584, 337, 34330, 11, 293, 309, 2732, 257, 857, 1101, 13], "temperature": 0.0, "avg_logprob": -0.12753795755320582, "compression_ratio": 1.5857142857142856, "no_speech_prob": 5.681984930561157e-06}, {"id": 535, "seek": 263972, "start": 2652.8399999999997, "end": 2655.74, "text": " But as you see, there are problems.", "tokens": [583, 382, 291, 536, 11, 456, 366, 2740, 13], "temperature": 0.0, "avg_logprob": -0.12753795755320582, "compression_ratio": 1.5857142857142856, "no_speech_prob": 5.681984930561157e-06}, {"id": 536, "seek": 263972, "start": 2655.74, "end": 2661.04, "text": " And we'll understand where those problems come from as we talk more about how this is", "tokens": [400, 321, 603, 1223, 689, 729, 2740, 808, 490, 382, 321, 751, 544, 466, 577, 341, 307], "temperature": 0.0, "avg_logprob": -0.12753795755320582, "compression_ratio": 1.5857142857142856, "no_speech_prob": 5.681984930561157e-06}, {"id": 537, "seek": 266104, "start": 2661.04, "end": 2671.12, "text": " trained in the rest of this lesson.", "tokens": [8895, 294, 264, 1472, 295, 341, 6898, 13], "temperature": 0.0, "avg_logprob": -0.1213417899224066, "compression_ratio": 1.4782608695652173, "no_speech_prob": 1.2679146266236785e-06}, {"id": 538, "seek": 266104, "start": 2671.12, "end": 2679.66, "text": " So I'm going to be relying on some understanding of the basic idea of how machine learning", "tokens": [407, 286, 478, 516, 281, 312, 24140, 322, 512, 3701, 295, 264, 3875, 1558, 295, 577, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.1213417899224066, "compression_ratio": 1.4782608695652173, "no_speech_prob": 1.2679146266236785e-06}, {"id": 539, "seek": 266104, "start": 2679.66, "end": 2682.04, "text": " models are trained here.", "tokens": [5245, 366, 8895, 510, 13], "temperature": 0.0, "avg_logprob": -0.1213417899224066, "compression_ratio": 1.4782608695652173, "no_speech_prob": 1.2679146266236785e-06}, {"id": 540, "seek": 266104, "start": 2682.04, "end": 2688.2799999999997, "text": " So if you start getting a bit lost at any point, you might want to go back to part one", "tokens": [407, 498, 291, 722, 1242, 257, 857, 2731, 412, 604, 935, 11, 291, 1062, 528, 281, 352, 646, 281, 644, 472], "temperature": 0.0, "avg_logprob": -0.1213417899224066, "compression_ratio": 1.4782608695652173, "no_speech_prob": 1.2679146266236785e-06}, {"id": 541, "seek": 268828, "start": 2688.28, "end": 2698.84, "text": " and then come back to this once you're unlost.", "tokens": [293, 550, 808, 646, 281, 341, 1564, 291, 434, 517, 75, 555, 13], "temperature": 0.0, "avg_logprob": -0.17005120595296225, "compression_ratio": 1.515625, "no_speech_prob": 1.777673423930537e-05}, {"id": 542, "seek": 268828, "start": 2698.84, "end": 2699.84, "text": " The way we're going to start...", "tokens": [440, 636, 321, 434, 516, 281, 722, 485], "temperature": 0.0, "avg_logprob": -0.17005120595296225, "compression_ratio": 1.515625, "no_speech_prob": 1.777673423930537e-05}, {"id": 543, "seek": 268828, "start": 2699.84, "end": 2702.2400000000002, "text": " Okay, so I need to explain.", "tokens": [1033, 11, 370, 286, 643, 281, 2903, 13], "temperature": 0.0, "avg_logprob": -0.17005120595296225, "compression_ratio": 1.515625, "no_speech_prob": 1.777673423930537e-05}, {"id": 544, "seek": 268828, "start": 2702.2400000000002, "end": 2707.8, "text": " The way stable diffusion is normally explained is focused very much on a particular mathematical", "tokens": [440, 636, 8351, 25242, 307, 5646, 8825, 307, 5178, 588, 709, 322, 257, 1729, 18894], "temperature": 0.0, "avg_logprob": -0.17005120595296225, "compression_ratio": 1.515625, "no_speech_prob": 1.777673423930537e-05}, {"id": 545, "seek": 268828, "start": 2707.8, "end": 2710.1600000000003, "text": " derivation.", "tokens": [10151, 399, 13], "temperature": 0.0, "avg_logprob": -0.17005120595296225, "compression_ratio": 1.515625, "no_speech_prob": 1.777673423930537e-05}, {"id": 546, "seek": 268828, "start": 2710.1600000000003, "end": 2715.28, "text": " We've been developing a totally new way of thinking about stable diffusion.", "tokens": [492, 600, 668, 6416, 257, 3879, 777, 636, 295, 1953, 466, 8351, 25242, 13], "temperature": 0.0, "avg_logprob": -0.17005120595296225, "compression_ratio": 1.515625, "no_speech_prob": 1.777673423930537e-05}, {"id": 547, "seek": 271528, "start": 2715.28, "end": 2719.36, "text": " And I'm going to be teaching you that.", "tokens": [400, 286, 478, 516, 281, 312, 4571, 291, 300, 13], "temperature": 0.0, "avg_logprob": -0.07717302788135617, "compression_ratio": 1.6053811659192825, "no_speech_prob": 4.092627023055684e-06}, {"id": 548, "seek": 271528, "start": 2719.36, "end": 2726.6600000000003, "text": " It's mathematically equivalent to the approach which you'll see in other places, but what", "tokens": [467, 311, 44003, 10344, 281, 264, 3109, 597, 291, 603, 536, 294, 661, 3190, 11, 457, 437], "temperature": 0.0, "avg_logprob": -0.07717302788135617, "compression_ratio": 1.6053811659192825, "no_speech_prob": 4.092627023055684e-06}, {"id": 549, "seek": 271528, "start": 2726.6600000000003, "end": 2732.96, "text": " you'll realize and discover is that it's actually conceptually much simpler.", "tokens": [291, 603, 4325, 293, 4411, 307, 300, 309, 311, 767, 3410, 671, 709, 18587, 13], "temperature": 0.0, "avg_logprob": -0.07717302788135617, "compression_ratio": 1.6053811659192825, "no_speech_prob": 4.092627023055684e-06}, {"id": 550, "seek": 271528, "start": 2732.96, "end": 2739.0800000000004, "text": " And also later in this course, we'll be showing you some really innovative directions that", "tokens": [400, 611, 1780, 294, 341, 1164, 11, 321, 603, 312, 4099, 291, 512, 534, 12999, 11095, 300], "temperature": 0.0, "avg_logprob": -0.07717302788135617, "compression_ratio": 1.6053811659192825, "no_speech_prob": 4.092627023055684e-06}, {"id": 551, "seek": 271528, "start": 2739.0800000000004, "end": 2742.2000000000003, "text": " this can take you when you think of it in this brand new way.", "tokens": [341, 393, 747, 291, 562, 291, 519, 295, 309, 294, 341, 3360, 777, 636, 13], "temperature": 0.0, "avg_logprob": -0.07717302788135617, "compression_ratio": 1.6053811659192825, "no_speech_prob": 4.092627023055684e-06}, {"id": 552, "seek": 274220, "start": 2742.2, "end": 2748.4399999999996, "text": " So all of which is to say, when you listen to this and then you go and look at some blog", "tokens": [407, 439, 295, 597, 307, 281, 584, 11, 562, 291, 2140, 281, 341, 293, 550, 291, 352, 293, 574, 412, 512, 6968], "temperature": 0.0, "avg_logprob": -0.10062677992714776, "compression_ratio": 1.6766467065868262, "no_speech_prob": 2.4439216304017464e-06}, {"id": 553, "seek": 274220, "start": 2748.4399999999996, "end": 2753.24, "text": " post and it looks like I'm saying something different, just keep that in mind.", "tokens": [2183, 293, 309, 1542, 411, 286, 478, 1566, 746, 819, 11, 445, 1066, 300, 294, 1575, 13], "temperature": 0.0, "avg_logprob": -0.10062677992714776, "compression_ratio": 1.6766467065868262, "no_speech_prob": 2.4439216304017464e-06}, {"id": 554, "seek": 274220, "start": 2753.24, "end": 2754.7599999999998, "text": " I'm not saying something different.", "tokens": [286, 478, 406, 1566, 746, 819, 13], "temperature": 0.0, "avg_logprob": -0.10062677992714776, "compression_ratio": 1.6766467065868262, "no_speech_prob": 2.4439216304017464e-06}, {"id": 555, "seek": 274220, "start": 2754.7599999999998, "end": 2766.9199999999996, "text": " I'm expressing it in a different way, but it's equally mathematically valid.", "tokens": [286, 478, 22171, 309, 294, 257, 819, 636, 11, 457, 309, 311, 12309, 44003, 7363, 13], "temperature": 0.0, "avg_logprob": -0.10062677992714776, "compression_ratio": 1.6766467065868262, "no_speech_prob": 2.4439216304017464e-06}, {"id": 556, "seek": 276692, "start": 2766.92, "end": 2775.6, "text": " What I'm going to do is I'm going to start by saying, let's imagine that we were trying", "tokens": [708, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 722, 538, 1566, 11, 718, 311, 3811, 300, 321, 645, 1382], "temperature": 0.0, "avg_logprob": -0.13121163476373732, "compression_ratio": 1.7149532710280373, "no_speech_prob": 3.340480361657683e-06}, {"id": 557, "seek": 276692, "start": 2775.6, "end": 2779.4, "text": " to get something to generate something much simpler, which is to generate handwritten", "tokens": [281, 483, 746, 281, 8460, 746, 709, 18587, 11, 597, 307, 281, 8460, 1011, 26859], "temperature": 0.0, "avg_logprob": -0.13121163476373732, "compression_ratio": 1.7149532710280373, "no_speech_prob": 3.340480361657683e-06}, {"id": 558, "seek": 276692, "start": 2779.4, "end": 2780.4, "text": " digits.", "tokens": [27011, 13], "temperature": 0.0, "avg_logprob": -0.13121163476373732, "compression_ratio": 1.7149532710280373, "no_speech_prob": 3.340480361657683e-06}, {"id": 559, "seek": 276692, "start": 2780.4, "end": 2787.1, "text": " Okay, so it's like the stable diffusion for handwritten digits.", "tokens": [1033, 11, 370, 309, 311, 411, 264, 8351, 25242, 337, 1011, 26859, 27011, 13], "temperature": 0.0, "avg_logprob": -0.13121163476373732, "compression_ratio": 1.7149532710280373, "no_speech_prob": 3.340480361657683e-06}, {"id": 560, "seek": 276692, "start": 2787.1, "end": 2792.84, "text": " And we're going to start by assuming there's some API, some web service or whatever out", "tokens": [400, 321, 434, 516, 281, 722, 538, 11926, 456, 311, 512, 9362, 11, 512, 3670, 2643, 420, 2035, 484], "temperature": 0.0, "avg_logprob": -0.13121163476373732, "compression_ratio": 1.7149532710280373, "no_speech_prob": 3.340480361657683e-06}, {"id": 561, "seek": 276692, "start": 2792.84, "end": 2793.84, "text": " there.", "tokens": [456, 13], "temperature": 0.0, "avg_logprob": -0.13121163476373732, "compression_ratio": 1.7149532710280373, "no_speech_prob": 3.340480361657683e-06}, {"id": 562, "seek": 276692, "start": 2793.84, "end": 2795.92, "text": " Who knows how it was made?", "tokens": [2102, 3255, 577, 309, 390, 1027, 30], "temperature": 0.0, "avg_logprob": -0.13121163476373732, "compression_ratio": 1.7149532710280373, "no_speech_prob": 3.340480361657683e-06}, {"id": 563, "seek": 279592, "start": 2795.92, "end": 2802.12, "text": " But what it does is something pretty nifty, which is that you can get an image of a handwritten", "tokens": [583, 437, 309, 775, 307, 746, 1238, 297, 37177, 11, 597, 307, 300, 291, 393, 483, 364, 3256, 295, 257, 1011, 26859], "temperature": 0.0, "avg_logprob": -0.11439928515204068, "compression_ratio": 1.5922330097087378, "no_speech_prob": 2.3320569653151324e-06}, {"id": 564, "seek": 279592, "start": 2802.12, "end": 2811.76, "text": " digit and you can pass it over into this web API, into this rest endpoint or whatever.", "tokens": [14293, 293, 291, 393, 1320, 309, 670, 666, 341, 3670, 9362, 11, 666, 341, 1472, 35795, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.11439928515204068, "compression_ratio": 1.5922330097087378, "no_speech_prob": 2.3320569653151324e-06}, {"id": 565, "seek": 279592, "start": 2811.76, "end": 2816.84, "text": " It's just a black box as far as we're concerned.", "tokens": [467, 311, 445, 257, 2211, 2424, 382, 1400, 382, 321, 434, 5922, 13], "temperature": 0.0, "avg_logprob": -0.11439928515204068, "compression_ratio": 1.5922330097087378, "no_speech_prob": 2.3320569653151324e-06}, {"id": 566, "seek": 279592, "start": 2816.84, "end": 2823.8, "text": " And it's going to spit out the probability that this thing you passed in is a handwritten", "tokens": [400, 309, 311, 516, 281, 22127, 484, 264, 8482, 300, 341, 551, 291, 4678, 294, 307, 257, 1011, 26859], "temperature": 0.0, "avg_logprob": -0.11439928515204068, "compression_ratio": 1.5922330097087378, "no_speech_prob": 2.3320569653151324e-06}, {"id": 567, "seek": 279592, "start": 2823.8, "end": 2824.8, "text": " digit.", "tokens": [14293, 13], "temperature": 0.0, "avg_logprob": -0.11439928515204068, "compression_ratio": 1.5922330097087378, "no_speech_prob": 2.3320569653151324e-06}, {"id": 568, "seek": 282480, "start": 2824.8, "end": 2831.32, "text": " So for this one, so let's say this image is called x1.", "tokens": [407, 337, 341, 472, 11, 370, 718, 311, 584, 341, 3256, 307, 1219, 2031, 16, 13], "temperature": 0.0, "avg_logprob": -0.10693901093279728, "compression_ratio": 1.3396226415094339, "no_speech_prob": 1.3287741467138403e-06}, {"id": 569, "seek": 282480, "start": 2831.32, "end": 2839.6400000000003, "text": " The probability that x1 is a handwritten digit, it might say is 0.98.", "tokens": [440, 8482, 300, 2031, 16, 307, 257, 1011, 26859, 14293, 11, 309, 1062, 584, 307, 1958, 13, 22516, 13], "temperature": 0.0, "avg_logprob": -0.10693901093279728, "compression_ratio": 1.3396226415094339, "no_speech_prob": 1.3287741467138403e-06}, {"id": 570, "seek": 282480, "start": 2839.6400000000003, "end": 2852.86, "text": " And so then you pass something else into this magic API endpoint, which looks like this.", "tokens": [400, 370, 550, 291, 1320, 746, 1646, 666, 341, 5585, 9362, 35795, 11, 597, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.10693901093279728, "compression_ratio": 1.3396226415094339, "no_speech_prob": 1.3287741467138403e-06}, {"id": 571, "seek": 285286, "start": 2852.86, "end": 2859.1600000000003, "text": " You pass that in and that looks a little bit like an 8, I guess, but it might not be.", "tokens": [509, 1320, 300, 294, 293, 300, 1542, 257, 707, 857, 411, 364, 1649, 11, 286, 2041, 11, 457, 309, 1062, 406, 312, 13], "temperature": 0.0, "avg_logprob": -0.20133716409856622, "compression_ratio": 1.3509933774834437, "no_speech_prob": 1.2679270184889901e-06}, {"id": 572, "seek": 285286, "start": 2859.1600000000003, "end": 2861.56, "text": " You pass it into this API and you see what happens.", "tokens": [509, 1320, 309, 666, 341, 9362, 293, 291, 536, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.20133716409856622, "compression_ratio": 1.3509933774834437, "no_speech_prob": 1.2679270184889901e-06}, {"id": 573, "seek": 285286, "start": 2861.56, "end": 2864.3, "text": " This is x2.", "tokens": [639, 307, 2031, 17, 13], "temperature": 0.0, "avg_logprob": -0.20133716409856622, "compression_ratio": 1.3509933774834437, "no_speech_prob": 1.2679270184889901e-06}, {"id": 574, "seek": 285286, "start": 2864.3, "end": 2875.6400000000003, "text": " And it says the probability that x2 is a digit is 0.4.", "tokens": [400, 309, 1619, 264, 8482, 300, 2031, 17, 307, 257, 14293, 307, 1958, 13, 19, 13], "temperature": 0.0, "avg_logprob": -0.20133716409856622, "compression_ratio": 1.3509933774834437, "no_speech_prob": 1.2679270184889901e-06}, {"id": 575, "seek": 287564, "start": 2875.64, "end": 2899.7599999999998, "text": " Okay, now we pass in our image x3 into our magic API and it returns the probability that", "tokens": [1033, 11, 586, 321, 1320, 294, 527, 3256, 2031, 18, 666, 527, 5585, 9362, 293, 309, 11247, 264, 8482, 300], "temperature": 0.0, "avg_logprob": -0.18754975001017252, "compression_ratio": 1.0476190476190477, "no_speech_prob": 3.9897093984109233e-07}, {"id": 576, "seek": 289976, "start": 2899.76, "end": 2908.92, "text": " x3 is a handwritten digit, pretty small.", "tokens": [2031, 18, 307, 257, 1011, 26859, 14293, 11, 1238, 1359, 13], "temperature": 0.0, "avg_logprob": -0.21658163804274339, "compression_ratio": 1.2575757575757576, "no_speech_prob": 1.209863853546267e-06}, {"id": 577, "seek": 289976, "start": 2908.92, "end": 2917.28, "text": " Okay, so why is this interesting?", "tokens": [1033, 11, 370, 983, 307, 341, 1880, 30], "temperature": 0.0, "avg_logprob": -0.21658163804274339, "compression_ratio": 1.2575757575757576, "no_speech_prob": 1.209863853546267e-06}, {"id": 578, "seek": 289976, "start": 2917.28, "end": 2926.5600000000004, "text": " Well, it turns out that if you have a function, you know, let's not call this an API, let's", "tokens": [1042, 11, 309, 4523, 484, 300, 498, 291, 362, 257, 2445, 11, 291, 458, 11, 718, 311, 406, 818, 341, 364, 9362, 11, 718, 311], "temperature": 0.0, "avg_logprob": -0.21658163804274339, "compression_ratio": 1.2575757575757576, "no_speech_prob": 1.209863853546267e-06}, {"id": 579, "seek": 292656, "start": 2926.56, "end": 2938.96, "text": " call this, let's call this, it's called f, some function, but it's like behind some web", "tokens": [818, 341, 11, 718, 311, 818, 341, 11, 309, 311, 1219, 283, 11, 512, 2445, 11, 457, 309, 311, 411, 2261, 512, 3670], "temperature": 0.0, "avg_logprob": -0.15261849061942395, "compression_ratio": 1.4871794871794872, "no_speech_prob": 1.328774260400678e-06}, {"id": 580, "seek": 292656, "start": 2938.96, "end": 2941.7999999999997, "text": " API, REST endpoint, whatever.", "tokens": [9362, 11, 497, 14497, 35795, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.15261849061942395, "compression_ratio": 1.4871794871794872, "no_speech_prob": 1.328774260400678e-06}, {"id": 581, "seek": 292656, "start": 2941.7999999999997, "end": 2949.88, "text": " If you have this function, we can actually use it to generate handwritten digits.", "tokens": [759, 291, 362, 341, 2445, 11, 321, 393, 767, 764, 309, 281, 8460, 1011, 26859, 27011, 13], "temperature": 0.0, "avg_logprob": -0.15261849061942395, "compression_ratio": 1.4871794871794872, "no_speech_prob": 1.328774260400678e-06}, {"id": 582, "seek": 292656, "start": 2949.88, "end": 2953.0, "text": " So that's something pretty magical.", "tokens": [407, 300, 311, 746, 1238, 12066, 13], "temperature": 0.0, "avg_logprob": -0.15261849061942395, "compression_ratio": 1.4871794871794872, "no_speech_prob": 1.328774260400678e-06}, {"id": 583, "seek": 292656, "start": 2953.0, "end": 2955.04, "text": " And we're going to see how on earth would you do that?", "tokens": [400, 321, 434, 516, 281, 536, 577, 322, 4120, 576, 291, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.15261849061942395, "compression_ratio": 1.4871794871794872, "no_speech_prob": 1.328774260400678e-06}, {"id": 584, "seek": 295504, "start": 2955.04, "end": 2960.96, "text": " If you have this function, which can take an image and tell you the probability that", "tokens": [759, 291, 362, 341, 2445, 11, 597, 393, 747, 364, 3256, 293, 980, 291, 264, 8482, 300], "temperature": 0.0, "avg_logprob": -0.14752434094746908, "compression_ratio": 1.5123456790123457, "no_speech_prob": 1.2482666988944402e-06}, {"id": 585, "seek": 295504, "start": 2960.96, "end": 2970.56, "text": " that is a handwritten digit, how could you use it to generate new images?", "tokens": [300, 307, 257, 1011, 26859, 14293, 11, 577, 727, 291, 764, 309, 281, 8460, 777, 5267, 30], "temperature": 0.0, "avg_logprob": -0.14752434094746908, "compression_ratio": 1.5123456790123457, "no_speech_prob": 1.2482666988944402e-06}, {"id": 586, "seek": 295504, "start": 2970.56, "end": 2983.6, "text": " Well, imagine you wanted to turn this mess into something that did look like an image.", "tokens": [1042, 11, 3811, 291, 1415, 281, 1261, 341, 2082, 666, 746, 300, 630, 574, 411, 364, 3256, 13], "temperature": 0.0, "avg_logprob": -0.14752434094746908, "compression_ratio": 1.5123456790123457, "no_speech_prob": 1.2482666988944402e-06}, {"id": 587, "seek": 298360, "start": 2983.6, "end": 2985.92, "text": " Here's something you could do.", "tokens": [1692, 311, 746, 291, 727, 360, 13], "temperature": 0.0, "avg_logprob": -0.4256941708651456, "compression_ratio": 1.1025641025641026, "no_speech_prob": 8.664588676765561e-06}, {"id": 588, "seek": 298360, "start": 2985.92, "end": 2996.7999999999997, "text": " Let's say it's a 28 by 28 image, which is what, 786?", "tokens": [961, 311, 584, 309, 311, 257, 7562, 538, 7562, 3256, 11, 597, 307, 437, 11, 1614, 22193, 30], "temperature": 0.0, "avg_logprob": -0.4256941708651456, "compression_ratio": 1.1025641025641026, "no_speech_prob": 8.664588676765561e-06}, {"id": 589, "seek": 298360, "start": 2996.7999999999997, "end": 2998.8399999999997, "text": " Oopsie-dozy.", "tokens": [21726, 414, 12, 2595, 1229, 13], "temperature": 0.0, "avg_logprob": -0.4256941708651456, "compression_ratio": 1.1025641025641026, "no_speech_prob": 8.664588676765561e-06}, {"id": 590, "seek": 298360, "start": 2998.8399999999997, "end": 3003.36, "text": " 28 times 28, 784.", "tokens": [7562, 1413, 7562, 11, 1614, 25494, 13], "temperature": 0.0, "avg_logprob": -0.4256941708651456, "compression_ratio": 1.1025641025641026, "no_speech_prob": 8.664588676765561e-06}, {"id": 591, "seek": 298360, "start": 3003.36, "end": 3009.22, "text": " So 794 pixels.", "tokens": [407, 1614, 27032, 18668, 13], "temperature": 0.0, "avg_logprob": -0.4256941708651456, "compression_ratio": 1.1025641025641026, "no_speech_prob": 8.664588676765561e-06}, {"id": 592, "seek": 300922, "start": 3009.22, "end": 3016.6, "text": " And we could pick one of these pixels and say, what if I increase this pixel to be a", "tokens": [400, 321, 727, 1888, 472, 295, 613, 18668, 293, 584, 11, 437, 498, 286, 3488, 341, 19261, 281, 312, 257], "temperature": 0.0, "avg_logprob": -0.0890138257633556, "compression_ratio": 1.6064814814814814, "no_speech_prob": 3.1381364351545926e-06}, {"id": 593, "seek": 300922, "start": 3016.6, "end": 3018.3799999999997, "text": " little bit darker?", "tokens": [707, 857, 12741, 30], "temperature": 0.0, "avg_logprob": -0.0890138257633556, "compression_ratio": 1.6064814814814814, "no_speech_prob": 3.1381364351545926e-06}, {"id": 594, "seek": 300922, "start": 3018.3799999999997, "end": 3024.02, "text": " And then we could pass that image through f and we could see what happens to the probability", "tokens": [400, 550, 321, 727, 1320, 300, 3256, 807, 283, 293, 321, 727, 536, 437, 2314, 281, 264, 8482], "temperature": 0.0, "avg_logprob": -0.0890138257633556, "compression_ratio": 1.6064814814814814, "no_speech_prob": 3.1381364351545926e-06}, {"id": 595, "seek": 300922, "start": 3024.02, "end": 3026.58, "text": " that it's a handwritten digit.", "tokens": [300, 309, 311, 257, 1011, 26859, 14293, 13], "temperature": 0.0, "avg_logprob": -0.0890138257633556, "compression_ratio": 1.6064814814814814, "no_speech_prob": 3.1381364351545926e-06}, {"id": 596, "seek": 300922, "start": 3026.58, "end": 3033.4199999999996, "text": " So for a specific example, handwritten digits don't normally have any pixels that are black", "tokens": [407, 337, 257, 2685, 1365, 11, 1011, 26859, 27011, 500, 380, 5646, 362, 604, 18668, 300, 366, 2211], "temperature": 0.0, "avg_logprob": -0.0890138257633556, "compression_ratio": 1.6064814814814814, "no_speech_prob": 3.1381364351545926e-06}, {"id": 597, "seek": 300922, "start": 3033.4199999999996, "end": 3035.2, "text": " in the very bottom corners.", "tokens": [294, 264, 588, 2767, 12413, 13], "temperature": 0.0, "avg_logprob": -0.0890138257633556, "compression_ratio": 1.6064814814814814, "no_speech_prob": 3.1381364351545926e-06}, {"id": 598, "seek": 303520, "start": 3035.2, "end": 3040.0, "text": " So if we took this here and we said, what would happen if we made this a little bit", "tokens": [407, 498, 321, 1890, 341, 510, 293, 321, 848, 11, 437, 576, 1051, 498, 321, 1027, 341, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.09319825548874705, "compression_ratio": 1.515625, "no_speech_prob": 3.5763551409218053e-07}, {"id": 599, "seek": 303520, "start": 3040.0, "end": 3042.64, "text": " lighter?", "tokens": [11546, 30], "temperature": 0.0, "avg_logprob": -0.09319825548874705, "compression_ratio": 1.515625, "no_speech_prob": 3.5763551409218053e-07}, {"id": 600, "seek": 303520, "start": 3042.64, "end": 3047.68, "text": " And then we passed that exact image through here, the probability would probably go up", "tokens": [400, 550, 321, 4678, 300, 1900, 3256, 807, 510, 11, 264, 8482, 576, 1391, 352, 493], "temperature": 0.0, "avg_logprob": -0.09319825548874705, "compression_ratio": 1.515625, "no_speech_prob": 3.5763551409218053e-07}, {"id": 601, "seek": 303520, "start": 3047.68, "end": 3054.7999999999997, "text": " a tiny bit, for example.", "tokens": [257, 5870, 857, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.09319825548874705, "compression_ratio": 1.515625, "no_speech_prob": 3.5763551409218053e-07}, {"id": 602, "seek": 303520, "start": 3054.7999999999997, "end": 3063.52, "text": " So now we've got an image which is slightly more like a handwritten digit than before.", "tokens": [407, 586, 321, 600, 658, 364, 3256, 597, 307, 4748, 544, 411, 257, 1011, 26859, 14293, 813, 949, 13], "temperature": 0.0, "avg_logprob": -0.09319825548874705, "compression_ratio": 1.515625, "no_speech_prob": 3.5763551409218053e-07}, {"id": 603, "seek": 306352, "start": 3063.52, "end": 3068.56, "text": " And also in digits, generally there are straight lines.", "tokens": [400, 611, 294, 27011, 11, 5101, 456, 366, 2997, 3876, 13], "temperature": 0.0, "avg_logprob": -0.07587649063630537, "compression_ratio": 1.624413145539906, "no_speech_prob": 9.874618172034388e-07}, {"id": 604, "seek": 306352, "start": 3068.56, "end": 3073.4, "text": " So this pixel here, it probably makes sense for it to be darker.", "tokens": [407, 341, 19261, 510, 11, 309, 1391, 1669, 2020, 337, 309, 281, 312, 12741, 13], "temperature": 0.0, "avg_logprob": -0.07587649063630537, "compression_ratio": 1.624413145539906, "no_speech_prob": 9.874618172034388e-07}, {"id": 605, "seek": 306352, "start": 3073.4, "end": 3078.28, "text": " So if we made a slightly darker version of this pixel and sent it through here, that", "tokens": [407, 498, 321, 1027, 257, 4748, 12741, 3037, 295, 341, 19261, 293, 2279, 309, 807, 510, 11, 300], "temperature": 0.0, "avg_logprob": -0.07587649063630537, "compression_ratio": 1.624413145539906, "no_speech_prob": 9.874618172034388e-07}, {"id": 606, "seek": 306352, "start": 3078.28, "end": 3085.44, "text": " would also increase the probability a little bit.", "tokens": [576, 611, 3488, 264, 8482, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.07587649063630537, "compression_ratio": 1.624413145539906, "no_speech_prob": 9.874618172034388e-07}, {"id": 607, "seek": 306352, "start": 3085.44, "end": 3090.7599999999998, "text": " And so we could do that for every single pixel of the 28 by 28, one at a time, finding out", "tokens": [400, 370, 321, 727, 360, 300, 337, 633, 2167, 19261, 295, 264, 7562, 538, 7562, 11, 472, 412, 257, 565, 11, 5006, 484], "temperature": 0.0, "avg_logprob": -0.07587649063630537, "compression_ratio": 1.624413145539906, "no_speech_prob": 9.874618172034388e-07}, {"id": 608, "seek": 309076, "start": 3090.76, "end": 3094.84, "text": " which ones, if we make them a little bit lighter, make it more like a handwritten digit, which", "tokens": [597, 2306, 11, 498, 321, 652, 552, 257, 707, 857, 11546, 11, 652, 309, 544, 411, 257, 1011, 26859, 14293, 11, 597], "temperature": 0.0, "avg_logprob": -0.08452247410285764, "compression_ratio": 1.8622754491017963, "no_speech_prob": 1.8738730886980193e-06}, {"id": 609, "seek": 309076, "start": 3094.84, "end": 3100.28, "text": " ones if we make it a little bit darker, make it more like a handwritten digit.", "tokens": [2306, 498, 321, 652, 309, 257, 707, 857, 12741, 11, 652, 309, 544, 411, 257, 1011, 26859, 14293, 13], "temperature": 0.0, "avg_logprob": -0.08452247410285764, "compression_ratio": 1.8622754491017963, "no_speech_prob": 1.8738730886980193e-06}, {"id": 610, "seek": 309076, "start": 3100.28, "end": 3111.28, "text": " What we've just done is we've calculated the gradient of the probability that x3 is a handwritten", "tokens": [708, 321, 600, 445, 1096, 307, 321, 600, 15598, 264, 16235, 295, 264, 8482, 300, 2031, 18, 307, 257, 1011, 26859], "temperature": 0.0, "avg_logprob": -0.08452247410285764, "compression_ratio": 1.8622754491017963, "no_speech_prob": 1.8738730886980193e-06}, {"id": 611, "seek": 309076, "start": 3111.28, "end": 3119.7200000000003, "text": " digit with respect to the pixels of x3.", "tokens": [14293, 365, 3104, 281, 264, 18668, 295, 2031, 18, 13], "temperature": 0.0, "avg_logprob": -0.08452247410285764, "compression_ratio": 1.8622754491017963, "no_speech_prob": 1.8738730886980193e-06}, {"id": 612, "seek": 311972, "start": 3119.72, "end": 3133.48, "text": " Now notice that I didn't say dpx3 dx3, which you might be familiar with from high school.", "tokens": [823, 3449, 300, 286, 994, 380, 584, 274, 79, 87, 18, 30017, 18, 11, 597, 291, 1062, 312, 4963, 365, 490, 1090, 1395, 13], "temperature": 0.0, "avg_logprob": -0.10280382973807199, "compression_ratio": 1.4488636363636365, "no_speech_prob": 4.785076725966064e-06}, {"id": 613, "seek": 311972, "start": 3133.48, "end": 3141.0, "text": " And the reason for that is that we've calculated this for every single pixel.", "tokens": [400, 264, 1778, 337, 300, 307, 300, 321, 600, 15598, 341, 337, 633, 2167, 19261, 13], "temperature": 0.0, "avg_logprob": -0.10280382973807199, "compression_ratio": 1.4488636363636365, "no_speech_prob": 4.785076725966064e-06}, {"id": 614, "seek": 311972, "start": 3141.0, "end": 3146.9199999999996, "text": " And so when you do it for lots of different inputs, you have to turn the d into a, this", "tokens": [400, 370, 562, 291, 360, 309, 337, 3195, 295, 819, 15743, 11, 291, 362, 281, 1261, 264, 274, 666, 257, 11, 341], "temperature": 0.0, "avg_logprob": -0.10280382973807199, "compression_ratio": 1.4488636363636365, "no_speech_prob": 4.785076725966064e-06}, {"id": 615, "seek": 314692, "start": 3146.92, "end": 3150.28, "text": " is called a del or a nabla.", "tokens": [307, 1219, 257, 1103, 420, 257, 297, 455, 875, 13], "temperature": 0.0, "avg_logprob": -0.16353686650594076, "compression_ratio": 1.8333333333333333, "no_speech_prob": 8.013466867851093e-06}, {"id": 616, "seek": 314692, "start": 3150.28, "end": 3152.36, "text": " And it just means there's lots of values here.", "tokens": [400, 309, 445, 1355, 456, 311, 3195, 295, 4190, 510, 13], "temperature": 0.0, "avg_logprob": -0.16353686650594076, "compression_ratio": 1.8333333333333333, "no_speech_prob": 8.013466867851093e-06}, {"id": 617, "seek": 314692, "start": 3152.36, "end": 3160.12, "text": " So this here contains lots of values, which is the, how much does the probability that", "tokens": [407, 341, 510, 8306, 3195, 295, 4190, 11, 597, 307, 264, 11, 577, 709, 775, 264, 8482, 300], "temperature": 0.0, "avg_logprob": -0.16353686650594076, "compression_ratio": 1.8333333333333333, "no_speech_prob": 8.013466867851093e-06}, {"id": 618, "seek": 314692, "start": 3160.12, "end": 3167.6, "text": " x3 is a digit increase as we increase this pixel value and as we increase this pixel", "tokens": [2031, 18, 307, 257, 14293, 3488, 382, 321, 3488, 341, 19261, 2158, 293, 382, 321, 3488, 341, 19261], "temperature": 0.0, "avg_logprob": -0.16353686650594076, "compression_ratio": 1.8333333333333333, "no_speech_prob": 8.013466867851093e-06}, {"id": 619, "seek": 314692, "start": 3167.6, "end": 3169.96, "text": " value, as we increase this pixel value.", "tokens": [2158, 11, 382, 321, 3488, 341, 19261, 2158, 13], "temperature": 0.0, "avg_logprob": -0.16353686650594076, "compression_ratio": 1.8333333333333333, "no_speech_prob": 8.013466867851093e-06}, {"id": 620, "seek": 316996, "start": 3169.96, "end": 3179.6, "text": " So there's going to be, for 28 by 28 inputs, there's going to be 784 pixels, which means", "tokens": [407, 456, 311, 516, 281, 312, 11, 337, 7562, 538, 7562, 15743, 11, 456, 311, 516, 281, 312, 1614, 25494, 18668, 11, 597, 1355], "temperature": 0.0, "avg_logprob": -0.1416564186414083, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.5534935755567858e-06}, {"id": 621, "seek": 316996, "start": 3179.6, "end": 3183.96, "text": " that this thing here has 784 values.", "tokens": [300, 341, 551, 510, 575, 1614, 25494, 4190, 13], "temperature": 0.0, "avg_logprob": -0.1416564186414083, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.5534935755567858e-06}, {"id": 622, "seek": 316996, "start": 3183.96, "end": 3189.68, "text": " Okay, I totally messed up the notation there.", "tokens": [1033, 11, 286, 3879, 16507, 493, 264, 24657, 456, 13], "temperature": 0.0, "avg_logprob": -0.1416564186414083, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.5534935755567858e-06}, {"id": 623, "seek": 316996, "start": 3189.68, "end": 3194.06, "text": " I did think about going back and re-recording it, but then I thought, well, maybe instead", "tokens": [286, 630, 519, 466, 516, 646, 293, 319, 12, 13867, 3357, 309, 11, 457, 550, 286, 1194, 11, 731, 11, 1310, 2602], "temperature": 0.0, "avg_logprob": -0.1416564186414083, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.5534935755567858e-06}, {"id": 624, "seek": 316996, "start": 3194.06, "end": 3199.94, "text": " as penance for my failure to get the notation right, I should instead record a little section", "tokens": [382, 3435, 719, 337, 452, 7763, 281, 483, 264, 24657, 558, 11, 286, 820, 2602, 2136, 257, 707, 3541], "temperature": 0.0, "avg_logprob": -0.1416564186414083, "compression_ratio": 1.6063348416289593, "no_speech_prob": 1.5534935755567858e-06}, {"id": 625, "seek": 319994, "start": 3199.94, "end": 3207.6, "text": " describing the notation in more detail, both for myself, so I don't make it a mistake again", "tokens": [16141, 264, 24657, 294, 544, 2607, 11, 1293, 337, 2059, 11, 370, 286, 500, 380, 652, 309, 257, 6146, 797], "temperature": 0.0, "avg_logprob": -0.17636809477934967, "compression_ratio": 1.6704980842911878, "no_speech_prob": 2.2473746867035516e-05}, {"id": 626, "seek": 319994, "start": 3207.6, "end": 3211.6, "text": " and for the rest of you, so you understand exactly what's going on.", "tokens": [293, 337, 264, 1472, 295, 291, 11, 370, 291, 1223, 2293, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.17636809477934967, "compression_ratio": 1.6704980842911878, "no_speech_prob": 2.2473746867035516e-05}, {"id": 627, "seek": 319994, "start": 3211.6, "end": 3216.56, "text": " I think it's actually pretty worthwhile because this notation does come up a lot and I've", "tokens": [286, 519, 309, 311, 767, 1238, 28159, 570, 341, 24657, 775, 808, 493, 257, 688, 293, 286, 600], "temperature": 0.0, "avg_logprob": -0.17636809477934967, "compression_ratio": 1.6704980842911878, "no_speech_prob": 2.2473746867035516e-05}, {"id": 628, "seek": 319994, "start": 3216.56, "end": 3222.64, "text": " been regularly butchering it in talks and notes for years now, so it's about time I", "tokens": [668, 11672, 457, 339, 1794, 309, 294, 6686, 293, 5570, 337, 924, 586, 11, 370, 309, 311, 466, 565, 286], "temperature": 0.0, "avg_logprob": -0.17636809477934967, "compression_ratio": 1.6704980842911878, "no_speech_prob": 2.2473746867035516e-05}, {"id": 629, "seek": 319994, "start": 3222.64, "end": 3225.16, "text": " got it right.", "tokens": [658, 309, 558, 13], "temperature": 0.0, "avg_logprob": -0.17636809477934967, "compression_ratio": 1.6704980842911878, "no_speech_prob": 2.2473746867035516e-05}, {"id": 630, "seek": 319994, "start": 3225.16, "end": 3229.52, "text": " So I should mention I have absolutely no excuse for butchering the notation like I have.", "tokens": [407, 286, 820, 2152, 286, 362, 3122, 572, 8960, 337, 457, 339, 1794, 264, 24657, 411, 286, 362, 13], "temperature": 0.0, "avg_logprob": -0.17636809477934967, "compression_ratio": 1.6704980842911878, "no_speech_prob": 2.2473746867035516e-05}, {"id": 631, "seek": 322952, "start": 3229.52, "end": 3238.8, "text": " On the basis that actually my friend Terrence and I wrote a, what is it, like 30, 40 page", "tokens": [1282, 264, 5143, 300, 767, 452, 1277, 6564, 10760, 293, 286, 4114, 257, 11, 437, 307, 309, 11, 411, 2217, 11, 3356, 3028], "temperature": 0.0, "avg_logprob": -0.19927946126685953, "compression_ratio": 1.3288590604026846, "no_speech_prob": 5.862708803761052e-06}, {"id": 632, "seek": 322952, "start": 3238.8, "end": 3251.72, "text": " tutorial on matrix calculus, and in that paper, he actually described everything I'm going", "tokens": [7073, 322, 8141, 33400, 11, 293, 294, 300, 3035, 11, 415, 767, 7619, 1203, 286, 478, 516], "temperature": 0.0, "avg_logprob": -0.19927946126685953, "compression_ratio": 1.3288590604026846, "no_speech_prob": 5.862708803761052e-06}, {"id": 633, "seek": 322952, "start": 3251.72, "end": 3253.48, "text": " to show you here.", "tokens": [281, 855, 291, 510, 13], "temperature": 0.0, "avg_logprob": -0.19927946126685953, "compression_ratio": 1.3288590604026846, "no_speech_prob": 5.862708803761052e-06}, {"id": 634, "seek": 325348, "start": 3253.48, "end": 3260.36, "text": " Having said that, you certainly don't need to read this rather lengthy tutorial.", "tokens": [10222, 848, 300, 11, 291, 3297, 500, 380, 643, 281, 1401, 341, 2831, 35374, 7073, 13], "temperature": 0.0, "avg_logprob": -0.06474290022978911, "compression_ratio": 1.529126213592233, "no_speech_prob": 1.3709509403270204e-06}, {"id": 635, "seek": 325348, "start": 3260.36, "end": 3264.36, "text": " I'm going to explain the key stuff that actually I think it's worth knowing, and then you'll", "tokens": [286, 478, 516, 281, 2903, 264, 2141, 1507, 300, 767, 286, 519, 309, 311, 3163, 5276, 11, 293, 550, 291, 603], "temperature": 0.0, "avg_logprob": -0.06474290022978911, "compression_ratio": 1.529126213592233, "no_speech_prob": 1.3709509403270204e-06}, {"id": 636, "seek": 325348, "start": 3264.36, "end": 3270.92, "text": " understand the mistake that I made during the lesson.", "tokens": [1223, 264, 6146, 300, 286, 1027, 1830, 264, 6898, 13], "temperature": 0.0, "avg_logprob": -0.06474290022978911, "compression_ratio": 1.529126213592233, "no_speech_prob": 1.3709509403270204e-06}, {"id": 637, "seek": 325348, "start": 3270.92, "end": 3279.16, "text": " Maybe let's start with some reminders from stuff that hopefully you did at high school.", "tokens": [2704, 718, 311, 722, 365, 512, 43458, 490, 1507, 300, 4696, 291, 630, 412, 1090, 1395, 13], "temperature": 0.0, "avg_logprob": -0.06474290022978911, "compression_ratio": 1.529126213592233, "no_speech_prob": 1.3709509403270204e-06}, {"id": 638, "seek": 327916, "start": 3279.16, "end": 3289.08, "text": " So let's create maybe a 2D version here and maybe a 3D version as well.", "tokens": [407, 718, 311, 1884, 1310, 257, 568, 35, 3037, 510, 293, 1310, 257, 805, 35, 3037, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.18360491232438522, "compression_ratio": 1.3109243697478992, "no_speech_prob": 6.339084393403027e-06}, {"id": 639, "seek": 327916, "start": 3289.08, "end": 3305.96, "text": " Okay, so let's say for example we've got a quadratic that looks something like this,", "tokens": [1033, 11, 370, 718, 311, 584, 337, 1365, 321, 600, 658, 257, 37262, 300, 1542, 746, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.18360491232438522, "compression_ratio": 1.3109243697478992, "no_speech_prob": 6.339084393403027e-06}, {"id": 640, "seek": 330596, "start": 3305.96, "end": 3314.8, "text": " and we can say this is an equation such as y equals x squared, and we might endeavor", "tokens": [293, 321, 393, 584, 341, 307, 364, 5367, 1270, 382, 288, 6915, 2031, 8889, 11, 293, 321, 1062, 34975], "temperature": 0.0, "avg_logprob": -0.13556172450383505, "compression_ratio": 1.4603174603174602, "no_speech_prob": 3.2377415664086584e-06}, {"id": 641, "seek": 330596, "start": 3314.8, "end": 3327.88, "text": " to identify the slope kind of at some exact moment like here.", "tokens": [281, 5876, 264, 13525, 733, 295, 412, 512, 1900, 1623, 411, 510, 13], "temperature": 0.0, "avg_logprob": -0.13556172450383505, "compression_ratio": 1.4603174603174602, "no_speech_prob": 3.2377415664086584e-06}, {"id": 642, "seek": 330596, "start": 3327.88, "end": 3331.32, "text": " This slope at this exact moment here.", "tokens": [639, 13525, 412, 341, 1900, 1623, 510, 13], "temperature": 0.0, "avg_logprob": -0.13556172450383505, "compression_ratio": 1.4603174603174602, "no_speech_prob": 3.2377415664086584e-06}, {"id": 643, "seek": 333132, "start": 3331.32, "end": 3337.6000000000004, "text": " So this thing is called the tangent, and the slope at a tangent is the derivative of a", "tokens": [407, 341, 551, 307, 1219, 264, 27747, 11, 293, 264, 13525, 412, 257, 27747, 307, 264, 13760, 295, 257], "temperature": 0.0, "avg_logprob": -0.17068141630326195, "compression_ratio": 1.675977653631285, "no_speech_prob": 3.138142346870154e-06}, {"id": 644, "seek": 333132, "start": 3337.6000000000004, "end": 3339.28, "text": " function.", "tokens": [2445, 13], "temperature": 0.0, "avg_logprob": -0.17068141630326195, "compression_ratio": 1.675977653631285, "no_speech_prob": 3.138142346870154e-06}, {"id": 645, "seek": 333132, "start": 3339.28, "end": 3344.84, "text": " And so the derivative, let me just try to make this look a bit more like a y than an", "tokens": [400, 370, 264, 13760, 11, 718, 385, 445, 853, 281, 652, 341, 574, 257, 857, 544, 411, 257, 288, 813, 364], "temperature": 0.0, "avg_logprob": -0.17068141630326195, "compression_ratio": 1.675977653631285, "no_speech_prob": 3.138142346870154e-06}, {"id": 646, "seek": 333132, "start": 3344.84, "end": 3345.84, "text": " x.", "tokens": [2031, 13], "temperature": 0.0, "avg_logprob": -0.17068141630326195, "compression_ratio": 1.675977653631285, "no_speech_prob": 3.138142346870154e-06}, {"id": 647, "seek": 333132, "start": 3345.84, "end": 3349.36, "text": " Maybe I'll write it like this.", "tokens": [2704, 286, 603, 2464, 309, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.17068141630326195, "compression_ratio": 1.675977653631285, "no_speech_prob": 3.138142346870154e-06}, {"id": 648, "seek": 333132, "start": 3349.36, "end": 3354.4, "text": " Okay, and so the derivative of a function we can write in a few ways, but one way we", "tokens": [1033, 11, 293, 370, 264, 13760, 295, 257, 2445, 321, 393, 2464, 294, 257, 1326, 2098, 11, 457, 472, 636, 321], "temperature": 0.0, "avg_logprob": -0.17068141630326195, "compression_ratio": 1.675977653631285, "no_speech_prob": 3.138142346870154e-06}, {"id": 649, "seek": 335440, "start": 3354.4, "end": 3361.6, "text": " can write it is like this dy dx, and there are some rules we can use to calculate it", "tokens": [393, 2464, 309, 307, 411, 341, 14584, 30017, 11, 293, 456, 366, 512, 4474, 321, 393, 764, 281, 8873, 309], "temperature": 0.0, "avg_logprob": -0.16678505930407295, "compression_ratio": 1.5034482758620689, "no_speech_prob": 3.393137831153581e-06}, {"id": 650, "seek": 335440, "start": 3361.6, "end": 3368.0, "text": " analytically, and for squared the rule is that we it's 2x.", "tokens": [10783, 984, 11, 293, 337, 8889, 264, 4978, 307, 300, 321, 309, 311, 568, 87, 13], "temperature": 0.0, "avg_logprob": -0.16678505930407295, "compression_ratio": 1.5034482758620689, "no_speech_prob": 3.393137831153581e-06}, {"id": 651, "seek": 335440, "start": 3368.0, "end": 3377.04, "text": " You basically move the index out to the front to calculate its derivative.", "tokens": [509, 1936, 1286, 264, 8186, 484, 281, 264, 1868, 281, 8873, 1080, 13760, 13], "temperature": 0.0, "avg_logprob": -0.16678505930407295, "compression_ratio": 1.5034482758620689, "no_speech_prob": 3.393137831153581e-06}, {"id": 652, "seek": 337704, "start": 3377.04, "end": 3385.88, "text": " Another way of writing y equals x squared is we could write f of x equals x squared,", "tokens": [3996, 636, 295, 3579, 288, 6915, 2031, 8889, 307, 321, 727, 2464, 283, 295, 2031, 6915, 2031, 8889, 11], "temperature": 0.0, "avg_logprob": -0.16016556786709144, "compression_ratio": 1.6025641025641026, "no_speech_prob": 3.785316266657901e-06}, {"id": 653, "seek": 337704, "start": 3385.88, "end": 3392.32, "text": " and another way of writing the derivative is we could use this for example.", "tokens": [293, 1071, 636, 295, 3579, 264, 13760, 307, 321, 727, 764, 341, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.16016556786709144, "compression_ratio": 1.6025641025641026, "no_speech_prob": 3.785316266657901e-06}, {"id": 654, "seek": 337704, "start": 3392.32, "end": 3400.96, "text": " Okay, so this is all stuff that hopefully you at least vaguely remember from high school.", "tokens": [1033, 11, 370, 341, 307, 439, 1507, 300, 4696, 291, 412, 1935, 13501, 48863, 1604, 490, 1090, 1395, 13], "temperature": 0.0, "avg_logprob": -0.16016556786709144, "compression_ratio": 1.6025641025641026, "no_speech_prob": 3.785316266657901e-06}, {"id": 655, "seek": 340096, "start": 3400.96, "end": 3409.28, "text": " Now functions are not necessarily just of one variable, so here we've got x and y, but", "tokens": [823, 6828, 366, 406, 4725, 445, 295, 472, 7006, 11, 370, 510, 321, 600, 658, 2031, 293, 288, 11, 457], "temperature": 0.0, "avg_logprob": -0.14040134974888394, "compression_ratio": 1.4770114942528736, "no_speech_prob": 2.8573110739671392e-06}, {"id": 656, "seek": 340096, "start": 3409.28, "end": 3417.52, "text": " they could be of two variables, x and y and z, see.", "tokens": [436, 727, 312, 295, 732, 9102, 11, 2031, 293, 288, 293, 710, 11, 536, 13], "temperature": 0.0, "avg_logprob": -0.14040134974888394, "compression_ratio": 1.4770114942528736, "no_speech_prob": 2.8573110739671392e-06}, {"id": 657, "seek": 340096, "start": 3417.52, "end": 3423.84, "text": " And so we could have functions which for example could be like a 3D parabola with this kind", "tokens": [400, 370, 321, 727, 362, 6828, 597, 337, 1365, 727, 312, 411, 257, 805, 35, 45729, 4711, 365, 341, 733], "temperature": 0.0, "avg_logprob": -0.14040134974888394, "compression_ratio": 1.4770114942528736, "no_speech_prob": 2.8573110739671392e-06}, {"id": 658, "seek": 340096, "start": 3423.84, "end": 3428.44, "text": " of curvature for instance.", "tokens": [295, 37638, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.14040134974888394, "compression_ratio": 1.4770114942528736, "no_speech_prob": 2.8573110739671392e-06}, {"id": 659, "seek": 342844, "start": 3428.44, "end": 3436.6, "text": " And so you can still find the derivative, but if you think about it there's one way", "tokens": [400, 370, 291, 393, 920, 915, 264, 13760, 11, 457, 498, 291, 519, 466, 309, 456, 311, 472, 636], "temperature": 0.0, "avg_logprob": -0.11941366929274339, "compression_ratio": 1.798283261802575, "no_speech_prob": 5.014713224227307e-06}, {"id": 660, "seek": 342844, "start": 3436.6, "end": 3440.7200000000003, "text": " to kind of get a derivative would be exactly what we did before, which would we say as", "tokens": [281, 733, 295, 483, 257, 13760, 576, 312, 2293, 437, 321, 630, 949, 11, 597, 576, 321, 584, 382], "temperature": 0.0, "avg_logprob": -0.11941366929274339, "compression_ratio": 1.798283261802575, "no_speech_prob": 5.014713224227307e-06}, {"id": 661, "seek": 342844, "start": 3440.7200000000003, "end": 3448.08, "text": " we change x how does it change z, so that would be this slope again.", "tokens": [321, 1319, 2031, 577, 775, 309, 1319, 710, 11, 370, 300, 576, 312, 341, 13525, 797, 13], "temperature": 0.0, "avg_logprob": -0.11941366929274339, "compression_ratio": 1.798283261802575, "no_speech_prob": 5.014713224227307e-06}, {"id": 662, "seek": 342844, "start": 3448.08, "end": 3452.88, "text": " But you could also have something that says as we change y how would we change z, that", "tokens": [583, 291, 727, 611, 362, 746, 300, 1619, 382, 321, 1319, 288, 577, 576, 321, 1319, 710, 11, 300], "temperature": 0.0, "avg_logprob": -0.11941366929274339, "compression_ratio": 1.798283261802575, "no_speech_prob": 5.014713224227307e-06}, {"id": 663, "seek": 342844, "start": 3452.88, "end": 3456.88, "text": " would be like rotating this whole thing around by 90 degrees and then kind of doing the same", "tokens": [576, 312, 411, 19627, 341, 1379, 551, 926, 538, 4289, 5310, 293, 550, 733, 295, 884, 264, 912], "temperature": 0.0, "avg_logprob": -0.11941366929274339, "compression_ratio": 1.798283261802575, "no_speech_prob": 5.014713224227307e-06}, {"id": 664, "seek": 345688, "start": 3456.88, "end": 3458.52, "text": " thing.", "tokens": [551, 13], "temperature": 0.0, "avg_logprob": -0.05956948057134101, "compression_ratio": 1.3153846153846154, "no_speech_prob": 5.338132723409217e-06}, {"id": 665, "seek": 345688, "start": 3458.52, "end": 3478.1600000000003, "text": " So it's a little bit trickier now because we've got a function of x and y.", "tokens": [407, 309, 311, 257, 707, 857, 4282, 811, 586, 570, 321, 600, 658, 257, 2445, 295, 2031, 293, 288, 13], "temperature": 0.0, "avg_logprob": -0.05956948057134101, "compression_ratio": 1.3153846153846154, "no_speech_prob": 5.338132723409217e-06}, {"id": 666, "seek": 345688, "start": 3478.1600000000003, "end": 3486.36, "text": " And so we could calculate the derivative of that with respect to just one of these things", "tokens": [400, 370, 321, 727, 8873, 264, 13760, 295, 300, 365, 3104, 281, 445, 472, 295, 613, 721], "temperature": 0.0, "avg_logprob": -0.05956948057134101, "compression_ratio": 1.3153846153846154, "no_speech_prob": 5.338132723409217e-06}, {"id": 667, "seek": 348636, "start": 3486.36, "end": 3489.08, "text": " or both of these things or whatever.", "tokens": [420, 1293, 295, 613, 721, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.11744415058809168, "compression_ratio": 1.5, "no_speech_prob": 1.1842980711662676e-05}, {"id": 668, "seek": 348636, "start": 3489.08, "end": 3500.1, "text": " And so what we do here is we write this little thing.", "tokens": [400, 370, 437, 321, 360, 510, 307, 321, 2464, 341, 707, 551, 13], "temperature": 0.0, "avg_logprob": -0.11744415058809168, "compression_ratio": 1.5, "no_speech_prob": 1.1842980711662676e-05}, {"id": 669, "seek": 348636, "start": 3500.1, "end": 3512.56, "text": " And so we can then say this is how our output z changes as we change one thing at a time,", "tokens": [400, 370, 321, 393, 550, 584, 341, 307, 577, 527, 5598, 710, 2962, 382, 321, 1319, 472, 551, 412, 257, 565, 11], "temperature": 0.0, "avg_logprob": -0.11744415058809168, "compression_ratio": 1.5, "no_speech_prob": 1.1842980711662676e-05}, {"id": 670, "seek": 351256, "start": 3512.56, "end": 3516.48, "text": " in this case just x.", "tokens": [294, 341, 1389, 445, 2031, 13], "temperature": 0.0, "avg_logprob": -0.16134877040468412, "compression_ratio": 1.4496644295302012, "no_speech_prob": 2.368795549045899e-06}, {"id": 671, "seek": 351256, "start": 3516.48, "end": 3525.72, "text": " There's another value which would be how does it change as we change just y one thing at", "tokens": [821, 311, 1071, 2158, 597, 576, 312, 577, 775, 309, 1319, 382, 321, 1319, 445, 288, 472, 551, 412], "temperature": 0.0, "avg_logprob": -0.16134877040468412, "compression_ratio": 1.4496644295302012, "no_speech_prob": 2.368795549045899e-06}, {"id": 672, "seek": 351256, "start": 3525.72, "end": 3528.36, "text": " a time.", "tokens": [257, 565, 13], "temperature": 0.0, "avg_logprob": -0.16134877040468412, "compression_ratio": 1.4496644295302012, "no_speech_prob": 2.368795549045899e-06}, {"id": 673, "seek": 351256, "start": 3528.36, "end": 3533.64, "text": " And so those are two separate numbers we could calculate at some particular point on this", "tokens": [400, 370, 729, 366, 732, 4994, 3547, 321, 727, 8873, 412, 512, 1729, 935, 322, 341], "temperature": 0.0, "avg_logprob": -0.16134877040468412, "compression_ratio": 1.4496644295302012, "no_speech_prob": 2.368795549045899e-06}, {"id": 674, "seek": 351256, "start": 3533.64, "end": 3534.92, "text": " surface.", "tokens": [3753, 13], "temperature": 0.0, "avg_logprob": -0.16134877040468412, "compression_ratio": 1.4496644295302012, "no_speech_prob": 2.368795549045899e-06}, {"id": 675, "seek": 353492, "start": 3534.92, "end": 3543.96, "text": " And so these things are called partial derivatives, or just partials.", "tokens": [400, 370, 613, 721, 366, 1219, 14641, 33733, 11, 420, 445, 644, 12356, 13], "temperature": 0.0, "avg_logprob": -0.14014597122485822, "compression_ratio": 1.3430656934306568, "no_speech_prob": 9.570819656801177e-07}, {"id": 676, "seek": 353492, "start": 3543.96, "end": 3556.32, "text": " So in our case we've got a 28 by 28 pixel image, which might be for example the number", "tokens": [407, 294, 527, 1389, 321, 600, 658, 257, 7562, 538, 7562, 19261, 3256, 11, 597, 1062, 312, 337, 1365, 264, 1230], "temperature": 0.0, "avg_logprob": -0.14014597122485822, "compression_ratio": 1.3430656934306568, "no_speech_prob": 9.570819656801177e-07}, {"id": 677, "seek": 353492, "start": 3556.32, "end": 3564.76, "text": " 7, made of 28 by 28 pixels.", "tokens": [1614, 11, 1027, 295, 7562, 538, 7562, 18668, 13], "temperature": 0.0, "avg_logprob": -0.14014597122485822, "compression_ratio": 1.3430656934306568, "no_speech_prob": 9.570819656801177e-07}, {"id": 678, "seek": 356476, "start": 3564.76, "end": 3577.5600000000004, "text": " So the pixels would be something like this.", "tokens": [407, 264, 18668, 576, 312, 746, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.169043231010437, "compression_ratio": 0.9402985074626866, "no_speech_prob": 6.5404024098825175e-06}, {"id": 679, "seek": 356476, "start": 3577.5600000000004, "end": 3586.2000000000003, "text": " And then down here.", "tokens": [400, 550, 760, 510, 13], "temperature": 0.0, "avg_logprob": -0.169043231010437, "compression_ratio": 0.9402985074626866, "no_speech_prob": 6.5404024098825175e-06}, {"id": 680, "seek": 358620, "start": 3586.2, "end": 3598.7999999999997, "text": " And so in our case we've got a situation where we're saying we've got some loss.", "tokens": [400, 370, 294, 527, 1389, 321, 600, 658, 257, 2590, 689, 321, 434, 1566, 321, 600, 658, 512, 4470, 13], "temperature": 0.0, "avg_logprob": -0.11887747446695963, "compression_ratio": 1.5163398692810457, "no_speech_prob": 3.668841827675351e-06}, {"id": 681, "seek": 358620, "start": 3598.7999999999997, "end": 3607.08, "text": " And our loss is calculated at some function of both some weights in a neural network as", "tokens": [400, 527, 4470, 307, 15598, 412, 512, 2445, 295, 1293, 512, 17443, 294, 257, 18161, 3209, 382], "temperature": 0.0, "avg_logprob": -0.11887747446695963, "compression_ratio": 1.5163398692810457, "no_speech_prob": 3.668841827675351e-06}, {"id": 682, "seek": 358620, "start": 3607.08, "end": 3615.12, "text": " well as some pixel values, such as the pixels in this number 7.", "tokens": [731, 382, 512, 19261, 4190, 11, 1270, 382, 264, 18668, 294, 341, 1230, 1614, 13], "temperature": 0.0, "avg_logprob": -0.11887747446695963, "compression_ratio": 1.5163398692810457, "no_speech_prob": 3.668841827675351e-06}, {"id": 683, "seek": 361512, "start": 3615.12, "end": 3619.52, "text": " And I guess you know actually the way it would work with these would be shaded.", "tokens": [400, 286, 2041, 291, 458, 767, 264, 636, 309, 576, 589, 365, 613, 576, 312, 48067, 13], "temperature": 0.0, "avg_logprob": -0.3654161735817238, "compression_ratio": 1.2727272727272727, "no_speech_prob": 8.013454134925269e-06}, {"id": 684, "seek": 361512, "start": 3619.52, "end": 3625.7999999999997, "text": " This is more like what MNIST looks like, right?", "tokens": [639, 307, 544, 411, 437, 376, 45, 19756, 1542, 411, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.3654161735817238, "compression_ratio": 1.2727272727272727, "no_speech_prob": 8.013454134925269e-06}, {"id": 685, "seek": 361512, "start": 3625.7999999999997, "end": 3628.72, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.3654161735817238, "compression_ratio": 1.2727272727272727, "no_speech_prob": 8.013454134925269e-06}, {"id": 686, "seek": 361512, "start": 3628.72, "end": 3635.12, "text": " So there's my pixels, aren't they terrible.", "tokens": [407, 456, 311, 452, 18668, 11, 3212, 380, 436, 6237, 13], "temperature": 0.0, "avg_logprob": -0.3654161735817238, "compression_ratio": 1.2727272727272727, "no_speech_prob": 8.013454134925269e-06}, {"id": 687, "seek": 363512, "start": 3635.12, "end": 3649.3199999999997, "text": " So the loss would be calculated more specifically as the MSE mean squared error of the actual", "tokens": [407, 264, 4470, 576, 312, 15598, 544, 4682, 382, 264, 376, 5879, 914, 8889, 6713, 295, 264, 3539], "temperature": 0.0, "avg_logprob": -0.2200551466508345, "compression_ratio": 1.1341463414634145, "no_speech_prob": 2.368780087635969e-06}, {"id": 688, "seek": 364932, "start": 3649.32, "end": 3671.0, "text": " answer of like which digit should it be, so let's call that y, minus the predicted y,", "tokens": [1867, 295, 411, 597, 14293, 820, 309, 312, 11, 370, 718, 311, 818, 300, 288, 11, 3175, 264, 19147, 288, 11], "temperature": 0.0, "avg_logprob": -0.23964246114095053, "compression_ratio": 1.3097345132743363, "no_speech_prob": 3.3931225971173262e-06}, {"id": 689, "seek": 364932, "start": 3671.0, "end": 3679.2400000000002, "text": " which is some neural network with some weights and our pixels.", "tokens": [597, 307, 512, 18161, 3209, 365, 512, 17443, 293, 527, 18668, 13], "temperature": 0.0, "avg_logprob": -0.23964246114095053, "compression_ratio": 1.3097345132743363, "no_speech_prob": 3.3931225971173262e-06}, {"id": 690, "seek": 367924, "start": 3679.24, "end": 3681.9599999999996, "text": " So that would be like delving in one layer deeply.", "tokens": [407, 300, 576, 312, 411, 1103, 798, 294, 472, 4583, 8760, 13], "temperature": 0.0, "avg_logprob": -0.14020717845243566, "compression_ratio": 1.756198347107438, "no_speech_prob": 1.2029494428134058e-05}, {"id": 691, "seek": 367924, "start": 3681.9599999999996, "end": 3687.2799999999997, "text": " But none of these details really matter too much of like what's the loss function or the", "tokens": [583, 6022, 295, 613, 4365, 534, 1871, 886, 709, 295, 411, 437, 311, 264, 4470, 2445, 420, 264], "temperature": 0.0, "avg_logprob": -0.14020717845243566, "compression_ratio": 1.756198347107438, "no_speech_prob": 1.2029494428134058e-05}, {"id": 692, "seek": 367924, "start": 3687.2799999999997, "end": 3688.4799999999996, "text": " neural network or whatever.", "tokens": [18161, 3209, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.14020717845243566, "compression_ratio": 1.756198347107438, "no_speech_prob": 1.2029494428134058e-05}, {"id": 693, "seek": 367924, "start": 3688.4799999999996, "end": 3690.56, "text": " It's just some function that's calculating loss.", "tokens": [467, 311, 445, 512, 2445, 300, 311, 28258, 4470, 13], "temperature": 0.0, "avg_logprob": -0.14020717845243566, "compression_ratio": 1.756198347107438, "no_speech_prob": 1.2029494428134058e-05}, {"id": 694, "seek": 367924, "start": 3690.56, "end": 3693.8399999999997, "text": " So let's get rid of all that.", "tokens": [407, 718, 311, 483, 3973, 295, 439, 300, 13], "temperature": 0.0, "avg_logprob": -0.14020717845243566, "compression_ratio": 1.756198347107438, "no_speech_prob": 1.2029494428134058e-05}, {"id": 695, "seek": 367924, "start": 3693.8399999999997, "end": 3701.24, "text": " And we can now say what happens as we change x.", "tokens": [400, 321, 393, 586, 584, 437, 2314, 382, 321, 1319, 2031, 13], "temperature": 0.0, "avg_logprob": -0.14020717845243566, "compression_ratio": 1.756198347107438, "no_speech_prob": 1.2029494428134058e-05}, {"id": 696, "seek": 367924, "start": 3701.24, "end": 3703.64, "text": " As we change x what happens to the loss?", "tokens": [1018, 321, 1319, 2031, 437, 2314, 281, 264, 4470, 30], "temperature": 0.0, "avg_logprob": -0.14020717845243566, "compression_ratio": 1.756198347107438, "no_speech_prob": 1.2029494428134058e-05}, {"id": 697, "seek": 367924, "start": 3703.64, "end": 3706.56, "text": " Because we want to change x in a way that the loss goes down.", "tokens": [1436, 321, 528, 281, 1319, 2031, 294, 257, 636, 300, 264, 4470, 1709, 760, 13], "temperature": 0.0, "avg_logprob": -0.14020717845243566, "compression_ratio": 1.756198347107438, "no_speech_prob": 1.2029494428134058e-05}, {"id": 698, "seek": 367924, "start": 3706.56, "end": 3708.64, "text": " But there isn't just one x.", "tokens": [583, 456, 1943, 380, 445, 472, 2031, 13], "temperature": 0.0, "avg_logprob": -0.14020717845243566, "compression_ratio": 1.756198347107438, "no_speech_prob": 1.2029494428134058e-05}, {"id": 699, "seek": 370864, "start": 3708.64, "end": 3712.4, "text": " There's, oh I wrote this as seven by seven.", "tokens": [821, 311, 11, 1954, 286, 4114, 341, 382, 3407, 538, 3407, 13], "temperature": 0.0, "avg_logprob": -0.2572650909423828, "compression_ratio": 1.6756756756756757, "no_speech_prob": 2.2473557692137547e-05}, {"id": 700, "seek": 370864, "start": 3712.4, "end": 3714.2, "text": " It's actually meant to be 28 by 28.", "tokens": [467, 311, 767, 4140, 281, 312, 7562, 538, 7562, 13], "temperature": 0.0, "avg_logprob": -0.2572650909423828, "compression_ratio": 1.6756756756756757, "no_speech_prob": 2.2473557692137547e-05}, {"id": 701, "seek": 370864, "start": 3714.2, "end": 3717.0, "text": " But let's just do a simple seven by seven version here.", "tokens": [583, 718, 311, 445, 360, 257, 2199, 3407, 538, 3407, 3037, 510, 13], "temperature": 0.0, "avg_logprob": -0.2572650909423828, "compression_ratio": 1.6756756756756757, "no_speech_prob": 2.2473557692137547e-05}, {"id": 702, "seek": 370864, "start": 3717.0, "end": 3719.7599999999998, "text": " So we've got seven pixels by seven pixels.", "tokens": [407, 321, 600, 658, 3407, 18668, 538, 3407, 18668, 13], "temperature": 0.0, "avg_logprob": -0.2572650909423828, "compression_ratio": 1.6756756756756757, "no_speech_prob": 2.2473557692137547e-05}, {"id": 703, "seek": 370864, "start": 3719.7599999999998, "end": 3725.92, "text": " This is a super low resolution 49 pixel image.", "tokens": [639, 307, 257, 1687, 2295, 8669, 16513, 19261, 3256, 13], "temperature": 0.0, "avg_logprob": -0.2572650909423828, "compression_ratio": 1.6756756756756757, "no_speech_prob": 2.2473557692137547e-05}, {"id": 704, "seek": 370864, "start": 3725.92, "end": 3727.96, "text": " So there's 49 different things we could change.", "tokens": [407, 456, 311, 16513, 819, 721, 321, 727, 1319, 13], "temperature": 0.0, "avg_logprob": -0.2572650909423828, "compression_ratio": 1.6756756756756757, "no_speech_prob": 2.2473557692137547e-05}, {"id": 705, "seek": 370864, "start": 3727.96, "end": 3732.3199999999997, "text": " We could make each of these pixels darker or lighter.", "tokens": [492, 727, 652, 1184, 295, 613, 18668, 12741, 420, 11546, 13], "temperature": 0.0, "avg_logprob": -0.2572650909423828, "compression_ratio": 1.6756756756756757, "no_speech_prob": 2.2473557692137547e-05}, {"id": 706, "seek": 370864, "start": 3732.3199999999997, "end": 3735.72, "text": " All right so let's take for example pixel, maybe we can write it like this.", "tokens": [1057, 558, 370, 718, 311, 747, 337, 1365, 19261, 11, 1310, 321, 393, 2464, 309, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.2572650909423828, "compression_ratio": 1.6756756756756757, "no_speech_prob": 2.2473557692137547e-05}, {"id": 707, "seek": 370864, "start": 3735.72, "end": 3738.3599999999997, "text": " We'll say pixel one comma one.", "tokens": [492, 603, 584, 19261, 472, 22117, 472, 13], "temperature": 0.0, "avg_logprob": -0.2572650909423828, "compression_ratio": 1.6756756756756757, "no_speech_prob": 2.2473557692137547e-05}, {"id": 708, "seek": 373836, "start": 3738.36, "end": 3740.56, "text": " We could write it like that for example.", "tokens": [492, 727, 2464, 309, 411, 300, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.15779984792073568, "compression_ratio": 1.6770186335403727, "no_speech_prob": 5.771885753347306e-06}, {"id": 709, "seek": 373836, "start": 3740.56, "end": 3747.36, "text": " And so we could say what happens to the loss as we change pixel one comma one.", "tokens": [400, 370, 321, 727, 584, 437, 2314, 281, 264, 4470, 382, 321, 1319, 19261, 472, 22117, 472, 13], "temperature": 0.0, "avg_logprob": -0.15779984792073568, "compression_ratio": 1.6770186335403727, "no_speech_prob": 5.771885753347306e-06}, {"id": 710, "seek": 373836, "start": 3747.36, "end": 3750.96, "text": " So we can calculate a derivative right.", "tokens": [407, 321, 393, 8873, 257, 13760, 558, 13], "temperature": 0.0, "avg_logprob": -0.15779984792073568, "compression_ratio": 1.6770186335403727, "no_speech_prob": 5.771885753347306e-06}, {"id": 711, "seek": 373836, "start": 3750.96, "end": 3753.36, "text": " And it's going to be a partial.", "tokens": [400, 309, 311, 516, 281, 312, 257, 14641, 13], "temperature": 0.0, "avg_logprob": -0.15779984792073568, "compression_ratio": 1.6770186335403727, "no_speech_prob": 5.771885753347306e-06}, {"id": 712, "seek": 373836, "start": 3753.36, "end": 3755.88, "text": " Partial derivative.", "tokens": [4100, 831, 13760, 13], "temperature": 0.0, "avg_logprob": -0.15779984792073568, "compression_ratio": 1.6770186335403727, "no_speech_prob": 5.771885753347306e-06}, {"id": 713, "seek": 373836, "start": 3755.88, "end": 3764.84, "text": " How does the loss change as we change pixel one comma one?", "tokens": [1012, 775, 264, 4470, 1319, 382, 321, 1319, 19261, 472, 22117, 472, 30], "temperature": 0.0, "avg_logprob": -0.15779984792073568, "compression_ratio": 1.6770186335403727, "no_speech_prob": 5.771885753347306e-06}, {"id": 714, "seek": 376484, "start": 3764.84, "end": 3768.56, "text": " So that would be a very useful thing to know because that would then tell us do we need", "tokens": [407, 300, 576, 312, 257, 588, 4420, 551, 281, 458, 570, 300, 576, 550, 980, 505, 360, 321, 643], "temperature": 0.0, "avg_logprob": -0.11915557280830714, "compression_ratio": 1.6742081447963801, "no_speech_prob": 1.8448154150974005e-06}, {"id": 715, "seek": 376484, "start": 3768.56, "end": 3774.6800000000003, "text": " to make pixel one comma one a bit brighter or a bit darker in order to improve the loss.", "tokens": [281, 652, 19261, 472, 22117, 472, 257, 857, 19764, 420, 257, 857, 12741, 294, 1668, 281, 3470, 264, 4470, 13], "temperature": 0.0, "avg_logprob": -0.11915557280830714, "compression_ratio": 1.6742081447963801, "no_speech_prob": 1.8448154150974005e-06}, {"id": 716, "seek": 376484, "start": 3774.6800000000003, "end": 3778.4, "text": " And we could also calculate that for pixel one comma two, one comma three and so forth", "tokens": [400, 321, 727, 611, 8873, 300, 337, 19261, 472, 22117, 732, 11, 472, 22117, 1045, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.11915557280830714, "compression_ratio": 1.6742081447963801, "no_speech_prob": 1.8448154150974005e-06}, {"id": 717, "seek": 376484, "start": 3778.4, "end": 3783.6000000000004, "text": " for all 49 pixels in this super low resolution digit.", "tokens": [337, 439, 16513, 18668, 294, 341, 1687, 2295, 8669, 14293, 13], "temperature": 0.0, "avg_logprob": -0.11915557280830714, "compression_ratio": 1.6742081447963801, "no_speech_prob": 1.8448154150974005e-06}, {"id": 718, "seek": 376484, "start": 3783.6000000000004, "end": 3786.1600000000003, "text": " So okay so there's the first thing we can calculate.", "tokens": [407, 1392, 370, 456, 311, 264, 700, 551, 321, 393, 8873, 13], "temperature": 0.0, "avg_logprob": -0.11915557280830714, "compression_ratio": 1.6742081447963801, "no_speech_prob": 1.8448154150974005e-06}, {"id": 719, "seek": 378616, "start": 3786.16, "end": 3795.6, "text": " And then the second thing I mentioned could be loss over pixel one comma two.", "tokens": [400, 550, 264, 1150, 551, 286, 2835, 727, 312, 4470, 670, 19261, 472, 22117, 732, 13], "temperature": 0.0, "avg_logprob": -0.20601425170898438, "compression_ratio": 1.5034013605442176, "no_speech_prob": 1.7061818198271794e-06}, {"id": 720, "seek": 378616, "start": 3795.6, "end": 3804.3599999999997, "text": " Okay so that's the slope as we change pixel one comma two.", "tokens": [1033, 370, 300, 311, 264, 13525, 382, 321, 1319, 19261, 472, 22117, 732, 13], "temperature": 0.0, "avg_logprob": -0.20601425170898438, "compression_ratio": 1.5034013605442176, "no_speech_prob": 1.7061818198271794e-06}, {"id": 721, "seek": 378616, "start": 3804.3599999999997, "end": 3811.16, "text": " And I'm not going to write all of them but there could be, well there will be, 49 of", "tokens": [400, 286, 478, 406, 516, 281, 2464, 439, 295, 552, 457, 456, 727, 312, 11, 731, 456, 486, 312, 11, 16513, 295], "temperature": 0.0, "avg_logprob": -0.20601425170898438, "compression_ratio": 1.5034013605442176, "no_speech_prob": 1.7061818198271794e-06}, {"id": 722, "seek": 381116, "start": 3811.16, "end": 3818.64, "text": " these for each seven by seven.", "tokens": [613, 337, 1184, 3407, 538, 3407, 13], "temperature": 0.0, "avg_logprob": -0.11783869699998335, "compression_ratio": 1.3275862068965518, "no_speech_prob": 5.25536870554788e-06}, {"id": 723, "seek": 381116, "start": 3818.64, "end": 3830.12, "text": " So rather than writing out all 49 of those it's nice to instead write them all at once", "tokens": [407, 2831, 813, 3579, 484, 439, 16513, 295, 729, 309, 311, 1481, 281, 2602, 2464, 552, 439, 412, 1564], "temperature": 0.0, "avg_logprob": -0.11783869699998335, "compression_ratio": 1.3275862068965518, "no_speech_prob": 5.25536870554788e-06}, {"id": 724, "seek": 381116, "start": 3830.12, "end": 3832.56, "text": " and say and you can do that like so.", "tokens": [293, 584, 293, 291, 393, 360, 300, 411, 370, 13], "temperature": 0.0, "avg_logprob": -0.11783869699998335, "compression_ratio": 1.3275862068965518, "no_speech_prob": 5.25536870554788e-06}, {"id": 725, "seek": 383256, "start": 3832.56, "end": 3847.32, "text": " You can say upside down triangle x loss and what that means is it's a vector of all of", "tokens": [509, 393, 584, 14119, 760, 13369, 2031, 4470, 293, 437, 300, 1355, 307, 309, 311, 257, 8062, 295, 439, 295], "temperature": 0.0, "avg_logprob": -0.1268146110303474, "compression_ratio": 1.5808383233532934, "no_speech_prob": 1.1015935115210596e-06}, {"id": 726, "seek": 383256, "start": 3847.32, "end": 3850.2, "text": " these derivatives.", "tokens": [613, 33733, 13], "temperature": 0.0, "avg_logprob": -0.1268146110303474, "compression_ratio": 1.5808383233532934, "no_speech_prob": 1.1015935115210596e-06}, {"id": 727, "seek": 383256, "start": 3850.2, "end": 3855.92, "text": " And this upside down triangle here is called either the del or the nabla and it's just", "tokens": [400, 341, 14119, 760, 13369, 510, 307, 1219, 2139, 264, 1103, 420, 264, 297, 455, 875, 293, 309, 311, 445], "temperature": 0.0, "avg_logprob": -0.1268146110303474, "compression_ratio": 1.5808383233532934, "no_speech_prob": 1.1015935115210596e-06}, {"id": 728, "seek": 383256, "start": 3855.92, "end": 3861.18, "text": " a little convenient notational shortcut to avoid writing all these out.", "tokens": [257, 707, 10851, 406, 1478, 24822, 281, 5042, 3579, 439, 613, 484, 13], "temperature": 0.0, "avg_logprob": -0.1268146110303474, "compression_ratio": 1.5808383233532934, "no_speech_prob": 1.1015935115210596e-06}, {"id": 729, "seek": 386118, "start": 3861.18, "end": 3867.2799999999997, "text": " And the x here is telling you about the thing that we're basically putting on the bottom.", "tokens": [400, 264, 2031, 510, 307, 3585, 291, 466, 264, 551, 300, 321, 434, 1936, 3372, 322, 264, 2767, 13], "temperature": 0.0, "avg_logprob": -0.11025098475014292, "compression_ratio": 1.6515151515151516, "no_speech_prob": 4.565929884847719e-06}, {"id": 730, "seek": 386118, "start": 3867.2799999999997, "end": 3868.2799999999997, "text": " What's it with respect to?", "tokens": [708, 311, 309, 365, 3104, 281, 30], "temperature": 0.0, "avg_logprob": -0.11025098475014292, "compression_ratio": 1.6515151515151516, "no_speech_prob": 4.565929884847719e-06}, {"id": 731, "seek": 386118, "start": 3868.2799999999997, "end": 3873.9199999999996, "text": " What's the direction that we're trying to go?", "tokens": [708, 311, 264, 3513, 300, 321, 434, 1382, 281, 352, 30], "temperature": 0.0, "avg_logprob": -0.11025098475014292, "compression_ratio": 1.6515151515151516, "no_speech_prob": 4.565929884847719e-06}, {"id": 732, "seek": 386118, "start": 3873.9199999999996, "end": 3879.8399999999997, "text": " So that's actually what I should have written in the notes that I was doing in the lesson.", "tokens": [407, 300, 311, 767, 437, 286, 820, 362, 3720, 294, 264, 5570, 300, 286, 390, 884, 294, 264, 6898, 13], "temperature": 0.0, "avg_logprob": -0.11025098475014292, "compression_ratio": 1.6515151515151516, "no_speech_prob": 4.565929884847719e-06}, {"id": 733, "seek": 386118, "start": 3879.8399999999997, "end": 3890.7, "text": " I wrote something else which is basically the equivalent of writing this.", "tokens": [286, 4114, 746, 1646, 597, 307, 1936, 264, 10344, 295, 3579, 341, 13], "temperature": 0.0, "avg_logprob": -0.11025098475014292, "compression_ratio": 1.6515151515151516, "no_speech_prob": 4.565929884847719e-06}, {"id": 734, "seek": 389070, "start": 3890.7, "end": 3892.7999999999997, "text": " And that's not a thing at all.", "tokens": [400, 300, 311, 406, 257, 551, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.13658253046182486, "compression_ratio": 1.8177777777777777, "no_speech_prob": 6.438967375288485e-06}, {"id": 735, "seek": 389070, "start": 3892.7999999999997, "end": 3898.52, "text": " So in my notes when you see me write this I actually mean this.", "tokens": [407, 294, 452, 5570, 562, 291, 536, 385, 2464, 341, 286, 767, 914, 341, 13], "temperature": 0.0, "avg_logprob": -0.13658253046182486, "compression_ratio": 1.8177777777777777, "no_speech_prob": 6.438967375288485e-06}, {"id": 736, "seek": 389070, "start": 3898.52, "end": 3902.8399999999997, "text": " So why does my brain get confused when I write this weird thing that doesn't even exist?", "tokens": [407, 983, 775, 452, 3567, 483, 9019, 562, 286, 2464, 341, 3657, 551, 300, 1177, 380, 754, 2514, 30], "temperature": 0.0, "avg_logprob": -0.13658253046182486, "compression_ratio": 1.8177777777777777, "no_speech_prob": 6.438967375288485e-06}, {"id": 737, "seek": 389070, "start": 3902.8399999999997, "end": 3903.8399999999997, "text": " As far as I know.", "tokens": [1018, 1400, 382, 286, 458, 13], "temperature": 0.0, "avg_logprob": -0.13658253046182486, "compression_ratio": 1.8177777777777777, "no_speech_prob": 6.438967375288485e-06}, {"id": 738, "seek": 389070, "start": 3903.8399999999997, "end": 3909.04, "text": " Well the reason is that this thing does exist if you turn the triangles upside down and", "tokens": [1042, 264, 1778, 307, 300, 341, 551, 775, 2514, 498, 291, 1261, 264, 29896, 14119, 760, 293], "temperature": 0.0, "avg_logprob": -0.13658253046182486, "compression_ratio": 1.8177777777777777, "no_speech_prob": 6.438967375288485e-06}, {"id": 739, "seek": 389070, "start": 3909.04, "end": 3911.9199999999996, "text": " hence my brain always gets confused.", "tokens": [16678, 452, 3567, 1009, 2170, 9019, 13], "temperature": 0.0, "avg_logprob": -0.13658253046182486, "compression_ratio": 1.8177777777777777, "no_speech_prob": 6.438967375288485e-06}, {"id": 740, "seek": 389070, "start": 3911.9199999999996, "end": 3918.72, "text": " And if I turn the triangles upside down these triangles are now totally different.", "tokens": [400, 498, 286, 1261, 264, 29896, 14119, 760, 613, 29896, 366, 586, 3879, 819, 13], "temperature": 0.0, "avg_logprob": -0.13658253046182486, "compression_ratio": 1.8177777777777777, "no_speech_prob": 6.438967375288485e-06}, {"id": 741, "seek": 391872, "start": 3918.72, "end": 3922.9599999999996, "text": " These triangles now mean a small change.", "tokens": [1981, 29896, 586, 914, 257, 1359, 1319, 13], "temperature": 0.0, "avg_logprob": -0.13968089608585132, "compression_ratio": 1.702127659574468, "no_speech_prob": 3.6688204545498593e-06}, {"id": 742, "seek": 391872, "start": 3922.9599999999996, "end": 3930.4399999999996, "text": " So this is a small change in loss divided by a small change in, let's for example one", "tokens": [407, 341, 307, 257, 1359, 1319, 294, 4470, 6666, 538, 257, 1359, 1319, 294, 11, 718, 311, 337, 1365, 472], "temperature": 0.0, "avg_logprob": -0.13968089608585132, "compression_ratio": 1.702127659574468, "no_speech_prob": 3.6688204545498593e-06}, {"id": 743, "seek": 391872, "start": 3930.4399999999996, "end": 3931.4399999999996, "text": " particular pixel.", "tokens": [1729, 19261, 13], "temperature": 0.0, "avg_logprob": -0.13968089608585132, "compression_ratio": 1.702127659574468, "no_speech_prob": 3.6688204545498593e-06}, {"id": 744, "seek": 391872, "start": 3931.4399999999996, "end": 3935.3599999999997, "text": " And that's a totally valid thing to say.", "tokens": [400, 300, 311, 257, 3879, 7363, 551, 281, 584, 13], "temperature": 0.0, "avg_logprob": -0.13968089608585132, "compression_ratio": 1.702127659574468, "no_speech_prob": 3.6688204545498593e-06}, {"id": 745, "seek": 391872, "start": 3935.3599999999997, "end": 3941.72, "text": " And in fact if you make that small change small enough then you're going to end up with", "tokens": [400, 294, 1186, 498, 291, 652, 300, 1359, 1319, 1359, 1547, 550, 291, 434, 516, 281, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.13968089608585132, "compression_ratio": 1.702127659574468, "no_speech_prob": 3.6688204545498593e-06}, {"id": 746, "seek": 391872, "start": 3941.72, "end": 3942.7999999999997, "text": " the derivative.", "tokens": [264, 13760, 13], "temperature": 0.0, "avg_logprob": -0.13968089608585132, "compression_ratio": 1.702127659574468, "no_speech_prob": 3.6688204545498593e-06}, {"id": 747, "seek": 394280, "start": 3942.8, "end": 3951.52, "text": " That's what the derivative is.", "tokens": [663, 311, 437, 264, 13760, 307, 13], "temperature": 0.0, "avg_logprob": -0.18999581770463422, "compression_ratio": 1.4728682170542635, "no_speech_prob": 2.521555416024057e-06}, {"id": 748, "seek": 394280, "start": 3951.52, "end": 3958.6000000000004, "text": " So the derivative is just our classic rise over run slope that we did in what is that", "tokens": [407, 264, 13760, 307, 445, 527, 7230, 6272, 670, 1190, 13525, 300, 321, 630, 294, 437, 307, 300], "temperature": 0.0, "avg_logprob": -0.18999581770463422, "compression_ratio": 1.4728682170542635, "no_speech_prob": 2.521555416024057e-06}, {"id": 749, "seek": 394280, "start": 3958.6000000000004, "end": 3962.5600000000004, "text": " grade 8, grade 9, rise over run.", "tokens": [7204, 1649, 11, 7204, 1722, 11, 6272, 670, 1190, 13], "temperature": 0.0, "avg_logprob": -0.18999581770463422, "compression_ratio": 1.4728682170542635, "no_speech_prob": 2.521555416024057e-06}, {"id": 750, "seek": 394280, "start": 3962.5600000000004, "end": 3971.4, "text": " Once we make our step in x small enough.", "tokens": [3443, 321, 652, 527, 1823, 294, 2031, 1359, 1547, 13], "temperature": 0.0, "avg_logprob": -0.18999581770463422, "compression_ratio": 1.4728682170542635, "no_speech_prob": 2.521555416024057e-06}, {"id": 751, "seek": 397140, "start": 3971.4, "end": 3980.48, "text": " And so then we can then do that for changing just one variable in a multivariable function.", "tokens": [400, 370, 550, 321, 393, 550, 360, 300, 337, 4473, 445, 472, 7006, 294, 257, 2120, 592, 3504, 712, 2445, 13], "temperature": 0.0, "avg_logprob": -0.13703189123244514, "compression_ratio": 1.7625, "no_speech_prob": 5.255338692222722e-06}, {"id": 752, "seek": 397140, "start": 3980.48, "end": 3984.2000000000003, "text": " So for example when I say variable I guess in this case I should give an example.", "tokens": [407, 337, 1365, 562, 286, 584, 7006, 286, 2041, 294, 341, 1389, 286, 820, 976, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.13703189123244514, "compression_ratio": 1.7625, "no_speech_prob": 5.255338692222722e-06}, {"id": 753, "seek": 397140, "start": 3984.2000000000003, "end": 3991.88, "text": " For example changing one pixel value in an image to see how that impacts our loss.", "tokens": [1171, 1365, 4473, 472, 19261, 2158, 294, 364, 3256, 281, 536, 577, 300, 11606, 527, 4470, 13], "temperature": 0.0, "avg_logprob": -0.13703189123244514, "compression_ratio": 1.7625, "no_speech_prob": 5.255338692222722e-06}, {"id": 754, "seek": 397140, "start": 3991.88, "end": 3993.28, "text": " Or we could do the same thing for the weights.", "tokens": [1610, 321, 727, 360, 264, 912, 551, 337, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.13703189123244514, "compression_ratio": 1.7625, "no_speech_prob": 5.255338692222722e-06}, {"id": 755, "seek": 397140, "start": 3993.28, "end": 3994.84, "text": " We could change one weight.", "tokens": [492, 727, 1319, 472, 3364, 13], "temperature": 0.0, "avg_logprob": -0.13703189123244514, "compression_ratio": 1.7625, "no_speech_prob": 5.255338692222722e-06}, {"id": 756, "seek": 397140, "start": 3994.84, "end": 3999.12, "text": " And if you just change one thing at a time and calculate the derivative of the loss against", "tokens": [400, 498, 291, 445, 1319, 472, 551, 412, 257, 565, 293, 8873, 264, 13760, 295, 264, 4470, 1970], "temperature": 0.0, "avg_logprob": -0.13703189123244514, "compression_ratio": 1.7625, "no_speech_prob": 5.255338692222722e-06}, {"id": 757, "seek": 399912, "start": 3999.12, "end": 4002.92, "text": " that one thing then we get these things called partials.", "tokens": [300, 472, 551, 550, 321, 483, 613, 721, 1219, 644, 12356, 13], "temperature": 0.0, "avg_logprob": -0.12156611519890863, "compression_ratio": 1.6844919786096257, "no_speech_prob": 2.813009132296429e-06}, {"id": 758, "seek": 399912, "start": 4002.92, "end": 4006.04, "text": " And if you then do it for all the different things that you could change such as every", "tokens": [400, 498, 291, 550, 360, 309, 337, 439, 264, 819, 721, 300, 291, 727, 1319, 1270, 382, 633], "temperature": 0.0, "avg_logprob": -0.12156611519890863, "compression_ratio": 1.6844919786096257, "no_speech_prob": 2.813009132296429e-06}, {"id": 759, "seek": 399912, "start": 4006.04, "end": 4016.96, "text": " pixel you get this whole gradient vector which we use the upside down triangle to represent.", "tokens": [19261, 291, 483, 341, 1379, 16235, 8062, 597, 321, 764, 264, 14119, 760, 13369, 281, 2906, 13], "temperature": 0.0, "avg_logprob": -0.12156611519890863, "compression_ratio": 1.6844919786096257, "no_speech_prob": 2.813009132296429e-06}, {"id": 760, "seek": 399912, "start": 4016.96, "end": 4023.3199999999997, "text": " And then finally the non-upside down triangle simply refers to a small change.", "tokens": [400, 550, 2721, 264, 2107, 12, 1010, 1812, 760, 13369, 2935, 14942, 281, 257, 1359, 1319, 13], "temperature": 0.0, "avg_logprob": -0.12156611519890863, "compression_ratio": 1.6844919786096257, "no_speech_prob": 2.813009132296429e-06}, {"id": 761, "seek": 402332, "start": 4023.32, "end": 4030.44, "text": " So this would be a small change in loss which is caused by changing pixel 1,1 by a small", "tokens": [407, 341, 576, 312, 257, 1359, 1319, 294, 4470, 597, 307, 7008, 538, 4473, 19261, 502, 11, 16, 538, 257, 1359], "temperature": 0.0, "avg_logprob": -0.21025922139485678, "compression_ratio": 1.5055555555555555, "no_speech_prob": 6.4389878389192745e-06}, {"id": 762, "seek": 402332, "start": 4030.44, "end": 4031.44, "text": " bit.", "tokens": [857, 13], "temperature": 0.0, "avg_logprob": -0.21025922139485678, "compression_ratio": 1.5055555555555555, "no_speech_prob": 6.4389878389192745e-06}, {"id": 763, "seek": 402332, "start": 4031.44, "end": 4037.4, "text": " And if you use a small enough bit, an infinitely small bit, we call that the derivative.", "tokens": [400, 498, 291, 764, 257, 1359, 1547, 857, 11, 364, 36227, 1359, 857, 11, 321, 818, 300, 264, 13760, 13], "temperature": 0.0, "avg_logprob": -0.21025922139485678, "compression_ratio": 1.5055555555555555, "no_speech_prob": 6.4389878389192745e-06}, {"id": 764, "seek": 402332, "start": 4037.4, "end": 4045.1200000000003, "text": " Okay, so with all that said net result every time you see me, well I think I only did it", "tokens": [1033, 11, 370, 365, 439, 300, 848, 2533, 1874, 633, 565, 291, 536, 385, 11, 731, 286, 519, 286, 787, 630, 309], "temperature": 0.0, "avg_logprob": -0.21025922139485678, "compression_ratio": 1.5055555555555555, "no_speech_prob": 6.4389878389192745e-06}, {"id": 765, "seek": 404512, "start": 4045.12, "end": 4058.8399999999997, "text": " once but in the notes where I say this, please throw that away in your head and replace it", "tokens": [1564, 457, 294, 264, 5570, 689, 286, 584, 341, 11, 1767, 3507, 300, 1314, 294, 428, 1378, 293, 7406, 309], "temperature": 0.0, "avg_logprob": -0.11686970436409728, "compression_ratio": 1.5257142857142858, "no_speech_prob": 5.59430100111058e-06}, {"id": 766, "seek": 404512, "start": 4058.8399999999997, "end": 4061.8399999999997, "text": " in your head with this.", "tokens": [294, 428, 1378, 365, 341, 13], "temperature": 0.0, "avg_logprob": -0.11686970436409728, "compression_ratio": 1.5257142857142858, "no_speech_prob": 5.59430100111058e-06}, {"id": 767, "seek": 404512, "start": 4061.8399999999997, "end": 4063.68, "text": " And that is the moral of the story.", "tokens": [400, 300, 307, 264, 9723, 295, 264, 1657, 13], "temperature": 0.0, "avg_logprob": -0.11686970436409728, "compression_ratio": 1.5257142857142858, "no_speech_prob": 5.59430100111058e-06}, {"id": 768, "seek": 404512, "start": 4063.68, "end": 4070.48, "text": " Okay, so thank you very much for bearing with me as I do my penance to actually get this", "tokens": [1033, 11, 370, 1309, 291, 588, 709, 337, 17350, 365, 385, 382, 286, 360, 452, 3435, 719, 281, 767, 483, 341], "temperature": 0.0, "avg_logprob": -0.11686970436409728, "compression_ratio": 1.5257142857142858, "no_speech_prob": 5.59430100111058e-06}, {"id": 769, "seek": 404512, "start": 4070.48, "end": 4072.16, "text": " notation correct this time.", "tokens": [24657, 3006, 341, 565, 13], "temperature": 0.0, "avg_logprob": -0.11686970436409728, "compression_ratio": 1.5257142857142858, "no_speech_prob": 5.59430100111058e-06}, {"id": 770, "seek": 407216, "start": 4072.16, "end": 4076.92, "text": " And I will endeavor not to make the same mistake again during this course but no promises.", "tokens": [400, 286, 486, 34975, 406, 281, 652, 264, 912, 6146, 797, 1830, 341, 1164, 457, 572, 16403, 13], "temperature": 0.0, "avg_logprob": -0.19287629281320878, "compression_ratio": 1.3726708074534162, "no_speech_prob": 8.013276783458423e-06}, {"id": 771, "seek": 407216, "start": 4076.92, "end": 4079.92, "text": " All right, back to the lesson.", "tokens": [1057, 558, 11, 646, 281, 264, 6898, 13], "temperature": 0.0, "avg_logprob": -0.19287629281320878, "compression_ratio": 1.3726708074534162, "no_speech_prob": 8.013276783458423e-06}, {"id": 772, "seek": 407216, "start": 4079.92, "end": 4090.8799999999997, "text": " Okay, so with those 784 values they tell us how can we change x3 to make it look more", "tokens": [1033, 11, 370, 365, 729, 1614, 25494, 4190, 436, 980, 505, 577, 393, 321, 1319, 2031, 18, 281, 652, 309, 574, 544], "temperature": 0.0, "avg_logprob": -0.19287629281320878, "compression_ratio": 1.3726708074534162, "no_speech_prob": 8.013276783458423e-06}, {"id": 773, "seek": 407216, "start": 4090.8799999999997, "end": 4092.56, "text": " like a digit.", "tokens": [411, 257, 14293, 13], "temperature": 0.0, "avg_logprob": -0.19287629281320878, "compression_ratio": 1.3726708074534162, "no_speech_prob": 8.013276783458423e-06}, {"id": 774, "seek": 409256, "start": 4092.56, "end": 4102.92, "text": " And so what we can then do is we can now change the pixels according to this gradient.", "tokens": [400, 370, 437, 321, 393, 550, 360, 307, 321, 393, 586, 1319, 264, 18668, 4650, 281, 341, 16235, 13], "temperature": 0.0, "avg_logprob": -0.07849010344474547, "compression_ratio": 1.8221153846153846, "no_speech_prob": 4.539606743492186e-05}, {"id": 775, "seek": 409256, "start": 4102.92, "end": 4106.6, "text": " And so we can do something a lot like what we do when we train neural networks except", "tokens": [400, 370, 321, 393, 360, 746, 257, 688, 411, 437, 321, 360, 562, 321, 3847, 18161, 9590, 3993], "temperature": 0.0, "avg_logprob": -0.07849010344474547, "compression_ratio": 1.8221153846153846, "no_speech_prob": 4.539606743492186e-05}, {"id": 776, "seek": 409256, "start": 4106.6, "end": 4112.9, "text": " instead of changing the weights in a model we're changing the inputs to the model.", "tokens": [2602, 295, 4473, 264, 17443, 294, 257, 2316, 321, 434, 4473, 264, 15743, 281, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.07849010344474547, "compression_ratio": 1.8221153846153846, "no_speech_prob": 4.539606743492186e-05}, {"id": 777, "seek": 409256, "start": 4112.9, "end": 4117.76, "text": " And so we're going to take every pixel and we're going to modify it, subtract its gradient", "tokens": [400, 370, 321, 434, 516, 281, 747, 633, 19261, 293, 321, 434, 516, 281, 16927, 309, 11, 16390, 1080, 16235], "temperature": 0.0, "avg_logprob": -0.07849010344474547, "compression_ratio": 1.8221153846153846, "no_speech_prob": 4.539606743492186e-05}, {"id": 778, "seek": 409256, "start": 4117.76, "end": 4119.48, "text": " a little bit times its gradient.", "tokens": [257, 707, 857, 1413, 1080, 16235, 13], "temperature": 0.0, "avg_logprob": -0.07849010344474547, "compression_ratio": 1.8221153846153846, "no_speech_prob": 4.539606743492186e-05}, {"id": 779, "seek": 411948, "start": 4119.48, "end": 4125.5599999999995, "text": " So we multiply this by some constant, let's call it c, and then we're going to subtract", "tokens": [407, 321, 12972, 341, 538, 512, 5754, 11, 718, 311, 818, 309, 269, 11, 293, 550, 321, 434, 516, 281, 16390], "temperature": 0.0, "avg_logprob": -0.18906151056289672, "compression_ratio": 1.727810650887574, "no_speech_prob": 2.994426040459075e-06}, {"id": 780, "seek": 411948, "start": 4125.5599999999995, "end": 4130.16, "text": " it to get some new image.", "tokens": [309, 281, 483, 512, 777, 3256, 13], "temperature": 0.0, "avg_logprob": -0.18906151056289672, "compression_ratio": 1.727810650887574, "no_speech_prob": 2.994426040459075e-06}, {"id": 781, "seek": 411948, "start": 4130.16, "end": 4137.04, "text": " So with the new image it's probably going to get rid of some of these bits at the bottom,", "tokens": [407, 365, 264, 777, 3256, 309, 311, 1391, 516, 281, 483, 3973, 295, 512, 295, 613, 9239, 412, 264, 2767, 11], "temperature": 0.0, "avg_logprob": -0.18906151056289672, "compression_ratio": 1.727810650887574, "no_speech_prob": 2.994426040459075e-06}, {"id": 782, "seek": 411948, "start": 4137.04, "end": 4148.599999999999, "text": " right, and it's probably going to add a few more bits between some of these here, right.", "tokens": [558, 11, 293, 309, 311, 1391, 516, 281, 909, 257, 1326, 544, 9239, 1296, 512, 295, 613, 510, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.18906151056289672, "compression_ratio": 1.727810650887574, "no_speech_prob": 2.994426040459075e-06}, {"id": 783, "seek": 414860, "start": 4148.6, "end": 4157.4400000000005, "text": " Okay, and we've now got something that looks slightly more like a handwritten digit than", "tokens": [1033, 11, 293, 321, 600, 586, 658, 746, 300, 1542, 4748, 544, 411, 257, 1011, 26859, 14293, 813], "temperature": 0.0, "avg_logprob": -0.13299244206126143, "compression_ratio": 1.5689655172413792, "no_speech_prob": 3.1875538297754247e-06}, {"id": 784, "seek": 414860, "start": 4157.4400000000005, "end": 4161.4400000000005, "text": " before.", "tokens": [949, 13], "temperature": 0.0, "avg_logprob": -0.13299244206126143, "compression_ratio": 1.5689655172413792, "no_speech_prob": 3.1875538297754247e-06}, {"id": 785, "seek": 414860, "start": 4161.4400000000005, "end": 4163.64, "text": " And this is the basic idea.", "tokens": [400, 341, 307, 264, 3875, 1558, 13], "temperature": 0.0, "avg_logprob": -0.13299244206126143, "compression_ratio": 1.5689655172413792, "no_speech_prob": 3.1875538297754247e-06}, {"id": 786, "seek": 414860, "start": 4163.64, "end": 4164.68, "text": " We can now do that again.", "tokens": [492, 393, 586, 360, 300, 797, 13], "temperature": 0.0, "avg_logprob": -0.13299244206126143, "compression_ratio": 1.5689655172413792, "no_speech_prob": 3.1875538297754247e-06}, {"id": 787, "seek": 414860, "start": 4164.68, "end": 4170.04, "text": " We can now take this, we can run it through f, and so we've now got something, let's say", "tokens": [492, 393, 586, 747, 341, 11, 321, 393, 1190, 309, 807, 283, 11, 293, 370, 321, 600, 586, 658, 746, 11, 718, 311, 584], "temperature": 0.0, "avg_logprob": -0.13299244206126143, "compression_ratio": 1.5689655172413792, "no_speech_prob": 3.1875538297754247e-06}, {"id": 788, "seek": 414860, "start": 4170.04, "end": 4177.9400000000005, "text": " we call it x3 prime, for example.", "tokens": [321, 818, 309, 2031, 18, 5835, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.13299244206126143, "compression_ratio": 1.5689655172413792, "no_speech_prob": 3.1875538297754247e-06}, {"id": 789, "seek": 417794, "start": 4177.94, "end": 4184.639999999999, "text": " So this new version x3 prime or whatever is now the probability that's a handwritten digit,", "tokens": [407, 341, 777, 3037, 2031, 18, 5835, 420, 2035, 307, 586, 264, 8482, 300, 311, 257, 1011, 26859, 14293, 11], "temperature": 0.0, "avg_logprob": -0.1733576456705729, "compression_ratio": 1.7188940092165899, "no_speech_prob": 5.862781108589843e-06}, {"id": 790, "seek": 417794, "start": 4184.639999999999, "end": 4190.24, "text": " it's quite a bit higher, I'd say it's probably like 0.2 maybe.", "tokens": [309, 311, 1596, 257, 857, 2946, 11, 286, 1116, 584, 309, 311, 1391, 411, 1958, 13, 17, 1310, 13], "temperature": 0.0, "avg_logprob": -0.1733576456705729, "compression_ratio": 1.7188940092165899, "no_speech_prob": 5.862781108589843e-06}, {"id": 791, "seek": 417794, "start": 4190.24, "end": 4192.099999999999, "text": " And we can now do the same thing.", "tokens": [400, 321, 393, 586, 360, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.1733576456705729, "compression_ratio": 1.7188940092165899, "no_speech_prob": 5.862781108589843e-06}, {"id": 792, "seek": 417794, "start": 4192.099999999999, "end": 4197.12, "text": " We can say for every pixel if I increase its value a little bit or decrease its value a", "tokens": [492, 393, 584, 337, 633, 19261, 498, 286, 3488, 1080, 2158, 257, 707, 857, 420, 11514, 1080, 2158, 257], "temperature": 0.0, "avg_logprob": -0.1733576456705729, "compression_ratio": 1.7188940092165899, "no_speech_prob": 5.862781108589843e-06}, {"id": 793, "seek": 417794, "start": 4197.12, "end": 4203.5599999999995, "text": " little bit, how does it change the probability that this new x3 whatever prime prime is a", "tokens": [707, 857, 11, 577, 775, 309, 1319, 264, 8482, 300, 341, 777, 2031, 18, 2035, 5835, 5835, 307, 257], "temperature": 0.0, "avg_logprob": -0.1733576456705729, "compression_ratio": 1.7188940092165899, "no_speech_prob": 5.862781108589843e-06}, {"id": 794, "seek": 417794, "start": 4203.5599999999995, "end": 4204.599999999999, "text": " digit?", "tokens": [14293, 30], "temperature": 0.0, "avg_logprob": -0.1733576456705729, "compression_ratio": 1.7188940092165899, "no_speech_prob": 5.862781108589843e-06}, {"id": 795, "seek": 420460, "start": 4204.6, "end": 4211.64, "text": " And so we'll now get a new gradient here, 794 values, and we can use that to change", "tokens": [400, 370, 321, 603, 586, 483, 257, 777, 16235, 510, 11, 1614, 27032, 4190, 11, 293, 321, 393, 764, 300, 281, 1319], "temperature": 0.0, "avg_logprob": -0.07739837603135542, "compression_ratio": 1.2704918032786885, "no_speech_prob": 1.1189412134626764e-06}, {"id": 796, "seek": 420460, "start": 4211.64, "end": 4226.52, "text": " every pixel to make it look a little bit more like a handwritten digit.", "tokens": [633, 19261, 281, 652, 309, 574, 257, 707, 857, 544, 411, 257, 1011, 26859, 14293, 13], "temperature": 0.0, "avg_logprob": -0.07739837603135542, "compression_ratio": 1.2704918032786885, "no_speech_prob": 1.1189412134626764e-06}, {"id": 797, "seek": 422652, "start": 4226.52, "end": 4237.320000000001, "text": " So as you can see, if we have this magic function, we can use it to turn any arbitrary noisy", "tokens": [407, 382, 291, 393, 536, 11, 498, 321, 362, 341, 5585, 2445, 11, 321, 393, 764, 309, 281, 1261, 604, 23211, 24518], "temperature": 0.0, "avg_logprob": -0.07902340378080096, "compression_ratio": 1.5170068027210883, "no_speech_prob": 8.714321211300557e-07}, {"id": 798, "seek": 422652, "start": 4237.320000000001, "end": 4247.200000000001, "text": " input into something that looks like a valid input, something that has a high p-value from", "tokens": [4846, 666, 746, 300, 1542, 411, 257, 7363, 4846, 11, 746, 300, 575, 257, 1090, 280, 12, 29155, 490], "temperature": 0.0, "avg_logprob": -0.07902340378080096, "compression_ratio": 1.5170068027210883, "no_speech_prob": 8.714321211300557e-07}, {"id": 799, "seek": 422652, "start": 4247.200000000001, "end": 4250.76, "text": " that function by using this derivative.", "tokens": [300, 2445, 538, 1228, 341, 13760, 13], "temperature": 0.0, "avg_logprob": -0.07902340378080096, "compression_ratio": 1.5170068027210883, "no_speech_prob": 8.714321211300557e-07}, {"id": 800, "seek": 425076, "start": 4250.76, "end": 4259.08, "text": " So a key thing to remember here is this thing is saying as I change the input pixels, how", "tokens": [407, 257, 2141, 551, 281, 1604, 510, 307, 341, 551, 307, 1566, 382, 286, 1319, 264, 4846, 18668, 11, 577], "temperature": 0.0, "avg_logprob": -0.09350544697529561, "compression_ratio": 1.6455026455026456, "no_speech_prob": 1.3709508266401826e-06}, {"id": 801, "seek": 425076, "start": 4259.08, "end": 4262.320000000001, "text": " does it change the probability that this is a digit?", "tokens": [775, 309, 1319, 264, 8482, 300, 341, 307, 257, 14293, 30], "temperature": 0.0, "avg_logprob": -0.09350544697529561, "compression_ratio": 1.6455026455026456, "no_speech_prob": 1.3709508266401826e-06}, {"id": 802, "seek": 425076, "start": 4262.320000000001, "end": 4272.68, "text": " And that tells me which pixels to make darker and which pixels to make lighter.", "tokens": [400, 300, 5112, 385, 597, 18668, 281, 652, 12741, 293, 597, 18668, 281, 652, 11546, 13], "temperature": 0.0, "avg_logprob": -0.09350544697529561, "compression_ratio": 1.6455026455026456, "no_speech_prob": 1.3709508266401826e-06}, {"id": 803, "seek": 425076, "start": 4272.68, "end": 4279.24, "text": " Now those of you who remember your high school calculus may recall that when you do this", "tokens": [823, 729, 295, 291, 567, 1604, 428, 1090, 1395, 33400, 815, 9901, 300, 562, 291, 360, 341], "temperature": 0.0, "avg_logprob": -0.09350544697529561, "compression_ratio": 1.6455026455026456, "no_speech_prob": 1.3709508266401826e-06}, {"id": 804, "seek": 427924, "start": 4279.24, "end": 4287.679999999999, "text": " by changing x pixel one at a time to calculate a derivative, this is called the finite differencing", "tokens": [538, 4473, 2031, 19261, 472, 412, 257, 565, 281, 8873, 257, 13760, 11, 341, 307, 1219, 264, 19362, 743, 13644], "temperature": 0.0, "avg_logprob": -0.21001346111297609, "compression_ratio": 1.7445652173913044, "no_speech_prob": 1.0129938345926348e-05}, {"id": 805, "seek": 427924, "start": 4287.679999999999, "end": 4290.08, "text": " method of calculating derivatives.", "tokens": [3170, 295, 28258, 33733, 13], "temperature": 0.0, "avg_logprob": -0.21001346111297609, "compression_ratio": 1.7445652173913044, "no_speech_prob": 1.0129938345926348e-05}, {"id": 806, "seek": 427924, "start": 4290.08, "end": 4301.08, "text": " And it's very slow because we have to call finite differ, sorry I can't spell, differencing.", "tokens": [400, 309, 311, 588, 2964, 570, 321, 362, 281, 818, 19362, 743, 11, 2597, 286, 393, 380, 9827, 11, 743, 13644, 13], "temperature": 0.0, "avg_logprob": -0.21001346111297609, "compression_ratio": 1.7445652173913044, "no_speech_prob": 1.0129938345926348e-05}, {"id": 807, "seek": 427924, "start": 4301.08, "end": 4306.5199999999995, "text": " It's very slow because we have to call it seven this function 784 times for every single", "tokens": [467, 311, 588, 2964, 570, 321, 362, 281, 818, 309, 3407, 341, 2445, 1614, 25494, 1413, 337, 633, 2167], "temperature": 0.0, "avg_logprob": -0.21001346111297609, "compression_ratio": 1.7445652173913044, "no_speech_prob": 1.0129938345926348e-05}, {"id": 808, "seek": 427924, "start": 4306.5199999999995, "end": 4307.8, "text": " one.", "tokens": [472, 13], "temperature": 0.0, "avg_logprob": -0.21001346111297609, "compression_ratio": 1.7445652173913044, "no_speech_prob": 1.0129938345926348e-05}, {"id": 809, "seek": 430780, "start": 4307.8, "end": 4310.08, "text": " We don't have to use finite differencing.", "tokens": [492, 500, 380, 362, 281, 764, 19362, 743, 13644, 13], "temperature": 0.0, "avg_logprob": -0.2041338048082717, "compression_ratio": 1.2803030303030303, "no_speech_prob": 4.860423359787092e-06}, {"id": 810, "seek": 430780, "start": 4310.08, "end": 4319.400000000001, "text": " Assuming the folks running this magic API endpoint use Python, we can just call f dot", "tokens": [6281, 24919, 264, 4024, 2614, 341, 5585, 9362, 35795, 764, 15329, 11, 321, 393, 445, 818, 283, 5893], "temperature": 0.0, "avg_logprob": -0.2041338048082717, "compression_ratio": 1.2803030303030303, "no_speech_prob": 4.860423359787092e-06}, {"id": 811, "seek": 430780, "start": 4319.400000000001, "end": 4332.72, "text": " backward and then we can get x3 dot grad.", "tokens": [23897, 293, 550, 321, 393, 483, 2031, 18, 5893, 2771, 13], "temperature": 0.0, "avg_logprob": -0.2041338048082717, "compression_ratio": 1.2803030303030303, "no_speech_prob": 4.860423359787092e-06}, {"id": 812, "seek": 433272, "start": 4332.72, "end": 4338.4800000000005, "text": " And that will tell us the same thing in one go by using the analytic derivatives.", "tokens": [400, 300, 486, 980, 505, 264, 912, 551, 294, 472, 352, 538, 1228, 264, 40358, 33733, 13], "temperature": 0.0, "avg_logprob": -0.1502822240193685, "compression_ratio": 1.6865079365079365, "no_speech_prob": 3.4465449516574154e-06}, {"id": 813, "seek": 433272, "start": 4338.4800000000005, "end": 4343.92, "text": " So we'll learn exactly about what this dot backward does.", "tokens": [407, 321, 603, 1466, 2293, 466, 437, 341, 5893, 23897, 775, 13], "temperature": 0.0, "avg_logprob": -0.1502822240193685, "compression_ratio": 1.6865079365079365, "no_speech_prob": 3.4465449516574154e-06}, {"id": 814, "seek": 433272, "start": 4343.92, "end": 4347.72, "text": " We'll write our own everything from scratch including our own calculus things from scratch", "tokens": [492, 603, 2464, 527, 1065, 1203, 490, 8459, 3009, 527, 1065, 33400, 721, 490, 8459], "temperature": 0.0, "avg_logprob": -0.1502822240193685, "compression_ratio": 1.6865079365079365, "no_speech_prob": 3.4465449516574154e-06}, {"id": 815, "seek": 433272, "start": 4347.72, "end": 4351.96, "text": " later but for now, just like we did in part one of the course, we're just going to assume", "tokens": [1780, 457, 337, 586, 11, 445, 411, 321, 630, 294, 644, 472, 295, 264, 1164, 11, 321, 434, 445, 516, 281, 6552], "temperature": 0.0, "avg_logprob": -0.1502822240193685, "compression_ratio": 1.6865079365079365, "no_speech_prob": 3.4465449516574154e-06}, {"id": 816, "seek": 433272, "start": 4351.96, "end": 4355.9800000000005, "text": " these things exist.", "tokens": [613, 721, 2514, 13], "temperature": 0.0, "avg_logprob": -0.1502822240193685, "compression_ratio": 1.6865079365079365, "no_speech_prob": 3.4465449516574154e-06}, {"id": 817, "seek": 433272, "start": 4355.9800000000005, "end": 4359.400000000001, "text": " So maybe then the nice folks that provide this endpoint could actually provide a new", "tokens": [407, 1310, 550, 264, 1481, 4024, 300, 2893, 341, 35795, 727, 767, 2893, 257, 777], "temperature": 0.0, "avg_logprob": -0.1502822240193685, "compression_ratio": 1.6865079365079365, "no_speech_prob": 3.4465449516574154e-06}, {"id": 818, "seek": 435940, "start": 4359.4, "end": 4365.0, "text": " endpoint that calls dot backward for us and gives us dot grad.", "tokens": [35795, 300, 5498, 5893, 23897, 337, 505, 293, 2709, 505, 5893, 2771, 13], "temperature": 0.0, "avg_logprob": -0.13142263385611522, "compression_ratio": 1.595505617977528, "no_speech_prob": 7.527933121309616e-06}, {"id": 819, "seek": 435940, "start": 4365.0, "end": 4367.92, "text": " And then we don't really have to use f at all.", "tokens": [400, 550, 321, 500, 380, 534, 362, 281, 764, 283, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.13142263385611522, "compression_ratio": 1.595505617977528, "no_speech_prob": 7.527933121309616e-06}, {"id": 820, "seek": 435940, "start": 4367.92, "end": 4381.04, "text": " We can instead just directly call this endpoint that gives us the gradient directly.", "tokens": [492, 393, 2602, 445, 3838, 818, 341, 35795, 300, 2709, 505, 264, 16235, 3838, 13], "temperature": 0.0, "avg_logprob": -0.13142263385611522, "compression_ratio": 1.595505617977528, "no_speech_prob": 7.527933121309616e-06}, {"id": 821, "seek": 435940, "start": 4381.04, "end": 4386.28, "text": " We'll multiply it by this smaller constant c, we'll subtract it from the pixels and we'll", "tokens": [492, 603, 12972, 309, 538, 341, 4356, 5754, 269, 11, 321, 603, 16390, 309, 490, 264, 18668, 293, 321, 603], "temperature": 0.0, "avg_logprob": -0.13142263385611522, "compression_ratio": 1.595505617977528, "no_speech_prob": 7.527933121309616e-06}, {"id": 822, "seek": 438628, "start": 4386.28, "end": 4396.12, "text": " do it a few times, making the input get larger and larger p-values, larger and larger probabilities", "tokens": [360, 309, 257, 1326, 1413, 11, 1455, 264, 4846, 483, 4833, 293, 4833, 280, 12, 46033, 11, 4833, 293, 4833, 33783], "temperature": 0.0, "avg_logprob": -0.1396592419321944, "compression_ratio": 1.9555555555555555, "no_speech_prob": 4.7108769649639726e-06}, {"id": 823, "seek": 438628, "start": 4396.12, "end": 4401.12, "text": " that this is actually a digit.", "tokens": [300, 341, 307, 767, 257, 14293, 13], "temperature": 0.0, "avg_logprob": -0.1396592419321944, "compression_ratio": 1.9555555555555555, "no_speech_prob": 4.7108769649639726e-06}, {"id": 824, "seek": 438628, "start": 4401.12, "end": 4405.639999999999, "text": " So we don't particularly need this thing at all.", "tokens": [407, 321, 500, 380, 4098, 643, 341, 551, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.1396592419321944, "compression_ratio": 1.9555555555555555, "no_speech_prob": 4.7108769649639726e-06}, {"id": 825, "seek": 438628, "start": 4405.639999999999, "end": 4408.88, "text": " We don't particularly need the thing that calculates these probabilities.", "tokens": [492, 500, 380, 4098, 643, 264, 551, 300, 4322, 1024, 613, 33783, 13], "temperature": 0.0, "avg_logprob": -0.1396592419321944, "compression_ratio": 1.9555555555555555, "no_speech_prob": 4.7108769649639726e-06}, {"id": 826, "seek": 438628, "start": 4408.88, "end": 4416.0, "text": " We only need the thing that tells us which pixels we should change to calculate the probabilities.", "tokens": [492, 787, 643, 264, 551, 300, 5112, 505, 597, 18668, 321, 820, 1319, 281, 8873, 264, 33783, 13], "temperature": 0.0, "avg_logprob": -0.1396592419321944, "compression_ratio": 1.9555555555555555, "no_speech_prob": 4.7108769649639726e-06}, {"id": 827, "seek": 441600, "start": 4416.0, "end": 4421.48, "text": " Okay, so that's great.", "tokens": [1033, 11, 370, 300, 311, 869, 13], "temperature": 0.0, "avg_logprob": -0.22233156887990124, "compression_ratio": 1.2941176470588236, "no_speech_prob": 7.527922662120545e-06}, {"id": 828, "seek": 441600, "start": 4421.48, "end": 4427.56, "text": " The problem is nobody's provided this for us.", "tokens": [440, 1154, 307, 5079, 311, 5649, 341, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.22233156887990124, "compression_ratio": 1.2941176470588236, "no_speech_prob": 7.527922662120545e-06}, {"id": 829, "seek": 441600, "start": 4427.56, "end": 4431.36, "text": " So we're going to have to write it.", "tokens": [407, 321, 434, 516, 281, 362, 281, 2464, 309, 13], "temperature": 0.0, "avg_logprob": -0.22233156887990124, "compression_ratio": 1.2941176470588236, "no_speech_prob": 7.527922662120545e-06}, {"id": 830, "seek": 441600, "start": 4431.36, "end": 4436.2, "text": " So how are we going to do that?", "tokens": [407, 577, 366, 321, 516, 281, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.22233156887990124, "compression_ratio": 1.2941176470588236, "no_speech_prob": 7.527922662120545e-06}, {"id": 831, "seek": 441600, "start": 4436.2, "end": 4441.72, "text": " Well, no problem.", "tokens": [1042, 11, 572, 1154, 13], "temperature": 0.0, "avg_logprob": -0.22233156887990124, "compression_ratio": 1.2941176470588236, "no_speech_prob": 7.527922662120545e-06}, {"id": 832, "seek": 444172, "start": 4441.72, "end": 4446.76, "text": " Simply speaking, in this course, when there's some magic black box that we want to exist", "tokens": [19596, 4124, 11, 294, 341, 1164, 11, 562, 456, 311, 512, 5585, 2211, 2424, 300, 321, 528, 281, 2514], "temperature": 0.0, "avg_logprob": -0.13996788729792056, "compression_ratio": 1.6551724137931034, "no_speech_prob": 2.8130057216912974e-06}, {"id": 833, "seek": 444172, "start": 4446.76, "end": 4452.68, "text": " and it doesn't exist, we create a neural net and we train it.", "tokens": [293, 309, 1177, 380, 2514, 11, 321, 1884, 257, 18161, 2533, 293, 321, 3847, 309, 13], "temperature": 0.0, "avg_logprob": -0.13996788729792056, "compression_ratio": 1.6551724137931034, "no_speech_prob": 2.8130057216912974e-06}, {"id": 834, "seek": 444172, "start": 4452.68, "end": 4461.16, "text": " So we want to train a neural net that tells us which pixels to change to make a digit", "tokens": [407, 321, 528, 281, 3847, 257, 18161, 2533, 300, 5112, 505, 597, 18668, 281, 1319, 281, 652, 257, 14293], "temperature": 0.0, "avg_logprob": -0.13996788729792056, "compression_ratio": 1.6551724137931034, "no_speech_prob": 2.8130057216912974e-06}, {"id": 835, "seek": 444172, "start": 4461.16, "end": 4466.72, "text": " look, well to make an image look more like a handwritten digit.", "tokens": [574, 11, 731, 281, 652, 364, 3256, 574, 544, 411, 257, 1011, 26859, 14293, 13], "temperature": 0.0, "avg_logprob": -0.13996788729792056, "compression_ratio": 1.6551724137931034, "no_speech_prob": 2.8130057216912974e-06}, {"id": 836, "seek": 444172, "start": 4466.72, "end": 4471.04, "text": " Okay, so here's how we can do that.", "tokens": [1033, 11, 370, 510, 311, 577, 321, 393, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.13996788729792056, "compression_ratio": 1.6551724137931034, "no_speech_prob": 2.8130057216912974e-06}, {"id": 837, "seek": 447104, "start": 4471.04, "end": 4482.04, "text": " We could create some training data and use that training data to get the information", "tokens": [492, 727, 1884, 512, 3097, 1412, 293, 764, 300, 3097, 1412, 281, 483, 264, 1589], "temperature": 0.0, "avg_logprob": -0.05639834989581192, "compression_ratio": 1.9008264462809918, "no_speech_prob": 2.1233699953882024e-06}, {"id": 838, "seek": 447104, "start": 4482.04, "end": 4483.2, "text": " we want.", "tokens": [321, 528, 13], "temperature": 0.0, "avg_logprob": -0.05639834989581192, "compression_ratio": 1.9008264462809918, "no_speech_prob": 2.1233699953882024e-06}, {"id": 839, "seek": 447104, "start": 4483.2, "end": 4487.7, "text": " We could pass in something that looks a lot like a handwritten digit.", "tokens": [492, 727, 1320, 294, 746, 300, 1542, 257, 688, 411, 257, 1011, 26859, 14293, 13], "temperature": 0.0, "avg_logprob": -0.05639834989581192, "compression_ratio": 1.9008264462809918, "no_speech_prob": 2.1233699953882024e-06}, {"id": 840, "seek": 447104, "start": 4487.7, "end": 4493.68, "text": " We could pass something that looks a bit like a handwritten digit.", "tokens": [492, 727, 1320, 746, 300, 1542, 257, 857, 411, 257, 1011, 26859, 14293, 13], "temperature": 0.0, "avg_logprob": -0.05639834989581192, "compression_ratio": 1.9008264462809918, "no_speech_prob": 2.1233699953882024e-06}, {"id": 841, "seek": 449368, "start": 4493.68, "end": 4502.240000000001, "text": " We could pass something in that doesn't look very much like a handwritten digit.", "tokens": [492, 727, 1320, 746, 294, 300, 1177, 380, 574, 588, 709, 411, 257, 1011, 26859, 14293, 13], "temperature": 0.0, "avg_logprob": -0.15213832372351538, "compression_ratio": 1.7252747252747254, "no_speech_prob": 7.934470431791851e-07}, {"id": 842, "seek": 449368, "start": 4502.240000000001, "end": 4511.64, "text": " And we could pass in something which doesn't really look like a handwritten digit at all.", "tokens": [400, 321, 727, 1320, 294, 746, 597, 1177, 380, 534, 574, 411, 257, 1011, 26859, 14293, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.15213832372351538, "compression_ratio": 1.7252747252747254, "no_speech_prob": 7.934470431791851e-07}, {"id": 843, "seek": 449368, "start": 4511.64, "end": 4514.4800000000005, "text": " Now you'll notice it was very easy for me to create these.", "tokens": [823, 291, 603, 3449, 309, 390, 588, 1858, 337, 385, 281, 1884, 613, 13], "temperature": 0.0, "avg_logprob": -0.15213832372351538, "compression_ratio": 1.7252747252747254, "no_speech_prob": 7.934470431791851e-07}, {"id": 844, "seek": 449368, "start": 4514.4800000000005, "end": 4520.72, "text": " I created real handwritten digits and then I just chucked random noise on top of it.", "tokens": [286, 2942, 957, 1011, 26859, 27011, 293, 550, 286, 445, 20870, 292, 4974, 5658, 322, 1192, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.15213832372351538, "compression_ratio": 1.7252747252747254, "no_speech_prob": 7.934470431791851e-07}, {"id": 845, "seek": 452072, "start": 4520.72, "end": 4530.12, "text": " It's a little bit awkward for us to come up with an exact score saying how much is that", "tokens": [467, 311, 257, 707, 857, 11411, 337, 505, 281, 808, 493, 365, 364, 1900, 6175, 1566, 577, 709, 307, 300], "temperature": 0.0, "avg_logprob": -0.10983543395996094, "compression_ratio": 1.8231707317073171, "no_speech_prob": 4.785055352840573e-06}, {"id": 846, "seek": 452072, "start": 4530.12, "end": 4533.88, "text": " like a handwritten digit, how much is that like a handwritten digit, how much is that", "tokens": [411, 257, 1011, 26859, 14293, 11, 577, 709, 307, 300, 411, 257, 1011, 26859, 14293, 11, 577, 709, 307, 300], "temperature": 0.0, "avg_logprob": -0.10983543395996094, "compression_ratio": 1.8231707317073171, "no_speech_prob": 4.785055352840573e-06}, {"id": 847, "seek": 452072, "start": 4533.88, "end": 4534.88, "text": " and how much is that.", "tokens": [293, 577, 709, 307, 300, 13], "temperature": 0.0, "avg_logprob": -0.10983543395996094, "compression_ratio": 1.8231707317073171, "no_speech_prob": 4.785055352840573e-06}, {"id": 848, "seek": 452072, "start": 4534.88, "end": 4536.92, "text": " It seems a bit arbitrary.", "tokens": [467, 2544, 257, 857, 23211, 13], "temperature": 0.0, "avg_logprob": -0.10983543395996094, "compression_ratio": 1.8231707317073171, "no_speech_prob": 4.785055352840573e-06}, {"id": 849, "seek": 452072, "start": 4536.92, "end": 4538.64, "text": " So let's not do that.", "tokens": [407, 718, 311, 406, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.10983543395996094, "compression_ratio": 1.8231707317073171, "no_speech_prob": 4.785055352840573e-06}, {"id": 850, "seek": 452072, "start": 4538.64, "end": 4542.96, "text": " Let's use something which is kind of like the opposite.", "tokens": [961, 311, 764, 746, 597, 307, 733, 295, 411, 264, 6182, 13], "temperature": 0.0, "avg_logprob": -0.10983543395996094, "compression_ratio": 1.8231707317073171, "no_speech_prob": 4.785055352840573e-06}, {"id": 851, "seek": 454296, "start": 4542.96, "end": 4551.08, "text": " And instead let's say oh why don't we predict how much noise I added, right, because this", "tokens": [400, 2602, 718, 311, 584, 1954, 983, 500, 380, 321, 6069, 577, 709, 5658, 286, 3869, 11, 558, 11, 570, 341], "temperature": 0.0, "avg_logprob": -0.1670194983482361, "compression_ratio": 1.3043478260869565, "no_speech_prob": 5.45329271517403e-07}, {"id": 852, "seek": 454296, "start": 4551.08, "end": 4562.38, "text": " number 7 is actually equal to this number 7 plus this noise.", "tokens": [1230, 1614, 307, 767, 2681, 281, 341, 1230, 1614, 1804, 341, 5658, 13], "temperature": 0.0, "avg_logprob": -0.1670194983482361, "compression_ratio": 1.3043478260869565, "no_speech_prob": 5.45329271517403e-07}, {"id": 853, "seek": 456238, "start": 4562.38, "end": 4574.22, "text": " And this number 3 is actually equal to this number 3 plus this noise.", "tokens": [400, 341, 1230, 805, 307, 767, 2681, 281, 341, 1230, 805, 1804, 341, 5658, 13], "temperature": 0.0, "avg_logprob": -0.07986052139945653, "compression_ratio": 1.9186046511627908, "no_speech_prob": 6.893607178426464e-07}, {"id": 854, "seek": 456238, "start": 4574.22, "end": 4584.32, "text": " And this number 6 is actually equal to this number 6 plus this noise.", "tokens": [400, 341, 1230, 1386, 307, 767, 2681, 281, 341, 1230, 1386, 1804, 341, 5658, 13], "temperature": 0.0, "avg_logprob": -0.07986052139945653, "compression_ratio": 1.9186046511627908, "no_speech_prob": 6.893607178426464e-07}, {"id": 855, "seek": 456238, "start": 4584.32, "end": 4586.76, "text": " And that one's got a lot.", "tokens": [400, 300, 472, 311, 658, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.07986052139945653, "compression_ratio": 1.9186046511627908, "no_speech_prob": 6.893607178426464e-07}, {"id": 856, "seek": 458676, "start": 4586.76, "end": 4599.42, "text": " And of course the very first one is equal to this number 9 plus this noise.", "tokens": [400, 295, 1164, 264, 588, 700, 472, 307, 2681, 281, 341, 1230, 1722, 1804, 341, 5658, 13], "temperature": 0.0, "avg_logprob": -0.06810963704035833, "compression_ratio": 1.4970760233918128, "no_speech_prob": 4.812510496776667e-07}, {"id": 857, "seek": 458676, "start": 4599.42, "end": 4607.76, "text": " So why don't we generate this data and then rather than trying to come up with some arbitrary", "tokens": [407, 983, 500, 380, 321, 8460, 341, 1412, 293, 550, 2831, 813, 1382, 281, 808, 493, 365, 512, 23211], "temperature": 0.0, "avg_logprob": -0.06810963704035833, "compression_ratio": 1.4970760233918128, "no_speech_prob": 4.812510496776667e-07}, {"id": 858, "seek": 458676, "start": 4607.76, "end": 4614.58, "text": " number of like how much like a digit is it, let's say the amount of noise tells us how", "tokens": [1230, 295, 411, 577, 709, 411, 257, 14293, 307, 309, 11, 718, 311, 584, 264, 2372, 295, 5658, 5112, 505, 577], "temperature": 0.0, "avg_logprob": -0.06810963704035833, "compression_ratio": 1.4970760233918128, "no_speech_prob": 4.812510496776667e-07}, {"id": 859, "seek": 461458, "start": 4614.58, "end": 4617.24, "text": " much like a digit it is.", "tokens": [709, 411, 257, 14293, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.117211546949161, "compression_ratio": 1.7525773195876289, "no_speech_prob": 8.059428182605188e-07}, {"id": 860, "seek": 461458, "start": 4617.24, "end": 4621.68, "text": " So something with no noise is very much like a digit and something with lots of noise isn't", "tokens": [407, 746, 365, 572, 5658, 307, 588, 709, 411, 257, 14293, 293, 746, 365, 3195, 295, 5658, 1943, 380], "temperature": 0.0, "avg_logprob": -0.117211546949161, "compression_ratio": 1.7525773195876289, "no_speech_prob": 8.059428182605188e-07}, {"id": 861, "seek": 461458, "start": 4621.68, "end": 4623.94, "text": " much like a digit at all.", "tokens": [709, 411, 257, 14293, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.117211546949161, "compression_ratio": 1.7525773195876289, "no_speech_prob": 8.059428182605188e-07}, {"id": 862, "seek": 461458, "start": 4623.94, "end": 4627.76, "text": " So let's feed in, let's create a neural net.", "tokens": [407, 718, 311, 3154, 294, 11, 718, 311, 1884, 257, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.117211546949161, "compression_ratio": 1.7525773195876289, "no_speech_prob": 8.059428182605188e-07}, {"id": 863, "seek": 461458, "start": 4627.76, "end": 4637.04, "text": " Who cares what the architecture is, right, it's just a neural net of some kind.", "tokens": [2102, 12310, 437, 264, 9482, 307, 11, 558, 11, 309, 311, 445, 257, 18161, 2533, 295, 512, 733, 13], "temperature": 0.0, "avg_logprob": -0.117211546949161, "compression_ratio": 1.7525773195876289, "no_speech_prob": 8.059428182605188e-07}, {"id": 864, "seek": 461458, "start": 4637.04, "end": 4644.04, "text": " And this is critical to your understanding of this course at this point.", "tokens": [400, 341, 307, 4924, 281, 428, 3701, 295, 341, 1164, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.117211546949161, "compression_ratio": 1.7525773195876289, "no_speech_prob": 8.059428182605188e-07}, {"id": 865, "seek": 464404, "start": 4644.04, "end": 4648.2, "text": " We're going to go beyond the idea of like worrying all the time about architectures", "tokens": [492, 434, 516, 281, 352, 4399, 264, 1558, 295, 411, 18788, 439, 264, 565, 466, 6331, 1303], "temperature": 0.0, "avg_logprob": -0.15478911957183442, "compression_ratio": 1.7540983606557377, "no_speech_prob": 9.972460247809067e-06}, {"id": 866, "seek": 464404, "start": 4648.2, "end": 4653.28, "text": " and details and we're going to be spending quite often, I mean we're going to get to", "tokens": [293, 4365, 293, 321, 434, 516, 281, 312, 6434, 1596, 2049, 11, 286, 914, 321, 434, 516, 281, 483, 281], "temperature": 0.0, "avg_logprob": -0.15478911957183442, "compression_ratio": 1.7540983606557377, "no_speech_prob": 9.972460247809067e-06}, {"id": 867, "seek": 464404, "start": 4653.28, "end": 4658.04, "text": " all those details, but the important thing to using this stuff well is to think about", "tokens": [439, 729, 4365, 11, 457, 264, 1021, 551, 281, 1228, 341, 1507, 731, 307, 281, 519, 466], "temperature": 0.0, "avg_logprob": -0.15478911957183442, "compression_ratio": 1.7540983606557377, "no_speech_prob": 9.972460247809067e-06}, {"id": 868, "seek": 464404, "start": 4658.04, "end": 4667.92, "text": " neural nets as being something that has some inputs, some outputs.", "tokens": [18161, 36170, 382, 885, 746, 300, 575, 512, 15743, 11, 512, 23930, 13], "temperature": 0.0, "avg_logprob": -0.15478911957183442, "compression_ratio": 1.7540983606557377, "no_speech_prob": 9.972460247809067e-06}, {"id": 869, "seek": 466792, "start": 4667.92, "end": 4676.52, "text": " Oopsie-daisy.", "tokens": [21726, 414, 12, 67, 1527, 88, 13], "temperature": 0.0, "avg_logprob": -0.19544000625610353, "compression_ratio": 1.335820895522388, "no_speech_prob": 9.874602255877107e-07}, {"id": 870, "seek": 466792, "start": 4676.52, "end": 4688.8, "text": " Some outputs and some loss function which takes those two and then the derivative is", "tokens": [2188, 23930, 293, 512, 4470, 2445, 597, 2516, 729, 732, 293, 550, 264, 13760, 307], "temperature": 0.0, "avg_logprob": -0.19544000625610353, "compression_ratio": 1.335820895522388, "no_speech_prob": 9.874602255877107e-07}, {"id": 871, "seek": 466792, "start": 4688.8, "end": 4693.72, "text": " used to update the weights.", "tokens": [1143, 281, 5623, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.19544000625610353, "compression_ratio": 1.335820895522388, "no_speech_prob": 9.874602255877107e-07}, {"id": 872, "seek": 466792, "start": 4693.72, "end": 4697.78, "text": " That's really what we care about, those four things.", "tokens": [663, 311, 534, 437, 321, 1127, 466, 11, 729, 1451, 721, 13], "temperature": 0.0, "avg_logprob": -0.19544000625610353, "compression_ratio": 1.335820895522388, "no_speech_prob": 9.874602255877107e-07}, {"id": 873, "seek": 469778, "start": 4697.78, "end": 4710.639999999999, "text": " Now the inputs to our model is this.", "tokens": [823, 264, 15743, 281, 527, 2316, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.1474482114197778, "compression_ratio": 1.6267605633802817, "no_speech_prob": 2.090450152536505e-06}, {"id": 874, "seek": 469778, "start": 4710.639999999999, "end": 4713.98, "text": " Okay that's the inputs to our model.", "tokens": [1033, 300, 311, 264, 15743, 281, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1474482114197778, "compression_ratio": 1.6267605633802817, "no_speech_prob": 2.090450152536505e-06}, {"id": 875, "seek": 469778, "start": 4713.98, "end": 4718.92, "text": " The outputs to our model is a measure of how much noise there is.", "tokens": [440, 23930, 281, 527, 2316, 307, 257, 3481, 295, 577, 709, 5658, 456, 307, 13], "temperature": 0.0, "avg_logprob": -0.1474482114197778, "compression_ratio": 1.6267605633802817, "no_speech_prob": 2.090450152536505e-06}, {"id": 876, "seek": 469778, "start": 4718.92, "end": 4724.639999999999, "text": " So maybe we could just say oh well what's the, these are all basically normally distributed", "tokens": [407, 1310, 321, 727, 445, 584, 1954, 731, 437, 311, 264, 11, 613, 366, 439, 1936, 5646, 12631], "temperature": 0.0, "avg_logprob": -0.1474482114197778, "compression_ratio": 1.6267605633802817, "no_speech_prob": 2.090450152536505e-06}, {"id": 877, "seek": 472464, "start": 4724.64, "end": 4731.320000000001, "text": " random variables with a mean of zero and a variance in this case of zero.", "tokens": [4974, 9102, 365, 257, 914, 295, 4018, 293, 257, 21977, 294, 341, 1389, 295, 4018, 13], "temperature": 0.0, "avg_logprob": -0.15547102689743042, "compression_ratio": 2.172661870503597, "no_speech_prob": 1.0129907423106488e-05}, {"id": 878, "seek": 472464, "start": 4731.320000000001, "end": 4734.0, "text": " In this case they're normally distributed random variables with a mean of zero and a", "tokens": [682, 341, 1389, 436, 434, 5646, 12631, 4974, 9102, 365, 257, 914, 295, 4018, 293, 257], "temperature": 0.0, "avg_logprob": -0.15547102689743042, "compression_ratio": 2.172661870503597, "no_speech_prob": 1.0129907423106488e-05}, {"id": 879, "seek": 472464, "start": 4734.0, "end": 4736.76, "text": " variance of like 0.1.", "tokens": [21977, 295, 411, 1958, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.15547102689743042, "compression_ratio": 2.172661870503597, "no_speech_prob": 1.0129907423106488e-05}, {"id": 880, "seek": 472464, "start": 4736.76, "end": 4741.400000000001, "text": " This one's normally distributed random variables pixels I guess with a mean of zero and like", "tokens": [639, 472, 311, 5646, 12631, 4974, 9102, 18668, 286, 2041, 365, 257, 914, 295, 4018, 293, 411], "temperature": 0.0, "avg_logprob": -0.15547102689743042, "compression_ratio": 2.172661870503597, "no_speech_prob": 1.0129907423106488e-05}, {"id": 881, "seek": 472464, "start": 4741.400000000001, "end": 4743.9800000000005, "text": " 0.3.", "tokens": [1958, 13, 18, 13], "temperature": 0.0, "avg_logprob": -0.15547102689743042, "compression_ratio": 2.172661870503597, "no_speech_prob": 1.0129907423106488e-05}, {"id": 882, "seek": 472464, "start": 4743.9800000000005, "end": 4751.68, "text": " This one's super noisy.", "tokens": [639, 472, 311, 1687, 24518, 13], "temperature": 0.0, "avg_logprob": -0.15547102689743042, "compression_ratio": 2.172661870503597, "no_speech_prob": 1.0129907423106488e-05}, {"id": 883, "seek": 475168, "start": 4751.68, "end": 4757.52, "text": " That's the mean and variance so that's the mean for each one and the variance for each", "tokens": [663, 311, 264, 914, 293, 21977, 370, 300, 311, 264, 914, 337, 1184, 472, 293, 264, 21977, 337, 1184], "temperature": 0.0, "avg_logprob": -0.12815074451634142, "compression_ratio": 1.7596899224806202, "no_speech_prob": 1.5534940303041367e-06}, {"id": 884, "seek": 475168, "start": 4757.52, "end": 4759.900000000001, "text": " one.", "tokens": [472, 13], "temperature": 0.0, "avg_logprob": -0.12815074451634142, "compression_ratio": 1.7596899224806202, "no_speech_prob": 1.5534940303041367e-06}, {"id": 885, "seek": 475168, "start": 4759.900000000001, "end": 4766.12, "text": " So why don't we as the output use the variance?", "tokens": [407, 983, 500, 380, 321, 382, 264, 5598, 764, 264, 21977, 30], "temperature": 0.0, "avg_logprob": -0.12815074451634142, "compression_ratio": 1.7596899224806202, "no_speech_prob": 1.5534940303041367e-06}, {"id": 886, "seek": 475168, "start": 4766.12, "end": 4773.68, "text": " So predict how much noise or better still why don't we predict the actual noise itself?", "tokens": [407, 6069, 577, 709, 5658, 420, 1101, 920, 983, 500, 380, 321, 6069, 264, 3539, 5658, 2564, 30], "temperature": 0.0, "avg_logprob": -0.12815074451634142, "compression_ratio": 1.7596899224806202, "no_speech_prob": 1.5534940303041367e-06}, {"id": 887, "seek": 477368, "start": 4773.68, "end": 4782.3, "text": " So why don't we actually use that?", "tokens": [407, 983, 500, 380, 321, 767, 764, 300, 30], "temperature": 0.0, "avg_logprob": -0.11006859021309094, "compression_ratio": 1.6, "no_speech_prob": 1.06770175989368e-06}, {"id": 888, "seek": 477368, "start": 4782.3, "end": 4787.02, "text": " Now we're not just predicting how much noise but we predict the actual noise.", "tokens": [823, 321, 434, 406, 445, 32884, 577, 709, 5658, 457, 321, 6069, 264, 3539, 5658, 13], "temperature": 0.0, "avg_logprob": -0.11006859021309094, "compression_ratio": 1.6, "no_speech_prob": 1.06770175989368e-06}, {"id": 889, "seek": 477368, "start": 4787.02, "end": 4789.14, "text": " That's our outputs.", "tokens": [663, 311, 527, 23930, 13], "temperature": 0.0, "avg_logprob": -0.11006859021309094, "compression_ratio": 1.6, "no_speech_prob": 1.06770175989368e-06}, {"id": 890, "seek": 477368, "start": 4789.14, "end": 4792.84, "text": " Now if we do that our loss is going to be very simple.", "tokens": [823, 498, 321, 360, 300, 527, 4470, 307, 516, 281, 312, 588, 2199, 13], "temperature": 0.0, "avg_logprob": -0.11006859021309094, "compression_ratio": 1.6, "no_speech_prob": 1.06770175989368e-06}, {"id": 891, "seek": 477368, "start": 4792.84, "end": 4800.360000000001, "text": " It's going to be we took the input, we passed it through our neural net, we tried to predict", "tokens": [467, 311, 516, 281, 312, 321, 1890, 264, 4846, 11, 321, 4678, 309, 807, 527, 18161, 2533, 11, 321, 3031, 281, 6069], "temperature": 0.0, "avg_logprob": -0.11006859021309094, "compression_ratio": 1.6, "no_speech_prob": 1.06770175989368e-06}, {"id": 892, "seek": 480036, "start": 4800.36, "end": 4808.759999999999, "text": " what the noise was and so the prediction of the noise is n hat and the actual noise is", "tokens": [437, 264, 5658, 390, 293, 370, 264, 17630, 295, 264, 5658, 307, 297, 2385, 293, 264, 3539, 5658, 307], "temperature": 0.0, "avg_logprob": -0.11634717354407677, "compression_ratio": 1.7655172413793103, "no_speech_prob": 2.684193304958171e-06}, {"id": 893, "seek": 480036, "start": 4808.759999999999, "end": 4816.82, "text": " n and so we can do something we've done a thousand times which is we can divide it by", "tokens": [297, 293, 370, 321, 393, 360, 746, 321, 600, 1096, 257, 4714, 1413, 597, 307, 321, 393, 9845, 309, 538], "temperature": 0.0, "avg_logprob": -0.11634717354407677, "compression_ratio": 1.7655172413793103, "no_speech_prob": 2.684193304958171e-06}, {"id": 894, "seek": 480036, "start": 4816.82, "end": 4826.88, "text": " the count squared and then we can sum all that up and this here is the mean squared", "tokens": [264, 1207, 8889, 293, 550, 321, 393, 2408, 439, 300, 493, 293, 341, 510, 307, 264, 914, 8889], "temperature": 0.0, "avg_logprob": -0.11634717354407677, "compression_ratio": 1.7655172413793103, "no_speech_prob": 2.684193304958171e-06}, {"id": 895, "seek": 482688, "start": 4826.88, "end": 4832.84, "text": " error which we use all the time.", "tokens": [6713, 597, 321, 764, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.06492137450438279, "compression_ratio": 1.5488721804511278, "no_speech_prob": 1.553493461869948e-06}, {"id": 896, "seek": 482688, "start": 4832.84, "end": 4841.8, "text": " So the mean squared error means that we've now got inputs which is noisy digits, we've", "tokens": [407, 264, 914, 8889, 6713, 1355, 300, 321, 600, 586, 658, 15743, 597, 307, 24518, 27011, 11, 321, 600], "temperature": 0.0, "avg_logprob": -0.06492137450438279, "compression_ratio": 1.5488721804511278, "no_speech_prob": 1.553493461869948e-06}, {"id": 897, "seek": 482688, "start": 4841.8, "end": 4853.08, "text": " got outputs which is noise and so this neural network is trying to predict this noise.", "tokens": [658, 23930, 597, 307, 5658, 293, 370, 341, 18161, 3209, 307, 1382, 281, 6069, 341, 5658, 13], "temperature": 0.0, "avg_logprob": -0.06492137450438279, "compression_ratio": 1.5488721804511278, "no_speech_prob": 1.553493461869948e-06}, {"id": 898, "seek": 485308, "start": 4853.08, "end": 4861.68, "text": " So we're basically jumping straight to the step that we had here.", "tokens": [407, 321, 434, 1936, 11233, 2997, 281, 264, 1823, 300, 321, 632, 510, 13], "temperature": 0.0, "avg_logprob": -0.11184128847989169, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.34969889131753e-06}, {"id": 899, "seek": 485308, "start": 4861.68, "end": 4864.16, "text": " Remember this is what we really wanted.", "tokens": [5459, 341, 307, 437, 321, 534, 1415, 13], "temperature": 0.0, "avg_logprob": -0.11184128847989169, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.34969889131753e-06}, {"id": 900, "seek": 485308, "start": 4864.16, "end": 4870.88, "text": " We wanted some ability to know how much do we have to change a pixel by to make it more", "tokens": [492, 1415, 512, 3485, 281, 458, 577, 709, 360, 321, 362, 281, 1319, 257, 19261, 538, 281, 652, 309, 544], "temperature": 0.0, "avg_logprob": -0.11184128847989169, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.34969889131753e-06}, {"id": 901, "seek": 485308, "start": 4870.88, "end": 4872.72, "text": " digit like.", "tokens": [14293, 411, 13], "temperature": 0.0, "avg_logprob": -0.11184128847989169, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.34969889131753e-06}, {"id": 902, "seek": 485308, "start": 4872.72, "end": 4882.42, "text": " Well to turn this number seven into this number seven, that's our goal, we have to remove", "tokens": [1042, 281, 1261, 341, 1230, 3407, 666, 341, 1230, 3407, 11, 300, 311, 527, 3387, 11, 321, 362, 281, 4159], "temperature": 0.0, "avg_logprob": -0.11184128847989169, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.34969889131753e-06}, {"id": 903, "seek": 488242, "start": 4882.42, "end": 4884.9800000000005, "text": " all of that.", "tokens": [439, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.10165983356841624, "compression_ratio": 1.6606060606060606, "no_speech_prob": 2.0580403088388266e-06}, {"id": 904, "seek": 488242, "start": 4884.9800000000005, "end": 4893.24, "text": " So if we can predict the noise then we've got exactly what we want which is this.", "tokens": [407, 498, 321, 393, 6069, 264, 5658, 550, 321, 600, 658, 2293, 437, 321, 528, 597, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.10165983356841624, "compression_ratio": 1.6606060606060606, "no_speech_prob": 2.0580403088388266e-06}, {"id": 905, "seek": 488242, "start": 4893.24, "end": 4894.78, "text": " We can then do this process.", "tokens": [492, 393, 550, 360, 341, 1399, 13], "temperature": 0.0, "avg_logprob": -0.10165983356841624, "compression_ratio": 1.6606060606060606, "no_speech_prob": 2.0580403088388266e-06}, {"id": 906, "seek": 488242, "start": 4894.78, "end": 4902.0, "text": " We can take multiply it by a constant and subtract it from our input and so if you subtract", "tokens": [492, 393, 747, 12972, 309, 538, 257, 5754, 293, 16390, 309, 490, 527, 4846, 293, 370, 498, 291, 16390], "temperature": 0.0, "avg_logprob": -0.10165983356841624, "compression_ratio": 1.6606060606060606, "no_speech_prob": 2.0580403088388266e-06}, {"id": 907, "seek": 488242, "start": 4902.0, "end": 4908.8, "text": " this noise from this input you get this handwritten digit.", "tokens": [341, 5658, 490, 341, 4846, 291, 483, 341, 1011, 26859, 14293, 13], "temperature": 0.0, "avg_logprob": -0.10165983356841624, "compression_ratio": 1.6606060606060606, "no_speech_prob": 2.0580403088388266e-06}, {"id": 908, "seek": 490880, "start": 4908.8, "end": 4918.76, "text": " So we're doing exactly what we wanted.", "tokens": [407, 321, 434, 884, 2293, 437, 321, 1415, 13], "temperature": 0.0, "avg_logprob": -0.13614533045520522, "compression_ratio": 1.4947916666666667, "no_speech_prob": 8.186340778593149e-07}, {"id": 909, "seek": 490880, "start": 4918.76, "end": 4921.360000000001, "text": " Well that seems easy enough.", "tokens": [1042, 300, 2544, 1858, 1547, 13], "temperature": 0.0, "avg_logprob": -0.13614533045520522, "compression_ratio": 1.4947916666666667, "no_speech_prob": 8.186340778593149e-07}, {"id": 910, "seek": 490880, "start": 4921.360000000001, "end": 4926.2, "text": " We already know from part one how to do this.", "tokens": [492, 1217, 458, 490, 644, 472, 577, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.13614533045520522, "compression_ratio": 1.4947916666666667, "no_speech_prob": 8.186340778593149e-07}, {"id": 911, "seek": 490880, "start": 4926.2, "end": 4931.96, "text": " So we just have any old neural network, so some kind of conv net or something that takes", "tokens": [407, 321, 445, 362, 604, 1331, 18161, 3209, 11, 370, 512, 733, 295, 3754, 2533, 420, 746, 300, 2516], "temperature": 0.0, "avg_logprob": -0.13614533045520522, "compression_ratio": 1.4947916666666667, "no_speech_prob": 8.186340778593149e-07}, {"id": 912, "seek": 490880, "start": 4931.96, "end": 4938.16, "text": " as input numbers where we've just randomly added different amounts of noise, lots of", "tokens": [382, 4846, 3547, 689, 321, 600, 445, 16979, 3869, 819, 11663, 295, 5658, 11, 3195, 295], "temperature": 0.0, "avg_logprob": -0.13614533045520522, "compression_ratio": 1.4947916666666667, "no_speech_prob": 8.186340778593149e-07}, {"id": 913, "seek": 493816, "start": 4938.16, "end": 4941.16, "text": " noise to some, not much noise to others.", "tokens": [5658, 281, 512, 11, 406, 709, 5658, 281, 2357, 13], "temperature": 0.0, "avg_logprob": -0.13092621585779024, "compression_ratio": 1.5434782608695652, "no_speech_prob": 1.7330484070043894e-06}, {"id": 914, "seek": 493816, "start": 4941.16, "end": 4945.099999999999, "text": " It predicts what the noise was that we added.", "tokens": [467, 6069, 82, 437, 264, 5658, 390, 300, 321, 3869, 13], "temperature": 0.0, "avg_logprob": -0.13092621585779024, "compression_ratio": 1.5434782608695652, "no_speech_prob": 1.7330484070043894e-06}, {"id": 915, "seek": 493816, "start": 4945.099999999999, "end": 4954.5599999999995, "text": " We take the loss between the predicted output and the actual noise, mean squared error,", "tokens": [492, 747, 264, 4470, 1296, 264, 19147, 5598, 293, 264, 3539, 5658, 11, 914, 8889, 6713, 11], "temperature": 0.0, "avg_logprob": -0.13092621585779024, "compression_ratio": 1.5434782608695652, "no_speech_prob": 1.7330484070043894e-06}, {"id": 916, "seek": 493816, "start": 4954.5599999999995, "end": 4957.0599999999995, "text": " and we use that to update the weights.", "tokens": [293, 321, 764, 300, 281, 5623, 264, 17443, 13], "temperature": 0.0, "avg_logprob": -0.13092621585779024, "compression_ratio": 1.5434782608695652, "no_speech_prob": 1.7330484070043894e-06}, {"id": 917, "seek": 495706, "start": 4957.06, "end": 4970.96, "text": " And so if we train this for a while, then if we pass this into our model it will return", "tokens": [400, 370, 498, 321, 3847, 341, 337, 257, 1339, 11, 550, 498, 321, 1320, 341, 666, 527, 2316, 309, 486, 2736], "temperature": 0.0, "avg_logprob": -0.20484339022168926, "compression_ratio": 1.3064516129032258, "no_speech_prob": 2.7852703965436376e-07}, {"id": 918, "seek": 495706, "start": 4970.96, "end": 4974.6, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.20484339022168926, "compression_ratio": 1.3064516129032258, "no_speech_prob": 2.7852703965436376e-07}, {"id": 919, "seek": 495706, "start": 4974.6, "end": 4979.900000000001, "text": " And we're done.", "tokens": [400, 321, 434, 1096, 13], "temperature": 0.0, "avg_logprob": -0.20484339022168926, "compression_ratio": 1.3064516129032258, "no_speech_prob": 2.7852703965436376e-07}, {"id": 920, "seek": 495706, "start": 4979.900000000001, "end": 4984.0, "text": " We now have something that can generate images.", "tokens": [492, 586, 362, 746, 300, 393, 8460, 5267, 13], "temperature": 0.0, "avg_logprob": -0.20484339022168926, "compression_ratio": 1.3064516129032258, "no_speech_prob": 2.7852703965436376e-07}, {"id": 921, "seek": 495706, "start": 4984.0, "end": 4985.0, "text": " How?", "tokens": [1012, 30], "temperature": 0.0, "avg_logprob": -0.20484339022168926, "compression_ratio": 1.3064516129032258, "no_speech_prob": 2.7852703965436376e-07}, {"id": 922, "seek": 498500, "start": 4985.0, "end": 4997.0, "text": " Because now we can take this trained neural network, so I'm going to copy it down here,", "tokens": [1436, 586, 321, 393, 747, 341, 8895, 18161, 3209, 11, 370, 286, 478, 516, 281, 5055, 309, 760, 510, 11], "temperature": 0.0, "avg_logprob": -0.1415596306324005, "compression_ratio": 1.5632911392405062, "no_speech_prob": 4.029434421681799e-06}, {"id": 923, "seek": 498500, "start": 4997.0, "end": 5007.2, "text": " and we can pass it something very, very, very noisy, which is pure noise.", "tokens": [293, 321, 393, 1320, 309, 746, 588, 11, 588, 11, 588, 24518, 11, 597, 307, 6075, 5658, 13], "temperature": 0.0, "avg_logprob": -0.1415596306324005, "compression_ratio": 1.5632911392405062, "no_speech_prob": 4.029434421681799e-06}, {"id": 924, "seek": 498500, "start": 5007.2, "end": 5013.88, "text": " We pass it to the neural net and it's going to spit out information saying which part", "tokens": [492, 1320, 309, 281, 264, 18161, 2533, 293, 309, 311, 516, 281, 22127, 484, 1589, 1566, 597, 644], "temperature": 0.0, "avg_logprob": -0.1415596306324005, "compression_ratio": 1.5632911392405062, "no_speech_prob": 4.029434421681799e-06}, {"id": 925, "seek": 501388, "start": 5013.88, "end": 5016.400000000001, "text": " of that does it think is noise.", "tokens": [295, 300, 775, 309, 519, 307, 5658, 13], "temperature": 0.0, "avg_logprob": -0.12089898334286077, "compression_ratio": 2.1391752577319587, "no_speech_prob": 2.9944053494546097e-06}, {"id": 926, "seek": 501388, "start": 5016.400000000001, "end": 5022.04, "text": " And it's going to leave behind the bits that look the most like a digit, just like we did", "tokens": [400, 309, 311, 516, 281, 1856, 2261, 264, 9239, 300, 574, 264, 881, 411, 257, 14293, 11, 445, 411, 321, 630], "temperature": 0.0, "avg_logprob": -0.12089898334286077, "compression_ratio": 2.1391752577319587, "no_speech_prob": 2.9944053494546097e-06}, {"id": 927, "seek": 501388, "start": 5022.04, "end": 5024.16, "text": " back here.", "tokens": [646, 510, 13], "temperature": 0.0, "avg_logprob": -0.12089898334286077, "compression_ratio": 2.1391752577319587, "no_speech_prob": 2.9944053494546097e-06}, {"id": 928, "seek": 501388, "start": 5024.16, "end": 5032.92, "text": " So it might say, oh, you know what, if you left behind just that bit, that bit, that", "tokens": [407, 309, 1062, 584, 11, 1954, 11, 291, 458, 437, 11, 498, 291, 1411, 2261, 445, 300, 857, 11, 300, 857, 11, 300], "temperature": 0.0, "avg_logprob": -0.12089898334286077, "compression_ratio": 2.1391752577319587, "no_speech_prob": 2.9944053494546097e-06}, {"id": 929, "seek": 501388, "start": 5032.92, "end": 5036.68, "text": " bit, that bit, that bit, that bit, that bit, and that bit, it's going to look a little", "tokens": [857, 11, 300, 857, 11, 300, 857, 11, 300, 857, 11, 300, 857, 11, 293, 300, 857, 11, 309, 311, 516, 281, 574, 257, 707], "temperature": 0.0, "avg_logprob": -0.12089898334286077, "compression_ratio": 2.1391752577319587, "no_speech_prob": 2.9944053494546097e-06}, {"id": 930, "seek": 501388, "start": 5036.68, "end": 5038.84, "text": " bit more like a digit.", "tokens": [857, 544, 411, 257, 14293, 13], "temperature": 0.0, "avg_logprob": -0.12089898334286077, "compression_ratio": 2.1391752577319587, "no_speech_prob": 2.9944053494546097e-06}, {"id": 931, "seek": 501388, "start": 5038.84, "end": 5042.96, "text": " And then maybe you could increase the values of that bit, that bit, that bit, that bit,", "tokens": [400, 550, 1310, 291, 727, 3488, 264, 4190, 295, 300, 857, 11, 300, 857, 11, 300, 857, 11, 300, 857, 11], "temperature": 0.0, "avg_logprob": -0.12089898334286077, "compression_ratio": 2.1391752577319587, "no_speech_prob": 2.9944053494546097e-06}, {"id": 932, "seek": 504296, "start": 5042.96, "end": 5045.0, "text": " that bit, and that bit.", "tokens": [300, 857, 11, 293, 300, 857, 13], "temperature": 0.0, "avg_logprob": -0.14266652771920868, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.9638002868305193e-06}, {"id": 933, "seek": 504296, "start": 5045.0, "end": 5058.16, "text": " And so after you do that, and so then everything else is noise, so we subtract those bits,", "tokens": [400, 370, 934, 291, 360, 300, 11, 293, 370, 550, 1203, 1646, 307, 5658, 11, 370, 321, 16390, 729, 9239, 11], "temperature": 0.0, "avg_logprob": -0.14266652771920868, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.9638002868305193e-06}, {"id": 934, "seek": 504296, "start": 5058.16, "end": 5068.4800000000005, "text": " subtract it, times some constant, we're now going to have something that looks more like", "tokens": [16390, 309, 11, 1413, 512, 5754, 11, 321, 434, 586, 516, 281, 362, 746, 300, 1542, 544, 411], "temperature": 0.0, "avg_logprob": -0.14266652771920868, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.9638002868305193e-06}, {"id": 935, "seek": 504296, "start": 5068.4800000000005, "end": 5072.4800000000005, "text": " a digit, which is what we hoped for.", "tokens": [257, 14293, 11, 597, 307, 437, 321, 19737, 337, 13], "temperature": 0.0, "avg_logprob": -0.14266652771920868, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.9638002868305193e-06}, {"id": 936, "seek": 507248, "start": 5072.48, "end": 5075.639999999999, "text": " And so then we can just do it again.", "tokens": [400, 370, 550, 321, 393, 445, 360, 309, 797, 13], "temperature": 0.0, "avg_logprob": -0.21613753925670276, "compression_ratio": 1.2991452991452992, "no_speech_prob": 1.2606740710907616e-05}, {"id": 937, "seek": 507248, "start": 5075.639999999999, "end": 5095.599999999999, "text": " And you can see now why we are doing this multiple times.", "tokens": [400, 291, 393, 536, 586, 983, 321, 366, 884, 341, 3866, 1413, 13], "temperature": 0.0, "avg_logprob": -0.21613753925670276, "compression_ratio": 1.2991452991452992, "no_speech_prob": 1.2606740710907616e-05}, {"id": 938, "seek": 507248, "start": 5095.599999999999, "end": 5099.639999999999, "text": " Somebody on the chat is saying they don't see me drawing?", "tokens": [13463, 322, 264, 5081, 307, 1566, 436, 500, 380, 536, 385, 6316, 30], "temperature": 0.0, "avg_logprob": -0.21613753925670276, "compression_ratio": 1.2991452991452992, "no_speech_prob": 1.2606740710907616e-05}, {"id": 939, "seek": 509964, "start": 5099.64, "end": 5102.8, "text": " Oh, you can see.", "tokens": [876, 11, 291, 393, 536, 13], "temperature": 0.0, "avg_logprob": -0.21855948807357192, "compression_ratio": 1.3762886597938144, "no_speech_prob": 6.747487987013301e-06}, {"id": 940, "seek": 509964, "start": 5102.8, "end": 5103.8, "text": " Thanks Jimmy.", "tokens": [2561, 15709, 13], "temperature": 0.0, "avg_logprob": -0.21855948807357192, "compression_ratio": 1.3762886597938144, "no_speech_prob": 6.747487987013301e-06}, {"id": 941, "seek": 509964, "start": 5103.8, "end": 5107.88, "text": " Don't know, Michelangelo, what's happening for you.", "tokens": [1468, 380, 458, 11, 23709, 656, 10590, 11, 437, 311, 2737, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.21855948807357192, "compression_ratio": 1.3762886597938144, "no_speech_prob": 6.747487987013301e-06}, {"id": 942, "seek": 509964, "start": 5107.88, "end": 5112.96, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.21855948807357192, "compression_ratio": 1.3762886597938144, "no_speech_prob": 6.747487987013301e-06}, {"id": 943, "seek": 509964, "start": 5112.96, "end": 5119.72, "text": " And to answer your earlier question about how am I drawing, I'm using a graphics tablet,", "tokens": [400, 281, 1867, 428, 3071, 1168, 466, 577, 669, 286, 6316, 11, 286, 478, 1228, 257, 11837, 14136, 11], "temperature": 0.0, "avg_logprob": -0.21855948807357192, "compression_ratio": 1.3762886597938144, "no_speech_prob": 6.747487987013301e-06}, {"id": 944, "seek": 509964, "start": 5119.72, "end": 5123.200000000001, "text": " which I'm not very expert at because on Windows you can just draw directly on the screen,", "tokens": [597, 286, 478, 406, 588, 5844, 412, 570, 322, 8591, 291, 393, 445, 2642, 3838, 322, 264, 2568, 11], "temperature": 0.0, "avg_logprob": -0.21855948807357192, "compression_ratio": 1.3762886597938144, "no_speech_prob": 6.747487987013301e-06}, {"id": 945, "seek": 512320, "start": 5123.2, "end": 5130.08, "text": " which is why this is particularly messy.", "tokens": [597, 307, 983, 341, 307, 4098, 16191, 13], "temperature": 0.0, "avg_logprob": -0.16009464460549896, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.805641113605816e-05}, {"id": 946, "seek": 512320, "start": 5130.08, "end": 5139.5199999999995, "text": " Alright, in practice, at the moment, this might change by the time you've watched this,", "tokens": [2798, 11, 294, 3124, 11, 412, 264, 1623, 11, 341, 1062, 1319, 538, 264, 565, 291, 600, 6337, 341, 11], "temperature": 0.0, "avg_logprob": -0.16009464460549896, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.805641113605816e-05}, {"id": 947, "seek": 512320, "start": 5139.5199999999995, "end": 5143.24, "text": " we use a particular type of neural net for this.", "tokens": [321, 764, 257, 1729, 2010, 295, 18161, 2533, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.16009464460549896, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.805641113605816e-05}, {"id": 948, "seek": 512320, "start": 5143.24, "end": 5147.4, "text": " The particular kind of neural net we use is something that was developed for medical imaging", "tokens": [440, 1729, 733, 295, 18161, 2533, 321, 764, 307, 746, 300, 390, 4743, 337, 4625, 25036], "temperature": 0.0, "avg_logprob": -0.16009464460549896, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.805641113605816e-05}, {"id": 949, "seek": 512320, "start": 5147.4, "end": 5149.98, "text": " called the U-net.", "tokens": [1219, 264, 624, 12, 7129, 13], "temperature": 0.0, "avg_logprob": -0.16009464460549896, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.805641113605816e-05}, {"id": 950, "seek": 512320, "start": 5149.98, "end": 5153.08, "text": " If you've done previous versions of the course, you'll have seen this, and don't worry, this", "tokens": [759, 291, 600, 1096, 3894, 9606, 295, 264, 1164, 11, 291, 603, 362, 1612, 341, 11, 293, 500, 380, 3292, 11, 341], "temperature": 0.0, "avg_logprob": -0.16009464460549896, "compression_ratio": 1.6565217391304348, "no_speech_prob": 1.805641113605816e-05}, {"id": 951, "seek": 515308, "start": 5153.08, "end": 5157.5199999999995, "text": " course will see exactly how a U-net works and we'll build them ourselves from scratch.", "tokens": [1164, 486, 536, 2293, 577, 257, 624, 12, 7129, 1985, 293, 321, 603, 1322, 552, 4175, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.14905052394657345, "compression_ratio": 1.4931506849315068, "no_speech_prob": 1.3006105291424319e-05}, {"id": 952, "seek": 515308, "start": 5157.5199999999995, "end": 5162.88, "text": " And this is the first component of stable diffusion.", "tokens": [400, 341, 307, 264, 700, 6542, 295, 8351, 25242, 13], "temperature": 0.0, "avg_logprob": -0.14905052394657345, "compression_ratio": 1.4931506849315068, "no_speech_prob": 1.3006105291424319e-05}, {"id": 953, "seek": 515308, "start": 5162.88, "end": 5165.88, "text": " It's the U-net.", "tokens": [467, 311, 264, 624, 12, 7129, 13], "temperature": 0.0, "avg_logprob": -0.14905052394657345, "compression_ratio": 1.4931506849315068, "no_speech_prob": 1.3006105291424319e-05}, {"id": 954, "seek": 515308, "start": 5165.88, "end": 5177.6, "text": " Okay, so there's going to be a few pieces and the details of why they're called these", "tokens": [1033, 11, 370, 456, 311, 516, 281, 312, 257, 1326, 3755, 293, 264, 4365, 295, 983, 436, 434, 1219, 613], "temperature": 0.0, "avg_logprob": -0.14905052394657345, "compression_ratio": 1.4931506849315068, "no_speech_prob": 1.3006105291424319e-05}, {"id": 955, "seek": 515308, "start": 5177.6, "end": 5180.0, "text": " things don't matter too much just yet.", "tokens": [721, 500, 380, 1871, 886, 709, 445, 1939, 13], "temperature": 0.0, "avg_logprob": -0.14905052394657345, "compression_ratio": 1.4931506849315068, "no_speech_prob": 1.3006105291424319e-05}, {"id": 956, "seek": 515308, "start": 5180.0, "end": 5182.32, "text": " Just take my word for it, this is their names.", "tokens": [1449, 747, 452, 1349, 337, 309, 11, 341, 307, 641, 5288, 13], "temperature": 0.0, "avg_logprob": -0.14905052394657345, "compression_ratio": 1.4931506849315068, "no_speech_prob": 1.3006105291424319e-05}, {"id": 957, "seek": 518232, "start": 5182.32, "end": 5186.0, "text": " And the thing that you do need to know for each thing is like, what's the input and what's", "tokens": [400, 264, 551, 300, 291, 360, 643, 281, 458, 337, 1184, 551, 307, 411, 11, 437, 311, 264, 4846, 293, 437, 311], "temperature": 0.0, "avg_logprob": -0.1761227690655252, "compression_ratio": 1.75, "no_speech_prob": 3.7852882996958215e-06}, {"id": 958, "seek": 518232, "start": 5186.0, "end": 5187.0, "text": " the output?", "tokens": [264, 5598, 30], "temperature": 0.0, "avg_logprob": -0.1761227690655252, "compression_ratio": 1.75, "no_speech_prob": 3.7852882996958215e-06}, {"id": 959, "seek": 518232, "start": 5187.0, "end": 5191.36, "text": " So the input to a U-net, well what does it do?", "tokens": [407, 264, 4846, 281, 257, 624, 12, 7129, 11, 731, 437, 775, 309, 360, 30], "temperature": 0.0, "avg_logprob": -0.1761227690655252, "compression_ratio": 1.75, "no_speech_prob": 3.7852882996958215e-06}, {"id": 960, "seek": 518232, "start": 5191.36, "end": 5202.759999999999, "text": " The input to the U-net is a somewhat noisy image.", "tokens": [440, 4846, 281, 264, 624, 12, 7129, 307, 257, 8344, 24518, 3256, 13], "temperature": 0.0, "avg_logprob": -0.1761227690655252, "compression_ratio": 1.75, "no_speech_prob": 3.7852882996958215e-06}, {"id": 961, "seek": 518232, "start": 5202.759999999999, "end": 5207.16, "text": " When I say somewhat, it could be not noisy at all, or it could be all noise.", "tokens": [1133, 286, 584, 8344, 11, 309, 727, 312, 406, 24518, 412, 439, 11, 420, 309, 727, 312, 439, 5658, 13], "temperature": 0.0, "avg_logprob": -0.1761227690655252, "compression_ratio": 1.75, "no_speech_prob": 3.7852882996958215e-06}, {"id": 962, "seek": 518232, "start": 5207.16, "end": 5209.2, "text": " That's the input.", "tokens": [663, 311, 264, 4846, 13], "temperature": 0.0, "avg_logprob": -0.1761227690655252, "compression_ratio": 1.75, "no_speech_prob": 3.7852882996958215e-06}, {"id": 963, "seek": 520920, "start": 5209.2, "end": 5215.8, "text": " And the output is the noise.", "tokens": [400, 264, 5598, 307, 264, 5658, 13], "temperature": 0.0, "avg_logprob": -0.16708606222401495, "compression_ratio": 1.5034013605442176, "no_speech_prob": 8.315247441714746e-07}, {"id": 964, "seek": 520920, "start": 5215.8, "end": 5221.679999999999, "text": " Such that if we subtract the output from the input, we end up with the un-noisy image,", "tokens": [9653, 300, 498, 321, 16390, 264, 5598, 490, 264, 4846, 11, 321, 917, 493, 365, 264, 517, 12, 1771, 14169, 3256, 11], "temperature": 0.0, "avg_logprob": -0.16708606222401495, "compression_ratio": 1.5034013605442176, "no_speech_prob": 8.315247441714746e-07}, {"id": 965, "seek": 520920, "start": 5221.679999999999, "end": 5229.12, "text": " or at least an approximation of it.", "tokens": [420, 412, 1935, 364, 28023, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.16708606222401495, "compression_ratio": 1.5034013605442176, "no_speech_prob": 8.315247441714746e-07}, {"id": 966, "seek": 520920, "start": 5229.12, "end": 5230.12, "text": " So that's the U-net.", "tokens": [407, 300, 311, 264, 624, 12, 7129, 13], "temperature": 0.0, "avg_logprob": -0.16708606222401495, "compression_ratio": 1.5034013605442176, "no_speech_prob": 8.315247441714746e-07}, {"id": 967, "seek": 520920, "start": 5230.12, "end": 5238.12, "text": " Now here's the problem, well here's our problem.", "tokens": [823, 510, 311, 264, 1154, 11, 731, 510, 311, 527, 1154, 13], "temperature": 0.0, "avg_logprob": -0.16708606222401495, "compression_ratio": 1.5034013605442176, "no_speech_prob": 8.315247441714746e-07}, {"id": 968, "seek": 523812, "start": 5238.12, "end": 5248.44, "text": " We have, oh why do I keep forgetting this, we have 28 times 28, 784, I should write that", "tokens": [492, 362, 11, 1954, 983, 360, 286, 1066, 25428, 341, 11, 321, 362, 7562, 1413, 7562, 11, 1614, 25494, 11, 286, 820, 2464, 300], "temperature": 0.0, "avg_logprob": -0.21091433933803014, "compression_ratio": 1.377245508982036, "no_speech_prob": 8.013453225430567e-06}, {"id": 969, "seek": 523812, "start": 5248.44, "end": 5249.68, "text": " down.", "tokens": [760, 13], "temperature": 0.0, "avg_logprob": -0.21091433933803014, "compression_ratio": 1.377245508982036, "no_speech_prob": 8.013453225430567e-06}, {"id": 970, "seek": 523812, "start": 5249.68, "end": 5258.96, "text": " We have in these things 784 pixels.", "tokens": [492, 362, 294, 613, 721, 1614, 25494, 18668, 13], "temperature": 0.0, "avg_logprob": -0.21091433933803014, "compression_ratio": 1.377245508982036, "no_speech_prob": 8.013453225430567e-06}, {"id": 971, "seek": 523812, "start": 5258.96, "end": 5260.24, "text": " That's quite a lot.", "tokens": [663, 311, 1596, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.21091433933803014, "compression_ratio": 1.377245508982036, "no_speech_prob": 8.013453225430567e-06}, {"id": 972, "seek": 523812, "start": 5260.24, "end": 5263.5199999999995, "text": " And it gets worse because in practice we don't want to draw handwritten digits.", "tokens": [400, 309, 2170, 5324, 570, 294, 3124, 321, 500, 380, 528, 281, 2642, 1011, 26859, 27011, 13], "temperature": 0.0, "avg_logprob": -0.21091433933803014, "compression_ratio": 1.377245508982036, "no_speech_prob": 8.013453225430567e-06}, {"id": 973, "seek": 526352, "start": 5263.52, "end": 5269.360000000001, "text": " The thing that we'd be passing in here is beautiful high definition photos or images", "tokens": [440, 551, 300, 321, 1116, 312, 8437, 294, 510, 307, 2238, 1090, 7123, 5787, 420, 5267], "temperature": 0.0, "avg_logprob": -0.2200696266303628, "compression_ratio": 1.37012987012987, "no_speech_prob": 4.495155735639855e-06}, {"id": 974, "seek": 526352, "start": 5269.360000000001, "end": 5272.120000000001, "text": " of like great paintings.", "tokens": [295, 411, 869, 14880, 13], "temperature": 0.0, "avg_logprob": -0.2200696266303628, "compression_ratio": 1.37012987012987, "no_speech_prob": 4.495155735639855e-06}, {"id": 975, "seek": 526352, "start": 5272.120000000001, "end": 5288.76, "text": " And at the moment the thing we tend to use for that is a 512 by 512 by 3 channel RGB.", "tokens": [400, 412, 264, 1623, 264, 551, 321, 3928, 281, 764, 337, 300, 307, 257, 1025, 4762, 538, 1025, 4762, 538, 805, 2269, 31231, 13], "temperature": 0.0, "avg_logprob": -0.2200696266303628, "compression_ratio": 1.37012987012987, "no_speech_prob": 4.495155735639855e-06}, {"id": 976, "seek": 526352, "start": 5288.76, "end": 5290.56, "text": " Nice big image.", "tokens": [5490, 955, 3256, 13], "temperature": 0.0, "avg_logprob": -0.2200696266303628, "compression_ratio": 1.37012987012987, "no_speech_prob": 4.495155735639855e-06}, {"id": 977, "seek": 529056, "start": 5290.56, "end": 5299.240000000001, "text": " 512 by 512 by 3.", "tokens": [1025, 4762, 538, 1025, 4762, 538, 805, 13], "temperature": 0.0, "avg_logprob": -0.2004147906636083, "compression_ratio": 1.123456790123457, "no_speech_prob": 4.289314802008448e-06}, {"id": 978, "seek": 529056, "start": 5299.240000000001, "end": 5301.400000000001, "text": " Red green blue.", "tokens": [4477, 3092, 3344, 13], "temperature": 0.0, "avg_logprob": -0.2004147906636083, "compression_ratio": 1.123456790123457, "no_speech_prob": 4.289314802008448e-06}, {"id": 979, "seek": 529056, "start": 5301.400000000001, "end": 5302.56, "text": " These are the pixels.", "tokens": [1981, 366, 264, 18668, 13], "temperature": 0.0, "avg_logprob": -0.2004147906636083, "compression_ratio": 1.123456790123457, "no_speech_prob": 4.289314802008448e-06}, {"id": 980, "seek": 529056, "start": 5302.56, "end": 5312.76, "text": " So that is 512 by 512 by 3, 786 432.", "tokens": [407, 300, 307, 1025, 4762, 538, 1025, 4762, 538, 805, 11, 1614, 22193, 1017, 11440, 13], "temperature": 0.0, "avg_logprob": -0.2004147906636083, "compression_ratio": 1.123456790123457, "no_speech_prob": 4.289314802008448e-06}, {"id": 981, "seek": 531276, "start": 5312.76, "end": 5320.6, "text": " So we've got 786 432 pixels in here.", "tokens": [407, 321, 600, 658, 1614, 22193, 1017, 11440, 18668, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.15254569053649902, "compression_ratio": 1.4195402298850575, "no_speech_prob": 2.090447651426075e-06}, {"id": 982, "seek": 531276, "start": 5320.6, "end": 5324.88, "text": " And so this is, I don't know, some beautiful picture.", "tokens": [400, 370, 341, 307, 11, 286, 500, 380, 458, 11, 512, 2238, 3036, 13], "temperature": 0.0, "avg_logprob": -0.15254569053649902, "compression_ratio": 1.4195402298850575, "no_speech_prob": 2.090447651426075e-06}, {"id": 983, "seek": 531276, "start": 5324.88, "end": 5330.2, "text": " This is my amazing portrait Van Gogh style in a dainty little hat.", "tokens": [639, 307, 452, 2243, 17126, 8979, 39690, 71, 3758, 294, 257, 274, 491, 874, 707, 2385, 13], "temperature": 0.0, "avg_logprob": -0.15254569053649902, "compression_ratio": 1.4195402298850575, "no_speech_prob": 2.090447651426075e-06}, {"id": 984, "seek": 531276, "start": 5330.2, "end": 5331.42, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.15254569053649902, "compression_ratio": 1.4195402298850575, "no_speech_prob": 2.090447651426075e-06}, {"id": 985, "seek": 531276, "start": 5331.42, "end": 5337.34, "text": " So this is the beautiful painting or an image of it.", "tokens": [407, 341, 307, 264, 2238, 5370, 420, 364, 3256, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.15254569053649902, "compression_ratio": 1.4195402298850575, "no_speech_prob": 2.090447651426075e-06}, {"id": 986, "seek": 531276, "start": 5337.34, "end": 5339.12, "text": " That's a lot of pixels.", "tokens": [663, 311, 257, 688, 295, 18668, 13], "temperature": 0.0, "avg_logprob": -0.15254569053649902, "compression_ratio": 1.4195402298850575, "no_speech_prob": 2.090447651426075e-06}, {"id": 987, "seek": 533912, "start": 5339.12, "end": 5348.5199999999995, "text": " And so training this model where we put noisy versions of millions of these beautiful images", "tokens": [400, 370, 3097, 341, 2316, 689, 321, 829, 24518, 9606, 295, 6803, 295, 613, 2238, 5267], "temperature": 0.0, "avg_logprob": -0.12963613622328815, "compression_ratio": 1.5045871559633028, "no_speech_prob": 3.1380477594211698e-06}, {"id": 988, "seek": 533912, "start": 5348.5199999999995, "end": 5351.76, "text": " is going to take us an awful long time.", "tokens": [307, 516, 281, 747, 505, 364, 11232, 938, 565, 13], "temperature": 0.0, "avg_logprob": -0.12963613622328815, "compression_ratio": 1.5045871559633028, "no_speech_prob": 3.1380477594211698e-06}, {"id": 989, "seek": 533912, "start": 5351.76, "end": 5358.4, "text": " And if you're Google with a huge cloud of TPUs or something, maybe that's okay.", "tokens": [400, 498, 291, 434, 3329, 365, 257, 2603, 4588, 295, 314, 8115, 82, 420, 746, 11, 1310, 300, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.12963613622328815, "compression_ratio": 1.5045871559633028, "no_speech_prob": 3.1380477594211698e-06}, {"id": 990, "seek": 533912, "start": 5358.4, "end": 5365.34, "text": " But for the rest of us, we would like to do this as efficiently as possible.", "tokens": [583, 337, 264, 1472, 295, 505, 11, 321, 576, 411, 281, 360, 341, 382, 19621, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.12963613622328815, "compression_ratio": 1.5045871559633028, "no_speech_prob": 3.1380477594211698e-06}, {"id": 991, "seek": 533912, "start": 5365.34, "end": 5367.36, "text": " How could we do this more efficiently?", "tokens": [1012, 727, 321, 360, 341, 544, 19621, 30], "temperature": 0.0, "avg_logprob": -0.12963613622328815, "compression_ratio": 1.5045871559633028, "no_speech_prob": 3.1380477594211698e-06}, {"id": 992, "seek": 536736, "start": 5367.36, "end": 5377.5599999999995, "text": " Well, when you think about it, in this beautiful picture I drew, storing the exact pixel value", "tokens": [1042, 11, 562, 291, 519, 466, 309, 11, 294, 341, 2238, 3036, 286, 12804, 11, 26085, 264, 1900, 19261, 2158], "temperature": 0.0, "avg_logprob": -0.15460675860208178, "compression_ratio": 1.4195402298850575, "no_speech_prob": 4.289298431103816e-06}, {"id": 993, "seek": 536736, "start": 5377.5599999999995, "end": 5385.92, "text": " of every single pixel is probably not the most efficient way to store it.", "tokens": [295, 633, 2167, 19261, 307, 1391, 406, 264, 881, 7148, 636, 281, 3531, 309, 13], "temperature": 0.0, "avg_logprob": -0.15460675860208178, "compression_ratio": 1.4195402298850575, "no_speech_prob": 4.289298431103816e-06}, {"id": 994, "seek": 536736, "start": 5385.92, "end": 5395.04, "text": " What if instead we said, oh, let's say this is like green rushes or something.", "tokens": [708, 498, 2602, 321, 848, 11, 1954, 11, 718, 311, 584, 341, 307, 411, 3092, 9300, 279, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.15460675860208178, "compression_ratio": 1.4195402298850575, "no_speech_prob": 4.289298431103816e-06}, {"id": 995, "seek": 539504, "start": 5395.04, "end": 5399.04, "text": " It might say like, oh, over here is green and everything kind of underneath, it's pretty", "tokens": [467, 1062, 584, 411, 11, 1954, 11, 670, 510, 307, 3092, 293, 1203, 733, 295, 7223, 11, 309, 311, 1238], "temperature": 0.0, "avg_logprob": -0.13045467029918323, "compression_ratio": 1.6026200873362446, "no_speech_prob": 2.9944135349069256e-06}, {"id": 996, "seek": 539504, "start": 5399.04, "end": 5400.04, "text": " much the same.", "tokens": [709, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.13045467029918323, "compression_ratio": 1.6026200873362446, "no_speech_prob": 2.9944135349069256e-06}, {"id": 997, "seek": 539504, "start": 5400.04, "end": 5405.16, "text": " Or, you know, maybe I'm wearing a blue top in this beautiful portrait and it could kind", "tokens": [1610, 11, 291, 458, 11, 1310, 286, 478, 4769, 257, 3344, 1192, 294, 341, 2238, 17126, 293, 309, 727, 733], "temperature": 0.0, "avg_logprob": -0.13045467029918323, "compression_ratio": 1.6026200873362446, "no_speech_prob": 2.9944135349069256e-06}, {"id": 998, "seek": 539504, "start": 5405.16, "end": 5408.04, "text": " of say like, oh, all the pixels in here are blue.", "tokens": [295, 584, 411, 11, 1954, 11, 439, 264, 18668, 294, 510, 366, 3344, 13], "temperature": 0.0, "avg_logprob": -0.13045467029918323, "compression_ratio": 1.6026200873362446, "no_speech_prob": 2.9944135349069256e-06}, {"id": 999, "seek": 539504, "start": 5408.04, "end": 5409.76, "text": " You know, you don't really have to do everyone individually.", "tokens": [509, 458, 11, 291, 500, 380, 534, 362, 281, 360, 1518, 16652, 13], "temperature": 0.0, "avg_logprob": -0.13045467029918323, "compression_ratio": 1.6026200873362446, "no_speech_prob": 2.9944135349069256e-06}, {"id": 1000, "seek": 539504, "start": 5409.76, "end": 5419.28, "text": " There are faster, more concise ways of storing what an image is.", "tokens": [821, 366, 4663, 11, 544, 44882, 2098, 295, 26085, 437, 364, 3256, 307, 13], "temperature": 0.0, "avg_logprob": -0.13045467029918323, "compression_ratio": 1.6026200873362446, "no_speech_prob": 2.9944135349069256e-06}, {"id": 1001, "seek": 541928, "start": 5419.28, "end": 5426.88, "text": " We know this is true because, for example, a JPEG picture is far fewer bytes than the", "tokens": [492, 458, 341, 307, 2074, 570, 11, 337, 1365, 11, 257, 508, 5208, 38, 3036, 307, 1400, 13366, 36088, 813, 264], "temperature": 0.0, "avg_logprob": -0.08666795573822439, "compression_ratio": 1.5698924731182795, "no_speech_prob": 1.8162119204134797e-06}, {"id": 1002, "seek": 541928, "start": 5426.88, "end": 5431.84, "text": " number of bytes you would get if you multiplied its height by its width by its channels.", "tokens": [1230, 295, 36088, 291, 576, 483, 498, 291, 17207, 1080, 6681, 538, 1080, 11402, 538, 1080, 9235, 13], "temperature": 0.0, "avg_logprob": -0.08666795573822439, "compression_ratio": 1.5698924731182795, "no_speech_prob": 1.8162119204134797e-06}, {"id": 1003, "seek": 541928, "start": 5431.84, "end": 5440.32, "text": " So we know that it's possible to compress pictures.", "tokens": [407, 321, 458, 300, 309, 311, 1944, 281, 14778, 5242, 13], "temperature": 0.0, "avg_logprob": -0.08666795573822439, "compression_ratio": 1.5698924731182795, "no_speech_prob": 1.8162119204134797e-06}, {"id": 1004, "seek": 541928, "start": 5440.32, "end": 5445.0, "text": " So let me show you a really interesting way to compress pictures.", "tokens": [407, 718, 385, 855, 291, 257, 534, 1880, 636, 281, 14778, 5242, 13], "temperature": 0.0, "avg_logprob": -0.08666795573822439, "compression_ratio": 1.5698924731182795, "no_speech_prob": 1.8162119204134797e-06}, {"id": 1005, "seek": 544500, "start": 5445.0, "end": 5451.88, "text": " Let's take this image and let's put it through a convolutional layer of stride2.", "tokens": [961, 311, 747, 341, 3256, 293, 718, 311, 829, 309, 807, 257, 45216, 304, 4583, 295, 1056, 482, 17, 13], "temperature": 0.0, "avg_logprob": -0.20531584999778055, "compression_ratio": 1.6023391812865497, "no_speech_prob": 4.0294444261235185e-06}, {"id": 1006, "seek": 544500, "start": 5451.88, "end": 5459.36, "text": " Now if we put it through a convolutional layer of stride2 with six features, with six channels,", "tokens": [823, 498, 321, 829, 309, 807, 257, 45216, 304, 4583, 295, 1056, 482, 17, 365, 2309, 4122, 11, 365, 2309, 9235, 11], "temperature": 0.0, "avg_logprob": -0.20531584999778055, "compression_ratio": 1.6023391812865497, "no_speech_prob": 4.0294444261235185e-06}, {"id": 1007, "seek": 544500, "start": 5459.36, "end": 5462.92, "text": " we would get back a 256 by 256.", "tokens": [321, 576, 483, 646, 257, 38882, 538, 38882, 13], "temperature": 0.0, "avg_logprob": -0.20531584999778055, "compression_ratio": 1.6023391812865497, "no_speech_prob": 4.0294444261235185e-06}, {"id": 1008, "seek": 544500, "start": 5462.92, "end": 5471.16, "text": " Gosh, that was a terrible attempt at drawing a square, wasn't it?", "tokens": [19185, 11, 300, 390, 257, 6237, 5217, 412, 6316, 257, 3732, 11, 2067, 380, 309, 30], "temperature": 0.0, "avg_logprob": -0.20531584999778055, "compression_ratio": 1.6023391812865497, "no_speech_prob": 4.0294444261235185e-06}, {"id": 1009, "seek": 547116, "start": 5471.16, "end": 5477.16, "text": " By 256, actually do it here.", "tokens": [3146, 38882, 11, 767, 360, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.2032687575728805, "compression_ratio": 1.5965665236051503, "no_speech_prob": 8.013448677957058e-06}, {"id": 1010, "seek": 547116, "start": 5477.16, "end": 5481.84, "text": " By, okay, let's double the number of channels to six.", "tokens": [3146, 11, 1392, 11, 718, 311, 3834, 264, 1230, 295, 9235, 281, 2309, 13], "temperature": 0.0, "avg_logprob": -0.2032687575728805, "compression_ratio": 1.5965665236051503, "no_speech_prob": 8.013448677957058e-06}, {"id": 1011, "seek": 547116, "start": 5481.84, "end": 5483.96, "text": " By six.", "tokens": [3146, 2309, 13], "temperature": 0.0, "avg_logprob": -0.2032687575728805, "compression_ratio": 1.5965665236051503, "no_speech_prob": 8.013448677957058e-06}, {"id": 1012, "seek": 547116, "start": 5483.96, "end": 5486.44, "text": " And then let's put it through another stride2 convolution.", "tokens": [400, 550, 718, 311, 829, 309, 807, 1071, 1056, 482, 17, 45216, 13], "temperature": 0.0, "avg_logprob": -0.2032687575728805, "compression_ratio": 1.5965665236051503, "no_speech_prob": 8.013448677957058e-06}, {"id": 1013, "seek": 547116, "start": 5486.44, "end": 5490.24, "text": " And remember, we're going to be seeing exactly how to do all these things and building them", "tokens": [400, 1604, 11, 321, 434, 516, 281, 312, 2577, 2293, 577, 281, 360, 439, 613, 721, 293, 2390, 552], "temperature": 0.0, "avg_logprob": -0.2032687575728805, "compression_ratio": 1.5965665236051503, "no_speech_prob": 8.013448677957058e-06}, {"id": 1014, "seek": 547116, "start": 5490.24, "end": 5491.24, "text": " all from scratch.", "tokens": [439, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.2032687575728805, "compression_ratio": 1.5965665236051503, "no_speech_prob": 8.013448677957058e-06}, {"id": 1015, "seek": 547116, "start": 5491.24, "end": 5496.36, "text": " So don't worry if you're not sure what a stride2 convolution exactly is.", "tokens": [407, 500, 380, 3292, 498, 291, 434, 406, 988, 437, 257, 1056, 482, 17, 45216, 2293, 307, 13], "temperature": 0.0, "avg_logprob": -0.2032687575728805, "compression_ratio": 1.5965665236051503, "no_speech_prob": 8.013448677957058e-06}, {"id": 1016, "seek": 547116, "start": 5496.36, "end": 5499.4, "text": " And just do it again to get 128 by 128.", "tokens": [400, 445, 360, 309, 797, 281, 483, 29810, 538, 29810, 13], "temperature": 0.0, "avg_logprob": -0.2032687575728805, "compression_ratio": 1.5965665236051503, "no_speech_prob": 8.013448677957058e-06}, {"id": 1017, "seek": 549940, "start": 5499.4, "end": 5502.92, "text": " And again, let's double the number of channels.", "tokens": [400, 797, 11, 718, 311, 3834, 264, 1230, 295, 9235, 13], "temperature": 0.0, "avg_logprob": -0.1350278975088385, "compression_ratio": 1.4838709677419355, "no_speech_prob": 2.7693999982147943e-06}, {"id": 1018, "seek": 549940, "start": 5502.92, "end": 5506.96, "text": " And then let's do it again, another stride2 convolution.", "tokens": [400, 550, 718, 311, 360, 309, 797, 11, 1071, 1056, 482, 17, 45216, 13], "temperature": 0.0, "avg_logprob": -0.1350278975088385, "compression_ratio": 1.4838709677419355, "no_speech_prob": 2.7693999982147943e-06}, {"id": 1019, "seek": 549940, "start": 5506.96, "end": 5512.28, "text": " So we're just building a neural network here.", "tokens": [407, 321, 434, 445, 2390, 257, 18161, 3209, 510, 13], "temperature": 0.0, "avg_logprob": -0.1350278975088385, "compression_ratio": 1.4838709677419355, "no_speech_prob": 2.7693999982147943e-06}, {"id": 1020, "seek": 549940, "start": 5512.28, "end": 5521.799999999999, "text": " So now we're down to 64 by 64 by 24.", "tokens": [407, 586, 321, 434, 760, 281, 12145, 538, 12145, 538, 4022, 13], "temperature": 0.0, "avg_logprob": -0.1350278975088385, "compression_ratio": 1.4838709677419355, "no_speech_prob": 2.7693999982147943e-06}, {"id": 1021, "seek": 549940, "start": 5521.799999999999, "end": 5527.4, "text": " Okay and then now let's put that through a few like resnet blocks to kind of squish down", "tokens": [1033, 293, 550, 586, 718, 311, 829, 300, 807, 257, 1326, 411, 725, 7129, 8474, 281, 733, 295, 31379, 760], "temperature": 0.0, "avg_logprob": -0.1350278975088385, "compression_ratio": 1.4838709677419355, "no_speech_prob": 2.7693999982147943e-06}, {"id": 1022, "seek": 552740, "start": 5527.4, "end": 5531.0, "text": " the number of channels as much as we can.", "tokens": [264, 1230, 295, 9235, 382, 709, 382, 321, 393, 13], "temperature": 0.0, "avg_logprob": -0.18698280507867987, "compression_ratio": 1.3716216216216217, "no_speech_prob": 5.594321009994019e-06}, {"id": 1023, "seek": 552740, "start": 5531.0, "end": 5536.679999999999, "text": " So it'll be now down to let's say 64 by 64 by 4.", "tokens": [407, 309, 603, 312, 586, 760, 281, 718, 311, 584, 12145, 538, 12145, 538, 1017, 13], "temperature": 0.0, "avg_logprob": -0.18698280507867987, "compression_ratio": 1.3716216216216217, "no_speech_prob": 5.594321009994019e-06}, {"id": 1024, "seek": 552740, "start": 5536.679999999999, "end": 5542.759999999999, "text": " Okay, so here's a neural network.", "tokens": [1033, 11, 370, 510, 311, 257, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.18698280507867987, "compression_ratio": 1.3716216216216217, "no_speech_prob": 5.594321009994019e-06}, {"id": 1025, "seek": 552740, "start": 5542.759999999999, "end": 5553.839999999999, "text": " And so the number of pixels in this version is now 64 times 64 times 4, 16384.", "tokens": [400, 370, 264, 1230, 295, 18668, 294, 341, 3037, 307, 586, 12145, 1413, 12145, 1413, 1017, 11, 3165, 12625, 19, 13], "temperature": 0.0, "avg_logprob": -0.18698280507867987, "compression_ratio": 1.3716216216216217, "no_speech_prob": 5.594321009994019e-06}, {"id": 1026, "seek": 555384, "start": 5553.84, "end": 5559.56, "text": " So there's 16384 pixels here.", "tokens": [407, 456, 311, 3165, 12625, 19, 18668, 510, 13], "temperature": 0.0, "avg_logprob": -0.18036928176879882, "compression_ratio": 1.087378640776699, "no_speech_prob": 4.49516346634482e-06}, {"id": 1027, "seek": 555384, "start": 5559.56, "end": 5579.76, "text": " Okay, so we've compressed it from 786, 432 to 16384, which is a 48 times decrease.", "tokens": [1033, 11, 370, 321, 600, 30353, 309, 490, 1614, 22193, 11, 1017, 11440, 281, 3165, 12625, 19, 11, 597, 307, 257, 11174, 1413, 11514, 13], "temperature": 0.0, "avg_logprob": -0.18036928176879882, "compression_ratio": 1.087378640776699, "no_speech_prob": 4.49516346634482e-06}, {"id": 1028, "seek": 557976, "start": 5579.76, "end": 5584.06, "text": " Now that's no use if we've lost our image.", "tokens": [823, 300, 311, 572, 764, 498, 321, 600, 2731, 527, 3256, 13], "temperature": 0.0, "avg_logprob": -0.15101149678230286, "compression_ratio": 1.3782051282051282, "no_speech_prob": 3.4465674616512842e-06}, {"id": 1029, "seek": 557976, "start": 5584.06, "end": 5586.96, "text": " So can we get the image back again?", "tokens": [407, 393, 321, 483, 264, 3256, 646, 797, 30], "temperature": 0.0, "avg_logprob": -0.15101149678230286, "compression_ratio": 1.3782051282051282, "no_speech_prob": 3.4465674616512842e-06}, {"id": 1030, "seek": 557976, "start": 5586.96, "end": 5591.6, "text": " Sure, why not?", "tokens": [4894, 11, 983, 406, 30], "temperature": 0.0, "avg_logprob": -0.15101149678230286, "compression_ratio": 1.3782051282051282, "no_speech_prob": 3.4465674616512842e-06}, {"id": 1031, "seek": 557976, "start": 5591.6, "end": 5602.3, "text": " What if we now create a kind of an inverse convolution which does the exact opposite?", "tokens": [708, 498, 321, 586, 1884, 257, 733, 295, 364, 17340, 45216, 597, 775, 264, 1900, 6182, 30], "temperature": 0.0, "avg_logprob": -0.15101149678230286, "compression_ratio": 1.3782051282051282, "no_speech_prob": 3.4465674616512842e-06}, {"id": 1032, "seek": 557976, "start": 5602.3, "end": 5608.76, "text": " So actually let's put it over here.", "tokens": [407, 767, 718, 311, 829, 309, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.15101149678230286, "compression_ratio": 1.3782051282051282, "no_speech_prob": 3.4465674616512842e-06}, {"id": 1033, "seek": 560876, "start": 5608.76, "end": 5619.08, "text": " So we're going to take our 64 by 64 by 4 image, put it through an inverse convolution.", "tokens": [407, 321, 434, 516, 281, 747, 527, 12145, 538, 12145, 538, 1017, 3256, 11, 829, 309, 807, 364, 17340, 45216, 13], "temperature": 0.0, "avg_logprob": -0.10581444530952268, "compression_ratio": 1.2612612612612613, "no_speech_prob": 3.393133738427423e-06}, {"id": 1034, "seek": 560876, "start": 5619.08, "end": 5632.18, "text": " So let's put it, let's keep moving this over further.", "tokens": [407, 718, 311, 829, 309, 11, 718, 311, 1066, 2684, 341, 670, 3052, 13], "temperature": 0.0, "avg_logprob": -0.10581444530952268, "compression_ratio": 1.2612612612612613, "no_speech_prob": 3.393133738427423e-06}, {"id": 1035, "seek": 563218, "start": 5632.18, "end": 5643.900000000001, "text": " Back to 128 by 128 by 12 and put it through another inverse convolution.", "tokens": [5833, 281, 29810, 538, 29810, 538, 2272, 293, 829, 309, 807, 1071, 17340, 45216, 13], "temperature": 0.0, "avg_logprob": -0.2081980932326544, "compression_ratio": 1.2764227642276422, "no_speech_prob": 1.2878934967375244e-06}, {"id": 1036, "seek": 563218, "start": 5643.900000000001, "end": 5648.16, "text": " So these are all just basically they're just neural network layers.", "tokens": [407, 613, 366, 439, 445, 1936, 436, 434, 445, 18161, 3209, 7914, 13], "temperature": 0.0, "avg_logprob": -0.2081980932326544, "compression_ratio": 1.2764227642276422, "no_speech_prob": 1.2878934967375244e-06}, {"id": 1037, "seek": 563218, "start": 5648.16, "end": 5657.68, "text": " 256 by 256 by 6.", "tokens": [38882, 538, 38882, 538, 1386, 13], "temperature": 0.0, "avg_logprob": -0.2081980932326544, "compression_ratio": 1.2764227642276422, "no_speech_prob": 1.2878934967375244e-06}, {"id": 1038, "seek": 565768, "start": 5657.68, "end": 5681.76, "text": " And then finally, wrap you out, all the way back to 512 by 512 by 3.", "tokens": [400, 550, 2721, 11, 7019, 291, 484, 11, 439, 264, 636, 646, 281, 1025, 4762, 538, 1025, 4762, 538, 805, 13], "temperature": 0.0, "avg_logprob": -0.19337289810180663, "compression_ratio": 1.0303030303030303, "no_speech_prob": 3.3931244161067298e-06}, {"id": 1039, "seek": 568176, "start": 5681.76, "end": 5688.12, "text": " Okay, we could put this whole thing inside a neural net.", "tokens": [1033, 11, 321, 727, 829, 341, 1379, 551, 1854, 257, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.15082156139871347, "compression_ratio": 1.532544378698225, "no_speech_prob": 2.948000883407076e-06}, {"id": 1040, "seek": 568176, "start": 5688.12, "end": 5694.96, "text": " Here's our single neural network.", "tokens": [1692, 311, 527, 2167, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.15082156139871347, "compression_ratio": 1.532544378698225, "no_speech_prob": 2.948000883407076e-06}, {"id": 1041, "seek": 568176, "start": 5694.96, "end": 5699.56, "text": " And what we could do is we can start feeding it images.", "tokens": [400, 437, 321, 727, 360, 307, 321, 393, 722, 12919, 309, 5267, 13], "temperature": 0.0, "avg_logprob": -0.15082156139871347, "compression_ratio": 1.532544378698225, "no_speech_prob": 2.948000883407076e-06}, {"id": 1042, "seek": 568176, "start": 5699.56, "end": 5704.4400000000005, "text": " It goes all the way through this neural network and out of the other end comes back, well", "tokens": [467, 1709, 439, 264, 636, 807, 341, 18161, 3209, 293, 484, 295, 264, 661, 917, 1487, 646, 11, 731], "temperature": 0.0, "avg_logprob": -0.15082156139871347, "compression_ratio": 1.532544378698225, "no_speech_prob": 2.948000883407076e-06}, {"id": 1043, "seek": 568176, "start": 5704.4400000000005, "end": 5707.88, "text": " initially it's random.", "tokens": [9105, 309, 311, 4974, 13], "temperature": 0.0, "avg_logprob": -0.15082156139871347, "compression_ratio": 1.532544378698225, "no_speech_prob": 2.948000883407076e-06}, {"id": 1044, "seek": 570788, "start": 5707.88, "end": 5712.16, "text": " So initially comes out of this is random noise.", "tokens": [407, 9105, 1487, 484, 295, 341, 307, 4974, 5658, 13], "temperature": 0.0, "avg_logprob": -0.14548800541804388, "compression_ratio": 1.5810055865921788, "no_speech_prob": 7.002159918556572e-07}, {"id": 1045, "seek": 570788, "start": 5712.16, "end": 5719.22, "text": " 512 by 512, because I draw it inside here.", "tokens": [1025, 4762, 538, 1025, 4762, 11, 570, 286, 2642, 309, 1854, 510, 13], "temperature": 0.0, "avg_logprob": -0.14548800541804388, "compression_ratio": 1.5810055865921788, "no_speech_prob": 7.002159918556572e-07}, {"id": 1046, "seek": 570788, "start": 5719.22, "end": 5723.88, "text": " So inside here initially it's going to give us random noise.", "tokens": [407, 1854, 510, 9105, 309, 311, 516, 281, 976, 505, 4974, 5658, 13], "temperature": 0.0, "avg_logprob": -0.14548800541804388, "compression_ratio": 1.5810055865921788, "no_speech_prob": 7.002159918556572e-07}, {"id": 1047, "seek": 570788, "start": 5723.88, "end": 5727.08, "text": " And so now we need a loss function, right?", "tokens": [400, 370, 586, 321, 643, 257, 4470, 2445, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14548800541804388, "compression_ratio": 1.5810055865921788, "no_speech_prob": 7.002159918556572e-07}, {"id": 1048, "seek": 570788, "start": 5727.08, "end": 5737.76, "text": " So the loss function we can create could be to say let's take this output and this input", "tokens": [407, 264, 4470, 2445, 321, 393, 1884, 727, 312, 281, 584, 718, 311, 747, 341, 5598, 293, 341, 4846], "temperature": 0.0, "avg_logprob": -0.14548800541804388, "compression_ratio": 1.5810055865921788, "no_speech_prob": 7.002159918556572e-07}, {"id": 1049, "seek": 573776, "start": 5737.76, "end": 5746.16, "text": " and compare them and create and do an MSE, mean squared error, directly on those two", "tokens": [293, 6794, 552, 293, 1884, 293, 360, 364, 376, 5879, 11, 914, 8889, 6713, 11, 3838, 322, 729, 732], "temperature": 0.0, "avg_logprob": -0.10290233550533172, "compression_ratio": 1.490066225165563, "no_speech_prob": 2.6016107312898384e-06}, {"id": 1050, "seek": 573776, "start": 5746.16, "end": 5748.56, "text": " pieces.", "tokens": [3755, 13], "temperature": 0.0, "avg_logprob": -0.10290233550533172, "compression_ratio": 1.490066225165563, "no_speech_prob": 2.6016107312898384e-06}, {"id": 1051, "seek": 573776, "start": 5748.56, "end": 5752.88, "text": " So what would that do if we train this model?", "tokens": [407, 437, 576, 300, 360, 498, 321, 3847, 341, 2316, 30], "temperature": 0.0, "avg_logprob": -0.10290233550533172, "compression_ratio": 1.490066225165563, "no_speech_prob": 2.6016107312898384e-06}, {"id": 1052, "seek": 573776, "start": 5752.88, "end": 5759.4800000000005, "text": " This model is going to try to put an image through and going to try to make it so that", "tokens": [639, 2316, 307, 516, 281, 853, 281, 829, 364, 3256, 807, 293, 516, 281, 853, 281, 652, 309, 370, 300], "temperature": 0.0, "avg_logprob": -0.10290233550533172, "compression_ratio": 1.490066225165563, "no_speech_prob": 2.6016107312898384e-06}, {"id": 1053, "seek": 575948, "start": 5759.48, "end": 5775.2, "text": " what comes out the other end is the exact same thing that went into it.", "tokens": [437, 1487, 484, 264, 661, 917, 307, 264, 1900, 912, 551, 300, 1437, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.1149137534347235, "compression_ratio": 1.3795620437956204, "no_speech_prob": 1.1365599448254216e-06}, {"id": 1054, "seek": 575948, "start": 5775.2, "end": 5777.4, "text": " That's what it's going to try to do.", "tokens": [663, 311, 437, 309, 311, 516, 281, 853, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1149137534347235, "compression_ratio": 1.3795620437956204, "no_speech_prob": 1.1365599448254216e-06}, {"id": 1055, "seek": 575948, "start": 5777.4, "end": 5786.379999999999, "text": " Because if it does that successfully, then the mean squared error would be zero.", "tokens": [1436, 498, 309, 775, 300, 10727, 11, 550, 264, 914, 8889, 6713, 576, 312, 4018, 13], "temperature": 0.0, "avg_logprob": -0.1149137534347235, "compression_ratio": 1.3795620437956204, "no_speech_prob": 1.1365599448254216e-06}, {"id": 1056, "seek": 578638, "start": 5786.38, "end": 5789.52, "text": " So I see some people in the chat saying that this is a UNET.", "tokens": [407, 286, 536, 512, 561, 294, 264, 5081, 1566, 300, 341, 307, 257, 8229, 4850, 13], "temperature": 0.0, "avg_logprob": -0.14650816075942097, "compression_ratio": 1.6743119266055047, "no_speech_prob": 2.225267508038087e-06}, {"id": 1057, "seek": 578638, "start": 5789.52, "end": 5790.52, "text": " This is not a UNET.", "tokens": [639, 307, 406, 257, 8229, 4850, 13], "temperature": 0.0, "avg_logprob": -0.14650816075942097, "compression_ratio": 1.6743119266055047, "no_speech_prob": 2.225267508038087e-06}, {"id": 1058, "seek": 578638, "start": 5790.52, "end": 5791.92, "text": " We'll get to that later.", "tokens": [492, 603, 483, 281, 300, 1780, 13], "temperature": 0.0, "avg_logprob": -0.14650816075942097, "compression_ratio": 1.6743119266055047, "no_speech_prob": 2.225267508038087e-06}, {"id": 1059, "seek": 578638, "start": 5791.92, "end": 5794.06, "text": " There's no cross connections.", "tokens": [821, 311, 572, 3278, 9271, 13], "temperature": 0.0, "avg_logprob": -0.14650816075942097, "compression_ratio": 1.6743119266055047, "no_speech_prob": 2.225267508038087e-06}, {"id": 1060, "seek": 578638, "start": 5794.06, "end": 5800.0, "text": " It's just a bunch of convolutions that decrease inside followed by a bunch of convolutions", "tokens": [467, 311, 445, 257, 3840, 295, 3754, 15892, 300, 11514, 1854, 6263, 538, 257, 3840, 295, 3754, 15892], "temperature": 0.0, "avg_logprob": -0.14650816075942097, "compression_ratio": 1.6743119266055047, "no_speech_prob": 2.225267508038087e-06}, {"id": 1061, "seek": 578638, "start": 5800.0, "end": 5803.8, "text": " that increase inside.", "tokens": [300, 3488, 1854, 13], "temperature": 0.0, "avg_logprob": -0.14650816075942097, "compression_ratio": 1.6743119266055047, "no_speech_prob": 2.225267508038087e-06}, {"id": 1062, "seek": 578638, "start": 5803.8, "end": 5812.76, "text": " And so we're going to try to train this model to spit out exactly what it received in.", "tokens": [400, 370, 321, 434, 516, 281, 853, 281, 3847, 341, 2316, 281, 22127, 484, 2293, 437, 309, 4613, 294, 13], "temperature": 0.0, "avg_logprob": -0.14650816075942097, "compression_ratio": 1.6743119266055047, "no_speech_prob": 2.225267508038087e-06}, {"id": 1063, "seek": 578638, "start": 5812.76, "end": 5815.16, "text": " And that seems really boring.", "tokens": [400, 300, 2544, 534, 9989, 13], "temperature": 0.0, "avg_logprob": -0.14650816075942097, "compression_ratio": 1.6743119266055047, "no_speech_prob": 2.225267508038087e-06}, {"id": 1064, "seek": 581516, "start": 5815.16, "end": 5823.72, "text": " What's the point of a model that only learns to give you back exactly what came in?", "tokens": [708, 311, 264, 935, 295, 257, 2316, 300, 787, 27152, 281, 976, 291, 646, 2293, 437, 1361, 294, 30], "temperature": 0.0, "avg_logprob": -0.08697419166564942, "compression_ratio": 1.5794871794871794, "no_speech_prob": 9.570811698722537e-07}, {"id": 1065, "seek": 581516, "start": 5823.72, "end": 5826.5199999999995, "text": " Well, this is actually extremely interesting.", "tokens": [1042, 11, 341, 307, 767, 4664, 1880, 13], "temperature": 0.0, "avg_logprob": -0.08697419166564942, "compression_ratio": 1.5794871794871794, "no_speech_prob": 9.570811698722537e-07}, {"id": 1066, "seek": 581516, "start": 5826.5199999999995, "end": 5831.4, "text": " This kind of model is called an autoencoder.", "tokens": [639, 733, 295, 2316, 307, 1219, 364, 8399, 22660, 19866, 13], "temperature": 0.0, "avg_logprob": -0.08697419166564942, "compression_ratio": 1.5794871794871794, "no_speech_prob": 9.570811698722537e-07}, {"id": 1067, "seek": 581516, "start": 5831.4, "end": 5835.8, "text": " It's something that gives you back what you gave it.", "tokens": [467, 311, 746, 300, 2709, 291, 646, 437, 291, 2729, 309, 13], "temperature": 0.0, "avg_logprob": -0.08697419166564942, "compression_ratio": 1.5794871794871794, "no_speech_prob": 9.570811698722537e-07}, {"id": 1068, "seek": 581516, "start": 5835.8, "end": 5844.32, "text": " And the reason an autoencoder is interesting is because we can split it in half.", "tokens": [400, 264, 1778, 364, 8399, 22660, 19866, 307, 1880, 307, 570, 321, 393, 7472, 309, 294, 1922, 13], "temperature": 0.0, "avg_logprob": -0.08697419166564942, "compression_ratio": 1.5794871794871794, "no_speech_prob": 9.570811698722537e-07}, {"id": 1069, "seek": 584432, "start": 5844.32, "end": 5847.759999999999, "text": " Let's grab just this bit.", "tokens": [961, 311, 4444, 445, 341, 857, 13], "temperature": 0.0, "avg_logprob": -0.18251011289399247, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.6028049003580236e-06}, {"id": 1070, "seek": 584432, "start": 5847.759999999999, "end": 5852.36, "text": " Okay, let's cut it up.", "tokens": [1033, 11, 718, 311, 1723, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.18251011289399247, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.6028049003580236e-06}, {"id": 1071, "seek": 584432, "start": 5852.36, "end": 5854.5, "text": " Let's grab just that bit.", "tokens": [961, 311, 4444, 445, 300, 857, 13], "temperature": 0.0, "avg_logprob": -0.18251011289399247, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.6028049003580236e-06}, {"id": 1072, "seek": 584432, "start": 5854.5, "end": 5855.96, "text": " And then we'll get a second half.", "tokens": [400, 550, 321, 603, 483, 257, 1150, 1922, 13], "temperature": 0.0, "avg_logprob": -0.18251011289399247, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.6028049003580236e-06}, {"id": 1073, "seek": 584432, "start": 5855.96, "end": 5861.92, "text": " Okay, they're not quite halves, but you know what I mean, which is just this bit.", "tokens": [1033, 11, 436, 434, 406, 1596, 38490, 11, 457, 291, 458, 437, 286, 914, 11, 597, 307, 445, 341, 857, 13], "temperature": 0.0, "avg_logprob": -0.18251011289399247, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.6028049003580236e-06}, {"id": 1074, "seek": 584432, "start": 5861.92, "end": 5869.4, "text": " And so let's say I take this image and I put it through just this first half, this green", "tokens": [400, 370, 718, 311, 584, 286, 747, 341, 3256, 293, 286, 829, 309, 807, 445, 341, 700, 1922, 11, 341, 3092], "temperature": 0.0, "avg_logprob": -0.18251011289399247, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.6028049003580236e-06}, {"id": 1075, "seek": 586940, "start": 5869.4, "end": 5874.48, "text": " half, which is called the encoder.", "tokens": [1922, 11, 597, 307, 1219, 264, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.16991917292277017, "compression_ratio": 1.328125, "no_speech_prob": 2.5215654204657767e-06}, {"id": 1076, "seek": 586940, "start": 5874.48, "end": 5884.839999999999, "text": " Okay, I can take this thing that comes out of it and I could save it.", "tokens": [1033, 11, 286, 393, 747, 341, 551, 300, 1487, 484, 295, 309, 293, 286, 727, 3155, 309, 13], "temperature": 0.0, "avg_logprob": -0.16991917292277017, "compression_ratio": 1.328125, "no_speech_prob": 2.5215654204657767e-06}, {"id": 1077, "seek": 586940, "start": 5884.839999999999, "end": 5892.0, "text": " And the thing that I'm going to save is going to be 16,384 bytes.", "tokens": [400, 264, 551, 300, 286, 478, 516, 281, 3155, 307, 516, 281, 312, 3165, 11, 12625, 19, 36088, 13], "temperature": 0.0, "avg_logprob": -0.16991917292277017, "compression_ratio": 1.328125, "no_speech_prob": 2.5215654204657767e-06}, {"id": 1078, "seek": 589200, "start": 5892.0, "end": 5902.52, "text": " I started with something that was 48 times bigger than that, 786,432 bytes.", "tokens": [286, 1409, 365, 746, 300, 390, 11174, 1413, 3801, 813, 300, 11, 1614, 22193, 11, 19, 11440, 36088, 13], "temperature": 0.0, "avg_logprob": -0.11290510016751576, "compression_ratio": 1.5543478260869565, "no_speech_prob": 2.601608457553084e-06}, {"id": 1079, "seek": 589200, "start": 5902.52, "end": 5905.56, "text": " And I've turned it into something that's 16,384 bytes.", "tokens": [400, 286, 600, 3574, 309, 666, 746, 300, 311, 3165, 11, 12625, 19, 36088, 13], "temperature": 0.0, "avg_logprob": -0.11290510016751576, "compression_ratio": 1.5543478260869565, "no_speech_prob": 2.601608457553084e-06}, {"id": 1080, "seek": 589200, "start": 5905.56, "end": 5912.04, "text": " I could now attach that to an email, say, or whatever, and I've now got something that's", "tokens": [286, 727, 586, 5085, 300, 281, 364, 3796, 11, 584, 11, 420, 2035, 11, 293, 286, 600, 586, 658, 746, 300, 311], "temperature": 0.0, "avg_logprob": -0.11290510016751576, "compression_ratio": 1.5543478260869565, "no_speech_prob": 2.601608457553084e-06}, {"id": 1081, "seek": 589200, "start": 5912.04, "end": 5915.0, "text": " 48 times more than my original picture.", "tokens": [11174, 1413, 544, 813, 452, 3380, 3036, 13], "temperature": 0.0, "avg_logprob": -0.11290510016751576, "compression_ratio": 1.5543478260869565, "no_speech_prob": 2.601608457553084e-06}, {"id": 1082, "seek": 589200, "start": 5915.0, "end": 5916.0, "text": " So what's going to happen?", "tokens": [407, 437, 311, 516, 281, 1051, 30], "temperature": 0.0, "avg_logprob": -0.11290510016751576, "compression_ratio": 1.5543478260869565, "no_speech_prob": 2.601608457553084e-06}, {"id": 1083, "seek": 591600, "start": 5916.0, "end": 5923.6, "text": " The person who receives these 16,384 bytes, well, as long as they have a copy of the decoder", "tokens": [440, 954, 567, 20717, 613, 3165, 11, 12625, 19, 36088, 11, 731, 11, 382, 938, 382, 436, 362, 257, 5055, 295, 264, 979, 19866], "temperature": 0.0, "avg_logprob": -0.09584961661809607, "compression_ratio": 1.4517766497461928, "no_speech_prob": 2.26031920647074e-06}, {"id": 1084, "seek": 591600, "start": 5923.6, "end": 5931.44, "text": " on their computer, they can feed those bytes into the decoder and get back the original", "tokens": [322, 641, 3820, 11, 436, 393, 3154, 729, 36088, 666, 264, 979, 19866, 293, 483, 646, 264, 3380], "temperature": 0.0, "avg_logprob": -0.09584961661809607, "compression_ratio": 1.4517766497461928, "no_speech_prob": 2.26031920647074e-06}, {"id": 1085, "seek": 591600, "start": 5931.44, "end": 5933.26, "text": " image.", "tokens": [3256, 13], "temperature": 0.0, "avg_logprob": -0.09584961661809607, "compression_ratio": 1.4517766497461928, "no_speech_prob": 2.26031920647074e-06}, {"id": 1086, "seek": 591600, "start": 5933.26, "end": 5942.92, "text": " So what we've just done is we've created a compression algorithm.", "tokens": [407, 437, 321, 600, 445, 1096, 307, 321, 600, 2942, 257, 19355, 9284, 13], "temperature": 0.0, "avg_logprob": -0.09584961661809607, "compression_ratio": 1.4517766497461928, "no_speech_prob": 2.26031920647074e-06}, {"id": 1087, "seek": 591600, "start": 5942.92, "end": 5944.96, "text": " That's pretty amazing, isn't it?", "tokens": [663, 311, 1238, 2243, 11, 1943, 380, 309, 30], "temperature": 0.0, "avg_logprob": -0.09584961661809607, "compression_ratio": 1.4517766497461928, "no_speech_prob": 2.26031920647074e-06}, {"id": 1088, "seek": 594496, "start": 5944.96, "end": 5952.76, "text": " And in fact, these compression algorithms work extremely, extremely well.", "tokens": [400, 294, 1186, 11, 613, 19355, 14642, 589, 4664, 11, 4664, 731, 13], "temperature": 0.0, "avg_logprob": -0.09957743353313869, "compression_ratio": 1.521978021978022, "no_speech_prob": 2.1907749214733485e-06}, {"id": 1089, "seek": 594496, "start": 5952.76, "end": 5957.52, "text": " And notice that we didn't train this on just this one image.", "tokens": [400, 3449, 300, 321, 994, 380, 3847, 341, 322, 445, 341, 472, 3256, 13], "temperature": 0.0, "avg_logprob": -0.09957743353313869, "compression_ratio": 1.521978021978022, "no_speech_prob": 2.1907749214733485e-06}, {"id": 1090, "seek": 594496, "start": 5957.52, "end": 5961.36, "text": " We've trained it on, say, millions and millions of images.", "tokens": [492, 600, 8895, 309, 322, 11, 584, 11, 6803, 293, 6803, 295, 5267, 13], "temperature": 0.0, "avg_logprob": -0.09957743353313869, "compression_ratio": 1.521978021978022, "no_speech_prob": 2.1907749214733485e-06}, {"id": 1091, "seek": 594496, "start": 5961.36, "end": 5967.24, "text": " And then so you and I both need to have a copy of these two neural nets, but now we", "tokens": [400, 550, 370, 291, 293, 286, 1293, 643, 281, 362, 257, 5055, 295, 613, 732, 18161, 36170, 11, 457, 586, 321], "temperature": 0.0, "avg_logprob": -0.09957743353313869, "compression_ratio": 1.521978021978022, "no_speech_prob": 2.1907749214733485e-06}, {"id": 1092, "seek": 596724, "start": 5967.24, "end": 5975.08, "text": " can share thousands of pictures that we send each other by sending just the 16,384 byte", "tokens": [393, 2073, 5383, 295, 5242, 300, 321, 2845, 1184, 661, 538, 7750, 445, 264, 3165, 11, 12625, 19, 40846], "temperature": 0.0, "avg_logprob": -0.11124637547661276, "compression_ratio": 1.4393939393939394, "no_speech_prob": 1.1365577847755048e-06}, {"id": 1093, "seek": 596724, "start": 5975.08, "end": 5977.04, "text": " version.", "tokens": [3037, 13], "temperature": 0.0, "avg_logprob": -0.11124637547661276, "compression_ratio": 1.4393939393939394, "no_speech_prob": 1.1365577847755048e-06}, {"id": 1094, "seek": 596724, "start": 5977.04, "end": 5983.44, "text": " So we've created a very powerful compression algorithm.", "tokens": [407, 321, 600, 2942, 257, 588, 4005, 19355, 9284, 13], "temperature": 0.0, "avg_logprob": -0.11124637547661276, "compression_ratio": 1.4393939393939394, "no_speech_prob": 1.1365577847755048e-06}, {"id": 1095, "seek": 596724, "start": 5983.44, "end": 5986.599999999999, "text": " And so maybe you can see where this is going.", "tokens": [400, 370, 1310, 291, 393, 536, 689, 341, 307, 516, 13], "temperature": 0.0, "avg_logprob": -0.11124637547661276, "compression_ratio": 1.4393939393939394, "no_speech_prob": 1.1365577847755048e-06}, {"id": 1096, "seek": 596724, "start": 5986.599999999999, "end": 5992.4, "text": " If this here is something which contains all of the interesting and useful information", "tokens": [759, 341, 510, 307, 746, 597, 8306, 439, 295, 264, 1880, 293, 4420, 1589], "temperature": 0.0, "avg_logprob": -0.11124637547661276, "compression_ratio": 1.4393939393939394, "no_speech_prob": 1.1365577847755048e-06}, {"id": 1097, "seek": 599240, "start": 5992.4, "end": 6005.36, "text": " of the image in 16,384 bytes, why on earth would we train our UNet with 786,432 pixels", "tokens": [295, 264, 3256, 294, 3165, 11, 12625, 19, 36088, 11, 983, 322, 4120, 576, 321, 3847, 527, 8229, 302, 365, 1614, 22193, 11, 19, 11440, 18668], "temperature": 0.0, "avg_logprob": -0.1388977390446075, "compression_ratio": 1.3278688524590163, "no_speech_prob": 9.516116733720992e-06}, {"id": 1098, "seek": 599240, "start": 6005.36, "end": 6008.36, "text": " of information?", "tokens": [295, 1589, 30], "temperature": 0.0, "avg_logprob": -0.1388977390446075, "compression_ratio": 1.3278688524590163, "no_speech_prob": 9.516116733720992e-06}, {"id": 1099, "seek": 599240, "start": 6008.36, "end": 6010.679999999999, "text": " And the answer is we wouldn't.", "tokens": [400, 264, 1867, 307, 321, 2759, 380, 13], "temperature": 0.0, "avg_logprob": -0.1388977390446075, "compression_ratio": 1.3278688524590163, "no_speech_prob": 9.516116733720992e-06}, {"id": 1100, "seek": 599240, "start": 6010.679999999999, "end": 6012.16, "text": " That would be stupid.", "tokens": [663, 576, 312, 6631, 13], "temperature": 0.0, "avg_logprob": -0.1388977390446075, "compression_ratio": 1.3278688524590163, "no_speech_prob": 9.516116733720992e-06}, {"id": 1101, "seek": 599240, "start": 6012.16, "end": 6022.12, "text": " Instead, we're going to do this entire thing using our encoded version of each picture.", "tokens": [7156, 11, 321, 434, 516, 281, 360, 341, 2302, 551, 1228, 527, 2058, 12340, 3037, 295, 1184, 3036, 13], "temperature": 0.0, "avg_logprob": -0.1388977390446075, "compression_ratio": 1.3278688524590163, "no_speech_prob": 9.516116733720992e-06}, {"id": 1102, "seek": 602212, "start": 6022.12, "end": 6031.68, "text": " So if we want to train this UNet on 10 million pictures, we put all 10 million pictures through", "tokens": [407, 498, 321, 528, 281, 3847, 341, 8229, 302, 322, 1266, 2459, 5242, 11, 321, 829, 439, 1266, 2459, 5242, 807], "temperature": 0.0, "avg_logprob": -0.13349911973283096, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.769383854683838e-06}, {"id": 1103, "seek": 602212, "start": 6031.68, "end": 6035.5199999999995, "text": " the autoencoder's encoder.", "tokens": [264, 8399, 22660, 19866, 311, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.13349911973283096, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.769383854683838e-06}, {"id": 1104, "seek": 602212, "start": 6035.5199999999995, "end": 6040.76, "text": " So we've now got 10 million of these smaller things.", "tokens": [407, 321, 600, 586, 658, 1266, 2459, 295, 613, 4356, 721, 13], "temperature": 0.0, "avg_logprob": -0.13349911973283096, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.769383854683838e-06}, {"id": 1105, "seek": 602212, "start": 6040.76, "end": 6048.76, "text": " And then we feed it into the UNet training hundreds or thousands of times to train our", "tokens": [400, 550, 321, 3154, 309, 666, 264, 8229, 302, 3097, 6779, 420, 5383, 295, 1413, 281, 3847, 527], "temperature": 0.0, "avg_logprob": -0.13349911973283096, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.769383854683838e-06}, {"id": 1106, "seek": 602212, "start": 6048.76, "end": 6049.88, "text": " UNet.", "tokens": [8229, 302, 13], "temperature": 0.0, "avg_logprob": -0.13349911973283096, "compression_ratio": 1.5952380952380953, "no_speech_prob": 2.769383854683838e-06}, {"id": 1107, "seek": 604988, "start": 6049.88, "end": 6053.04, "text": " And so what will that UNet now do?", "tokens": [400, 370, 437, 486, 300, 8229, 302, 586, 360, 30], "temperature": 0.0, "avg_logprob": -0.14471551109762754, "compression_ratio": 1.6210526315789473, "no_speech_prob": 3.611965667005279e-06}, {"id": 1108, "seek": 604988, "start": 6053.04, "end": 6055.4400000000005, "text": " Something slightly different to what we described.", "tokens": [6595, 4748, 819, 281, 437, 321, 7619, 13], "temperature": 0.0, "avg_logprob": -0.14471551109762754, "compression_ratio": 1.6210526315789473, "no_speech_prob": 3.611965667005279e-06}, {"id": 1109, "seek": 604988, "start": 6055.4400000000005, "end": 6059.8, "text": " It does not anymore take a somewhat noisy image.", "tokens": [467, 775, 406, 3602, 747, 257, 8344, 24518, 3256, 13], "temperature": 0.0, "avg_logprob": -0.14471551109762754, "compression_ratio": 1.6210526315789473, "no_speech_prob": 3.611965667005279e-06}, {"id": 1110, "seek": 604988, "start": 6059.8, "end": 6063.28, "text": " Instead it takes a somewhat noisy one of these.", "tokens": [7156, 309, 2516, 257, 8344, 24518, 472, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.14471551109762754, "compression_ratio": 1.6210526315789473, "no_speech_prob": 3.611965667005279e-06}, {"id": 1111, "seek": 604988, "start": 6063.28, "end": 6066.24, "text": " So it'd probably help to give this thing a name.", "tokens": [407, 309, 1116, 1391, 854, 281, 976, 341, 551, 257, 1315, 13], "temperature": 0.0, "avg_logprob": -0.14471551109762754, "compression_ratio": 1.6210526315789473, "no_speech_prob": 3.611965667005279e-06}, {"id": 1112, "seek": 604988, "start": 6066.24, "end": 6071.72, "text": " And so the name we give this thing is Latents.", "tokens": [400, 370, 264, 1315, 321, 976, 341, 551, 307, 7354, 791, 13], "temperature": 0.0, "avg_logprob": -0.14471551109762754, "compression_ratio": 1.6210526315789473, "no_speech_prob": 3.611965667005279e-06}, {"id": 1113, "seek": 604988, "start": 6071.72, "end": 6075.36, "text": " These are called the Latents.", "tokens": [1981, 366, 1219, 264, 7354, 791, 13], "temperature": 0.0, "avg_logprob": -0.14471551109762754, "compression_ratio": 1.6210526315789473, "no_speech_prob": 3.611965667005279e-06}, {"id": 1114, "seek": 607536, "start": 6075.36, "end": 6085.16, "text": " Okay, so instead the input is somewhat noisy Latents.", "tokens": [1033, 11, 370, 2602, 264, 4846, 307, 8344, 24518, 7354, 791, 13], "temperature": 0.0, "avg_logprob": -0.13801244388927114, "compression_ratio": 1.5725190839694656, "no_speech_prob": 8.990946298581548e-07}, {"id": 1115, "seek": 607536, "start": 6085.16, "end": 6087.719999999999, "text": " The output is still the noise.", "tokens": [440, 5598, 307, 920, 264, 5658, 13], "temperature": 0.0, "avg_logprob": -0.13801244388927114, "compression_ratio": 1.5725190839694656, "no_speech_prob": 8.990946298581548e-07}, {"id": 1116, "seek": 607536, "start": 6087.719999999999, "end": 6093.96, "text": " And so we can now subtract the noise from the slightly somewhat noisy Latents.", "tokens": [400, 370, 321, 393, 586, 16390, 264, 5658, 490, 264, 4748, 8344, 24518, 7354, 791, 13], "temperature": 0.0, "avg_logprob": -0.13801244388927114, "compression_ratio": 1.5725190839694656, "no_speech_prob": 8.990946298581548e-07}, {"id": 1117, "seek": 607536, "start": 6093.96, "end": 6100.12, "text": " And that would give us the actual Latents.", "tokens": [400, 300, 576, 976, 505, 264, 3539, 7354, 791, 13], "temperature": 0.0, "avg_logprob": -0.13801244388927114, "compression_ratio": 1.5725190839694656, "no_speech_prob": 8.990946298581548e-07}, {"id": 1118, "seek": 610012, "start": 6100.12, "end": 6116.12, "text": " And so we can then take the output of the UNet and pass it into our autoencoder's decoder.", "tokens": [400, 370, 321, 393, 550, 747, 264, 5598, 295, 264, 8229, 302, 293, 1320, 309, 666, 527, 8399, 22660, 19866, 311, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.12005934185451932, "compression_ratio": 1.3553719008264462, "no_speech_prob": 3.785299441005918e-06}, {"id": 1119, "seek": 610012, "start": 6116.12, "end": 6123.48, "text": " Because that's something which takes Latents and turns it into a picture.", "tokens": [1436, 300, 311, 746, 597, 2516, 7354, 791, 293, 4523, 309, 666, 257, 3036, 13], "temperature": 0.0, "avg_logprob": -0.12005934185451932, "compression_ratio": 1.3553719008264462, "no_speech_prob": 3.785299441005918e-06}, {"id": 1120, "seek": 612348, "start": 6123.48, "end": 6135.919999999999, "text": " So the input to this is SmallLatentsTensor.", "tokens": [407, 264, 4846, 281, 341, 307, 15287, 43, 267, 791, 51, 23153, 13], "temperature": 0.0, "avg_logprob": -0.26938440444621636, "compression_ratio": 1.238938053097345, "no_speech_prob": 1.6028000118240016e-06}, {"id": 1121, "seek": 612348, "start": 6135.919999999999, "end": 6142.599999999999, "text": " And the output is a large image.", "tokens": [400, 264, 5598, 307, 257, 2416, 3256, 13], "temperature": 0.0, "avg_logprob": -0.26938440444621636, "compression_ratio": 1.238938053097345, "no_speech_prob": 1.6028000118240016e-06}, {"id": 1122, "seek": 612348, "start": 6142.599999999999, "end": 6148.44, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.26938440444621636, "compression_ratio": 1.238938053097345, "no_speech_prob": 1.6028000118240016e-06}, {"id": 1123, "seek": 612348, "start": 6148.44, "end": 6152.799999999999, "text": " Now this thing here is not going to be called an encoder.", "tokens": [823, 341, 551, 510, 307, 406, 516, 281, 312, 1219, 364, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.26938440444621636, "compression_ratio": 1.238938053097345, "no_speech_prob": 1.6028000118240016e-06}, {"id": 1124, "seek": 615280, "start": 6152.8, "end": 6157.12, "text": " It's going to have the name the VAE.", "tokens": [467, 311, 516, 281, 362, 264, 1315, 264, 18527, 36, 13], "temperature": 0.0, "avg_logprob": -0.12455872853597005, "compression_ratio": 1.4968944099378882, "no_speech_prob": 7.071728759910911e-06}, {"id": 1125, "seek": 615280, "start": 6157.12, "end": 6160.400000000001, "text": " And we'll learn about why later.", "tokens": [400, 321, 603, 1466, 466, 983, 1780, 13], "temperature": 0.0, "avg_logprob": -0.12455872853597005, "compression_ratio": 1.4968944099378882, "no_speech_prob": 7.071728759910911e-06}, {"id": 1126, "seek": 615280, "start": 6160.400000000001, "end": 6168.24, "text": " Those details aren't too important, but let's put its correct name here.", "tokens": [3950, 4365, 3212, 380, 886, 1021, 11, 457, 718, 311, 829, 1080, 3006, 1315, 510, 13], "temperature": 0.0, "avg_logprob": -0.12455872853597005, "compression_ratio": 1.4968944099378882, "no_speech_prob": 7.071728759910911e-06}, {"id": 1127, "seek": 615280, "start": 6168.24, "end": 6176.0, "text": " The VAE's decoder.", "tokens": [440, 18527, 36, 311, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.12455872853597005, "compression_ratio": 1.4968944099378882, "no_speech_prob": 7.071728759910911e-06}, {"id": 1128, "seek": 615280, "start": 6176.0, "end": 6180.0, "text": " So you're only going to need the encoder for the VAE if you're training a unit.", "tokens": [407, 291, 434, 787, 516, 281, 643, 264, 2058, 19866, 337, 264, 18527, 36, 498, 291, 434, 3097, 257, 4985, 13], "temperature": 0.0, "avg_logprob": -0.12455872853597005, "compression_ratio": 1.4968944099378882, "no_speech_prob": 7.071728759910911e-06}, {"id": 1129, "seek": 618000, "start": 6180.0, "end": 6184.72, "text": " If you want to just do inference like we did today, you're only going to need the decoder", "tokens": [759, 291, 528, 281, 445, 360, 38253, 411, 321, 630, 965, 11, 291, 434, 787, 516, 281, 643, 264, 979, 19866], "temperature": 0.0, "avg_logprob": -0.13739152635846819, "compression_ratio": 1.4550264550264551, "no_speech_prob": 1.5056845086292014e-06}, {"id": 1130, "seek": 618000, "start": 6184.72, "end": 6191.56, "text": " of the unit.", "tokens": [295, 264, 4985, 13], "temperature": 0.0, "avg_logprob": -0.13739152635846819, "compression_ratio": 1.4550264550264551, "no_speech_prob": 1.5056845086292014e-06}, {"id": 1131, "seek": 618000, "start": 6191.56, "end": 6196.56, "text": " So this whole thing of Latents is entirely optional.", "tokens": [407, 341, 1379, 551, 295, 7354, 791, 307, 7696, 17312, 13], "temperature": 0.0, "avg_logprob": -0.13739152635846819, "compression_ratio": 1.4550264550264551, "no_speech_prob": 1.5056845086292014e-06}, {"id": 1132, "seek": 618000, "start": 6196.56, "end": 6199.96, "text": " This thing we described before works fine.", "tokens": [639, 551, 321, 7619, 949, 1985, 2489, 13], "temperature": 0.0, "avg_logprob": -0.13739152635846819, "compression_ratio": 1.4550264550264551, "no_speech_prob": 1.5056845086292014e-06}, {"id": 1133, "seek": 618000, "start": 6199.96, "end": 6205.14, "text": " But generally speaking, we would rather not use more compute than necessary.", "tokens": [583, 5101, 4124, 11, 321, 576, 2831, 406, 764, 544, 14722, 813, 4818, 13], "temperature": 0.0, "avg_logprob": -0.13739152635846819, "compression_ratio": 1.4550264550264551, "no_speech_prob": 1.5056845086292014e-06}, {"id": 1134, "seek": 620514, "start": 6205.14, "end": 6211.46, "text": " So unless you're trying to sell the world a room full of TPUs, you would probably rather", "tokens": [407, 5969, 291, 434, 1382, 281, 3607, 264, 1002, 257, 1808, 1577, 295, 314, 8115, 82, 11, 291, 576, 1391, 2831], "temperature": 0.0, "avg_logprob": -0.16153214617473324, "compression_ratio": 1.4598930481283423, "no_speech_prob": 1.3496759265763103e-06}, {"id": 1135, "seek": 620514, "start": 6211.46, "end": 6215.8, "text": " everybody was doing stuff on the thing that's 48 times smaller.", "tokens": [2201, 390, 884, 1507, 322, 264, 551, 300, 311, 11174, 1413, 4356, 13], "temperature": 0.0, "avg_logprob": -0.16153214617473324, "compression_ratio": 1.4598930481283423, "no_speech_prob": 1.3496759265763103e-06}, {"id": 1136, "seek": 620514, "start": 6215.8, "end": 6223.58, "text": " So the VAE is optional, but it saves us a whole lot of time and a whole lot of money.", "tokens": [407, 264, 18527, 36, 307, 17312, 11, 457, 309, 19155, 505, 257, 1379, 688, 295, 565, 293, 257, 1379, 688, 295, 1460, 13], "temperature": 0.0, "avg_logprob": -0.16153214617473324, "compression_ratio": 1.4598930481283423, "no_speech_prob": 1.3496759265763103e-06}, {"id": 1137, "seek": 620514, "start": 6223.58, "end": 6226.0, "text": " So that's good.", "tokens": [407, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.16153214617473324, "compression_ratio": 1.4598930481283423, "no_speech_prob": 1.3496759265763103e-06}, {"id": 1138, "seek": 620514, "start": 6226.0, "end": 6230.08, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.16153214617473324, "compression_ratio": 1.4598930481283423, "no_speech_prob": 1.3496759265763103e-06}, {"id": 1139, "seek": 620514, "start": 6230.08, "end": 6231.08, "text": " What's next?", "tokens": [708, 311, 958, 30], "temperature": 0.0, "avg_logprob": -0.16153214617473324, "compression_ratio": 1.4598930481283423, "no_speech_prob": 1.3496759265763103e-06}, {"id": 1140, "seek": 623108, "start": 6231.08, "end": 6240.36, "text": " Well, there's something else, which is we have not just been in this morning's, sorry,", "tokens": [1042, 11, 456, 311, 746, 1646, 11, 597, 307, 321, 362, 406, 445, 668, 294, 341, 2446, 311, 11, 2597, 11], "temperature": 0.0, "avg_logprob": -0.15370320540208082, "compression_ratio": 1.5414012738853504, "no_speech_prob": 2.8572987957886653e-06}, {"id": 1141, "seek": 623108, "start": 6240.36, "end": 6249.22, "text": " in the first half of today's lesson, we weren't just saying, produce me an image.", "tokens": [294, 264, 700, 1922, 295, 965, 311, 6898, 11, 321, 4999, 380, 445, 1566, 11, 5258, 385, 364, 3256, 13], "temperature": 0.0, "avg_logprob": -0.15370320540208082, "compression_ratio": 1.5414012738853504, "no_speech_prob": 2.8572987957886653e-06}, {"id": 1142, "seek": 623108, "start": 6249.22, "end": 6257.34, "text": " We were saying produce me an image of Tiny the Teddy Bear riding a horse.", "tokens": [492, 645, 1566, 5258, 385, 364, 3256, 295, 39992, 264, 34330, 19836, 9546, 257, 6832, 13], "temperature": 0.0, "avg_logprob": -0.15370320540208082, "compression_ratio": 1.5414012738853504, "no_speech_prob": 2.8572987957886653e-06}, {"id": 1143, "seek": 625734, "start": 6257.34, "end": 6261.16, "text": " So how does that bit work?", "tokens": [407, 577, 775, 300, 857, 589, 30], "temperature": 0.0, "avg_logprob": -0.0668179253001272, "compression_ratio": 1.5445544554455446, "no_speech_prob": 2.2959061425353866e-06}, {"id": 1144, "seek": 625734, "start": 6261.16, "end": 6266.400000000001, "text": " So the way that bit works is actually on the whole pretty straightforward.", "tokens": [407, 264, 636, 300, 857, 1985, 307, 767, 322, 264, 1379, 1238, 15325, 13], "temperature": 0.0, "avg_logprob": -0.0668179253001272, "compression_ratio": 1.5445544554455446, "no_speech_prob": 2.2959061425353866e-06}, {"id": 1145, "seek": 625734, "start": 6266.400000000001, "end": 6271.34, "text": " Let's think about how we could do exactly that for our MNIST example.", "tokens": [961, 311, 519, 466, 577, 321, 727, 360, 2293, 300, 337, 527, 376, 45, 19756, 1365, 13], "temperature": 0.0, "avg_logprob": -0.0668179253001272, "compression_ratio": 1.5445544554455446, "no_speech_prob": 2.2959061425353866e-06}, {"id": 1146, "seek": 625734, "start": 6271.34, "end": 6277.24, "text": " How could we get this so that rather than just feeding in noise and getting back some", "tokens": [1012, 727, 321, 483, 341, 370, 300, 2831, 813, 445, 12919, 294, 5658, 293, 1242, 646, 512], "temperature": 0.0, "avg_logprob": -0.0668179253001272, "compression_ratio": 1.5445544554455446, "no_speech_prob": 2.2959061425353866e-06}, {"id": 1147, "seek": 625734, "start": 6277.24, "end": 6281.24, "text": " digit, how do we get it to give us a particular digit?", "tokens": [14293, 11, 577, 360, 321, 483, 309, 281, 976, 505, 257, 1729, 14293, 30], "temperature": 0.0, "avg_logprob": -0.0668179253001272, "compression_ratio": 1.5445544554455446, "no_speech_prob": 2.2959061425353866e-06}, {"id": 1148, "seek": 628124, "start": 6281.24, "end": 6288.5599999999995, "text": " What if we wanted to pass in the literal number three plus some noise and have it attempt", "tokens": [708, 498, 321, 1415, 281, 1320, 294, 264, 20411, 1230, 1045, 1804, 512, 5658, 293, 362, 309, 5217], "temperature": 0.0, "avg_logprob": -0.13627430488323344, "compression_ratio": 1.613861386138614, "no_speech_prob": 5.896412176298327e-07}, {"id": 1149, "seek": 628124, "start": 6288.5599999999995, "end": 6292.719999999999, "text": " to generate a handwritten three for us?", "tokens": [281, 8460, 257, 1011, 26859, 1045, 337, 505, 30], "temperature": 0.0, "avg_logprob": -0.13627430488323344, "compression_ratio": 1.613861386138614, "no_speech_prob": 5.896412176298327e-07}, {"id": 1150, "seek": 628124, "start": 6292.719999999999, "end": 6294.32, "text": " How would we do that?", "tokens": [1012, 576, 321, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.13627430488323344, "compression_ratio": 1.613861386138614, "no_speech_prob": 5.896412176298327e-07}, {"id": 1151, "seek": 628124, "start": 6294.32, "end": 6302.639999999999, "text": " Well, what we could do is way back here for the input to this model, why don't in addition", "tokens": [1042, 11, 437, 321, 727, 360, 307, 636, 646, 510, 337, 264, 4846, 281, 341, 2316, 11, 983, 500, 380, 294, 4500], "temperature": 0.0, "avg_logprob": -0.13627430488323344, "compression_ratio": 1.613861386138614, "no_speech_prob": 5.896412176298327e-07}, {"id": 1152, "seek": 628124, "start": 6302.639999999999, "end": 6310.92, "text": " to passing in the noisy input, let's also pass in a one hot encoded version of what", "tokens": [281, 8437, 294, 264, 24518, 4846, 11, 718, 311, 611, 1320, 294, 257, 472, 2368, 2058, 12340, 3037, 295, 437], "temperature": 0.0, "avg_logprob": -0.13627430488323344, "compression_ratio": 1.613861386138614, "no_speech_prob": 5.896412176298327e-07}, {"id": 1153, "seek": 631092, "start": 6310.92, "end": 6317.88, "text": " digit it is.", "tokens": [14293, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.10963624523532006, "compression_ratio": 1.4864864864864864, "no_speech_prob": 3.6688488762592897e-06}, {"id": 1154, "seek": 631092, "start": 6317.88, "end": 6320.38, "text": " So we're now passing two things into this model.", "tokens": [407, 321, 434, 586, 8437, 732, 721, 666, 341, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10963624523532006, "compression_ratio": 1.4864864864864864, "no_speech_prob": 3.6688488762592897e-06}, {"id": 1155, "seek": 631092, "start": 6320.38, "end": 6331.02, "text": " So previously, this neural net took as inputs just the pixels, but now it's going to take", "tokens": [407, 8046, 11, 341, 18161, 2533, 1890, 382, 15743, 445, 264, 18668, 11, 457, 586, 309, 311, 516, 281, 747], "temperature": 0.0, "avg_logprob": -0.10963624523532006, "compression_ratio": 1.4864864864864864, "no_speech_prob": 3.6688488762592897e-06}, {"id": 1156, "seek": 631092, "start": 6331.02, "end": 6338.96, "text": " in the pixels and what digit is it as like a one hot encoded vector.", "tokens": [294, 264, 18668, 293, 437, 14293, 307, 309, 382, 411, 257, 472, 2368, 2058, 12340, 8062, 13], "temperature": 0.0, "avg_logprob": -0.10963624523532006, "compression_ratio": 1.4864864864864864, "no_speech_prob": 3.6688488762592897e-06}, {"id": 1157, "seek": 633896, "start": 6338.96, "end": 6347.8, "text": " So now it's going to learn how to predict what is the noise, right?", "tokens": [407, 586, 309, 311, 516, 281, 1466, 577, 281, 6069, 437, 307, 264, 5658, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.09011578791349836, "compression_ratio": 1.8761904761904762, "no_speech_prob": 8.990949140752491e-07}, {"id": 1158, "seek": 633896, "start": 6347.8, "end": 6352.8, "text": " And it's going to predict what is the noise and it's going to have some extra information,", "tokens": [400, 309, 311, 516, 281, 6069, 437, 307, 264, 5658, 293, 309, 311, 516, 281, 362, 512, 2857, 1589, 11], "temperature": 0.0, "avg_logprob": -0.09011578791349836, "compression_ratio": 1.8761904761904762, "no_speech_prob": 8.990949140752491e-07}, {"id": 1159, "seek": 633896, "start": 6352.8, "end": 6356.28, "text": " which is it's going to know what the original image was.", "tokens": [597, 307, 309, 311, 516, 281, 458, 437, 264, 3380, 3256, 390, 13], "temperature": 0.0, "avg_logprob": -0.09011578791349836, "compression_ratio": 1.8761904761904762, "no_speech_prob": 8.990949140752491e-07}, {"id": 1160, "seek": 633896, "start": 6356.28, "end": 6360.56, "text": " So we would expect this model to be better at predicting noise than the previous one", "tokens": [407, 321, 576, 2066, 341, 2316, 281, 312, 1101, 412, 32884, 5658, 813, 264, 3894, 472], "temperature": 0.0, "avg_logprob": -0.09011578791349836, "compression_ratio": 1.8761904761904762, "no_speech_prob": 8.990949140752491e-07}, {"id": 1161, "seek": 633896, "start": 6360.56, "end": 6363.12, "text": " because we're giving it more information.", "tokens": [570, 321, 434, 2902, 309, 544, 1589, 13], "temperature": 0.0, "avg_logprob": -0.09011578791349836, "compression_ratio": 1.8761904761904762, "no_speech_prob": 8.990949140752491e-07}, {"id": 1162, "seek": 633896, "start": 6363.12, "end": 6366.92, "text": " This was a three, this was a six, this was a seven.", "tokens": [639, 390, 257, 1045, 11, 341, 390, 257, 2309, 11, 341, 390, 257, 3407, 13], "temperature": 0.0, "avg_logprob": -0.09011578791349836, "compression_ratio": 1.8761904761904762, "no_speech_prob": 8.990949140752491e-07}, {"id": 1163, "seek": 636692, "start": 6366.92, "end": 6373.68, "text": " So this neural net is going to learn to estimate noise better by taking advantage of the fact", "tokens": [407, 341, 18161, 2533, 307, 516, 281, 1466, 281, 12539, 5658, 1101, 538, 1940, 5002, 295, 264, 1186], "temperature": 0.0, "avg_logprob": -0.11054815728980374, "compression_ratio": 1.5980861244019138, "no_speech_prob": 1.5534920976278954e-06}, {"id": 1164, "seek": 636692, "start": 6373.68, "end": 6379.24, "text": " that it knows what actual the input was.", "tokens": [300, 309, 3255, 437, 3539, 264, 4846, 390, 13], "temperature": 0.0, "avg_logprob": -0.11054815728980374, "compression_ratio": 1.5980861244019138, "no_speech_prob": 1.5534920976278954e-06}, {"id": 1165, "seek": 636692, "start": 6379.24, "end": 6380.24, "text": " And why is that useful?", "tokens": [400, 983, 307, 300, 4420, 30], "temperature": 0.0, "avg_logprob": -0.11054815728980374, "compression_ratio": 1.5980861244019138, "no_speech_prob": 1.5534920976278954e-06}, {"id": 1166, "seek": 636692, "start": 6380.24, "end": 6386.68, "text": " Well, the reason that's useful is because now when we feed in the number three, like", "tokens": [1042, 11, 264, 1778, 300, 311, 4420, 307, 570, 586, 562, 321, 3154, 294, 264, 1230, 1045, 11, 411], "temperature": 0.0, "avg_logprob": -0.11054815728980374, "compression_ratio": 1.5980861244019138, "no_speech_prob": 1.5534920976278954e-06}, {"id": 1167, "seek": 636692, "start": 6386.68, "end": 6392.72, "text": " the actual digit three is a one hot encoded vector plus noise after this has been trained,", "tokens": [264, 3539, 14293, 1045, 307, 257, 472, 2368, 2058, 12340, 8062, 1804, 5658, 934, 341, 575, 668, 8895, 11], "temperature": 0.0, "avg_logprob": -0.11054815728980374, "compression_ratio": 1.5980861244019138, "no_speech_prob": 1.5534920976278954e-06}, {"id": 1168, "seek": 639272, "start": 6392.72, "end": 6397.96, "text": " then our model is going to say the noise is everything that doesn't represent the number", "tokens": [550, 527, 2316, 307, 516, 281, 584, 264, 5658, 307, 1203, 300, 1177, 380, 2906, 264, 1230], "temperature": 0.0, "avg_logprob": -0.10813963413238525, "compression_ratio": 1.7465437788018434, "no_speech_prob": 2.769396814983338e-06}, {"id": 1169, "seek": 639272, "start": 6397.96, "end": 6402.04, "text": " three because that's what it's learned to do.", "tokens": [1045, 570, 300, 311, 437, 309, 311, 3264, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.10813963413238525, "compression_ratio": 1.7465437788018434, "no_speech_prob": 2.769396814983338e-06}, {"id": 1170, "seek": 639272, "start": 6402.04, "end": 6403.96, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.10813963413238525, "compression_ratio": 1.7465437788018434, "no_speech_prob": 2.769396814983338e-06}, {"id": 1171, "seek": 639272, "start": 6403.96, "end": 6410.240000000001, "text": " So that's a pretty straightforward way to give it the word we use is guidance about", "tokens": [407, 300, 311, 257, 1238, 15325, 636, 281, 976, 309, 264, 1349, 321, 764, 307, 10056, 466], "temperature": 0.0, "avg_logprob": -0.10813963413238525, "compression_ratio": 1.7465437788018434, "no_speech_prob": 2.769396814983338e-06}, {"id": 1172, "seek": 639272, "start": 6410.240000000001, "end": 6415.8, "text": " what it is that we're actually trying to remove the noise from.", "tokens": [437, 309, 307, 300, 321, 434, 767, 1382, 281, 4159, 264, 5658, 490, 13], "temperature": 0.0, "avg_logprob": -0.10813963413238525, "compression_ratio": 1.7465437788018434, "no_speech_prob": 2.769396814983338e-06}, {"id": 1173, "seek": 639272, "start": 6415.8, "end": 6422.66, "text": " And so then we can use that guidance to guide it as to what image we're trying to create.", "tokens": [400, 370, 550, 321, 393, 764, 300, 10056, 281, 5934, 309, 382, 281, 437, 3256, 321, 434, 1382, 281, 1884, 13], "temperature": 0.0, "avg_logprob": -0.10813963413238525, "compression_ratio": 1.7465437788018434, "no_speech_prob": 2.769396814983338e-06}, {"id": 1174, "seek": 642266, "start": 6422.66, "end": 6424.24, "text": " So that's the basic idea.", "tokens": [407, 300, 311, 264, 3875, 1558, 13], "temperature": 0.0, "avg_logprob": -0.12314817823212722, "compression_ratio": 1.417808219178082, "no_speech_prob": 4.425443421496311e-06}, {"id": 1175, "seek": 642266, "start": 6424.24, "end": 6441.44, "text": " Now the problem is if we want to create a picture of a cute teddy bear, we've got a", "tokens": [823, 264, 1154, 307, 498, 321, 528, 281, 1884, 257, 3036, 295, 257, 4052, 45116, 6155, 11, 321, 600, 658, 257], "temperature": 0.0, "avg_logprob": -0.12314817823212722, "compression_ratio": 1.417808219178082, "no_speech_prob": 4.425443421496311e-06}, {"id": 1176, "seek": 642266, "start": 6441.44, "end": 6442.44, "text": " problem.", "tokens": [1154, 13], "temperature": 0.0, "avg_logprob": -0.12314817823212722, "compression_ratio": 1.417808219178082, "no_speech_prob": 4.425443421496311e-06}, {"id": 1177, "seek": 642266, "start": 6442.44, "end": 6451.16, "text": " It was easy enough to pass the digit eight, the literal number eight into our neural net", "tokens": [467, 390, 1858, 1547, 281, 1320, 264, 14293, 3180, 11, 264, 20411, 1230, 3180, 666, 527, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.12314817823212722, "compression_ratio": 1.417808219178082, "no_speech_prob": 4.425443421496311e-06}, {"id": 1178, "seek": 645116, "start": 6451.16, "end": 6457.04, "text": " because we can just create a one hot encoded vector in which position number eight is a", "tokens": [570, 321, 393, 445, 1884, 257, 472, 2368, 2058, 12340, 8062, 294, 597, 2535, 1230, 3180, 307, 257], "temperature": 0.0, "avg_logprob": -0.11835172441270617, "compression_ratio": 1.751269035532995, "no_speech_prob": 6.643365395575529e-06}, {"id": 1179, "seek": 645116, "start": 6457.04, "end": 6459.48, "text": " one and everything else is a zero.", "tokens": [472, 293, 1203, 1646, 307, 257, 4018, 13], "temperature": 0.0, "avg_logprob": -0.11835172441270617, "compression_ratio": 1.751269035532995, "no_speech_prob": 6.643365395575529e-06}, {"id": 1180, "seek": 645116, "start": 6459.48, "end": 6463.08, "text": " But how do we do that for a cute teddy?", "tokens": [583, 577, 360, 321, 360, 300, 337, 257, 4052, 45116, 30], "temperature": 0.0, "avg_logprob": -0.11835172441270617, "compression_ratio": 1.751269035532995, "no_speech_prob": 6.643365395575529e-06}, {"id": 1181, "seek": 645116, "start": 6463.08, "end": 6465.2, "text": " We can't.", "tokens": [492, 393, 380, 13], "temperature": 0.0, "avg_logprob": -0.11835172441270617, "compression_ratio": 1.751269035532995, "no_speech_prob": 6.643365395575529e-06}, {"id": 1182, "seek": 645116, "start": 6465.2, "end": 6470.68, "text": " We can't create every possible sentence that can be uttered in the whole world and then", "tokens": [492, 393, 380, 1884, 633, 1944, 8174, 300, 393, 312, 17567, 292, 294, 264, 1379, 1002, 293, 550], "temperature": 0.0, "avg_logprob": -0.11835172441270617, "compression_ratio": 1.751269035532995, "no_speech_prob": 6.643365395575529e-06}, {"id": 1183, "seek": 645116, "start": 6470.68, "end": 6476.68, "text": " create a one hot encoded version of every sentence in the world because that's going", "tokens": [1884, 257, 472, 2368, 2058, 12340, 3037, 295, 633, 8174, 294, 264, 1002, 570, 300, 311, 516], "temperature": 0.0, "avg_logprob": -0.11835172441270617, "compression_ratio": 1.751269035532995, "no_speech_prob": 6.643365395575529e-06}, {"id": 1184, "seek": 647668, "start": 6476.68, "end": 6482.84, "text": " to take a vector that is too long to say the least.", "tokens": [281, 747, 257, 8062, 300, 307, 886, 938, 281, 584, 264, 1935, 13], "temperature": 0.0, "avg_logprob": -0.06846122992666144, "compression_ratio": 1.4776119402985075, "no_speech_prob": 1.3287744877743535e-06}, {"id": 1185, "seek": 647668, "start": 6482.84, "end": 6489.42, "text": " So we have to do something else to turn this into an embedding, something other than grabbing", "tokens": [407, 321, 362, 281, 360, 746, 1646, 281, 1261, 341, 666, 364, 12240, 3584, 11, 746, 661, 813, 23771], "temperature": 0.0, "avg_logprob": -0.06846122992666144, "compression_ratio": 1.4776119402985075, "no_speech_prob": 1.3287744877743535e-06}, {"id": 1186, "seek": 647668, "start": 6489.42, "end": 6493.18, "text": " a one hot encoded version of this.", "tokens": [257, 472, 2368, 2058, 12340, 3037, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.06846122992666144, "compression_ratio": 1.4776119402985075, "no_speech_prob": 1.3287744877743535e-06}, {"id": 1187, "seek": 647668, "start": 6493.18, "end": 6496.16, "text": " So what do we do?", "tokens": [407, 437, 360, 321, 360, 30], "temperature": 0.0, "avg_logprob": -0.06846122992666144, "compression_ratio": 1.4776119402985075, "no_speech_prob": 1.3287744877743535e-06}, {"id": 1188, "seek": 649616, "start": 6496.16, "end": 6510.5599999999995, "text": " So what we're going to do is we're going to try to create a model that can take a sentence", "tokens": [407, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 853, 281, 1884, 257, 2316, 300, 393, 747, 257, 8174], "temperature": 0.0, "avg_logprob": -0.06586603848439343, "compression_ratio": 1.5267175572519085, "no_speech_prob": 5.89641103942995e-07}, {"id": 1189, "seek": 649616, "start": 6510.5599999999995, "end": 6521.4, "text": " like a cute teddy and can return a vector of numbers that in some way represents what", "tokens": [411, 257, 4052, 45116, 293, 393, 2736, 257, 8062, 295, 3547, 300, 294, 512, 636, 8855, 437], "temperature": 0.0, "avg_logprob": -0.06586603848439343, "compression_ratio": 1.5267175572519085, "no_speech_prob": 5.89641103942995e-07}, {"id": 1190, "seek": 649616, "start": 6521.4, "end": 6524.94, "text": " cute teddies look like.", "tokens": [4052, 22337, 22018, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.06586603848439343, "compression_ratio": 1.5267175572519085, "no_speech_prob": 5.89641103942995e-07}, {"id": 1191, "seek": 652494, "start": 6524.94, "end": 6530.0, "text": " And the way we're going to do that is we're first going to surf the internet and download", "tokens": [400, 264, 636, 321, 434, 516, 281, 360, 300, 307, 321, 434, 700, 516, 281, 9684, 264, 4705, 293, 5484], "temperature": 0.0, "avg_logprob": -0.09657498527975643, "compression_ratio": 1.6649746192893402, "no_speech_prob": 1.628040649848117e-06}, {"id": 1192, "seek": 652494, "start": 6530.0, "end": 6531.0, "text": " images.", "tokens": [5267, 13], "temperature": 0.0, "avg_logprob": -0.09657498527975643, "compression_ratio": 1.6649746192893402, "no_speech_prob": 1.628040649848117e-06}, {"id": 1193, "seek": 652494, "start": 6531.0, "end": 6535.5199999999995, "text": " So here are four examples of images that I found on the internet.", "tokens": [407, 510, 366, 1451, 5110, 295, 5267, 300, 286, 1352, 322, 264, 4705, 13], "temperature": 0.0, "avg_logprob": -0.09657498527975643, "compression_ratio": 1.6649746192893402, "no_speech_prob": 1.628040649848117e-06}, {"id": 1194, "seek": 652494, "start": 6535.5199999999995, "end": 6548.4, "text": " And so for each of these images, they had an image tag next to them, right?", "tokens": [400, 370, 337, 1184, 295, 613, 5267, 11, 436, 632, 364, 3256, 6162, 958, 281, 552, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.09657498527975643, "compression_ratio": 1.6649746192893402, "no_speech_prob": 1.628040649848117e-06}, {"id": 1195, "seek": 652494, "start": 6548.4, "end": 6553.679999999999, "text": " And if people are being good, then they also added an alt tag to help with accessibility", "tokens": [400, 498, 561, 366, 885, 665, 11, 550, 436, 611, 3869, 364, 4955, 6162, 281, 854, 365, 15002], "temperature": 0.0, "avg_logprob": -0.09657498527975643, "compression_ratio": 1.6649746192893402, "no_speech_prob": 1.628040649848117e-06}, {"id": 1196, "seek": 655368, "start": 6553.68, "end": 6556.200000000001, "text": " and maybe for SEO purposes.", "tokens": [293, 1310, 337, 22964, 9932, 13], "temperature": 0.0, "avg_logprob": -0.08170160187615289, "compression_ratio": 1.2283464566929134, "no_speech_prob": 5.2553200475813355e-06}, {"id": 1197, "seek": 655368, "start": 6556.200000000001, "end": 6568.76, "text": " And they probably said things like a graceful swan.", "tokens": [400, 436, 1391, 848, 721, 411, 257, 10042, 906, 1693, 282, 13], "temperature": 0.0, "avg_logprob": -0.08170160187615289, "compression_ratio": 1.2283464566929134, "no_speech_prob": 5.2553200475813355e-06}, {"id": 1198, "seek": 655368, "start": 6568.76, "end": 6580.08, "text": " And the alt tag for this might have been a scene from Hitchcock's The Birds.", "tokens": [400, 264, 4955, 6162, 337, 341, 1062, 362, 668, 257, 4145, 490, 389, 1549, 29779, 311, 440, 41456, 13], "temperature": 0.0, "avg_logprob": -0.08170160187615289, "compression_ratio": 1.2283464566929134, "no_speech_prob": 5.2553200475813355e-06}, {"id": 1199, "seek": 658008, "start": 6580.08, "end": 6585.64, "text": " And the alt tag for this might have been Jeremy Howard.", "tokens": [400, 264, 4955, 6162, 337, 341, 1062, 362, 668, 17809, 17626, 13], "temperature": 0.0, "avg_logprob": -0.12391205001295659, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.6425509531691205e-06}, {"id": 1200, "seek": 658008, "start": 6585.64, "end": 6595.4, "text": " And the alt tag for this might have been Fast.ai's logo.", "tokens": [400, 264, 4955, 6162, 337, 341, 1062, 362, 668, 15968, 13, 1301, 311, 9699, 13], "temperature": 0.0, "avg_logprob": -0.12391205001295659, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.6425509531691205e-06}, {"id": 1201, "seek": 658008, "start": 6595.4, "end": 6599.24, "text": " And we could do that for millions and millions and millions of images that we find on the", "tokens": [400, 321, 727, 360, 300, 337, 6803, 293, 6803, 293, 6803, 295, 5267, 300, 321, 915, 322, 264], "temperature": 0.0, "avg_logprob": -0.12391205001295659, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.6425509531691205e-06}, {"id": 1202, "seek": 658008, "start": 6599.24, "end": 6603.12, "text": " internet.", "tokens": [4705, 13], "temperature": 0.0, "avg_logprob": -0.12391205001295659, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.6425509531691205e-06}, {"id": 1203, "seek": 660312, "start": 6603.12, "end": 6612.5599999999995, "text": " So what we can now do with these is we can create two models.", "tokens": [407, 437, 321, 393, 586, 360, 365, 613, 307, 321, 393, 1884, 732, 5245, 13], "temperature": 0.0, "avg_logprob": -0.19018066846407378, "compression_ratio": 1.4623655913978495, "no_speech_prob": 3.446536538831424e-06}, {"id": 1204, "seek": 660312, "start": 6612.5599999999995, "end": 6628.04, "text": " One model which is a text encoder and one model which is an image encoder.", "tokens": [1485, 2316, 597, 307, 257, 2487, 2058, 19866, 293, 472, 2316, 597, 307, 364, 3256, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.19018066846407378, "compression_ratio": 1.4623655913978495, "no_speech_prob": 3.446536538831424e-06}, {"id": 1205, "seek": 662804, "start": 6628.04, "end": 6634.64, "text": " Okay, so again, these are neural nets.", "tokens": [1033, 11, 370, 797, 11, 613, 366, 18161, 36170, 13], "temperature": 0.0, "avg_logprob": -0.09583748711480035, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.029413503303658e-06}, {"id": 1206, "seek": 662804, "start": 6634.64, "end": 6638.5199999999995, "text": " We don't care about what their architectures are or whatever.", "tokens": [492, 500, 380, 1127, 466, 437, 641, 6331, 1303, 366, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.09583748711480035, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.029413503303658e-06}, {"id": 1207, "seek": 662804, "start": 6638.5199999999995, "end": 6644.0, "text": " We know that they're just black boxes which contain weights, which means they need inputs", "tokens": [492, 458, 300, 436, 434, 445, 2211, 9002, 597, 5304, 17443, 11, 597, 1355, 436, 643, 15743], "temperature": 0.0, "avg_logprob": -0.09583748711480035, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.029413503303658e-06}, {"id": 1208, "seek": 662804, "start": 6644.0, "end": 6646.5199999999995, "text": " and outputs and a loss function.", "tokens": [293, 23930, 293, 257, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.09583748711480035, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.029413503303658e-06}, {"id": 1209, "seek": 662804, "start": 6646.5199999999995, "end": 6650.16, "text": " And then they'll do something.", "tokens": [400, 550, 436, 603, 360, 746, 13], "temperature": 0.0, "avg_logprob": -0.09583748711480035, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.029413503303658e-06}, {"id": 1210, "seek": 662804, "start": 6650.16, "end": 6654.6, "text": " Once we've defined inputs and outputs and a loss function, the neural nets will then", "tokens": [3443, 321, 600, 7642, 15743, 293, 23930, 293, 257, 4470, 2445, 11, 264, 18161, 36170, 486, 550], "temperature": 0.0, "avg_logprob": -0.09583748711480035, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.029413503303658e-06}, {"id": 1211, "seek": 662804, "start": 6654.6, "end": 6656.68, "text": " do something.", "tokens": [360, 746, 13], "temperature": 0.0, "avg_logprob": -0.09583748711480035, "compression_ratio": 1.756218905472637, "no_speech_prob": 4.029413503303658e-06}, {"id": 1212, "seek": 665668, "start": 6656.68, "end": 6664.4400000000005, "text": " So here's a really interesting idea.", "tokens": [407, 510, 311, 257, 534, 1880, 1558, 13], "temperature": 0.0, "avg_logprob": -0.15387175299904562, "compression_ratio": 1.2391304347826086, "no_speech_prob": 2.7693545234797057e-06}, {"id": 1213, "seek": 665668, "start": 6664.4400000000005, "end": 6680.4800000000005, "text": " What if we take this image and what if we then also take the text, a graceful", "tokens": [708, 498, 321, 747, 341, 3256, 293, 437, 498, 321, 550, 611, 747, 264, 2487, 11, 257, 10042, 906], "temperature": 0.0, "avg_logprob": -0.15387175299904562, "compression_ratio": 1.2391304347826086, "no_speech_prob": 2.7693545234797057e-06}, {"id": 1214, "seek": 668048, "start": 6680.48, "end": 6694.04, "text": " swan, and we're going to feed these into their respective models, which initially they of", "tokens": [1693, 282, 11, 293, 321, 434, 516, 281, 3154, 613, 666, 641, 23649, 5245, 11, 597, 9105, 436, 295], "temperature": 0.0, "avg_logprob": -0.21690068678422408, "compression_ratio": 1.4825174825174825, "no_speech_prob": 7.766889211779926e-06}, {"id": 1215, "seek": 668048, "start": 6694.04, "end": 6698.2, "text": " course have random weights.", "tokens": [1164, 362, 4974, 17443, 13], "temperature": 0.0, "avg_logprob": -0.21690068678422408, "compression_ratio": 1.4825174825174825, "no_speech_prob": 7.766889211779926e-06}, {"id": 1216, "seek": 668048, "start": 6698.2, "end": 6705.959999999999, "text": " And that means that they're going to spit out random features, a vector of stuff, random", "tokens": [400, 300, 1355, 300, 436, 434, 516, 281, 22127, 484, 4974, 4122, 11, 257, 8062, 295, 1507, 11, 4974], "temperature": 0.0, "avg_logprob": -0.21690068678422408, "compression_ratio": 1.4825174825174825, "no_speech_prob": 7.766889211779926e-06}, {"id": 1217, "seek": 668048, "start": 6705.959999999999, "end": 6706.959999999999, "text": " crap.", "tokens": [12426, 13], "temperature": 0.0, "avg_logprob": -0.21690068678422408, "compression_ratio": 1.4825174825174825, "no_speech_prob": 7.766889211779926e-06}, {"id": 1218, "seek": 670696, "start": 6706.96, "end": 6713.16, "text": " Because we haven't trained them yet, okay.", "tokens": [1436, 321, 2378, 380, 8895, 552, 1939, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.15496584085317758, "compression_ratio": 1.6993865030674846, "no_speech_prob": 6.64339677314274e-06}, {"id": 1219, "seek": 670696, "start": 6713.16, "end": 6716.28, "text": " And we can do the same thing with a scene from Hitchcock.", "tokens": [400, 321, 393, 360, 264, 912, 551, 365, 257, 4145, 490, 389, 1549, 29779, 13], "temperature": 0.0, "avg_logprob": -0.15496584085317758, "compression_ratio": 1.6993865030674846, "no_speech_prob": 6.64339677314274e-06}, {"id": 1220, "seek": 670696, "start": 6716.28, "end": 6720.44, "text": " We pass the scene from Hitchcock in and we'll pass in the word scene from Hitchcock and", "tokens": [492, 1320, 264, 4145, 490, 389, 1549, 29779, 294, 293, 321, 603, 1320, 294, 264, 1349, 4145, 490, 389, 1549, 29779, 293], "temperature": 0.0, "avg_logprob": -0.15496584085317758, "compression_ratio": 1.6993865030674846, "no_speech_prob": 6.64339677314274e-06}, {"id": 1221, "seek": 670696, "start": 6720.44, "end": 6725.96, "text": " then it'll give us two other vectors.", "tokens": [550, 309, 603, 976, 505, 732, 661, 18875, 13], "temperature": 0.0, "avg_logprob": -0.15496584085317758, "compression_ratio": 1.6993865030674846, "no_speech_prob": 6.64339677314274e-06}, {"id": 1222, "seek": 670696, "start": 6725.96, "end": 6734.16, "text": " And so we can do something really interesting now.", "tokens": [400, 370, 321, 393, 360, 746, 534, 1880, 586, 13], "temperature": 0.0, "avg_logprob": -0.15496584085317758, "compression_ratio": 1.6993865030674846, "no_speech_prob": 6.64339677314274e-06}, {"id": 1223, "seek": 673416, "start": 6734.16, "end": 6736.96, "text": " We can line these up.", "tokens": [492, 393, 1622, 613, 493, 13], "temperature": 0.0, "avg_logprob": -0.35018390308726915, "compression_ratio": 1.311926605504587, "no_speech_prob": 6.962186034797924e-06}, {"id": 1224, "seek": 673416, "start": 6736.96, "end": 6742.16, "text": " I guess we'll just move them.", "tokens": [286, 2041, 321, 603, 445, 1286, 552, 13], "temperature": 0.0, "avg_logprob": -0.35018390308726915, "compression_ratio": 1.311926605504587, "no_speech_prob": 6.962186034797924e-06}, {"id": 1225, "seek": 673416, "start": 6742.16, "end": 6745.2, "text": " We can line these up.", "tokens": [492, 393, 1622, 613, 493, 13], "temperature": 0.0, "avg_logprob": -0.35018390308726915, "compression_ratio": 1.311926605504587, "no_speech_prob": 6.962186034797924e-06}, {"id": 1226, "seek": 673416, "start": 6745.2, "end": 6754.8, "text": " Okay, here's all of our images.", "tokens": [1033, 11, 510, 311, 439, 295, 527, 5267, 13], "temperature": 0.0, "avg_logprob": -0.35018390308726915, "compression_ratio": 1.311926605504587, "no_speech_prob": 6.962186034797924e-06}, {"id": 1227, "seek": 673416, "start": 6754.8, "end": 6763.72, "text": " And then we can have, whoopsie-daisy.", "tokens": [400, 550, 321, 393, 362, 11, 567, 3370, 414, 12, 67, 1527, 88, 13], "temperature": 0.0, "avg_logprob": -0.35018390308726915, "compression_ratio": 1.311926605504587, "no_speech_prob": 6.962186034797924e-06}, {"id": 1228, "seek": 676372, "start": 6763.72, "end": 6766.76, "text": " And then we can have our text.", "tokens": [400, 550, 321, 393, 362, 527, 2487, 13], "temperature": 0.0, "avg_logprob": -0.21937414169311523, "compression_ratio": 1.2596153846153846, "no_speech_prob": 1.2218953997944482e-05}, {"id": 1229, "seek": 676372, "start": 6766.76, "end": 6777.08, "text": " So we've got graceful swan.", "tokens": [407, 321, 600, 658, 10042, 906, 1693, 282, 13], "temperature": 0.0, "avg_logprob": -0.21937414169311523, "compression_ratio": 1.2596153846153846, "no_speech_prob": 1.2218953997944482e-05}, {"id": 1230, "seek": 676372, "start": 6777.08, "end": 6784.0, "text": " We've got Hitchcock.", "tokens": [492, 600, 658, 389, 1549, 29779, 13], "temperature": 0.0, "avg_logprob": -0.21937414169311523, "compression_ratio": 1.2596153846153846, "no_speech_prob": 1.2218953997944482e-05}, {"id": 1231, "seek": 676372, "start": 6784.0, "end": 6787.280000000001, "text": " We've got Jeremy Howard.", "tokens": [492, 600, 658, 17809, 17626, 13], "temperature": 0.0, "avg_logprob": -0.21937414169311523, "compression_ratio": 1.2596153846153846, "no_speech_prob": 1.2218953997944482e-05}, {"id": 1232, "seek": 676372, "start": 6787.280000000001, "end": 6793.280000000001, "text": " And we've got FastAI logo.", "tokens": [400, 321, 600, 658, 15968, 48698, 9699, 13], "temperature": 0.0, "avg_logprob": -0.21937414169311523, "compression_ratio": 1.2596153846153846, "no_speech_prob": 1.2218953997944482e-05}, {"id": 1233, "seek": 679328, "start": 6793.28, "end": 6808.5599999999995, "text": " Now ideally when we pass the graceful swan through our model, what we'd like is that", "tokens": [823, 22915, 562, 321, 1320, 264, 10042, 906, 1693, 282, 807, 527, 2316, 11, 437, 321, 1116, 411, 307, 300], "temperature": 0.0, "avg_logprob": -0.09904444735983144, "compression_ratio": 1.3865546218487395, "no_speech_prob": 4.029417141282465e-06}, {"id": 1234, "seek": 679328, "start": 6808.5599999999995, "end": 6818.759999999999, "text": " it creates a set of embeddings that are a good match for the text graceful swan.", "tokens": [309, 7829, 257, 992, 295, 12240, 29432, 300, 366, 257, 665, 2995, 337, 264, 2487, 10042, 906, 1693, 282, 13], "temperature": 0.0, "avg_logprob": -0.09904444735983144, "compression_ratio": 1.3865546218487395, "no_speech_prob": 4.029417141282465e-06}, {"id": 1235, "seek": 681876, "start": 6818.76, "end": 6824.4800000000005, "text": " When we pass the scene from Hitchcock through our image model, we would like it to return", "tokens": [1133, 321, 1320, 264, 4145, 490, 389, 1549, 29779, 807, 527, 3256, 2316, 11, 321, 576, 411, 309, 281, 2736], "temperature": 0.0, "avg_logprob": -0.10883845601763044, "compression_ratio": 1.7567567567567568, "no_speech_prob": 2.225264097432955e-06}, {"id": 1236, "seek": 681876, "start": 6824.4800000000005, "end": 6829.92, "text": " embeddings which are similar to the embeddings for the text scene from Hitchcock.", "tokens": [12240, 29432, 597, 366, 2531, 281, 264, 12240, 29432, 337, 264, 2487, 4145, 490, 389, 1549, 29779, 13], "temperature": 0.0, "avg_logprob": -0.10883845601763044, "compression_ratio": 1.7567567567567568, "no_speech_prob": 2.225264097432955e-06}, {"id": 1237, "seek": 681876, "start": 6829.92, "end": 6833.72, "text": " And ditto for the picture of Jeremy Howard versus the name Jeremy Howard and ditto for", "tokens": [400, 274, 34924, 337, 264, 3036, 295, 17809, 17626, 5717, 264, 1315, 17809, 17626, 293, 274, 34924, 337], "temperature": 0.0, "avg_logprob": -0.10883845601763044, "compression_ratio": 1.7567567567567568, "no_speech_prob": 2.225264097432955e-06}, {"id": 1238, "seek": 681876, "start": 6833.72, "end": 6840.0, "text": " the image FastAI and, sorry, FastAI logo and the word FastAI logo.", "tokens": [264, 3256, 15968, 48698, 293, 11, 2597, 11, 15968, 48698, 9699, 293, 264, 1349, 15968, 48698, 9699, 13], "temperature": 0.0, "avg_logprob": -0.10883845601763044, "compression_ratio": 1.7567567567567568, "no_speech_prob": 2.225264097432955e-06}, {"id": 1239, "seek": 684000, "start": 6840.0, "end": 6849.52, "text": " So in other words, for this particular combination here, we would like this one's features and", "tokens": [407, 294, 661, 2283, 11, 337, 341, 1729, 6562, 510, 11, 321, 576, 411, 341, 472, 311, 4122, 293], "temperature": 0.0, "avg_logprob": -0.1610829695215765, "compression_ratio": 1.4772727272727273, "no_speech_prob": 7.934474410831172e-07}, {"id": 1240, "seek": 684000, "start": 6849.52, "end": 6855.4, "text": " this one's features to be similar.", "tokens": [341, 472, 311, 4122, 281, 312, 2531, 13], "temperature": 0.0, "avg_logprob": -0.1610829695215765, "compression_ratio": 1.4772727272727273, "no_speech_prob": 7.934474410831172e-07}, {"id": 1241, "seek": 684000, "start": 6855.4, "end": 6858.48, "text": " So how do we tell if two sets of things are similar?", "tokens": [407, 577, 360, 321, 980, 498, 732, 6352, 295, 721, 366, 2531, 30], "temperature": 0.0, "avg_logprob": -0.1610829695215765, "compression_ratio": 1.4772727272727273, "no_speech_prob": 7.934474410831172e-07}, {"id": 1242, "seek": 684000, "start": 6858.48, "end": 6861.48, "text": " Two vectors.", "tokens": [4453, 18875, 13], "temperature": 0.0, "avg_logprob": -0.1610829695215765, "compression_ratio": 1.4772727272727273, "no_speech_prob": 7.934474410831172e-07}, {"id": 1243, "seek": 686148, "start": 6861.48, "end": 6872.32, "text": " Well, what we can do is we can simply multiply them together element-wise and add them up.", "tokens": [1042, 11, 437, 321, 393, 360, 307, 321, 393, 2935, 12972, 552, 1214, 4478, 12, 3711, 293, 909, 552, 493, 13], "temperature": 0.0, "avg_logprob": -0.06823498163467799, "compression_ratio": 1.8294117647058823, "no_speech_prob": 1.9033744820262655e-06}, {"id": 1244, "seek": 686148, "start": 6872.32, "end": 6880.799999999999, "text": " And this thing here is called the dot product.", "tokens": [400, 341, 551, 510, 307, 1219, 264, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.06823498163467799, "compression_ratio": 1.8294117647058823, "no_speech_prob": 1.9033744820262655e-06}, {"id": 1245, "seek": 686148, "start": 6880.799999999999, "end": 6884.919999999999, "text": " And so we could take the dot product of the features from the image model for this one", "tokens": [400, 370, 321, 727, 747, 264, 5893, 1674, 295, 264, 4122, 490, 264, 3256, 2316, 337, 341, 472], "temperature": 0.0, "avg_logprob": -0.06823498163467799, "compression_ratio": 1.8294117647058823, "no_speech_prob": 1.9033744820262655e-06}, {"id": 1246, "seek": 686148, "start": 6884.919999999999, "end": 6890.5199999999995, "text": " and the dot product of the features from the text model for the word graceful swan and", "tokens": [293, 264, 5893, 1674, 295, 264, 4122, 490, 264, 2487, 2316, 337, 264, 1349, 10042, 906, 1693, 282, 293], "temperature": 0.0, "avg_logprob": -0.06823498163467799, "compression_ratio": 1.8294117647058823, "no_speech_prob": 1.9033744820262655e-06}, {"id": 1247, "seek": 689052, "start": 6890.52, "end": 6892.080000000001, "text": " take their dot product.", "tokens": [747, 641, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.13698232788400552, "compression_ratio": 1.7574257425742574, "no_speech_prob": 3.3931237339857034e-06}, {"id": 1248, "seek": 689052, "start": 6892.080000000001, "end": 6899.4800000000005, "text": " And we want that number to be nice and big.", "tokens": [400, 321, 528, 300, 1230, 281, 312, 1481, 293, 955, 13], "temperature": 0.0, "avg_logprob": -0.13698232788400552, "compression_ratio": 1.7574257425742574, "no_speech_prob": 3.3931237339857034e-06}, {"id": 1249, "seek": 689052, "start": 6899.4800000000005, "end": 6906.200000000001, "text": " And the scene from Hitchcock's features should be very similar to the text scene from Hitchcock's", "tokens": [400, 264, 4145, 490, 389, 1549, 29779, 311, 4122, 820, 312, 588, 2531, 281, 264, 2487, 4145, 490, 389, 1549, 29779, 311], "temperature": 0.0, "avg_logprob": -0.13698232788400552, "compression_ratio": 1.7574257425742574, "no_speech_prob": 3.3931237339857034e-06}, {"id": 1250, "seek": 689052, "start": 6906.200000000001, "end": 6907.200000000001, "text": " features.", "tokens": [4122, 13], "temperature": 0.0, "avg_logprob": -0.13698232788400552, "compression_ratio": 1.7574257425742574, "no_speech_prob": 3.3931237339857034e-06}, {"id": 1251, "seek": 689052, "start": 6907.200000000001, "end": 6910.320000000001, "text": " So we want their product to be nice and big.", "tokens": [407, 321, 528, 641, 1674, 281, 312, 1481, 293, 955, 13], "temperature": 0.0, "avg_logprob": -0.13698232788400552, "compression_ratio": 1.7574257425742574, "no_speech_prob": 3.3931237339857034e-06}, {"id": 1252, "seek": 689052, "start": 6910.320000000001, "end": 6912.76, "text": " And ditto for everything on this diagonal.", "tokens": [400, 274, 34924, 337, 1203, 322, 341, 21539, 13], "temperature": 0.0, "avg_logprob": -0.13698232788400552, "compression_ratio": 1.7574257425742574, "no_speech_prob": 3.3931237339857034e-06}, {"id": 1253, "seek": 689052, "start": 6912.76, "end": 6920.360000000001, "text": " Now, on the other hand, a graceful swan picture should not have embeddings that are similar", "tokens": [823, 11, 322, 264, 661, 1011, 11, 257, 10042, 906, 1693, 282, 3036, 820, 406, 362, 12240, 29432, 300, 366, 2531], "temperature": 0.0, "avg_logprob": -0.13698232788400552, "compression_ratio": 1.7574257425742574, "no_speech_prob": 3.3931237339857034e-06}, {"id": 1254, "seek": 692036, "start": 6920.36, "end": 6924.12, "text": " to the text as seen from Hitchcock.", "tokens": [281, 264, 2487, 382, 1612, 490, 389, 1549, 29779, 13], "temperature": 0.0, "avg_logprob": -0.10716033975283305, "compression_ratio": 1.2777777777777777, "no_speech_prob": 7.527938578277826e-06}, {"id": 1255, "seek": 692036, "start": 6924.12, "end": 6926.839999999999, "text": " So that should be nice and small.", "tokens": [407, 300, 820, 312, 1481, 293, 1359, 13], "temperature": 0.0, "avg_logprob": -0.10716033975283305, "compression_ratio": 1.2777777777777777, "no_speech_prob": 7.527938578277826e-06}, {"id": 1256, "seek": 692036, "start": 6926.839999999999, "end": 6934.12, "text": " And ditto for everything else off diagonal.", "tokens": [400, 274, 34924, 337, 1203, 1646, 766, 21539, 13], "temperature": 0.0, "avg_logprob": -0.10716033975283305, "compression_ratio": 1.2777777777777777, "no_speech_prob": 7.527938578277826e-06}, {"id": 1257, "seek": 692036, "start": 6934.12, "end": 6939.2, "text": " And so perhaps you can see where this is going.", "tokens": [400, 370, 4317, 291, 393, 536, 689, 341, 307, 516, 13], "temperature": 0.0, "avg_logprob": -0.10716033975283305, "compression_ratio": 1.2777777777777777, "no_speech_prob": 7.527938578277826e-06}, {"id": 1258, "seek": 693920, "start": 6939.2, "end": 6960.8, "text": " If we add up all of these, add those all together and then subtract all of these, we have a", "tokens": [759, 321, 909, 493, 439, 295, 613, 11, 909, 729, 439, 1214, 293, 550, 16390, 439, 295, 613, 11, 321, 362, 257], "temperature": 0.0, "avg_logprob": -0.10980707599270728, "compression_ratio": 1.2619047619047619, "no_speech_prob": 8.579224868299207e-07}, {"id": 1259, "seek": 693920, "start": 6960.8, "end": 6965.48, "text": " loss function.", "tokens": [4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.10980707599270728, "compression_ratio": 1.2619047619047619, "no_speech_prob": 8.579224868299207e-07}, {"id": 1260, "seek": 696548, "start": 6965.48, "end": 6971.879999999999, "text": " And so if we want this loss function to be good, then we're going to need the weights", "tokens": [400, 370, 498, 321, 528, 341, 4470, 2445, 281, 312, 665, 11, 550, 321, 434, 516, 281, 643, 264, 17443], "temperature": 0.0, "avg_logprob": -0.10697852240668403, "compression_ratio": 1.738888888888889, "no_speech_prob": 1.90337425465259e-06}, {"id": 1261, "seek": 696548, "start": 6971.879999999999, "end": 6982.16, "text": " of our model for the text encoder to spit out embeddings that are very similar to the", "tokens": [295, 527, 2316, 337, 264, 2487, 2058, 19866, 281, 22127, 484, 12240, 29432, 300, 366, 588, 2531, 281, 264], "temperature": 0.0, "avg_logprob": -0.10697852240668403, "compression_ratio": 1.738888888888889, "no_speech_prob": 1.90337425465259e-06}, {"id": 1262, "seek": 696548, "start": 6982.16, "end": 6984.66, "text": " images that they're paired with.", "tokens": [5267, 300, 436, 434, 25699, 365, 13], "temperature": 0.0, "avg_logprob": -0.10697852240668403, "compression_ratio": 1.738888888888889, "no_speech_prob": 1.90337425465259e-06}, {"id": 1263, "seek": 696548, "start": 6984.66, "end": 6991.12, "text": " And we need them to spit out embed features for things that they are not paired with,", "tokens": [400, 321, 643, 552, 281, 22127, 484, 12240, 4122, 337, 721, 300, 436, 366, 406, 25699, 365, 11], "temperature": 0.0, "avg_logprob": -0.10697852240668403, "compression_ratio": 1.738888888888889, "no_speech_prob": 1.90337425465259e-06}, {"id": 1264, "seek": 696548, "start": 6991.12, "end": 6994.099999999999, "text": " which are not similar.", "tokens": [597, 366, 406, 2531, 13], "temperature": 0.0, "avg_logprob": -0.10697852240668403, "compression_ratio": 1.738888888888889, "no_speech_prob": 1.90337425465259e-06}, {"id": 1265, "seek": 699410, "start": 6994.1, "end": 6999.4400000000005, "text": " And so if we can do that, then we're going to end up with a text encoder that we can", "tokens": [400, 370, 498, 321, 393, 360, 300, 11, 550, 321, 434, 516, 281, 917, 493, 365, 257, 2487, 2058, 19866, 300, 321, 393], "temperature": 0.0, "avg_logprob": -0.07847644858164331, "compression_ratio": 1.5976331360946745, "no_speech_prob": 4.2893016143352725e-06}, {"id": 1266, "seek": 699410, "start": 6999.4400000000005, "end": 7013.6, "text": " feed in things like a graceful swan, some beautiful swan, such a lovely swan.", "tokens": [3154, 294, 721, 411, 257, 10042, 906, 1693, 282, 11, 512, 2238, 1693, 282, 11, 1270, 257, 7496, 1693, 282, 13], "temperature": 0.0, "avg_logprob": -0.07847644858164331, "compression_ratio": 1.5976331360946745, "no_speech_prob": 4.2893016143352725e-06}, {"id": 1267, "seek": 699410, "start": 7013.6, "end": 7020.04, "text": " And these should all give very similar embeddings, because these would all represent very similar", "tokens": [400, 613, 820, 439, 976, 588, 2531, 12240, 29432, 11, 570, 613, 576, 439, 2906, 588, 2531], "temperature": 0.0, "avg_logprob": -0.07847644858164331, "compression_ratio": 1.5976331360946745, "no_speech_prob": 4.2893016143352725e-06}, {"id": 1268, "seek": 699410, "start": 7020.04, "end": 7022.92, "text": " pictures.", "tokens": [5242, 13], "temperature": 0.0, "avg_logprob": -0.07847644858164331, "compression_ratio": 1.5976331360946745, "no_speech_prob": 4.2893016143352725e-06}, {"id": 1269, "seek": 702292, "start": 7022.92, "end": 7034.4800000000005, "text": " And so what we've now done is we've successfully created two models that together put text", "tokens": [400, 370, 437, 321, 600, 586, 1096, 307, 321, 600, 10727, 2942, 732, 5245, 300, 1214, 829, 2487], "temperature": 0.0, "avg_logprob": -0.05249224259303166, "compression_ratio": 1.4113475177304964, "no_speech_prob": 2.561274186518858e-06}, {"id": 1270, "seek": 702292, "start": 7034.4800000000005, "end": 7037.68, "text": " and images into the same space.", "tokens": [293, 5267, 666, 264, 912, 1901, 13], "temperature": 0.0, "avg_logprob": -0.05249224259303166, "compression_ratio": 1.4113475177304964, "no_speech_prob": 2.561274186518858e-06}, {"id": 1271, "seek": 702292, "start": 7037.68, "end": 7048.4, "text": " So we've got this multimodal set of models, which is exactly what we wanted.", "tokens": [407, 321, 600, 658, 341, 32972, 378, 304, 992, 295, 5245, 11, 597, 307, 2293, 437, 321, 1415, 13], "temperature": 0.0, "avg_logprob": -0.05249224259303166, "compression_ratio": 1.4113475177304964, "no_speech_prob": 2.561274186518858e-06}, {"id": 1272, "seek": 704840, "start": 7048.4, "end": 7066.96, "text": " So now we can take our cute teddy bear, feed it in here, get out some features.", "tokens": [407, 586, 321, 393, 747, 527, 4052, 45116, 6155, 11, 3154, 309, 294, 510, 11, 483, 484, 512, 4122, 13], "temperature": 0.0, "avg_logprob": -0.1265053219265408, "compression_ratio": 1.346774193548387, "no_speech_prob": 1.3709452559851343e-06}, {"id": 1273, "seek": 704840, "start": 7066.96, "end": 7076.599999999999, "text": " And that is what we will use instead of these one hot encoded vectors when we train our", "tokens": [400, 300, 307, 437, 321, 486, 764, 2602, 295, 613, 472, 2368, 2058, 12340, 18875, 562, 321, 3847, 527], "temperature": 0.0, "avg_logprob": -0.1265053219265408, "compression_ratio": 1.346774193548387, "no_speech_prob": 1.3709452559851343e-06}, {"id": 1274, "seek": 707660, "start": 7076.6, "end": 7083.84, "text": " photo or painting or whatever unit.", "tokens": [5052, 420, 5370, 420, 2035, 4985, 13], "temperature": 0.0, "avg_logprob": -0.08745324933851087, "compression_ratio": 1.5944444444444446, "no_speech_prob": 2.090445605062996e-06}, {"id": 1275, "seek": 707660, "start": 7083.84, "end": 7085.96, "text": " And then we can do exactly the same thing with guidance.", "tokens": [400, 550, 321, 393, 360, 2293, 264, 912, 551, 365, 10056, 13], "temperature": 0.0, "avg_logprob": -0.08745324933851087, "compression_ratio": 1.5944444444444446, "no_speech_prob": 2.090445605062996e-06}, {"id": 1276, "seek": 707660, "start": 7085.96, "end": 7094.04, "text": " We can now pass in the text encoder's feature vector for a cute teddy, and it is going to", "tokens": [492, 393, 586, 1320, 294, 264, 2487, 2058, 19866, 311, 4111, 8062, 337, 257, 4052, 45116, 11, 293, 309, 307, 516, 281], "temperature": 0.0, "avg_logprob": -0.08745324933851087, "compression_ratio": 1.5944444444444446, "no_speech_prob": 2.090445605062996e-06}, {"id": 1277, "seek": 707660, "start": 7094.04, "end": 7102.72, "text": " turn the noise into something that is similar to things that it's previously seen that are", "tokens": [1261, 264, 5658, 666, 746, 300, 307, 2531, 281, 721, 300, 309, 311, 8046, 1612, 300, 366], "temperature": 0.0, "avg_logprob": -0.08745324933851087, "compression_ratio": 1.5944444444444446, "no_speech_prob": 2.090445605062996e-06}, {"id": 1278, "seek": 707660, "start": 7102.72, "end": 7105.68, "text": " cute teddies.", "tokens": [4052, 22337, 22018, 13], "temperature": 0.0, "avg_logprob": -0.08745324933851087, "compression_ratio": 1.5944444444444446, "no_speech_prob": 2.090445605062996e-06}, {"id": 1279, "seek": 710568, "start": 7105.68, "end": 7115.8, "text": " So the model that's used, or the pair of models that's used here, is called PLEP.", "tokens": [407, 264, 2316, 300, 311, 1143, 11, 420, 264, 6119, 295, 5245, 300, 311, 1143, 510, 11, 307, 1219, 430, 2634, 47, 13], "temperature": 0.0, "avg_logprob": -0.12485441565513611, "compression_ratio": 1.483221476510067, "no_speech_prob": 2.8130023110861657e-06}, {"id": 1280, "seek": 710568, "start": 7115.8, "end": 7121.0, "text": " This thing where we want these to be bigger and these to be smaller is called a contrastive", "tokens": [639, 551, 689, 321, 528, 613, 281, 312, 3801, 293, 613, 281, 312, 4356, 307, 1219, 257, 8712, 488], "temperature": 0.0, "avg_logprob": -0.12485441565513611, "compression_ratio": 1.483221476510067, "no_speech_prob": 2.8130023110861657e-06}, {"id": 1281, "seek": 710568, "start": 7121.0, "end": 7127.34, "text": " loss.", "tokens": [4470, 13], "temperature": 0.0, "avg_logprob": -0.12485441565513611, "compression_ratio": 1.483221476510067, "no_speech_prob": 2.8130023110861657e-06}, {"id": 1282, "seek": 710568, "start": 7127.34, "end": 7133.8, "text": " And now you know where the CL comes from.", "tokens": [400, 586, 291, 458, 689, 264, 12855, 1487, 490, 13], "temperature": 0.0, "avg_logprob": -0.12485441565513611, "compression_ratio": 1.483221476510067, "no_speech_prob": 2.8130023110861657e-06}, {"id": 1283, "seek": 713380, "start": 7133.8, "end": 7148.0, "text": " So here we have a CLP text encoder.", "tokens": [407, 510, 321, 362, 257, 12855, 47, 2487, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.27379280870611017, "compression_ratio": 1.2156862745098038, "no_speech_prob": 2.0904351458739256e-06}, {"id": 1284, "seek": 713380, "start": 7148.0, "end": 7154.64, "text": " Its input is some text.", "tokens": [6953, 4846, 307, 512, 2487, 13], "temperature": 0.0, "avg_logprob": -0.27379280870611017, "compression_ratio": 1.2156862745098038, "no_speech_prob": 2.0904351458739256e-06}, {"id": 1285, "seek": 713380, "start": 7154.64, "end": 7157.96, "text": " Its output is, we call it an embedding.", "tokens": [6953, 5598, 307, 11, 321, 818, 309, 364, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.27379280870611017, "compression_ratio": 1.2156862745098038, "no_speech_prob": 2.0904351458739256e-06}, {"id": 1286, "seek": 713380, "start": 7157.96, "end": 7160.96, "text": " It's just some features.", "tokens": [467, 311, 445, 512, 4122, 13], "temperature": 0.0, "avg_logprob": -0.27379280870611017, "compression_ratio": 1.2156862745098038, "no_speech_prob": 2.0904351458739256e-06}, {"id": 1287, "seek": 716096, "start": 7160.96, "end": 7165.76, "text": " Oops, embedding.", "tokens": [21726, 11, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.43765379587809244, "compression_ratio": 1.2530120481927711, "no_speech_prob": 3.59104888048023e-05}, {"id": 1288, "seek": 716096, "start": 7165.76, "end": 7176.04, "text": " Where similar sets of text with similar meanings will give us similar embeddings.", "tokens": [2305, 2531, 6352, 295, 2487, 365, 2531, 28138, 486, 976, 505, 2531, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.43765379587809244, "compression_ratio": 1.2530120481927711, "no_speech_prob": 3.59104888048023e-05}, {"id": 1289, "seek": 716096, "start": 7176.04, "end": 7178.04, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.43765379587809244, "compression_ratio": 1.2530120481927711, "no_speech_prob": 3.59104888048023e-05}, {"id": 1290, "seek": 717804, "start": 7178.04, "end": 7194.6, "text": " We need a bit more space, we're nearly done.", "tokens": [492, 643, 257, 857, 544, 1901, 11, 321, 434, 6217, 1096, 13], "temperature": 0.0, "avg_logprob": -0.12058087757655553, "compression_ratio": 1.523076923076923, "no_speech_prob": 2.4824571482895408e-06}, {"id": 1291, "seek": 717804, "start": 7194.6, "end": 7202.08, "text": " So we've got the unit that can denoise latency into unnoisy latency, including pure noise.", "tokens": [407, 321, 600, 658, 264, 4985, 300, 393, 1441, 38800, 27043, 666, 517, 1771, 14169, 27043, 11, 3009, 6075, 5658, 13], "temperature": 0.0, "avg_logprob": -0.12058087757655553, "compression_ratio": 1.523076923076923, "no_speech_prob": 2.4824571482895408e-06}, {"id": 1292, "seek": 717804, "start": 7202.08, "end": 7205.32, "text": " We've got a decoder that can take latency and create an image.", "tokens": [492, 600, 658, 257, 979, 19866, 300, 393, 747, 27043, 293, 1884, 364, 3256, 13], "temperature": 0.0, "avg_logprob": -0.12058087757655553, "compression_ratio": 1.523076923076923, "no_speech_prob": 2.4824571482895408e-06}, {"id": 1293, "seek": 720532, "start": 7205.32, "end": 7214.88, "text": " We've got a text encoder which can allow us to train a unit which is guided by captions.", "tokens": [492, 600, 658, 257, 2487, 2058, 19866, 597, 393, 2089, 505, 281, 3847, 257, 4985, 597, 307, 19663, 538, 44832, 13], "temperature": 0.0, "avg_logprob": -0.09350510438283284, "compression_ratio": 1.5449438202247192, "no_speech_prob": 8.446194215139258e-07}, {"id": 1294, "seek": 720532, "start": 7214.88, "end": 7224.719999999999, "text": " So the last thing we need is the question about how exactly do we do this inference", "tokens": [407, 264, 1036, 551, 321, 643, 307, 264, 1168, 466, 577, 2293, 360, 321, 360, 341, 38253], "temperature": 0.0, "avg_logprob": -0.09350510438283284, "compression_ratio": 1.5449438202247192, "no_speech_prob": 8.446194215139258e-07}, {"id": 1295, "seek": 720532, "start": 7224.719999999999, "end": 7228.759999999999, "text": " process here?", "tokens": [1399, 510, 30], "temperature": 0.0, "avg_logprob": -0.09350510438283284, "compression_ratio": 1.5449438202247192, "no_speech_prob": 8.446194215139258e-07}, {"id": 1296, "seek": 720532, "start": 7228.759999999999, "end": 7235.28, "text": " So how exactly, once we've got something that gives us the gradients we want, and by the", "tokens": [407, 577, 2293, 11, 1564, 321, 600, 658, 746, 300, 2709, 505, 264, 2771, 2448, 321, 528, 11, 293, 538, 264], "temperature": 0.0, "avg_logprob": -0.09350510438283284, "compression_ratio": 1.5449438202247192, "no_speech_prob": 8.446194215139258e-07}, {"id": 1297, "seek": 723528, "start": 7235.28, "end": 7246.639999999999, "text": " way these gradients are often called the score function, just in case you come across that.", "tokens": [636, 613, 2771, 2448, 366, 2049, 1219, 264, 6175, 2445, 11, 445, 294, 1389, 291, 808, 2108, 300, 13], "temperature": 0.0, "avg_logprob": -0.17999207455178964, "compression_ratio": 1.2781954887218046, "no_speech_prob": 6.681503350591811e-07}, {"id": 1298, "seek": 723528, "start": 7246.639999999999, "end": 7248.12, "text": " That's all that's referring to.", "tokens": [663, 311, 439, 300, 311, 13761, 281, 13], "temperature": 0.0, "avg_logprob": -0.17999207455178964, "compression_ratio": 1.2781954887218046, "no_speech_prob": 6.681503350591811e-07}, {"id": 1299, "seek": 723528, "start": 7248.12, "end": 7252.759999999999, "text": " Yeah, how exactly do we go about this process?", "tokens": [865, 11, 577, 2293, 360, 321, 352, 466, 341, 1399, 30], "temperature": 0.0, "avg_logprob": -0.17999207455178964, "compression_ratio": 1.2781954887218046, "no_speech_prob": 6.681503350591811e-07}, {"id": 1300, "seek": 725276, "start": 7252.76, "end": 7267.52, "text": " And unfortunately the language used around this is weird and confusing.", "tokens": [400, 7015, 264, 2856, 1143, 926, 341, 307, 3657, 293, 13181, 13], "temperature": 0.0, "avg_logprob": -0.10399887959162395, "compression_ratio": 1.514018691588785, "no_speech_prob": 2.5612639547034632e-06}, {"id": 1301, "seek": 725276, "start": 7267.52, "end": 7273.56, "text": " And so ideally you will learn to ignore the fact that the language is weird and confusing.", "tokens": [400, 370, 22915, 291, 486, 1466, 281, 11200, 264, 1186, 300, 264, 2856, 307, 3657, 293, 13181, 13], "temperature": 0.0, "avg_logprob": -0.10399887959162395, "compression_ratio": 1.514018691588785, "no_speech_prob": 2.5612639547034632e-06}, {"id": 1302, "seek": 727356, "start": 7273.56, "end": 7283.080000000001, "text": " And in particular the language you'll see a lot talks about time steps.", "tokens": [400, 294, 1729, 264, 2856, 291, 603, 536, 257, 688, 6686, 466, 565, 4439, 13], "temperature": 0.0, "avg_logprob": -0.10845987102653407, "compression_ratio": 1.6130653266331658, "no_speech_prob": 4.029394858662272e-06}, {"id": 1303, "seek": 727356, "start": 7283.080000000001, "end": 7288.4400000000005, "text": " And you'll notice that during our training process we never used any concept of time", "tokens": [400, 291, 603, 3449, 300, 1830, 527, 3097, 1399, 321, 1128, 1143, 604, 3410, 295, 565], "temperature": 0.0, "avg_logprob": -0.10845987102653407, "compression_ratio": 1.6130653266331658, "no_speech_prob": 4.029394858662272e-06}, {"id": 1304, "seek": 727356, "start": 7288.4400000000005, "end": 7290.320000000001, "text": " steps.", "tokens": [4439, 13], "temperature": 0.0, "avg_logprob": -0.10845987102653407, "compression_ratio": 1.6130653266331658, "no_speech_prob": 4.029394858662272e-06}, {"id": 1305, "seek": 727356, "start": 7290.320000000001, "end": 7297.4800000000005, "text": " This is basically an overhang from the particular way in which the math was formulated in the", "tokens": [639, 307, 1936, 364, 670, 23850, 490, 264, 1729, 636, 294, 597, 264, 5221, 390, 48936, 294, 264], "temperature": 0.0, "avg_logprob": -0.10845987102653407, "compression_ratio": 1.6130653266331658, "no_speech_prob": 4.029394858662272e-06}, {"id": 1306, "seek": 727356, "start": 7297.4800000000005, "end": 7298.4800000000005, "text": " first papers.", "tokens": [700, 10577, 13], "temperature": 0.0, "avg_logprob": -0.10845987102653407, "compression_ratio": 1.6130653266331658, "no_speech_prob": 4.029394858662272e-06}, {"id": 1307, "seek": 727356, "start": 7298.4800000000005, "end": 7302.8, "text": " There are lots of other ways we can formulate it.", "tokens": [821, 366, 3195, 295, 661, 2098, 321, 393, 47881, 309, 13], "temperature": 0.0, "avg_logprob": -0.10845987102653407, "compression_ratio": 1.6130653266331658, "no_speech_prob": 4.029394858662272e-06}, {"id": 1308, "seek": 730280, "start": 7302.8, "end": 7307.96, "text": " And during the course on the whole we will avoid using the term time steps.", "tokens": [400, 1830, 264, 1164, 322, 264, 1379, 321, 486, 5042, 1228, 264, 1433, 565, 4439, 13], "temperature": 0.0, "avg_logprob": -0.07950224774949094, "compression_ratio": 1.7209302325581395, "no_speech_prob": 2.1444287995109335e-05}, {"id": 1309, "seek": 730280, "start": 7307.96, "end": 7314.64, "text": " But to see what time steps are, even though it's got nothing to do with time in real life,", "tokens": [583, 281, 536, 437, 565, 4439, 366, 11, 754, 1673, 309, 311, 658, 1825, 281, 360, 365, 565, 294, 957, 993, 11], "temperature": 0.0, "avg_logprob": -0.07950224774949094, "compression_ratio": 1.7209302325581395, "no_speech_prob": 2.1444287995109335e-05}, {"id": 1310, "seek": 730280, "start": 7314.64, "end": 7318.6, "text": " consider the fact that we used varying levels of noise.", "tokens": [1949, 264, 1186, 300, 321, 1143, 22984, 4358, 295, 5658, 13], "temperature": 0.0, "avg_logprob": -0.07950224774949094, "compression_ratio": 1.7209302325581395, "no_speech_prob": 2.1444287995109335e-05}, {"id": 1311, "seek": 730280, "start": 7318.6, "end": 7324.16, "text": " Some things were very noisy, some things were not noisy at all, some things had no noise,", "tokens": [2188, 721, 645, 588, 24518, 11, 512, 721, 645, 406, 24518, 412, 439, 11, 512, 721, 632, 572, 5658, 11], "temperature": 0.0, "avg_logprob": -0.07950224774949094, "compression_ratio": 1.7209302325581395, "no_speech_prob": 2.1444287995109335e-05}, {"id": 1312, "seek": 730280, "start": 7324.16, "end": 7329.08, "text": " and some I haven't drawn here would have been pure noise.", "tokens": [293, 512, 286, 2378, 380, 10117, 510, 576, 362, 668, 6075, 5658, 13], "temperature": 0.0, "avg_logprob": -0.07950224774949094, "compression_ratio": 1.7209302325581395, "no_speech_prob": 2.1444287995109335e-05}, {"id": 1313, "seek": 732908, "start": 7329.08, "end": 7337.48, "text": " You could basically create a kind of a noising schedule where along here you could put say", "tokens": [509, 727, 1936, 1884, 257, 733, 295, 257, 572, 3436, 7567, 689, 2051, 510, 291, 727, 829, 584], "temperature": 0.0, "avg_logprob": -0.15961799068727356, "compression_ratio": 1.5914634146341464, "no_speech_prob": 8.800890100246761e-06}, {"id": 1314, "seek": 732908, "start": 7337.48, "end": 7345.72, "text": " the numbers from 1 to 1000.", "tokens": [264, 3547, 490, 502, 281, 9714, 13], "temperature": 0.0, "avg_logprob": -0.15961799068727356, "compression_ratio": 1.5914634146341464, "no_speech_prob": 8.800890100246761e-06}, {"id": 1315, "seek": 732908, "start": 7345.72, "end": 7350.24, "text": " And you could then say oh you know and we'll call this t.", "tokens": [400, 291, 727, 550, 584, 1954, 291, 458, 293, 321, 603, 818, 341, 256, 13], "temperature": 0.0, "avg_logprob": -0.15961799068727356, "compression_ratio": 1.5914634146341464, "no_speech_prob": 8.800890100246761e-06}, {"id": 1316, "seek": 732908, "start": 7350.24, "end": 7357.08, "text": " And maybe we randomly pick a number from 1 to 1000 and then we look up on this noise", "tokens": [400, 1310, 321, 16979, 1888, 257, 1230, 490, 502, 281, 9714, 293, 550, 321, 574, 493, 322, 341, 5658], "temperature": 0.0, "avg_logprob": -0.15961799068727356, "compression_ratio": 1.5914634146341464, "no_speech_prob": 8.800890100246761e-06}, {"id": 1317, "seek": 735708, "start": 7357.08, "end": 7364.12, "text": " schedule which would be some monotonically decreasing function.", "tokens": [7567, 597, 576, 312, 512, 1108, 27794, 984, 23223, 2445, 13], "temperature": 0.0, "avg_logprob": -0.14024360091597945, "compression_ratio": 1.566137566137566, "no_speech_prob": 6.5404065026086755e-06}, {"id": 1318, "seek": 735708, "start": 7364.12, "end": 7368.68, "text": " And we'd look up, let's say we happen to pick randomly a number 4, we would look up here", "tokens": [400, 321, 1116, 574, 493, 11, 718, 311, 584, 321, 1051, 281, 1888, 16979, 257, 1230, 1017, 11, 321, 576, 574, 493, 510], "temperature": 0.0, "avg_logprob": -0.14024360091597945, "compression_ratio": 1.566137566137566, "no_speech_prob": 6.5404065026086755e-06}, {"id": 1319, "seek": 735708, "start": 7368.68, "end": 7370.44, "text": " to find where that is.", "tokens": [281, 915, 689, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.14024360091597945, "compression_ratio": 1.566137566137566, "no_speech_prob": 6.5404065026086755e-06}, {"id": 1320, "seek": 735708, "start": 7370.44, "end": 7377.32, "text": " We'd look over here and this would return to us some sigma which is the amount of noise", "tokens": [492, 1116, 574, 670, 510, 293, 341, 576, 2736, 281, 505, 512, 12771, 597, 307, 264, 2372, 295, 5658], "temperature": 0.0, "avg_logprob": -0.14024360091597945, "compression_ratio": 1.566137566137566, "no_speech_prob": 6.5404065026086755e-06}, {"id": 1321, "seek": 735708, "start": 7377.32, "end": 7382.5599999999995, "text": " to use if you happen to get a 4.", "tokens": [281, 764, 498, 291, 1051, 281, 483, 257, 1017, 13], "temperature": 0.0, "avg_logprob": -0.14024360091597945, "compression_ratio": 1.566137566137566, "no_speech_prob": 6.5404065026086755e-06}, {"id": 1322, "seek": 738256, "start": 7382.56, "end": 7387.240000000001, "text": " So if you happen to get a 1, you're going to get a whole lot of noise and if you happen", "tokens": [407, 498, 291, 1051, 281, 483, 257, 502, 11, 291, 434, 516, 281, 483, 257, 1379, 688, 295, 5658, 293, 498, 291, 1051], "temperature": 0.0, "avg_logprob": -0.10624415533883232, "compression_ratio": 1.8534031413612566, "no_speech_prob": 8.315247441714746e-07}, {"id": 1323, "seek": 738256, "start": 7387.240000000001, "end": 7395.360000000001, "text": " to get a 1000, you're going to have hardly any noise.", "tokens": [281, 483, 257, 9714, 11, 291, 434, 516, 281, 362, 13572, 604, 5658, 13], "temperature": 0.0, "avg_logprob": -0.10624415533883232, "compression_ratio": 1.8534031413612566, "no_speech_prob": 8.315247441714746e-07}, {"id": 1324, "seek": 738256, "start": 7395.360000000001, "end": 7400.820000000001, "text": " So this is one way of picking, so remember when we were training we were going to pick", "tokens": [407, 341, 307, 472, 636, 295, 8867, 11, 370, 1604, 562, 321, 645, 3097, 321, 645, 516, 281, 1888], "temperature": 0.0, "avg_logprob": -0.10624415533883232, "compression_ratio": 1.8534031413612566, "no_speech_prob": 8.315247441714746e-07}, {"id": 1325, "seek": 738256, "start": 7400.820000000001, "end": 7403.6, "text": " for every image a random amount of noise.", "tokens": [337, 633, 3256, 257, 4974, 2372, 295, 5658, 13], "temperature": 0.0, "avg_logprob": -0.10624415533883232, "compression_ratio": 1.8534031413612566, "no_speech_prob": 8.315247441714746e-07}, {"id": 1326, "seek": 738256, "start": 7403.6, "end": 7407.88, "text": " So this would be one way to do that is to pick a random number from 1 to 1000, look", "tokens": [407, 341, 576, 312, 472, 636, 281, 360, 300, 307, 281, 1888, 257, 4974, 1230, 490, 502, 281, 9714, 11, 574], "temperature": 0.0, "avg_logprob": -0.10624415533883232, "compression_ratio": 1.8534031413612566, "no_speech_prob": 8.315247441714746e-07}, {"id": 1327, "seek": 740788, "start": 7407.88, "end": 7413.9400000000005, "text": " it up on this function and that tells us how much noise to use.", "tokens": [309, 493, 322, 341, 2445, 293, 300, 5112, 505, 577, 709, 5658, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.11622680761875251, "compression_ratio": 1.6358695652173914, "no_speech_prob": 6.144128747109789e-06}, {"id": 1328, "seek": 740788, "start": 7413.9400000000005, "end": 7421.04, "text": " So this t is what people refer to as the time step.", "tokens": [407, 341, 256, 307, 437, 561, 2864, 281, 382, 264, 565, 1823, 13], "temperature": 0.0, "avg_logprob": -0.11622680761875251, "compression_ratio": 1.6358695652173914, "no_speech_prob": 6.144128747109789e-06}, {"id": 1329, "seek": 740788, "start": 7421.04, "end": 7424.64, "text": " Nowadays you don't really have to do that that much and a lot of people are starting", "tokens": [28908, 291, 500, 380, 534, 362, 281, 360, 300, 300, 709, 293, 257, 688, 295, 561, 366, 2891], "temperature": 0.0, "avg_logprob": -0.11622680761875251, "compression_ratio": 1.6358695652173914, "no_speech_prob": 6.144128747109789e-06}, {"id": 1330, "seek": 740788, "start": 7424.64, "end": 7430.2, "text": " to get rid of this idea altogether and some people instead will simply say how much noise", "tokens": [281, 483, 3973, 295, 341, 1558, 19051, 293, 512, 561, 2602, 486, 2935, 584, 577, 709, 5658], "temperature": 0.0, "avg_logprob": -0.11622680761875251, "compression_ratio": 1.6358695652173914, "no_speech_prob": 6.144128747109789e-06}, {"id": 1331, "seek": 740788, "start": 7430.2, "end": 7433.36, "text": " was there.", "tokens": [390, 456, 13], "temperature": 0.0, "avg_logprob": -0.11622680761875251, "compression_ratio": 1.6358695652173914, "no_speech_prob": 6.144128747109789e-06}, {"id": 1332, "seek": 743336, "start": 7433.36, "end": 7438.4, "text": " Usually we would think of using sigma for standard deviations of Gaussians or normal", "tokens": [11419, 321, 576, 519, 295, 1228, 12771, 337, 3832, 31219, 763, 295, 10384, 2023, 2567, 420, 2710], "temperature": 0.0, "avg_logprob": -0.17466582750019274, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.1842670573969372e-05}, {"id": 1333, "seek": 743336, "start": 7438.4, "end": 7447.179999999999, "text": " distributions but actually much more common is to use the Greek letter beta.", "tokens": [37870, 457, 767, 709, 544, 2689, 307, 281, 764, 264, 10281, 5063, 9861, 13], "temperature": 0.0, "avg_logprob": -0.17466582750019274, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.1842670573969372e-05}, {"id": 1334, "seek": 743336, "start": 7447.179999999999, "end": 7451.2, "text": " And so if you see something talking about beta, they're just saying oh for that particular", "tokens": [400, 370, 498, 291, 536, 746, 1417, 466, 9861, 11, 436, 434, 445, 1566, 1954, 337, 300, 1729], "temperature": 0.0, "avg_logprob": -0.17466582750019274, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.1842670573969372e-05}, {"id": 1335, "seek": 743336, "start": 7451.2, "end": 7457.36, "text": " image when it was being trained, what standard deviation of noise was being used basically.", "tokens": [3256, 562, 309, 390, 885, 8895, 11, 437, 3832, 25163, 295, 5658, 390, 885, 1143, 1936, 13], "temperature": 0.0, "avg_logprob": -0.17466582750019274, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.1842670573969372e-05}, {"id": 1336, "seek": 745736, "start": 7457.36, "end": 7463.2, "text": " It's slightly hand wavy but close enough.", "tokens": [467, 311, 4748, 1011, 261, 15498, 457, 1998, 1547, 13], "temperature": 0.0, "avg_logprob": -0.14079986919056287, "compression_ratio": 1.7401129943502824, "no_speech_prob": 3.4465592761989683e-06}, {"id": 1337, "seek": 745736, "start": 7463.2, "end": 7469.719999999999, "text": " And so what you do each time you're going to create a mini batch to pass into your model,", "tokens": [400, 370, 437, 291, 360, 1184, 565, 291, 434, 516, 281, 1884, 257, 8382, 15245, 281, 1320, 666, 428, 2316, 11], "temperature": 0.0, "avg_logprob": -0.14079986919056287, "compression_ratio": 1.7401129943502824, "no_speech_prob": 3.4465592761989683e-06}, {"id": 1338, "seek": 745736, "start": 7469.719999999999, "end": 7476.36, "text": " you randomly pick an image from your training set, you randomly pick either an amount of", "tokens": [291, 16979, 1888, 364, 3256, 490, 428, 3097, 992, 11, 291, 16979, 1888, 2139, 364, 2372, 295], "temperature": 0.0, "avg_logprob": -0.14079986919056287, "compression_ratio": 1.7401129943502824, "no_speech_prob": 3.4465592761989683e-06}, {"id": 1339, "seek": 745736, "start": 7476.36, "end": 7482.759999999999, "text": " noise or some models you randomly pick a t and then look up an amount of noise and then", "tokens": [5658, 420, 512, 5245, 291, 16979, 1888, 257, 256, 293, 550, 574, 493, 364, 2372, 295, 5658, 293, 550], "temperature": 0.0, "avg_logprob": -0.14079986919056287, "compression_ratio": 1.7401129943502824, "no_speech_prob": 3.4465592761989683e-06}, {"id": 1340, "seek": 748276, "start": 7482.76, "end": 7489.4400000000005, "text": " you use that amount of noise for each one and then you pass that mini batch into your", "tokens": [291, 764, 300, 2372, 295, 5658, 337, 1184, 472, 293, 550, 291, 1320, 300, 8382, 15245, 666, 428], "temperature": 0.0, "avg_logprob": -0.06738609167245718, "compression_ratio": 1.6748466257668713, "no_speech_prob": 7.338193768191559e-07}, {"id": 1341, "seek": 748276, "start": 7489.4400000000005, "end": 7499.96, "text": " model to train it and that trains the weights in your model so it can learn to predict noise.", "tokens": [2316, 281, 3847, 309, 293, 300, 16329, 264, 17443, 294, 428, 2316, 370, 309, 393, 1466, 281, 6069, 5658, 13], "temperature": 0.0, "avg_logprob": -0.06738609167245718, "compression_ratio": 1.6748466257668713, "no_speech_prob": 7.338193768191559e-07}, {"id": 1342, "seek": 748276, "start": 7499.96, "end": 7507.84, "text": " And so then when you come to inference time, so inference is when you're generating a picture", "tokens": [400, 370, 550, 562, 291, 808, 281, 38253, 565, 11, 370, 38253, 307, 562, 291, 434, 17746, 257, 3036], "temperature": 0.0, "avg_logprob": -0.06738609167245718, "compression_ratio": 1.6748466257668713, "no_speech_prob": 7.338193768191559e-07}, {"id": 1343, "seek": 750784, "start": 7507.84, "end": 7521.32, "text": " from pure noise, you want your model, basically your model is now starting here, right, which", "tokens": [490, 6075, 5658, 11, 291, 528, 428, 2316, 11, 1936, 428, 2316, 307, 586, 2891, 510, 11, 558, 11, 597], "temperature": 0.0, "avg_logprob": -0.18341870929883874, "compression_ratio": 1.411764705882353, "no_speech_prob": 2.332044914510334e-06}, {"id": 1344, "seek": 750784, "start": 7521.32, "end": 7526.28, "text": " is as much noise as possible.", "tokens": [307, 382, 709, 5658, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.18341870929883874, "compression_ratio": 1.411764705882353, "no_speech_prob": 2.332044914510334e-06}, {"id": 1345, "seek": 750784, "start": 7526.28, "end": 7530.96, "text": " And so you want it to learn to remove noise.", "tokens": [400, 370, 291, 528, 309, 281, 1466, 281, 4159, 5658, 13], "temperature": 0.0, "avg_logprob": -0.18341870929883874, "compression_ratio": 1.411764705882353, "no_speech_prob": 2.332044914510334e-06}, {"id": 1346, "seek": 753096, "start": 7530.96, "end": 7538.1, "text": " But what it does in practice, as we saw in our notebook, is it actually creates some", "tokens": [583, 437, 309, 775, 294, 3124, 11, 382, 321, 1866, 294, 527, 21060, 11, 307, 309, 767, 7829, 512], "temperature": 0.0, "avg_logprob": -0.11545978983243306, "compression_ratio": 1.338235294117647, "no_speech_prob": 1.8161978232456022e-06}, {"id": 1347, "seek": 753096, "start": 7538.1, "end": 7544.16, "text": " hideous and rather random kind of thing.", "tokens": [6479, 563, 293, 2831, 4974, 733, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.11545978983243306, "compression_ratio": 1.338235294117647, "no_speech_prob": 1.8161978232456022e-06}, {"id": 1348, "seek": 753096, "start": 7544.16, "end": 7559.24, "text": " So in fact let's remind ourselves what that looked like.", "tokens": [407, 294, 1186, 718, 311, 4160, 4175, 437, 300, 2956, 411, 13], "temperature": 0.0, "avg_logprob": -0.11545978983243306, "compression_ratio": 1.338235294117647, "no_speech_prob": 1.8161978232456022e-06}, {"id": 1349, "seek": 755924, "start": 7559.24, "end": 7567.88, "text": " This is what it created when we tried to do it in one step.", "tokens": [639, 307, 437, 309, 2942, 562, 321, 3031, 281, 360, 309, 294, 472, 1823, 13], "temperature": 0.0, "avg_logprob": -0.11897214651107788, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.7266033814375987e-06}, {"id": 1350, "seek": 755924, "start": 7567.88, "end": 7574.719999999999, "text": " So remember what we then do is we say okay, what's the prediction of the noise and then", "tokens": [407, 1604, 437, 321, 550, 360, 307, 321, 584, 1392, 11, 437, 311, 264, 17630, 295, 264, 5658, 293, 550], "temperature": 0.0, "avg_logprob": -0.11897214651107788, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.7266033814375987e-06}, {"id": 1351, "seek": 755924, "start": 7574.719999999999, "end": 7581.8, "text": " we multiply the prediction of the noise I said by some constant, which is kind of like", "tokens": [321, 12972, 264, 17630, 295, 264, 5658, 286, 848, 538, 512, 5754, 11, 597, 307, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.11897214651107788, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.7266033814375987e-06}, {"id": 1352, "seek": 755924, "start": 7581.8, "end": 7586.88, "text": " a learning rate, but we're not updating weights now, we're updating pixels.", "tokens": [257, 2539, 3314, 11, 457, 321, 434, 406, 25113, 17443, 586, 11, 321, 434, 25113, 18668, 13], "temperature": 0.0, "avg_logprob": -0.11897214651107788, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.7266033814375987e-06}, {"id": 1353, "seek": 758688, "start": 7586.88, "end": 7591.400000000001, "text": " And we subtract it from the pixels.", "tokens": [400, 321, 16390, 309, 490, 264, 18668, 13], "temperature": 0.0, "avg_logprob": -0.10327806170024569, "compression_ratio": 1.6928571428571428, "no_speech_prob": 2.0580428099492565e-06}, {"id": 1354, "seek": 758688, "start": 7591.400000000001, "end": 7596.4400000000005, "text": " So it didn't actually predict the image, what it actually did was it predicted what the", "tokens": [407, 309, 994, 380, 767, 6069, 264, 3256, 11, 437, 309, 767, 630, 390, 309, 19147, 437, 264], "temperature": 0.0, "avg_logprob": -0.10327806170024569, "compression_ratio": 1.6928571428571428, "no_speech_prob": 2.0580428099492565e-06}, {"id": 1355, "seek": 758688, "start": 7596.4400000000005, "end": 7607.68, "text": " noise is so that it could then subtract that from the image, from the noisy image, to give", "tokens": [5658, 307, 370, 300, 309, 727, 550, 16390, 300, 490, 264, 3256, 11, 490, 264, 24518, 3256, 11, 281, 976], "temperature": 0.0, "avg_logprob": -0.10327806170024569, "compression_ratio": 1.6928571428571428, "no_speech_prob": 2.0580428099492565e-06}, {"id": 1356, "seek": 758688, "start": 7607.68, "end": 7611.32, "text": " us the denoised image.", "tokens": [505, 264, 1441, 78, 2640, 3256, 13], "temperature": 0.0, "avg_logprob": -0.10327806170024569, "compression_ratio": 1.6928571428571428, "no_speech_prob": 2.0580428099492565e-06}, {"id": 1357, "seek": 761132, "start": 7611.32, "end": 7618.08, "text": " And so what we do is we don't actually subtract all of it, we multiply that by a constant", "tokens": [400, 370, 437, 321, 360, 307, 321, 500, 380, 767, 16390, 439, 295, 309, 11, 321, 12972, 300, 538, 257, 5754], "temperature": 0.0, "avg_logprob": -0.05583829628793817, "compression_ratio": 1.4758620689655173, "no_speech_prob": 5.122885227137886e-07}, {"id": 1358, "seek": 761132, "start": 7618.08, "end": 7624.599999999999, "text": " and we get a somewhat noisy image.", "tokens": [293, 321, 483, 257, 8344, 24518, 3256, 13], "temperature": 0.0, "avg_logprob": -0.05583829628793817, "compression_ratio": 1.4758620689655173, "no_speech_prob": 5.122885227137886e-07}, {"id": 1359, "seek": 761132, "start": 7624.599999999999, "end": 7632.48, "text": " The reason we don't jump all the way to the best image we can find is because things that", "tokens": [440, 1778, 321, 500, 380, 3012, 439, 264, 636, 281, 264, 1151, 3256, 321, 393, 915, 307, 570, 721, 300], "temperature": 0.0, "avg_logprob": -0.05583829628793817, "compression_ratio": 1.4758620689655173, "no_speech_prob": 5.122885227137886e-07}, {"id": 1360, "seek": 763248, "start": 7632.48, "end": 7644.16, "text": " look like this never appeared in our training set.", "tokens": [574, 411, 341, 1128, 8516, 294, 527, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.0769590200003931, "compression_ratio": 1.6642335766423357, "no_speech_prob": 2.0580444015649846e-06}, {"id": 1361, "seek": 763248, "start": 7644.16, "end": 7649.12, "text": " And so since it never appeared in our training set, our model has no idea what to do with", "tokens": [400, 370, 1670, 309, 1128, 8516, 294, 527, 3097, 992, 11, 527, 2316, 575, 572, 1558, 437, 281, 360, 365], "temperature": 0.0, "avg_logprob": -0.0769590200003931, "compression_ratio": 1.6642335766423357, "no_speech_prob": 2.0580444015649846e-06}, {"id": 1362, "seek": 763248, "start": 7649.12, "end": 7650.12, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.0769590200003931, "compression_ratio": 1.6642335766423357, "no_speech_prob": 2.0580444015649846e-06}, {"id": 1363, "seek": 763248, "start": 7650.12, "end": 7658.24, "text": " Our model only knows how to deal with things that look like somewhat noisy latents.", "tokens": [2621, 2316, 787, 3255, 577, 281, 2028, 365, 721, 300, 574, 411, 8344, 24518, 4465, 791, 13], "temperature": 0.0, "avg_logprob": -0.0769590200003931, "compression_ratio": 1.6642335766423357, "no_speech_prob": 2.0580444015649846e-06}, {"id": 1364, "seek": 765824, "start": 7658.24, "end": 7665.04, "text": " And so that's why we subtract just a bit of the noise so that we still have a somewhat", "tokens": [400, 370, 300, 311, 983, 321, 16390, 445, 257, 857, 295, 264, 5658, 370, 300, 321, 920, 362, 257, 8344], "temperature": 0.0, "avg_logprob": -0.09744285174778529, "compression_ratio": 1.5739644970414202, "no_speech_prob": 9.080346899281722e-06}, {"id": 1365, "seek": 765824, "start": 7665.04, "end": 7667.76, "text": " noisy latent.", "tokens": [24518, 48994, 13], "temperature": 0.0, "avg_logprob": -0.09744285174778529, "compression_ratio": 1.5739644970414202, "no_speech_prob": 9.080346899281722e-06}, {"id": 1366, "seek": 765824, "start": 7667.76, "end": 7678.0, "text": " So this process repeats a bunch of times and questions like what do we use for C, right,", "tokens": [407, 341, 1399, 35038, 257, 3840, 295, 1413, 293, 1651, 411, 437, 360, 321, 764, 337, 383, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.09744285174778529, "compression_ratio": 1.5739644970414202, "no_speech_prob": 9.080346899281722e-06}, {"id": 1367, "seek": 765824, "start": 7678.0, "end": 7683.4, "text": " and how do we go from the prediction of noise to the thing that we subtract.", "tokens": [293, 577, 360, 321, 352, 490, 264, 17630, 295, 5658, 281, 264, 551, 300, 321, 16390, 13], "temperature": 0.0, "avg_logprob": -0.09744285174778529, "compression_ratio": 1.5739644970414202, "no_speech_prob": 9.080346899281722e-06}, {"id": 1368, "seek": 768340, "start": 7683.4, "end": 7690.5199999999995, "text": " These are all of the things that are the kind of the things that you decide in the actual", "tokens": [1981, 366, 439, 295, 264, 721, 300, 366, 264, 733, 295, 264, 721, 300, 291, 4536, 294, 264, 3539], "temperature": 0.0, "avg_logprob": -0.13939937987884918, "compression_ratio": 1.676300578034682, "no_speech_prob": 2.2959056877880357e-06}, {"id": 1369, "seek": 768340, "start": 7690.5199999999995, "end": 7698.0, "text": " sampler.", "tokens": [3247, 22732, 13], "temperature": 0.0, "avg_logprob": -0.13939937987884918, "compression_ratio": 1.676300578034682, "no_speech_prob": 2.2959056877880357e-06}, {"id": 1370, "seek": 768340, "start": 7698.0, "end": 7703.799999999999, "text": " And that's used both to think about like how do I add the noise and how do I subtract the", "tokens": [400, 300, 311, 1143, 1293, 281, 519, 466, 411, 577, 360, 286, 909, 264, 5658, 293, 577, 360, 286, 16390, 264], "temperature": 0.0, "avg_logprob": -0.13939937987884918, "compression_ratio": 1.676300578034682, "no_speech_prob": 2.2959056877880357e-06}, {"id": 1371, "seek": 768340, "start": 7703.799999999999, "end": 7705.2, "text": " noise.", "tokens": [5658, 13], "temperature": 0.0, "avg_logprob": -0.13939937987884918, "compression_ratio": 1.676300578034682, "no_speech_prob": 2.2959056877880357e-06}, {"id": 1372, "seek": 768340, "start": 7705.2, "end": 7711.4, "text": " And there's a few things that might be jumping into your head at this point if you're anything", "tokens": [400, 456, 311, 257, 1326, 721, 300, 1062, 312, 11233, 666, 428, 1378, 412, 341, 935, 498, 291, 434, 1340], "temperature": 0.0, "avg_logprob": -0.13939937987884918, "compression_ratio": 1.676300578034682, "no_speech_prob": 2.2959056877880357e-06}, {"id": 1373, "seek": 771140, "start": 7711.4, "end": 7714.12, "text": " like me.", "tokens": [411, 385, 13], "temperature": 0.0, "avg_logprob": -0.07744183961082907, "compression_ratio": 1.5705521472392638, "no_speech_prob": 9.422425932825718e-07}, {"id": 1374, "seek": 771140, "start": 7714.12, "end": 7724.28, "text": " And one is that, gosh, this looks an awful lot like deep learning optimizers.", "tokens": [400, 472, 307, 300, 11, 6502, 11, 341, 1542, 364, 11232, 688, 411, 2452, 2539, 5028, 22525, 13], "temperature": 0.0, "avg_logprob": -0.07744183961082907, "compression_ratio": 1.5705521472392638, "no_speech_prob": 9.422425932825718e-07}, {"id": 1375, "seek": 771140, "start": 7724.28, "end": 7731.96, "text": " So in a deep learning optimizer, this constant is called the learning rate.", "tokens": [407, 294, 257, 2452, 2539, 5028, 6545, 11, 341, 5754, 307, 1219, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.07744183961082907, "compression_ratio": 1.5705521472392638, "no_speech_prob": 9.422425932825718e-07}, {"id": 1376, "seek": 771140, "start": 7731.96, "end": 7740.759999999999, "text": " And we have some neat tricks where we say, for example, oh, if you change the same parameters", "tokens": [400, 321, 362, 512, 10654, 11733, 689, 321, 584, 11, 337, 1365, 11, 1954, 11, 498, 291, 1319, 264, 912, 9834], "temperature": 0.0, "avg_logprob": -0.07744183961082907, "compression_ratio": 1.5705521472392638, "no_speech_prob": 9.422425932825718e-07}, {"id": 1377, "seek": 774076, "start": 7740.76, "end": 7747.2, "text": " by a similar amount multiple times in multiple steps, maybe you should increase the amount", "tokens": [538, 257, 2531, 2372, 3866, 1413, 294, 3866, 4439, 11, 1310, 291, 820, 3488, 264, 2372], "temperature": 0.0, "avg_logprob": -0.11474500008679311, "compression_ratio": 1.671875, "no_speech_prob": 5.594288722932106e-06}, {"id": 1378, "seek": 774076, "start": 7747.2, "end": 7748.56, "text": " you change them.", "tokens": [291, 1319, 552, 13], "temperature": 0.0, "avg_logprob": -0.11474500008679311, "compression_ratio": 1.671875, "no_speech_prob": 5.594288722932106e-06}, {"id": 1379, "seek": 774076, "start": 7748.56, "end": 7755.2, "text": " This concept is something we call momentum.", "tokens": [639, 3410, 307, 746, 321, 818, 11244, 13], "temperature": 0.0, "avg_logprob": -0.11474500008679311, "compression_ratio": 1.671875, "no_speech_prob": 5.594288722932106e-06}, {"id": 1380, "seek": 774076, "start": 7755.2, "end": 7758.84, "text": " And we'll be doing all this from scratch during the course, don't worry.", "tokens": [400, 321, 603, 312, 884, 439, 341, 490, 8459, 1830, 264, 1164, 11, 500, 380, 3292, 13], "temperature": 0.0, "avg_logprob": -0.11474500008679311, "compression_ratio": 1.671875, "no_speech_prob": 5.594288722932106e-06}, {"id": 1381, "seek": 774076, "start": 7758.84, "end": 7762.16, "text": " And in fact, we even got better ways of doing that where we kind of say, well, what about", "tokens": [400, 294, 1186, 11, 321, 754, 658, 1101, 2098, 295, 884, 300, 689, 321, 733, 295, 584, 11, 731, 11, 437, 466], "temperature": 0.0, "avg_logprob": -0.11474500008679311, "compression_ratio": 1.671875, "no_speech_prob": 5.594288722932106e-06}, {"id": 1382, "seek": 774076, "start": 7762.16, "end": 7764.08, "text": " what happens as the variance changes?", "tokens": [437, 2314, 382, 264, 21977, 2962, 30], "temperature": 0.0, "avg_logprob": -0.11474500008679311, "compression_ratio": 1.671875, "no_speech_prob": 5.594288722932106e-06}, {"id": 1383, "seek": 774076, "start": 7764.08, "end": 7766.08, "text": " Maybe we can look at that as well.", "tokens": [2704, 321, 393, 574, 412, 300, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.11474500008679311, "compression_ratio": 1.671875, "no_speech_prob": 5.594288722932106e-06}, {"id": 1384, "seek": 774076, "start": 7766.08, "end": 7769.68, "text": " And that gives us something called Adam.", "tokens": [400, 300, 2709, 505, 746, 1219, 7938, 13], "temperature": 0.0, "avg_logprob": -0.11474500008679311, "compression_ratio": 1.671875, "no_speech_prob": 5.594288722932106e-06}, {"id": 1385, "seek": 776968, "start": 7769.68, "end": 7774.280000000001, "text": " And these are types of optimizer.", "tokens": [400, 613, 366, 3467, 295, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.11917844940634335, "compression_ratio": 1.4860335195530727, "no_speech_prob": 1.1911037063327967e-06}, {"id": 1386, "seek": 776968, "start": 7774.280000000001, "end": 7781.84, "text": " And so maybe you might be wondering, could we use these kinds of tricks?", "tokens": [400, 370, 1310, 291, 1062, 312, 6359, 11, 727, 321, 764, 613, 3685, 295, 11733, 30], "temperature": 0.0, "avg_logprob": -0.11917844940634335, "compression_ratio": 1.4860335195530727, "no_speech_prob": 1.1911037063327967e-06}, {"id": 1387, "seek": 776968, "start": 7781.84, "end": 7790.76, "text": " And the answer, based on our very early research, is yes, yes, we can.", "tokens": [400, 264, 1867, 11, 2361, 322, 527, 588, 2440, 2132, 11, 307, 2086, 11, 2086, 11, 321, 393, 13], "temperature": 0.0, "avg_logprob": -0.11917844940634335, "compression_ratio": 1.4860335195530727, "no_speech_prob": 1.1911037063327967e-06}, {"id": 1388, "seek": 776968, "start": 7790.76, "end": 7797.900000000001, "text": " The whole world of like where stable diffusion and all these diffusion based models came", "tokens": [440, 1379, 1002, 295, 411, 689, 8351, 25242, 293, 439, 613, 25242, 2361, 5245, 1361], "temperature": 0.0, "avg_logprob": -0.11917844940634335, "compression_ratio": 1.4860335195530727, "no_speech_prob": 1.1911037063327967e-06}, {"id": 1389, "seek": 779790, "start": 7797.9, "end": 7806.04, "text": " from a very different world of maths, which is the world of differential equations.", "tokens": [490, 257, 588, 819, 1002, 295, 36287, 11, 597, 307, 264, 1002, 295, 15756, 11787, 13], "temperature": 0.0, "avg_logprob": -0.09473849379498026, "compression_ratio": 1.9950248756218905, "no_speech_prob": 3.187548827554565e-06}, {"id": 1390, "seek": 779790, "start": 7806.04, "end": 7812.719999999999, "text": " And there's a whole lot of very parallel concepts in the world of differential equations, which", "tokens": [400, 456, 311, 257, 1379, 688, 295, 588, 8952, 10392, 294, 264, 1002, 295, 15756, 11787, 11, 597], "temperature": 0.0, "avg_logprob": -0.09473849379498026, "compression_ratio": 1.9950248756218905, "no_speech_prob": 3.187548827554565e-06}, {"id": 1391, "seek": 779790, "start": 7812.719999999999, "end": 7818.04, "text": " is really all about taking these like little steps, little steps, little steps, and trying", "tokens": [307, 534, 439, 466, 1940, 613, 411, 707, 4439, 11, 707, 4439, 11, 707, 4439, 11, 293, 1382], "temperature": 0.0, "avg_logprob": -0.09473849379498026, "compression_ratio": 1.9950248756218905, "no_speech_prob": 3.187548827554565e-06}, {"id": 1392, "seek": 779790, "start": 7818.04, "end": 7820.66, "text": " to figure out how to take bigger steps.", "tokens": [281, 2573, 484, 577, 281, 747, 3801, 4439, 13], "temperature": 0.0, "avg_logprob": -0.09473849379498026, "compression_ratio": 1.9950248756218905, "no_speech_prob": 3.187548827554565e-06}, {"id": 1393, "seek": 779790, "start": 7820.66, "end": 7826.2, "text": " And so different differential equation solvers use a lot of the same kind of ideas, if you", "tokens": [400, 370, 819, 15756, 5367, 1404, 840, 764, 257, 688, 295, 264, 912, 733, 295, 3487, 11, 498, 291], "temperature": 0.0, "avg_logprob": -0.09473849379498026, "compression_ratio": 1.9950248756218905, "no_speech_prob": 3.187548827554565e-06}, {"id": 1394, "seek": 782620, "start": 7826.2, "end": 7830.44, "text": " use Gwent, as optimizers.", "tokens": [764, 460, 34798, 11, 382, 5028, 22525, 13], "temperature": 0.0, "avg_logprob": -0.13983337581157684, "compression_ratio": 1.50920245398773, "no_speech_prob": 4.637849997379817e-06}, {"id": 1395, "seek": 782620, "start": 7830.44, "end": 7836.48, "text": " One thing that differential equations solvers do, which is kind of interesting though, is", "tokens": [1485, 551, 300, 15756, 11787, 1404, 840, 360, 11, 597, 307, 733, 295, 1880, 1673, 11, 307], "temperature": 0.0, "avg_logprob": -0.13983337581157684, "compression_ratio": 1.50920245398773, "no_speech_prob": 4.637849997379817e-06}, {"id": 1396, "seek": 782620, "start": 7836.48, "end": 7841.72, "text": " that they tend to take t as an input.", "tokens": [300, 436, 3928, 281, 747, 256, 382, 364, 4846, 13], "temperature": 0.0, "avg_logprob": -0.13983337581157684, "compression_ratio": 1.50920245398773, "no_speech_prob": 4.637849997379817e-06}, {"id": 1397, "seek": 782620, "start": 7841.72, "end": 7848.28, "text": " And in fact, pretty much all diffusion models, I've actually lied, pretty much all diffusion", "tokens": [400, 294, 1186, 11, 1238, 709, 439, 25242, 5245, 11, 286, 600, 767, 20101, 11, 1238, 709, 439, 25242], "temperature": 0.0, "avg_logprob": -0.13983337581157684, "compression_ratio": 1.50920245398773, "no_speech_prob": 4.637849997379817e-06}, {"id": 1398, "seek": 784828, "start": 7848.28, "end": 7857.04, "text": " models don't just take the input pixels and the digit or the caption or the prompt, they", "tokens": [5245, 500, 380, 445, 747, 264, 4846, 18668, 293, 264, 14293, 420, 264, 31974, 420, 264, 12391, 11, 436], "temperature": 0.0, "avg_logprob": -0.13600393136342367, "compression_ratio": 1.679245283018868, "no_speech_prob": 1.8448154150974005e-06}, {"id": 1399, "seek": 784828, "start": 7857.04, "end": 7861.04, "text": " also take t.", "tokens": [611, 747, 256, 13], "temperature": 0.0, "avg_logprob": -0.13600393136342367, "compression_ratio": 1.679245283018868, "no_speech_prob": 1.8448154150974005e-06}, {"id": 1400, "seek": 784828, "start": 7861.04, "end": 7868.599999999999, "text": " And the idea is that the model will be better at removing the noise if you tell it how much", "tokens": [400, 264, 1558, 307, 300, 264, 2316, 486, 312, 1101, 412, 12720, 264, 5658, 498, 291, 980, 309, 577, 709], "temperature": 0.0, "avg_logprob": -0.13600393136342367, "compression_ratio": 1.679245283018868, "no_speech_prob": 1.8448154150974005e-06}, {"id": 1401, "seek": 784828, "start": 7868.599999999999, "end": 7871.139999999999, "text": " noise there is.", "tokens": [5658, 456, 307, 13], "temperature": 0.0, "avg_logprob": -0.13600393136342367, "compression_ratio": 1.679245283018868, "no_speech_prob": 1.8448154150974005e-06}, {"id": 1402, "seek": 784828, "start": 7871.139999999999, "end": 7875.32, "text": " And remember, this is related to how much noise there is.", "tokens": [400, 1604, 11, 341, 307, 4077, 281, 577, 709, 5658, 456, 307, 13], "temperature": 0.0, "avg_logprob": -0.13600393136342367, "compression_ratio": 1.679245283018868, "no_speech_prob": 1.8448154150974005e-06}, {"id": 1403, "seek": 787532, "start": 7875.32, "end": 7880.12, "text": " I very strongly suspect that this premise is incorrect.", "tokens": [286, 588, 10613, 9091, 300, 341, 22045, 307, 18424, 13], "temperature": 0.0, "avg_logprob": -0.11142413535814606, "compression_ratio": 1.603448275862069, "no_speech_prob": 2.1233629468042636e-06}, {"id": 1404, "seek": 787532, "start": 7880.12, "end": 7885.679999999999, "text": " Because if you think about it, for a complicated fancy neural net, figuring out how noisy something", "tokens": [1436, 498, 291, 519, 466, 309, 11, 337, 257, 6179, 10247, 18161, 2533, 11, 15213, 484, 577, 24518, 746], "temperature": 0.0, "avg_logprob": -0.11142413535814606, "compression_ratio": 1.603448275862069, "no_speech_prob": 2.1233629468042636e-06}, {"id": 1405, "seek": 787532, "start": 7885.679999999999, "end": 7890.38, "text": " is, is very, very straightforward.", "tokens": [307, 11, 307, 588, 11, 588, 15325, 13], "temperature": 0.0, "avg_logprob": -0.11142413535814606, "compression_ratio": 1.603448275862069, "no_speech_prob": 2.1233629468042636e-06}, {"id": 1406, "seek": 787532, "start": 7890.38, "end": 7895.2, "text": " So I very much doubt we actually need to pass in t.", "tokens": [407, 286, 588, 709, 6385, 321, 767, 643, 281, 1320, 294, 256, 13], "temperature": 0.0, "avg_logprob": -0.11142413535814606, "compression_ratio": 1.603448275862069, "no_speech_prob": 2.1233629468042636e-06}, {"id": 1407, "seek": 787532, "start": 7895.2, "end": 7902.16, "text": " And as soon as you stop doing that, things stop looking like differential equations,", "tokens": [400, 382, 2321, 382, 291, 1590, 884, 300, 11, 721, 1590, 1237, 411, 15756, 11787, 11], "temperature": 0.0, "avg_logprob": -0.11142413535814606, "compression_ratio": 1.603448275862069, "no_speech_prob": 2.1233629468042636e-06}, {"id": 1408, "seek": 787532, "start": 7902.16, "end": 7904.639999999999, "text": " and they start looking more like optimizers.", "tokens": [293, 436, 722, 1237, 544, 411, 5028, 22525, 13], "temperature": 0.0, "avg_logprob": -0.11142413535814606, "compression_ratio": 1.603448275862069, "no_speech_prob": 2.1233629468042636e-06}, {"id": 1409, "seek": 790464, "start": 7904.64, "end": 7910.0, "text": " And so actually, Jono's started playing with this and experimenting a bit.", "tokens": [400, 370, 767, 11, 7745, 78, 311, 1409, 2433, 365, 341, 293, 29070, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.12881402209796736, "compression_ratio": 1.7262773722627738, "no_speech_prob": 4.356837052910123e-06}, {"id": 1410, "seek": 790464, "start": 7910.0, "end": 7916.240000000001, "text": " And early results suggest that, yeah, actually, when we rethink about the whole thing as being", "tokens": [400, 2440, 3542, 3402, 300, 11, 1338, 11, 767, 11, 562, 321, 34595, 466, 264, 1379, 551, 382, 885], "temperature": 0.0, "avg_logprob": -0.12881402209796736, "compression_ratio": 1.7262773722627738, "no_speech_prob": 4.356837052910123e-06}, {"id": 1411, "seek": 790464, "start": 7916.240000000001, "end": 7920.4800000000005, "text": " about learning rates and optimizers, maybe it actually works a bit better.", "tokens": [466, 2539, 6846, 293, 5028, 22525, 11, 1310, 309, 767, 1985, 257, 857, 1101, 13], "temperature": 0.0, "avg_logprob": -0.12881402209796736, "compression_ratio": 1.7262773722627738, "no_speech_prob": 4.356837052910123e-06}, {"id": 1412, "seek": 790464, "start": 7920.4800000000005, "end": 7924.240000000001, "text": " In fact, there's all kinds of things we could do.", "tokens": [682, 1186, 11, 456, 311, 439, 3685, 295, 721, 321, 727, 360, 13], "temperature": 0.0, "avg_logprob": -0.12881402209796736, "compression_ratio": 1.7262773722627738, "no_speech_prob": 4.356837052910123e-06}, {"id": 1413, "seek": 790464, "start": 7924.240000000001, "end": 7928.4800000000005, "text": " Once we stop thinking about them as differential equations, and worry about the math, don't", "tokens": [3443, 321, 1590, 1953, 466, 552, 382, 15756, 11787, 11, 293, 3292, 466, 264, 5221, 11, 500, 380], "temperature": 0.0, "avg_logprob": -0.12881402209796736, "compression_ratio": 1.7262773722627738, "no_speech_prob": 4.356837052910123e-06}, {"id": 1414, "seek": 790464, "start": 7928.4800000000005, "end": 7933.72, "text": " worry about the math so much about Gaussians and whatever, we can really switch things", "tokens": [3292, 466, 264, 5221, 370, 709, 466, 10384, 2023, 2567, 293, 2035, 11, 321, 393, 534, 3679, 721], "temperature": 0.0, "avg_logprob": -0.12881402209796736, "compression_ratio": 1.7262773722627738, "no_speech_prob": 4.356837052910123e-06}, {"id": 1415, "seek": 793372, "start": 7933.72, "end": 7934.92, "text": " around.", "tokens": [926, 13], "temperature": 0.0, "avg_logprob": -0.12934797265556422, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.436742536498059e-06}, {"id": 1416, "seek": 793372, "start": 7934.92, "end": 7944.12, "text": " So for example, we decided, for no particular obvious reason, to use MSE.", "tokens": [407, 337, 1365, 11, 321, 3047, 11, 337, 572, 1729, 6322, 1778, 11, 281, 764, 376, 5879, 13], "temperature": 0.0, "avg_logprob": -0.12934797265556422, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.436742536498059e-06}, {"id": 1417, "seek": 793372, "start": 7944.12, "end": 7948.92, "text": " Well the truth is, in statistics and machine learning, almost every time you see somebody", "tokens": [1042, 264, 3494, 307, 11, 294, 12523, 293, 3479, 2539, 11, 1920, 633, 565, 291, 536, 2618], "temperature": 0.0, "avg_logprob": -0.12934797265556422, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.436742536498059e-06}, {"id": 1418, "seek": 793372, "start": 7948.92, "end": 7953.16, "text": " use MSE, it's because the math worked out better that way.", "tokens": [764, 376, 5879, 11, 309, 311, 570, 264, 5221, 2732, 484, 1101, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.12934797265556422, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.436742536498059e-06}, {"id": 1419, "seek": 793372, "start": 7953.16, "end": 7958.400000000001, "text": " Not as in, it's a better thing to do, but as in, you know, it was kind of easier.", "tokens": [1726, 382, 294, 11, 309, 311, 257, 1101, 551, 281, 360, 11, 457, 382, 294, 11, 291, 458, 11, 309, 390, 733, 295, 3571, 13], "temperature": 0.0, "avg_logprob": -0.12934797265556422, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.436742536498059e-06}, {"id": 1420, "seek": 795840, "start": 7958.4, "end": 7964.879999999999, "text": " Now MSE does fall out quite nicely as being a good thing to do with some particular premises.", "tokens": [823, 376, 5879, 775, 2100, 484, 1596, 9594, 382, 885, 257, 665, 551, 281, 360, 365, 512, 1729, 34266, 13], "temperature": 0.0, "avg_logprob": -0.21496562435202402, "compression_ratio": 1.4390243902439024, "no_speech_prob": 2.4824601041473215e-06}, {"id": 1421, "seek": 795840, "start": 7964.879999999999, "end": 7970.0, "text": " You know, like it's not totally arbitrary.", "tokens": [509, 458, 11, 411, 309, 311, 406, 3879, 23211, 13], "temperature": 0.0, "avg_logprob": -0.21496562435202402, "compression_ratio": 1.4390243902439024, "no_speech_prob": 2.4824601041473215e-06}, {"id": 1422, "seek": 795840, "start": 7970.0, "end": 7980.2, "text": " But what if we instead used more sophisticated loss functions, where we actually said, well,", "tokens": [583, 437, 498, 321, 2602, 1143, 544, 16950, 4470, 6828, 11, 689, 321, 767, 848, 11, 731, 11], "temperature": 0.0, "avg_logprob": -0.21496562435202402, "compression_ratio": 1.4390243902439024, "no_speech_prob": 2.4824601041473215e-06}, {"id": 1423, "seek": 795840, "start": 7980.2, "end": 7984.12, "text": " you know, after we subtract the outputs, how good is this really?", "tokens": [291, 458, 11, 934, 321, 16390, 264, 23930, 11, 577, 665, 307, 341, 534, 30], "temperature": 0.0, "avg_logprob": -0.21496562435202402, "compression_ratio": 1.4390243902439024, "no_speech_prob": 2.4824601041473215e-06}, {"id": 1424, "seek": 798412, "start": 7984.12, "end": 7990.12, "text": " Does it look like a digit, or does it have the similar qualities to a digit?", "tokens": [4402, 309, 574, 411, 257, 14293, 11, 420, 775, 309, 362, 264, 2531, 16477, 281, 257, 14293, 30], "temperature": 0.0, "avg_logprob": -0.08717416709577533, "compression_ratio": 1.4777777777777779, "no_speech_prob": 3.0894659630575916e-06}, {"id": 1425, "seek": 798412, "start": 7990.12, "end": 7998.72, "text": " So we'll learn about this stuff, but there's things called, for example, perceptual loss.", "tokens": [407, 321, 603, 1466, 466, 341, 1507, 11, 457, 456, 311, 721, 1219, 11, 337, 1365, 11, 43276, 901, 4470, 13], "temperature": 0.0, "avg_logprob": -0.08717416709577533, "compression_ratio": 1.4777777777777779, "no_speech_prob": 3.0894659630575916e-06}, {"id": 1426, "seek": 798412, "start": 7998.72, "end": 8008.5599999999995, "text": " Or another question is, do we really need to do this thing where we actually put noise", "tokens": [1610, 1071, 1168, 307, 11, 360, 321, 534, 643, 281, 360, 341, 551, 689, 321, 767, 829, 5658], "temperature": 0.0, "avg_logprob": -0.08717416709577533, "compression_ratio": 1.4777777777777779, "no_speech_prob": 3.0894659630575916e-06}, {"id": 1427, "seek": 798412, "start": 8008.5599999999995, "end": 8010.72, "text": " back at all?", "tokens": [646, 412, 439, 30], "temperature": 0.0, "avg_logprob": -0.08717416709577533, "compression_ratio": 1.4777777777777779, "no_speech_prob": 3.0894659630575916e-06}, {"id": 1428, "seek": 801072, "start": 8010.72, "end": 8014.96, "text": " Could we instead use this directly?", "tokens": [7497, 321, 2602, 764, 341, 3838, 30], "temperature": 0.0, "avg_logprob": -0.07366131587200854, "compression_ratio": 1.6475770925110131, "no_speech_prob": 1.6028009213187033e-06}, {"id": 1429, "seek": 801072, "start": 8014.96, "end": 8020.280000000001, "text": " These are all things that suddenly become possible when we start thinking of this as", "tokens": [1981, 366, 439, 721, 300, 5800, 1813, 1944, 562, 321, 722, 1953, 295, 341, 382], "temperature": 0.0, "avg_logprob": -0.07366131587200854, "compression_ratio": 1.6475770925110131, "no_speech_prob": 1.6028009213187033e-06}, {"id": 1430, "seek": 801072, "start": 8020.280000000001, "end": 8026.04, "text": " an optimization problem, rather than a differential equation solving problem.", "tokens": [364, 19618, 1154, 11, 2831, 813, 257, 15756, 5367, 12606, 1154, 13], "temperature": 0.0, "avg_logprob": -0.07366131587200854, "compression_ratio": 1.6475770925110131, "no_speech_prob": 1.6028009213187033e-06}, {"id": 1431, "seek": 801072, "start": 8026.04, "end": 8031.6, "text": " So for those of you who are interested in kind of doing novel research, this is some", "tokens": [407, 337, 729, 295, 291, 567, 366, 3102, 294, 733, 295, 884, 7613, 2132, 11, 341, 307, 512], "temperature": 0.0, "avg_logprob": -0.07366131587200854, "compression_ratio": 1.6475770925110131, "no_speech_prob": 1.6028009213187033e-06}, {"id": 1432, "seek": 801072, "start": 8031.6, "end": 8037.42, "text": " of the kind of stuff that we are starting to research at the moment, and the early results", "tokens": [295, 264, 733, 295, 1507, 300, 321, 366, 2891, 281, 2132, 412, 264, 1623, 11, 293, 264, 2440, 3542], "temperature": 0.0, "avg_logprob": -0.07366131587200854, "compression_ratio": 1.6475770925110131, "no_speech_prob": 1.6028009213187033e-06}, {"id": 1433, "seek": 803742, "start": 8037.42, "end": 8043.2, "text": " are extremely positive, both in terms of how quickly we can do things and what kind of", "tokens": [366, 4664, 3353, 11, 1293, 294, 2115, 295, 577, 2661, 321, 393, 360, 721, 293, 437, 733, 295], "temperature": 0.0, "avg_logprob": -0.12757956641060966, "compression_ratio": 1.4886363636363635, "no_speech_prob": 3.5559428397391457e-06}, {"id": 1434, "seek": 803742, "start": 8043.2, "end": 8047.84, "text": " outputs we seem to be getting.", "tokens": [23930, 321, 1643, 281, 312, 1242, 13], "temperature": 0.0, "avg_logprob": -0.12757956641060966, "compression_ratio": 1.4886363636363635, "no_speech_prob": 3.5559428397391457e-06}, {"id": 1435, "seek": 803742, "start": 8047.84, "end": 8053.4, "text": " Okay, so I think that's probably a good place to stop it.", "tokens": [1033, 11, 370, 286, 519, 300, 311, 1391, 257, 665, 1081, 281, 1590, 309, 13], "temperature": 0.0, "avg_logprob": -0.12757956641060966, "compression_ratio": 1.4886363636363635, "no_speech_prob": 3.5559428397391457e-06}, {"id": 1436, "seek": 803742, "start": 8053.4, "end": 8060.2, "text": " So what we're going to do in the next lesson is we're going to finish our journey into", "tokens": [407, 437, 321, 434, 516, 281, 360, 294, 264, 958, 6898, 307, 321, 434, 516, 281, 2413, 527, 4671, 666], "temperature": 0.0, "avg_logprob": -0.12757956641060966, "compression_ratio": 1.4886363636363635, "no_speech_prob": 3.5559428397391457e-06}, {"id": 1437, "seek": 806020, "start": 8060.2, "end": 8067.5599999999995, "text": " this notebook to see some of the code behind the scenes of what's in a pipeline.", "tokens": [341, 21060, 281, 536, 512, 295, 264, 3089, 2261, 264, 8026, 295, 437, 311, 294, 257, 15517, 13], "temperature": 0.0, "avg_logprob": -0.16076524361320163, "compression_ratio": 1.697560975609756, "no_speech_prob": 7.411166279780446e-06}, {"id": 1438, "seek": 806020, "start": 8067.5599999999995, "end": 8071.4, "text": " When I get there.", "tokens": [1133, 286, 483, 456, 13], "temperature": 0.0, "avg_logprob": -0.16076524361320163, "compression_ratio": 1.697560975609756, "no_speech_prob": 7.411166279780446e-06}, {"id": 1439, "seek": 806020, "start": 8071.4, "end": 8075.54, "text": " So we'll do looking inside the pipeline and see exactly what's going on behind the scenes", "tokens": [407, 321, 603, 360, 1237, 1854, 264, 15517, 293, 536, 2293, 437, 311, 516, 322, 2261, 264, 8026], "temperature": 0.0, "avg_logprob": -0.16076524361320163, "compression_ratio": 1.697560975609756, "no_speech_prob": 7.411166279780446e-06}, {"id": 1440, "seek": 806020, "start": 8075.54, "end": 8077.82, "text": " a bit more in terms of the code.", "tokens": [257, 857, 544, 294, 2115, 295, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.16076524361320163, "compression_ratio": 1.697560975609756, "no_speech_prob": 7.411166279780446e-06}, {"id": 1441, "seek": 806020, "start": 8077.82, "end": 8082.7, "text": " And then we're going to do a huge rewind from the foundations, and we're going to build", "tokens": [400, 550, 321, 434, 516, 281, 360, 257, 2603, 41458, 490, 264, 22467, 11, 293, 321, 434, 516, 281, 1322], "temperature": 0.0, "avg_logprob": -0.16076524361320163, "compression_ratio": 1.697560975609756, "no_speech_prob": 7.411166279780446e-06}, {"id": 1442, "seek": 806020, "start": 8082.7, "end": 8086.84, "text": " up from some very tricky ground rules.", "tokens": [493, 490, 512, 588, 12414, 2727, 4474, 13], "temperature": 0.0, "avg_logprob": -0.16076524361320163, "compression_ratio": 1.697560975609756, "no_speech_prob": 7.411166279780446e-06}, {"id": 1443, "seek": 808684, "start": 8086.84, "end": 8094.16, "text": " The ground rules will be we're only allowed to use pure Python, the Python standard library,", "tokens": [440, 2727, 4474, 486, 312, 321, 434, 787, 4350, 281, 764, 6075, 15329, 11, 264, 15329, 3832, 6405, 11], "temperature": 0.0, "avg_logprob": -0.22664837897578372, "compression_ratio": 1.5520833333333333, "no_speech_prob": 3.4464896998542827e-06}, {"id": 1444, "seek": 808684, "start": 8094.16, "end": 8103.96, "text": " and nothing else, and build up from there until we have recreated all of this and possibly", "tokens": [293, 1825, 1646, 11, 293, 1322, 493, 490, 456, 1826, 321, 362, 850, 26559, 439, 295, 341, 293, 6264], "temperature": 0.0, "avg_logprob": -0.22664837897578372, "compression_ratio": 1.5520833333333333, "no_speech_prob": 3.4464896998542827e-06}, {"id": 1445, "seek": 808684, "start": 8103.96, "end": 8107.4800000000005, "text": " some new research directions at the same time.", "tokens": [512, 777, 2132, 11095, 412, 264, 912, 565, 13], "temperature": 0.0, "avg_logprob": -0.22664837897578372, "compression_ratio": 1.5520833333333333, "no_speech_prob": 3.4464896998542827e-06}, {"id": 1446, "seek": 808684, "start": 8107.4800000000005, "end": 8108.4800000000005, "text": " So that's our goal.", "tokens": [407, 300, 311, 527, 3387, 13], "temperature": 0.0, "avg_logprob": -0.22664837897578372, "compression_ratio": 1.5520833333333333, "no_speech_prob": 3.4464896998542827e-06}, {"id": 1447, "seek": 808684, "start": 8108.4800000000005, "end": 8112.4800000000005, "text": " So strap in and see you all next time.", "tokens": [407, 18359, 294, 293, 536, 291, 439, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.22664837897578372, "compression_ratio": 1.5520833333333333, "no_speech_prob": 3.4464896998542827e-06}, {"id": 1448, "seek": 811248, "start": 8112.48, "end": 8117.12, "text": " See you.", "tokens": [50364, 3008, 291, 13, 50596], "temperature": 0.0, "avg_logprob": -0.9498919645945231, "compression_ratio": 0.5, "no_speech_prob": 0.0006112526752986014}], "language": "en"}