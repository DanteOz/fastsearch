{"text": " So From here the next two or three lessons. We're going to be really diving deep into random forests So so far all we've learned is there's a thing called random forests For some particular data sets they seem to work really really well without too much trouble But we don't really know yet like well. How do they actually work? What do we do if they don't work properly? What are their pros and cons? What are the what can we tune and so forth so we're going to look at all that and then after that we're going to look At how do we interpret the results of random forests to get not just predictions? But to actually deeply understand our data in a model driven way, so that's where we're going to go here So let's just review where we're up to So we learned that there's this library called fast AI and the fast AI library is It's basically it's a highly opinionated library, which is to say We've spent a lot of time researching what are the best techniques to get like state-of-the-art results and then we take those techniques and package them into pieces of code so that you can use the state-of-the-art results yourself and so Where possible We wrap or provide things on top of Existing code and so in particular for the kind of structured data analysis. We're doing Psychic learn has a lot of really great code So most of the stuff that we're showing you from fast AI is stuff to help us get stuff into Psychic learn and then interpret stuff out from psychic learn the fast AI library The way it works in our Our environment here is that we've got Our Notebooks are inside fast AI repo slash courses and then slash ml1 and deal one And then inside there there's a sim link to the parent of the parent fast AI so this is a sim link to a directory Containing a bunch of modules, so if you want to use the fast AI Library in your own code There's a number of things you can do one is to put your Notebooks or scripts in the same directory as ML1 or deal one whether it's already this sim link and just import it just like I do You could copy this directory dot dot slash dot slash fast AI into Somewhere else and use it or you could sim link it just like I have from here to wherever you want to use it All right, so notice. It's mildly confusing There's a github repo called fast AI and inside the github repo called fast AI Which looks like this there is a folder called fast AI Okay, and so the fast AI folder in the fast AI repo contains the fast AI library And it's that library when we go from fast AI dot imports import star Then that's looking inside the fast AI folder for a file called imports imports dot py and importing everything from that Okay Yes, Danielle, I sketched and Just like as a clarifying question for the same link. It's just the LN thing The That's just the LN thing that you talked about last class. Yeah, so a sim link is something you can create by typing LN minus s and then the path to the source which in this case would be dot dot dot dot fast AI Could be relative or it could be absolute and then the name of the destination if you just put the current directory at the destination It'll use the same name as it comes from like a Alias on the Mac or a shortcut on Windows? and when you do the import sis can I just Yeah, go go import sis and then append that relative link that also creates the sim link in and the workbook to Was this I don't think I've created the same link anywhere in the workbooks the sim link actually lives inside the github repo Okay, I created some sim links in The deep learning notebook to some data that was different. Yeah At the top of Tim Lee's workbook from the last class there was Import sis then append the fast AI. Oh, yeah, don't do that probably I mean you you can but I think this is I think this is better like this way you can go from fast AI imports and Regardless of kind of how you got it there. It's it's gonna work. Okay, you know Great Okay, so then we had all of our data for blue books to bulldozers competition in data slash bulldozers and Here it is right so We were able to read that CSV file the only thing we really had to do is to say which columns were dates And having done that We were able to take a look at a few of the examples of the rows of the data And so we also noted that it's very important to deeply understand the Evaluation metric for this project and so for Kaggle they tell you what the evaluation metric is And in this case it was the root mean squared log error so that is the Sum of the actuals Minus The predictions Right, but it's the log of the actuals minus the log of the predictions Squared Right so if we replace Actuals with log actuals and replace predictions with log predictions, then it's just the same as root mean squared error So that's what we did was we replaced sale price with log of sale price and so now If we optimize for root mean squared error, we're actually optimizing for the root mean squared error of the logs Okay So then we learned that we need all of our columns to be numbers and So the first way we did that was to take the date column And remove it and instead replace it with a whole bunch of different columns Such as is that date the start of a quarter is at the end of a year How many days are elapsed since January the first 1970 what's the year? What's the month? What's the day of week and so forth? Okay, so they're all numbers Then We learned that we can use train underscore cats to replace all of the strings with categories Now when you do that It doesn't look like you've done anything different. They still look like strings Right, but if you actually take a deeper look You'll see that the data type now is not string but category and Category is a pandas class Where you can then go dot cat dot and find a whole bunch of different attributes such as cat dot Categories to find a list of all of the possible categories and this says high is going to be zero low will become one medium will Become two so we can then get codes To actually get the numbers So then what we need to do to actually use this data set to turn it into numbers is Take every categorical column and replace it with cat dot codes and so we did that using Proc DF Okay So how do I get the source code for Proc DF? Question question mark, okay All right, so if I scroll down I go through each Column and I numericalize it. Okay, that's actually the one I want so I'm going to now have to look up numericalized So Tab to complete it If it's not numeric Then replace the data frames field with that columns dot cat dot codes Plus one because otherwise unknown is minus one. We want unknown to be zero. Okay, so That's how we turn the strings into numbers, right? They get replaced with a unique basically arbitrary index. It's actually based on the alphabetical order of the feature names the other thing Proc DF did remember was Continuous columns that had missing values the missing got replaced with the median and we added an additional column called Column name underscore NA which is a Boolean column told you if that particular item was missing or not so once we did that we were able to call random forest regressor dot fit and get the dot score and turns out we have an R squared of point nine eight Can anybody tell me what an R squared is? So, sir you enjoy So R squared essentially it shows how much variance is explained by the model this is the Yeah, this is the Yeah, this is the relation of this is SSR which is like Trying to trying to remember the exact formal but I've been roughly intuitively Yeah, intuitively it's how much the model explains the how much it accounts for the variance in the data. Okay, good. So Let's talk about the formula and so with Formulas the idea is not to learn the formula and remember it but to learn what the formula does and understand it, right? so Here's the formula It's one minus Something divided by something else So what's the something else on the bottom? Ss. Tot okay. So what this is saying is we've got some actual data some Why eyes right we've got some actual data Three two four one Okay, and then we've got some average Okay, so Our top bit this ss. Tot is the sum of each of these Minus that So in other words, it's telling us how much does this data vary perhaps more interestingly is Remember when we talked about like last week, what's the simplest? Non stupid model you could come up with and I think the simplest non stupid model we came up with was Create a column of the mean just copy the mean a bunch of times and submit that to Kaggle If you did that then your root mean squared error would be this So this is the root mean squared error of the most naive non-stupid Model where the model is just predict the mean on the top We have SS res which is here Which is that we're now going to add a column of predictions Okay, and so now what we do is rather than taking the yi minus y mean we're going to take yi minus fi Right and so now instead of saying what's the root mean squared error of our naive model? we're saying what's the root mean squared error of the actual model that we're interested in and then We take the ratio So in other words if we actually were exactly as Effective as just predicting the mean then this would be top and bottom would be the same that would be one one minus one would be zero If we were perfect so fi minus yi was always zero then that's zero divided by something one minus that is one Okay, so What is the possible range of values of r squared? Okay, I heard a lot of zero to one does anybody want to give me an alternative negative one to one Anything less than one there's the right answer. Let's find out why we hit the box Okay, so why is it any number less than one? We should make a model basically as crap as you want and just like as like big arrows as you want And you're just subtracting from one in the formula exactly so interestingly I was talking to Our computer science professor Terrence this morning who was talking to a statistics professor Told him that the possible range of values was r squared was zero to one I said that is totally not true if you predict infinity for every column. That's right for every row Then you're going to have infinity for every residual and so you're going to have one minus infinity Okay, so the possible range of values is Less than one that's all we know and this will happen you will get negative values sometimes in your r squared and when that happens It's not a mistake. It's not a it's not like a bug it means your model is worse than predicting the mean Okay, which is suggest. It's not great So that's r squared It's not It's not necessarily What you're actually trying to optimize Right, but it's it's it's the nice thing about it is that it's a number that you can use kind of for every model And so you can kind of start try to get a feel of like what does point eight look like what is point nine look? like so like something I find interesting is to like Create some different synthetic data sets just to two dimensions with kind of different amounts of random noise and like see what they look like on a scatterplot And see what they are squared are just to kind of get a feel for like what does an ask where you know? Is it an aspect of point nine close or not about point seven close or not? Okay So I think r squared is a useful number to have a familiarity with and you don't Need to remember the formula if you remember the meaning which is what's the ratio between? How good your model is it means good error versus how good is the naive mean model for its good error? Okay in our case point nine eight. It's saying it's a very good model However, it might be a very good model because it looks like this All right, and this would be called Overfitting so we may well have created a model which is very good at running through the points that we gave it But it's not going to be very good at running through points that we didn't give it So that's why we always want to have a validation set Creating your validation set is the most important thing That I think you need to do when you're doing a machine learning project at least in terms of in the actual modeling Because what you need to do is come up with a data set where The score of your model on that data set is going to be representative of how well your model is going to do In the real world like in Kaggle on the leaderboard or off Kaggle like when you actually use it in production I I very very very often hear people in industry say I don't trust machine learning I tried modeling once it looked great. We put it in production. It didn't work Now whose fault is that right that means their validation set was not representative All right, so here's a very simple thing which generally speaking Kaggle is pretty good about doing if your data has a time Piece in it right as happens in blue book for bulldozers in blue book for bulldozers We're talking about the sale price of a piece of industrial equipment on a particular date So the startup doing this competition wanted to create a model that wouldn't predict last February's prices But would predict next month's prices so what they did was they gave us data Representing a particular date range in the training set and then the test set Represented a future set of dates that wasn't represented in the training set Right so that's pretty good right that means that if we're doing well on this model We've built something which can actually predict the future or at least it could predict the future Then assuming things haven't changed dramatically So that's the test set we have so we need to create a validation set that has the same properties So the test set had 12,000 rows in so let's create a validation set that has 12,000 rows Right and then let's split the data set into the first N minus 12,000 rows For the training set and the last 12,000 rows for the validation set and so we've now got something which hopefully looks like Kaggle's test set close enough that when we actually try and use this validation set we're going to get some Reasonably accurate scores and the reason we want this is because on Kaggle You can only submit so many times and if you submit too often you'll end up fitting to the leaderboard anyway and in real life You actually want to build a model that's going to work in real life. Did you have a question can we help the green box? Can you explain the difference between a validation set and a test set absolutely So what we're going to learn today is how to set one of the things to learn is how to set hyper parameters hyper parameters are Like tuning parameters that are going to change how your model behaves Now if you just have one holdout set so one set of data that you're not using to train with and we use that To decide which set of hyper parameters to use if we try a thousand different sets of hyper parameters We may end up overfitting to that holdout set that is to say we'll find something which only Accidentally worked so what we actually want to do is we really want to have a second holdout set where we can say, okay I'm finished Okay, I've done the best I can and now just once right at the end. I'm going to see whether it works and so This is something which almost nobody in industry does correctly You really actually need to remove that holdout set that's called the test set remove it from the data Give it to somebody else and tell them do not let me look at this data until I promise you I'm finished like it's so hard Otherwise not to look at it and for example in the world of psychology and sociology you might have heard about this replication crisis This is basically because people in these fields have accidentally or intentionally maybe been p-hacking Which means they've been basically Trying lots of different variations until they find something that works and then it turns out when they try to replicate it in other words It's like somebody creates a test set somebody says okay This study which shows you know the impact of whether you eat marshmallows on your tenacity later in life I'm going to read it and like Over half the time they're finding the effect turns out not to exist so that's why we want to have a test set You get that next door So for handling categorical data you converted those two numerics to numbers order numbers I've seen a lot of Models where we convert categorical data into different columns using one hot encoding yes So which approach to use in which model yeah, we're going to tackle that today. Yeah, it's a great question Okay, so so I'm splitting my My data into Validation and training sets and so you can see now that my validation set is 12,000 by 66 Where else my training set is? 399,000 by 66 okay, so we're going to use this set of data to train a model and this set of data to see how well It's working So when we then tried that last week We found out just a moment. We found out that our model which had point nine eight to ask where on the training set Only had point eight eight seven on the validation set which makes us think that we're overfitting quite badly But it turned out it wasn't too badly because the root mean squared error on the logs of the prices Actually would have put us in the top 25% of the competition anyway, so even although we're overfitting It wasn't the end of the world could you pass the microphone to Marsha please? In terms of you dividing the set into training and validation Validation it seems like you simply take the first and train observations of the data set and set them aside Why don't you like why don't you randomly pick up the observations? Because if I did that I wouldn't be replicating the test set so Kaggle Has a test set that when you actually look at the dates in the test set they are a Set of dates that are more recent than any date in the training set So if we used a validation set that was a random sample that is much easier Because we're predicting options like what's the value of this piece of industrial equipment on this day when we actually already have some observations from that day so in general any time you're building a model that has a time element you want your test set to be a Separate time period and therefore you really need your validation set to be a separate time period as well And in this case the data was already sorted. So that's why this works So, let's say we have our test the training set where we train the data and then we have the validation set Against which we are trying to find the R square in in case our R square turns out to be really bad We would want to tune our parameters and run it again Yes, so wouldn't that be eventually over fitting on the overall training set? Yeah, so actually that's that's the issue So that would eventually have the possibility of over fitting on the validation set and then when we try it on the test set Or we submit it to Kaggle it turns out not to be very good And this happens in Kaggle competitions all the time Kaggle actually has a fourth data set which is called the private leaderboard Set and every time you submit to Kaggle You actually only get feedback on how well it does on something called the public leaderboard set And you don't know which rows they are and at the end of the competition You can actually get judged on a different data set entirely called the private leaderboard set so the only way to avoid this is to actually be a good machine learning practitioner and Know how to set these parameters as effectively as possible Which we're going to be doing partly today and over the next few weeks Can you get that actually what you throw? Is it too early or late to ask what's the difference between a hyper parameter and a parameter? Okay, okay So Let's start tracking things on root mean squared error So here is root mean squared error in a line of code and you can see here like this is one of these examples where? I'm not Writing this the way a proper software engineer would write this right so a proper software engineer would be a number of things differently they Would have it on a different line They would use longer variable names They would have documentation blah blah blah right but I really think like for me. I really think that being able to Look at something in one go with your eyes and like Over time learn to immediately see what's going on has a lot of value and also to like consistently use like Particular letters to have mean particular things or abbreviations. I think works really well in data science if you're Doing it like a take-home interview test or something You should write your code according to pep8 standards right so pep8 is the The style guide for Python code, and you should know it and use it because a lot of software engineers are super anal About this kind of thing But for your own work, you know I Think this is I think this works well for me You know so I just wanted to make you aware a that you shouldn't necessarily use this as a role model for Dealing with software engineers, but be that I actually think this is not this is a reasonable approach Okay, so there's our root mean squared error and then from time to time. We're just going to print out the score which will give us the RMSC of the predictions on the training versus the actual their predictions on the valid versus the actual RMSC the R squared for the training and the R squared for the valid and we'll come back to OOB in a moment So when we ran that we found that this RMSC was in the top 25 percent and it's like okay. There's a good start now This took eight seconds of wall time so eight actual seconds if you put percent time it'll tell you how long things took And luckily I've got quite a few cores quite a few CPUs in this computer Because it actually took over a minute a compute time so it parallelized that across cores If your data set Was bigger or you had less cores? You know you could well find that this took a few minutes to run or even a few hours My rule of thumb is that if something takes more than 10 seconds to run It's too long for me to do like interactive Analysis with it right I want to be able to like run something Wait a moment and then continue So what we do is we try to make sure that things can run in a reasonable time And then when we're when we're finished at the end of the day We can then say okay this feature engineering these hyper parameters Whatever these are all working well, and I'll now rerun it. You know this the big slow Precise way so one way to speed things up is to pass in the subset parameter To proc DF and that will randomly sample My data right and so here. I'm going to randomly sample 30,000 rows now when I do that I Still need to be careful to make sure that my validation set Doesn't change and that my training set doesn't overlap with the dates otherwise. I'm cheating So I call split valves again to again do this split by dates and You'll also see I'm using rather than putting it into a validation set. I'm putting it into a variable called underscore This is kind of a standard approach in Python is to use a variable called underscore if you want to throw something away Because I don't want to change my validation set like no matter what different models I build I want to be able to compare them all to each other So I want to keep my validation set the same all the time okay, so all I'm doing here is I'm resampling my training set into a 20 the first 20,000 out of a 30,000 subset So I now can run that and it runs in 621 milliseconds So I can like really zip through things now try things out, okay? So with that Let's use this subset To build a model that is so simple that we can actually take a look at it and so we're going to build a Forest is made of trees and So before we look at the forest we look at the trees In scikit-learn they don't call them trees they call them estimators So we're going to pass in the parameter number of estimators equals one to create a forest with just one tree in and Then we're going to make a small tree so we pass in maximum depth equals three and A random forest as we're going to learn randomizes a whole bunch of things We want to turn that off so to turn that off you say bootstrap equals false So if I pass in these parameters it creates a small deterministic tree So if I fit it and say print score my R squared has gone down from point eight five to point four So this is not a good model. It's better than the mean model. This is better than zero, right? It's not a good model, but it's a model that we can draw All right, so let's learn about what it's built so a tree Consists of a sequence of binary decisions of binary splits So it first of all decided to split on coupler system Greater than or less than point five that's a Boolean variable. That's actually true or false and Then within the group where coupler system was true It decided to split into year made greater than or less than 1987 and Then where coupler system was true and year made was less than or equal to 1986 It used fi product class desk is less than or equal to point seven five and so forth right so right at the top We have 20,000 samples 20,000 rows right and the reason for that is because that's what we asked for here when we split our data in the sample I just want to double check that for your Decision tree that you have there that the coloration was whether it's true or false not so like it gets darker It's true for the next one not the darker is a higher value. We'll get to that in a moment, okay? So let's look at these numbers here, so in the whole data set well our Sample that we're using there are 20,000 rows the me at the average of the log of price is 10.1 and If we built a model Where we just use that average all the time then the mean squared error would be point four seven seven okay? So this is in other words the denominator of an R squared All right, this is like the most basic model is a tree with zero splits right which is just predict the average so the best single binary split we can make Turns out to be splitting by where the coupler system is Greater than or equal to sorry less than or equal to or greater than point five in other words whether it's true or false And it turns out if we do that the mean squared error of coupler system is less than point five so it's false Goes down from point four seven seven to point one one right so it's really improved the error a lot In the other group it's only improved it a bit. It's gone from point four seven to point four one and So we can see that the coupler system equals false group has a pretty small percentage It's only got twenty two hundred of the twenty thousand Right where else this other group has a much large percentage, but it hasn't improved it as much So let's say You wanted to create a Tree with just one split so you're just trying to find like what is the very best Single binary decision you can make For your data how might you be able to do that? How could you do it? You're gonna give it to fort Specify the max depth of one, but I mean you're writing you don't have a random first Right how are you gonna? How are you gonna like write? What's an algorithm a simple algorithm which you could use? sure So we want to start building a random forest from scratch So the first step is to create a tree the first step to create a tree is to create the first binary decision How are you going to do it? I'm going to give it to Chris Maybe in two steps or good So isn't this simply trying to find the best predictor based on maybe a linear regression? You could use a linear regression, but could you do something? Much simpler and more complete we're trying not to use any statistical assumptions here. I can't see your name, sir It's of course your prints I need that Can we just do like take just one variable if it is true give it like The true thing and if it is false so which variable are we going to choose so at each binary point we have to choose a variable and Something to split on how are we going to do that? And pass it over there how do I pronounce your name? So the variable to choose could be like which divides population into two groups which are kind of heterogeneous to each other and Homogeneous within themselves like having the same quality within themselves and they're very different could you be more specific? Like in terms of the target variable maybe yes, right? Let's say we have two groups after split so one has a different price altogether from the second group Yes, internally they have simpler prices. Okay, that's good So like to simplify things a little bit where we're saying find a variable that we could split into Such that the two groups are as different to each other as possible and Okay, how do you how would you pick which variable and which split point? That's the question? Yeah, what's your first cut which variable and which split point? We don't like we're making a tree from scratch we want to create our own tree That makes sense. We've got somebody over here mostly a positive Can we test all of the possible split and see which one has a small and RMSE and that sounds good Okay, so let's dig into this. So when you say test all of the possible splits What does that mean? How do we enumerate all the possible splits? Oh? I think of that but For each variable you could put one aside And then put a second aside and compare the two and if it was better Good okay, so for each variable for each possible value of that variable See whether it's better Now give it back to Maisley, so I want to dig into the better when you said see if the RMSE is better What does that mean though because after a split you've got two? RMSEs you've got two two groups So you're just gonna fit with that one variable comparing to the others not so so what I mean here Is that before we decided to spit on coupler system? Yeah, we had a root the mean squared of point four seven seven and after we've got two groups One with a mean squared error of point one another with a mean squared error of point four So you treat each individual model separately so for the first play you're just going to compare between each variable themselves And then you move on to the next note with the remaining, but but even the first node like So the model with zero splits has a single root mean squared error The model with one split so the very first thing we try we've now got two groups with two mean squared errors You want to give it to Daniel? Do you pick the one that gets them as different as they can be well which well okay, that would be one idea Get the two mean squared errors as different as possible But why might that not work? What might be a problem with that? sample size Go on because you could just literally leave one point out Yeah, so we could have like year made is less than 1950 and it might have a single sample with a low price and like that's not a great split Is it you know because the other group is actually not going to be very interesting at all? Can you improve it a bit can Jason improve it a bit? Could you take a weighted average Yeah, a weighted average so we could take point four one times seventeen thousand plus point one times two thousand That's good right and that would be the same as actually saying I've got a model The model is a single binary decision, and I'm going to say for everybody with year made less than 986.5. I'm going to fill in Point ten point two for everybody else are going to fill in nine point two and then I'm going to calculate the root mean squared error of this Crappy model and that would give exactly the same right as the weighted average that you're suggesting Okay, good, so we now have a single number that represents How good a split is which is the weighted average of the mean squared errors of the two groups it creates? Okay, and Thanks to I think it was was it Jake we have a way to find the best split Which is to try every variable and to try every possible value of that variable and see which variable and which value Gives us a split with the best score That makes sense Okay, what's your name, sir? Okay, can somebody give Natalie the box When you see every possible number for every possible variable like Are you saying like here? We have point five as like our? criteria to split the tree sorry Are you saying we're trying out every single number for? Every possible value right so coupler system only has two values true and false So there's only one way of splitting which is trues and falses EMA is an integer which varies between like I don't know 1960 and 2010 so we can just say what are all the possible unique values? If you're made and and try them all so we're trying all the possible Spec points can you pass that back to Daniel or pass it to me and I'll pass it to Daniel So I just want to clarify again for the first split Why? Did we split on coupler system true or false because what we did was we used Jake's technique we tried every variable For every variable we tried every possible split for each one we noted down I think it was Jason's idea which was the weighted average mean squared error of the two groups are created We found which one had the best Mean squared error and we picked it and it turned out it was coupler system true or false Does that make sense I guess my question is more like so coupler system is like one of the like Best indicators I guess it's the best okay. We tried every variable and every possible level Level after that it gets less and less everything else it tried wasn't as good Okay, and then you do that each time you split right so now that we've done that we now take this group here Everybody who's got coupler system equals true and we do it again for every possible variable for every possible level For people where coupler system equals true. What's the best possible split and then are there circumstances when it's not just like Binary like you split it into like three groups for like example you're made So I'm gonna make a claim and then I'm gonna see if you can justify it I'm gonna claim that it's never necessary to do more than one split at a level Because you can just let it again because you can just split it again exactly so you can get exactly the same result by splitting twice Okay good so That is the entirety of creating a decision tree You stop either when you hit some limit that was requested so we had a limit where we said max depth equals 3 So that's one one way to stop would be you asked to stop at some point and so we stopped otherwise you stop when your Your leaf nodes these things at the end are called leaf nodes when your leaf nodes only have one thing in them Okay, that's a decision tree That is how we grow a decision tree and this decision tree is not very good because it's got a validation R squared of 0.4 So we could try to make it better by removing max depth equals 3 Right and creating a deeper tree, so it's going to go all the way down We're going to keep splitting these things further until every leaf node only has one thing in it and If we do that the training R squared is of course one Because we can exactly predict every training element because it's in a leaf node all on its own But the validation R squared is not one. It's actually better than our really really shallow tree But it's not as good as we'd like Okay So we want to find some other way of of making these trees better and The way we're going to do it is to create a forest So what's a forest to create a forest? We're going to use a statistical technique called bagging and You can bag Any kind of model in fact Michael Jordan who is one of the speakers at the recent Data Institute conference here at University of San? Francisco developed a technique called the bag of little bootstraps and Which he shows how to use bagging for absolutely any kind of model to make it more robust and also to give you confidence intervals The random forest is simply a way of bagging trees, so what is bagging? Bagging is a really interesting idea, which is what if we created five different models? Each of which was only somewhat predictive, but the models weren't at all correlated with each other they gave Predictions that weren't correlated with each other that would mean that the five models would have to have found different insights into the relationships in the data And so if you took the average of those five models right then you're effectively bringing in the insights from each of them and So this idea of averaging models is a is is a technique for on the sampling right which is really important Now let's come up with a more specific idea of how to do this on sampling What if we created a whole lot of? these trees big deep Massively over fit trees right but each one. Let's say we only pick a random 1-10th of the data so we pick one out of every 10 rows at random build a deep tree right, which is Perfect on that subset and kind of crappy on the rest All right, let's say we do that a hundred times a different random sample every time So all of the trees are going to be better than nothing right because they do actually have a real random subset of the data And so they found some insight, but they're also overfitting terribly, but they're all using different random samples So they all overfit in different ways on different things so in other words they all have errors But the errors are random What is the average of a bunch of random errors? zero So in other words if we take the average of these trees each of which have been trained on a different random subset The errors will average out to zero and what's left is the true relationship? And that's the random forest So there's the technique right we've got a whole bunch of rows of data We grab a few at random right put them into a smaller data set and Build a tree based on that okay, and then we put that tree aside and Do it again with a different random subset and do it again with a different random subset do it a whole bunch of times and Then for each one we can then make predictions by running our test data Through the tree to get to the leaf node take the average in that leaf node for all the trees and average them all together So to do that We simply call random forest regressor and by default it creates 10 what scikit-learn calls Estimators an estimator is a tree right so this is going to create 10 trees and So we go ahead and train it I? Can't remember if I remember to Okay, so create our 10 trees, and we're just doing this on our little random subset of 20,000 and So let's take a look at one example. Can you pass the box to Devin? Just to make sure I'm understanding this so you're saying like we take 10 kind of crappy models We average 10 crappy models, and we get a good model exactly because the crappy models Based on different random subsets and so the errors are not correlated with each other if the other errors were correlated with other This isn't going to work okay, so the key insight here is to construct multiple models Which are better than nothing and where the errors are as much as possible not correlated with each other So is there like a certain number of trees that like we need that in order to be valid like There's no such thing as like valid or invalid There's like has a good validation set RMSE or not you know And so that's what we're going to look at is how to is how to make that metric higher And so this is the first of our hyper parameters And we're going to learn about how to tune hyper parameters and the first one is going to be the number of trees And we're about to look at that now Yes, mostly The subset that you are selecting are they exclusive can can you have overlapping? Yeah, so I mentioned you know one approach would be pick out like a tenth at random But actually what scikit-learn does by default is for n rows it picks out n rows with replacement Okay, and that's called bootstrapping and if memory serves me correctly that gets you an average 63.2 percent of the rows will be represented and you know a bunch of them will be represented multiple times Yeah Sure So rather than just picking out like a tenth of the rows at random instead We're going to pick out of an n row data set. We're going to pick out n rows with replacement Which on average gets about 63. I think 63.2 percent of the rows will be represented Many of those rows will appear multiple times Because a question behind you In essence what this model is doing is if I understand credit is just picking out the Date points that look very similar to the one you're looking at Yeah, that's a great insight. So what a tree is kind of doing There's not quite complicated way of going about doing that if you I mean there would be other ways of like assessing similarity There are other ways of saying assessing similarity, but what's interesting about this way is it's doing it in in tree space Right so we're basically saying what are in this case like for this little tree? What are the 593 samples you know closest to this one? And what's the average closest in tree space so other ways of doing that would be like and we'll learn later on in this course About k nearest neighbors you could use like Euclidean distance say right But here's the thing The whole point of machine learning is to identify Which variables actually matter the most and how do they relate to each other and to your dependent variable together? Right so if you've like imagine a synthetic data set where you create five variables That add together to your independent to create your dependent variable and 95 variables Which are entirely random and don't impact your dependent variable, and then if you do like a k nearest neighbors in Euclidean space You're going to get meaningless nearest neighbors because most of your columns are actually meaningless or Imagine your actual relationship is that your dependent variable equals? X1 times x2 then you're actually need to find this interaction right so you don't actually care about how close it is to X1 and how close to x2, but how close to the product so the entire purpose of modeling in machine learning is to find? A model which tells you which variables are important, and how do they interact together to drive your dependent variable? And so you'll find in practice the difference between Using like tree space or random forest space to find your nearest neighbors versus like Euclidean space It's the difference between a model that makes good predictions and the model that makes meaningless predictions Melissa do you have I did, but I feel like we've got only 35 minutes, so Great so So in general a machine learning model, which is effective is one which is Accurate when you look at the training data. It's it's it's accurate at predicting at actually Finding the relationships in that training data, and then it generalizes well to new data And so in bagging that means that each of your individual estimators If your individual trees you want to be as predictive as possible But for the predictions of your individual trees to be as uncorrelated as possible And so the inventor of random forests talks about this at length in his original paper that introduced this in the late 90s this idea of trying to come up with predictive, but poorly correlated trees the the research community in recent years has generally found that The more important thing seems to be creating uncorrelated trees rather than more accurate trees So more recent advances tend to create trees which are less predictive on their own But also less correlated with each other so for example in scikit-learn There's another class you can use called extra trees regressor or extra trees classifier with exactly the same API You can try it tonight. Just replace my random forest regressor with that that's called an extremely randomized trees Model and what that does is exactly the same as what we just discussed But rather than trying every split of every variable that randomly tries a few splits of a few variables Right so it's much faster to train it has more randomness Okay, but then you've that time you can build more trees and therefore get better generalization So in practice if you've got crappy individual models you just need more trees to get a good end-up model Melissa could you pass that over to Devon? Could you talk a little bit more about what you mean by like uncorrelated trees yeah? If I build a thousand trees each one on just ten data points Then it's quite likely that the ten data points for every tree are going to be totally different And so it's quite likely that those ten trees are going to a thousand trees are going to give totally different answers to each other so the correlation Between the predictions of tree one and tree two is going to be very small between tree one and tree three very small and so forth on the other hand if I create a thousand trees Where each time I use the entire data set with just one element removed all those trees are going to be nearly identical ie their predictions will be highly correlated and so in the latter case It's probably not going to generalize very well Where else in the former case the individual trees were not going to be very predictive, so I need to find some nice in between So yes Danielle And is there a case where you want to use one over the other like any particular times? Yeah, so again hyper parameter tuning so do you mean in terms of like random random forests versus extremely randomized trees? Yeah, so again a hyper parameter what tree architecture do we use so we're going to talk about that now Can you pass that to dinner? Yeah, I was just trying to understand how this random forest actually makes sense for continuous variables I mean I'm assuming that you build a tree structure and the last final nodes you'd be saying like maybe this node represents Maybe a category a or a category B, but how does it make sense for a continuous target? So this is actually what we have here, and so the value here is the average So this is the average Log of price for this subgroup And that's all we do the prediction is the average of the value of the dependent variable in that leaf node Finally if you have just like 10 leaf nodes you just have 10 values yes That's well if it was only one tree all right So a couple of things to remember the first is that by default we're actually going to train the tree all the way down until The leaf nodes are of size one Which means for a data set with n rows we're going to have n leaf nodes And then we're going to have multiple trees which we average together right so in practice We're going to have a you know lots of different possible values It's a question behind you So for the continuous variable how do we decide like which value to split out because there can be many values we try every possible Value of that in the training set Won't it be computationally computationally expensive? And this is where it's very good to remember that your CPU's performance is measured in gigahertz Which is billions of clock cycles per second and it has multiple cores and each core Has something called SIMD single instruction multiple data where it can direct up to eight computations per core at once And then if you do it on the GPU the performance is measured in teraflops So trillions of floating-point operations per second and so this is where when it comes to Designing algorithms, it's very difficult for us mere humans to realize how stupid algorithms should be Given how fast today's computers are so yeah, it's quite a few operations But at trillions per second you hardly notice it Masha I have a question so essentially at each mode we make a decision Like which category to which variable to use and which clip point yes Yeah, but one thing I can't understand so we have MSE a calculated for each node right so this is kind of our one of the decision criteria But this MSE it is calculated for which model like which model underlies like the model is the model is For the initial root mode is what if we just predicted the average Right which is here is 10.098 Oh just just the average and then the next model is what if we predicted the average of those people with coupler system equals false and For those people with coupler system equals true, and then the next is what if we predicted the average of coupler systems equals true? Here made less than 96 is it always average or we can use median or we can even run Linear regression there's all kinds of things we could do in practice the average works really well there are There are types of they're not called random forests, but there are kinds of trees where the leaf nodes are independent linear aggressions They're not terribly widely used, but there are certainly researchers who have worked on them, okay? Thank you And pass it back over that afford and then to check you So this tree has a depth of three yeah, and Then I on one of the next commands we get rid of the max depth yes the tree without the max depth Does that contain the tree with with the depth of three yeah? Yeah, well except in this case we've added randomness, but if you turn boots jumping off then yeah, the the deeper tree will you know the Less deep tree would be how it starts and then it just keeps spinning okay? So you have many trees you're going to have Different leaf nodes across trees hopefully so we want so how do you average leaf nodes? Across different trees, so we just take the first row in the validation set we run it through the first tree We find its average nine point two eight then do it through the next tree find its average in the second tree 9.95 and so forth and we're about to do that so you'll see it okay, so let's try it right so After you've built a random forest Each tree is stored in this attribute called estimators underscore Okay, so one of the things that you guys need to be very very comfortable with is using list comprehensions Okay, so I hope you've all been practicing okay, so here. I'm using a list comprehension to go through each tree in my model I'm going to call predict on it with my validation set and so that's going to give me a list of arrays of Predictions so each array will be all of the predictions for that tree, and I have ten trees NP dot stack Concatenates them together on a new axis so after I run this and Call dot shape You can see I now have the first axis ten means I have my ten different sets of predictions And for each one my validation set is a size 12,000 so here are my 12,000 predictions for each of the ten trees right, so Let's take the first row of that and print it out and so here are What we're just saying here are ten predictions one from each tree Okay, and so then if we say take the mean of that here is the mean of those ten predictions and Then what was the actual the actual was nine point one our prediction was nine point? Oh seven so you see how like none of our individual trees had very good predictions, but the main of them was actually pretty good right and so When I talk about experimenting like Jupiter notebook is great for experimenting This is the kind of stuff I mean dig inside these objects and like look at them plot them take your own averages Crosscheck to make sure that they work the way you thought they did write your own implementation of R-squared make sure it's the same as a psychic learn version plot it like here's an interesting plot. I did Let's go through each test the ten trees Right and then take the mean of all of the predictions up to the i3 Right so let's start by predicting Just based on the first tree then the first two trees than the first three trees And let's then plot the R squared So here's the R squared of just the first tree Here's the R squared of the first two trees three trees four trees blah blah blah blah up to ten trees And so not surprisingly R squared keeps improving right because the more Estimators we have the more bagging that we're doing the more it's well. It's going to generalize Right and you should find that that number there Bit under point eight six should match this number here Okay Let's rerun that yeah, okay, so that actually slightly above point eight six right so again These are all about the cross checks you can do the things you can visualize to deepen your understanding Okay, so as we add more trees our R squared improves. It seems to flatten out After a while so we might guess that if we increase the number of estimators to 20 Right it's maybe not going to be that much better So let's see we've got point eight six two Versus point eight six oh yeah, so doubling the number of trees didn't help very much, but double it again Eight six seven double it again eight six nine so you can see like There's some point at which you're going to you know not want to add more trees not because it's never going to get worse All right because every tree is you know giving you more? Semi-random models to bag together right, but it's going to stop improving things much Okay, and so this is like the first type of parameter if you learn to set is number of estimators and the method for setting it is as many as You have time to fit and that actually Seem to be hopping Okay, now in practice. We're going to learn to set a few more hyper parameters Adding more trees slows it down But with less trees you can still get the same insights so I build most of my models in practice with like 20 to 30 trees and It's only like then at the end of the project or maybe at the end of the day's work I'll then try doing like I don't know a thousand trees and run it overnight Was there a question yes, can we pass that to Prince? So each tree might have different estimators different combination of estimators which tree isn't estimator So this is a synonym so in scikit-learn when they say estimator they mean trees So I mean features features tree each tree will have different breakpoints on different on different columns But if at the end we want to look at the important features, we'll get to that Yeah, so after we finish with kind of setting hyper parameters the next stage of the course will be Learning about what it tells us about the data If you need to know it now, you know for your projects feel free to look ahead there's a Lesson to RF interpretation is where we can see it Okay, so that's our first hyper parameter I Want to talk next about out-of-bag score Sometimes your data set will be kind of small and you won't want to pull out a validation set Because doing so means you now don't have enough data to build a good model. What do you do? There's a cool trick which is pretty much unique to random forests, and it's this What we could do is recognize That some of our in our first tree some of our columns sorry some of our rows Didn't get used So what we could do would be to pass those rows through the first tree and treat it as a validation set and Then for the second tree We could pass through the rows that weren't used for the second tree through it to create a validation set for that and so effectively we would have a different validation set for each tree and So now to calculate our prediction We would average all of the trees where that row was not used for training right, so for tree number one We would have the ones I've marked in blue here and then maybe for tree number two It turned out it was like this one this one this one and this one and so forth right so as long as you've got enough trees Every rows going to appear in the out of bag sample for one of them at least so you'll be averaging You know hopefully a few trees So if you've got a hundred trees It's very likely that all of the rows are going to appear many times in these out-of-bag samples So what you can do is you can create an out-of-bag prediction by averaging all of the trees you didn't use to train each Individual row and then you can calculate your root mean squared error r squared etc on that If you pass OOB score equals true to psychic learn it will do that for you And it will create an attribute Called OOB score underscore and so my little print score function here if that attribute exists it it adds it to the print So if you take a look here OOB score equals true. We've now got one extra number and It's R squared that is the R squared for the OOB sample Its R squared is very similar the R squared and the validation set which is what we hoped for Can we pass it? Is it the case that the The prediction for the OOB score has to be must be mathematically lower than the one for our entire forest Certainly, it's not true that the prediction is lower. It's possible for the accuracy Yeah It's not mathematically necessary that it's true, but it's going to be true on average because your average for each row Appears in less trees in the OOB samples and it does in the full set of trees So as you see here, it's a little less good So in general, it's a great insight Chris in general the OOB R squared will slightly Underestimate how generalizable the model is the more trees you add the less serious that underestimation is and for me in practice I Find it's totally good enough, you know in practice Okay, so This OOB score is is super handy and one of the things that's super handy for is you're going to see there's quite a few hyper parameters that we're going to set and We would like to find some automated way to set them And one way to do that is to do what's called a grid search a grid search is where there's a scikit-learn Function called grid search and you pass in the list of all of the parameters all of the hyper parameters that you want to tune you pass in for each one a list of all of the values of that hyper parameter you want to try and and it runs your model on every possible combination of all of those hyper parameters and tells you which one is the best and OOB score is a great like choice for Forgetting it to tell you which one is best in terms of OOB score like that's an example of something you can do with Oh, okay, which works well now If you think about it I kind of did something pretty dumb earlier which is I Took a subset of 30,000 rows of the data and it built all my models of that Which means every tree in my random forest is a different subset of that subset of 30,000 Why do that why not? Pick a different like a totally different tree Why not pick a different like a totally different subset of 30,000 each time? So in other words, let's leave the entire 300,000 records as is right and if I want to make things faster Right pick a different subset of 30,000 each time. So rather than bootstrapping the entire set of rows Let's just randomly sample a subset of the data and so we can do that So let's go back and recall Procter yet without the subset parameter to get all of our data again And so to remind you That is okay 400,000 in the whole data frame of which we have 389,000 in our training set and instead We're going to go set RF samples 20,000 remember that was the site of the 30,000 We use 20,000 of them in our training set if I do this then now when I run a random forest It's not going to bootstrap an entire set of 391,000 rows it's going to just grab a subset of 20,000 rows Right and so now if I run this It will still run Just as quickly as if I had like originally done a random sample of 20,000, but now every tree Can have access to the whole data set right so if I do enough estimators enough trees eventually it's going to see everything Right so in this case with 10 trees, which is the default I get an R squared of 0.86 which is actually about the same as my R squared with the with the 20,000 subset and That's because I haven't used many estimators yet right, but if I increase the number of estimators It's going to make more of a difference right so if I increase the number of estimators To 40 It's going to take a little bit longer to run But it's going to be able to see a larger subset of the data set and so as you can see the R Squareds gone up from 0.86 to 0.876 Okay, so this is actually a great approach and for those of you who are doing the groceries competition That's got something like 120 million rows, but there's no way you would want to create a random forest Using 128 million rows in every tree like it's going to take forever ever so what you could do is use this set are of samples to do like I don't know a hundred thousand or a Million or play around with it so the trick here is that with a random forest using this technique? No data set is too big. I don't care if it's got a hundred billion rows right you can Create a bunch of trees each one of the different random subset can somebody pass the actually I can see So my question was for the all these scores and these ones does it take the only like for the the Ones from the sample or do you take from all the that's a great question um? so unfortunately Psychic learn does not support this functionality out of the box, so I had to write this and It's kind of a horrible hack right because we'd much rather be passing in like a Sample size parameter rather than doing this kind of setting up here, so what I actually do is Is if you look at the source code is I'm actually this is an internal This is the internal function I looked at their source code that they call and I've replaced it with a with a lambda function That has the behavior we want Unfortunately the current version is not changing how OOB is calculated so Yeah, so currently OOB scores and set RF samples are not compatible with each other so you need to turn OOB equals false If you use this approach Which I hope to fix But at this stage, it's it's not fixed So if you want to turn it off you just call Reset RF samples okay, and that returns it back to what it was Okay, so in practice when I'm like Doing interactive Machine learning using random forests in order to like explore my model explore hyper parameters The stuff we're going to learn in the future lesson where we actually analyze like feature importance and partial dependence and so forth I generally use subsets and reasonably small forests Because all the insights that I'm going to get are exactly the same as the big ones But I can run that in like you know three or four seconds rather than hours right, so this is one of the biggest tips I can give you and Very very few people in industry or academia Actually do this most people run all of their models on all of the data all of the time using their best possible parameters Which is just pointless right if you're trying to find out like which features are important, and how are they related to each other and so forth? Having that fourth decimal place of accuracy isn't going to change any of your insights at all okay? So I would say like do most of your models on you know a large enough sample size that your accuracy is You know reasonable when I say reasonable. It's like Within a reasonable distance of the best accuracy you can get And it's taking you know a small number of seconds to train so that you can interactively do your analysis So there's a couple more parameters. I wanted to talk about so I'm going to call reset RF samples to get back to our full Data set because in this case at least on this computer. It's actually running in less than 10 seconds So here's our baseline We're going to do a baseline with 40 estimators Okay, and so each of those 40 estimators is going to train all the way down to all the leaf nodes just have one sample in them So that's going to take a few seconds to run here we go so that gets us a point eight nine eight R squared on the validation set or point nine oh eight on the OOB Now this case the OOB is better. Why is it better? Well that's because remember our validation set is not a random sample our validation set is a different time period Okay, so it's actually much harder to predict a different time period than this one which is just predicting random Okay, so that's why this is not the way around we expected so The next the first parameter we can try fiddling with is min samples leaf and so min samples leaf says Stop training the tree further when your leaf node has Three or less Samples in they're out going all the way down until there's one we're going to go down until there's three So in practice this means it's going to be like one or two less levels of decision being made So it means we've got like half the number of actual decision criteria. We have to do so it's going to train more quickly It means that when we look at an individual tree rather than just taking one point We're taking the average of at least three points that's where we'd expect the trees to generalize each one to generalize a little bit better Okay, but each tree is probably going to be slightly less powerful on its own So let's try training that so Possible values of min samples leaf I find ones which work well are kind of one three five ten 25 You know like I find that kind of range Seems to work well, but like sometimes if you've got a really big data set and you're not using the small samples You know you might need a min samples leaf of hundreds or thousands, so it's you kind of got to think about like How big are your sub samples going through and try things out now in this case going from the default of one? To three has increased our Validation set R squared from 898 to 902 so it's a slight improvement, and it's going to train a little faster as well Okay, something else you can try which is and so since this worked I'm going to leave that in I'm going to add in max features equals point five what does max features do? well the idea is that The less correlated your trees are with each other the better Now imagine you had one column that was So much better than all of the other columns of being predictive that every single tree you built Regardless of like which subset of rows always started with that column So the trees are all going to be pretty similar right, but you can imagine There might be some interaction of variables where that interaction is more important than that individual column So if every tree always splits on the first thing the same thing the first time You're not going to get much variation in those trees So what we do is in addition to just taking a subset of rows We then at every single split point take a different subset of columns So it's slightly different to the row sampling for the row sampling each new tree is based on a random set of rows For column sampling every individual binary split we choose from a different subset of columns so in other words Rather than looking at every possible level of every possible column We look at every possible level of a random subset of columns Okay, and each time each decision point each binary split we use a different random subset How many well you get to pick point five means? randomly choose half of them The default is to use all of them There's also a couple of special values you can use here As you can see in max features You can also pass in square root to get square root of features or log 2 to get log 2 of features so in practice Good values I found are ranged from one 0.5 log 2 or square root that's going to give you a nice bit of variation right can somebody pass it to Danielle and So just to clarify does that Just like break it up smaller each time it goes through the tree or is it just taking half of what's left over or like? Hasn't been touched each time there's no such thing as what's left over after you've split on Yeah made less than or greater than 1984 Mm-hmm you made still there right so later on you might then split on you made less than or greater than 1989 So so it's just each time rather than checking every variable to see where its best split is you just check Half of them and so the next time you check a different half the next time you check a different half But I mean like terms is as you get like further to like the leafs You're gonna have less options right no no not you never remove the variables Okay, you can use them again and again and again because you've got lots of different split points So imagine for example that the relationship was just entirely linear between Year-made and price right then in practice to actually model that you know your real relationship is year-made versus price Right but the best we could do would be this kind of first of all split here Right and then to split here and here Right and like split and split and split so they were binary Yeah, even if they're binary Most random forest libraries don't do anything special about that they just kind of go okay We'll try this variable. Oh it turns out. There's only one level left. You know so yeah that definitely They don't do any kind of clever bookkeeping. Okay? Okay, so if we add max features equals 0.5. It goes up from 901 to 906 So that's better still and so as we've been doing this you also hopefully have noticed that our root mean squared error Of log price has been dropping on a validation set as well And so it's now down to point two two eight six So how good is that right so like our? Totally untuned random forest got us in about the top 25 percent now remember our validation set Isn't identical to the Kaggle test set right and this competition unfortunately is old enough that you can't Even put in an in a kind of after this after the time entry to find out how you would have gone So we can only approximate how we could have gone, but you know generally speaking It's going to be a pretty good approximation so two two eight six Here is the competition here's the public leaderboard Two two eight six there we go 14th or 15th place So you know roughly speaking looks like we would be about in the top 20 of this competition With basically totally brainless random forest with some totally brainless minor hyper parameter tuning and so This is kind of why the random forest is such an important Not just first step, but often only step from machine learning because it's kind of hard to screw it up like Even when we didn't tune the hyper parameters we still got a good result right and then a small amount of hyper parameter tuning got us a much better result and so Any kind of model so and I'm particularly thinking of like linear type models Which have a whole bunch of statistical assumptions, and you have to get a whole bunch of things right before they start to work at all Can really throw you off track right because they give you like totally wrong answers about how accurate the predictions can be For also the random forest You know generally speaking They tend to work on most data sets most of the time with most sets of hyper parameters so for example We Did this thing with it with our categorical variables in fact let's take a look at our tree Single tree Look at this right fi product class desk less than 7.5. What does that mean so? fi product Class desk Here's some examples of that column All right, so what does it mean to be less than or equal to 7 well we'd have to look at dot cat dot categories to find out Okay, and so it's 0 1 2 3 4 5 6 7 so what it's done is it's created a split where all of the backhoe loaders and These three types of hydraulic excavator enter in one group and everything else is in the other group so like that's like Weird you know like like these aren't even in order We could have made them in order if we had you know bothered to say the categories have this order, but we hadn't right so How come this even works like because when we turn it into codes? It's actually This is actually what the random forest sees and so imagine to think about this imagine like The only thing that mattered was whether it was a hydraulic excavator 0 to 2 metric tons and nothing else mattered Imagine that right so it has to pick out this this single level well It can do that because first of all it could say okay Let's pick out everything less than 7 versus greater than 7 to create You know this as one group and this is another group right and then within this group They could then pick out everything less than 6 versus greater than 6 which is going to pick out this one item right so with two split points We can pull out a single category So this is why it works right is because the tree is like infinitely flexible Even with a categorical variable if there's particular categories which have different levels of price It can like Gradually zoom in on those groups by using multiple splits All right now you can help it by telling it the order of your categorical variable, but even if you don't It's okay. It's just going to take a few more decisions to get there right and so you can see here It's actually using this product class desk quite a few times Right and and as you go deeper down the tree you'll see it used more and more right So we're else in a linear model or almost any kind of other model certainly any Any non-tree model pretty much? Encoding a categorical variable like this won't work at all because there's no linear relationship between totally arbitrary Identifiers and anything right so so these are the kinds of things that make random forests very easy to use and and Very resilient and so by using that you know we've gotten ourselves a model which is clearly You know world-class at this point already It's like you know probably well in the top 20 of this Kaggle competition and then in our next lesson we're going to learn about how to Analyze that model to learn more about the data to make it even better Okay, so this week I'm trying and like really experiment right have a look inside look try and draw the trees try and plot the Different errors try maybe using different data sets to see how they work Really experiment to try and get a sense and maybe try to like replicate things like write your own r-squared You know write your own versions of some of these functions See if yeah, see how much you can really learn about your data set about the random forest Okay, see you on Thursday", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 1.78, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.17487322489420573, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.004263680428266525}, {"id": 1, "seek": 0, "start": 1.78, "end": 7.38, "text": " From here the next two or three lessons. We're going to be really diving deep into random forests", "tokens": [3358, 510, 264, 958, 732, 420, 1045, 8820, 13, 492, 434, 516, 281, 312, 534, 20241, 2452, 666, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.17487322489420573, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.004263680428266525}, {"id": 2, "seek": 0, "start": 7.38, "end": 12.14, "text": " So so far all we've learned is there's a thing called random forests", "tokens": [407, 370, 1400, 439, 321, 600, 3264, 307, 456, 311, 257, 551, 1219, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.17487322489420573, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.004263680428266525}, {"id": 3, "seek": 0, "start": 12.98, "end": 18.22, "text": " For some particular data sets they seem to work really really well without too much trouble", "tokens": [1171, 512, 1729, 1412, 6352, 436, 1643, 281, 589, 534, 534, 731, 1553, 886, 709, 5253], "temperature": 0.0, "avg_logprob": -0.17487322489420573, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.004263680428266525}, {"id": 4, "seek": 0, "start": 18.7, "end": 21.94, "text": " But we don't really know yet like well. How do they actually work?", "tokens": [583, 321, 500, 380, 534, 458, 1939, 411, 731, 13, 1012, 360, 436, 767, 589, 30], "temperature": 0.0, "avg_logprob": -0.17487322489420573, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.004263680428266525}, {"id": 5, "seek": 0, "start": 23.1, "end": 26.400000000000002, "text": " What do we do if they don't work properly? What are their pros and cons?", "tokens": [708, 360, 321, 360, 498, 436, 500, 380, 589, 6108, 30, 708, 366, 641, 6267, 293, 1014, 30], "temperature": 0.0, "avg_logprob": -0.17487322489420573, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.004263680428266525}, {"id": 6, "seek": 2640, "start": 26.4, "end": 32.16, "text": " What are the what can we tune and so forth so we're going to look at all that and then after that we're going to look", "tokens": [708, 366, 264, 437, 393, 321, 10864, 293, 370, 5220, 370, 321, 434, 516, 281, 574, 412, 439, 300, 293, 550, 934, 300, 321, 434, 516, 281, 574], "temperature": 0.0, "avg_logprob": -0.1901499077125832, "compression_ratio": 1.7634854771784232, "no_speech_prob": 7.527863090217579e-06}, {"id": 7, "seek": 2640, "start": 32.16, "end": 36.4, "text": " At how do we interpret the results of random forests to get not just predictions?", "tokens": [1711, 577, 360, 321, 7302, 264, 3542, 295, 4974, 21700, 281, 483, 406, 445, 21264, 30], "temperature": 0.0, "avg_logprob": -0.1901499077125832, "compression_ratio": 1.7634854771784232, "no_speech_prob": 7.527863090217579e-06}, {"id": 8, "seek": 2640, "start": 36.9, "end": 42.56, "text": " But to actually deeply understand our data in a model driven way, so that's where we're going to go", "tokens": [583, 281, 767, 8760, 1223, 527, 1412, 294, 257, 2316, 9555, 636, 11, 370, 300, 311, 689, 321, 434, 516, 281, 352], "temperature": 0.0, "avg_logprob": -0.1901499077125832, "compression_ratio": 1.7634854771784232, "no_speech_prob": 7.527863090217579e-06}, {"id": 9, "seek": 2640, "start": 43.28, "end": 44.72, "text": " here", "tokens": [510], "temperature": 0.0, "avg_logprob": -0.1901499077125832, "compression_ratio": 1.7634854771784232, "no_speech_prob": 7.527863090217579e-06}, {"id": 10, "seek": 2640, "start": 44.72, "end": 46.72, "text": " So let's just review where we're up to", "tokens": [407, 718, 311, 445, 3131, 689, 321, 434, 493, 281], "temperature": 0.0, "avg_logprob": -0.1901499077125832, "compression_ratio": 1.7634854771784232, "no_speech_prob": 7.527863090217579e-06}, {"id": 11, "seek": 2640, "start": 48.44, "end": 55.64, "text": " So we learned that there's this library called fast AI and the fast AI library is", "tokens": [407, 321, 3264, 300, 456, 311, 341, 6405, 1219, 2370, 7318, 293, 264, 2370, 7318, 6405, 307], "temperature": 0.0, "avg_logprob": -0.1901499077125832, "compression_ratio": 1.7634854771784232, "no_speech_prob": 7.527863090217579e-06}, {"id": 12, "seek": 5564, "start": 55.64, "end": 60.18, "text": " It's basically it's a highly opinionated library, which is to say", "tokens": [467, 311, 1936, 309, 311, 257, 5405, 4800, 770, 6405, 11, 597, 307, 281, 584], "temperature": 0.0, "avg_logprob": -0.16814657436904087, "compression_ratio": 1.6481481481481481, "no_speech_prob": 5.173836598260095e-06}, {"id": 13, "seek": 5564, "start": 60.96, "end": 62.92, "text": " We've spent a lot of time", "tokens": [492, 600, 4418, 257, 688, 295, 565], "temperature": 0.0, "avg_logprob": -0.16814657436904087, "compression_ratio": 1.6481481481481481, "no_speech_prob": 5.173836598260095e-06}, {"id": 14, "seek": 5564, "start": 62.92, "end": 70.6, "text": " researching what are the best techniques to get like state-of-the-art results and then we take those techniques and package them into pieces of code", "tokens": [24176, 437, 366, 264, 1151, 7512, 281, 483, 411, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 293, 550, 321, 747, 729, 7512, 293, 7372, 552, 666, 3755, 295, 3089], "temperature": 0.0, "avg_logprob": -0.16814657436904087, "compression_ratio": 1.6481481481481481, "no_speech_prob": 5.173836598260095e-06}, {"id": 15, "seek": 5564, "start": 71.68, "end": 75.56, "text": " so that you can use the state-of-the-art results yourself and so", "tokens": [370, 300, 291, 393, 764, 264, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 1803, 293, 370], "temperature": 0.0, "avg_logprob": -0.16814657436904087, "compression_ratio": 1.6481481481481481, "no_speech_prob": 5.173836598260095e-06}, {"id": 16, "seek": 5564, "start": 78.16, "end": 79.88, "text": " Where possible", "tokens": [2305, 1944], "temperature": 0.0, "avg_logprob": -0.16814657436904087, "compression_ratio": 1.6481481481481481, "no_speech_prob": 5.173836598260095e-06}, {"id": 17, "seek": 5564, "start": 79.88, "end": 82.96000000000001, "text": " We wrap or provide things on top of", "tokens": [492, 7019, 420, 2893, 721, 322, 1192, 295], "temperature": 0.0, "avg_logprob": -0.16814657436904087, "compression_ratio": 1.6481481481481481, "no_speech_prob": 5.173836598260095e-06}, {"id": 18, "seek": 8296, "start": 82.96, "end": 87.91999999999999, "text": " Existing code and so in particular for the kind of structured data analysis. We're doing", "tokens": [2111, 468, 278, 3089, 293, 370, 294, 1729, 337, 264, 733, 295, 18519, 1412, 5215, 13, 492, 434, 884], "temperature": 0.0, "avg_logprob": -0.235067434871898, "compression_ratio": 1.6386138613861385, "no_speech_prob": 4.029414867545711e-06}, {"id": 19, "seek": 8296, "start": 88.63999999999999, "end": 91.47999999999999, "text": " Psychic learn has a lot of really great code", "tokens": [17303, 299, 1466, 575, 257, 688, 295, 534, 869, 3089], "temperature": 0.0, "avg_logprob": -0.235067434871898, "compression_ratio": 1.6386138613861385, "no_speech_prob": 4.029414867545711e-06}, {"id": 20, "seek": 8296, "start": 91.6, "end": 96.91999999999999, "text": " So most of the stuff that we're showing you from fast AI is stuff to help us get stuff into", "tokens": [407, 881, 295, 264, 1507, 300, 321, 434, 4099, 291, 490, 2370, 7318, 307, 1507, 281, 854, 505, 483, 1507, 666], "temperature": 0.0, "avg_logprob": -0.235067434871898, "compression_ratio": 1.6386138613861385, "no_speech_prob": 4.029414867545711e-06}, {"id": 21, "seek": 8296, "start": 97.63999999999999, "end": 101.83999999999999, "text": " Psychic learn and then interpret stuff out from psychic learn", "tokens": [17303, 299, 1466, 293, 550, 7302, 1507, 484, 490, 35406, 1466], "temperature": 0.0, "avg_logprob": -0.235067434871898, "compression_ratio": 1.6386138613861385, "no_speech_prob": 4.029414867545711e-06}, {"id": 22, "seek": 8296, "start": 103.32, "end": 105.32, "text": " the fast AI library", "tokens": [264, 2370, 7318, 6405], "temperature": 0.0, "avg_logprob": -0.235067434871898, "compression_ratio": 1.6386138613861385, "no_speech_prob": 4.029414867545711e-06}, {"id": 23, "seek": 8296, "start": 108.56, "end": 110.56, "text": " The way it works in our", "tokens": [440, 636, 309, 1985, 294, 527], "temperature": 0.0, "avg_logprob": -0.235067434871898, "compression_ratio": 1.6386138613861385, "no_speech_prob": 4.029414867545711e-06}, {"id": 24, "seek": 11056, "start": 110.56, "end": 113.56, "text": " Our environment here is that we've got", "tokens": [2621, 2823, 510, 307, 300, 321, 600, 658], "temperature": 0.0, "avg_logprob": -0.32077313983251177, "compression_ratio": 1.6028368794326242, "no_speech_prob": 4.4254302338231355e-06}, {"id": 25, "seek": 11056, "start": 117.28, "end": 118.8, "text": " Our", "tokens": [2621], "temperature": 0.0, "avg_logprob": -0.32077313983251177, "compression_ratio": 1.6028368794326242, "no_speech_prob": 4.4254302338231355e-06}, {"id": 26, "seek": 11056, "start": 118.8, "end": 124.08, "text": " Notebooks are inside fast AI repo slash courses and then slash ml1 and deal one", "tokens": [11633, 15170, 366, 1854, 2370, 7318, 49040, 17330, 7712, 293, 550, 17330, 23271, 16, 293, 2028, 472], "temperature": 0.0, "avg_logprob": -0.32077313983251177, "compression_ratio": 1.6028368794326242, "no_speech_prob": 4.4254302338231355e-06}, {"id": 27, "seek": 11056, "start": 126.24000000000001, "end": 130.28, "text": " And then inside there there's a sim link to", "tokens": [400, 550, 1854, 456, 456, 311, 257, 1034, 2113, 281], "temperature": 0.0, "avg_logprob": -0.32077313983251177, "compression_ratio": 1.6028368794326242, "no_speech_prob": 4.4254302338231355e-06}, {"id": 28, "seek": 11056, "start": 130.96, "end": 136.12, "text": " the parent of the parent fast AI so this is a sim link to a", "tokens": [264, 2596, 295, 264, 2596, 2370, 7318, 370, 341, 307, 257, 1034, 2113, 281, 257], "temperature": 0.0, "avg_logprob": -0.32077313983251177, "compression_ratio": 1.6028368794326242, "no_speech_prob": 4.4254302338231355e-06}, {"id": 29, "seek": 13612, "start": 136.12, "end": 138.12, "text": " directory", "tokens": [21120], "temperature": 0.4, "avg_logprob": -0.34242433117282006, "compression_ratio": 1.5863636363636364, "no_speech_prob": 1.370948666590266e-06}, {"id": 30, "seek": 13612, "start": 139.28, "end": 144.34, "text": " Containing a bunch of modules, so if you want to use the fast AI", "tokens": [4839, 3686, 257, 3840, 295, 16679, 11, 370, 498, 291, 528, 281, 764, 264, 2370, 7318], "temperature": 0.4, "avg_logprob": -0.34242433117282006, "compression_ratio": 1.5863636363636364, "no_speech_prob": 1.370948666590266e-06}, {"id": 31, "seek": 13612, "start": 145.66, "end": 147.66, "text": " Library in your own code", "tokens": [12806, 294, 428, 1065, 3089], "temperature": 0.4, "avg_logprob": -0.34242433117282006, "compression_ratio": 1.5863636363636364, "no_speech_prob": 1.370948666590266e-06}, {"id": 32, "seek": 13612, "start": 148.12, "end": 151.4, "text": " There's a number of things you can do one is to put your", "tokens": [821, 311, 257, 1230, 295, 721, 291, 393, 360, 472, 307, 281, 829, 428], "temperature": 0.4, "avg_logprob": -0.34242433117282006, "compression_ratio": 1.5863636363636364, "no_speech_prob": 1.370948666590266e-06}, {"id": 33, "seek": 13612, "start": 151.64000000000001, "end": 158.1, "text": " Notebooks or scripts in the same directory as ML1 or deal one whether it's already this sim link and just import it just like I do", "tokens": [11633, 15170, 420, 23294, 294, 264, 912, 21120, 382, 21601, 16, 420, 2028, 472, 1968, 309, 311, 1217, 341, 1034, 2113, 293, 445, 974, 309, 445, 411, 286, 360], "temperature": 0.4, "avg_logprob": -0.34242433117282006, "compression_ratio": 1.5863636363636364, "no_speech_prob": 1.370948666590266e-06}, {"id": 34, "seek": 13612, "start": 159.36, "end": 161.88, "text": " You could copy this directory", "tokens": [509, 727, 5055, 341, 21120], "temperature": 0.4, "avg_logprob": -0.34242433117282006, "compression_ratio": 1.5863636363636364, "no_speech_prob": 1.370948666590266e-06}, {"id": 35, "seek": 13612, "start": 162.32, "end": 164.32, "text": " dot dot slash dot slash fast AI", "tokens": [5893, 5893, 17330, 5893, 17330, 2370, 7318], "temperature": 0.4, "avg_logprob": -0.34242433117282006, "compression_ratio": 1.5863636363636364, "no_speech_prob": 1.370948666590266e-06}, {"id": 36, "seek": 16432, "start": 164.32, "end": 166.32, "text": " into", "tokens": [666], "temperature": 0.0, "avg_logprob": -0.17898621851084184, "compression_ratio": 1.7630331753554502, "no_speech_prob": 2.7264459276921116e-06}, {"id": 37, "seek": 16432, "start": 167.16, "end": 173.64, "text": " Somewhere else and use it or you could sim link it just like I have from here to wherever you want to use it", "tokens": [34500, 1646, 293, 764, 309, 420, 291, 727, 1034, 2113, 309, 445, 411, 286, 362, 490, 510, 281, 8660, 291, 528, 281, 764, 309], "temperature": 0.0, "avg_logprob": -0.17898621851084184, "compression_ratio": 1.7630331753554502, "no_speech_prob": 2.7264459276921116e-06}, {"id": 38, "seek": 16432, "start": 173.84, "end": 176.32, "text": " All right, so notice. It's mildly confusing", "tokens": [1057, 558, 11, 370, 3449, 13, 467, 311, 15154, 356, 13181], "temperature": 0.0, "avg_logprob": -0.17898621851084184, "compression_ratio": 1.7630331753554502, "no_speech_prob": 2.7264459276921116e-06}, {"id": 39, "seek": 16432, "start": 176.79999999999998, "end": 182.35999999999999, "text": " There's a github repo called fast AI and inside the github repo called fast AI", "tokens": [821, 311, 257, 290, 355, 836, 49040, 1219, 2370, 7318, 293, 1854, 264, 290, 355, 836, 49040, 1219, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.17898621851084184, "compression_ratio": 1.7630331753554502, "no_speech_prob": 2.7264459276921116e-06}, {"id": 40, "seek": 16432, "start": 183.2, "end": 187.04, "text": " Which looks like this there is a folder called fast AI", "tokens": [3013, 1542, 411, 341, 456, 307, 257, 10820, 1219, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.17898621851084184, "compression_ratio": 1.7630331753554502, "no_speech_prob": 2.7264459276921116e-06}, {"id": 41, "seek": 18704, "start": 187.04, "end": 194.04, "text": " Okay, and so the fast AI folder in the fast AI repo contains the fast AI library", "tokens": [1033, 11, 293, 370, 264, 2370, 7318, 10820, 294, 264, 2370, 7318, 49040, 8306, 264, 2370, 7318, 6405], "temperature": 0.0, "avg_logprob": -0.21557124885352882, "compression_ratio": 1.7218934911242603, "no_speech_prob": 1.0845100177903078e-06}, {"id": 42, "seek": 18704, "start": 194.72, "end": 200.44, "text": " And it's that library when we go from fast AI dot imports import star", "tokens": [400, 309, 311, 300, 6405, 562, 321, 352, 490, 2370, 7318, 5893, 41596, 974, 3543], "temperature": 0.0, "avg_logprob": -0.21557124885352882, "compression_ratio": 1.7218934911242603, "no_speech_prob": 1.0845100177903078e-06}, {"id": 43, "seek": 18704, "start": 200.84, "end": 203.44, "text": " Then that's looking inside the fast AI folder", "tokens": [1396, 300, 311, 1237, 1854, 264, 2370, 7318, 10820], "temperature": 0.0, "avg_logprob": -0.21557124885352882, "compression_ratio": 1.7218934911242603, "no_speech_prob": 1.0845100177903078e-06}, {"id": 44, "seek": 18704, "start": 204.0, "end": 207.68, "text": " for a file called imports imports dot py and", "tokens": [337, 257, 3991, 1219, 41596, 41596, 5893, 10664, 293], "temperature": 0.0, "avg_logprob": -0.21557124885352882, "compression_ratio": 1.7218934911242603, "no_speech_prob": 1.0845100177903078e-06}, {"id": 45, "seek": 18704, "start": 208.44, "end": 210.44, "text": " importing everything from that", "tokens": [43866, 1203, 490, 300], "temperature": 0.0, "avg_logprob": -0.21557124885352882, "compression_ratio": 1.7218934911242603, "no_speech_prob": 1.0845100177903078e-06}, {"id": 46, "seek": 18704, "start": 211.16, "end": 213.16, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.21557124885352882, "compression_ratio": 1.7218934911242603, "no_speech_prob": 1.0845100177903078e-06}, {"id": 47, "seek": 21316, "start": 213.16, "end": 219.07999999999998, "text": " Yes, Danielle, I sketched and", "tokens": [1079, 11, 21182, 11, 286, 12325, 292, 293], "temperature": 0.0, "avg_logprob": -0.3456095363316911, "compression_ratio": 1.5631067961165048, "no_speech_prob": 5.955020697001601e-06}, {"id": 48, "seek": 21316, "start": 220.6, "end": 225.32, "text": " Just like as a clarifying question for the same link. It's just the LN thing", "tokens": [1449, 411, 382, 257, 6093, 5489, 1168, 337, 264, 912, 2113, 13, 467, 311, 445, 264, 441, 45, 551], "temperature": 0.0, "avg_logprob": -0.3456095363316911, "compression_ratio": 1.5631067961165048, "no_speech_prob": 5.955020697001601e-06}, {"id": 49, "seek": 21316, "start": 228.2, "end": 229.32, "text": " The", "tokens": [440], "temperature": 0.0, "avg_logprob": -0.3456095363316911, "compression_ratio": 1.5631067961165048, "no_speech_prob": 5.955020697001601e-06}, {"id": 50, "seek": 21316, "start": 229.32, "end": 235.2, "text": " That's just the LN thing that you talked about last class. Yeah, so a sim link is something you can create by typing", "tokens": [663, 311, 445, 264, 441, 45, 551, 300, 291, 2825, 466, 1036, 1508, 13, 865, 11, 370, 257, 1034, 2113, 307, 746, 291, 393, 1884, 538, 18444], "temperature": 0.0, "avg_logprob": -0.3456095363316911, "compression_ratio": 1.5631067961165048, "no_speech_prob": 5.955020697001601e-06}, {"id": 51, "seek": 23520, "start": 235.2, "end": 242.32, "text": " LN minus s and then the path to the source which in this case would be dot dot dot dot fast AI", "tokens": [441, 45, 3175, 262, 293, 550, 264, 3100, 281, 264, 4009, 597, 294, 341, 1389, 576, 312, 5893, 5893, 5893, 5893, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.20816388542269482, "compression_ratio": 1.702127659574468, "no_speech_prob": 1.3419480637821835e-05}, {"id": 52, "seek": 23520, "start": 242.83999999999997, "end": 249.11999999999998, "text": " Could be relative or it could be absolute and then the name of the destination if you just put the current directory at the destination", "tokens": [7497, 312, 4972, 420, 309, 727, 312, 8236, 293, 550, 264, 1315, 295, 264, 12236, 498, 291, 445, 829, 264, 2190, 21120, 412, 264, 12236], "temperature": 0.0, "avg_logprob": -0.20816388542269482, "compression_ratio": 1.702127659574468, "no_speech_prob": 1.3419480637821835e-05}, {"id": 53, "seek": 23520, "start": 249.39999999999998, "end": 252.07999999999998, "text": " It'll use the same name as it comes from", "tokens": [467, 603, 764, 264, 912, 1315, 382, 309, 1487, 490], "temperature": 0.0, "avg_logprob": -0.20816388542269482, "compression_ratio": 1.702127659574468, "no_speech_prob": 1.3419480637821835e-05}, {"id": 54, "seek": 23520, "start": 252.72, "end": 254.35999999999999, "text": " like a", "tokens": [411, 257], "temperature": 0.0, "avg_logprob": -0.20816388542269482, "compression_ratio": 1.702127659574468, "no_speech_prob": 1.3419480637821835e-05}, {"id": 55, "seek": 25436, "start": 254.36, "end": 269.28000000000003, "text": " Alias on the Mac or a shortcut on Windows?", "tokens": [967, 4609, 322, 264, 5707, 420, 257, 24822, 322, 8591, 30], "temperature": 0.0, "avg_logprob": -0.46354385375976564, "compression_ratio": 1.408, "no_speech_prob": 2.0461018721107394e-05}, {"id": 56, "seek": 25436, "start": 269.28000000000003, "end": 271.28000000000003, "text": " and when you do the", "tokens": [293, 562, 291, 360, 264], "temperature": 0.0, "avg_logprob": -0.46354385375976564, "compression_ratio": 1.408, "no_speech_prob": 2.0461018721107394e-05}, {"id": 57, "seek": 25436, "start": 272.16, "end": 274.16, "text": " import sis can I just", "tokens": [974, 26288, 393, 286, 445], "temperature": 0.0, "avg_logprob": -0.46354385375976564, "compression_ratio": 1.408, "no_speech_prob": 2.0461018721107394e-05}, {"id": 58, "seek": 25436, "start": 275.32, "end": 281.84000000000003, "text": " Yeah, go go import sis and then append that relative link that also creates the sim link in", "tokens": [865, 11, 352, 352, 974, 26288, 293, 550, 34116, 300, 4972, 2113, 300, 611, 7829, 264, 1034, 2113, 294], "temperature": 0.0, "avg_logprob": -0.46354385375976564, "compression_ratio": 1.408, "no_speech_prob": 2.0461018721107394e-05}, {"id": 59, "seek": 28184, "start": 281.84, "end": 284.32, "text": " and the workbook to", "tokens": [293, 264, 589, 2939, 281], "temperature": 0.0, "avg_logprob": -0.27057647705078125, "compression_ratio": 1.5794871794871794, "no_speech_prob": 2.406071416771738e-06}, {"id": 60, "seek": 28184, "start": 285.11999999999995, "end": 291.7, "text": " Was this I don't think I've created the same link anywhere in the workbooks the sim link actually lives inside the github repo", "tokens": [3027, 341, 286, 500, 380, 519, 286, 600, 2942, 264, 912, 2113, 4992, 294, 264, 589, 15170, 264, 1034, 2113, 767, 2909, 1854, 264, 290, 355, 836, 49040], "temperature": 0.0, "avg_logprob": -0.27057647705078125, "compression_ratio": 1.5794871794871794, "no_speech_prob": 2.406071416771738e-06}, {"id": 61, "seek": 28184, "start": 292.28, "end": 295.4, "text": " Okay, I created some sim links in", "tokens": [1033, 11, 286, 2942, 512, 1034, 6123, 294], "temperature": 0.0, "avg_logprob": -0.27057647705078125, "compression_ratio": 1.5794871794871794, "no_speech_prob": 2.406071416771738e-06}, {"id": 62, "seek": 28184, "start": 296.96, "end": 301.12, "text": " The deep learning notebook to some data that was different. Yeah", "tokens": [440, 2452, 2539, 21060, 281, 512, 1412, 300, 390, 819, 13, 865], "temperature": 0.0, "avg_logprob": -0.27057647705078125, "compression_ratio": 1.5794871794871794, "no_speech_prob": 2.406071416771738e-06}, {"id": 63, "seek": 28184, "start": 302.28, "end": 306.94, "text": " At the top of Tim Lee's workbook from the last class there was", "tokens": [1711, 264, 1192, 295, 7172, 6957, 311, 589, 2939, 490, 264, 1036, 1508, 456, 390], "temperature": 0.0, "avg_logprob": -0.27057647705078125, "compression_ratio": 1.5794871794871794, "no_speech_prob": 2.406071416771738e-06}, {"id": 64, "seek": 30694, "start": 306.94, "end": 311.82, "text": " Import sis then append the fast AI. Oh, yeah, don't do that probably", "tokens": [26391, 26288, 550, 34116, 264, 2370, 7318, 13, 876, 11, 1338, 11, 500, 380, 360, 300, 1391], "temperature": 0.0, "avg_logprob": -0.2702478262094351, "compression_ratio": 1.6403508771929824, "no_speech_prob": 2.521550186429522e-06}, {"id": 65, "seek": 30694, "start": 311.82, "end": 315.2, "text": " I mean you you can but I think this is I", "tokens": [286, 914, 291, 291, 393, 457, 286, 519, 341, 307, 286], "temperature": 0.0, "avg_logprob": -0.2702478262094351, "compression_ratio": 1.6403508771929824, "no_speech_prob": 2.521550186429522e-06}, {"id": 66, "seek": 30694, "start": 315.92, "end": 320.56, "text": " think this is better like this way you can go from fast AI imports and", "tokens": [519, 341, 307, 1101, 411, 341, 636, 291, 393, 352, 490, 2370, 7318, 41596, 293], "temperature": 0.0, "avg_logprob": -0.2702478262094351, "compression_ratio": 1.6403508771929824, "no_speech_prob": 2.521550186429522e-06}, {"id": 67, "seek": 30694, "start": 321.8, "end": 325.12, "text": " Regardless of kind of how you got it there. It's it's gonna work. Okay, you know", "tokens": [25148, 295, 733, 295, 577, 291, 658, 309, 456, 13, 467, 311, 309, 311, 799, 589, 13, 1033, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.2702478262094351, "compression_ratio": 1.6403508771929824, "no_speech_prob": 2.521550186429522e-06}, {"id": 68, "seek": 30694, "start": 326.76, "end": 328.84, "text": " Great", "tokens": [3769], "temperature": 0.0, "avg_logprob": -0.2702478262094351, "compression_ratio": 1.6403508771929824, "no_speech_prob": 2.521550186429522e-06}, {"id": 69, "seek": 30694, "start": 328.84, "end": 335.04, "text": " Okay, so then we had all of our data for blue books to bulldozers competition in data slash bulldozers and", "tokens": [1033, 11, 370, 550, 321, 632, 439, 295, 527, 1412, 337, 3344, 3642, 281, 4693, 2595, 41698, 6211, 294, 1412, 17330, 4693, 2595, 41698, 293], "temperature": 0.0, "avg_logprob": -0.2702478262094351, "compression_ratio": 1.6403508771929824, "no_speech_prob": 2.521550186429522e-06}, {"id": 70, "seek": 33504, "start": 335.04, "end": 338.88, "text": " Here it is right so", "tokens": [1692, 309, 307, 558, 370], "temperature": 0.0, "avg_logprob": -0.22808762119240958, "compression_ratio": 1.5941176470588236, "no_speech_prob": 8.800988325674552e-06}, {"id": 71, "seek": 33504, "start": 339.96000000000004, "end": 347.0, "text": " We were able to read that CSV file the only thing we really had to do is to say which columns were dates", "tokens": [492, 645, 1075, 281, 1401, 300, 48814, 3991, 264, 787, 551, 321, 534, 632, 281, 360, 307, 281, 584, 597, 13766, 645, 11691], "temperature": 0.0, "avg_logprob": -0.22808762119240958, "compression_ratio": 1.5941176470588236, "no_speech_prob": 8.800988325674552e-06}, {"id": 72, "seek": 33504, "start": 347.8, "end": 349.8, "text": " And having done that", "tokens": [400, 1419, 1096, 300], "temperature": 0.0, "avg_logprob": -0.22808762119240958, "compression_ratio": 1.5941176470588236, "no_speech_prob": 8.800988325674552e-06}, {"id": 73, "seek": 33504, "start": 350.04, "end": 353.72, "text": " We were able to take a look at a few of the examples of the rows of the data", "tokens": [492, 645, 1075, 281, 747, 257, 574, 412, 257, 1326, 295, 264, 5110, 295, 264, 13241, 295, 264, 1412], "temperature": 0.0, "avg_logprob": -0.22808762119240958, "compression_ratio": 1.5941176470588236, "no_speech_prob": 8.800988325674552e-06}, {"id": 74, "seek": 35372, "start": 353.72, "end": 363.28000000000003, "text": " And so we also noted that it's very important to", "tokens": [400, 370, 321, 611, 12964, 300, 309, 311, 588, 1021, 281], "temperature": 0.0, "avg_logprob": -0.26645931444670024, "compression_ratio": 1.5294117647058822, "no_speech_prob": 4.78502306577866e-06}, {"id": 75, "seek": 35372, "start": 364.92, "end": 366.92, "text": " deeply understand the", "tokens": [8760, 1223, 264], "temperature": 0.0, "avg_logprob": -0.26645931444670024, "compression_ratio": 1.5294117647058822, "no_speech_prob": 4.78502306577866e-06}, {"id": 76, "seek": 35372, "start": 367.68, "end": 374.56, "text": " Evaluation metric for this project and so for Kaggle they tell you what the evaluation metric is", "tokens": [462, 46504, 20678, 337, 341, 1716, 293, 370, 337, 48751, 22631, 436, 980, 291, 437, 264, 13344, 20678, 307], "temperature": 0.0, "avg_logprob": -0.26645931444670024, "compression_ratio": 1.5294117647058822, "no_speech_prob": 4.78502306577866e-06}, {"id": 77, "seek": 37456, "start": 374.56, "end": 384.08, "text": " And in this case it was the root mean squared log error so that is the", "tokens": [400, 294, 341, 1389, 309, 390, 264, 5593, 914, 8889, 3565, 6713, 370, 300, 307, 264], "temperature": 0.0, "avg_logprob": -0.2528874609205458, "compression_ratio": 1.537190082644628, "no_speech_prob": 2.5612703211663757e-06}, {"id": 78, "seek": 37456, "start": 385.56, "end": 389.32, "text": " Sum of the actuals", "tokens": [8626, 295, 264, 3539, 82], "temperature": 0.0, "avg_logprob": -0.2528874609205458, "compression_ratio": 1.537190082644628, "no_speech_prob": 2.5612703211663757e-06}, {"id": 79, "seek": 37456, "start": 391.52, "end": 393.52, "text": " Minus", "tokens": [2829, 301], "temperature": 0.0, "avg_logprob": -0.2528874609205458, "compression_ratio": 1.537190082644628, "no_speech_prob": 2.5612703211663757e-06}, {"id": 80, "seek": 37456, "start": 394.76, "end": 396.76, "text": " The predictions", "tokens": [440, 21264], "temperature": 0.0, "avg_logprob": -0.2528874609205458, "compression_ratio": 1.537190082644628, "no_speech_prob": 2.5612703211663757e-06}, {"id": 81, "seek": 39676, "start": 396.76, "end": 404.56, "text": " Right, but it's the log of the actuals minus the log of the predictions", "tokens": [1779, 11, 457, 309, 311, 264, 3565, 295, 264, 3539, 82, 3175, 264, 3565, 295, 264, 21264], "temperature": 0.0, "avg_logprob": -0.21706492213879602, "compression_ratio": 1.696969696969697, "no_speech_prob": 2.406089379292098e-06}, {"id": 82, "seek": 39676, "start": 407.52, "end": 409.52, "text": " Squared", "tokens": [8683, 1642], "temperature": 0.0, "avg_logprob": -0.21706492213879602, "compression_ratio": 1.696969696969697, "no_speech_prob": 2.406089379292098e-06}, {"id": 83, "seek": 39676, "start": 409.68, "end": 411.68, "text": " Right so if we replace", "tokens": [1779, 370, 498, 321, 7406], "temperature": 0.0, "avg_logprob": -0.21706492213879602, "compression_ratio": 1.696969696969697, "no_speech_prob": 2.406089379292098e-06}, {"id": 84, "seek": 39676, "start": 413.84, "end": 420.92, "text": " Actuals with log actuals and replace predictions with log predictions, then it's just the same as root mean squared error", "tokens": [3251, 901, 82, 365, 3565, 3539, 82, 293, 7406, 21264, 365, 3565, 21264, 11, 550, 309, 311, 445, 264, 912, 382, 5593, 914, 8889, 6713], "temperature": 0.0, "avg_logprob": -0.21706492213879602, "compression_ratio": 1.696969696969697, "no_speech_prob": 2.406089379292098e-06}, {"id": 85, "seek": 42092, "start": 420.92, "end": 428.68, "text": " So that's what we did was we replaced sale price with log of sale price and so now", "tokens": [407, 300, 311, 437, 321, 630, 390, 321, 10772, 8680, 3218, 365, 3565, 295, 8680, 3218, 293, 370, 586], "temperature": 0.0, "avg_logprob": -0.2401587622506278, "compression_ratio": 1.712041884816754, "no_speech_prob": 5.453279072753503e-07}, {"id": 86, "seek": 42092, "start": 429.24, "end": 434.68, "text": " If we optimize for root mean squared error, we're actually optimizing for the", "tokens": [759, 321, 19719, 337, 5593, 914, 8889, 6713, 11, 321, 434, 767, 40425, 337, 264], "temperature": 0.0, "avg_logprob": -0.2401587622506278, "compression_ratio": 1.712041884816754, "no_speech_prob": 5.453279072753503e-07}, {"id": 87, "seek": 42092, "start": 435.44, "end": 437.44, "text": " root mean squared error of the logs", "tokens": [5593, 914, 8889, 6713, 295, 264, 20820], "temperature": 0.0, "avg_logprob": -0.2401587622506278, "compression_ratio": 1.712041884816754, "no_speech_prob": 5.453279072753503e-07}, {"id": 88, "seek": 42092, "start": 438.12, "end": 439.92, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2401587622506278, "compression_ratio": 1.712041884816754, "no_speech_prob": 5.453279072753503e-07}, {"id": 89, "seek": 42092, "start": 439.92, "end": 443.84000000000003, "text": " So then we learned that we need all of our columns to be numbers and", "tokens": [407, 550, 321, 3264, 300, 321, 643, 439, 295, 527, 13766, 281, 312, 3547, 293], "temperature": 0.0, "avg_logprob": -0.2401587622506278, "compression_ratio": 1.712041884816754, "no_speech_prob": 5.453279072753503e-07}, {"id": 90, "seek": 42092, "start": 445.44, "end": 449.98, "text": " So the first way we did that was to take the date column", "tokens": [407, 264, 700, 636, 321, 630, 300, 390, 281, 747, 264, 4002, 7738], "temperature": 0.0, "avg_logprob": -0.2401587622506278, "compression_ratio": 1.712041884816754, "no_speech_prob": 5.453279072753503e-07}, {"id": 91, "seek": 44998, "start": 449.98, "end": 455.62, "text": " And remove it and instead replace it with a whole bunch of different columns", "tokens": [400, 4159, 309, 293, 2602, 7406, 309, 365, 257, 1379, 3840, 295, 819, 13766], "temperature": 0.0, "avg_logprob": -0.1464088201522827, "compression_ratio": 1.530612244897959, "no_speech_prob": 4.6378399929380976e-06}, {"id": 92, "seek": 44998, "start": 462.14000000000004, "end": 466.18, "text": " Such as is that date the start of a quarter is at the end of a year", "tokens": [9653, 382, 307, 300, 4002, 264, 722, 295, 257, 6555, 307, 412, 264, 917, 295, 257, 1064], "temperature": 0.0, "avg_logprob": -0.1464088201522827, "compression_ratio": 1.530612244897959, "no_speech_prob": 4.6378399929380976e-06}, {"id": 93, "seek": 44998, "start": 467.02000000000004, "end": 474.1, "text": " How many days are elapsed since January the first 1970 what's the year? What's the month? What's the day of week and so forth?", "tokens": [1012, 867, 1708, 366, 806, 2382, 292, 1670, 7061, 264, 700, 14577, 437, 311, 264, 1064, 30, 708, 311, 264, 1618, 30, 708, 311, 264, 786, 295, 1243, 293, 370, 5220, 30], "temperature": 0.0, "avg_logprob": -0.1464088201522827, "compression_ratio": 1.530612244897959, "no_speech_prob": 4.6378399929380976e-06}, {"id": 94, "seek": 44998, "start": 474.34000000000003, "end": 476.34000000000003, "text": " Okay, so they're all numbers", "tokens": [1033, 11, 370, 436, 434, 439, 3547], "temperature": 0.0, "avg_logprob": -0.1464088201522827, "compression_ratio": 1.530612244897959, "no_speech_prob": 4.6378399929380976e-06}, {"id": 95, "seek": 47634, "start": 476.34, "end": 478.34, "text": " Then", "tokens": [1396], "temperature": 0.0, "avg_logprob": -0.17834589640299478, "compression_ratio": 1.5606060606060606, "no_speech_prob": 1.101590441976441e-06}, {"id": 96, "seek": 47634, "start": 478.97999999999996, "end": 486.21999999999997, "text": " We learned that we can use train underscore cats to replace all of the strings with categories", "tokens": [492, 3264, 300, 321, 393, 764, 3847, 37556, 11111, 281, 7406, 439, 295, 264, 13985, 365, 10479], "temperature": 0.0, "avg_logprob": -0.17834589640299478, "compression_ratio": 1.5606060606060606, "no_speech_prob": 1.101590441976441e-06}, {"id": 97, "seek": 47634, "start": 487.38, "end": 489.38, "text": " Now when you do that", "tokens": [823, 562, 291, 360, 300], "temperature": 0.0, "avg_logprob": -0.17834589640299478, "compression_ratio": 1.5606060606060606, "no_speech_prob": 1.101590441976441e-06}, {"id": 98, "seek": 47634, "start": 489.82, "end": 493.46, "text": " It doesn't look like you've done anything different. They still look like strings", "tokens": [467, 1177, 380, 574, 411, 291, 600, 1096, 1340, 819, 13, 814, 920, 574, 411, 13985], "temperature": 0.0, "avg_logprob": -0.17834589640299478, "compression_ratio": 1.5606060606060606, "no_speech_prob": 1.101590441976441e-06}, {"id": 99, "seek": 47634, "start": 494.53999999999996, "end": 497.46, "text": " Right, but if you actually take a deeper look", "tokens": [1779, 11, 457, 498, 291, 767, 747, 257, 7731, 574], "temperature": 0.0, "avg_logprob": -0.17834589640299478, "compression_ratio": 1.5606060606060606, "no_speech_prob": 1.101590441976441e-06}, {"id": 100, "seek": 49746, "start": 497.46, "end": 505.26, "text": " You'll see that the data type now is not string but category and", "tokens": [509, 603, 536, 300, 264, 1412, 2010, 586, 307, 406, 6798, 457, 7719, 293], "temperature": 0.0, "avg_logprob": -0.19042263755315467, "compression_ratio": 1.6358974358974359, "no_speech_prob": 2.948000883407076e-06}, {"id": 101, "seek": 49746, "start": 507.29999999999995, "end": 511.46, "text": " Category is a pandas class", "tokens": [383, 48701, 307, 257, 4565, 296, 1508], "temperature": 0.0, "avg_logprob": -0.19042263755315467, "compression_ratio": 1.6358974358974359, "no_speech_prob": 2.948000883407076e-06}, {"id": 102, "seek": 49746, "start": 512.42, "end": 518.8199999999999, "text": " Where you can then go dot cat dot and find a whole bunch of different attributes such as cat dot", "tokens": [2305, 291, 393, 550, 352, 5893, 3857, 5893, 293, 915, 257, 1379, 3840, 295, 819, 17212, 1270, 382, 3857, 5893], "temperature": 0.0, "avg_logprob": -0.19042263755315467, "compression_ratio": 1.6358974358974359, "no_speech_prob": 2.948000883407076e-06}, {"id": 103, "seek": 49746, "start": 519.14, "end": 526.1, "text": " Categories to find a list of all of the possible categories and this says high is going to be zero low will become one medium will", "tokens": [383, 2968, 2083, 281, 915, 257, 1329, 295, 439, 295, 264, 1944, 10479, 293, 341, 1619, 1090, 307, 516, 281, 312, 4018, 2295, 486, 1813, 472, 6399, 486], "temperature": 0.0, "avg_logprob": -0.19042263755315467, "compression_ratio": 1.6358974358974359, "no_speech_prob": 2.948000883407076e-06}, {"id": 104, "seek": 52610, "start": 526.1, "end": 528.86, "text": " Become two so we can then get codes", "tokens": [44308, 732, 370, 321, 393, 550, 483, 14211], "temperature": 0.0, "avg_logprob": -0.20919776824583491, "compression_ratio": 1.572192513368984, "no_speech_prob": 3.576351161882485e-07}, {"id": 105, "seek": 52610, "start": 529.98, "end": 531.98, "text": " To actually get the numbers", "tokens": [1407, 767, 483, 264, 3547], "temperature": 0.0, "avg_logprob": -0.20919776824583491, "compression_ratio": 1.572192513368984, "no_speech_prob": 3.576351161882485e-07}, {"id": 106, "seek": 52610, "start": 532.22, "end": 537.1800000000001, "text": " So then what we need to do to actually use this data set to turn it into numbers is", "tokens": [407, 550, 437, 321, 643, 281, 360, 281, 767, 764, 341, 1412, 992, 281, 1261, 309, 666, 3547, 307], "temperature": 0.0, "avg_logprob": -0.20919776824583491, "compression_ratio": 1.572192513368984, "no_speech_prob": 3.576351161882485e-07}, {"id": 107, "seek": 52610, "start": 537.5, "end": 543.26, "text": " Take every categorical column and replace it with cat dot codes and so we did that", "tokens": [3664, 633, 19250, 804, 7738, 293, 7406, 309, 365, 3857, 5893, 14211, 293, 370, 321, 630, 300], "temperature": 0.0, "avg_logprob": -0.20919776824583491, "compression_ratio": 1.572192513368984, "no_speech_prob": 3.576351161882485e-07}, {"id": 108, "seek": 52610, "start": 543.86, "end": 545.86, "text": " using", "tokens": [1228], "temperature": 0.0, "avg_logprob": -0.20919776824583491, "compression_ratio": 1.572192513368984, "no_speech_prob": 3.576351161882485e-07}, {"id": 109, "seek": 52610, "start": 548.14, "end": 550.14, "text": " Proc DF", "tokens": [1705, 66, 48336], "temperature": 0.0, "avg_logprob": -0.20919776824583491, "compression_ratio": 1.572192513368984, "no_speech_prob": 3.576351161882485e-07}, {"id": 110, "seek": 52610, "start": 550.4200000000001, "end": 551.74, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.20919776824583491, "compression_ratio": 1.572192513368984, "no_speech_prob": 3.576351161882485e-07}, {"id": 111, "seek": 55174, "start": 551.74, "end": 556.1800000000001, "text": " So how do I get the source code for Proc DF?", "tokens": [407, 577, 360, 286, 483, 264, 4009, 3089, 337, 1705, 66, 48336, 30], "temperature": 0.0, "avg_logprob": -0.20669975833616394, "compression_ratio": 1.3579545454545454, "no_speech_prob": 6.78671085552196e-07}, {"id": 112, "seek": 55174, "start": 561.66, "end": 563.66, "text": " Question question mark, okay", "tokens": [14464, 1168, 1491, 11, 1392], "temperature": 0.0, "avg_logprob": -0.20669975833616394, "compression_ratio": 1.3579545454545454, "no_speech_prob": 6.78671085552196e-07}, {"id": 113, "seek": 55174, "start": 567.74, "end": 570.82, "text": " All right, so if I scroll down I go through each", "tokens": [1057, 558, 11, 370, 498, 286, 11369, 760, 286, 352, 807, 1184], "temperature": 0.0, "avg_logprob": -0.20669975833616394, "compression_ratio": 1.3579545454545454, "no_speech_prob": 6.78671085552196e-07}, {"id": 114, "seek": 55174, "start": 571.42, "end": 577.38, "text": " Column and I numericalize it. Okay, that's actually the one I want so I'm going to now have to look up numericalized", "tokens": [4004, 16449, 293, 286, 29054, 1125, 309, 13, 1033, 11, 300, 311, 767, 264, 472, 286, 528, 370, 286, 478, 516, 281, 586, 362, 281, 574, 493, 29054, 1602], "temperature": 0.0, "avg_logprob": -0.20669975833616394, "compression_ratio": 1.3579545454545454, "no_speech_prob": 6.78671085552196e-07}, {"id": 115, "seek": 57738, "start": 577.38, "end": 579.38, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.20033304242120273, "compression_ratio": 1.3966480446927374, "no_speech_prob": 1.4367464018505416e-06}, {"id": 116, "seek": 57738, "start": 581.22, "end": 583.22, "text": " Tab to complete it", "tokens": [14106, 281, 3566, 309], "temperature": 0.0, "avg_logprob": -0.20033304242120273, "compression_ratio": 1.3966480446927374, "no_speech_prob": 1.4367464018505416e-06}, {"id": 117, "seek": 57738, "start": 584.42, "end": 586.42, "text": " If it's not numeric", "tokens": [759, 309, 311, 406, 7866, 299], "temperature": 0.0, "avg_logprob": -0.20033304242120273, "compression_ratio": 1.3966480446927374, "no_speech_prob": 1.4367464018505416e-06}, {"id": 118, "seek": 57738, "start": 586.86, "end": 592.1, "text": " Then replace the data frames field with that columns dot cat dot codes", "tokens": [1396, 7406, 264, 1412, 12083, 2519, 365, 300, 13766, 5893, 3857, 5893, 14211], "temperature": 0.0, "avg_logprob": -0.20033304242120273, "compression_ratio": 1.3966480446927374, "no_speech_prob": 1.4367464018505416e-06}, {"id": 119, "seek": 57738, "start": 592.98, "end": 598.26, "text": " Plus one because otherwise unknown is minus one. We want unknown to be zero. Okay, so", "tokens": [7721, 472, 570, 5911, 9841, 307, 3175, 472, 13, 492, 528, 9841, 281, 312, 4018, 13, 1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.20033304242120273, "compression_ratio": 1.3966480446927374, "no_speech_prob": 1.4367464018505416e-06}, {"id": 120, "seek": 57738, "start": 599.02, "end": 604.22, "text": " That's how we turn the strings into numbers, right?", "tokens": [663, 311, 577, 321, 1261, 264, 13985, 666, 3547, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20033304242120273, "compression_ratio": 1.3966480446927374, "no_speech_prob": 1.4367464018505416e-06}, {"id": 121, "seek": 60422, "start": 604.22, "end": 611.98, "text": " They get replaced with a unique basically arbitrary index. It's actually based on the alphabetical order of the", "tokens": [814, 483, 10772, 365, 257, 3845, 1936, 23211, 8186, 13, 467, 311, 767, 2361, 322, 264, 23339, 804, 1668, 295, 264], "temperature": 0.0, "avg_logprob": -0.17649454093841185, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.1189339375050622e-06}, {"id": 122, "seek": 60422, "start": 612.38, "end": 613.74, "text": " feature names", "tokens": [4111, 5288], "temperature": 0.0, "avg_logprob": -0.17649454093841185, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.1189339375050622e-06}, {"id": 123, "seek": 60422, "start": 613.74, "end": 616.74, "text": " the other thing Proc DF did remember was", "tokens": [264, 661, 551, 1705, 66, 48336, 630, 1604, 390], "temperature": 0.0, "avg_logprob": -0.17649454093841185, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.1189339375050622e-06}, {"id": 124, "seek": 60422, "start": 618.3000000000001, "end": 625.4200000000001, "text": " Continuous columns that had missing values the missing got replaced with the median and we added an additional column called", "tokens": [14674, 12549, 13766, 300, 632, 5361, 4190, 264, 5361, 658, 10772, 365, 264, 26779, 293, 321, 3869, 364, 4497, 7738, 1219], "temperature": 0.0, "avg_logprob": -0.17649454093841185, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.1189339375050622e-06}, {"id": 125, "seek": 60422, "start": 625.7, "end": 631.78, "text": " Column name underscore NA which is a Boolean column told you if that particular item was", "tokens": [4004, 16449, 1315, 37556, 16585, 597, 307, 257, 23351, 28499, 7738, 1907, 291, 498, 300, 1729, 3174, 390], "temperature": 0.0, "avg_logprob": -0.17649454093841185, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.1189339375050622e-06}, {"id": 126, "seek": 63178, "start": 631.78, "end": 633.78, "text": " missing or not", "tokens": [5361, 420, 406], "temperature": 0.0, "avg_logprob": -0.3005243634420728, "compression_ratio": 1.4820143884892085, "no_speech_prob": 1.8738656990535674e-06}, {"id": 127, "seek": 63178, "start": 634.5, "end": 635.8199999999999, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.3005243634420728, "compression_ratio": 1.4820143884892085, "no_speech_prob": 1.8738656990535674e-06}, {"id": 128, "seek": 63178, "start": 635.8199999999999, "end": 638.22, "text": " once we did that we were able to", "tokens": [1564, 321, 630, 300, 321, 645, 1075, 281], "temperature": 0.0, "avg_logprob": -0.3005243634420728, "compression_ratio": 1.4820143884892085, "no_speech_prob": 1.8738656990535674e-06}, {"id": 129, "seek": 63178, "start": 639.18, "end": 644.66, "text": " call random forest regressor dot fit and get the dot score and", "tokens": [818, 4974, 6719, 1121, 735, 284, 5893, 3318, 293, 483, 264, 5893, 6175, 293], "temperature": 0.0, "avg_logprob": -0.3005243634420728, "compression_ratio": 1.4820143884892085, "no_speech_prob": 1.8738656990535674e-06}, {"id": 130, "seek": 63178, "start": 645.18, "end": 647.18, "text": " turns out we have an R squared of", "tokens": [4523, 484, 321, 362, 364, 497, 8889, 295], "temperature": 0.0, "avg_logprob": -0.3005243634420728, "compression_ratio": 1.4820143884892085, "no_speech_prob": 1.8738656990535674e-06}, {"id": 131, "seek": 63178, "start": 648.26, "end": 650.26, "text": " point nine eight", "tokens": [935, 4949, 3180], "temperature": 0.0, "avg_logprob": -0.3005243634420728, "compression_ratio": 1.4820143884892085, "no_speech_prob": 1.8738656990535674e-06}, {"id": 132, "seek": 63178, "start": 650.4599999999999, "end": 654.42, "text": " Can anybody tell me what an R squared is?", "tokens": [1664, 4472, 980, 385, 437, 364, 497, 8889, 307, 30], "temperature": 0.0, "avg_logprob": -0.3005243634420728, "compression_ratio": 1.4820143884892085, "no_speech_prob": 1.8738656990535674e-06}, {"id": 133, "seek": 65442, "start": 654.42, "end": 657.54, "text": " So, sir you enjoy", "tokens": [407, 11, 4735, 291, 2103], "temperature": 0.0, "avg_logprob": -0.8980354102882179, "compression_ratio": 1.2376237623762376, "no_speech_prob": 2.584784488135483e-05}, {"id": 134, "seek": 65442, "start": 668.9799999999999, "end": 674.8199999999999, "text": " So R squared essentially it shows how much variance is explained by the model", "tokens": [407, 497, 8889, 4476, 309, 3110, 577, 709, 21977, 307, 8825, 538, 264, 2316], "temperature": 0.0, "avg_logprob": -0.8980354102882179, "compression_ratio": 1.2376237623762376, "no_speech_prob": 2.584784488135483e-05}, {"id": 135, "seek": 65442, "start": 675.54, "end": 677.54, "text": " this is the", "tokens": [341, 307, 264], "temperature": 0.0, "avg_logprob": -0.8980354102882179, "compression_ratio": 1.2376237623762376, "no_speech_prob": 2.584784488135483e-05}, {"id": 136, "seek": 65442, "start": 679.62, "end": 681.62, "text": " Yeah, this is the", "tokens": [865, 11, 341, 307, 264], "temperature": 0.0, "avg_logprob": -0.8980354102882179, "compression_ratio": 1.2376237623762376, "no_speech_prob": 2.584784488135483e-05}, {"id": 137, "seek": 68162, "start": 681.62, "end": 688.86, "text": " Yeah, this is the relation of this is SSR which is like", "tokens": [865, 11, 341, 307, 264, 9721, 295, 341, 307, 12238, 49, 597, 307, 411], "temperature": 0.0, "avg_logprob": -0.28726844787597655, "compression_ratio": 1.553191489361702, "no_speech_prob": 6.962151928746607e-06}, {"id": 138, "seek": 68162, "start": 692.38, "end": 694.98, "text": " Trying to trying to remember the exact formal but", "tokens": [20180, 281, 1382, 281, 1604, 264, 1900, 9860, 457], "temperature": 0.0, "avg_logprob": -0.28726844787597655, "compression_ratio": 1.553191489361702, "no_speech_prob": 6.962151928746607e-06}, {"id": 139, "seek": 68162, "start": 695.78, "end": 697.78, "text": " I've been roughly intuitively", "tokens": [286, 600, 668, 9810, 46506], "temperature": 0.0, "avg_logprob": -0.28726844787597655, "compression_ratio": 1.553191489361702, "no_speech_prob": 6.962151928746607e-06}, {"id": 140, "seek": 68162, "start": 697.94, "end": 706.02, "text": " Yeah, intuitively it's how much the model explains the how much it accounts for the variance in the data. Okay, good. So", "tokens": [865, 11, 46506, 309, 311, 577, 709, 264, 2316, 13948, 264, 577, 709, 309, 9402, 337, 264, 21977, 294, 264, 1412, 13, 1033, 11, 665, 13, 407], "temperature": 0.0, "avg_logprob": -0.28726844787597655, "compression_ratio": 1.553191489361702, "no_speech_prob": 6.962151928746607e-06}, {"id": 141, "seek": 68162, "start": 706.82, "end": 709.3, "text": " Let's talk about the formula and so", "tokens": [961, 311, 751, 466, 264, 8513, 293, 370], "temperature": 0.0, "avg_logprob": -0.28726844787597655, "compression_ratio": 1.553191489361702, "no_speech_prob": 6.962151928746607e-06}, {"id": 142, "seek": 70930, "start": 709.3, "end": 710.8599999999999, "text": " with", "tokens": [365], "temperature": 0.0, "avg_logprob": -0.18573010535467238, "compression_ratio": 1.6827956989247312, "no_speech_prob": 7.93447497926536e-07}, {"id": 143, "seek": 70930, "start": 710.8599999999999, "end": 718.6999999999999, "text": " Formulas the idea is not to learn the formula and remember it but to learn what the formula does and understand it, right?", "tokens": [10126, 16968, 264, 1558, 307, 406, 281, 1466, 264, 8513, 293, 1604, 309, 457, 281, 1466, 437, 264, 8513, 775, 293, 1223, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18573010535467238, "compression_ratio": 1.6827956989247312, "no_speech_prob": 7.93447497926536e-07}, {"id": 144, "seek": 70930, "start": 719.3399999999999, "end": 720.66, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.18573010535467238, "compression_ratio": 1.6827956989247312, "no_speech_prob": 7.93447497926536e-07}, {"id": 145, "seek": 70930, "start": 720.66, "end": 722.66, "text": " Here's the formula", "tokens": [1692, 311, 264, 8513], "temperature": 0.0, "avg_logprob": -0.18573010535467238, "compression_ratio": 1.6827956989247312, "no_speech_prob": 7.93447497926536e-07}, {"id": 146, "seek": 70930, "start": 722.78, "end": 724.78, "text": " It's one minus", "tokens": [467, 311, 472, 3175], "temperature": 0.0, "avg_logprob": -0.18573010535467238, "compression_ratio": 1.6827956989247312, "no_speech_prob": 7.93447497926536e-07}, {"id": 147, "seek": 70930, "start": 725.38, "end": 727.38, "text": " Something divided by something else", "tokens": [6595, 6666, 538, 746, 1646], "temperature": 0.0, "avg_logprob": -0.18573010535467238, "compression_ratio": 1.6827956989247312, "no_speech_prob": 7.93447497926536e-07}, {"id": 148, "seek": 70930, "start": 727.8599999999999, "end": 730.14, "text": " So what's the something else on the bottom?", "tokens": [407, 437, 311, 264, 746, 1646, 322, 264, 2767, 30], "temperature": 0.0, "avg_logprob": -0.18573010535467238, "compression_ratio": 1.6827956989247312, "no_speech_prob": 7.93447497926536e-07}, {"id": 149, "seek": 73014, "start": 730.14, "end": 738.9, "text": " Ss. Tot okay. So what this is saying is we've got some actual data some", "tokens": [318, 82, 13, 11236, 1392, 13, 407, 437, 341, 307, 1566, 307, 321, 600, 658, 512, 3539, 1412, 512], "temperature": 0.0, "avg_logprob": -0.3222737805596713, "compression_ratio": 1.5299145299145298, "no_speech_prob": 4.425462975632399e-06}, {"id": 150, "seek": 73014, "start": 740.6999999999999, "end": 743.1, "text": " Why eyes right we've got some actual data", "tokens": [1545, 2575, 558, 321, 600, 658, 512, 3539, 1412], "temperature": 0.0, "avg_logprob": -0.3222737805596713, "compression_ratio": 1.5299145299145298, "no_speech_prob": 4.425462975632399e-06}, {"id": 151, "seek": 73014, "start": 744.34, "end": 746.34, "text": " Three two four one", "tokens": [6244, 732, 1451, 472], "temperature": 0.0, "avg_logprob": -0.3222737805596713, "compression_ratio": 1.5299145299145298, "no_speech_prob": 4.425462975632399e-06}, {"id": 152, "seek": 73014, "start": 746.98, "end": 749.42, "text": " Okay, and then we've got some", "tokens": [1033, 11, 293, 550, 321, 600, 658, 512], "temperature": 0.0, "avg_logprob": -0.3222737805596713, "compression_ratio": 1.5299145299145298, "no_speech_prob": 4.425462975632399e-06}, {"id": 153, "seek": 73014, "start": 750.22, "end": 752.22, "text": " average", "tokens": [4274], "temperature": 0.0, "avg_logprob": -0.3222737805596713, "compression_ratio": 1.5299145299145298, "no_speech_prob": 4.425462975632399e-06}, {"id": 154, "seek": 73014, "start": 754.3, "end": 755.86, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.3222737805596713, "compression_ratio": 1.5299145299145298, "no_speech_prob": 4.425462975632399e-06}, {"id": 155, "seek": 75586, "start": 755.86, "end": 762.34, "text": " Our top bit this ss. Tot is the sum of each of these", "tokens": [2621, 1192, 857, 341, 262, 82, 13, 11236, 307, 264, 2408, 295, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.2400947298322405, "compression_ratio": 1.5748792270531402, "no_speech_prob": 1.6280442878269241e-06}, {"id": 156, "seek": 75586, "start": 763.9, "end": 765.9, "text": " Minus that", "tokens": [2829, 301, 300], "temperature": 0.0, "avg_logprob": -0.2400947298322405, "compression_ratio": 1.5748792270531402, "no_speech_prob": 1.6280442878269241e-06}, {"id": 157, "seek": 75586, "start": 767.58, "end": 774.26, "text": " So in other words, it's telling us how much does this data vary perhaps more interestingly is", "tokens": [407, 294, 661, 2283, 11, 309, 311, 3585, 505, 577, 709, 775, 341, 1412, 10559, 4317, 544, 25873, 307], "temperature": 0.0, "avg_logprob": -0.2400947298322405, "compression_ratio": 1.5748792270531402, "no_speech_prob": 1.6280442878269241e-06}, {"id": 158, "seek": 75586, "start": 775.02, "end": 778.74, "text": " Remember when we talked about like last week, what's the simplest?", "tokens": [5459, 562, 321, 2825, 466, 411, 1036, 1243, 11, 437, 311, 264, 22811, 30], "temperature": 0.0, "avg_logprob": -0.2400947298322405, "compression_ratio": 1.5748792270531402, "no_speech_prob": 1.6280442878269241e-06}, {"id": 159, "seek": 75586, "start": 779.5, "end": 785.0, "text": " Non stupid model you could come up with and I think the simplest non stupid model we came up with was", "tokens": [8774, 6631, 2316, 291, 727, 808, 493, 365, 293, 286, 519, 264, 22811, 2107, 6631, 2316, 321, 1361, 493, 365, 390], "temperature": 0.0, "avg_logprob": -0.2400947298322405, "compression_ratio": 1.5748792270531402, "no_speech_prob": 1.6280442878269241e-06}, {"id": 160, "seek": 78500, "start": 785.0, "end": 790.18, "text": " Create a column of the mean just copy the mean a bunch of times and submit that to Kaggle", "tokens": [20248, 257, 7738, 295, 264, 914, 445, 5055, 264, 914, 257, 3840, 295, 1413, 293, 10315, 300, 281, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.28493627634915436, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.9333504042151617e-06}, {"id": 161, "seek": 78500, "start": 791.04, "end": 794.76, "text": " If you did that then your root mean squared error", "tokens": [759, 291, 630, 300, 550, 428, 5593, 914, 8889, 6713], "temperature": 0.0, "avg_logprob": -0.28493627634915436, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.9333504042151617e-06}, {"id": 162, "seek": 78500, "start": 795.52, "end": 797.16, "text": " would be", "tokens": [576, 312], "temperature": 0.0, "avg_logprob": -0.28493627634915436, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.9333504042151617e-06}, {"id": 163, "seek": 78500, "start": 797.16, "end": 798.44, "text": " this", "tokens": [341], "temperature": 0.0, "avg_logprob": -0.28493627634915436, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.9333504042151617e-06}, {"id": 164, "seek": 78500, "start": 798.44, "end": 803.5, "text": " So this is the root mean squared error of the most naive", "tokens": [407, 341, 307, 264, 5593, 914, 8889, 6713, 295, 264, 881, 29052], "temperature": 0.0, "avg_logprob": -0.28493627634915436, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.9333504042151617e-06}, {"id": 165, "seek": 78500, "start": 804.4, "end": 805.52, "text": " non-stupid", "tokens": [2107, 12, 372, 6127], "temperature": 0.0, "avg_logprob": -0.28493627634915436, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.9333504042151617e-06}, {"id": 166, "seek": 78500, "start": 805.52, "end": 808.04, "text": " Model where the model is just predict the mean", "tokens": [17105, 689, 264, 2316, 307, 445, 6069, 264, 914], "temperature": 0.0, "avg_logprob": -0.28493627634915436, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.9333504042151617e-06}, {"id": 167, "seek": 78500, "start": 809.44, "end": 811.44, "text": " on the top", "tokens": [322, 264, 1192], "temperature": 0.0, "avg_logprob": -0.28493627634915436, "compression_ratio": 1.673913043478261, "no_speech_prob": 1.9333504042151617e-06}, {"id": 168, "seek": 81144, "start": 811.44, "end": 814.8800000000001, "text": " We have SS res which is here", "tokens": [492, 362, 12238, 725, 597, 307, 510], "temperature": 0.0, "avg_logprob": -0.22606287504497327, "compression_ratio": 1.4692307692307693, "no_speech_prob": 1.1544561857590452e-06}, {"id": 169, "seek": 81144, "start": 816.2, "end": 819.8800000000001, "text": " Which is that we're now going to add a column of predictions", "tokens": [3013, 307, 300, 321, 434, 586, 516, 281, 909, 257, 7738, 295, 21264], "temperature": 0.0, "avg_logprob": -0.22606287504497327, "compression_ratio": 1.4692307692307693, "no_speech_prob": 1.1544561857590452e-06}, {"id": 170, "seek": 81144, "start": 828.72, "end": 836.6400000000001, "text": " Okay, and so now what we do is rather than taking the yi minus y mean we're going to take yi", "tokens": [1033, 11, 293, 370, 586, 437, 321, 360, 307, 2831, 813, 1940, 264, 288, 72, 3175, 288, 914, 321, 434, 516, 281, 747, 288, 72], "temperature": 0.0, "avg_logprob": -0.22606287504497327, "compression_ratio": 1.4692307692307693, "no_speech_prob": 1.1544561857590452e-06}, {"id": 171, "seek": 81144, "start": 837.7600000000001, "end": 839.7600000000001, "text": " minus fi", "tokens": [3175, 15848], "temperature": 0.0, "avg_logprob": -0.22606287504497327, "compression_ratio": 1.4692307692307693, "no_speech_prob": 1.1544561857590452e-06}, {"id": 172, "seek": 83976, "start": 839.76, "end": 846.4399999999999, "text": " Right and so now instead of saying what's the root mean squared error of our naive model?", "tokens": [1779, 293, 370, 586, 2602, 295, 1566, 437, 311, 264, 5593, 914, 8889, 6713, 295, 527, 29052, 2316, 30], "temperature": 0.0, "avg_logprob": -0.18050641511615953, "compression_ratio": 1.8208955223880596, "no_speech_prob": 1.8162157857659622e-06}, {"id": 173, "seek": 83976, "start": 846.4399999999999, "end": 850.04, "text": " we're saying what's the root mean squared error of the actual model that we're interested in and", "tokens": [321, 434, 1566, 437, 311, 264, 5593, 914, 8889, 6713, 295, 264, 3539, 2316, 300, 321, 434, 3102, 294, 293], "temperature": 0.0, "avg_logprob": -0.18050641511615953, "compression_ratio": 1.8208955223880596, "no_speech_prob": 1.8162157857659622e-06}, {"id": 174, "seek": 83976, "start": 850.96, "end": 852.28, "text": " then", "tokens": [550], "temperature": 0.0, "avg_logprob": -0.18050641511615953, "compression_ratio": 1.8208955223880596, "no_speech_prob": 1.8162157857659622e-06}, {"id": 175, "seek": 83976, "start": 852.28, "end": 854.28, "text": " We take the ratio", "tokens": [492, 747, 264, 8509], "temperature": 0.0, "avg_logprob": -0.18050641511615953, "compression_ratio": 1.8208955223880596, "no_speech_prob": 1.8162157857659622e-06}, {"id": 176, "seek": 83976, "start": 855.24, "end": 857.24, "text": " So in other words", "tokens": [407, 294, 661, 2283], "temperature": 0.0, "avg_logprob": -0.18050641511615953, "compression_ratio": 1.8208955223880596, "no_speech_prob": 1.8162157857659622e-06}, {"id": 177, "seek": 83976, "start": 857.4399999999999, "end": 858.64, "text": " if", "tokens": [498], "temperature": 0.0, "avg_logprob": -0.18050641511615953, "compression_ratio": 1.8208955223880596, "no_speech_prob": 1.8162157857659622e-06}, {"id": 178, "seek": 83976, "start": 858.64, "end": 860.4, "text": " we", "tokens": [321], "temperature": 0.0, "avg_logprob": -0.18050641511615953, "compression_ratio": 1.8208955223880596, "no_speech_prob": 1.8162157857659622e-06}, {"id": 179, "seek": 83976, "start": 860.4, "end": 862.4, "text": " actually were exactly as", "tokens": [767, 645, 2293, 382], "temperature": 0.0, "avg_logprob": -0.18050641511615953, "compression_ratio": 1.8208955223880596, "no_speech_prob": 1.8162157857659622e-06}, {"id": 180, "seek": 86240, "start": 862.4, "end": 869.48, "text": " Effective as just predicting the mean then this would be top and bottom would be the same that would be one", "tokens": [17764, 488, 382, 445, 32884, 264, 914, 550, 341, 576, 312, 1192, 293, 2767, 576, 312, 264, 912, 300, 576, 312, 472], "temperature": 0.0, "avg_logprob": -0.2219376382948477, "compression_ratio": 1.6758241758241759, "no_speech_prob": 5.989257942928816e-07}, {"id": 181, "seek": 86240, "start": 869.72, "end": 871.72, "text": " one minus one would be zero", "tokens": [472, 3175, 472, 576, 312, 4018], "temperature": 0.0, "avg_logprob": -0.2219376382948477, "compression_ratio": 1.6758241758241759, "no_speech_prob": 5.989257942928816e-07}, {"id": 182, "seek": 86240, "start": 872.68, "end": 880.76, "text": " If we were perfect so fi minus yi was always zero then that's zero divided by something one minus that is", "tokens": [759, 321, 645, 2176, 370, 15848, 3175, 288, 72, 390, 1009, 4018, 550, 300, 311, 4018, 6666, 538, 746, 472, 3175, 300, 307], "temperature": 0.0, "avg_logprob": -0.2219376382948477, "compression_ratio": 1.6758241758241759, "no_speech_prob": 5.989257942928816e-07}, {"id": 183, "seek": 86240, "start": 881.0799999999999, "end": 882.92, "text": " one", "tokens": [472], "temperature": 0.0, "avg_logprob": -0.2219376382948477, "compression_ratio": 1.6758241758241759, "no_speech_prob": 5.989257942928816e-07}, {"id": 184, "seek": 86240, "start": 882.92, "end": 884.92, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.2219376382948477, "compression_ratio": 1.6758241758241759, "no_speech_prob": 5.989257942928816e-07}, {"id": 185, "seek": 88492, "start": 884.92, "end": 891.5999999999999, "text": " What is the possible range of values of r squared?", "tokens": [708, 307, 264, 1944, 3613, 295, 4190, 295, 367, 8889, 30], "temperature": 0.0, "avg_logprob": -0.2678491463095455, "compression_ratio": 1.4146341463414633, "no_speech_prob": 2.368772584304679e-06}, {"id": 186, "seek": 88492, "start": 893.24, "end": 900.12, "text": " Okay, I heard a lot of zero to one does anybody want to give me an alternative negative one to one", "tokens": [1033, 11, 286, 2198, 257, 688, 295, 4018, 281, 472, 775, 4472, 528, 281, 976, 385, 364, 8535, 3671, 472, 281, 472], "temperature": 0.0, "avg_logprob": -0.2678491463095455, "compression_ratio": 1.4146341463414633, "no_speech_prob": 2.368772584304679e-06}, {"id": 187, "seek": 90012, "start": 900.12, "end": 909.36, "text": " Anything less than one there's the right answer. Let's find out why we hit the box", "tokens": [11998, 1570, 813, 472, 456, 311, 264, 558, 1867, 13, 961, 311, 915, 484, 983, 321, 2045, 264, 2424], "temperature": 0.0, "avg_logprob": -0.2724221977027687, "compression_ratio": 1.233009708737864, "no_speech_prob": 2.0462059183046222e-05}, {"id": 188, "seek": 90012, "start": 923.48, "end": 925.92, "text": " Okay, so why is it any number less than one?", "tokens": [1033, 11, 370, 983, 307, 309, 604, 1230, 1570, 813, 472, 30], "temperature": 0.0, "avg_logprob": -0.2724221977027687, "compression_ratio": 1.233009708737864, "no_speech_prob": 2.0462059183046222e-05}, {"id": 189, "seek": 92592, "start": 925.92, "end": 930.68, "text": " We should make a model basically as crap as you want and just like as like big arrows as you want", "tokens": [492, 820, 652, 257, 2316, 1936, 382, 12426, 382, 291, 528, 293, 445, 411, 382, 411, 955, 19669, 382, 291, 528], "temperature": 0.0, "avg_logprob": -0.249119056845611, "compression_ratio": 1.7100371747211895, "no_speech_prob": 5.7718339121493045e-06}, {"id": 190, "seek": 92592, "start": 930.68, "end": 934.3399999999999, "text": " And you're just subtracting from one in the formula exactly so", "tokens": [400, 291, 434, 445, 16390, 278, 490, 472, 294, 264, 8513, 2293, 370], "temperature": 0.0, "avg_logprob": -0.249119056845611, "compression_ratio": 1.7100371747211895, "no_speech_prob": 5.7718339121493045e-06}, {"id": 191, "seek": 92592, "start": 934.9599999999999, "end": 936.9599999999999, "text": " interestingly I", "tokens": [25873, 286], "temperature": 0.0, "avg_logprob": -0.249119056845611, "compression_ratio": 1.7100371747211895, "no_speech_prob": 5.7718339121493045e-06}, {"id": 192, "seek": 92592, "start": 937.04, "end": 938.7199999999999, "text": " was talking to", "tokens": [390, 1417, 281], "temperature": 0.0, "avg_logprob": -0.249119056845611, "compression_ratio": 1.7100371747211895, "no_speech_prob": 5.7718339121493045e-06}, {"id": 193, "seek": 92592, "start": 938.7199999999999, "end": 943.28, "text": " Our computer science professor Terrence this morning who was talking to a statistics professor", "tokens": [2621, 3820, 3497, 8304, 6564, 10760, 341, 2446, 567, 390, 1417, 281, 257, 12523, 8304], "temperature": 0.0, "avg_logprob": -0.249119056845611, "compression_ratio": 1.7100371747211895, "no_speech_prob": 5.7718339121493045e-06}, {"id": 194, "seek": 92592, "start": 943.88, "end": 947.76, "text": " Told him that the possible range of values was r squared was zero to one", "tokens": [48220, 796, 300, 264, 1944, 3613, 295, 4190, 390, 367, 8889, 390, 4018, 281, 472], "temperature": 0.0, "avg_logprob": -0.249119056845611, "compression_ratio": 1.7100371747211895, "no_speech_prob": 5.7718339121493045e-06}, {"id": 195, "seek": 92592, "start": 947.76, "end": 954.36, "text": " I said that is totally not true if you predict infinity for every column. That's right for every row", "tokens": [286, 848, 300, 307, 3879, 406, 2074, 498, 291, 6069, 13202, 337, 633, 7738, 13, 663, 311, 558, 337, 633, 5386], "temperature": 0.0, "avg_logprob": -0.249119056845611, "compression_ratio": 1.7100371747211895, "no_speech_prob": 5.7718339121493045e-06}, {"id": 196, "seek": 95436, "start": 954.36, "end": 959.6800000000001, "text": " Then you're going to have infinity for every residual and so you're going to have one minus infinity", "tokens": [1396, 291, 434, 516, 281, 362, 13202, 337, 633, 27980, 293, 370, 291, 434, 516, 281, 362, 472, 3175, 13202], "temperature": 0.0, "avg_logprob": -0.19528383421666415, "compression_ratio": 1.7637130801687764, "no_speech_prob": 3.3931278267118614e-06}, {"id": 197, "seek": 95436, "start": 960.4, "end": 962.5600000000001, "text": " Okay, so the possible range of values is", "tokens": [1033, 11, 370, 264, 1944, 3613, 295, 4190, 307], "temperature": 0.0, "avg_logprob": -0.19528383421666415, "compression_ratio": 1.7637130801687764, "no_speech_prob": 3.3931278267118614e-06}, {"id": 198, "seek": 95436, "start": 963.8000000000001, "end": 971.4, "text": " Less than one that's all we know and this will happen you will get negative values sometimes in your r squared and when that happens", "tokens": [18649, 813, 472, 300, 311, 439, 321, 458, 293, 341, 486, 1051, 291, 486, 483, 3671, 4190, 2171, 294, 428, 367, 8889, 293, 562, 300, 2314], "temperature": 0.0, "avg_logprob": -0.19528383421666415, "compression_ratio": 1.7637130801687764, "no_speech_prob": 3.3931278267118614e-06}, {"id": 199, "seek": 95436, "start": 971.92, "end": 977.6, "text": " It's not a mistake. It's not a it's not like a bug it means your model is worse than predicting the mean", "tokens": [467, 311, 406, 257, 6146, 13, 467, 311, 406, 257, 309, 311, 406, 411, 257, 7426, 309, 1355, 428, 2316, 307, 5324, 813, 32884, 264, 914], "temperature": 0.0, "avg_logprob": -0.19528383421666415, "compression_ratio": 1.7637130801687764, "no_speech_prob": 3.3931278267118614e-06}, {"id": 200, "seek": 95436, "start": 978.64, "end": 981.2, "text": " Okay, which is suggest. It's not great", "tokens": [1033, 11, 597, 307, 3402, 13, 467, 311, 406, 869], "temperature": 0.0, "avg_logprob": -0.19528383421666415, "compression_ratio": 1.7637130801687764, "no_speech_prob": 3.3931278267118614e-06}, {"id": 201, "seek": 98120, "start": 981.2, "end": 983.2, "text": " So that's r squared", "tokens": [407, 300, 311, 367, 8889], "temperature": 0.0, "avg_logprob": -0.2689546012878418, "compression_ratio": 1.7311320754716981, "no_speech_prob": 8.059424203565868e-07}, {"id": 202, "seek": 98120, "start": 985.6400000000001, "end": 987.6400000000001, "text": " It's not", "tokens": [467, 311, 406], "temperature": 0.0, "avg_logprob": -0.2689546012878418, "compression_ratio": 1.7311320754716981, "no_speech_prob": 8.059424203565868e-07}, {"id": 203, "seek": 98120, "start": 988.72, "end": 990.72, "text": " It's not necessarily", "tokens": [467, 311, 406, 4725], "temperature": 0.0, "avg_logprob": -0.2689546012878418, "compression_ratio": 1.7311320754716981, "no_speech_prob": 8.059424203565868e-07}, {"id": 204, "seek": 98120, "start": 991.88, "end": 993.88, "text": " What you're actually trying to optimize", "tokens": [708, 291, 434, 767, 1382, 281, 19719], "temperature": 0.0, "avg_logprob": -0.2689546012878418, "compression_ratio": 1.7311320754716981, "no_speech_prob": 8.059424203565868e-07}, {"id": 205, "seek": 98120, "start": 994.2, "end": 1001.44, "text": " Right, but it's it's it's the nice thing about it is that it's a number that you can use kind of for every model", "tokens": [1779, 11, 457, 309, 311, 309, 311, 309, 311, 264, 1481, 551, 466, 309, 307, 300, 309, 311, 257, 1230, 300, 291, 393, 764, 733, 295, 337, 633, 2316], "temperature": 0.0, "avg_logprob": -0.2689546012878418, "compression_ratio": 1.7311320754716981, "no_speech_prob": 8.059424203565868e-07}, {"id": 206, "seek": 98120, "start": 1002.44, "end": 1007.6, "text": " And so you can kind of start try to get a feel of like what does point eight look like what is point nine look?", "tokens": [400, 370, 291, 393, 733, 295, 722, 853, 281, 483, 257, 841, 295, 411, 437, 775, 935, 3180, 574, 411, 437, 307, 935, 4949, 574, 30], "temperature": 0.0, "avg_logprob": -0.2689546012878418, "compression_ratio": 1.7311320754716981, "no_speech_prob": 8.059424203565868e-07}, {"id": 207, "seek": 98120, "start": 1007.6, "end": 1009.96, "text": " like so like something I find interesting is to like", "tokens": [411, 370, 411, 746, 286, 915, 1880, 307, 281, 411], "temperature": 0.0, "avg_logprob": -0.2689546012878418, "compression_ratio": 1.7311320754716981, "no_speech_prob": 8.059424203565868e-07}, {"id": 208, "seek": 100996, "start": 1009.96, "end": 1011.96, "text": " Create some different", "tokens": [20248, 512, 819], "temperature": 0.0, "avg_logprob": -0.2699326185079721, "compression_ratio": 1.7137096774193548, "no_speech_prob": 2.6425755095260683e-06}, {"id": 209, "seek": 100996, "start": 1012.88, "end": 1020.32, "text": " synthetic data sets just to two dimensions with kind of different amounts of random noise and like see what they look like on a scatterplot", "tokens": [23420, 1412, 6352, 445, 281, 732, 12819, 365, 733, 295, 819, 11663, 295, 4974, 5658, 293, 411, 536, 437, 436, 574, 411, 322, 257, 34951, 564, 310], "temperature": 0.0, "avg_logprob": -0.2699326185079721, "compression_ratio": 1.7137096774193548, "no_speech_prob": 2.6425755095260683e-06}, {"id": 210, "seek": 100996, "start": 1020.32, "end": 1025.48, "text": " And see what they are squared are just to kind of get a feel for like what does an ask where you know?", "tokens": [400, 536, 437, 436, 366, 8889, 366, 445, 281, 733, 295, 483, 257, 841, 337, 411, 437, 775, 364, 1029, 689, 291, 458, 30], "temperature": 0.0, "avg_logprob": -0.2699326185079721, "compression_ratio": 1.7137096774193548, "no_speech_prob": 2.6425755095260683e-06}, {"id": 211, "seek": 100996, "start": 1025.48, "end": 1029.52, "text": " Is it an aspect of point nine close or not about point seven close or not?", "tokens": [1119, 309, 364, 4171, 295, 935, 4949, 1998, 420, 406, 466, 935, 3407, 1998, 420, 406, 30], "temperature": 0.0, "avg_logprob": -0.2699326185079721, "compression_ratio": 1.7137096774193548, "no_speech_prob": 2.6425755095260683e-06}, {"id": 212, "seek": 100996, "start": 1032.24, "end": 1033.52, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2699326185079721, "compression_ratio": 1.7137096774193548, "no_speech_prob": 2.6425755095260683e-06}, {"id": 213, "seek": 100996, "start": 1033.52, "end": 1038.4, "text": " So I think r squared is a useful number to have a familiarity with and you don't", "tokens": [407, 286, 519, 367, 8889, 307, 257, 4420, 1230, 281, 362, 257, 49828, 365, 293, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.2699326185079721, "compression_ratio": 1.7137096774193548, "no_speech_prob": 2.6425755095260683e-06}, {"id": 214, "seek": 103840, "start": 1038.4, "end": 1044.96, "text": " Need to remember the formula if you remember the meaning which is what's the ratio between?", "tokens": [16984, 281, 1604, 264, 8513, 498, 291, 1604, 264, 3620, 597, 307, 437, 311, 264, 8509, 1296, 30], "temperature": 0.0, "avg_logprob": -0.27134077889578684, "compression_ratio": 1.7264150943396226, "no_speech_prob": 4.565932158584474e-06}, {"id": 215, "seek": 103840, "start": 1045.0, "end": 1051.16, "text": " How good your model is it means good error versus how good is the naive mean model for its good error?", "tokens": [1012, 665, 428, 2316, 307, 309, 1355, 665, 6713, 5717, 577, 665, 307, 264, 29052, 914, 2316, 337, 1080, 665, 6713, 30], "temperature": 0.0, "avg_logprob": -0.27134077889578684, "compression_ratio": 1.7264150943396226, "no_speech_prob": 4.565932158584474e-06}, {"id": 216, "seek": 103840, "start": 1052.2, "end": 1056.0, "text": " Okay in our case point nine eight. It's saying it's a very good model", "tokens": [1033, 294, 527, 1389, 935, 4949, 3180, 13, 467, 311, 1566, 309, 311, 257, 588, 665, 2316], "temperature": 0.0, "avg_logprob": -0.27134077889578684, "compression_ratio": 1.7264150943396226, "no_speech_prob": 4.565932158584474e-06}, {"id": 217, "seek": 103840, "start": 1057.6000000000001, "end": 1060.92, "text": " However, it might be a very good model because it looks like this", "tokens": [2908, 11, 309, 1062, 312, 257, 588, 665, 2316, 570, 309, 1542, 411, 341], "temperature": 0.0, "avg_logprob": -0.27134077889578684, "compression_ratio": 1.7264150943396226, "no_speech_prob": 4.565932158584474e-06}, {"id": 218, "seek": 103840, "start": 1062.0400000000002, "end": 1064.0400000000002, "text": " All right, and this would be called", "tokens": [1057, 558, 11, 293, 341, 576, 312, 1219], "temperature": 0.0, "avg_logprob": -0.27134077889578684, "compression_ratio": 1.7264150943396226, "no_speech_prob": 4.565932158584474e-06}, {"id": 219, "seek": 106404, "start": 1064.04, "end": 1070.08, "text": " Overfitting so we may well have created a model which is very good at running through the points that we gave it", "tokens": [4886, 69, 2414, 370, 321, 815, 731, 362, 2942, 257, 2316, 597, 307, 588, 665, 412, 2614, 807, 264, 2793, 300, 321, 2729, 309], "temperature": 0.0, "avg_logprob": -0.15503056844075522, "compression_ratio": 1.784037558685446, "no_speech_prob": 9.721456990519073e-07}, {"id": 220, "seek": 106404, "start": 1070.2, "end": 1072.36, "text": " But it's not going to be very good at running through", "tokens": [583, 309, 311, 406, 516, 281, 312, 588, 665, 412, 2614, 807], "temperature": 0.0, "avg_logprob": -0.15503056844075522, "compression_ratio": 1.784037558685446, "no_speech_prob": 9.721456990519073e-07}, {"id": 221, "seek": 106404, "start": 1072.8799999999999, "end": 1074.8, "text": " points that we didn't give it", "tokens": [2793, 300, 321, 994, 380, 976, 309], "temperature": 0.0, "avg_logprob": -0.15503056844075522, "compression_ratio": 1.784037558685446, "no_speech_prob": 9.721456990519073e-07}, {"id": 222, "seek": 106404, "start": 1074.8, "end": 1078.74, "text": " So that's why we always want to have a validation set", "tokens": [407, 300, 311, 983, 321, 1009, 528, 281, 362, 257, 24071, 992], "temperature": 0.0, "avg_logprob": -0.15503056844075522, "compression_ratio": 1.784037558685446, "no_speech_prob": 9.721456990519073e-07}, {"id": 223, "seek": 106404, "start": 1081.32, "end": 1085.5, "text": " Creating your validation set is the most important thing", "tokens": [40002, 428, 24071, 992, 307, 264, 881, 1021, 551], "temperature": 0.0, "avg_logprob": -0.15503056844075522, "compression_ratio": 1.784037558685446, "no_speech_prob": 9.721456990519073e-07}, {"id": 224, "seek": 106404, "start": 1086.36, "end": 1090.82, "text": " That I think you need to do when you're doing a machine learning project", "tokens": [663, 286, 519, 291, 643, 281, 360, 562, 291, 434, 884, 257, 3479, 2539, 1716], "temperature": 0.0, "avg_logprob": -0.15503056844075522, "compression_ratio": 1.784037558685446, "no_speech_prob": 9.721456990519073e-07}, {"id": 225, "seek": 109082, "start": 1090.82, "end": 1094.78, "text": " at least in terms of in the actual modeling", "tokens": [412, 1935, 294, 2115, 295, 294, 264, 3539, 15983], "temperature": 0.0, "avg_logprob": -0.16902266608344185, "compression_ratio": 1.6927083333333333, "no_speech_prob": 1.4823521041762433e-06}, {"id": 226, "seek": 109082, "start": 1098.34, "end": 1102.54, "text": " Because what you need to do is come up with a data set where", "tokens": [1436, 437, 291, 643, 281, 360, 307, 808, 493, 365, 257, 1412, 992, 689], "temperature": 0.0, "avg_logprob": -0.16902266608344185, "compression_ratio": 1.6927083333333333, "no_speech_prob": 1.4823521041762433e-06}, {"id": 227, "seek": 109082, "start": 1102.98, "end": 1109.32, "text": " The score of your model on that data set is going to be representative of how well your model is going to do", "tokens": [440, 6175, 295, 428, 2316, 322, 300, 1412, 992, 307, 516, 281, 312, 12424, 295, 577, 731, 428, 2316, 307, 516, 281, 360], "temperature": 0.0, "avg_logprob": -0.16902266608344185, "compression_ratio": 1.6927083333333333, "no_speech_prob": 1.4823521041762433e-06}, {"id": 228, "seek": 109082, "start": 1110.22, "end": 1116.58, "text": " In the real world like in Kaggle on the leaderboard or off Kaggle like when you actually use it in production I", "tokens": [682, 264, 957, 1002, 411, 294, 48751, 22631, 322, 264, 5263, 3787, 420, 766, 48751, 22631, 411, 562, 291, 767, 764, 309, 294, 4265, 286], "temperature": 0.0, "avg_logprob": -0.16902266608344185, "compression_ratio": 1.6927083333333333, "no_speech_prob": 1.4823521041762433e-06}, {"id": 229, "seek": 111658, "start": 1116.58, "end": 1124.22, "text": " I very very very often hear people in industry say", "tokens": [286, 588, 588, 588, 2049, 1568, 561, 294, 3518, 584], "temperature": 0.0, "avg_logprob": -0.250053824448004, "compression_ratio": 1.5622317596566524, "no_speech_prob": 1.2878872439614497e-06}, {"id": 230, "seek": 111658, "start": 1124.22, "end": 1131.1399999999999, "text": " I don't trust machine learning I tried modeling once it looked great. We put it in production. It didn't work", "tokens": [286, 500, 380, 3361, 3479, 2539, 286, 3031, 15983, 1564, 309, 2956, 869, 13, 492, 829, 309, 294, 4265, 13, 467, 994, 380, 589], "temperature": 0.0, "avg_logprob": -0.250053824448004, "compression_ratio": 1.5622317596566524, "no_speech_prob": 1.2878872439614497e-06}, {"id": 231, "seek": 111658, "start": 1132.3, "end": 1137.8999999999999, "text": " Now whose fault is that right that means their validation set was not representative", "tokens": [823, 6104, 7441, 307, 300, 558, 300, 1355, 641, 24071, 992, 390, 406, 12424], "temperature": 0.0, "avg_logprob": -0.250053824448004, "compression_ratio": 1.5622317596566524, "no_speech_prob": 1.2878872439614497e-06}, {"id": 232, "seek": 113790, "start": 1137.9, "end": 1146.94, "text": " All right, so here's a very simple thing which generally speaking Kaggle is pretty good about doing if your data has a time", "tokens": [1057, 558, 11, 370, 510, 311, 257, 588, 2199, 551, 597, 5101, 4124, 48751, 22631, 307, 1238, 665, 466, 884, 498, 428, 1412, 575, 257, 565], "temperature": 0.0, "avg_logprob": -0.14515688108361285, "compression_ratio": 1.6477732793522266, "no_speech_prob": 1.3496950259650475e-06}, {"id": 233, "seek": 113790, "start": 1147.3400000000001, "end": 1152.3400000000001, "text": " Piece in it right as happens in blue book for bulldozers in blue book for bulldozers", "tokens": [42868, 294, 309, 558, 382, 2314, 294, 3344, 1446, 337, 4693, 2595, 41698, 294, 3344, 1446, 337, 4693, 2595, 41698], "temperature": 0.0, "avg_logprob": -0.14515688108361285, "compression_ratio": 1.6477732793522266, "no_speech_prob": 1.3496950259650475e-06}, {"id": 234, "seek": 113790, "start": 1152.7, "end": 1158.5600000000002, "text": " We're talking about the sale price of a piece of industrial equipment on a particular date", "tokens": [492, 434, 1417, 466, 264, 8680, 3218, 295, 257, 2522, 295, 9987, 5927, 322, 257, 1729, 4002], "temperature": 0.0, "avg_logprob": -0.14515688108361285, "compression_ratio": 1.6477732793522266, "no_speech_prob": 1.3496950259650475e-06}, {"id": 235, "seek": 113790, "start": 1159.6200000000001, "end": 1166.5800000000002, "text": " So the startup doing this competition wanted to create a model that wouldn't predict last February's prices", "tokens": [407, 264, 18578, 884, 341, 6211, 1415, 281, 1884, 257, 2316, 300, 2759, 380, 6069, 1036, 8711, 311, 7901], "temperature": 0.0, "avg_logprob": -0.14515688108361285, "compression_ratio": 1.6477732793522266, "no_speech_prob": 1.3496950259650475e-06}, {"id": 236, "seek": 116658, "start": 1166.58, "end": 1171.6999999999998, "text": " But would predict next month's prices so what they did was they gave us data", "tokens": [583, 576, 6069, 958, 1618, 311, 7901, 370, 437, 436, 630, 390, 436, 2729, 505, 1412], "temperature": 0.0, "avg_logprob": -0.1706716280717116, "compression_ratio": 1.8458498023715415, "no_speech_prob": 8.851532697917719e-07}, {"id": 237, "seek": 116658, "start": 1172.22, "end": 1176.1799999999998, "text": " Representing a particular date range in the training set and then the test set", "tokens": [19945, 278, 257, 1729, 4002, 3613, 294, 264, 3097, 992, 293, 550, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.1706716280717116, "compression_ratio": 1.8458498023715415, "no_speech_prob": 8.851532697917719e-07}, {"id": 238, "seek": 116658, "start": 1176.6599999999999, "end": 1181.1599999999999, "text": " Represented a future set of dates that wasn't represented in the training set", "tokens": [3696, 495, 6003, 257, 2027, 992, 295, 11691, 300, 2067, 380, 10379, 294, 264, 3097, 992], "temperature": 0.0, "avg_logprob": -0.1706716280717116, "compression_ratio": 1.8458498023715415, "no_speech_prob": 8.851532697917719e-07}, {"id": 239, "seek": 116658, "start": 1181.5, "end": 1186.24, "text": " Right so that's pretty good right that means that if we're doing well on this model", "tokens": [1779, 370, 300, 311, 1238, 665, 558, 300, 1355, 300, 498, 321, 434, 884, 731, 322, 341, 2316], "temperature": 0.0, "avg_logprob": -0.1706716280717116, "compression_ratio": 1.8458498023715415, "no_speech_prob": 8.851532697917719e-07}, {"id": 240, "seek": 116658, "start": 1186.24, "end": 1191.62, "text": " We've built something which can actually predict the future or at least it could predict the future", "tokens": [492, 600, 3094, 746, 597, 393, 767, 6069, 264, 2027, 420, 412, 1935, 309, 727, 6069, 264, 2027], "temperature": 0.0, "avg_logprob": -0.1706716280717116, "compression_ratio": 1.8458498023715415, "no_speech_prob": 8.851532697917719e-07}, {"id": 241, "seek": 116658, "start": 1192.1, "end": 1194.46, "text": " Then assuming things haven't changed dramatically", "tokens": [1396, 11926, 721, 2378, 380, 3105, 17548], "temperature": 0.0, "avg_logprob": -0.1706716280717116, "compression_ratio": 1.8458498023715415, "no_speech_prob": 8.851532697917719e-07}, {"id": 242, "seek": 119446, "start": 1194.46, "end": 1200.9, "text": " So that's the test set we have so we need to create a validation set that has the same properties", "tokens": [407, 300, 311, 264, 1500, 992, 321, 362, 370, 321, 643, 281, 1884, 257, 24071, 992, 300, 575, 264, 912, 7221], "temperature": 0.0, "avg_logprob": -0.2051611980759954, "compression_ratio": 1.8553459119496856, "no_speech_prob": 6.475927989413321e-07}, {"id": 243, "seek": 119446, "start": 1201.82, "end": 1207.82, "text": " So the test set had 12,000 rows in so let's create a validation set that has 12,000 rows", "tokens": [407, 264, 1500, 992, 632, 2272, 11, 1360, 13241, 294, 370, 718, 311, 1884, 257, 24071, 992, 300, 575, 2272, 11, 1360, 13241], "temperature": 0.0, "avg_logprob": -0.2051611980759954, "compression_ratio": 1.8553459119496856, "no_speech_prob": 6.475927989413321e-07}, {"id": 244, "seek": 119446, "start": 1208.26, "end": 1211.9, "text": " Right and then let's split the data set into", "tokens": [1779, 293, 550, 718, 311, 7472, 264, 1412, 992, 666], "temperature": 0.0, "avg_logprob": -0.2051611980759954, "compression_ratio": 1.8553459119496856, "no_speech_prob": 6.475927989413321e-07}, {"id": 245, "seek": 119446, "start": 1212.66, "end": 1214.66, "text": " the first", "tokens": [264, 700], "temperature": 0.0, "avg_logprob": -0.2051611980759954, "compression_ratio": 1.8553459119496856, "no_speech_prob": 6.475927989413321e-07}, {"id": 246, "seek": 119446, "start": 1214.7, "end": 1216.7, "text": " N minus 12,000 rows", "tokens": [426, 3175, 2272, 11, 1360, 13241], "temperature": 0.0, "avg_logprob": -0.2051611980759954, "compression_ratio": 1.8553459119496856, "no_speech_prob": 6.475927989413321e-07}, {"id": 247, "seek": 119446, "start": 1218.26, "end": 1220.26, "text": " For the training set and the last", "tokens": [1171, 264, 3097, 992, 293, 264, 1036], "temperature": 0.0, "avg_logprob": -0.2051611980759954, "compression_ratio": 1.8553459119496856, "no_speech_prob": 6.475927989413321e-07}, {"id": 248, "seek": 122026, "start": 1220.26, "end": 1226.54, "text": " 12,000 rows for the validation set and so we've now got something which hopefully looks like", "tokens": [2272, 11, 1360, 13241, 337, 264, 24071, 992, 293, 370, 321, 600, 586, 658, 746, 597, 4696, 1542, 411], "temperature": 0.0, "avg_logprob": -0.16432179795934798, "compression_ratio": 1.6489795918367347, "no_speech_prob": 2.8130000373494113e-06}, {"id": 249, "seek": 122026, "start": 1227.66, "end": 1233.78, "text": " Kaggle's test set close enough that when we actually try and use this validation set we're going to get some", "tokens": [48751, 22631, 311, 1500, 992, 1998, 1547, 300, 562, 321, 767, 853, 293, 764, 341, 24071, 992, 321, 434, 516, 281, 483, 512], "temperature": 0.0, "avg_logprob": -0.16432179795934798, "compression_ratio": 1.6489795918367347, "no_speech_prob": 2.8130000373494113e-06}, {"id": 250, "seek": 122026, "start": 1234.58, "end": 1240.14, "text": " Reasonably accurate scores and the reason we want this is because on Kaggle", "tokens": [39693, 1188, 8559, 13444, 293, 264, 1778, 321, 528, 341, 307, 570, 322, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.16432179795934798, "compression_ratio": 1.6489795918367347, "no_speech_prob": 2.8130000373494113e-06}, {"id": 251, "seek": 122026, "start": 1240.14, "end": 1246.2, "text": " You can only submit so many times and if you submit too often you'll end up fitting to the leaderboard anyway and in real life", "tokens": [509, 393, 787, 10315, 370, 867, 1413, 293, 498, 291, 10315, 886, 2049, 291, 603, 917, 493, 15669, 281, 264, 5263, 3787, 4033, 293, 294, 957, 993], "temperature": 0.0, "avg_logprob": -0.16432179795934798, "compression_ratio": 1.6489795918367347, "no_speech_prob": 2.8130000373494113e-06}, {"id": 252, "seek": 124620, "start": 1246.2, "end": 1252.14, "text": " You actually want to build a model that's going to work in real life. Did you have a question can we help the green box?", "tokens": [509, 767, 528, 281, 1322, 257, 2316, 300, 311, 516, 281, 589, 294, 957, 993, 13, 2589, 291, 362, 257, 1168, 393, 321, 854, 264, 3092, 2424, 30], "temperature": 0.0, "avg_logprob": -0.16070233858548677, "compression_ratio": 1.7434782608695651, "no_speech_prob": 1.8738647895588656e-06}, {"id": 253, "seek": 124620, "start": 1258.92, "end": 1263.56, "text": " Can you explain the difference between a validation set and a test set absolutely", "tokens": [1664, 291, 2903, 264, 2649, 1296, 257, 24071, 992, 293, 257, 1500, 992, 3122], "temperature": 0.0, "avg_logprob": -0.16070233858548677, "compression_ratio": 1.7434782608695651, "no_speech_prob": 1.8738647895588656e-06}, {"id": 254, "seek": 124620, "start": 1264.28, "end": 1270.16, "text": " So what we're going to learn today is how to set one of the things to learn is how to set hyper parameters hyper parameters are", "tokens": [407, 437, 321, 434, 516, 281, 1466, 965, 307, 577, 281, 992, 472, 295, 264, 721, 281, 1466, 307, 577, 281, 992, 9848, 9834, 9848, 9834, 366], "temperature": 0.0, "avg_logprob": -0.16070233858548677, "compression_ratio": 1.7434782608695651, "no_speech_prob": 1.8738647895588656e-06}, {"id": 255, "seek": 124620, "start": 1270.16, "end": 1273.3, "text": " Like tuning parameters that are going to change how your model behaves", "tokens": [1743, 15164, 9834, 300, 366, 516, 281, 1319, 577, 428, 2316, 36896], "temperature": 0.0, "avg_logprob": -0.16070233858548677, "compression_ratio": 1.7434782608695651, "no_speech_prob": 1.8738647895588656e-06}, {"id": 256, "seek": 127330, "start": 1273.3, "end": 1281.06, "text": " Now if you just have one holdout set so one set of data that you're not using to train with and we use that", "tokens": [823, 498, 291, 445, 362, 472, 1797, 346, 992, 370, 472, 992, 295, 1412, 300, 291, 434, 406, 1228, 281, 3847, 365, 293, 321, 764, 300], "temperature": 0.0, "avg_logprob": -0.15685049984433236, "compression_ratio": 1.7836734693877552, "no_speech_prob": 3.2377463412558427e-06}, {"id": 257, "seek": 127330, "start": 1281.06, "end": 1286.3799999999999, "text": " To decide which set of hyper parameters to use if we try a thousand different sets of hyper parameters", "tokens": [1407, 4536, 597, 992, 295, 9848, 9834, 281, 764, 498, 321, 853, 257, 4714, 819, 6352, 295, 9848, 9834], "temperature": 0.0, "avg_logprob": -0.15685049984433236, "compression_ratio": 1.7836734693877552, "no_speech_prob": 3.2377463412558427e-06}, {"id": 258, "seek": 127330, "start": 1286.58, "end": 1292.06, "text": " We may end up overfitting to that holdout set that is to say we'll find something which only", "tokens": [492, 815, 917, 493, 670, 69, 2414, 281, 300, 1797, 346, 992, 300, 307, 281, 584, 321, 603, 915, 746, 597, 787], "temperature": 0.0, "avg_logprob": -0.15685049984433236, "compression_ratio": 1.7836734693877552, "no_speech_prob": 3.2377463412558427e-06}, {"id": 259, "seek": 127330, "start": 1292.78, "end": 1300.26, "text": " Accidentally worked so what we actually want to do is we really want to have a second holdout set where we can say, okay", "tokens": [5725, 36578, 2732, 370, 437, 321, 767, 528, 281, 360, 307, 321, 534, 528, 281, 362, 257, 1150, 1797, 346, 992, 689, 321, 393, 584, 11, 1392], "temperature": 0.0, "avg_logprob": -0.15685049984433236, "compression_ratio": 1.7836734693877552, "no_speech_prob": 3.2377463412558427e-06}, {"id": 260, "seek": 127330, "start": 1300.26, "end": 1302.02, "text": " I'm finished", "tokens": [286, 478, 4335], "temperature": 0.0, "avg_logprob": -0.15685049984433236, "compression_ratio": 1.7836734693877552, "no_speech_prob": 3.2377463412558427e-06}, {"id": 261, "seek": 130202, "start": 1302.02, "end": 1309.96, "text": " Okay, I've done the best I can and now just once right at the end. I'm going to see whether it works and so", "tokens": [1033, 11, 286, 600, 1096, 264, 1151, 286, 393, 293, 586, 445, 1564, 558, 412, 264, 917, 13, 286, 478, 516, 281, 536, 1968, 309, 1985, 293, 370], "temperature": 0.0, "avg_logprob": -0.13888713993977025, "compression_ratio": 1.6008064516129032, "no_speech_prob": 1.2098624893042143e-06}, {"id": 262, "seek": 130202, "start": 1311.46, "end": 1316.06, "text": " This is something which almost nobody in industry does correctly", "tokens": [639, 307, 746, 597, 1920, 5079, 294, 3518, 775, 8944], "temperature": 0.0, "avg_logprob": -0.13888713993977025, "compression_ratio": 1.6008064516129032, "no_speech_prob": 1.2098624893042143e-06}, {"id": 263, "seek": 130202, "start": 1318.34, "end": 1323.92, "text": " You really actually need to remove that holdout set that's called the test set remove it from the data", "tokens": [509, 534, 767, 643, 281, 4159, 300, 1797, 346, 992, 300, 311, 1219, 264, 1500, 992, 4159, 309, 490, 264, 1412], "temperature": 0.0, "avg_logprob": -0.13888713993977025, "compression_ratio": 1.6008064516129032, "no_speech_prob": 1.2098624893042143e-06}, {"id": 264, "seek": 130202, "start": 1324.34, "end": 1331.66, "text": " Give it to somebody else and tell them do not let me look at this data until I promise you I'm finished like it's so hard", "tokens": [5303, 309, 281, 2618, 1646, 293, 980, 552, 360, 406, 718, 385, 574, 412, 341, 1412, 1826, 286, 6228, 291, 286, 478, 4335, 411, 309, 311, 370, 1152], "temperature": 0.0, "avg_logprob": -0.13888713993977025, "compression_ratio": 1.6008064516129032, "no_speech_prob": 1.2098624893042143e-06}, {"id": 265, "seek": 133166, "start": 1331.66, "end": 1338.22, "text": " Otherwise not to look at it and for example in the world of psychology and sociology you might have heard about this replication crisis", "tokens": [10328, 406, 281, 574, 412, 309, 293, 337, 1365, 294, 264, 1002, 295, 15105, 293, 41744, 291, 1062, 362, 2198, 466, 341, 39911, 5869], "temperature": 0.0, "avg_logprob": -0.16679592927296957, "compression_ratio": 1.6978417266187051, "no_speech_prob": 2.0261270492483163e-06}, {"id": 266, "seek": 133166, "start": 1338.5, "end": 1345.3400000000001, "text": " This is basically because people in these fields have accidentally or intentionally maybe been p-hacking", "tokens": [639, 307, 1936, 570, 561, 294, 613, 7909, 362, 15715, 420, 22062, 1310, 668, 280, 12, 71, 14134], "temperature": 0.0, "avg_logprob": -0.16679592927296957, "compression_ratio": 1.6978417266187051, "no_speech_prob": 2.0261270492483163e-06}, {"id": 267, "seek": 133166, "start": 1345.5800000000002, "end": 1347.5800000000002, "text": " Which means they've been basically", "tokens": [3013, 1355, 436, 600, 668, 1936], "temperature": 0.0, "avg_logprob": -0.16679592927296957, "compression_ratio": 1.6978417266187051, "no_speech_prob": 2.0261270492483163e-06}, {"id": 268, "seek": 133166, "start": 1347.74, "end": 1354.74, "text": " Trying lots of different variations until they find something that works and then it turns out when they try to replicate it in other words", "tokens": [20180, 3195, 295, 819, 17840, 1826, 436, 915, 746, 300, 1985, 293, 550, 309, 4523, 484, 562, 436, 853, 281, 25356, 309, 294, 661, 2283], "temperature": 0.0, "avg_logprob": -0.16679592927296957, "compression_ratio": 1.6978417266187051, "no_speech_prob": 2.0261270492483163e-06}, {"id": 269, "seek": 133166, "start": 1354.74, "end": 1357.3000000000002, "text": " It's like somebody creates a test set somebody says okay", "tokens": [467, 311, 411, 2618, 7829, 257, 1500, 992, 2618, 1619, 1392], "temperature": 0.0, "avg_logprob": -0.16679592927296957, "compression_ratio": 1.6978417266187051, "no_speech_prob": 2.0261270492483163e-06}, {"id": 270, "seek": 135730, "start": 1357.3, "end": 1362.98, "text": " This study which shows you know the impact of whether you eat marshmallows on your tenacity later in life", "tokens": [639, 2979, 597, 3110, 291, 458, 264, 2712, 295, 1968, 291, 1862, 29817, 38811, 322, 428, 2064, 19008, 1780, 294, 993], "temperature": 0.0, "avg_logprob": -0.1941987489399157, "compression_ratio": 1.588235294117647, "no_speech_prob": 8.059383844738477e-07}, {"id": 271, "seek": 135730, "start": 1362.98, "end": 1365.46, "text": " I'm going to read it and like", "tokens": [286, 478, 516, 281, 1401, 309, 293, 411], "temperature": 0.0, "avg_logprob": -0.1941987489399157, "compression_ratio": 1.588235294117647, "no_speech_prob": 8.059383844738477e-07}, {"id": 272, "seek": 135730, "start": 1366.06, "end": 1371.52, "text": " Over half the time they're finding the effect turns out not to exist so that's why we want to have a test set", "tokens": [4886, 1922, 264, 565, 436, 434, 5006, 264, 1802, 4523, 484, 406, 281, 2514, 370, 300, 311, 983, 321, 528, 281, 362, 257, 1500, 992], "temperature": 0.0, "avg_logprob": -0.1941987489399157, "compression_ratio": 1.588235294117647, "no_speech_prob": 8.059383844738477e-07}, {"id": 273, "seek": 135730, "start": 1372.8999999999999, "end": 1374.8999999999999, "text": " You get that next door", "tokens": [509, 483, 300, 958, 2853], "temperature": 0.0, "avg_logprob": -0.1941987489399157, "compression_ratio": 1.588235294117647, "no_speech_prob": 8.059383844738477e-07}, {"id": 274, "seek": 135730, "start": 1376.22, "end": 1381.6599999999999, "text": " So for handling categorical data you converted those two numerics to numbers order numbers", "tokens": [407, 337, 13175, 19250, 804, 1412, 291, 16424, 729, 732, 7866, 1167, 281, 3547, 1668, 3547], "temperature": 0.0, "avg_logprob": -0.1941987489399157, "compression_ratio": 1.588235294117647, "no_speech_prob": 8.059383844738477e-07}, {"id": 275, "seek": 135730, "start": 1382.1399999999999, "end": 1384.06, "text": " I've seen a lot of", "tokens": [286, 600, 1612, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.1941987489399157, "compression_ratio": 1.588235294117647, "no_speech_prob": 8.059383844738477e-07}, {"id": 276, "seek": 138406, "start": 1384.06, "end": 1389.94, "text": " Models where we convert categorical data into different columns using one hot encoding yes", "tokens": [6583, 1625, 689, 321, 7620, 19250, 804, 1412, 666, 819, 13766, 1228, 472, 2368, 43430, 2086], "temperature": 0.0, "avg_logprob": -0.162330849226131, "compression_ratio": 1.484304932735426, "no_speech_prob": 6.048811428627232e-06}, {"id": 277, "seek": 138406, "start": 1389.94, "end": 1395.8999999999999, "text": " So which approach to use in which model yeah, we're going to tackle that today. Yeah, it's a great question", "tokens": [407, 597, 3109, 281, 764, 294, 597, 2316, 1338, 11, 321, 434, 516, 281, 14896, 300, 965, 13, 865, 11, 309, 311, 257, 869, 1168], "temperature": 0.0, "avg_logprob": -0.162330849226131, "compression_ratio": 1.484304932735426, "no_speech_prob": 6.048811428627232e-06}, {"id": 278, "seek": 138406, "start": 1397.06, "end": 1399.98, "text": " Okay, so so I'm splitting my", "tokens": [1033, 11, 370, 370, 286, 478, 30348, 452], "temperature": 0.0, "avg_logprob": -0.162330849226131, "compression_ratio": 1.484304932735426, "no_speech_prob": 6.048811428627232e-06}, {"id": 279, "seek": 138406, "start": 1402.06, "end": 1404.06, "text": " My data into", "tokens": [1222, 1412, 666], "temperature": 0.0, "avg_logprob": -0.162330849226131, "compression_ratio": 1.484304932735426, "no_speech_prob": 6.048811428627232e-06}, {"id": 280, "seek": 138406, "start": 1404.3, "end": 1410.78, "text": " Validation and training sets and so you can see now that my validation set is 12,000 by 66", "tokens": [7188, 327, 399, 293, 3097, 6352, 293, 370, 291, 393, 536, 586, 300, 452, 24071, 992, 307, 2272, 11, 1360, 538, 21126], "temperature": 0.0, "avg_logprob": -0.162330849226131, "compression_ratio": 1.484304932735426, "no_speech_prob": 6.048811428627232e-06}, {"id": 281, "seek": 141078, "start": 1410.78, "end": 1413.3, "text": " Where else my training set is?", "tokens": [2305, 1646, 452, 3097, 992, 307, 30], "temperature": 0.0, "avg_logprob": -0.2065556439486417, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.2408648773271125e-06}, {"id": 282, "seek": 141078, "start": 1414.06, "end": 1421.16, "text": " 399,000 by 66 okay, so we're going to use this set of data to train a model and this set of data to see how well", "tokens": [805, 8494, 11, 1360, 538, 21126, 1392, 11, 370, 321, 434, 516, 281, 764, 341, 992, 295, 1412, 281, 3847, 257, 2316, 293, 341, 992, 295, 1412, 281, 536, 577, 731], "temperature": 0.0, "avg_logprob": -0.2065556439486417, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.2408648773271125e-06}, {"id": 283, "seek": 141078, "start": 1421.16, "end": 1423.02, "text": " It's working", "tokens": [467, 311, 1364], "temperature": 0.0, "avg_logprob": -0.2065556439486417, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.2408648773271125e-06}, {"id": 284, "seek": 141078, "start": 1423.02, "end": 1425.36, "text": " So when we then tried that last week", "tokens": [407, 562, 321, 550, 3031, 300, 1036, 1243], "temperature": 0.0, "avg_logprob": -0.2065556439486417, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.2408648773271125e-06}, {"id": 285, "seek": 141078, "start": 1426.02, "end": 1432.54, "text": " We found out just a moment. We found out that our model which had point nine eight to ask where on the training set", "tokens": [492, 1352, 484, 445, 257, 1623, 13, 492, 1352, 484, 300, 527, 2316, 597, 632, 935, 4949, 3180, 281, 1029, 689, 322, 264, 3097, 992], "temperature": 0.0, "avg_logprob": -0.2065556439486417, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.2408648773271125e-06}, {"id": 286, "seek": 141078, "start": 1432.82, "end": 1439.5, "text": " Only had point eight eight seven on the validation set which makes us think that we're overfitting quite badly", "tokens": [5686, 632, 935, 3180, 3180, 3407, 322, 264, 24071, 992, 597, 1669, 505, 519, 300, 321, 434, 670, 69, 2414, 1596, 13425], "temperature": 0.0, "avg_logprob": -0.2065556439486417, "compression_ratio": 1.7004048582995952, "no_speech_prob": 6.2408648773271125e-06}, {"id": 287, "seek": 143950, "start": 1439.5, "end": 1446.06, "text": " But it turned out it wasn't too badly because the root mean squared error on the logs of the prices", "tokens": [583, 309, 3574, 484, 309, 2067, 380, 886, 13425, 570, 264, 5593, 914, 8889, 6713, 322, 264, 20820, 295, 264, 7901], "temperature": 0.0, "avg_logprob": -0.2123152951160109, "compression_ratio": 1.5495495495495495, "no_speech_prob": 7.411060323647689e-06}, {"id": 288, "seek": 143950, "start": 1446.34, "end": 1451.46, "text": " Actually would have put us in the top 25% of the competition anyway, so even although we're overfitting", "tokens": [5135, 576, 362, 829, 505, 294, 264, 1192, 3552, 4, 295, 264, 6211, 4033, 11, 370, 754, 4878, 321, 434, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.2123152951160109, "compression_ratio": 1.5495495495495495, "no_speech_prob": 7.411060323647689e-06}, {"id": 289, "seek": 143950, "start": 1451.94, "end": 1455.3, "text": " It wasn't the end of the world could you pass the microphone to Marsha please?", "tokens": [467, 2067, 380, 264, 917, 295, 264, 1002, 727, 291, 1320, 264, 10952, 281, 9692, 1641, 1767, 30], "temperature": 0.0, "avg_logprob": -0.2123152951160109, "compression_ratio": 1.5495495495495495, "no_speech_prob": 7.411060323647689e-06}, {"id": 290, "seek": 143950, "start": 1459.18, "end": 1463.7, "text": " In terms of you dividing the set into training and validation", "tokens": [682, 2115, 295, 291, 26764, 264, 992, 666, 3097, 293, 24071], "temperature": 0.0, "avg_logprob": -0.2123152951160109, "compression_ratio": 1.5495495495495495, "no_speech_prob": 7.411060323647689e-06}, {"id": 291, "seek": 146370, "start": 1463.7, "end": 1471.1000000000001, "text": " Validation it seems like you simply take the first and train observations of the data set and set them aside", "tokens": [7188, 327, 399, 309, 2544, 411, 291, 2935, 747, 264, 700, 293, 3847, 18163, 295, 264, 1412, 992, 293, 992, 552, 7359], "temperature": 0.0, "avg_logprob": -0.16038954378378512, "compression_ratio": 1.761061946902655, "no_speech_prob": 7.07176377545693e-06}, {"id": 292, "seek": 146370, "start": 1471.78, "end": 1476.42, "text": " Why don't you like why don't you randomly pick up the observations?", "tokens": [1545, 500, 380, 291, 411, 983, 500, 380, 291, 16979, 1888, 493, 264, 18163, 30], "temperature": 0.0, "avg_logprob": -0.16038954378378512, "compression_ratio": 1.761061946902655, "no_speech_prob": 7.07176377545693e-06}, {"id": 293, "seek": 146370, "start": 1477.14, "end": 1482.06, "text": " Because if I did that I wouldn't be replicating the test set so Kaggle", "tokens": [1436, 498, 286, 630, 300, 286, 2759, 380, 312, 3248, 30541, 264, 1500, 992, 370, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.16038954378378512, "compression_ratio": 1.761061946902655, "no_speech_prob": 7.07176377545693e-06}, {"id": 294, "seek": 146370, "start": 1482.3400000000001, "end": 1486.42, "text": " Has a test set that when you actually look at the dates in the test set they are a", "tokens": [8646, 257, 1500, 992, 300, 562, 291, 767, 574, 412, 264, 11691, 294, 264, 1500, 992, 436, 366, 257], "temperature": 0.0, "avg_logprob": -0.16038954378378512, "compression_ratio": 1.761061946902655, "no_speech_prob": 7.07176377545693e-06}, {"id": 295, "seek": 146370, "start": 1487.1000000000001, "end": 1491.66, "text": " Set of dates that are more recent than any date in the training set", "tokens": [8928, 295, 11691, 300, 366, 544, 5162, 813, 604, 4002, 294, 264, 3097, 992], "temperature": 0.0, "avg_logprob": -0.16038954378378512, "compression_ratio": 1.761061946902655, "no_speech_prob": 7.07176377545693e-06}, {"id": 296, "seek": 149166, "start": 1491.66, "end": 1496.9, "text": " So if we used a validation set that was a random sample that is much easier", "tokens": [407, 498, 321, 1143, 257, 24071, 992, 300, 390, 257, 4974, 6889, 300, 307, 709, 3571], "temperature": 0.0, "avg_logprob": -0.15057600021362305, "compression_ratio": 1.779527559055118, "no_speech_prob": 1.577949433340109e-06}, {"id": 297, "seek": 149166, "start": 1497.22, "end": 1504.38, "text": " Because we're predicting options like what's the value of this piece of industrial equipment on this day when we actually already have some", "tokens": [1436, 321, 434, 32884, 3956, 411, 437, 311, 264, 2158, 295, 341, 2522, 295, 9987, 5927, 322, 341, 786, 562, 321, 767, 1217, 362, 512], "temperature": 0.0, "avg_logprob": -0.15057600021362305, "compression_ratio": 1.779527559055118, "no_speech_prob": 1.577949433340109e-06}, {"id": 298, "seek": 149166, "start": 1504.5, "end": 1506.46, "text": " observations from that day", "tokens": [18163, 490, 300, 786], "temperature": 0.0, "avg_logprob": -0.15057600021362305, "compression_ratio": 1.779527559055118, "no_speech_prob": 1.577949433340109e-06}, {"id": 299, "seek": 149166, "start": 1506.46, "end": 1515.0400000000002, "text": " so in general any time you're building a model that has a time element you want your test set to be a", "tokens": [370, 294, 2674, 604, 565, 291, 434, 2390, 257, 2316, 300, 575, 257, 565, 4478, 291, 528, 428, 1500, 992, 281, 312, 257], "temperature": 0.0, "avg_logprob": -0.15057600021362305, "compression_ratio": 1.779527559055118, "no_speech_prob": 1.577949433340109e-06}, {"id": 300, "seek": 149166, "start": 1515.78, "end": 1520.66, "text": " Separate time period and therefore you really need your validation set to be a separate time period as well", "tokens": [43480, 473, 565, 2896, 293, 4412, 291, 534, 643, 428, 24071, 992, 281, 312, 257, 4994, 565, 2896, 382, 731], "temperature": 0.0, "avg_logprob": -0.15057600021362305, "compression_ratio": 1.779527559055118, "no_speech_prob": 1.577949433340109e-06}, {"id": 301, "seek": 152066, "start": 1520.66, "end": 1523.5400000000002, "text": " And in this case the data was already sorted. So that's why this works", "tokens": [400, 294, 341, 1389, 264, 1412, 390, 1217, 25462, 13, 407, 300, 311, 983, 341, 1985], "temperature": 0.0, "avg_logprob": -0.20708348377641425, "compression_ratio": 1.66, "no_speech_prob": 7.253442163346335e-05}, {"id": 302, "seek": 152066, "start": 1530.5400000000002, "end": 1536.8200000000002, "text": " So, let's say we have our test the training set where we train the data and then we have the validation set", "tokens": [407, 11, 718, 311, 584, 321, 362, 527, 1500, 264, 3097, 992, 689, 321, 3847, 264, 1412, 293, 550, 321, 362, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.20708348377641425, "compression_ratio": 1.66, "no_speech_prob": 7.253442163346335e-05}, {"id": 303, "seek": 152066, "start": 1537.1000000000001, "end": 1543.38, "text": " Against which we are trying to find the R square in in case our R square turns out to be really bad", "tokens": [29995, 597, 321, 366, 1382, 281, 915, 264, 497, 3732, 294, 294, 1389, 527, 497, 3732, 4523, 484, 281, 312, 534, 1578], "temperature": 0.0, "avg_logprob": -0.20708348377641425, "compression_ratio": 1.66, "no_speech_prob": 7.253442163346335e-05}, {"id": 304, "seek": 152066, "start": 1543.38, "end": 1547.14, "text": " We would want to tune our parameters and run it again", "tokens": [492, 576, 528, 281, 10864, 527, 9834, 293, 1190, 309, 797], "temperature": 0.0, "avg_logprob": -0.20708348377641425, "compression_ratio": 1.66, "no_speech_prob": 7.253442163346335e-05}, {"id": 305, "seek": 154714, "start": 1547.14, "end": 1554.5800000000002, "text": " Yes, so wouldn't that be eventually over fitting on the overall training set? Yeah, so actually that's that's the issue", "tokens": [1079, 11, 370, 2759, 380, 300, 312, 4728, 670, 15669, 322, 264, 4787, 3097, 992, 30, 865, 11, 370, 767, 300, 311, 300, 311, 264, 2734], "temperature": 0.0, "avg_logprob": -0.1595474621197125, "compression_ratio": 1.84765625, "no_speech_prob": 2.9944148991489783e-06}, {"id": 306, "seek": 154714, "start": 1554.5800000000002, "end": 1560.96, "text": " So that would eventually have the possibility of over fitting on the validation set and then when we try it on the test set", "tokens": [407, 300, 576, 4728, 362, 264, 7959, 295, 670, 15669, 322, 264, 24071, 992, 293, 550, 562, 321, 853, 309, 322, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.1595474621197125, "compression_ratio": 1.84765625, "no_speech_prob": 2.9944148991489783e-06}, {"id": 307, "seek": 154714, "start": 1560.96, "end": 1563.8200000000002, "text": " Or we submit it to Kaggle it turns out not to be very good", "tokens": [1610, 321, 10315, 309, 281, 48751, 22631, 309, 4523, 484, 406, 281, 312, 588, 665], "temperature": 0.0, "avg_logprob": -0.1595474621197125, "compression_ratio": 1.84765625, "no_speech_prob": 2.9944148991489783e-06}, {"id": 308, "seek": 154714, "start": 1563.8200000000002, "end": 1571.5400000000002, "text": " And this happens in Kaggle competitions all the time Kaggle actually has a fourth data set which is called the private leaderboard", "tokens": [400, 341, 2314, 294, 48751, 22631, 26185, 439, 264, 565, 48751, 22631, 767, 575, 257, 6409, 1412, 992, 597, 307, 1219, 264, 4551, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.1595474621197125, "compression_ratio": 1.84765625, "no_speech_prob": 2.9944148991489783e-06}, {"id": 309, "seek": 154714, "start": 1572.3000000000002, "end": 1574.94, "text": " Set and every time you submit to Kaggle", "tokens": [8928, 293, 633, 565, 291, 10315, 281, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.1595474621197125, "compression_ratio": 1.84765625, "no_speech_prob": 2.9944148991489783e-06}, {"id": 310, "seek": 157494, "start": 1574.94, "end": 1580.06, "text": " You actually only get feedback on how well it does on something called the public leaderboard set", "tokens": [509, 767, 787, 483, 5824, 322, 577, 731, 309, 775, 322, 746, 1219, 264, 1908, 5263, 3787, 992], "temperature": 0.0, "avg_logprob": -0.1498136340447192, "compression_ratio": 1.7357142857142858, "no_speech_prob": 4.965250468558224e-07}, {"id": 311, "seek": 157494, "start": 1580.5, "end": 1583.5800000000002, "text": " And you don't know which rows they are and at the end of the competition", "tokens": [400, 291, 500, 380, 458, 597, 13241, 436, 366, 293, 412, 264, 917, 295, 264, 6211], "temperature": 0.0, "avg_logprob": -0.1498136340447192, "compression_ratio": 1.7357142857142858, "no_speech_prob": 4.965250468558224e-07}, {"id": 312, "seek": 157494, "start": 1583.6200000000001, "end": 1588.1000000000001, "text": " You can actually get judged on a different data set entirely called the private leaderboard set", "tokens": [509, 393, 767, 483, 27485, 322, 257, 819, 1412, 992, 7696, 1219, 264, 4551, 5263, 3787, 992], "temperature": 0.0, "avg_logprob": -0.1498136340447192, "compression_ratio": 1.7357142857142858, "no_speech_prob": 4.965250468558224e-07}, {"id": 313, "seek": 157494, "start": 1588.5, "end": 1595.14, "text": " so the only way to avoid this is to actually be a good machine learning practitioner and", "tokens": [370, 264, 787, 636, 281, 5042, 341, 307, 281, 767, 312, 257, 665, 3479, 2539, 32125, 293], "temperature": 0.0, "avg_logprob": -0.1498136340447192, "compression_ratio": 1.7357142857142858, "no_speech_prob": 4.965250468558224e-07}, {"id": 314, "seek": 157494, "start": 1595.3, "end": 1598.8600000000001, "text": " Know how to set these parameters as effectively as possible", "tokens": [10265, 577, 281, 992, 613, 9834, 382, 8659, 382, 1944], "temperature": 0.0, "avg_logprob": -0.1498136340447192, "compression_ratio": 1.7357142857142858, "no_speech_prob": 4.965250468558224e-07}, {"id": 315, "seek": 159886, "start": 1598.86, "end": 1604.58, "text": " Which we're going to be doing partly today and over the next few weeks", "tokens": [3013, 321, 434, 516, 281, 312, 884, 17031, 965, 293, 670, 264, 958, 1326, 3259], "temperature": 0.0, "avg_logprob": -0.24672859686392326, "compression_ratio": 1.429530201342282, "no_speech_prob": 1.4737090168637224e-05}, {"id": 316, "seek": 159886, "start": 1605.58, "end": 1607.58, "text": " Can you get that actually what you throw?", "tokens": [1664, 291, 483, 300, 767, 437, 291, 3507, 30], "temperature": 0.0, "avg_logprob": -0.24672859686392326, "compression_ratio": 1.429530201342282, "no_speech_prob": 1.4737090168637224e-05}, {"id": 317, "seek": 159886, "start": 1613.26, "end": 1619.34, "text": " Is it too early or late to ask what's the difference between a hyper parameter and a parameter?", "tokens": [1119, 309, 886, 2440, 420, 3469, 281, 1029, 437, 311, 264, 2649, 1296, 257, 9848, 13075, 293, 257, 13075, 30], "temperature": 0.0, "avg_logprob": -0.24672859686392326, "compression_ratio": 1.429530201342282, "no_speech_prob": 1.4737090168637224e-05}, {"id": 318, "seek": 161934, "start": 1619.34, "end": 1623.34, "text": " Okay, okay", "tokens": [1033, 11, 1392], "temperature": 0.0, "avg_logprob": -0.20773239135742189, "compression_ratio": 1.7253886010362693, "no_speech_prob": 6.681471518277249e-07}, {"id": 319, "seek": 161934, "start": 1629.6999999999998, "end": 1631.6999999999998, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.20773239135742189, "compression_ratio": 1.7253886010362693, "no_speech_prob": 6.681471518277249e-07}, {"id": 320, "seek": 161934, "start": 1632.1, "end": 1634.3799999999999, "text": " Let's start tracking things on root mean squared error", "tokens": [961, 311, 722, 11603, 721, 322, 5593, 914, 8889, 6713], "temperature": 0.0, "avg_logprob": -0.20773239135742189, "compression_ratio": 1.7253886010362693, "no_speech_prob": 6.681471518277249e-07}, {"id": 321, "seek": 161934, "start": 1634.3799999999999, "end": 1641.78, "text": " So here is root mean squared error in a line of code and you can see here like this is one of these examples where?", "tokens": [407, 510, 307, 5593, 914, 8889, 6713, 294, 257, 1622, 295, 3089, 293, 291, 393, 536, 510, 411, 341, 307, 472, 295, 613, 5110, 689, 30], "temperature": 0.0, "avg_logprob": -0.20773239135742189, "compression_ratio": 1.7253886010362693, "no_speech_prob": 6.681471518277249e-07}, {"id": 322, "seek": 161934, "start": 1642.22, "end": 1643.86, "text": " I'm not", "tokens": [286, 478, 406], "temperature": 0.0, "avg_logprob": -0.20773239135742189, "compression_ratio": 1.7253886010362693, "no_speech_prob": 6.681471518277249e-07}, {"id": 323, "seek": 164386, "start": 1643.86, "end": 1649.6599999999999, "text": " Writing this the way a proper software engineer would write this right so a proper software engineer would be a number of things differently they", "tokens": [32774, 341, 264, 636, 257, 2296, 4722, 11403, 576, 2464, 341, 558, 370, 257, 2296, 4722, 11403, 576, 312, 257, 1230, 295, 721, 7614, 436], "temperature": 0.0, "avg_logprob": -0.18837965859307182, "compression_ratio": 1.8125, "no_speech_prob": 3.844892944471212e-06}, {"id": 324, "seek": 164386, "start": 1649.6599999999999, "end": 1651.5, "text": " Would have it on a different line", "tokens": [6068, 362, 309, 322, 257, 819, 1622], "temperature": 0.0, "avg_logprob": -0.18837965859307182, "compression_ratio": 1.8125, "no_speech_prob": 3.844892944471212e-06}, {"id": 325, "seek": 164386, "start": 1651.5, "end": 1653.5, "text": " They would use longer variable names", "tokens": [814, 576, 764, 2854, 7006, 5288], "temperature": 0.0, "avg_logprob": -0.18837965859307182, "compression_ratio": 1.8125, "no_speech_prob": 3.844892944471212e-06}, {"id": 326, "seek": 164386, "start": 1660.1, "end": 1662.1, "text": " They would have documentation", "tokens": [814, 576, 362, 14333], "temperature": 0.0, "avg_logprob": -0.18837965859307182, "compression_ratio": 1.8125, "no_speech_prob": 3.844892944471212e-06}, {"id": 327, "seek": 164386, "start": 1663.06, "end": 1665.62, "text": " blah blah blah right but", "tokens": [12288, 12288, 12288, 558, 457], "temperature": 0.0, "avg_logprob": -0.18837965859307182, "compression_ratio": 1.8125, "no_speech_prob": 3.844892944471212e-06}, {"id": 328, "seek": 164386, "start": 1667.9799999999998, "end": 1671.4599999999998, "text": " I really think like for me. I really think that", "tokens": [286, 534, 519, 411, 337, 385, 13, 286, 534, 519, 300], "temperature": 0.0, "avg_logprob": -0.18837965859307182, "compression_ratio": 1.8125, "no_speech_prob": 3.844892944471212e-06}, {"id": 329, "seek": 167146, "start": 1671.46, "end": 1673.46, "text": " being able to", "tokens": [885, 1075, 281], "temperature": 0.0, "avg_logprob": -0.22815938313802084, "compression_ratio": 1.5392670157068062, "no_speech_prob": 3.3405062822566833e-06}, {"id": 330, "seek": 167146, "start": 1676.38, "end": 1679.7, "text": " Look at something in one go with your eyes and like", "tokens": [2053, 412, 746, 294, 472, 352, 365, 428, 2575, 293, 411], "temperature": 0.0, "avg_logprob": -0.22815938313802084, "compression_ratio": 1.5392670157068062, "no_speech_prob": 3.3405062822566833e-06}, {"id": 331, "seek": 167146, "start": 1680.3400000000001, "end": 1684.42, "text": " Over time learn to immediately see what's going on has a lot of value", "tokens": [4886, 565, 1466, 281, 4258, 536, 437, 311, 516, 322, 575, 257, 688, 295, 2158], "temperature": 0.0, "avg_logprob": -0.22815938313802084, "compression_ratio": 1.5392670157068062, "no_speech_prob": 3.3405062822566833e-06}, {"id": 332, "seek": 167146, "start": 1685.3, "end": 1688.22, "text": " and also to like consistently use like", "tokens": [293, 611, 281, 411, 14961, 764, 411], "temperature": 0.0, "avg_logprob": -0.22815938313802084, "compression_ratio": 1.5392670157068062, "no_speech_prob": 3.3405062822566833e-06}, {"id": 333, "seek": 167146, "start": 1689.82, "end": 1696.9, "text": " Particular letters to have mean particular things or abbreviations. I think works really well in data science", "tokens": [4100, 14646, 7825, 281, 362, 914, 1729, 721, 420, 35839, 763, 13, 286, 519, 1985, 534, 731, 294, 1412, 3497], "temperature": 0.0, "avg_logprob": -0.22815938313802084, "compression_ratio": 1.5392670157068062, "no_speech_prob": 3.3405062822566833e-06}, {"id": 334, "seek": 167146, "start": 1697.6200000000001, "end": 1698.7, "text": " if", "tokens": [498], "temperature": 0.0, "avg_logprob": -0.22815938313802084, "compression_ratio": 1.5392670157068062, "no_speech_prob": 3.3405062822566833e-06}, {"id": 335, "seek": 167146, "start": 1698.7, "end": 1700.1000000000001, "text": " you're", "tokens": [291, 434], "temperature": 0.0, "avg_logprob": -0.22815938313802084, "compression_ratio": 1.5392670157068062, "no_speech_prob": 3.3405062822566833e-06}, {"id": 336, "seek": 170010, "start": 1700.1, "end": 1702.4599999999998, "text": " Doing it like a take-home interview", "tokens": [18496, 309, 411, 257, 747, 12, 25336, 4049], "temperature": 0.0, "avg_logprob": -0.20647030491982737, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.6433590291126166e-06}, {"id": 337, "seek": 170010, "start": 1702.98, "end": 1704.98, "text": " test or something", "tokens": [1500, 420, 746], "temperature": 0.0, "avg_logprob": -0.20647030491982737, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.6433590291126166e-06}, {"id": 338, "seek": 170010, "start": 1704.98, "end": 1711.5, "text": " You should write your code according to pep8 standards right so pep8 is the", "tokens": [509, 820, 2464, 428, 3089, 4650, 281, 520, 79, 23, 7787, 558, 370, 520, 79, 23, 307, 264], "temperature": 0.0, "avg_logprob": -0.20647030491982737, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.6433590291126166e-06}, {"id": 339, "seek": 170010, "start": 1712.3799999999999, "end": 1718.34, "text": " The style guide for Python code, and you should know it and use it because a lot of software engineers are super anal", "tokens": [440, 3758, 5934, 337, 15329, 3089, 11, 293, 291, 820, 458, 309, 293, 764, 309, 570, 257, 688, 295, 4722, 11955, 366, 1687, 2624], "temperature": 0.0, "avg_logprob": -0.20647030491982737, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.6433590291126166e-06}, {"id": 340, "seek": 170010, "start": 1718.62, "end": 1720.62, "text": " About this kind of thing", "tokens": [7769, 341, 733, 295, 551], "temperature": 0.0, "avg_logprob": -0.20647030491982737, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.6433590291126166e-06}, {"id": 341, "seek": 170010, "start": 1720.6599999999999, "end": 1723.6599999999999, "text": " But for your own work, you know I", "tokens": [583, 337, 428, 1065, 589, 11, 291, 458, 286], "temperature": 0.0, "avg_logprob": -0.20647030491982737, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.6433590291126166e-06}, {"id": 342, "seek": 170010, "start": 1725.26, "end": 1727.6599999999999, "text": " Think this is I think this works well for me", "tokens": [6557, 341, 307, 286, 519, 341, 1985, 731, 337, 385], "temperature": 0.0, "avg_logprob": -0.20647030491982737, "compression_ratio": 1.588235294117647, "no_speech_prob": 6.6433590291126166e-06}, {"id": 343, "seek": 172766, "start": 1727.66, "end": 1733.2, "text": " You know so I just wanted to make you aware a that you shouldn't necessarily use this as a role model for", "tokens": [509, 458, 370, 286, 445, 1415, 281, 652, 291, 3650, 257, 300, 291, 4659, 380, 4725, 764, 341, 382, 257, 3090, 2316, 337], "temperature": 0.0, "avg_logprob": -0.208838562467205, "compression_ratio": 1.796116504854369, "no_speech_prob": 1.1300555343041196e-05}, {"id": 344, "seek": 172766, "start": 1734.02, "end": 1739.7, "text": " Dealing with software engineers, but be that I actually think this is not this is a reasonable approach", "tokens": [1346, 4270, 365, 4722, 11955, 11, 457, 312, 300, 286, 767, 519, 341, 307, 406, 341, 307, 257, 10585, 3109], "temperature": 0.0, "avg_logprob": -0.208838562467205, "compression_ratio": 1.796116504854369, "no_speech_prob": 1.1300555343041196e-05}, {"id": 345, "seek": 172766, "start": 1740.1000000000001, "end": 1745.38, "text": " Okay, so there's our root mean squared error and then from time to time. We're just going to print out the score which will give us", "tokens": [1033, 11, 370, 456, 311, 527, 5593, 914, 8889, 6713, 293, 550, 490, 565, 281, 565, 13, 492, 434, 445, 516, 281, 4482, 484, 264, 6175, 597, 486, 976, 505], "temperature": 0.0, "avg_logprob": -0.208838562467205, "compression_ratio": 1.796116504854369, "no_speech_prob": 1.1300555343041196e-05}, {"id": 346, "seek": 172766, "start": 1745.38, "end": 1746.74, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.208838562467205, "compression_ratio": 1.796116504854369, "no_speech_prob": 1.1300555343041196e-05}, {"id": 347, "seek": 172766, "start": 1746.74, "end": 1752.0600000000002, "text": " RMSC of the predictions on the training versus the actual their predictions on the valid versus the actual", "tokens": [497, 10288, 34, 295, 264, 21264, 322, 264, 3097, 5717, 264, 3539, 641, 21264, 322, 264, 7363, 5717, 264, 3539], "temperature": 0.0, "avg_logprob": -0.208838562467205, "compression_ratio": 1.796116504854369, "no_speech_prob": 1.1300555343041196e-05}, {"id": 348, "seek": 175206, "start": 1752.06, "end": 1757.6599999999999, "text": " RMSC the R squared for the training and the R squared for the valid and we'll come back to OOB in a moment", "tokens": [497, 10288, 34, 264, 497, 8889, 337, 264, 3097, 293, 264, 497, 8889, 337, 264, 7363, 293, 321, 603, 808, 646, 281, 422, 46, 33, 294, 257, 1623], "temperature": 0.0, "avg_logprob": -0.20749054298744546, "compression_ratio": 1.6338582677165354, "no_speech_prob": 4.565933522826526e-06}, {"id": 349, "seek": 175206, "start": 1757.78, "end": 1760.58, "text": " So when we ran that we found that this", "tokens": [407, 562, 321, 5872, 300, 321, 1352, 300, 341], "temperature": 0.0, "avg_logprob": -0.20749054298744546, "compression_ratio": 1.6338582677165354, "no_speech_prob": 4.565933522826526e-06}, {"id": 350, "seek": 175206, "start": 1761.26, "end": 1765.4199999999998, "text": " RMSC was in the top 25 percent and it's like okay. There's a good start now", "tokens": [497, 10288, 34, 390, 294, 264, 1192, 3552, 3043, 293, 309, 311, 411, 1392, 13, 821, 311, 257, 665, 722, 586], "temperature": 0.0, "avg_logprob": -0.20749054298744546, "compression_ratio": 1.6338582677165354, "no_speech_prob": 4.565933522826526e-06}, {"id": 351, "seek": 175206, "start": 1769.1399999999999, "end": 1776.34, "text": " This took eight seconds of wall time so eight actual seconds if you put percent time it'll tell you how long things took", "tokens": [639, 1890, 3180, 3949, 295, 2929, 565, 370, 3180, 3539, 3949, 498, 291, 829, 3043, 565, 309, 603, 980, 291, 577, 938, 721, 1890], "temperature": 0.0, "avg_logprob": -0.20749054298744546, "compression_ratio": 1.6338582677165354, "no_speech_prob": 4.565933522826526e-06}, {"id": 352, "seek": 175206, "start": 1777.46, "end": 1781.26, "text": " And luckily I've got quite a few cores quite a few CPUs in this computer", "tokens": [400, 22880, 286, 600, 658, 1596, 257, 1326, 24826, 1596, 257, 1326, 13199, 82, 294, 341, 3820], "temperature": 0.0, "avg_logprob": -0.20749054298744546, "compression_ratio": 1.6338582677165354, "no_speech_prob": 4.565933522826526e-06}, {"id": 353, "seek": 178126, "start": 1781.26, "end": 1786.62, "text": " Because it actually took over a minute a compute time so it parallelized that across cores", "tokens": [1436, 309, 767, 1890, 670, 257, 3456, 257, 14722, 565, 370, 309, 8952, 1602, 300, 2108, 24826], "temperature": 0.0, "avg_logprob": -0.19256701327786588, "compression_ratio": 1.6330645161290323, "no_speech_prob": 2.225275238743052e-06}, {"id": 354, "seek": 178126, "start": 1787.3, "end": 1789.3, "text": " If your data set", "tokens": [759, 428, 1412, 992], "temperature": 0.0, "avg_logprob": -0.19256701327786588, "compression_ratio": 1.6330645161290323, "no_speech_prob": 2.225275238743052e-06}, {"id": 355, "seek": 178126, "start": 1789.62, "end": 1791.98, "text": " Was bigger or you had less cores?", "tokens": [3027, 3801, 420, 291, 632, 1570, 24826, 30], "temperature": 0.0, "avg_logprob": -0.19256701327786588, "compression_ratio": 1.6330645161290323, "no_speech_prob": 2.225275238743052e-06}, {"id": 356, "seek": 178126, "start": 1792.7, "end": 1796.9, "text": " You know you could well find that this took a few minutes to run or even a few hours", "tokens": [509, 458, 291, 727, 731, 915, 300, 341, 1890, 257, 1326, 2077, 281, 1190, 420, 754, 257, 1326, 2496], "temperature": 0.0, "avg_logprob": -0.19256701327786588, "compression_ratio": 1.6330645161290323, "no_speech_prob": 2.225275238743052e-06}, {"id": 357, "seek": 178126, "start": 1797.66, "end": 1802.1, "text": " My rule of thumb is that if something takes more than 10 seconds to run", "tokens": [1222, 4978, 295, 9298, 307, 300, 498, 746, 2516, 544, 813, 1266, 3949, 281, 1190], "temperature": 0.0, "avg_logprob": -0.19256701327786588, "compression_ratio": 1.6330645161290323, "no_speech_prob": 2.225275238743052e-06}, {"id": 358, "seek": 178126, "start": 1803.46, "end": 1807.1, "text": " It's too long for me to do like interactive", "tokens": [467, 311, 886, 938, 337, 385, 281, 360, 411, 15141], "temperature": 0.0, "avg_logprob": -0.19256701327786588, "compression_ratio": 1.6330645161290323, "no_speech_prob": 2.225275238743052e-06}, {"id": 359, "seek": 180710, "start": 1807.1, "end": 1810.8799999999999, "text": " Analysis with it right I want to be able to like run something", "tokens": [38172, 365, 309, 558, 286, 528, 281, 312, 1075, 281, 411, 1190, 746], "temperature": 0.0, "avg_logprob": -0.1574330913777254, "compression_ratio": 1.638655462184874, "no_speech_prob": 6.893592967571749e-07}, {"id": 360, "seek": 180710, "start": 1811.62, "end": 1813.62, "text": " Wait a moment and then continue", "tokens": [3802, 257, 1623, 293, 550, 2354], "temperature": 0.0, "avg_logprob": -0.1574330913777254, "compression_ratio": 1.638655462184874, "no_speech_prob": 6.893592967571749e-07}, {"id": 361, "seek": 180710, "start": 1815.1, "end": 1820.5, "text": " So what we do is we try to make sure that things can run in a reasonable time", "tokens": [407, 437, 321, 360, 307, 321, 853, 281, 652, 988, 300, 721, 393, 1190, 294, 257, 10585, 565], "temperature": 0.0, "avg_logprob": -0.1574330913777254, "compression_ratio": 1.638655462184874, "no_speech_prob": 6.893592967571749e-07}, {"id": 362, "seek": 180710, "start": 1821.5, "end": 1824.62, "text": " And then when we're when we're finished at the end of the day", "tokens": [400, 550, 562, 321, 434, 562, 321, 434, 4335, 412, 264, 917, 295, 264, 786], "temperature": 0.0, "avg_logprob": -0.1574330913777254, "compression_ratio": 1.638655462184874, "no_speech_prob": 6.893592967571749e-07}, {"id": 363, "seek": 180710, "start": 1825.1799999999998, "end": 1828.62, "text": " We can then say okay this feature engineering these hyper parameters", "tokens": [492, 393, 550, 584, 1392, 341, 4111, 7043, 613, 9848, 9834], "temperature": 0.0, "avg_logprob": -0.1574330913777254, "compression_ratio": 1.638655462184874, "no_speech_prob": 6.893592967571749e-07}, {"id": 364, "seek": 180710, "start": 1828.62, "end": 1834.12, "text": " Whatever these are all working well, and I'll now rerun it. You know this the big slow", "tokens": [8541, 613, 366, 439, 1364, 731, 11, 293, 286, 603, 586, 43819, 409, 309, 13, 509, 458, 341, 264, 955, 2964], "temperature": 0.0, "avg_logprob": -0.1574330913777254, "compression_ratio": 1.638655462184874, "no_speech_prob": 6.893592967571749e-07}, {"id": 365, "seek": 183412, "start": 1834.12, "end": 1839.76, "text": " Precise way so one way to speed things up is to pass in the subset parameter", "tokens": [6001, 66, 908, 636, 370, 472, 636, 281, 3073, 721, 493, 307, 281, 1320, 294, 264, 25993, 13075], "temperature": 0.0, "avg_logprob": -0.21625562336133874, "compression_ratio": 1.5807860262008733, "no_speech_prob": 6.681501645289245e-07}, {"id": 366, "seek": 183412, "start": 1840.36, "end": 1843.9599999999998, "text": " To proc DF and that will randomly sample", "tokens": [1407, 9510, 48336, 293, 300, 486, 16979, 6889], "temperature": 0.0, "avg_logprob": -0.21625562336133874, "compression_ratio": 1.5807860262008733, "no_speech_prob": 6.681501645289245e-07}, {"id": 367, "seek": 183412, "start": 1844.4799999999998, "end": 1851.6, "text": " My data right and so here. I'm going to randomly sample 30,000 rows now when I do that I", "tokens": [1222, 1412, 558, 293, 370, 510, 13, 286, 478, 516, 281, 16979, 6889, 2217, 11, 1360, 13241, 586, 562, 286, 360, 300, 286], "temperature": 0.0, "avg_logprob": -0.21625562336133874, "compression_ratio": 1.5807860262008733, "no_speech_prob": 6.681501645289245e-07}, {"id": 368, "seek": 183412, "start": 1853.1599999999999, "end": 1856.2399999999998, "text": " Still need to be careful to make sure that my validation set", "tokens": [8291, 643, 281, 312, 5026, 281, 652, 988, 300, 452, 24071, 992], "temperature": 0.0, "avg_logprob": -0.21625562336133874, "compression_ratio": 1.5807860262008733, "no_speech_prob": 6.681501645289245e-07}, {"id": 369, "seek": 183412, "start": 1857.0, "end": 1862.76, "text": " Doesn't change and that my training set doesn't overlap with the dates otherwise. I'm cheating", "tokens": [12955, 380, 1319, 293, 300, 452, 3097, 992, 1177, 380, 19959, 365, 264, 11691, 5911, 13, 286, 478, 18309], "temperature": 0.0, "avg_logprob": -0.21625562336133874, "compression_ratio": 1.5807860262008733, "no_speech_prob": 6.681501645289245e-07}, {"id": 370, "seek": 186276, "start": 1862.76, "end": 1867.12, "text": " So I call split valves again to again do this split by dates", "tokens": [407, 286, 818, 7472, 34950, 797, 281, 797, 360, 341, 7472, 538, 11691], "temperature": 0.0, "avg_logprob": -0.15433656676741672, "compression_ratio": 1.8478260869565217, "no_speech_prob": 1.2289152664379799e-06}, {"id": 371, "seek": 186276, "start": 1868.24, "end": 1869.52, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.15433656676741672, "compression_ratio": 1.8478260869565217, "no_speech_prob": 1.2289152664379799e-06}, {"id": 372, "seek": 186276, "start": 1869.52, "end": 1875.48, "text": " You'll also see I'm using rather than putting it into a validation set. I'm putting it into a variable called underscore", "tokens": [509, 603, 611, 536, 286, 478, 1228, 2831, 813, 3372, 309, 666, 257, 24071, 992, 13, 286, 478, 3372, 309, 666, 257, 7006, 1219, 37556], "temperature": 0.0, "avg_logprob": -0.15433656676741672, "compression_ratio": 1.8478260869565217, "no_speech_prob": 1.2289152664379799e-06}, {"id": 373, "seek": 186276, "start": 1875.48, "end": 1880.92, "text": " This is kind of a standard approach in Python is to use a variable called underscore if you want to throw something away", "tokens": [639, 307, 733, 295, 257, 3832, 3109, 294, 15329, 307, 281, 764, 257, 7006, 1219, 37556, 498, 291, 528, 281, 3507, 746, 1314], "temperature": 0.0, "avg_logprob": -0.15433656676741672, "compression_ratio": 1.8478260869565217, "no_speech_prob": 1.2289152664379799e-06}, {"id": 374, "seek": 186276, "start": 1881.4, "end": 1885.76, "text": " Because I don't want to change my validation set like no matter what different models", "tokens": [1436, 286, 500, 380, 528, 281, 1319, 452, 24071, 992, 411, 572, 1871, 437, 819, 5245], "temperature": 0.0, "avg_logprob": -0.15433656676741672, "compression_ratio": 1.8478260869565217, "no_speech_prob": 1.2289152664379799e-06}, {"id": 375, "seek": 186276, "start": 1885.76, "end": 1888.1, "text": " I build I want to be able to compare them all to each other", "tokens": [286, 1322, 286, 528, 281, 312, 1075, 281, 6794, 552, 439, 281, 1184, 661], "temperature": 0.0, "avg_logprob": -0.15433656676741672, "compression_ratio": 1.8478260869565217, "no_speech_prob": 1.2289152664379799e-06}, {"id": 376, "seek": 186276, "start": 1888.28, "end": 1890.86, "text": " So I want to keep my validation set the same all the time", "tokens": [407, 286, 528, 281, 1066, 452, 24071, 992, 264, 912, 439, 264, 565], "temperature": 0.0, "avg_logprob": -0.15433656676741672, "compression_ratio": 1.8478260869565217, "no_speech_prob": 1.2289152664379799e-06}, {"id": 377, "seek": 189086, "start": 1890.86, "end": 1895.1399999999999, "text": " okay, so all I'm doing here is I'm resampling my training set into a", "tokens": [1392, 11, 370, 439, 286, 478, 884, 510, 307, 286, 478, 725, 335, 11970, 452, 3097, 992, 666, 257], "temperature": 0.0, "avg_logprob": -0.15995010375976562, "compression_ratio": 1.566820276497696, "no_speech_prob": 8.1863481682376e-07}, {"id": 378, "seek": 189086, "start": 1896.1399999999999, "end": 1899.34, "text": " 20 the first 20,000 out of a 30,000 subset", "tokens": [945, 264, 700, 945, 11, 1360, 484, 295, 257, 2217, 11, 1360, 25993], "temperature": 0.0, "avg_logprob": -0.15995010375976562, "compression_ratio": 1.566820276497696, "no_speech_prob": 8.1863481682376e-07}, {"id": 379, "seek": 189086, "start": 1900.6999999999998, "end": 1905.58, "text": " So I now can run that and it runs in 621 milliseconds", "tokens": [407, 286, 586, 393, 1190, 300, 293, 309, 6676, 294, 1386, 4436, 34184], "temperature": 0.0, "avg_logprob": -0.15995010375976562, "compression_ratio": 1.566820276497696, "no_speech_prob": 8.1863481682376e-07}, {"id": 380, "seek": 189086, "start": 1905.58, "end": 1909.9599999999998, "text": " So I can like really zip through things now try things out, okay?", "tokens": [407, 286, 393, 411, 534, 20730, 807, 721, 586, 853, 721, 484, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.15995010375976562, "compression_ratio": 1.566820276497696, "no_speech_prob": 8.1863481682376e-07}, {"id": 381, "seek": 189086, "start": 1911.1799999999998, "end": 1912.9799999999998, "text": " So with that", "tokens": [407, 365, 300], "temperature": 0.0, "avg_logprob": -0.15995010375976562, "compression_ratio": 1.566820276497696, "no_speech_prob": 8.1863481682376e-07}, {"id": 382, "seek": 189086, "start": 1912.9799999999998, "end": 1914.9799999999998, "text": " Let's use this subset", "tokens": [961, 311, 764, 341, 25993], "temperature": 0.0, "avg_logprob": -0.15995010375976562, "compression_ratio": 1.566820276497696, "no_speech_prob": 8.1863481682376e-07}, {"id": 383, "seek": 191498, "start": 1914.98, "end": 1922.42, "text": " To build a model that is so simple that we can actually take a look at it and so we're going to build a", "tokens": [1407, 1322, 257, 2316, 300, 307, 370, 2199, 300, 321, 393, 767, 747, 257, 574, 412, 309, 293, 370, 321, 434, 516, 281, 1322, 257], "temperature": 0.0, "avg_logprob": -0.20618464283107482, "compression_ratio": 1.796116504854369, "no_speech_prob": 1.3709537824979634e-06}, {"id": 384, "seek": 191498, "start": 1922.82, "end": 1925.1, "text": " Forest is made of trees and", "tokens": [18124, 307, 1027, 295, 5852, 293], "temperature": 0.0, "avg_logprob": -0.20618464283107482, "compression_ratio": 1.796116504854369, "no_speech_prob": 1.3709537824979634e-06}, {"id": 385, "seek": 191498, "start": 1925.94, "end": 1928.26, "text": " So before we look at the forest we look at the trees", "tokens": [407, 949, 321, 574, 412, 264, 6719, 321, 574, 412, 264, 5852], "temperature": 0.0, "avg_logprob": -0.20618464283107482, "compression_ratio": 1.796116504854369, "no_speech_prob": 1.3709537824979634e-06}, {"id": 386, "seek": 191498, "start": 1929.46, "end": 1933.66, "text": " In scikit-learn they don't call them trees they call them estimators", "tokens": [682, 2180, 22681, 12, 306, 1083, 436, 500, 380, 818, 552, 5852, 436, 818, 552, 8017, 3391], "temperature": 0.0, "avg_logprob": -0.20618464283107482, "compression_ratio": 1.796116504854369, "no_speech_prob": 1.3709537824979634e-06}, {"id": 387, "seek": 191498, "start": 1933.78, "end": 1940.8600000000001, "text": " So we're going to pass in the parameter number of estimators equals one to create a forest with just one tree in and", "tokens": [407, 321, 434, 516, 281, 1320, 294, 264, 13075, 1230, 295, 8017, 3391, 6915, 472, 281, 1884, 257, 6719, 365, 445, 472, 4230, 294, 293], "temperature": 0.0, "avg_logprob": -0.20618464283107482, "compression_ratio": 1.796116504854369, "no_speech_prob": 1.3709537824979634e-06}, {"id": 388, "seek": 194086, "start": 1940.86, "end": 1946.54, "text": " Then we're going to make a small tree so we pass in maximum depth equals three and", "tokens": [1396, 321, 434, 516, 281, 652, 257, 1359, 4230, 370, 321, 1320, 294, 6674, 7161, 6915, 1045, 293], "temperature": 0.0, "avg_logprob": -0.18950567058488435, "compression_ratio": 1.7033898305084745, "no_speech_prob": 1.5779590967213153e-06}, {"id": 389, "seek": 194086, "start": 1947.1, "end": 1950.9399999999998, "text": " A random forest as we're going to learn randomizes a whole bunch of things", "tokens": [316, 4974, 6719, 382, 321, 434, 516, 281, 1466, 4974, 5660, 257, 1379, 3840, 295, 721], "temperature": 0.0, "avg_logprob": -0.18950567058488435, "compression_ratio": 1.7033898305084745, "no_speech_prob": 1.5779590967213153e-06}, {"id": 390, "seek": 194086, "start": 1951.54, "end": 1955.54, "text": " We want to turn that off so to turn that off you say bootstrap equals false", "tokens": [492, 528, 281, 1261, 300, 766, 370, 281, 1261, 300, 766, 291, 584, 11450, 372, 4007, 6915, 7908], "temperature": 0.0, "avg_logprob": -0.18950567058488435, "compression_ratio": 1.7033898305084745, "no_speech_prob": 1.5779590967213153e-06}, {"id": 391, "seek": 194086, "start": 1955.74, "end": 1959.5, "text": " So if I pass in these parameters it creates a small", "tokens": [407, 498, 286, 1320, 294, 613, 9834, 309, 7829, 257, 1359], "temperature": 0.0, "avg_logprob": -0.18950567058488435, "compression_ratio": 1.7033898305084745, "no_speech_prob": 1.5779590967213153e-06}, {"id": 392, "seek": 194086, "start": 1960.8999999999999, "end": 1962.6599999999999, "text": " deterministic tree", "tokens": [15957, 3142, 4230], "temperature": 0.0, "avg_logprob": -0.18950567058488435, "compression_ratio": 1.7033898305084745, "no_speech_prob": 1.5779590967213153e-06}, {"id": 393, "seek": 194086, "start": 1962.6599999999999, "end": 1970.2199999999998, "text": " So if I fit it and say print score my R squared has gone down from point eight five to point four", "tokens": [407, 498, 286, 3318, 309, 293, 584, 4482, 6175, 452, 497, 8889, 575, 2780, 760, 490, 935, 3180, 1732, 281, 935, 1451], "temperature": 0.0, "avg_logprob": -0.18950567058488435, "compression_ratio": 1.7033898305084745, "no_speech_prob": 1.5779590967213153e-06}, {"id": 394, "seek": 197022, "start": 1970.22, "end": 1975.9, "text": " So this is not a good model. It's better than the mean model. This is better than zero, right?", "tokens": [407, 341, 307, 406, 257, 665, 2316, 13, 467, 311, 1101, 813, 264, 914, 2316, 13, 639, 307, 1101, 813, 4018, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17324399948120117, "compression_ratio": 1.6649484536082475, "no_speech_prob": 8.851558845890395e-07}, {"id": 395, "seek": 197022, "start": 1975.9, "end": 1979.1000000000001, "text": " It's not a good model, but it's a model that we can draw", "tokens": [467, 311, 406, 257, 665, 2316, 11, 457, 309, 311, 257, 2316, 300, 321, 393, 2642], "temperature": 0.0, "avg_logprob": -0.17324399948120117, "compression_ratio": 1.6649484536082475, "no_speech_prob": 8.851558845890395e-07}, {"id": 396, "seek": 197022, "start": 1981.18, "end": 1985.8600000000001, "text": " All right, so let's learn about what it's built so a tree", "tokens": [1057, 558, 11, 370, 718, 311, 1466, 466, 437, 309, 311, 3094, 370, 257, 4230], "temperature": 0.0, "avg_logprob": -0.17324399948120117, "compression_ratio": 1.6649484536082475, "no_speech_prob": 8.851558845890395e-07}, {"id": 397, "seek": 197022, "start": 1987.74, "end": 1993.5, "text": " Consists of a sequence of binary decisions of binary splits", "tokens": [6923, 1751, 295, 257, 8310, 295, 17434, 5327, 295, 17434, 37741], "temperature": 0.0, "avg_logprob": -0.17324399948120117, "compression_ratio": 1.6649484536082475, "no_speech_prob": 8.851558845890395e-07}, {"id": 398, "seek": 197022, "start": 1994.38, "end": 1998.14, "text": " So it first of all decided to split on coupler system", "tokens": [407, 309, 700, 295, 439, 3047, 281, 7472, 322, 1384, 22732, 1185], "temperature": 0.0, "avg_logprob": -0.17324399948120117, "compression_ratio": 1.6649484536082475, "no_speech_prob": 8.851558845890395e-07}, {"id": 399, "seek": 199814, "start": 1998.14, "end": 2003.1000000000001, "text": " Greater than or less than point five that's a Boolean variable. That's actually true or false and", "tokens": [38410, 813, 420, 1570, 813, 935, 1732, 300, 311, 257, 23351, 28499, 7006, 13, 663, 311, 767, 2074, 420, 7908, 293], "temperature": 0.0, "avg_logprob": -0.21864813201281488, "compression_ratio": 1.8807339449541285, "no_speech_prob": 2.0904547000100138e-06}, {"id": 400, "seek": 199814, "start": 2003.8200000000002, "end": 2006.9, "text": " Then within the group where coupler system was true", "tokens": [1396, 1951, 264, 1594, 689, 1384, 22732, 1185, 390, 2074], "temperature": 0.0, "avg_logprob": -0.21864813201281488, "compression_ratio": 1.8807339449541285, "no_speech_prob": 2.0904547000100138e-06}, {"id": 401, "seek": 199814, "start": 2006.9, "end": 2011.0600000000002, "text": " It decided to split into year made greater than or less than 1987 and", "tokens": [467, 3047, 281, 7472, 666, 1064, 1027, 5044, 813, 420, 1570, 813, 29008, 293], "temperature": 0.0, "avg_logprob": -0.21864813201281488, "compression_ratio": 1.8807339449541285, "no_speech_prob": 2.0904547000100138e-06}, {"id": 402, "seek": 199814, "start": 2011.3000000000002, "end": 2016.22, "text": " Then where coupler system was true and year made was less than or equal to 1986", "tokens": [1396, 689, 1384, 22732, 1185, 390, 2074, 293, 1064, 1027, 390, 1570, 813, 420, 2681, 281, 27895], "temperature": 0.0, "avg_logprob": -0.21864813201281488, "compression_ratio": 1.8807339449541285, "no_speech_prob": 2.0904547000100138e-06}, {"id": 403, "seek": 199814, "start": 2016.42, "end": 2024.8600000000001, "text": " It used fi product class desk is less than or equal to point seven five and so forth right so right at the top", "tokens": [467, 1143, 15848, 1674, 1508, 10026, 307, 1570, 813, 420, 2681, 281, 935, 3407, 1732, 293, 370, 5220, 558, 370, 558, 412, 264, 1192], "temperature": 0.0, "avg_logprob": -0.21864813201281488, "compression_ratio": 1.8807339449541285, "no_speech_prob": 2.0904547000100138e-06}, {"id": 404, "seek": 202486, "start": 2024.86, "end": 2027.9799999999998, "text": " We have 20,000 samples", "tokens": [492, 362, 945, 11, 1360, 10938], "temperature": 0.0, "avg_logprob": -0.21039004074899773, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.8162143078370718e-06}, {"id": 405, "seek": 202486, "start": 2028.78, "end": 2036.62, "text": " 20,000 rows right and the reason for that is because that's what we asked for here when we split our data in the sample", "tokens": [945, 11, 1360, 13241, 558, 293, 264, 1778, 337, 300, 307, 570, 300, 311, 437, 321, 2351, 337, 510, 562, 321, 7472, 527, 1412, 294, 264, 6889], "temperature": 0.0, "avg_logprob": -0.21039004074899773, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.8162143078370718e-06}, {"id": 406, "seek": 202486, "start": 2042.3799999999999, "end": 2044.86, "text": " I just want to double check that for your", "tokens": [286, 445, 528, 281, 3834, 1520, 300, 337, 428], "temperature": 0.0, "avg_logprob": -0.21039004074899773, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.8162143078370718e-06}, {"id": 407, "seek": 202486, "start": 2045.4599999999998, "end": 2051.74, "text": " Decision tree that you have there that the coloration was whether it's true or false not so like it gets darker", "tokens": [12427, 1991, 4230, 300, 291, 362, 456, 300, 264, 2017, 399, 390, 1968, 309, 311, 2074, 420, 7908, 406, 370, 411, 309, 2170, 12741], "temperature": 0.0, "avg_logprob": -0.21039004074899773, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.8162143078370718e-06}, {"id": 408, "seek": 205174, "start": 2051.74, "end": 2058.22, "text": " It's true for the next one not the darker is a higher value. We'll get to that in a moment, okay?", "tokens": [467, 311, 2074, 337, 264, 958, 472, 406, 264, 12741, 307, 257, 2946, 2158, 13, 492, 603, 483, 281, 300, 294, 257, 1623, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1905846772370515, "compression_ratio": 1.5870445344129556, "no_speech_prob": 2.203324100946702e-07}, {"id": 409, "seek": 205174, "start": 2058.58, "end": 2063.18, "text": " So let's look at these numbers here, so in the whole data set well our", "tokens": [407, 718, 311, 574, 412, 613, 3547, 510, 11, 370, 294, 264, 1379, 1412, 992, 731, 527], "temperature": 0.0, "avg_logprob": -0.1905846772370515, "compression_ratio": 1.5870445344129556, "no_speech_prob": 2.203324100946702e-07}, {"id": 410, "seek": 205174, "start": 2064.1, "end": 2066.4199999999996, "text": " Sample that we're using there are 20,000 rows", "tokens": [4832, 781, 300, 321, 434, 1228, 456, 366, 945, 11, 1360, 13241], "temperature": 0.0, "avg_logprob": -0.1905846772370515, "compression_ratio": 1.5870445344129556, "no_speech_prob": 2.203324100946702e-07}, {"id": 411, "seek": 205174, "start": 2067.3799999999997, "end": 2072.18, "text": " the me at the average of the log of price is 10.1 and", "tokens": [264, 385, 412, 264, 4274, 295, 264, 3565, 295, 3218, 307, 1266, 13, 16, 293], "temperature": 0.0, "avg_logprob": -0.1905846772370515, "compression_ratio": 1.5870445344129556, "no_speech_prob": 2.203324100946702e-07}, {"id": 412, "seek": 205174, "start": 2072.74, "end": 2074.74, "text": " If we built a model", "tokens": [759, 321, 3094, 257, 2316], "temperature": 0.0, "avg_logprob": -0.1905846772370515, "compression_ratio": 1.5870445344129556, "no_speech_prob": 2.203324100946702e-07}, {"id": 413, "seek": 207474, "start": 2074.74, "end": 2082.2599999999998, "text": " Where we just use that average all the time then the mean squared error would be point four seven seven okay?", "tokens": [2305, 321, 445, 764, 300, 4274, 439, 264, 565, 550, 264, 914, 8889, 6713, 576, 312, 935, 1451, 3407, 3407, 1392, 30], "temperature": 0.0, "avg_logprob": -0.23740239765333093, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.0511462278373074e-06}, {"id": 414, "seek": 207474, "start": 2082.7, "end": 2085.3399999999997, "text": " So this is in other words the", "tokens": [407, 341, 307, 294, 661, 2283, 264], "temperature": 0.0, "avg_logprob": -0.23740239765333093, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.0511462278373074e-06}, {"id": 415, "seek": 207474, "start": 2086.06, "end": 2088.06, "text": " denominator of an R squared", "tokens": [20687, 295, 364, 497, 8889], "temperature": 0.0, "avg_logprob": -0.23740239765333093, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.0511462278373074e-06}, {"id": 416, "seek": 207474, "start": 2088.62, "end": 2094.7, "text": " All right, this is like the most basic model is a tree with zero splits right which is just predict the average", "tokens": [1057, 558, 11, 341, 307, 411, 264, 881, 3875, 2316, 307, 257, 4230, 365, 4018, 37741, 558, 597, 307, 445, 6069, 264, 4274], "temperature": 0.0, "avg_logprob": -0.23740239765333093, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.0511462278373074e-06}, {"id": 417, "seek": 207474, "start": 2095.54, "end": 2097.1, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.23740239765333093, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.0511462278373074e-06}, {"id": 418, "seek": 207474, "start": 2097.1, "end": 2100.24, "text": " the best single binary split we can make", "tokens": [264, 1151, 2167, 17434, 7472, 321, 393, 652], "temperature": 0.0, "avg_logprob": -0.23740239765333093, "compression_ratio": 1.632034632034632, "no_speech_prob": 1.0511462278373074e-06}, {"id": 419, "seek": 210024, "start": 2100.24, "end": 2104.3599999999997, "text": " Turns out to be splitting by where the coupler system is", "tokens": [29524, 484, 281, 312, 30348, 538, 689, 264, 1384, 22732, 1185, 307], "temperature": 0.0, "avg_logprob": -0.18788841443184096, "compression_ratio": 2.0041152263374484, "no_speech_prob": 1.9444304655280575e-07}, {"id": 420, "seek": 210024, "start": 2104.8799999999997, "end": 2109.16, "text": " Greater than or equal to sorry less than or equal to or greater than point five in other words whether it's true or false", "tokens": [38410, 813, 420, 2681, 281, 2597, 1570, 813, 420, 2681, 281, 420, 5044, 813, 935, 1732, 294, 661, 2283, 1968, 309, 311, 2074, 420, 7908], "temperature": 0.0, "avg_logprob": -0.18788841443184096, "compression_ratio": 2.0041152263374484, "no_speech_prob": 1.9444304655280575e-07}, {"id": 421, "seek": 210024, "start": 2109.72, "end": 2116.52, "text": " And it turns out if we do that the mean squared error of coupler system is less than point five so it's false", "tokens": [400, 309, 4523, 484, 498, 321, 360, 300, 264, 914, 8889, 6713, 295, 1384, 22732, 1185, 307, 1570, 813, 935, 1732, 370, 309, 311, 7908], "temperature": 0.0, "avg_logprob": -0.18788841443184096, "compression_ratio": 2.0041152263374484, "no_speech_prob": 1.9444304655280575e-07}, {"id": 422, "seek": 210024, "start": 2117.24, "end": 2124.0, "text": " Goes down from point four seven seven to point one one right so it's really improved the error a lot", "tokens": [44471, 760, 490, 935, 1451, 3407, 3407, 281, 935, 472, 472, 558, 370, 309, 311, 534, 9689, 264, 6713, 257, 688], "temperature": 0.0, "avg_logprob": -0.18788841443184096, "compression_ratio": 2.0041152263374484, "no_speech_prob": 1.9444304655280575e-07}, {"id": 423, "seek": 212400, "start": 2124.0, "end": 2129.68, "text": " In the other group it's only improved it a bit. It's gone from point four seven to point four one and", "tokens": [682, 264, 661, 1594, 309, 311, 787, 9689, 309, 257, 857, 13, 467, 311, 2780, 490, 935, 1451, 3407, 281, 935, 1451, 472, 293], "temperature": 0.0, "avg_logprob": -0.1967514439633018, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.5209310428617755e-07}, {"id": 424, "seek": 212400, "start": 2130.96, "end": 2137.22, "text": " So we can see that the coupler system equals false group has a pretty small percentage", "tokens": [407, 321, 393, 536, 300, 264, 1384, 22732, 1185, 6915, 7908, 1594, 575, 257, 1238, 1359, 9668], "temperature": 0.0, "avg_logprob": -0.1967514439633018, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.5209310428617755e-07}, {"id": 425, "seek": 212400, "start": 2137.22, "end": 2139.86, "text": " It's only got twenty two hundred of the twenty thousand", "tokens": [467, 311, 787, 658, 7699, 732, 3262, 295, 264, 7699, 4714], "temperature": 0.0, "avg_logprob": -0.1967514439633018, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.5209310428617755e-07}, {"id": 426, "seek": 212400, "start": 2141.08, "end": 2146.34, "text": " Right where else this other group has a much large percentage, but it hasn't improved it as much", "tokens": [1779, 689, 1646, 341, 661, 1594, 575, 257, 709, 2416, 9668, 11, 457, 309, 6132, 380, 9689, 309, 382, 709], "temperature": 0.0, "avg_logprob": -0.1967514439633018, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.5209310428617755e-07}, {"id": 427, "seek": 212400, "start": 2148.0, "end": 2150.0, "text": " So let's say", "tokens": [407, 718, 311, 584], "temperature": 0.0, "avg_logprob": -0.1967514439633018, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.5209310428617755e-07}, {"id": 428, "seek": 212400, "start": 2150.04, "end": 2152.04, "text": " You wanted to create a", "tokens": [509, 1415, 281, 1884, 257], "temperature": 0.0, "avg_logprob": -0.1967514439633018, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.5209310428617755e-07}, {"id": 429, "seek": 215204, "start": 2152.04, "end": 2156.72, "text": " Tree with just one split so you're just trying to find like what is", "tokens": [22291, 365, 445, 472, 7472, 370, 291, 434, 445, 1382, 281, 915, 411, 437, 307], "temperature": 0.0, "avg_logprob": -0.24204955782209123, "compression_ratio": 1.5125628140703518, "no_speech_prob": 2.4060918804025277e-06}, {"id": 430, "seek": 215204, "start": 2157.44, "end": 2159.44, "text": " the very best", "tokens": [264, 588, 1151], "temperature": 0.0, "avg_logprob": -0.24204955782209123, "compression_ratio": 1.5125628140703518, "no_speech_prob": 2.4060918804025277e-06}, {"id": 431, "seek": 215204, "start": 2159.88, "end": 2162.5, "text": " Single binary decision you can make", "tokens": [31248, 17434, 3537, 291, 393, 652], "temperature": 0.0, "avg_logprob": -0.24204955782209123, "compression_ratio": 1.5125628140703518, "no_speech_prob": 2.4060918804025277e-06}, {"id": 432, "seek": 215204, "start": 2163.32, "end": 2167.2599999999998, "text": " For your data how might you be able to do that? How could you do it?", "tokens": [1171, 428, 1412, 577, 1062, 291, 312, 1075, 281, 360, 300, 30, 1012, 727, 291, 360, 309, 30], "temperature": 0.0, "avg_logprob": -0.24204955782209123, "compression_ratio": 1.5125628140703518, "no_speech_prob": 2.4060918804025277e-06}, {"id": 433, "seek": 215204, "start": 2170.6, "end": 2172.6, "text": " You're gonna give it to fort", "tokens": [509, 434, 799, 976, 309, 281, 5009], "temperature": 0.0, "avg_logprob": -0.24204955782209123, "compression_ratio": 1.5125628140703518, "no_speech_prob": 2.4060918804025277e-06}, {"id": 434, "seek": 215204, "start": 2174.12, "end": 2178.92, "text": " Specify the max depth of one, but I mean you're writing you don't have a random first", "tokens": [20484, 2505, 264, 11469, 7161, 295, 472, 11, 457, 286, 914, 291, 434, 3579, 291, 500, 380, 362, 257, 4974, 700], "temperature": 0.0, "avg_logprob": -0.24204955782209123, "compression_ratio": 1.5125628140703518, "no_speech_prob": 2.4060918804025277e-06}, {"id": 435, "seek": 217892, "start": 2178.92, "end": 2185.48, "text": " Right how are you gonna? How are you gonna like write? What's an algorithm a simple algorithm which you could use?", "tokens": [1779, 577, 366, 291, 799, 30, 1012, 366, 291, 799, 411, 2464, 30, 708, 311, 364, 9284, 257, 2199, 9284, 597, 291, 727, 764, 30], "temperature": 0.0, "avg_logprob": -0.18083145797893566, "compression_ratio": 1.8059701492537314, "no_speech_prob": 2.947989059975953e-06}, {"id": 436, "seek": 217892, "start": 2186.4, "end": 2188.08, "text": " sure", "tokens": [988], "temperature": 0.0, "avg_logprob": -0.18083145797893566, "compression_ratio": 1.8059701492537314, "no_speech_prob": 2.947989059975953e-06}, {"id": 437, "seek": 217892, "start": 2188.08, "end": 2190.84, "text": " So we want to start building a random forest from scratch", "tokens": [407, 321, 528, 281, 722, 2390, 257, 4974, 6719, 490, 8459], "temperature": 0.0, "avg_logprob": -0.18083145797893566, "compression_ratio": 1.8059701492537314, "no_speech_prob": 2.947989059975953e-06}, {"id": 438, "seek": 217892, "start": 2191.64, "end": 2198.52, "text": " So the first step is to create a tree the first step to create a tree is to create the first binary decision", "tokens": [407, 264, 700, 1823, 307, 281, 1884, 257, 4230, 264, 700, 1823, 281, 1884, 257, 4230, 307, 281, 1884, 264, 700, 17434, 3537], "temperature": 0.0, "avg_logprob": -0.18083145797893566, "compression_ratio": 1.8059701492537314, "no_speech_prob": 2.947989059975953e-06}, {"id": 439, "seek": 217892, "start": 2199.4, "end": 2201.64, "text": " How are you going to do it? I'm going to give it to Chris", "tokens": [1012, 366, 291, 516, 281, 360, 309, 30, 286, 478, 516, 281, 976, 309, 281, 6688], "temperature": 0.0, "avg_logprob": -0.18083145797893566, "compression_ratio": 1.8059701492537314, "no_speech_prob": 2.947989059975953e-06}, {"id": 440, "seek": 220164, "start": 2201.64, "end": 2208.04, "text": " Maybe in two steps or good", "tokens": [2704, 294, 732, 4439, 420, 665], "temperature": 0.0, "avg_logprob": -0.28001512421502006, "compression_ratio": 1.5578947368421052, "no_speech_prob": 2.144433710782323e-05}, {"id": 441, "seek": 220164, "start": 2208.04, "end": 2215.0, "text": " So isn't this simply trying to find the best predictor based on maybe a linear regression?", "tokens": [407, 1943, 380, 341, 2935, 1382, 281, 915, 264, 1151, 6069, 284, 2361, 322, 1310, 257, 8213, 24590, 30], "temperature": 0.0, "avg_logprob": -0.28001512421502006, "compression_ratio": 1.5578947368421052, "no_speech_prob": 2.144433710782323e-05}, {"id": 442, "seek": 220164, "start": 2216.2799999999997, "end": 2219.48, "text": " You could use a linear regression, but could you do something?", "tokens": [509, 727, 764, 257, 8213, 24590, 11, 457, 727, 291, 360, 746, 30], "temperature": 0.0, "avg_logprob": -0.28001512421502006, "compression_ratio": 1.5578947368421052, "no_speech_prob": 2.144433710782323e-05}, {"id": 443, "seek": 220164, "start": 2220.8399999999997, "end": 2228.3599999999997, "text": " Much simpler and more complete we're trying not to use any statistical assumptions here. I can't see your name, sir", "tokens": [12313, 18587, 293, 544, 3566, 321, 434, 1382, 406, 281, 764, 604, 22820, 17695, 510, 13, 286, 393, 380, 536, 428, 1315, 11, 4735], "temperature": 0.0, "avg_logprob": -0.28001512421502006, "compression_ratio": 1.5578947368421052, "no_speech_prob": 2.144433710782323e-05}, {"id": 444, "seek": 222836, "start": 2228.36, "end": 2231.88, "text": " It's of course your prints I need that", "tokens": [467, 311, 295, 1164, 428, 22305, 286, 643, 300], "temperature": 0.0, "avg_logprob": -0.26745491936093285, "compression_ratio": 1.7112299465240641, "no_speech_prob": 5.014635917177657e-06}, {"id": 445, "seek": 222836, "start": 2233.56, "end": 2239.04, "text": " Can we just do like take just one variable if it is true give it like", "tokens": [1664, 321, 445, 360, 411, 747, 445, 472, 7006, 498, 309, 307, 2074, 976, 309, 411], "temperature": 0.0, "avg_logprob": -0.26745491936093285, "compression_ratio": 1.7112299465240641, "no_speech_prob": 5.014635917177657e-06}, {"id": 446, "seek": 222836, "start": 2240.08, "end": 2242.08, "text": " The true thing and if it is false", "tokens": [440, 2074, 551, 293, 498, 309, 307, 7908], "temperature": 0.0, "avg_logprob": -0.26745491936093285, "compression_ratio": 1.7112299465240641, "no_speech_prob": 5.014635917177657e-06}, {"id": 447, "seek": 222836, "start": 2243.28, "end": 2248.08, "text": " so which variable are we going to choose so at each binary point we have to choose a variable and", "tokens": [370, 597, 7006, 366, 321, 516, 281, 2826, 370, 412, 1184, 17434, 935, 321, 362, 281, 2826, 257, 7006, 293], "temperature": 0.0, "avg_logprob": -0.26745491936093285, "compression_ratio": 1.7112299465240641, "no_speech_prob": 5.014635917177657e-06}, {"id": 448, "seek": 222836, "start": 2249.8, "end": 2252.52, "text": " Something to split on how are we going to do that?", "tokens": [6595, 281, 7472, 322, 577, 366, 321, 516, 281, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.26745491936093285, "compression_ratio": 1.7112299465240641, "no_speech_prob": 5.014635917177657e-06}, {"id": 449, "seek": 225252, "start": 2252.52, "end": 2256.68, "text": " And pass it over there how do I pronounce your name?", "tokens": [400, 1320, 309, 670, 456, 577, 360, 286, 19567, 428, 1315, 30], "temperature": 0.0, "avg_logprob": -0.25838593875660615, "compression_ratio": 1.5671641791044777, "no_speech_prob": 7.026720413705334e-05}, {"id": 450, "seek": 225252, "start": 2259.88, "end": 2265.5, "text": " So the variable to choose could be like which divides population into two groups", "tokens": [407, 264, 7006, 281, 2826, 727, 312, 411, 597, 41347, 4415, 666, 732, 3935], "temperature": 0.0, "avg_logprob": -0.25838593875660615, "compression_ratio": 1.5671641791044777, "no_speech_prob": 7.026720413705334e-05}, {"id": 451, "seek": 225252, "start": 2266.12, "end": 2268.64, "text": " which are kind of heterogeneous to each other and", "tokens": [597, 366, 733, 295, 20789, 31112, 281, 1184, 661, 293], "temperature": 0.0, "avg_logprob": -0.25838593875660615, "compression_ratio": 1.5671641791044777, "no_speech_prob": 7.026720413705334e-05}, {"id": 452, "seek": 225252, "start": 2269.56, "end": 2276.6, "text": " Homogeneous within themselves like having the same quality within themselves and they're very different could you be more specific?", "tokens": [20903, 31112, 1951, 2969, 411, 1419, 264, 912, 3125, 1951, 2969, 293, 436, 434, 588, 819, 727, 291, 312, 544, 2685, 30], "temperature": 0.0, "avg_logprob": -0.25838593875660615, "compression_ratio": 1.5671641791044777, "no_speech_prob": 7.026720413705334e-05}, {"id": 453, "seek": 227660, "start": 2276.6, "end": 2281.2799999999997, "text": " Like in terms of the target variable maybe yes, right?", "tokens": [1743, 294, 2115, 295, 264, 3779, 7006, 1310, 2086, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.269429934922085, "compression_ratio": 1.634453781512605, "no_speech_prob": 3.844895672955317e-06}, {"id": 454, "seek": 227660, "start": 2281.64, "end": 2288.12, "text": " Let's say we have two groups after split so one has a different price altogether from the second group", "tokens": [961, 311, 584, 321, 362, 732, 3935, 934, 7472, 370, 472, 575, 257, 819, 3218, 19051, 490, 264, 1150, 1594], "temperature": 0.0, "avg_logprob": -0.269429934922085, "compression_ratio": 1.634453781512605, "no_speech_prob": 3.844895672955317e-06}, {"id": 455, "seek": 227660, "start": 2288.12, "end": 2291.2999999999997, "text": " Yes, internally they have simpler prices. Okay, that's good", "tokens": [1079, 11, 19501, 436, 362, 18587, 7901, 13, 1033, 11, 300, 311, 665], "temperature": 0.0, "avg_logprob": -0.269429934922085, "compression_ratio": 1.634453781512605, "no_speech_prob": 3.844895672955317e-06}, {"id": 456, "seek": 227660, "start": 2291.44, "end": 2297.48, "text": " So like to simplify things a little bit where we're saying find a variable that we could split into", "tokens": [407, 411, 281, 20460, 721, 257, 707, 857, 689, 321, 434, 1566, 915, 257, 7006, 300, 321, 727, 7472, 666], "temperature": 0.0, "avg_logprob": -0.269429934922085, "compression_ratio": 1.634453781512605, "no_speech_prob": 3.844895672955317e-06}, {"id": 457, "seek": 227660, "start": 2298.12, "end": 2302.12, "text": " Such that the two groups are as different to each other as possible", "tokens": [9653, 300, 264, 732, 3935, 366, 382, 819, 281, 1184, 661, 382, 1944], "temperature": 0.0, "avg_logprob": -0.269429934922085, "compression_ratio": 1.634453781512605, "no_speech_prob": 3.844895672955317e-06}, {"id": 458, "seek": 227660, "start": 2303.48, "end": 2305.04, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.269429934922085, "compression_ratio": 1.634453781512605, "no_speech_prob": 3.844895672955317e-06}, {"id": 459, "seek": 230504, "start": 2305.04, "end": 2309.8, "text": " Okay, how do you how would you pick which variable and which split point? That's the question?", "tokens": [1033, 11, 577, 360, 291, 577, 576, 291, 1888, 597, 7006, 293, 597, 7472, 935, 30, 663, 311, 264, 1168, 30], "temperature": 0.0, "avg_logprob": -0.1754580446191736, "compression_ratio": 1.575268817204301, "no_speech_prob": 5.338059509085724e-06}, {"id": 460, "seek": 230504, "start": 2317.52, "end": 2321.48, "text": " Yeah, what's your first cut which variable and which split point?", "tokens": [865, 11, 437, 311, 428, 700, 1723, 597, 7006, 293, 597, 7472, 935, 30], "temperature": 0.0, "avg_logprob": -0.1754580446191736, "compression_ratio": 1.575268817204301, "no_speech_prob": 5.338059509085724e-06}, {"id": 461, "seek": 230504, "start": 2326.6, "end": 2330.72, "text": " We don't like we're making a tree from scratch we want to create our own tree", "tokens": [492, 500, 380, 411, 321, 434, 1455, 257, 4230, 490, 8459, 321, 528, 281, 1884, 527, 1065, 4230], "temperature": 0.0, "avg_logprob": -0.1754580446191736, "compression_ratio": 1.575268817204301, "no_speech_prob": 5.338059509085724e-06}, {"id": 462, "seek": 233072, "start": 2330.72, "end": 2335.48, "text": " That makes sense. We've got somebody over here mostly a positive", "tokens": [663, 1669, 2020, 13, 492, 600, 658, 2618, 670, 510, 5240, 257, 3353], "temperature": 0.0, "avg_logprob": -0.25840331934675387, "compression_ratio": 1.59375, "no_speech_prob": 5.507494279299863e-06}, {"id": 463, "seek": 233072, "start": 2343.52, "end": 2350.08, "text": " Can we test all of the possible split and see which one has a small and RMSE and that sounds good", "tokens": [1664, 321, 1500, 439, 295, 264, 1944, 7472, 293, 536, 597, 472, 575, 257, 1359, 293, 23790, 5879, 293, 300, 3263, 665], "temperature": 0.0, "avg_logprob": -0.25840331934675387, "compression_ratio": 1.59375, "no_speech_prob": 5.507494279299863e-06}, {"id": 464, "seek": 233072, "start": 2350.08, "end": 2355.0, "text": " Okay, so let's dig into this. So when you say test all of the possible splits", "tokens": [1033, 11, 370, 718, 311, 2528, 666, 341, 13, 407, 562, 291, 584, 1500, 439, 295, 264, 1944, 37741], "temperature": 0.0, "avg_logprob": -0.25840331934675387, "compression_ratio": 1.59375, "no_speech_prob": 5.507494279299863e-06}, {"id": 465, "seek": 235500, "start": 2355.0, "end": 2360.48, "text": " What does that mean? How do we enumerate all the possible splits? Oh?", "tokens": [708, 775, 300, 914, 30, 1012, 360, 321, 465, 15583, 473, 439, 264, 1944, 37741, 30, 876, 30], "temperature": 0.0, "avg_logprob": -0.25123704804314506, "compression_ratio": 1.583815028901734, "no_speech_prob": 6.854244020360056e-06}, {"id": 466, "seek": 235500, "start": 2363.72, "end": 2365.72, "text": " I think of that but", "tokens": [286, 519, 295, 300, 457], "temperature": 0.0, "avg_logprob": -0.25123704804314506, "compression_ratio": 1.583815028901734, "no_speech_prob": 6.854244020360056e-06}, {"id": 467, "seek": 235500, "start": 2369.48, "end": 2373.6, "text": " For each variable you could put one aside", "tokens": [1171, 1184, 7006, 291, 727, 829, 472, 7359], "temperature": 0.0, "avg_logprob": -0.25123704804314506, "compression_ratio": 1.583815028901734, "no_speech_prob": 6.854244020360056e-06}, {"id": 468, "seek": 235500, "start": 2374.92, "end": 2378.48, "text": " And then put a second aside and compare the two and if it was better", "tokens": [400, 550, 829, 257, 1150, 7359, 293, 6794, 264, 732, 293, 498, 309, 390, 1101], "temperature": 0.0, "avg_logprob": -0.25123704804314506, "compression_ratio": 1.583815028901734, "no_speech_prob": 6.854244020360056e-06}, {"id": 469, "seek": 237848, "start": 2378.48, "end": 2384.56, "text": " Good okay, so for each variable for each possible value of that variable", "tokens": [2205, 1392, 11, 370, 337, 1184, 7006, 337, 1184, 1944, 2158, 295, 300, 7006], "temperature": 0.0, "avg_logprob": -0.1952841305038304, "compression_ratio": 1.6837606837606838, "no_speech_prob": 4.710871053248411e-06}, {"id": 470, "seek": 237848, "start": 2385.64, "end": 2387.64, "text": " See whether it's better", "tokens": [3008, 1968, 309, 311, 1101], "temperature": 0.0, "avg_logprob": -0.1952841305038304, "compression_ratio": 1.6837606837606838, "no_speech_prob": 4.710871053248411e-06}, {"id": 471, "seek": 237848, "start": 2387.68, "end": 2392.64, "text": " Now give it back to Maisley, so I want to dig into the better when you said see if the RMSE is better", "tokens": [823, 976, 309, 646, 281, 6313, 3420, 11, 370, 286, 528, 281, 2528, 666, 264, 1101, 562, 291, 848, 536, 498, 264, 23790, 5879, 307, 1101], "temperature": 0.0, "avg_logprob": -0.1952841305038304, "compression_ratio": 1.6837606837606838, "no_speech_prob": 4.710871053248411e-06}, {"id": 472, "seek": 237848, "start": 2393.44, "end": 2396.48, "text": " What does that mean though because after a split you've got two?", "tokens": [708, 775, 300, 914, 1673, 570, 934, 257, 7472, 291, 600, 658, 732, 30], "temperature": 0.0, "avg_logprob": -0.1952841305038304, "compression_ratio": 1.6837606837606838, "no_speech_prob": 4.710871053248411e-06}, {"id": 473, "seek": 237848, "start": 2397.16, "end": 2399.68, "text": " RMSEs you've got two two groups", "tokens": [23790, 5879, 82, 291, 600, 658, 732, 732, 3935], "temperature": 0.0, "avg_logprob": -0.1952841305038304, "compression_ratio": 1.6837606837606838, "no_speech_prob": 4.710871053248411e-06}, {"id": 474, "seek": 237848, "start": 2400.52, "end": 2407.3, "text": " So you're just gonna fit with that one variable comparing to the others not so so what I mean here", "tokens": [407, 291, 434, 445, 799, 3318, 365, 300, 472, 7006, 15763, 281, 264, 2357, 406, 370, 370, 437, 286, 914, 510], "temperature": 0.0, "avg_logprob": -0.1952841305038304, "compression_ratio": 1.6837606837606838, "no_speech_prob": 4.710871053248411e-06}, {"id": 475, "seek": 240730, "start": 2407.3, "end": 2409.96, "text": " Is that before we decided to spit on coupler system?", "tokens": [1119, 300, 949, 321, 3047, 281, 22127, 322, 1384, 22732, 1185, 30], "temperature": 0.0, "avg_logprob": -0.2151549865152234, "compression_ratio": 1.741444866920152, "no_speech_prob": 1.4144709439278813e-06}, {"id": 476, "seek": 240730, "start": 2410.0, "end": 2415.1200000000003, "text": " Yeah, we had a root the mean squared of point four seven seven and after we've got two groups", "tokens": [865, 11, 321, 632, 257, 5593, 264, 914, 8889, 295, 935, 1451, 3407, 3407, 293, 934, 321, 600, 658, 732, 3935], "temperature": 0.0, "avg_logprob": -0.2151549865152234, "compression_ratio": 1.741444866920152, "no_speech_prob": 1.4144709439278813e-06}, {"id": 477, "seek": 240730, "start": 2415.44, "end": 2419.7000000000003, "text": " One with a mean squared error of point one another with a mean squared error of point four", "tokens": [1485, 365, 257, 914, 8889, 6713, 295, 935, 472, 1071, 365, 257, 914, 8889, 6713, 295, 935, 1451], "temperature": 0.0, "avg_logprob": -0.2151549865152234, "compression_ratio": 1.741444866920152, "no_speech_prob": 1.4144709439278813e-06}, {"id": 478, "seek": 240730, "start": 2420.76, "end": 2428.84, "text": " So you treat each individual model separately so for the first play you're just going to compare between each variable themselves", "tokens": [407, 291, 2387, 1184, 2609, 2316, 14759, 370, 337, 264, 700, 862, 291, 434, 445, 516, 281, 6794, 1296, 1184, 7006, 2969], "temperature": 0.0, "avg_logprob": -0.2151549865152234, "compression_ratio": 1.741444866920152, "no_speech_prob": 1.4144709439278813e-06}, {"id": 479, "seek": 240730, "start": 2428.84, "end": 2433.3, "text": " And then you move on to the next note with the remaining, but but even the first node", "tokens": [400, 550, 291, 1286, 322, 281, 264, 958, 3637, 365, 264, 8877, 11, 457, 457, 754, 264, 700, 9984], "temperature": 0.0, "avg_logprob": -0.2151549865152234, "compression_ratio": 1.741444866920152, "no_speech_prob": 1.4144709439278813e-06}, {"id": 480, "seek": 240730, "start": 2434.0800000000004, "end": 2435.38, "text": " like", "tokens": [411], "temperature": 0.0, "avg_logprob": -0.2151549865152234, "compression_ratio": 1.741444866920152, "no_speech_prob": 1.4144709439278813e-06}, {"id": 481, "seek": 243538, "start": 2435.38, "end": 2439.6400000000003, "text": " So the model with zero splits has a single root mean squared error", "tokens": [407, 264, 2316, 365, 4018, 37741, 575, 257, 2167, 5593, 914, 8889, 6713], "temperature": 0.0, "avg_logprob": -0.19830123237941577, "compression_ratio": 1.7630331753554502, "no_speech_prob": 2.225269099653815e-06}, {"id": 482, "seek": 243538, "start": 2440.12, "end": 2445.32, "text": " The model with one split so the very first thing we try we've now got two groups", "tokens": [440, 2316, 365, 472, 7472, 370, 264, 588, 700, 551, 321, 853, 321, 600, 586, 658, 732, 3935], "temperature": 0.0, "avg_logprob": -0.19830123237941577, "compression_ratio": 1.7630331753554502, "no_speech_prob": 2.225269099653815e-06}, {"id": 483, "seek": 243538, "start": 2445.92, "end": 2447.92, "text": " with two mean squared errors", "tokens": [365, 732, 914, 8889, 13603], "temperature": 0.0, "avg_logprob": -0.19830123237941577, "compression_ratio": 1.7630331753554502, "no_speech_prob": 2.225269099653815e-06}, {"id": 484, "seek": 243538, "start": 2448.08, "end": 2450.08, "text": " You want to give it to Daniel?", "tokens": [509, 528, 281, 976, 309, 281, 8033, 30], "temperature": 0.0, "avg_logprob": -0.19830123237941577, "compression_ratio": 1.7630331753554502, "no_speech_prob": 2.225269099653815e-06}, {"id": 485, "seek": 243538, "start": 2451.6400000000003, "end": 2457.28, "text": " Do you pick the one that gets them as different as they can be well which well okay, that would be one idea", "tokens": [1144, 291, 1888, 264, 472, 300, 2170, 552, 382, 819, 382, 436, 393, 312, 731, 597, 731, 1392, 11, 300, 576, 312, 472, 1558], "temperature": 0.0, "avg_logprob": -0.19830123237941577, "compression_ratio": 1.7630331753554502, "no_speech_prob": 2.225269099653815e-06}, {"id": 486, "seek": 243538, "start": 2457.8, "end": 2461.56, "text": " Get the two mean squared errors as different as possible", "tokens": [3240, 264, 732, 914, 8889, 13603, 382, 819, 382, 1944], "temperature": 0.0, "avg_logprob": -0.19830123237941577, "compression_ratio": 1.7630331753554502, "no_speech_prob": 2.225269099653815e-06}, {"id": 487, "seek": 246156, "start": 2461.56, "end": 2465.64, "text": " But why might that not work? What might be a problem with that?", "tokens": [583, 983, 1062, 300, 406, 589, 30, 708, 1062, 312, 257, 1154, 365, 300, 30], "temperature": 0.0, "avg_logprob": -0.18483065667553483, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.9333549516886706e-06}, {"id": 488, "seek": 246156, "start": 2467.04, "end": 2468.32, "text": " sample size", "tokens": [6889, 2744], "temperature": 0.0, "avg_logprob": -0.18483065667553483, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.9333549516886706e-06}, {"id": 489, "seek": 246156, "start": 2468.32, "end": 2471.56, "text": " Go on because you could just literally leave one point out", "tokens": [1037, 322, 570, 291, 727, 445, 3736, 1856, 472, 935, 484], "temperature": 0.0, "avg_logprob": -0.18483065667553483, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.9333549516886706e-06}, {"id": 490, "seek": 246156, "start": 2471.56, "end": 2474.6, "text": " Yeah, so we could have like year made is less than", "tokens": [865, 11, 370, 321, 727, 362, 411, 1064, 1027, 307, 1570, 813], "temperature": 0.0, "avg_logprob": -0.18483065667553483, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.9333549516886706e-06}, {"id": 491, "seek": 246156, "start": 2475.24, "end": 2481.5, "text": " 1950 and it might have a single sample with a low price and like that's not a great split", "tokens": [18141, 293, 309, 1062, 362, 257, 2167, 6889, 365, 257, 2295, 3218, 293, 411, 300, 311, 406, 257, 869, 7472], "temperature": 0.0, "avg_logprob": -0.18483065667553483, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.9333549516886706e-06}, {"id": 492, "seek": 246156, "start": 2481.6, "end": 2486.34, "text": " Is it you know because the other group is actually not going to be very interesting at all?", "tokens": [1119, 309, 291, 458, 570, 264, 661, 1594, 307, 767, 406, 516, 281, 312, 588, 1880, 412, 439, 30], "temperature": 0.0, "avg_logprob": -0.18483065667553483, "compression_ratio": 1.627906976744186, "no_speech_prob": 1.9333549516886706e-06}, {"id": 493, "seek": 248634, "start": 2486.34, "end": 2490.42, "text": " Can you improve it a bit can Jason improve it a bit?", "tokens": [1664, 291, 3470, 309, 257, 857, 393, 11181, 3470, 309, 257, 857, 30], "temperature": 0.0, "avg_logprob": -0.21225908223320455, "compression_ratio": 1.6358381502890174, "no_speech_prob": 1.2679261089942884e-06}, {"id": 494, "seek": 248634, "start": 2492.94, "end": 2495.58, "text": " Could you take a weighted average", "tokens": [7497, 291, 747, 257, 32807, 4274], "temperature": 0.0, "avg_logprob": -0.21225908223320455, "compression_ratio": 1.6358381502890174, "no_speech_prob": 1.2679261089942884e-06}, {"id": 495, "seek": 248634, "start": 2496.5, "end": 2503.1400000000003, "text": " Yeah, a weighted average so we could take point four one times seventeen thousand plus point one times two thousand", "tokens": [865, 11, 257, 32807, 4274, 370, 321, 727, 747, 935, 1451, 472, 1413, 39532, 4714, 1804, 935, 472, 1413, 732, 4714], "temperature": 0.0, "avg_logprob": -0.21225908223320455, "compression_ratio": 1.6358381502890174, "no_speech_prob": 1.2679261089942884e-06}, {"id": 496, "seek": 248634, "start": 2503.46, "end": 2509.7000000000003, "text": " That's good right and that would be the same as actually saying I've got a model", "tokens": [663, 311, 665, 558, 293, 300, 576, 312, 264, 912, 382, 767, 1566, 286, 600, 658, 257, 2316], "temperature": 0.0, "avg_logprob": -0.21225908223320455, "compression_ratio": 1.6358381502890174, "no_speech_prob": 1.2679261089942884e-06}, {"id": 497, "seek": 250970, "start": 2509.7, "end": 2517.52, "text": " The model is a single binary decision, and I'm going to say for everybody with year made less than 986.5. I'm going to fill in", "tokens": [440, 2316, 307, 257, 2167, 17434, 3537, 11, 293, 286, 478, 516, 281, 584, 337, 2201, 365, 1064, 1027, 1570, 813, 20860, 21, 13, 20, 13, 286, 478, 516, 281, 2836, 294], "temperature": 0.0, "avg_logprob": -0.17790516603340223, "compression_ratio": 1.7171314741035857, "no_speech_prob": 6.893598083479446e-07}, {"id": 498, "seek": 250970, "start": 2518.06, "end": 2525.7, "text": " Point ten point two for everybody else are going to fill in nine point two and then I'm going to calculate the root mean squared error of this", "tokens": [12387, 2064, 935, 732, 337, 2201, 1646, 366, 516, 281, 2836, 294, 4949, 935, 732, 293, 550, 286, 478, 516, 281, 8873, 264, 5593, 914, 8889, 6713, 295, 341], "temperature": 0.0, "avg_logprob": -0.17790516603340223, "compression_ratio": 1.7171314741035857, "no_speech_prob": 6.893598083479446e-07}, {"id": 499, "seek": 250970, "start": 2526.8599999999997, "end": 2532.2599999999998, "text": " Crappy model and that would give exactly the same right as the weighted average that you're suggesting", "tokens": [11138, 7966, 2316, 293, 300, 576, 976, 2293, 264, 912, 558, 382, 264, 32807, 4274, 300, 291, 434, 18094], "temperature": 0.0, "avg_logprob": -0.17790516603340223, "compression_ratio": 1.7171314741035857, "no_speech_prob": 6.893598083479446e-07}, {"id": 500, "seek": 250970, "start": 2532.74, "end": 2538.22, "text": " Okay, good, so we now have a single number that represents", "tokens": [1033, 11, 665, 11, 370, 321, 586, 362, 257, 2167, 1230, 300, 8855], "temperature": 0.0, "avg_logprob": -0.17790516603340223, "compression_ratio": 1.7171314741035857, "no_speech_prob": 6.893598083479446e-07}, {"id": 501, "seek": 253822, "start": 2538.22, "end": 2543.3999999999996, "text": " How good a split is which is the weighted average of the mean squared errors of the two groups it creates?", "tokens": [1012, 665, 257, 7472, 307, 597, 307, 264, 32807, 4274, 295, 264, 914, 8889, 13603, 295, 264, 732, 3935, 309, 7829, 30], "temperature": 0.0, "avg_logprob": -0.20230745801738664, "compression_ratio": 1.7657657657657657, "no_speech_prob": 5.173882982489886e-06}, {"id": 502, "seek": 253822, "start": 2543.98, "end": 2545.7799999999997, "text": " Okay, and", "tokens": [1033, 11, 293], "temperature": 0.0, "avg_logprob": -0.20230745801738664, "compression_ratio": 1.7657657657657657, "no_speech_prob": 5.173882982489886e-06}, {"id": 503, "seek": 253822, "start": 2545.7799999999997, "end": 2550.2999999999997, "text": " Thanks to I think it was was it Jake we have a way to find the best split", "tokens": [2561, 281, 286, 519, 309, 390, 390, 309, 15822, 321, 362, 257, 636, 281, 915, 264, 1151, 7472], "temperature": 0.0, "avg_logprob": -0.20230745801738664, "compression_ratio": 1.7657657657657657, "no_speech_prob": 5.173882982489886e-06}, {"id": 504, "seek": 253822, "start": 2550.2999999999997, "end": 2556.54, "text": " Which is to try every variable and to try every possible value of that variable and see which variable and which value", "tokens": [3013, 307, 281, 853, 633, 7006, 293, 281, 853, 633, 1944, 2158, 295, 300, 7006, 293, 536, 597, 7006, 293, 597, 2158], "temperature": 0.0, "avg_logprob": -0.20230745801738664, "compression_ratio": 1.7657657657657657, "no_speech_prob": 5.173882982489886e-06}, {"id": 505, "seek": 253822, "start": 2556.8199999999997, "end": 2559.06, "text": " Gives us a split with the best score", "tokens": [460, 1539, 505, 257, 7472, 365, 264, 1151, 6175], "temperature": 0.0, "avg_logprob": -0.20230745801738664, "compression_ratio": 1.7657657657657657, "no_speech_prob": 5.173882982489886e-06}, {"id": 506, "seek": 253822, "start": 2560.58, "end": 2562.58, "text": " That makes sense", "tokens": [663, 1669, 2020], "temperature": 0.0, "avg_logprob": -0.20230745801738664, "compression_ratio": 1.7657657657657657, "no_speech_prob": 5.173882982489886e-06}, {"id": 507, "seek": 253822, "start": 2562.8599999999997, "end": 2564.8599999999997, "text": " Okay, what's your name, sir?", "tokens": [1033, 11, 437, 311, 428, 1315, 11, 4735, 30], "temperature": 0.0, "avg_logprob": -0.20230745801738664, "compression_ratio": 1.7657657657657657, "no_speech_prob": 5.173882982489886e-06}, {"id": 508, "seek": 256486, "start": 2564.86, "end": 2571.3, "text": " Okay, can somebody give Natalie the box", "tokens": [1033, 11, 393, 2618, 976, 29574, 264, 2424], "temperature": 0.0, "avg_logprob": -0.30059027204326555, "compression_ratio": 1.434782608695652, "no_speech_prob": 9.080158633878455e-06}, {"id": 509, "seek": 256486, "start": 2578.94, "end": 2582.82, "text": " When you see every possible number for every possible variable like", "tokens": [1133, 291, 536, 633, 1944, 1230, 337, 633, 1944, 7006, 411], "temperature": 0.0, "avg_logprob": -0.30059027204326555, "compression_ratio": 1.434782608695652, "no_speech_prob": 9.080158633878455e-06}, {"id": 510, "seek": 256486, "start": 2583.5, "end": 2587.86, "text": " Are you saying like here? We have point five as like our?", "tokens": [2014, 291, 1566, 411, 510, 30, 492, 362, 935, 1732, 382, 411, 527, 30], "temperature": 0.0, "avg_logprob": -0.30059027204326555, "compression_ratio": 1.434782608695652, "no_speech_prob": 9.080158633878455e-06}, {"id": 511, "seek": 256486, "start": 2588.7000000000003, "end": 2590.7000000000003, "text": " criteria to split", "tokens": [11101, 281, 7472], "temperature": 0.0, "avg_logprob": -0.30059027204326555, "compression_ratio": 1.434782608695652, "no_speech_prob": 9.080158633878455e-06}, {"id": 512, "seek": 256486, "start": 2590.7000000000003, "end": 2592.7000000000003, "text": " the tree sorry", "tokens": [264, 4230, 2597], "temperature": 0.0, "avg_logprob": -0.30059027204326555, "compression_ratio": 1.434782608695652, "no_speech_prob": 9.080158633878455e-06}, {"id": 513, "seek": 259270, "start": 2592.7, "end": 2596.2999999999997, "text": " Are you saying we're trying out every single number for?", "tokens": [2014, 291, 1566, 321, 434, 1382, 484, 633, 2167, 1230, 337, 30], "temperature": 0.0, "avg_logprob": -0.23136327724264125, "compression_ratio": 1.6419753086419753, "no_speech_prob": 8.139556484820787e-06}, {"id": 514, "seek": 259270, "start": 2597.22, "end": 2602.98, "text": " Every possible value right so coupler system only has two values true and false", "tokens": [2048, 1944, 2158, 558, 370, 1384, 22732, 1185, 787, 575, 732, 4190, 2074, 293, 7908], "temperature": 0.0, "avg_logprob": -0.23136327724264125, "compression_ratio": 1.6419753086419753, "no_speech_prob": 8.139556484820787e-06}, {"id": 515, "seek": 259270, "start": 2603.2599999999998, "end": 2606.7799999999997, "text": " So there's only one way of splitting which is trues and falses", "tokens": [407, 456, 311, 787, 472, 636, 295, 30348, 597, 307, 504, 1247, 293, 16720, 279], "temperature": 0.0, "avg_logprob": -0.23136327724264125, "compression_ratio": 1.6419753086419753, "no_speech_prob": 8.139556484820787e-06}, {"id": 516, "seek": 259270, "start": 2607.2999999999997, "end": 2614.54, "text": " EMA is an integer which varies between like I don't know 1960 and 2010 so we can just say what are all the possible unique values?", "tokens": [462, 9998, 307, 364, 24922, 597, 21716, 1296, 411, 286, 500, 380, 458, 16157, 293, 9657, 370, 321, 393, 445, 584, 437, 366, 439, 264, 1944, 3845, 4190, 30], "temperature": 0.0, "avg_logprob": -0.23136327724264125, "compression_ratio": 1.6419753086419753, "no_speech_prob": 8.139556484820787e-06}, {"id": 517, "seek": 259270, "start": 2614.54, "end": 2618.46, "text": " If you're made and and try them all so we're trying all the possible", "tokens": [759, 291, 434, 1027, 293, 293, 853, 552, 439, 370, 321, 434, 1382, 439, 264, 1944], "temperature": 0.0, "avg_logprob": -0.23136327724264125, "compression_ratio": 1.6419753086419753, "no_speech_prob": 8.139556484820787e-06}, {"id": 518, "seek": 261846, "start": 2618.46, "end": 2622.46, "text": " Spec points can you pass that back to Daniel or pass it to me and I'll pass it to Daniel", "tokens": [20484, 2793, 393, 291, 1320, 300, 646, 281, 8033, 420, 1320, 309, 281, 385, 293, 286, 603, 1320, 309, 281, 8033], "temperature": 0.0, "avg_logprob": -0.2569126422588642, "compression_ratio": 1.4850299401197604, "no_speech_prob": 1.147849798144307e-05}, {"id": 519, "seek": 261846, "start": 2636.02, "end": 2639.82, "text": " So I just want to clarify again for the first split", "tokens": [407, 286, 445, 528, 281, 17594, 797, 337, 264, 700, 7472], "temperature": 0.0, "avg_logprob": -0.2569126422588642, "compression_ratio": 1.4850299401197604, "no_speech_prob": 1.147849798144307e-05}, {"id": 520, "seek": 261846, "start": 2640.7, "end": 2641.82, "text": " Why?", "tokens": [1545, 30], "temperature": 0.0, "avg_logprob": -0.2569126422588642, "compression_ratio": 1.4850299401197604, "no_speech_prob": 1.147849798144307e-05}, {"id": 521, "seek": 264182, "start": 2641.82, "end": 2650.42, "text": " Did we split on coupler system true or false because what we did was we used Jake's technique we tried every variable", "tokens": [2589, 321, 7472, 322, 1384, 22732, 1185, 2074, 420, 7908, 570, 437, 321, 630, 390, 321, 1143, 15822, 311, 6532, 321, 3031, 633, 7006], "temperature": 0.0, "avg_logprob": -0.1503791312376658, "compression_ratio": 1.8391304347826087, "no_speech_prob": 2.406076191618922e-06}, {"id": 522, "seek": 264182, "start": 2650.86, "end": 2656.1800000000003, "text": " For every variable we tried every possible split for each one we noted down", "tokens": [1171, 633, 7006, 321, 3031, 633, 1944, 7472, 337, 1184, 472, 321, 12964, 760], "temperature": 0.0, "avg_logprob": -0.1503791312376658, "compression_ratio": 1.8391304347826087, "no_speech_prob": 2.406076191618922e-06}, {"id": 523, "seek": 264182, "start": 2656.1800000000003, "end": 2662.6200000000003, "text": " I think it was Jason's idea which was the weighted average mean squared error of the two groups are created", "tokens": [286, 519, 309, 390, 11181, 311, 1558, 597, 390, 264, 32807, 4274, 914, 8889, 6713, 295, 264, 732, 3935, 366, 2942], "temperature": 0.0, "avg_logprob": -0.1503791312376658, "compression_ratio": 1.8391304347826087, "no_speech_prob": 2.406076191618922e-06}, {"id": 524, "seek": 264182, "start": 2663.1000000000004, "end": 2665.1000000000004, "text": " We found which one had the best", "tokens": [492, 1352, 597, 472, 632, 264, 1151], "temperature": 0.0, "avg_logprob": -0.1503791312376658, "compression_ratio": 1.8391304347826087, "no_speech_prob": 2.406076191618922e-06}, {"id": 525, "seek": 266510, "start": 2665.1, "end": 2671.46, "text": " Mean squared error and we picked it and it turned out it was coupler system true or false", "tokens": [12302, 8889, 6713, 293, 321, 6183, 309, 293, 309, 3574, 484, 309, 390, 1384, 22732, 1185, 2074, 420, 7908], "temperature": 0.0, "avg_logprob": -0.24987496029246936, "compression_ratio": 1.6367713004484306, "no_speech_prob": 2.156807113351533e-06}, {"id": 526, "seek": 266510, "start": 2672.38, "end": 2675.94, "text": " Does that make sense I guess my question is", "tokens": [4402, 300, 652, 2020, 286, 2041, 452, 1168, 307], "temperature": 0.0, "avg_logprob": -0.24987496029246936, "compression_ratio": 1.6367713004484306, "no_speech_prob": 2.156807113351533e-06}, {"id": 527, "seek": 266510, "start": 2676.7799999999997, "end": 2680.7599999999998, "text": " more like so coupler system is like one of the like", "tokens": [544, 411, 370, 1384, 22732, 1185, 307, 411, 472, 295, 264, 411], "temperature": 0.0, "avg_logprob": -0.24987496029246936, "compression_ratio": 1.6367713004484306, "no_speech_prob": 2.156807113351533e-06}, {"id": 528, "seek": 266510, "start": 2681.58, "end": 2687.8199999999997, "text": " Best indicators I guess it's the best okay. We tried every variable and every possible level", "tokens": [9752, 22176, 286, 2041, 309, 311, 264, 1151, 1392, 13, 492, 3031, 633, 7006, 293, 633, 1944, 1496], "temperature": 0.0, "avg_logprob": -0.24987496029246936, "compression_ratio": 1.6367713004484306, "no_speech_prob": 2.156807113351533e-06}, {"id": 529, "seek": 268782, "start": 2687.82, "end": 2692.78, "text": " Level after that it gets less and less everything else it tried wasn't as good", "tokens": [16872, 934, 300, 309, 2170, 1570, 293, 1570, 1203, 1646, 309, 3031, 2067, 380, 382, 665], "temperature": 0.0, "avg_logprob": -0.15962143136997414, "compression_ratio": 1.7918367346938775, "no_speech_prob": 2.4439877961413004e-06}, {"id": 530, "seek": 268782, "start": 2692.78, "end": 2698.1800000000003, "text": " Okay, and then you do that each time you split right so now that we've done that we now take this group here", "tokens": [1033, 11, 293, 550, 291, 360, 300, 1184, 565, 291, 7472, 558, 370, 586, 300, 321, 600, 1096, 300, 321, 586, 747, 341, 1594, 510], "temperature": 0.0, "avg_logprob": -0.15962143136997414, "compression_ratio": 1.7918367346938775, "no_speech_prob": 2.4439877961413004e-06}, {"id": 531, "seek": 268782, "start": 2698.54, "end": 2706.06, "text": " Everybody who's got coupler system equals true and we do it again for every possible variable for every possible level", "tokens": [7646, 567, 311, 658, 1384, 22732, 1185, 6915, 2074, 293, 321, 360, 309, 797, 337, 633, 1944, 7006, 337, 633, 1944, 1496], "temperature": 0.0, "avg_logprob": -0.15962143136997414, "compression_ratio": 1.7918367346938775, "no_speech_prob": 2.4439877961413004e-06}, {"id": 532, "seek": 268782, "start": 2706.3, "end": 2714.7000000000003, "text": " For people where coupler system equals true. What's the best possible split and then are there circumstances when it's not just like", "tokens": [1171, 561, 689, 1384, 22732, 1185, 6915, 2074, 13, 708, 311, 264, 1151, 1944, 7472, 293, 550, 366, 456, 9121, 562, 309, 311, 406, 445, 411], "temperature": 0.0, "avg_logprob": -0.15962143136997414, "compression_ratio": 1.7918367346938775, "no_speech_prob": 2.4439877961413004e-06}, {"id": 533, "seek": 271470, "start": 2714.7, "end": 2719.7, "text": " Binary like you split it into like three groups for like example you're made", "tokens": [363, 4066, 411, 291, 7472, 309, 666, 411, 1045, 3935, 337, 411, 1365, 291, 434, 1027], "temperature": 0.0, "avg_logprob": -0.20924797878470472, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.4593707646781695e-06}, {"id": 534, "seek": 271470, "start": 2719.8599999999997, "end": 2723.7, "text": " So I'm gonna make a claim and then I'm gonna see if you can justify it", "tokens": [407, 286, 478, 799, 652, 257, 3932, 293, 550, 286, 478, 799, 536, 498, 291, 393, 20833, 309], "temperature": 0.0, "avg_logprob": -0.20924797878470472, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.4593707646781695e-06}, {"id": 535, "seek": 271470, "start": 2723.7, "end": 2730.02, "text": " I'm gonna claim that it's never necessary to do more than one split at a level", "tokens": [286, 478, 799, 3932, 300, 309, 311, 1128, 4818, 281, 360, 544, 813, 472, 7472, 412, 257, 1496], "temperature": 0.0, "avg_logprob": -0.20924797878470472, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.4593707646781695e-06}, {"id": 536, "seek": 271470, "start": 2730.18, "end": 2737.46, "text": " Because you can just let it again because you can just split it again exactly so you can get exactly the same result by splitting twice", "tokens": [1436, 291, 393, 445, 718, 309, 797, 570, 291, 393, 445, 7472, 309, 797, 2293, 370, 291, 393, 483, 2293, 264, 912, 1874, 538, 30348, 6091], "temperature": 0.0, "avg_logprob": -0.20924797878470472, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.4593707646781695e-06}, {"id": 537, "seek": 271470, "start": 2739.3799999999997, "end": 2741.3799999999997, "text": " Okay good so", "tokens": [1033, 665, 370], "temperature": 0.0, "avg_logprob": -0.20924797878470472, "compression_ratio": 1.7857142857142858, "no_speech_prob": 1.4593707646781695e-06}, {"id": 538, "seek": 274138, "start": 2741.38, "end": 2746.1, "text": " That is the entirety of creating a decision tree", "tokens": [663, 307, 264, 31557, 295, 4084, 257, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.1751729859246148, "compression_ratio": 1.7671232876712328, "no_speech_prob": 6.375535690494871e-07}, {"id": 539, "seek": 274138, "start": 2746.7400000000002, "end": 2755.1400000000003, "text": " You stop either when you hit some limit that was requested so we had a limit where we said max depth equals 3", "tokens": [509, 1590, 2139, 562, 291, 2045, 512, 4948, 300, 390, 16436, 370, 321, 632, 257, 4948, 689, 321, 848, 11469, 7161, 6915, 805], "temperature": 0.0, "avg_logprob": -0.1751729859246148, "compression_ratio": 1.7671232876712328, "no_speech_prob": 6.375535690494871e-07}, {"id": 540, "seek": 274138, "start": 2755.5, "end": 2762.62, "text": " So that's one one way to stop would be you asked to stop at some point and so we stopped otherwise you stop when your", "tokens": [407, 300, 311, 472, 472, 636, 281, 1590, 576, 312, 291, 2351, 281, 1590, 412, 512, 935, 293, 370, 321, 5936, 5911, 291, 1590, 562, 428], "temperature": 0.0, "avg_logprob": -0.1751729859246148, "compression_ratio": 1.7671232876712328, "no_speech_prob": 6.375535690494871e-07}, {"id": 541, "seek": 274138, "start": 2763.5, "end": 2768.6800000000003, "text": " Your leaf nodes these things at the end are called leaf nodes when your leaf nodes only have one thing in them", "tokens": [2260, 10871, 13891, 613, 721, 412, 264, 917, 366, 1219, 10871, 13891, 562, 428, 10871, 13891, 787, 362, 472, 551, 294, 552], "temperature": 0.0, "avg_logprob": -0.1751729859246148, "compression_ratio": 1.7671232876712328, "no_speech_prob": 6.375535690494871e-07}, {"id": 542, "seek": 276868, "start": 2768.68, "end": 2771.7999999999997, "text": " Okay, that's a decision tree", "tokens": [1033, 11, 300, 311, 257, 3537, 4230], "temperature": 0.0, "avg_logprob": -0.21075565960942483, "compression_ratio": 1.6483050847457628, "no_speech_prob": 4.181183612672612e-07}, {"id": 543, "seek": 276868, "start": 2771.96, "end": 2778.96, "text": " That is how we grow a decision tree and this decision tree is not very good because it's got a validation R squared of 0.4", "tokens": [663, 307, 577, 321, 1852, 257, 3537, 4230, 293, 341, 3537, 4230, 307, 406, 588, 665, 570, 309, 311, 658, 257, 24071, 497, 8889, 295, 1958, 13, 19], "temperature": 0.0, "avg_logprob": -0.21075565960942483, "compression_ratio": 1.6483050847457628, "no_speech_prob": 4.181183612672612e-07}, {"id": 544, "seek": 276868, "start": 2780.3199999999997, "end": 2784.44, "text": " So we could try to make it better by removing max depth equals 3", "tokens": [407, 321, 727, 853, 281, 652, 309, 1101, 538, 12720, 11469, 7161, 6915, 805], "temperature": 0.0, "avg_logprob": -0.21075565960942483, "compression_ratio": 1.6483050847457628, "no_speech_prob": 4.181183612672612e-07}, {"id": 545, "seek": 276868, "start": 2785.24, "end": 2788.74, "text": " Right and creating a deeper tree, so it's going to go all the way down", "tokens": [1779, 293, 4084, 257, 7731, 4230, 11, 370, 309, 311, 516, 281, 352, 439, 264, 636, 760], "temperature": 0.0, "avg_logprob": -0.21075565960942483, "compression_ratio": 1.6483050847457628, "no_speech_prob": 4.181183612672612e-07}, {"id": 546, "seek": 276868, "start": 2788.74, "end": 2793.8799999999997, "text": " We're going to keep splitting these things further until every leaf node only has one thing in it and", "tokens": [492, 434, 516, 281, 1066, 30348, 613, 721, 3052, 1826, 633, 10871, 9984, 787, 575, 472, 551, 294, 309, 293], "temperature": 0.0, "avg_logprob": -0.21075565960942483, "compression_ratio": 1.6483050847457628, "no_speech_prob": 4.181183612672612e-07}, {"id": 547, "seek": 279388, "start": 2793.88, "end": 2799.2000000000003, "text": " If we do that the training R squared is of course", "tokens": [759, 321, 360, 300, 264, 3097, 497, 8889, 307, 295, 1164], "temperature": 0.0, "avg_logprob": -0.1727297028829885, "compression_ratio": 1.5804878048780489, "no_speech_prob": 1.1015935115210596e-06}, {"id": 548, "seek": 279388, "start": 2800.0, "end": 2801.32, "text": " one", "tokens": [472], "temperature": 0.0, "avg_logprob": -0.1727297028829885, "compression_ratio": 1.5804878048780489, "no_speech_prob": 1.1015935115210596e-06}, {"id": 549, "seek": 279388, "start": 2801.32, "end": 2806.08, "text": " Because we can exactly predict every training element because it's in a leaf node all on its own", "tokens": [1436, 321, 393, 2293, 6069, 633, 3097, 4478, 570, 309, 311, 294, 257, 10871, 9984, 439, 322, 1080, 1065], "temperature": 0.0, "avg_logprob": -0.1727297028829885, "compression_ratio": 1.5804878048780489, "no_speech_prob": 1.1015935115210596e-06}, {"id": 550, "seek": 279388, "start": 2807.6800000000003, "end": 2814.2000000000003, "text": " But the validation R squared is not one. It's actually better than our really really shallow tree", "tokens": [583, 264, 24071, 497, 8889, 307, 406, 472, 13, 467, 311, 767, 1101, 813, 527, 534, 534, 20488, 4230], "temperature": 0.0, "avg_logprob": -0.1727297028829885, "compression_ratio": 1.5804878048780489, "no_speech_prob": 1.1015935115210596e-06}, {"id": 551, "seek": 279388, "start": 2814.76, "end": 2816.76, "text": " But it's not as good as we'd like", "tokens": [583, 309, 311, 406, 382, 665, 382, 321, 1116, 411], "temperature": 0.0, "avg_logprob": -0.1727297028829885, "compression_ratio": 1.5804878048780489, "no_speech_prob": 1.1015935115210596e-06}, {"id": 552, "seek": 279388, "start": 2817.88, "end": 2819.12, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.1727297028829885, "compression_ratio": 1.5804878048780489, "no_speech_prob": 1.1015935115210596e-06}, {"id": 553, "seek": 279388, "start": 2819.12, "end": 2821.56, "text": " So we want to find some other way of", "tokens": [407, 321, 528, 281, 915, 512, 661, 636, 295], "temperature": 0.0, "avg_logprob": -0.1727297028829885, "compression_ratio": 1.5804878048780489, "no_speech_prob": 1.1015935115210596e-06}, {"id": 554, "seek": 282156, "start": 2821.56, "end": 2823.56, "text": " of making these trees", "tokens": [295, 1455, 613, 5852], "temperature": 0.0, "avg_logprob": -0.21419977634511095, "compression_ratio": 1.6652360515021458, "no_speech_prob": 2.857308345483034e-06}, {"id": 555, "seek": 282156, "start": 2824.08, "end": 2826.08, "text": " better and", "tokens": [1101, 293], "temperature": 0.0, "avg_logprob": -0.21419977634511095, "compression_ratio": 1.6652360515021458, "no_speech_prob": 2.857308345483034e-06}, {"id": 556, "seek": 282156, "start": 2826.04, "end": 2829.7599999999998, "text": " The way we're going to do it is to create a forest", "tokens": [440, 636, 321, 434, 516, 281, 360, 309, 307, 281, 1884, 257, 6719], "temperature": 0.0, "avg_logprob": -0.21419977634511095, "compression_ratio": 1.6652360515021458, "no_speech_prob": 2.857308345483034e-06}, {"id": 557, "seek": 282156, "start": 2830.6, "end": 2837.04, "text": " So what's a forest to create a forest? We're going to use a statistical technique called bagging and", "tokens": [407, 437, 311, 257, 6719, 281, 1884, 257, 6719, 30, 492, 434, 516, 281, 764, 257, 22820, 6532, 1219, 3411, 3249, 293], "temperature": 0.0, "avg_logprob": -0.21419977634511095, "compression_ratio": 1.6652360515021458, "no_speech_prob": 2.857308345483034e-06}, {"id": 558, "seek": 282156, "start": 2838.08, "end": 2840.0, "text": " You can bag", "tokens": [509, 393, 3411], "temperature": 0.0, "avg_logprob": -0.21419977634511095, "compression_ratio": 1.6652360515021458, "no_speech_prob": 2.857308345483034e-06}, {"id": 559, "seek": 282156, "start": 2840.0, "end": 2847.36, "text": " Any kind of model in fact Michael Jordan who is one of the speakers at the recent Data Institute conference here at University of San?", "tokens": [2639, 733, 295, 2316, 294, 1186, 5116, 10979, 567, 307, 472, 295, 264, 9518, 412, 264, 5162, 11888, 9446, 7586, 510, 412, 3535, 295, 5271, 30], "temperature": 0.0, "avg_logprob": -0.21419977634511095, "compression_ratio": 1.6652360515021458, "no_speech_prob": 2.857308345483034e-06}, {"id": 560, "seek": 284736, "start": 2847.36, "end": 2851.28, "text": " Francisco developed a technique called the bag of little", "tokens": [12279, 4743, 257, 6532, 1219, 264, 3411, 295, 707], "temperature": 0.0, "avg_logprob": -0.18743794304983957, "compression_ratio": 1.5753424657534247, "no_speech_prob": 3.0415799301408697e-06}, {"id": 561, "seek": 284736, "start": 2852.2000000000003, "end": 2854.2000000000003, "text": " bootstraps and", "tokens": [11450, 19639, 1878, 293], "temperature": 0.0, "avg_logprob": -0.18743794304983957, "compression_ratio": 1.5753424657534247, "no_speech_prob": 3.0415799301408697e-06}, {"id": 562, "seek": 284736, "start": 2854.44, "end": 2862.1600000000003, "text": " Which he shows how to use bagging for absolutely any kind of model to make it more robust and also to give you confidence", "tokens": [3013, 415, 3110, 577, 281, 764, 3411, 3249, 337, 3122, 604, 733, 295, 2316, 281, 652, 309, 544, 13956, 293, 611, 281, 976, 291, 6687], "temperature": 0.0, "avg_logprob": -0.18743794304983957, "compression_ratio": 1.5753424657534247, "no_speech_prob": 3.0415799301408697e-06}, {"id": 563, "seek": 284736, "start": 2862.32, "end": 2863.92, "text": " intervals", "tokens": [26651], "temperature": 0.0, "avg_logprob": -0.18743794304983957, "compression_ratio": 1.5753424657534247, "no_speech_prob": 3.0415799301408697e-06}, {"id": 564, "seek": 284736, "start": 2863.92, "end": 2869.84, "text": " The random forest is simply a way of bagging trees, so what is bagging?", "tokens": [440, 4974, 6719, 307, 2935, 257, 636, 295, 3411, 3249, 5852, 11, 370, 437, 307, 3411, 3249, 30], "temperature": 0.0, "avg_logprob": -0.18743794304983957, "compression_ratio": 1.5753424657534247, "no_speech_prob": 3.0415799301408697e-06}, {"id": 565, "seek": 286984, "start": 2869.84, "end": 2878.2400000000002, "text": " Bagging is a really interesting idea, which is what if we created five different models?", "tokens": [24377, 3249, 307, 257, 534, 1880, 1558, 11, 597, 307, 437, 498, 321, 2942, 1732, 819, 5245, 30], "temperature": 0.0, "avg_logprob": -0.18336579039856626, "compression_ratio": 1.8458149779735682, "no_speech_prob": 2.40608028434508e-06}, {"id": 566, "seek": 286984, "start": 2879.08, "end": 2885.94, "text": " Each of which was only somewhat predictive, but the models weren't at all correlated with each other they gave", "tokens": [6947, 295, 597, 390, 787, 8344, 35521, 11, 457, 264, 5245, 4999, 380, 412, 439, 38574, 365, 1184, 661, 436, 2729], "temperature": 0.0, "avg_logprob": -0.18336579039856626, "compression_ratio": 1.8458149779735682, "no_speech_prob": 2.40608028434508e-06}, {"id": 567, "seek": 286984, "start": 2886.7200000000003, "end": 2891.76, "text": " Predictions that weren't correlated with each other that would mean that the five models would have to have found", "tokens": [32969, 15607, 300, 4999, 380, 38574, 365, 1184, 661, 300, 576, 914, 300, 264, 1732, 5245, 576, 362, 281, 362, 1352], "temperature": 0.0, "avg_logprob": -0.18336579039856626, "compression_ratio": 1.8458149779735682, "no_speech_prob": 2.40608028434508e-06}, {"id": 568, "seek": 286984, "start": 2892.6800000000003, "end": 2895.56, "text": " different insights into the relationships in the data", "tokens": [819, 14310, 666, 264, 6159, 294, 264, 1412], "temperature": 0.0, "avg_logprob": -0.18336579039856626, "compression_ratio": 1.8458149779735682, "no_speech_prob": 2.40608028434508e-06}, {"id": 569, "seek": 286984, "start": 2896.36, "end": 2899.0, "text": " And so if you took the average of those five models", "tokens": [400, 370, 498, 291, 1890, 264, 4274, 295, 729, 1732, 5245], "temperature": 0.0, "avg_logprob": -0.18336579039856626, "compression_ratio": 1.8458149779735682, "no_speech_prob": 2.40608028434508e-06}, {"id": 570, "seek": 289900, "start": 2899.0, "end": 2900.48, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.2548035044728974, "compression_ratio": 1.57, "no_speech_prob": 6.893593536005937e-07}, {"id": 571, "seek": 289900, "start": 2900.48, "end": 2903.96, "text": " then you're effectively bringing in the insights from each of them and", "tokens": [550, 291, 434, 8659, 5062, 294, 264, 14310, 490, 1184, 295, 552, 293], "temperature": 0.0, "avg_logprob": -0.2548035044728974, "compression_ratio": 1.57, "no_speech_prob": 6.893593536005937e-07}, {"id": 572, "seek": 289900, "start": 2905.04, "end": 2913.56, "text": " So this idea of averaging models is a is is a technique for on the sampling right which is really important", "tokens": [407, 341, 1558, 295, 47308, 5245, 307, 257, 307, 307, 257, 6532, 337, 322, 264, 21179, 558, 597, 307, 534, 1021], "temperature": 0.0, "avg_logprob": -0.2548035044728974, "compression_ratio": 1.57, "no_speech_prob": 6.893593536005937e-07}, {"id": 573, "seek": 289900, "start": 2915.32, "end": 2919.08, "text": " Now let's come up with a more specific idea of how to do this on sampling", "tokens": [823, 718, 311, 808, 493, 365, 257, 544, 2685, 1558, 295, 577, 281, 360, 341, 322, 21179], "temperature": 0.0, "avg_logprob": -0.2548035044728974, "compression_ratio": 1.57, "no_speech_prob": 6.893593536005937e-07}, {"id": 574, "seek": 289900, "start": 2919.76, "end": 2922.84, "text": " What if we created a whole lot of?", "tokens": [708, 498, 321, 2942, 257, 1379, 688, 295, 30], "temperature": 0.0, "avg_logprob": -0.2548035044728974, "compression_ratio": 1.57, "no_speech_prob": 6.893593536005937e-07}, {"id": 575, "seek": 289900, "start": 2923.52, "end": 2925.2, "text": " these trees", "tokens": [613, 5852], "temperature": 0.0, "avg_logprob": -0.2548035044728974, "compression_ratio": 1.57, "no_speech_prob": 6.893593536005937e-07}, {"id": 576, "seek": 289900, "start": 2925.2, "end": 2927.2, "text": " big deep", "tokens": [955, 2452], "temperature": 0.0, "avg_logprob": -0.2548035044728974, "compression_ratio": 1.57, "no_speech_prob": 6.893593536005937e-07}, {"id": 577, "seek": 292720, "start": 2927.2, "end": 2933.7599999999998, "text": " Massively over fit trees right but each one. Let's say we only pick a random", "tokens": [10482, 3413, 670, 3318, 5852, 558, 457, 1184, 472, 13, 961, 311, 584, 321, 787, 1888, 257, 4974], "temperature": 0.0, "avg_logprob": -0.24108164170209098, "compression_ratio": 1.57, "no_speech_prob": 2.332067424504203e-06}, {"id": 578, "seek": 292720, "start": 2934.52, "end": 2940.6, "text": " 1-10th of the data so we pick one out of every 10 rows at random build a deep tree", "tokens": [502, 12, 3279, 392, 295, 264, 1412, 370, 321, 1888, 472, 484, 295, 633, 1266, 13241, 412, 4974, 1322, 257, 2452, 4230], "temperature": 0.0, "avg_logprob": -0.24108164170209098, "compression_ratio": 1.57, "no_speech_prob": 2.332067424504203e-06}, {"id": 579, "seek": 292720, "start": 2941.16, "end": 2943.16, "text": " right, which is", "tokens": [558, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.24108164170209098, "compression_ratio": 1.57, "no_speech_prob": 2.332067424504203e-06}, {"id": 580, "seek": 292720, "start": 2943.16, "end": 2947.16, "text": " Perfect on that subset and kind of crappy on the rest", "tokens": [10246, 322, 300, 25993, 293, 733, 295, 36531, 322, 264, 1472], "temperature": 0.0, "avg_logprob": -0.24108164170209098, "compression_ratio": 1.57, "no_speech_prob": 2.332067424504203e-06}, {"id": 581, "seek": 292720, "start": 2947.96, "end": 2952.3199999999997, "text": " All right, let's say we do that a hundred times a different random sample every time", "tokens": [1057, 558, 11, 718, 311, 584, 321, 360, 300, 257, 3262, 1413, 257, 819, 4974, 6889, 633, 565], "temperature": 0.0, "avg_logprob": -0.24108164170209098, "compression_ratio": 1.57, "no_speech_prob": 2.332067424504203e-06}, {"id": 582, "seek": 295232, "start": 2952.32, "end": 2958.92, "text": " So all of the trees are going to be better than nothing right because they do actually have a real random subset of the data", "tokens": [407, 439, 295, 264, 5852, 366, 516, 281, 312, 1101, 813, 1825, 558, 570, 436, 360, 767, 362, 257, 957, 4974, 25993, 295, 264, 1412], "temperature": 0.0, "avg_logprob": -0.1440750661522451, "compression_ratio": 1.8296943231441047, "no_speech_prob": 4.520931611295964e-07}, {"id": 583, "seek": 295232, "start": 2958.92, "end": 2964.6400000000003, "text": " And so they found some insight, but they're also overfitting terribly, but they're all using different random samples", "tokens": [400, 370, 436, 1352, 512, 11269, 11, 457, 436, 434, 611, 670, 69, 2414, 22903, 11, 457, 436, 434, 439, 1228, 819, 4974, 10938], "temperature": 0.0, "avg_logprob": -0.1440750661522451, "compression_ratio": 1.8296943231441047, "no_speech_prob": 4.520931611295964e-07}, {"id": 584, "seek": 295232, "start": 2964.6400000000003, "end": 2970.52, "text": " So they all overfit in different ways on different things so in other words they all have errors", "tokens": [407, 436, 439, 670, 6845, 294, 819, 2098, 322, 819, 721, 370, 294, 661, 2283, 436, 439, 362, 13603], "temperature": 0.0, "avg_logprob": -0.1440750661522451, "compression_ratio": 1.8296943231441047, "no_speech_prob": 4.520931611295964e-07}, {"id": 585, "seek": 295232, "start": 2971.36, "end": 2973.4, "text": " But the errors are random", "tokens": [583, 264, 13603, 366, 4974], "temperature": 0.0, "avg_logprob": -0.1440750661522451, "compression_ratio": 1.8296943231441047, "no_speech_prob": 4.520931611295964e-07}, {"id": 586, "seek": 295232, "start": 2974.48, "end": 2978.1600000000003, "text": " What is the average of a bunch of random errors?", "tokens": [708, 307, 264, 4274, 295, 257, 3840, 295, 4974, 13603, 30], "temperature": 0.0, "avg_logprob": -0.1440750661522451, "compression_ratio": 1.8296943231441047, "no_speech_prob": 4.520931611295964e-07}, {"id": 587, "seek": 295232, "start": 2979.36, "end": 2980.52, "text": " zero", "tokens": [4018], "temperature": 0.0, "avg_logprob": -0.1440750661522451, "compression_ratio": 1.8296943231441047, "no_speech_prob": 4.520931611295964e-07}, {"id": 588, "seek": 298052, "start": 2980.52, "end": 2987.12, "text": " So in other words if we take the average of these trees each of which have been trained on a different random subset", "tokens": [407, 294, 661, 2283, 498, 321, 747, 264, 4274, 295, 613, 5852, 1184, 295, 597, 362, 668, 8895, 322, 257, 819, 4974, 25993], "temperature": 0.0, "avg_logprob": -0.21793245531849026, "compression_ratio": 1.651063829787234, "no_speech_prob": 5.896405923522252e-07}, {"id": 589, "seek": 298052, "start": 2987.12, "end": 2991.48, "text": " The errors will average out to zero and what's left is the true relationship?", "tokens": [440, 13603, 486, 4274, 484, 281, 4018, 293, 437, 311, 1411, 307, 264, 2074, 2480, 30], "temperature": 0.0, "avg_logprob": -0.21793245531849026, "compression_ratio": 1.651063829787234, "no_speech_prob": 5.896405923522252e-07}, {"id": 590, "seek": 298052, "start": 2992.04, "end": 2994.32, "text": " And that's the random forest", "tokens": [400, 300, 311, 264, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.21793245531849026, "compression_ratio": 1.651063829787234, "no_speech_prob": 5.896405923522252e-07}, {"id": 591, "seek": 298052, "start": 2995.12, "end": 2999.52, "text": " So there's the technique right we've got a whole bunch of rows of data", "tokens": [407, 456, 311, 264, 6532, 558, 321, 600, 658, 257, 1379, 3840, 295, 13241, 295, 1412], "temperature": 0.0, "avg_logprob": -0.21793245531849026, "compression_ratio": 1.651063829787234, "no_speech_prob": 5.896405923522252e-07}, {"id": 592, "seek": 298052, "start": 3001.08, "end": 3003.08, "text": " We grab a few at random", "tokens": [492, 4444, 257, 1326, 412, 4974], "temperature": 0.0, "avg_logprob": -0.21793245531849026, "compression_ratio": 1.651063829787234, "no_speech_prob": 5.896405923522252e-07}, {"id": 593, "seek": 298052, "start": 3004.0, "end": 3006.72, "text": " right put them into a smaller data set and", "tokens": [558, 829, 552, 666, 257, 4356, 1412, 992, 293], "temperature": 0.0, "avg_logprob": -0.21793245531849026, "compression_ratio": 1.651063829787234, "no_speech_prob": 5.896405923522252e-07}, {"id": 594, "seek": 300672, "start": 3006.72, "end": 3013.68, "text": " Build a tree based on that okay, and then we put that tree aside and", "tokens": [11875, 257, 4230, 2361, 322, 300, 1392, 11, 293, 550, 321, 829, 300, 4230, 7359, 293], "temperature": 0.0, "avg_logprob": -0.14825752344024315, "compression_ratio": 1.8786407766990292, "no_speech_prob": 2.2959097805141937e-06}, {"id": 595, "seek": 300672, "start": 3015.16, "end": 3021.7599999999998, "text": " Do it again with a different random subset and do it again with a different random subset do it a whole bunch of times and", "tokens": [1144, 309, 797, 365, 257, 819, 4974, 25993, 293, 360, 309, 797, 365, 257, 819, 4974, 25993, 360, 309, 257, 1379, 3840, 295, 1413, 293], "temperature": 0.0, "avg_logprob": -0.14825752344024315, "compression_ratio": 1.8786407766990292, "no_speech_prob": 2.2959097805141937e-06}, {"id": 596, "seek": 300672, "start": 3022.3599999999997, "end": 3027.48, "text": " Then for each one we can then make predictions by running our test data", "tokens": [1396, 337, 1184, 472, 321, 393, 550, 652, 21264, 538, 2614, 527, 1500, 1412], "temperature": 0.0, "avg_logprob": -0.14825752344024315, "compression_ratio": 1.8786407766990292, "no_speech_prob": 2.2959097805141937e-06}, {"id": 597, "seek": 302748, "start": 3027.48, "end": 3036.72, "text": " Through the tree to get to the leaf node take the average in that leaf node for all the trees and average them all together", "tokens": [8927, 264, 4230, 281, 483, 281, 264, 10871, 9984, 747, 264, 4274, 294, 300, 10871, 9984, 337, 439, 264, 5852, 293, 4274, 552, 439, 1214], "temperature": 0.0, "avg_logprob": -0.22087875109040336, "compression_ratio": 1.702020202020202, "no_speech_prob": 1.1726397133315913e-06}, {"id": 598, "seek": 302748, "start": 3036.92, "end": 3038.72, "text": " So to do that", "tokens": [407, 281, 360, 300], "temperature": 0.0, "avg_logprob": -0.22087875109040336, "compression_ratio": 1.702020202020202, "no_speech_prob": 1.1726397133315913e-06}, {"id": 599, "seek": 302748, "start": 3038.72, "end": 3045.88, "text": " We simply call random forest regressor and by default it creates 10 what scikit-learn calls", "tokens": [492, 2935, 818, 4974, 6719, 1121, 735, 284, 293, 538, 7576, 309, 7829, 1266, 437, 2180, 22681, 12, 306, 1083, 5498], "temperature": 0.0, "avg_logprob": -0.22087875109040336, "compression_ratio": 1.702020202020202, "no_speech_prob": 1.1726397133315913e-06}, {"id": 600, "seek": 302748, "start": 3046.28, "end": 3051.68, "text": " Estimators an estimator is a tree right so this is going to create 10 trees", "tokens": [4410, 332, 3391, 364, 8017, 1639, 307, 257, 4230, 558, 370, 341, 307, 516, 281, 1884, 1266, 5852], "temperature": 0.0, "avg_logprob": -0.22087875109040336, "compression_ratio": 1.702020202020202, "no_speech_prob": 1.1726397133315913e-06}, {"id": 601, "seek": 302748, "start": 3052.88, "end": 3054.52, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.22087875109040336, "compression_ratio": 1.702020202020202, "no_speech_prob": 1.1726397133315913e-06}, {"id": 602, "seek": 305452, "start": 3054.52, "end": 3057.52, "text": " So we go ahead and train it I?", "tokens": [407, 321, 352, 2286, 293, 3847, 309, 286, 30], "temperature": 0.0, "avg_logprob": -0.21163476214689367, "compression_ratio": 1.3652694610778444, "no_speech_prob": 2.902289452322293e-06}, {"id": 603, "seek": 305452, "start": 3059.04, "end": 3061.04, "text": " Can't remember if I remember to", "tokens": [1664, 380, 1604, 498, 286, 1604, 281], "temperature": 0.0, "avg_logprob": -0.21163476214689367, "compression_ratio": 1.3652694610778444, "no_speech_prob": 2.902289452322293e-06}, {"id": 604, "seek": 305452, "start": 3070.24, "end": 3076.32, "text": " Okay, so create our 10 trees, and we're just doing this on our little random subset of 20,000 and", "tokens": [1033, 11, 370, 1884, 527, 1266, 5852, 11, 293, 321, 434, 445, 884, 341, 322, 527, 707, 4974, 25993, 295, 945, 11, 1360, 293], "temperature": 0.0, "avg_logprob": -0.21163476214689367, "compression_ratio": 1.3652694610778444, "no_speech_prob": 2.902289452322293e-06}, {"id": 605, "seek": 307632, "start": 3076.32, "end": 3083.28, "text": " So let's take a look at one example. Can you pass the box to Devin?", "tokens": [407, 718, 311, 747, 257, 574, 412, 472, 1365, 13, 1664, 291, 1320, 264, 2424, 281, 1346, 4796, 30], "temperature": 0.0, "avg_logprob": -0.20240234798855253, "compression_ratio": 1.6623376623376624, "no_speech_prob": 1.2679242900048848e-06}, {"id": 606, "seek": 307632, "start": 3085.48, "end": 3090.52, "text": " Just to make sure I'm understanding this so you're saying like we take 10 kind of crappy models", "tokens": [1449, 281, 652, 988, 286, 478, 3701, 341, 370, 291, 434, 1566, 411, 321, 747, 1266, 733, 295, 36531, 5245], "temperature": 0.0, "avg_logprob": -0.20240234798855253, "compression_ratio": 1.6623376623376624, "no_speech_prob": 1.2679242900048848e-06}, {"id": 607, "seek": 307632, "start": 3090.7200000000003, "end": 3096.1200000000003, "text": " We average 10 crappy models, and we get a good model exactly because the crappy models", "tokens": [492, 4274, 1266, 36531, 5245, 11, 293, 321, 483, 257, 665, 2316, 2293, 570, 264, 36531, 5245], "temperature": 0.0, "avg_logprob": -0.20240234798855253, "compression_ratio": 1.6623376623376624, "no_speech_prob": 1.2679242900048848e-06}, {"id": 608, "seek": 307632, "start": 3097.0800000000004, "end": 3104.2400000000002, "text": " Based on different random subsets and so the errors are not correlated with each other if the other errors were correlated with other", "tokens": [18785, 322, 819, 4974, 2090, 1385, 293, 370, 264, 13603, 366, 406, 38574, 365, 1184, 661, 498, 264, 661, 13603, 645, 38574, 365, 661], "temperature": 0.0, "avg_logprob": -0.20240234798855253, "compression_ratio": 1.6623376623376624, "no_speech_prob": 1.2679242900048848e-06}, {"id": 609, "seek": 310424, "start": 3104.24, "end": 3109.9199999999996, "text": " This isn't going to work okay, so the key insight here is to construct multiple models", "tokens": [639, 1943, 380, 516, 281, 589, 1392, 11, 370, 264, 2141, 11269, 510, 307, 281, 7690, 3866, 5245], "temperature": 0.0, "avg_logprob": -0.19002887766848328, "compression_ratio": 1.6781115879828326, "no_speech_prob": 5.771887117589358e-06}, {"id": 610, "seek": 310424, "start": 3110.3599999999997, "end": 3115.56, "text": " Which are better than nothing and where the errors are as much as possible not correlated with each other", "tokens": [3013, 366, 1101, 813, 1825, 293, 689, 264, 13603, 366, 382, 709, 382, 1944, 406, 38574, 365, 1184, 661], "temperature": 0.0, "avg_logprob": -0.19002887766848328, "compression_ratio": 1.6781115879828326, "no_speech_prob": 5.771887117589358e-06}, {"id": 611, "seek": 310424, "start": 3117.64, "end": 3122.4399999999996, "text": " So is there like a certain number of trees that like we need that in order to be valid like", "tokens": [407, 307, 456, 411, 257, 1629, 1230, 295, 5852, 300, 411, 321, 643, 300, 294, 1668, 281, 312, 7363, 411], "temperature": 0.0, "avg_logprob": -0.19002887766848328, "compression_ratio": 1.6781115879828326, "no_speech_prob": 5.771887117589358e-06}, {"id": 612, "seek": 310424, "start": 3123.0, "end": 3125.0, "text": " There's no such thing as like valid or invalid", "tokens": [821, 311, 572, 1270, 551, 382, 411, 7363, 420, 34702], "temperature": 0.0, "avg_logprob": -0.19002887766848328, "compression_ratio": 1.6781115879828326, "no_speech_prob": 5.771887117589358e-06}, {"id": 613, "seek": 310424, "start": 3125.24, "end": 3130.7599999999998, "text": " There's like has a good validation set RMSE or not you know", "tokens": [821, 311, 411, 575, 257, 665, 24071, 992, 23790, 5879, 420, 406, 291, 458], "temperature": 0.0, "avg_logprob": -0.19002887766848328, "compression_ratio": 1.6781115879828326, "no_speech_prob": 5.771887117589358e-06}, {"id": 614, "seek": 313076, "start": 3130.76, "end": 3135.0, "text": " And so that's what we're going to look at is how to is how to make that metric higher", "tokens": [400, 370, 300, 311, 437, 321, 434, 516, 281, 574, 412, 307, 577, 281, 307, 577, 281, 652, 300, 20678, 2946], "temperature": 0.0, "avg_logprob": -0.18671274185180664, "compression_ratio": 1.826086956521739, "no_speech_prob": 6.048865088814637e-06}, {"id": 615, "seek": 313076, "start": 3135.0, "end": 3139.28, "text": " And so this is the first of our hyper parameters", "tokens": [400, 370, 341, 307, 264, 700, 295, 527, 9848, 9834], "temperature": 0.0, "avg_logprob": -0.18671274185180664, "compression_ratio": 1.826086956521739, "no_speech_prob": 6.048865088814637e-06}, {"id": 616, "seek": 313076, "start": 3139.44, "end": 3143.76, "text": " And we're going to learn about how to tune hyper parameters and the first one is going to be the number of trees", "tokens": [400, 321, 434, 516, 281, 1466, 466, 577, 281, 10864, 9848, 9834, 293, 264, 700, 472, 307, 516, 281, 312, 264, 1230, 295, 5852], "temperature": 0.0, "avg_logprob": -0.18671274185180664, "compression_ratio": 1.826086956521739, "no_speech_prob": 6.048865088814637e-06}, {"id": 617, "seek": 313076, "start": 3143.76, "end": 3145.76, "text": " And we're about to look at that now", "tokens": [400, 321, 434, 466, 281, 574, 412, 300, 586], "temperature": 0.0, "avg_logprob": -0.18671274185180664, "compression_ratio": 1.826086956521739, "no_speech_prob": 6.048865088814637e-06}, {"id": 618, "seek": 313076, "start": 3146.4, "end": 3148.4, "text": " Yes, mostly", "tokens": [1079, 11, 5240], "temperature": 0.0, "avg_logprob": -0.18671274185180664, "compression_ratio": 1.826086956521739, "no_speech_prob": 6.048865088814637e-06}, {"id": 619, "seek": 313076, "start": 3148.8, "end": 3154.5600000000004, "text": " The subset that you are selecting are they exclusive can can you have overlapping?", "tokens": [440, 25993, 300, 291, 366, 18182, 366, 436, 13005, 393, 393, 291, 362, 33535, 30], "temperature": 0.0, "avg_logprob": -0.18671274185180664, "compression_ratio": 1.826086956521739, "no_speech_prob": 6.048865088814637e-06}, {"id": 620, "seek": 313076, "start": 3154.6800000000003, "end": 3159.96, "text": " Yeah, so I mentioned you know one approach would be pick out like a tenth at random", "tokens": [865, 11, 370, 286, 2835, 291, 458, 472, 3109, 576, 312, 1888, 484, 411, 257, 27269, 412, 4974], "temperature": 0.0, "avg_logprob": -0.18671274185180664, "compression_ratio": 1.826086956521739, "no_speech_prob": 6.048865088814637e-06}, {"id": 621, "seek": 315996, "start": 3159.96, "end": 3168.44, "text": " But actually what scikit-learn does by default is for n rows it picks out n rows with replacement", "tokens": [583, 767, 437, 2180, 22681, 12, 306, 1083, 775, 538, 7576, 307, 337, 297, 13241, 309, 16137, 484, 297, 13241, 365, 14419], "temperature": 0.0, "avg_logprob": -0.24857731354542267, "compression_ratio": 1.526829268292683, "no_speech_prob": 6.854267667222302e-06}, {"id": 622, "seek": 315996, "start": 3168.88, "end": 3174.48, "text": " Okay, and that's called bootstrapping and if memory serves me correctly that gets you an average", "tokens": [1033, 11, 293, 300, 311, 1219, 11450, 19639, 3759, 293, 498, 4675, 13451, 385, 8944, 300, 2170, 291, 364, 4274], "temperature": 0.0, "avg_logprob": -0.24857731354542267, "compression_ratio": 1.526829268292683, "no_speech_prob": 6.854267667222302e-06}, {"id": 623, "seek": 315996, "start": 3175.32, "end": 3181.32, "text": " 63.2 percent of the rows will be represented and you know a bunch of them will be represented multiple times", "tokens": [25082, 13, 17, 3043, 295, 264, 13241, 486, 312, 10379, 293, 291, 458, 257, 3840, 295, 552, 486, 312, 10379, 3866, 1413], "temperature": 0.0, "avg_logprob": -0.24857731354542267, "compression_ratio": 1.526829268292683, "no_speech_prob": 6.854267667222302e-06}, {"id": 624, "seek": 315996, "start": 3181.92, "end": 3183.92, "text": " Yeah", "tokens": [865], "temperature": 0.0, "avg_logprob": -0.24857731354542267, "compression_ratio": 1.526829268292683, "no_speech_prob": 6.854267667222302e-06}, {"id": 625, "seek": 315996, "start": 3184.4, "end": 3185.44, "text": " Sure", "tokens": [4894], "temperature": 0.0, "avg_logprob": -0.24857731354542267, "compression_ratio": 1.526829268292683, "no_speech_prob": 6.854267667222302e-06}, {"id": 626, "seek": 318544, "start": 3185.44, "end": 3190.12, "text": " So rather than just picking out like a tenth of the rows at random instead", "tokens": [407, 2831, 813, 445, 8867, 484, 411, 257, 27269, 295, 264, 13241, 412, 4974, 2602], "temperature": 0.0, "avg_logprob": -0.21473582197980182, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.7603234709895332e-06}, {"id": 627, "seek": 318544, "start": 3190.2400000000002, "end": 3196.06, "text": " We're going to pick out of an n row data set. We're going to pick out n rows with replacement", "tokens": [492, 434, 516, 281, 1888, 484, 295, 364, 297, 5386, 1412, 992, 13, 492, 434, 516, 281, 1888, 484, 297, 13241, 365, 14419], "temperature": 0.0, "avg_logprob": -0.21473582197980182, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.7603234709895332e-06}, {"id": 628, "seek": 318544, "start": 3196.7200000000003, "end": 3202.66, "text": " Which on average gets about 63. I think 63.2 percent of the rows will be represented", "tokens": [3013, 322, 4274, 2170, 466, 25082, 13, 286, 519, 25082, 13, 17, 3043, 295, 264, 13241, 486, 312, 10379], "temperature": 0.0, "avg_logprob": -0.21473582197980182, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.7603234709895332e-06}, {"id": 629, "seek": 318544, "start": 3203.36, "end": 3205.98, "text": " Many of those rows will appear multiple times", "tokens": [5126, 295, 729, 13241, 486, 4204, 3866, 1413], "temperature": 0.0, "avg_logprob": -0.21473582197980182, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.7603234709895332e-06}, {"id": 630, "seek": 318544, "start": 3207.0, "end": 3209.0, "text": " Because a question behind you", "tokens": [1436, 257, 1168, 2261, 291], "temperature": 0.0, "avg_logprob": -0.21473582197980182, "compression_ratio": 1.6127450980392157, "no_speech_prob": 1.7603234709895332e-06}, {"id": 631, "seek": 320900, "start": 3209.0, "end": 3217.4, "text": " In essence what this model is doing is if I understand credit is just picking out the", "tokens": [682, 12801, 437, 341, 2316, 307, 884, 307, 498, 286, 1223, 5397, 307, 445, 8867, 484, 264], "temperature": 0.0, "avg_logprob": -0.252846969748443, "compression_ratio": 1.7748091603053435, "no_speech_prob": 6.747960014763521e-06}, {"id": 632, "seek": 320900, "start": 3218.16, "end": 3220.88, "text": " Date points that look very similar to the one you're looking at", "tokens": [31805, 2793, 300, 574, 588, 2531, 281, 264, 472, 291, 434, 1237, 412], "temperature": 0.0, "avg_logprob": -0.252846969748443, "compression_ratio": 1.7748091603053435, "no_speech_prob": 6.747960014763521e-06}, {"id": 633, "seek": 320900, "start": 3221.4, "end": 3225.12, "text": " Yeah, that's a great insight. So what a tree is kind of doing", "tokens": [865, 11, 300, 311, 257, 869, 11269, 13, 407, 437, 257, 4230, 307, 733, 295, 884], "temperature": 0.0, "avg_logprob": -0.252846969748443, "compression_ratio": 1.7748091603053435, "no_speech_prob": 6.747960014763521e-06}, {"id": 634, "seek": 320900, "start": 3225.2, "end": 3230.52, "text": " There's not quite complicated way of going about doing that if you I mean there would be other ways of like assessing similarity", "tokens": [821, 311, 406, 1596, 6179, 636, 295, 516, 466, 884, 300, 498, 291, 286, 914, 456, 576, 312, 661, 2098, 295, 411, 34348, 32194], "temperature": 0.0, "avg_logprob": -0.252846969748443, "compression_ratio": 1.7748091603053435, "no_speech_prob": 6.747960014763521e-06}, {"id": 635, "seek": 323052, "start": 3230.52, "end": 3238.4, "text": " There are other ways of saying assessing similarity, but what's interesting about this way is it's doing it in in tree space", "tokens": [821, 366, 661, 2098, 295, 1566, 34348, 32194, 11, 457, 437, 311, 1880, 466, 341, 636, 307, 309, 311, 884, 309, 294, 294, 4230, 1901], "temperature": 0.0, "avg_logprob": -0.14771750213903026, "compression_ratio": 1.77992277992278, "no_speech_prob": 7.338177852034278e-07}, {"id": 636, "seek": 323052, "start": 3238.8, "end": 3242.68, "text": " Right so we're basically saying what are in this case like for this little tree?", "tokens": [1779, 370, 321, 434, 1936, 1566, 437, 366, 294, 341, 1389, 411, 337, 341, 707, 4230, 30], "temperature": 0.0, "avg_logprob": -0.14771750213903026, "compression_ratio": 1.77992277992278, "no_speech_prob": 7.338177852034278e-07}, {"id": 637, "seek": 323052, "start": 3242.68, "end": 3246.24, "text": " What are the 593 samples you know closest to this one?", "tokens": [708, 366, 264, 24624, 18, 10938, 291, 458, 13699, 281, 341, 472, 30], "temperature": 0.0, "avg_logprob": -0.14771750213903026, "compression_ratio": 1.77992277992278, "no_speech_prob": 7.338177852034278e-07}, {"id": 638, "seek": 323052, "start": 3246.24, "end": 3253.0, "text": " And what's the average closest in tree space so other ways of doing that would be like and we'll learn later on in this course", "tokens": [400, 437, 311, 264, 4274, 13699, 294, 4230, 1901, 370, 661, 2098, 295, 884, 300, 576, 312, 411, 293, 321, 603, 1466, 1780, 322, 294, 341, 1164], "temperature": 0.0, "avg_logprob": -0.14771750213903026, "compression_ratio": 1.77992277992278, "no_speech_prob": 7.338177852034278e-07}, {"id": 639, "seek": 323052, "start": 3253.0, "end": 3257.64, "text": " About k nearest neighbors you could use like Euclidean distance say right", "tokens": [7769, 350, 23831, 12512, 291, 727, 764, 411, 462, 1311, 31264, 282, 4560, 584, 558], "temperature": 0.0, "avg_logprob": -0.14771750213903026, "compression_ratio": 1.77992277992278, "no_speech_prob": 7.338177852034278e-07}, {"id": 640, "seek": 325764, "start": 3257.64, "end": 3259.64, "text": " But here's the thing", "tokens": [583, 510, 311, 264, 551], "temperature": 0.0, "avg_logprob": -0.23848739227691254, "compression_ratio": 1.7333333333333334, "no_speech_prob": 2.2603101115237223e-06}, {"id": 641, "seek": 325764, "start": 3260.96, "end": 3264.24, "text": " The whole point of machine learning is to identify", "tokens": [440, 1379, 935, 295, 3479, 2539, 307, 281, 5876], "temperature": 0.0, "avg_logprob": -0.23848739227691254, "compression_ratio": 1.7333333333333334, "no_speech_prob": 2.2603101115237223e-06}, {"id": 642, "seek": 325764, "start": 3265.16, "end": 3272.3599999999997, "text": " Which variables actually matter the most and how do they relate to each other and to your dependent variable together?", "tokens": [3013, 9102, 767, 1871, 264, 881, 293, 577, 360, 436, 10961, 281, 1184, 661, 293, 281, 428, 12334, 7006, 1214, 30], "temperature": 0.0, "avg_logprob": -0.23848739227691254, "compression_ratio": 1.7333333333333334, "no_speech_prob": 2.2603101115237223e-06}, {"id": 643, "seek": 325764, "start": 3272.8399999999997, "end": 3277.8399999999997, "text": " Right so if you've like imagine a synthetic data set where you create five variables", "tokens": [1779, 370, 498, 291, 600, 411, 3811, 257, 23420, 1412, 992, 689, 291, 1884, 1732, 9102], "temperature": 0.0, "avg_logprob": -0.23848739227691254, "compression_ratio": 1.7333333333333334, "no_speech_prob": 2.2603101115237223e-06}, {"id": 644, "seek": 325764, "start": 3278.16, "end": 3283.2599999999998, "text": " That add together to your independent to create your dependent variable and 95 variables", "tokens": [663, 909, 1214, 281, 428, 6695, 281, 1884, 428, 12334, 7006, 293, 13420, 9102], "temperature": 0.0, "avg_logprob": -0.23848739227691254, "compression_ratio": 1.7333333333333334, "no_speech_prob": 2.2603101115237223e-06}, {"id": 645, "seek": 328326, "start": 3283.26, "end": 3290.1000000000004, "text": " Which are entirely random and don't impact your dependent variable, and then if you do like a k nearest neighbors in Euclidean space", "tokens": [3013, 366, 7696, 4974, 293, 500, 380, 2712, 428, 12334, 7006, 11, 293, 550, 498, 291, 360, 411, 257, 350, 23831, 12512, 294, 462, 1311, 31264, 282, 1901], "temperature": 0.0, "avg_logprob": -0.2157424803703062, "compression_ratio": 1.7673469387755103, "no_speech_prob": 3.466321345513279e-07}, {"id": 646, "seek": 328326, "start": 3290.7200000000003, "end": 3295.1600000000003, "text": " You're going to get meaningless nearest neighbors because most of your columns are actually meaningless or", "tokens": [509, 434, 516, 281, 483, 33232, 23831, 12512, 570, 881, 295, 428, 13766, 366, 767, 33232, 420], "temperature": 0.0, "avg_logprob": -0.2157424803703062, "compression_ratio": 1.7673469387755103, "no_speech_prob": 3.466321345513279e-07}, {"id": 647, "seek": 328326, "start": 3296.0400000000004, "end": 3299.5600000000004, "text": " Imagine your actual relationship is that your dependent variable equals?", "tokens": [11739, 428, 3539, 2480, 307, 300, 428, 12334, 7006, 6915, 30], "temperature": 0.0, "avg_logprob": -0.2157424803703062, "compression_ratio": 1.7673469387755103, "no_speech_prob": 3.466321345513279e-07}, {"id": 648, "seek": 328326, "start": 3300.32, "end": 3308.0800000000004, "text": " X1 times x2 then you're actually need to find this interaction right so you don't actually care about how close it is to", "tokens": [1783, 16, 1413, 2031, 17, 550, 291, 434, 767, 643, 281, 915, 341, 9285, 558, 370, 291, 500, 380, 767, 1127, 466, 577, 1998, 309, 307, 281], "temperature": 0.0, "avg_logprob": -0.2157424803703062, "compression_ratio": 1.7673469387755103, "no_speech_prob": 3.466321345513279e-07}, {"id": 649, "seek": 330808, "start": 3308.08, "end": 3316.0, "text": " X1 and how close to x2, but how close to the product so the entire purpose of modeling in machine learning is to find?", "tokens": [1783, 16, 293, 577, 1998, 281, 2031, 17, 11, 457, 577, 1998, 281, 264, 1674, 370, 264, 2302, 4334, 295, 15983, 294, 3479, 2539, 307, 281, 915, 30], "temperature": 0.0, "avg_logprob": -0.19903295305040147, "compression_ratio": 1.6764705882352942, "no_speech_prob": 1.3363866457893891e-07}, {"id": 650, "seek": 330808, "start": 3316.0, "end": 3322.2, "text": " A model which tells you which variables are important, and how do they interact together to drive your dependent variable?", "tokens": [316, 2316, 597, 5112, 291, 597, 9102, 366, 1021, 11, 293, 577, 360, 436, 4648, 1214, 281, 3332, 428, 12334, 7006, 30], "temperature": 0.0, "avg_logprob": -0.19903295305040147, "compression_ratio": 1.6764705882352942, "no_speech_prob": 1.3363866457893891e-07}, {"id": 651, "seek": 330808, "start": 3323.04, "end": 3326.7999999999997, "text": " And so you'll find in practice the difference between", "tokens": [400, 370, 291, 603, 915, 294, 3124, 264, 2649, 1296], "temperature": 0.0, "avg_logprob": -0.19903295305040147, "compression_ratio": 1.6764705882352942, "no_speech_prob": 1.3363866457893891e-07}, {"id": 652, "seek": 330808, "start": 3327.6, "end": 3334.08, "text": " Using like tree space or random forest space to find your nearest neighbors versus like Euclidean space", "tokens": [11142, 411, 4230, 1901, 420, 4974, 6719, 1901, 281, 915, 428, 23831, 12512, 5717, 411, 462, 1311, 31264, 282, 1901], "temperature": 0.0, "avg_logprob": -0.19903295305040147, "compression_ratio": 1.6764705882352942, "no_speech_prob": 1.3363866457893891e-07}, {"id": 653, "seek": 333408, "start": 3334.08, "end": 3339.64, "text": " It's the difference between a model that makes good predictions and the model that makes meaningless predictions", "tokens": [467, 311, 264, 2649, 1296, 257, 2316, 300, 1669, 665, 21264, 293, 264, 2316, 300, 1669, 33232, 21264], "temperature": 0.0, "avg_logprob": -0.2608202127309946, "compression_ratio": 1.5284090909090908, "no_speech_prob": 3.5559530715545407e-06}, {"id": 654, "seek": 333408, "start": 3341.04, "end": 3343.04, "text": " Melissa do you have", "tokens": [22844, 360, 291, 362], "temperature": 0.0, "avg_logprob": -0.2608202127309946, "compression_ratio": 1.5284090909090908, "no_speech_prob": 3.5559530715545407e-06}, {"id": 655, "seek": 333408, "start": 3343.68, "end": 3348.2, "text": " I did, but I feel like we've got only 35 minutes, so", "tokens": [286, 630, 11, 457, 286, 841, 411, 321, 600, 658, 787, 6976, 2077, 11, 370], "temperature": 0.0, "avg_logprob": -0.2608202127309946, "compression_ratio": 1.5284090909090908, "no_speech_prob": 3.5559530715545407e-06}, {"id": 656, "seek": 333408, "start": 3353.08, "end": 3355.08, "text": " Great so", "tokens": [3769, 370], "temperature": 0.0, "avg_logprob": -0.2608202127309946, "compression_ratio": 1.5284090909090908, "no_speech_prob": 3.5559530715545407e-06}, {"id": 657, "seek": 335508, "start": 3355.08, "end": 3363.72, "text": " So in general a machine learning model, which is effective is one which is", "tokens": [407, 294, 2674, 257, 3479, 2539, 2316, 11, 597, 307, 4942, 307, 472, 597, 307], "temperature": 0.0, "avg_logprob": -0.18596936543782552, "compression_ratio": 1.6091370558375635, "no_speech_prob": 2.813000946844113e-06}, {"id": 658, "seek": 335508, "start": 3366.04, "end": 3371.74, "text": " Accurate when you look at the training data. It's it's it's accurate at predicting at actually", "tokens": [5725, 33144, 562, 291, 574, 412, 264, 3097, 1412, 13, 467, 311, 309, 311, 309, 311, 8559, 412, 32884, 412, 767], "temperature": 0.0, "avg_logprob": -0.18596936543782552, "compression_ratio": 1.6091370558375635, "no_speech_prob": 2.813000946844113e-06}, {"id": 659, "seek": 335508, "start": 3372.7999999999997, "end": 3378.48, "text": " Finding the relationships in that training data, and then it generalizes well to new data", "tokens": [31947, 264, 6159, 294, 300, 3097, 1412, 11, 293, 550, 309, 2674, 5660, 731, 281, 777, 1412], "temperature": 0.0, "avg_logprob": -0.18596936543782552, "compression_ratio": 1.6091370558375635, "no_speech_prob": 2.813000946844113e-06}, {"id": 660, "seek": 335508, "start": 3378.48, "end": 3383.36, "text": " And so in bagging that means that each of your individual", "tokens": [400, 370, 294, 3411, 3249, 300, 1355, 300, 1184, 295, 428, 2609], "temperature": 0.0, "avg_logprob": -0.18596936543782552, "compression_ratio": 1.6091370558375635, "no_speech_prob": 2.813000946844113e-06}, {"id": 661, "seek": 338336, "start": 3383.36, "end": 3385.08, "text": " estimators", "tokens": [8017, 3391], "temperature": 0.0, "avg_logprob": -0.21536129777149488, "compression_ratio": 1.7656903765690377, "no_speech_prob": 3.966955318901455e-06}, {"id": 662, "seek": 338336, "start": 3385.08, "end": 3389.84, "text": " If your individual trees you want to be as predictive as possible", "tokens": [759, 428, 2609, 5852, 291, 528, 281, 312, 382, 35521, 382, 1944], "temperature": 0.0, "avg_logprob": -0.21536129777149488, "compression_ratio": 1.7656903765690377, "no_speech_prob": 3.966955318901455e-06}, {"id": 663, "seek": 338336, "start": 3390.1200000000003, "end": 3394.7200000000003, "text": " But for the predictions of your individual trees to be as uncorrelated as possible", "tokens": [583, 337, 264, 21264, 295, 428, 2609, 5852, 281, 312, 382, 6219, 284, 12004, 382, 1944], "temperature": 0.0, "avg_logprob": -0.21536129777149488, "compression_ratio": 1.7656903765690377, "no_speech_prob": 3.966955318901455e-06}, {"id": 664, "seek": 338336, "start": 3394.7200000000003, "end": 3400.88, "text": " And so the inventor of random forests talks about this at length in his original paper that introduced this in the late 90s", "tokens": [400, 370, 264, 41593, 295, 4974, 21700, 6686, 466, 341, 412, 4641, 294, 702, 3380, 3035, 300, 7268, 341, 294, 264, 3469, 4289, 82], "temperature": 0.0, "avg_logprob": -0.21536129777149488, "compression_ratio": 1.7656903765690377, "no_speech_prob": 3.966955318901455e-06}, {"id": 665, "seek": 338336, "start": 3400.92, "end": 3403.1200000000003, "text": " this idea of trying to come up with", "tokens": [341, 1558, 295, 1382, 281, 808, 493, 365], "temperature": 0.0, "avg_logprob": -0.21536129777149488, "compression_ratio": 1.7656903765690377, "no_speech_prob": 3.966955318901455e-06}, {"id": 666, "seek": 338336, "start": 3404.28, "end": 3406.56, "text": " predictive, but poorly correlated trees", "tokens": [35521, 11, 457, 22271, 38574, 5852], "temperature": 0.0, "avg_logprob": -0.21536129777149488, "compression_ratio": 1.7656903765690377, "no_speech_prob": 3.966955318901455e-06}, {"id": 667, "seek": 340656, "start": 3406.56, "end": 3413.52, "text": " the the research community in recent years has generally found that", "tokens": [264, 264, 2132, 1768, 294, 5162, 924, 575, 5101, 1352, 300], "temperature": 0.0, "avg_logprob": -0.1659131533857705, "compression_ratio": 1.6161616161616161, "no_speech_prob": 4.02941441279836e-06}, {"id": 668, "seek": 340656, "start": 3414.04, "end": 3421.32, "text": " The more important thing seems to be creating uncorrelated trees rather than more accurate trees", "tokens": [440, 544, 1021, 551, 2544, 281, 312, 4084, 6219, 284, 12004, 5852, 2831, 813, 544, 8559, 5852], "temperature": 0.0, "avg_logprob": -0.1659131533857705, "compression_ratio": 1.6161616161616161, "no_speech_prob": 4.02941441279836e-06}, {"id": 669, "seek": 340656, "start": 3421.6, "end": 3426.64, "text": " So more recent advances tend to create trees which are less predictive on their own", "tokens": [407, 544, 5162, 25297, 3928, 281, 1884, 5852, 597, 366, 1570, 35521, 322, 641, 1065], "temperature": 0.0, "avg_logprob": -0.1659131533857705, "compression_ratio": 1.6161616161616161, "no_speech_prob": 4.02941441279836e-06}, {"id": 670, "seek": 340656, "start": 3427.36, "end": 3432.0, "text": " But also less correlated with each other so for example in scikit-learn", "tokens": [583, 611, 1570, 38574, 365, 1184, 661, 370, 337, 1365, 294, 2180, 22681, 12, 306, 1083], "temperature": 0.0, "avg_logprob": -0.1659131533857705, "compression_ratio": 1.6161616161616161, "no_speech_prob": 4.02941441279836e-06}, {"id": 671, "seek": 343200, "start": 3432.0, "end": 3439.4, "text": " There's another class you can use called extra trees regressor or extra trees classifier with exactly the same API", "tokens": [821, 311, 1071, 1508, 291, 393, 764, 1219, 2857, 5852, 1121, 735, 284, 420, 2857, 5852, 1508, 9902, 365, 2293, 264, 912, 9362], "temperature": 0.0, "avg_logprob": -0.15591875354895432, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.6028053551053745e-06}, {"id": 672, "seek": 343200, "start": 3439.4, "end": 3446.24, "text": " You can try it tonight. Just replace my random forest regressor with that that's called an extremely randomized trees", "tokens": [509, 393, 853, 309, 4440, 13, 1449, 7406, 452, 4974, 6719, 1121, 735, 284, 365, 300, 300, 311, 1219, 364, 4664, 38513, 5852], "temperature": 0.0, "avg_logprob": -0.15591875354895432, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.6028053551053745e-06}, {"id": 673, "seek": 343200, "start": 3446.96, "end": 3450.44, "text": " Model and what that does is exactly the same as what we just discussed", "tokens": [17105, 293, 437, 300, 775, 307, 2293, 264, 912, 382, 437, 321, 445, 7152], "temperature": 0.0, "avg_logprob": -0.15591875354895432, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.6028053551053745e-06}, {"id": 674, "seek": 343200, "start": 3450.96, "end": 3457.68, "text": " But rather than trying every split of every variable that randomly tries a few splits of a few variables", "tokens": [583, 2831, 813, 1382, 633, 7472, 295, 633, 7006, 300, 16979, 9898, 257, 1326, 37741, 295, 257, 1326, 9102], "temperature": 0.0, "avg_logprob": -0.15591875354895432, "compression_ratio": 1.7586206896551724, "no_speech_prob": 1.6028053551053745e-06}, {"id": 675, "seek": 345768, "start": 3457.68, "end": 3462.8399999999997, "text": " Right so it's much faster to train it has more randomness", "tokens": [1779, 370, 309, 311, 709, 4663, 281, 3847, 309, 575, 544, 4974, 1287], "temperature": 0.0, "avg_logprob": -0.20558958689371745, "compression_ratio": 1.515, "no_speech_prob": 4.812508791474102e-07}, {"id": 676, "seek": 345768, "start": 3463.5, "end": 3468.64, "text": " Okay, but then you've that time you can build more trees and therefore get better generalization", "tokens": [1033, 11, 457, 550, 291, 600, 300, 565, 291, 393, 1322, 544, 5852, 293, 4412, 483, 1101, 2674, 2144], "temperature": 0.0, "avg_logprob": -0.20558958689371745, "compression_ratio": 1.515, "no_speech_prob": 4.812508791474102e-07}, {"id": 677, "seek": 345768, "start": 3470.44, "end": 3477.24, "text": " So in practice if you've got crappy individual models you just need more trees to get a good end-up model", "tokens": [407, 294, 3124, 498, 291, 600, 658, 36531, 2609, 5245, 291, 445, 643, 544, 5852, 281, 483, 257, 665, 917, 12, 1010, 2316], "temperature": 0.0, "avg_logprob": -0.20558958689371745, "compression_ratio": 1.515, "no_speech_prob": 4.812508791474102e-07}, {"id": 678, "seek": 345768, "start": 3477.24, "end": 3479.24, "text": " Melissa could you pass that over to Devon?", "tokens": [22844, 727, 291, 1320, 300, 670, 281, 9096, 266, 30], "temperature": 0.0, "avg_logprob": -0.20558958689371745, "compression_ratio": 1.515, "no_speech_prob": 4.812508791474102e-07}, {"id": 679, "seek": 347924, "start": 3479.24, "end": 3488.4799999999996, "text": " Could you talk a little bit more about what you mean by like uncorrelated trees yeah?", "tokens": [7497, 291, 751, 257, 707, 857, 544, 466, 437, 291, 914, 538, 411, 6219, 284, 12004, 5852, 1338, 30], "temperature": 0.0, "avg_logprob": -0.18168162203383173, "compression_ratio": 1.904040404040404, "no_speech_prob": 9.874589750324958e-07}, {"id": 680, "seek": 347924, "start": 3490.2799999999997, "end": 3495.72, "text": " If I build a thousand trees each one on just ten data points", "tokens": [759, 286, 1322, 257, 4714, 5852, 1184, 472, 322, 445, 2064, 1412, 2793], "temperature": 0.0, "avg_logprob": -0.18168162203383173, "compression_ratio": 1.904040404040404, "no_speech_prob": 9.874589750324958e-07}, {"id": 681, "seek": 347924, "start": 3497.3999999999996, "end": 3502.6, "text": " Then it's quite likely that the ten data points for every tree are going to be totally different", "tokens": [1396, 309, 311, 1596, 3700, 300, 264, 2064, 1412, 2793, 337, 633, 4230, 366, 516, 281, 312, 3879, 819], "temperature": 0.0, "avg_logprob": -0.18168162203383173, "compression_ratio": 1.904040404040404, "no_speech_prob": 9.874589750324958e-07}, {"id": 682, "seek": 347924, "start": 3502.6, "end": 3508.7999999999997, "text": " And so it's quite likely that those ten trees are going to a thousand trees are going to give totally different answers to each other", "tokens": [400, 370, 309, 311, 1596, 3700, 300, 729, 2064, 5852, 366, 516, 281, 257, 4714, 5852, 366, 516, 281, 976, 3879, 819, 6338, 281, 1184, 661], "temperature": 0.0, "avg_logprob": -0.18168162203383173, "compression_ratio": 1.904040404040404, "no_speech_prob": 9.874589750324958e-07}, {"id": 683, "seek": 350880, "start": 3508.8, "end": 3510.8, "text": " so the correlation", "tokens": [370, 264, 20009], "temperature": 0.0, "avg_logprob": -0.21944496848366477, "compression_ratio": 1.8638297872340426, "no_speech_prob": 9.874617035166011e-07}, {"id": 684, "seek": 350880, "start": 3511.76, "end": 3517.36, "text": " Between the predictions of tree one and tree two is going to be very small between tree one and tree three very small and so", "tokens": [18967, 264, 21264, 295, 4230, 472, 293, 4230, 732, 307, 516, 281, 312, 588, 1359, 1296, 4230, 472, 293, 4230, 1045, 588, 1359, 293, 370], "temperature": 0.0, "avg_logprob": -0.21944496848366477, "compression_ratio": 1.8638297872340426, "no_speech_prob": 9.874617035166011e-07}, {"id": 685, "seek": 350880, "start": 3517.36, "end": 3520.0, "text": " forth on the other hand if I create a thousand trees", "tokens": [5220, 322, 264, 661, 1011, 498, 286, 1884, 257, 4714, 5852], "temperature": 0.0, "avg_logprob": -0.21944496848366477, "compression_ratio": 1.8638297872340426, "no_speech_prob": 9.874617035166011e-07}, {"id": 686, "seek": 350880, "start": 3520.44, "end": 3527.96, "text": " Where each time I use the entire data set with just one element removed all those trees are going to be nearly identical", "tokens": [2305, 1184, 565, 286, 764, 264, 2302, 1412, 992, 365, 445, 472, 4478, 7261, 439, 729, 5852, 366, 516, 281, 312, 6217, 14800], "temperature": 0.0, "avg_logprob": -0.21944496848366477, "compression_ratio": 1.8638297872340426, "no_speech_prob": 9.874617035166011e-07}, {"id": 687, "seek": 350880, "start": 3528.96, "end": 3533.48, "text": " ie their predictions will be highly correlated and so in the latter case", "tokens": [43203, 641, 21264, 486, 312, 5405, 38574, 293, 370, 294, 264, 18481, 1389], "temperature": 0.0, "avg_logprob": -0.21944496848366477, "compression_ratio": 1.8638297872340426, "no_speech_prob": 9.874617035166011e-07}, {"id": 688, "seek": 350880, "start": 3533.6000000000004, "end": 3535.92, "text": " It's probably not going to generalize very well", "tokens": [467, 311, 1391, 406, 516, 281, 2674, 1125, 588, 731], "temperature": 0.0, "avg_logprob": -0.21944496848366477, "compression_ratio": 1.8638297872340426, "no_speech_prob": 9.874617035166011e-07}, {"id": 689, "seek": 353592, "start": 3535.92, "end": 3542.88, "text": " Where else in the former case the individual trees were not going to be very predictive, so I need to find some nice in", "tokens": [2305, 1646, 294, 264, 5819, 1389, 264, 2609, 5852, 645, 406, 516, 281, 312, 588, 35521, 11, 370, 286, 643, 281, 915, 512, 1481, 294], "temperature": 0.0, "avg_logprob": -0.21020666375217667, "compression_ratio": 1.584070796460177, "no_speech_prob": 3.0894664178049425e-06}, {"id": 690, "seek": 353592, "start": 3543.56, "end": 3545.56, "text": " between", "tokens": [1296], "temperature": 0.0, "avg_logprob": -0.21020666375217667, "compression_ratio": 1.584070796460177, "no_speech_prob": 3.0894664178049425e-06}, {"id": 691, "seek": 353592, "start": 3547.48, "end": 3549.48, "text": " So yes Danielle", "tokens": [407, 2086, 21182], "temperature": 0.0, "avg_logprob": -0.21020666375217667, "compression_ratio": 1.584070796460177, "no_speech_prob": 3.0894664178049425e-06}, {"id": 692, "seek": 353592, "start": 3550.52, "end": 3555.88, "text": " And is there a case where you want to use one over the other like any particular times?", "tokens": [400, 307, 456, 257, 1389, 689, 291, 528, 281, 764, 472, 670, 264, 661, 411, 604, 1729, 1413, 30], "temperature": 0.0, "avg_logprob": -0.21020666375217667, "compression_ratio": 1.584070796460177, "no_speech_prob": 3.0894664178049425e-06}, {"id": 693, "seek": 353592, "start": 3556.04, "end": 3562.04, "text": " Yeah, so again hyper parameter tuning so do you mean in terms of like random random forests versus extremely randomized trees?", "tokens": [865, 11, 370, 797, 9848, 13075, 15164, 370, 360, 291, 914, 294, 2115, 295, 411, 4974, 4974, 21700, 5717, 4664, 38513, 5852, 30], "temperature": 0.0, "avg_logprob": -0.21020666375217667, "compression_ratio": 1.584070796460177, "no_speech_prob": 3.0894664178049425e-06}, {"id": 694, "seek": 356204, "start": 3562.04, "end": 3568.56, "text": " Yeah, so again a hyper parameter what tree architecture do we use so we're going to talk about that now", "tokens": [865, 11, 370, 797, 257, 9848, 13075, 437, 4230, 9482, 360, 321, 764, 370, 321, 434, 516, 281, 751, 466, 300, 586], "temperature": 0.0, "avg_logprob": -0.1883236327261295, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.6441757907159626e-05}, {"id": 695, "seek": 356204, "start": 3568.8, "end": 3570.8, "text": " Can you pass that to dinner?", "tokens": [1664, 291, 1320, 300, 281, 6148, 30], "temperature": 0.0, "avg_logprob": -0.1883236327261295, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.6441757907159626e-05}, {"id": 696, "seek": 356204, "start": 3573.08, "end": 3578.24, "text": " Yeah, I was just trying to understand how this random forest actually makes sense for continuous variables", "tokens": [865, 11, 286, 390, 445, 1382, 281, 1223, 577, 341, 4974, 6719, 767, 1669, 2020, 337, 10957, 9102], "temperature": 0.0, "avg_logprob": -0.1883236327261295, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.6441757907159626e-05}, {"id": 697, "seek": 356204, "start": 3578.24, "end": 3578.88, "text": " I mean", "tokens": [286, 914], "temperature": 0.0, "avg_logprob": -0.1883236327261295, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.6441757907159626e-05}, {"id": 698, "seek": 356204, "start": 3578.88, "end": 3585.16, "text": " I'm assuming that you build a tree structure and the last final nodes you'd be saying like maybe this node represents", "tokens": [286, 478, 11926, 300, 291, 1322, 257, 4230, 3877, 293, 264, 1036, 2572, 13891, 291, 1116, 312, 1566, 411, 1310, 341, 9984, 8855], "temperature": 0.0, "avg_logprob": -0.1883236327261295, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.6441757907159626e-05}, {"id": 699, "seek": 356204, "start": 3585.16, "end": 3590.12, "text": " Maybe a category a or a category B, but how does it make sense for a continuous target?", "tokens": [2704, 257, 7719, 257, 420, 257, 7719, 363, 11, 457, 577, 775, 309, 652, 2020, 337, 257, 10957, 3779, 30], "temperature": 0.0, "avg_logprob": -0.1883236327261295, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.6441757907159626e-05}, {"id": 700, "seek": 359012, "start": 3590.12, "end": 3595.24, "text": " So this is actually what we have here, and so the value here is the average", "tokens": [407, 341, 307, 767, 437, 321, 362, 510, 11, 293, 370, 264, 2158, 510, 307, 264, 4274], "temperature": 0.0, "avg_logprob": -0.20804059374463427, "compression_ratio": 1.7317073170731707, "no_speech_prob": 1.1365575574018294e-06}, {"id": 701, "seek": 359012, "start": 3596.04, "end": 3598.04, "text": " So this is the average", "tokens": [407, 341, 307, 264, 4274], "temperature": 0.0, "avg_logprob": -0.20804059374463427, "compression_ratio": 1.7317073170731707, "no_speech_prob": 1.1365575574018294e-06}, {"id": 702, "seek": 359012, "start": 3598.04, "end": 3600.88, "text": " Log of price for this subgroup", "tokens": [10824, 295, 3218, 337, 341, 1422, 17377], "temperature": 0.0, "avg_logprob": -0.20804059374463427, "compression_ratio": 1.7317073170731707, "no_speech_prob": 1.1365575574018294e-06}, {"id": 703, "seek": 359012, "start": 3600.88, "end": 3606.7799999999997, "text": " And that's all we do the prediction is the average of the value of the dependent variable in that leaf node", "tokens": [400, 300, 311, 439, 321, 360, 264, 17630, 307, 264, 4274, 295, 264, 2158, 295, 264, 12334, 7006, 294, 300, 10871, 9984], "temperature": 0.0, "avg_logprob": -0.20804059374463427, "compression_ratio": 1.7317073170731707, "no_speech_prob": 1.1365575574018294e-06}, {"id": 704, "seek": 359012, "start": 3608.96, "end": 3613.6, "text": " Finally if you have just like 10 leaf nodes you just have 10 values yes", "tokens": [6288, 498, 291, 362, 445, 411, 1266, 10871, 13891, 291, 445, 362, 1266, 4190, 2086], "temperature": 0.0, "avg_logprob": -0.20804059374463427, "compression_ratio": 1.7317073170731707, "no_speech_prob": 1.1365575574018294e-06}, {"id": 705, "seek": 359012, "start": 3614.4, "end": 3617.2, "text": " That's well if it was only one tree all right", "tokens": [663, 311, 731, 498, 309, 390, 787, 472, 4230, 439, 558], "temperature": 0.0, "avg_logprob": -0.20804059374463427, "compression_ratio": 1.7317073170731707, "no_speech_prob": 1.1365575574018294e-06}, {"id": 706, "seek": 361720, "start": 3617.2, "end": 3622.5, "text": " So a couple of things to remember the first is that by default we're actually going to train the tree all the way down until", "tokens": [407, 257, 1916, 295, 721, 281, 1604, 264, 700, 307, 300, 538, 7576, 321, 434, 767, 516, 281, 3847, 264, 4230, 439, 264, 636, 760, 1826], "temperature": 0.0, "avg_logprob": -0.16901657104492188, "compression_ratio": 1.7226890756302522, "no_speech_prob": 8.851543498167302e-07}, {"id": 707, "seek": 361720, "start": 3622.96, "end": 3624.96, "text": " The leaf nodes are of size one", "tokens": [440, 10871, 13891, 366, 295, 2744, 472], "temperature": 0.0, "avg_logprob": -0.16901657104492188, "compression_ratio": 1.7226890756302522, "no_speech_prob": 8.851543498167302e-07}, {"id": 708, "seek": 361720, "start": 3625.2, "end": 3629.7599999999998, "text": " Which means for a data set with n rows we're going to have n leaf nodes", "tokens": [3013, 1355, 337, 257, 1412, 992, 365, 297, 13241, 321, 434, 516, 281, 362, 297, 10871, 13891], "temperature": 0.0, "avg_logprob": -0.16901657104492188, "compression_ratio": 1.7226890756302522, "no_speech_prob": 8.851543498167302e-07}, {"id": 709, "seek": 361720, "start": 3629.7599999999998, "end": 3636.0, "text": " And then we're going to have multiple trees which we average together right so in practice", "tokens": [400, 550, 321, 434, 516, 281, 362, 3866, 5852, 597, 321, 4274, 1214, 558, 370, 294, 3124], "temperature": 0.0, "avg_logprob": -0.16901657104492188, "compression_ratio": 1.7226890756302522, "no_speech_prob": 8.851543498167302e-07}, {"id": 710, "seek": 361720, "start": 3636.7599999999998, "end": 3640.4199999999996, "text": " We're going to have a you know lots of different possible values", "tokens": [492, 434, 516, 281, 362, 257, 291, 458, 3195, 295, 819, 1944, 4190], "temperature": 0.0, "avg_logprob": -0.16901657104492188, "compression_ratio": 1.7226890756302522, "no_speech_prob": 8.851543498167302e-07}, {"id": 711, "seek": 361720, "start": 3641.2, "end": 3643.2, "text": " It's a question behind you", "tokens": [467, 311, 257, 1168, 2261, 291], "temperature": 0.0, "avg_logprob": -0.16901657104492188, "compression_ratio": 1.7226890756302522, "no_speech_prob": 8.851543498167302e-07}, {"id": 712, "seek": 364320, "start": 3643.2, "end": 3651.7599999999998, "text": " So for the continuous variable how do we decide like which value to split out because there can be many values we try every possible", "tokens": [407, 337, 264, 10957, 7006, 577, 360, 321, 4536, 411, 597, 2158, 281, 7472, 484, 570, 456, 393, 312, 867, 4190, 321, 853, 633, 1944], "temperature": 0.0, "avg_logprob": -0.2300692338209886, "compression_ratio": 1.5905511811023623, "no_speech_prob": 2.7693888569046976e-06}, {"id": 713, "seek": 364320, "start": 3653.3999999999996, "end": 3655.3999999999996, "text": " Value of that in the training set", "tokens": [39352, 295, 300, 294, 264, 3097, 992], "temperature": 0.0, "avg_logprob": -0.2300692338209886, "compression_ratio": 1.5905511811023623, "no_speech_prob": 2.7693888569046976e-06}, {"id": 714, "seek": 364320, "start": 3656.2, "end": 3658.96, "text": " Won't it be computationally computationally expensive?", "tokens": [14710, 380, 309, 312, 24903, 379, 24903, 379, 5124, 30], "temperature": 0.0, "avg_logprob": -0.2300692338209886, "compression_ratio": 1.5905511811023623, "no_speech_prob": 2.7693888569046976e-06}, {"id": 715, "seek": 364320, "start": 3658.96, "end": 3664.7599999999998, "text": " And this is where it's very good to remember that your CPU's performance is measured in gigahertz", "tokens": [400, 341, 307, 689, 309, 311, 588, 665, 281, 1604, 300, 428, 13199, 311, 3389, 307, 12690, 294, 8741, 64, 35655], "temperature": 0.0, "avg_logprob": -0.2300692338209886, "compression_ratio": 1.5905511811023623, "no_speech_prob": 2.7693888569046976e-06}, {"id": 716, "seek": 364320, "start": 3665.16, "end": 3671.12, "text": " Which is billions of clock cycles per second and it has multiple cores and each core", "tokens": [3013, 307, 17375, 295, 7830, 17796, 680, 1150, 293, 309, 575, 3866, 24826, 293, 1184, 4965], "temperature": 0.0, "avg_logprob": -0.2300692338209886, "compression_ratio": 1.5905511811023623, "no_speech_prob": 2.7693888569046976e-06}, {"id": 717, "seek": 367112, "start": 3671.12, "end": 3677.8399999999997, "text": " Has something called SIMD single instruction multiple data where it can direct up to eight computations per core at once", "tokens": [8646, 746, 1219, 24738, 35, 2167, 10951, 3866, 1412, 689, 309, 393, 2047, 493, 281, 3180, 2807, 763, 680, 4965, 412, 1564], "temperature": 0.0, "avg_logprob": -0.2170544942220052, "compression_ratio": 1.5748987854251013, "no_speech_prob": 9.132526770372351e-07}, {"id": 718, "seek": 367112, "start": 3678.56, "end": 3683.0, "text": " And then if you do it on the GPU the performance is measured in", "tokens": [400, 550, 498, 291, 360, 309, 322, 264, 18407, 264, 3389, 307, 12690, 294], "temperature": 0.0, "avg_logprob": -0.2170544942220052, "compression_ratio": 1.5748987854251013, "no_speech_prob": 9.132526770372351e-07}, {"id": 719, "seek": 367112, "start": 3683.52, "end": 3684.96, "text": " teraflops", "tokens": [256, 1663, 3423, 3370], "temperature": 0.0, "avg_logprob": -0.2170544942220052, "compression_ratio": 1.5748987854251013, "no_speech_prob": 9.132526770372351e-07}, {"id": 720, "seek": 367112, "start": 3684.96, "end": 3691.12, "text": " So trillions of floating-point operations per second and so this is where when it comes to", "tokens": [407, 504, 46279, 295, 12607, 12, 6053, 7705, 680, 1150, 293, 370, 341, 307, 689, 562, 309, 1487, 281], "temperature": 0.0, "avg_logprob": -0.2170544942220052, "compression_ratio": 1.5748987854251013, "no_speech_prob": 9.132526770372351e-07}, {"id": 721, "seek": 367112, "start": 3691.72, "end": 3699.0, "text": " Designing algorithms, it's very difficult for us mere humans to realize how stupid algorithms should be", "tokens": [3885, 9676, 14642, 11, 309, 311, 588, 2252, 337, 505, 8401, 6255, 281, 4325, 577, 6631, 14642, 820, 312], "temperature": 0.0, "avg_logprob": -0.2170544942220052, "compression_ratio": 1.5748987854251013, "no_speech_prob": 9.132526770372351e-07}, {"id": 722, "seek": 369900, "start": 3699.0, "end": 3704.6, "text": " Given how fast today's computers are so yeah, it's quite a few operations", "tokens": [18600, 577, 2370, 965, 311, 10807, 366, 370, 1338, 11, 309, 311, 1596, 257, 1326, 7705], "temperature": 0.0, "avg_logprob": -0.3077902317047119, "compression_ratio": 1.5120772946859904, "no_speech_prob": 2.2125102987047285e-05}, {"id": 723, "seek": 369900, "start": 3705.32, "end": 3708.72, "text": " But at trillions per second you hardly notice it", "tokens": [583, 412, 504, 46279, 680, 1150, 291, 13572, 3449, 309], "temperature": 0.0, "avg_logprob": -0.3077902317047119, "compression_ratio": 1.5120772946859904, "no_speech_prob": 2.2125102987047285e-05}, {"id": 724, "seek": 369900, "start": 3709.84, "end": 3711.84, "text": " Masha", "tokens": [376, 12137], "temperature": 0.0, "avg_logprob": -0.3077902317047119, "compression_ratio": 1.5120772946859904, "no_speech_prob": 2.2125102987047285e-05}, {"id": 725, "seek": 369900, "start": 3712.32, "end": 3716.88, "text": " I have a question so essentially at each mode we make a decision", "tokens": [286, 362, 257, 1168, 370, 4476, 412, 1184, 4391, 321, 652, 257, 3537], "temperature": 0.0, "avg_logprob": -0.3077902317047119, "compression_ratio": 1.5120772946859904, "no_speech_prob": 2.2125102987047285e-05}, {"id": 726, "seek": 369900, "start": 3718.04, "end": 3722.8, "text": " Like which category to which variable to use and which clip point yes", "tokens": [1743, 597, 7719, 281, 597, 7006, 281, 764, 293, 597, 7353, 935, 2086], "temperature": 0.0, "avg_logprob": -0.3077902317047119, "compression_ratio": 1.5120772946859904, "no_speech_prob": 2.2125102987047285e-05}, {"id": 727, "seek": 369900, "start": 3723.08, "end": 3727.48, "text": " Yeah, but one thing I can't understand so we have", "tokens": [865, 11, 457, 472, 551, 286, 393, 380, 1223, 370, 321, 362], "temperature": 0.0, "avg_logprob": -0.3077902317047119, "compression_ratio": 1.5120772946859904, "no_speech_prob": 2.2125102987047285e-05}, {"id": 728, "seek": 372748, "start": 3727.48, "end": 3734.16, "text": " MSE a calculated for each node right so this is kind of our one of the decision criteria", "tokens": [376, 5879, 257, 15598, 337, 1184, 9984, 558, 370, 341, 307, 733, 295, 527, 472, 295, 264, 3537, 11101], "temperature": 0.0, "avg_logprob": -0.26549344380696616, "compression_ratio": 1.6514285714285715, "no_speech_prob": 3.7852728382858913e-06}, {"id": 729, "seek": 372748, "start": 3734.2400000000002, "end": 3739.6, "text": " But this MSE it is calculated for which model like which model underlies", "tokens": [583, 341, 376, 5879, 309, 307, 15598, 337, 597, 2316, 411, 597, 2316, 833, 24119], "temperature": 0.0, "avg_logprob": -0.26549344380696616, "compression_ratio": 1.6514285714285715, "no_speech_prob": 3.7852728382858913e-06}, {"id": 730, "seek": 372748, "start": 3740.4, "end": 3742.56, "text": " like the model is the model is", "tokens": [411, 264, 2316, 307, 264, 2316, 307], "temperature": 0.0, "avg_logprob": -0.26549344380696616, "compression_ratio": 1.6514285714285715, "no_speech_prob": 3.7852728382858913e-06}, {"id": 731, "seek": 372748, "start": 3743.8, "end": 3748.96, "text": " For the initial root mode is what if we just predicted the average", "tokens": [1171, 264, 5883, 5593, 4391, 307, 437, 498, 321, 445, 19147, 264, 4274], "temperature": 0.0, "avg_logprob": -0.26549344380696616, "compression_ratio": 1.6514285714285715, "no_speech_prob": 3.7852728382858913e-06}, {"id": 732, "seek": 372748, "start": 3750.12, "end": 3752.12, "text": " Right which is here is 10.098", "tokens": [1779, 597, 307, 510, 307, 1266, 13, 13811, 23], "temperature": 0.0, "avg_logprob": -0.26549344380696616, "compression_ratio": 1.6514285714285715, "no_speech_prob": 3.7852728382858913e-06}, {"id": 733, "seek": 375212, "start": 3752.12, "end": 3758.56, "text": " Oh just just the average and then the next model is what if we predicted the average of", "tokens": [876, 445, 445, 264, 4274, 293, 550, 264, 958, 2316, 307, 437, 498, 321, 19147, 264, 4274, 295], "temperature": 0.0, "avg_logprob": -0.22385861323429987, "compression_ratio": 2.0585585585585586, "no_speech_prob": 1.0783063771668822e-05}, {"id": 734, "seek": 375212, "start": 3759.04, "end": 3761.56, "text": " those people with coupler system equals false and", "tokens": [729, 561, 365, 1384, 22732, 1185, 6915, 7908, 293], "temperature": 0.0, "avg_logprob": -0.22385861323429987, "compression_ratio": 2.0585585585585586, "no_speech_prob": 1.0783063771668822e-05}, {"id": 735, "seek": 375212, "start": 3762.4, "end": 3769.48, "text": " For those people with coupler system equals true, and then the next is what if we predicted the average of coupler systems equals true?", "tokens": [1171, 729, 561, 365, 1384, 22732, 1185, 6915, 2074, 11, 293, 550, 264, 958, 307, 437, 498, 321, 19147, 264, 4274, 295, 1384, 22732, 3652, 6915, 2074, 30], "temperature": 0.0, "avg_logprob": -0.22385861323429987, "compression_ratio": 2.0585585585585586, "no_speech_prob": 1.0783063771668822e-05}, {"id": 736, "seek": 375212, "start": 3770.0, "end": 3775.52, "text": " Here made less than 96 is it always average or we can use median or we can even run", "tokens": [1692, 1027, 1570, 813, 24124, 307, 309, 1009, 4274, 420, 321, 393, 764, 26779, 420, 321, 393, 754, 1190], "temperature": 0.0, "avg_logprob": -0.22385861323429987, "compression_ratio": 2.0585585585585586, "no_speech_prob": 1.0783063771668822e-05}, {"id": 737, "seek": 377552, "start": 3775.52, "end": 3781.48, "text": " Linear regression there's all kinds of things we could do in practice the average works really well", "tokens": [14670, 289, 24590, 456, 311, 439, 3685, 295, 721, 321, 727, 360, 294, 3124, 264, 4274, 1985, 534, 731], "temperature": 0.0, "avg_logprob": -0.1882678736811099, "compression_ratio": 1.7196652719665273, "no_speech_prob": 5.0147009460488334e-06}, {"id": 738, "seek": 377552, "start": 3782.28, "end": 3784.28, "text": " there are", "tokens": [456, 366], "temperature": 0.0, "avg_logprob": -0.1882678736811099, "compression_ratio": 1.7196652719665273, "no_speech_prob": 5.0147009460488334e-06}, {"id": 739, "seek": 377552, "start": 3784.44, "end": 3791.2, "text": " There are types of they're not called random forests, but there are kinds of trees where the leaf nodes are independent linear aggressions", "tokens": [821, 366, 3467, 295, 436, 434, 406, 1219, 4974, 21700, 11, 457, 456, 366, 3685, 295, 5852, 689, 264, 10871, 13891, 366, 6695, 8213, 8939, 626], "temperature": 0.0, "avg_logprob": -0.1882678736811099, "compression_ratio": 1.7196652719665273, "no_speech_prob": 5.0147009460488334e-06}, {"id": 740, "seek": 377552, "start": 3791.7599999999998, "end": 3797.96, "text": " They're not terribly widely used, but there are certainly researchers who have worked on them, okay? Thank you", "tokens": [814, 434, 406, 22903, 13371, 1143, 11, 457, 456, 366, 3297, 10309, 567, 362, 2732, 322, 552, 11, 1392, 30, 1044, 291], "temperature": 0.0, "avg_logprob": -0.1882678736811099, "compression_ratio": 1.7196652719665273, "no_speech_prob": 5.0147009460488334e-06}, {"id": 741, "seek": 377552, "start": 3798.84, "end": 3801.04, "text": " And pass it back over that afford and then to check", "tokens": [400, 1320, 309, 646, 670, 300, 6157, 293, 550, 281, 1520], "temperature": 0.0, "avg_logprob": -0.1882678736811099, "compression_ratio": 1.7196652719665273, "no_speech_prob": 5.0147009460488334e-06}, {"id": 742, "seek": 380104, "start": 3801.04, "end": 3803.04, "text": " you", "tokens": [291], "temperature": 0.0, "avg_logprob": -0.26828401299971566, "compression_ratio": 1.6444444444444444, "no_speech_prob": 3.905437097273534e-06}, {"id": 743, "seek": 380104, "start": 3805.08, "end": 3807.96, "text": " So this tree has a depth of three yeah, and", "tokens": [407, 341, 4230, 575, 257, 7161, 295, 1045, 1338, 11, 293], "temperature": 0.0, "avg_logprob": -0.26828401299971566, "compression_ratio": 1.6444444444444444, "no_speech_prob": 3.905437097273534e-06}, {"id": 744, "seek": 380104, "start": 3808.6, "end": 3814.68, "text": " Then I on one of the next commands we get rid of the max depth yes the tree without the max depth", "tokens": [1396, 286, 322, 472, 295, 264, 958, 16901, 321, 483, 3973, 295, 264, 11469, 7161, 2086, 264, 4230, 1553, 264, 11469, 7161], "temperature": 0.0, "avg_logprob": -0.26828401299971566, "compression_ratio": 1.6444444444444444, "no_speech_prob": 3.905437097273534e-06}, {"id": 745, "seek": 380104, "start": 3815.72, "end": 3819.6, "text": " Does that contain the tree with with the depth of three yeah?", "tokens": [4402, 300, 5304, 264, 4230, 365, 365, 264, 7161, 295, 1045, 1338, 30], "temperature": 0.0, "avg_logprob": -0.26828401299971566, "compression_ratio": 1.6444444444444444, "no_speech_prob": 3.905437097273534e-06}, {"id": 746, "seek": 380104, "start": 3821.48, "end": 3825.4, "text": " Yeah, well except in this case we've added randomness, but if you turn boots jumping off", "tokens": [865, 11, 731, 3993, 294, 341, 1389, 321, 600, 3869, 4974, 1287, 11, 457, 498, 291, 1261, 15194, 11233, 766], "temperature": 0.0, "avg_logprob": -0.26828401299971566, "compression_ratio": 1.6444444444444444, "no_speech_prob": 3.905437097273534e-06}, {"id": 747, "seek": 382540, "start": 3825.4, "end": 3830.48, "text": " then yeah, the the deeper tree will you know the", "tokens": [550, 1338, 11, 264, 264, 7731, 4230, 486, 291, 458, 264], "temperature": 0.0, "avg_logprob": -0.29546335339546204, "compression_ratio": 1.5297619047619047, "no_speech_prob": 2.4060805117187556e-06}, {"id": 748, "seek": 382540, "start": 3831.52, "end": 3835.2000000000003, "text": " Less deep tree would be how it starts and then it just keeps spinning okay?", "tokens": [18649, 2452, 4230, 576, 312, 577, 309, 3719, 293, 550, 309, 445, 5965, 15640, 1392, 30], "temperature": 0.0, "avg_logprob": -0.29546335339546204, "compression_ratio": 1.5297619047619047, "no_speech_prob": 2.4060805117187556e-06}, {"id": 749, "seek": 382540, "start": 3839.84, "end": 3842.7200000000003, "text": " So you have many trees you're going to have", "tokens": [407, 291, 362, 867, 5852, 291, 434, 516, 281, 362], "temperature": 0.0, "avg_logprob": -0.29546335339546204, "compression_ratio": 1.5297619047619047, "no_speech_prob": 2.4060805117187556e-06}, {"id": 750, "seek": 382540, "start": 3843.76, "end": 3849.26, "text": " Different leaf nodes across trees hopefully so we want so how do you average leaf nodes?", "tokens": [20825, 10871, 13891, 2108, 5852, 4696, 370, 321, 528, 370, 577, 360, 291, 4274, 10871, 13891, 30], "temperature": 0.0, "avg_logprob": -0.29546335339546204, "compression_ratio": 1.5297619047619047, "no_speech_prob": 2.4060805117187556e-06}, {"id": 751, "seek": 384926, "start": 3849.26, "end": 3857.34, "text": " Across different trees, so we just take the first row in the validation set we run it through the first tree", "tokens": [34527, 819, 5852, 11, 370, 321, 445, 747, 264, 700, 5386, 294, 264, 24071, 992, 321, 1190, 309, 807, 264, 700, 4230], "temperature": 0.0, "avg_logprob": -0.1790902519226074, "compression_ratio": 1.7352941176470589, "no_speech_prob": 3.187534048265661e-06}, {"id": 752, "seek": 384926, "start": 3857.7400000000002, "end": 3863.5400000000004, "text": " We find its average nine point two eight then do it through the next tree find its average in the second tree", "tokens": [492, 915, 1080, 4274, 4949, 935, 732, 3180, 550, 360, 309, 807, 264, 958, 4230, 915, 1080, 4274, 294, 264, 1150, 4230], "temperature": 0.0, "avg_logprob": -0.1790902519226074, "compression_ratio": 1.7352941176470589, "no_speech_prob": 3.187534048265661e-06}, {"id": 753, "seek": 384926, "start": 3863.82, "end": 3871.38, "text": " 9.95 and so forth and we're about to do that so you'll see it okay, so let's try it right so", "tokens": [1722, 13, 15718, 293, 370, 5220, 293, 321, 434, 466, 281, 360, 300, 370, 291, 603, 536, 309, 1392, 11, 370, 718, 311, 853, 309, 558, 370], "temperature": 0.0, "avg_logprob": -0.1790902519226074, "compression_ratio": 1.7352941176470589, "no_speech_prob": 3.187534048265661e-06}, {"id": 754, "seek": 384926, "start": 3872.26, "end": 3874.26, "text": " After you've built a random forest", "tokens": [2381, 291, 600, 3094, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.1790902519226074, "compression_ratio": 1.7352941176470589, "no_speech_prob": 3.187534048265661e-06}, {"id": 755, "seek": 387426, "start": 3874.26, "end": 3879.1000000000004, "text": " Each tree is stored in this attribute called estimators underscore", "tokens": [6947, 4230, 307, 12187, 294, 341, 19667, 1219, 8017, 3391, 37556], "temperature": 0.0, "avg_logprob": -0.11772486176153626, "compression_ratio": 1.7161016949152543, "no_speech_prob": 1.7061771586668328e-06}, {"id": 756, "seek": 387426, "start": 3879.94, "end": 3886.92, "text": " Okay, so one of the things that you guys need to be very very comfortable with is using list comprehensions", "tokens": [1033, 11, 370, 472, 295, 264, 721, 300, 291, 1074, 643, 281, 312, 588, 588, 4619, 365, 307, 1228, 1329, 10753, 8302], "temperature": 0.0, "avg_logprob": -0.11772486176153626, "compression_ratio": 1.7161016949152543, "no_speech_prob": 1.7061771586668328e-06}, {"id": 757, "seek": 387426, "start": 3887.1800000000003, "end": 3893.3, "text": " Okay, so I hope you've all been practicing okay, so here. I'm using a list comprehension to go through each tree in my model", "tokens": [1033, 11, 370, 286, 1454, 291, 600, 439, 668, 11350, 1392, 11, 370, 510, 13, 286, 478, 1228, 257, 1329, 44991, 281, 352, 807, 1184, 4230, 294, 452, 2316], "temperature": 0.0, "avg_logprob": -0.11772486176153626, "compression_ratio": 1.7161016949152543, "no_speech_prob": 1.7061771586668328e-06}, {"id": 758, "seek": 387426, "start": 3893.3, "end": 3899.94, "text": " I'm going to call predict on it with my validation set and so that's going to give me a list of", "tokens": [286, 478, 516, 281, 818, 6069, 322, 309, 365, 452, 24071, 992, 293, 370, 300, 311, 516, 281, 976, 385, 257, 1329, 295], "temperature": 0.0, "avg_logprob": -0.11772486176153626, "compression_ratio": 1.7161016949152543, "no_speech_prob": 1.7061771586668328e-06}, {"id": 759, "seek": 387426, "start": 3901.1400000000003, "end": 3903.1400000000003, "text": " arrays of", "tokens": [41011, 295], "temperature": 0.0, "avg_logprob": -0.11772486176153626, "compression_ratio": 1.7161016949152543, "no_speech_prob": 1.7061771586668328e-06}, {"id": 760, "seek": 390314, "start": 3903.14, "end": 3909.6, "text": " Predictions so each array will be all of the predictions for that tree, and I have ten trees", "tokens": [32969, 15607, 370, 1184, 10225, 486, 312, 439, 295, 264, 21264, 337, 300, 4230, 11, 293, 286, 362, 2064, 5852], "temperature": 0.0, "avg_logprob": -0.21945073869493273, "compression_ratio": 1.5738636363636365, "no_speech_prob": 3.3931232792383526e-06}, {"id": 761, "seek": 390314, "start": 3910.7, "end": 3912.7, "text": " NP dot stack", "tokens": [38611, 5893, 8630], "temperature": 0.0, "avg_logprob": -0.21945073869493273, "compression_ratio": 1.5738636363636365, "no_speech_prob": 3.3931232792383526e-06}, {"id": 762, "seek": 390314, "start": 3913.02, "end": 3917.5, "text": " Concatenates them together on a new axis so after I run this and", "tokens": [18200, 7186, 1024, 552, 1214, 322, 257, 777, 10298, 370, 934, 286, 1190, 341, 293], "temperature": 0.0, "avg_logprob": -0.21945073869493273, "compression_ratio": 1.5738636363636365, "no_speech_prob": 3.3931232792383526e-06}, {"id": 763, "seek": 390314, "start": 3919.9, "end": 3921.7, "text": " Call dot shape", "tokens": [7807, 5893, 3909], "temperature": 0.0, "avg_logprob": -0.21945073869493273, "compression_ratio": 1.5738636363636365, "no_speech_prob": 3.3931232792383526e-06}, {"id": 764, "seek": 390314, "start": 3921.7, "end": 3928.1, "text": " You can see I now have the first axis ten means I have my ten different sets of predictions", "tokens": [509, 393, 536, 286, 586, 362, 264, 700, 10298, 2064, 1355, 286, 362, 452, 2064, 819, 6352, 295, 21264], "temperature": 0.0, "avg_logprob": -0.21945073869493273, "compression_ratio": 1.5738636363636365, "no_speech_prob": 3.3931232792383526e-06}, {"id": 765, "seek": 392810, "start": 3928.1, "end": 3935.02, "text": " And for each one my validation set is a size 12,000 so here are my 12,000 predictions for each of the ten trees", "tokens": [400, 337, 1184, 472, 452, 24071, 992, 307, 257, 2744, 2272, 11, 1360, 370, 510, 366, 452, 2272, 11, 1360, 21264, 337, 1184, 295, 264, 2064, 5852], "temperature": 0.0, "avg_logprob": -0.20090097126207854, "compression_ratio": 1.8518518518518519, "no_speech_prob": 8.851550887811754e-07}, {"id": 766, "seek": 392810, "start": 3935.98, "end": 3937.98, "text": " right, so", "tokens": [558, 11, 370], "temperature": 0.0, "avg_logprob": -0.20090097126207854, "compression_ratio": 1.8518518518518519, "no_speech_prob": 8.851550887811754e-07}, {"id": 767, "seek": 392810, "start": 3938.18, "end": 3943.66, "text": " Let's take the first row of that and print it out and so here are", "tokens": [961, 311, 747, 264, 700, 5386, 295, 300, 293, 4482, 309, 484, 293, 370, 510, 366], "temperature": 0.0, "avg_logprob": -0.20090097126207854, "compression_ratio": 1.8518518518518519, "no_speech_prob": 8.851550887811754e-07}, {"id": 768, "seek": 392810, "start": 3945.02, "end": 3948.56, "text": " What we're just saying here are ten predictions one from each tree", "tokens": [708, 321, 434, 445, 1566, 510, 366, 2064, 21264, 472, 490, 1184, 4230], "temperature": 0.0, "avg_logprob": -0.20090097126207854, "compression_ratio": 1.8518518518518519, "no_speech_prob": 8.851550887811754e-07}, {"id": 769, "seek": 392810, "start": 3949.18, "end": 3952.58, "text": " Okay, and so then if we say take the mean of that", "tokens": [1033, 11, 293, 370, 550, 498, 321, 584, 747, 264, 914, 295, 300], "temperature": 0.0, "avg_logprob": -0.20090097126207854, "compression_ratio": 1.8518518518518519, "no_speech_prob": 8.851550887811754e-07}, {"id": 770, "seek": 392810, "start": 3953.14, "end": 3955.7, "text": " here is the mean of those ten predictions and", "tokens": [510, 307, 264, 914, 295, 729, 2064, 21264, 293], "temperature": 0.0, "avg_logprob": -0.20090097126207854, "compression_ratio": 1.8518518518518519, "no_speech_prob": 8.851550887811754e-07}, {"id": 771, "seek": 395570, "start": 3955.7, "end": 3962.06, "text": " Then what was the actual the actual was nine point one our prediction was nine point?", "tokens": [1396, 437, 390, 264, 3539, 264, 3539, 390, 4949, 935, 472, 527, 17630, 390, 4949, 935, 30], "temperature": 0.0, "avg_logprob": -0.1672164515445107, "compression_ratio": 1.7551867219917012, "no_speech_prob": 1.733046588014986e-06}, {"id": 772, "seek": 395570, "start": 3962.06, "end": 3969.3399999999997, "text": " Oh seven so you see how like none of our individual trees had very good predictions, but the main of them was actually pretty good", "tokens": [876, 3407, 370, 291, 536, 577, 411, 6022, 295, 527, 2609, 5852, 632, 588, 665, 21264, 11, 457, 264, 2135, 295, 552, 390, 767, 1238, 665], "temperature": 0.0, "avg_logprob": -0.1672164515445107, "compression_ratio": 1.7551867219917012, "no_speech_prob": 1.733046588014986e-06}, {"id": 773, "seek": 395570, "start": 3969.98, "end": 3971.7, "text": " right and so", "tokens": [558, 293, 370], "temperature": 0.0, "avg_logprob": -0.1672164515445107, "compression_ratio": 1.7551867219917012, "no_speech_prob": 1.733046588014986e-06}, {"id": 774, "seek": 395570, "start": 3971.7, "end": 3976.3399999999997, "text": " When I talk about experimenting like Jupiter notebook is great for experimenting", "tokens": [1133, 286, 751, 466, 29070, 411, 24567, 21060, 307, 869, 337, 29070], "temperature": 0.0, "avg_logprob": -0.1672164515445107, "compression_ratio": 1.7551867219917012, "no_speech_prob": 1.733046588014986e-06}, {"id": 775, "seek": 395570, "start": 3976.3399999999997, "end": 3977.5, "text": " This is the kind of stuff", "tokens": [639, 307, 264, 733, 295, 1507], "temperature": 0.0, "avg_logprob": -0.1672164515445107, "compression_ratio": 1.7551867219917012, "no_speech_prob": 1.733046588014986e-06}, {"id": 776, "seek": 395570, "start": 3977.5, "end": 3983.14, "text": " I mean dig inside these objects and like look at them plot them take your own averages", "tokens": [286, 914, 2528, 1854, 613, 6565, 293, 411, 574, 412, 552, 7542, 552, 747, 428, 1065, 42257], "temperature": 0.0, "avg_logprob": -0.1672164515445107, "compression_ratio": 1.7551867219917012, "no_speech_prob": 1.733046588014986e-06}, {"id": 777, "seek": 398314, "start": 3983.14, "end": 3987.66, "text": " Crosscheck to make sure that they work the way you thought they did write your own implementation of", "tokens": [11623, 15723, 281, 652, 988, 300, 436, 589, 264, 636, 291, 1194, 436, 630, 2464, 428, 1065, 11420, 295], "temperature": 0.0, "avg_logprob": -0.2421488004310109, "compression_ratio": 1.732, "no_speech_prob": 1.034850356518291e-06}, {"id": 778, "seek": 398314, "start": 3988.2999999999997, "end": 3993.3199999999997, "text": " R-squared make sure it's the same as a psychic learn version plot it like here's an interesting plot. I did", "tokens": [497, 12, 33292, 1642, 652, 988, 309, 311, 264, 912, 382, 257, 35406, 1466, 3037, 7542, 309, 411, 510, 311, 364, 1880, 7542, 13, 286, 630], "temperature": 0.0, "avg_logprob": -0.2421488004310109, "compression_ratio": 1.732, "no_speech_prob": 1.034850356518291e-06}, {"id": 779, "seek": 398314, "start": 3993.8599999999997, "end": 3995.98, "text": " Let's go through each test the ten trees", "tokens": [961, 311, 352, 807, 1184, 1500, 264, 2064, 5852], "temperature": 0.0, "avg_logprob": -0.2421488004310109, "compression_ratio": 1.732, "no_speech_prob": 1.034850356518291e-06}, {"id": 780, "seek": 398314, "start": 3997.02, "end": 4003.72, "text": " Right and then take the mean of all of the predictions up to the i3", "tokens": [1779, 293, 550, 747, 264, 914, 295, 439, 295, 264, 21264, 493, 281, 264, 741, 18], "temperature": 0.0, "avg_logprob": -0.2421488004310109, "compression_ratio": 1.732, "no_speech_prob": 1.034850356518291e-06}, {"id": 781, "seek": 398314, "start": 4004.22, "end": 4006.22, "text": " Right so let's start by predicting", "tokens": [1779, 370, 718, 311, 722, 538, 32884], "temperature": 0.0, "avg_logprob": -0.2421488004310109, "compression_ratio": 1.732, "no_speech_prob": 1.034850356518291e-06}, {"id": 782, "seek": 398314, "start": 4006.54, "end": 4010.8199999999997, "text": " Just based on the first tree then the first two trees than the first three trees", "tokens": [1449, 2361, 322, 264, 700, 4230, 550, 264, 700, 732, 5852, 813, 264, 700, 1045, 5852], "temperature": 0.0, "avg_logprob": -0.2421488004310109, "compression_ratio": 1.732, "no_speech_prob": 1.034850356518291e-06}, {"id": 783, "seek": 401082, "start": 4010.82, "end": 4014.02, "text": " And let's then plot the R squared", "tokens": [400, 718, 311, 550, 7542, 264, 497, 8889], "temperature": 0.0, "avg_logprob": -0.1953394294965385, "compression_ratio": 1.848623853211009, "no_speech_prob": 2.332066515009501e-06}, {"id": 784, "seek": 401082, "start": 4014.6200000000003, "end": 4017.06, "text": " So here's the R squared of just the first tree", "tokens": [407, 510, 311, 264, 497, 8889, 295, 445, 264, 700, 4230], "temperature": 0.0, "avg_logprob": -0.1953394294965385, "compression_ratio": 1.848623853211009, "no_speech_prob": 2.332066515009501e-06}, {"id": 785, "seek": 401082, "start": 4017.3, "end": 4022.6600000000003, "text": " Here's the R squared of the first two trees three trees four trees blah blah blah blah up to ten trees", "tokens": [1692, 311, 264, 497, 8889, 295, 264, 700, 732, 5852, 1045, 5852, 1451, 5852, 12288, 12288, 12288, 12288, 493, 281, 2064, 5852], "temperature": 0.0, "avg_logprob": -0.1953394294965385, "compression_ratio": 1.848623853211009, "no_speech_prob": 2.332066515009501e-06}, {"id": 786, "seek": 401082, "start": 4022.6600000000003, "end": 4027.82, "text": " And so not surprisingly R squared keeps improving right because the more", "tokens": [400, 370, 406, 17600, 497, 8889, 5965, 11470, 558, 570, 264, 544], "temperature": 0.0, "avg_logprob": -0.1953394294965385, "compression_ratio": 1.848623853211009, "no_speech_prob": 2.332066515009501e-06}, {"id": 787, "seek": 401082, "start": 4028.6200000000003, "end": 4033.78, "text": " Estimators we have the more bagging that we're doing the more it's well. It's going to generalize", "tokens": [4410, 332, 3391, 321, 362, 264, 544, 3411, 3249, 300, 321, 434, 884, 264, 544, 309, 311, 731, 13, 467, 311, 516, 281, 2674, 1125], "temperature": 0.0, "avg_logprob": -0.1953394294965385, "compression_ratio": 1.848623853211009, "no_speech_prob": 2.332066515009501e-06}, {"id": 788, "seek": 401082, "start": 4034.26, "end": 4038.26, "text": " Right and you should find that that number there", "tokens": [1779, 293, 291, 820, 915, 300, 300, 1230, 456], "temperature": 0.0, "avg_logprob": -0.1953394294965385, "compression_ratio": 1.848623853211009, "no_speech_prob": 2.332066515009501e-06}, {"id": 789, "seek": 403826, "start": 4038.26, "end": 4043.0800000000004, "text": " Bit under point eight six should match this number here", "tokens": [9101, 833, 935, 3180, 2309, 820, 2995, 341, 1230, 510], "temperature": 0.0, "avg_logprob": -0.2171110153198242, "compression_ratio": 1.624031007751938, "no_speech_prob": 3.2887328416109085e-06}, {"id": 790, "seek": 403826, "start": 4043.98, "end": 4045.94, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2171110153198242, "compression_ratio": 1.624031007751938, "no_speech_prob": 3.2887328416109085e-06}, {"id": 791, "seek": 403826, "start": 4045.94, "end": 4051.1000000000004, "text": " Let's rerun that yeah, okay, so that actually slightly above point eight six right so again", "tokens": [961, 311, 43819, 409, 300, 1338, 11, 1392, 11, 370, 300, 767, 4748, 3673, 935, 3180, 2309, 558, 370, 797], "temperature": 0.0, "avg_logprob": -0.2171110153198242, "compression_ratio": 1.624031007751938, "no_speech_prob": 3.2887328416109085e-06}, {"id": 792, "seek": 403826, "start": 4051.1000000000004, "end": 4055.46, "text": " These are all about the cross checks you can do the things you can visualize to deepen your understanding", "tokens": [1981, 366, 439, 466, 264, 3278, 13834, 291, 393, 360, 264, 721, 291, 393, 23273, 281, 45806, 428, 3701], "temperature": 0.0, "avg_logprob": -0.2171110153198242, "compression_ratio": 1.624031007751938, "no_speech_prob": 3.2887328416109085e-06}, {"id": 793, "seek": 403826, "start": 4055.5400000000004, "end": 4060.3, "text": " Okay, so as we add more trees our R squared improves. It seems to flatten out", "tokens": [1033, 11, 370, 382, 321, 909, 544, 5852, 527, 497, 8889, 24771, 13, 467, 2544, 281, 24183, 484], "temperature": 0.0, "avg_logprob": -0.2171110153198242, "compression_ratio": 1.624031007751938, "no_speech_prob": 3.2887328416109085e-06}, {"id": 794, "seek": 403826, "start": 4061.1400000000003, "end": 4065.46, "text": " After a while so we might guess that if we increase the number of estimators to 20", "tokens": [2381, 257, 1339, 370, 321, 1062, 2041, 300, 498, 321, 3488, 264, 1230, 295, 8017, 3391, 281, 945], "temperature": 0.0, "avg_logprob": -0.2171110153198242, "compression_ratio": 1.624031007751938, "no_speech_prob": 3.2887328416109085e-06}, {"id": 795, "seek": 406546, "start": 4065.46, "end": 4068.18, "text": " Right it's maybe not going to be that much better", "tokens": [1779, 309, 311, 1310, 406, 516, 281, 312, 300, 709, 1101], "temperature": 0.0, "avg_logprob": -0.21299294063023158, "compression_ratio": 1.6319018404907975, "no_speech_prob": 1.2098602155674598e-06}, {"id": 796, "seek": 406546, "start": 4073.26, "end": 4076.42, "text": " So let's see we've got point eight six two", "tokens": [407, 718, 311, 536, 321, 600, 658, 935, 3180, 2309, 732], "temperature": 0.0, "avg_logprob": -0.21299294063023158, "compression_ratio": 1.6319018404907975, "no_speech_prob": 1.2098602155674598e-06}, {"id": 797, "seek": 406546, "start": 4078.02, "end": 4083.42, "text": " Versus point eight six oh yeah, so doubling the number of trees didn't help very much, but double it again", "tokens": [12226, 301, 935, 3180, 2309, 1954, 1338, 11, 370, 33651, 264, 1230, 295, 5852, 994, 380, 854, 588, 709, 11, 457, 3834, 309, 797], "temperature": 0.0, "avg_logprob": -0.21299294063023158, "compression_ratio": 1.6319018404907975, "no_speech_prob": 1.2098602155674598e-06}, {"id": 798, "seek": 406546, "start": 4084.66, "end": 4089.84, "text": " Eight six seven double it again eight six nine so you can see like", "tokens": [17708, 2309, 3407, 3834, 309, 797, 3180, 2309, 4949, 370, 291, 393, 536, 411], "temperature": 0.0, "avg_logprob": -0.21299294063023158, "compression_ratio": 1.6319018404907975, "no_speech_prob": 1.2098602155674598e-06}, {"id": 799, "seek": 408984, "start": 4089.84, "end": 4096.56, "text": " There's some point at which you're going to you know not want to add more trees not because it's never going to get worse", "tokens": [821, 311, 512, 935, 412, 597, 291, 434, 516, 281, 291, 458, 406, 528, 281, 909, 544, 5852, 406, 570, 309, 311, 1128, 516, 281, 483, 5324], "temperature": 0.0, "avg_logprob": -0.24430237927483123, "compression_ratio": 1.7, "no_speech_prob": 1.0188065289185033e-06}, {"id": 800, "seek": 408984, "start": 4097.28, "end": 4101.52, "text": " All right because every tree is you know giving you more?", "tokens": [1057, 558, 570, 633, 4230, 307, 291, 458, 2902, 291, 544, 30], "temperature": 0.0, "avg_logprob": -0.24430237927483123, "compression_ratio": 1.7, "no_speech_prob": 1.0188065289185033e-06}, {"id": 801, "seek": 408984, "start": 4102.68, "end": 4107.96, "text": " Semi-random models to bag together right, but it's going to stop improving things much", "tokens": [318, 13372, 12, 3699, 298, 5245, 281, 3411, 1214, 558, 11, 457, 309, 311, 516, 281, 1590, 11470, 721, 709], "temperature": 0.0, "avg_logprob": -0.24430237927483123, "compression_ratio": 1.7, "no_speech_prob": 1.0188065289185033e-06}, {"id": 802, "seek": 408984, "start": 4108.24, "end": 4115.04, "text": " Okay, and so this is like the first type of parameter if you learn to set is number of estimators and the method for setting it is", "tokens": [1033, 11, 293, 370, 341, 307, 411, 264, 700, 2010, 295, 13075, 498, 291, 1466, 281, 992, 307, 1230, 295, 8017, 3391, 293, 264, 3170, 337, 3287, 309, 307], "temperature": 0.0, "avg_logprob": -0.24430237927483123, "compression_ratio": 1.7, "no_speech_prob": 1.0188065289185033e-06}, {"id": 803, "seek": 408984, "start": 4115.76, "end": 4117.68, "text": " as many as", "tokens": [382, 867, 382], "temperature": 0.0, "avg_logprob": -0.24430237927483123, "compression_ratio": 1.7, "no_speech_prob": 1.0188065289185033e-06}, {"id": 804, "seek": 411768, "start": 4117.68, "end": 4121.240000000001, "text": " You have time to fit and that actually", "tokens": [509, 362, 565, 281, 3318, 293, 300, 767], "temperature": 0.0, "avg_logprob": -0.17809962300420965, "compression_ratio": 1.5815899581589958, "no_speech_prob": 1.6028056961658876e-06}, {"id": 805, "seek": 411768, "start": 4121.92, "end": 4123.56, "text": " Seem to be hopping", "tokens": [1100, 443, 281, 312, 47199], "temperature": 0.0, "avg_logprob": -0.17809962300420965, "compression_ratio": 1.5815899581589958, "no_speech_prob": 1.6028056961658876e-06}, {"id": 806, "seek": 411768, "start": 4123.56, "end": 4128.22, "text": " Okay, now in practice. We're going to learn to set a few more hyper parameters", "tokens": [1033, 11, 586, 294, 3124, 13, 492, 434, 516, 281, 1466, 281, 992, 257, 1326, 544, 9848, 9834], "temperature": 0.0, "avg_logprob": -0.17809962300420965, "compression_ratio": 1.5815899581589958, "no_speech_prob": 1.6028056961658876e-06}, {"id": 807, "seek": 411768, "start": 4128.8, "end": 4130.56, "text": " Adding more trees", "tokens": [31204, 544, 5852], "temperature": 0.0, "avg_logprob": -0.17809962300420965, "compression_ratio": 1.5815899581589958, "no_speech_prob": 1.6028056961658876e-06}, {"id": 808, "seek": 411768, "start": 4130.56, "end": 4132.4800000000005, "text": " slows it down", "tokens": [35789, 309, 760], "temperature": 0.0, "avg_logprob": -0.17809962300420965, "compression_ratio": 1.5815899581589958, "no_speech_prob": 1.6028056961658876e-06}, {"id": 809, "seek": 411768, "start": 4132.4800000000005, "end": 4136.280000000001, "text": " But with less trees you can still get the same insights", "tokens": [583, 365, 1570, 5852, 291, 393, 920, 483, 264, 912, 14310], "temperature": 0.0, "avg_logprob": -0.17809962300420965, "compression_ratio": 1.5815899581589958, "no_speech_prob": 1.6028056961658876e-06}, {"id": 810, "seek": 411768, "start": 4136.4800000000005, "end": 4142.04, "text": " so I build most of my models in practice with like 20 to 30 trees and", "tokens": [370, 286, 1322, 881, 295, 452, 5245, 294, 3124, 365, 411, 945, 281, 2217, 5852, 293], "temperature": 0.0, "avg_logprob": -0.17809962300420965, "compression_ratio": 1.5815899581589958, "no_speech_prob": 1.6028056961658876e-06}, {"id": 811, "seek": 411768, "start": 4142.4800000000005, "end": 4146.360000000001, "text": " It's only like then at the end of the project or maybe at the end of the day's work", "tokens": [467, 311, 787, 411, 550, 412, 264, 917, 295, 264, 1716, 420, 1310, 412, 264, 917, 295, 264, 786, 311, 589], "temperature": 0.0, "avg_logprob": -0.17809962300420965, "compression_ratio": 1.5815899581589958, "no_speech_prob": 1.6028056961658876e-06}, {"id": 812, "seek": 414636, "start": 4146.36, "end": 4150.08, "text": " I'll then try doing like I don't know a thousand trees and run it overnight", "tokens": [286, 603, 550, 853, 884, 411, 286, 500, 380, 458, 257, 4714, 5852, 293, 1190, 309, 13935], "temperature": 0.0, "avg_logprob": -0.24054582595825194, "compression_ratio": 1.8362068965517242, "no_speech_prob": 3.966944404965034e-06}, {"id": 813, "seek": 414636, "start": 4150.639999999999, "end": 4153.4, "text": " Was there a question yes, can we pass that to Prince?", "tokens": [3027, 456, 257, 1168, 2086, 11, 393, 321, 1320, 300, 281, 9821, 30], "temperature": 0.0, "avg_logprob": -0.24054582595825194, "compression_ratio": 1.8362068965517242, "no_speech_prob": 3.966944404965034e-06}, {"id": 814, "seek": 414636, "start": 4158.28, "end": 4163.839999999999, "text": " So each tree might have different estimators different combination of estimators which tree isn't estimator", "tokens": [407, 1184, 4230, 1062, 362, 819, 8017, 3391, 819, 6562, 295, 8017, 3391, 597, 4230, 1943, 380, 8017, 1639], "temperature": 0.0, "avg_logprob": -0.24054582595825194, "compression_ratio": 1.8362068965517242, "no_speech_prob": 3.966944404965034e-06}, {"id": 815, "seek": 414636, "start": 4163.839999999999, "end": 4167.96, "text": " So this is a synonym so in scikit-learn when they say estimator they mean trees", "tokens": [407, 341, 307, 257, 5451, 12732, 370, 294, 2180, 22681, 12, 306, 1083, 562, 436, 584, 8017, 1639, 436, 914, 5852], "temperature": 0.0, "avg_logprob": -0.24054582595825194, "compression_ratio": 1.8362068965517242, "no_speech_prob": 3.966944404965034e-06}, {"id": 816, "seek": 414636, "start": 4168.4, "end": 4175.5199999999995, "text": " So I mean features features tree each tree will have different breakpoints on different on different columns", "tokens": [407, 286, 914, 4122, 4122, 4230, 1184, 4230, 486, 362, 819, 1821, 20552, 322, 819, 322, 819, 13766], "temperature": 0.0, "avg_logprob": -0.24054582595825194, "compression_ratio": 1.8362068965517242, "no_speech_prob": 3.966944404965034e-06}, {"id": 817, "seek": 417552, "start": 4175.52, "end": 4179.4400000000005, "text": " But if at the end we want to look at the important features, we'll get to that", "tokens": [583, 498, 412, 264, 917, 321, 528, 281, 574, 412, 264, 1021, 4122, 11, 321, 603, 483, 281, 300], "temperature": 0.0, "avg_logprob": -0.19457873892276845, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.7231186575372703e-05}, {"id": 818, "seek": 417552, "start": 4180.4800000000005, "end": 4187.06, "text": " Yeah, so after we finish with kind of setting hyper parameters the next stage of the course will be", "tokens": [865, 11, 370, 934, 321, 2413, 365, 733, 295, 3287, 9848, 9834, 264, 958, 3233, 295, 264, 1164, 486, 312], "temperature": 0.0, "avg_logprob": -0.19457873892276845, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.7231186575372703e-05}, {"id": 819, "seek": 417552, "start": 4188.52, "end": 4190.64, "text": " Learning about what it tells us about the data", "tokens": [15205, 466, 437, 309, 5112, 505, 466, 264, 1412], "temperature": 0.0, "avg_logprob": -0.19457873892276845, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.7231186575372703e-05}, {"id": 820, "seek": 417552, "start": 4191.84, "end": 4195.64, "text": " If you need to know it now, you know for your projects feel free to look ahead", "tokens": [759, 291, 643, 281, 458, 309, 586, 11, 291, 458, 337, 428, 4455, 841, 1737, 281, 574, 2286], "temperature": 0.0, "avg_logprob": -0.19457873892276845, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.7231186575372703e-05}, {"id": 821, "seek": 417552, "start": 4196.8, "end": 4198.8, "text": " there's a", "tokens": [456, 311, 257], "temperature": 0.0, "avg_logprob": -0.19457873892276845, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.7231186575372703e-05}, {"id": 822, "seek": 417552, "start": 4199.080000000001, "end": 4202.320000000001, "text": " Lesson to RF interpretation is where we can see it", "tokens": [18649, 266, 281, 26204, 14174, 307, 689, 321, 393, 536, 309], "temperature": 0.0, "avg_logprob": -0.19457873892276845, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.7231186575372703e-05}, {"id": 823, "seek": 420232, "start": 4202.32, "end": 4207.12, "text": " Okay, so that's our first hyper parameter I", "tokens": [1033, 11, 370, 300, 311, 527, 700, 9848, 13075, 286], "temperature": 0.0, "avg_logprob": -0.15987883567810057, "compression_ratio": 1.516260162601626, "no_speech_prob": 6.962164661672432e-06}, {"id": 824, "seek": 420232, "start": 4208.44, "end": 4210.44, "text": " Want to talk next about out-of-bag score", "tokens": [11773, 281, 751, 958, 466, 484, 12, 2670, 12, 17282, 6175], "temperature": 0.0, "avg_logprob": -0.15987883567810057, "compression_ratio": 1.516260162601626, "no_speech_prob": 6.962164661672432e-06}, {"id": 825, "seek": 420232, "start": 4211.5599999999995, "end": 4217.5199999999995, "text": " Sometimes your data set will be kind of small and you won't want to pull out a validation set", "tokens": [4803, 428, 1412, 992, 486, 312, 733, 295, 1359, 293, 291, 1582, 380, 528, 281, 2235, 484, 257, 24071, 992], "temperature": 0.0, "avg_logprob": -0.15987883567810057, "compression_ratio": 1.516260162601626, "no_speech_prob": 6.962164661672432e-06}, {"id": 826, "seek": 420232, "start": 4218.44, "end": 4223.08, "text": " Because doing so means you now don't have enough data to build a good model. What do you do?", "tokens": [1436, 884, 370, 1355, 291, 586, 500, 380, 362, 1547, 1412, 281, 1322, 257, 665, 2316, 13, 708, 360, 291, 360, 30], "temperature": 0.0, "avg_logprob": -0.15987883567810057, "compression_ratio": 1.516260162601626, "no_speech_prob": 6.962164661672432e-06}, {"id": 827, "seek": 420232, "start": 4224.12, "end": 4228.48, "text": " There's a cool trick which is pretty much unique to random forests, and it's this", "tokens": [821, 311, 257, 1627, 4282, 597, 307, 1238, 709, 3845, 281, 4974, 21700, 11, 293, 309, 311, 341], "temperature": 0.0, "avg_logprob": -0.15987883567810057, "compression_ratio": 1.516260162601626, "no_speech_prob": 6.962164661672432e-06}, {"id": 828, "seek": 420232, "start": 4229.84, "end": 4231.84, "text": " What we could do is", "tokens": [708, 321, 727, 360, 307], "temperature": 0.0, "avg_logprob": -0.15987883567810057, "compression_ratio": 1.516260162601626, "no_speech_prob": 6.962164661672432e-06}, {"id": 829, "seek": 423184, "start": 4231.84, "end": 4233.84, "text": " recognize", "tokens": [5521], "temperature": 0.0, "avg_logprob": -0.16927467303329638, "compression_ratio": 1.9664804469273742, "no_speech_prob": 3.726615886989748e-06}, {"id": 830, "seek": 423184, "start": 4234.68, "end": 4239.46, "text": " That some of our in our first tree some of our columns sorry some of our rows", "tokens": [663, 512, 295, 527, 294, 527, 700, 4230, 512, 295, 527, 13766, 2597, 512, 295, 527, 13241], "temperature": 0.0, "avg_logprob": -0.16927467303329638, "compression_ratio": 1.9664804469273742, "no_speech_prob": 3.726615886989748e-06}, {"id": 831, "seek": 423184, "start": 4240.92, "end": 4242.92, "text": " Didn't get used", "tokens": [11151, 380, 483, 1143], "temperature": 0.0, "avg_logprob": -0.16927467303329638, "compression_ratio": 1.9664804469273742, "no_speech_prob": 3.726615886989748e-06}, {"id": 832, "seek": 423184, "start": 4243.04, "end": 4247.4400000000005, "text": " So what we could do would be to pass those rows", "tokens": [407, 437, 321, 727, 360, 576, 312, 281, 1320, 729, 13241], "temperature": 0.0, "avg_logprob": -0.16927467303329638, "compression_ratio": 1.9664804469273742, "no_speech_prob": 3.726615886989748e-06}, {"id": 833, "seek": 423184, "start": 4248.12, "end": 4252.2, "text": " through the first tree and treat it as a validation set and", "tokens": [807, 264, 700, 4230, 293, 2387, 309, 382, 257, 24071, 992, 293], "temperature": 0.0, "avg_logprob": -0.16927467303329638, "compression_ratio": 1.9664804469273742, "no_speech_prob": 3.726615886989748e-06}, {"id": 834, "seek": 423184, "start": 4253.360000000001, "end": 4255.08, "text": " Then for the second tree", "tokens": [1396, 337, 264, 1150, 4230], "temperature": 0.0, "avg_logprob": -0.16927467303329638, "compression_ratio": 1.9664804469273742, "no_speech_prob": 3.726615886989748e-06}, {"id": 835, "seek": 423184, "start": 4255.08, "end": 4260.78, "text": " We could pass through the rows that weren't used for the second tree through it to create a validation set for that", "tokens": [492, 727, 1320, 807, 264, 13241, 300, 4999, 380, 1143, 337, 264, 1150, 4230, 807, 309, 281, 1884, 257, 24071, 992, 337, 300], "temperature": 0.0, "avg_logprob": -0.16927467303329638, "compression_ratio": 1.9664804469273742, "no_speech_prob": 3.726615886989748e-06}, {"id": 836, "seek": 426078, "start": 4260.78, "end": 4265.599999999999, "text": " and so effectively we would have a different validation set for each tree and", "tokens": [293, 370, 8659, 321, 576, 362, 257, 819, 24071, 992, 337, 1184, 4230, 293], "temperature": 0.0, "avg_logprob": -0.22838934262593588, "compression_ratio": 1.6129032258064515, "no_speech_prob": 6.37553625892906e-07}, {"id": 837, "seek": 426078, "start": 4266.3, "end": 4269.179999999999, "text": " So now to calculate our prediction", "tokens": [407, 586, 281, 8873, 527, 17630], "temperature": 0.0, "avg_logprob": -0.22838934262593588, "compression_ratio": 1.6129032258064515, "no_speech_prob": 6.37553625892906e-07}, {"id": 838, "seek": 426078, "start": 4269.759999999999, "end": 4275.679999999999, "text": " We would average all of the trees where that row was not used for training", "tokens": [492, 576, 4274, 439, 295, 264, 5852, 689, 300, 5386, 390, 406, 1143, 337, 3097], "temperature": 0.0, "avg_logprob": -0.22838934262593588, "compression_ratio": 1.6129032258064515, "no_speech_prob": 6.37553625892906e-07}, {"id": 839, "seek": 426078, "start": 4276.639999999999, "end": 4279.88, "text": " right, so for tree number one", "tokens": [558, 11, 370, 337, 4230, 1230, 472], "temperature": 0.0, "avg_logprob": -0.22838934262593588, "compression_ratio": 1.6129032258064515, "no_speech_prob": 6.37553625892906e-07}, {"id": 840, "seek": 426078, "start": 4279.88, "end": 4284.48, "text": " We would have the ones I've marked in blue here and then maybe for tree number two", "tokens": [492, 576, 362, 264, 2306, 286, 600, 12658, 294, 3344, 510, 293, 550, 1310, 337, 4230, 1230, 732], "temperature": 0.0, "avg_logprob": -0.22838934262593588, "compression_ratio": 1.6129032258064515, "no_speech_prob": 6.37553625892906e-07}, {"id": 841, "seek": 428448, "start": 4284.48, "end": 4292.08, "text": " It turned out it was like this one this one this one and this one and so forth right so as long as you've got", "tokens": [467, 3574, 484, 309, 390, 411, 341, 472, 341, 472, 341, 472, 293, 341, 472, 293, 370, 5220, 558, 370, 382, 938, 382, 291, 600, 658], "temperature": 0.0, "avg_logprob": -0.14307362018245282, "compression_ratio": 1.7813953488372094, "no_speech_prob": 3.2058241572485713e-07}, {"id": 842, "seek": 428448, "start": 4292.08, "end": 4293.679999999999, "text": " enough trees", "tokens": [1547, 5852], "temperature": 0.0, "avg_logprob": -0.14307362018245282, "compression_ratio": 1.7813953488372094, "no_speech_prob": 3.2058241572485713e-07}, {"id": 843, "seek": 428448, "start": 4293.679999999999, "end": 4299.919999999999, "text": " Every rows going to appear in the out of bag sample for one of them at least so you'll be averaging", "tokens": [2048, 13241, 516, 281, 4204, 294, 264, 484, 295, 3411, 6889, 337, 472, 295, 552, 412, 1935, 370, 291, 603, 312, 47308], "temperature": 0.0, "avg_logprob": -0.14307362018245282, "compression_ratio": 1.7813953488372094, "no_speech_prob": 3.2058241572485713e-07}, {"id": 844, "seek": 428448, "start": 4300.28, "end": 4302.28, "text": " You know hopefully a few trees", "tokens": [509, 458, 4696, 257, 1326, 5852], "temperature": 0.0, "avg_logprob": -0.14307362018245282, "compression_ratio": 1.7813953488372094, "no_speech_prob": 3.2058241572485713e-07}, {"id": 845, "seek": 428448, "start": 4303.839999999999, "end": 4305.839999999999, "text": " So if you've got a hundred trees", "tokens": [407, 498, 291, 600, 658, 257, 3262, 5852], "temperature": 0.0, "avg_logprob": -0.14307362018245282, "compression_ratio": 1.7813953488372094, "no_speech_prob": 3.2058241572485713e-07}, {"id": 846, "seek": 428448, "start": 4306.36, "end": 4312.459999999999, "text": " It's very likely that all of the rows are going to appear many times in these out-of-bag samples", "tokens": [467, 311, 588, 3700, 300, 439, 295, 264, 13241, 366, 516, 281, 4204, 867, 1413, 294, 613, 484, 12, 2670, 12, 17282, 10938], "temperature": 0.0, "avg_logprob": -0.14307362018245282, "compression_ratio": 1.7813953488372094, "no_speech_prob": 3.2058241572485713e-07}, {"id": 847, "seek": 431246, "start": 4312.46, "end": 4319.46, "text": " So what you can do is you can create an out-of-bag prediction by averaging all of the trees you didn't use to train each", "tokens": [407, 437, 291, 393, 360, 307, 291, 393, 1884, 364, 484, 12, 2670, 12, 17282, 17630, 538, 47308, 439, 295, 264, 5852, 291, 994, 380, 764, 281, 3847, 1184], "temperature": 0.0, "avg_logprob": -0.2453173933358028, "compression_ratio": 1.6492890995260663, "no_speech_prob": 8.851545203469868e-07}, {"id": 848, "seek": 431246, "start": 4319.74, "end": 4325.46, "text": " Individual row and then you can calculate your root mean squared error r squared etc on that", "tokens": [37292, 5386, 293, 550, 291, 393, 8873, 428, 5593, 914, 8889, 6713, 367, 8889, 5183, 322, 300], "temperature": 0.0, "avg_logprob": -0.2453173933358028, "compression_ratio": 1.6492890995260663, "no_speech_prob": 8.851545203469868e-07}, {"id": 849, "seek": 431246, "start": 4326.38, "end": 4332.58, "text": " If you pass OOB score equals true to psychic learn it will do that for you", "tokens": [759, 291, 1320, 422, 46, 33, 6175, 6915, 2074, 281, 35406, 1466, 309, 486, 360, 300, 337, 291], "temperature": 0.0, "avg_logprob": -0.2453173933358028, "compression_ratio": 1.6492890995260663, "no_speech_prob": 8.851545203469868e-07}, {"id": 850, "seek": 431246, "start": 4333.38, "end": 4335.38, "text": " And it will create an attribute", "tokens": [400, 309, 486, 1884, 364, 19667], "temperature": 0.0, "avg_logprob": -0.2453173933358028, "compression_ratio": 1.6492890995260663, "no_speech_prob": 8.851545203469868e-07}, {"id": 851, "seek": 433538, "start": 4335.38, "end": 4342.38, "text": " Called OOB score underscore and so my little print score function here if that attribute exists it it adds it to the print", "tokens": [45001, 422, 46, 33, 6175, 37556, 293, 370, 452, 707, 4482, 6175, 2445, 510, 498, 300, 19667, 8198, 309, 309, 10860, 309, 281, 264, 4482], "temperature": 0.0, "avg_logprob": -0.2977436338152204, "compression_ratio": 1.5654761904761905, "no_speech_prob": 2.6841873932426097e-06}, {"id": 852, "seek": 433538, "start": 4348.900000000001, "end": 4355.58, "text": " So if you take a look here OOB score equals true. We've now got one extra number and", "tokens": [407, 498, 291, 747, 257, 574, 510, 422, 46, 33, 6175, 6915, 2074, 13, 492, 600, 586, 658, 472, 2857, 1230, 293], "temperature": 0.0, "avg_logprob": -0.2977436338152204, "compression_ratio": 1.5654761904761905, "no_speech_prob": 2.6841873932426097e-06}, {"id": 853, "seek": 433538, "start": 4357.42, "end": 4361.3, "text": " It's R squared that is the R squared for the OOB sample", "tokens": [467, 311, 497, 8889, 300, 307, 264, 497, 8889, 337, 264, 422, 46, 33, 6889], "temperature": 0.0, "avg_logprob": -0.2977436338152204, "compression_ratio": 1.5654761904761905, "no_speech_prob": 2.6841873932426097e-06}, {"id": 854, "seek": 436130, "start": 4361.3, "end": 4366.820000000001, "text": " Its R squared is very similar the R squared and the validation set which is what we hoped for", "tokens": [6953, 497, 8889, 307, 588, 2531, 264, 497, 8889, 293, 264, 24071, 992, 597, 307, 437, 321, 19737, 337], "temperature": 0.0, "avg_logprob": -0.2344692679012523, "compression_ratio": 1.6225490196078431, "no_speech_prob": 1.5534943713646499e-06}, {"id": 855, "seek": 436130, "start": 4367.66, "end": 4369.66, "text": " Can we pass it?", "tokens": [1664, 321, 1320, 309, 30], "temperature": 0.0, "avg_logprob": -0.2344692679012523, "compression_ratio": 1.6225490196078431, "no_speech_prob": 1.5534943713646499e-06}, {"id": 856, "seek": 436130, "start": 4371.62, "end": 4373.62, "text": " Is it the case that the", "tokens": [1119, 309, 264, 1389, 300, 264], "temperature": 0.0, "avg_logprob": -0.2344692679012523, "compression_ratio": 1.6225490196078431, "no_speech_prob": 1.5534943713646499e-06}, {"id": 857, "seek": 436130, "start": 4374.5, "end": 4380.58, "text": " The prediction for the OOB score has to be must be mathematically lower than the one for our entire forest", "tokens": [440, 17630, 337, 264, 422, 46, 33, 6175, 575, 281, 312, 1633, 312, 44003, 3126, 813, 264, 472, 337, 527, 2302, 6719], "temperature": 0.0, "avg_logprob": -0.2344692679012523, "compression_ratio": 1.6225490196078431, "no_speech_prob": 1.5534943713646499e-06}, {"id": 858, "seek": 436130, "start": 4381.66, "end": 4385.9800000000005, "text": " Certainly, it's not true that the prediction is lower. It's possible for the accuracy", "tokens": [16628, 11, 309, 311, 406, 2074, 300, 264, 17630, 307, 3126, 13, 467, 311, 1944, 337, 264, 14170], "temperature": 0.0, "avg_logprob": -0.2344692679012523, "compression_ratio": 1.6225490196078431, "no_speech_prob": 1.5534943713646499e-06}, {"id": 859, "seek": 436130, "start": 4386.9800000000005, "end": 4388.860000000001, "text": " Yeah", "tokens": [865], "temperature": 0.0, "avg_logprob": -0.2344692679012523, "compression_ratio": 1.6225490196078431, "no_speech_prob": 1.5534943713646499e-06}, {"id": 860, "seek": 438886, "start": 4388.86, "end": 4395.44, "text": " It's not mathematically necessary that it's true, but it's going to be true on average because your average for each row", "tokens": [467, 311, 406, 44003, 4818, 300, 309, 311, 2074, 11, 457, 309, 311, 516, 281, 312, 2074, 322, 4274, 570, 428, 4274, 337, 1184, 5386], "temperature": 0.0, "avg_logprob": -0.1509129885330941, "compression_ratio": 1.688034188034188, "no_speech_prob": 5.714994131267304e-07}, {"id": 861, "seek": 438886, "start": 4396.099999999999, "end": 4400.92, "text": " Appears in less trees in the OOB samples and it does in the full set of trees", "tokens": [41322, 685, 294, 1570, 5852, 294, 264, 422, 46, 33, 10938, 293, 309, 775, 294, 264, 1577, 992, 295, 5852], "temperature": 0.0, "avg_logprob": -0.1509129885330941, "compression_ratio": 1.688034188034188, "no_speech_prob": 5.714994131267304e-07}, {"id": 862, "seek": 438886, "start": 4400.94, "end": 4404.839999999999, "text": " So as you see here, it's a little less good", "tokens": [407, 382, 291, 536, 510, 11, 309, 311, 257, 707, 1570, 665], "temperature": 0.0, "avg_logprob": -0.1509129885330941, "compression_ratio": 1.688034188034188, "no_speech_prob": 5.714994131267304e-07}, {"id": 863, "seek": 438886, "start": 4405.36, "end": 4411.299999999999, "text": " So in general, it's a great insight Chris in general the OOB R squared will slightly", "tokens": [407, 294, 2674, 11, 309, 311, 257, 869, 11269, 6688, 294, 2674, 264, 422, 46, 33, 497, 8889, 486, 4748], "temperature": 0.0, "avg_logprob": -0.1509129885330941, "compression_ratio": 1.688034188034188, "no_speech_prob": 5.714994131267304e-07}, {"id": 864, "seek": 438886, "start": 4412.62, "end": 4416.36, "text": " Underestimate how generalizable the model is the more trees you add", "tokens": [6974, 377, 2905, 577, 2674, 22395, 264, 2316, 307, 264, 544, 5852, 291, 909], "temperature": 0.0, "avg_logprob": -0.1509129885330941, "compression_ratio": 1.688034188034188, "no_speech_prob": 5.714994131267304e-07}, {"id": 865, "seek": 441636, "start": 4416.36, "end": 4421.24, "text": " the less serious that underestimation is and for me in practice I", "tokens": [264, 1570, 3156, 300, 24612, 332, 399, 307, 293, 337, 385, 294, 3124, 286], "temperature": 0.0, "avg_logprob": -0.19241603390201106, "compression_ratio": 1.619718309859155, "no_speech_prob": 3.2887235192902153e-06}, {"id": 866, "seek": 441636, "start": 4422.0, "end": 4425.28, "text": " Find it's totally good enough, you know in practice", "tokens": [11809, 309, 311, 3879, 665, 1547, 11, 291, 458, 294, 3124], "temperature": 0.0, "avg_logprob": -0.19241603390201106, "compression_ratio": 1.619718309859155, "no_speech_prob": 3.2887235192902153e-06}, {"id": 867, "seek": 441636, "start": 4427.48, "end": 4429.48, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.19241603390201106, "compression_ratio": 1.619718309859155, "no_speech_prob": 3.2887235192902153e-06}, {"id": 868, "seek": 441636, "start": 4430.88, "end": 4437.24, "text": " This OOB score is is super handy and one of the things that's super handy for is", "tokens": [639, 422, 46, 33, 6175, 307, 307, 1687, 13239, 293, 472, 295, 264, 721, 300, 311, 1687, 13239, 337, 307], "temperature": 0.0, "avg_logprob": -0.19241603390201106, "compression_ratio": 1.619718309859155, "no_speech_prob": 3.2887235192902153e-06}, {"id": 869, "seek": 441636, "start": 4438.36, "end": 4441.599999999999, "text": " you're going to see there's quite a few hyper parameters that we're going to set and", "tokens": [291, 434, 516, 281, 536, 456, 311, 1596, 257, 1326, 9848, 9834, 300, 321, 434, 516, 281, 992, 293], "temperature": 0.0, "avg_logprob": -0.19241603390201106, "compression_ratio": 1.619718309859155, "no_speech_prob": 3.2887235192902153e-06}, {"id": 870, "seek": 444160, "start": 4441.6, "end": 4445.72, "text": " We would like to find some automated way to set them", "tokens": [492, 576, 411, 281, 915, 512, 18473, 636, 281, 992, 552], "temperature": 0.0, "avg_logprob": -0.19268708277230312, "compression_ratio": 1.964102564102564, "no_speech_prob": 5.255341420706827e-06}, {"id": 871, "seek": 444160, "start": 4446.88, "end": 4452.92, "text": " And one way to do that is to do what's called a grid search a grid search is where there's a scikit-learn", "tokens": [400, 472, 636, 281, 360, 300, 307, 281, 360, 437, 311, 1219, 257, 10748, 3164, 257, 10748, 3164, 307, 689, 456, 311, 257, 2180, 22681, 12, 306, 1083], "temperature": 0.0, "avg_logprob": -0.19268708277230312, "compression_ratio": 1.964102564102564, "no_speech_prob": 5.255341420706827e-06}, {"id": 872, "seek": 444160, "start": 4453.6, "end": 4459.320000000001, "text": " Function called grid search and you pass in the list of all of the parameters all of the hyper parameters that you want to tune", "tokens": [11166, 882, 1219, 10748, 3164, 293, 291, 1320, 294, 264, 1329, 295, 439, 295, 264, 9834, 439, 295, 264, 9848, 9834, 300, 291, 528, 281, 10864], "temperature": 0.0, "avg_logprob": -0.19268708277230312, "compression_ratio": 1.964102564102564, "no_speech_prob": 5.255341420706827e-06}, {"id": 873, "seek": 444160, "start": 4459.6, "end": 4462.04, "text": " you pass in for each one a list of all of the", "tokens": [291, 1320, 294, 337, 1184, 472, 257, 1329, 295, 439, 295, 264], "temperature": 0.0, "avg_logprob": -0.19268708277230312, "compression_ratio": 1.964102564102564, "no_speech_prob": 5.255341420706827e-06}, {"id": 874, "seek": 444160, "start": 4462.400000000001, "end": 4464.72, "text": " values of that hyper parameter you want to try and", "tokens": [4190, 295, 300, 9848, 13075, 291, 528, 281, 853, 293], "temperature": 0.0, "avg_logprob": -0.19268708277230312, "compression_ratio": 1.964102564102564, "no_speech_prob": 5.255341420706827e-06}, {"id": 875, "seek": 446472, "start": 4464.72, "end": 4472.84, "text": " and it runs your model on every possible combination of all of those hyper parameters and tells you which one is the best and", "tokens": [293, 309, 6676, 428, 2316, 322, 633, 1944, 6562, 295, 439, 295, 729, 9848, 9834, 293, 5112, 291, 597, 472, 307, 264, 1151, 293], "temperature": 0.0, "avg_logprob": -0.20296338709389292, "compression_ratio": 1.6030927835051547, "no_speech_prob": 1.6280450836347882e-06}, {"id": 876, "seek": 446472, "start": 4474.2, "end": 4477.96, "text": " OOB score is a great like choice for", "tokens": [422, 46, 33, 6175, 307, 257, 869, 411, 3922, 337], "temperature": 0.0, "avg_logprob": -0.20296338709389292, "compression_ratio": 1.6030927835051547, "no_speech_prob": 1.6280450836347882e-06}, {"id": 877, "seek": 446472, "start": 4478.6, "end": 4483.68, "text": " Forgetting it to tell you which one is best in terms of OOB score like that's an example of something you can do with", "tokens": [18675, 783, 309, 281, 980, 291, 597, 472, 307, 1151, 294, 2115, 295, 422, 46, 33, 6175, 411, 300, 311, 364, 1365, 295, 746, 291, 393, 360, 365], "temperature": 0.0, "avg_logprob": -0.20296338709389292, "compression_ratio": 1.6030927835051547, "no_speech_prob": 1.6280450836347882e-06}, {"id": 878, "seek": 446472, "start": 4483.68, "end": 4485.68, "text": " Oh, okay, which works well", "tokens": [876, 11, 1392, 11, 597, 1985, 731], "temperature": 0.0, "avg_logprob": -0.20296338709389292, "compression_ratio": 1.6030927835051547, "no_speech_prob": 1.6280450836347882e-06}, {"id": 879, "seek": 446472, "start": 4486.04, "end": 4488.04, "text": " now", "tokens": [586], "temperature": 0.0, "avg_logprob": -0.20296338709389292, "compression_ratio": 1.6030927835051547, "no_speech_prob": 1.6280450836347882e-06}, {"id": 880, "seek": 448804, "start": 4488.04, "end": 4494.44, "text": " If you think about it I kind of did something pretty dumb earlier which is I", "tokens": [759, 291, 519, 466, 309, 286, 733, 295, 630, 746, 1238, 10316, 3071, 597, 307, 286], "temperature": 0.0, "avg_logprob": -0.48623976707458494, "compression_ratio": 1.6062176165803108, "no_speech_prob": 2.482466470610234e-06}, {"id": 881, "seek": 448804, "start": 4494.96, "end": 4499.56, "text": " Took a subset of 30,000 rows of the data and it built all my models of that", "tokens": [38288, 257, 25993, 295, 2217, 11, 1360, 13241, 295, 264, 1412, 293, 309, 3094, 439, 452, 5245, 295, 300], "temperature": 0.0, "avg_logprob": -0.48623976707458494, "compression_ratio": 1.6062176165803108, "no_speech_prob": 2.482466470610234e-06}, {"id": 882, "seek": 448804, "start": 4500.4, "end": 4507.68, "text": " Which means every tree in my random forest is a different subset of that subset of 30,000", "tokens": [3013, 1355, 633, 4230, 294, 452, 4974, 6719, 307, 257, 819, 25993, 295, 300, 25993, 295, 2217, 11, 1360], "temperature": 0.0, "avg_logprob": -0.48623976707458494, "compression_ratio": 1.6062176165803108, "no_speech_prob": 2.482466470610234e-06}, {"id": 883, "seek": 448804, "start": 4509.16, "end": 4511.36, "text": " Why do that why not?", "tokens": [1545, 360, 300, 983, 406, 30], "temperature": 0.0, "avg_logprob": -0.48623976707458494, "compression_ratio": 1.6062176165803108, "no_speech_prob": 2.482466470610234e-06}, {"id": 884, "seek": 448804, "start": 4512.0, "end": 4514.56, "text": " Pick a different like a totally different tree", "tokens": [14129, 257, 819, 411, 257, 3879, 819, 4230], "temperature": 0.0, "avg_logprob": -0.48623976707458494, "compression_ratio": 1.6062176165803108, "no_speech_prob": 2.482466470610234e-06}, {"id": 885, "seek": 451456, "start": 4514.56, "end": 4521.52, "text": " Why not pick a different like a totally different subset of 30,000 each time?", "tokens": [1545, 406, 1888, 257, 819, 411, 257, 3879, 819, 25993, 295, 2217, 11, 1360, 1184, 565, 30], "temperature": 0.0, "avg_logprob": -0.15096860403542992, "compression_ratio": 1.6952380952380952, "no_speech_prob": 5.539159815270978e-07}, {"id": 886, "seek": 451456, "start": 4521.68, "end": 4528.160000000001, "text": " So in other words, let's leave the entire 300,000 records as is right and if I want to make things faster", "tokens": [407, 294, 661, 2283, 11, 718, 311, 1856, 264, 2302, 6641, 11, 1360, 7724, 382, 307, 558, 293, 498, 286, 528, 281, 652, 721, 4663], "temperature": 0.0, "avg_logprob": -0.15096860403542992, "compression_ratio": 1.6952380952380952, "no_speech_prob": 5.539159815270978e-07}, {"id": 887, "seek": 451456, "start": 4528.360000000001, "end": 4534.280000000001, "text": " Right pick a different subset of 30,000 each time. So rather than bootstrapping the entire set of rows", "tokens": [1779, 1888, 257, 819, 25993, 295, 2217, 11, 1360, 1184, 565, 13, 407, 2831, 813, 11450, 19639, 3759, 264, 2302, 992, 295, 13241], "temperature": 0.0, "avg_logprob": -0.15096860403542992, "compression_ratio": 1.6952380952380952, "no_speech_prob": 5.539159815270978e-07}, {"id": 888, "seek": 451456, "start": 4534.88, "end": 4540.38, "text": " Let's just randomly sample a subset of the data and so we can do that", "tokens": [961, 311, 445, 16979, 6889, 257, 25993, 295, 264, 1412, 293, 370, 321, 393, 360, 300], "temperature": 0.0, "avg_logprob": -0.15096860403542992, "compression_ratio": 1.6952380952380952, "no_speech_prob": 5.539159815270978e-07}, {"id": 889, "seek": 454038, "start": 4540.38, "end": 4547.14, "text": " So let's go back and recall Procter yet without the subset parameter to get all of our data again", "tokens": [407, 718, 311, 352, 646, 293, 9901, 1705, 349, 260, 1939, 1553, 264, 25993, 13075, 281, 483, 439, 295, 527, 1412, 797], "temperature": 0.0, "avg_logprob": -0.20816869993467588, "compression_ratio": 1.4055555555555554, "no_speech_prob": 7.81146638928476e-07}, {"id": 890, "seek": 454038, "start": 4547.14, "end": 4549.14, "text": " And so to remind you", "tokens": [400, 370, 281, 4160, 291], "temperature": 0.0, "avg_logprob": -0.20816869993467588, "compression_ratio": 1.4055555555555554, "no_speech_prob": 7.81146638928476e-07}, {"id": 891, "seek": 454038, "start": 4551.82, "end": 4557.74, "text": " That is okay 400,000 in the whole data frame of which we have", "tokens": [663, 307, 1392, 8423, 11, 1360, 294, 264, 1379, 1412, 3920, 295, 597, 321, 362], "temperature": 0.0, "avg_logprob": -0.20816869993467588, "compression_ratio": 1.4055555555555554, "no_speech_prob": 7.81146638928476e-07}, {"id": 892, "seek": 454038, "start": 4561.06, "end": 4563.06, "text": " 389,000 in our training set and", "tokens": [12843, 24, 11, 1360, 294, 527, 3097, 992, 293], "temperature": 0.0, "avg_logprob": -0.20816869993467588, "compression_ratio": 1.4055555555555554, "no_speech_prob": 7.81146638928476e-07}, {"id": 893, "seek": 454038, "start": 4563.66, "end": 4564.900000000001, "text": " instead", "tokens": [2602], "temperature": 0.0, "avg_logprob": -0.20816869993467588, "compression_ratio": 1.4055555555555554, "no_speech_prob": 7.81146638928476e-07}, {"id": 894, "seek": 454038, "start": 4564.900000000001, "end": 4567.18, "text": " We're going to go set RF samples", "tokens": [492, 434, 516, 281, 352, 992, 26204, 10938], "temperature": 0.0, "avg_logprob": -0.20816869993467588, "compression_ratio": 1.4055555555555554, "no_speech_prob": 7.81146638928476e-07}, {"id": 895, "seek": 456718, "start": 4567.18, "end": 4571.34, "text": " 20,000 remember that was the site of the 30,000", "tokens": [945, 11, 1360, 1604, 300, 390, 264, 3621, 295, 264, 2217, 11, 1360], "temperature": 0.0, "avg_logprob": -0.16256743344393643, "compression_ratio": 1.6576576576576576, "no_speech_prob": 2.0580439468176337e-06}, {"id": 896, "seek": 456718, "start": 4571.34, "end": 4576.9400000000005, "text": " We use 20,000 of them in our training set if I do this then now when I run a random forest", "tokens": [492, 764, 945, 11, 1360, 295, 552, 294, 527, 3097, 992, 498, 286, 360, 341, 550, 586, 562, 286, 1190, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.16256743344393643, "compression_ratio": 1.6576576576576576, "no_speech_prob": 2.0580439468176337e-06}, {"id": 897, "seek": 456718, "start": 4577.46, "end": 4580.9800000000005, "text": " It's not going to bootstrap an entire set of", "tokens": [467, 311, 406, 516, 281, 11450, 372, 4007, 364, 2302, 992, 295], "temperature": 0.0, "avg_logprob": -0.16256743344393643, "compression_ratio": 1.6576576576576576, "no_speech_prob": 2.0580439468176337e-06}, {"id": 898, "seek": 456718, "start": 4581.9800000000005, "end": 4585.9400000000005, "text": " 391,000 rows it's going to just grab a subset of 20,000 rows", "tokens": [15238, 16, 11, 1360, 13241, 309, 311, 516, 281, 445, 4444, 257, 25993, 295, 945, 11, 1360, 13241], "temperature": 0.0, "avg_logprob": -0.16256743344393643, "compression_ratio": 1.6576576576576576, "no_speech_prob": 2.0580439468176337e-06}, {"id": 899, "seek": 456718, "start": 4587.54, "end": 4589.54, "text": " Right and so now if I run this", "tokens": [1779, 293, 370, 586, 498, 286, 1190, 341], "temperature": 0.0, "avg_logprob": -0.16256743344393643, "compression_ratio": 1.6576576576576576, "no_speech_prob": 2.0580439468176337e-06}, {"id": 900, "seek": 456718, "start": 4590.1, "end": 4592.1, "text": " It will still run", "tokens": [467, 486, 920, 1190], "temperature": 0.0, "avg_logprob": -0.16256743344393643, "compression_ratio": 1.6576576576576576, "no_speech_prob": 2.0580439468176337e-06}, {"id": 901, "seek": 459210, "start": 4592.1, "end": 4598.34, "text": " Just as quickly as if I had like originally done a random sample of 20,000, but now every tree", "tokens": [1449, 382, 2661, 382, 498, 286, 632, 411, 7993, 1096, 257, 4974, 6889, 295, 945, 11, 1360, 11, 457, 586, 633, 4230], "temperature": 0.0, "avg_logprob": -0.1945174062574232, "compression_ratio": 1.5, "no_speech_prob": 1.191104843201174e-06}, {"id": 902, "seek": 459210, "start": 4599.46, "end": 4607.3, "text": " Can have access to the whole data set right so if I do enough estimators enough trees eventually it's going to see everything", "tokens": [1664, 362, 2105, 281, 264, 1379, 1412, 992, 558, 370, 498, 286, 360, 1547, 8017, 3391, 1547, 5852, 4728, 309, 311, 516, 281, 536, 1203], "temperature": 0.0, "avg_logprob": -0.1945174062574232, "compression_ratio": 1.5, "no_speech_prob": 1.191104843201174e-06}, {"id": 903, "seek": 459210, "start": 4608.34, "end": 4615.18, "text": " Right so in this case with 10 trees, which is the default I get an R squared of", "tokens": [1779, 370, 294, 341, 1389, 365, 1266, 5852, 11, 597, 307, 264, 7576, 286, 483, 364, 497, 8889, 295], "temperature": 0.0, "avg_logprob": -0.1945174062574232, "compression_ratio": 1.5, "no_speech_prob": 1.191104843201174e-06}, {"id": 904, "seek": 461518, "start": 4615.18, "end": 4622.9400000000005, "text": " 0.86 which is actually about the same as my R squared with the with the 20,000 subset and", "tokens": [1958, 13, 22193, 597, 307, 767, 466, 264, 912, 382, 452, 497, 8889, 365, 264, 365, 264, 945, 11, 1360, 25993, 293], "temperature": 0.0, "avg_logprob": -0.16837884079326282, "compression_ratio": 1.6785714285714286, "no_speech_prob": 2.4060936993919313e-06}, {"id": 905, "seek": 461518, "start": 4623.34, "end": 4628.02, "text": " That's because I haven't used many estimators yet right, but if I increase the number of estimators", "tokens": [663, 311, 570, 286, 2378, 380, 1143, 867, 8017, 3391, 1939, 558, 11, 457, 498, 286, 3488, 264, 1230, 295, 8017, 3391], "temperature": 0.0, "avg_logprob": -0.16837884079326282, "compression_ratio": 1.6785714285714286, "no_speech_prob": 2.4060936993919313e-06}, {"id": 906, "seek": 461518, "start": 4628.9800000000005, "end": 4632.5, "text": " It's going to make more of a difference right so if I increase the number of estimators", "tokens": [467, 311, 516, 281, 652, 544, 295, 257, 2649, 558, 370, 498, 286, 3488, 264, 1230, 295, 8017, 3391], "temperature": 0.0, "avg_logprob": -0.16837884079326282, "compression_ratio": 1.6785714285714286, "no_speech_prob": 2.4060936993919313e-06}, {"id": 907, "seek": 461518, "start": 4636.22, "end": 4638.22, "text": " To 40", "tokens": [1407, 3356], "temperature": 0.0, "avg_logprob": -0.16837884079326282, "compression_ratio": 1.6785714285714286, "no_speech_prob": 2.4060936993919313e-06}, {"id": 908, "seek": 461518, "start": 4638.5, "end": 4640.5, "text": " It's going to take a little bit longer to run", "tokens": [467, 311, 516, 281, 747, 257, 707, 857, 2854, 281, 1190], "temperature": 0.0, "avg_logprob": -0.16837884079326282, "compression_ratio": 1.6785714285714286, "no_speech_prob": 2.4060936993919313e-06}, {"id": 909, "seek": 464050, "start": 4640.5, "end": 4647.48, "text": " But it's going to be able to see a larger subset of the data set and so as you can see the R", "tokens": [583, 309, 311, 516, 281, 312, 1075, 281, 536, 257, 4833, 25993, 295, 264, 1412, 992, 293, 370, 382, 291, 393, 536, 264, 497], "temperature": 0.0, "avg_logprob": -0.18565037565411263, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.8738647895588656e-06}, {"id": 910, "seek": 464050, "start": 4647.48, "end": 4650.74, "text": " Squareds gone up from 0.86 to 0.876", "tokens": [8683, 1642, 82, 2780, 493, 490, 1958, 13, 22193, 281, 1958, 13, 23853, 21], "temperature": 0.0, "avg_logprob": -0.18565037565411263, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.8738647895588656e-06}, {"id": 911, "seek": 464050, "start": 4651.18, "end": 4656.58, "text": " Okay, so this is actually a great approach and for those of you who are doing the groceries competition", "tokens": [1033, 11, 370, 341, 307, 767, 257, 869, 3109, 293, 337, 729, 295, 291, 567, 366, 884, 264, 31391, 6211], "temperature": 0.0, "avg_logprob": -0.18565037565411263, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.8738647895588656e-06}, {"id": 912, "seek": 464050, "start": 4656.58, "end": 4662.9, "text": " That's got something like 120 million rows, but there's no way you would want to create a random forest", "tokens": [663, 311, 658, 746, 411, 10411, 2459, 13241, 11, 457, 456, 311, 572, 636, 291, 576, 528, 281, 1884, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.18565037565411263, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.8738647895588656e-06}, {"id": 913, "seek": 464050, "start": 4663.26, "end": 4667.3, "text": " Using 128 million rows in every tree like it's going to take forever", "tokens": [11142, 29810, 2459, 13241, 294, 633, 4230, 411, 309, 311, 516, 281, 747, 5680], "temperature": 0.0, "avg_logprob": -0.18565037565411263, "compression_ratio": 1.588235294117647, "no_speech_prob": 1.8738647895588656e-06}, {"id": 914, "seek": 466730, "start": 4667.3, "end": 4673.860000000001, "text": " ever so what you could do is use this set are of samples to do like I don't know a hundred thousand or a", "tokens": [1562, 370, 437, 291, 727, 360, 307, 764, 341, 992, 366, 295, 10938, 281, 360, 411, 286, 500, 380, 458, 257, 3262, 4714, 420, 257], "temperature": 0.0, "avg_logprob": -0.24659468538017684, "compression_ratio": 1.6244725738396624, "no_speech_prob": 1.1726335742423544e-06}, {"id": 915, "seek": 466730, "start": 4674.1, "end": 4679.26, "text": " Million or play around with it so the trick here is that with a random forest using this technique?", "tokens": [33959, 420, 862, 926, 365, 309, 370, 264, 4282, 510, 307, 300, 365, 257, 4974, 6719, 1228, 341, 6532, 30], "temperature": 0.0, "avg_logprob": -0.24659468538017684, "compression_ratio": 1.6244725738396624, "no_speech_prob": 1.1726335742423544e-06}, {"id": 916, "seek": 466730, "start": 4679.820000000001, "end": 4685.34, "text": " No data set is too big. I don't care if it's got a hundred billion rows right you can", "tokens": [883, 1412, 992, 307, 886, 955, 13, 286, 500, 380, 1127, 498, 309, 311, 658, 257, 3262, 5218, 13241, 558, 291, 393], "temperature": 0.0, "avg_logprob": -0.24659468538017684, "compression_ratio": 1.6244725738396624, "no_speech_prob": 1.1726335742423544e-06}, {"id": 917, "seek": 468534, "start": 4685.34, "end": 4699.54, "text": " Create a bunch of trees each one of the different random subset can somebody pass the actually I can see", "tokens": [20248, 257, 3840, 295, 5852, 1184, 472, 295, 264, 819, 4974, 25993, 393, 2618, 1320, 264, 767, 286, 393, 536], "temperature": 0.0, "avg_logprob": -0.36850050221318786, "compression_ratio": 1.4776119402985075, "no_speech_prob": 1.9222316041123122e-05}, {"id": 918, "seek": 468534, "start": 4707.3, "end": 4713.92, "text": " So my question was for the all these scores and these ones does it take the only like for the", "tokens": [407, 452, 1168, 390, 337, 264, 439, 613, 13444, 293, 613, 2306, 775, 309, 747, 264, 787, 411, 337, 264], "temperature": 0.0, "avg_logprob": -0.36850050221318786, "compression_ratio": 1.4776119402985075, "no_speech_prob": 1.9222316041123122e-05}, {"id": 919, "seek": 471392, "start": 4713.92, "end": 4715.4800000000005, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.24482464790344238, "compression_ratio": 1.5844155844155845, "no_speech_prob": 8.530260856787208e-06}, {"id": 920, "seek": 471392, "start": 4715.4800000000005, "end": 4719.4800000000005, "text": " Ones from the sample or do you take from all the that's a great question um?", "tokens": [1282, 279, 490, 264, 6889, 420, 360, 291, 747, 490, 439, 264, 300, 311, 257, 869, 1168, 1105, 30], "temperature": 0.0, "avg_logprob": -0.24482464790344238, "compression_ratio": 1.5844155844155845, "no_speech_prob": 8.530260856787208e-06}, {"id": 921, "seek": 471392, "start": 4720.68, "end": 4722.68, "text": " so unfortunately", "tokens": [370, 7015], "temperature": 0.0, "avg_logprob": -0.24482464790344238, "compression_ratio": 1.5844155844155845, "no_speech_prob": 8.530260856787208e-06}, {"id": 922, "seek": 471392, "start": 4723.08, "end": 4727.92, "text": " Psychic learn does not support this functionality out of the box, so I had to write this", "tokens": [17303, 299, 1466, 775, 406, 1406, 341, 14980, 484, 295, 264, 2424, 11, 370, 286, 632, 281, 2464, 341], "temperature": 0.0, "avg_logprob": -0.24482464790344238, "compression_ratio": 1.5844155844155845, "no_speech_prob": 8.530260856787208e-06}, {"id": 923, "seek": 471392, "start": 4729.04, "end": 4730.24, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.24482464790344238, "compression_ratio": 1.5844155844155845, "no_speech_prob": 8.530260856787208e-06}, {"id": 924, "seek": 471392, "start": 4730.24, "end": 4733.96, "text": " It's kind of a horrible hack right because we'd much rather be passing in like a", "tokens": [467, 311, 733, 295, 257, 9263, 10339, 558, 570, 321, 1116, 709, 2831, 312, 8437, 294, 411, 257], "temperature": 0.0, "avg_logprob": -0.24482464790344238, "compression_ratio": 1.5844155844155845, "no_speech_prob": 8.530260856787208e-06}, {"id": 925, "seek": 471392, "start": 4734.56, "end": 4740.04, "text": " Sample size parameter rather than doing this kind of setting up here, so what I actually do is", "tokens": [4832, 781, 2744, 13075, 2831, 813, 884, 341, 733, 295, 3287, 493, 510, 11, 370, 437, 286, 767, 360, 307], "temperature": 0.0, "avg_logprob": -0.24482464790344238, "compression_ratio": 1.5844155844155845, "no_speech_prob": 8.530260856787208e-06}, {"id": 926, "seek": 474004, "start": 4740.04, "end": 4745.4, "text": " Is if you look at the source code is I'm actually this is an internal", "tokens": [1119, 498, 291, 574, 412, 264, 4009, 3089, 307, 286, 478, 767, 341, 307, 364, 6920], "temperature": 0.0, "avg_logprob": -0.21518861794773536, "compression_ratio": 1.619289340101523, "no_speech_prob": 3.2377386105508776e-06}, {"id": 927, "seek": 474004, "start": 4745.4, "end": 4751.08, "text": " This is the internal function I looked at their source code that they call and I've replaced it with a with a lambda function", "tokens": [639, 307, 264, 6920, 2445, 286, 2956, 412, 641, 4009, 3089, 300, 436, 818, 293, 286, 600, 10772, 309, 365, 257, 365, 257, 13607, 2445], "temperature": 0.0, "avg_logprob": -0.21518861794773536, "compression_ratio": 1.619289340101523, "no_speech_prob": 3.2377386105508776e-06}, {"id": 928, "seek": 474004, "start": 4751.08, "end": 4753.08, "text": " That has the behavior we want", "tokens": [663, 575, 264, 5223, 321, 528], "temperature": 0.0, "avg_logprob": -0.21518861794773536, "compression_ratio": 1.619289340101523, "no_speech_prob": 3.2377386105508776e-06}, {"id": 929, "seek": 474004, "start": 4753.96, "end": 4758.44, "text": " Unfortunately the current version is not changing how OOB is calculated", "tokens": [8590, 264, 2190, 3037, 307, 406, 4473, 577, 422, 46, 33, 307, 15598], "temperature": 0.0, "avg_logprob": -0.21518861794773536, "compression_ratio": 1.619289340101523, "no_speech_prob": 3.2377386105508776e-06}, {"id": 930, "seek": 474004, "start": 4759.64, "end": 4761.04, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.21518861794773536, "compression_ratio": 1.619289340101523, "no_speech_prob": 3.2377386105508776e-06}, {"id": 931, "seek": 474004, "start": 4761.04, "end": 4763.04, "text": " Yeah, so currently", "tokens": [865, 11, 370, 4362], "temperature": 0.0, "avg_logprob": -0.21518861794773536, "compression_ratio": 1.619289340101523, "no_speech_prob": 3.2377386105508776e-06}, {"id": 932, "seek": 476304, "start": 4763.04, "end": 4770.8, "text": " OOB scores and set RF samples are not compatible with each other so you need to turn OOB equals false", "tokens": [422, 46, 33, 13444, 293, 992, 26204, 10938, 366, 406, 18218, 365, 1184, 661, 370, 291, 643, 281, 1261, 422, 46, 33, 6915, 7908], "temperature": 0.0, "avg_logprob": -0.20653637156767005, "compression_ratio": 1.553191489361702, "no_speech_prob": 3.5559608022595057e-06}, {"id": 933, "seek": 476304, "start": 4770.8, "end": 4772.8, "text": " If you use this approach", "tokens": [759, 291, 764, 341, 3109], "temperature": 0.0, "avg_logprob": -0.20653637156767005, "compression_ratio": 1.553191489361702, "no_speech_prob": 3.5559608022595057e-06}, {"id": 934, "seek": 476304, "start": 4774.96, "end": 4776.76, "text": " Which I hope to fix", "tokens": [3013, 286, 1454, 281, 3191], "temperature": 0.0, "avg_logprob": -0.20653637156767005, "compression_ratio": 1.553191489361702, "no_speech_prob": 3.5559608022595057e-06}, {"id": 935, "seek": 476304, "start": 4776.76, "end": 4778.96, "text": " But at this stage, it's it's not fixed", "tokens": [583, 412, 341, 3233, 11, 309, 311, 309, 311, 406, 6806], "temperature": 0.0, "avg_logprob": -0.20653637156767005, "compression_ratio": 1.553191489361702, "no_speech_prob": 3.5559608022595057e-06}, {"id": 936, "seek": 476304, "start": 4779.56, "end": 4782.08, "text": " So if you want to turn it off you just call", "tokens": [407, 498, 291, 528, 281, 1261, 309, 766, 291, 445, 818], "temperature": 0.0, "avg_logprob": -0.20653637156767005, "compression_ratio": 1.553191489361702, "no_speech_prob": 3.5559608022595057e-06}, {"id": 937, "seek": 476304, "start": 4782.96, "end": 4786.84, "text": " Reset RF samples okay, and that returns it back to what it was", "tokens": [5015, 302, 26204, 10938, 1392, 11, 293, 300, 11247, 309, 646, 281, 437, 309, 390], "temperature": 0.0, "avg_logprob": -0.20653637156767005, "compression_ratio": 1.553191489361702, "no_speech_prob": 3.5559608022595057e-06}, {"id": 938, "seek": 478684, "start": 4786.84, "end": 4792.68, "text": " Okay, so in practice when I'm like", "tokens": [1033, 11, 370, 294, 3124, 562, 286, 478, 411], "temperature": 0.0, "avg_logprob": -0.5644279148267664, "compression_ratio": 1.5170731707317073, "no_speech_prob": 2.332053782083676e-06}, {"id": 939, "seek": 478684, "start": 4794.4400000000005, "end": 4796.4400000000005, "text": " Doing", "tokens": [18496], "temperature": 0.0, "avg_logprob": -0.5644279148267664, "compression_ratio": 1.5170731707317073, "no_speech_prob": 2.332053782083676e-06}, {"id": 940, "seek": 478684, "start": 4796.4800000000005, "end": 4798.4800000000005, "text": " interactive", "tokens": [15141], "temperature": 0.0, "avg_logprob": -0.5644279148267664, "compression_ratio": 1.5170731707317073, "no_speech_prob": 2.332053782083676e-06}, {"id": 941, "seek": 478684, "start": 4799.4400000000005, "end": 4804.52, "text": " Machine learning using random forests in order to like explore my model explore hyper parameters", "tokens": [22155, 2539, 1228, 4974, 21700, 294, 1668, 281, 411, 6839, 452, 2316, 6839, 9848, 9834], "temperature": 0.0, "avg_logprob": -0.5644279148267664, "compression_ratio": 1.5170731707317073, "no_speech_prob": 2.332053782083676e-06}, {"id": 942, "seek": 478684, "start": 4805.0, "end": 4811.360000000001, "text": " The stuff we're going to learn in the future lesson where we actually analyze like feature importance and partial dependence and so forth I generally use", "tokens": [440, 1507, 321, 434, 516, 281, 1466, 294, 264, 2027, 6898, 689, 321, 767, 12477, 411, 4111, 7379, 293, 14641, 31704, 293, 370, 5220, 286, 5101, 764], "temperature": 0.0, "avg_logprob": -0.5644279148267664, "compression_ratio": 1.5170731707317073, "no_speech_prob": 2.332053782083676e-06}, {"id": 943, "seek": 478684, "start": 4812.04, "end": 4814.04, "text": " subsets", "tokens": [2090, 1385], "temperature": 0.0, "avg_logprob": -0.5644279148267664, "compression_ratio": 1.5170731707317073, "no_speech_prob": 2.332053782083676e-06}, {"id": 944, "seek": 481404, "start": 4814.04, "end": 4819.68, "text": " and reasonably small forests", "tokens": [293, 23551, 1359, 21700], "temperature": 0.0, "avg_logprob": -0.21765208570924524, "compression_ratio": 1.5392670157068062, "no_speech_prob": 5.255302312434651e-06}, {"id": 945, "seek": 481404, "start": 4820.28, "end": 4823.56, "text": " Because all the insights that I'm going to get are exactly the same as the big ones", "tokens": [1436, 439, 264, 14310, 300, 286, 478, 516, 281, 483, 366, 2293, 264, 912, 382, 264, 955, 2306], "temperature": 0.0, "avg_logprob": -0.21765208570924524, "compression_ratio": 1.5392670157068062, "no_speech_prob": 5.255302312434651e-06}, {"id": 946, "seek": 481404, "start": 4823.68, "end": 4828.8, "text": " But I can run that in like you know three or four seconds rather than hours", "tokens": [583, 286, 393, 1190, 300, 294, 411, 291, 458, 1045, 420, 1451, 3949, 2831, 813, 2496], "temperature": 0.0, "avg_logprob": -0.21765208570924524, "compression_ratio": 1.5392670157068062, "no_speech_prob": 5.255302312434651e-06}, {"id": 947, "seek": 481404, "start": 4829.16, "end": 4833.44, "text": " right, so this is one of the biggest tips I can give you and", "tokens": [558, 11, 370, 341, 307, 472, 295, 264, 3880, 6082, 286, 393, 976, 291, 293], "temperature": 0.0, "avg_logprob": -0.21765208570924524, "compression_ratio": 1.5392670157068062, "no_speech_prob": 5.255302312434651e-06}, {"id": 948, "seek": 481404, "start": 4833.84, "end": 4836.08, "text": " Very very few people in industry or academia", "tokens": [4372, 588, 1326, 561, 294, 3518, 420, 28937], "temperature": 0.0, "avg_logprob": -0.21765208570924524, "compression_ratio": 1.5392670157068062, "no_speech_prob": 5.255302312434651e-06}, {"id": 949, "seek": 483608, "start": 4836.08, "end": 4844.28, "text": " Actually do this most people run all of their models on all of the data all of the time using their best possible parameters", "tokens": [5135, 360, 341, 881, 561, 1190, 439, 295, 641, 5245, 322, 439, 295, 264, 1412, 439, 295, 264, 565, 1228, 641, 1151, 1944, 9834], "temperature": 0.0, "avg_logprob": -0.1175213669830898, "compression_ratio": 1.7435897435897436, "no_speech_prob": 4.092867584404303e-06}, {"id": 950, "seek": 483608, "start": 4844.64, "end": 4850.5599999999995, "text": " Which is just pointless right if you're trying to find out like which features are important, and how are they related to each other and so forth?", "tokens": [3013, 307, 445, 32824, 558, 498, 291, 434, 1382, 281, 915, 484, 411, 597, 4122, 366, 1021, 11, 293, 577, 366, 436, 4077, 281, 1184, 661, 293, 370, 5220, 30], "temperature": 0.0, "avg_logprob": -0.1175213669830898, "compression_ratio": 1.7435897435897436, "no_speech_prob": 4.092867584404303e-06}, {"id": 951, "seek": 483608, "start": 4851.8, "end": 4856.68, "text": " Having that fourth decimal place of accuracy isn't going to change any of your insights at all okay?", "tokens": [10222, 300, 6409, 26601, 1081, 295, 14170, 1943, 380, 516, 281, 1319, 604, 295, 428, 14310, 412, 439, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1175213669830898, "compression_ratio": 1.7435897435897436, "no_speech_prob": 4.092867584404303e-06}, {"id": 952, "seek": 483608, "start": 4856.68, "end": 4863.44, "text": " So I would say like do most of your models on you know a large enough sample size that your accuracy is", "tokens": [407, 286, 576, 584, 411, 360, 881, 295, 428, 5245, 322, 291, 458, 257, 2416, 1547, 6889, 2744, 300, 428, 14170, 307], "temperature": 0.0, "avg_logprob": -0.1175213669830898, "compression_ratio": 1.7435897435897436, "no_speech_prob": 4.092867584404303e-06}, {"id": 953, "seek": 486344, "start": 4863.44, "end": 4867.08, "text": " You know reasonable when I say reasonable. It's like", "tokens": [509, 458, 10585, 562, 286, 584, 10585, 13, 467, 311, 411], "temperature": 0.0, "avg_logprob": -0.13500828515915644, "compression_ratio": 1.6457564575645756, "no_speech_prob": 6.643364486080827e-06}, {"id": 954, "seek": 486344, "start": 4868.16, "end": 4870.96, "text": " Within a reasonable distance of the best accuracy you can get", "tokens": [15996, 257, 10585, 4560, 295, 264, 1151, 14170, 291, 393, 483], "temperature": 0.0, "avg_logprob": -0.13500828515915644, "compression_ratio": 1.6457564575645756, "no_speech_prob": 6.643364486080827e-06}, {"id": 955, "seek": 486344, "start": 4871.599999999999, "end": 4878.4, "text": " And it's taking you know a small number of seconds to train so that you can interactively do your analysis", "tokens": [400, 309, 311, 1940, 291, 458, 257, 1359, 1230, 295, 3949, 281, 3847, 370, 300, 291, 393, 4648, 3413, 360, 428, 5215], "temperature": 0.0, "avg_logprob": -0.13500828515915644, "compression_ratio": 1.6457564575645756, "no_speech_prob": 6.643364486080827e-06}, {"id": 956, "seek": 486344, "start": 4879.32, "end": 4884.639999999999, "text": " So there's a couple more parameters. I wanted to talk about so I'm going to call reset RF samples to get back to our full", "tokens": [407, 456, 311, 257, 1916, 544, 9834, 13, 286, 1415, 281, 751, 466, 370, 286, 478, 516, 281, 818, 14322, 26204, 10938, 281, 483, 646, 281, 527, 1577], "temperature": 0.0, "avg_logprob": -0.13500828515915644, "compression_ratio": 1.6457564575645756, "no_speech_prob": 6.643364486080827e-06}, {"id": 957, "seek": 486344, "start": 4884.639999999999, "end": 4890.219999999999, "text": " Data set because in this case at least on this computer. It's actually running in less than 10 seconds", "tokens": [11888, 992, 570, 294, 341, 1389, 412, 1935, 322, 341, 3820, 13, 467, 311, 767, 2614, 294, 1570, 813, 1266, 3949], "temperature": 0.0, "avg_logprob": -0.13500828515915644, "compression_ratio": 1.6457564575645756, "no_speech_prob": 6.643364486080827e-06}, {"id": 958, "seek": 489022, "start": 4890.22, "end": 4892.22, "text": " So here's our baseline", "tokens": [407, 510, 311, 527, 20518], "temperature": 0.0, "avg_logprob": -0.28132238388061526, "compression_ratio": 1.6153846153846154, "no_speech_prob": 5.338103164831409e-06}, {"id": 959, "seek": 489022, "start": 4895.18, "end": 4897.740000000001, "text": " We're going to do a baseline with 40 estimators", "tokens": [492, 434, 516, 281, 360, 257, 20518, 365, 3356, 8017, 3391], "temperature": 0.0, "avg_logprob": -0.28132238388061526, "compression_ratio": 1.6153846153846154, "no_speech_prob": 5.338103164831409e-06}, {"id": 960, "seek": 489022, "start": 4898.860000000001, "end": 4906.26, "text": " Okay, and so each of those 40 estimators is going to train all the way down to all the leaf nodes just have one", "tokens": [1033, 11, 293, 370, 1184, 295, 729, 3356, 8017, 3391, 307, 516, 281, 3847, 439, 264, 636, 760, 281, 439, 264, 10871, 13891, 445, 362, 472], "temperature": 0.0, "avg_logprob": -0.28132238388061526, "compression_ratio": 1.6153846153846154, "no_speech_prob": 5.338103164831409e-06}, {"id": 961, "seek": 489022, "start": 4907.42, "end": 4909.42, "text": " sample in them", "tokens": [6889, 294, 552], "temperature": 0.0, "avg_logprob": -0.28132238388061526, "compression_ratio": 1.6153846153846154, "no_speech_prob": 5.338103164831409e-06}, {"id": 962, "seek": 489022, "start": 4911.34, "end": 4918.22, "text": " So that's going to take a few seconds to run here we go so that gets us a point eight nine eight", "tokens": [407, 300, 311, 516, 281, 747, 257, 1326, 3949, 281, 1190, 510, 321, 352, 370, 300, 2170, 505, 257, 935, 3180, 4949, 3180], "temperature": 0.0, "avg_logprob": -0.28132238388061526, "compression_ratio": 1.6153846153846154, "no_speech_prob": 5.338103164831409e-06}, {"id": 963, "seek": 491822, "start": 4918.22, "end": 4924.06, "text": " R squared on the validation set or point nine oh eight on the OOB", "tokens": [497, 8889, 322, 264, 24071, 992, 420, 935, 4949, 1954, 3180, 322, 264, 422, 46, 33], "temperature": 0.0, "avg_logprob": -0.22551500679242728, "compression_ratio": 1.8026315789473684, "no_speech_prob": 2.7264586606179364e-06}, {"id": 964, "seek": 491822, "start": 4924.5, "end": 4927.34, "text": " Now this case the OOB is better. Why is it better?", "tokens": [823, 341, 1389, 264, 422, 46, 33, 307, 1101, 13, 1545, 307, 309, 1101, 30], "temperature": 0.0, "avg_logprob": -0.22551500679242728, "compression_ratio": 1.8026315789473684, "no_speech_prob": 2.7264586606179364e-06}, {"id": 965, "seek": 491822, "start": 4927.34, "end": 4933.900000000001, "text": " Well that's because remember our validation set is not a random sample our validation set is a different time period", "tokens": [1042, 300, 311, 570, 1604, 527, 24071, 992, 307, 406, 257, 4974, 6889, 527, 24071, 992, 307, 257, 819, 565, 2896], "temperature": 0.0, "avg_logprob": -0.22551500679242728, "compression_ratio": 1.8026315789473684, "no_speech_prob": 2.7264586606179364e-06}, {"id": 966, "seek": 491822, "start": 4934.38, "end": 4941.04, "text": " Okay, so it's actually much harder to predict a different time period than this one which is just predicting random", "tokens": [1033, 11, 370, 309, 311, 767, 709, 6081, 281, 6069, 257, 819, 565, 2896, 813, 341, 472, 597, 307, 445, 32884, 4974], "temperature": 0.0, "avg_logprob": -0.22551500679242728, "compression_ratio": 1.8026315789473684, "no_speech_prob": 2.7264586606179364e-06}, {"id": 967, "seek": 491822, "start": 4941.22, "end": 4945.7, "text": " Okay, so that's why this is not the way around we expected so", "tokens": [1033, 11, 370, 300, 311, 983, 341, 307, 406, 264, 636, 926, 321, 5176, 370], "temperature": 0.0, "avg_logprob": -0.22551500679242728, "compression_ratio": 1.8026315789473684, "no_speech_prob": 2.7264586606179364e-06}, {"id": 968, "seek": 494570, "start": 4945.7, "end": 4952.72, "text": " The next the first parameter we can try fiddling with is min samples leaf and so min samples leaf says", "tokens": [440, 958, 264, 700, 13075, 321, 393, 853, 283, 14273, 1688, 365, 307, 923, 10938, 10871, 293, 370, 923, 10938, 10871, 1619], "temperature": 0.0, "avg_logprob": -0.20609985513890045, "compression_ratio": 1.7799043062200957, "no_speech_prob": 6.577917019967572e-07}, {"id": 969, "seek": 494570, "start": 4952.98, "end": 4954.98, "text": " Stop training the tree further", "tokens": [5535, 3097, 264, 4230, 3052], "temperature": 0.0, "avg_logprob": -0.20609985513890045, "compression_ratio": 1.7799043062200957, "no_speech_prob": 6.577917019967572e-07}, {"id": 970, "seek": 494570, "start": 4955.86, "end": 4958.22, "text": " when your leaf node has", "tokens": [562, 428, 10871, 9984, 575], "temperature": 0.0, "avg_logprob": -0.20609985513890045, "compression_ratio": 1.7799043062200957, "no_speech_prob": 6.577917019967572e-07}, {"id": 971, "seek": 494570, "start": 4960.42, "end": 4962.42, "text": " Three or less", "tokens": [6244, 420, 1570], "temperature": 0.0, "avg_logprob": -0.20609985513890045, "compression_ratio": 1.7799043062200957, "no_speech_prob": 6.577917019967572e-07}, {"id": 972, "seek": 494570, "start": 4962.42, "end": 4968.08, "text": " Samples in they're out going all the way down until there's one we're going to go down until there's three", "tokens": [4832, 2622, 294, 436, 434, 484, 516, 439, 264, 636, 760, 1826, 456, 311, 472, 321, 434, 516, 281, 352, 760, 1826, 456, 311, 1045], "temperature": 0.0, "avg_logprob": -0.20609985513890045, "compression_ratio": 1.7799043062200957, "no_speech_prob": 6.577917019967572e-07}, {"id": 973, "seek": 494570, "start": 4968.86, "end": 4974.58, "text": " So in practice this means it's going to be like one or two less levels of decision being made", "tokens": [407, 294, 3124, 341, 1355, 309, 311, 516, 281, 312, 411, 472, 420, 732, 1570, 4358, 295, 3537, 885, 1027], "temperature": 0.0, "avg_logprob": -0.20609985513890045, "compression_ratio": 1.7799043062200957, "no_speech_prob": 6.577917019967572e-07}, {"id": 974, "seek": 497458, "start": 4974.58, "end": 4981.3, "text": " So it means we've got like half the number of actual decision criteria. We have to do so it's going to train more quickly", "tokens": [407, 309, 1355, 321, 600, 658, 411, 1922, 264, 1230, 295, 3539, 3537, 11101, 13, 492, 362, 281, 360, 370, 309, 311, 516, 281, 3847, 544, 2661], "temperature": 0.0, "avg_logprob": -0.14563201974939416, "compression_ratio": 1.687732342007435, "no_speech_prob": 2.058039399344125e-06}, {"id": 975, "seek": 497458, "start": 4981.94, "end": 4986.42, "text": " It means that when we look at an individual tree rather than just taking one point", "tokens": [467, 1355, 300, 562, 321, 574, 412, 364, 2609, 4230, 2831, 813, 445, 1940, 472, 935], "temperature": 0.0, "avg_logprob": -0.14563201974939416, "compression_ratio": 1.687732342007435, "no_speech_prob": 2.058039399344125e-06}, {"id": 976, "seek": 497458, "start": 4986.5, "end": 4993.34, "text": " We're taking the average of at least three points that's where we'd expect the trees to generalize each one to generalize a little bit better", "tokens": [492, 434, 1940, 264, 4274, 295, 412, 1935, 1045, 2793, 300, 311, 689, 321, 1116, 2066, 264, 5852, 281, 2674, 1125, 1184, 472, 281, 2674, 1125, 257, 707, 857, 1101], "temperature": 0.0, "avg_logprob": -0.14563201974939416, "compression_ratio": 1.687732342007435, "no_speech_prob": 2.058039399344125e-06}, {"id": 977, "seek": 497458, "start": 4993.66, "end": 4998.0199999999995, "text": " Okay, but each tree is probably going to be slightly less powerful on its own", "tokens": [1033, 11, 457, 1184, 4230, 307, 1391, 516, 281, 312, 4748, 1570, 4005, 322, 1080, 1065], "temperature": 0.0, "avg_logprob": -0.14563201974939416, "compression_ratio": 1.687732342007435, "no_speech_prob": 2.058039399344125e-06}, {"id": 978, "seek": 497458, "start": 4999.3, "end": 5001.66, "text": " So let's try training that so", "tokens": [407, 718, 311, 853, 3097, 300, 370], "temperature": 0.0, "avg_logprob": -0.14563201974939416, "compression_ratio": 1.687732342007435, "no_speech_prob": 2.058039399344125e-06}, {"id": 979, "seek": 500166, "start": 5001.66, "end": 5009.0, "text": " Possible values of min samples leaf I find ones which work well are kind of one three five", "tokens": [430, 5785, 4190, 295, 923, 10938, 10871, 286, 915, 2306, 597, 589, 731, 366, 733, 295, 472, 1045, 1732], "temperature": 0.0, "avg_logprob": -0.21796806653340658, "compression_ratio": 1.6545454545454545, "no_speech_prob": 2.295912508998299e-06}, {"id": 980, "seek": 500166, "start": 5009.98, "end": 5010.9, "text": " ten", "tokens": [2064], "temperature": 0.0, "avg_logprob": -0.21796806653340658, "compression_ratio": 1.6545454545454545, "no_speech_prob": 2.295912508998299e-06}, {"id": 981, "seek": 500166, "start": 5010.9, "end": 5012.0599999999995, "text": " 25", "tokens": [3552], "temperature": 0.0, "avg_logprob": -0.21796806653340658, "compression_ratio": 1.6545454545454545, "no_speech_prob": 2.295912508998299e-06}, {"id": 982, "seek": 500166, "start": 5012.0599999999995, "end": 5014.0599999999995, "text": " You know like I find that kind of range", "tokens": [509, 458, 411, 286, 915, 300, 733, 295, 3613], "temperature": 0.0, "avg_logprob": -0.21796806653340658, "compression_ratio": 1.6545454545454545, "no_speech_prob": 2.295912508998299e-06}, {"id": 983, "seek": 500166, "start": 5014.74, "end": 5022.32, "text": " Seems to work well, but like sometimes if you've got a really big data set and you're not using the small samples", "tokens": [22524, 281, 589, 731, 11, 457, 411, 2171, 498, 291, 600, 658, 257, 534, 955, 1412, 992, 293, 291, 434, 406, 1228, 264, 1359, 10938], "temperature": 0.0, "avg_logprob": -0.21796806653340658, "compression_ratio": 1.6545454545454545, "no_speech_prob": 2.295912508998299e-06}, {"id": 984, "seek": 500166, "start": 5022.38, "end": 5024.88, "text": " You know you might need a min samples leaf of", "tokens": [509, 458, 291, 1062, 643, 257, 923, 10938, 10871, 295], "temperature": 0.0, "avg_logprob": -0.21796806653340658, "compression_ratio": 1.6545454545454545, "no_speech_prob": 2.295912508998299e-06}, {"id": 985, "seek": 500166, "start": 5025.74, "end": 5029.099999999999, "text": " hundreds or thousands, so it's you kind of got to think about like", "tokens": [6779, 420, 5383, 11, 370, 309, 311, 291, 733, 295, 658, 281, 519, 466, 411], "temperature": 0.0, "avg_logprob": -0.21796806653340658, "compression_ratio": 1.6545454545454545, "no_speech_prob": 2.295912508998299e-06}, {"id": 986, "seek": 502910, "start": 5029.1, "end": 5035.52, "text": " How big are your sub samples going through and try things out now in this case going from the default of one?", "tokens": [1012, 955, 366, 428, 1422, 10938, 516, 807, 293, 853, 721, 484, 586, 294, 341, 1389, 516, 490, 264, 7576, 295, 472, 30], "temperature": 0.0, "avg_logprob": -0.20729605356852213, "compression_ratio": 1.6360153256704981, "no_speech_prob": 2.7693888569046976e-06}, {"id": 987, "seek": 502910, "start": 5035.860000000001, "end": 5037.860000000001, "text": " To three has increased our", "tokens": [1407, 1045, 575, 6505, 527], "temperature": 0.0, "avg_logprob": -0.20729605356852213, "compression_ratio": 1.6360153256704981, "no_speech_prob": 2.7693888569046976e-06}, {"id": 988, "seek": 502910, "start": 5039.3, "end": 5045.38, "text": " Validation set R squared from 898 to 902 so it's a slight improvement, and it's going to train a little faster as well", "tokens": [7188, 327, 399, 992, 497, 8889, 490, 1649, 22516, 281, 4289, 17, 370, 309, 311, 257, 4036, 10444, 11, 293, 309, 311, 516, 281, 3847, 257, 707, 4663, 382, 731], "temperature": 0.0, "avg_logprob": -0.20729605356852213, "compression_ratio": 1.6360153256704981, "no_speech_prob": 2.7693888569046976e-06}, {"id": 989, "seek": 502910, "start": 5046.860000000001, "end": 5050.46, "text": " Okay, something else you can try which is and so since this worked", "tokens": [1033, 11, 746, 1646, 291, 393, 853, 597, 307, 293, 370, 1670, 341, 2732], "temperature": 0.0, "avg_logprob": -0.20729605356852213, "compression_ratio": 1.6360153256704981, "no_speech_prob": 2.7693888569046976e-06}, {"id": 990, "seek": 502910, "start": 5050.46, "end": 5055.9800000000005, "text": " I'm going to leave that in I'm going to add in max features equals point five what does max features do?", "tokens": [286, 478, 516, 281, 1856, 300, 294, 286, 478, 516, 281, 909, 294, 11469, 4122, 6915, 935, 1732, 437, 775, 11469, 4122, 360, 30], "temperature": 0.0, "avg_logprob": -0.20729605356852213, "compression_ratio": 1.6360153256704981, "no_speech_prob": 2.7693888569046976e-06}, {"id": 991, "seek": 505598, "start": 5055.98, "end": 5058.62, "text": " well the idea is that", "tokens": [731, 264, 1558, 307, 300], "temperature": 0.0, "avg_logprob": -0.2154464494614374, "compression_ratio": 1.7116279069767442, "no_speech_prob": 2.090452198899584e-06}, {"id": 992, "seek": 505598, "start": 5059.82, "end": 5063.58, "text": " The less correlated your trees are with each other the better", "tokens": [440, 1570, 38574, 428, 5852, 366, 365, 1184, 661, 264, 1101], "temperature": 0.0, "avg_logprob": -0.2154464494614374, "compression_ratio": 1.7116279069767442, "no_speech_prob": 2.090452198899584e-06}, {"id": 993, "seek": 505598, "start": 5064.379999999999, "end": 5068.7, "text": " Now imagine you had one column that was", "tokens": [823, 3811, 291, 632, 472, 7738, 300, 390], "temperature": 0.0, "avg_logprob": -0.2154464494614374, "compression_ratio": 1.7116279069767442, "no_speech_prob": 2.090452198899584e-06}, {"id": 994, "seek": 505598, "start": 5069.7, "end": 5074.299999999999, "text": " So much better than all of the other columns of being predictive that every single tree you built", "tokens": [407, 709, 1101, 813, 439, 295, 264, 661, 13766, 295, 885, 35521, 300, 633, 2167, 4230, 291, 3094], "temperature": 0.0, "avg_logprob": -0.2154464494614374, "compression_ratio": 1.7116279069767442, "no_speech_prob": 2.090452198899584e-06}, {"id": 995, "seek": 505598, "start": 5074.5, "end": 5078.2, "text": " Regardless of like which subset of rows always started with that column", "tokens": [25148, 295, 411, 597, 25993, 295, 13241, 1009, 1409, 365, 300, 7738], "temperature": 0.0, "avg_logprob": -0.2154464494614374, "compression_ratio": 1.7116279069767442, "no_speech_prob": 2.090452198899584e-06}, {"id": 996, "seek": 505598, "start": 5078.74, "end": 5082.62, "text": " So the trees are all going to be pretty similar right, but you can imagine", "tokens": [407, 264, 5852, 366, 439, 516, 281, 312, 1238, 2531, 558, 11, 457, 291, 393, 3811], "temperature": 0.0, "avg_logprob": -0.2154464494614374, "compression_ratio": 1.7116279069767442, "no_speech_prob": 2.090452198899584e-06}, {"id": 997, "seek": 508262, "start": 5082.62, "end": 5090.5, "text": " There might be some interaction of variables where that interaction is more important than that individual column", "tokens": [821, 1062, 312, 512, 9285, 295, 9102, 689, 300, 9285, 307, 544, 1021, 813, 300, 2609, 7738], "temperature": 0.0, "avg_logprob": -0.1301398044679223, "compression_ratio": 1.7627906976744185, "no_speech_prob": 3.288732159489882e-06}, {"id": 998, "seek": 508262, "start": 5091.26, "end": 5096.98, "text": " So if every tree always splits on the first thing the same thing the first time", "tokens": [407, 498, 633, 4230, 1009, 37741, 322, 264, 700, 551, 264, 912, 551, 264, 700, 565], "temperature": 0.0, "avg_logprob": -0.1301398044679223, "compression_ratio": 1.7627906976744185, "no_speech_prob": 3.288732159489882e-06}, {"id": 999, "seek": 508262, "start": 5097.34, "end": 5099.62, "text": " You're not going to get much variation in those trees", "tokens": [509, 434, 406, 516, 281, 483, 709, 12990, 294, 729, 5852], "temperature": 0.0, "avg_logprob": -0.1301398044679223, "compression_ratio": 1.7627906976744185, "no_speech_prob": 3.288732159489882e-06}, {"id": 1000, "seek": 508262, "start": 5099.94, "end": 5105.86, "text": " So what we do is in addition to just taking a subset of rows", "tokens": [407, 437, 321, 360, 307, 294, 4500, 281, 445, 1940, 257, 25993, 295, 13241], "temperature": 0.0, "avg_logprob": -0.1301398044679223, "compression_ratio": 1.7627906976744185, "no_speech_prob": 3.288732159489882e-06}, {"id": 1001, "seek": 510586, "start": 5105.86, "end": 5112.42, "text": " We then at every single split point take a different subset of columns", "tokens": [492, 550, 412, 633, 2167, 7472, 935, 747, 257, 819, 25993, 295, 13766], "temperature": 0.0, "avg_logprob": -0.1310699058301521, "compression_ratio": 1.7529411764705882, "no_speech_prob": 1.3287728961586254e-06}, {"id": 1002, "seek": 510586, "start": 5113.54, "end": 5121.42, "text": " So it's slightly different to the row sampling for the row sampling each new tree is based on a random set of rows", "tokens": [407, 309, 311, 4748, 819, 281, 264, 5386, 21179, 337, 264, 5386, 21179, 1184, 777, 4230, 307, 2361, 322, 257, 4974, 992, 295, 13241], "temperature": 0.0, "avg_logprob": -0.1310699058301521, "compression_ratio": 1.7529411764705882, "no_speech_prob": 1.3287728961586254e-06}, {"id": 1003, "seek": 510586, "start": 5122.259999999999, "end": 5129.099999999999, "text": " For column sampling every individual binary split we choose from a different subset of columns", "tokens": [1171, 7738, 21179, 633, 2609, 17434, 7472, 321, 2826, 490, 257, 819, 25993, 295, 13766], "temperature": 0.0, "avg_logprob": -0.1310699058301521, "compression_ratio": 1.7529411764705882, "no_speech_prob": 1.3287728961586254e-06}, {"id": 1004, "seek": 510586, "start": 5129.86, "end": 5131.86, "text": " so in other words", "tokens": [370, 294, 661, 2283], "temperature": 0.0, "avg_logprob": -0.1310699058301521, "compression_ratio": 1.7529411764705882, "no_speech_prob": 1.3287728961586254e-06}, {"id": 1005, "seek": 513186, "start": 5131.86, "end": 5136.38, "text": " Rather than looking at every possible level of every possible column", "tokens": [16571, 813, 1237, 412, 633, 1944, 1496, 295, 633, 1944, 7738], "temperature": 0.0, "avg_logprob": -0.14265933403602013, "compression_ratio": 1.7202072538860103, "no_speech_prob": 1.018806642605341e-06}, {"id": 1006, "seek": 513186, "start": 5137.139999999999, "end": 5141.339999999999, "text": " We look at every possible level of a random subset of columns", "tokens": [492, 574, 412, 633, 1944, 1496, 295, 257, 4974, 25993, 295, 13766], "temperature": 0.0, "avg_logprob": -0.14265933403602013, "compression_ratio": 1.7202072538860103, "no_speech_prob": 1.018806642605341e-06}, {"id": 1007, "seek": 513186, "start": 5142.179999999999, "end": 5149.179999999999, "text": " Okay, and each time each decision point each binary split we use a different random subset", "tokens": [1033, 11, 293, 1184, 565, 1184, 3537, 935, 1184, 17434, 7472, 321, 764, 257, 819, 4974, 25993], "temperature": 0.0, "avg_logprob": -0.14265933403602013, "compression_ratio": 1.7202072538860103, "no_speech_prob": 1.018806642605341e-06}, {"id": 1008, "seek": 513186, "start": 5149.78, "end": 5153.62, "text": " How many well you get to pick point five means?", "tokens": [1012, 867, 731, 291, 483, 281, 1888, 935, 1732, 1355, 30], "temperature": 0.0, "avg_logprob": -0.14265933403602013, "compression_ratio": 1.7202072538860103, "no_speech_prob": 1.018806642605341e-06}, {"id": 1009, "seek": 513186, "start": 5154.259999999999, "end": 5156.259999999999, "text": " randomly choose half of them", "tokens": [16979, 2826, 1922, 295, 552], "temperature": 0.0, "avg_logprob": -0.14265933403602013, "compression_ratio": 1.7202072538860103, "no_speech_prob": 1.018806642605341e-06}, {"id": 1010, "seek": 513186, "start": 5156.86, "end": 5158.94, "text": " The default is to use all of them", "tokens": [440, 7576, 307, 281, 764, 439, 295, 552], "temperature": 0.0, "avg_logprob": -0.14265933403602013, "compression_ratio": 1.7202072538860103, "no_speech_prob": 1.018806642605341e-06}, {"id": 1011, "seek": 515894, "start": 5158.94, "end": 5163.339999999999, "text": " There's also a couple of special values you can use here", "tokens": [821, 311, 611, 257, 1916, 295, 2121, 4190, 291, 393, 764, 510], "temperature": 0.0, "avg_logprob": -0.24946527166680976, "compression_ratio": 1.7389162561576355, "no_speech_prob": 8.579210657444492e-07}, {"id": 1012, "seek": 515894, "start": 5166.7, "end": 5168.5, "text": " As you can see in max features", "tokens": [1018, 291, 393, 536, 294, 11469, 4122], "temperature": 0.0, "avg_logprob": -0.24946527166680976, "compression_ratio": 1.7389162561576355, "no_speech_prob": 8.579210657444492e-07}, {"id": 1013, "seek": 515894, "start": 5168.5, "end": 5175.66, "text": " You can also pass in square root to get square root of features or log 2 to get log 2 of features so in practice", "tokens": [509, 393, 611, 1320, 294, 3732, 5593, 281, 483, 3732, 5593, 295, 4122, 420, 3565, 568, 281, 483, 3565, 568, 295, 4122, 370, 294, 3124], "temperature": 0.0, "avg_logprob": -0.24946527166680976, "compression_ratio": 1.7389162561576355, "no_speech_prob": 8.579210657444492e-07}, {"id": 1014, "seek": 515894, "start": 5176.419999999999, "end": 5178.419999999999, "text": " Good values I found are", "tokens": [2205, 4190, 286, 1352, 366], "temperature": 0.0, "avg_logprob": -0.24946527166680976, "compression_ratio": 1.7389162561576355, "no_speech_prob": 8.579210657444492e-07}, {"id": 1015, "seek": 515894, "start": 5179.139999999999, "end": 5181.139999999999, "text": " ranged from one", "tokens": [45570, 490, 472], "temperature": 0.0, "avg_logprob": -0.24946527166680976, "compression_ratio": 1.7389162561576355, "no_speech_prob": 8.579210657444492e-07}, {"id": 1016, "seek": 518114, "start": 5181.14, "end": 5189.14, "text": " 0.5 log 2 or square root that's going to give you a nice bit of variation right can somebody pass it to Danielle and", "tokens": [1958, 13, 20, 3565, 568, 420, 3732, 5593, 300, 311, 516, 281, 976, 291, 257, 1481, 857, 295, 12990, 558, 393, 2618, 1320, 309, 281, 21182, 293], "temperature": 0.0, "avg_logprob": -0.2026514539531633, "compression_ratio": 1.606425702811245, "no_speech_prob": 1.9947235614381498e-06}, {"id": 1017, "seek": 518114, "start": 5192.06, "end": 5194.06, "text": " So just to clarify does that", "tokens": [407, 445, 281, 17594, 775, 300], "temperature": 0.0, "avg_logprob": -0.2026514539531633, "compression_ratio": 1.606425702811245, "no_speech_prob": 1.9947235614381498e-06}, {"id": 1018, "seek": 518114, "start": 5194.62, "end": 5200.42, "text": " Just like break it up smaller each time it goes through the tree or is it just taking half of what's left over or like?", "tokens": [1449, 411, 1821, 309, 493, 4356, 1184, 565, 309, 1709, 807, 264, 4230, 420, 307, 309, 445, 1940, 1922, 295, 437, 311, 1411, 670, 420, 411, 30], "temperature": 0.0, "avg_logprob": -0.2026514539531633, "compression_ratio": 1.606425702811245, "no_speech_prob": 1.9947235614381498e-06}, {"id": 1019, "seek": 518114, "start": 5200.42, "end": 5205.58, "text": " Hasn't been touched each time there's no such thing as what's left over after you've split on", "tokens": [8646, 77, 380, 668, 9828, 1184, 565, 456, 311, 572, 1270, 551, 382, 437, 311, 1411, 670, 934, 291, 600, 7472, 322], "temperature": 0.0, "avg_logprob": -0.2026514539531633, "compression_ratio": 1.606425702811245, "no_speech_prob": 1.9947235614381498e-06}, {"id": 1020, "seek": 518114, "start": 5206.06, "end": 5208.52, "text": " Yeah made less than or greater than 1984", "tokens": [865, 1027, 1570, 813, 420, 5044, 813, 27127], "temperature": 0.0, "avg_logprob": -0.2026514539531633, "compression_ratio": 1.606425702811245, "no_speech_prob": 1.9947235614381498e-06}, {"id": 1021, "seek": 520852, "start": 5208.52, "end": 5214.68, "text": " Mm-hmm you made still there right so later on you might then split on you made less than or greater than 1989", "tokens": [8266, 12, 10250, 291, 1027, 920, 456, 558, 370, 1780, 322, 291, 1062, 550, 7472, 322, 291, 1027, 1570, 813, 420, 5044, 813, 22427], "temperature": 0.0, "avg_logprob": -0.21778104939591994, "compression_ratio": 1.8634538152610443, "no_speech_prob": 1.6797259831946576e-06}, {"id": 1022, "seek": 520852, "start": 5215.4800000000005, "end": 5221.8, "text": " So so it's just each time rather than checking every variable to see where its best split is you just check", "tokens": [407, 370, 309, 311, 445, 1184, 565, 2831, 813, 8568, 633, 7006, 281, 536, 689, 1080, 1151, 7472, 307, 291, 445, 1520], "temperature": 0.0, "avg_logprob": -0.21778104939591994, "compression_ratio": 1.8634538152610443, "no_speech_prob": 1.6797259831946576e-06}, {"id": 1023, "seek": 520852, "start": 5222.040000000001, "end": 5226.240000000001, "text": " Half of them and so the next time you check a different half the next time you check a different half", "tokens": [15917, 295, 552, 293, 370, 264, 958, 565, 291, 1520, 257, 819, 1922, 264, 958, 565, 291, 1520, 257, 819, 1922], "temperature": 0.0, "avg_logprob": -0.21778104939591994, "compression_ratio": 1.8634538152610443, "no_speech_prob": 1.6797259831946576e-06}, {"id": 1024, "seek": 520852, "start": 5226.68, "end": 5231.400000000001, "text": " But I mean like terms is as you get like further to like the leafs", "tokens": [583, 286, 914, 411, 2115, 307, 382, 291, 483, 411, 3052, 281, 411, 264, 10871, 82], "temperature": 0.0, "avg_logprob": -0.21778104939591994, "compression_ratio": 1.8634538152610443, "no_speech_prob": 1.6797259831946576e-06}, {"id": 1025, "seek": 523140, "start": 5231.4, "end": 5238.08, "text": " You're gonna have less options right no no not you never remove the variables", "tokens": [509, 434, 799, 362, 1570, 3956, 558, 572, 572, 406, 291, 1128, 4159, 264, 9102], "temperature": 0.0, "avg_logprob": -0.23066095558993788, "compression_ratio": 1.6725663716814159, "no_speech_prob": 2.9944310426799348e-06}, {"id": 1026, "seek": 523140, "start": 5238.08, "end": 5242.16, "text": " Okay, you can use them again and again and again because you've got lots of different split points", "tokens": [1033, 11, 291, 393, 764, 552, 797, 293, 797, 293, 797, 570, 291, 600, 658, 3195, 295, 819, 7472, 2793], "temperature": 0.0, "avg_logprob": -0.23066095558993788, "compression_ratio": 1.6725663716814159, "no_speech_prob": 2.9944310426799348e-06}, {"id": 1027, "seek": 523140, "start": 5244.0, "end": 5248.36, "text": " So imagine for example that the relationship was just entirely linear between", "tokens": [407, 3811, 337, 1365, 300, 264, 2480, 390, 445, 7696, 8213, 1296], "temperature": 0.0, "avg_logprob": -0.23066095558993788, "compression_ratio": 1.6725663716814159, "no_speech_prob": 2.9944310426799348e-06}, {"id": 1028, "seek": 523140, "start": 5249.0, "end": 5255.4, "text": " Year-made and price right then in practice to actually model that you know your real relationship is", "tokens": [10289, 12, 10341, 293, 3218, 558, 550, 294, 3124, 281, 767, 2316, 300, 291, 458, 428, 957, 2480, 307], "temperature": 0.0, "avg_logprob": -0.23066095558993788, "compression_ratio": 1.6725663716814159, "no_speech_prob": 2.9944310426799348e-06}, {"id": 1029, "seek": 523140, "start": 5256.04, "end": 5258.04, "text": " year-made versus price", "tokens": [1064, 12, 10341, 5717, 3218], "temperature": 0.0, "avg_logprob": -0.23066095558993788, "compression_ratio": 1.6725663716814159, "no_speech_prob": 2.9944310426799348e-06}, {"id": 1030, "seek": 525804, "start": 5258.04, "end": 5263.68, "text": " Right but the best we could do would be this kind of first of all split here", "tokens": [1779, 457, 264, 1151, 321, 727, 360, 576, 312, 341, 733, 295, 700, 295, 439, 7472, 510], "temperature": 0.0, "avg_logprob": -0.20192638744007457, "compression_ratio": 1.7658730158730158, "no_speech_prob": 1.7330451100860955e-06}, {"id": 1031, "seek": 525804, "start": 5264.6, "end": 5267.08, "text": " Right and then to split here and here", "tokens": [1779, 293, 550, 281, 7472, 510, 293, 510], "temperature": 0.0, "avg_logprob": -0.20192638744007457, "compression_ratio": 1.7658730158730158, "no_speech_prob": 1.7330451100860955e-06}, {"id": 1032, "seek": 525804, "start": 5267.76, "end": 5271.28, "text": " Right and like split and split and split so they were binary", "tokens": [1779, 293, 411, 7472, 293, 7472, 293, 7472, 370, 436, 645, 17434], "temperature": 0.0, "avg_logprob": -0.20192638744007457, "compression_ratio": 1.7658730158730158, "no_speech_prob": 1.7330451100860955e-06}, {"id": 1033, "seek": 525804, "start": 5272.24, "end": 5274.24, "text": " Yeah, even if they're binary", "tokens": [865, 11, 754, 498, 436, 434, 17434], "temperature": 0.0, "avg_logprob": -0.20192638744007457, "compression_ratio": 1.7658730158730158, "no_speech_prob": 1.7330451100860955e-06}, {"id": 1034, "seek": 525804, "start": 5274.32, "end": 5279.4, "text": " Most random forest libraries don't do anything special about that they just kind of go okay", "tokens": [4534, 4974, 6719, 15148, 500, 380, 360, 1340, 2121, 466, 300, 436, 445, 733, 295, 352, 1392], "temperature": 0.0, "avg_logprob": -0.20192638744007457, "compression_ratio": 1.7658730158730158, "no_speech_prob": 1.7330451100860955e-06}, {"id": 1035, "seek": 525804, "start": 5279.4, "end": 5285.12, "text": " We'll try this variable. Oh it turns out. There's only one level left. You know so yeah that definitely", "tokens": [492, 603, 853, 341, 7006, 13, 876, 309, 4523, 484, 13, 821, 311, 787, 472, 1496, 1411, 13, 509, 458, 370, 1338, 300, 2138], "temperature": 0.0, "avg_logprob": -0.20192638744007457, "compression_ratio": 1.7658730158730158, "no_speech_prob": 1.7330451100860955e-06}, {"id": 1036, "seek": 528512, "start": 5285.12, "end": 5288.12, "text": " They don't do any kind of clever bookkeeping. Okay?", "tokens": [814, 500, 380, 360, 604, 733, 295, 13494, 1446, 25769, 13, 1033, 30], "temperature": 0.0, "avg_logprob": -0.2353478281121505, "compression_ratio": 1.4602510460251046, "no_speech_prob": 3.785307626458234e-06}, {"id": 1037, "seek": 528512, "start": 5291.68, "end": 5297.68, "text": " Okay, so if we add max features equals 0.5. It goes up from 901 to 906", "tokens": [1033, 11, 370, 498, 321, 909, 11469, 4122, 6915, 1958, 13, 20, 13, 467, 1709, 493, 490, 4289, 16, 281, 4289, 21], "temperature": 0.0, "avg_logprob": -0.2353478281121505, "compression_ratio": 1.4602510460251046, "no_speech_prob": 3.785307626458234e-06}, {"id": 1038, "seek": 528512, "start": 5298.4, "end": 5303.82, "text": " So that's better still and so as we've been doing this you also hopefully have noticed that our root mean squared error", "tokens": [407, 300, 311, 1101, 920, 293, 370, 382, 321, 600, 668, 884, 341, 291, 611, 4696, 362, 5694, 300, 527, 5593, 914, 8889, 6713], "temperature": 0.0, "avg_logprob": -0.2353478281121505, "compression_ratio": 1.4602510460251046, "no_speech_prob": 3.785307626458234e-06}, {"id": 1039, "seek": 528512, "start": 5303.92, "end": 5307.16, "text": " Of log price has been dropping on a validation set as well", "tokens": [2720, 3565, 3218, 575, 668, 13601, 322, 257, 24071, 992, 382, 731], "temperature": 0.0, "avg_logprob": -0.2353478281121505, "compression_ratio": 1.4602510460251046, "no_speech_prob": 3.785307626458234e-06}, {"id": 1040, "seek": 528512, "start": 5308.0, "end": 5312.12, "text": " And so it's now down to point two two eight six", "tokens": [400, 370, 309, 311, 586, 760, 281, 935, 732, 732, 3180, 2309], "temperature": 0.0, "avg_logprob": -0.2353478281121505, "compression_ratio": 1.4602510460251046, "no_speech_prob": 3.785307626458234e-06}, {"id": 1041, "seek": 531212, "start": 5312.12, "end": 5315.86, "text": " So how good is that right so like our?", "tokens": [407, 577, 665, 307, 300, 558, 370, 411, 527, 30], "temperature": 0.0, "avg_logprob": -0.20222864247331715, "compression_ratio": 1.626923076923077, "no_speech_prob": 1.3925429129812983e-06}, {"id": 1042, "seek": 531212, "start": 5316.48, "end": 5322.5199999999995, "text": " Totally untuned random forest got us in about the top 25 percent now remember our validation set", "tokens": [22837, 1701, 43703, 4974, 6719, 658, 505, 294, 466, 264, 1192, 3552, 3043, 586, 1604, 527, 24071, 992], "temperature": 0.0, "avg_logprob": -0.20222864247331715, "compression_ratio": 1.626923076923077, "no_speech_prob": 1.3925429129812983e-06}, {"id": 1043, "seek": 531212, "start": 5322.96, "end": 5329.16, "text": " Isn't identical to the Kaggle test set right and this competition unfortunately is old enough that you can't", "tokens": [6998, 380, 14800, 281, 264, 48751, 22631, 1500, 992, 558, 293, 341, 6211, 7015, 307, 1331, 1547, 300, 291, 393, 380], "temperature": 0.0, "avg_logprob": -0.20222864247331715, "compression_ratio": 1.626923076923077, "no_speech_prob": 1.3925429129812983e-06}, {"id": 1044, "seek": 531212, "start": 5329.8, "end": 5335.08, "text": " Even put in an in a kind of after this after the time entry to find out how you would have gone", "tokens": [2754, 829, 294, 364, 294, 257, 733, 295, 934, 341, 934, 264, 565, 8729, 281, 915, 484, 577, 291, 576, 362, 2780], "temperature": 0.0, "avg_logprob": -0.20222864247331715, "compression_ratio": 1.626923076923077, "no_speech_prob": 1.3925429129812983e-06}, {"id": 1045, "seek": 531212, "start": 5335.08, "end": 5338.96, "text": " So we can only approximate how we could have gone, but you know generally speaking", "tokens": [407, 321, 393, 787, 30874, 577, 321, 727, 362, 2780, 11, 457, 291, 458, 5101, 4124], "temperature": 0.0, "avg_logprob": -0.20222864247331715, "compression_ratio": 1.626923076923077, "no_speech_prob": 1.3925429129812983e-06}, {"id": 1046, "seek": 533896, "start": 5338.96, "end": 5342.88, "text": " It's going to be a pretty good approximation so two two eight six", "tokens": [467, 311, 516, 281, 312, 257, 1238, 665, 28023, 370, 732, 732, 3180, 2309], "temperature": 0.0, "avg_logprob": -0.22318219404954176, "compression_ratio": 1.5116279069767442, "no_speech_prob": 1.5056982647365658e-06}, {"id": 1047, "seek": 533896, "start": 5344.6, "end": 5346.96, "text": " Here is the competition here's the public leaderboard", "tokens": [1692, 307, 264, 6211, 510, 311, 264, 1908, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.22318219404954176, "compression_ratio": 1.5116279069767442, "no_speech_prob": 1.5056982647365658e-06}, {"id": 1048, "seek": 533896, "start": 5350.44, "end": 5355.82, "text": " Two two eight six there we go 14th or 15th place", "tokens": [4453, 732, 3180, 2309, 456, 321, 352, 3499, 392, 420, 2119, 392, 1081], "temperature": 0.0, "avg_logprob": -0.22318219404954176, "compression_ratio": 1.5116279069767442, "no_speech_prob": 1.5056982647365658e-06}, {"id": 1049, "seek": 533896, "start": 5356.4800000000005, "end": 5362.2, "text": " So you know roughly speaking looks like we would be about in the top 20 of this competition", "tokens": [407, 291, 458, 9810, 4124, 1542, 411, 321, 576, 312, 466, 294, 264, 1192, 945, 295, 341, 6211], "temperature": 0.0, "avg_logprob": -0.22318219404954176, "compression_ratio": 1.5116279069767442, "no_speech_prob": 1.5056982647365658e-06}, {"id": 1050, "seek": 536220, "start": 5362.2, "end": 5369.3, "text": " With basically totally brainless random forest with some totally brainless minor hyper parameter tuning", "tokens": [2022, 1936, 3879, 3567, 1832, 4974, 6719, 365, 512, 3879, 3567, 1832, 6696, 9848, 13075, 15164], "temperature": 0.0, "avg_logprob": -0.22050224304199217, "compression_ratio": 1.8040816326530613, "no_speech_prob": 1.328773464592814e-06}, {"id": 1051, "seek": 536220, "start": 5369.8, "end": 5371.8, "text": " and so", "tokens": [293, 370], "temperature": 0.0, "avg_logprob": -0.22050224304199217, "compression_ratio": 1.8040816326530613, "no_speech_prob": 1.328773464592814e-06}, {"id": 1052, "seek": 536220, "start": 5373.32, "end": 5376.36, "text": " This is kind of why the random forest is such an important", "tokens": [639, 307, 733, 295, 983, 264, 4974, 6719, 307, 1270, 364, 1021], "temperature": 0.0, "avg_logprob": -0.22050224304199217, "compression_ratio": 1.8040816326530613, "no_speech_prob": 1.328773464592814e-06}, {"id": 1053, "seek": 536220, "start": 5377.08, "end": 5383.08, "text": " Not just first step, but often only step from machine learning because it's kind of hard to screw it up like", "tokens": [1726, 445, 700, 1823, 11, 457, 2049, 787, 1823, 490, 3479, 2539, 570, 309, 311, 733, 295, 1152, 281, 5630, 309, 493, 411], "temperature": 0.0, "avg_logprob": -0.22050224304199217, "compression_ratio": 1.8040816326530613, "no_speech_prob": 1.328773464592814e-06}, {"id": 1054, "seek": 536220, "start": 5383.599999999999, "end": 5386.639999999999, "text": " Even when we didn't tune the hyper parameters we still got a good result", "tokens": [2754, 562, 321, 994, 380, 10864, 264, 9848, 9834, 321, 920, 658, 257, 665, 1874], "temperature": 0.0, "avg_logprob": -0.22050224304199217, "compression_ratio": 1.8040816326530613, "no_speech_prob": 1.328773464592814e-06}, {"id": 1055, "seek": 536220, "start": 5387.16, "end": 5391.38, "text": " right and then a small amount of hyper parameter tuning got us a much better result and so", "tokens": [558, 293, 550, 257, 1359, 2372, 295, 9848, 13075, 15164, 658, 505, 257, 709, 1101, 1874, 293, 370], "temperature": 0.0, "avg_logprob": -0.22050224304199217, "compression_ratio": 1.8040816326530613, "no_speech_prob": 1.328773464592814e-06}, {"id": 1056, "seek": 539138, "start": 5391.38, "end": 5396.78, "text": " Any kind of model so and I'm particularly thinking of like linear type models", "tokens": [2639, 733, 295, 2316, 370, 293, 286, 478, 4098, 1953, 295, 411, 8213, 2010, 5245], "temperature": 0.0, "avg_logprob": -0.1516508829025995, "compression_ratio": 1.7163120567375887, "no_speech_prob": 2.2252720555115957e-06}, {"id": 1057, "seek": 539138, "start": 5397.78, "end": 5403.42, "text": " Which have a whole bunch of statistical assumptions, and you have to get a whole bunch of things right before they start to work at all", "tokens": [3013, 362, 257, 1379, 3840, 295, 22820, 17695, 11, 293, 291, 362, 281, 483, 257, 1379, 3840, 295, 721, 558, 949, 436, 722, 281, 589, 412, 439], "temperature": 0.0, "avg_logprob": -0.1516508829025995, "compression_ratio": 1.7163120567375887, "no_speech_prob": 2.2252720555115957e-06}, {"id": 1058, "seek": 539138, "start": 5404.54, "end": 5411.42, "text": " Can really throw you off track right because they give you like totally wrong answers about how accurate the predictions can be", "tokens": [1664, 534, 3507, 291, 766, 2837, 558, 570, 436, 976, 291, 411, 3879, 2085, 6338, 466, 577, 8559, 264, 21264, 393, 312], "temperature": 0.0, "avg_logprob": -0.1516508829025995, "compression_ratio": 1.7163120567375887, "no_speech_prob": 2.2252720555115957e-06}, {"id": 1059, "seek": 539138, "start": 5411.42, "end": 5413.42, "text": " For also the random forest", "tokens": [1171, 611, 264, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.1516508829025995, "compression_ratio": 1.7163120567375887, "no_speech_prob": 2.2252720555115957e-06}, {"id": 1060, "seek": 539138, "start": 5413.86, "end": 5415.86, "text": " You know generally speaking", "tokens": [509, 458, 5101, 4124], "temperature": 0.0, "avg_logprob": -0.1516508829025995, "compression_ratio": 1.7163120567375887, "no_speech_prob": 2.2252720555115957e-06}, {"id": 1061, "seek": 539138, "start": 5416.22, "end": 5420.62, "text": " They tend to work on most data sets most of the time with most sets of hyper parameters", "tokens": [814, 3928, 281, 589, 322, 881, 1412, 6352, 881, 295, 264, 565, 365, 881, 6352, 295, 9848, 9834], "temperature": 0.0, "avg_logprob": -0.1516508829025995, "compression_ratio": 1.7163120567375887, "no_speech_prob": 2.2252720555115957e-06}, {"id": 1062, "seek": 542062, "start": 5420.62, "end": 5422.62, "text": " so for example", "tokens": [370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.23445087566710354, "compression_ratio": 1.3673469387755102, "no_speech_prob": 1.1544555036380189e-06}, {"id": 1063, "seek": 542062, "start": 5425.38, "end": 5427.38, "text": " We", "tokens": [492], "temperature": 0.0, "avg_logprob": -0.23445087566710354, "compression_ratio": 1.3673469387755102, "no_speech_prob": 1.1544555036380189e-06}, {"id": 1064, "seek": 542062, "start": 5429.42, "end": 5433.12, "text": " Did this thing with it with our categorical variables in fact let's take a look at our tree", "tokens": [2589, 341, 551, 365, 309, 365, 527, 19250, 804, 9102, 294, 1186, 718, 311, 747, 257, 574, 412, 527, 4230], "temperature": 0.0, "avg_logprob": -0.23445087566710354, "compression_ratio": 1.3673469387755102, "no_speech_prob": 1.1544555036380189e-06}, {"id": 1065, "seek": 542062, "start": 5438.22, "end": 5440.22, "text": " Single tree", "tokens": [31248, 4230], "temperature": 0.0, "avg_logprob": -0.23445087566710354, "compression_ratio": 1.3673469387755102, "no_speech_prob": 1.1544555036380189e-06}, {"id": 1066, "seek": 544022, "start": 5440.22, "end": 5451.22, "text": " Look at this right fi product class desk less than 7.5. What does that mean so?", "tokens": [2053, 412, 341, 558, 15848, 1674, 1508, 10026, 1570, 813, 1614, 13, 20, 13, 708, 775, 300, 914, 370, 30], "temperature": 0.0, "avg_logprob": -0.3207641386649978, "compression_ratio": 1.5696202531645569, "no_speech_prob": 1.7880569203043706e-06}, {"id": 1067, "seek": 544022, "start": 5451.740000000001, "end": 5453.740000000001, "text": " fi product", "tokens": [15848, 1674], "temperature": 0.0, "avg_logprob": -0.3207641386649978, "compression_ratio": 1.5696202531645569, "no_speech_prob": 1.7880569203043706e-06}, {"id": 1068, "seek": 544022, "start": 5455.54, "end": 5457.54, "text": " Class desk", "tokens": [9471, 10026], "temperature": 0.0, "avg_logprob": -0.3207641386649978, "compression_ratio": 1.5696202531645569, "no_speech_prob": 1.7880569203043706e-06}, {"id": 1069, "seek": 544022, "start": 5459.18, "end": 5461.18, "text": " Here's some examples of that column", "tokens": [1692, 311, 512, 5110, 295, 300, 7738], "temperature": 0.0, "avg_logprob": -0.3207641386649978, "compression_ratio": 1.5696202531645569, "no_speech_prob": 1.7880569203043706e-06}, {"id": 1070, "seek": 544022, "start": 5461.860000000001, "end": 5467.18, "text": " All right, so what does it mean to be less than or equal to 7 well we'd have to look at dot cat", "tokens": [1057, 558, 11, 370, 437, 775, 309, 914, 281, 312, 1570, 813, 420, 2681, 281, 1614, 731, 321, 1116, 362, 281, 574, 412, 5893, 3857], "temperature": 0.0, "avg_logprob": -0.3207641386649978, "compression_ratio": 1.5696202531645569, "no_speech_prob": 1.7880569203043706e-06}, {"id": 1071, "seek": 546718, "start": 5467.18, "end": 5470.62, "text": " dot categories to find out", "tokens": [5893, 10479, 281, 915, 484], "temperature": 0.0, "avg_logprob": -0.18089719252152878, "compression_ratio": 1.5495049504950495, "no_speech_prob": 1.4593692867492791e-06}, {"id": 1072, "seek": 546718, "start": 5471.46, "end": 5475.900000000001, "text": " Okay, and so it's 0 1 2 3 4 5 6 7", "tokens": [1033, 11, 293, 370, 309, 311, 1958, 502, 568, 805, 1017, 1025, 1386, 1614], "temperature": 0.0, "avg_logprob": -0.18089719252152878, "compression_ratio": 1.5495049504950495, "no_speech_prob": 1.4593692867492791e-06}, {"id": 1073, "seek": 546718, "start": 5476.62, "end": 5480.9800000000005, "text": " so what it's done is it's created a split where all of the backhoe loaders and", "tokens": [370, 437, 309, 311, 1096, 307, 309, 311, 2942, 257, 7472, 689, 439, 295, 264, 646, 33810, 3677, 433, 293], "temperature": 0.0, "avg_logprob": -0.18089719252152878, "compression_ratio": 1.5495049504950495, "no_speech_prob": 1.4593692867492791e-06}, {"id": 1074, "seek": 546718, "start": 5481.780000000001, "end": 5487.14, "text": " These three types of hydraulic excavator enter in one group and everything else is in the other group", "tokens": [1981, 1045, 3467, 295, 32134, 34351, 1639, 3242, 294, 472, 1594, 293, 1203, 1646, 307, 294, 264, 661, 1594], "temperature": 0.0, "avg_logprob": -0.18089719252152878, "compression_ratio": 1.5495049504950495, "no_speech_prob": 1.4593692867492791e-06}, {"id": 1075, "seek": 546718, "start": 5487.9800000000005, "end": 5489.9800000000005, "text": " so like that's like", "tokens": [370, 411, 300, 311, 411], "temperature": 0.0, "avg_logprob": -0.18089719252152878, "compression_ratio": 1.5495049504950495, "no_speech_prob": 1.4593692867492791e-06}, {"id": 1076, "seek": 546718, "start": 5490.9400000000005, "end": 5494.62, "text": " Weird you know like like these aren't even in order", "tokens": [32033, 291, 458, 411, 411, 613, 3212, 380, 754, 294, 1668], "temperature": 0.0, "avg_logprob": -0.18089719252152878, "compression_ratio": 1.5495049504950495, "no_speech_prob": 1.4593692867492791e-06}, {"id": 1077, "seek": 549462, "start": 5494.62, "end": 5501.46, "text": " We could have made them in order if we had you know bothered to say the categories have this order, but we hadn't right so", "tokens": [492, 727, 362, 1027, 552, 294, 1668, 498, 321, 632, 291, 458, 22996, 281, 584, 264, 10479, 362, 341, 1668, 11, 457, 321, 8782, 380, 558, 370], "temperature": 0.0, "avg_logprob": -0.19804671075608996, "compression_ratio": 1.560846560846561, "no_speech_prob": 1.93335199583089e-06}, {"id": 1078, "seek": 549462, "start": 5502.46, "end": 5506.78, "text": " How come this even works like because when we turn it into codes?", "tokens": [1012, 808, 341, 754, 1985, 411, 570, 562, 321, 1261, 309, 666, 14211, 30], "temperature": 0.0, "avg_logprob": -0.19804671075608996, "compression_ratio": 1.560846560846561, "no_speech_prob": 1.93335199583089e-06}, {"id": 1079, "seek": 549462, "start": 5513.66, "end": 5515.66, "text": " It's actually", "tokens": [467, 311, 767], "temperature": 0.0, "avg_logprob": -0.19804671075608996, "compression_ratio": 1.560846560846561, "no_speech_prob": 1.93335199583089e-06}, {"id": 1080, "seek": 549462, "start": 5515.94, "end": 5519.14, "text": " This is actually what the random forest sees", "tokens": [639, 307, 767, 437, 264, 4974, 6719, 8194], "temperature": 0.0, "avg_logprob": -0.19804671075608996, "compression_ratio": 1.560846560846561, "no_speech_prob": 1.93335199583089e-06}, {"id": 1081, "seek": 551914, "start": 5519.14, "end": 5524.42, "text": " and so imagine to think about this imagine like", "tokens": [293, 370, 3811, 281, 519, 466, 341, 3811, 411], "temperature": 0.0, "avg_logprob": -0.141031161848321, "compression_ratio": 1.6745283018867925, "no_speech_prob": 3.9897057035886974e-07}, {"id": 1082, "seek": 551914, "start": 5524.700000000001, "end": 5531.22, "text": " The only thing that mattered was whether it was a hydraulic excavator 0 to 2 metric tons and nothing else mattered", "tokens": [440, 787, 551, 300, 44282, 390, 1968, 309, 390, 257, 32134, 34351, 1639, 1958, 281, 568, 20678, 9131, 293, 1825, 1646, 44282], "temperature": 0.0, "avg_logprob": -0.141031161848321, "compression_ratio": 1.6745283018867925, "no_speech_prob": 3.9897057035886974e-07}, {"id": 1083, "seek": 551914, "start": 5531.34, "end": 5535.660000000001, "text": " Imagine that right so it has to pick out this this single level well", "tokens": [11739, 300, 558, 370, 309, 575, 281, 1888, 484, 341, 341, 2167, 1496, 731], "temperature": 0.0, "avg_logprob": -0.141031161848321, "compression_ratio": 1.6745283018867925, "no_speech_prob": 3.9897057035886974e-07}, {"id": 1084, "seek": 551914, "start": 5535.660000000001, "end": 5539.34, "text": " It can do that because first of all it could say okay", "tokens": [467, 393, 360, 300, 570, 700, 295, 439, 309, 727, 584, 1392], "temperature": 0.0, "avg_logprob": -0.141031161848321, "compression_ratio": 1.6745283018867925, "no_speech_prob": 3.9897057035886974e-07}, {"id": 1085, "seek": 551914, "start": 5539.34, "end": 5543.46, "text": " Let's pick out everything less than 7 versus greater than 7 to create", "tokens": [961, 311, 1888, 484, 1203, 1570, 813, 1614, 5717, 5044, 813, 1614, 281, 1884], "temperature": 0.0, "avg_logprob": -0.141031161848321, "compression_ratio": 1.6745283018867925, "no_speech_prob": 3.9897057035886974e-07}, {"id": 1086, "seek": 554346, "start": 5543.46, "end": 5549.58, "text": " You know this as one group and this is another group right and then within this group", "tokens": [509, 458, 341, 382, 472, 1594, 293, 341, 307, 1071, 1594, 558, 293, 550, 1951, 341, 1594], "temperature": 0.0, "avg_logprob": -0.1744715437597158, "compression_ratio": 1.7751004016064258, "no_speech_prob": 4.6644427698083746e-07}, {"id": 1087, "seek": 554346, "start": 5549.58, "end": 5556.78, "text": " They could then pick out everything less than 6 versus greater than 6 which is going to pick out this one item right so with two", "tokens": [814, 727, 550, 1888, 484, 1203, 1570, 813, 1386, 5717, 5044, 813, 1386, 597, 307, 516, 281, 1888, 484, 341, 472, 3174, 558, 370, 365, 732], "temperature": 0.0, "avg_logprob": -0.1744715437597158, "compression_ratio": 1.7751004016064258, "no_speech_prob": 4.6644427698083746e-07}, {"id": 1088, "seek": 554346, "start": 5556.78, "end": 5557.94, "text": " split points", "tokens": [7472, 2793], "temperature": 0.0, "avg_logprob": -0.1744715437597158, "compression_ratio": 1.7751004016064258, "no_speech_prob": 4.6644427698083746e-07}, {"id": 1089, "seek": 554346, "start": 5557.94, "end": 5559.94, "text": " We can pull out a single category", "tokens": [492, 393, 2235, 484, 257, 2167, 7719], "temperature": 0.0, "avg_logprob": -0.1744715437597158, "compression_ratio": 1.7751004016064258, "no_speech_prob": 4.6644427698083746e-07}, {"id": 1090, "seek": 554346, "start": 5561.1, "end": 5566.36, "text": " So this is why it works right is because the tree is like infinitely flexible", "tokens": [407, 341, 307, 983, 309, 1985, 558, 307, 570, 264, 4230, 307, 411, 36227, 11358], "temperature": 0.0, "avg_logprob": -0.1744715437597158, "compression_ratio": 1.7751004016064258, "no_speech_prob": 4.6644427698083746e-07}, {"id": 1091, "seek": 554346, "start": 5566.9800000000005, "end": 5572.76, "text": " Even with a categorical variable if there's particular categories which have different levels of price", "tokens": [2754, 365, 257, 19250, 804, 7006, 498, 456, 311, 1729, 10479, 597, 362, 819, 4358, 295, 3218], "temperature": 0.0, "avg_logprob": -0.1744715437597158, "compression_ratio": 1.7751004016064258, "no_speech_prob": 4.6644427698083746e-07}, {"id": 1092, "seek": 557276, "start": 5572.76, "end": 5574.76, "text": " It can like", "tokens": [467, 393, 411], "temperature": 0.0, "avg_logprob": -0.1711690670976015, "compression_ratio": 1.6706349206349207, "no_speech_prob": 1.0511472510188469e-06}, {"id": 1093, "seek": 557276, "start": 5574.84, "end": 5577.96, "text": " Gradually zoom in on those groups by using multiple splits", "tokens": [16710, 671, 8863, 294, 322, 729, 3935, 538, 1228, 3866, 37741], "temperature": 0.0, "avg_logprob": -0.1711690670976015, "compression_ratio": 1.6706349206349207, "no_speech_prob": 1.0511472510188469e-06}, {"id": 1094, "seek": 557276, "start": 5578.64, "end": 5584.900000000001, "text": " All right now you can help it by telling it the order of your categorical variable, but even if you don't", "tokens": [1057, 558, 586, 291, 393, 854, 309, 538, 3585, 309, 264, 1668, 295, 428, 19250, 804, 7006, 11, 457, 754, 498, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.1711690670976015, "compression_ratio": 1.6706349206349207, "no_speech_prob": 1.0511472510188469e-06}, {"id": 1095, "seek": 557276, "start": 5585.64, "end": 5590.8, "text": " It's okay. It's just going to take a few more decisions to get there right and so you can see here", "tokens": [467, 311, 1392, 13, 467, 311, 445, 516, 281, 747, 257, 1326, 544, 5327, 281, 483, 456, 558, 293, 370, 291, 393, 536, 510], "temperature": 0.0, "avg_logprob": -0.1711690670976015, "compression_ratio": 1.6706349206349207, "no_speech_prob": 1.0511472510188469e-06}, {"id": 1096, "seek": 557276, "start": 5591.12, "end": 5594.860000000001, "text": " It's actually using this product class desk quite a few times", "tokens": [467, 311, 767, 1228, 341, 1674, 1508, 10026, 1596, 257, 1326, 1413], "temperature": 0.0, "avg_logprob": -0.1711690670976015, "compression_ratio": 1.6706349206349207, "no_speech_prob": 1.0511472510188469e-06}, {"id": 1097, "seek": 557276, "start": 5595.92, "end": 5602.24, "text": " Right and and as you go deeper down the tree you'll see it used more and more right", "tokens": [1779, 293, 293, 382, 291, 352, 7731, 760, 264, 4230, 291, 603, 536, 309, 1143, 544, 293, 544, 558], "temperature": 0.0, "avg_logprob": -0.1711690670976015, "compression_ratio": 1.6706349206349207, "no_speech_prob": 1.0511472510188469e-06}, {"id": 1098, "seek": 560224, "start": 5602.24, "end": 5606.4, "text": " So we're else in a linear model or almost any kind of other model certainly any", "tokens": [407, 321, 434, 1646, 294, 257, 8213, 2316, 420, 1920, 604, 733, 295, 661, 2316, 3297, 604], "temperature": 0.0, "avg_logprob": -0.20334422735520352, "compression_ratio": 1.5758928571428572, "no_speech_prob": 7.69036091696762e-07}, {"id": 1099, "seek": 560224, "start": 5607.4, "end": 5609.4, "text": " Any non-tree model pretty much?", "tokens": [2639, 2107, 12, 83, 701, 2316, 1238, 709, 30], "temperature": 0.0, "avg_logprob": -0.20334422735520352, "compression_ratio": 1.5758928571428572, "no_speech_prob": 7.69036091696762e-07}, {"id": 1100, "seek": 560224, "start": 5610.96, "end": 5617.44, "text": " Encoding a categorical variable like this won't work at all because there's no linear relationship between totally arbitrary", "tokens": [29584, 8616, 257, 19250, 804, 7006, 411, 341, 1582, 380, 589, 412, 439, 570, 456, 311, 572, 8213, 2480, 1296, 3879, 23211], "temperature": 0.0, "avg_logprob": -0.20334422735520352, "compression_ratio": 1.5758928571428572, "no_speech_prob": 7.69036091696762e-07}, {"id": 1101, "seek": 560224, "start": 5618.2, "end": 5625.48, "text": " Identifiers and anything right so so these are the kinds of things that make random forests very easy to use and and", "tokens": [25905, 23463, 293, 1340, 558, 370, 370, 613, 366, 264, 3685, 295, 721, 300, 652, 4974, 21700, 588, 1858, 281, 764, 293, 293], "temperature": 0.0, "avg_logprob": -0.20334422735520352, "compression_ratio": 1.5758928571428572, "no_speech_prob": 7.69036091696762e-07}, {"id": 1102, "seek": 562548, "start": 5625.48, "end": 5633.16, "text": " Very resilient and so by using that you know we've gotten ourselves a model which is clearly", "tokens": [4372, 23699, 293, 370, 538, 1228, 300, 291, 458, 321, 600, 5768, 4175, 257, 2316, 597, 307, 4448], "temperature": 0.0, "avg_logprob": -0.20386080119920813, "compression_ratio": 1.5627705627705628, "no_speech_prob": 2.2959054604143603e-06}, {"id": 1103, "seek": 562548, "start": 5633.879999999999, "end": 5637.679999999999, "text": " You know world-class at this point already", "tokens": [509, 458, 1002, 12, 11665, 412, 341, 935, 1217], "temperature": 0.0, "avg_logprob": -0.20386080119920813, "compression_ratio": 1.5627705627705628, "no_speech_prob": 2.2959054604143603e-06}, {"id": 1104, "seek": 562548, "start": 5637.679999999999, "end": 5642.4, "text": " It's like you know probably well in the top 20 of this Kaggle competition and then in our next lesson", "tokens": [467, 311, 411, 291, 458, 1391, 731, 294, 264, 1192, 945, 295, 341, 48751, 22631, 6211, 293, 550, 294, 527, 958, 6898], "temperature": 0.0, "avg_logprob": -0.20386080119920813, "compression_ratio": 1.5627705627705628, "no_speech_prob": 2.2959054604143603e-06}, {"id": 1105, "seek": 562548, "start": 5643.4, "end": 5645.4, "text": " we're going to learn about how to", "tokens": [321, 434, 516, 281, 1466, 466, 577, 281], "temperature": 0.0, "avg_logprob": -0.20386080119920813, "compression_ratio": 1.5627705627705628, "no_speech_prob": 2.2959054604143603e-06}, {"id": 1106, "seek": 562548, "start": 5648.16, "end": 5651.24, "text": " Analyze that model to learn more about the data to make it even better", "tokens": [1107, 5222, 1381, 300, 2316, 281, 1466, 544, 466, 264, 1412, 281, 652, 309, 754, 1101], "temperature": 0.0, "avg_logprob": -0.20386080119920813, "compression_ratio": 1.5627705627705628, "no_speech_prob": 2.2959054604143603e-06}, {"id": 1107, "seek": 562548, "start": 5652.719999999999, "end": 5654.719999999999, "text": " Okay, so this week", "tokens": [1033, 11, 370, 341, 1243], "temperature": 0.0, "avg_logprob": -0.20386080119920813, "compression_ratio": 1.5627705627705628, "no_speech_prob": 2.2959054604143603e-06}, {"id": 1108, "seek": 565472, "start": 5654.72, "end": 5662.0, "text": " I'm trying and like really experiment right have a look inside look try and draw the trees try and plot the", "tokens": [286, 478, 1382, 293, 411, 534, 5120, 558, 362, 257, 574, 1854, 574, 853, 293, 2642, 264, 5852, 853, 293, 7542, 264], "temperature": 0.0, "avg_logprob": -0.18783168072970408, "compression_ratio": 1.813953488372093, "no_speech_prob": 4.356797489890596e-06}, {"id": 1109, "seek": 565472, "start": 5662.52, "end": 5666.76, "text": " Different errors try maybe using different data sets to see how they work", "tokens": [20825, 13603, 853, 1310, 1228, 819, 1412, 6352, 281, 536, 577, 436, 589], "temperature": 0.0, "avg_logprob": -0.18783168072970408, "compression_ratio": 1.813953488372093, "no_speech_prob": 4.356797489890596e-06}, {"id": 1110, "seek": 565472, "start": 5667.4400000000005, "end": 5673.400000000001, "text": " Really experiment to try and get a sense and maybe try to like replicate things like write your own r-squared", "tokens": [4083, 5120, 281, 853, 293, 483, 257, 2020, 293, 1310, 853, 281, 411, 25356, 721, 411, 2464, 428, 1065, 367, 12, 33292, 1642], "temperature": 0.0, "avg_logprob": -0.18783168072970408, "compression_ratio": 1.813953488372093, "no_speech_prob": 4.356797489890596e-06}, {"id": 1111, "seek": 565472, "start": 5674.64, "end": 5676.64, "text": " You know write your own versions of some of these functions", "tokens": [509, 458, 2464, 428, 1065, 9606, 295, 512, 295, 613, 6828], "temperature": 0.0, "avg_logprob": -0.18783168072970408, "compression_ratio": 1.813953488372093, "no_speech_prob": 4.356797489890596e-06}, {"id": 1112, "seek": 565472, "start": 5677.4800000000005, "end": 5681.68, "text": " See if yeah, see how much you can really learn about your data set about the random forest", "tokens": [3008, 498, 1338, 11, 536, 577, 709, 291, 393, 534, 1466, 466, 428, 1412, 992, 466, 264, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.18783168072970408, "compression_ratio": 1.813953488372093, "no_speech_prob": 4.356797489890596e-06}, {"id": 1113, "seek": 568168, "start": 5681.68, "end": 5684.280000000001, "text": " Okay, see you on Thursday", "tokens": [50364, 1033, 11, 536, 291, 322, 10383, 50494], "temperature": 0.0, "avg_logprob": -0.3580216036902534, "compression_ratio": 0.7575757575757576, "no_speech_prob": 3.117218511761166e-05}], "language": "en"}