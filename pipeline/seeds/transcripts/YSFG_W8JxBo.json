{"text": " Last lesson, we looked at what random forests are and we looked at some of the tweaks that we could use to make them work better. So in order to actually practice this, we needed to have a Jupyter Notebook environment running. So we can either install Anaconda on our own computers, we can use AWS, or we can use Cressell.com that has everything up and running straight away, or else paperspace.com also works really well. So assuming that you've got all that going, hopefully you've had a chance to practice running some random forests this week. I think one of the things to point out though is that before we did any tweaks of any hyper parameters or any tuning at all, the raw defaults already gave us a very good answer for an actual dataset that we've got off-cattle. The tweaks aren't always the main piece, they're just tweaks. Sometimes they're totally necessary. But quite often you can go a long way without doing many tweaks at all. So today we're going to look at something I think maybe even more important than building a predictive model that's good at predicting, which is to learn how to interpret that model to find out what it says about your data, to actually understand your data better by using machine learning. And this is kind of contrary to the common refrain that things like random forests are black boxes that hide meaning from us. Now you'll see today that the truth is quite the opposite. The truth is that random forests allow us to understand our data deeper and more quickly than traditional approaches. The other thing we're going to learn today is how to look at larger datasets than those which you can import with just the defaults. And specifically we're going to look at a dataset with over 100 million rows, which is the current Kaggle competition for groceries forecasting. Did anybody have any questions outside of those two areas since we're covering that today or comments that they want to talk about? Question- Can you just talk a little bit about in general, I understand the details more now, but when do you know this is an applicable model to use? In general, I should try random forest here because that's the part that I'm still like, if I'm told to I can. Answer- The short answer is, I can't really think of anything offhand that it's definitely not going to be at least somewhat useful for, so it's always worth trying. I think really the question is, in what situations should I try other things as well? And the short answer to that question is, for unstructured data, what I call unstructured data, so where all the different data points represent the same kind of thing, like a waveform in a sound or speech, or the words in a piece of text, or the pixels in an image, you're almost certainly going to want to try deep learning. And then outside of those two, there's a particular type of model we're going to look at today called a collaborative filtering model, which it so happens that the groceries competition is of that kind, where neither of those approaches are quite what you want without some tweaks to them. Answer- You're saying neither, you're saying deep learning in random forest? Answer- Neither deep learning or random forest is exactly what you want. You need to kind of do some tweaks. If anybody thinks of other places where maybe neither of those techniques is the right thing to use, mention it on the forums, even if you're not sure, so we can talk about it. I think this is one of the more interesting questions. And to some extent it is a case of practice and experience, but I do think there are two main classes to know about. So last week, at the point where we had kind of done some of the key steps, like the CSV reading in particular, which took a minute or two, at the end of that we saved it to a feather format file. Just to remind you, that's because this is basically almost the same format that it lives in in RAM, so it's ridiculously fast to read and write stuff from feather format. So what we're going to do today is we're going to look at lesson 2 RF interpretation, and the first thing we're going to do is read that feather format file. Now one thing to mention is a couple of you pointed out during the week a really interesting little bug or little issue, which is in the PROC DF function. The PROC DF function, remember, finds the numeric columns which have missing values and creates an additional Boolean column as well as replacing the missing with medians, and also turns the categorical objects into the integer codes. A couple of you pointed out some key points about the missing value handling. The first one is that your test set may have missing values in some columns that weren't in your training set or vice versa. If that happens, you're going to get an error when you try to do the random forest, because it's going to say if that is missing field appeared in your training set but not in your test set, it ended up in the model. It's going to say you can't use that data set with this model because you're missing one of the columns it requires. That's problem number one. Problem number two is that the median of the numeric values in the test set may be different for the training set, so it may actually process it into something which has different semantics. I thought that was a really interesting point. What I did was I changed prop.df so it returns a third thing, NAs. The NAs thing it returns, it doesn't matter in detail what it is, but I'll tell you just so you know, that's a dictionary where the keys are the names of the columns that had missing values, and the values of the dictionary are the medians. So then optionally you can pass NAs as an additional argument to prop.df, and it'll make sure that it adds those specific columns and it uses those specific medians. So it's giving you the ability to say process this test set in exactly the same way as we process this training set. Question from the audience. Before you start doing work any day, I would start doing a git pull. If something's not working today that was working yesterday, check the forum where there will be an explanation of why. This library in particular is moving fast, but pretty much all the libraries that we use, including PyTorch in particular, move fast. So one of the things to do if you're watching this through the MOOC is to make sure that you go to course.fast.ai and check the links there because there will be links saying these are the differences from the course. So we're kept up to date because I can't edit what I'm saying. But do a git pull before you start each day. So I haven't actually updated all of the notebooks to add the extra return value. I will over the next couple of days, but if you're using them you'll just need to put an extra comma and a's here. Otherwise you'll get an error that it's returned 3 things and you only have room for 3 things. What I want to do before I talk about interpretation is to show you what the exact same process looks like when you're working with a really large dataset. You'll see it's kind of almost the same thing, but there's going to be a few cases where we can't use the defaults because the defaults kind of just run a little bit too slowly. So specifically I'm going to look at the Cabell Groceries Competition. So this competition... who is entering this competition? Who would like to have a go at explaining what this competition involves, what the data is and what you're trying to predict? Trying to predict the items on the shelf depending on lots of factors like oil prices. What do you say, predicting the items on the shelf? What are you actually predicting? How much change to have in stock to maximize their... It's not quite what we're predicting, but we'll try and fix that in a moment, so go on. And then there's a bunch of different datasets that you can use to do that. There's oil prices, there's stores, there's locations, and each of those can be used to try to predict it. Does anybody want to have a go at expanding on that? So we have a bunch of information on different products. So for every store, for every item, for every day, we have a lot of related information available, like the location where the store was located, the class of the product, and the units sold. And then based on this, we are supposed to forecast in a much shorter timeframe compared to the training data. For every item number, how much we think it's going to sell. So only the units and nothing else. So your ability to explain the problem you're working on is really, really important. So if you don't currently feel confident of your ability to do that, practice with someone who is not in this competition, tell them all about it. So in this case, or in any case really, the key things to understand a machine learning problem would be to say what are the independent variables and what is the dependent variable. So the dependent variable is the thing that you're trying to predict. The thing you're trying to predict is how many units of each kind of product were sold in each store on each day during a two-week period. So that's the thing that you're trying to predict. And the information you have to predict is how many units of each product at each store on each day were sold in the last few years, and for each store, some metadata about it, like where is it located and what class of store is it. For each type of product, you have some metadata about it, such as what category of product is it and so forth. For each date, we have some metadata about it, such as what was the oil price on that date. So this is what we would call a relational dataset. So a relational dataset is one where we have a number of different pieces of information that we can join together. Specifically, this kind of relational dataset is what we would refer to as a star schema. A star schema is a kind of data warehousing schema where we basically say there's some central transactions table. In this case, the central transactions table is train.csv, and it contains the number of units that were sold by date, by store ID, by item ID. So that's the central transactions table, very small, very simple. And then from that we can join various bits of metadata. And it's called a star schema because you can kind of imagine the transactions table in the middle and then all these different metadata tables join onto it, giving you more information about the date, the item ID, and the store ID. Sometimes you'll also see a snowflake schema, which means there might then be additional information joined onto maybe the items table that tells you about different item categories, and joined to the store table telling you about the state that the store is in, and so forth. So you can have a whole snowflake. So that's the basic information about this problem. The independent variables, the dependent variable, and you probably also want to think about things like the time frame. Now we start in exactly the same way as we did before, loading in exactly the same stuff, setting the path. But when we go read CSV, if you say limit memory equals false, then you're basically saying use as much memory as you like to figure out what kinds of data is here. It's going to run out of memory pretty much regardless of how much memory you have. So what we do in order to limit the amount of space that it takes up when we read it in is we create a dictionary for each column name to the data type of that column. And so for you to create this, it's basically up to you to run less or head or whatever on the dataset to see what the types are and to figure that out and pass them in. So then you can just pass in Dtype equals with that dictionary. And so check this out. We can read in the whole CSV file in 1 minute and 48 seconds, and there are 125.5 million rows. So when people say Python's slow, no Python's not slow. Python can be slow if you don't use it right, but we can actually pass 125 million CSV records in less than 2 minutes. My language hat on for just a moment. Actually, if it's fast, almost certainly it's going to C. So Python is a wrapper around a bunch of C code usually. So Python itself isn't actually very fast. So that was Terence Parr who writes things for writing programming languages for a living, and he is right. Python itself is not fast, but almost everything we want to do in Python and data science has been written for us in C, or actually more often in Cython, which is a Python-like language which compiles to C. And so most of the stuff we run in Python is actually running not just C code, but in Pandas a lot of it's written in assembly language, it's heavily optimized, behind the scenes, a lot of that is going back to actually calling Fortran-based libraries for linear algebra. So there's layers upon layers of speed that actually allow us to spend less than 2 minutes reading that much data. If we wrote our own CSV reader in pure Python, it takes thousands of times, at least thousands of times longer than the optimized versions. So for us, what we care about is the speed we can get in practice. And so this is pretty cool. As well as telling it what the different data types were, we also have to tell it as before which things do you want to parse as dates. Question. I noticed that in this dictionary you specify int64, int33, and int8. I was wondering in practice, is it faster if you all specify them to int or slower, or any performance consideration? So the key performance consideration here was to use the smallest number of bits that I could to fully represent the column. So if I had used int8 for item number, there are more than 255 item numbers. More specifically, the maximum item number is bigger than 255. So on the other hand, if I had used int64 for store number, it's using more bits than necessary. Given that the whole purpose here was to avoid running out of RAM, we don't want to be using up 8 times more memory than necessary. So the key thing was really about memory. And in fact, when you're working with large datasets, very often you'll find the slow piece is the actually reading and writing to RAM, not the actual CPU operations. So very often that's the key performance consideration. Also however, as a rule of thumb, smaller data types often will run faster, particularly if you can use SIMD, so that's Single Instruction Multiple Data Vectorized Code. It can pack more numbers into a single vector to run at once. Powerful Explorer. I'm not sure if this was all thoughfully simplified, not exactly right. Question. Once you do this, the shuffle thing beforehand is not needed anymore? Like maybe you send a random substation? So although here I've read in the whole thing, when I start, I never start by reading in the whole thing. So if you search the forum for shuf, you'll find some tips about how to use this unix command to get a random sample of data at the command prompt. And then you can just read that. And the nice thing is that that's a good way, for example, to find out what data types to use, to read in a random sample and let pandas figure it out for you. And in general, I do as much work as possible on a sample until I feel confident that I understand the sample before I move on. Having said that, what we're about to learn is some techniques for running models on this pool data set that are actually going to work on arbitrarily large data sets. And also I specifically wanted to talk about how to read in large data sets. One thing to mention, on promotion objects, objects are like saying create a general-purpose Python data type which is slow and memory-heavy. And the reason for that is that this is a boolean which also has missing values, so we need to deal with this before we can turn it into a boolean. So you can see after that, I then go ahead and say fill in the missing values with false. Now you wouldn't just do this without doing some checking ahead of time, but some exploratory data analysis shows that it seems that this is probably an appropriate thing to do. It seems that missing does mean false. Objects generally read in as strings, so replace the strings true and false with actual booleans, and then finally convert it to an actual boolean type. So at this point when I save this, this file now of 123 million records takes up something under 2.5 gigabytes of memory. So you can look at pretty large data sets even on pretty small computers, which is interesting. So at that point, now that it's in a nice fast format, look how fast it is. I can save it to feather format in under 5 seconds. So that's nice. And then because Pandas is generally pretty fast, you can do stuff like summarize every column of all 125 million records in 20 seconds. So the first thing I looked at here actually is the dates. Generally speaking, dates are just going to be really important in a lot of the stuff you do, particularly because any model that you put in in practice, you're going to be putting it in at some date that is later than the date that you trained it by definition. So if anything in the world changes, you need to know how your predictive accuracy changes as well. And so what you'll see on Kaggle, and what you should always do in your on projects, is make sure that your dates don't overlap. So in this case, the dates that we have in the training set go from 2013 to mid-August 2017. And then in our test set, we have the dates that go from one day later, August 16th, until the end of the month. So this is a key thing. You can't really do any useful machine learning until you understand this basic piece here, which is you've got 4 years of data and you're trying to predict the next 2 weeks. So that's just a fundamental thing that you're going to need to understand before you can really do a good job of this. And so as soon as I see that, what does that say to you? If you wanted to now use a smaller data set, should you use a random sample, or is there something better you could do? Probably from the bottom, more recent. Yeah, get the most recent. And if you ever have trouble answering questions like this, just try to make it as physical as possible. So it's like, I'm going to go to a shop next week and I've got a $5 bet with my brother as to whether I can guess how many cans of Coke are going to be on the shelf. Probably the best way to do that would be to go to the shop same day of the previous week and see how many cans of Coke are on the shelf and guess it's going to be the same. You wouldn't go and look at how many were there 4 years ago. Question from the audience. But couldn't 4 years ago that same time frame of the year be important? For example, how much Coke they have on the shelf at Christmas time is going to be way more than... Exactly. So it's not that there's no useful information from 4 years ago, and so we don't want to entirely throw it away. But as a first step, what's the simplest possible thing, it's kind of like submitting the means. I wouldn't submit the mean of 2012 sales, I would probably submit the mean of last month's sales. So we're just trying to think about how might we want to create some initial easy models, and later on we might want to weight it. So for example, we might want to weight more recent dates more highly, they're probably more relevant. But we should do a whole bunch of exploratory data analysis to check that. So here's what the bottom of that dataset looks like. You can see literally it's got a date, a store number, an item number, and it tells you whether or not that particular item was on sale at that particular store on that particular date, and then there's some arbitrary ID. So that's it. So now that we have read that in, we can do stuff like, this is interesting, again we have to take the log of the sales, and it's the same reason as we looked at last week, because we're trying to predict something that varies according to ratios. They told us in this competition that the root mean squared log error is the thing they care about, so we take a log. They mentioned also if you check the competition details, which you always should read carefully the definition of any project you do, they say that there are some negative sales that represent returns, and they tell us that we should consider them to be 0 for the purpose of this competition. So I clip the sales so that they fall between 0 and no particular maximum. So clip just means cut it off at that point, truncate it, and then take the log of that plus 1. Why do I do plus 1? Because again, if you check the details of the competition, that's what they tell you they're going to use, is they're not actually just taking the root mean squared log error, but the root mean squared log plus 1 error. Because log of 0 doesn't make sense. We can add the date part as usual, and again, it's taking a couple of minutes. So I would run through all this on a sample first, so everything takes 10 seconds to make sure it works, just to check everything looks reasonable before I go back, because I don't want to wait 2 minutes for something I don't know is going to work. But as you can see, all these lines of code are identical to what we saw for the bulldozers competition. In this case, all I'm reading in is the training set. I didn't need to run train cats because all of my data types are already numeric. If they weren't, I would need to call train cats, and then I would need to call apply cats to apply the same categorical codes that I now have in the training set to the validation set. I call propdf as before to check for missing values and so forth. So all of those lines of code are identical. These lines of code again are identical because root mean squared error is still all we care about. And then I've got 2 changes. The first is setRFSamples, which we learned about last week. So we've got 120-something million records. We probably don't want to create a tree from 120 million records. I don't even know how long that's going to take. I haven't had the time and patience to wait and see. So you could start with 10,000 or 100,000, maybe it runs in a few seconds, make sure it works, and you can figure out how much you can run. And so I found getting it to a million, it runs in under a minute. So the point here is there's no relationship between the size of the data set and how long it takes to build a random forest. The relationship is between the number of estimators multiplied by the sample size. Question from the audience. Just curious what n jobs is, because in the past it's always been negative 1, you made it 8 here. Yes, so the number of jobs is the number of cores that it's going to use. And I was running this on a computer that has about 60 cores, and I just found if you try to use all of them, it's going to spend so much time picking up jobs, it was a bit slower. So if you've got lots and lots of cores on your computer, sometimes you want less than. Negative 1 means use every single core. There's one more change I made, which is that I converted the data frame into an array of floats and then I fitted on that. Why did I do that? Because internally, inside the random forest code, they do that anyway. And so given that I wanted to run a few different random forests with a few different hyper-parameters, by doing it once myself, I saved that minute 37 seconds. So if you run a line of code and it takes quite a long time, so the first time I ran this random forest regressor, it kind of took 2 or 3 minutes, and I thought I don't really want to wait 2 or 3 minutes. You can always add in front of the line of code %prun. And what %prun does is it runs something called a profiler. And what a profiler does is it will tell you which lines of code behind the scenes took the most time. In this case, I noticed that there was a line of code inside scikit-learn that was this line of code, and it was taking all the time, nearly all the time. And so I thought I'll do that first, and then I'll pass in the result and then I won't have to do it again. So this thing of looking to see which things is taking up the time is called profiling. And in software engineering, it's one of the most important tools you have. Data scientists really underappreciate this tool, but you'll find amongst conversations on GitHub issues or on Twitter or whatever amongst the top data scientists, they're sharing and talking about profiles all the time. And that's how easy it is to get a profile. So for fun, try running prun from time to time on stuff that's taking 10-20 seconds and see if you can learn to interpret and use profiler outputs. Even though in this case I didn't write this scikit-learn class, I was still able to use the profile to figure out how to make it run over twice as fast by avoiding recalculating this each time. So in this case, I built my regressor, I decided to use 20 estimators. Something else that I noticed in the profiler is that I can't use OOBScore when I use setRF samples, because if I do, it's going to use the other 124 million rows to calculate the OOBScore, which is like it's still going to take forever. So I may as well have a proper validation set anyway. Besides which, I want a validation set that's the most recent dates rather than is random. So if you use setRF samples on a large dataset, don't put the OOBScore parameter in because it takes forever. So that got me a 0.76 validation root mean squared log error. And then I tried fiddling around with different min samples. So if I decrease the min samples from 100 to 10, it took a little bit more time to run as you would expect. And the error went down from 76 to 71, so that looked pretty good. So I kept decreasing it down to 3, and that brought this error down to 0.70. When I decreased it down to 1, it didn't really help. So I kind of had a reasonable random forest. When I say reasonable, though, it's not reasonable in the sense that it does not give a good result on the leaderboard. And so this is a very interesting question about why is that. The reason is really coming back to Savannah's question earlier, like where might random forests not work as well. Let's go back and look at the data. Here's the entire dataset. Here's all the columns that we used. So the columns that we have to predict with are the date, the store number, the item number, and whether it was on promotion or not. And then of course we used add date parts, so it's also going to be day of week, day of month, day of year, is quarter, start, etc. etc. So if you think about it, most of the insight around how much of something do you expect to sell tomorrow is likely to be very wrapped up in the details about where is that store, what kind of things do they tend to sell at that store, for that item, what category of item is it. If it's like fresh bread, they might not sell much of it on Sundays because on Sundays fresh bread doesn't get made. Where obviously it's gasoline, maybe they're going to sell a lot of gasoline because on Sundays people go and fill up their cart for the week ahead. Now a random forest has no ability to do anything other than create a bunch of binary splits on things like day of week, store number, item number. It doesn't know which one represents gasoline. It doesn't know which stores are in the center of the city versus which ones are out in the states. It doesn't know any of these things. So its ability to really understand what's going on is somewhat limited. So we're probably going to need to use the entire 4 years of data to even get some useful insights. But then as soon as we start using the whole 4 years of data, a lot of the data we're using is really old. So interestingly, there's a cable kernel that points out that what you could do is just take the last 2 weeks and take the average sales by date, by store number, by item number, and just submit that. And if you just submit that, you come about 30th. So for those of you in the grocery store, Terence has a comment or a question. I think this may have tripped me up. I think you said date, store, item. I think it's actually store, item, sales, and then you mean across date. Oh yeah, you're right. Store, item, and on promotion. If you do it by date as well, each row represents basically like a cross-tabulation of all of the sales in that store for that item. So if you put date in there as well, there's only going to be 1 or 2 items being averaged in each of those cells, which is too much variation basically, too sparse. It doesn't give you a terrible result, but it's not 30th. So your job if you're looking at this competition, and we'll talk about this in the next class, is how do you start with that model and make it a little bit better. Because if you can, then by the time we meet up next, hopefully you'll be above the top 30. Because Kaggle being Kaggle, lots of people have now taken this kernel and submitted it, and they all have about the same score. And the scores are ordered not just by score, but by date submitted. So if you now submit this kernel, you're not going to be 30th because you're way down the list of when it was submitted. But if you can do a tiny bit better, you're going to be better than all of those people. So try and think of how can you make this a tiny bit better. Question. Could you try to capture seasonality and trend effects by creating new columns like these are the average sales in the month of August, these are the average sales for this year? Answer. Yeah, I think that's a great idea. So the thing for you to think about is how to do that. See if you can make it work. Because there are details to get right, which I know Terrence has been working on this for the last week and he's gone almost crazy. The details are difficult. They're not intellectually difficult, they're kind of difficult in the way that makes you want to headbutt your desk at 2am. This is something to mention in general. The coding you do for machine learning is like it's incredibly frustrating and incredibly difficult. Not difficult technically, but difficult. If you get a detail wrong, much of the time it's not going to give you an exception. It'll just silently be slightly less good than it otherwise would have been. And if you're on Kaggle, at least you know, I'm not doing as well as other people on Kaggle. But if you're not on Kaggle, you just don't know. You don't know if your company's model is half as good as it could be because you made a little mistake. That's one of the reasons why practicing on Kaggle now is great. You're going to get practice in finding all of the ways in which you can infuriatingly screw things up. And you'll be amazed. For me, there's an extraordinary array of them. But as you get to know what they are, you'll start to know how to check for them as you go. You should assume every button you press, you're going to press the wrong button. And that's fine as long as you have a way to find out. We'll talk about that more during the course, but unfortunately there isn't a set of specific things I can tell you to always do. You just always have to think, what do I know about the results of this thing I'm about to do. I'll give you a really simple example. If you've actually created that basic entry where you take the mean by date by store number by on promotion, submit it, and you've got a reasonable score, and then you think you've got something that's a little bit better, and you do predictions for that, how about you now create a scatter plot showing the predictions of your average model on one axis versus the predictions of your new model on the other axis. You should see that they just about form a line. And if they don't, then that's a very strong suggestion that you've screwed something up. For a problem like this, unlike the car insurance problem on Kaggle where columns are unnamed, we know what the columns represent and what they are. How often do you pull in data from other sources to supplement that? Maybe weather data, for example, or how often is that used? Very often. So the whole point of this star schema is that you've got your central table and then you've got these other tables coming off that provide metadata about it. So for example, weather is metadata about a date. On Kaggle specifically, most competitions have the rule that you can use external data as long as you post on the forum that you're using it and that it's publicly available. But you have to check on a competition-by-competition basis, they will tell you. Outside of Kaggle, you should always be looking for what external data could I possibly leverage here. Question from the audience. Are we still talking about how to tweak this dataset? If you wish. Well, I'm not familiar with the countries here. This is Ecuador. Ecuador. So maybe I would start looking for Ecuador's holidays and shopping holidays, maybe when they have a 3-day weekend or a week off. That information is provided in this case. So in general, one way of tackling this kind of problem is to create lots and lots of new columns containing things like average number of sales on holidays, average percent change in sale between January and February, and so on and so forth. If you have a look at, there's been a previous competition on Kaggle called Rossmann Store Sales that was almost identical. It was in Germany in this case for a major grocery chain, how many items are sold by day by item type by store. In this case, the person who won quite unusually actually was something of a domain expert in this space. They're actually a specialist in doing logistics predictions. This is basically what they did. He's a professional sales forecast consultant. He created just lots and lots and lots of columns based on his experience of what kinds of things tend to be useful for making predictions. So that's an approach that can work. The third-place team did almost no feature engineering, however, and also they had one big oversight which I think they would have won if they hadn't had it. So you don't necessarily have to use this approach. We'll be learning a lot more about how to win this competition and ones like it as we go. They did interview the third-place team, so if you Google for Kaggle Rossmann, you'll see it. The short answer is they used BigBlood. So Terrence is actually my teammate on this competition. Terrence drew a couple of these charts for us, and I wanted to talk about this. If you don't have a good validation set, it's hard if not impossible to create a good model. So in other words, if you're trying to predict next month's sales, and you try to build a model, and you have no way of really knowing whether the models you built are good at predicting sales a month ahead of time, then you have no way of knowing when you put your model in production whether it's actually going to be any good. So you need a validation set that you know is reliable at telling you whether or not your model is likely to work well when you put it into production or use it on the test set. So in this case, what Terrence has plotted here is, normally you should not use your test set for anything other than using it right at the end of the competition or right at the end of the project to find out how you've gone. But there's one thing I'm going to let you use the test set for in addition, and that is to calibrate your validation set. So what Terrence did here was he built four different models, some which he thought would be better than others, and he submitted each of the four models to Kaggle to find out its score. So the x-axis is the score that Kaggle told us on the leaderboard. And then on the y-axis, he plotted the score on a particular validation set he was trying out to see whether this validation set looked like it was going to be any good. So if your validation set is good, then the relationship between the leaderboard score, the test set score, and your validation set score should lie in a straight line. Ideally, it'll actually lie on the y equals x line. But honestly, that doesn't matter too much. As long as relatively speaking it tells you which models are better than which other models, then you know which model is the best. And you know how it's going to perform on the test set because you know the linear relationship between the two things. So in this case, Terrence has managed to come up with a validation set which is looking like it's going to predict our Kaggle leaderboard score pretty well. And that's really cool, right, because now he can go away and try 100 different types of models, feature engineering, weighting, tweaking, hyperparameters, whatever else, see how they go in the validation set and not have to submit to Kaggle. So we're going to get a lot more iterations, a lot more feedback. This is not just true of Kaggle, but every machine learning project you do. And so here's a different one he tried where it wasn't as good. It's like, oh, these ones that were quite close to each other, it's showing us the opposite direction. That's a really bad sign. It's like, okay, this validation set idea didn't seem like a good idea, this validation set idea didn't look like a good idea. So in general, if your validation set's not showing a nice straight line, you need to think carefully. How is the test set constructed? How is my validation set different? There's some way you're constructing it which is different. You're going to have to draw lots of charts and so forth. So one question is, and I'm going to try to guess how you did it. So how do you actually try to construct this validation set as close to the... So what I would try to do is to try to sample points from the training set that are very close or possible to some of the points in the test set. Close in what sense? I don't know. I will have to find the features. What would you guess? What in this case, for this grocery is? The last points? Close by date. So basically all the different things Terence was trying were different variations of close by date. So the most recent. What I noticed was, so first I looked at the date range of the test set and then I looked at the kernel that described how he or she... So here's the date range of the test set. So the last two weeks of August 26, 2017. That's right. And then the person who submitted the kernel that said how to get the 0.58 leaderboard position or whatever score. The average by group. I looked at the date range of that and that was... Like nine or 10 days. Well, it was actually 14 days and the test set is 16 days. But the interesting thing is the test set begins on the day after payday and ends on the payday. And so these are things I also paid attention to. And I think that's one of the bits of metadata that they told us. These are the kinds of things you just got to try. Like I said, plot lots of pictures. Even if you didn't know it was payday, you would want to draw the time series chart of sales and you would hopefully see that every two weeks there would be a spike or whatever. And you'd be like, oh I want to make sure that I have the same number of spikes in my validation set that I had in my test set. Let's take a 5-minute break and let's come back at 2.32. So this is my favorite bit, interpreting machine learning models. By the way, if you're looking for my notebook about the groceries competition, you won't find it in GitHub because I'm not allowed to share code for running competitions with you unless you're on the same team as me. That's the rule. After the competition is finished, it'll be on GitHub. So if you're doing this for the video, you should be able to find it. Let's start by reading in our feather file. Our feather file is exactly the same as our CSV file. This is for our Blue Book for Bulldozers competition. We're trying to predict the sale price of heavy industrial equipment at auction. So reading the feather format file means that we've already read in the CSV and processed it into categories. So the next thing we do is to run proc df in order to turn the categories into integers, deal with the missing values and pull out the dependent variable. This is exactly the same thing as we used last time to create a validation set, where the validation set represents the last couple of weeks, the last 12,000 records by date. I discovered, thanks to one of your excellent questions on the forum last week, I had a bug here, which is that proc df was shuffling the order. Last week we saw a particular version of proc df where we passed in a subset. When I passed in a subset, it was randomly shuffling. So then when I said split-vowels, it wasn't getting the last rows by date, but it was getting a random set of rows. So I've now fixed that. So if you rerun the Lesson 1 RF code, you'll see slightly different results. Specifically you'll see in that section that my validation set results look less good, but that's only for this tiny little bit where I had subset equals set. I'm a little bit confused about the notation here. So nas is both an input variable and it's also the output variable of this function. Why is that? The proc df returns a dictionary telling you which columns were missing and for each of those columns what the median was. So when you call it on the larger data set, the non-subset, you want to take that return value and you don't pass in an nad to that point, you just want to get back the result. Later on when you pass it into a subset, you want to have the same missing columns and the same medians, so you pass it in. And if this different subset, like if it was a whole different data set, turned out to have some different missing columns, it would update that dictionary with additional key values as well. So you don't have to pass it in. If you don't pass it in, then it just gives you the information about what was missing and the medians. If you do pass it in, it uses that information for any missing columns that are there, and if there are some new missing columns, it will update that dictionary with that additional information. So it's like keeping all data sets, column information. Yes, it's going to keep track of any missing columns that you came across in anything you passed a property at. So we split it into the training and test set just like we did last week. So to remind you, once we've done prop.df, this is what it looks like, this is the log of sale price. So the first thing to think about is we already know how to get the predictions, which is we take the average value in each leaf node in each tree after running a particular row through each tree. That's how we get the prediction. But normally we don't just want a prediction, we also want to know how confident we are of that prediction. And so we would be less confident of a prediction if we haven't seen many examples of rows like this one. And if we haven't seen many examples of rows like this one, then we wouldn't expect any of the trees to have a path through which is really designed to help us predict that row. And so conceptually you would expect then that as you pass this unusual row through different trees, it's going to end up in very different places. So in other words, rather than just taking the mean of the predictions of the trees and saying that's our prediction, what if we took the standard deviation of the predictions of the trees? So the standard deviation of the predictions of the trees, if that's high, that means each tree is giving us a very different estimate of this row's prediction. So if this was a really common kind of row, then the trees would have learned to make good predictions for it because it's seen lots of opportunities to split based on those kinds of rows. So the standard deviation of the predictions across the trees gives us some kind of at least relative understanding of how confident we are of this prediction. So that is not something which exists in Scikit-learn or in any library I know of, so we have to create it. But we already have almost the exact code we need, because remember last lesson we actually manually calculated the averages across different sets of trees, so we can do exactly the same thing to calculate the standard deviations. So when I'm doing random forest interpretation, I pretty much never use the full data set. I always call setRFSamples because we don't need a massively accurate random forest, we just need one which indicates the nature of the relationships involved. So I just make sure this number is high enough that if I call the same interpretation commands multiple times, I don't get different results back each time. That's like the rule of thumb about how big does it need to be. But in practice, 50,000 is a high number, and most of the time it would be surprising if that wasn't enough, and it runs in seconds. So I generally start with 50,000. So with my 50,000 samples per tree set, I create 40 estimators. I know from last time that minSamplesLeaf equals 3, maxFeatures equals.5 isn't bad, and again we're not trying to create the world's most predictive tree anyway, so that all sounds fine. We get an R squared on the validation set of.89. Again, we don't particularly care, but as long as it's good enough, which it certainly is. And so here's where we can do that exact same list comprehension as last time. Remember, go through each estimator, that's each tree, call.predict on it with our validation set, make that a list comprehension and pass that to np.stack, which concatenates everything in that list across a new axis. So now our rows are the results of each tree and our columns are the result of each row in the original data set. And then we remember we can calculate the mean. So here's the prediction for our data set row number 1, and here's our standard deviation. So here's how to do it for just one observation at the end here. We've calculated for all of them, just predicting it for one here. This can take quite a while, and specifically it's not taking advantage of the fact that my computer has lots of cores in it. The list comprehension itself is Python code, it's my Python code. Python code, unless you're doing special stuff, runs in serial, which means it runs on a single CPU. It doesn't take advantage of your multi-CPU hardware. And so if I wanted to run this on more trees and more data, this one second is going to go up. And you see here the wall time, the amount of actual time it took, is roughly equal to the CPU time, where else if it was running on lots of cores, the CPU time would be higher than the wall time. So it turns out that Fast.ai provides a handy function called parallelTrees, which calls some stuff inside scikit-learn. And parallelTrees takes two things. It takes a random forest model that I trained, and some function to call. And it calls that function on every tree in parallel. So in other words, rather than calling t.predictXvalid, let's create a function that calls t.predictXvalid. Let's use parallelTrees to call it on our model for every tree, and it will return a list of the result of applying that function to every tree. And so then we can np.stack that. So hopefully you can see that that code and that code are basically the same thing. But this one is doing it in parallel. And so you can see here now our wall time has gone down to 500 milliseconds, and it's now giving us exactly the same answer. So a little bit faster. Time permitting, we'll talk about more general ways of writing code that runs in parallel, because it turns out to be super useful for data science. But here's one that we can use that's very specific to random forests. So what we can now do is we can always call this to get our predictions for each tree, and then we can call standard deviation to then get them for every row. And so let's try using that. So what I could do is let's create a copy of our data and let's add an additional column to it, which is the standard deviation of the predictions across the first axis. And let's also add in the mean, so they're the predictions themselves. So you might remember from last lesson that one of the predictors we have is called enclosure, and we'll see later on that this is an important predictor. And so let's start by just doing a histogram. So one of the nice things in Pandas is it's got built-in plotting capabilities. It's well worth Googling for Pandas plotting to see how to do it. So we don't know what it means, and it doesn't matter. I guess the whole purpose of this process is that we're going to figure out, we're going to learn about what things are, or at least what things are important, and we'll figure out what they are and how they're important. So we're going to start out knowing nothing about this dataset. So I'm just going to look at something called enclosure that has something called erops and something called erops, and I don't even know what this is yet. All I know is that the only three that really appear in any great quantity are erops, erops, WAC, and erops. This is really common as a data scientist. You often find yourself looking at data that you're not that familiar with, and you've got to figure out at least which bits to study more carefully, and which bits seem to matter, and so forth. So in this case, I at least know that these three groups I really don't care about because they basically don't exist. So given that, we're going to ignore those three. So we're going to focus on this one here, this one here, and this one here. So here you can see what I've done is I've taken my data frame and I've grouped by enclosure, and I am taking the average of these three fields. So here you can see the average sale price, the average prediction, and the standard deviation of prediction for each of my three groups. So I can already start to learn a bit here. As you would expect, the prediction and the sale price are close to each other on average, so that's a good sign. And then the standard deviation varies a little bit. It's a little hard to see in a table, so what we could do is we could try to start printing these things out. So here we've got the sale price for each level of enclosure, and here we've got the prediction for each level of enclosure, and for the error bars, I'm using the standard deviation of prediction. So here you can see the actual, and here's the prediction, and here's my confidence interval. Or at least it's the average of the standard deviation of the random forests. So this tells us, it'll tell us if there's some groups or some rows that we're not very confident of at all. So we could do something similar for product size. So here's different product sizes. We can do exactly the same thing of looking at our predictions and our standard deviations. We could sort by, and what we could say is, what's the ratio of the standard deviation of the predictions to the predictions themselves? So you'd kind of expect on average that when you're predicting something that's a bigger number, that your standard deviation would be higher. So you can sort by that ratio. And what that tells us is that the product size large and product size compact, our predictions are less accurate, relatively speaking, as a ratio of the total price. So then if we go back and have a look, well there you go, that's why. From the histogram, those are the smallest groups. So as you would expect, in small groups we're doing a less good job. So this confidence interval you can really use for two main purposes. One is that you can group it up like this and look at the average confidence interval by group to find out are there some groups that you just don't seem to have confidence about those groups. But perhaps more importantly, you can look at them for specific rows. So when you put it in production, you might always want to see the confidence interval. So if you're doing say your credit scoring, so deciding whether to give somebody a loan, you probably want to see not only what's their level of risk, but how confident are we. And if they want to borrow lots of money and we're not at all confident about our ability to predict whether they'll pay it back, we might want to give them a smaller loan. So those are the two ways in which you would use this. Let me go to the next one, which is the most important. The most important is feature importance. The only reason I didn't do this first is because I think the intuitive understanding of how to calculate confidence interval is the easiest one to understand intuitively. In fact, it's almost identical to something we've already calculated. But in terms of which one do I look at first in practice, I always look at this in practice. So when I'm working on whether it be a cattle competition or a real-world project, I build a random forest as fast as I can, try and get it to the point that it's significantly better than random but doesn't have to be much better than that. And then the next thing I do is to plot the feature importance. And the feature importance tells us in this random forest which columns matter. So we had dozens and dozens of columns originally in this dataset, and here I'm just picking out the top 10. So you can just call rf feature importance, again this is part of the FastAI library, it's leveraging stuff that's in scikit-learn, pass in the model, pass in the data frame, because we need to know the names of the columns, and it'll give you back a pandas data frame showing you in order of importance how important was each column. And here I'm just going to pick out the top 10. So we can then plot that. So fi, because it's a data frame, we can use data frame plotting commands. So here I've plotted all of the feature importances. And so you can see here, and I haven't been able to write all of the names of the columns at the bottom, which that's not the important thing. The important thing is to see that some columns are really, really important, and most columns don't really matter at all. And in nearly every dataset you use in real life, this is what your feature importance is going to look like. It's going to say there's a handful of columns that you care about. And this is why I always start here, because at this point, in terms of looking into learning about this domain of heavy industrial equipment options, I'm only going to care about learning about the columns which matter. So are we going to bother learning about enclosure? It depends whether enclosure is important. And there it is, it's in the top 10. So we are going to have to learn about enclosure. So then we could also plot this as a bar plot. So here I've just created a tiny little function here that's going to just plot my bars, and I'm just going to do it for the top 30. So you can see the same basic shape here, and I can see there's my enclosure. So we're going to learn about how this is calculated in just a moment. But before we worry about how it's calculated, much more important is to know what to do with it. So the most important thing to do with it is to now sit down with your client or your data dictionary or whatever your source of information is, and say to them, tell me about year made. What does that mean? Where does that come from? Plot lots of things like histograms of year made, scatter plots of year made against price, and learn everything you can, because year made and coupler system, they're the things that matter. And what will often happen in real-world projects is that you'll sit with the client and you'll say, oh it turns out the coupler system is the second most important thing, and then they might say, that makes no sense. Now that doesn't mean there's a problem with your model. It means there's a problem with their understanding of the data that they gave you. So let me give you an example. I entered a Kaggle competition where the goal was to predict which applications for grants at a university would be successful. And I used this exact approach and I discovered a number of columns which were almost entirely predictive of the dependent variable. And specifically when I then looked to see in what way they were predictive, it turned out that whether they were missing or not was basically the only thing that mattered in this dataset. And so later on, I ended up winning that competition, and I think a lot of it was thanks to this insight. And so later on I heard what had happened. It turns out that at that university, there's an administrative burden to filling out the database. And so for a lot of the grant applications, they don't fill in the database for the folks whose applications weren't accepted. So in other words, these missing values in the dataset were saying, this grant wasn't accepted, because if it was accepted, then the admin folks are going to go in and type in that information. So this is what we call data leakage. And data leakage means there's information in the dataset that I was modeling with which the university wouldn't have had in real life at the point in time they were making a decision. So when they're actually deciding which grant applications should I prioritize, they don't actually know which ones the admin staff are going to add information to because it turns out they got accepted. So one of the key things you'll find here is data leakage problems, and that's a serious problem that you need to deal with. The other thing that will happen is you'll often find it's signs of collinearity. And I think that's what's happened here with Kapler system. I think Kapler system tells you whether or not a particular kind of heavy industrial equipment has a particular feature on it. But if it's not that kind of industrial equipment at all, it will be empty, it will be missing. And so Kapler system is really telling you whether or not it's a certain class of heavy industrial equipment. Now this is not leakage, this is actual information you actually have at the right time. It's just that interpreting it, you have to be careful. So I would go through at least the top 10 or look for where the natural breakpoints are and really study these things carefully. To make life easier for myself, what I tend to do is I try to throw some data away and see if that matters. So in this case, I had a random forest which, let's go and see how accurate it was,.889. What I did was I said here, let's go through our feature importance data frame and filter out those where the importance is greater than.005. So.005 is about here, it's kind of like where they really flatten off. So let's just keep those. And so that gives us a list of 25 column names. And so then I say, let's now create a new data frame view which just contains those 25 columns, call split-vowels on it again, and create a new random forest. And let's see what happens. And you can see here the R-squared basically didn't change,.891 versus.89. So it's actually increased a tiny bit. Generally speaking, removing redundant columns, obviously it shouldn't make it worse. If it makes it worse, they won't be redundant after all. It might make it a little better, because if you think about how we built these trees, when it's deciding what to split on, it's got less things to have to worry about trying, it's less often going to accidentally find a crappy column. So it's got a slightly better opportunity to create a slightly better tree with slightly less data, but it's not going to change it by much. But it's going to make it a bit faster and it's going to let us focus on what matters. So if I rerun feature importance now, I've now got 25. Now the key thing that's happened is that when you remove redundant columns, you're also removing sources of collinearity. In other words, two columns that might be related to each other. Now collinearity doesn't make your random forest less predictive, but if you have two columns that are related to each other, this column is a little bit related to this column, and this column is a strong driver of the dependent variable, then what's going to happen is that the importance is going to end up split between the two collinear columns. It's going to say both of those columns matter, so it's going to split it between the two. So by removing some of those columns with very little impact, it makes your feature importance plot clearer. So you can see here actually, yearMade was pretty close to couple system before, but there must have been a bunch of things that were collinear with yearMade, which makes perfect sense. Old industrial equipment wouldn't have had a bunch of technical features that new ones would, for example. So it's actually saying, oh, okay, yearMade really, really matters. So I trust this feature importance better. The predictive accuracy of the model is a tiny bit better, but this feature importance has a lot less collinearity to confuse us. So let's talk about how this works. It's actually really simple, and not only is it really simple, it's a technique you can use not just for random forests, but for basically any kind of machine learning model. Interestingly, almost no one knows that. Many people will tell you, oh, this particular kind of model, there's no way of interpreting it. The most important interpretation of a model is knowing which things are important. That's almost certainly not going to be true, because this technique I'm going to teach you actually works for any kind of model. So here's what we're going to do. We're going to take our dataset, the bulldozers, and we've got this column which we're trying to predict, which is price. And then we've got all of our independent variables. So here's an independent variable here, yearMade, plus a whole bunch of other variables. And remember, after we did a bit of trimming, we had 25 independent variables. How do we figure out how important yearMade is? Well, we've got our whole random forest, and we can find out our predictive accuracy. So we're going to put all of these rows through our random forest, and we're going to spit out some predictions, and we're going to compare them to the actual price to get, in this case for example, our root mean squared error and our R squared. And that's our starting point. So now let's do exactly the same thing, but let's take the yearMade column and randomly shuffle it, randomly permute just that column. So now yearMade has exactly the same distribution as before, same means, standard deviation, but it's going to have no relationship to the dependent variable at all because we totally randomly reordered it. So before we might have found our R squared was.89, and then after we shuffle yearMade, we check again and now it's like.8. Our score got much worse when we destroyed that variable. Okay, let's try again. Let's put yearMade back to how it was, and this time let's take enclosure and shuffle that. And we find this time with enclosure, it's.84. And we can say, oh okay, so the amount of decrease in our score for yearMade was.09, and the amount of decrease in our score for enclosure was.05. And this is going to give us our feature importances for each one of our columns. Question from the audience. You could remove the column and train a whole new random forest, but that's going to be really slow. This way we can keep our random forest and just test the predictive accuracy of it again. So this is nice and fast by comparison. In this case, we just have to rerun every row forward through the forest for each shuffled column. We're just basically doing predictions after shuffling. So if you want to do multi-coloniality, would you do two of them and then random shuffle and then three of them random shuffle? I don't think you mean multi-coloniality, I think you mean looking for interaction effects. So if you want to say which pairs of variables are most important, you could do exactly the same thing, each pair in turn. In practice, there are better ways to do that because that's obviously computationally pretty expensive. So we'll try and find time to do that if we can. We now have a model which is a little bit more accurate and we've learned a lot more about it. So we're out of time. What I would suggest you try doing now before next class for this bulldozer's dataset is go through the top 5 or 10 predictors and try and learn what you can about how to draw plots in pandas and try to come back with some insights about what's the relationship between year made and the dependent variable, what's the histogram of year made. Try and find some possible, now that you know year made is really important, is there some noise in that column which we could fix, are there some weird encodings in that column that we could fix. This idea I had that maybe a couple of systems is there entirely because it's collinear with something else. Do you want to try and figure out whether that's true? If so, how would you do it? Fi product class desk, that rings alarm bells for me. It sounds like it might be a high cardinality categorical variable. It might be something with lots and lots of levels because it sounds like it's like a model name. So go and have a look at that model name. Does it have some ordering to it? Could you make it an ordinal variable to make it better? Does it have some kind of hierarchical structure in the string that we could split it on like hyphen to create more sub-columns? Have a think about this. So try and make it so that by Tuesday when you come back, ideally you've got a better accuracy than what I just showed because you found some new insights, or at least that you can tell the class about some things you've learned about how heavy industrial equipment options work in practice.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.56, "text": " Last lesson, we looked at what random forests are and we looked at some of the tweaks that", "tokens": [5264, 6898, 11, 321, 2956, 412, 437, 4974, 21700, 366, 293, 321, 2956, 412, 512, 295, 264, 46664, 300], "temperature": 0.0, "avg_logprob": -0.12749282663518732, "compression_ratio": 1.4370860927152318, "no_speech_prob": 0.0012064630864188075}, {"id": 1, "seek": 0, "start": 10.56, "end": 17.54, "text": " we could use to make them work better.", "tokens": [321, 727, 764, 281, 652, 552, 589, 1101, 13], "temperature": 0.0, "avg_logprob": -0.12749282663518732, "compression_ratio": 1.4370860927152318, "no_speech_prob": 0.0012064630864188075}, {"id": 2, "seek": 0, "start": 17.54, "end": 23.44, "text": " So in order to actually practice this, we needed to have a Jupyter Notebook environment", "tokens": [407, 294, 1668, 281, 767, 3124, 341, 11, 321, 2978, 281, 362, 257, 22125, 88, 391, 11633, 2939, 2823], "temperature": 0.0, "avg_logprob": -0.12749282663518732, "compression_ratio": 1.4370860927152318, "no_speech_prob": 0.0012064630864188075}, {"id": 3, "seek": 2344, "start": 23.44, "end": 31.92, "text": " running. So we can either install Anaconda on our own computers, we can use AWS, or we", "tokens": [2614, 13, 407, 321, 393, 2139, 3625, 1107, 326, 12233, 322, 527, 1065, 10807, 11, 321, 393, 764, 17650, 11, 420, 321], "temperature": 0.0, "avg_logprob": -0.16046024251867225, "compression_ratio": 1.6381322957198443, "no_speech_prob": 1.983205038413871e-05}, {"id": 4, "seek": 2344, "start": 31.92, "end": 37.92, "text": " can use Cressell.com that has everything up and running straight away, or else paperspace.com", "tokens": [393, 764, 383, 495, 14555, 13, 1112, 300, 575, 1203, 493, 293, 2614, 2997, 1314, 11, 420, 1646, 10577, 17940, 13, 1112], "temperature": 0.0, "avg_logprob": -0.16046024251867225, "compression_ratio": 1.6381322957198443, "no_speech_prob": 1.983205038413871e-05}, {"id": 5, "seek": 2344, "start": 37.92, "end": 40.32, "text": " also works really well.", "tokens": [611, 1985, 534, 731, 13], "temperature": 0.0, "avg_logprob": -0.16046024251867225, "compression_ratio": 1.6381322957198443, "no_speech_prob": 1.983205038413871e-05}, {"id": 6, "seek": 2344, "start": 40.32, "end": 44.8, "text": " So assuming that you've got all that going, hopefully you've had a chance to practice", "tokens": [407, 11926, 300, 291, 600, 658, 439, 300, 516, 11, 4696, 291, 600, 632, 257, 2931, 281, 3124], "temperature": 0.0, "avg_logprob": -0.16046024251867225, "compression_ratio": 1.6381322957198443, "no_speech_prob": 1.983205038413871e-05}, {"id": 7, "seek": 2344, "start": 44.8, "end": 47.120000000000005, "text": " running some random forests this week.", "tokens": [2614, 512, 4974, 21700, 341, 1243, 13], "temperature": 0.0, "avg_logprob": -0.16046024251867225, "compression_ratio": 1.6381322957198443, "no_speech_prob": 1.983205038413871e-05}, {"id": 8, "seek": 2344, "start": 47.120000000000005, "end": 53.36, "text": " I think one of the things to point out though is that before we did any tweaks of any hyper", "tokens": [286, 519, 472, 295, 264, 721, 281, 935, 484, 1673, 307, 300, 949, 321, 630, 604, 46664, 295, 604, 9848], "temperature": 0.0, "avg_logprob": -0.16046024251867225, "compression_ratio": 1.6381322957198443, "no_speech_prob": 1.983205038413871e-05}, {"id": 9, "seek": 5336, "start": 53.36, "end": 60.28, "text": " parameters or any tuning at all, the raw defaults already gave us a very good answer for an", "tokens": [9834, 420, 604, 15164, 412, 439, 11, 264, 8936, 7576, 82, 1217, 2729, 505, 257, 588, 665, 1867, 337, 364], "temperature": 0.0, "avg_logprob": -0.20807377703778154, "compression_ratio": 1.5353535353535352, "no_speech_prob": 1.2805294318241067e-05}, {"id": 10, "seek": 5336, "start": 60.28, "end": 67.92, "text": " actual dataset that we've got off-cattle. The tweaks aren't always the main piece, they're", "tokens": [3539, 28872, 300, 321, 600, 658, 766, 12, 66, 3327, 13, 440, 46664, 3212, 380, 1009, 264, 2135, 2522, 11, 436, 434], "temperature": 0.0, "avg_logprob": -0.20807377703778154, "compression_ratio": 1.5353535353535352, "no_speech_prob": 1.2805294318241067e-05}, {"id": 11, "seek": 5336, "start": 67.92, "end": 75.12, "text": " just tweaks. Sometimes they're totally necessary. But quite often you can go a long way without", "tokens": [445, 46664, 13, 4803, 436, 434, 3879, 4818, 13, 583, 1596, 2049, 291, 393, 352, 257, 938, 636, 1553], "temperature": 0.0, "avg_logprob": -0.20807377703778154, "compression_ratio": 1.5353535353535352, "no_speech_prob": 1.2805294318241067e-05}, {"id": 12, "seek": 5336, "start": 75.12, "end": 78.28, "text": " doing many tweaks at all.", "tokens": [884, 867, 46664, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.20807377703778154, "compression_ratio": 1.5353535353535352, "no_speech_prob": 1.2805294318241067e-05}, {"id": 13, "seek": 7828, "start": 78.28, "end": 85.36, "text": " So today we're going to look at something I think maybe even more important than building", "tokens": [407, 965, 321, 434, 516, 281, 574, 412, 746, 286, 519, 1310, 754, 544, 1021, 813, 2390], "temperature": 0.0, "avg_logprob": -0.0817106259174836, "compression_ratio": 1.6192660550458715, "no_speech_prob": 6.64336585032288e-06}, {"id": 14, "seek": 7828, "start": 85.36, "end": 90.64, "text": " a predictive model that's good at predicting, which is to learn how to interpret that model", "tokens": [257, 35521, 2316, 300, 311, 665, 412, 32884, 11, 597, 307, 281, 1466, 577, 281, 7302, 300, 2316], "temperature": 0.0, "avg_logprob": -0.0817106259174836, "compression_ratio": 1.6192660550458715, "no_speech_prob": 6.64336585032288e-06}, {"id": 15, "seek": 7828, "start": 90.64, "end": 97.2, "text": " to find out what it says about your data, to actually understand your data better by", "tokens": [281, 915, 484, 437, 309, 1619, 466, 428, 1412, 11, 281, 767, 1223, 428, 1412, 1101, 538], "temperature": 0.0, "avg_logprob": -0.0817106259174836, "compression_ratio": 1.6192660550458715, "no_speech_prob": 6.64336585032288e-06}, {"id": 16, "seek": 7828, "start": 97.2, "end": 107.56, "text": " using machine learning. And this is kind of contrary to the common refrain that things", "tokens": [1228, 3479, 2539, 13, 400, 341, 307, 733, 295, 19506, 281, 264, 2689, 46177, 300, 721], "temperature": 0.0, "avg_logprob": -0.0817106259174836, "compression_ratio": 1.6192660550458715, "no_speech_prob": 6.64336585032288e-06}, {"id": 17, "seek": 10756, "start": 107.56, "end": 112.24000000000001, "text": " like random forests are black boxes that hide meaning from us.", "tokens": [411, 4974, 21700, 366, 2211, 9002, 300, 6479, 3620, 490, 505, 13], "temperature": 0.0, "avg_logprob": -0.12097871049921563, "compression_ratio": 1.725, "no_speech_prob": 6.144063718238613e-06}, {"id": 18, "seek": 10756, "start": 112.24000000000001, "end": 117.64, "text": " Now you'll see today that the truth is quite the opposite. The truth is that random forests", "tokens": [823, 291, 603, 536, 965, 300, 264, 3494, 307, 1596, 264, 6182, 13, 440, 3494, 307, 300, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.12097871049921563, "compression_ratio": 1.725, "no_speech_prob": 6.144063718238613e-06}, {"id": 19, "seek": 10756, "start": 117.64, "end": 124.88, "text": " allow us to understand our data deeper and more quickly than traditional approaches.", "tokens": [2089, 505, 281, 1223, 527, 1412, 7731, 293, 544, 2661, 813, 5164, 11587, 13], "temperature": 0.0, "avg_logprob": -0.12097871049921563, "compression_ratio": 1.725, "no_speech_prob": 6.144063718238613e-06}, {"id": 20, "seek": 10756, "start": 124.88, "end": 131.64000000000001, "text": " The other thing we're going to learn today is how to look at larger datasets than those", "tokens": [440, 661, 551, 321, 434, 516, 281, 1466, 965, 307, 577, 281, 574, 412, 4833, 42856, 813, 729], "temperature": 0.0, "avg_logprob": -0.12097871049921563, "compression_ratio": 1.725, "no_speech_prob": 6.144063718238613e-06}, {"id": 21, "seek": 10756, "start": 131.64000000000001, "end": 137.32, "text": " which you can import with just the defaults. And specifically we're going to look at a", "tokens": [597, 291, 393, 974, 365, 445, 264, 7576, 82, 13, 400, 4682, 321, 434, 516, 281, 574, 412, 257], "temperature": 0.0, "avg_logprob": -0.12097871049921563, "compression_ratio": 1.725, "no_speech_prob": 6.144063718238613e-06}, {"id": 22, "seek": 13732, "start": 137.32, "end": 143.4, "text": " dataset with over 100 million rows, which is the current Kaggle competition for groceries", "tokens": [28872, 365, 670, 2319, 2459, 13241, 11, 597, 307, 264, 2190, 48751, 22631, 6211, 337, 31391], "temperature": 0.0, "avg_logprob": -0.32745336136727965, "compression_ratio": 1.3928571428571428, "no_speech_prob": 0.00018234868184663355}, {"id": 23, "seek": 13732, "start": 143.4, "end": 144.4, "text": " forecasting.", "tokens": [44331, 13], "temperature": 0.0, "avg_logprob": -0.32745336136727965, "compression_ratio": 1.3928571428571428, "no_speech_prob": 0.00018234868184663355}, {"id": 24, "seek": 13732, "start": 144.4, "end": 149.44, "text": " Did anybody have any questions outside of those two areas since we're covering that", "tokens": [2589, 4472, 362, 604, 1651, 2380, 295, 729, 732, 3179, 1670, 321, 434, 10322, 300], "temperature": 0.0, "avg_logprob": -0.32745336136727965, "compression_ratio": 1.3928571428571428, "no_speech_prob": 0.00018234868184663355}, {"id": 25, "seek": 13732, "start": 149.44, "end": 154.12, "text": " today or comments that they want to talk about?", "tokens": [965, 420, 3053, 300, 436, 528, 281, 751, 466, 30], "temperature": 0.0, "avg_logprob": -0.32745336136727965, "compression_ratio": 1.3928571428571428, "no_speech_prob": 0.00018234868184663355}, {"id": 26, "seek": 15412, "start": 154.12, "end": 167.44, "text": " Question- Can you just talk a little bit about in general, I understand the details more", "tokens": [14464, 12, 1664, 291, 445, 751, 257, 707, 857, 466, 294, 2674, 11, 286, 1223, 264, 4365, 544], "temperature": 0.0, "avg_logprob": -0.3518713799075804, "compression_ratio": 1.458100558659218, "no_speech_prob": 1.1478069609438535e-05}, {"id": 27, "seek": 15412, "start": 167.44, "end": 173.44, "text": " now, but when do you know this is an applicable model to use? In general, I should try random", "tokens": [586, 11, 457, 562, 360, 291, 458, 341, 307, 364, 21142, 2316, 281, 764, 30, 682, 2674, 11, 286, 820, 853, 4974], "temperature": 0.0, "avg_logprob": -0.3518713799075804, "compression_ratio": 1.458100558659218, "no_speech_prob": 1.1478069609438535e-05}, {"id": 28, "seek": 15412, "start": 173.44, "end": 177.32, "text": " forest here because that's the part that I'm still like, if I'm told to I can.", "tokens": [6719, 510, 570, 300, 311, 264, 644, 300, 286, 478, 920, 411, 11, 498, 286, 478, 1907, 281, 286, 393, 13], "temperature": 0.0, "avg_logprob": -0.3518713799075804, "compression_ratio": 1.458100558659218, "no_speech_prob": 1.1478069609438535e-05}, {"id": 29, "seek": 17732, "start": 177.32, "end": 187.04, "text": " Answer- The short answer is, I can't really think of anything offhand that it's definitely", "tokens": [24545, 12, 440, 2099, 1867, 307, 11, 286, 393, 380, 534, 519, 295, 1340, 766, 5543, 300, 309, 311, 2138], "temperature": 0.0, "avg_logprob": -0.1518091396851973, "compression_ratio": 1.7365853658536585, "no_speech_prob": 1.3211672921897843e-05}, {"id": 30, "seek": 17732, "start": 187.04, "end": 194.16, "text": " not going to be at least somewhat useful for, so it's always worth trying. I think really", "tokens": [406, 516, 281, 312, 412, 1935, 8344, 4420, 337, 11, 370, 309, 311, 1009, 3163, 1382, 13, 286, 519, 534], "temperature": 0.0, "avg_logprob": -0.1518091396851973, "compression_ratio": 1.7365853658536585, "no_speech_prob": 1.3211672921897843e-05}, {"id": 31, "seek": 17732, "start": 194.16, "end": 201.12, "text": " the question is, in what situations should I try other things as well? And the short", "tokens": [264, 1168, 307, 11, 294, 437, 6851, 820, 286, 853, 661, 721, 382, 731, 30, 400, 264, 2099], "temperature": 0.0, "avg_logprob": -0.1518091396851973, "compression_ratio": 1.7365853658536585, "no_speech_prob": 1.3211672921897843e-05}, {"id": 32, "seek": 17732, "start": 201.12, "end": 205.35999999999999, "text": " answer to that question is, for unstructured data, what I call unstructured data, so where", "tokens": [1867, 281, 300, 1168, 307, 11, 337, 18799, 46847, 1412, 11, 437, 286, 818, 18799, 46847, 1412, 11, 370, 689], "temperature": 0.0, "avg_logprob": -0.1518091396851973, "compression_ratio": 1.7365853658536585, "no_speech_prob": 1.3211672921897843e-05}, {"id": 33, "seek": 20536, "start": 205.36, "end": 211.28, "text": " all the different data points represent the same kind of thing, like a waveform in a sound", "tokens": [439, 264, 819, 1412, 2793, 2906, 264, 912, 733, 295, 551, 11, 411, 257, 36512, 294, 257, 1626], "temperature": 0.0, "avg_logprob": -0.13175495862960815, "compression_ratio": 1.6040609137055837, "no_speech_prob": 3.3405160593247274e-06}, {"id": 34, "seek": 20536, "start": 211.28, "end": 217.04000000000002, "text": " or speech, or the words in a piece of text, or the pixels in an image, you're almost certainly", "tokens": [420, 6218, 11, 420, 264, 2283, 294, 257, 2522, 295, 2487, 11, 420, 264, 18668, 294, 364, 3256, 11, 291, 434, 1920, 3297], "temperature": 0.0, "avg_logprob": -0.13175495862960815, "compression_ratio": 1.6040609137055837, "no_speech_prob": 3.3405160593247274e-06}, {"id": 35, "seek": 20536, "start": 217.04000000000002, "end": 222.72000000000003, "text": " going to want to try deep learning.", "tokens": [516, 281, 528, 281, 853, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.13175495862960815, "compression_ratio": 1.6040609137055837, "no_speech_prob": 3.3405160593247274e-06}, {"id": 36, "seek": 20536, "start": 222.72000000000003, "end": 230.32000000000002, "text": " And then outside of those two, there's a particular type of model we're going to look at today", "tokens": [400, 550, 2380, 295, 729, 732, 11, 456, 311, 257, 1729, 2010, 295, 2316, 321, 434, 516, 281, 574, 412, 965], "temperature": 0.0, "avg_logprob": -0.13175495862960815, "compression_ratio": 1.6040609137055837, "no_speech_prob": 3.3405160593247274e-06}, {"id": 37, "seek": 23032, "start": 230.32, "end": 236.32, "text": " called a collaborative filtering model, which it so happens that the groceries competition", "tokens": [1219, 257, 16555, 30822, 2316, 11, 597, 309, 370, 2314, 300, 264, 31391, 6211], "temperature": 0.0, "avg_logprob": -0.28191247853365814, "compression_ratio": 1.7511520737327189, "no_speech_prob": 4.469271152629517e-05}, {"id": 38, "seek": 23032, "start": 236.32, "end": 242.16, "text": " is of that kind, where neither of those approaches are quite what you want without some tweaks", "tokens": [307, 295, 300, 733, 11, 689, 9662, 295, 729, 11587, 366, 1596, 437, 291, 528, 1553, 512, 46664], "temperature": 0.0, "avg_logprob": -0.28191247853365814, "compression_ratio": 1.7511520737327189, "no_speech_prob": 4.469271152629517e-05}, {"id": 39, "seek": 23032, "start": 242.16, "end": 243.16, "text": " to them.", "tokens": [281, 552, 13], "temperature": 0.0, "avg_logprob": -0.28191247853365814, "compression_ratio": 1.7511520737327189, "no_speech_prob": 4.469271152629517e-05}, {"id": 40, "seek": 23032, "start": 243.16, "end": 247.76, "text": " Answer- You're saying neither, you're saying deep learning in random forest?", "tokens": [24545, 12, 509, 434, 1566, 9662, 11, 291, 434, 1566, 2452, 2539, 294, 4974, 6719, 30], "temperature": 0.0, "avg_logprob": -0.28191247853365814, "compression_ratio": 1.7511520737327189, "no_speech_prob": 4.469271152629517e-05}, {"id": 41, "seek": 23032, "start": 247.76, "end": 251.64, "text": " Answer- Neither deep learning or random forest is exactly what you want. You need to kind", "tokens": [24545, 12, 23956, 2452, 2539, 420, 4974, 6719, 307, 2293, 437, 291, 528, 13, 509, 643, 281, 733], "temperature": 0.0, "avg_logprob": -0.28191247853365814, "compression_ratio": 1.7511520737327189, "no_speech_prob": 4.469271152629517e-05}, {"id": 42, "seek": 23032, "start": 251.64, "end": 254.64, "text": " of do some tweaks.", "tokens": [295, 360, 512, 46664, 13], "temperature": 0.0, "avg_logprob": -0.28191247853365814, "compression_ratio": 1.7511520737327189, "no_speech_prob": 4.469271152629517e-05}, {"id": 43, "seek": 25464, "start": 254.64, "end": 261.03999999999996, "text": " If anybody thinks of other places where maybe neither of those techniques is the right thing", "tokens": [759, 4472, 7309, 295, 661, 3190, 689, 1310, 9662, 295, 729, 7512, 307, 264, 558, 551], "temperature": 0.0, "avg_logprob": -0.16133309519568154, "compression_ratio": 1.5818181818181818, "no_speech_prob": 3.96695804738556e-06}, {"id": 44, "seek": 25464, "start": 261.03999999999996, "end": 266.56, "text": " to use, mention it on the forums, even if you're not sure, so we can talk about it.", "tokens": [281, 764, 11, 2152, 309, 322, 264, 26998, 11, 754, 498, 291, 434, 406, 988, 11, 370, 321, 393, 751, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.16133309519568154, "compression_ratio": 1.5818181818181818, "no_speech_prob": 3.96695804738556e-06}, {"id": 45, "seek": 25464, "start": 266.56, "end": 273.28, "text": " I think this is one of the more interesting questions. And to some extent it is a case", "tokens": [286, 519, 341, 307, 472, 295, 264, 544, 1880, 1651, 13, 400, 281, 512, 8396, 309, 307, 257, 1389], "temperature": 0.0, "avg_logprob": -0.16133309519568154, "compression_ratio": 1.5818181818181818, "no_speech_prob": 3.96695804738556e-06}, {"id": 46, "seek": 25464, "start": 273.28, "end": 283.0, "text": " of practice and experience, but I do think there are two main classes to know about.", "tokens": [295, 3124, 293, 1752, 11, 457, 286, 360, 519, 456, 366, 732, 2135, 5359, 281, 458, 466, 13], "temperature": 0.0, "avg_logprob": -0.16133309519568154, "compression_ratio": 1.5818181818181818, "no_speech_prob": 3.96695804738556e-06}, {"id": 47, "seek": 28300, "start": 283.0, "end": 299.56, "text": " So last week, at the point where we had kind of done some of the key steps, like the CSV", "tokens": [407, 1036, 1243, 11, 412, 264, 935, 689, 321, 632, 733, 295, 1096, 512, 295, 264, 2141, 4439, 11, 411, 264, 48814], "temperature": 0.0, "avg_logprob": -0.2133844248453776, "compression_ratio": 1.5052631578947369, "no_speech_prob": 1.2606644304469228e-05}, {"id": 48, "seek": 28300, "start": 299.56, "end": 303.72, "text": " reading in particular, which took a minute or two, at the end of that we saved it to", "tokens": [3760, 294, 1729, 11, 597, 1890, 257, 3456, 420, 732, 11, 412, 264, 917, 295, 300, 321, 6624, 309, 281], "temperature": 0.0, "avg_logprob": -0.2133844248453776, "compression_ratio": 1.5052631578947369, "no_speech_prob": 1.2606644304469228e-05}, {"id": 49, "seek": 28300, "start": 303.72, "end": 306.04, "text": " a feather format file.", "tokens": [257, 25852, 7877, 3991, 13], "temperature": 0.0, "avg_logprob": -0.2133844248453776, "compression_ratio": 1.5052631578947369, "no_speech_prob": 1.2606644304469228e-05}, {"id": 50, "seek": 28300, "start": 306.04, "end": 311.04, "text": " Just to remind you, that's because this is basically almost the same format that it lives", "tokens": [1449, 281, 4160, 291, 11, 300, 311, 570, 341, 307, 1936, 1920, 264, 912, 7877, 300, 309, 2909], "temperature": 0.0, "avg_logprob": -0.2133844248453776, "compression_ratio": 1.5052631578947369, "no_speech_prob": 1.2606644304469228e-05}, {"id": 51, "seek": 31104, "start": 311.04, "end": 317.16, "text": " in in RAM, so it's ridiculously fast to read and write stuff from feather format. So what", "tokens": [294, 294, 14561, 11, 370, 309, 311, 41358, 2370, 281, 1401, 293, 2464, 1507, 490, 25852, 7877, 13, 407, 437], "temperature": 0.0, "avg_logprob": -0.1718220129245665, "compression_ratio": 1.6435643564356435, "no_speech_prob": 3.5559580737754004e-06}, {"id": 52, "seek": 31104, "start": 317.16, "end": 323.52000000000004, "text": " we're going to do today is we're going to look at lesson 2 RF interpretation, and the", "tokens": [321, 434, 516, 281, 360, 965, 307, 321, 434, 516, 281, 574, 412, 6898, 568, 26204, 14174, 11, 293, 264], "temperature": 0.0, "avg_logprob": -0.1718220129245665, "compression_ratio": 1.6435643564356435, "no_speech_prob": 3.5559580737754004e-06}, {"id": 53, "seek": 31104, "start": 323.52000000000004, "end": 329.20000000000005, "text": " first thing we're going to do is read that feather format file.", "tokens": [700, 551, 321, 434, 516, 281, 360, 307, 1401, 300, 25852, 7877, 3991, 13], "temperature": 0.0, "avg_logprob": -0.1718220129245665, "compression_ratio": 1.6435643564356435, "no_speech_prob": 3.5559580737754004e-06}, {"id": 54, "seek": 31104, "start": 329.20000000000005, "end": 336.86, "text": " Now one thing to mention is a couple of you pointed out during the week a really interesting", "tokens": [823, 472, 551, 281, 2152, 307, 257, 1916, 295, 291, 10932, 484, 1830, 264, 1243, 257, 534, 1880], "temperature": 0.0, "avg_logprob": -0.1718220129245665, "compression_ratio": 1.6435643564356435, "no_speech_prob": 3.5559580737754004e-06}, {"id": 55, "seek": 33686, "start": 336.86, "end": 349.92, "text": " little bug or little issue, which is in the PROC DF function. The PROC DF function, remember,", "tokens": [707, 7426, 420, 707, 2734, 11, 597, 307, 294, 264, 15008, 34, 48336, 2445, 13, 440, 15008, 34, 48336, 2445, 11, 1604, 11], "temperature": 0.0, "avg_logprob": -0.16650025294377255, "compression_ratio": 1.547486033519553, "no_speech_prob": 2.840873275999911e-05}, {"id": 56, "seek": 33686, "start": 349.92, "end": 354.88, "text": " finds the numeric columns which have missing values and creates an additional Boolean column", "tokens": [10704, 264, 7866, 299, 13766, 597, 362, 5361, 4190, 293, 7829, 364, 4497, 23351, 28499, 7738], "temperature": 0.0, "avg_logprob": -0.16650025294377255, "compression_ratio": 1.547486033519553, "no_speech_prob": 2.840873275999911e-05}, {"id": 57, "seek": 33686, "start": 354.88, "end": 366.32, "text": " as well as replacing the missing with medians, and also turns the categorical objects into", "tokens": [382, 731, 382, 19139, 264, 5361, 365, 1205, 2567, 11, 293, 611, 4523, 264, 19250, 804, 6565, 666], "temperature": 0.0, "avg_logprob": -0.16650025294377255, "compression_ratio": 1.547486033519553, "no_speech_prob": 2.840873275999911e-05}, {"id": 58, "seek": 36632, "start": 366.32, "end": 367.32, "text": " the integer codes.", "tokens": [264, 24922, 14211, 13], "temperature": 0.0, "avg_logprob": -0.17146857579549155, "compression_ratio": 1.665158371040724, "no_speech_prob": 3.555961484380532e-06}, {"id": 59, "seek": 36632, "start": 367.32, "end": 375.32, "text": " A couple of you pointed out some key points about the missing value handling. The first", "tokens": [316, 1916, 295, 291, 10932, 484, 512, 2141, 2793, 466, 264, 5361, 2158, 13175, 13, 440, 700], "temperature": 0.0, "avg_logprob": -0.17146857579549155, "compression_ratio": 1.665158371040724, "no_speech_prob": 3.555961484380532e-06}, {"id": 60, "seek": 36632, "start": 375.32, "end": 384.52, "text": " one is that your test set may have missing values in some columns that weren't in your", "tokens": [472, 307, 300, 428, 1500, 992, 815, 362, 5361, 4190, 294, 512, 13766, 300, 4999, 380, 294, 428], "temperature": 0.0, "avg_logprob": -0.17146857579549155, "compression_ratio": 1.665158371040724, "no_speech_prob": 3.555961484380532e-06}, {"id": 61, "seek": 36632, "start": 384.52, "end": 389.03999999999996, "text": " training set or vice versa. If that happens, you're going to get an error when you try", "tokens": [3097, 992, 420, 11964, 25650, 13, 759, 300, 2314, 11, 291, 434, 516, 281, 483, 364, 6713, 562, 291, 853], "temperature": 0.0, "avg_logprob": -0.17146857579549155, "compression_ratio": 1.665158371040724, "no_speech_prob": 3.555961484380532e-06}, {"id": 62, "seek": 36632, "start": 389.03999999999996, "end": 394.9, "text": " to do the random forest, because it's going to say if that is missing field appeared in", "tokens": [281, 360, 264, 4974, 6719, 11, 570, 309, 311, 516, 281, 584, 498, 300, 307, 5361, 2519, 8516, 294], "temperature": 0.0, "avg_logprob": -0.17146857579549155, "compression_ratio": 1.665158371040724, "no_speech_prob": 3.555961484380532e-06}, {"id": 63, "seek": 39490, "start": 394.9, "end": 399.28, "text": " your training set but not in your test set, it ended up in the model. It's going to say", "tokens": [428, 3097, 992, 457, 406, 294, 428, 1500, 992, 11, 309, 4590, 493, 294, 264, 2316, 13, 467, 311, 516, 281, 584], "temperature": 0.0, "avg_logprob": -0.1898211469553938, "compression_ratio": 1.748917748917749, "no_speech_prob": 2.190774694099673e-06}, {"id": 64, "seek": 39490, "start": 399.28, "end": 405.15999999999997, "text": " you can't use that data set with this model because you're missing one of the columns", "tokens": [291, 393, 380, 764, 300, 1412, 992, 365, 341, 2316, 570, 291, 434, 5361, 472, 295, 264, 13766], "temperature": 0.0, "avg_logprob": -0.1898211469553938, "compression_ratio": 1.748917748917749, "no_speech_prob": 2.190774694099673e-06}, {"id": 65, "seek": 39490, "start": 405.15999999999997, "end": 409.2, "text": " it requires. That's problem number one.", "tokens": [309, 7029, 13, 663, 311, 1154, 1230, 472, 13], "temperature": 0.0, "avg_logprob": -0.1898211469553938, "compression_ratio": 1.748917748917749, "no_speech_prob": 2.190774694099673e-06}, {"id": 66, "seek": 39490, "start": 409.2, "end": 417.79999999999995, "text": " Problem number two is that the median of the numeric values in the test set may be different", "tokens": [11676, 1230, 732, 307, 300, 264, 26779, 295, 264, 7866, 299, 4190, 294, 264, 1500, 992, 815, 312, 819], "temperature": 0.0, "avg_logprob": -0.1898211469553938, "compression_ratio": 1.748917748917749, "no_speech_prob": 2.190774694099673e-06}, {"id": 67, "seek": 39490, "start": 417.79999999999995, "end": 424.56, "text": " for the training set, so it may actually process it into something which has different semantics.", "tokens": [337, 264, 3097, 992, 11, 370, 309, 815, 767, 1399, 309, 666, 746, 597, 575, 819, 4361, 45298, 13], "temperature": 0.0, "avg_logprob": -0.1898211469553938, "compression_ratio": 1.748917748917749, "no_speech_prob": 2.190774694099673e-06}, {"id": 68, "seek": 42456, "start": 424.56, "end": 434.12, "text": " I thought that was a really interesting point. What I did was I changed prop.df so it returns", "tokens": [286, 1194, 300, 390, 257, 534, 1880, 935, 13, 708, 286, 630, 390, 286, 3105, 2365, 13, 45953, 370, 309, 11247], "temperature": 0.0, "avg_logprob": -0.2177155143336246, "compression_ratio": 1.5, "no_speech_prob": 3.3405233352823416e-06}, {"id": 69, "seek": 42456, "start": 434.12, "end": 442.6, "text": " a third thing, NAs. The NAs thing it returns, it doesn't matter in detail what it is, but", "tokens": [257, 2636, 551, 11, 426, 10884, 13, 440, 426, 10884, 551, 309, 11247, 11, 309, 1177, 380, 1871, 294, 2607, 437, 309, 307, 11, 457], "temperature": 0.0, "avg_logprob": -0.2177155143336246, "compression_ratio": 1.5, "no_speech_prob": 3.3405233352823416e-06}, {"id": 70, "seek": 42456, "start": 442.6, "end": 449.12, "text": " I'll tell you just so you know, that's a dictionary where the keys are the names of the columns", "tokens": [286, 603, 980, 291, 445, 370, 291, 458, 11, 300, 311, 257, 25890, 689, 264, 9317, 366, 264, 5288, 295, 264, 13766], "temperature": 0.0, "avg_logprob": -0.2177155143336246, "compression_ratio": 1.5, "no_speech_prob": 3.3405233352823416e-06}, {"id": 71, "seek": 44912, "start": 449.12, "end": 456.28000000000003, "text": " that had missing values, and the values of the dictionary are the medians. So then optionally", "tokens": [300, 632, 5361, 4190, 11, 293, 264, 4190, 295, 264, 25890, 366, 264, 1205, 2567, 13, 407, 550, 3614, 379], "temperature": 0.0, "avg_logprob": -0.1360368175783019, "compression_ratio": 1.621301775147929, "no_speech_prob": 3.905468474840745e-06}, {"id": 72, "seek": 44912, "start": 456.28000000000003, "end": 465.08, "text": " you can pass NAs as an additional argument to prop.df, and it'll make sure that it adds", "tokens": [291, 393, 1320, 426, 10884, 382, 364, 4497, 6770, 281, 2365, 13, 45953, 11, 293, 309, 603, 652, 988, 300, 309, 10860], "temperature": 0.0, "avg_logprob": -0.1360368175783019, "compression_ratio": 1.621301775147929, "no_speech_prob": 3.905468474840745e-06}, {"id": 73, "seek": 44912, "start": 465.08, "end": 472.84000000000003, "text": " those specific columns and it uses those specific medians. So it's giving you the ability to", "tokens": [729, 2685, 13766, 293, 309, 4960, 729, 2685, 1205, 2567, 13, 407, 309, 311, 2902, 291, 264, 3485, 281], "temperature": 0.0, "avg_logprob": -0.1360368175783019, "compression_ratio": 1.621301775147929, "no_speech_prob": 3.905468474840745e-06}, {"id": 74, "seek": 47284, "start": 472.84, "end": 479.84, "text": " say process this test set in exactly the same way as we process this training set.", "tokens": [584, 1399, 341, 1500, 992, 294, 2293, 264, 912, 636, 382, 321, 1399, 341, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.381178092956543, "compression_ratio": 1.434782608695652, "no_speech_prob": 4.469197301659733e-05}, {"id": 75, "seek": 47284, "start": 479.84, "end": 484.88, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.381178092956543, "compression_ratio": 1.434782608695652, "no_speech_prob": 4.469197301659733e-05}, {"id": 76, "seek": 47284, "start": 484.88, "end": 499.88, "text": " Before you start doing work any day, I would start doing a git pull. If something's not", "tokens": [4546, 291, 722, 884, 589, 604, 786, 11, 286, 576, 722, 884, 257, 18331, 2235, 13, 759, 746, 311, 406], "temperature": 0.0, "avg_logprob": -0.381178092956543, "compression_ratio": 1.434782608695652, "no_speech_prob": 4.469197301659733e-05}, {"id": 77, "seek": 49988, "start": 499.88, "end": 504.08, "text": " working today that was working yesterday, check the forum where there will be an explanation", "tokens": [1364, 965, 300, 390, 1364, 5186, 11, 1520, 264, 17542, 689, 456, 486, 312, 364, 10835], "temperature": 0.0, "avg_logprob": -0.1729690983610333, "compression_ratio": 1.7011494252873562, "no_speech_prob": 4.3998428736813366e-05}, {"id": 78, "seek": 49988, "start": 504.08, "end": 511.4, "text": " of why. This library in particular is moving fast, but pretty much all the libraries that", "tokens": [295, 983, 13, 639, 6405, 294, 1729, 307, 2684, 2370, 11, 457, 1238, 709, 439, 264, 15148, 300], "temperature": 0.0, "avg_logprob": -0.1729690983610333, "compression_ratio": 1.7011494252873562, "no_speech_prob": 4.3998428736813366e-05}, {"id": 79, "seek": 49988, "start": 511.4, "end": 516.8, "text": " we use, including PyTorch in particular, move fast. So one of the things to do if you're", "tokens": [321, 764, 11, 3009, 9953, 51, 284, 339, 294, 1729, 11, 1286, 2370, 13, 407, 472, 295, 264, 721, 281, 360, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.1729690983610333, "compression_ratio": 1.7011494252873562, "no_speech_prob": 4.3998428736813366e-05}, {"id": 80, "seek": 49988, "start": 516.8, "end": 522.84, "text": " watching this through the MOOC is to make sure that you go to course.fast.ai and check", "tokens": [1976, 341, 807, 264, 49197, 34, 307, 281, 652, 988, 300, 291, 352, 281, 1164, 13, 7011, 13, 1301, 293, 1520], "temperature": 0.0, "avg_logprob": -0.1729690983610333, "compression_ratio": 1.7011494252873562, "no_speech_prob": 4.3998428736813366e-05}, {"id": 81, "seek": 49988, "start": 522.84, "end": 527.68, "text": " the links there because there will be links saying these are the differences from the", "tokens": [264, 6123, 456, 570, 456, 486, 312, 6123, 1566, 613, 366, 264, 7300, 490, 264], "temperature": 0.0, "avg_logprob": -0.1729690983610333, "compression_ratio": 1.7011494252873562, "no_speech_prob": 4.3998428736813366e-05}, {"id": 82, "seek": 52768, "start": 527.68, "end": 537.0, "text": " course. So we're kept up to date because I can't edit what I'm saying. But do a git pull", "tokens": [1164, 13, 407, 321, 434, 4305, 493, 281, 4002, 570, 286, 393, 380, 8129, 437, 286, 478, 1566, 13, 583, 360, 257, 18331, 2235], "temperature": 0.0, "avg_logprob": -0.2182659337550034, "compression_ratio": 1.5126903553299493, "no_speech_prob": 8.66457776282914e-06}, {"id": 83, "seek": 52768, "start": 537.0, "end": 543.3199999999999, "text": " before you start each day.", "tokens": [949, 291, 722, 1184, 786, 13], "temperature": 0.0, "avg_logprob": -0.2182659337550034, "compression_ratio": 1.5126903553299493, "no_speech_prob": 8.66457776282914e-06}, {"id": 84, "seek": 52768, "start": 543.3199999999999, "end": 551.12, "text": " So I haven't actually updated all of the notebooks to add the extra return value. I will over", "tokens": [407, 286, 2378, 380, 767, 10588, 439, 295, 264, 43782, 281, 909, 264, 2857, 2736, 2158, 13, 286, 486, 670], "temperature": 0.0, "avg_logprob": -0.2182659337550034, "compression_ratio": 1.5126903553299493, "no_speech_prob": 8.66457776282914e-06}, {"id": 85, "seek": 52768, "start": 551.12, "end": 554.24, "text": " the next couple of days, but if you're using them you'll just need to put an extra comma", "tokens": [264, 958, 1916, 295, 1708, 11, 457, 498, 291, 434, 1228, 552, 291, 603, 445, 643, 281, 829, 364, 2857, 22117], "temperature": 0.0, "avg_logprob": -0.2182659337550034, "compression_ratio": 1.5126903553299493, "no_speech_prob": 8.66457776282914e-06}, {"id": 86, "seek": 55424, "start": 554.24, "end": 559.24, "text": " and a's here. Otherwise you'll get an error that it's returned 3 things and you only have", "tokens": [293, 257, 311, 510, 13, 10328, 291, 603, 483, 364, 6713, 300, 309, 311, 8752, 805, 721, 293, 291, 787, 362], "temperature": 0.0, "avg_logprob": -0.2111761220296224, "compression_ratio": 1.5051020408163265, "no_speech_prob": 7.071826985338703e-06}, {"id": 87, "seek": 55424, "start": 559.24, "end": 566.24, "text": " room for 3 things.", "tokens": [1808, 337, 805, 721, 13], "temperature": 0.0, "avg_logprob": -0.2111761220296224, "compression_ratio": 1.5051020408163265, "no_speech_prob": 7.071826985338703e-06}, {"id": 88, "seek": 55424, "start": 566.24, "end": 573.64, "text": " What I want to do before I talk about interpretation is to show you what the exact same process", "tokens": [708, 286, 528, 281, 360, 949, 286, 751, 466, 14174, 307, 281, 855, 291, 437, 264, 1900, 912, 1399], "temperature": 0.0, "avg_logprob": -0.2111761220296224, "compression_ratio": 1.5051020408163265, "no_speech_prob": 7.071826985338703e-06}, {"id": 89, "seek": 55424, "start": 573.64, "end": 584.16, "text": " looks like when you're working with a really large dataset. You'll see it's kind of almost", "tokens": [1542, 411, 562, 291, 434, 1364, 365, 257, 534, 2416, 28872, 13, 509, 603, 536, 309, 311, 733, 295, 1920], "temperature": 0.0, "avg_logprob": -0.2111761220296224, "compression_ratio": 1.5051020408163265, "no_speech_prob": 7.071826985338703e-06}, {"id": 90, "seek": 58416, "start": 584.16, "end": 588.28, "text": " the same thing, but there's going to be a few cases where we can't use the defaults", "tokens": [264, 912, 551, 11, 457, 456, 311, 516, 281, 312, 257, 1326, 3331, 689, 321, 393, 380, 764, 264, 7576, 82], "temperature": 0.0, "avg_logprob": -0.20479509907384072, "compression_ratio": 1.5095541401273886, "no_speech_prob": 1.428532323188847e-05}, {"id": 91, "seek": 58416, "start": 588.28, "end": 593.8, "text": " because the defaults kind of just run a little bit too slowly.", "tokens": [570, 264, 7576, 82, 733, 295, 445, 1190, 257, 707, 857, 886, 5692, 13], "temperature": 0.0, "avg_logprob": -0.20479509907384072, "compression_ratio": 1.5095541401273886, "no_speech_prob": 1.428532323188847e-05}, {"id": 92, "seek": 58416, "start": 593.8, "end": 613.9599999999999, "text": " So specifically I'm going to look at the Cabell Groceries Competition. So this competition", "tokens": [407, 4682, 286, 478, 516, 281, 574, 412, 264, 14704, 898, 12981, 1776, 530, 43634, 13, 407, 341, 6211], "temperature": 0.0, "avg_logprob": -0.20479509907384072, "compression_ratio": 1.5095541401273886, "no_speech_prob": 1.428532323188847e-05}, {"id": 93, "seek": 61396, "start": 613.96, "end": 627.0400000000001, "text": "... who is entering this competition? Who would like to have a go at explaining what", "tokens": [1097, 567, 307, 11104, 341, 6211, 30, 2102, 576, 411, 281, 362, 257, 352, 412, 13468, 437], "temperature": 0.0, "avg_logprob": -0.2828755872002963, "compression_ratio": 1.5822784810126582, "no_speech_prob": 2.5867151634884067e-05}, {"id": 94, "seek": 61396, "start": 627.0400000000001, "end": 635.12, "text": " this competition involves, what the data is and what you're trying to predict?", "tokens": [341, 6211, 11626, 11, 437, 264, 1412, 307, 293, 437, 291, 434, 1382, 281, 6069, 30], "temperature": 0.0, "avg_logprob": -0.2828755872002963, "compression_ratio": 1.5822784810126582, "no_speech_prob": 2.5867151634884067e-05}, {"id": 95, "seek": 61396, "start": 635.12, "end": 641.36, "text": " Trying to predict the items on the shelf depending on lots of factors like oil prices.", "tokens": [20180, 281, 6069, 264, 4754, 322, 264, 15222, 5413, 322, 3195, 295, 6771, 411, 3184, 7901, 13], "temperature": 0.0, "avg_logprob": -0.2828755872002963, "compression_ratio": 1.5822784810126582, "no_speech_prob": 2.5867151634884067e-05}, {"id": 96, "seek": 64136, "start": 641.36, "end": 646.8000000000001, "text": " What do you say, predicting the items on the shelf? What are you actually predicting?", "tokens": [708, 360, 291, 584, 11, 32884, 264, 4754, 322, 264, 15222, 30, 708, 366, 291, 767, 32884, 30], "temperature": 0.0, "avg_logprob": -0.2290559248490767, "compression_ratio": 1.6599190283400809, "no_speech_prob": 5.390942169469781e-05}, {"id": 97, "seek": 64136, "start": 646.8000000000001, "end": 650.5600000000001, "text": " How much change to have in stock to maximize their...", "tokens": [1012, 709, 1319, 281, 362, 294, 4127, 281, 19874, 641, 1097], "temperature": 0.0, "avg_logprob": -0.2290559248490767, "compression_ratio": 1.6599190283400809, "no_speech_prob": 5.390942169469781e-05}, {"id": 98, "seek": 64136, "start": 650.5600000000001, "end": 654.16, "text": " It's not quite what we're predicting, but we'll try and fix that in a moment, so go", "tokens": [467, 311, 406, 1596, 437, 321, 434, 32884, 11, 457, 321, 603, 853, 293, 3191, 300, 294, 257, 1623, 11, 370, 352], "temperature": 0.0, "avg_logprob": -0.2290559248490767, "compression_ratio": 1.6599190283400809, "no_speech_prob": 5.390942169469781e-05}, {"id": 99, "seek": 64136, "start": 654.16, "end": 655.16, "text": " on.", "tokens": [322, 13], "temperature": 0.0, "avg_logprob": -0.2290559248490767, "compression_ratio": 1.6599190283400809, "no_speech_prob": 5.390942169469781e-05}, {"id": 100, "seek": 64136, "start": 655.16, "end": 658.64, "text": " And then there's a bunch of different datasets that you can use to do that. There's oil prices,", "tokens": [400, 550, 456, 311, 257, 3840, 295, 819, 42856, 300, 291, 393, 764, 281, 360, 300, 13, 821, 311, 3184, 7901, 11], "temperature": 0.0, "avg_logprob": -0.2290559248490767, "compression_ratio": 1.6599190283400809, "no_speech_prob": 5.390942169469781e-05}, {"id": 101, "seek": 64136, "start": 658.64, "end": 664.8000000000001, "text": " there's stores, there's locations, and each of those can be used to try to predict it.", "tokens": [456, 311, 9512, 11, 456, 311, 9253, 11, 293, 1184, 295, 729, 393, 312, 1143, 281, 853, 281, 6069, 309, 13], "temperature": 0.0, "avg_logprob": -0.2290559248490767, "compression_ratio": 1.6599190283400809, "no_speech_prob": 5.390942169469781e-05}, {"id": 102, "seek": 66480, "start": 664.8, "end": 674.68, "text": " Does anybody want to have a go at expanding on that?", "tokens": [4402, 4472, 528, 281, 362, 257, 352, 412, 14702, 322, 300, 30], "temperature": 0.0, "avg_logprob": -0.21003698047838712, "compression_ratio": 1.2608695652173914, "no_speech_prob": 3.3403525776520837e-06}, {"id": 103, "seek": 66480, "start": 674.68, "end": 687.0, "text": " So we have a bunch of information on different products. So for every store, for every item,", "tokens": [407, 321, 362, 257, 3840, 295, 1589, 322, 819, 3383, 13, 407, 337, 633, 3531, 11, 337, 633, 3174, 11], "temperature": 0.0, "avg_logprob": -0.21003698047838712, "compression_ratio": 1.2608695652173914, "no_speech_prob": 3.3403525776520837e-06}, {"id": 104, "seek": 68700, "start": 687.0, "end": 695.64, "text": " for every day, we have a lot of related information available, like the location where the store", "tokens": [337, 633, 786, 11, 321, 362, 257, 688, 295, 4077, 1589, 2435, 11, 411, 264, 4914, 689, 264, 3531], "temperature": 0.0, "avg_logprob": -0.17028259701199003, "compression_ratio": 1.6133333333333333, "no_speech_prob": 1.2028481251036283e-05}, {"id": 105, "seek": 68700, "start": 695.64, "end": 702.64, "text": " was located, the class of the product, and the units sold. And then based on this, we", "tokens": [390, 6870, 11, 264, 1508, 295, 264, 1674, 11, 293, 264, 6815, 3718, 13, 400, 550, 2361, 322, 341, 11, 321], "temperature": 0.0, "avg_logprob": -0.17028259701199003, "compression_ratio": 1.6133333333333333, "no_speech_prob": 1.2028481251036283e-05}, {"id": 106, "seek": 68700, "start": 702.64, "end": 708.2, "text": " are supposed to forecast in a much shorter timeframe compared to the training data. For", "tokens": [366, 3442, 281, 14330, 294, 257, 709, 11639, 34830, 5347, 281, 264, 3097, 1412, 13, 1171], "temperature": 0.0, "avg_logprob": -0.17028259701199003, "compression_ratio": 1.6133333333333333, "no_speech_prob": 1.2028481251036283e-05}, {"id": 107, "seek": 68700, "start": 708.2, "end": 713.08, "text": " every item number, how much we think it's going to sell. So only the units and nothing", "tokens": [633, 3174, 1230, 11, 577, 709, 321, 519, 309, 311, 516, 281, 3607, 13, 407, 787, 264, 6815, 293, 1825], "temperature": 0.0, "avg_logprob": -0.17028259701199003, "compression_ratio": 1.6133333333333333, "no_speech_prob": 1.2028481251036283e-05}, {"id": 108, "seek": 68700, "start": 713.08, "end": 714.08, "text": " else.", "tokens": [1646, 13], "temperature": 0.0, "avg_logprob": -0.17028259701199003, "compression_ratio": 1.6133333333333333, "no_speech_prob": 1.2028481251036283e-05}, {"id": 109, "seek": 71408, "start": 714.08, "end": 731.08, "text": " So your ability to explain the problem you're working on is really, really important. So", "tokens": [407, 428, 3485, 281, 2903, 264, 1154, 291, 434, 1364, 322, 307, 534, 11, 534, 1021, 13, 407], "temperature": 0.0, "avg_logprob": -0.18468689337009336, "compression_ratio": 1.3968253968253967, "no_speech_prob": 2.225252728749183e-06}, {"id": 110, "seek": 71408, "start": 731.08, "end": 736.62, "text": " if you don't currently feel confident of your ability to do that, practice with someone", "tokens": [498, 291, 500, 380, 4362, 841, 6679, 295, 428, 3485, 281, 360, 300, 11, 3124, 365, 1580], "temperature": 0.0, "avg_logprob": -0.18468689337009336, "compression_ratio": 1.3968253968253967, "no_speech_prob": 2.225252728749183e-06}, {"id": 111, "seek": 73662, "start": 736.62, "end": 745.64, "text": " who is not in this competition, tell them all about it. So in this case, or in any case", "tokens": [567, 307, 406, 294, 341, 6211, 11, 980, 552, 439, 466, 309, 13, 407, 294, 341, 1389, 11, 420, 294, 604, 1389], "temperature": 0.0, "avg_logprob": -0.15203645057284954, "compression_ratio": 1.8986175115207373, "no_speech_prob": 5.1738602451223414e-06}, {"id": 112, "seek": 73662, "start": 745.64, "end": 749.8, "text": " really, the key things to understand a machine learning problem would be to say what are", "tokens": [534, 11, 264, 2141, 721, 281, 1223, 257, 3479, 2539, 1154, 576, 312, 281, 584, 437, 366], "temperature": 0.0, "avg_logprob": -0.15203645057284954, "compression_ratio": 1.8986175115207373, "no_speech_prob": 5.1738602451223414e-06}, {"id": 113, "seek": 73662, "start": 749.8, "end": 752.52, "text": " the independent variables and what is the dependent variable.", "tokens": [264, 6695, 9102, 293, 437, 307, 264, 12334, 7006, 13], "temperature": 0.0, "avg_logprob": -0.15203645057284954, "compression_ratio": 1.8986175115207373, "no_speech_prob": 5.1738602451223414e-06}, {"id": 114, "seek": 73662, "start": 752.52, "end": 756.2, "text": " So the dependent variable is the thing that you're trying to predict. The thing you're", "tokens": [407, 264, 12334, 7006, 307, 264, 551, 300, 291, 434, 1382, 281, 6069, 13, 440, 551, 291, 434], "temperature": 0.0, "avg_logprob": -0.15203645057284954, "compression_ratio": 1.8986175115207373, "no_speech_prob": 5.1738602451223414e-06}, {"id": 115, "seek": 73662, "start": 756.2, "end": 764.6800000000001, "text": " trying to predict is how many units of each kind of product were sold in each store on", "tokens": [1382, 281, 6069, 307, 577, 867, 6815, 295, 1184, 733, 295, 1674, 645, 3718, 294, 1184, 3531, 322], "temperature": 0.0, "avg_logprob": -0.15203645057284954, "compression_ratio": 1.8986175115207373, "no_speech_prob": 5.1738602451223414e-06}, {"id": 116, "seek": 76468, "start": 764.68, "end": 770.56, "text": " each day during a two-week period. So that's the thing that you're trying to predict. And", "tokens": [1184, 786, 1830, 257, 732, 12, 23188, 2896, 13, 407, 300, 311, 264, 551, 300, 291, 434, 1382, 281, 6069, 13, 400], "temperature": 0.0, "avg_logprob": -0.14997821979308396, "compression_ratio": 1.6492890995260663, "no_speech_prob": 9.666048754297663e-06}, {"id": 117, "seek": 76468, "start": 770.56, "end": 777.92, "text": " the information you have to predict is how many units of each product at each store on", "tokens": [264, 1589, 291, 362, 281, 6069, 307, 577, 867, 6815, 295, 1184, 1674, 412, 1184, 3531, 322], "temperature": 0.0, "avg_logprob": -0.14997821979308396, "compression_ratio": 1.6492890995260663, "no_speech_prob": 9.666048754297663e-06}, {"id": 118, "seek": 76468, "start": 777.92, "end": 786.4399999999999, "text": " each day were sold in the last few years, and for each store, some metadata about it,", "tokens": [1184, 786, 645, 3718, 294, 264, 1036, 1326, 924, 11, 293, 337, 1184, 3531, 11, 512, 26603, 466, 309, 11], "temperature": 0.0, "avg_logprob": -0.14997821979308396, "compression_ratio": 1.6492890995260663, "no_speech_prob": 9.666048754297663e-06}, {"id": 119, "seek": 76468, "start": 786.4399999999999, "end": 792.76, "text": " like where is it located and what class of store is it. For each type of product, you", "tokens": [411, 689, 307, 309, 6870, 293, 437, 1508, 295, 3531, 307, 309, 13, 1171, 1184, 2010, 295, 1674, 11, 291], "temperature": 0.0, "avg_logprob": -0.14997821979308396, "compression_ratio": 1.6492890995260663, "no_speech_prob": 9.666048754297663e-06}, {"id": 120, "seek": 79276, "start": 792.76, "end": 797.72, "text": " have some metadata about it, such as what category of product is it and so forth. For", "tokens": [362, 512, 26603, 466, 309, 11, 1270, 382, 437, 7719, 295, 1674, 307, 309, 293, 370, 5220, 13, 1171], "temperature": 0.0, "avg_logprob": -0.12022111022356645, "compression_ratio": 1.9241071428571428, "no_speech_prob": 8.800979230727535e-06}, {"id": 121, "seek": 79276, "start": 797.72, "end": 804.56, "text": " each date, we have some metadata about it, such as what was the oil price on that date.", "tokens": [1184, 4002, 11, 321, 362, 512, 26603, 466, 309, 11, 1270, 382, 437, 390, 264, 3184, 3218, 322, 300, 4002, 13], "temperature": 0.0, "avg_logprob": -0.12022111022356645, "compression_ratio": 1.9241071428571428, "no_speech_prob": 8.800979230727535e-06}, {"id": 122, "seek": 79276, "start": 804.56, "end": 809.38, "text": " So this is what we would call a relational dataset. So a relational dataset is one where", "tokens": [407, 341, 307, 437, 321, 576, 818, 257, 38444, 28872, 13, 407, 257, 38444, 28872, 307, 472, 689], "temperature": 0.0, "avg_logprob": -0.12022111022356645, "compression_ratio": 1.9241071428571428, "no_speech_prob": 8.800979230727535e-06}, {"id": 123, "seek": 79276, "start": 809.38, "end": 815.2, "text": " we have a number of different pieces of information that we can join together.", "tokens": [321, 362, 257, 1230, 295, 819, 3755, 295, 1589, 300, 321, 393, 3917, 1214, 13], "temperature": 0.0, "avg_logprob": -0.12022111022356645, "compression_ratio": 1.9241071428571428, "no_speech_prob": 8.800979230727535e-06}, {"id": 124, "seek": 79276, "start": 815.2, "end": 821.96, "text": " Specifically, this kind of relational dataset is what we would refer to as a star schema.", "tokens": [26058, 11, 341, 733, 295, 38444, 28872, 307, 437, 321, 576, 2864, 281, 382, 257, 3543, 34078, 13], "temperature": 0.0, "avg_logprob": -0.12022111022356645, "compression_ratio": 1.9241071428571428, "no_speech_prob": 8.800979230727535e-06}, {"id": 125, "seek": 82196, "start": 821.96, "end": 826.8000000000001, "text": " A star schema is a kind of data warehousing schema where we basically say there's some", "tokens": [316, 3543, 34078, 307, 257, 733, 295, 1412, 17464, 71, 24220, 34078, 689, 321, 1936, 584, 456, 311, 512], "temperature": 0.0, "avg_logprob": -0.14997956290173886, "compression_ratio": 1.572289156626506, "no_speech_prob": 9.972839507099707e-06}, {"id": 126, "seek": 82196, "start": 826.8000000000001, "end": 838.88, "text": " central transactions table. In this case, the central transactions table is train.csv,", "tokens": [5777, 16856, 3199, 13, 682, 341, 1389, 11, 264, 5777, 16856, 3199, 307, 3847, 13, 14368, 85, 11], "temperature": 0.0, "avg_logprob": -0.14997956290173886, "compression_ratio": 1.572289156626506, "no_speech_prob": 9.972839507099707e-06}, {"id": 127, "seek": 82196, "start": 838.88, "end": 849.84, "text": " and it contains the number of units that were sold by date, by store ID, by item ID. So", "tokens": [293, 309, 8306, 264, 1230, 295, 6815, 300, 645, 3718, 538, 4002, 11, 538, 3531, 7348, 11, 538, 3174, 7348, 13, 407], "temperature": 0.0, "avg_logprob": -0.14997956290173886, "compression_ratio": 1.572289156626506, "no_speech_prob": 9.972839507099707e-06}, {"id": 128, "seek": 84984, "start": 849.84, "end": 853.5600000000001, "text": " that's the central transactions table, very small, very simple. And then from that we", "tokens": [300, 311, 264, 5777, 16856, 3199, 11, 588, 1359, 11, 588, 2199, 13, 400, 550, 490, 300, 321], "temperature": 0.0, "avg_logprob": -0.16913338462905128, "compression_ratio": 1.7065637065637065, "no_speech_prob": 2.769394768620259e-06}, {"id": 129, "seek": 84984, "start": 853.5600000000001, "end": 859.32, "text": " can join various bits of metadata. And it's called a star schema because you can kind", "tokens": [393, 3917, 3683, 9239, 295, 26603, 13, 400, 309, 311, 1219, 257, 3543, 34078, 570, 291, 393, 733], "temperature": 0.0, "avg_logprob": -0.16913338462905128, "compression_ratio": 1.7065637065637065, "no_speech_prob": 2.769394768620259e-06}, {"id": 130, "seek": 84984, "start": 859.32, "end": 864.64, "text": " of imagine the transactions table in the middle and then all these different metadata tables", "tokens": [295, 3811, 264, 16856, 3199, 294, 264, 2808, 293, 550, 439, 613, 819, 26603, 8020], "temperature": 0.0, "avg_logprob": -0.16913338462905128, "compression_ratio": 1.7065637065637065, "no_speech_prob": 2.769394768620259e-06}, {"id": 131, "seek": 84984, "start": 864.64, "end": 871.52, "text": " join onto it, giving you more information about the date, the item ID, and the store", "tokens": [3917, 3911, 309, 11, 2902, 291, 544, 1589, 466, 264, 4002, 11, 264, 3174, 7348, 11, 293, 264, 3531], "temperature": 0.0, "avg_logprob": -0.16913338462905128, "compression_ratio": 1.7065637065637065, "no_speech_prob": 2.769394768620259e-06}, {"id": 132, "seek": 84984, "start": 871.52, "end": 878.96, "text": " ID. Sometimes you'll also see a snowflake schema, which means there might then be additional", "tokens": [7348, 13, 4803, 291, 603, 611, 536, 257, 44124, 619, 34078, 11, 597, 1355, 456, 1062, 550, 312, 4497], "temperature": 0.0, "avg_logprob": -0.16913338462905128, "compression_ratio": 1.7065637065637065, "no_speech_prob": 2.769394768620259e-06}, {"id": 133, "seek": 87896, "start": 878.96, "end": 886.88, "text": " information joined onto maybe the items table that tells you about different item categories,", "tokens": [1589, 6869, 3911, 1310, 264, 4754, 3199, 300, 5112, 291, 466, 819, 3174, 10479, 11], "temperature": 0.0, "avg_logprob": -0.2267198965583049, "compression_ratio": 1.7403314917127073, "no_speech_prob": 4.0928807720774785e-06}, {"id": 134, "seek": 87896, "start": 886.88, "end": 890.76, "text": " and joined to the store table telling you about the state that the store is in, and", "tokens": [293, 6869, 281, 264, 3531, 3199, 3585, 291, 466, 264, 1785, 300, 264, 3531, 307, 294, 11, 293], "temperature": 0.0, "avg_logprob": -0.2267198965583049, "compression_ratio": 1.7403314917127073, "no_speech_prob": 4.0928807720774785e-06}, {"id": 135, "seek": 87896, "start": 890.76, "end": 895.9200000000001, "text": " so forth. So you can have a whole snowflake.", "tokens": [370, 5220, 13, 407, 291, 393, 362, 257, 1379, 44124, 619, 13], "temperature": 0.0, "avg_logprob": -0.2267198965583049, "compression_ratio": 1.7403314917127073, "no_speech_prob": 4.0928807720774785e-06}, {"id": 136, "seek": 87896, "start": 895.9200000000001, "end": 905.6800000000001, "text": " So that's the basic information about this problem. The independent variables, the dependent", "tokens": [407, 300, 311, 264, 3875, 1589, 466, 341, 1154, 13, 440, 6695, 9102, 11, 264, 12334], "temperature": 0.0, "avg_logprob": -0.2267198965583049, "compression_ratio": 1.7403314917127073, "no_speech_prob": 4.0928807720774785e-06}, {"id": 137, "seek": 90568, "start": 905.68, "end": 915.7199999999999, "text": " variable, and you probably also want to think about things like the time frame. Now we start", "tokens": [7006, 11, 293, 291, 1391, 611, 528, 281, 519, 466, 721, 411, 264, 565, 3920, 13, 823, 321, 722], "temperature": 0.0, "avg_logprob": -0.1832238559065194, "compression_ratio": 1.6073059360730593, "no_speech_prob": 2.443987568767625e-06}, {"id": 138, "seek": 90568, "start": 915.7199999999999, "end": 919.4, "text": " in exactly the same way as we did before, loading in exactly the same stuff, setting", "tokens": [294, 2293, 264, 912, 636, 382, 321, 630, 949, 11, 15114, 294, 2293, 264, 912, 1507, 11, 3287], "temperature": 0.0, "avg_logprob": -0.1832238559065194, "compression_ratio": 1.6073059360730593, "no_speech_prob": 2.443987568767625e-06}, {"id": 139, "seek": 90568, "start": 919.4, "end": 929.04, "text": " the path. But when we go read CSV, if you say limit memory equals false, then you're", "tokens": [264, 3100, 13, 583, 562, 321, 352, 1401, 48814, 11, 498, 291, 584, 4948, 4675, 6915, 7908, 11, 550, 291, 434], "temperature": 0.0, "avg_logprob": -0.1832238559065194, "compression_ratio": 1.6073059360730593, "no_speech_prob": 2.443987568767625e-06}, {"id": 140, "seek": 90568, "start": 929.04, "end": 934.16, "text": " basically saying use as much memory as you like to figure out what kinds of data is here.", "tokens": [1936, 1566, 764, 382, 709, 4675, 382, 291, 411, 281, 2573, 484, 437, 3685, 295, 1412, 307, 510, 13], "temperature": 0.0, "avg_logprob": -0.1832238559065194, "compression_ratio": 1.6073059360730593, "no_speech_prob": 2.443987568767625e-06}, {"id": 141, "seek": 93416, "start": 934.16, "end": 939.88, "text": " It's going to run out of memory pretty much regardless of how much memory you have.", "tokens": [467, 311, 516, 281, 1190, 484, 295, 4675, 1238, 709, 10060, 295, 577, 709, 4675, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.13271350422124753, "compression_ratio": 1.6142857142857143, "no_speech_prob": 1.1659384654194582e-05}, {"id": 142, "seek": 93416, "start": 939.88, "end": 945.36, "text": " So what we do in order to limit the amount of space that it takes up when we read it", "tokens": [407, 437, 321, 360, 294, 1668, 281, 4948, 264, 2372, 295, 1901, 300, 309, 2516, 493, 562, 321, 1401, 309], "temperature": 0.0, "avg_logprob": -0.13271350422124753, "compression_ratio": 1.6142857142857143, "no_speech_prob": 1.1659384654194582e-05}, {"id": 143, "seek": 93416, "start": 945.36, "end": 952.48, "text": " in is we create a dictionary for each column name to the data type of that column. And", "tokens": [294, 307, 321, 1884, 257, 25890, 337, 1184, 7738, 1315, 281, 264, 1412, 2010, 295, 300, 7738, 13, 400], "temperature": 0.0, "avg_logprob": -0.13271350422124753, "compression_ratio": 1.6142857142857143, "no_speech_prob": 1.1659384654194582e-05}, {"id": 144, "seek": 93416, "start": 952.48, "end": 958.68, "text": " so for you to create this, it's basically up to you to run less or head or whatever", "tokens": [370, 337, 291, 281, 1884, 341, 11, 309, 311, 1936, 493, 281, 291, 281, 1190, 1570, 420, 1378, 420, 2035], "temperature": 0.0, "avg_logprob": -0.13271350422124753, "compression_ratio": 1.6142857142857143, "no_speech_prob": 1.1659384654194582e-05}, {"id": 145, "seek": 95868, "start": 958.68, "end": 964.8399999999999, "text": " on the dataset to see what the types are and to figure that out and pass them in. So then", "tokens": [322, 264, 28872, 281, 536, 437, 264, 3467, 366, 293, 281, 2573, 300, 484, 293, 1320, 552, 294, 13, 407, 550], "temperature": 0.0, "avg_logprob": -0.16040137506300403, "compression_ratio": 1.4285714285714286, "no_speech_prob": 2.4060980194917647e-06}, {"id": 146, "seek": 95868, "start": 964.8399999999999, "end": 970.12, "text": " you can just pass in Dtype equals with that dictionary.", "tokens": [291, 393, 445, 1320, 294, 413, 20467, 6915, 365, 300, 25890, 13], "temperature": 0.0, "avg_logprob": -0.16040137506300403, "compression_ratio": 1.4285714285714286, "no_speech_prob": 2.4060980194917647e-06}, {"id": 147, "seek": 95868, "start": 970.12, "end": 981.76, "text": " And so check this out. We can read in the whole CSV file in 1 minute and 48 seconds,", "tokens": [400, 370, 1520, 341, 484, 13, 492, 393, 1401, 294, 264, 1379, 48814, 3991, 294, 502, 3456, 293, 11174, 3949, 11], "temperature": 0.0, "avg_logprob": -0.16040137506300403, "compression_ratio": 1.4285714285714286, "no_speech_prob": 2.4060980194917647e-06}, {"id": 148, "seek": 98176, "start": 981.76, "end": 996.6, "text": " and there are 125.5 million rows. So when people say Python's slow, no Python's not", "tokens": [293, 456, 366, 25276, 13, 20, 2459, 13241, 13, 407, 562, 561, 584, 15329, 311, 2964, 11, 572, 15329, 311, 406], "temperature": 0.0, "avg_logprob": -0.2583532826653842, "compression_ratio": 1.395973154362416, "no_speech_prob": 3.2377442948927637e-06}, {"id": 149, "seek": 98176, "start": 996.6, "end": 1003.0, "text": " slow. Python can be slow if you don't use it right, but we can actually pass 125 million", "tokens": [2964, 13, 15329, 393, 312, 2964, 498, 291, 500, 380, 764, 309, 558, 11, 457, 321, 393, 767, 1320, 25276, 2459], "temperature": 0.0, "avg_logprob": -0.2583532826653842, "compression_ratio": 1.395973154362416, "no_speech_prob": 3.2377442948927637e-06}, {"id": 150, "seek": 98176, "start": 1003.0, "end": 1011.4, "text": " CSV records in less than 2 minutes.", "tokens": [48814, 7724, 294, 1570, 813, 568, 2077, 13], "temperature": 0.0, "avg_logprob": -0.2583532826653842, "compression_ratio": 1.395973154362416, "no_speech_prob": 3.2377442948927637e-06}, {"id": 151, "seek": 101140, "start": 1011.4, "end": 1016.72, "text": " My language hat on for just a moment. Actually, if it's fast, almost certainly it's going", "tokens": [1222, 2856, 2385, 322, 337, 445, 257, 1623, 13, 5135, 11, 498, 309, 311, 2370, 11, 1920, 3297, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.1834563033221519, "compression_ratio": 1.4895833333333333, "no_speech_prob": 7.484446541639045e-05}, {"id": 152, "seek": 101140, "start": 1016.72, "end": 1026.04, "text": " to C. So Python is a wrapper around a bunch of C code usually. So Python itself isn't", "tokens": [281, 383, 13, 407, 15329, 307, 257, 46906, 926, 257, 3840, 295, 383, 3089, 2673, 13, 407, 15329, 2564, 1943, 380], "temperature": 0.0, "avg_logprob": -0.1834563033221519, "compression_ratio": 1.4895833333333333, "no_speech_prob": 7.484446541639045e-05}, {"id": 153, "seek": 101140, "start": 1026.04, "end": 1032.24, "text": " actually very fast.", "tokens": [767, 588, 2370, 13], "temperature": 0.0, "avg_logprob": -0.1834563033221519, "compression_ratio": 1.4895833333333333, "no_speech_prob": 7.484446541639045e-05}, {"id": 154, "seek": 101140, "start": 1032.24, "end": 1037.84, "text": " So that was Terence Parr who writes things for writing programming languages for a living,", "tokens": [407, 300, 390, 6564, 655, 47890, 567, 13657, 721, 337, 3579, 9410, 8650, 337, 257, 2647, 11], "temperature": 0.0, "avg_logprob": -0.1834563033221519, "compression_ratio": 1.4895833333333333, "no_speech_prob": 7.484446541639045e-05}, {"id": 155, "seek": 103784, "start": 1037.84, "end": 1045.1599999999999, "text": " and he is right. Python itself is not fast, but almost everything we want to do in Python", "tokens": [293, 415, 307, 558, 13, 15329, 2564, 307, 406, 2370, 11, 457, 1920, 1203, 321, 528, 281, 360, 294, 15329], "temperature": 0.0, "avg_logprob": -0.18575998389202616, "compression_ratio": 1.7346153846153847, "no_speech_prob": 2.5465889848419465e-05}, {"id": 156, "seek": 103784, "start": 1045.1599999999999, "end": 1050.36, "text": " and data science has been written for us in C, or actually more often in Cython, which", "tokens": [293, 1412, 3497, 575, 668, 3720, 337, 505, 294, 383, 11, 420, 767, 544, 2049, 294, 10295, 11943, 11, 597], "temperature": 0.0, "avg_logprob": -0.18575998389202616, "compression_ratio": 1.7346153846153847, "no_speech_prob": 2.5465889848419465e-05}, {"id": 157, "seek": 103784, "start": 1050.36, "end": 1057.08, "text": " is a Python-like language which compiles to C. And so most of the stuff we run in Python", "tokens": [307, 257, 15329, 12, 4092, 2856, 597, 715, 4680, 281, 383, 13, 400, 370, 881, 295, 264, 1507, 321, 1190, 294, 15329], "temperature": 0.0, "avg_logprob": -0.18575998389202616, "compression_ratio": 1.7346153846153847, "no_speech_prob": 2.5465889848419465e-05}, {"id": 158, "seek": 103784, "start": 1057.08, "end": 1062.6399999999999, "text": " is actually running not just C code, but in Pandas a lot of it's written in assembly language,", "tokens": [307, 767, 2614, 406, 445, 383, 3089, 11, 457, 294, 16995, 296, 257, 688, 295, 309, 311, 3720, 294, 12103, 2856, 11], "temperature": 0.0, "avg_logprob": -0.18575998389202616, "compression_ratio": 1.7346153846153847, "no_speech_prob": 2.5465889848419465e-05}, {"id": 159, "seek": 103784, "start": 1062.6399999999999, "end": 1066.52, "text": " it's heavily optimized, behind the scenes, a lot of that is going back to actually calling", "tokens": [309, 311, 10950, 26941, 11, 2261, 264, 8026, 11, 257, 688, 295, 300, 307, 516, 646, 281, 767, 5141], "temperature": 0.0, "avg_logprob": -0.18575998389202616, "compression_ratio": 1.7346153846153847, "no_speech_prob": 2.5465889848419465e-05}, {"id": 160, "seek": 106652, "start": 1066.52, "end": 1074.2, "text": " Fortran-based libraries for linear algebra. So there's layers upon layers of speed that", "tokens": [11002, 4257, 12, 6032, 15148, 337, 8213, 21989, 13, 407, 456, 311, 7914, 3564, 7914, 295, 3073, 300], "temperature": 0.0, "avg_logprob": -0.13140489033290317, "compression_ratio": 1.4974874371859297, "no_speech_prob": 5.955076176178409e-06}, {"id": 161, "seek": 106652, "start": 1074.2, "end": 1080.48, "text": " actually allow us to spend less than 2 minutes reading that much data.", "tokens": [767, 2089, 505, 281, 3496, 1570, 813, 568, 2077, 3760, 300, 709, 1412, 13], "temperature": 0.0, "avg_logprob": -0.13140489033290317, "compression_ratio": 1.4974874371859297, "no_speech_prob": 5.955076176178409e-06}, {"id": 162, "seek": 106652, "start": 1080.48, "end": 1088.76, "text": " If we wrote our own CSV reader in pure Python, it takes thousands of times, at least thousands", "tokens": [759, 321, 4114, 527, 1065, 48814, 15149, 294, 6075, 15329, 11, 309, 2516, 5383, 295, 1413, 11, 412, 1935, 5383], "temperature": 0.0, "avg_logprob": -0.13140489033290317, "compression_ratio": 1.4974874371859297, "no_speech_prob": 5.955076176178409e-06}, {"id": 163, "seek": 106652, "start": 1088.76, "end": 1093.68, "text": " of times longer than the optimized versions.", "tokens": [295, 1413, 2854, 813, 264, 26941, 9606, 13], "temperature": 0.0, "avg_logprob": -0.13140489033290317, "compression_ratio": 1.4974874371859297, "no_speech_prob": 5.955076176178409e-06}, {"id": 164, "seek": 109368, "start": 1093.68, "end": 1098.76, "text": " So for us, what we care about is the speed we can get in practice. And so this is pretty", "tokens": [407, 337, 505, 11, 437, 321, 1127, 466, 307, 264, 3073, 321, 393, 483, 294, 3124, 13, 400, 370, 341, 307, 1238], "temperature": 0.0, "avg_logprob": -0.26908549746951543, "compression_ratio": 1.5212765957446808, "no_speech_prob": 3.321309486636892e-05}, {"id": 165, "seek": 109368, "start": 1098.76, "end": 1105.16, "text": " cool. As well as telling it what the different data types were, we also have to tell it as", "tokens": [1627, 13, 1018, 731, 382, 3585, 309, 437, 264, 819, 1412, 3467, 645, 11, 321, 611, 362, 281, 980, 309, 382], "temperature": 0.0, "avg_logprob": -0.26908549746951543, "compression_ratio": 1.5212765957446808, "no_speech_prob": 3.321309486636892e-05}, {"id": 166, "seek": 109368, "start": 1105.16, "end": 1109.48, "text": " before which things do you want to parse as dates.", "tokens": [949, 597, 721, 360, 291, 528, 281, 48377, 382, 11691, 13], "temperature": 0.0, "avg_logprob": -0.26908549746951543, "compression_ratio": 1.5212765957446808, "no_speech_prob": 3.321309486636892e-05}, {"id": 167, "seek": 109368, "start": 1109.48, "end": 1116.1200000000001, "text": " Question. I noticed that in this dictionary you specify", "tokens": [14464, 13, 286, 5694, 300, 294, 341, 25890, 291, 16500], "temperature": 0.0, "avg_logprob": -0.26908549746951543, "compression_ratio": 1.5212765957446808, "no_speech_prob": 3.321309486636892e-05}, {"id": 168, "seek": 111612, "start": 1116.12, "end": 1125.1999999999998, "text": " int64, int33, and int8. I was wondering in practice, is it faster if you all specify", "tokens": [560, 19395, 11, 560, 10191, 11, 293, 560, 23, 13, 286, 390, 6359, 294, 3124, 11, 307, 309, 4663, 498, 291, 439, 16500], "temperature": 0.0, "avg_logprob": -0.17827559452430874, "compression_ratio": 1.677685950413223, "no_speech_prob": 5.862775651621632e-06}, {"id": 169, "seek": 111612, "start": 1125.1999999999998, "end": 1129.32, "text": " them to int or slower, or any performance consideration?", "tokens": [552, 281, 560, 420, 14009, 11, 420, 604, 3389, 12381, 30], "temperature": 0.0, "avg_logprob": -0.17827559452430874, "compression_ratio": 1.677685950413223, "no_speech_prob": 5.862775651621632e-06}, {"id": 170, "seek": 111612, "start": 1129.32, "end": 1134.4399999999998, "text": " So the key performance consideration here was to use the smallest number of bits that", "tokens": [407, 264, 2141, 3389, 12381, 510, 390, 281, 764, 264, 16998, 1230, 295, 9239, 300], "temperature": 0.0, "avg_logprob": -0.17827559452430874, "compression_ratio": 1.677685950413223, "no_speech_prob": 5.862775651621632e-06}, {"id": 171, "seek": 111612, "start": 1134.4399999999998, "end": 1139.9599999999998, "text": " I could to fully represent the column. So if I had used int8 for item number, there", "tokens": [286, 727, 281, 4498, 2906, 264, 7738, 13, 407, 498, 286, 632, 1143, 560, 23, 337, 3174, 1230, 11, 456], "temperature": 0.0, "avg_logprob": -0.17827559452430874, "compression_ratio": 1.677685950413223, "no_speech_prob": 5.862775651621632e-06}, {"id": 172, "seek": 111612, "start": 1139.9599999999998, "end": 1146.1, "text": " are more than 255 item numbers. More specifically, the maximum item number is bigger than 255.", "tokens": [366, 544, 813, 3552, 20, 3174, 3547, 13, 5048, 4682, 11, 264, 6674, 3174, 1230, 307, 3801, 813, 3552, 20, 13], "temperature": 0.0, "avg_logprob": -0.17827559452430874, "compression_ratio": 1.677685950413223, "no_speech_prob": 5.862775651621632e-06}, {"id": 173, "seek": 114610, "start": 1146.1, "end": 1152.84, "text": " So on the other hand, if I had used int64 for store number, it's using more bits than", "tokens": [407, 322, 264, 661, 1011, 11, 498, 286, 632, 1143, 560, 19395, 337, 3531, 1230, 11, 309, 311, 1228, 544, 9239, 813], "temperature": 0.0, "avg_logprob": -0.1495148468017578, "compression_ratio": 1.6048387096774193, "no_speech_prob": 1.045144017552957e-05}, {"id": 174, "seek": 114610, "start": 1152.84, "end": 1157.32, "text": " necessary. Given that the whole purpose here was to avoid running out of RAM, we don't", "tokens": [4818, 13, 18600, 300, 264, 1379, 4334, 510, 390, 281, 5042, 2614, 484, 295, 14561, 11, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.1495148468017578, "compression_ratio": 1.6048387096774193, "no_speech_prob": 1.045144017552957e-05}, {"id": 175, "seek": 114610, "start": 1157.32, "end": 1161.9199999999998, "text": " want to be using up 8 times more memory than necessary.", "tokens": [528, 281, 312, 1228, 493, 1649, 1413, 544, 4675, 813, 4818, 13], "temperature": 0.0, "avg_logprob": -0.1495148468017578, "compression_ratio": 1.6048387096774193, "no_speech_prob": 1.045144017552957e-05}, {"id": 176, "seek": 114610, "start": 1161.9199999999998, "end": 1165.8, "text": " So the key thing was really about memory. And in fact, when you're working with large", "tokens": [407, 264, 2141, 551, 390, 534, 466, 4675, 13, 400, 294, 1186, 11, 562, 291, 434, 1364, 365, 2416], "temperature": 0.0, "avg_logprob": -0.1495148468017578, "compression_ratio": 1.6048387096774193, "no_speech_prob": 1.045144017552957e-05}, {"id": 177, "seek": 114610, "start": 1165.8, "end": 1171.7199999999998, "text": " datasets, very often you'll find the slow piece is the actually reading and writing", "tokens": [42856, 11, 588, 2049, 291, 603, 915, 264, 2964, 2522, 307, 264, 767, 3760, 293, 3579], "temperature": 0.0, "avg_logprob": -0.1495148468017578, "compression_ratio": 1.6048387096774193, "no_speech_prob": 1.045144017552957e-05}, {"id": 178, "seek": 117172, "start": 1171.72, "end": 1179.6000000000001, "text": " to RAM, not the actual CPU operations. So very often that's the key performance consideration.", "tokens": [281, 14561, 11, 406, 264, 3539, 13199, 7705, 13, 407, 588, 2049, 300, 311, 264, 2141, 3389, 12381, 13], "temperature": 0.0, "avg_logprob": -0.2399955090181327, "compression_ratio": 1.4751131221719458, "no_speech_prob": 1.2805221558664925e-05}, {"id": 179, "seek": 117172, "start": 1179.6000000000001, "end": 1187.76, "text": " Also however, as a rule of thumb, smaller data types often will run faster, particularly", "tokens": [2743, 4461, 11, 382, 257, 4978, 295, 9298, 11, 4356, 1412, 3467, 2049, 486, 1190, 4663, 11, 4098], "temperature": 0.0, "avg_logprob": -0.2399955090181327, "compression_ratio": 1.4751131221719458, "no_speech_prob": 1.2805221558664925e-05}, {"id": 180, "seek": 117172, "start": 1187.76, "end": 1193.4, "text": " if you can use SIMD, so that's Single Instruction Multiple Data Vectorized Code. It can pack", "tokens": [498, 291, 393, 764, 24738, 35, 11, 370, 300, 311, 31248, 2730, 3826, 40056, 11888, 691, 20814, 1602, 15549, 13, 467, 393, 2844], "temperature": 0.0, "avg_logprob": -0.2399955090181327, "compression_ratio": 1.4751131221719458, "no_speech_prob": 1.2805221558664925e-05}, {"id": 181, "seek": 117172, "start": 1193.4, "end": 1199.84, "text": " more numbers into a single vector to run at once.", "tokens": [544, 3547, 666, 257, 2167, 8062, 281, 1190, 412, 1564, 13], "temperature": 0.0, "avg_logprob": -0.2399955090181327, "compression_ratio": 1.4751131221719458, "no_speech_prob": 1.2805221558664925e-05}, {"id": 182, "seek": 119984, "start": 1199.84, "end": 1205.9399999999998, "text": " Powerful Explorer.", "tokens": [7086, 906, 31895, 13], "temperature": 1.0, "avg_logprob": -2.729571122389573, "compression_ratio": 1.0227272727272727, "no_speech_prob": 1.3211711120675318e-05}, {"id": 183, "seek": 119984, "start": 1205.9399999999998, "end": 1210.1399999999999, "text": " I'm not sure if this was all thoughfully simplified, not exactly right.", "tokens": [286, 478, 406, 988, 498, 341, 390, 439, 1673, 2277, 26335, 11, 406, 2293, 558, 13], "temperature": 1.0, "avg_logprob": -2.729571122389573, "compression_ratio": 1.0227272727272727, "no_speech_prob": 1.3211711120675318e-05}, {"id": 184, "seek": 121014, "start": 1210.14, "end": 1215.0800000000002, "text": " Question. Once you do this, the shuffle thing beforehand is not needed anymore? Like maybe", "tokens": [14464, 13, 3443, 291, 360, 341, 11, 264, 39426, 551, 22893, 307, 406, 2978, 3602, 30, 1743, 1310], "temperature": 0.0, "avg_logprob": -0.4607102429425275, "compression_ratio": 1.375, "no_speech_prob": 0.02516513131558895}, {"id": 185, "seek": 121014, "start": 1215.0800000000002, "end": 1219.5800000000002, "text": " you send a random substation?", "tokens": [291, 2845, 257, 4974, 4594, 399, 30], "temperature": 0.0, "avg_logprob": -0.4607102429425275, "compression_ratio": 1.375, "no_speech_prob": 0.02516513131558895}, {"id": 186, "seek": 121014, "start": 1219.5800000000002, "end": 1230.74, "text": " So although here I've read in the whole thing, when I start, I never start by reading in", "tokens": [407, 4878, 510, 286, 600, 1401, 294, 264, 1379, 551, 11, 562, 286, 722, 11, 286, 1128, 722, 538, 3760, 294], "temperature": 0.0, "avg_logprob": -0.4607102429425275, "compression_ratio": 1.375, "no_speech_prob": 0.02516513131558895}, {"id": 187, "seek": 123074, "start": 1230.74, "end": 1241.78, "text": " the whole thing. So if you search the forum for shuf, you'll find some tips about how", "tokens": [264, 1379, 551, 13, 407, 498, 291, 3164, 264, 17542, 337, 402, 2947, 11, 291, 603, 915, 512, 6082, 466, 577], "temperature": 0.0, "avg_logprob": -0.1525705675535564, "compression_ratio": 1.5944444444444446, "no_speech_prob": 1.147850343841128e-05}, {"id": 188, "seek": 123074, "start": 1241.78, "end": 1248.46, "text": " to use this unix command to get a random sample of data at the command prompt. And then you", "tokens": [281, 764, 341, 517, 970, 5622, 281, 483, 257, 4974, 6889, 295, 1412, 412, 264, 5622, 12391, 13, 400, 550, 291], "temperature": 0.0, "avg_logprob": -0.1525705675535564, "compression_ratio": 1.5944444444444446, "no_speech_prob": 1.147850343841128e-05}, {"id": 189, "seek": 123074, "start": 1248.46, "end": 1249.46, "text": " can just read that.", "tokens": [393, 445, 1401, 300, 13], "temperature": 0.0, "avg_logprob": -0.1525705675535564, "compression_ratio": 1.5944444444444446, "no_speech_prob": 1.147850343841128e-05}, {"id": 190, "seek": 123074, "start": 1249.46, "end": 1254.18, "text": " And the nice thing is that that's a good way, for example, to find out what data types to", "tokens": [400, 264, 1481, 551, 307, 300, 300, 311, 257, 665, 636, 11, 337, 1365, 11, 281, 915, 484, 437, 1412, 3467, 281], "temperature": 0.0, "avg_logprob": -0.1525705675535564, "compression_ratio": 1.5944444444444446, "no_speech_prob": 1.147850343841128e-05}, {"id": 191, "seek": 125418, "start": 1254.18, "end": 1268.7, "text": " use, to read in a random sample and let pandas figure it out for you. And in general, I do", "tokens": [764, 11, 281, 1401, 294, 257, 4974, 6889, 293, 718, 4565, 296, 2573, 309, 484, 337, 291, 13, 400, 294, 2674, 11, 286, 360], "temperature": 0.0, "avg_logprob": -0.16223694827105548, "compression_ratio": 1.5026178010471205, "no_speech_prob": 1.1125516721222084e-05}, {"id": 192, "seek": 125418, "start": 1268.7, "end": 1273.78, "text": " as much work as possible on a sample until I feel confident that I understand the sample", "tokens": [382, 709, 589, 382, 1944, 322, 257, 6889, 1826, 286, 841, 6679, 300, 286, 1223, 264, 6889], "temperature": 0.0, "avg_logprob": -0.16223694827105548, "compression_ratio": 1.5026178010471205, "no_speech_prob": 1.1125516721222084e-05}, {"id": 193, "seek": 125418, "start": 1273.78, "end": 1277.42, "text": " before I move on.", "tokens": [949, 286, 1286, 322, 13], "temperature": 0.0, "avg_logprob": -0.16223694827105548, "compression_ratio": 1.5026178010471205, "no_speech_prob": 1.1125516721222084e-05}, {"id": 194, "seek": 125418, "start": 1277.42, "end": 1281.02, "text": " Having said that, what we're about to learn is some techniques for running models on this", "tokens": [10222, 848, 300, 11, 437, 321, 434, 466, 281, 1466, 307, 512, 7512, 337, 2614, 5245, 322, 341], "temperature": 0.0, "avg_logprob": -0.16223694827105548, "compression_ratio": 1.5026178010471205, "no_speech_prob": 1.1125516721222084e-05}, {"id": 195, "seek": 128102, "start": 1281.02, "end": 1285.18, "text": " pool data set that are actually going to work on arbitrarily large data sets. And also I", "tokens": [7005, 1412, 992, 300, 366, 767, 516, 281, 589, 322, 19071, 3289, 2416, 1412, 6352, 13, 400, 611, 286], "temperature": 0.0, "avg_logprob": -0.18435368796651677, "compression_ratio": 1.734006734006734, "no_speech_prob": 3.219143763999455e-05}, {"id": 196, "seek": 128102, "start": 1285.18, "end": 1289.62, "text": " specifically wanted to talk about how to read in large data sets.", "tokens": [4682, 1415, 281, 751, 466, 577, 281, 1401, 294, 2416, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.18435368796651677, "compression_ratio": 1.734006734006734, "no_speech_prob": 3.219143763999455e-05}, {"id": 197, "seek": 128102, "start": 1289.62, "end": 1295.82, "text": " One thing to mention, on promotion objects, objects are like saying create a general-purpose", "tokens": [1485, 551, 281, 2152, 11, 322, 15783, 6565, 11, 6565, 366, 411, 1566, 1884, 257, 2674, 12, 42601], "temperature": 0.0, "avg_logprob": -0.18435368796651677, "compression_ratio": 1.734006734006734, "no_speech_prob": 3.219143763999455e-05}, {"id": 198, "seek": 128102, "start": 1295.82, "end": 1301.02, "text": " Python data type which is slow and memory-heavy. And the reason for that is that this is a", "tokens": [15329, 1412, 2010, 597, 307, 2964, 293, 4675, 12, 37157, 13, 400, 264, 1778, 337, 300, 307, 300, 341, 307, 257], "temperature": 0.0, "avg_logprob": -0.18435368796651677, "compression_ratio": 1.734006734006734, "no_speech_prob": 3.219143763999455e-05}, {"id": 199, "seek": 128102, "start": 1301.02, "end": 1306.3, "text": " boolean which also has missing values, so we need to deal with this before we can turn", "tokens": [748, 4812, 282, 597, 611, 575, 5361, 4190, 11, 370, 321, 643, 281, 2028, 365, 341, 949, 321, 393, 1261], "temperature": 0.0, "avg_logprob": -0.18435368796651677, "compression_ratio": 1.734006734006734, "no_speech_prob": 3.219143763999455e-05}, {"id": 200, "seek": 128102, "start": 1306.3, "end": 1310.76, "text": " it into a boolean. So you can see after that, I then go ahead and say fill in the missing", "tokens": [309, 666, 257, 748, 4812, 282, 13, 407, 291, 393, 536, 934, 300, 11, 286, 550, 352, 2286, 293, 584, 2836, 294, 264, 5361], "temperature": 0.0, "avg_logprob": -0.18435368796651677, "compression_ratio": 1.734006734006734, "no_speech_prob": 3.219143763999455e-05}, {"id": 201, "seek": 131076, "start": 1310.76, "end": 1316.06, "text": " values with false. Now you wouldn't just do this without doing some checking ahead of", "tokens": [4190, 365, 7908, 13, 823, 291, 2759, 380, 445, 360, 341, 1553, 884, 512, 8568, 2286, 295], "temperature": 0.0, "avg_logprob": -0.16320689673562652, "compression_ratio": 1.712, "no_speech_prob": 5.014713224227307e-06}, {"id": 202, "seek": 131076, "start": 1316.06, "end": 1321.18, "text": " time, but some exploratory data analysis shows that it seems that this is probably an appropriate", "tokens": [565, 11, 457, 512, 24765, 4745, 1412, 5215, 3110, 300, 309, 2544, 300, 341, 307, 1391, 364, 6854], "temperature": 0.0, "avg_logprob": -0.16320689673562652, "compression_ratio": 1.712, "no_speech_prob": 5.014713224227307e-06}, {"id": 203, "seek": 131076, "start": 1321.18, "end": 1326.7, "text": " thing to do. It seems that missing does mean false.", "tokens": [551, 281, 360, 13, 467, 2544, 300, 5361, 775, 914, 7908, 13], "temperature": 0.0, "avg_logprob": -0.16320689673562652, "compression_ratio": 1.712, "no_speech_prob": 5.014713224227307e-06}, {"id": 204, "seek": 131076, "start": 1326.7, "end": 1331.9, "text": " Objects generally read in as strings, so replace the strings true and false with actual booleans,", "tokens": [24753, 82, 5101, 1401, 294, 382, 13985, 11, 370, 7406, 264, 13985, 2074, 293, 7908, 365, 3539, 748, 4812, 599, 11], "temperature": 0.0, "avg_logprob": -0.16320689673562652, "compression_ratio": 1.712, "no_speech_prob": 5.014713224227307e-06}, {"id": 205, "seek": 131076, "start": 1331.9, "end": 1337.94, "text": " and then finally convert it to an actual boolean type. So at this point when I save this, this", "tokens": [293, 550, 2721, 7620, 309, 281, 364, 3539, 748, 4812, 282, 2010, 13, 407, 412, 341, 935, 562, 286, 3155, 341, 11, 341], "temperature": 0.0, "avg_logprob": -0.16320689673562652, "compression_ratio": 1.712, "no_speech_prob": 5.014713224227307e-06}, {"id": 206, "seek": 133794, "start": 1337.94, "end": 1346.9, "text": " file now of 123 million records takes up something under 2.5 gigabytes of memory. So you can", "tokens": [3991, 586, 295, 34466, 2459, 7724, 2516, 493, 746, 833, 568, 13, 20, 42741, 295, 4675, 13, 407, 291, 393], "temperature": 0.0, "avg_logprob": -0.1665168943859282, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.2411421266733669e-05}, {"id": 207, "seek": 133794, "start": 1346.9, "end": 1353.7, "text": " look at pretty large data sets even on pretty small computers, which is interesting.", "tokens": [574, 412, 1238, 2416, 1412, 6352, 754, 322, 1238, 1359, 10807, 11, 597, 307, 1880, 13], "temperature": 0.0, "avg_logprob": -0.1665168943859282, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.2411421266733669e-05}, {"id": 208, "seek": 133794, "start": 1353.7, "end": 1357.9, "text": " So at that point, now that it's in a nice fast format, look how fast it is. I can save", "tokens": [407, 412, 300, 935, 11, 586, 300, 309, 311, 294, 257, 1481, 2370, 7877, 11, 574, 577, 2370, 309, 307, 13, 286, 393, 3155], "temperature": 0.0, "avg_logprob": -0.1665168943859282, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.2411421266733669e-05}, {"id": 209, "seek": 133794, "start": 1357.9, "end": 1363.9, "text": " it to feather format in under 5 seconds. So that's nice.", "tokens": [309, 281, 25852, 7877, 294, 833, 1025, 3949, 13, 407, 300, 311, 1481, 13], "temperature": 0.0, "avg_logprob": -0.1665168943859282, "compression_ratio": 1.5285714285714285, "no_speech_prob": 1.2411421266733669e-05}, {"id": 210, "seek": 136390, "start": 1363.9, "end": 1370.26, "text": " And then because Pandas is generally pretty fast, you can do stuff like summarize every", "tokens": [400, 550, 570, 16995, 296, 307, 5101, 1238, 2370, 11, 291, 393, 360, 1507, 411, 20858, 633], "temperature": 0.0, "avg_logprob": -0.11724187951338919, "compression_ratio": 1.606425702811245, "no_speech_prob": 4.029445790365571e-06}, {"id": 211, "seek": 136390, "start": 1370.26, "end": 1376.66, "text": " column of all 125 million records in 20 seconds.", "tokens": [7738, 295, 439, 25276, 2459, 7724, 294, 945, 3949, 13], "temperature": 0.0, "avg_logprob": -0.11724187951338919, "compression_ratio": 1.606425702811245, "no_speech_prob": 4.029445790365571e-06}, {"id": 212, "seek": 136390, "start": 1376.66, "end": 1382.22, "text": " So the first thing I looked at here actually is the dates. Generally speaking, dates are", "tokens": [407, 264, 700, 551, 286, 2956, 412, 510, 767, 307, 264, 11691, 13, 21082, 4124, 11, 11691, 366], "temperature": 0.0, "avg_logprob": -0.11724187951338919, "compression_ratio": 1.606425702811245, "no_speech_prob": 4.029445790365571e-06}, {"id": 213, "seek": 136390, "start": 1382.22, "end": 1386.74, "text": " just going to be really important in a lot of the stuff you do, particularly because", "tokens": [445, 516, 281, 312, 534, 1021, 294, 257, 688, 295, 264, 1507, 291, 360, 11, 4098, 570], "temperature": 0.0, "avg_logprob": -0.11724187951338919, "compression_ratio": 1.606425702811245, "no_speech_prob": 4.029445790365571e-06}, {"id": 214, "seek": 136390, "start": 1386.74, "end": 1392.14, "text": " any model that you put in in practice, you're going to be putting it in at some date that", "tokens": [604, 2316, 300, 291, 829, 294, 294, 3124, 11, 291, 434, 516, 281, 312, 3372, 309, 294, 412, 512, 4002, 300], "temperature": 0.0, "avg_logprob": -0.11724187951338919, "compression_ratio": 1.606425702811245, "no_speech_prob": 4.029445790365571e-06}, {"id": 215, "seek": 139214, "start": 1392.14, "end": 1397.0600000000002, "text": " is later than the date that you trained it by definition. So if anything in the world", "tokens": [307, 1780, 813, 264, 4002, 300, 291, 8895, 309, 538, 7123, 13, 407, 498, 1340, 294, 264, 1002], "temperature": 0.0, "avg_logprob": -0.2047708337957209, "compression_ratio": 1.6613545816733069, "no_speech_prob": 6.854269940959057e-06}, {"id": 216, "seek": 139214, "start": 1397.0600000000002, "end": 1402.14, "text": " changes, you need to know how your predictive accuracy changes as well.", "tokens": [2962, 11, 291, 643, 281, 458, 577, 428, 35521, 14170, 2962, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.2047708337957209, "compression_ratio": 1.6613545816733069, "no_speech_prob": 6.854269940959057e-06}, {"id": 217, "seek": 139214, "start": 1402.14, "end": 1405.7800000000002, "text": " And so what you'll see on Kaggle, and what you should always do in your on projects,", "tokens": [400, 370, 437, 291, 603, 536, 322, 48751, 22631, 11, 293, 437, 291, 820, 1009, 360, 294, 428, 322, 4455, 11], "temperature": 0.0, "avg_logprob": -0.2047708337957209, "compression_ratio": 1.6613545816733069, "no_speech_prob": 6.854269940959057e-06}, {"id": 218, "seek": 139214, "start": 1405.7800000000002, "end": 1410.5800000000002, "text": " is make sure that your dates don't overlap. So in this case, the dates that we have in", "tokens": [307, 652, 988, 300, 428, 11691, 500, 380, 19959, 13, 407, 294, 341, 1389, 11, 264, 11691, 300, 321, 362, 294], "temperature": 0.0, "avg_logprob": -0.2047708337957209, "compression_ratio": 1.6613545816733069, "no_speech_prob": 6.854269940959057e-06}, {"id": 219, "seek": 139214, "start": 1410.5800000000002, "end": 1421.5800000000002, "text": " the training set go from 2013 to mid-August 2017. And then in our test set, we have the", "tokens": [264, 3097, 992, 352, 490, 9012, 281, 2062, 12, 32, 697, 381, 6591, 13, 400, 550, 294, 527, 1500, 992, 11, 321, 362, 264], "temperature": 0.0, "avg_logprob": -0.2047708337957209, "compression_ratio": 1.6613545816733069, "no_speech_prob": 6.854269940959057e-06}, {"id": 220, "seek": 142158, "start": 1421.58, "end": 1428.78, "text": " dates that go from one day later, August 16th, until the end of the month.", "tokens": [11691, 300, 352, 490, 472, 786, 1780, 11, 6897, 3165, 392, 11, 1826, 264, 917, 295, 264, 1618, 13], "temperature": 0.0, "avg_logprob": -0.14188480377197266, "compression_ratio": 1.5818181818181818, "no_speech_prob": 1.2218984920764342e-05}, {"id": 221, "seek": 142158, "start": 1428.78, "end": 1435.22, "text": " So this is a key thing. You can't really do any useful machine learning until you understand", "tokens": [407, 341, 307, 257, 2141, 551, 13, 509, 393, 380, 534, 360, 604, 4420, 3479, 2539, 1826, 291, 1223], "temperature": 0.0, "avg_logprob": -0.14188480377197266, "compression_ratio": 1.5818181818181818, "no_speech_prob": 1.2218984920764342e-05}, {"id": 222, "seek": 142158, "start": 1435.22, "end": 1443.78, "text": " this basic piece here, which is you've got 4 years of data and you're trying to predict", "tokens": [341, 3875, 2522, 510, 11, 597, 307, 291, 600, 658, 1017, 924, 295, 1412, 293, 291, 434, 1382, 281, 6069], "temperature": 0.0, "avg_logprob": -0.14188480377197266, "compression_ratio": 1.5818181818181818, "no_speech_prob": 1.2218984920764342e-05}, {"id": 223, "seek": 142158, "start": 1443.78, "end": 1449.6599999999999, "text": " the next 2 weeks. So that's just a fundamental thing that you're going to need to understand", "tokens": [264, 958, 568, 3259, 13, 407, 300, 311, 445, 257, 8088, 551, 300, 291, 434, 516, 281, 643, 281, 1223], "temperature": 0.0, "avg_logprob": -0.14188480377197266, "compression_ratio": 1.5818181818181818, "no_speech_prob": 1.2218984920764342e-05}, {"id": 224, "seek": 144966, "start": 1449.66, "end": 1451.98, "text": " before you can really do a good job of this.", "tokens": [949, 291, 393, 534, 360, 257, 665, 1691, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.19347147319627844, "compression_ratio": 1.5772727272727274, "no_speech_prob": 1.321183572144946e-05}, {"id": 225, "seek": 144966, "start": 1451.98, "end": 1458.3000000000002, "text": " And so as soon as I see that, what does that say to you? If you wanted to now use a smaller", "tokens": [400, 370, 382, 2321, 382, 286, 536, 300, 11, 437, 775, 300, 584, 281, 291, 30, 759, 291, 1415, 281, 586, 764, 257, 4356], "temperature": 0.0, "avg_logprob": -0.19347147319627844, "compression_ratio": 1.5772727272727274, "no_speech_prob": 1.321183572144946e-05}, {"id": 226, "seek": 144966, "start": 1458.3000000000002, "end": 1466.22, "text": " data set, should you use a random sample, or is there something better you could do?", "tokens": [1412, 992, 11, 820, 291, 764, 257, 4974, 6889, 11, 420, 307, 456, 746, 1101, 291, 727, 360, 30], "temperature": 0.0, "avg_logprob": -0.19347147319627844, "compression_ratio": 1.5772727272727274, "no_speech_prob": 1.321183572144946e-05}, {"id": 227, "seek": 144966, "start": 1466.22, "end": 1467.22, "text": " Probably from the bottom, more recent.", "tokens": [9210, 490, 264, 2767, 11, 544, 5162, 13], "temperature": 0.0, "avg_logprob": -0.19347147319627844, "compression_ratio": 1.5772727272727274, "no_speech_prob": 1.321183572144946e-05}, {"id": 228, "seek": 144966, "start": 1467.22, "end": 1472.78, "text": " Yeah, get the most recent. And if you ever have trouble answering questions like this,", "tokens": [865, 11, 483, 264, 881, 5162, 13, 400, 498, 291, 1562, 362, 5253, 13430, 1651, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.19347147319627844, "compression_ratio": 1.5772727272727274, "no_speech_prob": 1.321183572144946e-05}, {"id": 229, "seek": 147278, "start": 1472.78, "end": 1480.02, "text": " just try to make it as physical as possible. So it's like, I'm going to go to a shop next", "tokens": [445, 853, 281, 652, 309, 382, 4001, 382, 1944, 13, 407, 309, 311, 411, 11, 286, 478, 516, 281, 352, 281, 257, 3945, 958], "temperature": 0.0, "avg_logprob": -0.11120052968174958, "compression_ratio": 1.7886178861788617, "no_speech_prob": 1.4970902157074306e-05}, {"id": 230, "seek": 147278, "start": 1480.02, "end": 1487.1399999999999, "text": " week and I've got a $5 bet with my brother as to whether I can guess how many cans of", "tokens": [1243, 293, 286, 600, 658, 257, 1848, 20, 778, 365, 452, 3708, 382, 281, 1968, 286, 393, 2041, 577, 867, 21835, 295], "temperature": 0.0, "avg_logprob": -0.11120052968174958, "compression_ratio": 1.7886178861788617, "no_speech_prob": 1.4970902157074306e-05}, {"id": 231, "seek": 147278, "start": 1487.1399999999999, "end": 1493.58, "text": " Coke are going to be on the shelf. Probably the best way to do that would be to go to", "tokens": [32996, 366, 516, 281, 312, 322, 264, 15222, 13, 9210, 264, 1151, 636, 281, 360, 300, 576, 312, 281, 352, 281], "temperature": 0.0, "avg_logprob": -0.11120052968174958, "compression_ratio": 1.7886178861788617, "no_speech_prob": 1.4970902157074306e-05}, {"id": 232, "seek": 147278, "start": 1493.58, "end": 1498.3799999999999, "text": " the shop same day of the previous week and see how many cans of Coke are on the shelf", "tokens": [264, 3945, 912, 786, 295, 264, 3894, 1243, 293, 536, 577, 867, 21835, 295, 32996, 366, 322, 264, 15222], "temperature": 0.0, "avg_logprob": -0.11120052968174958, "compression_ratio": 1.7886178861788617, "no_speech_prob": 1.4970902157074306e-05}, {"id": 233, "seek": 147278, "start": 1498.3799999999999, "end": 1502.58, "text": " and guess it's going to be the same. You wouldn't go and look at how many were there 4 years", "tokens": [293, 2041, 309, 311, 516, 281, 312, 264, 912, 13, 509, 2759, 380, 352, 293, 574, 412, 577, 867, 645, 456, 1017, 924], "temperature": 0.0, "avg_logprob": -0.11120052968174958, "compression_ratio": 1.7886178861788617, "no_speech_prob": 1.4970902157074306e-05}, {"id": 234, "seek": 150258, "start": 1502.58, "end": 1503.58, "text": " ago.", "tokens": [2057, 13], "temperature": 0.0, "avg_logprob": -0.2801858883092899, "compression_ratio": 1.556910569105691, "no_speech_prob": 2.046255031018518e-05}, {"id": 235, "seek": 150258, "start": 1503.58, "end": 1504.58, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.2801858883092899, "compression_ratio": 1.556910569105691, "no_speech_prob": 2.046255031018518e-05}, {"id": 236, "seek": 150258, "start": 1504.58, "end": 1512.9399999999998, "text": " But couldn't 4 years ago that same time frame of the year be important? For example, how", "tokens": [583, 2809, 380, 1017, 924, 2057, 300, 912, 565, 3920, 295, 264, 1064, 312, 1021, 30, 1171, 1365, 11, 577], "temperature": 0.0, "avg_logprob": -0.2801858883092899, "compression_ratio": 1.556910569105691, "no_speech_prob": 2.046255031018518e-05}, {"id": 237, "seek": 150258, "start": 1512.9399999999998, "end": 1515.6999999999998, "text": " much Coke they have on the shelf at Christmas time is going to be way more than...", "tokens": [709, 32996, 436, 362, 322, 264, 15222, 412, 5272, 565, 307, 516, 281, 312, 636, 544, 813, 1097], "temperature": 0.0, "avg_logprob": -0.2801858883092899, "compression_ratio": 1.556910569105691, "no_speech_prob": 2.046255031018518e-05}, {"id": 238, "seek": 150258, "start": 1515.6999999999998, "end": 1522.86, "text": " Exactly. So it's not that there's no useful information from 4 years ago, and so we don't", "tokens": [7587, 13, 407, 309, 311, 406, 300, 456, 311, 572, 4420, 1589, 490, 1017, 924, 2057, 11, 293, 370, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.2801858883092899, "compression_ratio": 1.556910569105691, "no_speech_prob": 2.046255031018518e-05}, {"id": 239, "seek": 150258, "start": 1522.86, "end": 1529.6999999999998, "text": " want to entirely throw it away. But as a first step, what's the simplest possible thing,", "tokens": [528, 281, 7696, 3507, 309, 1314, 13, 583, 382, 257, 700, 1823, 11, 437, 311, 264, 22811, 1944, 551, 11], "temperature": 0.0, "avg_logprob": -0.2801858883092899, "compression_ratio": 1.556910569105691, "no_speech_prob": 2.046255031018518e-05}, {"id": 240, "seek": 152970, "start": 1529.7, "end": 1536.5800000000002, "text": " it's kind of like submitting the means. I wouldn't submit the mean of 2012 sales, I", "tokens": [309, 311, 733, 295, 411, 31836, 264, 1355, 13, 286, 2759, 380, 10315, 264, 914, 295, 9125, 5763, 11, 286], "temperature": 0.0, "avg_logprob": -0.17229007720947265, "compression_ratio": 1.755458515283843, "no_speech_prob": 3.041571972062229e-06}, {"id": 241, "seek": 152970, "start": 1536.5800000000002, "end": 1542.5800000000002, "text": " would probably submit the mean of last month's sales.", "tokens": [576, 1391, 10315, 264, 914, 295, 1036, 1618, 311, 5763, 13], "temperature": 0.0, "avg_logprob": -0.17229007720947265, "compression_ratio": 1.755458515283843, "no_speech_prob": 3.041571972062229e-06}, {"id": 242, "seek": 152970, "start": 1542.5800000000002, "end": 1549.5, "text": " So we're just trying to think about how might we want to create some initial easy models,", "tokens": [407, 321, 434, 445, 1382, 281, 519, 466, 577, 1062, 321, 528, 281, 1884, 512, 5883, 1858, 5245, 11], "temperature": 0.0, "avg_logprob": -0.17229007720947265, "compression_ratio": 1.755458515283843, "no_speech_prob": 3.041571972062229e-06}, {"id": 243, "seek": 152970, "start": 1549.5, "end": 1553.14, "text": " and later on we might want to weight it. So for example, we might want to weight more", "tokens": [293, 1780, 322, 321, 1062, 528, 281, 3364, 309, 13, 407, 337, 1365, 11, 321, 1062, 528, 281, 3364, 544], "temperature": 0.0, "avg_logprob": -0.17229007720947265, "compression_ratio": 1.755458515283843, "no_speech_prob": 3.041571972062229e-06}, {"id": 244, "seek": 152970, "start": 1553.14, "end": 1558.14, "text": " recent dates more highly, they're probably more relevant. But we should do a whole bunch", "tokens": [5162, 11691, 544, 5405, 11, 436, 434, 1391, 544, 7340, 13, 583, 321, 820, 360, 257, 1379, 3840], "temperature": 0.0, "avg_logprob": -0.17229007720947265, "compression_ratio": 1.755458515283843, "no_speech_prob": 3.041571972062229e-06}, {"id": 245, "seek": 155814, "start": 1558.14, "end": 1561.42, "text": " of exploratory data analysis to check that.", "tokens": [295, 24765, 4745, 1412, 5215, 281, 1520, 300, 13], "temperature": 0.0, "avg_logprob": -0.188562881114871, "compression_ratio": 1.7076923076923076, "no_speech_prob": 1.0952940101560671e-05}, {"id": 246, "seek": 155814, "start": 1561.42, "end": 1567.66, "text": " So here's what the bottom of that dataset looks like. You can see literally it's got", "tokens": [407, 510, 311, 437, 264, 2767, 295, 300, 28872, 1542, 411, 13, 509, 393, 536, 3736, 309, 311, 658], "temperature": 0.0, "avg_logprob": -0.188562881114871, "compression_ratio": 1.7076923076923076, "no_speech_prob": 1.0952940101560671e-05}, {"id": 247, "seek": 155814, "start": 1567.66, "end": 1574.74, "text": " a date, a store number, an item number, and it tells you whether or not that particular", "tokens": [257, 4002, 11, 257, 3531, 1230, 11, 364, 3174, 1230, 11, 293, 309, 5112, 291, 1968, 420, 406, 300, 1729], "temperature": 0.0, "avg_logprob": -0.188562881114871, "compression_ratio": 1.7076923076923076, "no_speech_prob": 1.0952940101560671e-05}, {"id": 248, "seek": 155814, "start": 1574.74, "end": 1580.18, "text": " item was on sale at that particular store on that particular date, and then there's", "tokens": [3174, 390, 322, 8680, 412, 300, 1729, 3531, 322, 300, 1729, 4002, 11, 293, 550, 456, 311], "temperature": 0.0, "avg_logprob": -0.188562881114871, "compression_ratio": 1.7076923076923076, "no_speech_prob": 1.0952940101560671e-05}, {"id": 249, "seek": 155814, "start": 1580.18, "end": 1586.5800000000002, "text": " some arbitrary ID. So that's it.", "tokens": [512, 23211, 7348, 13, 407, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.188562881114871, "compression_ratio": 1.7076923076923076, "no_speech_prob": 1.0952940101560671e-05}, {"id": 250, "seek": 158658, "start": 1586.58, "end": 1593.6599999999999, "text": " So now that we have read that in, we can do stuff like, this is interesting, again we", "tokens": [407, 586, 300, 321, 362, 1401, 300, 294, 11, 321, 393, 360, 1507, 411, 11, 341, 307, 1880, 11, 797, 321], "temperature": 0.0, "avg_logprob": -0.1454797196895518, "compression_ratio": 1.6515837104072397, "no_speech_prob": 1.0952987395285163e-05}, {"id": 251, "seek": 158658, "start": 1593.6599999999999, "end": 1599.6599999999999, "text": " have to take the log of the sales, and it's the same reason as we looked at last week,", "tokens": [362, 281, 747, 264, 3565, 295, 264, 5763, 11, 293, 309, 311, 264, 912, 1778, 382, 321, 2956, 412, 1036, 1243, 11], "temperature": 0.0, "avg_logprob": -0.1454797196895518, "compression_ratio": 1.6515837104072397, "no_speech_prob": 1.0952987395285163e-05}, {"id": 252, "seek": 158658, "start": 1599.6599999999999, "end": 1604.26, "text": " because we're trying to predict something that varies according to ratios. They told", "tokens": [570, 321, 434, 1382, 281, 6069, 746, 300, 21716, 4650, 281, 32435, 13, 814, 1907], "temperature": 0.0, "avg_logprob": -0.1454797196895518, "compression_ratio": 1.6515837104072397, "no_speech_prob": 1.0952987395285163e-05}, {"id": 253, "seek": 158658, "start": 1604.26, "end": 1609.74, "text": " us in this competition that the root mean squared log error is the thing they care about,", "tokens": [505, 294, 341, 6211, 300, 264, 5593, 914, 8889, 3565, 6713, 307, 264, 551, 436, 1127, 466, 11], "temperature": 0.0, "avg_logprob": -0.1454797196895518, "compression_ratio": 1.6515837104072397, "no_speech_prob": 1.0952987395285163e-05}, {"id": 254, "seek": 158658, "start": 1609.74, "end": 1611.6999999999998, "text": " so we take a log.", "tokens": [370, 321, 747, 257, 3565, 13], "temperature": 0.0, "avg_logprob": -0.1454797196895518, "compression_ratio": 1.6515837104072397, "no_speech_prob": 1.0952987395285163e-05}, {"id": 255, "seek": 161170, "start": 1611.7, "end": 1616.94, "text": " They mentioned also if you check the competition details, which you always should read carefully", "tokens": [814, 2835, 611, 498, 291, 1520, 264, 6211, 4365, 11, 597, 291, 1009, 820, 1401, 7500], "temperature": 0.0, "avg_logprob": -0.1616739562795132, "compression_ratio": 1.6543778801843319, "no_speech_prob": 7.296348940144526e-06}, {"id": 256, "seek": 161170, "start": 1616.94, "end": 1621.74, "text": " the definition of any project you do, they say that there are some negative sales that", "tokens": [264, 7123, 295, 604, 1716, 291, 360, 11, 436, 584, 300, 456, 366, 512, 3671, 5763, 300], "temperature": 0.0, "avg_logprob": -0.1616739562795132, "compression_ratio": 1.6543778801843319, "no_speech_prob": 7.296348940144526e-06}, {"id": 257, "seek": 161170, "start": 1621.74, "end": 1627.42, "text": " represent returns, and they tell us that we should consider them to be 0 for the purpose", "tokens": [2906, 11247, 11, 293, 436, 980, 505, 300, 321, 820, 1949, 552, 281, 312, 1958, 337, 264, 4334], "temperature": 0.0, "avg_logprob": -0.1616739562795132, "compression_ratio": 1.6543778801843319, "no_speech_prob": 7.296348940144526e-06}, {"id": 258, "seek": 161170, "start": 1627.42, "end": 1635.5, "text": " of this competition. So I clip the sales so that they fall between 0 and no particular", "tokens": [295, 341, 6211, 13, 407, 286, 7353, 264, 5763, 370, 300, 436, 2100, 1296, 1958, 293, 572, 1729], "temperature": 0.0, "avg_logprob": -0.1616739562795132, "compression_ratio": 1.6543778801843319, "no_speech_prob": 7.296348940144526e-06}, {"id": 259, "seek": 163550, "start": 1635.5, "end": 1641.9, "text": " maximum. So clip just means cut it off at that point, truncate it, and then take the", "tokens": [6674, 13, 407, 7353, 445, 1355, 1723, 309, 766, 412, 300, 935, 11, 504, 409, 66, 473, 309, 11, 293, 550, 747, 264], "temperature": 0.0, "avg_logprob": -0.162478274390811, "compression_ratio": 1.7053571428571428, "no_speech_prob": 4.7850676310190465e-06}, {"id": 260, "seek": 163550, "start": 1641.9, "end": 1648.62, "text": " log of that plus 1. Why do I do plus 1? Because again, if you check the details of the competition,", "tokens": [3565, 295, 300, 1804, 502, 13, 1545, 360, 286, 360, 1804, 502, 30, 1436, 797, 11, 498, 291, 1520, 264, 4365, 295, 264, 6211, 11], "temperature": 0.0, "avg_logprob": -0.162478274390811, "compression_ratio": 1.7053571428571428, "no_speech_prob": 4.7850676310190465e-06}, {"id": 261, "seek": 163550, "start": 1648.62, "end": 1651.34, "text": " that's what they tell you they're going to use, is they're not actually just taking the", "tokens": [300, 311, 437, 436, 980, 291, 436, 434, 516, 281, 764, 11, 307, 436, 434, 406, 767, 445, 1940, 264], "temperature": 0.0, "avg_logprob": -0.162478274390811, "compression_ratio": 1.7053571428571428, "no_speech_prob": 4.7850676310190465e-06}, {"id": 262, "seek": 163550, "start": 1651.34, "end": 1658.02, "text": " root mean squared log error, but the root mean squared log plus 1 error. Because log", "tokens": [5593, 914, 8889, 3565, 6713, 11, 457, 264, 5593, 914, 8889, 3565, 1804, 502, 6713, 13, 1436, 3565], "temperature": 0.0, "avg_logprob": -0.162478274390811, "compression_ratio": 1.7053571428571428, "no_speech_prob": 4.7850676310190465e-06}, {"id": 263, "seek": 163550, "start": 1658.02, "end": 1661.54, "text": " of 0 doesn't make sense.", "tokens": [295, 1958, 1177, 380, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.162478274390811, "compression_ratio": 1.7053571428571428, "no_speech_prob": 4.7850676310190465e-06}, {"id": 264, "seek": 166154, "start": 1661.54, "end": 1667.86, "text": " We can add the date part as usual, and again, it's taking a couple of minutes. So I would", "tokens": [492, 393, 909, 264, 4002, 644, 382, 7713, 11, 293, 797, 11, 309, 311, 1940, 257, 1916, 295, 2077, 13, 407, 286, 576], "temperature": 0.0, "avg_logprob": -0.16707611083984375, "compression_ratio": 1.6, "no_speech_prob": 3.446567916398635e-06}, {"id": 265, "seek": 166154, "start": 1667.86, "end": 1672.26, "text": " run through all this on a sample first, so everything takes 10 seconds to make sure it", "tokens": [1190, 807, 439, 341, 322, 257, 6889, 700, 11, 370, 1203, 2516, 1266, 3949, 281, 652, 988, 309], "temperature": 0.0, "avg_logprob": -0.16707611083984375, "compression_ratio": 1.6, "no_speech_prob": 3.446567916398635e-06}, {"id": 266, "seek": 166154, "start": 1672.26, "end": 1675.74, "text": " works, just to check everything looks reasonable before I go back, because I don't want to", "tokens": [1985, 11, 445, 281, 1520, 1203, 1542, 10585, 949, 286, 352, 646, 11, 570, 286, 500, 380, 528, 281], "temperature": 0.0, "avg_logprob": -0.16707611083984375, "compression_ratio": 1.6, "no_speech_prob": 3.446567916398635e-06}, {"id": 267, "seek": 166154, "start": 1675.74, "end": 1680.86, "text": " wait 2 minutes for something I don't know is going to work. But as you can see, all", "tokens": [1699, 568, 2077, 337, 746, 286, 500, 380, 458, 307, 516, 281, 589, 13, 583, 382, 291, 393, 536, 11, 439], "temperature": 0.0, "avg_logprob": -0.16707611083984375, "compression_ratio": 1.6, "no_speech_prob": 3.446567916398635e-06}, {"id": 268, "seek": 166154, "start": 1680.86, "end": 1687.1399999999999, "text": " these lines of code are identical to what we saw for the bulldozers competition.", "tokens": [613, 3876, 295, 3089, 366, 14800, 281, 437, 321, 1866, 337, 264, 4693, 2595, 41698, 6211, 13], "temperature": 0.0, "avg_logprob": -0.16707611083984375, "compression_ratio": 1.6, "no_speech_prob": 3.446567916398635e-06}, {"id": 269, "seek": 168714, "start": 1687.14, "end": 1692.22, "text": " In this case, all I'm reading in is the training set. I didn't need to run train cats because", "tokens": [682, 341, 1389, 11, 439, 286, 478, 3760, 294, 307, 264, 3097, 992, 13, 286, 994, 380, 643, 281, 1190, 3847, 11111, 570], "temperature": 0.0, "avg_logprob": -0.15868623205955992, "compression_ratio": 1.7635467980295567, "no_speech_prob": 6.540397862409009e-06}, {"id": 270, "seek": 168714, "start": 1692.22, "end": 1699.9, "text": " all of my data types are already numeric. If they weren't, I would need to call train cats,", "tokens": [439, 295, 452, 1412, 3467, 366, 1217, 7866, 299, 13, 759, 436, 4999, 380, 11, 286, 576, 643, 281, 818, 3847, 11111, 11], "temperature": 0.0, "avg_logprob": -0.15868623205955992, "compression_ratio": 1.7635467980295567, "no_speech_prob": 6.540397862409009e-06}, {"id": 271, "seek": 168714, "start": 1699.9, "end": 1704.66, "text": " and then I would need to call apply cats to apply the same categorical codes that I now", "tokens": [293, 550, 286, 576, 643, 281, 818, 3079, 11111, 281, 3079, 264, 912, 19250, 804, 14211, 300, 286, 586], "temperature": 0.0, "avg_logprob": -0.15868623205955992, "compression_ratio": 1.7635467980295567, "no_speech_prob": 6.540397862409009e-06}, {"id": 272, "seek": 168714, "start": 1704.66, "end": 1714.66, "text": " have in the training set to the validation set. I call propdf as before to check for", "tokens": [362, 294, 264, 3097, 992, 281, 264, 24071, 992, 13, 286, 818, 2365, 45953, 382, 949, 281, 1520, 337], "temperature": 0.0, "avg_logprob": -0.15868623205955992, "compression_ratio": 1.7635467980295567, "no_speech_prob": 6.540397862409009e-06}, {"id": 273, "seek": 171466, "start": 1714.66, "end": 1720.94, "text": " missing values and so forth. So all of those lines of code are identical. These lines of", "tokens": [5361, 4190, 293, 370, 5220, 13, 407, 439, 295, 729, 3876, 295, 3089, 366, 14800, 13, 1981, 3876, 295], "temperature": 0.0, "avg_logprob": -0.16850991195507264, "compression_ratio": 1.550660792951542, "no_speech_prob": 3.138129386570654e-06}, {"id": 274, "seek": 171466, "start": 1720.94, "end": 1728.5400000000002, "text": " code again are identical because root mean squared error is still all we care about.", "tokens": [3089, 797, 366, 14800, 570, 5593, 914, 8889, 6713, 307, 920, 439, 321, 1127, 466, 13], "temperature": 0.0, "avg_logprob": -0.16850991195507264, "compression_ratio": 1.550660792951542, "no_speech_prob": 3.138129386570654e-06}, {"id": 275, "seek": 171466, "start": 1728.5400000000002, "end": 1734.38, "text": " And then I've got 2 changes. The first is setRFSamples, which we learned about last", "tokens": [400, 550, 286, 600, 658, 568, 2962, 13, 440, 700, 307, 992, 49, 37, 28743, 2622, 11, 597, 321, 3264, 466, 1036], "temperature": 0.0, "avg_logprob": -0.16850991195507264, "compression_ratio": 1.550660792951542, "no_speech_prob": 3.138129386570654e-06}, {"id": 276, "seek": 171466, "start": 1734.38, "end": 1741.8600000000001, "text": " week. So we've got 120-something million records. We probably don't want to create a tree from", "tokens": [1243, 13, 407, 321, 600, 658, 10411, 12, 31681, 2459, 7724, 13, 492, 1391, 500, 380, 528, 281, 1884, 257, 4230, 490], "temperature": 0.0, "avg_logprob": -0.16850991195507264, "compression_ratio": 1.550660792951542, "no_speech_prob": 3.138129386570654e-06}, {"id": 277, "seek": 174186, "start": 1741.86, "end": 1747.4199999999998, "text": " 120 million records. I don't even know how long that's going to take. I haven't had the", "tokens": [10411, 2459, 7724, 13, 286, 500, 380, 754, 458, 577, 938, 300, 311, 516, 281, 747, 13, 286, 2378, 380, 632, 264], "temperature": 0.0, "avg_logprob": -0.20354008188053052, "compression_ratio": 1.5844748858447488, "no_speech_prob": 7.889210792200174e-06}, {"id": 278, "seek": 174186, "start": 1747.4199999999998, "end": 1756.1, "text": " time and patience to wait and see. So you could start with 10,000 or 100,000, maybe", "tokens": [565, 293, 14826, 281, 1699, 293, 536, 13, 407, 291, 727, 722, 365, 1266, 11, 1360, 420, 2319, 11, 1360, 11, 1310], "temperature": 0.0, "avg_logprob": -0.20354008188053052, "compression_ratio": 1.5844748858447488, "no_speech_prob": 7.889210792200174e-06}, {"id": 279, "seek": 174186, "start": 1756.1, "end": 1760.5, "text": " it runs in a few seconds, make sure it works, and you can figure out how much you can run.", "tokens": [309, 6676, 294, 257, 1326, 3949, 11, 652, 988, 309, 1985, 11, 293, 291, 393, 2573, 484, 577, 709, 291, 393, 1190, 13], "temperature": 0.0, "avg_logprob": -0.20354008188053052, "compression_ratio": 1.5844748858447488, "no_speech_prob": 7.889210792200174e-06}, {"id": 280, "seek": 174186, "start": 1760.5, "end": 1767.3799999999999, "text": " And so I found getting it to a million, it runs in under a minute. So the point here", "tokens": [400, 370, 286, 1352, 1242, 309, 281, 257, 2459, 11, 309, 6676, 294, 833, 257, 3456, 13, 407, 264, 935, 510], "temperature": 0.0, "avg_logprob": -0.20354008188053052, "compression_ratio": 1.5844748858447488, "no_speech_prob": 7.889210792200174e-06}, {"id": 281, "seek": 176738, "start": 1767.38, "end": 1772.5, "text": " is there's no relationship between the size of the data set and how long it takes to build", "tokens": [307, 456, 311, 572, 2480, 1296, 264, 2744, 295, 264, 1412, 992, 293, 577, 938, 309, 2516, 281, 1322], "temperature": 0.0, "avg_logprob": -0.27117808359973833, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.1300739970465656e-05}, {"id": 282, "seek": 176738, "start": 1772.5, "end": 1777.5400000000002, "text": " a random forest. The relationship is between the number of estimators multiplied by the", "tokens": [257, 4974, 6719, 13, 440, 2480, 307, 1296, 264, 1230, 295, 8017, 3391, 17207, 538, 264], "temperature": 0.0, "avg_logprob": -0.27117808359973833, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.1300739970465656e-05}, {"id": 283, "seek": 176738, "start": 1777.5400000000002, "end": 1778.5400000000002, "text": " sample size.", "tokens": [6889, 2744, 13], "temperature": 0.0, "avg_logprob": -0.27117808359973833, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.1300739970465656e-05}, {"id": 284, "seek": 176738, "start": 1778.5400000000002, "end": 1779.5400000000002, "text": " Question from the audience.", "tokens": [14464, 490, 264, 4034, 13], "temperature": 0.0, "avg_logprob": -0.27117808359973833, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.1300739970465656e-05}, {"id": 285, "seek": 176738, "start": 1779.5400000000002, "end": 1787.0600000000002, "text": " Just curious what n jobs is, because in the past it's always been negative 1, you made", "tokens": [1449, 6369, 437, 297, 4782, 307, 11, 570, 294, 264, 1791, 309, 311, 1009, 668, 3671, 502, 11, 291, 1027], "temperature": 0.0, "avg_logprob": -0.27117808359973833, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.1300739970465656e-05}, {"id": 286, "seek": 176738, "start": 1787.0600000000002, "end": 1788.0600000000002, "text": " it 8 here.", "tokens": [309, 1649, 510, 13], "temperature": 0.0, "avg_logprob": -0.27117808359973833, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.1300739970465656e-05}, {"id": 287, "seek": 176738, "start": 1788.0600000000002, "end": 1793.74, "text": " Yes, so the number of jobs is the number of cores that it's going to use. And I was running", "tokens": [1079, 11, 370, 264, 1230, 295, 4782, 307, 264, 1230, 295, 24826, 300, 309, 311, 516, 281, 764, 13, 400, 286, 390, 2614], "temperature": 0.0, "avg_logprob": -0.27117808359973833, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.1300739970465656e-05}, {"id": 288, "seek": 179374, "start": 1793.74, "end": 1799.02, "text": " this on a computer that has about 60 cores, and I just found if you try to use all of", "tokens": [341, 322, 257, 3820, 300, 575, 466, 4060, 24826, 11, 293, 286, 445, 1352, 498, 291, 853, 281, 764, 439, 295], "temperature": 0.0, "avg_logprob": -0.22644802131275138, "compression_ratio": 1.5702479338842976, "no_speech_prob": 8.801035619399045e-06}, {"id": 289, "seek": 179374, "start": 1799.02, "end": 1802.6200000000001, "text": " them, it's going to spend so much time picking up jobs, it was a bit slower. So if you've", "tokens": [552, 11, 309, 311, 516, 281, 3496, 370, 709, 565, 8867, 493, 4782, 11, 309, 390, 257, 857, 14009, 13, 407, 498, 291, 600], "temperature": 0.0, "avg_logprob": -0.22644802131275138, "compression_ratio": 1.5702479338842976, "no_speech_prob": 8.801035619399045e-06}, {"id": 290, "seek": 179374, "start": 1802.6200000000001, "end": 1808.42, "text": " got lots and lots of cores on your computer, sometimes you want less than. Negative 1 means", "tokens": [658, 3195, 293, 3195, 295, 24826, 322, 428, 3820, 11, 2171, 291, 528, 1570, 813, 13, 43230, 502, 1355], "temperature": 0.0, "avg_logprob": -0.22644802131275138, "compression_ratio": 1.5702479338842976, "no_speech_prob": 8.801035619399045e-06}, {"id": 291, "seek": 179374, "start": 1808.42, "end": 1812.54, "text": " use every single core.", "tokens": [764, 633, 2167, 4965, 13], "temperature": 0.0, "avg_logprob": -0.22644802131275138, "compression_ratio": 1.5702479338842976, "no_speech_prob": 8.801035619399045e-06}, {"id": 292, "seek": 179374, "start": 1812.54, "end": 1818.06, "text": " There's one more change I made, which is that I converted the data frame into an array of", "tokens": [821, 311, 472, 544, 1319, 286, 1027, 11, 597, 307, 300, 286, 16424, 264, 1412, 3920, 666, 364, 10225, 295], "temperature": 0.0, "avg_logprob": -0.22644802131275138, "compression_ratio": 1.5702479338842976, "no_speech_prob": 8.801035619399045e-06}, {"id": 293, "seek": 181806, "start": 1818.06, "end": 1825.6599999999999, "text": " floats and then I fitted on that. Why did I do that? Because internally, inside the", "tokens": [37878, 293, 550, 286, 26321, 322, 300, 13, 1545, 630, 286, 360, 300, 30, 1436, 19501, 11, 1854, 264], "temperature": 0.0, "avg_logprob": -0.15951177236196157, "compression_ratio": 1.5240641711229947, "no_speech_prob": 2.521565647839452e-06}, {"id": 294, "seek": 181806, "start": 1825.6599999999999, "end": 1831.46, "text": " random forest code, they do that anyway. And so given that I wanted to run a few different", "tokens": [4974, 6719, 3089, 11, 436, 360, 300, 4033, 13, 400, 370, 2212, 300, 286, 1415, 281, 1190, 257, 1326, 819], "temperature": 0.0, "avg_logprob": -0.15951177236196157, "compression_ratio": 1.5240641711229947, "no_speech_prob": 2.521565647839452e-06}, {"id": 295, "seek": 181806, "start": 1831.46, "end": 1837.82, "text": " random forests with a few different hyper-parameters, by doing it once myself, I saved that minute", "tokens": [4974, 21700, 365, 257, 1326, 819, 9848, 12, 2181, 335, 6202, 11, 538, 884, 309, 1564, 2059, 11, 286, 6624, 300, 3456], "temperature": 0.0, "avg_logprob": -0.15951177236196157, "compression_ratio": 1.5240641711229947, "no_speech_prob": 2.521565647839452e-06}, {"id": 296, "seek": 181806, "start": 1837.82, "end": 1841.06, "text": " 37 seconds.", "tokens": [13435, 3949, 13], "temperature": 0.0, "avg_logprob": -0.15951177236196157, "compression_ratio": 1.5240641711229947, "no_speech_prob": 2.521565647839452e-06}, {"id": 297, "seek": 184106, "start": 1841.06, "end": 1849.7, "text": " So if you run a line of code and it takes quite a long time, so the first time I ran", "tokens": [407, 498, 291, 1190, 257, 1622, 295, 3089, 293, 309, 2516, 1596, 257, 938, 565, 11, 370, 264, 700, 565, 286, 5872], "temperature": 0.0, "avg_logprob": -0.17650330420767907, "compression_ratio": 1.6431924882629108, "no_speech_prob": 3.611969304984086e-06}, {"id": 298, "seek": 184106, "start": 1849.7, "end": 1853.5, "text": " this random forest regressor, it kind of took 2 or 3 minutes, and I thought I don't really", "tokens": [341, 4974, 6719, 1121, 735, 284, 11, 309, 733, 295, 1890, 568, 420, 805, 2077, 11, 293, 286, 1194, 286, 500, 380, 534], "temperature": 0.0, "avg_logprob": -0.17650330420767907, "compression_ratio": 1.6431924882629108, "no_speech_prob": 3.611969304984086e-06}, {"id": 299, "seek": 184106, "start": 1853.5, "end": 1862.7, "text": " want to wait 2 or 3 minutes. You can always add in front of the line of code %prun. And", "tokens": [528, 281, 1699, 568, 420, 805, 2077, 13, 509, 393, 1009, 909, 294, 1868, 295, 264, 1622, 295, 3089, 14189, 79, 12997, 13, 400], "temperature": 0.0, "avg_logprob": -0.17650330420767907, "compression_ratio": 1.6431924882629108, "no_speech_prob": 3.611969304984086e-06}, {"id": 300, "seek": 184106, "start": 1862.7, "end": 1868.4199999999998, "text": " what %prun does is it runs something called a profiler. And what a profiler does is it", "tokens": [437, 14189, 79, 12997, 775, 307, 309, 6676, 746, 1219, 257, 1740, 5441, 13, 400, 437, 257, 1740, 5441, 775, 307, 309], "temperature": 0.0, "avg_logprob": -0.17650330420767907, "compression_ratio": 1.6431924882629108, "no_speech_prob": 3.611969304984086e-06}, {"id": 301, "seek": 186842, "start": 1868.42, "end": 1874.42, "text": " will tell you which lines of code behind the scenes took the most time. In this case, I", "tokens": [486, 980, 291, 597, 3876, 295, 3089, 2261, 264, 8026, 1890, 264, 881, 565, 13, 682, 341, 1389, 11, 286], "temperature": 0.0, "avg_logprob": -0.17887844955712034, "compression_ratio": 1.7881355932203389, "no_speech_prob": 4.565949893731158e-06}, {"id": 302, "seek": 186842, "start": 1874.42, "end": 1879.94, "text": " noticed that there was a line of code inside scikit-learn that was this line of code, and", "tokens": [5694, 300, 456, 390, 257, 1622, 295, 3089, 1854, 2180, 22681, 12, 306, 1083, 300, 390, 341, 1622, 295, 3089, 11, 293], "temperature": 0.0, "avg_logprob": -0.17887844955712034, "compression_ratio": 1.7881355932203389, "no_speech_prob": 4.565949893731158e-06}, {"id": 303, "seek": 186842, "start": 1879.94, "end": 1884.54, "text": " it was taking all the time, nearly all the time. And so I thought I'll do that first,", "tokens": [309, 390, 1940, 439, 264, 565, 11, 6217, 439, 264, 565, 13, 400, 370, 286, 1194, 286, 603, 360, 300, 700, 11], "temperature": 0.0, "avg_logprob": -0.17887844955712034, "compression_ratio": 1.7881355932203389, "no_speech_prob": 4.565949893731158e-06}, {"id": 304, "seek": 186842, "start": 1884.54, "end": 1887.6200000000001, "text": " and then I'll pass in the result and then I won't have to do it again.", "tokens": [293, 550, 286, 603, 1320, 294, 264, 1874, 293, 550, 286, 1582, 380, 362, 281, 360, 309, 797, 13], "temperature": 0.0, "avg_logprob": -0.17887844955712034, "compression_ratio": 1.7881355932203389, "no_speech_prob": 4.565949893731158e-06}, {"id": 305, "seek": 186842, "start": 1887.6200000000001, "end": 1893.6200000000001, "text": " So this thing of looking to see which things is taking up the time is called profiling.", "tokens": [407, 341, 551, 295, 1237, 281, 536, 597, 721, 307, 1940, 493, 264, 565, 307, 1219, 1740, 4883, 13], "temperature": 0.0, "avg_logprob": -0.17887844955712034, "compression_ratio": 1.7881355932203389, "no_speech_prob": 4.565949893731158e-06}, {"id": 306, "seek": 189362, "start": 1893.62, "end": 1898.54, "text": " And in software engineering, it's one of the most important tools you have. Data scientists", "tokens": [400, 294, 4722, 7043, 11, 309, 311, 472, 295, 264, 881, 1021, 3873, 291, 362, 13, 11888, 7708], "temperature": 0.0, "avg_logprob": -0.1259623789319805, "compression_ratio": 1.606060606060606, "no_speech_prob": 9.223369488609023e-06}, {"id": 307, "seek": 189362, "start": 1898.54, "end": 1905.1, "text": " really underappreciate this tool, but you'll find amongst conversations on GitHub issues", "tokens": [534, 833, 1746, 3326, 473, 341, 2290, 11, 457, 291, 603, 915, 12918, 7315, 322, 23331, 2663], "temperature": 0.0, "avg_logprob": -0.1259623789319805, "compression_ratio": 1.606060606060606, "no_speech_prob": 9.223369488609023e-06}, {"id": 308, "seek": 189362, "start": 1905.1, "end": 1909.78, "text": " or on Twitter or whatever amongst the top data scientists, they're sharing and talking", "tokens": [420, 322, 5794, 420, 2035, 12918, 264, 1192, 1412, 7708, 11, 436, 434, 5414, 293, 1417], "temperature": 0.0, "avg_logprob": -0.1259623789319805, "compression_ratio": 1.606060606060606, "no_speech_prob": 9.223369488609023e-06}, {"id": 309, "seek": 189362, "start": 1909.78, "end": 1914.78, "text": " about profiles all the time. And that's how easy it is to get a profile.", "tokens": [466, 23693, 439, 264, 565, 13, 400, 300, 311, 577, 1858, 309, 307, 281, 483, 257, 7964, 13], "temperature": 0.0, "avg_logprob": -0.1259623789319805, "compression_ratio": 1.606060606060606, "no_speech_prob": 9.223369488609023e-06}, {"id": 310, "seek": 189362, "start": 1914.78, "end": 1922.3799999999999, "text": " So for fun, try running prun from time to time on stuff that's taking 10-20 seconds", "tokens": [407, 337, 1019, 11, 853, 2614, 280, 12997, 490, 565, 281, 565, 322, 1507, 300, 311, 1940, 1266, 12, 2009, 3949], "temperature": 0.0, "avg_logprob": -0.1259623789319805, "compression_ratio": 1.606060606060606, "no_speech_prob": 9.223369488609023e-06}, {"id": 311, "seek": 192238, "start": 1922.38, "end": 1928.5800000000002, "text": " and see if you can learn to interpret and use profiler outputs. Even though in this", "tokens": [293, 536, 498, 291, 393, 1466, 281, 7302, 293, 764, 1740, 5441, 23930, 13, 2754, 1673, 294, 341], "temperature": 0.0, "avg_logprob": -0.07469433926521464, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.5446134057128802e-05}, {"id": 312, "seek": 192238, "start": 1928.5800000000002, "end": 1935.5, "text": " case I didn't write this scikit-learn class, I was still able to use the profile to figure", "tokens": [1389, 286, 994, 380, 2464, 341, 2180, 22681, 12, 306, 1083, 1508, 11, 286, 390, 920, 1075, 281, 764, 264, 7964, 281, 2573], "temperature": 0.0, "avg_logprob": -0.07469433926521464, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.5446134057128802e-05}, {"id": 313, "seek": 192238, "start": 1935.5, "end": 1943.0200000000002, "text": " out how to make it run over twice as fast by avoiding recalculating this each time.", "tokens": [484, 577, 281, 652, 309, 1190, 670, 6091, 382, 2370, 538, 20220, 850, 304, 2444, 990, 341, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.07469433926521464, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.5446134057128802e-05}, {"id": 314, "seek": 192238, "start": 1943.0200000000002, "end": 1948.2600000000002, "text": " So in this case, I built my regressor, I decided to use 20 estimators. Something else that", "tokens": [407, 294, 341, 1389, 11, 286, 3094, 452, 1121, 735, 284, 11, 286, 3047, 281, 764, 945, 8017, 3391, 13, 6595, 1646, 300], "temperature": 0.0, "avg_logprob": -0.07469433926521464, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.5446134057128802e-05}, {"id": 315, "seek": 194826, "start": 1948.26, "end": 1954.62, "text": " I noticed in the profiler is that I can't use OOBScore when I use setRF samples, because", "tokens": [286, 5694, 294, 264, 1740, 5441, 307, 300, 286, 393, 380, 764, 422, 46, 8176, 12352, 562, 286, 764, 992, 49, 37, 10938, 11, 570], "temperature": 0.0, "avg_logprob": -0.17937801361083985, "compression_ratio": 1.567099567099567, "no_speech_prob": 7.889216249168385e-06}, {"id": 316, "seek": 194826, "start": 1954.62, "end": 1962.46, "text": " if I do, it's going to use the other 124 million rows to calculate the OOBScore, which is like", "tokens": [498, 286, 360, 11, 309, 311, 516, 281, 764, 264, 661, 2272, 19, 2459, 13241, 281, 8873, 264, 422, 46, 8176, 12352, 11, 597, 307, 411], "temperature": 0.0, "avg_logprob": -0.17937801361083985, "compression_ratio": 1.567099567099567, "no_speech_prob": 7.889216249168385e-06}, {"id": 317, "seek": 194826, "start": 1962.46, "end": 1968.18, "text": " it's still going to take forever. So I may as well have a proper validation set anyway.", "tokens": [309, 311, 920, 516, 281, 747, 5680, 13, 407, 286, 815, 382, 731, 362, 257, 2296, 24071, 992, 4033, 13], "temperature": 0.0, "avg_logprob": -0.17937801361083985, "compression_ratio": 1.567099567099567, "no_speech_prob": 7.889216249168385e-06}, {"id": 318, "seek": 194826, "start": 1968.18, "end": 1973.76, "text": " Besides which, I want a validation set that's the most recent dates rather than is random.", "tokens": [13212, 597, 11, 286, 528, 257, 24071, 992, 300, 311, 264, 881, 5162, 11691, 2831, 813, 307, 4974, 13], "temperature": 0.0, "avg_logprob": -0.17937801361083985, "compression_ratio": 1.567099567099567, "no_speech_prob": 7.889216249168385e-06}, {"id": 319, "seek": 197376, "start": 1973.76, "end": 1981.82, "text": " So if you use setRF samples on a large dataset, don't put the OOBScore parameter in because", "tokens": [407, 498, 291, 764, 992, 49, 37, 10938, 322, 257, 2416, 28872, 11, 500, 380, 829, 264, 422, 46, 8176, 12352, 13075, 294, 570], "temperature": 0.0, "avg_logprob": -0.20070202742950827, "compression_ratio": 1.4271356783919598, "no_speech_prob": 1.8738710423349403e-06}, {"id": 320, "seek": 197376, "start": 1981.82, "end": 1984.34, "text": " it takes forever.", "tokens": [309, 2516, 5680, 13], "temperature": 0.0, "avg_logprob": -0.20070202742950827, "compression_ratio": 1.4271356783919598, "no_speech_prob": 1.8738710423349403e-06}, {"id": 321, "seek": 197376, "start": 1984.34, "end": 1992.8, "text": " So that got me a 0.76 validation root mean squared log error. And then I tried fiddling", "tokens": [407, 300, 658, 385, 257, 1958, 13, 25026, 24071, 5593, 914, 8889, 3565, 6713, 13, 400, 550, 286, 3031, 283, 14273, 1688], "temperature": 0.0, "avg_logprob": -0.20070202742950827, "compression_ratio": 1.4271356783919598, "no_speech_prob": 1.8738710423349403e-06}, {"id": 322, "seek": 197376, "start": 1992.8, "end": 1998.18, "text": " around with different min samples. So if I decrease the min samples from 100 to 10, it", "tokens": [926, 365, 819, 923, 10938, 13, 407, 498, 286, 11514, 264, 923, 10938, 490, 2319, 281, 1266, 11, 309], "temperature": 0.0, "avg_logprob": -0.20070202742950827, "compression_ratio": 1.4271356783919598, "no_speech_prob": 1.8738710423349403e-06}, {"id": 323, "seek": 199818, "start": 1998.18, "end": 2005.9, "text": " took a little bit more time to run as you would expect. And the error went down from", "tokens": [1890, 257, 707, 857, 544, 565, 281, 1190, 382, 291, 576, 2066, 13, 400, 264, 6713, 1437, 760, 490], "temperature": 0.0, "avg_logprob": -0.1956946821097868, "compression_ratio": 1.505050505050505, "no_speech_prob": 4.157354851486161e-06}, {"id": 324, "seek": 199818, "start": 2005.9, "end": 2011.8600000000001, "text": " 76 to 71, so that looked pretty good. So I kept decreasing it down to 3, and that brought", "tokens": [24733, 281, 30942, 11, 370, 300, 2956, 1238, 665, 13, 407, 286, 4305, 23223, 309, 760, 281, 805, 11, 293, 300, 3038], "temperature": 0.0, "avg_logprob": -0.1956946821097868, "compression_ratio": 1.505050505050505, "no_speech_prob": 4.157354851486161e-06}, {"id": 325, "seek": 199818, "start": 2011.8600000000001, "end": 2017.3, "text": " this error down to 0.70. When I decreased it down to 1, it didn't really help. So I", "tokens": [341, 6713, 760, 281, 1958, 13, 5867, 13, 1133, 286, 24436, 309, 760, 281, 502, 11, 309, 994, 380, 534, 854, 13, 407, 286], "temperature": 0.0, "avg_logprob": -0.1956946821097868, "compression_ratio": 1.505050505050505, "no_speech_prob": 4.157354851486161e-06}, {"id": 326, "seek": 199818, "start": 2017.3, "end": 2023.46, "text": " kind of had a reasonable random forest.", "tokens": [733, 295, 632, 257, 10585, 4974, 6719, 13], "temperature": 0.0, "avg_logprob": -0.1956946821097868, "compression_ratio": 1.505050505050505, "no_speech_prob": 4.157354851486161e-06}, {"id": 327, "seek": 202346, "start": 2023.46, "end": 2030.66, "text": " When I say reasonable, though, it's not reasonable in the sense that it does not give a good", "tokens": [1133, 286, 584, 10585, 11, 1673, 11, 309, 311, 406, 10585, 294, 264, 2020, 300, 309, 775, 406, 976, 257, 665], "temperature": 0.0, "avg_logprob": -0.18305097881116364, "compression_ratio": 1.6394849785407726, "no_speech_prob": 8.398025784117635e-06}, {"id": 328, "seek": 202346, "start": 2030.66, "end": 2037.66, "text": " result on the leaderboard. And so this is a very interesting question about why is that.", "tokens": [1874, 322, 264, 5263, 3787, 13, 400, 370, 341, 307, 257, 588, 1880, 1168, 466, 983, 307, 300, 13], "temperature": 0.0, "avg_logprob": -0.18305097881116364, "compression_ratio": 1.6394849785407726, "no_speech_prob": 8.398025784117635e-06}, {"id": 329, "seek": 202346, "start": 2037.66, "end": 2042.82, "text": " The reason is really coming back to Savannah's question earlier, like where might random", "tokens": [440, 1778, 307, 534, 1348, 646, 281, 47902, 311, 1168, 3071, 11, 411, 689, 1062, 4974], "temperature": 0.0, "avg_logprob": -0.18305097881116364, "compression_ratio": 1.6394849785407726, "no_speech_prob": 8.398025784117635e-06}, {"id": 330, "seek": 202346, "start": 2042.82, "end": 2045.3400000000001, "text": " forests not work as well.", "tokens": [21700, 406, 589, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.18305097881116364, "compression_ratio": 1.6394849785407726, "no_speech_prob": 8.398025784117635e-06}, {"id": 331, "seek": 202346, "start": 2045.3400000000001, "end": 2051.54, "text": " Let's go back and look at the data. Here's the entire dataset. Here's all the columns", "tokens": [961, 311, 352, 646, 293, 574, 412, 264, 1412, 13, 1692, 311, 264, 2302, 28872, 13, 1692, 311, 439, 264, 13766], "temperature": 0.0, "avg_logprob": -0.18305097881116364, "compression_ratio": 1.6394849785407726, "no_speech_prob": 8.398025784117635e-06}, {"id": 332, "seek": 205154, "start": 2051.54, "end": 2059.5, "text": " that we used. So the columns that we have to predict with are the date, the store number,", "tokens": [300, 321, 1143, 13, 407, 264, 13766, 300, 321, 362, 281, 6069, 365, 366, 264, 4002, 11, 264, 3531, 1230, 11], "temperature": 0.0, "avg_logprob": -0.23256485636641339, "compression_ratio": 1.6171428571428572, "no_speech_prob": 9.818280886975117e-06}, {"id": 333, "seek": 205154, "start": 2059.5, "end": 2065.34, "text": " the item number, and whether it was on promotion or not. And then of course we used add date", "tokens": [264, 3174, 1230, 11, 293, 1968, 309, 390, 322, 15783, 420, 406, 13, 400, 550, 295, 1164, 321, 1143, 909, 4002], "temperature": 0.0, "avg_logprob": -0.23256485636641339, "compression_ratio": 1.6171428571428572, "no_speech_prob": 9.818280886975117e-06}, {"id": 334, "seek": 205154, "start": 2065.34, "end": 2071.62, "text": " parts, so it's also going to be day of week, day of month, day of year, is quarter, start,", "tokens": [3166, 11, 370, 309, 311, 611, 516, 281, 312, 786, 295, 1243, 11, 786, 295, 1618, 11, 786, 295, 1064, 11, 307, 6555, 11, 722, 11], "temperature": 0.0, "avg_logprob": -0.23256485636641339, "compression_ratio": 1.6171428571428572, "no_speech_prob": 9.818280886975117e-06}, {"id": 335, "seek": 205154, "start": 2071.62, "end": 2073.46, "text": " etc. etc.", "tokens": [5183, 13, 5183, 13], "temperature": 0.0, "avg_logprob": -0.23256485636641339, "compression_ratio": 1.6171428571428572, "no_speech_prob": 9.818280886975117e-06}, {"id": 336, "seek": 207346, "start": 2073.46, "end": 2083.02, "text": " So if you think about it, most of the insight around how much of something do you expect", "tokens": [407, 498, 291, 519, 466, 309, 11, 881, 295, 264, 11269, 926, 577, 709, 295, 746, 360, 291, 2066], "temperature": 0.0, "avg_logprob": -0.12276454454057673, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.9947233340644743e-06}, {"id": 337, "seek": 207346, "start": 2083.02, "end": 2090.06, "text": " to sell tomorrow is likely to be very wrapped up in the details about where is that store,", "tokens": [281, 3607, 4153, 307, 3700, 281, 312, 588, 14226, 493, 294, 264, 4365, 466, 689, 307, 300, 3531, 11], "temperature": 0.0, "avg_logprob": -0.12276454454057673, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.9947233340644743e-06}, {"id": 338, "seek": 207346, "start": 2090.06, "end": 2094.62, "text": " what kind of things do they tend to sell at that store, for that item, what category of", "tokens": [437, 733, 295, 721, 360, 436, 3928, 281, 3607, 412, 300, 3531, 11, 337, 300, 3174, 11, 437, 7719, 295], "temperature": 0.0, "avg_logprob": -0.12276454454057673, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.9947233340644743e-06}, {"id": 339, "seek": 207346, "start": 2094.62, "end": 2102.62, "text": " item is it. If it's like fresh bread, they might not sell much of it on Sundays because", "tokens": [3174, 307, 309, 13, 759, 309, 311, 411, 4451, 5961, 11, 436, 1062, 406, 3607, 709, 295, 309, 322, 44857, 570], "temperature": 0.0, "avg_logprob": -0.12276454454057673, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.9947233340644743e-06}, {"id": 340, "seek": 210262, "start": 2102.62, "end": 2108.42, "text": " on Sundays fresh bread doesn't get made. Where obviously it's gasoline, maybe they're going", "tokens": [322, 44857, 4451, 5961, 1177, 380, 483, 1027, 13, 2305, 2745, 309, 311, 28914, 11, 1310, 436, 434, 516], "temperature": 0.0, "avg_logprob": -0.13583640185269444, "compression_ratio": 1.7595419847328244, "no_speech_prob": 5.594311005552299e-06}, {"id": 341, "seek": 210262, "start": 2108.42, "end": 2113.3399999999997, "text": " to sell a lot of gasoline because on Sundays people go and fill up their cart for the week", "tokens": [281, 3607, 257, 688, 295, 28914, 570, 322, 44857, 561, 352, 293, 2836, 493, 641, 5467, 337, 264, 1243], "temperature": 0.0, "avg_logprob": -0.13583640185269444, "compression_ratio": 1.7595419847328244, "no_speech_prob": 5.594311005552299e-06}, {"id": 342, "seek": 210262, "start": 2113.3399999999997, "end": 2114.3399999999997, "text": " ahead.", "tokens": [2286, 13], "temperature": 0.0, "avg_logprob": -0.13583640185269444, "compression_ratio": 1.7595419847328244, "no_speech_prob": 5.594311005552299e-06}, {"id": 343, "seek": 210262, "start": 2114.3399999999997, "end": 2120.42, "text": " Now a random forest has no ability to do anything other than create a bunch of binary splits", "tokens": [823, 257, 4974, 6719, 575, 572, 3485, 281, 360, 1340, 661, 813, 1884, 257, 3840, 295, 17434, 37741], "temperature": 0.0, "avg_logprob": -0.13583640185269444, "compression_ratio": 1.7595419847328244, "no_speech_prob": 5.594311005552299e-06}, {"id": 344, "seek": 210262, "start": 2120.42, "end": 2126.18, "text": " on things like day of week, store number, item number. It doesn't know which one represents", "tokens": [322, 721, 411, 786, 295, 1243, 11, 3531, 1230, 11, 3174, 1230, 13, 467, 1177, 380, 458, 597, 472, 8855], "temperature": 0.0, "avg_logprob": -0.13583640185269444, "compression_ratio": 1.7595419847328244, "no_speech_prob": 5.594311005552299e-06}, {"id": 345, "seek": 210262, "start": 2126.18, "end": 2131.22, "text": " gasoline. It doesn't know which stores are in the center of the city versus which ones", "tokens": [28914, 13, 467, 1177, 380, 458, 597, 9512, 366, 294, 264, 3056, 295, 264, 2307, 5717, 597, 2306], "temperature": 0.0, "avg_logprob": -0.13583640185269444, "compression_ratio": 1.7595419847328244, "no_speech_prob": 5.594311005552299e-06}, {"id": 346, "seek": 213122, "start": 2131.22, "end": 2139.58, "text": " are out in the states. It doesn't know any of these things. So its ability to really", "tokens": [366, 484, 294, 264, 4368, 13, 467, 1177, 380, 458, 604, 295, 613, 721, 13, 407, 1080, 3485, 281, 534], "temperature": 0.0, "avg_logprob": -0.11795823140577837, "compression_ratio": 1.6536585365853658, "no_speech_prob": 2.0580412183335284e-06}, {"id": 347, "seek": 213122, "start": 2139.58, "end": 2143.58, "text": " understand what's going on is somewhat limited. So we're probably going to need to use the", "tokens": [1223, 437, 311, 516, 322, 307, 8344, 5567, 13, 407, 321, 434, 1391, 516, 281, 643, 281, 764, 264], "temperature": 0.0, "avg_logprob": -0.11795823140577837, "compression_ratio": 1.6536585365853658, "no_speech_prob": 2.0580412183335284e-06}, {"id": 348, "seek": 213122, "start": 2143.58, "end": 2149.54, "text": " entire 4 years of data to even get some useful insights. But then as soon as we start using", "tokens": [2302, 1017, 924, 295, 1412, 281, 754, 483, 512, 4420, 14310, 13, 583, 550, 382, 2321, 382, 321, 722, 1228], "temperature": 0.0, "avg_logprob": -0.11795823140577837, "compression_ratio": 1.6536585365853658, "no_speech_prob": 2.0580412183335284e-06}, {"id": 349, "seek": 213122, "start": 2149.54, "end": 2155.62, "text": " the whole 4 years of data, a lot of the data we're using is really old.", "tokens": [264, 1379, 1017, 924, 295, 1412, 11, 257, 688, 295, 264, 1412, 321, 434, 1228, 307, 534, 1331, 13], "temperature": 0.0, "avg_logprob": -0.11795823140577837, "compression_ratio": 1.6536585365853658, "no_speech_prob": 2.0580412183335284e-06}, {"id": 350, "seek": 215562, "start": 2155.62, "end": 2162.8199999999997, "text": " So interestingly, there's a cable kernel that points out that what you could do is just", "tokens": [407, 25873, 11, 456, 311, 257, 8220, 28256, 300, 2793, 484, 300, 437, 291, 727, 360, 307, 445], "temperature": 0.0, "avg_logprob": -0.163569215627817, "compression_ratio": 1.5527950310559007, "no_speech_prob": 8.013450496946461e-06}, {"id": 351, "seek": 215562, "start": 2162.8199999999997, "end": 2173.38, "text": " take the last 2 weeks and take the average sales by date, by store number, by item number,", "tokens": [747, 264, 1036, 568, 3259, 293, 747, 264, 4274, 5763, 538, 4002, 11, 538, 3531, 1230, 11, 538, 3174, 1230, 11], "temperature": 0.0, "avg_logprob": -0.163569215627817, "compression_ratio": 1.5527950310559007, "no_speech_prob": 8.013450496946461e-06}, {"id": 352, "seek": 215562, "start": 2173.38, "end": 2181.8199999999997, "text": " and just submit that. And if you just submit that, you come about 30th.", "tokens": [293, 445, 10315, 300, 13, 400, 498, 291, 445, 10315, 300, 11, 291, 808, 466, 2217, 392, 13], "temperature": 0.0, "avg_logprob": -0.163569215627817, "compression_ratio": 1.5527950310559007, "no_speech_prob": 8.013450496946461e-06}, {"id": 353, "seek": 218182, "start": 2181.82, "end": 2191.1800000000003, "text": " So for those of you in the grocery store, Terence has a comment or a question.", "tokens": [407, 337, 729, 295, 291, 294, 264, 14410, 3531, 11, 6564, 655, 575, 257, 2871, 420, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.27787880781220226, "compression_ratio": 1.5977011494252873, "no_speech_prob": 2.7968921131105162e-05}, {"id": 354, "seek": 218182, "start": 2191.1800000000003, "end": 2195.44, "text": " I think this may have tripped me up. I think you said date, store, item. I think it's actually", "tokens": [286, 519, 341, 815, 362, 1376, 3320, 385, 493, 13, 286, 519, 291, 848, 4002, 11, 3531, 11, 3174, 13, 286, 519, 309, 311, 767], "temperature": 0.0, "avg_logprob": -0.27787880781220226, "compression_ratio": 1.5977011494252873, "no_speech_prob": 2.7968921131105162e-05}, {"id": 355, "seek": 218182, "start": 2195.44, "end": 2198.6200000000003, "text": " store, item, sales, and then you mean across date.", "tokens": [3531, 11, 3174, 11, 5763, 11, 293, 550, 291, 914, 2108, 4002, 13], "temperature": 0.0, "avg_logprob": -0.27787880781220226, "compression_ratio": 1.5977011494252873, "no_speech_prob": 2.7968921131105162e-05}, {"id": 356, "seek": 218182, "start": 2198.6200000000003, "end": 2205.7400000000002, "text": " Oh yeah, you're right. Store, item, and on promotion.", "tokens": [876, 1338, 11, 291, 434, 558, 13, 17242, 11, 3174, 11, 293, 322, 15783, 13], "temperature": 0.0, "avg_logprob": -0.27787880781220226, "compression_ratio": 1.5977011494252873, "no_speech_prob": 2.7968921131105162e-05}, {"id": 357, "seek": 220574, "start": 2205.74, "end": 2215.02, "text": " If you do it by date as well, each row represents basically like a cross-tabulation of all of", "tokens": [759, 291, 360, 309, 538, 4002, 382, 731, 11, 1184, 5386, 8855, 1936, 411, 257, 3278, 12, 83, 455, 2776, 295, 439, 295], "temperature": 0.0, "avg_logprob": -0.1405430423970125, "compression_ratio": 1.5990990990990992, "no_speech_prob": 1.8162145352107473e-06}, {"id": 358, "seek": 220574, "start": 2215.02, "end": 2219.74, "text": " the sales in that store for that item. So if you put date in there as well, there's", "tokens": [264, 5763, 294, 300, 3531, 337, 300, 3174, 13, 407, 498, 291, 829, 4002, 294, 456, 382, 731, 11, 456, 311], "temperature": 0.0, "avg_logprob": -0.1405430423970125, "compression_ratio": 1.5990990990990992, "no_speech_prob": 1.8162145352107473e-06}, {"id": 359, "seek": 220574, "start": 2219.74, "end": 2226.22, "text": " only going to be 1 or 2 items being averaged in each of those cells, which is too much", "tokens": [787, 516, 281, 312, 502, 420, 568, 4754, 885, 18247, 2980, 294, 1184, 295, 729, 5438, 11, 597, 307, 886, 709], "temperature": 0.0, "avg_logprob": -0.1405430423970125, "compression_ratio": 1.5990990990990992, "no_speech_prob": 1.8162145352107473e-06}, {"id": 360, "seek": 220574, "start": 2226.22, "end": 2234.2999999999997, "text": " variation basically, too sparse. It doesn't give you a terrible result, but it's not 30th.", "tokens": [12990, 1936, 11, 886, 637, 11668, 13, 467, 1177, 380, 976, 291, 257, 6237, 1874, 11, 457, 309, 311, 406, 2217, 392, 13], "temperature": 0.0, "avg_logprob": -0.1405430423970125, "compression_ratio": 1.5990990990990992, "no_speech_prob": 1.8162145352107473e-06}, {"id": 361, "seek": 223430, "start": 2234.3, "end": 2243.38, "text": " So your job if you're looking at this competition, and we'll talk about this in the next class,", "tokens": [407, 428, 1691, 498, 291, 434, 1237, 412, 341, 6211, 11, 293, 321, 603, 751, 466, 341, 294, 264, 958, 1508, 11], "temperature": 0.0, "avg_logprob": -0.15590328640407985, "compression_ratio": 1.4728260869565217, "no_speech_prob": 2.0261354620743077e-06}, {"id": 362, "seek": 223430, "start": 2243.38, "end": 2254.34, "text": " is how do you start with that model and make it a little bit better. Because if you can,", "tokens": [307, 577, 360, 291, 722, 365, 300, 2316, 293, 652, 309, 257, 707, 857, 1101, 13, 1436, 498, 291, 393, 11], "temperature": 0.0, "avg_logprob": -0.15590328640407985, "compression_ratio": 1.4728260869565217, "no_speech_prob": 2.0261354620743077e-06}, {"id": 363, "seek": 223430, "start": 2254.34, "end": 2260.36, "text": " then by the time we meet up next, hopefully you'll be above the top 30. Because Kaggle", "tokens": [550, 538, 264, 565, 321, 1677, 493, 958, 11, 4696, 291, 603, 312, 3673, 264, 1192, 2217, 13, 1436, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.15590328640407985, "compression_ratio": 1.4728260869565217, "no_speech_prob": 2.0261354620743077e-06}, {"id": 364, "seek": 226036, "start": 2260.36, "end": 2264.7000000000003, "text": " being Kaggle, lots of people have now taken this kernel and submitted it, and they all", "tokens": [885, 48751, 22631, 11, 3195, 295, 561, 362, 586, 2726, 341, 28256, 293, 14405, 309, 11, 293, 436, 439], "temperature": 0.0, "avg_logprob": -0.16824195659266108, "compression_ratio": 1.778688524590164, "no_speech_prob": 3.446522214289871e-06}, {"id": 365, "seek": 226036, "start": 2264.7000000000003, "end": 2269.7400000000002, "text": " have about the same score. And the scores are ordered not just by score, but by date", "tokens": [362, 466, 264, 912, 6175, 13, 400, 264, 13444, 366, 8866, 406, 445, 538, 6175, 11, 457, 538, 4002], "temperature": 0.0, "avg_logprob": -0.16824195659266108, "compression_ratio": 1.778688524590164, "no_speech_prob": 3.446522214289871e-06}, {"id": 366, "seek": 226036, "start": 2269.7400000000002, "end": 2274.2000000000003, "text": " submitted. So if you now submit this kernel, you're not going to be 30th because you're", "tokens": [14405, 13, 407, 498, 291, 586, 10315, 341, 28256, 11, 291, 434, 406, 516, 281, 312, 2217, 392, 570, 291, 434], "temperature": 0.0, "avg_logprob": -0.16824195659266108, "compression_ratio": 1.778688524590164, "no_speech_prob": 3.446522214289871e-06}, {"id": 367, "seek": 226036, "start": 2274.2000000000003, "end": 2279.82, "text": " way down the list of when it was submitted. But if you can do a tiny bit better, you're", "tokens": [636, 760, 264, 1329, 295, 562, 309, 390, 14405, 13, 583, 498, 291, 393, 360, 257, 5870, 857, 1101, 11, 291, 434], "temperature": 0.0, "avg_logprob": -0.16824195659266108, "compression_ratio": 1.778688524590164, "no_speech_prob": 3.446522214289871e-06}, {"id": 368, "seek": 226036, "start": 2279.82, "end": 2283.5, "text": " going to be better than all of those people. So try and think of how can you make this", "tokens": [516, 281, 312, 1101, 813, 439, 295, 729, 561, 13, 407, 853, 293, 519, 295, 577, 393, 291, 652, 341], "temperature": 0.0, "avg_logprob": -0.16824195659266108, "compression_ratio": 1.778688524590164, "no_speech_prob": 3.446522214289871e-06}, {"id": 369, "seek": 228350, "start": 2283.5, "end": 2290.5, "text": " a tiny bit better.", "tokens": [257, 5870, 857, 1101, 13], "temperature": 0.0, "avg_logprob": -0.22627787070699257, "compression_ratio": 1.6329113924050633, "no_speech_prob": 1.56893420353299e-05}, {"id": 370, "seek": 228350, "start": 2290.5, "end": 2294.66, "text": " Question. Could you try to capture seasonality and trend effects by creating new columns", "tokens": [14464, 13, 7497, 291, 853, 281, 7983, 3196, 1860, 293, 6028, 5065, 538, 4084, 777, 13766], "temperature": 0.0, "avg_logprob": -0.22627787070699257, "compression_ratio": 1.6329113924050633, "no_speech_prob": 1.56893420353299e-05}, {"id": 371, "seek": 228350, "start": 2294.66, "end": 2298.34, "text": " like these are the average sales in the month of August, these are the average sales for", "tokens": [411, 613, 366, 264, 4274, 5763, 294, 264, 1618, 295, 6897, 11, 613, 366, 264, 4274, 5763, 337], "temperature": 0.0, "avg_logprob": -0.22627787070699257, "compression_ratio": 1.6329113924050633, "no_speech_prob": 1.56893420353299e-05}, {"id": 372, "seek": 228350, "start": 2298.34, "end": 2299.34, "text": " this year?", "tokens": [341, 1064, 30], "temperature": 0.0, "avg_logprob": -0.22627787070699257, "compression_ratio": 1.6329113924050633, "no_speech_prob": 1.56893420353299e-05}, {"id": 373, "seek": 228350, "start": 2299.34, "end": 2304.06, "text": " Answer. Yeah, I think that's a great idea. So the thing for you to think about is how", "tokens": [24545, 13, 865, 11, 286, 519, 300, 311, 257, 869, 1558, 13, 407, 264, 551, 337, 291, 281, 519, 466, 307, 577], "temperature": 0.0, "avg_logprob": -0.22627787070699257, "compression_ratio": 1.6329113924050633, "no_speech_prob": 1.56893420353299e-05}, {"id": 374, "seek": 228350, "start": 2304.06, "end": 2312.06, "text": " to do that. See if you can make it work. Because there are details to get right, which I know", "tokens": [281, 360, 300, 13, 3008, 498, 291, 393, 652, 309, 589, 13, 1436, 456, 366, 4365, 281, 483, 558, 11, 597, 286, 458], "temperature": 0.0, "avg_logprob": -0.22627787070699257, "compression_ratio": 1.6329113924050633, "no_speech_prob": 1.56893420353299e-05}, {"id": 375, "seek": 231206, "start": 2312.06, "end": 2318.46, "text": " Terrence has been working on this for the last week and he's gone almost crazy. The", "tokens": [6564, 10760, 575, 668, 1364, 322, 341, 337, 264, 1036, 1243, 293, 415, 311, 2780, 1920, 3219, 13, 440], "temperature": 0.0, "avg_logprob": -0.24754849458352113, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.3552358470624313e-05}, {"id": 376, "seek": 231206, "start": 2318.46, "end": 2326.94, "text": " details are difficult. They're not intellectually difficult, they're kind of difficult in the", "tokens": [4365, 366, 2252, 13, 814, 434, 406, 46481, 2252, 11, 436, 434, 733, 295, 2252, 294, 264], "temperature": 0.0, "avg_logprob": -0.24754849458352113, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.3552358470624313e-05}, {"id": 377, "seek": 231206, "start": 2326.94, "end": 2331.06, "text": " way that makes you want to headbutt your desk at 2am.", "tokens": [636, 300, 1669, 291, 528, 281, 1378, 5955, 83, 428, 10026, 412, 568, 335, 13], "temperature": 0.0, "avg_logprob": -0.24754849458352113, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.3552358470624313e-05}, {"id": 378, "seek": 231206, "start": 2331.06, "end": 2341.74, "text": " This is something to mention in general. The coding you do for machine learning is like", "tokens": [639, 307, 746, 281, 2152, 294, 2674, 13, 440, 17720, 291, 360, 337, 3479, 2539, 307, 411], "temperature": 0.0, "avg_logprob": -0.24754849458352113, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.3552358470624313e-05}, {"id": 379, "seek": 234174, "start": 2341.74, "end": 2347.06, "text": " it's incredibly frustrating and incredibly difficult. Not difficult technically, but", "tokens": [309, 311, 6252, 16522, 293, 6252, 2252, 13, 1726, 2252, 12120, 11, 457], "temperature": 0.0, "avg_logprob": -0.15960382782252489, "compression_ratio": 1.7283464566929134, "no_speech_prob": 1.5446206816704944e-05}, {"id": 380, "seek": 234174, "start": 2347.06, "end": 2353.54, "text": " difficult. If you get a detail wrong, much of the time it's not going to give you an", "tokens": [2252, 13, 759, 291, 483, 257, 2607, 2085, 11, 709, 295, 264, 565, 309, 311, 406, 516, 281, 976, 291, 364], "temperature": 0.0, "avg_logprob": -0.15960382782252489, "compression_ratio": 1.7283464566929134, "no_speech_prob": 1.5446206816704944e-05}, {"id": 381, "seek": 234174, "start": 2353.54, "end": 2359.22, "text": " exception. It'll just silently be slightly less good than it otherwise would have been.", "tokens": [11183, 13, 467, 603, 445, 40087, 312, 4748, 1570, 665, 813, 309, 5911, 576, 362, 668, 13], "temperature": 0.0, "avg_logprob": -0.15960382782252489, "compression_ratio": 1.7283464566929134, "no_speech_prob": 1.5446206816704944e-05}, {"id": 382, "seek": 234174, "start": 2359.22, "end": 2364.2999999999997, "text": " And if you're on Kaggle, at least you know, I'm not doing as well as other people on Kaggle.", "tokens": [400, 498, 291, 434, 322, 48751, 22631, 11, 412, 1935, 291, 458, 11, 286, 478, 406, 884, 382, 731, 382, 661, 561, 322, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.15960382782252489, "compression_ratio": 1.7283464566929134, "no_speech_prob": 1.5446206816704944e-05}, {"id": 383, "seek": 234174, "start": 2364.2999999999997, "end": 2370.14, "text": " But if you're not on Kaggle, you just don't know. You don't know if your company's model", "tokens": [583, 498, 291, 434, 406, 322, 48751, 22631, 11, 291, 445, 500, 380, 458, 13, 509, 500, 380, 458, 498, 428, 2237, 311, 2316], "temperature": 0.0, "avg_logprob": -0.15960382782252489, "compression_ratio": 1.7283464566929134, "no_speech_prob": 1.5446206816704944e-05}, {"id": 384, "seek": 237014, "start": 2370.14, "end": 2376.3799999999997, "text": " is half as good as it could be because you made a little mistake. That's one of the reasons", "tokens": [307, 1922, 382, 665, 382, 309, 727, 312, 570, 291, 1027, 257, 707, 6146, 13, 663, 311, 472, 295, 264, 4112], "temperature": 0.0, "avg_logprob": -0.141270067090186, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.723136483633425e-05}, {"id": 385, "seek": 237014, "start": 2376.3799999999997, "end": 2381.2599999999998, "text": " why practicing on Kaggle now is great. You're going to get practice in finding all of the", "tokens": [983, 11350, 322, 48751, 22631, 586, 307, 869, 13, 509, 434, 516, 281, 483, 3124, 294, 5006, 439, 295, 264], "temperature": 0.0, "avg_logprob": -0.141270067090186, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.723136483633425e-05}, {"id": 386, "seek": 237014, "start": 2381.2599999999998, "end": 2387.8599999999997, "text": " ways in which you can infuriatingly screw things up. And you'll be amazed. For me, there's", "tokens": [2098, 294, 597, 291, 393, 1536, 9744, 990, 356, 5630, 721, 493, 13, 400, 291, 603, 312, 20507, 13, 1171, 385, 11, 456, 311], "temperature": 0.0, "avg_logprob": -0.141270067090186, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.723136483633425e-05}, {"id": 387, "seek": 237014, "start": 2387.8599999999997, "end": 2392.7, "text": " an extraordinary array of them. But as you get to know what they are, you'll start to", "tokens": [364, 10581, 10225, 295, 552, 13, 583, 382, 291, 483, 281, 458, 437, 436, 366, 11, 291, 603, 722, 281], "temperature": 0.0, "avg_logprob": -0.141270067090186, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.723136483633425e-05}, {"id": 388, "seek": 237014, "start": 2392.7, "end": 2398.1, "text": " know how to check for them as you go.", "tokens": [458, 577, 281, 1520, 337, 552, 382, 291, 352, 13], "temperature": 0.0, "avg_logprob": -0.141270067090186, "compression_ratio": 1.6097560975609757, "no_speech_prob": 1.723136483633425e-05}, {"id": 389, "seek": 239810, "start": 2398.1, "end": 2402.62, "text": " You should assume every button you press, you're going to press the wrong button. And", "tokens": [509, 820, 6552, 633, 2960, 291, 1886, 11, 291, 434, 516, 281, 1886, 264, 2085, 2960, 13, 400], "temperature": 0.0, "avg_logprob": -0.1444943245877041, "compression_ratio": 1.5963302752293578, "no_speech_prob": 2.3923052140162326e-05}, {"id": 390, "seek": 239810, "start": 2402.62, "end": 2412.14, "text": " that's fine as long as you have a way to find out. We'll talk about that more during the", "tokens": [300, 311, 2489, 382, 938, 382, 291, 362, 257, 636, 281, 915, 484, 13, 492, 603, 751, 466, 300, 544, 1830, 264], "temperature": 0.0, "avg_logprob": -0.1444943245877041, "compression_ratio": 1.5963302752293578, "no_speech_prob": 2.3923052140162326e-05}, {"id": 391, "seek": 239810, "start": 2412.14, "end": 2417.98, "text": " course, but unfortunately there isn't a set of specific things I can tell you to always", "tokens": [1164, 11, 457, 7015, 456, 1943, 380, 257, 992, 295, 2685, 721, 286, 393, 980, 291, 281, 1009], "temperature": 0.0, "avg_logprob": -0.1444943245877041, "compression_ratio": 1.5963302752293578, "no_speech_prob": 2.3923052140162326e-05}, {"id": 392, "seek": 239810, "start": 2417.98, "end": 2424.5, "text": " do. You just always have to think, what do I know about the results of this thing I'm", "tokens": [360, 13, 509, 445, 1009, 362, 281, 519, 11, 437, 360, 286, 458, 466, 264, 3542, 295, 341, 551, 286, 478], "temperature": 0.0, "avg_logprob": -0.1444943245877041, "compression_ratio": 1.5963302752293578, "no_speech_prob": 2.3923052140162326e-05}, {"id": 393, "seek": 242450, "start": 2424.5, "end": 2431.78, "text": " about to do. I'll give you a really simple example. If you've actually created that basic", "tokens": [466, 281, 360, 13, 286, 603, 976, 291, 257, 534, 2199, 1365, 13, 759, 291, 600, 767, 2942, 300, 3875], "temperature": 0.0, "avg_logprob": -0.1572850834239613, "compression_ratio": 1.6854460093896713, "no_speech_prob": 6.962201041460503e-06}, {"id": 394, "seek": 242450, "start": 2431.78, "end": 2438.06, "text": " entry where you take the mean by date by store number by on promotion, submit it, and you've", "tokens": [8729, 689, 291, 747, 264, 914, 538, 4002, 538, 3531, 1230, 538, 322, 15783, 11, 10315, 309, 11, 293, 291, 600], "temperature": 0.0, "avg_logprob": -0.1572850834239613, "compression_ratio": 1.6854460093896713, "no_speech_prob": 6.962201041460503e-06}, {"id": 395, "seek": 242450, "start": 2438.06, "end": 2443.66, "text": " got a reasonable score, and then you think you've got something that's a little bit better,", "tokens": [658, 257, 10585, 6175, 11, 293, 550, 291, 519, 291, 600, 658, 746, 300, 311, 257, 707, 857, 1101, 11], "temperature": 0.0, "avg_logprob": -0.1572850834239613, "compression_ratio": 1.6854460093896713, "no_speech_prob": 6.962201041460503e-06}, {"id": 396, "seek": 242450, "start": 2443.66, "end": 2449.46, "text": " and you do predictions for that, how about you now create a scatter plot showing the", "tokens": [293, 291, 360, 21264, 337, 300, 11, 577, 466, 291, 586, 1884, 257, 34951, 7542, 4099, 264], "temperature": 0.0, "avg_logprob": -0.1572850834239613, "compression_ratio": 1.6854460093896713, "no_speech_prob": 6.962201041460503e-06}, {"id": 397, "seek": 244946, "start": 2449.46, "end": 2454.86, "text": " predictions of your average model on one axis versus the predictions of your new model on", "tokens": [21264, 295, 428, 4274, 2316, 322, 472, 10298, 5717, 264, 21264, 295, 428, 777, 2316, 322], "temperature": 0.0, "avg_logprob": -0.18230561078605007, "compression_ratio": 1.5443037974683544, "no_speech_prob": 4.356859790277667e-06}, {"id": 398, "seek": 244946, "start": 2454.86, "end": 2462.82, "text": " the other axis. You should see that they just about form a line. And if they don't, then", "tokens": [264, 661, 10298, 13, 509, 820, 536, 300, 436, 445, 466, 1254, 257, 1622, 13, 400, 498, 436, 500, 380, 11, 550], "temperature": 0.0, "avg_logprob": -0.18230561078605007, "compression_ratio": 1.5443037974683544, "no_speech_prob": 4.356859790277667e-06}, {"id": 399, "seek": 244946, "start": 2462.82, "end": 2466.66, "text": " that's a very strong suggestion that you've screwed something up.", "tokens": [300, 311, 257, 588, 2068, 16541, 300, 291, 600, 20331, 746, 493, 13], "temperature": 0.0, "avg_logprob": -0.18230561078605007, "compression_ratio": 1.5443037974683544, "no_speech_prob": 4.356859790277667e-06}, {"id": 400, "seek": 246666, "start": 2466.66, "end": 2488.5, "text": " For a problem like this, unlike the car insurance problem on Kaggle where columns are unnamed,", "tokens": [1171, 257, 1154, 411, 341, 11, 8343, 264, 1032, 7214, 1154, 322, 48751, 22631, 689, 13766, 366, 517, 33465, 11], "temperature": 0.0, "avg_logprob": -0.11648740768432617, "compression_ratio": 1.4296875, "no_speech_prob": 1.696393701422494e-05}, {"id": 401, "seek": 246666, "start": 2488.5, "end": 2493.22, "text": " we know what the columns represent and what they are. How often do you pull in data from", "tokens": [321, 458, 437, 264, 13766, 2906, 293, 437, 436, 366, 13, 1012, 2049, 360, 291, 2235, 294, 1412, 490], "temperature": 0.0, "avg_logprob": -0.11648740768432617, "compression_ratio": 1.4296875, "no_speech_prob": 1.696393701422494e-05}, {"id": 402, "seek": 249322, "start": 2493.22, "end": 2504.8999999999996, "text": " other sources to supplement that? Maybe weather data, for example, or how often is that used?", "tokens": [661, 7139, 281, 15436, 300, 30, 2704, 5503, 1412, 11, 337, 1365, 11, 420, 577, 2049, 307, 300, 1143, 30], "temperature": 0.0, "avg_logprob": -0.2592479705810547, "compression_ratio": 1.5397727272727273, "no_speech_prob": 1.7502794435131364e-05}, {"id": 403, "seek": 249322, "start": 2504.8999999999996, "end": 2511.7799999999997, "text": " Very often. So the whole point of this star schema is that you've got your central table", "tokens": [4372, 2049, 13, 407, 264, 1379, 935, 295, 341, 3543, 34078, 307, 300, 291, 600, 658, 428, 5777, 3199], "temperature": 0.0, "avg_logprob": -0.2592479705810547, "compression_ratio": 1.5397727272727273, "no_speech_prob": 1.7502794435131364e-05}, {"id": 404, "seek": 249322, "start": 2511.7799999999997, "end": 2516.18, "text": " and then you've got these other tables coming off that provide metadata about it. So for", "tokens": [293, 550, 291, 600, 658, 613, 661, 8020, 1348, 766, 300, 2893, 26603, 466, 309, 13, 407, 337], "temperature": 0.0, "avg_logprob": -0.2592479705810547, "compression_ratio": 1.5397727272727273, "no_speech_prob": 1.7502794435131364e-05}, {"id": 405, "seek": 251618, "start": 2516.18, "end": 2525.62, "text": " example, weather is metadata about a date. On Kaggle specifically, most competitions have", "tokens": [1365, 11, 5503, 307, 26603, 466, 257, 4002, 13, 1282, 48751, 22631, 4682, 11, 881, 26185, 362], "temperature": 0.0, "avg_logprob": -0.14774675047799443, "compression_ratio": 1.625, "no_speech_prob": 1.084513996829628e-06}, {"id": 406, "seek": 251618, "start": 2525.62, "end": 2532.7799999999997, "text": " the rule that you can use external data as long as you post on the forum that you're", "tokens": [264, 4978, 300, 291, 393, 764, 8320, 1412, 382, 938, 382, 291, 2183, 322, 264, 17542, 300, 291, 434], "temperature": 0.0, "avg_logprob": -0.14774675047799443, "compression_ratio": 1.625, "no_speech_prob": 1.084513996829628e-06}, {"id": 407, "seek": 251618, "start": 2532.7799999999997, "end": 2538.46, "text": " using it and that it's publicly available. But you have to check on a competition-by-competition", "tokens": [1228, 309, 293, 300, 309, 311, 14843, 2435, 13, 583, 291, 362, 281, 1520, 322, 257, 6211, 12, 2322, 12, 1112, 7275, 849], "temperature": 0.0, "avg_logprob": -0.14774675047799443, "compression_ratio": 1.625, "no_speech_prob": 1.084513996829628e-06}, {"id": 408, "seek": 251618, "start": 2538.46, "end": 2545.7799999999997, "text": " basis, they will tell you. Outside of Kaggle, you should always be looking for what external", "tokens": [5143, 11, 436, 486, 980, 291, 13, 28218, 295, 48751, 22631, 11, 291, 820, 1009, 312, 1237, 337, 437, 8320], "temperature": 0.0, "avg_logprob": -0.14774675047799443, "compression_ratio": 1.625, "no_speech_prob": 1.084513996829628e-06}, {"id": 409, "seek": 254578, "start": 2545.78, "end": 2550.78, "text": " data could I possibly leverage here.", "tokens": [1412, 727, 286, 6264, 13982, 510, 13], "temperature": 0.0, "avg_logprob": -0.3831370339464785, "compression_ratio": 1.3888888888888888, "no_speech_prob": 9.914753900375217e-05}, {"id": 410, "seek": 254578, "start": 2550.78, "end": 2559.78, "text": " Question from the audience. Are we still talking about how to tweak this dataset?", "tokens": [14464, 490, 264, 4034, 13, 2014, 321, 920, 1417, 466, 577, 281, 29879, 341, 28872, 30], "temperature": 0.0, "avg_logprob": -0.3831370339464785, "compression_ratio": 1.3888888888888888, "no_speech_prob": 9.914753900375217e-05}, {"id": 411, "seek": 254578, "start": 2559.78, "end": 2560.78, "text": " If you wish.", "tokens": [759, 291, 3172, 13], "temperature": 0.0, "avg_logprob": -0.3831370339464785, "compression_ratio": 1.3888888888888888, "no_speech_prob": 9.914753900375217e-05}, {"id": 412, "seek": 254578, "start": 2560.78, "end": 2564.78, "text": " Well, I'm not familiar with the countries here.", "tokens": [1042, 11, 286, 478, 406, 4963, 365, 264, 3517, 510, 13], "temperature": 0.0, "avg_logprob": -0.3831370339464785, "compression_ratio": 1.3888888888888888, "no_speech_prob": 9.914753900375217e-05}, {"id": 413, "seek": 254578, "start": 2564.78, "end": 2565.78, "text": " This is Ecuador.", "tokens": [639, 307, 41558, 13], "temperature": 0.0, "avg_logprob": -0.3831370339464785, "compression_ratio": 1.3888888888888888, "no_speech_prob": 9.914753900375217e-05}, {"id": 414, "seek": 254578, "start": 2565.78, "end": 2574.86, "text": " Ecuador. So maybe I would start looking for Ecuador's", "tokens": [41558, 13, 407, 1310, 286, 576, 722, 1237, 337, 41558, 311], "temperature": 0.0, "avg_logprob": -0.3831370339464785, "compression_ratio": 1.3888888888888888, "no_speech_prob": 9.914753900375217e-05}, {"id": 415, "seek": 257486, "start": 2574.86, "end": 2580.7400000000002, "text": " holidays and shopping holidays, maybe when they have a 3-day weekend or a week off.", "tokens": [15734, 293, 8688, 15734, 11, 1310, 562, 436, 362, 257, 805, 12, 810, 6711, 420, 257, 1243, 766, 13], "temperature": 0.0, "avg_logprob": -0.17440549633170985, "compression_ratio": 1.6009174311926606, "no_speech_prob": 1.4509604625345673e-05}, {"id": 416, "seek": 257486, "start": 2580.7400000000002, "end": 2588.6600000000003, "text": " That information is provided in this case. So in general, one way of tackling this kind", "tokens": [663, 1589, 307, 5649, 294, 341, 1389, 13, 407, 294, 2674, 11, 472, 636, 295, 34415, 341, 733], "temperature": 0.0, "avg_logprob": -0.17440549633170985, "compression_ratio": 1.6009174311926606, "no_speech_prob": 1.4509604625345673e-05}, {"id": 417, "seek": 257486, "start": 2588.6600000000003, "end": 2596.3, "text": " of problem is to create lots and lots of new columns containing things like average number", "tokens": [295, 1154, 307, 281, 1884, 3195, 293, 3195, 295, 777, 13766, 19273, 721, 411, 4274, 1230], "temperature": 0.0, "avg_logprob": -0.17440549633170985, "compression_ratio": 1.6009174311926606, "no_speech_prob": 1.4509604625345673e-05}, {"id": 418, "seek": 257486, "start": 2596.3, "end": 2602.38, "text": " of sales on holidays, average percent change in sale between January and February, and", "tokens": [295, 5763, 322, 15734, 11, 4274, 3043, 1319, 294, 8680, 1296, 7061, 293, 8711, 11, 293], "temperature": 0.0, "avg_logprob": -0.17440549633170985, "compression_ratio": 1.6009174311926606, "no_speech_prob": 1.4509604625345673e-05}, {"id": 419, "seek": 260238, "start": 2602.38, "end": 2604.98, "text": " so on and so forth.", "tokens": [370, 322, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.19124876323499176, "compression_ratio": 1.588235294117647, "no_speech_prob": 7.41113262847648e-06}, {"id": 420, "seek": 260238, "start": 2604.98, "end": 2611.62, "text": " If you have a look at, there's been a previous competition on Kaggle called Rossmann Store", "tokens": [759, 291, 362, 257, 574, 412, 11, 456, 311, 668, 257, 3894, 6211, 322, 48751, 22631, 1219, 16140, 14912, 17242], "temperature": 0.0, "avg_logprob": -0.19124876323499176, "compression_ratio": 1.588235294117647, "no_speech_prob": 7.41113262847648e-06}, {"id": 421, "seek": 260238, "start": 2611.62, "end": 2618.94, "text": " Sales that was almost identical. It was in Germany in this case for a major grocery chain,", "tokens": [23467, 300, 390, 1920, 14800, 13, 467, 390, 294, 7244, 294, 341, 1389, 337, 257, 2563, 14410, 5021, 11], "temperature": 0.0, "avg_logprob": -0.19124876323499176, "compression_ratio": 1.588235294117647, "no_speech_prob": 7.41113262847648e-06}, {"id": 422, "seek": 260238, "start": 2618.94, "end": 2627.1, "text": " how many items are sold by day by item type by store. In this case, the person who won", "tokens": [577, 867, 4754, 366, 3718, 538, 786, 538, 3174, 2010, 538, 3531, 13, 682, 341, 1389, 11, 264, 954, 567, 1582], "temperature": 0.0, "avg_logprob": -0.19124876323499176, "compression_ratio": 1.588235294117647, "no_speech_prob": 7.41113262847648e-06}, {"id": 423, "seek": 260238, "start": 2627.1, "end": 2632.02, "text": " quite unusually actually was something of a domain expert in this space. They're actually", "tokens": [1596, 10054, 671, 767, 390, 746, 295, 257, 9274, 5844, 294, 341, 1901, 13, 814, 434, 767], "temperature": 0.0, "avg_logprob": -0.19124876323499176, "compression_ratio": 1.588235294117647, "no_speech_prob": 7.41113262847648e-06}, {"id": 424, "seek": 263202, "start": 2632.02, "end": 2639.58, "text": " a specialist in doing logistics predictions. This is basically what they did. He's a professional", "tokens": [257, 17008, 294, 884, 27420, 21264, 13, 639, 307, 1936, 437, 436, 630, 13, 634, 311, 257, 4843], "temperature": 0.0, "avg_logprob": -0.17883707495296702, "compression_ratio": 1.5759162303664922, "no_speech_prob": 9.08019774215063e-06}, {"id": 425, "seek": 263202, "start": 2639.58, "end": 2647.78, "text": " sales forecast consultant. He created just lots and lots and lots of columns based on", "tokens": [5763, 14330, 24676, 13, 634, 2942, 445, 3195, 293, 3195, 293, 3195, 295, 13766, 2361, 322], "temperature": 0.0, "avg_logprob": -0.17883707495296702, "compression_ratio": 1.5759162303664922, "no_speech_prob": 9.08019774215063e-06}, {"id": 426, "seek": 263202, "start": 2647.78, "end": 2653.5, "text": " his experience of what kinds of things tend to be useful for making predictions. So that's", "tokens": [702, 1752, 295, 437, 3685, 295, 721, 3928, 281, 312, 4420, 337, 1455, 21264, 13, 407, 300, 311], "temperature": 0.0, "avg_logprob": -0.17883707495296702, "compression_ratio": 1.5759162303664922, "no_speech_prob": 9.08019774215063e-06}, {"id": 427, "seek": 263202, "start": 2653.5, "end": 2657.22, "text": " an approach that can work.", "tokens": [364, 3109, 300, 393, 589, 13], "temperature": 0.0, "avg_logprob": -0.17883707495296702, "compression_ratio": 1.5759162303664922, "no_speech_prob": 9.08019774215063e-06}, {"id": 428, "seek": 265722, "start": 2657.22, "end": 2663.22, "text": " The third-place team did almost no feature engineering, however, and also they had one", "tokens": [440, 2636, 12, 6742, 1469, 630, 1920, 572, 4111, 7043, 11, 4461, 11, 293, 611, 436, 632, 472], "temperature": 0.0, "avg_logprob": -0.1943766661364623, "compression_ratio": 1.5843621399176955, "no_speech_prob": 3.373642539372668e-05}, {"id": 429, "seek": 265722, "start": 2663.22, "end": 2668.02, "text": " big oversight which I think they would have won if they hadn't had it. So you don't necessarily", "tokens": [955, 29146, 597, 286, 519, 436, 576, 362, 1582, 498, 436, 8782, 380, 632, 309, 13, 407, 291, 500, 380, 4725], "temperature": 0.0, "avg_logprob": -0.1943766661364623, "compression_ratio": 1.5843621399176955, "no_speech_prob": 3.373642539372668e-05}, {"id": 430, "seek": 265722, "start": 2668.02, "end": 2676.8999999999996, "text": " have to use this approach. We'll be learning a lot more about how to win this competition", "tokens": [362, 281, 764, 341, 3109, 13, 492, 603, 312, 2539, 257, 688, 544, 466, 577, 281, 1942, 341, 6211], "temperature": 0.0, "avg_logprob": -0.1943766661364623, "compression_ratio": 1.5843621399176955, "no_speech_prob": 3.373642539372668e-05}, {"id": 431, "seek": 265722, "start": 2676.8999999999996, "end": 2680.1, "text": " and ones like it as we go.", "tokens": [293, 2306, 411, 309, 382, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.1943766661364623, "compression_ratio": 1.5843621399176955, "no_speech_prob": 3.373642539372668e-05}, {"id": 432, "seek": 265722, "start": 2680.1, "end": 2684.62, "text": " They did interview the third-place team, so if you Google for Kaggle Rossmann, you'll", "tokens": [814, 630, 4049, 264, 2636, 12, 6742, 1469, 11, 370, 498, 291, 3329, 337, 48751, 22631, 16140, 14912, 11, 291, 603], "temperature": 0.0, "avg_logprob": -0.1943766661364623, "compression_ratio": 1.5843621399176955, "no_speech_prob": 3.373642539372668e-05}, {"id": 433, "seek": 268462, "start": 2684.62, "end": 2690.14, "text": " see it. The short answer is they used BigBlood.", "tokens": [536, 309, 13, 440, 2099, 1867, 307, 436, 1143, 5429, 33, 752, 378, 13], "temperature": 0.0, "avg_logprob": -0.2320980117434547, "compression_ratio": 1.5, "no_speech_prob": 1.1659332812996581e-05}, {"id": 434, "seek": 268462, "start": 2690.14, "end": 2697.74, "text": " So Terrence is actually my teammate on this competition. Terrence drew a couple of these", "tokens": [407, 6564, 10760, 307, 767, 452, 25467, 322, 341, 6211, 13, 6564, 10760, 12804, 257, 1916, 295, 613], "temperature": 0.0, "avg_logprob": -0.2320980117434547, "compression_ratio": 1.5, "no_speech_prob": 1.1659332812996581e-05}, {"id": 435, "seek": 268462, "start": 2697.74, "end": 2703.02, "text": " charts for us, and I wanted to talk about this. If you don't have a good validation", "tokens": [17767, 337, 505, 11, 293, 286, 1415, 281, 751, 466, 341, 13, 759, 291, 500, 380, 362, 257, 665, 24071], "temperature": 0.0, "avg_logprob": -0.2320980117434547, "compression_ratio": 1.5, "no_speech_prob": 1.1659332812996581e-05}, {"id": 436, "seek": 268462, "start": 2703.02, "end": 2711.74, "text": " set, it's hard if not impossible to create a good model. So in other words, if you're", "tokens": [992, 11, 309, 311, 1152, 498, 406, 6243, 281, 1884, 257, 665, 2316, 13, 407, 294, 661, 2283, 11, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.2320980117434547, "compression_ratio": 1.5, "no_speech_prob": 1.1659332812996581e-05}, {"id": 437, "seek": 271174, "start": 2711.74, "end": 2719.9799999999996, "text": " trying to predict next month's sales, and you try to build a model, and you have no", "tokens": [1382, 281, 6069, 958, 1618, 311, 5763, 11, 293, 291, 853, 281, 1322, 257, 2316, 11, 293, 291, 362, 572], "temperature": 0.0, "avg_logprob": -0.12397916855350617, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.70616931427503e-06}, {"id": 438, "seek": 271174, "start": 2719.9799999999996, "end": 2724.4199999999996, "text": " way of really knowing whether the models you built are good at predicting sales a month", "tokens": [636, 295, 534, 5276, 1968, 264, 5245, 291, 3094, 366, 665, 412, 32884, 5763, 257, 1618], "temperature": 0.0, "avg_logprob": -0.12397916855350617, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.70616931427503e-06}, {"id": 439, "seek": 271174, "start": 2724.4199999999996, "end": 2729.02, "text": " ahead of time, then you have no way of knowing when you put your model in production whether", "tokens": [2286, 295, 565, 11, 550, 291, 362, 572, 636, 295, 5276, 562, 291, 829, 428, 2316, 294, 4265, 1968], "temperature": 0.0, "avg_logprob": -0.12397916855350617, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.70616931427503e-06}, {"id": 440, "seek": 271174, "start": 2729.02, "end": 2733.02, "text": " it's actually going to be any good.", "tokens": [309, 311, 767, 516, 281, 312, 604, 665, 13], "temperature": 0.0, "avg_logprob": -0.12397916855350617, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.70616931427503e-06}, {"id": 441, "seek": 271174, "start": 2733.02, "end": 2739.4199999999996, "text": " So you need a validation set that you know is reliable at telling you whether or not", "tokens": [407, 291, 643, 257, 24071, 992, 300, 291, 458, 307, 12924, 412, 3585, 291, 1968, 420, 406], "temperature": 0.0, "avg_logprob": -0.12397916855350617, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.70616931427503e-06}, {"id": 442, "seek": 273942, "start": 2739.42, "end": 2745.62, "text": " your model is likely to work well when you put it into production or use it on the test", "tokens": [428, 2316, 307, 3700, 281, 589, 731, 562, 291, 829, 309, 666, 4265, 420, 764, 309, 322, 264, 1500], "temperature": 0.0, "avg_logprob": -0.12954491899724593, "compression_ratio": 1.7131782945736433, "no_speech_prob": 7.646481208212208e-06}, {"id": 443, "seek": 273942, "start": 2745.62, "end": 2747.42, "text": " set.", "tokens": [992, 13], "temperature": 0.0, "avg_logprob": -0.12954491899724593, "compression_ratio": 1.7131782945736433, "no_speech_prob": 7.646481208212208e-06}, {"id": 444, "seek": 273942, "start": 2747.42, "end": 2754.1, "text": " So in this case, what Terrence has plotted here is, normally you should not use your", "tokens": [407, 294, 341, 1389, 11, 437, 6564, 10760, 575, 43288, 510, 307, 11, 5646, 291, 820, 406, 764, 428], "temperature": 0.0, "avg_logprob": -0.12954491899724593, "compression_ratio": 1.7131782945736433, "no_speech_prob": 7.646481208212208e-06}, {"id": 445, "seek": 273942, "start": 2754.1, "end": 2759.1, "text": " test set for anything other than using it right at the end of the competition or right", "tokens": [1500, 992, 337, 1340, 661, 813, 1228, 309, 558, 412, 264, 917, 295, 264, 6211, 420, 558], "temperature": 0.0, "avg_logprob": -0.12954491899724593, "compression_ratio": 1.7131782945736433, "no_speech_prob": 7.646481208212208e-06}, {"id": 446, "seek": 273942, "start": 2759.1, "end": 2763.66, "text": " at the end of the project to find out how you've gone. But there's one thing I'm going", "tokens": [412, 264, 917, 295, 264, 1716, 281, 915, 484, 577, 291, 600, 2780, 13, 583, 456, 311, 472, 551, 286, 478, 516], "temperature": 0.0, "avg_logprob": -0.12954491899724593, "compression_ratio": 1.7131782945736433, "no_speech_prob": 7.646481208212208e-06}, {"id": 447, "seek": 273942, "start": 2763.66, "end": 2769.38, "text": " to let you use the test set for in addition, and that is to calibrate your validation set.", "tokens": [281, 718, 291, 764, 264, 1500, 992, 337, 294, 4500, 11, 293, 300, 307, 281, 21583, 4404, 428, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.12954491899724593, "compression_ratio": 1.7131782945736433, "no_speech_prob": 7.646481208212208e-06}, {"id": 448, "seek": 276938, "start": 2769.38, "end": 2774.6600000000003, "text": " So what Terrence did here was he built four different models, some which he thought would", "tokens": [407, 437, 6564, 10760, 630, 510, 390, 415, 3094, 1451, 819, 5245, 11, 512, 597, 415, 1194, 576], "temperature": 0.0, "avg_logprob": -0.11954465779391202, "compression_ratio": 1.6179245283018868, "no_speech_prob": 5.507563855644548e-06}, {"id": 449, "seek": 276938, "start": 2774.6600000000003, "end": 2780.9, "text": " be better than others, and he submitted each of the four models to Kaggle to find out its", "tokens": [312, 1101, 813, 2357, 11, 293, 415, 14405, 1184, 295, 264, 1451, 5245, 281, 48751, 22631, 281, 915, 484, 1080], "temperature": 0.0, "avg_logprob": -0.11954465779391202, "compression_ratio": 1.6179245283018868, "no_speech_prob": 5.507563855644548e-06}, {"id": 450, "seek": 276938, "start": 2780.9, "end": 2788.54, "text": " score. So the x-axis is the score that Kaggle told us on the leaderboard.", "tokens": [6175, 13, 407, 264, 2031, 12, 24633, 307, 264, 6175, 300, 48751, 22631, 1907, 505, 322, 264, 5263, 3787, 13], "temperature": 0.0, "avg_logprob": -0.11954465779391202, "compression_ratio": 1.6179245283018868, "no_speech_prob": 5.507563855644548e-06}, {"id": 451, "seek": 276938, "start": 2788.54, "end": 2794.2200000000003, "text": " And then on the y-axis, he plotted the score on a particular validation set he was trying", "tokens": [400, 550, 322, 264, 288, 12, 24633, 11, 415, 43288, 264, 6175, 322, 257, 1729, 24071, 992, 415, 390, 1382], "temperature": 0.0, "avg_logprob": -0.11954465779391202, "compression_ratio": 1.6179245283018868, "no_speech_prob": 5.507563855644548e-06}, {"id": 452, "seek": 279422, "start": 2794.22, "end": 2801.02, "text": " out to see whether this validation set looked like it was going to be any good. So if your", "tokens": [484, 281, 536, 1968, 341, 24071, 992, 2956, 411, 309, 390, 516, 281, 312, 604, 665, 13, 407, 498, 428], "temperature": 0.0, "avg_logprob": -0.1449493169784546, "compression_ratio": 1.6944444444444444, "no_speech_prob": 1.6797245052657672e-06}, {"id": 453, "seek": 279422, "start": 2801.02, "end": 2807.14, "text": " validation set is good, then the relationship between the leaderboard score, the test set", "tokens": [24071, 992, 307, 665, 11, 550, 264, 2480, 1296, 264, 5263, 3787, 6175, 11, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.1449493169784546, "compression_ratio": 1.6944444444444444, "no_speech_prob": 1.6797245052657672e-06}, {"id": 454, "seek": 279422, "start": 2807.14, "end": 2813.9399999999996, "text": " score, and your validation set score should lie in a straight line. Ideally, it'll actually", "tokens": [6175, 11, 293, 428, 24071, 992, 6175, 820, 4544, 294, 257, 2997, 1622, 13, 40817, 11, 309, 603, 767], "temperature": 0.0, "avg_logprob": -0.1449493169784546, "compression_ratio": 1.6944444444444444, "no_speech_prob": 1.6797245052657672e-06}, {"id": 455, "seek": 279422, "start": 2813.9399999999996, "end": 2819.8599999999997, "text": " lie on the y equals x line. But honestly, that doesn't matter too much. As long as relatively", "tokens": [4544, 322, 264, 288, 6915, 2031, 1622, 13, 583, 6095, 11, 300, 1177, 380, 1871, 886, 709, 13, 1018, 938, 382, 7226], "temperature": 0.0, "avg_logprob": -0.1449493169784546, "compression_ratio": 1.6944444444444444, "no_speech_prob": 1.6797245052657672e-06}, {"id": 456, "seek": 281986, "start": 2819.86, "end": 2824.42, "text": " speaking it tells you which models are better than which other models, then you know which", "tokens": [4124, 309, 5112, 291, 597, 5245, 366, 1101, 813, 597, 661, 5245, 11, 550, 291, 458, 597], "temperature": 0.0, "avg_logprob": -0.14525006398433396, "compression_ratio": 1.731958762886598, "no_speech_prob": 1.2679253131864243e-06}, {"id": 457, "seek": 281986, "start": 2824.42, "end": 2830.46, "text": " model is the best. And you know how it's going to perform on the test set because you know", "tokens": [2316, 307, 264, 1151, 13, 400, 291, 458, 577, 309, 311, 516, 281, 2042, 322, 264, 1500, 992, 570, 291, 458], "temperature": 0.0, "avg_logprob": -0.14525006398433396, "compression_ratio": 1.731958762886598, "no_speech_prob": 1.2679253131864243e-06}, {"id": 458, "seek": 281986, "start": 2830.46, "end": 2833.5, "text": " the linear relationship between the two things.", "tokens": [264, 8213, 2480, 1296, 264, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.14525006398433396, "compression_ratio": 1.731958762886598, "no_speech_prob": 1.2679253131864243e-06}, {"id": 459, "seek": 281986, "start": 2833.5, "end": 2838.46, "text": " So in this case, Terrence has managed to come up with a validation set which is looking", "tokens": [407, 294, 341, 1389, 11, 6564, 10760, 575, 6453, 281, 808, 493, 365, 257, 24071, 992, 597, 307, 1237], "temperature": 0.0, "avg_logprob": -0.14525006398433396, "compression_ratio": 1.731958762886598, "no_speech_prob": 1.2679253131864243e-06}, {"id": 460, "seek": 281986, "start": 2838.46, "end": 2842.82, "text": " like it's going to predict our Kaggle leaderboard score pretty well. And that's really cool,", "tokens": [411, 309, 311, 516, 281, 6069, 527, 48751, 22631, 5263, 3787, 6175, 1238, 731, 13, 400, 300, 311, 534, 1627, 11], "temperature": 0.0, "avg_logprob": -0.14525006398433396, "compression_ratio": 1.731958762886598, "no_speech_prob": 1.2679253131864243e-06}, {"id": 461, "seek": 281986, "start": 2842.82, "end": 2847.78, "text": " right, because now he can go away and try 100 different types of models, feature engineering,", "tokens": [558, 11, 570, 586, 415, 393, 352, 1314, 293, 853, 2319, 819, 3467, 295, 5245, 11, 4111, 7043, 11], "temperature": 0.0, "avg_logprob": -0.14525006398433396, "compression_ratio": 1.731958762886598, "no_speech_prob": 1.2679253131864243e-06}, {"id": 462, "seek": 284778, "start": 2847.78, "end": 2852.78, "text": " weighting, tweaking, hyperparameters, whatever else, see how they go in the validation set", "tokens": [3364, 278, 11, 6986, 2456, 11, 9848, 2181, 335, 6202, 11, 2035, 1646, 11, 536, 577, 436, 352, 294, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.18117172435178594, "compression_ratio": 1.5942028985507246, "no_speech_prob": 3.1875515560386702e-06}, {"id": 463, "seek": 284778, "start": 2852.78, "end": 2856.6200000000003, "text": " and not have to submit to Kaggle. So we're going to get a lot more iterations, a lot", "tokens": [293, 406, 362, 281, 10315, 281, 48751, 22631, 13, 407, 321, 434, 516, 281, 483, 257, 688, 544, 36540, 11, 257, 688], "temperature": 0.0, "avg_logprob": -0.18117172435178594, "compression_ratio": 1.5942028985507246, "no_speech_prob": 3.1875515560386702e-06}, {"id": 464, "seek": 284778, "start": 2856.6200000000003, "end": 2863.42, "text": " more feedback. This is not just true of Kaggle, but every machine learning project you do.", "tokens": [544, 5824, 13, 639, 307, 406, 445, 2074, 295, 48751, 22631, 11, 457, 633, 3479, 2539, 1716, 291, 360, 13], "temperature": 0.0, "avg_logprob": -0.18117172435178594, "compression_ratio": 1.5942028985507246, "no_speech_prob": 3.1875515560386702e-06}, {"id": 465, "seek": 284778, "start": 2863.42, "end": 2869.86, "text": " And so here's a different one he tried where it wasn't as good. It's like, oh, these ones", "tokens": [400, 370, 510, 311, 257, 819, 472, 415, 3031, 689, 309, 2067, 380, 382, 665, 13, 467, 311, 411, 11, 1954, 11, 613, 2306], "temperature": 0.0, "avg_logprob": -0.18117172435178594, "compression_ratio": 1.5942028985507246, "no_speech_prob": 3.1875515560386702e-06}, {"id": 466, "seek": 284778, "start": 2869.86, "end": 2873.5800000000004, "text": " that were quite close to each other, it's showing us the opposite direction. That's", "tokens": [300, 645, 1596, 1998, 281, 1184, 661, 11, 309, 311, 4099, 505, 264, 6182, 3513, 13, 663, 311], "temperature": 0.0, "avg_logprob": -0.18117172435178594, "compression_ratio": 1.5942028985507246, "no_speech_prob": 3.1875515560386702e-06}, {"id": 467, "seek": 287358, "start": 2873.58, "end": 2879.46, "text": " a really bad sign. It's like, okay, this validation set idea didn't seem like a good idea, this", "tokens": [257, 534, 1578, 1465, 13, 467, 311, 411, 11, 1392, 11, 341, 24071, 992, 1558, 994, 380, 1643, 411, 257, 665, 1558, 11, 341], "temperature": 0.0, "avg_logprob": -0.17328456500629047, "compression_ratio": 1.8049792531120332, "no_speech_prob": 2.994380338350311e-06}, {"id": 468, "seek": 287358, "start": 2879.46, "end": 2882.1, "text": " validation set idea didn't look like a good idea.", "tokens": [24071, 992, 1558, 994, 380, 574, 411, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.17328456500629047, "compression_ratio": 1.8049792531120332, "no_speech_prob": 2.994380338350311e-06}, {"id": 469, "seek": 287358, "start": 2882.1, "end": 2886.22, "text": " So in general, if your validation set's not showing a nice straight line, you need to", "tokens": [407, 294, 2674, 11, 498, 428, 24071, 992, 311, 406, 4099, 257, 1481, 2997, 1622, 11, 291, 643, 281], "temperature": 0.0, "avg_logprob": -0.17328456500629047, "compression_ratio": 1.8049792531120332, "no_speech_prob": 2.994380338350311e-06}, {"id": 470, "seek": 287358, "start": 2886.22, "end": 2893.02, "text": " think carefully. How is the test set constructed? How is my validation set different? There's", "tokens": [519, 7500, 13, 1012, 307, 264, 1500, 992, 17083, 30, 1012, 307, 452, 24071, 992, 819, 30, 821, 311], "temperature": 0.0, "avg_logprob": -0.17328456500629047, "compression_ratio": 1.8049792531120332, "no_speech_prob": 2.994380338350311e-06}, {"id": 471, "seek": 287358, "start": 2893.02, "end": 2897.38, "text": " some way you're constructing it which is different. You're going to have to draw lots of charts", "tokens": [512, 636, 291, 434, 39969, 309, 597, 307, 819, 13, 509, 434, 516, 281, 362, 281, 2642, 3195, 295, 17767], "temperature": 0.0, "avg_logprob": -0.17328456500629047, "compression_ratio": 1.8049792531120332, "no_speech_prob": 2.994380338350311e-06}, {"id": 472, "seek": 287358, "start": 2897.38, "end": 2902.74, "text": " and so forth.", "tokens": [293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.17328456500629047, "compression_ratio": 1.8049792531120332, "no_speech_prob": 2.994380338350311e-06}, {"id": 473, "seek": 290274, "start": 2902.74, "end": 2908.4599999999996, "text": " So one question is, and I'm going to try to guess how you did it. So how do you actually", "tokens": [407, 472, 1168, 307, 11, 293, 286, 478, 516, 281, 853, 281, 2041, 577, 291, 630, 309, 13, 407, 577, 360, 291, 767], "temperature": 0.0, "avg_logprob": -0.27363246982380496, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00014176007243804634}, {"id": 474, "seek": 290274, "start": 2908.4599999999996, "end": 2913.54, "text": " try to construct this validation set as close to the... So what I would try to do is to", "tokens": [853, 281, 7690, 341, 24071, 992, 382, 1998, 281, 264, 485, 407, 437, 286, 576, 853, 281, 360, 307, 281], "temperature": 0.0, "avg_logprob": -0.27363246982380496, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00014176007243804634}, {"id": 475, "seek": 290274, "start": 2913.54, "end": 2919.7, "text": " try to sample points from the training set that are very close or possible to some of", "tokens": [853, 281, 6889, 2793, 490, 264, 3097, 992, 300, 366, 588, 1998, 420, 1944, 281, 512, 295], "temperature": 0.0, "avg_logprob": -0.27363246982380496, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00014176007243804634}, {"id": 476, "seek": 290274, "start": 2919.7, "end": 2921.8999999999996, "text": " the points in the test set.", "tokens": [264, 2793, 294, 264, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.27363246982380496, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00014176007243804634}, {"id": 477, "seek": 290274, "start": 2921.8999999999996, "end": 2923.66, "text": " Close in what sense?", "tokens": [16346, 294, 437, 2020, 30], "temperature": 0.0, "avg_logprob": -0.27363246982380496, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00014176007243804634}, {"id": 478, "seek": 290274, "start": 2923.66, "end": 2925.66, "text": " I don't know. I will have to find the features.", "tokens": [286, 500, 380, 458, 13, 286, 486, 362, 281, 915, 264, 4122, 13], "temperature": 0.0, "avg_logprob": -0.27363246982380496, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00014176007243804634}, {"id": 479, "seek": 290274, "start": 2925.66, "end": 2926.66, "text": " What would you guess?", "tokens": [708, 576, 291, 2041, 30], "temperature": 0.0, "avg_logprob": -0.27363246982380496, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00014176007243804634}, {"id": 480, "seek": 290274, "start": 2926.66, "end": 2929.66, "text": " What in this case, for this grocery is?", "tokens": [708, 294, 341, 1389, 11, 337, 341, 14410, 307, 30], "temperature": 0.0, "avg_logprob": -0.27363246982380496, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00014176007243804634}, {"id": 481, "seek": 292966, "start": 2929.66, "end": 2933.02, "text": " The last points?", "tokens": [440, 1036, 2793, 30], "temperature": 0.0, "avg_logprob": -0.3187951268376531, "compression_ratio": 1.6179775280898876, "no_speech_prob": 1.922251976793632e-05}, {"id": 482, "seek": 292966, "start": 2933.02, "end": 2937.8599999999997, "text": " Close by date. So basically all the different things Terence was trying were different variations", "tokens": [16346, 538, 4002, 13, 407, 1936, 439, 264, 819, 721, 6564, 655, 390, 1382, 645, 819, 17840], "temperature": 0.0, "avg_logprob": -0.3187951268376531, "compression_ratio": 1.6179775280898876, "no_speech_prob": 1.922251976793632e-05}, {"id": 483, "seek": 292966, "start": 2937.8599999999997, "end": 2942.98, "text": " of close by date. So the most recent.", "tokens": [295, 1998, 538, 4002, 13, 407, 264, 881, 5162, 13], "temperature": 0.0, "avg_logprob": -0.3187951268376531, "compression_ratio": 1.6179775280898876, "no_speech_prob": 1.922251976793632e-05}, {"id": 484, "seek": 292966, "start": 2942.98, "end": 2950.7799999999997, "text": " What I noticed was, so first I looked at the date range of the test set and then I looked", "tokens": [708, 286, 5694, 390, 11, 370, 700, 286, 2956, 412, 264, 4002, 3613, 295, 264, 1500, 992, 293, 550, 286, 2956], "temperature": 0.0, "avg_logprob": -0.3187951268376531, "compression_ratio": 1.6179775280898876, "no_speech_prob": 1.922251976793632e-05}, {"id": 485, "seek": 292966, "start": 2950.7799999999997, "end": 2955.2999999999997, "text": " at the kernel that described how he or she...", "tokens": [412, 264, 28256, 300, 7619, 577, 415, 420, 750, 485], "temperature": 0.0, "avg_logprob": -0.3187951268376531, "compression_ratio": 1.6179775280898876, "no_speech_prob": 1.922251976793632e-05}, {"id": 486, "seek": 295530, "start": 2955.3, "end": 2961.38, "text": " So here's the date range of the test set. So the last two weeks of August 26, 2017.", "tokens": [407, 510, 311, 264, 4002, 3613, 295, 264, 1500, 992, 13, 407, 264, 1036, 732, 3259, 295, 6897, 7551, 11, 6591, 13], "temperature": 0.0, "avg_logprob": -0.22170524243955259, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.9302162147359923e-05}, {"id": 487, "seek": 295530, "start": 2961.38, "end": 2967.02, "text": " That's right. And then the person who submitted the kernel that said how to get the 0.58 leaderboard", "tokens": [663, 311, 558, 13, 400, 550, 264, 954, 567, 14405, 264, 28256, 300, 848, 577, 281, 483, 264, 1958, 13, 20419, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.22170524243955259, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.9302162147359923e-05}, {"id": 488, "seek": 295530, "start": 2967.02, "end": 2968.34, "text": " position or whatever score.", "tokens": [2535, 420, 2035, 6175, 13], "temperature": 0.0, "avg_logprob": -0.22170524243955259, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.9302162147359923e-05}, {"id": 489, "seek": 295530, "start": 2968.34, "end": 2969.7400000000002, "text": " The average by group.", "tokens": [440, 4274, 538, 1594, 13], "temperature": 0.0, "avg_logprob": -0.22170524243955259, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.9302162147359923e-05}, {"id": 490, "seek": 295530, "start": 2969.7400000000002, "end": 2974.1000000000004, "text": " I looked at the date range of that and that was...", "tokens": [286, 2956, 412, 264, 4002, 3613, 295, 300, 293, 300, 390, 485], "temperature": 0.0, "avg_logprob": -0.22170524243955259, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.9302162147359923e-05}, {"id": 491, "seek": 295530, "start": 2974.1000000000004, "end": 2975.1000000000004, "text": " Like nine or 10 days.", "tokens": [1743, 4949, 420, 1266, 1708, 13], "temperature": 0.0, "avg_logprob": -0.22170524243955259, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.9302162147359923e-05}, {"id": 492, "seek": 295530, "start": 2975.1000000000004, "end": 2980.26, "text": " Well, it was actually 14 days and the test set is 16 days. But the interesting thing", "tokens": [1042, 11, 309, 390, 767, 3499, 1708, 293, 264, 1500, 992, 307, 3165, 1708, 13, 583, 264, 1880, 551], "temperature": 0.0, "avg_logprob": -0.22170524243955259, "compression_ratio": 1.5806451612903225, "no_speech_prob": 2.9302162147359923e-05}, {"id": 493, "seek": 298026, "start": 2980.26, "end": 2989.6600000000003, "text": " is the test set begins on the day after payday and ends on the payday. And so these are things", "tokens": [307, 264, 1500, 992, 7338, 322, 264, 786, 934, 1689, 810, 293, 5314, 322, 264, 1689, 810, 13, 400, 370, 613, 366, 721], "temperature": 0.0, "avg_logprob": -0.19336874668414777, "compression_ratio": 1.6810344827586208, "no_speech_prob": 2.6425689156894805e-06}, {"id": 494, "seek": 298026, "start": 2989.6600000000003, "end": 2991.38, "text": " I also paid attention to.", "tokens": [286, 611, 4835, 3202, 281, 13], "temperature": 0.0, "avg_logprob": -0.19336874668414777, "compression_ratio": 1.6810344827586208, "no_speech_prob": 2.6425689156894805e-06}, {"id": 495, "seek": 298026, "start": 2991.38, "end": 2998.5400000000004, "text": " And I think that's one of the bits of metadata that they told us. These are the kinds of", "tokens": [400, 286, 519, 300, 311, 472, 295, 264, 9239, 295, 26603, 300, 436, 1907, 505, 13, 1981, 366, 264, 3685, 295], "temperature": 0.0, "avg_logprob": -0.19336874668414777, "compression_ratio": 1.6810344827586208, "no_speech_prob": 2.6425689156894805e-06}, {"id": 496, "seek": 298026, "start": 2998.5400000000004, "end": 3004.94, "text": " things you just got to try. Like I said, plot lots of pictures. Even if you didn't know", "tokens": [721, 291, 445, 658, 281, 853, 13, 1743, 286, 848, 11, 7542, 3195, 295, 5242, 13, 2754, 498, 291, 994, 380, 458], "temperature": 0.0, "avg_logprob": -0.19336874668414777, "compression_ratio": 1.6810344827586208, "no_speech_prob": 2.6425689156894805e-06}, {"id": 497, "seek": 298026, "start": 3004.94, "end": 3010.1000000000004, "text": " it was payday, you would want to draw the time series chart of sales and you would hopefully", "tokens": [309, 390, 1689, 810, 11, 291, 576, 528, 281, 2642, 264, 565, 2638, 6927, 295, 5763, 293, 291, 576, 4696], "temperature": 0.0, "avg_logprob": -0.19336874668414777, "compression_ratio": 1.6810344827586208, "no_speech_prob": 2.6425689156894805e-06}, {"id": 498, "seek": 301010, "start": 3010.1, "end": 3014.5, "text": " see that every two weeks there would be a spike or whatever. And you'd be like, oh I", "tokens": [536, 300, 633, 732, 3259, 456, 576, 312, 257, 21053, 420, 2035, 13, 400, 291, 1116, 312, 411, 11, 1954, 286], "temperature": 0.0, "avg_logprob": -0.2732074665573408, "compression_ratio": 1.4351145038167938, "no_speech_prob": 1.221883030666504e-05}, {"id": 499, "seek": 301010, "start": 3014.5, "end": 3020.22, "text": " want to make sure that I have the same number of spikes in my validation set that I had", "tokens": [528, 281, 652, 988, 300, 286, 362, 264, 912, 1230, 295, 28997, 294, 452, 24071, 992, 300, 286, 632], "temperature": 0.0, "avg_logprob": -0.2732074665573408, "compression_ratio": 1.4351145038167938, "no_speech_prob": 1.221883030666504e-05}, {"id": 500, "seek": 301010, "start": 3020.22, "end": 3022.7799999999997, "text": " in my test set.", "tokens": [294, 452, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.2732074665573408, "compression_ratio": 1.4351145038167938, "no_speech_prob": 1.221883030666504e-05}, {"id": 501, "seek": 302278, "start": 3022.78, "end": 3041.6200000000003, "text": " Let's take a 5-minute break and let's come back at 2.32. So this is my favorite bit,", "tokens": [961, 311, 747, 257, 1025, 12, 18256, 1821, 293, 718, 311, 808, 646, 412, 568, 13, 11440, 13, 407, 341, 307, 452, 2954, 857, 11], "temperature": 0.0, "avg_logprob": -0.18825062534265352, "compression_ratio": 1.3790849673202614, "no_speech_prob": 3.7852616969757946e-06}, {"id": 502, "seek": 302278, "start": 3041.6200000000003, "end": 3044.6600000000003, "text": " interpreting machine learning models.", "tokens": [37395, 3479, 2539, 5245, 13], "temperature": 0.0, "avg_logprob": -0.18825062534265352, "compression_ratio": 1.3790849673202614, "no_speech_prob": 3.7852616969757946e-06}, {"id": 503, "seek": 302278, "start": 3044.6600000000003, "end": 3050.78, "text": " By the way, if you're looking for my notebook about the groceries competition, you won't", "tokens": [3146, 264, 636, 11, 498, 291, 434, 1237, 337, 452, 21060, 466, 264, 31391, 6211, 11, 291, 1582, 380], "temperature": 0.0, "avg_logprob": -0.18825062534265352, "compression_ratio": 1.3790849673202614, "no_speech_prob": 3.7852616969757946e-06}, {"id": 504, "seek": 305078, "start": 3050.78, "end": 3056.1800000000003, "text": " find it in GitHub because I'm not allowed to share code for running competitions with", "tokens": [915, 309, 294, 23331, 570, 286, 478, 406, 4350, 281, 2073, 3089, 337, 2614, 26185, 365], "temperature": 0.0, "avg_logprob": -0.23191519706479966, "compression_ratio": 1.6081081081081081, "no_speech_prob": 1.892494037747383e-05}, {"id": 505, "seek": 305078, "start": 3056.1800000000003, "end": 3061.1800000000003, "text": " you unless you're on the same team as me. That's the rule. After the competition is", "tokens": [291, 5969, 291, 434, 322, 264, 912, 1469, 382, 385, 13, 663, 311, 264, 4978, 13, 2381, 264, 6211, 307], "temperature": 0.0, "avg_logprob": -0.23191519706479966, "compression_ratio": 1.6081081081081081, "no_speech_prob": 1.892494037747383e-05}, {"id": 506, "seek": 305078, "start": 3061.1800000000003, "end": 3066.78, "text": " finished, it'll be on GitHub. So if you're doing this for the video, you should be able", "tokens": [4335, 11, 309, 603, 312, 322, 23331, 13, 407, 498, 291, 434, 884, 341, 337, 264, 960, 11, 291, 820, 312, 1075], "temperature": 0.0, "avg_logprob": -0.23191519706479966, "compression_ratio": 1.6081081081081081, "no_speech_prob": 1.892494037747383e-05}, {"id": 507, "seek": 305078, "start": 3066.78, "end": 3071.1800000000003, "text": " to find it.", "tokens": [281, 915, 309, 13], "temperature": 0.0, "avg_logprob": -0.23191519706479966, "compression_ratio": 1.6081081081081081, "no_speech_prob": 1.892494037747383e-05}, {"id": 508, "seek": 305078, "start": 3071.1800000000003, "end": 3078.78, "text": " Let's start by reading in our feather file. Our feather file is exactly the same as our", "tokens": [961, 311, 722, 538, 3760, 294, 527, 25852, 3991, 13, 2621, 25852, 3991, 307, 2293, 264, 912, 382, 527], "temperature": 0.0, "avg_logprob": -0.23191519706479966, "compression_ratio": 1.6081081081081081, "no_speech_prob": 1.892494037747383e-05}, {"id": 509, "seek": 307878, "start": 3078.78, "end": 3083.6200000000003, "text": " CSV file. This is for our Blue Book for Bulldozers competition. We're trying to predict the sale", "tokens": [48814, 3991, 13, 639, 307, 337, 527, 8510, 9476, 337, 14131, 2595, 41698, 6211, 13, 492, 434, 1382, 281, 6069, 264, 8680], "temperature": 0.0, "avg_logprob": -0.18698928833007813, "compression_ratio": 1.6374045801526718, "no_speech_prob": 1.6442189007648267e-05}, {"id": 510, "seek": 307878, "start": 3083.6200000000003, "end": 3090.6600000000003, "text": " price of heavy industrial equipment at auction. So reading the feather format file means that", "tokens": [3218, 295, 4676, 9987, 5927, 412, 24139, 13, 407, 3760, 264, 25852, 7877, 3991, 1355, 300], "temperature": 0.0, "avg_logprob": -0.18698928833007813, "compression_ratio": 1.6374045801526718, "no_speech_prob": 1.6442189007648267e-05}, {"id": 511, "seek": 307878, "start": 3090.6600000000003, "end": 3096.1800000000003, "text": " we've already read in the CSV and processed it into categories.", "tokens": [321, 600, 1217, 1401, 294, 264, 48814, 293, 18846, 309, 666, 10479, 13], "temperature": 0.0, "avg_logprob": -0.18698928833007813, "compression_ratio": 1.6374045801526718, "no_speech_prob": 1.6442189007648267e-05}, {"id": 512, "seek": 307878, "start": 3096.1800000000003, "end": 3101.5, "text": " So the next thing we do is to run proc df in order to turn the categories into integers,", "tokens": [407, 264, 958, 551, 321, 360, 307, 281, 1190, 9510, 274, 69, 294, 1668, 281, 1261, 264, 10479, 666, 41674, 11], "temperature": 0.0, "avg_logprob": -0.18698928833007813, "compression_ratio": 1.6374045801526718, "no_speech_prob": 1.6442189007648267e-05}, {"id": 513, "seek": 307878, "start": 3101.5, "end": 3107.46, "text": " deal with the missing values and pull out the dependent variable. This is exactly the", "tokens": [2028, 365, 264, 5361, 4190, 293, 2235, 484, 264, 12334, 7006, 13, 639, 307, 2293, 264], "temperature": 0.0, "avg_logprob": -0.18698928833007813, "compression_ratio": 1.6374045801526718, "no_speech_prob": 1.6442189007648267e-05}, {"id": 514, "seek": 310746, "start": 3107.46, "end": 3112.26, "text": " same thing as we used last time to create a validation set, where the validation set", "tokens": [912, 551, 382, 321, 1143, 1036, 565, 281, 1884, 257, 24071, 992, 11, 689, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.12888736724853517, "compression_ratio": 1.5512820512820513, "no_speech_prob": 3.373634899617173e-05}, {"id": 515, "seek": 310746, "start": 3112.26, "end": 3120.42, "text": " represents the last couple of weeks, the last 12,000 records by date.", "tokens": [8855, 264, 1036, 1916, 295, 3259, 11, 264, 1036, 2272, 11, 1360, 7724, 538, 4002, 13], "temperature": 0.0, "avg_logprob": -0.12888736724853517, "compression_ratio": 1.5512820512820513, "no_speech_prob": 3.373634899617173e-05}, {"id": 516, "seek": 310746, "start": 3120.42, "end": 3125.86, "text": " I discovered, thanks to one of your excellent questions on the forum last week, I had a", "tokens": [286, 6941, 11, 3231, 281, 472, 295, 428, 7103, 1651, 322, 264, 17542, 1036, 1243, 11, 286, 632, 257], "temperature": 0.0, "avg_logprob": -0.12888736724853517, "compression_ratio": 1.5512820512820513, "no_speech_prob": 3.373634899617173e-05}, {"id": 517, "seek": 312586, "start": 3125.86, "end": 3143.5, "text": " bug here, which is that proc df was shuffling the order. Last week we saw a particular version", "tokens": [7426, 510, 11, 597, 307, 300, 9510, 274, 69, 390, 402, 1245, 1688, 264, 1668, 13, 5264, 1243, 321, 1866, 257, 1729, 3037], "temperature": 0.0, "avg_logprob": -0.16823124002527307, "compression_ratio": 1.46875, "no_speech_prob": 3.7052213883725926e-05}, {"id": 518, "seek": 312586, "start": 3143.5, "end": 3152.86, "text": " of proc df where we passed in a subset. When I passed in a subset, it was randomly shuffling.", "tokens": [295, 9510, 274, 69, 689, 321, 4678, 294, 257, 25993, 13, 1133, 286, 4678, 294, 257, 25993, 11, 309, 390, 16979, 402, 1245, 1688, 13], "temperature": 0.0, "avg_logprob": -0.16823124002527307, "compression_ratio": 1.46875, "no_speech_prob": 3.7052213883725926e-05}, {"id": 519, "seek": 315286, "start": 3152.86, "end": 3159.02, "text": " So then when I said split-vowels, it wasn't getting the last rows by date, but it was", "tokens": [407, 550, 562, 286, 848, 7472, 12, 85, 305, 1625, 11, 309, 2067, 380, 1242, 264, 1036, 13241, 538, 4002, 11, 457, 309, 390], "temperature": 0.0, "avg_logprob": -0.16356263215514436, "compression_ratio": 1.5560975609756098, "no_speech_prob": 3.8227222830755636e-05}, {"id": 520, "seek": 315286, "start": 3159.02, "end": 3161.98, "text": " getting a random set of rows. So I've now fixed that.", "tokens": [1242, 257, 4974, 992, 295, 13241, 13, 407, 286, 600, 586, 6806, 300, 13], "temperature": 0.0, "avg_logprob": -0.16356263215514436, "compression_ratio": 1.5560975609756098, "no_speech_prob": 3.8227222830755636e-05}, {"id": 521, "seek": 315286, "start": 3161.98, "end": 3169.5, "text": " So if you rerun the Lesson 1 RF code, you'll see slightly different results. Specifically", "tokens": [407, 498, 291, 43819, 409, 264, 18649, 266, 502, 26204, 3089, 11, 291, 603, 536, 4748, 819, 3542, 13, 26058], "temperature": 0.0, "avg_logprob": -0.16356263215514436, "compression_ratio": 1.5560975609756098, "no_speech_prob": 3.8227222830755636e-05}, {"id": 522, "seek": 315286, "start": 3169.5, "end": 3175.42, "text": " you'll see in that section that my validation set results look less good, but that's only", "tokens": [291, 603, 536, 294, 300, 3541, 300, 452, 24071, 992, 3542, 574, 1570, 665, 11, 457, 300, 311, 787], "temperature": 0.0, "avg_logprob": -0.16356263215514436, "compression_ratio": 1.5560975609756098, "no_speech_prob": 3.8227222830755636e-05}, {"id": 523, "seek": 317542, "start": 3175.42, "end": 3184.5, "text": " for this tiny little bit where I had subset equals set.", "tokens": [337, 341, 5870, 707, 857, 689, 286, 632, 25993, 6915, 992, 13], "temperature": 0.0, "avg_logprob": -0.2508327196229179, "compression_ratio": 1.457142857142857, "no_speech_prob": 2.9310751415323466e-05}, {"id": 524, "seek": 317542, "start": 3184.5, "end": 3190.1800000000003, "text": " I'm a little bit confused about the notation here. So nas is both an input variable and", "tokens": [286, 478, 257, 707, 857, 9019, 466, 264, 24657, 510, 13, 407, 5382, 307, 1293, 364, 4846, 7006, 293], "temperature": 0.0, "avg_logprob": -0.2508327196229179, "compression_ratio": 1.457142857142857, "no_speech_prob": 2.9310751415323466e-05}, {"id": 525, "seek": 317542, "start": 3190.1800000000003, "end": 3197.66, "text": " it's also the output variable of this function. Why is that?", "tokens": [309, 311, 611, 264, 5598, 7006, 295, 341, 2445, 13, 1545, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.2508327196229179, "compression_ratio": 1.457142857142857, "no_speech_prob": 2.9310751415323466e-05}, {"id": 526, "seek": 319766, "start": 3197.66, "end": 3206.3799999999997, "text": " The proc df returns a dictionary telling you which columns were missing and for each of", "tokens": [440, 9510, 274, 69, 11247, 257, 25890, 3585, 291, 597, 13766, 645, 5361, 293, 337, 1184, 295], "temperature": 0.0, "avg_logprob": -0.18867557863645915, "compression_ratio": 1.5989304812834224, "no_speech_prob": 1.7778318579075858e-05}, {"id": 527, "seek": 319766, "start": 3206.3799999999997, "end": 3217.62, "text": " those columns what the median was. So when you call it on the larger data set, the non-subset,", "tokens": [729, 13766, 437, 264, 26779, 390, 13, 407, 562, 291, 818, 309, 322, 264, 4833, 1412, 992, 11, 264, 2107, 12, 30131, 3854, 11], "temperature": 0.0, "avg_logprob": -0.18867557863645915, "compression_ratio": 1.5989304812834224, "no_speech_prob": 1.7778318579075858e-05}, {"id": 528, "seek": 319766, "start": 3217.62, "end": 3222.58, "text": " you want to take that return value and you don't pass in an nad to that point, you just", "tokens": [291, 528, 281, 747, 300, 2736, 2158, 293, 291, 500, 380, 1320, 294, 364, 12617, 281, 300, 935, 11, 291, 445], "temperature": 0.0, "avg_logprob": -0.18867557863645915, "compression_ratio": 1.5989304812834224, "no_speech_prob": 1.7778318579075858e-05}, {"id": 529, "seek": 319766, "start": 3222.58, "end": 3224.7, "text": " want to get back the result.", "tokens": [528, 281, 483, 646, 264, 1874, 13], "temperature": 0.0, "avg_logprob": -0.18867557863645915, "compression_ratio": 1.5989304812834224, "no_speech_prob": 1.7778318579075858e-05}, {"id": 530, "seek": 322470, "start": 3224.7, "end": 3229.06, "text": " Later on when you pass it into a subset, you want to have the same missing columns and", "tokens": [11965, 322, 562, 291, 1320, 309, 666, 257, 25993, 11, 291, 528, 281, 362, 264, 912, 5361, 13766, 293], "temperature": 0.0, "avg_logprob": -0.152366042137146, "compression_ratio": 1.8475336322869955, "no_speech_prob": 9.972870429919567e-06}, {"id": 531, "seek": 322470, "start": 3229.06, "end": 3236.7, "text": " the same medians, so you pass it in. And if this different subset, like if it was a whole", "tokens": [264, 912, 1205, 2567, 11, 370, 291, 1320, 309, 294, 13, 400, 498, 341, 819, 25993, 11, 411, 498, 309, 390, 257, 1379], "temperature": 0.0, "avg_logprob": -0.152366042137146, "compression_ratio": 1.8475336322869955, "no_speech_prob": 9.972870429919567e-06}, {"id": 532, "seek": 322470, "start": 3236.7, "end": 3241.02, "text": " different data set, turned out to have some different missing columns, it would update", "tokens": [819, 1412, 992, 11, 3574, 484, 281, 362, 512, 819, 5361, 13766, 11, 309, 576, 5623], "temperature": 0.0, "avg_logprob": -0.152366042137146, "compression_ratio": 1.8475336322869955, "no_speech_prob": 9.972870429919567e-06}, {"id": 533, "seek": 322470, "start": 3241.02, "end": 3246.7799999999997, "text": " that dictionary with additional key values as well.", "tokens": [300, 25890, 365, 4497, 2141, 4190, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.152366042137146, "compression_ratio": 1.8475336322869955, "no_speech_prob": 9.972870429919567e-06}, {"id": 534, "seek": 322470, "start": 3246.7799999999997, "end": 3253.58, "text": " So you don't have to pass it in. If you don't pass it in, then it just gives you the information", "tokens": [407, 291, 500, 380, 362, 281, 1320, 309, 294, 13, 759, 291, 500, 380, 1320, 309, 294, 11, 550, 309, 445, 2709, 291, 264, 1589], "temperature": 0.0, "avg_logprob": -0.152366042137146, "compression_ratio": 1.8475336322869955, "no_speech_prob": 9.972870429919567e-06}, {"id": 535, "seek": 325358, "start": 3253.58, "end": 3258.94, "text": " about what was missing and the medians. If you do pass it in, it uses that information", "tokens": [466, 437, 390, 5361, 293, 264, 1205, 2567, 13, 759, 291, 360, 1320, 309, 294, 11, 309, 4960, 300, 1589], "temperature": 0.0, "avg_logprob": -0.26321090619588633, "compression_ratio": 1.8440366972477065, "no_speech_prob": 2.178232534788549e-05}, {"id": 536, "seek": 325358, "start": 3258.94, "end": 3264.86, "text": " for any missing columns that are there, and if there are some new missing columns, it", "tokens": [337, 604, 5361, 13766, 300, 366, 456, 11, 293, 498, 456, 366, 512, 777, 5361, 13766, 11, 309], "temperature": 0.0, "avg_logprob": -0.26321090619588633, "compression_ratio": 1.8440366972477065, "no_speech_prob": 2.178232534788549e-05}, {"id": 537, "seek": 325358, "start": 3264.86, "end": 3268.74, "text": " will update that dictionary with that additional information.", "tokens": [486, 5623, 300, 25890, 365, 300, 4497, 1589, 13], "temperature": 0.0, "avg_logprob": -0.26321090619588633, "compression_ratio": 1.8440366972477065, "no_speech_prob": 2.178232534788549e-05}, {"id": 538, "seek": 325358, "start": 3268.74, "end": 3271.98, "text": " So it's like keeping all data sets, column information.", "tokens": [407, 309, 311, 411, 5145, 439, 1412, 6352, 11, 7738, 1589, 13], "temperature": 0.0, "avg_logprob": -0.26321090619588633, "compression_ratio": 1.8440366972477065, "no_speech_prob": 2.178232534788549e-05}, {"id": 539, "seek": 325358, "start": 3271.98, "end": 3276.66, "text": " Yes, it's going to keep track of any missing columns that you came across in anything you", "tokens": [1079, 11, 309, 311, 516, 281, 1066, 2837, 295, 604, 5361, 13766, 300, 291, 1361, 2108, 294, 1340, 291], "temperature": 0.0, "avg_logprob": -0.26321090619588633, "compression_ratio": 1.8440366972477065, "no_speech_prob": 2.178232534788549e-05}, {"id": 540, "seek": 325358, "start": 3276.66, "end": 3282.1, "text": " passed a property at.", "tokens": [4678, 257, 4707, 412, 13], "temperature": 0.0, "avg_logprob": -0.26321090619588633, "compression_ratio": 1.8440366972477065, "no_speech_prob": 2.178232534788549e-05}, {"id": 541, "seek": 328210, "start": 3282.1, "end": 3288.14, "text": " So we split it into the training and test set just like we did last week. So to remind", "tokens": [407, 321, 7472, 309, 666, 264, 3097, 293, 1500, 992, 445, 411, 321, 630, 1036, 1243, 13, 407, 281, 4160], "temperature": 0.0, "avg_logprob": -0.15408689897138994, "compression_ratio": 1.6267281105990783, "no_speech_prob": 2.9023008210060652e-06}, {"id": 542, "seek": 328210, "start": 3288.14, "end": 3295.74, "text": " you, once we've done prop.df, this is what it looks like, this is the log of sale price.", "tokens": [291, 11, 1564, 321, 600, 1096, 2365, 13, 45953, 11, 341, 307, 437, 309, 1542, 411, 11, 341, 307, 264, 3565, 295, 8680, 3218, 13], "temperature": 0.0, "avg_logprob": -0.15408689897138994, "compression_ratio": 1.6267281105990783, "no_speech_prob": 2.9023008210060652e-06}, {"id": 543, "seek": 328210, "start": 3295.74, "end": 3302.3399999999997, "text": " So the first thing to think about is we already know how to get the predictions, which is", "tokens": [407, 264, 700, 551, 281, 519, 466, 307, 321, 1217, 458, 577, 281, 483, 264, 21264, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.15408689897138994, "compression_ratio": 1.6267281105990783, "no_speech_prob": 2.9023008210060652e-06}, {"id": 544, "seek": 328210, "start": 3302.3399999999997, "end": 3311.54, "text": " we take the average value in each leaf node in each tree after running a particular row", "tokens": [321, 747, 264, 4274, 2158, 294, 1184, 10871, 9984, 294, 1184, 4230, 934, 2614, 257, 1729, 5386], "temperature": 0.0, "avg_logprob": -0.15408689897138994, "compression_ratio": 1.6267281105990783, "no_speech_prob": 2.9023008210060652e-06}, {"id": 545, "seek": 331154, "start": 3311.54, "end": 3316.54, "text": " through each tree. That's how we get the prediction.", "tokens": [807, 1184, 4230, 13, 663, 311, 577, 321, 483, 264, 17630, 13], "temperature": 0.0, "avg_logprob": -0.10368126253538494, "compression_ratio": 1.880952380952381, "no_speech_prob": 4.02943987865001e-06}, {"id": 546, "seek": 331154, "start": 3316.54, "end": 3321.54, "text": " But normally we don't just want a prediction, we also want to know how confident we are", "tokens": [583, 5646, 321, 500, 380, 445, 528, 257, 17630, 11, 321, 611, 528, 281, 458, 577, 6679, 321, 366], "temperature": 0.0, "avg_logprob": -0.10368126253538494, "compression_ratio": 1.880952380952381, "no_speech_prob": 4.02943987865001e-06}, {"id": 547, "seek": 331154, "start": 3321.54, "end": 3328.5, "text": " of that prediction. And so we would be less confident of a prediction if we haven't seen", "tokens": [295, 300, 17630, 13, 400, 370, 321, 576, 312, 1570, 6679, 295, 257, 17630, 498, 321, 2378, 380, 1612], "temperature": 0.0, "avg_logprob": -0.10368126253538494, "compression_ratio": 1.880952380952381, "no_speech_prob": 4.02943987865001e-06}, {"id": 548, "seek": 331154, "start": 3328.5, "end": 3336.1, "text": " many examples of rows like this one. And if we haven't seen many examples of rows like", "tokens": [867, 5110, 295, 13241, 411, 341, 472, 13, 400, 498, 321, 2378, 380, 1612, 867, 5110, 295, 13241, 411], "temperature": 0.0, "avg_logprob": -0.10368126253538494, "compression_ratio": 1.880952380952381, "no_speech_prob": 4.02943987865001e-06}, {"id": 549, "seek": 333610, "start": 3336.1, "end": 3343.58, "text": " this one, then we wouldn't expect any of the trees to have a path through which is really", "tokens": [341, 472, 11, 550, 321, 2759, 380, 2066, 604, 295, 264, 5852, 281, 362, 257, 3100, 807, 597, 307, 534], "temperature": 0.0, "avg_logprob": -0.10332546842859146, "compression_ratio": 1.738938053097345, "no_speech_prob": 5.173882982489886e-06}, {"id": 550, "seek": 333610, "start": 3343.58, "end": 3347.3399999999997, "text": " designed to help us predict that row.", "tokens": [4761, 281, 854, 505, 6069, 300, 5386, 13], "temperature": 0.0, "avg_logprob": -0.10332546842859146, "compression_ratio": 1.738938053097345, "no_speech_prob": 5.173882982489886e-06}, {"id": 551, "seek": 333610, "start": 3347.3399999999997, "end": 3352.5, "text": " And so conceptually you would expect then that as you pass this unusual row through", "tokens": [400, 370, 3410, 671, 291, 576, 2066, 550, 300, 382, 291, 1320, 341, 10901, 5386, 807], "temperature": 0.0, "avg_logprob": -0.10332546842859146, "compression_ratio": 1.738938053097345, "no_speech_prob": 5.173882982489886e-06}, {"id": 552, "seek": 333610, "start": 3352.5, "end": 3359.98, "text": " different trees, it's going to end up in very different places. So in other words, rather", "tokens": [819, 5852, 11, 309, 311, 516, 281, 917, 493, 294, 588, 819, 3190, 13, 407, 294, 661, 2283, 11, 2831], "temperature": 0.0, "avg_logprob": -0.10332546842859146, "compression_ratio": 1.738938053097345, "no_speech_prob": 5.173882982489886e-06}, {"id": 553, "seek": 333610, "start": 3359.98, "end": 3365.22, "text": " than just taking the mean of the predictions of the trees and saying that's our prediction,", "tokens": [813, 445, 1940, 264, 914, 295, 264, 21264, 295, 264, 5852, 293, 1566, 300, 311, 527, 17630, 11], "temperature": 0.0, "avg_logprob": -0.10332546842859146, "compression_ratio": 1.738938053097345, "no_speech_prob": 5.173882982489886e-06}, {"id": 554, "seek": 336522, "start": 3365.22, "end": 3371.1, "text": " what if we took the standard deviation of the predictions of the trees? So the standard", "tokens": [437, 498, 321, 1890, 264, 3832, 25163, 295, 264, 21264, 295, 264, 5852, 30, 407, 264, 3832], "temperature": 0.0, "avg_logprob": -0.10565848099557977, "compression_ratio": 1.8381502890173411, "no_speech_prob": 6.0488900999189354e-06}, {"id": 555, "seek": 336522, "start": 3371.1, "end": 3379.18, "text": " deviation of the predictions of the trees, if that's high, that means each tree is giving", "tokens": [25163, 295, 264, 21264, 295, 264, 5852, 11, 498, 300, 311, 1090, 11, 300, 1355, 1184, 4230, 307, 2902], "temperature": 0.0, "avg_logprob": -0.10565848099557977, "compression_ratio": 1.8381502890173411, "no_speech_prob": 6.0488900999189354e-06}, {"id": 556, "seek": 336522, "start": 3379.18, "end": 3384.74, "text": " us a very different estimate of this row's prediction.", "tokens": [505, 257, 588, 819, 12539, 295, 341, 5386, 311, 17630, 13], "temperature": 0.0, "avg_logprob": -0.10565848099557977, "compression_ratio": 1.8381502890173411, "no_speech_prob": 6.0488900999189354e-06}, {"id": 557, "seek": 336522, "start": 3384.74, "end": 3393.0, "text": " So if this was a really common kind of row, then the trees would have learned to make", "tokens": [407, 498, 341, 390, 257, 534, 2689, 733, 295, 5386, 11, 550, 264, 5852, 576, 362, 3264, 281, 652], "temperature": 0.0, "avg_logprob": -0.10565848099557977, "compression_ratio": 1.8381502890173411, "no_speech_prob": 6.0488900999189354e-06}, {"id": 558, "seek": 339300, "start": 3393.0, "end": 3397.64, "text": " good predictions for it because it's seen lots of opportunities to split based on those", "tokens": [665, 21264, 337, 309, 570, 309, 311, 1612, 3195, 295, 4786, 281, 7472, 2361, 322, 729], "temperature": 0.0, "avg_logprob": -0.1004207900592259, "compression_ratio": 1.6273291925465838, "no_speech_prob": 4.092877134098671e-06}, {"id": 559, "seek": 339300, "start": 3397.64, "end": 3405.9, "text": " kinds of rows. So the standard deviation of the predictions across the trees gives us", "tokens": [3685, 295, 13241, 13, 407, 264, 3832, 25163, 295, 264, 21264, 2108, 264, 5852, 2709, 505], "temperature": 0.0, "avg_logprob": -0.1004207900592259, "compression_ratio": 1.6273291925465838, "no_speech_prob": 4.092877134098671e-06}, {"id": 560, "seek": 339300, "start": 3405.9, "end": 3416.02, "text": " some kind of at least relative understanding of how confident we are of this prediction.", "tokens": [512, 733, 295, 412, 1935, 4972, 3701, 295, 577, 6679, 321, 366, 295, 341, 17630, 13], "temperature": 0.0, "avg_logprob": -0.1004207900592259, "compression_ratio": 1.6273291925465838, "no_speech_prob": 4.092877134098671e-06}, {"id": 561, "seek": 341602, "start": 3416.02, "end": 3427.06, "text": " So that is not something which exists in Scikit-learn or in any library I know of, so we have to", "tokens": [407, 300, 307, 406, 746, 597, 8198, 294, 16942, 22681, 12, 306, 1083, 420, 294, 604, 6405, 286, 458, 295, 11, 370, 321, 362, 281], "temperature": 0.0, "avg_logprob": -0.13579673767089845, "compression_ratio": 1.5794392523364487, "no_speech_prob": 1.3081695442451746e-06}, {"id": 562, "seek": 341602, "start": 3427.06, "end": 3431.66, "text": " create it. But we already have almost the exact code we need, because remember last", "tokens": [1884, 309, 13, 583, 321, 1217, 362, 1920, 264, 1900, 3089, 321, 643, 11, 570, 1604, 1036], "temperature": 0.0, "avg_logprob": -0.13579673767089845, "compression_ratio": 1.5794392523364487, "no_speech_prob": 1.3081695442451746e-06}, {"id": 563, "seek": 341602, "start": 3431.66, "end": 3437.46, "text": " lesson we actually manually calculated the averages across different sets of trees, so", "tokens": [6898, 321, 767, 16945, 15598, 264, 42257, 2108, 819, 6352, 295, 5852, 11, 370], "temperature": 0.0, "avg_logprob": -0.13579673767089845, "compression_ratio": 1.5794392523364487, "no_speech_prob": 1.3081695442451746e-06}, {"id": 564, "seek": 341602, "start": 3437.46, "end": 3441.82, "text": " we can do exactly the same thing to calculate the standard deviations.", "tokens": [321, 393, 360, 2293, 264, 912, 551, 281, 8873, 264, 3832, 31219, 763, 13], "temperature": 0.0, "avg_logprob": -0.13579673767089845, "compression_ratio": 1.5794392523364487, "no_speech_prob": 1.3081695442451746e-06}, {"id": 565, "seek": 344182, "start": 3441.82, "end": 3448.54, "text": " So when I'm doing random forest interpretation, I pretty much never use the full data set.", "tokens": [407, 562, 286, 478, 884, 4974, 6719, 14174, 11, 286, 1238, 709, 1128, 764, 264, 1577, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.14215025901794434, "compression_ratio": 1.5874439461883407, "no_speech_prob": 2.6425796022522263e-06}, {"id": 566, "seek": 344182, "start": 3448.54, "end": 3456.2200000000003, "text": " I always call setRFSamples because we don't need a massively accurate random forest, we", "tokens": [286, 1009, 818, 992, 49, 37, 28743, 2622, 570, 321, 500, 380, 643, 257, 29379, 8559, 4974, 6719, 11, 321], "temperature": 0.0, "avg_logprob": -0.14215025901794434, "compression_ratio": 1.5874439461883407, "no_speech_prob": 2.6425796022522263e-06}, {"id": 567, "seek": 344182, "start": 3456.2200000000003, "end": 3463.1000000000004, "text": " just need one which indicates the nature of the relationships involved. So I just make", "tokens": [445, 643, 472, 597, 16203, 264, 3687, 295, 264, 6159, 3288, 13, 407, 286, 445, 652], "temperature": 0.0, "avg_logprob": -0.14215025901794434, "compression_ratio": 1.5874439461883407, "no_speech_prob": 2.6425796022522263e-06}, {"id": 568, "seek": 344182, "start": 3463.1000000000004, "end": 3469.06, "text": " sure this number is high enough that if I call the same interpretation commands multiple", "tokens": [988, 341, 1230, 307, 1090, 1547, 300, 498, 286, 818, 264, 912, 14174, 16901, 3866], "temperature": 0.0, "avg_logprob": -0.14215025901794434, "compression_ratio": 1.5874439461883407, "no_speech_prob": 2.6425796022522263e-06}, {"id": 569, "seek": 346906, "start": 3469.06, "end": 3473.94, "text": " times, I don't get different results back each time. That's like the rule of thumb about", "tokens": [1413, 11, 286, 500, 380, 483, 819, 3542, 646, 1184, 565, 13, 663, 311, 411, 264, 4978, 295, 9298, 466], "temperature": 0.0, "avg_logprob": -0.14518931278815636, "compression_ratio": 1.524793388429752, "no_speech_prob": 3.7853139929211466e-06}, {"id": 570, "seek": 346906, "start": 3473.94, "end": 3480.42, "text": " how big does it need to be. But in practice, 50,000 is a high number, and most of the time", "tokens": [577, 955, 775, 309, 643, 281, 312, 13, 583, 294, 3124, 11, 2625, 11, 1360, 307, 257, 1090, 1230, 11, 293, 881, 295, 264, 565], "temperature": 0.0, "avg_logprob": -0.14518931278815636, "compression_ratio": 1.524793388429752, "no_speech_prob": 3.7853139929211466e-06}, {"id": 571, "seek": 346906, "start": 3480.42, "end": 3486.2999999999997, "text": " it would be surprising if that wasn't enough, and it runs in seconds. So I generally start", "tokens": [309, 576, 312, 8830, 498, 300, 2067, 380, 1547, 11, 293, 309, 6676, 294, 3949, 13, 407, 286, 5101, 722], "temperature": 0.0, "avg_logprob": -0.14518931278815636, "compression_ratio": 1.524793388429752, "no_speech_prob": 3.7853139929211466e-06}, {"id": 572, "seek": 346906, "start": 3486.2999999999997, "end": 3488.06, "text": " with 50,000.", "tokens": [365, 2625, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.14518931278815636, "compression_ratio": 1.524793388429752, "no_speech_prob": 3.7853139929211466e-06}, {"id": 573, "seek": 346906, "start": 3488.06, "end": 3495.2999999999997, "text": " So with my 50,000 samples per tree set, I create 40 estimators. I know from last time", "tokens": [407, 365, 452, 2625, 11, 1360, 10938, 680, 4230, 992, 11, 286, 1884, 3356, 8017, 3391, 13, 286, 458, 490, 1036, 565], "temperature": 0.0, "avg_logprob": -0.14518931278815636, "compression_ratio": 1.524793388429752, "no_speech_prob": 3.7853139929211466e-06}, {"id": 574, "seek": 349530, "start": 3495.3, "end": 3500.2200000000003, "text": " that minSamplesLeaf equals 3, maxFeatures equals.5 isn't bad, and again we're not trying", "tokens": [300, 923, 28743, 2622, 11020, 2792, 6915, 805, 11, 11469, 29341, 3377, 6915, 2411, 20, 1943, 380, 1578, 11, 293, 797, 321, 434, 406, 1382], "temperature": 0.0, "avg_logprob": -0.18609098630530813, "compression_ratio": 1.5, "no_speech_prob": 1.1300696314719971e-05}, {"id": 575, "seek": 349530, "start": 3500.2200000000003, "end": 3506.1000000000004, "text": " to create the world's most predictive tree anyway, so that all sounds fine. We get an", "tokens": [281, 1884, 264, 1002, 311, 881, 35521, 4230, 4033, 11, 370, 300, 439, 3263, 2489, 13, 492, 483, 364], "temperature": 0.0, "avg_logprob": -0.18609098630530813, "compression_ratio": 1.5, "no_speech_prob": 1.1300696314719971e-05}, {"id": 576, "seek": 349530, "start": 3506.1000000000004, "end": 3512.3, "text": " R squared on the validation set of.89. Again, we don't particularly care, but as long as", "tokens": [497, 8889, 322, 264, 24071, 992, 295, 2411, 21115, 13, 3764, 11, 321, 500, 380, 4098, 1127, 11, 457, 382, 938, 382], "temperature": 0.0, "avg_logprob": -0.18609098630530813, "compression_ratio": 1.5, "no_speech_prob": 1.1300696314719971e-05}, {"id": 577, "seek": 349530, "start": 3512.3, "end": 3515.42, "text": " it's good enough, which it certainly is.", "tokens": [309, 311, 665, 1547, 11, 597, 309, 3297, 307, 13], "temperature": 0.0, "avg_logprob": -0.18609098630530813, "compression_ratio": 1.5, "no_speech_prob": 1.1300696314719971e-05}, {"id": 578, "seek": 349530, "start": 3515.42, "end": 3520.1800000000003, "text": " And so here's where we can do that exact same list comprehension as last time. Remember,", "tokens": [400, 370, 510, 311, 689, 321, 393, 360, 300, 1900, 912, 1329, 44991, 382, 1036, 565, 13, 5459, 11], "temperature": 0.0, "avg_logprob": -0.18609098630530813, "compression_ratio": 1.5, "no_speech_prob": 1.1300696314719971e-05}, {"id": 579, "seek": 352018, "start": 3520.18, "end": 3526.3799999999997, "text": " go through each estimator, that's each tree, call.predict on it with our validation set,", "tokens": [352, 807, 1184, 8017, 1639, 11, 300, 311, 1184, 4230, 11, 818, 2411, 79, 24945, 322, 309, 365, 527, 24071, 992, 11], "temperature": 0.0, "avg_logprob": -0.16278900418962752, "compression_ratio": 1.6428571428571428, "no_speech_prob": 7.183219622675097e-06}, {"id": 580, "seek": 352018, "start": 3526.3799999999997, "end": 3531.98, "text": " make that a list comprehension and pass that to np.stack, which concatenates everything", "tokens": [652, 300, 257, 1329, 44991, 293, 1320, 300, 281, 33808, 13, 372, 501, 11, 597, 1588, 7186, 1024, 1203], "temperature": 0.0, "avg_logprob": -0.16278900418962752, "compression_ratio": 1.6428571428571428, "no_speech_prob": 7.183219622675097e-06}, {"id": 581, "seek": 352018, "start": 3531.98, "end": 3539.54, "text": " in that list across a new axis. So now our rows are the results of each tree and our", "tokens": [294, 300, 1329, 2108, 257, 777, 10298, 13, 407, 586, 527, 13241, 366, 264, 3542, 295, 1184, 4230, 293, 527], "temperature": 0.0, "avg_logprob": -0.16278900418962752, "compression_ratio": 1.6428571428571428, "no_speech_prob": 7.183219622675097e-06}, {"id": 582, "seek": 352018, "start": 3539.54, "end": 3543.7999999999997, "text": " columns are the result of each row in the original data set.", "tokens": [13766, 366, 264, 1874, 295, 1184, 5386, 294, 264, 3380, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.16278900418962752, "compression_ratio": 1.6428571428571428, "no_speech_prob": 7.183219622675097e-06}, {"id": 583, "seek": 354380, "start": 3543.8, "end": 3550.34, "text": " And then we remember we can calculate the mean. So here's the prediction for our data", "tokens": [400, 550, 321, 1604, 321, 393, 8873, 264, 914, 13, 407, 510, 311, 264, 17630, 337, 527, 1412], "temperature": 0.0, "avg_logprob": -0.24981321415431063, "compression_ratio": 1.656441717791411, "no_speech_prob": 1.436745037608489e-06}, {"id": 584, "seek": 354380, "start": 3550.34, "end": 3556.5800000000004, "text": " set row number 1, and here's our standard deviation. So here's how to do it for just", "tokens": [992, 5386, 1230, 502, 11, 293, 510, 311, 527, 3832, 25163, 13, 407, 510, 311, 577, 281, 360, 309, 337, 445], "temperature": 0.0, "avg_logprob": -0.24981321415431063, "compression_ratio": 1.656441717791411, "no_speech_prob": 1.436745037608489e-06}, {"id": 585, "seek": 354380, "start": 3556.5800000000004, "end": 3565.1400000000003, "text": " one observation at the end here. We've calculated for all of them, just predicting it for one", "tokens": [472, 14816, 412, 264, 917, 510, 13, 492, 600, 15598, 337, 439, 295, 552, 11, 445, 32884, 309, 337, 472], "temperature": 0.0, "avg_logprob": -0.24981321415431063, "compression_ratio": 1.656441717791411, "no_speech_prob": 1.436745037608489e-06}, {"id": 586, "seek": 354380, "start": 3565.1400000000003, "end": 3567.3, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.24981321415431063, "compression_ratio": 1.656441717791411, "no_speech_prob": 1.436745037608489e-06}, {"id": 587, "seek": 356730, "start": 3567.3, "end": 3575.7400000000002, "text": " This can take quite a while, and specifically it's not taking advantage of the fact that", "tokens": [639, 393, 747, 1596, 257, 1339, 11, 293, 4682, 309, 311, 406, 1940, 5002, 295, 264, 1186, 300], "temperature": 0.0, "avg_logprob": -0.17008794844150543, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.8738717244559666e-06}, {"id": 588, "seek": 356730, "start": 3575.7400000000002, "end": 3586.42, "text": " my computer has lots of cores in it. The list comprehension itself is Python code, it's", "tokens": [452, 3820, 575, 3195, 295, 24826, 294, 309, 13, 440, 1329, 44991, 2564, 307, 15329, 3089, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.17008794844150543, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.8738717244559666e-06}, {"id": 589, "seek": 356730, "start": 3586.42, "end": 3593.02, "text": " my Python code. Python code, unless you're doing special stuff, runs in serial, which", "tokens": [452, 15329, 3089, 13, 15329, 3089, 11, 5969, 291, 434, 884, 2121, 1507, 11, 6676, 294, 17436, 11, 597], "temperature": 0.0, "avg_logprob": -0.17008794844150543, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.8738717244559666e-06}, {"id": 590, "seek": 359302, "start": 3593.02, "end": 3598.7, "text": " means it runs on a single CPU. It doesn't take advantage of your multi-CPU hardware.", "tokens": [1355, 309, 6676, 322, 257, 2167, 13199, 13, 467, 1177, 380, 747, 5002, 295, 428, 4825, 12, 34, 8115, 8837, 13], "temperature": 0.0, "avg_logprob": -0.14869392728342593, "compression_ratio": 1.663716814159292, "no_speech_prob": 9.81826178758638e-06}, {"id": 591, "seek": 359302, "start": 3598.7, "end": 3605.02, "text": " And so if I wanted to run this on more trees and more data, this one second is going to", "tokens": [400, 370, 498, 286, 1415, 281, 1190, 341, 322, 544, 5852, 293, 544, 1412, 11, 341, 472, 1150, 307, 516, 281], "temperature": 0.0, "avg_logprob": -0.14869392728342593, "compression_ratio": 1.663716814159292, "no_speech_prob": 9.81826178758638e-06}, {"id": 592, "seek": 359302, "start": 3605.02, "end": 3609.38, "text": " go up. And you see here the wall time, the amount of actual time it took, is roughly", "tokens": [352, 493, 13, 400, 291, 536, 510, 264, 2929, 565, 11, 264, 2372, 295, 3539, 565, 309, 1890, 11, 307, 9810], "temperature": 0.0, "avg_logprob": -0.14869392728342593, "compression_ratio": 1.663716814159292, "no_speech_prob": 9.81826178758638e-06}, {"id": 593, "seek": 359302, "start": 3609.38, "end": 3614.7, "text": " equal to the CPU time, where else if it was running on lots of cores, the CPU time would", "tokens": [2681, 281, 264, 13199, 565, 11, 689, 1646, 498, 309, 390, 2614, 322, 3195, 295, 24826, 11, 264, 13199, 565, 576], "temperature": 0.0, "avg_logprob": -0.14869392728342593, "compression_ratio": 1.663716814159292, "no_speech_prob": 9.81826178758638e-06}, {"id": 594, "seek": 359302, "start": 3614.7, "end": 3616.9, "text": " be higher than the wall time.", "tokens": [312, 2946, 813, 264, 2929, 565, 13], "temperature": 0.0, "avg_logprob": -0.14869392728342593, "compression_ratio": 1.663716814159292, "no_speech_prob": 9.81826178758638e-06}, {"id": 595, "seek": 361690, "start": 3616.9, "end": 3629.62, "text": " So it turns out that Fast.ai provides a handy function called parallelTrees, which calls", "tokens": [407, 309, 4523, 484, 300, 15968, 13, 1301, 6417, 257, 13239, 2445, 1219, 8952, 51, 4856, 11, 597, 5498], "temperature": 0.0, "avg_logprob": -0.15295151098450618, "compression_ratio": 1.5739644970414202, "no_speech_prob": 2.813008677549078e-06}, {"id": 596, "seek": 361690, "start": 3629.62, "end": 3635.7000000000003, "text": " some stuff inside scikit-learn. And parallelTrees takes two things. It takes a random forest", "tokens": [512, 1507, 1854, 2180, 22681, 12, 306, 1083, 13, 400, 8952, 51, 4856, 2516, 732, 721, 13, 467, 2516, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.15295151098450618, "compression_ratio": 1.5739644970414202, "no_speech_prob": 2.813008677549078e-06}, {"id": 597, "seek": 361690, "start": 3635.7000000000003, "end": 3645.38, "text": " model that I trained, and some function to call. And it calls that function on every", "tokens": [2316, 300, 286, 8895, 11, 293, 512, 2445, 281, 818, 13, 400, 309, 5498, 300, 2445, 322, 633], "temperature": 0.0, "avg_logprob": -0.15295151098450618, "compression_ratio": 1.5739644970414202, "no_speech_prob": 2.813008677549078e-06}, {"id": 598, "seek": 364538, "start": 3645.38, "end": 3651.46, "text": " tree in parallel. So in other words, rather than calling t.predictXvalid, let's create", "tokens": [4230, 294, 8952, 13, 407, 294, 661, 2283, 11, 2831, 813, 5141, 256, 13, 79, 24945, 55, 3337, 327, 11, 718, 311, 1884], "temperature": 0.0, "avg_logprob": -0.128030691254005, "compression_ratio": 1.6521739130434783, "no_speech_prob": 6.240910352062201e-06}, {"id": 599, "seek": 364538, "start": 3651.46, "end": 3657.98, "text": " a function that calls t.predictXvalid. Let's use parallelTrees to call it on our model", "tokens": [257, 2445, 300, 5498, 256, 13, 79, 24945, 55, 3337, 327, 13, 961, 311, 764, 8952, 51, 4856, 281, 818, 309, 322, 527, 2316], "temperature": 0.0, "avg_logprob": -0.128030691254005, "compression_ratio": 1.6521739130434783, "no_speech_prob": 6.240910352062201e-06}, {"id": 600, "seek": 364538, "start": 3657.98, "end": 3665.42, "text": " for every tree, and it will return a list of the result of applying that function to", "tokens": [337, 633, 4230, 11, 293, 309, 486, 2736, 257, 1329, 295, 264, 1874, 295, 9275, 300, 2445, 281], "temperature": 0.0, "avg_logprob": -0.128030691254005, "compression_ratio": 1.6521739130434783, "no_speech_prob": 6.240910352062201e-06}, {"id": 601, "seek": 364538, "start": 3665.42, "end": 3669.34, "text": " every tree. And so then we can np.stack that.", "tokens": [633, 4230, 13, 400, 370, 550, 321, 393, 33808, 13, 372, 501, 300, 13], "temperature": 0.0, "avg_logprob": -0.128030691254005, "compression_ratio": 1.6521739130434783, "no_speech_prob": 6.240910352062201e-06}, {"id": 602, "seek": 366934, "start": 3669.34, "end": 3676.1800000000003, "text": " So hopefully you can see that that code and that code are basically the same thing. But", "tokens": [407, 4696, 291, 393, 536, 300, 300, 3089, 293, 300, 3089, 366, 1936, 264, 912, 551, 13, 583], "temperature": 0.0, "avg_logprob": -0.14760144908776443, "compression_ratio": 1.6261261261261262, "no_speech_prob": 7.527960406150669e-06}, {"id": 603, "seek": 366934, "start": 3676.1800000000003, "end": 3683.42, "text": " this one is doing it in parallel. And so you can see here now our wall time has gone down", "tokens": [341, 472, 307, 884, 309, 294, 8952, 13, 400, 370, 291, 393, 536, 510, 586, 527, 2929, 565, 575, 2780, 760], "temperature": 0.0, "avg_logprob": -0.14760144908776443, "compression_ratio": 1.6261261261261262, "no_speech_prob": 7.527960406150669e-06}, {"id": 604, "seek": 366934, "start": 3683.42, "end": 3693.46, "text": " to 500 milliseconds, and it's now giving us exactly the same answer. So a little bit faster.", "tokens": [281, 5923, 34184, 11, 293, 309, 311, 586, 2902, 505, 2293, 264, 912, 1867, 13, 407, 257, 707, 857, 4663, 13], "temperature": 0.0, "avg_logprob": -0.14760144908776443, "compression_ratio": 1.6261261261261262, "no_speech_prob": 7.527960406150669e-06}, {"id": 605, "seek": 366934, "start": 3693.46, "end": 3698.26, "text": " Time permitting, we'll talk about more general ways of writing code that runs in parallel,", "tokens": [6161, 4784, 2414, 11, 321, 603, 751, 466, 544, 2674, 2098, 295, 3579, 3089, 300, 6676, 294, 8952, 11], "temperature": 0.0, "avg_logprob": -0.14760144908776443, "compression_ratio": 1.6261261261261262, "no_speech_prob": 7.527960406150669e-06}, {"id": 606, "seek": 369826, "start": 3698.26, "end": 3703.1800000000003, "text": " because it turns out to be super useful for data science. But here's one that we can use", "tokens": [570, 309, 4523, 484, 281, 312, 1687, 4420, 337, 1412, 3497, 13, 583, 510, 311, 472, 300, 321, 393, 764], "temperature": 0.0, "avg_logprob": -0.12480407566219182, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.2218993106216658e-05}, {"id": 607, "seek": 369826, "start": 3703.1800000000003, "end": 3708.5400000000004, "text": " that's very specific to random forests.", "tokens": [300, 311, 588, 2685, 281, 4974, 21700, 13], "temperature": 0.0, "avg_logprob": -0.12480407566219182, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.2218993106216658e-05}, {"id": 608, "seek": 369826, "start": 3708.5400000000004, "end": 3716.26, "text": " So what we can now do is we can always call this to get our predictions for each tree,", "tokens": [407, 437, 321, 393, 586, 360, 307, 321, 393, 1009, 818, 341, 281, 483, 527, 21264, 337, 1184, 4230, 11], "temperature": 0.0, "avg_logprob": -0.12480407566219182, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.2218993106216658e-05}, {"id": 609, "seek": 369826, "start": 3716.26, "end": 3722.9, "text": " and then we can call standard deviation to then get them for every row. And so let's", "tokens": [293, 550, 321, 393, 818, 3832, 25163, 281, 550, 483, 552, 337, 633, 5386, 13, 400, 370, 718, 311], "temperature": 0.0, "avg_logprob": -0.12480407566219182, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.2218993106216658e-05}, {"id": 610, "seek": 372290, "start": 3722.9, "end": 3728.94, "text": " try using that. So what I could do is let's create a copy of our data and let's add an", "tokens": [853, 1228, 300, 13, 407, 437, 286, 727, 360, 307, 718, 311, 1884, 257, 5055, 295, 527, 1412, 293, 718, 311, 909, 364], "temperature": 0.0, "avg_logprob": -0.1235597170316256, "compression_ratio": 1.5705521472392638, "no_speech_prob": 1.994724243559176e-06}, {"id": 611, "seek": 372290, "start": 3728.94, "end": 3734.94, "text": " additional column to it, which is the standard deviation of the predictions across the first", "tokens": [4497, 7738, 281, 309, 11, 597, 307, 264, 3832, 25163, 295, 264, 21264, 2108, 264, 700], "temperature": 0.0, "avg_logprob": -0.1235597170316256, "compression_ratio": 1.5705521472392638, "no_speech_prob": 1.994724243559176e-06}, {"id": 612, "seek": 372290, "start": 3734.94, "end": 3745.08, "text": " axis. And let's also add in the mean, so they're the predictions themselves.", "tokens": [10298, 13, 400, 718, 311, 611, 909, 294, 264, 914, 11, 370, 436, 434, 264, 21264, 2969, 13], "temperature": 0.0, "avg_logprob": -0.1235597170316256, "compression_ratio": 1.5705521472392638, "no_speech_prob": 1.994724243559176e-06}, {"id": 613, "seek": 374508, "start": 3745.08, "end": 3754.7, "text": " So you might remember from last lesson that one of the predictors we have is called enclosure,", "tokens": [407, 291, 1062, 1604, 490, 1036, 6898, 300, 472, 295, 264, 6069, 830, 321, 362, 307, 1219, 34093, 11], "temperature": 0.0, "avg_logprob": -0.18139112260606555, "compression_ratio": 1.5799086757990868, "no_speech_prob": 5.507566584128654e-06}, {"id": 614, "seek": 374508, "start": 3754.7, "end": 3759.18, "text": " and we'll see later on that this is an important predictor. And so let's start by just doing", "tokens": [293, 321, 603, 536, 1780, 322, 300, 341, 307, 364, 1021, 6069, 284, 13, 400, 370, 718, 311, 722, 538, 445, 884], "temperature": 0.0, "avg_logprob": -0.18139112260606555, "compression_ratio": 1.5799086757990868, "no_speech_prob": 5.507566584128654e-06}, {"id": 615, "seek": 374508, "start": 3759.18, "end": 3764.9, "text": " a histogram. So one of the nice things in Pandas is it's got built-in plotting capabilities.", "tokens": [257, 49816, 13, 407, 472, 295, 264, 1481, 721, 294, 16995, 296, 307, 309, 311, 658, 3094, 12, 259, 41178, 10862, 13], "temperature": 0.0, "avg_logprob": -0.18139112260606555, "compression_ratio": 1.5799086757990868, "no_speech_prob": 5.507566584128654e-06}, {"id": 616, "seek": 374508, "start": 3764.9, "end": 3769.54, "text": " It's well worth Googling for Pandas plotting to see how to do it.", "tokens": [467, 311, 731, 3163, 45005, 1688, 337, 16995, 296, 41178, 281, 536, 577, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.18139112260606555, "compression_ratio": 1.5799086757990868, "no_speech_prob": 5.507566584128654e-06}, {"id": 617, "seek": 376954, "start": 3769.54, "end": 3784.46, "text": " So we don't know what it means, and it doesn't matter. I guess the whole purpose of this", "tokens": [407, 321, 500, 380, 458, 437, 309, 1355, 11, 293, 309, 1177, 380, 1871, 13, 286, 2041, 264, 1379, 4334, 295, 341], "temperature": 0.0, "avg_logprob": -0.21096378823985223, "compression_ratio": 1.8453608247422681, "no_speech_prob": 6.8542635744961444e-06}, {"id": 618, "seek": 376954, "start": 3784.46, "end": 3788.7799999999997, "text": " process is that we're going to figure out, we're going to learn about what things are,", "tokens": [1399, 307, 300, 321, 434, 516, 281, 2573, 484, 11, 321, 434, 516, 281, 1466, 466, 437, 721, 366, 11], "temperature": 0.0, "avg_logprob": -0.21096378823985223, "compression_ratio": 1.8453608247422681, "no_speech_prob": 6.8542635744961444e-06}, {"id": 619, "seek": 376954, "start": 3788.7799999999997, "end": 3792.2599999999998, "text": " or at least what things are important, and we'll figure out what they are and how they're", "tokens": [420, 412, 1935, 437, 721, 366, 1021, 11, 293, 321, 603, 2573, 484, 437, 436, 366, 293, 577, 436, 434], "temperature": 0.0, "avg_logprob": -0.21096378823985223, "compression_ratio": 1.8453608247422681, "no_speech_prob": 6.8542635744961444e-06}, {"id": 620, "seek": 376954, "start": 3792.2599999999998, "end": 3798.98, "text": " important. So we're going to start out knowing nothing about this dataset. So I'm just going", "tokens": [1021, 13, 407, 321, 434, 516, 281, 722, 484, 5276, 1825, 466, 341, 28872, 13, 407, 286, 478, 445, 516], "temperature": 0.0, "avg_logprob": -0.21096378823985223, "compression_ratio": 1.8453608247422681, "no_speech_prob": 6.8542635744961444e-06}, {"id": 621, "seek": 379898, "start": 3798.98, "end": 3801.94, "text": " to look at something called enclosure that has something called erops and something called", "tokens": [281, 574, 412, 746, 1219, 34093, 300, 575, 746, 1219, 1189, 3370, 293, 746, 1219], "temperature": 0.0, "avg_logprob": -0.20647118075582005, "compression_ratio": 1.892063492063492, "no_speech_prob": 9.223364941135515e-06}, {"id": 622, "seek": 379898, "start": 3801.94, "end": 3804.06, "text": " erops, and I don't even know what this is yet.", "tokens": [1189, 3370, 11, 293, 286, 500, 380, 754, 458, 437, 341, 307, 1939, 13], "temperature": 0.0, "avg_logprob": -0.20647118075582005, "compression_ratio": 1.892063492063492, "no_speech_prob": 9.223364941135515e-06}, {"id": 623, "seek": 379898, "start": 3804.06, "end": 3810.42, "text": " All I know is that the only three that really appear in any great quantity are erops, erops,", "tokens": [1057, 286, 458, 307, 300, 264, 787, 1045, 300, 534, 4204, 294, 604, 869, 11275, 366, 1189, 3370, 11, 1189, 3370, 11], "temperature": 0.0, "avg_logprob": -0.20647118075582005, "compression_ratio": 1.892063492063492, "no_speech_prob": 9.223364941135515e-06}, {"id": 624, "seek": 379898, "start": 3810.42, "end": 3816.3, "text": " WAC, and erops. This is really common as a data scientist. You often find yourself looking", "tokens": [343, 4378, 11, 293, 1189, 3370, 13, 639, 307, 534, 2689, 382, 257, 1412, 12662, 13, 509, 2049, 915, 1803, 1237], "temperature": 0.0, "avg_logprob": -0.20647118075582005, "compression_ratio": 1.892063492063492, "no_speech_prob": 9.223364941135515e-06}, {"id": 625, "seek": 379898, "start": 3816.3, "end": 3820.48, "text": " at data that you're not that familiar with, and you've got to figure out at least which", "tokens": [412, 1412, 300, 291, 434, 406, 300, 4963, 365, 11, 293, 291, 600, 658, 281, 2573, 484, 412, 1935, 597], "temperature": 0.0, "avg_logprob": -0.20647118075582005, "compression_ratio": 1.892063492063492, "no_speech_prob": 9.223364941135515e-06}, {"id": 626, "seek": 379898, "start": 3820.48, "end": 3824.7400000000002, "text": " bits to study more carefully, and which bits seem to matter, and so forth. So in this case,", "tokens": [9239, 281, 2979, 544, 7500, 11, 293, 597, 9239, 1643, 281, 1871, 11, 293, 370, 5220, 13, 407, 294, 341, 1389, 11], "temperature": 0.0, "avg_logprob": -0.20647118075582005, "compression_ratio": 1.892063492063492, "no_speech_prob": 9.223364941135515e-06}, {"id": 627, "seek": 379898, "start": 3824.7400000000002, "end": 3828.94, "text": " I at least know that these three groups I really don't care about because they basically don't", "tokens": [286, 412, 1935, 458, 300, 613, 1045, 3935, 286, 534, 500, 380, 1127, 466, 570, 436, 1936, 500, 380], "temperature": 0.0, "avg_logprob": -0.20647118075582005, "compression_ratio": 1.892063492063492, "no_speech_prob": 9.223364941135515e-06}, {"id": 628, "seek": 382894, "start": 3828.94, "end": 3831.7400000000002, "text": " exist.", "tokens": [2514, 13], "temperature": 0.0, "avg_logprob": -0.14013984328822085, "compression_ratio": 1.7243589743589745, "no_speech_prob": 1.723130844766274e-05}, {"id": 629, "seek": 382894, "start": 3831.7400000000002, "end": 3837.46, "text": " So given that, we're going to ignore those three. So we're going to focus on this one", "tokens": [407, 2212, 300, 11, 321, 434, 516, 281, 11200, 729, 1045, 13, 407, 321, 434, 516, 281, 1879, 322, 341, 472], "temperature": 0.0, "avg_logprob": -0.14013984328822085, "compression_ratio": 1.7243589743589745, "no_speech_prob": 1.723130844766274e-05}, {"id": 630, "seek": 382894, "start": 3837.46, "end": 3845.06, "text": " here, this one here, and this one here. So here you can see what I've done is I've taken", "tokens": [510, 11, 341, 472, 510, 11, 293, 341, 472, 510, 13, 407, 510, 291, 393, 536, 437, 286, 600, 1096, 307, 286, 600, 2726], "temperature": 0.0, "avg_logprob": -0.14013984328822085, "compression_ratio": 1.7243589743589745, "no_speech_prob": 1.723130844766274e-05}, {"id": 631, "seek": 382894, "start": 3845.06, "end": 3856.1, "text": " my data frame and I've grouped by enclosure, and I am taking the average of these three", "tokens": [452, 1412, 3920, 293, 286, 600, 41877, 538, 34093, 11, 293, 286, 669, 1940, 264, 4274, 295, 613, 1045], "temperature": 0.0, "avg_logprob": -0.14013984328822085, "compression_ratio": 1.7243589743589745, "no_speech_prob": 1.723130844766274e-05}, {"id": 632, "seek": 385610, "start": 3856.1, "end": 3861.88, "text": " fields. So here you can see the average sale price, the average prediction, and the standard", "tokens": [7909, 13, 407, 510, 291, 393, 536, 264, 4274, 8680, 3218, 11, 264, 4274, 17630, 11, 293, 264, 3832], "temperature": 0.0, "avg_logprob": -0.11925841294802152, "compression_ratio": 1.84, "no_speech_prob": 6.04888737143483e-06}, {"id": 633, "seek": 385610, "start": 3861.88, "end": 3865.14, "text": " deviation of prediction for each of my three groups.", "tokens": [25163, 295, 17630, 337, 1184, 295, 452, 1045, 3935, 13], "temperature": 0.0, "avg_logprob": -0.11925841294802152, "compression_ratio": 1.84, "no_speech_prob": 6.04888737143483e-06}, {"id": 634, "seek": 385610, "start": 3865.14, "end": 3871.8199999999997, "text": " So I can already start to learn a bit here. As you would expect, the prediction and the", "tokens": [407, 286, 393, 1217, 722, 281, 1466, 257, 857, 510, 13, 1018, 291, 576, 2066, 11, 264, 17630, 293, 264], "temperature": 0.0, "avg_logprob": -0.11925841294802152, "compression_ratio": 1.84, "no_speech_prob": 6.04888737143483e-06}, {"id": 635, "seek": 385610, "start": 3871.8199999999997, "end": 3880.1, "text": " sale price are close to each other on average, so that's a good sign. And then the standard", "tokens": [8680, 3218, 366, 1998, 281, 1184, 661, 322, 4274, 11, 370, 300, 311, 257, 665, 1465, 13, 400, 550, 264, 3832], "temperature": 0.0, "avg_logprob": -0.11925841294802152, "compression_ratio": 1.84, "no_speech_prob": 6.04888737143483e-06}, {"id": 636, "seek": 385610, "start": 3880.1, "end": 3885.08, "text": " deviation varies a little bit. It's a little hard to see in a table, so what we could do", "tokens": [25163, 21716, 257, 707, 857, 13, 467, 311, 257, 707, 1152, 281, 536, 294, 257, 3199, 11, 370, 437, 321, 727, 360], "temperature": 0.0, "avg_logprob": -0.11925841294802152, "compression_ratio": 1.84, "no_speech_prob": 6.04888737143483e-06}, {"id": 637, "seek": 388508, "start": 3885.08, "end": 3896.22, "text": " is we could try to start printing these things out. So here we've got the sale price for", "tokens": [307, 321, 727, 853, 281, 722, 14699, 613, 721, 484, 13, 407, 510, 321, 600, 658, 264, 8680, 3218, 337], "temperature": 0.0, "avg_logprob": -0.11941080382376006, "compression_ratio": 1.763157894736842, "no_speech_prob": 6.540405593113974e-06}, {"id": 638, "seek": 388508, "start": 3896.22, "end": 3902.1, "text": " each level of enclosure, and here we've got the prediction for each level of enclosure,", "tokens": [1184, 1496, 295, 34093, 11, 293, 510, 321, 600, 658, 264, 17630, 337, 1184, 1496, 295, 34093, 11], "temperature": 0.0, "avg_logprob": -0.11941080382376006, "compression_ratio": 1.763157894736842, "no_speech_prob": 6.540405593113974e-06}, {"id": 639, "seek": 388508, "start": 3902.1, "end": 3906.86, "text": " and for the error bars, I'm using the standard deviation of prediction. So here you can see", "tokens": [293, 337, 264, 6713, 10228, 11, 286, 478, 1228, 264, 3832, 25163, 295, 17630, 13, 407, 510, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.11941080382376006, "compression_ratio": 1.763157894736842, "no_speech_prob": 6.540405593113974e-06}, {"id": 640, "seek": 390686, "start": 3906.86, "end": 3917.34, "text": " the actual, and here's the prediction, and here's my confidence interval. Or at least", "tokens": [264, 3539, 11, 293, 510, 311, 264, 17630, 11, 293, 510, 311, 452, 6687, 15035, 13, 1610, 412, 1935], "temperature": 0.0, "avg_logprob": -0.148890564240605, "compression_ratio": 1.6633165829145728, "no_speech_prob": 1.8448196215103962e-06}, {"id": 641, "seek": 390686, "start": 3917.34, "end": 3922.1200000000003, "text": " it's the average of the standard deviation of the random forests.", "tokens": [309, 311, 264, 4274, 295, 264, 3832, 25163, 295, 264, 4974, 21700, 13], "temperature": 0.0, "avg_logprob": -0.148890564240605, "compression_ratio": 1.6633165829145728, "no_speech_prob": 1.8448196215103962e-06}, {"id": 642, "seek": 390686, "start": 3922.1200000000003, "end": 3926.98, "text": " So this tells us, it'll tell us if there's some groups or some rows that we're not very", "tokens": [407, 341, 5112, 505, 11, 309, 603, 980, 505, 498, 456, 311, 512, 3935, 420, 512, 13241, 300, 321, 434, 406, 588], "temperature": 0.0, "avg_logprob": -0.148890564240605, "compression_ratio": 1.6633165829145728, "no_speech_prob": 1.8448196215103962e-06}, {"id": 643, "seek": 390686, "start": 3926.98, "end": 3933.98, "text": " confident of at all. So we could do something similar for product size. So here's different", "tokens": [6679, 295, 412, 439, 13, 407, 321, 727, 360, 746, 2531, 337, 1674, 2744, 13, 407, 510, 311, 819], "temperature": 0.0, "avg_logprob": -0.148890564240605, "compression_ratio": 1.6633165829145728, "no_speech_prob": 1.8448196215103962e-06}, {"id": 644, "seek": 393398, "start": 3933.98, "end": 3940.02, "text": " product sizes. We can do exactly the same thing of looking at our predictions and our", "tokens": [1674, 11602, 13, 492, 393, 360, 2293, 264, 912, 551, 295, 1237, 412, 527, 21264, 293, 527], "temperature": 0.0, "avg_logprob": -0.17758094158369242, "compression_ratio": 1.904109589041096, "no_speech_prob": 7.646483027201612e-06}, {"id": 645, "seek": 393398, "start": 3940.02, "end": 3948.34, "text": " standard deviations. We could sort by, and what we could say is, what's the ratio of", "tokens": [3832, 31219, 763, 13, 492, 727, 1333, 538, 11, 293, 437, 321, 727, 584, 307, 11, 437, 311, 264, 8509, 295], "temperature": 0.0, "avg_logprob": -0.17758094158369242, "compression_ratio": 1.904109589041096, "no_speech_prob": 7.646483027201612e-06}, {"id": 646, "seek": 393398, "start": 3948.34, "end": 3952.66, "text": " the standard deviation of the predictions to the predictions themselves? So you'd kind", "tokens": [264, 3832, 25163, 295, 264, 21264, 281, 264, 21264, 2969, 30, 407, 291, 1116, 733], "temperature": 0.0, "avg_logprob": -0.17758094158369242, "compression_ratio": 1.904109589041096, "no_speech_prob": 7.646483027201612e-06}, {"id": 647, "seek": 393398, "start": 3952.66, "end": 3956.82, "text": " of expect on average that when you're predicting something that's a bigger number, that your", "tokens": [295, 2066, 322, 4274, 300, 562, 291, 434, 32884, 746, 300, 311, 257, 3801, 1230, 11, 300, 428], "temperature": 0.0, "avg_logprob": -0.17758094158369242, "compression_ratio": 1.904109589041096, "no_speech_prob": 7.646483027201612e-06}, {"id": 648, "seek": 393398, "start": 3956.82, "end": 3962.14, "text": " standard deviation would be higher. So you can sort by that ratio.", "tokens": [3832, 25163, 576, 312, 2946, 13, 407, 291, 393, 1333, 538, 300, 8509, 13], "temperature": 0.0, "avg_logprob": -0.17758094158369242, "compression_ratio": 1.904109589041096, "no_speech_prob": 7.646483027201612e-06}, {"id": 649, "seek": 396214, "start": 3962.14, "end": 3969.74, "text": " And what that tells us is that the product size large and product size compact, our predictions", "tokens": [400, 437, 300, 5112, 505, 307, 300, 264, 1674, 2744, 2416, 293, 1674, 2744, 14679, 11, 527, 21264], "temperature": 0.0, "avg_logprob": -0.13028694788614908, "compression_ratio": 1.5964125560538116, "no_speech_prob": 2.813010041791131e-06}, {"id": 650, "seek": 396214, "start": 3969.74, "end": 3975.5, "text": " are less accurate, relatively speaking, as a ratio of the total price. So then if we", "tokens": [366, 1570, 8559, 11, 7226, 4124, 11, 382, 257, 8509, 295, 264, 3217, 3218, 13, 407, 550, 498, 321], "temperature": 0.0, "avg_logprob": -0.13028694788614908, "compression_ratio": 1.5964125560538116, "no_speech_prob": 2.813010041791131e-06}, {"id": 651, "seek": 396214, "start": 3975.5, "end": 3981.8599999999997, "text": " go back and have a look, well there you go, that's why. From the histogram, those are", "tokens": [352, 646, 293, 362, 257, 574, 11, 731, 456, 291, 352, 11, 300, 311, 983, 13, 3358, 264, 49816, 11, 729, 366], "temperature": 0.0, "avg_logprob": -0.13028694788614908, "compression_ratio": 1.5964125560538116, "no_speech_prob": 2.813010041791131e-06}, {"id": 652, "seek": 396214, "start": 3981.8599999999997, "end": 3989.9, "text": " the smallest groups. So as you would expect, in small groups we're doing a less good job.", "tokens": [264, 16998, 3935, 13, 407, 382, 291, 576, 2066, 11, 294, 1359, 3935, 321, 434, 884, 257, 1570, 665, 1691, 13], "temperature": 0.0, "avg_logprob": -0.13028694788614908, "compression_ratio": 1.5964125560538116, "no_speech_prob": 2.813010041791131e-06}, {"id": 653, "seek": 398990, "start": 3989.9, "end": 3994.98, "text": " So this confidence interval you can really use for two main purposes. One is that you", "tokens": [407, 341, 6687, 15035, 291, 393, 534, 764, 337, 732, 2135, 9932, 13, 1485, 307, 300, 291], "temperature": 0.0, "avg_logprob": -0.1457651424407959, "compression_ratio": 1.8117154811715481, "no_speech_prob": 7.64650303608505e-06}, {"id": 654, "seek": 398990, "start": 3994.98, "end": 4000.5, "text": " can group it up like this and look at the average confidence interval by group to find", "tokens": [393, 1594, 309, 493, 411, 341, 293, 574, 412, 264, 4274, 6687, 15035, 538, 1594, 281, 915], "temperature": 0.0, "avg_logprob": -0.1457651424407959, "compression_ratio": 1.8117154811715481, "no_speech_prob": 7.64650303608505e-06}, {"id": 655, "seek": 398990, "start": 4000.5, "end": 4007.1800000000003, "text": " out are there some groups that you just don't seem to have confidence about those groups.", "tokens": [484, 366, 456, 512, 3935, 300, 291, 445, 500, 380, 1643, 281, 362, 6687, 466, 729, 3935, 13], "temperature": 0.0, "avg_logprob": -0.1457651424407959, "compression_ratio": 1.8117154811715481, "no_speech_prob": 7.64650303608505e-06}, {"id": 656, "seek": 398990, "start": 4007.1800000000003, "end": 4011.1800000000003, "text": " But perhaps more importantly, you can look at them for specific rows. So when you put", "tokens": [583, 4317, 544, 8906, 11, 291, 393, 574, 412, 552, 337, 2685, 13241, 13, 407, 562, 291, 829], "temperature": 0.0, "avg_logprob": -0.1457651424407959, "compression_ratio": 1.8117154811715481, "no_speech_prob": 7.64650303608505e-06}, {"id": 657, "seek": 398990, "start": 4011.1800000000003, "end": 4017.6600000000003, "text": " it in production, you might always want to see the confidence interval. So if you're", "tokens": [309, 294, 4265, 11, 291, 1062, 1009, 528, 281, 536, 264, 6687, 15035, 13, 407, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.1457651424407959, "compression_ratio": 1.8117154811715481, "no_speech_prob": 7.64650303608505e-06}, {"id": 658, "seek": 401766, "start": 4017.66, "end": 4021.62, "text": " doing say your credit scoring, so deciding whether to give somebody a loan, you probably", "tokens": [884, 584, 428, 5397, 22358, 11, 370, 17990, 1968, 281, 976, 2618, 257, 10529, 11, 291, 1391], "temperature": 0.0, "avg_logprob": -0.13866631058621998, "compression_ratio": 1.7464285714285714, "no_speech_prob": 3.966974418290192e-06}, {"id": 659, "seek": 401766, "start": 4021.62, "end": 4026.2999999999997, "text": " want to see not only what's their level of risk, but how confident are we. And if they", "tokens": [528, 281, 536, 406, 787, 437, 311, 641, 1496, 295, 3148, 11, 457, 577, 6679, 366, 321, 13, 400, 498, 436], "temperature": 0.0, "avg_logprob": -0.13866631058621998, "compression_ratio": 1.7464285714285714, "no_speech_prob": 3.966974418290192e-06}, {"id": 660, "seek": 401766, "start": 4026.2999999999997, "end": 4031.02, "text": " want to borrow lots of money and we're not at all confident about our ability to predict", "tokens": [528, 281, 11172, 3195, 295, 1460, 293, 321, 434, 406, 412, 439, 6679, 466, 527, 3485, 281, 6069], "temperature": 0.0, "avg_logprob": -0.13866631058621998, "compression_ratio": 1.7464285714285714, "no_speech_prob": 3.966974418290192e-06}, {"id": 661, "seek": 401766, "start": 4031.02, "end": 4037.06, "text": " whether they'll pay it back, we might want to give them a smaller loan. So those are", "tokens": [1968, 436, 603, 1689, 309, 646, 11, 321, 1062, 528, 281, 976, 552, 257, 4356, 10529, 13, 407, 729, 366], "temperature": 0.0, "avg_logprob": -0.13866631058621998, "compression_ratio": 1.7464285714285714, "no_speech_prob": 3.966974418290192e-06}, {"id": 662, "seek": 401766, "start": 4037.06, "end": 4041.42, "text": " the two ways in which you would use this.", "tokens": [264, 732, 2098, 294, 597, 291, 576, 764, 341, 13], "temperature": 0.0, "avg_logprob": -0.13866631058621998, "compression_ratio": 1.7464285714285714, "no_speech_prob": 3.966974418290192e-06}, {"id": 663, "seek": 401766, "start": 4041.42, "end": 4047.58, "text": " Let me go to the next one, which is the most important. The most important is feature importance.", "tokens": [961, 385, 352, 281, 264, 958, 472, 11, 597, 307, 264, 881, 1021, 13, 440, 881, 1021, 307, 4111, 7379, 13], "temperature": 0.0, "avg_logprob": -0.13866631058621998, "compression_ratio": 1.7464285714285714, "no_speech_prob": 3.966974418290192e-06}, {"id": 664, "seek": 404758, "start": 4047.58, "end": 4052.66, "text": " The only reason I didn't do this first is because I think the intuitive understanding", "tokens": [440, 787, 1778, 286, 994, 380, 360, 341, 700, 307, 570, 286, 519, 264, 21769, 3701], "temperature": 0.0, "avg_logprob": -0.13015326887074083, "compression_ratio": 1.682170542635659, "no_speech_prob": 8.267777957371436e-06}, {"id": 665, "seek": 404758, "start": 4052.66, "end": 4057.18, "text": " of how to calculate confidence interval is the easiest one to understand intuitively.", "tokens": [295, 577, 281, 8873, 6687, 15035, 307, 264, 12889, 472, 281, 1223, 46506, 13], "temperature": 0.0, "avg_logprob": -0.13015326887074083, "compression_ratio": 1.682170542635659, "no_speech_prob": 8.267777957371436e-06}, {"id": 666, "seek": 404758, "start": 4057.18, "end": 4061.58, "text": " In fact, it's almost identical to something we've already calculated. But in terms of", "tokens": [682, 1186, 11, 309, 311, 1920, 14800, 281, 746, 321, 600, 1217, 15598, 13, 583, 294, 2115, 295], "temperature": 0.0, "avg_logprob": -0.13015326887074083, "compression_ratio": 1.682170542635659, "no_speech_prob": 8.267777957371436e-06}, {"id": 667, "seek": 404758, "start": 4061.58, "end": 4067.7, "text": " which one do I look at first in practice, I always look at this in practice. So when", "tokens": [597, 472, 360, 286, 574, 412, 700, 294, 3124, 11, 286, 1009, 574, 412, 341, 294, 3124, 13, 407, 562], "temperature": 0.0, "avg_logprob": -0.13015326887074083, "compression_ratio": 1.682170542635659, "no_speech_prob": 8.267777957371436e-06}, {"id": 668, "seek": 404758, "start": 4067.7, "end": 4073.74, "text": " I'm working on whether it be a cattle competition or a real-world project, I build a random", "tokens": [286, 478, 1364, 322, 1968, 309, 312, 257, 19992, 6211, 420, 257, 957, 12, 13217, 1716, 11, 286, 1322, 257, 4974], "temperature": 0.0, "avg_logprob": -0.13015326887074083, "compression_ratio": 1.682170542635659, "no_speech_prob": 8.267777957371436e-06}, {"id": 669, "seek": 407374, "start": 4073.74, "end": 4081.62, "text": " forest as fast as I can, try and get it to the point that it's significantly better than", "tokens": [6719, 382, 2370, 382, 286, 393, 11, 853, 293, 483, 309, 281, 264, 935, 300, 309, 311, 10591, 1101, 813], "temperature": 0.0, "avg_logprob": -0.2079137655404898, "compression_ratio": 1.6615384615384616, "no_speech_prob": 5.771904397988692e-06}, {"id": 670, "seek": 407374, "start": 4081.62, "end": 4083.66, "text": " random but doesn't have to be much better than that.", "tokens": [4974, 457, 1177, 380, 362, 281, 312, 709, 1101, 813, 300, 13], "temperature": 0.0, "avg_logprob": -0.2079137655404898, "compression_ratio": 1.6615384615384616, "no_speech_prob": 5.771904397988692e-06}, {"id": 671, "seek": 407374, "start": 4083.66, "end": 4088.7, "text": " And then the next thing I do is to plot the feature importance. And the feature importance", "tokens": [400, 550, 264, 958, 551, 286, 360, 307, 281, 7542, 264, 4111, 7379, 13, 400, 264, 4111, 7379], "temperature": 0.0, "avg_logprob": -0.2079137655404898, "compression_ratio": 1.6615384615384616, "no_speech_prob": 5.771904397988692e-06}, {"id": 672, "seek": 407374, "start": 4088.7, "end": 4100.139999999999, "text": " tells us in this random forest which columns matter. So we had dozens and dozens of columns", "tokens": [5112, 505, 294, 341, 4974, 6719, 597, 13766, 1871, 13, 407, 321, 632, 18431, 293, 18431, 295, 13766], "temperature": 0.0, "avg_logprob": -0.2079137655404898, "compression_ratio": 1.6615384615384616, "no_speech_prob": 5.771904397988692e-06}, {"id": 673, "seek": 410014, "start": 4100.14, "end": 4104.9400000000005, "text": " originally in this dataset, and here I'm just picking out the top 10. So you can just call", "tokens": [7993, 294, 341, 28872, 11, 293, 510, 286, 478, 445, 8867, 484, 264, 1192, 1266, 13, 407, 291, 393, 445, 818], "temperature": 0.0, "avg_logprob": -0.2442060553509256, "compression_ratio": 1.6988416988416988, "no_speech_prob": 5.8627706494007725e-06}, {"id": 674, "seek": 410014, "start": 4104.9400000000005, "end": 4110.26, "text": " rf feature importance, again this is part of the FastAI library, it's leveraging stuff", "tokens": [367, 69, 4111, 7379, 11, 797, 341, 307, 644, 295, 264, 15968, 48698, 6405, 11, 309, 311, 32666, 1507], "temperature": 0.0, "avg_logprob": -0.2442060553509256, "compression_ratio": 1.6988416988416988, "no_speech_prob": 5.8627706494007725e-06}, {"id": 675, "seek": 410014, "start": 4110.26, "end": 4115.660000000001, "text": " that's in scikit-learn, pass in the model, pass in the data frame, because we need to", "tokens": [300, 311, 294, 2180, 22681, 12, 306, 1083, 11, 1320, 294, 264, 2316, 11, 1320, 294, 264, 1412, 3920, 11, 570, 321, 643, 281], "temperature": 0.0, "avg_logprob": -0.2442060553509256, "compression_ratio": 1.6988416988416988, "no_speech_prob": 5.8627706494007725e-06}, {"id": 676, "seek": 410014, "start": 4115.660000000001, "end": 4123.740000000001, "text": " know the names of the columns, and it'll give you back a pandas data frame showing you in", "tokens": [458, 264, 5288, 295, 264, 13766, 11, 293, 309, 603, 976, 291, 646, 257, 4565, 296, 1412, 3920, 4099, 291, 294], "temperature": 0.0, "avg_logprob": -0.2442060553509256, "compression_ratio": 1.6988416988416988, "no_speech_prob": 5.8627706494007725e-06}, {"id": 677, "seek": 410014, "start": 4123.740000000001, "end": 4128.660000000001, "text": " order of importance how important was each column. And here I'm just going to pick out", "tokens": [1668, 295, 7379, 577, 1021, 390, 1184, 7738, 13, 400, 510, 286, 478, 445, 516, 281, 1888, 484], "temperature": 0.0, "avg_logprob": -0.2442060553509256, "compression_ratio": 1.6988416988416988, "no_speech_prob": 5.8627706494007725e-06}, {"id": 678, "seek": 412866, "start": 4128.66, "end": 4131.34, "text": " the top 10.", "tokens": [264, 1192, 1266, 13], "temperature": 0.0, "avg_logprob": -0.16222853612418126, "compression_ratio": 1.7440758293838863, "no_speech_prob": 3.6119670312473318e-06}, {"id": 679, "seek": 412866, "start": 4131.34, "end": 4139.82, "text": " So we can then plot that. So fi, because it's a data frame, we can use data frame plotting", "tokens": [407, 321, 393, 550, 7542, 300, 13, 407, 15848, 11, 570, 309, 311, 257, 1412, 3920, 11, 321, 393, 764, 1412, 3920, 41178], "temperature": 0.0, "avg_logprob": -0.16222853612418126, "compression_ratio": 1.7440758293838863, "no_speech_prob": 3.6119670312473318e-06}, {"id": 680, "seek": 412866, "start": 4139.82, "end": 4146.74, "text": " commands. So here I've plotted all of the feature importances. And so you can see here,", "tokens": [16901, 13, 407, 510, 286, 600, 43288, 439, 295, 264, 4111, 974, 2676, 13, 400, 370, 291, 393, 536, 510, 11], "temperature": 0.0, "avg_logprob": -0.16222853612418126, "compression_ratio": 1.7440758293838863, "no_speech_prob": 3.6119670312473318e-06}, {"id": 681, "seek": 412866, "start": 4146.74, "end": 4151.3, "text": " and I haven't been able to write all of the names of the columns at the bottom, which", "tokens": [293, 286, 2378, 380, 668, 1075, 281, 2464, 439, 295, 264, 5288, 295, 264, 13766, 412, 264, 2767, 11, 597], "temperature": 0.0, "avg_logprob": -0.16222853612418126, "compression_ratio": 1.7440758293838863, "no_speech_prob": 3.6119670312473318e-06}, {"id": 682, "seek": 412866, "start": 4151.3, "end": 4156.0199999999995, "text": " that's not the important thing. The important thing is to see that some columns are really,", "tokens": [300, 311, 406, 264, 1021, 551, 13, 440, 1021, 551, 307, 281, 536, 300, 512, 13766, 366, 534, 11], "temperature": 0.0, "avg_logprob": -0.16222853612418126, "compression_ratio": 1.7440758293838863, "no_speech_prob": 3.6119670312473318e-06}, {"id": 683, "seek": 415602, "start": 4156.02, "end": 4162.3, "text": " really important, and most columns don't really matter at all. And in nearly every dataset", "tokens": [534, 1021, 11, 293, 881, 13766, 500, 380, 534, 1871, 412, 439, 13, 400, 294, 6217, 633, 28872], "temperature": 0.0, "avg_logprob": -0.14841477233584566, "compression_ratio": 1.7510204081632652, "no_speech_prob": 8.664627785037737e-06}, {"id": 684, "seek": 415602, "start": 4162.3, "end": 4167.860000000001, "text": " you use in real life, this is what your feature importance is going to look like. It's going", "tokens": [291, 764, 294, 957, 993, 11, 341, 307, 437, 428, 4111, 7379, 307, 516, 281, 574, 411, 13, 467, 311, 516], "temperature": 0.0, "avg_logprob": -0.14841477233584566, "compression_ratio": 1.7510204081632652, "no_speech_prob": 8.664627785037737e-06}, {"id": 685, "seek": 415602, "start": 4167.860000000001, "end": 4170.860000000001, "text": " to say there's a handful of columns that you care about.", "tokens": [281, 584, 456, 311, 257, 16458, 295, 13766, 300, 291, 1127, 466, 13], "temperature": 0.0, "avg_logprob": -0.14841477233584566, "compression_ratio": 1.7510204081632652, "no_speech_prob": 8.664627785037737e-06}, {"id": 686, "seek": 415602, "start": 4170.860000000001, "end": 4178.06, "text": " And this is why I always start here, because at this point, in terms of looking into learning", "tokens": [400, 341, 307, 983, 286, 1009, 722, 510, 11, 570, 412, 341, 935, 11, 294, 2115, 295, 1237, 666, 2539], "temperature": 0.0, "avg_logprob": -0.14841477233584566, "compression_ratio": 1.7510204081632652, "no_speech_prob": 8.664627785037737e-06}, {"id": 687, "seek": 415602, "start": 4178.06, "end": 4184.42, "text": " about this domain of heavy industrial equipment options, I'm only going to care about learning", "tokens": [466, 341, 9274, 295, 4676, 9987, 5927, 3956, 11, 286, 478, 787, 516, 281, 1127, 466, 2539], "temperature": 0.0, "avg_logprob": -0.14841477233584566, "compression_ratio": 1.7510204081632652, "no_speech_prob": 8.664627785037737e-06}, {"id": 688, "seek": 418442, "start": 4184.42, "end": 4189.7, "text": " about the columns which matter. So are we going to bother learning about enclosure?", "tokens": [466, 264, 13766, 597, 1871, 13, 407, 366, 321, 516, 281, 8677, 2539, 466, 34093, 30], "temperature": 0.0, "avg_logprob": -0.16852183219714043, "compression_ratio": 1.6263157894736842, "no_speech_prob": 6.339184892567573e-06}, {"id": 689, "seek": 418442, "start": 4189.7, "end": 4197.7, "text": " It depends whether enclosure is important. And there it is, it's in the top 10. So we", "tokens": [467, 5946, 1968, 34093, 307, 1021, 13, 400, 456, 309, 307, 11, 309, 311, 294, 264, 1192, 1266, 13, 407, 321], "temperature": 0.0, "avg_logprob": -0.16852183219714043, "compression_ratio": 1.6263157894736842, "no_speech_prob": 6.339184892567573e-06}, {"id": 690, "seek": 418442, "start": 4197.7, "end": 4200.8, "text": " are going to have to learn about enclosure.", "tokens": [366, 516, 281, 362, 281, 1466, 466, 34093, 13], "temperature": 0.0, "avg_logprob": -0.16852183219714043, "compression_ratio": 1.6263157894736842, "no_speech_prob": 6.339184892567573e-06}, {"id": 691, "seek": 418442, "start": 4200.8, "end": 4208.3, "text": " So then we could also plot this as a bar plot. So here I've just created a tiny little function", "tokens": [407, 550, 321, 727, 611, 7542, 341, 382, 257, 2159, 7542, 13, 407, 510, 286, 600, 445, 2942, 257, 5870, 707, 2445], "temperature": 0.0, "avg_logprob": -0.16852183219714043, "compression_ratio": 1.6263157894736842, "no_speech_prob": 6.339184892567573e-06}, {"id": 692, "seek": 420830, "start": 4208.3, "end": 4215.18, "text": " here that's going to just plot my bars, and I'm just going to do it for the top 30. So", "tokens": [510, 300, 311, 516, 281, 445, 7542, 452, 10228, 11, 293, 286, 478, 445, 516, 281, 360, 309, 337, 264, 1192, 2217, 13, 407], "temperature": 0.0, "avg_logprob": -0.11799440213612147, "compression_ratio": 1.7615062761506277, "no_speech_prob": 2.7264566142548574e-06}, {"id": 693, "seek": 420830, "start": 4215.18, "end": 4222.22, "text": " you can see the same basic shape here, and I can see there's my enclosure.", "tokens": [291, 393, 536, 264, 912, 3875, 3909, 510, 11, 293, 286, 393, 536, 456, 311, 452, 34093, 13], "temperature": 0.0, "avg_logprob": -0.11799440213612147, "compression_ratio": 1.7615062761506277, "no_speech_prob": 2.7264566142548574e-06}, {"id": 694, "seek": 420830, "start": 4222.22, "end": 4227.54, "text": " So we're going to learn about how this is calculated in just a moment. But before we", "tokens": [407, 321, 434, 516, 281, 1466, 466, 577, 341, 307, 15598, 294, 445, 257, 1623, 13, 583, 949, 321], "temperature": 0.0, "avg_logprob": -0.11799440213612147, "compression_ratio": 1.7615062761506277, "no_speech_prob": 2.7264566142548574e-06}, {"id": 695, "seek": 420830, "start": 4227.54, "end": 4231.5, "text": " worry about how it's calculated, much more important is to know what to do with it. So", "tokens": [3292, 466, 577, 309, 311, 15598, 11, 709, 544, 1021, 307, 281, 458, 437, 281, 360, 365, 309, 13, 407], "temperature": 0.0, "avg_logprob": -0.11799440213612147, "compression_ratio": 1.7615062761506277, "no_speech_prob": 2.7264566142548574e-06}, {"id": 696, "seek": 420830, "start": 4231.5, "end": 4237.9400000000005, "text": " the most important thing to do with it is to now sit down with your client or your data", "tokens": [264, 881, 1021, 551, 281, 360, 365, 309, 307, 281, 586, 1394, 760, 365, 428, 6423, 420, 428, 1412], "temperature": 0.0, "avg_logprob": -0.11799440213612147, "compression_ratio": 1.7615062761506277, "no_speech_prob": 2.7264566142548574e-06}, {"id": 697, "seek": 423794, "start": 4237.94, "end": 4244.0199999999995, "text": " dictionary or whatever your source of information is, and say to them, tell me about year made.", "tokens": [25890, 420, 2035, 428, 4009, 295, 1589, 307, 11, 293, 584, 281, 552, 11, 980, 385, 466, 1064, 1027, 13], "temperature": 0.0, "avg_logprob": -0.18447753361293248, "compression_ratio": 1.7854671280276817, "no_speech_prob": 2.5466089937253855e-05}, {"id": 698, "seek": 423794, "start": 4244.0199999999995, "end": 4248.5, "text": " What does that mean? Where does that come from? Plot lots of things like histograms", "tokens": [708, 775, 300, 914, 30, 2305, 775, 300, 808, 490, 30, 2149, 310, 3195, 295, 721, 411, 49816, 82], "temperature": 0.0, "avg_logprob": -0.18447753361293248, "compression_ratio": 1.7854671280276817, "no_speech_prob": 2.5466089937253855e-05}, {"id": 699, "seek": 423794, "start": 4248.5, "end": 4252.339999999999, "text": " of year made, scatter plots of year made against price, and learn everything you can, because", "tokens": [295, 1064, 1027, 11, 34951, 28609, 295, 1064, 1027, 1970, 3218, 11, 293, 1466, 1203, 291, 393, 11, 570], "temperature": 0.0, "avg_logprob": -0.18447753361293248, "compression_ratio": 1.7854671280276817, "no_speech_prob": 2.5466089937253855e-05}, {"id": 700, "seek": 423794, "start": 4252.339999999999, "end": 4256.7, "text": " year made and coupler system, they're the things that matter.", "tokens": [1064, 1027, 293, 1384, 22732, 1185, 11, 436, 434, 264, 721, 300, 1871, 13], "temperature": 0.0, "avg_logprob": -0.18447753361293248, "compression_ratio": 1.7854671280276817, "no_speech_prob": 2.5466089937253855e-05}, {"id": 701, "seek": 423794, "start": 4256.7, "end": 4261.179999999999, "text": " And what will often happen in real-world projects is that you'll sit with the client and you'll", "tokens": [400, 437, 486, 2049, 1051, 294, 957, 12, 13217, 4455, 307, 300, 291, 603, 1394, 365, 264, 6423, 293, 291, 603], "temperature": 0.0, "avg_logprob": -0.18447753361293248, "compression_ratio": 1.7854671280276817, "no_speech_prob": 2.5466089937253855e-05}, {"id": 702, "seek": 423794, "start": 4261.179999999999, "end": 4265.419999999999, "text": " say, oh it turns out the coupler system is the second most important thing, and then", "tokens": [584, 11, 1954, 309, 4523, 484, 264, 1384, 22732, 1185, 307, 264, 1150, 881, 1021, 551, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.18447753361293248, "compression_ratio": 1.7854671280276817, "no_speech_prob": 2.5466089937253855e-05}, {"id": 703, "seek": 426542, "start": 4265.42, "end": 4271.9800000000005, "text": " they might say, that makes no sense. Now that doesn't mean there's a problem with your model.", "tokens": [436, 1062, 584, 11, 300, 1669, 572, 2020, 13, 823, 300, 1177, 380, 914, 456, 311, 257, 1154, 365, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13642379675018654, "compression_ratio": 1.6271186440677967, "no_speech_prob": 2.4060936993919313e-06}, {"id": 704, "seek": 426542, "start": 4271.9800000000005, "end": 4277.06, "text": " It means there's a problem with their understanding of the data that they gave you. So let me", "tokens": [467, 1355, 456, 311, 257, 1154, 365, 641, 3701, 295, 264, 1412, 300, 436, 2729, 291, 13, 407, 718, 385], "temperature": 0.0, "avg_logprob": -0.13642379675018654, "compression_ratio": 1.6271186440677967, "no_speech_prob": 2.4060936993919313e-06}, {"id": 705, "seek": 426542, "start": 4277.06, "end": 4283.9, "text": " give you an example. I entered a Kaggle competition where the goal was to predict which applications", "tokens": [976, 291, 364, 1365, 13, 286, 9065, 257, 48751, 22631, 6211, 689, 264, 3387, 390, 281, 6069, 597, 5821], "temperature": 0.0, "avg_logprob": -0.13642379675018654, "compression_ratio": 1.6271186440677967, "no_speech_prob": 2.4060936993919313e-06}, {"id": 706, "seek": 426542, "start": 4283.9, "end": 4290.86, "text": " for grants at a university would be successful. And I used this exact approach and I discovered", "tokens": [337, 16101, 412, 257, 5454, 576, 312, 4406, 13, 400, 286, 1143, 341, 1900, 3109, 293, 286, 6941], "temperature": 0.0, "avg_logprob": -0.13642379675018654, "compression_ratio": 1.6271186440677967, "no_speech_prob": 2.4060936993919313e-06}, {"id": 707, "seek": 429086, "start": 4290.86, "end": 4296.5, "text": " a number of columns which were almost entirely predictive of the dependent variable. And", "tokens": [257, 1230, 295, 13766, 597, 645, 1920, 7696, 35521, 295, 264, 12334, 7006, 13, 400], "temperature": 0.0, "avg_logprob": -0.19927989211037894, "compression_ratio": 1.803921568627451, "no_speech_prob": 7.646443009434734e-06}, {"id": 708, "seek": 429086, "start": 4296.5, "end": 4299.9, "text": " specifically when I then looked to see in what way they were predictive, it turned out", "tokens": [4682, 562, 286, 550, 2956, 281, 536, 294, 437, 636, 436, 645, 35521, 11, 309, 3574, 484], "temperature": 0.0, "avg_logprob": -0.19927989211037894, "compression_ratio": 1.803921568627451, "no_speech_prob": 7.646443009434734e-06}, {"id": 709, "seek": 429086, "start": 4299.9, "end": 4304.74, "text": " that whether they were missing or not was basically the only thing that mattered in", "tokens": [300, 1968, 436, 645, 5361, 420, 406, 390, 1936, 264, 787, 551, 300, 44282, 294], "temperature": 0.0, "avg_logprob": -0.19927989211037894, "compression_ratio": 1.803921568627451, "no_speech_prob": 7.646443009434734e-06}, {"id": 710, "seek": 429086, "start": 4304.74, "end": 4307.219999999999, "text": " this dataset.", "tokens": [341, 28872, 13], "temperature": 0.0, "avg_logprob": -0.19927989211037894, "compression_ratio": 1.803921568627451, "no_speech_prob": 7.646443009434734e-06}, {"id": 711, "seek": 429086, "start": 4307.219999999999, "end": 4311.7, "text": " And so later on, I ended up winning that competition, and I think a lot of it was thanks to this", "tokens": [400, 370, 1780, 322, 11, 286, 4590, 493, 8224, 300, 6211, 11, 293, 286, 519, 257, 688, 295, 309, 390, 3231, 281, 341], "temperature": 0.0, "avg_logprob": -0.19927989211037894, "compression_ratio": 1.803921568627451, "no_speech_prob": 7.646443009434734e-06}, {"id": 712, "seek": 429086, "start": 4311.7, "end": 4319.139999999999, "text": " insight. And so later on I heard what had happened. It turns out that at that university,", "tokens": [11269, 13, 400, 370, 1780, 322, 286, 2198, 437, 632, 2011, 13, 467, 4523, 484, 300, 412, 300, 5454, 11], "temperature": 0.0, "avg_logprob": -0.19927989211037894, "compression_ratio": 1.803921568627451, "no_speech_prob": 7.646443009434734e-06}, {"id": 713, "seek": 431914, "start": 4319.14, "end": 4324.860000000001, "text": " there's an administrative burden to filling out the database. And so for a lot of the", "tokens": [456, 311, 364, 17900, 12578, 281, 10623, 484, 264, 8149, 13, 400, 370, 337, 257, 688, 295, 264], "temperature": 0.0, "avg_logprob": -0.1465161853366428, "compression_ratio": 1.7934272300469483, "no_speech_prob": 2.1444539015647024e-05}, {"id": 714, "seek": 431914, "start": 4324.860000000001, "end": 4330.1, "text": " grant applications, they don't fill in the database for the folks whose applications", "tokens": [6386, 5821, 11, 436, 500, 380, 2836, 294, 264, 8149, 337, 264, 4024, 6104, 5821], "temperature": 0.0, "avg_logprob": -0.1465161853366428, "compression_ratio": 1.7934272300469483, "no_speech_prob": 2.1444539015647024e-05}, {"id": 715, "seek": 431914, "start": 4330.1, "end": 4336.660000000001, "text": " weren't accepted. So in other words, these missing values in the dataset were saying,", "tokens": [4999, 380, 9035, 13, 407, 294, 661, 2283, 11, 613, 5361, 4190, 294, 264, 28872, 645, 1566, 11], "temperature": 0.0, "avg_logprob": -0.1465161853366428, "compression_ratio": 1.7934272300469483, "no_speech_prob": 2.1444539015647024e-05}, {"id": 716, "seek": 431914, "start": 4336.660000000001, "end": 4342.02, "text": " this grant wasn't accepted, because if it was accepted, then the admin folks are going", "tokens": [341, 6386, 2067, 380, 9035, 11, 570, 498, 309, 390, 9035, 11, 550, 264, 24236, 4024, 366, 516], "temperature": 0.0, "avg_logprob": -0.1465161853366428, "compression_ratio": 1.7934272300469483, "no_speech_prob": 2.1444539015647024e-05}, {"id": 717, "seek": 431914, "start": 4342.02, "end": 4345.18, "text": " to go in and type in that information.", "tokens": [281, 352, 294, 293, 2010, 294, 300, 1589, 13], "temperature": 0.0, "avg_logprob": -0.1465161853366428, "compression_ratio": 1.7934272300469483, "no_speech_prob": 2.1444539015647024e-05}, {"id": 718, "seek": 434518, "start": 4345.18, "end": 4350.1, "text": " So this is what we call data leakage. And data leakage means there's information in", "tokens": [407, 341, 307, 437, 321, 818, 1412, 47799, 13, 400, 1412, 47799, 1355, 456, 311, 1589, 294], "temperature": 0.0, "avg_logprob": -0.10804750949521608, "compression_ratio": 1.6199095022624435, "no_speech_prob": 3.3931237339857034e-06}, {"id": 719, "seek": 434518, "start": 4350.1, "end": 4355.9800000000005, "text": " the dataset that I was modeling with which the university wouldn't have had in real life", "tokens": [264, 28872, 300, 286, 390, 15983, 365, 597, 264, 5454, 2759, 380, 362, 632, 294, 957, 993], "temperature": 0.0, "avg_logprob": -0.10804750949521608, "compression_ratio": 1.6199095022624435, "no_speech_prob": 3.3931237339857034e-06}, {"id": 720, "seek": 434518, "start": 4355.9800000000005, "end": 4363.42, "text": " at the point in time they were making a decision. So when they're actually deciding which grant", "tokens": [412, 264, 935, 294, 565, 436, 645, 1455, 257, 3537, 13, 407, 562, 436, 434, 767, 17990, 597, 6386], "temperature": 0.0, "avg_logprob": -0.10804750949521608, "compression_ratio": 1.6199095022624435, "no_speech_prob": 3.3931237339857034e-06}, {"id": 721, "seek": 434518, "start": 4363.42, "end": 4370.54, "text": " applications should I prioritize, they don't actually know which ones the admin staff are", "tokens": [5821, 820, 286, 25164, 11, 436, 500, 380, 767, 458, 597, 2306, 264, 24236, 3525, 366], "temperature": 0.0, "avg_logprob": -0.10804750949521608, "compression_ratio": 1.6199095022624435, "no_speech_prob": 3.3931237339857034e-06}, {"id": 722, "seek": 437054, "start": 4370.54, "end": 4377.78, "text": " going to add information to because it turns out they got accepted. So one of the key things", "tokens": [516, 281, 909, 1589, 281, 570, 309, 4523, 484, 436, 658, 9035, 13, 407, 472, 295, 264, 2141, 721], "temperature": 0.0, "avg_logprob": -0.21126743073159077, "compression_ratio": 1.6771300448430493, "no_speech_prob": 6.240881248231744e-06}, {"id": 723, "seek": 437054, "start": 4377.78, "end": 4383.98, "text": " you'll find here is data leakage problems, and that's a serious problem that you need", "tokens": [291, 603, 915, 510, 307, 1412, 47799, 2740, 11, 293, 300, 311, 257, 3156, 1154, 300, 291, 643], "temperature": 0.0, "avg_logprob": -0.21126743073159077, "compression_ratio": 1.6771300448430493, "no_speech_prob": 6.240881248231744e-06}, {"id": 724, "seek": 437054, "start": 4383.98, "end": 4386.5, "text": " to deal with.", "tokens": [281, 2028, 365, 13], "temperature": 0.0, "avg_logprob": -0.21126743073159077, "compression_ratio": 1.6771300448430493, "no_speech_prob": 6.240881248231744e-06}, {"id": 725, "seek": 437054, "start": 4386.5, "end": 4392.0199999999995, "text": " The other thing that will happen is you'll often find it's signs of collinearity. And", "tokens": [440, 661, 551, 300, 486, 1051, 307, 291, 603, 2049, 915, 309, 311, 7880, 295, 1263, 533, 17409, 13, 400], "temperature": 0.0, "avg_logprob": -0.21126743073159077, "compression_ratio": 1.6771300448430493, "no_speech_prob": 6.240881248231744e-06}, {"id": 726, "seek": 437054, "start": 4392.0199999999995, "end": 4396.42, "text": " I think that's what's happened here with Kapler system. I think Kapler system tells you whether", "tokens": [286, 519, 300, 311, 437, 311, 2011, 510, 365, 10988, 22732, 1185, 13, 286, 519, 10988, 22732, 1185, 5112, 291, 1968], "temperature": 0.0, "avg_logprob": -0.21126743073159077, "compression_ratio": 1.6771300448430493, "no_speech_prob": 6.240881248231744e-06}, {"id": 727, "seek": 439642, "start": 4396.42, "end": 4403.06, "text": " or not a particular kind of heavy industrial equipment has a particular feature on it.", "tokens": [420, 406, 257, 1729, 733, 295, 4676, 9987, 5927, 575, 257, 1729, 4111, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.11101197683683006, "compression_ratio": 1.787037037037037, "no_speech_prob": 5.989263627270702e-07}, {"id": 728, "seek": 439642, "start": 4403.06, "end": 4408.18, "text": " But if it's not that kind of industrial equipment at all, it will be empty, it will be missing.", "tokens": [583, 498, 309, 311, 406, 300, 733, 295, 9987, 5927, 412, 439, 11, 309, 486, 312, 6707, 11, 309, 486, 312, 5361, 13], "temperature": 0.0, "avg_logprob": -0.11101197683683006, "compression_ratio": 1.787037037037037, "no_speech_prob": 5.989263627270702e-07}, {"id": 729, "seek": 439642, "start": 4408.18, "end": 4413.34, "text": " And so Kapler system is really telling you whether or not it's a certain class of heavy", "tokens": [400, 370, 10988, 22732, 1185, 307, 534, 3585, 291, 1968, 420, 406, 309, 311, 257, 1629, 1508, 295, 4676], "temperature": 0.0, "avg_logprob": -0.11101197683683006, "compression_ratio": 1.787037037037037, "no_speech_prob": 5.989263627270702e-07}, {"id": 730, "seek": 439642, "start": 4413.34, "end": 4414.34, "text": " industrial equipment.", "tokens": [9987, 5927, 13], "temperature": 0.0, "avg_logprob": -0.11101197683683006, "compression_ratio": 1.787037037037037, "no_speech_prob": 5.989263627270702e-07}, {"id": 731, "seek": 439642, "start": 4414.34, "end": 4418.82, "text": " Now this is not leakage, this is actual information you actually have at the right time. It's", "tokens": [823, 341, 307, 406, 47799, 11, 341, 307, 3539, 1589, 291, 767, 362, 412, 264, 558, 565, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.11101197683683006, "compression_ratio": 1.787037037037037, "no_speech_prob": 5.989263627270702e-07}, {"id": 732, "seek": 441882, "start": 4418.82, "end": 4426.98, "text": " just that interpreting it, you have to be careful. So I would go through at least the", "tokens": [445, 300, 37395, 309, 11, 291, 362, 281, 312, 5026, 13, 407, 286, 576, 352, 807, 412, 1935, 264], "temperature": 0.0, "avg_logprob": -0.14031421745216455, "compression_ratio": 1.5663716814159292, "no_speech_prob": 2.9480079319910146e-06}, {"id": 733, "seek": 441882, "start": 4426.98, "end": 4433.38, "text": " top 10 or look for where the natural breakpoints are and really study these things carefully.", "tokens": [1192, 1266, 420, 574, 337, 689, 264, 3303, 1821, 20552, 366, 293, 534, 2979, 613, 721, 7500, 13], "temperature": 0.0, "avg_logprob": -0.14031421745216455, "compression_ratio": 1.5663716814159292, "no_speech_prob": 2.9480079319910146e-06}, {"id": 734, "seek": 441882, "start": 4433.38, "end": 4438.0199999999995, "text": " To make life easier for myself, what I tend to do is I try to throw some data away and", "tokens": [1407, 652, 993, 3571, 337, 2059, 11, 437, 286, 3928, 281, 360, 307, 286, 853, 281, 3507, 512, 1412, 1314, 293], "temperature": 0.0, "avg_logprob": -0.14031421745216455, "compression_ratio": 1.5663716814159292, "no_speech_prob": 2.9480079319910146e-06}, {"id": 735, "seek": 441882, "start": 4438.0199999999995, "end": 4446.139999999999, "text": " see if that matters. So in this case, I had a random forest which, let's go and see how", "tokens": [536, 498, 300, 7001, 13, 407, 294, 341, 1389, 11, 286, 632, 257, 4974, 6719, 597, 11, 718, 311, 352, 293, 536, 577], "temperature": 0.0, "avg_logprob": -0.14031421745216455, "compression_ratio": 1.5663716814159292, "no_speech_prob": 2.9480079319910146e-06}, {"id": 736, "seek": 444614, "start": 4446.14, "end": 4457.18, "text": " accurate it was,.889. What I did was I said here, let's go through our feature importance", "tokens": [8559, 309, 390, 11, 2411, 16919, 24, 13, 708, 286, 630, 390, 286, 848, 510, 11, 718, 311, 352, 807, 527, 4111, 7379], "temperature": 0.0, "avg_logprob": -0.21178677396954232, "compression_ratio": 1.3503649635036497, "no_speech_prob": 5.422200956672896e-06}, {"id": 737, "seek": 444614, "start": 4457.18, "end": 4468.780000000001, "text": " data frame and filter out those where the importance is greater than.005. So.005 is about here,", "tokens": [1412, 3920, 293, 6608, 484, 729, 689, 264, 7379, 307, 5044, 813, 2411, 628, 20, 13, 407, 2411, 628, 20, 307, 466, 510, 11], "temperature": 0.0, "avg_logprob": -0.21178677396954232, "compression_ratio": 1.3503649635036497, "no_speech_prob": 5.422200956672896e-06}, {"id": 738, "seek": 446878, "start": 4468.78, "end": 4477.9, "text": " it's kind of like where they really flatten off. So let's just keep those.", "tokens": [309, 311, 733, 295, 411, 689, 436, 534, 24183, 766, 13, 407, 718, 311, 445, 1066, 729, 13], "temperature": 0.0, "avg_logprob": -0.22998594556535992, "compression_ratio": 1.4821428571428572, "no_speech_prob": 2.2959102352615446e-06}, {"id": 739, "seek": 446878, "start": 4477.9, "end": 4485.34, "text": " And so that gives us a list of 25 column names. And so then I say, let's now create a new", "tokens": [400, 370, 300, 2709, 505, 257, 1329, 295, 3552, 7738, 5288, 13, 400, 370, 550, 286, 584, 11, 718, 311, 586, 1884, 257, 777], "temperature": 0.0, "avg_logprob": -0.22998594556535992, "compression_ratio": 1.4821428571428572, "no_speech_prob": 2.2959102352615446e-06}, {"id": 740, "seek": 446878, "start": 4485.34, "end": 4493.099999999999, "text": " data frame view which just contains those 25 columns, call split-vowels on it again,", "tokens": [1412, 3920, 1910, 597, 445, 8306, 729, 3552, 13766, 11, 818, 7472, 12, 85, 305, 1625, 322, 309, 797, 11], "temperature": 0.0, "avg_logprob": -0.22998594556535992, "compression_ratio": 1.4821428571428572, "no_speech_prob": 2.2959102352615446e-06}, {"id": 741, "seek": 449310, "start": 4493.1, "end": 4503.02, "text": " and create a new random forest. And let's see what happens. And you can see here the", "tokens": [293, 1884, 257, 777, 4974, 6719, 13, 400, 718, 311, 536, 437, 2314, 13, 400, 291, 393, 536, 510, 264], "temperature": 0.0, "avg_logprob": -0.29066731589181083, "compression_ratio": 1.402116402116402, "no_speech_prob": 2.6425766463944456e-06}, {"id": 742, "seek": 449310, "start": 4503.02, "end": 4514.46, "text": " R-squared basically didn't change,.891 versus.89. So it's actually increased a tiny bit.", "tokens": [497, 12, 33292, 1642, 1936, 994, 380, 1319, 11, 2411, 21115, 16, 5717, 2411, 21115, 13, 407, 309, 311, 767, 6505, 257, 5870, 857, 13], "temperature": 0.0, "avg_logprob": -0.29066731589181083, "compression_ratio": 1.402116402116402, "no_speech_prob": 2.6425766463944456e-06}, {"id": 743, "seek": 449310, "start": 4514.46, "end": 4521.660000000001, "text": " Generally speaking, removing redundant columns, obviously it shouldn't make it worse. If it", "tokens": [21082, 4124, 11, 12720, 40997, 13766, 11, 2745, 309, 4659, 380, 652, 309, 5324, 13, 759, 309], "temperature": 0.0, "avg_logprob": -0.29066731589181083, "compression_ratio": 1.402116402116402, "no_speech_prob": 2.6425766463944456e-06}, {"id": 744, "seek": 452166, "start": 4521.66, "end": 4525.58, "text": " makes it worse, they won't be redundant after all. It might make it a little better, because", "tokens": [1669, 309, 5324, 11, 436, 1582, 380, 312, 40997, 934, 439, 13, 467, 1062, 652, 309, 257, 707, 1101, 11, 570], "temperature": 0.0, "avg_logprob": -0.14588105128361628, "compression_ratio": 1.822695035460993, "no_speech_prob": 1.0289468264090829e-05}, {"id": 745, "seek": 452166, "start": 4525.58, "end": 4531.0599999999995, "text": " if you think about how we built these trees, when it's deciding what to split on, it's", "tokens": [498, 291, 519, 466, 577, 321, 3094, 613, 5852, 11, 562, 309, 311, 17990, 437, 281, 7472, 322, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.14588105128361628, "compression_ratio": 1.822695035460993, "no_speech_prob": 1.0289468264090829e-05}, {"id": 746, "seek": 452166, "start": 4531.0599999999995, "end": 4535.5, "text": " got less things to have to worry about trying, it's less often going to accidentally find", "tokens": [658, 1570, 721, 281, 362, 281, 3292, 466, 1382, 11, 309, 311, 1570, 2049, 516, 281, 15715, 915], "temperature": 0.0, "avg_logprob": -0.14588105128361628, "compression_ratio": 1.822695035460993, "no_speech_prob": 1.0289468264090829e-05}, {"id": 747, "seek": 452166, "start": 4535.5, "end": 4540.7, "text": " a crappy column. So it's got a slightly better opportunity to create a slightly better tree", "tokens": [257, 36531, 7738, 13, 407, 309, 311, 658, 257, 4748, 1101, 2650, 281, 1884, 257, 4748, 1101, 4230], "temperature": 0.0, "avg_logprob": -0.14588105128361628, "compression_ratio": 1.822695035460993, "no_speech_prob": 1.0289468264090829e-05}, {"id": 748, "seek": 452166, "start": 4540.7, "end": 4545.5, "text": " with slightly less data, but it's not going to change it by much. But it's going to make", "tokens": [365, 4748, 1570, 1412, 11, 457, 309, 311, 406, 516, 281, 1319, 309, 538, 709, 13, 583, 309, 311, 516, 281, 652], "temperature": 0.0, "avg_logprob": -0.14588105128361628, "compression_ratio": 1.822695035460993, "no_speech_prob": 1.0289468264090829e-05}, {"id": 749, "seek": 452166, "start": 4545.5, "end": 4549.42, "text": " it a bit faster and it's going to let us focus on what matters.", "tokens": [309, 257, 857, 4663, 293, 309, 311, 516, 281, 718, 505, 1879, 322, 437, 7001, 13], "temperature": 0.0, "avg_logprob": -0.14588105128361628, "compression_ratio": 1.822695035460993, "no_speech_prob": 1.0289468264090829e-05}, {"id": 750, "seek": 454942, "start": 4549.42, "end": 4557.7, "text": " So if I rerun feature importance now, I've now got 25. Now the key thing that's happened", "tokens": [407, 498, 286, 43819, 409, 4111, 7379, 586, 11, 286, 600, 586, 658, 3552, 13, 823, 264, 2141, 551, 300, 311, 2011], "temperature": 0.0, "avg_logprob": -0.12932663402338138, "compression_ratio": 1.6181818181818182, "no_speech_prob": 3.089483698204276e-06}, {"id": 751, "seek": 454942, "start": 4557.7, "end": 4565.1, "text": " is that when you remove redundant columns, you're also removing sources of collinearity.", "tokens": [307, 300, 562, 291, 4159, 40997, 13766, 11, 291, 434, 611, 12720, 7139, 295, 1263, 533, 17409, 13], "temperature": 0.0, "avg_logprob": -0.12932663402338138, "compression_ratio": 1.6181818181818182, "no_speech_prob": 3.089483698204276e-06}, {"id": 752, "seek": 454942, "start": 4565.1, "end": 4572.38, "text": " In other words, two columns that might be related to each other. Now collinearity doesn't", "tokens": [682, 661, 2283, 11, 732, 13766, 300, 1062, 312, 4077, 281, 1184, 661, 13, 823, 1263, 533, 17409, 1177, 380], "temperature": 0.0, "avg_logprob": -0.12932663402338138, "compression_ratio": 1.6181818181818182, "no_speech_prob": 3.089483698204276e-06}, {"id": 753, "seek": 454942, "start": 4572.38, "end": 4577.9400000000005, "text": " make your random forest less predictive, but if you have two columns that are related to", "tokens": [652, 428, 4974, 6719, 1570, 35521, 11, 457, 498, 291, 362, 732, 13766, 300, 366, 4077, 281], "temperature": 0.0, "avg_logprob": -0.12932663402338138, "compression_ratio": 1.6181818181818182, "no_speech_prob": 3.089483698204276e-06}, {"id": 754, "seek": 457794, "start": 4577.94, "end": 4583.259999999999, "text": " each other, this column is a little bit related to this column, and this column is a strong", "tokens": [1184, 661, 11, 341, 7738, 307, 257, 707, 857, 4077, 281, 341, 7738, 11, 293, 341, 7738, 307, 257, 2068], "temperature": 0.0, "avg_logprob": -0.12983436584472657, "compression_ratio": 1.8940092165898617, "no_speech_prob": 7.112422508726013e-07}, {"id": 755, "seek": 457794, "start": 4583.259999999999, "end": 4588.259999999999, "text": " driver of the dependent variable, then what's going to happen is that the importance is", "tokens": [6787, 295, 264, 12334, 7006, 11, 550, 437, 311, 516, 281, 1051, 307, 300, 264, 7379, 307], "temperature": 0.0, "avg_logprob": -0.12983436584472657, "compression_ratio": 1.8940092165898617, "no_speech_prob": 7.112422508726013e-07}, {"id": 756, "seek": 457794, "start": 4588.259999999999, "end": 4594.5, "text": " going to end up split between the two collinear columns. It's going to say both of those columns", "tokens": [516, 281, 917, 493, 7472, 1296, 264, 732, 1263, 533, 289, 13766, 13, 467, 311, 516, 281, 584, 1293, 295, 729, 13766], "temperature": 0.0, "avg_logprob": -0.12983436584472657, "compression_ratio": 1.8940092165898617, "no_speech_prob": 7.112422508726013e-07}, {"id": 757, "seek": 457794, "start": 4594.5, "end": 4597.099999999999, "text": " matter, so it's going to split it between the two.", "tokens": [1871, 11, 370, 309, 311, 516, 281, 7472, 309, 1296, 264, 732, 13], "temperature": 0.0, "avg_logprob": -0.12983436584472657, "compression_ratio": 1.8940092165898617, "no_speech_prob": 7.112422508726013e-07}, {"id": 758, "seek": 457794, "start": 4597.099999999999, "end": 4603.139999999999, "text": " So by removing some of those columns with very little impact, it makes your feature", "tokens": [407, 538, 12720, 512, 295, 729, 13766, 365, 588, 707, 2712, 11, 309, 1669, 428, 4111], "temperature": 0.0, "avg_logprob": -0.12983436584472657, "compression_ratio": 1.8940092165898617, "no_speech_prob": 7.112422508726013e-07}, {"id": 759, "seek": 460314, "start": 4603.14, "end": 4610.700000000001, "text": " importance plot clearer. So you can see here actually, yearMade was pretty close to couple", "tokens": [7379, 7542, 26131, 13, 407, 291, 393, 536, 510, 767, 11, 1064, 44, 762, 390, 1238, 1998, 281, 1916], "temperature": 0.0, "avg_logprob": -0.20966932746801484, "compression_ratio": 1.6044444444444443, "no_speech_prob": 1.679726210568333e-06}, {"id": 760, "seek": 460314, "start": 4610.700000000001, "end": 4615.62, "text": " system before, but there must have been a bunch of things that were collinear with yearMade,", "tokens": [1185, 949, 11, 457, 456, 1633, 362, 668, 257, 3840, 295, 721, 300, 645, 1263, 533, 289, 365, 1064, 44, 762, 11], "temperature": 0.0, "avg_logprob": -0.20966932746801484, "compression_ratio": 1.6044444444444443, "no_speech_prob": 1.679726210568333e-06}, {"id": 761, "seek": 460314, "start": 4615.62, "end": 4622.02, "text": " which makes perfect sense. Old industrial equipment wouldn't have had a bunch of technical", "tokens": [597, 1669, 2176, 2020, 13, 8633, 9987, 5927, 2759, 380, 362, 632, 257, 3840, 295, 6191], "temperature": 0.0, "avg_logprob": -0.20966932746801484, "compression_ratio": 1.6044444444444443, "no_speech_prob": 1.679726210568333e-06}, {"id": 762, "seek": 460314, "start": 4622.02, "end": 4626.900000000001, "text": " features that new ones would, for example. So it's actually saying, oh, okay, yearMade", "tokens": [4122, 300, 777, 2306, 576, 11, 337, 1365, 13, 407, 309, 311, 767, 1566, 11, 1954, 11, 1392, 11, 1064, 44, 762], "temperature": 0.0, "avg_logprob": -0.20966932746801484, "compression_ratio": 1.6044444444444443, "no_speech_prob": 1.679726210568333e-06}, {"id": 763, "seek": 462690, "start": 4626.9, "end": 4634.0599999999995, "text": " really, really matters. So I trust this feature importance better. The predictive accuracy", "tokens": [534, 11, 534, 7001, 13, 407, 286, 3361, 341, 4111, 7379, 1101, 13, 440, 35521, 14170], "temperature": 0.0, "avg_logprob": -0.1134990466538296, "compression_ratio": 1.704035874439462, "no_speech_prob": 4.356868885224685e-06}, {"id": 764, "seek": 462690, "start": 4634.0599999999995, "end": 4638.58, "text": " of the model is a tiny bit better, but this feature importance has a lot less collinearity", "tokens": [295, 264, 2316, 307, 257, 5870, 857, 1101, 11, 457, 341, 4111, 7379, 575, 257, 688, 1570, 1263, 533, 17409], "temperature": 0.0, "avg_logprob": -0.1134990466538296, "compression_ratio": 1.704035874439462, "no_speech_prob": 4.356868885224685e-06}, {"id": 765, "seek": 462690, "start": 4638.58, "end": 4642.16, "text": " to confuse us.", "tokens": [281, 28584, 505, 13], "temperature": 0.0, "avg_logprob": -0.1134990466538296, "compression_ratio": 1.704035874439462, "no_speech_prob": 4.356868885224685e-06}, {"id": 766, "seek": 462690, "start": 4642.16, "end": 4649.5, "text": " So let's talk about how this works. It's actually really simple, and not only is it really simple,", "tokens": [407, 718, 311, 751, 466, 577, 341, 1985, 13, 467, 311, 767, 534, 2199, 11, 293, 406, 787, 307, 309, 534, 2199, 11], "temperature": 0.0, "avg_logprob": -0.1134990466538296, "compression_ratio": 1.704035874439462, "no_speech_prob": 4.356868885224685e-06}, {"id": 767, "seek": 462690, "start": 4649.5, "end": 4656.5, "text": " it's a technique you can use not just for random forests, but for basically any kind", "tokens": [309, 311, 257, 6532, 291, 393, 764, 406, 445, 337, 4974, 21700, 11, 457, 337, 1936, 604, 733], "temperature": 0.0, "avg_logprob": -0.1134990466538296, "compression_ratio": 1.704035874439462, "no_speech_prob": 4.356868885224685e-06}, {"id": 768, "seek": 465650, "start": 4656.5, "end": 4664.54, "text": " of machine learning model. Interestingly, almost no one knows that. Many people will", "tokens": [295, 3479, 2539, 2316, 13, 30564, 11, 1920, 572, 472, 3255, 300, 13, 5126, 561, 486], "temperature": 0.0, "avg_logprob": -0.16766727381739124, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.9223010895075276e-05}, {"id": 769, "seek": 465650, "start": 4664.54, "end": 4670.18, "text": " tell you, oh, this particular kind of model, there's no way of interpreting it. The most", "tokens": [980, 291, 11, 1954, 11, 341, 1729, 733, 295, 2316, 11, 456, 311, 572, 636, 295, 37395, 309, 13, 440, 881], "temperature": 0.0, "avg_logprob": -0.16766727381739124, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.9223010895075276e-05}, {"id": 770, "seek": 465650, "start": 4670.18, "end": 4674.86, "text": " important interpretation of a model is knowing which things are important. That's almost", "tokens": [1021, 14174, 295, 257, 2316, 307, 5276, 597, 721, 366, 1021, 13, 663, 311, 1920], "temperature": 0.0, "avg_logprob": -0.16766727381739124, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.9223010895075276e-05}, {"id": 771, "seek": 465650, "start": 4674.86, "end": 4677.86, "text": " certainly not going to be true, because this technique I'm going to teach you actually", "tokens": [3297, 406, 516, 281, 312, 2074, 11, 570, 341, 6532, 286, 478, 516, 281, 2924, 291, 767], "temperature": 0.0, "avg_logprob": -0.16766727381739124, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.9223010895075276e-05}, {"id": 772, "seek": 465650, "start": 4677.86, "end": 4678.86, "text": " works for any kind of model.", "tokens": [1985, 337, 604, 733, 295, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16766727381739124, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.9223010895075276e-05}, {"id": 773, "seek": 465650, "start": 4678.86, "end": 4684.14, "text": " So here's what we're going to do. We're going to take our dataset, the bulldozers, and we've", "tokens": [407, 510, 311, 437, 321, 434, 516, 281, 360, 13, 492, 434, 516, 281, 747, 527, 28872, 11, 264, 4693, 2595, 41698, 11, 293, 321, 600], "temperature": 0.0, "avg_logprob": -0.16766727381739124, "compression_ratio": 1.7252747252747254, "no_speech_prob": 1.9223010895075276e-05}, {"id": 774, "seek": 468414, "start": 4684.14, "end": 4690.900000000001, "text": " got this column which we're trying to predict, which is price. And then we've got all of", "tokens": [658, 341, 7738, 597, 321, 434, 1382, 281, 6069, 11, 597, 307, 3218, 13, 400, 550, 321, 600, 658, 439, 295], "temperature": 0.0, "avg_logprob": -0.1668906741672092, "compression_ratio": 1.6079545454545454, "no_speech_prob": 1.4510367691400461e-05}, {"id": 775, "seek": 468414, "start": 4690.900000000001, "end": 4701.58, "text": " our independent variables. So here's an independent variable here, yearMade, plus a whole bunch", "tokens": [527, 6695, 9102, 13, 407, 510, 311, 364, 6695, 7006, 510, 11, 1064, 44, 762, 11, 1804, 257, 1379, 3840], "temperature": 0.0, "avg_logprob": -0.1668906741672092, "compression_ratio": 1.6079545454545454, "no_speech_prob": 1.4510367691400461e-05}, {"id": 776, "seek": 468414, "start": 4701.58, "end": 4706.26, "text": " of other variables. And remember, after we did a bit of trimming, we had 25 independent", "tokens": [295, 661, 9102, 13, 400, 1604, 11, 934, 321, 630, 257, 857, 295, 47212, 11, 321, 632, 3552, 6695], "temperature": 0.0, "avg_logprob": -0.1668906741672092, "compression_ratio": 1.6079545454545454, "no_speech_prob": 1.4510367691400461e-05}, {"id": 777, "seek": 468414, "start": 4706.26, "end": 4712.1, "text": " variables.", "tokens": [9102, 13], "temperature": 0.0, "avg_logprob": -0.1668906741672092, "compression_ratio": 1.6079545454545454, "no_speech_prob": 1.4510367691400461e-05}, {"id": 778, "seek": 471210, "start": 4712.1, "end": 4720.9800000000005, "text": " How do we figure out how important yearMade is? Well, we've got our whole random forest,", "tokens": [1012, 360, 321, 2573, 484, 577, 1021, 1064, 44, 762, 307, 30, 1042, 11, 321, 600, 658, 527, 1379, 4974, 6719, 11], "temperature": 0.0, "avg_logprob": -0.13433995454207712, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.586727714515291e-05}, {"id": 779, "seek": 471210, "start": 4720.9800000000005, "end": 4725.54, "text": " and we can find out our predictive accuracy. So we're going to put all of these rows through", "tokens": [293, 321, 393, 915, 484, 527, 35521, 14170, 13, 407, 321, 434, 516, 281, 829, 439, 295, 613, 13241, 807], "temperature": 0.0, "avg_logprob": -0.13433995454207712, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.586727714515291e-05}, {"id": 780, "seek": 471210, "start": 4725.54, "end": 4733.860000000001, "text": " our random forest, and we're going to spit out some predictions, and we're going to compare", "tokens": [527, 4974, 6719, 11, 293, 321, 434, 516, 281, 22127, 484, 512, 21264, 11, 293, 321, 434, 516, 281, 6794], "temperature": 0.0, "avg_logprob": -0.13433995454207712, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.586727714515291e-05}, {"id": 781, "seek": 471210, "start": 4733.860000000001, "end": 4739.1, "text": " them to the actual price to get, in this case for example, our root mean squared error and", "tokens": [552, 281, 264, 3539, 3218, 281, 483, 11, 294, 341, 1389, 337, 1365, 11, 527, 5593, 914, 8889, 6713, 293], "temperature": 0.0, "avg_logprob": -0.13433995454207712, "compression_ratio": 1.6851851851851851, "no_speech_prob": 2.586727714515291e-05}, {"id": 782, "seek": 473910, "start": 4739.1, "end": 4744.42, "text": " our R squared. And that's our starting point.", "tokens": [527, 497, 8889, 13, 400, 300, 311, 527, 2891, 935, 13], "temperature": 0.0, "avg_logprob": -0.1668773651123047, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.026138190558413e-06}, {"id": 783, "seek": 473910, "start": 4744.42, "end": 4752.240000000001, "text": " So now let's do exactly the same thing, but let's take the yearMade column and randomly", "tokens": [407, 586, 718, 311, 360, 2293, 264, 912, 551, 11, 457, 718, 311, 747, 264, 1064, 44, 762, 7738, 293, 16979], "temperature": 0.0, "avg_logprob": -0.1668773651123047, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.026138190558413e-06}, {"id": 784, "seek": 473910, "start": 4752.240000000001, "end": 4760.740000000001, "text": " shuffle it, randomly permute just that column. So now yearMade has exactly the same distribution", "tokens": [39426, 309, 11, 16979, 4784, 1169, 445, 300, 7738, 13, 407, 586, 1064, 44, 762, 575, 2293, 264, 912, 7316], "temperature": 0.0, "avg_logprob": -0.1668773651123047, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.026138190558413e-06}, {"id": 785, "seek": 473910, "start": 4760.740000000001, "end": 4764.860000000001, "text": " as before, same means, standard deviation, but it's going to have no relationship to", "tokens": [382, 949, 11, 912, 1355, 11, 3832, 25163, 11, 457, 309, 311, 516, 281, 362, 572, 2480, 281], "temperature": 0.0, "avg_logprob": -0.1668773651123047, "compression_ratio": 1.6237113402061856, "no_speech_prob": 2.026138190558413e-06}, {"id": 786, "seek": 476486, "start": 4764.86, "end": 4770.0199999999995, "text": " the dependent variable at all because we totally randomly reordered it. So before we might", "tokens": [264, 12334, 7006, 412, 439, 570, 321, 3879, 16979, 319, 765, 4073, 309, 13, 407, 949, 321, 1062], "temperature": 0.0, "avg_logprob": -0.16199267994273792, "compression_ratio": 1.4134078212290502, "no_speech_prob": 3.1875583772489335e-06}, {"id": 787, "seek": 476486, "start": 4770.0199999999995, "end": 4778.7, "text": " have found our R squared was.89, and then after we shuffle yearMade, we check again", "tokens": [362, 1352, 527, 497, 8889, 390, 2411, 21115, 11, 293, 550, 934, 321, 39426, 1064, 44, 762, 11, 321, 1520, 797], "temperature": 0.0, "avg_logprob": -0.16199267994273792, "compression_ratio": 1.4134078212290502, "no_speech_prob": 3.1875583772489335e-06}, {"id": 788, "seek": 476486, "start": 4778.7, "end": 4787.86, "text": " and now it's like.8. Our score got much worse when we destroyed that variable.", "tokens": [293, 586, 309, 311, 411, 2411, 23, 13, 2621, 6175, 658, 709, 5324, 562, 321, 8937, 300, 7006, 13], "temperature": 0.0, "avg_logprob": -0.16199267994273792, "compression_ratio": 1.4134078212290502, "no_speech_prob": 3.1875583772489335e-06}, {"id": 789, "seek": 478786, "start": 4787.86, "end": 4795.78, "text": " Okay, let's try again. Let's put yearMade back to how it was, and this time let's take", "tokens": [1033, 11, 718, 311, 853, 797, 13, 961, 311, 829, 1064, 44, 762, 646, 281, 577, 309, 390, 11, 293, 341, 565, 718, 311, 747], "temperature": 0.0, "avg_logprob": -0.1612672194456443, "compression_ratio": 1.572289156626506, "no_speech_prob": 2.4824753381835762e-06}, {"id": 790, "seek": 478786, "start": 4795.78, "end": 4806.259999999999, "text": " enclosure and shuffle that. And we find this time with enclosure, it's.84. And we can", "tokens": [34093, 293, 39426, 300, 13, 400, 321, 915, 341, 565, 365, 34093, 11, 309, 311, 2411, 25494, 13, 400, 321, 393], "temperature": 0.0, "avg_logprob": -0.1612672194456443, "compression_ratio": 1.572289156626506, "no_speech_prob": 2.4824753381835762e-06}, {"id": 791, "seek": 478786, "start": 4806.259999999999, "end": 4814.62, "text": " say, oh okay, so the amount of decrease in our score for yearMade was.09, and the amount", "tokens": [584, 11, 1954, 1392, 11, 370, 264, 2372, 295, 11514, 294, 527, 6175, 337, 1064, 44, 762, 390, 2411, 13811, 11, 293, 264, 2372], "temperature": 0.0, "avg_logprob": -0.1612672194456443, "compression_ratio": 1.572289156626506, "no_speech_prob": 2.4824753381835762e-06}, {"id": 792, "seek": 481462, "start": 4814.62, "end": 4822.9, "text": " of decrease in our score for enclosure was.05. And this is going to give us our feature", "tokens": [295, 11514, 294, 527, 6175, 337, 34093, 390, 2411, 13328, 13, 400, 341, 307, 516, 281, 976, 505, 527, 4111], "temperature": 0.0, "avg_logprob": -0.2813486916678292, "compression_ratio": 1.2427184466019416, "no_speech_prob": 1.1478678061394021e-05}, {"id": 793, "seek": 481462, "start": 4822.9, "end": 4828.5, "text": " importances for each one of our columns.", "tokens": [974, 2676, 337, 1184, 472, 295, 527, 13766, 13], "temperature": 0.0, "avg_logprob": -0.2813486916678292, "compression_ratio": 1.2427184466019416, "no_speech_prob": 1.1478678061394021e-05}, {"id": 794, "seek": 482850, "start": 4828.5, "end": 4847.98, "text": " Question from the audience. You could remove the column and train a whole new random forest,", "tokens": [14464, 490, 264, 4034, 13, 509, 727, 4159, 264, 7738, 293, 3847, 257, 1379, 777, 4974, 6719, 11], "temperature": 0.0, "avg_logprob": -0.2999321330677379, "compression_ratio": 1.3609022556390977, "no_speech_prob": 2.2827463908470236e-05}, {"id": 795, "seek": 482850, "start": 4847.98, "end": 4853.82, "text": " but that's going to be really slow. This way we can keep our random forest and just test", "tokens": [457, 300, 311, 516, 281, 312, 534, 2964, 13, 639, 636, 321, 393, 1066, 527, 4974, 6719, 293, 445, 1500], "temperature": 0.0, "avg_logprob": -0.2999321330677379, "compression_ratio": 1.3609022556390977, "no_speech_prob": 2.2827463908470236e-05}, {"id": 796, "seek": 485382, "start": 4853.82, "end": 4859.82, "text": " the predictive accuracy of it again. So this is nice and fast by comparison. In this case,", "tokens": [264, 35521, 14170, 295, 309, 797, 13, 407, 341, 307, 1481, 293, 2370, 538, 9660, 13, 682, 341, 1389, 11], "temperature": 0.0, "avg_logprob": -0.26368203370467475, "compression_ratio": 1.639269406392694, "no_speech_prob": 1.497085759183392e-05}, {"id": 797, "seek": 485382, "start": 4859.82, "end": 4867.9, "text": " we just have to rerun every row forward through the forest for each shuffled column.", "tokens": [321, 445, 362, 281, 43819, 409, 633, 5386, 2128, 807, 264, 6719, 337, 1184, 402, 33974, 7738, 13], "temperature": 0.0, "avg_logprob": -0.26368203370467475, "compression_ratio": 1.639269406392694, "no_speech_prob": 1.497085759183392e-05}, {"id": 798, "seek": 485382, "start": 4867.9, "end": 4874.38, "text": " We're just basically doing predictions after shuffling.", "tokens": [492, 434, 445, 1936, 884, 21264, 934, 402, 1245, 1688, 13], "temperature": 0.0, "avg_logprob": -0.26368203370467475, "compression_ratio": 1.639269406392694, "no_speech_prob": 1.497085759183392e-05}, {"id": 799, "seek": 485382, "start": 4874.38, "end": 4877.66, "text": " So if you want to do multi-coloniality, would you do two of them and then random shuffle", "tokens": [407, 498, 291, 528, 281, 360, 4825, 12, 8768, 266, 831, 507, 11, 576, 291, 360, 732, 295, 552, 293, 550, 4974, 39426], "temperature": 0.0, "avg_logprob": -0.26368203370467475, "compression_ratio": 1.639269406392694, "no_speech_prob": 1.497085759183392e-05}, {"id": 800, "seek": 485382, "start": 4877.66, "end": 4879.46, "text": " and then three of them random shuffle?", "tokens": [293, 550, 1045, 295, 552, 4974, 39426, 30], "temperature": 0.0, "avg_logprob": -0.26368203370467475, "compression_ratio": 1.639269406392694, "no_speech_prob": 1.497085759183392e-05}, {"id": 801, "seek": 487946, "start": 4879.46, "end": 4884.66, "text": " I don't think you mean multi-coloniality, I think you mean looking for interaction effects.", "tokens": [286, 500, 380, 519, 291, 914, 4825, 12, 8768, 266, 831, 507, 11, 286, 519, 291, 914, 1237, 337, 9285, 5065, 13], "temperature": 0.0, "avg_logprob": -0.16555274801051362, "compression_ratio": 1.5862068965517242, "no_speech_prob": 2.6688165235100314e-05}, {"id": 802, "seek": 487946, "start": 4884.66, "end": 4889.42, "text": " So if you want to say which pairs of variables are most important, you could do exactly the", "tokens": [407, 498, 291, 528, 281, 584, 597, 15494, 295, 9102, 366, 881, 1021, 11, 291, 727, 360, 2293, 264], "temperature": 0.0, "avg_logprob": -0.16555274801051362, "compression_ratio": 1.5862068965517242, "no_speech_prob": 2.6688165235100314e-05}, {"id": 803, "seek": 487946, "start": 4889.42, "end": 4897.9800000000005, "text": " same thing, each pair in turn. In practice, there are better ways to do that because that's", "tokens": [912, 551, 11, 1184, 6119, 294, 1261, 13, 682, 3124, 11, 456, 366, 1101, 2098, 281, 360, 300, 570, 300, 311], "temperature": 0.0, "avg_logprob": -0.16555274801051362, "compression_ratio": 1.5862068965517242, "no_speech_prob": 2.6688165235100314e-05}, {"id": 804, "seek": 487946, "start": 4897.9800000000005, "end": 4906.66, "text": " obviously computationally pretty expensive. So we'll try and find time to do that if we", "tokens": [2745, 24903, 379, 1238, 5124, 13, 407, 321, 603, 853, 293, 915, 565, 281, 360, 300, 498, 321], "temperature": 0.0, "avg_logprob": -0.16555274801051362, "compression_ratio": 1.5862068965517242, "no_speech_prob": 2.6688165235100314e-05}, {"id": 805, "seek": 487946, "start": 4906.66, "end": 4907.66, "text": " can.", "tokens": [393, 13], "temperature": 0.0, "avg_logprob": -0.16555274801051362, "compression_ratio": 1.5862068965517242, "no_speech_prob": 2.6688165235100314e-05}, {"id": 806, "seek": 490766, "start": 4907.66, "end": 4913.38, "text": " We now have a model which is a little bit more accurate and we've learned a lot more", "tokens": [492, 586, 362, 257, 2316, 597, 307, 257, 707, 857, 544, 8559, 293, 321, 600, 3264, 257, 688, 544], "temperature": 0.0, "avg_logprob": -0.16856750543566718, "compression_ratio": 1.45, "no_speech_prob": 5.862793386768317e-06}, {"id": 807, "seek": 490766, "start": 4913.38, "end": 4925.38, "text": " about it. So we're out of time. What I would suggest you try doing now before next class", "tokens": [466, 309, 13, 407, 321, 434, 484, 295, 565, 13, 708, 286, 576, 3402, 291, 853, 884, 586, 949, 958, 1508], "temperature": 0.0, "avg_logprob": -0.16856750543566718, "compression_ratio": 1.45, "no_speech_prob": 5.862793386768317e-06}, {"id": 808, "seek": 490766, "start": 4925.38, "end": 4935.5, "text": " for this bulldozer's dataset is go through the top 5 or 10 predictors and try and learn", "tokens": [337, 341, 4693, 2595, 4527, 311, 28872, 307, 352, 807, 264, 1192, 1025, 420, 1266, 6069, 830, 293, 853, 293, 1466], "temperature": 0.0, "avg_logprob": -0.16856750543566718, "compression_ratio": 1.45, "no_speech_prob": 5.862793386768317e-06}, {"id": 809, "seek": 493550, "start": 4935.5, "end": 4942.18, "text": " what you can about how to draw plots in pandas and try to come back with some insights about", "tokens": [437, 291, 393, 466, 577, 281, 2642, 28609, 294, 4565, 296, 293, 853, 281, 808, 646, 365, 512, 14310, 466], "temperature": 0.0, "avg_logprob": -0.21758576865508178, "compression_ratio": 1.814516129032258, "no_speech_prob": 1.3845818102709018e-05}, {"id": 810, "seek": 493550, "start": 4942.18, "end": 4945.9, "text": " what's the relationship between year made and the dependent variable, what's the histogram", "tokens": [437, 311, 264, 2480, 1296, 1064, 1027, 293, 264, 12334, 7006, 11, 437, 311, 264, 49816], "temperature": 0.0, "avg_logprob": -0.21758576865508178, "compression_ratio": 1.814516129032258, "no_speech_prob": 1.3845818102709018e-05}, {"id": 811, "seek": 493550, "start": 4945.9, "end": 4953.02, "text": " of year made. Try and find some possible, now that you know year made is really important,", "tokens": [295, 1064, 1027, 13, 6526, 293, 915, 512, 1944, 11, 586, 300, 291, 458, 1064, 1027, 307, 534, 1021, 11], "temperature": 0.0, "avg_logprob": -0.21758576865508178, "compression_ratio": 1.814516129032258, "no_speech_prob": 1.3845818102709018e-05}, {"id": 812, "seek": 493550, "start": 4953.02, "end": 4958.32, "text": " is there some noise in that column which we could fix, are there some weird encodings", "tokens": [307, 456, 512, 5658, 294, 300, 7738, 597, 321, 727, 3191, 11, 366, 456, 512, 3657, 2058, 378, 1109], "temperature": 0.0, "avg_logprob": -0.21758576865508178, "compression_ratio": 1.814516129032258, "no_speech_prob": 1.3845818102709018e-05}, {"id": 813, "seek": 493550, "start": 4958.32, "end": 4964.18, "text": " in that column that we could fix. This idea I had that maybe a couple of systems is there", "tokens": [294, 300, 7738, 300, 321, 727, 3191, 13, 639, 1558, 286, 632, 300, 1310, 257, 1916, 295, 3652, 307, 456], "temperature": 0.0, "avg_logprob": -0.21758576865508178, "compression_ratio": 1.814516129032258, "no_speech_prob": 1.3845818102709018e-05}, {"id": 814, "seek": 496418, "start": 4964.18, "end": 4968.58, "text": " entirely because it's collinear with something else. Do you want to try and figure out whether", "tokens": [7696, 570, 309, 311, 1263, 533, 289, 365, 746, 1646, 13, 1144, 291, 528, 281, 853, 293, 2573, 484, 1968], "temperature": 0.0, "avg_logprob": -0.21106763524333322, "compression_ratio": 1.781021897810219, "no_speech_prob": 1.8925000404124148e-05}, {"id": 815, "seek": 496418, "start": 4968.58, "end": 4972.58, "text": " that's true? If so, how would you do it?", "tokens": [300, 311, 2074, 30, 759, 370, 11, 577, 576, 291, 360, 309, 30], "temperature": 0.0, "avg_logprob": -0.21106763524333322, "compression_ratio": 1.781021897810219, "no_speech_prob": 1.8925000404124148e-05}, {"id": 816, "seek": 496418, "start": 4972.58, "end": 4978.26, "text": " Fi product class desk, that rings alarm bells for me. It sounds like it might be a high", "tokens": [38245, 1674, 1508, 10026, 11, 300, 11136, 14183, 25474, 337, 385, 13, 467, 3263, 411, 309, 1062, 312, 257, 1090], "temperature": 0.0, "avg_logprob": -0.21106763524333322, "compression_ratio": 1.781021897810219, "no_speech_prob": 1.8925000404124148e-05}, {"id": 817, "seek": 496418, "start": 4978.26, "end": 4982.18, "text": " cardinality categorical variable. It might be something with lots and lots of levels", "tokens": [2920, 259, 1860, 19250, 804, 7006, 13, 467, 1062, 312, 746, 365, 3195, 293, 3195, 295, 4358], "temperature": 0.0, "avg_logprob": -0.21106763524333322, "compression_ratio": 1.781021897810219, "no_speech_prob": 1.8925000404124148e-05}, {"id": 818, "seek": 496418, "start": 4982.18, "end": 4986.740000000001, "text": " because it sounds like it's like a model name. So go and have a look at that model name.", "tokens": [570, 309, 3263, 411, 309, 311, 411, 257, 2316, 1315, 13, 407, 352, 293, 362, 257, 574, 412, 300, 2316, 1315, 13], "temperature": 0.0, "avg_logprob": -0.21106763524333322, "compression_ratio": 1.781021897810219, "no_speech_prob": 1.8925000404124148e-05}, {"id": 819, "seek": 496418, "start": 4986.740000000001, "end": 4990.22, "text": " Does it have some ordering to it? Could you make it an ordinal variable to make it better?", "tokens": [4402, 309, 362, 512, 21739, 281, 309, 30, 7497, 291, 652, 309, 364, 4792, 2071, 7006, 281, 652, 309, 1101, 30], "temperature": 0.0, "avg_logprob": -0.21106763524333322, "compression_ratio": 1.781021897810219, "no_speech_prob": 1.8925000404124148e-05}, {"id": 820, "seek": 499022, "start": 4990.22, "end": 4994.46, "text": " Does it have some kind of hierarchical structure in the string that we could split it on like", "tokens": [4402, 309, 362, 512, 733, 295, 35250, 804, 3877, 294, 264, 6798, 300, 321, 727, 7472, 309, 322, 411], "temperature": 0.0, "avg_logprob": -0.17938050164116753, "compression_ratio": 1.5254237288135593, "no_speech_prob": 8.93961714609759e-06}, {"id": 821, "seek": 499022, "start": 4994.46, "end": 5001.9800000000005, "text": " hyphen to create more sub-columns? Have a think about this. So try and make it so that", "tokens": [2477, 47059, 281, 1884, 544, 1422, 12, 8768, 449, 3695, 30, 3560, 257, 519, 466, 341, 13, 407, 853, 293, 652, 309, 370, 300], "temperature": 0.0, "avg_logprob": -0.17938050164116753, "compression_ratio": 1.5254237288135593, "no_speech_prob": 8.93961714609759e-06}, {"id": 822, "seek": 499022, "start": 5001.9800000000005, "end": 5009.34, "text": " by Tuesday when you come back, ideally you've got a better accuracy than what I just showed", "tokens": [538, 10017, 562, 291, 808, 646, 11, 22915, 291, 600, 658, 257, 1101, 14170, 813, 437, 286, 445, 4712], "temperature": 0.0, "avg_logprob": -0.17938050164116753, "compression_ratio": 1.5254237288135593, "no_speech_prob": 8.93961714609759e-06}, {"id": 823, "seek": 499022, "start": 5009.34, "end": 5014.46, "text": " because you found some new insights, or at least that you can tell the class about some", "tokens": [570, 291, 1352, 512, 777, 14310, 11, 420, 412, 1935, 300, 291, 393, 980, 264, 1508, 466, 512], "temperature": 0.0, "avg_logprob": -0.17938050164116753, "compression_ratio": 1.5254237288135593, "no_speech_prob": 8.93961714609759e-06}, {"id": 824, "seek": 501446, "start": 5014.46, "end": 5021.46, "text": " things you've learned about how heavy industrial equipment options work in practice.", "tokens": [50364, 721, 291, 600, 3264, 466, 577, 4676, 9987, 5927, 3956, 589, 294, 3124, 13, 50714], "temperature": 0.0, "avg_logprob": -0.24738294938031366, "compression_ratio": 1.0769230769230769, "no_speech_prob": 1.9522500224411488e-05}], "language": "en"}