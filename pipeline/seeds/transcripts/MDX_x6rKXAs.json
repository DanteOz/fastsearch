{"text": " I just briefly wanted to say, so the reason I was gone last week was I was speaking at this international summer school on bias and discrimination in AI at Avado, which is the AI institute, Yoshua Bengio started in Montreal. And there was a really, really fantastic lineup of speakers. I really enjoyed it. And the whole thing was recorded and will be released as a MOOC later this year. So I just wanted to let you know about that to be on the lookout for it. Because it was a really neat blend of they had machine learning researchers from Google and Microsoft and other industry places, as well as academics, as well as a human rights lawyer and officials in the Canadian government talking about how Canada is dealing with AI. So it was a really interesting interdisciplinary week. So be on the lookout for that. Yeah, with that, I'll let Jeremy continue and then I'll be jumping back in later. Hi, so I had some interesting questions after the last lesson and also done some interesting research since the last lesson, so I thought I'd share some extra stuff with you. Before I do, is there any questions or requests for things that we've covered last time that you'd like more information about or things that you're not quite clear on? Okay we'll just jump in any time if you want me to dive into something more closely. But last time we got to an accuracy on IMDB movie review sentiment classification of 94.1%, which equaled the state of the art result as of 2017, which is when the ULM fit paper came out. However, the ULM fit paper increased that a lot up to 95.4% accuracy. It's important to note the difference between 94.1 and 95.4. You might think that's not much, it's just a percent. But actually any time you're talking about accuracies that are much higher than 50%, you're better off reframing them as error rates. So actually we've changed the error rate with ULM fit from 5.9% to 4.6%. When you think of it that way, you realize that the actual number of errors you would make has decreased in relative terms by about 25%. So when you're kind of talking about high accuracies, which is pretty standard, most of the time you want to be thinking in terms of error rates, not accuracy, and then actually thinking about the number of errors that are being made, so the relative change in error rate. So there's a really big jump in terms of the error rate improvement between 94.1 and 95.4. So I wanted to dig into how do we close that gap? What's the difference between the state-of-the-art model as of 2017 and the state-of-the-art model as of 2019? So I've added this extra notebook called ednimdb more where we will see what some of the extra things we can do are. The first difference is a single extra word here in the language model. This is the language model where we're starting with the Wikipedia pre-trained language model, which is automatically downloaded for us by Fast.ai, and we're fine-tuning it to become an IMDB movie review language model, so something that is good at predicting the next word of a movie review. And last time, we just asked to include two directories, train and test, and so that contains all of the movie reviews in the train set and all of the movie reviews in the test set, and they have labels, so that allows us in the training set to train and the test set to see how well we're doing. However, one of the interesting things about the IMDB data set is it also has a directory called unsup, which stands for unsupervised. And this is a directory of movie reviews that do not have a rating attached to them. So it's got the English text of a movie review, but there's nothing about polarity, there's nothing saying whether they liked it or not. This is a super common situation where you have some data that is labeled and some data that is unlabeled. And you might think initially, well what's the point in terms of building a sentiment analysis tool, what's the point of movie reviews that don't actually have any labels, so we don't actually know whether they liked them or not, because there's 50,000 of these movies in the unsupervised folder. That sounds like maybe it's a waste of time. But actually there's a whole area of machine learning which is called the study of semi-supervised learning. And semi-supervised learning tries to answer this question, can we do something useful with unlabeled data? And this turns out to be really important in practice, because most of the time you will have some data that has labels and some data that has no labels. And in industry, most companies I talk to are not aware of semi-supervised learning, and so the default situation I find when I talk to companies is they say, oh we can't start this machine learning project until we've finished labeling all the data. And that often can take years. And so if you hear people say that, your answer should be, no, let's not wait, let's use the data that we have already, and for the data that's not labeled, let's try and use same semi-supervised learning techniques. For NLP classification, there's actually something very simple we can do with the unsupervised folder, and that is we can add it to our language model. Because the language model is just something that learns to predict the next word of a sentence. It doesn't need at all to know whether it was positive or negative sentiment. And so we've got 25,000 movie reviews in train, 25,000 movie reviews in test, and another 50,000 movie reviews in the unsupervised folder. So if we add that folder into the language model, we have twice as much data to build a language model with. So although the unsupervised folder doesn't tell us anything about whether people liked the movie or not, remember that building a language model allows us to build something that can learn about the domain, about the language in that domain. So it can find out what kinds of words come after what kinds of other words and what kinds of concepts appear. And so by having twice as much data, we're going to be able to build, hopefully, a much better language model. So this is an incredibly easy thing to do. You just say include this extra folder. But conceptually, it's super powerful, right? Because now we can go ahead and train the model. And just like before, we can start by fine-tuning just the embedding layers, because by default we're using pre-training, so it's frozen, and then we unfreeze. But now, because we've got much more data, we can do like 10 epochs. We can train for much longer without overfitting the language model. So that was the first thing that we did differently in IMDB more. We added the unsupervised folder and we increased the number of epochs from 4 to 10. Now you'll find that this takes on an average consumer graphics card of kind of the 2018 frame, about an hour or so per epoch, so you're going to have to run this overnight. And so learning how to manage models that run overnight is an important part of becoming an effective deep learning practitioner. And really the key thing for stuff that takes that long is to do it as little as possible. So when I was developing this notebook, I spent most of my time working on the smaller data set without the unsupervised folder and just training for one epoch or two, so it would only take about an hour total. And then once everything was working really well, then I ran it overnight for longer. So if something's going to take ages to run, don't let that make you think, oh, I can't do it. Start it running before you go to bed and it'll be finished when you get up in the morning. The next thing I did was I added this little bit here, 2FP16. What that does is it tells Fast.ai to use something called mixed precision training. And mixed precision training means that rather than using 32-bit floats, which are also called single precision floats, we're going to use 16-bit floats, also called half precision floats. So I think Rachel, did you cover floating point briefly? I think you did, right? So Rachel's very briefly covered floating point and pointed out that there's more detail in the computational linear algebra course. But basically all of the numbers in your computer are stored as bits, as on and off, and until quite recently most people thought that 32 bits was the minimum you needed to do something useful with floating point values. Because for less than that, you get very little accuracy. But then people kind of started thinking, well, why do we need, what's wrong with having very little accuracy? Because deep learning models are meant to be approximate. Like they're kind of meant to be rough guesses. We kind of like things being a bit stochastic in them, it makes them generalize more. Maybe we could try using even smaller precision values. And so people started trying to play around with half precision floating point. So most CPUs actually don't support half precision floating point. But in recent times, like in the last two years or so, some GPUs started supporting half precision floating point. And interestingly, it doesn't just make it twice as fast. You might think, oh, there's half as many bits for each number to use, so maybe it'll run twice as fast. But what actually happened was Nvidia in their GPUs and Google in their TPUs both added special hardware that actually runs 8 to 10 times faster when you use half precision. So suddenly it becomes very important to try to use that. Because nowadays, if you buy a gaming card from Nvidia, like a GTX 2060 or 2070 or 2080, which will cost you $400, somewhere between $400 and $800, so like within the realm of regular folks, you actually get this half precision floating point speedup. Nvidia calls them tensor cores. So even if you're not working in some giant company or giant research lab, this is actually important for us normal folks nowadays. Now the problem is that 16-bit floating point, as I mentioned, it is pretty inaccurate. And although there are some parts of the neural net where we don't care about that, like generally actually calculating the forward pass and actually calculating the gradients, half precision floating point is fine. It turns out that single precision floating point is important for other kinds of calculations, like when we actually calculate the loss function or when we actually multiply the gradient by the learning rate. You can get these really small numbers and in half precision floating point, they can kind of vanish down to zero. So in practice, we have to use something called mixed precision. And so mixed precision is basically where your model has to do some calculations in half precision and some calculations in single precision. So if that sounds complicated, well that's because it is a bit complicated and you wouldn't want to do this in a library that doesn't support mixed precision. The good news is that fast.ai actually, if you asked add to FP16, it does all that for you. And so if you're running this on a 20 series GPU or if you're using AWS, if you're using one of their P3 instances, this will give you a 2-3x speedup on this LSTM, on this recurrent neural network. So this way I was able to get down to 20 minute epochs or so. I just wanted to note on the topic of recurrent neural networks that that's what we'll be diving into next, so either later today or next class to talk more about how those work under the hood. The next thing I wanted to talk about was a bit more detail about what happens when we save encoder. You see we've got two lines here. We save the model, so that's saving the parameters of the model, but then there's also something here that says save encoder. What's that doing? Rachel will be talking more about this when she talks about neural machine translation in an upcoming lesson, but I thought I'd kind of give you a quick summary of what's going on. So we start with some vector input representing the embeddings of the previous words in a sentence and the goal is that we then want to take that and we want to spit out of it is what is the next word going to be. So here's all the words up to word say t-1 and this is going to be the representation as a vector of word t. So what we do is we put all this through a neural net, specifically a recurrent neural network, in the case of ULMFIT, and out of that recurrent neural network it's going to spit out a vector of activations. That's what neural networks do. They spit out a vector of activations. And so what we're hoping is that we can train the recurrent neural network to spit out a vector of activations that represents the meaning and structure of the input sentence. Because if we can, we could then have another little model here which takes as input a vector representing the structure of a sentence so far and spits out as output a prediction of what the next word is. So you can think of this model that we built, which is a language model learner, as consisting of two parts. There's the part that takes the sentence so far as input and spits out a representation of that sentence so far, and there's a bit that takes the representation of the sentence so far and turns it into a prediction of the next word. And so these things have names. The thing that takes the input sentence and spits out a representation of it is called the encoder. And the thing which takes the encoded representation and turns it into a prediction of the next word is called the classifier. And so what we're going to be doing for fine-tuning, because we don't actually want a language model, we want to end up with a sentiment classification model, is that when we get to the next stage where we're going to train the classifier, train the sentiment classifier, is we don't care about this bit. We don't care about predicting the next word. So we want something that only has this bit. So the model has both bits in, but there's a function called saveEncoder which just saves that bit. And so later on, after we create a text classifier learner, something we can use to classify text rather than predict the next word of a sentence, we then use loadEncoder to just take that encoder and put it into our new model. So that's what's going on there. And so when you get to neural machine translation, you'll be keeping something that looks very, very similar for the encoder, but instead of a classifier, you'll be creating something called a decoder, which is kind of an encoder in reverse that will actually spit out not just a single word, but an entire translated sentence. So that's the thing that you'll be doing later with Rachel. And that kind of model is called a sequence-to-sequence model, often just written as sec2sec. Can you turn those ones on? Maybe just turn them off and on again. So we train this for longer, for 10 epochs, to get a more accurate language model. And then the rest of this is... oh, there was one other change we made, which is here. What does dropmult do? Have you learnt about dropout already? So quick refresher. Dropout is the thing where we have some activations, like here, and what we do is at random we delete some of them, say half. We delete a different bunch each time through. We can do that on any layer or every layer of the model. What that does is it allows the model to generalize better because it can't learn to have a single activation do a single thing. It has to find a more general solution. The particular kind of recurrent neural network that we're using for this is called an AWD LSTM. An AWD LSTM is just a regular recurrent neural network of the kind that Rachel will be telling you about. But the researchers who built it in this paper called Regularizing and Optimizing LSTM Language Models, the lead author, a guy called Stephen Meridy, he thought of lots of different ways to add dropout in many different parts of the model. So it basically allows us to add a lot more regularization of a lot more different types. There's actually five kinds of dropout in an AWD LSTM. It would be very annoying to have to try to find five different hyperparameters of how much dropout for every different type. And in their paper and in their recommendations in their GitHub repo, they actually have suggestions for specific amounts of dropout that they found work well for various different problems. What we do in Fast AI is we just have a standard set amount of dropout for each of those five things and then you just pick a single number and to say multiply them all by this number. So to use our default recommendations, you would just use one, which means it's actually quite a lot of dropout if you put one. Or if you wanted to use a tenth as much, you would put 0.1, for example. So if you're overfitting, you would generally increase dropmult, and if you're underfitting, you would generally decrease dropmult. So we did some new research this week into the impact of dropmult on the sentiment analysis classifier, which is something that hasn't been ever published in the research before. The kind of assumption had been, or our assumption had been, that the thing we should try to do is to create the most accurate language model we can. And we found that having quite a small amount of dropout gave us a more accurate language model. But then this week, I tried doing some experiments where I actually increased the amount of regularization. So I increased dropmult. The accuracy of the language model was worse, but I went on to train, fine-tune that, to transfer alone that to a classifier anyway. And what we discovered, to our surprise, was that language models with more regularization, even though they were a bit less accurate at predicting the next word of a sentence, turned out to be more accurate classifiers when we actually got to the sentiment analysis stage. And so, as I said, this is all new unpublished research, so it's a little bit early days, but we've tried quite a few different data sets and quite a few different hyperparameters and we are consistently finding this. The other thing we found is that when we use more regularization in the language model, the training of the classifier is much more resilient, it's much less fiddly. We found with less dropout, it would very often kind of go off to infinity, the loss, you know, when we'd have to find exactly the right learning rate and exactly the right amount of time to train it for, which was super annoying. Yes, Rachel. What range of values can you put in for the dropout multiplier? You can put in pretty much anything, I mean, obviously if it ends up with a dropout of greater than 1, that would be silly. In practice, we use between.01 and 1, we haven't tried anything higher than that. Does this give you a dropout of 1? No, you'll never get a dropout of 1. This will give us by default the kind of AWD LSTM standard dropouts, which you'd have to check the source code to see what they are. I can't remember off the top of my head. Looks like it goes here. Okay. So it comes from ModelMatter in TextMatter. There we go. So for an AWD LSTM, there's the dropouts, the 5 dropouts that we use. So if you have a dropout of 1, it deletes all of your activations, so that would be useless. So you would definitely want to drop more than 4. I think that's the answer to your question. Obviously it would have been easier if I had Visual Studio code running before I started. We could have jumped straight there, but I was not organized. So it's been really handy this week having this new trick up our sleeve, because previously trying to get these notebooks ready for class, I had to fiddle around a lot to find a learning rate that allowed the classifier to train without crashing off to infinity or staying at 0, and now I find a wider range of numbers of epochs and learning rates and so forth continue to give good results, so that's been nice. Back when we did the ULM fit paper, we were kind of still fiddling around a lot, a little bit shooting in the dark to try to find parameters that worked well. But my guess is that what's going on here is that when you train a language model with less regularization, it's going to overfit more. It may not overfit to the point that the accuracy is getting worse, but it might overfit to the point where the training loss is a lot smaller than the validation loss. Normally we don't care too much about that as long as the validation accuracy is good. But generally speaking, what we expect to find is that when the training loss is much smaller than the validation loss, it's probably a part of the function of the weight space that's quite pointy. It's kind of found a spot where on the training set it's got a good loss, on the validation set it's kind of a worse loss. Whereas if the two are both good, it's probably a flatter area. So that's my guess, is that then what happens when we try to fine-tune it for classification, if we've trained it too much with too little regularization, it's this spiky bit of the weight space where it can easily jump out to somewhere that's not very good. That's kind of my intuitive guess about what's going on. So then the classifier is trained actually in exactly the same way in this notebook, with the one exception that I've added, the 2FP16, just to make it a bit faster. And you can see what happens here is we, rather than getting 94.1%, we get 94.8%. So we've had a pretty big jump in accuracy. So this is trick number one. And then we're going to come back to IMDB in a moment. The next thing I want to do is take a little diversion into other languages. Because part of being able to be an effective NLP practitioner is being able to do NLP in languages other than English. Because lots of people speak languages other than English, including people in this room. And there's a few little things you kind of have to be aware of when you deal with other languages. So let's talk about those things. The first and most obvious one is that the language you're working with may not have a pre-trained Wikipedia model available from Fast.ai that you can download. We're actually planning to provide many, many more language models pretty soon. Right now the only one we have available is English, which is pretty sloppy of us. So I wanted to show you how you can create your own pre-trained model for other languages from scratch. And we're going to use Vietnamese. And we're not actually doing this at random. I went to the Wikipedia. There's actually a list of Wikipedia's page. And the list of Wikipedia's page on Wikipedia has lots of statistics, as you can see. And one of the things they do is they track both the number of articles and they also track the depth. So initially I was thinking, let's try and find a Wikipedia with lots of articles but isn't English. And so the next one is the Cebuano Wikipedia with 5.3 million articles. And I thought, that's weird. I've never even heard of Cebuano. Why is that? Surely that's not the next most important language to cover. I looked it up and it turned out that if I remember correctly, somebody created a tool that would translate using Google Translate, a Wikipedia from one language to another. And the guy who did it, his girlfriend, was Cebuano. And so he translated the whole Wikipedia. And so this is actually not a very good Wikipedia. So one of the things that the Wikipedia admins did was they developed this statistic called depth, which is kind of looking at how many edits actually happened and how many users are there. So you can see the depth of Cebuano is only 2. So what I did was I pulled this table out and put it into Excel and I multiplied depth by articles together and I actually found on that statistic the obvious choice for the next one to look at was Vietnamese, which for some reason in this table is now showing dash dash. I don't know why. When I looked at it recently, this was a super high value. You can kind of see it's got a million articles and 53 million edits and it's not a language which I had found, like there was no other pre-trained models available in the world that I could find, so I thought that would be a good thing to do. So I did try to do Vietnamese. I don't speak any Vietnamese at all, but I found from that table that the language code for Vietnamese Wikipedia is vi, so I just created a variable called vi and then I started just creating some variables for my new Wikipedia model. So the name of it I decided would be viwiki and I was going to try and create a directory called viwiki in my datapath which is where I'm going to put things and so then I needed to download the Wikipedia data. Initially I tried using wget, but the more I started preparing this lesson, I started doing more different Wikipedias and I thought it would be nice to do more of this in Python. There were quite a few steps I was doing and I found myself doing lots of different shell commands and then forgetting what order I was doing them in and so what I then decided to do was I created a little Python file called nlputils which you'll find in the course nlp folder. Here it is. And so I just created a little thing called getwiki. And so you just pass it a path to save the Wikipedia and the language. It just checks to see whether that already exists. If it does, it says looks like you're fine. I don't need to do it. Otherwise it just goes ahead and calls fastai's download URL function. This is the standard place that you'll find the saved XML of a complete Wikipedia for a given language. And it's saved in the zip file and then it unzips it. And so that ends up with an XML file containing all of the article data and lots of metadata. It's not great for a language model because we don't want a language model that predicts the next token in an XML file. So we really want to pull it out into a plain text file that just contains the titles and the text. It turns out there's a tool called WikiExtractor that already does this. So I just go ahead and I say get clone WikiExtractor and then I call WikiExtractor. And then I looked at, I Googled for Wikipedia article length histogram and I found some research that showed that Wikipedia article lengths are extremely bimodal. There's lots of articles that are about a thousand characters long and lots of articles that are about three to four thousand characters long. The thousand character articles, it turns out, generally, what do they call them? They're those pages which are not stub pages. They're basically things that say like, oh this Wikipedia page is a stub and it's just like a couple of sentences. So they're not really very useful or interesting for a language model because they're largely kind of just boilerplate. So I added a minimum text length of 1800 which I saw was that kind of point between the two modes. And so this makes sure that we end up with only proper interesting articles. So I mention these steps because they're like the kinds of things that you should think about when you start solving a new problem like this. The information you need is generally out there and it's normally just a Google away. And it's kind of important to get these little details right, otherwise you'll end up with a language model that's good at predicting the next word of Wikipedia boilerplate rather than the next word of actual articles. So that spits out a text file which always seemed to be called text AAWiki00, so I just moved that to the actual file name that we wanted. And that's all GetWiki does. So now that that's there, if you're just, so when you're working with notebooks, if you're going to use the same function in lots of different notebooks, it's not a bad idea just to chuck it in a text file like this. And once you've got a text file like that in the same directory as your notebook, you can then import it. So that's how you can get the best of both worlds of the notebook interactive experience and the kind of the module reproducible code editing experience. So once I call GetWiki, that goes ahead and does that. You'll see in this case actually this is ZHWiki00, I'll describe this next, I also did Chinese last time I ran that one, it was Chinese. This was what the start of the Vietnamese Wikipedia looks like. Remember in Jupyter, you can pop an exclamation mark at the start of a line and then just write bash shell commands. So a really easy way to look at the first four lines of the file is with head minus n4. Something you may not know is that when you do these shell commands in Jupyter, is anything you put in curly braces is an actual Python variable that gets replaced there. So you can see here I'm using the bash head minus n4 to show the Python path slash name. And so here's the first little bit of my downloaded file. Generally speaking, single giant files like this one, this is kind of like a couple of hundred gigabytes, a bit awkward to work with. So what I wanted to do was to split this file into multiple text files, one for each Wikipedia article. And I saw that they always start with this pattern. And the title is always here. So what I did was I wrote a tiny little function called splitWiki. Originally I wrote it in the notebook, and then once I started using it in other notebooks, I copied it into my little NLP utils. So you can see it here. And hopefully you might have guessed that the way I do this is with a regular expression. So I just copied and pasted that doc id equals line, and I replaced the number with backslash d+, so that means one or more digits. I replaced vi with curly brackets lang, so that's going to be replaced with my language variable which is vi. And I replaced current id equals 13 with current id equals backslash d+, so one or more digits. And then title equals double quote. And do you remember this little pattern from last week? Something followed by anything that's not that thing, plus. So a double quote followed by one or more non-double quotes followed by a double quote. So that was that little pattern we learned last week to find something like this, double quote, something that's not double quotes, and then a double quote. And then finally, the contents of the double quotes I put in parentheses to capture it. And so then in my splitWiki, you can use the findAll method in Python. And that will return all of the things in parentheses. And we only expect to find one, so I just grab it. I normally find when I try these things, there's always little things that pop up, little errors that pop up. In this particular case, some Wikipedia titles had a forward slash in, and so when I tried to save a file with a forward slash in, I got an error because forward slash is a directory separator. So I had this thing to replace forward slash with underscore. There was also one article length that was, the title was so long that it gave an error, so I just added another thing to skip over the really long ones. But that's basically it. So I just go through each line of my big file, and when I find a docId equals line, I grab the title and I create a new file with that title and then I just keep writing lines to it until the next time I find a docId equals. You'll see here that the regular expression I used re.compile first, that's never required. You can always just put the string in a re.findAll. But if you say re.compile, Python will spend some time turning that string into a faster kernel representation, and since we're going to be going over millions of lines, it's going to be a bit faster just to re.compile once. So that's a useful trick. So after I do my split wiki, I now end up with a directory, I just called it docs, which contains millions of files, one for each Vietnamese Wikipedia article. So from that, to create your Vietnamese pre-trained Wikipedia model is almost identical to creating your fine-tuned IMDB language model. The data blocks bit is exactly the same, and then you can save it. The language model learner is exactly the same, with one difference. Pre-trained equals false. So it won't try and download the English Wikipedia model and fine-tune it, because we're not doing English. So this way it's going to start with random weights. Since it's starting with random weights, there's no point starting with a frozen model, so we go unfreeze straight away. I think actually Fast.ai does this for you automatically, but I just wanted to make it super clear. And then I just do my 10 epochs with fit1 cycle, and you can see I end up with something that's 42% accurate. So it's trained a Vietnamese language model that 42% of the time correctly predicts what the next word's going to be. And so that gives you a sense of how predictable human language is. It's pretty normal for most of our language models from Wikipedia to have something around about a 40% accuracy. So then all we need to do now is to save the two parts of the language model that we need. The first thing we need to save is the actual language model itself. Now in this case, I trained the language model with FP16, but most people are probably going to want to actually get a FP32, a single precision language model, so I just convert it back to FP32 and then I save it. And you can just decide to save it wherever you like. What I did was I created two file names, so an array with two file names, one for the language model, so WT for WikiText, and then a second file name. The other thing we need to save is the vocab. If you think about it, the language model starts with a bunch of word embeddings, right? And so the word embeddings, each row represents a word. So row 1 needs to represent whatever row 1 in the vocab is. Row 2 is word 2 in the vocab. So there's no point sharing a pre-trained language model if you don't also share the vocab. So remember the vocab is literally just the list of unique words that we're training on. But they've got to be in exactly the same order, otherwise it won't know what's word 1, what's word 2, what's word 3. So we save the FP32 version of the model, and then we save the vocab. And so we've now got two files. And so what happens when you create a language model usually in FastAI, if you say pre-trained equals true, which is default, is if you haven't previously downloaded them, it will download from FastAI the pre-trained Wikipedia model and the vocab that it was used. So that's why we save both. So now we're going to go ahead and do exactly the same thing as we did for IMDB. So my next problem was I wanted to find a baseline. So I wanted to know like, well, I needed two things. I needed some sentiment analysis dataset in Vietnamese, and I needed to find an example of somebody who would try to use that sentiment analysis dataset to predict sentiment. This is often quite difficult in a language you don't know, because all of the information is generally in that other language. Anyway, so eventually I discovered that there was actually a super popular Vietnamese NLP competition. And here it is, called AIVIVN. So the nice thing is that we're not stuck, even if we don't know Vietnamese, because we have Google Translate. And so I was able to find out. Here it is. And I was able to also find out what do these scores actually represent. Okay, it's an F1 score, and even I found a forum. So this is great. So I was able to follow along and download the data for the competition and find out what the winner of the competition had done, and they even have their first-placed solution and so forth. So the first-placed getters had this pretty complicated solution involving an ensemble of four different models, four different deep learning models. So this is where you can grab the data from, and you can see that they have a test CSV and a trained CSV, like so. And so each one basically contains a unique number, then the review, and then 0 or 1, depending on whether it's positive or negative. This is quite different to the IMDB dataset, because the IMDB dataset, the reviews were generally about 1500 to 2000 words. These ones are much shorter. So recurrent neural networks are particularly effective for longer texts. Shorter texts are often not so difficult to handle, and RNNs do well, but they're not necessarily going to be as exceptional. So as you can see, we can just go pandas.read.csv to read that CSV file. And so we did that for the training file, and we did that for the test file. For the language model, I can use the test file, even though the test file has no labels. That's the point of this competition, is you're meant to predict the labels and submit them to the competition. So all I did was I just concatenated them together, and what pandas does is it'll just set the labels to missing for the test set. So now I had a data frame, and so FastAI has a text list from data frame, so we can now basically use exactly the same approach as the IMDB to create a language model data bunch, except instead of from path, it's from data frame. So then we just create a language model learner, just like before, but now we don't just pass pre-trained equals true, but we say pre-trained filenames equals, and we pass in an array of the model file name and the vocab file name. So that's those two files in this array we had here. So now that's how we can use our pre-trained Vietnamese model to create a fine-tuned language model for Vietnamese sentiment analysis. And then we just go ahead and fit this code's identical to IMDB. So you can see we're getting 37% accuracy for Vietnamese movie reviews. We save it, and we can save the encoder, and again, just like before, we can then create the classifier data bunch. Again we're going to use from df. And then we can create our learner. Now in this case, the competition was judged not on accuracy, but on F1. Have you done F1 yet? Yep, great. So F1 is the average of precision and recall. There isn't a binary F1 built into Fast.ai as far as I know, but there is one built into sklearn, so we can import it from sklearn. sklearn assumes that it's getting NumPy arrays, not torched tensors. So you can create a function, I created one called F1, which simply calls the sklearn version, and if you add the np func decorator from Fast.ai, it converts this to work with tensors instead of arrays. So this is a nice little trick to use any sklearn metric as a PyTorch metric. So now we can just pass in metrics equals accuracy, F1. Other than that, this is identical to IMDB, and we can now go ahead and train in the same way that we did before, and we end up with somewhere around 89, 90-ish. And so the competition's top three was 90, 89, 89, so we're like maybe slightly worse, tiny bit worse than, let's see, where was it, here it is, let's get the exact number, depending on whether it's the public or the private, 90, 0 or 901, and we're somewhere around 901 or 89, so yeah, we're maybe in first place, but it's not quite clear. So we want to try and do better because we don't do these things to come second. So let's do the next trick. So remember I told you that the winners of this competition created an ensemble of four different deep learning models, so it's going to be difficult for us to beat that with just one. We seem to be close, but we want to be sure. So there's a trick that you can improve any model by also creating what's called a backwards model. A backwards language model, for example, is something which you don't feed it a Wikipedia article, you feed it a Wikipedia article in reverse. So if you just pass backwards equals true to your data bunch, then what it will do is it will create a data bunch where every Wikipedia article is backwards. And so a language model trained on that will learn to predict the previous word of a sentence given the last words, which is kind of like a weird thing to do, but if you think about it, in language, the next words very often tell you context about what the previous word might have been or how to interpret it. So predicting the previous word of a sentence is likely to be just as useful as predicting the next word of a sentence. So what I did was I literally took the NNVietnamese notebook and I said file, make a copy, and then I renamed it, file, rename to vietnamese BWD for backward. I then added backwards equals true and then I just ran it again. And interestingly, vietnamese is slightly easier to predict backwards than forwards, so maybe vietnamese speakers should consider saying everything backwards from now on. So now I've got my backwards model. So when I saved it, I made sure that all my file names had underscore backward at the end. So I now have a pre-trained Wikipedia backwards model. So now for my fine-tuned language model, I did the same thing, I grabbed my data frames, but again I just added backwards equals true. So now I've got a vietnamese review fine-tuned backwards language model, and so I save that. And then my classifier, again, backwards equals true. So I now have something, so then I load my encoder, and so I now have something which can learn, predict with an accuracy of 89%, whether or not a review is positive or negative based on that review in reversed order. So I've now got two models, one for forwards, one for backwards, and so what we can do is we can simply ensemble them together. So an ensemble generally means you combine the predictions of two models, often by just taking the average. So you can see here I've just loaded up my forward classifier, and then here I've loaded up my backwards classifier, and I've got the predictions of the forward classifier, which as you can see the F1 is.895, and the predictions of my backwards classifier, which is 895, and then I take the average of the two, and now I get F1 of.9016, which definitely gets us into first place. So you can see here in the span of what was basically a day's work, which included downloading the Vietnamese Wikipedia and finding a Vietnamese sentiment analysis dataset, we've got something which would have won a pretty popular competition with 150 entrants for Vietnamese sentiment classification. And so this trick of ensembling the forwards and backwards can often give you a significant lift, particularly in situations where you don't have a lot of data. This approach of creating your own pre-trained model, it's not useful only for different languages, but it can also be useful if you're dealing with an extremely different domain. So like one particularly popular one would be medicine, if you've got a huge vocabulary of medical terms, and maybe it's in more detail than might have appeared in Wikipedia pages, so it just doesn't appear in the Wikipedia vocabulary, you could create a pre-trained medical model. Having said that, the big takeaway from our ULM-FIT research really was, it turns out you don't need to do that very often. It turns out that pre-training a model on Wikipedia, as long as it's in the correct language, is nearly always good enough, it's not very often that you would need to start from scratch. And even if you did want a specific medical model, you're probably better off transfer learning from Wikipedia model to the medical model, rather than starting from random. Okay, so we've nearly finished this, let's take a break, come back at 12.10, and then we'll finish up our other language models. So I mentioned when I looked at the Vietnamese one, that I also looked at Chinese. And so the Chinese Wikipedia is ZH, and so if you want to try this in Chinese rather than Vietnamese, you would just replace lang equals vi with lang equals ZH, and everything else is the same. But I wanted to talk about this as another interesting example of little complexities you have to deal with in non-English languages. And Chinese is a particularly important one to know about, because it's the language that's spoken by the other great economic superpower, and is also one of the most widely spoken languages in the world. So super commercially important language to know how to deal with, and it has a particularly interesting and important tweak you have to deal with, which is there are multiple ways to write the same sentence in Chinese. And the names of these two ways vary depending on who you ask on the mainland. They're known as simplified characters or traditional characters, fan ti tse or jian ti tse. And they are basically the traditional characters, was the characters used before 1954 or something, and there was a lot of controversy in Chinese literature that a lot of the most famous Chinese authors were saying our language is too complicated, and it's a fascinating history. The Chinese language, as was the case in many languages throughout history, was designed to be difficult so the commoners couldn't understand it, which means that the people that did understand it maintained higher status positions. And so it actually got kind of crazy in the 19th century in China, there were as many as 60,000 characters in use. By the mid-20th century things weren't quite as crazy, but there was still a view, particularly following the communist revolution in mainland China, that things need to be made simpler. And so they actually did something which is amazing, which is that they gathered lots of linguists together and they said let's design a new character set. And let's find all of the characters that people find difficult to read and write and replace them with new ones. But the interesting thing is they didn't change the language. So the same sentence will be written in two different ways. Some of the characters will be the same, some of them will be different, some of them half will be the same and half will be different. And so this is one of these interesting things that you have to know about if you're going to do NLP in practice. So it's kind of fun, you start to learn about other cultures and other kind of linguistic histories and so forth. So in Chinese, there is not an exact mapping from simplified to traditional. And I use those terminology because I've spent time in the mainland, not in Taiwan and Hong Kong where they use different terminology. Because actually a traditional character, there's actually multiple traditional characters can map to the same simplified character. But you can, there is a largely unique mapping from traditional to simplified, not from simplified to traditional. So what the Chinese Wikipedia editors decided to do, for a while they had two totally separate encyclopedias, they merged them and then they decided basically, okay, let's write it largely in traditional characters and create some scripts that will convert them. So when you download the Chinese Wikipedia dump, you'll find that they're in traditional characters. It's not quite as simple as that. There's also some slight variations in vocabulary that tends to be used in different parts of China, but that's the main thing. So I'm not smart enough to understand traditional characters, I only understood simplified, so I for my own sanity wanted to create the simplified version. So generally when you want to do some kind of language specific thing like this, you just have to go and do some Googling. And so it turns out that there's actually a program called OpenCC which will take a Chinese language document that's in traditional and convert it to simplified or vice versa automatically. So you basically just say OpenCC minus i for input and your input file minus o for output and your output file. So I created a ZHS, as in ZH simplified version. I found that took a really, really, really, really long time and I'm not very patient. So something I wanted to tell you about in case you didn't know is that there is a program called Parallel, more fully known as GNU Parallel. If you're on Ubuntu, it's just apt-install parallel. And what that will do is it will run any program over multiple processors. And pretty much everybody has more than one processor nowadays. And so here is how you can convert Chinese traditional characters to Chinese simplified characters automatically and quickly. So I just said, OK, look for all of the text files and type that to GNU Parallel. And then all you do is you tell GNU Parallel what's the command you want to run in parallel. And what it's going to do is it's going to replace every percent sign with the next thing that comes out of here, so the next file name. Why percent sign? Because you basically say to Parallel minus capital I blah, where blah is the thing that you're going to use to represent the next file name that's being fed in. So this is a nice little trick that I use all the time to run things much more quickly, particularly if you're using like AWS, if you're using something like an AWS P2, it's got lots of CPUs, so you may as well take advantage of them. Anyway, so this is just an example of the kinds of things you need to be aware of when you're dealing with non-English languages. And one of the best things to do is if you're dealing with a non-English language is to partner up with a native speaker of that non-English language so they can help you deal with all the little tricky things. Okay. I was just going to highlight this is an example or an illustration of how important domain expertise is, which is true in kind of any application of deep learning. Yeah, yeah. It is important, or at least domain respect, you know, like take the time to understand the domain yourself as best as you can and ideally work with a domain expert. So Chinese also has another interesting feature, but to keep things interesting, I'm going to tell you about this interesting feature by talking about another language, which is Turkish. And Turkish is interesting, are there any native Turkish speakers here? There is one. Okay. Can you pass the catch box over? We need your help, please. Okay. Here is one Turkish word in various forms. Are you able to read this word for us? So these are, my understanding is these are all different, more phemic extensions. How far would you go down in practice? Right there, right there. About here? Two lines down. Yeah. That won't be the longest. Can you pronounce that for us? Yeah, that's what I was going to say. So Turkish is what's called an agglutinative language. And agglutinative languages are languages which use a lot of morphemes to construct interesting variants. So in this case, what was your name? So Omar has apparently never needed to say, when it seems like that is one of those that we can make fearless, but if he did, he could use a single word to describe that situation. Now the problem is that normal tokenization that we do with spacey would treat this as one word, right, and as one concept, and so because it largely uses space as a delimiter, with some special cases like apostrophe s, for instance. And so in that case, we would end up with a vocabulary of millions of words, right, because this isn't really a separate word. It's this word with various morphemes appended to it, right? So these kinds of languages need special care. Now it's actually exactly the same care that Chinese needs, because Chinese looks like this, which is there are no spaces at all. So we can't rely when dealing with languages like Turkish or Chinese on using a space to tokenize our corpus. And this is a big problem. This is really tricky. And it's not as easy as you might imagine. People often say, oh well, let's just find the start and end of each word and put a space there. But Chinese, for example, is such a grammatically interesting language that it's not even clear sometimes where words start and end. Often, for example, in Chinese, you will insert a character in the middle of a word to give it some different meaning, right? Or you'll insert a character at the end of the word, something called a resultative complement, which means you create a new word which tells you about the outcome of some other word. So it's not a simple case of just saying, let's just find the words. So in recent times, a really, really cool solution was developed, which is called SentencePiece. And it's actually based on something that goes back even further called byte pair encoding. And what SentencePiece and byte pair encoding do is that they segment things into what are called subword units. And the subword unit is a sequence of letters that appears frequently in a corpus, so frequently that you tokenize it into those subsequences. So for example, in Turkish, after we've used subword encoding, we end up with something that looks like this. So this big underscore represents spaces, and spaces represent token boundaries. And you can see most of the time, we have each token is a single word, but you can see that sometimes you get things like this, where it's been turned into two tokens. We've actually split the word in the middle. Here's another example, where we've actually split the word in the middle. Just to clarify, the dark underscores, those were spaces in the original text, and the other spaces are ones that have been added by how you've tokenized this. Exactly. Thanks, Rachel. So the question is, how would you start with something like this, or something like this, and turn it into something like this? And the answer is, at a high level, what you basically do is you start out by looking through your corpus, and you find, I'll tell you the example for byte-pair encodings, it's the easiest to describe, you find which two characters appear next to each other the most frequently. And you take that pair, and you pull it out, and you say, oh, that's a token. So for example, if you're doing English, you might find T and H occur next to each other a lot, and you say, okay, TH is now a token. And then you repeat it, and you look for again, two characters that often appear next to each other, but now you can treat TH as one character. So down the track, you'll find TH and E often appear next to each other. That's now a token. And so you keep doing that until eventually you have some set number of unique tokens. That's called your vocab. So with SentencePiece, when you call SentencePiece, you can actually tell it how big a vocab do you want. And so we default to a vocab of size 30,000. So this here has been tokenized into Turkish subword units using a size of 30,000. SentencePiece actually goes a bit further than BPE, Bipair Encoding. It actually creates a neural network language model at a character level and finds combinations of characters that are most likely to appear together based on the language model. But it's the same basic idea. So this idea of using subword units, it's not very well studied, but it's absolutely necessary and super powerful, not only for Turkish, Chinese, Japanese, Polish, but also for things like medical texts. Because actually those big long chemical names in the scientific and medical literature contain well-defined subword units that are frequently reused. So you don't need a separate vocab for every one. And the problem with having a separate vocab for every one is none of those single words appears that often. So you really want to get a sense of the meaning of the underlying subword units. So if we want to do Turkish from scratch, it's basically the same as we've seen before. We can use my SplitWiki and GetWiki. We can grab something about Genghis Khan, SplitWiki, and then from folder as before. But we've got one extra step, which is we've added a processor line. So by default, FastAI uses the spacey tokenizer to tokenize the text. If you want to use something else, like in our case we want to use sentence piece, you can replace the processor with sp-processor, which stands for sentence piece processor. So if you do this, then this says don't tokenize on space boundaries, but instead learn a model from the text, please. And so now when you go show batch, you can see there is the sentence piece tokenized model. And from there, everything is exactly the same as before. Language model learner, and you fit it. 38% accuracy at predicting the next subword in Turkish Wikipedia, and then you can save it. And so now we can do sentiment analysis in Turkish. So after a lot of digging around, I eventually found a text classification dataset for Turkish, which you can find here. And so again, people love their movie reviews, apparently. This is a movie review dataset in Turkish. And in this case, there was one file called trpolarity.pos, which was a file containing all of the positive reviews, and there was a trpolarity.neg containing all the negative reviews. So all I did was I opened up that file, and I read the lines, and I got an error. And the error I got was that it was unable to read the file using UTF-8. And so here's the next thing that you're going to come across all the time as an NLP practitioner, which is the way that letters are stored on disk until recently was ill-defined. We used a character set called ASCII that had a unique mapping from the numbers 0 to 27 to letters of the alphabet and punctuation and so forth. The A in ASCII stands for American. So it's just designed for Americans. So when folks in France decided that they wanted to put things on computers as well, they changed some of the numbers to represent different characters. So if you open exactly the same file on a French computer versus an American computer, you'll see something different. And this got particularly crazy in Japan, for example, because Japan has 4,000 characters, which obviously you can't fit in 127. So they invented their own encoding using more than 127 numbers. So this all got kind of saved a few years ago by something called Unicode, which is nowadays maintained by something called the Unicode Consortium. And Unicode consists of millions of characters, including emojis, and they keep on adding new emojis every year. For example, this year they added emoji variants that provide different skin tones for the various people emojis. The problem is that the dataset that I was looking at, and this happens quite frequently, it was from 2013, where Unicode was around, but not everybody was using it, it was not saved in Unicode. And so there's actually no correct way to open a file that's saved not in Unicode. You have to guess. So I googled for Turkish language encoding, and I found a page that said, quite often in Turkey people tend to use ISO 8859-9 encoding except when they don't. So open it and then look for things that look like this, and if they look like that, then it's not that encoding, but it is that encoding. So I tried it, and they looked like the thing they were meant to look like, so that's why I had to add this. So again, little things you have to deal with. So I now know about the five letters that are represented differently in Icelandic versus Turkish, which is how you can tell the difference between 8859-9 and 8859-5, which is something I don't know I'll ever use again. But there you go. So thank heavens for the internet telling us these things. So that's what the encoding is. So I turned that list of lines into a data frame, and so I just call that the text column, and then I've got to add a label called positive as in positive or negative, and for my positive ones, I'll set it to 1, and then I'll do the same thing for the negative data set, and so the positive column I'll set to 0, and then I will concatenate the two together, and then the rest of it is exactly the same as we had before, except rather than going spprocessor parentheses to create spprocessor, I need to make sure I use the same sentence piece vocabulary and model as we used for the Turkish Wikipedia model. Otherwise it's going to tokenize it in a totally different way, and it won't make any sense. So if you say spprocessor.load and pass in the path to your data bunch from your pre-trained Wikipedia file, then that will load up the sentence piece vocab. I never saved the sentence piece vocab, it's actually saved automatically. So this way I now can say show batch, and you'll find that it's tokenized in the same way as before, and sometimes you can see that there are words which in this case is turned into 1, 2, 3, 4, 5 tokens for a single word, for instance. Is that a very rare word or a name? It's actually two words. There's no space there for some reason. So then we can create our language model learner, and like before we have to say pre-trained Fnames to grab our pre-trained Turkish Wikipedia, and then we train it in the usual way, and so we now have something that's 36% accurate at predicting the next subword unit of a Turkish movie review. And so now we can do the same thing to create our classifier data bunch and create a classifier, load in that encoder that we just saved, and train it for a while, and we get to 89% accuracy. So I Googled around and I found a 2018 paper where somebody used this dataset to do sentiment analysis and they got 75% accuracy. So this is like what you often find on, if you move outside the world of Chinese and English AI, because Chinese and English are the main languages that people are writing AI papers in, you'll often find that stuff is pretty low-hanging fruit. So in this case we absolutely thrashed the best sentiment analysis benchmark I could find from 75% to 89% just by applying the same techniques we've seen. So you can see that, as kind of Rachel's mentioned, where we're at with NLP right now is there's been dramatic change in the last couple of years. Almost nobody's familiar with it. Everything's low-hanging fruit. Pretty much everything you try and apply it to, you'll probably get much better results than anybody's ever got before. There's opportunities to create products that didn't exist before. Any product that currently uses language, you'll probably be able to make much better. And it's really all about taking advantage of transfer learning in this way. So then finally, if you look in the FastAI repo, there's an examples directory where you'll find ULMfit, which will show you the end-to-end steps for IMDb, basically what we've already seen, plus the backwards model, and at the end the ensemble. And so if you do this for English, you'll eventually get 95.4% accuracy, which as of today is to the best of my knowledge, the world's best result has ever been got on this dataset without using data augmentation. So now you know not just how to do this, but how to do it as well as anybody knows how, as far as I know. So now, Rachel, you're going to tell people a bit more about how it works. Actually, before I do that, I just wanted to ask a few questions to review what Jeremy just covered. And the first is, how are we able to use semi-supervised learning in the ULMfit example? Jeremy covered earlier. How are we able to use semi-supervised learning in the ULMfit example? Can I give you the catch box? I think it's by using a language model in the unlabeled data and then using the classifier to fine-tune on the label data. Correct, yeah, so the secret was that we were able to use unlabeled data for the language model. So it's a language model, it's just predicting what word comes next. As long as you have text, you can do that even though you don't know the sentiment of the text. Jeremy will grab it. Thank you. Good answer. Another question is, what was the secret for dealing with languages where you ended up with very long words like Turkish, where it had the different morphemes that might have meaning and show up across words, such as here, quark or quarku relating to fear, but showing up in a lot of words. Jeremy, can you pass the... By coming up with subwords, subword units, learning them. Exactly. Using subword units and so in this case, Jeremy used sentence piece. I guess one more question to review these. So we have, we ensemble a backwards model and a forwards model for our classifier. Would you be able to do that for your language model? Use an ensemble of a backwards and forwards model? You guys want to vote? Anyone who thinks yes, raise your hand. Anyone who thinks no? Okay. So it was mostly yeses. It's actually a no on this and that's because with the language model, because you're predicting what's going next, if you had both forward and backwards, that would kind of be cheating. Jeremy? Oh. Maybe we should call it a maybe. Maybe. Because there's been some people who have used tricks using masking to do something like that where they kind of predict the next word using everything except the ones that would be cheating. As I said, I mentioned this one other thing we didn't mention here and I don't know if it's actually in the literature yet, but as well as ensembling forward and backward, you could also do sentence piece tokenization for English and ensemble forward and backward spacey tokenized and forward and backward sentence piece tokenized and you would get an even better result. So that would probably give you something better than the current state of the art if you tried that. I like that idea. Yeah, so what I wanted to highlight, that's an important point about using masking, that if you were going to use a forward and backward model, you need to be very careful that you're not peeking ahead with the information since you want to make sure you're actually predicting the thing you're predicting and not using other information you have. Great. Are there any other questions on what Jeremy covered with ULMfit? Okay. So we'll move into the next notebook we're going to cover is notebook six, RNN English numbers. So just to say with ULMfit, we were using RNNs, but that was kind of hidden from you. So that was part of the AWD LSTM. Now we're going to really dig into kind of what was going on with the RNNs, what is an RNN. And to do this, we want to use a simpler data set than what we've been doing. And so we're going to use a list of the English word version of numbers as shown here. This is counting. So 8001, 8002, 8003, 8004, 8005, 8006. And this is a synthetic data set that Jeremy created, and Jeremy is a big fan of synthetic data sets as a technique when you're testing out different ideas or kind of wanting to explore something. It can be nice to have a simpler data set that you understand well because then you can kind of understand the behaviors you're testing something. In the deep learning courses, Jeremy created ImageNet and ImageWolf as simpler data sets to test ideas on. Did you want to say more about that technique, Jeremy? Sure. I guess these are two versions of a similar technique, so the synthetic data set technique. When we were developing the Fast.ai library, my experience is the first 12 times I write something, it's always wrong. It's very hard to know something's wrong, particularly because with machine learning it can be wrong in subtle ways. So for something which I clearly know what the next answer ought to be, it was just easier for me to see how it was wrong. Also it's just much easier to develop an algorithm using something that can train in 5-10 seconds rather than 5-10 hours. ImageNet and ImageWolf is a slightly different variant, which is it's not a synthetic data set, but it's a much smaller data set. In computer vision, people who had been struggling for a long time, they either trained things on the ImageNet data set, which used to take days to train, or they trained on something called CyPhar 10, which was so small it turned out to be useless. They were 32x32 pixels, and it turns out that things that work well on 32x32 pixels actually don't work well on normal-sized images. So I created something with full-sized images, but less of them, and I tried to create one version that would be easy to classify and one version that would be hard to classify. So I guess in general it's like trying to come up with different sampling versions of the problem that you're trying to solve is one of the really important things as a machine learning practitioner, so that you can quickly iterate and quickly identify your mistakes, even if you don't make as many mistakes as I do. Great, thank you. Yes, that gets at this recurring theme we've seen of exactly quick iteration, and so even that's the same idea of why you try stuff out on a sample of your data set. This is maybe two more sophisticated ways of doing samples of data sets, or getting at the same idea of being able to try things out quickly. One thing I wanted to cover before we go on is just that deep learning neural networks are really just made up of two different types of numbers. Those are parameters, which are the numbers that you're learning through back propagation and these variants on stochastic gradient descent, and then activations are the numbers that are calculated. And so you're really just combining these things to create a neural network, and as we go, I'll identify when you learn a new concept, it can be helpful to think, is this a parameter or an activation? But there's not any other magical component of what makes neural networks. You're just stacking up these linear transformations with non-linearities, and you're learning one set of parameters and calculating a set of activations as you go through a network. So here we're going to read in this set of English numbers, and again, this is a problem that you are not going to come across in the real world, but it's going to be very informative for us as we explore what an RNN is and how it works. And in this lesson, we'll be building up an RNN from the basics. BPTT stands for back propagation through time, and that tells us how many steps of history we're considering. And so here, we're going to start with 70, and so the idea is looking at this, it would be if you had 70 tokens, 8001, 8002, so that's, I guess, eight tokens right there. If you include the commas, we're going to have 70 of those, and then we want our network to predict what would come next, what would be the 71st thing. We'll divide our data set up, so we're going to have 70 tokens in a line of text and 64 lines of text per batch. Our batch size is 64. And so this will give us about three batches. And again, kind of the reason we're doing something on this smaller data set is so that it'll be easier to see what's happening. And here, we're going to explicitly name our batches, X1, which corresponds to the labels in Y1, X2 corresponds to Y2, X3 to Y3. And that's all our data. And typically, you're not taking your batches out and putting them in separate variables, but this, we really kind of want to see what happens. We can confirm. So there's a numL, is a PyTorch method to return the number of elements in a tensor. We can do that and check, OK, we're getting about 13,000 back. That's what we expected. You can see the size. A single batch is 64 by 70. So we have 64 different lines of text. Each line of text has 70 tokens in it. And then Y1. Oh, and so here, with Y1, basically, everything has been shifted over one. Let me see if I've shown this. Yeah, we'll take a look at this in a moment, kind of in terms of predicting what's next. I'm loading the vocab into V so we can take a look. This is into string, checking how we've mapped our tokens as, or how we're mapping our integers to the string tokens. We have the standard FastAI XX for kind of special tokens. And then here we have the word, we have a comma, 100,000. And then the numbers, 1, 2, 3, 4, 5, 6, 7, 8, 9, and so on up to 19. With, sorry, in this section, we've got our 20, 30, 40, 50, 60, 70, 80, 90. So these are the kind of the components we'll be using to make these list of numbers. We can see the, actually, it's probably easier to go back and forth. So here is X1. There's even a textify method built into the vocab that we can look. This is the 8001, 8002, 8003, 8004. And if we textify Y1, we'll see that it's just been shifted over. So we no longer have this XX beginning of string. But if we, you know, our task is to predict what comes next after beginning of string comes 8, after 8 comes 1,000, after 1,000 comes 1. You can see this kind of direct mapping. Jeremy? I don't know if we've talked much about these XXBOS things. I just sort of briefly mentioned. These special tokens that aren't words that signify a thing, like this is the start of a document, XXBOS, or the next word was capitalized, XXBEDGE, or this is the end of a document, XXEOS. They appear in lots of NLP things. One of the most common ones we'll see is UNK for an unknown character, which means this character didn't fit into our vocabulary. We looked at this in an earlier lesson. We saw with, I think this was with IMDB, that a lot of words could get mapped to XX unknown. So it's not a many-to-one mapping, that you can have many different words going to XX unknown. One of the nice things about sentence pieces is you don't generally get XXUNK, so you can represent long actors' names and stuff. I was just going to mention, most of the time outside FastAI, they tend to use some kind of punctuation, like UNK is often angle bracket, close angle bracket, and that tends to be really difficult when dealing with things like spaCy, because it'll tend to tokenize it as angle bracket space, space angle bracket, and that's not great for your current neural network, because when you have more tokens, it's longer that it has to remember the state for. So if you're wondering why does FastAI have this weird XX thing, it's because in every system it's going to tokenize sensibly, even with sentence pieces. Sentence pieces generally are going to say, oh, I see XXUNK very often, and so it will tend to automatically create a single sentence piece token for it. So when you're working with other systems that use angle brackets and stuff for their special tokens, you've got to be super careful of how they end up tokenizing. Thank you. Those are all good points. And again, as a reminder, tokenization is always implementation dependent. It's going to depend on the particular library, how things get tokenized. So continuing with this, we can see how we've tokenized, and also just to kind of be aware, X10 is giving you this line of our sequence that's 70 tokens long. There'll be 64 such lines in a batch. We can inspect. Oh, so something to note here is X20 follows X10. So see how we go 8,017, 8, and then when we skip ahead to X20, it's 8,018, 8,019, 8,020. So to note that the way these batches line up is that in going from one batch to the next, the first line of your first batch is going to be followed by the first line of the second batch. Let me just contrast that with if we were to look at the second line of the first batch. Oops, not run these. You might have expected that going from line one to line two of the same batch would have continuity, but that's not the case. The continuity is across batches. This is pretty quick to run. Okay. So now I've got V. We can take a look at here and see this is 8,046. So we've kind of jumped ahead for the second line of batch one, but we can expect that that'll line up with the second line of batch two because here X2 is the second batch and it does 8,059, 8,060, 8,061. So scrolling down, we can see we can use show batch to get another view of what the batch looks like. And so how we're going to approach this is we're going to iteratively consider a few different models kind of building up to the traditional RNN. We're going to start very, very simple. And let me switch to I have some slides that show this. Yes. And this was covered in lesson seven of the Fast AI Deep Learning course, although we're going to kind of be going through it much more slowly and in more detail. So there it's kind of covered as a quick example, but these slides are from there. And so use the same, well, they are the same, including the convention. And so here these slides, these diagrams have a lot of meaning in them. So it will be kind of important to keep track of. And so the first to note is the shape. Rectangle means input, circle means a hidden layer, and then a triangle means the output. So a very, very basic, kind of one of the most basic neural networks you can think of with a single hidden layer would be to take an input in, do a matrix product, and then a ReLU, measure non-linearity, and that would give you this hidden state. Then take another matrix product and a softmax, and you have your output. So this is kind of one of the simplest neural networks you can think of. And hidden state just refers to the activations. So here this corresponds to the hidden layer. It's the activations you get out from, you did this matrix product and ReLU, and the result sometimes people refer to as the hidden state, but those are just a set of activations in your hidden layer. And here are the numbers for the matrix product, like that matrix, those are parameters that you're training, and the result that you get from taking your input times this matrix, and then zeroing out all the negatives is a set of activations, aka your hidden state. So that's kind of a simplest neural network. So now we're going to move on to this idea of suppose we want to predict word three using words one and two. What we could do is feed word one in as input. I haven't written it here, but here when the arrow is happening that's a matrix multiplication and a ReLU, we get a set of activations, and then basically we want a way to combine those activations with our word two as input. And so we'll combine those two and get another set of activations, we can put that through a matrix multiplication and a ReLU or softmax to get our word three output. And so this is called the single fully connected model, so this is a very simplistic just how would you combine two words to get a third word. We'll see what that looks like in code. Go back to the notebook. So here I've put the same name so you can kind of see how the network sections correspond with the slides. And I can add these slides to the GitHub, I don't think I have them in there yet. So what we're going to do is define loss, we're interested in cross entropy loss and accuracy. Here the naming convention is I refers to input, H to hidden, so I underscore H is the input to hidden layer, H underscore H is the hidden to hidden layer, H underscore O hidden to output, and BN stands for batch norm. And so we're going to make this neural net that uses an embedding to go from an input to a hidden layer. We'll have just two linear layers and then we'll also have a batch norm. And so to take one forward step, we start with the input to hidden, which is an embedding, do a ReLU on that and a batch norm. And then if this is, we're just ensuring that this has a length of three, so if we have at least kind of our word one, this is the part where we want to input word two, we do that by H plus, now we're going to use input to hidden on word two. So here X zero you can think of as word one, this is word two, is now being added to what we got from that calculation with word one. We'll do our hidden to hidden and ReLU and batch norm. If we have a word three, then we can add that as well. So we're kind of just applying this transformation to each new word and adding that to kind of what we got from the previous one. Let me go back to the slide. Oh, and we're at one o'clock. So this is, that example was kind of showing predicting word four using words one, two, and three. And so, well, we'll come back to this next time. The key thing to note, if you look at these diagrams, is that the color of the arrow refers to kind of a single matrix of weights. And so here, word one is going to be multiplied by the same matrix of weights as word two will be later on, as word three will be later on. Yes, Jeremy? Back? Oh. I just realized on the previous slides, that's not true. The previous slides all use the same color, but they're not all the same weight matrix. So just wanted to clarify, they're only the same weight matrix on the slides where they actually have different colors. And I will update these previous slides to have the colors, although later ones, where it gets interesting, we do have this color coding. But we'll revisit this next time. This is kind of just a first pass. But yes, I'll see you on Thursday.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.44, "text": " I just briefly wanted to say, so the reason I was gone last week was I was speaking at", "tokens": [286, 445, 10515, 1415, 281, 584, 11, 370, 264, 1778, 286, 390, 2780, 1036, 1243, 390, 286, 390, 4124, 412], "temperature": 0.0, "avg_logprob": -0.2356602269360143, "compression_ratio": 1.6312056737588652, "no_speech_prob": 0.060879290103912354}, {"id": 1, "seek": 0, "start": 4.44, "end": 10.16, "text": " this international summer school on bias and discrimination in AI at Avado, which is the", "tokens": [341, 5058, 4266, 1395, 322, 12577, 293, 15973, 294, 7318, 412, 11667, 1573, 11, 597, 307, 264], "temperature": 0.0, "avg_logprob": -0.2356602269360143, "compression_ratio": 1.6312056737588652, "no_speech_prob": 0.060879290103912354}, {"id": 2, "seek": 0, "start": 10.16, "end": 14.6, "text": " AI institute, Yoshua Bengio started in Montreal.", "tokens": [7318, 26860, 11, 38949, 4398, 29425, 1004, 1409, 294, 34180, 13], "temperature": 0.0, "avg_logprob": -0.2356602269360143, "compression_ratio": 1.6312056737588652, "no_speech_prob": 0.060879290103912354}, {"id": 3, "seek": 0, "start": 14.6, "end": 17.240000000000002, "text": " And there was a really, really fantastic lineup of speakers.", "tokens": [400, 456, 390, 257, 534, 11, 534, 5456, 26461, 295, 9518, 13], "temperature": 0.0, "avg_logprob": -0.2356602269360143, "compression_ratio": 1.6312056737588652, "no_speech_prob": 0.060879290103912354}, {"id": 4, "seek": 0, "start": 17.240000000000002, "end": 18.84, "text": " I really enjoyed it.", "tokens": [286, 534, 4626, 309, 13], "temperature": 0.0, "avg_logprob": -0.2356602269360143, "compression_ratio": 1.6312056737588652, "no_speech_prob": 0.060879290103912354}, {"id": 5, "seek": 0, "start": 18.84, "end": 24.04, "text": " And the whole thing was recorded and will be released as a MOOC later this year.", "tokens": [400, 264, 1379, 551, 390, 8287, 293, 486, 312, 4736, 382, 257, 49197, 34, 1780, 341, 1064, 13], "temperature": 0.0, "avg_logprob": -0.2356602269360143, "compression_ratio": 1.6312056737588652, "no_speech_prob": 0.060879290103912354}, {"id": 6, "seek": 0, "start": 24.04, "end": 26.84, "text": " So I just wanted to let you know about that to be on the lookout for it.", "tokens": [407, 286, 445, 1415, 281, 718, 291, 458, 466, 300, 281, 312, 322, 264, 41025, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.2356602269360143, "compression_ratio": 1.6312056737588652, "no_speech_prob": 0.060879290103912354}, {"id": 7, "seek": 2684, "start": 26.84, "end": 32.64, "text": " Because it was a really neat blend of they had machine learning researchers from Google", "tokens": [1436, 309, 390, 257, 534, 10654, 10628, 295, 436, 632, 3479, 2539, 10309, 490, 3329], "temperature": 0.0, "avg_logprob": -0.21389842987060548, "compression_ratio": 1.6367041198501873, "no_speech_prob": 8.601957961218432e-05}, {"id": 8, "seek": 2684, "start": 32.64, "end": 38.68, "text": " and Microsoft and other industry places, as well as academics, as well as a human rights", "tokens": [293, 8116, 293, 661, 3518, 3190, 11, 382, 731, 382, 25695, 11, 382, 731, 382, 257, 1952, 4601], "temperature": 0.0, "avg_logprob": -0.21389842987060548, "compression_ratio": 1.6367041198501873, "no_speech_prob": 8.601957961218432e-05}, {"id": 9, "seek": 2684, "start": 38.68, "end": 45.32, "text": " lawyer and officials in the Canadian government talking about how Canada is dealing with AI.", "tokens": [11613, 293, 9798, 294, 264, 12641, 2463, 1417, 466, 577, 6309, 307, 6260, 365, 7318, 13], "temperature": 0.0, "avg_logprob": -0.21389842987060548, "compression_ratio": 1.6367041198501873, "no_speech_prob": 8.601957961218432e-05}, {"id": 10, "seek": 2684, "start": 45.32, "end": 48.36, "text": " So it was a really interesting interdisciplinary week.", "tokens": [407, 309, 390, 257, 534, 1880, 38280, 1243, 13], "temperature": 0.0, "avg_logprob": -0.21389842987060548, "compression_ratio": 1.6367041198501873, "no_speech_prob": 8.601957961218432e-05}, {"id": 11, "seek": 2684, "start": 48.36, "end": 49.84, "text": " So be on the lookout for that.", "tokens": [407, 312, 322, 264, 41025, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.21389842987060548, "compression_ratio": 1.6367041198501873, "no_speech_prob": 8.601957961218432e-05}, {"id": 12, "seek": 2684, "start": 49.84, "end": 56.04, "text": " Yeah, with that, I'll let Jeremy continue and then I'll be jumping back in later.", "tokens": [865, 11, 365, 300, 11, 286, 603, 718, 17809, 2354, 293, 550, 286, 603, 312, 11233, 646, 294, 1780, 13], "temperature": 0.0, "avg_logprob": -0.21389842987060548, "compression_ratio": 1.6367041198501873, "no_speech_prob": 8.601957961218432e-05}, {"id": 13, "seek": 5604, "start": 56.04, "end": 64.14, "text": " Hi, so I had some interesting questions after the last lesson and also done some interesting", "tokens": [2421, 11, 370, 286, 632, 512, 1880, 1651, 934, 264, 1036, 6898, 293, 611, 1096, 512, 1880], "temperature": 0.0, "avg_logprob": -0.13669633865356445, "compression_ratio": 1.66875, "no_speech_prob": 0.0002231664548162371}, {"id": 14, "seek": 5604, "start": 64.14, "end": 71.32, "text": " research since the last lesson, so I thought I'd share some extra stuff with you.", "tokens": [2132, 1670, 264, 1036, 6898, 11, 370, 286, 1194, 286, 1116, 2073, 512, 2857, 1507, 365, 291, 13], "temperature": 0.0, "avg_logprob": -0.13669633865356445, "compression_ratio": 1.66875, "no_speech_prob": 0.0002231664548162371}, {"id": 15, "seek": 5604, "start": 71.32, "end": 78.96000000000001, "text": " Before I do, is there any questions or requests for things that we've covered last time that", "tokens": [4546, 286, 360, 11, 307, 456, 604, 1651, 420, 12475, 337, 721, 300, 321, 600, 5343, 1036, 565, 300], "temperature": 0.0, "avg_logprob": -0.13669633865356445, "compression_ratio": 1.66875, "no_speech_prob": 0.0002231664548162371}, {"id": 16, "seek": 7896, "start": 78.96, "end": 87.11999999999999, "text": " you'd like more information about or things that you're not quite clear on?", "tokens": [291, 1116, 411, 544, 1589, 466, 420, 721, 300, 291, 434, 406, 1596, 1850, 322, 30], "temperature": 0.0, "avg_logprob": -0.15370455242338635, "compression_ratio": 1.3621621621621622, "no_speech_prob": 1.982652975129895e-05}, {"id": 17, "seek": 7896, "start": 87.11999999999999, "end": 92.8, "text": " Okay we'll just jump in any time if you want me to dive into something more closely.", "tokens": [1033, 321, 603, 445, 3012, 294, 604, 565, 498, 291, 528, 385, 281, 9192, 666, 746, 544, 8185, 13], "temperature": 0.0, "avg_logprob": -0.15370455242338635, "compression_ratio": 1.3621621621621622, "no_speech_prob": 1.982652975129895e-05}, {"id": 18, "seek": 7896, "start": 92.8, "end": 105.36, "text": " But last time we got to an accuracy on IMDB movie review sentiment classification of 94.1%,", "tokens": [583, 1036, 565, 321, 658, 281, 364, 14170, 322, 21463, 27735, 3169, 3131, 16149, 21538, 295, 30849, 13, 16, 8923], "temperature": 0.0, "avg_logprob": -0.15370455242338635, "compression_ratio": 1.3621621621621622, "no_speech_prob": 1.982652975129895e-05}, {"id": 19, "seek": 10536, "start": 105.36, "end": 113.52, "text": " which equaled the state of the art result as of 2017, which is when the ULM fit paper", "tokens": [597, 2681, 292, 264, 1785, 295, 264, 1523, 1874, 382, 295, 6591, 11, 597, 307, 562, 264, 624, 43, 44, 3318, 3035], "temperature": 0.0, "avg_logprob": -0.1052472087698923, "compression_ratio": 1.3950617283950617, "no_speech_prob": 1.8056320186587982e-05}, {"id": 20, "seek": 10536, "start": 113.52, "end": 114.52, "text": " came out.", "tokens": [1361, 484, 13], "temperature": 0.0, "avg_logprob": -0.1052472087698923, "compression_ratio": 1.3950617283950617, "no_speech_prob": 1.8056320186587982e-05}, {"id": 21, "seek": 10536, "start": 114.52, "end": 125.08, "text": " However, the ULM fit paper increased that a lot up to 95.4% accuracy.", "tokens": [2908, 11, 264, 624, 43, 44, 3318, 3035, 6505, 300, 257, 688, 493, 281, 13420, 13, 19, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.1052472087698923, "compression_ratio": 1.3950617283950617, "no_speech_prob": 1.8056320186587982e-05}, {"id": 22, "seek": 10536, "start": 125.08, "end": 133.76, "text": " It's important to note the difference between 94.1 and 95.4.", "tokens": [467, 311, 1021, 281, 3637, 264, 2649, 1296, 30849, 13, 16, 293, 13420, 13, 19, 13], "temperature": 0.0, "avg_logprob": -0.1052472087698923, "compression_ratio": 1.3950617283950617, "no_speech_prob": 1.8056320186587982e-05}, {"id": 23, "seek": 13376, "start": 133.76, "end": 136.95999999999998, "text": " You might think that's not much, it's just a percent.", "tokens": [509, 1062, 519, 300, 311, 406, 709, 11, 309, 311, 445, 257, 3043, 13], "temperature": 0.0, "avg_logprob": -0.18598373312699168, "compression_ratio": 1.4054054054054055, "no_speech_prob": 1.2029435310978442e-05}, {"id": 24, "seek": 13376, "start": 136.95999999999998, "end": 142.0, "text": " But actually any time you're talking about accuracies that are much higher than 50%,", "tokens": [583, 767, 604, 565, 291, 434, 1417, 466, 5771, 20330, 300, 366, 709, 2946, 813, 2625, 8923], "temperature": 0.0, "avg_logprob": -0.18598373312699168, "compression_ratio": 1.4054054054054055, "no_speech_prob": 1.2029435310978442e-05}, {"id": 25, "seek": 13376, "start": 142.0, "end": 145.48, "text": " you're better off reframing them as error rates.", "tokens": [291, 434, 1101, 766, 1895, 2356, 278, 552, 382, 6713, 6846, 13], "temperature": 0.0, "avg_logprob": -0.18598373312699168, "compression_ratio": 1.4054054054054055, "no_speech_prob": 1.2029435310978442e-05}, {"id": 26, "seek": 13376, "start": 145.48, "end": 158.35999999999999, "text": " So actually we've changed the error rate with ULM fit from 5.9% to 4.6%.", "tokens": [407, 767, 321, 600, 3105, 264, 6713, 3314, 365, 624, 43, 44, 3318, 490, 1025, 13, 24, 4, 281, 1017, 13, 21, 6856], "temperature": 0.0, "avg_logprob": -0.18598373312699168, "compression_ratio": 1.4054054054054055, "no_speech_prob": 1.2029435310978442e-05}, {"id": 27, "seek": 15836, "start": 158.36, "end": 163.88000000000002, "text": " When you think of it that way, you realize that the actual number of errors you would make", "tokens": [1133, 291, 519, 295, 309, 300, 636, 11, 291, 4325, 300, 264, 3539, 1230, 295, 13603, 291, 576, 652], "temperature": 0.0, "avg_logprob": -0.1348169307516079, "compression_ratio": 1.7404255319148936, "no_speech_prob": 2.1233643110463163e-06}, {"id": 28, "seek": 15836, "start": 163.88000000000002, "end": 168.72000000000003, "text": " has decreased in relative terms by about 25%.", "tokens": [575, 24436, 294, 4972, 2115, 538, 466, 3552, 6856], "temperature": 0.0, "avg_logprob": -0.1348169307516079, "compression_ratio": 1.7404255319148936, "no_speech_prob": 2.1233643110463163e-06}, {"id": 29, "seek": 15836, "start": 168.72000000000003, "end": 174.72000000000003, "text": " So when you're kind of talking about high accuracies, which is pretty standard, most", "tokens": [407, 562, 291, 434, 733, 295, 1417, 466, 1090, 5771, 20330, 11, 597, 307, 1238, 3832, 11, 881], "temperature": 0.0, "avg_logprob": -0.1348169307516079, "compression_ratio": 1.7404255319148936, "no_speech_prob": 2.1233643110463163e-06}, {"id": 30, "seek": 15836, "start": 174.72000000000003, "end": 178.24, "text": " of the time you want to be thinking in terms of error rates, not accuracy, and then actually", "tokens": [295, 264, 565, 291, 528, 281, 312, 1953, 294, 2115, 295, 6713, 6846, 11, 406, 14170, 11, 293, 550, 767], "temperature": 0.0, "avg_logprob": -0.1348169307516079, "compression_ratio": 1.7404255319148936, "no_speech_prob": 2.1233643110463163e-06}, {"id": 31, "seek": 15836, "start": 178.24, "end": 182.32000000000002, "text": " thinking about the number of errors that are being made, so the relative change in error", "tokens": [1953, 466, 264, 1230, 295, 13603, 300, 366, 885, 1027, 11, 370, 264, 4972, 1319, 294, 6713], "temperature": 0.0, "avg_logprob": -0.1348169307516079, "compression_ratio": 1.7404255319148936, "no_speech_prob": 2.1233643110463163e-06}, {"id": 32, "seek": 15836, "start": 182.32000000000002, "end": 183.88000000000002, "text": " rate.", "tokens": [3314, 13], "temperature": 0.0, "avg_logprob": -0.1348169307516079, "compression_ratio": 1.7404255319148936, "no_speech_prob": 2.1233643110463163e-06}, {"id": 33, "seek": 18388, "start": 183.88, "end": 191.07999999999998, "text": " So there's a really big jump in terms of the error rate improvement between 94.1 and 95.4.", "tokens": [407, 456, 311, 257, 534, 955, 3012, 294, 2115, 295, 264, 6713, 3314, 10444, 1296, 30849, 13, 16, 293, 13420, 13, 19, 13], "temperature": 0.0, "avg_logprob": -0.1391571456310796, "compression_ratio": 1.6018518518518519, "no_speech_prob": 6.962176939850906e-06}, {"id": 34, "seek": 18388, "start": 191.07999999999998, "end": 194.96, "text": " So I wanted to dig into how do we close that gap?", "tokens": [407, 286, 1415, 281, 2528, 666, 577, 360, 321, 1998, 300, 7417, 30], "temperature": 0.0, "avg_logprob": -0.1391571456310796, "compression_ratio": 1.6018518518518519, "no_speech_prob": 6.962176939850906e-06}, {"id": 35, "seek": 18388, "start": 194.96, "end": 199.88, "text": " What's the difference between the state-of-the-art model as of 2017 and the state-of-the-art", "tokens": [708, 311, 264, 2649, 1296, 264, 1785, 12, 2670, 12, 3322, 12, 446, 2316, 382, 295, 6591, 293, 264, 1785, 12, 2670, 12, 3322, 12, 446], "temperature": 0.0, "avg_logprob": -0.1391571456310796, "compression_ratio": 1.6018518518518519, "no_speech_prob": 6.962176939850906e-06}, {"id": 36, "seek": 18388, "start": 199.88, "end": 203.6, "text": " model as of 2019?", "tokens": [2316, 382, 295, 6071, 30], "temperature": 0.0, "avg_logprob": -0.1391571456310796, "compression_ratio": 1.6018518518518519, "no_speech_prob": 6.962176939850906e-06}, {"id": 37, "seek": 18388, "start": 203.6, "end": 212.56, "text": " So I've added this extra notebook called ednimdb more where we will see what some of the extra", "tokens": [407, 286, 600, 3869, 341, 2857, 21060, 1219, 1257, 77, 332, 67, 65, 544, 689, 321, 486, 536, 437, 512, 295, 264, 2857], "temperature": 0.0, "avg_logprob": -0.1391571456310796, "compression_ratio": 1.6018518518518519, "no_speech_prob": 6.962176939850906e-06}, {"id": 38, "seek": 21256, "start": 212.56, "end": 217.08, "text": " things we can do are.", "tokens": [721, 321, 393, 360, 366, 13], "temperature": 0.0, "avg_logprob": -0.1232018198285784, "compression_ratio": 1.5593220338983051, "no_speech_prob": 7.766864655422978e-06}, {"id": 39, "seek": 21256, "start": 217.08, "end": 224.86, "text": " The first difference is a single extra word here in the language model.", "tokens": [440, 700, 2649, 307, 257, 2167, 2857, 1349, 510, 294, 264, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1232018198285784, "compression_ratio": 1.5593220338983051, "no_speech_prob": 7.766864655422978e-06}, {"id": 40, "seek": 21256, "start": 224.86, "end": 233.04, "text": " This is the language model where we're starting with the Wikipedia pre-trained language model,", "tokens": [639, 307, 264, 2856, 2316, 689, 321, 434, 2891, 365, 264, 28999, 659, 12, 17227, 2001, 2856, 2316, 11], "temperature": 0.0, "avg_logprob": -0.1232018198285784, "compression_ratio": 1.5593220338983051, "no_speech_prob": 7.766864655422978e-06}, {"id": 41, "seek": 21256, "start": 233.04, "end": 237.96, "text": " which is automatically downloaded for us by Fast.ai, and we're fine-tuning it to become", "tokens": [597, 307, 6772, 21748, 337, 505, 538, 15968, 13, 1301, 11, 293, 321, 434, 2489, 12, 83, 37726, 309, 281, 1813], "temperature": 0.0, "avg_logprob": -0.1232018198285784, "compression_ratio": 1.5593220338983051, "no_speech_prob": 7.766864655422978e-06}, {"id": 42, "seek": 23796, "start": 237.96, "end": 243.0, "text": " an IMDB movie review language model, so something that is good at predicting the next word of", "tokens": [364, 21463, 27735, 3169, 3131, 2856, 2316, 11, 370, 746, 300, 307, 665, 412, 32884, 264, 958, 1349, 295], "temperature": 0.0, "avg_logprob": -0.1621286846342541, "compression_ratio": 1.8623853211009174, "no_speech_prob": 5.173853878659429e-06}, {"id": 43, "seek": 23796, "start": 243.0, "end": 244.36, "text": " a movie review.", "tokens": [257, 3169, 3131, 13], "temperature": 0.0, "avg_logprob": -0.1621286846342541, "compression_ratio": 1.8623853211009174, "no_speech_prob": 5.173853878659429e-06}, {"id": 44, "seek": 23796, "start": 244.36, "end": 251.20000000000002, "text": " And last time, we just asked to include two directories, train and test, and so that contains", "tokens": [400, 1036, 565, 11, 321, 445, 2351, 281, 4090, 732, 5391, 530, 11, 3847, 293, 1500, 11, 293, 370, 300, 8306], "temperature": 0.0, "avg_logprob": -0.1621286846342541, "compression_ratio": 1.8623853211009174, "no_speech_prob": 5.173853878659429e-06}, {"id": 45, "seek": 23796, "start": 251.20000000000002, "end": 256.7, "text": " all of the movie reviews in the train set and all of the movie reviews in the test set,", "tokens": [439, 295, 264, 3169, 10229, 294, 264, 3847, 992, 293, 439, 295, 264, 3169, 10229, 294, 264, 1500, 992, 11], "temperature": 0.0, "avg_logprob": -0.1621286846342541, "compression_ratio": 1.8623853211009174, "no_speech_prob": 5.173853878659429e-06}, {"id": 46, "seek": 23796, "start": 256.7, "end": 262.48, "text": " and they have labels, so that allows us in the training set to train and the test set", "tokens": [293, 436, 362, 16949, 11, 370, 300, 4045, 505, 294, 264, 3097, 992, 281, 3847, 293, 264, 1500, 992], "temperature": 0.0, "avg_logprob": -0.1621286846342541, "compression_ratio": 1.8623853211009174, "no_speech_prob": 5.173853878659429e-06}, {"id": 47, "seek": 23796, "start": 262.48, "end": 265.88, "text": " to see how well we're doing.", "tokens": [281, 536, 577, 731, 321, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.1621286846342541, "compression_ratio": 1.8623853211009174, "no_speech_prob": 5.173853878659429e-06}, {"id": 48, "seek": 26588, "start": 265.88, "end": 272.04, "text": " However, one of the interesting things about the IMDB data set is it also has a directory", "tokens": [2908, 11, 472, 295, 264, 1880, 721, 466, 264, 21463, 27735, 1412, 992, 307, 309, 611, 575, 257, 21120], "temperature": 0.0, "avg_logprob": -0.09668612218165136, "compression_ratio": 1.6255707762557077, "no_speech_prob": 9.2232548922766e-06}, {"id": 49, "seek": 26588, "start": 272.04, "end": 276.12, "text": " called unsup, which stands for unsupervised.", "tokens": [1219, 2693, 1010, 11, 597, 7382, 337, 2693, 12879, 24420, 13], "temperature": 0.0, "avg_logprob": -0.09668612218165136, "compression_ratio": 1.6255707762557077, "no_speech_prob": 9.2232548922766e-06}, {"id": 50, "seek": 26588, "start": 276.12, "end": 283.4, "text": " And this is a directory of movie reviews that do not have a rating attached to them.", "tokens": [400, 341, 307, 257, 21120, 295, 3169, 10229, 300, 360, 406, 362, 257, 10990, 8570, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.09668612218165136, "compression_ratio": 1.6255707762557077, "no_speech_prob": 9.2232548922766e-06}, {"id": 51, "seek": 26588, "start": 283.4, "end": 288.88, "text": " So it's got the English text of a movie review, but there's nothing about polarity, there's", "tokens": [407, 309, 311, 658, 264, 3669, 2487, 295, 257, 3169, 3131, 11, 457, 456, 311, 1825, 466, 12367, 507, 11, 456, 311], "temperature": 0.0, "avg_logprob": -0.09668612218165136, "compression_ratio": 1.6255707762557077, "no_speech_prob": 9.2232548922766e-06}, {"id": 52, "seek": 26588, "start": 288.88, "end": 292.36, "text": " nothing saying whether they liked it or not.", "tokens": [1825, 1566, 1968, 436, 4501, 309, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.09668612218165136, "compression_ratio": 1.6255707762557077, "no_speech_prob": 9.2232548922766e-06}, {"id": 53, "seek": 29236, "start": 292.36, "end": 298.28000000000003, "text": " This is a super common situation where you have some data that is labeled and some data", "tokens": [639, 307, 257, 1687, 2689, 2590, 689, 291, 362, 512, 1412, 300, 307, 21335, 293, 512, 1412], "temperature": 0.0, "avg_logprob": -0.1343744344878615, "compression_ratio": 1.7230769230769232, "no_speech_prob": 8.267767952929717e-06}, {"id": 54, "seek": 29236, "start": 298.28000000000003, "end": 300.16, "text": " that is unlabeled.", "tokens": [300, 307, 32118, 18657, 292, 13], "temperature": 0.0, "avg_logprob": -0.1343744344878615, "compression_ratio": 1.7230769230769232, "no_speech_prob": 8.267767952929717e-06}, {"id": 55, "seek": 29236, "start": 300.16, "end": 305.86, "text": " And you might think initially, well what's the point in terms of building a sentiment", "tokens": [400, 291, 1062, 519, 9105, 11, 731, 437, 311, 264, 935, 294, 2115, 295, 2390, 257, 16149], "temperature": 0.0, "avg_logprob": -0.1343744344878615, "compression_ratio": 1.7230769230769232, "no_speech_prob": 8.267767952929717e-06}, {"id": 56, "seek": 29236, "start": 305.86, "end": 311.36, "text": " analysis tool, what's the point of movie reviews that don't actually have any labels, so we", "tokens": [5215, 2290, 11, 437, 311, 264, 935, 295, 3169, 10229, 300, 500, 380, 767, 362, 604, 16949, 11, 370, 321], "temperature": 0.0, "avg_logprob": -0.1343744344878615, "compression_ratio": 1.7230769230769232, "no_speech_prob": 8.267767952929717e-06}, {"id": 57, "seek": 29236, "start": 311.36, "end": 315.44, "text": " don't actually know whether they liked them or not, because there's 50,000 of these movies", "tokens": [500, 380, 767, 458, 1968, 436, 4501, 552, 420, 406, 11, 570, 456, 311, 2625, 11, 1360, 295, 613, 6233], "temperature": 0.0, "avg_logprob": -0.1343744344878615, "compression_ratio": 1.7230769230769232, "no_speech_prob": 8.267767952929717e-06}, {"id": 58, "seek": 29236, "start": 315.44, "end": 317.94, "text": " in the unsupervised folder.", "tokens": [294, 264, 2693, 12879, 24420, 10820, 13], "temperature": 0.0, "avg_logprob": -0.1343744344878615, "compression_ratio": 1.7230769230769232, "no_speech_prob": 8.267767952929717e-06}, {"id": 59, "seek": 29236, "start": 317.94, "end": 320.48, "text": " That sounds like maybe it's a waste of time.", "tokens": [663, 3263, 411, 1310, 309, 311, 257, 5964, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.1343744344878615, "compression_ratio": 1.7230769230769232, "no_speech_prob": 8.267767952929717e-06}, {"id": 60, "seek": 32048, "start": 320.48, "end": 326.40000000000003, "text": " But actually there's a whole area of machine learning which is called the study of semi-supervised", "tokens": [583, 767, 456, 311, 257, 1379, 1859, 295, 3479, 2539, 597, 307, 1219, 264, 2979, 295, 12909, 12, 48172, 24420], "temperature": 0.0, "avg_logprob": -0.12289590101975661, "compression_ratio": 1.7209302325581395, "no_speech_prob": 2.994429678437882e-06}, {"id": 61, "seek": 32048, "start": 326.40000000000003, "end": 327.40000000000003, "text": " learning.", "tokens": [2539, 13], "temperature": 0.0, "avg_logprob": -0.12289590101975661, "compression_ratio": 1.7209302325581395, "no_speech_prob": 2.994429678437882e-06}, {"id": 62, "seek": 32048, "start": 327.40000000000003, "end": 332.8, "text": " And semi-supervised learning tries to answer this question, can we do something useful", "tokens": [400, 12909, 12, 48172, 24420, 2539, 9898, 281, 1867, 341, 1168, 11, 393, 321, 360, 746, 4420], "temperature": 0.0, "avg_logprob": -0.12289590101975661, "compression_ratio": 1.7209302325581395, "no_speech_prob": 2.994429678437882e-06}, {"id": 63, "seek": 32048, "start": 332.8, "end": 334.92, "text": " with unlabeled data?", "tokens": [365, 32118, 18657, 292, 1412, 30], "temperature": 0.0, "avg_logprob": -0.12289590101975661, "compression_ratio": 1.7209302325581395, "no_speech_prob": 2.994429678437882e-06}, {"id": 64, "seek": 32048, "start": 334.92, "end": 340.24, "text": " And this turns out to be really important in practice, because most of the time you", "tokens": [400, 341, 4523, 484, 281, 312, 534, 1021, 294, 3124, 11, 570, 881, 295, 264, 565, 291], "temperature": 0.0, "avg_logprob": -0.12289590101975661, "compression_ratio": 1.7209302325581395, "no_speech_prob": 2.994429678437882e-06}, {"id": 65, "seek": 32048, "start": 340.24, "end": 345.40000000000003, "text": " will have some data that has labels and some data that has no labels.", "tokens": [486, 362, 512, 1412, 300, 575, 16949, 293, 512, 1412, 300, 575, 572, 16949, 13], "temperature": 0.0, "avg_logprob": -0.12289590101975661, "compression_ratio": 1.7209302325581395, "no_speech_prob": 2.994429678437882e-06}, {"id": 66, "seek": 34540, "start": 345.4, "end": 354.2, "text": " And in industry, most companies I talk to are not aware of semi-supervised learning,", "tokens": [400, 294, 3518, 11, 881, 3431, 286, 751, 281, 366, 406, 3650, 295, 12909, 12, 48172, 24420, 2539, 11], "temperature": 0.0, "avg_logprob": -0.12766177654266359, "compression_ratio": 1.739622641509434, "no_speech_prob": 3.844912953354651e-06}, {"id": 67, "seek": 34540, "start": 354.2, "end": 358.2, "text": " and so the default situation I find when I talk to companies is they say, oh we can't", "tokens": [293, 370, 264, 7576, 2590, 286, 915, 562, 286, 751, 281, 3431, 307, 436, 584, 11, 1954, 321, 393, 380], "temperature": 0.0, "avg_logprob": -0.12766177654266359, "compression_ratio": 1.739622641509434, "no_speech_prob": 3.844912953354651e-06}, {"id": 68, "seek": 34540, "start": 358.2, "end": 364.08, "text": " start this machine learning project until we've finished labeling all the data.", "tokens": [722, 341, 3479, 2539, 1716, 1826, 321, 600, 4335, 40244, 439, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12766177654266359, "compression_ratio": 1.739622641509434, "no_speech_prob": 3.844912953354651e-06}, {"id": 69, "seek": 34540, "start": 364.08, "end": 366.59999999999997, "text": " And that often can take years.", "tokens": [400, 300, 2049, 393, 747, 924, 13], "temperature": 0.0, "avg_logprob": -0.12766177654266359, "compression_ratio": 1.739622641509434, "no_speech_prob": 3.844912953354651e-06}, {"id": 70, "seek": 34540, "start": 366.59999999999997, "end": 371.12, "text": " And so if you hear people say that, your answer should be, no, let's not wait, let's use the", "tokens": [400, 370, 498, 291, 1568, 561, 584, 300, 11, 428, 1867, 820, 312, 11, 572, 11, 718, 311, 406, 1699, 11, 718, 311, 764, 264], "temperature": 0.0, "avg_logprob": -0.12766177654266359, "compression_ratio": 1.739622641509434, "no_speech_prob": 3.844912953354651e-06}, {"id": 71, "seek": 34540, "start": 371.12, "end": 375.35999999999996, "text": " data that we have already, and for the data that's not labeled, let's try and use same", "tokens": [1412, 300, 321, 362, 1217, 11, 293, 337, 264, 1412, 300, 311, 406, 21335, 11, 718, 311, 853, 293, 764, 912], "temperature": 0.0, "avg_logprob": -0.12766177654266359, "compression_ratio": 1.739622641509434, "no_speech_prob": 3.844912953354651e-06}, {"id": 72, "seek": 37536, "start": 375.36, "end": 379.16, "text": " semi-supervised learning techniques.", "tokens": [12909, 12, 48172, 24420, 2539, 7512, 13], "temperature": 0.0, "avg_logprob": -0.14397059787403454, "compression_ratio": 1.5921052631578947, "no_speech_prob": 2.0145362213952467e-05}, {"id": 73, "seek": 37536, "start": 379.16, "end": 385.92, "text": " For NLP classification, there's actually something very simple we can do with the unsupervised", "tokens": [1171, 426, 45196, 21538, 11, 456, 311, 767, 746, 588, 2199, 321, 393, 360, 365, 264, 2693, 12879, 24420], "temperature": 0.0, "avg_logprob": -0.14397059787403454, "compression_ratio": 1.5921052631578947, "no_speech_prob": 2.0145362213952467e-05}, {"id": 74, "seek": 37536, "start": 385.92, "end": 389.6, "text": " folder, and that is we can add it to our language model.", "tokens": [10820, 11, 293, 300, 307, 321, 393, 909, 309, 281, 527, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14397059787403454, "compression_ratio": 1.5921052631578947, "no_speech_prob": 2.0145362213952467e-05}, {"id": 75, "seek": 37536, "start": 389.6, "end": 393.88, "text": " Because the language model is just something that learns to predict the next word of a", "tokens": [1436, 264, 2856, 2316, 307, 445, 746, 300, 27152, 281, 6069, 264, 958, 1349, 295, 257], "temperature": 0.0, "avg_logprob": -0.14397059787403454, "compression_ratio": 1.5921052631578947, "no_speech_prob": 2.0145362213952467e-05}, {"id": 76, "seek": 37536, "start": 393.88, "end": 394.88, "text": " sentence.", "tokens": [8174, 13], "temperature": 0.0, "avg_logprob": -0.14397059787403454, "compression_ratio": 1.5921052631578947, "no_speech_prob": 2.0145362213952467e-05}, {"id": 77, "seek": 37536, "start": 394.88, "end": 399.96000000000004, "text": " It doesn't need at all to know whether it was positive or negative sentiment.", "tokens": [467, 1177, 380, 643, 412, 439, 281, 458, 1968, 309, 390, 3353, 420, 3671, 16149, 13], "temperature": 0.0, "avg_logprob": -0.14397059787403454, "compression_ratio": 1.5921052631578947, "no_speech_prob": 2.0145362213952467e-05}, {"id": 78, "seek": 39996, "start": 399.96, "end": 407.47999999999996, "text": " And so we've got 25,000 movie reviews in train, 25,000 movie reviews in test, and another", "tokens": [400, 370, 321, 600, 658, 3552, 11, 1360, 3169, 10229, 294, 3847, 11, 3552, 11, 1360, 3169, 10229, 294, 1500, 11, 293, 1071], "temperature": 0.0, "avg_logprob": -0.09643458223890984, "compression_ratio": 1.763157894736842, "no_speech_prob": 2.7693963602359872e-06}, {"id": 79, "seek": 39996, "start": 407.47999999999996, "end": 411.47999999999996, "text": " 50,000 movie reviews in the unsupervised folder.", "tokens": [2625, 11, 1360, 3169, 10229, 294, 264, 2693, 12879, 24420, 10820, 13], "temperature": 0.0, "avg_logprob": -0.09643458223890984, "compression_ratio": 1.763157894736842, "no_speech_prob": 2.7693963602359872e-06}, {"id": 80, "seek": 39996, "start": 411.47999999999996, "end": 417.4, "text": " So if we add that folder into the language model, we have twice as much data to build", "tokens": [407, 498, 321, 909, 300, 10820, 666, 264, 2856, 2316, 11, 321, 362, 6091, 382, 709, 1412, 281, 1322], "temperature": 0.0, "avg_logprob": -0.09643458223890984, "compression_ratio": 1.763157894736842, "no_speech_prob": 2.7693963602359872e-06}, {"id": 81, "seek": 39996, "start": 417.4, "end": 420.09999999999997, "text": " a language model with.", "tokens": [257, 2856, 2316, 365, 13], "temperature": 0.0, "avg_logprob": -0.09643458223890984, "compression_ratio": 1.763157894736842, "no_speech_prob": 2.7693963602359872e-06}, {"id": 82, "seek": 39996, "start": 420.09999999999997, "end": 425.84, "text": " So although the unsupervised folder doesn't tell us anything about whether people liked", "tokens": [407, 4878, 264, 2693, 12879, 24420, 10820, 1177, 380, 980, 505, 1340, 466, 1968, 561, 4501], "temperature": 0.0, "avg_logprob": -0.09643458223890984, "compression_ratio": 1.763157894736842, "no_speech_prob": 2.7693963602359872e-06}, {"id": 83, "seek": 42584, "start": 425.84, "end": 431.32, "text": " the movie or not, remember that building a language model allows us to build something", "tokens": [264, 3169, 420, 406, 11, 1604, 300, 2390, 257, 2856, 2316, 4045, 505, 281, 1322, 746], "temperature": 0.0, "avg_logprob": -0.09081494467599051, "compression_ratio": 1.8138528138528138, "no_speech_prob": 4.289311164029641e-06}, {"id": 84, "seek": 42584, "start": 431.32, "end": 436.67999999999995, "text": " that can learn about the domain, about the language in that domain.", "tokens": [300, 393, 1466, 466, 264, 9274, 11, 466, 264, 2856, 294, 300, 9274, 13], "temperature": 0.0, "avg_logprob": -0.09081494467599051, "compression_ratio": 1.8138528138528138, "no_speech_prob": 4.289311164029641e-06}, {"id": 85, "seek": 42584, "start": 436.67999999999995, "end": 441.08, "text": " So it can find out what kinds of words come after what kinds of other words and what kinds", "tokens": [407, 309, 393, 915, 484, 437, 3685, 295, 2283, 808, 934, 437, 3685, 295, 661, 2283, 293, 437, 3685], "temperature": 0.0, "avg_logprob": -0.09081494467599051, "compression_ratio": 1.8138528138528138, "no_speech_prob": 4.289311164029641e-06}, {"id": 86, "seek": 42584, "start": 441.08, "end": 442.64, "text": " of concepts appear.", "tokens": [295, 10392, 4204, 13], "temperature": 0.0, "avg_logprob": -0.09081494467599051, "compression_ratio": 1.8138528138528138, "no_speech_prob": 4.289311164029641e-06}, {"id": 87, "seek": 42584, "start": 442.64, "end": 447.15999999999997, "text": " And so by having twice as much data, we're going to be able to build, hopefully, a much", "tokens": [400, 370, 538, 1419, 6091, 382, 709, 1412, 11, 321, 434, 516, 281, 312, 1075, 281, 1322, 11, 4696, 11, 257, 709], "temperature": 0.0, "avg_logprob": -0.09081494467599051, "compression_ratio": 1.8138528138528138, "no_speech_prob": 4.289311164029641e-06}, {"id": 88, "seek": 42584, "start": 447.15999999999997, "end": 450.29999999999995, "text": " better language model.", "tokens": [1101, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.09081494467599051, "compression_ratio": 1.8138528138528138, "no_speech_prob": 4.289311164029641e-06}, {"id": 89, "seek": 42584, "start": 450.29999999999995, "end": 452.67999999999995, "text": " So this is an incredibly easy thing to do.", "tokens": [407, 341, 307, 364, 6252, 1858, 551, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.09081494467599051, "compression_ratio": 1.8138528138528138, "no_speech_prob": 4.289311164029641e-06}, {"id": 90, "seek": 45268, "start": 452.68, "end": 456.04, "text": " You just say include this extra folder.", "tokens": [509, 445, 584, 4090, 341, 2857, 10820, 13], "temperature": 0.0, "avg_logprob": -0.1571354275658017, "compression_ratio": 1.5646551724137931, "no_speech_prob": 5.682382834493183e-06}, {"id": 91, "seek": 45268, "start": 456.04, "end": 458.24, "text": " But conceptually, it's super powerful, right?", "tokens": [583, 3410, 671, 11, 309, 311, 1687, 4005, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1571354275658017, "compression_ratio": 1.5646551724137931, "no_speech_prob": 5.682382834493183e-06}, {"id": 92, "seek": 45268, "start": 458.24, "end": 462.76, "text": " Because now we can go ahead and train the model.", "tokens": [1436, 586, 321, 393, 352, 2286, 293, 3847, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1571354275658017, "compression_ratio": 1.5646551724137931, "no_speech_prob": 5.682382834493183e-06}, {"id": 93, "seek": 45268, "start": 462.76, "end": 467.68, "text": " And just like before, we can start by fine-tuning just the embedding layers, because by default", "tokens": [400, 445, 411, 949, 11, 321, 393, 722, 538, 2489, 12, 83, 37726, 445, 264, 12240, 3584, 7914, 11, 570, 538, 7576], "temperature": 0.0, "avg_logprob": -0.1571354275658017, "compression_ratio": 1.5646551724137931, "no_speech_prob": 5.682382834493183e-06}, {"id": 94, "seek": 45268, "start": 467.68, "end": 471.56, "text": " we're using pre-training, so it's frozen, and then we unfreeze.", "tokens": [321, 434, 1228, 659, 12, 17227, 1760, 11, 370, 309, 311, 12496, 11, 293, 550, 321, 3971, 701, 1381, 13], "temperature": 0.0, "avg_logprob": -0.1571354275658017, "compression_ratio": 1.5646551724137931, "no_speech_prob": 5.682382834493183e-06}, {"id": 95, "seek": 45268, "start": 471.56, "end": 478.96000000000004, "text": " But now, because we've got much more data, we can do like 10 epochs.", "tokens": [583, 586, 11, 570, 321, 600, 658, 709, 544, 1412, 11, 321, 393, 360, 411, 1266, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.1571354275658017, "compression_ratio": 1.5646551724137931, "no_speech_prob": 5.682382834493183e-06}, {"id": 96, "seek": 47896, "start": 478.96, "end": 485.2, "text": " We can train for much longer without overfitting the language model.", "tokens": [492, 393, 3847, 337, 709, 2854, 1553, 670, 69, 2414, 264, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14063988722764054, "compression_ratio": 1.4193548387096775, "no_speech_prob": 1.4593686046282528e-06}, {"id": 97, "seek": 47896, "start": 485.2, "end": 490.08, "text": " So that was the first thing that we did differently in IMDB more.", "tokens": [407, 300, 390, 264, 700, 551, 300, 321, 630, 7614, 294, 21463, 27735, 544, 13], "temperature": 0.0, "avg_logprob": -0.14063988722764054, "compression_ratio": 1.4193548387096775, "no_speech_prob": 1.4593686046282528e-06}, {"id": 98, "seek": 47896, "start": 490.08, "end": 496.03999999999996, "text": " We added the unsupervised folder and we increased the number of epochs from 4 to 10.", "tokens": [492, 3869, 264, 2693, 12879, 24420, 10820, 293, 321, 6505, 264, 1230, 295, 30992, 28346, 490, 1017, 281, 1266, 13], "temperature": 0.0, "avg_logprob": -0.14063988722764054, "compression_ratio": 1.4193548387096775, "no_speech_prob": 1.4593686046282528e-06}, {"id": 99, "seek": 47896, "start": 496.03999999999996, "end": 503.84, "text": " Now you'll find that this takes on an average consumer graphics card of kind of the 2018", "tokens": [823, 291, 603, 915, 300, 341, 2516, 322, 364, 4274, 9711, 11837, 2920, 295, 733, 295, 264, 6096], "temperature": 0.0, "avg_logprob": -0.14063988722764054, "compression_ratio": 1.4193548387096775, "no_speech_prob": 1.4593686046282528e-06}, {"id": 100, "seek": 50384, "start": 503.84, "end": 512.9599999999999, "text": " frame, about an hour or so per epoch, so you're going to have to run this overnight.", "tokens": [3920, 11, 466, 364, 1773, 420, 370, 680, 30992, 339, 11, 370, 291, 434, 516, 281, 362, 281, 1190, 341, 13935, 13], "temperature": 0.0, "avg_logprob": -0.10186794641855601, "compression_ratio": 1.5803108808290156, "no_speech_prob": 1.0129822840099223e-05}, {"id": 101, "seek": 50384, "start": 512.9599999999999, "end": 519.48, "text": " And so learning how to manage models that run overnight is an important part of becoming", "tokens": [400, 370, 2539, 577, 281, 3067, 5245, 300, 1190, 13935, 307, 364, 1021, 644, 295, 5617], "temperature": 0.0, "avg_logprob": -0.10186794641855601, "compression_ratio": 1.5803108808290156, "no_speech_prob": 1.0129822840099223e-05}, {"id": 102, "seek": 50384, "start": 519.48, "end": 522.22, "text": " an effective deep learning practitioner.", "tokens": [364, 4942, 2452, 2539, 32125, 13], "temperature": 0.0, "avg_logprob": -0.10186794641855601, "compression_ratio": 1.5803108808290156, "no_speech_prob": 1.0129822840099223e-05}, {"id": 103, "seek": 50384, "start": 522.22, "end": 528.88, "text": " And really the key thing for stuff that takes that long is to do it as little as possible.", "tokens": [400, 534, 264, 2141, 551, 337, 1507, 300, 2516, 300, 938, 307, 281, 360, 309, 382, 707, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.10186794641855601, "compression_ratio": 1.5803108808290156, "no_speech_prob": 1.0129822840099223e-05}, {"id": 104, "seek": 52888, "start": 528.88, "end": 534.52, "text": " So when I was developing this notebook, I spent most of my time working on the smaller", "tokens": [407, 562, 286, 390, 6416, 341, 21060, 11, 286, 4418, 881, 295, 452, 565, 1364, 322, 264, 4356], "temperature": 0.0, "avg_logprob": -0.10040132816021259, "compression_ratio": 1.582995951417004, "no_speech_prob": 5.5075311138352845e-06}, {"id": 105, "seek": 52888, "start": 534.52, "end": 539.72, "text": " data set without the unsupervised folder and just training for one epoch or two, so it", "tokens": [1412, 992, 1553, 264, 2693, 12879, 24420, 10820, 293, 445, 3097, 337, 472, 30992, 339, 420, 732, 11, 370, 309], "temperature": 0.0, "avg_logprob": -0.10040132816021259, "compression_ratio": 1.582995951417004, "no_speech_prob": 5.5075311138352845e-06}, {"id": 106, "seek": 52888, "start": 539.72, "end": 541.88, "text": " would only take about an hour total.", "tokens": [576, 787, 747, 466, 364, 1773, 3217, 13], "temperature": 0.0, "avg_logprob": -0.10040132816021259, "compression_ratio": 1.582995951417004, "no_speech_prob": 5.5075311138352845e-06}, {"id": 107, "seek": 52888, "start": 541.88, "end": 549.68, "text": " And then once everything was working really well, then I ran it overnight for longer.", "tokens": [400, 550, 1564, 1203, 390, 1364, 534, 731, 11, 550, 286, 5872, 309, 13935, 337, 2854, 13], "temperature": 0.0, "avg_logprob": -0.10040132816021259, "compression_ratio": 1.582995951417004, "no_speech_prob": 5.5075311138352845e-06}, {"id": 108, "seek": 52888, "start": 549.68, "end": 556.72, "text": " So if something's going to take ages to run, don't let that make you think, oh, I can't", "tokens": [407, 498, 746, 311, 516, 281, 747, 12357, 281, 1190, 11, 500, 380, 718, 300, 652, 291, 519, 11, 1954, 11, 286, 393, 380], "temperature": 0.0, "avg_logprob": -0.10040132816021259, "compression_ratio": 1.582995951417004, "no_speech_prob": 5.5075311138352845e-06}, {"id": 109, "seek": 52888, "start": 556.72, "end": 557.72, "text": " do it.", "tokens": [360, 309, 13], "temperature": 0.0, "avg_logprob": -0.10040132816021259, "compression_ratio": 1.582995951417004, "no_speech_prob": 5.5075311138352845e-06}, {"id": 110, "seek": 55772, "start": 557.72, "end": 564.64, "text": " Start it running before you go to bed and it'll be finished when you get up in the morning.", "tokens": [6481, 309, 2614, 949, 291, 352, 281, 2901, 293, 309, 603, 312, 4335, 562, 291, 483, 493, 294, 264, 2446, 13], "temperature": 0.0, "avg_logprob": -0.1228090883737587, "compression_ratio": 1.570754716981132, "no_speech_prob": 6.048858722351724e-06}, {"id": 111, "seek": 55772, "start": 564.64, "end": 572.44, "text": " The next thing I did was I added this little bit here, 2FP16.", "tokens": [440, 958, 551, 286, 630, 390, 286, 3869, 341, 707, 857, 510, 11, 568, 45882, 6866, 13], "temperature": 0.0, "avg_logprob": -0.1228090883737587, "compression_ratio": 1.570754716981132, "no_speech_prob": 6.048858722351724e-06}, {"id": 112, "seek": 55772, "start": 572.44, "end": 578.0, "text": " What that does is it tells Fast.ai to use something called mixed precision training.", "tokens": [708, 300, 775, 307, 309, 5112, 15968, 13, 1301, 281, 764, 746, 1219, 7467, 18356, 3097, 13], "temperature": 0.0, "avg_logprob": -0.1228090883737587, "compression_ratio": 1.570754716981132, "no_speech_prob": 6.048858722351724e-06}, {"id": 113, "seek": 55772, "start": 578.0, "end": 585.48, "text": " And mixed precision training means that rather than using 32-bit floats, which are also called", "tokens": [400, 7467, 18356, 3097, 1355, 300, 2831, 813, 1228, 8858, 12, 5260, 37878, 11, 597, 366, 611, 1219], "temperature": 0.0, "avg_logprob": -0.1228090883737587, "compression_ratio": 1.570754716981132, "no_speech_prob": 6.048858722351724e-06}, {"id": 114, "seek": 58548, "start": 585.48, "end": 591.08, "text": " single precision floats, we're going to use 16-bit floats, also called half precision", "tokens": [2167, 18356, 37878, 11, 321, 434, 516, 281, 764, 3165, 12, 5260, 37878, 11, 611, 1219, 1922, 18356], "temperature": 0.0, "avg_logprob": -0.16401104011920967, "compression_ratio": 1.6820083682008369, "no_speech_prob": 1.3419818060356192e-05}, {"id": 115, "seek": 58548, "start": 591.08, "end": 592.84, "text": " floats.", "tokens": [37878, 13], "temperature": 0.0, "avg_logprob": -0.16401104011920967, "compression_ratio": 1.6820083682008369, "no_speech_prob": 1.3419818060356192e-05}, {"id": 116, "seek": 58548, "start": 592.84, "end": 598.28, "text": " So I think Rachel, did you cover floating point briefly?", "tokens": [407, 286, 519, 14246, 11, 630, 291, 2060, 12607, 935, 10515, 30], "temperature": 0.0, "avg_logprob": -0.16401104011920967, "compression_ratio": 1.6820083682008369, "no_speech_prob": 1.3419818060356192e-05}, {"id": 117, "seek": 58548, "start": 598.28, "end": 599.44, "text": " I think you did, right?", "tokens": [286, 519, 291, 630, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16401104011920967, "compression_ratio": 1.6820083682008369, "no_speech_prob": 1.3419818060356192e-05}, {"id": 118, "seek": 58548, "start": 599.44, "end": 603.28, "text": " So Rachel's very briefly covered floating point and pointed out that there's more detail", "tokens": [407, 14246, 311, 588, 10515, 5343, 12607, 935, 293, 10932, 484, 300, 456, 311, 544, 2607], "temperature": 0.0, "avg_logprob": -0.16401104011920967, "compression_ratio": 1.6820083682008369, "no_speech_prob": 1.3419818060356192e-05}, {"id": 119, "seek": 58548, "start": 603.28, "end": 606.08, "text": " in the computational linear algebra course.", "tokens": [294, 264, 28270, 8213, 21989, 1164, 13], "temperature": 0.0, "avg_logprob": -0.16401104011920967, "compression_ratio": 1.6820083682008369, "no_speech_prob": 1.3419818060356192e-05}, {"id": 120, "seek": 58548, "start": 606.08, "end": 614.96, "text": " But basically all of the numbers in your computer are stored as bits, as on and off, and until", "tokens": [583, 1936, 439, 295, 264, 3547, 294, 428, 3820, 366, 12187, 382, 9239, 11, 382, 322, 293, 766, 11, 293, 1826], "temperature": 0.0, "avg_logprob": -0.16401104011920967, "compression_ratio": 1.6820083682008369, "no_speech_prob": 1.3419818060356192e-05}, {"id": 121, "seek": 61496, "start": 614.96, "end": 621.52, "text": " quite recently most people thought that 32 bits was the minimum you needed to do something", "tokens": [1596, 3938, 881, 561, 1194, 300, 8858, 9239, 390, 264, 7285, 291, 2978, 281, 360, 746], "temperature": 0.0, "avg_logprob": -0.1862913108453518, "compression_ratio": 1.5855855855855856, "no_speech_prob": 3.4465408589312574e-06}, {"id": 122, "seek": 61496, "start": 621.52, "end": 623.72, "text": " useful with floating point values.", "tokens": [4420, 365, 12607, 935, 4190, 13], "temperature": 0.0, "avg_logprob": -0.1862913108453518, "compression_ratio": 1.5855855855855856, "no_speech_prob": 3.4465408589312574e-06}, {"id": 123, "seek": 61496, "start": 623.72, "end": 629.96, "text": " Because for less than that, you get very little accuracy.", "tokens": [1436, 337, 1570, 813, 300, 11, 291, 483, 588, 707, 14170, 13], "temperature": 0.0, "avg_logprob": -0.1862913108453518, "compression_ratio": 1.5855855855855856, "no_speech_prob": 3.4465408589312574e-06}, {"id": 124, "seek": 61496, "start": 629.96, "end": 637.44, "text": " But then people kind of started thinking, well, why do we need, what's wrong with having", "tokens": [583, 550, 561, 733, 295, 1409, 1953, 11, 731, 11, 983, 360, 321, 643, 11, 437, 311, 2085, 365, 1419], "temperature": 0.0, "avg_logprob": -0.1862913108453518, "compression_ratio": 1.5855855855855856, "no_speech_prob": 3.4465408589312574e-06}, {"id": 125, "seek": 61496, "start": 637.44, "end": 639.4000000000001, "text": " very little accuracy?", "tokens": [588, 707, 14170, 30], "temperature": 0.0, "avg_logprob": -0.1862913108453518, "compression_ratio": 1.5855855855855856, "no_speech_prob": 3.4465408589312574e-06}, {"id": 126, "seek": 61496, "start": 639.4000000000001, "end": 644.24, "text": " Because deep learning models are meant to be approximate.", "tokens": [1436, 2452, 2539, 5245, 366, 4140, 281, 312, 30874, 13], "temperature": 0.0, "avg_logprob": -0.1862913108453518, "compression_ratio": 1.5855855855855856, "no_speech_prob": 3.4465408589312574e-06}, {"id": 127, "seek": 64424, "start": 644.24, "end": 647.2, "text": " Like they're kind of meant to be rough guesses.", "tokens": [1743, 436, 434, 733, 295, 4140, 281, 312, 5903, 42703, 13], "temperature": 0.0, "avg_logprob": -0.10996397041980131, "compression_ratio": 1.5924170616113744, "no_speech_prob": 1.0129831025551539e-05}, {"id": 128, "seek": 64424, "start": 647.2, "end": 652.6800000000001, "text": " We kind of like things being a bit stochastic in them, it makes them generalize more.", "tokens": [492, 733, 295, 411, 721, 885, 257, 857, 342, 8997, 2750, 294, 552, 11, 309, 1669, 552, 2674, 1125, 544, 13], "temperature": 0.0, "avg_logprob": -0.10996397041980131, "compression_ratio": 1.5924170616113744, "no_speech_prob": 1.0129831025551539e-05}, {"id": 129, "seek": 64424, "start": 652.6800000000001, "end": 658.4, "text": " Maybe we could try using even smaller precision values.", "tokens": [2704, 321, 727, 853, 1228, 754, 4356, 18356, 4190, 13], "temperature": 0.0, "avg_logprob": -0.10996397041980131, "compression_ratio": 1.5924170616113744, "no_speech_prob": 1.0129831025551539e-05}, {"id": 130, "seek": 64424, "start": 658.4, "end": 663.38, "text": " And so people started trying to play around with half precision floating point.", "tokens": [400, 370, 561, 1409, 1382, 281, 862, 926, 365, 1922, 18356, 12607, 935, 13], "temperature": 0.0, "avg_logprob": -0.10996397041980131, "compression_ratio": 1.5924170616113744, "no_speech_prob": 1.0129831025551539e-05}, {"id": 131, "seek": 64424, "start": 663.38, "end": 668.72, "text": " So most CPUs actually don't support half precision floating point.", "tokens": [407, 881, 13199, 82, 767, 500, 380, 1406, 1922, 18356, 12607, 935, 13], "temperature": 0.0, "avg_logprob": -0.10996397041980131, "compression_ratio": 1.5924170616113744, "no_speech_prob": 1.0129831025551539e-05}, {"id": 132, "seek": 66872, "start": 668.72, "end": 675.4200000000001, "text": " But in recent times, like in the last two years or so, some GPUs started supporting", "tokens": [583, 294, 5162, 1413, 11, 411, 294, 264, 1036, 732, 924, 420, 370, 11, 512, 18407, 82, 1409, 7231], "temperature": 0.0, "avg_logprob": -0.1324528188121562, "compression_ratio": 1.5477178423236515, "no_speech_prob": 7.766891940264031e-06}, {"id": 133, "seek": 66872, "start": 675.4200000000001, "end": 677.84, "text": " half precision floating point.", "tokens": [1922, 18356, 12607, 935, 13], "temperature": 0.0, "avg_logprob": -0.1324528188121562, "compression_ratio": 1.5477178423236515, "no_speech_prob": 7.766891940264031e-06}, {"id": 134, "seek": 66872, "start": 677.84, "end": 680.76, "text": " And interestingly, it doesn't just make it twice as fast.", "tokens": [400, 25873, 11, 309, 1177, 380, 445, 652, 309, 6091, 382, 2370, 13], "temperature": 0.0, "avg_logprob": -0.1324528188121562, "compression_ratio": 1.5477178423236515, "no_speech_prob": 7.766891940264031e-06}, {"id": 135, "seek": 66872, "start": 680.76, "end": 684.0, "text": " You might think, oh, there's half as many bits for each number to use, so maybe it'll", "tokens": [509, 1062, 519, 11, 1954, 11, 456, 311, 1922, 382, 867, 9239, 337, 1184, 1230, 281, 764, 11, 370, 1310, 309, 603], "temperature": 0.0, "avg_logprob": -0.1324528188121562, "compression_ratio": 1.5477178423236515, "no_speech_prob": 7.766891940264031e-06}, {"id": 136, "seek": 66872, "start": 684.0, "end": 685.88, "text": " run twice as fast.", "tokens": [1190, 6091, 382, 2370, 13], "temperature": 0.0, "avg_logprob": -0.1324528188121562, "compression_ratio": 1.5477178423236515, "no_speech_prob": 7.766891940264031e-06}, {"id": 137, "seek": 66872, "start": 685.88, "end": 693.5600000000001, "text": " But what actually happened was Nvidia in their GPUs and Google in their TPUs both added special", "tokens": [583, 437, 767, 2011, 390, 46284, 294, 641, 18407, 82, 293, 3329, 294, 641, 314, 8115, 82, 1293, 3869, 2121], "temperature": 0.0, "avg_logprob": -0.1324528188121562, "compression_ratio": 1.5477178423236515, "no_speech_prob": 7.766891940264031e-06}, {"id": 138, "seek": 69356, "start": 693.56, "end": 701.0799999999999, "text": " hardware that actually runs 8 to 10 times faster when you use half precision.", "tokens": [8837, 300, 767, 6676, 1649, 281, 1266, 1413, 4663, 562, 291, 764, 1922, 18356, 13], "temperature": 0.0, "avg_logprob": -0.13024563387215857, "compression_ratio": 1.426605504587156, "no_speech_prob": 6.854273578937864e-06}, {"id": 139, "seek": 69356, "start": 701.0799999999999, "end": 705.3599999999999, "text": " So suddenly it becomes very important to try to use that.", "tokens": [407, 5800, 309, 3643, 588, 1021, 281, 853, 281, 764, 300, 13], "temperature": 0.0, "avg_logprob": -0.13024563387215857, "compression_ratio": 1.426605504587156, "no_speech_prob": 6.854273578937864e-06}, {"id": 140, "seek": 69356, "start": 705.3599999999999, "end": 713.1999999999999, "text": " Because nowadays, if you buy a gaming card from Nvidia, like a GTX 2060 or 2070 or 2080,", "tokens": [1436, 13434, 11, 498, 291, 2256, 257, 9703, 2920, 490, 46284, 11, 411, 257, 17530, 55, 945, 4550, 420, 945, 5867, 420, 945, 4702, 11], "temperature": 0.0, "avg_logprob": -0.13024563387215857, "compression_ratio": 1.426605504587156, "no_speech_prob": 6.854273578937864e-06}, {"id": 141, "seek": 69356, "start": 713.1999999999999, "end": 721.8, "text": " which will cost you $400, somewhere between $400 and $800, so like within the realm of", "tokens": [597, 486, 2063, 291, 1848, 13741, 11, 4079, 1296, 1848, 13741, 293, 1848, 14423, 11, 370, 411, 1951, 264, 15355, 295], "temperature": 0.0, "avg_logprob": -0.13024563387215857, "compression_ratio": 1.426605504587156, "no_speech_prob": 6.854273578937864e-06}, {"id": 142, "seek": 72180, "start": 721.8, "end": 728.68, "text": " regular folks, you actually get this half precision floating point speedup.", "tokens": [3890, 4024, 11, 291, 767, 483, 341, 1922, 18356, 12607, 935, 3073, 1010, 13], "temperature": 0.0, "avg_logprob": -0.13403048882117638, "compression_ratio": 1.5069124423963134, "no_speech_prob": 3.2377431580243865e-06}, {"id": 143, "seek": 72180, "start": 728.68, "end": 731.74, "text": " Nvidia calls them tensor cores.", "tokens": [46284, 5498, 552, 40863, 24826, 13], "temperature": 0.0, "avg_logprob": -0.13403048882117638, "compression_ratio": 1.5069124423963134, "no_speech_prob": 3.2377431580243865e-06}, {"id": 144, "seek": 72180, "start": 731.74, "end": 737.56, "text": " So even if you're not working in some giant company or giant research lab, this is actually", "tokens": [407, 754, 498, 291, 434, 406, 1364, 294, 512, 7410, 2237, 420, 7410, 2132, 2715, 11, 341, 307, 767], "temperature": 0.0, "avg_logprob": -0.13403048882117638, "compression_ratio": 1.5069124423963134, "no_speech_prob": 3.2377431580243865e-06}, {"id": 145, "seek": 72180, "start": 737.56, "end": 741.8399999999999, "text": " important for us normal folks nowadays.", "tokens": [1021, 337, 505, 2710, 4024, 13434, 13], "temperature": 0.0, "avg_logprob": -0.13403048882117638, "compression_ratio": 1.5069124423963134, "no_speech_prob": 3.2377431580243865e-06}, {"id": 146, "seek": 72180, "start": 741.8399999999999, "end": 751.24, "text": " Now the problem is that 16-bit floating point, as I mentioned, it is pretty inaccurate.", "tokens": [823, 264, 1154, 307, 300, 3165, 12, 5260, 12607, 935, 11, 382, 286, 2835, 11, 309, 307, 1238, 46443, 13], "temperature": 0.0, "avg_logprob": -0.13403048882117638, "compression_ratio": 1.5069124423963134, "no_speech_prob": 3.2377431580243865e-06}, {"id": 147, "seek": 75124, "start": 751.24, "end": 755.9, "text": " And although there are some parts of the neural net where we don't care about that, like generally", "tokens": [400, 4878, 456, 366, 512, 3166, 295, 264, 18161, 2533, 689, 321, 500, 380, 1127, 466, 300, 11, 411, 5101], "temperature": 0.0, "avg_logprob": -0.11931413133567739, "compression_ratio": 2.0277777777777777, "no_speech_prob": 1.0129837392014451e-05}, {"id": 148, "seek": 75124, "start": 755.9, "end": 762.32, "text": " actually calculating the forward pass and actually calculating the gradients, half precision", "tokens": [767, 28258, 264, 2128, 1320, 293, 767, 28258, 264, 2771, 2448, 11, 1922, 18356], "temperature": 0.0, "avg_logprob": -0.11931413133567739, "compression_ratio": 2.0277777777777777, "no_speech_prob": 1.0129837392014451e-05}, {"id": 149, "seek": 75124, "start": 762.32, "end": 763.88, "text": " floating point is fine.", "tokens": [12607, 935, 307, 2489, 13], "temperature": 0.0, "avg_logprob": -0.11931413133567739, "compression_ratio": 2.0277777777777777, "no_speech_prob": 1.0129837392014451e-05}, {"id": 150, "seek": 75124, "start": 763.88, "end": 770.72, "text": " It turns out that single precision floating point is important for other kinds of calculations,", "tokens": [467, 4523, 484, 300, 2167, 18356, 12607, 935, 307, 1021, 337, 661, 3685, 295, 20448, 11], "temperature": 0.0, "avg_logprob": -0.11931413133567739, "compression_ratio": 2.0277777777777777, "no_speech_prob": 1.0129837392014451e-05}, {"id": 151, "seek": 75124, "start": 770.72, "end": 775.44, "text": " like when we actually calculate the loss function or when we actually multiply the gradient", "tokens": [411, 562, 321, 767, 8873, 264, 4470, 2445, 420, 562, 321, 767, 12972, 264, 16235], "temperature": 0.0, "avg_logprob": -0.11931413133567739, "compression_ratio": 2.0277777777777777, "no_speech_prob": 1.0129837392014451e-05}, {"id": 152, "seek": 75124, "start": 775.44, "end": 776.96, "text": " by the learning rate.", "tokens": [538, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.11931413133567739, "compression_ratio": 2.0277777777777777, "no_speech_prob": 1.0129837392014451e-05}, {"id": 153, "seek": 75124, "start": 776.96, "end": 781.04, "text": " You can get these really small numbers and in half precision floating point, they can", "tokens": [509, 393, 483, 613, 534, 1359, 3547, 293, 294, 1922, 18356, 12607, 935, 11, 436, 393], "temperature": 0.0, "avg_logprob": -0.11931413133567739, "compression_ratio": 2.0277777777777777, "no_speech_prob": 1.0129837392014451e-05}, {"id": 154, "seek": 78104, "start": 781.04, "end": 783.76, "text": " kind of vanish down to zero.", "tokens": [733, 295, 43584, 760, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.10808739295372596, "compression_ratio": 1.790909090909091, "no_speech_prob": 8.53026176628191e-06}, {"id": 155, "seek": 78104, "start": 783.76, "end": 788.92, "text": " So in practice, we have to use something called mixed precision.", "tokens": [407, 294, 3124, 11, 321, 362, 281, 764, 746, 1219, 7467, 18356, 13], "temperature": 0.0, "avg_logprob": -0.10808739295372596, "compression_ratio": 1.790909090909091, "no_speech_prob": 8.53026176628191e-06}, {"id": 156, "seek": 78104, "start": 788.92, "end": 793.3199999999999, "text": " And so mixed precision is basically where your model has to do some calculations in", "tokens": [400, 370, 7467, 18356, 307, 1936, 689, 428, 2316, 575, 281, 360, 512, 20448, 294], "temperature": 0.0, "avg_logprob": -0.10808739295372596, "compression_ratio": 1.790909090909091, "no_speech_prob": 8.53026176628191e-06}, {"id": 157, "seek": 78104, "start": 793.3199999999999, "end": 799.68, "text": " half precision and some calculations in single precision.", "tokens": [1922, 18356, 293, 512, 20448, 294, 2167, 18356, 13], "temperature": 0.0, "avg_logprob": -0.10808739295372596, "compression_ratio": 1.790909090909091, "no_speech_prob": 8.53026176628191e-06}, {"id": 158, "seek": 78104, "start": 799.68, "end": 804.36, "text": " So if that sounds complicated, well that's because it is a bit complicated and you wouldn't", "tokens": [407, 498, 300, 3263, 6179, 11, 731, 300, 311, 570, 309, 307, 257, 857, 6179, 293, 291, 2759, 380], "temperature": 0.0, "avg_logprob": -0.10808739295372596, "compression_ratio": 1.790909090909091, "no_speech_prob": 8.53026176628191e-06}, {"id": 159, "seek": 78104, "start": 804.36, "end": 809.8, "text": " want to do this in a library that doesn't support mixed precision.", "tokens": [528, 281, 360, 341, 294, 257, 6405, 300, 1177, 380, 1406, 7467, 18356, 13], "temperature": 0.0, "avg_logprob": -0.10808739295372596, "compression_ratio": 1.790909090909091, "no_speech_prob": 8.53026176628191e-06}, {"id": 160, "seek": 80980, "start": 809.8, "end": 816.1999999999999, "text": " The good news is that fast.ai actually, if you asked add to FP16, it does all that for", "tokens": [440, 665, 2583, 307, 300, 2370, 13, 1301, 767, 11, 498, 291, 2351, 909, 281, 36655, 6866, 11, 309, 775, 439, 300, 337], "temperature": 0.0, "avg_logprob": -0.20875746704811274, "compression_ratio": 1.5, "no_speech_prob": 2.1782070689368993e-05}, {"id": 161, "seek": 80980, "start": 816.1999999999999, "end": 817.88, "text": " you.", "tokens": [291, 13], "temperature": 0.0, "avg_logprob": -0.20875746704811274, "compression_ratio": 1.5, "no_speech_prob": 2.1782070689368993e-05}, {"id": 162, "seek": 80980, "start": 817.88, "end": 827.92, "text": " And so if you're running this on a 20 series GPU or if you're using AWS, if you're using", "tokens": [400, 370, 498, 291, 434, 2614, 341, 322, 257, 945, 2638, 18407, 420, 498, 291, 434, 1228, 17650, 11, 498, 291, 434, 1228], "temperature": 0.0, "avg_logprob": -0.20875746704811274, "compression_ratio": 1.5, "no_speech_prob": 2.1782070689368993e-05}, {"id": 163, "seek": 80980, "start": 827.92, "end": 837.24, "text": " one of their P3 instances, this will give you a 2-3x speedup on this LSTM, on this recurrent", "tokens": [472, 295, 641, 430, 18, 14519, 11, 341, 486, 976, 291, 257, 568, 12, 18, 87, 3073, 1010, 322, 341, 441, 6840, 44, 11, 322, 341, 18680, 1753], "temperature": 0.0, "avg_logprob": -0.20875746704811274, "compression_ratio": 1.5, "no_speech_prob": 2.1782070689368993e-05}, {"id": 164, "seek": 83724, "start": 837.24, "end": 840.08, "text": " neural network.", "tokens": [18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.2165388416599583, "compression_ratio": 1.5055555555555555, "no_speech_prob": 2.6274414267390966e-05}, {"id": 165, "seek": 83724, "start": 840.08, "end": 846.88, "text": " So this way I was able to get down to 20 minute epochs or so.", "tokens": [407, 341, 636, 286, 390, 1075, 281, 483, 760, 281, 945, 3456, 30992, 28346, 420, 370, 13], "temperature": 0.0, "avg_logprob": -0.2165388416599583, "compression_ratio": 1.5055555555555555, "no_speech_prob": 2.6274414267390966e-05}, {"id": 166, "seek": 83724, "start": 846.88, "end": 852.28, "text": " I just wanted to note on the topic of recurrent neural networks that that's what we'll be", "tokens": [286, 445, 1415, 281, 3637, 322, 264, 4829, 295, 18680, 1753, 18161, 9590, 300, 300, 311, 437, 321, 603, 312], "temperature": 0.0, "avg_logprob": -0.2165388416599583, "compression_ratio": 1.5055555555555555, "no_speech_prob": 2.6274414267390966e-05}, {"id": 167, "seek": 83724, "start": 852.28, "end": 857.64, "text": " diving into next, so either later today or next class to talk more about how those work", "tokens": [20241, 666, 958, 11, 370, 2139, 1780, 965, 420, 958, 1508, 281, 751, 544, 466, 577, 729, 589], "temperature": 0.0, "avg_logprob": -0.2165388416599583, "compression_ratio": 1.5055555555555555, "no_speech_prob": 2.6274414267390966e-05}, {"id": 168, "seek": 83724, "start": 857.64, "end": 863.12, "text": " under the hood.", "tokens": [833, 264, 13376, 13], "temperature": 0.0, "avg_logprob": -0.2165388416599583, "compression_ratio": 1.5055555555555555, "no_speech_prob": 2.6274414267390966e-05}, {"id": 169, "seek": 86312, "start": 863.12, "end": 868.4, "text": " The next thing I wanted to talk about was a bit more detail about what happens when we", "tokens": [440, 958, 551, 286, 1415, 281, 751, 466, 390, 257, 857, 544, 2607, 466, 437, 2314, 562, 321], "temperature": 0.0, "avg_logprob": -0.1882871262570645, "compression_ratio": 1.6866359447004609, "no_speech_prob": 2.8856276912847534e-05}, {"id": 170, "seek": 86312, "start": 868.4, "end": 869.4, "text": " save encoder.", "tokens": [3155, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.1882871262570645, "compression_ratio": 1.6866359447004609, "no_speech_prob": 2.8856276912847534e-05}, {"id": 171, "seek": 86312, "start": 869.4, "end": 870.8, "text": " You see we've got two lines here.", "tokens": [509, 536, 321, 600, 658, 732, 3876, 510, 13], "temperature": 0.0, "avg_logprob": -0.1882871262570645, "compression_ratio": 1.6866359447004609, "no_speech_prob": 2.8856276912847534e-05}, {"id": 172, "seek": 86312, "start": 870.8, "end": 876.72, "text": " We save the model, so that's saving the parameters of the model, but then there's also something", "tokens": [492, 3155, 264, 2316, 11, 370, 300, 311, 6816, 264, 9834, 295, 264, 2316, 11, 457, 550, 456, 311, 611, 746], "temperature": 0.0, "avg_logprob": -0.1882871262570645, "compression_ratio": 1.6866359447004609, "no_speech_prob": 2.8856276912847534e-05}, {"id": 173, "seek": 86312, "start": 876.72, "end": 879.12, "text": " here that says save encoder.", "tokens": [510, 300, 1619, 3155, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.1882871262570645, "compression_ratio": 1.6866359447004609, "no_speech_prob": 2.8856276912847534e-05}, {"id": 174, "seek": 86312, "start": 879.12, "end": 881.8, "text": " What's that doing?", "tokens": [708, 311, 300, 884, 30], "temperature": 0.0, "avg_logprob": -0.1882871262570645, "compression_ratio": 1.6866359447004609, "no_speech_prob": 2.8856276912847534e-05}, {"id": 175, "seek": 86312, "start": 881.8, "end": 888.62, "text": " Rachel will be talking more about this when she talks about neural machine translation", "tokens": [14246, 486, 312, 1417, 544, 466, 341, 562, 750, 6686, 466, 18161, 3479, 12853], "temperature": 0.0, "avg_logprob": -0.1882871262570645, "compression_ratio": 1.6866359447004609, "no_speech_prob": 2.8856276912847534e-05}, {"id": 176, "seek": 88862, "start": 888.62, "end": 895.16, "text": " in an upcoming lesson, but I thought I'd kind of give you a quick summary of what's going", "tokens": [294, 364, 11500, 6898, 11, 457, 286, 1194, 286, 1116, 733, 295, 976, 291, 257, 1702, 12691, 295, 437, 311, 516], "temperature": 0.0, "avg_logprob": -0.12204678399222238, "compression_ratio": 1.5197740112994351, "no_speech_prob": 5.093635536468355e-06}, {"id": 177, "seek": 88862, "start": 895.16, "end": 896.16, "text": " on.", "tokens": [322, 13], "temperature": 0.0, "avg_logprob": -0.12204678399222238, "compression_ratio": 1.5197740112994351, "no_speech_prob": 5.093635536468355e-06}, {"id": 178, "seek": 88862, "start": 896.16, "end": 904.5600000000001, "text": " So we start with some vector input representing the embeddings of the previous words in a", "tokens": [407, 321, 722, 365, 512, 8062, 4846, 13460, 264, 12240, 29432, 295, 264, 3894, 2283, 294, 257], "temperature": 0.0, "avg_logprob": -0.12204678399222238, "compression_ratio": 1.5197740112994351, "no_speech_prob": 5.093635536468355e-06}, {"id": 179, "seek": 88862, "start": 904.5600000000001, "end": 915.6800000000001, "text": " sentence and the goal is that we then want to take that and we want to spit out of it", "tokens": [8174, 293, 264, 3387, 307, 300, 321, 550, 528, 281, 747, 300, 293, 321, 528, 281, 22127, 484, 295, 309], "temperature": 0.0, "avg_logprob": -0.12204678399222238, "compression_ratio": 1.5197740112994351, "no_speech_prob": 5.093635536468355e-06}, {"id": 180, "seek": 91568, "start": 915.68, "end": 922.88, "text": " is what is the next word going to be.", "tokens": [307, 437, 307, 264, 958, 1349, 516, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.12268951757630306, "compression_ratio": 1.5430463576158941, "no_speech_prob": 6.5403582993894815e-06}, {"id": 181, "seek": 91568, "start": 922.88, "end": 930.52, "text": " So here's all the words up to word say t-1 and this is going to be the representation", "tokens": [407, 510, 311, 439, 264, 2283, 493, 281, 1349, 584, 256, 12, 16, 293, 341, 307, 516, 281, 312, 264, 10290], "temperature": 0.0, "avg_logprob": -0.12268951757630306, "compression_ratio": 1.5430463576158941, "no_speech_prob": 6.5403582993894815e-06}, {"id": 182, "seek": 91568, "start": 930.52, "end": 933.8, "text": " as a vector of word t.", "tokens": [382, 257, 8062, 295, 1349, 256, 13], "temperature": 0.0, "avg_logprob": -0.12268951757630306, "compression_ratio": 1.5430463576158941, "no_speech_prob": 6.5403582993894815e-06}, {"id": 183, "seek": 91568, "start": 933.8, "end": 940.8399999999999, "text": " So what we do is we put all this through a neural net, specifically a recurrent neural", "tokens": [407, 437, 321, 360, 307, 321, 829, 439, 341, 807, 257, 18161, 2533, 11, 4682, 257, 18680, 1753, 18161], "temperature": 0.0, "avg_logprob": -0.12268951757630306, "compression_ratio": 1.5430463576158941, "no_speech_prob": 6.5403582993894815e-06}, {"id": 184, "seek": 94084, "start": 940.84, "end": 948.6800000000001, "text": " network, in the case of ULMFIT, and out of that recurrent neural network it's going to", "tokens": [3209, 11, 294, 264, 1389, 295, 624, 43, 44, 37, 3927, 11, 293, 484, 295, 300, 18680, 1753, 18161, 3209, 309, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.17204514741897584, "compression_ratio": 1.8181818181818181, "no_speech_prob": 2.36878850046196e-06}, {"id": 185, "seek": 94084, "start": 948.6800000000001, "end": 953.64, "text": " spit out a vector of activations.", "tokens": [22127, 484, 257, 8062, 295, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.17204514741897584, "compression_ratio": 1.8181818181818181, "no_speech_prob": 2.36878850046196e-06}, {"id": 186, "seek": 94084, "start": 953.64, "end": 955.6800000000001, "text": " That's what neural networks do.", "tokens": [663, 311, 437, 18161, 9590, 360, 13], "temperature": 0.0, "avg_logprob": -0.17204514741897584, "compression_ratio": 1.8181818181818181, "no_speech_prob": 2.36878850046196e-06}, {"id": 187, "seek": 94084, "start": 955.6800000000001, "end": 958.84, "text": " They spit out a vector of activations.", "tokens": [814, 22127, 484, 257, 8062, 295, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.17204514741897584, "compression_ratio": 1.8181818181818181, "no_speech_prob": 2.36878850046196e-06}, {"id": 188, "seek": 94084, "start": 958.84, "end": 964.96, "text": " And so what we're hoping is that we can train the recurrent neural network to spit out a", "tokens": [400, 370, 437, 321, 434, 7159, 307, 300, 321, 393, 3847, 264, 18680, 1753, 18161, 3209, 281, 22127, 484, 257], "temperature": 0.0, "avg_logprob": -0.17204514741897584, "compression_ratio": 1.8181818181818181, "no_speech_prob": 2.36878850046196e-06}, {"id": 189, "seek": 96496, "start": 964.96, "end": 975.34, "text": " vector of activations that represents the meaning and structure of the input sentence.", "tokens": [8062, 295, 2430, 763, 300, 8855, 264, 3620, 293, 3877, 295, 264, 4846, 8174, 13], "temperature": 0.0, "avg_logprob": -0.10732882709826454, "compression_ratio": 1.675, "no_speech_prob": 1.3081699989925255e-06}, {"id": 190, "seek": 96496, "start": 975.34, "end": 986.64, "text": " Because if we can, we could then have another little model here which takes as input a vector", "tokens": [1436, 498, 321, 393, 11, 321, 727, 550, 362, 1071, 707, 2316, 510, 597, 2516, 382, 4846, 257, 8062], "temperature": 0.0, "avg_logprob": -0.10732882709826454, "compression_ratio": 1.675, "no_speech_prob": 1.3081699989925255e-06}, {"id": 191, "seek": 96496, "start": 986.64, "end": 993.2800000000001, "text": " representing the structure of a sentence so far and spits out as output a prediction of", "tokens": [13460, 264, 3877, 295, 257, 8174, 370, 1400, 293, 637, 1208, 484, 382, 5598, 257, 17630, 295], "temperature": 0.0, "avg_logprob": -0.10732882709826454, "compression_ratio": 1.675, "no_speech_prob": 1.3081699989925255e-06}, {"id": 192, "seek": 99328, "start": 993.28, "end": 995.78, "text": " what the next word is.", "tokens": [437, 264, 958, 1349, 307, 13], "temperature": 0.0, "avg_logprob": -0.1144149428919742, "compression_ratio": 1.2476190476190476, "no_speech_prob": 4.7108410399232525e-06}, {"id": 193, "seek": 99328, "start": 995.78, "end": 1004.4399999999999, "text": " So you can think of this model that we built, which is a language model learner, as consisting", "tokens": [407, 291, 393, 519, 295, 341, 2316, 300, 321, 3094, 11, 597, 307, 257, 2856, 2316, 33347, 11, 382, 33921], "temperature": 0.0, "avg_logprob": -0.1144149428919742, "compression_ratio": 1.2476190476190476, "no_speech_prob": 4.7108410399232525e-06}, {"id": 194, "seek": 99328, "start": 1004.4399999999999, "end": 1012.56, "text": " of two parts.", "tokens": [295, 732, 3166, 13], "temperature": 0.0, "avg_logprob": -0.1144149428919742, "compression_ratio": 1.2476190476190476, "no_speech_prob": 4.7108410399232525e-06}, {"id": 195, "seek": 101256, "start": 1012.56, "end": 1024.8, "text": " There's the part that takes the sentence so far as input and spits out a representation", "tokens": [821, 311, 264, 644, 300, 2516, 264, 8174, 370, 1400, 382, 4846, 293, 637, 1208, 484, 257, 10290], "temperature": 0.0, "avg_logprob": -0.08718584523056493, "compression_ratio": 1.872340425531915, "no_speech_prob": 5.507411515282001e-06}, {"id": 196, "seek": 101256, "start": 1024.8, "end": 1029.6799999999998, "text": " of that sentence so far, and there's a bit that takes the representation of the sentence", "tokens": [295, 300, 8174, 370, 1400, 11, 293, 456, 311, 257, 857, 300, 2516, 264, 10290, 295, 264, 8174], "temperature": 0.0, "avg_logprob": -0.08718584523056493, "compression_ratio": 1.872340425531915, "no_speech_prob": 5.507411515282001e-06}, {"id": 197, "seek": 101256, "start": 1029.6799999999998, "end": 1037.32, "text": " so far and turns it into a prediction of the next word.", "tokens": [370, 1400, 293, 4523, 309, 666, 257, 17630, 295, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.08718584523056493, "compression_ratio": 1.872340425531915, "no_speech_prob": 5.507411515282001e-06}, {"id": 198, "seek": 101256, "start": 1037.32, "end": 1039.8799999999999, "text": " And so these things have names.", "tokens": [400, 370, 613, 721, 362, 5288, 13], "temperature": 0.0, "avg_logprob": -0.08718584523056493, "compression_ratio": 1.872340425531915, "no_speech_prob": 5.507411515282001e-06}, {"id": 199, "seek": 103988, "start": 1039.88, "end": 1045.96, "text": " The thing that takes the input sentence and spits out a representation of it is called", "tokens": [440, 551, 300, 2516, 264, 4846, 8174, 293, 637, 1208, 484, 257, 10290, 295, 309, 307, 1219], "temperature": 0.0, "avg_logprob": -0.08936514173235212, "compression_ratio": 1.699248120300752, "no_speech_prob": 3.2887242014112417e-06}, {"id": 200, "seek": 103988, "start": 1045.96, "end": 1053.48, "text": " the encoder.", "tokens": [264, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.08936514173235212, "compression_ratio": 1.699248120300752, "no_speech_prob": 3.2887242014112417e-06}, {"id": 201, "seek": 103988, "start": 1053.48, "end": 1059.1200000000001, "text": " And the thing which takes the encoded representation and turns it into a prediction of the next", "tokens": [400, 264, 551, 597, 2516, 264, 2058, 12340, 10290, 293, 4523, 309, 666, 257, 17630, 295, 264, 958], "temperature": 0.0, "avg_logprob": -0.08936514173235212, "compression_ratio": 1.699248120300752, "no_speech_prob": 3.2887242014112417e-06}, {"id": 202, "seek": 103988, "start": 1059.1200000000001, "end": 1066.3600000000001, "text": " word is called the classifier.", "tokens": [1349, 307, 1219, 264, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.08936514173235212, "compression_ratio": 1.699248120300752, "no_speech_prob": 3.2887242014112417e-06}, {"id": 203, "seek": 106636, "start": 1066.36, "end": 1075.1599999999999, "text": " And so what we're going to be doing for fine-tuning, because we don't actually want a language", "tokens": [400, 370, 437, 321, 434, 516, 281, 312, 884, 337, 2489, 12, 83, 37726, 11, 570, 321, 500, 380, 767, 528, 257, 2856], "temperature": 0.0, "avg_logprob": -0.10157432763472847, "compression_ratio": 1.8229166666666667, "no_speech_prob": 2.368783725614776e-06}, {"id": 204, "seek": 106636, "start": 1075.1599999999999, "end": 1079.8, "text": " model, we want to end up with a sentiment classification model, is that when we get", "tokens": [2316, 11, 321, 528, 281, 917, 493, 365, 257, 16149, 21538, 2316, 11, 307, 300, 562, 321, 483], "temperature": 0.0, "avg_logprob": -0.10157432763472847, "compression_ratio": 1.8229166666666667, "no_speech_prob": 2.368783725614776e-06}, {"id": 205, "seek": 106636, "start": 1079.8, "end": 1087.8, "text": " to the next stage where we're going to train the classifier, train the sentiment classifier,", "tokens": [281, 264, 958, 3233, 689, 321, 434, 516, 281, 3847, 264, 1508, 9902, 11, 3847, 264, 16149, 1508, 9902, 11], "temperature": 0.0, "avg_logprob": -0.10157432763472847, "compression_ratio": 1.8229166666666667, "no_speech_prob": 2.368783725614776e-06}, {"id": 206, "seek": 106636, "start": 1087.8, "end": 1090.76, "text": " is we don't care about this bit.", "tokens": [307, 321, 500, 380, 1127, 466, 341, 857, 13], "temperature": 0.0, "avg_logprob": -0.10157432763472847, "compression_ratio": 1.8229166666666667, "no_speech_prob": 2.368783725614776e-06}, {"id": 207, "seek": 106636, "start": 1090.76, "end": 1093.0, "text": " We don't care about predicting the next word.", "tokens": [492, 500, 380, 1127, 466, 32884, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.10157432763472847, "compression_ratio": 1.8229166666666667, "no_speech_prob": 2.368783725614776e-06}, {"id": 208, "seek": 109300, "start": 1093.0, "end": 1101.0, "text": " So we want something that only has this bit.", "tokens": [407, 321, 528, 746, 300, 787, 575, 341, 857, 13], "temperature": 0.0, "avg_logprob": -0.12547491490840912, "compression_ratio": 1.490566037735849, "no_speech_prob": 1.5056901929710875e-06}, {"id": 209, "seek": 109300, "start": 1101.0, "end": 1109.32, "text": " So the model has both bits in, but there's a function called saveEncoder which just saves", "tokens": [407, 264, 2316, 575, 1293, 9239, 294, 11, 457, 456, 311, 257, 2445, 1219, 3155, 16257, 66, 19866, 597, 445, 19155], "temperature": 0.0, "avg_logprob": -0.12547491490840912, "compression_ratio": 1.490566037735849, "no_speech_prob": 1.5056901929710875e-06}, {"id": 210, "seek": 109300, "start": 1109.32, "end": 1111.76, "text": " that bit.", "tokens": [300, 857, 13], "temperature": 0.0, "avg_logprob": -0.12547491490840912, "compression_ratio": 1.490566037735849, "no_speech_prob": 1.5056901929710875e-06}, {"id": 211, "seek": 109300, "start": 1111.76, "end": 1119.56, "text": " And so later on, after we create a text classifier learner, something we can use to classify", "tokens": [400, 370, 1780, 322, 11, 934, 321, 1884, 257, 2487, 1508, 9902, 33347, 11, 746, 321, 393, 764, 281, 33872], "temperature": 0.0, "avg_logprob": -0.12547491490840912, "compression_ratio": 1.490566037735849, "no_speech_prob": 1.5056901929710875e-06}, {"id": 212, "seek": 111956, "start": 1119.56, "end": 1127.0, "text": " text rather than predict the next word of a sentence, we then use loadEncoder to just", "tokens": [2487, 2831, 813, 6069, 264, 958, 1349, 295, 257, 8174, 11, 321, 550, 764, 3677, 16257, 66, 19866, 281, 445], "temperature": 0.0, "avg_logprob": -0.09346069608415876, "compression_ratio": 1.4775280898876404, "no_speech_prob": 2.6995715529665176e-07}, {"id": 213, "seek": 111956, "start": 1127.0, "end": 1136.2, "text": " take that encoder and put it into our new model.", "tokens": [747, 300, 2058, 19866, 293, 829, 309, 666, 527, 777, 2316, 13], "temperature": 0.0, "avg_logprob": -0.09346069608415876, "compression_ratio": 1.4775280898876404, "no_speech_prob": 2.6995715529665176e-07}, {"id": 214, "seek": 111956, "start": 1136.2, "end": 1137.72, "text": " So that's what's going on there.", "tokens": [407, 300, 311, 437, 311, 516, 322, 456, 13], "temperature": 0.0, "avg_logprob": -0.09346069608415876, "compression_ratio": 1.4775280898876404, "no_speech_prob": 2.6995715529665176e-07}, {"id": 215, "seek": 111956, "start": 1137.72, "end": 1144.3999999999999, "text": " And so when you get to neural machine translation, you'll be keeping something that looks very,", "tokens": [400, 370, 562, 291, 483, 281, 18161, 3479, 12853, 11, 291, 603, 312, 5145, 746, 300, 1542, 588, 11], "temperature": 0.0, "avg_logprob": -0.09346069608415876, "compression_ratio": 1.4775280898876404, "no_speech_prob": 2.6995715529665176e-07}, {"id": 216, "seek": 114440, "start": 1144.4, "end": 1151.0800000000002, "text": " very similar for the encoder, but instead of a classifier, you'll be creating something", "tokens": [588, 2531, 337, 264, 2058, 19866, 11, 457, 2602, 295, 257, 1508, 9902, 11, 291, 603, 312, 4084, 746], "temperature": 0.0, "avg_logprob": -0.11722241401672363, "compression_ratio": 1.6812227074235808, "no_speech_prob": 2.0904469693050487e-06}, {"id": 217, "seek": 114440, "start": 1151.0800000000002, "end": 1156.6000000000001, "text": " called a decoder, which is kind of an encoder in reverse that will actually spit out not", "tokens": [1219, 257, 979, 19866, 11, 597, 307, 733, 295, 364, 2058, 19866, 294, 9943, 300, 486, 767, 22127, 484, 406], "temperature": 0.0, "avg_logprob": -0.11722241401672363, "compression_ratio": 1.6812227074235808, "no_speech_prob": 2.0904469693050487e-06}, {"id": 218, "seek": 114440, "start": 1156.6000000000001, "end": 1161.0800000000002, "text": " just a single word, but an entire translated sentence.", "tokens": [445, 257, 2167, 1349, 11, 457, 364, 2302, 16805, 8174, 13], "temperature": 0.0, "avg_logprob": -0.11722241401672363, "compression_ratio": 1.6812227074235808, "no_speech_prob": 2.0904469693050487e-06}, {"id": 219, "seek": 114440, "start": 1161.0800000000002, "end": 1165.72, "text": " So that's the thing that you'll be doing later with Rachel.", "tokens": [407, 300, 311, 264, 551, 300, 291, 603, 312, 884, 1780, 365, 14246, 13], "temperature": 0.0, "avg_logprob": -0.11722241401672363, "compression_ratio": 1.6812227074235808, "no_speech_prob": 2.0904469693050487e-06}, {"id": 220, "seek": 114440, "start": 1165.72, "end": 1172.92, "text": " And that kind of model is called a sequence-to-sequence model, often just written as sec2sec.", "tokens": [400, 300, 733, 295, 2316, 307, 1219, 257, 8310, 12, 1353, 12, 11834, 655, 2316, 11, 2049, 445, 3720, 382, 907, 17, 8159, 13], "temperature": 0.0, "avg_logprob": -0.11722241401672363, "compression_ratio": 1.6812227074235808, "no_speech_prob": 2.0904469693050487e-06}, {"id": 221, "seek": 117292, "start": 1172.92, "end": 1180.76, "text": " Can you turn those ones on?", "tokens": [1664, 291, 1261, 729, 2306, 322, 30], "temperature": 0.0, "avg_logprob": -0.5199926116249778, "compression_ratio": 1.0153846153846153, "no_speech_prob": 2.9311027901712805e-05}, {"id": 222, "seek": 117292, "start": 1180.76, "end": 1197.48, "text": " Maybe just turn them off and on again.", "tokens": [2704, 445, 1261, 552, 766, 293, 322, 797, 13], "temperature": 0.0, "avg_logprob": -0.5199926116249778, "compression_ratio": 1.0153846153846153, "no_speech_prob": 2.9311027901712805e-05}, {"id": 223, "seek": 119748, "start": 1197.48, "end": 1203.72, "text": " So we train this for longer, for 10 epochs, to get a more accurate language model.", "tokens": [407, 321, 3847, 341, 337, 2854, 11, 337, 1266, 30992, 28346, 11, 281, 483, 257, 544, 8559, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.22785246050035632, "compression_ratio": 1.4180790960451977, "no_speech_prob": 1.2029499885102268e-05}, {"id": 224, "seek": 119748, "start": 1203.72, "end": 1214.2, "text": " And then the rest of this is... oh, there was one other change we made, which is here.", "tokens": [400, 550, 264, 1472, 295, 341, 307, 1097, 1954, 11, 456, 390, 472, 661, 1319, 321, 1027, 11, 597, 307, 510, 13], "temperature": 0.0, "avg_logprob": -0.22785246050035632, "compression_ratio": 1.4180790960451977, "no_speech_prob": 1.2029499885102268e-05}, {"id": 225, "seek": 119748, "start": 1214.2, "end": 1216.14, "text": " What does dropmult do?", "tokens": [708, 775, 3270, 76, 723, 360, 30], "temperature": 0.0, "avg_logprob": -0.22785246050035632, "compression_ratio": 1.4180790960451977, "no_speech_prob": 1.2029499885102268e-05}, {"id": 226, "seek": 119748, "start": 1216.14, "end": 1220.8, "text": " Have you learnt about dropout already?", "tokens": [3560, 291, 18991, 466, 3270, 346, 1217, 30], "temperature": 0.0, "avg_logprob": -0.22785246050035632, "compression_ratio": 1.4180790960451977, "no_speech_prob": 1.2029499885102268e-05}, {"id": 227, "seek": 119748, "start": 1220.8, "end": 1224.3600000000001, "text": " So quick refresher.", "tokens": [407, 1702, 17368, 511, 13], "temperature": 0.0, "avg_logprob": -0.22785246050035632, "compression_ratio": 1.4180790960451977, "no_speech_prob": 1.2029499885102268e-05}, {"id": 228, "seek": 122436, "start": 1224.36, "end": 1238.8, "text": " Dropout is the thing where we have some activations, like here, and what we do is at random we", "tokens": [17675, 346, 307, 264, 551, 689, 321, 362, 512, 2430, 763, 11, 411, 510, 11, 293, 437, 321, 360, 307, 412, 4974, 321], "temperature": 0.0, "avg_logprob": -0.1594451665878296, "compression_ratio": 1.5165562913907285, "no_speech_prob": 1.4510286746372003e-05}, {"id": 229, "seek": 122436, "start": 1238.8, "end": 1243.4799999999998, "text": " delete some of them, say half.", "tokens": [12097, 512, 295, 552, 11, 584, 1922, 13], "temperature": 0.0, "avg_logprob": -0.1594451665878296, "compression_ratio": 1.5165562913907285, "no_speech_prob": 1.4510286746372003e-05}, {"id": 230, "seek": 122436, "start": 1243.4799999999998, "end": 1246.52, "text": " We delete a different bunch each time through.", "tokens": [492, 12097, 257, 819, 3840, 1184, 565, 807, 13], "temperature": 0.0, "avg_logprob": -0.1594451665878296, "compression_ratio": 1.5165562913907285, "no_speech_prob": 1.4510286746372003e-05}, {"id": 231, "seek": 122436, "start": 1246.52, "end": 1250.24, "text": " We can do that on any layer or every layer of the model.", "tokens": [492, 393, 360, 300, 322, 604, 4583, 420, 633, 4583, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1594451665878296, "compression_ratio": 1.5165562913907285, "no_speech_prob": 1.4510286746372003e-05}, {"id": 232, "seek": 125024, "start": 1250.24, "end": 1256.52, "text": " What that does is it allows the model to generalize better because it can't learn to have a single", "tokens": [708, 300, 775, 307, 309, 4045, 264, 2316, 281, 2674, 1125, 1101, 570, 309, 393, 380, 1466, 281, 362, 257, 2167], "temperature": 0.0, "avg_logprob": -0.14822684543233522, "compression_ratio": 1.4480874316939891, "no_speech_prob": 3.966926215070998e-06}, {"id": 233, "seek": 125024, "start": 1256.52, "end": 1259.2, "text": " activation do a single thing.", "tokens": [24433, 360, 257, 2167, 551, 13], "temperature": 0.0, "avg_logprob": -0.14822684543233522, "compression_ratio": 1.4480874316939891, "no_speech_prob": 3.966926215070998e-06}, {"id": 234, "seek": 125024, "start": 1259.2, "end": 1264.96, "text": " It has to find a more general solution.", "tokens": [467, 575, 281, 915, 257, 544, 2674, 3827, 13], "temperature": 0.0, "avg_logprob": -0.14822684543233522, "compression_ratio": 1.4480874316939891, "no_speech_prob": 3.966926215070998e-06}, {"id": 235, "seek": 125024, "start": 1264.96, "end": 1272.92, "text": " The particular kind of recurrent neural network that we're using for this is called an AWD", "tokens": [440, 1729, 733, 295, 18680, 1753, 18161, 3209, 300, 321, 434, 1228, 337, 341, 307, 1219, 364, 25815, 35], "temperature": 0.0, "avg_logprob": -0.14822684543233522, "compression_ratio": 1.4480874316939891, "no_speech_prob": 3.966926215070998e-06}, {"id": 236, "seek": 125024, "start": 1272.92, "end": 1274.24, "text": " LSTM.", "tokens": [441, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.14822684543233522, "compression_ratio": 1.4480874316939891, "no_speech_prob": 3.966926215070998e-06}, {"id": 237, "seek": 127424, "start": 1274.24, "end": 1281.84, "text": " An AWD LSTM is just a regular recurrent neural network of the kind that Rachel will be telling", "tokens": [1107, 25815, 35, 441, 6840, 44, 307, 445, 257, 3890, 18680, 1753, 18161, 3209, 295, 264, 733, 300, 14246, 486, 312, 3585], "temperature": 0.0, "avg_logprob": -0.1636318842569987, "compression_ratio": 1.438423645320197, "no_speech_prob": 6.083552648306068e-07}, {"id": 238, "seek": 127424, "start": 1281.84, "end": 1283.8, "text": " you about.", "tokens": [291, 466, 13], "temperature": 0.0, "avg_logprob": -0.1636318842569987, "compression_ratio": 1.438423645320197, "no_speech_prob": 6.083552648306068e-07}, {"id": 239, "seek": 127424, "start": 1283.8, "end": 1291.6, "text": " But the researchers who built it in this paper called Regularizing and Optimizing LSTM Language", "tokens": [583, 264, 10309, 567, 3094, 309, 294, 341, 3035, 1219, 45659, 3319, 293, 35013, 3319, 441, 6840, 44, 24445], "temperature": 0.0, "avg_logprob": -0.1636318842569987, "compression_ratio": 1.438423645320197, "no_speech_prob": 6.083552648306068e-07}, {"id": 240, "seek": 127424, "start": 1291.6, "end": 1300.72, "text": " Models, the lead author, a guy called Stephen Meridy, he thought of lots of different ways", "tokens": [6583, 1625, 11, 264, 1477, 3793, 11, 257, 2146, 1219, 13391, 6124, 38836, 11, 415, 1194, 295, 3195, 295, 819, 2098], "temperature": 0.0, "avg_logprob": -0.1636318842569987, "compression_ratio": 1.438423645320197, "no_speech_prob": 6.083552648306068e-07}, {"id": 241, "seek": 130072, "start": 1300.72, "end": 1307.2, "text": " to add dropout in many different parts of the model.", "tokens": [281, 909, 3270, 346, 294, 867, 819, 3166, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11545166858406954, "compression_ratio": 1.6231155778894473, "no_speech_prob": 7.88913712312933e-06}, {"id": 242, "seek": 130072, "start": 1307.2, "end": 1313.68, "text": " So it basically allows us to add a lot more regularization of a lot more different types.", "tokens": [407, 309, 1936, 4045, 505, 281, 909, 257, 688, 544, 3890, 2144, 295, 257, 688, 544, 819, 3467, 13], "temperature": 0.0, "avg_logprob": -0.11545166858406954, "compression_ratio": 1.6231155778894473, "no_speech_prob": 7.88913712312933e-06}, {"id": 243, "seek": 130072, "start": 1313.68, "end": 1319.52, "text": " There's actually five kinds of dropout in an AWD LSTM.", "tokens": [821, 311, 767, 1732, 3685, 295, 3270, 346, 294, 364, 25815, 35, 441, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.11545166858406954, "compression_ratio": 1.6231155778894473, "no_speech_prob": 7.88913712312933e-06}, {"id": 244, "seek": 130072, "start": 1319.52, "end": 1325.26, "text": " It would be very annoying to have to try to find five different hyperparameters of how", "tokens": [467, 576, 312, 588, 11304, 281, 362, 281, 853, 281, 915, 1732, 819, 9848, 2181, 335, 6202, 295, 577], "temperature": 0.0, "avg_logprob": -0.11545166858406954, "compression_ratio": 1.6231155778894473, "no_speech_prob": 7.88913712312933e-06}, {"id": 245, "seek": 130072, "start": 1325.26, "end": 1328.88, "text": " much dropout for every different type.", "tokens": [709, 3270, 346, 337, 633, 819, 2010, 13], "temperature": 0.0, "avg_logprob": -0.11545166858406954, "compression_ratio": 1.6231155778894473, "no_speech_prob": 7.88913712312933e-06}, {"id": 246, "seek": 132888, "start": 1328.88, "end": 1335.48, "text": " And in their paper and in their recommendations in their GitHub repo, they actually have suggestions", "tokens": [400, 294, 641, 3035, 293, 294, 641, 10434, 294, 641, 23331, 49040, 11, 436, 767, 362, 13396], "temperature": 0.0, "avg_logprob": -0.15051889419555664, "compression_ratio": 1.644736842105263, "no_speech_prob": 8.530216291546822e-06}, {"id": 247, "seek": 132888, "start": 1335.48, "end": 1343.3600000000001, "text": " for specific amounts of dropout that they found work well for various different problems.", "tokens": [337, 2685, 11663, 295, 3270, 346, 300, 436, 1352, 589, 731, 337, 3683, 819, 2740, 13], "temperature": 0.0, "avg_logprob": -0.15051889419555664, "compression_ratio": 1.644736842105263, "no_speech_prob": 8.530216291546822e-06}, {"id": 248, "seek": 132888, "start": 1343.3600000000001, "end": 1350.22, "text": " What we do in Fast AI is we just have a standard set amount of dropout for each of those five", "tokens": [708, 321, 360, 294, 15968, 7318, 307, 321, 445, 362, 257, 3832, 992, 2372, 295, 3270, 346, 337, 1184, 295, 729, 1732], "temperature": 0.0, "avg_logprob": -0.15051889419555664, "compression_ratio": 1.644736842105263, "no_speech_prob": 8.530216291546822e-06}, {"id": 249, "seek": 132888, "start": 1350.22, "end": 1356.8000000000002, "text": " things and then you just pick a single number and to say multiply them all by this number.", "tokens": [721, 293, 550, 291, 445, 1888, 257, 2167, 1230, 293, 281, 584, 12972, 552, 439, 538, 341, 1230, 13], "temperature": 0.0, "avg_logprob": -0.15051889419555664, "compression_ratio": 1.644736842105263, "no_speech_prob": 8.530216291546822e-06}, {"id": 250, "seek": 135680, "start": 1356.8, "end": 1362.6, "text": " So to use our default recommendations, you would just use one, which means it's actually", "tokens": [407, 281, 764, 527, 7576, 10434, 11, 291, 576, 445, 764, 472, 11, 597, 1355, 309, 311, 767], "temperature": 0.0, "avg_logprob": -0.13137524178687562, "compression_ratio": 1.7659574468085106, "no_speech_prob": 5.422188223747071e-06}, {"id": 251, "seek": 135680, "start": 1362.6, "end": 1365.52, "text": " quite a lot of dropout if you put one.", "tokens": [1596, 257, 688, 295, 3270, 346, 498, 291, 829, 472, 13], "temperature": 0.0, "avg_logprob": -0.13137524178687562, "compression_ratio": 1.7659574468085106, "no_speech_prob": 5.422188223747071e-06}, {"id": 252, "seek": 135680, "start": 1365.52, "end": 1370.78, "text": " Or if you wanted to use a tenth as much, you would put 0.1, for example.", "tokens": [1610, 498, 291, 1415, 281, 764, 257, 27269, 382, 709, 11, 291, 576, 829, 1958, 13, 16, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.13137524178687562, "compression_ratio": 1.7659574468085106, "no_speech_prob": 5.422188223747071e-06}, {"id": 253, "seek": 135680, "start": 1370.78, "end": 1377.3, "text": " So if you're overfitting, you would generally increase dropmult, and if you're underfitting,", "tokens": [407, 498, 291, 434, 670, 69, 2414, 11, 291, 576, 5101, 3488, 3270, 76, 723, 11, 293, 498, 291, 434, 833, 69, 2414, 11], "temperature": 0.0, "avg_logprob": -0.13137524178687562, "compression_ratio": 1.7659574468085106, "no_speech_prob": 5.422188223747071e-06}, {"id": 254, "seek": 135680, "start": 1377.3, "end": 1380.6, "text": " you would generally decrease dropmult.", "tokens": [291, 576, 5101, 11514, 3270, 76, 723, 13], "temperature": 0.0, "avg_logprob": -0.13137524178687562, "compression_ratio": 1.7659574468085106, "no_speech_prob": 5.422188223747071e-06}, {"id": 255, "seek": 138060, "start": 1380.6, "end": 1389.08, "text": " So we did some new research this week into the impact of dropmult on the sentiment analysis", "tokens": [407, 321, 630, 512, 777, 2132, 341, 1243, 666, 264, 2712, 295, 3270, 76, 723, 322, 264, 16149, 5215], "temperature": 0.0, "avg_logprob": -0.09537565402495556, "compression_ratio": 1.6237623762376239, "no_speech_prob": 2.3320594664255623e-06}, {"id": 256, "seek": 138060, "start": 1389.08, "end": 1395.8, "text": " classifier, which is something that hasn't been ever published in the research before.", "tokens": [1508, 9902, 11, 597, 307, 746, 300, 6132, 380, 668, 1562, 6572, 294, 264, 2132, 949, 13], "temperature": 0.0, "avg_logprob": -0.09537565402495556, "compression_ratio": 1.6237623762376239, "no_speech_prob": 2.3320594664255623e-06}, {"id": 257, "seek": 138060, "start": 1395.8, "end": 1400.56, "text": " The kind of assumption had been, or our assumption had been, that the thing we should try to", "tokens": [440, 733, 295, 15302, 632, 668, 11, 420, 527, 15302, 632, 668, 11, 300, 264, 551, 321, 820, 853, 281], "temperature": 0.0, "avg_logprob": -0.09537565402495556, "compression_ratio": 1.6237623762376239, "no_speech_prob": 2.3320594664255623e-06}, {"id": 258, "seek": 138060, "start": 1400.56, "end": 1404.48, "text": " do is to create the most accurate language model we can.", "tokens": [360, 307, 281, 1884, 264, 881, 8559, 2856, 2316, 321, 393, 13], "temperature": 0.0, "avg_logprob": -0.09537565402495556, "compression_ratio": 1.6237623762376239, "no_speech_prob": 2.3320594664255623e-06}, {"id": 259, "seek": 140448, "start": 1404.48, "end": 1410.96, "text": " And we found that having quite a small amount of dropout gave us a more accurate language", "tokens": [400, 321, 1352, 300, 1419, 1596, 257, 1359, 2372, 295, 3270, 346, 2729, 505, 257, 544, 8559, 2856], "temperature": 0.0, "avg_logprob": -0.12751453153548703, "compression_ratio": 1.6088888888888888, "no_speech_prob": 2.81300708593335e-06}, {"id": 260, "seek": 140448, "start": 1410.96, "end": 1411.96, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.12751453153548703, "compression_ratio": 1.6088888888888888, "no_speech_prob": 2.81300708593335e-06}, {"id": 261, "seek": 140448, "start": 1411.96, "end": 1418.84, "text": " But then this week, I tried doing some experiments where I actually increased the amount of regularization.", "tokens": [583, 550, 341, 1243, 11, 286, 3031, 884, 512, 12050, 689, 286, 767, 6505, 264, 2372, 295, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.12751453153548703, "compression_ratio": 1.6088888888888888, "no_speech_prob": 2.81300708593335e-06}, {"id": 262, "seek": 140448, "start": 1418.84, "end": 1422.04, "text": " So I increased dropmult.", "tokens": [407, 286, 6505, 3270, 76, 723, 13], "temperature": 0.0, "avg_logprob": -0.12751453153548703, "compression_ratio": 1.6088888888888888, "no_speech_prob": 2.81300708593335e-06}, {"id": 263, "seek": 140448, "start": 1422.04, "end": 1428.72, "text": " The accuracy of the language model was worse, but I went on to train, fine-tune that, to", "tokens": [440, 14170, 295, 264, 2856, 2316, 390, 5324, 11, 457, 286, 1437, 322, 281, 3847, 11, 2489, 12, 83, 2613, 300, 11, 281], "temperature": 0.0, "avg_logprob": -0.12751453153548703, "compression_ratio": 1.6088888888888888, "no_speech_prob": 2.81300708593335e-06}, {"id": 264, "seek": 140448, "start": 1428.72, "end": 1432.52, "text": " transfer alone that to a classifier anyway.", "tokens": [5003, 3312, 300, 281, 257, 1508, 9902, 4033, 13], "temperature": 0.0, "avg_logprob": -0.12751453153548703, "compression_ratio": 1.6088888888888888, "no_speech_prob": 2.81300708593335e-06}, {"id": 265, "seek": 143252, "start": 1432.52, "end": 1439.76, "text": " And what we discovered, to our surprise, was that language models with more regularization,", "tokens": [400, 437, 321, 6941, 11, 281, 527, 6365, 11, 390, 300, 2856, 5245, 365, 544, 3890, 2144, 11], "temperature": 0.0, "avg_logprob": -0.12996818754408096, "compression_ratio": 1.55793991416309, "no_speech_prob": 1.2679257679337752e-06}, {"id": 266, "seek": 143252, "start": 1439.76, "end": 1444.52, "text": " even though they were a bit less accurate at predicting the next word of a sentence,", "tokens": [754, 1673, 436, 645, 257, 857, 1570, 8559, 412, 32884, 264, 958, 1349, 295, 257, 8174, 11], "temperature": 0.0, "avg_logprob": -0.12996818754408096, "compression_ratio": 1.55793991416309, "no_speech_prob": 1.2679257679337752e-06}, {"id": 267, "seek": 143252, "start": 1444.52, "end": 1450.68, "text": " turned out to be more accurate classifiers when we actually got to the sentiment analysis", "tokens": [3574, 484, 281, 312, 544, 8559, 1508, 23463, 562, 321, 767, 658, 281, 264, 16149, 5215], "temperature": 0.0, "avg_logprob": -0.12996818754408096, "compression_ratio": 1.55793991416309, "no_speech_prob": 1.2679257679337752e-06}, {"id": 268, "seek": 143252, "start": 1450.68, "end": 1452.76, "text": " stage.", "tokens": [3233, 13], "temperature": 0.0, "avg_logprob": -0.12996818754408096, "compression_ratio": 1.55793991416309, "no_speech_prob": 1.2679257679337752e-06}, {"id": 269, "seek": 143252, "start": 1452.76, "end": 1459.16, "text": " And so, as I said, this is all new unpublished research, so it's a little bit early days,", "tokens": [400, 370, 11, 382, 286, 848, 11, 341, 307, 439, 777, 20994, 836, 4173, 2132, 11, 370, 309, 311, 257, 707, 857, 2440, 1708, 11], "temperature": 0.0, "avg_logprob": -0.12996818754408096, "compression_ratio": 1.55793991416309, "no_speech_prob": 1.2679257679337752e-06}, {"id": 270, "seek": 145916, "start": 1459.16, "end": 1462.8400000000001, "text": " but we've tried quite a few different data sets and quite a few different hyperparameters", "tokens": [457, 321, 600, 3031, 1596, 257, 1326, 819, 1412, 6352, 293, 1596, 257, 1326, 819, 9848, 2181, 335, 6202], "temperature": 0.0, "avg_logprob": -0.10068132959563157, "compression_ratio": 1.7718631178707225, "no_speech_prob": 6.854221737739863e-06}, {"id": 271, "seek": 145916, "start": 1462.8400000000001, "end": 1465.44, "text": " and we are consistently finding this.", "tokens": [293, 321, 366, 14961, 5006, 341, 13], "temperature": 0.0, "avg_logprob": -0.10068132959563157, "compression_ratio": 1.7718631178707225, "no_speech_prob": 6.854221737739863e-06}, {"id": 272, "seek": 145916, "start": 1465.44, "end": 1471.8000000000002, "text": " The other thing we found is that when we use more regularization in the language model,", "tokens": [440, 661, 551, 321, 1352, 307, 300, 562, 321, 764, 544, 3890, 2144, 294, 264, 2856, 2316, 11], "temperature": 0.0, "avg_logprob": -0.10068132959563157, "compression_ratio": 1.7718631178707225, "no_speech_prob": 6.854221737739863e-06}, {"id": 273, "seek": 145916, "start": 1471.8000000000002, "end": 1477.96, "text": " the training of the classifier is much more resilient, it's much less fiddly.", "tokens": [264, 3097, 295, 264, 1508, 9902, 307, 709, 544, 23699, 11, 309, 311, 709, 1570, 283, 14273, 356, 13], "temperature": 0.0, "avg_logprob": -0.10068132959563157, "compression_ratio": 1.7718631178707225, "no_speech_prob": 6.854221737739863e-06}, {"id": 274, "seek": 145916, "start": 1477.96, "end": 1484.28, "text": " We found with less dropout, it would very often kind of go off to infinity, the loss,", "tokens": [492, 1352, 365, 1570, 3270, 346, 11, 309, 576, 588, 2049, 733, 295, 352, 766, 281, 13202, 11, 264, 4470, 11], "temperature": 0.0, "avg_logprob": -0.10068132959563157, "compression_ratio": 1.7718631178707225, "no_speech_prob": 6.854221737739863e-06}, {"id": 275, "seek": 145916, "start": 1484.28, "end": 1487.96, "text": " you know, when we'd have to find exactly the right learning rate and exactly the right", "tokens": [291, 458, 11, 562, 321, 1116, 362, 281, 915, 2293, 264, 558, 2539, 3314, 293, 2293, 264, 558], "temperature": 0.0, "avg_logprob": -0.10068132959563157, "compression_ratio": 1.7718631178707225, "no_speech_prob": 6.854221737739863e-06}, {"id": 276, "seek": 148796, "start": 1487.96, "end": 1492.16, "text": " amount of time to train it for, which was super annoying.", "tokens": [2372, 295, 565, 281, 3847, 309, 337, 11, 597, 390, 1687, 11304, 13], "temperature": 0.0, "avg_logprob": -0.18595859433008619, "compression_ratio": 1.5907335907335907, "no_speech_prob": 1.0451397429278586e-05}, {"id": 277, "seek": 148796, "start": 1492.16, "end": 1495.0, "text": " Yes, Rachel.", "tokens": [1079, 11, 14246, 13], "temperature": 0.0, "avg_logprob": -0.18595859433008619, "compression_ratio": 1.5907335907335907, "no_speech_prob": 1.0451397429278586e-05}, {"id": 278, "seek": 148796, "start": 1495.0, "end": 1499.08, "text": " What range of values can you put in for the dropout multiplier?", "tokens": [708, 3613, 295, 4190, 393, 291, 829, 294, 337, 264, 3270, 346, 44106, 30], "temperature": 0.0, "avg_logprob": -0.18595859433008619, "compression_ratio": 1.5907335907335907, "no_speech_prob": 1.0451397429278586e-05}, {"id": 279, "seek": 148796, "start": 1499.08, "end": 1503.76, "text": " You can put in pretty much anything, I mean, obviously if it ends up with a dropout of", "tokens": [509, 393, 829, 294, 1238, 709, 1340, 11, 286, 914, 11, 2745, 498, 309, 5314, 493, 365, 257, 3270, 346, 295], "temperature": 0.0, "avg_logprob": -0.18595859433008619, "compression_ratio": 1.5907335907335907, "no_speech_prob": 1.0451397429278586e-05}, {"id": 280, "seek": 148796, "start": 1503.76, "end": 1506.48, "text": " greater than 1, that would be silly.", "tokens": [5044, 813, 502, 11, 300, 576, 312, 11774, 13], "temperature": 0.0, "avg_logprob": -0.18595859433008619, "compression_ratio": 1.5907335907335907, "no_speech_prob": 1.0451397429278586e-05}, {"id": 281, "seek": 148796, "start": 1506.48, "end": 1513.4, "text": " In practice, we use between.01 and 1, we haven't tried anything higher than that.", "tokens": [682, 3124, 11, 321, 764, 1296, 2411, 10607, 293, 502, 11, 321, 2378, 380, 3031, 1340, 2946, 813, 300, 13], "temperature": 0.0, "avg_logprob": -0.18595859433008619, "compression_ratio": 1.5907335907335907, "no_speech_prob": 1.0451397429278586e-05}, {"id": 282, "seek": 148796, "start": 1513.4, "end": 1515.04, "text": " Does this give you a dropout of 1?", "tokens": [4402, 341, 976, 291, 257, 3270, 346, 295, 502, 30], "temperature": 0.0, "avg_logprob": -0.18595859433008619, "compression_ratio": 1.5907335907335907, "no_speech_prob": 1.0451397429278586e-05}, {"id": 283, "seek": 148796, "start": 1515.04, "end": 1517.5, "text": " No, you'll never get a dropout of 1.", "tokens": [883, 11, 291, 603, 1128, 483, 257, 3270, 346, 295, 502, 13], "temperature": 0.0, "avg_logprob": -0.18595859433008619, "compression_ratio": 1.5907335907335907, "no_speech_prob": 1.0451397429278586e-05}, {"id": 284, "seek": 151750, "start": 1517.5, "end": 1525.28, "text": " This will give us by default the kind of AWD LSTM standard dropouts, which you'd have to", "tokens": [639, 486, 976, 505, 538, 7576, 264, 733, 295, 25815, 35, 441, 6840, 44, 3832, 3270, 7711, 11, 597, 291, 1116, 362, 281], "temperature": 0.0, "avg_logprob": -0.14861999413906, "compression_ratio": 1.168141592920354, "no_speech_prob": 5.6493543524993584e-05}, {"id": 285, "seek": 151750, "start": 1525.28, "end": 1527.68, "text": " check the source code to see what they are.", "tokens": [1520, 264, 4009, 3089, 281, 536, 437, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.14861999413906, "compression_ratio": 1.168141592920354, "no_speech_prob": 5.6493543524993584e-05}, {"id": 286, "seek": 152768, "start": 1527.68, "end": 1556.72, "text": " I can't remember off the top of my head.", "tokens": [286, 393, 380, 1604, 766, 264, 1192, 295, 452, 1378, 13], "temperature": 0.0, "avg_logprob": -0.23440659840901693, "compression_ratio": 0.8695652173913043, "no_speech_prob": 8.091500058071688e-05}, {"id": 287, "seek": 155672, "start": 1556.72, "end": 1562.68, "text": " Looks like it goes here.", "tokens": [10027, 411, 309, 1709, 510, 13], "temperature": 0.0, "avg_logprob": -0.6423978805541992, "compression_ratio": 0.7894736842105263, "no_speech_prob": 5.649044396704994e-05}, {"id": 288, "seek": 155672, "start": 1562.68, "end": 1569.4, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.6423978805541992, "compression_ratio": 0.7894736842105263, "no_speech_prob": 5.649044396704994e-05}, {"id": 289, "seek": 156940, "start": 1569.4, "end": 1589.76, "text": " So it comes from ModelMatter in TextMatter.", "tokens": [407, 309, 1487, 490, 17105, 44, 1161, 294, 18643, 44, 1161, 13], "temperature": 0.0, "avg_logprob": -0.5046265775507147, "compression_ratio": 0.9491525423728814, "no_speech_prob": 1.8923705283668824e-05}, {"id": 290, "seek": 156940, "start": 1589.76, "end": 1595.3600000000001, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.5046265775507147, "compression_ratio": 0.9491525423728814, "no_speech_prob": 1.8923705283668824e-05}, {"id": 291, "seek": 159536, "start": 1595.36, "end": 1603.1999999999998, "text": " So for an AWD LSTM, there's the dropouts, the 5 dropouts that we use.", "tokens": [407, 337, 364, 25815, 35, 441, 6840, 44, 11, 456, 311, 264, 3270, 7711, 11, 264, 1025, 3270, 7711, 300, 321, 764, 13], "temperature": 0.0, "avg_logprob": -0.2364852523803711, "compression_ratio": 1.5585585585585586, "no_speech_prob": 4.3999705667374656e-05}, {"id": 292, "seek": 159536, "start": 1603.1999999999998, "end": 1607.9799999999998, "text": " So if you have a dropout of 1, it deletes all of your activations, so that would be", "tokens": [407, 498, 291, 362, 257, 3270, 346, 295, 502, 11, 309, 1103, 37996, 439, 295, 428, 2430, 763, 11, 370, 300, 576, 312], "temperature": 0.0, "avg_logprob": -0.2364852523803711, "compression_ratio": 1.5585585585585586, "no_speech_prob": 4.3999705667374656e-05}, {"id": 293, "seek": 159536, "start": 1607.9799999999998, "end": 1609.36, "text": " useless.", "tokens": [14115, 13], "temperature": 0.0, "avg_logprob": -0.2364852523803711, "compression_ratio": 1.5585585585585586, "no_speech_prob": 4.3999705667374656e-05}, {"id": 294, "seek": 159536, "start": 1609.36, "end": 1615.1999999999998, "text": " So you would definitely want to drop more than 4.", "tokens": [407, 291, 576, 2138, 528, 281, 3270, 544, 813, 1017, 13], "temperature": 0.0, "avg_logprob": -0.2364852523803711, "compression_ratio": 1.5585585585585586, "no_speech_prob": 4.3999705667374656e-05}, {"id": 295, "seek": 159536, "start": 1615.1999999999998, "end": 1620.9199999999998, "text": " I think that's the answer to your question.", "tokens": [286, 519, 300, 311, 264, 1867, 281, 428, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2364852523803711, "compression_ratio": 1.5585585585585586, "no_speech_prob": 4.3999705667374656e-05}, {"id": 296, "seek": 159536, "start": 1620.9199999999998, "end": 1623.6799999999998, "text": " Obviously it would have been easier if I had Visual Studio code running before I started.", "tokens": [7580, 309, 576, 362, 668, 3571, 498, 286, 632, 23187, 13500, 3089, 2614, 949, 286, 1409, 13], "temperature": 0.0, "avg_logprob": -0.2364852523803711, "compression_ratio": 1.5585585585585586, "no_speech_prob": 4.3999705667374656e-05}, {"id": 297, "seek": 162368, "start": 1623.68, "end": 1630.0800000000002, "text": " We could have jumped straight there, but I was not organized.", "tokens": [492, 727, 362, 13864, 2997, 456, 11, 457, 286, 390, 406, 9983, 13], "temperature": 0.0, "avg_logprob": -0.1330699634552002, "compression_ratio": 1.6293436293436294, "no_speech_prob": 1.3845635294273961e-05}, {"id": 298, "seek": 162368, "start": 1630.0800000000002, "end": 1634.64, "text": " So it's been really handy this week having this new trick up our sleeve, because previously", "tokens": [407, 309, 311, 668, 534, 13239, 341, 1243, 1419, 341, 777, 4282, 493, 527, 21138, 11, 570, 8046], "temperature": 0.0, "avg_logprob": -0.1330699634552002, "compression_ratio": 1.6293436293436294, "no_speech_prob": 1.3845635294273961e-05}, {"id": 299, "seek": 162368, "start": 1634.64, "end": 1639.52, "text": " trying to get these notebooks ready for class, I had to fiddle around a lot to find a learning", "tokens": [1382, 281, 483, 613, 43782, 1919, 337, 1508, 11, 286, 632, 281, 24553, 2285, 926, 257, 688, 281, 915, 257, 2539], "temperature": 0.0, "avg_logprob": -0.1330699634552002, "compression_ratio": 1.6293436293436294, "no_speech_prob": 1.3845635294273961e-05}, {"id": 300, "seek": 162368, "start": 1639.52, "end": 1644.14, "text": " rate that allowed the classifier to train without crashing off to infinity or staying", "tokens": [3314, 300, 4350, 264, 1508, 9902, 281, 3847, 1553, 26900, 766, 281, 13202, 420, 7939], "temperature": 0.0, "avg_logprob": -0.1330699634552002, "compression_ratio": 1.6293436293436294, "no_speech_prob": 1.3845635294273961e-05}, {"id": 301, "seek": 162368, "start": 1644.14, "end": 1651.68, "text": " at 0, and now I find a wider range of numbers of epochs and learning rates and so forth", "tokens": [412, 1958, 11, 293, 586, 286, 915, 257, 11842, 3613, 295, 3547, 295, 30992, 28346, 293, 2539, 6846, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.1330699634552002, "compression_ratio": 1.6293436293436294, "no_speech_prob": 1.3845635294273961e-05}, {"id": 302, "seek": 165168, "start": 1651.68, "end": 1655.72, "text": " continue to give good results, so that's been nice.", "tokens": [2354, 281, 976, 665, 3542, 11, 370, 300, 311, 668, 1481, 13], "temperature": 0.0, "avg_logprob": -0.12879448748649436, "compression_ratio": 1.5656108597285068, "no_speech_prob": 9.665986908657942e-06}, {"id": 303, "seek": 165168, "start": 1655.72, "end": 1662.76, "text": " Back when we did the ULM fit paper, we were kind of still fiddling around a lot, a little", "tokens": [5833, 562, 321, 630, 264, 624, 43, 44, 3318, 3035, 11, 321, 645, 733, 295, 920, 283, 14273, 1688, 926, 257, 688, 11, 257, 707], "temperature": 0.0, "avg_logprob": -0.12879448748649436, "compression_ratio": 1.5656108597285068, "no_speech_prob": 9.665986908657942e-06}, {"id": 304, "seek": 165168, "start": 1662.76, "end": 1667.48, "text": " bit shooting in the dark to try to find parameters that worked well.", "tokens": [857, 5942, 294, 264, 2877, 281, 853, 281, 915, 9834, 300, 2732, 731, 13], "temperature": 0.0, "avg_logprob": -0.12879448748649436, "compression_ratio": 1.5656108597285068, "no_speech_prob": 9.665986908657942e-06}, {"id": 305, "seek": 165168, "start": 1667.48, "end": 1674.28, "text": " But my guess is that what's going on here is that when you train a language model with", "tokens": [583, 452, 2041, 307, 300, 437, 311, 516, 322, 510, 307, 300, 562, 291, 3847, 257, 2856, 2316, 365], "temperature": 0.0, "avg_logprob": -0.12879448748649436, "compression_ratio": 1.5656108597285068, "no_speech_prob": 9.665986908657942e-06}, {"id": 306, "seek": 165168, "start": 1674.28, "end": 1678.44, "text": " less regularization, it's going to overfit more.", "tokens": [1570, 3890, 2144, 11, 309, 311, 516, 281, 670, 6845, 544, 13], "temperature": 0.0, "avg_logprob": -0.12879448748649436, "compression_ratio": 1.5656108597285068, "no_speech_prob": 9.665986908657942e-06}, {"id": 307, "seek": 167844, "start": 1678.44, "end": 1685.48, "text": " It may not overfit to the point that the accuracy is getting worse, but it might overfit to", "tokens": [467, 815, 406, 670, 6845, 281, 264, 935, 300, 264, 14170, 307, 1242, 5324, 11, 457, 309, 1062, 670, 6845, 281], "temperature": 0.0, "avg_logprob": -0.11276971557993948, "compression_ratio": 1.7760416666666667, "no_speech_prob": 2.0145354937994853e-05}, {"id": 308, "seek": 167844, "start": 1685.48, "end": 1690.1200000000001, "text": " the point where the training loss is a lot smaller than the validation loss.", "tokens": [264, 935, 689, 264, 3097, 4470, 307, 257, 688, 4356, 813, 264, 24071, 4470, 13], "temperature": 0.0, "avg_logprob": -0.11276971557993948, "compression_ratio": 1.7760416666666667, "no_speech_prob": 2.0145354937994853e-05}, {"id": 309, "seek": 167844, "start": 1690.1200000000001, "end": 1695.0, "text": " Normally we don't care too much about that as long as the validation accuracy is good.", "tokens": [17424, 321, 500, 380, 1127, 886, 709, 466, 300, 382, 938, 382, 264, 24071, 14170, 307, 665, 13], "temperature": 0.0, "avg_logprob": -0.11276971557993948, "compression_ratio": 1.7760416666666667, "no_speech_prob": 2.0145354937994853e-05}, {"id": 310, "seek": 167844, "start": 1695.0, "end": 1699.04, "text": " But generally speaking, what we expect to find is that when the training loss is much", "tokens": [583, 5101, 4124, 11, 437, 321, 2066, 281, 915, 307, 300, 562, 264, 3097, 4470, 307, 709], "temperature": 0.0, "avg_logprob": -0.11276971557993948, "compression_ratio": 1.7760416666666667, "no_speech_prob": 2.0145354937994853e-05}, {"id": 311, "seek": 169904, "start": 1699.04, "end": 1708.56, "text": " smaller than the validation loss, it's probably a part of the function of the weight space", "tokens": [4356, 813, 264, 24071, 4470, 11, 309, 311, 1391, 257, 644, 295, 264, 2445, 295, 264, 3364, 1901], "temperature": 0.0, "avg_logprob": -0.18106175360278548, "compression_ratio": 1.7737556561085972, "no_speech_prob": 8.267771590908524e-06}, {"id": 312, "seek": 169904, "start": 1708.56, "end": 1709.56, "text": " that's quite pointy.", "tokens": [300, 311, 1596, 935, 88, 13], "temperature": 0.0, "avg_logprob": -0.18106175360278548, "compression_ratio": 1.7737556561085972, "no_speech_prob": 8.267771590908524e-06}, {"id": 313, "seek": 169904, "start": 1709.56, "end": 1714.72, "text": " It's kind of found a spot where on the training set it's got a good loss, on the validation", "tokens": [467, 311, 733, 295, 1352, 257, 4008, 689, 322, 264, 3097, 992, 309, 311, 658, 257, 665, 4470, 11, 322, 264, 24071], "temperature": 0.0, "avg_logprob": -0.18106175360278548, "compression_ratio": 1.7737556561085972, "no_speech_prob": 8.267771590908524e-06}, {"id": 314, "seek": 169904, "start": 1714.72, "end": 1717.72, "text": " set it's kind of a worse loss.", "tokens": [992, 309, 311, 733, 295, 257, 5324, 4470, 13], "temperature": 0.0, "avg_logprob": -0.18106175360278548, "compression_ratio": 1.7737556561085972, "no_speech_prob": 8.267771590908524e-06}, {"id": 315, "seek": 169904, "start": 1717.72, "end": 1721.32, "text": " Whereas if the two are both good, it's probably a flatter area.", "tokens": [13813, 498, 264, 732, 366, 1293, 665, 11, 309, 311, 1391, 257, 41247, 1859, 13], "temperature": 0.0, "avg_logprob": -0.18106175360278548, "compression_ratio": 1.7737556561085972, "no_speech_prob": 8.267771590908524e-06}, {"id": 316, "seek": 169904, "start": 1721.32, "end": 1726.1599999999999, "text": " So that's my guess, is that then what happens when we try to fine-tune it for classification,", "tokens": [407, 300, 311, 452, 2041, 11, 307, 300, 550, 437, 2314, 562, 321, 853, 281, 2489, 12, 83, 2613, 309, 337, 21538, 11], "temperature": 0.0, "avg_logprob": -0.18106175360278548, "compression_ratio": 1.7737556561085972, "no_speech_prob": 8.267771590908524e-06}, {"id": 317, "seek": 172616, "start": 1726.16, "end": 1730.52, "text": " if we've trained it too much with too little regularization, it's this spiky bit of the", "tokens": [498, 321, 600, 8895, 309, 886, 709, 365, 886, 707, 3890, 2144, 11, 309, 311, 341, 637, 1035, 88, 857, 295, 264], "temperature": 0.0, "avg_logprob": -0.11646603315304487, "compression_ratio": 1.5454545454545454, "no_speech_prob": 5.682389655703446e-06}, {"id": 318, "seek": 172616, "start": 1730.52, "end": 1736.0400000000002, "text": " weight space where it can easily jump out to somewhere that's not very good.", "tokens": [3364, 1901, 689, 309, 393, 3612, 3012, 484, 281, 4079, 300, 311, 406, 588, 665, 13], "temperature": 0.0, "avg_logprob": -0.11646603315304487, "compression_ratio": 1.5454545454545454, "no_speech_prob": 5.682389655703446e-06}, {"id": 319, "seek": 172616, "start": 1736.0400000000002, "end": 1743.8400000000001, "text": " That's kind of my intuitive guess about what's going on.", "tokens": [663, 311, 733, 295, 452, 21769, 2041, 466, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.11646603315304487, "compression_ratio": 1.5454545454545454, "no_speech_prob": 5.682389655703446e-06}, {"id": 320, "seek": 172616, "start": 1743.8400000000001, "end": 1751.76, "text": " So then the classifier is trained actually in exactly the same way in this notebook,", "tokens": [407, 550, 264, 1508, 9902, 307, 8895, 767, 294, 2293, 264, 912, 636, 294, 341, 21060, 11], "temperature": 0.0, "avg_logprob": -0.11646603315304487, "compression_ratio": 1.5454545454545454, "no_speech_prob": 5.682389655703446e-06}, {"id": 321, "seek": 175176, "start": 1751.76, "end": 1757.28, "text": " with the one exception that I've added, the 2FP16, just to make it a bit faster.", "tokens": [365, 264, 472, 11183, 300, 286, 600, 3869, 11, 264, 568, 45882, 6866, 11, 445, 281, 652, 309, 257, 857, 4663, 13], "temperature": 0.0, "avg_logprob": -0.12140426635742188, "compression_ratio": 1.38755980861244, "no_speech_prob": 1.1842905223602429e-05}, {"id": 322, "seek": 175176, "start": 1757.28, "end": 1766.36, "text": " And you can see what happens here is we, rather than getting 94.1%, we get 94.8%.", "tokens": [400, 291, 393, 536, 437, 2314, 510, 307, 321, 11, 2831, 813, 1242, 30849, 13, 16, 8923, 321, 483, 30849, 13, 23, 6856], "temperature": 0.0, "avg_logprob": -0.12140426635742188, "compression_ratio": 1.38755980861244, "no_speech_prob": 1.1842905223602429e-05}, {"id": 323, "seek": 175176, "start": 1766.36, "end": 1773.1, "text": " So we've had a pretty big jump in accuracy.", "tokens": [407, 321, 600, 632, 257, 1238, 955, 3012, 294, 14170, 13], "temperature": 0.0, "avg_logprob": -0.12140426635742188, "compression_ratio": 1.38755980861244, "no_speech_prob": 1.1842905223602429e-05}, {"id": 324, "seek": 175176, "start": 1773.1, "end": 1776.28, "text": " So this is trick number one.", "tokens": [407, 341, 307, 4282, 1230, 472, 13], "temperature": 0.0, "avg_logprob": -0.12140426635742188, "compression_ratio": 1.38755980861244, "no_speech_prob": 1.1842905223602429e-05}, {"id": 325, "seek": 175176, "start": 1776.28, "end": 1781.36, "text": " And then we're going to come back to IMDB in a moment.", "tokens": [400, 550, 321, 434, 516, 281, 808, 646, 281, 21463, 27735, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.12140426635742188, "compression_ratio": 1.38755980861244, "no_speech_prob": 1.1842905223602429e-05}, {"id": 326, "seek": 178136, "start": 1781.36, "end": 1789.1999999999998, "text": " The next thing I want to do is take a little diversion into other languages.", "tokens": [440, 958, 551, 286, 528, 281, 360, 307, 747, 257, 707, 49422, 666, 661, 8650, 13], "temperature": 0.0, "avg_logprob": -0.09065413243562273, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.4510279470414389e-05}, {"id": 327, "seek": 178136, "start": 1789.1999999999998, "end": 1794.6799999999998, "text": " Because part of being able to be an effective NLP practitioner is being able to do NLP in", "tokens": [1436, 644, 295, 885, 1075, 281, 312, 364, 4942, 426, 45196, 32125, 307, 885, 1075, 281, 360, 426, 45196, 294], "temperature": 0.0, "avg_logprob": -0.09065413243562273, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.4510279470414389e-05}, {"id": 328, "seek": 178136, "start": 1794.6799999999998, "end": 1797.9599999999998, "text": " languages other than English.", "tokens": [8650, 661, 813, 3669, 13], "temperature": 0.0, "avg_logprob": -0.09065413243562273, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.4510279470414389e-05}, {"id": 329, "seek": 178136, "start": 1797.9599999999998, "end": 1803.52, "text": " Because lots of people speak languages other than English, including people in this room.", "tokens": [1436, 3195, 295, 561, 1710, 8650, 661, 813, 3669, 11, 3009, 561, 294, 341, 1808, 13], "temperature": 0.0, "avg_logprob": -0.09065413243562273, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.4510279470414389e-05}, {"id": 330, "seek": 178136, "start": 1803.52, "end": 1807.6399999999999, "text": " And there's a few little things you kind of have to be aware of when you deal with other", "tokens": [400, 456, 311, 257, 1326, 707, 721, 291, 733, 295, 362, 281, 312, 3650, 295, 562, 291, 2028, 365, 661], "temperature": 0.0, "avg_logprob": -0.09065413243562273, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.4510279470414389e-05}, {"id": 331, "seek": 178136, "start": 1807.6399999999999, "end": 1808.6399999999999, "text": " languages.", "tokens": [8650, 13], "temperature": 0.0, "avg_logprob": -0.09065413243562273, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.4510279470414389e-05}, {"id": 332, "seek": 178136, "start": 1808.6399999999999, "end": 1810.4799999999998, "text": " So let's talk about those things.", "tokens": [407, 718, 311, 751, 466, 729, 721, 13], "temperature": 0.0, "avg_logprob": -0.09065413243562273, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.4510279470414389e-05}, {"id": 333, "seek": 181048, "start": 1810.48, "end": 1817.72, "text": " The first and most obvious one is that the language you're working with may not have", "tokens": [440, 700, 293, 881, 6322, 472, 307, 300, 264, 2856, 291, 434, 1364, 365, 815, 406, 362], "temperature": 0.0, "avg_logprob": -0.07987675109466949, "compression_ratio": 1.5308056872037914, "no_speech_prob": 9.972788575396407e-06}, {"id": 334, "seek": 181048, "start": 1817.72, "end": 1824.1200000000001, "text": " a pre-trained Wikipedia model available from Fast.ai that you can download.", "tokens": [257, 659, 12, 17227, 2001, 28999, 2316, 2435, 490, 15968, 13, 1301, 300, 291, 393, 5484, 13], "temperature": 0.0, "avg_logprob": -0.07987675109466949, "compression_ratio": 1.5308056872037914, "no_speech_prob": 9.972788575396407e-06}, {"id": 335, "seek": 181048, "start": 1824.1200000000001, "end": 1829.96, "text": " We're actually planning to provide many, many more language models pretty soon.", "tokens": [492, 434, 767, 5038, 281, 2893, 867, 11, 867, 544, 2856, 5245, 1238, 2321, 13], "temperature": 0.0, "avg_logprob": -0.07987675109466949, "compression_ratio": 1.5308056872037914, "no_speech_prob": 9.972788575396407e-06}, {"id": 336, "seek": 181048, "start": 1829.96, "end": 1833.64, "text": " Right now the only one we have available is English, which is pretty sloppy of us.", "tokens": [1779, 586, 264, 787, 472, 321, 362, 2435, 307, 3669, 11, 597, 307, 1238, 43684, 295, 505, 13], "temperature": 0.0, "avg_logprob": -0.07987675109466949, "compression_ratio": 1.5308056872037914, "no_speech_prob": 9.972788575396407e-06}, {"id": 337, "seek": 183364, "start": 1833.64, "end": 1841.48, "text": " So I wanted to show you how you can create your own pre-trained model for other languages", "tokens": [407, 286, 1415, 281, 855, 291, 577, 291, 393, 1884, 428, 1065, 659, 12, 17227, 2001, 2316, 337, 661, 8650], "temperature": 0.0, "avg_logprob": -0.15036629958891531, "compression_ratio": 1.5149700598802396, "no_speech_prob": 5.255330506770406e-06}, {"id": 338, "seek": 183364, "start": 1841.48, "end": 1843.3200000000002, "text": " from scratch.", "tokens": [490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.15036629958891531, "compression_ratio": 1.5149700598802396, "no_speech_prob": 5.255330506770406e-06}, {"id": 339, "seek": 183364, "start": 1843.3200000000002, "end": 1847.3000000000002, "text": " And we're going to use Vietnamese.", "tokens": [400, 321, 434, 516, 281, 764, 25934, 13], "temperature": 0.0, "avg_logprob": -0.15036629958891531, "compression_ratio": 1.5149700598802396, "no_speech_prob": 5.255330506770406e-06}, {"id": 340, "seek": 183364, "start": 1847.3000000000002, "end": 1849.0, "text": " And we're not actually doing this at random.", "tokens": [400, 321, 434, 406, 767, 884, 341, 412, 4974, 13], "temperature": 0.0, "avg_logprob": -0.15036629958891531, "compression_ratio": 1.5149700598802396, "no_speech_prob": 5.255330506770406e-06}, {"id": 341, "seek": 183364, "start": 1849.0, "end": 1853.44, "text": " I went to the Wikipedia.", "tokens": [286, 1437, 281, 264, 28999, 13], "temperature": 0.0, "avg_logprob": -0.15036629958891531, "compression_ratio": 1.5149700598802396, "no_speech_prob": 5.255330506770406e-06}, {"id": 342, "seek": 183364, "start": 1853.44, "end": 1862.24, "text": " There's actually a list of Wikipedia's page.", "tokens": [821, 311, 767, 257, 1329, 295, 28999, 311, 3028, 13], "temperature": 0.0, "avg_logprob": -0.15036629958891531, "compression_ratio": 1.5149700598802396, "no_speech_prob": 5.255330506770406e-06}, {"id": 343, "seek": 186224, "start": 1862.24, "end": 1874.48, "text": " And the list of Wikipedia's page on Wikipedia has lots of statistics, as you can see.", "tokens": [400, 264, 1329, 295, 28999, 311, 3028, 322, 28999, 575, 3195, 295, 12523, 11, 382, 291, 393, 536, 13], "temperature": 0.0, "avg_logprob": -0.0877951685587565, "compression_ratio": 1.5621621621621622, "no_speech_prob": 1.4969856238167267e-05}, {"id": 344, "seek": 186224, "start": 1874.48, "end": 1882.84, "text": " And one of the things they do is they track both the number of articles and they also", "tokens": [400, 472, 295, 264, 721, 436, 360, 307, 436, 2837, 1293, 264, 1230, 295, 11290, 293, 436, 611], "temperature": 0.0, "avg_logprob": -0.0877951685587565, "compression_ratio": 1.5621621621621622, "no_speech_prob": 1.4969856238167267e-05}, {"id": 345, "seek": 186224, "start": 1882.84, "end": 1884.32, "text": " track the depth.", "tokens": [2837, 264, 7161, 13], "temperature": 0.0, "avg_logprob": -0.0877951685587565, "compression_ratio": 1.5621621621621622, "no_speech_prob": 1.4969856238167267e-05}, {"id": 346, "seek": 186224, "start": 1884.32, "end": 1890.04, "text": " So initially I was thinking, let's try and find a Wikipedia with lots of articles but", "tokens": [407, 9105, 286, 390, 1953, 11, 718, 311, 853, 293, 915, 257, 28999, 365, 3195, 295, 11290, 457], "temperature": 0.0, "avg_logprob": -0.0877951685587565, "compression_ratio": 1.5621621621621622, "no_speech_prob": 1.4969856238167267e-05}, {"id": 347, "seek": 186224, "start": 1890.04, "end": 1891.48, "text": " isn't English.", "tokens": [1943, 380, 3669, 13], "temperature": 0.0, "avg_logprob": -0.0877951685587565, "compression_ratio": 1.5621621621621622, "no_speech_prob": 1.4969856238167267e-05}, {"id": 348, "seek": 189148, "start": 1891.48, "end": 1898.2, "text": " And so the next one is the Cebuano Wikipedia with 5.3 million articles.", "tokens": [400, 370, 264, 958, 472, 307, 264, 8257, 6021, 3730, 28999, 365, 1025, 13, 18, 2459, 11290, 13], "temperature": 0.0, "avg_logprob": -0.1732128688267299, "compression_ratio": 1.4532019704433496, "no_speech_prob": 1.8057291526929475e-05}, {"id": 349, "seek": 189148, "start": 1898.2, "end": 1899.52, "text": " And I thought, that's weird.", "tokens": [400, 286, 1194, 11, 300, 311, 3657, 13], "temperature": 0.0, "avg_logprob": -0.1732128688267299, "compression_ratio": 1.4532019704433496, "no_speech_prob": 1.8057291526929475e-05}, {"id": 350, "seek": 189148, "start": 1899.52, "end": 1902.6, "text": " I've never even heard of Cebuano.", "tokens": [286, 600, 1128, 754, 2198, 295, 8257, 6021, 3730, 13], "temperature": 0.0, "avg_logprob": -0.1732128688267299, "compression_ratio": 1.4532019704433496, "no_speech_prob": 1.8057291526929475e-05}, {"id": 351, "seek": 189148, "start": 1902.6, "end": 1903.6, "text": " Why is that?", "tokens": [1545, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.1732128688267299, "compression_ratio": 1.4532019704433496, "no_speech_prob": 1.8057291526929475e-05}, {"id": 352, "seek": 189148, "start": 1903.6, "end": 1908.08, "text": " Surely that's not the next most important language to cover.", "tokens": [29803, 300, 311, 406, 264, 958, 881, 1021, 2856, 281, 2060, 13], "temperature": 0.0, "avg_logprob": -0.1732128688267299, "compression_ratio": 1.4532019704433496, "no_speech_prob": 1.8057291526929475e-05}, {"id": 353, "seek": 189148, "start": 1908.08, "end": 1915.18, "text": " I looked it up and it turned out that if I remember correctly, somebody created a tool", "tokens": [286, 2956, 309, 493, 293, 309, 3574, 484, 300, 498, 286, 1604, 8944, 11, 2618, 2942, 257, 2290], "temperature": 0.0, "avg_logprob": -0.1732128688267299, "compression_ratio": 1.4532019704433496, "no_speech_prob": 1.8057291526929475e-05}, {"id": 354, "seek": 191518, "start": 1915.18, "end": 1921.8, "text": " that would translate using Google Translate, a Wikipedia from one language to another.", "tokens": [300, 576, 13799, 1228, 3329, 6531, 17593, 11, 257, 28999, 490, 472, 2856, 281, 1071, 13], "temperature": 0.0, "avg_logprob": -0.10519888794537886, "compression_ratio": 1.7154471544715446, "no_speech_prob": 8.267550583695993e-06}, {"id": 355, "seek": 191518, "start": 1921.8, "end": 1925.5600000000002, "text": " And the guy who did it, his girlfriend, was Cebuano.", "tokens": [400, 264, 2146, 567, 630, 309, 11, 702, 10369, 11, 390, 8257, 6021, 3730, 13], "temperature": 0.0, "avg_logprob": -0.10519888794537886, "compression_ratio": 1.7154471544715446, "no_speech_prob": 8.267550583695993e-06}, {"id": 356, "seek": 191518, "start": 1925.5600000000002, "end": 1927.8400000000001, "text": " And so he translated the whole Wikipedia.", "tokens": [400, 370, 415, 16805, 264, 1379, 28999, 13], "temperature": 0.0, "avg_logprob": -0.10519888794537886, "compression_ratio": 1.7154471544715446, "no_speech_prob": 8.267550583695993e-06}, {"id": 357, "seek": 191518, "start": 1927.8400000000001, "end": 1931.7, "text": " And so this is actually not a very good Wikipedia.", "tokens": [400, 370, 341, 307, 767, 406, 257, 588, 665, 28999, 13], "temperature": 0.0, "avg_logprob": -0.10519888794537886, "compression_ratio": 1.7154471544715446, "no_speech_prob": 8.267550583695993e-06}, {"id": 358, "seek": 191518, "start": 1931.7, "end": 1935.92, "text": " So one of the things that the Wikipedia admins did was they developed this statistic called", "tokens": [407, 472, 295, 264, 721, 300, 264, 28999, 5910, 1292, 630, 390, 436, 4743, 341, 29588, 1219], "temperature": 0.0, "avg_logprob": -0.10519888794537886, "compression_ratio": 1.7154471544715446, "no_speech_prob": 8.267550583695993e-06}, {"id": 359, "seek": 191518, "start": 1935.92, "end": 1941.68, "text": " depth, which is kind of looking at how many edits actually happened and how many users", "tokens": [7161, 11, 597, 307, 733, 295, 1237, 412, 577, 867, 41752, 767, 2011, 293, 577, 867, 5022], "temperature": 0.0, "avg_logprob": -0.10519888794537886, "compression_ratio": 1.7154471544715446, "no_speech_prob": 8.267550583695993e-06}, {"id": 360, "seek": 191518, "start": 1941.68, "end": 1942.68, "text": " are there.", "tokens": [366, 456, 13], "temperature": 0.0, "avg_logprob": -0.10519888794537886, "compression_ratio": 1.7154471544715446, "no_speech_prob": 8.267550583695993e-06}, {"id": 361, "seek": 194268, "start": 1942.68, "end": 1946.92, "text": " So you can see the depth of Cebuano is only 2.", "tokens": [407, 291, 393, 536, 264, 7161, 295, 8257, 6021, 3730, 307, 787, 568, 13], "temperature": 0.0, "avg_logprob": -0.1337340815683429, "compression_ratio": 1.5203619909502262, "no_speech_prob": 1.0783026482386049e-05}, {"id": 362, "seek": 194268, "start": 1946.92, "end": 1951.88, "text": " So what I did was I pulled this table out and put it into Excel and I multiplied depth", "tokens": [407, 437, 286, 630, 390, 286, 7373, 341, 3199, 484, 293, 829, 309, 666, 19060, 293, 286, 17207, 7161], "temperature": 0.0, "avg_logprob": -0.1337340815683429, "compression_ratio": 1.5203619909502262, "no_speech_prob": 1.0783026482386049e-05}, {"id": 363, "seek": 194268, "start": 1951.88, "end": 1958.2, "text": " by articles together and I actually found on that statistic the obvious choice for the", "tokens": [538, 11290, 1214, 293, 286, 767, 1352, 322, 300, 29588, 264, 6322, 3922, 337, 264], "temperature": 0.0, "avg_logprob": -0.1337340815683429, "compression_ratio": 1.5203619909502262, "no_speech_prob": 1.0783026482386049e-05}, {"id": 364, "seek": 194268, "start": 1958.2, "end": 1963.64, "text": " next one to look at was Vietnamese, which for some reason in this table is now showing", "tokens": [958, 472, 281, 574, 412, 390, 25934, 11, 597, 337, 512, 1778, 294, 341, 3199, 307, 586, 4099], "temperature": 0.0, "avg_logprob": -0.1337340815683429, "compression_ratio": 1.5203619909502262, "no_speech_prob": 1.0783026482386049e-05}, {"id": 365, "seek": 194268, "start": 1963.64, "end": 1964.64, "text": " dash dash.", "tokens": [8240, 8240, 13], "temperature": 0.0, "avg_logprob": -0.1337340815683429, "compression_ratio": 1.5203619909502262, "no_speech_prob": 1.0783026482386049e-05}, {"id": 366, "seek": 194268, "start": 1964.64, "end": 1965.64, "text": " I don't know why.", "tokens": [286, 500, 380, 458, 983, 13], "temperature": 0.0, "avg_logprob": -0.1337340815683429, "compression_ratio": 1.5203619909502262, "no_speech_prob": 1.0783026482386049e-05}, {"id": 367, "seek": 196564, "start": 1965.64, "end": 1973.76, "text": " When I looked at it recently, this was a super high value.", "tokens": [1133, 286, 2956, 412, 309, 3938, 11, 341, 390, 257, 1687, 1090, 2158, 13], "temperature": 0.0, "avg_logprob": -0.12708072871952267, "compression_ratio": 1.5275229357798166, "no_speech_prob": 1.1300622645649128e-05}, {"id": 368, "seek": 196564, "start": 1973.76, "end": 1981.0800000000002, "text": " You can kind of see it's got a million articles and 53 million edits and it's not a language", "tokens": [509, 393, 733, 295, 536, 309, 311, 658, 257, 2459, 11290, 293, 21860, 2459, 41752, 293, 309, 311, 406, 257, 2856], "temperature": 0.0, "avg_logprob": -0.12708072871952267, "compression_ratio": 1.5275229357798166, "no_speech_prob": 1.1300622645649128e-05}, {"id": 369, "seek": 196564, "start": 1981.0800000000002, "end": 1984.72, "text": " which I had found, like there was no other pre-trained models available in the world", "tokens": [597, 286, 632, 1352, 11, 411, 456, 390, 572, 661, 659, 12, 17227, 2001, 5245, 2435, 294, 264, 1002], "temperature": 0.0, "avg_logprob": -0.12708072871952267, "compression_ratio": 1.5275229357798166, "no_speech_prob": 1.1300622645649128e-05}, {"id": 370, "seek": 196564, "start": 1984.72, "end": 1987.8200000000002, "text": " that I could find, so I thought that would be a good thing to do.", "tokens": [300, 286, 727, 915, 11, 370, 286, 1194, 300, 576, 312, 257, 665, 551, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.12708072871952267, "compression_ratio": 1.5275229357798166, "no_speech_prob": 1.1300622645649128e-05}, {"id": 371, "seek": 196564, "start": 1987.8200000000002, "end": 1991.16, "text": " So I did try to do Vietnamese.", "tokens": [407, 286, 630, 853, 281, 360, 25934, 13], "temperature": 0.0, "avg_logprob": -0.12708072871952267, "compression_ratio": 1.5275229357798166, "no_speech_prob": 1.1300622645649128e-05}, {"id": 372, "seek": 199116, "start": 1991.16, "end": 1997.0, "text": " I don't speak any Vietnamese at all, but I found from that table that the language code", "tokens": [286, 500, 380, 1710, 604, 25934, 412, 439, 11, 457, 286, 1352, 490, 300, 3199, 300, 264, 2856, 3089], "temperature": 0.0, "avg_logprob": -0.10947072958644433, "compression_ratio": 1.62, "no_speech_prob": 2.6273577532265335e-05}, {"id": 373, "seek": 199116, "start": 1997.0, "end": 2003.48, "text": " for Vietnamese Wikipedia is vi, so I just created a variable called vi and then I started", "tokens": [337, 25934, 28999, 307, 1932, 11, 370, 286, 445, 2942, 257, 7006, 1219, 1932, 293, 550, 286, 1409], "temperature": 0.0, "avg_logprob": -0.10947072958644433, "compression_ratio": 1.62, "no_speech_prob": 2.6273577532265335e-05}, {"id": 374, "seek": 199116, "start": 2003.48, "end": 2008.28, "text": " just creating some variables for my new Wikipedia model.", "tokens": [445, 4084, 512, 9102, 337, 452, 777, 28999, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10947072958644433, "compression_ratio": 1.62, "no_speech_prob": 2.6273577532265335e-05}, {"id": 375, "seek": 199116, "start": 2008.28, "end": 2017.5600000000002, "text": " So the name of it I decided would be viwiki and I was going to try and create a directory", "tokens": [407, 264, 1315, 295, 309, 286, 3047, 576, 312, 1932, 86, 9850, 293, 286, 390, 516, 281, 853, 293, 1884, 257, 21120], "temperature": 0.0, "avg_logprob": -0.10947072958644433, "compression_ratio": 1.62, "no_speech_prob": 2.6273577532265335e-05}, {"id": 376, "seek": 201756, "start": 2017.56, "end": 2024.44, "text": " called viwiki in my datapath which is where I'm going to put things and so then I needed", "tokens": [1219, 1932, 86, 9850, 294, 452, 1137, 569, 998, 597, 307, 689, 286, 478, 516, 281, 829, 721, 293, 370, 550, 286, 2978], "temperature": 0.0, "avg_logprob": -0.18665923285730107, "compression_ratio": 1.6695278969957081, "no_speech_prob": 1.7778113033273257e-05}, {"id": 377, "seek": 201756, "start": 2024.44, "end": 2030.96, "text": " to download the Wikipedia data.", "tokens": [281, 5484, 264, 28999, 1412, 13], "temperature": 0.0, "avg_logprob": -0.18665923285730107, "compression_ratio": 1.6695278969957081, "no_speech_prob": 1.7778113033273257e-05}, {"id": 378, "seek": 201756, "start": 2030.96, "end": 2035.76, "text": " Initially I tried using wget, but the more I started preparing this lesson, I started", "tokens": [29446, 286, 3031, 1228, 261, 847, 11, 457, 264, 544, 286, 1409, 10075, 341, 6898, 11, 286, 1409], "temperature": 0.0, "avg_logprob": -0.18665923285730107, "compression_ratio": 1.6695278969957081, "no_speech_prob": 1.7778113033273257e-05}, {"id": 379, "seek": 201756, "start": 2035.76, "end": 2041.56, "text": " doing more different Wikipedias and I thought it would be nice to do more of this in Python.", "tokens": [884, 544, 819, 28999, 82, 293, 286, 1194, 309, 576, 312, 1481, 281, 360, 544, 295, 341, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.18665923285730107, "compression_ratio": 1.6695278969957081, "no_speech_prob": 1.7778113033273257e-05}, {"id": 380, "seek": 201756, "start": 2041.56, "end": 2045.8799999999999, "text": " There were quite a few steps I was doing and I found myself doing lots of different shell", "tokens": [821, 645, 1596, 257, 1326, 4439, 286, 390, 884, 293, 286, 1352, 2059, 884, 3195, 295, 819, 8720], "temperature": 0.0, "avg_logprob": -0.18665923285730107, "compression_ratio": 1.6695278969957081, "no_speech_prob": 1.7778113033273257e-05}, {"id": 381, "seek": 204588, "start": 2045.88, "end": 2050.52, "text": " commands and then forgetting what order I was doing them in and so what I then decided to", "tokens": [16901, 293, 550, 25428, 437, 1668, 286, 390, 884, 552, 294, 293, 370, 437, 286, 550, 3047, 281], "temperature": 0.0, "avg_logprob": -0.22812056868043665, "compression_ratio": 1.5460122699386503, "no_speech_prob": 0.0001141070097219199}, {"id": 382, "seek": 204588, "start": 2050.52, "end": 2066.36, "text": " do was I created a little Python file called nlputils which you'll find in the course nlp", "tokens": [360, 390, 286, 2942, 257, 707, 15329, 3991, 1219, 297, 75, 79, 325, 4174, 597, 291, 603, 915, 294, 264, 1164, 297, 75, 79], "temperature": 0.0, "avg_logprob": -0.22812056868043665, "compression_ratio": 1.5460122699386503, "no_speech_prob": 0.0001141070097219199}, {"id": 383, "seek": 204588, "start": 2066.36, "end": 2067.92, "text": " folder.", "tokens": [10820, 13], "temperature": 0.0, "avg_logprob": -0.22812056868043665, "compression_ratio": 1.5460122699386503, "no_speech_prob": 0.0001141070097219199}, {"id": 384, "seek": 204588, "start": 2067.92, "end": 2069.8, "text": " Here it is.", "tokens": [1692, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.22812056868043665, "compression_ratio": 1.5460122699386503, "no_speech_prob": 0.0001141070097219199}, {"id": 385, "seek": 204588, "start": 2069.8, "end": 2074.76, "text": " And so I just created a little thing called getwiki.", "tokens": [400, 370, 286, 445, 2942, 257, 707, 551, 1219, 483, 86, 9850, 13], "temperature": 0.0, "avg_logprob": -0.22812056868043665, "compression_ratio": 1.5460122699386503, "no_speech_prob": 0.0001141070097219199}, {"id": 386, "seek": 207476, "start": 2074.76, "end": 2079.1600000000003, "text": " And so you just pass it a path to save the Wikipedia and the language.", "tokens": [400, 370, 291, 445, 1320, 309, 257, 3100, 281, 3155, 264, 28999, 293, 264, 2856, 13], "temperature": 0.0, "avg_logprob": -0.20958322348053923, "compression_ratio": 1.55793991416309, "no_speech_prob": 2.9309510864550248e-05}, {"id": 387, "seek": 207476, "start": 2079.1600000000003, "end": 2081.2000000000003, "text": " It just checks to see whether that already exists.", "tokens": [467, 445, 13834, 281, 536, 1968, 300, 1217, 8198, 13], "temperature": 0.0, "avg_logprob": -0.20958322348053923, "compression_ratio": 1.55793991416309, "no_speech_prob": 2.9309510864550248e-05}, {"id": 388, "seek": 207476, "start": 2081.2000000000003, "end": 2083.1200000000003, "text": " If it does, it says looks like you're fine.", "tokens": [759, 309, 775, 11, 309, 1619, 1542, 411, 291, 434, 2489, 13], "temperature": 0.0, "avg_logprob": -0.20958322348053923, "compression_ratio": 1.55793991416309, "no_speech_prob": 2.9309510864550248e-05}, {"id": 389, "seek": 207476, "start": 2083.1200000000003, "end": 2085.36, "text": " I don't need to do it.", "tokens": [286, 500, 380, 643, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.20958322348053923, "compression_ratio": 1.55793991416309, "no_speech_prob": 2.9309510864550248e-05}, {"id": 390, "seek": 207476, "start": 2085.36, "end": 2090.1600000000003, "text": " Otherwise it just goes ahead and calls fastai's download URL function.", "tokens": [10328, 309, 445, 1709, 2286, 293, 5498, 2370, 1301, 311, 5484, 12905, 2445, 13], "temperature": 0.0, "avg_logprob": -0.20958322348053923, "compression_ratio": 1.55793991416309, "no_speech_prob": 2.9309510864550248e-05}, {"id": 391, "seek": 207476, "start": 2090.1600000000003, "end": 2097.28, "text": " This is the standard place that you'll find the saved XML of a complete Wikipedia for", "tokens": [639, 307, 264, 3832, 1081, 300, 291, 603, 915, 264, 6624, 43484, 295, 257, 3566, 28999, 337], "temperature": 0.0, "avg_logprob": -0.20958322348053923, "compression_ratio": 1.55793991416309, "no_speech_prob": 2.9309510864550248e-05}, {"id": 392, "seek": 207476, "start": 2097.28, "end": 2099.2000000000003, "text": " a given language.", "tokens": [257, 2212, 2856, 13], "temperature": 0.0, "avg_logprob": -0.20958322348053923, "compression_ratio": 1.55793991416309, "no_speech_prob": 2.9309510864550248e-05}, {"id": 393, "seek": 209920, "start": 2099.2, "end": 2105.16, "text": " And it's saved in the zip file and then it unzips it.", "tokens": [400, 309, 311, 6624, 294, 264, 20730, 3991, 293, 550, 309, 517, 89, 2600, 309, 13], "temperature": 0.0, "avg_logprob": -0.09215360529282514, "compression_ratio": 1.6988416988416988, "no_speech_prob": 5.3381058933155145e-06}, {"id": 394, "seek": 209920, "start": 2105.16, "end": 2112.3599999999997, "text": " And so that ends up with an XML file containing all of the article data and lots of metadata.", "tokens": [400, 370, 300, 5314, 493, 365, 364, 43484, 3991, 19273, 439, 295, 264, 7222, 1412, 293, 3195, 295, 26603, 13], "temperature": 0.0, "avg_logprob": -0.09215360529282514, "compression_ratio": 1.6988416988416988, "no_speech_prob": 5.3381058933155145e-06}, {"id": 395, "seek": 209920, "start": 2112.3599999999997, "end": 2115.58, "text": " It's not great for a language model because we don't want a language model that predicts", "tokens": [467, 311, 406, 869, 337, 257, 2856, 2316, 570, 321, 500, 380, 528, 257, 2856, 2316, 300, 6069, 82], "temperature": 0.0, "avg_logprob": -0.09215360529282514, "compression_ratio": 1.6988416988416988, "no_speech_prob": 5.3381058933155145e-06}, {"id": 396, "seek": 209920, "start": 2115.58, "end": 2117.9399999999996, "text": " the next token in an XML file.", "tokens": [264, 958, 14862, 294, 364, 43484, 3991, 13], "temperature": 0.0, "avg_logprob": -0.09215360529282514, "compression_ratio": 1.6988416988416988, "no_speech_prob": 5.3381058933155145e-06}, {"id": 397, "seek": 209920, "start": 2117.9399999999996, "end": 2123.12, "text": " So we really want to pull it out into a plain text file that just contains the titles and", "tokens": [407, 321, 534, 528, 281, 2235, 309, 484, 666, 257, 11121, 2487, 3991, 300, 445, 8306, 264, 12992, 293], "temperature": 0.0, "avg_logprob": -0.09215360529282514, "compression_ratio": 1.6988416988416988, "no_speech_prob": 5.3381058933155145e-06}, {"id": 398, "seek": 209920, "start": 2123.12, "end": 2124.5, "text": " the text.", "tokens": [264, 2487, 13], "temperature": 0.0, "avg_logprob": -0.09215360529282514, "compression_ratio": 1.6988416988416988, "no_speech_prob": 5.3381058933155145e-06}, {"id": 399, "seek": 209920, "start": 2124.5, "end": 2128.72, "text": " It turns out there's a tool called WikiExtractor that already does this.", "tokens": [467, 4523, 484, 456, 311, 257, 2290, 1219, 35892, 36, 734, 1897, 284, 300, 1217, 775, 341, 13], "temperature": 0.0, "avg_logprob": -0.09215360529282514, "compression_ratio": 1.6988416988416988, "no_speech_prob": 5.3381058933155145e-06}, {"id": 400, "seek": 212872, "start": 2128.72, "end": 2135.64, "text": " So I just go ahead and I say get clone WikiExtractor and then I call WikiExtractor.", "tokens": [407, 286, 445, 352, 2286, 293, 286, 584, 483, 26506, 35892, 36, 734, 1897, 284, 293, 550, 286, 818, 35892, 36, 734, 1897, 284, 13], "temperature": 0.0, "avg_logprob": -0.1269044273778012, "compression_ratio": 1.8883495145631068, "no_speech_prob": 4.936947789246915e-06}, {"id": 401, "seek": 212872, "start": 2135.64, "end": 2143.72, "text": " And then I looked at, I Googled for Wikipedia article length histogram and I found some research", "tokens": [400, 550, 286, 2956, 412, 11, 286, 45005, 1493, 337, 28999, 7222, 4641, 49816, 293, 286, 1352, 512, 2132], "temperature": 0.0, "avg_logprob": -0.1269044273778012, "compression_ratio": 1.8883495145631068, "no_speech_prob": 4.936947789246915e-06}, {"id": 402, "seek": 212872, "start": 2143.72, "end": 2148.8399999999997, "text": " that showed that Wikipedia article lengths are extremely bimodal.", "tokens": [300, 4712, 300, 28999, 7222, 26329, 366, 4664, 272, 332, 378, 304, 13], "temperature": 0.0, "avg_logprob": -0.1269044273778012, "compression_ratio": 1.8883495145631068, "no_speech_prob": 4.936947789246915e-06}, {"id": 403, "seek": 212872, "start": 2148.8399999999997, "end": 2153.08, "text": " There's lots of articles that are about a thousand characters long and lots of articles", "tokens": [821, 311, 3195, 295, 11290, 300, 366, 466, 257, 4714, 4342, 938, 293, 3195, 295, 11290], "temperature": 0.0, "avg_logprob": -0.1269044273778012, "compression_ratio": 1.8883495145631068, "no_speech_prob": 4.936947789246915e-06}, {"id": 404, "seek": 212872, "start": 2153.08, "end": 2155.6, "text": " that are about three to four thousand characters long.", "tokens": [300, 366, 466, 1045, 281, 1451, 4714, 4342, 938, 13], "temperature": 0.0, "avg_logprob": -0.1269044273778012, "compression_ratio": 1.8883495145631068, "no_speech_prob": 4.936947789246915e-06}, {"id": 405, "seek": 215560, "start": 2155.6, "end": 2161.92, "text": " The thousand character articles, it turns out, generally, what do they call them?", "tokens": [440, 4714, 2517, 11290, 11, 309, 4523, 484, 11, 5101, 11, 437, 360, 436, 818, 552, 30], "temperature": 0.0, "avg_logprob": -0.216277368952719, "compression_ratio": 1.6088888888888888, "no_speech_prob": 3.6688456930278335e-06}, {"id": 406, "seek": 215560, "start": 2161.92, "end": 2166.04, "text": " They're those pages which are not stub pages.", "tokens": [814, 434, 729, 7183, 597, 366, 406, 20266, 7183, 13], "temperature": 0.0, "avg_logprob": -0.216277368952719, "compression_ratio": 1.6088888888888888, "no_speech_prob": 3.6688456930278335e-06}, {"id": 407, "seek": 215560, "start": 2166.04, "end": 2169.88, "text": " They're basically things that say like, oh this Wikipedia page is a stub and it's just", "tokens": [814, 434, 1936, 721, 300, 584, 411, 11, 1954, 341, 28999, 3028, 307, 257, 20266, 293, 309, 311, 445], "temperature": 0.0, "avg_logprob": -0.216277368952719, "compression_ratio": 1.6088888888888888, "no_speech_prob": 3.6688456930278335e-06}, {"id": 408, "seek": 215560, "start": 2169.88, "end": 2172.16, "text": " like a couple of sentences.", "tokens": [411, 257, 1916, 295, 16579, 13], "temperature": 0.0, "avg_logprob": -0.216277368952719, "compression_ratio": 1.6088888888888888, "no_speech_prob": 3.6688456930278335e-06}, {"id": 409, "seek": 215560, "start": 2172.16, "end": 2177.8399999999997, "text": " So they're not really very useful or interesting for a language model because they're largely", "tokens": [407, 436, 434, 406, 534, 588, 4420, 420, 1880, 337, 257, 2856, 2316, 570, 436, 434, 11611], "temperature": 0.0, "avg_logprob": -0.216277368952719, "compression_ratio": 1.6088888888888888, "no_speech_prob": 3.6688456930278335e-06}, {"id": 410, "seek": 215560, "start": 2177.8399999999997, "end": 2181.62, "text": " kind of just boilerplate.", "tokens": [733, 295, 445, 39228, 37008, 13], "temperature": 0.0, "avg_logprob": -0.216277368952719, "compression_ratio": 1.6088888888888888, "no_speech_prob": 3.6688456930278335e-06}, {"id": 411, "seek": 218162, "start": 2181.62, "end": 2187.52, "text": " So I added a minimum text length of 1800 which I saw was that kind of point between the two", "tokens": [407, 286, 3869, 257, 7285, 2487, 4641, 295, 24327, 597, 286, 1866, 390, 300, 733, 295, 935, 1296, 264, 732], "temperature": 0.0, "avg_logprob": -0.1356453001499176, "compression_ratio": 1.5866141732283465, "no_speech_prob": 3.3931285088328877e-06}, {"id": 412, "seek": 218162, "start": 2187.52, "end": 2188.96, "text": " modes.", "tokens": [14068, 13], "temperature": 0.0, "avg_logprob": -0.1356453001499176, "compression_ratio": 1.5866141732283465, "no_speech_prob": 3.3931285088328877e-06}, {"id": 413, "seek": 218162, "start": 2188.96, "end": 2194.8199999999997, "text": " And so this makes sure that we end up with only proper interesting articles.", "tokens": [400, 370, 341, 1669, 988, 300, 321, 917, 493, 365, 787, 2296, 1880, 11290, 13], "temperature": 0.0, "avg_logprob": -0.1356453001499176, "compression_ratio": 1.5866141732283465, "no_speech_prob": 3.3931285088328877e-06}, {"id": 414, "seek": 218162, "start": 2194.8199999999997, "end": 2199.44, "text": " So I mention these steps because they're like the kinds of things that you should think", "tokens": [407, 286, 2152, 613, 4439, 570, 436, 434, 411, 264, 3685, 295, 721, 300, 291, 820, 519], "temperature": 0.0, "avg_logprob": -0.1356453001499176, "compression_ratio": 1.5866141732283465, "no_speech_prob": 3.3931285088328877e-06}, {"id": 415, "seek": 218162, "start": 2199.44, "end": 2204.16, "text": " about when you start solving a new problem like this.", "tokens": [466, 562, 291, 722, 12606, 257, 777, 1154, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1356453001499176, "compression_ratio": 1.5866141732283465, "no_speech_prob": 3.3931285088328877e-06}, {"id": 416, "seek": 218162, "start": 2204.16, "end": 2211.12, "text": " The information you need is generally out there and it's normally just a Google away.", "tokens": [440, 1589, 291, 643, 307, 5101, 484, 456, 293, 309, 311, 5646, 445, 257, 3329, 1314, 13], "temperature": 0.0, "avg_logprob": -0.1356453001499176, "compression_ratio": 1.5866141732283465, "no_speech_prob": 3.3931285088328877e-06}, {"id": 417, "seek": 221112, "start": 2211.12, "end": 2214.3599999999997, "text": " And it's kind of important to get these little details right, otherwise you'll end up with", "tokens": [400, 309, 311, 733, 295, 1021, 281, 483, 613, 707, 4365, 558, 11, 5911, 291, 603, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.16958840942382813, "compression_ratio": 1.7158273381294964, "no_speech_prob": 2.7968908398179337e-05}, {"id": 418, "seek": 221112, "start": 2214.3599999999997, "end": 2218.3599999999997, "text": " a language model that's good at predicting the next word of Wikipedia boilerplate rather", "tokens": [257, 2856, 2316, 300, 311, 665, 412, 32884, 264, 958, 1349, 295, 28999, 39228, 37008, 2831], "temperature": 0.0, "avg_logprob": -0.16958840942382813, "compression_ratio": 1.7158273381294964, "no_speech_prob": 2.7968908398179337e-05}, {"id": 419, "seek": 221112, "start": 2218.3599999999997, "end": 2223.2799999999997, "text": " than the next word of actual articles.", "tokens": [813, 264, 958, 1349, 295, 3539, 11290, 13], "temperature": 0.0, "avg_logprob": -0.16958840942382813, "compression_ratio": 1.7158273381294964, "no_speech_prob": 2.7968908398179337e-05}, {"id": 420, "seek": 221112, "start": 2223.2799999999997, "end": 2229.56, "text": " So that spits out a text file which always seemed to be called text AAWiki00, so I just", "tokens": [407, 300, 637, 1208, 484, 257, 2487, 3991, 597, 1009, 6576, 281, 312, 1219, 2487, 30680, 54, 9850, 628, 11, 370, 286, 445], "temperature": 0.0, "avg_logprob": -0.16958840942382813, "compression_ratio": 1.7158273381294964, "no_speech_prob": 2.7968908398179337e-05}, {"id": 421, "seek": 221112, "start": 2229.56, "end": 2233.56, "text": " moved that to the actual file name that we wanted.", "tokens": [4259, 300, 281, 264, 3539, 3991, 1315, 300, 321, 1415, 13], "temperature": 0.0, "avg_logprob": -0.16958840942382813, "compression_ratio": 1.7158273381294964, "no_speech_prob": 2.7968908398179337e-05}, {"id": 422, "seek": 221112, "start": 2233.56, "end": 2235.2, "text": " And that's all GetWiki does.", "tokens": [400, 300, 311, 439, 3240, 54, 9850, 775, 13], "temperature": 0.0, "avg_logprob": -0.16958840942382813, "compression_ratio": 1.7158273381294964, "no_speech_prob": 2.7968908398179337e-05}, {"id": 423, "seek": 221112, "start": 2235.2, "end": 2240.12, "text": " So now that that's there, if you're just, so when you're working with notebooks, if you're", "tokens": [407, 586, 300, 300, 311, 456, 11, 498, 291, 434, 445, 11, 370, 562, 291, 434, 1364, 365, 43782, 11, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.16958840942382813, "compression_ratio": 1.7158273381294964, "no_speech_prob": 2.7968908398179337e-05}, {"id": 424, "seek": 224012, "start": 2240.12, "end": 2244.96, "text": " going to use the same function in lots of different notebooks, it's not a bad idea just", "tokens": [516, 281, 764, 264, 912, 2445, 294, 3195, 295, 819, 43782, 11, 309, 311, 406, 257, 1578, 1558, 445], "temperature": 0.0, "avg_logprob": -0.13600614666938782, "compression_ratio": 1.711111111111111, "no_speech_prob": 6.854261755506741e-06}, {"id": 425, "seek": 224012, "start": 2244.96, "end": 2248.12, "text": " to chuck it in a text file like this.", "tokens": [281, 20870, 309, 294, 257, 2487, 3991, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.13600614666938782, "compression_ratio": 1.711111111111111, "no_speech_prob": 6.854261755506741e-06}, {"id": 426, "seek": 224012, "start": 2248.12, "end": 2252.88, "text": " And once you've got a text file like that in the same directory as your notebook, you", "tokens": [400, 1564, 291, 600, 658, 257, 2487, 3991, 411, 300, 294, 264, 912, 21120, 382, 428, 21060, 11, 291], "temperature": 0.0, "avg_logprob": -0.13600614666938782, "compression_ratio": 1.711111111111111, "no_speech_prob": 6.854261755506741e-06}, {"id": 427, "seek": 224012, "start": 2252.88, "end": 2256.7, "text": " can then import it.", "tokens": [393, 550, 974, 309, 13], "temperature": 0.0, "avg_logprob": -0.13600614666938782, "compression_ratio": 1.711111111111111, "no_speech_prob": 6.854261755506741e-06}, {"id": 428, "seek": 224012, "start": 2256.7, "end": 2261.98, "text": " So that's how you can get the best of both worlds of the notebook interactive experience", "tokens": [407, 300, 311, 577, 291, 393, 483, 264, 1151, 295, 1293, 13401, 295, 264, 21060, 15141, 1752], "temperature": 0.0, "avg_logprob": -0.13600614666938782, "compression_ratio": 1.711111111111111, "no_speech_prob": 6.854261755506741e-06}, {"id": 429, "seek": 224012, "start": 2261.98, "end": 2268.12, "text": " and the kind of the module reproducible code editing experience.", "tokens": [293, 264, 733, 295, 264, 10088, 11408, 32128, 3089, 10000, 1752, 13], "temperature": 0.0, "avg_logprob": -0.13600614666938782, "compression_ratio": 1.711111111111111, "no_speech_prob": 6.854261755506741e-06}, {"id": 430, "seek": 226812, "start": 2268.12, "end": 2272.4, "text": " So once I call GetWiki, that goes ahead and does that.", "tokens": [407, 1564, 286, 818, 3240, 54, 9850, 11, 300, 1709, 2286, 293, 775, 300, 13], "temperature": 0.0, "avg_logprob": -0.21781129103440505, "compression_ratio": 1.5564853556485356, "no_speech_prob": 1.4510184882965405e-05}, {"id": 431, "seek": 226812, "start": 2272.4, "end": 2276.4, "text": " You'll see in this case actually this is ZHWiki00, I'll describe this next, I also did Chinese", "tokens": [509, 603, 536, 294, 341, 1389, 767, 341, 307, 1176, 39, 54, 9850, 628, 11, 286, 603, 6786, 341, 958, 11, 286, 611, 630, 4649], "temperature": 0.0, "avg_logprob": -0.21781129103440505, "compression_ratio": 1.5564853556485356, "no_speech_prob": 1.4510184882965405e-05}, {"id": 432, "seek": 226812, "start": 2276.4, "end": 2280.64, "text": " last time I ran that one, it was Chinese.", "tokens": [1036, 565, 286, 5872, 300, 472, 11, 309, 390, 4649, 13], "temperature": 0.0, "avg_logprob": -0.21781129103440505, "compression_ratio": 1.5564853556485356, "no_speech_prob": 1.4510184882965405e-05}, {"id": 433, "seek": 226812, "start": 2280.64, "end": 2286.4, "text": " This was what the start of the Vietnamese Wikipedia looks like.", "tokens": [639, 390, 437, 264, 722, 295, 264, 25934, 28999, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.21781129103440505, "compression_ratio": 1.5564853556485356, "no_speech_prob": 1.4510184882965405e-05}, {"id": 434, "seek": 226812, "start": 2286.4, "end": 2291.52, "text": " Remember in Jupyter, you can pop an exclamation mark at the start of a line and then just", "tokens": [5459, 294, 22125, 88, 391, 11, 291, 393, 1665, 364, 1624, 43233, 1491, 412, 264, 722, 295, 257, 1622, 293, 550, 445], "temperature": 0.0, "avg_logprob": -0.21781129103440505, "compression_ratio": 1.5564853556485356, "no_speech_prob": 1.4510184882965405e-05}, {"id": 435, "seek": 226812, "start": 2291.52, "end": 2295.16, "text": " write bash shell commands.", "tokens": [2464, 46183, 8720, 16901, 13], "temperature": 0.0, "avg_logprob": -0.21781129103440505, "compression_ratio": 1.5564853556485356, "no_speech_prob": 1.4510184882965405e-05}, {"id": 436, "seek": 229516, "start": 2295.16, "end": 2301.2, "text": " So a really easy way to look at the first four lines of the file is with head minus", "tokens": [407, 257, 534, 1858, 636, 281, 574, 412, 264, 700, 1451, 3876, 295, 264, 3991, 307, 365, 1378, 3175], "temperature": 0.0, "avg_logprob": -0.1159439715710315, "compression_ratio": 1.6214953271028036, "no_speech_prob": 4.425443876243662e-06}, {"id": 437, "seek": 229516, "start": 2301.2, "end": 2302.94, "text": " n4.", "tokens": [297, 19, 13], "temperature": 0.0, "avg_logprob": -0.1159439715710315, "compression_ratio": 1.6214953271028036, "no_speech_prob": 4.425443876243662e-06}, {"id": 438, "seek": 229516, "start": 2302.94, "end": 2308.2799999999997, "text": " Something you may not know is that when you do these shell commands in Jupyter, is anything", "tokens": [6595, 291, 815, 406, 458, 307, 300, 562, 291, 360, 613, 8720, 16901, 294, 22125, 88, 391, 11, 307, 1340], "temperature": 0.0, "avg_logprob": -0.1159439715710315, "compression_ratio": 1.6214953271028036, "no_speech_prob": 4.425443876243662e-06}, {"id": 439, "seek": 229516, "start": 2308.2799999999997, "end": 2313.5, "text": " you put in curly braces is an actual Python variable that gets replaced there.", "tokens": [291, 829, 294, 32066, 41537, 307, 364, 3539, 15329, 7006, 300, 2170, 10772, 456, 13], "temperature": 0.0, "avg_logprob": -0.1159439715710315, "compression_ratio": 1.6214953271028036, "no_speech_prob": 4.425443876243662e-06}, {"id": 440, "seek": 229516, "start": 2313.5, "end": 2323.3199999999997, "text": " So you can see here I'm using the bash head minus n4 to show the Python path slash name.", "tokens": [407, 291, 393, 536, 510, 286, 478, 1228, 264, 46183, 1378, 3175, 297, 19, 281, 855, 264, 15329, 3100, 17330, 1315, 13], "temperature": 0.0, "avg_logprob": -0.1159439715710315, "compression_ratio": 1.6214953271028036, "no_speech_prob": 4.425443876243662e-06}, {"id": 441, "seek": 232332, "start": 2323.32, "end": 2329.6400000000003, "text": " And so here's the first little bit of my downloaded file.", "tokens": [400, 370, 510, 311, 264, 700, 707, 857, 295, 452, 21748, 3991, 13], "temperature": 0.0, "avg_logprob": -0.14535447684201327, "compression_ratio": 1.5491071428571428, "no_speech_prob": 8.530222658009734e-06}, {"id": 442, "seek": 232332, "start": 2329.6400000000003, "end": 2335.4, "text": " Generally speaking, single giant files like this one, this is kind of like a couple of", "tokens": [21082, 4124, 11, 2167, 7410, 7098, 411, 341, 472, 11, 341, 307, 733, 295, 411, 257, 1916, 295], "temperature": 0.0, "avg_logprob": -0.14535447684201327, "compression_ratio": 1.5491071428571428, "no_speech_prob": 8.530222658009734e-06}, {"id": 443, "seek": 232332, "start": 2335.4, "end": 2340.02, "text": " hundred gigabytes, a bit awkward to work with.", "tokens": [3262, 42741, 11, 257, 857, 11411, 281, 589, 365, 13], "temperature": 0.0, "avg_logprob": -0.14535447684201327, "compression_ratio": 1.5491071428571428, "no_speech_prob": 8.530222658009734e-06}, {"id": 444, "seek": 232332, "start": 2340.02, "end": 2345.84, "text": " So what I wanted to do was to split this file into multiple text files, one for each Wikipedia", "tokens": [407, 437, 286, 1415, 281, 360, 390, 281, 7472, 341, 3991, 666, 3866, 2487, 7098, 11, 472, 337, 1184, 28999], "temperature": 0.0, "avg_logprob": -0.14535447684201327, "compression_ratio": 1.5491071428571428, "no_speech_prob": 8.530222658009734e-06}, {"id": 445, "seek": 232332, "start": 2345.84, "end": 2347.0, "text": " article.", "tokens": [7222, 13], "temperature": 0.0, "avg_logprob": -0.14535447684201327, "compression_ratio": 1.5491071428571428, "no_speech_prob": 8.530222658009734e-06}, {"id": 446, "seek": 232332, "start": 2347.0, "end": 2351.7200000000003, "text": " And I saw that they always start with this pattern.", "tokens": [400, 286, 1866, 300, 436, 1009, 722, 365, 341, 5102, 13], "temperature": 0.0, "avg_logprob": -0.14535447684201327, "compression_ratio": 1.5491071428571428, "no_speech_prob": 8.530222658009734e-06}, {"id": 447, "seek": 235172, "start": 2351.72, "end": 2356.08, "text": " And the title is always here.", "tokens": [400, 264, 4876, 307, 1009, 510, 13], "temperature": 0.0, "avg_logprob": -0.1285080909729004, "compression_ratio": 1.5596330275229358, "no_speech_prob": 1.2029515346512198e-05}, {"id": 448, "seek": 235172, "start": 2356.08, "end": 2360.3199999999997, "text": " So what I did was I wrote a tiny little function called splitWiki.", "tokens": [407, 437, 286, 630, 390, 286, 4114, 257, 5870, 707, 2445, 1219, 7472, 54, 9850, 13], "temperature": 0.0, "avg_logprob": -0.1285080909729004, "compression_ratio": 1.5596330275229358, "no_speech_prob": 1.2029515346512198e-05}, {"id": 449, "seek": 235172, "start": 2360.3199999999997, "end": 2365.08, "text": " Originally I wrote it in the notebook, and then once I started using it in other notebooks,", "tokens": [28696, 286, 4114, 309, 294, 264, 21060, 11, 293, 550, 1564, 286, 1409, 1228, 309, 294, 661, 43782, 11], "temperature": 0.0, "avg_logprob": -0.1285080909729004, "compression_ratio": 1.5596330275229358, "no_speech_prob": 1.2029515346512198e-05}, {"id": 450, "seek": 235172, "start": 2365.08, "end": 2372.16, "text": " I copied it into my little NLP utils.", "tokens": [286, 25365, 309, 666, 452, 707, 426, 45196, 2839, 4174, 13], "temperature": 0.0, "avg_logprob": -0.1285080909729004, "compression_ratio": 1.5596330275229358, "no_speech_prob": 1.2029515346512198e-05}, {"id": 451, "seek": 235172, "start": 2372.16, "end": 2375.3999999999996, "text": " So you can see it here.", "tokens": [407, 291, 393, 536, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.1285080909729004, "compression_ratio": 1.5596330275229358, "no_speech_prob": 1.2029515346512198e-05}, {"id": 452, "seek": 235172, "start": 2375.3999999999996, "end": 2380.56, "text": " And hopefully you might have guessed that the way I do this is with a regular expression.", "tokens": [400, 4696, 291, 1062, 362, 21852, 300, 264, 636, 286, 360, 341, 307, 365, 257, 3890, 6114, 13], "temperature": 0.0, "avg_logprob": -0.1285080909729004, "compression_ratio": 1.5596330275229358, "no_speech_prob": 1.2029515346512198e-05}, {"id": 453, "seek": 238056, "start": 2380.56, "end": 2390.96, "text": " So I just copied and pasted that doc id equals line, and I replaced the number with backslash", "tokens": [407, 286, 445, 25365, 293, 1791, 292, 300, 3211, 4496, 6915, 1622, 11, 293, 286, 10772, 264, 1230, 365, 646, 10418, 1299], "temperature": 0.0, "avg_logprob": -0.14936159574068508, "compression_ratio": 1.5253164556962024, "no_speech_prob": 3.668845010906807e-06}, {"id": 454, "seek": 238056, "start": 2390.96, "end": 2396.32, "text": " d+, so that means one or more digits.", "tokens": [274, 46797, 370, 300, 1355, 472, 420, 544, 27011, 13], "temperature": 0.0, "avg_logprob": -0.14936159574068508, "compression_ratio": 1.5253164556962024, "no_speech_prob": 3.668845010906807e-06}, {"id": 455, "seek": 238056, "start": 2396.32, "end": 2403.14, "text": " I replaced vi with curly brackets lang, so that's going to be replaced with my language", "tokens": [286, 10772, 1932, 365, 32066, 26179, 2265, 11, 370, 300, 311, 516, 281, 312, 10772, 365, 452, 2856], "temperature": 0.0, "avg_logprob": -0.14936159574068508, "compression_ratio": 1.5253164556962024, "no_speech_prob": 3.668845010906807e-06}, {"id": 456, "seek": 238056, "start": 2403.14, "end": 2406.7999999999997, "text": " variable which is vi.", "tokens": [7006, 597, 307, 1932, 13], "temperature": 0.0, "avg_logprob": -0.14936159574068508, "compression_ratio": 1.5253164556962024, "no_speech_prob": 3.668845010906807e-06}, {"id": 457, "seek": 240680, "start": 2406.8, "end": 2414.1800000000003, "text": " And I replaced current id equals 13 with current id equals backslash d+, so one or more digits.", "tokens": [400, 286, 10772, 2190, 4496, 6915, 3705, 365, 2190, 4496, 6915, 646, 10418, 1299, 274, 46797, 370, 472, 420, 544, 27011, 13], "temperature": 0.0, "avg_logprob": -0.15717739150637672, "compression_ratio": 1.7914438502673797, "no_speech_prob": 8.398010322707705e-06}, {"id": 458, "seek": 240680, "start": 2414.1800000000003, "end": 2416.88, "text": " And then title equals double quote.", "tokens": [400, 550, 4876, 6915, 3834, 6513, 13], "temperature": 0.0, "avg_logprob": -0.15717739150637672, "compression_ratio": 1.7914438502673797, "no_speech_prob": 8.398010322707705e-06}, {"id": 459, "seek": 240680, "start": 2416.88, "end": 2420.2000000000003, "text": " And do you remember this little pattern from last week?", "tokens": [400, 360, 291, 1604, 341, 707, 5102, 490, 1036, 1243, 30], "temperature": 0.0, "avg_logprob": -0.15717739150637672, "compression_ratio": 1.7914438502673797, "no_speech_prob": 8.398010322707705e-06}, {"id": 460, "seek": 240680, "start": 2420.2000000000003, "end": 2426.0, "text": " Something followed by anything that's not that thing, plus.", "tokens": [6595, 6263, 538, 1340, 300, 311, 406, 300, 551, 11, 1804, 13], "temperature": 0.0, "avg_logprob": -0.15717739150637672, "compression_ratio": 1.7914438502673797, "no_speech_prob": 8.398010322707705e-06}, {"id": 461, "seek": 240680, "start": 2426.0, "end": 2432.86, "text": " So a double quote followed by one or more non-double quotes followed by a double quote.", "tokens": [407, 257, 3834, 6513, 6263, 538, 472, 420, 544, 2107, 12, 67, 33147, 19963, 6263, 538, 257, 3834, 6513, 13], "temperature": 0.0, "avg_logprob": -0.15717739150637672, "compression_ratio": 1.7914438502673797, "no_speech_prob": 8.398010322707705e-06}, {"id": 462, "seek": 243286, "start": 2432.86, "end": 2439.2000000000003, "text": " So that was that little pattern we learned last week to find something like this, double", "tokens": [407, 300, 390, 300, 707, 5102, 321, 3264, 1036, 1243, 281, 915, 746, 411, 341, 11, 3834], "temperature": 0.0, "avg_logprob": -0.13784926450705226, "compression_ratio": 1.6844919786096257, "no_speech_prob": 4.785077635460766e-06}, {"id": 463, "seek": 243286, "start": 2439.2000000000003, "end": 2443.96, "text": " quote, something that's not double quotes, and then a double quote.", "tokens": [6513, 11, 746, 300, 311, 406, 3834, 19963, 11, 293, 550, 257, 3834, 6513, 13], "temperature": 0.0, "avg_logprob": -0.13784926450705226, "compression_ratio": 1.6844919786096257, "no_speech_prob": 4.785077635460766e-06}, {"id": 464, "seek": 243286, "start": 2443.96, "end": 2450.6400000000003, "text": " And then finally, the contents of the double quotes I put in parentheses to capture it.", "tokens": [400, 550, 2721, 11, 264, 15768, 295, 264, 3834, 19963, 286, 829, 294, 34153, 281, 7983, 309, 13], "temperature": 0.0, "avg_logprob": -0.13784926450705226, "compression_ratio": 1.6844919786096257, "no_speech_prob": 4.785077635460766e-06}, {"id": 465, "seek": 243286, "start": 2450.6400000000003, "end": 2462.6600000000003, "text": " And so then in my splitWiki, you can use the findAll method in Python.", "tokens": [400, 370, 550, 294, 452, 7472, 54, 9850, 11, 291, 393, 764, 264, 915, 7868, 3170, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.13784926450705226, "compression_ratio": 1.6844919786096257, "no_speech_prob": 4.785077635460766e-06}, {"id": 466, "seek": 246266, "start": 2462.66, "end": 2466.8799999999997, "text": " And that will return all of the things in parentheses.", "tokens": [400, 300, 486, 2736, 439, 295, 264, 721, 294, 34153, 13], "temperature": 0.0, "avg_logprob": -0.11862816320401486, "compression_ratio": 1.7330508474576272, "no_speech_prob": 7.766902854200453e-06}, {"id": 467, "seek": 246266, "start": 2466.8799999999997, "end": 2472.8799999999997, "text": " And we only expect to find one, so I just grab it.", "tokens": [400, 321, 787, 2066, 281, 915, 472, 11, 370, 286, 445, 4444, 309, 13], "temperature": 0.0, "avg_logprob": -0.11862816320401486, "compression_ratio": 1.7330508474576272, "no_speech_prob": 7.766902854200453e-06}, {"id": 468, "seek": 246266, "start": 2472.8799999999997, "end": 2477.48, "text": " I normally find when I try these things, there's always little things that pop up, little errors", "tokens": [286, 5646, 915, 562, 286, 853, 613, 721, 11, 456, 311, 1009, 707, 721, 300, 1665, 493, 11, 707, 13603], "temperature": 0.0, "avg_logprob": -0.11862816320401486, "compression_ratio": 1.7330508474576272, "no_speech_prob": 7.766902854200453e-06}, {"id": 469, "seek": 246266, "start": 2477.48, "end": 2478.48, "text": " that pop up.", "tokens": [300, 1665, 493, 13], "temperature": 0.0, "avg_logprob": -0.11862816320401486, "compression_ratio": 1.7330508474576272, "no_speech_prob": 7.766902854200453e-06}, {"id": 470, "seek": 246266, "start": 2478.48, "end": 2483.58, "text": " In this particular case, some Wikipedia titles had a forward slash in, and so when I tried", "tokens": [682, 341, 1729, 1389, 11, 512, 28999, 12992, 632, 257, 2128, 17330, 294, 11, 293, 370, 562, 286, 3031], "temperature": 0.0, "avg_logprob": -0.11862816320401486, "compression_ratio": 1.7330508474576272, "no_speech_prob": 7.766902854200453e-06}, {"id": 471, "seek": 246266, "start": 2483.58, "end": 2488.08, "text": " to save a file with a forward slash in, I got an error because forward slash is a directory", "tokens": [281, 3155, 257, 3991, 365, 257, 2128, 17330, 294, 11, 286, 658, 364, 6713, 570, 2128, 17330, 307, 257, 21120], "temperature": 0.0, "avg_logprob": -0.11862816320401486, "compression_ratio": 1.7330508474576272, "no_speech_prob": 7.766902854200453e-06}, {"id": 472, "seek": 246266, "start": 2488.08, "end": 2489.08, "text": " separator.", "tokens": [3128, 1639, 13], "temperature": 0.0, "avg_logprob": -0.11862816320401486, "compression_ratio": 1.7330508474576272, "no_speech_prob": 7.766902854200453e-06}, {"id": 473, "seek": 248908, "start": 2489.08, "end": 2493.0, "text": " So I had this thing to replace forward slash with underscore.", "tokens": [407, 286, 632, 341, 551, 281, 7406, 2128, 17330, 365, 37556, 13], "temperature": 0.0, "avg_logprob": -0.17474865694658473, "compression_ratio": 1.6827309236947792, "no_speech_prob": 5.5075634008971974e-06}, {"id": 474, "seek": 248908, "start": 2493.0, "end": 2498.68, "text": " There was also one article length that was, the title was so long that it gave an error,", "tokens": [821, 390, 611, 472, 7222, 4641, 300, 390, 11, 264, 4876, 390, 370, 938, 300, 309, 2729, 364, 6713, 11], "temperature": 0.0, "avg_logprob": -0.17474865694658473, "compression_ratio": 1.6827309236947792, "no_speech_prob": 5.5075634008971974e-06}, {"id": 475, "seek": 248908, "start": 2498.68, "end": 2502.48, "text": " so I just added another thing to skip over the really long ones.", "tokens": [370, 286, 445, 3869, 1071, 551, 281, 10023, 670, 264, 534, 938, 2306, 13], "temperature": 0.0, "avg_logprob": -0.17474865694658473, "compression_ratio": 1.6827309236947792, "no_speech_prob": 5.5075634008971974e-06}, {"id": 476, "seek": 248908, "start": 2502.48, "end": 2504.36, "text": " But that's basically it.", "tokens": [583, 300, 311, 1936, 309, 13], "temperature": 0.0, "avg_logprob": -0.17474865694658473, "compression_ratio": 1.6827309236947792, "no_speech_prob": 5.5075634008971974e-06}, {"id": 477, "seek": 248908, "start": 2504.36, "end": 2511.24, "text": " So I just go through each line of my big file, and when I find a docId equals line, I grab", "tokens": [407, 286, 445, 352, 807, 1184, 1622, 295, 452, 955, 3991, 11, 293, 562, 286, 915, 257, 3211, 42739, 6915, 1622, 11, 286, 4444], "temperature": 0.0, "avg_logprob": -0.17474865694658473, "compression_ratio": 1.6827309236947792, "no_speech_prob": 5.5075634008971974e-06}, {"id": 478, "seek": 248908, "start": 2511.24, "end": 2515.12, "text": " the title and I create a new file with that title and then I just keep writing lines to", "tokens": [264, 4876, 293, 286, 1884, 257, 777, 3991, 365, 300, 4876, 293, 550, 286, 445, 1066, 3579, 3876, 281], "temperature": 0.0, "avg_logprob": -0.17474865694658473, "compression_ratio": 1.6827309236947792, "no_speech_prob": 5.5075634008971974e-06}, {"id": 479, "seek": 251512, "start": 2515.12, "end": 2520.0, "text": " it until the next time I find a docId equals.", "tokens": [309, 1826, 264, 958, 565, 286, 915, 257, 3211, 42739, 6915, 13], "temperature": 0.0, "avg_logprob": -0.10791141955883472, "compression_ratio": 1.5027027027027027, "no_speech_prob": 4.7108933358686045e-06}, {"id": 480, "seek": 251512, "start": 2520.0, "end": 2527.8399999999997, "text": " You'll see here that the regular expression I used re.compile first, that's never required.", "tokens": [509, 603, 536, 510, 300, 264, 3890, 6114, 286, 1143, 319, 13, 21541, 794, 700, 11, 300, 311, 1128, 4739, 13], "temperature": 0.0, "avg_logprob": -0.10791141955883472, "compression_ratio": 1.5027027027027027, "no_speech_prob": 4.7108933358686045e-06}, {"id": 481, "seek": 251512, "start": 2527.8399999999997, "end": 2532.88, "text": " You can always just put the string in a re.findAll.", "tokens": [509, 393, 1009, 445, 829, 264, 6798, 294, 257, 319, 13, 35072, 7868, 13], "temperature": 0.0, "avg_logprob": -0.10791141955883472, "compression_ratio": 1.5027027027027027, "no_speech_prob": 4.7108933358686045e-06}, {"id": 482, "seek": 251512, "start": 2532.88, "end": 2541.16, "text": " But if you say re.compile, Python will spend some time turning that string into a faster", "tokens": [583, 498, 291, 584, 319, 13, 21541, 794, 11, 15329, 486, 3496, 512, 565, 6246, 300, 6798, 666, 257, 4663], "temperature": 0.0, "avg_logprob": -0.10791141955883472, "compression_ratio": 1.5027027027027027, "no_speech_prob": 4.7108933358686045e-06}, {"id": 483, "seek": 254116, "start": 2541.16, "end": 2547.04, "text": " kernel representation, and since we're going to be going over millions of lines, it's going", "tokens": [28256, 10290, 11, 293, 1670, 321, 434, 516, 281, 312, 516, 670, 6803, 295, 3876, 11, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.1471037407443948, "compression_ratio": 1.4792899408284024, "no_speech_prob": 5.682401024387218e-06}, {"id": 484, "seek": 254116, "start": 2547.04, "end": 2550.8399999999997, "text": " to be a bit faster just to re.compile once.", "tokens": [281, 312, 257, 857, 4663, 445, 281, 319, 13, 21541, 794, 1564, 13], "temperature": 0.0, "avg_logprob": -0.1471037407443948, "compression_ratio": 1.4792899408284024, "no_speech_prob": 5.682401024387218e-06}, {"id": 485, "seek": 254116, "start": 2550.8399999999997, "end": 2553.04, "text": " So that's a useful trick.", "tokens": [407, 300, 311, 257, 4420, 4282, 13], "temperature": 0.0, "avg_logprob": -0.1471037407443948, "compression_ratio": 1.4792899408284024, "no_speech_prob": 5.682401024387218e-06}, {"id": 486, "seek": 254116, "start": 2553.04, "end": 2560.72, "text": " So after I do my split wiki, I now end up with a directory, I just called it docs, which", "tokens": [407, 934, 286, 360, 452, 7472, 261, 9850, 11, 286, 586, 917, 493, 365, 257, 21120, 11, 286, 445, 1219, 309, 45623, 11, 597], "temperature": 0.0, "avg_logprob": -0.1471037407443948, "compression_ratio": 1.4792899408284024, "no_speech_prob": 5.682401024387218e-06}, {"id": 487, "seek": 256072, "start": 2560.72, "end": 2571.2799999999997, "text": " contains millions of files, one for each Vietnamese Wikipedia article.", "tokens": [8306, 6803, 295, 7098, 11, 472, 337, 1184, 25934, 28999, 7222, 13], "temperature": 0.0, "avg_logprob": -0.09514755989188578, "compression_ratio": 1.4972677595628416, "no_speech_prob": 5.682400569639867e-06}, {"id": 488, "seek": 256072, "start": 2571.2799999999997, "end": 2580.16, "text": " So from that, to create your Vietnamese pre-trained Wikipedia model is almost identical to creating", "tokens": [407, 490, 300, 11, 281, 1884, 428, 25934, 659, 12, 17227, 2001, 28999, 2316, 307, 1920, 14800, 281, 4084], "temperature": 0.0, "avg_logprob": -0.09514755989188578, "compression_ratio": 1.4972677595628416, "no_speech_prob": 5.682400569639867e-06}, {"id": 489, "seek": 256072, "start": 2580.16, "end": 2583.8799999999997, "text": " your fine-tuned IMDB language model.", "tokens": [428, 2489, 12, 83, 43703, 21463, 27735, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.09514755989188578, "compression_ratio": 1.4972677595628416, "no_speech_prob": 5.682400569639867e-06}, {"id": 490, "seek": 256072, "start": 2583.8799999999997, "end": 2589.8799999999997, "text": " The data blocks bit is exactly the same, and then you can save it.", "tokens": [440, 1412, 8474, 857, 307, 2293, 264, 912, 11, 293, 550, 291, 393, 3155, 309, 13], "temperature": 0.0, "avg_logprob": -0.09514755989188578, "compression_ratio": 1.4972677595628416, "no_speech_prob": 5.682400569639867e-06}, {"id": 491, "seek": 258988, "start": 2589.88, "end": 2595.8, "text": " The language model learner is exactly the same, with one difference.", "tokens": [440, 2856, 2316, 33347, 307, 2293, 264, 912, 11, 365, 472, 2649, 13], "temperature": 0.0, "avg_logprob": -0.12323992729187011, "compression_ratio": 1.6419213973799127, "no_speech_prob": 2.0145367670920677e-05}, {"id": 492, "seek": 258988, "start": 2595.8, "end": 2597.0, "text": " Pre-trained equals false.", "tokens": [6001, 12, 17227, 2001, 6915, 7908, 13], "temperature": 0.0, "avg_logprob": -0.12323992729187011, "compression_ratio": 1.6419213973799127, "no_speech_prob": 2.0145367670920677e-05}, {"id": 493, "seek": 258988, "start": 2597.0, "end": 2604.0, "text": " So it won't try and download the English Wikipedia model and fine-tune it, because we're not", "tokens": [407, 309, 1582, 380, 853, 293, 5484, 264, 3669, 28999, 2316, 293, 2489, 12, 83, 2613, 309, 11, 570, 321, 434, 406], "temperature": 0.0, "avg_logprob": -0.12323992729187011, "compression_ratio": 1.6419213973799127, "no_speech_prob": 2.0145367670920677e-05}, {"id": 494, "seek": 258988, "start": 2604.0, "end": 2606.12, "text": " doing English.", "tokens": [884, 3669, 13], "temperature": 0.0, "avg_logprob": -0.12323992729187011, "compression_ratio": 1.6419213973799127, "no_speech_prob": 2.0145367670920677e-05}, {"id": 495, "seek": 258988, "start": 2606.12, "end": 2609.6600000000003, "text": " So this way it's going to start with random weights.", "tokens": [407, 341, 636, 309, 311, 516, 281, 722, 365, 4974, 17443, 13], "temperature": 0.0, "avg_logprob": -0.12323992729187011, "compression_ratio": 1.6419213973799127, "no_speech_prob": 2.0145367670920677e-05}, {"id": 496, "seek": 258988, "start": 2609.6600000000003, "end": 2616.44, "text": " Since it's starting with random weights, there's no point starting with a frozen model, so", "tokens": [4162, 309, 311, 2891, 365, 4974, 17443, 11, 456, 311, 572, 935, 2891, 365, 257, 12496, 2316, 11, 370], "temperature": 0.0, "avg_logprob": -0.12323992729187011, "compression_ratio": 1.6419213973799127, "no_speech_prob": 2.0145367670920677e-05}, {"id": 497, "seek": 258988, "start": 2616.44, "end": 2618.76, "text": " we go unfreeze straight away.", "tokens": [321, 352, 3971, 701, 1381, 2997, 1314, 13], "temperature": 0.0, "avg_logprob": -0.12323992729187011, "compression_ratio": 1.6419213973799127, "no_speech_prob": 2.0145367670920677e-05}, {"id": 498, "seek": 261876, "start": 2618.76, "end": 2622.2000000000003, "text": " I think actually Fast.ai does this for you automatically, but I just wanted to make it", "tokens": [286, 519, 767, 15968, 13, 1301, 775, 341, 337, 291, 6772, 11, 457, 286, 445, 1415, 281, 652, 309], "temperature": 0.0, "avg_logprob": -0.12960763735191844, "compression_ratio": 1.5551181102362204, "no_speech_prob": 5.6495911849197e-05}, {"id": 499, "seek": 261876, "start": 2622.2000000000003, "end": 2623.76, "text": " super clear.", "tokens": [1687, 1850, 13], "temperature": 0.0, "avg_logprob": -0.12960763735191844, "compression_ratio": 1.5551181102362204, "no_speech_prob": 5.6495911849197e-05}, {"id": 500, "seek": 261876, "start": 2623.76, "end": 2630.2000000000003, "text": " And then I just do my 10 epochs with fit1 cycle, and you can see I end up with something", "tokens": [400, 550, 286, 445, 360, 452, 1266, 30992, 28346, 365, 3318, 16, 6586, 11, 293, 291, 393, 536, 286, 917, 493, 365, 746], "temperature": 0.0, "avg_logprob": -0.12960763735191844, "compression_ratio": 1.5551181102362204, "no_speech_prob": 5.6495911849197e-05}, {"id": 501, "seek": 261876, "start": 2630.2000000000003, "end": 2631.82, "text": " that's 42% accurate.", "tokens": [300, 311, 14034, 4, 8559, 13], "temperature": 0.0, "avg_logprob": -0.12960763735191844, "compression_ratio": 1.5551181102362204, "no_speech_prob": 5.6495911849197e-05}, {"id": 502, "seek": 261876, "start": 2631.82, "end": 2638.1200000000003, "text": " So it's trained a Vietnamese language model that 42% of the time correctly predicts what", "tokens": [407, 309, 311, 8895, 257, 25934, 2856, 2316, 300, 14034, 4, 295, 264, 565, 8944, 6069, 82, 437], "temperature": 0.0, "avg_logprob": -0.12960763735191844, "compression_ratio": 1.5551181102362204, "no_speech_prob": 5.6495911849197e-05}, {"id": 503, "seek": 261876, "start": 2638.1200000000003, "end": 2639.8, "text": " the next word's going to be.", "tokens": [264, 958, 1349, 311, 516, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.12960763735191844, "compression_ratio": 1.5551181102362204, "no_speech_prob": 5.6495911849197e-05}, {"id": 504, "seek": 261876, "start": 2639.8, "end": 2646.44, "text": " And so that gives you a sense of how predictable human language is.", "tokens": [400, 370, 300, 2709, 291, 257, 2020, 295, 577, 27737, 1952, 2856, 307, 13], "temperature": 0.0, "avg_logprob": -0.12960763735191844, "compression_ratio": 1.5551181102362204, "no_speech_prob": 5.6495911849197e-05}, {"id": 505, "seek": 264644, "start": 2646.44, "end": 2652.2000000000003, "text": " It's pretty normal for most of our language models from Wikipedia to have something around", "tokens": [467, 311, 1238, 2710, 337, 881, 295, 527, 2856, 5245, 490, 28999, 281, 362, 746, 926], "temperature": 0.0, "avg_logprob": -0.07634993160472196, "compression_ratio": 1.5722543352601157, "no_speech_prob": 7.183150955825113e-06}, {"id": 506, "seek": 264644, "start": 2652.2000000000003, "end": 2657.3, "text": " about a 40% accuracy.", "tokens": [466, 257, 3356, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.07634993160472196, "compression_ratio": 1.5722543352601157, "no_speech_prob": 7.183150955825113e-06}, {"id": 507, "seek": 264644, "start": 2657.3, "end": 2668.6, "text": " So then all we need to do now is to save the two parts of the language model that we need.", "tokens": [407, 550, 439, 321, 643, 281, 360, 586, 307, 281, 3155, 264, 732, 3166, 295, 264, 2856, 2316, 300, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.07634993160472196, "compression_ratio": 1.5722543352601157, "no_speech_prob": 7.183150955825113e-06}, {"id": 508, "seek": 264644, "start": 2668.6, "end": 2672.84, "text": " The first thing we need to save is the actual language model itself.", "tokens": [440, 700, 551, 321, 643, 281, 3155, 307, 264, 3539, 2856, 2316, 2564, 13], "temperature": 0.0, "avg_logprob": -0.07634993160472196, "compression_ratio": 1.5722543352601157, "no_speech_prob": 7.183150955825113e-06}, {"id": 509, "seek": 267284, "start": 2672.84, "end": 2679.32, "text": " Now in this case, I trained the language model with FP16, but most people are probably going", "tokens": [823, 294, 341, 1389, 11, 286, 8895, 264, 2856, 2316, 365, 36655, 6866, 11, 457, 881, 561, 366, 1391, 516], "temperature": 0.0, "avg_logprob": -0.12658358122173108, "compression_ratio": 1.5758928571428572, "no_speech_prob": 8.139559213304892e-06}, {"id": 510, "seek": 267284, "start": 2679.32, "end": 2685.08, "text": " to want to actually get a FP32, a single precision language model, so I just convert it back", "tokens": [281, 528, 281, 767, 483, 257, 36655, 11440, 11, 257, 2167, 18356, 2856, 2316, 11, 370, 286, 445, 7620, 309, 646], "temperature": 0.0, "avg_logprob": -0.12658358122173108, "compression_ratio": 1.5758928571428572, "no_speech_prob": 8.139559213304892e-06}, {"id": 511, "seek": 267284, "start": 2685.08, "end": 2688.28, "text": " to FP32 and then I save it.", "tokens": [281, 36655, 11440, 293, 550, 286, 3155, 309, 13], "temperature": 0.0, "avg_logprob": -0.12658358122173108, "compression_ratio": 1.5758928571428572, "no_speech_prob": 8.139559213304892e-06}, {"id": 512, "seek": 267284, "start": 2688.28, "end": 2691.6800000000003, "text": " And you can just decide to save it wherever you like.", "tokens": [400, 291, 393, 445, 4536, 281, 3155, 309, 8660, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.12658358122173108, "compression_ratio": 1.5758928571428572, "no_speech_prob": 8.139559213304892e-06}, {"id": 513, "seek": 267284, "start": 2691.6800000000003, "end": 2699.44, "text": " What I did was I created two file names, so an array with two file names, one for the", "tokens": [708, 286, 630, 390, 286, 2942, 732, 3991, 5288, 11, 370, 364, 10225, 365, 732, 3991, 5288, 11, 472, 337, 264], "temperature": 0.0, "avg_logprob": -0.12658358122173108, "compression_ratio": 1.5758928571428572, "no_speech_prob": 8.139559213304892e-06}, {"id": 514, "seek": 269944, "start": 2699.44, "end": 2704.2000000000003, "text": " language model, so WT for WikiText, and then a second file name.", "tokens": [2856, 2316, 11, 370, 343, 51, 337, 35892, 50198, 11, 293, 550, 257, 1150, 3991, 1315, 13], "temperature": 0.0, "avg_logprob": -0.12087978702960628, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.7231279343832284e-05}, {"id": 515, "seek": 269944, "start": 2704.2000000000003, "end": 2707.32, "text": " The other thing we need to save is the vocab.", "tokens": [440, 661, 551, 321, 643, 281, 3155, 307, 264, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.12087978702960628, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.7231279343832284e-05}, {"id": 516, "seek": 269944, "start": 2707.32, "end": 2714.8, "text": " If you think about it, the language model starts with a bunch of word embeddings, right?", "tokens": [759, 291, 519, 466, 309, 11, 264, 2856, 2316, 3719, 365, 257, 3840, 295, 1349, 12240, 29432, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12087978702960628, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.7231279343832284e-05}, {"id": 517, "seek": 269944, "start": 2714.8, "end": 2718.68, "text": " And so the word embeddings, each row represents a word.", "tokens": [400, 370, 264, 1349, 12240, 29432, 11, 1184, 5386, 8855, 257, 1349, 13], "temperature": 0.0, "avg_logprob": -0.12087978702960628, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.7231279343832284e-05}, {"id": 518, "seek": 269944, "start": 2718.68, "end": 2722.64, "text": " So row 1 needs to represent whatever row 1 in the vocab is.", "tokens": [407, 5386, 502, 2203, 281, 2906, 2035, 5386, 502, 294, 264, 2329, 455, 307, 13], "temperature": 0.0, "avg_logprob": -0.12087978702960628, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.7231279343832284e-05}, {"id": 519, "seek": 269944, "start": 2722.64, "end": 2724.84, "text": " Row 2 is word 2 in the vocab.", "tokens": [20309, 568, 307, 1349, 568, 294, 264, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.12087978702960628, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.7231279343832284e-05}, {"id": 520, "seek": 272484, "start": 2724.84, "end": 2731.92, "text": " So there's no point sharing a pre-trained language model if you don't also share the", "tokens": [407, 456, 311, 572, 935, 5414, 257, 659, 12, 17227, 2001, 2856, 2316, 498, 291, 500, 380, 611, 2073, 264], "temperature": 0.0, "avg_logprob": -0.14001543577327286, "compression_ratio": 1.5602094240837696, "no_speech_prob": 2.9944271773274522e-06}, {"id": 521, "seek": 272484, "start": 2731.92, "end": 2732.92, "text": " vocab.", "tokens": [2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.14001543577327286, "compression_ratio": 1.5602094240837696, "no_speech_prob": 2.9944271773274522e-06}, {"id": 522, "seek": 272484, "start": 2732.92, "end": 2739.92, "text": " So remember the vocab is literally just the list of unique words that we're training on.", "tokens": [407, 1604, 264, 2329, 455, 307, 3736, 445, 264, 1329, 295, 3845, 2283, 300, 321, 434, 3097, 322, 13], "temperature": 0.0, "avg_logprob": -0.14001543577327286, "compression_ratio": 1.5602094240837696, "no_speech_prob": 2.9944271773274522e-06}, {"id": 523, "seek": 272484, "start": 2739.92, "end": 2744.84, "text": " But they've got to be in exactly the same order, otherwise it won't know what's word", "tokens": [583, 436, 600, 658, 281, 312, 294, 2293, 264, 912, 1668, 11, 5911, 309, 1582, 380, 458, 437, 311, 1349], "temperature": 0.0, "avg_logprob": -0.14001543577327286, "compression_ratio": 1.5602094240837696, "no_speech_prob": 2.9944271773274522e-06}, {"id": 524, "seek": 272484, "start": 2744.84, "end": 2747.06, "text": " 1, what's word 2, what's word 3.", "tokens": [502, 11, 437, 311, 1349, 568, 11, 437, 311, 1349, 805, 13], "temperature": 0.0, "avg_logprob": -0.14001543577327286, "compression_ratio": 1.5602094240837696, "no_speech_prob": 2.9944271773274522e-06}, {"id": 525, "seek": 274706, "start": 2747.06, "end": 2755.62, "text": " So we save the FP32 version of the model, and then we save the vocab.", "tokens": [407, 321, 3155, 264, 36655, 11440, 3037, 295, 264, 2316, 11, 293, 550, 321, 3155, 264, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.1194742782206475, "compression_ratio": 1.5263157894736843, "no_speech_prob": 2.1568082502199104e-06}, {"id": 526, "seek": 274706, "start": 2755.62, "end": 2758.04, "text": " And so we've now got two files.", "tokens": [400, 370, 321, 600, 586, 658, 732, 7098, 13], "temperature": 0.0, "avg_logprob": -0.1194742782206475, "compression_ratio": 1.5263157894736843, "no_speech_prob": 2.1568082502199104e-06}, {"id": 527, "seek": 274706, "start": 2758.04, "end": 2765.2799999999997, "text": " And so what happens when you create a language model usually in FastAI, if you say pre-trained", "tokens": [400, 370, 437, 2314, 562, 291, 1884, 257, 2856, 2316, 2673, 294, 15968, 48698, 11, 498, 291, 584, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.1194742782206475, "compression_ratio": 1.5263157894736843, "no_speech_prob": 2.1568082502199104e-06}, {"id": 528, "seek": 274706, "start": 2765.2799999999997, "end": 2770.7599999999998, "text": " equals true, which is default, is if you haven't previously downloaded them, it will download", "tokens": [6915, 2074, 11, 597, 307, 7576, 11, 307, 498, 291, 2378, 380, 8046, 21748, 552, 11, 309, 486, 5484], "temperature": 0.0, "avg_logprob": -0.1194742782206475, "compression_ratio": 1.5263157894736843, "no_speech_prob": 2.1568082502199104e-06}, {"id": 529, "seek": 277076, "start": 2770.76, "end": 2777.4, "text": " from FastAI the pre-trained Wikipedia model and the vocab that it was used.", "tokens": [490, 15968, 48698, 264, 659, 12, 17227, 2001, 28999, 2316, 293, 264, 2329, 455, 300, 309, 390, 1143, 13], "temperature": 0.0, "avg_logprob": -0.12367957137351812, "compression_ratio": 1.4744897959183674, "no_speech_prob": 5.507577043317724e-06}, {"id": 530, "seek": 277076, "start": 2777.4, "end": 2779.38, "text": " So that's why we save both.", "tokens": [407, 300, 311, 983, 321, 3155, 1293, 13], "temperature": 0.0, "avg_logprob": -0.12367957137351812, "compression_ratio": 1.4744897959183674, "no_speech_prob": 5.507577043317724e-06}, {"id": 531, "seek": 277076, "start": 2779.38, "end": 2785.88, "text": " So now we're going to go ahead and do exactly the same thing as we did for IMDB.", "tokens": [407, 586, 321, 434, 516, 281, 352, 2286, 293, 360, 2293, 264, 912, 551, 382, 321, 630, 337, 21463, 27735, 13], "temperature": 0.0, "avg_logprob": -0.12367957137351812, "compression_ratio": 1.4744897959183674, "no_speech_prob": 5.507577043317724e-06}, {"id": 532, "seek": 277076, "start": 2785.88, "end": 2791.32, "text": " So my next problem was I wanted to find a baseline.", "tokens": [407, 452, 958, 1154, 390, 286, 1415, 281, 915, 257, 20518, 13], "temperature": 0.0, "avg_logprob": -0.12367957137351812, "compression_ratio": 1.4744897959183674, "no_speech_prob": 5.507577043317724e-06}, {"id": 533, "seek": 277076, "start": 2791.32, "end": 2795.6800000000003, "text": " So I wanted to know like, well, I needed two things.", "tokens": [407, 286, 1415, 281, 458, 411, 11, 731, 11, 286, 2978, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.12367957137351812, "compression_ratio": 1.4744897959183674, "no_speech_prob": 5.507577043317724e-06}, {"id": 534, "seek": 279568, "start": 2795.68, "end": 2800.66, "text": " I needed some sentiment analysis dataset in Vietnamese, and I needed to find an example", "tokens": [286, 2978, 512, 16149, 5215, 28872, 294, 25934, 11, 293, 286, 2978, 281, 915, 364, 1365], "temperature": 0.0, "avg_logprob": -0.16967698660763827, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.933346538862679e-06}, {"id": 535, "seek": 279568, "start": 2800.66, "end": 2806.04, "text": " of somebody who would try to use that sentiment analysis dataset to predict sentiment.", "tokens": [295, 2618, 567, 576, 853, 281, 764, 300, 16149, 5215, 28872, 281, 6069, 16149, 13], "temperature": 0.0, "avg_logprob": -0.16967698660763827, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.933346538862679e-06}, {"id": 536, "seek": 279568, "start": 2806.04, "end": 2811.7599999999998, "text": " This is often quite difficult in a language you don't know, because all of the information", "tokens": [639, 307, 2049, 1596, 2252, 294, 257, 2856, 291, 500, 380, 458, 11, 570, 439, 295, 264, 1589], "temperature": 0.0, "avg_logprob": -0.16967698660763827, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.933346538862679e-06}, {"id": 537, "seek": 279568, "start": 2811.7599999999998, "end": 2814.12, "text": " is generally in that other language.", "tokens": [307, 5101, 294, 300, 661, 2856, 13], "temperature": 0.0, "avg_logprob": -0.16967698660763827, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.933346538862679e-06}, {"id": 538, "seek": 279568, "start": 2814.12, "end": 2823.7999999999997, "text": " Anyway, so eventually I discovered that there was actually a super popular Vietnamese NLP", "tokens": [5684, 11, 370, 4728, 286, 6941, 300, 456, 390, 767, 257, 1687, 3743, 25934, 426, 45196], "temperature": 0.0, "avg_logprob": -0.16967698660763827, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.933346538862679e-06}, {"id": 539, "seek": 279568, "start": 2823.7999999999997, "end": 2825.5, "text": " competition.", "tokens": [6211, 13], "temperature": 0.0, "avg_logprob": -0.16967698660763827, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.933346538862679e-06}, {"id": 540, "seek": 282550, "start": 2825.5, "end": 2829.96, "text": " And here it is, called AIVIVN.", "tokens": [400, 510, 309, 307, 11, 1219, 7318, 25322, 53, 45, 13], "temperature": 0.0, "avg_logprob": -0.21497801285755785, "compression_ratio": 1.4602272727272727, "no_speech_prob": 4.565837116388138e-06}, {"id": 541, "seek": 282550, "start": 2829.96, "end": 2834.88, "text": " So the nice thing is that we're not stuck, even if we don't know Vietnamese, because", "tokens": [407, 264, 1481, 551, 307, 300, 321, 434, 406, 5541, 11, 754, 498, 321, 500, 380, 458, 25934, 11, 570], "temperature": 0.0, "avg_logprob": -0.21497801285755785, "compression_ratio": 1.4602272727272727, "no_speech_prob": 4.565837116388138e-06}, {"id": 542, "seek": 282550, "start": 2834.88, "end": 2837.14, "text": " we have Google Translate.", "tokens": [321, 362, 3329, 6531, 17593, 13], "temperature": 0.0, "avg_logprob": -0.21497801285755785, "compression_ratio": 1.4602272727272727, "no_speech_prob": 4.565837116388138e-06}, {"id": 543, "seek": 282550, "start": 2837.14, "end": 2844.96, "text": " And so I was able to find out.", "tokens": [400, 370, 286, 390, 1075, 281, 915, 484, 13], "temperature": 0.0, "avg_logprob": -0.21497801285755785, "compression_ratio": 1.4602272727272727, "no_speech_prob": 4.565837116388138e-06}, {"id": 544, "seek": 282550, "start": 2844.96, "end": 2846.44, "text": " Here it is.", "tokens": [1692, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.21497801285755785, "compression_ratio": 1.4602272727272727, "no_speech_prob": 4.565837116388138e-06}, {"id": 545, "seek": 282550, "start": 2846.44, "end": 2850.36, "text": " And I was able to also find out what do these scores actually represent.", "tokens": [400, 286, 390, 1075, 281, 611, 915, 484, 437, 360, 613, 13444, 767, 2906, 13], "temperature": 0.0, "avg_logprob": -0.21497801285755785, "compression_ratio": 1.4602272727272727, "no_speech_prob": 4.565837116388138e-06}, {"id": 546, "seek": 285036, "start": 2850.36, "end": 2860.52, "text": " Okay, it's an F1 score, and even I found a forum.", "tokens": [1033, 11, 309, 311, 364, 479, 16, 6175, 11, 293, 754, 286, 1352, 257, 17542, 13], "temperature": 0.0, "avg_logprob": -0.10673937079024641, "compression_ratio": 1.5266272189349113, "no_speech_prob": 7.071656909829471e-06}, {"id": 547, "seek": 285036, "start": 2860.52, "end": 2861.52, "text": " So this is great.", "tokens": [407, 341, 307, 869, 13], "temperature": 0.0, "avg_logprob": -0.10673937079024641, "compression_ratio": 1.5266272189349113, "no_speech_prob": 7.071656909829471e-06}, {"id": 548, "seek": 285036, "start": 2861.52, "end": 2867.76, "text": " So I was able to follow along and download the data for the competition and find out", "tokens": [407, 286, 390, 1075, 281, 1524, 2051, 293, 5484, 264, 1412, 337, 264, 6211, 293, 915, 484], "temperature": 0.0, "avg_logprob": -0.10673937079024641, "compression_ratio": 1.5266272189349113, "no_speech_prob": 7.071656909829471e-06}, {"id": 549, "seek": 285036, "start": 2867.76, "end": 2875.84, "text": " what the winner of the competition had done, and they even have their first-placed solution", "tokens": [437, 264, 8507, 295, 264, 6211, 632, 1096, 11, 293, 436, 754, 362, 641, 700, 12, 564, 3839, 3827], "temperature": 0.0, "avg_logprob": -0.10673937079024641, "compression_ratio": 1.5266272189349113, "no_speech_prob": 7.071656909829471e-06}, {"id": 550, "seek": 285036, "start": 2875.84, "end": 2876.84, "text": " and so forth.", "tokens": [293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.10673937079024641, "compression_ratio": 1.5266272189349113, "no_speech_prob": 7.071656909829471e-06}, {"id": 551, "seek": 287684, "start": 2876.84, "end": 2882.96, "text": " So the first-placed getters had this pretty complicated solution involving an ensemble", "tokens": [407, 264, 700, 12, 564, 3839, 483, 1559, 632, 341, 1238, 6179, 3827, 17030, 364, 19492], "temperature": 0.0, "avg_logprob": -0.22997049851851029, "compression_ratio": 1.5, "no_speech_prob": 2.144388054148294e-05}, {"id": 552, "seek": 287684, "start": 2882.96, "end": 2888.92, "text": " of four different models, four different deep learning models.", "tokens": [295, 1451, 819, 5245, 11, 1451, 819, 2452, 2539, 5245, 13], "temperature": 0.0, "avg_logprob": -0.22997049851851029, "compression_ratio": 1.5, "no_speech_prob": 2.144388054148294e-05}, {"id": 553, "seek": 287684, "start": 2888.92, "end": 2897.04, "text": " So this is where you can grab the data from, and you can see that they have a test CSV", "tokens": [407, 341, 307, 689, 291, 393, 4444, 264, 1412, 490, 11, 293, 291, 393, 536, 300, 436, 362, 257, 1500, 48814], "temperature": 0.0, "avg_logprob": -0.22997049851851029, "compression_ratio": 1.5, "no_speech_prob": 2.144388054148294e-05}, {"id": 554, "seek": 287684, "start": 2897.04, "end": 2904.88, "text": " and a trained CSV, like so.", "tokens": [293, 257, 8895, 48814, 11, 411, 370, 13], "temperature": 0.0, "avg_logprob": -0.22997049851851029, "compression_ratio": 1.5, "no_speech_prob": 2.144388054148294e-05}, {"id": 555, "seek": 290488, "start": 2904.88, "end": 2916.08, "text": " And so each one basically contains a unique number, then the review, and then 0 or 1,", "tokens": [400, 370, 1184, 472, 1936, 8306, 257, 3845, 1230, 11, 550, 264, 3131, 11, 293, 550, 1958, 420, 502, 11], "temperature": 0.0, "avg_logprob": -0.12929994766026326, "compression_ratio": 1.4517766497461928, "no_speech_prob": 3.321090480312705e-05}, {"id": 556, "seek": 290488, "start": 2916.08, "end": 2918.84, "text": " depending on whether it's positive or negative.", "tokens": [5413, 322, 1968, 309, 311, 3353, 420, 3671, 13], "temperature": 0.0, "avg_logprob": -0.12929994766026326, "compression_ratio": 1.4517766497461928, "no_speech_prob": 3.321090480312705e-05}, {"id": 557, "seek": 290488, "start": 2918.84, "end": 2923.2400000000002, "text": " This is quite different to the IMDB dataset, because the IMDB dataset, the reviews were", "tokens": [639, 307, 1596, 819, 281, 264, 21463, 27735, 28872, 11, 570, 264, 21463, 27735, 28872, 11, 264, 10229, 645], "temperature": 0.0, "avg_logprob": -0.12929994766026326, "compression_ratio": 1.4517766497461928, "no_speech_prob": 3.321090480312705e-05}, {"id": 558, "seek": 290488, "start": 2923.2400000000002, "end": 2926.96, "text": " generally about 1500 to 2000 words.", "tokens": [5101, 466, 22671, 281, 8132, 2283, 13], "temperature": 0.0, "avg_logprob": -0.12929994766026326, "compression_ratio": 1.4517766497461928, "no_speech_prob": 3.321090480312705e-05}, {"id": 559, "seek": 290488, "start": 2926.96, "end": 2929.04, "text": " These ones are much shorter.", "tokens": [1981, 2306, 366, 709, 11639, 13], "temperature": 0.0, "avg_logprob": -0.12929994766026326, "compression_ratio": 1.4517766497461928, "no_speech_prob": 3.321090480312705e-05}, {"id": 560, "seek": 292904, "start": 2929.04, "end": 2937.08, "text": " So recurrent neural networks are particularly effective for longer texts.", "tokens": [407, 18680, 1753, 18161, 9590, 366, 4098, 4942, 337, 2854, 15765, 13], "temperature": 0.0, "avg_logprob": -0.10094082033311999, "compression_ratio": 1.439153439153439, "no_speech_prob": 6.962196039239643e-06}, {"id": 561, "seek": 292904, "start": 2937.08, "end": 2944.32, "text": " Shorter texts are often not so difficult to handle, and RNNs do well, but they're not", "tokens": [1160, 6122, 15765, 366, 2049, 406, 370, 2252, 281, 4813, 11, 293, 45702, 45, 82, 360, 731, 11, 457, 436, 434, 406], "temperature": 0.0, "avg_logprob": -0.10094082033311999, "compression_ratio": 1.439153439153439, "no_speech_prob": 6.962196039239643e-06}, {"id": 562, "seek": 292904, "start": 2944.32, "end": 2949.36, "text": " necessarily going to be as exceptional.", "tokens": [4725, 516, 281, 312, 382, 19279, 13], "temperature": 0.0, "avg_logprob": -0.10094082033311999, "compression_ratio": 1.439153439153439, "no_speech_prob": 6.962196039239643e-06}, {"id": 563, "seek": 292904, "start": 2949.36, "end": 2958.62, "text": " So as you can see, we can just go pandas.read.csv to read that CSV file.", "tokens": [407, 382, 291, 393, 536, 11, 321, 393, 445, 352, 4565, 296, 13, 2538, 13, 14368, 85, 281, 1401, 300, 48814, 3991, 13], "temperature": 0.0, "avg_logprob": -0.10094082033311999, "compression_ratio": 1.439153439153439, "no_speech_prob": 6.962196039239643e-06}, {"id": 564, "seek": 295862, "start": 2958.62, "end": 2963.68, "text": " And so we did that for the training file, and we did that for the test file.", "tokens": [400, 370, 321, 630, 300, 337, 264, 3097, 3991, 11, 293, 321, 630, 300, 337, 264, 1500, 3991, 13], "temperature": 0.0, "avg_logprob": -0.10570210273112726, "compression_ratio": 1.8258928571428572, "no_speech_prob": 2.668780871317722e-05}, {"id": 565, "seek": 295862, "start": 2963.68, "end": 2969.4, "text": " For the language model, I can use the test file, even though the test file has no labels.", "tokens": [1171, 264, 2856, 2316, 11, 286, 393, 764, 264, 1500, 3991, 11, 754, 1673, 264, 1500, 3991, 575, 572, 16949, 13], "temperature": 0.0, "avg_logprob": -0.10570210273112726, "compression_ratio": 1.8258928571428572, "no_speech_prob": 2.668780871317722e-05}, {"id": 566, "seek": 295862, "start": 2969.4, "end": 2973.7999999999997, "text": " That's the point of this competition, is you're meant to predict the labels and submit them", "tokens": [663, 311, 264, 935, 295, 341, 6211, 11, 307, 291, 434, 4140, 281, 6069, 264, 16949, 293, 10315, 552], "temperature": 0.0, "avg_logprob": -0.10570210273112726, "compression_ratio": 1.8258928571428572, "no_speech_prob": 2.668780871317722e-05}, {"id": 567, "seek": 295862, "start": 2973.7999999999997, "end": 2975.16, "text": " to the competition.", "tokens": [281, 264, 6211, 13], "temperature": 0.0, "avg_logprob": -0.10570210273112726, "compression_ratio": 1.8258928571428572, "no_speech_prob": 2.668780871317722e-05}, {"id": 568, "seek": 295862, "start": 2975.16, "end": 2982.92, "text": " So all I did was I just concatenated them together, and what pandas does is it'll just", "tokens": [407, 439, 286, 630, 390, 286, 445, 1588, 7186, 770, 552, 1214, 11, 293, 437, 4565, 296, 775, 307, 309, 603, 445], "temperature": 0.0, "avg_logprob": -0.10570210273112726, "compression_ratio": 1.8258928571428572, "no_speech_prob": 2.668780871317722e-05}, {"id": 569, "seek": 295862, "start": 2982.92, "end": 2988.3599999999997, "text": " set the labels to missing for the test set.", "tokens": [992, 264, 16949, 281, 5361, 337, 264, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.10570210273112726, "compression_ratio": 1.8258928571428572, "no_speech_prob": 2.668780871317722e-05}, {"id": 570, "seek": 298836, "start": 2988.36, "end": 2996.08, "text": " So now I had a data frame, and so FastAI has a text list from data frame, so we can now", "tokens": [407, 586, 286, 632, 257, 1412, 3920, 11, 293, 370, 15968, 48698, 575, 257, 2487, 1329, 490, 1412, 3920, 11, 370, 321, 393, 586], "temperature": 0.0, "avg_logprob": -0.16039639641256892, "compression_ratio": 1.6395939086294415, "no_speech_prob": 9.368639439344406e-06}, {"id": 571, "seek": 298836, "start": 2996.08, "end": 3005.32, "text": " basically use exactly the same approach as the IMDB to create a language model data bunch,", "tokens": [1936, 764, 2293, 264, 912, 3109, 382, 264, 21463, 27735, 281, 1884, 257, 2856, 2316, 1412, 3840, 11], "temperature": 0.0, "avg_logprob": -0.16039639641256892, "compression_ratio": 1.6395939086294415, "no_speech_prob": 9.368639439344406e-06}, {"id": 572, "seek": 298836, "start": 3005.32, "end": 3011.1200000000003, "text": " except instead of from path, it's from data frame.", "tokens": [3993, 2602, 295, 490, 3100, 11, 309, 311, 490, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.16039639641256892, "compression_ratio": 1.6395939086294415, "no_speech_prob": 9.368639439344406e-06}, {"id": 573, "seek": 298836, "start": 3011.1200000000003, "end": 3016.6800000000003, "text": " So then we just create a language model learner, just like before, but now we don't just pass", "tokens": [407, 550, 321, 445, 1884, 257, 2856, 2316, 33347, 11, 445, 411, 949, 11, 457, 586, 321, 500, 380, 445, 1320], "temperature": 0.0, "avg_logprob": -0.16039639641256892, "compression_ratio": 1.6395939086294415, "no_speech_prob": 9.368639439344406e-06}, {"id": 574, "seek": 301668, "start": 3016.68, "end": 3025.08, "text": " pre-trained equals true, but we say pre-trained filenames equals, and we pass in an array", "tokens": [659, 12, 17227, 2001, 6915, 2074, 11, 457, 321, 584, 659, 12, 17227, 2001, 1387, 268, 1632, 6915, 11, 293, 321, 1320, 294, 364, 10225], "temperature": 0.0, "avg_logprob": -0.09964450008897896, "compression_ratio": 1.7005988023952097, "no_speech_prob": 6.04886145083583e-06}, {"id": 575, "seek": 301668, "start": 3025.08, "end": 3029.3999999999996, "text": " of the model file name and the vocab file name.", "tokens": [295, 264, 2316, 3991, 1315, 293, 264, 2329, 455, 3991, 1315, 13], "temperature": 0.0, "avg_logprob": -0.09964450008897896, "compression_ratio": 1.7005988023952097, "no_speech_prob": 6.04886145083583e-06}, {"id": 576, "seek": 301668, "start": 3029.3999999999996, "end": 3037.08, "text": " So that's those two files in this array we had here.", "tokens": [407, 300, 311, 729, 732, 7098, 294, 341, 10225, 321, 632, 510, 13], "temperature": 0.0, "avg_logprob": -0.09964450008897896, "compression_ratio": 1.7005988023952097, "no_speech_prob": 6.04886145083583e-06}, {"id": 577, "seek": 301668, "start": 3037.08, "end": 3044.8799999999997, "text": " So now that's how we can use our pre-trained Vietnamese model to create a fine-tuned language", "tokens": [407, 586, 300, 311, 577, 321, 393, 764, 527, 659, 12, 17227, 2001, 25934, 2316, 281, 1884, 257, 2489, 12, 83, 43703, 2856], "temperature": 0.0, "avg_logprob": -0.09964450008897896, "compression_ratio": 1.7005988023952097, "no_speech_prob": 6.04886145083583e-06}, {"id": 578, "seek": 304488, "start": 3044.88, "end": 3047.6800000000003, "text": " model for Vietnamese sentiment analysis.", "tokens": [2316, 337, 25934, 16149, 5215, 13], "temperature": 0.0, "avg_logprob": -0.14587641821967232, "compression_ratio": 1.52803738317757, "no_speech_prob": 1.112545305659296e-05}, {"id": 579, "seek": 304488, "start": 3047.6800000000003, "end": 3052.44, "text": " And then we just go ahead and fit this code's identical to IMDB.", "tokens": [400, 550, 321, 445, 352, 2286, 293, 3318, 341, 3089, 311, 14800, 281, 21463, 27735, 13], "temperature": 0.0, "avg_logprob": -0.14587641821967232, "compression_ratio": 1.52803738317757, "no_speech_prob": 1.112545305659296e-05}, {"id": 580, "seek": 304488, "start": 3052.44, "end": 3059.9, "text": " So you can see we're getting 37% accuracy for Vietnamese movie reviews.", "tokens": [407, 291, 393, 536, 321, 434, 1242, 13435, 4, 14170, 337, 25934, 3169, 10229, 13], "temperature": 0.0, "avg_logprob": -0.14587641821967232, "compression_ratio": 1.52803738317757, "no_speech_prob": 1.112545305659296e-05}, {"id": 581, "seek": 304488, "start": 3059.9, "end": 3064.84, "text": " We save it, and we can save the encoder, and again, just like before, we can then create", "tokens": [492, 3155, 309, 11, 293, 321, 393, 3155, 264, 2058, 19866, 11, 293, 797, 11, 445, 411, 949, 11, 321, 393, 550, 1884], "temperature": 0.0, "avg_logprob": -0.14587641821967232, "compression_ratio": 1.52803738317757, "no_speech_prob": 1.112545305659296e-05}, {"id": 582, "seek": 304488, "start": 3064.84, "end": 3069.56, "text": " the classifier data bunch.", "tokens": [264, 1508, 9902, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.14587641821967232, "compression_ratio": 1.52803738317757, "no_speech_prob": 1.112545305659296e-05}, {"id": 583, "seek": 304488, "start": 3069.56, "end": 3074.48, "text": " Again we're going to use from df.", "tokens": [3764, 321, 434, 516, 281, 764, 490, 274, 69, 13], "temperature": 0.0, "avg_logprob": -0.14587641821967232, "compression_ratio": 1.52803738317757, "no_speech_prob": 1.112545305659296e-05}, {"id": 584, "seek": 307448, "start": 3074.48, "end": 3077.08, "text": " And then we can create our learner.", "tokens": [400, 550, 321, 393, 1884, 527, 33347, 13], "temperature": 0.0, "avg_logprob": -0.19564477135153377, "compression_ratio": 1.5023474178403755, "no_speech_prob": 1.4970863958296832e-05}, {"id": 585, "seek": 307448, "start": 3077.08, "end": 3083.6, "text": " Now in this case, the competition was judged not on accuracy, but on F1.", "tokens": [823, 294, 341, 1389, 11, 264, 6211, 390, 27485, 406, 322, 14170, 11, 457, 322, 479, 16, 13], "temperature": 0.0, "avg_logprob": -0.19564477135153377, "compression_ratio": 1.5023474178403755, "no_speech_prob": 1.4970863958296832e-05}, {"id": 586, "seek": 307448, "start": 3083.6, "end": 3085.76, "text": " Have you done F1 yet?", "tokens": [3560, 291, 1096, 479, 16, 1939, 30], "temperature": 0.0, "avg_logprob": -0.19564477135153377, "compression_ratio": 1.5023474178403755, "no_speech_prob": 1.4970863958296832e-05}, {"id": 587, "seek": 307448, "start": 3085.76, "end": 3087.12, "text": " Yep, great.", "tokens": [7010, 11, 869, 13], "temperature": 0.0, "avg_logprob": -0.19564477135153377, "compression_ratio": 1.5023474178403755, "no_speech_prob": 1.4970863958296832e-05}, {"id": 588, "seek": 307448, "start": 3087.12, "end": 3092.72, "text": " So F1 is the average of precision and recall.", "tokens": [407, 479, 16, 307, 264, 4274, 295, 18356, 293, 9901, 13], "temperature": 0.0, "avg_logprob": -0.19564477135153377, "compression_ratio": 1.5023474178403755, "no_speech_prob": 1.4970863958296832e-05}, {"id": 589, "seek": 307448, "start": 3092.72, "end": 3098.56, "text": " There isn't a binary F1 built into Fast.ai as far as I know, but there is one built into", "tokens": [821, 1943, 380, 257, 17434, 479, 16, 3094, 666, 15968, 13, 1301, 382, 1400, 382, 286, 458, 11, 457, 456, 307, 472, 3094, 666], "temperature": 0.0, "avg_logprob": -0.19564477135153377, "compression_ratio": 1.5023474178403755, "no_speech_prob": 1.4970863958296832e-05}, {"id": 590, "seek": 307448, "start": 3098.56, "end": 3102.16, "text": " sklearn, so we can import it from sklearn.", "tokens": [1110, 306, 1083, 11, 370, 321, 393, 974, 309, 490, 1110, 306, 1083, 13], "temperature": 0.0, "avg_logprob": -0.19564477135153377, "compression_ratio": 1.5023474178403755, "no_speech_prob": 1.4970863958296832e-05}, {"id": 591, "seek": 310216, "start": 3102.16, "end": 3109.52, "text": " sklearn assumes that it's getting NumPy arrays, not torched tensors.", "tokens": [1110, 306, 1083, 37808, 300, 309, 311, 1242, 22592, 47, 88, 41011, 11, 406, 3930, 19318, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.17964684208737144, "compression_ratio": 1.4728260869565217, "no_speech_prob": 7.646443918929435e-06}, {"id": 592, "seek": 310216, "start": 3109.52, "end": 3116.7599999999998, "text": " So you can create a function, I created one called F1, which simply calls the sklearn", "tokens": [407, 291, 393, 1884, 257, 2445, 11, 286, 2942, 472, 1219, 479, 16, 11, 597, 2935, 5498, 264, 1110, 306, 1083], "temperature": 0.0, "avg_logprob": -0.17964684208737144, "compression_ratio": 1.4728260869565217, "no_speech_prob": 7.646443918929435e-06}, {"id": 593, "seek": 310216, "start": 3116.7599999999998, "end": 3124.54, "text": " version, and if you add the np func decorator from Fast.ai, it converts this to work with", "tokens": [3037, 11, 293, 498, 291, 909, 264, 33808, 1019, 66, 7919, 1639, 490, 15968, 13, 1301, 11, 309, 38874, 341, 281, 589, 365], "temperature": 0.0, "avg_logprob": -0.17964684208737144, "compression_ratio": 1.4728260869565217, "no_speech_prob": 7.646443918929435e-06}, {"id": 594, "seek": 310216, "start": 3124.54, "end": 3126.68, "text": " tensors instead of arrays.", "tokens": [10688, 830, 2602, 295, 41011, 13], "temperature": 0.0, "avg_logprob": -0.17964684208737144, "compression_ratio": 1.4728260869565217, "no_speech_prob": 7.646443918929435e-06}, {"id": 595, "seek": 312668, "start": 3126.68, "end": 3134.12, "text": " So this is a nice little trick to use any sklearn metric as a PyTorch metric.", "tokens": [407, 341, 307, 257, 1481, 707, 4282, 281, 764, 604, 1110, 306, 1083, 20678, 382, 257, 9953, 51, 284, 339, 20678, 13], "temperature": 0.0, "avg_logprob": -0.13096260202342067, "compression_ratio": 1.4676616915422886, "no_speech_prob": 5.682402388629271e-06}, {"id": 596, "seek": 312668, "start": 3134.12, "end": 3140.0, "text": " So now we can just pass in metrics equals accuracy, F1.", "tokens": [407, 586, 321, 393, 445, 1320, 294, 16367, 6915, 14170, 11, 479, 16, 13], "temperature": 0.0, "avg_logprob": -0.13096260202342067, "compression_ratio": 1.4676616915422886, "no_speech_prob": 5.682402388629271e-06}, {"id": 597, "seek": 312668, "start": 3140.0, "end": 3144.8799999999997, "text": " Other than that, this is identical to IMDB, and we can now go ahead and train in the same", "tokens": [5358, 813, 300, 11, 341, 307, 14800, 281, 21463, 27735, 11, 293, 321, 393, 586, 352, 2286, 293, 3847, 294, 264, 912], "temperature": 0.0, "avg_logprob": -0.13096260202342067, "compression_ratio": 1.4676616915422886, "no_speech_prob": 5.682402388629271e-06}, {"id": 598, "seek": 312668, "start": 3144.8799999999997, "end": 3154.74, "text": " way that we did before, and we end up with somewhere around 89, 90-ish.", "tokens": [636, 300, 321, 630, 949, 11, 293, 321, 917, 493, 365, 4079, 926, 31877, 11, 4289, 12, 742, 13], "temperature": 0.0, "avg_logprob": -0.13096260202342067, "compression_ratio": 1.4676616915422886, "no_speech_prob": 5.682402388629271e-06}, {"id": 599, "seek": 315474, "start": 3154.74, "end": 3172.12, "text": " And so the competition's top three was 90, 89, 89, so we're like maybe slightly worse,", "tokens": [400, 370, 264, 6211, 311, 1192, 1045, 390, 4289, 11, 31877, 11, 31877, 11, 370, 321, 434, 411, 1310, 4748, 5324, 11], "temperature": 0.0, "avg_logprob": -0.30361489149240345, "compression_ratio": 1.0238095238095237, "no_speech_prob": 4.985859777661972e-05}, {"id": 600, "seek": 317212, "start": 3172.12, "end": 3185.4, "text": " tiny bit worse than, let's see, where was it, here it is, let's get the exact number,", "tokens": [5870, 857, 5324, 813, 11, 718, 311, 536, 11, 689, 390, 309, 11, 510, 309, 307, 11, 718, 311, 483, 264, 1900, 1230, 11], "temperature": 0.0, "avg_logprob": -0.21237866290203936, "compression_ratio": 1.488235294117647, "no_speech_prob": 4.5395612687570974e-05}, {"id": 601, "seek": 317212, "start": 3185.4, "end": 3192.12, "text": " depending on whether it's the public or the private, 90, 0 or 901, and we're somewhere", "tokens": [5413, 322, 1968, 309, 311, 264, 1908, 420, 264, 4551, 11, 4289, 11, 1958, 420, 4289, 16, 11, 293, 321, 434, 4079], "temperature": 0.0, "avg_logprob": -0.21237866290203936, "compression_ratio": 1.488235294117647, "no_speech_prob": 4.5395612687570974e-05}, {"id": 602, "seek": 317212, "start": 3192.12, "end": 3198.46, "text": " around 901 or 89, so yeah, we're maybe in first place, but it's not quite clear.", "tokens": [926, 4289, 16, 420, 31877, 11, 370, 1338, 11, 321, 434, 1310, 294, 700, 1081, 11, 457, 309, 311, 406, 1596, 1850, 13], "temperature": 0.0, "avg_logprob": -0.21237866290203936, "compression_ratio": 1.488235294117647, "no_speech_prob": 4.5395612687570974e-05}, {"id": 603, "seek": 319846, "start": 3198.46, "end": 3203.2400000000002, "text": " So we want to try and do better because we don't do these things to come second.", "tokens": [407, 321, 528, 281, 853, 293, 360, 1101, 570, 321, 500, 380, 360, 613, 721, 281, 808, 1150, 13], "temperature": 0.0, "avg_logprob": -0.10337351049695696, "compression_ratio": 1.6705426356589148, "no_speech_prob": 3.6119618016527966e-06}, {"id": 604, "seek": 319846, "start": 3203.2400000000002, "end": 3206.2, "text": " So let's do the next trick.", "tokens": [407, 718, 311, 360, 264, 958, 4282, 13], "temperature": 0.0, "avg_logprob": -0.10337351049695696, "compression_ratio": 1.6705426356589148, "no_speech_prob": 3.6119618016527966e-06}, {"id": 605, "seek": 319846, "start": 3206.2, "end": 3212.02, "text": " So remember I told you that the winners of this competition created an ensemble of four", "tokens": [407, 1604, 286, 1907, 291, 300, 264, 17193, 295, 341, 6211, 2942, 364, 19492, 295, 1451], "temperature": 0.0, "avg_logprob": -0.10337351049695696, "compression_ratio": 1.6705426356589148, "no_speech_prob": 3.6119618016527966e-06}, {"id": 606, "seek": 319846, "start": 3212.02, "end": 3216.52, "text": " different deep learning models, so it's going to be difficult for us to beat that with just", "tokens": [819, 2452, 2539, 5245, 11, 370, 309, 311, 516, 281, 312, 2252, 337, 505, 281, 4224, 300, 365, 445], "temperature": 0.0, "avg_logprob": -0.10337351049695696, "compression_ratio": 1.6705426356589148, "no_speech_prob": 3.6119618016527966e-06}, {"id": 607, "seek": 319846, "start": 3216.52, "end": 3217.52, "text": " one.", "tokens": [472, 13], "temperature": 0.0, "avg_logprob": -0.10337351049695696, "compression_ratio": 1.6705426356589148, "no_speech_prob": 3.6119618016527966e-06}, {"id": 608, "seek": 319846, "start": 3217.52, "end": 3221.0, "text": " We seem to be close, but we want to be sure.", "tokens": [492, 1643, 281, 312, 1998, 11, 457, 321, 528, 281, 312, 988, 13], "temperature": 0.0, "avg_logprob": -0.10337351049695696, "compression_ratio": 1.6705426356589148, "no_speech_prob": 3.6119618016527966e-06}, {"id": 609, "seek": 319846, "start": 3221.0, "end": 3227.12, "text": " So there's a trick that you can improve any model by also creating what's called a backwards", "tokens": [407, 456, 311, 257, 4282, 300, 291, 393, 3470, 604, 2316, 538, 611, 4084, 437, 311, 1219, 257, 12204], "temperature": 0.0, "avg_logprob": -0.10337351049695696, "compression_ratio": 1.6705426356589148, "no_speech_prob": 3.6119618016527966e-06}, {"id": 610, "seek": 322712, "start": 3227.12, "end": 3229.4, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.187449312210083, "compression_ratio": 1.525974025974026, "no_speech_prob": 1.1300729056529235e-05}, {"id": 611, "seek": 322712, "start": 3229.4, "end": 3234.7999999999997, "text": " A backwards language model, for example, is something which you don't feed it a Wikipedia", "tokens": [316, 12204, 2856, 2316, 11, 337, 1365, 11, 307, 746, 597, 291, 500, 380, 3154, 309, 257, 28999], "temperature": 0.0, "avg_logprob": -0.187449312210083, "compression_ratio": 1.525974025974026, "no_speech_prob": 1.1300729056529235e-05}, {"id": 612, "seek": 322712, "start": 3234.7999999999997, "end": 3240.4, "text": " article, you feed it a Wikipedia article in reverse.", "tokens": [7222, 11, 291, 3154, 309, 257, 28999, 7222, 294, 9943, 13], "temperature": 0.0, "avg_logprob": -0.187449312210083, "compression_ratio": 1.525974025974026, "no_speech_prob": 1.1300729056529235e-05}, {"id": 613, "seek": 322712, "start": 3240.4, "end": 3250.7599999999998, "text": " So if you just pass backwards equals true to your data bunch, then what it will do is", "tokens": [407, 498, 291, 445, 1320, 12204, 6915, 2074, 281, 428, 1412, 3840, 11, 550, 437, 309, 486, 360, 307], "temperature": 0.0, "avg_logprob": -0.187449312210083, "compression_ratio": 1.525974025974026, "no_speech_prob": 1.1300729056529235e-05}, {"id": 614, "seek": 325076, "start": 3250.76, "end": 3257.88, "text": " it will create a data bunch where every Wikipedia article is backwards.", "tokens": [309, 486, 1884, 257, 1412, 3840, 689, 633, 28999, 7222, 307, 12204, 13], "temperature": 0.0, "avg_logprob": -0.0834239363670349, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.209861352435837e-06}, {"id": 615, "seek": 325076, "start": 3257.88, "end": 3265.6400000000003, "text": " And so a language model trained on that will learn to predict the previous word of a sentence", "tokens": [400, 370, 257, 2856, 2316, 8895, 322, 300, 486, 1466, 281, 6069, 264, 3894, 1349, 295, 257, 8174], "temperature": 0.0, "avg_logprob": -0.0834239363670349, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.209861352435837e-06}, {"id": 616, "seek": 325076, "start": 3265.6400000000003, "end": 3271.44, "text": " given the last words, which is kind of like a weird thing to do, but if you think about", "tokens": [2212, 264, 1036, 2283, 11, 597, 307, 733, 295, 411, 257, 3657, 551, 281, 360, 11, 457, 498, 291, 519, 466], "temperature": 0.0, "avg_logprob": -0.0834239363670349, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.209861352435837e-06}, {"id": 617, "seek": 325076, "start": 3271.44, "end": 3279.28, "text": " it, in language, the next words very often tell you context about what the previous word", "tokens": [309, 11, 294, 2856, 11, 264, 958, 2283, 588, 2049, 980, 291, 4319, 466, 437, 264, 3894, 1349], "temperature": 0.0, "avg_logprob": -0.0834239363670349, "compression_ratio": 1.6521739130434783, "no_speech_prob": 1.209861352435837e-06}, {"id": 618, "seek": 327928, "start": 3279.28, "end": 3281.26, "text": " might have been or how to interpret it.", "tokens": [1062, 362, 668, 420, 577, 281, 7302, 309, 13], "temperature": 0.0, "avg_logprob": -0.17611026763916016, "compression_ratio": 1.6872246696035242, "no_speech_prob": 8.01340866019018e-06}, {"id": 619, "seek": 327928, "start": 3281.26, "end": 3286.7200000000003, "text": " So predicting the previous word of a sentence is likely to be just as useful as predicting", "tokens": [407, 32884, 264, 3894, 1349, 295, 257, 8174, 307, 3700, 281, 312, 445, 382, 4420, 382, 32884], "temperature": 0.0, "avg_logprob": -0.17611026763916016, "compression_ratio": 1.6872246696035242, "no_speech_prob": 8.01340866019018e-06}, {"id": 620, "seek": 327928, "start": 3286.7200000000003, "end": 3288.6800000000003, "text": " the next word of a sentence.", "tokens": [264, 958, 1349, 295, 257, 8174, 13], "temperature": 0.0, "avg_logprob": -0.17611026763916016, "compression_ratio": 1.6872246696035242, "no_speech_prob": 8.01340866019018e-06}, {"id": 621, "seek": 327928, "start": 3288.6800000000003, "end": 3296.4, "text": " So what I did was I literally took the NNVietnamese notebook and I said file, make a copy, and", "tokens": [407, 437, 286, 630, 390, 286, 3736, 1890, 264, 426, 45, 53, 10507, 1130, 21060, 293, 286, 848, 3991, 11, 652, 257, 5055, 11, 293], "temperature": 0.0, "avg_logprob": -0.17611026763916016, "compression_ratio": 1.6872246696035242, "no_speech_prob": 8.01340866019018e-06}, {"id": 622, "seek": 327928, "start": 3296.4, "end": 3301.6400000000003, "text": " then I renamed it, file, rename to vietnamese BWD for backward.", "tokens": [550, 286, 40949, 309, 11, 3991, 11, 36741, 281, 371, 10507, 1130, 363, 54, 35, 337, 23897, 13], "temperature": 0.0, "avg_logprob": -0.17611026763916016, "compression_ratio": 1.6872246696035242, "no_speech_prob": 8.01340866019018e-06}, {"id": 623, "seek": 327928, "start": 3301.6400000000003, "end": 3307.84, "text": " I then added backwards equals true and then I just ran it again.", "tokens": [286, 550, 3869, 12204, 6915, 2074, 293, 550, 286, 445, 5872, 309, 797, 13], "temperature": 0.0, "avg_logprob": -0.17611026763916016, "compression_ratio": 1.6872246696035242, "no_speech_prob": 8.01340866019018e-06}, {"id": 624, "seek": 330784, "start": 3307.84, "end": 3312.96, "text": " And interestingly, vietnamese is slightly easier to predict backwards than forwards,", "tokens": [400, 25873, 11, 371, 10507, 1130, 307, 4748, 3571, 281, 6069, 12204, 813, 30126, 11], "temperature": 0.0, "avg_logprob": -0.12653180922584972, "compression_ratio": 1.6372093023255814, "no_speech_prob": 3.024110264959745e-05}, {"id": 625, "seek": 330784, "start": 3312.96, "end": 3319.44, "text": " so maybe vietnamese speakers should consider saying everything backwards from now on.", "tokens": [370, 1310, 371, 10507, 1130, 9518, 820, 1949, 1566, 1203, 12204, 490, 586, 322, 13], "temperature": 0.0, "avg_logprob": -0.12653180922584972, "compression_ratio": 1.6372093023255814, "no_speech_prob": 3.024110264959745e-05}, {"id": 626, "seek": 330784, "start": 3319.44, "end": 3320.96, "text": " So now I've got my backwards model.", "tokens": [407, 586, 286, 600, 658, 452, 12204, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12653180922584972, "compression_ratio": 1.6372093023255814, "no_speech_prob": 3.024110264959745e-05}, {"id": 627, "seek": 330784, "start": 3320.96, "end": 3325.2000000000003, "text": " So when I saved it, I made sure that all my file names had underscore backward at the", "tokens": [407, 562, 286, 6624, 309, 11, 286, 1027, 988, 300, 439, 452, 3991, 5288, 632, 37556, 23897, 412, 264], "temperature": 0.0, "avg_logprob": -0.12653180922584972, "compression_ratio": 1.6372093023255814, "no_speech_prob": 3.024110264959745e-05}, {"id": 628, "seek": 330784, "start": 3325.2000000000003, "end": 3327.04, "text": " end.", "tokens": [917, 13], "temperature": 0.0, "avg_logprob": -0.12653180922584972, "compression_ratio": 1.6372093023255814, "no_speech_prob": 3.024110264959745e-05}, {"id": 629, "seek": 330784, "start": 3327.04, "end": 3333.52, "text": " So I now have a pre-trained Wikipedia backwards model.", "tokens": [407, 286, 586, 362, 257, 659, 12, 17227, 2001, 28999, 12204, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12653180922584972, "compression_ratio": 1.6372093023255814, "no_speech_prob": 3.024110264959745e-05}, {"id": 630, "seek": 333352, "start": 3333.52, "end": 3339.24, "text": " So now for my fine-tuned language model, I did the same thing, I grabbed my data frames,", "tokens": [407, 586, 337, 452, 2489, 12, 83, 43703, 2856, 2316, 11, 286, 630, 264, 912, 551, 11, 286, 18607, 452, 1412, 12083, 11], "temperature": 0.0, "avg_logprob": -0.14899618197710085, "compression_ratio": 1.6529411764705881, "no_speech_prob": 1.7502383343526162e-05}, {"id": 631, "seek": 333352, "start": 3339.24, "end": 3342.42, "text": " but again I just added backwards equals true.", "tokens": [457, 797, 286, 445, 3869, 12204, 6915, 2074, 13], "temperature": 0.0, "avg_logprob": -0.14899618197710085, "compression_ratio": 1.6529411764705881, "no_speech_prob": 1.7502383343526162e-05}, {"id": 632, "seek": 333352, "start": 3342.42, "end": 3354.36, "text": " So now I've got a vietnamese review fine-tuned backwards language model, and so I save that.", "tokens": [407, 586, 286, 600, 658, 257, 371, 10507, 1130, 3131, 2489, 12, 83, 43703, 12204, 2856, 2316, 11, 293, 370, 286, 3155, 300, 13], "temperature": 0.0, "avg_logprob": -0.14899618197710085, "compression_ratio": 1.6529411764705881, "no_speech_prob": 1.7502383343526162e-05}, {"id": 633, "seek": 333352, "start": 3354.36, "end": 3357.94, "text": " And then my classifier, again, backwards equals true.", "tokens": [400, 550, 452, 1508, 9902, 11, 797, 11, 12204, 6915, 2074, 13], "temperature": 0.0, "avg_logprob": -0.14899618197710085, "compression_ratio": 1.6529411764705881, "no_speech_prob": 1.7502383343526162e-05}, {"id": 634, "seek": 335794, "start": 3357.94, "end": 3368.76, "text": " So I now have something, so then I load my encoder, and so I now have something which", "tokens": [407, 286, 586, 362, 746, 11, 370, 550, 286, 3677, 452, 2058, 19866, 11, 293, 370, 286, 586, 362, 746, 597], "temperature": 0.0, "avg_logprob": -0.12244106810769917, "compression_ratio": 1.6243386243386244, "no_speech_prob": 7.4111267167609185e-06}, {"id": 635, "seek": 335794, "start": 3368.76, "end": 3379.3, "text": " can learn, predict with an accuracy of 89%, whether or not a review is positive or negative", "tokens": [393, 1466, 11, 6069, 365, 364, 14170, 295, 31877, 8923, 1968, 420, 406, 257, 3131, 307, 3353, 420, 3671], "temperature": 0.0, "avg_logprob": -0.12244106810769917, "compression_ratio": 1.6243386243386244, "no_speech_prob": 7.4111267167609185e-06}, {"id": 636, "seek": 335794, "start": 3379.3, "end": 3382.6, "text": " based on that review in reversed order.", "tokens": [2361, 322, 300, 3131, 294, 30563, 1668, 13], "temperature": 0.0, "avg_logprob": -0.12244106810769917, "compression_ratio": 1.6243386243386244, "no_speech_prob": 7.4111267167609185e-06}, {"id": 637, "seek": 335794, "start": 3382.6, "end": 3387.0, "text": " So I've now got two models, one for forwards, one for backwards, and so what we can do is", "tokens": [407, 286, 600, 586, 658, 732, 5245, 11, 472, 337, 30126, 11, 472, 337, 12204, 11, 293, 370, 437, 321, 393, 360, 307], "temperature": 0.0, "avg_logprob": -0.12244106810769917, "compression_ratio": 1.6243386243386244, "no_speech_prob": 7.4111267167609185e-06}, {"id": 638, "seek": 338700, "start": 3387.0, "end": 3389.68, "text": " we can simply ensemble them together.", "tokens": [321, 393, 2935, 19492, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.07610978355890588, "compression_ratio": 1.7900552486187846, "no_speech_prob": 2.6687590434448794e-05}, {"id": 639, "seek": 338700, "start": 3389.68, "end": 3395.0, "text": " So an ensemble generally means you combine the predictions of two models, often by just", "tokens": [407, 364, 19492, 5101, 1355, 291, 10432, 264, 21264, 295, 732, 5245, 11, 2049, 538, 445], "temperature": 0.0, "avg_logprob": -0.07610978355890588, "compression_ratio": 1.7900552486187846, "no_speech_prob": 2.6687590434448794e-05}, {"id": 640, "seek": 338700, "start": 3395.0, "end": 3396.48, "text": " taking the average.", "tokens": [1940, 264, 4274, 13], "temperature": 0.0, "avg_logprob": -0.07610978355890588, "compression_ratio": 1.7900552486187846, "no_speech_prob": 2.6687590434448794e-05}, {"id": 641, "seek": 338700, "start": 3396.48, "end": 3405.36, "text": " So you can see here I've just loaded up my forward classifier, and then here I've loaded", "tokens": [407, 291, 393, 536, 510, 286, 600, 445, 13210, 493, 452, 2128, 1508, 9902, 11, 293, 550, 510, 286, 600, 13210], "temperature": 0.0, "avg_logprob": -0.07610978355890588, "compression_ratio": 1.7900552486187846, "no_speech_prob": 2.6687590434448794e-05}, {"id": 642, "seek": 338700, "start": 3405.36, "end": 3412.96, "text": " up my backwards classifier, and I've got the predictions of the forward classifier, which", "tokens": [493, 452, 12204, 1508, 9902, 11, 293, 286, 600, 658, 264, 21264, 295, 264, 2128, 1508, 9902, 11, 597], "temperature": 0.0, "avg_logprob": -0.07610978355890588, "compression_ratio": 1.7900552486187846, "no_speech_prob": 2.6687590434448794e-05}, {"id": 643, "seek": 341296, "start": 3412.96, "end": 3421.44, "text": " as you can see the F1 is.895, and the predictions of my backwards classifier, which is 895,", "tokens": [382, 291, 393, 536, 264, 479, 16, 307, 2411, 23, 15718, 11, 293, 264, 21264, 295, 452, 12204, 1508, 9902, 11, 597, 307, 1649, 15718, 11], "temperature": 0.0, "avg_logprob": -0.10374289119944853, "compression_ratio": 1.5128205128205128, "no_speech_prob": 2.3552001948701218e-05}, {"id": 644, "seek": 341296, "start": 3421.44, "end": 3430.9, "text": " and then I take the average of the two, and now I get F1 of.9016, which definitely gets", "tokens": [293, 550, 286, 747, 264, 4274, 295, 264, 732, 11, 293, 586, 286, 483, 479, 16, 295, 2411, 7771, 6866, 11, 597, 2138, 2170], "temperature": 0.0, "avg_logprob": -0.10374289119944853, "compression_ratio": 1.5128205128205128, "no_speech_prob": 2.3552001948701218e-05}, {"id": 645, "seek": 341296, "start": 3430.9, "end": 3433.64, "text": " us into first place.", "tokens": [505, 666, 700, 1081, 13], "temperature": 0.0, "avg_logprob": -0.10374289119944853, "compression_ratio": 1.5128205128205128, "no_speech_prob": 2.3552001948701218e-05}, {"id": 646, "seek": 341296, "start": 3433.64, "end": 3441.32, "text": " So you can see here in the span of what was basically a day's work, which included downloading", "tokens": [407, 291, 393, 536, 510, 294, 264, 16174, 295, 437, 390, 1936, 257, 786, 311, 589, 11, 597, 5556, 32529], "temperature": 0.0, "avg_logprob": -0.10374289119944853, "compression_ratio": 1.5128205128205128, "no_speech_prob": 2.3552001948701218e-05}, {"id": 647, "seek": 344132, "start": 3441.32, "end": 3447.76, "text": " the Vietnamese Wikipedia and finding a Vietnamese sentiment analysis dataset, we've got something", "tokens": [264, 25934, 28999, 293, 5006, 257, 25934, 16149, 5215, 28872, 11, 321, 600, 658, 746], "temperature": 0.0, "avg_logprob": -0.12773827144077846, "compression_ratio": 1.5887445887445888, "no_speech_prob": 2.5866729629342444e-05}, {"id": 648, "seek": 344132, "start": 3447.76, "end": 3454.1200000000003, "text": " which would have won a pretty popular competition with 150 entrants for Vietnamese sentiment", "tokens": [597, 576, 362, 1582, 257, 1238, 3743, 6211, 365, 8451, 8041, 1719, 337, 25934, 16149], "temperature": 0.0, "avg_logprob": -0.12773827144077846, "compression_ratio": 1.5887445887445888, "no_speech_prob": 2.5866729629342444e-05}, {"id": 649, "seek": 344132, "start": 3454.1200000000003, "end": 3455.96, "text": " classification.", "tokens": [21538, 13], "temperature": 0.0, "avg_logprob": -0.12773827144077846, "compression_ratio": 1.5887445887445888, "no_speech_prob": 2.5866729629342444e-05}, {"id": 650, "seek": 344132, "start": 3455.96, "end": 3463.84, "text": " And so this trick of ensembling the forwards and backwards can often give you a significant", "tokens": [400, 370, 341, 4282, 295, 12567, 2504, 1688, 264, 30126, 293, 12204, 393, 2049, 976, 291, 257, 4776], "temperature": 0.0, "avg_logprob": -0.12773827144077846, "compression_ratio": 1.5887445887445888, "no_speech_prob": 2.5866729629342444e-05}, {"id": 651, "seek": 344132, "start": 3463.84, "end": 3470.1800000000003, "text": " lift, particularly in situations where you don't have a lot of data.", "tokens": [5533, 11, 4098, 294, 6851, 689, 291, 500, 380, 362, 257, 688, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12773827144077846, "compression_ratio": 1.5887445887445888, "no_speech_prob": 2.5866729629342444e-05}, {"id": 652, "seek": 347018, "start": 3470.18, "end": 3477.7999999999997, "text": " This approach of creating your own pre-trained model, it's not useful only for different", "tokens": [639, 3109, 295, 4084, 428, 1065, 659, 12, 17227, 2001, 2316, 11, 309, 311, 406, 4420, 787, 337, 819], "temperature": 0.0, "avg_logprob": -0.10152668550790074, "compression_ratio": 1.565217391304348, "no_speech_prob": 8.664498636790086e-06}, {"id": 653, "seek": 347018, "start": 3477.7999999999997, "end": 3485.8399999999997, "text": " languages, but it can also be useful if you're dealing with an extremely different domain.", "tokens": [8650, 11, 457, 309, 393, 611, 312, 4420, 498, 291, 434, 6260, 365, 364, 4664, 819, 9274, 13], "temperature": 0.0, "avg_logprob": -0.10152668550790074, "compression_ratio": 1.565217391304348, "no_speech_prob": 8.664498636790086e-06}, {"id": 654, "seek": 347018, "start": 3485.8399999999997, "end": 3491.64, "text": " So like one particularly popular one would be medicine, if you've got a huge vocabulary", "tokens": [407, 411, 472, 4098, 3743, 472, 576, 312, 7195, 11, 498, 291, 600, 658, 257, 2603, 19864], "temperature": 0.0, "avg_logprob": -0.10152668550790074, "compression_ratio": 1.565217391304348, "no_speech_prob": 8.664498636790086e-06}, {"id": 655, "seek": 347018, "start": 3491.64, "end": 3498.8399999999997, "text": " of medical terms, and maybe it's in more detail than might have appeared in Wikipedia pages,", "tokens": [295, 4625, 2115, 11, 293, 1310, 309, 311, 294, 544, 2607, 813, 1062, 362, 8516, 294, 28999, 7183, 11], "temperature": 0.0, "avg_logprob": -0.10152668550790074, "compression_ratio": 1.565217391304348, "no_speech_prob": 8.664498636790086e-06}, {"id": 656, "seek": 349884, "start": 3498.84, "end": 3504.34, "text": " so it just doesn't appear in the Wikipedia vocabulary, you could create a pre-trained", "tokens": [370, 309, 445, 1177, 380, 4204, 294, 264, 28999, 19864, 11, 291, 727, 1884, 257, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.11905135231456537, "compression_ratio": 1.5172413793103448, "no_speech_prob": 6.643342203460634e-06}, {"id": 657, "seek": 349884, "start": 3504.34, "end": 3507.32, "text": " medical model.", "tokens": [4625, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11905135231456537, "compression_ratio": 1.5172413793103448, "no_speech_prob": 6.643342203460634e-06}, {"id": 658, "seek": 349884, "start": 3507.32, "end": 3516.7200000000003, "text": " Having said that, the big takeaway from our ULM-FIT research really was, it turns out", "tokens": [10222, 848, 300, 11, 264, 955, 30681, 490, 527, 624, 43, 44, 12, 37, 3927, 2132, 534, 390, 11, 309, 4523, 484], "temperature": 0.0, "avg_logprob": -0.11905135231456537, "compression_ratio": 1.5172413793103448, "no_speech_prob": 6.643342203460634e-06}, {"id": 659, "seek": 349884, "start": 3516.7200000000003, "end": 3518.52, "text": " you don't need to do that very often.", "tokens": [291, 500, 380, 643, 281, 360, 300, 588, 2049, 13], "temperature": 0.0, "avg_logprob": -0.11905135231456537, "compression_ratio": 1.5172413793103448, "no_speech_prob": 6.643342203460634e-06}, {"id": 660, "seek": 349884, "start": 3518.52, "end": 3524.04, "text": " It turns out that pre-training a model on Wikipedia, as long as it's in the correct", "tokens": [467, 4523, 484, 300, 659, 12, 17227, 1760, 257, 2316, 322, 28999, 11, 382, 938, 382, 309, 311, 294, 264, 3006], "temperature": 0.0, "avg_logprob": -0.11905135231456537, "compression_ratio": 1.5172413793103448, "no_speech_prob": 6.643342203460634e-06}, {"id": 661, "seek": 352404, "start": 3524.04, "end": 3531.4, "text": " language, is nearly always good enough, it's not very often that you would need to start", "tokens": [2856, 11, 307, 6217, 1009, 665, 1547, 11, 309, 311, 406, 588, 2049, 300, 291, 576, 643, 281, 722], "temperature": 0.0, "avg_logprob": -0.12258297602335612, "compression_ratio": 1.5493562231759657, "no_speech_prob": 8.530137165507767e-06}, {"id": 662, "seek": 352404, "start": 3531.4, "end": 3532.4, "text": " from scratch.", "tokens": [490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.12258297602335612, "compression_ratio": 1.5493562231759657, "no_speech_prob": 8.530137165507767e-06}, {"id": 663, "seek": 352404, "start": 3532.4, "end": 3537.64, "text": " And even if you did want a specific medical model, you're probably better off transfer", "tokens": [400, 754, 498, 291, 630, 528, 257, 2685, 4625, 2316, 11, 291, 434, 1391, 1101, 766, 5003], "temperature": 0.0, "avg_logprob": -0.12258297602335612, "compression_ratio": 1.5493562231759657, "no_speech_prob": 8.530137165507767e-06}, {"id": 664, "seek": 352404, "start": 3537.64, "end": 3543.36, "text": " learning from Wikipedia model to the medical model, rather than starting from random.", "tokens": [2539, 490, 28999, 2316, 281, 264, 4625, 2316, 11, 2831, 813, 2891, 490, 4974, 13], "temperature": 0.0, "avg_logprob": -0.12258297602335612, "compression_ratio": 1.5493562231759657, "no_speech_prob": 8.530137165507767e-06}, {"id": 665, "seek": 352404, "start": 3543.36, "end": 3549.24, "text": " Okay, so we've nearly finished this, let's take a break, come back at 12.10, and then", "tokens": [1033, 11, 370, 321, 600, 6217, 4335, 341, 11, 718, 311, 747, 257, 1821, 11, 808, 646, 412, 2272, 13, 3279, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.12258297602335612, "compression_ratio": 1.5493562231759657, "no_speech_prob": 8.530137165507767e-06}, {"id": 666, "seek": 354924, "start": 3549.24, "end": 3556.0, "text": " we'll finish up our other language models.", "tokens": [321, 603, 2413, 493, 527, 661, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.2135059183294123, "compression_ratio": 1.2254901960784315, "no_speech_prob": 8.7495289335493e-05}, {"id": 667, "seek": 354924, "start": 3556.0, "end": 3570.7599999999998, "text": " So I mentioned when I looked at the Vietnamese one, that I also looked at Chinese.", "tokens": [407, 286, 2835, 562, 286, 2956, 412, 264, 25934, 472, 11, 300, 286, 611, 2956, 412, 4649, 13], "temperature": 0.0, "avg_logprob": -0.2135059183294123, "compression_ratio": 1.2254901960784315, "no_speech_prob": 8.7495289335493e-05}, {"id": 668, "seek": 357076, "start": 3570.76, "end": 3581.4, "text": " And so the Chinese Wikipedia is ZH, and so if you want to try this in Chinese rather", "tokens": [400, 370, 264, 4649, 28999, 307, 1176, 39, 11, 293, 370, 498, 291, 528, 281, 853, 341, 294, 4649, 2831], "temperature": 0.0, "avg_logprob": -0.16357155788091965, "compression_ratio": 1.5721153846153846, "no_speech_prob": 1.6700432752259076e-05}, {"id": 669, "seek": 357076, "start": 3581.4, "end": 3587.32, "text": " than Vietnamese, you would just replace lang equals vi with lang equals ZH, and everything", "tokens": [813, 25934, 11, 291, 576, 445, 7406, 2265, 6915, 1932, 365, 2265, 6915, 1176, 39, 11, 293, 1203], "temperature": 0.0, "avg_logprob": -0.16357155788091965, "compression_ratio": 1.5721153846153846, "no_speech_prob": 1.6700432752259076e-05}, {"id": 670, "seek": 357076, "start": 3587.32, "end": 3588.82, "text": " else is the same.", "tokens": [1646, 307, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.16357155788091965, "compression_ratio": 1.5721153846153846, "no_speech_prob": 1.6700432752259076e-05}, {"id": 671, "seek": 357076, "start": 3588.82, "end": 3595.46, "text": " But I wanted to talk about this as another interesting example of little complexities", "tokens": [583, 286, 1415, 281, 751, 466, 341, 382, 1071, 1880, 1365, 295, 707, 48705], "temperature": 0.0, "avg_logprob": -0.16357155788091965, "compression_ratio": 1.5721153846153846, "no_speech_prob": 1.6700432752259076e-05}, {"id": 672, "seek": 357076, "start": 3595.46, "end": 3598.0800000000004, "text": " you have to deal with in non-English languages.", "tokens": [291, 362, 281, 2028, 365, 294, 2107, 12, 31254, 1933, 8650, 13], "temperature": 0.0, "avg_logprob": -0.16357155788091965, "compression_ratio": 1.5721153846153846, "no_speech_prob": 1.6700432752259076e-05}, {"id": 673, "seek": 359808, "start": 3598.08, "end": 3605.72, "text": " And Chinese is a particularly important one to know about, because it's the language that's", "tokens": [400, 4649, 307, 257, 4098, 1021, 472, 281, 458, 466, 11, 570, 309, 311, 264, 2856, 300, 311], "temperature": 0.0, "avg_logprob": -0.11574349474551072, "compression_ratio": 1.6318681318681318, "no_speech_prob": 5.4747084504924715e-05}, {"id": 674, "seek": 359808, "start": 3605.72, "end": 3612.9, "text": " spoken by the other great economic superpower, and is also one of the most widely spoken", "tokens": [10759, 538, 264, 661, 869, 4836, 45765, 11, 293, 307, 611, 472, 295, 264, 881, 13371, 10759], "temperature": 0.0, "avg_logprob": -0.11574349474551072, "compression_ratio": 1.6318681318681318, "no_speech_prob": 5.4747084504924715e-05}, {"id": 675, "seek": 359808, "start": 3612.9, "end": 3615.08, "text": " languages in the world.", "tokens": [8650, 294, 264, 1002, 13], "temperature": 0.0, "avg_logprob": -0.11574349474551072, "compression_ratio": 1.6318681318681318, "no_speech_prob": 5.4747084504924715e-05}, {"id": 676, "seek": 359808, "start": 3615.08, "end": 3620.88, "text": " So super commercially important language to know how to deal with, and it has a particularly", "tokens": [407, 1687, 41751, 1021, 2856, 281, 458, 577, 281, 2028, 365, 11, 293, 309, 575, 257, 4098], "temperature": 0.0, "avg_logprob": -0.11574349474551072, "compression_ratio": 1.6318681318681318, "no_speech_prob": 5.4747084504924715e-05}, {"id": 677, "seek": 362088, "start": 3620.88, "end": 3628.92, "text": " interesting and important tweak you have to deal with, which is there are multiple ways", "tokens": [1880, 293, 1021, 29879, 291, 362, 281, 2028, 365, 11, 597, 307, 456, 366, 3866, 2098], "temperature": 0.0, "avg_logprob": -0.2018390603967615, "compression_ratio": 1.5204081632653061, "no_speech_prob": 5.388393037719652e-05}, {"id": 678, "seek": 362088, "start": 3628.92, "end": 3634.78, "text": " to write the same sentence in Chinese.", "tokens": [281, 2464, 264, 912, 8174, 294, 4649, 13], "temperature": 0.0, "avg_logprob": -0.2018390603967615, "compression_ratio": 1.5204081632653061, "no_speech_prob": 5.388393037719652e-05}, {"id": 679, "seek": 362088, "start": 3634.78, "end": 3641.08, "text": " And the names of these two ways vary depending on who you ask on the mainland.", "tokens": [400, 264, 5288, 295, 613, 732, 2098, 10559, 5413, 322, 567, 291, 1029, 322, 264, 32365, 13], "temperature": 0.0, "avg_logprob": -0.2018390603967615, "compression_ratio": 1.5204081632653061, "no_speech_prob": 5.388393037719652e-05}, {"id": 680, "seek": 362088, "start": 3641.08, "end": 3647.12, "text": " They're known as simplified characters or traditional characters, fan ti tse or jian", "tokens": [814, 434, 2570, 382, 26335, 4342, 420, 5164, 4342, 11, 3429, 8757, 256, 405, 420, 361, 952], "temperature": 0.0, "avg_logprob": -0.2018390603967615, "compression_ratio": 1.5204081632653061, "no_speech_prob": 5.388393037719652e-05}, {"id": 681, "seek": 362088, "start": 3647.12, "end": 3648.76, "text": " ti tse.", "tokens": [8757, 256, 405, 13], "temperature": 0.0, "avg_logprob": -0.2018390603967615, "compression_ratio": 1.5204081632653061, "no_speech_prob": 5.388393037719652e-05}, {"id": 682, "seek": 364876, "start": 3648.76, "end": 3658.44, "text": " And they are basically the traditional characters, was the characters used before 1954 or something,", "tokens": [400, 436, 366, 1936, 264, 5164, 4342, 11, 390, 264, 4342, 1143, 949, 46590, 420, 746, 11], "temperature": 0.0, "avg_logprob": -0.17829107834120927, "compression_ratio": 1.530054644808743, "no_speech_prob": 1.834179238358047e-05}, {"id": 683, "seek": 364876, "start": 3658.44, "end": 3665.88, "text": " and there was a lot of controversy in Chinese literature that a lot of the most famous Chinese", "tokens": [293, 456, 390, 257, 688, 295, 22976, 294, 4649, 10394, 300, 257, 688, 295, 264, 881, 4618, 4649], "temperature": 0.0, "avg_logprob": -0.17829107834120927, "compression_ratio": 1.530054644808743, "no_speech_prob": 1.834179238358047e-05}, {"id": 684, "seek": 364876, "start": 3665.88, "end": 3672.96, "text": " authors were saying our language is too complicated, and it's a fascinating history.", "tokens": [16552, 645, 1566, 527, 2856, 307, 886, 6179, 11, 293, 309, 311, 257, 10343, 2503, 13], "temperature": 0.0, "avg_logprob": -0.17829107834120927, "compression_ratio": 1.530054644808743, "no_speech_prob": 1.834179238358047e-05}, {"id": 685, "seek": 367296, "start": 3672.96, "end": 3679.36, "text": " The Chinese language, as was the case in many languages throughout history, was designed", "tokens": [440, 4649, 2856, 11, 382, 390, 264, 1389, 294, 867, 8650, 3710, 2503, 11, 390, 4761], "temperature": 0.0, "avg_logprob": -0.11200039561201887, "compression_ratio": 1.5560538116591929, "no_speech_prob": 1.2410835552145727e-05}, {"id": 686, "seek": 367296, "start": 3679.36, "end": 3684.32, "text": " to be difficult so the commoners couldn't understand it, which means that the people", "tokens": [281, 312, 2252, 370, 264, 2689, 433, 2809, 380, 1223, 309, 11, 597, 1355, 300, 264, 561], "temperature": 0.0, "avg_logprob": -0.11200039561201887, "compression_ratio": 1.5560538116591929, "no_speech_prob": 1.2410835552145727e-05}, {"id": 687, "seek": 367296, "start": 3684.32, "end": 3689.32, "text": " that did understand it maintained higher status positions.", "tokens": [300, 630, 1223, 309, 17578, 2946, 6558, 8432, 13], "temperature": 0.0, "avg_logprob": -0.11200039561201887, "compression_ratio": 1.5560538116591929, "no_speech_prob": 1.2410835552145727e-05}, {"id": 688, "seek": 367296, "start": 3689.32, "end": 3693.88, "text": " And so it actually got kind of crazy in the 19th century in China, there were as many", "tokens": [400, 370, 309, 767, 658, 733, 295, 3219, 294, 264, 1294, 392, 4901, 294, 3533, 11, 456, 645, 382, 867], "temperature": 0.0, "avg_logprob": -0.11200039561201887, "compression_ratio": 1.5560538116591929, "no_speech_prob": 1.2410835552145727e-05}, {"id": 689, "seek": 367296, "start": 3693.88, "end": 3697.84, "text": " as 60,000 characters in use.", "tokens": [382, 4060, 11, 1360, 4342, 294, 764, 13], "temperature": 0.0, "avg_logprob": -0.11200039561201887, "compression_ratio": 1.5560538116591929, "no_speech_prob": 1.2410835552145727e-05}, {"id": 690, "seek": 369784, "start": 3697.84, "end": 3705.04, "text": " By the mid-20th century things weren't quite as crazy, but there was still a view, particularly", "tokens": [3146, 264, 2062, 12, 2009, 392, 4901, 721, 4999, 380, 1596, 382, 3219, 11, 457, 456, 390, 920, 257, 1910, 11, 4098], "temperature": 0.0, "avg_logprob": -0.16036672412224537, "compression_ratio": 1.6703296703296704, "no_speech_prob": 1.2805076039512642e-05}, {"id": 691, "seek": 369784, "start": 3705.04, "end": 3712.6800000000003, "text": " following the communist revolution in mainland China, that things need to be made simpler.", "tokens": [3480, 264, 29347, 8894, 294, 32365, 3533, 11, 300, 721, 643, 281, 312, 1027, 18587, 13], "temperature": 0.0, "avg_logprob": -0.16036672412224537, "compression_ratio": 1.6703296703296704, "no_speech_prob": 1.2805076039512642e-05}, {"id": 692, "seek": 369784, "start": 3712.6800000000003, "end": 3716.0, "text": " And so they actually did something which is amazing, which is that they gathered lots", "tokens": [400, 370, 436, 767, 630, 746, 597, 307, 2243, 11, 597, 307, 300, 436, 13032, 3195], "temperature": 0.0, "avg_logprob": -0.16036672412224537, "compression_ratio": 1.6703296703296704, "no_speech_prob": 1.2805076039512642e-05}, {"id": 693, "seek": 369784, "start": 3716.0, "end": 3720.32, "text": " of linguists together and they said let's design a new character set.", "tokens": [295, 21766, 1751, 1214, 293, 436, 848, 718, 311, 1715, 257, 777, 2517, 992, 13], "temperature": 0.0, "avg_logprob": -0.16036672412224537, "compression_ratio": 1.6703296703296704, "no_speech_prob": 1.2805076039512642e-05}, {"id": 694, "seek": 369784, "start": 3720.32, "end": 3726.1600000000003, "text": " And let's find all of the characters that people find difficult to read and write and", "tokens": [400, 718, 311, 915, 439, 295, 264, 4342, 300, 561, 915, 2252, 281, 1401, 293, 2464, 293], "temperature": 0.0, "avg_logprob": -0.16036672412224537, "compression_ratio": 1.6703296703296704, "no_speech_prob": 1.2805076039512642e-05}, {"id": 695, "seek": 369784, "start": 3726.1600000000003, "end": 3727.76, "text": " replace them with new ones.", "tokens": [7406, 552, 365, 777, 2306, 13], "temperature": 0.0, "avg_logprob": -0.16036672412224537, "compression_ratio": 1.6703296703296704, "no_speech_prob": 1.2805076039512642e-05}, {"id": 696, "seek": 372776, "start": 3727.76, "end": 3730.84, "text": " But the interesting thing is they didn't change the language.", "tokens": [583, 264, 1880, 551, 307, 436, 994, 380, 1319, 264, 2856, 13], "temperature": 0.0, "avg_logprob": -0.15902786254882811, "compression_ratio": 1.9208333333333334, "no_speech_prob": 3.882941018673591e-05}, {"id": 697, "seek": 372776, "start": 3730.84, "end": 3736.0, "text": " So the same sentence will be written in two different ways.", "tokens": [407, 264, 912, 8174, 486, 312, 3720, 294, 732, 819, 2098, 13], "temperature": 0.0, "avg_logprob": -0.15902786254882811, "compression_ratio": 1.9208333333333334, "no_speech_prob": 3.882941018673591e-05}, {"id": 698, "seek": 372776, "start": 3736.0, "end": 3739.1600000000003, "text": " Some of the characters will be the same, some of them will be different, some of them half", "tokens": [2188, 295, 264, 4342, 486, 312, 264, 912, 11, 512, 295, 552, 486, 312, 819, 11, 512, 295, 552, 1922], "temperature": 0.0, "avg_logprob": -0.15902786254882811, "compression_ratio": 1.9208333333333334, "no_speech_prob": 3.882941018673591e-05}, {"id": 699, "seek": 372776, "start": 3739.1600000000003, "end": 3742.84, "text": " will be the same and half will be different.", "tokens": [486, 312, 264, 912, 293, 1922, 486, 312, 819, 13], "temperature": 0.0, "avg_logprob": -0.15902786254882811, "compression_ratio": 1.9208333333333334, "no_speech_prob": 3.882941018673591e-05}, {"id": 700, "seek": 372776, "start": 3742.84, "end": 3746.7200000000003, "text": " And so this is one of these interesting things that you have to know about if you're going", "tokens": [400, 370, 341, 307, 472, 295, 613, 1880, 721, 300, 291, 362, 281, 458, 466, 498, 291, 434, 516], "temperature": 0.0, "avg_logprob": -0.15902786254882811, "compression_ratio": 1.9208333333333334, "no_speech_prob": 3.882941018673591e-05}, {"id": 701, "seek": 372776, "start": 3746.7200000000003, "end": 3748.3, "text": " to do NLP in practice.", "tokens": [281, 360, 426, 45196, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.15902786254882811, "compression_ratio": 1.9208333333333334, "no_speech_prob": 3.882941018673591e-05}, {"id": 702, "seek": 372776, "start": 3748.3, "end": 3754.48, "text": " So it's kind of fun, you start to learn about other cultures and other kind of linguistic", "tokens": [407, 309, 311, 733, 295, 1019, 11, 291, 722, 281, 1466, 466, 661, 12951, 293, 661, 733, 295, 43002], "temperature": 0.0, "avg_logprob": -0.15902786254882811, "compression_ratio": 1.9208333333333334, "no_speech_prob": 3.882941018673591e-05}, {"id": 703, "seek": 375448, "start": 3754.48, "end": 3758.84, "text": " histories and so forth.", "tokens": [30631, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.16263701915740966, "compression_ratio": 1.695852534562212, "no_speech_prob": 1.5445839380845428e-05}, {"id": 704, "seek": 375448, "start": 3758.84, "end": 3767.92, "text": " So in Chinese, there is not an exact mapping from simplified to traditional.", "tokens": [407, 294, 4649, 11, 456, 307, 406, 364, 1900, 18350, 490, 26335, 281, 5164, 13], "temperature": 0.0, "avg_logprob": -0.16263701915740966, "compression_ratio": 1.695852534562212, "no_speech_prob": 1.5445839380845428e-05}, {"id": 705, "seek": 375448, "start": 3767.92, "end": 3772.68, "text": " And I use those terminology because I've spent time in the mainland, not in Taiwan and Hong", "tokens": [400, 286, 764, 729, 27575, 570, 286, 600, 4418, 565, 294, 264, 32365, 11, 406, 294, 12296, 293, 8868], "temperature": 0.0, "avg_logprob": -0.16263701915740966, "compression_ratio": 1.695852534562212, "no_speech_prob": 1.5445839380845428e-05}, {"id": 706, "seek": 375448, "start": 3772.68, "end": 3775.48, "text": " Kong where they use different terminology.", "tokens": [9832, 689, 436, 764, 819, 27575, 13], "temperature": 0.0, "avg_logprob": -0.16263701915740966, "compression_ratio": 1.695852534562212, "no_speech_prob": 1.5445839380845428e-05}, {"id": 707, "seek": 375448, "start": 3775.48, "end": 3781.0, "text": " Because actually a traditional character, there's actually multiple traditional characters", "tokens": [1436, 767, 257, 5164, 2517, 11, 456, 311, 767, 3866, 5164, 4342], "temperature": 0.0, "avg_logprob": -0.16263701915740966, "compression_ratio": 1.695852534562212, "no_speech_prob": 1.5445839380845428e-05}, {"id": 708, "seek": 375448, "start": 3781.0, "end": 3783.8, "text": " can map to the same simplified character.", "tokens": [393, 4471, 281, 264, 912, 26335, 2517, 13], "temperature": 0.0, "avg_logprob": -0.16263701915740966, "compression_ratio": 1.695852534562212, "no_speech_prob": 1.5445839380845428e-05}, {"id": 709, "seek": 378380, "start": 3783.8, "end": 3789.04, "text": " But you can, there is a largely unique mapping from traditional to simplified, not from simplified", "tokens": [583, 291, 393, 11, 456, 307, 257, 11611, 3845, 18350, 490, 5164, 281, 26335, 11, 406, 490, 26335], "temperature": 0.0, "avg_logprob": -0.1451497835533641, "compression_ratio": 1.8384615384615384, "no_speech_prob": 7.646357516932767e-06}, {"id": 710, "seek": 378380, "start": 3789.04, "end": 3790.1200000000003, "text": " to traditional.", "tokens": [281, 5164, 13], "temperature": 0.0, "avg_logprob": -0.1451497835533641, "compression_ratio": 1.8384615384615384, "no_speech_prob": 7.646357516932767e-06}, {"id": 711, "seek": 378380, "start": 3790.1200000000003, "end": 3795.96, "text": " So what the Chinese Wikipedia editors decided to do, for a while they had two totally separate", "tokens": [407, 437, 264, 4649, 28999, 31446, 3047, 281, 360, 11, 337, 257, 1339, 436, 632, 732, 3879, 4994], "temperature": 0.0, "avg_logprob": -0.1451497835533641, "compression_ratio": 1.8384615384615384, "no_speech_prob": 7.646357516932767e-06}, {"id": 712, "seek": 378380, "start": 3795.96, "end": 3803.1400000000003, "text": " encyclopedias, they merged them and then they decided basically, okay, let's write it largely", "tokens": [465, 34080, 27277, 4609, 11, 436, 36427, 552, 293, 550, 436, 3047, 1936, 11, 1392, 11, 718, 311, 2464, 309, 11611], "temperature": 0.0, "avg_logprob": -0.1451497835533641, "compression_ratio": 1.8384615384615384, "no_speech_prob": 7.646357516932767e-06}, {"id": 713, "seek": 378380, "start": 3803.1400000000003, "end": 3807.8, "text": " in traditional characters and create some scripts that will convert them.", "tokens": [294, 5164, 4342, 293, 1884, 512, 23294, 300, 486, 7620, 552, 13], "temperature": 0.0, "avg_logprob": -0.1451497835533641, "compression_ratio": 1.8384615384615384, "no_speech_prob": 7.646357516932767e-06}, {"id": 714, "seek": 378380, "start": 3807.8, "end": 3812.5600000000004, "text": " So when you download the Chinese Wikipedia dump, you'll find that they're in traditional", "tokens": [407, 562, 291, 5484, 264, 4649, 28999, 11430, 11, 291, 603, 915, 300, 436, 434, 294, 5164], "temperature": 0.0, "avg_logprob": -0.1451497835533641, "compression_ratio": 1.8384615384615384, "no_speech_prob": 7.646357516932767e-06}, {"id": 715, "seek": 378380, "start": 3812.5600000000004, "end": 3813.5600000000004, "text": " characters.", "tokens": [4342, 13], "temperature": 0.0, "avg_logprob": -0.1451497835533641, "compression_ratio": 1.8384615384615384, "no_speech_prob": 7.646357516932767e-06}, {"id": 716, "seek": 381356, "start": 3813.56, "end": 3816.68, "text": " It's not quite as simple as that.", "tokens": [467, 311, 406, 1596, 382, 2199, 382, 300, 13], "temperature": 0.0, "avg_logprob": -0.14020015146130713, "compression_ratio": 1.6236162361623616, "no_speech_prob": 3.4266471629962325e-05}, {"id": 717, "seek": 381356, "start": 3816.68, "end": 3820.7999999999997, "text": " There's also some slight variations in vocabulary that tends to be used in different parts of", "tokens": [821, 311, 611, 512, 4036, 17840, 294, 19864, 300, 12258, 281, 312, 1143, 294, 819, 3166, 295], "temperature": 0.0, "avg_logprob": -0.14020015146130713, "compression_ratio": 1.6236162361623616, "no_speech_prob": 3.4266471629962325e-05}, {"id": 718, "seek": 381356, "start": 3820.7999999999997, "end": 3823.6, "text": " China, but that's the main thing.", "tokens": [3533, 11, 457, 300, 311, 264, 2135, 551, 13], "temperature": 0.0, "avg_logprob": -0.14020015146130713, "compression_ratio": 1.6236162361623616, "no_speech_prob": 3.4266471629962325e-05}, {"id": 719, "seek": 381356, "start": 3823.6, "end": 3829.7999999999997, "text": " So I'm not smart enough to understand traditional characters, I only understood simplified, so", "tokens": [407, 286, 478, 406, 4069, 1547, 281, 1223, 5164, 4342, 11, 286, 787, 7320, 26335, 11, 370], "temperature": 0.0, "avg_logprob": -0.14020015146130713, "compression_ratio": 1.6236162361623616, "no_speech_prob": 3.4266471629962325e-05}, {"id": 720, "seek": 381356, "start": 3829.7999999999997, "end": 3835.36, "text": " I for my own sanity wanted to create the simplified version.", "tokens": [286, 337, 452, 1065, 47892, 1415, 281, 1884, 264, 26335, 3037, 13], "temperature": 0.0, "avg_logprob": -0.14020015146130713, "compression_ratio": 1.6236162361623616, "no_speech_prob": 3.4266471629962325e-05}, {"id": 721, "seek": 381356, "start": 3835.36, "end": 3840.7999999999997, "text": " So generally when you want to do some kind of language specific thing like this, you", "tokens": [407, 5101, 562, 291, 528, 281, 360, 512, 733, 295, 2856, 2685, 551, 411, 341, 11, 291], "temperature": 0.0, "avg_logprob": -0.14020015146130713, "compression_ratio": 1.6236162361623616, "no_speech_prob": 3.4266471629962325e-05}, {"id": 722, "seek": 381356, "start": 3840.7999999999997, "end": 3842.56, "text": " just have to go and do some Googling.", "tokens": [445, 362, 281, 352, 293, 360, 512, 45005, 1688, 13], "temperature": 0.0, "avg_logprob": -0.14020015146130713, "compression_ratio": 1.6236162361623616, "no_speech_prob": 3.4266471629962325e-05}, {"id": 723, "seek": 384256, "start": 3842.56, "end": 3849.44, "text": " And so it turns out that there's actually a program called OpenCC which will take a", "tokens": [400, 370, 309, 4523, 484, 300, 456, 311, 767, 257, 1461, 1219, 7238, 11717, 597, 486, 747, 257], "temperature": 0.0, "avg_logprob": -0.19756698608398438, "compression_ratio": 1.5981735159817352, "no_speech_prob": 2.8406964702298865e-05}, {"id": 724, "seek": 384256, "start": 3849.44, "end": 3855.36, "text": " Chinese language document that's in traditional and convert it to simplified or vice versa", "tokens": [4649, 2856, 4166, 300, 311, 294, 5164, 293, 7620, 309, 281, 26335, 420, 11964, 25650], "temperature": 0.0, "avg_logprob": -0.19756698608398438, "compression_ratio": 1.5981735159817352, "no_speech_prob": 2.8406964702298865e-05}, {"id": 725, "seek": 384256, "start": 3855.36, "end": 3858.02, "text": " automatically.", "tokens": [6772, 13], "temperature": 0.0, "avg_logprob": -0.19756698608398438, "compression_ratio": 1.5981735159817352, "no_speech_prob": 2.8406964702298865e-05}, {"id": 726, "seek": 384256, "start": 3858.02, "end": 3864.44, "text": " So you basically just say OpenCC minus i for input and your input file minus o for output", "tokens": [407, 291, 1936, 445, 584, 7238, 11717, 3175, 741, 337, 4846, 293, 428, 4846, 3991, 3175, 277, 337, 5598], "temperature": 0.0, "avg_logprob": -0.19756698608398438, "compression_ratio": 1.5981735159817352, "no_speech_prob": 2.8406964702298865e-05}, {"id": 727, "seek": 384256, "start": 3864.44, "end": 3866.0, "text": " and your output file.", "tokens": [293, 428, 5598, 3991, 13], "temperature": 0.0, "avg_logprob": -0.19756698608398438, "compression_ratio": 1.5981735159817352, "no_speech_prob": 2.8406964702298865e-05}, {"id": 728, "seek": 384256, "start": 3866.0, "end": 3872.48, "text": " So I created a ZHS, as in ZH simplified version.", "tokens": [407, 286, 2942, 257, 1176, 12527, 11, 382, 294, 1176, 39, 26335, 3037, 13], "temperature": 0.0, "avg_logprob": -0.19756698608398438, "compression_ratio": 1.5981735159817352, "no_speech_prob": 2.8406964702298865e-05}, {"id": 729, "seek": 387248, "start": 3872.48, "end": 3877.96, "text": " I found that took a really, really, really, really long time and I'm not very patient.", "tokens": [286, 1352, 300, 1890, 257, 534, 11, 534, 11, 534, 11, 534, 938, 565, 293, 286, 478, 406, 588, 4537, 13], "temperature": 0.0, "avg_logprob": -0.15251606337878168, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.5866831492749043e-05}, {"id": 730, "seek": 387248, "start": 3877.96, "end": 3881.52, "text": " So something I wanted to tell you about in case you didn't know is that there is a program", "tokens": [407, 746, 286, 1415, 281, 980, 291, 466, 294, 1389, 291, 994, 380, 458, 307, 300, 456, 307, 257, 1461], "temperature": 0.0, "avg_logprob": -0.15251606337878168, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.5866831492749043e-05}, {"id": 731, "seek": 387248, "start": 3881.52, "end": 3887.84, "text": " called Parallel, more fully known as GNU Parallel.", "tokens": [1219, 3457, 336, 338, 11, 544, 4498, 2570, 382, 46411, 52, 3457, 336, 338, 13], "temperature": 0.0, "avg_logprob": -0.15251606337878168, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.5866831492749043e-05}, {"id": 732, "seek": 387248, "start": 3887.84, "end": 3890.64, "text": " If you're on Ubuntu, it's just apt-install parallel.", "tokens": [759, 291, 434, 322, 30230, 45605, 11, 309, 311, 445, 29427, 12, 40166, 8952, 13], "temperature": 0.0, "avg_logprob": -0.15251606337878168, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.5866831492749043e-05}, {"id": 733, "seek": 387248, "start": 3890.64, "end": 3897.72, "text": " And what that will do is it will run any program over multiple processors.", "tokens": [400, 437, 300, 486, 360, 307, 309, 486, 1190, 604, 1461, 670, 3866, 27751, 13], "temperature": 0.0, "avg_logprob": -0.15251606337878168, "compression_ratio": 1.6036036036036037, "no_speech_prob": 2.5866831492749043e-05}, {"id": 734, "seek": 389772, "start": 3897.72, "end": 3902.7999999999997, "text": " And pretty much everybody has more than one processor nowadays.", "tokens": [400, 1238, 709, 2201, 575, 544, 813, 472, 15321, 13434, 13], "temperature": 0.0, "avg_logprob": -0.14802674187554254, "compression_ratio": 1.5757575757575757, "no_speech_prob": 5.093647359899478e-06}, {"id": 735, "seek": 389772, "start": 3902.7999999999997, "end": 3908.4399999999996, "text": " And so here is how you can convert Chinese traditional characters to Chinese simplified", "tokens": [400, 370, 510, 307, 577, 291, 393, 7620, 4649, 5164, 4342, 281, 4649, 26335], "temperature": 0.0, "avg_logprob": -0.14802674187554254, "compression_ratio": 1.5757575757575757, "no_speech_prob": 5.093647359899478e-06}, {"id": 736, "seek": 389772, "start": 3908.4399999999996, "end": 3911.9399999999996, "text": " characters automatically and quickly.", "tokens": [4342, 6772, 293, 2661, 13], "temperature": 0.0, "avg_logprob": -0.14802674187554254, "compression_ratio": 1.5757575757575757, "no_speech_prob": 5.093647359899478e-06}, {"id": 737, "seek": 389772, "start": 3911.9399999999996, "end": 3920.4399999999996, "text": " So I just said, OK, look for all of the text files and type that to GNU Parallel.", "tokens": [407, 286, 445, 848, 11, 2264, 11, 574, 337, 439, 295, 264, 2487, 7098, 293, 2010, 300, 281, 46411, 52, 3457, 336, 338, 13], "temperature": 0.0, "avg_logprob": -0.14802674187554254, "compression_ratio": 1.5757575757575757, "no_speech_prob": 5.093647359899478e-06}, {"id": 738, "seek": 389772, "start": 3920.4399999999996, "end": 3927.64, "text": " And then all you do is you tell GNU Parallel what's the command you want to run in parallel.", "tokens": [400, 550, 439, 291, 360, 307, 291, 980, 46411, 52, 3457, 336, 338, 437, 311, 264, 5622, 291, 528, 281, 1190, 294, 8952, 13], "temperature": 0.0, "avg_logprob": -0.14802674187554254, "compression_ratio": 1.5757575757575757, "no_speech_prob": 5.093647359899478e-06}, {"id": 739, "seek": 392764, "start": 3927.64, "end": 3934.12, "text": " And what it's going to do is it's going to replace every percent sign with the next thing", "tokens": [400, 437, 309, 311, 516, 281, 360, 307, 309, 311, 516, 281, 7406, 633, 3043, 1465, 365, 264, 958, 551], "temperature": 0.0, "avg_logprob": -0.1467574323926653, "compression_ratio": 1.6989247311827957, "no_speech_prob": 1.3419793503999244e-05}, {"id": 740, "seek": 392764, "start": 3934.12, "end": 3937.52, "text": " that comes out of here, so the next file name.", "tokens": [300, 1487, 484, 295, 510, 11, 370, 264, 958, 3991, 1315, 13], "temperature": 0.0, "avg_logprob": -0.1467574323926653, "compression_ratio": 1.6989247311827957, "no_speech_prob": 1.3419793503999244e-05}, {"id": 741, "seek": 392764, "start": 3937.52, "end": 3939.2, "text": " Why percent sign?", "tokens": [1545, 3043, 1465, 30], "temperature": 0.0, "avg_logprob": -0.1467574323926653, "compression_ratio": 1.6989247311827957, "no_speech_prob": 1.3419793503999244e-05}, {"id": 742, "seek": 392764, "start": 3939.2, "end": 3945.08, "text": " Because you basically say to Parallel minus capital I blah, where blah is the thing that", "tokens": [1436, 291, 1936, 584, 281, 3457, 336, 338, 3175, 4238, 286, 12288, 11, 689, 12288, 307, 264, 551, 300], "temperature": 0.0, "avg_logprob": -0.1467574323926653, "compression_ratio": 1.6989247311827957, "no_speech_prob": 1.3419793503999244e-05}, {"id": 743, "seek": 392764, "start": 3945.08, "end": 3950.12, "text": " you're going to use to represent the next file name that's being fed in.", "tokens": [291, 434, 516, 281, 764, 281, 2906, 264, 958, 3991, 1315, 300, 311, 885, 4636, 294, 13], "temperature": 0.0, "avg_logprob": -0.1467574323926653, "compression_ratio": 1.6989247311827957, "no_speech_prob": 1.3419793503999244e-05}, {"id": 744, "seek": 395012, "start": 3950.12, "end": 3958.24, "text": " So this is a nice little trick that I use all the time to run things much more quickly,", "tokens": [407, 341, 307, 257, 1481, 707, 4282, 300, 286, 764, 439, 264, 565, 281, 1190, 721, 709, 544, 2661, 11], "temperature": 0.0, "avg_logprob": -0.15612032919219046, "compression_ratio": 1.5895196506550218, "no_speech_prob": 2.601604364826926e-06}, {"id": 745, "seek": 395012, "start": 3958.24, "end": 3964.8399999999997, "text": " particularly if you're using like AWS, if you're using something like an AWS P2, it's", "tokens": [4098, 498, 291, 434, 1228, 411, 17650, 11, 498, 291, 434, 1228, 746, 411, 364, 17650, 430, 17, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.15612032919219046, "compression_ratio": 1.5895196506550218, "no_speech_prob": 2.601604364826926e-06}, {"id": 746, "seek": 395012, "start": 3964.8399999999997, "end": 3970.6, "text": " got lots of CPUs, so you may as well take advantage of them.", "tokens": [658, 3195, 295, 13199, 82, 11, 370, 291, 815, 382, 731, 747, 5002, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.15612032919219046, "compression_ratio": 1.5895196506550218, "no_speech_prob": 2.601604364826926e-06}, {"id": 747, "seek": 395012, "start": 3970.6, "end": 3976.64, "text": " Anyway, so this is just an example of the kinds of things you need to be aware of when", "tokens": [5684, 11, 370, 341, 307, 445, 364, 1365, 295, 264, 3685, 295, 721, 291, 643, 281, 312, 3650, 295, 562], "temperature": 0.0, "avg_logprob": -0.15612032919219046, "compression_ratio": 1.5895196506550218, "no_speech_prob": 2.601604364826926e-06}, {"id": 748, "seek": 395012, "start": 3976.64, "end": 3979.92, "text": " you're dealing with non-English languages.", "tokens": [291, 434, 6260, 365, 2107, 12, 31254, 1933, 8650, 13], "temperature": 0.0, "avg_logprob": -0.15612032919219046, "compression_ratio": 1.5895196506550218, "no_speech_prob": 2.601604364826926e-06}, {"id": 749, "seek": 397992, "start": 3979.92, "end": 3984.7200000000003, "text": " And one of the best things to do is if you're dealing with a non-English language is to", "tokens": [400, 472, 295, 264, 1151, 721, 281, 360, 307, 498, 291, 434, 6260, 365, 257, 2107, 12, 31254, 1933, 2856, 307, 281], "temperature": 0.0, "avg_logprob": -0.25756119746787876, "compression_ratio": 1.6375, "no_speech_prob": 1.2411358511599246e-05}, {"id": 750, "seek": 397992, "start": 3984.7200000000003, "end": 3992.36, "text": " partner up with a native speaker of that non-English language so they can help you deal with all", "tokens": [4975, 493, 365, 257, 8470, 8145, 295, 300, 2107, 12, 31254, 1933, 2856, 370, 436, 393, 854, 291, 2028, 365, 439], "temperature": 0.0, "avg_logprob": -0.25756119746787876, "compression_ratio": 1.6375, "no_speech_prob": 1.2411358511599246e-05}, {"id": 751, "seek": 397992, "start": 3992.36, "end": 3995.36, "text": " the little tricky things.", "tokens": [264, 707, 12414, 721, 13], "temperature": 0.0, "avg_logprob": -0.25756119746787876, "compression_ratio": 1.6375, "no_speech_prob": 1.2411358511599246e-05}, {"id": 752, "seek": 397992, "start": 3995.36, "end": 3996.36, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.25756119746787876, "compression_ratio": 1.6375, "no_speech_prob": 1.2411358511599246e-05}, {"id": 753, "seek": 397992, "start": 3996.36, "end": 4002.08, "text": " I was just going to highlight this is an example or an illustration of how important domain", "tokens": [286, 390, 445, 516, 281, 5078, 341, 307, 364, 1365, 420, 364, 22645, 295, 577, 1021, 9274], "temperature": 0.0, "avg_logprob": -0.25756119746787876, "compression_ratio": 1.6375, "no_speech_prob": 1.2411358511599246e-05}, {"id": 754, "seek": 397992, "start": 4002.08, "end": 4006.2400000000002, "text": " expertise is, which is true in kind of any application of deep learning.", "tokens": [11769, 307, 11, 597, 307, 2074, 294, 733, 295, 604, 3861, 295, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.25756119746787876, "compression_ratio": 1.6375, "no_speech_prob": 1.2411358511599246e-05}, {"id": 755, "seek": 397992, "start": 4006.2400000000002, "end": 4007.2400000000002, "text": " Yeah, yeah.", "tokens": [865, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.25756119746787876, "compression_ratio": 1.6375, "no_speech_prob": 1.2411358511599246e-05}, {"id": 756, "seek": 400724, "start": 4007.24, "end": 4014.72, "text": " It is important, or at least domain respect, you know, like take the time to understand", "tokens": [467, 307, 1021, 11, 420, 412, 1935, 9274, 3104, 11, 291, 458, 11, 411, 747, 264, 565, 281, 1223], "temperature": 0.0, "avg_logprob": -0.15759477497618876, "compression_ratio": 1.6221198156682028, "no_speech_prob": 2.6273281036992557e-05}, {"id": 757, "seek": 400724, "start": 4014.72, "end": 4021.3999999999996, "text": " the domain yourself as best as you can and ideally work with a domain expert.", "tokens": [264, 9274, 1803, 382, 1151, 382, 291, 393, 293, 22915, 589, 365, 257, 9274, 5844, 13], "temperature": 0.0, "avg_logprob": -0.15759477497618876, "compression_ratio": 1.6221198156682028, "no_speech_prob": 2.6273281036992557e-05}, {"id": 758, "seek": 400724, "start": 4021.3999999999996, "end": 4028.56, "text": " So Chinese also has another interesting feature, but to keep things interesting, I'm going", "tokens": [407, 4649, 611, 575, 1071, 1880, 4111, 11, 457, 281, 1066, 721, 1880, 11, 286, 478, 516], "temperature": 0.0, "avg_logprob": -0.15759477497618876, "compression_ratio": 1.6221198156682028, "no_speech_prob": 2.6273281036992557e-05}, {"id": 759, "seek": 400724, "start": 4028.56, "end": 4032.3799999999997, "text": " to tell you about this interesting feature by talking about another language, which is", "tokens": [281, 980, 291, 466, 341, 1880, 4111, 538, 1417, 466, 1071, 2856, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.15759477497618876, "compression_ratio": 1.6221198156682028, "no_speech_prob": 2.6273281036992557e-05}, {"id": 760, "seek": 400724, "start": 4032.3799999999997, "end": 4034.3399999999997, "text": " Turkish.", "tokens": [18565, 13], "temperature": 0.0, "avg_logprob": -0.15759477497618876, "compression_ratio": 1.6221198156682028, "no_speech_prob": 2.6273281036992557e-05}, {"id": 761, "seek": 403434, "start": 4034.34, "end": 4040.08, "text": " And Turkish is interesting, are there any native Turkish speakers here?", "tokens": [400, 18565, 307, 1880, 11, 366, 456, 604, 8470, 18565, 9518, 510, 30], "temperature": 0.0, "avg_logprob": -0.19657313987000347, "compression_ratio": 1.484472049689441, "no_speech_prob": 5.142896043253131e-05}, {"id": 762, "seek": 403434, "start": 4040.08, "end": 4041.08, "text": " There is one.", "tokens": [821, 307, 472, 13], "temperature": 0.0, "avg_logprob": -0.19657313987000347, "compression_ratio": 1.484472049689441, "no_speech_prob": 5.142896043253131e-05}, {"id": 763, "seek": 403434, "start": 4041.08, "end": 4042.08, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.19657313987000347, "compression_ratio": 1.484472049689441, "no_speech_prob": 5.142896043253131e-05}, {"id": 764, "seek": 403434, "start": 4042.08, "end": 4043.88, "text": " Can you pass the catch box over?", "tokens": [1664, 291, 1320, 264, 3745, 2424, 670, 30], "temperature": 0.0, "avg_logprob": -0.19657313987000347, "compression_ratio": 1.484472049689441, "no_speech_prob": 5.142896043253131e-05}, {"id": 765, "seek": 403434, "start": 4043.88, "end": 4047.04, "text": " We need your help, please.", "tokens": [492, 643, 428, 854, 11, 1767, 13], "temperature": 0.0, "avg_logprob": -0.19657313987000347, "compression_ratio": 1.484472049689441, "no_speech_prob": 5.142896043253131e-05}, {"id": 766, "seek": 403434, "start": 4047.04, "end": 4048.28, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.19657313987000347, "compression_ratio": 1.484472049689441, "no_speech_prob": 5.142896043253131e-05}, {"id": 767, "seek": 403434, "start": 4048.28, "end": 4052.04, "text": " Here is one Turkish word in various forms.", "tokens": [1692, 307, 472, 18565, 1349, 294, 3683, 6422, 13], "temperature": 0.0, "avg_logprob": -0.19657313987000347, "compression_ratio": 1.484472049689441, "no_speech_prob": 5.142896043253131e-05}, {"id": 768, "seek": 403434, "start": 4052.04, "end": 4059.44, "text": " Are you able to read this word for us?", "tokens": [2014, 291, 1075, 281, 1401, 341, 1349, 337, 505, 30], "temperature": 0.0, "avg_logprob": -0.19657313987000347, "compression_ratio": 1.484472049689441, "no_speech_prob": 5.142896043253131e-05}, {"id": 769, "seek": 405944, "start": 4059.44, "end": 4065.8, "text": " So these are, my understanding is these are all different, more phemic extensions.", "tokens": [407, 613, 366, 11, 452, 3701, 307, 613, 366, 439, 819, 11, 544, 903, 3438, 25129, 13], "temperature": 0.0, "avg_logprob": -0.21581496444402956, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.00027037429390475154}, {"id": 770, "seek": 405944, "start": 4065.8, "end": 4067.8, "text": " How far would you go down in practice?", "tokens": [1012, 1400, 576, 291, 352, 760, 294, 3124, 30], "temperature": 0.0, "avg_logprob": -0.21581496444402956, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.00027037429390475154}, {"id": 771, "seek": 405944, "start": 4067.8, "end": 4068.8, "text": " Right there, right there.", "tokens": [1779, 456, 11, 558, 456, 13], "temperature": 0.0, "avg_logprob": -0.21581496444402956, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.00027037429390475154}, {"id": 772, "seek": 405944, "start": 4068.8, "end": 4069.8, "text": " About here?", "tokens": [7769, 510, 30], "temperature": 0.0, "avg_logprob": -0.21581496444402956, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.00027037429390475154}, {"id": 773, "seek": 405944, "start": 4069.8, "end": 4070.8, "text": " Two lines down.", "tokens": [4453, 3876, 760, 13], "temperature": 0.0, "avg_logprob": -0.21581496444402956, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.00027037429390475154}, {"id": 774, "seek": 405944, "start": 4070.8, "end": 4071.8, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.21581496444402956, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.00027037429390475154}, {"id": 775, "seek": 405944, "start": 4071.8, "end": 4072.8, "text": " That won't be the longest.", "tokens": [663, 1582, 380, 312, 264, 15438, 13], "temperature": 0.0, "avg_logprob": -0.21581496444402956, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.00027037429390475154}, {"id": 776, "seek": 405944, "start": 4072.8, "end": 4075.8, "text": " Can you pronounce that for us?", "tokens": [1664, 291, 19567, 300, 337, 505, 30], "temperature": 0.0, "avg_logprob": -0.21581496444402956, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.00027037429390475154}, {"id": 777, "seek": 405944, "start": 4075.8, "end": 4079.48, "text": " Yeah, that's what I was going to say.", "tokens": [865, 11, 300, 311, 437, 286, 390, 516, 281, 584, 13], "temperature": 0.0, "avg_logprob": -0.21581496444402956, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.00027037429390475154}, {"id": 778, "seek": 405944, "start": 4079.48, "end": 4086.04, "text": " So Turkish is what's called an agglutinative language.", "tokens": [407, 18565, 307, 437, 311, 1219, 364, 623, 7191, 325, 259, 1166, 2856, 13], "temperature": 0.0, "avg_logprob": -0.21581496444402956, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.00027037429390475154}, {"id": 779, "seek": 408604, "start": 4086.04, "end": 4093.84, "text": " And agglutinative languages are languages which use a lot of morphemes to construct", "tokens": [400, 623, 7191, 325, 259, 1166, 8650, 366, 8650, 597, 764, 257, 688, 295, 25778, 443, 279, 281, 7690], "temperature": 0.0, "avg_logprob": -0.1697956153324672, "compression_ratio": 1.5380952380952382, "no_speech_prob": 9.665706784289796e-06}, {"id": 780, "seek": 408604, "start": 4093.84, "end": 4095.92, "text": " interesting variants.", "tokens": [1880, 21669, 13], "temperature": 0.0, "avg_logprob": -0.1697956153324672, "compression_ratio": 1.5380952380952382, "no_speech_prob": 9.665706784289796e-06}, {"id": 781, "seek": 408604, "start": 4095.92, "end": 4100.2, "text": " So in this case, what was your name?", "tokens": [407, 294, 341, 1389, 11, 437, 390, 428, 1315, 30], "temperature": 0.0, "avg_logprob": -0.1697956153324672, "compression_ratio": 1.5380952380952382, "no_speech_prob": 9.665706784289796e-06}, {"id": 782, "seek": 408604, "start": 4100.2, "end": 4105.36, "text": " So Omar has apparently never needed to say, when it seems like that is one of those that", "tokens": [407, 33784, 575, 7970, 1128, 2978, 281, 584, 11, 562, 309, 2544, 411, 300, 307, 472, 295, 729, 300], "temperature": 0.0, "avg_logprob": -0.1697956153324672, "compression_ratio": 1.5380952380952382, "no_speech_prob": 9.665706784289796e-06}, {"id": 783, "seek": 408604, "start": 4105.36, "end": 4112.28, "text": " we can make fearless, but if he did, he could use a single word to describe that situation.", "tokens": [321, 393, 652, 44139, 11, 457, 498, 415, 630, 11, 415, 727, 764, 257, 2167, 1349, 281, 6786, 300, 2590, 13], "temperature": 0.0, "avg_logprob": -0.1697956153324672, "compression_ratio": 1.5380952380952382, "no_speech_prob": 9.665706784289796e-06}, {"id": 784, "seek": 411228, "start": 4112.28, "end": 4121.88, "text": " Now the problem is that normal tokenization that we do with spacey would treat this as", "tokens": [823, 264, 1154, 307, 300, 2710, 14862, 2144, 300, 321, 360, 365, 1901, 88, 576, 2387, 341, 382], "temperature": 0.0, "avg_logprob": -0.17682080096509084, "compression_ratio": 1.634020618556701, "no_speech_prob": 1.4063397429708857e-05}, {"id": 785, "seek": 411228, "start": 4121.88, "end": 4130.04, "text": " one word, right, and as one concept, and so because it largely uses space as a delimiter,", "tokens": [472, 1349, 11, 558, 11, 293, 382, 472, 3410, 11, 293, 370, 570, 309, 11611, 4960, 1901, 382, 257, 1103, 332, 1681, 11], "temperature": 0.0, "avg_logprob": -0.17682080096509084, "compression_ratio": 1.634020618556701, "no_speech_prob": 1.4063397429708857e-05}, {"id": 786, "seek": 411228, "start": 4130.04, "end": 4134.0, "text": " with some special cases like apostrophe s, for instance.", "tokens": [365, 512, 2121, 3331, 411, 19484, 27194, 262, 11, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.17682080096509084, "compression_ratio": 1.634020618556701, "no_speech_prob": 1.4063397429708857e-05}, {"id": 787, "seek": 411228, "start": 4134.0, "end": 4140.599999999999, "text": " And so in that case, we would end up with a vocabulary of millions of words, right,", "tokens": [400, 370, 294, 300, 1389, 11, 321, 576, 917, 493, 365, 257, 19864, 295, 6803, 295, 2283, 11, 558, 11], "temperature": 0.0, "avg_logprob": -0.17682080096509084, "compression_ratio": 1.634020618556701, "no_speech_prob": 1.4063397429708857e-05}, {"id": 788, "seek": 414060, "start": 4140.6, "end": 4143.240000000001, "text": " because this isn't really a separate word.", "tokens": [570, 341, 1943, 380, 534, 257, 4994, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1196729342142741, "compression_ratio": 1.427710843373494, "no_speech_prob": 4.222766165185021e-06}, {"id": 789, "seek": 414060, "start": 4143.240000000001, "end": 4148.96, "text": " It's this word with various morphemes appended to it, right?", "tokens": [467, 311, 341, 1349, 365, 3683, 25778, 443, 279, 724, 3502, 281, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1196729342142741, "compression_ratio": 1.427710843373494, "no_speech_prob": 4.222766165185021e-06}, {"id": 790, "seek": 414060, "start": 4148.96, "end": 4154.160000000001, "text": " So these kinds of languages need special care.", "tokens": [407, 613, 3685, 295, 8650, 643, 2121, 1127, 13], "temperature": 0.0, "avg_logprob": -0.1196729342142741, "compression_ratio": 1.427710843373494, "no_speech_prob": 4.222766165185021e-06}, {"id": 791, "seek": 414060, "start": 4154.160000000001, "end": 4165.92, "text": " Now it's actually exactly the same care that Chinese needs, because Chinese looks like", "tokens": [823, 309, 311, 767, 2293, 264, 912, 1127, 300, 4649, 2203, 11, 570, 4649, 1542, 411], "temperature": 0.0, "avg_logprob": -0.1196729342142741, "compression_ratio": 1.427710843373494, "no_speech_prob": 4.222766165185021e-06}, {"id": 792, "seek": 416592, "start": 4165.92, "end": 4175.56, "text": " this, which is there are no spaces at all.", "tokens": [341, 11, 597, 307, 456, 366, 572, 7673, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.12875729900295452, "compression_ratio": 1.3466666666666667, "no_speech_prob": 5.1738602451223414e-06}, {"id": 793, "seek": 416592, "start": 4175.56, "end": 4184.28, "text": " So we can't rely when dealing with languages like Turkish or Chinese on using a space to", "tokens": [407, 321, 393, 380, 10687, 562, 6260, 365, 8650, 411, 18565, 420, 4649, 322, 1228, 257, 1901, 281], "temperature": 0.0, "avg_logprob": -0.12875729900295452, "compression_ratio": 1.3466666666666667, "no_speech_prob": 5.1738602451223414e-06}, {"id": 794, "seek": 416592, "start": 4184.28, "end": 4189.36, "text": " tokenize our corpus.", "tokens": [14862, 1125, 527, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.12875729900295452, "compression_ratio": 1.3466666666666667, "no_speech_prob": 5.1738602451223414e-06}, {"id": 795, "seek": 416592, "start": 4189.36, "end": 4191.2, "text": " And this is a big problem.", "tokens": [400, 341, 307, 257, 955, 1154, 13], "temperature": 0.0, "avg_logprob": -0.12875729900295452, "compression_ratio": 1.3466666666666667, "no_speech_prob": 5.1738602451223414e-06}, {"id": 796, "seek": 416592, "start": 4191.2, "end": 4194.16, "text": " This is really tricky.", "tokens": [639, 307, 534, 12414, 13], "temperature": 0.0, "avg_logprob": -0.12875729900295452, "compression_ratio": 1.3466666666666667, "no_speech_prob": 5.1738602451223414e-06}, {"id": 797, "seek": 419416, "start": 4194.16, "end": 4198.88, "text": " And it's not as easy as you might imagine.", "tokens": [400, 309, 311, 406, 382, 1858, 382, 291, 1062, 3811, 13], "temperature": 0.0, "avg_logprob": -0.14918866753578186, "compression_ratio": 1.6545454545454545, "no_speech_prob": 8.267555131169502e-06}, {"id": 798, "seek": 419416, "start": 4198.88, "end": 4204.0, "text": " People often say, oh well, let's just find the start and end of each word and put a space", "tokens": [3432, 2049, 584, 11, 1954, 731, 11, 718, 311, 445, 915, 264, 722, 293, 917, 295, 1184, 1349, 293, 829, 257, 1901], "temperature": 0.0, "avg_logprob": -0.14918866753578186, "compression_ratio": 1.6545454545454545, "no_speech_prob": 8.267555131169502e-06}, {"id": 799, "seek": 419416, "start": 4204.0, "end": 4205.0, "text": " there.", "tokens": [456, 13], "temperature": 0.0, "avg_logprob": -0.14918866753578186, "compression_ratio": 1.6545454545454545, "no_speech_prob": 8.267555131169502e-06}, {"id": 800, "seek": 419416, "start": 4205.0, "end": 4212.28, "text": " But Chinese, for example, is such a grammatically interesting language that it's not even clear", "tokens": [583, 4649, 11, 337, 1365, 11, 307, 1270, 257, 17570, 5030, 1880, 2856, 300, 309, 311, 406, 754, 1850], "temperature": 0.0, "avg_logprob": -0.14918866753578186, "compression_ratio": 1.6545454545454545, "no_speech_prob": 8.267555131169502e-06}, {"id": 801, "seek": 419416, "start": 4212.28, "end": 4215.639999999999, "text": " sometimes where words start and end.", "tokens": [2171, 689, 2283, 722, 293, 917, 13], "temperature": 0.0, "avg_logprob": -0.14918866753578186, "compression_ratio": 1.6545454545454545, "no_speech_prob": 8.267555131169502e-06}, {"id": 802, "seek": 419416, "start": 4215.639999999999, "end": 4222.5199999999995, "text": " Often, for example, in Chinese, you will insert a character in the middle of a word to give", "tokens": [20043, 11, 337, 1365, 11, 294, 4649, 11, 291, 486, 8969, 257, 2517, 294, 264, 2808, 295, 257, 1349, 281, 976], "temperature": 0.0, "avg_logprob": -0.14918866753578186, "compression_ratio": 1.6545454545454545, "no_speech_prob": 8.267555131169502e-06}, {"id": 803, "seek": 422252, "start": 4222.52, "end": 4227.280000000001, "text": " it some different meaning, right?", "tokens": [309, 512, 819, 3620, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.10981610703141723, "compression_ratio": 1.545945945945946, "no_speech_prob": 2.392173519183416e-05}, {"id": 804, "seek": 422252, "start": 4227.280000000001, "end": 4232.56, "text": " Or you'll insert a character at the end of the word, something called a resultative complement,", "tokens": [1610, 291, 603, 8969, 257, 2517, 412, 264, 917, 295, 264, 1349, 11, 746, 1219, 257, 1874, 1166, 17103, 11], "temperature": 0.0, "avg_logprob": -0.10981610703141723, "compression_ratio": 1.545945945945946, "no_speech_prob": 2.392173519183416e-05}, {"id": 805, "seek": 422252, "start": 4232.56, "end": 4238.1, "text": " which means you create a new word which tells you about the outcome of some other word.", "tokens": [597, 1355, 291, 1884, 257, 777, 1349, 597, 5112, 291, 466, 264, 9700, 295, 512, 661, 1349, 13], "temperature": 0.0, "avg_logprob": -0.10981610703141723, "compression_ratio": 1.545945945945946, "no_speech_prob": 2.392173519183416e-05}, {"id": 806, "seek": 422252, "start": 4238.1, "end": 4245.240000000001, "text": " So it's not a simple case of just saying, let's just find the words.", "tokens": [407, 309, 311, 406, 257, 2199, 1389, 295, 445, 1566, 11, 718, 311, 445, 915, 264, 2283, 13], "temperature": 0.0, "avg_logprob": -0.10981610703141723, "compression_ratio": 1.545945945945946, "no_speech_prob": 2.392173519183416e-05}, {"id": 807, "seek": 424524, "start": 4245.24, "end": 4255.32, "text": " So in recent times, a really, really cool solution was developed, which is called SentencePiece.", "tokens": [407, 294, 5162, 1413, 11, 257, 534, 11, 534, 1627, 3827, 390, 4743, 11, 597, 307, 1219, 23652, 655, 47, 46566, 13], "temperature": 0.0, "avg_logprob": -0.11219406127929688, "compression_ratio": 1.618279569892473, "no_speech_prob": 2.561245310062077e-06}, {"id": 808, "seek": 424524, "start": 4255.32, "end": 4261.0, "text": " And it's actually based on something that goes back even further called byte pair encoding.", "tokens": [400, 309, 311, 767, 2361, 322, 746, 300, 1709, 646, 754, 3052, 1219, 40846, 6119, 43430, 13], "temperature": 0.0, "avg_logprob": -0.11219406127929688, "compression_ratio": 1.618279569892473, "no_speech_prob": 2.561245310062077e-06}, {"id": 809, "seek": 424524, "start": 4261.0, "end": 4267.5199999999995, "text": " And what SentencePiece and byte pair encoding do is that they segment things into what are", "tokens": [400, 437, 23652, 655, 47, 46566, 293, 40846, 6119, 43430, 360, 307, 300, 436, 9469, 721, 666, 437, 366], "temperature": 0.0, "avg_logprob": -0.11219406127929688, "compression_ratio": 1.618279569892473, "no_speech_prob": 2.561245310062077e-06}, {"id": 810, "seek": 424524, "start": 4267.5199999999995, "end": 4271.16, "text": " called subword units.", "tokens": [1219, 1422, 7462, 6815, 13], "temperature": 0.0, "avg_logprob": -0.11219406127929688, "compression_ratio": 1.618279569892473, "no_speech_prob": 2.561245310062077e-06}, {"id": 811, "seek": 427116, "start": 4271.16, "end": 4280.68, "text": " And the subword unit is a sequence of letters that appears frequently in a corpus, so frequently", "tokens": [400, 264, 1422, 7462, 4985, 307, 257, 8310, 295, 7825, 300, 7038, 10374, 294, 257, 1181, 31624, 11, 370, 10374], "temperature": 0.0, "avg_logprob": -0.07993794954740084, "compression_ratio": 1.5089820359281436, "no_speech_prob": 3.7265981518430635e-06}, {"id": 812, "seek": 427116, "start": 4280.68, "end": 4286.0199999999995, "text": " that you tokenize it into those subsequences.", "tokens": [300, 291, 14862, 1125, 309, 666, 729, 13924, 2667, 13], "temperature": 0.0, "avg_logprob": -0.07993794954740084, "compression_ratio": 1.5089820359281436, "no_speech_prob": 3.7265981518430635e-06}, {"id": 813, "seek": 427116, "start": 4286.0199999999995, "end": 4294.8, "text": " So for example, in Turkish, after we've used subword encoding, we end up with something", "tokens": [407, 337, 1365, 11, 294, 18565, 11, 934, 321, 600, 1143, 1422, 7462, 43430, 11, 321, 917, 493, 365, 746], "temperature": 0.0, "avg_logprob": -0.07993794954740084, "compression_ratio": 1.5089820359281436, "no_speech_prob": 3.7265981518430635e-06}, {"id": 814, "seek": 427116, "start": 4294.8, "end": 4296.22, "text": " that looks like this.", "tokens": [300, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.07993794954740084, "compression_ratio": 1.5089820359281436, "no_speech_prob": 3.7265981518430635e-06}, {"id": 815, "seek": 429622, "start": 4296.22, "end": 4302.56, "text": " So this big underscore represents spaces, and spaces represent token boundaries.", "tokens": [407, 341, 955, 37556, 8855, 7673, 11, 293, 7673, 2906, 14862, 13180, 13], "temperature": 0.0, "avg_logprob": -0.1668727927737766, "compression_ratio": 1.5585106382978724, "no_speech_prob": 2.726446609813138e-06}, {"id": 816, "seek": 429622, "start": 4302.56, "end": 4309.04, "text": " And you can see most of the time, we have each token is a single word, but you can see", "tokens": [400, 291, 393, 536, 881, 295, 264, 565, 11, 321, 362, 1184, 14862, 307, 257, 2167, 1349, 11, 457, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.1668727927737766, "compression_ratio": 1.5585106382978724, "no_speech_prob": 2.726446609813138e-06}, {"id": 817, "seek": 429622, "start": 4309.04, "end": 4316.72, "text": " that sometimes you get things like this, where it's been turned into two tokens.", "tokens": [300, 2171, 291, 483, 721, 411, 341, 11, 689, 309, 311, 668, 3574, 666, 732, 22667, 13], "temperature": 0.0, "avg_logprob": -0.1668727927737766, "compression_ratio": 1.5585106382978724, "no_speech_prob": 2.726446609813138e-06}, {"id": 818, "seek": 429622, "start": 4316.72, "end": 4321.72, "text": " We've actually split the word in the middle.", "tokens": [492, 600, 767, 7472, 264, 1349, 294, 264, 2808, 13], "temperature": 0.0, "avg_logprob": -0.1668727927737766, "compression_ratio": 1.5585106382978724, "no_speech_prob": 2.726446609813138e-06}, {"id": 819, "seek": 432172, "start": 4321.72, "end": 4326.76, "text": " Here's another example, where we've actually split the word in the middle.", "tokens": [1692, 311, 1071, 1365, 11, 689, 321, 600, 767, 7472, 264, 1349, 294, 264, 2808, 13], "temperature": 0.0, "avg_logprob": -0.2644015993390765, "compression_ratio": 1.4230769230769231, "no_speech_prob": 2.8407575882738456e-05}, {"id": 820, "seek": 432172, "start": 4326.76, "end": 4334.52, "text": " Just to clarify, the dark underscores, those were spaces in the original text, and the", "tokens": [1449, 281, 17594, 11, 264, 2877, 16692, 66, 2706, 11, 729, 645, 7673, 294, 264, 3380, 2487, 11, 293, 264], "temperature": 0.0, "avg_logprob": -0.2644015993390765, "compression_ratio": 1.4230769230769231, "no_speech_prob": 2.8407575882738456e-05}, {"id": 821, "seek": 432172, "start": 4334.52, "end": 4339.240000000001, "text": " other spaces are ones that have been added by how you've tokenized this.", "tokens": [661, 7673, 366, 2306, 300, 362, 668, 3869, 538, 577, 291, 600, 14862, 1602, 341, 13], "temperature": 0.0, "avg_logprob": -0.2644015993390765, "compression_ratio": 1.4230769230769231, "no_speech_prob": 2.8407575882738456e-05}, {"id": 822, "seek": 432172, "start": 4339.240000000001, "end": 4340.240000000001, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.2644015993390765, "compression_ratio": 1.4230769230769231, "no_speech_prob": 2.8407575882738456e-05}, {"id": 823, "seek": 432172, "start": 4340.240000000001, "end": 4344.02, "text": " Thanks, Rachel.", "tokens": [2561, 11, 14246, 13], "temperature": 0.0, "avg_logprob": -0.2644015993390765, "compression_ratio": 1.4230769230769231, "no_speech_prob": 2.8407575882738456e-05}, {"id": 824, "seek": 434402, "start": 4344.02, "end": 4356.360000000001, "text": " So the question is, how would you start with something like this, or something like this,", "tokens": [407, 264, 1168, 307, 11, 577, 576, 291, 722, 365, 746, 411, 341, 11, 420, 746, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.11772216217858451, "compression_ratio": 1.625, "no_speech_prob": 7.889118933235295e-06}, {"id": 825, "seek": 434402, "start": 4356.360000000001, "end": 4362.76, "text": " and turn it into something like this?", "tokens": [293, 1261, 309, 666, 746, 411, 341, 30], "temperature": 0.0, "avg_logprob": -0.11772216217858451, "compression_ratio": 1.625, "no_speech_prob": 7.889118933235295e-06}, {"id": 826, "seek": 434402, "start": 4362.76, "end": 4369.92, "text": " And the answer is, at a high level, what you basically do is you start out by looking through", "tokens": [400, 264, 1867, 307, 11, 412, 257, 1090, 1496, 11, 437, 291, 1936, 360, 307, 291, 722, 484, 538, 1237, 807], "temperature": 0.0, "avg_logprob": -0.11772216217858451, "compression_ratio": 1.625, "no_speech_prob": 7.889118933235295e-06}, {"id": 827, "seek": 436992, "start": 4369.92, "end": 4374.6, "text": " your corpus, and you find, I'll tell you the example for byte-pair encodings, it's the", "tokens": [428, 1181, 31624, 11, 293, 291, 915, 11, 286, 603, 980, 291, 264, 1365, 337, 40846, 12, 79, 1246, 2058, 378, 1109, 11, 309, 311, 264], "temperature": 0.0, "avg_logprob": -0.1461152125449076, "compression_ratio": 1.8773946360153257, "no_speech_prob": 3.944018317270093e-05}, {"id": 828, "seek": 436992, "start": 4374.6, "end": 4380.72, "text": " easiest to describe, you find which two characters appear next to each other the most frequently.", "tokens": [12889, 281, 6786, 11, 291, 915, 597, 732, 4342, 4204, 958, 281, 1184, 661, 264, 881, 10374, 13], "temperature": 0.0, "avg_logprob": -0.1461152125449076, "compression_ratio": 1.8773946360153257, "no_speech_prob": 3.944018317270093e-05}, {"id": 829, "seek": 436992, "start": 4380.72, "end": 4385.66, "text": " And you take that pair, and you pull it out, and you say, oh, that's a token.", "tokens": [400, 291, 747, 300, 6119, 11, 293, 291, 2235, 309, 484, 11, 293, 291, 584, 11, 1954, 11, 300, 311, 257, 14862, 13], "temperature": 0.0, "avg_logprob": -0.1461152125449076, "compression_ratio": 1.8773946360153257, "no_speech_prob": 3.944018317270093e-05}, {"id": 830, "seek": 436992, "start": 4385.66, "end": 4390.04, "text": " So for example, if you're doing English, you might find T and H occur next to each other", "tokens": [407, 337, 1365, 11, 498, 291, 434, 884, 3669, 11, 291, 1062, 915, 314, 293, 389, 5160, 958, 281, 1184, 661], "temperature": 0.0, "avg_logprob": -0.1461152125449076, "compression_ratio": 1.8773946360153257, "no_speech_prob": 3.944018317270093e-05}, {"id": 831, "seek": 436992, "start": 4390.04, "end": 4393.72, "text": " a lot, and you say, okay, TH is now a token.", "tokens": [257, 688, 11, 293, 291, 584, 11, 1392, 11, 3578, 307, 586, 257, 14862, 13], "temperature": 0.0, "avg_logprob": -0.1461152125449076, "compression_ratio": 1.8773946360153257, "no_speech_prob": 3.944018317270093e-05}, {"id": 832, "seek": 436992, "start": 4393.72, "end": 4397.84, "text": " And then you repeat it, and you look for again, two characters that often appear next to each", "tokens": [400, 550, 291, 7149, 309, 11, 293, 291, 574, 337, 797, 11, 732, 4342, 300, 2049, 4204, 958, 281, 1184], "temperature": 0.0, "avg_logprob": -0.1461152125449076, "compression_ratio": 1.8773946360153257, "no_speech_prob": 3.944018317270093e-05}, {"id": 833, "seek": 439784, "start": 4397.84, "end": 4403.04, "text": " other, but now you can treat TH as one character.", "tokens": [661, 11, 457, 586, 291, 393, 2387, 3578, 382, 472, 2517, 13], "temperature": 0.0, "avg_logprob": -0.12166604541596912, "compression_ratio": 1.6116071428571428, "no_speech_prob": 3.5008388294954784e-06}, {"id": 834, "seek": 439784, "start": 4403.04, "end": 4407.2, "text": " So down the track, you'll find TH and E often appear next to each other.", "tokens": [407, 760, 264, 2837, 11, 291, 603, 915, 3578, 293, 462, 2049, 4204, 958, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.12166604541596912, "compression_ratio": 1.6116071428571428, "no_speech_prob": 3.5008388294954784e-06}, {"id": 835, "seek": 439784, "start": 4407.2, "end": 4409.12, "text": " That's now a token.", "tokens": [663, 311, 586, 257, 14862, 13], "temperature": 0.0, "avg_logprob": -0.12166604541596912, "compression_ratio": 1.6116071428571428, "no_speech_prob": 3.5008388294954784e-06}, {"id": 836, "seek": 439784, "start": 4409.12, "end": 4416.12, "text": " And so you keep doing that until eventually you have some set number of unique tokens.", "tokens": [400, 370, 291, 1066, 884, 300, 1826, 4728, 291, 362, 512, 992, 1230, 295, 3845, 22667, 13], "temperature": 0.0, "avg_logprob": -0.12166604541596912, "compression_ratio": 1.6116071428571428, "no_speech_prob": 3.5008388294954784e-06}, {"id": 837, "seek": 439784, "start": 4416.12, "end": 4417.52, "text": " That's called your vocab.", "tokens": [663, 311, 1219, 428, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.12166604541596912, "compression_ratio": 1.6116071428571428, "no_speech_prob": 3.5008388294954784e-06}, {"id": 838, "seek": 439784, "start": 4417.52, "end": 4424.84, "text": " So with SentencePiece, when you call SentencePiece, you can actually tell it how big a vocab do", "tokens": [407, 365, 23652, 655, 47, 46566, 11, 562, 291, 818, 23652, 655, 47, 46566, 11, 291, 393, 767, 980, 309, 577, 955, 257, 2329, 455, 360], "temperature": 0.0, "avg_logprob": -0.12166604541596912, "compression_ratio": 1.6116071428571428, "no_speech_prob": 3.5008388294954784e-06}, {"id": 839, "seek": 439784, "start": 4424.84, "end": 4425.84, "text": " you want.", "tokens": [291, 528, 13], "temperature": 0.0, "avg_logprob": -0.12166604541596912, "compression_ratio": 1.6116071428571428, "no_speech_prob": 3.5008388294954784e-06}, {"id": 840, "seek": 442584, "start": 4425.84, "end": 4430.360000000001, "text": " And so we default to a vocab of size 30,000.", "tokens": [400, 370, 321, 7576, 281, 257, 2329, 455, 295, 2744, 2217, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.12984106164229545, "compression_ratio": 1.5560165975103735, "no_speech_prob": 1.3925420034865965e-06}, {"id": 841, "seek": 442584, "start": 4430.360000000001, "end": 4439.4800000000005, "text": " So this here has been tokenized into Turkish subword units using a size of 30,000.", "tokens": [407, 341, 510, 575, 668, 14862, 1602, 666, 18565, 1422, 7462, 6815, 1228, 257, 2744, 295, 2217, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.12984106164229545, "compression_ratio": 1.5560165975103735, "no_speech_prob": 1.3925420034865965e-06}, {"id": 842, "seek": 442584, "start": 4439.4800000000005, "end": 4445.2, "text": " SentencePiece actually goes a bit further than BPE, Bipair Encoding.", "tokens": [23652, 655, 47, 46566, 767, 1709, 257, 857, 3052, 813, 363, 5208, 11, 363, 647, 1246, 29584, 8616, 13], "temperature": 0.0, "avg_logprob": -0.12984106164229545, "compression_ratio": 1.5560165975103735, "no_speech_prob": 1.3925420034865965e-06}, {"id": 843, "seek": 442584, "start": 4445.2, "end": 4451.3, "text": " It actually creates a neural network language model at a character level and finds combinations", "tokens": [467, 767, 7829, 257, 18161, 3209, 2856, 2316, 412, 257, 2517, 1496, 293, 10704, 21267], "temperature": 0.0, "avg_logprob": -0.12984106164229545, "compression_ratio": 1.5560165975103735, "no_speech_prob": 1.3925420034865965e-06}, {"id": 844, "seek": 442584, "start": 4451.3, "end": 4455.2, "text": " of characters that are most likely to appear together based on the language model.", "tokens": [295, 4342, 300, 366, 881, 3700, 281, 4204, 1214, 2361, 322, 264, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12984106164229545, "compression_ratio": 1.5560165975103735, "no_speech_prob": 1.3925420034865965e-06}, {"id": 845, "seek": 445520, "start": 4455.2, "end": 4458.08, "text": " But it's the same basic idea.", "tokens": [583, 309, 311, 264, 912, 3875, 1558, 13], "temperature": 0.0, "avg_logprob": -0.11195020521840741, "compression_ratio": 1.412121212121212, "no_speech_prob": 8.801009244052693e-06}, {"id": 846, "seek": 445520, "start": 4458.08, "end": 4471.4, "text": " So this idea of using subword units, it's not very well studied, but it's absolutely", "tokens": [407, 341, 1558, 295, 1228, 1422, 7462, 6815, 11, 309, 311, 406, 588, 731, 9454, 11, 457, 309, 311, 3122], "temperature": 0.0, "avg_logprob": -0.11195020521840741, "compression_ratio": 1.412121212121212, "no_speech_prob": 8.801009244052693e-06}, {"id": 847, "seek": 445520, "start": 4471.4, "end": 4480.32, "text": " necessary and super powerful, not only for Turkish, Chinese, Japanese, Polish, but also", "tokens": [4818, 293, 1687, 4005, 11, 406, 787, 337, 18565, 11, 4649, 11, 5433, 11, 18504, 11, 457, 611], "temperature": 0.0, "avg_logprob": -0.11195020521840741, "compression_ratio": 1.412121212121212, "no_speech_prob": 8.801009244052693e-06}, {"id": 848, "seek": 445520, "start": 4480.32, "end": 4483.5599999999995, "text": " for things like medical texts.", "tokens": [337, 721, 411, 4625, 15765, 13], "temperature": 0.0, "avg_logprob": -0.11195020521840741, "compression_ratio": 1.412121212121212, "no_speech_prob": 8.801009244052693e-06}, {"id": 849, "seek": 448356, "start": 4483.56, "end": 4490.580000000001, "text": " Because actually those big long chemical names in the scientific and medical literature contain", "tokens": [1436, 767, 729, 955, 938, 7313, 5288, 294, 264, 8134, 293, 4625, 10394, 5304], "temperature": 0.0, "avg_logprob": -0.10563353056548744, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.1781559553346597e-05}, {"id": 850, "seek": 448356, "start": 4490.580000000001, "end": 4493.68, "text": " well-defined subword units that are frequently reused.", "tokens": [731, 12, 37716, 1422, 7462, 6815, 300, 366, 10374, 319, 4717, 13], "temperature": 0.0, "avg_logprob": -0.10563353056548744, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.1781559553346597e-05}, {"id": 851, "seek": 448356, "start": 4493.68, "end": 4497.160000000001, "text": " So you don't need a separate vocab for every one.", "tokens": [407, 291, 500, 380, 643, 257, 4994, 2329, 455, 337, 633, 472, 13], "temperature": 0.0, "avg_logprob": -0.10563353056548744, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.1781559553346597e-05}, {"id": 852, "seek": 448356, "start": 4497.160000000001, "end": 4501.160000000001, "text": " And the problem with having a separate vocab for every one is none of those single words", "tokens": [400, 264, 1154, 365, 1419, 257, 4994, 2329, 455, 337, 633, 472, 307, 6022, 295, 729, 2167, 2283], "temperature": 0.0, "avg_logprob": -0.10563353056548744, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.1781559553346597e-05}, {"id": 853, "seek": 448356, "start": 4501.160000000001, "end": 4502.780000000001, "text": " appears that often.", "tokens": [7038, 300, 2049, 13], "temperature": 0.0, "avg_logprob": -0.10563353056548744, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.1781559553346597e-05}, {"id": 854, "seek": 448356, "start": 4502.780000000001, "end": 4508.84, "text": " So you really want to get a sense of the meaning of the underlying subword units.", "tokens": [407, 291, 534, 528, 281, 483, 257, 2020, 295, 264, 3620, 295, 264, 14217, 1422, 7462, 6815, 13], "temperature": 0.0, "avg_logprob": -0.10563353056548744, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.1781559553346597e-05}, {"id": 855, "seek": 450884, "start": 4508.84, "end": 4516.68, "text": " So if we want to do Turkish from scratch, it's basically the same as we've seen before.", "tokens": [407, 498, 321, 528, 281, 360, 18565, 490, 8459, 11, 309, 311, 1936, 264, 912, 382, 321, 600, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.1685687034360824, "compression_ratio": 1.3933333333333333, "no_speech_prob": 3.763306813198142e-05}, {"id": 856, "seek": 450884, "start": 4516.68, "end": 4519.28, "text": " We can use my SplitWiki and GetWiki.", "tokens": [492, 393, 764, 452, 45111, 54, 9850, 293, 3240, 54, 9850, 13], "temperature": 0.0, "avg_logprob": -0.1685687034360824, "compression_ratio": 1.3933333333333333, "no_speech_prob": 3.763306813198142e-05}, {"id": 857, "seek": 450884, "start": 4519.28, "end": 4538.8, "text": " We can grab something about Genghis Khan, SplitWiki, and then from folder as before.", "tokens": [492, 393, 4444, 746, 466, 460, 1501, 18300, 18136, 11, 45111, 54, 9850, 11, 293, 550, 490, 10820, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.1685687034360824, "compression_ratio": 1.3933333333333333, "no_speech_prob": 3.763306813198142e-05}, {"id": 858, "seek": 453880, "start": 4538.8, "end": 4544.6, "text": " But we've got one extra step, which is we've added a processor line.", "tokens": [583, 321, 600, 658, 472, 2857, 1823, 11, 597, 307, 321, 600, 3869, 257, 15321, 1622, 13], "temperature": 0.0, "avg_logprob": -0.13821513478348896, "compression_ratio": 1.656084656084656, "no_speech_prob": 3.590894630178809e-05}, {"id": 859, "seek": 453880, "start": 4544.6, "end": 4552.66, "text": " So by default, FastAI uses the spacey tokenizer to tokenize the text.", "tokens": [407, 538, 7576, 11, 15968, 48698, 4960, 264, 1901, 88, 14862, 6545, 281, 14862, 1125, 264, 2487, 13], "temperature": 0.0, "avg_logprob": -0.13821513478348896, "compression_ratio": 1.656084656084656, "no_speech_prob": 3.590894630178809e-05}, {"id": 860, "seek": 453880, "start": 4552.66, "end": 4556.84, "text": " If you want to use something else, like in our case we want to use sentence piece, you", "tokens": [759, 291, 528, 281, 764, 746, 1646, 11, 411, 294, 527, 1389, 321, 528, 281, 764, 8174, 2522, 11, 291], "temperature": 0.0, "avg_logprob": -0.13821513478348896, "compression_ratio": 1.656084656084656, "no_speech_prob": 3.590894630178809e-05}, {"id": 861, "seek": 453880, "start": 4556.84, "end": 4562.52, "text": " can replace the processor with sp-processor, which stands for sentence piece processor.", "tokens": [393, 7406, 264, 15321, 365, 637, 12, 4318, 25432, 11, 597, 7382, 337, 8174, 2522, 15321, 13], "temperature": 0.0, "avg_logprob": -0.13821513478348896, "compression_ratio": 1.656084656084656, "no_speech_prob": 3.590894630178809e-05}, {"id": 862, "seek": 456252, "start": 4562.52, "end": 4570.88, "text": " So if you do this, then this says don't tokenize on space boundaries, but instead learn a model", "tokens": [407, 498, 291, 360, 341, 11, 550, 341, 1619, 500, 380, 14862, 1125, 322, 1901, 13180, 11, 457, 2602, 1466, 257, 2316], "temperature": 0.0, "avg_logprob": -0.18007173992338635, "compression_ratio": 1.5634517766497462, "no_speech_prob": 7.889161679486278e-06}, {"id": 863, "seek": 456252, "start": 4570.88, "end": 4573.8, "text": " from the text, please.", "tokens": [490, 264, 2487, 11, 1767, 13], "temperature": 0.0, "avg_logprob": -0.18007173992338635, "compression_ratio": 1.5634517766497462, "no_speech_prob": 7.889161679486278e-06}, {"id": 864, "seek": 456252, "start": 4573.8, "end": 4580.4800000000005, "text": " And so now when you go show batch, you can see there is the sentence piece tokenized", "tokens": [400, 370, 586, 562, 291, 352, 855, 15245, 11, 291, 393, 536, 456, 307, 264, 8174, 2522, 14862, 1602], "temperature": 0.0, "avg_logprob": -0.18007173992338635, "compression_ratio": 1.5634517766497462, "no_speech_prob": 7.889161679486278e-06}, {"id": 865, "seek": 456252, "start": 4580.4800000000005, "end": 4582.6, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.18007173992338635, "compression_ratio": 1.5634517766497462, "no_speech_prob": 7.889161679486278e-06}, {"id": 866, "seek": 456252, "start": 4582.6, "end": 4584.76, "text": " And from there, everything is exactly the same as before.", "tokens": [400, 490, 456, 11, 1203, 307, 2293, 264, 912, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.18007173992338635, "compression_ratio": 1.5634517766497462, "no_speech_prob": 7.889161679486278e-06}, {"id": 867, "seek": 456252, "start": 4584.76, "end": 4587.400000000001, "text": " Language model learner, and you fit it.", "tokens": [24445, 2316, 33347, 11, 293, 291, 3318, 309, 13], "temperature": 0.0, "avg_logprob": -0.18007173992338635, "compression_ratio": 1.5634517766497462, "no_speech_prob": 7.889161679486278e-06}, {"id": 868, "seek": 458740, "start": 4587.4, "end": 4593.4, "text": " 38% accuracy at predicting the next subword in Turkish Wikipedia, and then you can save", "tokens": [12843, 4, 14170, 412, 32884, 264, 958, 1422, 7462, 294, 18565, 28999, 11, 293, 550, 291, 393, 3155], "temperature": 0.0, "avg_logprob": -0.15959558369200907, "compression_ratio": 1.5093457943925233, "no_speech_prob": 3.882985402015038e-05}, {"id": 869, "seek": 458740, "start": 4593.4, "end": 4595.4, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.15959558369200907, "compression_ratio": 1.5093457943925233, "no_speech_prob": 3.882985402015038e-05}, {"id": 870, "seek": 458740, "start": 4595.4, "end": 4602.48, "text": " And so now we can do sentiment analysis in Turkish.", "tokens": [400, 370, 586, 321, 393, 360, 16149, 5215, 294, 18565, 13], "temperature": 0.0, "avg_logprob": -0.15959558369200907, "compression_ratio": 1.5093457943925233, "no_speech_prob": 3.882985402015038e-05}, {"id": 871, "seek": 458740, "start": 4602.48, "end": 4609.12, "text": " So after a lot of digging around, I eventually found a text classification dataset for Turkish,", "tokens": [407, 934, 257, 688, 295, 17343, 926, 11, 286, 4728, 1352, 257, 2487, 21538, 28872, 337, 18565, 11], "temperature": 0.0, "avg_logprob": -0.15959558369200907, "compression_ratio": 1.5093457943925233, "no_speech_prob": 3.882985402015038e-05}, {"id": 872, "seek": 458740, "start": 4609.12, "end": 4612.139999999999, "text": " which you can find here.", "tokens": [597, 291, 393, 915, 510, 13], "temperature": 0.0, "avg_logprob": -0.15959558369200907, "compression_ratio": 1.5093457943925233, "no_speech_prob": 3.882985402015038e-05}, {"id": 873, "seek": 458740, "start": 4612.139999999999, "end": 4614.759999999999, "text": " And so again, people love their movie reviews, apparently.", "tokens": [400, 370, 797, 11, 561, 959, 641, 3169, 10229, 11, 7970, 13], "temperature": 0.0, "avg_logprob": -0.15959558369200907, "compression_ratio": 1.5093457943925233, "no_speech_prob": 3.882985402015038e-05}, {"id": 874, "seek": 461476, "start": 4614.76, "end": 4617.84, "text": " This is a movie review dataset in Turkish.", "tokens": [639, 307, 257, 3169, 3131, 28872, 294, 18565, 13], "temperature": 0.0, "avg_logprob": -0.12031881199326626, "compression_ratio": 1.7556818181818181, "no_speech_prob": 4.75742454000283e-05}, {"id": 875, "seek": 461476, "start": 4617.84, "end": 4624.4400000000005, "text": " And in this case, there was one file called trpolarity.pos, which was a file containing", "tokens": [400, 294, 341, 1389, 11, 456, 390, 472, 3991, 1219, 504, 12892, 17409, 13, 30010, 11, 597, 390, 257, 3991, 19273], "temperature": 0.0, "avg_logprob": -0.12031881199326626, "compression_ratio": 1.7556818181818181, "no_speech_prob": 4.75742454000283e-05}, {"id": 876, "seek": 461476, "start": 4624.4400000000005, "end": 4629.16, "text": " all of the positive reviews, and there was a trpolarity.neg containing all the negative", "tokens": [439, 295, 264, 3353, 10229, 11, 293, 456, 390, 257, 504, 12892, 17409, 13, 28561, 19273, 439, 264, 3671], "temperature": 0.0, "avg_logprob": -0.12031881199326626, "compression_ratio": 1.7556818181818181, "no_speech_prob": 4.75742454000283e-05}, {"id": 877, "seek": 461476, "start": 4629.16, "end": 4630.96, "text": " reviews.", "tokens": [10229, 13], "temperature": 0.0, "avg_logprob": -0.12031881199326626, "compression_ratio": 1.7556818181818181, "no_speech_prob": 4.75742454000283e-05}, {"id": 878, "seek": 461476, "start": 4630.96, "end": 4638.360000000001, "text": " So all I did was I opened up that file, and I read the lines, and I got an error.", "tokens": [407, 439, 286, 630, 390, 286, 5625, 493, 300, 3991, 11, 293, 286, 1401, 264, 3876, 11, 293, 286, 658, 364, 6713, 13], "temperature": 0.0, "avg_logprob": -0.12031881199326626, "compression_ratio": 1.7556818181818181, "no_speech_prob": 4.75742454000283e-05}, {"id": 879, "seek": 463836, "start": 4638.36, "end": 4644.759999999999, "text": " And the error I got was that it was unable to read the file using UTF-8.", "tokens": [400, 264, 6713, 286, 658, 390, 300, 309, 390, 11299, 281, 1401, 264, 3991, 1228, 624, 20527, 12, 23, 13], "temperature": 0.0, "avg_logprob": -0.10815916168555785, "compression_ratio": 1.5112107623318385, "no_speech_prob": 1.863081706687808e-05}, {"id": 880, "seek": 463836, "start": 4644.759999999999, "end": 4650.96, "text": " And so here's the next thing that you're going to come across all the time as an NLP practitioner,", "tokens": [400, 370, 510, 311, 264, 958, 551, 300, 291, 434, 516, 281, 808, 2108, 439, 264, 565, 382, 364, 426, 45196, 32125, 11], "temperature": 0.0, "avg_logprob": -0.10815916168555785, "compression_ratio": 1.5112107623318385, "no_speech_prob": 1.863081706687808e-05}, {"id": 881, "seek": 463836, "start": 4650.96, "end": 4660.5199999999995, "text": " which is the way that letters are stored on disk until recently was ill-defined.", "tokens": [597, 307, 264, 636, 300, 7825, 366, 12187, 322, 12355, 1826, 3938, 390, 3171, 12, 37716, 13], "temperature": 0.0, "avg_logprob": -0.10815916168555785, "compression_ratio": 1.5112107623318385, "no_speech_prob": 1.863081706687808e-05}, {"id": 882, "seek": 463836, "start": 4660.5199999999995, "end": 4667.599999999999, "text": " We used a character set called ASCII that had a unique mapping from the numbers 0 to", "tokens": [492, 1143, 257, 2517, 992, 1219, 7469, 34, 9503, 300, 632, 257, 3845, 18350, 490, 264, 3547, 1958, 281], "temperature": 0.0, "avg_logprob": -0.10815916168555785, "compression_ratio": 1.5112107623318385, "no_speech_prob": 1.863081706687808e-05}, {"id": 883, "seek": 466760, "start": 4667.6, "end": 4675.68, "text": " 27 to letters of the alphabet and punctuation and so forth.", "tokens": [7634, 281, 7825, 295, 264, 23339, 293, 27006, 16073, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.17411497963799372, "compression_ratio": 1.6103896103896105, "no_speech_prob": 2.7967680580331944e-05}, {"id": 884, "seek": 466760, "start": 4675.68, "end": 4679.160000000001, "text": " The A in ASCII stands for American.", "tokens": [440, 316, 294, 7469, 34, 9503, 7382, 337, 2665, 13], "temperature": 0.0, "avg_logprob": -0.17411497963799372, "compression_ratio": 1.6103896103896105, "no_speech_prob": 2.7967680580331944e-05}, {"id": 885, "seek": 466760, "start": 4679.160000000001, "end": 4680.54, "text": " So it's just designed for Americans.", "tokens": [407, 309, 311, 445, 4761, 337, 6280, 13], "temperature": 0.0, "avg_logprob": -0.17411497963799372, "compression_ratio": 1.6103896103896105, "no_speech_prob": 2.7967680580331944e-05}, {"id": 886, "seek": 466760, "start": 4680.54, "end": 4685.200000000001, "text": " So when folks in France decided that they wanted to put things on computers as well,", "tokens": [407, 562, 4024, 294, 6190, 3047, 300, 436, 1415, 281, 829, 721, 322, 10807, 382, 731, 11], "temperature": 0.0, "avg_logprob": -0.17411497963799372, "compression_ratio": 1.6103896103896105, "no_speech_prob": 2.7967680580331944e-05}, {"id": 887, "seek": 466760, "start": 4685.200000000001, "end": 4689.52, "text": " they changed some of the numbers to represent different characters.", "tokens": [436, 3105, 512, 295, 264, 3547, 281, 2906, 819, 4342, 13], "temperature": 0.0, "avg_logprob": -0.17411497963799372, "compression_ratio": 1.6103896103896105, "no_speech_prob": 2.7967680580331944e-05}, {"id": 888, "seek": 466760, "start": 4689.52, "end": 4694.4800000000005, "text": " So if you open exactly the same file on a French computer versus an American computer,", "tokens": [407, 498, 291, 1269, 2293, 264, 912, 3991, 322, 257, 5522, 3820, 5717, 364, 2665, 3820, 11], "temperature": 0.0, "avg_logprob": -0.17411497963799372, "compression_ratio": 1.6103896103896105, "no_speech_prob": 2.7967680580331944e-05}, {"id": 889, "seek": 469448, "start": 4694.48, "end": 4698.4, "text": " you'll see something different.", "tokens": [291, 603, 536, 746, 819, 13], "temperature": 0.0, "avg_logprob": -0.10044600413395809, "compression_ratio": 1.4575471698113207, "no_speech_prob": 4.63783590021194e-06}, {"id": 890, "seek": 469448, "start": 4698.4, "end": 4706.16, "text": " And this got particularly crazy in Japan, for example, because Japan has 4,000 characters,", "tokens": [400, 341, 658, 4098, 3219, 294, 3367, 11, 337, 1365, 11, 570, 3367, 575, 1017, 11, 1360, 4342, 11], "temperature": 0.0, "avg_logprob": -0.10044600413395809, "compression_ratio": 1.4575471698113207, "no_speech_prob": 4.63783590021194e-06}, {"id": 891, "seek": 469448, "start": 4706.16, "end": 4709.379999999999, "text": " which obviously you can't fit in 127.", "tokens": [597, 2745, 291, 393, 380, 3318, 294, 47561, 13], "temperature": 0.0, "avg_logprob": -0.10044600413395809, "compression_ratio": 1.4575471698113207, "no_speech_prob": 4.63783590021194e-06}, {"id": 892, "seek": 469448, "start": 4709.379999999999, "end": 4715.48, "text": " So they invented their own encoding using more than 127 numbers.", "tokens": [407, 436, 14479, 641, 1065, 43430, 1228, 544, 813, 47561, 3547, 13], "temperature": 0.0, "avg_logprob": -0.10044600413395809, "compression_ratio": 1.4575471698113207, "no_speech_prob": 4.63783590021194e-06}, {"id": 893, "seek": 469448, "start": 4715.48, "end": 4721.4, "text": " So this all got kind of saved a few years ago by something called Unicode, which is", "tokens": [407, 341, 439, 658, 733, 295, 6624, 257, 1326, 924, 2057, 538, 746, 1219, 1156, 299, 1429, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.10044600413395809, "compression_ratio": 1.4575471698113207, "no_speech_prob": 4.63783590021194e-06}, {"id": 894, "seek": 472140, "start": 4721.4, "end": 4725.96, "text": " nowadays maintained by something called the Unicode Consortium.", "tokens": [13434, 17578, 538, 746, 1219, 264, 1156, 299, 1429, 31719, 2197, 13], "temperature": 0.0, "avg_logprob": -0.12955288205827986, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.6700374544598162e-05}, {"id": 895, "seek": 472140, "start": 4725.96, "end": 4735.36, "text": " And Unicode consists of millions of characters, including emojis, and they keep on adding", "tokens": [400, 1156, 299, 1429, 14689, 295, 6803, 295, 4342, 11, 3009, 19611, 40371, 11, 293, 436, 1066, 322, 5127], "temperature": 0.0, "avg_logprob": -0.12955288205827986, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.6700374544598162e-05}, {"id": 896, "seek": 472140, "start": 4735.36, "end": 4737.719999999999, "text": " new emojis every year.", "tokens": [777, 19611, 40371, 633, 1064, 13], "temperature": 0.0, "avg_logprob": -0.12955288205827986, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.6700374544598162e-05}, {"id": 897, "seek": 472140, "start": 4737.719999999999, "end": 4744.58, "text": " For example, this year they added emoji variants that provide different skin tones for the", "tokens": [1171, 1365, 11, 341, 1064, 436, 3869, 31595, 21669, 300, 2893, 819, 3178, 19995, 337, 264], "temperature": 0.0, "avg_logprob": -0.12955288205827986, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.6700374544598162e-05}, {"id": 898, "seek": 472140, "start": 4744.58, "end": 4749.0, "text": " various people emojis.", "tokens": [3683, 561, 19611, 40371, 13], "temperature": 0.0, "avg_logprob": -0.12955288205827986, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.6700374544598162e-05}, {"id": 899, "seek": 474900, "start": 4749.0, "end": 4755.48, "text": " The problem is that the dataset that I was looking at, and this happens quite frequently,", "tokens": [440, 1154, 307, 300, 264, 28872, 300, 286, 390, 1237, 412, 11, 293, 341, 2314, 1596, 10374, 11], "temperature": 0.0, "avg_logprob": -0.11183115414210729, "compression_ratio": 1.50253807106599, "no_speech_prob": 2.0144396330579184e-05}, {"id": 900, "seek": 474900, "start": 4755.48, "end": 4760.56, "text": " it was from 2013, where Unicode was around, but not everybody was using it, it was not", "tokens": [309, 390, 490, 9012, 11, 689, 1156, 299, 1429, 390, 926, 11, 457, 406, 2201, 390, 1228, 309, 11, 309, 390, 406], "temperature": 0.0, "avg_logprob": -0.11183115414210729, "compression_ratio": 1.50253807106599, "no_speech_prob": 2.0144396330579184e-05}, {"id": 901, "seek": 474900, "start": 4760.56, "end": 4763.12, "text": " saved in Unicode.", "tokens": [6624, 294, 1156, 299, 1429, 13], "temperature": 0.0, "avg_logprob": -0.11183115414210729, "compression_ratio": 1.50253807106599, "no_speech_prob": 2.0144396330579184e-05}, {"id": 902, "seek": 474900, "start": 4763.12, "end": 4770.68, "text": " And so there's actually no correct way to open a file that's saved not in Unicode.", "tokens": [400, 370, 456, 311, 767, 572, 3006, 636, 281, 1269, 257, 3991, 300, 311, 6624, 406, 294, 1156, 299, 1429, 13], "temperature": 0.0, "avg_logprob": -0.11183115414210729, "compression_ratio": 1.50253807106599, "no_speech_prob": 2.0144396330579184e-05}, {"id": 903, "seek": 474900, "start": 4770.68, "end": 4771.88, "text": " You have to guess.", "tokens": [509, 362, 281, 2041, 13], "temperature": 0.0, "avg_logprob": -0.11183115414210729, "compression_ratio": 1.50253807106599, "no_speech_prob": 2.0144396330579184e-05}, {"id": 904, "seek": 477188, "start": 4771.88, "end": 4779.2, "text": " So I googled for Turkish language encoding, and I found a page that said, quite often", "tokens": [407, 286, 50061, 1493, 337, 18565, 2856, 43430, 11, 293, 286, 1352, 257, 3028, 300, 848, 11, 1596, 2049], "temperature": 0.0, "avg_logprob": -0.1445699202275909, "compression_ratio": 1.75, "no_speech_prob": 1.4970800293667708e-05}, {"id": 905, "seek": 477188, "start": 4779.2, "end": 4785.16, "text": " in Turkey people tend to use ISO 8859-9 encoding except when they don't.", "tokens": [294, 12647, 561, 3928, 281, 764, 25042, 1649, 19287, 24, 12, 24, 43430, 3993, 562, 436, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.1445699202275909, "compression_ratio": 1.75, "no_speech_prob": 1.4970800293667708e-05}, {"id": 906, "seek": 477188, "start": 4785.16, "end": 4789.52, "text": " So open it and then look for things that look like this, and if they look like that, then", "tokens": [407, 1269, 309, 293, 550, 574, 337, 721, 300, 574, 411, 341, 11, 293, 498, 436, 574, 411, 300, 11, 550], "temperature": 0.0, "avg_logprob": -0.1445699202275909, "compression_ratio": 1.75, "no_speech_prob": 1.4970800293667708e-05}, {"id": 907, "seek": 477188, "start": 4789.52, "end": 4792.66, "text": " it's not that encoding, but it is that encoding.", "tokens": [309, 311, 406, 300, 43430, 11, 457, 309, 307, 300, 43430, 13], "temperature": 0.0, "avg_logprob": -0.1445699202275909, "compression_ratio": 1.75, "no_speech_prob": 1.4970800293667708e-05}, {"id": 908, "seek": 477188, "start": 4792.66, "end": 4796.68, "text": " So I tried it, and they looked like the thing they were meant to look like, so that's why", "tokens": [407, 286, 3031, 309, 11, 293, 436, 2956, 411, 264, 551, 436, 645, 4140, 281, 574, 411, 11, 370, 300, 311, 983], "temperature": 0.0, "avg_logprob": -0.1445699202275909, "compression_ratio": 1.75, "no_speech_prob": 1.4970800293667708e-05}, {"id": 909, "seek": 477188, "start": 4796.68, "end": 4799.12, "text": " I had to add this.", "tokens": [286, 632, 281, 909, 341, 13], "temperature": 0.0, "avg_logprob": -0.1445699202275909, "compression_ratio": 1.75, "no_speech_prob": 1.4970800293667708e-05}, {"id": 910, "seek": 479912, "start": 4799.12, "end": 4802.92, "text": " So again, little things you have to deal with.", "tokens": [407, 797, 11, 707, 721, 291, 362, 281, 2028, 365, 13], "temperature": 0.0, "avg_logprob": -0.11411890419580603, "compression_ratio": 1.545045045045045, "no_speech_prob": 1.723117384244688e-05}, {"id": 911, "seek": 479912, "start": 4802.92, "end": 4811.76, "text": " So I now know about the five letters that are represented differently in Icelandic versus", "tokens": [407, 286, 586, 458, 466, 264, 1732, 7825, 300, 366, 10379, 7614, 294, 28004, 299, 5717], "temperature": 0.0, "avg_logprob": -0.11411890419580603, "compression_ratio": 1.545045045045045, "no_speech_prob": 1.723117384244688e-05}, {"id": 912, "seek": 479912, "start": 4811.76, "end": 4819.04, "text": " Turkish, which is how you can tell the difference between 8859-9 and 8859-5, which is something", "tokens": [18565, 11, 597, 307, 577, 291, 393, 980, 264, 2649, 1296, 1649, 19287, 24, 12, 24, 293, 1649, 19287, 24, 12, 20, 11, 597, 307, 746], "temperature": 0.0, "avg_logprob": -0.11411890419580603, "compression_ratio": 1.545045045045045, "no_speech_prob": 1.723117384244688e-05}, {"id": 913, "seek": 479912, "start": 4819.04, "end": 4822.599999999999, "text": " I don't know I'll ever use again.", "tokens": [286, 500, 380, 458, 286, 603, 1562, 764, 797, 13], "temperature": 0.0, "avg_logprob": -0.11411890419580603, "compression_ratio": 1.545045045045045, "no_speech_prob": 1.723117384244688e-05}, {"id": 914, "seek": 479912, "start": 4822.599999999999, "end": 4823.599999999999, "text": " But there you go.", "tokens": [583, 456, 291, 352, 13], "temperature": 0.0, "avg_logprob": -0.11411890419580603, "compression_ratio": 1.545045045045045, "no_speech_prob": 1.723117384244688e-05}, {"id": 915, "seek": 479912, "start": 4823.599999999999, "end": 4827.48, "text": " So thank heavens for the internet telling us these things.", "tokens": [407, 1309, 26011, 337, 264, 4705, 3585, 505, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.11411890419580603, "compression_ratio": 1.545045045045045, "no_speech_prob": 1.723117384244688e-05}, {"id": 916, "seek": 482748, "start": 4827.48, "end": 4830.0, "text": " So that's what the encoding is.", "tokens": [407, 300, 311, 437, 264, 43430, 307, 13], "temperature": 0.0, "avg_logprob": -0.13258453651710791, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.1659119991236366e-05}, {"id": 917, "seek": 482748, "start": 4830.0, "end": 4836.5599999999995, "text": " So I turned that list of lines into a data frame, and so I just call that the text column,", "tokens": [407, 286, 3574, 300, 1329, 295, 3876, 666, 257, 1412, 3920, 11, 293, 370, 286, 445, 818, 300, 264, 2487, 7738, 11], "temperature": 0.0, "avg_logprob": -0.13258453651710791, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.1659119991236366e-05}, {"id": 918, "seek": 482748, "start": 4836.5599999999995, "end": 4842.08, "text": " and then I've got to add a label called positive as in positive or negative, and for my positive", "tokens": [293, 550, 286, 600, 658, 281, 909, 257, 7645, 1219, 3353, 382, 294, 3353, 420, 3671, 11, 293, 337, 452, 3353], "temperature": 0.0, "avg_logprob": -0.13258453651710791, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.1659119991236366e-05}, {"id": 919, "seek": 482748, "start": 4842.08, "end": 4847.2, "text": " ones, I'll set it to 1, and then I'll do the same thing for the negative data set, and", "tokens": [2306, 11, 286, 603, 992, 309, 281, 502, 11, 293, 550, 286, 603, 360, 264, 912, 551, 337, 264, 3671, 1412, 992, 11, 293], "temperature": 0.0, "avg_logprob": -0.13258453651710791, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.1659119991236366e-05}, {"id": 920, "seek": 482748, "start": 4847.2, "end": 4853.16, "text": " so the positive column I'll set to 0, and then I will concatenate the two together,", "tokens": [370, 264, 3353, 7738, 286, 603, 992, 281, 1958, 11, 293, 550, 286, 486, 1588, 7186, 473, 264, 732, 1214, 11], "temperature": 0.0, "avg_logprob": -0.13258453651710791, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.1659119991236366e-05}, {"id": 921, "seek": 485316, "start": 4853.16, "end": 4859.5599999999995, "text": " and then the rest of it is exactly the same as we had before, except rather than going", "tokens": [293, 550, 264, 1472, 295, 309, 307, 2293, 264, 912, 382, 321, 632, 949, 11, 3993, 2831, 813, 516], "temperature": 0.0, "avg_logprob": -0.13659309205554782, "compression_ratio": 1.586046511627907, "no_speech_prob": 9.972542102332227e-06}, {"id": 922, "seek": 485316, "start": 4859.5599999999995, "end": 4867.48, "text": " spprocessor parentheses to create spprocessor, I need to make sure I use the same sentence", "tokens": [637, 4318, 25432, 34153, 281, 1884, 637, 4318, 25432, 11, 286, 643, 281, 652, 988, 286, 764, 264, 912, 8174], "temperature": 0.0, "avg_logprob": -0.13659309205554782, "compression_ratio": 1.586046511627907, "no_speech_prob": 9.972542102332227e-06}, {"id": 923, "seek": 485316, "start": 4867.48, "end": 4873.92, "text": " piece vocabulary and model as we used for the Turkish Wikipedia model.", "tokens": [2522, 19864, 293, 2316, 382, 321, 1143, 337, 264, 18565, 28999, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13659309205554782, "compression_ratio": 1.586046511627907, "no_speech_prob": 9.972542102332227e-06}, {"id": 924, "seek": 485316, "start": 4873.92, "end": 4878.12, "text": " Otherwise it's going to tokenize it in a totally different way, and it won't make any sense.", "tokens": [10328, 309, 311, 516, 281, 14862, 1125, 309, 294, 257, 3879, 819, 636, 11, 293, 309, 1582, 380, 652, 604, 2020, 13], "temperature": 0.0, "avg_logprob": -0.13659309205554782, "compression_ratio": 1.586046511627907, "no_speech_prob": 9.972542102332227e-06}, {"id": 925, "seek": 487812, "start": 4878.12, "end": 4887.76, "text": " So if you say spprocessor.load and pass in the path to your data bunch from your pre-trained", "tokens": [407, 498, 291, 584, 637, 4318, 25432, 13, 2907, 293, 1320, 294, 264, 3100, 281, 428, 1412, 3840, 490, 428, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.09299721483324395, "compression_ratio": 1.5064935064935066, "no_speech_prob": 3.0241253625717945e-05}, {"id": 926, "seek": 487812, "start": 4887.76, "end": 4896.0, "text": " Wikipedia file, then that will load up the sentence piece vocab.", "tokens": [28999, 3991, 11, 550, 300, 486, 3677, 493, 264, 8174, 2522, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.09299721483324395, "compression_ratio": 1.5064935064935066, "no_speech_prob": 3.0241253625717945e-05}, {"id": 927, "seek": 487812, "start": 4896.0, "end": 4902.68, "text": " I never saved the sentence piece vocab, it's actually saved automatically.", "tokens": [286, 1128, 6624, 264, 8174, 2522, 2329, 455, 11, 309, 311, 767, 6624, 6772, 13], "temperature": 0.0, "avg_logprob": -0.09299721483324395, "compression_ratio": 1.5064935064935066, "no_speech_prob": 3.0241253625717945e-05}, {"id": 928, "seek": 490268, "start": 4902.68, "end": 4908.240000000001, "text": " So this way I now can say show batch, and you'll find that it's tokenized in the same", "tokens": [407, 341, 636, 286, 586, 393, 584, 855, 15245, 11, 293, 291, 603, 915, 300, 309, 311, 14862, 1602, 294, 264, 912], "temperature": 0.0, "avg_logprob": -0.18414579738270154, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0003513634146656841}, {"id": 929, "seek": 490268, "start": 4908.240000000001, "end": 4913.320000000001, "text": " way as before, and sometimes you can see that there are words which in this case is turned", "tokens": [636, 382, 949, 11, 293, 2171, 291, 393, 536, 300, 456, 366, 2283, 597, 294, 341, 1389, 307, 3574], "temperature": 0.0, "avg_logprob": -0.18414579738270154, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0003513634146656841}, {"id": 930, "seek": 490268, "start": 4913.320000000001, "end": 4920.4800000000005, "text": " into 1, 2, 3, 4, 5 tokens for a single word, for instance.", "tokens": [666, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 22667, 337, 257, 2167, 1349, 11, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.18414579738270154, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0003513634146656841}, {"id": 931, "seek": 490268, "start": 4920.4800000000005, "end": 4923.6, "text": " Is that a very rare word or a name?", "tokens": [1119, 300, 257, 588, 5892, 1349, 420, 257, 1315, 30], "temperature": 0.0, "avg_logprob": -0.18414579738270154, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0003513634146656841}, {"id": 932, "seek": 490268, "start": 4923.6, "end": 4924.6, "text": " It's actually two words.", "tokens": [467, 311, 767, 732, 2283, 13], "temperature": 0.0, "avg_logprob": -0.18414579738270154, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0003513634146656841}, {"id": 933, "seek": 492460, "start": 4924.6, "end": 4934.240000000001, "text": " There's no space there for some reason.", "tokens": [821, 311, 572, 1901, 456, 337, 512, 1778, 13], "temperature": 0.0, "avg_logprob": -0.14486157492305454, "compression_ratio": 1.5633802816901408, "no_speech_prob": 1.5206214811769314e-05}, {"id": 934, "seek": 492460, "start": 4934.240000000001, "end": 4938.04, "text": " So then we can create our language model learner, and like before we have to say pre-trained", "tokens": [407, 550, 321, 393, 1884, 527, 2856, 2316, 33347, 11, 293, 411, 949, 321, 362, 281, 584, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.14486157492305454, "compression_ratio": 1.5633802816901408, "no_speech_prob": 1.5206214811769314e-05}, {"id": 935, "seek": 492460, "start": 4938.04, "end": 4943.68, "text": " Fnames to grab our pre-trained Turkish Wikipedia, and then we train it in the usual way, and", "tokens": [479, 77, 1632, 281, 4444, 527, 659, 12, 17227, 2001, 18565, 28999, 11, 293, 550, 321, 3847, 309, 294, 264, 7713, 636, 11, 293], "temperature": 0.0, "avg_logprob": -0.14486157492305454, "compression_ratio": 1.5633802816901408, "no_speech_prob": 1.5206214811769314e-05}, {"id": 936, "seek": 492460, "start": 4943.68, "end": 4948.68, "text": " so we now have something that's 36% accurate at predicting the next subword unit of a Turkish", "tokens": [370, 321, 586, 362, 746, 300, 311, 8652, 4, 8559, 412, 32884, 264, 958, 1422, 7462, 4985, 295, 257, 18565], "temperature": 0.0, "avg_logprob": -0.14486157492305454, "compression_ratio": 1.5633802816901408, "no_speech_prob": 1.5206214811769314e-05}, {"id": 937, "seek": 492460, "start": 4948.68, "end": 4950.84, "text": " movie review.", "tokens": [3169, 3131, 13], "temperature": 0.0, "avg_logprob": -0.14486157492305454, "compression_ratio": 1.5633802816901408, "no_speech_prob": 1.5206214811769314e-05}, {"id": 938, "seek": 495084, "start": 4950.84, "end": 4959.64, "text": " And so now we can do the same thing to create our classifier data bunch and create a classifier,", "tokens": [400, 370, 586, 321, 393, 360, 264, 912, 551, 281, 1884, 527, 1508, 9902, 1412, 3840, 293, 1884, 257, 1508, 9902, 11], "temperature": 0.0, "avg_logprob": -0.13050021444048202, "compression_ratio": 1.5285714285714285, "no_speech_prob": 4.710828761744779e-06}, {"id": 939, "seek": 495084, "start": 4959.64, "end": 4970.6, "text": " load in that encoder that we just saved, and train it for a while, and we get to 89% accuracy.", "tokens": [3677, 294, 300, 2058, 19866, 300, 321, 445, 6624, 11, 293, 3847, 309, 337, 257, 1339, 11, 293, 321, 483, 281, 31877, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.13050021444048202, "compression_ratio": 1.5285714285714285, "no_speech_prob": 4.710828761744779e-06}, {"id": 940, "seek": 495084, "start": 4970.6, "end": 4975.24, "text": " So I Googled around and I found a 2018 paper where somebody used this dataset to do sentiment", "tokens": [407, 286, 45005, 1493, 926, 293, 286, 1352, 257, 6096, 3035, 689, 2618, 1143, 341, 28872, 281, 360, 16149], "temperature": 0.0, "avg_logprob": -0.13050021444048202, "compression_ratio": 1.5285714285714285, "no_speech_prob": 4.710828761744779e-06}, {"id": 941, "seek": 495084, "start": 4975.24, "end": 4979.0, "text": " analysis and they got 75% accuracy.", "tokens": [5215, 293, 436, 658, 9562, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.13050021444048202, "compression_ratio": 1.5285714285714285, "no_speech_prob": 4.710828761744779e-06}, {"id": 942, "seek": 497900, "start": 4979.0, "end": 4987.52, "text": " So this is like what you often find on, if you move outside the world of Chinese and", "tokens": [407, 341, 307, 411, 437, 291, 2049, 915, 322, 11, 498, 291, 1286, 2380, 264, 1002, 295, 4649, 293], "temperature": 0.0, "avg_logprob": -0.12347409632298853, "compression_ratio": 1.5545023696682465, "no_speech_prob": 1.0129592737939674e-05}, {"id": 943, "seek": 497900, "start": 4987.52, "end": 4993.32, "text": " English AI, because Chinese and English are the main languages that people are writing", "tokens": [3669, 7318, 11, 570, 4649, 293, 3669, 366, 264, 2135, 8650, 300, 561, 366, 3579], "temperature": 0.0, "avg_logprob": -0.12347409632298853, "compression_ratio": 1.5545023696682465, "no_speech_prob": 1.0129592737939674e-05}, {"id": 944, "seek": 497900, "start": 4993.32, "end": 4999.46, "text": " AI papers in, you'll often find that stuff is pretty low-hanging fruit.", "tokens": [7318, 10577, 294, 11, 291, 603, 2049, 915, 300, 1507, 307, 1238, 2295, 12, 71, 9741, 6773, 13], "temperature": 0.0, "avg_logprob": -0.12347409632298853, "compression_ratio": 1.5545023696682465, "no_speech_prob": 1.0129592737939674e-05}, {"id": 945, "seek": 497900, "start": 4999.46, "end": 5004.72, "text": " So in this case we absolutely thrashed the best sentiment analysis benchmark I could", "tokens": [407, 294, 341, 1389, 321, 3122, 739, 12219, 264, 1151, 16149, 5215, 18927, 286, 727], "temperature": 0.0, "avg_logprob": -0.12347409632298853, "compression_ratio": 1.5545023696682465, "no_speech_prob": 1.0129592737939674e-05}, {"id": 946, "seek": 500472, "start": 5004.72, "end": 5012.2, "text": " find from 75% to 89% just by applying the same techniques we've seen.", "tokens": [915, 490, 9562, 4, 281, 31877, 4, 445, 538, 9275, 264, 912, 7512, 321, 600, 1612, 13], "temperature": 0.0, "avg_logprob": -0.12433756177670488, "compression_ratio": 1.5, "no_speech_prob": 4.860374701820547e-06}, {"id": 947, "seek": 500472, "start": 5012.2, "end": 5021.400000000001, "text": " So you can see that, as kind of Rachel's mentioned, where we're at with NLP right now is there's", "tokens": [407, 291, 393, 536, 300, 11, 382, 733, 295, 14246, 311, 2835, 11, 689, 321, 434, 412, 365, 426, 45196, 558, 586, 307, 456, 311], "temperature": 0.0, "avg_logprob": -0.12433756177670488, "compression_ratio": 1.5, "no_speech_prob": 4.860374701820547e-06}, {"id": 948, "seek": 500472, "start": 5021.400000000001, "end": 5024.4800000000005, "text": " been dramatic change in the last couple of years.", "tokens": [668, 12023, 1319, 294, 264, 1036, 1916, 295, 924, 13], "temperature": 0.0, "avg_logprob": -0.12433756177670488, "compression_ratio": 1.5, "no_speech_prob": 4.860374701820547e-06}, {"id": 949, "seek": 500472, "start": 5024.4800000000005, "end": 5026.16, "text": " Almost nobody's familiar with it.", "tokens": [12627, 5079, 311, 4963, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.12433756177670488, "compression_ratio": 1.5, "no_speech_prob": 4.860374701820547e-06}, {"id": 950, "seek": 500472, "start": 5026.16, "end": 5027.7, "text": " Everything's low-hanging fruit.", "tokens": [5471, 311, 2295, 12, 71, 9741, 6773, 13], "temperature": 0.0, "avg_logprob": -0.12433756177670488, "compression_ratio": 1.5, "no_speech_prob": 4.860374701820547e-06}, {"id": 951, "seek": 500472, "start": 5027.7, "end": 5032.0, "text": " Pretty much everything you try and apply it to, you'll probably get much better results", "tokens": [10693, 709, 1203, 291, 853, 293, 3079, 309, 281, 11, 291, 603, 1391, 483, 709, 1101, 3542], "temperature": 0.0, "avg_logprob": -0.12433756177670488, "compression_ratio": 1.5, "no_speech_prob": 4.860374701820547e-06}, {"id": 952, "seek": 500472, "start": 5032.0, "end": 5033.76, "text": " than anybody's ever got before.", "tokens": [813, 4472, 311, 1562, 658, 949, 13], "temperature": 0.0, "avg_logprob": -0.12433756177670488, "compression_ratio": 1.5, "no_speech_prob": 4.860374701820547e-06}, {"id": 953, "seek": 503376, "start": 5033.76, "end": 5036.9400000000005, "text": " There's opportunities to create products that didn't exist before.", "tokens": [821, 311, 4786, 281, 1884, 3383, 300, 994, 380, 2514, 949, 13], "temperature": 0.0, "avg_logprob": -0.12376859743301183, "compression_ratio": 1.4788732394366197, "no_speech_prob": 1.1842954336316325e-05}, {"id": 954, "seek": 503376, "start": 5036.9400000000005, "end": 5041.320000000001, "text": " Any product that currently uses language, you'll probably be able to make much better.", "tokens": [2639, 1674, 300, 4362, 4960, 2856, 11, 291, 603, 1391, 312, 1075, 281, 652, 709, 1101, 13], "temperature": 0.0, "avg_logprob": -0.12376859743301183, "compression_ratio": 1.4788732394366197, "no_speech_prob": 1.1842954336316325e-05}, {"id": 955, "seek": 503376, "start": 5041.320000000001, "end": 5048.84, "text": " And it's really all about taking advantage of transfer learning in this way.", "tokens": [400, 309, 311, 534, 439, 466, 1940, 5002, 295, 5003, 2539, 294, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.12376859743301183, "compression_ratio": 1.4788732394366197, "no_speech_prob": 1.1842954336316325e-05}, {"id": 956, "seek": 503376, "start": 5048.84, "end": 5054.68, "text": " So then finally, if you look in the FastAI repo, there's an examples directory where", "tokens": [407, 550, 2721, 11, 498, 291, 574, 294, 264, 15968, 48698, 49040, 11, 456, 311, 364, 5110, 21120, 689], "temperature": 0.0, "avg_logprob": -0.12376859743301183, "compression_ratio": 1.4788732394366197, "no_speech_prob": 1.1842954336316325e-05}, {"id": 957, "seek": 505468, "start": 5054.68, "end": 5064.6, "text": " you'll find ULMfit, which will show you the end-to-end steps for IMDb, basically what", "tokens": [291, 603, 915, 624, 43, 44, 6845, 11, 597, 486, 855, 291, 264, 917, 12, 1353, 12, 521, 4439, 337, 21463, 35, 65, 11, 1936, 437], "temperature": 0.0, "avg_logprob": -0.13557755457211848, "compression_ratio": 1.3611111111111112, "no_speech_prob": 7.411048045469215e-06}, {"id": 958, "seek": 505468, "start": 5064.6, "end": 5075.16, "text": " we've already seen, plus the backwards model, and at the end the ensemble.", "tokens": [321, 600, 1217, 1612, 11, 1804, 264, 12204, 2316, 11, 293, 412, 264, 917, 264, 19492, 13], "temperature": 0.0, "avg_logprob": -0.13557755457211848, "compression_ratio": 1.3611111111111112, "no_speech_prob": 7.411048045469215e-06}, {"id": 959, "seek": 505468, "start": 5075.16, "end": 5083.0, "text": " And so if you do this for English, you'll eventually get 95.4% accuracy, which as of", "tokens": [400, 370, 498, 291, 360, 341, 337, 3669, 11, 291, 603, 4728, 483, 13420, 13, 19, 4, 14170, 11, 597, 382, 295], "temperature": 0.0, "avg_logprob": -0.13557755457211848, "compression_ratio": 1.3611111111111112, "no_speech_prob": 7.411048045469215e-06}, {"id": 960, "seek": 508300, "start": 5083.0, "end": 5087.4, "text": " today is to the best of my knowledge, the world's best result has ever been got on this", "tokens": [965, 307, 281, 264, 1151, 295, 452, 3601, 11, 264, 1002, 311, 1151, 1874, 575, 1562, 668, 658, 322, 341], "temperature": 0.0, "avg_logprob": -0.19833753300809312, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.1659235497063491e-05}, {"id": 961, "seek": 508300, "start": 5087.4, "end": 5090.96, "text": " dataset without using data augmentation.", "tokens": [28872, 1553, 1228, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.19833753300809312, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.1659235497063491e-05}, {"id": 962, "seek": 508300, "start": 5090.96, "end": 5098.36, "text": " So now you know not just how to do this, but how to do it as well as anybody knows how,", "tokens": [407, 586, 291, 458, 406, 445, 577, 281, 360, 341, 11, 457, 577, 281, 360, 309, 382, 731, 382, 4472, 3255, 577, 11], "temperature": 0.0, "avg_logprob": -0.19833753300809312, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.1659235497063491e-05}, {"id": 963, "seek": 508300, "start": 5098.36, "end": 5100.74, "text": " as far as I know.", "tokens": [382, 1400, 382, 286, 458, 13], "temperature": 0.0, "avg_logprob": -0.19833753300809312, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.1659235497063491e-05}, {"id": 964, "seek": 508300, "start": 5100.74, "end": 5106.96, "text": " So now, Rachel, you're going to tell people a bit more about how it works.", "tokens": [407, 586, 11, 14246, 11, 291, 434, 516, 281, 980, 561, 257, 857, 544, 466, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.19833753300809312, "compression_ratio": 1.592783505154639, "no_speech_prob": 1.1659235497063491e-05}, {"id": 965, "seek": 510696, "start": 5106.96, "end": 5113.04, "text": " Actually, before I do that, I just wanted to ask a few questions to review what Jeremy", "tokens": [5135, 11, 949, 286, 360, 300, 11, 286, 445, 1415, 281, 1029, 257, 1326, 1651, 281, 3131, 437, 17809], "temperature": 0.0, "avg_logprob": -0.17782761255900065, "compression_ratio": 1.3741935483870968, "no_speech_prob": 9.168391989078373e-05}, {"id": 966, "seek": 510696, "start": 5113.04, "end": 5114.04, "text": " just covered.", "tokens": [445, 5343, 13], "temperature": 0.0, "avg_logprob": -0.17782761255900065, "compression_ratio": 1.3741935483870968, "no_speech_prob": 9.168391989078373e-05}, {"id": 967, "seek": 510696, "start": 5114.04, "end": 5124.04, "text": " And the first is, how are we able to use semi-supervised learning in the ULMfit example?", "tokens": [400, 264, 700, 307, 11, 577, 366, 321, 1075, 281, 764, 12909, 12, 48172, 24420, 2539, 294, 264, 624, 43, 44, 6845, 1365, 30], "temperature": 0.0, "avg_logprob": -0.17782761255900065, "compression_ratio": 1.3741935483870968, "no_speech_prob": 9.168391989078373e-05}, {"id": 968, "seek": 510696, "start": 5124.04, "end": 5130.32, "text": " Jeremy covered earlier.", "tokens": [17809, 5343, 3071, 13], "temperature": 0.0, "avg_logprob": -0.17782761255900065, "compression_ratio": 1.3741935483870968, "no_speech_prob": 9.168391989078373e-05}, {"id": 969, "seek": 513032, "start": 5130.32, "end": 5138.08, "text": " How are we able to use semi-supervised learning in the ULMfit example?", "tokens": [1012, 366, 321, 1075, 281, 764, 12909, 12, 48172, 24420, 2539, 294, 264, 624, 43, 44, 6845, 1365, 30], "temperature": 0.0, "avg_logprob": -0.3261582711163689, "compression_ratio": 1.4113924050632911, "no_speech_prob": 0.00010067791299661621}, {"id": 970, "seek": 513032, "start": 5138.08, "end": 5141.08, "text": " Can I give you the catch box?", "tokens": [1664, 286, 976, 291, 264, 3745, 2424, 30], "temperature": 0.0, "avg_logprob": -0.3261582711163689, "compression_ratio": 1.4113924050632911, "no_speech_prob": 0.00010067791299661621}, {"id": 971, "seek": 513032, "start": 5141.08, "end": 5152.88, "text": " I think it's by using a language model in the unlabeled data and then using the classifier", "tokens": [286, 519, 309, 311, 538, 1228, 257, 2856, 2316, 294, 264, 32118, 18657, 292, 1412, 293, 550, 1228, 264, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.3261582711163689, "compression_ratio": 1.4113924050632911, "no_speech_prob": 0.00010067791299661621}, {"id": 972, "seek": 513032, "start": 5152.88, "end": 5157.719999999999, "text": " to fine-tune on the label data.", "tokens": [281, 2489, 12, 83, 2613, 322, 264, 7645, 1412, 13], "temperature": 0.0, "avg_logprob": -0.3261582711163689, "compression_ratio": 1.4113924050632911, "no_speech_prob": 0.00010067791299661621}, {"id": 973, "seek": 515772, "start": 5157.72, "end": 5163.56, "text": " Correct, yeah, so the secret was that we were able to use unlabeled data for the language", "tokens": [12753, 11, 1338, 11, 370, 264, 4054, 390, 300, 321, 645, 1075, 281, 764, 32118, 18657, 292, 1412, 337, 264, 2856], "temperature": 0.0, "avg_logprob": -0.19107426634622277, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.119879693258554e-05}, {"id": 974, "seek": 515772, "start": 5163.56, "end": 5164.56, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.19107426634622277, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.119879693258554e-05}, {"id": 975, "seek": 515772, "start": 5164.56, "end": 5168.240000000001, "text": " So it's a language model, it's just predicting what word comes next.", "tokens": [407, 309, 311, 257, 2856, 2316, 11, 309, 311, 445, 32884, 437, 1349, 1487, 958, 13], "temperature": 0.0, "avg_logprob": -0.19107426634622277, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.119879693258554e-05}, {"id": 976, "seek": 515772, "start": 5168.240000000001, "end": 5171.4400000000005, "text": " As long as you have text, you can do that even though you don't know the sentiment of", "tokens": [1018, 938, 382, 291, 362, 2487, 11, 291, 393, 360, 300, 754, 1673, 291, 500, 380, 458, 264, 16149, 295], "temperature": 0.0, "avg_logprob": -0.19107426634622277, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.119879693258554e-05}, {"id": 977, "seek": 515772, "start": 5171.4400000000005, "end": 5172.4400000000005, "text": " the text.", "tokens": [264, 2487, 13], "temperature": 0.0, "avg_logprob": -0.19107426634622277, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.119879693258554e-05}, {"id": 978, "seek": 515772, "start": 5172.4400000000005, "end": 5175.4400000000005, "text": " Jeremy will grab it.", "tokens": [17809, 486, 4444, 309, 13], "temperature": 0.0, "avg_logprob": -0.19107426634622277, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.119879693258554e-05}, {"id": 979, "seek": 515772, "start": 5175.4400000000005, "end": 5176.4400000000005, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.19107426634622277, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.119879693258554e-05}, {"id": 980, "seek": 515772, "start": 5176.4400000000005, "end": 5179.2, "text": " Good answer.", "tokens": [2205, 1867, 13], "temperature": 0.0, "avg_logprob": -0.19107426634622277, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.119879693258554e-05}, {"id": 981, "seek": 515772, "start": 5179.2, "end": 5184.88, "text": " Another question is, what was the secret for dealing with languages where you ended up", "tokens": [3996, 1168, 307, 11, 437, 390, 264, 4054, 337, 6260, 365, 8650, 689, 291, 4590, 493], "temperature": 0.0, "avg_logprob": -0.19107426634622277, "compression_ratio": 1.617283950617284, "no_speech_prob": 3.119879693258554e-05}, {"id": 982, "seek": 518488, "start": 5184.88, "end": 5194.04, "text": " with very long words like Turkish, where it had the different morphemes that might have", "tokens": [365, 588, 938, 2283, 411, 18565, 11, 689, 309, 632, 264, 819, 25778, 443, 279, 300, 1062, 362], "temperature": 0.0, "avg_logprob": -0.3311856438131893, "compression_ratio": 1.5126903553299493, "no_speech_prob": 6.811157800257206e-05}, {"id": 983, "seek": 518488, "start": 5194.04, "end": 5201.56, "text": " meaning and show up across words, such as here, quark or quarku relating to fear, but", "tokens": [3620, 293, 855, 493, 2108, 2283, 11, 1270, 382, 510, 11, 421, 809, 420, 421, 809, 84, 23968, 281, 4240, 11, 457], "temperature": 0.0, "avg_logprob": -0.3311856438131893, "compression_ratio": 1.5126903553299493, "no_speech_prob": 6.811157800257206e-05}, {"id": 984, "seek": 518488, "start": 5201.56, "end": 5203.08, "text": " showing up in a lot of words.", "tokens": [4099, 493, 294, 257, 688, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.3311856438131893, "compression_ratio": 1.5126903553299493, "no_speech_prob": 6.811157800257206e-05}, {"id": 985, "seek": 518488, "start": 5203.08, "end": 5205.08, "text": " Jeremy, can you pass the...", "tokens": [17809, 11, 393, 291, 1320, 264, 485], "temperature": 0.0, "avg_logprob": -0.3311856438131893, "compression_ratio": 1.5126903553299493, "no_speech_prob": 6.811157800257206e-05}, {"id": 986, "seek": 518488, "start": 5205.08, "end": 5212.08, "text": " By coming up with subwords, subword units, learning them.", "tokens": [3146, 1348, 493, 365, 1422, 13832, 11, 1422, 7462, 6815, 11, 2539, 552, 13], "temperature": 0.0, "avg_logprob": -0.3311856438131893, "compression_ratio": 1.5126903553299493, "no_speech_prob": 6.811157800257206e-05}, {"id": 987, "seek": 518488, "start": 5212.08, "end": 5213.08, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.3311856438131893, "compression_ratio": 1.5126903553299493, "no_speech_prob": 6.811157800257206e-05}, {"id": 988, "seek": 521308, "start": 5213.08, "end": 5217.76, "text": " Using subword units and so in this case, Jeremy used sentence piece.", "tokens": [11142, 1422, 7462, 6815, 293, 370, 294, 341, 1389, 11, 17809, 1143, 8174, 2522, 13], "temperature": 0.0, "avg_logprob": -0.3008922024777061, "compression_ratio": 1.5989304812834224, "no_speech_prob": 2.1442032448248938e-05}, {"id": 989, "seek": 521308, "start": 5217.76, "end": 5225.32, "text": " I guess one more question to review these.", "tokens": [286, 2041, 472, 544, 1168, 281, 3131, 613, 13], "temperature": 0.0, "avg_logprob": -0.3008922024777061, "compression_ratio": 1.5989304812834224, "no_speech_prob": 2.1442032448248938e-05}, {"id": 990, "seek": 521308, "start": 5225.32, "end": 5231.72, "text": " So we have, we ensemble a backwards model and a forwards model for our classifier.", "tokens": [407, 321, 362, 11, 321, 19492, 257, 12204, 2316, 293, 257, 30126, 2316, 337, 527, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.3008922024777061, "compression_ratio": 1.5989304812834224, "no_speech_prob": 2.1442032448248938e-05}, {"id": 991, "seek": 521308, "start": 5231.72, "end": 5236.18, "text": " Would you be able to do that for your language model?", "tokens": [6068, 291, 312, 1075, 281, 360, 300, 337, 428, 2856, 2316, 30], "temperature": 0.0, "avg_logprob": -0.3008922024777061, "compression_ratio": 1.5989304812834224, "no_speech_prob": 2.1442032448248938e-05}, {"id": 992, "seek": 521308, "start": 5236.18, "end": 5240.24, "text": " Use an ensemble of a backwards and forwards model?", "tokens": [8278, 364, 19492, 295, 257, 12204, 293, 30126, 2316, 30], "temperature": 0.0, "avg_logprob": -0.3008922024777061, "compression_ratio": 1.5989304812834224, "no_speech_prob": 2.1442032448248938e-05}, {"id": 993, "seek": 524024, "start": 5240.24, "end": 5244.44, "text": " You guys want to vote?", "tokens": [509, 1074, 528, 281, 4740, 30], "temperature": 0.0, "avg_logprob": -0.27940723460207706, "compression_ratio": 1.5120772946859904, "no_speech_prob": 1.862899080151692e-05}, {"id": 994, "seek": 524024, "start": 5244.44, "end": 5248.2, "text": " Anyone who thinks yes, raise your hand.", "tokens": [14643, 567, 7309, 2086, 11, 5300, 428, 1011, 13], "temperature": 0.0, "avg_logprob": -0.27940723460207706, "compression_ratio": 1.5120772946859904, "no_speech_prob": 1.862899080151692e-05}, {"id": 995, "seek": 524024, "start": 5248.2, "end": 5249.2, "text": " Anyone who thinks no?", "tokens": [14643, 567, 7309, 572, 30], "temperature": 0.0, "avg_logprob": -0.27940723460207706, "compression_ratio": 1.5120772946859904, "no_speech_prob": 1.862899080151692e-05}, {"id": 996, "seek": 524024, "start": 5249.2, "end": 5250.2, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.27940723460207706, "compression_ratio": 1.5120772946859904, "no_speech_prob": 1.862899080151692e-05}, {"id": 997, "seek": 524024, "start": 5250.2, "end": 5254.599999999999, "text": " So it was mostly yeses.", "tokens": [407, 309, 390, 5240, 2086, 279, 13], "temperature": 0.0, "avg_logprob": -0.27940723460207706, "compression_ratio": 1.5120772946859904, "no_speech_prob": 1.862899080151692e-05}, {"id": 998, "seek": 524024, "start": 5254.599999999999, "end": 5260.2, "text": " It's actually a no on this and that's because with the language model, because you're predicting", "tokens": [467, 311, 767, 257, 572, 322, 341, 293, 300, 311, 570, 365, 264, 2856, 2316, 11, 570, 291, 434, 32884], "temperature": 0.0, "avg_logprob": -0.27940723460207706, "compression_ratio": 1.5120772946859904, "no_speech_prob": 1.862899080151692e-05}, {"id": 999, "seek": 524024, "start": 5260.2, "end": 5267.4, "text": " what's going next, if you had both forward and backwards, that would kind of be cheating.", "tokens": [437, 311, 516, 958, 11, 498, 291, 632, 1293, 2128, 293, 12204, 11, 300, 576, 733, 295, 312, 18309, 13], "temperature": 0.0, "avg_logprob": -0.27940723460207706, "compression_ratio": 1.5120772946859904, "no_speech_prob": 1.862899080151692e-05}, {"id": 1000, "seek": 524024, "start": 5267.4, "end": 5268.4, "text": " Jeremy?", "tokens": [17809, 30], "temperature": 0.0, "avg_logprob": -0.27940723460207706, "compression_ratio": 1.5120772946859904, "no_speech_prob": 1.862899080151692e-05}, {"id": 1001, "seek": 524024, "start": 5268.4, "end": 5269.4, "text": " Oh.", "tokens": [876, 13], "temperature": 0.0, "avg_logprob": -0.27940723460207706, "compression_ratio": 1.5120772946859904, "no_speech_prob": 1.862899080151692e-05}, {"id": 1002, "seek": 526940, "start": 5269.4, "end": 5272.679999999999, "text": " Maybe we should call it a maybe.", "tokens": [2704, 321, 820, 818, 309, 257, 1310, 13], "temperature": 0.0, "avg_logprob": -0.22451101564893536, "compression_ratio": 1.6345381526104417, "no_speech_prob": 1.2410759154590778e-05}, {"id": 1003, "seek": 526940, "start": 5272.679999999999, "end": 5273.679999999999, "text": " Maybe.", "tokens": [2704, 13], "temperature": 0.0, "avg_logprob": -0.22451101564893536, "compression_ratio": 1.6345381526104417, "no_speech_prob": 1.2410759154590778e-05}, {"id": 1004, "seek": 526940, "start": 5273.679999999999, "end": 5278.04, "text": " Because there's been some people who have used tricks using masking to do something", "tokens": [1436, 456, 311, 668, 512, 561, 567, 362, 1143, 11733, 1228, 31226, 281, 360, 746], "temperature": 0.0, "avg_logprob": -0.22451101564893536, "compression_ratio": 1.6345381526104417, "no_speech_prob": 1.2410759154590778e-05}, {"id": 1005, "seek": 526940, "start": 5278.04, "end": 5283.799999999999, "text": " like that where they kind of predict the next word using everything except the ones that", "tokens": [411, 300, 689, 436, 733, 295, 6069, 264, 958, 1349, 1228, 1203, 3993, 264, 2306, 300], "temperature": 0.0, "avg_logprob": -0.22451101564893536, "compression_ratio": 1.6345381526104417, "no_speech_prob": 1.2410759154590778e-05}, {"id": 1006, "seek": 526940, "start": 5283.799999999999, "end": 5284.799999999999, "text": " would be cheating.", "tokens": [576, 312, 18309, 13], "temperature": 0.0, "avg_logprob": -0.22451101564893536, "compression_ratio": 1.6345381526104417, "no_speech_prob": 1.2410759154590778e-05}, {"id": 1007, "seek": 526940, "start": 5284.799999999999, "end": 5290.24, "text": " As I said, I mentioned this one other thing we didn't mention here and I don't know if", "tokens": [1018, 286, 848, 11, 286, 2835, 341, 472, 661, 551, 321, 994, 380, 2152, 510, 293, 286, 500, 380, 458, 498], "temperature": 0.0, "avg_logprob": -0.22451101564893536, "compression_ratio": 1.6345381526104417, "no_speech_prob": 1.2410759154590778e-05}, {"id": 1008, "seek": 526940, "start": 5290.24, "end": 5294.5199999999995, "text": " it's actually in the literature yet, but as well as ensembling forward and backward, you", "tokens": [309, 311, 767, 294, 264, 10394, 1939, 11, 457, 382, 731, 382, 12567, 2504, 1688, 2128, 293, 23897, 11, 291], "temperature": 0.0, "avg_logprob": -0.22451101564893536, "compression_ratio": 1.6345381526104417, "no_speech_prob": 1.2410759154590778e-05}, {"id": 1009, "seek": 529452, "start": 5294.52, "end": 5300.8, "text": " could also do sentence piece tokenization for English and ensemble forward and backward", "tokens": [727, 611, 360, 8174, 2522, 14862, 2144, 337, 3669, 293, 19492, 2128, 293, 23897], "temperature": 0.0, "avg_logprob": -0.14284565489170914, "compression_ratio": 1.8759398496240602, "no_speech_prob": 7.888470463512931e-06}, {"id": 1010, "seek": 529452, "start": 5300.8, "end": 5306.4400000000005, "text": " spacey tokenized and forward and backward sentence piece tokenized and you would get", "tokens": [1901, 88, 14862, 1602, 293, 2128, 293, 23897, 8174, 2522, 14862, 1602, 293, 291, 576, 483], "temperature": 0.0, "avg_logprob": -0.14284565489170914, "compression_ratio": 1.8759398496240602, "no_speech_prob": 7.888470463512931e-06}, {"id": 1011, "seek": 529452, "start": 5306.4400000000005, "end": 5308.040000000001, "text": " an even better result.", "tokens": [364, 754, 1101, 1874, 13], "temperature": 0.0, "avg_logprob": -0.14284565489170914, "compression_ratio": 1.8759398496240602, "no_speech_prob": 7.888470463512931e-06}, {"id": 1012, "seek": 529452, "start": 5308.040000000001, "end": 5310.88, "text": " So that would probably give you something better than the current state of the art if", "tokens": [407, 300, 576, 1391, 976, 291, 746, 1101, 813, 264, 2190, 1785, 295, 264, 1523, 498], "temperature": 0.0, "avg_logprob": -0.14284565489170914, "compression_ratio": 1.8759398496240602, "no_speech_prob": 7.888470463512931e-06}, {"id": 1013, "seek": 529452, "start": 5310.88, "end": 5312.96, "text": " you tried that.", "tokens": [291, 3031, 300, 13], "temperature": 0.0, "avg_logprob": -0.14284565489170914, "compression_ratio": 1.8759398496240602, "no_speech_prob": 7.888470463512931e-06}, {"id": 1014, "seek": 529452, "start": 5312.96, "end": 5313.96, "text": " I like that idea.", "tokens": [286, 411, 300, 1558, 13], "temperature": 0.0, "avg_logprob": -0.14284565489170914, "compression_ratio": 1.8759398496240602, "no_speech_prob": 7.888470463512931e-06}, {"id": 1015, "seek": 529452, "start": 5313.96, "end": 5318.72, "text": " Yeah, so what I wanted to highlight, that's an important point about using masking, that", "tokens": [865, 11, 370, 437, 286, 1415, 281, 5078, 11, 300, 311, 364, 1021, 935, 466, 1228, 31226, 11, 300], "temperature": 0.0, "avg_logprob": -0.14284565489170914, "compression_ratio": 1.8759398496240602, "no_speech_prob": 7.888470463512931e-06}, {"id": 1016, "seek": 529452, "start": 5318.72, "end": 5322.400000000001, "text": " if you were going to use a forward and backward model, you need to be very careful that you're", "tokens": [498, 291, 645, 516, 281, 764, 257, 2128, 293, 23897, 2316, 11, 291, 643, 281, 312, 588, 5026, 300, 291, 434], "temperature": 0.0, "avg_logprob": -0.14284565489170914, "compression_ratio": 1.8759398496240602, "no_speech_prob": 7.888470463512931e-06}, {"id": 1017, "seek": 532240, "start": 5322.4, "end": 5330.24, "text": " not peeking ahead with the information since you want to make sure you're actually predicting", "tokens": [406, 19604, 278, 2286, 365, 264, 1589, 1670, 291, 528, 281, 652, 988, 291, 434, 767, 32884], "temperature": 0.0, "avg_logprob": -0.26115279901223104, "compression_ratio": 1.4938271604938271, "no_speech_prob": 5.173686076886952e-06}, {"id": 1018, "seek": 532240, "start": 5330.24, "end": 5334.0, "text": " the thing you're predicting and not using other information you have.", "tokens": [264, 551, 291, 434, 32884, 293, 406, 1228, 661, 1589, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.26115279901223104, "compression_ratio": 1.4938271604938271, "no_speech_prob": 5.173686076886952e-06}, {"id": 1019, "seek": 532240, "start": 5334.0, "end": 5335.0, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.26115279901223104, "compression_ratio": 1.4938271604938271, "no_speech_prob": 5.173686076886952e-06}, {"id": 1020, "seek": 532240, "start": 5335.0, "end": 5340.839999999999, "text": " Are there any other questions on what Jeremy covered with ULMfit?", "tokens": [2014, 456, 604, 661, 1651, 322, 437, 17809, 5343, 365, 624, 43, 44, 6845, 30], "temperature": 0.0, "avg_logprob": -0.26115279901223104, "compression_ratio": 1.4938271604938271, "no_speech_prob": 5.173686076886952e-06}, {"id": 1021, "seek": 532240, "start": 5340.839999999999, "end": 5344.5599999999995, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.26115279901223104, "compression_ratio": 1.4938271604938271, "no_speech_prob": 5.173686076886952e-06}, {"id": 1022, "seek": 534456, "start": 5344.56, "end": 5352.56, "text": " So we'll move into the next notebook we're going to cover is notebook six, RNN English", "tokens": [407, 321, 603, 1286, 666, 264, 958, 21060, 321, 434, 516, 281, 2060, 307, 21060, 2309, 11, 45702, 45, 3669], "temperature": 0.0, "avg_logprob": -0.14702834129333497, "compression_ratio": 1.566326530612245, "no_speech_prob": 4.610461837728508e-05}, {"id": 1023, "seek": 534456, "start": 5352.56, "end": 5353.56, "text": " numbers.", "tokens": [3547, 13], "temperature": 0.0, "avg_logprob": -0.14702834129333497, "compression_ratio": 1.566326530612245, "no_speech_prob": 4.610461837728508e-05}, {"id": 1024, "seek": 534456, "start": 5353.56, "end": 5360.76, "text": " So just to say with ULMfit, we were using RNNs, but that was kind of hidden from you.", "tokens": [407, 445, 281, 584, 365, 624, 43, 44, 6845, 11, 321, 645, 1228, 45702, 45, 82, 11, 457, 300, 390, 733, 295, 7633, 490, 291, 13], "temperature": 0.0, "avg_logprob": -0.14702834129333497, "compression_ratio": 1.566326530612245, "no_speech_prob": 4.610461837728508e-05}, {"id": 1025, "seek": 534456, "start": 5360.76, "end": 5363.4400000000005, "text": " So that was part of the AWD LSTM.", "tokens": [407, 300, 390, 644, 295, 264, 25815, 35, 441, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.14702834129333497, "compression_ratio": 1.566326530612245, "no_speech_prob": 4.610461837728508e-05}, {"id": 1026, "seek": 534456, "start": 5363.4400000000005, "end": 5367.280000000001, "text": " Now we're going to really dig into kind of what was going on with the RNNs, what is an", "tokens": [823, 321, 434, 516, 281, 534, 2528, 666, 733, 295, 437, 390, 516, 322, 365, 264, 45702, 45, 82, 11, 437, 307, 364], "temperature": 0.0, "avg_logprob": -0.14702834129333497, "compression_ratio": 1.566326530612245, "no_speech_prob": 4.610461837728508e-05}, {"id": 1027, "seek": 534456, "start": 5367.280000000001, "end": 5370.120000000001, "text": " RNN.", "tokens": [45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.14702834129333497, "compression_ratio": 1.566326530612245, "no_speech_prob": 4.610461837728508e-05}, {"id": 1028, "seek": 537012, "start": 5370.12, "end": 5380.8, "text": " And to do this, we want to use a simpler data set than what we've been doing.", "tokens": [400, 281, 360, 341, 11, 321, 528, 281, 764, 257, 18587, 1412, 992, 813, 437, 321, 600, 668, 884, 13], "temperature": 0.0, "avg_logprob": -0.1203727852808286, "compression_ratio": 1.4166666666666667, "no_speech_prob": 9.368074643134605e-06}, {"id": 1029, "seek": 537012, "start": 5380.8, "end": 5387.08, "text": " And so we're going to use a list of the English word version of numbers as shown here.", "tokens": [400, 370, 321, 434, 516, 281, 764, 257, 1329, 295, 264, 3669, 1349, 3037, 295, 3547, 382, 4898, 510, 13], "temperature": 0.0, "avg_logprob": -0.1203727852808286, "compression_ratio": 1.4166666666666667, "no_speech_prob": 9.368074643134605e-06}, {"id": 1030, "seek": 537012, "start": 5387.08, "end": 5388.68, "text": " This is counting.", "tokens": [639, 307, 13251, 13], "temperature": 0.0, "avg_logprob": -0.1203727852808286, "compression_ratio": 1.4166666666666667, "no_speech_prob": 9.368074643134605e-06}, {"id": 1031, "seek": 537012, "start": 5388.68, "end": 5395.24, "text": " So 8001, 8002, 8003, 8004, 8005, 8006.", "tokens": [407, 13083, 16, 11, 13083, 17, 11, 13083, 18, 11, 13083, 19, 11, 13083, 20, 11, 13083, 21, 13], "temperature": 0.0, "avg_logprob": -0.1203727852808286, "compression_ratio": 1.4166666666666667, "no_speech_prob": 9.368074643134605e-06}, {"id": 1032, "seek": 539524, "start": 5395.24, "end": 5400.719999999999, "text": " And this is a synthetic data set that Jeremy created, and Jeremy is a big fan of synthetic", "tokens": [400, 341, 307, 257, 23420, 1412, 992, 300, 17809, 2942, 11, 293, 17809, 307, 257, 955, 3429, 295, 23420], "temperature": 0.0, "avg_logprob": -0.1547730119922493, "compression_ratio": 1.758974358974359, "no_speech_prob": 6.240481070562964e-06}, {"id": 1033, "seek": 539524, "start": 5400.719999999999, "end": 5406.76, "text": " data sets as a technique when you're testing out different ideas or kind of wanting to", "tokens": [1412, 6352, 382, 257, 6532, 562, 291, 434, 4997, 484, 819, 3487, 420, 733, 295, 7935, 281], "temperature": 0.0, "avg_logprob": -0.1547730119922493, "compression_ratio": 1.758974358974359, "no_speech_prob": 6.240481070562964e-06}, {"id": 1034, "seek": 539524, "start": 5406.76, "end": 5408.5199999999995, "text": " explore something.", "tokens": [6839, 746, 13], "temperature": 0.0, "avg_logprob": -0.1547730119922493, "compression_ratio": 1.758974358974359, "no_speech_prob": 6.240481070562964e-06}, {"id": 1035, "seek": 539524, "start": 5408.5199999999995, "end": 5413.24, "text": " It can be nice to have a simpler data set that you understand well because then you", "tokens": [467, 393, 312, 1481, 281, 362, 257, 18587, 1412, 992, 300, 291, 1223, 731, 570, 550, 291], "temperature": 0.0, "avg_logprob": -0.1547730119922493, "compression_ratio": 1.758974358974359, "no_speech_prob": 6.240481070562964e-06}, {"id": 1036, "seek": 539524, "start": 5413.24, "end": 5416.92, "text": " can kind of understand the behaviors you're testing something.", "tokens": [393, 733, 295, 1223, 264, 15501, 291, 434, 4997, 746, 13], "temperature": 0.0, "avg_logprob": -0.1547730119922493, "compression_ratio": 1.758974358974359, "no_speech_prob": 6.240481070562964e-06}, {"id": 1037, "seek": 541692, "start": 5416.92, "end": 5426.36, "text": " In the deep learning courses, Jeremy created ImageNet and ImageWolf as simpler data sets", "tokens": [682, 264, 2452, 2539, 7712, 11, 17809, 2942, 29903, 31890, 293, 29903, 54, 7491, 382, 18587, 1412, 6352], "temperature": 0.0, "avg_logprob": -0.25045399232344195, "compression_ratio": 1.5263157894736843, "no_speech_prob": 3.726542900039931e-06}, {"id": 1038, "seek": 541692, "start": 5426.36, "end": 5427.8, "text": " to test ideas on.", "tokens": [281, 1500, 3487, 322, 13], "temperature": 0.0, "avg_logprob": -0.25045399232344195, "compression_ratio": 1.5263157894736843, "no_speech_prob": 3.726542900039931e-06}, {"id": 1039, "seek": 541692, "start": 5427.8, "end": 5432.0, "text": " Did you want to say more about that technique, Jeremy?", "tokens": [2589, 291, 528, 281, 584, 544, 466, 300, 6532, 11, 17809, 30], "temperature": 0.0, "avg_logprob": -0.25045399232344195, "compression_ratio": 1.5263157894736843, "no_speech_prob": 3.726542900039931e-06}, {"id": 1040, "seek": 541692, "start": 5432.0, "end": 5434.4800000000005, "text": " Sure.", "tokens": [4894, 13], "temperature": 0.0, "avg_logprob": -0.25045399232344195, "compression_ratio": 1.5263157894736843, "no_speech_prob": 3.726542900039931e-06}, {"id": 1041, "seek": 541692, "start": 5434.4800000000005, "end": 5440.08, "text": " I guess these are two versions of a similar technique, so the synthetic data set technique.", "tokens": [286, 2041, 613, 366, 732, 9606, 295, 257, 2531, 6532, 11, 370, 264, 23420, 1412, 992, 6532, 13], "temperature": 0.0, "avg_logprob": -0.25045399232344195, "compression_ratio": 1.5263157894736843, "no_speech_prob": 3.726542900039931e-06}, {"id": 1042, "seek": 541692, "start": 5440.08, "end": 5445.4800000000005, "text": " When we were developing the Fast.ai library, my experience is the first 12 times I write", "tokens": [1133, 321, 645, 6416, 264, 15968, 13, 1301, 6405, 11, 452, 1752, 307, 264, 700, 2272, 1413, 286, 2464], "temperature": 0.0, "avg_logprob": -0.25045399232344195, "compression_ratio": 1.5263157894736843, "no_speech_prob": 3.726542900039931e-06}, {"id": 1043, "seek": 544548, "start": 5445.48, "end": 5447.36, "text": " something, it's always wrong.", "tokens": [746, 11, 309, 311, 1009, 2085, 13], "temperature": 0.0, "avg_logprob": -0.14496353583607247, "compression_ratio": 1.6853146853146854, "no_speech_prob": 0.00015355182404164225}, {"id": 1044, "seek": 544548, "start": 5447.36, "end": 5452.04, "text": " It's very hard to know something's wrong, particularly because with machine learning", "tokens": [467, 311, 588, 1152, 281, 458, 746, 311, 2085, 11, 4098, 570, 365, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.14496353583607247, "compression_ratio": 1.6853146853146854, "no_speech_prob": 0.00015355182404164225}, {"id": 1045, "seek": 544548, "start": 5452.04, "end": 5454.04, "text": " it can be wrong in subtle ways.", "tokens": [309, 393, 312, 2085, 294, 13743, 2098, 13], "temperature": 0.0, "avg_logprob": -0.14496353583607247, "compression_ratio": 1.6853146853146854, "no_speech_prob": 0.00015355182404164225}, {"id": 1046, "seek": 544548, "start": 5454.04, "end": 5459.36, "text": " So for something which I clearly know what the next answer ought to be, it was just easier", "tokens": [407, 337, 746, 597, 286, 4448, 458, 437, 264, 958, 1867, 13416, 281, 312, 11, 309, 390, 445, 3571], "temperature": 0.0, "avg_logprob": -0.14496353583607247, "compression_ratio": 1.6853146853146854, "no_speech_prob": 0.00015355182404164225}, {"id": 1047, "seek": 544548, "start": 5459.36, "end": 5462.28, "text": " for me to see how it was wrong.", "tokens": [337, 385, 281, 536, 577, 309, 390, 2085, 13], "temperature": 0.0, "avg_logprob": -0.14496353583607247, "compression_ratio": 1.6853146853146854, "no_speech_prob": 0.00015355182404164225}, {"id": 1048, "seek": 544548, "start": 5462.28, "end": 5467.04, "text": " Also it's just much easier to develop an algorithm using something that can train in 5-10 seconds", "tokens": [2743, 309, 311, 445, 709, 3571, 281, 1499, 364, 9284, 1228, 746, 300, 393, 3847, 294, 1025, 12, 3279, 3949], "temperature": 0.0, "avg_logprob": -0.14496353583607247, "compression_ratio": 1.6853146853146854, "no_speech_prob": 0.00015355182404164225}, {"id": 1049, "seek": 544548, "start": 5467.04, "end": 5470.839999999999, "text": " rather than 5-10 hours.", "tokens": [2831, 813, 1025, 12, 3279, 2496, 13], "temperature": 0.0, "avg_logprob": -0.14496353583607247, "compression_ratio": 1.6853146853146854, "no_speech_prob": 0.00015355182404164225}, {"id": 1050, "seek": 544548, "start": 5470.839999999999, "end": 5474.959999999999, "text": " ImageNet and ImageWolf is a slightly different variant, which is it's not a synthetic data", "tokens": [29903, 31890, 293, 29903, 54, 7491, 307, 257, 4748, 819, 17501, 11, 597, 307, 309, 311, 406, 257, 23420, 1412], "temperature": 0.0, "avg_logprob": -0.14496353583607247, "compression_ratio": 1.6853146853146854, "no_speech_prob": 0.00015355182404164225}, {"id": 1051, "seek": 547496, "start": 5474.96, "end": 5477.88, "text": " set, but it's a much smaller data set.", "tokens": [992, 11, 457, 309, 311, 257, 709, 4356, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.15925661155155726, "compression_ratio": 1.6509803921568627, "no_speech_prob": 1.983306538022589e-05}, {"id": 1052, "seek": 547496, "start": 5477.88, "end": 5481.9, "text": " In computer vision, people who had been struggling for a long time, they either trained things", "tokens": [682, 3820, 5201, 11, 561, 567, 632, 668, 9314, 337, 257, 938, 565, 11, 436, 2139, 8895, 721], "temperature": 0.0, "avg_logprob": -0.15925661155155726, "compression_ratio": 1.6509803921568627, "no_speech_prob": 1.983306538022589e-05}, {"id": 1053, "seek": 547496, "start": 5481.9, "end": 5488.6, "text": " on the ImageNet data set, which used to take days to train, or they trained on something", "tokens": [322, 264, 29903, 31890, 1412, 992, 11, 597, 1143, 281, 747, 1708, 281, 3847, 11, 420, 436, 8895, 322, 746], "temperature": 0.0, "avg_logprob": -0.15925661155155726, "compression_ratio": 1.6509803921568627, "no_speech_prob": 1.983306538022589e-05}, {"id": 1054, "seek": 547496, "start": 5488.6, "end": 5493.28, "text": " called CyPhar 10, which was so small it turned out to be useless.", "tokens": [1219, 10295, 47, 5854, 1266, 11, 597, 390, 370, 1359, 309, 3574, 484, 281, 312, 14115, 13], "temperature": 0.0, "avg_logprob": -0.15925661155155726, "compression_ratio": 1.6509803921568627, "no_speech_prob": 1.983306538022589e-05}, {"id": 1055, "seek": 547496, "start": 5493.28, "end": 5499.52, "text": " They were 32x32 pixels, and it turns out that things that work well on 32x32 pixels actually", "tokens": [814, 645, 8858, 87, 11440, 18668, 11, 293, 309, 4523, 484, 300, 721, 300, 589, 731, 322, 8858, 87, 11440, 18668, 767], "temperature": 0.0, "avg_logprob": -0.15925661155155726, "compression_ratio": 1.6509803921568627, "no_speech_prob": 1.983306538022589e-05}, {"id": 1056, "seek": 547496, "start": 5499.52, "end": 5503.24, "text": " don't work well on normal-sized images.", "tokens": [500, 380, 589, 731, 322, 2710, 12, 20614, 5267, 13], "temperature": 0.0, "avg_logprob": -0.15925661155155726, "compression_ratio": 1.6509803921568627, "no_speech_prob": 1.983306538022589e-05}, {"id": 1057, "seek": 550324, "start": 5503.24, "end": 5509.04, "text": " So I created something with full-sized images, but less of them, and I tried to create one", "tokens": [407, 286, 2942, 746, 365, 1577, 12, 20614, 5267, 11, 457, 1570, 295, 552, 11, 293, 286, 3031, 281, 1884, 472], "temperature": 0.0, "avg_logprob": -0.15380191802978516, "compression_ratio": 1.7658730158730158, "no_speech_prob": 1.1842971616715658e-05}, {"id": 1058, "seek": 550324, "start": 5509.04, "end": 5513.4, "text": " version that would be easy to classify and one version that would be hard to classify.", "tokens": [3037, 300, 576, 312, 1858, 281, 33872, 293, 472, 3037, 300, 576, 312, 1152, 281, 33872, 13], "temperature": 0.0, "avg_logprob": -0.15380191802978516, "compression_ratio": 1.7658730158730158, "no_speech_prob": 1.1842971616715658e-05}, {"id": 1059, "seek": 550324, "start": 5513.4, "end": 5520.88, "text": " So I guess in general it's like trying to come up with different sampling versions of", "tokens": [407, 286, 2041, 294, 2674, 309, 311, 411, 1382, 281, 808, 493, 365, 819, 21179, 9606, 295], "temperature": 0.0, "avg_logprob": -0.15380191802978516, "compression_ratio": 1.7658730158730158, "no_speech_prob": 1.1842971616715658e-05}, {"id": 1060, "seek": 550324, "start": 5520.88, "end": 5524.639999999999, "text": " the problem that you're trying to solve is one of the really important things as a machine", "tokens": [264, 1154, 300, 291, 434, 1382, 281, 5039, 307, 472, 295, 264, 534, 1021, 721, 382, 257, 3479], "temperature": 0.0, "avg_logprob": -0.15380191802978516, "compression_ratio": 1.7658730158730158, "no_speech_prob": 1.1842971616715658e-05}, {"id": 1061, "seek": 550324, "start": 5524.639999999999, "end": 5531.4, "text": " learning practitioner, so that you can quickly iterate and quickly identify your mistakes,", "tokens": [2539, 32125, 11, 370, 300, 291, 393, 2661, 44497, 293, 2661, 5876, 428, 8038, 11], "temperature": 0.0, "avg_logprob": -0.15380191802978516, "compression_ratio": 1.7658730158730158, "no_speech_prob": 1.1842971616715658e-05}, {"id": 1062, "seek": 553140, "start": 5531.4, "end": 5533.4, "text": " even if you don't make as many mistakes as I do.", "tokens": [754, 498, 291, 500, 380, 652, 382, 867, 8038, 382, 286, 360, 13], "temperature": 0.0, "avg_logprob": -0.20897886488172743, "compression_ratio": 1.6475770925110131, "no_speech_prob": 6.108140951255336e-05}, {"id": 1063, "seek": 553140, "start": 5533.4, "end": 5534.4, "text": " Great, thank you.", "tokens": [3769, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.20897886488172743, "compression_ratio": 1.6475770925110131, "no_speech_prob": 6.108140951255336e-05}, {"id": 1064, "seek": 553140, "start": 5534.4, "end": 5540.0, "text": " Yes, that gets at this recurring theme we've seen of exactly quick iteration, and so even", "tokens": [1079, 11, 300, 2170, 412, 341, 32279, 6314, 321, 600, 1612, 295, 2293, 1702, 24784, 11, 293, 370, 754], "temperature": 0.0, "avg_logprob": -0.20897886488172743, "compression_ratio": 1.6475770925110131, "no_speech_prob": 6.108140951255336e-05}, {"id": 1065, "seek": 553140, "start": 5540.0, "end": 5544.08, "text": " that's the same idea of why you try stuff out on a sample of your data set.", "tokens": [300, 311, 264, 912, 1558, 295, 983, 291, 853, 1507, 484, 322, 257, 6889, 295, 428, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.20897886488172743, "compression_ratio": 1.6475770925110131, "no_speech_prob": 6.108140951255336e-05}, {"id": 1066, "seek": 553140, "start": 5544.08, "end": 5550.24, "text": " This is maybe two more sophisticated ways of doing samples of data sets, or getting", "tokens": [639, 307, 1310, 732, 544, 16950, 2098, 295, 884, 10938, 295, 1412, 6352, 11, 420, 1242], "temperature": 0.0, "avg_logprob": -0.20897886488172743, "compression_ratio": 1.6475770925110131, "no_speech_prob": 6.108140951255336e-05}, {"id": 1067, "seek": 553140, "start": 5550.24, "end": 5556.0, "text": " at the same idea of being able to try things out quickly.", "tokens": [412, 264, 912, 1558, 295, 885, 1075, 281, 853, 721, 484, 2661, 13], "temperature": 0.0, "avg_logprob": -0.20897886488172743, "compression_ratio": 1.6475770925110131, "no_speech_prob": 6.108140951255336e-05}, {"id": 1068, "seek": 555600, "start": 5556.0, "end": 5562.88, "text": " One thing I wanted to cover before we go on is just that deep learning neural networks", "tokens": [1485, 551, 286, 1415, 281, 2060, 949, 321, 352, 322, 307, 445, 300, 2452, 2539, 18161, 9590], "temperature": 0.0, "avg_logprob": -0.1273199056650137, "compression_ratio": 1.6226415094339623, "no_speech_prob": 3.943598494515754e-05}, {"id": 1069, "seek": 555600, "start": 5562.88, "end": 5566.08, "text": " are really just made up of two different types of numbers.", "tokens": [366, 534, 445, 1027, 493, 295, 732, 819, 3467, 295, 3547, 13], "temperature": 0.0, "avg_logprob": -0.1273199056650137, "compression_ratio": 1.6226415094339623, "no_speech_prob": 3.943598494515754e-05}, {"id": 1070, "seek": 555600, "start": 5566.08, "end": 5572.76, "text": " Those are parameters, which are the numbers that you're learning through back propagation", "tokens": [3950, 366, 9834, 11, 597, 366, 264, 3547, 300, 291, 434, 2539, 807, 646, 38377], "temperature": 0.0, "avg_logprob": -0.1273199056650137, "compression_ratio": 1.6226415094339623, "no_speech_prob": 3.943598494515754e-05}, {"id": 1071, "seek": 555600, "start": 5572.76, "end": 5578.52, "text": " and these variants on stochastic gradient descent, and then activations are the numbers", "tokens": [293, 613, 21669, 322, 342, 8997, 2750, 16235, 23475, 11, 293, 550, 2430, 763, 366, 264, 3547], "temperature": 0.0, "avg_logprob": -0.1273199056650137, "compression_ratio": 1.6226415094339623, "no_speech_prob": 3.943598494515754e-05}, {"id": 1072, "seek": 555600, "start": 5578.52, "end": 5580.36, "text": " that are calculated.", "tokens": [300, 366, 15598, 13], "temperature": 0.0, "avg_logprob": -0.1273199056650137, "compression_ratio": 1.6226415094339623, "no_speech_prob": 3.943598494515754e-05}, {"id": 1073, "seek": 558036, "start": 5580.36, "end": 5586.28, "text": " And so you're really just combining these things to create a neural network, and as", "tokens": [400, 370, 291, 434, 534, 445, 21928, 613, 721, 281, 1884, 257, 18161, 3209, 11, 293, 382], "temperature": 0.0, "avg_logprob": -0.16339979278907346, "compression_ratio": 1.5948275862068966, "no_speech_prob": 2.2124651877675205e-05}, {"id": 1074, "seek": 558036, "start": 5586.28, "end": 5590.759999999999, "text": " we go, I'll identify when you learn a new concept, it can be helpful to think, is this", "tokens": [321, 352, 11, 286, 603, 5876, 562, 291, 1466, 257, 777, 3410, 11, 309, 393, 312, 4961, 281, 519, 11, 307, 341], "temperature": 0.0, "avg_logprob": -0.16339979278907346, "compression_ratio": 1.5948275862068966, "no_speech_prob": 2.2124651877675205e-05}, {"id": 1075, "seek": 558036, "start": 5590.759999999999, "end": 5593.599999999999, "text": " a parameter or an activation?", "tokens": [257, 13075, 420, 364, 24433, 30], "temperature": 0.0, "avg_logprob": -0.16339979278907346, "compression_ratio": 1.5948275862068966, "no_speech_prob": 2.2124651877675205e-05}, {"id": 1076, "seek": 558036, "start": 5593.599999999999, "end": 5599.32, "text": " But there's not any other magical component of what makes neural networks.", "tokens": [583, 456, 311, 406, 604, 661, 12066, 6542, 295, 437, 1669, 18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.16339979278907346, "compression_ratio": 1.5948275862068966, "no_speech_prob": 2.2124651877675205e-05}, {"id": 1077, "seek": 558036, "start": 5599.32, "end": 5606.08, "text": " You're just stacking up these linear transformations with non-linearities, and you're learning", "tokens": [509, 434, 445, 41376, 493, 613, 8213, 34852, 365, 2107, 12, 28263, 1088, 11, 293, 291, 434, 2539], "temperature": 0.0, "avg_logprob": -0.16339979278907346, "compression_ratio": 1.5948275862068966, "no_speech_prob": 2.2124651877675205e-05}, {"id": 1078, "seek": 560608, "start": 5606.08, "end": 5617.5599999999995, "text": " one set of parameters and calculating a set of activations as you go through a network.", "tokens": [472, 992, 295, 9834, 293, 28258, 257, 992, 295, 2430, 763, 382, 291, 352, 807, 257, 3209, 13], "temperature": 0.0, "avg_logprob": -0.14462524862850415, "compression_ratio": 1.570048309178744, "no_speech_prob": 1.8631137209013104e-05}, {"id": 1079, "seek": 560608, "start": 5617.5599999999995, "end": 5623.64, "text": " So here we're going to read in this set of English numbers, and again, this is a problem", "tokens": [407, 510, 321, 434, 516, 281, 1401, 294, 341, 992, 295, 3669, 3547, 11, 293, 797, 11, 341, 307, 257, 1154], "temperature": 0.0, "avg_logprob": -0.14462524862850415, "compression_ratio": 1.570048309178744, "no_speech_prob": 1.8631137209013104e-05}, {"id": 1080, "seek": 560608, "start": 5623.64, "end": 5629.8, "text": " that you are not going to come across in the real world, but it's going to be very informative", "tokens": [300, 291, 366, 406, 516, 281, 808, 2108, 294, 264, 957, 1002, 11, 457, 309, 311, 516, 281, 312, 588, 27759], "temperature": 0.0, "avg_logprob": -0.14462524862850415, "compression_ratio": 1.570048309178744, "no_speech_prob": 1.8631137209013104e-05}, {"id": 1081, "seek": 560608, "start": 5629.8, "end": 5634.16, "text": " for us as we explore what an RNN is and how it works.", "tokens": [337, 505, 382, 321, 6839, 437, 364, 45702, 45, 307, 293, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.14462524862850415, "compression_ratio": 1.570048309178744, "no_speech_prob": 1.8631137209013104e-05}, {"id": 1082, "seek": 563416, "start": 5634.16, "end": 5639.88, "text": " And in this lesson, we'll be building up an RNN from the basics.", "tokens": [400, 294, 341, 6898, 11, 321, 603, 312, 2390, 493, 364, 45702, 45, 490, 264, 14688, 13], "temperature": 0.0, "avg_logprob": -0.18537515582460345, "compression_ratio": 1.48068669527897, "no_speech_prob": 3.1690065952716395e-05}, {"id": 1083, "seek": 563416, "start": 5639.88, "end": 5647.16, "text": " BPTT stands for back propagation through time, and that tells us how many steps of history", "tokens": [40533, 28178, 7382, 337, 646, 38377, 807, 565, 11, 293, 300, 5112, 505, 577, 867, 4439, 295, 2503], "temperature": 0.0, "avg_logprob": -0.18537515582460345, "compression_ratio": 1.48068669527897, "no_speech_prob": 3.1690065952716395e-05}, {"id": 1084, "seek": 563416, "start": 5647.16, "end": 5648.92, "text": " we're considering.", "tokens": [321, 434, 8079, 13], "temperature": 0.0, "avg_logprob": -0.18537515582460345, "compression_ratio": 1.48068669527897, "no_speech_prob": 3.1690065952716395e-05}, {"id": 1085, "seek": 563416, "start": 5648.92, "end": 5655.4, "text": " And so here, we're going to start with 70, and so the idea is looking at this, it would", "tokens": [400, 370, 510, 11, 321, 434, 516, 281, 722, 365, 5285, 11, 293, 370, 264, 1558, 307, 1237, 412, 341, 11, 309, 576], "temperature": 0.0, "avg_logprob": -0.18537515582460345, "compression_ratio": 1.48068669527897, "no_speech_prob": 3.1690065952716395e-05}, {"id": 1086, "seek": 563416, "start": 5655.4, "end": 5663.96, "text": " be if you had 70 tokens, 8001, 8002, so that's, I guess, eight tokens right there.", "tokens": [312, 498, 291, 632, 5285, 22667, 11, 13083, 16, 11, 13083, 17, 11, 370, 300, 311, 11, 286, 2041, 11, 3180, 22667, 558, 456, 13], "temperature": 0.0, "avg_logprob": -0.18537515582460345, "compression_ratio": 1.48068669527897, "no_speech_prob": 3.1690065952716395e-05}, {"id": 1087, "seek": 566396, "start": 5663.96, "end": 5668.6, "text": " If you include the commas, we're going to have 70 of those, and then we want our network", "tokens": [759, 291, 4090, 264, 800, 296, 11, 321, 434, 516, 281, 362, 5285, 295, 729, 11, 293, 550, 321, 528, 527, 3209], "temperature": 0.0, "avg_logprob": -0.1513942459882316, "compression_ratio": 1.7004048582995952, "no_speech_prob": 2.5865807401714846e-05}, {"id": 1088, "seek": 566396, "start": 5668.6, "end": 5673.32, "text": " to predict what would come next, what would be the 71st thing.", "tokens": [281, 6069, 437, 576, 808, 958, 11, 437, 576, 312, 264, 30942, 372, 551, 13], "temperature": 0.0, "avg_logprob": -0.1513942459882316, "compression_ratio": 1.7004048582995952, "no_speech_prob": 2.5865807401714846e-05}, {"id": 1089, "seek": 566396, "start": 5673.32, "end": 5681.56, "text": " We'll divide our data set up, so we're going to have 70 tokens in a line of text and 64", "tokens": [492, 603, 9845, 527, 1412, 992, 493, 11, 370, 321, 434, 516, 281, 362, 5285, 22667, 294, 257, 1622, 295, 2487, 293, 12145], "temperature": 0.0, "avg_logprob": -0.1513942459882316, "compression_ratio": 1.7004048582995952, "no_speech_prob": 2.5865807401714846e-05}, {"id": 1090, "seek": 566396, "start": 5681.56, "end": 5683.12, "text": " lines of text per batch.", "tokens": [3876, 295, 2487, 680, 15245, 13], "temperature": 0.0, "avg_logprob": -0.1513942459882316, "compression_ratio": 1.7004048582995952, "no_speech_prob": 2.5865807401714846e-05}, {"id": 1091, "seek": 566396, "start": 5683.12, "end": 5686.32, "text": " Our batch size is 64.", "tokens": [2621, 15245, 2744, 307, 12145, 13], "temperature": 0.0, "avg_logprob": -0.1513942459882316, "compression_ratio": 1.7004048582995952, "no_speech_prob": 2.5865807401714846e-05}, {"id": 1092, "seek": 566396, "start": 5686.32, "end": 5688.16, "text": " And so this will give us about three batches.", "tokens": [400, 370, 341, 486, 976, 505, 466, 1045, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.1513942459882316, "compression_ratio": 1.7004048582995952, "no_speech_prob": 2.5865807401714846e-05}, {"id": 1093, "seek": 566396, "start": 5688.16, "end": 5692.4, "text": " And again, kind of the reason we're doing something on this smaller data set is so that", "tokens": [400, 797, 11, 733, 295, 264, 1778, 321, 434, 884, 746, 322, 341, 4356, 1412, 992, 307, 370, 300], "temperature": 0.0, "avg_logprob": -0.1513942459882316, "compression_ratio": 1.7004048582995952, "no_speech_prob": 2.5865807401714846e-05}, {"id": 1094, "seek": 569240, "start": 5692.4, "end": 5695.96, "text": " it'll be easier to see what's happening.", "tokens": [309, 603, 312, 3571, 281, 536, 437, 311, 2737, 13], "temperature": 0.0, "avg_logprob": -0.1460167057109329, "compression_ratio": 1.6090909090909091, "no_speech_prob": 7.411128081002971e-06}, {"id": 1095, "seek": 569240, "start": 5695.96, "end": 5703.32, "text": " And here, we're going to explicitly name our batches, X1, which corresponds to the labels", "tokens": [400, 510, 11, 321, 434, 516, 281, 20803, 1315, 527, 15245, 279, 11, 1783, 16, 11, 597, 23249, 281, 264, 16949], "temperature": 0.0, "avg_logprob": -0.1460167057109329, "compression_ratio": 1.6090909090909091, "no_speech_prob": 7.411128081002971e-06}, {"id": 1096, "seek": 569240, "start": 5703.32, "end": 5708.44, "text": " in Y1, X2 corresponds to Y2, X3 to Y3.", "tokens": [294, 398, 16, 11, 1783, 17, 23249, 281, 398, 17, 11, 1783, 18, 281, 398, 18, 13], "temperature": 0.0, "avg_logprob": -0.1460167057109329, "compression_ratio": 1.6090909090909091, "no_speech_prob": 7.411128081002971e-06}, {"id": 1097, "seek": 569240, "start": 5708.44, "end": 5709.92, "text": " And that's all our data.", "tokens": [400, 300, 311, 439, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1460167057109329, "compression_ratio": 1.6090909090909091, "no_speech_prob": 7.411128081002971e-06}, {"id": 1098, "seek": 569240, "start": 5709.92, "end": 5713.92, "text": " And typically, you're not taking your batches out and putting them in separate variables,", "tokens": [400, 5850, 11, 291, 434, 406, 1940, 428, 15245, 279, 484, 293, 3372, 552, 294, 4994, 9102, 11], "temperature": 0.0, "avg_logprob": -0.1460167057109329, "compression_ratio": 1.6090909090909091, "no_speech_prob": 7.411128081002971e-06}, {"id": 1099, "seek": 569240, "start": 5713.92, "end": 5719.5199999999995, "text": " but this, we really kind of want to see what happens.", "tokens": [457, 341, 11, 321, 534, 733, 295, 528, 281, 536, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.1460167057109329, "compression_ratio": 1.6090909090909091, "no_speech_prob": 7.411128081002971e-06}, {"id": 1100, "seek": 569240, "start": 5719.5199999999995, "end": 5720.5199999999995, "text": " We can confirm.", "tokens": [492, 393, 9064, 13], "temperature": 0.0, "avg_logprob": -0.1460167057109329, "compression_ratio": 1.6090909090909091, "no_speech_prob": 7.411128081002971e-06}, {"id": 1101, "seek": 572052, "start": 5720.52, "end": 5725.88, "text": " So there's a numL, is a PyTorch method to return the number of elements in a tensor.", "tokens": [407, 456, 311, 257, 1031, 43, 11, 307, 257, 9953, 51, 284, 339, 3170, 281, 2736, 264, 1230, 295, 4959, 294, 257, 40863, 13], "temperature": 0.0, "avg_logprob": -0.22115601026094878, "compression_ratio": 1.4035874439461884, "no_speech_prob": 1.6700954802217893e-05}, {"id": 1102, "seek": 572052, "start": 5725.88, "end": 5729.56, "text": " We can do that and check, OK, we're getting about 13,000 back.", "tokens": [492, 393, 360, 300, 293, 1520, 11, 2264, 11, 321, 434, 1242, 466, 3705, 11, 1360, 646, 13], "temperature": 0.0, "avg_logprob": -0.22115601026094878, "compression_ratio": 1.4035874439461884, "no_speech_prob": 1.6700954802217893e-05}, {"id": 1103, "seek": 572052, "start": 5729.56, "end": 5731.6, "text": " That's what we expected.", "tokens": [663, 311, 437, 321, 5176, 13], "temperature": 0.0, "avg_logprob": -0.22115601026094878, "compression_ratio": 1.4035874439461884, "no_speech_prob": 1.6700954802217893e-05}, {"id": 1104, "seek": 572052, "start": 5731.6, "end": 5736.080000000001, "text": " You can see the size.", "tokens": [509, 393, 536, 264, 2744, 13], "temperature": 0.0, "avg_logprob": -0.22115601026094878, "compression_ratio": 1.4035874439461884, "no_speech_prob": 1.6700954802217893e-05}, {"id": 1105, "seek": 572052, "start": 5736.080000000001, "end": 5739.160000000001, "text": " A single batch is 64 by 70.", "tokens": [316, 2167, 15245, 307, 12145, 538, 5285, 13], "temperature": 0.0, "avg_logprob": -0.22115601026094878, "compression_ratio": 1.4035874439461884, "no_speech_prob": 1.6700954802217893e-05}, {"id": 1106, "seek": 572052, "start": 5739.160000000001, "end": 5742.360000000001, "text": " So we have 64 different lines of text.", "tokens": [407, 321, 362, 12145, 819, 3876, 295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.22115601026094878, "compression_ratio": 1.4035874439461884, "no_speech_prob": 1.6700954802217893e-05}, {"id": 1107, "seek": 572052, "start": 5742.360000000001, "end": 5746.280000000001, "text": " Each line of text has 70 tokens in it.", "tokens": [6947, 1622, 295, 2487, 575, 5285, 22667, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.22115601026094878, "compression_ratio": 1.4035874439461884, "no_speech_prob": 1.6700954802217893e-05}, {"id": 1108, "seek": 572052, "start": 5746.280000000001, "end": 5747.280000000001, "text": " And then Y1.", "tokens": [400, 550, 398, 16, 13], "temperature": 0.0, "avg_logprob": -0.22115601026094878, "compression_ratio": 1.4035874439461884, "no_speech_prob": 1.6700954802217893e-05}, {"id": 1109, "seek": 574728, "start": 5747.28, "end": 5754.48, "text": " Oh, and so here, with Y1, basically, everything has been shifted over one.", "tokens": [876, 11, 293, 370, 510, 11, 365, 398, 16, 11, 1936, 11, 1203, 575, 668, 18892, 670, 472, 13], "temperature": 0.0, "avg_logprob": -0.16163003324258207, "compression_ratio": 1.5336322869955157, "no_speech_prob": 1.5689171050325967e-05}, {"id": 1110, "seek": 574728, "start": 5754.48, "end": 5756.16, "text": " Let me see if I've shown this.", "tokens": [961, 385, 536, 498, 286, 600, 4898, 341, 13], "temperature": 0.0, "avg_logprob": -0.16163003324258207, "compression_ratio": 1.5336322869955157, "no_speech_prob": 1.5689171050325967e-05}, {"id": 1111, "seek": 574728, "start": 5756.16, "end": 5764.84, "text": " Yeah, we'll take a look at this in a moment, kind of in terms of predicting what's next.", "tokens": [865, 11, 321, 603, 747, 257, 574, 412, 341, 294, 257, 1623, 11, 733, 295, 294, 2115, 295, 32884, 437, 311, 958, 13], "temperature": 0.0, "avg_logprob": -0.16163003324258207, "compression_ratio": 1.5336322869955157, "no_speech_prob": 1.5689171050325967e-05}, {"id": 1112, "seek": 574728, "start": 5764.84, "end": 5769.0, "text": " I'm loading the vocab into V so we can take a look.", "tokens": [286, 478, 15114, 264, 2329, 455, 666, 691, 370, 321, 393, 747, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.16163003324258207, "compression_ratio": 1.5336322869955157, "no_speech_prob": 1.5689171050325967e-05}, {"id": 1113, "seek": 574728, "start": 5769.0, "end": 5776.54, "text": " This is into string, checking how we've mapped our tokens as, or how we're mapping our integers", "tokens": [639, 307, 666, 6798, 11, 8568, 577, 321, 600, 33318, 527, 22667, 382, 11, 420, 577, 321, 434, 18350, 527, 41674], "temperature": 0.0, "avg_logprob": -0.16163003324258207, "compression_ratio": 1.5336322869955157, "no_speech_prob": 1.5689171050325967e-05}, {"id": 1114, "seek": 577654, "start": 5776.54, "end": 5780.48, "text": " to the string tokens.", "tokens": [281, 264, 6798, 22667, 13], "temperature": 0.0, "avg_logprob": -0.19277772449311756, "compression_ratio": 1.4973544973544974, "no_speech_prob": 1.0451340131112374e-05}, {"id": 1115, "seek": 577654, "start": 5780.48, "end": 5787.16, "text": " We have the standard FastAI XX for kind of special tokens.", "tokens": [492, 362, 264, 3832, 15968, 48698, 27050, 337, 733, 295, 2121, 22667, 13], "temperature": 0.0, "avg_logprob": -0.19277772449311756, "compression_ratio": 1.4973544973544974, "no_speech_prob": 1.0451340131112374e-05}, {"id": 1116, "seek": 577654, "start": 5787.16, "end": 5790.44, "text": " And then here we have the word, we have a comma, 100,000.", "tokens": [400, 550, 510, 321, 362, 264, 1349, 11, 321, 362, 257, 22117, 11, 2319, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.19277772449311756, "compression_ratio": 1.4973544973544974, "no_speech_prob": 1.0451340131112374e-05}, {"id": 1117, "seek": 577654, "start": 5790.44, "end": 5797.28, "text": " And then the numbers, 1, 2, 3, 4, 5, 6, 7, 8, 9, and so on up to 19.", "tokens": [400, 550, 264, 3547, 11, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 1386, 11, 1614, 11, 1649, 11, 1722, 11, 293, 370, 322, 493, 281, 1294, 13], "temperature": 0.0, "avg_logprob": -0.19277772449311756, "compression_ratio": 1.4973544973544974, "no_speech_prob": 1.0451340131112374e-05}, {"id": 1118, "seek": 577654, "start": 5797.28, "end": 5803.96, "text": " With, sorry, in this section, we've got our 20, 30, 40, 50, 60, 70, 80, 90.", "tokens": [2022, 11, 2597, 11, 294, 341, 3541, 11, 321, 600, 658, 527, 945, 11, 2217, 11, 3356, 11, 2625, 11, 4060, 11, 5285, 11, 4688, 11, 4289, 13], "temperature": 0.0, "avg_logprob": -0.19277772449311756, "compression_ratio": 1.4973544973544974, "no_speech_prob": 1.0451340131112374e-05}, {"id": 1119, "seek": 580396, "start": 5803.96, "end": 5812.52, "text": " So these are the kind of the components we'll be using to make these list of numbers.", "tokens": [407, 613, 366, 264, 733, 295, 264, 6677, 321, 603, 312, 1228, 281, 652, 613, 1329, 295, 3547, 13], "temperature": 0.0, "avg_logprob": -0.12893531488817792, "compression_ratio": 1.4421052631578948, "no_speech_prob": 7.766800990793854e-06}, {"id": 1120, "seek": 580396, "start": 5812.52, "end": 5818.24, "text": " We can see the, actually, it's probably easier to go back and forth.", "tokens": [492, 393, 536, 264, 11, 767, 11, 309, 311, 1391, 3571, 281, 352, 646, 293, 5220, 13], "temperature": 0.0, "avg_logprob": -0.12893531488817792, "compression_ratio": 1.4421052631578948, "no_speech_prob": 7.766800990793854e-06}, {"id": 1121, "seek": 580396, "start": 5818.24, "end": 5820.76, "text": " So here is X1.", "tokens": [407, 510, 307, 1783, 16, 13], "temperature": 0.0, "avg_logprob": -0.12893531488817792, "compression_ratio": 1.4421052631578948, "no_speech_prob": 7.766800990793854e-06}, {"id": 1122, "seek": 580396, "start": 5820.76, "end": 5824.6, "text": " There's even a textify method built into the vocab that we can look.", "tokens": [821, 311, 754, 257, 2487, 2505, 3170, 3094, 666, 264, 2329, 455, 300, 321, 393, 574, 13], "temperature": 0.0, "avg_logprob": -0.12893531488817792, "compression_ratio": 1.4421052631578948, "no_speech_prob": 7.766800990793854e-06}, {"id": 1123, "seek": 580396, "start": 5824.6, "end": 5829.92, "text": " This is the 8001, 8002, 8003, 8004.", "tokens": [639, 307, 264, 13083, 16, 11, 13083, 17, 11, 13083, 18, 11, 13083, 19, 13], "temperature": 0.0, "avg_logprob": -0.12893531488817792, "compression_ratio": 1.4421052631578948, "no_speech_prob": 7.766800990793854e-06}, {"id": 1124, "seek": 582992, "start": 5829.92, "end": 5835.56, "text": " And if we textify Y1, we'll see that it's just been shifted over.", "tokens": [400, 498, 321, 2487, 2505, 398, 16, 11, 321, 603, 536, 300, 309, 311, 445, 668, 18892, 670, 13], "temperature": 0.0, "avg_logprob": -0.2313713461665784, "compression_ratio": 1.5863453815261044, "no_speech_prob": 1.3631072761199903e-05}, {"id": 1125, "seek": 582992, "start": 5835.56, "end": 5838.96, "text": " So we no longer have this XX beginning of string.", "tokens": [407, 321, 572, 2854, 362, 341, 27050, 2863, 295, 6798, 13], "temperature": 0.0, "avg_logprob": -0.2313713461665784, "compression_ratio": 1.5863453815261044, "no_speech_prob": 1.3631072761199903e-05}, {"id": 1126, "seek": 582992, "start": 5838.96, "end": 5843.8, "text": " But if we, you know, our task is to predict what comes next after beginning of string", "tokens": [583, 498, 321, 11, 291, 458, 11, 527, 5633, 307, 281, 6069, 437, 1487, 958, 934, 2863, 295, 6798], "temperature": 0.0, "avg_logprob": -0.2313713461665784, "compression_ratio": 1.5863453815261044, "no_speech_prob": 1.3631072761199903e-05}, {"id": 1127, "seek": 582992, "start": 5843.8, "end": 5850.04, "text": " comes 8, after 8 comes 1,000, after 1,000 comes 1.", "tokens": [1487, 1649, 11, 934, 1649, 1487, 502, 11, 1360, 11, 934, 502, 11, 1360, 1487, 502, 13], "temperature": 0.0, "avg_logprob": -0.2313713461665784, "compression_ratio": 1.5863453815261044, "no_speech_prob": 1.3631072761199903e-05}, {"id": 1128, "seek": 582992, "start": 5850.04, "end": 5853.0, "text": " You can see this kind of direct mapping.", "tokens": [509, 393, 536, 341, 733, 295, 2047, 18350, 13], "temperature": 0.0, "avg_logprob": -0.2313713461665784, "compression_ratio": 1.5863453815261044, "no_speech_prob": 1.3631072761199903e-05}, {"id": 1129, "seek": 582992, "start": 5853.0, "end": 5854.0, "text": " Jeremy?", "tokens": [17809, 30], "temperature": 0.0, "avg_logprob": -0.2313713461665784, "compression_ratio": 1.5863453815261044, "no_speech_prob": 1.3631072761199903e-05}, {"id": 1130, "seek": 582992, "start": 5854.0, "end": 5858.04, "text": " I don't know if we've talked much about these XXBOS things.", "tokens": [286, 500, 380, 458, 498, 321, 600, 2825, 709, 466, 613, 27050, 33, 4367, 721, 13], "temperature": 0.0, "avg_logprob": -0.2313713461665784, "compression_ratio": 1.5863453815261044, "no_speech_prob": 1.3631072761199903e-05}, {"id": 1131, "seek": 582992, "start": 5858.04, "end": 5859.6, "text": " I just sort of briefly mentioned.", "tokens": [286, 445, 1333, 295, 10515, 2835, 13], "temperature": 0.0, "avg_logprob": -0.2313713461665784, "compression_ratio": 1.5863453815261044, "no_speech_prob": 1.3631072761199903e-05}, {"id": 1132, "seek": 585960, "start": 5859.6, "end": 5867.6, "text": " These special tokens that aren't words that signify a thing, like this is the start of", "tokens": [1981, 2121, 22667, 300, 3212, 380, 2283, 300, 1465, 2505, 257, 551, 11, 411, 341, 307, 264, 722, 295], "temperature": 0.0, "avg_logprob": -0.24457117537377585, "compression_ratio": 1.4451612903225806, "no_speech_prob": 2.9309592719073407e-05}, {"id": 1133, "seek": 585960, "start": 5867.6, "end": 5877.64, "text": " a document, XXBOS, or the next word was capitalized, XXBEDGE, or this is the end of a document,", "tokens": [257, 4166, 11, 27050, 33, 4367, 11, 420, 264, 958, 1349, 390, 4238, 1602, 11, 27050, 33, 4731, 9177, 11, 420, 341, 307, 264, 917, 295, 257, 4166, 11], "temperature": 0.0, "avg_logprob": -0.24457117537377585, "compression_ratio": 1.4451612903225806, "no_speech_prob": 2.9309592719073407e-05}, {"id": 1134, "seek": 585960, "start": 5877.64, "end": 5879.08, "text": " XXEOS.", "tokens": [27050, 36, 4367, 13], "temperature": 0.0, "avg_logprob": -0.24457117537377585, "compression_ratio": 1.4451612903225806, "no_speech_prob": 2.9309592719073407e-05}, {"id": 1135, "seek": 585960, "start": 5879.08, "end": 5882.92, "text": " They appear in lots of NLP things.", "tokens": [814, 4204, 294, 3195, 295, 426, 45196, 721, 13], "temperature": 0.0, "avg_logprob": -0.24457117537377585, "compression_ratio": 1.4451612903225806, "no_speech_prob": 2.9309592719073407e-05}, {"id": 1136, "seek": 588292, "start": 5882.92, "end": 5889.88, "text": " One of the most common ones we'll see is UNK for an unknown character, which means this", "tokens": [1485, 295, 264, 881, 2689, 2306, 321, 603, 536, 307, 8229, 42, 337, 364, 9841, 2517, 11, 597, 1355, 341], "temperature": 0.0, "avg_logprob": -0.24144301751647332, "compression_ratio": 1.5589519650655022, "no_speech_prob": 3.9437807572539896e-05}, {"id": 1137, "seek": 588292, "start": 5889.88, "end": 5896.56, "text": " character didn't fit into our vocabulary.", "tokens": [2517, 994, 380, 3318, 666, 527, 19864, 13], "temperature": 0.0, "avg_logprob": -0.24144301751647332, "compression_ratio": 1.5589519650655022, "no_speech_prob": 3.9437807572539896e-05}, {"id": 1138, "seek": 588292, "start": 5896.56, "end": 5899.32, "text": " We looked at this in an earlier lesson.", "tokens": [492, 2956, 412, 341, 294, 364, 3071, 6898, 13], "temperature": 0.0, "avg_logprob": -0.24144301751647332, "compression_ratio": 1.5589519650655022, "no_speech_prob": 3.9437807572539896e-05}, {"id": 1139, "seek": 588292, "start": 5899.32, "end": 5906.0, "text": " We saw with, I think this was with IMDB, that a lot of words could get mapped to XX unknown.", "tokens": [492, 1866, 365, 11, 286, 519, 341, 390, 365, 21463, 27735, 11, 300, 257, 688, 295, 2283, 727, 483, 33318, 281, 27050, 9841, 13], "temperature": 0.0, "avg_logprob": -0.24144301751647332, "compression_ratio": 1.5589519650655022, "no_speech_prob": 3.9437807572539896e-05}, {"id": 1140, "seek": 588292, "start": 5906.0, "end": 5911.4800000000005, "text": " So it's not a many-to-one mapping, that you can have many different words going to XX", "tokens": [407, 309, 311, 406, 257, 867, 12, 1353, 12, 546, 18350, 11, 300, 291, 393, 362, 867, 819, 2283, 516, 281, 27050], "temperature": 0.0, "avg_logprob": -0.24144301751647332, "compression_ratio": 1.5589519650655022, "no_speech_prob": 3.9437807572539896e-05}, {"id": 1141, "seek": 588292, "start": 5911.4800000000005, "end": 5912.4800000000005, "text": " unknown.", "tokens": [9841, 13], "temperature": 0.0, "avg_logprob": -0.24144301751647332, "compression_ratio": 1.5589519650655022, "no_speech_prob": 3.9437807572539896e-05}, {"id": 1142, "seek": 591248, "start": 5912.48, "end": 5918.4, "text": " One of the nice things about sentence pieces is you don't generally get XXUNK, so you can", "tokens": [1485, 295, 264, 1481, 721, 466, 8174, 3755, 307, 291, 500, 380, 5101, 483, 27050, 3979, 42, 11, 370, 291, 393], "temperature": 0.0, "avg_logprob": -0.23748207092285156, "compression_ratio": 1.5697211155378485, "no_speech_prob": 1.805821739253588e-05}, {"id": 1143, "seek": 591248, "start": 5918.4, "end": 5922.679999999999, "text": " represent long actors' names and stuff.", "tokens": [2906, 938, 10037, 6, 5288, 293, 1507, 13], "temperature": 0.0, "avg_logprob": -0.23748207092285156, "compression_ratio": 1.5697211155378485, "no_speech_prob": 1.805821739253588e-05}, {"id": 1144, "seek": 591248, "start": 5922.679999999999, "end": 5929.48, "text": " I was just going to mention, most of the time outside FastAI, they tend to use some kind", "tokens": [286, 390, 445, 516, 281, 2152, 11, 881, 295, 264, 565, 2380, 15968, 48698, 11, 436, 3928, 281, 764, 512, 733], "temperature": 0.0, "avg_logprob": -0.23748207092285156, "compression_ratio": 1.5697211155378485, "no_speech_prob": 1.805821739253588e-05}, {"id": 1145, "seek": 591248, "start": 5929.48, "end": 5935.04, "text": " of punctuation, like UNK is often angle bracket, close angle bracket, and that tends to be", "tokens": [295, 27006, 16073, 11, 411, 8229, 42, 307, 2049, 5802, 16904, 11, 1998, 5802, 16904, 11, 293, 300, 12258, 281, 312], "temperature": 0.0, "avg_logprob": -0.23748207092285156, "compression_ratio": 1.5697211155378485, "no_speech_prob": 1.805821739253588e-05}, {"id": 1146, "seek": 591248, "start": 5935.04, "end": 5939.78, "text": " really difficult when dealing with things like spaCy, because it'll tend to tokenize", "tokens": [534, 2252, 562, 6260, 365, 721, 411, 32543, 34, 88, 11, 570, 309, 603, 3928, 281, 14862, 1125], "temperature": 0.0, "avg_logprob": -0.23748207092285156, "compression_ratio": 1.5697211155378485, "no_speech_prob": 1.805821739253588e-05}, {"id": 1147, "seek": 593978, "start": 5939.78, "end": 5947.5599999999995, "text": " it as angle bracket space, space angle bracket, and that's not great for your current neural", "tokens": [309, 382, 5802, 16904, 1901, 11, 1901, 5802, 16904, 11, 293, 300, 311, 406, 869, 337, 428, 2190, 18161], "temperature": 0.0, "avg_logprob": -0.1940733024052211, "compression_ratio": 1.6525096525096525, "no_speech_prob": 3.7634210457326844e-05}, {"id": 1148, "seek": 593978, "start": 5947.5599999999995, "end": 5952.32, "text": " network, because when you have more tokens, it's longer that it has to remember the state", "tokens": [3209, 11, 570, 562, 291, 362, 544, 22667, 11, 309, 311, 2854, 300, 309, 575, 281, 1604, 264, 1785], "temperature": 0.0, "avg_logprob": -0.1940733024052211, "compression_ratio": 1.6525096525096525, "no_speech_prob": 3.7634210457326844e-05}, {"id": 1149, "seek": 593978, "start": 5952.32, "end": 5953.32, "text": " for.", "tokens": [337, 13], "temperature": 0.0, "avg_logprob": -0.1940733024052211, "compression_ratio": 1.6525096525096525, "no_speech_prob": 3.7634210457326844e-05}, {"id": 1150, "seek": 593978, "start": 5953.32, "end": 5958.88, "text": " So if you're wondering why does FastAI have this weird XX thing, it's because in every", "tokens": [407, 498, 291, 434, 6359, 983, 775, 15968, 48698, 362, 341, 3657, 27050, 551, 11, 309, 311, 570, 294, 633], "temperature": 0.0, "avg_logprob": -0.1940733024052211, "compression_ratio": 1.6525096525096525, "no_speech_prob": 3.7634210457326844e-05}, {"id": 1151, "seek": 593978, "start": 5958.88, "end": 5964.639999999999, "text": " system it's going to tokenize sensibly, even with sentence pieces.", "tokens": [1185, 309, 311, 516, 281, 14862, 1125, 2923, 3545, 11, 754, 365, 8174, 3755, 13], "temperature": 0.0, "avg_logprob": -0.1940733024052211, "compression_ratio": 1.6525096525096525, "no_speech_prob": 3.7634210457326844e-05}, {"id": 1152, "seek": 593978, "start": 5964.639999999999, "end": 5968.84, "text": " Sentence pieces generally are going to say, oh, I see XXUNK very often, and so it will", "tokens": [23652, 655, 3755, 5101, 366, 516, 281, 584, 11, 1954, 11, 286, 536, 27050, 3979, 42, 588, 2049, 11, 293, 370, 309, 486], "temperature": 0.0, "avg_logprob": -0.1940733024052211, "compression_ratio": 1.6525096525096525, "no_speech_prob": 3.7634210457326844e-05}, {"id": 1153, "seek": 596884, "start": 5968.84, "end": 5975.0, "text": " tend to automatically create a single sentence piece token for it.", "tokens": [3928, 281, 6772, 1884, 257, 2167, 8174, 2522, 14862, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.16181041945272417, "compression_ratio": 1.488888888888889, "no_speech_prob": 2.467987360432744e-05}, {"id": 1154, "seek": 596884, "start": 5975.0, "end": 5979.52, "text": " So when you're working with other systems that use angle brackets and stuff for their", "tokens": [407, 562, 291, 434, 1364, 365, 661, 3652, 300, 764, 5802, 26179, 293, 1507, 337, 641], "temperature": 0.0, "avg_logprob": -0.16181041945272417, "compression_ratio": 1.488888888888889, "no_speech_prob": 2.467987360432744e-05}, {"id": 1155, "seek": 596884, "start": 5979.52, "end": 5983.96, "text": " special tokens, you've got to be super careful of how they end up tokenizing.", "tokens": [2121, 22667, 11, 291, 600, 658, 281, 312, 1687, 5026, 295, 577, 436, 917, 493, 14862, 3319, 13], "temperature": 0.0, "avg_logprob": -0.16181041945272417, "compression_ratio": 1.488888888888889, "no_speech_prob": 2.467987360432744e-05}, {"id": 1156, "seek": 596884, "start": 5983.96, "end": 5984.96, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.16181041945272417, "compression_ratio": 1.488888888888889, "no_speech_prob": 2.467987360432744e-05}, {"id": 1157, "seek": 596884, "start": 5984.96, "end": 5985.96, "text": " Those are all good points.", "tokens": [3950, 366, 439, 665, 2793, 13], "temperature": 0.0, "avg_logprob": -0.16181041945272417, "compression_ratio": 1.488888888888889, "no_speech_prob": 2.467987360432744e-05}, {"id": 1158, "seek": 598596, "start": 5985.96, "end": 6005.52, "text": " And again, as a reminder, tokenization is always implementation dependent.", "tokens": [400, 797, 11, 382, 257, 13548, 11, 14862, 2144, 307, 1009, 11420, 12334, 13], "temperature": 0.0, "avg_logprob": -0.29459794362386066, "compression_ratio": 1.0277777777777777, "no_speech_prob": 1.1478048691060394e-05}, {"id": 1159, "seek": 600552, "start": 6005.52, "end": 6016.8, "text": " It's going to depend on the particular library, how things get tokenized.", "tokens": [467, 311, 516, 281, 5672, 322, 264, 1729, 6405, 11, 577, 721, 483, 14862, 1602, 13], "temperature": 0.0, "avg_logprob": -0.14427057902018228, "compression_ratio": 1.4146341463414633, "no_speech_prob": 2.9770795663353056e-05}, {"id": 1160, "seek": 600552, "start": 6016.8, "end": 6025.88, "text": " So continuing with this, we can see how we've tokenized, and also just to kind of be aware,", "tokens": [407, 9289, 365, 341, 11, 321, 393, 536, 577, 321, 600, 14862, 1602, 11, 293, 611, 445, 281, 733, 295, 312, 3650, 11], "temperature": 0.0, "avg_logprob": -0.14427057902018228, "compression_ratio": 1.4146341463414633, "no_speech_prob": 2.9770795663353056e-05}, {"id": 1161, "seek": 600552, "start": 6025.88, "end": 6032.68, "text": " X10 is giving you this line of our sequence that's 70 tokens long.", "tokens": [1783, 3279, 307, 2902, 291, 341, 1622, 295, 527, 8310, 300, 311, 5285, 22667, 938, 13], "temperature": 0.0, "avg_logprob": -0.14427057902018228, "compression_ratio": 1.4146341463414633, "no_speech_prob": 2.9770795663353056e-05}, {"id": 1162, "seek": 603268, "start": 6032.68, "end": 6037.320000000001, "text": " There'll be 64 such lines in a batch.", "tokens": [821, 603, 312, 12145, 1270, 3876, 294, 257, 15245, 13], "temperature": 0.0, "avg_logprob": -0.13502584184919084, "compression_ratio": 1.3061224489795917, "no_speech_prob": 0.00012532314576674253}, {"id": 1163, "seek": 603268, "start": 6037.320000000001, "end": 6038.320000000001, "text": " We can inspect.", "tokens": [492, 393, 15018, 13], "temperature": 0.0, "avg_logprob": -0.13502584184919084, "compression_ratio": 1.3061224489795917, "no_speech_prob": 0.00012532314576674253}, {"id": 1164, "seek": 603268, "start": 6038.320000000001, "end": 6048.400000000001, "text": " Oh, so something to note here is X20 follows X10.", "tokens": [876, 11, 370, 746, 281, 3637, 510, 307, 1783, 2009, 10002, 1783, 3279, 13], "temperature": 0.0, "avg_logprob": -0.13502584184919084, "compression_ratio": 1.3061224489795917, "no_speech_prob": 0.00012532314576674253}, {"id": 1165, "seek": 603268, "start": 6048.400000000001, "end": 6058.26, "text": " So see how we go 8,017, 8, and then when we skip ahead to X20, it's 8,018, 8,019, 8,020.", "tokens": [407, 536, 577, 321, 352, 1649, 11, 15, 7773, 11, 1649, 11, 293, 550, 562, 321, 10023, 2286, 281, 1783, 2009, 11, 309, 311, 1649, 11, 15, 6494, 11, 1649, 11, 15, 3405, 11, 1649, 11, 15, 2009, 13], "temperature": 0.0, "avg_logprob": -0.13502584184919084, "compression_ratio": 1.3061224489795917, "no_speech_prob": 0.00012532314576674253}, {"id": 1166, "seek": 605826, "start": 6058.26, "end": 6064.56, "text": " So to note that the way these batches line up is that in going from one batch to the", "tokens": [407, 281, 3637, 300, 264, 636, 613, 15245, 279, 1622, 493, 307, 300, 294, 516, 490, 472, 15245, 281, 264], "temperature": 0.0, "avg_logprob": -0.2021183249068587, "compression_ratio": 1.7870967741935484, "no_speech_prob": 2.046169356617611e-05}, {"id": 1167, "seek": 605826, "start": 6064.56, "end": 6071.2, "text": " next, the first line of your first batch is going to be followed by the first line of", "tokens": [958, 11, 264, 700, 1622, 295, 428, 700, 15245, 307, 516, 281, 312, 6263, 538, 264, 700, 1622, 295], "temperature": 0.0, "avg_logprob": -0.2021183249068587, "compression_ratio": 1.7870967741935484, "no_speech_prob": 2.046169356617611e-05}, {"id": 1168, "seek": 605826, "start": 6071.2, "end": 6072.64, "text": " the second batch.", "tokens": [264, 1150, 15245, 13], "temperature": 0.0, "avg_logprob": -0.2021183249068587, "compression_ratio": 1.7870967741935484, "no_speech_prob": 2.046169356617611e-05}, {"id": 1169, "seek": 605826, "start": 6072.64, "end": 6083.320000000001, "text": " Let me just contrast that with if we were to look at the second line of the first batch.", "tokens": [961, 385, 445, 8712, 300, 365, 498, 321, 645, 281, 574, 412, 264, 1150, 1622, 295, 264, 700, 15245, 13], "temperature": 0.0, "avg_logprob": -0.2021183249068587, "compression_ratio": 1.7870967741935484, "no_speech_prob": 2.046169356617611e-05}, {"id": 1170, "seek": 608332, "start": 6083.32, "end": 6088.88, "text": " Oops, not run these.", "tokens": [21726, 11, 406, 1190, 613, 13], "temperature": 0.0, "avg_logprob": -0.26684149106343585, "compression_ratio": 1.4242424242424243, "no_speech_prob": 5.4221900427364744e-06}, {"id": 1171, "seek": 608332, "start": 6088.88, "end": 6096.0, "text": " You might have expected that going from line one to line two of the same batch would have", "tokens": [509, 1062, 362, 5176, 300, 516, 490, 1622, 472, 281, 1622, 732, 295, 264, 912, 15245, 576, 362], "temperature": 0.0, "avg_logprob": -0.26684149106343585, "compression_ratio": 1.4242424242424243, "no_speech_prob": 5.4221900427364744e-06}, {"id": 1172, "seek": 608332, "start": 6096.0, "end": 6098.08, "text": " continuity, but that's not the case.", "tokens": [23807, 11, 457, 300, 311, 406, 264, 1389, 13], "temperature": 0.0, "avg_logprob": -0.26684149106343585, "compression_ratio": 1.4242424242424243, "no_speech_prob": 5.4221900427364744e-06}, {"id": 1173, "seek": 608332, "start": 6098.08, "end": 6100.679999999999, "text": " The continuity is across batches.", "tokens": [440, 23807, 307, 2108, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.26684149106343585, "compression_ratio": 1.4242424242424243, "no_speech_prob": 5.4221900427364744e-06}, {"id": 1174, "seek": 608332, "start": 6100.679999999999, "end": 6103.599999999999, "text": " This is pretty quick to run.", "tokens": [639, 307, 1238, 1702, 281, 1190, 13], "temperature": 0.0, "avg_logprob": -0.26684149106343585, "compression_ratio": 1.4242424242424243, "no_speech_prob": 5.4221900427364744e-06}, {"id": 1175, "seek": 608332, "start": 6103.599999999999, "end": 6104.599999999999, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.26684149106343585, "compression_ratio": 1.4242424242424243, "no_speech_prob": 5.4221900427364744e-06}, {"id": 1176, "seek": 608332, "start": 6104.599999999999, "end": 6110.96, "text": " So now I've got V.", "tokens": [407, 586, 286, 600, 658, 691, 13], "temperature": 0.0, "avg_logprob": -0.26684149106343585, "compression_ratio": 1.4242424242424243, "no_speech_prob": 5.4221900427364744e-06}, {"id": 1177, "seek": 611096, "start": 6110.96, "end": 6117.44, "text": " We can take a look at here and see this is 8,046.", "tokens": [492, 393, 747, 257, 574, 412, 510, 293, 536, 341, 307, 1649, 11, 14565, 21, 13], "temperature": 0.0, "avg_logprob": -0.15765770372137966, "compression_ratio": 1.5644171779141105, "no_speech_prob": 3.4465608678146964e-06}, {"id": 1178, "seek": 611096, "start": 6117.44, "end": 6123.92, "text": " So we've kind of jumped ahead for the second line of batch one, but we can expect that", "tokens": [407, 321, 600, 733, 295, 13864, 2286, 337, 264, 1150, 1622, 295, 15245, 472, 11, 457, 321, 393, 2066, 300], "temperature": 0.0, "avg_logprob": -0.15765770372137966, "compression_ratio": 1.5644171779141105, "no_speech_prob": 3.4465608678146964e-06}, {"id": 1179, "seek": 611096, "start": 6123.92, "end": 6132.2, "text": " that'll line up with the second line of batch two because here X2 is the second batch and", "tokens": [300, 603, 1622, 493, 365, 264, 1150, 1622, 295, 15245, 732, 570, 510, 1783, 17, 307, 264, 1150, 15245, 293], "temperature": 0.0, "avg_logprob": -0.15765770372137966, "compression_ratio": 1.5644171779141105, "no_speech_prob": 3.4465608678146964e-06}, {"id": 1180, "seek": 611096, "start": 6132.2, "end": 6138.52, "text": " it does 8,059, 8,060, 8,061.", "tokens": [309, 775, 1649, 11, 13328, 24, 11, 1649, 11, 15, 4550, 11, 1649, 11, 12791, 16, 13], "temperature": 0.0, "avg_logprob": -0.15765770372137966, "compression_ratio": 1.5644171779141105, "no_speech_prob": 3.4465608678146964e-06}, {"id": 1181, "seek": 613852, "start": 6138.52, "end": 6148.6, "text": " So scrolling down, we can see we can use show batch to get another view of what the batch", "tokens": [407, 29053, 760, 11, 321, 393, 536, 321, 393, 764, 855, 15245, 281, 483, 1071, 1910, 295, 437, 264, 15245], "temperature": 0.0, "avg_logprob": -0.1717236042022705, "compression_ratio": 1.5265957446808511, "no_speech_prob": 1.2029439858451951e-05}, {"id": 1182, "seek": 613852, "start": 6148.6, "end": 6149.6, "text": " looks like.", "tokens": [1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.1717236042022705, "compression_ratio": 1.5265957446808511, "no_speech_prob": 1.2029439858451951e-05}, {"id": 1183, "seek": 613852, "start": 6149.6, "end": 6153.88, "text": " And so how we're going to approach this is we're going to iteratively consider a few", "tokens": [400, 370, 577, 321, 434, 516, 281, 3109, 341, 307, 321, 434, 516, 281, 17138, 19020, 1949, 257, 1326], "temperature": 0.0, "avg_logprob": -0.1717236042022705, "compression_ratio": 1.5265957446808511, "no_speech_prob": 1.2029439858451951e-05}, {"id": 1184, "seek": 613852, "start": 6153.88, "end": 6157.400000000001, "text": " different models kind of building up to the traditional RNN.", "tokens": [819, 5245, 733, 295, 2390, 493, 281, 264, 5164, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.1717236042022705, "compression_ratio": 1.5265957446808511, "no_speech_prob": 1.2029439858451951e-05}, {"id": 1185, "seek": 613852, "start": 6157.400000000001, "end": 6159.8, "text": " We're going to start very, very simple.", "tokens": [492, 434, 516, 281, 722, 588, 11, 588, 2199, 13], "temperature": 0.0, "avg_logprob": -0.1717236042022705, "compression_ratio": 1.5265957446808511, "no_speech_prob": 1.2029439858451951e-05}, {"id": 1186, "seek": 615980, "start": 6159.8, "end": 6169.12, "text": " And let me switch to I have some slides that show this.", "tokens": [400, 718, 385, 3679, 281, 286, 362, 512, 9788, 300, 855, 341, 13], "temperature": 0.0, "avg_logprob": -0.19312957525253296, "compression_ratio": 1.5148514851485149, "no_speech_prob": 4.565905783238122e-06}, {"id": 1187, "seek": 615980, "start": 6169.12, "end": 6172.56, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.19312957525253296, "compression_ratio": 1.5148514851485149, "no_speech_prob": 4.565905783238122e-06}, {"id": 1188, "seek": 615980, "start": 6172.56, "end": 6178.96, "text": " And this was covered in lesson seven of the Fast AI Deep Learning course, although we're", "tokens": [400, 341, 390, 5343, 294, 6898, 3407, 295, 264, 15968, 7318, 14895, 15205, 1164, 11, 4878, 321, 434], "temperature": 0.0, "avg_logprob": -0.19312957525253296, "compression_ratio": 1.5148514851485149, "no_speech_prob": 4.565905783238122e-06}, {"id": 1189, "seek": 615980, "start": 6178.96, "end": 6183.28, "text": " going to kind of be going through it much more slowly and in more detail.", "tokens": [516, 281, 733, 295, 312, 516, 807, 309, 709, 544, 5692, 293, 294, 544, 2607, 13], "temperature": 0.0, "avg_logprob": -0.19312957525253296, "compression_ratio": 1.5148514851485149, "no_speech_prob": 4.565905783238122e-06}, {"id": 1190, "seek": 615980, "start": 6183.28, "end": 6188.24, "text": " So there it's kind of covered as a quick example, but these slides are from there.", "tokens": [407, 456, 309, 311, 733, 295, 5343, 382, 257, 1702, 1365, 11, 457, 613, 9788, 366, 490, 456, 13], "temperature": 0.0, "avg_logprob": -0.19312957525253296, "compression_ratio": 1.5148514851485149, "no_speech_prob": 4.565905783238122e-06}, {"id": 1191, "seek": 618824, "start": 6188.24, "end": 6193.24, "text": " And so use the same, well, they are the same, including the convention.", "tokens": [400, 370, 764, 264, 912, 11, 731, 11, 436, 366, 264, 912, 11, 3009, 264, 10286, 13], "temperature": 0.0, "avg_logprob": -0.14436517942936047, "compression_ratio": 1.7361702127659575, "no_speech_prob": 2.1110499801579863e-05}, {"id": 1192, "seek": 618824, "start": 6193.24, "end": 6198.24, "text": " And so here these slides, these diagrams have a lot of meaning in them.", "tokens": [400, 370, 510, 613, 9788, 11, 613, 36709, 362, 257, 688, 295, 3620, 294, 552, 13], "temperature": 0.0, "avg_logprob": -0.14436517942936047, "compression_ratio": 1.7361702127659575, "no_speech_prob": 2.1110499801579863e-05}, {"id": 1193, "seek": 618824, "start": 6198.24, "end": 6201.28, "text": " So it will be kind of important to keep track of.", "tokens": [407, 309, 486, 312, 733, 295, 1021, 281, 1066, 2837, 295, 13], "temperature": 0.0, "avg_logprob": -0.14436517942936047, "compression_ratio": 1.7361702127659575, "no_speech_prob": 2.1110499801579863e-05}, {"id": 1194, "seek": 618824, "start": 6201.28, "end": 6204.92, "text": " And so the first to note is the shape.", "tokens": [400, 370, 264, 700, 281, 3637, 307, 264, 3909, 13], "temperature": 0.0, "avg_logprob": -0.14436517942936047, "compression_ratio": 1.7361702127659575, "no_speech_prob": 2.1110499801579863e-05}, {"id": 1195, "seek": 618824, "start": 6204.92, "end": 6212.42, "text": " Rectangle means input, circle means a hidden layer, and then a triangle means the output.", "tokens": [497, 557, 7846, 1355, 4846, 11, 6329, 1355, 257, 7633, 4583, 11, 293, 550, 257, 13369, 1355, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.14436517942936047, "compression_ratio": 1.7361702127659575, "no_speech_prob": 2.1110499801579863e-05}, {"id": 1196, "seek": 618824, "start": 6212.42, "end": 6216.639999999999, "text": " So a very, very basic, kind of one of the most basic neural networks you can think of", "tokens": [407, 257, 588, 11, 588, 3875, 11, 733, 295, 472, 295, 264, 881, 3875, 18161, 9590, 291, 393, 519, 295], "temperature": 0.0, "avg_logprob": -0.14436517942936047, "compression_ratio": 1.7361702127659575, "no_speech_prob": 2.1110499801579863e-05}, {"id": 1197, "seek": 621664, "start": 6216.64, "end": 6223.0, "text": " with a single hidden layer would be to take an input in, do a matrix product, and then", "tokens": [365, 257, 2167, 7633, 4583, 576, 312, 281, 747, 364, 4846, 294, 11, 360, 257, 8141, 1674, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.18372411930814703, "compression_ratio": 1.6255707762557077, "no_speech_prob": 1.5935398550936952e-05}, {"id": 1198, "seek": 621664, "start": 6223.0, "end": 6230.52, "text": " a ReLU, measure non-linearity, and that would give you this hidden state.", "tokens": [257, 1300, 43, 52, 11, 3481, 2107, 12, 1889, 17409, 11, 293, 300, 576, 976, 291, 341, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.18372411930814703, "compression_ratio": 1.6255707762557077, "no_speech_prob": 1.5935398550936952e-05}, {"id": 1199, "seek": 621664, "start": 6230.52, "end": 6234.4800000000005, "text": " Then take another matrix product and a softmax, and you have your output.", "tokens": [1396, 747, 1071, 8141, 1674, 293, 257, 2787, 41167, 11, 293, 291, 362, 428, 5598, 13], "temperature": 0.0, "avg_logprob": -0.18372411930814703, "compression_ratio": 1.6255707762557077, "no_speech_prob": 1.5935398550936952e-05}, {"id": 1200, "seek": 621664, "start": 6234.4800000000005, "end": 6243.160000000001, "text": " So this is kind of one of the simplest neural networks you can think of.", "tokens": [407, 341, 307, 733, 295, 472, 295, 264, 22811, 18161, 9590, 291, 393, 519, 295, 13], "temperature": 0.0, "avg_logprob": -0.18372411930814703, "compression_ratio": 1.6255707762557077, "no_speech_prob": 1.5935398550936952e-05}, {"id": 1201, "seek": 621664, "start": 6243.160000000001, "end": 6246.6, "text": " And hidden state just refers to the activations.", "tokens": [400, 7633, 1785, 445, 14942, 281, 264, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.18372411930814703, "compression_ratio": 1.6255707762557077, "no_speech_prob": 1.5935398550936952e-05}, {"id": 1202, "seek": 624660, "start": 6246.6, "end": 6250.6, "text": " So here this corresponds to the hidden layer.", "tokens": [407, 510, 341, 23249, 281, 264, 7633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.14989849272228423, "compression_ratio": 1.844155844155844, "no_speech_prob": 1.8631297280080616e-05}, {"id": 1203, "seek": 624660, "start": 6250.6, "end": 6255.72, "text": " It's the activations you get out from, you did this matrix product and ReLU, and the", "tokens": [467, 311, 264, 2430, 763, 291, 483, 484, 490, 11, 291, 630, 341, 8141, 1674, 293, 1300, 43, 52, 11, 293, 264], "temperature": 0.0, "avg_logprob": -0.14989849272228423, "compression_ratio": 1.844155844155844, "no_speech_prob": 1.8631297280080616e-05}, {"id": 1204, "seek": 624660, "start": 6255.72, "end": 6261.68, "text": " result sometimes people refer to as the hidden state, but those are just a set of activations", "tokens": [1874, 2171, 561, 2864, 281, 382, 264, 7633, 1785, 11, 457, 729, 366, 445, 257, 992, 295, 2430, 763], "temperature": 0.0, "avg_logprob": -0.14989849272228423, "compression_ratio": 1.844155844155844, "no_speech_prob": 1.8631297280080616e-05}, {"id": 1205, "seek": 624660, "start": 6261.68, "end": 6264.240000000001, "text": " in your hidden layer.", "tokens": [294, 428, 7633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.14989849272228423, "compression_ratio": 1.844155844155844, "no_speech_prob": 1.8631297280080616e-05}, {"id": 1206, "seek": 624660, "start": 6264.240000000001, "end": 6269.92, "text": " And here are the numbers for the matrix product, like that matrix, those are parameters that", "tokens": [400, 510, 366, 264, 3547, 337, 264, 8141, 1674, 11, 411, 300, 8141, 11, 729, 366, 9834, 300], "temperature": 0.0, "avg_logprob": -0.14989849272228423, "compression_ratio": 1.844155844155844, "no_speech_prob": 1.8631297280080616e-05}, {"id": 1207, "seek": 624660, "start": 6269.92, "end": 6276.52, "text": " you're training, and the result that you get from taking your input times this matrix,", "tokens": [291, 434, 3097, 11, 293, 264, 1874, 300, 291, 483, 490, 1940, 428, 4846, 1413, 341, 8141, 11], "temperature": 0.0, "avg_logprob": -0.14989849272228423, "compression_ratio": 1.844155844155844, "no_speech_prob": 1.8631297280080616e-05}, {"id": 1208, "seek": 627652, "start": 6276.52, "end": 6286.68, "text": " and then zeroing out all the negatives is a set of activations, aka your hidden state.", "tokens": [293, 550, 4018, 278, 484, 439, 264, 40019, 307, 257, 992, 295, 2430, 763, 11, 28042, 428, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.17013150227220752, "compression_ratio": 1.53475935828877, "no_speech_prob": 5.33809316038969e-06}, {"id": 1209, "seek": 627652, "start": 6286.68, "end": 6290.200000000001, "text": " So that's kind of a simplest neural network.", "tokens": [407, 300, 311, 733, 295, 257, 22811, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.17013150227220752, "compression_ratio": 1.53475935828877, "no_speech_prob": 5.33809316038969e-06}, {"id": 1210, "seek": 627652, "start": 6290.200000000001, "end": 6295.540000000001, "text": " So now we're going to move on to this idea of suppose we want to predict word three using", "tokens": [407, 586, 321, 434, 516, 281, 1286, 322, 281, 341, 1558, 295, 7297, 321, 528, 281, 6069, 1349, 1045, 1228], "temperature": 0.0, "avg_logprob": -0.17013150227220752, "compression_ratio": 1.53475935828877, "no_speech_prob": 5.33809316038969e-06}, {"id": 1211, "seek": 627652, "start": 6295.540000000001, "end": 6297.8, "text": " words one and two.", "tokens": [2283, 472, 293, 732, 13], "temperature": 0.0, "avg_logprob": -0.17013150227220752, "compression_ratio": 1.53475935828877, "no_speech_prob": 5.33809316038969e-06}, {"id": 1212, "seek": 627652, "start": 6297.8, "end": 6301.72, "text": " What we could do is feed word one in as input.", "tokens": [708, 321, 727, 360, 307, 3154, 1349, 472, 294, 382, 4846, 13], "temperature": 0.0, "avg_logprob": -0.17013150227220752, "compression_ratio": 1.53475935828877, "no_speech_prob": 5.33809316038969e-06}, {"id": 1213, "seek": 630172, "start": 6301.72, "end": 6307.96, "text": " I haven't written it here, but here when the arrow is happening that's a matrix multiplication", "tokens": [286, 2378, 380, 3720, 309, 510, 11, 457, 510, 562, 264, 11610, 307, 2737, 300, 311, 257, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.09419937831599538, "compression_ratio": 1.6205128205128205, "no_speech_prob": 1.6964093447313644e-05}, {"id": 1214, "seek": 630172, "start": 6307.96, "end": 6313.64, "text": " and a ReLU, we get a set of activations, and then basically we want a way to combine those", "tokens": [293, 257, 1300, 43, 52, 11, 321, 483, 257, 992, 295, 2430, 763, 11, 293, 550, 1936, 321, 528, 257, 636, 281, 10432, 729], "temperature": 0.0, "avg_logprob": -0.09419937831599538, "compression_ratio": 1.6205128205128205, "no_speech_prob": 1.6964093447313644e-05}, {"id": 1215, "seek": 630172, "start": 6313.64, "end": 6319.4400000000005, "text": " activations with our word two as input.", "tokens": [2430, 763, 365, 527, 1349, 732, 382, 4846, 13], "temperature": 0.0, "avg_logprob": -0.09419937831599538, "compression_ratio": 1.6205128205128205, "no_speech_prob": 1.6964093447313644e-05}, {"id": 1216, "seek": 630172, "start": 6319.4400000000005, "end": 6325.92, "text": " And so we'll combine those two and get another set of activations, we can put that through", "tokens": [400, 370, 321, 603, 10432, 729, 732, 293, 483, 1071, 992, 295, 2430, 763, 11, 321, 393, 829, 300, 807], "temperature": 0.0, "avg_logprob": -0.09419937831599538, "compression_ratio": 1.6205128205128205, "no_speech_prob": 1.6964093447313644e-05}, {"id": 1217, "seek": 632592, "start": 6325.92, "end": 6334.4, "text": " a matrix multiplication and a ReLU or softmax to get our word three output.", "tokens": [257, 8141, 27290, 293, 257, 1300, 43, 52, 420, 2787, 41167, 281, 483, 527, 1349, 1045, 5598, 13], "temperature": 0.0, "avg_logprob": -0.20089424283880936, "compression_ratio": 1.4512820512820512, "no_speech_prob": 8.267737030109856e-06}, {"id": 1218, "seek": 632592, "start": 6334.4, "end": 6340.12, "text": " And so this is called the single fully connected model, so this is a very simplistic just how", "tokens": [400, 370, 341, 307, 1219, 264, 2167, 4498, 4582, 2316, 11, 370, 341, 307, 257, 588, 44199, 445, 577], "temperature": 0.0, "avg_logprob": -0.20089424283880936, "compression_ratio": 1.4512820512820512, "no_speech_prob": 8.267737030109856e-06}, {"id": 1219, "seek": 632592, "start": 6340.12, "end": 6344.92, "text": " would you combine two words to get a third word.", "tokens": [576, 291, 10432, 732, 2283, 281, 483, 257, 2636, 1349, 13], "temperature": 0.0, "avg_logprob": -0.20089424283880936, "compression_ratio": 1.4512820512820512, "no_speech_prob": 8.267737030109856e-06}, {"id": 1220, "seek": 632592, "start": 6344.92, "end": 6348.08, "text": " We'll see what that looks like in code.", "tokens": [492, 603, 536, 437, 300, 1542, 411, 294, 3089, 13], "temperature": 0.0, "avg_logprob": -0.20089424283880936, "compression_ratio": 1.4512820512820512, "no_speech_prob": 8.267737030109856e-06}, {"id": 1221, "seek": 632592, "start": 6348.08, "end": 6353.08, "text": " Go back to the notebook.", "tokens": [1037, 646, 281, 264, 21060, 13], "temperature": 0.0, "avg_logprob": -0.20089424283880936, "compression_ratio": 1.4512820512820512, "no_speech_prob": 8.267737030109856e-06}, {"id": 1222, "seek": 635308, "start": 6353.08, "end": 6358.12, "text": " So here I've put the same name so you can kind of see how the network sections correspond", "tokens": [407, 510, 286, 600, 829, 264, 912, 1315, 370, 291, 393, 733, 295, 536, 577, 264, 3209, 10863, 6805], "temperature": 0.0, "avg_logprob": -0.13888667465804458, "compression_ratio": 1.5053191489361701, "no_speech_prob": 4.1983566916314885e-05}, {"id": 1223, "seek": 635308, "start": 6358.12, "end": 6359.12, "text": " with the slides.", "tokens": [365, 264, 9788, 13], "temperature": 0.0, "avg_logprob": -0.13888667465804458, "compression_ratio": 1.5053191489361701, "no_speech_prob": 4.1983566916314885e-05}, {"id": 1224, "seek": 635308, "start": 6359.12, "end": 6366.72, "text": " And I can add these slides to the GitHub, I don't think I have them in there yet.", "tokens": [400, 286, 393, 909, 613, 9788, 281, 264, 23331, 11, 286, 500, 380, 519, 286, 362, 552, 294, 456, 1939, 13], "temperature": 0.0, "avg_logprob": -0.13888667465804458, "compression_ratio": 1.5053191489361701, "no_speech_prob": 4.1983566916314885e-05}, {"id": 1225, "seek": 635308, "start": 6366.72, "end": 6375.0, "text": " So what we're going to do is define loss, we're interested in cross entropy loss and", "tokens": [407, 437, 321, 434, 516, 281, 360, 307, 6964, 4470, 11, 321, 434, 3102, 294, 3278, 30867, 4470, 293], "temperature": 0.0, "avg_logprob": -0.13888667465804458, "compression_ratio": 1.5053191489361701, "no_speech_prob": 4.1983566916314885e-05}, {"id": 1226, "seek": 635308, "start": 6375.0, "end": 6377.12, "text": " accuracy.", "tokens": [14170, 13], "temperature": 0.0, "avg_logprob": -0.13888667465804458, "compression_ratio": 1.5053191489361701, "no_speech_prob": 4.1983566916314885e-05}, {"id": 1227, "seek": 637712, "start": 6377.12, "end": 6385.48, "text": " Here the naming convention is I refers to input, H to hidden, so I underscore H is the", "tokens": [1692, 264, 25290, 10286, 307, 286, 14942, 281, 4846, 11, 389, 281, 7633, 11, 370, 286, 37556, 389, 307, 264], "temperature": 0.0, "avg_logprob": -0.12932834406008667, "compression_ratio": 1.8146067415730338, "no_speech_prob": 1.1842677849926986e-05}, {"id": 1228, "seek": 637712, "start": 6385.48, "end": 6392.32, "text": " input to hidden layer, H underscore H is the hidden to hidden layer, H underscore O hidden", "tokens": [4846, 281, 7633, 4583, 11, 389, 37556, 389, 307, 264, 7633, 281, 7633, 4583, 11, 389, 37556, 422, 7633], "temperature": 0.0, "avg_logprob": -0.12932834406008667, "compression_ratio": 1.8146067415730338, "no_speech_prob": 1.1842677849926986e-05}, {"id": 1229, "seek": 637712, "start": 6392.32, "end": 6395.96, "text": " to output, and BN stands for batch norm.", "tokens": [281, 5598, 11, 293, 363, 45, 7382, 337, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.12932834406008667, "compression_ratio": 1.8146067415730338, "no_speech_prob": 1.1842677849926986e-05}, {"id": 1230, "seek": 637712, "start": 6395.96, "end": 6403.5, "text": " And so we're going to make this neural net that uses an embedding to go from an input", "tokens": [400, 370, 321, 434, 516, 281, 652, 341, 18161, 2533, 300, 4960, 364, 12240, 3584, 281, 352, 490, 364, 4846], "temperature": 0.0, "avg_logprob": -0.12932834406008667, "compression_ratio": 1.8146067415730338, "no_speech_prob": 1.1842677849926986e-05}, {"id": 1231, "seek": 637712, "start": 6403.5, "end": 6405.44, "text": " to a hidden layer.", "tokens": [281, 257, 7633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.12932834406008667, "compression_ratio": 1.8146067415730338, "no_speech_prob": 1.1842677849926986e-05}, {"id": 1232, "seek": 640544, "start": 6405.44, "end": 6417.0, "text": " We'll have just two linear layers and then we'll also have a batch norm.", "tokens": [492, 603, 362, 445, 732, 8213, 7914, 293, 550, 321, 603, 611, 362, 257, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.11599455041400457, "compression_ratio": 1.3819444444444444, "no_speech_prob": 1.9221937691327184e-05}, {"id": 1233, "seek": 640544, "start": 6417.0, "end": 6423.4, "text": " And so to take one forward step, we start with the input to hidden, which is an embedding,", "tokens": [400, 370, 281, 747, 472, 2128, 1823, 11, 321, 722, 365, 264, 4846, 281, 7633, 11, 597, 307, 364, 12240, 3584, 11], "temperature": 0.0, "avg_logprob": -0.11599455041400457, "compression_ratio": 1.3819444444444444, "no_speech_prob": 1.9221937691327184e-05}, {"id": 1234, "seek": 640544, "start": 6423.4, "end": 6427.839999999999, "text": " do a ReLU on that and a batch norm.", "tokens": [360, 257, 1300, 43, 52, 322, 300, 293, 257, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.11599455041400457, "compression_ratio": 1.3819444444444444, "no_speech_prob": 1.9221937691327184e-05}, {"id": 1235, "seek": 642784, "start": 6427.84, "end": 6437.400000000001, "text": " And then if this is, we're just ensuring that this has a length of three, so if we have", "tokens": [400, 550, 498, 341, 307, 11, 321, 434, 445, 16882, 300, 341, 575, 257, 4641, 295, 1045, 11, 370, 498, 321, 362], "temperature": 0.0, "avg_logprob": -0.12345733642578124, "compression_ratio": 1.5612903225806452, "no_speech_prob": 1.0615903192956466e-05}, {"id": 1236, "seek": 642784, "start": 6437.400000000001, "end": 6443.400000000001, "text": " at least kind of our word one, this is the part where we want to input word two, we do", "tokens": [412, 1935, 733, 295, 527, 1349, 472, 11, 341, 307, 264, 644, 689, 321, 528, 281, 4846, 1349, 732, 11, 321, 360], "temperature": 0.0, "avg_logprob": -0.12345733642578124, "compression_ratio": 1.5612903225806452, "no_speech_prob": 1.0615903192956466e-05}, {"id": 1237, "seek": 642784, "start": 6443.400000000001, "end": 6452.4400000000005, "text": " that by H plus, now we're going to use input to hidden on word two.", "tokens": [300, 538, 389, 1804, 11, 586, 321, 434, 516, 281, 764, 4846, 281, 7633, 322, 1349, 732, 13], "temperature": 0.0, "avg_logprob": -0.12345733642578124, "compression_ratio": 1.5612903225806452, "no_speech_prob": 1.0615903192956466e-05}, {"id": 1238, "seek": 645244, "start": 6452.44, "end": 6460.599999999999, "text": " So here X zero you can think of as word one, this is word two, is now being added to what", "tokens": [407, 510, 1783, 4018, 291, 393, 519, 295, 382, 1349, 472, 11, 341, 307, 1349, 732, 11, 307, 586, 885, 3869, 281, 437], "temperature": 0.0, "avg_logprob": -0.10794373353322347, "compression_ratio": 1.5, "no_speech_prob": 1.6964149835985154e-05}, {"id": 1239, "seek": 645244, "start": 6460.599999999999, "end": 6464.04, "text": " we got from that calculation with word one.", "tokens": [321, 658, 490, 300, 17108, 365, 1349, 472, 13], "temperature": 0.0, "avg_logprob": -0.10794373353322347, "compression_ratio": 1.5, "no_speech_prob": 1.6964149835985154e-05}, {"id": 1240, "seek": 645244, "start": 6464.04, "end": 6469.32, "text": " We'll do our hidden to hidden and ReLU and batch norm.", "tokens": [492, 603, 360, 527, 7633, 281, 7633, 293, 1300, 43, 52, 293, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.10794373353322347, "compression_ratio": 1.5, "no_speech_prob": 1.6964149835985154e-05}, {"id": 1241, "seek": 645244, "start": 6469.32, "end": 6475.46, "text": " If we have a word three, then we can add that as well.", "tokens": [759, 321, 362, 257, 1349, 1045, 11, 550, 321, 393, 909, 300, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.10794373353322347, "compression_ratio": 1.5, "no_speech_prob": 1.6964149835985154e-05}, {"id": 1242, "seek": 647546, "start": 6475.46, "end": 6484.24, "text": " So we're kind of just applying this transformation to each new word and adding that to kind of", "tokens": [407, 321, 434, 733, 295, 445, 9275, 341, 9887, 281, 1184, 777, 1349, 293, 5127, 300, 281, 733, 295], "temperature": 0.0, "avg_logprob": -0.18900417794986646, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.766851013002452e-06}, {"id": 1243, "seek": 647546, "start": 6484.24, "end": 6485.76, "text": " what we got from the previous one.", "tokens": [437, 321, 658, 490, 264, 3894, 472, 13], "temperature": 0.0, "avg_logprob": -0.18900417794986646, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.766851013002452e-06}, {"id": 1244, "seek": 647546, "start": 6485.76, "end": 6487.32, "text": " Let me go back to the slide.", "tokens": [961, 385, 352, 646, 281, 264, 4137, 13], "temperature": 0.0, "avg_logprob": -0.18900417794986646, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.766851013002452e-06}, {"id": 1245, "seek": 647546, "start": 6487.32, "end": 6493.2, "text": " Oh, and we're at one o'clock.", "tokens": [876, 11, 293, 321, 434, 412, 472, 277, 6, 9023, 13], "temperature": 0.0, "avg_logprob": -0.18900417794986646, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.766851013002452e-06}, {"id": 1246, "seek": 647546, "start": 6493.2, "end": 6497.68, "text": " So this is, that example was kind of showing predicting word four using words one, two,", "tokens": [407, 341, 307, 11, 300, 1365, 390, 733, 295, 4099, 32884, 1349, 1451, 1228, 2283, 472, 11, 732, 11], "temperature": 0.0, "avg_logprob": -0.18900417794986646, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.766851013002452e-06}, {"id": 1247, "seek": 647546, "start": 6497.68, "end": 6498.68, "text": " and three.", "tokens": [293, 1045, 13], "temperature": 0.0, "avg_logprob": -0.18900417794986646, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.766851013002452e-06}, {"id": 1248, "seek": 647546, "start": 6498.68, "end": 6501.56, "text": " And so, well, we'll come back to this next time.", "tokens": [400, 370, 11, 731, 11, 321, 603, 808, 646, 281, 341, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.18900417794986646, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.766851013002452e-06}, {"id": 1249, "seek": 650156, "start": 6501.56, "end": 6508.080000000001, "text": " The key thing to note, if you look at these diagrams, is that the color of the arrow refers", "tokens": [440, 2141, 551, 281, 3637, 11, 498, 291, 574, 412, 613, 36709, 11, 307, 300, 264, 2017, 295, 264, 11610, 14942], "temperature": 0.0, "avg_logprob": -0.25006175863331764, "compression_ratio": 1.6010928961748634, "no_speech_prob": 2.33206856137258e-06}, {"id": 1250, "seek": 650156, "start": 6508.080000000001, "end": 6511.160000000001, "text": " to kind of a single matrix of weights.", "tokens": [281, 733, 295, 257, 2167, 8141, 295, 17443, 13], "temperature": 0.0, "avg_logprob": -0.25006175863331764, "compression_ratio": 1.6010928961748634, "no_speech_prob": 2.33206856137258e-06}, {"id": 1251, "seek": 650156, "start": 6511.160000000001, "end": 6517.84, "text": " And so here, word one is going to be multiplied by the same matrix of weights as word two", "tokens": [400, 370, 510, 11, 1349, 472, 307, 516, 281, 312, 17207, 538, 264, 912, 8141, 295, 17443, 382, 1349, 732], "temperature": 0.0, "avg_logprob": -0.25006175863331764, "compression_ratio": 1.6010928961748634, "no_speech_prob": 2.33206856137258e-06}, {"id": 1252, "seek": 650156, "start": 6517.84, "end": 6521.4400000000005, "text": " will be later on, as word three will be later on.", "tokens": [486, 312, 1780, 322, 11, 382, 1349, 1045, 486, 312, 1780, 322, 13], "temperature": 0.0, "avg_logprob": -0.25006175863331764, "compression_ratio": 1.6010928961748634, "no_speech_prob": 2.33206856137258e-06}, {"id": 1253, "seek": 650156, "start": 6521.4400000000005, "end": 6523.4400000000005, "text": " Yes, Jeremy?", "tokens": [1079, 11, 17809, 30], "temperature": 0.0, "avg_logprob": -0.25006175863331764, "compression_ratio": 1.6010928961748634, "no_speech_prob": 2.33206856137258e-06}, {"id": 1254, "seek": 650156, "start": 6523.4400000000005, "end": 6524.8, "text": " Back?", "tokens": [5833, 30], "temperature": 0.0, "avg_logprob": -0.25006175863331764, "compression_ratio": 1.6010928961748634, "no_speech_prob": 2.33206856137258e-06}, {"id": 1255, "seek": 650156, "start": 6524.8, "end": 6526.200000000001, "text": " Oh.", "tokens": [876, 13], "temperature": 0.0, "avg_logprob": -0.25006175863331764, "compression_ratio": 1.6010928961748634, "no_speech_prob": 2.33206856137258e-06}, {"id": 1256, "seek": 652620, "start": 6526.2, "end": 6531.92, "text": " I just realized on the previous slides, that's not true.", "tokens": [286, 445, 5334, 322, 264, 3894, 9788, 11, 300, 311, 406, 2074, 13], "temperature": 0.0, "avg_logprob": -0.20629569080388435, "compression_ratio": 1.8242677824267783, "no_speech_prob": 4.637853635358624e-06}, {"id": 1257, "seek": 652620, "start": 6531.92, "end": 6537.28, "text": " The previous slides all use the same color, but they're not all the same weight matrix.", "tokens": [440, 3894, 9788, 439, 764, 264, 912, 2017, 11, 457, 436, 434, 406, 439, 264, 912, 3364, 8141, 13], "temperature": 0.0, "avg_logprob": -0.20629569080388435, "compression_ratio": 1.8242677824267783, "no_speech_prob": 4.637853635358624e-06}, {"id": 1258, "seek": 652620, "start": 6537.28, "end": 6542.08, "text": " So just wanted to clarify, they're only the same weight matrix on the slides where they", "tokens": [407, 445, 1415, 281, 17594, 11, 436, 434, 787, 264, 912, 3364, 8141, 322, 264, 9788, 689, 436], "temperature": 0.0, "avg_logprob": -0.20629569080388435, "compression_ratio": 1.8242677824267783, "no_speech_prob": 4.637853635358624e-06}, {"id": 1259, "seek": 652620, "start": 6542.08, "end": 6544.0, "text": " actually have different colors.", "tokens": [767, 362, 819, 4577, 13], "temperature": 0.0, "avg_logprob": -0.20629569080388435, "compression_ratio": 1.8242677824267783, "no_speech_prob": 4.637853635358624e-06}, {"id": 1260, "seek": 652620, "start": 6544.0, "end": 6548.679999999999, "text": " And I will update these previous slides to have the colors, although later ones, where", "tokens": [400, 286, 486, 5623, 613, 3894, 9788, 281, 362, 264, 4577, 11, 4878, 1780, 2306, 11, 689], "temperature": 0.0, "avg_logprob": -0.20629569080388435, "compression_ratio": 1.8242677824267783, "no_speech_prob": 4.637853635358624e-06}, {"id": 1261, "seek": 652620, "start": 6548.679999999999, "end": 6552.36, "text": " it gets interesting, we do have this color coding.", "tokens": [309, 2170, 1880, 11, 321, 360, 362, 341, 2017, 17720, 13], "temperature": 0.0, "avg_logprob": -0.20629569080388435, "compression_ratio": 1.8242677824267783, "no_speech_prob": 4.637853635358624e-06}, {"id": 1262, "seek": 652620, "start": 6552.36, "end": 6553.96, "text": " But we'll revisit this next time.", "tokens": [583, 321, 603, 32676, 341, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.20629569080388435, "compression_ratio": 1.8242677824267783, "no_speech_prob": 4.637853635358624e-06}, {"id": 1263, "seek": 655396, "start": 6553.96, "end": 6556.56, "text": " This is kind of just a first pass.", "tokens": [639, 307, 733, 295, 445, 257, 700, 1320, 13], "temperature": 0.0, "avg_logprob": -0.2835717995961507, "compression_ratio": 0.9452054794520548, "no_speech_prob": 7.361555617535487e-05}, {"id": 1264, "seek": 655656, "start": 6556.56, "end": 6584.56, "text": " But yes, I'll see you on Thursday.", "tokens": [50364, 583, 2086, 11, 286, 603, 536, 291, 322, 10383, 13, 51764], "temperature": 0.0, "avg_logprob": -0.3551893967848558, "compression_ratio": 0.8095238095238095, "no_speech_prob": 3.9359401853289455e-05}], "language": "en"}