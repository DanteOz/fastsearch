{"text": " Okay, hi everybody, thanks for joining. This is an entirely optional presentation which I'll call a Lesson 0, which is all about how to fast AI. It's all about how to get the most out of this course, how to make sure you finish it, and how to make sure you feel like it's been a productive time. And the reason I'm doing this is because a lot of people who take the course, when they get to the end of it, they say to me, oh, it wasn't until I got to the end of the course that I realized how I should have done the whole course, so now I'm going to go back and redo the whole thing over again. And so I'm going to tell you about what the messages I've heard are, about what people have found the best approaches to making the course work. I'm also going to go through the actual mechanics of how to get set up with two systems, Google Colab and AWS EC2, and I'll talk about why you might use one versus the other. So a lot of people now, as in many hundreds of thousands, have gone through the Fast AI Practical Deep Learning for Coders course, and many, many, many of them have gone on to create successful startups, to write research papers with high impact factors, to create new products at their companies. It's a pretty well proven course at this time, but there's also a lot of people that never finish the course. And so if you're watching this, it's because you've decided you do want to learn deep learning. So I'm going to talk a bit about what's it going to take for you to be one of the people that makes this into a great experience. When I talk about the course, I'm also talking about the book. So just to be clear, there's a book that Sylvia Guzhera and I wrote, which you can either buy from Amazon, and people like it happily, or believe it or not, you can read the whole thing for free. So it's called Fastbook. It's a fast book repo. Honestly, I make basically nothing from the book, so don't feel like you need to buy it to say thank you or something. Buy it if you want the book. If you're happy using notebooks, use the free one. It's all good. So the book was actually written as Jupyter Notebooks, and we wrote something to turn it into a book book. The book also, by the way, actually looks great on Kindle, online, as well as paper. I know often technical books don't. This one actually does. And then the course goes through half of the book. And so quite soon we'll do a part two, which will go through the other half of the book plus some other new stuff. Basically each lesson covers a chapter or so of the book. So if you're doing this course, you'll be going through the book, at least in the notebooks, and you might want the paper one as well. So here is the main thing that you should commit to right now, which is to finish the damn course, right, or finish at least half of the book. Because everybody, I think, who joins comes in thinking, okay, I'm going to do this. I'm going to do deep learning. But if you, when I look at our YouTube analytics, a lot of people don't finish. Okay, so you just need to decide what day are you going to watch the course each week? What day are you going to do the assignment? What day, like how are you going to structure your time to finish the course? And maybe you're coming in deciding, I don't want to finish it, which is fine, right? Maybe, you know, if that's your intention upfront, no problem. But if your intention is to be a really effective deep learning practitioner, you need to finish the damn course. Okay, so put it in your head that that's your goal. Talk to your friends or your spouse and tell them that's my goal. Get that social pressure that you're going to finish it. You're not just going to finish the course, but try to finish a project. So Christine McLevy is one of our fantastic alumni. She's now at OpenAI, one of the world's top research organizations. She built a fantastic system for creating new music with deep learning. She used to be a pianist herself. And I remember this discussion, I told her focus on making one project great and polishing it off and finishing it. And she did. And that project has ended up creating music which the BBC Orchestra played. And amongst other things helped her get this extremely exclusive job at OpenAI. So this is a clip from a podcast with one of our students, Sanyam, and Christine, in which Christine is saying this is one of her key insights. And so I'm going to be giving you a few key insights, some of which are from me or some of them are from me via students. But they're all like things I've heard a bunch of times. And so this is one example. So finish the course and finish a project. The project doesn't have to be something no one's ever built before. Maybe it's just like, oh, I really love that thing that person built. Gosh, it would be a real stretch if I could build it too. You know, great. Or it doesn't have to be world changing. You know, so one of our students built something for his fiance, which was a cousin recognizer. He had, I think, 14 cousins. And so his fiance could look at could take a picture of one of the cousins and it would tell them which cousin it was. Right. In our first course, one of our students built the app for the Silicon Valley TV show, which did Hot Dog or Not Hot Dog, which was actually a huge smash hit, like millions of downloads that it was written about in the media. And it did exactly one thing just to tell you whether or not something was a hot dog. Anyway, or it could, you know, solve medicine. That would be fine too. I mean, whatever. So finishing the course means being tenacious. And one of the things I hear a lot is a lot of the approaches people learn as they do Fast AI around how to learn and how to study are useful more generally. And in fact, this is a quote from our book, the number one thing I see the difference between successful deep learning practitioners and not is tenacity. Okay. And tenacity is on the whole something you can choose. Now something you can't choose is whether you find yourself in the middle of a global pandemic or, you know, somebody in your family dies or you come down with a terrible cold or whatever like obstacles happen. Right. And so part of being tenacious is being understanding with yourself. Right. And saying, okay, something's happened. I can't do what I hope to do right now. But then getting back to it. Right. So tenacity is not about ignoring the bumps, but keeping going after the bumps. And maybe that's, you know, quite often I'll have a bump that's like a year long. Right. But if I've decided to finish something, you know, at the end of that year, I'll go back and finish it. So sometimes that involves me emailing somebody more than a year after they've sent me something and saying, okay, I'm ready to reply now. And they forgot that they even sent me an email. Okay. So what I'm going to do now is I'm going to share with you a bunch of insights from this book called Meta Learning. If you haven't seen it before, that's okay. It came out yesterday and it was written by a guy called Redek, who is one of the top alumni of this course. And it's a book well worth reading because his journey is extraordinary. You know, this is a guy without a degree who couldn't code just a few years ago with a job that he found boring. And he set out to learn deep learning and repeatedly failed to do so. But Redek is extremely tenacious and each time he failed to do so, he tried again. And eventually he figured out a way to do it. And the way he did it was very intensely based on fast AI, both the course and the philosophy of learning. And he is now a Kaggle competition winner. He was the only non San Francisco person at QAI, which is one of the world's top medical AI startups. And now he works at a new nonprofit that is literally trying to translate animal language. So he's kind of a good example. I always think it's a good idea to have a role model. In the fast AI community, there's a lot of role models. And so here's somebody who's like both a role model for like trying, failing, trying, failing, trying, failing, and then, you know, finding some success. And so I'm going to show you some things from his book. And a lot of his book is him taking stuff I say and kind of casting it into what he took away from it. Some of it's his ideas. So one of the things we hear again and again from unsuccessful deep learning students is they keep preparing to do deep learning and they keep preparing to do projects. So they study linear algebra. They study calculus. They study C++. They study all these different things. They do a MOOC and then another MOOC and then they read a book and then another book. You know, and at what point are they actually going to start doing something? So the fast AI philosophy is you start doing something week one. OK, so week one, you need to actually train a model. OK, which is not to say that you're not going to learn theory. You will, right, as needed in the context of getting stuff done. OK. So if you do finish it, right, particularly if you finish the full two parts of the course, right, you'll have implemented basically all of fast AI's library just about from scratch. You'll know all about batch normalization. You'll have benchmarked various matrix multiplication approaches. You'll know how to write bare metal GPU optimized code. You'll understand how to do back propagation and calculus of that from scratch. You'll do all of that. OK, but it will all be as you go along in the context of like solving a particular problem or understanding the next piece of the puzzle. So yeah, really just reading books and watching videos is not going to get you there. The thing which is going to get you there is writing code, doing experiments and training models. Some of you might not be that great at coding. Fine. OK, that's that's a perfectly OK place to be. And but you guys are going to find it the most challenging because being good at coding is the thing that lets you zip through quickly. So rather than think, oh, that's a shame. I'm not that good at coding yet. This is actually an opportunity because now you have a really fun project to learn to code in. So a lot of people have become good coders by doing the course, because as you do the course, you'll learn about a lot of computer science concepts like object oriented programming and functional programming and mapping over a list and list comprehensions and GPU acceleration and so on and so forth. So the thing is, though, if you're not if you come across a computer science concept or a programming idea or a piece of syntax that you're not that familiar with, that's a place it's worth pausing for a moment and making sure that you know that you do understand how that code works, because the code coding is the kind of critical foundational skill. This is a pretty good course for getting started with basic computer science. It's Harvard CS50 course, which everybody at Harvard does for computer science to get started, and that's all available for free online. So I would recommend, well, and so would Radek, start there. And so these quotes are all from Radek's book, by the way. And then the other piece, so Radek talks about this four legged table of the things that are going to help you do your deep learning experiments more effectively and efficiently. And these are the ideas, like knowing the basic ideas around code, knowing your tools. So an editor, Jupyter notebook, knowing stuff like Git, like how to save your work and pull in other people's work and so forth, and understanding kind of SSH and Linux, like how to access a server and manipulate it and do stuff with it. So there's this great course called The Missing Semester of Your CS Education, which was actually created, I believe, by students at MIT who said, oh, everybody at MIT is assuming we already know this stuff, but a lot of us don't. So there's nothing to be ashamed of if you've never used Git or you've never used SSH, you know, or whatever. They're just tools which at some point in the journey, most people will just kind of have to figure out. So this is actually a great time to do it. And this is a great course to use to help you get there. And of course, again, the main thing is to practice these tools. So that's the kind of foundation around coding and your kind of development environment. The next big piece of advice, which we talk about a lot in the course and that Radek talks about in his book, is sharing your work, communicating your work, and writing about your work. This is something that a lot of people feel very uncomfortable, like tweeting or blogging or whatever, right? It's like, who the hell am I to start writing about deep learning? I've just started. Well, here's the thing. No one is better placed than you to write for like, what would you have wanted to know six months ago? All right, so you now know more than you did six months ago, and you'll know more in a week and more in a week, more in a week. And so if you've got a background in, say, the hospitality industry, you know, you could probably write something very interesting for your colleagues in the hospitality industry about ideas around around deep learning, for example. Or if you teach in high school, you know, you might have ideas that you could write down about what high school students might find interesting or teachers might find interesting. So you know, everybody's got something to say. And the key thing is to write it down because that is going to help embed your understanding a lot better, and it's going to start to build up your portfolio. And so we'll talk more about that in a moment. But a lot of people have found that this message of sharing their work has been a critical part of their journey of learning and of also building up their personal brand that has ended up getting them a job. OK, so what does it mean to do a Fast AI lesson? So a Fast AI lesson is basically a chapter of the book or one video from the course. Or both. So what does it mean to to do one of these lessons? Assuming you're doing the video, then it means, OK, obviously watching the video. So there's a couple of hours. Right. And then it means running the notebook, which we'll look at in a moment. When you run the notebook, you have the whole book with all of its code and all of its outputs there. You're playing with it. You should experiment. Right. You should you should try things out. So if you wonder, oh, why is this done before that? Well try removing it. Try doing it in a different order. If you're wondering, you know, what would happen if I did that, but this to this other image? Try it. Right. The more you can start to experiment, the more you're feeding your brain with these kind of like your own deep learning happening in your brain. Input output patterns. You try something. What happens? You try something. What happens? So after that, the next step is to try to reproduce the notebook from scratch. OK, now you're going to have to look things up, obviously. But the idea is, can you with a fresh new notebook, can you can you go back and recreate some of those models, retrain them or redo some of that data processing pipeline. So try to like type it in yourself. You know, you can switch back to the answer as much as you like, but you're really trying to start to actually fill in your own write your own code. And then what you really the point you really want to get to is repeating some parts of the lesson with a different data set which you collect or download. Now this whole process often takes people a number of times through the course. Right. So often the first time through, people might just watch each lecture and try to kind of run it and just get to the end to get a kind of a general sense of what's going on. So people often kind of go through the whole thing like three times and then come back and try to go further and further. So don't worry if you can't do all this right away. Certainly in lesson one, that's going to be challenging. Just take it as far as you can. Right. And as you go along, try to push yourself to do more and more. And you could even go back to an earlier notebook and see if you can understand more and more of it. So let's take a look at what that looks like. So here's the course. Okay. And here's the lessons which you can watch. And then here are the places you can run the notebooks. So there's two types of platform for running the notebooks. There are notebook servers. These are things that as soon as you click into it, the actual environment we use, Jupyter Notebook, will pop up and you can just start running it pretty much straight away. So that is obviously the easiest. Colab is free. Gradient has a free tier and SageMaker is not free. So we're going to look at Colab today. The other option is to use a full Linux server and this is something where you're going to have to basically set up Linux and install the Python system and install notebooks and get the code from GitHub and run the server and log into SSH and do all that. That's obviously a lot more work. You might want to skip it for now in like lesson one. But I would recommend at some point you go through this path. And the reason why is that in real life at your workplace or if you do your own startup or whatever, this is what you'll be doing. You will be interacting with a Linux server using SSH that's running a GPU and you'll want to understand how it all works. And once you're using your own Linux server, you'll suddenly learn about all these productivity enhancing tips and tools that make your life easier. So I'll be showing how to set up AWS EC2. That's the Amazon platform today. You'll find Google Cloud looks very, very similar indeed. Jarvis Labs was created by a fast AI alum and this is probably at this stage the best value of the full Linux servers. So that would certainly be also very much worth checking out. One good thing about AWS, so a couple of things. AWS is currently the most popular platform for cloud computing. So it's very likely that whatever company you're at or end up at is already using it. They're also pretty generous with credits for startups and students. So even though it can set you back 60 or 70 cents an hour, you might well find you can get a few hundred dollars worth of credits through your school or even a few thousand dollars worth of credits through their startup programs and so forth. So let's have a look at what Colab looks like. So Colab is, it's wonderful how easy it is to get started. You literally just click on the chapter. So let's do chapter one. And it pops up Colab. You can pay, I think it's $10 a month for Colab Pro to get like longer sessions and more likely that you'll get a better GPU. But for most people you'll find the free version is totally fine. One of the biggest problems with Colab is that it's not persistent, which is to say when I go to this notebook, it thinks it's never seen me before. Nothing's set up for me the way I want it. But we've set up the notebook so that the very first cell actually installs everything you need. So if I click this little Run Cell button here, it will run the cell. Although what I will do is I'm going to pop over to Colab here and let's also read the steps here. And actually it says here before running anything you should tell Colab you're interested in using a GPU. So if you find that when you run a cell in the, from the course and it's going to take like half an hour or an hour or more, it's very likely you forgot to use GPU. The GPU runs things many hundreds of times faster. So all you do as it says here is go runtime, change runtime type and say GPU. Okay, so now I can run this cell. And this is all Python code except lines that start with an explanation mark are actually sent to a terminal. Okay, so pip is something that installs Python software and Fastbook contains all of the Python software necessarily necessary for the course. And so it's going to go away and set it all up. And so this is this like mildly annoying bit. You can then connect Colab to Google Drive and that's going to be how you can save your notebooks and save your work as you go. Okay, I'm not going to do that right now. But if you go to this link that it says and it'll give you a code and then that'll connect it up to your Google Drive. And so at this point now everything from the for the course is now available. And you can see the whole book is here. Okay, so here's the book and you can open up sections to read them. Okay, you can go to the table of contents. And so eventually we'll get to this cell here which contains all the code needed to run a model. So if I click run, here's the way it goes. Now this is going to it's amazing how much this little bit of code is going to do. It's going to download tens of thousands of pictures of dogs and cats. It's going to use a simple rule to recognize the dogs from the cats based on their file names. Basically the way that this this has been set up is that you can tell from the file name whether it's a dog or a cat. It's then going to download something called a pre-trained model which is something that already knows how to recognize various types of images. It's then going to construct it's going to then going to train that model to make it particularly good at recognizing dogs from cats and then it's going to validate that model to see how good it is at recognizing dogs from cats using a set of pictures that it hasn't seen before. And that's all happening. So so far it's already downloaded the data set. It's already downloaded the pre-trained model and it's now busily going through the first epoch which is to look at every picture once to try to learn how to recognize dogs from cats. And that's it. The lines starting with a hash are just comments. Because this is also the source of an actual book there's a few like slightly weird comments that you can ignore. Just things that are used for setting up references in the book. There's the caption so forth. Okay so it's now testing out I think that first epoch. Okay so it's finished an epoch and so far it's got a 1% error rate. So after 54 seconds it has learnt to recognize dogs from cats with 99% accuracy. And so yeah we're going to let that finish off. So that's how we get started with Colab. Okay and there's nothing else to set up. Now what you can do is you can open Notebook and you can open a notebook from GitHub. And here is the Fastbook repository. And you'll see in the Fastbook repository for every notebook there's a second copy inside the clean folder with the same name. So I was just looking at 01intro. There's also a clean 01intro. If I open that up you'll see that it's got exactly the same thing as the last one I was just looking at but all the pros is now missing. It's just got headings and code. Also all the outputs are missing. So the reason that we have this clean version is to help you with these stages here is our suggestion is once you've gone through the lesson and you've run the notebook and you feel like okay I think I get it is you open up this clean version and before you run each cell try to think okay why is this cell here? What's it for? What's it going to do? What's the output going to look like? So once you remove all that context this is a good test for you to kind of get your brain going to think what was actually going on. So this is a kind of much more active approach to reading and recall. And so then once you've done that and you've finished going through this at the bottom one thing that is kept is the questionnaire. So at the end of every chapter is a questionnaire and so then at this point you should now as much as you can without looking go through and try to answer each of those questions. They all have answers in the notebook, in the book, okay so you can you know if you can't remember you can always look it up but you know if you can't remember that's a sign to you that like oh you know did I skip over that bit too quickly like what what's happened that I've not remembered and then try to remind yourself and then go back and yeah finish the questionnaire. Okay so there's a lot of pieces to help take this from a passive I'm just watching a video I'm just reading a book into a participatory exercise that you're a part of. So as soon as you can we want you to create something that's yours and so this is the easiest way to do that is basically at the end of lesson one once you kind of up and running try to do it with your own data set. And if you go to forums.fast.ai which is something that you're going to want to be deeply familiar with because this is going to be full of people just like you. The people who want to learn deep learning okay and these people are all asking questions and making comments and you can see there's like a lot going on all the time and so you can see here's the part one course topic. And you can see there's 1.4 thousand topics there and each one is going to have lots and lots of replies. So this is where amongst other things you'll find if you search for it something called share your work here which has 2,000 replies and you can see links to and pictures of lots of examples of things that other people have done after the first week or two of the course and so hopefully that might help give you some inspiration. And it would be great if you could reply and add a picture or a link to what you build and you'll see you know everybody is very positive to each other on the forums in general and in this topic in particular. Nobody's going to go oh my god I could have done that years ago right. People are going to be excited for you that you have now joined the ranks of people that have built their first deep learning model and I will be excited for you. So as I said Radek this is again from his book expresses in his book a way of not doing fast AI which I have heard now probably hundreds of times. I don't know why this is so common but many many people do what Radek did which was basically to learn all these math things right. So he started with calculus and then once he got to a certain point in calculus he found that he had to start understanding real analysis and then as he started understanding real analysis he had found he had to learn set theory you know and you get the idea right. If you want to learn all of math that's going to take a while. There's a lot of gatekeeping out there that says like oh if you're going to be a real deep learning practitioner you have to finish you know a graduate level course in linear algebra. Here's the truth the actual linear algebra you do in in basically all deep learning is matrix multiplication and if you've forgotten what that is that is multiplying things together and then adding them up. So what you need to be able to do is multiply things together and add them up. So if you can do that you're good to go. So yeah don't get you know you're not going to finish it if a you never start it because you keep preparing or B you keep thinking oh I wonder exactly what's happening here and you go all the way down to the bottom until you found yourself in the midst of set theory. Don't worry you'll get deeper and deeper over time but if you're learning mathematical theory you're not coding you're not experimenting you're not practicing you're not actually building deep learning models and if you're watching this course and your goal is not to build deep learning models you're in the wrong course. And if your goal is to build deep learning models then don't do this. So as Radek says here it's as you train actual models that you're going to get feedback right and the feedback that a lot of people get is oh my god I can already train useful models like a lot of people are surprised at how early on they can actually get astonishingly good results. Okay so you know jump in and be open to surprising yourself that you can do a bit more than you thought. You can't do everything right away okay but start that feedback loop of figuring out what do you know what can you do what can you get working what can't you get working. So one of the key things that you're going to need to do if you're going to finish all of the course is become an even better developer than you are now even better coder than you are now wherever you're up to and so to do this you need to read code and write code. The fast AI source code is designed to be extremely readable so you can read that code you can obviously read the code in the notebooks but yeah you want to be spending as much time as possible reading and writing code and particularly reading and writing deep learning code. All right how do you find out what's going on in the world of deep learning and how do you get yourself on the map of people doing deep learning. Probably the best answer is Twitter. For those of you whose only knowledge of Twitter is the Kardashians and Donald Trump this might come as a surprise but actually to create this slide I opened Twitter and I copied and pasted the first three tweets that appeared on my screen. So one of them is somebody has a discussion about costs and impacts of different approaches to labeling this is a fast AI alum who's a 17 year old PhD graduate who's doing well who shows how to mix PyTorch and fast AI and then Hilary Mason who's a professor I guess not a professor anymore but now an industry talking about organizational issues in data science. So you know there's a whole world out there of machine learning on Twitter and there you know if you want to get your work noticed that's a great place to do it because really everybody everybody's there okay and if you want me to highlight your work you know that's where I can see it and I can retweet it. So yeah Twitter is a really good place to be if you're just starting with Twitter and you don't know who to follow go to my Twitter go to my likes and go through my likes and find tweets that you think you actually like that tweet to and then follow the person who did that tweet okay and pretty quickly you'll have a hundred people you're following okay and then they'll retweet things and you'll find other people you like and before you know it hopefully you've got a nice big lot of interesting deep learning stuff to read every day. At first you'll understand like 1% of it which is fine but you know you're there you're in it and it'll be all washing over you and you'll start to find the people who write stuff you find engaging and interesting and you'll also find the people that actually you don't and make sure you unfollow them so that you don't have your feed have stuff you don't care about. So then beyond Twitter you want to start blogging okay and again blogging is not about writing what you had for dinner okay it's about writing something that you of six months ago would have found interesting okay so you know more than you did six months ago so write that down. We have something called FastPages that makes it ridiculously easy to start a blog and so there's no reason for you not to you know at least create a blog. There we go and one of the nice things about FastPages is you can even turn Jupyter Notebooks into blog posts so it's great for kind of technical ones. So this is what a FastPages blog looks like this is a FastPages blog about FastPages I had to write FastPages in order to write the FastPages blog about FastPages but basically it and one of the other nice things it's all in it's all in github right so it's as you're blogging you're learning more about gip it's all written with markdown which is something that you're definitely going to need to know anyway so as you're blogging you'll be learning about a lot of the tools you need to learn about anyway. So one interesting idea for things to blog about is this example from Amano Arora who is an Aussie fast AI alum who is now working at Weights and Biases which is one of the top AI startups in the world. This is a really interesting kind of blog post what Amano did was he took a video that I did at the launch here of the Queensland AI Hub and he wrote down what I said and that's an example of something that you could do if there are videos out there that you liked and nobody's turned it into a post be the first to do so because there's all these benefits when somebody sends me something saying I've written up this talk you gave I'm very grateful to that person because now my talk is now available in a second medium a lot of people prefer to read rather than listen to a talk you know that person's taken the time to do this they've given taken the time to have me check you know their work and kind of everybody ends up winning from this so I've seen with Amano's post about my talk it's got attention from people that my talk did so for example I noticed on my LinkedIn feed the CEO of Data61 which is the CSIRO so the top data science body in Australia highlighted it and said check out this post from Amano Moro right like so this is like an example of the kind of stuff you can do is like try to be helpful right and at the same time you're also learning so there's an example of an interesting kind of blog post which very few people are writing and so there's a huge amount of opportunity here for you to practice your your writing. Okay now what is the difference between machine learning and other kinds of coding that's what Dex says in in this chapter of his book the key about machine learning is that we can generalize we can train a model with one set of data and apply it to a different set of data and still get good results and everything just about that we're doing in this course is all about creating models that are going to generalize well and we're going to be learning about how you can measure how well your model generalizes so answering these questions about can we trust our model to be correct on new data that we feed it is absolutely critical to to every model that you build whether it be in a Kaggle competition or a little prototype or a production model you're creating at work. One of the most important things here is creating a good validation set and this is something that you're you're here about in lesson one of the course but you know I really wanted to highlight it here as did Rudek in his book it's it's a really important idea is you need a good way to measure whether your model is any good so you need a data set that really represents what kind of data is your model likely to have to deal with in real life and my partner Rachel wrote this really great blog post on the Fast.ai blog about this actually interestingly you know this was kind of came out of a lesson that I did at the University of San Francisco and then Rachel turned it into a blog post and Rachel's blog post has ended up much more influential than my video ever was you know so this is actually a good example of what I was talking about and she took it a lot further. Okay the next key thing that Rudek mentions and I totally agree with is it's hard to write correct machine learning code. I always assume that every line of machine learning code I write is wrong and I'm normally correct about that it normally is wrong because there's lots of ways to be wrong and unlike creating a you know a context management app on the web whatever it's much harder to see that you're wrong you know you can't see that the name didn't get stored in the database or you can't see that the title isn't centered right often it's wrong that it's going to be like half a percent less accurate you know or your image is upside down but it's kind of maybe you didn't even look at it I got straight into sent into the system and you end up with something that can only recognize upside down images or whatever. So whenever you're doing you know whenever you're building a project make sure you start with a simple baseline right like create the simplest possible model you can that's that you know solves the problem so simply that you can't have made a mistake so often that'll be like just taking the average of the data or if there's two groups take the average of each of the two groups or you know something that's something really really simple and then you can gradually build up from there. So another very common beginner mistake with projects remember we want you all doing projects is somebody in a project group will say oh I read about this new Bayesian learning thing with these clusters and this you know advanced transformers pipeline and we could put all that together it's going to be better than anything before and they then spend months creating this complex thing and at the end it doesn't work. Now why doesn't it work? Well I don't know it's so big and so complicated maybe it was a stupid idea maybe there's a bug in one piece of it maybe that one piece there shouldn't be there but it should be somewhere else. I don't know right that's not how anybody creates successful machine learning projects. Machine successful machine learning projects are always built in my experience by creating a simplest possible solution that gets something all the way from end to end first and then very gradually it makes it incrementally slightly better. So keep that in mind right you might feel a bit silly when you build that first model that just takes the average of the data right but that's how that's how the pros do it. That's how everybody that actually gets it to work does it. Also often I've had you know Silicon Valley startup hotshots come to me and ask me to like check out their amazing new startup and I'll ask them you know oh you reckon this can separate you know sick people from well people or whatever have you taken the average of each of these two groups and compared that to your model example and they'll say oh no and then they try it and they find out their models worse. So you need to know whether your model is actually doing something useful. For projects one of the things you might want to do is join a Kaggle competition. That might be the last thing you see yourself as doing is being a Kaggle competitor but actually this is one of the best possible projects you can do because to enter a Kaggle competition even to come last you have to go through the entire process of downloading a data set formatting it into the right method ready for a model getting it through the model saving the output getting it into the correct submission format and submitting it back to Kaggle. So getting a model actually up onto the Kaggle leaderboard is really going to test out your end-to-end understanding right and once you've done that you can start to iterate you can start to make it slightly better slightly better slightly better. So although in a lot of ways Kaggle is not representative of the real world you know you don't have to worry about deployment you don't particularly have to worry about kind of inference speed stuff like that and a lot of ways it is closer to the real world than you might expect and that it really does force you to go through the whole process and also to think about engine about kind of planning your project carefully. So enter a competition with your kind of goal that I want to win right now obviously on your first one you're not going to win but the whole point is it's a competition so you're going to try to do your best right and so to do your best join a competition that's early right give yourself plenty of time and every single day try to make a small improvement and then you'll find that but you know if you keep reading the forums on Kaggle and keep trying a bit more every day you'd be amazed at the end of the three months how much you've learned how much of the stuff that at the start you thought this is I have no idea what's going on and then you'll realize oh suddenly I do know what's going on and you might find you get in the top 50% which might be better than you expected. So that this is you know highly recommended at some point during this course is have a real go at a Kaggle competition. So at the end of all of this you might be looking for a job. Now this could mean a number of things a lot of people just want to bring some deep learning into their current job and so you know that's if your organization is already doing some deep learning that might be easier than if it's not if it's not you might just have to start prototyping some things and try to build up some kind of you know proof of concepts internally or maybe you're going to try and go out and get and get you know get a new role as a researcher or a data scientist or whatever. Most people are not going to be able to rely on their you know Stanford PhD to get them there. Right most people are going to rely have to rely on their portfolio. So your portfolio is going to be all the stuff you build along the way. It's your footprint on the deep learning community and that footprint is going to include you know things things like your contributions to the Fast AI forums and your tweets and your stuff on Discord. I would say pretty much every one of the Fast AI alumni that have come to my attention as being thoughtful and effective community members all have very very very good jobs now. And so like people really really notice this footprint right so your blog posts your GitHub projects these are the these are the things that are going to get you a job. They probably won't get you a job at a big company a big old company in a you know kind of standard established IT job right. That's going to go through HR and HR are going to like they're not going to understand any of your GitHub code or know any about your community impact. They're just going to know about credentials right and you'll come up against somebody with a Stanford PhD and they'll get the job right. But startups particularly startups from other people who've got similar backgrounds of which there are many are going to appreciate you or companies that don't really have an established AI group yet or the startup you built yourself will certainly appreciate you. So it's the more you've got a portfolio and that you can show that you've really built stuff the better and so start early. Another reason to finish this first course is that it's going to allow you to do the second course and if you're doing this live part two we're going to be doing actually a whole new part two towards you know basically shortly after this is finished right. So if you if you finish this and do a good job of it then you could actually be one of the first to do part two. Now we've seen how easy Colab is to get started. We've also talked about some of the downsides of it. It's kind of ephemeral you start from scratch every time you've got this kind of hacky stuff of saving notebooks into your Google Drive blah blah blah. AWS on the other hand is going to give you and Google Cloud and Java slabs and so forth are going to give you a real Linux server. And it's going to cost you Java slabs is the cheapest about 40 cents AWS I think about 60 cents US per hour. It's not going to send you broke but it's you know it's not nothing. It's a good idea to try it if you can and I'm going to show you how to get started there. And what we might do Michael is I'll do some Q&A while things are running. So I'm going to head over to AWS EC2. So one of the tricky things about AWS is they've got hundreds of products. This is Amazon Web Services and they all have names that are totally meaningless. So you just have to know EC2 is the name of the thing that you go to to rent a computer. So they don't call it Amazon computer rental they call it EC2. So the first thing you need to do is you need to sign up to AWS. And one of the things that they get is a lot of fraud. So a lot of people try to use their GPUs to mine Bitcoin. So you have to ask them to give you permission to use their GPUs and that's called requesting a service limit increase. So you'll need to follow the steps here to ask them for a limit increase. If you write these exact words with this exact formatting it might come through a little bit quicker. If you're from a country where there's a lot of fraud you might not even get this permission. Maybe Java Slabs is going to be easier. I'm not sure Java Slabs even has the fraud checks. So anyway there's quite a few places you can you can try to get an instance. So if AWS has a problem with your quota try somewhere else. But generally speaking most people should get a response pretty quickly saying you've now got approved. So for you doing this course if you're going to try out AWS EC2 I suggest you log in and request this service limit increase right away so that you know by the time you come back tomorrow or the next day it'll be done. And so what I'm currently doing is I'm on course Fast AI and I've gone to Slitix servers AWS EC2 and we're following through that project process. Okay now to log in to your server you're going to need to use something called SSH secure shell. So this is something where on your computer screen that server's computer screen effectively is going to appear and the stuff you type is actually running on that remote server not on your computer. Since pretty much nobody uses usernames and passwords for SSH instead we use something called public key cryptography which is where you basically have a secret number which only you know and then there's another public number you tell other people and basically there's a really cool math trick which allows people to check whether you have the secret number without actually anybody without actually telling them the secret number and the process so that's called so that's what an SSH key is. So there's this thing called a public key and that's the number that you're the code that you're going to give to anybody you want to be able to log into and then there's your private key which you're going to keep for yourself. So you're going to need a terminal so on Windows in the store there's something called the Windows terminal which Microsoft provides for free which is pretty good. Mac has a terminal that comes with it, Linux has a terminal that comes with it. So I'm using Windows but it'll basically look the same for everybody. Now on Windows you need a Ubuntu Linux shell not a normal Windows shell so to do that you need something called WSL, Windows Subsystem for Linux and that will give you a full Ubuntu system on your Windows computer. Again it's free, it only takes a couple of minutes to set up so there's a link to how to do it here. So once you've done it, whether you're on Mac or Linux or Windows it's going to look basically the same. And so you'll create your SSH key by following the instructions in the documentation which is basically you run SSH keygen and it's just going to go through and create these two files. So you just run it and it creates these two files. And so this is the one that we have to give Amazon and this is the one that we're going to keep for ourselves. So following along the documentation here it says to click on services, EC2, find key pairs, here, ok, and then we'll go here import key pair and whatever, AWS, and this is where we're going to find the IDRSAPUB that we just created and you can see this here it is right it's just a big long code and it's fine you can all look at this this is public not secret this is the cool thing right there's no passwords and I say import and so now we have an SSH key and we can use that to log in. Ok so this is just all this is, is all those steps. So renting a server in AWS speak is called launching an instance. So to launch an instance we'll scroll back up to the top to instances and we will say launch instance ok and it'll say ok what kind of thing do you want to run Amazon Linux or Windows or Red Hat or whatever. I strongly strongly suggest you use Ubuntu and the latest version which is currently 20. So I'm just going to say select. Ok and then it'll say ok what kind of server do you want. For playing around there's actually one that you can get for free. Now it doesn't do it's pretty it's kind of slow right but for learning about SSH and Linux and stuff this is actually a great one to use. It's no good for deep learning it doesn't have a GPU so if I go to G4DN that's the cheapest kind of good GPUs we can get and I'll get the smallest one there G4DNXL and then I'll say next. So how big a hard drive do I want. I'd normally say about 100 gig. Launch and launch and so now it's going to say ok when you log into this which key pair are you going to use. Ok so you just select the one that you just imported and say yep I know that I have that and then launch. And you'll see Dell says this has now been initiated and it's got a code so this is the thing that I've just launched. If I click on it here it shows me here's my instance. So as you if you haven't done much with servers and Linux and SSH and stuff there's going to be this whole world of new stuff for you to learn about but this is an opportunity it's not a problem so if you're not familiar with things like IP addresses that's cool there's lots of tutorials around at the moment but for now just know this is the unique address like a street address that your new computer has and so we're going to connect to it. So this button here will click will copy that address. So we can then go to our terminal and we can type SSH and paste in the address and then the only other thing I do need to do is I need to say provide a username and AWS always uses the username Ubuntu for all of its Ubuntu images. So you say Ubuntu at and then the IP. So if I now press enter we're in. OK so now everything I type here is actually being typed on that remote computer. So for example to list the contents of a directory I type LS. OK so the thing I'm actually typing into here is bash a bash shell. So bash is something other of these things need to be familiar with and you can learn about it in that missing semester MIT course I mentioned. You know it takes a few weeks to get somewhat comfortable with bash. It's a very different feel to using a GUI if you're more familiar with Explorer or Finder or whatever but you'll find it's you'll be much more productive soon enough because you can replicate things quickly you can script things you can copy and paste things and so forth. Anyway so here's my here's my computer. It's going to sit here running until you tell it not to even if you turn your computer off your server is still running and that means you're still paying for it. OK so one of the things I guarantee you're going to learn the hard way by wasting money is that you're going to forget to turn it off. OK so to turn it off you're just going to go stop instance. So you make sure you you do that. Let's see how we're going here. So we've launched our instance. And we SSH into it. OK so keeping a Linux server up to date and running used to be kind of annoying but luckily I've created something called fast setup for you which makes it easy. And all you need to do is copy this and paste it into your terminal. And this is one of the really cool things about Linux and using bash is like in Windows or with Mac Finder you'd have pages and pages of click this and drag that and scroll here. But I've just crypted the whole thing. So I'm just going to go ahead and paste it over here. And it's off. OK now what this is going to do is it's going to fully set up this Linux server. It's going to make it automatically update with the latest software. It's going to configure it all correctly. And so forth. And it's going to ask a minimum number of questions. So I'm just going to show you the questions it's going to ask you. It's going to ask for a hostname. So a hostname is just a more convenient way to access a server. And so you can basically write anything you like as long as it's got at least two dots in it. So I'm going to call this course test dot fast dot AI example. OK and then asks for an email address. Now the email address is basically just goes what is where it's going to send kind of error locks and stuff too. So maybe we'll say info at fast dot AI. OK do you want to set a password. Probably do. So hit enter for yes. So I'm going to put in a password. Asks you to type it again. OK reboot automatically when required. I'll say yes. And that's it. OK so that's all the information that it needed. So behind the scenes what's actually happening here is it's grabbed the latest Git repo from fast setup and it's running this thing called Ubuntu initial. And you know this is something you can check out if you're interested. It's basically 125 lines of bash script which is going to set up your firewall for you set up SSH security for you set up your swap file for you set up your SSH configuration for you install all the software you need for you set up your logging and upgrades for you set up your password and hostname for you. OK so it's going to do all that. And you know this is the kind of thing that if you know from time to time you can just might think oh I'm interested in how X works. And since everything is open source you can just go in and see how X works. And at first none of this might make any sense. And so you go oh all right let's pick something and learn about it. Enable firewall. UFW. No what's UFW. Copy paste. UFW. Probably not United Farm Workers. Uncomplicated firewall. Did Jeremy mention firewall. OK what the hell is a firewall. And you know you could start reading right. And then you could be like oh maybe firewall tutorial. Often adding tutorial can be helpful. OK so you know you can start to just jump in here and there. OK don't get too distracted. We want to spend as much time as possible training models. But this is how we learn about our tools. OK so this is now going and downloading the latest version of all the software that it's going to need from Linux. So maybe good time for questions if we have any Michael. What's your current opinion regarding Swift and Julia as replacements of Python. So Swift is basically out now. So Google has basically archived the Swift for TensorFlow project. So you can safely ignore that. Yeah Julia is. Julia is interesting. You know I think it's a lovely language. Nothing has the ecosystem that Python does. So you know if you use Julia you're going to have to figure out a lot more stuff on your own and you'll find a lot more hard edges. But I do think at some point Python is going to have to be replaced. And Julia seems like one of if not the most likely thing to replace it. Or maybe it won't be replaced by Julia maybe it'll be replaced by something else that's kind of Python like Jax which actually takes Python and compiles it using something called XLA into a much faster thing than Python otherwise would be. OK do you think that deep learning or more traditional ML or stats approaches are more useful for traditional industry applications right now. So before I answer that question I'm going to press Y which is going to reboot our computer now that it's all updated. And obviously when we reboot that computer running at the AWS data center it closes the connection because it's busy rebooting. So we'll give it a couple of minutes. There's not a single good answer to that question. And you don't really need to answer that question because basically any time you want to try any kind of machine learning model on a problem you should try a few different algorithms and switching from a random forest to a gradient boosting machine to logistic regression to deep learning is you know an extra half hour. So you should just try a few different approaches. I find personally for me deep learning is increasingly turning out to be the easiest thing to get started with and gives me the best results for most projects I seem to do nowadays. But you know have a look at like Kaggle competitions from time to time there are still things where gradient boosting machines work better or very often people use both and ensemble them. But yeah it's not a question that you actually need to answer. You want to get to a point where it just takes you a few minutes to try another algorithm out and so you don't need to be wedded to one or the other. So I'm just going to see if I can I don't know how long it's going to take to reboot so I'm just going to press up arrow to get back my last SSH command and I'll press enter and we'll see if we're back. We're back. OK. So this is finished rebooting. Oh actually this time it says to do something slightly different which is to add this minus L here. This is the thing that's going to let us connect to Jupyter Notebook. So I'm going to type exit to exit from the server and this time I'm going to add the extra bit of the command. There we go. OK. And all right so the next thing is we're going to install something called Miniconda. Miniconda is a very nice distribution of Python the programming language. A lot of people have bad experiences of their computers getting really confused with Python packages and things conflicting and all kinds of stuff like that. That's because pretty much all the major operating systems now come with a version of Python that is used by your computer for important operating system tasks. You should not be using that Python to train your machine learning models. Leave that Python alone. You should always install Miniconda which is going to give you your own version of Python which is nothing to do with your operating system as you can play around with as you like. It's really easy just you can delete the whole folder and create it again in like three minutes. You can create new environments which is like little testing grounds. You can try different things. This is a very strong recommendation is to make sure that you install even if you're just playing around on Windows or a Mac not on a server install Miniconda. It's cross platform. You can use it everywhere and use that Python. Okay so Miniconda is now installed so we now have our own Python setup. So the last setup step is we have to install drivers for the GPU and Ubuntu actually comes with something that figures out for you what the best drivers are for your device. So this is just what this step here is and so I'm going to look down look here it says recommended. Okay. So here's the driver I want. Okay. But what I actually recommend is you use that but also the one at the dash server to the end that's going to make like not install the stuff for playing computer games or whatever. Okay. So let's go ahead and run these lines of code. This is the bit here. See this is 460 depending on your graphics card when you run this you might have some different number. Okay. But since I wrote this today it's still 460. So we'll go ahead and do that and this is going to go ahead and install this. Oh sudo sudo is a special thing you can add to the front of a command that runs it as an administrator. Okay so some things you know by default commands you run basically can't break your system right. Because things like installing new software you have to tell it to run it as an administrator. So and when you do that it'll ask you for your password. This is the password that you put in just just a moment ago with the setup. There we go. Okay let's keep going. Is there a section of the course that people skip over too quickly. Yes part two. Yeah not enough people do part two. And the difference between part one and part two is the difference between being a pretty handy practitioner you know who can who can do some pretty good work as long as it's in reasonably well established kinds of areas. And versus being somebody who understands how everything's put together you could you know if you're told to create a deep learning model on in a domain that's like there are no published models you'll be able to create one. If you understand how to create models which combine multiple different data types you know you're it's it's yeah it's it's a really great thing to finish and yeah not enough people realize how much is is there. And just the later lessons in in general you know it can like after you've done three lessons you you are pretty handy and you'll feel pretty handy right. But it's pretty easy to stop there because it feels like OK I get it. You know I can train a model I get what's going on and to be fair it does very dramatically kind of scale up in terms of intensity after that because in lesson four you'll have to write your own optimizer from scratch and you'll be getting into the calculus and stuff. But you know it is a big difference in terms of what what you can do and what you understand. So I think in general you know not enough people are getting deeper into the lessons. OK so this is now finished installing the Nvidia drivers. Normally at this point people say to reboot but there's actually a magic thing you can do which means you don't have to reboot and the Nvidia provides something called Nvidia SMI which will tell you about your installed GPUs. And so if you run it and it pops up anything at all other than an error it means that you are you successfully have your GPUs installed. So this case we have a Tesla T4. It's currently 36 centigrade in there and the most important thing to know about is that it has 15 gigabytes of memory of which we're using nothing at all. And there are no processors currently running on the GPU. So if you're finding something's going very slowly and you're wondering maybe it's not using the GPU you can always run Nvidia SMI and if it says no running processors found you're not using the GPU. OK so one more setup step which is we have to install all of the software all of the Python libraries needed so PyTorch, FastAI, Jupyter Notebook and so forth. And so I've created a package which has that whole lot it's called Fastbook. If you're if you've used HANA Conda or MiniConda before you might be surprised it says Mamba rather than Conda. You should definitely use Mamba and not Conda it's way way way faster. So anytime you see something saying Conda install you should instead type Mamba install. It's way faster. OK so off it goes. Mamba is now going to install all of the all this Python software getting installed for us. PyTorch is well over a gigabyte so this is going to take a few minutes just because it has to download that that whole thing and yeah that can take a while. So while this is going do we got any more questions Michael? Do you recommend any software for experiment tracking? So the most popular experiment tracking software would be TensorBoard and Weights and Biases. Experiment tracking software is stuff which will basically you can use a FastAO callback and you basically will say train whilst tracking with TensorBoard or train whilst tracking with Weights and Biases. And what it will do is it will kind of create a little database showing you all the training results from all the different experiments you've run and create some little graphs of them and so forth. Personally I don't use any experiment tracking software and the reason I don't is I found that many many many people just about everybody I know who uses them finds it incredibly distracting. So the trick to training models is don't watch them train. So if you've done an EC programming it's like don't watch it compile right. Go and do something else preferably set up your next experiment. Experiment tracking software just makes it so tempting to look at all the pretty graphs in my opinion. So I would suggest get it running leave come back when it's done and there should be a bit of reason you were running that experiment so check whatever that reason was right. Having said that if you're really sure you need the services of experiment tracking software for what you're doing and there are some things that genuinely need it then I think Weights and Biases is the best at the moment. I think it's really great and furthermore they've hired lots of fast AI alumni and they're super nice people so definitely recommend that. So that's all the installation. So the last step is just to grab the book the notebooks and so you use something called git clone to grab a repository of code and this is going to grab the fastbook repository paste. So you can see it's saying cloning this repository and so you'll now find that there is a fastbook directory. So you can cd into it and there is our book. So I think something on Anaconda is going slowly so we're not going to wait for it to download but so I want to show the very last step but the very last step is to run Jupyter notebook and then you'll be able to click on the URL that pops up and it'll bring up something that basically looks just like we saw in curlup. But the nice thing is everything you save, everything you do will be remembered so all of your experiments are going to be there, the datasets you download are still there, so on and so forth. So that's that. So when you're done it'll remind you here to as I mentioned before stop your instance so you can either choose stop in this menu or you can choose stop here or personally what I quite like to do is to run sudo, remember sudo is this thing that runs as an administrator, shutdown, halt, now. And so that shuts it down from here without having to go into the AWS GUI. And there we go. So if we look back at the EC2 here in a moment this will switch from running state to stop state. So I think that's everything. Michael is there anything else we need to cover? Okay great. Alright well thank you everybody for listening in to lesson 0 and I look forward to hearing how you go with lesson 1 and seeing your projects that you create and don't forget to get involved in the forums. If you do get stuck with something the first thing to do is to search the forums because out of the hundreds of thousands of people that have done this before somebody's probably got stuck in the same way before so hopefully they can answer your question. Otherwise feel free to ask your own questions and hopefully somebody will answer for you. Thanks everybody. Design,.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.36, "text": " Okay, hi everybody, thanks for joining.", "tokens": [1033, 11, 4879, 2201, 11, 3231, 337, 5549, 13], "temperature": 0.0, "avg_logprob": -0.16209721198448768, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.008312514051795006}, {"id": 1, "seek": 0, "start": 7.36, "end": 14.88, "text": " This is an entirely optional presentation which I'll call a Lesson 0, which is all about", "tokens": [639, 307, 364, 7696, 17312, 5860, 597, 286, 603, 818, 257, 18649, 266, 1958, 11, 597, 307, 439, 466], "temperature": 0.0, "avg_logprob": -0.16209721198448768, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.008312514051795006}, {"id": 2, "seek": 0, "start": 14.88, "end": 18.28, "text": " how to fast AI.", "tokens": [577, 281, 2370, 7318, 13], "temperature": 0.0, "avg_logprob": -0.16209721198448768, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.008312514051795006}, {"id": 3, "seek": 0, "start": 18.28, "end": 24.72, "text": " It's all about how to get the most out of this course, how to make sure you finish it,", "tokens": [467, 311, 439, 466, 577, 281, 483, 264, 881, 484, 295, 341, 1164, 11, 577, 281, 652, 988, 291, 2413, 309, 11], "temperature": 0.0, "avg_logprob": -0.16209721198448768, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.008312514051795006}, {"id": 4, "seek": 2472, "start": 24.72, "end": 31.88, "text": " and how to make sure you feel like it's been a productive time.", "tokens": [293, 577, 281, 652, 988, 291, 841, 411, 309, 311, 668, 257, 13304, 565, 13], "temperature": 0.0, "avg_logprob": -0.07064008325096069, "compression_ratio": 1.8032128514056225, "no_speech_prob": 2.0781017155968584e-05}, {"id": 5, "seek": 2472, "start": 31.88, "end": 38.84, "text": " And the reason I'm doing this is because a lot of people who take the course, when they", "tokens": [400, 264, 1778, 286, 478, 884, 341, 307, 570, 257, 688, 295, 561, 567, 747, 264, 1164, 11, 562, 436], "temperature": 0.0, "avg_logprob": -0.07064008325096069, "compression_ratio": 1.8032128514056225, "no_speech_prob": 2.0781017155968584e-05}, {"id": 6, "seek": 2472, "start": 38.84, "end": 42.68, "text": " get to the end of it, they say to me, oh, it wasn't until I got to the end of the course", "tokens": [483, 281, 264, 917, 295, 309, 11, 436, 584, 281, 385, 11, 1954, 11, 309, 2067, 380, 1826, 286, 658, 281, 264, 917, 295, 264, 1164], "temperature": 0.0, "avg_logprob": -0.07064008325096069, "compression_ratio": 1.8032128514056225, "no_speech_prob": 2.0781017155968584e-05}, {"id": 7, "seek": 2472, "start": 42.68, "end": 45.32, "text": " that I realized how I should have done the whole course, so now I'm going to go back", "tokens": [300, 286, 5334, 577, 286, 820, 362, 1096, 264, 1379, 1164, 11, 370, 586, 286, 478, 516, 281, 352, 646], "temperature": 0.0, "avg_logprob": -0.07064008325096069, "compression_ratio": 1.8032128514056225, "no_speech_prob": 2.0781017155968584e-05}, {"id": 8, "seek": 2472, "start": 45.32, "end": 48.14, "text": " and redo the whole thing over again.", "tokens": [293, 29956, 264, 1379, 551, 670, 797, 13], "temperature": 0.0, "avg_logprob": -0.07064008325096069, "compression_ratio": 1.8032128514056225, "no_speech_prob": 2.0781017155968584e-05}, {"id": 9, "seek": 2472, "start": 48.14, "end": 54.120000000000005, "text": " And so I'm going to tell you about what the messages I've heard are, about what people", "tokens": [400, 370, 286, 478, 516, 281, 980, 291, 466, 437, 264, 7897, 286, 600, 2198, 366, 11, 466, 437, 561], "temperature": 0.0, "avg_logprob": -0.07064008325096069, "compression_ratio": 1.8032128514056225, "no_speech_prob": 2.0781017155968584e-05}, {"id": 10, "seek": 5412, "start": 54.12, "end": 59.16, "text": " have found the best approaches to making the course work.", "tokens": [362, 1352, 264, 1151, 11587, 281, 1455, 264, 1164, 589, 13], "temperature": 0.0, "avg_logprob": -0.1374199534037027, "compression_ratio": 1.4678899082568808, "no_speech_prob": 2.5460672986810096e-05}, {"id": 11, "seek": 5412, "start": 59.16, "end": 67.24, "text": " I'm also going to go through the actual mechanics of how to get set up with two systems, Google", "tokens": [286, 478, 611, 516, 281, 352, 807, 264, 3539, 12939, 295, 577, 281, 483, 992, 493, 365, 732, 3652, 11, 3329], "temperature": 0.0, "avg_logprob": -0.1374199534037027, "compression_ratio": 1.4678899082568808, "no_speech_prob": 2.5460672986810096e-05}, {"id": 12, "seek": 5412, "start": 67.24, "end": 74.9, "text": " Colab and AWS EC2, and I'll talk about why you might use one versus the other.", "tokens": [4004, 455, 293, 17650, 19081, 17, 11, 293, 286, 603, 751, 466, 983, 291, 1062, 764, 472, 5717, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.1374199534037027, "compression_ratio": 1.4678899082568808, "no_speech_prob": 2.5460672986810096e-05}, {"id": 13, "seek": 5412, "start": 74.9, "end": 80.6, "text": " So a lot of people now, as in many hundreds of thousands, have gone through the Fast AI", "tokens": [407, 257, 688, 295, 561, 586, 11, 382, 294, 867, 6779, 295, 5383, 11, 362, 2780, 807, 264, 15968, 7318], "temperature": 0.0, "avg_logprob": -0.1374199534037027, "compression_ratio": 1.4678899082568808, "no_speech_prob": 2.5460672986810096e-05}, {"id": 14, "seek": 8060, "start": 80.6, "end": 86.67999999999999, "text": " Practical Deep Learning for Coders course, and many, many, many of them have gone on", "tokens": [19170, 804, 14895, 15205, 337, 383, 378, 433, 1164, 11, 293, 867, 11, 867, 11, 867, 295, 552, 362, 2780, 322], "temperature": 0.0, "avg_logprob": -0.10733054876327515, "compression_ratio": 1.5560975609756098, "no_speech_prob": 1.2804855032300111e-05}, {"id": 15, "seek": 8060, "start": 86.67999999999999, "end": 95.39999999999999, "text": " to create successful startups, to write research papers with high impact factors, to create", "tokens": [281, 1884, 4406, 28041, 11, 281, 2464, 2132, 10577, 365, 1090, 2712, 6771, 11, 281, 1884], "temperature": 0.0, "avg_logprob": -0.10733054876327515, "compression_ratio": 1.5560975609756098, "no_speech_prob": 1.2804855032300111e-05}, {"id": 16, "seek": 8060, "start": 95.39999999999999, "end": 98.11999999999999, "text": " new products at their companies.", "tokens": [777, 3383, 412, 641, 3431, 13], "temperature": 0.0, "avg_logprob": -0.10733054876327515, "compression_ratio": 1.5560975609756098, "no_speech_prob": 1.2804855032300111e-05}, {"id": 17, "seek": 8060, "start": 98.11999999999999, "end": 104.28, "text": " It's a pretty well proven course at this time, but there's also a lot of people that never", "tokens": [467, 311, 257, 1238, 731, 12785, 1164, 412, 341, 565, 11, 457, 456, 311, 611, 257, 688, 295, 561, 300, 1128], "temperature": 0.0, "avg_logprob": -0.10733054876327515, "compression_ratio": 1.5560975609756098, "no_speech_prob": 1.2804855032300111e-05}, {"id": 18, "seek": 8060, "start": 104.28, "end": 106.69999999999999, "text": " finish the course.", "tokens": [2413, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.10733054876327515, "compression_ratio": 1.5560975609756098, "no_speech_prob": 1.2804855032300111e-05}, {"id": 19, "seek": 10670, "start": 106.7, "end": 113.46000000000001, "text": " And so if you're watching this, it's because you've decided you do want to learn deep learning.", "tokens": [400, 370, 498, 291, 434, 1976, 341, 11, 309, 311, 570, 291, 600, 3047, 291, 360, 528, 281, 1466, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.16614632379441036, "compression_ratio": 1.6293103448275863, "no_speech_prob": 8.664124834467657e-06}, {"id": 20, "seek": 10670, "start": 113.46000000000001, "end": 117.60000000000001, "text": " So I'm going to talk a bit about what's it going to take for you to be one of the people", "tokens": [407, 286, 478, 516, 281, 751, 257, 857, 466, 437, 311, 309, 516, 281, 747, 337, 291, 281, 312, 472, 295, 264, 561], "temperature": 0.0, "avg_logprob": -0.16614632379441036, "compression_ratio": 1.6293103448275863, "no_speech_prob": 8.664124834467657e-06}, {"id": 21, "seek": 10670, "start": 117.60000000000001, "end": 119.88, "text": " that makes this into a great experience.", "tokens": [300, 1669, 341, 666, 257, 869, 1752, 13], "temperature": 0.0, "avg_logprob": -0.16614632379441036, "compression_ratio": 1.6293103448275863, "no_speech_prob": 8.664124834467657e-06}, {"id": 22, "seek": 10670, "start": 119.88, "end": 125.4, "text": " When I talk about the course, I'm also talking about the book.", "tokens": [1133, 286, 751, 466, 264, 1164, 11, 286, 478, 611, 1417, 466, 264, 1446, 13], "temperature": 0.0, "avg_logprob": -0.16614632379441036, "compression_ratio": 1.6293103448275863, "no_speech_prob": 8.664124834467657e-06}, {"id": 23, "seek": 10670, "start": 125.4, "end": 131.92000000000002, "text": " So just to be clear, there's a book that Sylvia Guzhera and I wrote, which you can either", "tokens": [407, 445, 281, 312, 1850, 11, 456, 311, 257, 1446, 300, 33349, 11617, 2694, 89, 511, 64, 293, 286, 4114, 11, 597, 291, 393, 2139], "temperature": 0.0, "avg_logprob": -0.16614632379441036, "compression_ratio": 1.6293103448275863, "no_speech_prob": 8.664124834467657e-06}, {"id": 24, "seek": 13192, "start": 131.92, "end": 139.14, "text": " buy from Amazon, and people like it happily, or believe it or not, you can read the whole", "tokens": [2256, 490, 6795, 11, 293, 561, 411, 309, 19909, 11, 420, 1697, 309, 420, 406, 11, 291, 393, 1401, 264, 1379], "temperature": 0.0, "avg_logprob": -0.1543950776796083, "compression_ratio": 1.6096491228070176, "no_speech_prob": 6.048710474715335e-06}, {"id": 25, "seek": 13192, "start": 139.14, "end": 141.45999999999998, "text": " thing for free.", "tokens": [551, 337, 1737, 13], "temperature": 0.0, "avg_logprob": -0.1543950776796083, "compression_ratio": 1.6096491228070176, "no_speech_prob": 6.048710474715335e-06}, {"id": 26, "seek": 13192, "start": 141.45999999999998, "end": 142.72, "text": " So it's called Fastbook.", "tokens": [407, 309, 311, 1219, 15968, 2939, 13], "temperature": 0.0, "avg_logprob": -0.1543950776796083, "compression_ratio": 1.6096491228070176, "no_speech_prob": 6.048710474715335e-06}, {"id": 27, "seek": 13192, "start": 142.72, "end": 145.07999999999998, "text": " It's a fast book repo.", "tokens": [467, 311, 257, 2370, 1446, 49040, 13], "temperature": 0.0, "avg_logprob": -0.1543950776796083, "compression_ratio": 1.6096491228070176, "no_speech_prob": 6.048710474715335e-06}, {"id": 28, "seek": 13192, "start": 145.07999999999998, "end": 150.6, "text": " Honestly, I make basically nothing from the book, so don't feel like you need to buy it", "tokens": [12348, 11, 286, 652, 1936, 1825, 490, 264, 1446, 11, 370, 500, 380, 841, 411, 291, 643, 281, 2256, 309], "temperature": 0.0, "avg_logprob": -0.1543950776796083, "compression_ratio": 1.6096491228070176, "no_speech_prob": 6.048710474715335e-06}, {"id": 29, "seek": 13192, "start": 150.6, "end": 152.04, "text": " to say thank you or something.", "tokens": [281, 584, 1309, 291, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.1543950776796083, "compression_ratio": 1.6096491228070176, "no_speech_prob": 6.048710474715335e-06}, {"id": 30, "seek": 13192, "start": 152.04, "end": 153.82, "text": " Buy it if you want the book.", "tokens": [19146, 309, 498, 291, 528, 264, 1446, 13], "temperature": 0.0, "avg_logprob": -0.1543950776796083, "compression_ratio": 1.6096491228070176, "no_speech_prob": 6.048710474715335e-06}, {"id": 31, "seek": 13192, "start": 153.82, "end": 156.66, "text": " If you're happy using notebooks, use the free one.", "tokens": [759, 291, 434, 2055, 1228, 43782, 11, 764, 264, 1737, 472, 13], "temperature": 0.0, "avg_logprob": -0.1543950776796083, "compression_ratio": 1.6096491228070176, "no_speech_prob": 6.048710474715335e-06}, {"id": 32, "seek": 13192, "start": 156.66, "end": 159.83999999999997, "text": " It's all good.", "tokens": [467, 311, 439, 665, 13], "temperature": 0.0, "avg_logprob": -0.1543950776796083, "compression_ratio": 1.6096491228070176, "no_speech_prob": 6.048710474715335e-06}, {"id": 33, "seek": 15984, "start": 159.84, "end": 164.38, "text": " So the book was actually written as Jupyter Notebooks, and we wrote something to turn", "tokens": [407, 264, 1446, 390, 767, 3720, 382, 22125, 88, 391, 11633, 15170, 11, 293, 321, 4114, 746, 281, 1261], "temperature": 0.0, "avg_logprob": -0.1609216229668979, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.4738091522303876e-05}, {"id": 34, "seek": 15984, "start": 164.38, "end": 166.44, "text": " it into a book book.", "tokens": [309, 666, 257, 1446, 1446, 13], "temperature": 0.0, "avg_logprob": -0.1609216229668979, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.4738091522303876e-05}, {"id": 35, "seek": 15984, "start": 166.44, "end": 170.68, "text": " The book also, by the way, actually looks great on Kindle, online, as well as paper.", "tokens": [440, 1446, 611, 11, 538, 264, 636, 11, 767, 1542, 869, 322, 9242, 306, 11, 2950, 11, 382, 731, 382, 3035, 13], "temperature": 0.0, "avg_logprob": -0.1609216229668979, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.4738091522303876e-05}, {"id": 36, "seek": 15984, "start": 170.68, "end": 173.84, "text": " I know often technical books don't.", "tokens": [286, 458, 2049, 6191, 3642, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.1609216229668979, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.4738091522303876e-05}, {"id": 37, "seek": 15984, "start": 173.84, "end": 177.44, "text": " This one actually does.", "tokens": [639, 472, 767, 775, 13], "temperature": 0.0, "avg_logprob": -0.1609216229668979, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.4738091522303876e-05}, {"id": 38, "seek": 15984, "start": 177.44, "end": 182.48000000000002, "text": " And then the course goes through half of the book.", "tokens": [400, 550, 264, 1164, 1709, 807, 1922, 295, 264, 1446, 13], "temperature": 0.0, "avg_logprob": -0.1609216229668979, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.4738091522303876e-05}, {"id": 39, "seek": 15984, "start": 182.48000000000002, "end": 186.96, "text": " And so quite soon we'll do a part two, which will go through the other half of the book", "tokens": [400, 370, 1596, 2321, 321, 603, 360, 257, 644, 732, 11, 597, 486, 352, 807, 264, 661, 1922, 295, 264, 1446], "temperature": 0.0, "avg_logprob": -0.1609216229668979, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.4738091522303876e-05}, {"id": 40, "seek": 15984, "start": 186.96, "end": 189.68, "text": " plus some other new stuff.", "tokens": [1804, 512, 661, 777, 1507, 13], "temperature": 0.0, "avg_logprob": -0.1609216229668979, "compression_ratio": 1.6951219512195121, "no_speech_prob": 1.4738091522303876e-05}, {"id": 41, "seek": 18968, "start": 189.68, "end": 195.02, "text": " Basically each lesson covers a chapter or so of the book.", "tokens": [8537, 1184, 6898, 10538, 257, 7187, 420, 370, 295, 264, 1446, 13], "temperature": 0.0, "avg_logprob": -0.11864099609717894, "compression_ratio": 1.6919191919191918, "no_speech_prob": 2.8129704787716037e-06}, {"id": 42, "seek": 18968, "start": 195.02, "end": 199.32, "text": " So if you're doing this course, you'll be going through the book, at least in the notebooks,", "tokens": [407, 498, 291, 434, 884, 341, 1164, 11, 291, 603, 312, 516, 807, 264, 1446, 11, 412, 1935, 294, 264, 43782, 11], "temperature": 0.0, "avg_logprob": -0.11864099609717894, "compression_ratio": 1.6919191919191918, "no_speech_prob": 2.8129704787716037e-06}, {"id": 43, "seek": 18968, "start": 199.32, "end": 202.52, "text": " and you might want the paper one as well.", "tokens": [293, 291, 1062, 528, 264, 3035, 472, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.11864099609717894, "compression_ratio": 1.6919191919191918, "no_speech_prob": 2.8129704787716037e-06}, {"id": 44, "seek": 18968, "start": 202.52, "end": 210.28, "text": " So here is the main thing that you should commit to right now, which is to finish the", "tokens": [407, 510, 307, 264, 2135, 551, 300, 291, 820, 5599, 281, 558, 586, 11, 597, 307, 281, 2413, 264], "temperature": 0.0, "avg_logprob": -0.11864099609717894, "compression_ratio": 1.6919191919191918, "no_speech_prob": 2.8129704787716037e-06}, {"id": 45, "seek": 18968, "start": 210.28, "end": 217.22, "text": " damn course, right, or finish at least half of the book.", "tokens": [8151, 1164, 11, 558, 11, 420, 2413, 412, 1935, 1922, 295, 264, 1446, 13], "temperature": 0.0, "avg_logprob": -0.11864099609717894, "compression_ratio": 1.6919191919191918, "no_speech_prob": 2.8129704787716037e-06}, {"id": 46, "seek": 21722, "start": 217.22, "end": 220.96, "text": " Because everybody, I think, who joins comes in thinking, okay, I'm going to do this.", "tokens": [1436, 2201, 11, 286, 519, 11, 567, 24397, 1487, 294, 1953, 11, 1392, 11, 286, 478, 516, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.17182671845848882, "compression_ratio": 1.8051470588235294, "no_speech_prob": 8.139478268276434e-06}, {"id": 47, "seek": 21722, "start": 220.96, "end": 222.28, "text": " I'm going to do deep learning.", "tokens": [286, 478, 516, 281, 360, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.17182671845848882, "compression_ratio": 1.8051470588235294, "no_speech_prob": 8.139478268276434e-06}, {"id": 48, "seek": 21722, "start": 222.28, "end": 227.56, "text": " But if you, when I look at our YouTube analytics, a lot of people don't finish.", "tokens": [583, 498, 291, 11, 562, 286, 574, 412, 527, 3088, 15370, 11, 257, 688, 295, 561, 500, 380, 2413, 13], "temperature": 0.0, "avg_logprob": -0.17182671845848882, "compression_ratio": 1.8051470588235294, "no_speech_prob": 8.139478268276434e-06}, {"id": 49, "seek": 21722, "start": 227.56, "end": 234.24, "text": " Okay, so you just need to decide what day are you going to watch the course each week?", "tokens": [1033, 11, 370, 291, 445, 643, 281, 4536, 437, 786, 366, 291, 516, 281, 1159, 264, 1164, 1184, 1243, 30], "temperature": 0.0, "avg_logprob": -0.17182671845848882, "compression_ratio": 1.8051470588235294, "no_speech_prob": 8.139478268276434e-06}, {"id": 50, "seek": 21722, "start": 234.24, "end": 235.9, "text": " What day are you going to do the assignment?", "tokens": [708, 786, 366, 291, 516, 281, 360, 264, 15187, 30], "temperature": 0.0, "avg_logprob": -0.17182671845848882, "compression_ratio": 1.8051470588235294, "no_speech_prob": 8.139478268276434e-06}, {"id": 51, "seek": 21722, "start": 235.9, "end": 239.4, "text": " What day, like how are you going to structure your time to finish the course?", "tokens": [708, 786, 11, 411, 577, 366, 291, 516, 281, 3877, 428, 565, 281, 2413, 264, 1164, 30], "temperature": 0.0, "avg_logprob": -0.17182671845848882, "compression_ratio": 1.8051470588235294, "no_speech_prob": 8.139478268276434e-06}, {"id": 52, "seek": 21722, "start": 239.4, "end": 243.16, "text": " And maybe you're coming in deciding, I don't want to finish it, which is fine, right?", "tokens": [400, 1310, 291, 434, 1348, 294, 17990, 11, 286, 500, 380, 528, 281, 2413, 309, 11, 597, 307, 2489, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17182671845848882, "compression_ratio": 1.8051470588235294, "no_speech_prob": 8.139478268276434e-06}, {"id": 53, "seek": 24316, "start": 243.16, "end": 247.4, "text": " Maybe, you know, if that's your intention upfront, no problem.", "tokens": [2704, 11, 291, 458, 11, 498, 300, 311, 428, 7789, 30264, 11, 572, 1154, 13], "temperature": 0.0, "avg_logprob": -0.13660901525746222, "compression_ratio": 1.647887323943662, "no_speech_prob": 3.9669371290074196e-06}, {"id": 54, "seek": 24316, "start": 247.4, "end": 255.84, "text": " But if your intention is to be a really effective deep learning practitioner, you need to finish", "tokens": [583, 498, 428, 7789, 307, 281, 312, 257, 534, 4942, 2452, 2539, 32125, 11, 291, 643, 281, 2413], "temperature": 0.0, "avg_logprob": -0.13660901525746222, "compression_ratio": 1.647887323943662, "no_speech_prob": 3.9669371290074196e-06}, {"id": 55, "seek": 24316, "start": 255.84, "end": 256.84, "text": " the damn course.", "tokens": [264, 8151, 1164, 13], "temperature": 0.0, "avg_logprob": -0.13660901525746222, "compression_ratio": 1.647887323943662, "no_speech_prob": 3.9669371290074196e-06}, {"id": 56, "seek": 24316, "start": 256.84, "end": 260.56, "text": " Okay, so put it in your head that that's your goal.", "tokens": [1033, 11, 370, 829, 309, 294, 428, 1378, 300, 300, 311, 428, 3387, 13], "temperature": 0.0, "avg_logprob": -0.13660901525746222, "compression_ratio": 1.647887323943662, "no_speech_prob": 3.9669371290074196e-06}, {"id": 57, "seek": 24316, "start": 260.56, "end": 263.64, "text": " Talk to your friends or your spouse and tell them that's my goal.", "tokens": [8780, 281, 428, 1855, 420, 428, 23013, 293, 980, 552, 300, 311, 452, 3387, 13], "temperature": 0.0, "avg_logprob": -0.13660901525746222, "compression_ratio": 1.647887323943662, "no_speech_prob": 3.9669371290074196e-06}, {"id": 58, "seek": 24316, "start": 263.64, "end": 267.48, "text": " Get that social pressure that you're going to finish it.", "tokens": [3240, 300, 2093, 3321, 300, 291, 434, 516, 281, 2413, 309, 13], "temperature": 0.0, "avg_logprob": -0.13660901525746222, "compression_ratio": 1.647887323943662, "no_speech_prob": 3.9669371290074196e-06}, {"id": 59, "seek": 26748, "start": 267.48, "end": 273.8, "text": " You're not just going to finish the course, but try to finish a project.", "tokens": [509, 434, 406, 445, 516, 281, 2413, 264, 1164, 11, 457, 853, 281, 2413, 257, 1716, 13], "temperature": 0.0, "avg_logprob": -0.1340054741388635, "compression_ratio": 1.49, "no_speech_prob": 2.1233045117696747e-06}, {"id": 60, "seek": 26748, "start": 273.8, "end": 277.78000000000003, "text": " So Christine McLevy is one of our fantastic alumni.", "tokens": [407, 24038, 4050, 11020, 11869, 307, 472, 295, 527, 5456, 16347, 13], "temperature": 0.0, "avg_logprob": -0.1340054741388635, "compression_ratio": 1.49, "no_speech_prob": 2.1233045117696747e-06}, {"id": 61, "seek": 26748, "start": 277.78000000000003, "end": 283.16, "text": " She's now at OpenAI, one of the world's top research organizations.", "tokens": [1240, 311, 586, 412, 7238, 48698, 11, 472, 295, 264, 1002, 311, 1192, 2132, 6150, 13], "temperature": 0.0, "avg_logprob": -0.1340054741388635, "compression_ratio": 1.49, "no_speech_prob": 2.1233045117696747e-06}, {"id": 62, "seek": 26748, "start": 283.16, "end": 288.82, "text": " She built a fantastic system for creating new music with deep learning.", "tokens": [1240, 3094, 257, 5456, 1185, 337, 4084, 777, 1318, 365, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.1340054741388635, "compression_ratio": 1.49, "no_speech_prob": 2.1233045117696747e-06}, {"id": 63, "seek": 26748, "start": 288.82, "end": 292.20000000000005, "text": " She used to be a pianist herself.", "tokens": [1240, 1143, 281, 312, 257, 32198, 468, 7530, 13], "temperature": 0.0, "avg_logprob": -0.1340054741388635, "compression_ratio": 1.49, "no_speech_prob": 2.1233045117696747e-06}, {"id": 64, "seek": 29220, "start": 292.2, "end": 301.71999999999997, "text": " And I remember this discussion, I told her focus on making one project great and polishing", "tokens": [400, 286, 1604, 341, 5017, 11, 286, 1907, 720, 1879, 322, 1455, 472, 1716, 869, 293, 47258], "temperature": 0.0, "avg_logprob": -0.15057975170659085, "compression_ratio": 1.4137931034482758, "no_speech_prob": 5.5938976402103435e-06}, {"id": 65, "seek": 29220, "start": 301.71999999999997, "end": 305.48, "text": " it off and finishing it.", "tokens": [309, 766, 293, 12693, 309, 13], "temperature": 0.0, "avg_logprob": -0.15057975170659085, "compression_ratio": 1.4137931034482758, "no_speech_prob": 5.5938976402103435e-06}, {"id": 66, "seek": 29220, "start": 305.48, "end": 307.2, "text": " And she did.", "tokens": [400, 750, 630, 13], "temperature": 0.0, "avg_logprob": -0.15057975170659085, "compression_ratio": 1.4137931034482758, "no_speech_prob": 5.5938976402103435e-06}, {"id": 67, "seek": 29220, "start": 307.2, "end": 314.48, "text": " And that project has ended up creating music which the BBC Orchestra played.", "tokens": [400, 300, 1716, 575, 4590, 493, 4084, 1318, 597, 264, 22669, 46692, 3737, 13], "temperature": 0.0, "avg_logprob": -0.15057975170659085, "compression_ratio": 1.4137931034482758, "no_speech_prob": 5.5938976402103435e-06}, {"id": 68, "seek": 31448, "start": 314.48, "end": 322.70000000000005, "text": " And amongst other things helped her get this extremely exclusive job at OpenAI.", "tokens": [400, 12918, 661, 721, 4254, 720, 483, 341, 4664, 13005, 1691, 412, 7238, 48698, 13], "temperature": 0.0, "avg_logprob": -0.15665801217622846, "compression_ratio": 1.6680497925311204, "no_speech_prob": 7.181977252912475e-06}, {"id": 69, "seek": 31448, "start": 322.70000000000005, "end": 328.8, "text": " So this is a clip from a podcast with one of our students, Sanyam, and Christine, in", "tokens": [407, 341, 307, 257, 7353, 490, 257, 7367, 365, 472, 295, 527, 1731, 11, 318, 1325, 335, 11, 293, 24038, 11, 294], "temperature": 0.0, "avg_logprob": -0.15665801217622846, "compression_ratio": 1.6680497925311204, "no_speech_prob": 7.181977252912475e-06}, {"id": 70, "seek": 31448, "start": 328.8, "end": 333.44, "text": " which Christine is saying this is one of her key insights.", "tokens": [597, 24038, 307, 1566, 341, 307, 472, 295, 720, 2141, 14310, 13], "temperature": 0.0, "avg_logprob": -0.15665801217622846, "compression_ratio": 1.6680497925311204, "no_speech_prob": 7.181977252912475e-06}, {"id": 71, "seek": 31448, "start": 333.44, "end": 337.6, "text": " And so I'm going to be giving you a few key insights, some of which are from me or some", "tokens": [400, 370, 286, 478, 516, 281, 312, 2902, 291, 257, 1326, 2141, 14310, 11, 512, 295, 597, 366, 490, 385, 420, 512], "temperature": 0.0, "avg_logprob": -0.15665801217622846, "compression_ratio": 1.6680497925311204, "no_speech_prob": 7.181977252912475e-06}, {"id": 72, "seek": 31448, "start": 337.6, "end": 339.64000000000004, "text": " of them are from me via students.", "tokens": [295, 552, 366, 490, 385, 5766, 1731, 13], "temperature": 0.0, "avg_logprob": -0.15665801217622846, "compression_ratio": 1.6680497925311204, "no_speech_prob": 7.181977252912475e-06}, {"id": 73, "seek": 31448, "start": 339.64000000000004, "end": 342.8, "text": " But they're all like things I've heard a bunch of times.", "tokens": [583, 436, 434, 439, 411, 721, 286, 600, 2198, 257, 3840, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.15665801217622846, "compression_ratio": 1.6680497925311204, "no_speech_prob": 7.181977252912475e-06}, {"id": 74, "seek": 34280, "start": 342.8, "end": 344.92, "text": " And so this is one example.", "tokens": [400, 370, 341, 307, 472, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1754420665984458, "compression_ratio": 1.555023923444976, "no_speech_prob": 8.938955033954699e-06}, {"id": 75, "seek": 34280, "start": 344.92, "end": 349.24, "text": " So finish the course and finish a project.", "tokens": [407, 2413, 264, 1164, 293, 2413, 257, 1716, 13], "temperature": 0.0, "avg_logprob": -0.1754420665984458, "compression_ratio": 1.555023923444976, "no_speech_prob": 8.938955033954699e-06}, {"id": 76, "seek": 34280, "start": 349.24, "end": 352.88, "text": " The project doesn't have to be something no one's ever built before.", "tokens": [440, 1716, 1177, 380, 362, 281, 312, 746, 572, 472, 311, 1562, 3094, 949, 13], "temperature": 0.0, "avg_logprob": -0.1754420665984458, "compression_ratio": 1.555023923444976, "no_speech_prob": 8.938955033954699e-06}, {"id": 77, "seek": 34280, "start": 352.88, "end": 356.28000000000003, "text": " Maybe it's just like, oh, I really love that thing that person built.", "tokens": [2704, 309, 311, 445, 411, 11, 1954, 11, 286, 534, 959, 300, 551, 300, 954, 3094, 13], "temperature": 0.0, "avg_logprob": -0.1754420665984458, "compression_ratio": 1.555023923444976, "no_speech_prob": 8.938955033954699e-06}, {"id": 78, "seek": 34280, "start": 356.28000000000003, "end": 360.88, "text": " Gosh, it would be a real stretch if I could build it too.", "tokens": [19185, 11, 309, 576, 312, 257, 957, 5985, 498, 286, 727, 1322, 309, 886, 13], "temperature": 0.0, "avg_logprob": -0.1754420665984458, "compression_ratio": 1.555023923444976, "no_speech_prob": 8.938955033954699e-06}, {"id": 79, "seek": 34280, "start": 360.88, "end": 363.0, "text": " You know, great.", "tokens": [509, 458, 11, 869, 13], "temperature": 0.0, "avg_logprob": -0.1754420665984458, "compression_ratio": 1.555023923444976, "no_speech_prob": 8.938955033954699e-06}, {"id": 80, "seek": 34280, "start": 363.0, "end": 365.44, "text": " Or it doesn't have to be world changing.", "tokens": [1610, 309, 1177, 380, 362, 281, 312, 1002, 4473, 13], "temperature": 0.0, "avg_logprob": -0.1754420665984458, "compression_ratio": 1.555023923444976, "no_speech_prob": 8.938955033954699e-06}, {"id": 81, "seek": 36544, "start": 365.44, "end": 373.16, "text": " You know, so one of our students built something for his fiance, which was a cousin recognizer.", "tokens": [509, 458, 11, 370, 472, 295, 527, 1731, 3094, 746, 337, 702, 46552, 11, 597, 390, 257, 16207, 3068, 6545, 13], "temperature": 0.0, "avg_logprob": -0.1446338425511899, "compression_ratio": 1.6490384615384615, "no_speech_prob": 1.833767419157084e-05}, {"id": 82, "seek": 36544, "start": 373.16, "end": 375.6, "text": " He had, I think, 14 cousins.", "tokens": [634, 632, 11, 286, 519, 11, 3499, 29246, 13], "temperature": 0.0, "avg_logprob": -0.1446338425511899, "compression_ratio": 1.6490384615384615, "no_speech_prob": 1.833767419157084e-05}, {"id": 83, "seek": 36544, "start": 375.6, "end": 380.02, "text": " And so his fiance could look at could take a picture of one of the cousins and it would", "tokens": [400, 370, 702, 46552, 727, 574, 412, 727, 747, 257, 3036, 295, 472, 295, 264, 29246, 293, 309, 576], "temperature": 0.0, "avg_logprob": -0.1446338425511899, "compression_ratio": 1.6490384615384615, "no_speech_prob": 1.833767419157084e-05}, {"id": 84, "seek": 36544, "start": 380.02, "end": 382.56, "text": " tell them which cousin it was.", "tokens": [980, 552, 597, 16207, 309, 390, 13], "temperature": 0.0, "avg_logprob": -0.1446338425511899, "compression_ratio": 1.6490384615384615, "no_speech_prob": 1.833767419157084e-05}, {"id": 85, "seek": 36544, "start": 382.56, "end": 384.15999999999997, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.1446338425511899, "compression_ratio": 1.6490384615384615, "no_speech_prob": 1.833767419157084e-05}, {"id": 86, "seek": 36544, "start": 384.15999999999997, "end": 391.92, "text": " In our first course, one of our students built the app for the Silicon Valley TV show, which", "tokens": [682, 527, 700, 1164, 11, 472, 295, 527, 1731, 3094, 264, 724, 337, 264, 25351, 10666, 3558, 855, 11, 597], "temperature": 0.0, "avg_logprob": -0.1446338425511899, "compression_ratio": 1.6490384615384615, "no_speech_prob": 1.833767419157084e-05}, {"id": 87, "seek": 39192, "start": 391.92, "end": 398.40000000000003, "text": " did Hot Dog or Not Hot Dog, which was actually a huge smash hit, like millions of downloads", "tokens": [630, 9423, 13472, 420, 1726, 9423, 13472, 11, 597, 390, 767, 257, 2603, 17960, 2045, 11, 411, 6803, 295, 36553], "temperature": 0.0, "avg_logprob": -0.161481906970342, "compression_ratio": 1.5550660792951543, "no_speech_prob": 1.8055150576401502e-05}, {"id": 88, "seek": 39192, "start": 398.40000000000003, "end": 400.16, "text": " that it was written about in the media.", "tokens": [300, 309, 390, 3720, 466, 294, 264, 3021, 13], "temperature": 0.0, "avg_logprob": -0.161481906970342, "compression_ratio": 1.5550660792951543, "no_speech_prob": 1.8055150576401502e-05}, {"id": 89, "seek": 39192, "start": 400.16, "end": 404.8, "text": " And it did exactly one thing just to tell you whether or not something was a hot dog.", "tokens": [400, 309, 630, 2293, 472, 551, 445, 281, 980, 291, 1968, 420, 406, 746, 390, 257, 2368, 3000, 13], "temperature": 0.0, "avg_logprob": -0.161481906970342, "compression_ratio": 1.5550660792951543, "no_speech_prob": 1.8055150576401502e-05}, {"id": 90, "seek": 39192, "start": 404.8, "end": 410.20000000000005, "text": " Anyway, or it could, you know, solve medicine.", "tokens": [5684, 11, 420, 309, 727, 11, 291, 458, 11, 5039, 7195, 13], "temperature": 0.0, "avg_logprob": -0.161481906970342, "compression_ratio": 1.5550660792951543, "no_speech_prob": 1.8055150576401502e-05}, {"id": 91, "seek": 39192, "start": 410.20000000000005, "end": 411.20000000000005, "text": " That would be fine too.", "tokens": [663, 576, 312, 2489, 886, 13], "temperature": 0.0, "avg_logprob": -0.161481906970342, "compression_ratio": 1.5550660792951543, "no_speech_prob": 1.8055150576401502e-05}, {"id": 92, "seek": 39192, "start": 411.20000000000005, "end": 415.64, "text": " I mean, whatever.", "tokens": [286, 914, 11, 2035, 13], "temperature": 0.0, "avg_logprob": -0.161481906970342, "compression_ratio": 1.5550660792951543, "no_speech_prob": 1.8055150576401502e-05}, {"id": 93, "seek": 39192, "start": 415.64, "end": 420.36, "text": " So finishing the course means being tenacious.", "tokens": [407, 12693, 264, 1164, 1355, 885, 2064, 22641, 13], "temperature": 0.0, "avg_logprob": -0.161481906970342, "compression_ratio": 1.5550660792951543, "no_speech_prob": 1.8055150576401502e-05}, {"id": 94, "seek": 42036, "start": 420.36, "end": 427.52000000000004, "text": " And one of the things I hear a lot is a lot of the approaches people learn as they do", "tokens": [400, 472, 295, 264, 721, 286, 1568, 257, 688, 307, 257, 688, 295, 264, 11587, 561, 1466, 382, 436, 360], "temperature": 0.0, "avg_logprob": -0.1534643274672488, "compression_ratio": 1.6533333333333333, "no_speech_prob": 4.425407496455591e-06}, {"id": 95, "seek": 42036, "start": 427.52000000000004, "end": 432.52000000000004, "text": " Fast AI around how to learn and how to study are useful more generally.", "tokens": [15968, 7318, 926, 577, 281, 1466, 293, 577, 281, 2979, 366, 4420, 544, 5101, 13], "temperature": 0.0, "avg_logprob": -0.1534643274672488, "compression_ratio": 1.6533333333333333, "no_speech_prob": 4.425407496455591e-06}, {"id": 96, "seek": 42036, "start": 432.52000000000004, "end": 438.48, "text": " And in fact, this is a quote from our book, the number one thing I see the difference", "tokens": [400, 294, 1186, 11, 341, 307, 257, 6513, 490, 527, 1446, 11, 264, 1230, 472, 551, 286, 536, 264, 2649], "temperature": 0.0, "avg_logprob": -0.1534643274672488, "compression_ratio": 1.6533333333333333, "no_speech_prob": 4.425407496455591e-06}, {"id": 97, "seek": 42036, "start": 438.48, "end": 443.48, "text": " between successful deep learning practitioners and not is tenacity.", "tokens": [1296, 4406, 2452, 2539, 25742, 293, 406, 307, 2064, 19008, 13], "temperature": 0.0, "avg_logprob": -0.1534643274672488, "compression_ratio": 1.6533333333333333, "no_speech_prob": 4.425407496455591e-06}, {"id": 98, "seek": 42036, "start": 443.48, "end": 444.48, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1534643274672488, "compression_ratio": 1.6533333333333333, "no_speech_prob": 4.425407496455591e-06}, {"id": 99, "seek": 42036, "start": 444.48, "end": 447.92, "text": " And tenacity is on the whole something you can choose.", "tokens": [400, 2064, 19008, 307, 322, 264, 1379, 746, 291, 393, 2826, 13], "temperature": 0.0, "avg_logprob": -0.1534643274672488, "compression_ratio": 1.6533333333333333, "no_speech_prob": 4.425407496455591e-06}, {"id": 100, "seek": 44792, "start": 447.92, "end": 453.08000000000004, "text": " Now something you can't choose is whether you find yourself in the middle of a global", "tokens": [823, 746, 291, 393, 380, 2826, 307, 1968, 291, 915, 1803, 294, 264, 2808, 295, 257, 4338], "temperature": 0.0, "avg_logprob": -0.14612983153747008, "compression_ratio": 1.6626016260162602, "no_speech_prob": 3.905349331034813e-06}, {"id": 101, "seek": 44792, "start": 453.08000000000004, "end": 459.8, "text": " pandemic or, you know, somebody in your family dies or you come down with a terrible cold", "tokens": [5388, 420, 11, 291, 458, 11, 2618, 294, 428, 1605, 2714, 420, 291, 808, 760, 365, 257, 6237, 3554], "temperature": 0.0, "avg_logprob": -0.14612983153747008, "compression_ratio": 1.6626016260162602, "no_speech_prob": 3.905349331034813e-06}, {"id": 102, "seek": 44792, "start": 459.8, "end": 462.12, "text": " or whatever like obstacles happen.", "tokens": [420, 2035, 411, 17735, 1051, 13], "temperature": 0.0, "avg_logprob": -0.14612983153747008, "compression_ratio": 1.6626016260162602, "no_speech_prob": 3.905349331034813e-06}, {"id": 103, "seek": 44792, "start": 462.12, "end": 463.12, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.14612983153747008, "compression_ratio": 1.6626016260162602, "no_speech_prob": 3.905349331034813e-06}, {"id": 104, "seek": 44792, "start": 463.12, "end": 467.76, "text": " And so part of being tenacious is being understanding with yourself.", "tokens": [400, 370, 644, 295, 885, 2064, 22641, 307, 885, 3701, 365, 1803, 13], "temperature": 0.0, "avg_logprob": -0.14612983153747008, "compression_ratio": 1.6626016260162602, "no_speech_prob": 3.905349331034813e-06}, {"id": 105, "seek": 44792, "start": 467.76, "end": 468.76, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.14612983153747008, "compression_ratio": 1.6626016260162602, "no_speech_prob": 3.905349331034813e-06}, {"id": 106, "seek": 44792, "start": 468.76, "end": 470.48, "text": " And saying, okay, something's happened.", "tokens": [400, 1566, 11, 1392, 11, 746, 311, 2011, 13], "temperature": 0.0, "avg_logprob": -0.14612983153747008, "compression_ratio": 1.6626016260162602, "no_speech_prob": 3.905349331034813e-06}, {"id": 107, "seek": 44792, "start": 470.48, "end": 472.52000000000004, "text": " I can't do what I hope to do right now.", "tokens": [286, 393, 380, 360, 437, 286, 1454, 281, 360, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.14612983153747008, "compression_ratio": 1.6626016260162602, "no_speech_prob": 3.905349331034813e-06}, {"id": 108, "seek": 44792, "start": 472.52000000000004, "end": 474.40000000000003, "text": " But then getting back to it.", "tokens": [583, 550, 1242, 646, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.14612983153747008, "compression_ratio": 1.6626016260162602, "no_speech_prob": 3.905349331034813e-06}, {"id": 109, "seek": 44792, "start": 474.40000000000003, "end": 475.40000000000003, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.14612983153747008, "compression_ratio": 1.6626016260162602, "no_speech_prob": 3.905349331034813e-06}, {"id": 110, "seek": 47540, "start": 475.4, "end": 481.64, "text": " So tenacity is not about ignoring the bumps, but keeping going after the bumps.", "tokens": [407, 2064, 19008, 307, 406, 466, 26258, 264, 27719, 11, 457, 5145, 516, 934, 264, 27719, 13], "temperature": 0.0, "avg_logprob": -0.18559226655123526, "compression_ratio": 1.6844262295081966, "no_speech_prob": 6.747865427314537e-06}, {"id": 111, "seek": 47540, "start": 481.64, "end": 486.28, "text": " And maybe that's, you know, quite often I'll have a bump that's like a year long.", "tokens": [400, 1310, 300, 311, 11, 291, 458, 11, 1596, 2049, 286, 603, 362, 257, 9961, 300, 311, 411, 257, 1064, 938, 13], "temperature": 0.0, "avg_logprob": -0.18559226655123526, "compression_ratio": 1.6844262295081966, "no_speech_prob": 6.747865427314537e-06}, {"id": 112, "seek": 47540, "start": 486.28, "end": 487.28, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.18559226655123526, "compression_ratio": 1.6844262295081966, "no_speech_prob": 6.747865427314537e-06}, {"id": 113, "seek": 47540, "start": 487.28, "end": 491.32, "text": " But if I've decided to finish something, you know, at the end of that year, I'll go back", "tokens": [583, 498, 286, 600, 3047, 281, 2413, 746, 11, 291, 458, 11, 412, 264, 917, 295, 300, 1064, 11, 286, 603, 352, 646], "temperature": 0.0, "avg_logprob": -0.18559226655123526, "compression_ratio": 1.6844262295081966, "no_speech_prob": 6.747865427314537e-06}, {"id": 114, "seek": 47540, "start": 491.32, "end": 494.32, "text": " and finish it.", "tokens": [293, 2413, 309, 13], "temperature": 0.0, "avg_logprob": -0.18559226655123526, "compression_ratio": 1.6844262295081966, "no_speech_prob": 6.747865427314537e-06}, {"id": 115, "seek": 47540, "start": 494.32, "end": 499.28, "text": " So sometimes that involves me emailing somebody more than a year after they've sent me something", "tokens": [407, 2171, 300, 11626, 385, 3796, 278, 2618, 544, 813, 257, 1064, 934, 436, 600, 2279, 385, 746], "temperature": 0.0, "avg_logprob": -0.18559226655123526, "compression_ratio": 1.6844262295081966, "no_speech_prob": 6.747865427314537e-06}, {"id": 116, "seek": 47540, "start": 499.28, "end": 501.32, "text": " and saying, okay, I'm ready to reply now.", "tokens": [293, 1566, 11, 1392, 11, 286, 478, 1919, 281, 16972, 586, 13], "temperature": 0.0, "avg_logprob": -0.18559226655123526, "compression_ratio": 1.6844262295081966, "no_speech_prob": 6.747865427314537e-06}, {"id": 117, "seek": 50132, "start": 501.32, "end": 505.88, "text": " And they forgot that they even sent me an email.", "tokens": [400, 436, 5298, 300, 436, 754, 2279, 385, 364, 3796, 13], "temperature": 0.0, "avg_logprob": -0.15877599214252672, "compression_ratio": 1.5093457943925233, "no_speech_prob": 4.157311650487827e-06}, {"id": 118, "seek": 50132, "start": 505.88, "end": 507.71999999999997, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.15877599214252672, "compression_ratio": 1.5093457943925233, "no_speech_prob": 4.157311650487827e-06}, {"id": 119, "seek": 50132, "start": 507.71999999999997, "end": 512.72, "text": " So what I'm going to do now is I'm going to share with you a bunch of insights from this", "tokens": [407, 437, 286, 478, 516, 281, 360, 586, 307, 286, 478, 516, 281, 2073, 365, 291, 257, 3840, 295, 14310, 490, 341], "temperature": 0.0, "avg_logprob": -0.15877599214252672, "compression_ratio": 1.5093457943925233, "no_speech_prob": 4.157311650487827e-06}, {"id": 120, "seek": 50132, "start": 512.72, "end": 515.16, "text": " book called Meta Learning.", "tokens": [1446, 1219, 6377, 64, 15205, 13], "temperature": 0.0, "avg_logprob": -0.15877599214252672, "compression_ratio": 1.5093457943925233, "no_speech_prob": 4.157311650487827e-06}, {"id": 121, "seek": 50132, "start": 515.16, "end": 517.0, "text": " If you haven't seen it before, that's okay.", "tokens": [759, 291, 2378, 380, 1612, 309, 949, 11, 300, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.15877599214252672, "compression_ratio": 1.5093457943925233, "no_speech_prob": 4.157311650487827e-06}, {"id": 122, "seek": 50132, "start": 517.0, "end": 525.24, "text": " It came out yesterday and it was written by a guy called Redek, who is one of the top", "tokens": [467, 1361, 484, 5186, 293, 309, 390, 3720, 538, 257, 2146, 1219, 4477, 916, 11, 567, 307, 472, 295, 264, 1192], "temperature": 0.0, "avg_logprob": -0.15877599214252672, "compression_ratio": 1.5093457943925233, "no_speech_prob": 4.157311650487827e-06}, {"id": 123, "seek": 50132, "start": 525.24, "end": 528.36, "text": " alumni of this course.", "tokens": [16347, 295, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.15877599214252672, "compression_ratio": 1.5093457943925233, "no_speech_prob": 4.157311650487827e-06}, {"id": 124, "seek": 52836, "start": 528.36, "end": 535.32, "text": " And it's a book well worth reading because his journey is extraordinary.", "tokens": [400, 309, 311, 257, 1446, 731, 3163, 3760, 570, 702, 4671, 307, 10581, 13], "temperature": 0.0, "avg_logprob": -0.10001788717327696, "compression_ratio": 1.457142857142857, "no_speech_prob": 3.6113467558607226e-06}, {"id": 125, "seek": 52836, "start": 535.32, "end": 543.48, "text": " You know, this is a guy without a degree who couldn't code just a few years ago with a", "tokens": [509, 458, 11, 341, 307, 257, 2146, 1553, 257, 4314, 567, 2809, 380, 3089, 445, 257, 1326, 924, 2057, 365, 257], "temperature": 0.0, "avg_logprob": -0.10001788717327696, "compression_ratio": 1.457142857142857, "no_speech_prob": 3.6113467558607226e-06}, {"id": 126, "seek": 52836, "start": 543.48, "end": 546.6800000000001, "text": " job that he found boring.", "tokens": [1691, 300, 415, 1352, 9989, 13], "temperature": 0.0, "avg_logprob": -0.10001788717327696, "compression_ratio": 1.457142857142857, "no_speech_prob": 3.6113467558607226e-06}, {"id": 127, "seek": 52836, "start": 546.6800000000001, "end": 555.6, "text": " And he set out to learn deep learning and repeatedly failed to do so.", "tokens": [400, 415, 992, 484, 281, 1466, 2452, 2539, 293, 18227, 7612, 281, 360, 370, 13], "temperature": 0.0, "avg_logprob": -0.10001788717327696, "compression_ratio": 1.457142857142857, "no_speech_prob": 3.6113467558607226e-06}, {"id": 128, "seek": 55560, "start": 555.6, "end": 562.6800000000001, "text": " But Redek is extremely tenacious and each time he failed to do so, he tried again.", "tokens": [583, 4477, 916, 307, 4664, 2064, 22641, 293, 1184, 565, 415, 7612, 281, 360, 370, 11, 415, 3031, 797, 13], "temperature": 0.0, "avg_logprob": -0.13591100040235018, "compression_ratio": 1.4866310160427807, "no_speech_prob": 7.294189344975166e-06}, {"id": 129, "seek": 55560, "start": 562.6800000000001, "end": 565.12, "text": " And eventually he figured out a way to do it.", "tokens": [400, 4728, 415, 8932, 484, 257, 636, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.13591100040235018, "compression_ratio": 1.4866310160427807, "no_speech_prob": 7.294189344975166e-06}, {"id": 130, "seek": 55560, "start": 565.12, "end": 572.8000000000001, "text": " And the way he did it was very intensely based on fast AI, both the course and the philosophy", "tokens": [400, 264, 636, 415, 630, 309, 390, 588, 43235, 2361, 322, 2370, 7318, 11, 1293, 264, 1164, 293, 264, 10675], "temperature": 0.0, "avg_logprob": -0.13591100040235018, "compression_ratio": 1.4866310160427807, "no_speech_prob": 7.294189344975166e-06}, {"id": 131, "seek": 55560, "start": 572.8000000000001, "end": 574.2, "text": " of learning.", "tokens": [295, 2539, 13], "temperature": 0.0, "avg_logprob": -0.13591100040235018, "compression_ratio": 1.4866310160427807, "no_speech_prob": 7.294189344975166e-06}, {"id": 132, "seek": 55560, "start": 574.2, "end": 579.48, "text": " And he is now a Kaggle competition winner.", "tokens": [400, 415, 307, 586, 257, 48751, 22631, 6211, 8507, 13], "temperature": 0.0, "avg_logprob": -0.13591100040235018, "compression_ratio": 1.4866310160427807, "no_speech_prob": 7.294189344975166e-06}, {"id": 133, "seek": 57948, "start": 579.48, "end": 587.44, "text": " He was the only non San Francisco person at QAI, which is one of the world's top medical", "tokens": [634, 390, 264, 787, 2107, 5271, 12279, 954, 412, 1249, 48698, 11, 597, 307, 472, 295, 264, 1002, 311, 1192, 4625], "temperature": 0.0, "avg_logprob": -0.15947565825089163, "compression_ratio": 1.518181818181818, "no_speech_prob": 4.092526523891138e-06}, {"id": 134, "seek": 57948, "start": 587.44, "end": 588.52, "text": " AI startups.", "tokens": [7318, 28041, 13], "temperature": 0.0, "avg_logprob": -0.15947565825089163, "compression_ratio": 1.518181818181818, "no_speech_prob": 4.092526523891138e-06}, {"id": 135, "seek": 57948, "start": 588.52, "end": 598.72, "text": " And now he works at a new nonprofit that is literally trying to translate animal language.", "tokens": [400, 586, 415, 1985, 412, 257, 777, 23348, 300, 307, 3736, 1382, 281, 13799, 5496, 2856, 13], "temperature": 0.0, "avg_logprob": -0.15947565825089163, "compression_ratio": 1.518181818181818, "no_speech_prob": 4.092526523891138e-06}, {"id": 136, "seek": 57948, "start": 598.72, "end": 600.9200000000001, "text": " So he's kind of a good example.", "tokens": [407, 415, 311, 733, 295, 257, 665, 1365, 13], "temperature": 0.0, "avg_logprob": -0.15947565825089163, "compression_ratio": 1.518181818181818, "no_speech_prob": 4.092526523891138e-06}, {"id": 137, "seek": 57948, "start": 600.9200000000001, "end": 602.8000000000001, "text": " I always think it's a good idea to have a role model.", "tokens": [286, 1009, 519, 309, 311, 257, 665, 1558, 281, 362, 257, 3090, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15947565825089163, "compression_ratio": 1.518181818181818, "no_speech_prob": 4.092526523891138e-06}, {"id": 138, "seek": 57948, "start": 602.8000000000001, "end": 605.8000000000001, "text": " In the fast AI community, there's a lot of role models.", "tokens": [682, 264, 2370, 7318, 1768, 11, 456, 311, 257, 688, 295, 3090, 5245, 13], "temperature": 0.0, "avg_logprob": -0.15947565825089163, "compression_ratio": 1.518181818181818, "no_speech_prob": 4.092526523891138e-06}, {"id": 139, "seek": 60580, "start": 605.8, "end": 611.28, "text": " And so here's somebody who's like both a role model for like trying, failing, trying, failing,", "tokens": [400, 370, 510, 311, 2618, 567, 311, 411, 1293, 257, 3090, 2316, 337, 411, 1382, 11, 18223, 11, 1382, 11, 18223, 11], "temperature": 0.0, "avg_logprob": -0.11359336972236633, "compression_ratio": 1.7230769230769232, "no_speech_prob": 2.1777424990432337e-05}, {"id": 140, "seek": 60580, "start": 611.28, "end": 615.64, "text": " trying, failing, and then, you know, finding some success.", "tokens": [1382, 11, 18223, 11, 293, 550, 11, 291, 458, 11, 5006, 512, 2245, 13], "temperature": 0.0, "avg_logprob": -0.11359336972236633, "compression_ratio": 1.7230769230769232, "no_speech_prob": 2.1777424990432337e-05}, {"id": 141, "seek": 60580, "start": 615.64, "end": 621.12, "text": " And so I'm going to show you some things from his book.", "tokens": [400, 370, 286, 478, 516, 281, 855, 291, 512, 721, 490, 702, 1446, 13], "temperature": 0.0, "avg_logprob": -0.11359336972236633, "compression_ratio": 1.7230769230769232, "no_speech_prob": 2.1777424990432337e-05}, {"id": 142, "seek": 60580, "start": 621.12, "end": 628.4799999999999, "text": " And a lot of his book is him taking stuff I say and kind of casting it into what he", "tokens": [400, 257, 688, 295, 702, 1446, 307, 796, 1940, 1507, 286, 584, 293, 733, 295, 17301, 309, 666, 437, 415], "temperature": 0.0, "avg_logprob": -0.11359336972236633, "compression_ratio": 1.7230769230769232, "no_speech_prob": 2.1777424990432337e-05}, {"id": 143, "seek": 60580, "start": 628.4799999999999, "end": 629.4799999999999, "text": " took away from it.", "tokens": [1890, 1314, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.11359336972236633, "compression_ratio": 1.7230769230769232, "no_speech_prob": 2.1777424990432337e-05}, {"id": 144, "seek": 60580, "start": 629.4799999999999, "end": 632.68, "text": " Some of it's his ideas.", "tokens": [2188, 295, 309, 311, 702, 3487, 13], "temperature": 0.0, "avg_logprob": -0.11359336972236633, "compression_ratio": 1.7230769230769232, "no_speech_prob": 2.1777424990432337e-05}, {"id": 145, "seek": 63268, "start": 632.68, "end": 641.3599999999999, "text": " So one of the things we hear again and again from unsuccessful deep learning students is", "tokens": [407, 472, 295, 264, 721, 321, 1568, 797, 293, 797, 490, 46258, 2452, 2539, 1731, 307], "temperature": 0.0, "avg_logprob": -0.11573575337727865, "compression_ratio": 1.9565217391304348, "no_speech_prob": 6.642943390033906e-06}, {"id": 146, "seek": 63268, "start": 641.3599999999999, "end": 647.14, "text": " they keep preparing to do deep learning and they keep preparing to do projects.", "tokens": [436, 1066, 10075, 281, 360, 2452, 2539, 293, 436, 1066, 10075, 281, 360, 4455, 13], "temperature": 0.0, "avg_logprob": -0.11573575337727865, "compression_ratio": 1.9565217391304348, "no_speech_prob": 6.642943390033906e-06}, {"id": 147, "seek": 63268, "start": 647.14, "end": 649.5999999999999, "text": " So they study linear algebra.", "tokens": [407, 436, 2979, 8213, 21989, 13], "temperature": 0.0, "avg_logprob": -0.11573575337727865, "compression_ratio": 1.9565217391304348, "no_speech_prob": 6.642943390033906e-06}, {"id": 148, "seek": 63268, "start": 649.5999999999999, "end": 651.1999999999999, "text": " They study calculus.", "tokens": [814, 2979, 33400, 13], "temperature": 0.0, "avg_logprob": -0.11573575337727865, "compression_ratio": 1.9565217391304348, "no_speech_prob": 6.642943390033906e-06}, {"id": 149, "seek": 63268, "start": 651.1999999999999, "end": 653.4599999999999, "text": " They study C++.", "tokens": [814, 2979, 383, 25472, 13], "temperature": 0.0, "avg_logprob": -0.11573575337727865, "compression_ratio": 1.9565217391304348, "no_speech_prob": 6.642943390033906e-06}, {"id": 150, "seek": 63268, "start": 653.4599999999999, "end": 656.1999999999999, "text": " They study all these different things.", "tokens": [814, 2979, 439, 613, 819, 721, 13], "temperature": 0.0, "avg_logprob": -0.11573575337727865, "compression_ratio": 1.9565217391304348, "no_speech_prob": 6.642943390033906e-06}, {"id": 151, "seek": 63268, "start": 656.1999999999999, "end": 661.3199999999999, "text": " They do a MOOC and then another MOOC and then they read a book and then another book.", "tokens": [814, 360, 257, 49197, 34, 293, 550, 1071, 49197, 34, 293, 550, 436, 1401, 257, 1446, 293, 550, 1071, 1446, 13], "temperature": 0.0, "avg_logprob": -0.11573575337727865, "compression_ratio": 1.9565217391304348, "no_speech_prob": 6.642943390033906e-06}, {"id": 152, "seek": 66132, "start": 661.32, "end": 665.44, "text": " You know, and at what point are they actually going to start doing something?", "tokens": [509, 458, 11, 293, 412, 437, 935, 366, 436, 767, 516, 281, 722, 884, 746, 30], "temperature": 0.0, "avg_logprob": -0.17159965303209093, "compression_ratio": 1.618811881188119, "no_speech_prob": 8.267619705293328e-06}, {"id": 153, "seek": 66132, "start": 665.44, "end": 670.2800000000001, "text": " So the fast AI philosophy is you start doing something week one.", "tokens": [407, 264, 2370, 7318, 10675, 307, 291, 722, 884, 746, 1243, 472, 13], "temperature": 0.0, "avg_logprob": -0.17159965303209093, "compression_ratio": 1.618811881188119, "no_speech_prob": 8.267619705293328e-06}, {"id": 154, "seek": 66132, "start": 670.2800000000001, "end": 676.32, "text": " OK, so week one, you need to actually train a model.", "tokens": [2264, 11, 370, 1243, 472, 11, 291, 643, 281, 767, 3847, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17159965303209093, "compression_ratio": 1.618811881188119, "no_speech_prob": 8.267619705293328e-06}, {"id": 155, "seek": 66132, "start": 676.32, "end": 683.34, "text": " OK, which is not to say that you're not going to learn theory.", "tokens": [2264, 11, 597, 307, 406, 281, 584, 300, 291, 434, 406, 516, 281, 1466, 5261, 13], "temperature": 0.0, "avg_logprob": -0.17159965303209093, "compression_ratio": 1.618811881188119, "no_speech_prob": 8.267619705293328e-06}, {"id": 156, "seek": 66132, "start": 683.34, "end": 688.5200000000001, "text": " You will, right, as needed in the context of getting stuff done.", "tokens": [509, 486, 11, 558, 11, 382, 2978, 294, 264, 4319, 295, 1242, 1507, 1096, 13], "temperature": 0.0, "avg_logprob": -0.17159965303209093, "compression_ratio": 1.618811881188119, "no_speech_prob": 8.267619705293328e-06}, {"id": 157, "seek": 66132, "start": 688.5200000000001, "end": 689.5200000000001, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.17159965303209093, "compression_ratio": 1.618811881188119, "no_speech_prob": 8.267619705293328e-06}, {"id": 158, "seek": 68952, "start": 689.52, "end": 696.96, "text": " So if you do finish it, right, particularly if you finish the full two parts of the course,", "tokens": [407, 498, 291, 360, 2413, 309, 11, 558, 11, 4098, 498, 291, 2413, 264, 1577, 732, 3166, 295, 264, 1164, 11], "temperature": 0.0, "avg_logprob": -0.14642511232934816, "compression_ratio": 1.6877470355731226, "no_speech_prob": 1.473740667279344e-05}, {"id": 159, "seek": 68952, "start": 696.96, "end": 702.84, "text": " right, you'll have implemented basically all of fast AI's library just about from scratch.", "tokens": [558, 11, 291, 603, 362, 12270, 1936, 439, 295, 2370, 7318, 311, 6405, 445, 466, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.14642511232934816, "compression_ratio": 1.6877470355731226, "no_speech_prob": 1.473740667279344e-05}, {"id": 160, "seek": 68952, "start": 702.84, "end": 705.1999999999999, "text": " You'll know all about batch normalization.", "tokens": [509, 603, 458, 439, 466, 15245, 2710, 2144, 13], "temperature": 0.0, "avg_logprob": -0.14642511232934816, "compression_ratio": 1.6877470355731226, "no_speech_prob": 1.473740667279344e-05}, {"id": 161, "seek": 68952, "start": 705.1999999999999, "end": 709.36, "text": " You'll have benchmarked various matrix multiplication approaches.", "tokens": [509, 603, 362, 18927, 292, 3683, 8141, 27290, 11587, 13], "temperature": 0.0, "avg_logprob": -0.14642511232934816, "compression_ratio": 1.6877470355731226, "no_speech_prob": 1.473740667279344e-05}, {"id": 162, "seek": 68952, "start": 709.36, "end": 713.24, "text": " You'll know how to write bare metal GPU optimized code.", "tokens": [509, 603, 458, 577, 281, 2464, 6949, 5760, 18407, 26941, 3089, 13], "temperature": 0.0, "avg_logprob": -0.14642511232934816, "compression_ratio": 1.6877470355731226, "no_speech_prob": 1.473740667279344e-05}, {"id": 163, "seek": 68952, "start": 713.24, "end": 719.28, "text": " You'll understand how to do back propagation and calculus of that from scratch.", "tokens": [509, 603, 1223, 577, 281, 360, 646, 38377, 293, 33400, 295, 300, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.14642511232934816, "compression_ratio": 1.6877470355731226, "no_speech_prob": 1.473740667279344e-05}, {"id": 164, "seek": 71928, "start": 719.28, "end": 720.48, "text": " You'll do all of that.", "tokens": [509, 603, 360, 439, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.14039056054477034, "compression_ratio": 1.6074766355140186, "no_speech_prob": 2.947989059975953e-06}, {"id": 165, "seek": 71928, "start": 720.48, "end": 726.04, "text": " OK, but it will all be as you go along in the context of like solving a particular problem", "tokens": [2264, 11, 457, 309, 486, 439, 312, 382, 291, 352, 2051, 294, 264, 4319, 295, 411, 12606, 257, 1729, 1154], "temperature": 0.0, "avg_logprob": -0.14039056054477034, "compression_ratio": 1.6074766355140186, "no_speech_prob": 2.947989059975953e-06}, {"id": 166, "seek": 71928, "start": 726.04, "end": 730.16, "text": " or understanding the next piece of the puzzle.", "tokens": [420, 3701, 264, 958, 2522, 295, 264, 12805, 13], "temperature": 0.0, "avg_logprob": -0.14039056054477034, "compression_ratio": 1.6074766355140186, "no_speech_prob": 2.947989059975953e-06}, {"id": 167, "seek": 71928, "start": 730.16, "end": 738.6, "text": " So yeah, really just reading books and watching videos is not going to get you there.", "tokens": [407, 1338, 11, 534, 445, 3760, 3642, 293, 1976, 2145, 307, 406, 516, 281, 483, 291, 456, 13], "temperature": 0.0, "avg_logprob": -0.14039056054477034, "compression_ratio": 1.6074766355140186, "no_speech_prob": 2.947989059975953e-06}, {"id": 168, "seek": 71928, "start": 738.6, "end": 744.28, "text": " The thing which is going to get you there is writing code, doing experiments and training", "tokens": [440, 551, 597, 307, 516, 281, 483, 291, 456, 307, 3579, 3089, 11, 884, 12050, 293, 3097], "temperature": 0.0, "avg_logprob": -0.14039056054477034, "compression_ratio": 1.6074766355140186, "no_speech_prob": 2.947989059975953e-06}, {"id": 169, "seek": 71928, "start": 744.28, "end": 745.8, "text": " models.", "tokens": [5245, 13], "temperature": 0.0, "avg_logprob": -0.14039056054477034, "compression_ratio": 1.6074766355140186, "no_speech_prob": 2.947989059975953e-06}, {"id": 170, "seek": 74580, "start": 745.8, "end": 749.4799999999999, "text": " Some of you might not be that great at coding.", "tokens": [2188, 295, 291, 1062, 406, 312, 300, 869, 412, 17720, 13], "temperature": 0.0, "avg_logprob": -0.11999034881591797, "compression_ratio": 1.6356275303643724, "no_speech_prob": 3.393100996618159e-06}, {"id": 171, "seek": 74580, "start": 749.4799999999999, "end": 750.4799999999999, "text": " Fine.", "tokens": [12024, 13], "temperature": 0.0, "avg_logprob": -0.11999034881591797, "compression_ratio": 1.6356275303643724, "no_speech_prob": 3.393100996618159e-06}, {"id": 172, "seek": 74580, "start": 750.4799999999999, "end": 754.4399999999999, "text": " OK, that's that's a perfectly OK place to be.", "tokens": [2264, 11, 300, 311, 300, 311, 257, 6239, 2264, 1081, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.11999034881591797, "compression_ratio": 1.6356275303643724, "no_speech_prob": 3.393100996618159e-06}, {"id": 173, "seek": 74580, "start": 754.4399999999999, "end": 760.9599999999999, "text": " And but you guys are going to find it the most challenging because being good at coding", "tokens": [400, 457, 291, 1074, 366, 516, 281, 915, 309, 264, 881, 7595, 570, 885, 665, 412, 17720], "temperature": 0.0, "avg_logprob": -0.11999034881591797, "compression_ratio": 1.6356275303643724, "no_speech_prob": 3.393100996618159e-06}, {"id": 174, "seek": 74580, "start": 760.9599999999999, "end": 763.1999999999999, "text": " is the thing that lets you zip through quickly.", "tokens": [307, 264, 551, 300, 6653, 291, 20730, 807, 2661, 13], "temperature": 0.0, "avg_logprob": -0.11999034881591797, "compression_ratio": 1.6356275303643724, "no_speech_prob": 3.393100996618159e-06}, {"id": 175, "seek": 74580, "start": 763.1999999999999, "end": 765.88, "text": " So rather than think, oh, that's a shame.", "tokens": [407, 2831, 813, 519, 11, 1954, 11, 300, 311, 257, 10069, 13], "temperature": 0.0, "avg_logprob": -0.11999034881591797, "compression_ratio": 1.6356275303643724, "no_speech_prob": 3.393100996618159e-06}, {"id": 176, "seek": 74580, "start": 765.88, "end": 767.24, "text": " I'm not that good at coding yet.", "tokens": [286, 478, 406, 300, 665, 412, 17720, 1939, 13], "temperature": 0.0, "avg_logprob": -0.11999034881591797, "compression_ratio": 1.6356275303643724, "no_speech_prob": 3.393100996618159e-06}, {"id": 177, "seek": 74580, "start": 767.24, "end": 773.1999999999999, "text": " This is actually an opportunity because now you have a really fun project to learn to", "tokens": [639, 307, 767, 364, 2650, 570, 586, 291, 362, 257, 534, 1019, 1716, 281, 1466, 281], "temperature": 0.0, "avg_logprob": -0.11999034881591797, "compression_ratio": 1.6356275303643724, "no_speech_prob": 3.393100996618159e-06}, {"id": 178, "seek": 74580, "start": 773.1999999999999, "end": 774.1999999999999, "text": " code in.", "tokens": [3089, 294, 13], "temperature": 0.0, "avg_logprob": -0.11999034881591797, "compression_ratio": 1.6356275303643724, "no_speech_prob": 3.393100996618159e-06}, {"id": 179, "seek": 77420, "start": 774.2, "end": 780.88, "text": " So a lot of people have become good coders by doing the course, because as you do the", "tokens": [407, 257, 688, 295, 561, 362, 1813, 665, 17656, 433, 538, 884, 264, 1164, 11, 570, 382, 291, 360, 264], "temperature": 0.0, "avg_logprob": -0.13926004559806224, "compression_ratio": 1.7824074074074074, "no_speech_prob": 7.071698291838402e-06}, {"id": 180, "seek": 77420, "start": 780.88, "end": 786.0400000000001, "text": " course, you'll learn about a lot of computer science concepts like object oriented programming", "tokens": [1164, 11, 291, 603, 1466, 466, 257, 688, 295, 3820, 3497, 10392, 411, 2657, 21841, 9410], "temperature": 0.0, "avg_logprob": -0.13926004559806224, "compression_ratio": 1.7824074074074074, "no_speech_prob": 7.071698291838402e-06}, {"id": 181, "seek": 77420, "start": 786.0400000000001, "end": 793.32, "text": " and functional programming and mapping over a list and list comprehensions and GPU acceleration", "tokens": [293, 11745, 9410, 293, 18350, 670, 257, 1329, 293, 1329, 10753, 8302, 293, 18407, 17162], "temperature": 0.0, "avg_logprob": -0.13926004559806224, "compression_ratio": 1.7824074074074074, "no_speech_prob": 7.071698291838402e-06}, {"id": 182, "seek": 77420, "start": 793.32, "end": 795.2, "text": " and so on and so forth.", "tokens": [293, 370, 322, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.13926004559806224, "compression_ratio": 1.7824074074074074, "no_speech_prob": 7.071698291838402e-06}, {"id": 183, "seek": 77420, "start": 795.2, "end": 800.9200000000001, "text": " So the thing is, though, if you're not if you come across a computer science concept", "tokens": [407, 264, 551, 307, 11, 1673, 11, 498, 291, 434, 406, 498, 291, 808, 2108, 257, 3820, 3497, 3410], "temperature": 0.0, "avg_logprob": -0.13926004559806224, "compression_ratio": 1.7824074074074074, "no_speech_prob": 7.071698291838402e-06}, {"id": 184, "seek": 80092, "start": 800.92, "end": 805.76, "text": " or a programming idea or a piece of syntax that you're not that familiar with, that's", "tokens": [420, 257, 9410, 1558, 420, 257, 2522, 295, 28431, 300, 291, 434, 406, 300, 4963, 365, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.11280212980328184, "compression_ratio": 1.6968503937007875, "no_speech_prob": 4.637766778614605e-06}, {"id": 185, "seek": 80092, "start": 805.76, "end": 811.4399999999999, "text": " a place it's worth pausing for a moment and making sure that you know that you do understand", "tokens": [257, 1081, 309, 311, 3163, 2502, 7981, 337, 257, 1623, 293, 1455, 988, 300, 291, 458, 300, 291, 360, 1223], "temperature": 0.0, "avg_logprob": -0.11280212980328184, "compression_ratio": 1.6968503937007875, "no_speech_prob": 4.637766778614605e-06}, {"id": 186, "seek": 80092, "start": 811.4399999999999, "end": 818.88, "text": " how that code works, because the code coding is the kind of critical foundational skill.", "tokens": [577, 300, 3089, 1985, 11, 570, 264, 3089, 17720, 307, 264, 733, 295, 4924, 32195, 5389, 13], "temperature": 0.0, "avg_logprob": -0.11280212980328184, "compression_ratio": 1.6968503937007875, "no_speech_prob": 4.637766778614605e-06}, {"id": 187, "seek": 80092, "start": 818.88, "end": 823.28, "text": " This is a pretty good course for getting started with basic computer science.", "tokens": [639, 307, 257, 1238, 665, 1164, 337, 1242, 1409, 365, 3875, 3820, 3497, 13], "temperature": 0.0, "avg_logprob": -0.11280212980328184, "compression_ratio": 1.6968503937007875, "no_speech_prob": 4.637766778614605e-06}, {"id": 188, "seek": 80092, "start": 823.28, "end": 829.16, "text": " It's Harvard CS50 course, which everybody at Harvard does for computer science to get", "tokens": [467, 311, 13378, 9460, 2803, 1164, 11, 597, 2201, 412, 13378, 775, 337, 3820, 3497, 281, 483], "temperature": 0.0, "avg_logprob": -0.11280212980328184, "compression_ratio": 1.6968503937007875, "no_speech_prob": 4.637766778614605e-06}, {"id": 189, "seek": 82916, "start": 829.16, "end": 832.88, "text": " started, and that's all available for free online.", "tokens": [1409, 11, 293, 300, 311, 439, 2435, 337, 1737, 2950, 13], "temperature": 0.0, "avg_logprob": -0.11927350362141927, "compression_ratio": 1.608294930875576, "no_speech_prob": 2.0580266664183e-06}, {"id": 190, "seek": 82916, "start": 832.88, "end": 838.1999999999999, "text": " So I would recommend, well, and so would Radek, start there.", "tokens": [407, 286, 576, 2748, 11, 731, 11, 293, 370, 576, 497, 762, 74, 11, 722, 456, 13], "temperature": 0.0, "avg_logprob": -0.11927350362141927, "compression_ratio": 1.608294930875576, "no_speech_prob": 2.0580266664183e-06}, {"id": 191, "seek": 82916, "start": 838.1999999999999, "end": 842.88, "text": " And so these quotes are all from Radek's book, by the way.", "tokens": [400, 370, 613, 19963, 366, 439, 490, 497, 762, 74, 311, 1446, 11, 538, 264, 636, 13], "temperature": 0.0, "avg_logprob": -0.11927350362141927, "compression_ratio": 1.608294930875576, "no_speech_prob": 2.0580266664183e-06}, {"id": 192, "seek": 82916, "start": 842.88, "end": 848.28, "text": " And then the other piece, so Radek talks about this four legged table of the things that", "tokens": [400, 550, 264, 661, 2522, 11, 370, 497, 762, 74, 6686, 466, 341, 1451, 476, 12244, 3199, 295, 264, 721, 300], "temperature": 0.0, "avg_logprob": -0.11927350362141927, "compression_ratio": 1.608294930875576, "no_speech_prob": 2.0580266664183e-06}, {"id": 193, "seek": 82916, "start": 848.28, "end": 856.24, "text": " are going to help you do your deep learning experiments more effectively and efficiently.", "tokens": [366, 516, 281, 854, 291, 360, 428, 2452, 2539, 12050, 544, 8659, 293, 19621, 13], "temperature": 0.0, "avg_logprob": -0.11927350362141927, "compression_ratio": 1.608294930875576, "no_speech_prob": 2.0580266664183e-06}, {"id": 194, "seek": 85624, "start": 856.24, "end": 862.6800000000001, "text": " And these are the ideas, like knowing the basic ideas around code, knowing your tools.", "tokens": [400, 613, 366, 264, 3487, 11, 411, 5276, 264, 3875, 3487, 926, 3089, 11, 5276, 428, 3873, 13], "temperature": 0.0, "avg_logprob": -0.10531970104539251, "compression_ratio": 1.62, "no_speech_prob": 8.267258635896724e-06}, {"id": 195, "seek": 85624, "start": 862.6800000000001, "end": 871.5600000000001, "text": " So an editor, Jupyter notebook, knowing stuff like Git, like how to save your work and pull", "tokens": [407, 364, 9839, 11, 22125, 88, 391, 21060, 11, 5276, 1507, 411, 16939, 11, 411, 577, 281, 3155, 428, 589, 293, 2235], "temperature": 0.0, "avg_logprob": -0.10531970104539251, "compression_ratio": 1.62, "no_speech_prob": 8.267258635896724e-06}, {"id": 196, "seek": 85624, "start": 871.5600000000001, "end": 878.88, "text": " in other people's work and so forth, and understanding kind of SSH and Linux, like how to access", "tokens": [294, 661, 561, 311, 589, 293, 370, 5220, 11, 293, 3701, 733, 295, 12238, 39, 293, 18734, 11, 411, 577, 281, 2105], "temperature": 0.0, "avg_logprob": -0.10531970104539251, "compression_ratio": 1.62, "no_speech_prob": 8.267258635896724e-06}, {"id": 197, "seek": 85624, "start": 878.88, "end": 883.6, "text": " a server and manipulate it and do stuff with it.", "tokens": [257, 7154, 293, 20459, 309, 293, 360, 1507, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.10531970104539251, "compression_ratio": 1.62, "no_speech_prob": 8.267258635896724e-06}, {"id": 198, "seek": 88360, "start": 883.6, "end": 889.48, "text": " So there's this great course called The Missing Semester of Your CS Education, which was actually", "tokens": [407, 456, 311, 341, 869, 1164, 1219, 440, 5275, 278, 14421, 3011, 295, 2260, 9460, 10680, 11, 597, 390, 767], "temperature": 0.0, "avg_logprob": -0.16506301715809812, "compression_ratio": 1.5384615384615385, "no_speech_prob": 5.6822627811925486e-06}, {"id": 199, "seek": 88360, "start": 889.48, "end": 897.78, "text": " created, I believe, by students at MIT who said, oh, everybody at MIT is assuming we", "tokens": [2942, 11, 286, 1697, 11, 538, 1731, 412, 13100, 567, 848, 11, 1954, 11, 2201, 412, 13100, 307, 11926, 321], "temperature": 0.0, "avg_logprob": -0.16506301715809812, "compression_ratio": 1.5384615384615385, "no_speech_prob": 5.6822627811925486e-06}, {"id": 200, "seek": 88360, "start": 897.78, "end": 902.12, "text": " already know this stuff, but a lot of us don't.", "tokens": [1217, 458, 341, 1507, 11, 457, 257, 688, 295, 505, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.16506301715809812, "compression_ratio": 1.5384615384615385, "no_speech_prob": 5.6822627811925486e-06}, {"id": 201, "seek": 88360, "start": 902.12, "end": 908.6, "text": " So there's nothing to be ashamed of if you've never used Git or you've never used SSH, you", "tokens": [407, 456, 311, 1825, 281, 312, 19489, 295, 498, 291, 600, 1128, 1143, 16939, 420, 291, 600, 1128, 1143, 12238, 39, 11, 291], "temperature": 0.0, "avg_logprob": -0.16506301715809812, "compression_ratio": 1.5384615384615385, "no_speech_prob": 5.6822627811925486e-06}, {"id": 202, "seek": 88360, "start": 908.6, "end": 910.96, "text": " know, or whatever.", "tokens": [458, 11, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.16506301715809812, "compression_ratio": 1.5384615384615385, "no_speech_prob": 5.6822627811925486e-06}, {"id": 203, "seek": 91096, "start": 910.96, "end": 916.2, "text": " They're just tools which at some point in the journey, most people will just kind of", "tokens": [814, 434, 445, 3873, 597, 412, 512, 935, 294, 264, 4671, 11, 881, 561, 486, 445, 733, 295], "temperature": 0.0, "avg_logprob": -0.14288701293289022, "compression_ratio": 1.6651162790697673, "no_speech_prob": 4.637783604266588e-06}, {"id": 204, "seek": 91096, "start": 916.2, "end": 917.8000000000001, "text": " have to figure out.", "tokens": [362, 281, 2573, 484, 13], "temperature": 0.0, "avg_logprob": -0.14288701293289022, "compression_ratio": 1.6651162790697673, "no_speech_prob": 4.637783604266588e-06}, {"id": 205, "seek": 91096, "start": 917.8000000000001, "end": 920.08, "text": " So this is actually a great time to do it.", "tokens": [407, 341, 307, 767, 257, 869, 565, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.14288701293289022, "compression_ratio": 1.6651162790697673, "no_speech_prob": 4.637783604266588e-06}, {"id": 206, "seek": 91096, "start": 920.08, "end": 924.5600000000001, "text": " And this is a great course to use to help you get there.", "tokens": [400, 341, 307, 257, 869, 1164, 281, 764, 281, 854, 291, 483, 456, 13], "temperature": 0.0, "avg_logprob": -0.14288701293289022, "compression_ratio": 1.6651162790697673, "no_speech_prob": 4.637783604266588e-06}, {"id": 207, "seek": 91096, "start": 924.5600000000001, "end": 930.36, "text": " And of course, again, the main thing is to practice these tools.", "tokens": [400, 295, 1164, 11, 797, 11, 264, 2135, 551, 307, 281, 3124, 613, 3873, 13], "temperature": 0.0, "avg_logprob": -0.14288701293289022, "compression_ratio": 1.6651162790697673, "no_speech_prob": 4.637783604266588e-06}, {"id": 208, "seek": 91096, "start": 930.36, "end": 939.6800000000001, "text": " So that's the kind of foundation around coding and your kind of development environment.", "tokens": [407, 300, 311, 264, 733, 295, 7030, 926, 17720, 293, 428, 733, 295, 3250, 2823, 13], "temperature": 0.0, "avg_logprob": -0.14288701293289022, "compression_ratio": 1.6651162790697673, "no_speech_prob": 4.637783604266588e-06}, {"id": 209, "seek": 93968, "start": 939.68, "end": 944.28, "text": " The next big piece of advice, which we talk about a lot in the course and that Radek talks", "tokens": [440, 958, 955, 2522, 295, 5192, 11, 597, 321, 751, 466, 257, 688, 294, 264, 1164, 293, 300, 497, 762, 74, 6686], "temperature": 0.0, "avg_logprob": -0.13244975131490958, "compression_ratio": 1.6605504587155964, "no_speech_prob": 7.410984380840091e-06}, {"id": 210, "seek": 93968, "start": 944.28, "end": 954.8399999999999, "text": " about in his book, is sharing your work, communicating your work, and writing about your work.", "tokens": [466, 294, 702, 1446, 11, 307, 5414, 428, 589, 11, 17559, 428, 589, 11, 293, 3579, 466, 428, 589, 13], "temperature": 0.0, "avg_logprob": -0.13244975131490958, "compression_ratio": 1.6605504587155964, "no_speech_prob": 7.410984380840091e-06}, {"id": 211, "seek": 93968, "start": 954.8399999999999, "end": 960.9599999999999, "text": " This is something that a lot of people feel very uncomfortable, like tweeting or blogging", "tokens": [639, 307, 746, 300, 257, 688, 295, 561, 841, 588, 10532, 11, 411, 40090, 420, 6968, 3249], "temperature": 0.0, "avg_logprob": -0.13244975131490958, "compression_ratio": 1.6605504587155964, "no_speech_prob": 7.410984380840091e-06}, {"id": 212, "seek": 93968, "start": 960.9599999999999, "end": 962.76, "text": " or whatever, right?", "tokens": [420, 2035, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13244975131490958, "compression_ratio": 1.6605504587155964, "no_speech_prob": 7.410984380840091e-06}, {"id": 213, "seek": 93968, "start": 962.76, "end": 968.16, "text": " It's like, who the hell am I to start writing about deep learning?", "tokens": [467, 311, 411, 11, 567, 264, 4921, 669, 286, 281, 722, 3579, 466, 2452, 2539, 30], "temperature": 0.0, "avg_logprob": -0.13244975131490958, "compression_ratio": 1.6605504587155964, "no_speech_prob": 7.410984380840091e-06}, {"id": 214, "seek": 96816, "start": 968.16, "end": 970.0799999999999, "text": " I've just started.", "tokens": [286, 600, 445, 1409, 13], "temperature": 0.0, "avg_logprob": -0.18650454395222213, "compression_ratio": 1.6837209302325582, "no_speech_prob": 1.7777556422515772e-05}, {"id": 215, "seek": 96816, "start": 970.0799999999999, "end": 973.4399999999999, "text": " Well, here's the thing.", "tokens": [1042, 11, 510, 311, 264, 551, 13], "temperature": 0.0, "avg_logprob": -0.18650454395222213, "compression_ratio": 1.6837209302325582, "no_speech_prob": 1.7777556422515772e-05}, {"id": 216, "seek": 96816, "start": 973.4399999999999, "end": 980.3199999999999, "text": " No one is better placed than you to write for like, what would you have wanted to know", "tokens": [883, 472, 307, 1101, 7074, 813, 291, 281, 2464, 337, 411, 11, 437, 576, 291, 362, 1415, 281, 458], "temperature": 0.0, "avg_logprob": -0.18650454395222213, "compression_ratio": 1.6837209302325582, "no_speech_prob": 1.7777556422515772e-05}, {"id": 217, "seek": 96816, "start": 980.3199999999999, "end": 981.3199999999999, "text": " six months ago?", "tokens": [2309, 2493, 2057, 30], "temperature": 0.0, "avg_logprob": -0.18650454395222213, "compression_ratio": 1.6837209302325582, "no_speech_prob": 1.7777556422515772e-05}, {"id": 218, "seek": 96816, "start": 981.3199999999999, "end": 986.48, "text": " All right, so you now know more than you did six months ago, and you'll know more in a", "tokens": [1057, 558, 11, 370, 291, 586, 458, 544, 813, 291, 630, 2309, 2493, 2057, 11, 293, 291, 603, 458, 544, 294, 257], "temperature": 0.0, "avg_logprob": -0.18650454395222213, "compression_ratio": 1.6837209302325582, "no_speech_prob": 1.7777556422515772e-05}, {"id": 219, "seek": 96816, "start": 986.48, "end": 988.36, "text": " week and more in a week, more in a week.", "tokens": [1243, 293, 544, 294, 257, 1243, 11, 544, 294, 257, 1243, 13], "temperature": 0.0, "avg_logprob": -0.18650454395222213, "compression_ratio": 1.6837209302325582, "no_speech_prob": 1.7777556422515772e-05}, {"id": 220, "seek": 96816, "start": 988.36, "end": 993.76, "text": " And so if you've got a background in, say, the hospitality industry, you know, you could", "tokens": [400, 370, 498, 291, 600, 658, 257, 3678, 294, 11, 584, 11, 264, 31207, 3518, 11, 291, 458, 11, 291, 727], "temperature": 0.0, "avg_logprob": -0.18650454395222213, "compression_ratio": 1.6837209302325582, "no_speech_prob": 1.7777556422515772e-05}, {"id": 221, "seek": 99376, "start": 993.76, "end": 998.52, "text": " probably write something very interesting for your colleagues in the hospitality industry", "tokens": [1391, 2464, 746, 588, 1880, 337, 428, 7734, 294, 264, 31207, 3518], "temperature": 0.0, "avg_logprob": -0.11568181201665088, "compression_ratio": 1.844621513944223, "no_speech_prob": 4.936855020787334e-06}, {"id": 222, "seek": 99376, "start": 998.52, "end": 1003.88, "text": " about ideas around around deep learning, for example.", "tokens": [466, 3487, 926, 926, 2452, 2539, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.11568181201665088, "compression_ratio": 1.844621513944223, "no_speech_prob": 4.936855020787334e-06}, {"id": 223, "seek": 99376, "start": 1003.88, "end": 1008.12, "text": " Or if you teach in high school, you know, you might have ideas that you could write", "tokens": [1610, 498, 291, 2924, 294, 1090, 1395, 11, 291, 458, 11, 291, 1062, 362, 3487, 300, 291, 727, 2464], "temperature": 0.0, "avg_logprob": -0.11568181201665088, "compression_ratio": 1.844621513944223, "no_speech_prob": 4.936855020787334e-06}, {"id": 224, "seek": 99376, "start": 1008.12, "end": 1012.54, "text": " down about what high school students might find interesting or teachers might find interesting.", "tokens": [760, 466, 437, 1090, 1395, 1731, 1062, 915, 1880, 420, 6023, 1062, 915, 1880, 13], "temperature": 0.0, "avg_logprob": -0.11568181201665088, "compression_ratio": 1.844621513944223, "no_speech_prob": 4.936855020787334e-06}, {"id": 225, "seek": 99376, "start": 1012.54, "end": 1015.3199999999999, "text": " So you know, everybody's got something to say.", "tokens": [407, 291, 458, 11, 2201, 311, 658, 746, 281, 584, 13], "temperature": 0.0, "avg_logprob": -0.11568181201665088, "compression_ratio": 1.844621513944223, "no_speech_prob": 4.936855020787334e-06}, {"id": 226, "seek": 99376, "start": 1015.3199999999999, "end": 1021.28, "text": " And the key thing is to write it down because that is going to help embed your understanding", "tokens": [400, 264, 2141, 551, 307, 281, 2464, 309, 760, 570, 300, 307, 516, 281, 854, 12240, 428, 3701], "temperature": 0.0, "avg_logprob": -0.11568181201665088, "compression_ratio": 1.844621513944223, "no_speech_prob": 4.936855020787334e-06}, {"id": 227, "seek": 102128, "start": 1021.28, "end": 1029.08, "text": " a lot better, and it's going to start to build up your portfolio.", "tokens": [257, 688, 1101, 11, 293, 309, 311, 516, 281, 722, 281, 1322, 493, 428, 12583, 13], "temperature": 0.0, "avg_logprob": -0.14771626319414305, "compression_ratio": 1.6, "no_speech_prob": 5.77179071115097e-06}, {"id": 228, "seek": 102128, "start": 1029.08, "end": 1031.12, "text": " And so we'll talk more about that in a moment.", "tokens": [400, 370, 321, 603, 751, 544, 466, 300, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.14771626319414305, "compression_ratio": 1.6, "no_speech_prob": 5.77179071115097e-06}, {"id": 229, "seek": 102128, "start": 1031.12, "end": 1037.44, "text": " But a lot of people have found that this message of sharing their work has been a critical", "tokens": [583, 257, 688, 295, 561, 362, 1352, 300, 341, 3636, 295, 5414, 641, 589, 575, 668, 257, 4924], "temperature": 0.0, "avg_logprob": -0.14771626319414305, "compression_ratio": 1.6, "no_speech_prob": 5.77179071115097e-06}, {"id": 230, "seek": 102128, "start": 1037.44, "end": 1042.96, "text": " part of their journey of learning and of also building up their personal brand that has", "tokens": [644, 295, 641, 4671, 295, 2539, 293, 295, 611, 2390, 493, 641, 2973, 3360, 300, 575], "temperature": 0.0, "avg_logprob": -0.14771626319414305, "compression_ratio": 1.6, "no_speech_prob": 5.77179071115097e-06}, {"id": 231, "seek": 102128, "start": 1042.96, "end": 1046.36, "text": " ended up getting them a job.", "tokens": [4590, 493, 1242, 552, 257, 1691, 13], "temperature": 0.0, "avg_logprob": -0.14771626319414305, "compression_ratio": 1.6, "no_speech_prob": 5.77179071115097e-06}, {"id": 232, "seek": 104636, "start": 1046.36, "end": 1053.7199999999998, "text": " OK, so what does it mean to do a Fast AI lesson?", "tokens": [2264, 11, 370, 437, 775, 309, 914, 281, 360, 257, 15968, 7318, 6898, 30], "temperature": 0.0, "avg_logprob": -0.16600589752197265, "compression_ratio": 1.6331360946745561, "no_speech_prob": 7.646233825653326e-06}, {"id": 233, "seek": 104636, "start": 1053.7199999999998, "end": 1065.1599999999999, "text": " So a Fast AI lesson is basically a chapter of the book or one video from the course.", "tokens": [407, 257, 15968, 7318, 6898, 307, 1936, 257, 7187, 295, 264, 1446, 420, 472, 960, 490, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.16600589752197265, "compression_ratio": 1.6331360946745561, "no_speech_prob": 7.646233825653326e-06}, {"id": 234, "seek": 104636, "start": 1065.1599999999999, "end": 1066.1599999999999, "text": " Or both.", "tokens": [1610, 1293, 13], "temperature": 0.0, "avg_logprob": -0.16600589752197265, "compression_ratio": 1.6331360946745561, "no_speech_prob": 7.646233825653326e-06}, {"id": 235, "seek": 104636, "start": 1066.1599999999999, "end": 1070.8, "text": " So what does it mean to to do one of these lessons?", "tokens": [407, 437, 775, 309, 914, 281, 281, 360, 472, 295, 613, 8820, 30], "temperature": 0.0, "avg_logprob": -0.16600589752197265, "compression_ratio": 1.6331360946745561, "no_speech_prob": 7.646233825653326e-06}, {"id": 236, "seek": 104636, "start": 1070.8, "end": 1075.9199999999998, "text": " Assuming you're doing the video, then it means, OK, obviously watching the video.", "tokens": [6281, 24919, 291, 434, 884, 264, 960, 11, 550, 309, 1355, 11, 2264, 11, 2745, 1976, 264, 960, 13], "temperature": 0.0, "avg_logprob": -0.16600589752197265, "compression_ratio": 1.6331360946745561, "no_speech_prob": 7.646233825653326e-06}, {"id": 237, "seek": 107592, "start": 1075.92, "end": 1077.76, "text": " So there's a couple of hours.", "tokens": [407, 456, 311, 257, 1916, 295, 2496, 13], "temperature": 0.0, "avg_logprob": -0.14102683597140842, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.8129441034252523e-06}, {"id": 238, "seek": 107592, "start": 1077.76, "end": 1078.8400000000001, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.14102683597140842, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.8129441034252523e-06}, {"id": 239, "seek": 107592, "start": 1078.8400000000001, "end": 1086.16, "text": " And then it means running the notebook, which we'll look at in a moment.", "tokens": [400, 550, 309, 1355, 2614, 264, 21060, 11, 597, 321, 603, 574, 412, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.14102683597140842, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.8129441034252523e-06}, {"id": 240, "seek": 107592, "start": 1086.16, "end": 1091.76, "text": " When you run the notebook, you have the whole book with all of its code and all of its outputs", "tokens": [1133, 291, 1190, 264, 21060, 11, 291, 362, 264, 1379, 1446, 365, 439, 295, 1080, 3089, 293, 439, 295, 1080, 23930], "temperature": 0.0, "avg_logprob": -0.14102683597140842, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.8129441034252523e-06}, {"id": 241, "seek": 107592, "start": 1091.76, "end": 1092.76, "text": " there.", "tokens": [456, 13], "temperature": 0.0, "avg_logprob": -0.14102683597140842, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.8129441034252523e-06}, {"id": 242, "seek": 107592, "start": 1092.76, "end": 1096.72, "text": " You're playing with it.", "tokens": [509, 434, 2433, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.14102683597140842, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.8129441034252523e-06}, {"id": 243, "seek": 107592, "start": 1096.72, "end": 1097.72, "text": " You should experiment.", "tokens": [509, 820, 5120, 13], "temperature": 0.0, "avg_logprob": -0.14102683597140842, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.8129441034252523e-06}, {"id": 244, "seek": 107592, "start": 1097.72, "end": 1098.72, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.14102683597140842, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.8129441034252523e-06}, {"id": 245, "seek": 107592, "start": 1098.72, "end": 1101.2, "text": " You should you should try things out.", "tokens": [509, 820, 291, 820, 853, 721, 484, 13], "temperature": 0.0, "avg_logprob": -0.14102683597140842, "compression_ratio": 1.6378378378378378, "no_speech_prob": 2.8129441034252523e-06}, {"id": 246, "seek": 110120, "start": 1101.2, "end": 1106.32, "text": " So if you wonder, oh, why is this done before that?", "tokens": [407, 498, 291, 2441, 11, 1954, 11, 983, 307, 341, 1096, 949, 300, 30], "temperature": 0.0, "avg_logprob": -0.14471624847641565, "compression_ratio": 1.76171875, "no_speech_prob": 1.8447917682351544e-06}, {"id": 247, "seek": 110120, "start": 1106.32, "end": 1107.72, "text": " Well try removing it.", "tokens": [1042, 853, 12720, 309, 13], "temperature": 0.0, "avg_logprob": -0.14471624847641565, "compression_ratio": 1.76171875, "no_speech_prob": 1.8447917682351544e-06}, {"id": 248, "seek": 110120, "start": 1107.72, "end": 1109.3400000000001, "text": " Try doing it in a different order.", "tokens": [6526, 884, 309, 294, 257, 819, 1668, 13], "temperature": 0.0, "avg_logprob": -0.14471624847641565, "compression_ratio": 1.76171875, "no_speech_prob": 1.8447917682351544e-06}, {"id": 249, "seek": 110120, "start": 1109.3400000000001, "end": 1112.64, "text": " If you're wondering, you know, what would happen if I did that, but this to this other", "tokens": [759, 291, 434, 6359, 11, 291, 458, 11, 437, 576, 1051, 498, 286, 630, 300, 11, 457, 341, 281, 341, 661], "temperature": 0.0, "avg_logprob": -0.14471624847641565, "compression_ratio": 1.76171875, "no_speech_prob": 1.8447917682351544e-06}, {"id": 250, "seek": 110120, "start": 1112.64, "end": 1114.2, "text": " image?", "tokens": [3256, 30], "temperature": 0.0, "avg_logprob": -0.14471624847641565, "compression_ratio": 1.76171875, "no_speech_prob": 1.8447917682351544e-06}, {"id": 251, "seek": 110120, "start": 1114.2, "end": 1115.2, "text": " Try it.", "tokens": [6526, 309, 13], "temperature": 0.0, "avg_logprob": -0.14471624847641565, "compression_ratio": 1.76171875, "no_speech_prob": 1.8447917682351544e-06}, {"id": 252, "seek": 110120, "start": 1115.2, "end": 1116.2, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.14471624847641565, "compression_ratio": 1.76171875, "no_speech_prob": 1.8447917682351544e-06}, {"id": 253, "seek": 110120, "start": 1116.2, "end": 1119.72, "text": " The more you can start to experiment, the more you're feeding your brain with these", "tokens": [440, 544, 291, 393, 722, 281, 5120, 11, 264, 544, 291, 434, 12919, 428, 3567, 365, 613], "temperature": 0.0, "avg_logprob": -0.14471624847641565, "compression_ratio": 1.76171875, "no_speech_prob": 1.8447917682351544e-06}, {"id": 254, "seek": 110120, "start": 1119.72, "end": 1123.24, "text": " kind of like your own deep learning happening in your brain.", "tokens": [733, 295, 411, 428, 1065, 2452, 2539, 2737, 294, 428, 3567, 13], "temperature": 0.0, "avg_logprob": -0.14471624847641565, "compression_ratio": 1.76171875, "no_speech_prob": 1.8447917682351544e-06}, {"id": 255, "seek": 110120, "start": 1123.24, "end": 1124.24, "text": " Input output patterns.", "tokens": [682, 2582, 5598, 8294, 13], "temperature": 0.0, "avg_logprob": -0.14471624847641565, "compression_ratio": 1.76171875, "no_speech_prob": 1.8447917682351544e-06}, {"id": 256, "seek": 110120, "start": 1124.24, "end": 1125.24, "text": " You try something.", "tokens": [509, 853, 746, 13], "temperature": 0.0, "avg_logprob": -0.14471624847641565, "compression_ratio": 1.76171875, "no_speech_prob": 1.8447917682351544e-06}, {"id": 257, "seek": 110120, "start": 1125.24, "end": 1126.24, "text": " What happens?", "tokens": [708, 2314, 30], "temperature": 0.0, "avg_logprob": -0.14471624847641565, "compression_ratio": 1.76171875, "no_speech_prob": 1.8447917682351544e-06}, {"id": 258, "seek": 110120, "start": 1126.24, "end": 1127.24, "text": " You try something.", "tokens": [509, 853, 746, 13], "temperature": 0.0, "avg_logprob": -0.14471624847641565, "compression_ratio": 1.76171875, "no_speech_prob": 1.8447917682351544e-06}, {"id": 259, "seek": 110120, "start": 1127.24, "end": 1129.72, "text": " What happens?", "tokens": [708, 2314, 30], "temperature": 0.0, "avg_logprob": -0.14471624847641565, "compression_ratio": 1.76171875, "no_speech_prob": 1.8447917682351544e-06}, {"id": 260, "seek": 112972, "start": 1129.72, "end": 1140.76, "text": " So after that, the next step is to try to reproduce the notebook from scratch.", "tokens": [407, 934, 300, 11, 264, 958, 1823, 307, 281, 853, 281, 29501, 264, 21060, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.16895917986260087, "compression_ratio": 1.4580645161290322, "no_speech_prob": 1.5294024251488736e-06}, {"id": 261, "seek": 112972, "start": 1140.76, "end": 1145.76, "text": " OK, now you're going to have to look things up, obviously.", "tokens": [2264, 11, 586, 291, 434, 516, 281, 362, 281, 574, 721, 493, 11, 2745, 13], "temperature": 0.0, "avg_logprob": -0.16895917986260087, "compression_ratio": 1.4580645161290322, "no_speech_prob": 1.5294024251488736e-06}, {"id": 262, "seek": 112972, "start": 1145.76, "end": 1158.52, "text": " But the idea is, can you with a fresh new notebook, can you can you go back and recreate", "tokens": [583, 264, 1558, 307, 11, 393, 291, 365, 257, 4451, 777, 21060, 11, 393, 291, 393, 291, 352, 646, 293, 25833], "temperature": 0.0, "avg_logprob": -0.16895917986260087, "compression_ratio": 1.4580645161290322, "no_speech_prob": 1.5294024251488736e-06}, {"id": 263, "seek": 115852, "start": 1158.52, "end": 1163.12, "text": " some of those models, retrain them or redo some of that data processing pipeline.", "tokens": [512, 295, 729, 5245, 11, 1533, 7146, 552, 420, 29956, 512, 295, 300, 1412, 9007, 15517, 13], "temperature": 0.0, "avg_logprob": -0.1280695809258355, "compression_ratio": 1.6619718309859155, "no_speech_prob": 7.071716936479788e-06}, {"id": 264, "seek": 115852, "start": 1163.12, "end": 1165.76, "text": " So try to like type it in yourself.", "tokens": [407, 853, 281, 411, 2010, 309, 294, 1803, 13], "temperature": 0.0, "avg_logprob": -0.1280695809258355, "compression_ratio": 1.6619718309859155, "no_speech_prob": 7.071716936479788e-06}, {"id": 265, "seek": 115852, "start": 1165.76, "end": 1170.18, "text": " You know, you can switch back to the answer as much as you like, but you're really trying", "tokens": [509, 458, 11, 291, 393, 3679, 646, 281, 264, 1867, 382, 709, 382, 291, 411, 11, 457, 291, 434, 534, 1382], "temperature": 0.0, "avg_logprob": -0.1280695809258355, "compression_ratio": 1.6619718309859155, "no_speech_prob": 7.071716936479788e-06}, {"id": 266, "seek": 115852, "start": 1170.18, "end": 1178.04, "text": " to start to actually fill in your own write your own code.", "tokens": [281, 722, 281, 767, 2836, 294, 428, 1065, 2464, 428, 1065, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1280695809258355, "compression_ratio": 1.6619718309859155, "no_speech_prob": 7.071716936479788e-06}, {"id": 267, "seek": 115852, "start": 1178.04, "end": 1183.04, "text": " And then what you really the point you really want to get to is repeating some parts of", "tokens": [400, 550, 437, 291, 534, 264, 935, 291, 534, 528, 281, 483, 281, 307, 18617, 512, 3166, 295], "temperature": 0.0, "avg_logprob": -0.1280695809258355, "compression_ratio": 1.6619718309859155, "no_speech_prob": 7.071716936479788e-06}, {"id": 268, "seek": 118304, "start": 1183.04, "end": 1189.68, "text": " the lesson with a different data set which you collect or download.", "tokens": [264, 6898, 365, 257, 819, 1412, 992, 597, 291, 2500, 420, 5484, 13], "temperature": 0.0, "avg_logprob": -0.11984988486412729, "compression_ratio": 1.7322175732217573, "no_speech_prob": 1.370944573864108e-06}, {"id": 269, "seek": 118304, "start": 1189.68, "end": 1197.8, "text": " Now this whole process often takes people a number of times through the course.", "tokens": [823, 341, 1379, 1399, 2049, 2516, 561, 257, 1230, 295, 1413, 807, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.11984988486412729, "compression_ratio": 1.7322175732217573, "no_speech_prob": 1.370944573864108e-06}, {"id": 270, "seek": 118304, "start": 1197.8, "end": 1198.8, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.11984988486412729, "compression_ratio": 1.7322175732217573, "no_speech_prob": 1.370944573864108e-06}, {"id": 271, "seek": 118304, "start": 1198.8, "end": 1203.12, "text": " So often the first time through, people might just watch each lecture and try to kind of", "tokens": [407, 2049, 264, 700, 565, 807, 11, 561, 1062, 445, 1159, 1184, 7991, 293, 853, 281, 733, 295], "temperature": 0.0, "avg_logprob": -0.11984988486412729, "compression_ratio": 1.7322175732217573, "no_speech_prob": 1.370944573864108e-06}, {"id": 272, "seek": 118304, "start": 1203.12, "end": 1208.6399999999999, "text": " run it and just get to the end to get a kind of a general sense of what's going on.", "tokens": [1190, 309, 293, 445, 483, 281, 264, 917, 281, 483, 257, 733, 295, 257, 2674, 2020, 295, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.11984988486412729, "compression_ratio": 1.7322175732217573, "no_speech_prob": 1.370944573864108e-06}, {"id": 273, "seek": 118304, "start": 1208.6399999999999, "end": 1212.6, "text": " So people often kind of go through the whole thing like three times and then come back", "tokens": [407, 561, 2049, 733, 295, 352, 807, 264, 1379, 551, 411, 1045, 1413, 293, 550, 808, 646], "temperature": 0.0, "avg_logprob": -0.11984988486412729, "compression_ratio": 1.7322175732217573, "no_speech_prob": 1.370944573864108e-06}, {"id": 274, "seek": 121260, "start": 1212.6, "end": 1215.3999999999999, "text": " and try to go further and further.", "tokens": [293, 853, 281, 352, 3052, 293, 3052, 13], "temperature": 0.0, "avg_logprob": -0.18067170691302442, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.1125441233161837e-05}, {"id": 275, "seek": 121260, "start": 1215.3999999999999, "end": 1220.9199999999998, "text": " So don't worry if you can't do all this right away.", "tokens": [407, 500, 380, 3292, 498, 291, 393, 380, 360, 439, 341, 558, 1314, 13], "temperature": 0.0, "avg_logprob": -0.18067170691302442, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.1125441233161837e-05}, {"id": 276, "seek": 121260, "start": 1220.9199999999998, "end": 1223.26, "text": " Certainly in lesson one, that's going to be challenging.", "tokens": [16628, 294, 6898, 472, 11, 300, 311, 516, 281, 312, 7595, 13], "temperature": 0.0, "avg_logprob": -0.18067170691302442, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.1125441233161837e-05}, {"id": 277, "seek": 121260, "start": 1223.26, "end": 1224.4399999999998, "text": " Just take it as far as you can.", "tokens": [1449, 747, 309, 382, 1400, 382, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.18067170691302442, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.1125441233161837e-05}, {"id": 278, "seek": 121260, "start": 1224.4399999999998, "end": 1225.4399999999998, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.18067170691302442, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.1125441233161837e-05}, {"id": 279, "seek": 121260, "start": 1225.4399999999998, "end": 1228.84, "text": " And as you go along, try to push yourself to do more and more.", "tokens": [400, 382, 291, 352, 2051, 11, 853, 281, 2944, 1803, 281, 360, 544, 293, 544, 13], "temperature": 0.0, "avg_logprob": -0.18067170691302442, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.1125441233161837e-05}, {"id": 280, "seek": 121260, "start": 1228.84, "end": 1232.28, "text": " And you could even go back to an earlier notebook and see if you can understand more and more", "tokens": [400, 291, 727, 754, 352, 646, 281, 364, 3071, 21060, 293, 536, 498, 291, 393, 1223, 544, 293, 544], "temperature": 0.0, "avg_logprob": -0.18067170691302442, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.1125441233161837e-05}, {"id": 281, "seek": 121260, "start": 1232.28, "end": 1233.82, "text": " of it.", "tokens": [295, 309, 13], "temperature": 0.0, "avg_logprob": -0.18067170691302442, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.1125441233161837e-05}, {"id": 282, "seek": 121260, "start": 1233.82, "end": 1238.34, "text": " So let's take a look at what that looks like.", "tokens": [407, 718, 311, 747, 257, 574, 412, 437, 300, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.18067170691302442, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.1125441233161837e-05}, {"id": 283, "seek": 121260, "start": 1238.34, "end": 1240.48, "text": " So here's the course.", "tokens": [407, 510, 311, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.18067170691302442, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.1125441233161837e-05}, {"id": 284, "seek": 121260, "start": 1240.48, "end": 1242.24, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.18067170691302442, "compression_ratio": 1.7073170731707317, "no_speech_prob": 1.1125441233161837e-05}, {"id": 285, "seek": 124224, "start": 1242.24, "end": 1249.04, "text": " And here's the lessons which you can watch.", "tokens": [400, 510, 311, 264, 8820, 597, 291, 393, 1159, 13], "temperature": 0.0, "avg_logprob": -0.1207501490910848, "compression_ratio": 1.6257309941520468, "no_speech_prob": 2.857282652257709e-06}, {"id": 286, "seek": 124224, "start": 1249.04, "end": 1254.58, "text": " And then here are the places you can run the notebooks.", "tokens": [400, 550, 510, 366, 264, 3190, 291, 393, 1190, 264, 43782, 13], "temperature": 0.0, "avg_logprob": -0.1207501490910848, "compression_ratio": 1.6257309941520468, "no_speech_prob": 2.857282652257709e-06}, {"id": 287, "seek": 124224, "start": 1254.58, "end": 1261.84, "text": " So there's two types of platform for running the notebooks.", "tokens": [407, 456, 311, 732, 3467, 295, 3663, 337, 2614, 264, 43782, 13], "temperature": 0.0, "avg_logprob": -0.1207501490910848, "compression_ratio": 1.6257309941520468, "no_speech_prob": 2.857282652257709e-06}, {"id": 288, "seek": 124224, "start": 1261.84, "end": 1263.1, "text": " There are notebook servers.", "tokens": [821, 366, 21060, 15909, 13], "temperature": 0.0, "avg_logprob": -0.1207501490910848, "compression_ratio": 1.6257309941520468, "no_speech_prob": 2.857282652257709e-06}, {"id": 289, "seek": 124224, "start": 1263.1, "end": 1269.44, "text": " These are things that as soon as you click into it, the actual environment we use, Jupyter", "tokens": [1981, 366, 721, 300, 382, 2321, 382, 291, 2052, 666, 309, 11, 264, 3539, 2823, 321, 764, 11, 22125, 88, 391], "temperature": 0.0, "avg_logprob": -0.1207501490910848, "compression_ratio": 1.6257309941520468, "no_speech_prob": 2.857282652257709e-06}, {"id": 290, "seek": 126944, "start": 1269.44, "end": 1275.48, "text": " Notebook, will pop up and you can just start running it pretty much straight away.", "tokens": [11633, 2939, 11, 486, 1665, 493, 293, 291, 393, 445, 722, 2614, 309, 1238, 709, 2997, 1314, 13], "temperature": 0.0, "avg_logprob": -0.15460717500145757, "compression_ratio": 1.3788819875776397, "no_speech_prob": 4.029399860883132e-06}, {"id": 291, "seek": 126944, "start": 1275.48, "end": 1280.3600000000001, "text": " So that is obviously the easiest.", "tokens": [407, 300, 307, 2745, 264, 12889, 13], "temperature": 0.0, "avg_logprob": -0.15460717500145757, "compression_ratio": 1.3788819875776397, "no_speech_prob": 4.029399860883132e-06}, {"id": 292, "seek": 126944, "start": 1280.3600000000001, "end": 1282.96, "text": " Colab is free.", "tokens": [4004, 455, 307, 1737, 13], "temperature": 0.0, "avg_logprob": -0.15460717500145757, "compression_ratio": 1.3788819875776397, "no_speech_prob": 4.029399860883132e-06}, {"id": 293, "seek": 126944, "start": 1282.96, "end": 1291.3600000000001, "text": " Gradient has a free tier and SageMaker is not free.", "tokens": [16710, 1196, 575, 257, 1737, 12362, 293, 33812, 44, 4003, 307, 406, 1737, 13], "temperature": 0.0, "avg_logprob": -0.15460717500145757, "compression_ratio": 1.3788819875776397, "no_speech_prob": 4.029399860883132e-06}, {"id": 294, "seek": 126944, "start": 1291.3600000000001, "end": 1295.72, "text": " So we're going to look at Colab today.", "tokens": [407, 321, 434, 516, 281, 574, 412, 4004, 455, 965, 13], "temperature": 0.0, "avg_logprob": -0.15460717500145757, "compression_ratio": 1.3788819875776397, "no_speech_prob": 4.029399860883132e-06}, {"id": 295, "seek": 129572, "start": 1295.72, "end": 1300.2, "text": " The other option is to use a full Linux server and this is something where you're going to", "tokens": [440, 661, 3614, 307, 281, 764, 257, 1577, 18734, 7154, 293, 341, 307, 746, 689, 291, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.11068213263223338, "compression_ratio": 1.5520361990950227, "no_speech_prob": 3.340494231451885e-06}, {"id": 296, "seek": 129572, "start": 1300.2, "end": 1306.56, "text": " have to basically set up Linux and install the Python system and install notebooks and", "tokens": [362, 281, 1936, 992, 493, 18734, 293, 3625, 264, 15329, 1185, 293, 3625, 43782, 293], "temperature": 0.0, "avg_logprob": -0.11068213263223338, "compression_ratio": 1.5520361990950227, "no_speech_prob": 3.340494231451885e-06}, {"id": 297, "seek": 129572, "start": 1306.56, "end": 1312.06, "text": " get the code from GitHub and run the server and log into SSH and do all that.", "tokens": [483, 264, 3089, 490, 23331, 293, 1190, 264, 7154, 293, 3565, 666, 12238, 39, 293, 360, 439, 300, 13], "temperature": 0.0, "avg_logprob": -0.11068213263223338, "compression_ratio": 1.5520361990950227, "no_speech_prob": 3.340494231451885e-06}, {"id": 298, "seek": 129572, "start": 1312.06, "end": 1314.38, "text": " That's obviously a lot more work.", "tokens": [663, 311, 2745, 257, 688, 544, 589, 13], "temperature": 0.0, "avg_logprob": -0.11068213263223338, "compression_ratio": 1.5520361990950227, "no_speech_prob": 3.340494231451885e-06}, {"id": 299, "seek": 129572, "start": 1314.38, "end": 1321.2, "text": " You might want to skip it for now in like lesson one.", "tokens": [509, 1062, 528, 281, 10023, 309, 337, 586, 294, 411, 6898, 472, 13], "temperature": 0.0, "avg_logprob": -0.11068213263223338, "compression_ratio": 1.5520361990950227, "no_speech_prob": 3.340494231451885e-06}, {"id": 300, "seek": 132120, "start": 1321.2, "end": 1327.52, "text": " But I would recommend at some point you go through this path.", "tokens": [583, 286, 576, 2748, 412, 512, 935, 291, 352, 807, 341, 3100, 13], "temperature": 0.0, "avg_logprob": -0.07814865018807206, "compression_ratio": 1.6245059288537549, "no_speech_prob": 2.857277195289498e-06}, {"id": 301, "seek": 132120, "start": 1327.52, "end": 1333.32, "text": " And the reason why is that in real life at your workplace or if you do your own startup", "tokens": [400, 264, 1778, 983, 307, 300, 294, 957, 993, 412, 428, 15328, 420, 498, 291, 360, 428, 1065, 18578], "temperature": 0.0, "avg_logprob": -0.07814865018807206, "compression_ratio": 1.6245059288537549, "no_speech_prob": 2.857277195289498e-06}, {"id": 302, "seek": 132120, "start": 1333.32, "end": 1336.72, "text": " or whatever, this is what you'll be doing.", "tokens": [420, 2035, 11, 341, 307, 437, 291, 603, 312, 884, 13], "temperature": 0.0, "avg_logprob": -0.07814865018807206, "compression_ratio": 1.6245059288537549, "no_speech_prob": 2.857277195289498e-06}, {"id": 303, "seek": 132120, "start": 1336.72, "end": 1342.92, "text": " You will be interacting with a Linux server using SSH that's running a GPU and you'll", "tokens": [509, 486, 312, 18017, 365, 257, 18734, 7154, 1228, 12238, 39, 300, 311, 2614, 257, 18407, 293, 291, 603], "temperature": 0.0, "avg_logprob": -0.07814865018807206, "compression_ratio": 1.6245059288537549, "no_speech_prob": 2.857277195289498e-06}, {"id": 304, "seek": 132120, "start": 1342.92, "end": 1345.44, "text": " want to understand how it all works.", "tokens": [528, 281, 1223, 577, 309, 439, 1985, 13], "temperature": 0.0, "avg_logprob": -0.07814865018807206, "compression_ratio": 1.6245059288537549, "no_speech_prob": 2.857277195289498e-06}, {"id": 305, "seek": 132120, "start": 1345.44, "end": 1350.8, "text": " And once you're using your own Linux server, you'll suddenly learn about all these productivity", "tokens": [400, 1564, 291, 434, 1228, 428, 1065, 18734, 7154, 11, 291, 603, 5800, 1466, 466, 439, 613, 15604], "temperature": 0.0, "avg_logprob": -0.07814865018807206, "compression_ratio": 1.6245059288537549, "no_speech_prob": 2.857277195289498e-06}, {"id": 306, "seek": 135080, "start": 1350.8, "end": 1356.22, "text": " enhancing tips and tools that make your life easier.", "tokens": [36579, 6082, 293, 3873, 300, 652, 428, 993, 3571, 13], "temperature": 0.0, "avg_logprob": -0.16275465978335027, "compression_ratio": 1.36, "no_speech_prob": 7.64644119044533e-06}, {"id": 307, "seek": 135080, "start": 1356.22, "end": 1362.56, "text": " So I'll be showing how to set up AWS EC2.", "tokens": [407, 286, 603, 312, 4099, 577, 281, 992, 493, 17650, 19081, 17, 13], "temperature": 0.0, "avg_logprob": -0.16275465978335027, "compression_ratio": 1.36, "no_speech_prob": 7.64644119044533e-06}, {"id": 308, "seek": 135080, "start": 1362.56, "end": 1366.9199999999998, "text": " That's the Amazon platform today.", "tokens": [663, 311, 264, 6795, 3663, 965, 13], "temperature": 0.0, "avg_logprob": -0.16275465978335027, "compression_ratio": 1.36, "no_speech_prob": 7.64644119044533e-06}, {"id": 309, "seek": 135080, "start": 1366.9199999999998, "end": 1374.12, "text": " You'll find Google Cloud looks very, very similar indeed.", "tokens": [509, 603, 915, 3329, 8061, 1542, 588, 11, 588, 2531, 6451, 13], "temperature": 0.0, "avg_logprob": -0.16275465978335027, "compression_ratio": 1.36, "no_speech_prob": 7.64644119044533e-06}, {"id": 310, "seek": 135080, "start": 1374.12, "end": 1378.84, "text": " Jarvis Labs was created by a fast AI alum and this is probably at this stage the best", "tokens": [23941, 4938, 40047, 390, 2942, 538, 257, 2370, 7318, 12064, 293, 341, 307, 1391, 412, 341, 3233, 264, 1151], "temperature": 0.0, "avg_logprob": -0.16275465978335027, "compression_ratio": 1.36, "no_speech_prob": 7.64644119044533e-06}, {"id": 311, "seek": 137884, "start": 1378.84, "end": 1382.08, "text": " value of the full Linux servers.", "tokens": [2158, 295, 264, 1577, 18734, 15909, 13], "temperature": 0.0, "avg_logprob": -0.12214854630556973, "compression_ratio": 1.5338983050847457, "no_speech_prob": 7.766640919726342e-06}, {"id": 312, "seek": 137884, "start": 1382.08, "end": 1386.9199999999998, "text": " So that would certainly be also very much worth checking out.", "tokens": [407, 300, 576, 3297, 312, 611, 588, 709, 3163, 8568, 484, 13], "temperature": 0.0, "avg_logprob": -0.12214854630556973, "compression_ratio": 1.5338983050847457, "no_speech_prob": 7.766640919726342e-06}, {"id": 313, "seek": 137884, "start": 1386.9199999999998, "end": 1389.9599999999998, "text": " One good thing about AWS, so a couple of things.", "tokens": [1485, 665, 551, 466, 17650, 11, 370, 257, 1916, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.12214854630556973, "compression_ratio": 1.5338983050847457, "no_speech_prob": 7.766640919726342e-06}, {"id": 314, "seek": 137884, "start": 1389.9599999999998, "end": 1395.0, "text": " AWS is currently the most popular platform for cloud computing.", "tokens": [17650, 307, 4362, 264, 881, 3743, 3663, 337, 4588, 15866, 13], "temperature": 0.0, "avg_logprob": -0.12214854630556973, "compression_ratio": 1.5338983050847457, "no_speech_prob": 7.766640919726342e-06}, {"id": 315, "seek": 137884, "start": 1395.0, "end": 1402.12, "text": " So it's very likely that whatever company you're at or end up at is already using it.", "tokens": [407, 309, 311, 588, 3700, 300, 2035, 2237, 291, 434, 412, 420, 917, 493, 412, 307, 1217, 1228, 309, 13], "temperature": 0.0, "avg_logprob": -0.12214854630556973, "compression_ratio": 1.5338983050847457, "no_speech_prob": 7.766640919726342e-06}, {"id": 316, "seek": 137884, "start": 1402.12, "end": 1406.72, "text": " They're also pretty generous with credits for startups and students.", "tokens": [814, 434, 611, 1238, 14537, 365, 16816, 337, 28041, 293, 1731, 13], "temperature": 0.0, "avg_logprob": -0.12214854630556973, "compression_ratio": 1.5338983050847457, "no_speech_prob": 7.766640919726342e-06}, {"id": 317, "seek": 140672, "start": 1406.72, "end": 1413.48, "text": " So even though it can set you back 60 or 70 cents an hour, you might well find you can", "tokens": [407, 754, 1673, 309, 393, 992, 291, 646, 4060, 420, 5285, 14941, 364, 1773, 11, 291, 1062, 731, 915, 291, 393], "temperature": 0.0, "avg_logprob": -0.07078590657975939, "compression_ratio": 1.5966850828729282, "no_speech_prob": 7.64624110161094e-06}, {"id": 318, "seek": 140672, "start": 1413.48, "end": 1419.72, "text": " get a few hundred dollars worth of credits through your school or even a few thousand", "tokens": [483, 257, 1326, 3262, 3808, 3163, 295, 16816, 807, 428, 1395, 420, 754, 257, 1326, 4714], "temperature": 0.0, "avg_logprob": -0.07078590657975939, "compression_ratio": 1.5966850828729282, "no_speech_prob": 7.64624110161094e-06}, {"id": 319, "seek": 140672, "start": 1419.72, "end": 1425.48, "text": " dollars worth of credits through their startup programs and so forth.", "tokens": [3808, 3163, 295, 16816, 807, 641, 18578, 4268, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.07078590657975939, "compression_ratio": 1.5966850828729282, "no_speech_prob": 7.64624110161094e-06}, {"id": 320, "seek": 140672, "start": 1425.48, "end": 1432.0, "text": " So let's have a look at what Colab looks like.", "tokens": [407, 718, 311, 362, 257, 574, 412, 437, 4004, 455, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.07078590657975939, "compression_ratio": 1.5966850828729282, "no_speech_prob": 7.64624110161094e-06}, {"id": 321, "seek": 143200, "start": 1432.0, "end": 1438.02, "text": " So Colab is, it's wonderful how easy it is to get started.", "tokens": [407, 4004, 455, 307, 11, 309, 311, 3715, 577, 1858, 309, 307, 281, 483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.15553947857448033, "compression_ratio": 1.431578947368421, "no_speech_prob": 2.9944087600597413e-06}, {"id": 322, "seek": 143200, "start": 1438.02, "end": 1441.44, "text": " You literally just click on the chapter.", "tokens": [509, 3736, 445, 2052, 322, 264, 7187, 13], "temperature": 0.0, "avg_logprob": -0.15553947857448033, "compression_ratio": 1.431578947368421, "no_speech_prob": 2.9944087600597413e-06}, {"id": 323, "seek": 143200, "start": 1441.44, "end": 1446.4, "text": " So let's do chapter one.", "tokens": [407, 718, 311, 360, 7187, 472, 13], "temperature": 0.0, "avg_logprob": -0.15553947857448033, "compression_ratio": 1.431578947368421, "no_speech_prob": 2.9944087600597413e-06}, {"id": 324, "seek": 143200, "start": 1446.4, "end": 1450.32, "text": " And it pops up Colab.", "tokens": [400, 309, 16795, 493, 4004, 455, 13], "temperature": 0.0, "avg_logprob": -0.15553947857448033, "compression_ratio": 1.431578947368421, "no_speech_prob": 2.9944087600597413e-06}, {"id": 325, "seek": 143200, "start": 1450.32, "end": 1457.36, "text": " You can pay, I think it's $10 a month for Colab Pro to get like longer sessions and", "tokens": [509, 393, 1689, 11, 286, 519, 309, 311, 1848, 3279, 257, 1618, 337, 4004, 455, 1705, 281, 483, 411, 2854, 11081, 293], "temperature": 0.0, "avg_logprob": -0.15553947857448033, "compression_ratio": 1.431578947368421, "no_speech_prob": 2.9944087600597413e-06}, {"id": 326, "seek": 143200, "start": 1457.36, "end": 1459.56, "text": " more likely that you'll get a better GPU.", "tokens": [544, 3700, 300, 291, 603, 483, 257, 1101, 18407, 13], "temperature": 0.0, "avg_logprob": -0.15553947857448033, "compression_ratio": 1.431578947368421, "no_speech_prob": 2.9944087600597413e-06}, {"id": 327, "seek": 145956, "start": 1459.56, "end": 1464.6399999999999, "text": " But for most people you'll find the free version is totally fine.", "tokens": [583, 337, 881, 561, 291, 603, 915, 264, 1737, 3037, 307, 3879, 2489, 13], "temperature": 0.0, "avg_logprob": -0.1014486312866211, "compression_ratio": 1.6090909090909091, "no_speech_prob": 2.4824596493999707e-06}, {"id": 328, "seek": 145956, "start": 1464.6399999999999, "end": 1470.48, "text": " One of the biggest problems with Colab is that it's not persistent, which is to say", "tokens": [1485, 295, 264, 3880, 2740, 365, 4004, 455, 307, 300, 309, 311, 406, 24315, 11, 597, 307, 281, 584], "temperature": 0.0, "avg_logprob": -0.1014486312866211, "compression_ratio": 1.6090909090909091, "no_speech_prob": 2.4824596493999707e-06}, {"id": 329, "seek": 145956, "start": 1470.48, "end": 1475.28, "text": " when I go to this notebook, it thinks it's never seen me before.", "tokens": [562, 286, 352, 281, 341, 21060, 11, 309, 7309, 309, 311, 1128, 1612, 385, 949, 13], "temperature": 0.0, "avg_logprob": -0.1014486312866211, "compression_ratio": 1.6090909090909091, "no_speech_prob": 2.4824596493999707e-06}, {"id": 330, "seek": 145956, "start": 1475.28, "end": 1477.3999999999999, "text": " Nothing's set up for me the way I want it.", "tokens": [6693, 311, 992, 493, 337, 385, 264, 636, 286, 528, 309, 13], "temperature": 0.0, "avg_logprob": -0.1014486312866211, "compression_ratio": 1.6090909090909091, "no_speech_prob": 2.4824596493999707e-06}, {"id": 331, "seek": 145956, "start": 1477.3999999999999, "end": 1483.34, "text": " But we've set up the notebook so that the very first cell actually installs everything", "tokens": [583, 321, 600, 992, 493, 264, 21060, 370, 300, 264, 588, 700, 2815, 767, 3625, 82, 1203], "temperature": 0.0, "avg_logprob": -0.1014486312866211, "compression_ratio": 1.6090909090909091, "no_speech_prob": 2.4824596493999707e-06}, {"id": 332, "seek": 145956, "start": 1483.34, "end": 1484.34, "text": " you need.", "tokens": [291, 643, 13], "temperature": 0.0, "avg_logprob": -0.1014486312866211, "compression_ratio": 1.6090909090909091, "no_speech_prob": 2.4824596493999707e-06}, {"id": 333, "seek": 148434, "start": 1484.34, "end": 1491.84, "text": " So if I click this little Run Cell button here, it will run the cell.", "tokens": [407, 498, 286, 2052, 341, 707, 8950, 28859, 2960, 510, 11, 309, 486, 1190, 264, 2815, 13], "temperature": 0.0, "avg_logprob": -0.15878395080566407, "compression_ratio": 1.4574468085106382, "no_speech_prob": 7.002155939517252e-07}, {"id": 334, "seek": 148434, "start": 1491.84, "end": 1504.4399999999998, "text": " Although what I will do is I'm going to pop over to Colab here and let's also read the", "tokens": [5780, 437, 286, 486, 360, 307, 286, 478, 516, 281, 1665, 670, 281, 4004, 455, 510, 293, 718, 311, 611, 1401, 264], "temperature": 0.0, "avg_logprob": -0.15878395080566407, "compression_ratio": 1.4574468085106382, "no_speech_prob": 7.002155939517252e-07}, {"id": 335, "seek": 148434, "start": 1504.4399999999998, "end": 1505.4399999999998, "text": " steps here.", "tokens": [4439, 510, 13], "temperature": 0.0, "avg_logprob": -0.15878395080566407, "compression_ratio": 1.4574468085106382, "no_speech_prob": 7.002155939517252e-07}, {"id": 336, "seek": 148434, "start": 1505.4399999999998, "end": 1509.6799999999998, "text": " And actually it says here before running anything you should tell Colab you're interested in", "tokens": [400, 767, 309, 1619, 510, 949, 2614, 1340, 291, 820, 980, 4004, 455, 291, 434, 3102, 294], "temperature": 0.0, "avg_logprob": -0.15878395080566407, "compression_ratio": 1.4574468085106382, "no_speech_prob": 7.002155939517252e-07}, {"id": 337, "seek": 148434, "start": 1509.6799999999998, "end": 1511.22, "text": " using a GPU.", "tokens": [1228, 257, 18407, 13], "temperature": 0.0, "avg_logprob": -0.15878395080566407, "compression_ratio": 1.4574468085106382, "no_speech_prob": 7.002155939517252e-07}, {"id": 338, "seek": 151122, "start": 1511.22, "end": 1519.88, "text": " So if you find that when you run a cell in the, from the course and it's going to take", "tokens": [407, 498, 291, 915, 300, 562, 291, 1190, 257, 2815, 294, 264, 11, 490, 264, 1164, 293, 309, 311, 516, 281, 747], "temperature": 0.0, "avg_logprob": -0.1930289798312717, "compression_ratio": 1.575268817204301, "no_speech_prob": 1.0676982356017106e-06}, {"id": 339, "seek": 151122, "start": 1519.88, "end": 1526.52, "text": " like half an hour or an hour or more, it's very likely you forgot to use GPU.", "tokens": [411, 1922, 364, 1773, 420, 364, 1773, 420, 544, 11, 309, 311, 588, 3700, 291, 5298, 281, 764, 18407, 13], "temperature": 0.0, "avg_logprob": -0.1930289798312717, "compression_ratio": 1.575268817204301, "no_speech_prob": 1.0676982356017106e-06}, {"id": 340, "seek": 151122, "start": 1526.52, "end": 1529.6200000000001, "text": " The GPU runs things many hundreds of times faster.", "tokens": [440, 18407, 6676, 721, 867, 6779, 295, 1413, 4663, 13], "temperature": 0.0, "avg_logprob": -0.1930289798312717, "compression_ratio": 1.575268817204301, "no_speech_prob": 1.0676982356017106e-06}, {"id": 341, "seek": 151122, "start": 1529.6200000000001, "end": 1538.52, "text": " So all you do as it says here is go runtime, change runtime type and say GPU.", "tokens": [407, 439, 291, 360, 382, 309, 1619, 510, 307, 352, 34474, 11, 1319, 34474, 2010, 293, 584, 18407, 13], "temperature": 0.0, "avg_logprob": -0.1930289798312717, "compression_ratio": 1.575268817204301, "no_speech_prob": 1.0676982356017106e-06}, {"id": 342, "seek": 153852, "start": 1538.52, "end": 1550.08, "text": " Okay, so now I can run this cell.", "tokens": [1033, 11, 370, 586, 286, 393, 1190, 341, 2815, 13], "temperature": 0.0, "avg_logprob": -0.22023017164589703, "compression_ratio": 1.580110497237569, "no_speech_prob": 2.813003447954543e-06}, {"id": 343, "seek": 153852, "start": 1550.08, "end": 1556.62, "text": " And this is all Python code except lines that start with an explanation mark are actually", "tokens": [400, 341, 307, 439, 15329, 3089, 3993, 3876, 300, 722, 365, 364, 10835, 1491, 366, 767], "temperature": 0.0, "avg_logprob": -0.22023017164589703, "compression_ratio": 1.580110497237569, "no_speech_prob": 2.813003447954543e-06}, {"id": 344, "seek": 153852, "start": 1556.62, "end": 1558.12, "text": " sent to a terminal.", "tokens": [2279, 281, 257, 14709, 13], "temperature": 0.0, "avg_logprob": -0.22023017164589703, "compression_ratio": 1.580110497237569, "no_speech_prob": 2.813003447954543e-06}, {"id": 345, "seek": 153852, "start": 1558.12, "end": 1564.28, "text": " Okay, so pip is something that installs Python software and Fastbook contains all of the", "tokens": [1033, 11, 370, 8489, 307, 746, 300, 3625, 82, 15329, 4722, 293, 15968, 2939, 8306, 439, 295, 264], "temperature": 0.0, "avg_logprob": -0.22023017164589703, "compression_ratio": 1.580110497237569, "no_speech_prob": 2.813003447954543e-06}, {"id": 346, "seek": 153852, "start": 1564.28, "end": 1568.36, "text": " Python software necessarily necessary for the course.", "tokens": [15329, 4722, 4725, 4818, 337, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.22023017164589703, "compression_ratio": 1.580110497237569, "no_speech_prob": 2.813003447954543e-06}, {"id": 347, "seek": 156836, "start": 1568.36, "end": 1571.8799999999999, "text": " And so it's going to go away and set it all up.", "tokens": [400, 370, 309, 311, 516, 281, 352, 1314, 293, 992, 309, 439, 493, 13], "temperature": 0.0, "avg_logprob": -0.13593457726871266, "compression_ratio": 1.6857142857142857, "no_speech_prob": 5.422140475275228e-06}, {"id": 348, "seek": 156836, "start": 1571.8799999999999, "end": 1578.08, "text": " And so this is this like mildly annoying bit.", "tokens": [400, 370, 341, 307, 341, 411, 15154, 356, 11304, 857, 13], "temperature": 0.0, "avg_logprob": -0.13593457726871266, "compression_ratio": 1.6857142857142857, "no_speech_prob": 5.422140475275228e-06}, {"id": 349, "seek": 156836, "start": 1578.08, "end": 1586.28, "text": " You can then connect Colab to Google Drive and that's going to be how you can save your", "tokens": [509, 393, 550, 1745, 4004, 455, 281, 3329, 15622, 293, 300, 311, 516, 281, 312, 577, 291, 393, 3155, 428], "temperature": 0.0, "avg_logprob": -0.13593457726871266, "compression_ratio": 1.6857142857142857, "no_speech_prob": 5.422140475275228e-06}, {"id": 350, "seek": 156836, "start": 1586.28, "end": 1588.36, "text": " notebooks and save your work as you go.", "tokens": [43782, 293, 3155, 428, 589, 382, 291, 352, 13], "temperature": 0.0, "avg_logprob": -0.13593457726871266, "compression_ratio": 1.6857142857142857, "no_speech_prob": 5.422140475275228e-06}, {"id": 351, "seek": 156836, "start": 1588.36, "end": 1590.6, "text": " Okay, I'm not going to do that right now.", "tokens": [1033, 11, 286, 478, 406, 516, 281, 360, 300, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.13593457726871266, "compression_ratio": 1.6857142857142857, "no_speech_prob": 5.422140475275228e-06}, {"id": 352, "seek": 156836, "start": 1590.6, "end": 1595.0, "text": " But if you go to this link that it says and it'll give you a code and then that'll connect", "tokens": [583, 498, 291, 352, 281, 341, 2113, 300, 309, 1619, 293, 309, 603, 976, 291, 257, 3089, 293, 550, 300, 603, 1745], "temperature": 0.0, "avg_logprob": -0.13593457726871266, "compression_ratio": 1.6857142857142857, "no_speech_prob": 5.422140475275228e-06}, {"id": 353, "seek": 159500, "start": 1595.0, "end": 1599.28, "text": " it up to your Google Drive.", "tokens": [309, 493, 281, 428, 3329, 15622, 13], "temperature": 0.0, "avg_logprob": -0.21336112022399903, "compression_ratio": 1.4391891891891893, "no_speech_prob": 3.2887246561585926e-06}, {"id": 354, "seek": 159500, "start": 1599.28, "end": 1607.24, "text": " And so at this point now everything from the for the course is now available.", "tokens": [400, 370, 412, 341, 935, 586, 1203, 490, 264, 337, 264, 1164, 307, 586, 2435, 13], "temperature": 0.0, "avg_logprob": -0.21336112022399903, "compression_ratio": 1.4391891891891893, "no_speech_prob": 3.2887246561585926e-06}, {"id": 355, "seek": 159500, "start": 1607.24, "end": 1611.68, "text": " And you can see the whole book is here.", "tokens": [400, 291, 393, 536, 264, 1379, 1446, 307, 510, 13], "temperature": 0.0, "avg_logprob": -0.21336112022399903, "compression_ratio": 1.4391891891891893, "no_speech_prob": 3.2887246561585926e-06}, {"id": 356, "seek": 159500, "start": 1611.68, "end": 1621.24, "text": " Okay, so here's the book and you can open up sections to read them.", "tokens": [1033, 11, 370, 510, 311, 264, 1446, 293, 291, 393, 1269, 493, 10863, 281, 1401, 552, 13], "temperature": 0.0, "avg_logprob": -0.21336112022399903, "compression_ratio": 1.4391891891891893, "no_speech_prob": 3.2887246561585926e-06}, {"id": 357, "seek": 162124, "start": 1621.24, "end": 1635.96, "text": " Okay, you can go to the table of contents.", "tokens": [1033, 11, 291, 393, 352, 281, 264, 3199, 295, 15768, 13], "temperature": 0.0, "avg_logprob": -0.16620821952819825, "compression_ratio": 1.2522522522522523, "no_speech_prob": 1.4367382163982256e-06}, {"id": 358, "seek": 162124, "start": 1635.96, "end": 1645.92, "text": " And so eventually we'll get to this cell here which contains all the code needed to run", "tokens": [400, 370, 4728, 321, 603, 483, 281, 341, 2815, 510, 597, 8306, 439, 264, 3089, 2978, 281, 1190], "temperature": 0.0, "avg_logprob": -0.16620821952819825, "compression_ratio": 1.2522522522522523, "no_speech_prob": 1.4367382163982256e-06}, {"id": 359, "seek": 162124, "start": 1645.92, "end": 1647.48, "text": " a model.", "tokens": [257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16620821952819825, "compression_ratio": 1.2522522522522523, "no_speech_prob": 1.4367382163982256e-06}, {"id": 360, "seek": 164748, "start": 1647.48, "end": 1654.1200000000001, "text": " So if I click run, here's the way it goes.", "tokens": [407, 498, 286, 2052, 1190, 11, 510, 311, 264, 636, 309, 1709, 13], "temperature": 0.0, "avg_logprob": -0.14353678486134747, "compression_ratio": 1.7045454545454546, "no_speech_prob": 1.6797173429949908e-06}, {"id": 361, "seek": 164748, "start": 1654.1200000000001, "end": 1660.3600000000001, "text": " Now this is going to it's amazing how much this little bit of code is going to do.", "tokens": [823, 341, 307, 516, 281, 309, 311, 2243, 577, 709, 341, 707, 857, 295, 3089, 307, 516, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.14353678486134747, "compression_ratio": 1.7045454545454546, "no_speech_prob": 1.6797173429949908e-06}, {"id": 362, "seek": 164748, "start": 1660.3600000000001, "end": 1666.44, "text": " It's going to download tens of thousands of pictures of dogs and cats.", "tokens": [467, 311, 516, 281, 5484, 10688, 295, 5383, 295, 5242, 295, 7197, 293, 11111, 13], "temperature": 0.0, "avg_logprob": -0.14353678486134747, "compression_ratio": 1.7045454545454546, "no_speech_prob": 1.6797173429949908e-06}, {"id": 363, "seek": 164748, "start": 1666.44, "end": 1672.56, "text": " It's going to use a simple rule to recognize the dogs from the cats based on their file", "tokens": [467, 311, 516, 281, 764, 257, 2199, 4978, 281, 5521, 264, 7197, 490, 264, 11111, 2361, 322, 641, 3991], "temperature": 0.0, "avg_logprob": -0.14353678486134747, "compression_ratio": 1.7045454545454546, "no_speech_prob": 1.6797173429949908e-06}, {"id": 364, "seek": 164748, "start": 1672.56, "end": 1673.56, "text": " names.", "tokens": [5288, 13], "temperature": 0.0, "avg_logprob": -0.14353678486134747, "compression_ratio": 1.7045454545454546, "no_speech_prob": 1.6797173429949908e-06}, {"id": 365, "seek": 164748, "start": 1673.56, "end": 1676.96, "text": " Basically the way that this this has been set up is that you can tell from the file", "tokens": [8537, 264, 636, 300, 341, 341, 575, 668, 992, 493, 307, 300, 291, 393, 980, 490, 264, 3991], "temperature": 0.0, "avg_logprob": -0.14353678486134747, "compression_ratio": 1.7045454545454546, "no_speech_prob": 1.6797173429949908e-06}, {"id": 366, "seek": 167696, "start": 1676.96, "end": 1680.28, "text": " name whether it's a dog or a cat.", "tokens": [1315, 1968, 309, 311, 257, 3000, 420, 257, 3857, 13], "temperature": 0.0, "avg_logprob": -0.12108759085337321, "compression_ratio": 1.936, "no_speech_prob": 5.507537480298197e-06}, {"id": 367, "seek": 167696, "start": 1680.28, "end": 1684.04, "text": " It's then going to download something called a pre-trained model which is something that", "tokens": [467, 311, 550, 516, 281, 5484, 746, 1219, 257, 659, 12, 17227, 2001, 2316, 597, 307, 746, 300], "temperature": 0.0, "avg_logprob": -0.12108759085337321, "compression_ratio": 1.936, "no_speech_prob": 5.507537480298197e-06}, {"id": 368, "seek": 167696, "start": 1684.04, "end": 1689.04, "text": " already knows how to recognize various types of images.", "tokens": [1217, 3255, 577, 281, 5521, 3683, 3467, 295, 5267, 13], "temperature": 0.0, "avg_logprob": -0.12108759085337321, "compression_ratio": 1.936, "no_speech_prob": 5.507537480298197e-06}, {"id": 369, "seek": 167696, "start": 1689.04, "end": 1694.04, "text": " It's then going to construct it's going to then going to train that model to make it", "tokens": [467, 311, 550, 516, 281, 7690, 309, 311, 516, 281, 550, 516, 281, 3847, 300, 2316, 281, 652, 309], "temperature": 0.0, "avg_logprob": -0.12108759085337321, "compression_ratio": 1.936, "no_speech_prob": 5.507537480298197e-06}, {"id": 370, "seek": 167696, "start": 1694.04, "end": 1698.64, "text": " particularly good at recognizing dogs from cats and then it's going to validate that", "tokens": [4098, 665, 412, 18538, 7197, 490, 11111, 293, 550, 309, 311, 516, 281, 29562, 300], "temperature": 0.0, "avg_logprob": -0.12108759085337321, "compression_ratio": 1.936, "no_speech_prob": 5.507537480298197e-06}, {"id": 371, "seek": 167696, "start": 1698.64, "end": 1703.52, "text": " model to see how good it is at recognizing dogs from cats using a set of pictures that", "tokens": [2316, 281, 536, 577, 665, 309, 307, 412, 18538, 7197, 490, 11111, 1228, 257, 992, 295, 5242, 300], "temperature": 0.0, "avg_logprob": -0.12108759085337321, "compression_ratio": 1.936, "no_speech_prob": 5.507537480298197e-06}, {"id": 372, "seek": 167696, "start": 1703.52, "end": 1705.08, "text": " it hasn't seen before.", "tokens": [309, 6132, 380, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.12108759085337321, "compression_ratio": 1.936, "no_speech_prob": 5.507537480298197e-06}, {"id": 373, "seek": 167696, "start": 1705.08, "end": 1706.9, "text": " And that's all happening.", "tokens": [400, 300, 311, 439, 2737, 13], "temperature": 0.0, "avg_logprob": -0.12108759085337321, "compression_ratio": 1.936, "no_speech_prob": 5.507537480298197e-06}, {"id": 374, "seek": 170690, "start": 1706.9, "end": 1709.88, "text": " So so far it's already downloaded the data set.", "tokens": [407, 370, 1400, 309, 311, 1217, 21748, 264, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.10790973239474827, "compression_ratio": 1.6334661354581674, "no_speech_prob": 2.6425648229633225e-06}, {"id": 375, "seek": 170690, "start": 1709.88, "end": 1716.5600000000002, "text": " It's already downloaded the pre-trained model and it's now busily going through the first", "tokens": [467, 311, 1217, 21748, 264, 659, 12, 17227, 2001, 2316, 293, 309, 311, 586, 1255, 953, 516, 807, 264, 700], "temperature": 0.0, "avg_logprob": -0.10790973239474827, "compression_ratio": 1.6334661354581674, "no_speech_prob": 2.6425648229633225e-06}, {"id": 376, "seek": 170690, "start": 1716.5600000000002, "end": 1720.8400000000001, "text": " epoch which is to look at every picture once to try to learn how to recognize dogs from", "tokens": [30992, 339, 597, 307, 281, 574, 412, 633, 3036, 1564, 281, 853, 281, 1466, 577, 281, 5521, 7197, 490], "temperature": 0.0, "avg_logprob": -0.10790973239474827, "compression_ratio": 1.6334661354581674, "no_speech_prob": 2.6425648229633225e-06}, {"id": 377, "seek": 170690, "start": 1720.8400000000001, "end": 1724.16, "text": " cats.", "tokens": [11111, 13], "temperature": 0.0, "avg_logprob": -0.10790973239474827, "compression_ratio": 1.6334661354581674, "no_speech_prob": 2.6425648229633225e-06}, {"id": 378, "seek": 170690, "start": 1724.16, "end": 1725.16, "text": " And that's it.", "tokens": [400, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.10790973239474827, "compression_ratio": 1.6334661354581674, "no_speech_prob": 2.6425648229633225e-06}, {"id": 379, "seek": 170690, "start": 1725.16, "end": 1727.88, "text": " The lines starting with a hash are just comments.", "tokens": [440, 3876, 2891, 365, 257, 22019, 366, 445, 3053, 13], "temperature": 0.0, "avg_logprob": -0.10790973239474827, "compression_ratio": 1.6334661354581674, "no_speech_prob": 2.6425648229633225e-06}, {"id": 380, "seek": 170690, "start": 1727.88, "end": 1733.4, "text": " Because this is also the source of an actual book there's a few like slightly weird comments", "tokens": [1436, 341, 307, 611, 264, 4009, 295, 364, 3539, 1446, 456, 311, 257, 1326, 411, 4748, 3657, 3053], "temperature": 0.0, "avg_logprob": -0.10790973239474827, "compression_ratio": 1.6334661354581674, "no_speech_prob": 2.6425648229633225e-06}, {"id": 381, "seek": 170690, "start": 1733.4, "end": 1734.4, "text": " that you can ignore.", "tokens": [300, 291, 393, 11200, 13], "temperature": 0.0, "avg_logprob": -0.10790973239474827, "compression_ratio": 1.6334661354581674, "no_speech_prob": 2.6425648229633225e-06}, {"id": 382, "seek": 173440, "start": 1734.4, "end": 1738.52, "text": " Just things that are used for setting up references in the book.", "tokens": [1449, 721, 300, 366, 1143, 337, 3287, 493, 15400, 294, 264, 1446, 13], "temperature": 0.0, "avg_logprob": -0.2095848221376718, "compression_ratio": 1.4825870646766168, "no_speech_prob": 3.5559241950977594e-06}, {"id": 383, "seek": 173440, "start": 1738.52, "end": 1740.1200000000001, "text": " There's the caption so forth.", "tokens": [821, 311, 264, 31974, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.2095848221376718, "compression_ratio": 1.4825870646766168, "no_speech_prob": 3.5559241950977594e-06}, {"id": 384, "seek": 173440, "start": 1740.1200000000001, "end": 1746.52, "text": " Okay so it's now testing out I think that first epoch.", "tokens": [1033, 370, 309, 311, 586, 4997, 484, 286, 519, 300, 700, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.2095848221376718, "compression_ratio": 1.4825870646766168, "no_speech_prob": 3.5559241950977594e-06}, {"id": 385, "seek": 173440, "start": 1746.52, "end": 1751.0800000000002, "text": " Okay so it's finished an epoch and so far it's got a 1% error rate.", "tokens": [1033, 370, 309, 311, 4335, 364, 30992, 339, 293, 370, 1400, 309, 311, 658, 257, 502, 4, 6713, 3314, 13], "temperature": 0.0, "avg_logprob": -0.2095848221376718, "compression_ratio": 1.4825870646766168, "no_speech_prob": 3.5559241950977594e-06}, {"id": 386, "seek": 173440, "start": 1751.0800000000002, "end": 1760.0, "text": " So after 54 seconds it has learnt to recognize dogs from cats with 99% accuracy.", "tokens": [407, 934, 20793, 3949, 309, 575, 18991, 281, 5521, 7197, 490, 11111, 365, 11803, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.2095848221376718, "compression_ratio": 1.4825870646766168, "no_speech_prob": 3.5559241950977594e-06}, {"id": 387, "seek": 176000, "start": 1760.0, "end": 1765.6, "text": " And so yeah we're going to let that finish off.", "tokens": [400, 370, 1338, 321, 434, 516, 281, 718, 300, 2413, 766, 13], "temperature": 0.0, "avg_logprob": -0.15679319893441548, "compression_ratio": 1.205607476635514, "no_speech_prob": 5.1738156798819546e-06}, {"id": 388, "seek": 176000, "start": 1765.6, "end": 1767.52, "text": " So that's how we get started with Colab.", "tokens": [407, 300, 311, 577, 321, 483, 1409, 365, 4004, 455, 13], "temperature": 0.0, "avg_logprob": -0.15679319893441548, "compression_ratio": 1.205607476635514, "no_speech_prob": 5.1738156798819546e-06}, {"id": 389, "seek": 176000, "start": 1767.52, "end": 1774.52, "text": " Okay and there's nothing else to set up.", "tokens": [1033, 293, 456, 311, 1825, 1646, 281, 992, 493, 13], "temperature": 0.0, "avg_logprob": -0.15679319893441548, "compression_ratio": 1.205607476635514, "no_speech_prob": 5.1738156798819546e-06}, {"id": 390, "seek": 177452, "start": 1774.52, "end": 1797.6399999999999, "text": " Now what you can do is you can open Notebook and you can open a notebook from GitHub.", "tokens": [823, 437, 291, 393, 360, 307, 291, 393, 1269, 11633, 2939, 293, 291, 393, 1269, 257, 21060, 490, 23331, 13], "temperature": 0.0, "avg_logprob": -0.11249451835950215, "compression_ratio": 1.2686567164179106, "no_speech_prob": 3.668811132229166e-06}, {"id": 391, "seek": 179764, "start": 1797.64, "end": 1804.64, "text": " And here is the Fastbook repository.", "tokens": [400, 510, 307, 264, 15968, 2939, 25841, 13], "temperature": 0.0, "avg_logprob": -0.142262790979964, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.57080601438065e-07}, {"id": 392, "seek": 179764, "start": 1804.64, "end": 1808.72, "text": " And you'll see in the Fastbook repository for every notebook there's a second copy inside", "tokens": [400, 291, 603, 536, 294, 264, 15968, 2939, 25841, 337, 633, 21060, 456, 311, 257, 1150, 5055, 1854], "temperature": 0.0, "avg_logprob": -0.142262790979964, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.57080601438065e-07}, {"id": 393, "seek": 179764, "start": 1808.72, "end": 1811.38, "text": " the clean folder with the same name.", "tokens": [264, 2541, 10820, 365, 264, 912, 1315, 13], "temperature": 0.0, "avg_logprob": -0.142262790979964, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.57080601438065e-07}, {"id": 394, "seek": 179764, "start": 1811.38, "end": 1813.76, "text": " So I was just looking at 01intro.", "tokens": [407, 286, 390, 445, 1237, 412, 23185, 686, 340, 13], "temperature": 0.0, "avg_logprob": -0.142262790979964, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.57080601438065e-07}, {"id": 395, "seek": 179764, "start": 1813.76, "end": 1815.72, "text": " There's also a clean 01intro.", "tokens": [821, 311, 611, 257, 2541, 23185, 686, 340, 13], "temperature": 0.0, "avg_logprob": -0.142262790979964, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.57080601438065e-07}, {"id": 396, "seek": 179764, "start": 1815.72, "end": 1826.5200000000002, "text": " If I open that up you'll see that it's got exactly the same thing as the last one I was", "tokens": [759, 286, 1269, 300, 493, 291, 603, 536, 300, 309, 311, 658, 2293, 264, 912, 551, 382, 264, 1036, 472, 286, 390], "temperature": 0.0, "avg_logprob": -0.142262790979964, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.57080601438065e-07}, {"id": 397, "seek": 182652, "start": 1826.52, "end": 1830.6399999999999, "text": " just looking at but all the pros is now missing.", "tokens": [445, 1237, 412, 457, 439, 264, 6267, 307, 586, 5361, 13], "temperature": 0.0, "avg_logprob": -0.10179738998413086, "compression_ratio": 1.6043956043956045, "no_speech_prob": 7.571124456262623e-07}, {"id": 398, "seek": 182652, "start": 1830.6399999999999, "end": 1834.56, "text": " It's just got headings and code.", "tokens": [467, 311, 445, 658, 1378, 1109, 293, 3089, 13], "temperature": 0.0, "avg_logprob": -0.10179738998413086, "compression_ratio": 1.6043956043956045, "no_speech_prob": 7.571124456262623e-07}, {"id": 399, "seek": 182652, "start": 1834.56, "end": 1837.8799999999999, "text": " Also all the outputs are missing.", "tokens": [2743, 439, 264, 23930, 366, 5361, 13], "temperature": 0.0, "avg_logprob": -0.10179738998413086, "compression_ratio": 1.6043956043956045, "no_speech_prob": 7.571124456262623e-07}, {"id": 400, "seek": 182652, "start": 1837.8799999999999, "end": 1845.8, "text": " So the reason that we have this clean version is to help you with these stages here is our", "tokens": [407, 264, 1778, 300, 321, 362, 341, 2541, 3037, 307, 281, 854, 291, 365, 613, 10232, 510, 307, 527], "temperature": 0.0, "avg_logprob": -0.10179738998413086, "compression_ratio": 1.6043956043956045, "no_speech_prob": 7.571124456262623e-07}, {"id": 401, "seek": 182652, "start": 1845.8, "end": 1852.2, "text": " suggestion is once you've gone through the lesson and you've run the notebook and you", "tokens": [16541, 307, 1564, 291, 600, 2780, 807, 264, 6898, 293, 291, 600, 1190, 264, 21060, 293, 291], "temperature": 0.0, "avg_logprob": -0.10179738998413086, "compression_ratio": 1.6043956043956045, "no_speech_prob": 7.571124456262623e-07}, {"id": 402, "seek": 185220, "start": 1852.2, "end": 1859.72, "text": " feel like okay I think I get it is you open up this clean version and before you run each", "tokens": [841, 411, 1392, 286, 519, 286, 483, 309, 307, 291, 1269, 493, 341, 2541, 3037, 293, 949, 291, 1190, 1184], "temperature": 0.0, "avg_logprob": -0.11501048766460616, "compression_ratio": 1.702970297029703, "no_speech_prob": 3.785303988479427e-06}, {"id": 403, "seek": 185220, "start": 1859.72, "end": 1864.8400000000001, "text": " cell try to think okay why is this cell here?", "tokens": [2815, 853, 281, 519, 1392, 983, 307, 341, 2815, 510, 30], "temperature": 0.0, "avg_logprob": -0.11501048766460616, "compression_ratio": 1.702970297029703, "no_speech_prob": 3.785303988479427e-06}, {"id": 404, "seek": 185220, "start": 1864.8400000000001, "end": 1866.1000000000001, "text": " What's it for?", "tokens": [708, 311, 309, 337, 30], "temperature": 0.0, "avg_logprob": -0.11501048766460616, "compression_ratio": 1.702970297029703, "no_speech_prob": 3.785303988479427e-06}, {"id": 405, "seek": 185220, "start": 1866.1000000000001, "end": 1867.3600000000001, "text": " What's it going to do?", "tokens": [708, 311, 309, 516, 281, 360, 30], "temperature": 0.0, "avg_logprob": -0.11501048766460616, "compression_ratio": 1.702970297029703, "no_speech_prob": 3.785303988479427e-06}, {"id": 406, "seek": 185220, "start": 1867.3600000000001, "end": 1869.68, "text": " What's the output going to look like?", "tokens": [708, 311, 264, 5598, 516, 281, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.11501048766460616, "compression_ratio": 1.702970297029703, "no_speech_prob": 3.785303988479427e-06}, {"id": 407, "seek": 185220, "start": 1869.68, "end": 1875.56, "text": " So once you remove all that context this is a good test for you to kind of get your brain", "tokens": [407, 1564, 291, 4159, 439, 300, 4319, 341, 307, 257, 665, 1500, 337, 291, 281, 733, 295, 483, 428, 3567], "temperature": 0.0, "avg_logprob": -0.11501048766460616, "compression_ratio": 1.702970297029703, "no_speech_prob": 3.785303988479427e-06}, {"id": 408, "seek": 185220, "start": 1875.56, "end": 1878.54, "text": " going to think what was actually going on.", "tokens": [516, 281, 519, 437, 390, 767, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.11501048766460616, "compression_ratio": 1.702970297029703, "no_speech_prob": 3.785303988479427e-06}, {"id": 409, "seek": 187854, "start": 1878.54, "end": 1883.94, "text": " So this is a kind of much more active approach to reading and recall.", "tokens": [407, 341, 307, 257, 733, 295, 709, 544, 4967, 3109, 281, 3760, 293, 9901, 13], "temperature": 0.0, "avg_logprob": -0.0816088318824768, "compression_ratio": 1.6480446927374302, "no_speech_prob": 2.6425705073052086e-06}, {"id": 410, "seek": 187854, "start": 1883.94, "end": 1891.92, "text": " And so then once you've done that and you've finished going through this at the bottom", "tokens": [400, 370, 550, 1564, 291, 600, 1096, 300, 293, 291, 600, 4335, 516, 807, 341, 412, 264, 2767], "temperature": 0.0, "avg_logprob": -0.0816088318824768, "compression_ratio": 1.6480446927374302, "no_speech_prob": 2.6425705073052086e-06}, {"id": 411, "seek": 187854, "start": 1891.92, "end": 1895.82, "text": " one thing that is kept is the questionnaire.", "tokens": [472, 551, 300, 307, 4305, 307, 264, 44702, 13], "temperature": 0.0, "avg_logprob": -0.0816088318824768, "compression_ratio": 1.6480446927374302, "no_speech_prob": 2.6425705073052086e-06}, {"id": 412, "seek": 187854, "start": 1895.82, "end": 1901.32, "text": " So at the end of every chapter is a questionnaire and so then at this point you should now as", "tokens": [407, 412, 264, 917, 295, 633, 7187, 307, 257, 44702, 293, 370, 550, 412, 341, 935, 291, 820, 586, 382], "temperature": 0.0, "avg_logprob": -0.0816088318824768, "compression_ratio": 1.6480446927374302, "no_speech_prob": 2.6425705073052086e-06}, {"id": 413, "seek": 190132, "start": 1901.32, "end": 1908.52, "text": " much as you can without looking go through and try to answer each of those questions.", "tokens": [709, 382, 291, 393, 1553, 1237, 352, 807, 293, 853, 281, 1867, 1184, 295, 729, 1651, 13], "temperature": 0.0, "avg_logprob": -0.1139817061247649, "compression_ratio": 1.86864406779661, "no_speech_prob": 1.6536779412490432e-06}, {"id": 414, "seek": 190132, "start": 1908.52, "end": 1914.0, "text": " They all have answers in the notebook, in the book, okay so you can you know if you", "tokens": [814, 439, 362, 6338, 294, 264, 21060, 11, 294, 264, 1446, 11, 1392, 370, 291, 393, 291, 458, 498, 291], "temperature": 0.0, "avg_logprob": -0.1139817061247649, "compression_ratio": 1.86864406779661, "no_speech_prob": 1.6536779412490432e-06}, {"id": 415, "seek": 190132, "start": 1914.0, "end": 1919.2, "text": " can't remember you can always look it up but you know if you can't remember that's a sign", "tokens": [393, 380, 1604, 291, 393, 1009, 574, 309, 493, 457, 291, 458, 498, 291, 393, 380, 1604, 300, 311, 257, 1465], "temperature": 0.0, "avg_logprob": -0.1139817061247649, "compression_ratio": 1.86864406779661, "no_speech_prob": 1.6536779412490432e-06}, {"id": 416, "seek": 190132, "start": 1919.2, "end": 1924.24, "text": " to you that like oh you know did I skip over that bit too quickly like what what's happened", "tokens": [281, 291, 300, 411, 1954, 291, 458, 630, 286, 10023, 670, 300, 857, 886, 2661, 411, 437, 437, 311, 2011], "temperature": 0.0, "avg_logprob": -0.1139817061247649, "compression_ratio": 1.86864406779661, "no_speech_prob": 1.6536779412490432e-06}, {"id": 417, "seek": 190132, "start": 1924.24, "end": 1930.48, "text": " that I've not remembered and then try to remind yourself and then go back and yeah finish", "tokens": [300, 286, 600, 406, 13745, 293, 550, 853, 281, 4160, 1803, 293, 550, 352, 646, 293, 1338, 2413], "temperature": 0.0, "avg_logprob": -0.1139817061247649, "compression_ratio": 1.86864406779661, "no_speech_prob": 1.6536779412490432e-06}, {"id": 418, "seek": 193048, "start": 1930.48, "end": 1931.48, "text": " the questionnaire.", "tokens": [264, 44702, 13], "temperature": 0.0, "avg_logprob": -0.12780990600585937, "compression_ratio": 1.3880597014925373, "no_speech_prob": 5.59429099666886e-06}, {"id": 419, "seek": 193048, "start": 1931.48, "end": 1939.68, "text": " Okay so there's a lot of pieces to help take this from a passive I'm just watching a video", "tokens": [1033, 370, 456, 311, 257, 688, 295, 3755, 281, 854, 747, 341, 490, 257, 14975, 286, 478, 445, 1976, 257, 960], "temperature": 0.0, "avg_logprob": -0.12780990600585937, "compression_ratio": 1.3880597014925373, "no_speech_prob": 5.59429099666886e-06}, {"id": 420, "seek": 193048, "start": 1939.68, "end": 1953.72, "text": " I'm just reading a book into a participatory exercise that you're a part of.", "tokens": [286, 478, 445, 3760, 257, 1446, 666, 257, 3421, 4745, 5380, 300, 291, 434, 257, 644, 295, 13], "temperature": 0.0, "avg_logprob": -0.12780990600585937, "compression_ratio": 1.3880597014925373, "no_speech_prob": 5.59429099666886e-06}, {"id": 421, "seek": 195372, "start": 1953.72, "end": 1963.68, "text": " So as soon as you can we want you to create something that's yours and so this is the", "tokens": [407, 382, 2321, 382, 291, 393, 321, 528, 291, 281, 1884, 746, 300, 311, 6342, 293, 370, 341, 307, 264], "temperature": 0.0, "avg_logprob": -0.10588927220816564, "compression_ratio": 1.737327188940092, "no_speech_prob": 2.9943610115878982e-06}, {"id": 422, "seek": 195372, "start": 1963.68, "end": 1967.84, "text": " easiest way to do that is basically at the end of lesson one once you kind of up and", "tokens": [12889, 636, 281, 360, 300, 307, 1936, 412, 264, 917, 295, 6898, 472, 1564, 291, 733, 295, 493, 293], "temperature": 0.0, "avg_logprob": -0.10588927220816564, "compression_ratio": 1.737327188940092, "no_speech_prob": 2.9943610115878982e-06}, {"id": 423, "seek": 195372, "start": 1967.84, "end": 1971.84, "text": " running try to do it with your own data set.", "tokens": [2614, 853, 281, 360, 309, 365, 428, 1065, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.10588927220816564, "compression_ratio": 1.737327188940092, "no_speech_prob": 2.9943610115878982e-06}, {"id": 424, "seek": 195372, "start": 1971.84, "end": 1978.82, "text": " And if you go to forums.fast.ai which is something that you're going to want to be deeply familiar", "tokens": [400, 498, 291, 352, 281, 26998, 13, 7011, 13, 1301, 597, 307, 746, 300, 291, 434, 516, 281, 528, 281, 312, 8760, 4963], "temperature": 0.0, "avg_logprob": -0.10588927220816564, "compression_ratio": 1.737327188940092, "no_speech_prob": 2.9943610115878982e-06}, {"id": 425, "seek": 195372, "start": 1978.82, "end": 1982.78, "text": " with because this is going to be full of people just like you.", "tokens": [365, 570, 341, 307, 516, 281, 312, 1577, 295, 561, 445, 411, 291, 13], "temperature": 0.0, "avg_logprob": -0.10588927220816564, "compression_ratio": 1.737327188940092, "no_speech_prob": 2.9943610115878982e-06}, {"id": 426, "seek": 198278, "start": 1982.78, "end": 1989.24, "text": " The people who want to learn deep learning okay and these people are all asking questions", "tokens": [440, 561, 567, 528, 281, 1466, 2452, 2539, 1392, 293, 613, 561, 366, 439, 3365, 1651], "temperature": 0.0, "avg_logprob": -0.11577097191868058, "compression_ratio": 1.7473118279569892, "no_speech_prob": 9.368548489874229e-06}, {"id": 427, "seek": 198278, "start": 1989.24, "end": 1994.8799999999999, "text": " and making comments and you can see there's like a lot going on all the time and so you", "tokens": [293, 1455, 3053, 293, 291, 393, 536, 456, 311, 411, 257, 688, 516, 322, 439, 264, 565, 293, 370, 291], "temperature": 0.0, "avg_logprob": -0.11577097191868058, "compression_ratio": 1.7473118279569892, "no_speech_prob": 9.368548489874229e-06}, {"id": 428, "seek": 198278, "start": 1994.8799999999999, "end": 2002.46, "text": " can see here's the part one course topic.", "tokens": [393, 536, 510, 311, 264, 644, 472, 1164, 4829, 13], "temperature": 0.0, "avg_logprob": -0.11577097191868058, "compression_ratio": 1.7473118279569892, "no_speech_prob": 9.368548489874229e-06}, {"id": 429, "seek": 198278, "start": 2002.46, "end": 2006.44, "text": " And you can see there's 1.4 thousand topics there and each one is going to have lots and", "tokens": [400, 291, 393, 536, 456, 311, 502, 13, 19, 4714, 8378, 456, 293, 1184, 472, 307, 516, 281, 362, 3195, 293], "temperature": 0.0, "avg_logprob": -0.11577097191868058, "compression_ratio": 1.7473118279569892, "no_speech_prob": 9.368548489874229e-06}, {"id": 430, "seek": 198278, "start": 2006.44, "end": 2009.68, "text": " lots of replies.", "tokens": [3195, 295, 42289, 13], "temperature": 0.0, "avg_logprob": -0.11577097191868058, "compression_ratio": 1.7473118279569892, "no_speech_prob": 9.368548489874229e-06}, {"id": 431, "seek": 200968, "start": 2009.68, "end": 2015.68, "text": " So this is where amongst other things you'll find if you search for it something called", "tokens": [407, 341, 307, 689, 12918, 661, 721, 291, 603, 915, 498, 291, 3164, 337, 309, 746, 1219], "temperature": 0.0, "avg_logprob": -0.11024113705283717, "compression_ratio": 1.5865384615384615, "no_speech_prob": 5.368723918763862e-07}, {"id": 432, "seek": 200968, "start": 2015.68, "end": 2021.44, "text": " share your work here which has 2,000 replies and you can see links to and pictures of lots", "tokens": [2073, 428, 589, 510, 597, 575, 568, 11, 1360, 42289, 293, 291, 393, 536, 6123, 281, 293, 5242, 295, 3195], "temperature": 0.0, "avg_logprob": -0.11024113705283717, "compression_ratio": 1.5865384615384615, "no_speech_prob": 5.368723918763862e-07}, {"id": 433, "seek": 200968, "start": 2021.44, "end": 2027.02, "text": " of examples of things that other people have done after the first week or two of the course", "tokens": [295, 5110, 295, 721, 300, 661, 561, 362, 1096, 934, 264, 700, 1243, 420, 732, 295, 264, 1164], "temperature": 0.0, "avg_logprob": -0.11024113705283717, "compression_ratio": 1.5865384615384615, "no_speech_prob": 5.368723918763862e-07}, {"id": 434, "seek": 200968, "start": 2027.02, "end": 2032.92, "text": " and so hopefully that might help give you some inspiration.", "tokens": [293, 370, 4696, 300, 1062, 854, 976, 291, 512, 10249, 13], "temperature": 0.0, "avg_logprob": -0.11024113705283717, "compression_ratio": 1.5865384615384615, "no_speech_prob": 5.368723918763862e-07}, {"id": 435, "seek": 203292, "start": 2032.92, "end": 2040.6000000000001, "text": " And it would be great if you could reply and add a picture or a link to what you build", "tokens": [400, 309, 576, 312, 869, 498, 291, 727, 16972, 293, 909, 257, 3036, 420, 257, 2113, 281, 437, 291, 1322], "temperature": 0.0, "avg_logprob": -0.1301473223644754, "compression_ratio": 1.618421052631579, "no_speech_prob": 3.6119054129812866e-06}, {"id": 436, "seek": 203292, "start": 2040.6000000000001, "end": 2048.1, "text": " and you'll see you know everybody is very positive to each other on the forums in general", "tokens": [293, 291, 603, 536, 291, 458, 2201, 307, 588, 3353, 281, 1184, 661, 322, 264, 26998, 294, 2674], "temperature": 0.0, "avg_logprob": -0.1301473223644754, "compression_ratio": 1.618421052631579, "no_speech_prob": 3.6119054129812866e-06}, {"id": 437, "seek": 203292, "start": 2048.1, "end": 2049.88, "text": " and in this topic in particular.", "tokens": [293, 294, 341, 4829, 294, 1729, 13], "temperature": 0.0, "avg_logprob": -0.1301473223644754, "compression_ratio": 1.618421052631579, "no_speech_prob": 3.6119054129812866e-06}, {"id": 438, "seek": 203292, "start": 2049.88, "end": 2055.38, "text": " Nobody's going to go oh my god I could have done that years ago right.", "tokens": [9297, 311, 516, 281, 352, 1954, 452, 3044, 286, 727, 362, 1096, 300, 924, 2057, 558, 13], "temperature": 0.0, "avg_logprob": -0.1301473223644754, "compression_ratio": 1.618421052631579, "no_speech_prob": 3.6119054129812866e-06}, {"id": 439, "seek": 203292, "start": 2055.38, "end": 2060.32, "text": " People are going to be excited for you that you have now joined the ranks of people that", "tokens": [3432, 366, 516, 281, 312, 2919, 337, 291, 300, 291, 362, 586, 6869, 264, 21406, 295, 561, 300], "temperature": 0.0, "avg_logprob": -0.1301473223644754, "compression_ratio": 1.618421052631579, "no_speech_prob": 3.6119054129812866e-06}, {"id": 440, "seek": 206032, "start": 2060.32, "end": 2070.2000000000003, "text": " have built their first deep learning model and I will be excited for you.", "tokens": [362, 3094, 641, 700, 2452, 2539, 2316, 293, 286, 486, 312, 2919, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.10709000811164762, "compression_ratio": 1.5242718446601942, "no_speech_prob": 8.800713658274617e-06}, {"id": 441, "seek": 206032, "start": 2070.2000000000003, "end": 2079.6800000000003, "text": " So as I said Radek this is again from his book expresses in his book a way of not doing", "tokens": [407, 382, 286, 848, 497, 762, 74, 341, 307, 797, 490, 702, 1446, 39204, 294, 702, 1446, 257, 636, 295, 406, 884], "temperature": 0.0, "avg_logprob": -0.10709000811164762, "compression_ratio": 1.5242718446601942, "no_speech_prob": 8.800713658274617e-06}, {"id": 442, "seek": 206032, "start": 2079.6800000000003, "end": 2084.04, "text": " fast AI which I have heard now probably hundreds of times.", "tokens": [2370, 7318, 597, 286, 362, 2198, 586, 1391, 6779, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.10709000811164762, "compression_ratio": 1.5242718446601942, "no_speech_prob": 8.800713658274617e-06}, {"id": 443, "seek": 206032, "start": 2084.04, "end": 2090.1200000000003, "text": " I don't know why this is so common but many many people do what Radek did which was basically", "tokens": [286, 500, 380, 458, 983, 341, 307, 370, 2689, 457, 867, 867, 561, 360, 437, 497, 762, 74, 630, 597, 390, 1936], "temperature": 0.0, "avg_logprob": -0.10709000811164762, "compression_ratio": 1.5242718446601942, "no_speech_prob": 8.800713658274617e-06}, {"id": 444, "seek": 209012, "start": 2090.12, "end": 2094.7999999999997, "text": " to learn all these math things right.", "tokens": [281, 1466, 439, 613, 5221, 721, 558, 13], "temperature": 0.0, "avg_logprob": -0.09494240624564035, "compression_ratio": 1.89375, "no_speech_prob": 7.48037637094967e-05}, {"id": 445, "seek": 209012, "start": 2094.7999999999997, "end": 2101.48, "text": " So he started with calculus and then once he got to a certain point in calculus he found", "tokens": [407, 415, 1409, 365, 33400, 293, 550, 1564, 415, 658, 281, 257, 1629, 935, 294, 33400, 415, 1352], "temperature": 0.0, "avg_logprob": -0.09494240624564035, "compression_ratio": 1.89375, "no_speech_prob": 7.48037637094967e-05}, {"id": 446, "seek": 209012, "start": 2101.48, "end": 2108.24, "text": " that he had to start understanding real analysis and then as he started understanding real", "tokens": [300, 415, 632, 281, 722, 3701, 957, 5215, 293, 550, 382, 415, 1409, 3701, 957], "temperature": 0.0, "avg_logprob": -0.09494240624564035, "compression_ratio": 1.89375, "no_speech_prob": 7.48037637094967e-05}, {"id": 447, "seek": 209012, "start": 2108.24, "end": 2113.7999999999997, "text": " analysis he had found he had to learn set theory you know and you get the idea right.", "tokens": [5215, 415, 632, 1352, 415, 632, 281, 1466, 992, 5261, 291, 458, 293, 291, 483, 264, 1558, 558, 13], "temperature": 0.0, "avg_logprob": -0.09494240624564035, "compression_ratio": 1.89375, "no_speech_prob": 7.48037637094967e-05}, {"id": 448, "seek": 211380, "start": 2113.8, "end": 2120.7200000000003, "text": " If you want to learn all of math that's going to take a while.", "tokens": [759, 291, 528, 281, 1466, 439, 295, 5221, 300, 311, 516, 281, 747, 257, 1339, 13], "temperature": 0.0, "avg_logprob": -0.09629712047347103, "compression_ratio": 1.665, "no_speech_prob": 3.905392532033147e-06}, {"id": 449, "seek": 211380, "start": 2120.7200000000003, "end": 2125.6800000000003, "text": " There's a lot of gatekeeping out there that says like oh if you're going to be a real", "tokens": [821, 311, 257, 688, 295, 8539, 25769, 484, 456, 300, 1619, 411, 1954, 498, 291, 434, 516, 281, 312, 257, 957], "temperature": 0.0, "avg_logprob": -0.09629712047347103, "compression_ratio": 1.665, "no_speech_prob": 3.905392532033147e-06}, {"id": 450, "seek": 211380, "start": 2125.6800000000003, "end": 2130.6400000000003, "text": " deep learning practitioner you have to finish you know a graduate level course in linear", "tokens": [2452, 2539, 32125, 291, 362, 281, 2413, 291, 458, 257, 8080, 1496, 1164, 294, 8213], "temperature": 0.0, "avg_logprob": -0.09629712047347103, "compression_ratio": 1.665, "no_speech_prob": 3.905392532033147e-06}, {"id": 451, "seek": 211380, "start": 2130.6400000000003, "end": 2131.6400000000003, "text": " algebra.", "tokens": [21989, 13], "temperature": 0.0, "avg_logprob": -0.09629712047347103, "compression_ratio": 1.665, "no_speech_prob": 3.905392532033147e-06}, {"id": 452, "seek": 211380, "start": 2131.6400000000003, "end": 2139.6000000000004, "text": " Here's the truth the actual linear algebra you do in in basically all deep learning is", "tokens": [1692, 311, 264, 3494, 264, 3539, 8213, 21989, 291, 360, 294, 294, 1936, 439, 2452, 2539, 307], "temperature": 0.0, "avg_logprob": -0.09629712047347103, "compression_ratio": 1.665, "no_speech_prob": 3.905392532033147e-06}, {"id": 453, "seek": 213960, "start": 2139.6, "end": 2145.94, "text": " matrix multiplication and if you've forgotten what that is that is multiplying things together", "tokens": [8141, 27290, 293, 498, 291, 600, 11832, 437, 300, 307, 300, 307, 30955, 721, 1214], "temperature": 0.0, "avg_logprob": -0.1156163215637207, "compression_ratio": 1.8426966292134832, "no_speech_prob": 1.1659292795229703e-05}, {"id": 454, "seek": 213960, "start": 2145.94, "end": 2148.24, "text": " and then adding them up.", "tokens": [293, 550, 5127, 552, 493, 13], "temperature": 0.0, "avg_logprob": -0.1156163215637207, "compression_ratio": 1.8426966292134832, "no_speech_prob": 1.1659292795229703e-05}, {"id": 455, "seek": 213960, "start": 2148.24, "end": 2153.22, "text": " So what you need to be able to do is multiply things together and add them up.", "tokens": [407, 437, 291, 643, 281, 312, 1075, 281, 360, 307, 12972, 721, 1214, 293, 909, 552, 493, 13], "temperature": 0.0, "avg_logprob": -0.1156163215637207, "compression_ratio": 1.8426966292134832, "no_speech_prob": 1.1659292795229703e-05}, {"id": 456, "seek": 213960, "start": 2153.22, "end": 2157.52, "text": " So if you can do that you're good to go.", "tokens": [407, 498, 291, 393, 360, 300, 291, 434, 665, 281, 352, 13], "temperature": 0.0, "avg_logprob": -0.1156163215637207, "compression_ratio": 1.8426966292134832, "no_speech_prob": 1.1659292795229703e-05}, {"id": 457, "seek": 213960, "start": 2157.52, "end": 2165.12, "text": " So yeah don't get you know you're not going to finish it if a you never start it because", "tokens": [407, 1338, 500, 380, 483, 291, 458, 291, 434, 406, 516, 281, 2413, 309, 498, 257, 291, 1128, 722, 309, 570], "temperature": 0.0, "avg_logprob": -0.1156163215637207, "compression_ratio": 1.8426966292134832, "no_speech_prob": 1.1659292795229703e-05}, {"id": 458, "seek": 216512, "start": 2165.12, "end": 2170.24, "text": " you keep preparing or B you keep thinking oh I wonder exactly what's happening here", "tokens": [291, 1066, 10075, 420, 363, 291, 1066, 1953, 1954, 286, 2441, 2293, 437, 311, 2737, 510], "temperature": 0.0, "avg_logprob": -0.10787805236212097, "compression_ratio": 1.838174273858921, "no_speech_prob": 4.710864686785499e-06}, {"id": 459, "seek": 216512, "start": 2170.24, "end": 2173.64, "text": " and you go all the way down to the bottom until you found yourself in the midst of set", "tokens": [293, 291, 352, 439, 264, 636, 760, 281, 264, 2767, 1826, 291, 1352, 1803, 294, 264, 18629, 295, 992], "temperature": 0.0, "avg_logprob": -0.10787805236212097, "compression_ratio": 1.838174273858921, "no_speech_prob": 4.710864686785499e-06}, {"id": 460, "seek": 216512, "start": 2173.64, "end": 2174.64, "text": " theory.", "tokens": [5261, 13], "temperature": 0.0, "avg_logprob": -0.10787805236212097, "compression_ratio": 1.838174273858921, "no_speech_prob": 4.710864686785499e-06}, {"id": 461, "seek": 216512, "start": 2174.64, "end": 2183.2, "text": " Don't worry you'll get deeper and deeper over time but if you're learning mathematical theory", "tokens": [1468, 380, 3292, 291, 603, 483, 7731, 293, 7731, 670, 565, 457, 498, 291, 434, 2539, 18894, 5261], "temperature": 0.0, "avg_logprob": -0.10787805236212097, "compression_ratio": 1.838174273858921, "no_speech_prob": 4.710864686785499e-06}, {"id": 462, "seek": 216512, "start": 2183.2, "end": 2187.2, "text": " you're not coding you're not experimenting you're not practicing you're not actually", "tokens": [291, 434, 406, 17720, 291, 434, 406, 29070, 291, 434, 406, 11350, 291, 434, 406, 767], "temperature": 0.0, "avg_logprob": -0.10787805236212097, "compression_ratio": 1.838174273858921, "no_speech_prob": 4.710864686785499e-06}, {"id": 463, "seek": 216512, "start": 2187.2, "end": 2191.8399999999997, "text": " building deep learning models and if you're watching this course and your goal is not", "tokens": [2390, 2452, 2539, 5245, 293, 498, 291, 434, 1976, 341, 1164, 293, 428, 3387, 307, 406], "temperature": 0.0, "avg_logprob": -0.10787805236212097, "compression_ratio": 1.838174273858921, "no_speech_prob": 4.710864686785499e-06}, {"id": 464, "seek": 219184, "start": 2191.84, "end": 2195.6000000000004, "text": " to build deep learning models you're in the wrong course.", "tokens": [281, 1322, 2452, 2539, 5245, 291, 434, 294, 264, 2085, 1164, 13], "temperature": 0.0, "avg_logprob": -0.11836252333242682, "compression_ratio": 1.7032967032967032, "no_speech_prob": 6.577913040928252e-07}, {"id": 465, "seek": 219184, "start": 2195.6000000000004, "end": 2206.4, "text": " And if your goal is to build deep learning models then don't do this.", "tokens": [400, 498, 428, 3387, 307, 281, 1322, 2452, 2539, 5245, 550, 500, 380, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.11836252333242682, "compression_ratio": 1.7032967032967032, "no_speech_prob": 6.577913040928252e-07}, {"id": 466, "seek": 219184, "start": 2206.4, "end": 2216.0, "text": " So as Radek says here it's as you train actual models that you're going to get feedback right", "tokens": [407, 382, 497, 762, 74, 1619, 510, 309, 311, 382, 291, 3847, 3539, 5245, 300, 291, 434, 516, 281, 483, 5824, 558], "temperature": 0.0, "avg_logprob": -0.11836252333242682, "compression_ratio": 1.7032967032967032, "no_speech_prob": 6.577913040928252e-07}, {"id": 467, "seek": 219184, "start": 2216.0, "end": 2221.76, "text": " and the feedback that a lot of people get is oh my god I can already train useful models", "tokens": [293, 264, 5824, 300, 257, 688, 295, 561, 483, 307, 1954, 452, 3044, 286, 393, 1217, 3847, 4420, 5245], "temperature": 0.0, "avg_logprob": -0.11836252333242682, "compression_ratio": 1.7032967032967032, "no_speech_prob": 6.577913040928252e-07}, {"id": 468, "seek": 222176, "start": 2221.76, "end": 2229.6400000000003, "text": " like a lot of people are surprised at how early on they can actually get astonishingly", "tokens": [411, 257, 688, 295, 561, 366, 6100, 412, 577, 2440, 322, 436, 393, 767, 483, 35264, 356], "temperature": 0.0, "avg_logprob": -0.12546422404627647, "compression_ratio": 1.7488372093023257, "no_speech_prob": 1.2029040590277873e-05}, {"id": 469, "seek": 222176, "start": 2229.6400000000003, "end": 2230.6400000000003, "text": " good results.", "tokens": [665, 3542, 13], "temperature": 0.0, "avg_logprob": -0.12546422404627647, "compression_ratio": 1.7488372093023257, "no_speech_prob": 1.2029040590277873e-05}, {"id": 470, "seek": 222176, "start": 2230.6400000000003, "end": 2236.8, "text": " Okay so you know jump in and be open to surprising yourself that you can do a bit more than you", "tokens": [1033, 370, 291, 458, 3012, 294, 293, 312, 1269, 281, 8830, 1803, 300, 291, 393, 360, 257, 857, 544, 813, 291], "temperature": 0.0, "avg_logprob": -0.12546422404627647, "compression_ratio": 1.7488372093023257, "no_speech_prob": 1.2029040590277873e-05}, {"id": 471, "seek": 222176, "start": 2236.8, "end": 2237.8, "text": " thought.", "tokens": [1194, 13], "temperature": 0.0, "avg_logprob": -0.12546422404627647, "compression_ratio": 1.7488372093023257, "no_speech_prob": 1.2029040590277873e-05}, {"id": 472, "seek": 222176, "start": 2237.8, "end": 2242.32, "text": " You can't do everything right away okay but start that feedback loop of figuring out what", "tokens": [509, 393, 380, 360, 1203, 558, 1314, 1392, 457, 722, 300, 5824, 6367, 295, 15213, 484, 437], "temperature": 0.0, "avg_logprob": -0.12546422404627647, "compression_ratio": 1.7488372093023257, "no_speech_prob": 1.2029040590277873e-05}, {"id": 473, "seek": 222176, "start": 2242.32, "end": 2250.28, "text": " do you know what can you do what can you get working what can't you get working.", "tokens": [360, 291, 458, 437, 393, 291, 360, 437, 393, 291, 483, 1364, 437, 393, 380, 291, 483, 1364, 13], "temperature": 0.0, "avg_logprob": -0.12546422404627647, "compression_ratio": 1.7488372093023257, "no_speech_prob": 1.2029040590277873e-05}, {"id": 474, "seek": 225028, "start": 2250.28, "end": 2254.28, "text": " So one of the key things that you're going to need to do if you're going to finish all", "tokens": [407, 472, 295, 264, 2141, 721, 300, 291, 434, 516, 281, 643, 281, 360, 498, 291, 434, 516, 281, 2413, 439], "temperature": 0.0, "avg_logprob": -0.08743058650865468, "compression_ratio": 1.8940677966101696, "no_speech_prob": 6.9620664362446405e-06}, {"id": 475, "seek": 225028, "start": 2254.28, "end": 2260.28, "text": " of the course is become an even better developer than you are now even better coder than you", "tokens": [295, 264, 1164, 307, 1813, 364, 754, 1101, 10754, 813, 291, 366, 586, 754, 1101, 17656, 260, 813, 291], "temperature": 0.0, "avg_logprob": -0.08743058650865468, "compression_ratio": 1.8940677966101696, "no_speech_prob": 6.9620664362446405e-06}, {"id": 476, "seek": 225028, "start": 2260.28, "end": 2268.76, "text": " are now wherever you're up to and so to do this you need to read code and write code.", "tokens": [366, 586, 8660, 291, 434, 493, 281, 293, 370, 281, 360, 341, 291, 643, 281, 1401, 3089, 293, 2464, 3089, 13], "temperature": 0.0, "avg_logprob": -0.08743058650865468, "compression_ratio": 1.8940677966101696, "no_speech_prob": 6.9620664362446405e-06}, {"id": 477, "seek": 225028, "start": 2268.76, "end": 2273.2400000000002, "text": " The fast AI source code is designed to be extremely readable so you can read that code", "tokens": [440, 2370, 7318, 4009, 3089, 307, 4761, 281, 312, 4664, 49857, 370, 291, 393, 1401, 300, 3089], "temperature": 0.0, "avg_logprob": -0.08743058650865468, "compression_ratio": 1.8940677966101696, "no_speech_prob": 6.9620664362446405e-06}, {"id": 478, "seek": 225028, "start": 2273.2400000000002, "end": 2279.38, "text": " you can obviously read the code in the notebooks but yeah you want to be spending as much time", "tokens": [291, 393, 2745, 1401, 264, 3089, 294, 264, 43782, 457, 1338, 291, 528, 281, 312, 6434, 382, 709, 565], "temperature": 0.0, "avg_logprob": -0.08743058650865468, "compression_ratio": 1.8940677966101696, "no_speech_prob": 6.9620664362446405e-06}, {"id": 479, "seek": 227938, "start": 2279.38, "end": 2288.0, "text": " as possible reading and writing code and particularly reading and writing deep learning code.", "tokens": [382, 1944, 3760, 293, 3579, 3089, 293, 4098, 3760, 293, 3579, 2452, 2539, 3089, 13], "temperature": 0.0, "avg_logprob": -0.18260729312896729, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.3496938890966703e-06}, {"id": 480, "seek": 227938, "start": 2288.0, "end": 2297.44, "text": " All right how do you find out what's going on in the world of deep learning and how do", "tokens": [1057, 558, 577, 360, 291, 915, 484, 437, 311, 516, 322, 294, 264, 1002, 295, 2452, 2539, 293, 577, 360], "temperature": 0.0, "avg_logprob": -0.18260729312896729, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.3496938890966703e-06}, {"id": 481, "seek": 227938, "start": 2297.44, "end": 2302.6, "text": " you get yourself on the map of people doing deep learning.", "tokens": [291, 483, 1803, 322, 264, 4471, 295, 561, 884, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.18260729312896729, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.3496938890966703e-06}, {"id": 482, "seek": 227938, "start": 2302.6, "end": 2305.6400000000003, "text": " Probably the best answer is Twitter.", "tokens": [9210, 264, 1151, 1867, 307, 5794, 13], "temperature": 0.0, "avg_logprob": -0.18260729312896729, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.3496938890966703e-06}, {"id": 483, "seek": 230564, "start": 2305.64, "end": 2311.7999999999997, "text": " For those of you whose only knowledge of Twitter is the Kardashians and Donald Trump this might", "tokens": [1171, 729, 295, 291, 6104, 787, 3601, 295, 5794, 307, 264, 37959, 2567, 293, 8632, 3899, 341, 1062], "temperature": 0.0, "avg_logprob": -0.09247593030537644, "compression_ratio": 1.5734597156398105, "no_speech_prob": 1.0289113561157137e-05}, {"id": 484, "seek": 230564, "start": 2311.7999999999997, "end": 2319.7999999999997, "text": " come as a surprise but actually to create this slide I opened Twitter and I copied and", "tokens": [808, 382, 257, 6365, 457, 767, 281, 1884, 341, 4137, 286, 5625, 5794, 293, 286, 25365, 293], "temperature": 0.0, "avg_logprob": -0.09247593030537644, "compression_ratio": 1.5734597156398105, "no_speech_prob": 1.0289113561157137e-05}, {"id": 485, "seek": 230564, "start": 2319.7999999999997, "end": 2323.44, "text": " pasted the first three tweets that appeared on my screen.", "tokens": [1791, 292, 264, 700, 1045, 25671, 300, 8516, 322, 452, 2568, 13], "temperature": 0.0, "avg_logprob": -0.09247593030537644, "compression_ratio": 1.5734597156398105, "no_speech_prob": 1.0289113561157137e-05}, {"id": 486, "seek": 230564, "start": 2323.44, "end": 2330.66, "text": " So one of them is somebody has a discussion about costs and impacts of different approaches", "tokens": [407, 472, 295, 552, 307, 2618, 575, 257, 5017, 466, 5497, 293, 11606, 295, 819, 11587], "temperature": 0.0, "avg_logprob": -0.09247593030537644, "compression_ratio": 1.5734597156398105, "no_speech_prob": 1.0289113561157137e-05}, {"id": 487, "seek": 233066, "start": 2330.66, "end": 2338.3999999999996, "text": " to labeling this is a fast AI alum who's a 17 year old PhD graduate who's doing well", "tokens": [281, 40244, 341, 307, 257, 2370, 7318, 12064, 567, 311, 257, 3282, 1064, 1331, 14476, 8080, 567, 311, 884, 731], "temperature": 0.0, "avg_logprob": -0.1662563241046408, "compression_ratio": 1.4594594594594594, "no_speech_prob": 4.565814379020594e-06}, {"id": 488, "seek": 233066, "start": 2338.3999999999996, "end": 2345.48, "text": " who shows how to mix PyTorch and fast AI and then Hilary Mason who's a professor I guess", "tokens": [567, 3110, 577, 281, 2890, 9953, 51, 284, 339, 293, 2370, 7318, 293, 550, 19914, 822, 25730, 567, 311, 257, 8304, 286, 2041], "temperature": 0.0, "avg_logprob": -0.1662563241046408, "compression_ratio": 1.4594594594594594, "no_speech_prob": 4.565814379020594e-06}, {"id": 489, "seek": 233066, "start": 2345.48, "end": 2352.8799999999997, "text": " not a professor anymore but now an industry talking about organizational issues in data", "tokens": [406, 257, 8304, 3602, 457, 586, 364, 3518, 1417, 466, 24730, 2663, 294, 1412], "temperature": 0.0, "avg_logprob": -0.1662563241046408, "compression_ratio": 1.4594594594594594, "no_speech_prob": 4.565814379020594e-06}, {"id": 490, "seek": 233066, "start": 2352.8799999999997, "end": 2353.8799999999997, "text": " science.", "tokens": [3497, 13], "temperature": 0.0, "avg_logprob": -0.1662563241046408, "compression_ratio": 1.4594594594594594, "no_speech_prob": 4.565814379020594e-06}, {"id": 491, "seek": 235388, "start": 2353.88, "end": 2362.1600000000003, "text": " So you know there's a whole world out there of machine learning on Twitter and there you", "tokens": [407, 291, 458, 456, 311, 257, 1379, 1002, 484, 456, 295, 3479, 2539, 322, 5794, 293, 456, 291], "temperature": 0.0, "avg_logprob": -0.13057047360903257, "compression_ratio": 1.764367816091954, "no_speech_prob": 7.766740964143537e-06}, {"id": 492, "seek": 235388, "start": 2362.1600000000003, "end": 2366.6400000000003, "text": " know if you want to get your work noticed that's a great place to do it because really", "tokens": [458, 498, 291, 528, 281, 483, 428, 589, 5694, 300, 311, 257, 869, 1081, 281, 360, 309, 570, 534], "temperature": 0.0, "avg_logprob": -0.13057047360903257, "compression_ratio": 1.764367816091954, "no_speech_prob": 7.766740964143537e-06}, {"id": 493, "seek": 235388, "start": 2366.6400000000003, "end": 2373.96, "text": " everybody everybody's there okay and if you want me to highlight your work you know that's", "tokens": [2201, 2201, 311, 456, 1392, 293, 498, 291, 528, 385, 281, 5078, 428, 589, 291, 458, 300, 311], "temperature": 0.0, "avg_logprob": -0.13057047360903257, "compression_ratio": 1.764367816091954, "no_speech_prob": 7.766740964143537e-06}, {"id": 494, "seek": 235388, "start": 2373.96, "end": 2376.8, "text": " where I can see it and I can retweet it.", "tokens": [689, 286, 393, 536, 309, 293, 286, 393, 1533, 10354, 309, 13], "temperature": 0.0, "avg_logprob": -0.13057047360903257, "compression_ratio": 1.764367816091954, "no_speech_prob": 7.766740964143537e-06}, {"id": 495, "seek": 237680, "start": 2376.8, "end": 2385.1600000000003, "text": " So yeah Twitter is a really good place to be if you're just starting with Twitter and", "tokens": [407, 1338, 5794, 307, 257, 534, 665, 1081, 281, 312, 498, 291, 434, 445, 2891, 365, 5794, 293], "temperature": 0.0, "avg_logprob": -0.07850526901612799, "compression_ratio": 1.7969543147208122, "no_speech_prob": 1.5206537682388443e-05}, {"id": 496, "seek": 237680, "start": 2385.1600000000003, "end": 2392.44, "text": " you don't know who to follow go to my Twitter go to my likes and go through my likes and", "tokens": [291, 500, 380, 458, 567, 281, 1524, 352, 281, 452, 5794, 352, 281, 452, 5902, 293, 352, 807, 452, 5902, 293], "temperature": 0.0, "avg_logprob": -0.07850526901612799, "compression_ratio": 1.7969543147208122, "no_speech_prob": 1.5206537682388443e-05}, {"id": 497, "seek": 237680, "start": 2392.44, "end": 2397.92, "text": " find tweets that you think you actually like that tweet to and then follow the person who", "tokens": [915, 25671, 300, 291, 519, 291, 767, 411, 300, 15258, 281, 293, 550, 1524, 264, 954, 567], "temperature": 0.0, "avg_logprob": -0.07850526901612799, "compression_ratio": 1.7969543147208122, "no_speech_prob": 1.5206537682388443e-05}, {"id": 498, "seek": 237680, "start": 2397.92, "end": 2403.4, "text": " did that tweet okay and pretty quickly you'll have a hundred people you're following okay", "tokens": [630, 300, 15258, 1392, 293, 1238, 2661, 291, 603, 362, 257, 3262, 561, 291, 434, 3480, 1392], "temperature": 0.0, "avg_logprob": -0.07850526901612799, "compression_ratio": 1.7969543147208122, "no_speech_prob": 1.5206537682388443e-05}, {"id": 499, "seek": 240340, "start": 2403.4, "end": 2407.86, "text": " and then they'll retweet things and you'll find other people you like and before you", "tokens": [293, 550, 436, 603, 1533, 10354, 721, 293, 291, 603, 915, 661, 561, 291, 411, 293, 949, 291], "temperature": 0.0, "avg_logprob": -0.08022122723715645, "compression_ratio": 1.8688524590163935, "no_speech_prob": 1.0129540896741673e-05}, {"id": 500, "seek": 240340, "start": 2407.86, "end": 2412.6800000000003, "text": " know it hopefully you've got a nice big lot of interesting deep learning stuff to read", "tokens": [458, 309, 4696, 291, 600, 658, 257, 1481, 955, 688, 295, 1880, 2452, 2539, 1507, 281, 1401], "temperature": 0.0, "avg_logprob": -0.08022122723715645, "compression_ratio": 1.8688524590163935, "no_speech_prob": 1.0129540896741673e-05}, {"id": 501, "seek": 240340, "start": 2412.6800000000003, "end": 2413.88, "text": " every day.", "tokens": [633, 786, 13], "temperature": 0.0, "avg_logprob": -0.08022122723715645, "compression_ratio": 1.8688524590163935, "no_speech_prob": 1.0129540896741673e-05}, {"id": 502, "seek": 240340, "start": 2413.88, "end": 2419.76, "text": " At first you'll understand like 1% of it which is fine but you know you're there you're in", "tokens": [1711, 700, 291, 603, 1223, 411, 502, 4, 295, 309, 597, 307, 2489, 457, 291, 458, 291, 434, 456, 291, 434, 294], "temperature": 0.0, "avg_logprob": -0.08022122723715645, "compression_ratio": 1.8688524590163935, "no_speech_prob": 1.0129540896741673e-05}, {"id": 503, "seek": 240340, "start": 2419.76, "end": 2425.84, "text": " it and it'll be all washing over you and you'll start to find the people who write stuff you", "tokens": [309, 293, 309, 603, 312, 439, 13836, 670, 291, 293, 291, 603, 722, 281, 915, 264, 561, 567, 2464, 1507, 291], "temperature": 0.0, "avg_logprob": -0.08022122723715645, "compression_ratio": 1.8688524590163935, "no_speech_prob": 1.0129540896741673e-05}, {"id": 504, "seek": 240340, "start": 2425.84, "end": 2430.32, "text": " find engaging and interesting and you'll also find the people that actually you don't and", "tokens": [915, 11268, 293, 1880, 293, 291, 603, 611, 915, 264, 561, 300, 767, 291, 500, 380, 293], "temperature": 0.0, "avg_logprob": -0.08022122723715645, "compression_ratio": 1.8688524590163935, "no_speech_prob": 1.0129540896741673e-05}, {"id": 505, "seek": 243032, "start": 2430.32, "end": 2439.1200000000003, "text": " make sure you unfollow them so that you don't have your feed have stuff you don't care about.", "tokens": [652, 988, 291, 3971, 49082, 552, 370, 300, 291, 500, 380, 362, 428, 3154, 362, 1507, 291, 500, 380, 1127, 466, 13], "temperature": 0.0, "avg_logprob": -0.06366563397784565, "compression_ratio": 1.811881188118812, "no_speech_prob": 5.338091796147637e-06}, {"id": 506, "seek": 243032, "start": 2439.1200000000003, "end": 2447.2000000000003, "text": " So then beyond Twitter you want to start blogging okay and again blogging is not about writing", "tokens": [407, 550, 4399, 5794, 291, 528, 281, 722, 6968, 3249, 1392, 293, 797, 6968, 3249, 307, 406, 466, 3579], "temperature": 0.0, "avg_logprob": -0.06366563397784565, "compression_ratio": 1.811881188118812, "no_speech_prob": 5.338091796147637e-06}, {"id": 507, "seek": 243032, "start": 2447.2000000000003, "end": 2454.2000000000003, "text": " what you had for dinner okay it's about writing something that you of six months ago would", "tokens": [437, 291, 632, 337, 6148, 1392, 309, 311, 466, 3579, 746, 300, 291, 295, 2309, 2493, 2057, 576], "temperature": 0.0, "avg_logprob": -0.06366563397784565, "compression_ratio": 1.811881188118812, "no_speech_prob": 5.338091796147637e-06}, {"id": 508, "seek": 243032, "start": 2454.2000000000003, "end": 2459.48, "text": " have found interesting okay so you know more than you did six months ago so write that", "tokens": [362, 1352, 1880, 1392, 370, 291, 458, 544, 813, 291, 630, 2309, 2493, 2057, 370, 2464, 300], "temperature": 0.0, "avg_logprob": -0.06366563397784565, "compression_ratio": 1.811881188118812, "no_speech_prob": 5.338091796147637e-06}, {"id": 509, "seek": 245948, "start": 2459.48, "end": 2461.92, "text": " down.", "tokens": [760, 13], "temperature": 0.0, "avg_logprob": -0.19859348024640763, "compression_ratio": 1.5495049504950495, "no_speech_prob": 4.092868039151654e-06}, {"id": 510, "seek": 245948, "start": 2461.92, "end": 2467.72, "text": " We have something called FastPages that makes it ridiculously easy to start a blog and so", "tokens": [492, 362, 746, 1219, 15968, 47, 1660, 300, 1669, 309, 41358, 1858, 281, 722, 257, 6968, 293, 370], "temperature": 0.0, "avg_logprob": -0.19859348024640763, "compression_ratio": 1.5495049504950495, "no_speech_prob": 4.092868039151654e-06}, {"id": 511, "seek": 245948, "start": 2467.72, "end": 2475.32, "text": " there's no reason for you not to you know at least create a blog.", "tokens": [456, 311, 572, 1778, 337, 291, 406, 281, 291, 458, 412, 1935, 1884, 257, 6968, 13], "temperature": 0.0, "avg_logprob": -0.19859348024640763, "compression_ratio": 1.5495049504950495, "no_speech_prob": 4.092868039151654e-06}, {"id": 512, "seek": 245948, "start": 2475.32, "end": 2482.32, "text": " There we go and one of the nice things about FastPages is you can even turn Jupyter Notebooks", "tokens": [821, 321, 352, 293, 472, 295, 264, 1481, 721, 466, 15968, 47, 1660, 307, 291, 393, 754, 1261, 22125, 88, 391, 11633, 15170], "temperature": 0.0, "avg_logprob": -0.19859348024640763, "compression_ratio": 1.5495049504950495, "no_speech_prob": 4.092868039151654e-06}, {"id": 513, "seek": 245948, "start": 2482.32, "end": 2485.48, "text": " into blog posts so it's great for kind of technical ones.", "tokens": [666, 6968, 12300, 370, 309, 311, 869, 337, 733, 295, 6191, 2306, 13], "temperature": 0.0, "avg_logprob": -0.19859348024640763, "compression_ratio": 1.5495049504950495, "no_speech_prob": 4.092868039151654e-06}, {"id": 514, "seek": 248548, "start": 2485.48, "end": 2490.6, "text": " So this is what a FastPages blog looks like this is a FastPages blog about FastPages I", "tokens": [407, 341, 307, 437, 257, 15968, 47, 1660, 6968, 1542, 411, 341, 307, 257, 15968, 47, 1660, 6968, 466, 15968, 47, 1660, 286], "temperature": 0.0, "avg_logprob": -0.13052880468447348, "compression_ratio": 2.0361990950226243, "no_speech_prob": 4.5659126044483855e-06}, {"id": 515, "seek": 248548, "start": 2490.6, "end": 2496.76, "text": " had to write FastPages in order to write the FastPages blog about FastPages but basically", "tokens": [632, 281, 2464, 15968, 47, 1660, 294, 1668, 281, 2464, 264, 15968, 47, 1660, 6968, 466, 15968, 47, 1660, 457, 1936], "temperature": 0.0, "avg_logprob": -0.13052880468447348, "compression_ratio": 2.0361990950226243, "no_speech_prob": 4.5659126044483855e-06}, {"id": 516, "seek": 248548, "start": 2496.76, "end": 2501.36, "text": " it and one of the other nice things it's all in it's all in github right so it's as you're", "tokens": [309, 293, 472, 295, 264, 661, 1481, 721, 309, 311, 439, 294, 309, 311, 439, 294, 290, 355, 836, 558, 370, 309, 311, 382, 291, 434], "temperature": 0.0, "avg_logprob": -0.13052880468447348, "compression_ratio": 2.0361990950226243, "no_speech_prob": 4.5659126044483855e-06}, {"id": 517, "seek": 248548, "start": 2501.36, "end": 2506.08, "text": " blogging you're learning more about gip it's all written with markdown which is something", "tokens": [6968, 3249, 291, 434, 2539, 544, 466, 290, 647, 309, 311, 439, 3720, 365, 1491, 5093, 597, 307, 746], "temperature": 0.0, "avg_logprob": -0.13052880468447348, "compression_ratio": 2.0361990950226243, "no_speech_prob": 4.5659126044483855e-06}, {"id": 518, "seek": 248548, "start": 2506.08, "end": 2510.12, "text": " that you're definitely going to need to know anyway so as you're blogging you'll be learning", "tokens": [300, 291, 434, 2138, 516, 281, 643, 281, 458, 4033, 370, 382, 291, 434, 6968, 3249, 291, 603, 312, 2539], "temperature": 0.0, "avg_logprob": -0.13052880468447348, "compression_ratio": 2.0361990950226243, "no_speech_prob": 4.5659126044483855e-06}, {"id": 519, "seek": 251012, "start": 2510.12, "end": 2524.6, "text": " about a lot of the tools you need to learn about anyway.", "tokens": [466, 257, 688, 295, 264, 3873, 291, 643, 281, 1466, 466, 4033, 13], "temperature": 0.0, "avg_logprob": -0.11534430550747231, "compression_ratio": 1.4774193548387098, "no_speech_prob": 5.862505986442557e-06}, {"id": 520, "seek": 251012, "start": 2524.6, "end": 2531.3199999999997, "text": " So one interesting idea for things to blog about is this example from Amano Arora who", "tokens": [407, 472, 1880, 1558, 337, 721, 281, 6968, 466, 307, 341, 1365, 490, 35466, 78, 1587, 3252, 567], "temperature": 0.0, "avg_logprob": -0.11534430550747231, "compression_ratio": 1.4774193548387098, "no_speech_prob": 5.862505986442557e-06}, {"id": 521, "seek": 251012, "start": 2531.3199999999997, "end": 2536.92, "text": " is an Aussie fast AI alum who is now working at Weights and Biases which is one of the", "tokens": [307, 364, 21286, 414, 2370, 7318, 12064, 567, 307, 586, 1364, 412, 492, 5761, 293, 13007, 1957, 597, 307, 472, 295, 264], "temperature": 0.0, "avg_logprob": -0.11534430550747231, "compression_ratio": 1.4774193548387098, "no_speech_prob": 5.862505986442557e-06}, {"id": 522, "seek": 253692, "start": 2536.92, "end": 2542.8, "text": " top AI startups in the world.", "tokens": [1192, 7318, 28041, 294, 264, 1002, 13], "temperature": 0.0, "avg_logprob": -0.08377125456526473, "compression_ratio": 1.5392670157068062, "no_speech_prob": 2.7530892111826688e-05}, {"id": 523, "seek": 253692, "start": 2542.8, "end": 2547.6800000000003, "text": " This is a really interesting kind of blog post what Amano did was he took a video that", "tokens": [639, 307, 257, 534, 1880, 733, 295, 6968, 2183, 437, 35466, 78, 630, 390, 415, 1890, 257, 960, 300], "temperature": 0.0, "avg_logprob": -0.08377125456526473, "compression_ratio": 1.5392670157068062, "no_speech_prob": 2.7530892111826688e-05}, {"id": 524, "seek": 253692, "start": 2547.6800000000003, "end": 2557.28, "text": " I did at the launch here of the Queensland AI Hub and he wrote down what I said and that's", "tokens": [286, 630, 412, 264, 4025, 510, 295, 264, 36913, 7318, 18986, 293, 415, 4114, 760, 437, 286, 848, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.08377125456526473, "compression_ratio": 1.5392670157068062, "no_speech_prob": 2.7530892111826688e-05}, {"id": 525, "seek": 253692, "start": 2557.28, "end": 2563.48, "text": " an example of something that you could do if there are videos out there that you liked", "tokens": [364, 1365, 295, 746, 300, 291, 727, 360, 498, 456, 366, 2145, 484, 456, 300, 291, 4501], "temperature": 0.0, "avg_logprob": -0.08377125456526473, "compression_ratio": 1.5392670157068062, "no_speech_prob": 2.7530892111826688e-05}, {"id": 526, "seek": 256348, "start": 2563.48, "end": 2571.68, "text": " and nobody's turned it into a post be the first to do so because there's all these benefits", "tokens": [293, 5079, 311, 3574, 309, 666, 257, 2183, 312, 264, 700, 281, 360, 370, 570, 456, 311, 439, 613, 5311], "temperature": 0.0, "avg_logprob": -0.08230945923749139, "compression_ratio": 1.6543778801843319, "no_speech_prob": 6.012280937284231e-05}, {"id": 527, "seek": 256348, "start": 2571.68, "end": 2578.72, "text": " when somebody sends me something saying I've written up this talk you gave I'm very grateful", "tokens": [562, 2618, 14790, 385, 746, 1566, 286, 600, 3720, 493, 341, 751, 291, 2729, 286, 478, 588, 7941], "temperature": 0.0, "avg_logprob": -0.08230945923749139, "compression_ratio": 1.6543778801843319, "no_speech_prob": 6.012280937284231e-05}, {"id": 528, "seek": 256348, "start": 2578.72, "end": 2582.84, "text": " to that person because now my talk is now available in a second medium a lot of people", "tokens": [281, 300, 954, 570, 586, 452, 751, 307, 586, 2435, 294, 257, 1150, 6399, 257, 688, 295, 561], "temperature": 0.0, "avg_logprob": -0.08230945923749139, "compression_ratio": 1.6543778801843319, "no_speech_prob": 6.012280937284231e-05}, {"id": 529, "seek": 256348, "start": 2582.84, "end": 2588.88, "text": " prefer to read rather than listen to a talk you know that person's taken the time to do", "tokens": [4382, 281, 1401, 2831, 813, 2140, 281, 257, 751, 291, 458, 300, 954, 311, 2726, 264, 565, 281, 360], "temperature": 0.0, "avg_logprob": -0.08230945923749139, "compression_ratio": 1.6543778801843319, "no_speech_prob": 6.012280937284231e-05}, {"id": 530, "seek": 258888, "start": 2588.88, "end": 2595.58, "text": " this they've given taken the time to have me check you know their work and kind of everybody", "tokens": [341, 436, 600, 2212, 2726, 264, 565, 281, 362, 385, 1520, 291, 458, 641, 589, 293, 733, 295, 2201], "temperature": 0.0, "avg_logprob": -0.12142069773240523, "compression_ratio": 1.502092050209205, "no_speech_prob": 4.3979493057122454e-05}, {"id": 531, "seek": 258888, "start": 2595.58, "end": 2603.92, "text": " ends up winning from this so I've seen with Amano's post about my talk it's got attention", "tokens": [5314, 493, 8224, 490, 341, 370, 286, 600, 1612, 365, 35466, 78, 311, 2183, 466, 452, 751, 309, 311, 658, 3202], "temperature": 0.0, "avg_logprob": -0.12142069773240523, "compression_ratio": 1.502092050209205, "no_speech_prob": 4.3979493057122454e-05}, {"id": 532, "seek": 258888, "start": 2603.92, "end": 2611.12, "text": " from people that my talk did so for example I noticed on my LinkedIn feed the CEO of Data61", "tokens": [490, 561, 300, 452, 751, 630, 370, 337, 1365, 286, 5694, 322, 452, 20657, 3154, 264, 9282, 295, 11888, 31537], "temperature": 0.0, "avg_logprob": -0.12142069773240523, "compression_ratio": 1.502092050209205, "no_speech_prob": 4.3979493057122454e-05}, {"id": 533, "seek": 258888, "start": 2611.12, "end": 2618.1400000000003, "text": " which is the CSIRO so the top data science body in Australia highlighted it and said", "tokens": [597, 307, 264, 9460, 40, 7142, 370, 264, 1192, 1412, 3497, 1772, 294, 7060, 17173, 309, 293, 848], "temperature": 0.0, "avg_logprob": -0.12142069773240523, "compression_ratio": 1.502092050209205, "no_speech_prob": 4.3979493057122454e-05}, {"id": 534, "seek": 261814, "start": 2618.14, "end": 2624.44, "text": " check out this post from Amano Moro right like so this is like an example of the kind", "tokens": [1520, 484, 341, 2183, 490, 35466, 78, 5146, 78, 558, 411, 370, 341, 307, 411, 364, 1365, 295, 264, 733], "temperature": 0.0, "avg_logprob": -0.16913469906510978, "compression_ratio": 1.7342995169082125, "no_speech_prob": 1.9831108147627674e-05}, {"id": 535, "seek": 261814, "start": 2624.44, "end": 2631.66, "text": " of stuff you can do is like try to be helpful right and at the same time you're also learning", "tokens": [295, 1507, 291, 393, 360, 307, 411, 853, 281, 312, 4961, 558, 293, 412, 264, 912, 565, 291, 434, 611, 2539], "temperature": 0.0, "avg_logprob": -0.16913469906510978, "compression_ratio": 1.7342995169082125, "no_speech_prob": 1.9831108147627674e-05}, {"id": 536, "seek": 261814, "start": 2631.66, "end": 2638.06, "text": " so there's an example of an interesting kind of blog post which very few people are writing", "tokens": [370, 456, 311, 364, 1365, 295, 364, 1880, 733, 295, 6968, 2183, 597, 588, 1326, 561, 366, 3579], "temperature": 0.0, "avg_logprob": -0.16913469906510978, "compression_ratio": 1.7342995169082125, "no_speech_prob": 1.9831108147627674e-05}, {"id": 537, "seek": 261814, "start": 2638.06, "end": 2647.04, "text": " and so there's a huge amount of opportunity here for you to practice your your writing.", "tokens": [293, 370, 456, 311, 257, 2603, 2372, 295, 2650, 510, 337, 291, 281, 3124, 428, 428, 3579, 13], "temperature": 0.0, "avg_logprob": -0.16913469906510978, "compression_ratio": 1.7342995169082125, "no_speech_prob": 1.9831108147627674e-05}, {"id": 538, "seek": 264704, "start": 2647.04, "end": 2656.22, "text": " Okay now what is the difference between machine learning and other kinds of coding that's", "tokens": [1033, 586, 437, 307, 264, 2649, 1296, 3479, 2539, 293, 661, 3685, 295, 17720, 300, 311], "temperature": 0.0, "avg_logprob": -0.13735899849543495, "compression_ratio": 1.5963855421686748, "no_speech_prob": 1.1297845048829913e-05}, {"id": 539, "seek": 264704, "start": 2656.22, "end": 2662.8, "text": " what Dex says in in this chapter of his book the key about machine learning is that we", "tokens": [437, 1346, 87, 1619, 294, 294, 341, 7187, 295, 702, 1446, 264, 2141, 466, 3479, 2539, 307, 300, 321], "temperature": 0.0, "avg_logprob": -0.13735899849543495, "compression_ratio": 1.5963855421686748, "no_speech_prob": 1.1297845048829913e-05}, {"id": 540, "seek": 264704, "start": 2662.8, "end": 2671.5, "text": " can generalize we can train a model with one set of data and apply it to a different set", "tokens": [393, 2674, 1125, 321, 393, 3847, 257, 2316, 365, 472, 992, 295, 1412, 293, 3079, 309, 281, 257, 819, 992], "temperature": 0.0, "avg_logprob": -0.13735899849543495, "compression_ratio": 1.5963855421686748, "no_speech_prob": 1.1297845048829913e-05}, {"id": 541, "seek": 267150, "start": 2671.5, "end": 2679.28, "text": " of data and still get good results and everything just about that we're doing in this course", "tokens": [295, 1412, 293, 920, 483, 665, 3542, 293, 1203, 445, 466, 300, 321, 434, 884, 294, 341, 1164], "temperature": 0.0, "avg_logprob": -0.05623482876136655, "compression_ratio": 1.690909090909091, "no_speech_prob": 3.1380598102259682e-06}, {"id": 542, "seek": 267150, "start": 2679.28, "end": 2685.88, "text": " is all about creating models that are going to generalize well and we're going to be learning", "tokens": [307, 439, 466, 4084, 5245, 300, 366, 516, 281, 2674, 1125, 731, 293, 321, 434, 516, 281, 312, 2539], "temperature": 0.0, "avg_logprob": -0.05623482876136655, "compression_ratio": 1.690909090909091, "no_speech_prob": 3.1380598102259682e-06}, {"id": 543, "seek": 267150, "start": 2685.88, "end": 2696.94, "text": " about how you can measure how well your model generalizes so answering these questions about", "tokens": [466, 577, 291, 393, 3481, 577, 731, 428, 2316, 2674, 5660, 370, 13430, 613, 1651, 466], "temperature": 0.0, "avg_logprob": -0.05623482876136655, "compression_ratio": 1.690909090909091, "no_speech_prob": 3.1380598102259682e-06}, {"id": 544, "seek": 269694, "start": 2696.94, "end": 2704.02, "text": " can we trust our model to be correct on new data that we feed it is absolutely critical", "tokens": [393, 321, 3361, 527, 2316, 281, 312, 3006, 322, 777, 1412, 300, 321, 3154, 309, 307, 3122, 4924], "temperature": 0.0, "avg_logprob": -0.07725672464112977, "compression_ratio": 1.605, "no_speech_prob": 2.4824009869917063e-06}, {"id": 545, "seek": 269694, "start": 2704.02, "end": 2711.42, "text": " to to every model that you build whether it be in a Kaggle competition or a little prototype", "tokens": [281, 281, 633, 2316, 300, 291, 1322, 1968, 309, 312, 294, 257, 48751, 22631, 6211, 420, 257, 707, 19475], "temperature": 0.0, "avg_logprob": -0.07725672464112977, "compression_ratio": 1.605, "no_speech_prob": 2.4824009869917063e-06}, {"id": 546, "seek": 269694, "start": 2711.42, "end": 2718.3, "text": " or a production model you're creating at work.", "tokens": [420, 257, 4265, 2316, 291, 434, 4084, 412, 589, 13], "temperature": 0.0, "avg_logprob": -0.07725672464112977, "compression_ratio": 1.605, "no_speech_prob": 2.4824009869917063e-06}, {"id": 547, "seek": 269694, "start": 2718.3, "end": 2722.4, "text": " One of the most important things here is creating a good validation set and this is something", "tokens": [1485, 295, 264, 881, 1021, 721, 510, 307, 4084, 257, 665, 24071, 992, 293, 341, 307, 746], "temperature": 0.0, "avg_logprob": -0.07725672464112977, "compression_ratio": 1.605, "no_speech_prob": 2.4824009869917063e-06}, {"id": 548, "seek": 272240, "start": 2722.4, "end": 2727.46, "text": " that you're you're here about in lesson one of the course but you know I really wanted", "tokens": [300, 291, 434, 291, 434, 510, 466, 294, 6898, 472, 295, 264, 1164, 457, 291, 458, 286, 534, 1415], "temperature": 0.0, "avg_logprob": -0.0964494835246693, "compression_ratio": 1.7184466019417475, "no_speech_prob": 1.7330258970105206e-06}, {"id": 549, "seek": 272240, "start": 2727.46, "end": 2734.26, "text": " to highlight it here as did Rudek in his book it's it's a really important idea is you need", "tokens": [281, 5078, 309, 510, 382, 630, 18636, 916, 294, 702, 1446, 309, 311, 309, 311, 257, 534, 1021, 1558, 307, 291, 643], "temperature": 0.0, "avg_logprob": -0.0964494835246693, "compression_ratio": 1.7184466019417475, "no_speech_prob": 1.7330258970105206e-06}, {"id": 550, "seek": 272240, "start": 2734.26, "end": 2740.02, "text": " a good way to measure whether your model is any good so you need a data set that really", "tokens": [257, 665, 636, 281, 3481, 1968, 428, 2316, 307, 604, 665, 370, 291, 643, 257, 1412, 992, 300, 534], "temperature": 0.0, "avg_logprob": -0.0964494835246693, "compression_ratio": 1.7184466019417475, "no_speech_prob": 1.7330258970105206e-06}, {"id": 551, "seek": 272240, "start": 2740.02, "end": 2750.1800000000003, "text": " represents what kind of data is your model likely to have to deal with in real life and", "tokens": [8855, 437, 733, 295, 1412, 307, 428, 2316, 3700, 281, 362, 281, 2028, 365, 294, 957, 993, 293], "temperature": 0.0, "avg_logprob": -0.0964494835246693, "compression_ratio": 1.7184466019417475, "no_speech_prob": 1.7330258970105206e-06}, {"id": 552, "seek": 275018, "start": 2750.18, "end": 2755.98, "text": " my partner Rachel wrote this really great blog post on the Fast.ai blog about this actually", "tokens": [452, 4975, 14246, 4114, 341, 534, 869, 6968, 2183, 322, 264, 15968, 13, 1301, 6968, 466, 341, 767], "temperature": 0.0, "avg_logprob": -0.0999221220249083, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.3419123206404038e-05}, {"id": 553, "seek": 275018, "start": 2755.98, "end": 2762.62, "text": " interestingly you know this was kind of came out of a lesson that I did at the University", "tokens": [25873, 291, 458, 341, 390, 733, 295, 1361, 484, 295, 257, 6898, 300, 286, 630, 412, 264, 3535], "temperature": 0.0, "avg_logprob": -0.0999221220249083, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.3419123206404038e-05}, {"id": 554, "seek": 275018, "start": 2762.62, "end": 2767.3799999999997, "text": " of San Francisco and then Rachel turned it into a blog post and Rachel's blog post has", "tokens": [295, 5271, 12279, 293, 550, 14246, 3574, 309, 666, 257, 6968, 2183, 293, 14246, 311, 6968, 2183, 575], "temperature": 0.0, "avg_logprob": -0.0999221220249083, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.3419123206404038e-05}, {"id": 555, "seek": 275018, "start": 2767.3799999999997, "end": 2771.94, "text": " ended up much more influential than my video ever was you know so this is actually a good", "tokens": [4590, 493, 709, 544, 22215, 813, 452, 960, 1562, 390, 291, 458, 370, 341, 307, 767, 257, 665], "temperature": 0.0, "avg_logprob": -0.0999221220249083, "compression_ratio": 1.6886792452830188, "no_speech_prob": 1.3419123206404038e-05}, {"id": 556, "seek": 277194, "start": 2771.94, "end": 2781.66, "text": " example of what I was talking about and she took it a lot further.", "tokens": [1365, 295, 437, 286, 390, 1417, 466, 293, 750, 1890, 309, 257, 688, 3052, 13], "temperature": 0.0, "avg_logprob": -0.09694112048429601, "compression_ratio": 1.55, "no_speech_prob": 5.338037681212882e-06}, {"id": 557, "seek": 277194, "start": 2781.66, "end": 2789.38, "text": " Okay the next key thing that Rudek mentions and I totally agree with is it's hard to write", "tokens": [1033, 264, 958, 2141, 551, 300, 18636, 916, 23844, 293, 286, 3879, 3986, 365, 307, 309, 311, 1152, 281, 2464], "temperature": 0.0, "avg_logprob": -0.09694112048429601, "compression_ratio": 1.55, "no_speech_prob": 5.338037681212882e-06}, {"id": 558, "seek": 277194, "start": 2789.38, "end": 2792.94, "text": " correct machine learning code.", "tokens": [3006, 3479, 2539, 3089, 13], "temperature": 0.0, "avg_logprob": -0.09694112048429601, "compression_ratio": 1.55, "no_speech_prob": 5.338037681212882e-06}, {"id": 559, "seek": 277194, "start": 2792.94, "end": 2799.54, "text": " I always assume that every line of machine learning code I write is wrong and I'm normally", "tokens": [286, 1009, 6552, 300, 633, 1622, 295, 3479, 2539, 3089, 286, 2464, 307, 2085, 293, 286, 478, 5646], "temperature": 0.0, "avg_logprob": -0.09694112048429601, "compression_ratio": 1.55, "no_speech_prob": 5.338037681212882e-06}, {"id": 560, "seek": 279954, "start": 2799.54, "end": 2808.5, "text": " correct about that it normally is wrong because there's lots of ways to be wrong and unlike", "tokens": [3006, 466, 300, 309, 5646, 307, 2085, 570, 456, 311, 3195, 295, 2098, 281, 312, 2085, 293, 8343], "temperature": 0.0, "avg_logprob": -0.08965436617533366, "compression_ratio": 1.8471074380165289, "no_speech_prob": 1.8342252587899566e-05}, {"id": 561, "seek": 279954, "start": 2808.5, "end": 2814.02, "text": " creating a you know a context management app on the web whatever it's much harder to see", "tokens": [4084, 257, 291, 458, 257, 4319, 4592, 724, 322, 264, 3670, 2035, 309, 311, 709, 6081, 281, 536], "temperature": 0.0, "avg_logprob": -0.08965436617533366, "compression_ratio": 1.8471074380165289, "no_speech_prob": 1.8342252587899566e-05}, {"id": 562, "seek": 279954, "start": 2814.02, "end": 2818.3, "text": " that you're wrong you know you can't see that the name didn't get stored in the database", "tokens": [300, 291, 434, 2085, 291, 458, 291, 393, 380, 536, 300, 264, 1315, 994, 380, 483, 12187, 294, 264, 8149], "temperature": 0.0, "avg_logprob": -0.08965436617533366, "compression_ratio": 1.8471074380165289, "no_speech_prob": 1.8342252587899566e-05}, {"id": 563, "seek": 279954, "start": 2818.3, "end": 2823.7799999999997, "text": " or you can't see that the title isn't centered right often it's wrong that it's going to", "tokens": [420, 291, 393, 380, 536, 300, 264, 4876, 1943, 380, 18988, 558, 2049, 309, 311, 2085, 300, 309, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.08965436617533366, "compression_ratio": 1.8471074380165289, "no_speech_prob": 1.8342252587899566e-05}, {"id": 564, "seek": 279954, "start": 2823.7799999999997, "end": 2828.62, "text": " be like half a percent less accurate you know or your image is upside down but it's kind", "tokens": [312, 411, 1922, 257, 3043, 1570, 8559, 291, 458, 420, 428, 3256, 307, 14119, 760, 457, 309, 311, 733], "temperature": 0.0, "avg_logprob": -0.08965436617533366, "compression_ratio": 1.8471074380165289, "no_speech_prob": 1.8342252587899566e-05}, {"id": 565, "seek": 282862, "start": 2828.62, "end": 2832.5, "text": " of maybe you didn't even look at it I got straight into sent into the system and you", "tokens": [295, 1310, 291, 994, 380, 754, 574, 412, 309, 286, 658, 2997, 666, 2279, 666, 264, 1185, 293, 291], "temperature": 0.0, "avg_logprob": -0.1142789690118087, "compression_ratio": 1.642512077294686, "no_speech_prob": 8.139341844071168e-06}, {"id": 566, "seek": 282862, "start": 2832.5, "end": 2838.06, "text": " end up with something that can only recognize upside down images or whatever.", "tokens": [917, 493, 365, 746, 300, 393, 787, 5521, 14119, 760, 5267, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.1142789690118087, "compression_ratio": 1.642512077294686, "no_speech_prob": 8.139341844071168e-06}, {"id": 567, "seek": 282862, "start": 2838.06, "end": 2844.54, "text": " So whenever you're doing you know whenever you're building a project make sure you start", "tokens": [407, 5699, 291, 434, 884, 291, 458, 5699, 291, 434, 2390, 257, 1716, 652, 988, 291, 722], "temperature": 0.0, "avg_logprob": -0.1142789690118087, "compression_ratio": 1.642512077294686, "no_speech_prob": 8.139341844071168e-06}, {"id": 568, "seek": 282862, "start": 2844.54, "end": 2852.62, "text": " with a simple baseline right like create the simplest possible model you can that's that", "tokens": [365, 257, 2199, 20518, 558, 411, 1884, 264, 22811, 1944, 2316, 291, 393, 300, 311, 300], "temperature": 0.0, "avg_logprob": -0.1142789690118087, "compression_ratio": 1.642512077294686, "no_speech_prob": 8.139341844071168e-06}, {"id": 569, "seek": 285262, "start": 2852.62, "end": 2858.94, "text": " you know solves the problem so simply that you can't have made a mistake so often that'll", "tokens": [291, 458, 39890, 264, 1154, 370, 2935, 300, 291, 393, 380, 362, 1027, 257, 6146, 370, 2049, 300, 603], "temperature": 0.0, "avg_logprob": -0.08109079466925727, "compression_ratio": 1.8153153153153154, "no_speech_prob": 3.5559467050916282e-06}, {"id": 570, "seek": 285262, "start": 2858.94, "end": 2863.48, "text": " be like just taking the average of the data or if there's two groups take the average", "tokens": [312, 411, 445, 1940, 264, 4274, 295, 264, 1412, 420, 498, 456, 311, 732, 3935, 747, 264, 4274], "temperature": 0.0, "avg_logprob": -0.08109079466925727, "compression_ratio": 1.8153153153153154, "no_speech_prob": 3.5559467050916282e-06}, {"id": 571, "seek": 285262, "start": 2863.48, "end": 2868.9, "text": " of each of the two groups or you know something that's something really really simple and", "tokens": [295, 1184, 295, 264, 732, 3935, 420, 291, 458, 746, 300, 311, 746, 534, 534, 2199, 293], "temperature": 0.0, "avg_logprob": -0.08109079466925727, "compression_ratio": 1.8153153153153154, "no_speech_prob": 3.5559467050916282e-06}, {"id": 572, "seek": 285262, "start": 2868.9, "end": 2872.14, "text": " then you can gradually build up from there.", "tokens": [550, 291, 393, 13145, 1322, 493, 490, 456, 13], "temperature": 0.0, "avg_logprob": -0.08109079466925727, "compression_ratio": 1.8153153153153154, "no_speech_prob": 3.5559467050916282e-06}, {"id": 573, "seek": 285262, "start": 2872.14, "end": 2877.8199999999997, "text": " So another very common beginner mistake with projects remember we want you all doing projects", "tokens": [407, 1071, 588, 2689, 22080, 6146, 365, 4455, 1604, 321, 528, 291, 439, 884, 4455], "temperature": 0.0, "avg_logprob": -0.08109079466925727, "compression_ratio": 1.8153153153153154, "no_speech_prob": 3.5559467050916282e-06}, {"id": 574, "seek": 287782, "start": 2877.82, "end": 2885.1000000000004, "text": " is somebody in a project group will say oh I read about this new Bayesian learning thing", "tokens": [307, 2618, 294, 257, 1716, 1594, 486, 584, 1954, 286, 1401, 466, 341, 777, 7840, 42434, 2539, 551], "temperature": 0.0, "avg_logprob": -0.14272934053002334, "compression_ratio": 1.6009174311926606, "no_speech_prob": 4.092806648259284e-06}, {"id": 575, "seek": 287782, "start": 2885.1000000000004, "end": 2890.86, "text": " with these clusters and this you know advanced transformers pipeline and we could put all", "tokens": [365, 613, 23313, 293, 341, 291, 458, 7339, 4088, 433, 15517, 293, 321, 727, 829, 439], "temperature": 0.0, "avg_logprob": -0.14272934053002334, "compression_ratio": 1.6009174311926606, "no_speech_prob": 4.092806648259284e-06}, {"id": 576, "seek": 287782, "start": 2890.86, "end": 2896.84, "text": " that together it's going to be better than anything before and they then spend months", "tokens": [300, 1214, 309, 311, 516, 281, 312, 1101, 813, 1340, 949, 293, 436, 550, 3496, 2493], "temperature": 0.0, "avg_logprob": -0.14272934053002334, "compression_ratio": 1.6009174311926606, "no_speech_prob": 4.092806648259284e-06}, {"id": 577, "seek": 287782, "start": 2896.84, "end": 2902.3, "text": " creating this complex thing and at the end it doesn't work.", "tokens": [4084, 341, 3997, 551, 293, 412, 264, 917, 309, 1177, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.14272934053002334, "compression_ratio": 1.6009174311926606, "no_speech_prob": 4.092806648259284e-06}, {"id": 578, "seek": 287782, "start": 2902.3, "end": 2903.5800000000004, "text": " Now why doesn't it work?", "tokens": [823, 983, 1177, 380, 309, 589, 30], "temperature": 0.0, "avg_logprob": -0.14272934053002334, "compression_ratio": 1.6009174311926606, "no_speech_prob": 4.092806648259284e-06}, {"id": 579, "seek": 290358, "start": 2903.58, "end": 2908.7799999999997, "text": " Well I don't know it's so big and so complicated maybe it was a stupid idea maybe there's a", "tokens": [1042, 286, 500, 380, 458, 309, 311, 370, 955, 293, 370, 6179, 1310, 309, 390, 257, 6631, 1558, 1310, 456, 311, 257], "temperature": 0.0, "avg_logprob": -0.1332568785723518, "compression_ratio": 1.7971014492753623, "no_speech_prob": 2.601574124128092e-06}, {"id": 580, "seek": 290358, "start": 2908.7799999999997, "end": 2912.46, "text": " bug in one piece of it maybe that one piece there shouldn't be there but it should be", "tokens": [7426, 294, 472, 2522, 295, 309, 1310, 300, 472, 2522, 456, 4659, 380, 312, 456, 457, 309, 820, 312], "temperature": 0.0, "avg_logprob": -0.1332568785723518, "compression_ratio": 1.7971014492753623, "no_speech_prob": 2.601574124128092e-06}, {"id": 581, "seek": 290358, "start": 2912.46, "end": 2913.94, "text": " somewhere else.", "tokens": [4079, 1646, 13], "temperature": 0.0, "avg_logprob": -0.1332568785723518, "compression_ratio": 1.7971014492753623, "no_speech_prob": 2.601574124128092e-06}, {"id": 582, "seek": 290358, "start": 2913.94, "end": 2920.46, "text": " I don't know right that's not how anybody creates successful machine learning projects.", "tokens": [286, 500, 380, 458, 558, 300, 311, 406, 577, 4472, 7829, 4406, 3479, 2539, 4455, 13], "temperature": 0.0, "avg_logprob": -0.1332568785723518, "compression_ratio": 1.7971014492753623, "no_speech_prob": 2.601574124128092e-06}, {"id": 583, "seek": 290358, "start": 2920.46, "end": 2927.22, "text": " Machine successful machine learning projects are always built in my experience by creating", "tokens": [22155, 4406, 3479, 2539, 4455, 366, 1009, 3094, 294, 452, 1752, 538, 4084], "temperature": 0.0, "avg_logprob": -0.1332568785723518, "compression_ratio": 1.7971014492753623, "no_speech_prob": 2.601574124128092e-06}, {"id": 584, "seek": 292722, "start": 2927.22, "end": 2934.06, "text": " a simplest possible solution that gets something all the way from end to end first and then", "tokens": [257, 22811, 1944, 3827, 300, 2170, 746, 439, 264, 636, 490, 917, 281, 917, 700, 293, 550], "temperature": 0.0, "avg_logprob": -0.10924461909702846, "compression_ratio": 1.7252252252252251, "no_speech_prob": 4.029403044114588e-06}, {"id": 585, "seek": 292722, "start": 2934.06, "end": 2939.74, "text": " very gradually it makes it incrementally slightly better.", "tokens": [588, 13145, 309, 1669, 309, 26200, 379, 4748, 1101, 13], "temperature": 0.0, "avg_logprob": -0.10924461909702846, "compression_ratio": 1.7252252252252251, "no_speech_prob": 4.029403044114588e-06}, {"id": 586, "seek": 292722, "start": 2939.74, "end": 2945.58, "text": " So keep that in mind right you might feel a bit silly when you build that first model", "tokens": [407, 1066, 300, 294, 1575, 558, 291, 1062, 841, 257, 857, 11774, 562, 291, 1322, 300, 700, 2316], "temperature": 0.0, "avg_logprob": -0.10924461909702846, "compression_ratio": 1.7252252252252251, "no_speech_prob": 4.029403044114588e-06}, {"id": 587, "seek": 292722, "start": 2945.58, "end": 2951.68, "text": " that just takes the average of the data right but that's how that's how the pros do it.", "tokens": [300, 445, 2516, 264, 4274, 295, 264, 1412, 558, 457, 300, 311, 577, 300, 311, 577, 264, 6267, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.10924461909702846, "compression_ratio": 1.7252252252252251, "no_speech_prob": 4.029403044114588e-06}, {"id": 588, "seek": 292722, "start": 2951.68, "end": 2956.3399999999997, "text": " That's how everybody that actually gets it to work does it.", "tokens": [663, 311, 577, 2201, 300, 767, 2170, 309, 281, 589, 775, 309, 13], "temperature": 0.0, "avg_logprob": -0.10924461909702846, "compression_ratio": 1.7252252252252251, "no_speech_prob": 4.029403044114588e-06}, {"id": 589, "seek": 295634, "start": 2956.34, "end": 2961.54, "text": " Also often I've had you know Silicon Valley startup hotshots come to me and ask me to", "tokens": [2743, 2049, 286, 600, 632, 291, 458, 25351, 10666, 18578, 36121, 27495, 808, 281, 385, 293, 1029, 385, 281], "temperature": 0.0, "avg_logprob": -0.13935716373404278, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.289297066861764e-06}, {"id": 590, "seek": 295634, "start": 2961.54, "end": 2967.28, "text": " like check out their amazing new startup and I'll ask them you know oh you reckon this", "tokens": [411, 1520, 484, 641, 2243, 777, 18578, 293, 286, 603, 1029, 552, 291, 458, 1954, 291, 29548, 341], "temperature": 0.0, "avg_logprob": -0.13935716373404278, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.289297066861764e-06}, {"id": 591, "seek": 295634, "start": 2967.28, "end": 2974.56, "text": " can separate you know sick people from well people or whatever have you taken the average", "tokens": [393, 4994, 291, 458, 4998, 561, 490, 731, 561, 420, 2035, 362, 291, 2726, 264, 4274], "temperature": 0.0, "avg_logprob": -0.13935716373404278, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.289297066861764e-06}, {"id": 592, "seek": 295634, "start": 2974.56, "end": 2980.52, "text": " of each of these two groups and compared that to your model example and they'll say oh no", "tokens": [295, 1184, 295, 613, 732, 3935, 293, 5347, 300, 281, 428, 2316, 1365, 293, 436, 603, 584, 1954, 572], "temperature": 0.0, "avg_logprob": -0.13935716373404278, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.289297066861764e-06}, {"id": 593, "seek": 295634, "start": 2980.52, "end": 2983.38, "text": " and then they try it and they find out their models worse.", "tokens": [293, 550, 436, 853, 309, 293, 436, 915, 484, 641, 5245, 5324, 13], "temperature": 0.0, "avg_logprob": -0.13935716373404278, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.289297066861764e-06}, {"id": 594, "seek": 298338, "start": 2983.38, "end": 2994.54, "text": " So you need to know whether your model is actually doing something useful.", "tokens": [407, 291, 643, 281, 458, 1968, 428, 2316, 307, 767, 884, 746, 4420, 13], "temperature": 0.0, "avg_logprob": -0.08596963718019683, "compression_ratio": 1.564102564102564, "no_speech_prob": 1.5534905060121673e-06}, {"id": 595, "seek": 298338, "start": 2994.54, "end": 3003.2200000000003, "text": " For projects one of the things you might want to do is join a Kaggle competition.", "tokens": [1171, 4455, 472, 295, 264, 721, 291, 1062, 528, 281, 360, 307, 3917, 257, 48751, 22631, 6211, 13], "temperature": 0.0, "avg_logprob": -0.08596963718019683, "compression_ratio": 1.564102564102564, "no_speech_prob": 1.5534905060121673e-06}, {"id": 596, "seek": 298338, "start": 3003.2200000000003, "end": 3008.1400000000003, "text": " That might be the last thing you see yourself as doing is being a Kaggle competitor but", "tokens": [663, 1062, 312, 264, 1036, 551, 291, 536, 1803, 382, 884, 307, 885, 257, 48751, 22631, 27266, 457], "temperature": 0.0, "avg_logprob": -0.08596963718019683, "compression_ratio": 1.564102564102564, "no_speech_prob": 1.5534905060121673e-06}, {"id": 597, "seek": 300814, "start": 3008.14, "end": 3014.02, "text": " actually this is one of the best possible projects you can do because to enter a Kaggle", "tokens": [767, 341, 307, 472, 295, 264, 1151, 1944, 4455, 291, 393, 360, 570, 281, 3242, 257, 48751, 22631], "temperature": 0.0, "avg_logprob": -0.10393653681248795, "compression_ratio": 1.7596153846153846, "no_speech_prob": 2.4439739263470983e-06}, {"id": 598, "seek": 300814, "start": 3014.02, "end": 3020.2799999999997, "text": " competition even to come last you have to go through the entire process of downloading", "tokens": [6211, 754, 281, 808, 1036, 291, 362, 281, 352, 807, 264, 2302, 1399, 295, 32529], "temperature": 0.0, "avg_logprob": -0.10393653681248795, "compression_ratio": 1.7596153846153846, "no_speech_prob": 2.4439739263470983e-06}, {"id": 599, "seek": 300814, "start": 3020.2799999999997, "end": 3026.14, "text": " a data set formatting it into the right method ready for a model getting it through the model", "tokens": [257, 1412, 992, 39366, 309, 666, 264, 558, 3170, 1919, 337, 257, 2316, 1242, 309, 807, 264, 2316], "temperature": 0.0, "avg_logprob": -0.10393653681248795, "compression_ratio": 1.7596153846153846, "no_speech_prob": 2.4439739263470983e-06}, {"id": 600, "seek": 300814, "start": 3026.14, "end": 3030.5, "text": " saving the output getting it into the correct submission format and submitting it back to", "tokens": [6816, 264, 5598, 1242, 309, 666, 264, 3006, 23689, 7877, 293, 31836, 309, 646, 281], "temperature": 0.0, "avg_logprob": -0.10393653681248795, "compression_ratio": 1.7596153846153846, "no_speech_prob": 2.4439739263470983e-06}, {"id": 601, "seek": 300814, "start": 3030.5, "end": 3031.7799999999997, "text": " Kaggle.", "tokens": [48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.10393653681248795, "compression_ratio": 1.7596153846153846, "no_speech_prob": 2.4439739263470983e-06}, {"id": 602, "seek": 303178, "start": 3031.78, "end": 3041.9, "text": " So getting a model actually up onto the Kaggle leaderboard is really going to test out your", "tokens": [407, 1242, 257, 2316, 767, 493, 3911, 264, 48751, 22631, 5263, 3787, 307, 534, 516, 281, 1500, 484, 428], "temperature": 0.0, "avg_logprob": -0.08520999397199179, "compression_ratio": 1.8225108225108224, "no_speech_prob": 8.579196446589776e-07}, {"id": 603, "seek": 303178, "start": 3041.9, "end": 3046.6600000000003, "text": " end-to-end understanding right and once you've done that you can start to iterate you can", "tokens": [917, 12, 1353, 12, 521, 3701, 558, 293, 1564, 291, 600, 1096, 300, 291, 393, 722, 281, 44497, 291, 393], "temperature": 0.0, "avg_logprob": -0.08520999397199179, "compression_ratio": 1.8225108225108224, "no_speech_prob": 8.579196446589776e-07}, {"id": 604, "seek": 303178, "start": 3046.6600000000003, "end": 3052.46, "text": " start to make it slightly better slightly better slightly better.", "tokens": [722, 281, 652, 309, 4748, 1101, 4748, 1101, 4748, 1101, 13], "temperature": 0.0, "avg_logprob": -0.08520999397199179, "compression_ratio": 1.8225108225108224, "no_speech_prob": 8.579196446589776e-07}, {"id": 605, "seek": 303178, "start": 3052.46, "end": 3057.1400000000003, "text": " So although in a lot of ways Kaggle is not representative of the real world you know", "tokens": [407, 4878, 294, 257, 688, 295, 2098, 48751, 22631, 307, 406, 12424, 295, 264, 957, 1002, 291, 458], "temperature": 0.0, "avg_logprob": -0.08520999397199179, "compression_ratio": 1.8225108225108224, "no_speech_prob": 8.579196446589776e-07}, {"id": 606, "seek": 303178, "start": 3057.1400000000003, "end": 3060.6600000000003, "text": " you don't have to worry about deployment you don't particularly have to worry about kind", "tokens": [291, 500, 380, 362, 281, 3292, 466, 19317, 291, 500, 380, 4098, 362, 281, 3292, 466, 733], "temperature": 0.0, "avg_logprob": -0.08520999397199179, "compression_ratio": 1.8225108225108224, "no_speech_prob": 8.579196446589776e-07}, {"id": 607, "seek": 306066, "start": 3060.66, "end": 3067.3799999999997, "text": " of inference speed stuff like that and a lot of ways it is closer to the real world than", "tokens": [295, 38253, 3073, 1507, 411, 300, 293, 257, 688, 295, 2098, 309, 307, 4966, 281, 264, 957, 1002, 813], "temperature": 0.0, "avg_logprob": -0.10805637359619141, "compression_ratio": 1.7469387755102042, "no_speech_prob": 3.393100996618159e-06}, {"id": 608, "seek": 306066, "start": 3067.3799999999997, "end": 3072.62, "text": " you might expect and that it really does force you to go through the whole process and also", "tokens": [291, 1062, 2066, 293, 300, 309, 534, 775, 3464, 291, 281, 352, 807, 264, 1379, 1399, 293, 611], "temperature": 0.0, "avg_logprob": -0.10805637359619141, "compression_ratio": 1.7469387755102042, "no_speech_prob": 3.393100996618159e-06}, {"id": 609, "seek": 306066, "start": 3072.62, "end": 3078.06, "text": " to think about engine about kind of planning your project carefully.", "tokens": [281, 519, 466, 2848, 466, 733, 295, 5038, 428, 1716, 7500, 13], "temperature": 0.0, "avg_logprob": -0.10805637359619141, "compression_ratio": 1.7469387755102042, "no_speech_prob": 3.393100996618159e-06}, {"id": 610, "seek": 306066, "start": 3078.06, "end": 3086.3399999999997, "text": " So enter a competition with your kind of goal that I want to win right now obviously on", "tokens": [407, 3242, 257, 6211, 365, 428, 733, 295, 3387, 300, 286, 528, 281, 1942, 558, 586, 2745, 322], "temperature": 0.0, "avg_logprob": -0.10805637359619141, "compression_ratio": 1.7469387755102042, "no_speech_prob": 3.393100996618159e-06}, {"id": 611, "seek": 306066, "start": 3086.3399999999997, "end": 3090.58, "text": " your first one you're not going to win but the whole point is it's a competition so you're", "tokens": [428, 700, 472, 291, 434, 406, 516, 281, 1942, 457, 264, 1379, 935, 307, 309, 311, 257, 6211, 370, 291, 434], "temperature": 0.0, "avg_logprob": -0.10805637359619141, "compression_ratio": 1.7469387755102042, "no_speech_prob": 3.393100996618159e-06}, {"id": 612, "seek": 309058, "start": 3090.58, "end": 3097.98, "text": " going to try to do your best right and so to do your best join a competition that's", "tokens": [516, 281, 853, 281, 360, 428, 1151, 558, 293, 370, 281, 360, 428, 1151, 3917, 257, 6211, 300, 311], "temperature": 0.0, "avg_logprob": -0.09295347191038586, "compression_ratio": 1.6995073891625616, "no_speech_prob": 5.255229098111158e-06}, {"id": 613, "seek": 309058, "start": 3097.98, "end": 3110.14, "text": " early right give yourself plenty of time and every single day try to make a small improvement", "tokens": [2440, 558, 976, 1803, 7140, 295, 565, 293, 633, 2167, 786, 853, 281, 652, 257, 1359, 10444], "temperature": 0.0, "avg_logprob": -0.09295347191038586, "compression_ratio": 1.6995073891625616, "no_speech_prob": 5.255229098111158e-06}, {"id": 614, "seek": 309058, "start": 3110.14, "end": 3114.46, "text": " and then you'll find that but you know if you keep reading the forums on Kaggle and", "tokens": [293, 550, 291, 603, 915, 300, 457, 291, 458, 498, 291, 1066, 3760, 264, 26998, 322, 48751, 22631, 293], "temperature": 0.0, "avg_logprob": -0.09295347191038586, "compression_ratio": 1.6995073891625616, "no_speech_prob": 5.255229098111158e-06}, {"id": 615, "seek": 309058, "start": 3114.46, "end": 3119.7, "text": " keep trying a bit more every day you'd be amazed at the end of the three months how", "tokens": [1066, 1382, 257, 857, 544, 633, 786, 291, 1116, 312, 20507, 412, 264, 917, 295, 264, 1045, 2493, 577], "temperature": 0.0, "avg_logprob": -0.09295347191038586, "compression_ratio": 1.6995073891625616, "no_speech_prob": 5.255229098111158e-06}, {"id": 616, "seek": 311970, "start": 3119.7, "end": 3125.18, "text": " much you've learned how much of the stuff that at the start you thought this is I have", "tokens": [709, 291, 600, 3264, 577, 709, 295, 264, 1507, 300, 412, 264, 722, 291, 1194, 341, 307, 286, 362], "temperature": 0.0, "avg_logprob": -0.1053604799158433, "compression_ratio": 1.6521739130434783, "no_speech_prob": 3.6688129512185697e-06}, {"id": 617, "seek": 311970, "start": 3125.18, "end": 3131.14, "text": " no idea what's going on and then you'll realize oh suddenly I do know what's going on and", "tokens": [572, 1558, 437, 311, 516, 322, 293, 550, 291, 603, 4325, 1954, 5800, 286, 360, 458, 437, 311, 516, 322, 293], "temperature": 0.0, "avg_logprob": -0.1053604799158433, "compression_ratio": 1.6521739130434783, "no_speech_prob": 3.6688129512185697e-06}, {"id": 618, "seek": 311970, "start": 3131.14, "end": 3139.2999999999997, "text": " you might find you get in the top 50% which might be better than you expected.", "tokens": [291, 1062, 915, 291, 483, 294, 264, 1192, 2625, 4, 597, 1062, 312, 1101, 813, 291, 5176, 13], "temperature": 0.0, "avg_logprob": -0.1053604799158433, "compression_ratio": 1.6521739130434783, "no_speech_prob": 3.6688129512185697e-06}, {"id": 619, "seek": 311970, "start": 3139.2999999999997, "end": 3146.8999999999996, "text": " So that this is you know highly recommended at some point during this course is have a", "tokens": [407, 300, 341, 307, 291, 458, 5405, 9628, 412, 512, 935, 1830, 341, 1164, 307, 362, 257], "temperature": 0.0, "avg_logprob": -0.1053604799158433, "compression_ratio": 1.6521739130434783, "no_speech_prob": 3.6688129512185697e-06}, {"id": 620, "seek": 314690, "start": 3146.9, "end": 3154.1, "text": " real go at a Kaggle competition.", "tokens": [957, 352, 412, 257, 48751, 22631, 6211, 13], "temperature": 0.0, "avg_logprob": -0.09210082759027896, "compression_ratio": 1.699530516431925, "no_speech_prob": 8.99088831829431e-07}, {"id": 621, "seek": 314690, "start": 3154.1, "end": 3160.14, "text": " So at the end of all of this you might be looking for a job.", "tokens": [407, 412, 264, 917, 295, 439, 295, 341, 291, 1062, 312, 1237, 337, 257, 1691, 13], "temperature": 0.0, "avg_logprob": -0.09210082759027896, "compression_ratio": 1.699530516431925, "no_speech_prob": 8.99088831829431e-07}, {"id": 622, "seek": 314690, "start": 3160.14, "end": 3165.86, "text": " Now this could mean a number of things a lot of people just want to bring some deep learning", "tokens": [823, 341, 727, 914, 257, 1230, 295, 721, 257, 688, 295, 561, 445, 528, 281, 1565, 512, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.09210082759027896, "compression_ratio": 1.699530516431925, "no_speech_prob": 8.99088831829431e-07}, {"id": 623, "seek": 314690, "start": 3165.86, "end": 3171.86, "text": " into their current job and so you know that's if your organization is already doing some", "tokens": [666, 641, 2190, 1691, 293, 370, 291, 458, 300, 311, 498, 428, 4475, 307, 1217, 884, 512], "temperature": 0.0, "avg_logprob": -0.09210082759027896, "compression_ratio": 1.699530516431925, "no_speech_prob": 8.99088831829431e-07}, {"id": 624, "seek": 314690, "start": 3171.86, "end": 3175.5, "text": " deep learning that might be easier than if it's not if it's not you might just have to", "tokens": [2452, 2539, 300, 1062, 312, 3571, 813, 498, 309, 311, 406, 498, 309, 311, 406, 291, 1062, 445, 362, 281], "temperature": 0.0, "avg_logprob": -0.09210082759027896, "compression_ratio": 1.699530516431925, "no_speech_prob": 8.99088831829431e-07}, {"id": 625, "seek": 317550, "start": 3175.5, "end": 3181.34, "text": " start prototyping some things and try to build up some kind of you know proof of concepts", "tokens": [722, 46219, 3381, 512, 721, 293, 853, 281, 1322, 493, 512, 733, 295, 291, 458, 8177, 295, 10392], "temperature": 0.0, "avg_logprob": -0.12349303563435872, "compression_ratio": 1.5578231292517006, "no_speech_prob": 4.495074335864047e-06}, {"id": 626, "seek": 317550, "start": 3181.34, "end": 3187.18, "text": " internally or maybe you're going to try and go out and get and get you know get a new", "tokens": [19501, 420, 1310, 291, 434, 516, 281, 853, 293, 352, 484, 293, 483, 293, 483, 291, 458, 483, 257, 777], "temperature": 0.0, "avg_logprob": -0.12349303563435872, "compression_ratio": 1.5578231292517006, "no_speech_prob": 4.495074335864047e-06}, {"id": 627, "seek": 317550, "start": 3187.18, "end": 3195.78, "text": " role as a researcher or a data scientist or whatever.", "tokens": [3090, 382, 257, 21751, 420, 257, 1412, 12662, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.12349303563435872, "compression_ratio": 1.5578231292517006, "no_speech_prob": 4.495074335864047e-06}, {"id": 628, "seek": 319578, "start": 3195.78, "end": 3205.5800000000004, "text": " Most people are not going to be able to rely on their you know Stanford PhD to get them", "tokens": [4534, 561, 366, 406, 516, 281, 312, 1075, 281, 10687, 322, 641, 291, 458, 20374, 14476, 281, 483, 552], "temperature": 0.0, "avg_logprob": -0.12858831882476807, "compression_ratio": 1.7593582887700534, "no_speech_prob": 3.059018069961894e-07}, {"id": 629, "seek": 319578, "start": 3205.5800000000004, "end": 3206.5800000000004, "text": " there.", "tokens": [456, 13], "temperature": 0.0, "avg_logprob": -0.12858831882476807, "compression_ratio": 1.7593582887700534, "no_speech_prob": 3.059018069961894e-07}, {"id": 630, "seek": 319578, "start": 3206.5800000000004, "end": 3211.78, "text": " Right most people are going to rely have to rely on their portfolio.", "tokens": [1779, 881, 561, 366, 516, 281, 10687, 362, 281, 10687, 322, 641, 12583, 13], "temperature": 0.0, "avg_logprob": -0.12858831882476807, "compression_ratio": 1.7593582887700534, "no_speech_prob": 3.059018069961894e-07}, {"id": 631, "seek": 319578, "start": 3211.78, "end": 3216.6200000000003, "text": " So your portfolio is going to be all the stuff you build along the way.", "tokens": [407, 428, 12583, 307, 516, 281, 312, 439, 264, 1507, 291, 1322, 2051, 264, 636, 13], "temperature": 0.0, "avg_logprob": -0.12858831882476807, "compression_ratio": 1.7593582887700534, "no_speech_prob": 3.059018069961894e-07}, {"id": 632, "seek": 319578, "start": 3216.6200000000003, "end": 3225.0600000000004, "text": " It's your footprint on the deep learning community and that footprint is going to include you", "tokens": [467, 311, 428, 24222, 322, 264, 2452, 2539, 1768, 293, 300, 24222, 307, 516, 281, 4090, 291], "temperature": 0.0, "avg_logprob": -0.12858831882476807, "compression_ratio": 1.7593582887700534, "no_speech_prob": 3.059018069961894e-07}, {"id": 633, "seek": 322506, "start": 3225.06, "end": 3230.74, "text": " know things things like your contributions to the Fast AI forums and your tweets and", "tokens": [458, 721, 721, 411, 428, 15725, 281, 264, 15968, 7318, 26998, 293, 428, 25671, 293], "temperature": 0.0, "avg_logprob": -0.13590723276138306, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.089337951678317e-06}, {"id": 634, "seek": 322506, "start": 3230.74, "end": 3234.66, "text": " your stuff on Discord.", "tokens": [428, 1507, 322, 32623, 13], "temperature": 0.0, "avg_logprob": -0.13590723276138306, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.089337951678317e-06}, {"id": 635, "seek": 322506, "start": 3234.66, "end": 3243.22, "text": " I would say pretty much every one of the Fast AI alumni that have come to my attention as", "tokens": [286, 576, 584, 1238, 709, 633, 472, 295, 264, 15968, 7318, 16347, 300, 362, 808, 281, 452, 3202, 382], "temperature": 0.0, "avg_logprob": -0.13590723276138306, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.089337951678317e-06}, {"id": 636, "seek": 322506, "start": 3243.22, "end": 3251.54, "text": " being thoughtful and effective community members all have very very very good jobs now.", "tokens": [885, 21566, 293, 4942, 1768, 2679, 439, 362, 588, 588, 588, 665, 4782, 586, 13], "temperature": 0.0, "avg_logprob": -0.13590723276138306, "compression_ratio": 1.5833333333333333, "no_speech_prob": 3.089337951678317e-06}, {"id": 637, "seek": 325154, "start": 3251.54, "end": 3259.62, "text": " And so like people really really notice this footprint right so your blog posts your GitHub", "tokens": [400, 370, 411, 561, 534, 534, 3449, 341, 24222, 558, 370, 428, 6968, 12300, 428, 23331], "temperature": 0.0, "avg_logprob": -0.12340121884499827, "compression_ratio": 1.620253164556962, "no_speech_prob": 2.9022794478805736e-06}, {"id": 638, "seek": 325154, "start": 3259.62, "end": 3266.94, "text": " projects these are the these are the things that are going to get you a job.", "tokens": [4455, 613, 366, 264, 613, 366, 264, 721, 300, 366, 516, 281, 483, 291, 257, 1691, 13], "temperature": 0.0, "avg_logprob": -0.12340121884499827, "compression_ratio": 1.620253164556962, "no_speech_prob": 2.9022794478805736e-06}, {"id": 639, "seek": 325154, "start": 3266.94, "end": 3278.3, "text": " They probably won't get you a job at a big company a big old company in a you know kind", "tokens": [814, 1391, 1582, 380, 483, 291, 257, 1691, 412, 257, 955, 2237, 257, 955, 1331, 2237, 294, 257, 291, 458, 733], "temperature": 0.0, "avg_logprob": -0.12340121884499827, "compression_ratio": 1.620253164556962, "no_speech_prob": 2.9022794478805736e-06}, {"id": 640, "seek": 327830, "start": 3278.3, "end": 3282.7400000000002, "text": " of standard established IT job right.", "tokens": [295, 3832, 7545, 6783, 1691, 558, 13], "temperature": 0.0, "avg_logprob": -0.16230485815750925, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.048786417522933e-06}, {"id": 641, "seek": 327830, "start": 3282.7400000000002, "end": 3286.2200000000003, "text": " That's going to go through HR and HR are going to like they're not going to understand any", "tokens": [663, 311, 516, 281, 352, 807, 19460, 293, 19460, 366, 516, 281, 411, 436, 434, 406, 516, 281, 1223, 604], "temperature": 0.0, "avg_logprob": -0.16230485815750925, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.048786417522933e-06}, {"id": 642, "seek": 327830, "start": 3286.2200000000003, "end": 3289.94, "text": " of your GitHub code or know any about your community impact.", "tokens": [295, 428, 23331, 3089, 420, 458, 604, 466, 428, 1768, 2712, 13], "temperature": 0.0, "avg_logprob": -0.16230485815750925, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.048786417522933e-06}, {"id": 643, "seek": 327830, "start": 3289.94, "end": 3293.82, "text": " They're just going to know about credentials right and you'll come up against somebody", "tokens": [814, 434, 445, 516, 281, 458, 466, 27404, 558, 293, 291, 603, 808, 493, 1970, 2618], "temperature": 0.0, "avg_logprob": -0.16230485815750925, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.048786417522933e-06}, {"id": 644, "seek": 327830, "start": 3293.82, "end": 3296.86, "text": " with a Stanford PhD and they'll get the job right.", "tokens": [365, 257, 20374, 14476, 293, 436, 603, 483, 264, 1691, 558, 13], "temperature": 0.0, "avg_logprob": -0.16230485815750925, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.048786417522933e-06}, {"id": 645, "seek": 327830, "start": 3296.86, "end": 3303.0, "text": " But startups particularly startups from other people who've got similar backgrounds of which", "tokens": [583, 28041, 4098, 28041, 490, 661, 561, 567, 600, 658, 2531, 17336, 295, 597], "temperature": 0.0, "avg_logprob": -0.16230485815750925, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.048786417522933e-06}, {"id": 646, "seek": 330300, "start": 3303.0, "end": 3309.82, "text": " there are many are going to appreciate you or companies that don't really have an established", "tokens": [456, 366, 867, 366, 516, 281, 4449, 291, 420, 3431, 300, 500, 380, 534, 362, 364, 7545], "temperature": 0.0, "avg_logprob": -0.13322414786128675, "compression_ratio": 1.5449101796407185, "no_speech_prob": 1.3925275652582059e-06}, {"id": 647, "seek": 330300, "start": 3309.82, "end": 3318.46, "text": " AI group yet or the startup you built yourself will certainly appreciate you.", "tokens": [7318, 1594, 1939, 420, 264, 18578, 291, 3094, 1803, 486, 3297, 4449, 291, 13], "temperature": 0.0, "avg_logprob": -0.13322414786128675, "compression_ratio": 1.5449101796407185, "no_speech_prob": 1.3925275652582059e-06}, {"id": 648, "seek": 330300, "start": 3318.46, "end": 3325.46, "text": " So it's the more you've got a portfolio and that you can show that you've really built", "tokens": [407, 309, 311, 264, 544, 291, 600, 658, 257, 12583, 293, 300, 291, 393, 855, 300, 291, 600, 534, 3094], "temperature": 0.0, "avg_logprob": -0.13322414786128675, "compression_ratio": 1.5449101796407185, "no_speech_prob": 1.3925275652582059e-06}, {"id": 649, "seek": 332546, "start": 3325.46, "end": 3335.9, "text": " stuff the better and so start early.", "tokens": [1507, 264, 1101, 293, 370, 722, 2440, 13], "temperature": 0.0, "avg_logprob": -0.08983276440547062, "compression_ratio": 1.5220588235294117, "no_speech_prob": 2.6015934508905048e-06}, {"id": 650, "seek": 332546, "start": 3335.9, "end": 3347.7, "text": " Another reason to finish this first course is that it's going to allow you to do the", "tokens": [3996, 1778, 281, 2413, 341, 700, 1164, 307, 300, 309, 311, 516, 281, 2089, 291, 281, 360, 264], "temperature": 0.0, "avg_logprob": -0.08983276440547062, "compression_ratio": 1.5220588235294117, "no_speech_prob": 2.6015934508905048e-06}, {"id": 651, "seek": 332546, "start": 3347.7, "end": 3352.7400000000002, "text": " second course and if you're doing this live part two we're going to be doing actually", "tokens": [1150, 1164, 293, 498, 291, 434, 884, 341, 1621, 644, 732, 321, 434, 516, 281, 312, 884, 767], "temperature": 0.0, "avg_logprob": -0.08983276440547062, "compression_ratio": 1.5220588235294117, "no_speech_prob": 2.6015934508905048e-06}, {"id": 652, "seek": 335274, "start": 3352.74, "end": 3359.74, "text": " a whole new part two towards you know basically shortly after this is finished right.", "tokens": [257, 1379, 777, 644, 732, 3030, 291, 458, 1936, 13392, 934, 341, 307, 4335, 558, 13], "temperature": 0.0, "avg_logprob": -0.15538126084862686, "compression_ratio": 1.5706806282722514, "no_speech_prob": 4.356816134531982e-06}, {"id": 653, "seek": 335274, "start": 3359.74, "end": 3365.1, "text": " So if you if you finish this and do a good job of it then you could actually be one of", "tokens": [407, 498, 291, 498, 291, 2413, 341, 293, 360, 257, 665, 1691, 295, 309, 550, 291, 727, 767, 312, 472, 295], "temperature": 0.0, "avg_logprob": -0.15538126084862686, "compression_ratio": 1.5706806282722514, "no_speech_prob": 4.356816134531982e-06}, {"id": 654, "seek": 335274, "start": 3365.1, "end": 3368.9799999999996, "text": " the first to do part two.", "tokens": [264, 700, 281, 360, 644, 732, 13], "temperature": 0.0, "avg_logprob": -0.15538126084862686, "compression_ratio": 1.5706806282722514, "no_speech_prob": 4.356816134531982e-06}, {"id": 655, "seek": 335274, "start": 3368.9799999999996, "end": 3379.2999999999997, "text": " Now we've seen how easy Colab is to get started.", "tokens": [823, 321, 600, 1612, 577, 1858, 4004, 455, 307, 281, 483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.15538126084862686, "compression_ratio": 1.5706806282722514, "no_speech_prob": 4.356816134531982e-06}, {"id": 656, "seek": 335274, "start": 3379.2999999999997, "end": 3381.8999999999996, "text": " We've also talked about some of the downsides of it.", "tokens": [492, 600, 611, 2825, 466, 512, 295, 264, 21554, 1875, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.15538126084862686, "compression_ratio": 1.5706806282722514, "no_speech_prob": 4.356816134531982e-06}, {"id": 657, "seek": 338190, "start": 3381.9, "end": 3386.78, "text": " It's kind of ephemeral you start from scratch every time you've got this kind of hacky stuff", "tokens": [467, 311, 733, 295, 308, 41245, 2790, 291, 722, 490, 8459, 633, 565, 291, 600, 658, 341, 733, 295, 10339, 88, 1507], "temperature": 0.0, "avg_logprob": -0.21180391813579358, "compression_ratio": 1.6473214285714286, "no_speech_prob": 3.119967004749924e-05}, {"id": 658, "seek": 338190, "start": 3386.78, "end": 3391.94, "text": " of saving notebooks into your Google Drive blah blah blah.", "tokens": [295, 6816, 43782, 666, 428, 3329, 15622, 12288, 12288, 12288, 13], "temperature": 0.0, "avg_logprob": -0.21180391813579358, "compression_ratio": 1.6473214285714286, "no_speech_prob": 3.119967004749924e-05}, {"id": 659, "seek": 338190, "start": 3391.94, "end": 3397.34, "text": " AWS on the other hand is going to give you and Google Cloud and Java slabs and so forth", "tokens": [17650, 322, 264, 661, 1011, 307, 516, 281, 976, 291, 293, 3329, 8061, 293, 10745, 1061, 17243, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.21180391813579358, "compression_ratio": 1.6473214285714286, "no_speech_prob": 3.119967004749924e-05}, {"id": 660, "seek": 338190, "start": 3397.34, "end": 3401.42, "text": " are going to give you a real Linux server.", "tokens": [366, 516, 281, 976, 291, 257, 957, 18734, 7154, 13], "temperature": 0.0, "avg_logprob": -0.21180391813579358, "compression_ratio": 1.6473214285714286, "no_speech_prob": 3.119967004749924e-05}, {"id": 661, "seek": 338190, "start": 3401.42, "end": 3408.46, "text": " And it's going to cost you Java slabs is the cheapest about 40 cents AWS I think about", "tokens": [400, 309, 311, 516, 281, 2063, 291, 10745, 1061, 17243, 307, 264, 29167, 466, 3356, 14941, 17650, 286, 519, 466], "temperature": 0.0, "avg_logprob": -0.21180391813579358, "compression_ratio": 1.6473214285714286, "no_speech_prob": 3.119967004749924e-05}, {"id": 662, "seek": 340846, "start": 3408.46, "end": 3412.26, "text": " 60 cents US per hour.", "tokens": [4060, 14941, 2546, 680, 1773, 13], "temperature": 0.0, "avg_logprob": -0.15924398001138265, "compression_ratio": 1.457142857142857, "no_speech_prob": 2.0904381017317064e-06}, {"id": 663, "seek": 340846, "start": 3412.26, "end": 3418.2200000000003, "text": " It's not going to send you broke but it's you know it's not nothing.", "tokens": [467, 311, 406, 516, 281, 2845, 291, 6902, 457, 309, 311, 291, 458, 309, 311, 406, 1825, 13], "temperature": 0.0, "avg_logprob": -0.15924398001138265, "compression_ratio": 1.457142857142857, "no_speech_prob": 2.0904381017317064e-06}, {"id": 664, "seek": 340846, "start": 3418.2200000000003, "end": 3425.34, "text": " It's a good idea to try it if you can and I'm going to show you how to get started there.", "tokens": [467, 311, 257, 665, 1558, 281, 853, 309, 498, 291, 393, 293, 286, 478, 516, 281, 855, 291, 577, 281, 483, 1409, 456, 13], "temperature": 0.0, "avg_logprob": -0.15924398001138265, "compression_ratio": 1.457142857142857, "no_speech_prob": 2.0904381017317064e-06}, {"id": 665, "seek": 340846, "start": 3425.34, "end": 3435.78, "text": " And what we might do Michael is I'll do some Q&A while things are running.", "tokens": [400, 437, 321, 1062, 360, 5116, 307, 286, 603, 360, 512, 1249, 5, 32, 1339, 721, 366, 2614, 13], "temperature": 0.0, "avg_logprob": -0.15924398001138265, "compression_ratio": 1.457142857142857, "no_speech_prob": 2.0904381017317064e-06}, {"id": 666, "seek": 343578, "start": 3435.78, "end": 3441.92, "text": " So I'm going to head over to AWS EC2.", "tokens": [407, 286, 478, 516, 281, 1378, 670, 281, 17650, 19081, 17, 13], "temperature": 0.0, "avg_logprob": -0.09311105871713289, "compression_ratio": 1.6093023255813954, "no_speech_prob": 6.1440441641025245e-06}, {"id": 667, "seek": 343578, "start": 3441.92, "end": 3446.1400000000003, "text": " So one of the tricky things about AWS is they've got hundreds of products.", "tokens": [407, 472, 295, 264, 12414, 721, 466, 17650, 307, 436, 600, 658, 6779, 295, 3383, 13], "temperature": 0.0, "avg_logprob": -0.09311105871713289, "compression_ratio": 1.6093023255813954, "no_speech_prob": 6.1440441641025245e-06}, {"id": 668, "seek": 343578, "start": 3446.1400000000003, "end": 3450.78, "text": " This is Amazon Web Services and they all have names that are totally meaningless.", "tokens": [639, 307, 6795, 9573, 12124, 293, 436, 439, 362, 5288, 300, 366, 3879, 33232, 13], "temperature": 0.0, "avg_logprob": -0.09311105871713289, "compression_ratio": 1.6093023255813954, "no_speech_prob": 6.1440441641025245e-06}, {"id": 669, "seek": 343578, "start": 3450.78, "end": 3456.78, "text": " So you just have to know EC2 is the name of the thing that you go to to rent a computer.", "tokens": [407, 291, 445, 362, 281, 458, 19081, 17, 307, 264, 1315, 295, 264, 551, 300, 291, 352, 281, 281, 6214, 257, 3820, 13], "temperature": 0.0, "avg_logprob": -0.09311105871713289, "compression_ratio": 1.6093023255813954, "no_speech_prob": 6.1440441641025245e-06}, {"id": 670, "seek": 343578, "start": 3456.78, "end": 3465.2200000000003, "text": " So they don't call it Amazon computer rental they call it EC2.", "tokens": [407, 436, 500, 380, 818, 309, 6795, 3820, 21468, 436, 818, 309, 19081, 17, 13], "temperature": 0.0, "avg_logprob": -0.09311105871713289, "compression_ratio": 1.6093023255813954, "no_speech_prob": 6.1440441641025245e-06}, {"id": 671, "seek": 346522, "start": 3465.22, "end": 3472.3399999999997, "text": " So the first thing you need to do is you need to sign up to AWS.", "tokens": [407, 264, 700, 551, 291, 643, 281, 360, 307, 291, 643, 281, 1465, 493, 281, 17650, 13], "temperature": 0.0, "avg_logprob": -0.08457578397264667, "compression_ratio": 1.7666666666666666, "no_speech_prob": 1.5056839401950128e-06}, {"id": 672, "seek": 346522, "start": 3472.3399999999997, "end": 3476.14, "text": " And one of the things that they get is a lot of fraud.", "tokens": [400, 472, 295, 264, 721, 300, 436, 483, 307, 257, 688, 295, 14560, 13], "temperature": 0.0, "avg_logprob": -0.08457578397264667, "compression_ratio": 1.7666666666666666, "no_speech_prob": 1.5056839401950128e-06}, {"id": 673, "seek": 346522, "start": 3476.14, "end": 3479.4399999999996, "text": " So a lot of people try to use their GPUs to mine Bitcoin.", "tokens": [407, 257, 688, 295, 561, 853, 281, 764, 641, 18407, 82, 281, 3892, 11414, 13], "temperature": 0.0, "avg_logprob": -0.08457578397264667, "compression_ratio": 1.7666666666666666, "no_speech_prob": 1.5056839401950128e-06}, {"id": 674, "seek": 346522, "start": 3479.4399999999996, "end": 3485.14, "text": " So you have to ask them to give you permission to use their GPUs and that's called requesting", "tokens": [407, 291, 362, 281, 1029, 552, 281, 976, 291, 11226, 281, 764, 641, 18407, 82, 293, 300, 311, 1219, 31937], "temperature": 0.0, "avg_logprob": -0.08457578397264667, "compression_ratio": 1.7666666666666666, "no_speech_prob": 1.5056839401950128e-06}, {"id": 675, "seek": 346522, "start": 3485.14, "end": 3487.5, "text": " a service limit increase.", "tokens": [257, 2643, 4948, 3488, 13], "temperature": 0.0, "avg_logprob": -0.08457578397264667, "compression_ratio": 1.7666666666666666, "no_speech_prob": 1.5056839401950128e-06}, {"id": 676, "seek": 346522, "start": 3487.5, "end": 3492.7799999999997, "text": " So you'll need to follow the steps here to ask them for a limit increase.", "tokens": [407, 291, 603, 643, 281, 1524, 264, 4439, 510, 281, 1029, 552, 337, 257, 4948, 3488, 13], "temperature": 0.0, "avg_logprob": -0.08457578397264667, "compression_ratio": 1.7666666666666666, "no_speech_prob": 1.5056839401950128e-06}, {"id": 677, "seek": 349278, "start": 3492.78, "end": 3498.0400000000004, "text": " If you write these exact words with this exact formatting it might come through a little", "tokens": [759, 291, 2464, 613, 1900, 2283, 365, 341, 1900, 39366, 309, 1062, 808, 807, 257, 707], "temperature": 0.0, "avg_logprob": -0.14547549799868936, "compression_ratio": 1.6278026905829597, "no_speech_prob": 5.955062079010531e-06}, {"id": 678, "seek": 349278, "start": 3498.0400000000004, "end": 3500.5, "text": " bit quicker.", "tokens": [857, 16255, 13], "temperature": 0.0, "avg_logprob": -0.14547549799868936, "compression_ratio": 1.6278026905829597, "no_speech_prob": 5.955062079010531e-06}, {"id": 679, "seek": 349278, "start": 3500.5, "end": 3512.42, "text": " If you're from a country where there's a lot of fraud you might not even get this permission.", "tokens": [759, 291, 434, 490, 257, 1941, 689, 456, 311, 257, 688, 295, 14560, 291, 1062, 406, 754, 483, 341, 11226, 13], "temperature": 0.0, "avg_logprob": -0.14547549799868936, "compression_ratio": 1.6278026905829597, "no_speech_prob": 5.955062079010531e-06}, {"id": 680, "seek": 349278, "start": 3512.42, "end": 3514.1000000000004, "text": " Maybe Java Slabs is going to be easier.", "tokens": [2704, 10745, 6187, 17243, 307, 516, 281, 312, 3571, 13], "temperature": 0.0, "avg_logprob": -0.14547549799868936, "compression_ratio": 1.6278026905829597, "no_speech_prob": 5.955062079010531e-06}, {"id": 681, "seek": 349278, "start": 3514.1000000000004, "end": 3516.1400000000003, "text": " I'm not sure Java Slabs even has the fraud checks.", "tokens": [286, 478, 406, 988, 10745, 6187, 17243, 754, 575, 264, 14560, 13834, 13], "temperature": 0.0, "avg_logprob": -0.14547549799868936, "compression_ratio": 1.6278026905829597, "no_speech_prob": 5.955062079010531e-06}, {"id": 682, "seek": 349278, "start": 3516.1400000000003, "end": 3521.6800000000003, "text": " So anyway there's quite a few places you can you can try to get an instance.", "tokens": [407, 4033, 456, 311, 1596, 257, 1326, 3190, 291, 393, 291, 393, 853, 281, 483, 364, 5197, 13], "temperature": 0.0, "avg_logprob": -0.14547549799868936, "compression_ratio": 1.6278026905829597, "no_speech_prob": 5.955062079010531e-06}, {"id": 683, "seek": 352168, "start": 3521.68, "end": 3525.2599999999998, "text": " So if AWS has a problem with your quota try somewhere else.", "tokens": [407, 498, 17650, 575, 257, 1154, 365, 428, 45171, 853, 4079, 1646, 13], "temperature": 0.0, "avg_logprob": -0.157379150390625, "compression_ratio": 1.5966101694915253, "no_speech_prob": 6.048817340342794e-06}, {"id": 684, "seek": 352168, "start": 3525.2599999999998, "end": 3528.62, "text": " But generally speaking most people should get a response pretty quickly saying you've", "tokens": [583, 5101, 4124, 881, 561, 820, 483, 257, 4134, 1238, 2661, 1566, 291, 600], "temperature": 0.0, "avg_logprob": -0.157379150390625, "compression_ratio": 1.5966101694915253, "no_speech_prob": 6.048817340342794e-06}, {"id": 685, "seek": 352168, "start": 3528.62, "end": 3529.94, "text": " now got approved.", "tokens": [586, 658, 10826, 13], "temperature": 0.0, "avg_logprob": -0.157379150390625, "compression_ratio": 1.5966101694915253, "no_speech_prob": 6.048817340342794e-06}, {"id": 686, "seek": 352168, "start": 3529.94, "end": 3536.62, "text": " So for you doing this course if you're going to try out AWS EC2 I suggest you log in and", "tokens": [407, 337, 291, 884, 341, 1164, 498, 291, 434, 516, 281, 853, 484, 17650, 19081, 17, 286, 3402, 291, 3565, 294, 293], "temperature": 0.0, "avg_logprob": -0.157379150390625, "compression_ratio": 1.5966101694915253, "no_speech_prob": 6.048817340342794e-06}, {"id": 687, "seek": 352168, "start": 3536.62, "end": 3542.3199999999997, "text": " request this service limit increase right away so that you know by the time you come", "tokens": [5308, 341, 2643, 4948, 3488, 558, 1314, 370, 300, 291, 458, 538, 264, 565, 291, 808], "temperature": 0.0, "avg_logprob": -0.157379150390625, "compression_ratio": 1.5966101694915253, "no_speech_prob": 6.048817340342794e-06}, {"id": 688, "seek": 352168, "start": 3542.3199999999997, "end": 3544.14, "text": " back tomorrow or the next day it'll be done.", "tokens": [646, 4153, 420, 264, 958, 786, 309, 603, 312, 1096, 13], "temperature": 0.0, "avg_logprob": -0.157379150390625, "compression_ratio": 1.5966101694915253, "no_speech_prob": 6.048817340342794e-06}, {"id": 689, "seek": 352168, "start": 3544.14, "end": 3548.46, "text": " And so what I'm currently doing is I'm on course Fast AI and I've gone to Slitix servers", "tokens": [400, 370, 437, 286, 478, 4362, 884, 307, 286, 478, 322, 1164, 15968, 7318, 293, 286, 600, 2780, 281, 6187, 270, 970, 15909], "temperature": 0.0, "avg_logprob": -0.157379150390625, "compression_ratio": 1.5966101694915253, "no_speech_prob": 6.048817340342794e-06}, {"id": 690, "seek": 354846, "start": 3548.46, "end": 3552.26, "text": " AWS EC2 and we're following through that project process.", "tokens": [17650, 19081, 17, 293, 321, 434, 3480, 807, 300, 1716, 1399, 13], "temperature": 0.0, "avg_logprob": -0.15248985517592656, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.4060839223238872e-06}, {"id": 691, "seek": 354846, "start": 3552.26, "end": 3560.06, "text": " Okay now to log in to your server you're going to need to use something called SSH secure", "tokens": [1033, 586, 281, 3565, 294, 281, 428, 7154, 291, 434, 516, 281, 643, 281, 764, 746, 1219, 12238, 39, 7144], "temperature": 0.0, "avg_logprob": -0.15248985517592656, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.4060839223238872e-06}, {"id": 692, "seek": 354846, "start": 3560.06, "end": 3561.1, "text": " shell.", "tokens": [8720, 13], "temperature": 0.0, "avg_logprob": -0.15248985517592656, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.4060839223238872e-06}, {"id": 693, "seek": 354846, "start": 3561.1, "end": 3567.7, "text": " So this is something where on your computer screen that server's computer screen effectively", "tokens": [407, 341, 307, 746, 689, 322, 428, 3820, 2568, 300, 7154, 311, 3820, 2568, 8659], "temperature": 0.0, "avg_logprob": -0.15248985517592656, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.4060839223238872e-06}, {"id": 694, "seek": 354846, "start": 3567.7, "end": 3572.98, "text": " is going to appear and the stuff you type is actually running on that remote server", "tokens": [307, 516, 281, 4204, 293, 264, 1507, 291, 2010, 307, 767, 2614, 322, 300, 8607, 7154], "temperature": 0.0, "avg_logprob": -0.15248985517592656, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.4060839223238872e-06}, {"id": 695, "seek": 354846, "start": 3572.98, "end": 3576.2200000000003, "text": " not on your computer.", "tokens": [406, 322, 428, 3820, 13], "temperature": 0.0, "avg_logprob": -0.15248985517592656, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.4060839223238872e-06}, {"id": 696, "seek": 357622, "start": 3576.22, "end": 3582.98, "text": " Since pretty much nobody uses usernames and passwords for SSH instead we use something", "tokens": [4162, 1238, 709, 5079, 4960, 505, 1248, 1632, 293, 33149, 337, 12238, 39, 2602, 321, 764, 746], "temperature": 0.0, "avg_logprob": -0.1232060758691085, "compression_ratio": 1.795, "no_speech_prob": 8.664440429129172e-06}, {"id": 697, "seek": 357622, "start": 3582.98, "end": 3590.2599999999998, "text": " called public key cryptography which is where you basically have a secret number which only", "tokens": [1219, 1908, 2141, 9844, 5820, 597, 307, 689, 291, 1936, 362, 257, 4054, 1230, 597, 787], "temperature": 0.0, "avg_logprob": -0.1232060758691085, "compression_ratio": 1.795, "no_speech_prob": 8.664440429129172e-06}, {"id": 698, "seek": 357622, "start": 3590.2599999999998, "end": 3597.54, "text": " you know and then there's another public number you tell other people and basically there's", "tokens": [291, 458, 293, 550, 456, 311, 1071, 1908, 1230, 291, 980, 661, 561, 293, 1936, 456, 311], "temperature": 0.0, "avg_logprob": -0.1232060758691085, "compression_ratio": 1.795, "no_speech_prob": 8.664440429129172e-06}, {"id": 699, "seek": 357622, "start": 3597.54, "end": 3604.2799999999997, "text": " a really cool math trick which allows people to check whether you have the secret number", "tokens": [257, 534, 1627, 5221, 4282, 597, 4045, 561, 281, 1520, 1968, 291, 362, 264, 4054, 1230], "temperature": 0.0, "avg_logprob": -0.1232060758691085, "compression_ratio": 1.795, "no_speech_prob": 8.664440429129172e-06}, {"id": 700, "seek": 360428, "start": 3604.28, "end": 3608.42, "text": " without actually anybody without actually telling them the secret number and the process", "tokens": [1553, 767, 4472, 1553, 767, 3585, 552, 264, 4054, 1230, 293, 264, 1399], "temperature": 0.0, "avg_logprob": -0.1220097647772895, "compression_ratio": 1.8666666666666667, "no_speech_prob": 6.3391216826858e-06}, {"id": 701, "seek": 360428, "start": 3608.42, "end": 3613.02, "text": " so that's called so that's what an SSH key is.", "tokens": [370, 300, 311, 1219, 370, 300, 311, 437, 364, 12238, 39, 2141, 307, 13], "temperature": 0.0, "avg_logprob": -0.1220097647772895, "compression_ratio": 1.8666666666666667, "no_speech_prob": 6.3391216826858e-06}, {"id": 702, "seek": 360428, "start": 3613.02, "end": 3617.34, "text": " So there's this thing called a public key and that's the number that you're the code", "tokens": [407, 456, 311, 341, 551, 1219, 257, 1908, 2141, 293, 300, 311, 264, 1230, 300, 291, 434, 264, 3089], "temperature": 0.0, "avg_logprob": -0.1220097647772895, "compression_ratio": 1.8666666666666667, "no_speech_prob": 6.3391216826858e-06}, {"id": 703, "seek": 360428, "start": 3617.34, "end": 3620.94, "text": " that you're going to give to anybody you want to be able to log into and then there's your", "tokens": [300, 291, 434, 516, 281, 976, 281, 4472, 291, 528, 281, 312, 1075, 281, 3565, 666, 293, 550, 456, 311, 428], "temperature": 0.0, "avg_logprob": -0.1220097647772895, "compression_ratio": 1.8666666666666667, "no_speech_prob": 6.3391216826858e-06}, {"id": 704, "seek": 360428, "start": 3620.94, "end": 3625.1200000000003, "text": " private key which you're going to keep for yourself.", "tokens": [4551, 2141, 597, 291, 434, 516, 281, 1066, 337, 1803, 13], "temperature": 0.0, "avg_logprob": -0.1220097647772895, "compression_ratio": 1.8666666666666667, "no_speech_prob": 6.3391216826858e-06}, {"id": 705, "seek": 362512, "start": 3625.12, "end": 3634.8199999999997, "text": " So you're going to need a terminal so on Windows in the store there's something called the", "tokens": [407, 291, 434, 516, 281, 643, 257, 14709, 370, 322, 8591, 294, 264, 3531, 456, 311, 746, 1219, 264], "temperature": 0.0, "avg_logprob": -0.13008686014123866, "compression_ratio": 1.6648936170212767, "no_speech_prob": 3.966936219512718e-06}, {"id": 706, "seek": 362512, "start": 3634.8199999999997, "end": 3639.18, "text": " Windows terminal which Microsoft provides for free which is pretty good.", "tokens": [8591, 14709, 597, 8116, 6417, 337, 1737, 597, 307, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.13008686014123866, "compression_ratio": 1.6648936170212767, "no_speech_prob": 3.966936219512718e-06}, {"id": 707, "seek": 362512, "start": 3639.18, "end": 3646.48, "text": " Mac has a terminal that comes with it, Linux has a terminal that comes with it.", "tokens": [5707, 575, 257, 14709, 300, 1487, 365, 309, 11, 18734, 575, 257, 14709, 300, 1487, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.13008686014123866, "compression_ratio": 1.6648936170212767, "no_speech_prob": 3.966936219512718e-06}, {"id": 708, "seek": 362512, "start": 3646.48, "end": 3651.3599999999997, "text": " So I'm using Windows but it'll basically look the same for everybody.", "tokens": [407, 286, 478, 1228, 8591, 457, 309, 603, 1936, 574, 264, 912, 337, 2201, 13], "temperature": 0.0, "avg_logprob": -0.13008686014123866, "compression_ratio": 1.6648936170212767, "no_speech_prob": 3.966936219512718e-06}, {"id": 709, "seek": 365136, "start": 3651.36, "end": 3663.02, "text": " Now on Windows you need a Ubuntu Linux shell not a normal Windows shell so to do that you", "tokens": [823, 322, 8591, 291, 643, 257, 30230, 45605, 18734, 8720, 406, 257, 2710, 8591, 8720, 370, 281, 360, 300, 291], "temperature": 0.0, "avg_logprob": -0.17565000197466682, "compression_ratio": 1.5829145728643217, "no_speech_prob": 4.092870767635759e-06}, {"id": 710, "seek": 365136, "start": 3663.02, "end": 3668.52, "text": " need something called WSL, Windows Subsystem for Linux and that will give you a full Ubuntu", "tokens": [643, 746, 1219, 343, 47012, 11, 8591, 8511, 28215, 337, 18734, 293, 300, 486, 976, 291, 257, 1577, 30230, 45605], "temperature": 0.0, "avg_logprob": -0.17565000197466682, "compression_ratio": 1.5829145728643217, "no_speech_prob": 4.092870767635759e-06}, {"id": 711, "seek": 365136, "start": 3668.52, "end": 3670.02, "text": " system on your Windows computer.", "tokens": [1185, 322, 428, 8591, 3820, 13], "temperature": 0.0, "avg_logprob": -0.17565000197466682, "compression_ratio": 1.5829145728643217, "no_speech_prob": 4.092870767635759e-06}, {"id": 712, "seek": 365136, "start": 3670.02, "end": 3674.26, "text": " Again it's free, it only takes a couple of minutes to set up so there's a link to how", "tokens": [3764, 309, 311, 1737, 11, 309, 787, 2516, 257, 1916, 295, 2077, 281, 992, 493, 370, 456, 311, 257, 2113, 281, 577], "temperature": 0.0, "avg_logprob": -0.17565000197466682, "compression_ratio": 1.5829145728643217, "no_speech_prob": 4.092870767635759e-06}, {"id": 713, "seek": 365136, "start": 3674.26, "end": 3675.26, "text": " to do it here.", "tokens": [281, 360, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.17565000197466682, "compression_ratio": 1.5829145728643217, "no_speech_prob": 4.092870767635759e-06}, {"id": 714, "seek": 367526, "start": 3675.26, "end": 3681.94, "text": " So once you've done it, whether you're on Mac or Linux or Windows it's going to look", "tokens": [407, 1564, 291, 600, 1096, 309, 11, 1968, 291, 434, 322, 5707, 420, 18734, 420, 8591, 309, 311, 516, 281, 574], "temperature": 0.0, "avg_logprob": -0.13630268790505148, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3081732959108194e-06}, {"id": 715, "seek": 367526, "start": 3681.94, "end": 3683.82, "text": " basically the same.", "tokens": [1936, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.13630268790505148, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3081732959108194e-06}, {"id": 716, "seek": 367526, "start": 3683.82, "end": 3691.0200000000004, "text": " And so you'll create your SSH key by following the instructions in the documentation which", "tokens": [400, 370, 291, 603, 1884, 428, 12238, 39, 2141, 538, 3480, 264, 9415, 294, 264, 14333, 597], "temperature": 0.0, "avg_logprob": -0.13630268790505148, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3081732959108194e-06}, {"id": 717, "seek": 367526, "start": 3691.0200000000004, "end": 3702.92, "text": " is basically you run SSH keygen and it's just going to go through and create these two files.", "tokens": [307, 1936, 291, 1190, 12238, 39, 2141, 1766, 293, 309, 311, 445, 516, 281, 352, 807, 293, 1884, 613, 732, 7098, 13], "temperature": 0.0, "avg_logprob": -0.13630268790505148, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3081732959108194e-06}, {"id": 718, "seek": 367526, "start": 3702.92, "end": 3704.92, "text": " So you just run it and it creates these two files.", "tokens": [407, 291, 445, 1190, 309, 293, 309, 7829, 613, 732, 7098, 13], "temperature": 0.0, "avg_logprob": -0.13630268790505148, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3081732959108194e-06}, {"id": 719, "seek": 370492, "start": 3704.92, "end": 3708.28, "text": " And so this is the one that we have to give Amazon and this is the one that we're going", "tokens": [400, 370, 341, 307, 264, 472, 300, 321, 362, 281, 976, 6795, 293, 341, 307, 264, 472, 300, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.1683319348555345, "compression_ratio": 1.3900709219858156, "no_speech_prob": 2.090448106173426e-06}, {"id": 720, "seek": 370492, "start": 3708.28, "end": 3710.7200000000003, "text": " to keep for ourselves.", "tokens": [281, 1066, 337, 4175, 13], "temperature": 0.0, "avg_logprob": -0.1683319348555345, "compression_ratio": 1.3900709219858156, "no_speech_prob": 2.090448106173426e-06}, {"id": 721, "seek": 370492, "start": 3710.7200000000003, "end": 3725.76, "text": " So following along the documentation here it says to click on services, EC2, find key", "tokens": [407, 3480, 2051, 264, 14333, 510, 309, 1619, 281, 2052, 322, 3328, 11, 19081, 17, 11, 915, 2141], "temperature": 0.0, "avg_logprob": -0.1683319348555345, "compression_ratio": 1.3900709219858156, "no_speech_prob": 2.090448106173426e-06}, {"id": 722, "seek": 372576, "start": 3725.76, "end": 3743.7400000000002, "text": " pairs, here, ok, and then we'll go here import key pair and whatever, AWS, and this is where", "tokens": [15494, 11, 510, 11, 3133, 11, 293, 550, 321, 603, 352, 510, 974, 2141, 6119, 293, 2035, 11, 17650, 11, 293, 341, 307, 689], "temperature": 0.0, "avg_logprob": -0.3096701758248465, "compression_ratio": 1.108433734939759, "no_speech_prob": 6.854222647234565e-06}, {"id": 723, "seek": 374374, "start": 3743.74, "end": 3758.3199999999997, "text": " we're going to find the IDRSAPUB that we just created and you can see this here it is right", "tokens": [321, 434, 516, 281, 915, 264, 7348, 49, 8886, 8115, 33, 300, 321, 445, 2942, 293, 291, 393, 536, 341, 510, 309, 307, 558], "temperature": 0.0, "avg_logprob": -0.2298191974037572, "compression_ratio": 1.5277777777777777, "no_speech_prob": 3.989701724549377e-07}, {"id": 724, "seek": 374374, "start": 3758.3199999999997, "end": 3762.8399999999997, "text": " it's just a big long code and it's fine you can all look at this this is public not secret", "tokens": [309, 311, 445, 257, 955, 938, 3089, 293, 309, 311, 2489, 291, 393, 439, 574, 412, 341, 341, 307, 1908, 406, 4054], "temperature": 0.0, "avg_logprob": -0.2298191974037572, "compression_ratio": 1.5277777777777777, "no_speech_prob": 3.989701724549377e-07}, {"id": 725, "seek": 374374, "start": 3762.8399999999997, "end": 3770.3199999999997, "text": " this is the cool thing right there's no passwords and I say import and so now we have an SSH", "tokens": [341, 307, 264, 1627, 551, 558, 456, 311, 572, 33149, 293, 286, 584, 974, 293, 370, 586, 321, 362, 364, 12238, 39], "temperature": 0.0, "avg_logprob": -0.2298191974037572, "compression_ratio": 1.5277777777777777, "no_speech_prob": 3.989701724549377e-07}, {"id": 726, "seek": 377032, "start": 3770.32, "end": 3775.36, "text": " key and we can use that to log in.", "tokens": [2141, 293, 321, 393, 764, 300, 281, 3565, 294, 13], "temperature": 0.0, "avg_logprob": -0.21535383571277966, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.7330418131678016e-06}, {"id": 727, "seek": 377032, "start": 3775.36, "end": 3780.04, "text": " Ok so this is just all this is, is all those steps.", "tokens": [3477, 370, 341, 307, 445, 439, 341, 307, 11, 307, 439, 729, 4439, 13], "temperature": 0.0, "avg_logprob": -0.21535383571277966, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.7330418131678016e-06}, {"id": 728, "seek": 377032, "start": 3780.04, "end": 3787.78, "text": " So renting a server in AWS speak is called launching an instance.", "tokens": [407, 40598, 257, 7154, 294, 17650, 1710, 307, 1219, 18354, 364, 5197, 13], "temperature": 0.0, "avg_logprob": -0.21535383571277966, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.7330418131678016e-06}, {"id": 729, "seek": 377032, "start": 3787.78, "end": 3794.6000000000004, "text": " So to launch an instance we'll scroll back up to the top to instances and we will say", "tokens": [407, 281, 4025, 364, 5197, 321, 603, 11369, 646, 493, 281, 264, 1192, 281, 14519, 293, 321, 486, 584], "temperature": 0.0, "avg_logprob": -0.21535383571277966, "compression_ratio": 1.5454545454545454, "no_speech_prob": 1.7330418131678016e-06}, {"id": 730, "seek": 379460, "start": 3794.6, "end": 3803.08, "text": " launch instance ok and it'll say ok what kind of thing do you want to run Amazon Linux or", "tokens": [4025, 5197, 3133, 293, 309, 603, 584, 3133, 437, 733, 295, 551, 360, 291, 528, 281, 1190, 6795, 18734, 420], "temperature": 0.0, "avg_logprob": -0.24523209552375638, "compression_ratio": 1.6140350877192982, "no_speech_prob": 4.936895493301563e-06}, {"id": 731, "seek": 379460, "start": 3803.08, "end": 3804.68, "text": " Windows or Red Hat or whatever.", "tokens": [8591, 420, 4477, 15867, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.24523209552375638, "compression_ratio": 1.6140350877192982, "no_speech_prob": 4.936895493301563e-06}, {"id": 732, "seek": 379460, "start": 3804.68, "end": 3809.04, "text": " I strongly strongly suggest you use Ubuntu and the latest version which is currently", "tokens": [286, 10613, 10613, 3402, 291, 764, 30230, 45605, 293, 264, 6792, 3037, 597, 307, 4362], "temperature": 0.0, "avg_logprob": -0.24523209552375638, "compression_ratio": 1.6140350877192982, "no_speech_prob": 4.936895493301563e-06}, {"id": 733, "seek": 379460, "start": 3809.04, "end": 3810.04, "text": " 20.", "tokens": [945, 13], "temperature": 0.0, "avg_logprob": -0.24523209552375638, "compression_ratio": 1.6140350877192982, "no_speech_prob": 4.936895493301563e-06}, {"id": 734, "seek": 379460, "start": 3810.04, "end": 3813.92, "text": " So I'm just going to say select.", "tokens": [407, 286, 478, 445, 516, 281, 584, 3048, 13], "temperature": 0.0, "avg_logprob": -0.24523209552375638, "compression_ratio": 1.6140350877192982, "no_speech_prob": 4.936895493301563e-06}, {"id": 735, "seek": 379460, "start": 3813.92, "end": 3818.04, "text": " Ok and then it'll say ok what kind of server do you want.", "tokens": [3477, 293, 550, 309, 603, 584, 3133, 437, 733, 295, 7154, 360, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.24523209552375638, "compression_ratio": 1.6140350877192982, "no_speech_prob": 4.936895493301563e-06}, {"id": 736, "seek": 379460, "start": 3818.04, "end": 3821.4, "text": " For playing around there's actually one that you can get for free.", "tokens": [1171, 2433, 926, 456, 311, 767, 472, 300, 291, 393, 483, 337, 1737, 13], "temperature": 0.0, "avg_logprob": -0.24523209552375638, "compression_ratio": 1.6140350877192982, "no_speech_prob": 4.936895493301563e-06}, {"id": 737, "seek": 382140, "start": 3821.4, "end": 3826.7200000000003, "text": " Now it doesn't do it's pretty it's kind of slow right but for learning about SSH and", "tokens": [823, 309, 1177, 380, 360, 309, 311, 1238, 309, 311, 733, 295, 2964, 558, 457, 337, 2539, 466, 12238, 39, 293], "temperature": 0.0, "avg_logprob": -0.1658078384399414, "compression_ratio": 1.5476190476190477, "no_speech_prob": 7.18315959602478e-06}, {"id": 738, "seek": 382140, "start": 3826.7200000000003, "end": 3830.0, "text": " Linux and stuff this is actually a great one to use.", "tokens": [18734, 293, 1507, 341, 307, 767, 257, 869, 472, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.1658078384399414, "compression_ratio": 1.5476190476190477, "no_speech_prob": 7.18315959602478e-06}, {"id": 739, "seek": 382140, "start": 3830.0, "end": 3837.4, "text": " It's no good for deep learning it doesn't have a GPU so if I go to G4DN that's the cheapest", "tokens": [467, 311, 572, 665, 337, 2452, 2539, 309, 1177, 380, 362, 257, 18407, 370, 498, 286, 352, 281, 460, 19, 35, 45, 300, 311, 264, 29167], "temperature": 0.0, "avg_logprob": -0.1658078384399414, "compression_ratio": 1.5476190476190477, "no_speech_prob": 7.18315959602478e-06}, {"id": 740, "seek": 382140, "start": 3837.4, "end": 3846.0, "text": " kind of good GPUs we can get and I'll get the smallest one there G4DNXL and then I'll", "tokens": [733, 295, 665, 18407, 82, 321, 393, 483, 293, 286, 603, 483, 264, 16998, 472, 456, 460, 19, 35, 45, 55, 43, 293, 550, 286, 603], "temperature": 0.0, "avg_logprob": -0.1658078384399414, "compression_ratio": 1.5476190476190477, "no_speech_prob": 7.18315959602478e-06}, {"id": 741, "seek": 382140, "start": 3846.0, "end": 3848.2400000000002, "text": " say next.", "tokens": [584, 958, 13], "temperature": 0.0, "avg_logprob": -0.1658078384399414, "compression_ratio": 1.5476190476190477, "no_speech_prob": 7.18315959602478e-06}, {"id": 742, "seek": 384824, "start": 3848.24, "end": 3852.3999999999996, "text": " So how big a hard drive do I want.", "tokens": [407, 577, 955, 257, 1152, 3332, 360, 286, 528, 13], "temperature": 0.0, "avg_logprob": -0.21590900421142578, "compression_ratio": 1.5326086956521738, "no_speech_prob": 3.0415465062105795e-06}, {"id": 743, "seek": 384824, "start": 3852.3999999999996, "end": 3857.0, "text": " I'd normally say about 100 gig.", "tokens": [286, 1116, 5646, 584, 466, 2319, 8741, 13], "temperature": 0.0, "avg_logprob": -0.21590900421142578, "compression_ratio": 1.5326086956521738, "no_speech_prob": 3.0415465062105795e-06}, {"id": 744, "seek": 384824, "start": 3857.0, "end": 3865.12, "text": " Launch and launch and so now it's going to say ok when you log into this which key pair", "tokens": [28119, 293, 4025, 293, 370, 586, 309, 311, 516, 281, 584, 3133, 562, 291, 3565, 666, 341, 597, 2141, 6119], "temperature": 0.0, "avg_logprob": -0.21590900421142578, "compression_ratio": 1.5326086956521738, "no_speech_prob": 3.0415465062105795e-06}, {"id": 745, "seek": 384824, "start": 3865.12, "end": 3866.12, "text": " are you going to use.", "tokens": [366, 291, 516, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.21590900421142578, "compression_ratio": 1.5326086956521738, "no_speech_prob": 3.0415465062105795e-06}, {"id": 746, "seek": 384824, "start": 3866.12, "end": 3873.24, "text": " Ok so you just select the one that you just imported and say yep I know that I have that", "tokens": [3477, 370, 291, 445, 3048, 264, 472, 300, 291, 445, 25524, 293, 584, 18633, 286, 458, 300, 286, 362, 300], "temperature": 0.0, "avg_logprob": -0.21590900421142578, "compression_ratio": 1.5326086956521738, "no_speech_prob": 3.0415465062105795e-06}, {"id": 747, "seek": 384824, "start": 3873.24, "end": 3877.9199999999996, "text": " and then launch.", "tokens": [293, 550, 4025, 13], "temperature": 0.0, "avg_logprob": -0.21590900421142578, "compression_ratio": 1.5326086956521738, "no_speech_prob": 3.0415465062105795e-06}, {"id": 748, "seek": 387792, "start": 3877.92, "end": 3883.28, "text": " And you'll see Dell says this has now been initiated and it's got a code so this is the", "tokens": [400, 291, 603, 536, 33319, 1619, 341, 575, 586, 668, 28578, 293, 309, 311, 658, 257, 3089, 370, 341, 307, 264], "temperature": 0.0, "avg_logprob": -0.18660856273076307, "compression_ratio": 1.4555555555555555, "no_speech_prob": 3.39311691277544e-06}, {"id": 749, "seek": 387792, "start": 3883.28, "end": 3884.28, "text": " thing that I've just launched.", "tokens": [551, 300, 286, 600, 445, 8730, 13], "temperature": 0.0, "avg_logprob": -0.18660856273076307, "compression_ratio": 1.4555555555555555, "no_speech_prob": 3.39311691277544e-06}, {"id": 750, "seek": 387792, "start": 3884.28, "end": 3896.12, "text": " If I click on it here it shows me here's my instance.", "tokens": [759, 286, 2052, 322, 309, 510, 309, 3110, 385, 510, 311, 452, 5197, 13], "temperature": 0.0, "avg_logprob": -0.18660856273076307, "compression_ratio": 1.4555555555555555, "no_speech_prob": 3.39311691277544e-06}, {"id": 751, "seek": 387792, "start": 3896.12, "end": 3903.56, "text": " So as you if you haven't done much with servers and Linux and SSH and stuff there's going", "tokens": [407, 382, 291, 498, 291, 2378, 380, 1096, 709, 365, 15909, 293, 18734, 293, 12238, 39, 293, 1507, 456, 311, 516], "temperature": 0.0, "avg_logprob": -0.18660856273076307, "compression_ratio": 1.4555555555555555, "no_speech_prob": 3.39311691277544e-06}, {"id": 752, "seek": 390356, "start": 3903.56, "end": 3908.92, "text": " to be this whole world of new stuff for you to learn about but this is an opportunity", "tokens": [281, 312, 341, 1379, 1002, 295, 777, 1507, 337, 291, 281, 1466, 466, 457, 341, 307, 364, 2650], "temperature": 0.0, "avg_logprob": -0.10046174574871453, "compression_ratio": 1.6900826446280992, "no_speech_prob": 4.637807705876185e-06}, {"id": 753, "seek": 390356, "start": 3908.92, "end": 3913.04, "text": " it's not a problem so if you're not familiar with things like IP addresses that's cool", "tokens": [309, 311, 406, 257, 1154, 370, 498, 291, 434, 406, 4963, 365, 721, 411, 8671, 16862, 300, 311, 1627], "temperature": 0.0, "avg_logprob": -0.10046174574871453, "compression_ratio": 1.6900826446280992, "no_speech_prob": 4.637807705876185e-06}, {"id": 754, "seek": 390356, "start": 3913.04, "end": 3919.64, "text": " there's lots of tutorials around at the moment but for now just know this is the unique address", "tokens": [456, 311, 3195, 295, 17616, 926, 412, 264, 1623, 457, 337, 586, 445, 458, 341, 307, 264, 3845, 2985], "temperature": 0.0, "avg_logprob": -0.10046174574871453, "compression_ratio": 1.6900826446280992, "no_speech_prob": 4.637807705876185e-06}, {"id": 755, "seek": 390356, "start": 3919.64, "end": 3923.7599999999998, "text": " like a street address that your new computer has and so we're going to connect to it.", "tokens": [411, 257, 4838, 2985, 300, 428, 777, 3820, 575, 293, 370, 321, 434, 516, 281, 1745, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.10046174574871453, "compression_ratio": 1.6900826446280992, "no_speech_prob": 4.637807705876185e-06}, {"id": 756, "seek": 390356, "start": 3923.7599999999998, "end": 3928.12, "text": " So this button here will click will copy that address.", "tokens": [407, 341, 2960, 510, 486, 2052, 486, 5055, 300, 2985, 13], "temperature": 0.0, "avg_logprob": -0.10046174574871453, "compression_ratio": 1.6900826446280992, "no_speech_prob": 4.637807705876185e-06}, {"id": 757, "seek": 392812, "start": 3928.12, "end": 3935.68, "text": " So we can then go to our terminal and we can type SSH and paste in the address and then", "tokens": [407, 321, 393, 550, 352, 281, 527, 14709, 293, 321, 393, 2010, 12238, 39, 293, 9163, 294, 264, 2985, 293, 550], "temperature": 0.0, "avg_logprob": -0.11141104380289714, "compression_ratio": 1.6341463414634145, "no_speech_prob": 9.87458861345658e-07}, {"id": 758, "seek": 392812, "start": 3935.68, "end": 3944.14, "text": " the only other thing I do need to do is I need to say provide a username and AWS always", "tokens": [264, 787, 661, 551, 286, 360, 643, 281, 360, 307, 286, 643, 281, 584, 2893, 257, 30351, 293, 17650, 1009], "temperature": 0.0, "avg_logprob": -0.11141104380289714, "compression_ratio": 1.6341463414634145, "no_speech_prob": 9.87458861345658e-07}, {"id": 759, "seek": 392812, "start": 3944.14, "end": 3950.2799999999997, "text": " uses the username Ubuntu for all of its Ubuntu images.", "tokens": [4960, 264, 30351, 30230, 45605, 337, 439, 295, 1080, 30230, 45605, 5267, 13], "temperature": 0.0, "avg_logprob": -0.11141104380289714, "compression_ratio": 1.6341463414634145, "no_speech_prob": 9.87458861345658e-07}, {"id": 760, "seek": 392812, "start": 3950.2799999999997, "end": 3953.48, "text": " So you say Ubuntu at and then the IP.", "tokens": [407, 291, 584, 30230, 45605, 412, 293, 550, 264, 8671, 13], "temperature": 0.0, "avg_logprob": -0.11141104380289714, "compression_ratio": 1.6341463414634145, "no_speech_prob": 9.87458861345658e-07}, {"id": 761, "seek": 395348, "start": 3953.48, "end": 3958.4, "text": " So if I now press enter we're in.", "tokens": [407, 498, 286, 586, 1886, 3242, 321, 434, 294, 13], "temperature": 0.0, "avg_logprob": -0.19218532406553931, "compression_ratio": 1.6553191489361703, "no_speech_prob": 4.092885319550987e-06}, {"id": 762, "seek": 395348, "start": 3958.4, "end": 3965.04, "text": " OK so now everything I type here is actually being typed on that remote computer.", "tokens": [2264, 370, 586, 1203, 286, 2010, 510, 307, 767, 885, 33941, 322, 300, 8607, 3820, 13], "temperature": 0.0, "avg_logprob": -0.19218532406553931, "compression_ratio": 1.6553191489361703, "no_speech_prob": 4.092885319550987e-06}, {"id": 763, "seek": 395348, "start": 3965.04, "end": 3968.8, "text": " So for example to list the contents of a directory I type LS.", "tokens": [407, 337, 1365, 281, 1329, 264, 15768, 295, 257, 21120, 286, 2010, 36657, 13], "temperature": 0.0, "avg_logprob": -0.19218532406553931, "compression_ratio": 1.6553191489361703, "no_speech_prob": 4.092885319550987e-06}, {"id": 764, "seek": 395348, "start": 3968.8, "end": 3973.2, "text": " OK so the thing I'm actually typing into here is bash a bash shell.", "tokens": [2264, 370, 264, 551, 286, 478, 767, 18444, 666, 510, 307, 46183, 257, 46183, 8720, 13], "temperature": 0.0, "avg_logprob": -0.19218532406553931, "compression_ratio": 1.6553191489361703, "no_speech_prob": 4.092885319550987e-06}, {"id": 765, "seek": 395348, "start": 3973.2, "end": 3976.52, "text": " So bash is something other of these things need to be familiar with and you can learn", "tokens": [407, 46183, 307, 746, 661, 295, 613, 721, 643, 281, 312, 4963, 365, 293, 291, 393, 1466], "temperature": 0.0, "avg_logprob": -0.19218532406553931, "compression_ratio": 1.6553191489361703, "no_speech_prob": 4.092885319550987e-06}, {"id": 766, "seek": 395348, "start": 3976.52, "end": 3980.56, "text": " about it in that missing semester MIT course I mentioned.", "tokens": [466, 309, 294, 300, 5361, 11894, 13100, 1164, 286, 2835, 13], "temperature": 0.0, "avg_logprob": -0.19218532406553931, "compression_ratio": 1.6553191489361703, "no_speech_prob": 4.092885319550987e-06}, {"id": 767, "seek": 398056, "start": 3980.56, "end": 3987.36, "text": " You know it takes a few weeks to get somewhat comfortable with bash.", "tokens": [509, 458, 309, 2516, 257, 1326, 3259, 281, 483, 8344, 4619, 365, 46183, 13], "temperature": 0.0, "avg_logprob": -0.15323985250372635, "compression_ratio": 1.6008403361344539, "no_speech_prob": 1.3419599781627767e-05}, {"id": 768, "seek": 398056, "start": 3987.36, "end": 3991.7999999999997, "text": " It's a very different feel to using a GUI if you're more familiar with Explorer or Finder", "tokens": [467, 311, 257, 588, 819, 841, 281, 1228, 257, 17917, 40, 498, 291, 434, 544, 4963, 365, 31895, 420, 479, 5669], "temperature": 0.0, "avg_logprob": -0.15323985250372635, "compression_ratio": 1.6008403361344539, "no_speech_prob": 1.3419599781627767e-05}, {"id": 769, "seek": 398056, "start": 3991.7999999999997, "end": 3995.92, "text": " or whatever but you'll find it's you'll be much more productive soon enough because you", "tokens": [420, 2035, 457, 291, 603, 915, 309, 311, 291, 603, 312, 709, 544, 13304, 2321, 1547, 570, 291], "temperature": 0.0, "avg_logprob": -0.15323985250372635, "compression_ratio": 1.6008403361344539, "no_speech_prob": 1.3419599781627767e-05}, {"id": 770, "seek": 398056, "start": 3995.92, "end": 4001.12, "text": " can replicate things quickly you can script things you can copy and paste things and so", "tokens": [393, 25356, 721, 2661, 291, 393, 5755, 721, 291, 393, 5055, 293, 9163, 721, 293, 370], "temperature": 0.0, "avg_logprob": -0.15323985250372635, "compression_ratio": 1.6008403361344539, "no_speech_prob": 1.3419599781627767e-05}, {"id": 771, "seek": 398056, "start": 4001.12, "end": 4002.12, "text": " forth.", "tokens": [5220, 13], "temperature": 0.0, "avg_logprob": -0.15323985250372635, "compression_ratio": 1.6008403361344539, "no_speech_prob": 1.3419599781627767e-05}, {"id": 772, "seek": 398056, "start": 4002.12, "end": 4009.6, "text": " Anyway so here's my here's my computer.", "tokens": [5684, 370, 510, 311, 452, 510, 311, 452, 3820, 13], "temperature": 0.0, "avg_logprob": -0.15323985250372635, "compression_ratio": 1.6008403361344539, "no_speech_prob": 1.3419599781627767e-05}, {"id": 773, "seek": 400960, "start": 4009.6, "end": 4015.96, "text": " It's going to sit here running until you tell it not to even if you turn your computer off", "tokens": [467, 311, 516, 281, 1394, 510, 2614, 1826, 291, 980, 309, 406, 281, 754, 498, 291, 1261, 428, 3820, 766], "temperature": 0.0, "avg_logprob": -0.1473789317633516, "compression_ratio": 1.785, "no_speech_prob": 1.4144634405965917e-06}, {"id": 774, "seek": 400960, "start": 4015.96, "end": 4019.12, "text": " your server is still running and that means you're still paying for it.", "tokens": [428, 7154, 307, 920, 2614, 293, 300, 1355, 291, 434, 920, 6229, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.1473789317633516, "compression_ratio": 1.785, "no_speech_prob": 1.4144634405965917e-06}, {"id": 775, "seek": 400960, "start": 4019.12, "end": 4024.5, "text": " OK so one of the things I guarantee you're going to learn the hard way by wasting money", "tokens": [2264, 370, 472, 295, 264, 721, 286, 10815, 291, 434, 516, 281, 1466, 264, 1152, 636, 538, 20457, 1460], "temperature": 0.0, "avg_logprob": -0.1473789317633516, "compression_ratio": 1.785, "no_speech_prob": 1.4144634405965917e-06}, {"id": 776, "seek": 400960, "start": 4024.5, "end": 4026.56, "text": " is that you're going to forget to turn it off.", "tokens": [307, 300, 291, 434, 516, 281, 2870, 281, 1261, 309, 766, 13], "temperature": 0.0, "avg_logprob": -0.1473789317633516, "compression_ratio": 1.785, "no_speech_prob": 1.4144634405965917e-06}, {"id": 777, "seek": 400960, "start": 4026.56, "end": 4033.64, "text": " OK so to turn it off you're just going to go stop instance.", "tokens": [2264, 370, 281, 1261, 309, 766, 291, 434, 445, 516, 281, 352, 1590, 5197, 13], "temperature": 0.0, "avg_logprob": -0.1473789317633516, "compression_ratio": 1.785, "no_speech_prob": 1.4144634405965917e-06}, {"id": 778, "seek": 403364, "start": 4033.64, "end": 4040.04, "text": " So you make sure you you do that.", "tokens": [407, 291, 652, 988, 291, 291, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.3475480760846819, "compression_ratio": 1.1818181818181819, "no_speech_prob": 2.3320437776419567e-06}, {"id": 779, "seek": 403364, "start": 4040.04, "end": 4041.16, "text": " Let's see how we're going here.", "tokens": [961, 311, 536, 577, 321, 434, 516, 510, 13], "temperature": 0.0, "avg_logprob": -0.3475480760846819, "compression_ratio": 1.1818181818181819, "no_speech_prob": 2.3320437776419567e-06}, {"id": 780, "seek": 403364, "start": 4041.16, "end": 4047.56, "text": " So we've launched our instance.", "tokens": [407, 321, 600, 8730, 527, 5197, 13], "temperature": 0.0, "avg_logprob": -0.3475480760846819, "compression_ratio": 1.1818181818181819, "no_speech_prob": 2.3320437776419567e-06}, {"id": 781, "seek": 403364, "start": 4047.56, "end": 4052.04, "text": " And we SSH into it.", "tokens": [400, 321, 12238, 39, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.3475480760846819, "compression_ratio": 1.1818181818181819, "no_speech_prob": 2.3320437776419567e-06}, {"id": 782, "seek": 405204, "start": 4052.04, "end": 4066.52, "text": " OK so keeping a Linux server up to date and running used to be kind of annoying but luckily", "tokens": [2264, 370, 5145, 257, 18734, 7154, 493, 281, 4002, 293, 2614, 1143, 281, 312, 733, 295, 11304, 457, 22880], "temperature": 0.0, "avg_logprob": -0.13177241288222274, "compression_ratio": 1.5588235294117647, "no_speech_prob": 2.058030759144458e-06}, {"id": 783, "seek": 405204, "start": 4066.52, "end": 4071.92, "text": " I've created something called fast setup for you which makes it easy.", "tokens": [286, 600, 2942, 746, 1219, 2370, 8657, 337, 291, 597, 1669, 309, 1858, 13], "temperature": 0.0, "avg_logprob": -0.13177241288222274, "compression_ratio": 1.5588235294117647, "no_speech_prob": 2.058030759144458e-06}, {"id": 784, "seek": 405204, "start": 4071.92, "end": 4077.08, "text": " And all you need to do is copy this and paste it into your terminal.", "tokens": [400, 439, 291, 643, 281, 360, 307, 5055, 341, 293, 9163, 309, 666, 428, 14709, 13], "temperature": 0.0, "avg_logprob": -0.13177241288222274, "compression_ratio": 1.5588235294117647, "no_speech_prob": 2.058030759144458e-06}, {"id": 785, "seek": 405204, "start": 4077.08, "end": 4081.82, "text": " And this is one of the really cool things about Linux and using bash is like in Windows", "tokens": [400, 341, 307, 472, 295, 264, 534, 1627, 721, 466, 18734, 293, 1228, 46183, 307, 411, 294, 8591], "temperature": 0.0, "avg_logprob": -0.13177241288222274, "compression_ratio": 1.5588235294117647, "no_speech_prob": 2.058030759144458e-06}, {"id": 786, "seek": 408182, "start": 4081.82, "end": 4088.44, "text": " or with Mac Finder you'd have pages and pages of click this and drag that and scroll here.", "tokens": [420, 365, 5707, 479, 5669, 291, 1116, 362, 7183, 293, 7183, 295, 2052, 341, 293, 5286, 300, 293, 11369, 510, 13], "temperature": 0.0, "avg_logprob": -0.14414894139325177, "compression_ratio": 1.6695278969957081, "no_speech_prob": 2.0580346244969405e-06}, {"id": 787, "seek": 408182, "start": 4088.44, "end": 4089.6000000000004, "text": " But I've just crypted the whole thing.", "tokens": [583, 286, 600, 445, 9844, 292, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.14414894139325177, "compression_ratio": 1.6695278969957081, "no_speech_prob": 2.0580346244969405e-06}, {"id": 788, "seek": 408182, "start": 4089.6000000000004, "end": 4094.88, "text": " So I'm just going to go ahead and paste it over here.", "tokens": [407, 286, 478, 445, 516, 281, 352, 2286, 293, 9163, 309, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.14414894139325177, "compression_ratio": 1.6695278969957081, "no_speech_prob": 2.0580346244969405e-06}, {"id": 789, "seek": 408182, "start": 4094.88, "end": 4095.88, "text": " And it's off.", "tokens": [400, 309, 311, 766, 13], "temperature": 0.0, "avg_logprob": -0.14414894139325177, "compression_ratio": 1.6695278969957081, "no_speech_prob": 2.0580346244969405e-06}, {"id": 790, "seek": 408182, "start": 4095.88, "end": 4101.4400000000005, "text": " OK now what this is going to do is it's going to fully set up this Linux server.", "tokens": [2264, 586, 437, 341, 307, 516, 281, 360, 307, 309, 311, 516, 281, 4498, 992, 493, 341, 18734, 7154, 13], "temperature": 0.0, "avg_logprob": -0.14414894139325177, "compression_ratio": 1.6695278969957081, "no_speech_prob": 2.0580346244969405e-06}, {"id": 791, "seek": 408182, "start": 4101.4400000000005, "end": 4105.04, "text": " It's going to make it automatically update with the latest software.", "tokens": [467, 311, 516, 281, 652, 309, 6772, 5623, 365, 264, 6792, 4722, 13], "temperature": 0.0, "avg_logprob": -0.14414894139325177, "compression_ratio": 1.6695278969957081, "no_speech_prob": 2.0580346244969405e-06}, {"id": 792, "seek": 408182, "start": 4105.04, "end": 4111.08, "text": " It's going to configure it all correctly.", "tokens": [467, 311, 516, 281, 22162, 309, 439, 8944, 13], "temperature": 0.0, "avg_logprob": -0.14414894139325177, "compression_ratio": 1.6695278969957081, "no_speech_prob": 2.0580346244969405e-06}, {"id": 793, "seek": 411108, "start": 4111.08, "end": 4112.08, "text": " And so forth.", "tokens": [400, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.17753411877539851, "compression_ratio": 1.786610878661088, "no_speech_prob": 5.862705620529596e-06}, {"id": 794, "seek": 411108, "start": 4112.08, "end": 4114.04, "text": " And it's going to ask a minimum number of questions.", "tokens": [400, 309, 311, 516, 281, 1029, 257, 7285, 1230, 295, 1651, 13], "temperature": 0.0, "avg_logprob": -0.17753411877539851, "compression_ratio": 1.786610878661088, "no_speech_prob": 5.862705620529596e-06}, {"id": 795, "seek": 411108, "start": 4114.04, "end": 4116.24, "text": " So I'm just going to show you the questions it's going to ask you.", "tokens": [407, 286, 478, 445, 516, 281, 855, 291, 264, 1651, 309, 311, 516, 281, 1029, 291, 13], "temperature": 0.0, "avg_logprob": -0.17753411877539851, "compression_ratio": 1.786610878661088, "no_speech_prob": 5.862705620529596e-06}, {"id": 796, "seek": 411108, "start": 4116.24, "end": 4118.44, "text": " It's going to ask for a hostname.", "tokens": [467, 311, 516, 281, 1029, 337, 257, 3975, 16344, 13], "temperature": 0.0, "avg_logprob": -0.17753411877539851, "compression_ratio": 1.786610878661088, "no_speech_prob": 5.862705620529596e-06}, {"id": 797, "seek": 411108, "start": 4118.44, "end": 4122.64, "text": " So a hostname is just a more convenient way to access a server.", "tokens": [407, 257, 3975, 16344, 307, 445, 257, 544, 10851, 636, 281, 2105, 257, 7154, 13], "temperature": 0.0, "avg_logprob": -0.17753411877539851, "compression_ratio": 1.786610878661088, "no_speech_prob": 5.862705620529596e-06}, {"id": 798, "seek": 411108, "start": 4122.64, "end": 4125.76, "text": " And so you can basically write anything you like as long as it's got at least two dots", "tokens": [400, 370, 291, 393, 1936, 2464, 1340, 291, 411, 382, 938, 382, 309, 311, 658, 412, 1935, 732, 15026], "temperature": 0.0, "avg_logprob": -0.17753411877539851, "compression_ratio": 1.786610878661088, "no_speech_prob": 5.862705620529596e-06}, {"id": 799, "seek": 411108, "start": 4125.76, "end": 4127.14, "text": " in it.", "tokens": [294, 309, 13], "temperature": 0.0, "avg_logprob": -0.17753411877539851, "compression_ratio": 1.786610878661088, "no_speech_prob": 5.862705620529596e-06}, {"id": 800, "seek": 411108, "start": 4127.14, "end": 4133.32, "text": " So I'm going to call this course test dot fast dot AI example.", "tokens": [407, 286, 478, 516, 281, 818, 341, 1164, 1500, 5893, 2370, 5893, 7318, 1365, 13], "temperature": 0.0, "avg_logprob": -0.17753411877539851, "compression_ratio": 1.786610878661088, "no_speech_prob": 5.862705620529596e-06}, {"id": 801, "seek": 411108, "start": 4133.32, "end": 4136.5199999999995, "text": " OK and then asks for an email address.", "tokens": [2264, 293, 550, 8962, 337, 364, 3796, 2985, 13], "temperature": 0.0, "avg_logprob": -0.17753411877539851, "compression_ratio": 1.786610878661088, "no_speech_prob": 5.862705620529596e-06}, {"id": 802, "seek": 413652, "start": 4136.52, "end": 4142.52, "text": " Now the email address is basically just goes what is where it's going to send kind of error", "tokens": [823, 264, 3796, 2985, 307, 1936, 445, 1709, 437, 307, 689, 309, 311, 516, 281, 2845, 733, 295, 6713], "temperature": 0.0, "avg_logprob": -0.27299204560899243, "compression_ratio": 1.511737089201878, "no_speech_prob": 6.179371894177166e-07}, {"id": 803, "seek": 413652, "start": 4142.52, "end": 4144.46, "text": " locks and stuff too.", "tokens": [20703, 293, 1507, 886, 13], "temperature": 0.0, "avg_logprob": -0.27299204560899243, "compression_ratio": 1.511737089201878, "no_speech_prob": 6.179371894177166e-07}, {"id": 804, "seek": 413652, "start": 4144.46, "end": 4148.96, "text": " So maybe we'll say info at fast dot AI.", "tokens": [407, 1310, 321, 603, 584, 13614, 412, 2370, 5893, 7318, 13], "temperature": 0.0, "avg_logprob": -0.27299204560899243, "compression_ratio": 1.511737089201878, "no_speech_prob": 6.179371894177166e-07}, {"id": 805, "seek": 413652, "start": 4148.96, "end": 4151.64, "text": " OK do you want to set a password.", "tokens": [2264, 360, 291, 528, 281, 992, 257, 11524, 13], "temperature": 0.0, "avg_logprob": -0.27299204560899243, "compression_ratio": 1.511737089201878, "no_speech_prob": 6.179371894177166e-07}, {"id": 806, "seek": 413652, "start": 4151.64, "end": 4152.64, "text": " Probably do.", "tokens": [9210, 360, 13], "temperature": 0.0, "avg_logprob": -0.27299204560899243, "compression_ratio": 1.511737089201878, "no_speech_prob": 6.179371894177166e-07}, {"id": 807, "seek": 413652, "start": 4152.64, "end": 4153.64, "text": " So hit enter for yes.", "tokens": [407, 2045, 3242, 337, 2086, 13], "temperature": 0.0, "avg_logprob": -0.27299204560899243, "compression_ratio": 1.511737089201878, "no_speech_prob": 6.179371894177166e-07}, {"id": 808, "seek": 413652, "start": 4153.64, "end": 4157.52, "text": " So I'm going to put in a password.", "tokens": [407, 286, 478, 516, 281, 829, 294, 257, 11524, 13], "temperature": 0.0, "avg_logprob": -0.27299204560899243, "compression_ratio": 1.511737089201878, "no_speech_prob": 6.179371894177166e-07}, {"id": 809, "seek": 413652, "start": 4157.52, "end": 4161.76, "text": " Asks you to type it again.", "tokens": [1018, 1694, 291, 281, 2010, 309, 797, 13], "temperature": 0.0, "avg_logprob": -0.27299204560899243, "compression_ratio": 1.511737089201878, "no_speech_prob": 6.179371894177166e-07}, {"id": 810, "seek": 413652, "start": 4161.76, "end": 4164.96, "text": " OK reboot automatically when required.", "tokens": [2264, 33818, 6772, 562, 4739, 13], "temperature": 0.0, "avg_logprob": -0.27299204560899243, "compression_ratio": 1.511737089201878, "no_speech_prob": 6.179371894177166e-07}, {"id": 811, "seek": 416496, "start": 4164.96, "end": 4167.56, "text": " I'll say yes.", "tokens": [286, 603, 584, 2086, 13], "temperature": 0.0, "avg_logprob": -0.1413132938337915, "compression_ratio": 1.515, "no_speech_prob": 1.3081682936899597e-06}, {"id": 812, "seek": 416496, "start": 4167.56, "end": 4168.56, "text": " And that's it.", "tokens": [400, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.1413132938337915, "compression_ratio": 1.515, "no_speech_prob": 1.3081682936899597e-06}, {"id": 813, "seek": 416496, "start": 4168.56, "end": 4171.46, "text": " OK so that's all the information that it needed.", "tokens": [2264, 370, 300, 311, 439, 264, 1589, 300, 309, 2978, 13], "temperature": 0.0, "avg_logprob": -0.1413132938337915, "compression_ratio": 1.515, "no_speech_prob": 1.3081682936899597e-06}, {"id": 814, "seek": 416496, "start": 4171.46, "end": 4186.16, "text": " So behind the scenes what's actually happening here is it's grabbed the latest Git repo from", "tokens": [407, 2261, 264, 8026, 437, 311, 767, 2737, 510, 307, 309, 311, 18607, 264, 6792, 16939, 49040, 490], "temperature": 0.0, "avg_logprob": -0.1413132938337915, "compression_ratio": 1.515, "no_speech_prob": 1.3081682936899597e-06}, {"id": 815, "seek": 416496, "start": 4186.16, "end": 4190.68, "text": " fast setup and it's running this thing called Ubuntu initial.", "tokens": [2370, 8657, 293, 309, 311, 2614, 341, 551, 1219, 30230, 45605, 5883, 13], "temperature": 0.0, "avg_logprob": -0.1413132938337915, "compression_ratio": 1.515, "no_speech_prob": 1.3081682936899597e-06}, {"id": 816, "seek": 416496, "start": 4190.68, "end": 4193.44, "text": " And you know this is something you can check out if you're interested.", "tokens": [400, 291, 458, 341, 307, 746, 291, 393, 1520, 484, 498, 291, 434, 3102, 13], "temperature": 0.0, "avg_logprob": -0.1413132938337915, "compression_ratio": 1.515, "no_speech_prob": 1.3081682936899597e-06}, {"id": 817, "seek": 419344, "start": 4193.44, "end": 4202.5199999999995, "text": " It's basically 125 lines of bash script which is going to set up your firewall for you set", "tokens": [467, 311, 1936, 25276, 3876, 295, 46183, 5755, 597, 307, 516, 281, 992, 493, 428, 36109, 337, 291, 992], "temperature": 0.0, "avg_logprob": -0.1187531514601274, "compression_ratio": 1.8633879781420766, "no_speech_prob": 5.955060714768479e-06}, {"id": 818, "seek": 419344, "start": 4202.5199999999995, "end": 4208.639999999999, "text": " up SSH security for you set up your swap file for you set up your SSH configuration for", "tokens": [493, 12238, 39, 3825, 337, 291, 992, 493, 428, 18135, 3991, 337, 291, 992, 493, 428, 12238, 39, 11694, 337], "temperature": 0.0, "avg_logprob": -0.1187531514601274, "compression_ratio": 1.8633879781420766, "no_speech_prob": 5.955060714768479e-06}, {"id": 819, "seek": 419344, "start": 4208.639999999999, "end": 4216.679999999999, "text": " you install all the software you need for you set up your logging and upgrades for you", "tokens": [291, 3625, 439, 264, 4722, 291, 643, 337, 291, 992, 493, 428, 27991, 293, 24868, 337, 291], "temperature": 0.0, "avg_logprob": -0.1187531514601274, "compression_ratio": 1.8633879781420766, "no_speech_prob": 5.955060714768479e-06}, {"id": 820, "seek": 419344, "start": 4216.679999999999, "end": 4219.099999999999, "text": " set up your password and hostname for you.", "tokens": [992, 493, 428, 11524, 293, 3975, 16344, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.1187531514601274, "compression_ratio": 1.8633879781420766, "no_speech_prob": 5.955060714768479e-06}, {"id": 821, "seek": 419344, "start": 4219.099999999999, "end": 4223.24, "text": " OK so it's going to do all that.", "tokens": [2264, 370, 309, 311, 516, 281, 360, 439, 300, 13], "temperature": 0.0, "avg_logprob": -0.1187531514601274, "compression_ratio": 1.8633879781420766, "no_speech_prob": 5.955060714768479e-06}, {"id": 822, "seek": 422324, "start": 4223.24, "end": 4226.76, "text": " And you know this is the kind of thing that if you know from time to time you can just", "tokens": [400, 291, 458, 341, 307, 264, 733, 295, 551, 300, 498, 291, 458, 490, 565, 281, 565, 291, 393, 445], "temperature": 0.0, "avg_logprob": -0.23000657642986758, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.2218856682011392e-05}, {"id": 823, "seek": 422324, "start": 4226.76, "end": 4230.2, "text": " might think oh I'm interested in how X works.", "tokens": [1062, 519, 1954, 286, 478, 3102, 294, 577, 1783, 1985, 13], "temperature": 0.0, "avg_logprob": -0.23000657642986758, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.2218856682011392e-05}, {"id": 824, "seek": 422324, "start": 4230.2, "end": 4233.679999999999, "text": " And since everything is open source you can just go in and see how X works.", "tokens": [400, 1670, 1203, 307, 1269, 4009, 291, 393, 445, 352, 294, 293, 536, 577, 1783, 1985, 13], "temperature": 0.0, "avg_logprob": -0.23000657642986758, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.2218856682011392e-05}, {"id": 825, "seek": 422324, "start": 4233.679999999999, "end": 4237.08, "text": " And at first none of this might make any sense.", "tokens": [400, 412, 700, 6022, 295, 341, 1062, 652, 604, 2020, 13], "temperature": 0.0, "avg_logprob": -0.23000657642986758, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.2218856682011392e-05}, {"id": 826, "seek": 422324, "start": 4237.08, "end": 4240.2, "text": " And so you go oh all right let's pick something and learn about it.", "tokens": [400, 370, 291, 352, 1954, 439, 558, 718, 311, 1888, 746, 293, 1466, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.23000657642986758, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.2218856682011392e-05}, {"id": 827, "seek": 422324, "start": 4240.2, "end": 4241.2, "text": " Enable firewall.", "tokens": [2193, 712, 36109, 13], "temperature": 0.0, "avg_logprob": -0.23000657642986758, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.2218856682011392e-05}, {"id": 828, "seek": 422324, "start": 4241.2, "end": 4242.2, "text": " UFW.", "tokens": [624, 37, 54, 13], "temperature": 0.0, "avg_logprob": -0.23000657642986758, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.2218856682011392e-05}, {"id": 829, "seek": 422324, "start": 4242.2, "end": 4244.04, "text": " No what's UFW.", "tokens": [883, 437, 311, 624, 37, 54, 13], "temperature": 0.0, "avg_logprob": -0.23000657642986758, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.2218856682011392e-05}, {"id": 830, "seek": 422324, "start": 4244.04, "end": 4246.04, "text": " Copy paste.", "tokens": [25653, 9163, 13], "temperature": 0.0, "avg_logprob": -0.23000657642986758, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.2218856682011392e-05}, {"id": 831, "seek": 422324, "start": 4246.04, "end": 4248.12, "text": " UFW.", "tokens": [624, 37, 54, 13], "temperature": 0.0, "avg_logprob": -0.23000657642986758, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.2218856682011392e-05}, {"id": 832, "seek": 422324, "start": 4248.12, "end": 4250.599999999999, "text": " Probably not United Farm Workers.", "tokens": [9210, 406, 2824, 19991, 42375, 13], "temperature": 0.0, "avg_logprob": -0.23000657642986758, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.2218856682011392e-05}, {"id": 833, "seek": 422324, "start": 4250.599999999999, "end": 4251.599999999999, "text": " Uncomplicated firewall.", "tokens": [1156, 43856, 3587, 36109, 13], "temperature": 0.0, "avg_logprob": -0.23000657642986758, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.2218856682011392e-05}, {"id": 834, "seek": 422324, "start": 4251.599999999999, "end": 4253.0, "text": " Did Jeremy mention firewall.", "tokens": [2589, 17809, 2152, 36109, 13], "temperature": 0.0, "avg_logprob": -0.23000657642986758, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.2218856682011392e-05}, {"id": 835, "seek": 425300, "start": 4253.0, "end": 4254.6, "text": " OK what the hell is a firewall.", "tokens": [2264, 437, 264, 4921, 307, 257, 36109, 13], "temperature": 0.0, "avg_logprob": -0.23669828827848138, "compression_ratio": 1.6318181818181818, "no_speech_prob": 3.500825869195978e-06}, {"id": 836, "seek": 425300, "start": 4254.6, "end": 4258.04, "text": " And you know you could start reading right.", "tokens": [400, 291, 458, 291, 727, 722, 3760, 558, 13], "temperature": 0.0, "avg_logprob": -0.23669828827848138, "compression_ratio": 1.6318181818181818, "no_speech_prob": 3.500825869195978e-06}, {"id": 837, "seek": 425300, "start": 4258.04, "end": 4262.88, "text": " And then you could be like oh maybe firewall tutorial.", "tokens": [400, 550, 291, 727, 312, 411, 1954, 1310, 36109, 7073, 13], "temperature": 0.0, "avg_logprob": -0.23669828827848138, "compression_ratio": 1.6318181818181818, "no_speech_prob": 3.500825869195978e-06}, {"id": 838, "seek": 425300, "start": 4262.88, "end": 4266.32, "text": " Often adding tutorial can be helpful.", "tokens": [20043, 5127, 7073, 393, 312, 4961, 13], "temperature": 0.0, "avg_logprob": -0.23669828827848138, "compression_ratio": 1.6318181818181818, "no_speech_prob": 3.500825869195978e-06}, {"id": 839, "seek": 425300, "start": 4266.32, "end": 4270.04, "text": " OK so you know you can start to just jump in here and there.", "tokens": [2264, 370, 291, 458, 291, 393, 722, 281, 445, 3012, 294, 510, 293, 456, 13], "temperature": 0.0, "avg_logprob": -0.23669828827848138, "compression_ratio": 1.6318181818181818, "no_speech_prob": 3.500825869195978e-06}, {"id": 840, "seek": 425300, "start": 4270.04, "end": 4271.12, "text": " OK don't get too distracted.", "tokens": [2264, 500, 380, 483, 886, 21658, 13], "temperature": 0.0, "avg_logprob": -0.23669828827848138, "compression_ratio": 1.6318181818181818, "no_speech_prob": 3.500825869195978e-06}, {"id": 841, "seek": 425300, "start": 4271.12, "end": 4274.64, "text": " We want to spend as much time as possible training models.", "tokens": [492, 528, 281, 3496, 382, 709, 565, 382, 1944, 3097, 5245, 13], "temperature": 0.0, "avg_logprob": -0.23669828827848138, "compression_ratio": 1.6318181818181818, "no_speech_prob": 3.500825869195978e-06}, {"id": 842, "seek": 425300, "start": 4274.64, "end": 4278.96, "text": " But this is how we learn about our tools.", "tokens": [583, 341, 307, 577, 321, 1466, 466, 527, 3873, 13], "temperature": 0.0, "avg_logprob": -0.23669828827848138, "compression_ratio": 1.6318181818181818, "no_speech_prob": 3.500825869195978e-06}, {"id": 843, "seek": 427896, "start": 4278.96, "end": 4285.84, "text": " OK so this is now going and downloading the latest version of all the software that it's", "tokens": [2264, 370, 341, 307, 586, 516, 293, 32529, 264, 6792, 3037, 295, 439, 264, 4722, 300, 309, 311], "temperature": 0.0, "avg_logprob": -0.13494882275981288, "compression_ratio": 1.5551020408163265, "no_speech_prob": 1.04513528640382e-05}, {"id": 844, "seek": 427896, "start": 4285.84, "end": 4286.84, "text": " going to need from Linux.", "tokens": [516, 281, 643, 490, 18734, 13], "temperature": 0.0, "avg_logprob": -0.13494882275981288, "compression_ratio": 1.5551020408163265, "no_speech_prob": 1.04513528640382e-05}, {"id": 845, "seek": 427896, "start": 4286.84, "end": 4291.64, "text": " So maybe good time for questions if we have any Michael.", "tokens": [407, 1310, 665, 565, 337, 1651, 498, 321, 362, 604, 5116, 13], "temperature": 0.0, "avg_logprob": -0.13494882275981288, "compression_ratio": 1.5551020408163265, "no_speech_prob": 1.04513528640382e-05}, {"id": 846, "seek": 427896, "start": 4291.64, "end": 4295.92, "text": " What's your current opinion regarding Swift and Julia as replacements of Python.", "tokens": [708, 311, 428, 2190, 4800, 8595, 25539, 293, 18551, 382, 3248, 41140, 295, 15329, 13], "temperature": 0.0, "avg_logprob": -0.13494882275981288, "compression_ratio": 1.5551020408163265, "no_speech_prob": 1.04513528640382e-05}, {"id": 847, "seek": 427896, "start": 4295.92, "end": 4298.84, "text": " So Swift is basically out now.", "tokens": [407, 25539, 307, 1936, 484, 586, 13], "temperature": 0.0, "avg_logprob": -0.13494882275981288, "compression_ratio": 1.5551020408163265, "no_speech_prob": 1.04513528640382e-05}, {"id": 848, "seek": 427896, "start": 4298.84, "end": 4305.0, "text": " So Google has basically archived the Swift for TensorFlow project.", "tokens": [407, 3329, 575, 1936, 3912, 3194, 264, 25539, 337, 37624, 1716, 13], "temperature": 0.0, "avg_logprob": -0.13494882275981288, "compression_ratio": 1.5551020408163265, "no_speech_prob": 1.04513528640382e-05}, {"id": 849, "seek": 427896, "start": 4305.0, "end": 4308.08, "text": " So you can safely ignore that.", "tokens": [407, 291, 393, 11750, 11200, 300, 13], "temperature": 0.0, "avg_logprob": -0.13494882275981288, "compression_ratio": 1.5551020408163265, "no_speech_prob": 1.04513528640382e-05}, {"id": 850, "seek": 430808, "start": 4308.08, "end": 4311.6, "text": " Yeah Julia is.", "tokens": [865, 18551, 307, 13], "temperature": 0.0, "avg_logprob": -0.14417348437839084, "compression_ratio": 1.6262626262626263, "no_speech_prob": 8.013310434762388e-06}, {"id": 851, "seek": 430808, "start": 4311.6, "end": 4312.6, "text": " Julia is interesting.", "tokens": [18551, 307, 1880, 13], "temperature": 0.0, "avg_logprob": -0.14417348437839084, "compression_ratio": 1.6262626262626263, "no_speech_prob": 8.013310434762388e-06}, {"id": 852, "seek": 430808, "start": 4312.6, "end": 4316.76, "text": " You know I think it's a lovely language.", "tokens": [509, 458, 286, 519, 309, 311, 257, 7496, 2856, 13], "temperature": 0.0, "avg_logprob": -0.14417348437839084, "compression_ratio": 1.6262626262626263, "no_speech_prob": 8.013310434762388e-06}, {"id": 853, "seek": 430808, "start": 4316.76, "end": 4320.6, "text": " Nothing has the ecosystem that Python does.", "tokens": [6693, 575, 264, 11311, 300, 15329, 775, 13], "temperature": 0.0, "avg_logprob": -0.14417348437839084, "compression_ratio": 1.6262626262626263, "no_speech_prob": 8.013310434762388e-06}, {"id": 854, "seek": 430808, "start": 4320.6, "end": 4327.8, "text": " So you know if you use Julia you're going to have to figure out a lot more stuff on", "tokens": [407, 291, 458, 498, 291, 764, 18551, 291, 434, 516, 281, 362, 281, 2573, 484, 257, 688, 544, 1507, 322], "temperature": 0.0, "avg_logprob": -0.14417348437839084, "compression_ratio": 1.6262626262626263, "no_speech_prob": 8.013310434762388e-06}, {"id": 855, "seek": 430808, "start": 4327.8, "end": 4331.88, "text": " your own and you'll find a lot more hard edges.", "tokens": [428, 1065, 293, 291, 603, 915, 257, 688, 544, 1152, 8819, 13], "temperature": 0.0, "avg_logprob": -0.14417348437839084, "compression_ratio": 1.6262626262626263, "no_speech_prob": 8.013310434762388e-06}, {"id": 856, "seek": 430808, "start": 4331.88, "end": 4334.72, "text": " But I do think at some point Python is going to have to be replaced.", "tokens": [583, 286, 360, 519, 412, 512, 935, 15329, 307, 516, 281, 362, 281, 312, 10772, 13], "temperature": 0.0, "avg_logprob": -0.14417348437839084, "compression_ratio": 1.6262626262626263, "no_speech_prob": 8.013310434762388e-06}, {"id": 857, "seek": 433472, "start": 4334.72, "end": 4340.52, "text": " And Julia seems like one of if not the most likely thing to replace it.", "tokens": [400, 18551, 2544, 411, 472, 295, 498, 406, 264, 881, 3700, 551, 281, 7406, 309, 13], "temperature": 0.0, "avg_logprob": -0.15907635688781738, "compression_ratio": 1.5822784810126582, "no_speech_prob": 4.4950506890018005e-06}, {"id": 858, "seek": 433472, "start": 4340.52, "end": 4345.400000000001, "text": " Or maybe it won't be replaced by Julia maybe it'll be replaced by something else that's", "tokens": [1610, 1310, 309, 1582, 380, 312, 10772, 538, 18551, 1310, 309, 603, 312, 10772, 538, 746, 1646, 300, 311], "temperature": 0.0, "avg_logprob": -0.15907635688781738, "compression_ratio": 1.5822784810126582, "no_speech_prob": 4.4950506890018005e-06}, {"id": 859, "seek": 433472, "start": 4345.400000000001, "end": 4351.92, "text": " kind of Python like Jax which actually takes Python and compiles it using something called", "tokens": [733, 295, 15329, 411, 508, 2797, 597, 767, 2516, 15329, 293, 715, 4680, 309, 1228, 746, 1219], "temperature": 0.0, "avg_logprob": -0.15907635688781738, "compression_ratio": 1.5822784810126582, "no_speech_prob": 4.4950506890018005e-06}, {"id": 860, "seek": 435192, "start": 4351.92, "end": 4366.52, "text": " XLA into a much faster thing than Python otherwise would be.", "tokens": [1783, 11435, 666, 257, 709, 4663, 551, 813, 15329, 5911, 576, 312, 13], "temperature": 0.0, "avg_logprob": -0.11041798736109878, "compression_ratio": 1.4824120603015076, "no_speech_prob": 2.090447651426075e-06}, {"id": 861, "seek": 435192, "start": 4366.52, "end": 4373.24, "text": " OK do you think that deep learning or more traditional ML or stats approaches are more", "tokens": [2264, 360, 291, 519, 300, 2452, 2539, 420, 544, 5164, 21601, 420, 18152, 11587, 366, 544], "temperature": 0.0, "avg_logprob": -0.11041798736109878, "compression_ratio": 1.4824120603015076, "no_speech_prob": 2.090447651426075e-06}, {"id": 862, "seek": 435192, "start": 4373.24, "end": 4377.0, "text": " useful for traditional industry applications right now.", "tokens": [4420, 337, 5164, 3518, 5821, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.11041798736109878, "compression_ratio": 1.4824120603015076, "no_speech_prob": 2.090447651426075e-06}, {"id": 863, "seek": 435192, "start": 4377.0, "end": 4381.88, "text": " So before I answer that question I'm going to press Y which is going to reboot our computer", "tokens": [407, 949, 286, 1867, 300, 1168, 286, 478, 516, 281, 1886, 398, 597, 307, 516, 281, 33818, 527, 3820], "temperature": 0.0, "avg_logprob": -0.11041798736109878, "compression_ratio": 1.4824120603015076, "no_speech_prob": 2.090447651426075e-06}, {"id": 864, "seek": 438188, "start": 4381.88, "end": 4384.32, "text": " now that it's all updated.", "tokens": [586, 300, 309, 311, 439, 10588, 13], "temperature": 0.0, "avg_logprob": -0.15726783162071592, "compression_ratio": 1.6390243902439023, "no_speech_prob": 5.422142294264631e-06}, {"id": 865, "seek": 438188, "start": 4384.32, "end": 4392.52, "text": " And obviously when we reboot that computer running at the AWS data center it closes the", "tokens": [400, 2745, 562, 321, 33818, 300, 3820, 2614, 412, 264, 17650, 1412, 3056, 309, 24157, 264], "temperature": 0.0, "avg_logprob": -0.15726783162071592, "compression_ratio": 1.6390243902439023, "no_speech_prob": 5.422142294264631e-06}, {"id": 866, "seek": 438188, "start": 4392.52, "end": 4394.28, "text": " connection because it's busy rebooting.", "tokens": [4984, 570, 309, 311, 5856, 26802, 17001, 13], "temperature": 0.0, "avg_logprob": -0.15726783162071592, "compression_ratio": 1.6390243902439023, "no_speech_prob": 5.422142294264631e-06}, {"id": 867, "seek": 438188, "start": 4394.28, "end": 4398.84, "text": " So we'll give it a couple of minutes.", "tokens": [407, 321, 603, 976, 309, 257, 1916, 295, 2077, 13], "temperature": 0.0, "avg_logprob": -0.15726783162071592, "compression_ratio": 1.6390243902439023, "no_speech_prob": 5.422142294264631e-06}, {"id": 868, "seek": 438188, "start": 4398.84, "end": 4404.32, "text": " There's not a single good answer to that question.", "tokens": [821, 311, 406, 257, 2167, 665, 1867, 281, 300, 1168, 13], "temperature": 0.0, "avg_logprob": -0.15726783162071592, "compression_ratio": 1.6390243902439023, "no_speech_prob": 5.422142294264631e-06}, {"id": 869, "seek": 438188, "start": 4404.32, "end": 4410.16, "text": " And you don't really need to answer that question because basically any time you want to try", "tokens": [400, 291, 500, 380, 534, 643, 281, 1867, 300, 1168, 570, 1936, 604, 565, 291, 528, 281, 853], "temperature": 0.0, "avg_logprob": -0.15726783162071592, "compression_ratio": 1.6390243902439023, "no_speech_prob": 5.422142294264631e-06}, {"id": 870, "seek": 441016, "start": 4410.16, "end": 4418.76, "text": " any kind of machine learning model on a problem you should try a few different algorithms", "tokens": [604, 733, 295, 3479, 2539, 2316, 322, 257, 1154, 291, 820, 853, 257, 1326, 819, 14642], "temperature": 0.0, "avg_logprob": -0.12415065303925545, "compression_ratio": 1.6257309941520468, "no_speech_prob": 2.8572087558131898e-06}, {"id": 871, "seek": 441016, "start": 4418.76, "end": 4423.16, "text": " and switching from a random forest to a gradient boosting machine to logistic regression to", "tokens": [293, 16493, 490, 257, 4974, 6719, 281, 257, 16235, 43117, 3479, 281, 3565, 3142, 24590, 281], "temperature": 0.0, "avg_logprob": -0.12415065303925545, "compression_ratio": 1.6257309941520468, "no_speech_prob": 2.8572087558131898e-06}, {"id": 872, "seek": 441016, "start": 4423.16, "end": 4427.12, "text": " deep learning is you know an extra half hour.", "tokens": [2452, 2539, 307, 291, 458, 364, 2857, 1922, 1773, 13], "temperature": 0.0, "avg_logprob": -0.12415065303925545, "compression_ratio": 1.6257309941520468, "no_speech_prob": 2.8572087558131898e-06}, {"id": 873, "seek": 441016, "start": 4427.12, "end": 4433.2, "text": " So you should just try a few different approaches.", "tokens": [407, 291, 820, 445, 853, 257, 1326, 819, 11587, 13], "temperature": 0.0, "avg_logprob": -0.12415065303925545, "compression_ratio": 1.6257309941520468, "no_speech_prob": 2.8572087558131898e-06}, {"id": 874, "seek": 443320, "start": 4433.2, "end": 4440.96, "text": " I find personally for me deep learning is increasingly turning out to be the easiest", "tokens": [286, 915, 5665, 337, 385, 2452, 2539, 307, 12980, 6246, 484, 281, 312, 264, 12889], "temperature": 0.0, "avg_logprob": -0.13812330292492378, "compression_ratio": 1.6043478260869566, "no_speech_prob": 3.4464947020751424e-06}, {"id": 875, "seek": 443320, "start": 4440.96, "end": 4447.4, "text": " thing to get started with and gives me the best results for most projects I seem to do", "tokens": [551, 281, 483, 1409, 365, 293, 2709, 385, 264, 1151, 3542, 337, 881, 4455, 286, 1643, 281, 360], "temperature": 0.0, "avg_logprob": -0.13812330292492378, "compression_ratio": 1.6043478260869566, "no_speech_prob": 3.4464947020751424e-06}, {"id": 876, "seek": 443320, "start": 4447.4, "end": 4448.4, "text": " nowadays.", "tokens": [13434, 13], "temperature": 0.0, "avg_logprob": -0.13812330292492378, "compression_ratio": 1.6043478260869566, "no_speech_prob": 3.4464947020751424e-06}, {"id": 877, "seek": 443320, "start": 4448.4, "end": 4454.32, "text": " But you know have a look at like Kaggle competitions from time to time there are still things where", "tokens": [583, 291, 458, 362, 257, 574, 412, 411, 48751, 22631, 26185, 490, 565, 281, 565, 456, 366, 920, 721, 689], "temperature": 0.0, "avg_logprob": -0.13812330292492378, "compression_ratio": 1.6043478260869566, "no_speech_prob": 3.4464947020751424e-06}, {"id": 878, "seek": 443320, "start": 4454.32, "end": 4459.92, "text": " gradient boosting machines work better or very often people use both and ensemble them.", "tokens": [16235, 43117, 8379, 589, 1101, 420, 588, 2049, 561, 764, 1293, 293, 19492, 552, 13], "temperature": 0.0, "avg_logprob": -0.13812330292492378, "compression_ratio": 1.6043478260869566, "no_speech_prob": 3.4464947020751424e-06}, {"id": 879, "seek": 445992, "start": 4459.92, "end": 4465.16, "text": " But yeah it's not a question that you actually need to answer.", "tokens": [583, 1338, 309, 311, 406, 257, 1168, 300, 291, 767, 643, 281, 1867, 13], "temperature": 0.0, "avg_logprob": -0.1947524634590984, "compression_ratio": 1.7238805970149254, "no_speech_prob": 3.668817271318403e-06}, {"id": 880, "seek": 445992, "start": 4465.16, "end": 4470.52, "text": " You want to get to a point where it just takes you a few minutes to try another algorithm", "tokens": [509, 528, 281, 483, 281, 257, 935, 689, 309, 445, 2516, 291, 257, 1326, 2077, 281, 853, 1071, 9284], "temperature": 0.0, "avg_logprob": -0.1947524634590984, "compression_ratio": 1.7238805970149254, "no_speech_prob": 3.668817271318403e-06}, {"id": 881, "seek": 445992, "start": 4470.52, "end": 4475.64, "text": " out and so you don't need to be wedded to one or the other.", "tokens": [484, 293, 370, 291, 500, 380, 643, 281, 312, 6393, 9207, 281, 472, 420, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.1947524634590984, "compression_ratio": 1.7238805970149254, "no_speech_prob": 3.668817271318403e-06}, {"id": 882, "seek": 445992, "start": 4475.64, "end": 4477.88, "text": " So I'm just going to see if I can I don't know how long it's going to take to reboot", "tokens": [407, 286, 478, 445, 516, 281, 536, 498, 286, 393, 286, 500, 380, 458, 577, 938, 309, 311, 516, 281, 747, 281, 33818], "temperature": 0.0, "avg_logprob": -0.1947524634590984, "compression_ratio": 1.7238805970149254, "no_speech_prob": 3.668817271318403e-06}, {"id": 883, "seek": 445992, "start": 4477.88, "end": 4482.24, "text": " so I'm just going to press up arrow to get back my last SSH command and I'll press enter", "tokens": [370, 286, 478, 445, 516, 281, 1886, 493, 11610, 281, 483, 646, 452, 1036, 12238, 39, 5622, 293, 286, 603, 1886, 3242], "temperature": 0.0, "avg_logprob": -0.1947524634590984, "compression_ratio": 1.7238805970149254, "no_speech_prob": 3.668817271318403e-06}, {"id": 884, "seek": 445992, "start": 4482.24, "end": 4483.56, "text": " and we'll see if we're back.", "tokens": [293, 321, 603, 536, 498, 321, 434, 646, 13], "temperature": 0.0, "avg_logprob": -0.1947524634590984, "compression_ratio": 1.7238805970149254, "no_speech_prob": 3.668817271318403e-06}, {"id": 885, "seek": 445992, "start": 4483.56, "end": 4484.56, "text": " We're back.", "tokens": [492, 434, 646, 13], "temperature": 0.0, "avg_logprob": -0.1947524634590984, "compression_ratio": 1.7238805970149254, "no_speech_prob": 3.668817271318403e-06}, {"id": 886, "seek": 445992, "start": 4484.56, "end": 4485.56, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.1947524634590984, "compression_ratio": 1.7238805970149254, "no_speech_prob": 3.668817271318403e-06}, {"id": 887, "seek": 445992, "start": 4485.56, "end": 4488.56, "text": " So this is finished rebooting.", "tokens": [407, 341, 307, 4335, 26802, 17001, 13], "temperature": 0.0, "avg_logprob": -0.1947524634590984, "compression_ratio": 1.7238805970149254, "no_speech_prob": 3.668817271318403e-06}, {"id": 888, "seek": 448856, "start": 4488.56, "end": 4498.72, "text": " Oh actually this time it says to do something slightly different which is to add this minus", "tokens": [876, 767, 341, 565, 309, 1619, 281, 360, 746, 4748, 819, 597, 307, 281, 909, 341, 3175], "temperature": 0.0, "avg_logprob": -0.18724588416088586, "compression_ratio": 1.6054054054054054, "no_speech_prob": 4.222751613269793e-06}, {"id": 889, "seek": 448856, "start": 4498.72, "end": 4499.72, "text": " L here.", "tokens": [441, 510, 13], "temperature": 0.0, "avg_logprob": -0.18724588416088586, "compression_ratio": 1.6054054054054054, "no_speech_prob": 4.222751613269793e-06}, {"id": 890, "seek": 448856, "start": 4499.72, "end": 4503.84, "text": " This is the thing that's going to let us connect to Jupyter Notebook.", "tokens": [639, 307, 264, 551, 300, 311, 516, 281, 718, 505, 1745, 281, 22125, 88, 391, 11633, 2939, 13], "temperature": 0.0, "avg_logprob": -0.18724588416088586, "compression_ratio": 1.6054054054054054, "no_speech_prob": 4.222751613269793e-06}, {"id": 891, "seek": 448856, "start": 4503.84, "end": 4511.0, "text": " So I'm going to type exit to exit from the server and this time I'm going to add the", "tokens": [407, 286, 478, 516, 281, 2010, 11043, 281, 11043, 490, 264, 7154, 293, 341, 565, 286, 478, 516, 281, 909, 264], "temperature": 0.0, "avg_logprob": -0.18724588416088586, "compression_ratio": 1.6054054054054054, "no_speech_prob": 4.222751613269793e-06}, {"id": 892, "seek": 448856, "start": 4511.0, "end": 4512.0, "text": " extra bit of the command.", "tokens": [2857, 857, 295, 264, 5622, 13], "temperature": 0.0, "avg_logprob": -0.18724588416088586, "compression_ratio": 1.6054054054054054, "no_speech_prob": 4.222751613269793e-06}, {"id": 893, "seek": 448856, "start": 4512.0, "end": 4515.0, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.18724588416088586, "compression_ratio": 1.6054054054054054, "no_speech_prob": 4.222751613269793e-06}, {"id": 894, "seek": 448856, "start": 4515.0, "end": 4516.0, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.18724588416088586, "compression_ratio": 1.6054054054054054, "no_speech_prob": 4.222751613269793e-06}, {"id": 895, "seek": 451600, "start": 4516.0, "end": 4520.04, "text": " And all right so the next thing is we're going to install something called Miniconda.", "tokens": [400, 439, 558, 370, 264, 958, 551, 307, 321, 434, 516, 281, 3625, 746, 1219, 2829, 299, 12233, 13], "temperature": 0.0, "avg_logprob": -0.141351989146029, "compression_ratio": 1.6334661354581674, "no_speech_prob": 1.4063816706766374e-05}, {"id": 896, "seek": 451600, "start": 4520.04, "end": 4526.76, "text": " Miniconda is a very nice distribution of Python the programming language.", "tokens": [2829, 299, 12233, 307, 257, 588, 1481, 7316, 295, 15329, 264, 9410, 2856, 13], "temperature": 0.0, "avg_logprob": -0.141351989146029, "compression_ratio": 1.6334661354581674, "no_speech_prob": 1.4063816706766374e-05}, {"id": 897, "seek": 451600, "start": 4526.76, "end": 4536.0, "text": " A lot of people have bad experiences of their computers getting really confused with Python", "tokens": [316, 688, 295, 561, 362, 1578, 5235, 295, 641, 10807, 1242, 534, 9019, 365, 15329], "temperature": 0.0, "avg_logprob": -0.141351989146029, "compression_ratio": 1.6334661354581674, "no_speech_prob": 1.4063816706766374e-05}, {"id": 898, "seek": 451600, "start": 4536.0, "end": 4539.62, "text": " packages and things conflicting and all kinds of stuff like that.", "tokens": [17401, 293, 721, 43784, 293, 439, 3685, 295, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.141351989146029, "compression_ratio": 1.6334661354581674, "no_speech_prob": 1.4063816706766374e-05}, {"id": 899, "seek": 451600, "start": 4539.62, "end": 4545.92, "text": " That's because pretty much all the major operating systems now come with a version of Python", "tokens": [663, 311, 570, 1238, 709, 439, 264, 2563, 7447, 3652, 586, 808, 365, 257, 3037, 295, 15329], "temperature": 0.0, "avg_logprob": -0.141351989146029, "compression_ratio": 1.6334661354581674, "no_speech_prob": 1.4063816706766374e-05}, {"id": 900, "seek": 454592, "start": 4545.92, "end": 4551.68, "text": " that is used by your computer for important operating system tasks.", "tokens": [300, 307, 1143, 538, 428, 3820, 337, 1021, 7447, 1185, 9608, 13], "temperature": 0.0, "avg_logprob": -0.14635157003635313, "compression_ratio": 1.7487437185929648, "no_speech_prob": 1.4144660553938593e-06}, {"id": 901, "seek": 454592, "start": 4551.68, "end": 4555.72, "text": " You should not be using that Python to train your machine learning models.", "tokens": [509, 820, 406, 312, 1228, 300, 15329, 281, 3847, 428, 3479, 2539, 5245, 13], "temperature": 0.0, "avg_logprob": -0.14635157003635313, "compression_ratio": 1.7487437185929648, "no_speech_prob": 1.4144660553938593e-06}, {"id": 902, "seek": 454592, "start": 4555.72, "end": 4558.28, "text": " Leave that Python alone.", "tokens": [9825, 300, 15329, 3312, 13], "temperature": 0.0, "avg_logprob": -0.14635157003635313, "compression_ratio": 1.7487437185929648, "no_speech_prob": 1.4144660553938593e-06}, {"id": 903, "seek": 454592, "start": 4558.28, "end": 4564.8, "text": " You should always install Miniconda which is going to give you your own version of Python", "tokens": [509, 820, 1009, 3625, 2829, 299, 12233, 597, 307, 516, 281, 976, 291, 428, 1065, 3037, 295, 15329], "temperature": 0.0, "avg_logprob": -0.14635157003635313, "compression_ratio": 1.7487437185929648, "no_speech_prob": 1.4144660553938593e-06}, {"id": 904, "seek": 454592, "start": 4564.8, "end": 4569.72, "text": " which is nothing to do with your operating system as you can play around with as you", "tokens": [597, 307, 1825, 281, 360, 365, 428, 7447, 1185, 382, 291, 393, 862, 926, 365, 382, 291], "temperature": 0.0, "avg_logprob": -0.14635157003635313, "compression_ratio": 1.7487437185929648, "no_speech_prob": 1.4144660553938593e-06}, {"id": 905, "seek": 454592, "start": 4569.72, "end": 4571.08, "text": " like.", "tokens": [411, 13], "temperature": 0.0, "avg_logprob": -0.14635157003635313, "compression_ratio": 1.7487437185929648, "no_speech_prob": 1.4144660553938593e-06}, {"id": 906, "seek": 457108, "start": 4571.08, "end": 4576.26, "text": " It's really easy just you can delete the whole folder and create it again in like three minutes.", "tokens": [467, 311, 534, 1858, 445, 291, 393, 12097, 264, 1379, 10820, 293, 1884, 309, 797, 294, 411, 1045, 2077, 13], "temperature": 0.0, "avg_logprob": -0.15243633048048297, "compression_ratio": 1.669291338582677, "no_speech_prob": 9.36847754928749e-06}, {"id": 907, "seek": 457108, "start": 4576.26, "end": 4579.4, "text": " You can create new environments which is like little testing grounds.", "tokens": [509, 393, 1884, 777, 12388, 597, 307, 411, 707, 4997, 19196, 13], "temperature": 0.0, "avg_logprob": -0.15243633048048297, "compression_ratio": 1.669291338582677, "no_speech_prob": 9.36847754928749e-06}, {"id": 908, "seek": 457108, "start": 4579.4, "end": 4581.24, "text": " You can try different things.", "tokens": [509, 393, 853, 819, 721, 13], "temperature": 0.0, "avg_logprob": -0.15243633048048297, "compression_ratio": 1.669291338582677, "no_speech_prob": 9.36847754928749e-06}, {"id": 909, "seek": 457108, "start": 4581.24, "end": 4586.74, "text": " This is a very strong recommendation is to make sure that you install even if you're", "tokens": [639, 307, 257, 588, 2068, 11879, 307, 281, 652, 988, 300, 291, 3625, 754, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.15243633048048297, "compression_ratio": 1.669291338582677, "no_speech_prob": 9.36847754928749e-06}, {"id": 910, "seek": 457108, "start": 4586.74, "end": 4591.84, "text": " just playing around on Windows or a Mac not on a server install Miniconda.", "tokens": [445, 2433, 926, 322, 8591, 420, 257, 5707, 406, 322, 257, 7154, 3625, 2829, 299, 12233, 13], "temperature": 0.0, "avg_logprob": -0.15243633048048297, "compression_ratio": 1.669291338582677, "no_speech_prob": 9.36847754928749e-06}, {"id": 911, "seek": 457108, "start": 4591.84, "end": 4592.84, "text": " It's cross platform.", "tokens": [467, 311, 3278, 3663, 13], "temperature": 0.0, "avg_logprob": -0.15243633048048297, "compression_ratio": 1.669291338582677, "no_speech_prob": 9.36847754928749e-06}, {"id": 912, "seek": 457108, "start": 4592.84, "end": 4597.04, "text": " You can use it everywhere and use that Python.", "tokens": [509, 393, 764, 309, 5315, 293, 764, 300, 15329, 13], "temperature": 0.0, "avg_logprob": -0.15243633048048297, "compression_ratio": 1.669291338582677, "no_speech_prob": 9.36847754928749e-06}, {"id": 913, "seek": 459704, "start": 4597.04, "end": 4606.04, "text": " Okay so Miniconda is now installed so we now have our own Python setup.", "tokens": [1033, 370, 2829, 299, 12233, 307, 586, 8899, 370, 321, 586, 362, 527, 1065, 15329, 8657, 13], "temperature": 0.0, "avg_logprob": -0.14994021824428014, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.2603153411182575e-06}, {"id": 914, "seek": 459704, "start": 4606.04, "end": 4614.48, "text": " So the last setup step is we have to install drivers for the GPU and Ubuntu actually comes", "tokens": [407, 264, 1036, 8657, 1823, 307, 321, 362, 281, 3625, 11590, 337, 264, 18407, 293, 30230, 45605, 767, 1487], "temperature": 0.0, "avg_logprob": -0.14994021824428014, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.2603153411182575e-06}, {"id": 915, "seek": 459704, "start": 4614.48, "end": 4619.74, "text": " with something that figures out for you what the best drivers are for your device.", "tokens": [365, 746, 300, 9624, 484, 337, 291, 437, 264, 1151, 11590, 366, 337, 428, 4302, 13], "temperature": 0.0, "avg_logprob": -0.14994021824428014, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.2603153411182575e-06}, {"id": 916, "seek": 459704, "start": 4619.74, "end": 4623.48, "text": " So this is just what this step here is and so I'm going to look down look here it says", "tokens": [407, 341, 307, 445, 437, 341, 1823, 510, 307, 293, 370, 286, 478, 516, 281, 574, 760, 574, 510, 309, 1619], "temperature": 0.0, "avg_logprob": -0.14994021824428014, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.2603153411182575e-06}, {"id": 917, "seek": 459704, "start": 4623.48, "end": 4625.24, "text": " recommended.", "tokens": [9628, 13], "temperature": 0.0, "avg_logprob": -0.14994021824428014, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.2603153411182575e-06}, {"id": 918, "seek": 459704, "start": 4625.24, "end": 4626.4, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.14994021824428014, "compression_ratio": 1.610091743119266, "no_speech_prob": 2.2603153411182575e-06}, {"id": 919, "seek": 462640, "start": 4626.4, "end": 4628.44, "text": " So here's the driver I want.", "tokens": [407, 510, 311, 264, 6787, 286, 528, 13], "temperature": 0.0, "avg_logprob": -0.15687924339657738, "compression_ratio": 1.6481481481481481, "no_speech_prob": 7.646418453077786e-06}, {"id": 920, "seek": 462640, "start": 4628.44, "end": 4629.44, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.15687924339657738, "compression_ratio": 1.6481481481481481, "no_speech_prob": 7.646418453077786e-06}, {"id": 921, "seek": 462640, "start": 4629.44, "end": 4633.5599999999995, "text": " But what I actually recommend is you use that but also the one at the dash server to the", "tokens": [583, 437, 286, 767, 2748, 307, 291, 764, 300, 457, 611, 264, 472, 412, 264, 8240, 7154, 281, 264], "temperature": 0.0, "avg_logprob": -0.15687924339657738, "compression_ratio": 1.6481481481481481, "no_speech_prob": 7.646418453077786e-06}, {"id": 922, "seek": 462640, "start": 4633.5599999999995, "end": 4638.36, "text": " end that's going to make like not install the stuff for playing computer games or whatever.", "tokens": [917, 300, 311, 516, 281, 652, 411, 406, 3625, 264, 1507, 337, 2433, 3820, 2813, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.15687924339657738, "compression_ratio": 1.6481481481481481, "no_speech_prob": 7.646418453077786e-06}, {"id": 923, "seek": 462640, "start": 4638.36, "end": 4639.36, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.15687924339657738, "compression_ratio": 1.6481481481481481, "no_speech_prob": 7.646418453077786e-06}, {"id": 924, "seek": 462640, "start": 4639.36, "end": 4644.2, "text": " So let's go ahead and run these lines of code.", "tokens": [407, 718, 311, 352, 2286, 293, 1190, 613, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.15687924339657738, "compression_ratio": 1.6481481481481481, "no_speech_prob": 7.646418453077786e-06}, {"id": 925, "seek": 462640, "start": 4644.2, "end": 4645.2, "text": " This is the bit here.", "tokens": [639, 307, 264, 857, 510, 13], "temperature": 0.0, "avg_logprob": -0.15687924339657738, "compression_ratio": 1.6481481481481481, "no_speech_prob": 7.646418453077786e-06}, {"id": 926, "seek": 462640, "start": 4645.2, "end": 4649.839999999999, "text": " See this is 460 depending on your graphics card when you run this you might have some", "tokens": [3008, 341, 307, 1017, 4550, 5413, 322, 428, 11837, 2920, 562, 291, 1190, 341, 291, 1062, 362, 512], "temperature": 0.0, "avg_logprob": -0.15687924339657738, "compression_ratio": 1.6481481481481481, "no_speech_prob": 7.646418453077786e-06}, {"id": 927, "seek": 462640, "start": 4649.839999999999, "end": 4650.839999999999, "text": " different number.", "tokens": [819, 1230, 13], "temperature": 0.0, "avg_logprob": -0.15687924339657738, "compression_ratio": 1.6481481481481481, "no_speech_prob": 7.646418453077786e-06}, {"id": 928, "seek": 462640, "start": 4650.839999999999, "end": 4651.839999999999, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.15687924339657738, "compression_ratio": 1.6481481481481481, "no_speech_prob": 7.646418453077786e-06}, {"id": 929, "seek": 462640, "start": 4651.839999999999, "end": 4654.4, "text": " But since I wrote this today it's still 460.", "tokens": [583, 1670, 286, 4114, 341, 965, 309, 311, 920, 1017, 4550, 13], "temperature": 0.0, "avg_logprob": -0.15687924339657738, "compression_ratio": 1.6481481481481481, "no_speech_prob": 7.646418453077786e-06}, {"id": 930, "seek": 465440, "start": 4654.4, "end": 4662.879999999999, "text": " So we'll go ahead and do that and this is going to go ahead and install this.", "tokens": [407, 321, 603, 352, 2286, 293, 360, 300, 293, 341, 307, 516, 281, 352, 2286, 293, 3625, 341, 13], "temperature": 0.0, "avg_logprob": -0.18545216878255208, "compression_ratio": 1.5942857142857143, "no_speech_prob": 4.092889867024496e-06}, {"id": 931, "seek": 465440, "start": 4662.879999999999, "end": 4671.08, "text": " Oh sudo sudo is a special thing you can add to the front of a command that runs it as", "tokens": [876, 459, 2595, 459, 2595, 307, 257, 2121, 551, 291, 393, 909, 281, 264, 1868, 295, 257, 5622, 300, 6676, 309, 382], "temperature": 0.0, "avg_logprob": -0.18545216878255208, "compression_ratio": 1.5942857142857143, "no_speech_prob": 4.092889867024496e-06}, {"id": 932, "seek": 465440, "start": 4671.08, "end": 4672.719999999999, "text": " an administrator.", "tokens": [364, 25529, 13], "temperature": 0.0, "avg_logprob": -0.18545216878255208, "compression_ratio": 1.5942857142857143, "no_speech_prob": 4.092889867024496e-06}, {"id": 933, "seek": 465440, "start": 4672.719999999999, "end": 4680.879999999999, "text": " Okay so some things you know by default commands you run basically can't break your system", "tokens": [1033, 370, 512, 721, 291, 458, 538, 7576, 16901, 291, 1190, 1936, 393, 380, 1821, 428, 1185], "temperature": 0.0, "avg_logprob": -0.18545216878255208, "compression_ratio": 1.5942857142857143, "no_speech_prob": 4.092889867024496e-06}, {"id": 934, "seek": 465440, "start": 4680.879999999999, "end": 4681.879999999999, "text": " right.", "tokens": [558, 13], "temperature": 0.0, "avg_logprob": -0.18545216878255208, "compression_ratio": 1.5942857142857143, "no_speech_prob": 4.092889867024496e-06}, {"id": 935, "seek": 468188, "start": 4681.88, "end": 4686.56, "text": " Because things like installing new software you have to tell it to run it as an administrator.", "tokens": [1436, 721, 411, 20762, 777, 4722, 291, 362, 281, 980, 309, 281, 1190, 309, 382, 364, 25529, 13], "temperature": 0.0, "avg_logprob": -0.2731132507324219, "compression_ratio": 1.502857142857143, "no_speech_prob": 3.90545028494671e-06}, {"id": 936, "seek": 468188, "start": 4686.56, "end": 4689.52, "text": " So and when you do that it'll ask you for your password.", "tokens": [407, 293, 562, 291, 360, 300, 309, 603, 1029, 291, 337, 428, 11524, 13], "temperature": 0.0, "avg_logprob": -0.2731132507324219, "compression_ratio": 1.502857142857143, "no_speech_prob": 3.90545028494671e-06}, {"id": 937, "seek": 468188, "start": 4689.52, "end": 4694.0, "text": " This is the password that you put in just just a moment ago with the setup.", "tokens": [639, 307, 264, 11524, 300, 291, 829, 294, 445, 445, 257, 1623, 2057, 365, 264, 8657, 13], "temperature": 0.0, "avg_logprob": -0.2731132507324219, "compression_ratio": 1.502857142857143, "no_speech_prob": 3.90545028494671e-06}, {"id": 938, "seek": 468188, "start": 4694.0, "end": 4697.16, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.2731132507324219, "compression_ratio": 1.502857142857143, "no_speech_prob": 3.90545028494671e-06}, {"id": 939, "seek": 468188, "start": 4697.16, "end": 4707.92, "text": " Okay let's keep going.", "tokens": [1033, 718, 311, 1066, 516, 13], "temperature": 0.0, "avg_logprob": -0.2731132507324219, "compression_ratio": 1.502857142857143, "no_speech_prob": 3.90545028494671e-06}, {"id": 940, "seek": 470792, "start": 4707.92, "end": 4713.56, "text": " Is there a section of the course that people skip over too quickly.", "tokens": [1119, 456, 257, 3541, 295, 264, 1164, 300, 561, 10023, 670, 886, 2661, 13], "temperature": 0.0, "avg_logprob": -0.20332610770447612, "compression_ratio": 1.6761363636363635, "no_speech_prob": 4.157311650487827e-06}, {"id": 941, "seek": 470792, "start": 4713.56, "end": 4717.4400000000005, "text": " Yes part two.", "tokens": [1079, 644, 732, 13], "temperature": 0.0, "avg_logprob": -0.20332610770447612, "compression_ratio": 1.6761363636363635, "no_speech_prob": 4.157311650487827e-06}, {"id": 942, "seek": 470792, "start": 4717.4400000000005, "end": 4722.56, "text": " Yeah not enough people do part two.", "tokens": [865, 406, 1547, 561, 360, 644, 732, 13], "temperature": 0.0, "avg_logprob": -0.20332610770447612, "compression_ratio": 1.6761363636363635, "no_speech_prob": 4.157311650487827e-06}, {"id": 943, "seek": 470792, "start": 4722.56, "end": 4731.04, "text": " And the difference between part one and part two is the difference between being a pretty", "tokens": [400, 264, 2649, 1296, 644, 472, 293, 644, 732, 307, 264, 2649, 1296, 885, 257, 1238], "temperature": 0.0, "avg_logprob": -0.20332610770447612, "compression_ratio": 1.6761363636363635, "no_speech_prob": 4.157311650487827e-06}, {"id": 944, "seek": 470792, "start": 4731.04, "end": 4737.52, "text": " handy practitioner you know who can who can do some pretty good work as long as it's in", "tokens": [13239, 32125, 291, 458, 567, 393, 567, 393, 360, 512, 1238, 665, 589, 382, 938, 382, 309, 311, 294], "temperature": 0.0, "avg_logprob": -0.20332610770447612, "compression_ratio": 1.6761363636363635, "no_speech_prob": 4.157311650487827e-06}, {"id": 945, "seek": 473752, "start": 4737.52, "end": 4742.52, "text": " reasonably well established kinds of areas.", "tokens": [23551, 731, 7545, 3685, 295, 3179, 13], "temperature": 0.0, "avg_logprob": -0.13597234090169272, "compression_ratio": 1.6745283018867925, "no_speech_prob": 1.2804960533685517e-05}, {"id": 946, "seek": 473752, "start": 4742.52, "end": 4747.240000000001, "text": " And versus being somebody who understands how everything's put together you could you", "tokens": [400, 5717, 885, 2618, 567, 15146, 577, 1203, 311, 829, 1214, 291, 727, 291], "temperature": 0.0, "avg_logprob": -0.13597234090169272, "compression_ratio": 1.6745283018867925, "no_speech_prob": 1.2804960533685517e-05}, {"id": 947, "seek": 473752, "start": 4747.240000000001, "end": 4751.76, "text": " know if you're told to create a deep learning model on in a domain that's like there are", "tokens": [458, 498, 291, 434, 1907, 281, 1884, 257, 2452, 2539, 2316, 322, 294, 257, 9274, 300, 311, 411, 456, 366], "temperature": 0.0, "avg_logprob": -0.13597234090169272, "compression_ratio": 1.6745283018867925, "no_speech_prob": 1.2804960533685517e-05}, {"id": 948, "seek": 473752, "start": 4751.76, "end": 4755.360000000001, "text": " no published models you'll be able to create one.", "tokens": [572, 6572, 5245, 291, 603, 312, 1075, 281, 1884, 472, 13], "temperature": 0.0, "avg_logprob": -0.13597234090169272, "compression_ratio": 1.6745283018867925, "no_speech_prob": 1.2804960533685517e-05}, {"id": 949, "seek": 473752, "start": 4755.360000000001, "end": 4763.4800000000005, "text": " If you understand how to create models which combine multiple different data types you", "tokens": [759, 291, 1223, 577, 281, 1884, 5245, 597, 10432, 3866, 819, 1412, 3467, 291], "temperature": 0.0, "avg_logprob": -0.13597234090169272, "compression_ratio": 1.6745283018867925, "no_speech_prob": 1.2804960533685517e-05}, {"id": 950, "seek": 476348, "start": 4763.48, "end": 4772.759999999999, "text": " know you're it's it's yeah it's it's a really great thing to finish and yeah not enough", "tokens": [458, 291, 434, 309, 311, 309, 311, 1338, 309, 311, 309, 311, 257, 534, 869, 551, 281, 2413, 293, 1338, 406, 1547], "temperature": 0.0, "avg_logprob": -0.2114959160486857, "compression_ratio": 1.631578947368421, "no_speech_prob": 5.507550213224022e-06}, {"id": 951, "seek": 476348, "start": 4772.759999999999, "end": 4777.599999999999, "text": " people realize how much is is there.", "tokens": [561, 4325, 577, 709, 307, 307, 456, 13], "temperature": 0.0, "avg_logprob": -0.2114959160486857, "compression_ratio": 1.631578947368421, "no_speech_prob": 5.507550213224022e-06}, {"id": 952, "seek": 476348, "start": 4777.599999999999, "end": 4786.32, "text": " And just the later lessons in in general you know it can like after you've done three lessons", "tokens": [400, 445, 264, 1780, 8820, 294, 294, 2674, 291, 458, 309, 393, 411, 934, 291, 600, 1096, 1045, 8820], "temperature": 0.0, "avg_logprob": -0.2114959160486857, "compression_ratio": 1.631578947368421, "no_speech_prob": 5.507550213224022e-06}, {"id": 953, "seek": 476348, "start": 4786.32, "end": 4789.599999999999, "text": " you you are pretty handy and you'll feel pretty handy right.", "tokens": [291, 291, 366, 1238, 13239, 293, 291, 603, 841, 1238, 13239, 558, 13], "temperature": 0.0, "avg_logprob": -0.2114959160486857, "compression_ratio": 1.631578947368421, "no_speech_prob": 5.507550213224022e-06}, {"id": 954, "seek": 478960, "start": 4789.6, "end": 4795.240000000001, "text": " But it's pretty easy to stop there because it feels like OK I get it.", "tokens": [583, 309, 311, 1238, 1858, 281, 1590, 456, 570, 309, 3417, 411, 2264, 286, 483, 309, 13], "temperature": 0.0, "avg_logprob": -0.15180392492385136, "compression_ratio": 1.5363636363636364, "no_speech_prob": 1.0451361049490515e-05}, {"id": 955, "seek": 478960, "start": 4795.240000000001, "end": 4804.52, "text": " You know I can train a model I get what's going on and to be fair it does very dramatically", "tokens": [509, 458, 286, 393, 3847, 257, 2316, 286, 483, 437, 311, 516, 322, 293, 281, 312, 3143, 309, 775, 588, 17548], "temperature": 0.0, "avg_logprob": -0.15180392492385136, "compression_ratio": 1.5363636363636364, "no_speech_prob": 1.0451361049490515e-05}, {"id": 956, "seek": 478960, "start": 4804.52, "end": 4809.8, "text": " kind of scale up in terms of intensity after that because in lesson four you'll have to", "tokens": [733, 295, 4373, 493, 294, 2115, 295, 13749, 934, 300, 570, 294, 6898, 1451, 291, 603, 362, 281], "temperature": 0.0, "avg_logprob": -0.15180392492385136, "compression_ratio": 1.5363636363636364, "no_speech_prob": 1.0451361049490515e-05}, {"id": 957, "seek": 478960, "start": 4809.8, "end": 4816.240000000001, "text": " write your own optimizer from scratch and you'll be getting into the calculus and stuff.", "tokens": [2464, 428, 1065, 5028, 6545, 490, 8459, 293, 291, 603, 312, 1242, 666, 264, 33400, 293, 1507, 13], "temperature": 0.0, "avg_logprob": -0.15180392492385136, "compression_ratio": 1.5363636363636364, "no_speech_prob": 1.0451361049490515e-05}, {"id": 958, "seek": 481624, "start": 4816.24, "end": 4822.2, "text": " But you know it is a big difference in terms of what what you can do and what you understand.", "tokens": [583, 291, 458, 309, 307, 257, 955, 2649, 294, 2115, 295, 437, 437, 291, 393, 360, 293, 437, 291, 1223, 13], "temperature": 0.0, "avg_logprob": -0.1387864539497777, "compression_ratio": 1.5728155339805825, "no_speech_prob": 4.936923232889967e-06}, {"id": 959, "seek": 481624, "start": 4822.2, "end": 4829.04, "text": " So I think in general you know not enough people are getting deeper into the lessons.", "tokens": [407, 286, 519, 294, 2674, 291, 458, 406, 1547, 561, 366, 1242, 7731, 666, 264, 8820, 13], "temperature": 0.0, "avg_logprob": -0.1387864539497777, "compression_ratio": 1.5728155339805825, "no_speech_prob": 4.936923232889967e-06}, {"id": 960, "seek": 481624, "start": 4829.04, "end": 4838.5199999999995, "text": " OK so this is now finished installing the Nvidia drivers.", "tokens": [2264, 370, 341, 307, 586, 4335, 20762, 264, 46284, 11590, 13], "temperature": 0.0, "avg_logprob": -0.1387864539497777, "compression_ratio": 1.5728155339805825, "no_speech_prob": 4.936923232889967e-06}, {"id": 961, "seek": 481624, "start": 4838.5199999999995, "end": 4842.12, "text": " Normally at this point people say to reboot but there's actually a magic thing you can", "tokens": [17424, 412, 341, 935, 561, 584, 281, 33818, 457, 456, 311, 767, 257, 5585, 551, 291, 393], "temperature": 0.0, "avg_logprob": -0.1387864539497777, "compression_ratio": 1.5728155339805825, "no_speech_prob": 4.936923232889967e-06}, {"id": 962, "seek": 484212, "start": 4842.12, "end": 4850.12, "text": " do which means you don't have to reboot and the Nvidia provides something called Nvidia", "tokens": [360, 597, 1355, 291, 500, 380, 362, 281, 33818, 293, 264, 46284, 6417, 746, 1219, 46284], "temperature": 0.0, "avg_logprob": -0.16766320466995238, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.5779505702084862e-06}, {"id": 963, "seek": 484212, "start": 4850.12, "end": 4854.64, "text": " SMI which will tell you about your installed GPUs.", "tokens": [13115, 40, 597, 486, 980, 291, 466, 428, 8899, 18407, 82, 13], "temperature": 0.0, "avg_logprob": -0.16766320466995238, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.5779505702084862e-06}, {"id": 964, "seek": 484212, "start": 4854.64, "end": 4858.88, "text": " And so if you run it and it pops up anything at all other than an error it means that you", "tokens": [400, 370, 498, 291, 1190, 309, 293, 309, 16795, 493, 1340, 412, 439, 661, 813, 364, 6713, 309, 1355, 300, 291], "temperature": 0.0, "avg_logprob": -0.16766320466995238, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.5779505702084862e-06}, {"id": 965, "seek": 484212, "start": 4858.88, "end": 4861.84, "text": " are you successfully have your GPUs installed.", "tokens": [366, 291, 10727, 362, 428, 18407, 82, 8899, 13], "temperature": 0.0, "avg_logprob": -0.16766320466995238, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.5779505702084862e-06}, {"id": 966, "seek": 484212, "start": 4861.84, "end": 4868.5599999999995, "text": " So this case we have a Tesla T4.", "tokens": [407, 341, 1389, 321, 362, 257, 13666, 314, 19, 13], "temperature": 0.0, "avg_logprob": -0.16766320466995238, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.5779505702084862e-06}, {"id": 967, "seek": 486856, "start": 4868.56, "end": 4872.72, "text": " It's currently 36 centigrade in there and the most important thing to know about is", "tokens": [467, 311, 4362, 8652, 44731, 294, 456, 293, 264, 881, 1021, 551, 281, 458, 466, 307], "temperature": 0.0, "avg_logprob": -0.11553160698859247, "compression_ratio": 1.5991735537190082, "no_speech_prob": 4.029416686535114e-06}, {"id": 968, "seek": 486856, "start": 4872.72, "end": 4879.120000000001, "text": " that it has 15 gigabytes of memory of which we're using nothing at all.", "tokens": [300, 309, 575, 2119, 42741, 295, 4675, 295, 597, 321, 434, 1228, 1825, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.11553160698859247, "compression_ratio": 1.5991735537190082, "no_speech_prob": 4.029416686535114e-06}, {"id": 969, "seek": 486856, "start": 4879.120000000001, "end": 4883.88, "text": " And there are no processors currently running on the GPU.", "tokens": [400, 456, 366, 572, 27751, 4362, 2614, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.11553160698859247, "compression_ratio": 1.5991735537190082, "no_speech_prob": 4.029416686535114e-06}, {"id": 970, "seek": 486856, "start": 4883.88, "end": 4888.04, "text": " So if you're finding something's going very slowly and you're wondering maybe it's not", "tokens": [407, 498, 291, 434, 5006, 746, 311, 516, 588, 5692, 293, 291, 434, 6359, 1310, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.11553160698859247, "compression_ratio": 1.5991735537190082, "no_speech_prob": 4.029416686535114e-06}, {"id": 971, "seek": 486856, "start": 4888.04, "end": 4893.68, "text": " using the GPU you can always run Nvidia SMI and if it says no running processors found", "tokens": [1228, 264, 18407, 291, 393, 1009, 1190, 46284, 13115, 40, 293, 498, 309, 1619, 572, 2614, 27751, 1352], "temperature": 0.0, "avg_logprob": -0.11553160698859247, "compression_ratio": 1.5991735537190082, "no_speech_prob": 4.029416686535114e-06}, {"id": 972, "seek": 489368, "start": 4893.68, "end": 4899.360000000001, "text": " you're not using the GPU.", "tokens": [291, 434, 406, 1228, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.19446811109486192, "compression_ratio": 1.4915966386554622, "no_speech_prob": 4.2892656892945524e-06}, {"id": 973, "seek": 489368, "start": 4899.360000000001, "end": 4906.64, "text": " OK so one more setup step which is we have to install all of the software all of the", "tokens": [2264, 370, 472, 544, 8657, 1823, 597, 307, 321, 362, 281, 3625, 439, 295, 264, 4722, 439, 295, 264], "temperature": 0.0, "avg_logprob": -0.19446811109486192, "compression_ratio": 1.4915966386554622, "no_speech_prob": 4.2892656892945524e-06}, {"id": 974, "seek": 489368, "start": 4906.64, "end": 4913.320000000001, "text": " Python libraries needed so PyTorch, FastAI, Jupyter Notebook and so forth.", "tokens": [15329, 15148, 2978, 370, 9953, 51, 284, 339, 11, 15968, 48698, 11, 22125, 88, 391, 11633, 2939, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.19446811109486192, "compression_ratio": 1.4915966386554622, "no_speech_prob": 4.2892656892945524e-06}, {"id": 975, "seek": 489368, "start": 4913.320000000001, "end": 4918.12, "text": " And so I've created a package which has that whole lot it's called Fastbook.", "tokens": [400, 370, 286, 600, 2942, 257, 7372, 597, 575, 300, 1379, 688, 309, 311, 1219, 15968, 2939, 13], "temperature": 0.0, "avg_logprob": -0.19446811109486192, "compression_ratio": 1.4915966386554622, "no_speech_prob": 4.2892656892945524e-06}, {"id": 976, "seek": 489368, "start": 4918.12, "end": 4923.240000000001, "text": " If you're if you've used HANA Conda or MiniConda before you might be surprised it says Mamba", "tokens": [759, 291, 434, 498, 291, 600, 1143, 389, 21429, 383, 12233, 420, 18239, 34, 12233, 949, 291, 1062, 312, 6100, 309, 1619, 376, 23337], "temperature": 0.0, "avg_logprob": -0.19446811109486192, "compression_ratio": 1.4915966386554622, "no_speech_prob": 4.2892656892945524e-06}, {"id": 977, "seek": 492324, "start": 4923.24, "end": 4925.54, "text": " rather than Conda.", "tokens": [2831, 813, 383, 12233, 13], "temperature": 0.0, "avg_logprob": -0.20983868767233455, "compression_ratio": 1.654054054054054, "no_speech_prob": 2.947992243207409e-06}, {"id": 978, "seek": 492324, "start": 4925.54, "end": 4929.0, "text": " You should definitely use Mamba and not Conda it's way way way faster.", "tokens": [509, 820, 2138, 764, 376, 23337, 293, 406, 383, 12233, 309, 311, 636, 636, 636, 4663, 13], "temperature": 0.0, "avg_logprob": -0.20983868767233455, "compression_ratio": 1.654054054054054, "no_speech_prob": 2.947992243207409e-06}, {"id": 979, "seek": 492324, "start": 4929.0, "end": 4935.04, "text": " So anytime you see something saying Conda install you should instead type Mamba install.", "tokens": [407, 13038, 291, 536, 746, 1566, 383, 12233, 3625, 291, 820, 2602, 2010, 376, 23337, 3625, 13], "temperature": 0.0, "avg_logprob": -0.20983868767233455, "compression_ratio": 1.654054054054054, "no_speech_prob": 2.947992243207409e-06}, {"id": 980, "seek": 492324, "start": 4935.04, "end": 4938.48, "text": " It's way faster.", "tokens": [467, 311, 636, 4663, 13], "temperature": 0.0, "avg_logprob": -0.20983868767233455, "compression_ratio": 1.654054054054054, "no_speech_prob": 2.947992243207409e-06}, {"id": 981, "seek": 492324, "start": 4938.48, "end": 4941.5599999999995, "text": " OK so off it goes.", "tokens": [2264, 370, 766, 309, 1709, 13], "temperature": 0.0, "avg_logprob": -0.20983868767233455, "compression_ratio": 1.654054054054054, "no_speech_prob": 2.947992243207409e-06}, {"id": 982, "seek": 492324, "start": 4941.5599999999995, "end": 4946.639999999999, "text": " Mamba is now going to install all of the all this Python software getting installed for", "tokens": [376, 23337, 307, 586, 516, 281, 3625, 439, 295, 264, 439, 341, 15329, 4722, 1242, 8899, 337], "temperature": 0.0, "avg_logprob": -0.20983868767233455, "compression_ratio": 1.654054054054054, "no_speech_prob": 2.947992243207409e-06}, {"id": 983, "seek": 492324, "start": 4946.639999999999, "end": 4949.679999999999, "text": " us.", "tokens": [505, 13], "temperature": 0.0, "avg_logprob": -0.20983868767233455, "compression_ratio": 1.654054054054054, "no_speech_prob": 2.947992243207409e-06}, {"id": 984, "seek": 494968, "start": 4949.68, "end": 4955.96, "text": " PyTorch is well over a gigabyte so this is going to take a few minutes just because it", "tokens": [9953, 51, 284, 339, 307, 731, 670, 257, 8741, 34529, 370, 341, 307, 516, 281, 747, 257, 1326, 2077, 445, 570, 309], "temperature": 0.0, "avg_logprob": -0.12791626695273578, "compression_ratio": 1.4863387978142077, "no_speech_prob": 4.222753432259196e-06}, {"id": 985, "seek": 494968, "start": 4955.96, "end": 4961.280000000001, "text": " has to download that that whole thing and yeah that can take a while.", "tokens": [575, 281, 5484, 300, 300, 1379, 551, 293, 1338, 300, 393, 747, 257, 1339, 13], "temperature": 0.0, "avg_logprob": -0.12791626695273578, "compression_ratio": 1.4863387978142077, "no_speech_prob": 4.222753432259196e-06}, {"id": 986, "seek": 494968, "start": 4961.280000000001, "end": 4968.280000000001, "text": " So while this is going do we got any more questions Michael?", "tokens": [407, 1339, 341, 307, 516, 360, 321, 658, 604, 544, 1651, 5116, 30], "temperature": 0.0, "avg_logprob": -0.12791626695273578, "compression_ratio": 1.4863387978142077, "no_speech_prob": 4.222753432259196e-06}, {"id": 987, "seek": 494968, "start": 4968.280000000001, "end": 4973.200000000001, "text": " Do you recommend any software for experiment tracking?", "tokens": [1144, 291, 2748, 604, 4722, 337, 5120, 11603, 30], "temperature": 0.0, "avg_logprob": -0.12791626695273578, "compression_ratio": 1.4863387978142077, "no_speech_prob": 4.222753432259196e-06}, {"id": 988, "seek": 497320, "start": 4973.2, "end": 4982.88, "text": " So the most popular experiment tracking software would be TensorBoard and Weights and Biases.", "tokens": [407, 264, 881, 3743, 5120, 11603, 4722, 576, 312, 34306, 22493, 515, 293, 492, 5761, 293, 13007, 1957, 13], "temperature": 0.0, "avg_logprob": -0.13206358923428302, "compression_ratio": 1.8509316770186335, "no_speech_prob": 1.5779349951117183e-06}, {"id": 989, "seek": 497320, "start": 4982.88, "end": 4988.88, "text": " Experiment tracking software is stuff which will basically you can use a FastAO callback", "tokens": [37933, 11603, 4722, 307, 1507, 597, 486, 1936, 291, 393, 764, 257, 15968, 32, 46, 818, 3207], "temperature": 0.0, "avg_logprob": -0.13206358923428302, "compression_ratio": 1.8509316770186335, "no_speech_prob": 1.5779349951117183e-06}, {"id": 990, "seek": 497320, "start": 4988.88, "end": 4995.44, "text": " and you basically will say train whilst tracking with TensorBoard or train whilst tracking with", "tokens": [293, 291, 1936, 486, 584, 3847, 18534, 11603, 365, 34306, 22493, 515, 420, 3847, 18534, 11603, 365], "temperature": 0.0, "avg_logprob": -0.13206358923428302, "compression_ratio": 1.8509316770186335, "no_speech_prob": 1.5779349951117183e-06}, {"id": 991, "seek": 497320, "start": 4995.44, "end": 4996.88, "text": " Weights and Biases.", "tokens": [492, 5761, 293, 13007, 1957, 13], "temperature": 0.0, "avg_logprob": -0.13206358923428302, "compression_ratio": 1.8509316770186335, "no_speech_prob": 1.5779349951117183e-06}, {"id": 992, "seek": 499688, "start": 4996.88, "end": 5004.52, "text": " And what it will do is it will kind of create a little database showing you all the training", "tokens": [400, 437, 309, 486, 360, 307, 309, 486, 733, 295, 1884, 257, 707, 8149, 4099, 291, 439, 264, 3097], "temperature": 0.0, "avg_logprob": -0.11983050178079044, "compression_ratio": 1.6607929515418502, "no_speech_prob": 2.964877126032661e-07}, {"id": 993, "seek": 499688, "start": 5004.52, "end": 5010.0, "text": " results from all the different experiments you've run and create some little graphs of", "tokens": [3542, 490, 439, 264, 819, 12050, 291, 600, 1190, 293, 1884, 512, 707, 24877, 295], "temperature": 0.0, "avg_logprob": -0.11983050178079044, "compression_ratio": 1.6607929515418502, "no_speech_prob": 2.964877126032661e-07}, {"id": 994, "seek": 499688, "start": 5010.0, "end": 5013.68, "text": " them and so forth.", "tokens": [552, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.11983050178079044, "compression_ratio": 1.6607929515418502, "no_speech_prob": 2.964877126032661e-07}, {"id": 995, "seek": 499688, "start": 5013.68, "end": 5021.36, "text": " Personally I don't use any experiment tracking software and the reason I don't is I found", "tokens": [21079, 286, 500, 380, 764, 604, 5120, 11603, 4722, 293, 264, 1778, 286, 500, 380, 307, 286, 1352], "temperature": 0.0, "avg_logprob": -0.11983050178079044, "compression_ratio": 1.6607929515418502, "no_speech_prob": 2.964877126032661e-07}, {"id": 996, "seek": 499688, "start": 5021.36, "end": 5026.76, "text": " that many many many people just about everybody I know who uses them finds it incredibly", "tokens": [300, 867, 867, 867, 561, 445, 466, 2201, 286, 458, 567, 4960, 552, 10704, 309, 6252], "temperature": 0.0, "avg_logprob": -0.11983050178079044, "compression_ratio": 1.6607929515418502, "no_speech_prob": 2.964877126032661e-07}, {"id": 997, "seek": 502676, "start": 5026.76, "end": 5028.24, "text": " distracting.", "tokens": [36689, 13], "temperature": 0.0, "avg_logprob": -0.10761605776273288, "compression_ratio": 1.529126213592233, "no_speech_prob": 3.785286480706418e-06}, {"id": 998, "seek": 502676, "start": 5028.24, "end": 5032.84, "text": " So the trick to training models is don't watch them train.", "tokens": [407, 264, 4282, 281, 3097, 5245, 307, 500, 380, 1159, 552, 3847, 13], "temperature": 0.0, "avg_logprob": -0.10761605776273288, "compression_ratio": 1.529126213592233, "no_speech_prob": 3.785286480706418e-06}, {"id": 999, "seek": 502676, "start": 5032.84, "end": 5036.76, "text": " So if you've done an EC programming it's like don't watch it compile right.", "tokens": [407, 498, 291, 600, 1096, 364, 19081, 9410, 309, 311, 411, 500, 380, 1159, 309, 31413, 558, 13], "temperature": 0.0, "avg_logprob": -0.10761605776273288, "compression_ratio": 1.529126213592233, "no_speech_prob": 3.785286480706418e-06}, {"id": 1000, "seek": 502676, "start": 5036.76, "end": 5043.4400000000005, "text": " Go and do something else preferably set up your next experiment.", "tokens": [1037, 293, 360, 746, 1646, 45916, 992, 493, 428, 958, 5120, 13], "temperature": 0.0, "avg_logprob": -0.10761605776273288, "compression_ratio": 1.529126213592233, "no_speech_prob": 3.785286480706418e-06}, {"id": 1001, "seek": 502676, "start": 5043.4400000000005, "end": 5048.280000000001, "text": " Experiment tracking software just makes it so tempting to look at all the pretty graphs", "tokens": [37933, 11603, 4722, 445, 1669, 309, 370, 37900, 281, 574, 412, 439, 264, 1238, 24877], "temperature": 0.0, "avg_logprob": -0.10761605776273288, "compression_ratio": 1.529126213592233, "no_speech_prob": 3.785286480706418e-06}, {"id": 1002, "seek": 502676, "start": 5048.280000000001, "end": 5051.4800000000005, "text": " in my opinion.", "tokens": [294, 452, 4800, 13], "temperature": 0.0, "avg_logprob": -0.10761605776273288, "compression_ratio": 1.529126213592233, "no_speech_prob": 3.785286480706418e-06}, {"id": 1003, "seek": 505148, "start": 5051.48, "end": 5058.36, "text": " So I would suggest get it running leave come back when it's done and there should be a", "tokens": [407, 286, 576, 3402, 483, 309, 2614, 1856, 808, 646, 562, 309, 311, 1096, 293, 456, 820, 312, 257], "temperature": 0.0, "avg_logprob": -0.1263880968093872, "compression_ratio": 1.6854460093896713, "no_speech_prob": 2.9479813292709878e-06}, {"id": 1004, "seek": 505148, "start": 5058.36, "end": 5066.919999999999, "text": " bit of reason you were running that experiment so check whatever that reason was right.", "tokens": [857, 295, 1778, 291, 645, 2614, 300, 5120, 370, 1520, 2035, 300, 1778, 390, 558, 13], "temperature": 0.0, "avg_logprob": -0.1263880968093872, "compression_ratio": 1.6854460093896713, "no_speech_prob": 2.9479813292709878e-06}, {"id": 1005, "seek": 505148, "start": 5066.919999999999, "end": 5073.639999999999, "text": " Having said that if you're really sure you need the services of experiment tracking software", "tokens": [10222, 848, 300, 498, 291, 434, 534, 988, 291, 643, 264, 3328, 295, 5120, 11603, 4722], "temperature": 0.0, "avg_logprob": -0.1263880968093872, "compression_ratio": 1.6854460093896713, "no_speech_prob": 2.9479813292709878e-06}, {"id": 1006, "seek": 505148, "start": 5073.639999999999, "end": 5079.16, "text": " for what you're doing and there are some things that genuinely need it then I think Weights", "tokens": [337, 437, 291, 434, 884, 293, 456, 366, 512, 721, 300, 17839, 643, 309, 550, 286, 519, 492, 5761], "temperature": 0.0, "avg_logprob": -0.1263880968093872, "compression_ratio": 1.6854460093896713, "no_speech_prob": 2.9479813292709878e-06}, {"id": 1007, "seek": 507916, "start": 5079.16, "end": 5083.0, "text": " and Biases is the best at the moment.", "tokens": [293, 13007, 1957, 307, 264, 1151, 412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.1423303476969401, "compression_ratio": 1.566137566137566, "no_speech_prob": 4.936870936944615e-06}, {"id": 1008, "seek": 507916, "start": 5083.0, "end": 5088.32, "text": " I think it's really great and furthermore they've hired lots of fast AI alumni and they're", "tokens": [286, 519, 309, 311, 534, 869, 293, 3052, 3138, 436, 600, 13144, 3195, 295, 2370, 7318, 16347, 293, 436, 434], "temperature": 0.0, "avg_logprob": -0.1423303476969401, "compression_ratio": 1.566137566137566, "no_speech_prob": 4.936870936944615e-06}, {"id": 1009, "seek": 507916, "start": 5088.32, "end": 5099.92, "text": " super nice people so definitely recommend that.", "tokens": [1687, 1481, 561, 370, 2138, 2748, 300, 13], "temperature": 0.0, "avg_logprob": -0.1423303476969401, "compression_ratio": 1.566137566137566, "no_speech_prob": 4.936870936944615e-06}, {"id": 1010, "seek": 507916, "start": 5099.92, "end": 5101.599999999999, "text": " So that's all the installation.", "tokens": [407, 300, 311, 439, 264, 13260, 13], "temperature": 0.0, "avg_logprob": -0.1423303476969401, "compression_ratio": 1.566137566137566, "no_speech_prob": 4.936870936944615e-06}, {"id": 1011, "seek": 507916, "start": 5101.599999999999, "end": 5107.96, "text": " So the last step is just to grab the book the notebooks and so you use something called", "tokens": [407, 264, 1036, 1823, 307, 445, 281, 4444, 264, 1446, 264, 43782, 293, 370, 291, 764, 746, 1219], "temperature": 0.0, "avg_logprob": -0.1423303476969401, "compression_ratio": 1.566137566137566, "no_speech_prob": 4.936870936944615e-06}, {"id": 1012, "seek": 510796, "start": 5107.96, "end": 5113.04, "text": " git clone to grab a repository of code and this is going to grab the fastbook repository", "tokens": [18331, 26506, 281, 4444, 257, 25841, 295, 3089, 293, 341, 307, 516, 281, 4444, 264, 2370, 2939, 25841], "temperature": 0.0, "avg_logprob": -0.17453550858931108, "compression_ratio": 1.6141732283464567, "no_speech_prob": 8.186328273040999e-07}, {"id": 1013, "seek": 510796, "start": 5113.04, "end": 5119.84, "text": " paste.", "tokens": [9163, 13], "temperature": 0.0, "avg_logprob": -0.17453550858931108, "compression_ratio": 1.6141732283464567, "no_speech_prob": 8.186328273040999e-07}, {"id": 1014, "seek": 510796, "start": 5119.84, "end": 5128.32, "text": " So you can see it's saying cloning this repository and so you'll now find that there is a fastbook", "tokens": [407, 291, 393, 536, 309, 311, 1566, 596, 16638, 341, 25841, 293, 370, 291, 603, 586, 915, 300, 456, 307, 257, 2370, 2939], "temperature": 0.0, "avg_logprob": -0.17453550858931108, "compression_ratio": 1.6141732283464567, "no_speech_prob": 8.186328273040999e-07}, {"id": 1015, "seek": 510796, "start": 5128.32, "end": 5129.32, "text": " directory.", "tokens": [21120, 13], "temperature": 0.0, "avg_logprob": -0.17453550858931108, "compression_ratio": 1.6141732283464567, "no_speech_prob": 8.186328273040999e-07}, {"id": 1016, "seek": 512932, "start": 5129.32, "end": 5140.5599999999995, "text": " So you can cd into it and there is our book.", "tokens": [407, 291, 393, 269, 67, 666, 309, 293, 456, 307, 527, 1446, 13], "temperature": 0.0, "avg_logprob": -0.19014371871948244, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.4823494893789757e-06}, {"id": 1017, "seek": 512932, "start": 5140.5599999999995, "end": 5143.719999999999, "text": " So I think something on Anaconda is going slowly so we're not going to wait for it to", "tokens": [407, 286, 519, 746, 322, 1107, 326, 12233, 307, 516, 5692, 370, 321, 434, 406, 516, 281, 1699, 337, 309, 281], "temperature": 0.0, "avg_logprob": -0.19014371871948244, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.4823494893789757e-06}, {"id": 1018, "seek": 512932, "start": 5143.719999999999, "end": 5149.5199999999995, "text": " download but so I want to show the very last step but the very last step is to run Jupyter", "tokens": [5484, 457, 370, 286, 528, 281, 855, 264, 588, 1036, 1823, 457, 264, 588, 1036, 1823, 307, 281, 1190, 22125, 88, 391], "temperature": 0.0, "avg_logprob": -0.19014371871948244, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.4823494893789757e-06}, {"id": 1019, "seek": 512932, "start": 5149.5199999999995, "end": 5154.5199999999995, "text": " notebook and then you'll be able to click on the URL that pops up and it'll bring up", "tokens": [21060, 293, 550, 291, 603, 312, 1075, 281, 2052, 322, 264, 12905, 300, 16795, 493, 293, 309, 603, 1565, 493], "temperature": 0.0, "avg_logprob": -0.19014371871948244, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.4823494893789757e-06}, {"id": 1020, "seek": 512932, "start": 5154.5199999999995, "end": 5159.2, "text": " something that basically looks just like we saw in curlup.", "tokens": [746, 300, 1936, 1542, 445, 411, 321, 1866, 294, 22591, 1010, 13], "temperature": 0.0, "avg_logprob": -0.19014371871948244, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.4823494893789757e-06}, {"id": 1021, "seek": 515920, "start": 5159.2, "end": 5166.96, "text": " But the nice thing is everything you save, everything you do will be remembered so all", "tokens": [583, 264, 1481, 551, 307, 1203, 291, 3155, 11, 1203, 291, 360, 486, 312, 13745, 370, 439], "temperature": 0.0, "avg_logprob": -0.16399855295817056, "compression_ratio": 1.6243093922651934, "no_speech_prob": 8.267583325505257e-06}, {"id": 1022, "seek": 515920, "start": 5166.96, "end": 5171.92, "text": " of your experiments are going to be there, the datasets you download are still there,", "tokens": [295, 428, 12050, 366, 516, 281, 312, 456, 11, 264, 42856, 291, 5484, 366, 920, 456, 11], "temperature": 0.0, "avg_logprob": -0.16399855295817056, "compression_ratio": 1.6243093922651934, "no_speech_prob": 8.267583325505257e-06}, {"id": 1023, "seek": 515920, "start": 5171.92, "end": 5174.5199999999995, "text": " so on and so forth.", "tokens": [370, 322, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.16399855295817056, "compression_ratio": 1.6243093922651934, "no_speech_prob": 8.267583325505257e-06}, {"id": 1024, "seek": 515920, "start": 5174.5199999999995, "end": 5178.62, "text": " So that's that.", "tokens": [407, 300, 311, 300, 13], "temperature": 0.0, "avg_logprob": -0.16399855295817056, "compression_ratio": 1.6243093922651934, "no_speech_prob": 8.267583325505257e-06}, {"id": 1025, "seek": 515920, "start": 5178.62, "end": 5186.72, "text": " So when you're done it'll remind you here to as I mentioned before stop your instance", "tokens": [407, 562, 291, 434, 1096, 309, 603, 4160, 291, 510, 281, 382, 286, 2835, 949, 1590, 428, 5197], "temperature": 0.0, "avg_logprob": -0.16399855295817056, "compression_ratio": 1.6243093922651934, "no_speech_prob": 8.267583325505257e-06}, {"id": 1026, "seek": 518672, "start": 5186.72, "end": 5195.64, "text": " so you can either choose stop in this menu or you can choose stop here or personally", "tokens": [370, 291, 393, 2139, 2826, 1590, 294, 341, 6510, 420, 291, 393, 2826, 1590, 510, 420, 5665], "temperature": 0.0, "avg_logprob": -0.1331240463256836, "compression_ratio": 1.5245901639344261, "no_speech_prob": 2.8573010695254197e-06}, {"id": 1027, "seek": 518672, "start": 5195.64, "end": 5203.72, "text": " what I quite like to do is to run sudo, remember sudo is this thing that runs as an administrator,", "tokens": [437, 286, 1596, 411, 281, 360, 307, 281, 1190, 459, 2595, 11, 1604, 459, 2595, 307, 341, 551, 300, 6676, 382, 364, 25529, 11], "temperature": 0.0, "avg_logprob": -0.1331240463256836, "compression_ratio": 1.5245901639344261, "no_speech_prob": 2.8573010695254197e-06}, {"id": 1028, "seek": 518672, "start": 5203.72, "end": 5207.400000000001, "text": " shutdown, halt, now.", "tokens": [34927, 11, 12479, 11, 586, 13], "temperature": 0.0, "avg_logprob": -0.1331240463256836, "compression_ratio": 1.5245901639344261, "no_speech_prob": 2.8573010695254197e-06}, {"id": 1029, "seek": 518672, "start": 5207.400000000001, "end": 5216.04, "text": " And so that shuts it down from here without having to go into the AWS GUI.", "tokens": [400, 370, 300, 48590, 309, 760, 490, 510, 1553, 1419, 281, 352, 666, 264, 17650, 17917, 40, 13], "temperature": 0.0, "avg_logprob": -0.1331240463256836, "compression_ratio": 1.5245901639344261, "no_speech_prob": 2.8573010695254197e-06}, {"id": 1030, "seek": 521604, "start": 5216.04, "end": 5218.16, "text": " And there we go.", "tokens": [400, 456, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.2783215122838174, "compression_ratio": 1.3758389261744965, "no_speech_prob": 7.071821983117843e-06}, {"id": 1031, "seek": 521604, "start": 5218.16, "end": 5230.12, "text": " So if we look back at the EC2 here in a moment this will switch from running state to stop", "tokens": [407, 498, 321, 574, 646, 412, 264, 19081, 17, 510, 294, 257, 1623, 341, 486, 3679, 490, 2614, 1785, 281, 1590], "temperature": 0.0, "avg_logprob": -0.2783215122838174, "compression_ratio": 1.3758389261744965, "no_speech_prob": 7.071821983117843e-06}, {"id": 1032, "seek": 521604, "start": 5230.12, "end": 5234.96, "text": " state.", "tokens": [1785, 13], "temperature": 0.0, "avg_logprob": -0.2783215122838174, "compression_ratio": 1.3758389261744965, "no_speech_prob": 7.071821983117843e-06}, {"id": 1033, "seek": 521604, "start": 5234.96, "end": 5237.48, "text": " So I think that's everything.", "tokens": [407, 286, 519, 300, 311, 1203, 13], "temperature": 0.0, "avg_logprob": -0.2783215122838174, "compression_ratio": 1.3758389261744965, "no_speech_prob": 7.071821983117843e-06}, {"id": 1034, "seek": 521604, "start": 5237.48, "end": 5241.12, "text": " Michael is there anything else we need to cover?", "tokens": [5116, 307, 456, 1340, 1646, 321, 643, 281, 2060, 30], "temperature": 0.0, "avg_logprob": -0.2783215122838174, "compression_ratio": 1.3758389261744965, "no_speech_prob": 7.071821983117843e-06}, {"id": 1035, "seek": 521604, "start": 5241.12, "end": 5242.12, "text": " Okay great.", "tokens": [1033, 869, 13], "temperature": 0.0, "avg_logprob": -0.2783215122838174, "compression_ratio": 1.3758389261744965, "no_speech_prob": 7.071821983117843e-06}, {"id": 1036, "seek": 524212, "start": 5242.12, "end": 5251.4, "text": " Alright well thank you everybody for listening in to lesson 0 and I look forward to hearing", "tokens": [2798, 731, 1309, 291, 2201, 337, 4764, 294, 281, 6898, 1958, 293, 286, 574, 2128, 281, 4763], "temperature": 0.0, "avg_logprob": -0.11585890728494395, "compression_ratio": 1.6243093922651934, "no_speech_prob": 1.0783015568449628e-05}, {"id": 1037, "seek": 524212, "start": 5251.4, "end": 5261.599999999999, "text": " how you go with lesson 1 and seeing your projects that you create and don't forget to get involved", "tokens": [577, 291, 352, 365, 6898, 502, 293, 2577, 428, 4455, 300, 291, 1884, 293, 500, 380, 2870, 281, 483, 3288], "temperature": 0.0, "avg_logprob": -0.11585890728494395, "compression_ratio": 1.6243093922651934, "no_speech_prob": 1.0783015568449628e-05}, {"id": 1038, "seek": 524212, "start": 5261.599999999999, "end": 5263.4, "text": " in the forums.", "tokens": [294, 264, 26998, 13], "temperature": 0.0, "avg_logprob": -0.11585890728494395, "compression_ratio": 1.6243093922651934, "no_speech_prob": 1.0783015568449628e-05}, {"id": 1039, "seek": 524212, "start": 5263.4, "end": 5269.0199999999995, "text": " If you do get stuck with something the first thing to do is to search the forums because", "tokens": [759, 291, 360, 483, 5541, 365, 746, 264, 700, 551, 281, 360, 307, 281, 3164, 264, 26998, 570], "temperature": 0.0, "avg_logprob": -0.11585890728494395, "compression_ratio": 1.6243093922651934, "no_speech_prob": 1.0783015568449628e-05}, {"id": 1040, "seek": 526902, "start": 5269.02, "end": 5272.4400000000005, "text": " out of the hundreds of thousands of people that have done this before somebody's probably", "tokens": [484, 295, 264, 6779, 295, 5383, 295, 561, 300, 362, 1096, 341, 949, 2618, 311, 1391], "temperature": 0.0, "avg_logprob": -0.17594012003096324, "compression_ratio": 1.5942857142857143, "no_speech_prob": 3.168049443047494e-05}, {"id": 1041, "seek": 526902, "start": 5272.4400000000005, "end": 5276.64, "text": " got stuck in the same way before so hopefully they can answer your question.", "tokens": [658, 5541, 294, 264, 912, 636, 949, 370, 4696, 436, 393, 1867, 428, 1168, 13], "temperature": 0.0, "avg_logprob": -0.17594012003096324, "compression_ratio": 1.5942857142857143, "no_speech_prob": 3.168049443047494e-05}, {"id": 1042, "seek": 526902, "start": 5276.64, "end": 5282.400000000001, "text": " Otherwise feel free to ask your own questions and hopefully somebody will answer for you.", "tokens": [10328, 841, 1737, 281, 1029, 428, 1065, 1651, 293, 4696, 2618, 486, 1867, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.17594012003096324, "compression_ratio": 1.5942857142857143, "no_speech_prob": 3.168049443047494e-05}, {"id": 1043, "seek": 526902, "start": 5282.400000000001, "end": 5283.400000000001, "text": " Thanks everybody.", "tokens": [2561, 2201, 13], "temperature": 0.0, "avg_logprob": -0.17594012003096324, "compression_ratio": 1.5942857142857143, "no_speech_prob": 3.168049443047494e-05}, {"id": 1044, "seek": 528340, "start": 5283.4, "end": 5299.48, "text": " Design,.", "tokens": [50364, 12748, 40698, 51168], "temperature": 1.0, "avg_logprob": -5.5032470703125, "compression_ratio": 0.5, "no_speech_prob": 0.0004861643828917295}], "language": "en"}