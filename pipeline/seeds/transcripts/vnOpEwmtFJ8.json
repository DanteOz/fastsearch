{"text": " Welcome to lesson 12. Wow, we're moving along. And this is an exciting lesson because it's where we're going to wrap up all the pieces both for computer vision and for NLP. And you might be surprised to hear that we're going to wrap up all the pieces for NLP because we haven't really done any NLP yet. But actually everything we've done is equally applicable to NLP. So there's very little to do to get a state of the art result on IMDB sentiment analysis from scratch. So that's what we're going to do. Before we do, let's finally finish off this slide we've been going through for three lessons now. I promised, not promised, that we would get something state of the art on ImageNet. Turns out we did. So you're going to see that today. So we're going to finish off mixup, label smoothing, and ResNets. Okay. So let's do it. Before we look at the new stuff, 09b learner. I've made a couple of minor changes that I thought you might be interested in. It's kind of like as you refactor things. So remember last week we refactored the learner to get rid of that awful separate runner. So there's just now one thing. Made a lot of our code a lot easier. But there's still this concept left behind that when you started fitting, you had to tell each callback what its learner or runner was. I've moved that because we don't, you know, they're all totally attached now. I've moved that to the, in it. And so now you can call addCBs to add a whole bunch of callbacks or addCB to add one callback. And that happens automatically at the start of training. So it's a very minor thing. More interesting was when I did this little reformatting exercise where I took all these callbacks that used to be on the line underneath the thing before them and lined them up over here and suddenly realized that now I can answer all the questions I have in my head about our callback system, which is what exactly are the steps in the training loop? What exactly are the callbacks that you can use in the training loop? Which step goes with which callback? Which steps don't have a callback? Are there any callbacks that don't have a step? So like it's one of these interesting things where I really don't like the idea of kind of automating your formatting and creating rules for formatting when like something like this can just like as soon as I did this I understood my code better. And for me understanding my code is the only way to make it work because debugging machine learning code is awful. So you've got to make sure that the thing you write makes sense. It's got to be simple. It's got to be really simple. It's got to be really simple. Then more interestingly we used to create the optimizer in init and you could actually pass in an already created optimizer. I removed that and the only thing now you can pass in is an optimization function. So something that will create an optimizer, which is what we've always been doing anyway. And by doing that we can now create our optimizer when we start fitting. And that turns out to be really important because when we do things like discriminative learning rates and gradual unfreezing and layer groups and stuff, we can change things and then when we fit it will all just work. So that's a more significant, it's like one line of code but it's conceptually a very significant change. Okay, so that's some minor changes to 9b. And now let's move on to mixup and label smoothing. So I'm really excited about the stuff we saw at the end of the last lesson where we saw how we can use the GPU to do data augmentation, fully randomized, fully GPU accelerated data augmentation using just plain PyTorch operations. I think that's a big win. But it's quite possible we don't need that kind of data augmentation anymore because in our experimentation with this data augmentation called mixup, we found we can remove most other data augmentation and get amazingly good results. So it's just a kind of a simplicity result. And also when you use mixup you can train for a really long time and get really good results. So let me show you mixup. And in terms of the results you can get, what happened in the bag of tricks paper was they, when they turned mixup on they also started training for 200 epochs instead of 90, sorry instead of 120. So be a bit careful when you interpret their paper table where it goes from label smoothing 94.1 to mixup without distillation 94.6. They're also nearly doubling the number of epochs they do. But you can kind of get a sense that you can get a big decrease in error. The other thing they mention in the paper is distillation. I'm not going to talk about that because it's a thing where you pre-train some much bigger model like a ResNet 152 and then you try and train something that predicts the output of that. And to me the idea of like training a really big model to train a smaller model, it's interesting but it's not exactly training in the way I normally think about it. So we're not looking at distillation. It would be an interesting assignment if somebody wanted to try adding it to the notebooks though. You have all the information and I think all the skills you need to do that now. All right. So mixup, we start by grabbing our ImageNet data set and we grab the makeRGB and resize and turn it into a float tensor. This is just our quick and dirty resize. We're only doing this for testing purposes. Split it up, create a data bunch, all the normal stuff. And what we're going to do is we're going to take an image like this and an image like this and we're going to combine them. We're going to take.3 times this image plus.7 times this image and this is what it's going to look like. Unfortunately, Sylvia and I have different orderings of file names on our things. So I wrote it's a French horn and a tench but actually Sylvia clearly doesn't have French horn or tenches but you get the idea. It's a mixup of two different images. So we're going to create a data augmentation where every time we predict something we're going to be predicting a mix of two things like this. So we're going to both take the linear combination,.3 and.7 of the two images but then we're going to have to do that for the labels as well, right? There's no point predicting the one hot encoded output of this breed of doggy where there's also a bit of a gas pump. So we're also going to have, we're not going to have one hot encoded output. We're going to have a.7 encoded doggy and a.3 encoded gas pump, right? So that's the basic idea. So the mixup paper was super cool. Wow, there are people talking about things that aren't deep learning. I guess that's their priorities. So the papers are pretty nice, easy read by paper standards and I would definitely suggest you check it out. So I've told you what we're going to do. Implementation wise we have to decide what number to use here. Is it.3 or.1 or.5 or what? And this is a data augmentation method so the answer is we'll randomize it. But we're not going to randomize it from 0 to 1 uniform or 0 to.5 uniform but instead we're going to randomize it using shapes like this. In other words, when we grab a random number most of the time it will be really close to 0 or really close to 1 and just occasionally it will be close to.5. So that way most of the time it will be pretty easy for our model because we've been predicting one and only one thing and just occasionally it will be predicting something that's a pretty evenly mixed combination. So the ability to grab random numbers that this is basically the histogram, the smooth histogram of how often we're going to see those numbers is called sampling from a probability distribution. And basically in nearly all these cases you can start with a uniform random number or a normal random number and put it through some kind of function or process to turn it into something like this. So the details don't matter at all. But the paper points out that this particular shape is nicely characterized by something called the beta distribution so that's what we're going to use. So it was interesting drawing these because it requires a few interesting bits of math which some of you may be less comfortable with or entirely uncomfortable with. For me every time I see this function which is called the gamma function I kind of break out in sweats. Not just because I got a cold but it's like you know the idea of functions that I don't like how do you describe this thing. But actually it turns out that like most things once you look at it it's actually pretty straightforward. And we're going to be using this function so I'll just quickly explain what's going on. We're going to start with a factorial function so 1 times 2 times 3 times 4 whatever right. And here these red dots is just the value of the factorial function for a few different places. But don't think of the factorial function as being 1 times 2 times 3 times 4 but times n you know whatever. But divide both sides by n and now you've got or divide both sides by n and so now you've got like factorial n divided by n equals 1 times 2 times 3. So it equals the factorial of n minus 1. And so when you define it like that you suddenly realize there's no reason that you can't have a function that's not just on the integers but is everywhere. This is the point where I stop with the math right because to me if I need a sine function or a log function or an x function or whatever I type it into my computer and I get it right. So the actual how you get it is not at all important. But the fact of knowing what these functions are and how they're defined is useful. PyTorch doesn't have this function. Weirdly enough they have a log gamma function. So we can take log gamma and go e to the power of that to get a gamma function. And you'll see here I am breaking my no Greek letters rule. And the reason I'm breaking that rule is because a function like this doesn't have a kind of domain specific meaning or a pure physical analogy which is how we always think about it. It's just a math function. And so we call it gamma right. And so if you're going to call it gamma you may as well write it like that. And why this matters is when you start using it like look at the difference between writing it out with the actual uni code and operators versus what would happen if you wrote it out long form in Python. Like when you're comparing something to a paper you want something that you can look at and straight away say like oh that looks very familiar. And as long as it's not familiar you might want to think about how to make it more familiar. So I'll just briefly mention that writing these math symbols nowadays is actually pretty easy. On Linux there's a thing called a compose key which is probably already set up for you. And if you Google it you can learn how to turn it on. And it's basically like you'll press like the right alt button or the caps lock button. You can choose what your compose key is. And then a few more letters. So for example all the Greek letters are compose and then star and then the English letter that corresponds with it. So for example if I want to do lambda I would go compose star L. So it's just as quick as typing non-uni code characters. Most of the Greek letters are available on a Mac keyboard just with option. Unfortunately nobody's created a decent compose key for Mac yet. There's a great compose key for Windows called WinCompose. Anybody who's working with, you know, Greek letters should definitely install and learn to use these things. So there's our gamma function, nice and concise. It looks exactly like the paper. And so it turns out that this is how you calculate the value of the beta function which is beta distribution. And so now here it is. So as I said the details aren't important but there's the tools that you can use. And the basic idea is that we now have something where we can pick some parameter which is called alpha. Where if it's high then it's much more likely that we get equal mix. And if it's low it's very unlikely. And this is really important because for data augmentation we need to be able to tune a lever that says how much regularization am I doing, how much augmentation am I doing. So you can move your alpha up and down. And the reason it's important to be able to print these plots out is that when you change your alpha you want to plot it out and see what it looks like. Right? Make sure it looks sensible. Okay. So it turns out that all we need to do then is we don't actually have to 0.7 hot encode one thing and 0.3 hot encode another thing. It's actually identical to simply go, I guess, which is lambda times the first loss plus 1 minus lambda times the second loss because we're using T here. So that's actually all we need to do. So this is our mix up. And again, as you can see we're using the same letters that we'd expect to see in the paper. So everything should look very familiar. And mix up, remember, is something which is going to change our loss function. So we need to know what loss function to change. So when you begin fitting you find out what the old loss function on the learner was and you store it away. And then when we calculate loss we can just go ahead and say, oh, if it's invalidation there's no mix up involved. And if we're training then we'll calculate the loss on two different sets of images. One is just the regular set and the second is we'll grab all the other images and randomly permute one and randomly pick one to share with. So we do that for the image and we do that for the loss. And that's basically it. A couple of minor things to mention. In the last lesson I created an EWMA function, exponentially weighted moving average function, which is a really dumb name for it. Because actually it was just a linear combination of two things. It was like V times alpha plus V1 times alpha plus V2 times 1 minus alpha. You create exponentially weighted moving averages with it by applying it multiple times. But the actual function is a linear combination. So I've renamed that to linear combination. And you'll see that so many places. So this mix up is a linear combination of our actual images and some randomly permuted images in that mini-batch. And our loss is a linear combination of the loss of our two different parts, our normal mini-batch and our randomly permuted mini-batch. One of the nice things about this is if you think about it, this is all being applied on the GPU. So this is pretty much instant. So a super powerful augmentation system, which isn't going to add any overhead to our code. One thing to be careful of is that we're actually replacing the loss function. And loss functions have something called a reduction. And most PyTorch loss functions you can say, after calculating the loss function for everything in the mini-batch, either return a rank one tensor of all of the loss functions for the mini-batch, or add them all up, or take the average. We pretty much always take the average. But we just have to make sure that we do the right thing. So I've just got a little function here that does the mean or sum, or nothing at all, as requested. And so then we need to make sure that we create our new loss function that at the end, it's going to reduce it in the way that they actually asked for. But then we have to turn off the reduction when we actually do mixup, because we actually need to calculate the loss on every image for both halves of our mixup. So this is a good place to use a context manager, which we've seen before. So we just created a tiny little context manager, which will just find out what the previous reduction was, save it away, get rid of it, and then put it back when it's finished. So there's a lot of minor details there. But with that in place, the actual mixup itself is very little code. It's a single callback. And we can then run it in the usual way, just add mixup. Our default alpha here is 0.4. I've been mainly playing with alpha at 0.2, so this is a bit more than I'm used to. But somewhere around that vicinity is pretty normal. So that's mixup. And it's really interesting, because you could use this for layers other than the input layer. You could use it on the first layer, maybe with the embeddings. So you could do mixup augmentation in NLP, for instance. It's something which people haven't really dug into deeply yet. But it seems to be an opportunity to add augmentation in many places where we don't really see it at the moment, which means we can train better models with less data, which is why we're here. So here's the problem. How does Softmax interact with this? So now we've drawn some random number, lambda. It's 0.7. So I've got 0.7 of a dog and 0.3 of a gas station. And the correct answer would be a rank 1 tensor, which has 0.7 in one spot and 0.3 in the other spot, and 0 everywhere else. Softmax isn't going to want to do that for me. Because Softmax really wants just one of my values to be high, because it's got an e to the top, as we've talked about. So to really use mixup well, and not just to use mixup well, but any time the labels on the data, you're not 100% sure they're correct, you don't want to be asking your model to predict one. You want to don't predict, I'm 100% sure it's this label, because you've got label noise. You've got incorrect labels, or you've got mixup mixing, or whatever. So instead, we say, oh, don't use one hot encoding for the dependent variable, but use a little bit less than one hot encoding. So say 0.9 hot encoding. So then the correct answer is to say, I'm 90% sure this is the answer. And then all of your probabilities have to add to 1. So then all of the negatives, you just put 0.1 divided by n minus 1 on all the rest. And that's called label smoothing. And it's a really simple, but astonishingly effective way to handle noisy labels. Like, I keep on hearing people saying, oh, we can't use deep learning in this medical problem, because the diagnostic labels in the reports are not perfect, and we don't have a gold standard, and whatever. It actually turns out that particularly if you lose label smoothing, noisy data is generally not an option. Like, there's plenty of examples of people using this where they literally randomly permute half the labels to make them like 50% wrong, and they still get good results, really good results. So don't listen to people in your organization saying, we can't start modeling until we do all this cleanup work. Start modeling right now. See if the results are OK. And if they are, then maybe you can skip all the cleanup work, or do them simultaneously. So label smoothing ends up just being the cross entropy loss, as before, times if epsilon is 0.1 and 0.9, plus 0.1 times the cross entropy for everything divided by n. And the nice thing is that's another linear combination. So once you kind of create one of these little mathematical refactorings that tend to pop up everywhere and make your code a little bit easier to read, and a little bit harder to stuff up. Every time I have to write a piece of code, there's a very high probability that I'm going to screw it up. So the less I have to write, the less debugging I'm going to have to do later. So we can just pop that in as a loss function, and away we go. So that's a super powerful technique, which it's been around for a couple of years, those two techniques, but not nearly as widely used as they should be. Then if you're using a Volta, Tensor Core, 2080, pretty much any current generation Nvidia graphics card, you can train using half precision floating point, in theory, like 10 times faster. In practice, it doesn't quite work out that way, because there's other things going on. But we certainly often see 3x speed ups. So the other thing we've got is some work here to allow you to train in half precision floating point. Now the reason it's not as simple as saying model.half, which would convert all of your weights and biases and everything to half precision floating point, is because of this. This is from Nvidia's materials. And what they point out is that you can't just use half precision everywhere, because it's not accurate. It's bumpy. So it's hard to get good, useful gradients if you do everything in half precision. And particularly often, things will round off to zero. So instead, what we do is we do the forward pass in FP16. We do the backward pass in FP16. So all the hard work is done in half precision floating point. And pretty much everywhere else, we convert things to full precision floating point and do everything else in full precision. So for example, when we actually apply the gradients by multiplying by the learning rate, we do that in FP32, single precision. And that means that if your learning rate's really small, in FP16, it might basically round down to zero. So we do it in FP32. In Fast AI version 1, we wrote all this by hand. For the lessons, we're experimenting with using a library from Nvidia called Apex. Apex basically have some of the functions to do this there for you. So we're using it here. And basically, you can see there's a thing called model to half, where we just go model.half. Batch norm goes to float, and so forth. So these are not particularly interesting. But they're just going through each one and making sure that the right layers have the right types. So once we've got those kind of utility functions in place, the actual callback's really quite small. And you'll be able to map every stage to that picture I showed you before. So you'll be able to see when we start fitting, we convert the network to half precision floating point, for example. One of the things that's kind of interesting is there's something here called loss scale. After the backward pass, well, probably more interestingly, after the loss is calculated, we multiply it by this number called loss scale, which is generally something around 512. The reason we do that is that losses tend to be pretty small in a region where half precision floating point's not very accurate. So we just multiply it by 512. Put it in a region that is accurate. And then later on, in the backward step, we just divide by that again. So that's a little tweak, but it's the difference we find generally between things working and not working. So the nice thing is now we have something which you can just add mixed precision and train, and you will get often 2x, 3x speed up, certainly on vision models, also on transformers, quite a few places. One obvious question is, is 512 the right number? And it turns out getting this number right actually does make quite a difference to your training. And so something slightly more recently is called dynamic loss scaling, which literally tries a few different values of loss scale to find out at what point does it become infinity. And so it dynamically figures out the highest loss scale we can go to. And so this version just has the dynamic loss scaling added. It's interesting that sometimes training with half precision gives you better results than training with FP32, because there's just a bit more randomness. Maybe it regularizes a little bit. But generally, it's super, super similar, just faster. We have a question about mixup. Great. Is there an intuitive way to understand why mixup is better than other data augmentation techniques? I think one of the things that's really nice about mixup is that it doesn't require any domain-specific thinking. Do we flip horizontally or also vertically? How much can we rotate? It doesn't create any kind of lossiness, like in the corners. There's no reflection padding or black padding. So it's kind of like quite nice and clean. It's also almost infinite in terms of the number of different images it can create. So you've kind of got this permutation of every image with every other image, which is already giant, and then in different mixes. So it's just a lot of augmentation that you can do with it. And there are other similar things. So there's another thing, which there's something called cutout, where you just delete a square and replace it with black. There's another one where you delete a square and replace it with random pixels. Something I haven't seen, but I'd really like to see people do is to delete a square and replace it with a different image. So I'd love somebody to try doing mixup. But instead of taking the linear combination, instead pick an alpha-sized, sorry, a lambda percent of the pixels, like in a square, and paste them on top. There's another one which basically finds four different images and puts them in four corners. So there's a few different variations. And they really get great results. And I'm surprised how few people are using them. So let's put it all together. So here's EmojNet. So let's use our random resize crop. A minimum scale of 0.35, we find, works pretty well. And we're not going to do any other other than flip. We're not going to do any other augmentation. And now we need to create a model. So far, all of our models have been boring, convolutional models. But obviously, what we really want to be using is a ResNet model. We have the XResNet, which there's some debate about whether this is the mutant version of ResNet or the extended version of ResNet. So you can choose what the X stands for. And basically, the XResNet is the bag of tricks, wherever it was. It's basically the bag of tricks ResNet. So they have a few suggested tweaks to ResNet. And here they are. So these are their little tweaks. So the first tweak is something that we've kind of talked about, and they call it ResNet C. And it's basically, hey, let's not do a big 7 by 7 convolution as our first layer, because that's super inefficient. And it's just a single linear model, which doesn't have much richness to it. So instead, let's do three comms in a row, 3 by 3. And so three 3 by 3 comms in a row, if you think about it, the receptive field of that final one is still going to be about 7 by 7. But it's got there through a much richer set of things that it can learn, because it's a three-layer neural net. So that's the first thing that we do in our XResNet. So here is XResNet. And when we create it, we set up how many filters are there going to be for each of the first three layers. So the first three layers will start with channels in, inputs. So that'll default to three, because normally we have three channel images. And the number of outputs that we'll use for the first layer will be that plus 1 times 8. Why is that? It's a bit of a long story. One reason is that that gives you 32 at the second layer, which is the same as what the bag of tricks paper recommends, as you can see. The second reason is that I've kind of played around with this quite a lot to try to figure out what makes sense in terms of receptive field. And I think this gives you the right amount. The times 8 is here because NVIDIA graphics cards like everything to be a multiple of 8. So if this is not 8, it's probably going to be slower. But one of the things here is now, if you have a one channel input, like black and white, or a five channel input, like some kind of hyperspectral imaging or microscopy, then you're actually changing your model dynamically to say, oh, if I've got more inputs, then my first layer should have more activations. Which is not something I've seen anybody do before. But it's a kind of really simple, nice way to improve your ResNet for different kinds of domains. So that's the number of filters we have for each layer. So our stem, so the stem is the very start of a CNN. So our stem is just those three conv layers. So that's all the paper says. What's a conv layer? A conv layer is a sequential containing a bunch of layers, which starts with a conv of some stride, followed by a batch norm, and then optionally, followed by an activation function. And our activation function, we're just going to use ReLU for now because that's what they use in the paper. The batch norm, we do something interesting. This is another tweak from the bag of tricks, although it goes back a couple more years than that. We initialize the batch norm sometimes to have weights of 1 and sometimes to have weights of 0. Why do we do that? Well, all right. Have a look here at ResNet D. This is a standard ResNet block. This path here, normally, it doesn't have the conv and the average pool. So pretend they're not there. We'll talk about why they're there sometimes in a moment. But this is just the identity. And the other goes 1 by 1 conv, 3 by 3 conv, 1 by 1 conv. And remember, in each case, it's conv batch norm ReLU, conv batch norm ReLU. And then what actually happens is it then goes conv batch norm, and then the ReLU happens after the plus. There's another variant where the ReLU happens before the plus, which is called preact or preactivation ResNet. Turns out it doesn't work quite as well for smaller models. So we're using the non-preact version. Now, see this conv here? What if we set the batch norm layer weights there to 0? What's going to happen? Well, we've got an input. This is identity. This does some conv, some conv, some conv, and then batch norm where the weights are 0. So everything gets multiplied by 0. And so out of here comes 0. So why is that interesting? Because now we're adding 0 to the identity block. So in other words, the whole block does nothing at all. That's a great way to initialize a model, right? Because we really don't want to be in a position, as we've seen, where if you've got 1,000 layers deep model, that any layer is even slightly changing the variance, because they kind of cause the gradients to spiral off to 0 or to infinity. This way, literally, the entire activations are the same all the way through. So that's what we do. We set the 1, 2, 3 third conv layer to have 0 in that batch norm layer. And this lets us train very deep models at very high learning rates. You'll see nearly all of the academic literature about this talks about large batch sizes, because of course, academics, particularly at big companies like Google and OpenAI and Nvidia and Facebook, love to show off their giant data centers. And so they like to say, oh, if we do 1,000 TPUs, how big a batch size can we create? But for us normal people, these are also interesting, because the exact same things tell us how higher learning rate can we go. So the exact same things that let you create really big batch sizes. So you do a giant batch, and then you take a giant step. Well, we could just take a normal sized batch, but a much bigger than usual step. And by using higher learning rates, we train faster, and we generalize better. And so that's all good. So this is a really good little trick. OK, so that's conv layer. So there's our stem. And then we're going to create a bunch of res blocks. So a res block is one of these, except this is an identity path, right? Conv, conv, conv. Unless we're doing a resnet 34 or a resnet 18, in which case one of these cons goes away. So a resnet 34 and resnet 18 only have two cons here, and resnet 50 onwards have three cons. And then in resnet 50 and above, the second conv, they actually squish the number of channels down by four. And then they expand it back up again. So it could go like 64 channels to 16 channels to 64 channels. Let's call it a bottleneck layer. So a bottleneck block is the normal block for larger resnets. And then just two 3 by 3 cons is the normal for smaller resnets. So you can see in our res block that we pass in this thing called expansion. It's either 1 or 4. It's 1 if it's a resnet 18 or 34, and it's 4 if it's bigger. And so if it's expansion equals 1, then we just add one extra conv. Oh, sorry, the first conv is always a 1 by 1. And then we add a 3 by 3 conv. Or if expansion equals 4, we add two extra cons. So that's what the res blocks are. Now, I mentioned that there's two other things here. Why are there two other things here? Well, we can't use standard res blocks all the way through our model, can we? Because a res block can't change the grid size. We can't have a stride 2 anywhere here. Because if we had a stride 2 somewhere here, we can't add it back to the identity because they're now different sizes. Also, we can't change the number of channels. Because if we change the number of channels, we can't add it to the identity. So what do we do? Well, as you know, from time to time, we do like to throw in a stride 2. And generally, when we throw in a stride 2, we like to double the number of channels. And so when we do that, we're going to add to the identity path two extra layers. Well, at an average pooling layer, that's going to cause the grid size to shift down by 2 in each dimension. And we'll add a 1 by 1 conv to change the number of filters. So that's what this is. And this particular way of doing it is specific to the x resnet. And it gives you a nice little boost over the standard approach. And so you can see that here. If the number of inputs is different to the number of filters, then we add an extra conv layer. Otherwise, we just do no op, no operation, which is defined here. And if the stride is something other than 1, we add an average pooling. Otherwise, it's a no op. And so here is our final resnet block calculation. So that's the res block. So tweak for resnet d is this way of doing the, they call it, a down sampling path. OK. And then the final tweak is the actual ordering here of where the stride 2 is. Normally, the stride 2 in normal resnet is at the start. And then there's a 3 by 3 after that. Doing a stride 2 on a 1 by 1 conv is a terrible idea, because you're literally throwing away 3 quarters of the data. And it's interesting, it took people years to realize they're literally throwing away 3 quarters of the data. So the bag of tricks folks said, let's just move the stride 2 to the 3 by 3. And that makes a lot more sense, right? Because the stride 2 3 by 3, you're actually hitting every pixel. So the reason I'm mentioning these details is so that you can read that paper and spend time thinking about each of those resnet tweaks. Do you understand why they did that? It wasn't some neural architecture search, try everything brainless, use all our computers approach. It was, let's sit back and think about how do we actually use all the inputs we have? And how do we actually take advantage of all the computation that we're doing? So it's a very, most of the tweaks is stuff that exists from before. And they've cited all those. But if you put them all together, it's just a nice, here's how to think through. Architecture design. And that's about it, right? So we create a resnet block for every res layer. And so here it is, creating the resnet block. And so now we can create all of our resnets by simply saying this is how many blocks we have in each layer. So resnet 18 is just 2, 2, 2, 2, 2, 34 is 3, 4, 6, 3. And then secondly is changing the expansion factor, which as I said for 18 and 34 is 1. And for the bigger ones is 4. So that's a lot of information there. And if you haven't spent time thinking about architecture before, it might take you a few reads and lessons to put the sink in. But I think it's a really good idea to try to spend time thinking about that and also to experiment and try to think about what's going on. The other thing to point out here is that this, the way I've written this, it's like this is the whole resnet. Other than the definition of conv layer, this is the whole resnet. It fits on a screen. And this is really unusual. Most resnets you see, even without the bag of tricks, 500, 600, 700 lines of code. And if every single line of code has a different arbitrary number at 16 here and 32 there and average pool here and something else there, how are you going to get it right? And how are you going to be able to look at it and say, what if I did this a little bit differently? So for research and for production, you want to get your code refactored like this for your architecture so that you can look at it and say, what exactly is going on? Is it written correctly? OK, I want to change this stride to being in a different layer. How do I do it? It's really important for effective practitioners to be able to write nice, concise architectures so that you can change them and understand them. OK, so that's our X resnet. We can train it with or without mix up. It's up to us. Label smoothing cross-entropy is probably always a good idea unless you know that your labels are basically perfect. Let's just create a little resnet 18. And let's check out to see what our model is doing. So we've already got a model summary, but we're just going to rewrite it to use the new version of Learner that doesn't have runner anymore. And so we can print out and see what happens to our shapes as they go through the model. And you can change this print mod here to true, and it'll print out the entire blocks and then show you what's going on. So that would be a really useful thing to help you understand what's going on in the model. All right, so here's our architecture. It's nice and easy. We can tell you how many channels are coming in, how many channels are coming out, and it'll adapt automatically to our data that way. So we can create our Learner. We can do our LR find. And now that we've done that, let's create a one cycle learning rate annealing. So a one cycle learning rate annealing. We've seen all this before. We keep on creating these things like 0.3, 0.7 for the two phases or 0.3, 0.2, 0.5 for three phases. So I added little create phases that will build those for us automatically. This one we built before. So here's our standard one cycle annealing. And here's our parameter scheduler. And so one other thing I did last week was I made it that callbacks, you don't have to pass to the initializer. You can also pass them to the fit function. And it'll just run those callbacks to the fit functions. This is a great way to do parameter scheduling. And there we go. And so 83.2. So I would love to see people beat my benchmarks here. So here's the ImageNet site. And so so far, the best I've got for 128.5 epochs is 84.6. So yeah, we're super close. So maybe with some fiddling around, you can find something that's even better. And with these kind of leaderboards where a lot of these things can train in, this is 2 and 1 1<|id|><|translate|> This is 2 and 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 minutes on a standard I think it was a GTX 1080 Ti you can quickly try things out and what I've noticed is that the results I get in 5 epochs on 128 pixel image net models carry over a lot to image net training or bigger models so like you can learn a lot by not trying to train giant models so compete on this leaderboard to become a better practitioner to try out things right and if you do have some more time you can go all the way to 400 epochs that might take a couple of hours and then of course also we've got image wharf which is just doggy photos and it's much harder and actually this one I find an even better test case because it's a more difficult data set so we've got a 90% is my best for this so I hope somebody can can beat me I really do. So we can refactor all that stuff of adding all these different callbacks and stuff into a single function called CNN learner and we can just pass in an architecture and our data and our loss function and optimization function and what kind of callbacks do we want just yes or no and we'll just set everything up. And if you don't pass in see it and see out will grab it from your data for you and then we'll just pass that off to the. So that makes things easier so now if you want to create a CNN it's just one line of code. Adding in whatever we want mix up labels moving blah blah blah. And so we get the same result when we fit it. So we can see this all put together. In this image net training script which is in fast AI in examples slash train image net and this entire thing will look entirely familiar to you it's all stuff that we've now built from scratch with one exception which is this bit which is using multiple GPU's so we're not covering that but that's just a. You know an acceleration tweak and you can easily use multiple GPU's by simply doing data parallel or two distributed. Other than that yeah this is all stuff that you see in those labels moving cross entropy. As mix up. Here's something we haven't written save the model after every epoch maybe when I write that one that would be a good exercise. So what happens if we try to train this for just 60 epochs. This is what happens so benchmark results on image net these are all the Keras and pipe torch models it's very hard to compare them because they have different input sizes so we really should compare the ones with our input size which is 224 so a standard. Resnet oh scroll off the screen. So Resnet 50 so bad it's actually scrolled off the screen so let's take Resnet 101 as a 93.3% accuracy so that's twice as many layers as we used and it was also trained for 90 epochs so trained for 50% longer 93.3 when I train this on image net I got 94.1. So this like. Extremely simple architecture that fits on a single screen and was built entirely using common sense trained for just 60 epochs actually gets us even above Resnet 152. Because that's 93.8 we've got 94.1 so the only things above it were trained on much much larger images and also like nasnet large is so big I can't train it I just keep on training it. And also like nasnet large is so big I can't train it I just keep on running out of memory and time and inception Resnet version 2 is really really fiddly and also really really slow so we've now got you know this beautiful nice Resnet extra is that 50 model which. You know is built in this very first principles common sense way and gets astonishingly great results so you know I really don't think. We all need to be running to neural architecture search and hyper parameter optimization and blah blah blah we just need to use you know good common sense thinking. So I'm super excited to see how well that worked out. So now that we have a nice model we want to be able to do transfer learning so how do we do transfer learning I mean you all know how to do transfer learning but let's do it from scratch. So what I'm going to do is I'm going to transfer learn from image wolf to the pets data set that we used in lesson one. That's our goal so we start by grabbing image wolf we do the standard data block stuff. Let's use labels moving cross entropy notice how we're using all the stuff we've built this is our Adam optimizer this is our label smoothing cross entropy this is the data blocks API we wrote so this is that we're still not using anything from faster I V1. This is all stuff that if you want to know what's going on you can go back to that previous lesson and see what did we build and how did we build it and step through the code there's a CNN learner that we just built in the last notebook. These five lines of code I got sick of typing so let's dump them into a single function called schedule one cycle going to create our phases it's going to create our momentum annealing and a learning rate annealing and create our schedulers. So now with that we can just say schedule one cycle with a learning rate what percentage of the epochs are at the start batches I should say at the start and we can go ahead and fit so I thought okay the transfer learning we should try and fit a decent model. So I did 40 epochs at 11 seconds per epoch on a 1080 TI so a few minutes later. So a few minutes later we've got 79.6% accuracy which is pretty good you know training from scratch for 10 different dog breeds with a resident 18. So let's try and use this to create a good pets model. It's going to be a little bit tricky because the pets data set has cats as well and this model's never seen cats and also this model has only been trained on I think less than 10,000 images so it's kind of unusually small thing that we're trying to do here so it's an interesting experiment to see if this works. So the first thing we have to do is we have to save the model so that we can load it into a pets model so when we save a model. What we do is we grab its state dict now we actually haven't written this but it would be like three lines of code if you want to write it yourself because all it does is it literally creates a dictionary and order dict is just a Python standard library dictionary that has an order where the keys are just the names of all the layers and for sequential the index of each one. And then you can look up say 10 dot bias and it just returns the weights okay so you can easily turn a module into a dictionary and so then we can create somewhere to save a model and torch dot save will save that dictionary you can actually just use pickle here. Works fine and actually behind the scenes torch dot save is using pickle but they kind of like add some header to it to say like it's basically a magic number that when they read it back they make sure it is a pie torch model file and that it's the right version and stuff like that but you can totally use pickle. And so the nice thing is now that we know that the thing we've saved is just a dictionary so you can fiddle with it right if you have trouble loading something in the future just open up just go torch dot load put it into a dictionary and look at the keys and look at the values and see what's going on. So let's try and use this for pets so we've seen pets before so the nice thing is that we've never used pets in part 2 but our data blocks API totally works. And in this case the there's one images directory that contains all the images and there isn't a separate validation set directory so we can't use that label with sorry yeah label with so split with grandparent thing so we're going to have to split it randomly but remember how we've already created split by funk so let's just write a function that returns. True or false. Depending on whether some random number is. Large or small. And so now. We can just pass that to your split by funk and. We're done. And we're done. So the nice thing is when you kind of understand the what's going on behind the scenes it's super easy for you to customize things and fast AIV one is basically identical there's a split by funk that you do the same thing for. So now that split into training and validation. And you can see how nice it is that we created that Dundas repress so that we can print things out so easily to see what's going on so that's if something doesn't have a nice representation you should like monkey patch in a Dundas repress so you can print out what's going on. Now we have to label it so we can't label it by folder because they're not put into folders instead we have to look at the file name. So let's grab one file name so I had to build all this stuff in a jupyter notebook just interactively to see what's going on. So in this case we'll grab one name and then let's try to construct a regular expression that grabs just the doggy's name from that and once we've got it we can now turn that into a function and we can now go ahead and use that category processor we built last week to label it and there we go. There's all the kinds of doggy we have not just doggies now doggies and kitties. Okay so now we can train from scratch pets 37% not great. So maybe with transfer learning we can do better. So transfer learning we can read in that image woof model and then we will customize it for pets. So let's create a CNN for pets this is now the pets data bunch. But let's tell it to create a model with 10 filters out 10 activations at the end because remember image woof has 10 types of dog 10 breeds. So to load in the pre train model we're going to need to ask for a learner with 10 activations. So that is something we can now grab our state dictionary that we saved earlier and we can load it into our model. So this is now an image woof model but the learner for it is pointing at the pets data bunch. So what we now have to do is remove the final linear layer and replace it with one that has the right number of activations to handle all these which I think is 37 pet breeds. So what we do is we look through all the children of the model and we try to find the adaptive average pooling layer because that's that kind of penultimate bit and we grab the index of that. And then let's create a new model that has everything up to but not including that bit. So this is everything before the adaptive average pooling. So this is the body. So now we need to attach a new head to this body which is going to have 37 activations in the linear layer instead of 10. Which is a bit tricky because we need to know how many inputs are going to be required in this new linear layer. The number of inputs will be however many outputs come out of this. So in other words just before the average pooling happens in the X ResNet how many activations are there? How many channels? How many channels? Well there's an easy way to find out. Grab a batch of data. Put it through a cut down model and look at the shape. And the answer is there's 512. OK. So we've got a 128 mini batch of 512 4 by 4 activations. So that pred dot shape one is the number of inputs to our head. And so we can now create our head. This is basically it here our linear layer. But remember we tend to not just use a max pool or just an average pool. We tend to do both and concatenate them together. Which is something we've been doing in this course forever. But a couple of years somebody finally did actually write a paper about it. So I think this is actually an official thing now. And it generally gives a nice little boost. So our linear layout needs twice as many inputs because we've got two sets of pooling we did. So our new model contains the whole head plus an adaptive concat pooling, flatten and our linear. And so let's replace the model with that new model we created and fit. And look at that 71 percent by fine tuning versus 37 percent training from scratch. So that looks good. So we have a simple transfer learning working. So what I did then is I do this in Jupyter all the time. I basically grabbed kind of all the cells. I hit C to copy and then I hit V to paste. And then I grabbed the model and I hit shift M to merge. And chucked a function header on top. So now I've got a function that does all this. So these are all the lines you saw just before. And I've just stuck them all together into a function. I call it adapt model. It's going to take a learner and adapt it for the new data. So these are all the lines of code you've already seen. And so now we can just go CNN learner, load the state dict, adapt the model and then we can start training. But of course what we really like to do is to first of all train only the head. So let's grab all the parameters in the body. And remember when we did that nn.sequential, the body is just the first thing. That's the whole ResNet body. So let's grab all the parameters in the body and set them to requires grad equals false. So it's frozen. And so now we can train just the head and we get 54 percent, which is great. So now we, as you know, unfreeze. Okay. And train some more. Uh oh. So it's better than not fine tuning. But interestingly it's worse, 71 versus 56. It's worse than the kind of naive fine tuning where we didn't do any freezing. So what's going on there? So what's going on there? Anytime something weird happens in your neural net, it's almost certainly because of batch norm. Because batch norm makes everything weird. And that's true here too. What happened was our frozen part of our model, which was designed for ImageWolf, those layers were tuned for some particular set of mean and standard deviations. Because remember the batch norm is going to divide by the, subtract the mean and divide by the standard deviation. But the PETS data set has different means and standard deviations. Not for the input, but inside the model. So then when we unfroze this, it basically said like, you know, this final layer was getting trained for everything being frozen. But that was for a different set of batch norm statistics. So then when we unfroze it, everything tried to kind of catch up. And it would be very interesting to look at the kind of histograms and stuff that we did earlier in the course and like see what's really going on. Because I haven't really seen anybody, I haven't really seen a paper about this. It's something we've been doing in Fast AI for a few years now. But I think this is the first course where we've actually drawn attention to it. That's kind of something that's been hidden away in the library before. But as you can see, it's a huge difference, right? The difference between 56 versus 71. So the good news is it's easily fixed. And the trick is to not freeze all of the body parameters, but freeze all of the body parameters that aren't in the batch norm layers. And that way, when we fine tune the final layer, we're also fine tuning all of the batch norm layers, weights and biases. So we can create, just like before, adapt the model. And let's create something called set gradient, which says, oh, if it's a linear layer at the end or a batch norm layer in the middle, return. Don't change the gradient. Otherwise, if it's got weights, set requires grad two, whatever you asked for, which we're going to start false. Here's a little convenient function that will apply any function you pass to it recursively to all of the children of a model. So now that we have apply to a model, or apply to a module, I guess, we can just pass in a module and that will be applied throughout. So this way we freeze just the non-batch norm layers and, of course, not the last layer. And so actually fine tuning immediately is a bit better, goes from 54 to 58. But more importantly, then when we unfreeze, we're back into the 70s again. So this is just a super important thing to remember if you're doing fine tuning. And I don't think there's any library other than Fast AI that does this, weirdly enough. So if you're using TensorFlow or something, you'll have to write this yourself to make sure that you don't freeze ever, don't ever freeze the weights in the batch norm layers anytime you're doing partial layer training. Oh, by the way, that apply mod, I only wrote it because we're not allowed to use stuff in PyTorch, but actually PyTorch has its own. It's called model.apply, so you can use that now. It's the same thing. Okay, so finally for this half of the course, we're going to look at discriminative learning rates. So for discriminative learning rates, there's a few things we can do with them. One is it's a simple way to do layer freezing without actually worrying about setting requires grad. We could just set the learning rate to zero for some layers. So let's start by doing that. So what we're going to do is we're going to split our parameters into two or more groups. With a function. Here's our function. It's called bnSplitter. It's going to create two groups of parameters. And it's going to pass the body to underscore bnSplitter. Which will recursively look for batch norm layers and put them in the second group. Or anything else with a weight goes in the first group. And then do it recursively. And then also the second group will add everything after the head. So this is basically doing something where we're putting all our parameters into the two groups we want to treat differently. So we can check, for example, that when we do bnSplitter on a model, that the number of parameters in the two halves is equal to the total number of parameters in the model. And so now I want to check this works. I want to make sure that if I pass this, because we now have a splitter function in the learner. And that's another thing I added this week. That when you start training. It's literally just this. When we get our credit optimizer, it passes the model to self.splitter. Which by default does nothing at all. And so we're going to be using our bnSplitter to split it into multiple parameter groups. And so how do we debug that? How do we make sure it's working? Because this is one of these things that if I screw it up, I probably won't get an error. But instead it probably won't train my last layer. It'll train all the layers at the same learning rate. Or it would be hard to know if the model was bad because I screwed up my code or not. So we need a way to debug it. We can't just look inside and make sure it's working. Because what we're going to be doing is we're going to be passing it. Let's see this one. We're going to be. We're going to be passing it to the splitter parameter when we create the learner. Right. So after this, it set the splitter parameter. And then when we start training, we're hoping that it's going to create these two layer groups. So we need some way to look inside the model. So of course we're going to use a callback. And this is something that's super cool. Do you remember how I told you that you can actually override Dundercall itself? You don't just have to override a specific callback. And by overriding Dundercall itself, we can actually say, which callback do we want to debug? And when we hit that callback, please run this function. And if you don't pass in a function, it just jumps into the debugger as soon as that callback is hit. Otherwise, call the function. So this is super handy, right? Because now I can create a function called print details that just prints out how many parameter groups there are and what the hyperparameters there are. And then immediately raises the cancel training exception to stop. And so then I can fit with my discriminative LR scheduler and my debug callback and my discriminative LR scheduler. Is something that now doesn't just take a learning rate, but an array of learning rates and creates a scheduler for every learning rate. And so I can pass that in. So I'm going to use 0 and 0.02. So in other words, no training for the body and 0.03 for the head and the batch norm. And so as soon as I fit, it immediately stops because the cancel training exception was raised. And it prints out and it says there's two parameter groups, which is what we want. And the first parameter group has a learning rate of 0, which is what we want. And the second is 0.003, which is right because it's 0.03 and we're using the learning rate scheduler. So it starts out 10 times smaller. So this is just a way of saying like, if you're anything like me, every time you write code, it will always be wrong. And for this kind of code, you won't know it's wrong. And you could be writing a paper or doing a project at work or whatever in which you're not using discriminative learning rates at all because of some bug. Because you didn't know how to check. So make sure you can check and always assume that you screw up everything. Okay, so now we can train with zero learning rate on the first layer group. And then we can use discriminative learning rates with one in neg three and one in neg two and train a little bit more. And that all works. Okay, so that's all the tweaks we have. Any questions, Rachel? We had two tangential questions come up. They're my favorite. The first is we heard that you're against cross validation for deep learning. We heard that you're against cross validation for deep learning and wanted to know why that is. And the second question. Okay. So cross validation is a very useful technique for getting a reasonably sized validation set if you don't have enough data to otherwise create a reasonably sized validation set. So it's particularly popular in the days when most studies were like 50 or 60 rows. If you've got a few thousand rows, it's just pointless, right? Like the kind of statistical significance is going to be there regardless. So I wouldn't say I'm against it. Just most of the time you don't need it because if you've got a thousand things in the validation set and you only care whether it's like plus or minus 1%, it's totally pointless. So yeah, have a look and see how much your validation set accuracy is varying from run to run. And if it's too much that you can't make the decisions you need to make, then you can add cross validation. And what are your best tips for debugging deep learning? So Chris Latner asked me this today as well, actually. So I'll answer the same I answered him, which is don't make mistakes in the first place. And the only way to do that is to make your code so simple that it can't possibly have a mistake and to check every single intermediate result along the way to make sure it doesn't have a mistake. Otherwise, your last month might have been like my last month. What happened in my last month? Well, a month ago, I got 94.1% accuracy on ImageNet and I was very happy. And then I started a couple of weeks ago trying various tweaks and none of the tweaks seemed to help. And after a while, I got so frustrated, I thought I'll just repeat the previous training to see if it was like what was going on with the fluke. And I couldn't repeat it. I was now getting 93.5 instead of 94.1. And I trained it like a bunch of times. And every time I trained it, it was costing me $150 of AWS credits. So I wasn't thrilled about this. And it was six hours of waiting. So that was quite a process to even realize like it's broken. This is the kind of thing like when something when you've written that kind of code wrong, it gets broken in ways you don't even notice. It was broken for weeks in fast AI and nobody noticed. So eventually I realized, yeah, I mean, so the first thing I'll say is you've got to be a great scientist, which means you need a journal notebook, right? You need to keep track of your journal results. So I had a good journal. I pasted everything that was going on, all my models into a file. So I went back and I confirmed it really was 94.1. I could see exactly when it was. And so then I could revert to the exact commit that was in fast AI at that time. And I reran it and I got 94.1. So I now had to figure out which change in the previous month of the entire fast AI code base caused this to break. So the first thing I tried to do is try to find a way to quickly figure out whether something is broken. But after doing a few runs and plotting them in Excel, it was very clear that the training was identical until epoch 50. So until epoch 50 out of 60. So there was no shortcut. And so I did a bisection search one module at a time, looking through the 15 modules that had changed in that diff until I eventually I find it was in the mixed precision module. And then I went through each change that happened in the mixed position module. So like $5,000 later, I finally found the one line of code where we had forgotten to write the four letters dot opt. And so by failing to write dot opt, it meant that we were wrapping an optum wrapper in an optum wrapper rather than wrapping an optum wrapper with an optimizer. And that meant that weight decay was being applied twice. So that tiny difference was so insignificant that no one using the library even noticed it wasn't working. I didn't notice it wasn't working until I started trying to get state of the art results on ImageNet in the 60 epochs with ResNet 50. So yeah, I mean, debugging is hard and worse still is most of the time you don't know. So I mean, honestly, training models sucks and deep learning is a miserable experience and you shouldn't do it. But on the other hand, it gives you much better results than anything else and it's taking over the world. So it's either that or get eaten by everybody else, I guess. So yeah, I mean, it's so much easier to write normal code where like, oh, you have to implement OAuth authentication in your web service. And so you go in and you say, oh, here's the API and we have to take these five steps. And after each one, I check that this has happened and you check off each one. And at the end, you're done and you push it and you have integration tests and that's it. Even testing, it requires a totally different mindset. So you don't want reproducible tests. You want tests with randomness. You want to be able to see if something's changing just occasionally. Because if it only tests, if it tests correctly all the time with a random seed of 42, you're sure it's going to work with a random seed of 41. So you want like non-reproducible tests. You want randomness. You want tests that aren't guaranteed to always pass, but like the accuracy of this integration test should be better than 0.9 nearly all the time. You want to be warned if something looks off. And this means it's a very different software development process. Because if you push something to the Fast AI repo and a test fails, it might not be your fault. It might be that Jeremy screwed something up a month ago and one test fails one out of every thousand times. So as soon as that happens, then we try to write a test that fails every time. So like once you realize there's a problem with this thing, you try to find a way to make it fail every time. But it's yeah, debugging is difficult and in the end you just have to go through each step, look at your data, make sure it looks sensible, plot it, and try not to make mistakes in the first place. Great. Well, let's have a break and see you back here at 7.55. So we've all done ULM fit in part one. And there's been a lot of stuff happening in the oh, okay, let's do the question. What do you mean by a scientific journal? Ah, yeah, that's a good one. This is something I'm quite passionate about. When you look at the great scientists in history, they all, that I can tell, had careful scientific journal practices. In my case, my scientific journal is a file and a piece of software called Windows Notepad, and I paste things into it at the bottom. And when I want to find something, I press Ctrl F. It just needs to be something that has a record of what you're doing and what the results of that are. Because scientists who make breakthroughs generally make the breakthrough because they look at something that shouldn't be, and they go, oh, that's odd. I wonder what's going on. So the discovery of the noble gases was because the scientists saw like one little bubble left in the beaker, which they were pretty sure there shouldn't have been a little bubble there anymore. You know, most people would just be like, oops, there's a bubble, or we wouldn't even notice, but they studied the bubble and they found noble gases. Or penicillin was discovered because of, oh, that's odd. And I find in deep learning, this is true as well. I spent a lot of time studying batch normalization and transfer learning because a few years ago in Keras, I was getting terrible transfer learning results for something I thought should be much more accurate. And I thought, oh, that's odd. And I spent weeks changing everything I could and then almost randomly tried changing batch norm. So the problem is that all this fiddling around, you know, 90% of it doesn't really go anywhere, but it's the other 10% that you won't be able to pick it out unless you can go back and say, like, OK, that really did happen. I copied and pasted the log here. So that's all I mean. Are you also linking to your GitHub commits and datasets? No, because I've got the date there and the time. So I know the GitHub commit. So I do make sure I'm pushing all the time. So, yeah. OK. Yeah, so there's been a lot happening in NLP transfer learning recently. The famous GPT-2 from OpenAI and BERT and stuff like that. Lots of interest in transformers, which we will cover in a future lesson. One could think that LSTMs are out of favor and not interesting anymore. But when you look at actually recent competitive machine learning results, you see ULMfit beating BERT. Now, I should say this is not just ULMfit beating BERT. The guys at MWaves are super smart, amazing people. So it's like two super smart, amazing people using ULMfit, but some other people doing BERT. It's definitely not true that RNNs are in the past. I think what's happened is, in fact, as you'll see, transformers and CNNs for text have a lot of problems. They basically, they don't have state. So like if you're doing speech recognition, every sample you look at, you have to do an entire analysis of all the samples around it again and again and again. Like it's ridiculously wasteful. Or else RNNs have state. But they're fiddly. And they're hard to deal with, as you'll see, when you want to actually do research and change things. But partly RNNs have state, but also partly RNNs are the only thing which has had the level of carefulness around regularization. That AWD LSTM did. So Stephen Meridy looked at all the ways I can regularize this model and came up with a great set of hyperparameters for that. And there's nothing like that outside of the RNN world. So at the moment, my go-to choice definitely is still ULM fit for most real world NLP tasks. And if people find BERT or GPT-2 or whatever better for some real world tasks, that would be fascinating. I would love that to happen. But I haven't been hearing that from people that are actually working in industry yet. I'm not seeing them win competitive machine learning stuff and so forth. So I still think RNNs should be our focus. But we will also learn about transformers later. And so ULM fit is just the normal transfer learning path applied to an RNN, which could be on text. But interestingly, there's also been a lot of state of the art results recently on genomics applications. And on chemical bonding analysis and drug discovery. There's lots of things that are sequences. And it turns out, and we're still just at the tip of the iceberg, right? Because most people that are studying like drug discovery or chemical bonding or genomics have never heard of ULM fit, right? So it's still the tip of the iceberg. But those who are trying it are consistently getting breakthrough results. So I think it's really interesting, not just for NLP, but for all kinds of sequence classification tasks. So the basic process is going to be create a language model on some large data set. And notice a language model is a very general term. It means predict the next item in the sequence. So it could be an audio language model that predicts the next sample in a piece of music or speech. It could be predicting the next genome in a sequence or whatever, right? So that's what I mean by language model. And then we fine tune it, that language model using our in-domain corpus, which in this case is going to be IMDB. And then in each case, we first have to pre-process our data sets to get them ready for using an RNN on them. Language models require one kind of pre-processing. Classification models require another one. And then finally, we can fine tune our IMDB language model for classification. So this is the process we're going to go through from scratch. So Sylvia has done an amazing thing in the last week, which is basically to recreate the entire ADWD, LSTM, and ULM fit process from scratch in the next four notebooks. And there's quite a lot in here. But a lot of it's kind of specific to text processing, and so some of it I might skip over a little bit quickly. But we'll talk about which bits are interesting. So we're going to start with the IMDB data set as we have before. And to remind you, it contains a training folder, an unsupervised folder, and a testing folder. So the first thing we need to do is we need to create a data blocks item list subclass for text. Believe it or not, that's the entire code. Because we already have a get files. So here's a get files with dot text. And all you have to do is override get to open a text file like so. And we're now ready to create an item list. So this is like the data blocks API is just so super easy to handle your domain. So if you've got genomic sequences or audio or whatever, this is basically what you'll need to do. So now we've got an item list with 100,000 things in it. We've got the train, the test, and the unsupervised. And we can index into it and see a text. So here's a movie review. And we can use all the same stuff that we've used before. So for the previous notebook, we just built a random splitter. So now we can use it on texts. So the nice thing about this decoupled API is that we can mix and match things and things just work. And we can see the representation of them. They just work. Okay. So we can't throw this movie review into a model. It needs to be numbers. And so as you know, we need to tokenize and numericalize this. So let's look at the details. We use spaCy for tokenizing. And we do a few things as we tokenize. One thing we do is we have a few pre-rules. These are bits of code that get run before tokenization. So for example, if we find br slash, we replace it with a new line. Or if we find a slash or a hash, we put spaces around it. If we find more than two spaces in a row, we just make it one space. Then we have these special tokens. And this is what they look like as strings, but we use symbolic names for them, mainly. And these different tokens have various special meanings. So for example, if we see some non-whitespace character more than three times in a row, we replace it with... this is really cool, right? In Python substitution, you can pass in a function. So rep.sub here is going to look for this, and then it's going to replace it with the result of calling this function, which is really nice. And so what we're going to do is we're going to stick in the tkrep special token. So this means that there was a repeating token. We're then going to put a number, which is how many times it repeated, and then the thing that was actually there. We'll do the same thing with words. There's a lot of bits of little crappy things that we see in texts that we replace, mainly HTML entities. And we call those our default pre-rules. And then this is our default list of special tokens. So for example, replace rep.ccccc would be xxrep4c. Or replace wrep, word, word, word, word, word, word, would be xxwrep5.word. Why? Well, think about the alternatives. So what if you read a tweet that said, this was amazing! So you could either treat those 28 exclamation marks as one token. And so now you have a vocab item that is specifically 28 exclamation marks. You'll probably never see that again, so it probably won't even end up in your vocab. And if it did, it's going to be so rare that you won't be able to learn anything interesting about it. But if instead we replaced it with xxrep28 exclamation mark, then this is just three tokens where it can learn that lots of repeating exclamation marks is a general concept that has certain semantics to it. So that's what we're trying to do in NLP, is we're trying to make it so that the things in our vocab are as meaningful as possible. And the nice thing is that because we're using an LSTM, we can have multi-word sequences and be confident that the LSTM will create some stateful computation that can handle that sequence. Another alternative is we could have turned the 28 exclamation marks into 28 tokens in a row, each one of the single exclamation mark. But now we're asking our LSTM to hang on to that state for 28 time steps, which is just a lot more work for it to do, and it's not going to do as good a job. So we want to make things easy for our models. That's what preprocessing is all about. So same with all caps. If you've got, I am shouting, then it's pretty likely that there's going to be exclamation marks after that. There might be swearing after that, like the fact that there's lots of capitalized words is semantic of itself. So we replace capitalized words with a token saying this is a capitalized word. And then we replace it with the lowercase word so we don't have a separate vocab item for capital am, capital shouting, capital every damn word in the dictionary. OK, same thing for mixed case. So I don't know, I haven't come across other libraries that do this kind of preprocessing. There's little bits and pieces in various papers, but I think this is a pretty good default set of rules. Notice that these rules have to happen after tokenization because they're happening at a word level. So we have default post rules. And then this one here adds a beginning of stream and an end of stream on either side of a list of tokens. Why do we do that? These tokens turn out to be very important because when your language model sees an end of stream character token meaning that's the end of a document, then it knows the next document is something new. So it's going to have to learn to kind of reset its state to say, oh, we're not talking about the old thing anymore. So we're doing Wikipedia, we were talking about Melbourne, Australia, oh, and now there's a new token, and we're talking about the Emmys. So when it sees EOS, it has to learn to kind of reset its state somehow. So you need to make sure that you have the tokens in place to allow your model to know that these things are happening. Tokenization is kind of slow because Spacey does it so carefully. I thought it couldn't possibly be necessary to do it so carefully because it just doesn't seem that important. So last year I tried removing Spacey and replacing it with something much simpler. My IMDB accuracy went down a lot. So actually it seems like Spacey's sophisticated parser-based tokenization actually does better. So at least we can try and make it fast. So Python comes with something called a process pool executor, which runs things in parallel, and I wrap it around with this little thing called parallel. And so here's my thing that runs, look, compose, appears everywhere, compose the prerules on every chunk, run the tokenizer, compose the post rules on every dock. That's processing one chunk. So run them all in parallel for all the chunks. So that's that. So this is a processor, which we saw last week, and this is a processor which tokenizes. And so we can try it out, so we can create one and try, here's a bit of text, and let's try tokenizing. And so you can see we've got beginning of stream, did int, so int is a token, comma is a token, xxmage, da1, so that was a capital D, and so forth. All right, so now we need to turn those into numbers, not just to have a list of words. We can turn them into numbers by numericalizing, which is another processor, which basically when you call it, we find out, do we have a vocab yet? Because numericalizing is just saying, what are all the unique words? And the list of unique words is the vocab. So if we don't have a vocab, we'll create it. Okay? And then after we create it, it's just a case of calling object to int on each one. So o2i is just a dictionary, right? Or if deprocessing is just grabbing each thing from the vocab, so that's just in the right. Okay, so we can tokenize, numericalize, run it for two and a half minutes. And so we've got the xobj is the thing which returns the object version, so as opposed to the numericalized version. And so we put it back together and this is what we have after it's been turned into numbers and back again. So since that takes a couple of minutes, good idea to dump the labeled list so that we can then load it again later without having to rerun that. All right, this is the bit which a lot of people get confused about, which is how do we batch up language model data? So here's a bit of text. It's very meta, it's a bit of text which is from this notebook. So the first thing we're going to do is we're going to say let's create some batch sizes, create a small one for showing you what's going on, six. So let's go through and create six batches, which is just all the tokens for each of those six batches. So here's in this notebook, we will go back over the example of is the first element of, so this is the first row and then of classifying movie reviews you started in part one, this is the second. Okay, so we just put it into six groups, right? And then let's say we have a BPTT of five, so it's kind of like our back prop through time sequence length of five. Then we can split these up into groups of five. And so that'll create three of them. In this notebook, we will go back over the example of classifying movie reviews we studied in part one. These three things then are three mini batches. And this is where people get confused because it's not that each one has a different bunch of documents. Each one has the same documents over consecutive time steps. This is really important. Why is it important? Because this row here in the RNN is going to be getting some state about this document. So when it goes to the next batch, it needs to use that state. And then it goes to the next batch, it needs to use that state. So from batch to batch, the state that it's building up needs to be consistent. That's why we do the batches this way. I wanted to ask if you did any other preprocessing such as removing stop words, stemming, or lemmatization? Yeah, great question. So in traditional NLP, those are important things to do. Removing stop words is removing words like ah and on. And the stemming is like getting rid of the ing suffix or stuff like that. It's kind of like universal in traditional NLP. It's an absolutely terrible idea. Never ever do this. Because the first question is why would you do it? Why would you remove information from your neural net which might be useful? And the fact is that it is useful. Like stop words, your use of stop words tells you a lot about what style of language. So you'll often have a lot less articles and stuff if you're really angry and speaking really quickly. The tense you're talking about is obviously very important, so stemming gets rid of it. So yeah, all that kind of stuff is in the past. You basically never want to do it. And in general, preprocessing data for neural nets, leave it as raw as you can is the rule of thumb. So for a language model, each mini batch is basically going to look something like this for the independent variable. And then the dependent variable will be exactly the same thing but shifted over by one word. So let's create that. This thing's called LM Preloader. It would actually be better off being called an LM data set. Why don't we do it right now? LM Preloader, LM data set. That's really what it is. Okay, so an LM data set is a data set for a language model. Remember that a data set is defined as something with a length and a get item. So this is a data set which you can index into it, and it will grab an independent variable and a dependent variable. And the independent variable is just the text from wherever you asked for for a BPTT. And the dependent variable is the same thing offset by one. So you can see it here. We can create a data loader using that data set. Remember that's how data loaders work. You pass them a data set. And now we have something that we can iterate through, grabbing a mini batch at a time. And you can see here, X is XXBOS well worth watching, and Y is just well worth watching. And then you can see the second batch, best performance to date. So make sure you print out things that all make sense. So that's stuff that we can all dump into a single function and use it again later and chuck it into a data bunch. So that's all we need for a data bunch for language models. We're also going to need a data bunch for classification. And that one's going to be super easy because we already know how to create data bunches for classification, because we've already done it for lots of image models. And for NLP, it's going to be exactly the same. So we create an item list. We split. We label. That's it. So the stuff we did for image is not different. The only thing we've added is two preprocessors. Question. What are the trade-offs to consider between batch size and back propagation through time? For example, BPTT 10 with BS 100 versus BPTT 100 with BS 10, both would be passing a thousand tokens at a time to the model. What should you consider when tuning the ratio? It's a great question. I don't know the answer. I would love to know. So try it, because I haven't had time to fiddle with it. I haven't seen anybody else experiment with it. So that would make a super great experiment. I think the batch size is the thing that lets it parallelize. So if you don't have a large enough batch size, it's just going to be really slow. But on the other hand, a large batch size with a short BPTT, depending on how you use it, you may end up ending up with less state that's being back propagated. So the question of how much that matters, I'm not sure. But when we get to our ULM fit classification model, I'll actually show you this, kind of where this comes in. OK, so here's a couple of examples of a document and a dependent variable. And what we're going to be doing is we're going to be creating data loaders for them. But we do have one trick here, which is that with images, our images were always, by the time we got to modeling, they were all the same size. Now, this is probably not how things should be. And we have started doing some experiments with training with rectangular images of different sizes, but we're not quite ready to show you that work because it's still a little bit fiddly. But for text, we can't avoid it. We've got different size texts coming in, so we have to deal with it. And the way we deal with it is almost identical to how actually we're going to end up dealing with when we do do rectangular images. So if you are interested in rectangular images, try and basically copy this approach. Here's the approach. We are going to pad each document by adding a bunch of padding tokens. So we just pick some arbitrary token, which we're going to tell PyTorch this token isn't text. It's just thrown in there because we have to put in something to make a rectangular tensor. If we have a mini-batch with a 1,000 word document and then a 2,000 word document and then a 20 word document, the 20 word document is going to end up with 1,980 padding tokens on the end. And as we go through the RNN, we're going to be totally pointlessly calculating on all these padding tokens. We don't want to do that. So the trick is to sort the data first by length. So that way, your first mini-batch will contain your really long documents and your last mini-batch will contain your really short documents. Each mini-batch will not contain a very wide variety of lengths of documents, so there won't be much padding and so there won't be much wasted computation. So we've already looked at samplers. If you've forgotten, go back to when we created our data loader from scratch and we actually created a sampler. And so here we're going to create a different type of sampler. And it is simply one that goes through our data, looks at how many documents is in it, creates the range from zero to the number of documents, sorts them by some key and returns that iterator, sorts them in reverse order. So we're going to use sort sampler, passing in the key, which is a lambda function that grabs the length of the document. So that way, our sampler is going to cause each mini-batch to be documents of similar lengths. The problem is we can only do this for validation, not for training, because for training we want to shuffle. And sorting would undo any shuffling because sorting is deterministic. So that's why we create something called sort-ish sampler. And the sort-ish sampler approximately orders things by length. So every mini-batch has things of similar lengths, but with some randomness. And the way we do this, the details don't particularly matter, but basically I've created this idea of a mega-batch, which is something that's 50 times bigger than a batch, and basically I sort those. And so you end up with these sort of like sorted mega-batches, and then I have random permutations within that. So you can see random permutations there and there. So you can look at the code if you care, the details don't matter. In the end, it's a random sort in which things of similar lengths tend to be next to each other, and the biggest ones tend to be at the start. So now we've got a mini-batch of numericalized, tokenized documents of similar lengths, but they're not identical lengths. And so you might remember the other thing when we first created a data loader, we gave it two things, a sampler and a collate function. And the collate function that we wrote simply said torch.stack, because all our images were the same size. So we could just literally just stick them together. We can't do that for documents because they're different sizes. So we've written something called pad collate. And what Sylvain did here was he basically said, let's create something that's big enough to handle the longest document in the mini-batch, and then go through every document and dump it into that big tensor, either at the start or at the end, depending on whether you said pad first. So now we can pass the sampler and the collate function to our data loader, and that allows us to grab some mini-batches, which as you can see contain padding at the end. And so here's our normal convenience functions that do all those things for us. And that's that. Okay. So that's quite a bit of pre-processing. And I guess the main tricky bit is this dealing with different lengths. And at that point, we can create our AWD LSTM. So these are just the steps we just did to create our data loader. And now we're going to create an RNN. So an RNN, remember, is just a multi-layer network. But it's a multi-layer network that could be very, very, very many layers. There could be, like, if it was a 2,000-word document, this is going to be 2,000 layers. So to avoid us having to write 2,000 layers, we used a for loop. And between every pair of hidden layers, we use the same weight matrix. That's why they're the same color. That's why we can use a for loop. The problem is, as we've seen, trying to handle 2,000 layers of neural net, we get vanishing gradients or exploding gradients. It's really, really difficult to get it to work. So what are we going to do? Because it's even worse than that, because often we have layers going into RNNs going into other RNNs. So we actually have stacked RNNs, which when we unstack them, it's going to be even more thousands of layers effectively. So the trick is, we create something called an LSTM cell. Rather than just doing a matrix multiply as our layer, we instead do this thing called an LSTM cell as our layer. This is it here. So this is a sigmoid function, and this is a tanch function. So the sigmoid function, remember, goes from 0 to 1, and kind of nice and smooth between the two. And the tanch function is identical to a sigmoid, except it goes from minus 1 to 1, rather than 0 to 1. So sigmoid is 0 to 1, tanch is minus 1 to 1. So here's what we're going to do. We're going to take our input, and we're going to have some hidden state, as we've always had in our RNNs. This is just our usual hidden state. And we're going to multiply our input by some weight matrix in the usual way. Then we're going to multiply our hidden state by some weight matrix in the usual way, and then we add the two together in the way we've done before for RNNs. And then we're going to do something interesting. We're going to split the result into four equal-sized tensors. So the first one quarter of the activations will go through this path. The next will go through this path. The next will go through this path. The next will go through this path. So what this means is we kind of have like four little neural nets, effectively. And so this path goes through a sigmoid, and it hits this thing called the cell. Now this is the new thing. So the cell, just like hidden state, is just a rank 1 tensor, or for a mini-batch, a rank 2 tensor. It's just some activations. And what happens is we multiply it by the output of this sigmoid. So the sigmoid can go between 0 and 1. So this gate has the ability to basically zero out bits of the cell state. So we have the ability to basically take this state and say, like, delete some of it. So we could look at some of these words or whatever in this LSTM and say, based on looking at that, we think we should zero out some of our cell state. And so now the cell state has been selectively forgotten. So that's the forget gate. We then add it to the second chunk, the second little mini-neuronet, which goes through sigmoid. So this is just our input. And we multiply it by the third one, which goes through a tensh. So this basically allows us to say, which bits of input do we care about? And then this gives us the numbers from minus 1 to 1, multiply them together. And this adds. So this is how do we update our cell state? So we add on some new state. And so now we take that cell state and we put it through another. Well, one thing that happens is it goes through to the next time step. And the other thing that happens is it goes through one more tensh to get multiplied by the fourth little mini-neuronet, which is the output. So this is the actual, this actually creates the output hidden state. So it looks like there's a lot going on, but actually it's just this. Right. So you've got one neural net that goes from input to hidden. It's a linear layer, one that goes from hidden to hidden. Each one is going to be four times the number of hidden, because after we compute it and add them together, the chunk splits it up into four equal size groups. Three of them go through a sigmoid. One of them goes through a tensh. And then this is just the multiply and add that you saw. So there's kind of like conceptually a lot going on in LSTM. And it's certainly worth doing some more reading about why this particular architecture. But one thing I will say is there's lots of other ways you can set up a layer which has the ability to selectively update and selectively forget things. For example, there's something called a GRU, which has one less gate. The key thing seems to be giving it some way to make a decision to forget things. Because if you do that, then it has the ability to not push state through all thousand time steps or whatever. So that's our LSTM cell. And so an LSTM layer, an LSTM, assuming we only have one layer, is just that for loop that we've seen before. And we're just going to call whatever cell we asked for. So we're going to ask for an LSTM cell. And you just loop through and see how the state, we could take the state and we update the state. So this you can see this is the classic deep learning. It's like an NN.sequential, right? It's looping through a bunch of functions that are updating itself. That's what makes it a deep learning network. So that's an LSTM. So that takes 105 milliseconds for a small net on the CPU. You could pop it onto CUDA, then it's 24 milliseconds on GPU. It's not that much faster because this loop, every time step, it's having to push off another kernel launch off to the GPU. And that's just slow, right? So that's why we use the built-in version. And the built-in version behind the scenes calls a library from Nvidia called cuDNN, which has created a C++ version of this. That it's about the same on the CPU, right? Not surprisingly, it's really not doing anything different. But on the GPU, goes from 24 milliseconds to 8 milliseconds. So it's dramatically faster. The good news is we can create a faster version by taking advantage of something in PyTorch called JIT. And what JIT does is it reads our Python and it converts it into C++ that does the same thing, CUDA C++ that does the same thing. It compiles it the first time you use it. And then it uses that compiled code. And so that way it can create an on GPU loop. And so the result of that is, again, pretty similar on the CPU, but on the GPU, 12 milliseconds. So, you know, not as fast as the cuDNN version, but certainly a lot better than our non-JIT version. So this seems like some kind of magic thing that's going to save our lives and not require us to have to come to the Swift for TensorFlow lectures. But I've got bad news for you. Trying to get JIT working has been honestly a bit of a nightmare. This is the third time we've tried to introduce it in this course. And the other two times we've just not gotten it working or we've gotten worse results. It doesn't work very well that often. And it's got a lot of weird things going on. Like, for example, if you decide to comment out a line, right, and then run it, you'll get this error saying, like, unexpected indent. Like, literally, it's not Python, right? So it doesn't even know how to comment out lines. It's this kind of, like, weird thing where they try to, like, it's heroic. And it's amazing that it works at all. But the idea that you could try and turn Python, which is so not C++, into C++ is really pushing it what's possible. So it's astonishing this works at all. And occasionally it might be useful, but it's very, very hard to use. And when something isn't as fast as you want, it's very, very hard to... You can't profile it. You can't debug it. Not in the normal ways. But, you know, obviously it will improve. It's pretty early days. It will improve. But the idea of trying to parse Python and turn it into C++, literally they're doing, like, string interpolation behind the scenes, is kind of trying to reinvent all the stuff that compilers already do, converting a language that was very explicitly not designed to do this kind of thing into one that does. And I just... I don't think this is the future. So I say for now, be aware that JIT exists. Be very careful in the short term. I've found places where it literally gives the wrong gradients. So it goes down a totally different auto grad path. And I've had models that trained incorrectly without any warnings because it was just wrong. So be very careful. But sometimes, like, for a researcher, if you want to play with different types of RNNs, this is your only option unless you write your own C++ or unless you try out Julia or Swift, I guess. Is there a question? Yeah. Why do we need torch.cuda.synchronize? Is it kind of a lock to synchronize CUDA threads or something? Yeah, this is something that thanks to Tom on the forum for pointing this out. It's just when we're timing, without the synchronize, it's... Let's find it. So I just created a little timing function here. Without the synchronize, the CUDA thing will just keep on running things in the background, but will return... It will let your CPU thread keep going. So it could end up looking much faster than it actually is. So synchronize says, don't keep going in my Python world until my CUDA world is finished. Okay. So now we need dropout. And this is the bit that really is fantastic about AWD LSTM, is that Stephen Meridy thought about all the ways in which we can regularize a model. So basically dropout is just Bernoulli random noise. So Bernoulli random noise simply means create ones and zeros, and it's one with this probability. Right? So create a bunch of random ones and zeros, and then divide by one minus P. So that makes them, in this case, 0.5, it's randomly zeros and twos. And the reason they're zeros and twos is because that way the standard deviation doesn't change. So we can remove dropout for inference time, and the activations will be still scaled correctly. And we talked about that a little bit in part one. And so now we can create our RNN dropout. And one of the nifty things here is the way that Sylvia wrote this is you don't just pass in the thing to dropout, but you also pass in a size. Now, normally you would just pass in the size of the thing to dropout like this. But what he did here was he passed in for the size, size 0, 1, size 2. And so if you remember back to broadcasting, this means that this is going to create something with a unit axis in the middle. And so when we multiply that, so here's our rep matrix, when we multiply the dropout by that, our zeros get broadcast. This is really important, right? Because this is the sequence dimension. So every time step, if you drop out time step number 3, but not time step 2 or 4, you've basically broken that whole sequence's ability to calculate anything because you just killed it. So this is called RNN dropout or also called variational dropout. There's a couple of different papers that introduce the same idea. And it's simply this, that you do dropout on the entire sequence at a time. So there's RNN dropout. The second one that Stephen Marody showed was something he called weight drop. It actually turns out that this already existed in the computer vision world where it was called drop connect. So there's now two things with different names, but are the same, weight drop and drop connect. And this is dropout not on the activations, but on the weights themselves. So you can see here when we do the forward pass, we go set weights that applies dropout to the actual weights. So that's our second type of dropout. The next one is embedding dropout. And this one, as you can see, it drops out an entire row. This is actually a coincidence that all these rows are in order, but it drops out an entire row. So what it does is it says, okay, you've got an embedding. And what I'm going to do is I'm going to drop out all of the entire embedding vector for whatever word this is. So it's dropping out entire words at a time. So that's embedding dropout. So with all that in place, we can create an LSTM model. It can be a number of layers. So we can create lots of LSTMs for however many layers you want. And we can loop through them and we can basically call each layer. And we've got all our different dropouts. And so basically this code is just calling all the different dropouts. So that is an AWD LSTM. So then we can put on top of that a simple linear model with dropout. And so this simple linear model, so it's literally just a linear model where we go dropout and then call our linear model, is we're going to create a sequential model which takes the RNN, so the AWD LSTM, and passes the result to a single linear layer with dropout. And that is our language model because that final linear layer is the thing which will figure out what is the next word. So the size of that is the size of the vocab. It's good to look at these little tests that we do along the way. These are the things we use to help us check that everything looks sensible. And we found, yep, everything does look sensible. And then we added something that AWD LSTM did which is called gradient clipping, which is a callback that just checks after the backward pass one of the gradients. And if the total norm of the gradients, so the root sum of the gradients, is bigger than some number, then we'll divide them all so that they're not bigger than that number anymore. So it's just clipping those gradients. So that's how easy it is to add gradient clipping. This is a super good idea, not as used as much as it should be, because it really lets you train things at higher learning rates and avoid gradients blowing out. Then there's two other kinds of regularization. This one here is called activation regularization. And it's actually just an L2 loss, an L2 penalty, just like weight decay. Except the L2 penalty is not on the weights, it's on the activations. So this is going to make sure that our activations are never too high. And then this one's really interesting. This is called temporal activation regularization. This checks how much does each activation change by from sequence step to sequence step, and then take the square of that. So this is regularizing the RNN to say, try not to have things that massively change from time step to time step, because if it's doing that, that's probably not a good sign. Okay, so that's our RNN trainer callback. We set up our loss functions, which are just normal cross entropy loss, and also a metric, which is normal accuracy. But we just make sure that our batch and sequence length is all flattened. So we can create our language model, add our callbacks, and fit. So once we've got all that, we can use it to train a language model on Wikitext 103. So I'm not going to go through this, because it literally just uses what's in the previous notebook, but this shows you here's how you can download Wikitext 103, split it into articles, create the text lists, split into train and valid, tokenize, numericalize, data bunchify, create the model that we just saw, and train it for, in this case, about five hours. Because it's quite a big model, right? So because we don't want you to have to train for five hours, this RNN, you will find that you can download that small pre-trained model from this link. So you can now use that on IMDB. So you can, again, grab your IMDB dataset, download that pre-trained model, load it in, and then we need to do one more step, which is that the embedding matrix for the pre-trained Wikitext 103 model is for a different bunch of words to the IMDB vocab. So they've got different vocabs with some overlap. So I won't go through the code, but what we just do is we just go through each vocab item in the IMDB vocab, and we find out if it's in the Wikitext 103 vocab, and if it is, we copy Wikitext 103's vocab over. It's embedding over. So that way we'll end up with an embedding matrix for IMDB that is the same as the Wikitext 103 embedding matrix any time there's a word that's the same, and any time there's a word that's missing, we're just going to use the mean bias and the mean weights. So that's all that is. Okay, so once we've done that, we can then define a splitter, just like before, to create our layer groups. We can set up our callbacks, our learner, we can fit, and so then we'll train that for an hour or so, and at the end of that we have a fine-tuned IMDB language model. So now we can load up our classifier data bunch, which we created earlier. That's exactly the same lines of code we had before. I'm going to ignore this pack padded sequence stuff, but basically there's a neat little trick in PyTorch where you can take data that's of different lengths and call pack padded sequence, pass that to an RNN, and then call pad packed sequence, and it basically takes things of different lengths and optimally handles them in an RNN. So we basically update our AWD LSTM to use that. You might remember that for ULM fit, we kind of create our hidden state in the LSTM for lots of time steps, and we want to say, like, oh, which bit of state do we actually want to use for classification? People used to basically use the final state. Something that I tried and it turned out to work really well, so ended up in the paper, was that we actually do an average pool and a max pool and use the final state, and we concatenate them all together. So this is like the concat pooling we do for images. We do the same kind of thing for text. So we put all that together. This is just a chain of checking that everything looks sensible, and that gives us something that we call the pooling linear classifier, which is just a list of batch norm dropout linear layers and our concat pooling. And that's about it. So we just go through our sentence one BPTT at a time and keep calling that thing and keep appending the results. So once we've done all that, we can train it. So here's our normal set of callbacks. We can load our fine-tuned encoder, and we can train. And 92% accuracy, which is pretty close to where the state of the art was a very small number of years ago. And this is not the same as we got about 94.5% or something like that, or 95% for the paper, because that used a bigger model that we trained for longer. Okay. So that was a super fast zip through ULM fit, and plenty of stuff which is probably worth reading in more detail, and we can answer questions on the forum as well. So let's spend the last 10 minutes talking about Swift, because the next two classes are going to be about Swift. So I think anybody who's got to lesson 12 in this course should be learning Swift for TensorFlow. The reason why is I think basically the Python stays a numbered. That stuff I showed you about JIT, the more I use JIT, the more I think about it, the more it looks like failed examples of software development processes I've seen in the last 25 years. Whenever people try to convert one language into a different language, and then you're kind of using the language that you're not really using, it requires brilliant, brilliant people like the PyTorch team years to make it almost kind of work. So I think Julia or Swift will eventually in the coming years take over. I just don't think Python can survive, because we can't write CUDA kernels in Python. We can't write RNN cells in Python and have them work reliably and fast. DL libraries change all the time anyway, so if you're spending all your time just studying one library in one language, then you're not going to be ready for that change. So you'll need to learn something new anyway. It could probably be Swift or Julia, and I think they're both perfectly good things to look at. Regardless, I've spent time using in real world scenarios at least a couple of dozen languages, and every time I learn a new language, I become a better developer. So it's just a good idea to learn a new language. And the TensorFlow bit might put you off a bit, because I've complained a lot about TensorFlow, but actually TensorFlow in the future is going to look almost totally different to TensorFlow in the past. The things that are happening with Swift for TensorFlow are so exciting. So there's basically almost no data science ecosystem for Swift, which means the whole thing is open for you to contribute to. So you can make serious contributions, look at any Python little library or just one function that doesn't exist in Swift and write it. The Swift community doesn't have people like us, people that understand deep learning. They're just not people who are generally in the Swift community right now, with some exceptions. So we are valued, and you'll be working on stuff that will look pretty familiar, because we're building something a lot like Fast AI, but hopefully much better. So with that, I have here Chris Latner, who started the Swift project and is now running Swift for TensorFlow team at Google. And we have time for, I think, three questions from the community for Chris and I. Sure. Assuming someone has zero knowledge of Swift, what would be the most efficient way to learn it and get up to speed with using Swift for TensorFlow? Sure. So the courses we're teaching will assume that you don't have prior Swift experience, but if you're interested, you can go to swift.org. In the documentation tab, there's a whole book online. The thing I recommend is there's a thing called a Swift Tour. You can just Google for that. It gives you a really quick sense of what it looks like, and it explains the basic concepts, it's super accessible, and that's where I would start. The best version of the Swift book is on the iPad. It uses something called Swift Playgrounds, which is one of these amazing things that Chris built, which basically lets you go through the book in a very interactive way. It'll feel a lot like the experience of using a Jupyter Notebook, but it's even more fancy in some ways, so you can read the book as you experiment. As Swift for TensorFlow evolves, what do you think will be the first kind of machine learning work accessible to people who don't have access to big corporate data centers, where Swift for TensorFlow's particular strengths will make it a better choice than the more traditional Python frameworks? Sure. I don't know what that first thing will be, but I think you have to look at the goals of the project, and I think there's two goals for this project overall. One is to be very subtractive, and subtractive of complexity. I think that one of the things that Jeremy's highlighting is that in practice, being effective in the machine learning field means you end up doing a lot of weird things at different levels, and so you may be dropping down to C++ or writing CUDA code, depending on what you're doing. You may be playing with TorchJet or playing with these other systems or these other C libraries that get wrapped up with Python, but these become leaky abstractions you have to deal with, and so we're trying to make it so you don't have to deal with a lot of that complexity, so you can stay in one language. It works top to bottom. It's fast, has lots of other good things that go with it, so that's one aspect of it. The other piece is we're thinking about it from the bottom up, including the compiler bits, all the systems integration pieces, the application integration pieces, and I have a theory that once we get past the world of Python here, that people are going to start doing a lot of really interesting things where you integrate deep learning into applications, and right now the application world and the ML world are different. I mean, people literally export their model into an ONNX or TF serving or whatever and dump it into some C++ thing where it's a whole new world. It's a completely different world, and so now you have this barrier between the training, the learning, and the ML pieces, and you have the application pieces, and often these are different teams or different people thinking about things in different ways, and breaking down those kinds of barriers, I think, is a really big opportunity that enables new kinds of work to be done. Very powerful. That leads well into the next pair of questions. Does it make sense to spend efforts learning and writing in Swift only, or is it worth to have some understanding of C++ as well to be good in numerical computations? And then secondly, after going through some of the Swift documentations, it seems like it's a very versatile language. If I understand correctly, deep learning, robotics, web development, and systems programming all seem well under its purview. Do you foresee Swift's influence flourishing in all these separate areas and allowing for a tighter and more fluid development between disciplines? Sure. So I think these are two sides of the same coin. I totally agree with Jeremy. Learning new programming languages is good, just because often you learn to think about things in a new way, or they open up new kinds of approaches, and having more different kinds of mental frameworks gives you the ability to solve problems that otherwise you might not be able to do. So learning C++ in the abstract is a good thing. Having to use C++ is a little bit of a different thing, in my opinion. And so C++ has lots of drawbacks. This is coming from somebody who's written a C++ compiler. Yeah, I've written way too much C++ myself, and maybe I'm a little bit damaged here, but C++ is a super complicated language. It's also full of memory safety problems and security vulnerabilities and a lot of other things that are pretty well known. It's a great language, supports tons of really important work, but one of the goals with Swift is to be a full-stack language and really span from the scripting all the way down to the things C++ is good at, and getting C++ level performance in the same language that you can do high-level machine learning frameworks in is pretty cool. And so I think that that's one of the really unique aspects of Swift, is it was designed for compilation, for usability, for accessibility, and I'm not aware of a system that's similar in that way. Great. I'm really looking forward to it. Can we ask one more question? I think we're out of time. Oh, OK. Sorry. So, yeah. We'll come back next time. One more question next time. Thanks, everybody. Thank you, Chris Lattner, and we'll see you next week. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.86, "text": " Welcome to lesson 12.", "tokens": [4027, 281, 6898, 2272, 13], "temperature": 0.0, "avg_logprob": -0.15339522802529215, "compression_ratio": 1.7091633466135459, "no_speech_prob": 0.027568640187382698}, {"id": 1, "seek": 0, "start": 4.86, "end": 6.24, "text": " Wow, we're moving along.", "tokens": [3153, 11, 321, 434, 2684, 2051, 13], "temperature": 0.0, "avg_logprob": -0.15339522802529215, "compression_ratio": 1.7091633466135459, "no_speech_prob": 0.027568640187382698}, {"id": 2, "seek": 0, "start": 6.24, "end": 9.76, "text": " And this is an exciting lesson because it's where we're going", "tokens": [400, 341, 307, 364, 4670, 6898, 570, 309, 311, 689, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.15339522802529215, "compression_ratio": 1.7091633466135459, "no_speech_prob": 0.027568640187382698}, {"id": 3, "seek": 0, "start": 9.76, "end": 14.040000000000001, "text": " to wrap up all the pieces both for computer vision and for NLP.", "tokens": [281, 7019, 493, 439, 264, 3755, 1293, 337, 3820, 5201, 293, 337, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.15339522802529215, "compression_ratio": 1.7091633466135459, "no_speech_prob": 0.027568640187382698}, {"id": 4, "seek": 0, "start": 14.040000000000001, "end": 16.28, "text": " And you might be surprised to hear that we're going to wrap", "tokens": [400, 291, 1062, 312, 6100, 281, 1568, 300, 321, 434, 516, 281, 7019], "temperature": 0.0, "avg_logprob": -0.15339522802529215, "compression_ratio": 1.7091633466135459, "no_speech_prob": 0.027568640187382698}, {"id": 5, "seek": 0, "start": 16.28, "end": 17.6, "text": " up all the pieces for NLP", "tokens": [493, 439, 264, 3755, 337, 426, 45196], "temperature": 0.0, "avg_logprob": -0.15339522802529215, "compression_ratio": 1.7091633466135459, "no_speech_prob": 0.027568640187382698}, {"id": 6, "seek": 0, "start": 17.6, "end": 19.52, "text": " because we haven't really done any NLP yet.", "tokens": [570, 321, 2378, 380, 534, 1096, 604, 426, 45196, 1939, 13], "temperature": 0.0, "avg_logprob": -0.15339522802529215, "compression_ratio": 1.7091633466135459, "no_speech_prob": 0.027568640187382698}, {"id": 7, "seek": 0, "start": 19.52, "end": 24.48, "text": " But actually everything we've done is equally applicable to NLP.", "tokens": [583, 767, 1203, 321, 600, 1096, 307, 12309, 21142, 281, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.15339522802529215, "compression_ratio": 1.7091633466135459, "no_speech_prob": 0.027568640187382698}, {"id": 8, "seek": 0, "start": 24.48, "end": 28.900000000000002, "text": " So there's very little to do to get a state of the art result", "tokens": [407, 456, 311, 588, 707, 281, 360, 281, 483, 257, 1785, 295, 264, 1523, 1874], "temperature": 0.0, "avg_logprob": -0.15339522802529215, "compression_ratio": 1.7091633466135459, "no_speech_prob": 0.027568640187382698}, {"id": 9, "seek": 2890, "start": 28.9, "end": 32.0, "text": " on IMDB sentiment analysis from scratch.", "tokens": [322, 21463, 27735, 16149, 5215, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.18016877721567623, "compression_ratio": 1.640495867768595, "no_speech_prob": 3.8806767406640574e-05}, {"id": 10, "seek": 2890, "start": 32.0, "end": 33.72, "text": " So that's what we're going to do.", "tokens": [407, 300, 311, 437, 321, 434, 516, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.18016877721567623, "compression_ratio": 1.640495867768595, "no_speech_prob": 3.8806767406640574e-05}, {"id": 11, "seek": 2890, "start": 33.72, "end": 37.16, "text": " Before we do, let's finally finish off this slide we've been", "tokens": [4546, 321, 360, 11, 718, 311, 2721, 2413, 766, 341, 4137, 321, 600, 668], "temperature": 0.0, "avg_logprob": -0.18016877721567623, "compression_ratio": 1.640495867768595, "no_speech_prob": 3.8806767406640574e-05}, {"id": 12, "seek": 2890, "start": 37.16, "end": 39.72, "text": " going through for three lessons now.", "tokens": [516, 807, 337, 1045, 8820, 586, 13], "temperature": 0.0, "avg_logprob": -0.18016877721567623, "compression_ratio": 1.640495867768595, "no_speech_prob": 3.8806767406640574e-05}, {"id": 13, "seek": 2890, "start": 39.72, "end": 43.44, "text": " I promised, not promised, that we would get something state", "tokens": [286, 10768, 11, 406, 10768, 11, 300, 321, 576, 483, 746, 1785], "temperature": 0.0, "avg_logprob": -0.18016877721567623, "compression_ratio": 1.640495867768595, "no_speech_prob": 3.8806767406640574e-05}, {"id": 14, "seek": 2890, "start": 43.44, "end": 45.0, "text": " of the art on ImageNet.", "tokens": [295, 264, 1523, 322, 29903, 31890, 13], "temperature": 0.0, "avg_logprob": -0.18016877721567623, "compression_ratio": 1.640495867768595, "no_speech_prob": 3.8806767406640574e-05}, {"id": 15, "seek": 2890, "start": 45.0, "end": 46.3, "text": " Turns out we did.", "tokens": [29524, 484, 321, 630, 13], "temperature": 0.0, "avg_logprob": -0.18016877721567623, "compression_ratio": 1.640495867768595, "no_speech_prob": 3.8806767406640574e-05}, {"id": 16, "seek": 2890, "start": 46.3, "end": 47.879999999999995, "text": " So you're going to see that today.", "tokens": [407, 291, 434, 516, 281, 536, 300, 965, 13], "temperature": 0.0, "avg_logprob": -0.18016877721567623, "compression_ratio": 1.640495867768595, "no_speech_prob": 3.8806767406640574e-05}, {"id": 17, "seek": 2890, "start": 47.879999999999995, "end": 53.76, "text": " So we're going to finish off mixup, label smoothing, and ResNets.", "tokens": [407, 321, 434, 516, 281, 2413, 766, 2890, 1010, 11, 7645, 899, 6259, 571, 11, 293, 5015, 45, 1385, 13], "temperature": 0.0, "avg_logprob": -0.18016877721567623, "compression_ratio": 1.640495867768595, "no_speech_prob": 3.8806767406640574e-05}, {"id": 18, "seek": 2890, "start": 56.2, "end": 58.32, "text": " Okay. So let's do it.", "tokens": [1033, 13, 407, 718, 311, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.18016877721567623, "compression_ratio": 1.640495867768595, "no_speech_prob": 3.8806767406640574e-05}, {"id": 19, "seek": 5832, "start": 58.32, "end": 66.88, "text": " Before we look at the new stuff, 09b learner.", "tokens": [4546, 321, 574, 412, 264, 777, 1507, 11, 48729, 65, 33347, 13], "temperature": 0.0, "avg_logprob": -0.1649748034181848, "compression_ratio": 1.6244897959183673, "no_speech_prob": 9.368070095661096e-06}, {"id": 20, "seek": 5832, "start": 66.88, "end": 68.4, "text": " I've made a couple of minor changes", "tokens": [286, 600, 1027, 257, 1916, 295, 6696, 2962], "temperature": 0.0, "avg_logprob": -0.1649748034181848, "compression_ratio": 1.6244897959183673, "no_speech_prob": 9.368070095661096e-06}, {"id": 21, "seek": 5832, "start": 68.4, "end": 70.6, "text": " that I thought you might be interested in.", "tokens": [300, 286, 1194, 291, 1062, 312, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.1649748034181848, "compression_ratio": 1.6244897959183673, "no_speech_prob": 9.368070095661096e-06}, {"id": 22, "seek": 5832, "start": 70.6, "end": 72.64, "text": " It's kind of like as you refactor things.", "tokens": [467, 311, 733, 295, 411, 382, 291, 1895, 15104, 721, 13], "temperature": 0.0, "avg_logprob": -0.1649748034181848, "compression_ratio": 1.6244897959183673, "no_speech_prob": 9.368070095661096e-06}, {"id": 23, "seek": 5832, "start": 72.64, "end": 75.24000000000001, "text": " So remember last week we refactored the learner to get rid", "tokens": [407, 1604, 1036, 1243, 321, 1895, 578, 2769, 264, 33347, 281, 483, 3973], "temperature": 0.0, "avg_logprob": -0.1649748034181848, "compression_ratio": 1.6244897959183673, "no_speech_prob": 9.368070095661096e-06}, {"id": 24, "seek": 5832, "start": 75.24000000000001, "end": 77.24000000000001, "text": " of that awful separate runner.", "tokens": [295, 300, 11232, 4994, 24376, 13], "temperature": 0.0, "avg_logprob": -0.1649748034181848, "compression_ratio": 1.6244897959183673, "no_speech_prob": 9.368070095661096e-06}, {"id": 25, "seek": 5832, "start": 77.24000000000001, "end": 78.76, "text": " So there's just now one thing.", "tokens": [407, 456, 311, 445, 586, 472, 551, 13], "temperature": 0.0, "avg_logprob": -0.1649748034181848, "compression_ratio": 1.6244897959183673, "no_speech_prob": 9.368070095661096e-06}, {"id": 26, "seek": 5832, "start": 78.76, "end": 80.52, "text": " Made a lot of our code a lot easier.", "tokens": [18330, 257, 688, 295, 527, 3089, 257, 688, 3571, 13], "temperature": 0.0, "avg_logprob": -0.1649748034181848, "compression_ratio": 1.6244897959183673, "no_speech_prob": 9.368070095661096e-06}, {"id": 27, "seek": 5832, "start": 80.52, "end": 82.64, "text": " But there's still this concept left behind", "tokens": [583, 456, 311, 920, 341, 3410, 1411, 2261], "temperature": 0.0, "avg_logprob": -0.1649748034181848, "compression_ratio": 1.6244897959183673, "no_speech_prob": 9.368070095661096e-06}, {"id": 28, "seek": 5832, "start": 82.64, "end": 84.32, "text": " that when you started fitting,", "tokens": [300, 562, 291, 1409, 15669, 11], "temperature": 0.0, "avg_logprob": -0.1649748034181848, "compression_ratio": 1.6244897959183673, "no_speech_prob": 9.368070095661096e-06}, {"id": 29, "seek": 8432, "start": 84.32, "end": 88.44, "text": " you had to tell each callback what its learner or runner was.", "tokens": [291, 632, 281, 980, 1184, 818, 3207, 437, 1080, 33347, 420, 24376, 390, 13], "temperature": 0.0, "avg_logprob": -0.13395481267251258, "compression_ratio": 1.6774193548387097, "no_speech_prob": 4.4252542465983424e-06}, {"id": 30, "seek": 8432, "start": 88.44, "end": 90.63999999999999, "text": " I've moved that because we don't, you know,", "tokens": [286, 600, 4259, 300, 570, 321, 500, 380, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.13395481267251258, "compression_ratio": 1.6774193548387097, "no_speech_prob": 4.4252542465983424e-06}, {"id": 31, "seek": 8432, "start": 90.63999999999999, "end": 92.72, "text": " they're all totally attached now.", "tokens": [436, 434, 439, 3879, 8570, 586, 13], "temperature": 0.0, "avg_logprob": -0.13395481267251258, "compression_ratio": 1.6774193548387097, "no_speech_prob": 4.4252542465983424e-06}, {"id": 32, "seek": 8432, "start": 92.72, "end": 95.52, "text": " I've moved that to the, in it.", "tokens": [286, 600, 4259, 300, 281, 264, 11, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.13395481267251258, "compression_ratio": 1.6774193548387097, "no_speech_prob": 4.4252542465983424e-06}, {"id": 33, "seek": 8432, "start": 95.52, "end": 100.52, "text": " And so now you can call addCBs to add a whole bunch of callbacks", "tokens": [400, 370, 586, 291, 393, 818, 909, 34, 33, 82, 281, 909, 257, 1379, 3840, 295, 818, 17758], "temperature": 0.0, "avg_logprob": -0.13395481267251258, "compression_ratio": 1.6774193548387097, "no_speech_prob": 4.4252542465983424e-06}, {"id": 34, "seek": 8432, "start": 100.52, "end": 102.52, "text": " or addCB to add one callback.", "tokens": [420, 909, 34, 33, 281, 909, 472, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.13395481267251258, "compression_ratio": 1.6774193548387097, "no_speech_prob": 4.4252542465983424e-06}, {"id": 35, "seek": 8432, "start": 102.52, "end": 105.11999999999999, "text": " And that happens automatically at the start of training.", "tokens": [400, 300, 2314, 6772, 412, 264, 722, 295, 3097, 13], "temperature": 0.0, "avg_logprob": -0.13395481267251258, "compression_ratio": 1.6774193548387097, "no_speech_prob": 4.4252542465983424e-06}, {"id": 36, "seek": 8432, "start": 105.11999999999999, "end": 106.6, "text": " So it's a very minor thing.", "tokens": [407, 309, 311, 257, 588, 6696, 551, 13], "temperature": 0.0, "avg_logprob": -0.13395481267251258, "compression_ratio": 1.6774193548387097, "no_speech_prob": 4.4252542465983424e-06}, {"id": 37, "seek": 8432, "start": 106.6, "end": 112.69999999999999, "text": " More interesting was when I did this little reformatting exercise", "tokens": [5048, 1880, 390, 562, 286, 630, 341, 707, 8290, 267, 783, 5380], "temperature": 0.0, "avg_logprob": -0.13395481267251258, "compression_ratio": 1.6774193548387097, "no_speech_prob": 4.4252542465983424e-06}, {"id": 38, "seek": 11270, "start": 112.7, "end": 114.96000000000001, "text": " where I took all these callbacks that used to be", "tokens": [689, 286, 1890, 439, 613, 818, 17758, 300, 1143, 281, 312], "temperature": 0.0, "avg_logprob": -0.08410237530085045, "compression_ratio": 1.9833333333333334, "no_speech_prob": 4.157014700467698e-06}, {"id": 39, "seek": 11270, "start": 114.96000000000001, "end": 118.12, "text": " on the line underneath the thing before them and lined them", "tokens": [322, 264, 1622, 7223, 264, 551, 949, 552, 293, 17189, 552], "temperature": 0.0, "avg_logprob": -0.08410237530085045, "compression_ratio": 1.9833333333333334, "no_speech_prob": 4.157014700467698e-06}, {"id": 40, "seek": 11270, "start": 118.12, "end": 120.28, "text": " up over here and suddenly realized", "tokens": [493, 670, 510, 293, 5800, 5334], "temperature": 0.0, "avg_logprob": -0.08410237530085045, "compression_ratio": 1.9833333333333334, "no_speech_prob": 4.157014700467698e-06}, {"id": 41, "seek": 11270, "start": 120.28, "end": 123.16, "text": " that now I can answer all the questions I have in my head", "tokens": [300, 586, 286, 393, 1867, 439, 264, 1651, 286, 362, 294, 452, 1378], "temperature": 0.0, "avg_logprob": -0.08410237530085045, "compression_ratio": 1.9833333333333334, "no_speech_prob": 4.157014700467698e-06}, {"id": 42, "seek": 11270, "start": 123.16, "end": 127.28, "text": " about our callback system, which is what exactly are the steps", "tokens": [466, 527, 818, 3207, 1185, 11, 597, 307, 437, 2293, 366, 264, 4439], "temperature": 0.0, "avg_logprob": -0.08410237530085045, "compression_ratio": 1.9833333333333334, "no_speech_prob": 4.157014700467698e-06}, {"id": 43, "seek": 11270, "start": 127.28, "end": 129.36, "text": " in the training loop?", "tokens": [294, 264, 3097, 6367, 30], "temperature": 0.0, "avg_logprob": -0.08410237530085045, "compression_ratio": 1.9833333333333334, "no_speech_prob": 4.157014700467698e-06}, {"id": 44, "seek": 11270, "start": 129.36, "end": 130.66, "text": " What exactly are the callbacks", "tokens": [708, 2293, 366, 264, 818, 17758], "temperature": 0.0, "avg_logprob": -0.08410237530085045, "compression_ratio": 1.9833333333333334, "no_speech_prob": 4.157014700467698e-06}, {"id": 45, "seek": 11270, "start": 130.66, "end": 133.24, "text": " that you can use in the training loop?", "tokens": [300, 291, 393, 764, 294, 264, 3097, 6367, 30], "temperature": 0.0, "avg_logprob": -0.08410237530085045, "compression_ratio": 1.9833333333333334, "no_speech_prob": 4.157014700467698e-06}, {"id": 46, "seek": 11270, "start": 133.24, "end": 136.88, "text": " Which step goes with which callback?", "tokens": [3013, 1823, 1709, 365, 597, 818, 3207, 30], "temperature": 0.0, "avg_logprob": -0.08410237530085045, "compression_ratio": 1.9833333333333334, "no_speech_prob": 4.157014700467698e-06}, {"id": 47, "seek": 11270, "start": 136.88, "end": 139.64000000000001, "text": " Which steps don't have a callback?", "tokens": [3013, 4439, 500, 380, 362, 257, 818, 3207, 30], "temperature": 0.0, "avg_logprob": -0.08410237530085045, "compression_ratio": 1.9833333333333334, "no_speech_prob": 4.157014700467698e-06}, {"id": 48, "seek": 11270, "start": 139.64000000000001, "end": 142.14000000000001, "text": " Are there any callbacks that don't have a step?", "tokens": [2014, 456, 604, 818, 17758, 300, 500, 380, 362, 257, 1823, 30], "temperature": 0.0, "avg_logprob": -0.08410237530085045, "compression_ratio": 1.9833333333333334, "no_speech_prob": 4.157014700467698e-06}, {"id": 49, "seek": 14214, "start": 142.14, "end": 144.2, "text": " So like it's one of these interesting things", "tokens": [407, 411, 309, 311, 472, 295, 613, 1880, 721], "temperature": 0.0, "avg_logprob": -0.12320139312744141, "compression_ratio": 1.8565891472868217, "no_speech_prob": 2.4439195840386674e-06}, {"id": 50, "seek": 14214, "start": 144.2, "end": 149.11999999999998, "text": " where I really don't like the idea of kind", "tokens": [689, 286, 534, 500, 380, 411, 264, 1558, 295, 733], "temperature": 0.0, "avg_logprob": -0.12320139312744141, "compression_ratio": 1.8565891472868217, "no_speech_prob": 2.4439195840386674e-06}, {"id": 51, "seek": 14214, "start": 149.11999999999998, "end": 153.11999999999998, "text": " of automating your formatting and creating rules for formatting", "tokens": [295, 3553, 990, 428, 39366, 293, 4084, 4474, 337, 39366], "temperature": 0.0, "avg_logprob": -0.12320139312744141, "compression_ratio": 1.8565891472868217, "no_speech_prob": 2.4439195840386674e-06}, {"id": 52, "seek": 14214, "start": 153.11999999999998, "end": 155.32, "text": " when like something like this can just like as soon", "tokens": [562, 411, 746, 411, 341, 393, 445, 411, 382, 2321], "temperature": 0.0, "avg_logprob": -0.12320139312744141, "compression_ratio": 1.8565891472868217, "no_speech_prob": 2.4439195840386674e-06}, {"id": 53, "seek": 14214, "start": 155.32, "end": 158.32, "text": " as I did this I understood my code better.", "tokens": [382, 286, 630, 341, 286, 7320, 452, 3089, 1101, 13], "temperature": 0.0, "avg_logprob": -0.12320139312744141, "compression_ratio": 1.8565891472868217, "no_speech_prob": 2.4439195840386674e-06}, {"id": 54, "seek": 14214, "start": 158.32, "end": 161.64, "text": " And for me understanding my code is the only way to make it work", "tokens": [400, 337, 385, 3701, 452, 3089, 307, 264, 787, 636, 281, 652, 309, 589], "temperature": 0.0, "avg_logprob": -0.12320139312744141, "compression_ratio": 1.8565891472868217, "no_speech_prob": 2.4439195840386674e-06}, {"id": 55, "seek": 14214, "start": 161.64, "end": 165.83999999999997, "text": " because debugging machine learning code is awful.", "tokens": [570, 45592, 3479, 2539, 3089, 307, 11232, 13], "temperature": 0.0, "avg_logprob": -0.12320139312744141, "compression_ratio": 1.8565891472868217, "no_speech_prob": 2.4439195840386674e-06}, {"id": 56, "seek": 14214, "start": 165.83999999999997, "end": 167.14, "text": " So you've got to make sure", "tokens": [407, 291, 600, 658, 281, 652, 988], "temperature": 0.0, "avg_logprob": -0.12320139312744141, "compression_ratio": 1.8565891472868217, "no_speech_prob": 2.4439195840386674e-06}, {"id": 57, "seek": 14214, "start": 167.14, "end": 168.64, "text": " that the thing you write makes sense.", "tokens": [300, 264, 551, 291, 2464, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.12320139312744141, "compression_ratio": 1.8565891472868217, "no_speech_prob": 2.4439195840386674e-06}, {"id": 58, "seek": 14214, "start": 168.64, "end": 169.94, "text": " It's got to be simple.", "tokens": [467, 311, 658, 281, 312, 2199, 13], "temperature": 0.0, "avg_logprob": -0.12320139312744141, "compression_ratio": 1.8565891472868217, "no_speech_prob": 2.4439195840386674e-06}, {"id": 59, "seek": 14214, "start": 169.94, "end": 171.23999999999998, "text": " It's got to be really simple.", "tokens": [467, 311, 658, 281, 312, 534, 2199, 13], "temperature": 0.0, "avg_logprob": -0.12320139312744141, "compression_ratio": 1.8565891472868217, "no_speech_prob": 2.4439195840386674e-06}, {"id": 60, "seek": 17124, "start": 171.24, "end": 176.24, "text": " It's got to be really simple.", "tokens": [467, 311, 658, 281, 312, 534, 2199, 13], "temperature": 0.0, "avg_logprob": -0.1645365861745981, "compression_ratio": 1.7391304347826086, "no_speech_prob": 2.2251226710068295e-06}, {"id": 61, "seek": 17124, "start": 176.24, "end": 182.48000000000002, "text": " Then more interestingly we used to create the optimizer in init", "tokens": [1396, 544, 25873, 321, 1143, 281, 1884, 264, 5028, 6545, 294, 3157], "temperature": 0.0, "avg_logprob": -0.1645365861745981, "compression_ratio": 1.7391304347826086, "no_speech_prob": 2.2251226710068295e-06}, {"id": 62, "seek": 17124, "start": 182.48000000000002, "end": 185.64000000000001, "text": " and you could actually pass in an already created optimizer.", "tokens": [293, 291, 727, 767, 1320, 294, 364, 1217, 2942, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.1645365861745981, "compression_ratio": 1.7391304347826086, "no_speech_prob": 2.2251226710068295e-06}, {"id": 63, "seek": 17124, "start": 185.64000000000001, "end": 188.08, "text": " I removed that and the only thing now you can pass", "tokens": [286, 7261, 300, 293, 264, 787, 551, 586, 291, 393, 1320], "temperature": 0.0, "avg_logprob": -0.1645365861745981, "compression_ratio": 1.7391304347826086, "no_speech_prob": 2.2251226710068295e-06}, {"id": 64, "seek": 17124, "start": 188.08, "end": 189.92000000000002, "text": " in is an optimization function.", "tokens": [294, 307, 364, 19618, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1645365861745981, "compression_ratio": 1.7391304347826086, "no_speech_prob": 2.2251226710068295e-06}, {"id": 65, "seek": 17124, "start": 189.92000000000002, "end": 191.92000000000002, "text": " So something that will create an optimizer,", "tokens": [407, 746, 300, 486, 1884, 364, 5028, 6545, 11], "temperature": 0.0, "avg_logprob": -0.1645365861745981, "compression_ratio": 1.7391304347826086, "no_speech_prob": 2.2251226710068295e-06}, {"id": 66, "seek": 17124, "start": 191.92000000000002, "end": 193.88, "text": " which is what we've always been doing anyway.", "tokens": [597, 307, 437, 321, 600, 1009, 668, 884, 4033, 13], "temperature": 0.0, "avg_logprob": -0.1645365861745981, "compression_ratio": 1.7391304347826086, "no_speech_prob": 2.2251226710068295e-06}, {"id": 67, "seek": 17124, "start": 193.88, "end": 197.12, "text": " And by doing that we can now create our optimizer", "tokens": [400, 538, 884, 300, 321, 393, 586, 1884, 527, 5028, 6545], "temperature": 0.0, "avg_logprob": -0.1645365861745981, "compression_ratio": 1.7391304347826086, "no_speech_prob": 2.2251226710068295e-06}, {"id": 68, "seek": 17124, "start": 197.12, "end": 198.84, "text": " when we start fitting.", "tokens": [562, 321, 722, 15669, 13], "temperature": 0.0, "avg_logprob": -0.1645365861745981, "compression_ratio": 1.7391304347826086, "no_speech_prob": 2.2251226710068295e-06}, {"id": 69, "seek": 19884, "start": 198.84, "end": 201.36, "text": " And that turns out to be really important", "tokens": [400, 300, 4523, 484, 281, 312, 534, 1021], "temperature": 0.0, "avg_logprob": -0.1336518599923733, "compression_ratio": 1.6328125, "no_speech_prob": 5.014229827793315e-06}, {"id": 70, "seek": 19884, "start": 201.36, "end": 203.6, "text": " because when we do things like discriminative learning rates", "tokens": [570, 562, 321, 360, 721, 411, 20828, 1166, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.1336518599923733, "compression_ratio": 1.6328125, "no_speech_prob": 5.014229827793315e-06}, {"id": 71, "seek": 19884, "start": 203.6, "end": 206.56, "text": " and gradual unfreezing and layer groups and stuff,", "tokens": [293, 32890, 3971, 701, 8781, 293, 4583, 3935, 293, 1507, 11], "temperature": 0.0, "avg_logprob": -0.1336518599923733, "compression_ratio": 1.6328125, "no_speech_prob": 5.014229827793315e-06}, {"id": 72, "seek": 19884, "start": 206.56, "end": 210.28, "text": " we can change things and then when we fit it will all just work.", "tokens": [321, 393, 1319, 721, 293, 550, 562, 321, 3318, 309, 486, 439, 445, 589, 13], "temperature": 0.0, "avg_logprob": -0.1336518599923733, "compression_ratio": 1.6328125, "no_speech_prob": 5.014229827793315e-06}, {"id": 73, "seek": 19884, "start": 210.28, "end": 213.28, "text": " So that's a more significant, it's like one line of code", "tokens": [407, 300, 311, 257, 544, 4776, 11, 309, 311, 411, 472, 1622, 295, 3089], "temperature": 0.0, "avg_logprob": -0.1336518599923733, "compression_ratio": 1.6328125, "no_speech_prob": 5.014229827793315e-06}, {"id": 74, "seek": 19884, "start": 213.28, "end": 216.16, "text": " but it's conceptually a very significant change.", "tokens": [457, 309, 311, 3410, 671, 257, 588, 4776, 1319, 13], "temperature": 0.0, "avg_logprob": -0.1336518599923733, "compression_ratio": 1.6328125, "no_speech_prob": 5.014229827793315e-06}, {"id": 75, "seek": 19884, "start": 216.16, "end": 220.16, "text": " Okay, so that's some minor changes to 9b.", "tokens": [1033, 11, 370, 300, 311, 512, 6696, 2962, 281, 1722, 65, 13], "temperature": 0.0, "avg_logprob": -0.1336518599923733, "compression_ratio": 1.6328125, "no_speech_prob": 5.014229827793315e-06}, {"id": 76, "seek": 19884, "start": 220.16, "end": 226.6, "text": " And now let's move on to mixup and label smoothing.", "tokens": [400, 586, 718, 311, 1286, 322, 281, 2890, 1010, 293, 7645, 899, 6259, 571, 13], "temperature": 0.0, "avg_logprob": -0.1336518599923733, "compression_ratio": 1.6328125, "no_speech_prob": 5.014229827793315e-06}, {"id": 77, "seek": 22660, "start": 226.6, "end": 230.44, "text": " So I'm really excited about the stuff we saw at the end", "tokens": [407, 286, 478, 534, 2919, 466, 264, 1507, 321, 1866, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.1352884078694281, "compression_ratio": 1.677685950413223, "no_speech_prob": 3.2883629046409624e-06}, {"id": 78, "seek": 22660, "start": 230.44, "end": 233.56, "text": " of the last lesson where we saw how we can use the GPU", "tokens": [295, 264, 1036, 6898, 689, 321, 1866, 577, 321, 393, 764, 264, 18407], "temperature": 0.0, "avg_logprob": -0.1352884078694281, "compression_ratio": 1.677685950413223, "no_speech_prob": 3.2883629046409624e-06}, {"id": 79, "seek": 22660, "start": 233.56, "end": 236.32, "text": " to do data augmentation, fully randomized,", "tokens": [281, 360, 1412, 14501, 19631, 11, 4498, 38513, 11], "temperature": 0.0, "avg_logprob": -0.1352884078694281, "compression_ratio": 1.677685950413223, "no_speech_prob": 3.2883629046409624e-06}, {"id": 80, "seek": 22660, "start": 236.32, "end": 239.51999999999998, "text": " fully GPU accelerated data augmentation using just plain", "tokens": [4498, 18407, 29763, 1412, 14501, 19631, 1228, 445, 11121], "temperature": 0.0, "avg_logprob": -0.1352884078694281, "compression_ratio": 1.677685950413223, "no_speech_prob": 3.2883629046409624e-06}, {"id": 81, "seek": 22660, "start": 239.51999999999998, "end": 241.68, "text": " PyTorch operations.", "tokens": [9953, 51, 284, 339, 7705, 13], "temperature": 0.0, "avg_logprob": -0.1352884078694281, "compression_ratio": 1.677685950413223, "no_speech_prob": 3.2883629046409624e-06}, {"id": 82, "seek": 22660, "start": 241.68, "end": 243.68, "text": " I think that's a big win.", "tokens": [286, 519, 300, 311, 257, 955, 1942, 13], "temperature": 0.0, "avg_logprob": -0.1352884078694281, "compression_ratio": 1.677685950413223, "no_speech_prob": 3.2883629046409624e-06}, {"id": 83, "seek": 22660, "start": 243.68, "end": 248.28, "text": " But it's quite possible we don't need that kind", "tokens": [583, 309, 311, 1596, 1944, 321, 500, 380, 643, 300, 733], "temperature": 0.0, "avg_logprob": -0.1352884078694281, "compression_ratio": 1.677685950413223, "no_speech_prob": 3.2883629046409624e-06}, {"id": 84, "seek": 22660, "start": 248.28, "end": 252.48, "text": " of data augmentation anymore because in our experimentation", "tokens": [295, 1412, 14501, 19631, 3602, 570, 294, 527, 37142], "temperature": 0.0, "avg_logprob": -0.1352884078694281, "compression_ratio": 1.677685950413223, "no_speech_prob": 3.2883629046409624e-06}, {"id": 85, "seek": 22660, "start": 252.48, "end": 254.79999999999998, "text": " with this data augmentation called mixup,", "tokens": [365, 341, 1412, 14501, 19631, 1219, 2890, 1010, 11], "temperature": 0.0, "avg_logprob": -0.1352884078694281, "compression_ratio": 1.677685950413223, "no_speech_prob": 3.2883629046409624e-06}, {"id": 86, "seek": 25480, "start": 254.8, "end": 258.76, "text": " we found we can remove most other data augmentation", "tokens": [321, 1352, 321, 393, 4159, 881, 661, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.11569055557250976, "compression_ratio": 1.699530516431925, "no_speech_prob": 4.9364966798748355e-06}, {"id": 87, "seek": 25480, "start": 258.76, "end": 260.84000000000003, "text": " and get amazingly good results.", "tokens": [293, 483, 31762, 665, 3542, 13], "temperature": 0.0, "avg_logprob": -0.11569055557250976, "compression_ratio": 1.699530516431925, "no_speech_prob": 4.9364966798748355e-06}, {"id": 88, "seek": 25480, "start": 260.84000000000003, "end": 263.6, "text": " So it's just a kind of a simplicity result.", "tokens": [407, 309, 311, 445, 257, 733, 295, 257, 25632, 1874, 13], "temperature": 0.0, "avg_logprob": -0.11569055557250976, "compression_ratio": 1.699530516431925, "no_speech_prob": 4.9364966798748355e-06}, {"id": 89, "seek": 25480, "start": 263.6, "end": 265.84000000000003, "text": " And also when you use mixup you can train", "tokens": [400, 611, 562, 291, 764, 2890, 1010, 291, 393, 3847], "temperature": 0.0, "avg_logprob": -0.11569055557250976, "compression_ratio": 1.699530516431925, "no_speech_prob": 4.9364966798748355e-06}, {"id": 90, "seek": 25480, "start": 265.84000000000003, "end": 268.96000000000004, "text": " for a really long time and get really good results.", "tokens": [337, 257, 534, 938, 565, 293, 483, 534, 665, 3542, 13], "temperature": 0.0, "avg_logprob": -0.11569055557250976, "compression_ratio": 1.699530516431925, "no_speech_prob": 4.9364966798748355e-06}, {"id": 91, "seek": 25480, "start": 268.96000000000004, "end": 271.92, "text": " So let me show you mixup.", "tokens": [407, 718, 385, 855, 291, 2890, 1010, 13], "temperature": 0.0, "avg_logprob": -0.11569055557250976, "compression_ratio": 1.699530516431925, "no_speech_prob": 4.9364966798748355e-06}, {"id": 92, "seek": 25480, "start": 271.92, "end": 276.48, "text": " And in terms of the results you can get, what happened", "tokens": [400, 294, 2115, 295, 264, 3542, 291, 393, 483, 11, 437, 2011], "temperature": 0.0, "avg_logprob": -0.11569055557250976, "compression_ratio": 1.699530516431925, "no_speech_prob": 4.9364966798748355e-06}, {"id": 93, "seek": 25480, "start": 276.48, "end": 282.0, "text": " in the bag of tricks paper was they, when they turned mixup", "tokens": [294, 264, 3411, 295, 11733, 3035, 390, 436, 11, 562, 436, 3574, 2890, 1010], "temperature": 0.0, "avg_logprob": -0.11569055557250976, "compression_ratio": 1.699530516431925, "no_speech_prob": 4.9364966798748355e-06}, {"id": 94, "seek": 28200, "start": 282.0, "end": 286.88, "text": " on they also started training for 200 epochs instead of 90,", "tokens": [322, 436, 611, 1409, 3097, 337, 2331, 30992, 28346, 2602, 295, 4289, 11], "temperature": 0.0, "avg_logprob": -0.15295333423833737, "compression_ratio": 1.4523809523809523, "no_speech_prob": 1.5442417861777358e-05}, {"id": 95, "seek": 28200, "start": 286.88, "end": 288.36, "text": " sorry instead of 120.", "tokens": [2597, 2602, 295, 10411, 13], "temperature": 0.0, "avg_logprob": -0.15295333423833737, "compression_ratio": 1.4523809523809523, "no_speech_prob": 1.5442417861777358e-05}, {"id": 96, "seek": 28200, "start": 288.36, "end": 293.04, "text": " So be a bit careful when you interpret their paper table", "tokens": [407, 312, 257, 857, 5026, 562, 291, 7302, 641, 3035, 3199], "temperature": 0.0, "avg_logprob": -0.15295333423833737, "compression_ratio": 1.4523809523809523, "no_speech_prob": 1.5442417861777358e-05}, {"id": 97, "seek": 28200, "start": 293.04, "end": 301.68, "text": " where it goes from label smoothing 94.1 to mixup", "tokens": [689, 309, 1709, 490, 7645, 899, 6259, 571, 30849, 13, 16, 281, 2890, 1010], "temperature": 0.0, "avg_logprob": -0.15295333423833737, "compression_ratio": 1.4523809523809523, "no_speech_prob": 1.5442417861777358e-05}, {"id": 98, "seek": 28200, "start": 301.68, "end": 304.04, "text": " without distillation 94.6.", "tokens": [1553, 42923, 399, 30849, 13, 21, 13], "temperature": 0.0, "avg_logprob": -0.15295333423833737, "compression_ratio": 1.4523809523809523, "no_speech_prob": 1.5442417861777358e-05}, {"id": 99, "seek": 28200, "start": 304.04, "end": 307.16, "text": " They're also nearly doubling the number of epochs they do.", "tokens": [814, 434, 611, 6217, 33651, 264, 1230, 295, 30992, 28346, 436, 360, 13], "temperature": 0.0, "avg_logprob": -0.15295333423833737, "compression_ratio": 1.4523809523809523, "no_speech_prob": 1.5442417861777358e-05}, {"id": 100, "seek": 28200, "start": 307.16, "end": 308.52, "text": " But you can kind of get a sense", "tokens": [583, 291, 393, 733, 295, 483, 257, 2020], "temperature": 0.0, "avg_logprob": -0.15295333423833737, "compression_ratio": 1.4523809523809523, "no_speech_prob": 1.5442417861777358e-05}, {"id": 101, "seek": 30852, "start": 308.52, "end": 312.56, "text": " that you can get a big decrease in error.", "tokens": [300, 291, 393, 483, 257, 955, 11514, 294, 6713, 13], "temperature": 0.0, "avg_logprob": -0.12002404095375374, "compression_ratio": 1.775974025974026, "no_speech_prob": 1.7773787476471625e-05}, {"id": 102, "seek": 30852, "start": 312.56, "end": 315.12, "text": " The other thing they mention in the paper is distillation.", "tokens": [440, 661, 551, 436, 2152, 294, 264, 3035, 307, 42923, 399, 13], "temperature": 0.0, "avg_logprob": -0.12002404095375374, "compression_ratio": 1.775974025974026, "no_speech_prob": 1.7773787476471625e-05}, {"id": 103, "seek": 30852, "start": 315.12, "end": 317.2, "text": " I'm not going to talk about that because it's a thing", "tokens": [286, 478, 406, 516, 281, 751, 466, 300, 570, 309, 311, 257, 551], "temperature": 0.0, "avg_logprob": -0.12002404095375374, "compression_ratio": 1.775974025974026, "no_speech_prob": 1.7773787476471625e-05}, {"id": 104, "seek": 30852, "start": 317.2, "end": 321.88, "text": " where you pre-train some much bigger model like a ResNet 152", "tokens": [689, 291, 659, 12, 83, 7146, 512, 709, 3801, 2316, 411, 257, 5015, 31890, 2119, 17], "temperature": 0.0, "avg_logprob": -0.12002404095375374, "compression_ratio": 1.775974025974026, "no_speech_prob": 1.7773787476471625e-05}, {"id": 105, "seek": 30852, "start": 321.88, "end": 323.03999999999996, "text": " and then you try and train something", "tokens": [293, 550, 291, 853, 293, 3847, 746], "temperature": 0.0, "avg_logprob": -0.12002404095375374, "compression_ratio": 1.775974025974026, "no_speech_prob": 1.7773787476471625e-05}, {"id": 106, "seek": 30852, "start": 323.03999999999996, "end": 324.47999999999996, "text": " that predicts the output of that.", "tokens": [300, 6069, 82, 264, 5598, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.12002404095375374, "compression_ratio": 1.775974025974026, "no_speech_prob": 1.7773787476471625e-05}, {"id": 107, "seek": 30852, "start": 324.47999999999996, "end": 326.59999999999997, "text": " And to me the idea of like training a really big model", "tokens": [400, 281, 385, 264, 1558, 295, 411, 3097, 257, 534, 955, 2316], "temperature": 0.0, "avg_logprob": -0.12002404095375374, "compression_ratio": 1.775974025974026, "no_speech_prob": 1.7773787476471625e-05}, {"id": 108, "seek": 30852, "start": 326.59999999999997, "end": 329.4, "text": " to train a smaller model, it's interesting", "tokens": [281, 3847, 257, 4356, 2316, 11, 309, 311, 1880], "temperature": 0.0, "avg_logprob": -0.12002404095375374, "compression_ratio": 1.775974025974026, "no_speech_prob": 1.7773787476471625e-05}, {"id": 109, "seek": 30852, "start": 329.4, "end": 330.96, "text": " but it's not exactly training", "tokens": [457, 309, 311, 406, 2293, 3097], "temperature": 0.0, "avg_logprob": -0.12002404095375374, "compression_ratio": 1.775974025974026, "no_speech_prob": 1.7773787476471625e-05}, {"id": 110, "seek": 30852, "start": 330.96, "end": 332.28, "text": " in the way I normally think about it.", "tokens": [294, 264, 636, 286, 5646, 519, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.12002404095375374, "compression_ratio": 1.775974025974026, "no_speech_prob": 1.7773787476471625e-05}, {"id": 111, "seek": 30852, "start": 332.28, "end": 334.12, "text": " So we're not looking at distillation.", "tokens": [407, 321, 434, 406, 1237, 412, 42923, 399, 13], "temperature": 0.0, "avg_logprob": -0.12002404095375374, "compression_ratio": 1.775974025974026, "no_speech_prob": 1.7773787476471625e-05}, {"id": 112, "seek": 30852, "start": 334.12, "end": 337.12, "text": " It would be an interesting assignment if somebody wanted", "tokens": [467, 576, 312, 364, 1880, 15187, 498, 2618, 1415], "temperature": 0.0, "avg_logprob": -0.12002404095375374, "compression_ratio": 1.775974025974026, "no_speech_prob": 1.7773787476471625e-05}, {"id": 113, "seek": 33712, "start": 337.12, "end": 339.6, "text": " to try adding it to the notebooks though.", "tokens": [281, 853, 5127, 309, 281, 264, 43782, 1673, 13], "temperature": 0.0, "avg_logprob": -0.21220154421670095, "compression_ratio": 1.54296875, "no_speech_prob": 1.0782043318613432e-05}, {"id": 114, "seek": 33712, "start": 339.6, "end": 341.64, "text": " You have all the information and I think all the skills you need", "tokens": [509, 362, 439, 264, 1589, 293, 286, 519, 439, 264, 3942, 291, 643], "temperature": 0.0, "avg_logprob": -0.21220154421670095, "compression_ratio": 1.54296875, "no_speech_prob": 1.0782043318613432e-05}, {"id": 115, "seek": 33712, "start": 341.64, "end": 343.44, "text": " to do that now.", "tokens": [281, 360, 300, 586, 13], "temperature": 0.0, "avg_logprob": -0.21220154421670095, "compression_ratio": 1.54296875, "no_speech_prob": 1.0782043318613432e-05}, {"id": 116, "seek": 33712, "start": 343.44, "end": 345.6, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.21220154421670095, "compression_ratio": 1.54296875, "no_speech_prob": 1.0782043318613432e-05}, {"id": 117, "seek": 33712, "start": 345.6, "end": 351.6, "text": " So mixup, we start by grabbing our ImageNet data set", "tokens": [407, 2890, 1010, 11, 321, 722, 538, 23771, 527, 29903, 31890, 1412, 992], "temperature": 0.0, "avg_logprob": -0.21220154421670095, "compression_ratio": 1.54296875, "no_speech_prob": 1.0782043318613432e-05}, {"id": 118, "seek": 33712, "start": 351.6, "end": 357.96, "text": " and we grab the makeRGB and resize and turn it into a float tensor.", "tokens": [293, 321, 4444, 264, 652, 49, 8769, 293, 50069, 293, 1261, 309, 666, 257, 15706, 40863, 13], "temperature": 0.0, "avg_logprob": -0.21220154421670095, "compression_ratio": 1.54296875, "no_speech_prob": 1.0782043318613432e-05}, {"id": 119, "seek": 33712, "start": 357.96, "end": 360.24, "text": " This is just our quick and dirty resize.", "tokens": [639, 307, 445, 527, 1702, 293, 9360, 50069, 13], "temperature": 0.0, "avg_logprob": -0.21220154421670095, "compression_ratio": 1.54296875, "no_speech_prob": 1.0782043318613432e-05}, {"id": 120, "seek": 33712, "start": 360.24, "end": 362.2, "text": " We're only doing this for testing purposes.", "tokens": [492, 434, 787, 884, 341, 337, 4997, 9932, 13], "temperature": 0.0, "avg_logprob": -0.21220154421670095, "compression_ratio": 1.54296875, "no_speech_prob": 1.0782043318613432e-05}, {"id": 121, "seek": 33712, "start": 362.2, "end": 366.8, "text": " Split it up, create a data bunch, all the normal stuff.", "tokens": [45111, 309, 493, 11, 1884, 257, 1412, 3840, 11, 439, 264, 2710, 1507, 13], "temperature": 0.0, "avg_logprob": -0.21220154421670095, "compression_ratio": 1.54296875, "no_speech_prob": 1.0782043318613432e-05}, {"id": 122, "seek": 36680, "start": 366.8, "end": 372.32, "text": " And what we're going to do is we're going to take an image", "tokens": [400, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 747, 364, 3256], "temperature": 0.0, "avg_logprob": -0.1357119083404541, "compression_ratio": 1.8511904761904763, "no_speech_prob": 2.8129416023148224e-06}, {"id": 123, "seek": 36680, "start": 372.32, "end": 379.44, "text": " like this and an image like this and we're going to combine them.", "tokens": [411, 341, 293, 364, 3256, 411, 341, 293, 321, 434, 516, 281, 10432, 552, 13], "temperature": 0.0, "avg_logprob": -0.1357119083404541, "compression_ratio": 1.8511904761904763, "no_speech_prob": 2.8129416023148224e-06}, {"id": 124, "seek": 36680, "start": 379.44, "end": 387.24, "text": " We're going to take.3 times this image plus.7 times this image", "tokens": [492, 434, 516, 281, 747, 2411, 18, 1413, 341, 3256, 1804, 2411, 22, 1413, 341, 3256], "temperature": 0.0, "avg_logprob": -0.1357119083404541, "compression_ratio": 1.8511904761904763, "no_speech_prob": 2.8129416023148224e-06}, {"id": 125, "seek": 36680, "start": 387.24, "end": 389.2, "text": " and this is what it's going to look like.", "tokens": [293, 341, 307, 437, 309, 311, 516, 281, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.1357119083404541, "compression_ratio": 1.8511904761904763, "no_speech_prob": 2.8129416023148224e-06}, {"id": 126, "seek": 36680, "start": 389.2, "end": 392.64, "text": " Unfortunately, Sylvia and I have different orderings", "tokens": [8590, 11, 33349, 11617, 293, 286, 362, 819, 1668, 1109], "temperature": 0.0, "avg_logprob": -0.1357119083404541, "compression_ratio": 1.8511904761904763, "no_speech_prob": 2.8129416023148224e-06}, {"id": 127, "seek": 36680, "start": 392.64, "end": 394.0, "text": " of file names on our things.", "tokens": [295, 3991, 5288, 322, 527, 721, 13], "temperature": 0.0, "avg_logprob": -0.1357119083404541, "compression_ratio": 1.8511904761904763, "no_speech_prob": 2.8129416023148224e-06}, {"id": 128, "seek": 39400, "start": 394.0, "end": 398.08, "text": " So I wrote it's a French horn and a tench but actually Sylvia clearly doesn't have", "tokens": [407, 286, 4114, 309, 311, 257, 5522, 13482, 293, 257, 2064, 339, 457, 767, 33349, 11617, 4448, 1177, 380, 362], "temperature": 0.0, "avg_logprob": -0.12207009071527525, "compression_ratio": 1.7782101167315174, "no_speech_prob": 5.9548892750171944e-06}, {"id": 129, "seek": 39400, "start": 398.08, "end": 400.4, "text": " French horn or tenches but you get the idea.", "tokens": [5522, 13482, 420, 2064, 3781, 457, 291, 483, 264, 1558, 13], "temperature": 0.0, "avg_logprob": -0.12207009071527525, "compression_ratio": 1.7782101167315174, "no_speech_prob": 5.9548892750171944e-06}, {"id": 130, "seek": 39400, "start": 400.4, "end": 402.8, "text": " It's a mixup of two different images.", "tokens": [467, 311, 257, 2890, 1010, 295, 732, 819, 5267, 13], "temperature": 0.0, "avg_logprob": -0.12207009071527525, "compression_ratio": 1.7782101167315174, "no_speech_prob": 5.9548892750171944e-06}, {"id": 131, "seek": 39400, "start": 402.8, "end": 406.0, "text": " So we're going to create a data augmentation", "tokens": [407, 321, 434, 516, 281, 1884, 257, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.12207009071527525, "compression_ratio": 1.7782101167315174, "no_speech_prob": 5.9548892750171944e-06}, {"id": 132, "seek": 39400, "start": 406.0, "end": 409.32, "text": " where every time we predict something we're going", "tokens": [689, 633, 565, 321, 6069, 746, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.12207009071527525, "compression_ratio": 1.7782101167315174, "no_speech_prob": 5.9548892750171944e-06}, {"id": 133, "seek": 39400, "start": 409.32, "end": 412.08, "text": " to be predicting a mix of two things like this.", "tokens": [281, 312, 32884, 257, 2890, 295, 732, 721, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.12207009071527525, "compression_ratio": 1.7782101167315174, "no_speech_prob": 5.9548892750171944e-06}, {"id": 134, "seek": 39400, "start": 412.08, "end": 417.72, "text": " So we're going to both take the linear combination,.3 and.7", "tokens": [407, 321, 434, 516, 281, 1293, 747, 264, 8213, 6562, 11, 2411, 18, 293, 2411, 22], "temperature": 0.0, "avg_logprob": -0.12207009071527525, "compression_ratio": 1.7782101167315174, "no_speech_prob": 5.9548892750171944e-06}, {"id": 135, "seek": 39400, "start": 417.72, "end": 421.6, "text": " of the two images but then we're going to have to do", "tokens": [295, 264, 732, 5267, 457, 550, 321, 434, 516, 281, 362, 281, 360], "temperature": 0.0, "avg_logprob": -0.12207009071527525, "compression_ratio": 1.7782101167315174, "no_speech_prob": 5.9548892750171944e-06}, {"id": 136, "seek": 39400, "start": 421.6, "end": 423.36, "text": " that for the labels as well, right?", "tokens": [300, 337, 264, 16949, 382, 731, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12207009071527525, "compression_ratio": 1.7782101167315174, "no_speech_prob": 5.9548892750171944e-06}, {"id": 137, "seek": 42336, "start": 423.36, "end": 430.36, "text": " There's no point predicting the one hot encoded output of this breed of doggy", "tokens": [821, 311, 572, 935, 32884, 264, 472, 2368, 2058, 12340, 5598, 295, 341, 18971, 295, 3000, 1480], "temperature": 0.0, "avg_logprob": -0.15008399797522504, "compression_ratio": 1.7425149700598803, "no_speech_prob": 2.368735067648231e-06}, {"id": 138, "seek": 42336, "start": 430.36, "end": 433.88, "text": " where there's also a bit of a gas pump.", "tokens": [689, 456, 311, 611, 257, 857, 295, 257, 4211, 5889, 13], "temperature": 0.0, "avg_logprob": -0.15008399797522504, "compression_ratio": 1.7425149700598803, "no_speech_prob": 2.368735067648231e-06}, {"id": 139, "seek": 42336, "start": 433.88, "end": 436.2, "text": " So we're also going to have, we're not going", "tokens": [407, 321, 434, 611, 516, 281, 362, 11, 321, 434, 406, 516], "temperature": 0.0, "avg_logprob": -0.15008399797522504, "compression_ratio": 1.7425149700598803, "no_speech_prob": 2.368735067648231e-06}, {"id": 140, "seek": 42336, "start": 436.2, "end": 437.44, "text": " to have one hot encoded output.", "tokens": [281, 362, 472, 2368, 2058, 12340, 5598, 13], "temperature": 0.0, "avg_logprob": -0.15008399797522504, "compression_ratio": 1.7425149700598803, "no_speech_prob": 2.368735067648231e-06}, {"id": 141, "seek": 42336, "start": 437.44, "end": 445.56, "text": " We're going to have a.7 encoded doggy and a.3 encoded gas pump, right?", "tokens": [492, 434, 516, 281, 362, 257, 2411, 22, 2058, 12340, 3000, 1480, 293, 257, 2411, 18, 2058, 12340, 4211, 5889, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15008399797522504, "compression_ratio": 1.7425149700598803, "no_speech_prob": 2.368735067648231e-06}, {"id": 142, "seek": 42336, "start": 445.56, "end": 448.12, "text": " So that's the basic idea.", "tokens": [407, 300, 311, 264, 3875, 1558, 13], "temperature": 0.0, "avg_logprob": -0.15008399797522504, "compression_ratio": 1.7425149700598803, "no_speech_prob": 2.368735067648231e-06}, {"id": 143, "seek": 44812, "start": 448.12, "end": 453.44, "text": " So the mixup paper was super cool.", "tokens": [407, 264, 2890, 1010, 3035, 390, 1687, 1627, 13], "temperature": 0.0, "avg_logprob": -0.23896649479866028, "compression_ratio": 1.4470588235294117, "no_speech_prob": 6.3386469264514744e-06}, {"id": 144, "seek": 44812, "start": 453.44, "end": 458.64, "text": " Wow, there are people talking about things that aren't deep learning.", "tokens": [3153, 11, 456, 366, 561, 1417, 466, 721, 300, 3212, 380, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.23896649479866028, "compression_ratio": 1.4470588235294117, "no_speech_prob": 6.3386469264514744e-06}, {"id": 145, "seek": 44812, "start": 458.64, "end": 465.04, "text": " I guess that's their priorities.", "tokens": [286, 2041, 300, 311, 641, 15503, 13], "temperature": 0.0, "avg_logprob": -0.23896649479866028, "compression_ratio": 1.4470588235294117, "no_speech_prob": 6.3386469264514744e-06}, {"id": 146, "seek": 44812, "start": 465.04, "end": 471.2, "text": " So the papers are pretty nice, easy read by paper standards", "tokens": [407, 264, 10577, 366, 1238, 1481, 11, 1858, 1401, 538, 3035, 7787], "temperature": 0.0, "avg_logprob": -0.23896649479866028, "compression_ratio": 1.4470588235294117, "no_speech_prob": 6.3386469264514744e-06}, {"id": 147, "seek": 44812, "start": 471.2, "end": 475.04, "text": " and I would definitely suggest you check it out.", "tokens": [293, 286, 576, 2138, 3402, 291, 1520, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.23896649479866028, "compression_ratio": 1.4470588235294117, "no_speech_prob": 6.3386469264514744e-06}, {"id": 148, "seek": 47504, "start": 475.04, "end": 479.64000000000004, "text": " So I've told you what we're going to do.", "tokens": [407, 286, 600, 1907, 291, 437, 321, 434, 516, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.12025288923071065, "compression_ratio": 1.6780487804878048, "no_speech_prob": 4.029133378935512e-06}, {"id": 149, "seek": 47504, "start": 479.64000000000004, "end": 483.32, "text": " Implementation wise we have to decide what number to use here.", "tokens": [4331, 781, 19631, 10829, 321, 362, 281, 4536, 437, 1230, 281, 764, 510, 13], "temperature": 0.0, "avg_logprob": -0.12025288923071065, "compression_ratio": 1.6780487804878048, "no_speech_prob": 4.029133378935512e-06}, {"id": 150, "seek": 47504, "start": 483.32, "end": 486.36, "text": " Is it.3 or.1 or.5 or what?", "tokens": [1119, 309, 2411, 18, 420, 2411, 16, 420, 2411, 20, 420, 437, 30], "temperature": 0.0, "avg_logprob": -0.12025288923071065, "compression_ratio": 1.6780487804878048, "no_speech_prob": 4.029133378935512e-06}, {"id": 151, "seek": 47504, "start": 486.36, "end": 488.20000000000005, "text": " And this is a data augmentation method", "tokens": [400, 341, 307, 257, 1412, 14501, 19631, 3170], "temperature": 0.0, "avg_logprob": -0.12025288923071065, "compression_ratio": 1.6780487804878048, "no_speech_prob": 4.029133378935512e-06}, {"id": 152, "seek": 47504, "start": 488.20000000000005, "end": 490.04, "text": " so the answer is we'll randomize it.", "tokens": [370, 264, 1867, 307, 321, 603, 4974, 1125, 309, 13], "temperature": 0.0, "avg_logprob": -0.12025288923071065, "compression_ratio": 1.6780487804878048, "no_speech_prob": 4.029133378935512e-06}, {"id": 153, "seek": 47504, "start": 490.04, "end": 494.36, "text": " But we're not going to randomize it from 0 to 1 uniform", "tokens": [583, 321, 434, 406, 516, 281, 4974, 1125, 309, 490, 1958, 281, 502, 9452], "temperature": 0.0, "avg_logprob": -0.12025288923071065, "compression_ratio": 1.6780487804878048, "no_speech_prob": 4.029133378935512e-06}, {"id": 154, "seek": 47504, "start": 494.36, "end": 497.16, "text": " or 0 to.5 uniform but instead we're going", "tokens": [420, 1958, 281, 2411, 20, 9452, 457, 2602, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.12025288923071065, "compression_ratio": 1.6780487804878048, "no_speech_prob": 4.029133378935512e-06}, {"id": 155, "seek": 47504, "start": 497.16, "end": 502.08000000000004, "text": " to randomize it using shapes like this.", "tokens": [281, 4974, 1125, 309, 1228, 10854, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.12025288923071065, "compression_ratio": 1.6780487804878048, "no_speech_prob": 4.029133378935512e-06}, {"id": 156, "seek": 50208, "start": 502.08, "end": 506.35999999999996, "text": " In other words, when we grab a random number most", "tokens": [682, 661, 2283, 11, 562, 321, 4444, 257, 4974, 1230, 881], "temperature": 0.0, "avg_logprob": -0.12881544361943784, "compression_ratio": 1.9663865546218486, "no_speech_prob": 1.4061838555790018e-05}, {"id": 157, "seek": 50208, "start": 506.35999999999996, "end": 509.32, "text": " of the time it will be really close to 0 or really close to 1", "tokens": [295, 264, 565, 309, 486, 312, 534, 1998, 281, 1958, 420, 534, 1998, 281, 502], "temperature": 0.0, "avg_logprob": -0.12881544361943784, "compression_ratio": 1.9663865546218486, "no_speech_prob": 1.4061838555790018e-05}, {"id": 158, "seek": 50208, "start": 509.32, "end": 511.56, "text": " and just occasionally it will be close to.5.", "tokens": [293, 445, 16895, 309, 486, 312, 1998, 281, 2411, 20, 13], "temperature": 0.0, "avg_logprob": -0.12881544361943784, "compression_ratio": 1.9663865546218486, "no_speech_prob": 1.4061838555790018e-05}, {"id": 159, "seek": 50208, "start": 511.56, "end": 515.0799999999999, "text": " So that way most of the time it will be pretty easy for our model", "tokens": [407, 300, 636, 881, 295, 264, 565, 309, 486, 312, 1238, 1858, 337, 527, 2316], "temperature": 0.0, "avg_logprob": -0.12881544361943784, "compression_ratio": 1.9663865546218486, "no_speech_prob": 1.4061838555790018e-05}, {"id": 160, "seek": 50208, "start": 515.0799999999999, "end": 517.3199999999999, "text": " because we've been predicting one and only one thing", "tokens": [570, 321, 600, 668, 32884, 472, 293, 787, 472, 551], "temperature": 0.0, "avg_logprob": -0.12881544361943784, "compression_ratio": 1.9663865546218486, "no_speech_prob": 1.4061838555790018e-05}, {"id": 161, "seek": 50208, "start": 517.3199999999999, "end": 519.76, "text": " and just occasionally it will be predicting something", "tokens": [293, 445, 16895, 309, 486, 312, 32884, 746], "temperature": 0.0, "avg_logprob": -0.12881544361943784, "compression_ratio": 1.9663865546218486, "no_speech_prob": 1.4061838555790018e-05}, {"id": 162, "seek": 50208, "start": 519.76, "end": 523.24, "text": " that's a pretty evenly mixed combination.", "tokens": [300, 311, 257, 1238, 17658, 7467, 6562, 13], "temperature": 0.0, "avg_logprob": -0.12881544361943784, "compression_ratio": 1.9663865546218486, "no_speech_prob": 1.4061838555790018e-05}, {"id": 163, "seek": 50208, "start": 523.24, "end": 526.16, "text": " So the ability to grab random numbers", "tokens": [407, 264, 3485, 281, 4444, 4974, 3547], "temperature": 0.0, "avg_logprob": -0.12881544361943784, "compression_ratio": 1.9663865546218486, "no_speech_prob": 1.4061838555790018e-05}, {"id": 164, "seek": 50208, "start": 526.16, "end": 530.8, "text": " that this is basically the histogram, the smooth histogram", "tokens": [300, 341, 307, 1936, 264, 49816, 11, 264, 5508, 49816], "temperature": 0.0, "avg_logprob": -0.12881544361943784, "compression_ratio": 1.9663865546218486, "no_speech_prob": 1.4061838555790018e-05}, {"id": 165, "seek": 53080, "start": 530.8, "end": 537.16, "text": " of how often we're going to see those numbers is called sampling", "tokens": [295, 577, 2049, 321, 434, 516, 281, 536, 729, 3547, 307, 1219, 21179], "temperature": 0.0, "avg_logprob": -0.11566745424733578, "compression_ratio": 1.7072243346007605, "no_speech_prob": 6.438581749534933e-06}, {"id": 166, "seek": 53080, "start": 537.16, "end": 538.76, "text": " from a probability distribution.", "tokens": [490, 257, 8482, 7316, 13], "temperature": 0.0, "avg_logprob": -0.11566745424733578, "compression_ratio": 1.7072243346007605, "no_speech_prob": 6.438581749534933e-06}, {"id": 167, "seek": 53080, "start": 538.76, "end": 541.56, "text": " And basically in nearly all these cases you can start", "tokens": [400, 1936, 294, 6217, 439, 613, 3331, 291, 393, 722], "temperature": 0.0, "avg_logprob": -0.11566745424733578, "compression_ratio": 1.7072243346007605, "no_speech_prob": 6.438581749534933e-06}, {"id": 168, "seek": 53080, "start": 541.56, "end": 545.0, "text": " with a uniform random number or a normal random number and put it", "tokens": [365, 257, 9452, 4974, 1230, 420, 257, 2710, 4974, 1230, 293, 829, 309], "temperature": 0.0, "avg_logprob": -0.11566745424733578, "compression_ratio": 1.7072243346007605, "no_speech_prob": 6.438581749534933e-06}, {"id": 169, "seek": 53080, "start": 545.0, "end": 547.76, "text": " through some kind of function or process to turn it", "tokens": [807, 512, 733, 295, 2445, 420, 1399, 281, 1261, 309], "temperature": 0.0, "avg_logprob": -0.11566745424733578, "compression_ratio": 1.7072243346007605, "no_speech_prob": 6.438581749534933e-06}, {"id": 170, "seek": 53080, "start": 547.76, "end": 549.0799999999999, "text": " into something like this.", "tokens": [666, 746, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.11566745424733578, "compression_ratio": 1.7072243346007605, "no_speech_prob": 6.438581749534933e-06}, {"id": 171, "seek": 53080, "start": 549.0799999999999, "end": 551.64, "text": " So the details don't matter at all.", "tokens": [407, 264, 4365, 500, 380, 1871, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.11566745424733578, "compression_ratio": 1.7072243346007605, "no_speech_prob": 6.438581749534933e-06}, {"id": 172, "seek": 53080, "start": 551.64, "end": 556.3599999999999, "text": " But the paper points out that this particular shape is nicely", "tokens": [583, 264, 3035, 2793, 484, 300, 341, 1729, 3909, 307, 9594], "temperature": 0.0, "avg_logprob": -0.11566745424733578, "compression_ratio": 1.7072243346007605, "no_speech_prob": 6.438581749534933e-06}, {"id": 173, "seek": 53080, "start": 556.3599999999999, "end": 558.7199999999999, "text": " characterized by something called the beta distribution", "tokens": [29361, 538, 746, 1219, 264, 9861, 7316], "temperature": 0.0, "avg_logprob": -0.11566745424733578, "compression_ratio": 1.7072243346007605, "no_speech_prob": 6.438581749534933e-06}, {"id": 174, "seek": 55872, "start": 558.72, "end": 561.2, "text": " so that's what we're going to use.", "tokens": [370, 300, 311, 437, 321, 434, 516, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.13513992187824655, "compression_ratio": 1.6486486486486487, "no_speech_prob": 5.593516561930301e-06}, {"id": 175, "seek": 55872, "start": 561.2, "end": 564.88, "text": " So it was interesting drawing these", "tokens": [407, 309, 390, 1880, 6316, 613], "temperature": 0.0, "avg_logprob": -0.13513992187824655, "compression_ratio": 1.6486486486486487, "no_speech_prob": 5.593516561930301e-06}, {"id": 176, "seek": 55872, "start": 564.88, "end": 569.6800000000001, "text": " because it requires a few interesting bits of math", "tokens": [570, 309, 7029, 257, 1326, 1880, 9239, 295, 5221], "temperature": 0.0, "avg_logprob": -0.13513992187824655, "compression_ratio": 1.6486486486486487, "no_speech_prob": 5.593516561930301e-06}, {"id": 177, "seek": 55872, "start": 569.6800000000001, "end": 572.08, "text": " which some of you may be less comfortable with", "tokens": [597, 512, 295, 291, 815, 312, 1570, 4619, 365], "temperature": 0.0, "avg_logprob": -0.13513992187824655, "compression_ratio": 1.6486486486486487, "no_speech_prob": 5.593516561930301e-06}, {"id": 178, "seek": 55872, "start": 572.08, "end": 575.32, "text": " or entirely uncomfortable with.", "tokens": [420, 7696, 10532, 365, 13], "temperature": 0.0, "avg_logprob": -0.13513992187824655, "compression_ratio": 1.6486486486486487, "no_speech_prob": 5.593516561930301e-06}, {"id": 179, "seek": 55872, "start": 575.32, "end": 579.96, "text": " For me every time I see this function", "tokens": [1171, 385, 633, 565, 286, 536, 341, 2445], "temperature": 0.0, "avg_logprob": -0.13513992187824655, "compression_ratio": 1.6486486486486487, "no_speech_prob": 5.593516561930301e-06}, {"id": 180, "seek": 55872, "start": 579.96, "end": 583.32, "text": " which is called the gamma function I kind of break out in sweats.", "tokens": [597, 307, 1219, 264, 15546, 2445, 286, 733, 295, 1821, 484, 294, 38712, 13], "temperature": 0.0, "avg_logprob": -0.13513992187824655, "compression_ratio": 1.6486486486486487, "no_speech_prob": 5.593516561930301e-06}, {"id": 181, "seek": 55872, "start": 583.32, "end": 586.64, "text": " Not just because I got a cold but it's like you know the idea", "tokens": [1726, 445, 570, 286, 658, 257, 3554, 457, 309, 311, 411, 291, 458, 264, 1558], "temperature": 0.0, "avg_logprob": -0.13513992187824655, "compression_ratio": 1.6486486486486487, "no_speech_prob": 5.593516561930301e-06}, {"id": 182, "seek": 58664, "start": 586.64, "end": 590.24, "text": " of functions that I don't like how do you describe this thing.", "tokens": [295, 6828, 300, 286, 500, 380, 411, 577, 360, 291, 6786, 341, 551, 13], "temperature": 0.0, "avg_logprob": -0.15122616185551196, "compression_ratio": 1.734375, "no_speech_prob": 1.6963411326287314e-05}, {"id": 183, "seek": 58664, "start": 590.24, "end": 594.36, "text": " But actually it turns out that like most things once you look", "tokens": [583, 767, 309, 4523, 484, 300, 411, 881, 721, 1564, 291, 574], "temperature": 0.0, "avg_logprob": -0.15122616185551196, "compression_ratio": 1.734375, "no_speech_prob": 1.6963411326287314e-05}, {"id": 184, "seek": 58664, "start": 594.36, "end": 596.88, "text": " at it it's actually pretty straightforward.", "tokens": [412, 309, 309, 311, 767, 1238, 15325, 13], "temperature": 0.0, "avg_logprob": -0.15122616185551196, "compression_ratio": 1.734375, "no_speech_prob": 1.6963411326287314e-05}, {"id": 185, "seek": 58664, "start": 596.88, "end": 598.4, "text": " And we're going to be using this function", "tokens": [400, 321, 434, 516, 281, 312, 1228, 341, 2445], "temperature": 0.0, "avg_logprob": -0.15122616185551196, "compression_ratio": 1.734375, "no_speech_prob": 1.6963411326287314e-05}, {"id": 186, "seek": 58664, "start": 598.4, "end": 600.24, "text": " so I'll just quickly explain what's going on.", "tokens": [370, 286, 603, 445, 2661, 2903, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.15122616185551196, "compression_ratio": 1.734375, "no_speech_prob": 1.6963411326287314e-05}, {"id": 187, "seek": 58664, "start": 600.24, "end": 603.48, "text": " We're going to start with a factorial function", "tokens": [492, 434, 516, 281, 722, 365, 257, 36916, 2445], "temperature": 0.0, "avg_logprob": -0.15122616185551196, "compression_ratio": 1.734375, "no_speech_prob": 1.6963411326287314e-05}, {"id": 188, "seek": 58664, "start": 603.48, "end": 606.56, "text": " so 1 times 2 times 3 times 4 whatever right.", "tokens": [370, 502, 1413, 568, 1413, 805, 1413, 1017, 2035, 558, 13], "temperature": 0.0, "avg_logprob": -0.15122616185551196, "compression_ratio": 1.734375, "no_speech_prob": 1.6963411326287314e-05}, {"id": 189, "seek": 58664, "start": 606.56, "end": 609.72, "text": " And here these red dots is just the value", "tokens": [400, 510, 613, 2182, 15026, 307, 445, 264, 2158], "temperature": 0.0, "avg_logprob": -0.15122616185551196, "compression_ratio": 1.734375, "no_speech_prob": 1.6963411326287314e-05}, {"id": 190, "seek": 58664, "start": 609.72, "end": 613.24, "text": " of the factorial function for a few different places.", "tokens": [295, 264, 36916, 2445, 337, 257, 1326, 819, 3190, 13], "temperature": 0.0, "avg_logprob": -0.15122616185551196, "compression_ratio": 1.734375, "no_speech_prob": 1.6963411326287314e-05}, {"id": 191, "seek": 61324, "start": 613.24, "end": 618.2, "text": " But don't think of the factorial function as being 1 times 2 times 3", "tokens": [583, 500, 380, 519, 295, 264, 36916, 2445, 382, 885, 502, 1413, 568, 1413, 805], "temperature": 0.0, "avg_logprob": -0.1319923778571705, "compression_ratio": 1.855721393034826, "no_speech_prob": 8.939555300457869e-06}, {"id": 192, "seek": 61324, "start": 618.2, "end": 621.52, "text": " times 4 but times n you know whatever.", "tokens": [1413, 1017, 457, 1413, 297, 291, 458, 2035, 13], "temperature": 0.0, "avg_logprob": -0.1319923778571705, "compression_ratio": 1.855721393034826, "no_speech_prob": 8.939555300457869e-06}, {"id": 193, "seek": 61324, "start": 621.52, "end": 625.96, "text": " But divide both sides by n and now you've got", "tokens": [583, 9845, 1293, 4881, 538, 297, 293, 586, 291, 600, 658], "temperature": 0.0, "avg_logprob": -0.1319923778571705, "compression_ratio": 1.855721393034826, "no_speech_prob": 8.939555300457869e-06}, {"id": 194, "seek": 61324, "start": 625.96, "end": 629.6800000000001, "text": " or divide both sides by n and so now you've got", "tokens": [420, 9845, 1293, 4881, 538, 297, 293, 370, 586, 291, 600, 658], "temperature": 0.0, "avg_logprob": -0.1319923778571705, "compression_ratio": 1.855721393034826, "no_speech_prob": 8.939555300457869e-06}, {"id": 195, "seek": 61324, "start": 629.6800000000001, "end": 636.44, "text": " like factorial n divided by n equals 1 times 2 times 3.", "tokens": [411, 36916, 297, 6666, 538, 297, 6915, 502, 1413, 568, 1413, 805, 13], "temperature": 0.0, "avg_logprob": -0.1319923778571705, "compression_ratio": 1.855721393034826, "no_speech_prob": 8.939555300457869e-06}, {"id": 196, "seek": 61324, "start": 636.44, "end": 639.84, "text": " So it equals the factorial of n minus 1.", "tokens": [407, 309, 6915, 264, 36916, 295, 297, 3175, 502, 13], "temperature": 0.0, "avg_logprob": -0.1319923778571705, "compression_ratio": 1.855721393034826, "no_speech_prob": 8.939555300457869e-06}, {"id": 197, "seek": 61324, "start": 639.84, "end": 643.08, "text": " And so when you define it like that you suddenly realize there's no reason", "tokens": [400, 370, 562, 291, 6964, 309, 411, 300, 291, 5800, 4325, 456, 311, 572, 1778], "temperature": 0.0, "avg_logprob": -0.1319923778571705, "compression_ratio": 1.855721393034826, "no_speech_prob": 8.939555300457869e-06}, {"id": 198, "seek": 64308, "start": 643.08, "end": 646.76, "text": " that you can't have a function that's not just on the integers", "tokens": [300, 291, 393, 380, 362, 257, 2445, 300, 311, 406, 445, 322, 264, 41674], "temperature": 0.0, "avg_logprob": -0.12291357968304609, "compression_ratio": 1.8127090301003344, "no_speech_prob": 8.664347660669591e-06}, {"id": 199, "seek": 64308, "start": 646.76, "end": 649.0, "text": " but is everywhere.", "tokens": [457, 307, 5315, 13], "temperature": 0.0, "avg_logprob": -0.12291357968304609, "compression_ratio": 1.8127090301003344, "no_speech_prob": 8.664347660669591e-06}, {"id": 200, "seek": 64308, "start": 649.0, "end": 651.36, "text": " This is the point where I stop with the math right because to me", "tokens": [639, 307, 264, 935, 689, 286, 1590, 365, 264, 5221, 558, 570, 281, 385], "temperature": 0.0, "avg_logprob": -0.12291357968304609, "compression_ratio": 1.8127090301003344, "no_speech_prob": 8.664347660669591e-06}, {"id": 201, "seek": 64308, "start": 651.36, "end": 654.36, "text": " if I need a sine function or a log function or an x function", "tokens": [498, 286, 643, 257, 18609, 2445, 420, 257, 3565, 2445, 420, 364, 2031, 2445], "temperature": 0.0, "avg_logprob": -0.12291357968304609, "compression_ratio": 1.8127090301003344, "no_speech_prob": 8.664347660669591e-06}, {"id": 202, "seek": 64308, "start": 654.36, "end": 657.0400000000001, "text": " or whatever I type it into my computer and I get it right.", "tokens": [420, 2035, 286, 2010, 309, 666, 452, 3820, 293, 286, 483, 309, 558, 13], "temperature": 0.0, "avg_logprob": -0.12291357968304609, "compression_ratio": 1.8127090301003344, "no_speech_prob": 8.664347660669591e-06}, {"id": 203, "seek": 64308, "start": 657.0400000000001, "end": 660.4000000000001, "text": " So the actual how you get it is not at all important.", "tokens": [407, 264, 3539, 577, 291, 483, 309, 307, 406, 412, 439, 1021, 13], "temperature": 0.0, "avg_logprob": -0.12291357968304609, "compression_ratio": 1.8127090301003344, "no_speech_prob": 8.664347660669591e-06}, {"id": 204, "seek": 64308, "start": 660.4000000000001, "end": 662.08, "text": " But the fact of knowing what these functions are", "tokens": [583, 264, 1186, 295, 5276, 437, 613, 6828, 366], "temperature": 0.0, "avg_logprob": -0.12291357968304609, "compression_ratio": 1.8127090301003344, "no_speech_prob": 8.664347660669591e-06}, {"id": 205, "seek": 64308, "start": 662.08, "end": 664.24, "text": " and how they're defined is useful.", "tokens": [293, 577, 436, 434, 7642, 307, 4420, 13], "temperature": 0.0, "avg_logprob": -0.12291357968304609, "compression_ratio": 1.8127090301003344, "no_speech_prob": 8.664347660669591e-06}, {"id": 206, "seek": 64308, "start": 664.24, "end": 667.4000000000001, "text": " PyTorch doesn't have this function.", "tokens": [9953, 51, 284, 339, 1177, 380, 362, 341, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12291357968304609, "compression_ratio": 1.8127090301003344, "no_speech_prob": 8.664347660669591e-06}, {"id": 207, "seek": 64308, "start": 667.4000000000001, "end": 669.6800000000001, "text": " Weirdly enough they have a log gamma function.", "tokens": [32033, 356, 1547, 436, 362, 257, 3565, 15546, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12291357968304609, "compression_ratio": 1.8127090301003344, "no_speech_prob": 8.664347660669591e-06}, {"id": 208, "seek": 64308, "start": 669.6800000000001, "end": 671.8000000000001, "text": " So we can take log gamma and go e to the power of that", "tokens": [407, 321, 393, 747, 3565, 15546, 293, 352, 308, 281, 264, 1347, 295, 300], "temperature": 0.0, "avg_logprob": -0.12291357968304609, "compression_ratio": 1.8127090301003344, "no_speech_prob": 8.664347660669591e-06}, {"id": 209, "seek": 67180, "start": 671.8, "end": 673.3199999999999, "text": " to get a gamma function.", "tokens": [281, 483, 257, 15546, 2445, 13], "temperature": 0.0, "avg_logprob": -0.10506303460748347, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.844761067739455e-06}, {"id": 210, "seek": 67180, "start": 673.3199999999999, "end": 677.7199999999999, "text": " And you'll see here I am breaking my no Greek letters rule.", "tokens": [400, 291, 603, 536, 510, 286, 669, 7697, 452, 572, 10281, 7825, 4978, 13], "temperature": 0.0, "avg_logprob": -0.10506303460748347, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.844761067739455e-06}, {"id": 211, "seek": 67180, "start": 677.7199999999999, "end": 681.0, "text": " And the reason I'm breaking that rule is because a function", "tokens": [400, 264, 1778, 286, 478, 7697, 300, 4978, 307, 570, 257, 2445], "temperature": 0.0, "avg_logprob": -0.10506303460748347, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.844761067739455e-06}, {"id": 212, "seek": 67180, "start": 681.0, "end": 687.8, "text": " like this doesn't have a kind of domain specific meaning", "tokens": [411, 341, 1177, 380, 362, 257, 733, 295, 9274, 2685, 3620], "temperature": 0.0, "avg_logprob": -0.10506303460748347, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.844761067739455e-06}, {"id": 213, "seek": 67180, "start": 687.8, "end": 690.4, "text": " or a pure physical analogy which is how we always think about it.", "tokens": [420, 257, 6075, 4001, 21663, 597, 307, 577, 321, 1009, 519, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.10506303460748347, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.844761067739455e-06}, {"id": 214, "seek": 67180, "start": 690.4, "end": 692.4, "text": " It's just a math function.", "tokens": [467, 311, 445, 257, 5221, 2445, 13], "temperature": 0.0, "avg_logprob": -0.10506303460748347, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.844761067739455e-06}, {"id": 215, "seek": 67180, "start": 692.4, "end": 694.3599999999999, "text": " And so we call it gamma right.", "tokens": [400, 370, 321, 818, 309, 15546, 558, 13], "temperature": 0.0, "avg_logprob": -0.10506303460748347, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.844761067739455e-06}, {"id": 216, "seek": 67180, "start": 694.3599999999999, "end": 696.7199999999999, "text": " And so if you're going to call it gamma you may as well write it", "tokens": [400, 370, 498, 291, 434, 516, 281, 818, 309, 15546, 291, 815, 382, 731, 2464, 309], "temperature": 0.0, "avg_logprob": -0.10506303460748347, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.844761067739455e-06}, {"id": 217, "seek": 67180, "start": 696.7199999999999, "end": 698.16, "text": " like that.", "tokens": [411, 300, 13], "temperature": 0.0, "avg_logprob": -0.10506303460748347, "compression_ratio": 1.7136752136752136, "no_speech_prob": 3.844761067739455e-06}, {"id": 218, "seek": 69816, "start": 698.16, "end": 702.64, "text": " And why this matters is when you start using it like look", "tokens": [400, 983, 341, 7001, 307, 562, 291, 722, 1228, 309, 411, 574], "temperature": 0.0, "avg_logprob": -0.10807466944423291, "compression_ratio": 1.752988047808765, "no_speech_prob": 7.646062840649392e-06}, {"id": 219, "seek": 69816, "start": 702.64, "end": 707.7199999999999, "text": " at the difference between writing it out with the actual uni code", "tokens": [412, 264, 2649, 1296, 3579, 309, 484, 365, 264, 3539, 36435, 3089], "temperature": 0.0, "avg_logprob": -0.10807466944423291, "compression_ratio": 1.752988047808765, "no_speech_prob": 7.646062840649392e-06}, {"id": 220, "seek": 69816, "start": 707.7199999999999, "end": 710.16, "text": " and operators versus what would happen", "tokens": [293, 19077, 5717, 437, 576, 1051], "temperature": 0.0, "avg_logprob": -0.10807466944423291, "compression_ratio": 1.752988047808765, "no_speech_prob": 7.646062840649392e-06}, {"id": 221, "seek": 69816, "start": 710.16, "end": 713.9599999999999, "text": " if you wrote it out long form in Python.", "tokens": [498, 291, 4114, 309, 484, 938, 1254, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.10807466944423291, "compression_ratio": 1.752988047808765, "no_speech_prob": 7.646062840649392e-06}, {"id": 222, "seek": 69816, "start": 713.9599999999999, "end": 717.12, "text": " Like when you're comparing something to a paper you want", "tokens": [1743, 562, 291, 434, 15763, 746, 281, 257, 3035, 291, 528], "temperature": 0.0, "avg_logprob": -0.10807466944423291, "compression_ratio": 1.752988047808765, "no_speech_prob": 7.646062840649392e-06}, {"id": 223, "seek": 69816, "start": 717.12, "end": 720.52, "text": " something that you can look at and straight away say like oh", "tokens": [746, 300, 291, 393, 574, 412, 293, 2997, 1314, 584, 411, 1954], "temperature": 0.0, "avg_logprob": -0.10807466944423291, "compression_ratio": 1.752988047808765, "no_speech_prob": 7.646062840649392e-06}, {"id": 224, "seek": 69816, "start": 720.52, "end": 722.12, "text": " that looks very familiar.", "tokens": [300, 1542, 588, 4963, 13], "temperature": 0.0, "avg_logprob": -0.10807466944423291, "compression_ratio": 1.752988047808765, "no_speech_prob": 7.646062840649392e-06}, {"id": 225, "seek": 69816, "start": 722.12, "end": 724.6, "text": " And as long as it's not familiar you might want to think", "tokens": [400, 382, 938, 382, 309, 311, 406, 4963, 291, 1062, 528, 281, 519], "temperature": 0.0, "avg_logprob": -0.10807466944423291, "compression_ratio": 1.752988047808765, "no_speech_prob": 7.646062840649392e-06}, {"id": 226, "seek": 69816, "start": 724.6, "end": 726.12, "text": " about how to make it more familiar.", "tokens": [466, 577, 281, 652, 309, 544, 4963, 13], "temperature": 0.0, "avg_logprob": -0.10807466944423291, "compression_ratio": 1.752988047808765, "no_speech_prob": 7.646062840649392e-06}, {"id": 227, "seek": 72612, "start": 726.12, "end": 729.64, "text": " So I'll just briefly mention that writing these math symbols nowadays", "tokens": [407, 286, 603, 445, 10515, 2152, 300, 3579, 613, 5221, 16944, 13434], "temperature": 0.0, "avg_logprob": -0.1239746035510347, "compression_ratio": 1.6920529801324504, "no_speech_prob": 1.8054844986181706e-05}, {"id": 228, "seek": 72612, "start": 729.64, "end": 730.96, "text": " is actually pretty easy.", "tokens": [307, 767, 1238, 1858, 13], "temperature": 0.0, "avg_logprob": -0.1239746035510347, "compression_ratio": 1.6920529801324504, "no_speech_prob": 1.8054844986181706e-05}, {"id": 229, "seek": 72612, "start": 730.96, "end": 735.08, "text": " On Linux there's a thing called a compose key", "tokens": [1282, 18734, 456, 311, 257, 551, 1219, 257, 35925, 2141], "temperature": 0.0, "avg_logprob": -0.1239746035510347, "compression_ratio": 1.6920529801324504, "no_speech_prob": 1.8054844986181706e-05}, {"id": 230, "seek": 72612, "start": 735.08, "end": 736.76, "text": " which is probably already set up for you.", "tokens": [597, 307, 1391, 1217, 992, 493, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.1239746035510347, "compression_ratio": 1.6920529801324504, "no_speech_prob": 1.8054844986181706e-05}, {"id": 231, "seek": 72612, "start": 736.76, "end": 738.8, "text": " And if you Google it you can learn how to turn it on.", "tokens": [400, 498, 291, 3329, 309, 291, 393, 1466, 577, 281, 1261, 309, 322, 13], "temperature": 0.0, "avg_logprob": -0.1239746035510347, "compression_ratio": 1.6920529801324504, "no_speech_prob": 1.8054844986181706e-05}, {"id": 232, "seek": 72612, "start": 738.8, "end": 741.24, "text": " And it's basically like you'll press like the right alt button", "tokens": [400, 309, 311, 1936, 411, 291, 603, 1886, 411, 264, 558, 4955, 2960], "temperature": 0.0, "avg_logprob": -0.1239746035510347, "compression_ratio": 1.6920529801324504, "no_speech_prob": 1.8054844986181706e-05}, {"id": 233, "seek": 72612, "start": 741.24, "end": 742.5600000000001, "text": " or the caps lock button.", "tokens": [420, 264, 13855, 4017, 2960, 13], "temperature": 0.0, "avg_logprob": -0.1239746035510347, "compression_ratio": 1.6920529801324504, "no_speech_prob": 1.8054844986181706e-05}, {"id": 234, "seek": 72612, "start": 742.5600000000001, "end": 744.32, "text": " You can choose what your compose key is.", "tokens": [509, 393, 2826, 437, 428, 35925, 2141, 307, 13], "temperature": 0.0, "avg_logprob": -0.1239746035510347, "compression_ratio": 1.6920529801324504, "no_speech_prob": 1.8054844986181706e-05}, {"id": 235, "seek": 72612, "start": 744.32, "end": 746.04, "text": " And then a few more letters.", "tokens": [400, 550, 257, 1326, 544, 7825, 13], "temperature": 0.0, "avg_logprob": -0.1239746035510347, "compression_ratio": 1.6920529801324504, "no_speech_prob": 1.8054844986181706e-05}, {"id": 236, "seek": 72612, "start": 746.04, "end": 749.44, "text": " So for example all the Greek letters are compose and then star", "tokens": [407, 337, 1365, 439, 264, 10281, 7825, 366, 35925, 293, 550, 3543], "temperature": 0.0, "avg_logprob": -0.1239746035510347, "compression_ratio": 1.6920529801324504, "no_speech_prob": 1.8054844986181706e-05}, {"id": 237, "seek": 72612, "start": 749.44, "end": 751.52, "text": " and then the English letter that corresponds with it.", "tokens": [293, 550, 264, 3669, 5063, 300, 23249, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.1239746035510347, "compression_ratio": 1.6920529801324504, "no_speech_prob": 1.8054844986181706e-05}, {"id": 238, "seek": 75152, "start": 751.52, "end": 757.56, "text": " So for example if I want to do lambda I would go compose star L.", "tokens": [407, 337, 1365, 498, 286, 528, 281, 360, 13607, 286, 576, 352, 35925, 3543, 441, 13], "temperature": 0.0, "avg_logprob": -0.12560931273869105, "compression_ratio": 1.5985130111524164, "no_speech_prob": 9.663272976467852e-06}, {"id": 239, "seek": 75152, "start": 757.56, "end": 760.48, "text": " So it's just as quick as typing non-uni code characters.", "tokens": [407, 309, 311, 445, 382, 1702, 382, 18444, 2107, 12, 24307, 3089, 4342, 13], "temperature": 0.0, "avg_logprob": -0.12560931273869105, "compression_ratio": 1.5985130111524164, "no_speech_prob": 9.663272976467852e-06}, {"id": 240, "seek": 75152, "start": 760.48, "end": 761.96, "text": " Most of the Greek letters are available", "tokens": [4534, 295, 264, 10281, 7825, 366, 2435], "temperature": 0.0, "avg_logprob": -0.12560931273869105, "compression_ratio": 1.5985130111524164, "no_speech_prob": 9.663272976467852e-06}, {"id": 241, "seek": 75152, "start": 761.96, "end": 763.76, "text": " on a Mac keyboard just with option.", "tokens": [322, 257, 5707, 10186, 445, 365, 3614, 13], "temperature": 0.0, "avg_logprob": -0.12560931273869105, "compression_ratio": 1.5985130111524164, "no_speech_prob": 9.663272976467852e-06}, {"id": 242, "seek": 75152, "start": 763.76, "end": 766.88, "text": " Unfortunately nobody's created a decent compose key for Mac yet.", "tokens": [8590, 5079, 311, 2942, 257, 8681, 35925, 2141, 337, 5707, 1939, 13], "temperature": 0.0, "avg_logprob": -0.12560931273869105, "compression_ratio": 1.5985130111524164, "no_speech_prob": 9.663272976467852e-06}, {"id": 243, "seek": 75152, "start": 766.88, "end": 769.6, "text": " There's a great compose key for Windows called WinCompose.", "tokens": [821, 311, 257, 869, 35925, 2141, 337, 8591, 1219, 10427, 34, 8586, 541, 13], "temperature": 0.0, "avg_logprob": -0.12560931273869105, "compression_ratio": 1.5985130111524164, "no_speech_prob": 9.663272976467852e-06}, {"id": 244, "seek": 75152, "start": 769.6, "end": 772.24, "text": " Anybody who's working with, you know,", "tokens": [19082, 567, 311, 1364, 365, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.12560931273869105, "compression_ratio": 1.5985130111524164, "no_speech_prob": 9.663272976467852e-06}, {"id": 245, "seek": 75152, "start": 772.24, "end": 774.96, "text": " Greek letters should definitely install", "tokens": [10281, 7825, 820, 2138, 3625], "temperature": 0.0, "avg_logprob": -0.12560931273869105, "compression_ratio": 1.5985130111524164, "no_speech_prob": 9.663272976467852e-06}, {"id": 246, "seek": 75152, "start": 774.96, "end": 776.68, "text": " and learn to use these things.", "tokens": [293, 1466, 281, 764, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.12560931273869105, "compression_ratio": 1.5985130111524164, "no_speech_prob": 9.663272976467852e-06}, {"id": 247, "seek": 77668, "start": 776.68, "end": 781.52, "text": " So there's our gamma function, nice and concise.", "tokens": [407, 456, 311, 527, 15546, 2445, 11, 1481, 293, 44882, 13], "temperature": 0.0, "avg_logprob": -0.1325409859418869, "compression_ratio": 1.709090909090909, "no_speech_prob": 8.748468098929152e-05}, {"id": 248, "seek": 77668, "start": 781.52, "end": 783.12, "text": " It looks exactly like the paper.", "tokens": [467, 1542, 2293, 411, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.1325409859418869, "compression_ratio": 1.709090909090909, "no_speech_prob": 8.748468098929152e-05}, {"id": 249, "seek": 77668, "start": 783.12, "end": 785.9599999999999, "text": " And so it turns out that this is how you calculate the value", "tokens": [400, 370, 309, 4523, 484, 300, 341, 307, 577, 291, 8873, 264, 2158], "temperature": 0.0, "avg_logprob": -0.1325409859418869, "compression_ratio": 1.709090909090909, "no_speech_prob": 8.748468098929152e-05}, {"id": 250, "seek": 77668, "start": 785.9599999999999, "end": 788.04, "text": " of the beta function which is beta distribution.", "tokens": [295, 264, 9861, 2445, 597, 307, 9861, 7316, 13], "temperature": 0.0, "avg_logprob": -0.1325409859418869, "compression_ratio": 1.709090909090909, "no_speech_prob": 8.748468098929152e-05}, {"id": 251, "seek": 77668, "start": 788.04, "end": 789.4399999999999, "text": " And so now here it is.", "tokens": [400, 370, 586, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1325409859418869, "compression_ratio": 1.709090909090909, "no_speech_prob": 8.748468098929152e-05}, {"id": 252, "seek": 77668, "start": 789.4399999999999, "end": 791.0799999999999, "text": " So as I said the details aren't important", "tokens": [407, 382, 286, 848, 264, 4365, 3212, 380, 1021], "temperature": 0.0, "avg_logprob": -0.1325409859418869, "compression_ratio": 1.709090909090909, "no_speech_prob": 8.748468098929152e-05}, {"id": 253, "seek": 77668, "start": 791.0799999999999, "end": 792.4399999999999, "text": " but there's the tools that you can use.", "tokens": [457, 456, 311, 264, 3873, 300, 291, 393, 764, 13], "temperature": 0.0, "avg_logprob": -0.1325409859418869, "compression_ratio": 1.709090909090909, "no_speech_prob": 8.748468098929152e-05}, {"id": 254, "seek": 77668, "start": 792.4399999999999, "end": 795.3199999999999, "text": " And the basic idea is that we now have something", "tokens": [400, 264, 3875, 1558, 307, 300, 321, 586, 362, 746], "temperature": 0.0, "avg_logprob": -0.1325409859418869, "compression_ratio": 1.709090909090909, "no_speech_prob": 8.748468098929152e-05}, {"id": 255, "seek": 77668, "start": 795.3199999999999, "end": 799.0799999999999, "text": " where we can pick some parameter which is called alpha.", "tokens": [689, 321, 393, 1888, 512, 13075, 597, 307, 1219, 8961, 13], "temperature": 0.0, "avg_logprob": -0.1325409859418869, "compression_ratio": 1.709090909090909, "no_speech_prob": 8.748468098929152e-05}, {"id": 256, "seek": 77668, "start": 799.0799999999999, "end": 802.9599999999999, "text": " Where if it's high then it's much more likely", "tokens": [2305, 498, 309, 311, 1090, 550, 309, 311, 709, 544, 3700], "temperature": 0.0, "avg_logprob": -0.1325409859418869, "compression_ratio": 1.709090909090909, "no_speech_prob": 8.748468098929152e-05}, {"id": 257, "seek": 77668, "start": 802.9599999999999, "end": 804.9599999999999, "text": " that we get equal mix.", "tokens": [300, 321, 483, 2681, 2890, 13], "temperature": 0.0, "avg_logprob": -0.1325409859418869, "compression_ratio": 1.709090909090909, "no_speech_prob": 8.748468098929152e-05}, {"id": 258, "seek": 80496, "start": 804.96, "end": 807.12, "text": " And if it's low it's very unlikely.", "tokens": [400, 498, 309, 311, 2295, 309, 311, 588, 17518, 13], "temperature": 0.0, "avg_logprob": -0.12441134270820908, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.9524011804605834e-05}, {"id": 259, "seek": 80496, "start": 807.12, "end": 809.5600000000001, "text": " And this is really important because for data augmentation we need", "tokens": [400, 341, 307, 534, 1021, 570, 337, 1412, 14501, 19631, 321, 643], "temperature": 0.0, "avg_logprob": -0.12441134270820908, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.9524011804605834e-05}, {"id": 260, "seek": 80496, "start": 809.5600000000001, "end": 813.5600000000001, "text": " to be able to tune a lever that says how much regularization am", "tokens": [281, 312, 1075, 281, 10864, 257, 12451, 300, 1619, 577, 709, 3890, 2144, 669], "temperature": 0.0, "avg_logprob": -0.12441134270820908, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.9524011804605834e-05}, {"id": 261, "seek": 80496, "start": 813.5600000000001, "end": 815.32, "text": " I doing, how much augmentation am I doing.", "tokens": [286, 884, 11, 577, 709, 14501, 19631, 669, 286, 884, 13], "temperature": 0.0, "avg_logprob": -0.12441134270820908, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.9524011804605834e-05}, {"id": 262, "seek": 80496, "start": 815.32, "end": 817.2800000000001, "text": " So you can move your alpha up and down.", "tokens": [407, 291, 393, 1286, 428, 8961, 493, 293, 760, 13], "temperature": 0.0, "avg_logprob": -0.12441134270820908, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.9524011804605834e-05}, {"id": 263, "seek": 80496, "start": 817.2800000000001, "end": 821.0, "text": " And the reason it's important to be able to print these plots out is", "tokens": [400, 264, 1778, 309, 311, 1021, 281, 312, 1075, 281, 4482, 613, 28609, 484, 307], "temperature": 0.0, "avg_logprob": -0.12441134270820908, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.9524011804605834e-05}, {"id": 264, "seek": 80496, "start": 821.0, "end": 822.84, "text": " that when you change your alpha you want to plot it", "tokens": [300, 562, 291, 1319, 428, 8961, 291, 528, 281, 7542, 309], "temperature": 0.0, "avg_logprob": -0.12441134270820908, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.9524011804605834e-05}, {"id": 265, "seek": 80496, "start": 822.84, "end": 824.44, "text": " out and see what it looks like.", "tokens": [484, 293, 536, 437, 309, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.12441134270820908, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.9524011804605834e-05}, {"id": 266, "seek": 80496, "start": 824.44, "end": 828.0400000000001, "text": " Right? Make sure it looks sensible.", "tokens": [1779, 30, 4387, 988, 309, 1542, 25380, 13], "temperature": 0.0, "avg_logprob": -0.12441134270820908, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.9524011804605834e-05}, {"id": 267, "seek": 80496, "start": 828.0400000000001, "end": 833.8000000000001, "text": " Okay. So it turns out that all we need", "tokens": [1033, 13, 407, 309, 4523, 484, 300, 439, 321, 643], "temperature": 0.0, "avg_logprob": -0.12441134270820908, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.9524011804605834e-05}, {"id": 268, "seek": 83380, "start": 833.8, "end": 840.24, "text": " to do then is we don't actually have to 0.7 hot encode one thing", "tokens": [281, 360, 550, 307, 321, 500, 380, 767, 362, 281, 1958, 13, 22, 2368, 2058, 1429, 472, 551], "temperature": 0.0, "avg_logprob": -0.19091957173448929, "compression_ratio": 1.596938775510204, "no_speech_prob": 6.240758466447005e-06}, {"id": 269, "seek": 83380, "start": 840.24, "end": 842.28, "text": " and 0.3 hot encode another thing.", "tokens": [293, 1958, 13, 18, 2368, 2058, 1429, 1071, 551, 13], "temperature": 0.0, "avg_logprob": -0.19091957173448929, "compression_ratio": 1.596938775510204, "no_speech_prob": 6.240758466447005e-06}, {"id": 270, "seek": 83380, "start": 842.28, "end": 846.56, "text": " It's actually identical to simply go, I guess,", "tokens": [467, 311, 767, 14800, 281, 2935, 352, 11, 286, 2041, 11], "temperature": 0.0, "avg_logprob": -0.19091957173448929, "compression_ratio": 1.596938775510204, "no_speech_prob": 6.240758466447005e-06}, {"id": 271, "seek": 83380, "start": 846.56, "end": 852.4799999999999, "text": " which is lambda times the first loss plus 1 minus lambda times the", "tokens": [597, 307, 13607, 1413, 264, 700, 4470, 1804, 502, 3175, 13607, 1413, 264], "temperature": 0.0, "avg_logprob": -0.19091957173448929, "compression_ratio": 1.596938775510204, "no_speech_prob": 6.240758466447005e-06}, {"id": 272, "seek": 83380, "start": 852.4799999999999, "end": 854.56, "text": " second loss because we're using T here.", "tokens": [1150, 4470, 570, 321, 434, 1228, 314, 510, 13], "temperature": 0.0, "avg_logprob": -0.19091957173448929, "compression_ratio": 1.596938775510204, "no_speech_prob": 6.240758466447005e-06}, {"id": 273, "seek": 83380, "start": 854.56, "end": 857.9599999999999, "text": " So that's actually all we need to do.", "tokens": [407, 300, 311, 767, 439, 321, 643, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.19091957173448929, "compression_ratio": 1.596938775510204, "no_speech_prob": 6.240758466447005e-06}, {"id": 274, "seek": 83380, "start": 857.9599999999999, "end": 863.0799999999999, "text": " So this is our mix up.", "tokens": [407, 341, 307, 527, 2890, 493, 13], "temperature": 0.0, "avg_logprob": -0.19091957173448929, "compression_ratio": 1.596938775510204, "no_speech_prob": 6.240758466447005e-06}, {"id": 275, "seek": 86308, "start": 863.08, "end": 865.08, "text": " And again, as you can see we're using the same letters", "tokens": [400, 797, 11, 382, 291, 393, 536, 321, 434, 1228, 264, 912, 7825], "temperature": 0.0, "avg_logprob": -0.11192184186163749, "compression_ratio": 1.76, "no_speech_prob": 4.356552381068468e-06}, {"id": 276, "seek": 86308, "start": 865.08, "end": 866.36, "text": " that we'd expect to see in the paper.", "tokens": [300, 321, 1116, 2066, 281, 536, 294, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.11192184186163749, "compression_ratio": 1.76, "no_speech_prob": 4.356552381068468e-06}, {"id": 277, "seek": 86308, "start": 866.36, "end": 868.5600000000001, "text": " So everything should look very familiar.", "tokens": [407, 1203, 820, 574, 588, 4963, 13], "temperature": 0.0, "avg_logprob": -0.11192184186163749, "compression_ratio": 1.76, "no_speech_prob": 4.356552381068468e-06}, {"id": 278, "seek": 86308, "start": 868.5600000000001, "end": 871.6800000000001, "text": " And mix up, remember, is something which is going", "tokens": [400, 2890, 493, 11, 1604, 11, 307, 746, 597, 307, 516], "temperature": 0.0, "avg_logprob": -0.11192184186163749, "compression_ratio": 1.76, "no_speech_prob": 4.356552381068468e-06}, {"id": 279, "seek": 86308, "start": 871.6800000000001, "end": 873.36, "text": " to change our loss function.", "tokens": [281, 1319, 527, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.11192184186163749, "compression_ratio": 1.76, "no_speech_prob": 4.356552381068468e-06}, {"id": 280, "seek": 86308, "start": 873.36, "end": 875.64, "text": " So we need to know what loss function to change.", "tokens": [407, 321, 643, 281, 458, 437, 4470, 2445, 281, 1319, 13], "temperature": 0.0, "avg_logprob": -0.11192184186163749, "compression_ratio": 1.76, "no_speech_prob": 4.356552381068468e-06}, {"id": 281, "seek": 86308, "start": 875.64, "end": 881.24, "text": " So when you begin fitting you find out what the old loss function", "tokens": [407, 562, 291, 1841, 15669, 291, 915, 484, 437, 264, 1331, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.11192184186163749, "compression_ratio": 1.76, "no_speech_prob": 4.356552381068468e-06}, {"id": 282, "seek": 86308, "start": 881.24, "end": 883.6, "text": " on the learner was and you store it away.", "tokens": [322, 264, 33347, 390, 293, 291, 3531, 309, 1314, 13], "temperature": 0.0, "avg_logprob": -0.11192184186163749, "compression_ratio": 1.76, "no_speech_prob": 4.356552381068468e-06}, {"id": 283, "seek": 86308, "start": 883.6, "end": 887.96, "text": " And then when we calculate loss we can just go ahead and say, oh,", "tokens": [400, 550, 562, 321, 8873, 4470, 321, 393, 445, 352, 2286, 293, 584, 11, 1954, 11], "temperature": 0.0, "avg_logprob": -0.11192184186163749, "compression_ratio": 1.76, "no_speech_prob": 4.356552381068468e-06}, {"id": 284, "seek": 86308, "start": 887.96, "end": 891.2, "text": " if it's invalidation there's no mix up involved.", "tokens": [498, 309, 311, 34702, 399, 456, 311, 572, 2890, 493, 3288, 13], "temperature": 0.0, "avg_logprob": -0.11192184186163749, "compression_ratio": 1.76, "no_speech_prob": 4.356552381068468e-06}, {"id": 285, "seek": 89120, "start": 891.2, "end": 893.88, "text": " And if we're training then we'll calculate the loss", "tokens": [400, 498, 321, 434, 3097, 550, 321, 603, 8873, 264, 4470], "temperature": 0.0, "avg_logprob": -0.12490273525840358, "compression_ratio": 1.6813725490196079, "no_speech_prob": 2.260264409414958e-06}, {"id": 286, "seek": 89120, "start": 893.88, "end": 896.88, "text": " on two different sets of images.", "tokens": [322, 732, 819, 6352, 295, 5267, 13], "temperature": 0.0, "avg_logprob": -0.12490273525840358, "compression_ratio": 1.6813725490196079, "no_speech_prob": 2.260264409414958e-06}, {"id": 287, "seek": 89120, "start": 896.88, "end": 903.44, "text": " One is just the regular set and the second is we'll grab all the other", "tokens": [1485, 307, 445, 264, 3890, 992, 293, 264, 1150, 307, 321, 603, 4444, 439, 264, 661], "temperature": 0.0, "avg_logprob": -0.12490273525840358, "compression_ratio": 1.6813725490196079, "no_speech_prob": 2.260264409414958e-06}, {"id": 288, "seek": 89120, "start": 903.44, "end": 905.12, "text": " images and randomly permute one", "tokens": [5267, 293, 16979, 4784, 1169, 472], "temperature": 0.0, "avg_logprob": -0.12490273525840358, "compression_ratio": 1.6813725490196079, "no_speech_prob": 2.260264409414958e-06}, {"id": 289, "seek": 89120, "start": 905.12, "end": 908.5600000000001, "text": " and randomly pick one to share with.", "tokens": [293, 16979, 1888, 472, 281, 2073, 365, 13], "temperature": 0.0, "avg_logprob": -0.12490273525840358, "compression_ratio": 1.6813725490196079, "no_speech_prob": 2.260264409414958e-06}, {"id": 290, "seek": 89120, "start": 908.5600000000001, "end": 913.6800000000001, "text": " So we do that for the image and we do that for the loss.", "tokens": [407, 321, 360, 300, 337, 264, 3256, 293, 321, 360, 300, 337, 264, 4470, 13], "temperature": 0.0, "avg_logprob": -0.12490273525840358, "compression_ratio": 1.6813725490196079, "no_speech_prob": 2.260264409414958e-06}, {"id": 291, "seek": 89120, "start": 913.6800000000001, "end": 915.8000000000001, "text": " And that's basically it.", "tokens": [400, 300, 311, 1936, 309, 13], "temperature": 0.0, "avg_logprob": -0.12490273525840358, "compression_ratio": 1.6813725490196079, "no_speech_prob": 2.260264409414958e-06}, {"id": 292, "seek": 89120, "start": 915.8000000000001, "end": 919.12, "text": " A couple of minor things to mention.", "tokens": [316, 1916, 295, 6696, 721, 281, 2152, 13], "temperature": 0.0, "avg_logprob": -0.12490273525840358, "compression_ratio": 1.6813725490196079, "no_speech_prob": 2.260264409414958e-06}, {"id": 293, "seek": 91912, "start": 919.12, "end": 923.96, "text": " In the last lesson I created an EWMA function,", "tokens": [682, 264, 1036, 6898, 286, 2942, 364, 462, 54, 9998, 2445, 11], "temperature": 0.0, "avg_logprob": -0.18185880361509718, "compression_ratio": 1.8415094339622642, "no_speech_prob": 8.26644100015983e-06}, {"id": 294, "seek": 91912, "start": 923.96, "end": 926.12, "text": " exponentially weighted moving average function,", "tokens": [37330, 32807, 2684, 4274, 2445, 11], "temperature": 0.0, "avg_logprob": -0.18185880361509718, "compression_ratio": 1.8415094339622642, "no_speech_prob": 8.26644100015983e-06}, {"id": 295, "seek": 91912, "start": 926.12, "end": 928.04, "text": " which is a really dumb name for it.", "tokens": [597, 307, 257, 534, 10316, 1315, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.18185880361509718, "compression_ratio": 1.8415094339622642, "no_speech_prob": 8.26644100015983e-06}, {"id": 296, "seek": 91912, "start": 928.04, "end": 931.2, "text": " Because actually it was just a linear combination of two things.", "tokens": [1436, 767, 309, 390, 445, 257, 8213, 6562, 295, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.18185880361509718, "compression_ratio": 1.8415094339622642, "no_speech_prob": 8.26644100015983e-06}, {"id": 297, "seek": 91912, "start": 931.2, "end": 936.96, "text": " It was like V times alpha plus V1 times alpha plus V2 times 1 minus alpha.", "tokens": [467, 390, 411, 691, 1413, 8961, 1804, 691, 16, 1413, 8961, 1804, 691, 17, 1413, 502, 3175, 8961, 13], "temperature": 0.0, "avg_logprob": -0.18185880361509718, "compression_ratio": 1.8415094339622642, "no_speech_prob": 8.26644100015983e-06}, {"id": 298, "seek": 91912, "start": 936.96, "end": 940.6, "text": " You create exponentially weighted moving averages with it", "tokens": [509, 1884, 37330, 32807, 2684, 42257, 365, 309], "temperature": 0.0, "avg_logprob": -0.18185880361509718, "compression_ratio": 1.8415094339622642, "no_speech_prob": 8.26644100015983e-06}, {"id": 299, "seek": 91912, "start": 940.6, "end": 942.48, "text": " by applying it multiple times.", "tokens": [538, 9275, 309, 3866, 1413, 13], "temperature": 0.0, "avg_logprob": -0.18185880361509718, "compression_ratio": 1.8415094339622642, "no_speech_prob": 8.26644100015983e-06}, {"id": 300, "seek": 91912, "start": 942.48, "end": 944.5600000000001, "text": " But the actual function is a linear combination.", "tokens": [583, 264, 3539, 2445, 307, 257, 8213, 6562, 13], "temperature": 0.0, "avg_logprob": -0.18185880361509718, "compression_ratio": 1.8415094339622642, "no_speech_prob": 8.26644100015983e-06}, {"id": 301, "seek": 91912, "start": 944.5600000000001, "end": 946.76, "text": " So I've renamed that to linear combination.", "tokens": [407, 286, 600, 40949, 300, 281, 8213, 6562, 13], "temperature": 0.0, "avg_logprob": -0.18185880361509718, "compression_ratio": 1.8415094339622642, "no_speech_prob": 8.26644100015983e-06}, {"id": 302, "seek": 91912, "start": 946.76, "end": 948.16, "text": " And you'll see that so many places.", "tokens": [400, 291, 603, 536, 300, 370, 867, 3190, 13], "temperature": 0.0, "avg_logprob": -0.18185880361509718, "compression_ratio": 1.8415094339622642, "no_speech_prob": 8.26644100015983e-06}, {"id": 303, "seek": 94816, "start": 948.16, "end": 953.8, "text": " So this mix up is a linear combination of our actual images", "tokens": [407, 341, 2890, 493, 307, 257, 8213, 6562, 295, 527, 3539, 5267], "temperature": 0.0, "avg_logprob": -0.11815316336495536, "compression_ratio": 1.8140495867768596, "no_speech_prob": 2.6424977477290668e-06}, {"id": 304, "seek": 94816, "start": 953.8, "end": 957.4, "text": " and some randomly permuted images in that mini-batch.", "tokens": [293, 512, 16979, 4784, 4866, 5267, 294, 300, 8382, 12, 65, 852, 13], "temperature": 0.0, "avg_logprob": -0.11815316336495536, "compression_ratio": 1.8140495867768596, "no_speech_prob": 2.6424977477290668e-06}, {"id": 305, "seek": 94816, "start": 957.4, "end": 961.8399999999999, "text": " And our loss is a linear combination of the loss", "tokens": [400, 527, 4470, 307, 257, 8213, 6562, 295, 264, 4470], "temperature": 0.0, "avg_logprob": -0.11815316336495536, "compression_ratio": 1.8140495867768596, "no_speech_prob": 2.6424977477290668e-06}, {"id": 306, "seek": 94816, "start": 961.8399999999999, "end": 964.8399999999999, "text": " of our two different parts, our normal mini-batch", "tokens": [295, 527, 732, 819, 3166, 11, 527, 2710, 8382, 12, 65, 852], "temperature": 0.0, "avg_logprob": -0.11815316336495536, "compression_ratio": 1.8140495867768596, "no_speech_prob": 2.6424977477290668e-06}, {"id": 307, "seek": 94816, "start": 964.8399999999999, "end": 966.7199999999999, "text": " and our randomly permuted mini-batch.", "tokens": [293, 527, 16979, 4784, 4866, 8382, 12, 65, 852, 13], "temperature": 0.0, "avg_logprob": -0.11815316336495536, "compression_ratio": 1.8140495867768596, "no_speech_prob": 2.6424977477290668e-06}, {"id": 308, "seek": 94816, "start": 966.7199999999999, "end": 969.12, "text": " One of the nice things about this is if you think about it,", "tokens": [1485, 295, 264, 1481, 721, 466, 341, 307, 498, 291, 519, 466, 309, 11], "temperature": 0.0, "avg_logprob": -0.11815316336495536, "compression_ratio": 1.8140495867768596, "no_speech_prob": 2.6424977477290668e-06}, {"id": 309, "seek": 94816, "start": 969.12, "end": 971.28, "text": " this is all being applied on the GPU.", "tokens": [341, 307, 439, 885, 6456, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.11815316336495536, "compression_ratio": 1.8140495867768596, "no_speech_prob": 2.6424977477290668e-06}, {"id": 310, "seek": 94816, "start": 971.28, "end": 973.68, "text": " So this is pretty much instant.", "tokens": [407, 341, 307, 1238, 709, 9836, 13], "temperature": 0.0, "avg_logprob": -0.11815316336495536, "compression_ratio": 1.8140495867768596, "no_speech_prob": 2.6424977477290668e-06}, {"id": 311, "seek": 94816, "start": 973.68, "end": 978.0799999999999, "text": " So a super powerful augmentation system, which isn't going", "tokens": [407, 257, 1687, 4005, 14501, 19631, 1185, 11, 597, 1943, 380, 516], "temperature": 0.0, "avg_logprob": -0.11815316336495536, "compression_ratio": 1.8140495867768596, "no_speech_prob": 2.6424977477290668e-06}, {"id": 312, "seek": 97808, "start": 978.08, "end": 982.36, "text": " to add any overhead to our code.", "tokens": [281, 909, 604, 19922, 281, 527, 3089, 13], "temperature": 0.0, "avg_logprob": -0.15126724804148955, "compression_ratio": 1.8142857142857143, "no_speech_prob": 7.766032467770856e-06}, {"id": 313, "seek": 97808, "start": 982.36, "end": 988.48, "text": " One thing to be careful of is that we're actually replacing the loss", "tokens": [1485, 551, 281, 312, 5026, 295, 307, 300, 321, 434, 767, 19139, 264, 4470], "temperature": 0.0, "avg_logprob": -0.15126724804148955, "compression_ratio": 1.8142857142857143, "no_speech_prob": 7.766032467770856e-06}, {"id": 314, "seek": 97808, "start": 988.48, "end": 989.5200000000001, "text": " function.", "tokens": [2445, 13], "temperature": 0.0, "avg_logprob": -0.15126724804148955, "compression_ratio": 1.8142857142857143, "no_speech_prob": 7.766032467770856e-06}, {"id": 315, "seek": 97808, "start": 989.5200000000001, "end": 994.08, "text": " And loss functions have something called a reduction.", "tokens": [400, 4470, 6828, 362, 746, 1219, 257, 11004, 13], "temperature": 0.0, "avg_logprob": -0.15126724804148955, "compression_ratio": 1.8142857142857143, "no_speech_prob": 7.766032467770856e-06}, {"id": 316, "seek": 97808, "start": 994.08, "end": 997.36, "text": " And most PyTorch loss functions you can say,", "tokens": [400, 881, 9953, 51, 284, 339, 4470, 6828, 291, 393, 584, 11], "temperature": 0.0, "avg_logprob": -0.15126724804148955, "compression_ratio": 1.8142857142857143, "no_speech_prob": 7.766032467770856e-06}, {"id": 317, "seek": 97808, "start": 997.36, "end": 999.48, "text": " after calculating the loss function for everything", "tokens": [934, 28258, 264, 4470, 2445, 337, 1203], "temperature": 0.0, "avg_logprob": -0.15126724804148955, "compression_ratio": 1.8142857142857143, "no_speech_prob": 7.766032467770856e-06}, {"id": 318, "seek": 97808, "start": 999.48, "end": 1004.0, "text": " in the mini-batch, either return a rank one tensor of all", "tokens": [294, 264, 8382, 12, 65, 852, 11, 2139, 2736, 257, 6181, 472, 40863, 295, 439], "temperature": 0.0, "avg_logprob": -0.15126724804148955, "compression_ratio": 1.8142857142857143, "no_speech_prob": 7.766032467770856e-06}, {"id": 319, "seek": 97808, "start": 1004.0, "end": 1007.2800000000001, "text": " of the loss functions for the mini-batch, or add them all up,", "tokens": [295, 264, 4470, 6828, 337, 264, 8382, 12, 65, 852, 11, 420, 909, 552, 439, 493, 11], "temperature": 0.0, "avg_logprob": -0.15126724804148955, "compression_ratio": 1.8142857142857143, "no_speech_prob": 7.766032467770856e-06}, {"id": 320, "seek": 100728, "start": 1007.28, "end": 1009.12, "text": " or take the average.", "tokens": [420, 747, 264, 4274, 13], "temperature": 0.0, "avg_logprob": -0.12748003005981445, "compression_ratio": 1.8297872340425532, "no_speech_prob": 1.1477690350147896e-05}, {"id": 321, "seek": 100728, "start": 1009.12, "end": 1010.64, "text": " We pretty much always take the average.", "tokens": [492, 1238, 709, 1009, 747, 264, 4274, 13], "temperature": 0.0, "avg_logprob": -0.12748003005981445, "compression_ratio": 1.8297872340425532, "no_speech_prob": 1.1477690350147896e-05}, {"id": 322, "seek": 100728, "start": 1010.64, "end": 1013.72, "text": " But we just have to make sure that we do the right thing.", "tokens": [583, 321, 445, 362, 281, 652, 988, 300, 321, 360, 264, 558, 551, 13], "temperature": 0.0, "avg_logprob": -0.12748003005981445, "compression_ratio": 1.8297872340425532, "no_speech_prob": 1.1477690350147896e-05}, {"id": 323, "seek": 100728, "start": 1013.72, "end": 1015.9599999999999, "text": " So I've just got a little function here that does the mean", "tokens": [407, 286, 600, 445, 658, 257, 707, 2445, 510, 300, 775, 264, 914], "temperature": 0.0, "avg_logprob": -0.12748003005981445, "compression_ratio": 1.8297872340425532, "no_speech_prob": 1.1477690350147896e-05}, {"id": 324, "seek": 100728, "start": 1015.9599999999999, "end": 1019.72, "text": " or sum, or nothing at all, as requested.", "tokens": [420, 2408, 11, 420, 1825, 412, 439, 11, 382, 16436, 13], "temperature": 0.0, "avg_logprob": -0.12748003005981445, "compression_ratio": 1.8297872340425532, "no_speech_prob": 1.1477690350147896e-05}, {"id": 325, "seek": 100728, "start": 1019.72, "end": 1023.8, "text": " And so then we need to make sure that we create our new loss", "tokens": [400, 370, 550, 321, 643, 281, 652, 988, 300, 321, 1884, 527, 777, 4470], "temperature": 0.0, "avg_logprob": -0.12748003005981445, "compression_ratio": 1.8297872340425532, "no_speech_prob": 1.1477690350147896e-05}, {"id": 326, "seek": 100728, "start": 1023.8, "end": 1027.44, "text": " function that at the end, it's going", "tokens": [2445, 300, 412, 264, 917, 11, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.12748003005981445, "compression_ratio": 1.8297872340425532, "no_speech_prob": 1.1477690350147896e-05}, {"id": 327, "seek": 100728, "start": 1027.44, "end": 1031.36, "text": " to reduce it in the way that they actually asked for.", "tokens": [281, 5407, 309, 294, 264, 636, 300, 436, 767, 2351, 337, 13], "temperature": 0.0, "avg_logprob": -0.12748003005981445, "compression_ratio": 1.8297872340425532, "no_speech_prob": 1.1477690350147896e-05}, {"id": 328, "seek": 100728, "start": 1031.36, "end": 1035.6399999999999, "text": " But then we have to turn off the reduction when we actually", "tokens": [583, 550, 321, 362, 281, 1261, 766, 264, 11004, 562, 321, 767], "temperature": 0.0, "avg_logprob": -0.12748003005981445, "compression_ratio": 1.8297872340425532, "no_speech_prob": 1.1477690350147896e-05}, {"id": 329, "seek": 103564, "start": 1035.64, "end": 1037.8000000000002, "text": " do mixup, because we actually need to calculate the loss", "tokens": [360, 2890, 1010, 11, 570, 321, 767, 643, 281, 8873, 264, 4470], "temperature": 0.0, "avg_logprob": -0.10203221736063484, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.0461959138629027e-05}, {"id": 330, "seek": 103564, "start": 1037.8000000000002, "end": 1043.24, "text": " on every image for both halves of our mixup.", "tokens": [322, 633, 3256, 337, 1293, 38490, 295, 527, 2890, 1010, 13], "temperature": 0.0, "avg_logprob": -0.10203221736063484, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.0461959138629027e-05}, {"id": 331, "seek": 103564, "start": 1043.24, "end": 1046.5200000000002, "text": " So this is a good place to use a context manager, which", "tokens": [407, 341, 307, 257, 665, 1081, 281, 764, 257, 4319, 6598, 11, 597], "temperature": 0.0, "avg_logprob": -0.10203221736063484, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.0461959138629027e-05}, {"id": 332, "seek": 103564, "start": 1046.5200000000002, "end": 1047.5600000000002, "text": " we've seen before.", "tokens": [321, 600, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.10203221736063484, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.0461959138629027e-05}, {"id": 333, "seek": 103564, "start": 1047.5600000000002, "end": 1050.16, "text": " So we just created a tiny little context manager,", "tokens": [407, 321, 445, 2942, 257, 5870, 707, 4319, 6598, 11], "temperature": 0.0, "avg_logprob": -0.10203221736063484, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.0461959138629027e-05}, {"id": 334, "seek": 103564, "start": 1050.16, "end": 1053.16, "text": " which will just find out what the previous reduction was,", "tokens": [597, 486, 445, 915, 484, 437, 264, 3894, 11004, 390, 11], "temperature": 0.0, "avg_logprob": -0.10203221736063484, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.0461959138629027e-05}, {"id": 335, "seek": 103564, "start": 1053.16, "end": 1056.88, "text": " save it away, get rid of it, and then put it back", "tokens": [3155, 309, 1314, 11, 483, 3973, 295, 309, 11, 293, 550, 829, 309, 646], "temperature": 0.0, "avg_logprob": -0.10203221736063484, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.0461959138629027e-05}, {"id": 336, "seek": 103564, "start": 1056.88, "end": 1058.3600000000001, "text": " when it's finished.", "tokens": [562, 309, 311, 4335, 13], "temperature": 0.0, "avg_logprob": -0.10203221736063484, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.0461959138629027e-05}, {"id": 337, "seek": 103564, "start": 1058.3600000000001, "end": 1061.16, "text": " So there's a lot of minor details there.", "tokens": [407, 456, 311, 257, 688, 295, 6696, 4365, 456, 13], "temperature": 0.0, "avg_logprob": -0.10203221736063484, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.0461959138629027e-05}, {"id": 338, "seek": 103564, "start": 1061.16, "end": 1063.44, "text": " But with that in place, the actual mixup itself", "tokens": [583, 365, 300, 294, 1081, 11, 264, 3539, 2890, 1010, 2564], "temperature": 0.0, "avg_logprob": -0.10203221736063484, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.0461959138629027e-05}, {"id": 339, "seek": 103564, "start": 1063.44, "end": 1064.5200000000002, "text": " is very little code.", "tokens": [307, 588, 707, 3089, 13], "temperature": 0.0, "avg_logprob": -0.10203221736063484, "compression_ratio": 1.7058823529411764, "no_speech_prob": 2.0461959138629027e-05}, {"id": 340, "seek": 106452, "start": 1064.52, "end": 1066.08, "text": " It's a single callback.", "tokens": [467, 311, 257, 2167, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.12787533789565883, "compression_ratio": 1.4433962264150944, "no_speech_prob": 4.710759185400093e-06}, {"id": 341, "seek": 106452, "start": 1066.08, "end": 1072.24, "text": " And we can then run it in the usual way, just add mixup.", "tokens": [400, 321, 393, 550, 1190, 309, 294, 264, 7713, 636, 11, 445, 909, 2890, 1010, 13], "temperature": 0.0, "avg_logprob": -0.12787533789565883, "compression_ratio": 1.4433962264150944, "no_speech_prob": 4.710759185400093e-06}, {"id": 342, "seek": 106452, "start": 1072.24, "end": 1075.6, "text": " Our default alpha here is 0.4.", "tokens": [2621, 7576, 8961, 510, 307, 1958, 13, 19, 13], "temperature": 0.0, "avg_logprob": -0.12787533789565883, "compression_ratio": 1.4433962264150944, "no_speech_prob": 4.710759185400093e-06}, {"id": 343, "seek": 106452, "start": 1075.6, "end": 1077.44, "text": " I've been mainly playing with alpha at 0.2,", "tokens": [286, 600, 668, 8704, 2433, 365, 8961, 412, 1958, 13, 17, 11], "temperature": 0.0, "avg_logprob": -0.12787533789565883, "compression_ratio": 1.4433962264150944, "no_speech_prob": 4.710759185400093e-06}, {"id": 344, "seek": 106452, "start": 1077.44, "end": 1079.28, "text": " so this is a bit more than I'm used to.", "tokens": [370, 341, 307, 257, 857, 544, 813, 286, 478, 1143, 281, 13], "temperature": 0.0, "avg_logprob": -0.12787533789565883, "compression_ratio": 1.4433962264150944, "no_speech_prob": 4.710759185400093e-06}, {"id": 345, "seek": 106452, "start": 1079.28, "end": 1084.24, "text": " But somewhere around that vicinity is pretty normal.", "tokens": [583, 4079, 926, 300, 42387, 307, 1238, 2710, 13], "temperature": 0.0, "avg_logprob": -0.12787533789565883, "compression_ratio": 1.4433962264150944, "no_speech_prob": 4.710759185400093e-06}, {"id": 346, "seek": 106452, "start": 1084.24, "end": 1085.56, "text": " So that's mixup.", "tokens": [407, 300, 311, 2890, 1010, 13], "temperature": 0.0, "avg_logprob": -0.12787533789565883, "compression_ratio": 1.4433962264150944, "no_speech_prob": 4.710759185400093e-06}, {"id": 347, "seek": 106452, "start": 1085.56, "end": 1090.8799999999999, "text": " And it's really interesting, because you", "tokens": [400, 309, 311, 534, 1880, 11, 570, 291], "temperature": 0.0, "avg_logprob": -0.12787533789565883, "compression_ratio": 1.4433962264150944, "no_speech_prob": 4.710759185400093e-06}, {"id": 348, "seek": 109088, "start": 1090.88, "end": 1094.92, "text": " could use this for layers other than the input layer.", "tokens": [727, 764, 341, 337, 7914, 661, 813, 264, 4846, 4583, 13], "temperature": 0.0, "avg_logprob": -0.11451000449931727, "compression_ratio": 1.648, "no_speech_prob": 2.4679538910277188e-05}, {"id": 349, "seek": 109088, "start": 1094.92, "end": 1097.92, "text": " You could use it on the first layer, maybe with the embeddings.", "tokens": [509, 727, 764, 309, 322, 264, 700, 4583, 11, 1310, 365, 264, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.11451000449931727, "compression_ratio": 1.648, "no_speech_prob": 2.4679538910277188e-05}, {"id": 350, "seek": 109088, "start": 1097.92, "end": 1100.88, "text": " So you could do mixup augmentation in NLP,", "tokens": [407, 291, 727, 360, 2890, 1010, 14501, 19631, 294, 426, 45196, 11], "temperature": 0.0, "avg_logprob": -0.11451000449931727, "compression_ratio": 1.648, "no_speech_prob": 2.4679538910277188e-05}, {"id": 351, "seek": 109088, "start": 1100.88, "end": 1103.4, "text": " for instance.", "tokens": [337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.11451000449931727, "compression_ratio": 1.648, "no_speech_prob": 2.4679538910277188e-05}, {"id": 352, "seek": 109088, "start": 1103.4, "end": 1106.0800000000002, "text": " It's something which people haven't really", "tokens": [467, 311, 746, 597, 561, 2378, 380, 534], "temperature": 0.0, "avg_logprob": -0.11451000449931727, "compression_ratio": 1.648, "no_speech_prob": 2.4679538910277188e-05}, {"id": 353, "seek": 109088, "start": 1106.0800000000002, "end": 1107.8000000000002, "text": " dug into deeply yet.", "tokens": [22954, 666, 8760, 1939, 13], "temperature": 0.0, "avg_logprob": -0.11451000449931727, "compression_ratio": 1.648, "no_speech_prob": 2.4679538910277188e-05}, {"id": 354, "seek": 109088, "start": 1107.8000000000002, "end": 1110.24, "text": " But it seems to be an opportunity", "tokens": [583, 309, 2544, 281, 312, 364, 2650], "temperature": 0.0, "avg_logprob": -0.11451000449931727, "compression_ratio": 1.648, "no_speech_prob": 2.4679538910277188e-05}, {"id": 355, "seek": 109088, "start": 1110.24, "end": 1114.3200000000002, "text": " to add augmentation in many places where we don't really", "tokens": [281, 909, 14501, 19631, 294, 867, 3190, 689, 321, 500, 380, 534], "temperature": 0.0, "avg_logprob": -0.11451000449931727, "compression_ratio": 1.648, "no_speech_prob": 2.4679538910277188e-05}, {"id": 356, "seek": 109088, "start": 1114.3200000000002, "end": 1116.0400000000002, "text": " see it at the moment, which means", "tokens": [536, 309, 412, 264, 1623, 11, 597, 1355], "temperature": 0.0, "avg_logprob": -0.11451000449931727, "compression_ratio": 1.648, "no_speech_prob": 2.4679538910277188e-05}, {"id": 357, "seek": 109088, "start": 1116.0400000000002, "end": 1118.2800000000002, "text": " we can train better models with less data, which", "tokens": [321, 393, 3847, 1101, 5245, 365, 1570, 1412, 11, 597], "temperature": 0.0, "avg_logprob": -0.11451000449931727, "compression_ratio": 1.648, "no_speech_prob": 2.4679538910277188e-05}, {"id": 358, "seek": 111828, "start": 1118.28, "end": 1121.6, "text": " is why we're here.", "tokens": [307, 983, 321, 434, 510, 13], "temperature": 0.0, "avg_logprob": -0.11869042150435909, "compression_ratio": 1.536480686695279, "no_speech_prob": 8.267208613688126e-06}, {"id": 359, "seek": 111828, "start": 1121.6, "end": 1122.68, "text": " So here's the problem.", "tokens": [407, 510, 311, 264, 1154, 13], "temperature": 0.0, "avg_logprob": -0.11869042150435909, "compression_ratio": 1.536480686695279, "no_speech_prob": 8.267208613688126e-06}, {"id": 360, "seek": 111828, "start": 1122.68, "end": 1125.32, "text": " How does Softmax interact with this?", "tokens": [1012, 775, 16985, 41167, 4648, 365, 341, 30], "temperature": 0.0, "avg_logprob": -0.11869042150435909, "compression_ratio": 1.536480686695279, "no_speech_prob": 8.267208613688126e-06}, {"id": 361, "seek": 111828, "start": 1125.32, "end": 1129.0, "text": " So now we've drawn some random number, lambda.", "tokens": [407, 586, 321, 600, 10117, 512, 4974, 1230, 11, 13607, 13], "temperature": 0.0, "avg_logprob": -0.11869042150435909, "compression_ratio": 1.536480686695279, "no_speech_prob": 8.267208613688126e-06}, {"id": 362, "seek": 111828, "start": 1129.0, "end": 1130.04, "text": " It's 0.7.", "tokens": [467, 311, 1958, 13, 22, 13], "temperature": 0.0, "avg_logprob": -0.11869042150435909, "compression_ratio": 1.536480686695279, "no_speech_prob": 8.267208613688126e-06}, {"id": 363, "seek": 111828, "start": 1130.04, "end": 1133.48, "text": " So I've got 0.7 of a dog and 0.3 of a gas station.", "tokens": [407, 286, 600, 658, 1958, 13, 22, 295, 257, 3000, 293, 1958, 13, 18, 295, 257, 4211, 5214, 13], "temperature": 0.0, "avg_logprob": -0.11869042150435909, "compression_ratio": 1.536480686695279, "no_speech_prob": 8.267208613688126e-06}, {"id": 364, "seek": 111828, "start": 1133.48, "end": 1138.8799999999999, "text": " And the correct answer would be a rank 1 tensor,", "tokens": [400, 264, 3006, 1867, 576, 312, 257, 6181, 502, 40863, 11], "temperature": 0.0, "avg_logprob": -0.11869042150435909, "compression_ratio": 1.536480686695279, "no_speech_prob": 8.267208613688126e-06}, {"id": 365, "seek": 111828, "start": 1138.8799999999999, "end": 1142.16, "text": " which has 0.7 in one spot and 0.3 in the other spot,", "tokens": [597, 575, 1958, 13, 22, 294, 472, 4008, 293, 1958, 13, 18, 294, 264, 661, 4008, 11], "temperature": 0.0, "avg_logprob": -0.11869042150435909, "compression_ratio": 1.536480686695279, "no_speech_prob": 8.267208613688126e-06}, {"id": 366, "seek": 111828, "start": 1142.16, "end": 1144.3999999999999, "text": " and 0 everywhere else.", "tokens": [293, 1958, 5315, 1646, 13], "temperature": 0.0, "avg_logprob": -0.11869042150435909, "compression_ratio": 1.536480686695279, "no_speech_prob": 8.267208613688126e-06}, {"id": 367, "seek": 111828, "start": 1144.3999999999999, "end": 1146.56, "text": " Softmax isn't going to want to do that for me.", "tokens": [16985, 41167, 1943, 380, 516, 281, 528, 281, 360, 300, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.11869042150435909, "compression_ratio": 1.536480686695279, "no_speech_prob": 8.267208613688126e-06}, {"id": 368, "seek": 114656, "start": 1146.56, "end": 1149.08, "text": " Because Softmax really wants just one of my values", "tokens": [1436, 16985, 41167, 534, 2738, 445, 472, 295, 452, 4190], "temperature": 0.0, "avg_logprob": -0.13213427473859088, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.784760221809847e-06}, {"id": 369, "seek": 114656, "start": 1149.08, "end": 1152.6399999999999, "text": " to be high, because it's got an e to the top,", "tokens": [281, 312, 1090, 11, 570, 309, 311, 658, 364, 308, 281, 264, 1192, 11], "temperature": 0.0, "avg_logprob": -0.13213427473859088, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.784760221809847e-06}, {"id": 370, "seek": 114656, "start": 1152.6399999999999, "end": 1154.72, "text": " as we've talked about.", "tokens": [382, 321, 600, 2825, 466, 13], "temperature": 0.0, "avg_logprob": -0.13213427473859088, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.784760221809847e-06}, {"id": 371, "seek": 114656, "start": 1154.72, "end": 1158.72, "text": " So to really use mixup well, and not just to use mixup well,", "tokens": [407, 281, 534, 764, 2890, 1010, 731, 11, 293, 406, 445, 281, 764, 2890, 1010, 731, 11], "temperature": 0.0, "avg_logprob": -0.13213427473859088, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.784760221809847e-06}, {"id": 372, "seek": 114656, "start": 1158.72, "end": 1163.32, "text": " but any time the labels on the data,", "tokens": [457, 604, 565, 264, 16949, 322, 264, 1412, 11], "temperature": 0.0, "avg_logprob": -0.13213427473859088, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.784760221809847e-06}, {"id": 373, "seek": 114656, "start": 1163.32, "end": 1165.6, "text": " you're not 100% sure they're correct,", "tokens": [291, 434, 406, 2319, 4, 988, 436, 434, 3006, 11], "temperature": 0.0, "avg_logprob": -0.13213427473859088, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.784760221809847e-06}, {"id": 374, "seek": 114656, "start": 1165.6, "end": 1170.24, "text": " you don't want to be asking your model to predict one.", "tokens": [291, 500, 380, 528, 281, 312, 3365, 428, 2316, 281, 6069, 472, 13], "temperature": 0.0, "avg_logprob": -0.13213427473859088, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.784760221809847e-06}, {"id": 375, "seek": 114656, "start": 1170.24, "end": 1173.8, "text": " You want to don't predict, I'm 100% sure it's this label,", "tokens": [509, 528, 281, 500, 380, 6069, 11, 286, 478, 2319, 4, 988, 309, 311, 341, 7645, 11], "temperature": 0.0, "avg_logprob": -0.13213427473859088, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.784760221809847e-06}, {"id": 376, "seek": 114656, "start": 1173.8, "end": 1175.36, "text": " because you've got label noise.", "tokens": [570, 291, 600, 658, 7645, 5658, 13], "temperature": 0.0, "avg_logprob": -0.13213427473859088, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.784760221809847e-06}, {"id": 377, "seek": 117536, "start": 1175.36, "end": 1177.7199999999998, "text": " You've got incorrect labels, or you've got mixup mixing,", "tokens": [509, 600, 658, 18424, 16949, 11, 420, 291, 600, 658, 2890, 1010, 11983, 11], "temperature": 0.0, "avg_logprob": -0.1406360069910685, "compression_ratio": 1.7195571955719557, "no_speech_prob": 4.222713869239669e-06}, {"id": 378, "seek": 117536, "start": 1177.7199999999998, "end": 1178.84, "text": " or whatever.", "tokens": [420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.1406360069910685, "compression_ratio": 1.7195571955719557, "no_speech_prob": 4.222713869239669e-06}, {"id": 379, "seek": 117536, "start": 1178.84, "end": 1183.1999999999998, "text": " So instead, we say, oh, don't use one hot encoding", "tokens": [407, 2602, 11, 321, 584, 11, 1954, 11, 500, 380, 764, 472, 2368, 43430], "temperature": 0.0, "avg_logprob": -0.1406360069910685, "compression_ratio": 1.7195571955719557, "no_speech_prob": 4.222713869239669e-06}, {"id": 380, "seek": 117536, "start": 1183.1999999999998, "end": 1186.36, "text": " for the dependent variable, but use a little bit less", "tokens": [337, 264, 12334, 7006, 11, 457, 764, 257, 707, 857, 1570], "temperature": 0.0, "avg_logprob": -0.1406360069910685, "compression_ratio": 1.7195571955719557, "no_speech_prob": 4.222713869239669e-06}, {"id": 381, "seek": 117536, "start": 1186.36, "end": 1187.56, "text": " than one hot encoding.", "tokens": [813, 472, 2368, 43430, 13], "temperature": 0.0, "avg_logprob": -0.1406360069910685, "compression_ratio": 1.7195571955719557, "no_speech_prob": 4.222713869239669e-06}, {"id": 382, "seek": 117536, "start": 1187.56, "end": 1189.8799999999999, "text": " So say 0.9 hot encoding.", "tokens": [407, 584, 1958, 13, 24, 2368, 43430, 13], "temperature": 0.0, "avg_logprob": -0.1406360069910685, "compression_ratio": 1.7195571955719557, "no_speech_prob": 4.222713869239669e-06}, {"id": 383, "seek": 117536, "start": 1189.8799999999999, "end": 1193.24, "text": " So then the correct answer is to say, I'm 90% sure this", "tokens": [407, 550, 264, 3006, 1867, 307, 281, 584, 11, 286, 478, 4289, 4, 988, 341], "temperature": 0.0, "avg_logprob": -0.1406360069910685, "compression_ratio": 1.7195571955719557, "no_speech_prob": 4.222713869239669e-06}, {"id": 384, "seek": 117536, "start": 1193.24, "end": 1194.56, "text": " is the answer.", "tokens": [307, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.1406360069910685, "compression_ratio": 1.7195571955719557, "no_speech_prob": 4.222713869239669e-06}, {"id": 385, "seek": 117536, "start": 1194.56, "end": 1197.24, "text": " And then all of your probabilities have to add to 1.", "tokens": [400, 550, 439, 295, 428, 33783, 362, 281, 909, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.1406360069910685, "compression_ratio": 1.7195571955719557, "no_speech_prob": 4.222713869239669e-06}, {"id": 386, "seek": 117536, "start": 1197.24, "end": 1201.0, "text": " So then all of the negatives, you just put 0.1 divided", "tokens": [407, 550, 439, 295, 264, 40019, 11, 291, 445, 829, 1958, 13, 16, 6666], "temperature": 0.0, "avg_logprob": -0.1406360069910685, "compression_ratio": 1.7195571955719557, "no_speech_prob": 4.222713869239669e-06}, {"id": 387, "seek": 117536, "start": 1201.0, "end": 1203.56, "text": " by n minus 1 on all the rest.", "tokens": [538, 297, 3175, 502, 322, 439, 264, 1472, 13], "temperature": 0.0, "avg_logprob": -0.1406360069910685, "compression_ratio": 1.7195571955719557, "no_speech_prob": 4.222713869239669e-06}, {"id": 388, "seek": 117536, "start": 1203.56, "end": 1205.24, "text": " And that's called label smoothing.", "tokens": [400, 300, 311, 1219, 7645, 899, 6259, 571, 13], "temperature": 0.0, "avg_logprob": -0.1406360069910685, "compression_ratio": 1.7195571955719557, "no_speech_prob": 4.222713869239669e-06}, {"id": 389, "seek": 120524, "start": 1205.24, "end": 1210.48, "text": " And it's a really simple, but astonishingly effective way", "tokens": [400, 309, 311, 257, 534, 2199, 11, 457, 35264, 356, 4942, 636], "temperature": 0.0, "avg_logprob": -0.1536873505178806, "compression_ratio": 1.6282527881040891, "no_speech_prob": 3.6118847219768213e-06}, {"id": 390, "seek": 120524, "start": 1210.48, "end": 1213.36, "text": " to handle noisy labels.", "tokens": [281, 4813, 24518, 16949, 13], "temperature": 0.0, "avg_logprob": -0.1536873505178806, "compression_ratio": 1.6282527881040891, "no_speech_prob": 3.6118847219768213e-06}, {"id": 391, "seek": 120524, "start": 1213.36, "end": 1216.2, "text": " Like, I keep on hearing people saying, oh,", "tokens": [1743, 11, 286, 1066, 322, 4763, 561, 1566, 11, 1954, 11], "temperature": 0.0, "avg_logprob": -0.1536873505178806, "compression_ratio": 1.6282527881040891, "no_speech_prob": 3.6118847219768213e-06}, {"id": 392, "seek": 120524, "start": 1216.2, "end": 1220.2, "text": " we can't use deep learning in this medical problem,", "tokens": [321, 393, 380, 764, 2452, 2539, 294, 341, 4625, 1154, 11], "temperature": 0.0, "avg_logprob": -0.1536873505178806, "compression_ratio": 1.6282527881040891, "no_speech_prob": 3.6118847219768213e-06}, {"id": 393, "seek": 120524, "start": 1220.2, "end": 1224.1200000000001, "text": " because the diagnostic labels in the reports are not perfect,", "tokens": [570, 264, 27897, 16949, 294, 264, 7122, 366, 406, 2176, 11], "temperature": 0.0, "avg_logprob": -0.1536873505178806, "compression_ratio": 1.6282527881040891, "no_speech_prob": 3.6118847219768213e-06}, {"id": 394, "seek": 120524, "start": 1224.1200000000001, "end": 1226.84, "text": " and we don't have a gold standard, and whatever.", "tokens": [293, 321, 500, 380, 362, 257, 3821, 3832, 11, 293, 2035, 13], "temperature": 0.0, "avg_logprob": -0.1536873505178806, "compression_ratio": 1.6282527881040891, "no_speech_prob": 3.6118847219768213e-06}, {"id": 395, "seek": 120524, "start": 1226.84, "end": 1228.88, "text": " It actually turns out that particularly if you lose", "tokens": [467, 767, 4523, 484, 300, 4098, 498, 291, 3624], "temperature": 0.0, "avg_logprob": -0.1536873505178806, "compression_ratio": 1.6282527881040891, "no_speech_prob": 3.6118847219768213e-06}, {"id": 396, "seek": 120524, "start": 1228.88, "end": 1232.28, "text": " label smoothing, noisy data is generally not an option.", "tokens": [7645, 899, 6259, 571, 11, 24518, 1412, 307, 5101, 406, 364, 3614, 13], "temperature": 0.0, "avg_logprob": -0.1536873505178806, "compression_ratio": 1.6282527881040891, "no_speech_prob": 3.6118847219768213e-06}, {"id": 397, "seek": 120524, "start": 1232.28, "end": 1234.76, "text": " Like, there's plenty of examples of people", "tokens": [1743, 11, 456, 311, 7140, 295, 5110, 295, 561], "temperature": 0.0, "avg_logprob": -0.1536873505178806, "compression_ratio": 1.6282527881040891, "no_speech_prob": 3.6118847219768213e-06}, {"id": 398, "seek": 123476, "start": 1234.76, "end": 1237.76, "text": " using this where they literally randomly", "tokens": [1228, 341, 689, 436, 3736, 16979], "temperature": 0.0, "avg_logprob": -0.10116182183319668, "compression_ratio": 1.662551440329218, "no_speech_prob": 1.7330055470665684e-06}, {"id": 399, "seek": 123476, "start": 1237.76, "end": 1241.04, "text": " permute half the labels to make them like 50% wrong,", "tokens": [4784, 1169, 1922, 264, 16949, 281, 652, 552, 411, 2625, 4, 2085, 11], "temperature": 0.0, "avg_logprob": -0.10116182183319668, "compression_ratio": 1.662551440329218, "no_speech_prob": 1.7330055470665684e-06}, {"id": 400, "seek": 123476, "start": 1241.04, "end": 1244.04, "text": " and they still get good results, really good results.", "tokens": [293, 436, 920, 483, 665, 3542, 11, 534, 665, 3542, 13], "temperature": 0.0, "avg_logprob": -0.10116182183319668, "compression_ratio": 1.662551440329218, "no_speech_prob": 1.7330055470665684e-06}, {"id": 401, "seek": 123476, "start": 1244.04, "end": 1248.12, "text": " So don't listen to people in your organization saying,", "tokens": [407, 500, 380, 2140, 281, 561, 294, 428, 4475, 1566, 11], "temperature": 0.0, "avg_logprob": -0.10116182183319668, "compression_ratio": 1.662551440329218, "no_speech_prob": 1.7330055470665684e-06}, {"id": 402, "seek": 123476, "start": 1248.12, "end": 1252.64, "text": " we can't start modeling until we do all this cleanup work.", "tokens": [321, 393, 380, 722, 15983, 1826, 321, 360, 439, 341, 40991, 589, 13], "temperature": 0.0, "avg_logprob": -0.10116182183319668, "compression_ratio": 1.662551440329218, "no_speech_prob": 1.7330055470665684e-06}, {"id": 403, "seek": 123476, "start": 1252.64, "end": 1253.96, "text": " Start modeling right now.", "tokens": [6481, 15983, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.10116182183319668, "compression_ratio": 1.662551440329218, "no_speech_prob": 1.7330055470665684e-06}, {"id": 404, "seek": 123476, "start": 1253.96, "end": 1256.08, "text": " See if the results are OK.", "tokens": [3008, 498, 264, 3542, 366, 2264, 13], "temperature": 0.0, "avg_logprob": -0.10116182183319668, "compression_ratio": 1.662551440329218, "no_speech_prob": 1.7330055470665684e-06}, {"id": 405, "seek": 123476, "start": 1256.08, "end": 1259.04, "text": " And if they are, then maybe you can skip all the cleanup work,", "tokens": [400, 498, 436, 366, 11, 550, 1310, 291, 393, 10023, 439, 264, 40991, 589, 11], "temperature": 0.0, "avg_logprob": -0.10116182183319668, "compression_ratio": 1.662551440329218, "no_speech_prob": 1.7330055470665684e-06}, {"id": 406, "seek": 123476, "start": 1259.04, "end": 1261.24, "text": " or do them simultaneously.", "tokens": [420, 360, 552, 16561, 13], "temperature": 0.0, "avg_logprob": -0.10116182183319668, "compression_ratio": 1.662551440329218, "no_speech_prob": 1.7330055470665684e-06}, {"id": 407, "seek": 126124, "start": 1261.24, "end": 1267.1200000000001, "text": " So label smoothing ends up just being the cross entropy loss,", "tokens": [407, 7645, 899, 6259, 571, 5314, 493, 445, 885, 264, 3278, 30867, 4470, 11], "temperature": 0.0, "avg_logprob": -0.11574574879237584, "compression_ratio": 1.7066115702479339, "no_speech_prob": 5.6818948905856814e-06}, {"id": 408, "seek": 126124, "start": 1267.1200000000001, "end": 1271.72, "text": " as before, times if epsilon is 0.1 and 0.9,", "tokens": [382, 949, 11, 1413, 498, 17889, 307, 1958, 13, 16, 293, 1958, 13, 24, 11], "temperature": 0.0, "avg_logprob": -0.11574574879237584, "compression_ratio": 1.7066115702479339, "no_speech_prob": 5.6818948905856814e-06}, {"id": 409, "seek": 126124, "start": 1271.72, "end": 1277.8, "text": " plus 0.1 times the cross entropy for everything divided by n.", "tokens": [1804, 1958, 13, 16, 1413, 264, 3278, 30867, 337, 1203, 6666, 538, 297, 13], "temperature": 0.0, "avg_logprob": -0.11574574879237584, "compression_ratio": 1.7066115702479339, "no_speech_prob": 5.6818948905856814e-06}, {"id": 410, "seek": 126124, "start": 1277.8, "end": 1280.92, "text": " And the nice thing is that's another linear combination.", "tokens": [400, 264, 1481, 551, 307, 300, 311, 1071, 8213, 6562, 13], "temperature": 0.0, "avg_logprob": -0.11574574879237584, "compression_ratio": 1.7066115702479339, "no_speech_prob": 5.6818948905856814e-06}, {"id": 411, "seek": 126124, "start": 1280.92, "end": 1283.44, "text": " So once you kind of create one of these little mathematical", "tokens": [407, 1564, 291, 733, 295, 1884, 472, 295, 613, 707, 18894], "temperature": 0.0, "avg_logprob": -0.11574574879237584, "compression_ratio": 1.7066115702479339, "no_speech_prob": 5.6818948905856814e-06}, {"id": 412, "seek": 126124, "start": 1283.44, "end": 1285.36, "text": " refactorings that tend to pop up everywhere", "tokens": [1895, 15104, 1109, 300, 3928, 281, 1665, 493, 5315], "temperature": 0.0, "avg_logprob": -0.11574574879237584, "compression_ratio": 1.7066115702479339, "no_speech_prob": 5.6818948905856814e-06}, {"id": 413, "seek": 126124, "start": 1285.36, "end": 1288.04, "text": " and make your code a little bit easier to read,", "tokens": [293, 652, 428, 3089, 257, 707, 857, 3571, 281, 1401, 11], "temperature": 0.0, "avg_logprob": -0.11574574879237584, "compression_ratio": 1.7066115702479339, "no_speech_prob": 5.6818948905856814e-06}, {"id": 414, "seek": 126124, "start": 1288.04, "end": 1290.48, "text": " and a little bit harder to stuff up.", "tokens": [293, 257, 707, 857, 6081, 281, 1507, 493, 13], "temperature": 0.0, "avg_logprob": -0.11574574879237584, "compression_ratio": 1.7066115702479339, "no_speech_prob": 5.6818948905856814e-06}, {"id": 415, "seek": 129048, "start": 1290.48, "end": 1292.44, "text": " Every time I have to write a piece of code,", "tokens": [2048, 565, 286, 362, 281, 2464, 257, 2522, 295, 3089, 11], "temperature": 0.0, "avg_logprob": -0.12047687758747329, "compression_ratio": 1.6721991701244814, "no_speech_prob": 2.7693492938851705e-06}, {"id": 416, "seek": 129048, "start": 1292.44, "end": 1294.96, "text": " there's a very high probability that I'm going to screw it up.", "tokens": [456, 311, 257, 588, 1090, 8482, 300, 286, 478, 516, 281, 5630, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.12047687758747329, "compression_ratio": 1.6721991701244814, "no_speech_prob": 2.7693492938851705e-06}, {"id": 417, "seek": 129048, "start": 1294.96, "end": 1297.6, "text": " So the less I have to write, the less debugging", "tokens": [407, 264, 1570, 286, 362, 281, 2464, 11, 264, 1570, 45592], "temperature": 0.0, "avg_logprob": -0.12047687758747329, "compression_ratio": 1.6721991701244814, "no_speech_prob": 2.7693492938851705e-06}, {"id": 418, "seek": 129048, "start": 1297.6, "end": 1299.48, "text": " I'm going to have to do later.", "tokens": [286, 478, 516, 281, 362, 281, 360, 1780, 13], "temperature": 0.0, "avg_logprob": -0.12047687758747329, "compression_ratio": 1.6721991701244814, "no_speech_prob": 2.7693492938851705e-06}, {"id": 419, "seek": 129048, "start": 1299.48, "end": 1306.28, "text": " So we can just pop that in as a loss function, and away we go.", "tokens": [407, 321, 393, 445, 1665, 300, 294, 382, 257, 4470, 2445, 11, 293, 1314, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.12047687758747329, "compression_ratio": 1.6721991701244814, "no_speech_prob": 2.7693492938851705e-06}, {"id": 420, "seek": 129048, "start": 1306.28, "end": 1311.52, "text": " So that's a super powerful technique,", "tokens": [407, 300, 311, 257, 1687, 4005, 6532, 11], "temperature": 0.0, "avg_logprob": -0.12047687758747329, "compression_ratio": 1.6721991701244814, "no_speech_prob": 2.7693492938851705e-06}, {"id": 421, "seek": 129048, "start": 1311.52, "end": 1313.32, "text": " which it's been around for a couple of years,", "tokens": [597, 309, 311, 668, 926, 337, 257, 1916, 295, 924, 11], "temperature": 0.0, "avg_logprob": -0.12047687758747329, "compression_ratio": 1.6721991701244814, "no_speech_prob": 2.7693492938851705e-06}, {"id": 422, "seek": 129048, "start": 1313.32, "end": 1316.04, "text": " those two techniques, but not nearly as widely used", "tokens": [729, 732, 7512, 11, 457, 406, 6217, 382, 13371, 1143], "temperature": 0.0, "avg_logprob": -0.12047687758747329, "compression_ratio": 1.6721991701244814, "no_speech_prob": 2.7693492938851705e-06}, {"id": 423, "seek": 129048, "start": 1316.04, "end": 1318.68, "text": " as they should be.", "tokens": [382, 436, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.12047687758747329, "compression_ratio": 1.6721991701244814, "no_speech_prob": 2.7693492938851705e-06}, {"id": 424, "seek": 131868, "start": 1318.68, "end": 1324.72, "text": " Then if you're using a Volta, Tensor Core, 2080,", "tokens": [1396, 498, 291, 434, 1228, 257, 8911, 1328, 11, 34306, 14798, 11, 945, 4702, 11], "temperature": 0.0, "avg_logprob": -0.17522842407226563, "compression_ratio": 1.4959016393442623, "no_speech_prob": 8.800032446742989e-06}, {"id": 425, "seek": 131868, "start": 1324.72, "end": 1328.6000000000001, "text": " pretty much any current generation Nvidia graphics", "tokens": [1238, 709, 604, 2190, 5125, 46284, 11837], "temperature": 0.0, "avg_logprob": -0.17522842407226563, "compression_ratio": 1.4959016393442623, "no_speech_prob": 8.800032446742989e-06}, {"id": 426, "seek": 131868, "start": 1328.6000000000001, "end": 1333.3600000000001, "text": " card, you can train using half precision floating point,", "tokens": [2920, 11, 291, 393, 3847, 1228, 1922, 18356, 12607, 935, 11], "temperature": 0.0, "avg_logprob": -0.17522842407226563, "compression_ratio": 1.4959016393442623, "no_speech_prob": 8.800032446742989e-06}, {"id": 427, "seek": 131868, "start": 1333.3600000000001, "end": 1335.76, "text": " in theory, like 10 times faster.", "tokens": [294, 5261, 11, 411, 1266, 1413, 4663, 13], "temperature": 0.0, "avg_logprob": -0.17522842407226563, "compression_ratio": 1.4959016393442623, "no_speech_prob": 8.800032446742989e-06}, {"id": 428, "seek": 131868, "start": 1335.76, "end": 1337.68, "text": " In practice, it doesn't quite work out that way,", "tokens": [682, 3124, 11, 309, 1177, 380, 1596, 589, 484, 300, 636, 11], "temperature": 0.0, "avg_logprob": -0.17522842407226563, "compression_ratio": 1.4959016393442623, "no_speech_prob": 8.800032446742989e-06}, {"id": 429, "seek": 131868, "start": 1337.68, "end": 1339.26, "text": " because there's other things going on.", "tokens": [570, 456, 311, 661, 721, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.17522842407226563, "compression_ratio": 1.4959016393442623, "no_speech_prob": 8.800032446742989e-06}, {"id": 430, "seek": 131868, "start": 1339.26, "end": 1342.8, "text": " But we certainly often see 3x speed ups.", "tokens": [583, 321, 3297, 2049, 536, 805, 87, 3073, 15497, 13], "temperature": 0.0, "avg_logprob": -0.17522842407226563, "compression_ratio": 1.4959016393442623, "no_speech_prob": 8.800032446742989e-06}, {"id": 431, "seek": 131868, "start": 1342.8, "end": 1345.1200000000001, "text": " So the other thing we've got is some work here", "tokens": [407, 264, 661, 551, 321, 600, 658, 307, 512, 589, 510], "temperature": 0.0, "avg_logprob": -0.17522842407226563, "compression_ratio": 1.4959016393442623, "no_speech_prob": 8.800032446742989e-06}, {"id": 432, "seek": 134512, "start": 1345.12, "end": 1350.52, "text": " to allow you to train in half precision floating point.", "tokens": [281, 2089, 291, 281, 3847, 294, 1922, 18356, 12607, 935, 13], "temperature": 0.0, "avg_logprob": -0.14556742687614596, "compression_ratio": 1.6858407079646018, "no_speech_prob": 2.3319646516029024e-06}, {"id": 433, "seek": 134512, "start": 1350.52, "end": 1353.6799999999998, "text": " Now the reason it's not as simple as saying model.half,", "tokens": [823, 264, 1778, 309, 311, 406, 382, 2199, 382, 1566, 2316, 13, 25461, 11], "temperature": 0.0, "avg_logprob": -0.14556742687614596, "compression_ratio": 1.6858407079646018, "no_speech_prob": 2.3319646516029024e-06}, {"id": 434, "seek": 134512, "start": 1353.6799999999998, "end": 1356.3999999999999, "text": " which would convert all of your weights and biases and everything", "tokens": [597, 576, 7620, 439, 295, 428, 17443, 293, 32152, 293, 1203], "temperature": 0.0, "avg_logprob": -0.14556742687614596, "compression_ratio": 1.6858407079646018, "no_speech_prob": 2.3319646516029024e-06}, {"id": 435, "seek": 134512, "start": 1356.3999999999999, "end": 1361.1599999999999, "text": " to half precision floating point, is because of this.", "tokens": [281, 1922, 18356, 12607, 935, 11, 307, 570, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.14556742687614596, "compression_ratio": 1.6858407079646018, "no_speech_prob": 2.3319646516029024e-06}, {"id": 436, "seek": 134512, "start": 1361.1599999999999, "end": 1365.36, "text": " This is from Nvidia's materials.", "tokens": [639, 307, 490, 46284, 311, 5319, 13], "temperature": 0.0, "avg_logprob": -0.14556742687614596, "compression_ratio": 1.6858407079646018, "no_speech_prob": 2.3319646516029024e-06}, {"id": 437, "seek": 134512, "start": 1365.36, "end": 1368.4399999999998, "text": " And what they point out is that you can't just", "tokens": [400, 437, 436, 935, 484, 307, 300, 291, 393, 380, 445], "temperature": 0.0, "avg_logprob": -0.14556742687614596, "compression_ratio": 1.6858407079646018, "no_speech_prob": 2.3319646516029024e-06}, {"id": 438, "seek": 134512, "start": 1368.4399999999998, "end": 1372.36, "text": " use half precision everywhere, because it's not accurate.", "tokens": [764, 1922, 18356, 5315, 11, 570, 309, 311, 406, 8559, 13], "temperature": 0.0, "avg_logprob": -0.14556742687614596, "compression_ratio": 1.6858407079646018, "no_speech_prob": 2.3319646516029024e-06}, {"id": 439, "seek": 134512, "start": 1372.36, "end": 1373.7199999999998, "text": " It's bumpy.", "tokens": [467, 311, 49400, 13], "temperature": 0.0, "avg_logprob": -0.14556742687614596, "compression_ratio": 1.6858407079646018, "no_speech_prob": 2.3319646516029024e-06}, {"id": 440, "seek": 137372, "start": 1373.72, "end": 1378.04, "text": " So it's hard to get good, useful gradients if you do everything", "tokens": [407, 309, 311, 1152, 281, 483, 665, 11, 4420, 2771, 2448, 498, 291, 360, 1203], "temperature": 0.0, "avg_logprob": -0.1234200640422542, "compression_ratio": 1.821705426356589, "no_speech_prob": 3.7265140235831495e-06}, {"id": 441, "seek": 137372, "start": 1378.04, "end": 1379.2, "text": " in half precision.", "tokens": [294, 1922, 18356, 13], "temperature": 0.0, "avg_logprob": -0.1234200640422542, "compression_ratio": 1.821705426356589, "no_speech_prob": 3.7265140235831495e-06}, {"id": 442, "seek": 137372, "start": 1379.2, "end": 1382.24, "text": " And particularly often, things will round off to zero.", "tokens": [400, 4098, 2049, 11, 721, 486, 3098, 766, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.1234200640422542, "compression_ratio": 1.821705426356589, "no_speech_prob": 3.7265140235831495e-06}, {"id": 443, "seek": 137372, "start": 1382.24, "end": 1387.4, "text": " So instead, what we do is we do the forward pass in FP16.", "tokens": [407, 2602, 11, 437, 321, 360, 307, 321, 360, 264, 2128, 1320, 294, 36655, 6866, 13], "temperature": 0.0, "avg_logprob": -0.1234200640422542, "compression_ratio": 1.821705426356589, "no_speech_prob": 3.7265140235831495e-06}, {"id": 444, "seek": 137372, "start": 1387.4, "end": 1389.92, "text": " We do the backward pass in FP16.", "tokens": [492, 360, 264, 23897, 1320, 294, 36655, 6866, 13], "temperature": 0.0, "avg_logprob": -0.1234200640422542, "compression_ratio": 1.821705426356589, "no_speech_prob": 3.7265140235831495e-06}, {"id": 445, "seek": 137372, "start": 1389.92, "end": 1393.92, "text": " So all the hard work is done in half precision floating point.", "tokens": [407, 439, 264, 1152, 589, 307, 1096, 294, 1922, 18356, 12607, 935, 13], "temperature": 0.0, "avg_logprob": -0.1234200640422542, "compression_ratio": 1.821705426356589, "no_speech_prob": 3.7265140235831495e-06}, {"id": 446, "seek": 137372, "start": 1393.92, "end": 1396.0, "text": " And pretty much everywhere else, we", "tokens": [400, 1238, 709, 5315, 1646, 11, 321], "temperature": 0.0, "avg_logprob": -0.1234200640422542, "compression_ratio": 1.821705426356589, "no_speech_prob": 3.7265140235831495e-06}, {"id": 447, "seek": 137372, "start": 1396.0, "end": 1398.32, "text": " convert things to full precision floating point", "tokens": [7620, 721, 281, 1577, 18356, 12607, 935], "temperature": 0.0, "avg_logprob": -0.1234200640422542, "compression_ratio": 1.821705426356589, "no_speech_prob": 3.7265140235831495e-06}, {"id": 448, "seek": 137372, "start": 1398.32, "end": 1400.6000000000001, "text": " and do everything else in full precision.", "tokens": [293, 360, 1203, 1646, 294, 1577, 18356, 13], "temperature": 0.0, "avg_logprob": -0.1234200640422542, "compression_ratio": 1.821705426356589, "no_speech_prob": 3.7265140235831495e-06}, {"id": 449, "seek": 137372, "start": 1400.6000000000001, "end": 1402.84, "text": " So for example, when we actually apply the gradients", "tokens": [407, 337, 1365, 11, 562, 321, 767, 3079, 264, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.1234200640422542, "compression_ratio": 1.821705426356589, "no_speech_prob": 3.7265140235831495e-06}, {"id": 450, "seek": 140284, "start": 1402.84, "end": 1404.48, "text": " by multiplying by the learning rate,", "tokens": [538, 30955, 538, 264, 2539, 3314, 11], "temperature": 0.0, "avg_logprob": -0.14681895737795486, "compression_ratio": 1.5205479452054795, "no_speech_prob": 5.771664746134775e-06}, {"id": 451, "seek": 140284, "start": 1404.48, "end": 1408.6799999999998, "text": " we do that in FP32, single precision.", "tokens": [321, 360, 300, 294, 36655, 11440, 11, 2167, 18356, 13], "temperature": 0.0, "avg_logprob": -0.14681895737795486, "compression_ratio": 1.5205479452054795, "no_speech_prob": 5.771664746134775e-06}, {"id": 452, "seek": 140284, "start": 1408.6799999999998, "end": 1412.52, "text": " And that means that if your learning rate's really small,", "tokens": [400, 300, 1355, 300, 498, 428, 2539, 3314, 311, 534, 1359, 11], "temperature": 0.0, "avg_logprob": -0.14681895737795486, "compression_ratio": 1.5205479452054795, "no_speech_prob": 5.771664746134775e-06}, {"id": 453, "seek": 140284, "start": 1412.52, "end": 1416.1599999999999, "text": " in FP16, it might basically round down to zero.", "tokens": [294, 36655, 6866, 11, 309, 1062, 1936, 3098, 760, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.14681895737795486, "compression_ratio": 1.5205479452054795, "no_speech_prob": 5.771664746134775e-06}, {"id": 454, "seek": 140284, "start": 1416.1599999999999, "end": 1420.1999999999998, "text": " So we do it in FP32.", "tokens": [407, 321, 360, 309, 294, 36655, 11440, 13], "temperature": 0.0, "avg_logprob": -0.14681895737795486, "compression_ratio": 1.5205479452054795, "no_speech_prob": 5.771664746134775e-06}, {"id": 455, "seek": 140284, "start": 1420.1999999999998, "end": 1424.48, "text": " In Fast AI version 1, we wrote all this by hand.", "tokens": [682, 15968, 7318, 3037, 502, 11, 321, 4114, 439, 341, 538, 1011, 13], "temperature": 0.0, "avg_logprob": -0.14681895737795486, "compression_ratio": 1.5205479452054795, "no_speech_prob": 5.771664746134775e-06}, {"id": 456, "seek": 140284, "start": 1424.48, "end": 1426.34, "text": " For the lessons, we're experimenting", "tokens": [1171, 264, 8820, 11, 321, 434, 29070], "temperature": 0.0, "avg_logprob": -0.14681895737795486, "compression_ratio": 1.5205479452054795, "no_speech_prob": 5.771664746134775e-06}, {"id": 457, "seek": 140284, "start": 1426.34, "end": 1430.12, "text": " with using a library from Nvidia called Apex.", "tokens": [365, 1228, 257, 6405, 490, 46284, 1219, 316, 29420, 13], "temperature": 0.0, "avg_logprob": -0.14681895737795486, "compression_ratio": 1.5205479452054795, "no_speech_prob": 5.771664746134775e-06}, {"id": 458, "seek": 143012, "start": 1430.12, "end": 1433.36, "text": " Apex basically have some of the functions to do this there", "tokens": [316, 29420, 1936, 362, 512, 295, 264, 6828, 281, 360, 341, 456], "temperature": 0.0, "avg_logprob": -0.17592940896244372, "compression_ratio": 1.705179282868526, "no_speech_prob": 1.3005993423576001e-05}, {"id": 459, "seek": 143012, "start": 1433.36, "end": 1435.4799999999998, "text": " for you.", "tokens": [337, 291, 13], "temperature": 0.0, "avg_logprob": -0.17592940896244372, "compression_ratio": 1.705179282868526, "no_speech_prob": 1.3005993423576001e-05}, {"id": 460, "seek": 143012, "start": 1435.4799999999998, "end": 1436.52, "text": " So we're using it here.", "tokens": [407, 321, 434, 1228, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.17592940896244372, "compression_ratio": 1.705179282868526, "no_speech_prob": 1.3005993423576001e-05}, {"id": 461, "seek": 143012, "start": 1438.9199999999998, "end": 1440.36, "text": " And basically, you can see there's", "tokens": [400, 1936, 11, 291, 393, 536, 456, 311], "temperature": 0.0, "avg_logprob": -0.17592940896244372, "compression_ratio": 1.705179282868526, "no_speech_prob": 1.3005993423576001e-05}, {"id": 462, "seek": 143012, "start": 1440.36, "end": 1443.36, "text": " a thing called model to half, where we just go model.half.", "tokens": [257, 551, 1219, 2316, 281, 1922, 11, 689, 321, 445, 352, 2316, 13, 25461, 13], "temperature": 0.0, "avg_logprob": -0.17592940896244372, "compression_ratio": 1.705179282868526, "no_speech_prob": 1.3005993423576001e-05}, {"id": 463, "seek": 143012, "start": 1443.36, "end": 1445.8, "text": " Batch norm goes to float, and so forth.", "tokens": [363, 852, 2026, 1709, 281, 15706, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.17592940896244372, "compression_ratio": 1.705179282868526, "no_speech_prob": 1.3005993423576001e-05}, {"id": 464, "seek": 143012, "start": 1445.8, "end": 1447.7199999999998, "text": " So these are not particularly interesting.", "tokens": [407, 613, 366, 406, 4098, 1880, 13], "temperature": 0.0, "avg_logprob": -0.17592940896244372, "compression_ratio": 1.705179282868526, "no_speech_prob": 1.3005993423576001e-05}, {"id": 465, "seek": 143012, "start": 1447.7199999999998, "end": 1449.3799999999999, "text": " But they're just going through each one", "tokens": [583, 436, 434, 445, 516, 807, 1184, 472], "temperature": 0.0, "avg_logprob": -0.17592940896244372, "compression_ratio": 1.705179282868526, "no_speech_prob": 1.3005993423576001e-05}, {"id": 466, "seek": 143012, "start": 1449.3799999999999, "end": 1453.1599999999999, "text": " and making sure that the right layers have the right types.", "tokens": [293, 1455, 988, 300, 264, 558, 7914, 362, 264, 558, 3467, 13], "temperature": 0.0, "avg_logprob": -0.17592940896244372, "compression_ratio": 1.705179282868526, "no_speech_prob": 1.3005993423576001e-05}, {"id": 467, "seek": 143012, "start": 1453.1599999999999, "end": 1458.12, "text": " So once we've got those kind of utility functions in place,", "tokens": [407, 1564, 321, 600, 658, 729, 733, 295, 14877, 6828, 294, 1081, 11], "temperature": 0.0, "avg_logprob": -0.17592940896244372, "compression_ratio": 1.705179282868526, "no_speech_prob": 1.3005993423576001e-05}, {"id": 468, "seek": 145812, "start": 1458.12, "end": 1461.4799999999998, "text": " the actual callback's really quite small.", "tokens": [264, 3539, 818, 3207, 311, 534, 1596, 1359, 13], "temperature": 0.0, "avg_logprob": -0.15163307189941405, "compression_ratio": 1.641025641025641, "no_speech_prob": 5.173717909201514e-06}, {"id": 469, "seek": 145812, "start": 1461.4799999999998, "end": 1464.6399999999999, "text": " And you'll be able to map every stage to that picture", "tokens": [400, 291, 603, 312, 1075, 281, 4471, 633, 3233, 281, 300, 3036], "temperature": 0.0, "avg_logprob": -0.15163307189941405, "compression_ratio": 1.641025641025641, "no_speech_prob": 5.173717909201514e-06}, {"id": 470, "seek": 145812, "start": 1464.6399999999999, "end": 1466.4799999999998, "text": " I showed you before.", "tokens": [286, 4712, 291, 949, 13], "temperature": 0.0, "avg_logprob": -0.15163307189941405, "compression_ratio": 1.641025641025641, "no_speech_prob": 5.173717909201514e-06}, {"id": 471, "seek": 145812, "start": 1466.4799999999998, "end": 1468.84, "text": " So you'll be able to see when we start fitting,", "tokens": [407, 291, 603, 312, 1075, 281, 536, 562, 321, 722, 15669, 11], "temperature": 0.0, "avg_logprob": -0.15163307189941405, "compression_ratio": 1.641025641025641, "no_speech_prob": 5.173717909201514e-06}, {"id": 472, "seek": 145812, "start": 1468.84, "end": 1472.6, "text": " we convert the network to half precision floating point,", "tokens": [321, 7620, 264, 3209, 281, 1922, 18356, 12607, 935, 11], "temperature": 0.0, "avg_logprob": -0.15163307189941405, "compression_ratio": 1.641025641025641, "no_speech_prob": 5.173717909201514e-06}, {"id": 473, "seek": 145812, "start": 1472.6, "end": 1474.7199999999998, "text": " for example.", "tokens": [337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.15163307189941405, "compression_ratio": 1.641025641025641, "no_speech_prob": 5.173717909201514e-06}, {"id": 474, "seek": 145812, "start": 1474.7199999999998, "end": 1476.56, "text": " One of the things that's kind of interesting", "tokens": [1485, 295, 264, 721, 300, 311, 733, 295, 1880], "temperature": 0.0, "avg_logprob": -0.15163307189941405, "compression_ratio": 1.641025641025641, "no_speech_prob": 5.173717909201514e-06}, {"id": 475, "seek": 145812, "start": 1476.56, "end": 1478.7199999999998, "text": " is there's something here called loss scale.", "tokens": [307, 456, 311, 746, 510, 1219, 4470, 4373, 13], "temperature": 0.0, "avg_logprob": -0.15163307189941405, "compression_ratio": 1.641025641025641, "no_speech_prob": 5.173717909201514e-06}, {"id": 476, "seek": 145812, "start": 1478.7199999999998, "end": 1486.4799999999998, "text": " After the backward pass, well, probably more interestingly,", "tokens": [2381, 264, 23897, 1320, 11, 731, 11, 1391, 544, 25873, 11], "temperature": 0.0, "avg_logprob": -0.15163307189941405, "compression_ratio": 1.641025641025641, "no_speech_prob": 5.173717909201514e-06}, {"id": 477, "seek": 148648, "start": 1486.48, "end": 1488.8, "text": " after the loss is calculated, we multiply it", "tokens": [934, 264, 4470, 307, 15598, 11, 321, 12972, 309], "temperature": 0.0, "avg_logprob": -0.15511581592990042, "compression_ratio": 1.7588652482269505, "no_speech_prob": 8.397568308282644e-06}, {"id": 478, "seek": 148648, "start": 1488.8, "end": 1491.0, "text": " by this number called loss scale, which is generally", "tokens": [538, 341, 1230, 1219, 4470, 4373, 11, 597, 307, 5101], "temperature": 0.0, "avg_logprob": -0.15511581592990042, "compression_ratio": 1.7588652482269505, "no_speech_prob": 8.397568308282644e-06}, {"id": 479, "seek": 148648, "start": 1491.0, "end": 1492.68, "text": " something around 512.", "tokens": [746, 926, 1025, 4762, 13], "temperature": 0.0, "avg_logprob": -0.15511581592990042, "compression_ratio": 1.7588652482269505, "no_speech_prob": 8.397568308282644e-06}, {"id": 480, "seek": 148648, "start": 1492.68, "end": 1494.56, "text": " The reason we do that is that losses", "tokens": [440, 1778, 321, 360, 300, 307, 300, 15352], "temperature": 0.0, "avg_logprob": -0.15511581592990042, "compression_ratio": 1.7588652482269505, "no_speech_prob": 8.397568308282644e-06}, {"id": 481, "seek": 148648, "start": 1494.56, "end": 1498.0, "text": " tend to be pretty small in a region where half precision", "tokens": [3928, 281, 312, 1238, 1359, 294, 257, 4458, 689, 1922, 18356], "temperature": 0.0, "avg_logprob": -0.15511581592990042, "compression_ratio": 1.7588652482269505, "no_speech_prob": 8.397568308282644e-06}, {"id": 482, "seek": 148648, "start": 1498.0, "end": 1499.68, "text": " floating point's not very accurate.", "tokens": [12607, 935, 311, 406, 588, 8559, 13], "temperature": 0.0, "avg_logprob": -0.15511581592990042, "compression_ratio": 1.7588652482269505, "no_speech_prob": 8.397568308282644e-06}, {"id": 483, "seek": 148648, "start": 1499.68, "end": 1501.48, "text": " So we just multiply it by 512.", "tokens": [407, 321, 445, 12972, 309, 538, 1025, 4762, 13], "temperature": 0.0, "avg_logprob": -0.15511581592990042, "compression_ratio": 1.7588652482269505, "no_speech_prob": 8.397568308282644e-06}, {"id": 484, "seek": 148648, "start": 1501.48, "end": 1503.28, "text": " Put it in a region that is accurate.", "tokens": [4935, 309, 294, 257, 4458, 300, 307, 8559, 13], "temperature": 0.0, "avg_logprob": -0.15511581592990042, "compression_ratio": 1.7588652482269505, "no_speech_prob": 8.397568308282644e-06}, {"id": 485, "seek": 148648, "start": 1503.28, "end": 1504.88, "text": " And then later on, in the backward step,", "tokens": [400, 550, 1780, 322, 11, 294, 264, 23897, 1823, 11], "temperature": 0.0, "avg_logprob": -0.15511581592990042, "compression_ratio": 1.7588652482269505, "no_speech_prob": 8.397568308282644e-06}, {"id": 486, "seek": 148648, "start": 1504.88, "end": 1506.64, "text": " we just divide by that again.", "tokens": [321, 445, 9845, 538, 300, 797, 13], "temperature": 0.0, "avg_logprob": -0.15511581592990042, "compression_ratio": 1.7588652482269505, "no_speech_prob": 8.397568308282644e-06}, {"id": 487, "seek": 148648, "start": 1506.64, "end": 1509.64, "text": " So that's a little tweak, but it's the difference", "tokens": [407, 300, 311, 257, 707, 29879, 11, 457, 309, 311, 264, 2649], "temperature": 0.0, "avg_logprob": -0.15511581592990042, "compression_ratio": 1.7588652482269505, "no_speech_prob": 8.397568308282644e-06}, {"id": 488, "seek": 148648, "start": 1509.64, "end": 1512.82, "text": " we find generally between things working and not working.", "tokens": [321, 915, 5101, 1296, 721, 1364, 293, 406, 1364, 13], "temperature": 0.0, "avg_logprob": -0.15511581592990042, "compression_ratio": 1.7588652482269505, "no_speech_prob": 8.397568308282644e-06}, {"id": 489, "seek": 151282, "start": 1512.82, "end": 1516.6, "text": " So the nice thing is now we have something", "tokens": [407, 264, 1481, 551, 307, 586, 321, 362, 746], "temperature": 0.0, "avg_logprob": -0.15101721173241026, "compression_ratio": 1.4829268292682927, "no_speech_prob": 5.25508630744298e-06}, {"id": 490, "seek": 151282, "start": 1516.6, "end": 1521.32, "text": " which you can just add mixed precision and train,", "tokens": [597, 291, 393, 445, 909, 7467, 18356, 293, 3847, 11], "temperature": 0.0, "avg_logprob": -0.15101721173241026, "compression_ratio": 1.4829268292682927, "no_speech_prob": 5.25508630744298e-06}, {"id": 491, "seek": 151282, "start": 1521.32, "end": 1528.72, "text": " and you will get often 2x, 3x speed up,", "tokens": [293, 291, 486, 483, 2049, 568, 87, 11, 805, 87, 3073, 493, 11], "temperature": 0.0, "avg_logprob": -0.15101721173241026, "compression_ratio": 1.4829268292682927, "no_speech_prob": 5.25508630744298e-06}, {"id": 492, "seek": 151282, "start": 1528.72, "end": 1532.1599999999999, "text": " certainly on vision models, also on transformers,", "tokens": [3297, 322, 5201, 5245, 11, 611, 322, 4088, 433, 11], "temperature": 0.0, "avg_logprob": -0.15101721173241026, "compression_ratio": 1.4829268292682927, "no_speech_prob": 5.25508630744298e-06}, {"id": 493, "seek": 151282, "start": 1532.1599999999999, "end": 1534.3799999999999, "text": " quite a few places.", "tokens": [1596, 257, 1326, 3190, 13], "temperature": 0.0, "avg_logprob": -0.15101721173241026, "compression_ratio": 1.4829268292682927, "no_speech_prob": 5.25508630744298e-06}, {"id": 494, "seek": 151282, "start": 1534.3799999999999, "end": 1539.08, "text": " One obvious question is, is 512 the right number?", "tokens": [1485, 6322, 1168, 307, 11, 307, 1025, 4762, 264, 558, 1230, 30], "temperature": 0.0, "avg_logprob": -0.15101721173241026, "compression_ratio": 1.4829268292682927, "no_speech_prob": 5.25508630744298e-06}, {"id": 495, "seek": 151282, "start": 1539.08, "end": 1541.1599999999999, "text": " And it turns out getting this number right actually", "tokens": [400, 309, 4523, 484, 1242, 341, 1230, 558, 767], "temperature": 0.0, "avg_logprob": -0.15101721173241026, "compression_ratio": 1.4829268292682927, "no_speech_prob": 5.25508630744298e-06}, {"id": 496, "seek": 154116, "start": 1541.16, "end": 1543.64, "text": " does make quite a difference to your training.", "tokens": [775, 652, 1596, 257, 2649, 281, 428, 3097, 13], "temperature": 0.0, "avg_logprob": -0.10559040416370738, "compression_ratio": 1.7509293680297398, "no_speech_prob": 3.668741783258156e-06}, {"id": 497, "seek": 154116, "start": 1543.64, "end": 1545.76, "text": " And so something slightly more recently", "tokens": [400, 370, 746, 4748, 544, 3938], "temperature": 0.0, "avg_logprob": -0.10559040416370738, "compression_ratio": 1.7509293680297398, "no_speech_prob": 3.668741783258156e-06}, {"id": 498, "seek": 154116, "start": 1545.76, "end": 1548.3600000000001, "text": " is called dynamic loss scaling, which literally", "tokens": [307, 1219, 8546, 4470, 21589, 11, 597, 3736], "temperature": 0.0, "avg_logprob": -0.10559040416370738, "compression_ratio": 1.7509293680297398, "no_speech_prob": 3.668741783258156e-06}, {"id": 499, "seek": 154116, "start": 1548.3600000000001, "end": 1551.2, "text": " tries a few different values of loss scale", "tokens": [9898, 257, 1326, 819, 4190, 295, 4470, 4373], "temperature": 0.0, "avg_logprob": -0.10559040416370738, "compression_ratio": 1.7509293680297398, "no_speech_prob": 3.668741783258156e-06}, {"id": 500, "seek": 154116, "start": 1551.2, "end": 1554.0, "text": " to find out at what point does it become infinity.", "tokens": [281, 915, 484, 412, 437, 935, 775, 309, 1813, 13202, 13], "temperature": 0.0, "avg_logprob": -0.10559040416370738, "compression_ratio": 1.7509293680297398, "no_speech_prob": 3.668741783258156e-06}, {"id": 501, "seek": 154116, "start": 1554.0, "end": 1556.6000000000001, "text": " And so it dynamically figures out the highest loss", "tokens": [400, 370, 309, 43492, 9624, 484, 264, 6343, 4470], "temperature": 0.0, "avg_logprob": -0.10559040416370738, "compression_ratio": 1.7509293680297398, "no_speech_prob": 3.668741783258156e-06}, {"id": 502, "seek": 154116, "start": 1556.6000000000001, "end": 1559.0800000000002, "text": " scale we can go to.", "tokens": [4373, 321, 393, 352, 281, 13], "temperature": 0.0, "avg_logprob": -0.10559040416370738, "compression_ratio": 1.7509293680297398, "no_speech_prob": 3.668741783258156e-06}, {"id": 503, "seek": 154116, "start": 1559.0800000000002, "end": 1563.2, "text": " And so this version just has the dynamic loss scaling added.", "tokens": [400, 370, 341, 3037, 445, 575, 264, 8546, 4470, 21589, 3869, 13], "temperature": 0.0, "avg_logprob": -0.10559040416370738, "compression_ratio": 1.7509293680297398, "no_speech_prob": 3.668741783258156e-06}, {"id": 504, "seek": 154116, "start": 1563.2, "end": 1567.0, "text": " It's interesting that sometimes training with half precision", "tokens": [467, 311, 1880, 300, 2171, 3097, 365, 1922, 18356], "temperature": 0.0, "avg_logprob": -0.10559040416370738, "compression_ratio": 1.7509293680297398, "no_speech_prob": 3.668741783258156e-06}, {"id": 505, "seek": 154116, "start": 1567.0, "end": 1571.0, "text": " gives you better results than training with FP32,", "tokens": [2709, 291, 1101, 3542, 813, 3097, 365, 36655, 11440, 11], "temperature": 0.0, "avg_logprob": -0.10559040416370738, "compression_ratio": 1.7509293680297398, "no_speech_prob": 3.668741783258156e-06}, {"id": 506, "seek": 157100, "start": 1571.0, "end": 1573.04, "text": " because there's just a bit more randomness.", "tokens": [570, 456, 311, 445, 257, 857, 544, 4974, 1287, 13], "temperature": 0.0, "avg_logprob": -0.15353969076405402, "compression_ratio": 1.5845588235294117, "no_speech_prob": 3.4798991691786796e-05}, {"id": 507, "seek": 157100, "start": 1573.04, "end": 1574.88, "text": " Maybe it regularizes a little bit.", "tokens": [2704, 309, 3890, 5660, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.15353969076405402, "compression_ratio": 1.5845588235294117, "no_speech_prob": 3.4798991691786796e-05}, {"id": 508, "seek": 157100, "start": 1574.88, "end": 1578.32, "text": " But generally, it's super, super similar, just faster.", "tokens": [583, 5101, 11, 309, 311, 1687, 11, 1687, 2531, 11, 445, 4663, 13], "temperature": 0.0, "avg_logprob": -0.15353969076405402, "compression_ratio": 1.5845588235294117, "no_speech_prob": 3.4798991691786796e-05}, {"id": 509, "seek": 157100, "start": 1578.32, "end": 1580.12, "text": " We have a question about mixup.", "tokens": [492, 362, 257, 1168, 466, 2890, 1010, 13], "temperature": 0.0, "avg_logprob": -0.15353969076405402, "compression_ratio": 1.5845588235294117, "no_speech_prob": 3.4798991691786796e-05}, {"id": 510, "seek": 157100, "start": 1580.12, "end": 1581.56, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.15353969076405402, "compression_ratio": 1.5845588235294117, "no_speech_prob": 3.4798991691786796e-05}, {"id": 511, "seek": 157100, "start": 1581.56, "end": 1583.44, "text": " Is there an intuitive way to understand", "tokens": [1119, 456, 364, 21769, 636, 281, 1223], "temperature": 0.0, "avg_logprob": -0.15353969076405402, "compression_ratio": 1.5845588235294117, "no_speech_prob": 3.4798991691786796e-05}, {"id": 512, "seek": 157100, "start": 1583.44, "end": 1586.64, "text": " why mixup is better than other data augmentation techniques?", "tokens": [983, 2890, 1010, 307, 1101, 813, 661, 1412, 14501, 19631, 7512, 30], "temperature": 0.0, "avg_logprob": -0.15353969076405402, "compression_ratio": 1.5845588235294117, "no_speech_prob": 3.4798991691786796e-05}, {"id": 513, "seek": 157100, "start": 1590.68, "end": 1593.24, "text": " I think one of the things that's really nice about mixup", "tokens": [286, 519, 472, 295, 264, 721, 300, 311, 534, 1481, 466, 2890, 1010], "temperature": 0.0, "avg_logprob": -0.15353969076405402, "compression_ratio": 1.5845588235294117, "no_speech_prob": 3.4798991691786796e-05}, {"id": 514, "seek": 157100, "start": 1593.24, "end": 1598.4, "text": " is that it doesn't require any domain-specific thinking.", "tokens": [307, 300, 309, 1177, 380, 3651, 604, 9274, 12, 29258, 1953, 13], "temperature": 0.0, "avg_logprob": -0.15353969076405402, "compression_ratio": 1.5845588235294117, "no_speech_prob": 3.4798991691786796e-05}, {"id": 515, "seek": 157100, "start": 1598.4, "end": 1600.88, "text": " Do we flip horizontally or also vertically?", "tokens": [1144, 321, 7929, 33796, 420, 611, 28450, 30], "temperature": 0.0, "avg_logprob": -0.15353969076405402, "compression_ratio": 1.5845588235294117, "no_speech_prob": 3.4798991691786796e-05}, {"id": 516, "seek": 160088, "start": 1600.88, "end": 1603.48, "text": " How much can we rotate?", "tokens": [1012, 709, 393, 321, 13121, 30], "temperature": 0.0, "avg_logprob": -0.10924481122921674, "compression_ratio": 1.6640625, "no_speech_prob": 6.338514140225016e-06}, {"id": 517, "seek": 160088, "start": 1603.48, "end": 1606.2, "text": " It doesn't create any kind of lossiness, like in the corners.", "tokens": [467, 1177, 380, 1884, 604, 733, 295, 4470, 1324, 11, 411, 294, 264, 12413, 13], "temperature": 0.0, "avg_logprob": -0.10924481122921674, "compression_ratio": 1.6640625, "no_speech_prob": 6.338514140225016e-06}, {"id": 518, "seek": 160088, "start": 1606.2, "end": 1608.64, "text": " There's no reflection padding or black padding.", "tokens": [821, 311, 572, 12914, 39562, 420, 2211, 39562, 13], "temperature": 0.0, "avg_logprob": -0.10924481122921674, "compression_ratio": 1.6640625, "no_speech_prob": 6.338514140225016e-06}, {"id": 519, "seek": 160088, "start": 1608.64, "end": 1612.7600000000002, "text": " So it's kind of like quite nice and clean.", "tokens": [407, 309, 311, 733, 295, 411, 1596, 1481, 293, 2541, 13], "temperature": 0.0, "avg_logprob": -0.10924481122921674, "compression_ratio": 1.6640625, "no_speech_prob": 6.338514140225016e-06}, {"id": 520, "seek": 160088, "start": 1612.7600000000002, "end": 1616.5200000000002, "text": " It's also almost infinite in terms", "tokens": [467, 311, 611, 1920, 13785, 294, 2115], "temperature": 0.0, "avg_logprob": -0.10924481122921674, "compression_ratio": 1.6640625, "no_speech_prob": 6.338514140225016e-06}, {"id": 521, "seek": 160088, "start": 1616.5200000000002, "end": 1619.2800000000002, "text": " of the number of different images it can create.", "tokens": [295, 264, 1230, 295, 819, 5267, 309, 393, 1884, 13], "temperature": 0.0, "avg_logprob": -0.10924481122921674, "compression_ratio": 1.6640625, "no_speech_prob": 6.338514140225016e-06}, {"id": 522, "seek": 160088, "start": 1619.2800000000002, "end": 1622.48, "text": " So you've kind of got this permutation of every image", "tokens": [407, 291, 600, 733, 295, 658, 341, 4784, 11380, 295, 633, 3256], "temperature": 0.0, "avg_logprob": -0.10924481122921674, "compression_ratio": 1.6640625, "no_speech_prob": 6.338514140225016e-06}, {"id": 523, "seek": 160088, "start": 1622.48, "end": 1625.64, "text": " with every other image, which is already giant,", "tokens": [365, 633, 661, 3256, 11, 597, 307, 1217, 7410, 11], "temperature": 0.0, "avg_logprob": -0.10924481122921674, "compression_ratio": 1.6640625, "no_speech_prob": 6.338514140225016e-06}, {"id": 524, "seek": 160088, "start": 1625.64, "end": 1627.24, "text": " and then in different mixes.", "tokens": [293, 550, 294, 819, 37121, 13], "temperature": 0.0, "avg_logprob": -0.10924481122921674, "compression_ratio": 1.6640625, "no_speech_prob": 6.338514140225016e-06}, {"id": 525, "seek": 160088, "start": 1627.24, "end": 1630.16, "text": " So it's just a lot of augmentation", "tokens": [407, 309, 311, 445, 257, 688, 295, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.10924481122921674, "compression_ratio": 1.6640625, "no_speech_prob": 6.338514140225016e-06}, {"id": 526, "seek": 163016, "start": 1630.16, "end": 1633.88, "text": " that you can do with it.", "tokens": [300, 291, 393, 360, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.14986753833386324, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.429443884466309e-05}, {"id": 527, "seek": 163016, "start": 1633.88, "end": 1636.72, "text": " And there are other similar things.", "tokens": [400, 456, 366, 661, 2531, 721, 13], "temperature": 0.0, "avg_logprob": -0.14986753833386324, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.429443884466309e-05}, {"id": 528, "seek": 163016, "start": 1636.72, "end": 1639.6000000000001, "text": " So there's another thing, which there's", "tokens": [407, 456, 311, 1071, 551, 11, 597, 456, 311], "temperature": 0.0, "avg_logprob": -0.14986753833386324, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.429443884466309e-05}, {"id": 529, "seek": 163016, "start": 1639.6000000000001, "end": 1642.3600000000001, "text": " something called cutout, where you just delete a square", "tokens": [746, 1219, 1723, 346, 11, 689, 291, 445, 12097, 257, 3732], "temperature": 0.0, "avg_logprob": -0.14986753833386324, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.429443884466309e-05}, {"id": 530, "seek": 163016, "start": 1642.3600000000001, "end": 1643.64, "text": " and replace it with black.", "tokens": [293, 7406, 309, 365, 2211, 13], "temperature": 0.0, "avg_logprob": -0.14986753833386324, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.429443884466309e-05}, {"id": 531, "seek": 163016, "start": 1643.64, "end": 1645.2, "text": " There's another one where you delete a square", "tokens": [821, 311, 1071, 472, 689, 291, 12097, 257, 3732], "temperature": 0.0, "avg_logprob": -0.14986753833386324, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.429443884466309e-05}, {"id": 532, "seek": 163016, "start": 1645.2, "end": 1647.0400000000002, "text": " and replace it with random pixels.", "tokens": [293, 7406, 309, 365, 4974, 18668, 13], "temperature": 0.0, "avg_logprob": -0.14986753833386324, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.429443884466309e-05}, {"id": 533, "seek": 163016, "start": 1647.0400000000002, "end": 1649.64, "text": " Something I haven't seen, but I'd really like to see people do", "tokens": [6595, 286, 2378, 380, 1612, 11, 457, 286, 1116, 534, 411, 281, 536, 561, 360], "temperature": 0.0, "avg_logprob": -0.14986753833386324, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.429443884466309e-05}, {"id": 534, "seek": 163016, "start": 1649.64, "end": 1651.24, "text": " is to delete a square and replace it", "tokens": [307, 281, 12097, 257, 3732, 293, 7406, 309], "temperature": 0.0, "avg_logprob": -0.14986753833386324, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.429443884466309e-05}, {"id": 535, "seek": 163016, "start": 1651.24, "end": 1652.68, "text": " with a different image.", "tokens": [365, 257, 819, 3256, 13], "temperature": 0.0, "avg_logprob": -0.14986753833386324, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.429443884466309e-05}, {"id": 536, "seek": 163016, "start": 1652.68, "end": 1655.28, "text": " So I'd love somebody to try doing mixup.", "tokens": [407, 286, 1116, 959, 2618, 281, 853, 884, 2890, 1010, 13], "temperature": 0.0, "avg_logprob": -0.14986753833386324, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.429443884466309e-05}, {"id": 537, "seek": 163016, "start": 1655.28, "end": 1658.0800000000002, "text": " But instead of taking the linear combination,", "tokens": [583, 2602, 295, 1940, 264, 8213, 6562, 11], "temperature": 0.0, "avg_logprob": -0.14986753833386324, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.429443884466309e-05}, {"id": 538, "seek": 165808, "start": 1658.08, "end": 1661.76, "text": " instead pick an alpha-sized, sorry,", "tokens": [2602, 1888, 364, 8961, 12, 20614, 11, 2597, 11], "temperature": 0.0, "avg_logprob": -0.15557165499086734, "compression_ratio": 1.5625, "no_speech_prob": 7.888643267506268e-06}, {"id": 539, "seek": 165808, "start": 1661.76, "end": 1664.4399999999998, "text": " a lambda percent of the pixels, like in a square,", "tokens": [257, 13607, 3043, 295, 264, 18668, 11, 411, 294, 257, 3732, 11], "temperature": 0.0, "avg_logprob": -0.15557165499086734, "compression_ratio": 1.5625, "no_speech_prob": 7.888643267506268e-06}, {"id": 540, "seek": 165808, "start": 1664.4399999999998, "end": 1667.24, "text": " and paste them on top.", "tokens": [293, 9163, 552, 322, 1192, 13], "temperature": 0.0, "avg_logprob": -0.15557165499086734, "compression_ratio": 1.5625, "no_speech_prob": 7.888643267506268e-06}, {"id": 541, "seek": 165808, "start": 1667.24, "end": 1670.3999999999999, "text": " There's another one which basically finds", "tokens": [821, 311, 1071, 472, 597, 1936, 10704], "temperature": 0.0, "avg_logprob": -0.15557165499086734, "compression_ratio": 1.5625, "no_speech_prob": 7.888643267506268e-06}, {"id": 542, "seek": 165808, "start": 1670.3999999999999, "end": 1673.1599999999999, "text": " four different images and puts them in four corners.", "tokens": [1451, 819, 5267, 293, 8137, 552, 294, 1451, 12413, 13], "temperature": 0.0, "avg_logprob": -0.15557165499086734, "compression_ratio": 1.5625, "no_speech_prob": 7.888643267506268e-06}, {"id": 543, "seek": 165808, "start": 1673.1599999999999, "end": 1675.12, "text": " So there's a few different variations.", "tokens": [407, 456, 311, 257, 1326, 819, 17840, 13], "temperature": 0.0, "avg_logprob": -0.15557165499086734, "compression_ratio": 1.5625, "no_speech_prob": 7.888643267506268e-06}, {"id": 544, "seek": 165808, "start": 1675.12, "end": 1677.24, "text": " And they really get great results.", "tokens": [400, 436, 534, 483, 869, 3542, 13], "temperature": 0.0, "avg_logprob": -0.15557165499086734, "compression_ratio": 1.5625, "no_speech_prob": 7.888643267506268e-06}, {"id": 545, "seek": 165808, "start": 1677.24, "end": 1682.3999999999999, "text": " And I'm surprised how few people are using them.", "tokens": [400, 286, 478, 6100, 577, 1326, 561, 366, 1228, 552, 13], "temperature": 0.0, "avg_logprob": -0.15557165499086734, "compression_ratio": 1.5625, "no_speech_prob": 7.888643267506268e-06}, {"id": 546, "seek": 165808, "start": 1682.3999999999999, "end": 1684.3999999999999, "text": " So let's put it all together.", "tokens": [407, 718, 311, 829, 309, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.15557165499086734, "compression_ratio": 1.5625, "no_speech_prob": 7.888643267506268e-06}, {"id": 547, "seek": 165808, "start": 1684.3999999999999, "end": 1686.6799999999998, "text": " So here's EmojNet.", "tokens": [407, 510, 311, 462, 3280, 73, 31890, 13], "temperature": 0.0, "avg_logprob": -0.15557165499086734, "compression_ratio": 1.5625, "no_speech_prob": 7.888643267506268e-06}, {"id": 548, "seek": 168668, "start": 1686.68, "end": 1689.64, "text": " So let's use our random resize crop.", "tokens": [407, 718, 311, 764, 527, 4974, 50069, 9086, 13], "temperature": 0.0, "avg_logprob": -0.15239279358475297, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.2606031305040233e-05}, {"id": 549, "seek": 168668, "start": 1689.64, "end": 1694.8, "text": " A minimum scale of 0.35, we find, works pretty well.", "tokens": [316, 7285, 4373, 295, 1958, 13, 8794, 11, 321, 915, 11, 1985, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.15239279358475297, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.2606031305040233e-05}, {"id": 550, "seek": 168668, "start": 1694.8, "end": 1698.2, "text": " And we're not going to do any other other than flip.", "tokens": [400, 321, 434, 406, 516, 281, 360, 604, 661, 661, 813, 7929, 13], "temperature": 0.0, "avg_logprob": -0.15239279358475297, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.2606031305040233e-05}, {"id": 551, "seek": 168668, "start": 1698.2, "end": 1701.16, "text": " We're not going to do any other augmentation.", "tokens": [492, 434, 406, 516, 281, 360, 604, 661, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.15239279358475297, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.2606031305040233e-05}, {"id": 552, "seek": 168668, "start": 1701.16, "end": 1704.96, "text": " And now we need to create a model.", "tokens": [400, 586, 321, 643, 281, 1884, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15239279358475297, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.2606031305040233e-05}, {"id": 553, "seek": 168668, "start": 1704.96, "end": 1707.68, "text": " So far, all of our models have been boring,", "tokens": [407, 1400, 11, 439, 295, 527, 5245, 362, 668, 9989, 11], "temperature": 0.0, "avg_logprob": -0.15239279358475297, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.2606031305040233e-05}, {"id": 554, "seek": 168668, "start": 1707.68, "end": 1710.6000000000001, "text": " convolutional models.", "tokens": [45216, 304, 5245, 13], "temperature": 0.0, "avg_logprob": -0.15239279358475297, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.2606031305040233e-05}, {"id": 555, "seek": 168668, "start": 1710.6000000000001, "end": 1714.44, "text": " But obviously, what we really want to be using is a ResNet", "tokens": [583, 2745, 11, 437, 321, 534, 528, 281, 312, 1228, 307, 257, 5015, 31890], "temperature": 0.0, "avg_logprob": -0.15239279358475297, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.2606031305040233e-05}, {"id": 556, "seek": 168668, "start": 1714.44, "end": 1715.96, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.15239279358475297, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.2606031305040233e-05}, {"id": 557, "seek": 171596, "start": 1715.96, "end": 1718.72, "text": " We have the XResNet, which there's", "tokens": [492, 362, 264, 1783, 33274, 31890, 11, 597, 456, 311], "temperature": 0.0, "avg_logprob": -0.14738175241570722, "compression_ratio": 1.712041884816754, "no_speech_prob": 1.2804676771338563e-05}, {"id": 558, "seek": 171596, "start": 1718.72, "end": 1722.3600000000001, "text": " some debate about whether this is the mutant version of ResNet", "tokens": [512, 7958, 466, 1968, 341, 307, 264, 47198, 3037, 295, 5015, 31890], "temperature": 0.0, "avg_logprob": -0.14738175241570722, "compression_ratio": 1.712041884816754, "no_speech_prob": 1.2804676771338563e-05}, {"id": 559, "seek": 171596, "start": 1722.3600000000001, "end": 1724.24, "text": " or the extended version of ResNet.", "tokens": [420, 264, 10913, 3037, 295, 5015, 31890, 13], "temperature": 0.0, "avg_logprob": -0.14738175241570722, "compression_ratio": 1.712041884816754, "no_speech_prob": 1.2804676771338563e-05}, {"id": 560, "seek": 171596, "start": 1724.24, "end": 1727.72, "text": " So you can choose what the X stands for.", "tokens": [407, 291, 393, 2826, 437, 264, 1783, 7382, 337, 13], "temperature": 0.0, "avg_logprob": -0.14738175241570722, "compression_ratio": 1.712041884816754, "no_speech_prob": 1.2804676771338563e-05}, {"id": 561, "seek": 171596, "start": 1727.72, "end": 1733.72, "text": " And basically, the XResNet is the bag of tricks,", "tokens": [400, 1936, 11, 264, 1783, 33274, 31890, 307, 264, 3411, 295, 11733, 11], "temperature": 0.0, "avg_logprob": -0.14738175241570722, "compression_ratio": 1.712041884816754, "no_speech_prob": 1.2804676771338563e-05}, {"id": 562, "seek": 171596, "start": 1733.72, "end": 1735.0, "text": " wherever it was.", "tokens": [8660, 309, 390, 13], "temperature": 0.0, "avg_logprob": -0.14738175241570722, "compression_ratio": 1.712041884816754, "no_speech_prob": 1.2804676771338563e-05}, {"id": 563, "seek": 171596, "start": 1735.0, "end": 1738.72, "text": " It's basically the bag of tricks ResNet.", "tokens": [467, 311, 1936, 264, 3411, 295, 11733, 5015, 31890, 13], "temperature": 0.0, "avg_logprob": -0.14738175241570722, "compression_ratio": 1.712041884816754, "no_speech_prob": 1.2804676771338563e-05}, {"id": 564, "seek": 173872, "start": 1738.72, "end": 1746.72, "text": " So they have a few suggested tweaks to ResNet.", "tokens": [407, 436, 362, 257, 1326, 10945, 46664, 281, 5015, 31890, 13], "temperature": 0.0, "avg_logprob": -0.12995497028479416, "compression_ratio": 1.54, "no_speech_prob": 1.436742536498059e-06}, {"id": 565, "seek": 173872, "start": 1746.72, "end": 1749.48, "text": " And here they are.", "tokens": [400, 510, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.12995497028479416, "compression_ratio": 1.54, "no_speech_prob": 1.436742536498059e-06}, {"id": 566, "seek": 173872, "start": 1749.48, "end": 1753.16, "text": " So these are their little tweaks.", "tokens": [407, 613, 366, 641, 707, 46664, 13], "temperature": 0.0, "avg_logprob": -0.12995497028479416, "compression_ratio": 1.54, "no_speech_prob": 1.436742536498059e-06}, {"id": 567, "seek": 173872, "start": 1753.16, "end": 1756.04, "text": " So the first tweak is something that we've kind of talked", "tokens": [407, 264, 700, 29879, 307, 746, 300, 321, 600, 733, 295, 2825], "temperature": 0.0, "avg_logprob": -0.12995497028479416, "compression_ratio": 1.54, "no_speech_prob": 1.436742536498059e-06}, {"id": 568, "seek": 173872, "start": 1756.04, "end": 1759.84, "text": " about, and they call it ResNet C. And it's basically, hey,", "tokens": [466, 11, 293, 436, 818, 309, 5015, 31890, 383, 13, 400, 309, 311, 1936, 11, 4177, 11], "temperature": 0.0, "avg_logprob": -0.12995497028479416, "compression_ratio": 1.54, "no_speech_prob": 1.436742536498059e-06}, {"id": 569, "seek": 173872, "start": 1759.84, "end": 1764.48, "text": " let's not do a big 7 by 7 convolution as our first layer,", "tokens": [718, 311, 406, 360, 257, 955, 1614, 538, 1614, 45216, 382, 527, 700, 4583, 11], "temperature": 0.0, "avg_logprob": -0.12995497028479416, "compression_ratio": 1.54, "no_speech_prob": 1.436742536498059e-06}, {"id": 570, "seek": 173872, "start": 1764.48, "end": 1766.84, "text": " because that's super inefficient.", "tokens": [570, 300, 311, 1687, 43495, 13], "temperature": 0.0, "avg_logprob": -0.12995497028479416, "compression_ratio": 1.54, "no_speech_prob": 1.436742536498059e-06}, {"id": 571, "seek": 176684, "start": 1766.84, "end": 1769.6799999999998, "text": " And it's just a single linear model,", "tokens": [400, 309, 311, 445, 257, 2167, 8213, 2316, 11], "temperature": 0.0, "avg_logprob": -0.10306479190957957, "compression_ratio": 1.6244541484716157, "no_speech_prob": 7.646237463632133e-06}, {"id": 572, "seek": 176684, "start": 1769.6799999999998, "end": 1773.1599999999999, "text": " which doesn't have much richness to it.", "tokens": [597, 1177, 380, 362, 709, 44506, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.10306479190957957, "compression_ratio": 1.6244541484716157, "no_speech_prob": 7.646237463632133e-06}, {"id": 573, "seek": 176684, "start": 1773.1599999999999, "end": 1778.8799999999999, "text": " So instead, let's do three comms in a row, 3 by 3.", "tokens": [407, 2602, 11, 718, 311, 360, 1045, 800, 82, 294, 257, 5386, 11, 805, 538, 805, 13], "temperature": 0.0, "avg_logprob": -0.10306479190957957, "compression_ratio": 1.6244541484716157, "no_speech_prob": 7.646237463632133e-06}, {"id": 574, "seek": 176684, "start": 1778.8799999999999, "end": 1781.84, "text": " And so three 3 by 3 comms in a row, if you think about it,", "tokens": [400, 370, 1045, 805, 538, 805, 800, 82, 294, 257, 5386, 11, 498, 291, 519, 466, 309, 11], "temperature": 0.0, "avg_logprob": -0.10306479190957957, "compression_ratio": 1.6244541484716157, "no_speech_prob": 7.646237463632133e-06}, {"id": 575, "seek": 176684, "start": 1781.84, "end": 1785.0, "text": " the receptive field of that final one", "tokens": [264, 45838, 2519, 295, 300, 2572, 472], "temperature": 0.0, "avg_logprob": -0.10306479190957957, "compression_ratio": 1.6244541484716157, "no_speech_prob": 7.646237463632133e-06}, {"id": 576, "seek": 176684, "start": 1785.0, "end": 1788.04, "text": " is still going to be about 7 by 7.", "tokens": [307, 920, 516, 281, 312, 466, 1614, 538, 1614, 13], "temperature": 0.0, "avg_logprob": -0.10306479190957957, "compression_ratio": 1.6244541484716157, "no_speech_prob": 7.646237463632133e-06}, {"id": 577, "seek": 176684, "start": 1788.04, "end": 1791.72, "text": " But it's got there through a much richer set of things", "tokens": [583, 309, 311, 658, 456, 807, 257, 709, 29021, 992, 295, 721], "temperature": 0.0, "avg_logprob": -0.10306479190957957, "compression_ratio": 1.6244541484716157, "no_speech_prob": 7.646237463632133e-06}, {"id": 578, "seek": 176684, "start": 1791.72, "end": 1794.84, "text": " that it can learn, because it's a three-layer neural net.", "tokens": [300, 309, 393, 1466, 11, 570, 309, 311, 257, 1045, 12, 8376, 260, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.10306479190957957, "compression_ratio": 1.6244541484716157, "no_speech_prob": 7.646237463632133e-06}, {"id": 579, "seek": 179484, "start": 1794.84, "end": 1799.52, "text": " So that's the first thing that we do in our XResNet.", "tokens": [407, 300, 311, 264, 700, 551, 300, 321, 360, 294, 527, 1783, 33274, 31890, 13], "temperature": 0.0, "avg_logprob": -0.13478185390603953, "compression_ratio": 1.7478632478632479, "no_speech_prob": 2.64256095761084e-06}, {"id": 580, "seek": 179484, "start": 1799.52, "end": 1803.76, "text": " So here is XResNet.", "tokens": [407, 510, 307, 1783, 33274, 31890, 13], "temperature": 0.0, "avg_logprob": -0.13478185390603953, "compression_ratio": 1.7478632478632479, "no_speech_prob": 2.64256095761084e-06}, {"id": 581, "seek": 179484, "start": 1803.76, "end": 1807.76, "text": " And when we create it, we set up how many filters", "tokens": [400, 562, 321, 1884, 309, 11, 321, 992, 493, 577, 867, 15995], "temperature": 0.0, "avg_logprob": -0.13478185390603953, "compression_ratio": 1.7478632478632479, "no_speech_prob": 2.64256095761084e-06}, {"id": 582, "seek": 179484, "start": 1807.76, "end": 1810.6399999999999, "text": " are there going to be for each of the first three layers.", "tokens": [366, 456, 516, 281, 312, 337, 1184, 295, 264, 700, 1045, 7914, 13], "temperature": 0.0, "avg_logprob": -0.13478185390603953, "compression_ratio": 1.7478632478632479, "no_speech_prob": 2.64256095761084e-06}, {"id": 583, "seek": 179484, "start": 1810.6399999999999, "end": 1813.9199999999998, "text": " So the first three layers will start with channels in, inputs.", "tokens": [407, 264, 700, 1045, 7914, 486, 722, 365, 9235, 294, 11, 15743, 13], "temperature": 0.0, "avg_logprob": -0.13478185390603953, "compression_ratio": 1.7478632478632479, "no_speech_prob": 2.64256095761084e-06}, {"id": 584, "seek": 179484, "start": 1813.9199999999998, "end": 1815.8799999999999, "text": " So that'll default to three, because normally we", "tokens": [407, 300, 603, 7576, 281, 1045, 11, 570, 5646, 321], "temperature": 0.0, "avg_logprob": -0.13478185390603953, "compression_ratio": 1.7478632478632479, "no_speech_prob": 2.64256095761084e-06}, {"id": 585, "seek": 179484, "start": 1815.8799999999999, "end": 1818.12, "text": " have three channel images.", "tokens": [362, 1045, 2269, 5267, 13], "temperature": 0.0, "avg_logprob": -0.13478185390603953, "compression_ratio": 1.7478632478632479, "no_speech_prob": 2.64256095761084e-06}, {"id": 586, "seek": 179484, "start": 1818.12, "end": 1820.04, "text": " And the number of outputs that we'll", "tokens": [400, 264, 1230, 295, 23930, 300, 321, 603], "temperature": 0.0, "avg_logprob": -0.13478185390603953, "compression_ratio": 1.7478632478632479, "no_speech_prob": 2.64256095761084e-06}, {"id": 587, "seek": 179484, "start": 1820.04, "end": 1824.36, "text": " use for the first layer will be that plus 1 times 8.", "tokens": [764, 337, 264, 700, 4583, 486, 312, 300, 1804, 502, 1413, 1649, 13], "temperature": 0.0, "avg_logprob": -0.13478185390603953, "compression_ratio": 1.7478632478632479, "no_speech_prob": 2.64256095761084e-06}, {"id": 588, "seek": 182436, "start": 1824.36, "end": 1825.56, "text": " Why is that?", "tokens": [1545, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.10277734632077425, "compression_ratio": 1.551219512195122, "no_speech_prob": 3.187512447766494e-06}, {"id": 589, "seek": 182436, "start": 1825.56, "end": 1827.6, "text": " It's a bit of a long story.", "tokens": [467, 311, 257, 857, 295, 257, 938, 1657, 13], "temperature": 0.0, "avg_logprob": -0.10277734632077425, "compression_ratio": 1.551219512195122, "no_speech_prob": 3.187512447766494e-06}, {"id": 590, "seek": 182436, "start": 1827.6, "end": 1833.84, "text": " One reason is that that gives you 32 at the second layer, which", "tokens": [1485, 1778, 307, 300, 300, 2709, 291, 8858, 412, 264, 1150, 4583, 11, 597], "temperature": 0.0, "avg_logprob": -0.10277734632077425, "compression_ratio": 1.551219512195122, "no_speech_prob": 3.187512447766494e-06}, {"id": 591, "seek": 182436, "start": 1833.84, "end": 1841.3999999999999, "text": " is the same as what the bag of tricks paper recommends,", "tokens": [307, 264, 912, 382, 437, 264, 3411, 295, 11733, 3035, 34556, 11], "temperature": 0.0, "avg_logprob": -0.10277734632077425, "compression_ratio": 1.551219512195122, "no_speech_prob": 3.187512447766494e-06}, {"id": 592, "seek": 182436, "start": 1841.3999999999999, "end": 1842.84, "text": " as you can see.", "tokens": [382, 291, 393, 536, 13], "temperature": 0.0, "avg_logprob": -0.10277734632077425, "compression_ratio": 1.551219512195122, "no_speech_prob": 3.187512447766494e-06}, {"id": 593, "seek": 182436, "start": 1842.84, "end": 1847.6799999999998, "text": " The second reason is that I've kind of played around", "tokens": [440, 1150, 1778, 307, 300, 286, 600, 733, 295, 3737, 926], "temperature": 0.0, "avg_logprob": -0.10277734632077425, "compression_ratio": 1.551219512195122, "no_speech_prob": 3.187512447766494e-06}, {"id": 594, "seek": 182436, "start": 1847.6799999999998, "end": 1850.32, "text": " with this quite a lot to try to figure out", "tokens": [365, 341, 1596, 257, 688, 281, 853, 281, 2573, 484], "temperature": 0.0, "avg_logprob": -0.10277734632077425, "compression_ratio": 1.551219512195122, "no_speech_prob": 3.187512447766494e-06}, {"id": 595, "seek": 182436, "start": 1850.32, "end": 1852.24, "text": " what makes sense in terms of receptive field.", "tokens": [437, 1669, 2020, 294, 2115, 295, 45838, 2519, 13], "temperature": 0.0, "avg_logprob": -0.10277734632077425, "compression_ratio": 1.551219512195122, "no_speech_prob": 3.187512447766494e-06}, {"id": 596, "seek": 185224, "start": 1852.24, "end": 1855.84, "text": " And I think this gives you the right amount.", "tokens": [400, 286, 519, 341, 2709, 291, 264, 558, 2372, 13], "temperature": 0.0, "avg_logprob": -0.12879236470098082, "compression_ratio": 1.5871212121212122, "no_speech_prob": 5.593864443653729e-06}, {"id": 597, "seek": 185224, "start": 1855.84, "end": 1863.1200000000001, "text": " The times 8 is here because NVIDIA graphics cards", "tokens": [440, 1413, 1649, 307, 510, 570, 426, 3958, 6914, 11837, 5632], "temperature": 0.0, "avg_logprob": -0.12879236470098082, "compression_ratio": 1.5871212121212122, "no_speech_prob": 5.593864443653729e-06}, {"id": 598, "seek": 185224, "start": 1863.1200000000001, "end": 1865.04, "text": " like everything to be a multiple of 8.", "tokens": [411, 1203, 281, 312, 257, 3866, 295, 1649, 13], "temperature": 0.0, "avg_logprob": -0.12879236470098082, "compression_ratio": 1.5871212121212122, "no_speech_prob": 5.593864443653729e-06}, {"id": 599, "seek": 185224, "start": 1865.04, "end": 1868.2, "text": " So if this is not 8, it's probably going to be slower.", "tokens": [407, 498, 341, 307, 406, 1649, 11, 309, 311, 1391, 516, 281, 312, 14009, 13], "temperature": 0.0, "avg_logprob": -0.12879236470098082, "compression_ratio": 1.5871212121212122, "no_speech_prob": 5.593864443653729e-06}, {"id": 600, "seek": 185224, "start": 1868.2, "end": 1871.32, "text": " But one of the things here is now, if you have a one channel", "tokens": [583, 472, 295, 264, 721, 510, 307, 586, 11, 498, 291, 362, 257, 472, 2269], "temperature": 0.0, "avg_logprob": -0.12879236470098082, "compression_ratio": 1.5871212121212122, "no_speech_prob": 5.593864443653729e-06}, {"id": 601, "seek": 185224, "start": 1871.32, "end": 1874.16, "text": " input, like black and white, or a five channel input,", "tokens": [4846, 11, 411, 2211, 293, 2418, 11, 420, 257, 1732, 2269, 4846, 11], "temperature": 0.0, "avg_logprob": -0.12879236470098082, "compression_ratio": 1.5871212121212122, "no_speech_prob": 5.593864443653729e-06}, {"id": 602, "seek": 185224, "start": 1874.16, "end": 1877.08, "text": " like some kind of hyperspectral imaging or microscopy,", "tokens": [411, 512, 733, 295, 7420, 433, 1043, 2155, 25036, 420, 30483, 88, 11], "temperature": 0.0, "avg_logprob": -0.12879236470098082, "compression_ratio": 1.5871212121212122, "no_speech_prob": 5.593864443653729e-06}, {"id": 603, "seek": 185224, "start": 1877.08, "end": 1880.88, "text": " then you're actually changing your model dynamically to say,", "tokens": [550, 291, 434, 767, 4473, 428, 2316, 43492, 281, 584, 11], "temperature": 0.0, "avg_logprob": -0.12879236470098082, "compression_ratio": 1.5871212121212122, "no_speech_prob": 5.593864443653729e-06}, {"id": 604, "seek": 188088, "start": 1880.88, "end": 1883.5600000000002, "text": " oh, if I've got more inputs, then my first layer", "tokens": [1954, 11, 498, 286, 600, 658, 544, 15743, 11, 550, 452, 700, 4583], "temperature": 0.0, "avg_logprob": -0.13625431060791016, "compression_ratio": 1.6220472440944882, "no_speech_prob": 6.143780865386361e-06}, {"id": 605, "seek": 188088, "start": 1883.5600000000002, "end": 1885.0400000000002, "text": " should have more activations.", "tokens": [820, 362, 544, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.13625431060791016, "compression_ratio": 1.6220472440944882, "no_speech_prob": 6.143780865386361e-06}, {"id": 606, "seek": 188088, "start": 1885.0400000000002, "end": 1887.44, "text": " Which is not something I've seen anybody do before.", "tokens": [3013, 307, 406, 746, 286, 600, 1612, 4472, 360, 949, 13], "temperature": 0.0, "avg_logprob": -0.13625431060791016, "compression_ratio": 1.6220472440944882, "no_speech_prob": 6.143780865386361e-06}, {"id": 607, "seek": 188088, "start": 1887.44, "end": 1889.64, "text": " But it's a kind of really simple, nice way", "tokens": [583, 309, 311, 257, 733, 295, 534, 2199, 11, 1481, 636], "temperature": 0.0, "avg_logprob": -0.13625431060791016, "compression_ratio": 1.6220472440944882, "no_speech_prob": 6.143780865386361e-06}, {"id": 608, "seek": 188088, "start": 1889.64, "end": 1893.92, "text": " to improve your ResNet for different kinds of domains.", "tokens": [281, 3470, 428, 5015, 31890, 337, 819, 3685, 295, 25514, 13], "temperature": 0.0, "avg_logprob": -0.13625431060791016, "compression_ratio": 1.6220472440944882, "no_speech_prob": 6.143780865386361e-06}, {"id": 609, "seek": 188088, "start": 1893.92, "end": 1897.2800000000002, "text": " So that's the number of filters we have for each layer.", "tokens": [407, 300, 311, 264, 1230, 295, 15995, 321, 362, 337, 1184, 4583, 13], "temperature": 0.0, "avg_logprob": -0.13625431060791016, "compression_ratio": 1.6220472440944882, "no_speech_prob": 6.143780865386361e-06}, {"id": 610, "seek": 188088, "start": 1897.2800000000002, "end": 1901.5600000000002, "text": " So our stem, so the stem is the very start of a CNN.", "tokens": [407, 527, 12312, 11, 370, 264, 12312, 307, 264, 588, 722, 295, 257, 24859, 13], "temperature": 0.0, "avg_logprob": -0.13625431060791016, "compression_ratio": 1.6220472440944882, "no_speech_prob": 6.143780865386361e-06}, {"id": 611, "seek": 188088, "start": 1901.5600000000002, "end": 1907.3200000000002, "text": " So our stem is just those three conv layers.", "tokens": [407, 527, 12312, 307, 445, 729, 1045, 3754, 7914, 13], "temperature": 0.0, "avg_logprob": -0.13625431060791016, "compression_ratio": 1.6220472440944882, "no_speech_prob": 6.143780865386361e-06}, {"id": 612, "seek": 188088, "start": 1907.3200000000002, "end": 1908.92, "text": " So that's all the paper says.", "tokens": [407, 300, 311, 439, 264, 3035, 1619, 13], "temperature": 0.0, "avg_logprob": -0.13625431060791016, "compression_ratio": 1.6220472440944882, "no_speech_prob": 6.143780865386361e-06}, {"id": 613, "seek": 190892, "start": 1908.92, "end": 1910.88, "text": " What's a conv layer?", "tokens": [708, 311, 257, 3754, 4583, 30], "temperature": 0.0, "avg_logprob": -0.1688990478515625, "compression_ratio": 1.7207547169811321, "no_speech_prob": 6.96179949954967e-06}, {"id": 614, "seek": 190892, "start": 1910.88, "end": 1914.64, "text": " A conv layer is a sequential containing", "tokens": [316, 3754, 4583, 307, 257, 42881, 19273], "temperature": 0.0, "avg_logprob": -0.1688990478515625, "compression_ratio": 1.7207547169811321, "no_speech_prob": 6.96179949954967e-06}, {"id": 615, "seek": 190892, "start": 1914.64, "end": 1919.52, "text": " a bunch of layers, which starts with a conv of some stride,", "tokens": [257, 3840, 295, 7914, 11, 597, 3719, 365, 257, 3754, 295, 512, 1056, 482, 11], "temperature": 0.0, "avg_logprob": -0.1688990478515625, "compression_ratio": 1.7207547169811321, "no_speech_prob": 6.96179949954967e-06}, {"id": 616, "seek": 190892, "start": 1919.52, "end": 1923.04, "text": " followed by a batch norm, and then optionally,", "tokens": [6263, 538, 257, 15245, 2026, 11, 293, 550, 3614, 379, 11], "temperature": 0.0, "avg_logprob": -0.1688990478515625, "compression_ratio": 1.7207547169811321, "no_speech_prob": 6.96179949954967e-06}, {"id": 617, "seek": 190892, "start": 1923.04, "end": 1925.52, "text": " followed by an activation function.", "tokens": [6263, 538, 364, 24433, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1688990478515625, "compression_ratio": 1.7207547169811321, "no_speech_prob": 6.96179949954967e-06}, {"id": 618, "seek": 190892, "start": 1925.52, "end": 1926.98, "text": " And our activation function, we're", "tokens": [400, 527, 24433, 2445, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.1688990478515625, "compression_ratio": 1.7207547169811321, "no_speech_prob": 6.96179949954967e-06}, {"id": 619, "seek": 190892, "start": 1926.98, "end": 1928.3600000000001, "text": " just going to use ReLU for now because that's", "tokens": [445, 516, 281, 764, 1300, 43, 52, 337, 586, 570, 300, 311], "temperature": 0.0, "avg_logprob": -0.1688990478515625, "compression_ratio": 1.7207547169811321, "no_speech_prob": 6.96179949954967e-06}, {"id": 620, "seek": 190892, "start": 1928.3600000000001, "end": 1931.2, "text": " what they use in the paper.", "tokens": [437, 436, 764, 294, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.1688990478515625, "compression_ratio": 1.7207547169811321, "no_speech_prob": 6.96179949954967e-06}, {"id": 621, "seek": 190892, "start": 1931.2, "end": 1933.92, "text": " The batch norm, we do something interesting.", "tokens": [440, 15245, 2026, 11, 321, 360, 746, 1880, 13], "temperature": 0.0, "avg_logprob": -0.1688990478515625, "compression_ratio": 1.7207547169811321, "no_speech_prob": 6.96179949954967e-06}, {"id": 622, "seek": 190892, "start": 1933.92, "end": 1935.92, "text": " This is another tweak from the bag of tricks,", "tokens": [639, 307, 1071, 29879, 490, 264, 3411, 295, 11733, 11], "temperature": 0.0, "avg_logprob": -0.1688990478515625, "compression_ratio": 1.7207547169811321, "no_speech_prob": 6.96179949954967e-06}, {"id": 623, "seek": 190892, "start": 1935.92, "end": 1938.3600000000001, "text": " although it goes back a couple more years than that.", "tokens": [4878, 309, 1709, 646, 257, 1916, 544, 924, 813, 300, 13], "temperature": 0.0, "avg_logprob": -0.1688990478515625, "compression_ratio": 1.7207547169811321, "no_speech_prob": 6.96179949954967e-06}, {"id": 624, "seek": 193836, "start": 1938.36, "end": 1945.6399999999999, "text": " We initialize the batch norm sometimes to have weights of 1", "tokens": [492, 5883, 1125, 264, 15245, 2026, 2171, 281, 362, 17443, 295, 502], "temperature": 0.0, "avg_logprob": -0.16673057192847843, "compression_ratio": 1.628440366972477, "no_speech_prob": 3.966797976318048e-06}, {"id": 625, "seek": 193836, "start": 1945.6399999999999, "end": 1950.28, "text": " and sometimes to have weights of 0.", "tokens": [293, 2171, 281, 362, 17443, 295, 1958, 13], "temperature": 0.0, "avg_logprob": -0.16673057192847843, "compression_ratio": 1.628440366972477, "no_speech_prob": 3.966797976318048e-06}, {"id": 626, "seek": 193836, "start": 1950.28, "end": 1951.8799999999999, "text": " Why do we do that?", "tokens": [1545, 360, 321, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.16673057192847843, "compression_ratio": 1.628440366972477, "no_speech_prob": 3.966797976318048e-06}, {"id": 627, "seek": 193836, "start": 1951.8799999999999, "end": 1954.84, "text": " Well, all right.", "tokens": [1042, 11, 439, 558, 13], "temperature": 0.0, "avg_logprob": -0.16673057192847843, "compression_ratio": 1.628440366972477, "no_speech_prob": 3.966797976318048e-06}, {"id": 628, "seek": 193836, "start": 1954.84, "end": 1961.36, "text": " Have a look here at ResNet D. This is a standard ResNet block.", "tokens": [3560, 257, 574, 510, 412, 5015, 31890, 413, 13, 639, 307, 257, 3832, 5015, 31890, 3461, 13], "temperature": 0.0, "avg_logprob": -0.16673057192847843, "compression_ratio": 1.628440366972477, "no_speech_prob": 3.966797976318048e-06}, {"id": 629, "seek": 193836, "start": 1961.36, "end": 1963.56, "text": " This path here, normally, it doesn't", "tokens": [639, 3100, 510, 11, 5646, 11, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.16673057192847843, "compression_ratio": 1.628440366972477, "no_speech_prob": 3.966797976318048e-06}, {"id": 630, "seek": 193836, "start": 1963.56, "end": 1965.06, "text": " have the conv and the average pool.", "tokens": [362, 264, 3754, 293, 264, 4274, 7005, 13], "temperature": 0.0, "avg_logprob": -0.16673057192847843, "compression_ratio": 1.628440366972477, "no_speech_prob": 3.966797976318048e-06}, {"id": 631, "seek": 193836, "start": 1965.06, "end": 1966.0, "text": " So pretend they're not there.", "tokens": [407, 11865, 436, 434, 406, 456, 13], "temperature": 0.0, "avg_logprob": -0.16673057192847843, "compression_ratio": 1.628440366972477, "no_speech_prob": 3.966797976318048e-06}, {"id": 632, "seek": 193836, "start": 1966.0, "end": 1968.02, "text": " We'll talk about why they're there sometimes in a moment.", "tokens": [492, 603, 751, 466, 983, 436, 434, 456, 2171, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.16673057192847843, "compression_ratio": 1.628440366972477, "no_speech_prob": 3.966797976318048e-06}, {"id": 633, "seek": 196802, "start": 1968.02, "end": 1970.24, "text": " But this is just the identity.", "tokens": [583, 341, 307, 445, 264, 6575, 13], "temperature": 0.0, "avg_logprob": -0.13966160728817895, "compression_ratio": 1.8120300751879699, "no_speech_prob": 6.143890914245276e-06}, {"id": 634, "seek": 196802, "start": 1970.24, "end": 1975.84, "text": " And the other goes 1 by 1 conv, 3 by 3 conv, 1 by 1 conv.", "tokens": [400, 264, 661, 1709, 502, 538, 502, 3754, 11, 805, 538, 805, 3754, 11, 502, 538, 502, 3754, 13], "temperature": 0.0, "avg_logprob": -0.13966160728817895, "compression_ratio": 1.8120300751879699, "no_speech_prob": 6.143890914245276e-06}, {"id": 635, "seek": 196802, "start": 1975.84, "end": 1978.0, "text": " And remember, in each case, it's conv batch norm ReLU,", "tokens": [400, 1604, 11, 294, 1184, 1389, 11, 309, 311, 3754, 15245, 2026, 1300, 43, 52, 11], "temperature": 0.0, "avg_logprob": -0.13966160728817895, "compression_ratio": 1.8120300751879699, "no_speech_prob": 6.143890914245276e-06}, {"id": 636, "seek": 196802, "start": 1978.0, "end": 1979.62, "text": " conv batch norm ReLU.", "tokens": [3754, 15245, 2026, 1300, 43, 52, 13], "temperature": 0.0, "avg_logprob": -0.13966160728817895, "compression_ratio": 1.8120300751879699, "no_speech_prob": 6.143890914245276e-06}, {"id": 637, "seek": 196802, "start": 1979.62, "end": 1982.66, "text": " And then what actually happens is it then goes conv batch norm,", "tokens": [400, 550, 437, 767, 2314, 307, 309, 550, 1709, 3754, 15245, 2026, 11], "temperature": 0.0, "avg_logprob": -0.13966160728817895, "compression_ratio": 1.8120300751879699, "no_speech_prob": 6.143890914245276e-06}, {"id": 638, "seek": 196802, "start": 1982.66, "end": 1985.52, "text": " and then the ReLU happens after the plus.", "tokens": [293, 550, 264, 1300, 43, 52, 2314, 934, 264, 1804, 13], "temperature": 0.0, "avg_logprob": -0.13966160728817895, "compression_ratio": 1.8120300751879699, "no_speech_prob": 6.143890914245276e-06}, {"id": 639, "seek": 196802, "start": 1985.52, "end": 1987.32, "text": " There's another variant where the ReLU", "tokens": [821, 311, 1071, 17501, 689, 264, 1300, 43, 52], "temperature": 0.0, "avg_logprob": -0.13966160728817895, "compression_ratio": 1.8120300751879699, "no_speech_prob": 6.143890914245276e-06}, {"id": 640, "seek": 196802, "start": 1987.32, "end": 1989.2, "text": " happens before the plus, which is called", "tokens": [2314, 949, 264, 1804, 11, 597, 307, 1219], "temperature": 0.0, "avg_logprob": -0.13966160728817895, "compression_ratio": 1.8120300751879699, "no_speech_prob": 6.143890914245276e-06}, {"id": 641, "seek": 196802, "start": 1989.2, "end": 1991.76, "text": " preact or preactivation ResNet.", "tokens": [659, 578, 420, 659, 23397, 399, 5015, 31890, 13], "temperature": 0.0, "avg_logprob": -0.13966160728817895, "compression_ratio": 1.8120300751879699, "no_speech_prob": 6.143890914245276e-06}, {"id": 642, "seek": 196802, "start": 1991.76, "end": 1995.52, "text": " Turns out it doesn't work quite as well for smaller models.", "tokens": [29524, 484, 309, 1177, 380, 589, 1596, 382, 731, 337, 4356, 5245, 13], "temperature": 0.0, "avg_logprob": -0.13966160728817895, "compression_ratio": 1.8120300751879699, "no_speech_prob": 6.143890914245276e-06}, {"id": 643, "seek": 196802, "start": 1995.52, "end": 1997.44, "text": " So we're using the non-preact version.", "tokens": [407, 321, 434, 1228, 264, 2107, 12, 3712, 578, 3037, 13], "temperature": 0.0, "avg_logprob": -0.13966160728817895, "compression_ratio": 1.8120300751879699, "no_speech_prob": 6.143890914245276e-06}, {"id": 644, "seek": 199744, "start": 1997.44, "end": 2000.76, "text": " Now, see this conv here?", "tokens": [823, 11, 536, 341, 3754, 510, 30], "temperature": 0.0, "avg_logprob": -0.1439958323603091, "compression_ratio": 1.6434782608695653, "no_speech_prob": 4.495089342526626e-06}, {"id": 645, "seek": 199744, "start": 2000.76, "end": 2005.2, "text": " What if we set the batch norm layer weights there to 0?", "tokens": [708, 498, 321, 992, 264, 15245, 2026, 4583, 17443, 456, 281, 1958, 30], "temperature": 0.0, "avg_logprob": -0.1439958323603091, "compression_ratio": 1.6434782608695653, "no_speech_prob": 4.495089342526626e-06}, {"id": 646, "seek": 199744, "start": 2005.2, "end": 2006.52, "text": " What's going to happen?", "tokens": [708, 311, 516, 281, 1051, 30], "temperature": 0.0, "avg_logprob": -0.1439958323603091, "compression_ratio": 1.6434782608695653, "no_speech_prob": 4.495089342526626e-06}, {"id": 647, "seek": 199744, "start": 2006.52, "end": 2007.64, "text": " Well, we've got an input.", "tokens": [1042, 11, 321, 600, 658, 364, 4846, 13], "temperature": 0.0, "avg_logprob": -0.1439958323603091, "compression_ratio": 1.6434782608695653, "no_speech_prob": 4.495089342526626e-06}, {"id": 648, "seek": 199744, "start": 2007.64, "end": 2009.0, "text": " This is identity.", "tokens": [639, 307, 6575, 13], "temperature": 0.0, "avg_logprob": -0.1439958323603091, "compression_ratio": 1.6434782608695653, "no_speech_prob": 4.495089342526626e-06}, {"id": 649, "seek": 199744, "start": 2009.0, "end": 2012.2, "text": " This does some conv, some conv, some conv,", "tokens": [639, 775, 512, 3754, 11, 512, 3754, 11, 512, 3754, 11], "temperature": 0.0, "avg_logprob": -0.1439958323603091, "compression_ratio": 1.6434782608695653, "no_speech_prob": 4.495089342526626e-06}, {"id": 650, "seek": 199744, "start": 2012.2, "end": 2013.96, "text": " and then batch norm where the weights are 0.", "tokens": [293, 550, 15245, 2026, 689, 264, 17443, 366, 1958, 13], "temperature": 0.0, "avg_logprob": -0.1439958323603091, "compression_ratio": 1.6434782608695653, "no_speech_prob": 4.495089342526626e-06}, {"id": 651, "seek": 199744, "start": 2013.96, "end": 2015.76, "text": " So everything gets multiplied by 0.", "tokens": [407, 1203, 2170, 17207, 538, 1958, 13], "temperature": 0.0, "avg_logprob": -0.1439958323603091, "compression_ratio": 1.6434782608695653, "no_speech_prob": 4.495089342526626e-06}, {"id": 652, "seek": 199744, "start": 2015.76, "end": 2019.28, "text": " And so out of here comes 0.", "tokens": [400, 370, 484, 295, 510, 1487, 1958, 13], "temperature": 0.0, "avg_logprob": -0.1439958323603091, "compression_ratio": 1.6434782608695653, "no_speech_prob": 4.495089342526626e-06}, {"id": 653, "seek": 199744, "start": 2019.28, "end": 2020.44, "text": " So why is that interesting?", "tokens": [407, 983, 307, 300, 1880, 30], "temperature": 0.0, "avg_logprob": -0.1439958323603091, "compression_ratio": 1.6434782608695653, "no_speech_prob": 4.495089342526626e-06}, {"id": 654, "seek": 199744, "start": 2020.44, "end": 2023.76, "text": " Because now we're adding 0 to the identity block.", "tokens": [1436, 586, 321, 434, 5127, 1958, 281, 264, 6575, 3461, 13], "temperature": 0.0, "avg_logprob": -0.1439958323603091, "compression_ratio": 1.6434782608695653, "no_speech_prob": 4.495089342526626e-06}, {"id": 655, "seek": 202376, "start": 2023.76, "end": 2027.76, "text": " So in other words, the whole block does nothing at all.", "tokens": [407, 294, 661, 2283, 11, 264, 1379, 3461, 775, 1825, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.1081242710351944, "compression_ratio": 1.6123188405797102, "no_speech_prob": 4.965261837241997e-07}, {"id": 656, "seek": 202376, "start": 2027.76, "end": 2030.2, "text": " That's a great way to initialize a model, right?", "tokens": [663, 311, 257, 869, 636, 281, 5883, 1125, 257, 2316, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1081242710351944, "compression_ratio": 1.6123188405797102, "no_speech_prob": 4.965261837241997e-07}, {"id": 657, "seek": 202376, "start": 2030.2, "end": 2032.2, "text": " Because we really don't want to be in a position,", "tokens": [1436, 321, 534, 500, 380, 528, 281, 312, 294, 257, 2535, 11], "temperature": 0.0, "avg_logprob": -0.1081242710351944, "compression_ratio": 1.6123188405797102, "no_speech_prob": 4.965261837241997e-07}, {"id": 658, "seek": 202376, "start": 2032.2, "end": 2035.84, "text": " as we've seen, where if you've got 1,000 layers deep model,", "tokens": [382, 321, 600, 1612, 11, 689, 498, 291, 600, 658, 502, 11, 1360, 7914, 2452, 2316, 11], "temperature": 0.0, "avg_logprob": -0.1081242710351944, "compression_ratio": 1.6123188405797102, "no_speech_prob": 4.965261837241997e-07}, {"id": 659, "seek": 202376, "start": 2035.84, "end": 2038.84, "text": " that any layer is even slightly changing the variance,", "tokens": [300, 604, 4583, 307, 754, 4748, 4473, 264, 21977, 11], "temperature": 0.0, "avg_logprob": -0.1081242710351944, "compression_ratio": 1.6123188405797102, "no_speech_prob": 4.965261837241997e-07}, {"id": 660, "seek": 202376, "start": 2038.84, "end": 2041.48, "text": " because they kind of cause the gradients to spiral off", "tokens": [570, 436, 733, 295, 3082, 264, 2771, 2448, 281, 25165, 766], "temperature": 0.0, "avg_logprob": -0.1081242710351944, "compression_ratio": 1.6123188405797102, "no_speech_prob": 4.965261837241997e-07}, {"id": 661, "seek": 202376, "start": 2041.48, "end": 2043.24, "text": " to 0 or to infinity.", "tokens": [281, 1958, 420, 281, 13202, 13], "temperature": 0.0, "avg_logprob": -0.1081242710351944, "compression_ratio": 1.6123188405797102, "no_speech_prob": 4.965261837241997e-07}, {"id": 662, "seek": 202376, "start": 2043.24, "end": 2046.76, "text": " This way, literally, the entire activations", "tokens": [639, 636, 11, 3736, 11, 264, 2302, 2430, 763], "temperature": 0.0, "avg_logprob": -0.1081242710351944, "compression_ratio": 1.6123188405797102, "no_speech_prob": 4.965261837241997e-07}, {"id": 663, "seek": 202376, "start": 2046.76, "end": 2049.2, "text": " are the same all the way through.", "tokens": [366, 264, 912, 439, 264, 636, 807, 13], "temperature": 0.0, "avg_logprob": -0.1081242710351944, "compression_ratio": 1.6123188405797102, "no_speech_prob": 4.965261837241997e-07}, {"id": 664, "seek": 202376, "start": 2049.2, "end": 2050.6, "text": " So that's what we do.", "tokens": [407, 300, 311, 437, 321, 360, 13], "temperature": 0.0, "avg_logprob": -0.1081242710351944, "compression_ratio": 1.6123188405797102, "no_speech_prob": 4.965261837241997e-07}, {"id": 665, "seek": 205060, "start": 2050.6, "end": 2057.56, "text": " We set the 1, 2, 3 third conv layer", "tokens": [492, 992, 264, 502, 11, 568, 11, 805, 2636, 3754, 4583], "temperature": 0.0, "avg_logprob": -0.13352932929992675, "compression_ratio": 1.528, "no_speech_prob": 2.156747541448567e-06}, {"id": 666, "seek": 205060, "start": 2057.56, "end": 2061.68, "text": " to have 0 in that batch norm layer.", "tokens": [281, 362, 1958, 294, 300, 15245, 2026, 4583, 13], "temperature": 0.0, "avg_logprob": -0.13352932929992675, "compression_ratio": 1.528, "no_speech_prob": 2.156747541448567e-06}, {"id": 667, "seek": 205060, "start": 2061.68, "end": 2064.4, "text": " And this lets us train very deep models", "tokens": [400, 341, 6653, 505, 3847, 588, 2452, 5245], "temperature": 0.0, "avg_logprob": -0.13352932929992675, "compression_ratio": 1.528, "no_speech_prob": 2.156747541448567e-06}, {"id": 668, "seek": 205060, "start": 2064.4, "end": 2065.92, "text": " at very high learning rates.", "tokens": [412, 588, 1090, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.13352932929992675, "compression_ratio": 1.528, "no_speech_prob": 2.156747541448567e-06}, {"id": 669, "seek": 205060, "start": 2065.92, "end": 2067.92, "text": " You'll see nearly all of the academic literature", "tokens": [509, 603, 536, 6217, 439, 295, 264, 7778, 10394], "temperature": 0.0, "avg_logprob": -0.13352932929992675, "compression_ratio": 1.528, "no_speech_prob": 2.156747541448567e-06}, {"id": 670, "seek": 205060, "start": 2067.92, "end": 2070.56, "text": " about this talks about large batch sizes,", "tokens": [466, 341, 6686, 466, 2416, 15245, 11602, 11], "temperature": 0.0, "avg_logprob": -0.13352932929992675, "compression_ratio": 1.528, "no_speech_prob": 2.156747541448567e-06}, {"id": 671, "seek": 205060, "start": 2070.56, "end": 2073.36, "text": " because of course, academics, particularly at big companies", "tokens": [570, 295, 1164, 11, 25695, 11, 4098, 412, 955, 3431], "temperature": 0.0, "avg_logprob": -0.13352932929992675, "compression_ratio": 1.528, "no_speech_prob": 2.156747541448567e-06}, {"id": 672, "seek": 205060, "start": 2073.36, "end": 2076.44, "text": " like Google and OpenAI and Nvidia and Facebook,", "tokens": [411, 3329, 293, 7238, 48698, 293, 46284, 293, 4384, 11], "temperature": 0.0, "avg_logprob": -0.13352932929992675, "compression_ratio": 1.528, "no_speech_prob": 2.156747541448567e-06}, {"id": 673, "seek": 205060, "start": 2076.44, "end": 2079.72, "text": " love to show off their giant data centers.", "tokens": [959, 281, 855, 766, 641, 7410, 1412, 10898, 13], "temperature": 0.0, "avg_logprob": -0.13352932929992675, "compression_ratio": 1.528, "no_speech_prob": 2.156747541448567e-06}, {"id": 674, "seek": 207972, "start": 2079.72, "end": 2082.8399999999997, "text": " And so they like to say, oh, if we do 1,000 TPUs,", "tokens": [400, 370, 436, 411, 281, 584, 11, 1954, 11, 498, 321, 360, 502, 11, 1360, 314, 8115, 82, 11], "temperature": 0.0, "avg_logprob": -0.1643678244058188, "compression_ratio": 1.78839590443686, "no_speech_prob": 8.012759280973114e-06}, {"id": 675, "seek": 207972, "start": 2082.8399999999997, "end": 2084.9199999999996, "text": " how big a batch size can we create?", "tokens": [577, 955, 257, 15245, 2744, 393, 321, 1884, 30], "temperature": 0.0, "avg_logprob": -0.1643678244058188, "compression_ratio": 1.78839590443686, "no_speech_prob": 8.012759280973114e-06}, {"id": 676, "seek": 207972, "start": 2084.9199999999996, "end": 2088.6, "text": " But for us normal people, these are also interesting,", "tokens": [583, 337, 505, 2710, 561, 11, 613, 366, 611, 1880, 11], "temperature": 0.0, "avg_logprob": -0.1643678244058188, "compression_ratio": 1.78839590443686, "no_speech_prob": 8.012759280973114e-06}, {"id": 677, "seek": 207972, "start": 2088.6, "end": 2090.7999999999997, "text": " because the exact same things tell us how higher learning", "tokens": [570, 264, 1900, 912, 721, 980, 505, 577, 2946, 2539], "temperature": 0.0, "avg_logprob": -0.1643678244058188, "compression_ratio": 1.78839590443686, "no_speech_prob": 8.012759280973114e-06}, {"id": 678, "seek": 207972, "start": 2090.7999999999997, "end": 2091.9199999999996, "text": " rate can we go.", "tokens": [3314, 393, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.1643678244058188, "compression_ratio": 1.78839590443686, "no_speech_prob": 8.012759280973114e-06}, {"id": 679, "seek": 207972, "start": 2091.9199999999996, "end": 2094.2799999999997, "text": " So the exact same things that let you create really big batch", "tokens": [407, 264, 1900, 912, 721, 300, 718, 291, 1884, 534, 955, 15245], "temperature": 0.0, "avg_logprob": -0.1643678244058188, "compression_ratio": 1.78839590443686, "no_speech_prob": 8.012759280973114e-06}, {"id": 680, "seek": 207972, "start": 2094.2799999999997, "end": 2094.7999999999997, "text": " sizes.", "tokens": [11602, 13], "temperature": 0.0, "avg_logprob": -0.1643678244058188, "compression_ratio": 1.78839590443686, "no_speech_prob": 8.012759280973114e-06}, {"id": 681, "seek": 207972, "start": 2094.7999999999997, "end": 2098.12, "text": " So you do a giant batch, and then you take a giant step.", "tokens": [407, 291, 360, 257, 7410, 15245, 11, 293, 550, 291, 747, 257, 7410, 1823, 13], "temperature": 0.0, "avg_logprob": -0.1643678244058188, "compression_ratio": 1.78839590443686, "no_speech_prob": 8.012759280973114e-06}, {"id": 682, "seek": 207972, "start": 2098.12, "end": 2100.6, "text": " Well, we could just take a normal sized batch,", "tokens": [1042, 11, 321, 727, 445, 747, 257, 2710, 20004, 15245, 11], "temperature": 0.0, "avg_logprob": -0.1643678244058188, "compression_ratio": 1.78839590443686, "no_speech_prob": 8.012759280973114e-06}, {"id": 683, "seek": 207972, "start": 2100.6, "end": 2102.52, "text": " but a much bigger than usual step.", "tokens": [457, 257, 709, 3801, 813, 7713, 1823, 13], "temperature": 0.0, "avg_logprob": -0.1643678244058188, "compression_ratio": 1.78839590443686, "no_speech_prob": 8.012759280973114e-06}, {"id": 684, "seek": 207972, "start": 2102.52, "end": 2105.64, "text": " And by using higher learning rates, we train faster,", "tokens": [400, 538, 1228, 2946, 2539, 6846, 11, 321, 3847, 4663, 11], "temperature": 0.0, "avg_logprob": -0.1643678244058188, "compression_ratio": 1.78839590443686, "no_speech_prob": 8.012759280973114e-06}, {"id": 685, "seek": 207972, "start": 2105.64, "end": 2107.2799999999997, "text": " and we generalize better.", "tokens": [293, 321, 2674, 1125, 1101, 13], "temperature": 0.0, "avg_logprob": -0.1643678244058188, "compression_ratio": 1.78839590443686, "no_speech_prob": 8.012759280973114e-06}, {"id": 686, "seek": 207972, "start": 2107.2799999999997, "end": 2108.52, "text": " And so that's all good.", "tokens": [400, 370, 300, 311, 439, 665, 13], "temperature": 0.0, "avg_logprob": -0.1643678244058188, "compression_ratio": 1.78839590443686, "no_speech_prob": 8.012759280973114e-06}, {"id": 687, "seek": 210852, "start": 2108.52, "end": 2112.36, "text": " So this is a really good little trick.", "tokens": [407, 341, 307, 257, 534, 665, 707, 4282, 13], "temperature": 0.0, "avg_logprob": -0.1788004598309917, "compression_ratio": 1.491891891891892, "no_speech_prob": 9.223065717378631e-06}, {"id": 688, "seek": 210852, "start": 2112.36, "end": 2113.92, "text": " OK, so that's conv layer.", "tokens": [2264, 11, 370, 300, 311, 3754, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1788004598309917, "compression_ratio": 1.491891891891892, "no_speech_prob": 9.223065717378631e-06}, {"id": 689, "seek": 210852, "start": 2117.64, "end": 2119.92, "text": " So there's our stem.", "tokens": [407, 456, 311, 527, 12312, 13], "temperature": 0.0, "avg_logprob": -0.1788004598309917, "compression_ratio": 1.491891891891892, "no_speech_prob": 9.223065717378631e-06}, {"id": 690, "seek": 210852, "start": 2119.92, "end": 2124.12, "text": " And then we're going to create a bunch of res blocks.", "tokens": [400, 550, 321, 434, 516, 281, 1884, 257, 3840, 295, 725, 8474, 13], "temperature": 0.0, "avg_logprob": -0.1788004598309917, "compression_ratio": 1.491891891891892, "no_speech_prob": 9.223065717378631e-06}, {"id": 691, "seek": 210852, "start": 2124.12, "end": 2126.72, "text": " So a res block is one of these, except this", "tokens": [407, 257, 725, 3461, 307, 472, 295, 613, 11, 3993, 341], "temperature": 0.0, "avg_logprob": -0.1788004598309917, "compression_ratio": 1.491891891891892, "no_speech_prob": 9.223065717378631e-06}, {"id": 692, "seek": 210852, "start": 2126.72, "end": 2128.96, "text": " is an identity path, right?", "tokens": [307, 364, 6575, 3100, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1788004598309917, "compression_ratio": 1.491891891891892, "no_speech_prob": 9.223065717378631e-06}, {"id": 693, "seek": 210852, "start": 2128.96, "end": 2132.12, "text": " Conv, conv, conv.", "tokens": [2656, 85, 11, 3754, 11, 3754, 13], "temperature": 0.0, "avg_logprob": -0.1788004598309917, "compression_ratio": 1.491891891891892, "no_speech_prob": 9.223065717378631e-06}, {"id": 694, "seek": 210852, "start": 2132.12, "end": 2136.88, "text": " Unless we're doing a resnet 34 or a resnet 18,", "tokens": [16581, 321, 434, 884, 257, 725, 7129, 12790, 420, 257, 725, 7129, 2443, 11], "temperature": 0.0, "avg_logprob": -0.1788004598309917, "compression_ratio": 1.491891891891892, "no_speech_prob": 9.223065717378631e-06}, {"id": 695, "seek": 213688, "start": 2136.88, "end": 2139.8, "text": " in which case one of these cons goes away.", "tokens": [294, 597, 1389, 472, 295, 613, 1014, 1709, 1314, 13], "temperature": 0.0, "avg_logprob": -0.11886744145993833, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.8342407201998867e-05}, {"id": 696, "seek": 213688, "start": 2139.8, "end": 2143.56, "text": " So a resnet 34 and resnet 18 only have two cons here,", "tokens": [407, 257, 725, 7129, 12790, 293, 725, 7129, 2443, 787, 362, 732, 1014, 510, 11], "temperature": 0.0, "avg_logprob": -0.11886744145993833, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.8342407201998867e-05}, {"id": 697, "seek": 213688, "start": 2143.56, "end": 2145.8, "text": " and resnet 50 onwards have three cons.", "tokens": [293, 725, 7129, 2625, 34230, 362, 1045, 1014, 13], "temperature": 0.0, "avg_logprob": -0.11886744145993833, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.8342407201998867e-05}, {"id": 698, "seek": 213688, "start": 2148.6, "end": 2152.52, "text": " And then in resnet 50 and above, the second conv,", "tokens": [400, 550, 294, 725, 7129, 2625, 293, 3673, 11, 264, 1150, 3754, 11], "temperature": 0.0, "avg_logprob": -0.11886744145993833, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.8342407201998867e-05}, {"id": 699, "seek": 213688, "start": 2152.52, "end": 2155.96, "text": " they actually squish the number of channels down by four.", "tokens": [436, 767, 31379, 264, 1230, 295, 9235, 760, 538, 1451, 13], "temperature": 0.0, "avg_logprob": -0.11886744145993833, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.8342407201998867e-05}, {"id": 700, "seek": 213688, "start": 2155.96, "end": 2157.76, "text": " And then they expand it back up again.", "tokens": [400, 550, 436, 5268, 309, 646, 493, 797, 13], "temperature": 0.0, "avg_logprob": -0.11886744145993833, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.8342407201998867e-05}, {"id": 701, "seek": 213688, "start": 2157.76, "end": 2162.56, "text": " So it could go like 64 channels to 16 channels to 64 channels.", "tokens": [407, 309, 727, 352, 411, 12145, 9235, 281, 3165, 9235, 281, 12145, 9235, 13], "temperature": 0.0, "avg_logprob": -0.11886744145993833, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.8342407201998867e-05}, {"id": 702, "seek": 213688, "start": 2162.56, "end": 2164.52, "text": " Let's call it a bottleneck layer.", "tokens": [961, 311, 818, 309, 257, 44641, 547, 4583, 13], "temperature": 0.0, "avg_logprob": -0.11886744145993833, "compression_ratio": 1.676991150442478, "no_speech_prob": 1.8342407201998867e-05}, {"id": 703, "seek": 216452, "start": 2164.52, "end": 2168.36, "text": " So a bottleneck block is the normal block for larger resnets.", "tokens": [407, 257, 44641, 547, 3461, 307, 264, 2710, 3461, 337, 4833, 725, 77, 1385, 13], "temperature": 0.0, "avg_logprob": -0.1378643154043012, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.561235078246682e-06}, {"id": 704, "seek": 216452, "start": 2168.36, "end": 2174.12, "text": " And then just two 3 by 3 cons is the normal for smaller resnets.", "tokens": [400, 550, 445, 732, 805, 538, 805, 1014, 307, 264, 2710, 337, 4356, 725, 77, 1385, 13], "temperature": 0.0, "avg_logprob": -0.1378643154043012, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.561235078246682e-06}, {"id": 705, "seek": 216452, "start": 2174.12, "end": 2178.32, "text": " So you can see in our res block that we pass in this thing", "tokens": [407, 291, 393, 536, 294, 527, 725, 3461, 300, 321, 1320, 294, 341, 551], "temperature": 0.0, "avg_logprob": -0.1378643154043012, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.561235078246682e-06}, {"id": 706, "seek": 216452, "start": 2178.32, "end": 2179.32, "text": " called expansion.", "tokens": [1219, 11260, 13], "temperature": 0.0, "avg_logprob": -0.1378643154043012, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.561235078246682e-06}, {"id": 707, "seek": 216452, "start": 2179.32, "end": 2180.7599999999998, "text": " It's either 1 or 4.", "tokens": [467, 311, 2139, 502, 420, 1017, 13], "temperature": 0.0, "avg_logprob": -0.1378643154043012, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.561235078246682e-06}, {"id": 708, "seek": 216452, "start": 2180.7599999999998, "end": 2186.2, "text": " It's 1 if it's a resnet 18 or 34, and it's 4 if it's bigger.", "tokens": [467, 311, 502, 498, 309, 311, 257, 725, 7129, 2443, 420, 12790, 11, 293, 309, 311, 1017, 498, 309, 311, 3801, 13], "temperature": 0.0, "avg_logprob": -0.1378643154043012, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.561235078246682e-06}, {"id": 709, "seek": 216452, "start": 2186.2, "end": 2189.52, "text": " And so if it's expansion equals 1,", "tokens": [400, 370, 498, 309, 311, 11260, 6915, 502, 11], "temperature": 0.0, "avg_logprob": -0.1378643154043012, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.561235078246682e-06}, {"id": 710, "seek": 216452, "start": 2189.52, "end": 2193.44, "text": " then we just add one extra conv.", "tokens": [550, 321, 445, 909, 472, 2857, 3754, 13], "temperature": 0.0, "avg_logprob": -0.1378643154043012, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.561235078246682e-06}, {"id": 711, "seek": 219344, "start": 2193.44, "end": 2195.76, "text": " Oh, sorry, the first conv is always a 1 by 1.", "tokens": [876, 11, 2597, 11, 264, 700, 3754, 307, 1009, 257, 502, 538, 502, 13], "temperature": 0.0, "avg_logprob": -0.0980394695116126, "compression_ratio": 1.6347826086956523, "no_speech_prob": 1.3419667084235698e-05}, {"id": 712, "seek": 219344, "start": 2195.76, "end": 2197.64, "text": " And then we add a 3 by 3 conv.", "tokens": [400, 550, 321, 909, 257, 805, 538, 805, 3754, 13], "temperature": 0.0, "avg_logprob": -0.0980394695116126, "compression_ratio": 1.6347826086956523, "no_speech_prob": 1.3419667084235698e-05}, {"id": 713, "seek": 219344, "start": 2197.64, "end": 2202.64, "text": " Or if expansion equals 4, we add two extra cons.", "tokens": [1610, 498, 11260, 6915, 1017, 11, 321, 909, 732, 2857, 1014, 13], "temperature": 0.0, "avg_logprob": -0.0980394695116126, "compression_ratio": 1.6347826086956523, "no_speech_prob": 1.3419667084235698e-05}, {"id": 714, "seek": 219344, "start": 2202.64, "end": 2206.4, "text": " So that's what the res blocks are.", "tokens": [407, 300, 311, 437, 264, 725, 8474, 366, 13], "temperature": 0.0, "avg_logprob": -0.0980394695116126, "compression_ratio": 1.6347826086956523, "no_speech_prob": 1.3419667084235698e-05}, {"id": 715, "seek": 219344, "start": 2206.4, "end": 2210.6, "text": " Now, I mentioned that there's two other things here.", "tokens": [823, 11, 286, 2835, 300, 456, 311, 732, 661, 721, 510, 13], "temperature": 0.0, "avg_logprob": -0.0980394695116126, "compression_ratio": 1.6347826086956523, "no_speech_prob": 1.3419667084235698e-05}, {"id": 716, "seek": 219344, "start": 2210.6, "end": 2212.92, "text": " Why are there two other things here?", "tokens": [1545, 366, 456, 732, 661, 721, 510, 30], "temperature": 0.0, "avg_logprob": -0.0980394695116126, "compression_ratio": 1.6347826086956523, "no_speech_prob": 1.3419667084235698e-05}, {"id": 717, "seek": 219344, "start": 2212.92, "end": 2215.92, "text": " Well, we can't use standard res blocks all the way", "tokens": [1042, 11, 321, 393, 380, 764, 3832, 725, 8474, 439, 264, 636], "temperature": 0.0, "avg_logprob": -0.0980394695116126, "compression_ratio": 1.6347826086956523, "no_speech_prob": 1.3419667084235698e-05}, {"id": 718, "seek": 219344, "start": 2215.92, "end": 2217.56, "text": " through our model, can we?", "tokens": [807, 527, 2316, 11, 393, 321, 30], "temperature": 0.0, "avg_logprob": -0.0980394695116126, "compression_ratio": 1.6347826086956523, "no_speech_prob": 1.3419667084235698e-05}, {"id": 719, "seek": 219344, "start": 2217.56, "end": 2221.12, "text": " Because a res block can't change the grid size.", "tokens": [1436, 257, 725, 3461, 393, 380, 1319, 264, 10748, 2744, 13], "temperature": 0.0, "avg_logprob": -0.0980394695116126, "compression_ratio": 1.6347826086956523, "no_speech_prob": 1.3419667084235698e-05}, {"id": 720, "seek": 222112, "start": 2221.12, "end": 2224.24, "text": " We can't have a stride 2 anywhere here.", "tokens": [492, 393, 380, 362, 257, 1056, 482, 568, 4992, 510, 13], "temperature": 0.0, "avg_logprob": -0.13106098422756443, "compression_ratio": 2.0798319327731094, "no_speech_prob": 9.818048056331463e-06}, {"id": 721, "seek": 222112, "start": 2224.24, "end": 2227.24, "text": " Because if we had a stride 2 somewhere here,", "tokens": [1436, 498, 321, 632, 257, 1056, 482, 568, 4079, 510, 11], "temperature": 0.0, "avg_logprob": -0.13106098422756443, "compression_ratio": 2.0798319327731094, "no_speech_prob": 9.818048056331463e-06}, {"id": 722, "seek": 222112, "start": 2227.24, "end": 2228.8399999999997, "text": " we can't add it back to the identity", "tokens": [321, 393, 380, 909, 309, 646, 281, 264, 6575], "temperature": 0.0, "avg_logprob": -0.13106098422756443, "compression_ratio": 2.0798319327731094, "no_speech_prob": 9.818048056331463e-06}, {"id": 723, "seek": 222112, "start": 2228.8399999999997, "end": 2230.88, "text": " because they're now different sizes.", "tokens": [570, 436, 434, 586, 819, 11602, 13], "temperature": 0.0, "avg_logprob": -0.13106098422756443, "compression_ratio": 2.0798319327731094, "no_speech_prob": 9.818048056331463e-06}, {"id": 724, "seek": 222112, "start": 2230.88, "end": 2233.44, "text": " Also, we can't change the number of channels.", "tokens": [2743, 11, 321, 393, 380, 1319, 264, 1230, 295, 9235, 13], "temperature": 0.0, "avg_logprob": -0.13106098422756443, "compression_ratio": 2.0798319327731094, "no_speech_prob": 9.818048056331463e-06}, {"id": 725, "seek": 222112, "start": 2233.44, "end": 2235.4, "text": " Because if we change the number of channels,", "tokens": [1436, 498, 321, 1319, 264, 1230, 295, 9235, 11], "temperature": 0.0, "avg_logprob": -0.13106098422756443, "compression_ratio": 2.0798319327731094, "no_speech_prob": 9.818048056331463e-06}, {"id": 726, "seek": 222112, "start": 2235.4, "end": 2237.3599999999997, "text": " we can't add it to the identity.", "tokens": [321, 393, 380, 909, 309, 281, 264, 6575, 13], "temperature": 0.0, "avg_logprob": -0.13106098422756443, "compression_ratio": 2.0798319327731094, "no_speech_prob": 9.818048056331463e-06}, {"id": 727, "seek": 222112, "start": 2237.3599999999997, "end": 2238.64, "text": " So what do we do?", "tokens": [407, 437, 360, 321, 360, 30], "temperature": 0.0, "avg_logprob": -0.13106098422756443, "compression_ratio": 2.0798319327731094, "no_speech_prob": 9.818048056331463e-06}, {"id": 728, "seek": 222112, "start": 2238.64, "end": 2241.3199999999997, "text": " Well, as you know, from time to time,", "tokens": [1042, 11, 382, 291, 458, 11, 490, 565, 281, 565, 11], "temperature": 0.0, "avg_logprob": -0.13106098422756443, "compression_ratio": 2.0798319327731094, "no_speech_prob": 9.818048056331463e-06}, {"id": 729, "seek": 222112, "start": 2241.3199999999997, "end": 2243.44, "text": " we do like to throw in a stride 2.", "tokens": [321, 360, 411, 281, 3507, 294, 257, 1056, 482, 568, 13], "temperature": 0.0, "avg_logprob": -0.13106098422756443, "compression_ratio": 2.0798319327731094, "no_speech_prob": 9.818048056331463e-06}, {"id": 730, "seek": 222112, "start": 2243.44, "end": 2245.2799999999997, "text": " And generally, when we throw in a stride 2,", "tokens": [400, 5101, 11, 562, 321, 3507, 294, 257, 1056, 482, 568, 11], "temperature": 0.0, "avg_logprob": -0.13106098422756443, "compression_ratio": 2.0798319327731094, "no_speech_prob": 9.818048056331463e-06}, {"id": 731, "seek": 222112, "start": 2245.2799999999997, "end": 2247.4, "text": " we like to double the number of channels.", "tokens": [321, 411, 281, 3834, 264, 1230, 295, 9235, 13], "temperature": 0.0, "avg_logprob": -0.13106098422756443, "compression_ratio": 2.0798319327731094, "no_speech_prob": 9.818048056331463e-06}, {"id": 732, "seek": 222112, "start": 2247.4, "end": 2249.7999999999997, "text": " And so when we do that, we're going", "tokens": [400, 370, 562, 321, 360, 300, 11, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.13106098422756443, "compression_ratio": 2.0798319327731094, "no_speech_prob": 9.818048056331463e-06}, {"id": 733, "seek": 224980, "start": 2249.8, "end": 2253.1600000000003, "text": " to add to the identity path two extra layers.", "tokens": [281, 909, 281, 264, 6575, 3100, 732, 2857, 7914, 13], "temperature": 0.0, "avg_logprob": -0.1389879178600151, "compression_ratio": 1.6219512195121952, "no_speech_prob": 1.1658740731945727e-05}, {"id": 734, "seek": 224980, "start": 2253.1600000000003, "end": 2255.1200000000003, "text": " Well, at an average pooling layer,", "tokens": [1042, 11, 412, 364, 4274, 7005, 278, 4583, 11], "temperature": 0.0, "avg_logprob": -0.1389879178600151, "compression_ratio": 1.6219512195121952, "no_speech_prob": 1.1658740731945727e-05}, {"id": 735, "seek": 224980, "start": 2255.1200000000003, "end": 2257.28, "text": " that's going to cause the grid size to shift down", "tokens": [300, 311, 516, 281, 3082, 264, 10748, 2744, 281, 5513, 760], "temperature": 0.0, "avg_logprob": -0.1389879178600151, "compression_ratio": 1.6219512195121952, "no_speech_prob": 1.1658740731945727e-05}, {"id": 736, "seek": 224980, "start": 2257.28, "end": 2258.88, "text": " by 2 in each dimension.", "tokens": [538, 568, 294, 1184, 10139, 13], "temperature": 0.0, "avg_logprob": -0.1389879178600151, "compression_ratio": 1.6219512195121952, "no_speech_prob": 1.1658740731945727e-05}, {"id": 737, "seek": 224980, "start": 2258.88, "end": 2263.2400000000002, "text": " And we'll add a 1 by 1 conv to change the number of filters.", "tokens": [400, 321, 603, 909, 257, 502, 538, 502, 3754, 281, 1319, 264, 1230, 295, 15995, 13], "temperature": 0.0, "avg_logprob": -0.1389879178600151, "compression_ratio": 1.6219512195121952, "no_speech_prob": 1.1658740731945727e-05}, {"id": 738, "seek": 224980, "start": 2263.2400000000002, "end": 2264.96, "text": " So that's what this is.", "tokens": [407, 300, 311, 437, 341, 307, 13], "temperature": 0.0, "avg_logprob": -0.1389879178600151, "compression_ratio": 1.6219512195121952, "no_speech_prob": 1.1658740731945727e-05}, {"id": 739, "seek": 224980, "start": 2264.96, "end": 2266.8, "text": " And this particular way of doing it", "tokens": [400, 341, 1729, 636, 295, 884, 309], "temperature": 0.0, "avg_logprob": -0.1389879178600151, "compression_ratio": 1.6219512195121952, "no_speech_prob": 1.1658740731945727e-05}, {"id": 740, "seek": 224980, "start": 2266.8, "end": 2270.36, "text": " is specific to the x resnet.", "tokens": [307, 2685, 281, 264, 2031, 725, 7129, 13], "temperature": 0.0, "avg_logprob": -0.1389879178600151, "compression_ratio": 1.6219512195121952, "no_speech_prob": 1.1658740731945727e-05}, {"id": 741, "seek": 224980, "start": 2270.36, "end": 2272.0, "text": " And it gives you a nice little boost", "tokens": [400, 309, 2709, 291, 257, 1481, 707, 9194], "temperature": 0.0, "avg_logprob": -0.1389879178600151, "compression_ratio": 1.6219512195121952, "no_speech_prob": 1.1658740731945727e-05}, {"id": 742, "seek": 224980, "start": 2272.0, "end": 2274.0800000000004, "text": " over the standard approach.", "tokens": [670, 264, 3832, 3109, 13], "temperature": 0.0, "avg_logprob": -0.1389879178600151, "compression_ratio": 1.6219512195121952, "no_speech_prob": 1.1658740731945727e-05}, {"id": 743, "seek": 224980, "start": 2274.0800000000004, "end": 2278.52, "text": " And so you can see that here.", "tokens": [400, 370, 291, 393, 536, 300, 510, 13], "temperature": 0.0, "avg_logprob": -0.1389879178600151, "compression_ratio": 1.6219512195121952, "no_speech_prob": 1.1658740731945727e-05}, {"id": 744, "seek": 227852, "start": 2278.52, "end": 2280.7599999999998, "text": " If the number of inputs is different to the number", "tokens": [759, 264, 1230, 295, 15743, 307, 819, 281, 264, 1230], "temperature": 0.0, "avg_logprob": -0.10378853637393158, "compression_ratio": 1.6292682926829267, "no_speech_prob": 2.8129782094765687e-06}, {"id": 745, "seek": 227852, "start": 2280.7599999999998, "end": 2283.72, "text": " of filters, then we add an extra conv layer.", "tokens": [295, 15995, 11, 550, 321, 909, 364, 2857, 3754, 4583, 13], "temperature": 0.0, "avg_logprob": -0.10378853637393158, "compression_ratio": 1.6292682926829267, "no_speech_prob": 2.8129782094765687e-06}, {"id": 746, "seek": 227852, "start": 2283.72, "end": 2286.84, "text": " Otherwise, we just do no op, no operation,", "tokens": [10328, 11, 321, 445, 360, 572, 999, 11, 572, 6916, 11], "temperature": 0.0, "avg_logprob": -0.10378853637393158, "compression_ratio": 1.6292682926829267, "no_speech_prob": 2.8129782094765687e-06}, {"id": 747, "seek": 227852, "start": 2286.84, "end": 2290.2, "text": " which is defined here.", "tokens": [597, 307, 7642, 510, 13], "temperature": 0.0, "avg_logprob": -0.10378853637393158, "compression_ratio": 1.6292682926829267, "no_speech_prob": 2.8129782094765687e-06}, {"id": 748, "seek": 227852, "start": 2290.2, "end": 2292.8, "text": " And if the stride is something other than 1,", "tokens": [400, 498, 264, 1056, 482, 307, 746, 661, 813, 502, 11], "temperature": 0.0, "avg_logprob": -0.10378853637393158, "compression_ratio": 1.6292682926829267, "no_speech_prob": 2.8129782094765687e-06}, {"id": 749, "seek": 227852, "start": 2292.8, "end": 2294.84, "text": " we add an average pooling.", "tokens": [321, 909, 364, 4274, 7005, 278, 13], "temperature": 0.0, "avg_logprob": -0.10378853637393158, "compression_ratio": 1.6292682926829267, "no_speech_prob": 2.8129782094765687e-06}, {"id": 750, "seek": 227852, "start": 2294.84, "end": 2296.04, "text": " Otherwise, it's a no op.", "tokens": [10328, 11, 309, 311, 257, 572, 999, 13], "temperature": 0.0, "avg_logprob": -0.10378853637393158, "compression_ratio": 1.6292682926829267, "no_speech_prob": 2.8129782094765687e-06}, {"id": 751, "seek": 227852, "start": 2296.04, "end": 2300.64, "text": " And so here is our final resnet block calculation.", "tokens": [400, 370, 510, 307, 527, 2572, 725, 7129, 3461, 17108, 13], "temperature": 0.0, "avg_logprob": -0.10378853637393158, "compression_ratio": 1.6292682926829267, "no_speech_prob": 2.8129782094765687e-06}, {"id": 752, "seek": 227852, "start": 2300.64, "end": 2303.6, "text": " So that's the res block.", "tokens": [407, 300, 311, 264, 725, 3461, 13], "temperature": 0.0, "avg_logprob": -0.10378853637393158, "compression_ratio": 1.6292682926829267, "no_speech_prob": 2.8129782094765687e-06}, {"id": 753, "seek": 230360, "start": 2303.6, "end": 2309.0, "text": " So tweak for resnet d is this way of doing the, they call it,", "tokens": [407, 29879, 337, 725, 7129, 274, 307, 341, 636, 295, 884, 264, 11, 436, 818, 309, 11], "temperature": 0.0, "avg_logprob": -0.17767151331497452, "compression_ratio": 1.6725663716814159, "no_speech_prob": 8.990916171569552e-07}, {"id": 754, "seek": 230360, "start": 2309.0, "end": 2311.7599999999998, "text": " a down sampling path.", "tokens": [257, 760, 21179, 3100, 13], "temperature": 0.0, "avg_logprob": -0.17767151331497452, "compression_ratio": 1.6725663716814159, "no_speech_prob": 8.990916171569552e-07}, {"id": 755, "seek": 230360, "start": 2311.7599999999998, "end": 2312.72, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.17767151331497452, "compression_ratio": 1.6725663716814159, "no_speech_prob": 8.990916171569552e-07}, {"id": 756, "seek": 230360, "start": 2312.72, "end": 2316.04, "text": " And then the final tweak is the actual ordering here", "tokens": [400, 550, 264, 2572, 29879, 307, 264, 3539, 21739, 510], "temperature": 0.0, "avg_logprob": -0.17767151331497452, "compression_ratio": 1.6725663716814159, "no_speech_prob": 8.990916171569552e-07}, {"id": 757, "seek": 230360, "start": 2316.04, "end": 2317.2, "text": " of where the stride 2 is.", "tokens": [295, 689, 264, 1056, 482, 568, 307, 13], "temperature": 0.0, "avg_logprob": -0.17767151331497452, "compression_ratio": 1.6725663716814159, "no_speech_prob": 8.990916171569552e-07}, {"id": 758, "seek": 230360, "start": 2317.2, "end": 2321.2799999999997, "text": " Normally, the stride 2 in normal resnet is at the start.", "tokens": [17424, 11, 264, 1056, 482, 568, 294, 2710, 725, 7129, 307, 412, 264, 722, 13], "temperature": 0.0, "avg_logprob": -0.17767151331497452, "compression_ratio": 1.6725663716814159, "no_speech_prob": 8.990916171569552e-07}, {"id": 759, "seek": 230360, "start": 2321.2799999999997, "end": 2324.12, "text": " And then there's a 3 by 3 after that.", "tokens": [400, 550, 456, 311, 257, 805, 538, 805, 934, 300, 13], "temperature": 0.0, "avg_logprob": -0.17767151331497452, "compression_ratio": 1.6725663716814159, "no_speech_prob": 8.990916171569552e-07}, {"id": 760, "seek": 230360, "start": 2324.12, "end": 2327.8399999999997, "text": " Doing a stride 2 on a 1 by 1 conv is a terrible idea,", "tokens": [18496, 257, 1056, 482, 568, 322, 257, 502, 538, 502, 3754, 307, 257, 6237, 1558, 11], "temperature": 0.0, "avg_logprob": -0.17767151331497452, "compression_ratio": 1.6725663716814159, "no_speech_prob": 8.990916171569552e-07}, {"id": 761, "seek": 230360, "start": 2327.8399999999997, "end": 2329.92, "text": " because you're literally throwing away 3 quarters", "tokens": [570, 291, 434, 3736, 10238, 1314, 805, 20612], "temperature": 0.0, "avg_logprob": -0.17767151331497452, "compression_ratio": 1.6725663716814159, "no_speech_prob": 8.990916171569552e-07}, {"id": 762, "seek": 230360, "start": 2329.92, "end": 2331.4, "text": " of the data.", "tokens": [295, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.17767151331497452, "compression_ratio": 1.6725663716814159, "no_speech_prob": 8.990916171569552e-07}, {"id": 763, "seek": 233140, "start": 2331.4, "end": 2334.0, "text": " And it's interesting, it took people years to realize they're", "tokens": [400, 309, 311, 1880, 11, 309, 1890, 561, 924, 281, 4325, 436, 434], "temperature": 0.0, "avg_logprob": -0.15220088958740235, "compression_ratio": 1.6180555555555556, "no_speech_prob": 6.438854143198114e-06}, {"id": 764, "seek": 233140, "start": 2334.0, "end": 2336.2000000000003, "text": " literally throwing away 3 quarters of the data.", "tokens": [3736, 10238, 1314, 805, 20612, 295, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15220088958740235, "compression_ratio": 1.6180555555555556, "no_speech_prob": 6.438854143198114e-06}, {"id": 765, "seek": 233140, "start": 2336.2000000000003, "end": 2339.48, "text": " So the bag of tricks folks said, let's just move the stride 2", "tokens": [407, 264, 3411, 295, 11733, 4024, 848, 11, 718, 311, 445, 1286, 264, 1056, 482, 568], "temperature": 0.0, "avg_logprob": -0.15220088958740235, "compression_ratio": 1.6180555555555556, "no_speech_prob": 6.438854143198114e-06}, {"id": 766, "seek": 233140, "start": 2339.48, "end": 2340.64, "text": " to the 3 by 3.", "tokens": [281, 264, 805, 538, 805, 13], "temperature": 0.0, "avg_logprob": -0.15220088958740235, "compression_ratio": 1.6180555555555556, "no_speech_prob": 6.438854143198114e-06}, {"id": 767, "seek": 233140, "start": 2340.64, "end": 2342.14, "text": " And that makes a lot more sense, right?", "tokens": [400, 300, 1669, 257, 688, 544, 2020, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15220088958740235, "compression_ratio": 1.6180555555555556, "no_speech_prob": 6.438854143198114e-06}, {"id": 768, "seek": 233140, "start": 2342.14, "end": 2344.1600000000003, "text": " Because the stride 2 3 by 3, you're actually", "tokens": [1436, 264, 1056, 482, 568, 805, 538, 805, 11, 291, 434, 767], "temperature": 0.0, "avg_logprob": -0.15220088958740235, "compression_ratio": 1.6180555555555556, "no_speech_prob": 6.438854143198114e-06}, {"id": 769, "seek": 233140, "start": 2344.1600000000003, "end": 2347.52, "text": " hitting every pixel.", "tokens": [8850, 633, 19261, 13], "temperature": 0.0, "avg_logprob": -0.15220088958740235, "compression_ratio": 1.6180555555555556, "no_speech_prob": 6.438854143198114e-06}, {"id": 770, "seek": 233140, "start": 2347.52, "end": 2349.84, "text": " So the reason I'm mentioning these details", "tokens": [407, 264, 1778, 286, 478, 18315, 613, 4365], "temperature": 0.0, "avg_logprob": -0.15220088958740235, "compression_ratio": 1.6180555555555556, "no_speech_prob": 6.438854143198114e-06}, {"id": 771, "seek": 233140, "start": 2349.84, "end": 2352.48, "text": " is so that you can read that paper", "tokens": [307, 370, 300, 291, 393, 1401, 300, 3035], "temperature": 0.0, "avg_logprob": -0.15220088958740235, "compression_ratio": 1.6180555555555556, "no_speech_prob": 6.438854143198114e-06}, {"id": 772, "seek": 233140, "start": 2352.48, "end": 2356.04, "text": " and spend time thinking about each of those resnet tweaks.", "tokens": [293, 3496, 565, 1953, 466, 1184, 295, 729, 725, 7129, 46664, 13], "temperature": 0.0, "avg_logprob": -0.15220088958740235, "compression_ratio": 1.6180555555555556, "no_speech_prob": 6.438854143198114e-06}, {"id": 773, "seek": 233140, "start": 2356.04, "end": 2358.92, "text": " Do you understand why they did that?", "tokens": [1144, 291, 1223, 983, 436, 630, 300, 30], "temperature": 0.0, "avg_logprob": -0.15220088958740235, "compression_ratio": 1.6180555555555556, "no_speech_prob": 6.438854143198114e-06}, {"id": 774, "seek": 235892, "start": 2358.92, "end": 2362.76, "text": " It wasn't some neural architecture search,", "tokens": [467, 2067, 380, 512, 18161, 9482, 3164, 11], "temperature": 0.0, "avg_logprob": -0.18156549072265624, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.663622793392278e-06}, {"id": 775, "seek": 235892, "start": 2362.76, "end": 2366.64, "text": " try everything brainless, use all our computers approach.", "tokens": [853, 1203, 3567, 1832, 11, 764, 439, 527, 10807, 3109, 13], "temperature": 0.0, "avg_logprob": -0.18156549072265624, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.663622793392278e-06}, {"id": 776, "seek": 235892, "start": 2366.64, "end": 2370.56, "text": " It was, let's sit back and think about how do we actually", "tokens": [467, 390, 11, 718, 311, 1394, 646, 293, 519, 466, 577, 360, 321, 767], "temperature": 0.0, "avg_logprob": -0.18156549072265624, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.663622793392278e-06}, {"id": 777, "seek": 235892, "start": 2370.56, "end": 2372.88, "text": " use all the inputs we have?", "tokens": [764, 439, 264, 15743, 321, 362, 30], "temperature": 0.0, "avg_logprob": -0.18156549072265624, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.663622793392278e-06}, {"id": 778, "seek": 235892, "start": 2372.88, "end": 2376.2400000000002, "text": " And how do we actually take advantage of all the computation", "tokens": [400, 577, 360, 321, 767, 747, 5002, 295, 439, 264, 24903], "temperature": 0.0, "avg_logprob": -0.18156549072265624, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.663622793392278e-06}, {"id": 779, "seek": 235892, "start": 2376.2400000000002, "end": 2377.6, "text": " that we're doing?", "tokens": [300, 321, 434, 884, 30], "temperature": 0.0, "avg_logprob": -0.18156549072265624, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.663622793392278e-06}, {"id": 780, "seek": 235892, "start": 2377.6, "end": 2380.7200000000003, "text": " So it's a very, most of the tweaks", "tokens": [407, 309, 311, 257, 588, 11, 881, 295, 264, 46664], "temperature": 0.0, "avg_logprob": -0.18156549072265624, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.663622793392278e-06}, {"id": 781, "seek": 235892, "start": 2380.7200000000003, "end": 2382.48, "text": " is stuff that exists from before.", "tokens": [307, 1507, 300, 8198, 490, 949, 13], "temperature": 0.0, "avg_logprob": -0.18156549072265624, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.663622793392278e-06}, {"id": 782, "seek": 235892, "start": 2382.48, "end": 2383.7200000000003, "text": " And they've cited all those.", "tokens": [400, 436, 600, 30134, 439, 729, 13], "temperature": 0.0, "avg_logprob": -0.18156549072265624, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.663622793392278e-06}, {"id": 783, "seek": 235892, "start": 2383.7200000000003, "end": 2386.08, "text": " But if you put them all together, it's just a nice,", "tokens": [583, 498, 291, 829, 552, 439, 1214, 11, 309, 311, 445, 257, 1481, 11], "temperature": 0.0, "avg_logprob": -0.18156549072265624, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.663622793392278e-06}, {"id": 784, "seek": 235892, "start": 2386.08, "end": 2388.88, "text": " here's how to think through.", "tokens": [510, 311, 577, 281, 519, 807, 13], "temperature": 0.0, "avg_logprob": -0.18156549072265624, "compression_ratio": 1.6818181818181819, "no_speech_prob": 8.663622793392278e-06}, {"id": 785, "seek": 238888, "start": 2388.88, "end": 2392.28, "text": " Architecture design.", "tokens": [43049, 1715, 13], "temperature": 0.0, "avg_logprob": -0.14272420065743582, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.0784280422958545e-05}, {"id": 786, "seek": 238888, "start": 2392.28, "end": 2393.6, "text": " And that's about it, right?", "tokens": [400, 300, 311, 466, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14272420065743582, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.0784280422958545e-05}, {"id": 787, "seek": 238888, "start": 2393.6, "end": 2400.84, "text": " So we create a resnet block for every res layer.", "tokens": [407, 321, 1884, 257, 725, 7129, 3461, 337, 633, 725, 4583, 13], "temperature": 0.0, "avg_logprob": -0.14272420065743582, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.0784280422958545e-05}, {"id": 788, "seek": 238888, "start": 2400.84, "end": 2403.2400000000002, "text": " And so here it is, creating the resnet block.", "tokens": [400, 370, 510, 309, 307, 11, 4084, 264, 725, 7129, 3461, 13], "temperature": 0.0, "avg_logprob": -0.14272420065743582, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.0784280422958545e-05}, {"id": 789, "seek": 238888, "start": 2403.2400000000002, "end": 2407.28, "text": " And so now we can create all of our resnets", "tokens": [400, 370, 586, 321, 393, 1884, 439, 295, 527, 725, 77, 1385], "temperature": 0.0, "avg_logprob": -0.14272420065743582, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.0784280422958545e-05}, {"id": 790, "seek": 238888, "start": 2407.28, "end": 2411.04, "text": " by simply saying this is how many blocks we", "tokens": [538, 2935, 1566, 341, 307, 577, 867, 8474, 321], "temperature": 0.0, "avg_logprob": -0.14272420065743582, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.0784280422958545e-05}, {"id": 791, "seek": 238888, "start": 2411.04, "end": 2412.56, "text": " have in each layer.", "tokens": [362, 294, 1184, 4583, 13], "temperature": 0.0, "avg_logprob": -0.14272420065743582, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.0784280422958545e-05}, {"id": 792, "seek": 238888, "start": 2412.56, "end": 2417.0, "text": " So resnet 18 is just 2, 2, 2, 2, 2, 34 is 3, 4, 6, 3.", "tokens": [407, 725, 7129, 2443, 307, 445, 568, 11, 568, 11, 568, 11, 568, 11, 568, 11, 12790, 307, 805, 11, 1017, 11, 1386, 11, 805, 13], "temperature": 0.0, "avg_logprob": -0.14272420065743582, "compression_ratio": 1.605263157894737, "no_speech_prob": 2.0784280422958545e-05}, {"id": 793, "seek": 241700, "start": 2417.0, "end": 2420.24, "text": " And then secondly is changing the expansion factor,", "tokens": [400, 550, 26246, 307, 4473, 264, 11260, 5952, 11], "temperature": 0.0, "avg_logprob": -0.13227393484523153, "compression_ratio": 1.6719367588932805, "no_speech_prob": 4.710529537987895e-06}, {"id": 794, "seek": 241700, "start": 2420.24, "end": 2423.04, "text": " which as I said for 18 and 34 is 1.", "tokens": [597, 382, 286, 848, 337, 2443, 293, 12790, 307, 502, 13], "temperature": 0.0, "avg_logprob": -0.13227393484523153, "compression_ratio": 1.6719367588932805, "no_speech_prob": 4.710529537987895e-06}, {"id": 795, "seek": 241700, "start": 2423.04, "end": 2426.12, "text": " And for the bigger ones is 4.", "tokens": [400, 337, 264, 3801, 2306, 307, 1017, 13], "temperature": 0.0, "avg_logprob": -0.13227393484523153, "compression_ratio": 1.6719367588932805, "no_speech_prob": 4.710529537987895e-06}, {"id": 796, "seek": 241700, "start": 2426.12, "end": 2427.96, "text": " So that's a lot of information there.", "tokens": [407, 300, 311, 257, 688, 295, 1589, 456, 13], "temperature": 0.0, "avg_logprob": -0.13227393484523153, "compression_ratio": 1.6719367588932805, "no_speech_prob": 4.710529537987895e-06}, {"id": 797, "seek": 241700, "start": 2427.96, "end": 2430.48, "text": " And if you haven't spent time thinking about architecture", "tokens": [400, 498, 291, 2378, 380, 4418, 565, 1953, 466, 9482], "temperature": 0.0, "avg_logprob": -0.13227393484523153, "compression_ratio": 1.6719367588932805, "no_speech_prob": 4.710529537987895e-06}, {"id": 798, "seek": 241700, "start": 2430.48, "end": 2433.28, "text": " before, it might take you a few reads and lessons", "tokens": [949, 11, 309, 1062, 747, 291, 257, 1326, 15700, 293, 8820], "temperature": 0.0, "avg_logprob": -0.13227393484523153, "compression_ratio": 1.6719367588932805, "no_speech_prob": 4.710529537987895e-06}, {"id": 799, "seek": 241700, "start": 2433.28, "end": 2434.76, "text": " to put the sink in.", "tokens": [281, 829, 264, 9500, 294, 13], "temperature": 0.0, "avg_logprob": -0.13227393484523153, "compression_ratio": 1.6719367588932805, "no_speech_prob": 4.710529537987895e-06}, {"id": 800, "seek": 241700, "start": 2434.76, "end": 2436.32, "text": " But I think it's a really good idea", "tokens": [583, 286, 519, 309, 311, 257, 534, 665, 1558], "temperature": 0.0, "avg_logprob": -0.13227393484523153, "compression_ratio": 1.6719367588932805, "no_speech_prob": 4.710529537987895e-06}, {"id": 801, "seek": 241700, "start": 2436.32, "end": 2438.72, "text": " to try to spend time thinking about that", "tokens": [281, 853, 281, 3496, 565, 1953, 466, 300], "temperature": 0.0, "avg_logprob": -0.13227393484523153, "compression_ratio": 1.6719367588932805, "no_speech_prob": 4.710529537987895e-06}, {"id": 802, "seek": 241700, "start": 2438.72, "end": 2444.64, "text": " and also to experiment and try to think about what's going on.", "tokens": [293, 611, 281, 5120, 293, 853, 281, 519, 466, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.13227393484523153, "compression_ratio": 1.6719367588932805, "no_speech_prob": 4.710529537987895e-06}, {"id": 803, "seek": 244464, "start": 2444.64, "end": 2450.7599999999998, "text": " The other thing to point out here is that this,", "tokens": [440, 661, 551, 281, 935, 484, 510, 307, 300, 341, 11], "temperature": 0.0, "avg_logprob": -0.1748222510019938, "compression_ratio": 1.592783505154639, "no_speech_prob": 8.139250894600991e-06}, {"id": 804, "seek": 244464, "start": 2450.7599999999998, "end": 2457.6, "text": " the way I've written this, it's like this is the whole resnet.", "tokens": [264, 636, 286, 600, 3720, 341, 11, 309, 311, 411, 341, 307, 264, 1379, 725, 7129, 13], "temperature": 0.0, "avg_logprob": -0.1748222510019938, "compression_ratio": 1.592783505154639, "no_speech_prob": 8.139250894600991e-06}, {"id": 805, "seek": 244464, "start": 2457.6, "end": 2459.24, "text": " Other than the definition of conv layer,", "tokens": [5358, 813, 264, 7123, 295, 3754, 4583, 11], "temperature": 0.0, "avg_logprob": -0.1748222510019938, "compression_ratio": 1.592783505154639, "no_speech_prob": 8.139250894600991e-06}, {"id": 806, "seek": 244464, "start": 2459.24, "end": 2460.2799999999997, "text": " this is the whole resnet.", "tokens": [341, 307, 264, 1379, 725, 7129, 13], "temperature": 0.0, "avg_logprob": -0.1748222510019938, "compression_ratio": 1.592783505154639, "no_speech_prob": 8.139250894600991e-06}, {"id": 807, "seek": 244464, "start": 2460.2799999999997, "end": 2462.3199999999997, "text": " It fits on a screen.", "tokens": [467, 9001, 322, 257, 2568, 13], "temperature": 0.0, "avg_logprob": -0.1748222510019938, "compression_ratio": 1.592783505154639, "no_speech_prob": 8.139250894600991e-06}, {"id": 808, "seek": 244464, "start": 2462.3199999999997, "end": 2463.44, "text": " And this is really unusual.", "tokens": [400, 341, 307, 534, 10901, 13], "temperature": 0.0, "avg_logprob": -0.1748222510019938, "compression_ratio": 1.592783505154639, "no_speech_prob": 8.139250894600991e-06}, {"id": 809, "seek": 244464, "start": 2463.44, "end": 2467.24, "text": " Most resnets you see, even without the bag of tricks,", "tokens": [4534, 725, 77, 1385, 291, 536, 11, 754, 1553, 264, 3411, 295, 11733, 11], "temperature": 0.0, "avg_logprob": -0.1748222510019938, "compression_ratio": 1.592783505154639, "no_speech_prob": 8.139250894600991e-06}, {"id": 810, "seek": 244464, "start": 2467.24, "end": 2470.52, "text": " 500, 600, 700 lines of code.", "tokens": [5923, 11, 11849, 11, 15204, 3876, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1748222510019938, "compression_ratio": 1.592783505154639, "no_speech_prob": 8.139250894600991e-06}, {"id": 811, "seek": 247052, "start": 2470.52, "end": 2475.52, "text": " And if every single line of code has a different arbitrary number", "tokens": [400, 498, 633, 2167, 1622, 295, 3089, 575, 257, 819, 23211, 1230], "temperature": 0.0, "avg_logprob": -0.12061100769042969, "compression_ratio": 1.81640625, "no_speech_prob": 1.8056864064419642e-05}, {"id": 812, "seek": 247052, "start": 2475.52, "end": 2478.28, "text": " at 16 here and 32 there and average pool here", "tokens": [412, 3165, 510, 293, 8858, 456, 293, 4274, 7005, 510], "temperature": 0.0, "avg_logprob": -0.12061100769042969, "compression_ratio": 1.81640625, "no_speech_prob": 1.8056864064419642e-05}, {"id": 813, "seek": 247052, "start": 2478.28, "end": 2480.72, "text": " and something else there, how are you going to get it right?", "tokens": [293, 746, 1646, 456, 11, 577, 366, 291, 516, 281, 483, 309, 558, 30], "temperature": 0.0, "avg_logprob": -0.12061100769042969, "compression_ratio": 1.81640625, "no_speech_prob": 1.8056864064419642e-05}, {"id": 814, "seek": 247052, "start": 2480.72, "end": 2483.44, "text": " And how are you going to be able to look at it and say,", "tokens": [400, 577, 366, 291, 516, 281, 312, 1075, 281, 574, 412, 309, 293, 584, 11], "temperature": 0.0, "avg_logprob": -0.12061100769042969, "compression_ratio": 1.81640625, "no_speech_prob": 1.8056864064419642e-05}, {"id": 815, "seek": 247052, "start": 2483.44, "end": 2486.16, "text": " what if I did this a little bit differently?", "tokens": [437, 498, 286, 630, 341, 257, 707, 857, 7614, 30], "temperature": 0.0, "avg_logprob": -0.12061100769042969, "compression_ratio": 1.81640625, "no_speech_prob": 1.8056864064419642e-05}, {"id": 816, "seek": 247052, "start": 2486.16, "end": 2490.28, "text": " So for research and for production,", "tokens": [407, 337, 2132, 293, 337, 4265, 11], "temperature": 0.0, "avg_logprob": -0.12061100769042969, "compression_ratio": 1.81640625, "no_speech_prob": 1.8056864064419642e-05}, {"id": 817, "seek": 247052, "start": 2490.28, "end": 2492.16, "text": " you want to get your code refactored", "tokens": [291, 528, 281, 483, 428, 3089, 1895, 578, 2769], "temperature": 0.0, "avg_logprob": -0.12061100769042969, "compression_ratio": 1.81640625, "no_speech_prob": 1.8056864064419642e-05}, {"id": 818, "seek": 247052, "start": 2492.16, "end": 2494.44, "text": " like this for your architecture so that you can look at it", "tokens": [411, 341, 337, 428, 9482, 370, 300, 291, 393, 574, 412, 309], "temperature": 0.0, "avg_logprob": -0.12061100769042969, "compression_ratio": 1.81640625, "no_speech_prob": 1.8056864064419642e-05}, {"id": 819, "seek": 247052, "start": 2494.44, "end": 2497.7599999999998, "text": " and say, what exactly is going on?", "tokens": [293, 584, 11, 437, 2293, 307, 516, 322, 30], "temperature": 0.0, "avg_logprob": -0.12061100769042969, "compression_ratio": 1.81640625, "no_speech_prob": 1.8056864064419642e-05}, {"id": 820, "seek": 247052, "start": 2497.7599999999998, "end": 2499.52, "text": " Is it written correctly?", "tokens": [1119, 309, 3720, 8944, 30], "temperature": 0.0, "avg_logprob": -0.12061100769042969, "compression_ratio": 1.81640625, "no_speech_prob": 1.8056864064419642e-05}, {"id": 821, "seek": 249952, "start": 2499.52, "end": 2502.48, "text": " OK, I want to change this stride to being in a different layer.", "tokens": [2264, 11, 286, 528, 281, 1319, 341, 1056, 482, 281, 885, 294, 257, 819, 4583, 13], "temperature": 0.0, "avg_logprob": -0.15113242727811219, "compression_ratio": 1.5661764705882353, "no_speech_prob": 8.267361408798024e-06}, {"id": 822, "seek": 249952, "start": 2502.48, "end": 2504.2, "text": " How do I do it?", "tokens": [1012, 360, 286, 360, 309, 30], "temperature": 0.0, "avg_logprob": -0.15113242727811219, "compression_ratio": 1.5661764705882353, "no_speech_prob": 8.267361408798024e-06}, {"id": 823, "seek": 249952, "start": 2504.2, "end": 2508.7599999999998, "text": " It's really important for effective practitioners", "tokens": [467, 311, 534, 1021, 337, 4942, 25742], "temperature": 0.0, "avg_logprob": -0.15113242727811219, "compression_ratio": 1.5661764705882353, "no_speech_prob": 8.267361408798024e-06}, {"id": 824, "seek": 249952, "start": 2508.7599999999998, "end": 2512.52, "text": " to be able to write nice, concise architectures", "tokens": [281, 312, 1075, 281, 2464, 1481, 11, 44882, 6331, 1303], "temperature": 0.0, "avg_logprob": -0.15113242727811219, "compression_ratio": 1.5661764705882353, "no_speech_prob": 8.267361408798024e-06}, {"id": 825, "seek": 249952, "start": 2512.52, "end": 2516.32, "text": " so that you can change them and understand them.", "tokens": [370, 300, 291, 393, 1319, 552, 293, 1223, 552, 13], "temperature": 0.0, "avg_logprob": -0.15113242727811219, "compression_ratio": 1.5661764705882353, "no_speech_prob": 8.267361408798024e-06}, {"id": 826, "seek": 249952, "start": 2516.32, "end": 2518.88, "text": " OK, so that's our X resnet.", "tokens": [2264, 11, 370, 300, 311, 527, 1783, 725, 7129, 13], "temperature": 0.0, "avg_logprob": -0.15113242727811219, "compression_ratio": 1.5661764705882353, "no_speech_prob": 8.267361408798024e-06}, {"id": 827, "seek": 249952, "start": 2518.88, "end": 2522.2, "text": " We can train it with or without mix up.", "tokens": [492, 393, 3847, 309, 365, 420, 1553, 2890, 493, 13], "temperature": 0.0, "avg_logprob": -0.15113242727811219, "compression_ratio": 1.5661764705882353, "no_speech_prob": 8.267361408798024e-06}, {"id": 828, "seek": 249952, "start": 2522.2, "end": 2523.16, "text": " It's up to us.", "tokens": [467, 311, 493, 281, 505, 13], "temperature": 0.0, "avg_logprob": -0.15113242727811219, "compression_ratio": 1.5661764705882353, "no_speech_prob": 8.267361408798024e-06}, {"id": 829, "seek": 249952, "start": 2523.16, "end": 2526.32, "text": " Label smoothing cross-entropy is probably always a good idea", "tokens": [10137, 338, 899, 6259, 571, 3278, 12, 317, 27514, 307, 1391, 1009, 257, 665, 1558], "temperature": 0.0, "avg_logprob": -0.15113242727811219, "compression_ratio": 1.5661764705882353, "no_speech_prob": 8.267361408798024e-06}, {"id": 830, "seek": 249952, "start": 2526.32, "end": 2529.0, "text": " unless you know that your labels are basically perfect.", "tokens": [5969, 291, 458, 300, 428, 16949, 366, 1936, 2176, 13], "temperature": 0.0, "avg_logprob": -0.15113242727811219, "compression_ratio": 1.5661764705882353, "no_speech_prob": 8.267361408798024e-06}, {"id": 831, "seek": 252900, "start": 2529.0, "end": 2532.88, "text": " Let's just create a little resnet 18.", "tokens": [961, 311, 445, 1884, 257, 707, 725, 7129, 2443, 13], "temperature": 0.0, "avg_logprob": -0.1474104303074634, "compression_ratio": 1.673003802281369, "no_speech_prob": 5.0144271881436e-06}, {"id": 832, "seek": 252900, "start": 2532.88, "end": 2536.48, "text": " And let's check out to see what our model is doing.", "tokens": [400, 718, 311, 1520, 484, 281, 536, 437, 527, 2316, 307, 884, 13], "temperature": 0.0, "avg_logprob": -0.1474104303074634, "compression_ratio": 1.673003802281369, "no_speech_prob": 5.0144271881436e-06}, {"id": 833, "seek": 252900, "start": 2536.48, "end": 2538.08, "text": " So we've already got a model summary,", "tokens": [407, 321, 600, 1217, 658, 257, 2316, 12691, 11], "temperature": 0.0, "avg_logprob": -0.1474104303074634, "compression_ratio": 1.673003802281369, "no_speech_prob": 5.0144271881436e-06}, {"id": 834, "seek": 252900, "start": 2538.08, "end": 2539.48, "text": " but we're just going to rewrite it", "tokens": [457, 321, 434, 445, 516, 281, 28132, 309], "temperature": 0.0, "avg_logprob": -0.1474104303074634, "compression_ratio": 1.673003802281369, "no_speech_prob": 5.0144271881436e-06}, {"id": 835, "seek": 252900, "start": 2539.48, "end": 2541.84, "text": " to use the new version of Learner that", "tokens": [281, 764, 264, 777, 3037, 295, 17216, 260, 300], "temperature": 0.0, "avg_logprob": -0.1474104303074634, "compression_ratio": 1.673003802281369, "no_speech_prob": 5.0144271881436e-06}, {"id": 836, "seek": 252900, "start": 2541.84, "end": 2543.64, "text": " doesn't have runner anymore.", "tokens": [1177, 380, 362, 24376, 3602, 13], "temperature": 0.0, "avg_logprob": -0.1474104303074634, "compression_ratio": 1.673003802281369, "no_speech_prob": 5.0144271881436e-06}, {"id": 837, "seek": 252900, "start": 2543.64, "end": 2548.24, "text": " And so we can print out and see what happens to our shapes", "tokens": [400, 370, 321, 393, 4482, 484, 293, 536, 437, 2314, 281, 527, 10854], "temperature": 0.0, "avg_logprob": -0.1474104303074634, "compression_ratio": 1.673003802281369, "no_speech_prob": 5.0144271881436e-06}, {"id": 838, "seek": 252900, "start": 2548.24, "end": 2550.08, "text": " as they go through the model.", "tokens": [382, 436, 352, 807, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1474104303074634, "compression_ratio": 1.673003802281369, "no_speech_prob": 5.0144271881436e-06}, {"id": 839, "seek": 252900, "start": 2550.08, "end": 2552.96, "text": " And you can change this print mod here to true,", "tokens": [400, 291, 393, 1319, 341, 4482, 1072, 510, 281, 2074, 11], "temperature": 0.0, "avg_logprob": -0.1474104303074634, "compression_ratio": 1.673003802281369, "no_speech_prob": 5.0144271881436e-06}, {"id": 840, "seek": 252900, "start": 2552.96, "end": 2554.96, "text": " and it'll print out the entire blocks", "tokens": [293, 309, 603, 4482, 484, 264, 2302, 8474], "temperature": 0.0, "avg_logprob": -0.1474104303074634, "compression_ratio": 1.673003802281369, "no_speech_prob": 5.0144271881436e-06}, {"id": 841, "seek": 252900, "start": 2554.96, "end": 2556.36, "text": " and then show you what's going on.", "tokens": [293, 550, 855, 291, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.1474104303074634, "compression_ratio": 1.673003802281369, "no_speech_prob": 5.0144271881436e-06}, {"id": 842, "seek": 255636, "start": 2556.36, "end": 2559.04, "text": " So that would be a really useful thing to help you understand", "tokens": [407, 300, 576, 312, 257, 534, 4420, 551, 281, 854, 291, 1223], "temperature": 0.0, "avg_logprob": -0.13785998026529947, "compression_ratio": 1.7061224489795919, "no_speech_prob": 1.095279094442958e-05}, {"id": 843, "seek": 255636, "start": 2559.04, "end": 2561.48, "text": " what's going on in the model.", "tokens": [437, 311, 516, 322, 294, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13785998026529947, "compression_ratio": 1.7061224489795919, "no_speech_prob": 1.095279094442958e-05}, {"id": 844, "seek": 255636, "start": 2561.48, "end": 2563.84, "text": " All right, so here's our architecture.", "tokens": [1057, 558, 11, 370, 510, 311, 527, 9482, 13], "temperature": 0.0, "avg_logprob": -0.13785998026529947, "compression_ratio": 1.7061224489795919, "no_speech_prob": 1.095279094442958e-05}, {"id": 845, "seek": 255636, "start": 2563.84, "end": 2565.48, "text": " It's nice and easy.", "tokens": [467, 311, 1481, 293, 1858, 13], "temperature": 0.0, "avg_logprob": -0.13785998026529947, "compression_ratio": 1.7061224489795919, "no_speech_prob": 1.095279094442958e-05}, {"id": 846, "seek": 255636, "start": 2565.48, "end": 2567.36, "text": " We can tell you how many channels are coming in,", "tokens": [492, 393, 980, 291, 577, 867, 9235, 366, 1348, 294, 11], "temperature": 0.0, "avg_logprob": -0.13785998026529947, "compression_ratio": 1.7061224489795919, "no_speech_prob": 1.095279094442958e-05}, {"id": 847, "seek": 255636, "start": 2567.36, "end": 2569.6800000000003, "text": " how many channels are coming out,", "tokens": [577, 867, 9235, 366, 1348, 484, 11], "temperature": 0.0, "avg_logprob": -0.13785998026529947, "compression_ratio": 1.7061224489795919, "no_speech_prob": 1.095279094442958e-05}, {"id": 848, "seek": 255636, "start": 2569.6800000000003, "end": 2573.92, "text": " and it'll adapt automatically to our data that way.", "tokens": [293, 309, 603, 6231, 6772, 281, 527, 1412, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.13785998026529947, "compression_ratio": 1.7061224489795919, "no_speech_prob": 1.095279094442958e-05}, {"id": 849, "seek": 255636, "start": 2573.92, "end": 2575.1200000000003, "text": " So we can create our Learner.", "tokens": [407, 321, 393, 1884, 527, 17216, 260, 13], "temperature": 0.0, "avg_logprob": -0.13785998026529947, "compression_ratio": 1.7061224489795919, "no_speech_prob": 1.095279094442958e-05}, {"id": 850, "seek": 255636, "start": 2575.1200000000003, "end": 2578.84, "text": " We can do our LR find.", "tokens": [492, 393, 360, 527, 441, 49, 915, 13], "temperature": 0.0, "avg_logprob": -0.13785998026529947, "compression_ratio": 1.7061224489795919, "no_speech_prob": 1.095279094442958e-05}, {"id": 851, "seek": 255636, "start": 2578.84, "end": 2582.6400000000003, "text": " And now that we've done that, let's create a one cycle", "tokens": [400, 586, 300, 321, 600, 1096, 300, 11, 718, 311, 1884, 257, 472, 6586], "temperature": 0.0, "avg_logprob": -0.13785998026529947, "compression_ratio": 1.7061224489795919, "no_speech_prob": 1.095279094442958e-05}, {"id": 852, "seek": 255636, "start": 2582.6400000000003, "end": 2584.92, "text": " learning rate annealing.", "tokens": [2539, 3314, 22256, 4270, 13], "temperature": 0.0, "avg_logprob": -0.13785998026529947, "compression_ratio": 1.7061224489795919, "no_speech_prob": 1.095279094442958e-05}, {"id": 853, "seek": 258492, "start": 2584.92, "end": 2587.84, "text": " So a one cycle learning rate annealing.", "tokens": [407, 257, 472, 6586, 2539, 3314, 22256, 4270, 13], "temperature": 0.0, "avg_logprob": -0.12418296557514608, "compression_ratio": 1.6767241379310345, "no_speech_prob": 4.092759809282143e-06}, {"id": 854, "seek": 258492, "start": 2587.84, "end": 2589.36, "text": " We've seen all this before.", "tokens": [492, 600, 1612, 439, 341, 949, 13], "temperature": 0.0, "avg_logprob": -0.12418296557514608, "compression_ratio": 1.6767241379310345, "no_speech_prob": 4.092759809282143e-06}, {"id": 855, "seek": 258492, "start": 2589.36, "end": 2592.48, "text": " We keep on creating these things like 0.3, 0.7", "tokens": [492, 1066, 322, 4084, 613, 721, 411, 1958, 13, 18, 11, 1958, 13, 22], "temperature": 0.0, "avg_logprob": -0.12418296557514608, "compression_ratio": 1.6767241379310345, "no_speech_prob": 4.092759809282143e-06}, {"id": 856, "seek": 258492, "start": 2592.48, "end": 2596.16, "text": " for the two phases or 0.3, 0.2, 0.5 for three phases.", "tokens": [337, 264, 732, 18764, 420, 1958, 13, 18, 11, 1958, 13, 17, 11, 1958, 13, 20, 337, 1045, 18764, 13], "temperature": 0.0, "avg_logprob": -0.12418296557514608, "compression_ratio": 1.6767241379310345, "no_speech_prob": 4.092759809282143e-06}, {"id": 857, "seek": 258492, "start": 2596.16, "end": 2599.16, "text": " So I added little create phases that will build those for us", "tokens": [407, 286, 3869, 707, 1884, 18764, 300, 486, 1322, 729, 337, 505], "temperature": 0.0, "avg_logprob": -0.12418296557514608, "compression_ratio": 1.6767241379310345, "no_speech_prob": 4.092759809282143e-06}, {"id": 858, "seek": 258492, "start": 2599.16, "end": 2601.32, "text": " automatically.", "tokens": [6772, 13], "temperature": 0.0, "avg_logprob": -0.12418296557514608, "compression_ratio": 1.6767241379310345, "no_speech_prob": 4.092759809282143e-06}, {"id": 859, "seek": 258492, "start": 2601.32, "end": 2603.12, "text": " This one we built before.", "tokens": [639, 472, 321, 3094, 949, 13], "temperature": 0.0, "avg_logprob": -0.12418296557514608, "compression_ratio": 1.6767241379310345, "no_speech_prob": 4.092759809282143e-06}, {"id": 860, "seek": 258492, "start": 2603.12, "end": 2608.16, "text": " So here's our standard one cycle annealing.", "tokens": [407, 510, 311, 527, 3832, 472, 6586, 22256, 4270, 13], "temperature": 0.0, "avg_logprob": -0.12418296557514608, "compression_ratio": 1.6767241379310345, "no_speech_prob": 4.092759809282143e-06}, {"id": 861, "seek": 258492, "start": 2608.16, "end": 2610.56, "text": " And here's our parameter scheduler.", "tokens": [400, 510, 311, 527, 13075, 12000, 260, 13], "temperature": 0.0, "avg_logprob": -0.12418296557514608, "compression_ratio": 1.6767241379310345, "no_speech_prob": 4.092759809282143e-06}, {"id": 862, "seek": 258492, "start": 2610.56, "end": 2613.28, "text": " And so one other thing I did last week", "tokens": [400, 370, 472, 661, 551, 286, 630, 1036, 1243], "temperature": 0.0, "avg_logprob": -0.12418296557514608, "compression_ratio": 1.6767241379310345, "no_speech_prob": 4.092759809282143e-06}, {"id": 863, "seek": 261328, "start": 2613.28, "end": 2616.2000000000003, "text": " was I made it that callbacks, you", "tokens": [390, 286, 1027, 309, 300, 818, 17758, 11, 291], "temperature": 0.0, "avg_logprob": -0.13186438221576785, "compression_ratio": 1.5778688524590163, "no_speech_prob": 1.3419527931546327e-05}, {"id": 864, "seek": 261328, "start": 2616.2000000000003, "end": 2618.4, "text": " don't have to pass to the initializer.", "tokens": [500, 380, 362, 281, 1320, 281, 264, 5883, 6545, 13], "temperature": 0.0, "avg_logprob": -0.13186438221576785, "compression_ratio": 1.5778688524590163, "no_speech_prob": 1.3419527931546327e-05}, {"id": 865, "seek": 261328, "start": 2618.4, "end": 2620.52, "text": " You can also pass them to the fit function.", "tokens": [509, 393, 611, 1320, 552, 281, 264, 3318, 2445, 13], "temperature": 0.0, "avg_logprob": -0.13186438221576785, "compression_ratio": 1.5778688524590163, "no_speech_prob": 1.3419527931546327e-05}, {"id": 866, "seek": 261328, "start": 2620.52, "end": 2622.8, "text": " And it'll just run those callbacks to the fit functions.", "tokens": [400, 309, 603, 445, 1190, 729, 818, 17758, 281, 264, 3318, 6828, 13], "temperature": 0.0, "avg_logprob": -0.13186438221576785, "compression_ratio": 1.5778688524590163, "no_speech_prob": 1.3419527931546327e-05}, {"id": 867, "seek": 261328, "start": 2622.8, "end": 2625.4, "text": " This is a great way to do parameter scheduling.", "tokens": [639, 307, 257, 869, 636, 281, 360, 13075, 29055, 13], "temperature": 0.0, "avg_logprob": -0.13186438221576785, "compression_ratio": 1.5778688524590163, "no_speech_prob": 1.3419527931546327e-05}, {"id": 868, "seek": 261328, "start": 2625.4, "end": 2626.6800000000003, "text": " And there we go.", "tokens": [400, 456, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.13186438221576785, "compression_ratio": 1.5778688524590163, "no_speech_prob": 1.3419527931546327e-05}, {"id": 869, "seek": 261328, "start": 2626.6800000000003, "end": 2631.6400000000003, "text": " And so 83.2.", "tokens": [400, 370, 30997, 13, 17, 13], "temperature": 0.0, "avg_logprob": -0.13186438221576785, "compression_ratio": 1.5778688524590163, "no_speech_prob": 1.3419527931546327e-05}, {"id": 870, "seek": 261328, "start": 2631.6400000000003, "end": 2635.36, "text": " So I would love to see people beat my benchmarks here.", "tokens": [407, 286, 576, 959, 281, 536, 561, 4224, 452, 43751, 510, 13], "temperature": 0.0, "avg_logprob": -0.13186438221576785, "compression_ratio": 1.5778688524590163, "no_speech_prob": 1.3419527931546327e-05}, {"id": 871, "seek": 261328, "start": 2635.36, "end": 2637.8, "text": " So here's the ImageNet site.", "tokens": [407, 510, 311, 264, 29903, 31890, 3621, 13], "temperature": 0.0, "avg_logprob": -0.13186438221576785, "compression_ratio": 1.5778688524590163, "no_speech_prob": 1.3419527931546327e-05}, {"id": 872, "seek": 261328, "start": 2637.8, "end": 2641.5600000000004, "text": " And so so far, the best I've got for 128.5 epochs", "tokens": [400, 370, 370, 1400, 11, 264, 1151, 286, 600, 658, 337, 29810, 13, 20, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.13186438221576785, "compression_ratio": 1.5778688524590163, "no_speech_prob": 1.3419527931546327e-05}, {"id": 873, "seek": 264156, "start": 2641.56, "end": 2643.68, "text": " is 84.6.", "tokens": [307, 29018, 13, 21, 13], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 874, "seek": 264156, "start": 2643.68, "end": 2647.52, "text": " So yeah, we're super close.", "tokens": [407, 1338, 11, 321, 434, 1687, 1998, 13], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 875, "seek": 264156, "start": 2647.52, "end": 2649.04, "text": " So maybe with some fiddling around,", "tokens": [407, 1310, 365, 512, 283, 14273, 1688, 926, 11], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 876, "seek": 264156, "start": 2649.04, "end": 2651.44, "text": " you can find something that's even better.", "tokens": [291, 393, 915, 746, 300, 311, 754, 1101, 13], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 877, "seek": 264156, "start": 2651.44, "end": 2653.2799999999997, "text": " And with these kind of leaderboards", "tokens": [400, 365, 613, 733, 295, 5263, 17228], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 878, "seek": 264156, "start": 2653.2799999999997, "end": 2655.56, "text": " where a lot of these things can train in,", "tokens": [689, 257, 688, 295, 613, 721, 393, 3847, 294, 11], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 879, "seek": 264156, "start": 2655.56, "end": 2641.56, "text": " this is 2 and 1 1", "tokens": [341, 307, 568, 293, 502, 502, 50275, 50358], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 880, "seek": 264156, "start": 2655.56, "end": 2656.2799999999997, "text": " This is 2 and 1 1", "tokens": [639, 307, 568, 293, 502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 881, "seek": 264156, "start": 2656.2799999999997, "end": 2656.2799999999997, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 882, "seek": 264156, "start": 2656.2799999999997, "end": 2656.2799999999997, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 883, "seek": 264156, "start": 2656.2799999999997, "end": 2656.2799999999997, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 884, "seek": 264156, "start": 2656.2799999999997, "end": 2656.2799999999997, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 885, "seek": 264156, "start": 2656.2799999999997, "end": 2656.2799999999997, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 886, "seek": 264156, "start": 2656.2799999999997, "end": 2656.2799999999997, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 887, "seek": 264156, "start": 2656.2799999999997, "end": 2656.2799999999997, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 888, "seek": 264156, "start": 2656.2799999999997, "end": 2656.2799999999997, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 889, "seek": 264156, "start": 2656.2799999999997, "end": 2656.36, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 890, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 891, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 892, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 893, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 894, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 895, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 896, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 897, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1 1", "tokens": [502, 502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 898, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 899, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 900, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 901, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 902, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 903, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 904, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 905, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 906, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 907, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 908, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 909, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 910, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 911, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 912, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 913, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 914, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 915, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 916, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 917, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 918, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 919, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 920, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 921, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 922, "seek": 264156, "start": 2656.36, "end": 2656.36, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.28598811059789275, "compression_ratio": 2.033333333333333, "no_speech_prob": 8.3977511167177e-06}, {"id": 923, "seek": 265636, "start": 2656.36, "end": 2685.1600000000003, "text": " minutes on a standard I think it was a GTX 1080 Ti you can quickly try things out and what I've noticed is that the results I get in 5 epochs on 128 pixel image net models carry over a lot to image net training or bigger models so like you can learn a lot by not trying to train giant models so compete on this leaderboard to become a better practitioner to try out things right and if you do have some more", "tokens": [50364, 2077, 322, 257, 3832, 286, 519, 309, 390, 257, 17530, 55, 24547, 20456, 291, 393, 2661, 853, 721, 484, 293, 437, 286, 600, 5694, 307, 300, 264, 3542, 286, 483, 294, 1025, 30992, 28346, 322, 29810, 19261, 3256, 2533, 5245, 3985, 670, 257, 688, 281, 3256, 2533, 3097, 420, 3801, 5245, 370, 411, 291, 393, 1466, 257, 688, 538, 406, 1382, 281, 3847, 7410, 5245, 370, 11831, 322, 341, 5263, 3787, 281, 1813, 257, 1101, 32125, 281, 853, 484, 721, 558, 293, 498, 291, 360, 362, 512, 544, 51804], "temperature": 0.0, "avg_logprob": -0.2590979691390153, "compression_ratio": 1.628, "no_speech_prob": 0.205993190407753}, {"id": 924, "seek": 268636, "start": 2686.36, "end": 2707.96, "text": " time you can go all the way to 400 epochs that might take a couple of hours and then of course also we've got image wharf which is just doggy photos and it's much harder and actually this one I find an even better test case because it's a more difficult data set so we've got a 90% is my best for this so I hope somebody can can beat me I really do.", "tokens": [565, 291, 393, 352, 439, 264, 636, 281, 8423, 30992, 28346, 300, 1062, 747, 257, 1916, 295, 2496, 293, 550, 295, 1164, 611, 321, 600, 658, 3256, 315, 30224, 597, 307, 445, 3000, 1480, 5787, 293, 309, 311, 709, 6081, 293, 767, 341, 472, 286, 915, 364, 754, 1101, 1500, 1389, 570, 309, 311, 257, 544, 2252, 1412, 992, 370, 321, 600, 658, 257, 4289, 4, 307, 452, 1151, 337, 341, 370, 286, 1454, 2618, 393, 393, 4224, 385, 286, 534, 360, 13], "temperature": 0.0, "avg_logprob": -0.14643939884229637, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.4285759925842285}, {"id": 925, "seek": 270796, "start": 2707.96, "end": 2731.16, "text": " So we can refactor all that stuff of adding all these different callbacks and stuff into a single function called CNN learner and we can just pass in an architecture and our data and our loss function and optimization function and what kind of callbacks do we want just yes or no and we'll just set everything up.", "tokens": [407, 321, 393, 1895, 15104, 439, 300, 1507, 295, 5127, 439, 613, 819, 818, 17758, 293, 1507, 666, 257, 2167, 2445, 1219, 24859, 33347, 293, 321, 393, 445, 1320, 294, 364, 9482, 293, 527, 1412, 293, 527, 4470, 2445, 293, 19618, 2445, 293, 437, 733, 295, 818, 17758, 360, 321, 528, 445, 2086, 420, 572, 293, 321, 603, 445, 992, 1203, 493, 13], "temperature": 0.0, "avg_logprob": -0.19773480429578183, "compression_ratio": 1.701086956521739, "no_speech_prob": 0.00012525040074251592}, {"id": 926, "seek": 273116, "start": 2731.16, "end": 2738.7599999999998, "text": " And if you don't pass in see it and see out will grab it from your data for you and then we'll just pass that off to the.", "tokens": [400, 498, 291, 500, 380, 1320, 294, 536, 309, 293, 536, 484, 486, 4444, 309, 490, 428, 1412, 337, 291, 293, 550, 321, 603, 445, 1320, 300, 766, 281, 264, 13], "temperature": 0.0, "avg_logprob": -0.2519826022061435, "compression_ratio": 1.6244343891402715, "no_speech_prob": 5.390545629779808e-05}, {"id": 927, "seek": 273116, "start": 2740.7599999999998, "end": 2744.96, "text": " So that makes things easier so now if you want to create a CNN it's just one line of code.", "tokens": [407, 300, 1669, 721, 3571, 370, 586, 498, 291, 528, 281, 1884, 257, 24859, 309, 311, 445, 472, 1622, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.2519826022061435, "compression_ratio": 1.6244343891402715, "no_speech_prob": 5.390545629779808e-05}, {"id": 928, "seek": 273116, "start": 2745.56, "end": 2748.7599999999998, "text": " Adding in whatever we want mix up labels moving blah blah blah.", "tokens": [31204, 294, 2035, 321, 528, 2890, 493, 16949, 2684, 12288, 12288, 12288, 13], "temperature": 0.0, "avg_logprob": -0.2519826022061435, "compression_ratio": 1.6244343891402715, "no_speech_prob": 5.390545629779808e-05}, {"id": 929, "seek": 273116, "start": 2749.96, "end": 2751.7599999999998, "text": " And so we get the same result when we fit it.", "tokens": [400, 370, 321, 483, 264, 912, 1874, 562, 321, 3318, 309, 13], "temperature": 0.0, "avg_logprob": -0.2519826022061435, "compression_ratio": 1.6244343891402715, "no_speech_prob": 5.390545629779808e-05}, {"id": 930, "seek": 273116, "start": 2752.7599999999998, "end": 2754.96, "text": " So we can see this all put together.", "tokens": [407, 321, 393, 536, 341, 439, 829, 1214, 13], "temperature": 0.0, "avg_logprob": -0.2519826022061435, "compression_ratio": 1.6244343891402715, "no_speech_prob": 5.390545629779808e-05}, {"id": 931, "seek": 275496, "start": 2754.96, "end": 2777.96, "text": " In this image net training script which is in fast AI in examples slash train image net and this entire thing will look entirely familiar to you it's all stuff that we've now built from scratch with one exception which is this bit which is using multiple GPU's so we're not covering that but that's just a.", "tokens": [682, 341, 3256, 2533, 3097, 5755, 597, 307, 294, 2370, 7318, 294, 5110, 17330, 3847, 3256, 2533, 293, 341, 2302, 551, 486, 574, 7696, 4963, 281, 291, 309, 311, 439, 1507, 300, 321, 600, 586, 3094, 490, 8459, 365, 472, 11183, 597, 307, 341, 857, 597, 307, 1228, 3866, 18407, 311, 370, 321, 434, 406, 10322, 300, 457, 300, 311, 445, 257, 13], "temperature": 0.0, "avg_logprob": -0.22745576545373716, "compression_ratio": 1.577319587628866, "no_speech_prob": 5.2245533879613504e-05}, {"id": 932, "seek": 277796, "start": 2777.96, "end": 2787.16, "text": " You know an acceleration tweak and you can easily use multiple GPU's by simply doing data parallel or two distributed.", "tokens": [509, 458, 364, 17162, 29879, 293, 291, 393, 3612, 764, 3866, 18407, 311, 538, 2935, 884, 1412, 8952, 420, 732, 12631, 13], "temperature": 0.0, "avg_logprob": -0.24740942024890286, "compression_ratio": 1.5021645021645023, "no_speech_prob": 8.267380508186761e-06}, {"id": 933, "seek": 277796, "start": 2788.16, "end": 2792.36, "text": " Other than that yeah this is all stuff that you see in those labels moving cross entropy.", "tokens": [5358, 813, 300, 1338, 341, 307, 439, 1507, 300, 291, 536, 294, 729, 16949, 2684, 3278, 30867, 13], "temperature": 0.0, "avg_logprob": -0.24740942024890286, "compression_ratio": 1.5021645021645023, "no_speech_prob": 8.267380508186761e-06}, {"id": 934, "seek": 277796, "start": 2793.76, "end": 2794.76, "text": " As mix up.", "tokens": [1018, 2890, 493, 13], "temperature": 0.0, "avg_logprob": -0.24740942024890286, "compression_ratio": 1.5021645021645023, "no_speech_prob": 8.267380508186761e-06}, {"id": 935, "seek": 277796, "start": 2797.76, "end": 2803.76, "text": " Here's something we haven't written save the model after every epoch maybe when I write that one that would be a good exercise.", "tokens": [1692, 311, 746, 321, 2378, 380, 3720, 3155, 264, 2316, 934, 633, 30992, 339, 1310, 562, 286, 2464, 300, 472, 300, 576, 312, 257, 665, 5380, 13], "temperature": 0.0, "avg_logprob": -0.24740942024890286, "compression_ratio": 1.5021645021645023, "no_speech_prob": 8.267380508186761e-06}, {"id": 936, "seek": 280376, "start": 2803.76, "end": 2808.5600000000004, "text": " So what happens if we try to train this for just 60 epochs.", "tokens": [407, 437, 2314, 498, 321, 853, 281, 3847, 341, 337, 445, 4060, 30992, 28346, 13], "temperature": 0.0, "avg_logprob": -0.20064611320036002, "compression_ratio": 1.579185520361991, "no_speech_prob": 9.223087545251474e-06}, {"id": 937, "seek": 280376, "start": 2810.5600000000004, "end": 2824.36, "text": " This is what happens so benchmark results on image net these are all the Keras and pipe torch models it's very hard to compare them because they have different input sizes so we really should compare the ones with our input size which is 224 so a standard.", "tokens": [639, 307, 437, 2314, 370, 18927, 3542, 322, 3256, 2533, 613, 366, 439, 264, 591, 6985, 293, 11240, 27822, 5245, 309, 311, 588, 1152, 281, 6794, 552, 570, 436, 362, 819, 4846, 11602, 370, 321, 534, 820, 6794, 264, 2306, 365, 527, 4846, 2744, 597, 307, 5853, 19, 370, 257, 3832, 13], "temperature": 0.0, "avg_logprob": -0.20064611320036002, "compression_ratio": 1.579185520361991, "no_speech_prob": 9.223087545251474e-06}, {"id": 938, "seek": 280376, "start": 2825.76, "end": 2828.1600000000003, "text": " Resnet oh scroll off the screen.", "tokens": [5015, 7129, 1954, 11369, 766, 264, 2568, 13], "temperature": 0.0, "avg_logprob": -0.20064611320036002, "compression_ratio": 1.579185520361991, "no_speech_prob": 9.223087545251474e-06}, {"id": 939, "seek": 282816, "start": 2828.16, "end": 2845.3599999999997, "text": " So Resnet 50 so bad it's actually scrolled off the screen so let's take Resnet 101 as a 93.3% accuracy so that's twice as many layers as we used and it was also trained for 90 epochs so trained for 50% longer 93.3 when I train this on image net I got 94.1.", "tokens": [407, 5015, 7129, 2625, 370, 1578, 309, 311, 767, 11369, 292, 766, 264, 2568, 370, 718, 311, 747, 5015, 7129, 21055, 382, 257, 28876, 13, 18, 4, 14170, 370, 300, 311, 6091, 382, 867, 7914, 382, 321, 1143, 293, 309, 390, 611, 8895, 337, 4289, 30992, 28346, 370, 8895, 337, 2625, 4, 2854, 28876, 13, 18, 562, 286, 3847, 341, 322, 3256, 2533, 286, 658, 30849, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.16096457952185522, "compression_ratio": 1.5, "no_speech_prob": 2.1443420337163843e-05}, {"id": 940, "seek": 282816, "start": 2846.3599999999997, "end": 2848.16, "text": " So this like.", "tokens": [407, 341, 411, 13], "temperature": 0.0, "avg_logprob": -0.16096457952185522, "compression_ratio": 1.5, "no_speech_prob": 2.1443420337163843e-05}, {"id": 941, "seek": 284816, "start": 2848.16, "end": 2859.16, "text": " Extremely simple architecture that fits on a single screen and was built entirely using common sense trained for just 60 epochs actually gets us even above Resnet 152.", "tokens": [24921, 736, 2199, 9482, 300, 9001, 322, 257, 2167, 2568, 293, 390, 3094, 7696, 1228, 2689, 2020, 8895, 337, 445, 4060, 30992, 28346, 767, 2170, 505, 754, 3673, 5015, 7129, 2119, 17, 13], "temperature": 0.0, "avg_logprob": -0.23596761366900276, "compression_ratio": 1.5172413793103448, "no_speech_prob": 3.1380106975120725e-06}, {"id": 942, "seek": 284816, "start": 2859.7599999999998, "end": 2872.96, "text": " Because that's 93.8 we've got 94.1 so the only things above it were trained on much much larger images and also like nasnet large is so big I can't train it I just keep on training it.", "tokens": [1436, 300, 311, 28876, 13, 23, 321, 600, 658, 30849, 13, 16, 370, 264, 787, 721, 3673, 309, 645, 8895, 322, 709, 709, 4833, 5267, 293, 611, 411, 5382, 7129, 2416, 307, 370, 955, 286, 393, 380, 3847, 309, 286, 445, 1066, 322, 3097, 309, 13], "temperature": 0.0, "avg_logprob": -0.23596761366900276, "compression_ratio": 1.5172413793103448, "no_speech_prob": 3.1380106975120725e-06}, {"id": 943, "seek": 287296, "start": 2872.96, "end": 2890.76, "text": " And also like nasnet large is so big I can't train it I just keep on running out of memory and time and inception Resnet version 2 is really really fiddly and also really really slow so we've now got you know this beautiful nice Resnet extra is that 50 model which.", "tokens": [400, 611, 411, 5382, 7129, 2416, 307, 370, 955, 286, 393, 380, 3847, 309, 286, 445, 1066, 322, 2614, 484, 295, 4675, 293, 565, 293, 49834, 5015, 7129, 3037, 568, 307, 534, 534, 283, 14273, 356, 293, 611, 534, 534, 2964, 370, 321, 600, 586, 658, 291, 458, 341, 2238, 1481, 5015, 7129, 2857, 307, 300, 2625, 2316, 597, 13], "temperature": 0.0, "avg_logprob": -0.13841172923212466, "compression_ratio": 1.6434426229508197, "no_speech_prob": 7.409830686810892e-06}, {"id": 944, "seek": 289076, "start": 2890.76, "end": 2901.36, "text": " You know is built in this very first principles common sense way and gets astonishingly great results so you know I really don't think.", "tokens": [509, 458, 307, 3094, 294, 341, 588, 700, 9156, 2689, 2020, 636, 293, 2170, 35264, 356, 869, 3542, 370, 291, 458, 286, 534, 500, 380, 519, 13], "temperature": 0.0, "avg_logprob": -0.12513011541122046, "compression_ratio": 1.6018099547511313, "no_speech_prob": 3.089152642132831e-06}, {"id": 945, "seek": 289076, "start": 2902.36, "end": 2912.36, "text": " We all need to be running to neural architecture search and hyper parameter optimization and blah blah blah we just need to use you know good common sense thinking.", "tokens": [492, 439, 643, 281, 312, 2614, 281, 18161, 9482, 3164, 293, 9848, 13075, 19618, 293, 12288, 12288, 12288, 321, 445, 643, 281, 764, 291, 458, 665, 2689, 2020, 1953, 13], "temperature": 0.0, "avg_logprob": -0.12513011541122046, "compression_ratio": 1.6018099547511313, "no_speech_prob": 3.089152642132831e-06}, {"id": 946, "seek": 289076, "start": 2913.36, "end": 2918.36, "text": " So I'm super excited to see how well that worked out.", "tokens": [407, 286, 478, 1687, 2919, 281, 536, 577, 731, 300, 2732, 484, 13], "temperature": 0.0, "avg_logprob": -0.12513011541122046, "compression_ratio": 1.6018099547511313, "no_speech_prob": 3.089152642132831e-06}, {"id": 947, "seek": 291836, "start": 2918.36, "end": 2928.36, "text": " So now that we have a nice model we want to be able to do transfer learning so how do we do transfer learning I mean you all know how to do transfer learning but let's do it from scratch.", "tokens": [407, 586, 300, 321, 362, 257, 1481, 2316, 321, 528, 281, 312, 1075, 281, 360, 5003, 2539, 370, 577, 360, 321, 360, 5003, 2539, 286, 914, 291, 439, 458, 577, 281, 360, 5003, 2539, 457, 718, 311, 360, 309, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.13261671190137986, "compression_ratio": 1.8058823529411765, "no_speech_prob": 7.888591426308267e-06}, {"id": 948, "seek": 291836, "start": 2929.36, "end": 2936.96, "text": " So what I'm going to do is I'm going to transfer learn from image wolf to the pets data set that we used in lesson one.", "tokens": [407, 437, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 5003, 1466, 490, 3256, 19216, 281, 264, 19897, 1412, 992, 300, 321, 1143, 294, 6898, 472, 13], "temperature": 0.0, "avg_logprob": -0.13261671190137986, "compression_ratio": 1.8058823529411765, "no_speech_prob": 7.888591426308267e-06}, {"id": 949, "seek": 293696, "start": 2936.96, "end": 2947.56, "text": " That's our goal so we start by grabbing image wolf we do the standard data block stuff.", "tokens": [663, 311, 527, 3387, 370, 321, 722, 538, 23771, 3256, 19216, 321, 360, 264, 3832, 1412, 3461, 1507, 13], "temperature": 0.0, "avg_logprob": -0.21296194123058784, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.0614464372338261e-05}, {"id": 950, "seek": 293696, "start": 2948.96, "end": 2960.76, "text": " Let's use labels moving cross entropy notice how we're using all the stuff we've built this is our Adam optimizer this is our label smoothing cross entropy this is the data blocks API we wrote so this is that we're still not using anything from faster I V1.", "tokens": [961, 311, 764, 16949, 2684, 3278, 30867, 3449, 577, 321, 434, 1228, 439, 264, 1507, 321, 600, 3094, 341, 307, 527, 7938, 5028, 6545, 341, 307, 527, 7645, 899, 6259, 571, 3278, 30867, 341, 307, 264, 1412, 8474, 9362, 321, 4114, 370, 341, 307, 300, 321, 434, 920, 406, 1228, 1340, 490, 4663, 286, 691, 16, 13], "temperature": 0.0, "avg_logprob": -0.21296194123058784, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.0614464372338261e-05}, {"id": 951, "seek": 296076, "start": 2960.76, "end": 2972.96, "text": " This is all stuff that if you want to know what's going on you can go back to that previous lesson and see what did we build and how did we build it and step through the code there's a CNN learner that we just built in the last notebook.", "tokens": [639, 307, 439, 1507, 300, 498, 291, 528, 281, 458, 437, 311, 516, 322, 291, 393, 352, 646, 281, 300, 3894, 6898, 293, 536, 437, 630, 321, 1322, 293, 577, 630, 321, 1322, 309, 293, 1823, 807, 264, 3089, 456, 311, 257, 24859, 33347, 300, 321, 445, 3094, 294, 264, 1036, 21060, 13], "temperature": 0.0, "avg_logprob": -0.1367144849565294, "compression_ratio": 1.787313432835821, "no_speech_prob": 1.0782436220324598e-05}, {"id": 952, "seek": 296076, "start": 2974.76, "end": 2988.76, "text": " These five lines of code I got sick of typing so let's dump them into a single function called schedule one cycle going to create our phases it's going to create our momentum annealing and a learning rate annealing and create our schedulers.", "tokens": [1981, 1732, 3876, 295, 3089, 286, 658, 4998, 295, 18444, 370, 718, 311, 11430, 552, 666, 257, 2167, 2445, 1219, 7567, 472, 6586, 516, 281, 1884, 527, 18764, 309, 311, 516, 281, 1884, 527, 11244, 22256, 4270, 293, 257, 2539, 3314, 22256, 4270, 293, 1884, 527, 12000, 433, 13], "temperature": 0.0, "avg_logprob": -0.1367144849565294, "compression_ratio": 1.787313432835821, "no_speech_prob": 1.0782436220324598e-05}, {"id": 953, "seek": 298876, "start": 2988.76, "end": 3001.36, "text": " So now with that we can just say schedule one cycle with a learning rate what percentage of the epochs are at the start batches I should say at the start and we can go ahead and fit so I thought okay the transfer learning we should try and fit a decent model.", "tokens": [407, 586, 365, 300, 321, 393, 445, 584, 7567, 472, 6586, 365, 257, 2539, 3314, 437, 9668, 295, 264, 30992, 28346, 366, 412, 264, 722, 15245, 279, 286, 820, 584, 412, 264, 722, 293, 321, 393, 352, 2286, 293, 3318, 370, 286, 1194, 1392, 264, 5003, 2539, 321, 820, 853, 293, 3318, 257, 8681, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1825346379053025, "compression_ratio": 1.6298076923076923, "no_speech_prob": 1.1478132364572957e-05}, {"id": 954, "seek": 298876, "start": 3002.1600000000003, "end": 3009.1600000000003, "text": " So I did 40 epochs at 11 seconds per epoch on a 1080 TI so a few minutes later.", "tokens": [407, 286, 630, 3356, 30992, 28346, 412, 2975, 3949, 680, 30992, 339, 322, 257, 24547, 28819, 370, 257, 1326, 2077, 1780, 13], "temperature": 0.0, "avg_logprob": -0.1825346379053025, "compression_ratio": 1.6298076923076923, "no_speech_prob": 1.1478132364572957e-05}, {"id": 955, "seek": 300916, "start": 3009.16, "end": 3021.7599999999998, "text": " So a few minutes later we've got 79.6% accuracy which is pretty good you know training from scratch for 10 different dog breeds with a resident 18.", "tokens": [407, 257, 1326, 2077, 1780, 321, 600, 658, 32803, 13, 21, 4, 14170, 597, 307, 1238, 665, 291, 458, 3097, 490, 8459, 337, 1266, 819, 3000, 41609, 365, 257, 10832, 2443, 13], "temperature": 0.0, "avg_logprob": -0.16225737791794997, "compression_ratio": 1.3116883116883118, "no_speech_prob": 4.710444045485929e-06}, {"id": 956, "seek": 300916, "start": 3022.7599999999998, "end": 3027.3599999999997, "text": " So let's try and use this to create a good pets model.", "tokens": [407, 718, 311, 853, 293, 764, 341, 281, 1884, 257, 665, 19897, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16225737791794997, "compression_ratio": 1.3116883116883118, "no_speech_prob": 4.710444045485929e-06}, {"id": 957, "seek": 302736, "start": 3027.36, "end": 3044.36, "text": " It's going to be a little bit tricky because the pets data set has cats as well and this model's never seen cats and also this model has only been trained on I think less than 10,000 images so it's kind of unusually small thing that we're trying to do here so it's an interesting experiment to see if this works.", "tokens": [467, 311, 516, 281, 312, 257, 707, 857, 12414, 570, 264, 19897, 1412, 992, 575, 11111, 382, 731, 293, 341, 2316, 311, 1128, 1612, 11111, 293, 611, 341, 2316, 575, 787, 668, 8895, 322, 286, 519, 1570, 813, 1266, 11, 1360, 5267, 370, 309, 311, 733, 295, 10054, 671, 1359, 551, 300, 321, 434, 1382, 281, 360, 510, 370, 309, 311, 364, 1880, 5120, 281, 536, 498, 341, 1985, 13], "temperature": 0.0, "avg_logprob": -0.10864983664618598, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.1658660696411971e-05}, {"id": 958, "seek": 302736, "start": 3045.36, "end": 3051.96, "text": " So the first thing we have to do is we have to save the model so that we can load it into a pets model so when we save a model.", "tokens": [407, 264, 700, 551, 321, 362, 281, 360, 307, 321, 362, 281, 3155, 264, 2316, 370, 300, 321, 393, 3677, 309, 666, 257, 19897, 2316, 370, 562, 321, 3155, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10864983664618598, "compression_ratio": 1.752988047808765, "no_speech_prob": 1.1658660696411971e-05}, {"id": 959, "seek": 305196, "start": 3051.96, "end": 3072.96, "text": " What we do is we grab its state dict now we actually haven't written this but it would be like three lines of code if you want to write it yourself because all it does is it literally creates a dictionary and order dict is just a Python standard library dictionary that has an order where the keys are just the names of all the layers and for sequential the index of each one.", "tokens": [708, 321, 360, 307, 321, 4444, 1080, 1785, 12569, 586, 321, 767, 2378, 380, 3720, 341, 457, 309, 576, 312, 411, 1045, 3876, 295, 3089, 498, 291, 528, 281, 2464, 309, 1803, 570, 439, 309, 775, 307, 309, 3736, 7829, 257, 25890, 293, 1668, 12569, 307, 445, 257, 15329, 3832, 6405, 25890, 300, 575, 364, 1668, 689, 264, 9317, 366, 445, 264, 5288, 295, 439, 264, 7914, 293, 337, 42881, 264, 8186, 295, 1184, 472, 13], "temperature": 0.0, "avg_logprob": -0.1412377953529358, "compression_ratio": 1.6860986547085202, "no_speech_prob": 1.3418501112028025e-05}, {"id": 960, "seek": 307296, "start": 3072.96, "end": 3090.96, "text": " And then you can look up say 10 dot bias and it just returns the weights okay so you can easily turn a module into a dictionary and so then we can create somewhere to save a model and torch dot save will save that dictionary you can actually just use pickle here.", "tokens": [400, 550, 291, 393, 574, 493, 584, 1266, 5893, 12577, 293, 309, 445, 11247, 264, 17443, 1392, 370, 291, 393, 3612, 1261, 257, 10088, 666, 257, 25890, 293, 370, 550, 321, 393, 1884, 4079, 281, 3155, 257, 2316, 293, 27822, 5893, 3155, 486, 3155, 300, 25890, 291, 393, 767, 445, 764, 31433, 510, 13], "temperature": 0.0, "avg_logprob": -0.20949537777206273, "compression_ratio": 1.8774703557312253, "no_speech_prob": 2.282552122778725e-05}, {"id": 961, "seek": 309096, "start": 3090.96, "end": 3106.96, "text": " Works fine and actually behind the scenes torch dot save is using pickle but they kind of like add some header to it to say like it's basically a magic number that when they read it back they make sure it is a pie torch model file and that it's the right version and stuff like that but you can totally use pickle.", "tokens": [27914, 2489, 293, 767, 2261, 264, 8026, 27822, 5893, 3155, 307, 1228, 31433, 457, 436, 733, 295, 411, 909, 512, 23117, 281, 309, 281, 584, 411, 309, 311, 1936, 257, 5585, 1230, 300, 562, 436, 1401, 309, 646, 436, 652, 988, 309, 307, 257, 1730, 27822, 2316, 3991, 293, 300, 309, 311, 264, 558, 3037, 293, 1507, 411, 300, 457, 291, 393, 3879, 764, 31433, 13], "temperature": 0.0, "avg_logprob": -0.10189421517508371, "compression_ratio": 1.6526315789473685, "no_speech_prob": 1.6962656445684843e-05}, {"id": 962, "seek": 310696, "start": 3106.96, "end": 3122.96, "text": " And so the nice thing is now that we know that the thing we've saved is just a dictionary so you can fiddle with it right if you have trouble loading something in the future just open up just go torch dot load put it into a dictionary and look at the keys and look at the values and see what's going on.", "tokens": [400, 370, 264, 1481, 551, 307, 586, 300, 321, 458, 300, 264, 551, 321, 600, 6624, 307, 445, 257, 25890, 370, 291, 393, 24553, 2285, 365, 309, 558, 498, 291, 362, 5253, 15114, 746, 294, 264, 2027, 445, 1269, 493, 445, 352, 27822, 5893, 3677, 829, 309, 666, 257, 25890, 293, 574, 412, 264, 9317, 293, 574, 412, 264, 4190, 293, 536, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.10681356510645906, "compression_ratio": 1.7215909090909092, "no_speech_prob": 1.5444538803421892e-05}, {"id": 963, "seek": 312296, "start": 3122.96, "end": 3135.96, "text": " So let's try and use this for pets so we've seen pets before so the nice thing is that we've never used pets in part 2 but our data blocks API totally works.", "tokens": [407, 718, 311, 853, 293, 764, 341, 337, 19897, 370, 321, 600, 1612, 19897, 949, 370, 264, 1481, 551, 307, 300, 321, 600, 1128, 1143, 19897, 294, 644, 568, 457, 527, 1412, 8474, 9362, 3879, 1985, 13], "temperature": 0.0, "avg_logprob": -0.0952518393353718, "compression_ratio": 1.3083333333333333, "no_speech_prob": 2.668567503860686e-05}, {"id": 964, "seek": 313596, "start": 3135.96, "end": 3151.96, "text": " And in this case the there's one images directory that contains all the images and there isn't a separate validation set directory so we can't use that label with sorry yeah label with so split with grandparent thing so we're going to have to split it randomly but remember how we've already created split by funk so let's just write a function that returns.", "tokens": [400, 294, 341, 1389, 264, 456, 311, 472, 5267, 21120, 300, 8306, 439, 264, 5267, 293, 456, 1943, 380, 257, 4994, 24071, 992, 21120, 370, 321, 393, 380, 764, 300, 7645, 365, 2597, 1338, 7645, 365, 370, 7472, 365, 2697, 38321, 551, 370, 321, 434, 516, 281, 362, 281, 7472, 309, 16979, 457, 1604, 577, 321, 600, 1217, 2942, 7472, 538, 26476, 370, 718, 311, 445, 2464, 257, 2445, 300, 11247, 13], "temperature": 0.0, "avg_logprob": -0.18404918763695693, "compression_ratio": 1.6954545454545455, "no_speech_prob": 1.5934911061776802e-05}, {"id": 965, "seek": 313596, "start": 3153.96, "end": 3154.96, "text": " True or false.", "tokens": [13587, 420, 7908, 13], "temperature": 0.0, "avg_logprob": -0.18404918763695693, "compression_ratio": 1.6954545454545455, "no_speech_prob": 1.5934911061776802e-05}, {"id": 966, "seek": 315496, "start": 3154.96, "end": 3164.96, "text": " Depending on whether some random number is.", "tokens": [22539, 322, 1968, 512, 4974, 1230, 307, 13], "temperature": 0.0, "avg_logprob": -0.30234167792580346, "compression_ratio": 1.1891891891891893, "no_speech_prob": 2.977042277052533e-05}, {"id": 967, "seek": 315496, "start": 3165.96, "end": 3166.96, "text": " Large or small.", "tokens": [33092, 420, 1359, 13], "temperature": 0.0, "avg_logprob": -0.30234167792580346, "compression_ratio": 1.1891891891891893, "no_speech_prob": 2.977042277052533e-05}, {"id": 968, "seek": 315496, "start": 3167.96, "end": 3168.96, "text": " And so now.", "tokens": [400, 370, 586, 13], "temperature": 0.0, "avg_logprob": -0.30234167792580346, "compression_ratio": 1.1891891891891893, "no_speech_prob": 2.977042277052533e-05}, {"id": 969, "seek": 315496, "start": 3169.96, "end": 3172.96, "text": " We can just pass that to your split by funk and.", "tokens": [492, 393, 445, 1320, 300, 281, 428, 7472, 538, 26476, 293, 13], "temperature": 0.0, "avg_logprob": -0.30234167792580346, "compression_ratio": 1.1891891891891893, "no_speech_prob": 2.977042277052533e-05}, {"id": 970, "seek": 315496, "start": 3173.96, "end": 3174.96, "text": " We're done.", "tokens": [492, 434, 1096, 13], "temperature": 0.0, "avg_logprob": -0.30234167792580346, "compression_ratio": 1.1891891891891893, "no_speech_prob": 2.977042277052533e-05}, {"id": 971, "seek": 317496, "start": 3174.96, "end": 3190.96, "text": " And we're done. So the nice thing is when you kind of understand the what's going on behind the scenes it's super easy for you to customize things and fast AIV one is basically identical there's a split by funk that you do the same thing for.", "tokens": [400, 321, 434, 1096, 13, 407, 264, 1481, 551, 307, 562, 291, 733, 295, 1223, 264, 437, 311, 516, 322, 2261, 264, 8026, 309, 311, 1687, 1858, 337, 291, 281, 19734, 721, 293, 2370, 7318, 53, 472, 307, 1936, 14800, 456, 311, 257, 7472, 538, 26476, 300, 291, 360, 264, 912, 551, 337, 13], "temperature": 0.0, "avg_logprob": -0.13492993340975995, "compression_ratio": 1.5591397849462365, "no_speech_prob": 1.2028468518110458e-05}, {"id": 972, "seek": 317496, "start": 3192.96, "end": 3196.96, "text": " So now that split into training and validation.", "tokens": [407, 586, 300, 7472, 666, 3097, 293, 24071, 13], "temperature": 0.0, "avg_logprob": -0.13492993340975995, "compression_ratio": 1.5591397849462365, "no_speech_prob": 1.2028468518110458e-05}, {"id": 973, "seek": 319696, "start": 3196.96, "end": 3209.96, "text": " And you can see how nice it is that we created that Dundas repress so that we can print things out so easily to see what's going on so that's if something doesn't have a nice representation you should like monkey patch in a Dundas repress so you can print out what's going on.", "tokens": [400, 291, 393, 536, 577, 1481, 309, 307, 300, 321, 2942, 300, 413, 997, 296, 1085, 735, 370, 300, 321, 393, 4482, 721, 484, 370, 3612, 281, 536, 437, 311, 516, 322, 370, 300, 311, 498, 746, 1177, 380, 362, 257, 1481, 10290, 291, 820, 411, 17847, 9972, 294, 257, 413, 997, 296, 1085, 735, 370, 291, 393, 4482, 484, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.10182750926298254, "compression_ratio": 1.8939929328621907, "no_speech_prob": 1.8341841496294364e-05}, {"id": 974, "seek": 319696, "start": 3210.96, "end": 3217.96, "text": " Now we have to label it so we can't label it by folder because they're not put into folders instead we have to look at the file name.", "tokens": [823, 321, 362, 281, 7645, 309, 370, 321, 393, 380, 7645, 309, 538, 10820, 570, 436, 434, 406, 829, 666, 31082, 2602, 321, 362, 281, 574, 412, 264, 3991, 1315, 13], "temperature": 0.0, "avg_logprob": -0.10182750926298254, "compression_ratio": 1.8939929328621907, "no_speech_prob": 1.8341841496294364e-05}, {"id": 975, "seek": 319696, "start": 3218.96, "end": 3224.96, "text": " So let's grab one file name so I had to build all this stuff in a jupyter notebook just interactively to see what's going on.", "tokens": [407, 718, 311, 4444, 472, 3991, 1315, 370, 286, 632, 281, 1322, 439, 341, 1507, 294, 257, 361, 1010, 88, 391, 21060, 445, 4648, 3413, 281, 536, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.10182750926298254, "compression_ratio": 1.8939929328621907, "no_speech_prob": 1.8341841496294364e-05}, {"id": 976, "seek": 322496, "start": 3224.96, "end": 3245.96, "text": " So in this case we'll grab one name and then let's try to construct a regular expression that grabs just the doggy's name from that and once we've got it we can now turn that into a function and we can now go ahead and use that category processor we built last week to label it and there we go.", "tokens": [407, 294, 341, 1389, 321, 603, 4444, 472, 1315, 293, 550, 718, 311, 853, 281, 7690, 257, 3890, 6114, 300, 30028, 445, 264, 3000, 1480, 311, 1315, 490, 300, 293, 1564, 321, 600, 658, 309, 321, 393, 586, 1261, 300, 666, 257, 2445, 293, 321, 393, 586, 352, 2286, 293, 764, 300, 7719, 15321, 321, 3094, 1036, 1243, 281, 7645, 309, 293, 456, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.16011513040420858, "compression_ratio": 1.7045454545454546, "no_speech_prob": 3.882301098201424e-05}, {"id": 977, "seek": 322496, "start": 3246.96, "end": 3250.96, "text": " There's all the kinds of doggy we have not just doggies now doggies and kitties.", "tokens": [821, 311, 439, 264, 3685, 295, 3000, 1480, 321, 362, 406, 445, 3000, 25480, 586, 3000, 25480, 293, 350, 593, 530, 13], "temperature": 0.0, "avg_logprob": -0.16011513040420858, "compression_ratio": 1.7045454545454546, "no_speech_prob": 3.882301098201424e-05}, {"id": 978, "seek": 325096, "start": 3250.96, "end": 3259.96, "text": " Okay so now we can train from scratch pets 37% not great.", "tokens": [1033, 370, 586, 321, 393, 3847, 490, 8459, 19897, 13435, 4, 406, 869, 13], "temperature": 0.0, "avg_logprob": -0.1667637735042932, "compression_ratio": 1.474820143884892, "no_speech_prob": 2.3921886167954654e-05}, {"id": 979, "seek": 325096, "start": 3260.96, "end": 3262.96, "text": " So maybe with transfer learning we can do better.", "tokens": [407, 1310, 365, 5003, 2539, 321, 393, 360, 1101, 13], "temperature": 0.0, "avg_logprob": -0.1667637735042932, "compression_ratio": 1.474820143884892, "no_speech_prob": 2.3921886167954654e-05}, {"id": 980, "seek": 325096, "start": 3264.96, "end": 3275.96, "text": " So transfer learning we can read in that image woof model and then we will customize it for pets.", "tokens": [407, 5003, 2539, 321, 393, 1401, 294, 300, 3256, 21657, 69, 2316, 293, 550, 321, 486, 19734, 309, 337, 19897, 13], "temperature": 0.0, "avg_logprob": -0.1667637735042932, "compression_ratio": 1.474820143884892, "no_speech_prob": 2.3921886167954654e-05}, {"id": 981, "seek": 327596, "start": 3275.96, "end": 3282.96, "text": " So let's create a CNN for pets this is now the pets data bunch.", "tokens": [407, 718, 311, 1884, 257, 24859, 337, 19897, 341, 307, 586, 264, 19897, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.12034177176560028, "compression_ratio": 1.5706806282722514, "no_speech_prob": 6.962105544516817e-06}, {"id": 982, "seek": 327596, "start": 3283.96, "end": 3295.96, "text": " But let's tell it to create a model with 10 filters out 10 activations at the end because remember image woof has 10 types of dog 10 breeds.", "tokens": [583, 718, 311, 980, 309, 281, 1884, 257, 2316, 365, 1266, 15995, 484, 1266, 2430, 763, 412, 264, 917, 570, 1604, 3256, 21657, 69, 575, 1266, 3467, 295, 3000, 1266, 41609, 13], "temperature": 0.0, "avg_logprob": -0.12034177176560028, "compression_ratio": 1.5706806282722514, "no_speech_prob": 6.962105544516817e-06}, {"id": 983, "seek": 327596, "start": 3296.96, "end": 3301.96, "text": " So to load in the pre train model we're going to need to ask for a learner with 10 activations.", "tokens": [407, 281, 3677, 294, 264, 659, 3847, 2316, 321, 434, 516, 281, 643, 281, 1029, 337, 257, 33347, 365, 1266, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.12034177176560028, "compression_ratio": 1.5706806282722514, "no_speech_prob": 6.962105544516817e-06}, {"id": 984, "seek": 330196, "start": 3301.96, "end": 3310.96, "text": " So that is something we can now grab our state dictionary that we saved earlier and we can load it into our model.", "tokens": [407, 300, 307, 746, 321, 393, 586, 4444, 527, 1785, 25890, 300, 321, 6624, 3071, 293, 321, 393, 3677, 309, 666, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10471283472501315, "compression_ratio": 1.5182481751824817, "no_speech_prob": 1.706148054836376e-06}, {"id": 985, "seek": 330196, "start": 3311.96, "end": 3321.96, "text": " So this is now an image woof model but the learner for it is pointing at the pets data bunch.", "tokens": [407, 341, 307, 586, 364, 3256, 21657, 69, 2316, 457, 264, 33347, 337, 309, 307, 12166, 412, 264, 19897, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.10471283472501315, "compression_ratio": 1.5182481751824817, "no_speech_prob": 1.706148054836376e-06}, {"id": 986, "seek": 332196, "start": 3321.96, "end": 3335.96, "text": " So what we now have to do is remove the final linear layer and replace it with one that has the right number of activations to handle all these which I think is 37 pet breeds.", "tokens": [407, 437, 321, 586, 362, 281, 360, 307, 4159, 264, 2572, 8213, 4583, 293, 7406, 309, 365, 472, 300, 575, 264, 558, 1230, 295, 2430, 763, 281, 4813, 439, 613, 597, 286, 519, 307, 13435, 3817, 41609, 13], "temperature": 0.0, "avg_logprob": -0.058221674513542786, "compression_ratio": 1.6383928571428572, "no_speech_prob": 1.9032989939660183e-06}, {"id": 987, "seek": 332196, "start": 3337.96, "end": 3347.96, "text": " So what we do is we look through all the children of the model and we try to find the adaptive average pooling layer because that's that kind of penultimate bit and we grab the index of that.", "tokens": [407, 437, 321, 360, 307, 321, 574, 807, 439, 264, 2227, 295, 264, 2316, 293, 321, 853, 281, 915, 264, 27912, 4274, 7005, 278, 4583, 570, 300, 311, 300, 733, 295, 3435, 723, 2905, 857, 293, 321, 4444, 264, 8186, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.058221674513542786, "compression_ratio": 1.6383928571428572, "no_speech_prob": 1.9032989939660183e-06}, {"id": 988, "seek": 334796, "start": 3347.96, "end": 3353.96, "text": " And then let's create a new model that has everything up to but not including that bit.", "tokens": [400, 550, 718, 311, 1884, 257, 777, 2316, 300, 575, 1203, 493, 281, 457, 406, 3009, 300, 857, 13], "temperature": 0.0, "avg_logprob": -0.06671208948702426, "compression_ratio": 1.5966850828729282, "no_speech_prob": 1.3630926332552917e-05}, {"id": 989, "seek": 334796, "start": 3354.96, "end": 3356.96, "text": " So this is everything before the adaptive average pooling.", "tokens": [407, 341, 307, 1203, 949, 264, 27912, 4274, 7005, 278, 13], "temperature": 0.0, "avg_logprob": -0.06671208948702426, "compression_ratio": 1.5966850828729282, "no_speech_prob": 1.3630926332552917e-05}, {"id": 990, "seek": 334796, "start": 3357.96, "end": 3358.96, "text": " So this is the body.", "tokens": [407, 341, 307, 264, 1772, 13], "temperature": 0.0, "avg_logprob": -0.06671208948702426, "compression_ratio": 1.5966850828729282, "no_speech_prob": 1.3630926332552917e-05}, {"id": 991, "seek": 334796, "start": 3360.96, "end": 3368.96, "text": " So now we need to attach a new head to this body which is going to have 37 activations in the linear layer instead of 10.", "tokens": [407, 586, 321, 643, 281, 5085, 257, 777, 1378, 281, 341, 1772, 597, 307, 516, 281, 362, 13435, 2430, 763, 294, 264, 8213, 4583, 2602, 295, 1266, 13], "temperature": 0.0, "avg_logprob": -0.06671208948702426, "compression_ratio": 1.5966850828729282, "no_speech_prob": 1.3630926332552917e-05}, {"id": 992, "seek": 336896, "start": 3368.96, "end": 3376.96, "text": " Which is a bit tricky because we need to know how many inputs are going to be required in this new linear layer.", "tokens": [3013, 307, 257, 857, 12414, 570, 321, 643, 281, 458, 577, 867, 15743, 366, 516, 281, 312, 4739, 294, 341, 777, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.13136035919189454, "compression_ratio": 1.5174129353233832, "no_speech_prob": 7.646121957805008e-06}, {"id": 993, "seek": 336896, "start": 3377.96, "end": 3381.96, "text": " The number of inputs will be however many outputs come out of this.", "tokens": [440, 1230, 295, 15743, 486, 312, 4461, 867, 23930, 808, 484, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.13136035919189454, "compression_ratio": 1.5174129353233832, "no_speech_prob": 7.646121957805008e-06}, {"id": 994, "seek": 336896, "start": 3382.96, "end": 3392.96, "text": " So in other words just before the average pooling happens in the X ResNet how many activations are there?", "tokens": [407, 294, 661, 2283, 445, 949, 264, 4274, 7005, 278, 2314, 294, 264, 1783, 5015, 31890, 577, 867, 2430, 763, 366, 456, 30], "temperature": 0.0, "avg_logprob": -0.13136035919189454, "compression_ratio": 1.5174129353233832, "no_speech_prob": 7.646121957805008e-06}, {"id": 995, "seek": 336896, "start": 3393.96, "end": 3394.96, "text": " How many channels?", "tokens": [1012, 867, 9235, 30], "temperature": 0.0, "avg_logprob": -0.13136035919189454, "compression_ratio": 1.5174129353233832, "no_speech_prob": 7.646121957805008e-06}, {"id": 996, "seek": 339496, "start": 3394.96, "end": 3397.96, "text": " How many channels? Well there's an easy way to find out.", "tokens": [1012, 867, 9235, 30, 1042, 456, 311, 364, 1858, 636, 281, 915, 484, 13], "temperature": 0.0, "avg_logprob": -0.15020860283120166, "compression_ratio": 1.4727272727272727, "no_speech_prob": 7.296160219993908e-06}, {"id": 997, "seek": 339496, "start": 3398.96, "end": 3399.96, "text": " Grab a batch of data.", "tokens": [20357, 257, 15245, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15020860283120166, "compression_ratio": 1.4727272727272727, "no_speech_prob": 7.296160219993908e-06}, {"id": 998, "seek": 339496, "start": 3400.96, "end": 3404.96, "text": " Put it through a cut down model and look at the shape.", "tokens": [4935, 309, 807, 257, 1723, 760, 2316, 293, 574, 412, 264, 3909, 13], "temperature": 0.0, "avg_logprob": -0.15020860283120166, "compression_ratio": 1.4727272727272727, "no_speech_prob": 7.296160219993908e-06}, {"id": 999, "seek": 339496, "start": 3405.96, "end": 3407.96, "text": " And the answer is there's 512.", "tokens": [400, 264, 1867, 307, 456, 311, 1025, 4762, 13], "temperature": 0.0, "avg_logprob": -0.15020860283120166, "compression_ratio": 1.4727272727272727, "no_speech_prob": 7.296160219993908e-06}, {"id": 1000, "seek": 339496, "start": 3408.96, "end": 3408.96, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.15020860283120166, "compression_ratio": 1.4727272727272727, "no_speech_prob": 7.296160219993908e-06}, {"id": 1001, "seek": 339496, "start": 3409.96, "end": 3414.96, "text": " So we've got a 128 mini batch of 512 4 by 4 activations.", "tokens": [407, 321, 600, 658, 257, 29810, 8382, 15245, 295, 1025, 4762, 1017, 538, 1017, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.15020860283120166, "compression_ratio": 1.4727272727272727, "no_speech_prob": 7.296160219993908e-06}, {"id": 1002, "seek": 339496, "start": 3415.96, "end": 3418.96, "text": " So that pred dot shape one is the number of inputs to our head.", "tokens": [407, 300, 3852, 5893, 3909, 472, 307, 264, 1230, 295, 15743, 281, 527, 1378, 13], "temperature": 0.0, "avg_logprob": -0.15020860283120166, "compression_ratio": 1.4727272727272727, "no_speech_prob": 7.296160219993908e-06}, {"id": 1003, "seek": 339496, "start": 3419.96, "end": 3421.96, "text": " And so we can now create our head.", "tokens": [400, 370, 321, 393, 586, 1884, 527, 1378, 13], "temperature": 0.0, "avg_logprob": -0.15020860283120166, "compression_ratio": 1.4727272727272727, "no_speech_prob": 7.296160219993908e-06}, {"id": 1004, "seek": 342196, "start": 3421.96, "end": 3423.96, "text": " This is basically it here our linear layer.", "tokens": [639, 307, 1936, 309, 510, 527, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.10700855632819752, "compression_ratio": 1.613821138211382, "no_speech_prob": 9.223128472513054e-06}, {"id": 1005, "seek": 342196, "start": 3424.96, "end": 3430.96, "text": " But remember we tend to not just use a max pool or just an average pool.", "tokens": [583, 1604, 321, 3928, 281, 406, 445, 764, 257, 11469, 7005, 420, 445, 364, 4274, 7005, 13], "temperature": 0.0, "avg_logprob": -0.10700855632819752, "compression_ratio": 1.613821138211382, "no_speech_prob": 9.223128472513054e-06}, {"id": 1006, "seek": 342196, "start": 3431.96, "end": 3434.96, "text": " We tend to do both and concatenate them together.", "tokens": [492, 3928, 281, 360, 1293, 293, 1588, 7186, 473, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.10700855632819752, "compression_ratio": 1.613821138211382, "no_speech_prob": 9.223128472513054e-06}, {"id": 1007, "seek": 342196, "start": 3435.96, "end": 3438.96, "text": " Which is something we've been doing in this course forever.", "tokens": [3013, 307, 746, 321, 600, 668, 884, 294, 341, 1164, 5680, 13], "temperature": 0.0, "avg_logprob": -0.10700855632819752, "compression_ratio": 1.613821138211382, "no_speech_prob": 9.223128472513054e-06}, {"id": 1008, "seek": 342196, "start": 3439.96, "end": 3441.96, "text": " But a couple of years somebody finally did actually write a paper about it.", "tokens": [583, 257, 1916, 295, 924, 2618, 2721, 630, 767, 2464, 257, 3035, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.10700855632819752, "compression_ratio": 1.613821138211382, "no_speech_prob": 9.223128472513054e-06}, {"id": 1009, "seek": 342196, "start": 3442.96, "end": 3444.96, "text": " So I think this is actually an official thing now.", "tokens": [407, 286, 519, 341, 307, 767, 364, 4783, 551, 586, 13], "temperature": 0.0, "avg_logprob": -0.10700855632819752, "compression_ratio": 1.613821138211382, "no_speech_prob": 9.223128472513054e-06}, {"id": 1010, "seek": 342196, "start": 3445.96, "end": 3447.96, "text": " And it generally gives a nice little boost.", "tokens": [400, 309, 5101, 2709, 257, 1481, 707, 9194, 13], "temperature": 0.0, "avg_logprob": -0.10700855632819752, "compression_ratio": 1.613821138211382, "no_speech_prob": 9.223128472513054e-06}, {"id": 1011, "seek": 344796, "start": 3447.96, "end": 3452.96, "text": " So our linear layout needs twice as many inputs because we've got two sets of pooling we did.", "tokens": [407, 527, 8213, 13333, 2203, 6091, 382, 867, 15743, 570, 321, 600, 658, 732, 6352, 295, 7005, 278, 321, 630, 13], "temperature": 0.0, "avg_logprob": -0.10353209858848936, "compression_ratio": 1.6018518518518519, "no_speech_prob": 4.029255705972901e-06}, {"id": 1012, "seek": 344796, "start": 3453.96, "end": 3462.96, "text": " So our new model contains the whole head plus an adaptive concat pooling, flatten and our linear.", "tokens": [407, 527, 777, 2316, 8306, 264, 1379, 1378, 1804, 364, 27912, 1588, 267, 7005, 278, 11, 24183, 293, 527, 8213, 13], "temperature": 0.0, "avg_logprob": -0.10353209858848936, "compression_ratio": 1.6018518518518519, "no_speech_prob": 4.029255705972901e-06}, {"id": 1013, "seek": 344796, "start": 3463.96, "end": 3467.96, "text": " And so let's replace the model with that new model we created and fit.", "tokens": [400, 370, 718, 311, 7406, 264, 2316, 365, 300, 777, 2316, 321, 2942, 293, 3318, 13], "temperature": 0.0, "avg_logprob": -0.10353209858848936, "compression_ratio": 1.6018518518518519, "no_speech_prob": 4.029255705972901e-06}, {"id": 1014, "seek": 344796, "start": 3468.96, "end": 3475.96, "text": " And look at that 71 percent by fine tuning versus 37 percent training from scratch.", "tokens": [400, 574, 412, 300, 30942, 3043, 538, 2489, 15164, 5717, 13435, 3043, 3097, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.10353209858848936, "compression_ratio": 1.6018518518518519, "no_speech_prob": 4.029255705972901e-06}, {"id": 1015, "seek": 347596, "start": 3475.96, "end": 3481.96, "text": " So that looks good. So we have a simple transfer learning working.", "tokens": [407, 300, 1542, 665, 13, 407, 321, 362, 257, 2199, 5003, 2539, 1364, 13], "temperature": 0.0, "avg_logprob": -0.12866636422964242, "compression_ratio": 1.647887323943662, "no_speech_prob": 1.4509513675875496e-05}, {"id": 1016, "seek": 347596, "start": 3482.96, "end": 3487.96, "text": " So what I did then is I do this in Jupyter all the time.", "tokens": [407, 437, 286, 630, 550, 307, 286, 360, 341, 294, 22125, 88, 391, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.12866636422964242, "compression_ratio": 1.647887323943662, "no_speech_prob": 1.4509513675875496e-05}, {"id": 1017, "seek": 347596, "start": 3488.96, "end": 3492.96, "text": " I basically grabbed kind of all the cells.", "tokens": [286, 1936, 18607, 733, 295, 439, 264, 5438, 13], "temperature": 0.0, "avg_logprob": -0.12866636422964242, "compression_ratio": 1.647887323943662, "no_speech_prob": 1.4509513675875496e-05}, {"id": 1018, "seek": 347596, "start": 3493.96, "end": 3495.96, "text": " I hit C to copy and then I hit V to paste.", "tokens": [286, 2045, 383, 281, 5055, 293, 550, 286, 2045, 691, 281, 9163, 13], "temperature": 0.0, "avg_logprob": -0.12866636422964242, "compression_ratio": 1.647887323943662, "no_speech_prob": 1.4509513675875496e-05}, {"id": 1019, "seek": 347596, "start": 3496.96, "end": 3498.96, "text": " And then I grabbed the model and I hit shift M to merge.", "tokens": [400, 550, 286, 18607, 264, 2316, 293, 286, 2045, 5513, 376, 281, 22183, 13], "temperature": 0.0, "avg_logprob": -0.12866636422964242, "compression_ratio": 1.647887323943662, "no_speech_prob": 1.4509513675875496e-05}, {"id": 1020, "seek": 347596, "start": 3499.96, "end": 3501.96, "text": " And chucked a function header on top.", "tokens": [400, 20870, 292, 257, 2445, 23117, 322, 1192, 13], "temperature": 0.0, "avg_logprob": -0.12866636422964242, "compression_ratio": 1.647887323943662, "no_speech_prob": 1.4509513675875496e-05}, {"id": 1021, "seek": 347596, "start": 3502.96, "end": 3503.96, "text": " So now I've got a function that does all this.", "tokens": [407, 586, 286, 600, 658, 257, 2445, 300, 775, 439, 341, 13], "temperature": 0.0, "avg_logprob": -0.12866636422964242, "compression_ratio": 1.647887323943662, "no_speech_prob": 1.4509513675875496e-05}, {"id": 1022, "seek": 350396, "start": 3503.96, "end": 3505.96, "text": " So these are all the lines you saw just before.", "tokens": [407, 613, 366, 439, 264, 3876, 291, 1866, 445, 949, 13], "temperature": 0.0, "avg_logprob": -0.08349362397805238, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.061588500306243e-05}, {"id": 1023, "seek": 350396, "start": 3506.96, "end": 3508.96, "text": " And I've just stuck them all together into a function.", "tokens": [400, 286, 600, 445, 5541, 552, 439, 1214, 666, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.08349362397805238, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.061588500306243e-05}, {"id": 1024, "seek": 350396, "start": 3509.96, "end": 3510.96, "text": " I call it adapt model.", "tokens": [286, 818, 309, 6231, 2316, 13], "temperature": 0.0, "avg_logprob": -0.08349362397805238, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.061588500306243e-05}, {"id": 1025, "seek": 350396, "start": 3511.96, "end": 3513.96, "text": " It's going to take a learner and adapt it for the new data.", "tokens": [467, 311, 516, 281, 747, 257, 33347, 293, 6231, 309, 337, 264, 777, 1412, 13], "temperature": 0.0, "avg_logprob": -0.08349362397805238, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.061588500306243e-05}, {"id": 1026, "seek": 350396, "start": 3514.96, "end": 3516.96, "text": " So these are all the lines of code you've already seen.", "tokens": [407, 613, 366, 439, 264, 3876, 295, 3089, 291, 600, 1217, 1612, 13], "temperature": 0.0, "avg_logprob": -0.08349362397805238, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.061588500306243e-05}, {"id": 1027, "seek": 350396, "start": 3517.96, "end": 3524.96, "text": " And so now we can just go CNN learner, load the state dict, adapt the model and then we can start training.", "tokens": [400, 370, 586, 321, 393, 445, 352, 24859, 33347, 11, 3677, 264, 1785, 12569, 11, 6231, 264, 2316, 293, 550, 321, 393, 722, 3097, 13], "temperature": 0.0, "avg_logprob": -0.08349362397805238, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.061588500306243e-05}, {"id": 1028, "seek": 350396, "start": 3525.96, "end": 3530.96, "text": " But of course what we really like to do is to first of all train only the head.", "tokens": [583, 295, 1164, 437, 321, 534, 411, 281, 360, 307, 281, 700, 295, 439, 3847, 787, 264, 1378, 13], "temperature": 0.0, "avg_logprob": -0.08349362397805238, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.061588500306243e-05}, {"id": 1029, "seek": 353096, "start": 3530.96, "end": 3534.96, "text": " So let's grab all the parameters in the body.", "tokens": [407, 718, 311, 4444, 439, 264, 9834, 294, 264, 1772, 13], "temperature": 0.0, "avg_logprob": -0.09234385689099629, "compression_ratio": 1.7055837563451777, "no_speech_prob": 2.0784540538443252e-05}, {"id": 1030, "seek": 353096, "start": 3535.96, "end": 3540.96, "text": " And remember when we did that nn.sequential, the body is just the first thing.", "tokens": [400, 1604, 562, 321, 630, 300, 297, 77, 13, 11834, 2549, 11, 264, 1772, 307, 445, 264, 700, 551, 13], "temperature": 0.0, "avg_logprob": -0.09234385689099629, "compression_ratio": 1.7055837563451777, "no_speech_prob": 2.0784540538443252e-05}, {"id": 1031, "seek": 353096, "start": 3541.96, "end": 3543.96, "text": " That's the whole ResNet body.", "tokens": [663, 311, 264, 1379, 5015, 31890, 1772, 13], "temperature": 0.0, "avg_logprob": -0.09234385689099629, "compression_ratio": 1.7055837563451777, "no_speech_prob": 2.0784540538443252e-05}, {"id": 1032, "seek": 353096, "start": 3544.96, "end": 3551.96, "text": " So let's grab all the parameters in the body and set them to requires grad equals false.", "tokens": [407, 718, 311, 4444, 439, 264, 9834, 294, 264, 1772, 293, 992, 552, 281, 7029, 2771, 6915, 7908, 13], "temperature": 0.0, "avg_logprob": -0.09234385689099629, "compression_ratio": 1.7055837563451777, "no_speech_prob": 2.0784540538443252e-05}, {"id": 1033, "seek": 353096, "start": 3552.96, "end": 3553.96, "text": " So it's frozen.", "tokens": [407, 309, 311, 12496, 13], "temperature": 0.0, "avg_logprob": -0.09234385689099629, "compression_ratio": 1.7055837563451777, "no_speech_prob": 2.0784540538443252e-05}, {"id": 1034, "seek": 353096, "start": 3554.96, "end": 3558.96, "text": " And so now we can train just the head and we get 54 percent, which is great.", "tokens": [400, 370, 586, 321, 393, 3847, 445, 264, 1378, 293, 321, 483, 20793, 3043, 11, 597, 307, 869, 13], "temperature": 0.0, "avg_logprob": -0.09234385689099629, "compression_ratio": 1.7055837563451777, "no_speech_prob": 2.0784540538443252e-05}, {"id": 1035, "seek": 355896, "start": 3558.96, "end": 3560.96, "text": " So now we, as you know, unfreeze.", "tokens": [407, 586, 321, 11, 382, 291, 458, 11, 3971, 701, 1381, 13], "temperature": 0.0, "avg_logprob": -0.14186518366743878, "compression_ratio": 1.4375, "no_speech_prob": 1.5689114661654457e-05}, {"id": 1036, "seek": 355896, "start": 3561.96, "end": 3563.96, "text": " Okay. And train some more.", "tokens": [1033, 13, 400, 3847, 512, 544, 13], "temperature": 0.0, "avg_logprob": -0.14186518366743878, "compression_ratio": 1.4375, "no_speech_prob": 1.5689114661654457e-05}, {"id": 1037, "seek": 355896, "start": 3564.96, "end": 3565.96, "text": " Uh oh.", "tokens": [4019, 1954, 13], "temperature": 0.0, "avg_logprob": -0.14186518366743878, "compression_ratio": 1.4375, "no_speech_prob": 1.5689114661654457e-05}, {"id": 1038, "seek": 355896, "start": 3566.96, "end": 3571.96, "text": " So it's better than not fine tuning.", "tokens": [407, 309, 311, 1101, 813, 406, 2489, 15164, 13], "temperature": 0.0, "avg_logprob": -0.14186518366743878, "compression_ratio": 1.4375, "no_speech_prob": 1.5689114661654457e-05}, {"id": 1039, "seek": 355896, "start": 3572.96, "end": 3577.96, "text": " But interestingly it's worse, 71 versus 56.", "tokens": [583, 25873, 309, 311, 5324, 11, 30942, 5717, 19687, 13], "temperature": 0.0, "avg_logprob": -0.14186518366743878, "compression_ratio": 1.4375, "no_speech_prob": 1.5689114661654457e-05}, {"id": 1040, "seek": 355896, "start": 3578.96, "end": 3583.96, "text": " It's worse than the kind of naive fine tuning where we didn't do any freezing.", "tokens": [467, 311, 5324, 813, 264, 733, 295, 29052, 2489, 15164, 689, 321, 994, 380, 360, 604, 20200, 13], "temperature": 0.0, "avg_logprob": -0.14186518366743878, "compression_ratio": 1.4375, "no_speech_prob": 1.5689114661654457e-05}, {"id": 1041, "seek": 355896, "start": 3584.96, "end": 3585.96, "text": " So what's going on there?", "tokens": [407, 437, 311, 516, 322, 456, 30], "temperature": 0.0, "avg_logprob": -0.14186518366743878, "compression_ratio": 1.4375, "no_speech_prob": 1.5689114661654457e-05}, {"id": 1042, "seek": 358596, "start": 3585.96, "end": 3587.96, "text": " So what's going on there?", "tokens": [407, 437, 311, 516, 322, 456, 30], "temperature": 0.0, "avg_logprob": -0.1166563588519429, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.9033238913834793e-06}, {"id": 1043, "seek": 358596, "start": 3588.96, "end": 3592.96, "text": " Anytime something weird happens in your neural net, it's almost certainly because of batch norm.", "tokens": [39401, 746, 3657, 2314, 294, 428, 18161, 2533, 11, 309, 311, 1920, 3297, 570, 295, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.1166563588519429, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.9033238913834793e-06}, {"id": 1044, "seek": 358596, "start": 3593.96, "end": 3594.96, "text": " Because batch norm makes everything weird.", "tokens": [1436, 15245, 2026, 1669, 1203, 3657, 13], "temperature": 0.0, "avg_logprob": -0.1166563588519429, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.9033238913834793e-06}, {"id": 1045, "seek": 358596, "start": 3595.96, "end": 3596.96, "text": " And that's true here too.", "tokens": [400, 300, 311, 2074, 510, 886, 13], "temperature": 0.0, "avg_logprob": -0.1166563588519429, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.9033238913834793e-06}, {"id": 1046, "seek": 358596, "start": 3597.96, "end": 3603.96, "text": " What happened was our frozen part of our model, which was designed for ImageWolf,", "tokens": [708, 2011, 390, 527, 12496, 644, 295, 527, 2316, 11, 597, 390, 4761, 337, 29903, 54, 7491, 11], "temperature": 0.0, "avg_logprob": -0.1166563588519429, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.9033238913834793e-06}, {"id": 1047, "seek": 358596, "start": 3604.96, "end": 3612.96, "text": " those layers were tuned for some particular set of mean and standard deviations.", "tokens": [729, 7914, 645, 10870, 337, 512, 1729, 992, 295, 914, 293, 3832, 31219, 763, 13], "temperature": 0.0, "avg_logprob": -0.1166563588519429, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.9033238913834793e-06}, {"id": 1048, "seek": 361296, "start": 3612.96, "end": 3618.96, "text": " Because remember the batch norm is going to divide by the, subtract the mean and divide by the standard deviation.", "tokens": [1436, 1604, 264, 15245, 2026, 307, 516, 281, 9845, 538, 264, 11, 16390, 264, 914, 293, 9845, 538, 264, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.10893381209600539, "compression_ratio": 1.6221198156682028, "no_speech_prob": 3.4464803775335895e-06}, {"id": 1049, "seek": 361296, "start": 3619.96, "end": 3624.96, "text": " But the PETS data set has different means and standard deviations.", "tokens": [583, 264, 21968, 50, 1412, 992, 575, 819, 1355, 293, 3832, 31219, 763, 13], "temperature": 0.0, "avg_logprob": -0.10893381209600539, "compression_ratio": 1.6221198156682028, "no_speech_prob": 3.4464803775335895e-06}, {"id": 1050, "seek": 361296, "start": 3625.96, "end": 3627.96, "text": " Not for the input, but inside the model.", "tokens": [1726, 337, 264, 4846, 11, 457, 1854, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10893381209600539, "compression_ratio": 1.6221198156682028, "no_speech_prob": 3.4464803775335895e-06}, {"id": 1051, "seek": 361296, "start": 3628.96, "end": 3638.96, "text": " So then when we unfroze this, it basically said like, you know, this final layer was getting trained for everything being frozen.", "tokens": [407, 550, 562, 321, 3971, 340, 1381, 341, 11, 309, 1936, 848, 411, 11, 291, 458, 11, 341, 2572, 4583, 390, 1242, 8895, 337, 1203, 885, 12496, 13], "temperature": 0.0, "avg_logprob": -0.10893381209600539, "compression_ratio": 1.6221198156682028, "no_speech_prob": 3.4464803775335895e-06}, {"id": 1052, "seek": 363896, "start": 3638.96, "end": 3641.96, "text": " But that was for a different set of batch norm statistics.", "tokens": [583, 300, 390, 337, 257, 819, 992, 295, 15245, 2026, 12523, 13], "temperature": 0.0, "avg_logprob": -0.07010086059570313, "compression_ratio": 1.711340206185567, "no_speech_prob": 1.2218435585964471e-05}, {"id": 1053, "seek": 363896, "start": 3642.96, "end": 3645.96, "text": " So then when we unfroze it, everything tried to kind of catch up.", "tokens": [407, 550, 562, 321, 3971, 340, 1381, 309, 11, 1203, 3031, 281, 733, 295, 3745, 493, 13], "temperature": 0.0, "avg_logprob": -0.07010086059570313, "compression_ratio": 1.711340206185567, "no_speech_prob": 1.2218435585964471e-05}, {"id": 1054, "seek": 363896, "start": 3646.96, "end": 3654.96, "text": " And it would be very interesting to look at the kind of histograms and stuff that we did earlier in the course and like see what's really going on.", "tokens": [400, 309, 576, 312, 588, 1880, 281, 574, 412, 264, 733, 295, 49816, 82, 293, 1507, 300, 321, 630, 3071, 294, 264, 1164, 293, 411, 536, 437, 311, 534, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.07010086059570313, "compression_ratio": 1.711340206185567, "no_speech_prob": 1.2218435585964471e-05}, {"id": 1055, "seek": 363896, "start": 3655.96, "end": 3658.96, "text": " Because I haven't really seen anybody, I haven't really seen a paper about this.", "tokens": [1436, 286, 2378, 380, 534, 1612, 4472, 11, 286, 2378, 380, 534, 1612, 257, 3035, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.07010086059570313, "compression_ratio": 1.711340206185567, "no_speech_prob": 1.2218435585964471e-05}, {"id": 1056, "seek": 363896, "start": 3659.96, "end": 3662.96, "text": " It's something we've been doing in Fast AI for a few years now.", "tokens": [467, 311, 746, 321, 600, 668, 884, 294, 15968, 7318, 337, 257, 1326, 924, 586, 13], "temperature": 0.0, "avg_logprob": -0.07010086059570313, "compression_ratio": 1.711340206185567, "no_speech_prob": 1.2218435585964471e-05}, {"id": 1057, "seek": 366296, "start": 3662.96, "end": 3667.96, "text": " But I think this is the first course where we've actually drawn attention to it.", "tokens": [583, 286, 519, 341, 307, 264, 700, 1164, 689, 321, 600, 767, 10117, 3202, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.09418007532755533, "compression_ratio": 1.4759825327510918, "no_speech_prob": 1.1125071068818215e-05}, {"id": 1058, "seek": 366296, "start": 3668.96, "end": 3671.96, "text": " That's kind of something that's been hidden away in the library before.", "tokens": [663, 311, 733, 295, 746, 300, 311, 668, 7633, 1314, 294, 264, 6405, 949, 13], "temperature": 0.0, "avg_logprob": -0.09418007532755533, "compression_ratio": 1.4759825327510918, "no_speech_prob": 1.1125071068818215e-05}, {"id": 1059, "seek": 366296, "start": 3672.96, "end": 3673.96, "text": " But as you can see, it's a huge difference, right?", "tokens": [583, 382, 291, 393, 536, 11, 309, 311, 257, 2603, 2649, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.09418007532755533, "compression_ratio": 1.4759825327510918, "no_speech_prob": 1.1125071068818215e-05}, {"id": 1060, "seek": 366296, "start": 3674.96, "end": 3676.96, "text": " The difference between 56 versus 71.", "tokens": [440, 2649, 1296, 19687, 5717, 30942, 13], "temperature": 0.0, "avg_logprob": -0.09418007532755533, "compression_ratio": 1.4759825327510918, "no_speech_prob": 1.1125071068818215e-05}, {"id": 1061, "seek": 366296, "start": 3677.96, "end": 3679.96, "text": " So the good news is it's easily fixed.", "tokens": [407, 264, 665, 2583, 307, 309, 311, 3612, 6806, 13], "temperature": 0.0, "avg_logprob": -0.09418007532755533, "compression_ratio": 1.4759825327510918, "no_speech_prob": 1.1125071068818215e-05}, {"id": 1062, "seek": 366296, "start": 3680.96, "end": 3686.96, "text": " And the trick is to not freeze all of the body parameters,", "tokens": [400, 264, 4282, 307, 281, 406, 15959, 439, 295, 264, 1772, 9834, 11], "temperature": 0.0, "avg_logprob": -0.09418007532755533, "compression_ratio": 1.4759825327510918, "no_speech_prob": 1.1125071068818215e-05}, {"id": 1063, "seek": 368696, "start": 3686.96, "end": 3691.96, "text": " but freeze all of the body parameters that aren't in the batch norm layers.", "tokens": [457, 15959, 439, 295, 264, 1772, 9834, 300, 3212, 380, 294, 264, 15245, 2026, 7914, 13], "temperature": 0.0, "avg_logprob": -0.08912635943211547, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.4970251868362539e-05}, {"id": 1064, "seek": 368696, "start": 3692.96, "end": 3699.96, "text": " And that way, when we fine tune the final layer, we're also fine tuning all of the batch norm layers, weights and biases.", "tokens": [400, 300, 636, 11, 562, 321, 2489, 10864, 264, 2572, 4583, 11, 321, 434, 611, 2489, 15164, 439, 295, 264, 15245, 2026, 7914, 11, 17443, 293, 32152, 13], "temperature": 0.0, "avg_logprob": -0.08912635943211547, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.4970251868362539e-05}, {"id": 1065, "seek": 368696, "start": 3700.96, "end": 3702.96, "text": " So we can create, just like before, adapt the model.", "tokens": [407, 321, 393, 1884, 11, 445, 411, 949, 11, 6231, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.08912635943211547, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.4970251868362539e-05}, {"id": 1066, "seek": 368696, "start": 3703.96, "end": 3712.96, "text": " And let's create something called set gradient, which says, oh, if it's a linear layer at the end or a batch norm layer in the middle, return.", "tokens": [400, 718, 311, 1884, 746, 1219, 992, 16235, 11, 597, 1619, 11, 1954, 11, 498, 309, 311, 257, 8213, 4583, 412, 264, 917, 420, 257, 15245, 2026, 4583, 294, 264, 2808, 11, 2736, 13], "temperature": 0.0, "avg_logprob": -0.08912635943211547, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.4970251868362539e-05}, {"id": 1067, "seek": 368696, "start": 3713.96, "end": 3714.96, "text": " Don't change the gradient.", "tokens": [1468, 380, 1319, 264, 16235, 13], "temperature": 0.0, "avg_logprob": -0.08912635943211547, "compression_ratio": 1.7721518987341771, "no_speech_prob": 1.4970251868362539e-05}, {"id": 1068, "seek": 371496, "start": 3714.96, "end": 3720.96, "text": " Otherwise, if it's got weights, set requires grad two, whatever you asked for, which we're going to start false.", "tokens": [10328, 11, 498, 309, 311, 658, 17443, 11, 992, 7029, 2771, 732, 11, 2035, 291, 2351, 337, 11, 597, 321, 434, 516, 281, 722, 7908, 13], "temperature": 0.0, "avg_logprob": -0.08291336797898816, "compression_ratio": 1.654867256637168, "no_speech_prob": 7.88901343184989e-06}, {"id": 1069, "seek": 371496, "start": 3721.96, "end": 3730.96, "text": " Here's a little convenient function that will apply any function you pass to it recursively to all of the children of a model.", "tokens": [1692, 311, 257, 707, 10851, 2445, 300, 486, 3079, 604, 2445, 291, 1320, 281, 309, 20560, 3413, 281, 439, 295, 264, 2227, 295, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.08291336797898816, "compression_ratio": 1.654867256637168, "no_speech_prob": 7.88901343184989e-06}, {"id": 1070, "seek": 371496, "start": 3731.96, "end": 3740.96, "text": " So now that we have apply to a model, or apply to a module, I guess, we can just pass in a module and that will be applied throughout.", "tokens": [407, 586, 300, 321, 362, 3079, 281, 257, 2316, 11, 420, 3079, 281, 257, 10088, 11, 286, 2041, 11, 321, 393, 445, 1320, 294, 257, 10088, 293, 300, 486, 312, 6456, 3710, 13], "temperature": 0.0, "avg_logprob": -0.08291336797898816, "compression_ratio": 1.654867256637168, "no_speech_prob": 7.88901343184989e-06}, {"id": 1071, "seek": 374096, "start": 3740.96, "end": 3748.96, "text": " So this way we freeze just the non-batch norm layers and, of course, not the last layer.", "tokens": [407, 341, 636, 321, 15959, 445, 264, 2107, 12, 65, 852, 2026, 7914, 293, 11, 295, 1164, 11, 406, 264, 1036, 4583, 13], "temperature": 0.0, "avg_logprob": -0.11086529272573965, "compression_ratio": 1.5807692307692307, "no_speech_prob": 5.771779797214549e-06}, {"id": 1072, "seek": 374096, "start": 3749.96, "end": 3753.96, "text": " And so actually fine tuning immediately is a bit better, goes from 54 to 58.", "tokens": [400, 370, 767, 2489, 15164, 4258, 307, 257, 857, 1101, 11, 1709, 490, 20793, 281, 21786, 13], "temperature": 0.0, "avg_logprob": -0.11086529272573965, "compression_ratio": 1.5807692307692307, "no_speech_prob": 5.771779797214549e-06}, {"id": 1073, "seek": 374096, "start": 3754.96, "end": 3759.96, "text": " But more importantly, then when we unfreeze, we're back into the 70s again.", "tokens": [583, 544, 8906, 11, 550, 562, 321, 3971, 701, 1381, 11, 321, 434, 646, 666, 264, 5285, 82, 797, 13], "temperature": 0.0, "avg_logprob": -0.11086529272573965, "compression_ratio": 1.5807692307692307, "no_speech_prob": 5.771779797214549e-06}, {"id": 1074, "seek": 374096, "start": 3760.96, "end": 3764.96, "text": " So this is just a super important thing to remember if you're doing fine tuning.", "tokens": [407, 341, 307, 445, 257, 1687, 1021, 551, 281, 1604, 498, 291, 434, 884, 2489, 15164, 13], "temperature": 0.0, "avg_logprob": -0.11086529272573965, "compression_ratio": 1.5807692307692307, "no_speech_prob": 5.771779797214549e-06}, {"id": 1075, "seek": 376496, "start": 3764.96, "end": 3769.96, "text": " And I don't think there's any library other than Fast AI that does this, weirdly enough.", "tokens": [400, 286, 500, 380, 519, 456, 311, 604, 6405, 661, 813, 15968, 7318, 300, 775, 341, 11, 48931, 1547, 13], "temperature": 0.0, "avg_logprob": -0.07870110520371446, "compression_ratio": 1.6333333333333333, "no_speech_prob": 4.092741164640756e-06}, {"id": 1076, "seek": 376496, "start": 3770.96, "end": 3784.96, "text": " So if you're using TensorFlow or something, you'll have to write this yourself to make sure that you don't freeze ever, don't ever freeze the weights in the batch norm layers anytime you're doing partial layer training.", "tokens": [407, 498, 291, 434, 1228, 37624, 420, 746, 11, 291, 603, 362, 281, 2464, 341, 1803, 281, 652, 988, 300, 291, 500, 380, 15959, 1562, 11, 500, 380, 1562, 15959, 264, 17443, 294, 264, 15245, 2026, 7914, 13038, 291, 434, 884, 14641, 4583, 3097, 13], "temperature": 0.0, "avg_logprob": -0.07870110520371446, "compression_ratio": 1.6333333333333333, "no_speech_prob": 4.092741164640756e-06}, {"id": 1077, "seek": 376496, "start": 3785.96, "end": 3791.96, "text": " Oh, by the way, that apply mod, I only wrote it because we're not allowed to use stuff in PyTorch, but actually PyTorch has its own.", "tokens": [876, 11, 538, 264, 636, 11, 300, 3079, 1072, 11, 286, 787, 4114, 309, 570, 321, 434, 406, 4350, 281, 764, 1507, 294, 9953, 51, 284, 339, 11, 457, 767, 9953, 51, 284, 339, 575, 1080, 1065, 13], "temperature": 0.0, "avg_logprob": -0.07870110520371446, "compression_ratio": 1.6333333333333333, "no_speech_prob": 4.092741164640756e-06}, {"id": 1078, "seek": 379196, "start": 3791.96, "end": 3794.96, "text": " It's called model.apply, so you can use that now.", "tokens": [467, 311, 1219, 2316, 13, 1746, 356, 11, 370, 291, 393, 764, 300, 586, 13], "temperature": 0.0, "avg_logprob": -0.0742704935162981, "compression_ratio": 1.7457627118644068, "no_speech_prob": 3.611870170061593e-06}, {"id": 1079, "seek": 379196, "start": 3795.96, "end": 3796.96, "text": " It's the same thing.", "tokens": [467, 311, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.0742704935162981, "compression_ratio": 1.7457627118644068, "no_speech_prob": 3.611870170061593e-06}, {"id": 1080, "seek": 379196, "start": 3797.96, "end": 3805.96, "text": " Okay, so finally for this half of the course, we're going to look at discriminative learning rates.", "tokens": [1033, 11, 370, 2721, 337, 341, 1922, 295, 264, 1164, 11, 321, 434, 516, 281, 574, 412, 20828, 1166, 2539, 6846, 13], "temperature": 0.0, "avg_logprob": -0.0742704935162981, "compression_ratio": 1.7457627118644068, "no_speech_prob": 3.611870170061593e-06}, {"id": 1081, "seek": 379196, "start": 3806.96, "end": 3809.96, "text": " So for discriminative learning rates, there's a few things we can do with them.", "tokens": [407, 337, 20828, 1166, 2539, 6846, 11, 456, 311, 257, 1326, 721, 321, 393, 360, 365, 552, 13], "temperature": 0.0, "avg_logprob": -0.0742704935162981, "compression_ratio": 1.7457627118644068, "no_speech_prob": 3.611870170061593e-06}, {"id": 1082, "seek": 379196, "start": 3810.96, "end": 3815.96, "text": " One is it's a simple way to do layer freezing without actually worrying about setting requires grad.", "tokens": [1485, 307, 309, 311, 257, 2199, 636, 281, 360, 4583, 20200, 1553, 767, 18788, 466, 3287, 7029, 2771, 13], "temperature": 0.0, "avg_logprob": -0.0742704935162981, "compression_ratio": 1.7457627118644068, "no_speech_prob": 3.611870170061593e-06}, {"id": 1083, "seek": 379196, "start": 3816.96, "end": 3818.96, "text": " We could just set the learning rate to zero for some layers.", "tokens": [492, 727, 445, 992, 264, 2539, 3314, 281, 4018, 337, 512, 7914, 13], "temperature": 0.0, "avg_logprob": -0.0742704935162981, "compression_ratio": 1.7457627118644068, "no_speech_prob": 3.611870170061593e-06}, {"id": 1084, "seek": 381896, "start": 3818.96, "end": 3820.96, "text": " So let's start by doing that.", "tokens": [407, 718, 311, 722, 538, 884, 300, 13], "temperature": 0.0, "avg_logprob": -0.12747195597445027, "compression_ratio": 1.7333333333333334, "no_speech_prob": 9.97249753709184e-06}, {"id": 1085, "seek": 381896, "start": 3821.96, "end": 3828.96, "text": " So what we're going to do is we're going to split our parameters into two or more groups.", "tokens": [407, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 7472, 527, 9834, 666, 732, 420, 544, 3935, 13], "temperature": 0.0, "avg_logprob": -0.12747195597445027, "compression_ratio": 1.7333333333333334, "no_speech_prob": 9.97249753709184e-06}, {"id": 1086, "seek": 381896, "start": 3829.96, "end": 3830.96, "text": " With a function.", "tokens": [2022, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12747195597445027, "compression_ratio": 1.7333333333333334, "no_speech_prob": 9.97249753709184e-06}, {"id": 1087, "seek": 381896, "start": 3831.96, "end": 3832.96, "text": " Here's our function.", "tokens": [1692, 311, 527, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12747195597445027, "compression_ratio": 1.7333333333333334, "no_speech_prob": 9.97249753709184e-06}, {"id": 1088, "seek": 381896, "start": 3834.96, "end": 3835.96, "text": " It's called bnSplitter.", "tokens": [467, 311, 1219, 272, 77, 50, 564, 3904, 13], "temperature": 0.0, "avg_logprob": -0.12747195597445027, "compression_ratio": 1.7333333333333334, "no_speech_prob": 9.97249753709184e-06}, {"id": 1089, "seek": 381896, "start": 3836.96, "end": 3838.96, "text": " It's going to create two groups of parameters.", "tokens": [467, 311, 516, 281, 1884, 732, 3935, 295, 9834, 13], "temperature": 0.0, "avg_logprob": -0.12747195597445027, "compression_ratio": 1.7333333333333334, "no_speech_prob": 9.97249753709184e-06}, {"id": 1090, "seek": 381896, "start": 3839.96, "end": 3843.96, "text": " And it's going to pass the body to underscore bnSplitter.", "tokens": [400, 309, 311, 516, 281, 1320, 264, 1772, 281, 37556, 272, 77, 50, 564, 3904, 13], "temperature": 0.0, "avg_logprob": -0.12747195597445027, "compression_ratio": 1.7333333333333334, "no_speech_prob": 9.97249753709184e-06}, {"id": 1091, "seek": 384396, "start": 3843.96, "end": 3848.96, "text": " Which will recursively look for batch norm layers and put them in the second group.", "tokens": [3013, 486, 20560, 3413, 574, 337, 15245, 2026, 7914, 293, 829, 552, 294, 264, 1150, 1594, 13], "temperature": 0.0, "avg_logprob": -0.08147543613041673, "compression_ratio": 1.688976377952756, "no_speech_prob": 5.68233554076869e-06}, {"id": 1092, "seek": 384396, "start": 3849.96, "end": 3851.96, "text": " Or anything else with a weight goes in the first group.", "tokens": [1610, 1340, 1646, 365, 257, 3364, 1709, 294, 264, 700, 1594, 13], "temperature": 0.0, "avg_logprob": -0.08147543613041673, "compression_ratio": 1.688976377952756, "no_speech_prob": 5.68233554076869e-06}, {"id": 1093, "seek": 384396, "start": 3852.96, "end": 3853.96, "text": " And then do it recursively.", "tokens": [400, 550, 360, 309, 20560, 3413, 13], "temperature": 0.0, "avg_logprob": -0.08147543613041673, "compression_ratio": 1.688976377952756, "no_speech_prob": 5.68233554076869e-06}, {"id": 1094, "seek": 384396, "start": 3854.96, "end": 3857.96, "text": " And then also the second group will add everything after the head.", "tokens": [400, 550, 611, 264, 1150, 1594, 486, 909, 1203, 934, 264, 1378, 13], "temperature": 0.0, "avg_logprob": -0.08147543613041673, "compression_ratio": 1.688976377952756, "no_speech_prob": 5.68233554076869e-06}, {"id": 1095, "seek": 384396, "start": 3858.96, "end": 3864.96, "text": " So this is basically doing something where we're putting all our parameters into the two groups we want to treat differently.", "tokens": [407, 341, 307, 1936, 884, 746, 689, 321, 434, 3372, 439, 527, 9834, 666, 264, 732, 3935, 321, 528, 281, 2387, 7614, 13], "temperature": 0.0, "avg_logprob": -0.08147543613041673, "compression_ratio": 1.688976377952756, "no_speech_prob": 5.68233554076869e-06}, {"id": 1096, "seek": 384396, "start": 3866.96, "end": 3869.96, "text": " So we can check, for example, that when we do bnSplitter on a model,", "tokens": [407, 321, 393, 1520, 11, 337, 1365, 11, 300, 562, 321, 360, 272, 77, 50, 564, 3904, 322, 257, 2316, 11], "temperature": 0.0, "avg_logprob": -0.08147543613041673, "compression_ratio": 1.688976377952756, "no_speech_prob": 5.68233554076869e-06}, {"id": 1097, "seek": 386996, "start": 3869.96, "end": 3875.96, "text": " that the number of parameters in the two halves is equal to the total number of parameters in the model.", "tokens": [300, 264, 1230, 295, 9834, 294, 264, 732, 38490, 307, 2681, 281, 264, 3217, 1230, 295, 9834, 294, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14373967430808327, "compression_ratio": 1.7166666666666666, "no_speech_prob": 1.892482760013081e-05}, {"id": 1098, "seek": 386996, "start": 3876.96, "end": 3878.96, "text": " And so now I want to check this works.", "tokens": [400, 370, 586, 286, 528, 281, 1520, 341, 1985, 13], "temperature": 0.0, "avg_logprob": -0.14373967430808327, "compression_ratio": 1.7166666666666666, "no_speech_prob": 1.892482760013081e-05}, {"id": 1099, "seek": 386996, "start": 3879.96, "end": 3884.96, "text": " I want to make sure that if I pass this, because we now have a splitter function in the learner.", "tokens": [286, 528, 281, 652, 988, 300, 498, 286, 1320, 341, 11, 570, 321, 586, 362, 257, 4732, 3904, 2445, 294, 264, 33347, 13], "temperature": 0.0, "avg_logprob": -0.14373967430808327, "compression_ratio": 1.7166666666666666, "no_speech_prob": 1.892482760013081e-05}, {"id": 1100, "seek": 386996, "start": 3885.96, "end": 3887.96, "text": " And that's another thing I added this week.", "tokens": [400, 300, 311, 1071, 551, 286, 3869, 341, 1243, 13], "temperature": 0.0, "avg_logprob": -0.14373967430808327, "compression_ratio": 1.7166666666666666, "no_speech_prob": 1.892482760013081e-05}, {"id": 1101, "seek": 386996, "start": 3888.96, "end": 3889.96, "text": " That when you start training.", "tokens": [663, 562, 291, 722, 3097, 13], "temperature": 0.0, "avg_logprob": -0.14373967430808327, "compression_ratio": 1.7166666666666666, "no_speech_prob": 1.892482760013081e-05}, {"id": 1102, "seek": 386996, "start": 3890.96, "end": 3891.96, "text": " It's literally just this.", "tokens": [467, 311, 3736, 445, 341, 13], "temperature": 0.0, "avg_logprob": -0.14373967430808327, "compression_ratio": 1.7166666666666666, "no_speech_prob": 1.892482760013081e-05}, {"id": 1103, "seek": 386996, "start": 3892.96, "end": 3896.96, "text": " When we get our credit optimizer, it passes the model to self.splitter.", "tokens": [1133, 321, 483, 527, 5397, 5028, 6545, 11, 309, 11335, 264, 2316, 281, 2698, 13, 46535, 3904, 13], "temperature": 0.0, "avg_logprob": -0.14373967430808327, "compression_ratio": 1.7166666666666666, "no_speech_prob": 1.892482760013081e-05}, {"id": 1104, "seek": 389696, "start": 3896.96, "end": 3898.96, "text": " Which by default does nothing at all.", "tokens": [3013, 538, 7576, 775, 1825, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.08237981115068709, "compression_ratio": 1.6868686868686869, "no_speech_prob": 1.1659334631985985e-05}, {"id": 1105, "seek": 389696, "start": 3899.96, "end": 3903.96, "text": " And so we're going to be using our bnSplitter to split it into multiple parameter groups.", "tokens": [400, 370, 321, 434, 516, 281, 312, 1228, 527, 272, 77, 50, 564, 3904, 281, 7472, 309, 666, 3866, 13075, 3935, 13], "temperature": 0.0, "avg_logprob": -0.08237981115068709, "compression_ratio": 1.6868686868686869, "no_speech_prob": 1.1659334631985985e-05}, {"id": 1106, "seek": 389696, "start": 3904.96, "end": 3906.96, "text": " And so how do we debug that? How do we make sure it's working?", "tokens": [400, 370, 577, 360, 321, 24083, 300, 30, 1012, 360, 321, 652, 988, 309, 311, 1364, 30], "temperature": 0.0, "avg_logprob": -0.08237981115068709, "compression_ratio": 1.6868686868686869, "no_speech_prob": 1.1659334631985985e-05}, {"id": 1107, "seek": 389696, "start": 3907.96, "end": 3911.96, "text": " Because this is one of these things that if I screw it up, I probably won't get an error.", "tokens": [1436, 341, 307, 472, 295, 613, 721, 300, 498, 286, 5630, 309, 493, 11, 286, 1391, 1582, 380, 483, 364, 6713, 13], "temperature": 0.0, "avg_logprob": -0.08237981115068709, "compression_ratio": 1.6868686868686869, "no_speech_prob": 1.1659334631985985e-05}, {"id": 1108, "seek": 389696, "start": 3912.96, "end": 3914.96, "text": " But instead it probably won't train my last layer.", "tokens": [583, 2602, 309, 1391, 1582, 380, 3847, 452, 1036, 4583, 13], "temperature": 0.0, "avg_logprob": -0.08237981115068709, "compression_ratio": 1.6868686868686869, "no_speech_prob": 1.1659334631985985e-05}, {"id": 1109, "seek": 389696, "start": 3915.96, "end": 3917.96, "text": " It'll train all the layers at the same learning rate.", "tokens": [467, 603, 3847, 439, 264, 7914, 412, 264, 912, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.08237981115068709, "compression_ratio": 1.6868686868686869, "no_speech_prob": 1.1659334631985985e-05}, {"id": 1110, "seek": 389696, "start": 3918.96, "end": 3921.96, "text": " Or it would be hard to know if the model was bad because I screwed up my code or not.", "tokens": [1610, 309, 576, 312, 1152, 281, 458, 498, 264, 2316, 390, 1578, 570, 286, 20331, 493, 452, 3089, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.08237981115068709, "compression_ratio": 1.6868686868686869, "no_speech_prob": 1.1659334631985985e-05}, {"id": 1111, "seek": 389696, "start": 3922.96, "end": 3923.96, "text": " So we need a way to debug it.", "tokens": [407, 321, 643, 257, 636, 281, 24083, 309, 13], "temperature": 0.0, "avg_logprob": -0.08237981115068709, "compression_ratio": 1.6868686868686869, "no_speech_prob": 1.1659334631985985e-05}, {"id": 1112, "seek": 392396, "start": 3923.96, "end": 3926.96, "text": " We can't just look inside and make sure it's working.", "tokens": [492, 393, 380, 445, 574, 1854, 293, 652, 988, 309, 311, 1364, 13], "temperature": 0.0, "avg_logprob": -0.12773195019474737, "compression_ratio": 1.867298578199052, "no_speech_prob": 1.221861930389423e-05}, {"id": 1113, "seek": 392396, "start": 3927.96, "end": 3930.96, "text": " Because what we're going to be doing is we're going to be passing it.", "tokens": [1436, 437, 321, 434, 516, 281, 312, 884, 307, 321, 434, 516, 281, 312, 8437, 309, 13], "temperature": 0.0, "avg_logprob": -0.12773195019474737, "compression_ratio": 1.867298578199052, "no_speech_prob": 1.221861930389423e-05}, {"id": 1114, "seek": 392396, "start": 3932.96, "end": 3934.96, "text": " Let's see this one. We're going to be.", "tokens": [961, 311, 536, 341, 472, 13, 492, 434, 516, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.12773195019474737, "compression_ratio": 1.867298578199052, "no_speech_prob": 1.221861930389423e-05}, {"id": 1115, "seek": 392396, "start": 3938.96, "end": 3942.96, "text": " We're going to be passing it to the splitter parameter when we create the learner.", "tokens": [492, 434, 516, 281, 312, 8437, 309, 281, 264, 4732, 3904, 13075, 562, 321, 1884, 264, 33347, 13], "temperature": 0.0, "avg_logprob": -0.12773195019474737, "compression_ratio": 1.867298578199052, "no_speech_prob": 1.221861930389423e-05}, {"id": 1116, "seek": 392396, "start": 3943.96, "end": 3945.96, "text": " Right. So after this, it set the splitter parameter.", "tokens": [1779, 13, 407, 934, 341, 11, 309, 992, 264, 4732, 3904, 13075, 13], "temperature": 0.0, "avg_logprob": -0.12773195019474737, "compression_ratio": 1.867298578199052, "no_speech_prob": 1.221861930389423e-05}, {"id": 1117, "seek": 392396, "start": 3946.96, "end": 3950.96, "text": " And then when we start training, we're hoping that it's going to create these two layer groups.", "tokens": [400, 550, 562, 321, 722, 3097, 11, 321, 434, 7159, 300, 309, 311, 516, 281, 1884, 613, 732, 4583, 3935, 13], "temperature": 0.0, "avg_logprob": -0.12773195019474737, "compression_ratio": 1.867298578199052, "no_speech_prob": 1.221861930389423e-05}, {"id": 1118, "seek": 395096, "start": 3950.96, "end": 3952.96, "text": " So we need some way to look inside the model.", "tokens": [407, 321, 643, 512, 636, 281, 574, 1854, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10479966525373788, "compression_ratio": 1.7078189300411524, "no_speech_prob": 1.3006652807234786e-05}, {"id": 1119, "seek": 395096, "start": 3953.96, "end": 3955.96, "text": " So of course we're going to use a callback.", "tokens": [407, 295, 1164, 321, 434, 516, 281, 764, 257, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.10479966525373788, "compression_ratio": 1.7078189300411524, "no_speech_prob": 1.3006652807234786e-05}, {"id": 1120, "seek": 395096, "start": 3956.96, "end": 3957.96, "text": " And this is something that's super cool.", "tokens": [400, 341, 307, 746, 300, 311, 1687, 1627, 13], "temperature": 0.0, "avg_logprob": -0.10479966525373788, "compression_ratio": 1.7078189300411524, "no_speech_prob": 1.3006652807234786e-05}, {"id": 1121, "seek": 395096, "start": 3958.96, "end": 3962.96, "text": " Do you remember how I told you that you can actually override Dundercall itself?", "tokens": [1144, 291, 1604, 577, 286, 1907, 291, 300, 291, 393, 767, 42321, 413, 997, 2869, 336, 2564, 30], "temperature": 0.0, "avg_logprob": -0.10479966525373788, "compression_ratio": 1.7078189300411524, "no_speech_prob": 1.3006652807234786e-05}, {"id": 1122, "seek": 395096, "start": 3963.96, "end": 3965.96, "text": " You don't just have to override a specific callback.", "tokens": [509, 500, 380, 445, 362, 281, 42321, 257, 2685, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.10479966525373788, "compression_ratio": 1.7078189300411524, "no_speech_prob": 1.3006652807234786e-05}, {"id": 1123, "seek": 395096, "start": 3966.96, "end": 3972.96, "text": " And by overriding Dundercall itself, we can actually say, which callback do we want to debug?", "tokens": [400, 538, 670, 81, 2819, 413, 997, 2869, 336, 2564, 11, 321, 393, 767, 584, 11, 597, 818, 3207, 360, 321, 528, 281, 24083, 30], "temperature": 0.0, "avg_logprob": -0.10479966525373788, "compression_ratio": 1.7078189300411524, "no_speech_prob": 1.3006652807234786e-05}, {"id": 1124, "seek": 395096, "start": 3973.96, "end": 3976.96, "text": " And when we hit that callback, please run this function.", "tokens": [400, 562, 321, 2045, 300, 818, 3207, 11, 1767, 1190, 341, 2445, 13], "temperature": 0.0, "avg_logprob": -0.10479966525373788, "compression_ratio": 1.7078189300411524, "no_speech_prob": 1.3006652807234786e-05}, {"id": 1125, "seek": 397696, "start": 3976.96, "end": 3980.96, "text": " And if you don't pass in a function, it just jumps into the debugger as soon as that callback is hit.", "tokens": [400, 498, 291, 500, 380, 1320, 294, 257, 2445, 11, 309, 445, 16704, 666, 264, 24083, 1321, 382, 2321, 382, 300, 818, 3207, 307, 2045, 13], "temperature": 0.0, "avg_logprob": -0.10302963877111916, "compression_ratio": 1.7570422535211268, "no_speech_prob": 8.801001058600377e-06}, {"id": 1126, "seek": 397696, "start": 3981.96, "end": 3982.96, "text": " Otherwise, call the function.", "tokens": [10328, 11, 818, 264, 2445, 13], "temperature": 0.0, "avg_logprob": -0.10302963877111916, "compression_ratio": 1.7570422535211268, "no_speech_prob": 8.801001058600377e-06}, {"id": 1127, "seek": 397696, "start": 3983.96, "end": 3984.96, "text": " So this is super handy, right?", "tokens": [407, 341, 307, 1687, 13239, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.10302963877111916, "compression_ratio": 1.7570422535211268, "no_speech_prob": 8.801001058600377e-06}, {"id": 1128, "seek": 397696, "start": 3985.96, "end": 3991.96, "text": " Because now I can create a function called print details that just prints out how many parameter groups there are and what the hyperparameters there are.", "tokens": [1436, 586, 286, 393, 1884, 257, 2445, 1219, 4482, 4365, 300, 445, 22305, 484, 577, 867, 13075, 3935, 456, 366, 293, 437, 264, 9848, 2181, 335, 6202, 456, 366, 13], "temperature": 0.0, "avg_logprob": -0.10302963877111916, "compression_ratio": 1.7570422535211268, "no_speech_prob": 8.801001058600377e-06}, {"id": 1129, "seek": 397696, "start": 3992.96, "end": 3995.96, "text": " And then immediately raises the cancel training exception to stop.", "tokens": [400, 550, 4258, 19658, 264, 10373, 3097, 11183, 281, 1590, 13], "temperature": 0.0, "avg_logprob": -0.10302963877111916, "compression_ratio": 1.7570422535211268, "no_speech_prob": 8.801001058600377e-06}, {"id": 1130, "seek": 397696, "start": 3996.96, "end": 4004.96, "text": " And so then I can fit with my discriminative LR scheduler and my debug callback and my discriminative LR scheduler.", "tokens": [400, 370, 550, 286, 393, 3318, 365, 452, 20828, 1166, 441, 49, 12000, 260, 293, 452, 24083, 818, 3207, 293, 452, 20828, 1166, 441, 49, 12000, 260, 13], "temperature": 0.0, "avg_logprob": -0.10302963877111916, "compression_ratio": 1.7570422535211268, "no_speech_prob": 8.801001058600377e-06}, {"id": 1131, "seek": 400496, "start": 4004.96, "end": 4011.96, "text": " Is something that now doesn't just take a learning rate, but an array of learning rates and creates a scheduler for every learning rate.", "tokens": [1119, 746, 300, 586, 1177, 380, 445, 747, 257, 2539, 3314, 11, 457, 364, 10225, 295, 2539, 6846, 293, 7829, 257, 12000, 260, 337, 633, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.2456229153801413, "compression_ratio": 1.716279069767442, "no_speech_prob": 2.0784227672265843e-05}, {"id": 1132, "seek": 400496, "start": 4012.96, "end": 4013.96, "text": " And so I can pass that in.", "tokens": [400, 370, 286, 393, 1320, 300, 294, 13], "temperature": 0.0, "avg_logprob": -0.2456229153801413, "compression_ratio": 1.716279069767442, "no_speech_prob": 2.0784227672265843e-05}, {"id": 1133, "seek": 400496, "start": 4014.96, "end": 4017.96, "text": " So I'm going to use 0 and 0.02.", "tokens": [407, 286, 478, 516, 281, 764, 1958, 293, 1958, 13, 12756, 13], "temperature": 0.0, "avg_logprob": -0.2456229153801413, "compression_ratio": 1.716279069767442, "no_speech_prob": 2.0784227672265843e-05}, {"id": 1134, "seek": 400496, "start": 4018.96, "end": 4027.96, "text": " So in other words, no training for the body and 0.03 for the head and the batch norm.", "tokens": [407, 294, 661, 2283, 11, 572, 3097, 337, 264, 1772, 293, 1958, 13, 11592, 337, 264, 1378, 293, 264, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.2456229153801413, "compression_ratio": 1.716279069767442, "no_speech_prob": 2.0784227672265843e-05}, {"id": 1135, "seek": 402796, "start": 4027.96, "end": 4034.96, "text": " And so as soon as I fit, it immediately stops because the cancel training exception was raised.", "tokens": [400, 370, 382, 2321, 382, 286, 3318, 11, 309, 4258, 10094, 570, 264, 10373, 3097, 11183, 390, 6005, 13], "temperature": 0.0, "avg_logprob": -0.10100089586698092, "compression_ratio": 1.7633928571428572, "no_speech_prob": 5.594189133262262e-06}, {"id": 1136, "seek": 402796, "start": 4035.96, "end": 4039.96, "text": " And it prints out and it says there's two parameter groups, which is what we want.", "tokens": [400, 309, 22305, 484, 293, 309, 1619, 456, 311, 732, 13075, 3935, 11, 597, 307, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.10100089586698092, "compression_ratio": 1.7633928571428572, "no_speech_prob": 5.594189133262262e-06}, {"id": 1137, "seek": 402796, "start": 4040.96, "end": 4043.96, "text": " And the first parameter group has a learning rate of 0, which is what we want.", "tokens": [400, 264, 700, 13075, 1594, 575, 257, 2539, 3314, 295, 1958, 11, 597, 307, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.10100089586698092, "compression_ratio": 1.7633928571428572, "no_speech_prob": 5.594189133262262e-06}, {"id": 1138, "seek": 402796, "start": 4044.96, "end": 4052.96, "text": " And the second is 0.003, which is right because it's 0.03 and we're using the learning rate scheduler.", "tokens": [400, 264, 1150, 307, 1958, 13, 628, 18, 11, 597, 307, 558, 570, 309, 311, 1958, 13, 11592, 293, 321, 434, 1228, 264, 2539, 3314, 12000, 260, 13], "temperature": 0.0, "avg_logprob": -0.10100089586698092, "compression_ratio": 1.7633928571428572, "no_speech_prob": 5.594189133262262e-06}, {"id": 1139, "seek": 402796, "start": 4053.96, "end": 4054.96, "text": " So it starts out 10 times smaller.", "tokens": [407, 309, 3719, 484, 1266, 1413, 4356, 13], "temperature": 0.0, "avg_logprob": -0.10100089586698092, "compression_ratio": 1.7633928571428572, "no_speech_prob": 5.594189133262262e-06}, {"id": 1140, "seek": 405496, "start": 4054.96, "end": 4061.96, "text": " So this is just a way of saying like, if you're anything like me, every time you write code, it will always be wrong.", "tokens": [407, 341, 307, 445, 257, 636, 295, 1566, 411, 11, 498, 291, 434, 1340, 411, 385, 11, 633, 565, 291, 2464, 3089, 11, 309, 486, 1009, 312, 2085, 13], "temperature": 0.0, "avg_logprob": -0.09251893418175834, "compression_ratio": 1.71875, "no_speech_prob": 1.7776936147129163e-05}, {"id": 1141, "seek": 405496, "start": 4062.96, "end": 4065.96, "text": " And for this kind of code, you won't know it's wrong.", "tokens": [400, 337, 341, 733, 295, 3089, 11, 291, 1582, 380, 458, 309, 311, 2085, 13], "temperature": 0.0, "avg_logprob": -0.09251893418175834, "compression_ratio": 1.71875, "no_speech_prob": 1.7776936147129163e-05}, {"id": 1142, "seek": 405496, "start": 4066.96, "end": 4074.96, "text": " And you could be writing a paper or doing a project at work or whatever in which you're not using discriminative learning rates at all because of some bug.", "tokens": [400, 291, 727, 312, 3579, 257, 3035, 420, 884, 257, 1716, 412, 589, 420, 2035, 294, 597, 291, 434, 406, 1228, 20828, 1166, 2539, 6846, 412, 439, 570, 295, 512, 7426, 13], "temperature": 0.0, "avg_logprob": -0.09251893418175834, "compression_ratio": 1.71875, "no_speech_prob": 1.7776936147129163e-05}, {"id": 1143, "seek": 405496, "start": 4075.96, "end": 4076.96, "text": " Because you didn't know how to check.", "tokens": [1436, 291, 994, 380, 458, 577, 281, 1520, 13], "temperature": 0.0, "avg_logprob": -0.09251893418175834, "compression_ratio": 1.71875, "no_speech_prob": 1.7776936147129163e-05}, {"id": 1144, "seek": 405496, "start": 4077.96, "end": 4081.96, "text": " So make sure you can check and always assume that you screw up everything.", "tokens": [407, 652, 988, 291, 393, 1520, 293, 1009, 6552, 300, 291, 5630, 493, 1203, 13], "temperature": 0.0, "avg_logprob": -0.09251893418175834, "compression_ratio": 1.71875, "no_speech_prob": 1.7776936147129163e-05}, {"id": 1145, "seek": 408196, "start": 4081.96, "end": 4091.96, "text": " Okay, so now we can train with zero learning rate on the first layer group.", "tokens": [1033, 11, 370, 586, 321, 393, 3847, 365, 4018, 2539, 3314, 322, 264, 700, 4583, 1594, 13], "temperature": 0.0, "avg_logprob": -0.14839787178851188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.800131581665482e-06}, {"id": 1146, "seek": 408196, "start": 4092.96, "end": 4096.96, "text": " And then we can use discriminative learning rates with one in neg three and one in neg two and train a little bit more.", "tokens": [400, 550, 321, 393, 764, 20828, 1166, 2539, 6846, 365, 472, 294, 2485, 1045, 293, 472, 294, 2485, 732, 293, 3847, 257, 707, 857, 544, 13], "temperature": 0.0, "avg_logprob": -0.14839787178851188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.800131581665482e-06}, {"id": 1147, "seek": 408196, "start": 4097.96, "end": 4098.96, "text": " And that all works.", "tokens": [400, 300, 439, 1985, 13], "temperature": 0.0, "avg_logprob": -0.14839787178851188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.800131581665482e-06}, {"id": 1148, "seek": 408196, "start": 4099.96, "end": 4103.96, "text": " Okay, so that's all the tweaks we have.", "tokens": [1033, 11, 370, 300, 311, 439, 264, 46664, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.14839787178851188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.800131581665482e-06}, {"id": 1149, "seek": 408196, "start": 4104.96, "end": 4105.96, "text": " Any questions, Rachel?", "tokens": [2639, 1651, 11, 14246, 30], "temperature": 0.0, "avg_logprob": -0.14839787178851188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.800131581665482e-06}, {"id": 1150, "seek": 408196, "start": 4106.96, "end": 4107.96, "text": " We had two tangential questions come up.", "tokens": [492, 632, 732, 10266, 2549, 1651, 808, 493, 13], "temperature": 0.0, "avg_logprob": -0.14839787178851188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.800131581665482e-06}, {"id": 1151, "seek": 408196, "start": 4108.96, "end": 4109.96, "text": " They're my favorite.", "tokens": [814, 434, 452, 2954, 13], "temperature": 0.0, "avg_logprob": -0.14839787178851188, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.800131581665482e-06}, {"id": 1152, "seek": 410996, "start": 4109.96, "end": 4113.96, "text": " The first is we heard that you're against cross validation for deep learning.", "tokens": [440, 700, 307, 321, 2198, 300, 291, 434, 1970, 3278, 24071, 337, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.10947841689700172, "compression_ratio": 1.9489795918367347, "no_speech_prob": 3.5339446185389534e-05}, {"id": 1153, "seek": 410996, "start": 4114.96, "end": 4119.96, "text": " We heard that you're against cross validation for deep learning and wanted to know why that is.", "tokens": [492, 2198, 300, 291, 434, 1970, 3278, 24071, 337, 2452, 2539, 293, 1415, 281, 458, 983, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.10947841689700172, "compression_ratio": 1.9489795918367347, "no_speech_prob": 3.5339446185389534e-05}, {"id": 1154, "seek": 410996, "start": 4120.96, "end": 4121.96, "text": " And the second question.", "tokens": [400, 264, 1150, 1168, 13], "temperature": 0.0, "avg_logprob": -0.10947841689700172, "compression_ratio": 1.9489795918367347, "no_speech_prob": 3.5339446185389534e-05}, {"id": 1155, "seek": 410996, "start": 4122.96, "end": 4123.96, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.10947841689700172, "compression_ratio": 1.9489795918367347, "no_speech_prob": 3.5339446185389534e-05}, {"id": 1156, "seek": 410996, "start": 4125.96, "end": 4134.96, "text": " So cross validation is a very useful technique for getting a reasonably sized validation set if you don't have enough data to otherwise create a reasonably sized validation set.", "tokens": [407, 3278, 24071, 307, 257, 588, 4420, 6532, 337, 1242, 257, 23551, 20004, 24071, 992, 498, 291, 500, 380, 362, 1547, 1412, 281, 5911, 1884, 257, 23551, 20004, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.10947841689700172, "compression_ratio": 1.9489795918367347, "no_speech_prob": 3.5339446185389534e-05}, {"id": 1157, "seek": 413496, "start": 4134.96, "end": 4140.96, "text": " So it's particularly popular in the days when most studies were like 50 or 60 rows.", "tokens": [407, 309, 311, 4098, 3743, 294, 264, 1708, 562, 881, 5313, 645, 411, 2625, 420, 4060, 13241, 13], "temperature": 0.0, "avg_logprob": -0.11098741828848463, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.529224942321889e-06}, {"id": 1158, "seek": 413496, "start": 4141.96, "end": 4145.96, "text": " If you've got a few thousand rows, it's just pointless, right?", "tokens": [759, 291, 600, 658, 257, 1326, 4714, 13241, 11, 309, 311, 445, 32824, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11098741828848463, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.529224942321889e-06}, {"id": 1159, "seek": 413496, "start": 4146.96, "end": 4149.96, "text": " Like the kind of statistical significance is going to be there regardless.", "tokens": [1743, 264, 733, 295, 22820, 17687, 307, 516, 281, 312, 456, 10060, 13], "temperature": 0.0, "avg_logprob": -0.11098741828848463, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.529224942321889e-06}, {"id": 1160, "seek": 413496, "start": 4150.96, "end": 4151.96, "text": " So I wouldn't say I'm against it.", "tokens": [407, 286, 2759, 380, 584, 286, 478, 1970, 309, 13], "temperature": 0.0, "avg_logprob": -0.11098741828848463, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.529224942321889e-06}, {"id": 1161, "seek": 413496, "start": 4152.96, "end": 4160.96, "text": " Just most of the time you don't need it because if you've got a thousand things in the validation set and you only care whether it's like plus or minus 1%, it's totally pointless.", "tokens": [1449, 881, 295, 264, 565, 291, 500, 380, 643, 309, 570, 498, 291, 600, 658, 257, 4714, 721, 294, 264, 24071, 992, 293, 291, 787, 1127, 1968, 309, 311, 411, 1804, 420, 3175, 502, 8923, 309, 311, 3879, 32824, 13], "temperature": 0.0, "avg_logprob": -0.11098741828848463, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.529224942321889e-06}, {"id": 1162, "seek": 416096, "start": 4160.96, "end": 4166.96, "text": " So yeah, have a look and see how much your validation set accuracy is varying from run to run.", "tokens": [407, 1338, 11, 362, 257, 574, 293, 536, 577, 709, 428, 24071, 992, 14170, 307, 22984, 490, 1190, 281, 1190, 13], "temperature": 0.0, "avg_logprob": -0.09392120838165283, "compression_ratio": 1.5317073170731708, "no_speech_prob": 6.43785688225762e-06}, {"id": 1163, "seek": 416096, "start": 4167.96, "end": 4173.96, "text": " And if it's too much that you can't make the decisions you need to make, then you can add cross validation.", "tokens": [400, 498, 309, 311, 886, 709, 300, 291, 393, 380, 652, 264, 5327, 291, 643, 281, 652, 11, 550, 291, 393, 909, 3278, 24071, 13], "temperature": 0.0, "avg_logprob": -0.09392120838165283, "compression_ratio": 1.5317073170731708, "no_speech_prob": 6.43785688225762e-06}, {"id": 1164, "seek": 416096, "start": 4175.96, "end": 4179.96, "text": " And what are your best tips for debugging deep learning?", "tokens": [400, 437, 366, 428, 1151, 6082, 337, 45592, 2452, 2539, 30], "temperature": 0.0, "avg_logprob": -0.09392120838165283, "compression_ratio": 1.5317073170731708, "no_speech_prob": 6.43785688225762e-06}, {"id": 1165, "seek": 416096, "start": 4183.96, "end": 4186.96, "text": " So Chris Latner asked me this today as well, actually.", "tokens": [407, 6688, 7354, 1193, 2351, 385, 341, 965, 382, 731, 11, 767, 13], "temperature": 0.0, "avg_logprob": -0.09392120838165283, "compression_ratio": 1.5317073170731708, "no_speech_prob": 6.43785688225762e-06}, {"id": 1166, "seek": 418696, "start": 4186.96, "end": 4193.96, "text": " So I'll answer the same I answered him, which is don't make mistakes in the first place.", "tokens": [407, 286, 603, 1867, 264, 912, 286, 10103, 796, 11, 597, 307, 500, 380, 652, 8038, 294, 264, 700, 1081, 13], "temperature": 0.0, "avg_logprob": -0.06955329320764028, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.646362165454775e-05}, {"id": 1167, "seek": 418696, "start": 4195.96, "end": 4206.96, "text": " And the only way to do that is to make your code so simple that it can't possibly have a mistake and to check every single intermediate result along the way to make sure it doesn't have a mistake.", "tokens": [400, 264, 787, 636, 281, 360, 300, 307, 281, 652, 428, 3089, 370, 2199, 300, 309, 393, 380, 6264, 362, 257, 6146, 293, 281, 1520, 633, 2167, 19376, 1874, 2051, 264, 636, 281, 652, 988, 309, 1177, 380, 362, 257, 6146, 13], "temperature": 0.0, "avg_logprob": -0.06955329320764028, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.646362165454775e-05}, {"id": 1168, "seek": 418696, "start": 4207.96, "end": 4211.96, "text": " Otherwise, your last month might have been like my last month.", "tokens": [10328, 11, 428, 1036, 1618, 1062, 362, 668, 411, 452, 1036, 1618, 13], "temperature": 0.0, "avg_logprob": -0.06955329320764028, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.646362165454775e-05}, {"id": 1169, "seek": 418696, "start": 4212.96, "end": 4213.96, "text": " What happened in my last month?", "tokens": [708, 2011, 294, 452, 1036, 1618, 30], "temperature": 0.0, "avg_logprob": -0.06955329320764028, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.646362165454775e-05}, {"id": 1170, "seek": 421396, "start": 4213.96, "end": 4221.96, "text": " Well, a month ago, I got 94.1% accuracy on ImageNet and I was very happy.", "tokens": [1042, 11, 257, 1618, 2057, 11, 286, 658, 30849, 13, 16, 4, 14170, 322, 29903, 31890, 293, 286, 390, 588, 2055, 13], "temperature": 0.0, "avg_logprob": -0.11556547824467454, "compression_ratio": 1.562753036437247, "no_speech_prob": 1.1299973266432062e-05}, {"id": 1171, "seek": 421396, "start": 4222.96, "end": 4227.96, "text": " And then I started a couple of weeks ago trying various tweaks and none of the tweaks seemed to help.", "tokens": [400, 550, 286, 1409, 257, 1916, 295, 3259, 2057, 1382, 3683, 46664, 293, 6022, 295, 264, 46664, 6576, 281, 854, 13], "temperature": 0.0, "avg_logprob": -0.11556547824467454, "compression_ratio": 1.562753036437247, "no_speech_prob": 1.1299973266432062e-05}, {"id": 1172, "seek": 421396, "start": 4228.96, "end": 4234.96, "text": " And after a while, I got so frustrated, I thought I'll just repeat the previous training to see if it was like what was going on with the fluke.", "tokens": [400, 934, 257, 1339, 11, 286, 658, 370, 15751, 11, 286, 1194, 286, 603, 445, 7149, 264, 3894, 3097, 281, 536, 498, 309, 390, 411, 437, 390, 516, 322, 365, 264, 5029, 330, 13], "temperature": 0.0, "avg_logprob": -0.11556547824467454, "compression_ratio": 1.562753036437247, "no_speech_prob": 1.1299973266432062e-05}, {"id": 1173, "seek": 421396, "start": 4235.96, "end": 4239.96, "text": " And I couldn't repeat it. I was now getting 93.5 instead of 94.1.", "tokens": [400, 286, 2809, 380, 7149, 309, 13, 286, 390, 586, 1242, 28876, 13, 20, 2602, 295, 30849, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.11556547824467454, "compression_ratio": 1.562753036437247, "no_speech_prob": 1.1299973266432062e-05}, {"id": 1174, "seek": 423996, "start": 4239.96, "end": 4247.96, "text": " And I trained it like a bunch of times. And every time I trained it, it was costing me $150 of AWS credits.", "tokens": [400, 286, 8895, 309, 411, 257, 3840, 295, 1413, 13, 400, 633, 565, 286, 8895, 309, 11, 309, 390, 37917, 385, 1848, 20120, 295, 17650, 16816, 13], "temperature": 0.0, "avg_logprob": -0.13644368869742168, "compression_ratio": 1.617391304347826, "no_speech_prob": 5.771114956587553e-06}, {"id": 1175, "seek": 423996, "start": 4248.96, "end": 4252.96, "text": " So I wasn't thrilled about this. And it was six hours of waiting.", "tokens": [407, 286, 2067, 380, 18744, 466, 341, 13, 400, 309, 390, 2309, 2496, 295, 3806, 13], "temperature": 0.0, "avg_logprob": -0.13644368869742168, "compression_ratio": 1.617391304347826, "no_speech_prob": 5.771114956587553e-06}, {"id": 1176, "seek": 423996, "start": 4253.96, "end": 4257.96, "text": " So that was quite a process to even realize like it's broken.", "tokens": [407, 300, 390, 1596, 257, 1399, 281, 754, 4325, 411, 309, 311, 5463, 13], "temperature": 0.0, "avg_logprob": -0.13644368869742168, "compression_ratio": 1.617391304347826, "no_speech_prob": 5.771114956587553e-06}, {"id": 1177, "seek": 423996, "start": 4258.96, "end": 4264.96, "text": " This is the kind of thing like when something when you've written that kind of code wrong, it gets broken in ways you don't even notice.", "tokens": [639, 307, 264, 733, 295, 551, 411, 562, 746, 562, 291, 600, 3720, 300, 733, 295, 3089, 2085, 11, 309, 2170, 5463, 294, 2098, 291, 500, 380, 754, 3449, 13], "temperature": 0.0, "avg_logprob": -0.13644368869742168, "compression_ratio": 1.617391304347826, "no_speech_prob": 5.771114956587553e-06}, {"id": 1178, "seek": 426496, "start": 4264.96, "end": 4269.96, "text": " It was broken for weeks in fast AI and nobody noticed.", "tokens": [467, 390, 5463, 337, 3259, 294, 2370, 7318, 293, 5079, 5694, 13], "temperature": 0.0, "avg_logprob": -0.12353498002757198, "compression_ratio": 1.5263157894736843, "no_speech_prob": 7.071581876516575e-06}, {"id": 1179, "seek": 426496, "start": 4270.96, "end": 4279.96, "text": " So eventually I realized, yeah, I mean, so the first thing I'll say is you've got to be a great scientist, which means you need a journal notebook, right?", "tokens": [407, 4728, 286, 5334, 11, 1338, 11, 286, 914, 11, 370, 264, 700, 551, 286, 603, 584, 307, 291, 600, 658, 281, 312, 257, 869, 12662, 11, 597, 1355, 291, 643, 257, 6708, 21060, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12353498002757198, "compression_ratio": 1.5263157894736843, "no_speech_prob": 7.071581876516575e-06}, {"id": 1180, "seek": 426496, "start": 4279.96, "end": 4282.96, "text": " You need to keep track of your journal results.", "tokens": [509, 643, 281, 1066, 2837, 295, 428, 6708, 3542, 13], "temperature": 0.0, "avg_logprob": -0.12353498002757198, "compression_ratio": 1.5263157894736843, "no_speech_prob": 7.071581876516575e-06}, {"id": 1181, "seek": 426496, "start": 4282.96, "end": 4287.96, "text": " So I had a good journal. I pasted everything that was going on, all my models into a file.", "tokens": [407, 286, 632, 257, 665, 6708, 13, 286, 1791, 292, 1203, 300, 390, 516, 322, 11, 439, 452, 5245, 666, 257, 3991, 13], "temperature": 0.0, "avg_logprob": -0.12353498002757198, "compression_ratio": 1.5263157894736843, "no_speech_prob": 7.071581876516575e-06}, {"id": 1182, "seek": 428796, "start": 4287.96, "end": 4293.96, "text": " So I went back and I confirmed it really was 94.1. I could see exactly when it was.", "tokens": [407, 286, 1437, 646, 293, 286, 11341, 309, 534, 390, 30849, 13, 16, 13, 286, 727, 536, 2293, 562, 309, 390, 13], "temperature": 0.0, "avg_logprob": -0.09901115331756935, "compression_ratio": 1.586734693877551, "no_speech_prob": 2.123313834090368e-06}, {"id": 1183, "seek": 428796, "start": 4294.96, "end": 4298.96, "text": " And so then I could revert to the exact commit that was in fast AI at that time.", "tokens": [400, 370, 550, 286, 727, 319, 3281, 281, 264, 1900, 5599, 300, 390, 294, 2370, 7318, 412, 300, 565, 13], "temperature": 0.0, "avg_logprob": -0.09901115331756935, "compression_ratio": 1.586734693877551, "no_speech_prob": 2.123313834090368e-06}, {"id": 1184, "seek": 428796, "start": 4299.96, "end": 4301.96, "text": " And I reran it and I got 94.1.", "tokens": [400, 286, 43819, 282, 309, 293, 286, 658, 30849, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.09901115331756935, "compression_ratio": 1.586734693877551, "no_speech_prob": 2.123313834090368e-06}, {"id": 1185, "seek": 428796, "start": 4303.96, "end": 4312.96, "text": " So I now had to figure out which change in the previous month of the entire fast AI code base caused this to break.", "tokens": [407, 286, 586, 632, 281, 2573, 484, 597, 1319, 294, 264, 3894, 1618, 295, 264, 2302, 2370, 7318, 3089, 3096, 7008, 341, 281, 1821, 13], "temperature": 0.0, "avg_logprob": -0.09901115331756935, "compression_ratio": 1.586734693877551, "no_speech_prob": 2.123313834090368e-06}, {"id": 1186, "seek": 431296, "start": 4312.96, "end": 4316.96, "text": " So the first thing I tried to do is try to find a way to quickly figure out whether something is broken.", "tokens": [407, 264, 700, 551, 286, 3031, 281, 360, 307, 853, 281, 915, 257, 636, 281, 2661, 2573, 484, 1968, 746, 307, 5463, 13], "temperature": 0.0, "avg_logprob": -0.11385607301143177, "compression_ratio": 1.6630434782608696, "no_speech_prob": 5.4217771321418695e-06}, {"id": 1187, "seek": 431296, "start": 4317.96, "end": 4326.96, "text": " But after doing a few runs and plotting them in Excel, it was very clear that the training was identical until epoch 50.", "tokens": [583, 934, 884, 257, 1326, 6676, 293, 41178, 552, 294, 19060, 11, 309, 390, 588, 1850, 300, 264, 3097, 390, 14800, 1826, 30992, 339, 2625, 13], "temperature": 0.0, "avg_logprob": -0.11385607301143177, "compression_ratio": 1.6630434782608696, "no_speech_prob": 5.4217771321418695e-06}, {"id": 1188, "seek": 431296, "start": 4326.96, "end": 4328.96, "text": " So until epoch 50 out of 60.", "tokens": [407, 1826, 30992, 339, 2625, 484, 295, 4060, 13], "temperature": 0.0, "avg_logprob": -0.11385607301143177, "compression_ratio": 1.6630434782608696, "no_speech_prob": 5.4217771321418695e-06}, {"id": 1189, "seek": 431296, "start": 4329.96, "end": 4331.96, "text": " So there was no shortcut.", "tokens": [407, 456, 390, 572, 24822, 13], "temperature": 0.0, "avg_logprob": -0.11385607301143177, "compression_ratio": 1.6630434782608696, "no_speech_prob": 5.4217771321418695e-06}, {"id": 1190, "seek": 431296, "start": 4331.96, "end": 4340.96, "text": " And so I did a bisection search one module at a time, looking through the 15 modules that had changed in that diff until I eventually I find it was in the mixed precision module.", "tokens": [400, 370, 286, 630, 257, 7393, 10183, 3164, 472, 10088, 412, 257, 565, 11, 1237, 807, 264, 2119, 16679, 300, 632, 3105, 294, 300, 7593, 1826, 286, 4728, 286, 915, 309, 390, 294, 264, 7467, 18356, 10088, 13], "temperature": 0.0, "avg_logprob": -0.11385607301143177, "compression_ratio": 1.6630434782608696, "no_speech_prob": 5.4217771321418695e-06}, {"id": 1191, "seek": 434096, "start": 4340.96, "end": 4343.96, "text": " And then I went through each change that happened in the mixed position module.", "tokens": [400, 550, 286, 1437, 807, 1184, 1319, 300, 2011, 294, 264, 7467, 2535, 10088, 13], "temperature": 0.0, "avg_logprob": -0.11219228744506836, "compression_ratio": 1.751054852320675, "no_speech_prob": 5.25485029356787e-06}, {"id": 1192, "seek": 434096, "start": 4343.96, "end": 4350.96, "text": " So like $5,000 later, I finally found the one line of code where we had forgotten to write the four letters dot opt.", "tokens": [407, 411, 1848, 20, 11, 1360, 1780, 11, 286, 2721, 1352, 264, 472, 1622, 295, 3089, 689, 321, 632, 11832, 281, 2464, 264, 1451, 7825, 5893, 2427, 13], "temperature": 0.0, "avg_logprob": -0.11219228744506836, "compression_ratio": 1.751054852320675, "no_speech_prob": 5.25485029356787e-06}, {"id": 1193, "seek": 434096, "start": 4351.96, "end": 4359.96, "text": " And so by failing to write dot opt, it meant that we were wrapping an optum wrapper in an optum wrapper rather than wrapping an optum wrapper with an optimizer.", "tokens": [400, 370, 538, 18223, 281, 2464, 5893, 2427, 11, 309, 4140, 300, 321, 645, 21993, 364, 2427, 449, 46906, 294, 364, 2427, 449, 46906, 2831, 813, 21993, 364, 2427, 449, 46906, 365, 364, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.11219228744506836, "compression_ratio": 1.751054852320675, "no_speech_prob": 5.25485029356787e-06}, {"id": 1194, "seek": 434096, "start": 4359.96, "end": 4362.96, "text": " And that meant that weight decay was being applied twice.", "tokens": [400, 300, 4140, 300, 3364, 21039, 390, 885, 6456, 6091, 13], "temperature": 0.0, "avg_logprob": -0.11219228744506836, "compression_ratio": 1.751054852320675, "no_speech_prob": 5.25485029356787e-06}, {"id": 1195, "seek": 436296, "start": 4362.96, "end": 4370.96, "text": " So that tiny difference was so insignificant that no one using the library even noticed it wasn't working.", "tokens": [407, 300, 5870, 2649, 390, 370, 43685, 300, 572, 472, 1228, 264, 6405, 754, 5694, 309, 2067, 380, 1364, 13], "temperature": 0.0, "avg_logprob": -0.15191946960077052, "compression_ratio": 1.5645933014354068, "no_speech_prob": 2.6838847588805947e-06}, {"id": 1196, "seek": 436296, "start": 4370.96, "end": 4381.96, "text": " I didn't notice it wasn't working until I started trying to get state of the art results on ImageNet in the 60 epochs with ResNet 50.", "tokens": [286, 994, 380, 3449, 309, 2067, 380, 1364, 1826, 286, 1409, 1382, 281, 483, 1785, 295, 264, 1523, 3542, 322, 29903, 31890, 294, 264, 4060, 30992, 28346, 365, 5015, 31890, 2625, 13], "temperature": 0.0, "avg_logprob": -0.15191946960077052, "compression_ratio": 1.5645933014354068, "no_speech_prob": 2.6838847588805947e-06}, {"id": 1197, "seek": 436296, "start": 4382.96, "end": 4389.96, "text": " So yeah, I mean, debugging is hard and worse still is most of the time you don't know.", "tokens": [407, 1338, 11, 286, 914, 11, 45592, 307, 1152, 293, 5324, 920, 307, 881, 295, 264, 565, 291, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.15191946960077052, "compression_ratio": 1.5645933014354068, "no_speech_prob": 2.6838847588805947e-06}, {"id": 1198, "seek": 438996, "start": 4389.96, "end": 4397.96, "text": " So I mean, honestly, training models sucks and deep learning is a miserable experience and you shouldn't do it.", "tokens": [407, 286, 914, 11, 6095, 11, 3097, 5245, 15846, 293, 2452, 2539, 307, 257, 22321, 1752, 293, 291, 4659, 380, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.08388599840182702, "compression_ratio": 1.603846153846154, "no_speech_prob": 2.546045470808167e-05}, {"id": 1199, "seek": 438996, "start": 4397.96, "end": 4403.96, "text": " But on the other hand, it gives you much better results than anything else and it's taking over the world.", "tokens": [583, 322, 264, 661, 1011, 11, 309, 2709, 291, 709, 1101, 3542, 813, 1340, 1646, 293, 309, 311, 1940, 670, 264, 1002, 13], "temperature": 0.0, "avg_logprob": -0.08388599840182702, "compression_ratio": 1.603846153846154, "no_speech_prob": 2.546045470808167e-05}, {"id": 1200, "seek": 438996, "start": 4403.96, "end": 4407.96, "text": " So it's either that or get eaten by everybody else, I guess.", "tokens": [407, 309, 311, 2139, 300, 420, 483, 12158, 538, 2201, 1646, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.08388599840182702, "compression_ratio": 1.603846153846154, "no_speech_prob": 2.546045470808167e-05}, {"id": 1201, "seek": 438996, "start": 4407.96, "end": 4415.96, "text": " So yeah, I mean, it's so much easier to write normal code where like, oh, you have to implement OAuth authentication in your web service.", "tokens": [407, 1338, 11, 286, 914, 11, 309, 311, 370, 709, 3571, 281, 2464, 2710, 3089, 689, 411, 11, 1954, 11, 291, 362, 281, 4445, 48424, 2910, 26643, 294, 428, 3670, 2643, 13], "temperature": 0.0, "avg_logprob": -0.08388599840182702, "compression_ratio": 1.603846153846154, "no_speech_prob": 2.546045470808167e-05}, {"id": 1202, "seek": 441596, "start": 4415.96, "end": 4420.96, "text": " And so you go in and you say, oh, here's the API and we have to take these five steps.", "tokens": [400, 370, 291, 352, 294, 293, 291, 584, 11, 1954, 11, 510, 311, 264, 9362, 293, 321, 362, 281, 747, 613, 1732, 4439, 13], "temperature": 0.0, "avg_logprob": -0.09583435853322347, "compression_ratio": 1.6917293233082706, "no_speech_prob": 2.177897113142535e-05}, {"id": 1203, "seek": 441596, "start": 4420.96, "end": 4423.96, "text": " And after each one, I check that this has happened and you check off each one.", "tokens": [400, 934, 1184, 472, 11, 286, 1520, 300, 341, 575, 2011, 293, 291, 1520, 766, 1184, 472, 13], "temperature": 0.0, "avg_logprob": -0.09583435853322347, "compression_ratio": 1.6917293233082706, "no_speech_prob": 2.177897113142535e-05}, {"id": 1204, "seek": 441596, "start": 4423.96, "end": 4427.96, "text": " And at the end, you're done and you push it and you have integration tests and that's it.", "tokens": [400, 412, 264, 917, 11, 291, 434, 1096, 293, 291, 2944, 309, 293, 291, 362, 10980, 6921, 293, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.09583435853322347, "compression_ratio": 1.6917293233082706, "no_speech_prob": 2.177897113142535e-05}, {"id": 1205, "seek": 441596, "start": 4427.96, "end": 4431.96, "text": " Even testing, it requires a totally different mindset.", "tokens": [2754, 4997, 11, 309, 7029, 257, 3879, 819, 12543, 13], "temperature": 0.0, "avg_logprob": -0.09583435853322347, "compression_ratio": 1.6917293233082706, "no_speech_prob": 2.177897113142535e-05}, {"id": 1206, "seek": 441596, "start": 4431.96, "end": 4435.96, "text": " So you don't want reproducible tests.", "tokens": [407, 291, 500, 380, 528, 11408, 32128, 6921, 13], "temperature": 0.0, "avg_logprob": -0.09583435853322347, "compression_ratio": 1.6917293233082706, "no_speech_prob": 2.177897113142535e-05}, {"id": 1207, "seek": 441596, "start": 4435.96, "end": 4436.96, "text": " You want tests with randomness.", "tokens": [509, 528, 6921, 365, 4974, 1287, 13], "temperature": 0.0, "avg_logprob": -0.09583435853322347, "compression_ratio": 1.6917293233082706, "no_speech_prob": 2.177897113142535e-05}, {"id": 1208, "seek": 441596, "start": 4436.96, "end": 4439.96, "text": " You want to be able to see if something's changing just occasionally.", "tokens": [509, 528, 281, 312, 1075, 281, 536, 498, 746, 311, 4473, 445, 16895, 13], "temperature": 0.0, "avg_logprob": -0.09583435853322347, "compression_ratio": 1.6917293233082706, "no_speech_prob": 2.177897113142535e-05}, {"id": 1209, "seek": 443996, "start": 4439.96, "end": 4446.96, "text": " Because if it only tests, if it tests correctly all the time with a random seed of 42, you're sure it's going to work with a random seed of 41.", "tokens": [1436, 498, 309, 787, 6921, 11, 498, 309, 6921, 8944, 439, 264, 565, 365, 257, 4974, 8871, 295, 14034, 11, 291, 434, 988, 309, 311, 516, 281, 589, 365, 257, 4974, 8871, 295, 18173, 13], "temperature": 0.0, "avg_logprob": -0.09197612536155571, "compression_ratio": 1.713235294117647, "no_speech_prob": 4.860140052187489e-06}, {"id": 1210, "seek": 443996, "start": 4446.96, "end": 4449.96, "text": " So you want like non-reproducible tests.", "tokens": [407, 291, 528, 411, 2107, 12, 265, 14314, 32128, 6921, 13], "temperature": 0.0, "avg_logprob": -0.09197612536155571, "compression_ratio": 1.713235294117647, "no_speech_prob": 4.860140052187489e-06}, {"id": 1211, "seek": 443996, "start": 4449.96, "end": 4450.96, "text": " You want randomness.", "tokens": [509, 528, 4974, 1287, 13], "temperature": 0.0, "avg_logprob": -0.09197612536155571, "compression_ratio": 1.713235294117647, "no_speech_prob": 4.860140052187489e-06}, {"id": 1212, "seek": 443996, "start": 4450.96, "end": 4458.96, "text": " You want tests that aren't guaranteed to always pass, but like the accuracy of this integration test should be better than 0.9 nearly all the time.", "tokens": [509, 528, 6921, 300, 3212, 380, 18031, 281, 1009, 1320, 11, 457, 411, 264, 14170, 295, 341, 10980, 1500, 820, 312, 1101, 813, 1958, 13, 24, 6217, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.09197612536155571, "compression_ratio": 1.713235294117647, "no_speech_prob": 4.860140052187489e-06}, {"id": 1213, "seek": 443996, "start": 4458.96, "end": 4463.96, "text": " You want to be warned if something looks off.", "tokens": [509, 528, 281, 312, 21284, 498, 746, 1542, 766, 13], "temperature": 0.0, "avg_logprob": -0.09197612536155571, "compression_ratio": 1.713235294117647, "no_speech_prob": 4.860140052187489e-06}, {"id": 1214, "seek": 443996, "start": 4463.96, "end": 4466.96, "text": " And this means it's a very different software development process.", "tokens": [400, 341, 1355, 309, 311, 257, 588, 819, 4722, 3250, 1399, 13], "temperature": 0.0, "avg_logprob": -0.09197612536155571, "compression_ratio": 1.713235294117647, "no_speech_prob": 4.860140052187489e-06}, {"id": 1215, "seek": 446696, "start": 4466.96, "end": 4472.96, "text": " Because if you push something to the Fast AI repo and a test fails, it might not be your fault.", "tokens": [1436, 498, 291, 2944, 746, 281, 264, 15968, 7318, 49040, 293, 257, 1500, 18199, 11, 309, 1062, 406, 312, 428, 7441, 13], "temperature": 0.0, "avg_logprob": -0.06511566855690697, "compression_ratio": 1.7161572052401746, "no_speech_prob": 8.663892003824003e-06}, {"id": 1216, "seek": 446696, "start": 4472.96, "end": 4477.96, "text": " It might be that Jeremy screwed something up a month ago and one test fails one out of every thousand times.", "tokens": [467, 1062, 312, 300, 17809, 20331, 746, 493, 257, 1618, 2057, 293, 472, 1500, 18199, 472, 484, 295, 633, 4714, 1413, 13], "temperature": 0.0, "avg_logprob": -0.06511566855690697, "compression_ratio": 1.7161572052401746, "no_speech_prob": 8.663892003824003e-06}, {"id": 1217, "seek": 446696, "start": 4477.96, "end": 4484.96, "text": " So as soon as that happens, then we try to write a test that fails every time.", "tokens": [407, 382, 2321, 382, 300, 2314, 11, 550, 321, 853, 281, 2464, 257, 1500, 300, 18199, 633, 565, 13], "temperature": 0.0, "avg_logprob": -0.06511566855690697, "compression_ratio": 1.7161572052401746, "no_speech_prob": 8.663892003824003e-06}, {"id": 1218, "seek": 446696, "start": 4484.96, "end": 4488.96, "text": " So like once you realize there's a problem with this thing, you try to find a way to make it fail every time.", "tokens": [407, 411, 1564, 291, 4325, 456, 311, 257, 1154, 365, 341, 551, 11, 291, 853, 281, 915, 257, 636, 281, 652, 309, 3061, 633, 565, 13], "temperature": 0.0, "avg_logprob": -0.06511566855690697, "compression_ratio": 1.7161572052401746, "no_speech_prob": 8.663892003824003e-06}, {"id": 1219, "seek": 448896, "start": 4488.96, "end": 4505.96, "text": " But it's yeah, debugging is difficult and in the end you just have to go through each step, look at your data, make sure it looks sensible, plot it, and try not to make mistakes in the first place.", "tokens": [583, 309, 311, 1338, 11, 45592, 307, 2252, 293, 294, 264, 917, 291, 445, 362, 281, 352, 807, 1184, 1823, 11, 574, 412, 428, 1412, 11, 652, 988, 309, 1542, 25380, 11, 7542, 309, 11, 293, 853, 406, 281, 652, 8038, 294, 264, 700, 1081, 13], "temperature": 0.0, "avg_logprob": -0.11527018479897942, "compression_ratio": 1.4364640883977902, "no_speech_prob": 1.406212686561048e-05}, {"id": 1220, "seek": 448896, "start": 4505.96, "end": 4511.96, "text": " Great. Well, let's have a break and see you back here at 7.55.", "tokens": [3769, 13, 1042, 11, 718, 311, 362, 257, 1821, 293, 536, 291, 646, 510, 412, 1614, 13, 13622, 13], "temperature": 0.0, "avg_logprob": -0.11527018479897942, "compression_ratio": 1.4364640883977902, "no_speech_prob": 1.406212686561048e-05}, {"id": 1221, "seek": 451196, "start": 4511.96, "end": 4519.96, "text": " So we've all done ULM fit in part one.", "tokens": [407, 321, 600, 439, 1096, 624, 43, 44, 3318, 294, 644, 472, 13], "temperature": 0.0, "avg_logprob": -0.20000188490923712, "compression_ratio": 1.2461538461538462, "no_speech_prob": 8.527190402674023e-06}, {"id": 1222, "seek": 451196, "start": 4519.96, "end": 4533.96, "text": " And there's been a lot of stuff happening in the oh, okay, let's do the question.", "tokens": [400, 456, 311, 668, 257, 688, 295, 1507, 2737, 294, 264, 1954, 11, 1392, 11, 718, 311, 360, 264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.20000188490923712, "compression_ratio": 1.2461538461538462, "no_speech_prob": 8.527190402674023e-06}, {"id": 1223, "seek": 451196, "start": 4533.96, "end": 4536.96, "text": " What do you mean by a scientific journal?", "tokens": [708, 360, 291, 914, 538, 257, 8134, 6708, 30], "temperature": 0.0, "avg_logprob": -0.20000188490923712, "compression_ratio": 1.2461538461538462, "no_speech_prob": 8.527190402674023e-06}, {"id": 1224, "seek": 453696, "start": 4536.96, "end": 4544.96, "text": " Ah, yeah, that's a good one. This is something I'm quite passionate about.", "tokens": [2438, 11, 1338, 11, 300, 311, 257, 665, 472, 13, 639, 307, 746, 286, 478, 1596, 11410, 466, 13], "temperature": 0.0, "avg_logprob": -0.08459678961306202, "compression_ratio": 1.3402777777777777, "no_speech_prob": 6.960330665606307e-06}, {"id": 1225, "seek": 453696, "start": 4544.96, "end": 4556.96, "text": " When you look at the great scientists in history, they all, that I can tell, had careful scientific journal practices.", "tokens": [1133, 291, 574, 412, 264, 869, 7708, 294, 2503, 11, 436, 439, 11, 300, 286, 393, 980, 11, 632, 5026, 8134, 6708, 7525, 13], "temperature": 0.0, "avg_logprob": -0.08459678961306202, "compression_ratio": 1.3402777777777777, "no_speech_prob": 6.960330665606307e-06}, {"id": 1226, "seek": 455696, "start": 4556.96, "end": 4567.96, "text": " In my case, my scientific journal is a file and a piece of software called Windows Notepad, and I paste things into it at the bottom.", "tokens": [682, 452, 1389, 11, 452, 8134, 6708, 307, 257, 3991, 293, 257, 2522, 295, 4722, 1219, 8591, 1726, 595, 345, 11, 293, 286, 9163, 721, 666, 309, 412, 264, 2767, 13], "temperature": 0.0, "avg_logprob": -0.07940121332804362, "compression_ratio": 1.5026178010471205, "no_speech_prob": 6.642551397817442e-06}, {"id": 1227, "seek": 455696, "start": 4567.96, "end": 4572.96, "text": " And when I want to find something, I press Ctrl F.", "tokens": [400, 562, 286, 528, 281, 915, 746, 11, 286, 1886, 35233, 479, 13], "temperature": 0.0, "avg_logprob": -0.07940121332804362, "compression_ratio": 1.5026178010471205, "no_speech_prob": 6.642551397817442e-06}, {"id": 1228, "seek": 455696, "start": 4572.96, "end": 4581.96, "text": " It just needs to be something that has a record of what you're doing and what the results of that are.", "tokens": [467, 445, 2203, 281, 312, 746, 300, 575, 257, 2136, 295, 437, 291, 434, 884, 293, 437, 264, 3542, 295, 300, 366, 13], "temperature": 0.0, "avg_logprob": -0.07940121332804362, "compression_ratio": 1.5026178010471205, "no_speech_prob": 6.642551397817442e-06}, {"id": 1229, "seek": 458196, "start": 4581.96, "end": 4592.96, "text": " Because scientists who make breakthroughs generally make the breakthrough because they look at something that shouldn't be, and they go, oh, that's odd.", "tokens": [1436, 7708, 567, 652, 22397, 82, 5101, 652, 264, 22397, 570, 436, 574, 412, 746, 300, 4659, 380, 312, 11, 293, 436, 352, 11, 1954, 11, 300, 311, 7401, 13], "temperature": 0.0, "avg_logprob": -0.08187784057065665, "compression_ratio": 1.7429906542056075, "no_speech_prob": 1.130026976170484e-05}, {"id": 1230, "seek": 458196, "start": 4592.96, "end": 4594.96, "text": " I wonder what's going on.", "tokens": [286, 2441, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.08187784057065665, "compression_ratio": 1.7429906542056075, "no_speech_prob": 1.130026976170484e-05}, {"id": 1231, "seek": 458196, "start": 4594.96, "end": 4604.96, "text": " So the discovery of the noble gases was because the scientists saw like one little bubble left in the beaker, which they were pretty sure there shouldn't have been a little bubble there anymore.", "tokens": [407, 264, 12114, 295, 264, 20171, 21452, 390, 570, 264, 7708, 1866, 411, 472, 707, 12212, 1411, 294, 264, 312, 4003, 11, 597, 436, 645, 1238, 988, 456, 4659, 380, 362, 668, 257, 707, 12212, 456, 3602, 13], "temperature": 0.0, "avg_logprob": -0.08187784057065665, "compression_ratio": 1.7429906542056075, "no_speech_prob": 1.130026976170484e-05}, {"id": 1232, "seek": 460496, "start": 4604.96, "end": 4611.96, "text": " You know, most people would just be like, oops, there's a bubble, or we wouldn't even notice, but they studied the bubble and they found noble gases.", "tokens": [509, 458, 11, 881, 561, 576, 445, 312, 411, 11, 34166, 11, 456, 311, 257, 12212, 11, 420, 321, 2759, 380, 754, 3449, 11, 457, 436, 9454, 264, 12212, 293, 436, 1352, 20171, 21452, 13], "temperature": 0.0, "avg_logprob": -0.12906567255655924, "compression_ratio": 1.4357541899441342, "no_speech_prob": 6.853756076452555e-06}, {"id": 1233, "seek": 460496, "start": 4611.96, "end": 4618.96, "text": " Or penicillin was discovered because of, oh, that's odd.", "tokens": [1610, 3435, 299, 373, 259, 390, 6941, 570, 295, 11, 1954, 11, 300, 311, 7401, 13], "temperature": 0.0, "avg_logprob": -0.12906567255655924, "compression_ratio": 1.4357541899441342, "no_speech_prob": 6.853756076452555e-06}, {"id": 1234, "seek": 460496, "start": 4618.96, "end": 4623.96, "text": " And I find in deep learning, this is true as well.", "tokens": [400, 286, 915, 294, 2452, 2539, 11, 341, 307, 2074, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12906567255655924, "compression_ratio": 1.4357541899441342, "no_speech_prob": 6.853756076452555e-06}, {"id": 1235, "seek": 462396, "start": 4623.96, "end": 4637.96, "text": " I spent a lot of time studying batch normalization and transfer learning because a few years ago in Keras, I was getting terrible transfer learning results for something I thought should be much more accurate.", "tokens": [286, 4418, 257, 688, 295, 565, 7601, 15245, 2710, 2144, 293, 5003, 2539, 570, 257, 1326, 924, 2057, 294, 591, 6985, 11, 286, 390, 1242, 6237, 5003, 2539, 3542, 337, 746, 286, 1194, 820, 312, 709, 544, 8559, 13], "temperature": 0.0, "avg_logprob": -0.09178700318207612, "compression_ratio": 1.6407766990291262, "no_speech_prob": 4.5656124711968005e-06}, {"id": 1236, "seek": 462396, "start": 4637.96, "end": 4640.96, "text": " And I thought, oh, that's odd.", "tokens": [400, 286, 1194, 11, 1954, 11, 300, 311, 7401, 13], "temperature": 0.0, "avg_logprob": -0.09178700318207612, "compression_ratio": 1.6407766990291262, "no_speech_prob": 4.5656124711968005e-06}, {"id": 1237, "seek": 462396, "start": 4640.96, "end": 4650.96, "text": " And I spent weeks changing everything I could and then almost randomly tried changing batch norm.", "tokens": [400, 286, 4418, 3259, 4473, 1203, 286, 727, 293, 550, 1920, 16979, 3031, 4473, 15245, 2026, 13], "temperature": 0.0, "avg_logprob": -0.09178700318207612, "compression_ratio": 1.6407766990291262, "no_speech_prob": 4.5656124711968005e-06}, {"id": 1238, "seek": 465096, "start": 4650.96, "end": 4665.96, "text": " So the problem is that all this fiddling around, you know, 90% of it doesn't really go anywhere, but it's the other 10% that you won't be able to pick it out unless you can go back and say, like, OK, that really did happen.", "tokens": [407, 264, 1154, 307, 300, 439, 341, 283, 14273, 1688, 926, 11, 291, 458, 11, 4289, 4, 295, 309, 1177, 380, 534, 352, 4992, 11, 457, 309, 311, 264, 661, 1266, 4, 300, 291, 1582, 380, 312, 1075, 281, 1888, 309, 484, 5969, 291, 393, 352, 646, 293, 584, 11, 411, 11, 2264, 11, 300, 534, 630, 1051, 13], "temperature": 0.0, "avg_logprob": -0.09982548962842237, "compression_ratio": 1.5296442687747036, "no_speech_prob": 2.3168653569882736e-05}, {"id": 1239, "seek": 465096, "start": 4665.96, "end": 4669.96, "text": " I copied and pasted the log here.", "tokens": [286, 25365, 293, 1791, 292, 264, 3565, 510, 13], "temperature": 0.0, "avg_logprob": -0.09982548962842237, "compression_ratio": 1.5296442687747036, "no_speech_prob": 2.3168653569882736e-05}, {"id": 1240, "seek": 465096, "start": 4669.96, "end": 4671.96, "text": " So that's all I mean.", "tokens": [407, 300, 311, 439, 286, 914, 13], "temperature": 0.0, "avg_logprob": -0.09982548962842237, "compression_ratio": 1.5296442687747036, "no_speech_prob": 2.3168653569882736e-05}, {"id": 1241, "seek": 465096, "start": 4671.96, "end": 4675.96, "text": " Are you also linking to your GitHub commits and datasets?", "tokens": [2014, 291, 611, 25775, 281, 428, 23331, 48311, 293, 42856, 30], "temperature": 0.0, "avg_logprob": -0.09982548962842237, "compression_ratio": 1.5296442687747036, "no_speech_prob": 2.3168653569882736e-05}, {"id": 1242, "seek": 465096, "start": 4675.96, "end": 4678.96, "text": " No, because I've got the date there and the time.", "tokens": [883, 11, 570, 286, 600, 658, 264, 4002, 456, 293, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.09982548962842237, "compression_ratio": 1.5296442687747036, "no_speech_prob": 2.3168653569882736e-05}, {"id": 1243, "seek": 467896, "start": 4678.96, "end": 4680.96, "text": " So I know the GitHub commit.", "tokens": [407, 286, 458, 264, 23331, 5599, 13], "temperature": 0.0, "avg_logprob": -0.12575141059027778, "compression_ratio": 1.3581395348837209, "no_speech_prob": 2.2468138922704384e-05}, {"id": 1244, "seek": 467896, "start": 4680.96, "end": 4686.96, "text": " So I do make sure I'm pushing all the time.", "tokens": [407, 286, 360, 652, 988, 286, 478, 7380, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.12575141059027778, "compression_ratio": 1.3581395348837209, "no_speech_prob": 2.2468138922704384e-05}, {"id": 1245, "seek": 467896, "start": 4686.96, "end": 4688.96, "text": " So, yeah.", "tokens": [407, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.12575141059027778, "compression_ratio": 1.3581395348837209, "no_speech_prob": 2.2468138922704384e-05}, {"id": 1246, "seek": 467896, "start": 4688.96, "end": 4692.96, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.12575141059027778, "compression_ratio": 1.3581395348837209, "no_speech_prob": 2.2468138922704384e-05}, {"id": 1247, "seek": 467896, "start": 4692.96, "end": 4695.96, "text": " Yeah, so there's been a lot happening in NLP transfer learning recently.", "tokens": [865, 11, 370, 456, 311, 668, 257, 688, 2737, 294, 426, 45196, 5003, 2539, 3938, 13], "temperature": 0.0, "avg_logprob": -0.12575141059027778, "compression_ratio": 1.3581395348837209, "no_speech_prob": 2.2468138922704384e-05}, {"id": 1248, "seek": 467896, "start": 4695.96, "end": 4699.96, "text": " The famous GPT-2 from OpenAI and BERT and stuff like that.", "tokens": [440, 4618, 26039, 51, 12, 17, 490, 7238, 48698, 293, 363, 31479, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.12575141059027778, "compression_ratio": 1.3581395348837209, "no_speech_prob": 2.2468138922704384e-05}, {"id": 1249, "seek": 467896, "start": 4699.96, "end": 4707.96, "text": " Lots of interest in transformers, which we will cover in a future lesson.", "tokens": [15908, 295, 1179, 294, 4088, 433, 11, 597, 321, 486, 2060, 294, 257, 2027, 6898, 13], "temperature": 0.0, "avg_logprob": -0.12575141059027778, "compression_ratio": 1.3581395348837209, "no_speech_prob": 2.2468138922704384e-05}, {"id": 1250, "seek": 470796, "start": 4707.96, "end": 4713.96, "text": " One could think that LSTMs are out of favor and not interesting anymore.", "tokens": [1485, 727, 519, 300, 441, 6840, 26386, 366, 484, 295, 2294, 293, 406, 1880, 3602, 13], "temperature": 0.0, "avg_logprob": -0.09662766640002911, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.3839493476552889e-05}, {"id": 1251, "seek": 470796, "start": 4713.96, "end": 4722.96, "text": " But when you look at actually recent competitive machine learning results, you see ULMfit beating BERT.", "tokens": [583, 562, 291, 574, 412, 767, 5162, 10043, 3479, 2539, 3542, 11, 291, 536, 624, 43, 44, 6845, 13497, 363, 31479, 13], "temperature": 0.0, "avg_logprob": -0.09662766640002911, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.3839493476552889e-05}, {"id": 1252, "seek": 470796, "start": 4722.96, "end": 4725.96, "text": " Now, I should say this is not just ULMfit beating BERT.", "tokens": [823, 11, 286, 820, 584, 341, 307, 406, 445, 624, 43, 44, 6845, 13497, 363, 31479, 13], "temperature": 0.0, "avg_logprob": -0.09662766640002911, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.3839493476552889e-05}, {"id": 1253, "seek": 470796, "start": 4725.96, "end": 4729.96, "text": " The guys at MWaves are super smart, amazing people.", "tokens": [440, 1074, 412, 376, 54, 5423, 366, 1687, 4069, 11, 2243, 561, 13], "temperature": 0.0, "avg_logprob": -0.09662766640002911, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.3839493476552889e-05}, {"id": 1254, "seek": 470796, "start": 4729.96, "end": 4735.96, "text": " So it's like two super smart, amazing people using ULMfit, but some other people doing BERT.", "tokens": [407, 309, 311, 411, 732, 1687, 4069, 11, 2243, 561, 1228, 624, 43, 44, 6845, 11, 457, 512, 661, 561, 884, 363, 31479, 13], "temperature": 0.0, "avg_logprob": -0.09662766640002911, "compression_ratio": 1.6111111111111112, "no_speech_prob": 1.3839493476552889e-05}, {"id": 1255, "seek": 473596, "start": 4735.96, "end": 4741.96, "text": " It's definitely not true that RNNs are in the past.", "tokens": [467, 311, 2138, 406, 2074, 300, 45702, 45, 82, 366, 294, 264, 1791, 13], "temperature": 0.0, "avg_logprob": -0.09221481524015727, "compression_ratio": 1.5454545454545454, "no_speech_prob": 7.070230822137091e-06}, {"id": 1256, "seek": 473596, "start": 4741.96, "end": 4750.96, "text": " I think what's happened is, in fact, as you'll see, transformers and CNNs for text have a lot of problems.", "tokens": [286, 519, 437, 311, 2011, 307, 11, 294, 1186, 11, 382, 291, 603, 536, 11, 4088, 433, 293, 24859, 82, 337, 2487, 362, 257, 688, 295, 2740, 13], "temperature": 0.0, "avg_logprob": -0.09221481524015727, "compression_ratio": 1.5454545454545454, "no_speech_prob": 7.070230822137091e-06}, {"id": 1257, "seek": 473596, "start": 4750.96, "end": 4753.96, "text": " They basically, they don't have state.", "tokens": [814, 1936, 11, 436, 500, 380, 362, 1785, 13], "temperature": 0.0, "avg_logprob": -0.09221481524015727, "compression_ratio": 1.5454545454545454, "no_speech_prob": 7.070230822137091e-06}, {"id": 1258, "seek": 473596, "start": 4753.96, "end": 4763.96, "text": " So like if you're doing speech recognition, every sample you look at, you have to do an entire analysis of all the samples around it again and again and again.", "tokens": [407, 411, 498, 291, 434, 884, 6218, 11150, 11, 633, 6889, 291, 574, 412, 11, 291, 362, 281, 360, 364, 2302, 5215, 295, 439, 264, 10938, 926, 309, 797, 293, 797, 293, 797, 13], "temperature": 0.0, "avg_logprob": -0.09221481524015727, "compression_ratio": 1.5454545454545454, "no_speech_prob": 7.070230822137091e-06}, {"id": 1259, "seek": 476396, "start": 4763.96, "end": 4766.96, "text": " Like it's ridiculously wasteful.", "tokens": [1743, 309, 311, 41358, 5964, 906, 13], "temperature": 0.0, "avg_logprob": -0.11252805319699374, "compression_ratio": 1.55, "no_speech_prob": 1.4968084542488214e-05}, {"id": 1260, "seek": 476396, "start": 4766.96, "end": 4769.96, "text": " Or else RNNs have state.", "tokens": [1610, 1646, 45702, 45, 82, 362, 1785, 13], "temperature": 0.0, "avg_logprob": -0.11252805319699374, "compression_ratio": 1.55, "no_speech_prob": 1.4968084542488214e-05}, {"id": 1261, "seek": 476396, "start": 4769.96, "end": 4772.96, "text": " But they're fiddly.", "tokens": [583, 436, 434, 283, 14273, 356, 13], "temperature": 0.0, "avg_logprob": -0.11252805319699374, "compression_ratio": 1.55, "no_speech_prob": 1.4968084542488214e-05}, {"id": 1262, "seek": 476396, "start": 4772.96, "end": 4780.96, "text": " And they're hard to deal with, as you'll see, when you want to actually do research and change things.", "tokens": [400, 436, 434, 1152, 281, 2028, 365, 11, 382, 291, 603, 536, 11, 562, 291, 528, 281, 767, 360, 2132, 293, 1319, 721, 13], "temperature": 0.0, "avg_logprob": -0.11252805319699374, "compression_ratio": 1.55, "no_speech_prob": 1.4968084542488214e-05}, {"id": 1263, "seek": 476396, "start": 4780.96, "end": 4792.96, "text": " But partly RNNs have state, but also partly RNNs are the only thing which has had the level of carefulness around regularization.", "tokens": [583, 17031, 45702, 45, 82, 362, 1785, 11, 457, 611, 17031, 45702, 45, 82, 366, 264, 787, 551, 597, 575, 632, 264, 1496, 295, 5026, 1287, 926, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.11252805319699374, "compression_ratio": 1.55, "no_speech_prob": 1.4968084542488214e-05}, {"id": 1264, "seek": 479296, "start": 4792.96, "end": 4795.96, "text": " That AWD LSTM did.", "tokens": [663, 25815, 35, 441, 6840, 44, 630, 13], "temperature": 0.0, "avg_logprob": -0.12238694045503261, "compression_ratio": 1.2967741935483872, "no_speech_prob": 2.0781510102096945e-05}, {"id": 1265, "seek": 479296, "start": 4795.96, "end": 4805.96, "text": " So Stephen Meridy looked at all the ways I can regularize this model and came up with a great set of hyperparameters for that.", "tokens": [407, 13391, 6124, 38836, 2956, 412, 439, 264, 2098, 286, 393, 3890, 1125, 341, 2316, 293, 1361, 493, 365, 257, 869, 992, 295, 9848, 2181, 335, 6202, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.12238694045503261, "compression_ratio": 1.2967741935483872, "no_speech_prob": 2.0781510102096945e-05}, {"id": 1266, "seek": 479296, "start": 4805.96, "end": 4811.96, "text": " And there's nothing like that outside of the RNN world.", "tokens": [400, 456, 311, 1825, 411, 300, 2380, 295, 264, 45702, 45, 1002, 13], "temperature": 0.0, "avg_logprob": -0.12238694045503261, "compression_ratio": 1.2967741935483872, "no_speech_prob": 2.0781510102096945e-05}, {"id": 1267, "seek": 481196, "start": 4811.96, "end": 4824.96, "text": " So at the moment, my go-to choice definitely is still ULM fit for most real world NLP tasks.", "tokens": [407, 412, 264, 1623, 11, 452, 352, 12, 1353, 3922, 2138, 307, 920, 624, 43, 44, 3318, 337, 881, 957, 1002, 426, 45196, 9608, 13], "temperature": 0.0, "avg_logprob": -0.09838039534432548, "compression_ratio": 1.4403669724770642, "no_speech_prob": 5.253930794424377e-06}, {"id": 1268, "seek": 481196, "start": 4824.96, "end": 4834.96, "text": " And if people find BERT or GPT-2 or whatever better for some real world tasks, that would be fascinating.", "tokens": [400, 498, 561, 915, 363, 31479, 420, 26039, 51, 12, 17, 420, 2035, 1101, 337, 512, 957, 1002, 9608, 11, 300, 576, 312, 10343, 13], "temperature": 0.0, "avg_logprob": -0.09838039534432548, "compression_ratio": 1.4403669724770642, "no_speech_prob": 5.253930794424377e-06}, {"id": 1269, "seek": 481196, "start": 4834.96, "end": 4835.96, "text": " I would love that to happen.", "tokens": [286, 576, 959, 300, 281, 1051, 13], "temperature": 0.0, "avg_logprob": -0.09838039534432548, "compression_ratio": 1.4403669724770642, "no_speech_prob": 5.253930794424377e-06}, {"id": 1270, "seek": 481196, "start": 4835.96, "end": 4839.96, "text": " But I haven't been hearing that from people that are actually working in industry yet.", "tokens": [583, 286, 2378, 380, 668, 4763, 300, 490, 561, 300, 366, 767, 1364, 294, 3518, 1939, 13], "temperature": 0.0, "avg_logprob": -0.09838039534432548, "compression_ratio": 1.4403669724770642, "no_speech_prob": 5.253930794424377e-06}, {"id": 1271, "seek": 483996, "start": 4839.96, "end": 4845.96, "text": " I'm not seeing them win competitive machine learning stuff and so forth.", "tokens": [286, 478, 406, 2577, 552, 1942, 10043, 3479, 2539, 1507, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.053786987953997674, "compression_ratio": 1.5352697095435686, "no_speech_prob": 8.267375960713252e-06}, {"id": 1272, "seek": 483996, "start": 4845.96, "end": 4849.96, "text": " So I still think RNNs should be our focus.", "tokens": [407, 286, 920, 519, 45702, 45, 82, 820, 312, 527, 1879, 13], "temperature": 0.0, "avg_logprob": -0.053786987953997674, "compression_ratio": 1.5352697095435686, "no_speech_prob": 8.267375960713252e-06}, {"id": 1273, "seek": 483996, "start": 4849.96, "end": 4852.96, "text": " But we will also learn about transformers later.", "tokens": [583, 321, 486, 611, 1466, 466, 4088, 433, 1780, 13], "temperature": 0.0, "avg_logprob": -0.053786987953997674, "compression_ratio": 1.5352697095435686, "no_speech_prob": 8.267375960713252e-06}, {"id": 1274, "seek": 483996, "start": 4852.96, "end": 4862.96, "text": " And so ULM fit is just the normal transfer learning path applied to an RNN, which could be on text.", "tokens": [400, 370, 624, 43, 44, 3318, 307, 445, 264, 2710, 5003, 2539, 3100, 6456, 281, 364, 45702, 45, 11, 597, 727, 312, 322, 2487, 13], "temperature": 0.0, "avg_logprob": -0.053786987953997674, "compression_ratio": 1.5352697095435686, "no_speech_prob": 8.267375960713252e-06}, {"id": 1275, "seek": 483996, "start": 4862.96, "end": 4868.96, "text": " But interestingly, there's also been a lot of state of the art results recently on genomics applications.", "tokens": [583, 25873, 11, 456, 311, 611, 668, 257, 688, 295, 1785, 295, 264, 1523, 3542, 3938, 322, 1049, 29884, 5821, 13], "temperature": 0.0, "avg_logprob": -0.053786987953997674, "compression_ratio": 1.5352697095435686, "no_speech_prob": 8.267375960713252e-06}, {"id": 1276, "seek": 486896, "start": 4868.96, "end": 4874.96, "text": " And on chemical bonding analysis and drug discovery.", "tokens": [400, 322, 7313, 28824, 5215, 293, 4110, 12114, 13], "temperature": 0.0, "avg_logprob": -0.07915796414770261, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.833976239140611e-05}, {"id": 1277, "seek": 486896, "start": 4874.96, "end": 4876.96, "text": " There's lots of things that are sequences.", "tokens": [821, 311, 3195, 295, 721, 300, 366, 22978, 13], "temperature": 0.0, "avg_logprob": -0.07915796414770261, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.833976239140611e-05}, {"id": 1278, "seek": 486896, "start": 4876.96, "end": 4881.96, "text": " And it turns out, and we're still just at the tip of the iceberg, right?", "tokens": [400, 309, 4523, 484, 11, 293, 321, 434, 920, 445, 412, 264, 4125, 295, 264, 38880, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.07915796414770261, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.833976239140611e-05}, {"id": 1279, "seek": 486896, "start": 4881.96, "end": 4888.96, "text": " Because most people that are studying like drug discovery or chemical bonding or genomics have never heard of ULM fit, right?", "tokens": [1436, 881, 561, 300, 366, 7601, 411, 4110, 12114, 420, 7313, 28824, 420, 1049, 29884, 362, 1128, 2198, 295, 624, 43, 44, 3318, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.07915796414770261, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.833976239140611e-05}, {"id": 1280, "seek": 486896, "start": 4888.96, "end": 4889.96, "text": " So it's still the tip of the iceberg.", "tokens": [407, 309, 311, 920, 264, 4125, 295, 264, 38880, 13], "temperature": 0.0, "avg_logprob": -0.07915796414770261, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.833976239140611e-05}, {"id": 1281, "seek": 486896, "start": 4889.96, "end": 4893.96, "text": " But those who are trying it are consistently getting breakthrough results.", "tokens": [583, 729, 567, 366, 1382, 309, 366, 14961, 1242, 22397, 3542, 13], "temperature": 0.0, "avg_logprob": -0.07915796414770261, "compression_ratio": 1.6887966804979253, "no_speech_prob": 1.833976239140611e-05}, {"id": 1282, "seek": 489396, "start": 4893.96, "end": 4899.96, "text": " So I think it's really interesting, not just for NLP, but for all kinds of sequence classification tasks.", "tokens": [407, 286, 519, 309, 311, 534, 1880, 11, 406, 445, 337, 426, 45196, 11, 457, 337, 439, 3685, 295, 8310, 21538, 9608, 13], "temperature": 0.0, "avg_logprob": -0.05391727884610494, "compression_ratio": 1.634453781512605, "no_speech_prob": 9.07846879272256e-06}, {"id": 1283, "seek": 489396, "start": 4899.96, "end": 4906.96, "text": " So the basic process is going to be create a language model on some large data set.", "tokens": [407, 264, 3875, 1399, 307, 516, 281, 312, 1884, 257, 2856, 2316, 322, 512, 2416, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.05391727884610494, "compression_ratio": 1.634453781512605, "no_speech_prob": 9.07846879272256e-06}, {"id": 1284, "seek": 489396, "start": 4906.96, "end": 4909.96, "text": " And notice a language model is a very general term.", "tokens": [400, 3449, 257, 2856, 2316, 307, 257, 588, 2674, 1433, 13], "temperature": 0.0, "avg_logprob": -0.05391727884610494, "compression_ratio": 1.634453781512605, "no_speech_prob": 9.07846879272256e-06}, {"id": 1285, "seek": 489396, "start": 4909.96, "end": 4912.96, "text": " It means predict the next item in the sequence.", "tokens": [467, 1355, 6069, 264, 958, 3174, 294, 264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.05391727884610494, "compression_ratio": 1.634453781512605, "no_speech_prob": 9.07846879272256e-06}, {"id": 1286, "seek": 489396, "start": 4912.96, "end": 4918.96, "text": " So it could be an audio language model that predicts the next sample in a piece of music or speech.", "tokens": [407, 309, 727, 312, 364, 6278, 2856, 2316, 300, 6069, 82, 264, 958, 6889, 294, 257, 2522, 295, 1318, 420, 6218, 13], "temperature": 0.0, "avg_logprob": -0.05391727884610494, "compression_ratio": 1.634453781512605, "no_speech_prob": 9.07846879272256e-06}, {"id": 1287, "seek": 491896, "start": 4918.96, "end": 4925.96, "text": " It could be predicting the next genome in a sequence or whatever, right?", "tokens": [467, 727, 312, 32884, 264, 958, 21953, 294, 257, 8310, 420, 2035, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.0766737179089618, "compression_ratio": 1.5529953917050692, "no_speech_prob": 1.3629300156026147e-05}, {"id": 1288, "seek": 491896, "start": 4925.96, "end": 4928.96, "text": " So that's what I mean by language model.", "tokens": [407, 300, 311, 437, 286, 914, 538, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.0766737179089618, "compression_ratio": 1.5529953917050692, "no_speech_prob": 1.3629300156026147e-05}, {"id": 1289, "seek": 491896, "start": 4928.96, "end": 4937.96, "text": " And then we fine tune it, that language model using our in-domain corpus, which in this case is going to be IMDB.", "tokens": [400, 550, 321, 2489, 10864, 309, 11, 300, 2856, 2316, 1228, 527, 294, 12, 4121, 491, 1181, 31624, 11, 597, 294, 341, 1389, 307, 516, 281, 312, 21463, 27735, 13], "temperature": 0.0, "avg_logprob": -0.0766737179089618, "compression_ratio": 1.5529953917050692, "no_speech_prob": 1.3629300156026147e-05}, {"id": 1290, "seek": 491896, "start": 4937.96, "end": 4947.96, "text": " And then in each case, we first have to pre-process our data sets to get them ready for using an RNN on them.", "tokens": [400, 550, 294, 1184, 1389, 11, 321, 700, 362, 281, 659, 12, 41075, 527, 1412, 6352, 281, 483, 552, 1919, 337, 1228, 364, 45702, 45, 322, 552, 13], "temperature": 0.0, "avg_logprob": -0.0766737179089618, "compression_ratio": 1.5529953917050692, "no_speech_prob": 1.3629300156026147e-05}, {"id": 1291, "seek": 494796, "start": 4947.96, "end": 4950.96, "text": " Language models require one kind of pre-processing.", "tokens": [24445, 5245, 3651, 472, 733, 295, 659, 12, 41075, 278, 13], "temperature": 0.0, "avg_logprob": -0.09396233278162339, "compression_ratio": 1.6330645161290323, "no_speech_prob": 2.3915212295833044e-05}, {"id": 1292, "seek": 494796, "start": 4950.96, "end": 4953.96, "text": " Classification models require another one.", "tokens": [9471, 3774, 5245, 3651, 1071, 472, 13], "temperature": 0.0, "avg_logprob": -0.09396233278162339, "compression_ratio": 1.6330645161290323, "no_speech_prob": 2.3915212295833044e-05}, {"id": 1293, "seek": 494796, "start": 4953.96, "end": 4958.96, "text": " And then finally, we can fine tune our IMDB language model for classification.", "tokens": [400, 550, 2721, 11, 321, 393, 2489, 10864, 527, 21463, 27735, 2856, 2316, 337, 21538, 13], "temperature": 0.0, "avg_logprob": -0.09396233278162339, "compression_ratio": 1.6330645161290323, "no_speech_prob": 2.3915212295833044e-05}, {"id": 1294, "seek": 494796, "start": 4958.96, "end": 4962.96, "text": " So this is the process we're going to go through from scratch.", "tokens": [407, 341, 307, 264, 1399, 321, 434, 516, 281, 352, 807, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.09396233278162339, "compression_ratio": 1.6330645161290323, "no_speech_prob": 2.3915212295833044e-05}, {"id": 1295, "seek": 494796, "start": 4962.96, "end": 4976.96, "text": " So Sylvia has done an amazing thing in the last week, which is basically to recreate the entire ADWD, LSTM, and ULM fit process from scratch in the next four notebooks.", "tokens": [407, 33349, 11617, 575, 1096, 364, 2243, 551, 294, 264, 1036, 1243, 11, 597, 307, 1936, 281, 25833, 264, 2302, 9135, 54, 35, 11, 441, 6840, 44, 11, 293, 624, 43, 44, 3318, 1399, 490, 8459, 294, 264, 958, 1451, 43782, 13], "temperature": 0.0, "avg_logprob": -0.09396233278162339, "compression_ratio": 1.6330645161290323, "no_speech_prob": 2.3915212295833044e-05}, {"id": 1296, "seek": 497696, "start": 4976.96, "end": 4980.96, "text": " And there's quite a lot in here.", "tokens": [400, 456, 311, 1596, 257, 688, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.07581265439692232, "compression_ratio": 1.575221238938053, "no_speech_prob": 1.644049734750297e-05}, {"id": 1297, "seek": 497696, "start": 4980.96, "end": 4988.96, "text": " But a lot of it's kind of specific to text processing, and so some of it I might skip over a little bit quickly.", "tokens": [583, 257, 688, 295, 309, 311, 733, 295, 2685, 281, 2487, 9007, 11, 293, 370, 512, 295, 309, 286, 1062, 10023, 670, 257, 707, 857, 2661, 13], "temperature": 0.0, "avg_logprob": -0.07581265439692232, "compression_ratio": 1.575221238938053, "no_speech_prob": 1.644049734750297e-05}, {"id": 1298, "seek": 497696, "start": 4988.96, "end": 4991.96, "text": " But we'll talk about which bits are interesting.", "tokens": [583, 321, 603, 751, 466, 597, 9239, 366, 1880, 13], "temperature": 0.0, "avg_logprob": -0.07581265439692232, "compression_ratio": 1.575221238938053, "no_speech_prob": 1.644049734750297e-05}, {"id": 1299, "seek": 497696, "start": 4991.96, "end": 4995.96, "text": " So we're going to start with the IMDB data set as we have before.", "tokens": [407, 321, 434, 516, 281, 722, 365, 264, 21463, 27735, 1412, 992, 382, 321, 362, 949, 13], "temperature": 0.0, "avg_logprob": -0.07581265439692232, "compression_ratio": 1.575221238938053, "no_speech_prob": 1.644049734750297e-05}, {"id": 1300, "seek": 497696, "start": 4995.96, "end": 5002.96, "text": " And to remind you, it contains a training folder, an unsupervised folder, and a testing folder.", "tokens": [400, 281, 4160, 291, 11, 309, 8306, 257, 3097, 10820, 11, 364, 2693, 12879, 24420, 10820, 11, 293, 257, 4997, 10820, 13], "temperature": 0.0, "avg_logprob": -0.07581265439692232, "compression_ratio": 1.575221238938053, "no_speech_prob": 1.644049734750297e-05}, {"id": 1301, "seek": 500296, "start": 5002.96, "end": 5008.96, "text": " So the first thing we need to do is we need to create a data blocks item list subclass for text.", "tokens": [407, 264, 700, 551, 321, 643, 281, 360, 307, 321, 643, 281, 1884, 257, 1412, 8474, 3174, 1329, 1422, 11665, 337, 2487, 13], "temperature": 0.0, "avg_logprob": -0.06687676145675335, "compression_ratio": 1.675257731958763, "no_speech_prob": 8.664325832796749e-06}, {"id": 1302, "seek": 500296, "start": 5008.96, "end": 5012.96, "text": " Believe it or not, that's the entire code.", "tokens": [21486, 309, 420, 406, 11, 300, 311, 264, 2302, 3089, 13], "temperature": 0.0, "avg_logprob": -0.06687676145675335, "compression_ratio": 1.675257731958763, "no_speech_prob": 8.664325832796749e-06}, {"id": 1303, "seek": 500296, "start": 5012.96, "end": 5014.96, "text": " Because we already have a get files.", "tokens": [1436, 321, 1217, 362, 257, 483, 7098, 13], "temperature": 0.0, "avg_logprob": -0.06687676145675335, "compression_ratio": 1.675257731958763, "no_speech_prob": 8.664325832796749e-06}, {"id": 1304, "seek": 500296, "start": 5014.96, "end": 5017.96, "text": " So here's a get files with dot text.", "tokens": [407, 510, 311, 257, 483, 7098, 365, 5893, 2487, 13], "temperature": 0.0, "avg_logprob": -0.06687676145675335, "compression_ratio": 1.675257731958763, "no_speech_prob": 8.664325832796749e-06}, {"id": 1305, "seek": 500296, "start": 5017.96, "end": 5023.96, "text": " And all you have to do is override get to open a text file like so.", "tokens": [400, 439, 291, 362, 281, 360, 307, 42321, 483, 281, 1269, 257, 2487, 3991, 411, 370, 13], "temperature": 0.0, "avg_logprob": -0.06687676145675335, "compression_ratio": 1.675257731958763, "no_speech_prob": 8.664325832796749e-06}, {"id": 1306, "seek": 500296, "start": 5023.96, "end": 5026.96, "text": " And we're now ready to create an item list.", "tokens": [400, 321, 434, 586, 1919, 281, 1884, 364, 3174, 1329, 13], "temperature": 0.0, "avg_logprob": -0.06687676145675335, "compression_ratio": 1.675257731958763, "no_speech_prob": 8.664325832796749e-06}, {"id": 1307, "seek": 502696, "start": 5026.96, "end": 5034.96, "text": " So this is like the data blocks API is just so super easy to handle your domain.", "tokens": [407, 341, 307, 411, 264, 1412, 8474, 9362, 307, 445, 370, 1687, 1858, 281, 4813, 428, 9274, 13], "temperature": 0.0, "avg_logprob": -0.06735382171777579, "compression_ratio": 1.554585152838428, "no_speech_prob": 2.768883177850512e-06}, {"id": 1308, "seek": 502696, "start": 5034.96, "end": 5041.96, "text": " So if you've got genomic sequences or audio or whatever, this is basically what you'll need to do.", "tokens": [407, 498, 291, 600, 658, 1049, 21401, 22978, 420, 6278, 420, 2035, 11, 341, 307, 1936, 437, 291, 603, 643, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.06735382171777579, "compression_ratio": 1.554585152838428, "no_speech_prob": 2.768883177850512e-06}, {"id": 1309, "seek": 502696, "start": 5041.96, "end": 5045.96, "text": " So now we've got an item list with 100,000 things in it.", "tokens": [407, 586, 321, 600, 658, 364, 3174, 1329, 365, 2319, 11, 1360, 721, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.06735382171777579, "compression_ratio": 1.554585152838428, "no_speech_prob": 2.768883177850512e-06}, {"id": 1310, "seek": 502696, "start": 5045.96, "end": 5048.96, "text": " We've got the train, the test, and the unsupervised.", "tokens": [492, 600, 658, 264, 3847, 11, 264, 1500, 11, 293, 264, 2693, 12879, 24420, 13], "temperature": 0.0, "avg_logprob": -0.06735382171777579, "compression_ratio": 1.554585152838428, "no_speech_prob": 2.768883177850512e-06}, {"id": 1311, "seek": 502696, "start": 5048.96, "end": 5051.96, "text": " And we can index into it and see a text.", "tokens": [400, 321, 393, 8186, 666, 309, 293, 536, 257, 2487, 13], "temperature": 0.0, "avg_logprob": -0.06735382171777579, "compression_ratio": 1.554585152838428, "no_speech_prob": 2.768883177850512e-06}, {"id": 1312, "seek": 502696, "start": 5051.96, "end": 5053.96, "text": " So here's a movie review.", "tokens": [407, 510, 311, 257, 3169, 3131, 13], "temperature": 0.0, "avg_logprob": -0.06735382171777579, "compression_ratio": 1.554585152838428, "no_speech_prob": 2.768883177850512e-06}, {"id": 1313, "seek": 505396, "start": 5053.96, "end": 5056.96, "text": " And we can use all the same stuff that we've used before.", "tokens": [400, 321, 393, 764, 439, 264, 912, 1507, 300, 321, 600, 1143, 949, 13], "temperature": 0.0, "avg_logprob": -0.056879479115403545, "compression_ratio": 1.6940298507462686, "no_speech_prob": 1.2410875569912605e-05}, {"id": 1314, "seek": 505396, "start": 5056.96, "end": 5059.96, "text": " So for the previous notebook, we just built a random splitter.", "tokens": [407, 337, 264, 3894, 21060, 11, 321, 445, 3094, 257, 4974, 4732, 3904, 13], "temperature": 0.0, "avg_logprob": -0.056879479115403545, "compression_ratio": 1.6940298507462686, "no_speech_prob": 1.2410875569912605e-05}, {"id": 1315, "seek": 505396, "start": 5059.96, "end": 5061.96, "text": " So now we can use it on texts.", "tokens": [407, 586, 321, 393, 764, 309, 322, 15765, 13], "temperature": 0.0, "avg_logprob": -0.056879479115403545, "compression_ratio": 1.6940298507462686, "no_speech_prob": 1.2410875569912605e-05}, {"id": 1316, "seek": 505396, "start": 5061.96, "end": 5066.96, "text": " So the nice thing about this decoupled API is that we can mix and match things and things just work.", "tokens": [407, 264, 1481, 551, 466, 341, 979, 263, 15551, 9362, 307, 300, 321, 393, 2890, 293, 2995, 721, 293, 721, 445, 589, 13], "temperature": 0.0, "avg_logprob": -0.056879479115403545, "compression_ratio": 1.6940298507462686, "no_speech_prob": 1.2410875569912605e-05}, {"id": 1317, "seek": 505396, "start": 5066.96, "end": 5070.96, "text": " And we can see the representation of them. They just work.", "tokens": [400, 321, 393, 536, 264, 10290, 295, 552, 13, 814, 445, 589, 13], "temperature": 0.0, "avg_logprob": -0.056879479115403545, "compression_ratio": 1.6940298507462686, "no_speech_prob": 1.2410875569912605e-05}, {"id": 1318, "seek": 505396, "start": 5070.96, "end": 5074.96, "text": " Okay. So we can't throw this movie review into a model.", "tokens": [1033, 13, 407, 321, 393, 380, 3507, 341, 3169, 3131, 666, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.056879479115403545, "compression_ratio": 1.6940298507462686, "no_speech_prob": 1.2410875569912605e-05}, {"id": 1319, "seek": 505396, "start": 5074.96, "end": 5076.96, "text": " It needs to be numbers.", "tokens": [467, 2203, 281, 312, 3547, 13], "temperature": 0.0, "avg_logprob": -0.056879479115403545, "compression_ratio": 1.6940298507462686, "no_speech_prob": 1.2410875569912605e-05}, {"id": 1320, "seek": 505396, "start": 5076.96, "end": 5081.96, "text": " And so as you know, we need to tokenize and numericalize this.", "tokens": [400, 370, 382, 291, 458, 11, 321, 643, 281, 14862, 1125, 293, 29054, 1125, 341, 13], "temperature": 0.0, "avg_logprob": -0.056879479115403545, "compression_ratio": 1.6940298507462686, "no_speech_prob": 1.2410875569912605e-05}, {"id": 1321, "seek": 508196, "start": 5081.96, "end": 5083.96, "text": " So let's look at the details.", "tokens": [407, 718, 311, 574, 412, 264, 4365, 13], "temperature": 0.0, "avg_logprob": -0.04233825665253859, "compression_ratio": 1.5804878048780489, "no_speech_prob": 9.368227438244503e-06}, {"id": 1322, "seek": 508196, "start": 5083.96, "end": 5087.96, "text": " We use spaCy for tokenizing.", "tokens": [492, 764, 32543, 34, 88, 337, 14862, 3319, 13], "temperature": 0.0, "avg_logprob": -0.04233825665253859, "compression_ratio": 1.5804878048780489, "no_speech_prob": 9.368227438244503e-06}, {"id": 1323, "seek": 508196, "start": 5087.96, "end": 5090.96, "text": " And we do a few things as we tokenize.", "tokens": [400, 321, 360, 257, 1326, 721, 382, 321, 14862, 1125, 13], "temperature": 0.0, "avg_logprob": -0.04233825665253859, "compression_ratio": 1.5804878048780489, "no_speech_prob": 9.368227438244503e-06}, {"id": 1324, "seek": 508196, "start": 5090.96, "end": 5094.96, "text": " One thing we do is we have a few pre-rules.", "tokens": [1485, 551, 321, 360, 307, 321, 362, 257, 1326, 659, 12, 81, 3473, 13], "temperature": 0.0, "avg_logprob": -0.04233825665253859, "compression_ratio": 1.5804878048780489, "no_speech_prob": 9.368227438244503e-06}, {"id": 1325, "seek": 508196, "start": 5094.96, "end": 5099.96, "text": " These are bits of code that get run before tokenization.", "tokens": [1981, 366, 9239, 295, 3089, 300, 483, 1190, 949, 14862, 2144, 13], "temperature": 0.0, "avg_logprob": -0.04233825665253859, "compression_ratio": 1.5804878048780489, "no_speech_prob": 9.368227438244503e-06}, {"id": 1326, "seek": 508196, "start": 5099.96, "end": 5104.96, "text": " So for example, if we find br slash, we replace it with a new line.", "tokens": [407, 337, 1365, 11, 498, 321, 915, 738, 17330, 11, 321, 7406, 309, 365, 257, 777, 1622, 13], "temperature": 0.0, "avg_logprob": -0.04233825665253859, "compression_ratio": 1.5804878048780489, "no_speech_prob": 9.368227438244503e-06}, {"id": 1327, "seek": 508196, "start": 5104.96, "end": 5108.96, "text": " Or if we find a slash or a hash, we put spaces around it.", "tokens": [1610, 498, 321, 915, 257, 17330, 420, 257, 22019, 11, 321, 829, 7673, 926, 309, 13], "temperature": 0.0, "avg_logprob": -0.04233825665253859, "compression_ratio": 1.5804878048780489, "no_speech_prob": 9.368227438244503e-06}, {"id": 1328, "seek": 510896, "start": 5108.96, "end": 5114.96, "text": " If we find more than two spaces in a row, we just make it one space.", "tokens": [759, 321, 915, 544, 813, 732, 7673, 294, 257, 5386, 11, 321, 445, 652, 309, 472, 1901, 13], "temperature": 0.0, "avg_logprob": -0.07337959110736847, "compression_ratio": 1.5182926829268293, "no_speech_prob": 2.190740815422032e-06}, {"id": 1329, "seek": 510896, "start": 5114.96, "end": 5118.96, "text": " Then we have these special tokens.", "tokens": [1396, 321, 362, 613, 2121, 22667, 13], "temperature": 0.0, "avg_logprob": -0.07337959110736847, "compression_ratio": 1.5182926829268293, "no_speech_prob": 2.190740815422032e-06}, {"id": 1330, "seek": 510896, "start": 5118.96, "end": 5124.96, "text": " And this is what they look like as strings, but we use symbolic names for them, mainly.", "tokens": [400, 341, 307, 437, 436, 574, 411, 382, 13985, 11, 457, 321, 764, 25755, 5288, 337, 552, 11, 8704, 13], "temperature": 0.0, "avg_logprob": -0.07337959110736847, "compression_ratio": 1.5182926829268293, "no_speech_prob": 2.190740815422032e-06}, {"id": 1331, "seek": 510896, "start": 5124.96, "end": 5127.96, "text": " And these different tokens have various special meanings.", "tokens": [400, 613, 819, 22667, 362, 3683, 2121, 28138, 13], "temperature": 0.0, "avg_logprob": -0.07337959110736847, "compression_ratio": 1.5182926829268293, "no_speech_prob": 2.190740815422032e-06}, {"id": 1332, "seek": 512796, "start": 5127.96, "end": 5139.96, "text": " So for example, if we see some non-whitespace character more than three times in a row,", "tokens": [407, 337, 1365, 11, 498, 321, 536, 512, 2107, 12, 1363, 3324, 17940, 2517, 544, 813, 1045, 1413, 294, 257, 5386, 11], "temperature": 0.0, "avg_logprob": -0.12802804311116536, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.540351932926569e-06}, {"id": 1333, "seek": 512796, "start": 5139.96, "end": 5142.96, "text": " we replace it with... this is really cool, right?", "tokens": [321, 7406, 309, 365, 485, 341, 307, 534, 1627, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12802804311116536, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.540351932926569e-06}, {"id": 1334, "seek": 512796, "start": 5142.96, "end": 5147.96, "text": " In Python substitution, you can pass in a function.", "tokens": [682, 15329, 35827, 11, 291, 393, 1320, 294, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12802804311116536, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.540351932926569e-06}, {"id": 1335, "seek": 512796, "start": 5147.96, "end": 5153.96, "text": " So rep.sub here is going to look for this, and then it's going to replace it with the result of calling this function,", "tokens": [407, 1085, 13, 30131, 510, 307, 516, 281, 574, 337, 341, 11, 293, 550, 309, 311, 516, 281, 7406, 309, 365, 264, 1874, 295, 5141, 341, 2445, 11], "temperature": 0.0, "avg_logprob": -0.12802804311116536, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.540351932926569e-06}, {"id": 1336, "seek": 512796, "start": 5153.96, "end": 5155.96, "text": " which is really nice.", "tokens": [597, 307, 534, 1481, 13], "temperature": 0.0, "avg_logprob": -0.12802804311116536, "compression_ratio": 1.5942028985507246, "no_speech_prob": 6.540351932926569e-06}, {"id": 1337, "seek": 515596, "start": 5155.96, "end": 5162.96, "text": " And so what we're going to do is we're going to stick in the tkrep special token.", "tokens": [400, 370, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 2897, 294, 264, 256, 74, 19919, 2121, 14862, 13], "temperature": 0.0, "avg_logprob": -0.061550140380859375, "compression_ratio": 1.6944444444444444, "no_speech_prob": 4.425328825163888e-06}, {"id": 1338, "seek": 515596, "start": 5162.96, "end": 5165.96, "text": " So this means that there was a repeating token.", "tokens": [407, 341, 1355, 300, 456, 390, 257, 18617, 14862, 13], "temperature": 0.0, "avg_logprob": -0.061550140380859375, "compression_ratio": 1.6944444444444444, "no_speech_prob": 4.425328825163888e-06}, {"id": 1339, "seek": 515596, "start": 5165.96, "end": 5170.96, "text": " We're then going to put a number, which is how many times it repeated, and then the thing that was actually there.", "tokens": [492, 434, 550, 516, 281, 829, 257, 1230, 11, 597, 307, 577, 867, 1413, 309, 10477, 11, 293, 550, 264, 551, 300, 390, 767, 456, 13], "temperature": 0.0, "avg_logprob": -0.061550140380859375, "compression_ratio": 1.6944444444444444, "no_speech_prob": 4.425328825163888e-06}, {"id": 1340, "seek": 515596, "start": 5170.96, "end": 5173.96, "text": " We'll do the same thing with words.", "tokens": [492, 603, 360, 264, 912, 551, 365, 2283, 13], "temperature": 0.0, "avg_logprob": -0.061550140380859375, "compression_ratio": 1.6944444444444444, "no_speech_prob": 4.425328825163888e-06}, {"id": 1341, "seek": 515596, "start": 5173.96, "end": 5180.96, "text": " There's a lot of bits of little crappy things that we see in texts that we replace, mainly HTML entities.", "tokens": [821, 311, 257, 688, 295, 9239, 295, 707, 36531, 721, 300, 321, 536, 294, 15765, 300, 321, 7406, 11, 8704, 17995, 16667, 13], "temperature": 0.0, "avg_logprob": -0.061550140380859375, "compression_ratio": 1.6944444444444444, "no_speech_prob": 4.425328825163888e-06}, {"id": 1342, "seek": 515596, "start": 5180.96, "end": 5183.96, "text": " And we call those our default pre-rules.", "tokens": [400, 321, 818, 729, 527, 7576, 659, 12, 81, 3473, 13], "temperature": 0.0, "avg_logprob": -0.061550140380859375, "compression_ratio": 1.6944444444444444, "no_speech_prob": 4.425328825163888e-06}, {"id": 1343, "seek": 518396, "start": 5183.96, "end": 5186.96, "text": " And then this is our default list of special tokens.", "tokens": [400, 550, 341, 307, 527, 7576, 1329, 295, 2121, 22667, 13], "temperature": 0.0, "avg_logprob": -0.17186369947207872, "compression_ratio": 1.5977011494252873, "no_speech_prob": 8.664296728966292e-06}, {"id": 1344, "seek": 518396, "start": 5186.96, "end": 5190.96, "text": " So for example, replace rep.ccccc would be xxrep4c.", "tokens": [407, 337, 1365, 11, 7406, 1085, 13, 1914, 1914, 66, 576, 312, 2031, 87, 19919, 19, 66, 13], "temperature": 0.0, "avg_logprob": -0.17186369947207872, "compression_ratio": 1.5977011494252873, "no_speech_prob": 8.664296728966292e-06}, {"id": 1345, "seek": 518396, "start": 5190.96, "end": 5196.96, "text": " Or replace wrep, word, word, word, word, word, word, would be xxwrep5.word.", "tokens": [1610, 7406, 261, 19919, 11, 1349, 11, 1349, 11, 1349, 11, 1349, 11, 1349, 11, 1349, 11, 576, 312, 2031, 87, 86, 19919, 20, 13, 7462, 13], "temperature": 0.0, "avg_logprob": -0.17186369947207872, "compression_ratio": 1.5977011494252873, "no_speech_prob": 8.664296728966292e-06}, {"id": 1346, "seek": 518396, "start": 5196.96, "end": 5198.96, "text": " Why?", "tokens": [1545, 30], "temperature": 0.0, "avg_logprob": -0.17186369947207872, "compression_ratio": 1.5977011494252873, "no_speech_prob": 8.664296728966292e-06}, {"id": 1347, "seek": 518396, "start": 5198.96, "end": 5202.96, "text": " Well, think about the alternatives.", "tokens": [1042, 11, 519, 466, 264, 20478, 13], "temperature": 0.0, "avg_logprob": -0.17186369947207872, "compression_ratio": 1.5977011494252873, "no_speech_prob": 8.664296728966292e-06}, {"id": 1348, "seek": 518396, "start": 5202.96, "end": 5211.96, "text": " So what if you read a tweet that said, this was amazing!", "tokens": [407, 437, 498, 291, 1401, 257, 15258, 300, 848, 11, 341, 390, 2243, 0], "temperature": 0.0, "avg_logprob": -0.17186369947207872, "compression_ratio": 1.5977011494252873, "no_speech_prob": 8.664296728966292e-06}, {"id": 1349, "seek": 521196, "start": 5211.96, "end": 5215.96, "text": " So you could either treat those 28 exclamation marks as one token.", "tokens": [407, 291, 727, 2139, 2387, 729, 7562, 1624, 43233, 10640, 382, 472, 14862, 13], "temperature": 0.0, "avg_logprob": -0.08566283262692966, "compression_ratio": 1.6824034334763949, "no_speech_prob": 2.5866469513857737e-05}, {"id": 1350, "seek": 521196, "start": 5215.96, "end": 5221.96, "text": " And so now you have a vocab item that is specifically 28 exclamation marks.", "tokens": [400, 370, 586, 291, 362, 257, 2329, 455, 3174, 300, 307, 4682, 7562, 1624, 43233, 10640, 13], "temperature": 0.0, "avg_logprob": -0.08566283262692966, "compression_ratio": 1.6824034334763949, "no_speech_prob": 2.5866469513857737e-05}, {"id": 1351, "seek": 521196, "start": 5221.96, "end": 5224.96, "text": " You'll probably never see that again, so it probably won't even end up in your vocab.", "tokens": [509, 603, 1391, 1128, 536, 300, 797, 11, 370, 309, 1391, 1582, 380, 754, 917, 493, 294, 428, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.08566283262692966, "compression_ratio": 1.6824034334763949, "no_speech_prob": 2.5866469513857737e-05}, {"id": 1352, "seek": 521196, "start": 5224.96, "end": 5231.96, "text": " And if it did, it's going to be so rare that you won't be able to learn anything interesting about it.", "tokens": [400, 498, 309, 630, 11, 309, 311, 516, 281, 312, 370, 5892, 300, 291, 1582, 380, 312, 1075, 281, 1466, 1340, 1880, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.08566283262692966, "compression_ratio": 1.6824034334763949, "no_speech_prob": 2.5866469513857737e-05}, {"id": 1353, "seek": 521196, "start": 5231.96, "end": 5237.96, "text": " But if instead we replaced it with xxrep28 exclamation mark,", "tokens": [583, 498, 2602, 321, 10772, 309, 365, 2031, 87, 19919, 11205, 1624, 43233, 1491, 11], "temperature": 0.0, "avg_logprob": -0.08566283262692966, "compression_ratio": 1.6824034334763949, "no_speech_prob": 2.5866469513857737e-05}, {"id": 1354, "seek": 523796, "start": 5237.96, "end": 5244.96, "text": " then this is just three tokens where it can learn that lots of repeating exclamation marks", "tokens": [550, 341, 307, 445, 1045, 22667, 689, 309, 393, 1466, 300, 3195, 295, 18617, 1624, 43233, 10640], "temperature": 0.0, "avg_logprob": -0.08072444514224404, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.669999437581282e-05}, {"id": 1355, "seek": 523796, "start": 5244.96, "end": 5248.96, "text": " is a general concept that has certain semantics to it.", "tokens": [307, 257, 2674, 3410, 300, 575, 1629, 4361, 45298, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.08072444514224404, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.669999437581282e-05}, {"id": 1356, "seek": 523796, "start": 5248.96, "end": 5256.96, "text": " So that's what we're trying to do in NLP, is we're trying to make it so that the things in our vocab are as meaningful as possible.", "tokens": [407, 300, 311, 437, 321, 434, 1382, 281, 360, 294, 426, 45196, 11, 307, 321, 434, 1382, 281, 652, 309, 370, 300, 264, 721, 294, 527, 2329, 455, 366, 382, 10995, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.08072444514224404, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.669999437581282e-05}, {"id": 1357, "seek": 523796, "start": 5256.96, "end": 5261.96, "text": " And the nice thing is that because we're using an LSTM, we can have multi-word sequences", "tokens": [400, 264, 1481, 551, 307, 300, 570, 321, 434, 1228, 364, 441, 6840, 44, 11, 321, 393, 362, 4825, 12, 7462, 22978], "temperature": 0.0, "avg_logprob": -0.08072444514224404, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.669999437581282e-05}, {"id": 1358, "seek": 526196, "start": 5261.96, "end": 5269.96, "text": " and be confident that the LSTM will create some stateful computation that can handle that sequence.", "tokens": [293, 312, 6679, 300, 264, 441, 6840, 44, 486, 1884, 512, 1785, 906, 24903, 300, 393, 4813, 300, 8310, 13], "temperature": 0.0, "avg_logprob": -0.05828158995684456, "compression_ratio": 1.6333333333333333, "no_speech_prob": 5.014351245336002e-06}, {"id": 1359, "seek": 526196, "start": 5269.96, "end": 5277.96, "text": " Another alternative is we could have turned the 28 exclamation marks into 28 tokens in a row, each one of the single exclamation mark.", "tokens": [3996, 8535, 307, 321, 727, 362, 3574, 264, 7562, 1624, 43233, 10640, 666, 7562, 22667, 294, 257, 5386, 11, 1184, 472, 295, 264, 2167, 1624, 43233, 1491, 13], "temperature": 0.0, "avg_logprob": -0.05828158995684456, "compression_ratio": 1.6333333333333333, "no_speech_prob": 5.014351245336002e-06}, {"id": 1360, "seek": 526196, "start": 5277.96, "end": 5282.96, "text": " But now we're asking our LSTM to hang on to that state for 28 time steps,", "tokens": [583, 586, 321, 434, 3365, 527, 441, 6840, 44, 281, 3967, 322, 281, 300, 1785, 337, 7562, 565, 4439, 11], "temperature": 0.0, "avg_logprob": -0.05828158995684456, "compression_ratio": 1.6333333333333333, "no_speech_prob": 5.014351245336002e-06}, {"id": 1361, "seek": 526196, "start": 5282.96, "end": 5286.96, "text": " which is just a lot more work for it to do, and it's not going to do as good a job.", "tokens": [597, 307, 445, 257, 688, 544, 589, 337, 309, 281, 360, 11, 293, 309, 311, 406, 516, 281, 360, 382, 665, 257, 1691, 13], "temperature": 0.0, "avg_logprob": -0.05828158995684456, "compression_ratio": 1.6333333333333333, "no_speech_prob": 5.014351245336002e-06}, {"id": 1362, "seek": 528696, "start": 5286.96, "end": 5291.96, "text": " So we want to make things easy for our models. That's what preprocessing is all about.", "tokens": [407, 321, 528, 281, 652, 721, 1858, 337, 527, 5245, 13, 663, 311, 437, 2666, 340, 780, 278, 307, 439, 466, 13], "temperature": 0.0, "avg_logprob": -0.08488096255008305, "compression_ratio": 1.729957805907173, "no_speech_prob": 1.22184501378797e-05}, {"id": 1363, "seek": 528696, "start": 5291.96, "end": 5296.96, "text": " So same with all caps. If you've got, I am shouting,", "tokens": [407, 912, 365, 439, 13855, 13, 759, 291, 600, 658, 11, 286, 669, 20382, 11], "temperature": 0.0, "avg_logprob": -0.08488096255008305, "compression_ratio": 1.729957805907173, "no_speech_prob": 1.22184501378797e-05}, {"id": 1364, "seek": 528696, "start": 5296.96, "end": 5300.96, "text": " then it's pretty likely that there's going to be exclamation marks after that.", "tokens": [550, 309, 311, 1238, 3700, 300, 456, 311, 516, 281, 312, 1624, 43233, 10640, 934, 300, 13], "temperature": 0.0, "avg_logprob": -0.08488096255008305, "compression_ratio": 1.729957805907173, "no_speech_prob": 1.22184501378797e-05}, {"id": 1365, "seek": 528696, "start": 5300.96, "end": 5306.96, "text": " There might be swearing after that, like the fact that there's lots of capitalized words is semantic of itself.", "tokens": [821, 1062, 312, 2484, 1921, 934, 300, 11, 411, 264, 1186, 300, 456, 311, 3195, 295, 4238, 1602, 2283, 307, 47982, 295, 2564, 13], "temperature": 0.0, "avg_logprob": -0.08488096255008305, "compression_ratio": 1.729957805907173, "no_speech_prob": 1.22184501378797e-05}, {"id": 1366, "seek": 528696, "start": 5306.96, "end": 5310.96, "text": " So we replace capitalized words with a token saying this is a capitalized word.", "tokens": [407, 321, 7406, 4238, 1602, 2283, 365, 257, 14862, 1566, 341, 307, 257, 4238, 1602, 1349, 13], "temperature": 0.0, "avg_logprob": -0.08488096255008305, "compression_ratio": 1.729957805907173, "no_speech_prob": 1.22184501378797e-05}, {"id": 1367, "seek": 531096, "start": 5310.96, "end": 5316.96, "text": " And then we replace it with the lowercase word so we don't have a separate vocab item for capital am,", "tokens": [400, 550, 321, 7406, 309, 365, 264, 3126, 9765, 1349, 370, 321, 500, 380, 362, 257, 4994, 2329, 455, 3174, 337, 4238, 669, 11], "temperature": 0.0, "avg_logprob": -0.0929316246863639, "compression_ratio": 1.5662650602409638, "no_speech_prob": 5.507404239324387e-06}, {"id": 1368, "seek": 531096, "start": 5316.96, "end": 5321.96, "text": " capital shouting, capital every damn word in the dictionary.", "tokens": [4238, 20382, 11, 4238, 633, 8151, 1349, 294, 264, 25890, 13], "temperature": 0.0, "avg_logprob": -0.0929316246863639, "compression_ratio": 1.5662650602409638, "no_speech_prob": 5.507404239324387e-06}, {"id": 1369, "seek": 531096, "start": 5321.96, "end": 5326.96, "text": " OK, same thing for mixed case.", "tokens": [2264, 11, 912, 551, 337, 7467, 1389, 13], "temperature": 0.0, "avg_logprob": -0.0929316246863639, "compression_ratio": 1.5662650602409638, "no_speech_prob": 5.507404239324387e-06}, {"id": 1370, "seek": 531096, "start": 5326.96, "end": 5332.96, "text": " So I don't know, I haven't come across other libraries that do this kind of preprocessing.", "tokens": [407, 286, 500, 380, 458, 11, 286, 2378, 380, 808, 2108, 661, 15148, 300, 360, 341, 733, 295, 2666, 340, 780, 278, 13], "temperature": 0.0, "avg_logprob": -0.0929316246863639, "compression_ratio": 1.5662650602409638, "no_speech_prob": 5.507404239324387e-06}, {"id": 1371, "seek": 531096, "start": 5332.96, "end": 5339.96, "text": " There's little bits and pieces in various papers, but I think this is a pretty good default set of rules.", "tokens": [821, 311, 707, 9239, 293, 3755, 294, 3683, 10577, 11, 457, 286, 519, 341, 307, 257, 1238, 665, 7576, 992, 295, 4474, 13], "temperature": 0.0, "avg_logprob": -0.0929316246863639, "compression_ratio": 1.5662650602409638, "no_speech_prob": 5.507404239324387e-06}, {"id": 1372, "seek": 533996, "start": 5339.96, "end": 5344.96, "text": " Notice that these rules have to happen after tokenization because they're happening at a word level.", "tokens": [13428, 300, 613, 4474, 362, 281, 1051, 934, 14862, 2144, 570, 436, 434, 2737, 412, 257, 1349, 1496, 13], "temperature": 0.0, "avg_logprob": -0.07866165161132813, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.473798602091847e-05}, {"id": 1373, "seek": 533996, "start": 5344.96, "end": 5347.96, "text": " So we have default post rules.", "tokens": [407, 321, 362, 7576, 2183, 4474, 13], "temperature": 0.0, "avg_logprob": -0.07866165161132813, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.473798602091847e-05}, {"id": 1374, "seek": 533996, "start": 5347.96, "end": 5357.96, "text": " And then this one here adds a beginning of stream and an end of stream on either side of a list of tokens.", "tokens": [400, 550, 341, 472, 510, 10860, 257, 2863, 295, 4309, 293, 364, 917, 295, 4309, 322, 2139, 1252, 295, 257, 1329, 295, 22667, 13], "temperature": 0.0, "avg_logprob": -0.07866165161132813, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.473798602091847e-05}, {"id": 1375, "seek": 533996, "start": 5357.96, "end": 5363.96, "text": " Why do we do that? These tokens turn out to be very important because", "tokens": [1545, 360, 321, 360, 300, 30, 1981, 22667, 1261, 484, 281, 312, 588, 1021, 570], "temperature": 0.0, "avg_logprob": -0.07866165161132813, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.473798602091847e-05}, {"id": 1376, "seek": 536396, "start": 5363.96, "end": 5371.96, "text": " when your language model sees an end of stream character token meaning that's the end of a document,", "tokens": [562, 428, 2856, 2316, 8194, 364, 917, 295, 4309, 2517, 14862, 3620, 300, 311, 264, 917, 295, 257, 4166, 11], "temperature": 0.0, "avg_logprob": -0.12869577224438006, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.9022105536569143e-06}, {"id": 1377, "seek": 536396, "start": 5371.96, "end": 5375.96, "text": " then it knows the next document is something new.", "tokens": [550, 309, 3255, 264, 958, 4166, 307, 746, 777, 13], "temperature": 0.0, "avg_logprob": -0.12869577224438006, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.9022105536569143e-06}, {"id": 1378, "seek": 536396, "start": 5375.96, "end": 5380.96, "text": " So it's going to have to learn to kind of reset its state to say, oh, we're not talking about the old thing anymore.", "tokens": [407, 309, 311, 516, 281, 362, 281, 1466, 281, 733, 295, 14322, 1080, 1785, 281, 584, 11, 1954, 11, 321, 434, 406, 1417, 466, 264, 1331, 551, 3602, 13], "temperature": 0.0, "avg_logprob": -0.12869577224438006, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.9022105536569143e-06}, {"id": 1379, "seek": 536396, "start": 5380.96, "end": 5385.96, "text": " So we're doing Wikipedia, we were talking about Melbourne, Australia, oh, and now there's a new token,", "tokens": [407, 321, 434, 884, 28999, 11, 321, 645, 1417, 466, 27496, 11, 7060, 11, 1954, 11, 293, 586, 456, 311, 257, 777, 14862, 11], "temperature": 0.0, "avg_logprob": -0.12869577224438006, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.9022105536569143e-06}, {"id": 1380, "seek": 536396, "start": 5385.96, "end": 5388.96, "text": " and we're talking about the Emmys.", "tokens": [293, 321, 434, 1417, 466, 264, 28237, 749, 13], "temperature": 0.0, "avg_logprob": -0.12869577224438006, "compression_ratio": 1.7307692307692308, "no_speech_prob": 2.9022105536569143e-06}, {"id": 1381, "seek": 538896, "start": 5388.96, "end": 5396.96, "text": " So when it sees EOS, it has to learn to kind of reset its state somehow.", "tokens": [407, 562, 309, 8194, 462, 4367, 11, 309, 575, 281, 1466, 281, 733, 295, 14322, 1080, 1785, 6063, 13], "temperature": 0.0, "avg_logprob": -0.04312989213964441, "compression_ratio": 1.6043478260869566, "no_speech_prob": 7.295788236660883e-06}, {"id": 1382, "seek": 538896, "start": 5396.96, "end": 5405.96, "text": " So you need to make sure that you have the tokens in place to allow your model to know that these things are happening.", "tokens": [407, 291, 643, 281, 652, 988, 300, 291, 362, 264, 22667, 294, 1081, 281, 2089, 428, 2316, 281, 458, 300, 613, 721, 366, 2737, 13], "temperature": 0.0, "avg_logprob": -0.04312989213964441, "compression_ratio": 1.6043478260869566, "no_speech_prob": 7.295788236660883e-06}, {"id": 1383, "seek": 538896, "start": 5405.96, "end": 5410.96, "text": " Tokenization is kind of slow because Spacey does it so carefully.", "tokens": [314, 8406, 2144, 307, 733, 295, 2964, 570, 8705, 88, 775, 309, 370, 7500, 13], "temperature": 0.0, "avg_logprob": -0.04312989213964441, "compression_ratio": 1.6043478260869566, "no_speech_prob": 7.295788236660883e-06}, {"id": 1384, "seek": 538896, "start": 5410.96, "end": 5415.96, "text": " I thought it couldn't possibly be necessary to do it so carefully because it just doesn't seem that important.", "tokens": [286, 1194, 309, 2809, 380, 6264, 312, 4818, 281, 360, 309, 370, 7500, 570, 309, 445, 1177, 380, 1643, 300, 1021, 13], "temperature": 0.0, "avg_logprob": -0.04312989213964441, "compression_ratio": 1.6043478260869566, "no_speech_prob": 7.295788236660883e-06}, {"id": 1385, "seek": 541596, "start": 5415.96, "end": 5420.96, "text": " So last year I tried removing Spacey and replacing it with something much simpler.", "tokens": [407, 1036, 1064, 286, 3031, 12720, 8705, 88, 293, 19139, 309, 365, 746, 709, 18587, 13], "temperature": 0.0, "avg_logprob": -0.08988561533918285, "compression_ratio": 1.642570281124498, "no_speech_prob": 1.4968214600230567e-05}, {"id": 1386, "seek": 541596, "start": 5420.96, "end": 5423.96, "text": " My IMDB accuracy went down a lot.", "tokens": [1222, 21463, 27735, 14170, 1437, 760, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.08988561533918285, "compression_ratio": 1.642570281124498, "no_speech_prob": 1.4968214600230567e-05}, {"id": 1387, "seek": 541596, "start": 5423.96, "end": 5432.96, "text": " So actually it seems like Spacey's sophisticated parser-based tokenization actually does better.", "tokens": [407, 767, 309, 2544, 411, 8705, 88, 311, 16950, 21156, 260, 12, 6032, 14862, 2144, 767, 775, 1101, 13], "temperature": 0.0, "avg_logprob": -0.08988561533918285, "compression_ratio": 1.642570281124498, "no_speech_prob": 1.4968214600230567e-05}, {"id": 1388, "seek": 541596, "start": 5432.96, "end": 5434.96, "text": " So at least we can try and make it fast.", "tokens": [407, 412, 1935, 321, 393, 853, 293, 652, 309, 2370, 13], "temperature": 0.0, "avg_logprob": -0.08988561533918285, "compression_ratio": 1.642570281124498, "no_speech_prob": 1.4968214600230567e-05}, {"id": 1389, "seek": 541596, "start": 5434.96, "end": 5441.96, "text": " So Python comes with something called a process pool executor, which runs things in parallel,", "tokens": [407, 15329, 1487, 365, 746, 1219, 257, 1399, 7005, 7568, 284, 11, 597, 6676, 721, 294, 8952, 11], "temperature": 0.0, "avg_logprob": -0.08988561533918285, "compression_ratio": 1.642570281124498, "no_speech_prob": 1.4968214600230567e-05}, {"id": 1390, "seek": 541596, "start": 5441.96, "end": 5443.96, "text": " and I wrap it around with this little thing called parallel.", "tokens": [293, 286, 7019, 309, 926, 365, 341, 707, 551, 1219, 8952, 13], "temperature": 0.0, "avg_logprob": -0.08988561533918285, "compression_ratio": 1.642570281124498, "no_speech_prob": 1.4968214600230567e-05}, {"id": 1391, "seek": 544396, "start": 5443.96, "end": 5451.96, "text": " And so here's my thing that runs, look, compose, appears everywhere, compose the prerules on every chunk, run the tokenizer,", "tokens": [400, 370, 510, 311, 452, 551, 300, 6676, 11, 574, 11, 35925, 11, 7038, 5315, 11, 35925, 264, 582, 260, 3473, 322, 633, 16635, 11, 1190, 264, 14862, 6545, 11], "temperature": 0.0, "avg_logprob": -0.12986359847219367, "compression_ratio": 1.7916666666666667, "no_speech_prob": 4.710740540758707e-06}, {"id": 1392, "seek": 544396, "start": 5451.96, "end": 5454.96, "text": " compose the post rules on every dock.", "tokens": [35925, 264, 2183, 4474, 322, 633, 20929, 13], "temperature": 0.0, "avg_logprob": -0.12986359847219367, "compression_ratio": 1.7916666666666667, "no_speech_prob": 4.710740540758707e-06}, {"id": 1393, "seek": 544396, "start": 5454.96, "end": 5456.96, "text": " That's processing one chunk.", "tokens": [663, 311, 9007, 472, 16635, 13], "temperature": 0.0, "avg_logprob": -0.12986359847219367, "compression_ratio": 1.7916666666666667, "no_speech_prob": 4.710740540758707e-06}, {"id": 1394, "seek": 544396, "start": 5456.96, "end": 5459.96, "text": " So run them all in parallel for all the chunks.", "tokens": [407, 1190, 552, 439, 294, 8952, 337, 439, 264, 24004, 13], "temperature": 0.0, "avg_logprob": -0.12986359847219367, "compression_ratio": 1.7916666666666667, "no_speech_prob": 4.710740540758707e-06}, {"id": 1395, "seek": 544396, "start": 5459.96, "end": 5461.96, "text": " So that's that.", "tokens": [407, 300, 311, 300, 13], "temperature": 0.0, "avg_logprob": -0.12986359847219367, "compression_ratio": 1.7916666666666667, "no_speech_prob": 4.710740540758707e-06}, {"id": 1396, "seek": 544396, "start": 5461.96, "end": 5469.96, "text": " So this is a processor, which we saw last week, and this is a processor which tokenizes.", "tokens": [407, 341, 307, 257, 15321, 11, 597, 321, 1866, 1036, 1243, 11, 293, 341, 307, 257, 15321, 597, 14862, 5660, 13], "temperature": 0.0, "avg_logprob": -0.12986359847219367, "compression_ratio": 1.7916666666666667, "no_speech_prob": 4.710740540758707e-06}, {"id": 1397, "seek": 546996, "start": 5469.96, "end": 5477.96, "text": " And so we can try it out, so we can create one and try, here's a bit of text, and let's try tokenizing.", "tokens": [400, 370, 321, 393, 853, 309, 484, 11, 370, 321, 393, 1884, 472, 293, 853, 11, 510, 311, 257, 857, 295, 2487, 11, 293, 718, 311, 853, 14862, 3319, 13], "temperature": 0.0, "avg_logprob": -0.14724192218245746, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.952502498170361e-05}, {"id": 1398, "seek": 546996, "start": 5477.96, "end": 5486.96, "text": " And so you can see we've got beginning of stream, did int, so int is a token, comma is a token,", "tokens": [400, 370, 291, 393, 536, 321, 600, 658, 2863, 295, 4309, 11, 630, 560, 11, 370, 560, 307, 257, 14862, 11, 22117, 307, 257, 14862, 11], "temperature": 0.0, "avg_logprob": -0.14724192218245746, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.952502498170361e-05}, {"id": 1399, "seek": 546996, "start": 5486.96, "end": 5491.96, "text": " xxmage, da1, so that was a capital D, and so forth.", "tokens": [2031, 87, 76, 609, 11, 1120, 16, 11, 370, 300, 390, 257, 4238, 413, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.14724192218245746, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.952502498170361e-05}, {"id": 1400, "seek": 546996, "start": 5491.96, "end": 5497.96, "text": " All right, so now we need to turn those into numbers, not just to have a list of words.", "tokens": [1057, 558, 11, 370, 586, 321, 643, 281, 1261, 729, 666, 3547, 11, 406, 445, 281, 362, 257, 1329, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.14724192218245746, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.952502498170361e-05}, {"id": 1401, "seek": 549796, "start": 5497.96, "end": 5502.96, "text": " We can turn them into numbers by numericalizing, which is another processor,", "tokens": [492, 393, 1261, 552, 666, 3547, 538, 29054, 3319, 11, 597, 307, 1071, 15321, 11], "temperature": 0.0, "avg_logprob": -0.07720625717028053, "compression_ratio": 1.6995708154506437, "no_speech_prob": 1.0451202797412407e-05}, {"id": 1402, "seek": 549796, "start": 5502.96, "end": 5506.96, "text": " which basically when you call it, we find out, do we have a vocab yet?", "tokens": [597, 1936, 562, 291, 818, 309, 11, 321, 915, 484, 11, 360, 321, 362, 257, 2329, 455, 1939, 30], "temperature": 0.0, "avg_logprob": -0.07720625717028053, "compression_ratio": 1.6995708154506437, "no_speech_prob": 1.0451202797412407e-05}, {"id": 1403, "seek": 549796, "start": 5506.96, "end": 5509.96, "text": " Because numericalizing is just saying, what are all the unique words?", "tokens": [1436, 29054, 3319, 307, 445, 1566, 11, 437, 366, 439, 264, 3845, 2283, 30], "temperature": 0.0, "avg_logprob": -0.07720625717028053, "compression_ratio": 1.6995708154506437, "no_speech_prob": 1.0451202797412407e-05}, {"id": 1404, "seek": 549796, "start": 5509.96, "end": 5512.96, "text": " And the list of unique words is the vocab.", "tokens": [400, 264, 1329, 295, 3845, 2283, 307, 264, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.07720625717028053, "compression_ratio": 1.6995708154506437, "no_speech_prob": 1.0451202797412407e-05}, {"id": 1405, "seek": 549796, "start": 5512.96, "end": 5516.96, "text": " So if we don't have a vocab, we'll create it.", "tokens": [407, 498, 321, 500, 380, 362, 257, 2329, 455, 11, 321, 603, 1884, 309, 13], "temperature": 0.0, "avg_logprob": -0.07720625717028053, "compression_ratio": 1.6995708154506437, "no_speech_prob": 1.0451202797412407e-05}, {"id": 1406, "seek": 549796, "start": 5516.96, "end": 5518.96, "text": " Okay?", "tokens": [1033, 30], "temperature": 0.0, "avg_logprob": -0.07720625717028053, "compression_ratio": 1.6995708154506437, "no_speech_prob": 1.0451202797412407e-05}, {"id": 1407, "seek": 549796, "start": 5518.96, "end": 5525.96, "text": " And then after we create it, it's just a case of calling object to int on each one.", "tokens": [400, 550, 934, 321, 1884, 309, 11, 309, 311, 445, 257, 1389, 295, 5141, 2657, 281, 560, 322, 1184, 472, 13], "temperature": 0.0, "avg_logprob": -0.07720625717028053, "compression_ratio": 1.6995708154506437, "no_speech_prob": 1.0451202797412407e-05}, {"id": 1408, "seek": 552596, "start": 5525.96, "end": 5529.96, "text": " So o2i is just a dictionary, right?", "tokens": [407, 277, 17, 72, 307, 445, 257, 25890, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12276844356371008, "compression_ratio": 1.5637254901960784, "no_speech_prob": 1.1299948710075114e-05}, {"id": 1409, "seek": 552596, "start": 5529.96, "end": 5533.96, "text": " Or if deprocessing is just grabbing each thing from the vocab, so that's just in the right.", "tokens": [1610, 498, 1367, 340, 780, 278, 307, 445, 23771, 1184, 551, 490, 264, 2329, 455, 11, 370, 300, 311, 445, 294, 264, 558, 13], "temperature": 0.0, "avg_logprob": -0.12276844356371008, "compression_ratio": 1.5637254901960784, "no_speech_prob": 1.1299948710075114e-05}, {"id": 1410, "seek": 552596, "start": 5533.96, "end": 5542.96, "text": " Okay, so we can tokenize, numericalize, run it for two and a half minutes.", "tokens": [1033, 11, 370, 321, 393, 14862, 1125, 11, 29054, 1125, 11, 1190, 309, 337, 732, 293, 257, 1922, 2077, 13], "temperature": 0.0, "avg_logprob": -0.12276844356371008, "compression_ratio": 1.5637254901960784, "no_speech_prob": 1.1299948710075114e-05}, {"id": 1411, "seek": 552596, "start": 5542.96, "end": 5550.96, "text": " And so we've got the xobj is the thing which returns the object version, so as opposed to the numericalized version.", "tokens": [400, 370, 321, 600, 658, 264, 2031, 996, 73, 307, 264, 551, 597, 11247, 264, 2657, 3037, 11, 370, 382, 8851, 281, 264, 29054, 1602, 3037, 13], "temperature": 0.0, "avg_logprob": -0.12276844356371008, "compression_ratio": 1.5637254901960784, "no_speech_prob": 1.1299948710075114e-05}, {"id": 1412, "seek": 555096, "start": 5550.96, "end": 5555.96, "text": " And so we put it back together and this is what we have after it's been turned into numbers and back again.", "tokens": [400, 370, 321, 829, 309, 646, 1214, 293, 341, 307, 437, 321, 362, 934, 309, 311, 668, 3574, 666, 3547, 293, 646, 797, 13], "temperature": 0.0, "avg_logprob": -0.05243076525236431, "compression_ratio": 1.592274678111588, "no_speech_prob": 1.0952790034934878e-05}, {"id": 1413, "seek": 555096, "start": 5555.96, "end": 5561.96, "text": " So since that takes a couple of minutes, good idea to dump the labeled list", "tokens": [407, 1670, 300, 2516, 257, 1916, 295, 2077, 11, 665, 1558, 281, 11430, 264, 21335, 1329], "temperature": 0.0, "avg_logprob": -0.05243076525236431, "compression_ratio": 1.592274678111588, "no_speech_prob": 1.0952790034934878e-05}, {"id": 1414, "seek": 555096, "start": 5561.96, "end": 5568.96, "text": " so that we can then load it again later without having to rerun that.", "tokens": [370, 300, 321, 393, 550, 3677, 309, 797, 1780, 1553, 1419, 281, 43819, 409, 300, 13], "temperature": 0.0, "avg_logprob": -0.05243076525236431, "compression_ratio": 1.592274678111588, "no_speech_prob": 1.0952790034934878e-05}, {"id": 1415, "seek": 555096, "start": 5568.96, "end": 5572.96, "text": " All right, this is the bit which a lot of people get confused about,", "tokens": [1057, 558, 11, 341, 307, 264, 857, 597, 257, 688, 295, 561, 483, 9019, 466, 11], "temperature": 0.0, "avg_logprob": -0.05243076525236431, "compression_ratio": 1.592274678111588, "no_speech_prob": 1.0952790034934878e-05}, {"id": 1416, "seek": 555096, "start": 5572.96, "end": 5577.96, "text": " which is how do we batch up language model data?", "tokens": [597, 307, 577, 360, 321, 15245, 493, 2856, 2316, 1412, 30], "temperature": 0.0, "avg_logprob": -0.05243076525236431, "compression_ratio": 1.592274678111588, "no_speech_prob": 1.0952790034934878e-05}, {"id": 1417, "seek": 557796, "start": 5577.96, "end": 5580.96, "text": " So here's a bit of text.", "tokens": [407, 510, 311, 257, 857, 295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.09496064619584517, "compression_ratio": 1.8448275862068966, "no_speech_prob": 3.373189611011185e-05}, {"id": 1418, "seek": 557796, "start": 5580.96, "end": 5586.96, "text": " It's very meta, it's a bit of text which is from this notebook.", "tokens": [467, 311, 588, 19616, 11, 309, 311, 257, 857, 295, 2487, 597, 307, 490, 341, 21060, 13], "temperature": 0.0, "avg_logprob": -0.09496064619584517, "compression_ratio": 1.8448275862068966, "no_speech_prob": 3.373189611011185e-05}, {"id": 1419, "seek": 557796, "start": 5586.96, "end": 5590.96, "text": " So the first thing we're going to do is we're going to say let's create some batch sizes,", "tokens": [407, 264, 700, 551, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 584, 718, 311, 1884, 512, 15245, 11602, 11], "temperature": 0.0, "avg_logprob": -0.09496064619584517, "compression_ratio": 1.8448275862068966, "no_speech_prob": 3.373189611011185e-05}, {"id": 1420, "seek": 557796, "start": 5590.96, "end": 5593.96, "text": " create a small one for showing you what's going on, six.", "tokens": [1884, 257, 1359, 472, 337, 4099, 291, 437, 311, 516, 322, 11, 2309, 13], "temperature": 0.0, "avg_logprob": -0.09496064619584517, "compression_ratio": 1.8448275862068966, "no_speech_prob": 3.373189611011185e-05}, {"id": 1421, "seek": 557796, "start": 5593.96, "end": 5598.96, "text": " So let's go through and create six batches, which is just all the tokens for each of those six batches.", "tokens": [407, 718, 311, 352, 807, 293, 1884, 2309, 15245, 279, 11, 597, 307, 445, 439, 264, 22667, 337, 1184, 295, 729, 2309, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.09496064619584517, "compression_ratio": 1.8448275862068966, "no_speech_prob": 3.373189611011185e-05}, {"id": 1422, "seek": 557796, "start": 5598.96, "end": 5604.96, "text": " So here's in this notebook, we will go back over the example of is the first element of,", "tokens": [407, 510, 311, 294, 341, 21060, 11, 321, 486, 352, 646, 670, 264, 1365, 295, 307, 264, 700, 4478, 295, 11], "temperature": 0.0, "avg_logprob": -0.09496064619584517, "compression_ratio": 1.8448275862068966, "no_speech_prob": 3.373189611011185e-05}, {"id": 1423, "seek": 560496, "start": 5604.96, "end": 5609.96, "text": " so this is the first row and then of classifying movie reviews you started in part one, this is the second.", "tokens": [370, 341, 307, 264, 700, 5386, 293, 550, 295, 1508, 5489, 3169, 10229, 291, 1409, 294, 644, 472, 11, 341, 307, 264, 1150, 13], "temperature": 0.0, "avg_logprob": -0.15145568458401426, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.0783671061508358e-05}, {"id": 1424, "seek": 560496, "start": 5609.96, "end": 5614.96, "text": " Okay, so we just put it into six groups, right?", "tokens": [1033, 11, 370, 321, 445, 829, 309, 666, 2309, 3935, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15145568458401426, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.0783671061508358e-05}, {"id": 1425, "seek": 560496, "start": 5614.96, "end": 5622.96, "text": " And then let's say we have a BPTT of five, so it's kind of like our back prop through time sequence length of five.", "tokens": [400, 550, 718, 311, 584, 321, 362, 257, 40533, 28178, 295, 1732, 11, 370, 309, 311, 733, 295, 411, 527, 646, 2365, 807, 565, 8310, 4641, 295, 1732, 13], "temperature": 0.0, "avg_logprob": -0.15145568458401426, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.0783671061508358e-05}, {"id": 1426, "seek": 560496, "start": 5622.96, "end": 5626.96, "text": " Then we can split these up into groups of five.", "tokens": [1396, 321, 393, 7472, 613, 493, 666, 3935, 295, 1732, 13], "temperature": 0.0, "avg_logprob": -0.15145568458401426, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.0783671061508358e-05}, {"id": 1427, "seek": 560496, "start": 5626.96, "end": 5628.96, "text": " And so that'll create three of them.", "tokens": [400, 370, 300, 603, 1884, 1045, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.15145568458401426, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.0783671061508358e-05}, {"id": 1428, "seek": 562896, "start": 5628.96, "end": 5635.96, "text": " In this notebook, we will go back over the example of classifying movie reviews we studied in part one.", "tokens": [682, 341, 21060, 11, 321, 486, 352, 646, 670, 264, 1365, 295, 1508, 5489, 3169, 10229, 321, 9454, 294, 644, 472, 13], "temperature": 0.0, "avg_logprob": -0.0790644745494044, "compression_ratio": 1.60352422907489, "no_speech_prob": 1.3630446119350381e-05}, {"id": 1429, "seek": 562896, "start": 5635.96, "end": 5638.96, "text": " These three things then are three mini batches.", "tokens": [1981, 1045, 721, 550, 366, 1045, 8382, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.0790644745494044, "compression_ratio": 1.60352422907489, "no_speech_prob": 1.3630446119350381e-05}, {"id": 1430, "seek": 562896, "start": 5638.96, "end": 5644.96, "text": " And this is where people get confused because it's not that each one has a different bunch of documents.", "tokens": [400, 341, 307, 689, 561, 483, 9019, 570, 309, 311, 406, 300, 1184, 472, 575, 257, 819, 3840, 295, 8512, 13], "temperature": 0.0, "avg_logprob": -0.0790644745494044, "compression_ratio": 1.60352422907489, "no_speech_prob": 1.3630446119350381e-05}, {"id": 1431, "seek": 562896, "start": 5644.96, "end": 5649.96, "text": " Each one has the same documents over consecutive time steps.", "tokens": [6947, 472, 575, 264, 912, 8512, 670, 30497, 565, 4439, 13], "temperature": 0.0, "avg_logprob": -0.0790644745494044, "compression_ratio": 1.60352422907489, "no_speech_prob": 1.3630446119350381e-05}, {"id": 1432, "seek": 562896, "start": 5649.96, "end": 5653.96, "text": " This is really important. Why is it important?", "tokens": [639, 307, 534, 1021, 13, 1545, 307, 309, 1021, 30], "temperature": 0.0, "avg_logprob": -0.0790644745494044, "compression_ratio": 1.60352422907489, "no_speech_prob": 1.3630446119350381e-05}, {"id": 1433, "seek": 565396, "start": 5653.96, "end": 5660.96, "text": " Because this row here in the RNN is going to be getting some state about this document.", "tokens": [1436, 341, 5386, 510, 294, 264, 45702, 45, 307, 516, 281, 312, 1242, 512, 1785, 466, 341, 4166, 13], "temperature": 0.0, "avg_logprob": -0.06153547763824463, "compression_ratio": 1.88135593220339, "no_speech_prob": 1.1124226148240268e-05}, {"id": 1434, "seek": 565396, "start": 5660.96, "end": 5664.96, "text": " So when it goes to the next batch, it needs to use that state.", "tokens": [407, 562, 309, 1709, 281, 264, 958, 15245, 11, 309, 2203, 281, 764, 300, 1785, 13], "temperature": 0.0, "avg_logprob": -0.06153547763824463, "compression_ratio": 1.88135593220339, "no_speech_prob": 1.1124226148240268e-05}, {"id": 1435, "seek": 565396, "start": 5664.96, "end": 5666.96, "text": " And then it goes to the next batch, it needs to use that state.", "tokens": [400, 550, 309, 1709, 281, 264, 958, 15245, 11, 309, 2203, 281, 764, 300, 1785, 13], "temperature": 0.0, "avg_logprob": -0.06153547763824463, "compression_ratio": 1.88135593220339, "no_speech_prob": 1.1124226148240268e-05}, {"id": 1436, "seek": 565396, "start": 5666.96, "end": 5671.96, "text": " So from batch to batch, the state that it's building up needs to be consistent.", "tokens": [407, 490, 15245, 281, 15245, 11, 264, 1785, 300, 309, 311, 2390, 493, 2203, 281, 312, 8398, 13], "temperature": 0.0, "avg_logprob": -0.06153547763824463, "compression_ratio": 1.88135593220339, "no_speech_prob": 1.1124226148240268e-05}, {"id": 1437, "seek": 565396, "start": 5671.96, "end": 5678.96, "text": " That's why we do the batches this way.", "tokens": [663, 311, 983, 321, 360, 264, 15245, 279, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.06153547763824463, "compression_ratio": 1.88135593220339, "no_speech_prob": 1.1124226148240268e-05}, {"id": 1438, "seek": 567896, "start": 5678.96, "end": 5691.96, "text": " I wanted to ask if you did any other preprocessing such as removing stop words, stemming, or lemmatization?", "tokens": [286, 1415, 281, 1029, 498, 291, 630, 604, 661, 2666, 340, 780, 278, 1270, 382, 12720, 1590, 2283, 11, 12312, 2810, 11, 420, 7495, 15677, 2144, 30], "temperature": 0.0, "avg_logprob": -0.13533994521217785, "compression_ratio": 1.5671641791044777, "no_speech_prob": 3.0225008231354877e-05}, {"id": 1439, "seek": 567896, "start": 5691.96, "end": 5692.96, "text": " Yeah, great question.", "tokens": [865, 11, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.13533994521217785, "compression_ratio": 1.5671641791044777, "no_speech_prob": 3.0225008231354877e-05}, {"id": 1440, "seek": 567896, "start": 5692.96, "end": 5696.96, "text": " So in traditional NLP, those are important things to do.", "tokens": [407, 294, 5164, 426, 45196, 11, 729, 366, 1021, 721, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.13533994521217785, "compression_ratio": 1.5671641791044777, "no_speech_prob": 3.0225008231354877e-05}, {"id": 1441, "seek": 567896, "start": 5696.96, "end": 5699.96, "text": " Removing stop words is removing words like ah and on.", "tokens": [46445, 798, 1590, 2283, 307, 12720, 2283, 411, 3716, 293, 322, 13], "temperature": 0.0, "avg_logprob": -0.13533994521217785, "compression_ratio": 1.5671641791044777, "no_speech_prob": 3.0225008231354877e-05}, {"id": 1442, "seek": 567896, "start": 5699.96, "end": 5706.96, "text": " And the stemming is like getting rid of the ing suffix or stuff like that.", "tokens": [400, 264, 12312, 2810, 307, 411, 1242, 3973, 295, 264, 3957, 3889, 970, 420, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.13533994521217785, "compression_ratio": 1.5671641791044777, "no_speech_prob": 3.0225008231354877e-05}, {"id": 1443, "seek": 570696, "start": 5706.96, "end": 5710.96, "text": " It's kind of like universal in traditional NLP.", "tokens": [467, 311, 733, 295, 411, 11455, 294, 5164, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.110481189644855, "compression_ratio": 1.5486725663716814, "no_speech_prob": 8.138753400999121e-06}, {"id": 1444, "seek": 570696, "start": 5710.96, "end": 5712.96, "text": " It's an absolutely terrible idea.", "tokens": [467, 311, 364, 3122, 6237, 1558, 13], "temperature": 0.0, "avg_logprob": -0.110481189644855, "compression_ratio": 1.5486725663716814, "no_speech_prob": 8.138753400999121e-06}, {"id": 1445, "seek": 570696, "start": 5712.96, "end": 5714.96, "text": " Never ever do this.", "tokens": [7344, 1562, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.110481189644855, "compression_ratio": 1.5486725663716814, "no_speech_prob": 8.138753400999121e-06}, {"id": 1446, "seek": 570696, "start": 5714.96, "end": 5718.96, "text": " Because the first question is why would you do it?", "tokens": [1436, 264, 700, 1168, 307, 983, 576, 291, 360, 309, 30], "temperature": 0.0, "avg_logprob": -0.110481189644855, "compression_ratio": 1.5486725663716814, "no_speech_prob": 8.138753400999121e-06}, {"id": 1447, "seek": 570696, "start": 5718.96, "end": 5724.96, "text": " Why would you remove information from your neural net which might be useful?", "tokens": [1545, 576, 291, 4159, 1589, 490, 428, 18161, 2533, 597, 1062, 312, 4420, 30], "temperature": 0.0, "avg_logprob": -0.110481189644855, "compression_ratio": 1.5486725663716814, "no_speech_prob": 8.138753400999121e-06}, {"id": 1448, "seek": 570696, "start": 5724.96, "end": 5726.96, "text": " And the fact is that it is useful.", "tokens": [400, 264, 1186, 307, 300, 309, 307, 4420, 13], "temperature": 0.0, "avg_logprob": -0.110481189644855, "compression_ratio": 1.5486725663716814, "no_speech_prob": 8.138753400999121e-06}, {"id": 1449, "seek": 570696, "start": 5726.96, "end": 5733.96, "text": " Like stop words, your use of stop words tells you a lot about what style of language.", "tokens": [1743, 1590, 2283, 11, 428, 764, 295, 1590, 2283, 5112, 291, 257, 688, 466, 437, 3758, 295, 2856, 13], "temperature": 0.0, "avg_logprob": -0.110481189644855, "compression_ratio": 1.5486725663716814, "no_speech_prob": 8.138753400999121e-06}, {"id": 1450, "seek": 573396, "start": 5733.96, "end": 5741.96, "text": " So you'll often have a lot less articles and stuff if you're really angry and speaking really quickly.", "tokens": [407, 291, 603, 2049, 362, 257, 688, 1570, 11290, 293, 1507, 498, 291, 434, 534, 6884, 293, 4124, 534, 2661, 13], "temperature": 0.0, "avg_logprob": -0.08761520578403666, "compression_ratio": 1.5847457627118644, "no_speech_prob": 3.5556704460759647e-06}, {"id": 1451, "seek": 573396, "start": 5741.96, "end": 5746.96, "text": " The tense you're talking about is obviously very important, so stemming gets rid of it.", "tokens": [440, 18760, 291, 434, 1417, 466, 307, 2745, 588, 1021, 11, 370, 12312, 2810, 2170, 3973, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.08761520578403666, "compression_ratio": 1.5847457627118644, "no_speech_prob": 3.5556704460759647e-06}, {"id": 1452, "seek": 573396, "start": 5746.96, "end": 5751.96, "text": " So yeah, all that kind of stuff is in the past.", "tokens": [407, 1338, 11, 439, 300, 733, 295, 1507, 307, 294, 264, 1791, 13], "temperature": 0.0, "avg_logprob": -0.08761520578403666, "compression_ratio": 1.5847457627118644, "no_speech_prob": 3.5556704460759647e-06}, {"id": 1453, "seek": 573396, "start": 5751.96, "end": 5753.96, "text": " You basically never want to do it.", "tokens": [509, 1936, 1128, 528, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.08761520578403666, "compression_ratio": 1.5847457627118644, "no_speech_prob": 3.5556704460759647e-06}, {"id": 1454, "seek": 573396, "start": 5753.96, "end": 5762.96, "text": " And in general, preprocessing data for neural nets, leave it as raw as you can is the rule of thumb.", "tokens": [400, 294, 2674, 11, 2666, 340, 780, 278, 1412, 337, 18161, 36170, 11, 1856, 309, 382, 8936, 382, 291, 393, 307, 264, 4978, 295, 9298, 13], "temperature": 0.0, "avg_logprob": -0.08761520578403666, "compression_ratio": 1.5847457627118644, "no_speech_prob": 3.5556704460759647e-06}, {"id": 1455, "seek": 576296, "start": 5762.96, "end": 5771.96, "text": " So for a language model, each mini batch is basically going to look something like this for the independent variable.", "tokens": [407, 337, 257, 2856, 2316, 11, 1184, 8382, 15245, 307, 1936, 516, 281, 574, 746, 411, 341, 337, 264, 6695, 7006, 13], "temperature": 0.0, "avg_logprob": -0.11719304388696021, "compression_ratio": 1.5726872246696035, "no_speech_prob": 2.1443558580358513e-05}, {"id": 1456, "seek": 576296, "start": 5771.96, "end": 5776.96, "text": " And then the dependent variable will be exactly the same thing but shifted over by one word.", "tokens": [400, 550, 264, 12334, 7006, 486, 312, 2293, 264, 912, 551, 457, 18892, 670, 538, 472, 1349, 13], "temperature": 0.0, "avg_logprob": -0.11719304388696021, "compression_ratio": 1.5726872246696035, "no_speech_prob": 2.1443558580358513e-05}, {"id": 1457, "seek": 576296, "start": 5776.96, "end": 5779.96, "text": " So let's create that.", "tokens": [407, 718, 311, 1884, 300, 13], "temperature": 0.0, "avg_logprob": -0.11719304388696021, "compression_ratio": 1.5726872246696035, "no_speech_prob": 2.1443558580358513e-05}, {"id": 1458, "seek": 576296, "start": 5779.96, "end": 5781.96, "text": " This thing's called LM Preloader.", "tokens": [639, 551, 311, 1219, 46529, 6001, 2907, 260, 13], "temperature": 0.0, "avg_logprob": -0.11719304388696021, "compression_ratio": 1.5726872246696035, "no_speech_prob": 2.1443558580358513e-05}, {"id": 1459, "seek": 576296, "start": 5781.96, "end": 5786.96, "text": " It would actually be better off being called an LM data set.", "tokens": [467, 576, 767, 312, 1101, 766, 885, 1219, 364, 46529, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.11719304388696021, "compression_ratio": 1.5726872246696035, "no_speech_prob": 2.1443558580358513e-05}, {"id": 1460, "seek": 576296, "start": 5786.96, "end": 5788.96, "text": " Why don't we do it right now?", "tokens": [1545, 500, 380, 321, 360, 309, 558, 586, 30], "temperature": 0.0, "avg_logprob": -0.11719304388696021, "compression_ratio": 1.5726872246696035, "no_speech_prob": 2.1443558580358513e-05}, {"id": 1461, "seek": 578896, "start": 5788.96, "end": 5797.96, "text": " LM Preloader, LM data set.", "tokens": [46529, 6001, 2907, 260, 11, 46529, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.10526986849510063, "compression_ratio": 1.4580152671755726, "no_speech_prob": 9.66561128734611e-06}, {"id": 1462, "seek": 578896, "start": 5797.96, "end": 5803.96, "text": " That's really what it is.", "tokens": [663, 311, 534, 437, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.10526986849510063, "compression_ratio": 1.4580152671755726, "no_speech_prob": 9.66561128734611e-06}, {"id": 1463, "seek": 578896, "start": 5803.96, "end": 5806.96, "text": " Okay, so an LM data set is a data set for a language model.", "tokens": [1033, 11, 370, 364, 46529, 1412, 992, 307, 257, 1412, 992, 337, 257, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10526986849510063, "compression_ratio": 1.4580152671755726, "no_speech_prob": 9.66561128734611e-06}, {"id": 1464, "seek": 578896, "start": 5806.96, "end": 5812.96, "text": " Remember that a data set is defined as something with a length and a get item.", "tokens": [5459, 300, 257, 1412, 992, 307, 7642, 382, 746, 365, 257, 4641, 293, 257, 483, 3174, 13], "temperature": 0.0, "avg_logprob": -0.10526986849510063, "compression_ratio": 1.4580152671755726, "no_speech_prob": 9.66561128734611e-06}, {"id": 1465, "seek": 581296, "start": 5812.96, "end": 5821.96, "text": " So this is a data set which you can index into it, and it will grab an independent variable and a dependent variable.", "tokens": [407, 341, 307, 257, 1412, 992, 597, 291, 393, 8186, 666, 309, 11, 293, 309, 486, 4444, 364, 6695, 7006, 293, 257, 12334, 7006, 13], "temperature": 0.0, "avg_logprob": -0.08772528171539307, "compression_ratio": 1.847926267281106, "no_speech_prob": 1.1124886441393755e-05}, {"id": 1466, "seek": 581296, "start": 5821.96, "end": 5828.96, "text": " And the independent variable is just the text from wherever you asked for for a BPTT.", "tokens": [400, 264, 6695, 7006, 307, 445, 264, 2487, 490, 8660, 291, 2351, 337, 337, 257, 40533, 28178, 13], "temperature": 0.0, "avg_logprob": -0.08772528171539307, "compression_ratio": 1.847926267281106, "no_speech_prob": 1.1124886441393755e-05}, {"id": 1467, "seek": 581296, "start": 5828.96, "end": 5832.96, "text": " And the dependent variable is the same thing offset by one.", "tokens": [400, 264, 12334, 7006, 307, 264, 912, 551, 18687, 538, 472, 13], "temperature": 0.0, "avg_logprob": -0.08772528171539307, "compression_ratio": 1.847926267281106, "no_speech_prob": 1.1124886441393755e-05}, {"id": 1468, "seek": 581296, "start": 5832.96, "end": 5834.96, "text": " So you can see it here.", "tokens": [407, 291, 393, 536, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.08772528171539307, "compression_ratio": 1.847926267281106, "no_speech_prob": 1.1124886441393755e-05}, {"id": 1469, "seek": 581296, "start": 5834.96, "end": 5837.96, "text": " We can create a data loader using that data set.", "tokens": [492, 393, 1884, 257, 1412, 3677, 260, 1228, 300, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.08772528171539307, "compression_ratio": 1.847926267281106, "no_speech_prob": 1.1124886441393755e-05}, {"id": 1470, "seek": 581296, "start": 5837.96, "end": 5840.96, "text": " Remember that's how data loaders work. You pass them a data set.", "tokens": [5459, 300, 311, 577, 1412, 3677, 433, 589, 13, 509, 1320, 552, 257, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.08772528171539307, "compression_ratio": 1.847926267281106, "no_speech_prob": 1.1124886441393755e-05}, {"id": 1471, "seek": 584096, "start": 5840.96, "end": 5845.96, "text": " And now we have something that we can iterate through, grabbing a mini batch at a time.", "tokens": [400, 586, 321, 362, 746, 300, 321, 393, 44497, 807, 11, 23771, 257, 8382, 15245, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.10111868156577056, "compression_ratio": 1.6983471074380165, "no_speech_prob": 1.2606549717020243e-05}, {"id": 1472, "seek": 584096, "start": 5845.96, "end": 5854.96, "text": " And you can see here, X is XXBOS well worth watching, and Y is just well worth watching.", "tokens": [400, 291, 393, 536, 510, 11, 1783, 307, 27050, 33, 4367, 731, 3163, 1976, 11, 293, 398, 307, 445, 731, 3163, 1976, 13], "temperature": 0.0, "avg_logprob": -0.10111868156577056, "compression_ratio": 1.6983471074380165, "no_speech_prob": 1.2606549717020243e-05}, {"id": 1473, "seek": 584096, "start": 5854.96, "end": 5860.96, "text": " And then you can see the second batch, best performance to date.", "tokens": [400, 550, 291, 393, 536, 264, 1150, 15245, 11, 1151, 3389, 281, 4002, 13], "temperature": 0.0, "avg_logprob": -0.10111868156577056, "compression_ratio": 1.6983471074380165, "no_speech_prob": 1.2606549717020243e-05}, {"id": 1474, "seek": 584096, "start": 5860.96, "end": 5863.96, "text": " So make sure you print out things that all make sense.", "tokens": [407, 652, 988, 291, 4482, 484, 721, 300, 439, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.10111868156577056, "compression_ratio": 1.6983471074380165, "no_speech_prob": 1.2606549717020243e-05}, {"id": 1475, "seek": 584096, "start": 5863.96, "end": 5869.96, "text": " So that's stuff that we can all dump into a single function and use it again later and chuck it into a data bunch.", "tokens": [407, 300, 311, 1507, 300, 321, 393, 439, 11430, 666, 257, 2167, 2445, 293, 764, 309, 797, 1780, 293, 20870, 309, 666, 257, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.10111868156577056, "compression_ratio": 1.6983471074380165, "no_speech_prob": 1.2606549717020243e-05}, {"id": 1476, "seek": 586996, "start": 5869.96, "end": 5876.96, "text": " So that's all we need for a data bunch for language models.", "tokens": [407, 300, 311, 439, 321, 643, 337, 257, 1412, 3840, 337, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.08478597153064817, "compression_ratio": 1.7553191489361701, "no_speech_prob": 2.144324935215991e-05}, {"id": 1477, "seek": 586996, "start": 5876.96, "end": 5879.96, "text": " We're also going to need a data bunch for classification.", "tokens": [492, 434, 611, 516, 281, 643, 257, 1412, 3840, 337, 21538, 13], "temperature": 0.0, "avg_logprob": -0.08478597153064817, "compression_ratio": 1.7553191489361701, "no_speech_prob": 2.144324935215991e-05}, {"id": 1478, "seek": 586996, "start": 5879.96, "end": 5885.96, "text": " And that one's going to be super easy because we already know how to create data bunches for classification,", "tokens": [400, 300, 472, 311, 516, 281, 312, 1687, 1858, 570, 321, 1217, 458, 577, 281, 1884, 1412, 3840, 279, 337, 21538, 11], "temperature": 0.0, "avg_logprob": -0.08478597153064817, "compression_ratio": 1.7553191489361701, "no_speech_prob": 2.144324935215991e-05}, {"id": 1479, "seek": 586996, "start": 5885.96, "end": 5888.96, "text": " because we've already done it for lots of image models.", "tokens": [570, 321, 600, 1217, 1096, 309, 337, 3195, 295, 3256, 5245, 13], "temperature": 0.0, "avg_logprob": -0.08478597153064817, "compression_ratio": 1.7553191489361701, "no_speech_prob": 2.144324935215991e-05}, {"id": 1480, "seek": 586996, "start": 5888.96, "end": 5891.96, "text": " And for NLP, it's going to be exactly the same.", "tokens": [400, 337, 426, 45196, 11, 309, 311, 516, 281, 312, 2293, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.08478597153064817, "compression_ratio": 1.7553191489361701, "no_speech_prob": 2.144324935215991e-05}, {"id": 1481, "seek": 589196, "start": 5891.96, "end": 5900.96, "text": " So we create an item list. We split. We label. That's it.", "tokens": [407, 321, 1884, 364, 3174, 1329, 13, 492, 7472, 13, 492, 7645, 13, 663, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.11043474707804934, "compression_ratio": 1.4189944134078212, "no_speech_prob": 3.1380604923469946e-06}, {"id": 1482, "seek": 589196, "start": 5900.96, "end": 5903.96, "text": " So the stuff we did for image is not different.", "tokens": [407, 264, 1507, 321, 630, 337, 3256, 307, 406, 819, 13], "temperature": 0.0, "avg_logprob": -0.11043474707804934, "compression_ratio": 1.4189944134078212, "no_speech_prob": 3.1380604923469946e-06}, {"id": 1483, "seek": 589196, "start": 5903.96, "end": 5910.96, "text": " The only thing we've added is two preprocessors.", "tokens": [440, 787, 551, 321, 600, 3869, 307, 732, 2666, 340, 45700, 13], "temperature": 0.0, "avg_logprob": -0.11043474707804934, "compression_ratio": 1.4189944134078212, "no_speech_prob": 3.1380604923469946e-06}, {"id": 1484, "seek": 589196, "start": 5910.96, "end": 5915.96, "text": " Question.", "tokens": [14464, 13], "temperature": 0.0, "avg_logprob": -0.11043474707804934, "compression_ratio": 1.4189944134078212, "no_speech_prob": 3.1380604923469946e-06}, {"id": 1485, "seek": 589196, "start": 5915.96, "end": 5920.96, "text": " What are the trade-offs to consider between batch size and back propagation through time?", "tokens": [708, 366, 264, 4923, 12, 19231, 281, 1949, 1296, 15245, 2744, 293, 646, 38377, 807, 565, 30], "temperature": 0.0, "avg_logprob": -0.11043474707804934, "compression_ratio": 1.4189944134078212, "no_speech_prob": 3.1380604923469946e-06}, {"id": 1486, "seek": 592096, "start": 5920.96, "end": 5930.96, "text": " For example, BPTT 10 with BS 100 versus BPTT 100 with BS 10, both would be passing a thousand tokens at a time to the model.", "tokens": [1171, 1365, 11, 40533, 28178, 1266, 365, 27253, 2319, 5717, 40533, 28178, 2319, 365, 27253, 1266, 11, 1293, 576, 312, 8437, 257, 4714, 22667, 412, 257, 565, 281, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.0833761287185381, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.3620058780361433e-05}, {"id": 1487, "seek": 592096, "start": 5930.96, "end": 5933.96, "text": " What should you consider when tuning the ratio?", "tokens": [708, 820, 291, 1949, 562, 15164, 264, 8509, 30], "temperature": 0.0, "avg_logprob": -0.0833761287185381, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.3620058780361433e-05}, {"id": 1488, "seek": 592096, "start": 5933.96, "end": 5937.96, "text": " It's a great question. I don't know the answer. I would love to know.", "tokens": [467, 311, 257, 869, 1168, 13, 286, 500, 380, 458, 264, 1867, 13, 286, 576, 959, 281, 458, 13], "temperature": 0.0, "avg_logprob": -0.0833761287185381, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.3620058780361433e-05}, {"id": 1489, "seek": 592096, "start": 5937.96, "end": 5940.96, "text": " So try it, because I haven't had time to fiddle with it.", "tokens": [407, 853, 309, 11, 570, 286, 2378, 380, 632, 565, 281, 24553, 2285, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.0833761287185381, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.3620058780361433e-05}, {"id": 1490, "seek": 592096, "start": 5940.96, "end": 5948.96, "text": " I haven't seen anybody else experiment with it. So that would make a super great experiment.", "tokens": [286, 2378, 380, 1612, 4472, 1646, 5120, 365, 309, 13, 407, 300, 576, 652, 257, 1687, 869, 5120, 13], "temperature": 0.0, "avg_logprob": -0.0833761287185381, "compression_ratio": 1.6265560165975104, "no_speech_prob": 1.3620058780361433e-05}, {"id": 1491, "seek": 594896, "start": 5948.96, "end": 5954.96, "text": " I think the batch size is the thing that lets it parallelize.", "tokens": [286, 519, 264, 15245, 2744, 307, 264, 551, 300, 6653, 309, 8952, 1125, 13], "temperature": 0.0, "avg_logprob": -0.08121414573825135, "compression_ratio": 1.5973451327433628, "no_speech_prob": 2.7100282750325277e-05}, {"id": 1492, "seek": 594896, "start": 5954.96, "end": 5959.96, "text": " So if you don't have a large enough batch size, it's just going to be really slow.", "tokens": [407, 498, 291, 500, 380, 362, 257, 2416, 1547, 15245, 2744, 11, 309, 311, 445, 516, 281, 312, 534, 2964, 13], "temperature": 0.0, "avg_logprob": -0.08121414573825135, "compression_ratio": 1.5973451327433628, "no_speech_prob": 2.7100282750325277e-05}, {"id": 1493, "seek": 594896, "start": 5959.96, "end": 5969.96, "text": " But on the other hand, a large batch size with a short BPTT, depending on how you use it, you may end up ending up with less state that's being back propagated.", "tokens": [583, 322, 264, 661, 1011, 11, 257, 2416, 15245, 2744, 365, 257, 2099, 40533, 28178, 11, 5413, 322, 577, 291, 764, 309, 11, 291, 815, 917, 493, 8121, 493, 365, 1570, 1785, 300, 311, 885, 646, 12425, 770, 13], "temperature": 0.0, "avg_logprob": -0.08121414573825135, "compression_ratio": 1.5973451327433628, "no_speech_prob": 2.7100282750325277e-05}, {"id": 1494, "seek": 594896, "start": 5969.96, "end": 5973.96, "text": " So the question of how much that matters, I'm not sure.", "tokens": [407, 264, 1168, 295, 577, 709, 300, 7001, 11, 286, 478, 406, 988, 13], "temperature": 0.0, "avg_logprob": -0.08121414573825135, "compression_ratio": 1.5973451327433628, "no_speech_prob": 2.7100282750325277e-05}, {"id": 1495, "seek": 597396, "start": 5973.96, "end": 5980.96, "text": " But when we get to our ULM fit classification model, I'll actually show you this, kind of where this comes in.", "tokens": [583, 562, 321, 483, 281, 527, 624, 43, 44, 3318, 21538, 2316, 11, 286, 603, 767, 855, 291, 341, 11, 733, 295, 689, 341, 1487, 294, 13], "temperature": 0.0, "avg_logprob": -0.08620021114610646, "compression_ratio": 1.4728260869565217, "no_speech_prob": 1.6963500456768088e-05}, {"id": 1496, "seek": 597396, "start": 5980.96, "end": 5987.96, "text": " OK, so here's a couple of examples of a document and a dependent variable.", "tokens": [2264, 11, 370, 510, 311, 257, 1916, 295, 5110, 295, 257, 4166, 293, 257, 12334, 7006, 13], "temperature": 0.0, "avg_logprob": -0.08620021114610646, "compression_ratio": 1.4728260869565217, "no_speech_prob": 1.6963500456768088e-05}, {"id": 1497, "seek": 597396, "start": 5987.96, "end": 5995.96, "text": " And what we're going to be doing is we're going to be creating data loaders for them.", "tokens": [400, 437, 321, 434, 516, 281, 312, 884, 307, 321, 434, 516, 281, 312, 4084, 1412, 3677, 433, 337, 552, 13], "temperature": 0.0, "avg_logprob": -0.08620021114610646, "compression_ratio": 1.4728260869565217, "no_speech_prob": 1.6963500456768088e-05}, {"id": 1498, "seek": 599596, "start": 5995.96, "end": 6008.96, "text": " But we do have one trick here, which is that with images, our images were always, by the time we got to modeling, they were all the same size.", "tokens": [583, 321, 360, 362, 472, 4282, 510, 11, 597, 307, 300, 365, 5267, 11, 527, 5267, 645, 1009, 11, 538, 264, 565, 321, 658, 281, 15983, 11, 436, 645, 439, 264, 912, 2744, 13], "temperature": 0.0, "avg_logprob": -0.05391173834329123, "compression_ratio": 1.5924369747899159, "no_speech_prob": 4.936878212902229e-06}, {"id": 1499, "seek": 599596, "start": 6008.96, "end": 6011.96, "text": " Now, this is probably not how things should be.", "tokens": [823, 11, 341, 307, 1391, 406, 577, 721, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.05391173834329123, "compression_ratio": 1.5924369747899159, "no_speech_prob": 4.936878212902229e-06}, {"id": 1500, "seek": 599596, "start": 6011.96, "end": 6022.96, "text": " And we have started doing some experiments with training with rectangular images of different sizes, but we're not quite ready to show you that work because it's still a little bit fiddly.", "tokens": [400, 321, 362, 1409, 884, 512, 12050, 365, 3097, 365, 31167, 5267, 295, 819, 11602, 11, 457, 321, 434, 406, 1596, 1919, 281, 855, 291, 300, 589, 570, 309, 311, 920, 257, 707, 857, 283, 14273, 356, 13], "temperature": 0.0, "avg_logprob": -0.05391173834329123, "compression_ratio": 1.5924369747899159, "no_speech_prob": 4.936878212902229e-06}, {"id": 1501, "seek": 602296, "start": 6022.96, "end": 6025.96, "text": " But for text, we can't avoid it.", "tokens": [583, 337, 2487, 11, 321, 393, 380, 5042, 309, 13], "temperature": 0.0, "avg_logprob": -0.056536501929873514, "compression_ratio": 1.6720647773279351, "no_speech_prob": 1.1841891137009952e-05}, {"id": 1502, "seek": 602296, "start": 6025.96, "end": 6029.96, "text": " We've got different size texts coming in, so we have to deal with it.", "tokens": [492, 600, 658, 819, 2744, 15765, 1348, 294, 11, 370, 321, 362, 281, 2028, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.056536501929873514, "compression_ratio": 1.6720647773279351, "no_speech_prob": 1.1841891137009952e-05}, {"id": 1503, "seek": 602296, "start": 6029.96, "end": 6036.96, "text": " And the way we deal with it is almost identical to how actually we're going to end up dealing with when we do do rectangular images.", "tokens": [400, 264, 636, 321, 2028, 365, 309, 307, 1920, 14800, 281, 577, 767, 321, 434, 516, 281, 917, 493, 6260, 365, 562, 321, 360, 360, 31167, 5267, 13], "temperature": 0.0, "avg_logprob": -0.056536501929873514, "compression_ratio": 1.6720647773279351, "no_speech_prob": 1.1841891137009952e-05}, {"id": 1504, "seek": 602296, "start": 6036.96, "end": 6040.96, "text": " So if you are interested in rectangular images, try and basically copy this approach.", "tokens": [407, 498, 291, 366, 3102, 294, 31167, 5267, 11, 853, 293, 1936, 5055, 341, 3109, 13], "temperature": 0.0, "avg_logprob": -0.056536501929873514, "compression_ratio": 1.6720647773279351, "no_speech_prob": 1.1841891137009952e-05}, {"id": 1505, "seek": 602296, "start": 6040.96, "end": 6043.96, "text": " Here's the approach.", "tokens": [1692, 311, 264, 3109, 13], "temperature": 0.0, "avg_logprob": -0.056536501929873514, "compression_ratio": 1.6720647773279351, "no_speech_prob": 1.1841891137009952e-05}, {"id": 1506, "seek": 602296, "start": 6043.96, "end": 6050.96, "text": " We are going to pad each document by adding a bunch of padding tokens.", "tokens": [492, 366, 516, 281, 6887, 1184, 4166, 538, 5127, 257, 3840, 295, 39562, 22667, 13], "temperature": 0.0, "avg_logprob": -0.056536501929873514, "compression_ratio": 1.6720647773279351, "no_speech_prob": 1.1841891137009952e-05}, {"id": 1507, "seek": 605096, "start": 6050.96, "end": 6056.96, "text": " So we just pick some arbitrary token, which we're going to tell PyTorch this token isn't text.", "tokens": [407, 321, 445, 1888, 512, 23211, 14862, 11, 597, 321, 434, 516, 281, 980, 9953, 51, 284, 339, 341, 14862, 1943, 380, 2487, 13], "temperature": 0.0, "avg_logprob": -0.08101985478165126, "compression_ratio": 1.728110599078341, "no_speech_prob": 1.2410542694851756e-05}, {"id": 1508, "seek": 605096, "start": 6056.96, "end": 6063.96, "text": " It's just thrown in there because we have to put in something to make a rectangular tensor.", "tokens": [467, 311, 445, 11732, 294, 456, 570, 321, 362, 281, 829, 294, 746, 281, 652, 257, 31167, 40863, 13], "temperature": 0.0, "avg_logprob": -0.08101985478165126, "compression_ratio": 1.728110599078341, "no_speech_prob": 1.2410542694851756e-05}, {"id": 1509, "seek": 605096, "start": 6063.96, "end": 6075.96, "text": " If we have a mini-batch with a 1,000 word document and then a 2,000 word document and then a 20 word document, the 20 word document is going to end up with 1,980 padding tokens on the end.", "tokens": [759, 321, 362, 257, 8382, 12, 65, 852, 365, 257, 502, 11, 1360, 1349, 4166, 293, 550, 257, 568, 11, 1360, 1349, 4166, 293, 550, 257, 945, 1349, 4166, 11, 264, 945, 1349, 4166, 307, 516, 281, 917, 493, 365, 502, 11, 24, 4702, 39562, 22667, 322, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.08101985478165126, "compression_ratio": 1.728110599078341, "no_speech_prob": 1.2410542694851756e-05}, {"id": 1510, "seek": 607596, "start": 6075.96, "end": 6083.96, "text": " And as we go through the RNN, we're going to be totally pointlessly calculating on all these padding tokens.", "tokens": [400, 382, 321, 352, 807, 264, 45702, 45, 11, 321, 434, 516, 281, 312, 3879, 935, 12048, 28258, 322, 439, 613, 39562, 22667, 13], "temperature": 0.0, "avg_logprob": -0.07598515919276647, "compression_ratio": 1.6804123711340206, "no_speech_prob": 1.6441921616205946e-05}, {"id": 1511, "seek": 607596, "start": 6083.96, "end": 6085.96, "text": " We don't want to do that.", "tokens": [492, 500, 380, 528, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.07598515919276647, "compression_ratio": 1.6804123711340206, "no_speech_prob": 1.6441921616205946e-05}, {"id": 1512, "seek": 607596, "start": 6085.96, "end": 6091.96, "text": " So the trick is to sort the data first by length.", "tokens": [407, 264, 4282, 307, 281, 1333, 264, 1412, 700, 538, 4641, 13], "temperature": 0.0, "avg_logprob": -0.07598515919276647, "compression_ratio": 1.6804123711340206, "no_speech_prob": 1.6441921616205946e-05}, {"id": 1513, "seek": 607596, "start": 6091.96, "end": 6100.96, "text": " So that way, your first mini-batch will contain your really long documents and your last mini-batch will contain your really short documents.", "tokens": [407, 300, 636, 11, 428, 700, 8382, 12, 65, 852, 486, 5304, 428, 534, 938, 8512, 293, 428, 1036, 8382, 12, 65, 852, 486, 5304, 428, 534, 2099, 8512, 13], "temperature": 0.0, "avg_logprob": -0.07598515919276647, "compression_ratio": 1.6804123711340206, "no_speech_prob": 1.6441921616205946e-05}, {"id": 1514, "seek": 610096, "start": 6100.96, "end": 6109.96, "text": " Each mini-batch will not contain a very wide variety of lengths of documents, so there won't be much padding and so there won't be much wasted computation.", "tokens": [6947, 8382, 12, 65, 852, 486, 406, 5304, 257, 588, 4874, 5673, 295, 26329, 295, 8512, 11, 370, 456, 1582, 380, 312, 709, 39562, 293, 370, 456, 1582, 380, 312, 709, 19496, 24903, 13], "temperature": 0.0, "avg_logprob": -0.06640527096200496, "compression_ratio": 1.6238938053097345, "no_speech_prob": 3.905433004547376e-06}, {"id": 1515, "seek": 610096, "start": 6109.96, "end": 6112.96, "text": " So we've already looked at samplers.", "tokens": [407, 321, 600, 1217, 2956, 412, 3247, 564, 433, 13], "temperature": 0.0, "avg_logprob": -0.06640527096200496, "compression_ratio": 1.6238938053097345, "no_speech_prob": 3.905433004547376e-06}, {"id": 1516, "seek": 610096, "start": 6112.96, "end": 6120.96, "text": " If you've forgotten, go back to when we created our data loader from scratch and we actually created a sampler.", "tokens": [759, 291, 600, 11832, 11, 352, 646, 281, 562, 321, 2942, 527, 1412, 3677, 260, 490, 8459, 293, 321, 767, 2942, 257, 3247, 22732, 13], "temperature": 0.0, "avg_logprob": -0.06640527096200496, "compression_ratio": 1.6238938053097345, "no_speech_prob": 3.905433004547376e-06}, {"id": 1517, "seek": 610096, "start": 6120.96, "end": 6122.96, "text": " And so here we're going to create a different type of sampler.", "tokens": [400, 370, 510, 321, 434, 516, 281, 1884, 257, 819, 2010, 295, 3247, 22732, 13], "temperature": 0.0, "avg_logprob": -0.06640527096200496, "compression_ratio": 1.6238938053097345, "no_speech_prob": 3.905433004547376e-06}, {"id": 1518, "seek": 612296, "start": 6122.96, "end": 6134.96, "text": " And it is simply one that goes through our data, looks at how many documents is in it, creates the range from zero to the number of documents,", "tokens": [400, 309, 307, 2935, 472, 300, 1709, 807, 527, 1412, 11, 1542, 412, 577, 867, 8512, 307, 294, 309, 11, 7829, 264, 3613, 490, 4018, 281, 264, 1230, 295, 8512, 11], "temperature": 0.0, "avg_logprob": -0.06914554891132173, "compression_ratio": 1.6103286384976525, "no_speech_prob": 2.5612400804675417e-06}, {"id": 1519, "seek": 612296, "start": 6134.96, "end": 6141.96, "text": " sorts them by some key and returns that iterator, sorts them in reverse order.", "tokens": [7527, 552, 538, 512, 2141, 293, 11247, 300, 17138, 1639, 11, 7527, 552, 294, 9943, 1668, 13], "temperature": 0.0, "avg_logprob": -0.06914554891132173, "compression_ratio": 1.6103286384976525, "no_speech_prob": 2.5612400804675417e-06}, {"id": 1520, "seek": 612296, "start": 6141.96, "end": 6150.96, "text": " So we're going to use sort sampler, passing in the key, which is a lambda function that grabs the length of the document.", "tokens": [407, 321, 434, 516, 281, 764, 1333, 3247, 22732, 11, 8437, 294, 264, 2141, 11, 597, 307, 257, 13607, 2445, 300, 30028, 264, 4641, 295, 264, 4166, 13], "temperature": 0.0, "avg_logprob": -0.06914554891132173, "compression_ratio": 1.6103286384976525, "no_speech_prob": 2.5612400804675417e-06}, {"id": 1521, "seek": 615096, "start": 6150.96, "end": 6163.96, "text": " So that way, our sampler is going to cause each mini-batch to be documents of similar lengths.", "tokens": [407, 300, 636, 11, 527, 3247, 22732, 307, 516, 281, 3082, 1184, 8382, 12, 65, 852, 281, 312, 8512, 295, 2531, 26329, 13], "temperature": 0.0, "avg_logprob": -0.0633106512181899, "compression_ratio": 1.5363128491620113, "no_speech_prob": 2.2958870431466494e-06}, {"id": 1522, "seek": 615096, "start": 6163.96, "end": 6170.96, "text": " The problem is we can only do this for validation, not for training, because for training we want to shuffle.", "tokens": [440, 1154, 307, 321, 393, 787, 360, 341, 337, 24071, 11, 406, 337, 3097, 11, 570, 337, 3097, 321, 528, 281, 39426, 13], "temperature": 0.0, "avg_logprob": -0.0633106512181899, "compression_ratio": 1.5363128491620113, "no_speech_prob": 2.2958870431466494e-06}, {"id": 1523, "seek": 615096, "start": 6170.96, "end": 6176.96, "text": " And sorting would undo any shuffling because sorting is deterministic.", "tokens": [400, 32411, 576, 23779, 604, 402, 1245, 1688, 570, 32411, 307, 15957, 3142, 13], "temperature": 0.0, "avg_logprob": -0.0633106512181899, "compression_ratio": 1.5363128491620113, "no_speech_prob": 2.2958870431466494e-06}, {"id": 1524, "seek": 617696, "start": 6176.96, "end": 6180.96, "text": " So that's why we create something called sort-ish sampler.", "tokens": [407, 300, 311, 983, 321, 1884, 746, 1219, 1333, 12, 742, 3247, 22732, 13], "temperature": 0.0, "avg_logprob": -0.05854479317526216, "compression_ratio": 1.702127659574468, "no_speech_prob": 5.1737356443481985e-06}, {"id": 1525, "seek": 617696, "start": 6180.96, "end": 6187.96, "text": " And the sort-ish sampler approximately orders things by length.", "tokens": [400, 264, 1333, 12, 742, 3247, 22732, 10447, 9470, 721, 538, 4641, 13], "temperature": 0.0, "avg_logprob": -0.05854479317526216, "compression_ratio": 1.702127659574468, "no_speech_prob": 5.1737356443481985e-06}, {"id": 1526, "seek": 617696, "start": 6187.96, "end": 6192.96, "text": " So every mini-batch has things of similar lengths, but with some randomness.", "tokens": [407, 633, 8382, 12, 65, 852, 575, 721, 295, 2531, 26329, 11, 457, 365, 512, 4974, 1287, 13], "temperature": 0.0, "avg_logprob": -0.05854479317526216, "compression_ratio": 1.702127659574468, "no_speech_prob": 5.1737356443481985e-06}, {"id": 1527, "seek": 617696, "start": 6192.96, "end": 6198.96, "text": " And the way we do this, the details don't particularly matter, but basically I've created this idea of a mega-batch,", "tokens": [400, 264, 636, 321, 360, 341, 11, 264, 4365, 500, 380, 4098, 1871, 11, 457, 1936, 286, 600, 2942, 341, 1558, 295, 257, 17986, 12, 65, 852, 11], "temperature": 0.0, "avg_logprob": -0.05854479317526216, "compression_ratio": 1.702127659574468, "no_speech_prob": 5.1737356443481985e-06}, {"id": 1528, "seek": 617696, "start": 6198.96, "end": 6204.96, "text": " which is something that's 50 times bigger than a batch, and basically I sort those.", "tokens": [597, 307, 746, 300, 311, 2625, 1413, 3801, 813, 257, 15245, 11, 293, 1936, 286, 1333, 729, 13], "temperature": 0.0, "avg_logprob": -0.05854479317526216, "compression_ratio": 1.702127659574468, "no_speech_prob": 5.1737356443481985e-06}, {"id": 1529, "seek": 620496, "start": 6204.96, "end": 6211.96, "text": " And so you end up with these sort of like sorted mega-batches, and then I have random permutations within that.", "tokens": [400, 370, 291, 917, 493, 365, 613, 1333, 295, 411, 25462, 17986, 12, 65, 852, 279, 11, 293, 550, 286, 362, 4974, 4784, 325, 763, 1951, 300, 13], "temperature": 0.0, "avg_logprob": -0.08661246535801652, "compression_ratio": 1.759433962264151, "no_speech_prob": 2.318386577826459e-05}, {"id": 1530, "seek": 620496, "start": 6211.96, "end": 6214.96, "text": " So you can see random permutations there and there.", "tokens": [407, 291, 393, 536, 4974, 4784, 325, 763, 456, 293, 456, 13], "temperature": 0.0, "avg_logprob": -0.08661246535801652, "compression_ratio": 1.759433962264151, "no_speech_prob": 2.318386577826459e-05}, {"id": 1531, "seek": 620496, "start": 6214.96, "end": 6217.96, "text": " So you can look at the code if you care, the details don't matter.", "tokens": [407, 291, 393, 574, 412, 264, 3089, 498, 291, 1127, 11, 264, 4365, 500, 380, 1871, 13], "temperature": 0.0, "avg_logprob": -0.08661246535801652, "compression_ratio": 1.759433962264151, "no_speech_prob": 2.318386577826459e-05}, {"id": 1532, "seek": 620496, "start": 6217.96, "end": 6227.96, "text": " In the end, it's a random sort in which things of similar lengths tend to be next to each other, and the biggest ones tend to be at the start.", "tokens": [682, 264, 917, 11, 309, 311, 257, 4974, 1333, 294, 597, 721, 295, 2531, 26329, 3928, 281, 312, 958, 281, 1184, 661, 11, 293, 264, 3880, 2306, 3928, 281, 312, 412, 264, 722, 13], "temperature": 0.0, "avg_logprob": -0.08661246535801652, "compression_ratio": 1.759433962264151, "no_speech_prob": 2.318386577826459e-05}, {"id": 1533, "seek": 622796, "start": 6227.96, "end": 6235.96, "text": " So now we've got a mini-batch of numericalized, tokenized documents of similar lengths, but they're not identical lengths.", "tokens": [407, 586, 321, 600, 658, 257, 8382, 12, 65, 852, 295, 29054, 1602, 11, 14862, 1602, 8512, 295, 2531, 26329, 11, 457, 436, 434, 406, 14800, 26329, 13], "temperature": 0.0, "avg_logprob": -0.08068815521571947, "compression_ratio": 1.6079295154185023, "no_speech_prob": 5.9548169701884035e-06}, {"id": 1534, "seek": 622796, "start": 6235.96, "end": 6245.96, "text": " And so you might remember the other thing when we first created a data loader, we gave it two things, a sampler and a collate function.", "tokens": [400, 370, 291, 1062, 1604, 264, 661, 551, 562, 321, 700, 2942, 257, 1412, 3677, 260, 11, 321, 2729, 309, 732, 721, 11, 257, 3247, 22732, 293, 257, 1263, 473, 2445, 13], "temperature": 0.0, "avg_logprob": -0.08068815521571947, "compression_ratio": 1.6079295154185023, "no_speech_prob": 5.9548169701884035e-06}, {"id": 1535, "seek": 622796, "start": 6245.96, "end": 6253.96, "text": " And the collate function that we wrote simply said torch.stack, because all our images were the same size.", "tokens": [400, 264, 1263, 473, 2445, 300, 321, 4114, 2935, 848, 27822, 13, 372, 501, 11, 570, 439, 527, 5267, 645, 264, 912, 2744, 13], "temperature": 0.0, "avg_logprob": -0.08068815521571947, "compression_ratio": 1.6079295154185023, "no_speech_prob": 5.9548169701884035e-06}, {"id": 1536, "seek": 625396, "start": 6253.96, "end": 6259.96, "text": " So we could just literally just stick them together. We can't do that for documents because they're different sizes.", "tokens": [407, 321, 727, 445, 3736, 445, 2897, 552, 1214, 13, 492, 393, 380, 360, 300, 337, 8512, 570, 436, 434, 819, 11602, 13], "temperature": 0.0, "avg_logprob": -0.09600966164235318, "compression_ratio": 1.6103896103896105, "no_speech_prob": 8.93944798008306e-06}, {"id": 1537, "seek": 625396, "start": 6259.96, "end": 6262.96, "text": " So we've written something called pad collate.", "tokens": [407, 321, 600, 3720, 746, 1219, 6887, 1263, 473, 13], "temperature": 0.0, "avg_logprob": -0.09600966164235318, "compression_ratio": 1.6103896103896105, "no_speech_prob": 8.93944798008306e-06}, {"id": 1538, "seek": 625396, "start": 6262.96, "end": 6270.96, "text": " And what Sylvain did here was he basically said, let's create something that's big enough to handle the longest document in the mini-batch,", "tokens": [400, 437, 3902, 14574, 491, 630, 510, 390, 415, 1936, 848, 11, 718, 311, 1884, 746, 300, 311, 955, 1547, 281, 4813, 264, 15438, 4166, 294, 264, 8382, 12, 65, 852, 11], "temperature": 0.0, "avg_logprob": -0.09600966164235318, "compression_ratio": 1.6103896103896105, "no_speech_prob": 8.93944798008306e-06}, {"id": 1539, "seek": 625396, "start": 6270.96, "end": 6276.96, "text": " and then go through every document and dump it into that big tensor,", "tokens": [293, 550, 352, 807, 633, 4166, 293, 11430, 309, 666, 300, 955, 40863, 11], "temperature": 0.0, "avg_logprob": -0.09600966164235318, "compression_ratio": 1.6103896103896105, "no_speech_prob": 8.93944798008306e-06}, {"id": 1540, "seek": 627696, "start": 6276.96, "end": 6284.96, "text": " either at the start or at the end, depending on whether you said pad first.", "tokens": [2139, 412, 264, 722, 420, 412, 264, 917, 11, 5413, 322, 1968, 291, 848, 6887, 700, 13], "temperature": 0.0, "avg_logprob": -0.06754428286885106, "compression_ratio": 1.63, "no_speech_prob": 2.3320401396631496e-06}, {"id": 1541, "seek": 627696, "start": 6284.96, "end": 6290.96, "text": " So now we can pass the sampler and the collate function to our data loader,", "tokens": [407, 586, 321, 393, 1320, 264, 3247, 22732, 293, 264, 1263, 473, 2445, 281, 527, 1412, 3677, 260, 11], "temperature": 0.0, "avg_logprob": -0.06754428286885106, "compression_ratio": 1.63, "no_speech_prob": 2.3320401396631496e-06}, {"id": 1542, "seek": 627696, "start": 6290.96, "end": 6300.96, "text": " and that allows us to grab some mini-batches, which as you can see contain padding at the end.", "tokens": [293, 300, 4045, 505, 281, 4444, 512, 8382, 12, 65, 852, 279, 11, 597, 382, 291, 393, 536, 5304, 39562, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.06754428286885106, "compression_ratio": 1.63, "no_speech_prob": 2.3320401396631496e-06}, {"id": 1543, "seek": 627696, "start": 6300.96, "end": 6305.96, "text": " And so here's our normal convenience functions that do all those things for us.", "tokens": [400, 370, 510, 311, 527, 2710, 19283, 6828, 300, 360, 439, 729, 721, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.06754428286885106, "compression_ratio": 1.63, "no_speech_prob": 2.3320401396631496e-06}, {"id": 1544, "seek": 630596, "start": 6305.96, "end": 6312.96, "text": " And that's that. Okay. So that's quite a bit of pre-processing.", "tokens": [400, 300, 311, 300, 13, 1033, 13, 407, 300, 311, 1596, 257, 857, 295, 659, 12, 41075, 278, 13], "temperature": 0.0, "avg_logprob": -0.09023450982981715, "compression_ratio": 1.4870466321243523, "no_speech_prob": 2.3186172256828286e-05}, {"id": 1545, "seek": 630596, "start": 6312.96, "end": 6317.96, "text": " And I guess the main tricky bit is this dealing with different lengths.", "tokens": [400, 286, 2041, 264, 2135, 12414, 857, 307, 341, 6260, 365, 819, 26329, 13], "temperature": 0.0, "avg_logprob": -0.09023450982981715, "compression_ratio": 1.4870466321243523, "no_speech_prob": 2.3186172256828286e-05}, {"id": 1546, "seek": 630596, "start": 6317.96, "end": 6322.96, "text": " And at that point, we can create our AWD LSTM.", "tokens": [400, 412, 300, 935, 11, 321, 393, 1884, 527, 25815, 35, 441, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.09023450982981715, "compression_ratio": 1.4870466321243523, "no_speech_prob": 2.3186172256828286e-05}, {"id": 1547, "seek": 630596, "start": 6322.96, "end": 6329.96, "text": " So these are just the steps we just did to create our data loader.", "tokens": [407, 613, 366, 445, 264, 4439, 321, 445, 630, 281, 1884, 527, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.09023450982981715, "compression_ratio": 1.4870466321243523, "no_speech_prob": 2.3186172256828286e-05}, {"id": 1548, "seek": 630596, "start": 6329.96, "end": 6334.96, "text": " And now we're going to create an RNN.", "tokens": [400, 586, 321, 434, 516, 281, 1884, 364, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.09023450982981715, "compression_ratio": 1.4870466321243523, "no_speech_prob": 2.3186172256828286e-05}, {"id": 1549, "seek": 633496, "start": 6334.96, "end": 6340.96, "text": " So an RNN, remember, is just a multi-layer network.", "tokens": [407, 364, 45702, 45, 11, 1604, 11, 307, 445, 257, 4825, 12, 8376, 260, 3209, 13], "temperature": 0.0, "avg_logprob": -0.08437638405041817, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.1658486982923932e-05}, {"id": 1550, "seek": 633496, "start": 6340.96, "end": 6343.96, "text": " But it's a multi-layer network that could be very, very, very many layers.", "tokens": [583, 309, 311, 257, 4825, 12, 8376, 260, 3209, 300, 727, 312, 588, 11, 588, 11, 588, 867, 7914, 13], "temperature": 0.0, "avg_logprob": -0.08437638405041817, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.1658486982923932e-05}, {"id": 1551, "seek": 633496, "start": 6343.96, "end": 6348.96, "text": " There could be, like, if it was a 2,000-word document, this is going to be 2,000 layers.", "tokens": [821, 727, 312, 11, 411, 11, 498, 309, 390, 257, 568, 11, 1360, 12, 7462, 4166, 11, 341, 307, 516, 281, 312, 568, 11, 1360, 7914, 13], "temperature": 0.0, "avg_logprob": -0.08437638405041817, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.1658486982923932e-05}, {"id": 1552, "seek": 633496, "start": 6348.96, "end": 6354.96, "text": " So to avoid us having to write 2,000 layers, we used a for loop.", "tokens": [407, 281, 5042, 505, 1419, 281, 2464, 568, 11, 1360, 7914, 11, 321, 1143, 257, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.08437638405041817, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.1658486982923932e-05}, {"id": 1553, "seek": 633496, "start": 6354.96, "end": 6360.96, "text": " And between every pair of hidden layers, we use the same weight matrix. That's why they're the same color.", "tokens": [400, 1296, 633, 6119, 295, 7633, 7914, 11, 321, 764, 264, 912, 3364, 8141, 13, 663, 311, 983, 436, 434, 264, 912, 2017, 13], "temperature": 0.0, "avg_logprob": -0.08437638405041817, "compression_ratio": 1.682608695652174, "no_speech_prob": 1.1658486982923932e-05}, {"id": 1554, "seek": 636096, "start": 6360.96, "end": 6364.96, "text": " That's why we can use a for loop.", "tokens": [663, 311, 983, 321, 393, 764, 257, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.07743223129756867, "compression_ratio": 1.6985294117647058, "no_speech_prob": 5.594253252638737e-06}, {"id": 1555, "seek": 636096, "start": 6364.96, "end": 6371.96, "text": " The problem is, as we've seen, trying to handle 2,000 layers of neural net, we get vanishing gradients or exploding gradients.", "tokens": [440, 1154, 307, 11, 382, 321, 600, 1612, 11, 1382, 281, 4813, 568, 11, 1360, 7914, 295, 18161, 2533, 11, 321, 483, 3161, 3807, 2771, 2448, 420, 35175, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.07743223129756867, "compression_ratio": 1.6985294117647058, "no_speech_prob": 5.594253252638737e-06}, {"id": 1556, "seek": 636096, "start": 6371.96, "end": 6374.96, "text": " It's really, really difficult to get it to work.", "tokens": [467, 311, 534, 11, 534, 2252, 281, 483, 309, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.07743223129756867, "compression_ratio": 1.6985294117647058, "no_speech_prob": 5.594253252638737e-06}, {"id": 1557, "seek": 636096, "start": 6374.96, "end": 6381.96, "text": " So what are we going to do? Because it's even worse than that, because often we have layers going into RNNs going into other RNNs.", "tokens": [407, 437, 366, 321, 516, 281, 360, 30, 1436, 309, 311, 754, 5324, 813, 300, 11, 570, 2049, 321, 362, 7914, 516, 666, 45702, 45, 82, 516, 666, 661, 45702, 45, 82, 13], "temperature": 0.0, "avg_logprob": -0.07743223129756867, "compression_ratio": 1.6985294117647058, "no_speech_prob": 5.594253252638737e-06}, {"id": 1558, "seek": 636096, "start": 6381.96, "end": 6389.96, "text": " So we actually have stacked RNNs, which when we unstack them, it's going to be even more thousands of layers effectively.", "tokens": [407, 321, 767, 362, 28867, 45702, 45, 82, 11, 597, 562, 321, 18799, 501, 552, 11, 309, 311, 516, 281, 312, 754, 544, 5383, 295, 7914, 8659, 13], "temperature": 0.0, "avg_logprob": -0.07743223129756867, "compression_ratio": 1.6985294117647058, "no_speech_prob": 5.594253252638737e-06}, {"id": 1559, "seek": 638996, "start": 6389.96, "end": 6392.96, "text": " So the trick is, we create something called an LSTM cell.", "tokens": [407, 264, 4282, 307, 11, 321, 1884, 746, 1219, 364, 441, 6840, 44, 2815, 13], "temperature": 0.0, "avg_logprob": -0.06327617168426514, "compression_ratio": 1.7525252525252526, "no_speech_prob": 2.014469646383077e-05}, {"id": 1560, "seek": 638996, "start": 6392.96, "end": 6401.96, "text": " Rather than just doing a matrix multiply as our layer, we instead do this thing called an LSTM cell as our layer.", "tokens": [16571, 813, 445, 884, 257, 8141, 12972, 382, 527, 4583, 11, 321, 2602, 360, 341, 551, 1219, 364, 441, 6840, 44, 2815, 382, 527, 4583, 13], "temperature": 0.0, "avg_logprob": -0.06327617168426514, "compression_ratio": 1.7525252525252526, "no_speech_prob": 2.014469646383077e-05}, {"id": 1561, "seek": 638996, "start": 6401.96, "end": 6403.96, "text": " This is it here.", "tokens": [639, 307, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.06327617168426514, "compression_ratio": 1.7525252525252526, "no_speech_prob": 2.014469646383077e-05}, {"id": 1562, "seek": 638996, "start": 6403.96, "end": 6408.96, "text": " So this is a sigmoid function, and this is a tanch function.", "tokens": [407, 341, 307, 257, 4556, 3280, 327, 2445, 11, 293, 341, 307, 257, 256, 4778, 2445, 13], "temperature": 0.0, "avg_logprob": -0.06327617168426514, "compression_ratio": 1.7525252525252526, "no_speech_prob": 2.014469646383077e-05}, {"id": 1563, "seek": 638996, "start": 6408.96, "end": 6413.96, "text": " So the sigmoid function, remember, goes from 0 to 1, and kind of nice and smooth between the two.", "tokens": [407, 264, 4556, 3280, 327, 2445, 11, 1604, 11, 1709, 490, 1958, 281, 502, 11, 293, 733, 295, 1481, 293, 5508, 1296, 264, 732, 13], "temperature": 0.0, "avg_logprob": -0.06327617168426514, "compression_ratio": 1.7525252525252526, "no_speech_prob": 2.014469646383077e-05}, {"id": 1564, "seek": 641396, "start": 6413.96, "end": 6419.96, "text": " And the tanch function is identical to a sigmoid, except it goes from minus 1 to 1, rather than 0 to 1.", "tokens": [400, 264, 256, 4778, 2445, 307, 14800, 281, 257, 4556, 3280, 327, 11, 3993, 309, 1709, 490, 3175, 502, 281, 502, 11, 2831, 813, 1958, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.08200910268736279, "compression_ratio": 1.748917748917749, "no_speech_prob": 5.954945663688704e-06}, {"id": 1565, "seek": 641396, "start": 6419.96, "end": 6423.96, "text": " So sigmoid is 0 to 1, tanch is minus 1 to 1.", "tokens": [407, 4556, 3280, 327, 307, 1958, 281, 502, 11, 256, 4778, 307, 3175, 502, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.08200910268736279, "compression_ratio": 1.748917748917749, "no_speech_prob": 5.954945663688704e-06}, {"id": 1566, "seek": 641396, "start": 6423.96, "end": 6430.96, "text": " So here's what we're going to do. We're going to take our input, and we're going to have some hidden state, as we've always had in our RNNs.", "tokens": [407, 510, 311, 437, 321, 434, 516, 281, 360, 13, 492, 434, 516, 281, 747, 527, 4846, 11, 293, 321, 434, 516, 281, 362, 512, 7633, 1785, 11, 382, 321, 600, 1009, 632, 294, 527, 45702, 45, 82, 13], "temperature": 0.0, "avg_logprob": -0.08200910268736279, "compression_ratio": 1.748917748917749, "no_speech_prob": 5.954945663688704e-06}, {"id": 1567, "seek": 641396, "start": 6430.96, "end": 6432.96, "text": " This is just our usual hidden state.", "tokens": [639, 307, 445, 527, 7713, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.08200910268736279, "compression_ratio": 1.748917748917749, "no_speech_prob": 5.954945663688704e-06}, {"id": 1568, "seek": 641396, "start": 6432.96, "end": 6439.96, "text": " And we're going to multiply our input by some weight matrix in the usual way.", "tokens": [400, 321, 434, 516, 281, 12972, 527, 4846, 538, 512, 3364, 8141, 294, 264, 7713, 636, 13], "temperature": 0.0, "avg_logprob": -0.08200910268736279, "compression_ratio": 1.748917748917749, "no_speech_prob": 5.954945663688704e-06}, {"id": 1569, "seek": 643996, "start": 6439.96, "end": 6446.96, "text": " Then we're going to multiply our hidden state by some weight matrix in the usual way, and then we add the two together in the way we've done before for RNNs.", "tokens": [1396, 321, 434, 516, 281, 12972, 527, 7633, 1785, 538, 512, 3364, 8141, 294, 264, 7713, 636, 11, 293, 550, 321, 909, 264, 732, 1214, 294, 264, 636, 321, 600, 1096, 949, 337, 45702, 45, 82, 13], "temperature": 0.0, "avg_logprob": -0.060847235991891506, "compression_ratio": 2.0225225225225225, "no_speech_prob": 1.9830851670121774e-05}, {"id": 1570, "seek": 643996, "start": 6446.96, "end": 6448.96, "text": " And then we're going to do something interesting.", "tokens": [400, 550, 321, 434, 516, 281, 360, 746, 1880, 13], "temperature": 0.0, "avg_logprob": -0.060847235991891506, "compression_ratio": 2.0225225225225225, "no_speech_prob": 1.9830851670121774e-05}, {"id": 1571, "seek": 643996, "start": 6448.96, "end": 6453.96, "text": " We're going to split the result into four equal-sized tensors.", "tokens": [492, 434, 516, 281, 7472, 264, 1874, 666, 1451, 2681, 12, 20614, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.060847235991891506, "compression_ratio": 2.0225225225225225, "no_speech_prob": 1.9830851670121774e-05}, {"id": 1572, "seek": 643996, "start": 6453.96, "end": 6458.96, "text": " So the first one quarter of the activations will go through this path.", "tokens": [407, 264, 700, 472, 6555, 295, 264, 2430, 763, 486, 352, 807, 341, 3100, 13], "temperature": 0.0, "avg_logprob": -0.060847235991891506, "compression_ratio": 2.0225225225225225, "no_speech_prob": 1.9830851670121774e-05}, {"id": 1573, "seek": 643996, "start": 6458.96, "end": 6462.96, "text": " The next will go through this path. The next will go through this path. The next will go through this path.", "tokens": [440, 958, 486, 352, 807, 341, 3100, 13, 440, 958, 486, 352, 807, 341, 3100, 13, 440, 958, 486, 352, 807, 341, 3100, 13], "temperature": 0.0, "avg_logprob": -0.060847235991891506, "compression_ratio": 2.0225225225225225, "no_speech_prob": 1.9830851670121774e-05}, {"id": 1574, "seek": 646296, "start": 6462.96, "end": 6470.96, "text": " So what this means is we kind of have like four little neural nets, effectively.", "tokens": [407, 437, 341, 1355, 307, 321, 733, 295, 362, 411, 1451, 707, 18161, 36170, 11, 8659, 13], "temperature": 0.0, "avg_logprob": -0.11762401792738172, "compression_ratio": 1.5643564356435644, "no_speech_prob": 2.5612537228880683e-06}, {"id": 1575, "seek": 646296, "start": 6470.96, "end": 6477.96, "text": " And so this path goes through a sigmoid, and it hits this thing called the cell.", "tokens": [400, 370, 341, 3100, 1709, 807, 257, 4556, 3280, 327, 11, 293, 309, 8664, 341, 551, 1219, 264, 2815, 13], "temperature": 0.0, "avg_logprob": -0.11762401792738172, "compression_ratio": 1.5643564356435644, "no_speech_prob": 2.5612537228880683e-06}, {"id": 1576, "seek": 646296, "start": 6477.96, "end": 6484.96, "text": " Now this is the new thing. So the cell, just like hidden state, is just a rank 1 tensor, or for a mini-batch, a rank 2 tensor.", "tokens": [823, 341, 307, 264, 777, 551, 13, 407, 264, 2815, 11, 445, 411, 7633, 1785, 11, 307, 445, 257, 6181, 502, 40863, 11, 420, 337, 257, 8382, 12, 65, 852, 11, 257, 6181, 568, 40863, 13], "temperature": 0.0, "avg_logprob": -0.11762401792738172, "compression_ratio": 1.5643564356435644, "no_speech_prob": 2.5612537228880683e-06}, {"id": 1577, "seek": 646296, "start": 6484.96, "end": 6488.96, "text": " It's just some activations.", "tokens": [467, 311, 445, 512, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.11762401792738172, "compression_ratio": 1.5643564356435644, "no_speech_prob": 2.5612537228880683e-06}, {"id": 1578, "seek": 648896, "start": 6488.96, "end": 6493.96, "text": " And what happens is we multiply it by the output of this sigmoid.", "tokens": [400, 437, 2314, 307, 321, 12972, 309, 538, 264, 5598, 295, 341, 4556, 3280, 327, 13], "temperature": 0.0, "avg_logprob": -0.06280027237613645, "compression_ratio": 1.8051948051948052, "no_speech_prob": 1.0952906450256705e-05}, {"id": 1579, "seek": 648896, "start": 6493.96, "end": 6496.96, "text": " So the sigmoid can go between 0 and 1.", "tokens": [407, 264, 4556, 3280, 327, 393, 352, 1296, 1958, 293, 502, 13], "temperature": 0.0, "avg_logprob": -0.06280027237613645, "compression_ratio": 1.8051948051948052, "no_speech_prob": 1.0952906450256705e-05}, {"id": 1580, "seek": 648896, "start": 6496.96, "end": 6504.96, "text": " So this gate has the ability to basically zero out bits of the cell state.", "tokens": [407, 341, 8539, 575, 264, 3485, 281, 1936, 4018, 484, 9239, 295, 264, 2815, 1785, 13], "temperature": 0.0, "avg_logprob": -0.06280027237613645, "compression_ratio": 1.8051948051948052, "no_speech_prob": 1.0952906450256705e-05}, {"id": 1581, "seek": 648896, "start": 6504.96, "end": 6508.96, "text": " So we have the ability to basically take this state and say, like, delete some of it.", "tokens": [407, 321, 362, 264, 3485, 281, 1936, 747, 341, 1785, 293, 584, 11, 411, 11, 12097, 512, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.06280027237613645, "compression_ratio": 1.8051948051948052, "no_speech_prob": 1.0952906450256705e-05}, {"id": 1582, "seek": 648896, "start": 6508.96, "end": 6516.96, "text": " So we could look at some of these words or whatever in this LSTM and say, based on looking at that, we think we should zero out some of our cell state.", "tokens": [407, 321, 727, 574, 412, 512, 295, 613, 2283, 420, 2035, 294, 341, 441, 6840, 44, 293, 584, 11, 2361, 322, 1237, 412, 300, 11, 321, 519, 321, 820, 4018, 484, 512, 295, 527, 2815, 1785, 13], "temperature": 0.0, "avg_logprob": -0.06280027237613645, "compression_ratio": 1.8051948051948052, "no_speech_prob": 1.0952906450256705e-05}, {"id": 1583, "seek": 651696, "start": 6516.96, "end": 6521.96, "text": " And so now the cell state has been selectively forgotten. So that's the forget gate.", "tokens": [400, 370, 586, 264, 2815, 1785, 575, 668, 3048, 3413, 11832, 13, 407, 300, 311, 264, 2870, 8539, 13], "temperature": 0.0, "avg_logprob": -0.09465157358269942, "compression_ratio": 1.6367924528301887, "no_speech_prob": 5.507521109393565e-06}, {"id": 1584, "seek": 651696, "start": 6521.96, "end": 6530.96, "text": " We then add it to the second chunk, the second little mini-neuronet, which goes through sigmoid.", "tokens": [492, 550, 909, 309, 281, 264, 1150, 16635, 11, 264, 1150, 707, 8382, 12, 716, 374, 266, 302, 11, 597, 1709, 807, 4556, 3280, 327, 13], "temperature": 0.0, "avg_logprob": -0.09465157358269942, "compression_ratio": 1.6367924528301887, "no_speech_prob": 5.507521109393565e-06}, {"id": 1585, "seek": 651696, "start": 6530.96, "end": 6538.96, "text": " So this is just our input. And we multiply it by the third one, which goes through a tensh.", "tokens": [407, 341, 307, 445, 527, 4846, 13, 400, 321, 12972, 309, 538, 264, 2636, 472, 11, 597, 1709, 807, 257, 10688, 71, 13], "temperature": 0.0, "avg_logprob": -0.09465157358269942, "compression_ratio": 1.6367924528301887, "no_speech_prob": 5.507521109393565e-06}, {"id": 1586, "seek": 651696, "start": 6538.96, "end": 6543.96, "text": " So this basically allows us to say, which bits of input do we care about?", "tokens": [407, 341, 1936, 4045, 505, 281, 584, 11, 597, 9239, 295, 4846, 360, 321, 1127, 466, 30], "temperature": 0.0, "avg_logprob": -0.09465157358269942, "compression_ratio": 1.6367924528301887, "no_speech_prob": 5.507521109393565e-06}, {"id": 1587, "seek": 654396, "start": 6543.96, "end": 6548.96, "text": " And then this gives us the numbers from minus 1 to 1, multiply them together.", "tokens": [400, 550, 341, 2709, 505, 264, 3547, 490, 3175, 502, 281, 502, 11, 12972, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.0688363576339463, "compression_ratio": 1.819672131147541, "no_speech_prob": 2.4682385628693737e-05}, {"id": 1588, "seek": 654396, "start": 6548.96, "end": 6551.96, "text": " And this adds. So this is how do we update our cell state?", "tokens": [400, 341, 10860, 13, 407, 341, 307, 577, 360, 321, 5623, 527, 2815, 1785, 30], "temperature": 0.0, "avg_logprob": -0.0688363576339463, "compression_ratio": 1.819672131147541, "no_speech_prob": 2.4682385628693737e-05}, {"id": 1589, "seek": 654396, "start": 6551.96, "end": 6554.96, "text": " So we add on some new state.", "tokens": [407, 321, 909, 322, 512, 777, 1785, 13], "temperature": 0.0, "avg_logprob": -0.0688363576339463, "compression_ratio": 1.819672131147541, "no_speech_prob": 2.4682385628693737e-05}, {"id": 1590, "seek": 654396, "start": 6554.96, "end": 6558.96, "text": " And so now we take that cell state and we put it through another.", "tokens": [400, 370, 586, 321, 747, 300, 2815, 1785, 293, 321, 829, 309, 807, 1071, 13], "temperature": 0.0, "avg_logprob": -0.0688363576339463, "compression_ratio": 1.819672131147541, "no_speech_prob": 2.4682385628693737e-05}, {"id": 1591, "seek": 654396, "start": 6558.96, "end": 6562.96, "text": " Well, one thing that happens is it goes through to the next time step.", "tokens": [1042, 11, 472, 551, 300, 2314, 307, 309, 1709, 807, 281, 264, 958, 565, 1823, 13], "temperature": 0.0, "avg_logprob": -0.0688363576339463, "compression_ratio": 1.819672131147541, "no_speech_prob": 2.4682385628693737e-05}, {"id": 1592, "seek": 654396, "start": 6562.96, "end": 6572.96, "text": " And the other thing that happens is it goes through one more tensh to get multiplied by the fourth little mini-neuronet, which is the output.", "tokens": [400, 264, 661, 551, 300, 2314, 307, 309, 1709, 807, 472, 544, 10688, 71, 281, 483, 17207, 538, 264, 6409, 707, 8382, 12, 716, 374, 266, 302, 11, 597, 307, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.0688363576339463, "compression_ratio": 1.819672131147541, "no_speech_prob": 2.4682385628693737e-05}, {"id": 1593, "seek": 657296, "start": 6572.96, "end": 6577.96, "text": " So this is the actual, this actually creates the output hidden state.", "tokens": [407, 341, 307, 264, 3539, 11, 341, 767, 7829, 264, 5598, 7633, 1785, 13], "temperature": 0.0, "avg_logprob": -0.11824418097427211, "compression_ratio": 1.6954545454545455, "no_speech_prob": 6.438981017709011e-06}, {"id": 1594, "seek": 657296, "start": 6577.96, "end": 6583.96, "text": " So it looks like there's a lot going on, but actually it's just this.", "tokens": [407, 309, 1542, 411, 456, 311, 257, 688, 516, 322, 11, 457, 767, 309, 311, 445, 341, 13], "temperature": 0.0, "avg_logprob": -0.11824418097427211, "compression_ratio": 1.6954545454545455, "no_speech_prob": 6.438981017709011e-06}, {"id": 1595, "seek": 657296, "start": 6583.96, "end": 6588.96, "text": " Right. So you've got one neural net that goes from input to hidden.", "tokens": [1779, 13, 407, 291, 600, 658, 472, 18161, 2533, 300, 1709, 490, 4846, 281, 7633, 13], "temperature": 0.0, "avg_logprob": -0.11824418097427211, "compression_ratio": 1.6954545454545455, "no_speech_prob": 6.438981017709011e-06}, {"id": 1596, "seek": 657296, "start": 6588.96, "end": 6591.96, "text": " It's a linear layer, one that goes from hidden to hidden.", "tokens": [467, 311, 257, 8213, 4583, 11, 472, 300, 1709, 490, 7633, 281, 7633, 13], "temperature": 0.0, "avg_logprob": -0.11824418097427211, "compression_ratio": 1.6954545454545455, "no_speech_prob": 6.438981017709011e-06}, {"id": 1597, "seek": 657296, "start": 6591.96, "end": 6597.96, "text": " Each one is going to be four times the number of hidden, because after we compute it and add them together,", "tokens": [6947, 472, 307, 516, 281, 312, 1451, 1413, 264, 1230, 295, 7633, 11, 570, 934, 321, 14722, 309, 293, 909, 552, 1214, 11], "temperature": 0.0, "avg_logprob": -0.11824418097427211, "compression_ratio": 1.6954545454545455, "no_speech_prob": 6.438981017709011e-06}, {"id": 1598, "seek": 659796, "start": 6597.96, "end": 6602.96, "text": " the chunk splits it up into four equal size groups.", "tokens": [264, 16635, 37741, 309, 493, 666, 1451, 2681, 2744, 3935, 13], "temperature": 0.0, "avg_logprob": -0.07714777850033191, "compression_ratio": 1.539906103286385, "no_speech_prob": 3.3405196973035345e-06}, {"id": 1599, "seek": 659796, "start": 6602.96, "end": 6605.96, "text": " Three of them go through a sigmoid.", "tokens": [6244, 295, 552, 352, 807, 257, 4556, 3280, 327, 13], "temperature": 0.0, "avg_logprob": -0.07714777850033191, "compression_ratio": 1.539906103286385, "no_speech_prob": 3.3405196973035345e-06}, {"id": 1600, "seek": 659796, "start": 6605.96, "end": 6607.96, "text": " One of them goes through a tensh.", "tokens": [1485, 295, 552, 1709, 807, 257, 10688, 71, 13], "temperature": 0.0, "avg_logprob": -0.07714777850033191, "compression_ratio": 1.539906103286385, "no_speech_prob": 3.3405196973035345e-06}, {"id": 1601, "seek": 659796, "start": 6607.96, "end": 6611.96, "text": " And then this is just the multiply and add that you saw.", "tokens": [400, 550, 341, 307, 445, 264, 12972, 293, 909, 300, 291, 1866, 13], "temperature": 0.0, "avg_logprob": -0.07714777850033191, "compression_ratio": 1.539906103286385, "no_speech_prob": 3.3405196973035345e-06}, {"id": 1602, "seek": 659796, "start": 6611.96, "end": 6614.96, "text": " So there's kind of like conceptually a lot going on in LSTM.", "tokens": [407, 456, 311, 733, 295, 411, 3410, 671, 257, 688, 516, 322, 294, 441, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.07714777850033191, "compression_ratio": 1.539906103286385, "no_speech_prob": 3.3405196973035345e-06}, {"id": 1603, "seek": 659796, "start": 6614.96, "end": 6620.96, "text": " And it's certainly worth doing some more reading about why this particular architecture.", "tokens": [400, 309, 311, 3297, 3163, 884, 512, 544, 3760, 466, 983, 341, 1729, 9482, 13], "temperature": 0.0, "avg_logprob": -0.07714777850033191, "compression_ratio": 1.539906103286385, "no_speech_prob": 3.3405196973035345e-06}, {"id": 1604, "seek": 662096, "start": 6620.96, "end": 6630.96, "text": " But one thing I will say is there's lots of other ways you can set up a layer which has the ability to selectively update and selectively forget things.", "tokens": [583, 472, 551, 286, 486, 584, 307, 456, 311, 3195, 295, 661, 2098, 291, 393, 992, 493, 257, 4583, 597, 575, 264, 3485, 281, 3048, 3413, 5623, 293, 3048, 3413, 2870, 721, 13], "temperature": 0.0, "avg_logprob": -0.06120852668686669, "compression_ratio": 1.7148760330578512, "no_speech_prob": 7.296186595340259e-06}, {"id": 1605, "seek": 662096, "start": 6630.96, "end": 6633.96, "text": " For example, there's something called a GRU, which has one less gate.", "tokens": [1171, 1365, 11, 456, 311, 746, 1219, 257, 10903, 52, 11, 597, 575, 472, 1570, 8539, 13], "temperature": 0.0, "avg_logprob": -0.06120852668686669, "compression_ratio": 1.7148760330578512, "no_speech_prob": 7.296186595340259e-06}, {"id": 1606, "seek": 662096, "start": 6633.96, "end": 6640.96, "text": " The key thing seems to be giving it some way to make a decision to forget things.", "tokens": [440, 2141, 551, 2544, 281, 312, 2902, 309, 512, 636, 281, 652, 257, 3537, 281, 2870, 721, 13], "temperature": 0.0, "avg_logprob": -0.06120852668686669, "compression_ratio": 1.7148760330578512, "no_speech_prob": 7.296186595340259e-06}, {"id": 1607, "seek": 662096, "start": 6640.96, "end": 6647.96, "text": " Because if you do that, then it has the ability to not push state through all thousand time steps or whatever.", "tokens": [1436, 498, 291, 360, 300, 11, 550, 309, 575, 264, 3485, 281, 406, 2944, 1785, 807, 439, 4714, 565, 4439, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.06120852668686669, "compression_ratio": 1.7148760330578512, "no_speech_prob": 7.296186595340259e-06}, {"id": 1608, "seek": 664796, "start": 6647.96, "end": 6650.96, "text": " So that's our LSTM cell.", "tokens": [407, 300, 311, 527, 441, 6840, 44, 2815, 13], "temperature": 0.0, "avg_logprob": -0.11089628621151573, "compression_ratio": 1.786046511627907, "no_speech_prob": 1.3845110515831038e-05}, {"id": 1609, "seek": 664796, "start": 6650.96, "end": 6658.96, "text": " And so an LSTM layer, an LSTM, assuming we only have one layer, is just that for loop that we've seen before.", "tokens": [400, 370, 364, 441, 6840, 44, 4583, 11, 364, 441, 6840, 44, 11, 11926, 321, 787, 362, 472, 4583, 11, 307, 445, 300, 337, 6367, 300, 321, 600, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.11089628621151573, "compression_ratio": 1.786046511627907, "no_speech_prob": 1.3845110515831038e-05}, {"id": 1610, "seek": 664796, "start": 6658.96, "end": 6661.96, "text": " And we're just going to call whatever cell we asked for.", "tokens": [400, 321, 434, 445, 516, 281, 818, 2035, 2815, 321, 2351, 337, 13], "temperature": 0.0, "avg_logprob": -0.11089628621151573, "compression_ratio": 1.786046511627907, "no_speech_prob": 1.3845110515831038e-05}, {"id": 1611, "seek": 664796, "start": 6661.96, "end": 6664.96, "text": " So we're going to ask for an LSTM cell.", "tokens": [407, 321, 434, 516, 281, 1029, 337, 364, 441, 6840, 44, 2815, 13], "temperature": 0.0, "avg_logprob": -0.11089628621151573, "compression_ratio": 1.786046511627907, "no_speech_prob": 1.3845110515831038e-05}, {"id": 1612, "seek": 664796, "start": 6664.96, "end": 6671.96, "text": " And you just loop through and see how the state, we could take the state and we update the state.", "tokens": [400, 291, 445, 6367, 807, 293, 536, 577, 264, 1785, 11, 321, 727, 747, 264, 1785, 293, 321, 5623, 264, 1785, 13], "temperature": 0.0, "avg_logprob": -0.11089628621151573, "compression_ratio": 1.786046511627907, "no_speech_prob": 1.3845110515831038e-05}, {"id": 1613, "seek": 664796, "start": 6671.96, "end": 6675.96, "text": " So this you can see this is the classic deep learning.", "tokens": [407, 341, 291, 393, 536, 341, 307, 264, 7230, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.11089628621151573, "compression_ratio": 1.786046511627907, "no_speech_prob": 1.3845110515831038e-05}, {"id": 1614, "seek": 667596, "start": 6675.96, "end": 6677.96, "text": " It's like an NN.sequential, right?", "tokens": [467, 311, 411, 364, 426, 45, 13, 11834, 2549, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.09694686143294624, "compression_ratio": 1.3452380952380953, "no_speech_prob": 6.240670245460933e-06}, {"id": 1615, "seek": 667596, "start": 6677.96, "end": 6680.96, "text": " It's looping through a bunch of functions that are updating itself.", "tokens": [467, 311, 6367, 278, 807, 257, 3840, 295, 6828, 300, 366, 25113, 2564, 13], "temperature": 0.0, "avg_logprob": -0.09694686143294624, "compression_ratio": 1.3452380952380953, "no_speech_prob": 6.240670245460933e-06}, {"id": 1616, "seek": 667596, "start": 6680.96, "end": 6685.96, "text": " That's what makes it a deep learning network.", "tokens": [663, 311, 437, 1669, 309, 257, 2452, 2539, 3209, 13], "temperature": 0.0, "avg_logprob": -0.09694686143294624, "compression_ratio": 1.3452380952380953, "no_speech_prob": 6.240670245460933e-06}, {"id": 1617, "seek": 667596, "start": 6685.96, "end": 6689.96, "text": " So that's an LSTM.", "tokens": [407, 300, 311, 364, 441, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.09694686143294624, "compression_ratio": 1.3452380952380953, "no_speech_prob": 6.240670245460933e-06}, {"id": 1618, "seek": 667596, "start": 6689.96, "end": 6697.96, "text": " So that takes 105 milliseconds for a small net on the CPU.", "tokens": [407, 300, 2516, 33705, 34184, 337, 257, 1359, 2533, 322, 264, 13199, 13], "temperature": 0.0, "avg_logprob": -0.09694686143294624, "compression_ratio": 1.3452380952380953, "no_speech_prob": 6.240670245460933e-06}, {"id": 1619, "seek": 669796, "start": 6697.96, "end": 6705.96, "text": " You could pop it onto CUDA, then it's 24 milliseconds on GPU.", "tokens": [509, 727, 1665, 309, 3911, 29777, 7509, 11, 550, 309, 311, 4022, 34184, 322, 18407, 13], "temperature": 0.0, "avg_logprob": -0.1127500276307802, "compression_ratio": 1.381720430107527, "no_speech_prob": 5.0935532271978445e-06}, {"id": 1620, "seek": 669796, "start": 6705.96, "end": 6714.96, "text": " It's not that much faster because this loop, every time step, it's having to push off another kernel launch off to the GPU.", "tokens": [467, 311, 406, 300, 709, 4663, 570, 341, 6367, 11, 633, 565, 1823, 11, 309, 311, 1419, 281, 2944, 766, 1071, 28256, 4025, 766, 281, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.1127500276307802, "compression_ratio": 1.381720430107527, "no_speech_prob": 5.0935532271978445e-06}, {"id": 1621, "seek": 669796, "start": 6714.96, "end": 6718.96, "text": " And that's just slow, right?", "tokens": [400, 300, 311, 445, 2964, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1127500276307802, "compression_ratio": 1.381720430107527, "no_speech_prob": 5.0935532271978445e-06}, {"id": 1622, "seek": 669796, "start": 6718.96, "end": 6722.96, "text": " So that's why we use the built-in version.", "tokens": [407, 300, 311, 983, 321, 764, 264, 3094, 12, 259, 3037, 13], "temperature": 0.0, "avg_logprob": -0.1127500276307802, "compression_ratio": 1.381720430107527, "no_speech_prob": 5.0935532271978445e-06}, {"id": 1623, "seek": 672296, "start": 6722.96, "end": 6730.96, "text": " And the built-in version behind the scenes calls a library from Nvidia called cuDNN, which has created a C++ version of this.", "tokens": [400, 264, 3094, 12, 259, 3037, 2261, 264, 8026, 5498, 257, 6405, 490, 46284, 1219, 2702, 35, 45, 45, 11, 597, 575, 2942, 257, 383, 25472, 3037, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.0986613620411266, "compression_ratio": 1.5666666666666667, "no_speech_prob": 6.540256890730234e-06}, {"id": 1624, "seek": 672296, "start": 6730.96, "end": 6733.96, "text": " That it's about the same on the CPU, right?", "tokens": [663, 309, 311, 466, 264, 912, 322, 264, 13199, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.0986613620411266, "compression_ratio": 1.5666666666666667, "no_speech_prob": 6.540256890730234e-06}, {"id": 1625, "seek": 672296, "start": 6733.96, "end": 6735.96, "text": " Not surprisingly, it's really not doing anything different.", "tokens": [1726, 17600, 11, 309, 311, 534, 406, 884, 1340, 819, 13], "temperature": 0.0, "avg_logprob": -0.0986613620411266, "compression_ratio": 1.5666666666666667, "no_speech_prob": 6.540256890730234e-06}, {"id": 1626, "seek": 672296, "start": 6735.96, "end": 6741.96, "text": " But on the GPU, goes from 24 milliseconds to 8 milliseconds.", "tokens": [583, 322, 264, 18407, 11, 1709, 490, 4022, 34184, 281, 1649, 34184, 13], "temperature": 0.0, "avg_logprob": -0.0986613620411266, "compression_ratio": 1.5666666666666667, "no_speech_prob": 6.540256890730234e-06}, {"id": 1627, "seek": 672296, "start": 6741.96, "end": 6743.96, "text": " So it's dramatically faster.", "tokens": [407, 309, 311, 17548, 4663, 13], "temperature": 0.0, "avg_logprob": -0.0986613620411266, "compression_ratio": 1.5666666666666667, "no_speech_prob": 6.540256890730234e-06}, {"id": 1628, "seek": 672296, "start": 6743.96, "end": 6750.96, "text": " The good news is we can create a faster version by taking advantage of something in PyTorch called JIT.", "tokens": [440, 665, 2583, 307, 321, 393, 1884, 257, 4663, 3037, 538, 1940, 5002, 295, 746, 294, 9953, 51, 284, 339, 1219, 508, 3927, 13], "temperature": 0.0, "avg_logprob": -0.0986613620411266, "compression_ratio": 1.5666666666666667, "no_speech_prob": 6.540256890730234e-06}, {"id": 1629, "seek": 675096, "start": 6750.96, "end": 6760.96, "text": " And what JIT does is it reads our Python and it converts it into C++ that does the same thing, CUDA C++ that does the same thing.", "tokens": [400, 437, 508, 3927, 775, 307, 309, 15700, 527, 15329, 293, 309, 38874, 309, 666, 383, 25472, 300, 775, 264, 912, 551, 11, 29777, 7509, 383, 25472, 300, 775, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.0857590103149414, "compression_ratio": 1.6401869158878504, "no_speech_prob": 4.784948941960465e-06}, {"id": 1630, "seek": 675096, "start": 6760.96, "end": 6762.96, "text": " It compiles it the first time you use it.", "tokens": [467, 715, 4680, 309, 264, 700, 565, 291, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.0857590103149414, "compression_ratio": 1.6401869158878504, "no_speech_prob": 4.784948941960465e-06}, {"id": 1631, "seek": 675096, "start": 6762.96, "end": 6765.96, "text": " And then it uses that compiled code.", "tokens": [400, 550, 309, 4960, 300, 36548, 3089, 13], "temperature": 0.0, "avg_logprob": -0.0857590103149414, "compression_ratio": 1.6401869158878504, "no_speech_prob": 4.784948941960465e-06}, {"id": 1632, "seek": 675096, "start": 6765.96, "end": 6770.96, "text": " And so that way it can create an on GPU loop.", "tokens": [400, 370, 300, 636, 309, 393, 1884, 364, 322, 18407, 6367, 13], "temperature": 0.0, "avg_logprob": -0.0857590103149414, "compression_ratio": 1.6401869158878504, "no_speech_prob": 4.784948941960465e-06}, {"id": 1633, "seek": 675096, "start": 6770.96, "end": 6777.96, "text": " And so the result of that is, again, pretty similar on the CPU, but on the GPU, 12 milliseconds.", "tokens": [400, 370, 264, 1874, 295, 300, 307, 11, 797, 11, 1238, 2531, 322, 264, 13199, 11, 457, 322, 264, 18407, 11, 2272, 34184, 13], "temperature": 0.0, "avg_logprob": -0.0857590103149414, "compression_ratio": 1.6401869158878504, "no_speech_prob": 4.784948941960465e-06}, {"id": 1634, "seek": 677796, "start": 6777.96, "end": 6787.96, "text": " So, you know, not as fast as the cuDNN version, but certainly a lot better than our non-JIT version.", "tokens": [407, 11, 291, 458, 11, 406, 382, 2370, 382, 264, 2702, 35, 45, 45, 3037, 11, 457, 3297, 257, 688, 1101, 813, 527, 2107, 12, 41, 3927, 3037, 13], "temperature": 0.0, "avg_logprob": -0.05784934526914126, "compression_ratio": 1.4307692307692308, "no_speech_prob": 9.972044608730357e-06}, {"id": 1635, "seek": 677796, "start": 6787.96, "end": 6795.96, "text": " So this seems like some kind of magic thing that's going to save our lives and not require us to have to come to the Swift for TensorFlow lectures.", "tokens": [407, 341, 2544, 411, 512, 733, 295, 5585, 551, 300, 311, 516, 281, 3155, 527, 2909, 293, 406, 3651, 505, 281, 362, 281, 808, 281, 264, 25539, 337, 37624, 16564, 13], "temperature": 0.0, "avg_logprob": -0.05784934526914126, "compression_ratio": 1.4307692307692308, "no_speech_prob": 9.972044608730357e-06}, {"id": 1636, "seek": 677796, "start": 6795.96, "end": 6800.96, "text": " But I've got bad news for you.", "tokens": [583, 286, 600, 658, 1578, 2583, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.05784934526914126, "compression_ratio": 1.4307692307692308, "no_speech_prob": 9.972044608730357e-06}, {"id": 1637, "seek": 680096, "start": 6800.96, "end": 6808.96, "text": " Trying to get JIT working has been honestly a bit of a nightmare.", "tokens": [20180, 281, 483, 508, 3927, 1364, 575, 668, 6095, 257, 857, 295, 257, 18724, 13], "temperature": 0.0, "avg_logprob": -0.07380697306464701, "compression_ratio": 1.5176470588235293, "no_speech_prob": 5.173718363948865e-06}, {"id": 1638, "seek": 680096, "start": 6808.96, "end": 6813.96, "text": " This is the third time we've tried to introduce it in this course.", "tokens": [639, 307, 264, 2636, 565, 321, 600, 3031, 281, 5366, 309, 294, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.07380697306464701, "compression_ratio": 1.5176470588235293, "no_speech_prob": 5.173718363948865e-06}, {"id": 1639, "seek": 680096, "start": 6813.96, "end": 6824.96, "text": " And the other two times we've just not gotten it working or we've gotten worse results.", "tokens": [400, 264, 661, 732, 1413, 321, 600, 445, 406, 5768, 309, 1364, 420, 321, 600, 5768, 5324, 3542, 13], "temperature": 0.0, "avg_logprob": -0.07380697306464701, "compression_ratio": 1.5176470588235293, "no_speech_prob": 5.173718363948865e-06}, {"id": 1640, "seek": 680096, "start": 6824.96, "end": 6828.96, "text": " It doesn't work very well that often.", "tokens": [467, 1177, 380, 589, 588, 731, 300, 2049, 13], "temperature": 0.0, "avg_logprob": -0.07380697306464701, "compression_ratio": 1.5176470588235293, "no_speech_prob": 5.173718363948865e-06}, {"id": 1641, "seek": 682896, "start": 6828.96, "end": 6830.96, "text": " And it's got a lot of weird things going on.", "tokens": [400, 309, 311, 658, 257, 688, 295, 3657, 721, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.13040130917388615, "compression_ratio": 1.6121495327102804, "no_speech_prob": 1.9524075469234958e-05}, {"id": 1642, "seek": 682896, "start": 6830.96, "end": 6845.96, "text": " Like, for example, if you decide to comment out a line, right, and then run it, you'll get this error saying, like, unexpected indent.", "tokens": [1743, 11, 337, 1365, 11, 498, 291, 4536, 281, 2871, 484, 257, 1622, 11, 558, 11, 293, 550, 1190, 309, 11, 291, 603, 483, 341, 6713, 1566, 11, 411, 11, 13106, 44494, 13], "temperature": 0.0, "avg_logprob": -0.13040130917388615, "compression_ratio": 1.6121495327102804, "no_speech_prob": 1.9524075469234958e-05}, {"id": 1643, "seek": 682896, "start": 6845.96, "end": 6848.96, "text": " Like, literally, it's not Python, right?", "tokens": [1743, 11, 3736, 11, 309, 311, 406, 15329, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13040130917388615, "compression_ratio": 1.6121495327102804, "no_speech_prob": 1.9524075469234958e-05}, {"id": 1644, "seek": 682896, "start": 6848.96, "end": 6852.96, "text": " So it doesn't even know how to comment out lines.", "tokens": [407, 309, 1177, 380, 754, 458, 577, 281, 2871, 484, 3876, 13], "temperature": 0.0, "avg_logprob": -0.13040130917388615, "compression_ratio": 1.6121495327102804, "no_speech_prob": 1.9524075469234958e-05}, {"id": 1645, "seek": 682896, "start": 6852.96, "end": 6856.96, "text": " It's this kind of, like, weird thing where they try to, like, it's heroic.", "tokens": [467, 311, 341, 733, 295, 11, 411, 11, 3657, 551, 689, 436, 853, 281, 11, 411, 11, 309, 311, 32915, 13], "temperature": 0.0, "avg_logprob": -0.13040130917388615, "compression_ratio": 1.6121495327102804, "no_speech_prob": 1.9524075469234958e-05}, {"id": 1646, "seek": 685696, "start": 6856.96, "end": 6858.96, "text": " And it's amazing that it works at all.", "tokens": [400, 309, 311, 2243, 300, 309, 1985, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.10821473237239954, "compression_ratio": 1.6920152091254752, "no_speech_prob": 2.546190080465749e-05}, {"id": 1647, "seek": 685696, "start": 6858.96, "end": 6865.96, "text": " But the idea that you could try and turn Python, which is so not C++, into C++ is really pushing it what's possible.", "tokens": [583, 264, 1558, 300, 291, 727, 853, 293, 1261, 15329, 11, 597, 307, 370, 406, 383, 25472, 11, 666, 383, 25472, 307, 534, 7380, 309, 437, 311, 1944, 13], "temperature": 0.0, "avg_logprob": -0.10821473237239954, "compression_ratio": 1.6920152091254752, "no_speech_prob": 2.546190080465749e-05}, {"id": 1648, "seek": 685696, "start": 6865.96, "end": 6868.96, "text": " So it's astonishing this works at all.", "tokens": [407, 309, 311, 35264, 341, 1985, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.10821473237239954, "compression_ratio": 1.6920152091254752, "no_speech_prob": 2.546190080465749e-05}, {"id": 1649, "seek": 685696, "start": 6868.96, "end": 6873.96, "text": " And occasionally it might be useful, but it's very, very hard to use.", "tokens": [400, 16895, 309, 1062, 312, 4420, 11, 457, 309, 311, 588, 11, 588, 1152, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.10821473237239954, "compression_ratio": 1.6920152091254752, "no_speech_prob": 2.546190080465749e-05}, {"id": 1650, "seek": 685696, "start": 6873.96, "end": 6877.96, "text": " And when something isn't as fast as you want, it's very, very hard to...", "tokens": [400, 562, 746, 1943, 380, 382, 2370, 382, 291, 528, 11, 309, 311, 588, 11, 588, 1152, 281, 485], "temperature": 0.0, "avg_logprob": -0.10821473237239954, "compression_ratio": 1.6920152091254752, "no_speech_prob": 2.546190080465749e-05}, {"id": 1651, "seek": 685696, "start": 6877.96, "end": 6878.96, "text": " You can't profile it.", "tokens": [509, 393, 380, 7964, 309, 13], "temperature": 0.0, "avg_logprob": -0.10821473237239954, "compression_ratio": 1.6920152091254752, "no_speech_prob": 2.546190080465749e-05}, {"id": 1652, "seek": 685696, "start": 6878.96, "end": 6879.96, "text": " You can't debug it.", "tokens": [509, 393, 380, 24083, 309, 13], "temperature": 0.0, "avg_logprob": -0.10821473237239954, "compression_ratio": 1.6920152091254752, "no_speech_prob": 2.546190080465749e-05}, {"id": 1653, "seek": 685696, "start": 6879.96, "end": 6882.96, "text": " Not in the normal ways.", "tokens": [1726, 294, 264, 2710, 2098, 13], "temperature": 0.0, "avg_logprob": -0.10821473237239954, "compression_ratio": 1.6920152091254752, "no_speech_prob": 2.546190080465749e-05}, {"id": 1654, "seek": 685696, "start": 6882.96, "end": 6885.96, "text": " But, you know, obviously it will improve.", "tokens": [583, 11, 291, 458, 11, 2745, 309, 486, 3470, 13], "temperature": 0.0, "avg_logprob": -0.10821473237239954, "compression_ratio": 1.6920152091254752, "no_speech_prob": 2.546190080465749e-05}, {"id": 1655, "seek": 688596, "start": 6885.96, "end": 6886.96, "text": " It's pretty early days.", "tokens": [467, 311, 1238, 2440, 1708, 13], "temperature": 0.0, "avg_logprob": -0.06416810727586933, "compression_ratio": 1.6032388663967612, "no_speech_prob": 5.33772390554077e-06}, {"id": 1656, "seek": 688596, "start": 6886.96, "end": 6887.96, "text": " It will improve.", "tokens": [467, 486, 3470, 13], "temperature": 0.0, "avg_logprob": -0.06416810727586933, "compression_ratio": 1.6032388663967612, "no_speech_prob": 5.33772390554077e-06}, {"id": 1657, "seek": 688596, "start": 6887.96, "end": 6896.96, "text": " But the idea of trying to parse Python and turn it into C++, literally they're doing, like, string interpolation behind the scenes,", "tokens": [583, 264, 1558, 295, 1382, 281, 48377, 15329, 293, 1261, 309, 666, 383, 25472, 11, 3736, 436, 434, 884, 11, 411, 11, 6798, 44902, 399, 2261, 264, 8026, 11], "temperature": 0.0, "avg_logprob": -0.06416810727586933, "compression_ratio": 1.6032388663967612, "no_speech_prob": 5.33772390554077e-06}, {"id": 1658, "seek": 688596, "start": 6896.96, "end": 6908.96, "text": " is kind of trying to reinvent all the stuff that compilers already do, converting a language that was very explicitly not designed to do this kind of thing into one that does.", "tokens": [307, 733, 295, 1382, 281, 33477, 439, 264, 1507, 300, 715, 388, 433, 1217, 360, 11, 29942, 257, 2856, 300, 390, 588, 20803, 406, 4761, 281, 360, 341, 733, 295, 551, 666, 472, 300, 775, 13], "temperature": 0.0, "avg_logprob": -0.06416810727586933, "compression_ratio": 1.6032388663967612, "no_speech_prob": 5.33772390554077e-06}, {"id": 1659, "seek": 688596, "start": 6908.96, "end": 6909.96, "text": " And I just...", "tokens": [400, 286, 445, 485], "temperature": 0.0, "avg_logprob": -0.06416810727586933, "compression_ratio": 1.6032388663967612, "no_speech_prob": 5.33772390554077e-06}, {"id": 1660, "seek": 688596, "start": 6909.96, "end": 6911.96, "text": " I don't think this is the future.", "tokens": [286, 500, 380, 519, 341, 307, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.06416810727586933, "compression_ratio": 1.6032388663967612, "no_speech_prob": 5.33772390554077e-06}, {"id": 1661, "seek": 691196, "start": 6911.96, "end": 6915.96, "text": " So I say for now, be aware that JIT exists.", "tokens": [407, 286, 584, 337, 586, 11, 312, 3650, 300, 508, 3927, 8198, 13], "temperature": 0.0, "avg_logprob": -0.09007208435623734, "compression_ratio": 1.5049019607843137, "no_speech_prob": 1.3630237845063675e-05}, {"id": 1662, "seek": 691196, "start": 6915.96, "end": 6917.96, "text": " Be very careful in the short term.", "tokens": [879, 588, 5026, 294, 264, 2099, 1433, 13], "temperature": 0.0, "avg_logprob": -0.09007208435623734, "compression_ratio": 1.5049019607843137, "no_speech_prob": 1.3630237845063675e-05}, {"id": 1663, "seek": 691196, "start": 6917.96, "end": 6920.96, "text": " I've found places where it literally gives the wrong gradients.", "tokens": [286, 600, 1352, 3190, 689, 309, 3736, 2709, 264, 2085, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.09007208435623734, "compression_ratio": 1.5049019607843137, "no_speech_prob": 1.3630237845063675e-05}, {"id": 1664, "seek": 691196, "start": 6920.96, "end": 6923.96, "text": " So it goes down a totally different auto grad path.", "tokens": [407, 309, 1709, 760, 257, 3879, 819, 8399, 2771, 3100, 13], "temperature": 0.0, "avg_logprob": -0.09007208435623734, "compression_ratio": 1.5049019607843137, "no_speech_prob": 1.3630237845063675e-05}, {"id": 1665, "seek": 691196, "start": 6923.96, "end": 6930.96, "text": " And I've had models that trained incorrectly without any warnings because it was just wrong.", "tokens": [400, 286, 600, 632, 5245, 300, 8895, 42892, 1553, 604, 30009, 570, 309, 390, 445, 2085, 13], "temperature": 0.0, "avg_logprob": -0.09007208435623734, "compression_ratio": 1.5049019607843137, "no_speech_prob": 1.3630237845063675e-05}, {"id": 1666, "seek": 691196, "start": 6930.96, "end": 6931.96, "text": " So be very careful.", "tokens": [407, 312, 588, 5026, 13], "temperature": 0.0, "avg_logprob": -0.09007208435623734, "compression_ratio": 1.5049019607843137, "no_speech_prob": 1.3630237845063675e-05}, {"id": 1667, "seek": 693196, "start": 6931.96, "end": 6951.96, "text": " But sometimes, like, for a researcher, if you want to play with different types of RNNs, this is your only option unless you write your own C++ or unless you try out Julia or Swift, I guess.", "tokens": [583, 2171, 11, 411, 11, 337, 257, 21751, 11, 498, 291, 528, 281, 862, 365, 819, 3467, 295, 45702, 45, 82, 11, 341, 307, 428, 787, 3614, 5969, 291, 2464, 428, 1065, 383, 25472, 420, 5969, 291, 853, 484, 18551, 420, 25539, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.09015497408415142, "compression_ratio": 1.3689839572192513, "no_speech_prob": 1.061389957612846e-05}, {"id": 1668, "seek": 693196, "start": 6951.96, "end": 6952.96, "text": " Is there a question?", "tokens": [1119, 456, 257, 1168, 30], "temperature": 0.0, "avg_logprob": -0.09015497408415142, "compression_ratio": 1.3689839572192513, "no_speech_prob": 1.061389957612846e-05}, {"id": 1669, "seek": 693196, "start": 6952.96, "end": 6954.96, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.09015497408415142, "compression_ratio": 1.3689839572192513, "no_speech_prob": 1.061389957612846e-05}, {"id": 1670, "seek": 693196, "start": 6954.96, "end": 6959.96, "text": " Why do we need torch.cuda.synchronize?", "tokens": [1545, 360, 321, 643, 27822, 13, 66, 11152, 13, 82, 36420, 1125, 30], "temperature": 0.0, "avg_logprob": -0.09015497408415142, "compression_ratio": 1.3689839572192513, "no_speech_prob": 1.061389957612846e-05}, {"id": 1671, "seek": 695996, "start": 6959.96, "end": 6964.96, "text": " Is it kind of a lock to synchronize CUDA threads or something?", "tokens": [1119, 309, 733, 295, 257, 4017, 281, 19331, 1125, 29777, 7509, 19314, 420, 746, 30], "temperature": 0.0, "avg_logprob": -0.07746632893880208, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.3628507986140903e-05}, {"id": 1672, "seek": 695996, "start": 6964.96, "end": 6967.96, "text": " Yeah, this is something that thanks to Tom on the forum for pointing this out.", "tokens": [865, 11, 341, 307, 746, 300, 3231, 281, 5041, 322, 264, 17542, 337, 12166, 341, 484, 13], "temperature": 0.0, "avg_logprob": -0.07746632893880208, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.3628507986140903e-05}, {"id": 1673, "seek": 695996, "start": 6967.96, "end": 6974.96, "text": " It's just when we're timing, without the synchronize, it's...", "tokens": [467, 311, 445, 562, 321, 434, 10822, 11, 1553, 264, 19331, 1125, 11, 309, 311, 485], "temperature": 0.0, "avg_logprob": -0.07746632893880208, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.3628507986140903e-05}, {"id": 1674, "seek": 695996, "start": 6974.96, "end": 6979.96, "text": " Let's find it.", "tokens": [961, 311, 915, 309, 13], "temperature": 0.0, "avg_logprob": -0.07746632893880208, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.3628507986140903e-05}, {"id": 1675, "seek": 695996, "start": 6979.96, "end": 6981.96, "text": " So I just created a little timing function here.", "tokens": [407, 286, 445, 2942, 257, 707, 10822, 2445, 510, 13], "temperature": 0.0, "avg_logprob": -0.07746632893880208, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.3628507986140903e-05}, {"id": 1676, "seek": 698196, "start": 6981.96, "end": 6989.96, "text": " Without the synchronize, the CUDA thing will just keep on running things in the background, but will return...", "tokens": [9129, 264, 19331, 1125, 11, 264, 29777, 7509, 551, 486, 445, 1066, 322, 2614, 721, 294, 264, 3678, 11, 457, 486, 2736, 485], "temperature": 0.0, "avg_logprob": -0.08699590168642195, "compression_ratio": 1.5277777777777777, "no_speech_prob": 6.539770311064785e-06}, {"id": 1677, "seek": 698196, "start": 6989.96, "end": 6992.96, "text": " It will let your CPU thread keep going.", "tokens": [467, 486, 718, 428, 13199, 7207, 1066, 516, 13], "temperature": 0.0, "avg_logprob": -0.08699590168642195, "compression_ratio": 1.5277777777777777, "no_speech_prob": 6.539770311064785e-06}, {"id": 1678, "seek": 698196, "start": 6992.96, "end": 6995.96, "text": " So it could end up looking much faster than it actually is.", "tokens": [407, 309, 727, 917, 493, 1237, 709, 4663, 813, 309, 767, 307, 13], "temperature": 0.0, "avg_logprob": -0.08699590168642195, "compression_ratio": 1.5277777777777777, "no_speech_prob": 6.539770311064785e-06}, {"id": 1679, "seek": 698196, "start": 6995.96, "end": 7002.96, "text": " So synchronize says, don't keep going in my Python world until my CUDA world is finished.", "tokens": [407, 19331, 1125, 1619, 11, 500, 380, 1066, 516, 294, 452, 15329, 1002, 1826, 452, 29777, 7509, 1002, 307, 4335, 13], "temperature": 0.0, "avg_logprob": -0.08699590168642195, "compression_ratio": 1.5277777777777777, "no_speech_prob": 6.539770311064785e-06}, {"id": 1680, "seek": 698196, "start": 7002.96, "end": 7003.96, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.08699590168642195, "compression_ratio": 1.5277777777777777, "no_speech_prob": 6.539770311064785e-06}, {"id": 1681, "seek": 698196, "start": 7003.96, "end": 7005.96, "text": " So now we need dropout.", "tokens": [407, 586, 321, 643, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.08699590168642195, "compression_ratio": 1.5277777777777777, "no_speech_prob": 6.539770311064785e-06}, {"id": 1682, "seek": 700596, "start": 7005.96, "end": 7016.96, "text": " And this is the bit that really is fantastic about AWD LSTM, is that Stephen Meridy thought about all the ways in which we can regularize a model.", "tokens": [400, 341, 307, 264, 857, 300, 534, 307, 5456, 466, 25815, 35, 441, 6840, 44, 11, 307, 300, 13391, 6124, 38836, 1194, 466, 439, 264, 2098, 294, 597, 321, 393, 3890, 1125, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15182942390441895, "compression_ratio": 1.623931623931624, "no_speech_prob": 4.6377604121516924e-06}, {"id": 1683, "seek": 700596, "start": 7016.96, "end": 7021.96, "text": " So basically dropout is just Bernoulli random noise.", "tokens": [407, 1936, 3270, 346, 307, 445, 10781, 263, 16320, 4974, 5658, 13], "temperature": 0.0, "avg_logprob": -0.15182942390441895, "compression_ratio": 1.623931623931624, "no_speech_prob": 4.6377604121516924e-06}, {"id": 1684, "seek": 700596, "start": 7021.96, "end": 7028.96, "text": " So Bernoulli random noise simply means create ones and zeros, and it's one with this probability.", "tokens": [407, 10781, 263, 16320, 4974, 5658, 2935, 1355, 1884, 2306, 293, 35193, 11, 293, 309, 311, 472, 365, 341, 8482, 13], "temperature": 0.0, "avg_logprob": -0.15182942390441895, "compression_ratio": 1.623931623931624, "no_speech_prob": 4.6377604121516924e-06}, {"id": 1685, "seek": 700596, "start": 7028.96, "end": 7029.96, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.15182942390441895, "compression_ratio": 1.623931623931624, "no_speech_prob": 4.6377604121516924e-06}, {"id": 1686, "seek": 700596, "start": 7029.96, "end": 7033.96, "text": " So create a bunch of random ones and zeros, and then divide by one minus P.", "tokens": [407, 1884, 257, 3840, 295, 4974, 2306, 293, 35193, 11, 293, 550, 9845, 538, 472, 3175, 430, 13], "temperature": 0.0, "avg_logprob": -0.15182942390441895, "compression_ratio": 1.623931623931624, "no_speech_prob": 4.6377604121516924e-06}, {"id": 1687, "seek": 703396, "start": 7033.96, "end": 7037.96, "text": " So that makes them, in this case, 0.5, it's randomly zeros and twos.", "tokens": [407, 300, 1669, 552, 11, 294, 341, 1389, 11, 1958, 13, 20, 11, 309, 311, 16979, 35193, 293, 683, 329, 13], "temperature": 0.0, "avg_logprob": -0.09392726544252376, "compression_ratio": 1.5777777777777777, "no_speech_prob": 3.7852087189094163e-06}, {"id": 1688, "seek": 703396, "start": 7037.96, "end": 7041.96, "text": " And the reason they're zeros and twos is because that way the standard deviation doesn't change.", "tokens": [400, 264, 1778, 436, 434, 35193, 293, 683, 329, 307, 570, 300, 636, 264, 3832, 25163, 1177, 380, 1319, 13], "temperature": 0.0, "avg_logprob": -0.09392726544252376, "compression_ratio": 1.5777777777777777, "no_speech_prob": 3.7852087189094163e-06}, {"id": 1689, "seek": 703396, "start": 7041.96, "end": 7047.96, "text": " So we can remove dropout for inference time, and the activations will be still scaled correctly.", "tokens": [407, 321, 393, 4159, 3270, 346, 337, 38253, 565, 11, 293, 264, 2430, 763, 486, 312, 920, 36039, 8944, 13], "temperature": 0.0, "avg_logprob": -0.09392726544252376, "compression_ratio": 1.5777777777777777, "no_speech_prob": 3.7852087189094163e-06}, {"id": 1690, "seek": 703396, "start": 7047.96, "end": 7051.96, "text": " And we talked about that a little bit in part one.", "tokens": [400, 321, 2825, 466, 300, 257, 707, 857, 294, 644, 472, 13], "temperature": 0.0, "avg_logprob": -0.09392726544252376, "compression_ratio": 1.5777777777777777, "no_speech_prob": 3.7852087189094163e-06}, {"id": 1691, "seek": 703396, "start": 7051.96, "end": 7055.96, "text": " And so now we can create our RNN dropout.", "tokens": [400, 370, 586, 321, 393, 1884, 527, 45702, 45, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.09392726544252376, "compression_ratio": 1.5777777777777777, "no_speech_prob": 3.7852087189094163e-06}, {"id": 1692, "seek": 705596, "start": 7055.96, "end": 7065.96, "text": " And one of the nifty things here is the way that Sylvia wrote this is you don't just pass in the thing to dropout, but you also pass in a size.", "tokens": [400, 472, 295, 264, 297, 37177, 721, 510, 307, 264, 636, 300, 33349, 11617, 4114, 341, 307, 291, 500, 380, 445, 1320, 294, 264, 551, 281, 3270, 346, 11, 457, 291, 611, 1320, 294, 257, 2744, 13], "temperature": 0.0, "avg_logprob": -0.10307912265553194, "compression_ratio": 1.7151162790697674, "no_speech_prob": 4.356776116765104e-06}, {"id": 1693, "seek": 705596, "start": 7065.96, "end": 7069.96, "text": " Now, normally you would just pass in the size of the thing to dropout like this.", "tokens": [823, 11, 5646, 291, 576, 445, 1320, 294, 264, 2744, 295, 264, 551, 281, 3270, 346, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.10307912265553194, "compression_ratio": 1.7151162790697674, "no_speech_prob": 4.356776116765104e-06}, {"id": 1694, "seek": 705596, "start": 7069.96, "end": 7079.96, "text": " But what he did here was he passed in for the size, size 0, 1, size 2.", "tokens": [583, 437, 415, 630, 510, 390, 415, 4678, 294, 337, 264, 2744, 11, 2744, 1958, 11, 502, 11, 2744, 568, 13], "temperature": 0.0, "avg_logprob": -0.10307912265553194, "compression_ratio": 1.7151162790697674, "no_speech_prob": 4.356776116765104e-06}, {"id": 1695, "seek": 707996, "start": 7079.96, "end": 7086.96, "text": " And so if you remember back to broadcasting, this means that this is going to create something with a unit axis in the middle.", "tokens": [400, 370, 498, 291, 1604, 646, 281, 30024, 11, 341, 1355, 300, 341, 307, 516, 281, 1884, 746, 365, 257, 4985, 10298, 294, 264, 2808, 13], "temperature": 0.0, "avg_logprob": -0.07046959950373723, "compression_ratio": 1.59, "no_speech_prob": 5.014681846660096e-06}, {"id": 1696, "seek": 707996, "start": 7086.96, "end": 7095.96, "text": " And so when we multiply that, so here's our rep matrix, when we multiply the dropout by that, our zeros get broadcast.", "tokens": [400, 370, 562, 321, 12972, 300, 11, 370, 510, 311, 527, 1085, 8141, 11, 562, 321, 12972, 264, 3270, 346, 538, 300, 11, 527, 35193, 483, 9975, 13], "temperature": 0.0, "avg_logprob": -0.07046959950373723, "compression_ratio": 1.59, "no_speech_prob": 5.014681846660096e-06}, {"id": 1697, "seek": 707996, "start": 7095.96, "end": 7097.96, "text": " This is really important, right?", "tokens": [639, 307, 534, 1021, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.07046959950373723, "compression_ratio": 1.59, "no_speech_prob": 5.014681846660096e-06}, {"id": 1698, "seek": 707996, "start": 7097.96, "end": 7099.96, "text": " Because this is the sequence dimension.", "tokens": [1436, 341, 307, 264, 8310, 10139, 13], "temperature": 0.0, "avg_logprob": -0.07046959950373723, "compression_ratio": 1.59, "no_speech_prob": 5.014681846660096e-06}, {"id": 1699, "seek": 709996, "start": 7099.96, "end": 7113.96, "text": " So every time step, if you drop out time step number 3, but not time step 2 or 4, you've basically broken that whole sequence's ability to calculate anything because you just killed it.", "tokens": [407, 633, 565, 1823, 11, 498, 291, 3270, 484, 565, 1823, 1230, 805, 11, 457, 406, 565, 1823, 568, 420, 1017, 11, 291, 600, 1936, 5463, 300, 1379, 8310, 311, 3485, 281, 8873, 1340, 570, 291, 445, 4652, 309, 13], "temperature": 0.0, "avg_logprob": -0.0806591900027528, "compression_ratio": 1.628099173553719, "no_speech_prob": 1.1658914445433766e-05}, {"id": 1700, "seek": 709996, "start": 7113.96, "end": 7117.96, "text": " So this is called RNN dropout or also called variational dropout.", "tokens": [407, 341, 307, 1219, 45702, 45, 3270, 346, 420, 611, 1219, 3034, 1478, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.0806591900027528, "compression_ratio": 1.628099173553719, "no_speech_prob": 1.1658914445433766e-05}, {"id": 1701, "seek": 709996, "start": 7117.96, "end": 7119.96, "text": " There's a couple of different papers that introduce the same idea.", "tokens": [821, 311, 257, 1916, 295, 819, 10577, 300, 5366, 264, 912, 1558, 13], "temperature": 0.0, "avg_logprob": -0.0806591900027528, "compression_ratio": 1.628099173553719, "no_speech_prob": 1.1658914445433766e-05}, {"id": 1702, "seek": 709996, "start": 7119.96, "end": 7125.96, "text": " And it's simply this, that you do dropout on the entire sequence at a time.", "tokens": [400, 309, 311, 2935, 341, 11, 300, 291, 360, 3270, 346, 322, 264, 2302, 8310, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.0806591900027528, "compression_ratio": 1.628099173553719, "no_speech_prob": 1.1658914445433766e-05}, {"id": 1703, "seek": 712596, "start": 7125.96, "end": 7129.96, "text": " So there's RNN dropout.", "tokens": [407, 456, 311, 45702, 45, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.11784896335086308, "compression_ratio": 1.5846153846153845, "no_speech_prob": 8.267295015684795e-06}, {"id": 1704, "seek": 712596, "start": 7129.96, "end": 7135.96, "text": " The second one that Stephen Marody showed was something he called weight drop.", "tokens": [440, 1150, 472, 300, 13391, 2039, 843, 4712, 390, 746, 415, 1219, 3364, 3270, 13], "temperature": 0.0, "avg_logprob": -0.11784896335086308, "compression_ratio": 1.5846153846153845, "no_speech_prob": 8.267295015684795e-06}, {"id": 1705, "seek": 712596, "start": 7135.96, "end": 7141.96, "text": " It actually turns out that this already existed in the computer vision world where it was called drop connect.", "tokens": [467, 767, 4523, 484, 300, 341, 1217, 13135, 294, 264, 3820, 5201, 1002, 689, 309, 390, 1219, 3270, 1745, 13], "temperature": 0.0, "avg_logprob": -0.11784896335086308, "compression_ratio": 1.5846153846153845, "no_speech_prob": 8.267295015684795e-06}, {"id": 1706, "seek": 712596, "start": 7141.96, "end": 7149.96, "text": " So there's now two things with different names, but are the same, weight drop and drop connect.", "tokens": [407, 456, 311, 586, 732, 721, 365, 819, 5288, 11, 457, 366, 264, 912, 11, 3364, 3270, 293, 3270, 1745, 13], "temperature": 0.0, "avg_logprob": -0.11784896335086308, "compression_ratio": 1.5846153846153845, "no_speech_prob": 8.267295015684795e-06}, {"id": 1707, "seek": 714996, "start": 7149.96, "end": 7155.96, "text": " And this is dropout not on the activations, but on the weights themselves.", "tokens": [400, 341, 307, 3270, 346, 406, 322, 264, 2430, 763, 11, 457, 322, 264, 17443, 2969, 13], "temperature": 0.0, "avg_logprob": -0.05196193095003621, "compression_ratio": 1.7173913043478262, "no_speech_prob": 5.093644631415373e-06}, {"id": 1708, "seek": 714996, "start": 7155.96, "end": 7163.96, "text": " So you can see here when we do the forward pass, we go set weights that applies dropout to the actual weights.", "tokens": [407, 291, 393, 536, 510, 562, 321, 360, 264, 2128, 1320, 11, 321, 352, 992, 17443, 300, 13165, 3270, 346, 281, 264, 3539, 17443, 13], "temperature": 0.0, "avg_logprob": -0.05196193095003621, "compression_ratio": 1.7173913043478262, "no_speech_prob": 5.093644631415373e-06}, {"id": 1709, "seek": 714996, "start": 7163.96, "end": 7167.96, "text": " So that's our second type of dropout.", "tokens": [407, 300, 311, 527, 1150, 2010, 295, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.05196193095003621, "compression_ratio": 1.7173913043478262, "no_speech_prob": 5.093644631415373e-06}, {"id": 1710, "seek": 714996, "start": 7167.96, "end": 7170.96, "text": " The next one is embedding dropout.", "tokens": [440, 958, 472, 307, 12240, 3584, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.05196193095003621, "compression_ratio": 1.7173913043478262, "no_speech_prob": 5.093644631415373e-06}, {"id": 1711, "seek": 714996, "start": 7170.96, "end": 7177.96, "text": " And this one, as you can see, it drops out an entire row.", "tokens": [400, 341, 472, 11, 382, 291, 393, 536, 11, 309, 11438, 484, 364, 2302, 5386, 13], "temperature": 0.0, "avg_logprob": -0.05196193095003621, "compression_ratio": 1.7173913043478262, "no_speech_prob": 5.093644631415373e-06}, {"id": 1712, "seek": 717796, "start": 7177.96, "end": 7181.96, "text": " This is actually a coincidence that all these rows are in order, but it drops out an entire row.", "tokens": [639, 307, 767, 257, 22137, 300, 439, 613, 13241, 366, 294, 1668, 11, 457, 309, 11438, 484, 364, 2302, 5386, 13], "temperature": 0.0, "avg_logprob": -0.057948097586631775, "compression_ratio": 1.705, "no_speech_prob": 1.593583874637261e-05}, {"id": 1713, "seek": 717796, "start": 7181.96, "end": 7185.96, "text": " So what it does is it says, okay, you've got an embedding.", "tokens": [407, 437, 309, 775, 307, 309, 1619, 11, 1392, 11, 291, 600, 658, 364, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.057948097586631775, "compression_ratio": 1.705, "no_speech_prob": 1.593583874637261e-05}, {"id": 1714, "seek": 717796, "start": 7185.96, "end": 7196.96, "text": " And what I'm going to do is I'm going to drop out all of the entire embedding vector for whatever word this is.", "tokens": [400, 437, 286, 478, 516, 281, 360, 307, 286, 478, 516, 281, 3270, 484, 439, 295, 264, 2302, 12240, 3584, 8062, 337, 2035, 1349, 341, 307, 13], "temperature": 0.0, "avg_logprob": -0.057948097586631775, "compression_ratio": 1.705, "no_speech_prob": 1.593583874637261e-05}, {"id": 1715, "seek": 717796, "start": 7196.96, "end": 7200.96, "text": " So it's dropping out entire words at a time.", "tokens": [407, 309, 311, 13601, 484, 2302, 2283, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.057948097586631775, "compression_ratio": 1.705, "no_speech_prob": 1.593583874637261e-05}, {"id": 1716, "seek": 717796, "start": 7200.96, "end": 7203.96, "text": " So that's embedding dropout.", "tokens": [407, 300, 311, 12240, 3584, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.057948097586631775, "compression_ratio": 1.705, "no_speech_prob": 1.593583874637261e-05}, {"id": 1717, "seek": 720396, "start": 7203.96, "end": 7209.96, "text": " So with all that in place, we can create an LSTM model.", "tokens": [407, 365, 439, 300, 294, 1081, 11, 321, 393, 1884, 364, 441, 6840, 44, 2316, 13], "temperature": 0.0, "avg_logprob": -0.07390026862804706, "compression_ratio": 1.6777251184834123, "no_speech_prob": 6.4388468672405e-06}, {"id": 1718, "seek": 720396, "start": 7209.96, "end": 7210.96, "text": " It can be a number of layers.", "tokens": [467, 393, 312, 257, 1230, 295, 7914, 13], "temperature": 0.0, "avg_logprob": -0.07390026862804706, "compression_ratio": 1.6777251184834123, "no_speech_prob": 6.4388468672405e-06}, {"id": 1719, "seek": 720396, "start": 7210.96, "end": 7214.96, "text": " So we can create lots of LSTMs for however many layers you want.", "tokens": [407, 321, 393, 1884, 3195, 295, 441, 6840, 26386, 337, 4461, 867, 7914, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.07390026862804706, "compression_ratio": 1.6777251184834123, "no_speech_prob": 6.4388468672405e-06}, {"id": 1720, "seek": 720396, "start": 7214.96, "end": 7218.96, "text": " And we can loop through them and we can basically call each layer.", "tokens": [400, 321, 393, 6367, 807, 552, 293, 321, 393, 1936, 818, 1184, 4583, 13], "temperature": 0.0, "avg_logprob": -0.07390026862804706, "compression_ratio": 1.6777251184834123, "no_speech_prob": 6.4388468672405e-06}, {"id": 1721, "seek": 720396, "start": 7218.96, "end": 7220.96, "text": " And we've got all our different dropouts.", "tokens": [400, 321, 600, 658, 439, 527, 819, 3270, 7711, 13], "temperature": 0.0, "avg_logprob": -0.07390026862804706, "compression_ratio": 1.6777251184834123, "no_speech_prob": 6.4388468672405e-06}, {"id": 1722, "seek": 720396, "start": 7220.96, "end": 7225.96, "text": " And so basically this code is just calling all the different dropouts.", "tokens": [400, 370, 1936, 341, 3089, 307, 445, 5141, 439, 264, 819, 3270, 7711, 13], "temperature": 0.0, "avg_logprob": -0.07390026862804706, "compression_ratio": 1.6777251184834123, "no_speech_prob": 6.4388468672405e-06}, {"id": 1723, "seek": 720396, "start": 7225.96, "end": 7230.96, "text": " So that is an AWD LSTM.", "tokens": [407, 300, 307, 364, 25815, 35, 441, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.07390026862804706, "compression_ratio": 1.6777251184834123, "no_speech_prob": 6.4388468672405e-06}, {"id": 1724, "seek": 723096, "start": 7230.96, "end": 7240.96, "text": " So then we can put on top of that a simple linear model with dropout.", "tokens": [407, 550, 321, 393, 829, 322, 1192, 295, 300, 257, 2199, 8213, 2316, 365, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.07161846796671549, "compression_ratio": 1.621301775147929, "no_speech_prob": 5.338069058780093e-06}, {"id": 1725, "seek": 723096, "start": 7240.96, "end": 7248.96, "text": " And so this simple linear model, so it's literally just a linear model where we go dropout and then call our linear model,", "tokens": [400, 370, 341, 2199, 8213, 2316, 11, 370, 309, 311, 3736, 445, 257, 8213, 2316, 689, 321, 352, 3270, 346, 293, 550, 818, 527, 8213, 2316, 11], "temperature": 0.0, "avg_logprob": -0.07161846796671549, "compression_ratio": 1.621301775147929, "no_speech_prob": 5.338069058780093e-06}, {"id": 1726, "seek": 723096, "start": 7248.96, "end": 7255.96, "text": " is we're going to create a sequential model which takes the RNN, so the AWD LSTM,", "tokens": [307, 321, 434, 516, 281, 1884, 257, 42881, 2316, 597, 2516, 264, 45702, 45, 11, 370, 264, 25815, 35, 441, 6840, 44, 11], "temperature": 0.0, "avg_logprob": -0.07161846796671549, "compression_ratio": 1.621301775147929, "no_speech_prob": 5.338069058780093e-06}, {"id": 1727, "seek": 725596, "start": 7255.96, "end": 7260.96, "text": " and passes the result to a single linear layer with dropout.", "tokens": [293, 11335, 264, 1874, 281, 257, 2167, 8213, 4583, 365, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.059296643862160305, "compression_ratio": 1.705069124423963, "no_speech_prob": 7.071509571687784e-06}, {"id": 1728, "seek": 725596, "start": 7260.96, "end": 7269.96, "text": " And that is our language model because that final linear layer is the thing which will figure out what is the next word.", "tokens": [400, 300, 307, 527, 2856, 2316, 570, 300, 2572, 8213, 4583, 307, 264, 551, 597, 486, 2573, 484, 437, 307, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.059296643862160305, "compression_ratio": 1.705069124423963, "no_speech_prob": 7.071509571687784e-06}, {"id": 1729, "seek": 725596, "start": 7269.96, "end": 7274.96, "text": " So the size of that is the size of the vocab.", "tokens": [407, 264, 2744, 295, 300, 307, 264, 2744, 295, 264, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.059296643862160305, "compression_ratio": 1.705069124423963, "no_speech_prob": 7.071509571687784e-06}, {"id": 1730, "seek": 725596, "start": 7274.96, "end": 7277.96, "text": " It's good to look at these little tests that we do along the way.", "tokens": [467, 311, 665, 281, 574, 412, 613, 707, 6921, 300, 321, 360, 2051, 264, 636, 13], "temperature": 0.0, "avg_logprob": -0.059296643862160305, "compression_ratio": 1.705069124423963, "no_speech_prob": 7.071509571687784e-06}, {"id": 1731, "seek": 725596, "start": 7277.96, "end": 7281.96, "text": " These are the things we use to help us check that everything looks sensible.", "tokens": [1981, 366, 264, 721, 321, 764, 281, 854, 505, 1520, 300, 1203, 1542, 25380, 13], "temperature": 0.0, "avg_logprob": -0.059296643862160305, "compression_ratio": 1.705069124423963, "no_speech_prob": 7.071509571687784e-06}, {"id": 1732, "seek": 728196, "start": 7281.96, "end": 7285.96, "text": " And we found, yep, everything does look sensible.", "tokens": [400, 321, 1352, 11, 18633, 11, 1203, 775, 574, 25380, 13], "temperature": 0.0, "avg_logprob": -0.07215173755373273, "compression_ratio": 1.8008298755186722, "no_speech_prob": 3.647244011517614e-05}, {"id": 1733, "seek": 728196, "start": 7285.96, "end": 7289.96, "text": " And then we added something that AWD LSTM did which is called gradient clipping,", "tokens": [400, 550, 321, 3869, 746, 300, 25815, 35, 441, 6840, 44, 630, 597, 307, 1219, 16235, 49320, 11], "temperature": 0.0, "avg_logprob": -0.07215173755373273, "compression_ratio": 1.8008298755186722, "no_speech_prob": 3.647244011517614e-05}, {"id": 1734, "seek": 728196, "start": 7289.96, "end": 7296.96, "text": " which is a callback that just checks after the backward pass one of the gradients.", "tokens": [597, 307, 257, 818, 3207, 300, 445, 13834, 934, 264, 23897, 1320, 472, 295, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.07215173755373273, "compression_ratio": 1.8008298755186722, "no_speech_prob": 3.647244011517614e-05}, {"id": 1735, "seek": 728196, "start": 7296.96, "end": 7302.96, "text": " And if the total norm of the gradients, so the root sum of the gradients, is bigger than some number,", "tokens": [400, 498, 264, 3217, 2026, 295, 264, 2771, 2448, 11, 370, 264, 5593, 2408, 295, 264, 2771, 2448, 11, 307, 3801, 813, 512, 1230, 11], "temperature": 0.0, "avg_logprob": -0.07215173755373273, "compression_ratio": 1.8008298755186722, "no_speech_prob": 3.647244011517614e-05}, {"id": 1736, "seek": 728196, "start": 7302.96, "end": 7306.96, "text": " then we'll divide them all so that they're not bigger than that number anymore.", "tokens": [550, 321, 603, 9845, 552, 439, 370, 300, 436, 434, 406, 3801, 813, 300, 1230, 3602, 13], "temperature": 0.0, "avg_logprob": -0.07215173755373273, "compression_ratio": 1.8008298755186722, "no_speech_prob": 3.647244011517614e-05}, {"id": 1737, "seek": 728196, "start": 7306.96, "end": 7309.96, "text": " So it's just clipping those gradients.", "tokens": [407, 309, 311, 445, 49320, 729, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.07215173755373273, "compression_ratio": 1.8008298755186722, "no_speech_prob": 3.647244011517614e-05}, {"id": 1738, "seek": 730996, "start": 7309.96, "end": 7311.96, "text": " So that's how easy it is to add gradient clipping.", "tokens": [407, 300, 311, 577, 1858, 309, 307, 281, 909, 16235, 49320, 13], "temperature": 0.0, "avg_logprob": -0.07145294776329628, "compression_ratio": 1.6030927835051547, "no_speech_prob": 1.1842650565085933e-05}, {"id": 1739, "seek": 730996, "start": 7311.96, "end": 7315.96, "text": " This is a super good idea, not as used as much as it should be,", "tokens": [639, 307, 257, 1687, 665, 1558, 11, 406, 382, 1143, 382, 709, 382, 309, 820, 312, 11], "temperature": 0.0, "avg_logprob": -0.07145294776329628, "compression_ratio": 1.6030927835051547, "no_speech_prob": 1.1842650565085933e-05}, {"id": 1740, "seek": 730996, "start": 7315.96, "end": 7325.96, "text": " because it really lets you train things at higher learning rates and avoid gradients blowing out.", "tokens": [570, 309, 534, 6653, 291, 3847, 721, 412, 2946, 2539, 6846, 293, 5042, 2771, 2448, 15068, 484, 13], "temperature": 0.0, "avg_logprob": -0.07145294776329628, "compression_ratio": 1.6030927835051547, "no_speech_prob": 1.1842650565085933e-05}, {"id": 1741, "seek": 730996, "start": 7325.96, "end": 7329.96, "text": " Then there's two other kinds of regularization.", "tokens": [1396, 456, 311, 732, 661, 3685, 295, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.07145294776329628, "compression_ratio": 1.6030927835051547, "no_speech_prob": 1.1842650565085933e-05}, {"id": 1742, "seek": 730996, "start": 7329.96, "end": 7333.96, "text": " This one here is called activation regularization.", "tokens": [639, 472, 510, 307, 1219, 24433, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.07145294776329628, "compression_ratio": 1.6030927835051547, "no_speech_prob": 1.1842650565085933e-05}, {"id": 1743, "seek": 733396, "start": 7333.96, "end": 7341.96, "text": " And it's actually just an L2 loss, an L2 penalty, just like weight decay.", "tokens": [400, 309, 311, 767, 445, 364, 441, 17, 4470, 11, 364, 441, 17, 16263, 11, 445, 411, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.05160425901412964, "compression_ratio": 1.6137566137566137, "no_speech_prob": 7.5279099291947205e-06}, {"id": 1744, "seek": 733396, "start": 7341.96, "end": 7348.96, "text": " Except the L2 penalty is not on the weights, it's on the activations.", "tokens": [16192, 264, 441, 17, 16263, 307, 406, 322, 264, 17443, 11, 309, 311, 322, 264, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.05160425901412964, "compression_ratio": 1.6137566137566137, "no_speech_prob": 7.5279099291947205e-06}, {"id": 1745, "seek": 733396, "start": 7348.96, "end": 7352.96, "text": " So this is going to make sure that our activations are never too high.", "tokens": [407, 341, 307, 516, 281, 652, 988, 300, 527, 2430, 763, 366, 1128, 886, 1090, 13], "temperature": 0.0, "avg_logprob": -0.05160425901412964, "compression_ratio": 1.6137566137566137, "no_speech_prob": 7.5279099291947205e-06}, {"id": 1746, "seek": 733396, "start": 7352.96, "end": 7357.96, "text": " And then this one's really interesting. This is called temporal activation regularization.", "tokens": [400, 550, 341, 472, 311, 534, 1880, 13, 639, 307, 1219, 30881, 24433, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.05160425901412964, "compression_ratio": 1.6137566137566137, "no_speech_prob": 7.5279099291947205e-06}, {"id": 1747, "seek": 735796, "start": 7357.96, "end": 7367.96, "text": " This checks how much does each activation change by from sequence step to sequence step,", "tokens": [639, 13834, 577, 709, 775, 1184, 24433, 1319, 538, 490, 8310, 1823, 281, 8310, 1823, 11], "temperature": 0.0, "avg_logprob": -0.07412313461303711, "compression_ratio": 1.5797872340425532, "no_speech_prob": 1.3211028999648988e-05}, {"id": 1748, "seek": 735796, "start": 7367.96, "end": 7371.96, "text": " and then take the square of that.", "tokens": [293, 550, 747, 264, 3732, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.07412313461303711, "compression_ratio": 1.5797872340425532, "no_speech_prob": 1.3211028999648988e-05}, {"id": 1749, "seek": 735796, "start": 7371.96, "end": 7381.96, "text": " So this is regularizing the RNN to say, try not to have things that massively change from time step to time step,", "tokens": [407, 341, 307, 3890, 3319, 264, 45702, 45, 281, 584, 11, 853, 406, 281, 362, 721, 300, 29379, 1319, 490, 565, 1823, 281, 565, 1823, 11], "temperature": 0.0, "avg_logprob": -0.07412313461303711, "compression_ratio": 1.5797872340425532, "no_speech_prob": 1.3211028999648988e-05}, {"id": 1750, "seek": 735796, "start": 7381.96, "end": 7385.96, "text": " because if it's doing that, that's probably not a good sign.", "tokens": [570, 498, 309, 311, 884, 300, 11, 300, 311, 1391, 406, 257, 665, 1465, 13], "temperature": 0.0, "avg_logprob": -0.07412313461303711, "compression_ratio": 1.5797872340425532, "no_speech_prob": 1.3211028999648988e-05}, {"id": 1751, "seek": 738596, "start": 7385.96, "end": 7389.96, "text": " Okay, so that's our RNN trainer callback.", "tokens": [1033, 11, 370, 300, 311, 527, 45702, 45, 21110, 818, 3207, 13], "temperature": 0.0, "avg_logprob": -0.0994431605705848, "compression_ratio": 1.6228813559322033, "no_speech_prob": 6.962166935409186e-06}, {"id": 1752, "seek": 738596, "start": 7389.96, "end": 7395.96, "text": " We set up our loss functions, which are just normal cross entropy loss, and also a metric, which is normal accuracy.", "tokens": [492, 992, 493, 527, 4470, 6828, 11, 597, 366, 445, 2710, 3278, 30867, 4470, 11, 293, 611, 257, 20678, 11, 597, 307, 2710, 14170, 13], "temperature": 0.0, "avg_logprob": -0.0994431605705848, "compression_ratio": 1.6228813559322033, "no_speech_prob": 6.962166935409186e-06}, {"id": 1753, "seek": 738596, "start": 7395.96, "end": 7400.96, "text": " But we just make sure that our batch and sequence length is all flattened.", "tokens": [583, 321, 445, 652, 988, 300, 527, 15245, 293, 8310, 4641, 307, 439, 24183, 292, 13], "temperature": 0.0, "avg_logprob": -0.0994431605705848, "compression_ratio": 1.6228813559322033, "no_speech_prob": 6.962166935409186e-06}, {"id": 1754, "seek": 738596, "start": 7400.96, "end": 7406.96, "text": " So we can create our language model, add our callbacks, and fit.", "tokens": [407, 321, 393, 1884, 527, 2856, 2316, 11, 909, 527, 818, 17758, 11, 293, 3318, 13], "temperature": 0.0, "avg_logprob": -0.0994431605705848, "compression_ratio": 1.6228813559322033, "no_speech_prob": 6.962166935409186e-06}, {"id": 1755, "seek": 738596, "start": 7406.96, "end": 7412.96, "text": " So once we've got all that, we can use it to train a language model on Wikitext 103.", "tokens": [407, 1564, 321, 600, 658, 439, 300, 11, 321, 393, 764, 309, 281, 3847, 257, 2856, 2316, 322, 23377, 642, 734, 48784, 13], "temperature": 0.0, "avg_logprob": -0.0994431605705848, "compression_ratio": 1.6228813559322033, "no_speech_prob": 6.962166935409186e-06}, {"id": 1756, "seek": 741296, "start": 7412.96, "end": 7416.96, "text": " So I'm not going to go through this, because it literally just uses what's in the previous notebook,", "tokens": [407, 286, 478, 406, 516, 281, 352, 807, 341, 11, 570, 309, 3736, 445, 4960, 437, 311, 294, 264, 3894, 21060, 11], "temperature": 0.0, "avg_logprob": -0.08423522685436492, "compression_ratio": 1.565217391304348, "no_speech_prob": 2.2602773697144585e-06}, {"id": 1757, "seek": 741296, "start": 7416.96, "end": 7425.96, "text": " but this shows you here's how you can download Wikitext 103, split it into articles, create the text lists,", "tokens": [457, 341, 3110, 291, 510, 311, 577, 291, 393, 5484, 23377, 642, 734, 48784, 11, 7472, 309, 666, 11290, 11, 1884, 264, 2487, 14511, 11], "temperature": 0.0, "avg_logprob": -0.08423522685436492, "compression_ratio": 1.565217391304348, "no_speech_prob": 2.2602773697144585e-06}, {"id": 1758, "seek": 741296, "start": 7425.96, "end": 7434.96, "text": " split into train and valid, tokenize, numericalize, data bunchify, create the model that we just saw,", "tokens": [7472, 666, 3847, 293, 7363, 11, 14862, 1125, 11, 29054, 1125, 11, 1412, 3840, 2505, 11, 1884, 264, 2316, 300, 321, 445, 1866, 11], "temperature": 0.0, "avg_logprob": -0.08423522685436492, "compression_ratio": 1.565217391304348, "no_speech_prob": 2.2602773697144585e-06}, {"id": 1759, "seek": 741296, "start": 7434.96, "end": 7441.96, "text": " and train it for, in this case, about five hours.", "tokens": [293, 3847, 309, 337, 11, 294, 341, 1389, 11, 466, 1732, 2496, 13], "temperature": 0.0, "avg_logprob": -0.08423522685436492, "compression_ratio": 1.565217391304348, "no_speech_prob": 2.2602773697144585e-06}, {"id": 1760, "seek": 744196, "start": 7441.96, "end": 7444.96, "text": " Because it's quite a big model, right?", "tokens": [1436, 309, 311, 1596, 257, 955, 2316, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.07396860532863166, "compression_ratio": 1.6421052631578947, "no_speech_prob": 5.50752383787767e-06}, {"id": 1761, "seek": 744196, "start": 7444.96, "end": 7451.96, "text": " So because we don't want you to have to train for five hours, this RNN,", "tokens": [407, 570, 321, 500, 380, 528, 291, 281, 362, 281, 3847, 337, 1732, 2496, 11, 341, 45702, 45, 11], "temperature": 0.0, "avg_logprob": -0.07396860532863166, "compression_ratio": 1.6421052631578947, "no_speech_prob": 5.50752383787767e-06}, {"id": 1762, "seek": 744196, "start": 7451.96, "end": 7458.96, "text": " you will find that you can download that small pre-trained model from this link.", "tokens": [291, 486, 915, 300, 291, 393, 5484, 300, 1359, 659, 12, 17227, 2001, 2316, 490, 341, 2113, 13], "temperature": 0.0, "avg_logprob": -0.07396860532863166, "compression_ratio": 1.6421052631578947, "no_speech_prob": 5.50752383787767e-06}, {"id": 1763, "seek": 744196, "start": 7458.96, "end": 7462.96, "text": " So you can now use that on IMDB.", "tokens": [407, 291, 393, 586, 764, 300, 322, 21463, 27735, 13], "temperature": 0.0, "avg_logprob": -0.07396860532863166, "compression_ratio": 1.6421052631578947, "no_speech_prob": 5.50752383787767e-06}, {"id": 1764, "seek": 744196, "start": 7462.96, "end": 7470.96, "text": " So you can, again, grab your IMDB dataset, download that pre-trained model, load it in,", "tokens": [407, 291, 393, 11, 797, 11, 4444, 428, 21463, 27735, 28872, 11, 5484, 300, 659, 12, 17227, 2001, 2316, 11, 3677, 309, 294, 11], "temperature": 0.0, "avg_logprob": -0.07396860532863166, "compression_ratio": 1.6421052631578947, "no_speech_prob": 5.50752383787767e-06}, {"id": 1765, "seek": 747096, "start": 7470.96, "end": 7481.96, "text": " and then we need to do one more step, which is that the embedding matrix for the pre-trained Wikitext 103 model", "tokens": [293, 550, 321, 643, 281, 360, 472, 544, 1823, 11, 597, 307, 300, 264, 12240, 3584, 8141, 337, 264, 659, 12, 17227, 2001, 23377, 642, 734, 48784, 2316], "temperature": 0.0, "avg_logprob": -0.06724404263240034, "compression_ratio": 1.5576923076923077, "no_speech_prob": 1.7229889635927975e-05}, {"id": 1766, "seek": 747096, "start": 7481.96, "end": 7486.96, "text": " is for a different bunch of words to the IMDB vocab.", "tokens": [307, 337, 257, 819, 3840, 295, 2283, 281, 264, 21463, 27735, 2329, 455, 13], "temperature": 0.0, "avg_logprob": -0.06724404263240034, "compression_ratio": 1.5576923076923077, "no_speech_prob": 1.7229889635927975e-05}, {"id": 1767, "seek": 747096, "start": 7486.96, "end": 7489.96, "text": " So they've got different vocabs with some overlap.", "tokens": [407, 436, 600, 658, 819, 2329, 17243, 365, 512, 19959, 13], "temperature": 0.0, "avg_logprob": -0.06724404263240034, "compression_ratio": 1.5576923076923077, "no_speech_prob": 1.7229889635927975e-05}, {"id": 1768, "seek": 747096, "start": 7489.96, "end": 7498.96, "text": " So I won't go through the code, but what we just do is we just go through each vocab item in the IMDB vocab,", "tokens": [407, 286, 1582, 380, 352, 807, 264, 3089, 11, 457, 437, 321, 445, 360, 307, 321, 445, 352, 807, 1184, 2329, 455, 3174, 294, 264, 21463, 27735, 2329, 455, 11], "temperature": 0.0, "avg_logprob": -0.06724404263240034, "compression_ratio": 1.5576923076923077, "no_speech_prob": 1.7229889635927975e-05}, {"id": 1769, "seek": 749896, "start": 7498.96, "end": 7507.96, "text": " and we find out if it's in the Wikitext 103 vocab, and if it is, we copy Wikitext 103's vocab over.", "tokens": [293, 321, 915, 484, 498, 309, 311, 294, 264, 23377, 642, 734, 48784, 2329, 455, 11, 293, 498, 309, 307, 11, 321, 5055, 23377, 642, 734, 48784, 311, 2329, 455, 670, 13], "temperature": 0.0, "avg_logprob": -0.05884640258655213, "compression_ratio": 1.826086956521739, "no_speech_prob": 8.397411875193939e-06}, {"id": 1770, "seek": 749896, "start": 7507.96, "end": 7509.96, "text": " It's embedding over.", "tokens": [467, 311, 12240, 3584, 670, 13], "temperature": 0.0, "avg_logprob": -0.05884640258655213, "compression_ratio": 1.826086956521739, "no_speech_prob": 8.397411875193939e-06}, {"id": 1771, "seek": 749896, "start": 7509.96, "end": 7517.96, "text": " So that way we'll end up with an embedding matrix for IMDB that is the same as the Wikitext 103 embedding matrix", "tokens": [407, 300, 636, 321, 603, 917, 493, 365, 364, 12240, 3584, 8141, 337, 21463, 27735, 300, 307, 264, 912, 382, 264, 23377, 642, 734, 48784, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.05884640258655213, "compression_ratio": 1.826086956521739, "no_speech_prob": 8.397411875193939e-06}, {"id": 1772, "seek": 749896, "start": 7517.96, "end": 7522.96, "text": " any time there's a word that's the same, and any time there's a word that's missing,", "tokens": [604, 565, 456, 311, 257, 1349, 300, 311, 264, 912, 11, 293, 604, 565, 456, 311, 257, 1349, 300, 311, 5361, 11], "temperature": 0.0, "avg_logprob": -0.05884640258655213, "compression_ratio": 1.826086956521739, "no_speech_prob": 8.397411875193939e-06}, {"id": 1773, "seek": 749896, "start": 7522.96, "end": 7526.96, "text": " we're just going to use the mean bias and the mean weights.", "tokens": [321, 434, 445, 516, 281, 764, 264, 914, 12577, 293, 264, 914, 17443, 13], "temperature": 0.0, "avg_logprob": -0.05884640258655213, "compression_ratio": 1.826086956521739, "no_speech_prob": 8.397411875193939e-06}, {"id": 1774, "seek": 752696, "start": 7526.96, "end": 7529.96, "text": " So that's all that is.", "tokens": [407, 300, 311, 439, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.08273036215040419, "compression_ratio": 1.5076142131979695, "no_speech_prob": 1.0782953722809907e-05}, {"id": 1775, "seek": 752696, "start": 7529.96, "end": 7537.96, "text": " Okay, so once we've done that, we can then define a splitter, just like before, to create our layer groups.", "tokens": [1033, 11, 370, 1564, 321, 600, 1096, 300, 11, 321, 393, 550, 6964, 257, 4732, 3904, 11, 445, 411, 949, 11, 281, 1884, 527, 4583, 3935, 13], "temperature": 0.0, "avg_logprob": -0.08273036215040419, "compression_ratio": 1.5076142131979695, "no_speech_prob": 1.0782953722809907e-05}, {"id": 1776, "seek": 752696, "start": 7537.96, "end": 7547.96, "text": " We can set up our callbacks, our learner, we can fit, and so then we'll train that for an hour or so,", "tokens": [492, 393, 992, 493, 527, 818, 17758, 11, 527, 33347, 11, 321, 393, 3318, 11, 293, 370, 550, 321, 603, 3847, 300, 337, 364, 1773, 420, 370, 11], "temperature": 0.0, "avg_logprob": -0.08273036215040419, "compression_ratio": 1.5076142131979695, "no_speech_prob": 1.0782953722809907e-05}, {"id": 1777, "seek": 752696, "start": 7547.96, "end": 7555.96, "text": " and at the end of that we have a fine-tuned IMDB language model.", "tokens": [293, 412, 264, 917, 295, 300, 321, 362, 257, 2489, 12, 83, 43703, 21463, 27735, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.08273036215040419, "compression_ratio": 1.5076142131979695, "no_speech_prob": 1.0782953722809907e-05}, {"id": 1778, "seek": 755596, "start": 7555.96, "end": 7562.96, "text": " So now we can load up our classifier data bunch, which we created earlier.", "tokens": [407, 586, 321, 393, 3677, 493, 527, 1508, 9902, 1412, 3840, 11, 597, 321, 2942, 3071, 13], "temperature": 0.0, "avg_logprob": -0.07075626467481072, "compression_ratio": 1.5490196078431373, "no_speech_prob": 1.6186775610549375e-05}, {"id": 1779, "seek": 755596, "start": 7562.96, "end": 7567.96, "text": " That's exactly the same lines of code we had before.", "tokens": [663, 311, 2293, 264, 912, 3876, 295, 3089, 321, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.07075626467481072, "compression_ratio": 1.5490196078431373, "no_speech_prob": 1.6186775610549375e-05}, {"id": 1780, "seek": 755596, "start": 7567.96, "end": 7574.96, "text": " I'm going to ignore this pack padded sequence stuff, but basically there's a neat little trick in PyTorch", "tokens": [286, 478, 516, 281, 11200, 341, 2844, 6887, 9207, 8310, 1507, 11, 457, 1936, 456, 311, 257, 10654, 707, 4282, 294, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.07075626467481072, "compression_ratio": 1.5490196078431373, "no_speech_prob": 1.6186775610549375e-05}, {"id": 1781, "seek": 755596, "start": 7574.96, "end": 7580.96, "text": " where you can take data that's of different lengths and call pack padded sequence,", "tokens": [689, 291, 393, 747, 1412, 300, 311, 295, 819, 26329, 293, 818, 2844, 6887, 9207, 8310, 11], "temperature": 0.0, "avg_logprob": -0.07075626467481072, "compression_ratio": 1.5490196078431373, "no_speech_prob": 1.6186775610549375e-05}, {"id": 1782, "seek": 758096, "start": 7580.96, "end": 7586.96, "text": " pass that to an RNN, and then call pad packed sequence,", "tokens": [1320, 300, 281, 364, 45702, 45, 11, 293, 550, 818, 6887, 13265, 8310, 11], "temperature": 0.0, "avg_logprob": -0.10507452929461444, "compression_ratio": 1.3591549295774648, "no_speech_prob": 8.266272743640002e-06}, {"id": 1783, "seek": 758096, "start": 7586.96, "end": 7594.96, "text": " and it basically takes things of different lengths and optimally handles them in an RNN.", "tokens": [293, 309, 1936, 2516, 721, 295, 819, 26329, 293, 5028, 379, 18722, 552, 294, 364, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.10507452929461444, "compression_ratio": 1.3591549295774648, "no_speech_prob": 8.266272743640002e-06}, {"id": 1784, "seek": 758096, "start": 7594.96, "end": 7606.96, "text": " So we basically update our AWD LSTM to use that.", "tokens": [407, 321, 1936, 5623, 527, 25815, 35, 441, 6840, 44, 281, 764, 300, 13], "temperature": 0.0, "avg_logprob": -0.10507452929461444, "compression_ratio": 1.3591549295774648, "no_speech_prob": 8.266272743640002e-06}, {"id": 1785, "seek": 760696, "start": 7606.96, "end": 7616.96, "text": " You might remember that for ULM fit, we kind of create our hidden state in the LSTM for lots of time steps,", "tokens": [509, 1062, 1604, 300, 337, 624, 43, 44, 3318, 11, 321, 733, 295, 1884, 527, 7633, 1785, 294, 264, 441, 6840, 44, 337, 3195, 295, 565, 4439, 11], "temperature": 0.0, "avg_logprob": -0.08627422120836047, "compression_ratio": 1.538812785388128, "no_speech_prob": 1.1125149285362568e-05}, {"id": 1786, "seek": 760696, "start": 7616.96, "end": 7621.96, "text": " and we want to say, like, oh, which bit of state do we actually want to use for classification?", "tokens": [293, 321, 528, 281, 584, 11, 411, 11, 1954, 11, 597, 857, 295, 1785, 360, 321, 767, 528, 281, 764, 337, 21538, 30], "temperature": 0.0, "avg_logprob": -0.08627422120836047, "compression_ratio": 1.538812785388128, "no_speech_prob": 1.1125149285362568e-05}, {"id": 1787, "seek": 760696, "start": 7621.96, "end": 7625.96, "text": " People used to basically use the final state.", "tokens": [3432, 1143, 281, 1936, 764, 264, 2572, 1785, 13], "temperature": 0.0, "avg_logprob": -0.08627422120836047, "compression_ratio": 1.538812785388128, "no_speech_prob": 1.1125149285362568e-05}, {"id": 1788, "seek": 760696, "start": 7625.96, "end": 7629.96, "text": " Something that I tried and it turned out to work really well, so ended up in the paper,", "tokens": [6595, 300, 286, 3031, 293, 309, 3574, 484, 281, 589, 534, 731, 11, 370, 4590, 493, 294, 264, 3035, 11], "temperature": 0.0, "avg_logprob": -0.08627422120836047, "compression_ratio": 1.538812785388128, "no_speech_prob": 1.1125149285362568e-05}, {"id": 1789, "seek": 762996, "start": 7629.96, "end": 7639.96, "text": " was that we actually do an average pool and a max pool and use the final state,", "tokens": [390, 300, 321, 767, 360, 364, 4274, 7005, 293, 257, 11469, 7005, 293, 764, 264, 2572, 1785, 11], "temperature": 0.0, "avg_logprob": -0.07540751638866607, "compression_ratio": 1.664835164835165, "no_speech_prob": 3.3930436984519474e-06}, {"id": 1790, "seek": 762996, "start": 7639.96, "end": 7641.96, "text": " and we concatenate them all together.", "tokens": [293, 321, 1588, 7186, 473, 552, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.07540751638866607, "compression_ratio": 1.664835164835165, "no_speech_prob": 3.3930436984519474e-06}, {"id": 1791, "seek": 762996, "start": 7641.96, "end": 7645.96, "text": " So this is like the concat pooling we do for images.", "tokens": [407, 341, 307, 411, 264, 1588, 267, 7005, 278, 321, 360, 337, 5267, 13], "temperature": 0.0, "avg_logprob": -0.07540751638866607, "compression_ratio": 1.664835164835165, "no_speech_prob": 3.3930436984519474e-06}, {"id": 1792, "seek": 762996, "start": 7645.96, "end": 7649.96, "text": " We do the same kind of thing for text.", "tokens": [492, 360, 264, 912, 733, 295, 551, 337, 2487, 13], "temperature": 0.0, "avg_logprob": -0.07540751638866607, "compression_ratio": 1.664835164835165, "no_speech_prob": 3.3930436984519474e-06}, {"id": 1793, "seek": 762996, "start": 7649.96, "end": 7651.96, "text": " So we put all that together.", "tokens": [407, 321, 829, 439, 300, 1214, 13], "temperature": 0.0, "avg_logprob": -0.07540751638866607, "compression_ratio": 1.664835164835165, "no_speech_prob": 3.3930436984519474e-06}, {"id": 1794, "seek": 762996, "start": 7651.96, "end": 7655.96, "text": " This is just a chain of checking that everything looks sensible,", "tokens": [639, 307, 445, 257, 5021, 295, 8568, 300, 1203, 1542, 25380, 11], "temperature": 0.0, "avg_logprob": -0.07540751638866607, "compression_ratio": 1.664835164835165, "no_speech_prob": 3.3930436984519474e-06}, {"id": 1795, "seek": 765596, "start": 7655.96, "end": 7659.96, "text": " and that gives us something that we call the pooling linear classifier,", "tokens": [293, 300, 2709, 505, 746, 300, 321, 818, 264, 7005, 278, 8213, 1508, 9902, 11], "temperature": 0.0, "avg_logprob": -0.06854901843600802, "compression_ratio": 1.6323529411764706, "no_speech_prob": 6.240773473109584e-06}, {"id": 1796, "seek": 765596, "start": 7659.96, "end": 7671.96, "text": " which is just a list of batch norm dropout linear layers and our concat pooling.", "tokens": [597, 307, 445, 257, 1329, 295, 15245, 2026, 3270, 346, 8213, 7914, 293, 527, 1588, 267, 7005, 278, 13], "temperature": 0.0, "avg_logprob": -0.06854901843600802, "compression_ratio": 1.6323529411764706, "no_speech_prob": 6.240773473109584e-06}, {"id": 1797, "seek": 765596, "start": 7671.96, "end": 7672.96, "text": " And that's about it.", "tokens": [400, 300, 311, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.06854901843600802, "compression_ratio": 1.6323529411764706, "no_speech_prob": 6.240773473109584e-06}, {"id": 1798, "seek": 765596, "start": 7672.96, "end": 7680.96, "text": " So we just go through our sentence one BPTT at a time and keep calling that thing and keep appending the results.", "tokens": [407, 321, 445, 352, 807, 527, 8174, 472, 40533, 28178, 412, 257, 565, 293, 1066, 5141, 300, 551, 293, 1066, 724, 2029, 264, 3542, 13], "temperature": 0.0, "avg_logprob": -0.06854901843600802, "compression_ratio": 1.6323529411764706, "no_speech_prob": 6.240773473109584e-06}, {"id": 1799, "seek": 765596, "start": 7680.96, "end": 7683.96, "text": " So once we've done all that, we can train it.", "tokens": [407, 1564, 321, 600, 1096, 439, 300, 11, 321, 393, 3847, 309, 13], "temperature": 0.0, "avg_logprob": -0.06854901843600802, "compression_ratio": 1.6323529411764706, "no_speech_prob": 6.240773473109584e-06}, {"id": 1800, "seek": 768396, "start": 7683.96, "end": 7686.96, "text": " So here's our normal set of callbacks.", "tokens": [407, 510, 311, 527, 2710, 992, 295, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.0873164527717678, "compression_ratio": 1.4504950495049505, "no_speech_prob": 5.4221136451815255e-06}, {"id": 1801, "seek": 768396, "start": 7686.96, "end": 7692.96, "text": " We can load our fine-tuned encoder, and we can train.", "tokens": [492, 393, 3677, 527, 2489, 12, 83, 43703, 2058, 19866, 11, 293, 321, 393, 3847, 13], "temperature": 0.0, "avg_logprob": -0.0873164527717678, "compression_ratio": 1.4504950495049505, "no_speech_prob": 5.4221136451815255e-06}, {"id": 1802, "seek": 768396, "start": 7692.96, "end": 7703.96, "text": " And 92% accuracy, which is pretty close to where the state of the art was a very small number of years ago.", "tokens": [400, 28225, 4, 14170, 11, 597, 307, 1238, 1998, 281, 689, 264, 1785, 295, 264, 1523, 390, 257, 588, 1359, 1230, 295, 924, 2057, 13], "temperature": 0.0, "avg_logprob": -0.0873164527717678, "compression_ratio": 1.4504950495049505, "no_speech_prob": 5.4221136451815255e-06}, {"id": 1803, "seek": 768396, "start": 7703.96, "end": 7709.96, "text": " And this is not the same as we got about 94.5% or something like that, or 95% for the paper,", "tokens": [400, 341, 307, 406, 264, 912, 382, 321, 658, 466, 30849, 13, 20, 4, 420, 746, 411, 300, 11, 420, 13420, 4, 337, 264, 3035, 11], "temperature": 0.0, "avg_logprob": -0.0873164527717678, "compression_ratio": 1.4504950495049505, "no_speech_prob": 5.4221136451815255e-06}, {"id": 1804, "seek": 770996, "start": 7709.96, "end": 7714.96, "text": " because that used a bigger model that we trained for longer.", "tokens": [570, 300, 1143, 257, 3801, 2316, 300, 321, 8895, 337, 2854, 13], "temperature": 0.0, "avg_logprob": -0.11976809728713263, "compression_ratio": 1.393939393939394, "no_speech_prob": 2.090356929329573e-06}, {"id": 1805, "seek": 770996, "start": 7714.96, "end": 7715.96, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.11976809728713263, "compression_ratio": 1.393939393939394, "no_speech_prob": 2.090356929329573e-06}, {"id": 1806, "seek": 770996, "start": 7715.96, "end": 7723.96, "text": " So that was a super fast zip through ULM fit,", "tokens": [407, 300, 390, 257, 1687, 2370, 20730, 807, 624, 43, 44, 3318, 11], "temperature": 0.0, "avg_logprob": -0.11976809728713263, "compression_ratio": 1.393939393939394, "no_speech_prob": 2.090356929329573e-06}, {"id": 1807, "seek": 770996, "start": 7723.96, "end": 7729.96, "text": " and plenty of stuff which is probably worth reading in more detail,", "tokens": [293, 7140, 295, 1507, 597, 307, 1391, 3163, 3760, 294, 544, 2607, 11], "temperature": 0.0, "avg_logprob": -0.11976809728713263, "compression_ratio": 1.393939393939394, "no_speech_prob": 2.090356929329573e-06}, {"id": 1808, "seek": 770996, "start": 7729.96, "end": 7732.96, "text": " and we can answer questions on the forum as well.", "tokens": [293, 321, 393, 1867, 1651, 322, 264, 17542, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.11976809728713263, "compression_ratio": 1.393939393939394, "no_speech_prob": 2.090356929329573e-06}, {"id": 1809, "seek": 773296, "start": 7732.96, "end": 7740.96, "text": " So let's spend the last 10 minutes talking about Swift, because the next two classes are going to be about Swift.", "tokens": [407, 718, 311, 3496, 264, 1036, 1266, 2077, 1417, 466, 25539, 11, 570, 264, 958, 732, 5359, 366, 516, 281, 312, 466, 25539, 13], "temperature": 0.0, "avg_logprob": -0.08331729571024576, "compression_ratio": 1.5911111111111111, "no_speech_prob": 1.1300017831672449e-05}, {"id": 1810, "seek": 773296, "start": 7740.96, "end": 7748.96, "text": " So I think anybody who's got to lesson 12 in this course should be learning Swift for TensorFlow.", "tokens": [407, 286, 519, 4472, 567, 311, 658, 281, 6898, 2272, 294, 341, 1164, 820, 312, 2539, 25539, 337, 37624, 13], "temperature": 0.0, "avg_logprob": -0.08331729571024576, "compression_ratio": 1.5911111111111111, "no_speech_prob": 1.1300017831672449e-05}, {"id": 1811, "seek": 773296, "start": 7748.96, "end": 7752.96, "text": " The reason why is I think basically the Python stays a numbered.", "tokens": [440, 1778, 983, 307, 286, 519, 1936, 264, 15329, 10834, 257, 40936, 13], "temperature": 0.0, "avg_logprob": -0.08331729571024576, "compression_ratio": 1.5911111111111111, "no_speech_prob": 1.1300017831672449e-05}, {"id": 1812, "seek": 773296, "start": 7752.96, "end": 7756.96, "text": " That stuff I showed you about JIT, the more I use JIT, the more I think about it,", "tokens": [663, 1507, 286, 4712, 291, 466, 508, 3927, 11, 264, 544, 286, 764, 508, 3927, 11, 264, 544, 286, 519, 466, 309, 11], "temperature": 0.0, "avg_logprob": -0.08331729571024576, "compression_ratio": 1.5911111111111111, "no_speech_prob": 1.1300017831672449e-05}, {"id": 1813, "seek": 775696, "start": 7756.96, "end": 7762.96, "text": " the more it looks like failed examples of software development processes I've seen in the last 25 years.", "tokens": [264, 544, 309, 1542, 411, 7612, 5110, 295, 4722, 3250, 7555, 286, 600, 1612, 294, 264, 1036, 3552, 924, 13], "temperature": 0.0, "avg_logprob": -0.050707094448128924, "compression_ratio": 1.6317829457364341, "no_speech_prob": 1.6436606529168785e-05}, {"id": 1814, "seek": 775696, "start": 7762.96, "end": 7765.96, "text": " Whenever people try to convert one language into a different language,", "tokens": [14159, 561, 853, 281, 7620, 472, 2856, 666, 257, 819, 2856, 11], "temperature": 0.0, "avg_logprob": -0.050707094448128924, "compression_ratio": 1.6317829457364341, "no_speech_prob": 1.6436606529168785e-05}, {"id": 1815, "seek": 775696, "start": 7765.96, "end": 7768.96, "text": " and then you're kind of using the language that you're not really using,", "tokens": [293, 550, 291, 434, 733, 295, 1228, 264, 2856, 300, 291, 434, 406, 534, 1228, 11], "temperature": 0.0, "avg_logprob": -0.050707094448128924, "compression_ratio": 1.6317829457364341, "no_speech_prob": 1.6436606529168785e-05}, {"id": 1816, "seek": 775696, "start": 7768.96, "end": 7776.96, "text": " it requires brilliant, brilliant people like the PyTorch team years to make it almost kind of work.", "tokens": [309, 7029, 10248, 11, 10248, 561, 411, 264, 9953, 51, 284, 339, 1469, 924, 281, 652, 309, 1920, 733, 295, 589, 13], "temperature": 0.0, "avg_logprob": -0.050707094448128924, "compression_ratio": 1.6317829457364341, "no_speech_prob": 1.6436606529168785e-05}, {"id": 1817, "seek": 775696, "start": 7776.96, "end": 7784.96, "text": " So I think Julia or Swift will eventually in the coming years take over.", "tokens": [407, 286, 519, 18551, 420, 25539, 486, 4728, 294, 264, 1348, 924, 747, 670, 13], "temperature": 0.0, "avg_logprob": -0.050707094448128924, "compression_ratio": 1.6317829457364341, "no_speech_prob": 1.6436606529168785e-05}, {"id": 1818, "seek": 778496, "start": 7784.96, "end": 7790.96, "text": " I just don't think Python can survive, because we can't write CUDA kernels in Python.", "tokens": [286, 445, 500, 380, 519, 15329, 393, 7867, 11, 570, 321, 393, 380, 2464, 29777, 7509, 23434, 1625, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.07885200952746205, "compression_ratio": 1.609442060085837, "no_speech_prob": 2.2818250727141276e-05}, {"id": 1819, "seek": 778496, "start": 7790.96, "end": 7795.96, "text": " We can't write RNN cells in Python and have them work reliably and fast.", "tokens": [492, 393, 380, 2464, 45702, 45, 5438, 294, 15329, 293, 362, 552, 589, 49927, 293, 2370, 13], "temperature": 0.0, "avg_logprob": -0.07885200952746205, "compression_ratio": 1.609442060085837, "no_speech_prob": 2.2818250727141276e-05}, {"id": 1820, "seek": 778496, "start": 7795.96, "end": 7800.96, "text": " DL libraries change all the time anyway, so if you're spending all your time just studying one library in one language,", "tokens": [413, 43, 15148, 1319, 439, 264, 565, 4033, 11, 370, 498, 291, 434, 6434, 439, 428, 565, 445, 7601, 472, 6405, 294, 472, 2856, 11], "temperature": 0.0, "avg_logprob": -0.07885200952746205, "compression_ratio": 1.609442060085837, "no_speech_prob": 2.2818250727141276e-05}, {"id": 1821, "seek": 778496, "start": 7800.96, "end": 7805.96, "text": " then you're not going to be ready for that change.", "tokens": [550, 291, 434, 406, 516, 281, 312, 1919, 337, 300, 1319, 13], "temperature": 0.0, "avg_logprob": -0.07885200952746205, "compression_ratio": 1.609442060085837, "no_speech_prob": 2.2818250727141276e-05}, {"id": 1822, "seek": 778496, "start": 7805.96, "end": 7808.96, "text": " So you'll need to learn something new anyway.", "tokens": [407, 291, 603, 643, 281, 1466, 746, 777, 4033, 13], "temperature": 0.0, "avg_logprob": -0.07885200952746205, "compression_ratio": 1.609442060085837, "no_speech_prob": 2.2818250727141276e-05}, {"id": 1823, "seek": 780896, "start": 7808.96, "end": 7814.96, "text": " It could probably be Swift or Julia, and I think they're both perfectly good things to look at.", "tokens": [467, 727, 1391, 312, 25539, 420, 18551, 11, 293, 286, 519, 436, 434, 1293, 6239, 665, 721, 281, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.08709578514099121, "compression_ratio": 1.6007905138339922, "no_speech_prob": 1.2604787116288207e-05}, {"id": 1824, "seek": 780896, "start": 7814.96, "end": 7822.96, "text": " Regardless, I've spent time using in real world scenarios at least a couple of dozen languages,", "tokens": [25148, 11, 286, 600, 4418, 565, 1228, 294, 957, 1002, 15077, 412, 1935, 257, 1916, 295, 16654, 8650, 11], "temperature": 0.0, "avg_logprob": -0.08709578514099121, "compression_ratio": 1.6007905138339922, "no_speech_prob": 1.2604787116288207e-05}, {"id": 1825, "seek": 780896, "start": 7822.96, "end": 7826.96, "text": " and every time I learn a new language, I become a better developer.", "tokens": [293, 633, 565, 286, 1466, 257, 777, 2856, 11, 286, 1813, 257, 1101, 10754, 13], "temperature": 0.0, "avg_logprob": -0.08709578514099121, "compression_ratio": 1.6007905138339922, "no_speech_prob": 1.2604787116288207e-05}, {"id": 1826, "seek": 780896, "start": 7826.96, "end": 7830.96, "text": " So it's just a good idea to learn a new language.", "tokens": [407, 309, 311, 445, 257, 665, 1558, 281, 1466, 257, 777, 2856, 13], "temperature": 0.0, "avg_logprob": -0.08709578514099121, "compression_ratio": 1.6007905138339922, "no_speech_prob": 1.2604787116288207e-05}, {"id": 1827, "seek": 780896, "start": 7830.96, "end": 7835.96, "text": " And the TensorFlow bit might put you off a bit, because I've complained a lot about TensorFlow,", "tokens": [400, 264, 37624, 857, 1062, 829, 291, 766, 257, 857, 11, 570, 286, 600, 33951, 257, 688, 466, 37624, 11], "temperature": 0.0, "avg_logprob": -0.08709578514099121, "compression_ratio": 1.6007905138339922, "no_speech_prob": 1.2604787116288207e-05}, {"id": 1828, "seek": 783596, "start": 7835.96, "end": 7842.96, "text": " but actually TensorFlow in the future is going to look almost totally different to TensorFlow in the past.", "tokens": [457, 767, 37624, 294, 264, 2027, 307, 516, 281, 574, 1920, 3879, 819, 281, 37624, 294, 264, 1791, 13], "temperature": 0.0, "avg_logprob": -0.07539449736129405, "compression_ratio": 1.680672268907563, "no_speech_prob": 1.618485475773923e-05}, {"id": 1829, "seek": 783596, "start": 7842.96, "end": 7846.96, "text": " The things that are happening with Swift for TensorFlow are so exciting.", "tokens": [440, 721, 300, 366, 2737, 365, 25539, 337, 37624, 366, 370, 4670, 13], "temperature": 0.0, "avg_logprob": -0.07539449736129405, "compression_ratio": 1.680672268907563, "no_speech_prob": 1.618485475773923e-05}, {"id": 1830, "seek": 783596, "start": 7846.96, "end": 7852.96, "text": " So there's basically almost no data science ecosystem for Swift,", "tokens": [407, 456, 311, 1936, 1920, 572, 1412, 3497, 11311, 337, 25539, 11], "temperature": 0.0, "avg_logprob": -0.07539449736129405, "compression_ratio": 1.680672268907563, "no_speech_prob": 1.618485475773923e-05}, {"id": 1831, "seek": 783596, "start": 7852.96, "end": 7855.96, "text": " which means the whole thing is open for you to contribute to.", "tokens": [597, 1355, 264, 1379, 551, 307, 1269, 337, 291, 281, 10586, 281, 13], "temperature": 0.0, "avg_logprob": -0.07539449736129405, "compression_ratio": 1.680672268907563, "no_speech_prob": 1.618485475773923e-05}, {"id": 1832, "seek": 783596, "start": 7855.96, "end": 7862.96, "text": " So you can make serious contributions, look at any Python little library or just one function", "tokens": [407, 291, 393, 652, 3156, 15725, 11, 574, 412, 604, 15329, 707, 6405, 420, 445, 472, 2445], "temperature": 0.0, "avg_logprob": -0.07539449736129405, "compression_ratio": 1.680672268907563, "no_speech_prob": 1.618485475773923e-05}, {"id": 1833, "seek": 786296, "start": 7862.96, "end": 7868.96, "text": " that doesn't exist in Swift and write it.", "tokens": [300, 1177, 380, 2514, 294, 25539, 293, 2464, 309, 13], "temperature": 0.0, "avg_logprob": -0.09722054659665286, "compression_ratio": 1.6125, "no_speech_prob": 3.84464055969147e-06}, {"id": 1834, "seek": 786296, "start": 7868.96, "end": 7874.96, "text": " The Swift community doesn't have people like us, people that understand deep learning.", "tokens": [440, 25539, 1768, 1177, 380, 362, 561, 411, 505, 11, 561, 300, 1223, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.09722054659665286, "compression_ratio": 1.6125, "no_speech_prob": 3.84464055969147e-06}, {"id": 1835, "seek": 786296, "start": 7874.96, "end": 7881.96, "text": " They're just not people who are generally in the Swift community right now, with some exceptions.", "tokens": [814, 434, 445, 406, 561, 567, 366, 5101, 294, 264, 25539, 1768, 558, 586, 11, 365, 512, 22847, 13], "temperature": 0.0, "avg_logprob": -0.09722054659665286, "compression_ratio": 1.6125, "no_speech_prob": 3.84464055969147e-06}, {"id": 1836, "seek": 786296, "start": 7881.96, "end": 7886.96, "text": " So we are valued, and you'll be working on stuff that will look pretty familiar,", "tokens": [407, 321, 366, 22608, 11, 293, 291, 603, 312, 1364, 322, 1507, 300, 486, 574, 1238, 4963, 11], "temperature": 0.0, "avg_logprob": -0.09722054659665286, "compression_ratio": 1.6125, "no_speech_prob": 3.84464055969147e-06}, {"id": 1837, "seek": 786296, "start": 7886.96, "end": 7891.96, "text": " because we're building something a lot like Fast AI, but hopefully much better.", "tokens": [570, 321, 434, 2390, 746, 257, 688, 411, 15968, 7318, 11, 457, 4696, 709, 1101, 13], "temperature": 0.0, "avg_logprob": -0.09722054659665286, "compression_ratio": 1.6125, "no_speech_prob": 3.84464055969147e-06}, {"id": 1838, "seek": 789196, "start": 7891.96, "end": 7899.96, "text": " So with that, I have here Chris Latner, who started the Swift project", "tokens": [407, 365, 300, 11, 286, 362, 510, 6688, 7354, 1193, 11, 567, 1409, 264, 25539, 1716], "temperature": 0.0, "avg_logprob": -0.07477934625413683, "compression_ratio": 1.587719298245614, "no_speech_prob": 7.473418372683227e-05}, {"id": 1839, "seek": 789196, "start": 7899.96, "end": 7903.96, "text": " and is now running Swift for TensorFlow team at Google.", "tokens": [293, 307, 586, 2614, 25539, 337, 37624, 1469, 412, 3329, 13], "temperature": 0.0, "avg_logprob": -0.07477934625413683, "compression_ratio": 1.587719298245614, "no_speech_prob": 7.473418372683227e-05}, {"id": 1840, "seek": 789196, "start": 7903.96, "end": 7908.96, "text": " And we have time for, I think, three questions from the community for Chris and I.", "tokens": [400, 321, 362, 565, 337, 11, 286, 519, 11, 1045, 1651, 490, 264, 1768, 337, 6688, 293, 286, 13], "temperature": 0.0, "avg_logprob": -0.07477934625413683, "compression_ratio": 1.587719298245614, "no_speech_prob": 7.473418372683227e-05}, {"id": 1841, "seek": 789196, "start": 7908.96, "end": 7911.96, "text": " Sure. Assuming someone has zero knowledge of Swift,", "tokens": [4894, 13, 6281, 24919, 1580, 575, 4018, 3601, 295, 25539, 11], "temperature": 0.0, "avg_logprob": -0.07477934625413683, "compression_ratio": 1.587719298245614, "no_speech_prob": 7.473418372683227e-05}, {"id": 1842, "seek": 789196, "start": 7911.96, "end": 7916.96, "text": " what would be the most efficient way to learn it and get up to speed with using Swift for TensorFlow?", "tokens": [437, 576, 312, 264, 881, 7148, 636, 281, 1466, 309, 293, 483, 493, 281, 3073, 365, 1228, 25539, 337, 37624, 30], "temperature": 0.0, "avg_logprob": -0.07477934625413683, "compression_ratio": 1.587719298245614, "no_speech_prob": 7.473418372683227e-05}, {"id": 1843, "seek": 791696, "start": 7916.96, "end": 7922.96, "text": " Sure. So the courses we're teaching will assume that you don't have prior Swift experience,", "tokens": [4894, 13, 407, 264, 7712, 321, 434, 4571, 486, 6552, 300, 291, 500, 380, 362, 4059, 25539, 1752, 11], "temperature": 0.0, "avg_logprob": -0.105358596920043, "compression_ratio": 1.6375838926174497, "no_speech_prob": 0.00012119508755858988}, {"id": 1844, "seek": 791696, "start": 7922.96, "end": 7924.96, "text": " but if you're interested, you can go to swift.org.", "tokens": [457, 498, 291, 434, 3102, 11, 291, 393, 352, 281, 29184, 13, 4646, 13], "temperature": 0.0, "avg_logprob": -0.105358596920043, "compression_ratio": 1.6375838926174497, "no_speech_prob": 0.00012119508755858988}, {"id": 1845, "seek": 791696, "start": 7924.96, "end": 7927.96, "text": " In the documentation tab, there's a whole book online.", "tokens": [682, 264, 14333, 4421, 11, 456, 311, 257, 1379, 1446, 2950, 13], "temperature": 0.0, "avg_logprob": -0.105358596920043, "compression_ratio": 1.6375838926174497, "no_speech_prob": 0.00012119508755858988}, {"id": 1846, "seek": 791696, "start": 7927.96, "end": 7931.96, "text": " The thing I recommend is there's a thing called a Swift Tour.", "tokens": [440, 551, 286, 2748, 307, 456, 311, 257, 551, 1219, 257, 25539, 13077, 13], "temperature": 0.0, "avg_logprob": -0.105358596920043, "compression_ratio": 1.6375838926174497, "no_speech_prob": 0.00012119508755858988}, {"id": 1847, "seek": 791696, "start": 7931.96, "end": 7933.96, "text": " You can just Google for that.", "tokens": [509, 393, 445, 3329, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.105358596920043, "compression_ratio": 1.6375838926174497, "no_speech_prob": 0.00012119508755858988}, {"id": 1848, "seek": 791696, "start": 7933.96, "end": 7936.96, "text": " It gives you a really quick sense of what it looks like,", "tokens": [467, 2709, 291, 257, 534, 1702, 2020, 295, 437, 309, 1542, 411, 11], "temperature": 0.0, "avg_logprob": -0.105358596920043, "compression_ratio": 1.6375838926174497, "no_speech_prob": 0.00012119508755858988}, {"id": 1849, "seek": 791696, "start": 7936.96, "end": 7940.96, "text": " and it explains the basic concepts, it's super accessible, and that's where I would start.", "tokens": [293, 309, 13948, 264, 3875, 10392, 11, 309, 311, 1687, 9515, 11, 293, 300, 311, 689, 286, 576, 722, 13], "temperature": 0.0, "avg_logprob": -0.105358596920043, "compression_ratio": 1.6375838926174497, "no_speech_prob": 0.00012119508755858988}, {"id": 1850, "seek": 791696, "start": 7940.96, "end": 7943.96, "text": " The best version of the Swift book is on the iPad.", "tokens": [440, 1151, 3037, 295, 264, 25539, 1446, 307, 322, 264, 12945, 13], "temperature": 0.0, "avg_logprob": -0.105358596920043, "compression_ratio": 1.6375838926174497, "no_speech_prob": 0.00012119508755858988}, {"id": 1851, "seek": 794396, "start": 7943.96, "end": 7947.96, "text": " It uses something called Swift Playgrounds, which is one of these amazing things that Chris built,", "tokens": [467, 4960, 746, 1219, 25539, 5506, 2921, 82, 11, 597, 307, 472, 295, 613, 2243, 721, 300, 6688, 3094, 11], "temperature": 0.0, "avg_logprob": -0.04818396649118197, "compression_ratio": 1.618421052631579, "no_speech_prob": 4.465054735192098e-05}, {"id": 1852, "seek": 794396, "start": 7947.96, "end": 7950.96, "text": " which basically lets you go through the book in a very interactive way.", "tokens": [597, 1936, 6653, 291, 352, 807, 264, 1446, 294, 257, 588, 15141, 636, 13], "temperature": 0.0, "avg_logprob": -0.04818396649118197, "compression_ratio": 1.618421052631579, "no_speech_prob": 4.465054735192098e-05}, {"id": 1853, "seek": 794396, "start": 7950.96, "end": 7954.96, "text": " It'll feel a lot like the experience of using a Jupyter Notebook,", "tokens": [467, 603, 841, 257, 688, 411, 264, 1752, 295, 1228, 257, 22125, 88, 391, 11633, 2939, 11], "temperature": 0.0, "avg_logprob": -0.04818396649118197, "compression_ratio": 1.618421052631579, "no_speech_prob": 4.465054735192098e-05}, {"id": 1854, "seek": 794396, "start": 7954.96, "end": 7960.96, "text": " but it's even more fancy in some ways, so you can read the book as you experiment.", "tokens": [457, 309, 311, 754, 544, 10247, 294, 512, 2098, 11, 370, 291, 393, 1401, 264, 1446, 382, 291, 5120, 13], "temperature": 0.0, "avg_logprob": -0.04818396649118197, "compression_ratio": 1.618421052631579, "no_speech_prob": 4.465054735192098e-05}, {"id": 1855, "seek": 794396, "start": 7960.96, "end": 7965.96, "text": " As Swift for TensorFlow evolves, what do you think will be the first kind of machine learning work", "tokens": [1018, 25539, 337, 37624, 43737, 11, 437, 360, 291, 519, 486, 312, 264, 700, 733, 295, 3479, 2539, 589], "temperature": 0.0, "avg_logprob": -0.04818396649118197, "compression_ratio": 1.618421052631579, "no_speech_prob": 4.465054735192098e-05}, {"id": 1856, "seek": 794396, "start": 7965.96, "end": 7969.96, "text": " accessible to people who don't have access to big corporate data centers,", "tokens": [9515, 281, 561, 567, 500, 380, 362, 2105, 281, 955, 10896, 1412, 10898, 11], "temperature": 0.0, "avg_logprob": -0.04818396649118197, "compression_ratio": 1.618421052631579, "no_speech_prob": 4.465054735192098e-05}, {"id": 1857, "seek": 796996, "start": 7969.96, "end": 7975.96, "text": " where Swift for TensorFlow's particular strengths will make it a better choice than the more traditional Python frameworks?", "tokens": [689, 25539, 337, 37624, 311, 1729, 16986, 486, 652, 309, 257, 1101, 3922, 813, 264, 544, 5164, 15329, 29834, 30], "temperature": 0.0, "avg_logprob": -0.07614067734264937, "compression_ratio": 1.696078431372549, "no_speech_prob": 3.0202054404071532e-05}, {"id": 1858, "seek": 796996, "start": 7975.96, "end": 7979.96, "text": " Sure. I don't know what that first thing will be,", "tokens": [4894, 13, 286, 500, 380, 458, 437, 300, 700, 551, 486, 312, 11], "temperature": 0.0, "avg_logprob": -0.07614067734264937, "compression_ratio": 1.696078431372549, "no_speech_prob": 3.0202054404071532e-05}, {"id": 1859, "seek": 796996, "start": 7979.96, "end": 7982.96, "text": " but I think you have to look at the goals of the project,", "tokens": [457, 286, 519, 291, 362, 281, 574, 412, 264, 5493, 295, 264, 1716, 11], "temperature": 0.0, "avg_logprob": -0.07614067734264937, "compression_ratio": 1.696078431372549, "no_speech_prob": 3.0202054404071532e-05}, {"id": 1860, "seek": 796996, "start": 7982.96, "end": 7985.96, "text": " and I think there's two goals for this project overall.", "tokens": [293, 286, 519, 456, 311, 732, 5493, 337, 341, 1716, 4787, 13], "temperature": 0.0, "avg_logprob": -0.07614067734264937, "compression_ratio": 1.696078431372549, "no_speech_prob": 3.0202054404071532e-05}, {"id": 1861, "seek": 796996, "start": 7985.96, "end": 7989.96, "text": " One is to be very subtractive, and subtractive of complexity.", "tokens": [1485, 307, 281, 312, 588, 16390, 488, 11, 293, 16390, 488, 295, 14024, 13], "temperature": 0.0, "avg_logprob": -0.07614067734264937, "compression_ratio": 1.696078431372549, "no_speech_prob": 3.0202054404071532e-05}, {"id": 1862, "seek": 796996, "start": 7989.96, "end": 7993.96, "text": " I think that one of the things that Jeremy's highlighting is that in practice,", "tokens": [286, 519, 300, 472, 295, 264, 721, 300, 17809, 311, 26551, 307, 300, 294, 3124, 11], "temperature": 0.0, "avg_logprob": -0.07614067734264937, "compression_ratio": 1.696078431372549, "no_speech_prob": 3.0202054404071532e-05}, {"id": 1863, "seek": 796996, "start": 7993.96, "end": 7998.96, "text": " being effective in the machine learning field means you end up doing a lot of weird things", "tokens": [885, 4942, 294, 264, 3479, 2539, 2519, 1355, 291, 917, 493, 884, 257, 688, 295, 3657, 721], "temperature": 0.0, "avg_logprob": -0.07614067734264937, "compression_ratio": 1.696078431372549, "no_speech_prob": 3.0202054404071532e-05}, {"id": 1864, "seek": 799896, "start": 7998.96, "end": 8002.96, "text": " at different levels, and so you may be dropping down to C++ or writing CUDA code,", "tokens": [412, 819, 4358, 11, 293, 370, 291, 815, 312, 13601, 760, 281, 383, 25472, 420, 3579, 29777, 7509, 3089, 11], "temperature": 0.0, "avg_logprob": -0.07883615258299274, "compression_ratio": 1.781437125748503, "no_speech_prob": 5.809276626678184e-05}, {"id": 1865, "seek": 799896, "start": 8002.96, "end": 8003.96, "text": " depending on what you're doing.", "tokens": [5413, 322, 437, 291, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.07883615258299274, "compression_ratio": 1.781437125748503, "no_speech_prob": 5.809276626678184e-05}, {"id": 1866, "seek": 799896, "start": 8003.96, "end": 8006.96, "text": " You may be playing with TorchJet or playing with these other systems", "tokens": [509, 815, 312, 2433, 365, 7160, 339, 41, 302, 420, 2433, 365, 613, 661, 3652], "temperature": 0.0, "avg_logprob": -0.07883615258299274, "compression_ratio": 1.781437125748503, "no_speech_prob": 5.809276626678184e-05}, {"id": 1867, "seek": 799896, "start": 8006.96, "end": 8010.96, "text": " or these other C libraries that get wrapped up with Python,", "tokens": [420, 613, 661, 383, 15148, 300, 483, 14226, 493, 365, 15329, 11], "temperature": 0.0, "avg_logprob": -0.07883615258299274, "compression_ratio": 1.781437125748503, "no_speech_prob": 5.809276626678184e-05}, {"id": 1868, "seek": 799896, "start": 8010.96, "end": 8013.96, "text": " but these become leaky abstractions you have to deal with,", "tokens": [457, 613, 1813, 476, 15681, 12649, 626, 291, 362, 281, 2028, 365, 11], "temperature": 0.0, "avg_logprob": -0.07883615258299274, "compression_ratio": 1.781437125748503, "no_speech_prob": 5.809276626678184e-05}, {"id": 1869, "seek": 799896, "start": 8013.96, "end": 8016.96, "text": " and so we're trying to make it so you don't have to deal with a lot of that complexity,", "tokens": [293, 370, 321, 434, 1382, 281, 652, 309, 370, 291, 500, 380, 362, 281, 2028, 365, 257, 688, 295, 300, 14024, 11], "temperature": 0.0, "avg_logprob": -0.07883615258299274, "compression_ratio": 1.781437125748503, "no_speech_prob": 5.809276626678184e-05}, {"id": 1870, "seek": 799896, "start": 8016.96, "end": 8017.96, "text": " so you can stay in one language.", "tokens": [370, 291, 393, 1754, 294, 472, 2856, 13], "temperature": 0.0, "avg_logprob": -0.07883615258299274, "compression_ratio": 1.781437125748503, "no_speech_prob": 5.809276626678184e-05}, {"id": 1871, "seek": 799896, "start": 8017.96, "end": 8021.96, "text": " It works top to bottom. It's fast, has lots of other good things that go with it,", "tokens": [467, 1985, 1192, 281, 2767, 13, 467, 311, 2370, 11, 575, 3195, 295, 661, 665, 721, 300, 352, 365, 309, 11], "temperature": 0.0, "avg_logprob": -0.07883615258299274, "compression_ratio": 1.781437125748503, "no_speech_prob": 5.809276626678184e-05}, {"id": 1872, "seek": 799896, "start": 8021.96, "end": 8023.96, "text": " so that's one aspect of it.", "tokens": [370, 300, 311, 472, 4171, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.07883615258299274, "compression_ratio": 1.781437125748503, "no_speech_prob": 5.809276626678184e-05}, {"id": 1873, "seek": 799896, "start": 8023.96, "end": 8025.96, "text": " The other piece is we're thinking about it from the bottom up,", "tokens": [440, 661, 2522, 307, 321, 434, 1953, 466, 309, 490, 264, 2767, 493, 11], "temperature": 0.0, "avg_logprob": -0.07883615258299274, "compression_ratio": 1.781437125748503, "no_speech_prob": 5.809276626678184e-05}, {"id": 1874, "seek": 802596, "start": 8025.96, "end": 8029.96, "text": " including the compiler bits, all the systems integration pieces,", "tokens": [3009, 264, 31958, 9239, 11, 439, 264, 3652, 10980, 3755, 11], "temperature": 0.0, "avg_logprob": -0.07953506946563721, "compression_ratio": 1.776, "no_speech_prob": 4.82972536701709e-05}, {"id": 1875, "seek": 802596, "start": 8029.96, "end": 8031.96, "text": " the application integration pieces,", "tokens": [264, 3861, 10980, 3755, 11], "temperature": 0.0, "avg_logprob": -0.07953506946563721, "compression_ratio": 1.776, "no_speech_prob": 4.82972536701709e-05}, {"id": 1876, "seek": 802596, "start": 8031.96, "end": 8035.96, "text": " and I have a theory that once we get past the world of Python here,", "tokens": [293, 286, 362, 257, 5261, 300, 1564, 321, 483, 1791, 264, 1002, 295, 15329, 510, 11], "temperature": 0.0, "avg_logprob": -0.07953506946563721, "compression_ratio": 1.776, "no_speech_prob": 4.82972536701709e-05}, {"id": 1877, "seek": 802596, "start": 8035.96, "end": 8038.96, "text": " that people are going to start doing a lot of really interesting things", "tokens": [300, 561, 366, 516, 281, 722, 884, 257, 688, 295, 534, 1880, 721], "temperature": 0.0, "avg_logprob": -0.07953506946563721, "compression_ratio": 1.776, "no_speech_prob": 4.82972536701709e-05}, {"id": 1878, "seek": 802596, "start": 8038.96, "end": 8042.96, "text": " where you integrate deep learning into applications,", "tokens": [689, 291, 13365, 2452, 2539, 666, 5821, 11], "temperature": 0.0, "avg_logprob": -0.07953506946563721, "compression_ratio": 1.776, "no_speech_prob": 4.82972536701709e-05}, {"id": 1879, "seek": 802596, "start": 8042.96, "end": 8047.96, "text": " and right now the application world and the ML world are different.", "tokens": [293, 558, 586, 264, 3861, 1002, 293, 264, 21601, 1002, 366, 819, 13], "temperature": 0.0, "avg_logprob": -0.07953506946563721, "compression_ratio": 1.776, "no_speech_prob": 4.82972536701709e-05}, {"id": 1880, "seek": 802596, "start": 8047.96, "end": 8053.96, "text": " I mean, people literally export their model into an ONNX or TF serving or whatever", "tokens": [286, 914, 11, 561, 3736, 10725, 641, 2316, 666, 364, 9299, 45, 55, 420, 40964, 8148, 420, 2035], "temperature": 0.0, "avg_logprob": -0.07953506946563721, "compression_ratio": 1.776, "no_speech_prob": 4.82972536701709e-05}, {"id": 1881, "seek": 805396, "start": 8053.96, "end": 8056.96, "text": " and dump it into some C++ thing where it's a whole new world.", "tokens": [293, 11430, 309, 666, 512, 383, 25472, 551, 689, 309, 311, 257, 1379, 777, 1002, 13], "temperature": 0.0, "avg_logprob": -0.059912325843932135, "compression_ratio": 1.778169014084507, "no_speech_prob": 2.7513786335475743e-05}, {"id": 1882, "seek": 805396, "start": 8056.96, "end": 8060.96, "text": " It's a completely different world, and so now you have this barrier", "tokens": [467, 311, 257, 2584, 819, 1002, 11, 293, 370, 586, 291, 362, 341, 13357], "temperature": 0.0, "avg_logprob": -0.059912325843932135, "compression_ratio": 1.778169014084507, "no_speech_prob": 2.7513786335475743e-05}, {"id": 1883, "seek": 805396, "start": 8060.96, "end": 8064.96, "text": " between the training, the learning, and the ML pieces,", "tokens": [1296, 264, 3097, 11, 264, 2539, 11, 293, 264, 21601, 3755, 11], "temperature": 0.0, "avg_logprob": -0.059912325843932135, "compression_ratio": 1.778169014084507, "no_speech_prob": 2.7513786335475743e-05}, {"id": 1884, "seek": 805396, "start": 8064.96, "end": 8066.96, "text": " and you have the application pieces,", "tokens": [293, 291, 362, 264, 3861, 3755, 11], "temperature": 0.0, "avg_logprob": -0.059912325843932135, "compression_ratio": 1.778169014084507, "no_speech_prob": 2.7513786335475743e-05}, {"id": 1885, "seek": 805396, "start": 8066.96, "end": 8068.96, "text": " and often these are different teams or different people", "tokens": [293, 2049, 613, 366, 819, 5491, 420, 819, 561], "temperature": 0.0, "avg_logprob": -0.059912325843932135, "compression_ratio": 1.778169014084507, "no_speech_prob": 2.7513786335475743e-05}, {"id": 1886, "seek": 805396, "start": 8068.96, "end": 8070.96, "text": " thinking about things in different ways,", "tokens": [1953, 466, 721, 294, 819, 2098, 11], "temperature": 0.0, "avg_logprob": -0.059912325843932135, "compression_ratio": 1.778169014084507, "no_speech_prob": 2.7513786335475743e-05}, {"id": 1887, "seek": 805396, "start": 8070.96, "end": 8072.96, "text": " and breaking down those kinds of barriers, I think,", "tokens": [293, 7697, 760, 729, 3685, 295, 13565, 11, 286, 519, 11], "temperature": 0.0, "avg_logprob": -0.059912325843932135, "compression_ratio": 1.778169014084507, "no_speech_prob": 2.7513786335475743e-05}, {"id": 1888, "seek": 805396, "start": 8072.96, "end": 8076.96, "text": " is a really big opportunity that enables new kinds of work to be done.", "tokens": [307, 257, 534, 955, 2650, 300, 17077, 777, 3685, 295, 589, 281, 312, 1096, 13], "temperature": 0.0, "avg_logprob": -0.059912325843932135, "compression_ratio": 1.778169014084507, "no_speech_prob": 2.7513786335475743e-05}, {"id": 1889, "seek": 805396, "start": 8076.96, "end": 8077.96, "text": " Very powerful.", "tokens": [4372, 4005, 13], "temperature": 0.0, "avg_logprob": -0.059912325843932135, "compression_ratio": 1.778169014084507, "no_speech_prob": 2.7513786335475743e-05}, {"id": 1890, "seek": 805396, "start": 8077.96, "end": 8081.96, "text": " That leads well into the next pair of questions.", "tokens": [663, 6689, 731, 666, 264, 958, 6119, 295, 1651, 13], "temperature": 0.0, "avg_logprob": -0.059912325843932135, "compression_ratio": 1.778169014084507, "no_speech_prob": 2.7513786335475743e-05}, {"id": 1891, "seek": 808196, "start": 8081.96, "end": 8084.96, "text": " Does it make sense to spend efforts learning and writing in Swift only,", "tokens": [4402, 309, 652, 2020, 281, 3496, 6484, 2539, 293, 3579, 294, 25539, 787, 11], "temperature": 0.0, "avg_logprob": -0.04158920422196388, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.00013720848073717207}, {"id": 1892, "seek": 808196, "start": 8084.96, "end": 8088.96, "text": " or is it worth to have some understanding of C++ as well", "tokens": [420, 307, 309, 3163, 281, 362, 512, 3701, 295, 383, 25472, 382, 731], "temperature": 0.0, "avg_logprob": -0.04158920422196388, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.00013720848073717207}, {"id": 1893, "seek": 808196, "start": 8088.96, "end": 8090.96, "text": " to be good in numerical computations?", "tokens": [281, 312, 665, 294, 29054, 2807, 763, 30], "temperature": 0.0, "avg_logprob": -0.04158920422196388, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.00013720848073717207}, {"id": 1894, "seek": 808196, "start": 8090.96, "end": 8093.96, "text": " And then secondly, after going through some of the Swift documentations,", "tokens": [400, 550, 26246, 11, 934, 516, 807, 512, 295, 264, 25539, 4166, 763, 11], "temperature": 0.0, "avg_logprob": -0.04158920422196388, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.00013720848073717207}, {"id": 1895, "seek": 808196, "start": 8093.96, "end": 8096.96, "text": " it seems like it's a very versatile language.", "tokens": [309, 2544, 411, 309, 311, 257, 588, 25057, 2856, 13], "temperature": 0.0, "avg_logprob": -0.04158920422196388, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.00013720848073717207}, {"id": 1896, "seek": 808196, "start": 8096.96, "end": 8099.96, "text": " If I understand correctly, deep learning, robotics, web development,", "tokens": [759, 286, 1223, 8944, 11, 2452, 2539, 11, 34145, 11, 3670, 3250, 11], "temperature": 0.0, "avg_logprob": -0.04158920422196388, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.00013720848073717207}, {"id": 1897, "seek": 808196, "start": 8099.96, "end": 8102.96, "text": " and systems programming all seem well under its purview.", "tokens": [293, 3652, 9410, 439, 1643, 731, 833, 1080, 1864, 1759, 13], "temperature": 0.0, "avg_logprob": -0.04158920422196388, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.00013720848073717207}, {"id": 1898, "seek": 808196, "start": 8102.96, "end": 8106.96, "text": " Do you foresee Swift's influence flourishing in all these separate areas", "tokens": [1144, 291, 38736, 25539, 311, 6503, 7693, 3807, 294, 439, 613, 4994, 3179], "temperature": 0.0, "avg_logprob": -0.04158920422196388, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.00013720848073717207}, {"id": 1899, "seek": 808196, "start": 8106.96, "end": 8110.96, "text": " and allowing for a tighter and more fluid development between disciplines?", "tokens": [293, 8293, 337, 257, 30443, 293, 544, 9113, 3250, 1296, 21919, 30], "temperature": 0.0, "avg_logprob": -0.04158920422196388, "compression_ratio": 1.688821752265861, "no_speech_prob": 0.00013720848073717207}, {"id": 1900, "seek": 811096, "start": 8110.96, "end": 8113.96, "text": " Sure. So I think these are two sides of the same coin.", "tokens": [4894, 13, 407, 286, 519, 613, 366, 732, 4881, 295, 264, 912, 11464, 13], "temperature": 0.0, "avg_logprob": -0.07183755238850911, "compression_ratio": 1.6053639846743295, "no_speech_prob": 0.00014053625636734068}, {"id": 1901, "seek": 811096, "start": 8113.96, "end": 8115.96, "text": " I totally agree with Jeremy.", "tokens": [286, 3879, 3986, 365, 17809, 13], "temperature": 0.0, "avg_logprob": -0.07183755238850911, "compression_ratio": 1.6053639846743295, "no_speech_prob": 0.00014053625636734068}, {"id": 1902, "seek": 811096, "start": 8115.96, "end": 8118.96, "text": " Learning new programming languages is good,", "tokens": [15205, 777, 9410, 8650, 307, 665, 11], "temperature": 0.0, "avg_logprob": -0.07183755238850911, "compression_ratio": 1.6053639846743295, "no_speech_prob": 0.00014053625636734068}, {"id": 1903, "seek": 811096, "start": 8118.96, "end": 8122.96, "text": " just because often you learn to think about things in a new way,", "tokens": [445, 570, 2049, 291, 1466, 281, 519, 466, 721, 294, 257, 777, 636, 11], "temperature": 0.0, "avg_logprob": -0.07183755238850911, "compression_ratio": 1.6053639846743295, "no_speech_prob": 0.00014053625636734068}, {"id": 1904, "seek": 811096, "start": 8122.96, "end": 8125.96, "text": " or they open up new kinds of approaches,", "tokens": [420, 436, 1269, 493, 777, 3685, 295, 11587, 11], "temperature": 0.0, "avg_logprob": -0.07183755238850911, "compression_ratio": 1.6053639846743295, "no_speech_prob": 0.00014053625636734068}, {"id": 1905, "seek": 811096, "start": 8125.96, "end": 8128.96, "text": " and having more different kinds of mental frameworks", "tokens": [293, 1419, 544, 819, 3685, 295, 4973, 29834], "temperature": 0.0, "avg_logprob": -0.07183755238850911, "compression_ratio": 1.6053639846743295, "no_speech_prob": 0.00014053625636734068}, {"id": 1906, "seek": 811096, "start": 8128.96, "end": 8132.96, "text": " gives you the ability to solve problems that otherwise you might not be able to do.", "tokens": [2709, 291, 264, 3485, 281, 5039, 2740, 300, 5911, 291, 1062, 406, 312, 1075, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.07183755238850911, "compression_ratio": 1.6053639846743295, "no_speech_prob": 0.00014053625636734068}, {"id": 1907, "seek": 811096, "start": 8132.96, "end": 8136.96, "text": " So learning C++ in the abstract is a good thing.", "tokens": [407, 2539, 383, 25472, 294, 264, 12649, 307, 257, 665, 551, 13], "temperature": 0.0, "avg_logprob": -0.07183755238850911, "compression_ratio": 1.6053639846743295, "no_speech_prob": 0.00014053625636734068}, {"id": 1908, "seek": 813696, "start": 8136.96, "end": 8142.96, "text": " Having to use C++ is a little bit of a different thing, in my opinion.", "tokens": [10222, 281, 764, 383, 25472, 307, 257, 707, 857, 295, 257, 819, 551, 11, 294, 452, 4800, 13], "temperature": 0.0, "avg_logprob": -0.06454048305749893, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.00042128420318476856}, {"id": 1909, "seek": 813696, "start": 8142.96, "end": 8144.96, "text": " And so C++ has lots of drawbacks.", "tokens": [400, 370, 383, 25472, 575, 3195, 295, 2642, 17758, 13], "temperature": 0.0, "avg_logprob": -0.06454048305749893, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.00042128420318476856}, {"id": 1910, "seek": 813696, "start": 8144.96, "end": 8147.96, "text": " This is coming from somebody who's written a C++ compiler.", "tokens": [639, 307, 1348, 490, 2618, 567, 311, 3720, 257, 383, 25472, 31958, 13], "temperature": 0.0, "avg_logprob": -0.06454048305749893, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.00042128420318476856}, {"id": 1911, "seek": 813696, "start": 8147.96, "end": 8149.96, "text": " Yeah, I've written way too much C++ myself,", "tokens": [865, 11, 286, 600, 3720, 636, 886, 709, 383, 25472, 2059, 11], "temperature": 0.0, "avg_logprob": -0.06454048305749893, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.00042128420318476856}, {"id": 1912, "seek": 813696, "start": 8149.96, "end": 8151.96, "text": " and maybe I'm a little bit damaged here,", "tokens": [293, 1310, 286, 478, 257, 707, 857, 14080, 510, 11], "temperature": 0.0, "avg_logprob": -0.06454048305749893, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.00042128420318476856}, {"id": 1913, "seek": 813696, "start": 8151.96, "end": 8154.96, "text": " but C++ is a super complicated language.", "tokens": [457, 383, 25472, 307, 257, 1687, 6179, 2856, 13], "temperature": 0.0, "avg_logprob": -0.06454048305749893, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.00042128420318476856}, {"id": 1914, "seek": 813696, "start": 8154.96, "end": 8158.96, "text": " It's also full of memory safety problems and security vulnerabilities", "tokens": [467, 311, 611, 1577, 295, 4675, 4514, 2740, 293, 3825, 37633], "temperature": 0.0, "avg_logprob": -0.06454048305749893, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.00042128420318476856}, {"id": 1915, "seek": 813696, "start": 8158.96, "end": 8161.96, "text": " and a lot of other things that are pretty well known.", "tokens": [293, 257, 688, 295, 661, 721, 300, 366, 1238, 731, 2570, 13], "temperature": 0.0, "avg_logprob": -0.06454048305749893, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.00042128420318476856}, {"id": 1916, "seek": 813696, "start": 8161.96, "end": 8164.96, "text": " It's a great language, supports tons of really important work,", "tokens": [467, 311, 257, 869, 2856, 11, 9346, 9131, 295, 534, 1021, 589, 11], "temperature": 0.0, "avg_logprob": -0.06454048305749893, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.00042128420318476856}, {"id": 1917, "seek": 816496, "start": 8164.96, "end": 8167.96, "text": " but one of the goals with Swift is to be a full-stack language", "tokens": [457, 472, 295, 264, 5493, 365, 25539, 307, 281, 312, 257, 1577, 12, 372, 501, 2856], "temperature": 0.0, "avg_logprob": -0.08493287746722882, "compression_ratio": 1.6388888888888888, "no_speech_prob": 7.098163769114763e-05}, {"id": 1918, "seek": 816496, "start": 8167.96, "end": 8172.96, "text": " and really span from the scripting all the way down to the things C++ is good at,", "tokens": [293, 534, 16174, 490, 264, 5755, 278, 439, 264, 636, 760, 281, 264, 721, 383, 25472, 307, 665, 412, 11], "temperature": 0.0, "avg_logprob": -0.08493287746722882, "compression_ratio": 1.6388888888888888, "no_speech_prob": 7.098163769114763e-05}, {"id": 1919, "seek": 816496, "start": 8172.96, "end": 8176.96, "text": " and getting C++ level performance in the same language", "tokens": [293, 1242, 383, 25472, 1496, 3389, 294, 264, 912, 2856], "temperature": 0.0, "avg_logprob": -0.08493287746722882, "compression_ratio": 1.6388888888888888, "no_speech_prob": 7.098163769114763e-05}, {"id": 1920, "seek": 816496, "start": 8176.96, "end": 8181.96, "text": " that you can do high-level machine learning frameworks in is pretty cool.", "tokens": [300, 291, 393, 360, 1090, 12, 12418, 3479, 2539, 29834, 294, 307, 1238, 1627, 13], "temperature": 0.0, "avg_logprob": -0.08493287746722882, "compression_ratio": 1.6388888888888888, "no_speech_prob": 7.098163769114763e-05}, {"id": 1921, "seek": 816496, "start": 8181.96, "end": 8185.96, "text": " And so I think that that's one of the really unique aspects of Swift,", "tokens": [400, 370, 286, 519, 300, 300, 311, 472, 295, 264, 534, 3845, 7270, 295, 25539, 11], "temperature": 0.0, "avg_logprob": -0.08493287746722882, "compression_ratio": 1.6388888888888888, "no_speech_prob": 7.098163769114763e-05}, {"id": 1922, "seek": 816496, "start": 8185.96, "end": 8190.96, "text": " is it was designed for compilation, for usability, for accessibility,", "tokens": [307, 309, 390, 4761, 337, 40261, 11, 337, 46878, 11, 337, 15002, 11], "temperature": 0.0, "avg_logprob": -0.08493287746722882, "compression_ratio": 1.6388888888888888, "no_speech_prob": 7.098163769114763e-05}, {"id": 1923, "seek": 819096, "start": 8190.96, "end": 8194.96, "text": " and I'm not aware of a system that's similar in that way.", "tokens": [293, 286, 478, 406, 3650, 295, 257, 1185, 300, 311, 2531, 294, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.12974808594890844, "compression_ratio": 1.497584541062802, "no_speech_prob": 8.323362999362871e-05}, {"id": 1924, "seek": 819096, "start": 8194.96, "end": 8196.96, "text": " Great. I'm really looking forward to it.", "tokens": [3769, 13, 286, 478, 534, 1237, 2128, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.12974808594890844, "compression_ratio": 1.497584541062802, "no_speech_prob": 8.323362999362871e-05}, {"id": 1925, "seek": 819096, "start": 8196.96, "end": 8197.96, "text": " Can we ask one more question?", "tokens": [1664, 321, 1029, 472, 544, 1168, 30], "temperature": 0.0, "avg_logprob": -0.12974808594890844, "compression_ratio": 1.497584541062802, "no_speech_prob": 8.323362999362871e-05}, {"id": 1926, "seek": 819096, "start": 8197.96, "end": 8198.96, "text": " I think we're out of time.", "tokens": [286, 519, 321, 434, 484, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.12974808594890844, "compression_ratio": 1.497584541062802, "no_speech_prob": 8.323362999362871e-05}, {"id": 1927, "seek": 819096, "start": 8198.96, "end": 8199.96, "text": " Oh, OK.", "tokens": [876, 11, 2264, 13], "temperature": 0.0, "avg_logprob": -0.12974808594890844, "compression_ratio": 1.497584541062802, "no_speech_prob": 8.323362999362871e-05}, {"id": 1928, "seek": 819096, "start": 8199.96, "end": 8200.96, "text": " Sorry.", "tokens": [4919, 13], "temperature": 0.0, "avg_logprob": -0.12974808594890844, "compression_ratio": 1.497584541062802, "no_speech_prob": 8.323362999362871e-05}, {"id": 1929, "seek": 819096, "start": 8200.96, "end": 8201.96, "text": " So, yeah.", "tokens": [407, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.12974808594890844, "compression_ratio": 1.497584541062802, "no_speech_prob": 8.323362999362871e-05}, {"id": 1930, "seek": 819096, "start": 8201.96, "end": 8202.96, "text": " We'll come back next time.", "tokens": [492, 603, 808, 646, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.12974808594890844, "compression_ratio": 1.497584541062802, "no_speech_prob": 8.323362999362871e-05}, {"id": 1931, "seek": 819096, "start": 8202.96, "end": 8203.96, "text": " One more question next time.", "tokens": [1485, 544, 1168, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.12974808594890844, "compression_ratio": 1.497584541062802, "no_speech_prob": 8.323362999362871e-05}, {"id": 1932, "seek": 819096, "start": 8203.96, "end": 8204.96, "text": " Thanks, everybody.", "tokens": [2561, 11, 2201, 13], "temperature": 0.0, "avg_logprob": -0.12974808594890844, "compression_ratio": 1.497584541062802, "no_speech_prob": 8.323362999362871e-05}, {"id": 1933, "seek": 819096, "start": 8204.96, "end": 8206.96, "text": " Thank you, Chris Lattner, and we'll see you next week.", "tokens": [1044, 291, 11, 6688, 441, 1591, 1193, 11, 293, 321, 603, 536, 291, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.12974808594890844, "compression_ratio": 1.497584541062802, "no_speech_prob": 8.323362999362871e-05}, {"id": 1934, "seek": 820696, "start": 8206.96, "end": 8221.96, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 51114], "temperature": 0.0, "avg_logprob": -0.6318384408950806, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.001954068196937442}], "language": "en"}