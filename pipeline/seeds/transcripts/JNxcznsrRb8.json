{"text": " Okay, so welcome back to deep learning lesson two Last week we Got to the point where we had successfully trained a pretty accurate image classifier and So just to remind you about how we did that Can you guys see okay, I think actually we can't turn the front lights off in there. Can you guys all see the screen? Okay? We can turn just these ones can we? Some pictures all into darkness, but if that works then Is that okay that's better isn't it yeah Do you mind doing the other two? And maybe that one as well Oh, not that one. Oh, that's great. Sorry that know you're right. Okay, great. That's better isn't it? So just to remind you the way that we built this image classifier was we used a small amount of code basically three lines of code And these three lines of code pointed at a particular path Which already had some data in it and so the key thing for this to know how to train this model Was that this path which was data dogs cats? had to have a particular structure, which is that it had a Train folder and a valid folder and in each of those train invalid folders There was a cat's folder and the dogs folder and each of the cats and the dogs folders was a bunch of images of cats and dogs So this is like a pretty standard. It's one of two Main structures that are used to say here is the data that I want you to train an image model from so I know some Of you during the week went away and tried different data sets where you had folders with different sets of images in and created your own image classifiers And generally that seems to be working pretty well from what I could see on the forums so to make it clear at this point this is everything you need to to get started so if you Create your own folders with different sets of images. You know a few hundred Or a few thousand at each folder And run the same three lines of code that'll give you An image classifier, and you'll be able to see this third column tells you how accurate is so we looked at Some kind of simple visualizations to see like What was it uncertain about? What was it wrong about and so forth and that's always a really good idea and Then we learned about the the one key number you have to pick so this is this number here is the one key number is 0.01 and this is called the learning rate and so I wanted to go over this again And we'll learn about the theory behind what this is During the rest of the course in quite a lot of detail, but for now. I just wanted to talk about the practice Yes, you're next Oh They cannot see you in the video I guess and now I just turned it around Also was wondering could you tell us about the older three numbers in that? We check these three here. We're going to talk about the other other ones shortly So the main one we're going to look at for now is is the last column which is the accuracy The first column as you can see is the epoch number so this tells us how many times has it been through the entire data set? Trying to learn a better classifier, and then the next two columns is what's called the loss which we'll be learning about Either later today or next week the first one is the loss on the training set These are the images that we're looking at in order to try to make a better classifier And the second is the loss on the validation set these are the images that we're not looking at when we're training But we're just sitting on the side to see how accurate we are so we'll learn about the history and loss and accuracy later Okay, so So we've got the epoch number the training loss is the second column the Validation loss is the third column and the accuracy is the fourth column? Okay, so the basic idea of the learning rate So the basic idea of the learning rate is it's the thing that's going to decide how quickly do we zoom do we kind of? Hone in on the solution and so I find that a good way to think about this is to think about like well What if we were trying to? fit to a function That looks something like this right we're trying to say okay. Where's whereabouts is the minimum point? This is basically what we do when we do deep learning as we try to find the minimum point of a function Now our function happens to have Millions or hundreds of millions of parameters, but it works the same basic way and so when we look at it You know we can immediately see that the lowest point is here But how would you do that if you were a computer algorithm? And what we do is we we start out at some point at random So we pick say here, and we have a look and we say okay What's the what's the loss or the error at this point and we say what's the gradient in other words? Which way is up and which way is down and it tells us that down is going to be in that direction And it also tells us how fast is it going down? Which is at this point is going down pretty quickly And so then we take a step in the direction that's down and the distance we travel is going to be Proportional to the gradient is going to be important how steep it is the idea is if it's deeper Then we're probably further away That's the general idea right and so specifically what we do is we take the gradient Which is how steep is it at this point and we multiply it by some number and that numbers called the learning rate Okay, so if we pick a number that is Very small then we're guaranteed that we're going to go a little bit closer and a little bit closer and a little bit closer Each time right, but it's going to take us a very long time To eventually get to the bottom if we pick a number that's very big We could actually step too far could go in the right direction, but we could step all the way over to here Right as a result of which we end up further away than we started and we could oscillate It gets worse and worse So if you start training a neural net and you find that your accuracy or your loss is like Spitting off into infinity almost certainly your learning rates too high so in a sense learning rate too low is Is is a better problem to have because you're going to have to wait a long time But wouldn't it be nice if there was a way to figure out like? What's the best learning rate something where you could kind of go quickly go like bomb bomb bomb? right and so that's why we use this thing called a learning rate finder and What the learning rate finder does is it tries each each time? It looks at another remember the term mini batch and mini batch is a few images that we look at each time So that we're using the parallel processing power of the GPU effectively we look generally at around 64 or 128 images at a time for each mini batch which is labeled here as an iteration We gradually increase the learning rate that multiplicatively increase the learning rate We start at really really tiny learning rates to make sure that we don't start at something too high And we gradually increase it and so the idea is that That eventually the learning rate will be so big that the loss will start getting worse and so what we're going to do then is we're going to look at the plot of learning rate against loss right so when the learning rates tiny It increases slowly then it starts to increase a bit faster And then eventually it starts not increasing as quickly and in fact it starts getting worse Right so clearly here and make sure you're you want to be familiar with this scientific notation okay, so 10 to the negative 1 is 0.1 10 to the 0 is 1 10 to the negative 2 is 0.01 and when we write this in Python we'll generally write it like this rather than writing 10 to the negative 1 or 10 to the negative 2 We'll just write 1 a Meg 1 or 1 a Meg 2 okay, I mean the same thing you're going to see that all the time so and Remember that equals 0.1. Oh point oh one Right so Don't be confused by this text that it prints out here this this loss here is The the final loss at the very end of it's not of any interest right so ignore this this is only interesting When we're doing regular training, but not interesting for the learning rate finder the thing that's interesting for the learning rate finder is this learn dot shed dot plot and Specifically we're not looking for the point where it's the lowest back to the point where it's the lowest It's actually not getting better anymore, so that's too high a learning rate So I generally look to see like where is it the lowest and then I go back like one of magnitude so One e neg two would be a pretty good choice Yeah, okay, so that's why you saw when we ran our Fit here we picked point oh one which is one a neg two So an important point to make here is like this. This is the one key number that we've learned to adjust and If you just adjust this number and nothing else most of the time you're going to be able to get pretty good results And this is like a very different message to what you would hear or see in any textbook or any video or any course because Up until now there's been like dozens and dozens of these they're called hyper parameters dozens and dozens of hyper parameters to set and They've been thought of as highly sensitive and difficult to set so inside the fast AI library We kind of do all that stuff for you As much as we can and during the course we're going to learn that there are some more we can tweak to get slightly better results but it's kind of like It's kind of in a funny situation here because for those of you that haven't done any deep learning before it's kind of like oh this is That's all there is to it. This is very easy and then when you talk to people outside this class They'll be like deep learning so difficult. There's so much to say it's a real art form and so that's why there's this Difference right and so that the truth is that the learning rate really is the key thing to set and this ability to Use this trick to figure out how to set it although the paper is now Probably 18 months old almost nobody knows about this paper And it was from a guy who's not from a famous research lab so most people kind of ignored it and in fact even this Particular technique was one sub part of a paper that was about something else So again this idea of like this is how you can set the learning rate Really nobody outside this classroom just about knows about it obviously the guy who wrote it Leslie Smith knows about it So it's a good thing to tell your colleagues about this like here is actually a great way to set the learning rate and There's even been papers called like one of the famous papers is called no more pesky learning rates Which actually is a less effective technique than this one But this idea that like setting learning rates is is very difficult and fiddly is has been true for most of the kind of deep learning history So here's the trick right go look at this plot I'll find kind of the lowest to go back about a multiple of 10 and try that Right and if that doesn't quite work you can always try you know going back another multiple of 10 But this has always worked for me so far What's Why does this learning rate this method work versus something else like momentum base or what's like the advantages? It is advantageous with this learning rate like technique versus scales That's a great question so We're going to learn during this course about a number of ways of improving gradient descent like you mentioned momentum and Adam and so forth This is orthogonal in fact so one of the things the fast AI library tries to do is figure out the right gradient descent version and in fact behind the scenes this is actually using something called Adam and So this technique is telling us. This is the best learning rate to use given What are the other tweaks you're using in this case the atom optimizer? So it's not that there's some compromise between this and some other approaches this sits on top of those approaches And you still have to set the learning rate when you use with other approaches So we're trying to find the best kind of optimizer to use for a problem But you still have to set the learning rate, and this is how we can do it and in fact this idea of using this technique on top of More advanced optimizers like Adam might haven't even seen mentioned in a paper before so I think this is like a it's not a Huge breakthrough it seems obvious, but nobody else seems to have tried it so as you can see it works well When we use optimizers like Adam which are like adaptive learning rates, so when we set this learning rate Is it like initial learning rate because it changes during the people? So We're going to be learning about things like Adam the details about it later in the class But the basic answer is no even with even the atom that there actually is a learning rate It's just being It's being basically divided by the the gradient The average previous gradient and also the recent summer squareds of gradients So there's still like a number called the learning rate there There isn't a even these so-called dynamic learning rate methods still have a learning rate Okay, so The most important Thing that you can do to make your model better Is to give it more data So the challenge that happens is that these models have hundreds of millions of parameters And if you train them for a while they start to do what's called overfitting And so overfitting means that they're going to start to see like the specific details of the images you're giving them rather than the more general Learning that can transfer across to the validation center So the best thing we can do to avoid overfitting is to find more data now Obviously one way to do that would just be to collect more data from wherever you're getting it from or label more data But a really easy way that we should always do is to use something called data augmentation so Data augmentation is one of these things that's In many courses it's not even mentioned at all or if it is it's kind of like an advanced topic right at the end But actually it's like the most important thing that you can do to make a better model Okay, and so it's built into the fast AI library to make it very easy to do and so we're going to look at the details Of the code shortly, but the basic idea is that at this in our initial code we Had a line that said image classifier data from paths, and we passed in the path to our data and for transforms We passed in basically The size in the architecture we'll look at this in more detail shortly We just add one more parameter which is what kind of data augmentation do you want to do? and so To understand data augmentation. It's maybe easiest to look at some pictures of data augmentation So what I've done here again. We'll look at the code in more detail later, but the basic idea is I've I've run I've built a Data class like multiple times. I'm going to do it six times and each time. I'm going to plot the same cat and You can see that what happens is that this cat here is further over to the left This one here is further over to the right and this one here is flipped horizontally and so forth so data augmentation Different types of image you're going to want different types of data augmentation right so for example if you were trying to recognize Letters and digits you wouldn't want to flip horizontally because like it's actually has a different meaning Whereas on the other hand if you're looking at Photos of cats and dogs you probably don't want to flip vertically because cats aren't generally upside down Right where else if you're looking at there's a current Kaggle competition which is recognizing icebergs in satellite images You probably do want to flip them upside down because it doesn't really matter which way around the iceberg or the satellite was right so One of the examples of the transform sets we have is transforms side on So in other words if you have photos that are like generally taken from the side Which generally means you want to be able to flip them horizontally but not vertically this is going to give you all the Transforms you need for that so it'll flip them sideways rotate them by small amounts, but not too much and slightly bury their contrast and brightness And slightly zoom in and out a little bit and move them around a little bit so each time. It's a slightly different I fight with an edge Getting a couple of questions from people About could you explain again the reason why you don't take the minimum of the loss curve? Yeah? But it's like the higher rate so yeah, and also could you people will understand if This works for every CNN for CNN's for every unit on it this being the learning rate finder. Yeah exactly yeah Okay, great Could you? Put your hand up if there's a spare seat next to you So there was a question about the learning rate finder about why do we use the learning rate? That's less than the lowest point and so the reason why is to understand what's going on with this learning rate finder So let's go back to our picture here Like how do we figure out? What learning rate to use right and so what we're going to do is we're going to take Steps and each time we're going to double The learning rate so kind of double the amount by which we're multiplying the greater gradient so in other words We'd go tiny step slightly bigger slightly bigger slightly bigger slightly bigger slightly bigger slightly bigger Okay, and so the question is the purpose of this is not to find the minimum The purpose of this is to figure out what learning rate is allowing us to decrease quickly right? So the point at which the loss was lowest here is actually there right but that learning rate actually looks like it's probably too High it's going to just jump like probably backwards and forwards Okay, so instead what we do is we go back to the point where the learning rates quickly giving us a quick increase in the loss So here is So here is the actual learning rate increasing every single time we look at a new mini-batch, right? So mini-batch or iteration versus learning rate and then here is learning rate versus loss So here's that point at the bottom where it was now already too high Okay, and so here's the point where we go back a little bit and it's increasing nice and quickly We're going to learn about something called Stochastic gradient descent with restarts shortly where we're going to see like in a sense You might want to go back to 1e neg 3 where it's actually even steeper still and maybe we would actually find this will actually Learn even quicker you could try it But we're going to see later why actually using a higher number is going to give us a better generalization So for now, let's put that aside Do you mean higher learning rate when you say higher do I mean higher learning rate when I say higher? Yeah I mean higher learning rate so as we increase the iterations in the learning rate finder the learning rate is going up This is iterations versus learning rate Okay, so as we do that as the learning rate increases and we plot it here the loss goes down Until we get to the point where the learning rate is too high And at that point the loss is now getting worse because I asked the question because you were just indicating that you know Even though the minimum was that 10 to the minus 1 You were going to you suggest that we should choose 10 to the minus 2 But now you're saying that maybe we should go back the other way higher, so I didn't mean to say that I'm sorry if I said Something backwards, so I want to go back down to the lower learning rate So possibly I said a higher when I meant higher in that's lower Lower learning rate okay, thanks. Yeah In the last class you said that the local all the local minima are the same and And this graph also shows the same is that is that something that was observed or is that logic theory behind it? That's not what this graph is showing This graph is simply showing that there's a point where if we increase the learning rate more Then it stops getting better, and it actually starts getting worse the idea that all local minima are the same is a totally separate issue and And it's actually something we'll see a picture of shortly, so let's come back to that Jeremy do we have to find the best learning rate every time we're going on? run on the poke every time we're Running on a poke and a pop so how many times should I run this like learning right fine in my training? That's a great question unit um I I certainly run it once when I start Later on in this class we're going to learn about unfreezing layers And after I unfreeze layers I sometimes run it again if I do something to like change the thing I'm training or change the way. I'm training it you may want to run it again basically Or you know if you particularly if you've changed something about how you train like unfreezing layers Which we're going to soon learn about and you're finding now the training is Unstable or too slow no again. You can run it again. There's never any harm in Running it it doesn't take very long That's great question Okay, so back to data augmentation So if we add to our when we run this little transforms from model Function we pass in augmentation transforms we can pass in the main to a Transform side on or transforms top-down later on we'll learn about creating your own custom transform lists as well But for now because we're taking pictures from the side of cats and dogs We'll say transform side on and now each time we look at an image It's going to be zoomed in or out a little bit moved around a little bit rotated a little bit Possibly flipped okay, and so what this does is it's not exactly creating new data But as far as the convolutional neural net is concerned It's it's a different way of looking at this thing and it actually therefore allows it to learn How to recognize cats or dogs from somewhat different angles right so when we do data augmentation We're basically trying to say based on our domain knowledge Here here are different ways that we can mess with this image that we know Still make it the same image. You know and that we could expect that you might actually see that kind of image in the real world So what we can do now is when we call this from paths function Which we'll learn more about shortly we can now pass in this set of transforms which actually have these augmentations in now So that's kind of we're going to start from scratch here. We do a fit and initially the Augmentations actually don't do anything and the reason initially they don't do anything is because we've got here something that says pre-compute Equals true, and we're going to come back to this lots of times But basically what this is doing is do you remember this picture we saw where we learned each different layer has these activations that basically look for you know anything from the middle of flowers to eyeballs of Birds or whatever right and so literally what happens is that? the the later layers of this convolutional neural network have these things called activations and Activation literally it's a number an activation is a number that says this feature like Eyeball of bird is in this location with this level of confidence with its probability right, and so we're going to see a lot of this later, but What we can do is we can say all right well in this We've got a pre-trained network remember and a pre-trained network is one where it's already learned to recognize certain things in this case It's learned to recognize the one and a half million images in the image net data set and so What we could do is we could take the the second last layer so the one which is like Got all of the information necessary to figure out what kind of thing a thing is and we can save those activations So basically saving things saying you know there's this level of eyeballness Here and this level of dog's faceness here and this level of fluffy ear there and so forth and so we save for every image these Activations and that we call them the pre computed activations And so the idea is now that when we want to create a new classifier Which can basically take advantage of these pre computed activations? We can just very quickly train. We'll learn all the details of this shortly We can very quickly train a simple linear model based on those and so that's what happens when we say pre compute equals true And that's why you may have noticed this week the first time that you run a model a new model It takes a minute or two Whereas you saw when I ran it took like five or ten seconds took you a minute or two and that's because it had to pre compute These activations and just has to do that once if you're using like your own computer Or AWS it just has to do it once ever If you're using Cressel it actually has to do it once every single time you rerun Cressel because Cressel uses a Just for these pre computed activations It uses a special little kind of scratch space that disappears each time you restart your Cressel instance So other than the special case of Cressel generally speak you just have to run it once ever for a data set Okay So the issue with that is that since we've pre computed for each image you know how much does it have an ear here, and how much does it have a Lizards eyeball there and so forth that means that data augmentations don't work right in other words even though We're trying to show it a different version of the cat each time we've pre computed the activations for a particular version of that cat So in order to use data augmentation. We just have to go learn dot pre compute equals false Okay, and then we can run a few more Epochs right and so you can see here that as we run more Epochs The accuracy isn't particularly getting better All right, that's the bad news the good news is that you can see the the train loss, right? This is like the a way of measuring the error of this model although. That's getting better the error is going down The validation error isn't going down But we're not overfitting and overfitting would mean that the training loss is much lower than the validation loss And we're going to talk about that a lot during this course But the general idea here is if you're doing a much better job on the training set Then you are on the validation set that means your models not generalize so we're not at that point Which is good, but we're not really improving So we're going to have to figure out how to deal with that Before we do I want to show you one other cool trick. I've added here Cycle length equals one And this is another really interesting idea Here's the basic idea Cycle length equals one enables a recent fairly recent discovery in deep learning called stochastic gradient descent with restarts and the basic idea is this as you as You get closer and closer as you get closer and closer to the right spot right I'm getting closer and closer. I may want to start to decrease my learning rate right because I get closer I'm kind of like oh, I'm pretty close now, so let's let's slow down my steps to try to get exactly to the right spot Right and so as we do more iterations Our learning rate Perhaps should actually go Down right because as we go along we're getting closer and closer to where we want to be and we want to like get exactly To the right spot okay So the idea of decreasing the learning rate as you train is called learning rate annealing and It's it's very very common very very popular Everybody uses it basically all the time The most common kind of learning rate annealing is Really horrendously hacky It's basically that researchers like pick a learning rate that seems to work for a while and then when it stops learning Well, they drop it down by about ten times, and then they keep learning a bit more until it doesn't seem to be improving And they drop it down by another ten times That's what most academic research papers and most people in industry do so this would be like stepwise annealing very manual very annoying a Better approach is simply to pick some kind of functional form like a line It turns out that a really good functional form is one half of a cosine curve Right and the reason why is that for a while when you're not very close you kind of have a really high learning rate And then as you do get close you kind of quickly drop down and do a few iterations with a really low Learning rate and so this is called cosine annealing So to those of you haven't done trigonometry for a while cosine basically looks Something like this right so we've picked one little half piece Okay So we're going to use cosine annealing But here's the thing when you're in a very High dimensional space right and here. We're only able to show three dimensions right but in reality We've got hundreds of millions of dimensions We've got lots of different Fairly flat points they may not be actual local minima But they're fairly flat points all of which are pretty good right but they might differ in a really interesting way Which is that some of those flat points? Let me show you Let's imagine we've got a surface that looks something like this you Right now imagine that we kind of random guess started here and Our initial therefore kind of learning rate annealing schedule got us down to here Now indeed that's a pretty nice low error right, but it probably doesn't generalize very well Which is to say if we use a different data set where things are just kind of slightly different in one of these directions Suddenly it's a terrible solution right where else over here It's basically equally good in terms of loss Right, but it rather suggests that if you move if you have slightly different data sets that are slightly moved in different directions It's still going to be good right so in other words we would expect this solution here is probably going to generalize better than the spiky one So here's what we do is we've got like a bunch of different low bits right then our standard Learning rate annealing approach will start to go downhill downhill downhill downhill downhill to one spot Right, but what we could do instead is use a learning rate schedule That looks like this Which is to say we do a cosine annealing and then suddenly jump up again and do a cosine annealing and then jump up again And so each time we jump up It means that if we're in a spiky bit And then we suddenly increase the learning rate and it jumps now all the way over to here and So then we kind of learning rate and near learning rate near that down to here And then we jump up again to a high learning rate. Oh And it stays here Right so in other words each time we jump up the learning rate That means that if it's in a nasty spiky part of the surface It's going to hop out of the spiky part and hopefully if we do that enough times. It'll eventually find a nice smooth bowl You Could you get the same effect by running multiple iterations through the different randomized starting points and eventually you explore all possible Yeah, so in fact that that's a great question and before this approach Which is called stochastic gradient descent with restarts was was created That's exactly what people used to do they used to create these things called ensembles where they would basically Relearn a whole new model ten times in the hope that one of them is like going to end up being better and so The cool thing about this stochastic gradient descent with restarts is that the model once we're in a reasonably good spot Each time we jump up the learning rate It doesn't restart it actually hangs out in this nice part part of the space and then keeps getting better So interestingly it turns out that this approach where we do this a bunch of separate cosine and the only steps we end up with a better result Than if we just randomly tried a few different starting points so it's a super neat trick and it's a Fairly recent development, but and again almost nobody's heard of it but I found like It's now like my superpower like using this along with the learning rate finder like I Can get better results than nearly anybody like in a Kaggle competition? You know in the first week or two I can like jump in spend an hour or two and and back I've got a fantastically good result And so this is why I didn't pick the point where it's got the steepest slope I actually tried to pick something kind of aggressively high It's still getting down But maybe like getting to the point where it's nearly too high and that's because I want to make because that's because when we do this Stochastic gradient descent with restarts this 10 to the negative 2 represents the Highest number that it uses so it goes up to 10 to the negative 2 and then goes down And then up to 10 to negative 2 and then down so if I use to lower learning rate It's not going to jump to a different part of the function So I have a few questions, but the first one is how many times do you change the learning rate in one epoch? We don't change the learning rate mode. Oh, sorry how many times do it? Okay, so in terms of this part here where it's going down we change the learning rate every single mini-match right and then the number of times we reset it is set by the Cycle length parameter and so one means reset it after every epoch so if I had two there it would reset it up to every two epochs and Interestingly this this point that when we do the learning rate annealing that we actually change it every single batch it turns out to be Really critical to making this work and and it's again It's very different to what nearly everybody in industry and academia has done before When you any chance could you explain pre compute it was true because it's still Very confused. Yeah, we're going to come back to that multiple times in this course So the way this course is going to work is we're going to like do a really high level Version of each thing and then we're going to like come back to it in two or three lessons and then come back to it At the end of the course and each time we're going to see like more of the math more of the code and get a deeper View okay, and we can talk about it also on the forums during the week Our main goal is to generalize and we don't want to get those like narrow Optimus yeah, that's a very good summary this method are we keeping track of the minima's and averaging them? Something that's that's another Level of sophistication and indeed you can see there's something here called snapshot ensemble, so we're not doing it in the code right now But yes, if you wanted to make this generalize even better You can save the weights here and here and here and then take the average of the conditions But for now, we're just going to pick the last one If you want to skip ahead If you want to skip ahead there's a parameter called cycle save name Which you can add as well as cycle then and that will save a set of weights at the end of every Learning rate cycle and then you can ensemble them Okay So we've got a Pretty decent model here 99.3 percent accuracy And we've gone through you know a few steps that have taken you know a minute or two to run And so from time to time I tend to save my weight So if you go learn dot save and then pass in a file name It's going to go ahead and save that for you later on if you go learn dot load You'll be straight back to where you came from okay, so it's good idea to do that from time to time This is a good time to mention What happens when you do this? When you go learn dot save when you create pre computed activations another thing We'll learn about soon when you create resized images these are all creating various temporary files, okay? and so what happens is If we go to data and we go to Dogs cats this is my data folder, and you'll see there's a folder here called TMP or the tomb and And so this is automatically created and all of my pre computed activations end up in here I mentioned this because if if things aren't if you're getting weird errors it might be because you've got some like pre computed activations that like were only half completed or Are in some way incompatible with what you're doing so you can always go ahead and just delete this TMP this temporary directory and see if that causes your error to go away. This is the fast AI equivalent of turning it off and then on again You'll also see there's a directory called models, and that's where all of these when you say dot save with a model That's where that's going to go Actually it reminds me when this cast a gradient descent with restarts paper came out I saw a tweet that was somebody who was like oh to make your deep learning work better turn it off and then on again So if I want to see I want to retrain my model from squash again do I just delete everything? if you want to If you want to train your model from scratch There's generally no reason to delete the pre computed activations because the pre computed activations are Without any training. That's what the pre trained model Created with the with the weights that you downloaded off the internet the only Yeah, I mean the only reason you want to delete the pre computed activations if there was some error caused by like half Creating them and crashing or some something like that As you change the size of your input change different architectures and so forth they all create different sets of activations with different file names So you don't generally you shouldn't have to worry about it And if you want to start training again from scratch all you have to do is create a new Learn object so each time you go like con learner dot pre trained that creates a new Object with with new sets of weights be trained from Okay So before our break we'll finish off by talking about About fine tuning and differential learning rates and so so far Everything we've done Has not changed any of these pre trained filters right so we've used a pre trained model that already knows how to find At the early stages edges and gradients And then corners and curves And then repeating patterns and bits of text and eventually eyeballs right we have not Retrained any of those activations any of those features Or more specifically any of those weights in the convolutional kernels all we've done is we've learned Some new layers that we've added on top of these things we've learned how to mix and match these pre trained features now obviously it may turn out that Your pictures have you know different kinds of eyeballs or faces or if you're using different kinds of images like satellite images? Totally different kinds of features all together right so if you're like training to recognize icebergs You're probably want to go all the way back and learn you know all the way back to kind of different combinations of these simple gradients and edges In our case is dogs versus cats We're going to have some minor differences, but we still may find it's helpful to slightly tune some of these later layers as well So to tell the learner that we now want to start Actually changing the convolutional filters themselves We simply say unfreeze okay, so a frozen layer is a layer which is not trained which is not updated Okay, so unfreeze unfreezes all of the layers now when you think about it, it's pretty obvious that Layer one Right which is like a diagonal edge or a gradient Probably doesn't need to change by much if at all right from the one and a half million images on image net It probably already is figured out pretty well how to find like edges and gradients It probably already knows also like which kind of corners to look for and how to find which kinds of curves and so forth So in other words these early layers probably need little if any learning Where else these later ones are much more likely to need more learning and this is universally true regardless of whether you're looking for Satellite images of rainforests or icebergs or whether you're looking for cats versus dogs right? So what we do is we create an array of learning rates Where we say okay? These are the learning rates to use for our additional layers that we've added on top these are the learning rates to use in the middle few layers and These are the learning rates to use for the first few layers So these are the ones for the lights that represent like very basic geometric features These are the ones that are used to for the more complex Kind of sophisticated Compolitional features and these are the ones that are used for the features that we've added and learn from scratch Right so we can create an array of learning rates and Then when we call dot fit and pass in an array of learning rates It's now going to use those different learning rates for different parts of the model this is not something that We've like invented, but I'd also say it's like it's so not that common that it doesn't even have a name as far as I know So we're going to call it differential learning rates If it actually has a name or indeed if somebody's actually written a paper specifically talking about it. I don't know There's a great researcher called Jason Yusinski who who did write a paper about the kind of the idea that you might want different learning rates and showing why but I don't think any other libraries support it and Yeah, I don't know of a name for it having said that though This ability to like unfreeze and then use these differential learning rates I found is like the secret to taking a pretty good model and turning it into an awesome So just to clarify So you have three numbers there right three hyper parameters the first one is the photo Late models the model late layers the other way around so we The short answer is many many right and they're kind of in groups and we're going to learn about the architecture This is called a resnet or residual network. It kind of has resnet blocks And so what we're doing is we're grouping the blocks into three Groups and so this one is actually this first number is for the earliest layers Yeah, the ones closest to the pixels represent like corners and edges and gradients, but why Why do you I thought those layers are frozen at first? They are right. So we just hit unfreeze unfreeze. So we so you're unfreezing them because you have kind of partially trained All the late layers. We've trained we've trained our added layers. Yes. Now you're retraining the whole set exactly I see so but say and the learning rate is particularly small for the early layers That's right, because you just kind of want to fine-tune it. Yeah Yeah, we probably don't want to change them at all But you know if it does need to then then it can Thanks, no problem So using the differential inner rates how different from like grid search There's no similarity to grid search so grid search is where we're trying to find the best hyper parameter for something so for example you could kind of think of the Learning rate finder as a really sophisticated grid search, which is like trying lots and lots of learning rates to find which one is best But this has nothing to do with that. This is actually for the entire training from now on It's actually going to use a different learning rate for each layer And so I was wondering so you have a pre trained model then you have to use the same input Dimensions right because I was thinking okay. Let's say you have this big They use like big machines to train these things and you want to take advantage of it How would you go about you know, you have like images that are like bigger than the ones that they used or we're going to be talking About sizes later But the short answer is that with this library and the modern architectures were using we can use any size we like So Jeremy do we need a can we unfreeze just a specific layer We can we're not doing it yet But if you wanted to you can type learn dot freeze underscore two and pass in a letter number Learn dot freeze underscore two and passing a letter number Much to my surprise or at least initial my surprise it turns out I Almost never need to do that. I almost never find it helpful, and I think it's because we're using differential learning rates the the the optimizer can kind of learn just as much as it needs to so yeah, it's What if you have a little data like very little data data yeah, it's still Doesn't seem to help the one place. I have found it helpful is if I'm using like a really big memory intensive model, and I'm like running out of GPU freeze having The less layers you unfreeze the less memory it takes and the less time it takes so there's that kind of practical aspect So to make sure also I asked the question right? Can I just like unfreeze a specific layer? No you you can only unfreeze layers from layer in onwards You could probably delve inside the library and freeze one unfreeze one layer, but I don't know why you would Okay So I'm really excited to be showing you guys this stuff because it's like it's something we've been kind of researching all year It's figuring out how to train State-of-the-art models, and we've kind of found these like tiny number of tricks and so once we do that We now go learn dot fit right and you can see look at this we get right up to like 99.5 Accuracy which is crazy There's one other trick you might see here that as well as using stochastic gradient descent with restarts a cycle length equals one We've done three cycles so earlier on I lied to you. I said this is this is the number of epochs It's actually the number of cycles right so if you said cycle length equals two it would do three cycles of each of two epochs or do six epochs So here I've said do three cycles yet somehow it's done seven epochs and the reason why is I've got one last trick to show you which is cycle molt equals two and To tell you what that does. I'm simply going to draw you a picture show you the picture If I go learn dot chef dot plot learning rate there it is Now you can see what cycle mode equals two is doing okay? It's it's in doubling the length of the cycle after each cycle and so in the paper that introduced this stochastic gradient descent with restarts But the researcher kind of said hey, this is something that seems to sometimes work pretty well And I've certainly found that often to be the case so basically Intuitively speaking if your cycle length is too short Right then it's kind of starts going down to find a good spot And then it pops out and it goes down to try and find a good spot and pops out it never actually gets to find A good spot right so earlier on You want it to do that because it's trying to find the bit that's like smoother But then later on you want it to find do more exploring and then more exploring right so that's why this Cycle molt equals two thing often seems to be a pretty good approach right so Suddenly we're introducing more and more hyper parameters having told you there aren't that many But but the reason is that like you can really get away with just taking a good learning rate But then adding these extra tweaks Really helps get that extra level up without any effort right and so in practice I find This kind of three cycles starting at one molecules to Works very very often to get a pretty decent model If it does doesn't then often I'll just do three cycles of length to With no mold like that's kind of like two things that seem to work a lot There's not too much fiddling I find necessary And as I say even even if you just if you use this line every time I'd be surprised if you didn't get a reasonable result so a question here Why does smoother services correlate to more generalized networks? So it's kind of this um This intuitive explanation. I tried to kill the whole thing. I tried to give back here, which is that if you've got Something spiky All right, and so what this What this x-axis is showing is like how How good is this at recognizing dogs versus cats as you change this particular parameter right and so to something to be generalizable It means that we wanted to work when we give it when we give it a slightly different data set and so a slightly different data set May have a slightly different relationship between this parameter and how caddy versus doggy it is it may instead look a little bit like this Right so in other words if we end up at this point Right then it's not going to do a good job on this slightly different data set or else if we end up on this point It's still going to do a good job on this data set Okay, so that's what cycle mult equals do okay? So we've got one last thing before we're going to take a break Which is we're now going to take this model which has 99 and a half percent accuracy And we're going to try and make it better still and what we're going to do is we're not actually going to change the model at all right, but instead we're going to look back at the Original visual visualization we did where we looked at some of our incorrect pictures Now What I've done is I've printed out the whole of these incorrect pictures, but the key thing to realize is that Particularly in fact when we do the the validation set all of our inputs To our model all the time have to be square right and the reason for that is That's kind of a minor technical detail But basically the GPU doesn't go very quickly if you have like different dimensions for different images Because it needs things to be consistent so that every part of the GPU can do the same thing All right, and I think this is probably fixable, but it now that's the state of the technology We have so our validation set when we actually say for this particular thing is is the dog What we actually do to make it square is we just pick out? The square in the middle right so we would take off its two edges And so we take the whole height and then as much of the middle as we can and so you can see in this case We wouldn't actually see this dog's head All right, so I think the reason this was actually not correctly classified was because the validation set only got to see the body and the body Doesn't look particularly dog like or cat like it's not at all Sure, sure what it is so what we're going to do When we calculate the predictions for our validation set is we're going to use something called test time augmentation And what this means is that every time we decide is this cat or a dog not in the training? But after we've trained the model is we're going to actually take For random data augmentations and remember the data augmentations move around and And so you mean and out and flip Okay, so we're going to take four of them at random and we're going to take the original unaugmented center crop image And we're going to do a prediction for all of those and then we're going to take the average of those predictions So we're going to say is this a cat is this a cat is this a cat is this a cat right and so hopefully In one of those random ones we actually make sure that the face is there Zoomed in by a similar amount to other dog spaces at seen it's rotated by the amount that it expects to see it and so forth and so to do that All we have to do is just call TTA TTA stands for test time augmentation This term of like what are what do we call it when we're making predictions from a model? We've trained sometimes. It's called inference time sometimes. It's called test time everybody seems have a different name So TTA and so when we do that we go learn dot TTA check the accuracy And lo and behold we're now at 99 point six five percent, which is kind of crazy Where's our green box? But for every part we are only Showing one type of augmentation of a particular image right so when we are training back here We're not doing any TTA right so TTA is not like You could and sometimes like I've written libraries where after each epoch I run TTA to see how well it's going But that's not what's happening here. I trained the whole thing with Training time augmentation which doesn't have a special name because that's what we mean when we say data augmentation We need training time augmentation so here every time we showed it a picture We were randomly changing it a little bit so each epoch each of these seven epochs It was seen slightly different versions of the picture Having done that we now have a fully trained model. We then said okay Let's look at the validation set so TTA by default uses the validation set and said okay What are your predictions of which ones are cats and which ones are dogs and it did four? Predictions with different random augmentations plus one on the organ Unauthorized version average them all together, and that's what we got and that's what we capture the accuracy problem So is there a high probability of having? a sample in TTA that was not shown in during training Yeah, actually every data augmented for image is is unique because the rotation could be like point zero three four degrees and Zoom could be one point zero one six five so every time it's slightly different No problem. It's behind you What's your Why not use white padding or something like that this one of your white padding like this? You know put like a white border around me. Oh padding is not Yeah, so like there's lots of different types of Augmentation you can do and so one of the things you can do is to add a border around it Basically adding a border around it in my experiments doesn't doesn't help it doesn't make it any less cat-like It's not the convolutional neural network doesn't seem to find it very interesting basically Something that I do do we'll see later is I do something called reflection padding which is where I add some borders that are the Outside just reflected. It's a way to kind of make some bigger images works well with satellite imagery in particular But yeah in general. I don't do I have a lot of padding instead. I do a bit of zooming It's kind of follow-up to that last one, but Rather than cropping just add white space because when you crop you lose the dog's face But if you added white space you wouldn't have yeah So that's that's where the the kind of the reflection padding or the zooming or whatever can help so there are ways in the fast AI library when you do custom transforms of making that happen I Find that It kind of depends on the image size you know but Generally speaking it seems that using TTA plus data augmentation The best thing to do is to try to use as large image as possible And so if you kind of crop the thing down and put white borders on top and bottom It's now quite a lot smaller and so to make it as big as it was before you now have to use more GPU And if you're going to use all that more GPU you could have zoomed in and used a bigger image so In my playing around that doesn't seem to be generally as successful. Okay? There is a lot of interest on the topic of how to do the documentation in older than images In data that is not images Yeah No one seems to know I actually I Asked some of my friends in the natural language processing community about this and we'll get to natural language processing in a couple of lessons You know it seems like it'd be really helpful. There's been a few Example like a very very few number of examples of people where papers would like try replacing synonyms for instance but on the whole an understanding of like appropriate data augmentation for non image domains is Under researched and under underdeveloped The question was could couldn't we just use a sliding window to generate all the images so in that dog picture couldn't we generate three? Parts of that wouldn't that be better yeah for PTA you mean Just just in general when you're creating your so For training time I would say no that wouldn't be better because we're not going to get as much variation You know we want to have it like like one degree off five You know five degrees off ten pixels up like lots of slightly different versions, and so if you just had three standard Ways then you're not giving it as many different ways of looking at the data for test time augmentation Having fixed crop locations. I think probably would be better And I just haven't gotten around to writing that yet. I have a version in an old library I think having fixed crop locations plus random Contrast brightness rotation changes might be better The reason I haven't got around to it yet is because in my testing it didn't seem to help in practice very much And it made the code a lot more complicated, so you know it's kind of it's an interesting question Yeah, that's a great question so the faster. I library is open source, and let's talk about it a bit more generally because You know it's like the fact that The fact that we're using this library is kind of interesting and unusual and it sits on top of something called pie torch, right so pie torch is a Fairly recent development, and it's kind of I've noticed all the Researchers that I respect pretty much are now using high torch I found in part two of last year's course that a lot of the cutting-edge stuff I wanted to teach I couldn't do it in keras and tensorflow, which is what we used to teach with And so I had to switch the course to pie torch halfway through part two the problem was that Pie torch isn't very easy to use you have to write your own training loop from scratch I basically write everything from scratch all the stuff you see inside the past day our library We would have had to have written it You know to learn and so it really makes it very hard to learn deep learning when you have to write hundreds of lines of code to do anything so so We decided to create a library on top of pie torches because we you know this Our mission is to teach world-class deep learning so we wanted to show you like here's how you can be the best in the world at doing X And we found that a lot of the world-class stuff. We needed to show Really needed pie torch or at least with pie torch it was far easier but then pie torch itself just wasn't suitable as a a first thing to teach with for new For new deep learning practitioners So we built this library on top of pie torch Initially heavily influenced by Keras which is what we taught last year But then we realized we could actually make things much much much easier than Keras So in Keras if you look back at last year's course notes You'll find that all of the code is two to three times longer And there's lots more opportunities for mistakes because there's just a lot of things you have to get right So we ended up kind of building this this this library in order to make it easier to get into deep learning But also easier to get state-of-the-art results and then over the last year as we started developing on top of that we started Discovering that by using this library It made us so much more productive that we actually started kind of developing new state-of-the-art results and new methods ourselves And we started realizing that there's a whole bunch of like papers that have kind of been ignored or lost which when you use them It could like automate or semi automate start like burning red finder. That's not in any other library, so So I kind of got to the point where now not only is kind of fast AI I let's just do things easier much easier than any other approach But at the same time it actually has a lot more Kind of sophisticated stuff behind the scenes and anything else so so it's kind of an interesting mix So yeah, so we've released this library like at this stage It's like very early version and so through this course by the end of this course I hope as a group you know we all A lot of people are already helping have developed it into something that's You know really pretty stable and rough solid And yeah, anybody can then can use it To build your own models under an open source license as you can see it's available on github Behind the scenes it's it's creating pytorch models and so pytorch models can then be exported into various different formats Having said that like a lot of folks like if you want to do something on a mobile phone for example You're probably going to need to use tensorflow and so Later on in this course We're going to show like how some of the things that we're doing in the fast AI library you can do in Keras and TensorFlow so you can kind of get a sense of what the different libraries look like Generally speaking the simple stuff Is like it'll take you a small number of days to learn to do it in Keras and tensorflow versus fast AI And pytorch and the more complex stuff often Just won't be possible so like if you need it to be in tensorflow you'll just have to kind of simplify it often a little bit But you know I think the more important thing to realize is every year The kind of the the libraries that are available and which ones are the best totally changes So like the main thing I hope that you get out of this course is an understanding of the concepts like Here's how you find the learning rate. Here's why differential learning rates are important. Here's how you do learning rate annealing You know here's what stochastic gradient descent with restarts does so on and so forth Because you know by the time we do this course again next year You know the the library Situation is going to be different again That's a question Was Wondering if you've had an opinion on pyro which is uber's new release. I haven't looked at it No, I'm very interested in probabilistic programming, and it's really cool. That's built on top of pytorch So one of the things we'll learn about in this course is we'll see that pytorch is much more than just a deep learning library It actually lets us write arbitrary GPU accelerated algorithms From scratch which we're actually going to do and pyro is a great example of what people are now doing with pytorch outside of the deep learning world Great okay, let's take a eight minute break, and we'll come back at 755 So 99 point six five percent accuracy What does that mean so in? Classification when we do classification in machine learning The it's really simple way to look at the result of a classification is what's called the confusion matrix This is not just deep learning, but in any kind of classifier machine learning where we say okay? What was the actual truth? There were a thousand cats and a thousand dogs and if the thousand actual cats How many did we predict were cats this is obviously in the validation sets? This is the images that we didn't use to train with It turns out there were 998 cats that we actually predicted as cats and two that we got wrong Okay, and then for dogs there were 995 that we predicted were dogs and then five that we got wrong and so often these Confusion matrices can be helpful particularly if you've got like four or five classes You're trying to predict to see like which group you're having the most trouble with and you can see it uses color coding To tell you you know to highlight the large the large bits you're going to help that the diagonal is the highlighted section So now that we've retrained the model it can be quite helpful now That's better to actually look back and see like okay, which ones in particular were incorrect and we can see here There were actually only two incorrect cats it prints out four by default so you can actually see these two Actually less than 0.5, so they weren't they weren't wrong. Okay, so it's actually only these two were wrong cats This one isn't obviously a cat at all This one is but it looks like it's got a lot of weird artifacts, and you can't see its eyeballs at all so And then here are the how many dogs where they were wrong there were five wrong dogs here are four of them That's not obviously a dog That looks like a mistake that looks like a mistake that one. I guess it doesn't have enough information, but I guess it's a mistake so So we've done a pretty good job here of creating a good classifier I would based on Entering a lot of Kaggle competitions and comparing results. I've done to various research papers I can tell you it's it's a state-of-the-art classifier. It's it's right up there with the best in the world We're going to make it a little bit better in a moment, but here are the basic steps right so if you want to create a world-class Image classifier the steps that we just went through was that we started our we turn data augmentation on By saying all transforms equals and you either say side on or top down depending on what you're doing Start with pre compute equals true Find a decent learning rate We then train just like at one or two epochs which like takes a few seconds because we've got pre compute equals true Then we turn off pre compute which allows us to use data augmentation to do another two or three epochs generally with cycle length equals one Then I unfreeze all the layers I then set the earlier layers to be like I have somewhere between a three times to ten times lower learning rate than the previous so in this case I did Ten times right so it's like this was my learning rate that I found from learning rate finder Then I went ten times smaller and then ten times smaller as a rule of thumb like Knowing that you're starting with a pre trained image net model If you know if you can see that the things that you're now trying to classify are pretty similar to the kinds of things in Image net I eat pictures of normal objects in normal environments You probably want about a 10x difference because you want those earlier layers like you think that the earlier layers are probably very good already Whereas if you're doing something like satellite imagery or medical imaging which is not at all like image net Then you probably want to be training those earlier layers a lot more so you might have like a just a 3x difference so that's like One change that I I make is to try to make it out of 10x or 3x Yeah, so then after unfreezing You can now call LR find again, right? And I actually didn't in this case, but like once you've unfrozen all the layers you've turned on differential learning rates You can then call LR find again, right? And so you can then check like oh does it still look like the same point I had last time is about right Something to note is that if you call LR find Having set differential learning rates the thing that's actually going to print out is the learning rate of the last layers Right because you've got three different learning rates, so it's actually showing you the last layer So then yeah Then I train the full network with cycle multi calls to and to either it starts with the fitting or I run out of time Right so like let me show you that so let's do this again for a totally different data set so this morning I noticed that some of you on the forums were playing around with this playground Kaggle competition Very similar called dog breed identification So the dog breed identification Kaggle challenge Is one where you don't actually have to decide which ones are cats and which ones are dogs they're all dogs But you have to decide what kind of dog it is, but there are 120 different breeds of dogs okay, so You know obviously this could be like different types of Cells and pathology slides it could be different kinds of cancers in CT scans it could be Different kinds of icebergs and satellite images whatever right as long as you've got some kind of labeled images So I want to show you what I did this morning, so it took me about an hour basically to go end-to-end From something I've never seen before so I Downloaded the data from Kaggle, and I'll show you how to do that shortly But the short answer is there's something called Kaggle CLI Which is a github project you can search for and if you read the docs you basically run kg download Provide the competition name and it'll grab all the data for you to your press or Amazon or whatever instance I put it in my data folder and I then went LS, and I saw that it's a little bit different to Our previous data set it's not that there's a train folder which has a separate folder for each kind of dog But instead it turned out there was a CSV file and the CSV file I read it in with pandas so pandas is the thing we use in Python to do structured data analysis like CSV files So if you take pandas we call PD. That's pretty much universal PD dot read CSV reads in a CSV file we can then take a look at it and you can see that basically it had like some kind of identifier and Then the debris right so this is like a different way This is the second main way that people kind of give you image labels one is to put different images into different folders The second is generally to give you a some kind of file like a CSV file to tell you here's the image name And here's the label okay, so What I then did was I used Pandas again to create a pivot table which basically groups it up just to see how many of each breed there were and I sorted Them and so I saw okay. They've got like about a hundred Some of the more common breeds and some of the less common breeds that got like 60 or so okay? Altogether there are 120 rows and I've been 120 different breeds represented okay, so I'm going to go through the steps right so Enable data augmentation so to enable data augmentation when we call this transforms from model Are you just pass in and org transforms in this case? I chose side on again These are pictures of dots and stuff so there's side on photos I will talk about max zoom More detail later, but max zoom basically says when you do the data augmentation We like zoom into it by up to one point one times Okay, so randomly between one the original image size and one point one times So it's not always cropping out in the middle or an edge, but it could be cropping out a smaller part, okay, so Having done that the key step now is that rather than going from paths? So previously we went from paths and that tells it that the names of the folders are the names of the labels we go From CSV and we pass in the CSV file that contains the labels So we're passing in the path that contains all of the data The name of the folder that contains the training data the CSV that contains the labels We need to also tell it where the test set is if we want to submit to Kaggle later talk more about that next week now this time The previous data set we had I had actually separated a validation set out into a separate folder Right, but in this case you'll see that there is not a separate folder called validation Validation right so we want to be able to track how good our performance is locally So we're going to have to separate some of the images out to put it into a validation set Okay, so I do that at random and so up here. You can see I've basically opened up the CSV file Turned it into a list of rows and then taken the length of that Minus one because there's a header at the top right and so that's the number of rows in the CSV file Which must be the number of images that we have and then this is a fast AI thing get cross validation indexes We'll talk about cross validation later, but basically if you call this and pass in a number. It's going to return to you by default a Random 20% of the rows to use as your validation set and you can pass in parameters to get different amounts right? so this is now going to grab 20% of the data and Say all right. This is the this is the indexes the numbers of the files which we're going to use as a validation set Okay, so Now that we've got that in fact. Let's kind of run this so you can see what that looks like So valve indexes is Just a big bunch of numbers okay, and so and is 10,000 right and so we have about 20% of those is going to be in the validation set so when we call From CSV We can pass in a parameter which is to tell it which indexes to treat as a validation set and so let's pass in those indexes One thing that's a little bit tricky here is that The File names Actually have I checked they actually have a dot JPG on the end and these obviously don't have a dot JPG So you can pass in when you call from CSV you can pass in a suffix That says that the labels don't actually contain the full file names you need to add this to them, okay? So that's basically all I need to do to set up my data And as a lot of you have noticed during the week Inside that data object you can actually get access to the data set by call the training data set by saying train DS And inside train DS is a whole bunch of things including the file names Okay, so train DS dot file names contains all of the file names of everything in the training set and so here's like one Clowning okay, so here's an example of one file name So I can now go ahead and open that file and take a look at it All right, so that's the next thing I did was to try and understand what my file my data set looks like and it Found an adorable puppy so that was very nice so feeling good about this I also want to know like how big are these files right like how big are the images? Because that's a key issue if they're huge and then I have to think really carefully about how to deal with huge images That's really challenging if they're tiny well. That's also challenging Most of image net models are trained on either 224 by 224 or 299 by 299 Images so anytime you have images in that kind of range. That's that's really hopeful You're probably not going to do too much different in this case the first image. I looked at was about the right size So I'm thinking oh, it's looking pretty hopeful So what I did then is I created a dictionary comprehension now if you don't know about list comprehensions and dictionary comprehensions in Python Go study them. They're the most useful thing super handy You can see the basic idea here is that are going through all of the files And I'm creating a dictionary that maps the name of the file to the size of that file Again this is a handy little Python feature Which I'll let you think learn about during the week if you don't know about it which is zip and using this special star notation is now going to take this dictionary and turn it into the rows and the columns And so I can now turn those into numpy arrays and like okay here are the first five row sizes for each of my images and Then matplotlib is something you want to be very familiar with if you do any kind of data science on machine learning in Python Matplotlib we always refer to as PLT. So it's just a histogram and so I got a histogram of the How high how many rows there are in each image so you can see here? I'm kind of getting a sense before I start doing any modeling I kind of need to know what I'm modeling with and I can see some of the images are going to be like 2500 3000 pixels high but most of them seem to be around 500 So given that so few of them were bigger than a thousand I use standard numpy Slicing to just grab those that are smaller than a thousand and histogram that just to zoom in a little bit And I can see here alright. It looks like yeah the vast majority around 500 and so this actually also prints out The histogram so I can actually go through and I can see here for 4,500 of them are about 450 Okay, so I get about that sense about you now So Jeremy how many images should we get in the validation set is always a 20% So the size of the validation set like Using 20% is fine unless you kind of feeling like oh my data is my data sets really small. I'm not sure that's enough you Know like if you've got Basically think of it this way if you train like the same model multiple times And you're getting very different validation set results and your validation sets kind of small like smaller than a thousand or so Then it's going to be quite hard to interpret how well you're doing now This is particularly true like if you're like if you care about the third decimal place of accuracy And you've got like a thousand things in your validation set then you're going about like a single image changing class is changing You know is what you're looking at so It's it really depends on like how accurate you how much difference you care about I Would say in general like at the point where you care about the difference train like I don't know point Oh one and point oh two like the second decimal place you want that to represent like Ten or twenty rows you know like I'm changing the class of like ten or twenty rows then That's something you can be pretty confident on So like most of the time You know given the data sizes. We normally have 20% things to work fine But yeah, it's it's it's kind of a it depends a lot on Specifically what you're doing and what you care about And It's not it's not a deep learning specific question either you know so those who are interested in this kind of thing We've got to look into it a lot more detail in our machine learning course Which will also be available online? Okay, so I did the same thing for the columns just to make sure that these aren't like super wide and I got similar results And checked in and again found there kind of like four or five hundred seemed to be about the average size So based on all of that I kind of thought okay This looks like a pretty normal kind of image data set that I can probably use pretty normal kinds of models on I was also particularly encouraged to see that when I looked at the dog that the dog like takes up most of the frame Right so I'm not too worried about like cropping problems You know If the if the dog was just like a tiny little piece of one little corner that I'd be thinking about doing different You know maybe zooming in a lot more or something like a medical imaging that happens a lot like often the tumor or the cell Whatever is like one tiny piece, and that's much more complex So yeah based on all that this morning. I kind of thought like okay this this looks pretty standard so I Went ahead and created a little function called get data that basically had my normal two lines of code in it but I made it so I could pass in a size and a batch size the Reason for this is that when I start working with a new data set I want everything to go super fast and so if I use small images. It's going to go super fast So I actually started out with size equals 64 just to create some super small images that just go like a second to Run through and see how it went Later on I started using some big images and some and some also some bigger architectures at which point I started running out of GPU memory so I started getting these errors saying Cuda out of memory error when you get a Cuda out of memory error the first thing you need to do is to go kernel Restart once you get a code an out-of-memory error on your GPU. You can't really recover from it right doesn't matter What you do you know you have to go restart? And once I restarted I then just changed my batch size to something smaller so when you call create your data object You can pass in a batch size parameter Okay, and like I normally use 64 until I get something that says out of memory And then I'll just have it if I still get out of memory. I'll just have it again, okay So that's where I created this to allow me to like start making my size as bigger as I looked into it more And you know as I started running out of memory to decrease my batch size so at this point You know I went through this a couple of iterations, but I basically found everything was working fine so once it's working fine I set size to 224 and I Created my you know precomputed equals true first time I did that it took a minute to create the precomputed activations And then it ran through this in about four or five seconds, and you can see I was getting 83% accuracy Now remember accuracy means it's it's exactly right and so it's predicting out of 120 categories It's predicting exactly right so when you see something with two classes is you know 80% accurate Versus something with 120 classes is 80% accurate. They're very different Levels you know so when I saw like 83% accuracy with just a precomputed classifier No, data augmentation. No one freezing anything else across 120 classes something. Oh this looks good, right so Then I just kind of kept going through our little standard process right so then I turn Precompute off Okay, and Cycle length equals one and I started doing a few more cycles a few more epochs so remember an epoch is one pass through the data a cycle is However many epochs you said is in a cycle It's one it's the learning rate going from the top that you asked for all the way down to zero So since here cycle length equals one a cycle and an epoch are the same okay, so I did I tried a few epochs I Did actually do the learning rate finder, and I found one in egg two again looks fine it often looks fine And I found it kind of kept improving so I tried five epochs, and I found my accuracy getting better So then I Saved that and I tried something which we haven't looked at before, but it's kind of cool If you train something on a smaller size you can then actually call learn dot set data and Pass in a larger size data set and that's going to take your model However, it's trained so far, and it's going to let you continue to train on on larger images And I'll tell you something amazing This actually is another way you can get state-of-the-art results And I've never seen this written in any paper or discussed anywhere as far as I know this is a new insight Basically, I've got a pre-trained model which in this case I've trained a few epochs with the size of 224 by 224 And I'm now going to do a few more epochs with a size of 299 by 299 Now I've got very little data kind of by deep learning standards only got 10,000 images right so with a 224 by 224 I kind of built this these final layers to try to find things that worked well at 224 by 224 But I go to 299 by 299 I Basically if I overfit the floor I'm definitely not going to overfit now like I've changed the size of my images They're kind of like totally different But like conceptually they're still picked the same kinds of pictures of the same kinds of things So I found this trick of like starting training on small images for a few epochs and then switching to bigger images and continuing Training is an amazingly effective way to avoid overfitting and It's like it's so easy and so obvious I don't understand why it's never been written about before maybe it's in some paper somewhere, and I haven't found it But it's I haven't seen it Would it be possible to do the same thing using let's take errors or TensorFlow as well to feed a Measure of different sides yeah, I think so like as long as you use one of these more modern architectures what we call fully convolutional architectures Which means not VGG and you'll see we don't use VGG in this course because it doesn't have this property, but most of the Architectures developed in the last couple of years can handle pretty much arbitrary sizes Yeah, be worth trying yeah, I think it ought to work Okay, so I call get data again remember get data is that just the little function that I created back up here right get Data is just this little function. That's why I just passed a different size to it and so I Call freeze just to make sure that that everything set the last layer is frozen I mean actually it already was at this point that really do anything and You can see now with free compute off I've never got data augmentation working So I kind of run a few more epochs and what I noticed here is that the loss of my training set and the loss of My validation set my validation set loss is a lot lower than my training set. This is still just training the last layer So what this is telling me is I'm I'm under fitting right and so if I'm under fitting It means this cycle length equals one is too short It means it's like finding something better pop popping out and it's like never getting a chance to zoom in properly So then I set cycle mult equals two to give it more time So like the first time is one epoch the second one is two epochs The third one is four epochs and you can see now the validation train and training are about the same Okay, so that's kind of thinking yeah, this is this is about the right track and so then I tried using test type Augmentation to see if that gets any better still didn't actually help a hell of a lot just a tiny bit And just kind of at this point. I'm thinking this is nearly done So I just did it like you know one more cycle of two to see if it got any better and It did get a little bit better and then I'm like, okay That looks pretty good I've got a validation set loss of point one nine nine And so you'll notice here actually haven't tried unfreezing The reason why I was going to try to unfreezing and training more it didn't get me better and so the reason for this clearly is that this data set is so similar the image net that the Training the convolutional layers actually doesn't help in the slightest And actually when I later looked into it it turns out that this competition is actually using A subset of image net so okay, so then if we check this out point one nine nine against the leaderboard This is only a playground competition, so it's not like the best of here, but you know it's still interesting It gets us Somewhere around 10th 11th okay, and in fact we're Competing against I notice other this is a fast AI student. This is a fast AI student These people up here. I know they actually posted that they they cheated they actually went We downloaded the original images and trained to that so This is why this is a playground competition they call it it's not it's not real right you know It's just to allow us to try things out, but you can basically see out of 200 and something people where you know we're getting some very good results Without doing anything remotely interesting or clever, and we haven't even used the whole data set We've only used 80% of it like to get a better result I would go back and remove that validation set and just rerun the same steps and then submit that That lets us use 100% of the data What's our Last three questions the first one is like the class in this case is very it's not balanced unlike the dogs and cats It's not unbalanced like it's not totally balanced, but it's not bad right it's like Between 60 and 100 like it's it's it's it's not unbalanced enough that I would give it a second thought Okay Yeah, let's get to that later in this course And don't let me forget right the short answer is that there was a recent list of paper came out about two or three weeks Ago on this and it said the best way to deal with very unbalanced data sets is to basically make copies of the rare cases Yeah My second question is I want to pin down a difference between pre compute was true and So you have these two options so when you beginning Right right so it's and not only they're frozen they're pre computed so the data orientation doesn't do anything at that point Right before you own freeze everything What does exactly do like you only you only offer is the activation is that So we're going to learn more about the details as we look into the math and stuff in coming lessons But basically what happened was we started with a pre trained network right? Which was kind of finding activations that had these kind of rich features And we were adding then we add a couple of layers on the end of it which which which start out random and So with freeze equals with with everything frozen and indeed with pre compute equals true all we're learning Is to is those couple of layers that we've added and so with pre compute equals true We actually pretty calculate like how much does this image have something that looks like this eyeball and looks like this face so forth and therefore data augmentation doesn't do anything with pre compute equals true because You know we're actually showing exactly the same activations each time We can then set pre compute equals false which means it's still only Training those last two layers that we added it's still frozen, but data augmentations now working It's actually going through and recap letting all of the activations from scratch And then finally when we unfreeze that's actually saying okay now you can go ahead and change all of these earlier convolutional filters so what you just do pre compute response and when you unfreeze the last layer So the only reason to have pre compute equals true is it's just much faster So it's like it is it's about you know ten or more times faster So particularly if you're working with like quite a large data set you know it can save quite a bit of time, but it's never There's no like comp this like Accuracy reason ever to use pre compute equals true. It's just a it's just a shortcut. It's also like quite handy if you're like Throwing together a quick model. You know it can take a few seconds to create it My last question You have this stage What if Like we just wanted to one initial setting without Checking out Is that I mean if you wanted like if your question is like is there some shorter version of this That's like a bit quicker and easier. I could like to lead a few things here Okay, I think this is a kind of a minimal version to get you a very good result Which is like don't worry about pre compute equals true because that's just saving a little bit of time You know so so I still suggest use LR find at the start to find a good learning rate By default everything is frozen from the start so then you can just go ahead and run a two or three epochs of cyclin pickles one unfreeze And then train the rest of the network with differential learning rates, so it's basically three steps learning rate finder train frozen network with cycling pickles one and Then train unfrozen network with differential learning rates and cycle molecules two so like that's something you could turn into I Guess five or six lines of code total I think it's a question behind you oh next door By reducing the batch size does it only affect the speed of training? Yeah pretty much So each batch and again we're going to see like all this stuff about pre computing batch sizes We dig into the details of the algorithms. It's going to make a lot more sense intuitively, but basically if you're showing it Less images each time then it's calculating the gradient with less images Which means it's less accurate which means like knowing which direction to go and how far to go in that direction is less accurate So as you make the batch size baller you're basically making it kind of more volatile it's kind of like It kind of impacts the Optimal learning rate that you would need to use but in practice We're only you know I generally find I'm only dividing the batch size by like two or at most four It doesn't seem to change things very much should I reduce the line of rate? Accordingly if you if you change the batch size by much you can rerun the learning rate finder to see if it's changed if I'm Not sure if it you know since we're only generally looking at like a power of 10 It probably is not going to change things enough that you care because possibly This is sort of a conceptual Question so going back to the previous slide where you showed could you live better than I am sorry yeah This is more of a conceptual sort of basic question going back to your previous slide Where you showed what the different layers were doing? Yeah, so this slide I understand right meaning of See the third column relative to the fourth column is that What you're interpreting what the layer is doing based on what images actually? Yeah, so we're going to look at this in more detail So these these gray ones basically say this is kind of what the filter looks like So on the first layer you can say exactly what the filter looks like because the input to it at pixels Right so you can absolutely say and remember we looked at what a convolutional kernel was like is that three by three thing So this looked like there's seven by seven kernels You can say this is actually what it looks like but later on it's combining You know that the the input to it are themselves activations which are combinations of activations which are combinations of activations So you can't draw it, but there's clever technique that's I learned Fergus created which allowed them to say This is kind of what the filters tended to look like on average Right so this is kind of what the filters look like and then here is specific examples of patches of image which Activated that filter highly so the pictures are the ones that I kind of find more useful because it tells you this Kernel is kind of a unicycle wheel finder Well we'll come back well we may come back to that if not in this part in the next part that Probably in part two actually because this paper this paper uses to create these things this paper uses something called a deconvolution Which I'm pretty sure we won't do in this part, but we will do it in part two so if you're interested Check out the paper. It's in the notebook. There's a link to it. It's either in Fergus It's a very clever technique and not terribly intuitive Right so So you mentioned that it was good that the dog took up the full picture And it would have been a problem if it was kind of like off in one of the corners and really tiny What what would you have done? What would your technique have been to try to make that work? Something that we'll learn about in part two but basically there's a technique that allows you to kind of Pick her out roughly which parts of an image are most likely to have the interesting things in them And then you can like crop out those bits if you're interested in learning about it We did cover it briefly in lesson 7 of part 1, but I'm going to actually do it Properly in part 2 of this course Because I didn't really cover it thoroughly enough. Yeah, maybe We'll find time to have a quick look at it, but we'll see I know you nets written some of the code that we need already So once I have something like this notebook, it's basically working I can Immediately make it better by doing two things Assuming that the size image I was using is smaller than the average size of the image that we've been given I can increase the size and as I showed before with the dog breeds you can actually increase it during training The other thing I can do is to create is to use a better architecture Now an architecture we're going to talk a lot in this course about architectures, but basically there are Different ways of putting together like what size convolutional filters, and how are they connected just to each other and so forth and Different architectures have different like numbers of layers and sizes of kernels number of filters and so forth and so There are some the one that we've been using resnet 34 is a great Starting point and often a good finishing point because it's like it's pretty it doesn't have too many parameters often It works pretty well with small amounts of data as we've seen and so forth But there's actually an architecture that I really like called not resnet but resnext Which was actually the second place winner in last year's image net competition? and Like resnet you can put a number after the resnext to say like how big it is and Like my next step after resnet 34 is always resnext 50 now You'll find resnext 50 takes like and take like twice as long as Resnet 34 it can take like two to four times as much memory as Resnet 34 so what I wanted to do was I wanted to rerun that previous notebook with resnext and Increasing the image size to turn on you know so here I just said architecture equals resnext 50 size equals to 99 and then I found that I had to take the batch size all The way back to 28 to get it to fit my GPU is 11 gig if you're using AWS or Cressul I think they're like 12 gigs they might be able to make this bit higher But this is what I found I had to do so then I this is literally a copy of the previous notebook So you can actually go file make a copy right and then rerun it with with these different parameters And so I deleted some of the pros and some of the exploratory stuff to see you know basically I said everything else is the same all the same steps as before There's my in fact you can kind of see what this minimum set of steps looks like I didn't need to worry about learning rate Find us, so I just left it as is so transforms data equals learn equals fit Precomputed equals false fit with cycling tickles one unfreeze differential learning rates Fit some more and you can see here I didn't do the cycle malt thing because I found like now that I'm using a bigger architecture It's got more parameters. It was over fitting pretty quickly So rather than like cycle length equals one never finding the right spot It actually did find the right spot, and if I used longer cycle lengths. I found that my validation Error was higher than my training error. It was over fitting So check this out though by using these you know Three steps I got plus TTA 99.75 So what does that mean that means I have one incorrect dog for incorrect cats and when we look at the pictures of them my Incorrect dog has a cat in it this one is not a either this one is not either, so I've actually got one mistake and then my incorrect dog is teeth Right so like we're at a point Where we're now able to train a classifier. That's so good that it has like basically one mistake right and so when people say like we have superhuman image performance now This is kind of what they're talking about right so if you're actually when I looked at the dog breed one I did this morning. I was like it was it was getting the dog breeds much better than I ever could So like it's this is what we can get to if you use a really modern architecture like resnext And this only took I don't know like Remember until like 20 minutes to train So that's kind of where we're up to So If you wanted to do satellite imagery instead Right then it's the same thing and in fact the the planet satellite data sets already on Cresall if you're using Cresall you can jump straight there right and I Just linked it into data slash planet, and I can do exactly the same thing right I can Image classifier from CSV Right and you can see these three lines are actually exactly the same as my dog breed lines You know how big how many lines are in the file grab my validation indexes this get data as you can see it's identical Except I've changed Side on to top down the satellite images about top down so I can fit them Vertically and they still make sense right and so you can see here. I'm doing this trick run back to size equals 64 and Train a little bit first learning rate finder right and interestingly in this case you can see it. I want really high learning rates I don't know what it is about this particular data set. This is true, but it's clearly I can use super high learning rates So I used a learning rate of point two and so I've trained for a while differential learning rates right and so remember I said like if the Datasets very different to image net I probably want to train those middle layers a lot more So I'm using divided by three rather than divided by ten right doesn't that is the same thing cycle molecules two? right And then I was just kind of keeping an eye on it So you can actually plot the loss if you go learn dot shed dot plot loss you can see here the here's the first cycle Is the second cycle is the third cycle right so you can see it's like it's better pops out gets better pops out It better pops out and each time it finds something better than the last time Then set the size up to 128 and just repeat exactly the last few steps and then set up 256 and repeat the last two steps and Then do TTA and if you submit this then this gets about 30th place in this competition So these basic steps work super well this this thing where I went all the way back to a size of 64 I Wouldn't do that if I was doing like dogs and cats or dog breeds because like this is so small That if the thing I was working on is very similar to image net I would kind of destroy those image net weights like 64 by 64 is so small But in this case the satellite imagery data is so different to image net You know I really found that it worked pretty well to start right back at these tiny images It really helped me to avoid overfitting And interestingly using this kind of approach I actually found that even with using only 128 by 128 I was getting like much better cackle results than nearly everybody on the leaderboard And when I say 30th place, this is a very recent competition right and so I find like in The last year like a lot of people have got a lot better at computer vision And so the people in the top 50 in this competition were generally on assembling dozens of models Lots of people on a team Lots of pre-processing specific satellite data and so forth so like to be able to get 30th Using this totally standard technique is pretty cool All right So now that we've got to this point right we've got through two lessons if you're still here Then hopefully you're thinking okay. This is actually pretty useful I want to do more in which case Cressul might not be where you want to stay The issues with Cressul. I mean it's it's it's pretty handy It's pretty cheap and something we haven't talked about much is paper space is another great choice by the way Paper space is shortly going to be releasing Cressul like instant jupyter notebooks unfortunately. They're not ready quite yet But they do have an ability to basically they have the best price Performance relationship right now, and they you can SSH into them and use them so they're also a great choice And probably by the time this is a MOOC We'll probably have a separate lesson showing you how to set up paper space because they're likely to be the great option But at some point you're probably going to want to look at AWS a couple of reasons why The first is as you all know by now Amazon have been kind enough to donate about $200,000 worth of compute time to this course so I want to say thank you very much to Amazon. We've all been given Credits everybody who's here, so thanks very much AWS so sorry if you're on the MOOC we didn't get it for you, but everybody here is like AWS credits for everybody so But you can get even if you're not here in person you can get AWS credits from lots of places GitHub has a student pack Google for GitHub student pack. That's like 150 bucks worth of credits AWS educate and get credits these are all for students So there's lots of places you can get started on AWS pretty much everybody Everybody a lot of the people that you might work with Will be using AWS Because it's like super flexible Right now AWS has the fastest Available GPU so you can get in the cloud their p3s They're kind of expensive at three bucks an hour But if you've got like a model where you've done all the steps before you're thinking this is looking pretty good You know for six bucks you could get a p3 for two hours and run at turbo speed right? We didn't start with AWS because well a it's like twice as expensive as Cressel for the cheapest GPU And being it takes some setup right, but I wanted to kind of go through and show you how to get your AWS Setup and so we're going to be going slightly over time to do that But I want to show you very quickly so feel free to go if you have to But I want to show you very quickly how you can get your AWS set up right from scratch so Basically you have to go to a console dot AWS But Amazon calm and it'll take you to the console right and so you can follow along on the video with this I'm going to do it very quickly from here. You have to go to EC2 this is where you set up your instances and so from EC2 You need to do what's called launching an instance so launching an instance means you're basically creating a computer right you're creating a computer on Amazon so I say launch instance and What we've done is we've created a fast AI It's kind of AMI and AMI is like a template for how your computer is going to be created So if you go to community AMIs and type in class AI You'll see that there's one there called fast AI part 1 version 2 for the P2 Okay, so I'm going to select that and Then we need to say what kind of computer do you want and so I can say I want a GPU compute computer and Then I can say I want a P2x large. This is the cheapest Reasonably effective for deep learning instance type they have and then I can say launch and then I can say launch and So at this point they ask you to choose a key pair Right now if you don't have the key pair you have to create one right so to create a key pair You need to open your terminal If you don't have a Terminal if you've got a Mac or a Linux box you've definitely got one if you've got Windows Hopefully you've got Ubuntu if you don't already have Ubuntu set up you can go to the Windows Store and Click on Ubuntu right we'll get it from the Windows Store So from there you basically go SSH Dash key gen and That will create like a special password for your computer to be able to log into Amazon And then you just hit enter three times Okay, and that's going to create for you your key that you can use to get into Amazon All right, so then what I do is I copy that key somewhere that I know where it is So it'll be in the dot SSH folder, and it's called ID RSA dot pub and so I'm going to copy it to My hard drive So if you're in a macro and Linux it'll already be in an easy-to-find place. It'll be in your dot SSH folder I'm going to put that in documents So from there Back in AWS you have to tell it that you've created this key so you can go to key pairs and You say import key pair, and you just browse to that file that you just created There it is I say import Okay, so if you've ever used SSH before you've already got the key pair You don't have to do those steps if you've used AWS before you've already imported it You don't have to do that step. Maybe you haven't done any of those things you have to do both steps So now I could go ahead and launch my instance Community am is search fast AI select Launch and So now it asks me What's where's your key pair and I can choose that one that I just grabbed okay? So this is going to go ahead and create a new computer for me to log into And you can see here. It says the following have been initiated and so if I click on that It'll show me this new computer that I've created Okay So to be able to log into it I Need to know its IP address So here it is the IP address there okay, so I can copy that and That's the IP address of my computer So to get to this computer I need to SSH to it so SSH into a computer means connecting to that computer So that it's like you're typing that computer, so I type SSH and the username For this instance is always Ubuntu right and then I can paste in that IP address and Then there's one more thing I have to do Which is I have to connect up the Jupiter notebook on that instance to the Jupiter notebook on my machine And so to do that there's just a particular flag that I said okay We can talk about it on the forums as to exactly what it does But you just type minus L a to date a localhost a date a date Okay, so like once you've done it once you can like save that as an alias and type in the same thing every time So we can check here we can see it says that it's running so we should be able to now here enter First time ever which set we connect to it. It just checks. This is okay. I'll say yes And then that goes ahead and SSH is in so This AMI is all set up for you alright So you'll find that the very first time you log in it takes a few extra seconds because it just kind of is getting everything Set up, but once it's logged in you'll see there that there's a directory called fast AI And the fast AI directory contains our fast AI repo that contains all the notebooks All the code etc so I can just go CD fast AI right first thing you do when you get in is to make sure It's updated so you just go get pull Right and that updates to make sure that your repo is the same as the most recent repo and So as you can see there we go Let's make sure it's got all the most recent code the second thing you should do is type conda and update You can just do this maybe once a month or so and that makes sure that the libraries there are all the most recent libraries I'm not going to run that so it takes a couple of minutes. Okay, and then the last step is to type to a notebook Okay, so this is going to go ahead and launch the jupyter notebook Server on this machine again the first time I do it the first time you do everything on AWS it just takes like a minute or two And then once you've done it in the future. It'll be just as fast as running it locally basically So you can see it's going ahead and firing up the notebook And so what's going to happen is that because when we SSH into it we said to like connect our Notebook port to the remote notebook port. We're just going to be able to use this locally So I see he says here copy paste this URL, so I'm going to grab that URL and I'm going to paste it into my browser And that's it okay, so this notebook is now actually not running on my machine It's actually running on AWS okay using the AWS GPU. We've got a lot of memory It's not the fastest around but it's not terrible You can always fire up a p3 if you want something that's super fast. This is costing me 90 cents a minute Okay, so when you're finished, please don't forget to shut it down right so to shut it down you can Right click on it and say instant state stop Okay We've got 500 bucks of credit Assuming that you put your code down in the spreadsheet one thing I forgot to do the first time I showed you this by the way. I said make sure you choose a P2 the second time I went through I didn't choose p2 by mistake So just don't forget to choose GPU compute p2 you have a question My buddy said it's an hour. Thank you 90 cents an hour It also costs like I don't know three or four bucks a month for the storage as well Thanks for checking that. All right. See you next week. Sorry. We're over", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.34, "text": " Okay, so welcome back to deep learning lesson two", "tokens": [1033, 11, 370, 2928, 646, 281, 2452, 2539, 6898, 732], "temperature": 0.0, "avg_logprob": -0.2704716401941636, "compression_ratio": 1.497737556561086, "no_speech_prob": 0.009121804498136044}, {"id": 1, "seek": 0, "start": 6.96, "end": 8.96, "text": " Last week we", "tokens": [5264, 1243, 321], "temperature": 0.0, "avg_logprob": -0.2704716401941636, "compression_ratio": 1.497737556561086, "no_speech_prob": 0.009121804498136044}, {"id": 2, "seek": 0, "start": 9.38, "end": 12.8, "text": " Got to the point where we had successfully trained a pretty accurate", "tokens": [5803, 281, 264, 935, 689, 321, 632, 10727, 8895, 257, 1238, 8559], "temperature": 0.0, "avg_logprob": -0.2704716401941636, "compression_ratio": 1.497737556561086, "no_speech_prob": 0.009121804498136044}, {"id": 3, "seek": 0, "start": 13.76, "end": 15.76, "text": " image classifier and", "tokens": [3256, 1508, 9902, 293], "temperature": 0.0, "avg_logprob": -0.2704716401941636, "compression_ratio": 1.497737556561086, "no_speech_prob": 0.009121804498136044}, {"id": 4, "seek": 0, "start": 16.0, "end": 18.72, "text": " So just to remind you about how we did that", "tokens": [407, 445, 281, 4160, 291, 466, 577, 321, 630, 300], "temperature": 0.0, "avg_logprob": -0.2704716401941636, "compression_ratio": 1.497737556561086, "no_speech_prob": 0.009121804498136044}, {"id": 5, "seek": 1872, "start": 18.72, "end": 28.479999999999997, "text": " Can you guys see okay, I think actually we can't turn the front lights off in there. Can you guys all see the screen? Okay?", "tokens": [1664, 291, 1074, 536, 1392, 11, 286, 519, 767, 321, 393, 380, 1261, 264, 1868, 5811, 766, 294, 456, 13, 1664, 291, 1074, 439, 536, 264, 2568, 30, 1033, 30], "temperature": 0.0, "avg_logprob": -0.3540431261062622, "compression_ratio": 1.529100529100529, "no_speech_prob": 7.720950816292316e-05}, {"id": 6, "seek": 1872, "start": 29.96, "end": 32.08, "text": " We can turn just these ones can we?", "tokens": [492, 393, 1261, 445, 613, 2306, 393, 321, 30], "temperature": 0.0, "avg_logprob": -0.3540431261062622, "compression_ratio": 1.529100529100529, "no_speech_prob": 7.720950816292316e-05}, {"id": 7, "seek": 1872, "start": 34.879999999999995, "end": 38.0, "text": " Some pictures all into darkness, but if that works then", "tokens": [2188, 5242, 439, 666, 11262, 11, 457, 498, 300, 1985, 550], "temperature": 0.0, "avg_logprob": -0.3540431261062622, "compression_ratio": 1.529100529100529, "no_speech_prob": 7.720950816292316e-05}, {"id": 8, "seek": 1872, "start": 40.0, "end": 42.4, "text": " Is that okay that's better isn't it yeah", "tokens": [1119, 300, 1392, 300, 311, 1101, 1943, 380, 309, 1338], "temperature": 0.0, "avg_logprob": -0.3540431261062622, "compression_ratio": 1.529100529100529, "no_speech_prob": 7.720950816292316e-05}, {"id": 9, "seek": 1872, "start": 43.92, "end": 45.92, "text": " Do you mind doing the other two?", "tokens": [1144, 291, 1575, 884, 264, 661, 732, 30], "temperature": 0.0, "avg_logprob": -0.3540431261062622, "compression_ratio": 1.529100529100529, "no_speech_prob": 7.720950816292316e-05}, {"id": 10, "seek": 4592, "start": 45.92, "end": 47.92, "text": " And maybe that one as well", "tokens": [400, 1310, 300, 472, 382, 731], "temperature": 0.0, "avg_logprob": -0.27838798418436966, "compression_ratio": 1.5116279069767442, "no_speech_prob": 4.133289257879369e-05}, {"id": 11, "seek": 4592, "start": 50.6, "end": 57.36, "text": " Oh, not that one. Oh, that's great. Sorry that know you're right. Okay, great. That's better isn't it?", "tokens": [876, 11, 406, 300, 472, 13, 876, 11, 300, 311, 869, 13, 4919, 300, 458, 291, 434, 558, 13, 1033, 11, 869, 13, 663, 311, 1101, 1943, 380, 309, 30], "temperature": 0.0, "avg_logprob": -0.27838798418436966, "compression_ratio": 1.5116279069767442, "no_speech_prob": 4.133289257879369e-05}, {"id": 12, "seek": 4592, "start": 62.44, "end": 69.48, "text": " So just to remind you the way that we built this image classifier was we used a small amount of code", "tokens": [407, 445, 281, 4160, 291, 264, 636, 300, 321, 3094, 341, 3256, 1508, 9902, 390, 321, 1143, 257, 1359, 2372, 295, 3089], "temperature": 0.0, "avg_logprob": -0.27838798418436966, "compression_ratio": 1.5116279069767442, "no_speech_prob": 4.133289257879369e-05}, {"id": 13, "seek": 4592, "start": 70.6, "end": 72.6, "text": " basically three lines of code", "tokens": [1936, 1045, 3876, 295, 3089], "temperature": 0.0, "avg_logprob": -0.27838798418436966, "compression_ratio": 1.5116279069767442, "no_speech_prob": 4.133289257879369e-05}, {"id": 14, "seek": 7260, "start": 72.6, "end": 76.08, "text": " And these three lines of code pointed at a particular path", "tokens": [400, 613, 1045, 3876, 295, 3089, 10932, 412, 257, 1729, 3100], "temperature": 0.0, "avg_logprob": -0.15819977847012606, "compression_ratio": 1.9615384615384615, "no_speech_prob": 1.9525437892298214e-05}, {"id": 15, "seek": 7260, "start": 76.96, "end": 82.28, "text": " Which already had some data in it and so the key thing for this to know how to train this model", "tokens": [3013, 1217, 632, 512, 1412, 294, 309, 293, 370, 264, 2141, 551, 337, 341, 281, 458, 577, 281, 3847, 341, 2316], "temperature": 0.0, "avg_logprob": -0.15819977847012606, "compression_ratio": 1.9615384615384615, "no_speech_prob": 1.9525437892298214e-05}, {"id": 16, "seek": 7260, "start": 83.03999999999999, "end": 86.47999999999999, "text": " Was that this path which was data dogs cats?", "tokens": [3027, 300, 341, 3100, 597, 390, 1412, 7197, 11111, 30], "temperature": 0.0, "avg_logprob": -0.15819977847012606, "compression_ratio": 1.9615384615384615, "no_speech_prob": 1.9525437892298214e-05}, {"id": 17, "seek": 7260, "start": 87.24, "end": 90.47999999999999, "text": " had to have a particular structure, which is that it had a", "tokens": [632, 281, 362, 257, 1729, 3877, 11, 597, 307, 300, 309, 632, 257], "temperature": 0.0, "avg_logprob": -0.15819977847012606, "compression_ratio": 1.9615384615384615, "no_speech_prob": 1.9525437892298214e-05}, {"id": 18, "seek": 7260, "start": 91.24, "end": 96.0, "text": " Train folder and a valid folder and in each of those train invalid folders", "tokens": [28029, 10820, 293, 257, 7363, 10820, 293, 294, 1184, 295, 729, 3847, 34702, 31082], "temperature": 0.0, "avg_logprob": -0.15819977847012606, "compression_ratio": 1.9615384615384615, "no_speech_prob": 1.9525437892298214e-05}, {"id": 19, "seek": 9600, "start": 96.0, "end": 102.76, "text": " There was a cat's folder and the dogs folder and each of the cats and the dogs folders was a bunch of images of cats and dogs", "tokens": [821, 390, 257, 3857, 311, 10820, 293, 264, 7197, 10820, 293, 1184, 295, 264, 11111, 293, 264, 7197, 31082, 390, 257, 3840, 295, 5267, 295, 11111, 293, 7197], "temperature": 0.0, "avg_logprob": -0.16561051841094115, "compression_ratio": 1.8395061728395061, "no_speech_prob": 8.397989404329564e-06}, {"id": 20, "seek": 9600, "start": 102.92, "end": 106.24, "text": " So this is like a pretty standard. It's one of two", "tokens": [407, 341, 307, 411, 257, 1238, 3832, 13, 467, 311, 472, 295, 732], "temperature": 0.0, "avg_logprob": -0.16561051841094115, "compression_ratio": 1.8395061728395061, "no_speech_prob": 8.397989404329564e-06}, {"id": 21, "seek": 9600, "start": 107.36, "end": 114.14, "text": " Main structures that are used to say here is the data that I want you to train an image model from so I know some", "tokens": [12383, 9227, 300, 366, 1143, 281, 584, 510, 307, 264, 1412, 300, 286, 528, 291, 281, 3847, 364, 3256, 2316, 490, 370, 286, 458, 512], "temperature": 0.0, "avg_logprob": -0.16561051841094115, "compression_ratio": 1.8395061728395061, "no_speech_prob": 8.397989404329564e-06}, {"id": 22, "seek": 9600, "start": 114.14, "end": 119.58, "text": " Of you during the week went away and tried different data sets where you had", "tokens": [2720, 291, 1830, 264, 1243, 1437, 1314, 293, 3031, 819, 1412, 6352, 689, 291, 632], "temperature": 0.0, "avg_logprob": -0.16561051841094115, "compression_ratio": 1.8395061728395061, "no_speech_prob": 8.397989404329564e-06}, {"id": 23, "seek": 9600, "start": 120.32, "end": 123.92, "text": " folders with different sets of images in and created your own image classifiers", "tokens": [31082, 365, 819, 6352, 295, 5267, 294, 293, 2942, 428, 1065, 3256, 1508, 23463], "temperature": 0.0, "avg_logprob": -0.16561051841094115, "compression_ratio": 1.8395061728395061, "no_speech_prob": 8.397989404329564e-06}, {"id": 24, "seek": 12392, "start": 123.92, "end": 129.16, "text": " And generally that seems to be working pretty well from what I could see on the forums so to make it clear", "tokens": [400, 5101, 300, 2544, 281, 312, 1364, 1238, 731, 490, 437, 286, 727, 536, 322, 264, 26998, 370, 281, 652, 309, 1850], "temperature": 0.0, "avg_logprob": -0.2495340010699104, "compression_ratio": 1.5642201834862386, "no_speech_prob": 2.6016025458375225e-06}, {"id": 25, "seek": 12392, "start": 130.04, "end": 132.04, "text": " at this point this is", "tokens": [412, 341, 935, 341, 307], "temperature": 0.0, "avg_logprob": -0.2495340010699104, "compression_ratio": 1.5642201834862386, "no_speech_prob": 2.6016025458375225e-06}, {"id": 26, "seek": 12392, "start": 132.52, "end": 135.52, "text": " everything you need to to get started so if you", "tokens": [1203, 291, 643, 281, 281, 483, 1409, 370, 498, 291], "temperature": 0.0, "avg_logprob": -0.2495340010699104, "compression_ratio": 1.5642201834862386, "no_speech_prob": 2.6016025458375225e-06}, {"id": 27, "seek": 12392, "start": 136.24, "end": 141.66, "text": " Create your own folders with different sets of images. You know a few hundred", "tokens": [20248, 428, 1065, 31082, 365, 819, 6352, 295, 5267, 13, 509, 458, 257, 1326, 3262], "temperature": 0.0, "avg_logprob": -0.2495340010699104, "compression_ratio": 1.5642201834862386, "no_speech_prob": 2.6016025458375225e-06}, {"id": 28, "seek": 12392, "start": 142.24, "end": 144.8, "text": " Or a few thousand at each folder", "tokens": [1610, 257, 1326, 4714, 412, 1184, 10820], "temperature": 0.0, "avg_logprob": -0.2495340010699104, "compression_ratio": 1.5642201834862386, "no_speech_prob": 2.6016025458375225e-06}, {"id": 29, "seek": 12392, "start": 145.6, "end": 149.96, "text": " And run the same three lines of code that'll give you", "tokens": [400, 1190, 264, 912, 1045, 3876, 295, 3089, 300, 603, 976, 291], "temperature": 0.0, "avg_logprob": -0.2495340010699104, "compression_ratio": 1.5642201834862386, "no_speech_prob": 2.6016025458375225e-06}, {"id": 30, "seek": 14996, "start": 149.96, "end": 155.76000000000002, "text": " An image classifier, and you'll be able to see this third column tells you how accurate is", "tokens": [1107, 3256, 1508, 9902, 11, 293, 291, 603, 312, 1075, 281, 536, 341, 2636, 7738, 5112, 291, 577, 8559, 307], "temperature": 0.0, "avg_logprob": -0.1686534881591797, "compression_ratio": 1.6756756756756757, "no_speech_prob": 4.157336206844775e-06}, {"id": 31, "seek": 14996, "start": 157.44, "end": 159.28, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.1686534881591797, "compression_ratio": 1.6756756756756757, "no_speech_prob": 4.157336206844775e-06}, {"id": 32, "seek": 14996, "start": 159.28, "end": 161.28, "text": " we looked at", "tokens": [321, 2956, 412], "temperature": 0.0, "avg_logprob": -0.1686534881591797, "compression_ratio": 1.6756756756756757, "no_speech_prob": 4.157336206844775e-06}, {"id": 33, "seek": 14996, "start": 162.0, "end": 164.62, "text": " Some kind of simple visualizations to see like", "tokens": [2188, 733, 295, 2199, 5056, 14455, 281, 536, 411], "temperature": 0.0, "avg_logprob": -0.1686534881591797, "compression_ratio": 1.6756756756756757, "no_speech_prob": 4.157336206844775e-06}, {"id": 34, "seek": 14996, "start": 165.64000000000001, "end": 167.64000000000001, "text": " What was it uncertain about?", "tokens": [708, 390, 309, 11308, 466, 30], "temperature": 0.0, "avg_logprob": -0.1686534881591797, "compression_ratio": 1.6756756756756757, "no_speech_prob": 4.157336206844775e-06}, {"id": 35, "seek": 14996, "start": 167.68, "end": 171.44, "text": " What was it wrong about and so forth and that's always a really good idea", "tokens": [708, 390, 309, 2085, 466, 293, 370, 5220, 293, 300, 311, 1009, 257, 534, 665, 1558], "temperature": 0.0, "avg_logprob": -0.1686534881591797, "compression_ratio": 1.6756756756756757, "no_speech_prob": 4.157336206844775e-06}, {"id": 36, "seek": 14996, "start": 172.56, "end": 174.20000000000002, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.1686534881591797, "compression_ratio": 1.6756756756756757, "no_speech_prob": 4.157336206844775e-06}, {"id": 37, "seek": 17420, "start": 174.2, "end": 180.39999999999998, "text": " Then we learned about the the one key number you have to pick so this is this number here is the one key number is", "tokens": [1396, 321, 3264, 466, 264, 264, 472, 2141, 1230, 291, 362, 281, 1888, 370, 341, 307, 341, 1230, 510, 307, 264, 472, 2141, 1230, 307], "temperature": 0.0, "avg_logprob": -0.18121253338056742, "compression_ratio": 1.7, "no_speech_prob": 8.139512829075102e-06}, {"id": 38, "seek": 17420, "start": 180.67999999999998, "end": 186.33999999999997, "text": " 0.01 and this is called the learning rate and so I wanted to go over this again", "tokens": [1958, 13, 10607, 293, 341, 307, 1219, 264, 2539, 3314, 293, 370, 286, 1415, 281, 352, 670, 341, 797], "temperature": 0.0, "avg_logprob": -0.18121253338056742, "compression_ratio": 1.7, "no_speech_prob": 8.139512829075102e-06}, {"id": 39, "seek": 17420, "start": 187.16, "end": 189.95999999999998, "text": " And we'll learn about the theory behind what this is", "tokens": [400, 321, 603, 1466, 466, 264, 5261, 2261, 437, 341, 307], "temperature": 0.0, "avg_logprob": -0.18121253338056742, "compression_ratio": 1.7, "no_speech_prob": 8.139512829075102e-06}, {"id": 40, "seek": 17420, "start": 190.64, "end": 195.51999999999998, "text": " During the rest of the course in quite a lot of detail, but for now. I just wanted to talk about the practice", "tokens": [6842, 264, 1472, 295, 264, 1164, 294, 1596, 257, 688, 295, 2607, 11, 457, 337, 586, 13, 286, 445, 1415, 281, 751, 466, 264, 3124], "temperature": 0.0, "avg_logprob": -0.18121253338056742, "compression_ratio": 1.7, "no_speech_prob": 8.139512829075102e-06}, {"id": 41, "seek": 17420, "start": 198.79999999999998, "end": 200.79999999999998, "text": " Yes, you're next", "tokens": [1079, 11, 291, 434, 958], "temperature": 0.0, "avg_logprob": -0.18121253338056742, "compression_ratio": 1.7, "no_speech_prob": 8.139512829075102e-06}, {"id": 42, "seek": 20080, "start": 200.8, "end": 202.8, "text": " Oh", "tokens": [876], "temperature": 0.0, "avg_logprob": -0.3633269100654416, "compression_ratio": 1.6119402985074627, "no_speech_prob": 1.202925432153279e-05}, {"id": 43, "seek": 20080, "start": 203.68, "end": 207.48000000000002, "text": " They cannot see you in the video I guess and now I just turned it around", "tokens": [814, 2644, 536, 291, 294, 264, 960, 286, 2041, 293, 586, 286, 445, 3574, 309, 926], "temperature": 0.0, "avg_logprob": -0.3633269100654416, "compression_ratio": 1.6119402985074627, "no_speech_prob": 1.202925432153279e-05}, {"id": 44, "seek": 20080, "start": 209.48000000000002, "end": 214.60000000000002, "text": " Also was wondering could you tell us about the older three numbers in that?", "tokens": [2743, 390, 6359, 727, 291, 980, 505, 466, 264, 4906, 1045, 3547, 294, 300, 30], "temperature": 0.0, "avg_logprob": -0.3633269100654416, "compression_ratio": 1.6119402985074627, "no_speech_prob": 1.202925432153279e-05}, {"id": 45, "seek": 20080, "start": 215.20000000000002, "end": 220.56, "text": " We check these three here. We're going to talk about the other other ones shortly", "tokens": [492, 1520, 613, 1045, 510, 13, 492, 434, 516, 281, 751, 466, 264, 661, 661, 2306, 13392], "temperature": 0.0, "avg_logprob": -0.3633269100654416, "compression_ratio": 1.6119402985074627, "no_speech_prob": 1.202925432153279e-05}, {"id": 46, "seek": 20080, "start": 220.56, "end": 225.28, "text": " So the main one we're going to look at for now is is the last column which is the accuracy", "tokens": [407, 264, 2135, 472, 321, 434, 516, 281, 574, 412, 337, 586, 307, 307, 264, 1036, 7738, 597, 307, 264, 14170], "temperature": 0.0, "avg_logprob": -0.3633269100654416, "compression_ratio": 1.6119402985074627, "no_speech_prob": 1.202925432153279e-05}, {"id": 47, "seek": 22528, "start": 225.28, "end": 233.6, "text": " The first column as you can see is the epoch number so this tells us how many times has it been through the entire data set?", "tokens": [440, 700, 7738, 382, 291, 393, 536, 307, 264, 30992, 339, 1230, 370, 341, 5112, 505, 577, 867, 1413, 575, 309, 668, 807, 264, 2302, 1412, 992, 30], "temperature": 0.0, "avg_logprob": -0.1541416778564453, "compression_ratio": 1.934065934065934, "no_speech_prob": 1.2411227544362191e-05}, {"id": 48, "seek": 22528, "start": 234.64, "end": 240.24, "text": " Trying to learn a better classifier, and then the next two columns is what's called the loss which we'll be learning about", "tokens": [20180, 281, 1466, 257, 1101, 1508, 9902, 11, 293, 550, 264, 958, 732, 13766, 307, 437, 311, 1219, 264, 4470, 597, 321, 603, 312, 2539, 466], "temperature": 0.0, "avg_logprob": -0.1541416778564453, "compression_ratio": 1.934065934065934, "no_speech_prob": 1.2411227544362191e-05}, {"id": 49, "seek": 22528, "start": 241.16, "end": 245.4, "text": " Either later today or next week the first one is the loss on the training set", "tokens": [13746, 1780, 965, 420, 958, 1243, 264, 700, 472, 307, 264, 4470, 322, 264, 3097, 992], "temperature": 0.0, "avg_logprob": -0.1541416778564453, "compression_ratio": 1.934065934065934, "no_speech_prob": 1.2411227544362191e-05}, {"id": 50, "seek": 22528, "start": 245.52, "end": 249.18, "text": " These are the images that we're looking at in order to try to make a better classifier", "tokens": [1981, 366, 264, 5267, 300, 321, 434, 1237, 412, 294, 1668, 281, 853, 281, 652, 257, 1101, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.1541416778564453, "compression_ratio": 1.934065934065934, "no_speech_prob": 1.2411227544362191e-05}, {"id": 51, "seek": 22528, "start": 249.2, "end": 254.44, "text": " And the second is the loss on the validation set these are the images that we're not looking at when we're training", "tokens": [400, 264, 1150, 307, 264, 4470, 322, 264, 24071, 992, 613, 366, 264, 5267, 300, 321, 434, 406, 1237, 412, 562, 321, 434, 3097], "temperature": 0.0, "avg_logprob": -0.1541416778564453, "compression_ratio": 1.934065934065934, "no_speech_prob": 1.2411227544362191e-05}, {"id": 52, "seek": 25444, "start": 254.44, "end": 259.92, "text": " But we're just sitting on the side to see how accurate we are so we'll learn about the history and loss and accuracy", "tokens": [583, 321, 434, 445, 3798, 322, 264, 1252, 281, 536, 577, 8559, 321, 366, 370, 321, 603, 1466, 466, 264, 2503, 293, 4470, 293, 14170], "temperature": 0.0, "avg_logprob": -0.20256371565268072, "compression_ratio": 1.6547619047619047, "no_speech_prob": 7.889144399086945e-06}, {"id": 53, "seek": 25444, "start": 260.71999999999997, "end": 262.71999999999997, "text": " later", "tokens": [1780], "temperature": 0.0, "avg_logprob": -0.20256371565268072, "compression_ratio": 1.6547619047619047, "no_speech_prob": 7.889144399086945e-06}, {"id": 54, "seek": 25444, "start": 265.4, "end": 267.4, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.20256371565268072, "compression_ratio": 1.6547619047619047, "no_speech_prob": 7.889144399086945e-06}, {"id": 55, "seek": 25444, "start": 269.32, "end": 273.8, "text": " So we've got the epoch number the training loss is the second column the", "tokens": [407, 321, 600, 658, 264, 30992, 339, 1230, 264, 3097, 4470, 307, 264, 1150, 7738, 264], "temperature": 0.0, "avg_logprob": -0.20256371565268072, "compression_ratio": 1.6547619047619047, "no_speech_prob": 7.889144399086945e-06}, {"id": 56, "seek": 27380, "start": 273.8, "end": 283.92, "text": " Validation loss is the third column and the accuracy is the fourth column?", "tokens": [7188, 327, 399, 4470, 307, 264, 2636, 7738, 293, 264, 14170, 307, 264, 6409, 7738, 30], "temperature": 0.0, "avg_logprob": -0.2149341183324014, "compression_ratio": 1.6573426573426573, "no_speech_prob": 1.0845116094060359e-06}, {"id": 57, "seek": 27380, "start": 285.40000000000003, "end": 288.62, "text": " Okay, so the basic idea of the learning rate", "tokens": [1033, 11, 370, 264, 3875, 1558, 295, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.2149341183324014, "compression_ratio": 1.6573426573426573, "no_speech_prob": 1.0845116094060359e-06}, {"id": 58, "seek": 27380, "start": 294.52, "end": 301.48, "text": " So the basic idea of the learning rate is it's the thing that's going to decide how quickly do we zoom do we kind of?", "tokens": [407, 264, 3875, 1558, 295, 264, 2539, 3314, 307, 309, 311, 264, 551, 300, 311, 516, 281, 4536, 577, 2661, 360, 321, 8863, 360, 321, 733, 295, 30], "temperature": 0.0, "avg_logprob": -0.2149341183324014, "compression_ratio": 1.6573426573426573, "no_speech_prob": 1.0845116094060359e-06}, {"id": 59, "seek": 30148, "start": 301.48, "end": 308.20000000000005, "text": " Hone in on the solution and so I find that a good way to think about this is to think about like well", "tokens": [389, 546, 294, 322, 264, 3827, 293, 370, 286, 915, 300, 257, 665, 636, 281, 519, 466, 341, 307, 281, 519, 466, 411, 731], "temperature": 0.0, "avg_logprob": -0.1411030316593671, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.422173217084492e-06}, {"id": 60, "seek": 30148, "start": 308.20000000000005, "end": 310.20000000000005, "text": " What if we were trying to?", "tokens": [708, 498, 321, 645, 1382, 281, 30], "temperature": 0.0, "avg_logprob": -0.1411030316593671, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.422173217084492e-06}, {"id": 61, "seek": 30148, "start": 310.56, "end": 312.56, "text": " fit to a function", "tokens": [3318, 281, 257, 2445], "temperature": 0.0, "avg_logprob": -0.1411030316593671, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.422173217084492e-06}, {"id": 62, "seek": 30148, "start": 313.20000000000005, "end": 318.44, "text": " That looks something like this right we're trying to say okay. Where's whereabouts is the minimum point?", "tokens": [663, 1542, 746, 411, 341, 558, 321, 434, 1382, 281, 584, 1392, 13, 2305, 311, 689, 41620, 307, 264, 7285, 935, 30], "temperature": 0.0, "avg_logprob": -0.1411030316593671, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.422173217084492e-06}, {"id": 63, "seek": 30148, "start": 319.04, "end": 324.78000000000003, "text": " This is basically what we do when we do deep learning as we try to find the minimum point of a function", "tokens": [639, 307, 1936, 437, 321, 360, 562, 321, 360, 2452, 2539, 382, 321, 853, 281, 915, 264, 7285, 935, 295, 257, 2445], "temperature": 0.0, "avg_logprob": -0.1411030316593671, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.422173217084492e-06}, {"id": 64, "seek": 30148, "start": 326.36, "end": 328.36, "text": " Now our function happens to have", "tokens": [823, 527, 2445, 2314, 281, 362], "temperature": 0.0, "avg_logprob": -0.1411030316593671, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.422173217084492e-06}, {"id": 65, "seek": 32836, "start": 328.36, "end": 333.32, "text": " Millions or hundreds of millions of parameters, but it works the same basic way and so when we look at it", "tokens": [7190, 626, 420, 6779, 295, 6803, 295, 9834, 11, 457, 309, 1985, 264, 912, 3875, 636, 293, 370, 562, 321, 574, 412, 309], "temperature": 0.0, "avg_logprob": -0.13550184498662535, "compression_ratio": 1.7198443579766538, "no_speech_prob": 6.96218921802938e-06}, {"id": 66, "seek": 32836, "start": 333.32, "end": 336.92, "text": " You know we can immediately see that the lowest point is here", "tokens": [509, 458, 321, 393, 4258, 536, 300, 264, 12437, 935, 307, 510], "temperature": 0.0, "avg_logprob": -0.13550184498662535, "compression_ratio": 1.7198443579766538, "no_speech_prob": 6.96218921802938e-06}, {"id": 67, "seek": 32836, "start": 338.68, "end": 341.64, "text": " But how would you do that if you were a computer algorithm?", "tokens": [583, 577, 576, 291, 360, 300, 498, 291, 645, 257, 3820, 9284, 30], "temperature": 0.0, "avg_logprob": -0.13550184498662535, "compression_ratio": 1.7198443579766538, "no_speech_prob": 6.96218921802938e-06}, {"id": 68, "seek": 32836, "start": 341.64, "end": 345.6, "text": " And what we do is we we start out at some point at random", "tokens": [400, 437, 321, 360, 307, 321, 321, 722, 484, 412, 512, 935, 412, 4974], "temperature": 0.0, "avg_logprob": -0.13550184498662535, "compression_ratio": 1.7198443579766538, "no_speech_prob": 6.96218921802938e-06}, {"id": 69, "seek": 32836, "start": 345.6, "end": 348.88, "text": " So we pick say here, and we have a look and we say okay", "tokens": [407, 321, 1888, 584, 510, 11, 293, 321, 362, 257, 574, 293, 321, 584, 1392], "temperature": 0.0, "avg_logprob": -0.13550184498662535, "compression_ratio": 1.7198443579766538, "no_speech_prob": 6.96218921802938e-06}, {"id": 70, "seek": 32836, "start": 348.88, "end": 354.04, "text": " What's the what's the loss or the error at this point and we say what's the gradient in other words?", "tokens": [708, 311, 264, 437, 311, 264, 4470, 420, 264, 6713, 412, 341, 935, 293, 321, 584, 437, 311, 264, 16235, 294, 661, 2283, 30], "temperature": 0.0, "avg_logprob": -0.13550184498662535, "compression_ratio": 1.7198443579766538, "no_speech_prob": 6.96218921802938e-06}, {"id": 71, "seek": 35404, "start": 354.04, "end": 359.32, "text": " Which way is up and which way is down and it tells us that down is going to be in that direction", "tokens": [3013, 636, 307, 493, 293, 597, 636, 307, 760, 293, 309, 5112, 505, 300, 760, 307, 516, 281, 312, 294, 300, 3513], "temperature": 0.0, "avg_logprob": -0.22942659589979383, "compression_ratio": 1.90990990990991, "no_speech_prob": 3.7853071717108833e-06}, {"id": 72, "seek": 35404, "start": 359.44, "end": 362.74, "text": " And it also tells us how fast is it going down?", "tokens": [400, 309, 611, 5112, 505, 577, 2370, 307, 309, 516, 760, 30], "temperature": 0.0, "avg_logprob": -0.22942659589979383, "compression_ratio": 1.90990990990991, "no_speech_prob": 3.7853071717108833e-06}, {"id": 73, "seek": 35404, "start": 363.32, "end": 365.72, "text": " Which is at this point is going down pretty quickly", "tokens": [3013, 307, 412, 341, 935, 307, 516, 760, 1238, 2661], "temperature": 0.0, "avg_logprob": -0.22942659589979383, "compression_ratio": 1.90990990990991, "no_speech_prob": 3.7853071717108833e-06}, {"id": 74, "seek": 35404, "start": 366.52000000000004, "end": 373.84000000000003, "text": " And so then we take a step in the direction that's down and the distance we travel is going to be", "tokens": [400, 370, 550, 321, 747, 257, 1823, 294, 264, 3513, 300, 311, 760, 293, 264, 4560, 321, 3147, 307, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.22942659589979383, "compression_ratio": 1.90990990990991, "no_speech_prob": 3.7853071717108833e-06}, {"id": 75, "seek": 35404, "start": 374.0, "end": 378.6, "text": " Proportional to the gradient is going to be important how steep it is the idea is if it's deeper", "tokens": [21944, 477, 1966, 281, 264, 16235, 307, 516, 281, 312, 1021, 577, 16841, 309, 307, 264, 1558, 307, 498, 309, 311, 7731], "temperature": 0.0, "avg_logprob": -0.22942659589979383, "compression_ratio": 1.90990990990991, "no_speech_prob": 3.7853071717108833e-06}, {"id": 76, "seek": 35404, "start": 379.08000000000004, "end": 381.08000000000004, "text": " Then we're probably further away", "tokens": [1396, 321, 434, 1391, 3052, 1314], "temperature": 0.0, "avg_logprob": -0.22942659589979383, "compression_ratio": 1.90990990990991, "no_speech_prob": 3.7853071717108833e-06}, {"id": 77, "seek": 38108, "start": 381.08, "end": 386.08, "text": " That's the general idea right and so specifically what we do is we take the gradient", "tokens": [663, 311, 264, 2674, 1558, 558, 293, 370, 4682, 437, 321, 360, 307, 321, 747, 264, 16235], "temperature": 0.0, "avg_logprob": -0.13715314865112305, "compression_ratio": 1.9137254901960785, "no_speech_prob": 5.539165499612864e-07}, {"id": 78, "seek": 38108, "start": 386.08, "end": 390.82, "text": " Which is how steep is it at this point and we multiply it by some number and that numbers called the learning rate", "tokens": [3013, 307, 577, 16841, 307, 309, 412, 341, 935, 293, 321, 12972, 309, 538, 512, 1230, 293, 300, 3547, 1219, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.13715314865112305, "compression_ratio": 1.9137254901960785, "no_speech_prob": 5.539165499612864e-07}, {"id": 79, "seek": 38108, "start": 391.2, "end": 394.36, "text": " Okay, so if we pick a number that is", "tokens": [1033, 11, 370, 498, 321, 1888, 257, 1230, 300, 307], "temperature": 0.0, "avg_logprob": -0.13715314865112305, "compression_ratio": 1.9137254901960785, "no_speech_prob": 5.539165499612864e-07}, {"id": 80, "seek": 38108, "start": 395.28, "end": 400.96, "text": " Very small then we're guaranteed that we're going to go a little bit closer and a little bit closer and a little bit closer", "tokens": [4372, 1359, 550, 321, 434, 18031, 300, 321, 434, 516, 281, 352, 257, 707, 857, 4966, 293, 257, 707, 857, 4966, 293, 257, 707, 857, 4966], "temperature": 0.0, "avg_logprob": -0.13715314865112305, "compression_ratio": 1.9137254901960785, "no_speech_prob": 5.539165499612864e-07}, {"id": 81, "seek": 38108, "start": 400.96, "end": 403.9, "text": " Each time right, but it's going to take us a very long time", "tokens": [6947, 565, 558, 11, 457, 309, 311, 516, 281, 747, 505, 257, 588, 938, 565], "temperature": 0.0, "avg_logprob": -0.13715314865112305, "compression_ratio": 1.9137254901960785, "no_speech_prob": 5.539165499612864e-07}, {"id": 82, "seek": 38108, "start": 404.64, "end": 409.68, "text": " To eventually get to the bottom if we pick a number that's very big", "tokens": [1407, 4728, 483, 281, 264, 2767, 498, 321, 1888, 257, 1230, 300, 311, 588, 955], "temperature": 0.0, "avg_logprob": -0.13715314865112305, "compression_ratio": 1.9137254901960785, "no_speech_prob": 5.539165499612864e-07}, {"id": 83, "seek": 40968, "start": 409.68, "end": 415.28000000000003, "text": " We could actually step too far could go in the right direction, but we could step all the way over to here", "tokens": [492, 727, 767, 1823, 886, 1400, 727, 352, 294, 264, 558, 3513, 11, 457, 321, 727, 1823, 439, 264, 636, 670, 281, 510], "temperature": 0.0, "avg_logprob": -0.21552873365949876, "compression_ratio": 1.7583333333333333, "no_speech_prob": 8.186348736671789e-07}, {"id": 84, "seek": 40968, "start": 416.16, "end": 421.44, "text": " Right as a result of which we end up further away than we started and we could oscillate", "tokens": [1779, 382, 257, 1874, 295, 597, 321, 917, 493, 3052, 1314, 813, 321, 1409, 293, 321, 727, 18225, 473], "temperature": 0.0, "avg_logprob": -0.21552873365949876, "compression_ratio": 1.7583333333333333, "no_speech_prob": 8.186348736671789e-07}, {"id": 85, "seek": 40968, "start": 421.8, "end": 423.08, "text": " It gets worse and worse", "tokens": [467, 2170, 5324, 293, 5324], "temperature": 0.0, "avg_logprob": -0.21552873365949876, "compression_ratio": 1.7583333333333333, "no_speech_prob": 8.186348736671789e-07}, {"id": 86, "seek": 40968, "start": 423.08, "end": 428.68, "text": " So if you start training a neural net and you find that your accuracy or your loss is like", "tokens": [407, 498, 291, 722, 3097, 257, 18161, 2533, 293, 291, 915, 300, 428, 14170, 420, 428, 4470, 307, 411], "temperature": 0.0, "avg_logprob": -0.21552873365949876, "compression_ratio": 1.7583333333333333, "no_speech_prob": 8.186348736671789e-07}, {"id": 87, "seek": 40968, "start": 429.12, "end": 432.96000000000004, "text": " Spitting off into infinity almost certainly your learning rates too high", "tokens": [1738, 2414, 766, 666, 13202, 1920, 3297, 428, 2539, 6846, 886, 1090], "temperature": 0.0, "avg_logprob": -0.21552873365949876, "compression_ratio": 1.7583333333333333, "no_speech_prob": 8.186348736671789e-07}, {"id": 88, "seek": 40968, "start": 434.04, "end": 437.88, "text": " so in a sense learning rate too low is", "tokens": [370, 294, 257, 2020, 2539, 3314, 886, 2295, 307], "temperature": 0.0, "avg_logprob": -0.21552873365949876, "compression_ratio": 1.7583333333333333, "no_speech_prob": 8.186348736671789e-07}, {"id": 89, "seek": 43788, "start": 437.88, "end": 441.76, "text": " Is is a better problem to have because you're going to have to wait a long time", "tokens": [1119, 307, 257, 1101, 1154, 281, 362, 570, 291, 434, 516, 281, 362, 281, 1699, 257, 938, 565], "temperature": 0.0, "avg_logprob": -0.1807182879487345, "compression_ratio": 1.8134328358208955, "no_speech_prob": 2.295908416272141e-06}, {"id": 90, "seek": 43788, "start": 441.76, "end": 444.08, "text": " But wouldn't it be nice if there was a way to figure out like?", "tokens": [583, 2759, 380, 309, 312, 1481, 498, 456, 390, 257, 636, 281, 2573, 484, 411, 30], "temperature": 0.0, "avg_logprob": -0.1807182879487345, "compression_ratio": 1.8134328358208955, "no_speech_prob": 2.295908416272141e-06}, {"id": 91, "seek": 43788, "start": 444.64, "end": 450.68, "text": " What's the best learning rate something where you could kind of go quickly go like bomb bomb bomb?", "tokens": [708, 311, 264, 1151, 2539, 3314, 746, 689, 291, 727, 733, 295, 352, 2661, 352, 411, 7851, 7851, 7851, 30], "temperature": 0.0, "avg_logprob": -0.1807182879487345, "compression_ratio": 1.8134328358208955, "no_speech_prob": 2.295908416272141e-06}, {"id": 92, "seek": 43788, "start": 451.0, "end": 456.28, "text": " right and so that's why we use this thing called a learning rate finder and", "tokens": [558, 293, 370, 300, 311, 983, 321, 764, 341, 551, 1219, 257, 2539, 3314, 915, 260, 293], "temperature": 0.0, "avg_logprob": -0.1807182879487345, "compression_ratio": 1.8134328358208955, "no_speech_prob": 2.295908416272141e-06}, {"id": 93, "seek": 43788, "start": 456.52, "end": 461.15999999999997, "text": " What the learning rate finder does is it tries each each time?", "tokens": [708, 264, 2539, 3314, 915, 260, 775, 307, 309, 9898, 1184, 1184, 565, 30], "temperature": 0.0, "avg_logprob": -0.1807182879487345, "compression_ratio": 1.8134328358208955, "no_speech_prob": 2.295908416272141e-06}, {"id": 94, "seek": 43788, "start": 461.15999999999997, "end": 466.88, "text": " It looks at another remember the term mini batch and mini batch is a few images that we look at each time", "tokens": [467, 1542, 412, 1071, 1604, 264, 1433, 8382, 15245, 293, 8382, 15245, 307, 257, 1326, 5267, 300, 321, 574, 412, 1184, 565], "temperature": 0.0, "avg_logprob": -0.1807182879487345, "compression_ratio": 1.8134328358208955, "no_speech_prob": 2.295908416272141e-06}, {"id": 95, "seek": 46688, "start": 466.88, "end": 471.88, "text": " So that we're using the parallel processing power of the GPU effectively we look generally at around", "tokens": [407, 300, 321, 434, 1228, 264, 8952, 9007, 1347, 295, 264, 18407, 8659, 321, 574, 5101, 412, 926], "temperature": 0.0, "avg_logprob": -0.19332835985266644, "compression_ratio": 1.7759336099585061, "no_speech_prob": 1.7880588529806118e-06}, {"id": 96, "seek": 46688, "start": 472.4, "end": 477.84, "text": " 64 or 128 images at a time for each mini batch which is labeled here as an iteration", "tokens": [12145, 420, 29810, 5267, 412, 257, 565, 337, 1184, 8382, 15245, 597, 307, 21335, 510, 382, 364, 24784], "temperature": 0.0, "avg_logprob": -0.19332835985266644, "compression_ratio": 1.7759336099585061, "no_speech_prob": 1.7880588529806118e-06}, {"id": 97, "seek": 46688, "start": 478.84, "end": 482.88, "text": " We gradually increase the learning rate that multiplicatively increase the learning rate", "tokens": [492, 13145, 3488, 264, 2539, 3314, 300, 17596, 19020, 3488, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.19332835985266644, "compression_ratio": 1.7759336099585061, "no_speech_prob": 1.7880588529806118e-06}, {"id": 98, "seek": 46688, "start": 482.88, "end": 488.36, "text": " We start at really really tiny learning rates to make sure that we don't start at something too high", "tokens": [492, 722, 412, 534, 534, 5870, 2539, 6846, 281, 652, 988, 300, 321, 500, 380, 722, 412, 746, 886, 1090], "temperature": 0.0, "avg_logprob": -0.19332835985266644, "compression_ratio": 1.7759336099585061, "no_speech_prob": 1.7880588529806118e-06}, {"id": 99, "seek": 46688, "start": 488.36, "end": 492.15999999999997, "text": " And we gradually increase it and so the idea is that", "tokens": [400, 321, 13145, 3488, 309, 293, 370, 264, 1558, 307, 300], "temperature": 0.0, "avg_logprob": -0.19332835985266644, "compression_ratio": 1.7759336099585061, "no_speech_prob": 1.7880588529806118e-06}, {"id": 100, "seek": 49216, "start": 492.16, "end": 497.44, "text": " That eventually the learning rate will be so big that the loss will start getting worse", "tokens": [663, 4728, 264, 2539, 3314, 486, 312, 370, 955, 300, 264, 4470, 486, 722, 1242, 5324], "temperature": 0.0, "avg_logprob": -0.1797472018640018, "compression_ratio": 1.956896551724138, "no_speech_prob": 3.138115516776452e-06}, {"id": 101, "seek": 49216, "start": 497.44, "end": 500.8, "text": " and so what we're going to do then is we're going to look at the plot of", "tokens": [293, 370, 437, 321, 434, 516, 281, 360, 550, 307, 321, 434, 516, 281, 574, 412, 264, 7542, 295], "temperature": 0.0, "avg_logprob": -0.1797472018640018, "compression_ratio": 1.956896551724138, "no_speech_prob": 3.138115516776452e-06}, {"id": 102, "seek": 49216, "start": 501.96000000000004, "end": 506.24, "text": " learning rate against loss right so when the learning rates tiny", "tokens": [2539, 3314, 1970, 4470, 558, 370, 562, 264, 2539, 6846, 5870], "temperature": 0.0, "avg_logprob": -0.1797472018640018, "compression_ratio": 1.956896551724138, "no_speech_prob": 3.138115516776452e-06}, {"id": 103, "seek": 49216, "start": 507.28000000000003, "end": 510.48, "text": " It increases slowly then it starts to increase a bit faster", "tokens": [467, 8637, 5692, 550, 309, 3719, 281, 3488, 257, 857, 4663], "temperature": 0.0, "avg_logprob": -0.1797472018640018, "compression_ratio": 1.956896551724138, "no_speech_prob": 3.138115516776452e-06}, {"id": 104, "seek": 49216, "start": 510.48, "end": 515.12, "text": " And then eventually it starts not increasing as quickly and in fact it starts getting worse", "tokens": [400, 550, 4728, 309, 3719, 406, 5662, 382, 2661, 293, 294, 1186, 309, 3719, 1242, 5324], "temperature": 0.0, "avg_logprob": -0.1797472018640018, "compression_ratio": 1.956896551724138, "no_speech_prob": 3.138115516776452e-06}, {"id": 105, "seek": 49216, "start": 515.4, "end": 519.5600000000001, "text": " Right so clearly here and make sure you're you want to be familiar with this", "tokens": [1779, 370, 4448, 510, 293, 652, 988, 291, 434, 291, 528, 281, 312, 4963, 365, 341], "temperature": 0.0, "avg_logprob": -0.1797472018640018, "compression_ratio": 1.956896551724138, "no_speech_prob": 3.138115516776452e-06}, {"id": 106, "seek": 51956, "start": 519.56, "end": 521.1999999999999, "text": " scientific", "tokens": [8134], "temperature": 0.0, "avg_logprob": -0.3391491449796237, "compression_ratio": 1.8810810810810812, "no_speech_prob": 1.2029393474222161e-05}, {"id": 107, "seek": 51956, "start": 521.1999999999999, "end": 524.1199999999999, "text": " notation okay, so 10 to the negative 1 is", "tokens": [24657, 1392, 11, 370, 1266, 281, 264, 3671, 502, 307], "temperature": 0.0, "avg_logprob": -0.3391491449796237, "compression_ratio": 1.8810810810810812, "no_speech_prob": 1.2029393474222161e-05}, {"id": 108, "seek": 51956, "start": 525.3599999999999, "end": 531.3199999999999, "text": " 0.1 10 to the 0 is 1 10 to the negative 2 is 0.01 and when we write this in", "tokens": [1958, 13, 16, 1266, 281, 264, 1958, 307, 502, 1266, 281, 264, 3671, 568, 307, 1958, 13, 10607, 293, 562, 321, 2464, 341, 294], "temperature": 0.0, "avg_logprob": -0.3391491449796237, "compression_ratio": 1.8810810810810812, "no_speech_prob": 1.2029393474222161e-05}, {"id": 109, "seek": 51956, "start": 532.4, "end": 538.16, "text": " Python we'll generally write it like this rather than writing 10 to the negative 1 or 10 to the negative 2", "tokens": [15329, 321, 603, 5101, 2464, 309, 411, 341, 2831, 813, 3579, 1266, 281, 264, 3671, 502, 420, 1266, 281, 264, 3671, 568], "temperature": 0.0, "avg_logprob": -0.3391491449796237, "compression_ratio": 1.8810810810810812, "no_speech_prob": 1.2029393474222161e-05}, {"id": 110, "seek": 51956, "start": 538.2399999999999, "end": 540.2399999999999, "text": " We'll just write 1 a", "tokens": [492, 603, 445, 2464, 502, 257], "temperature": 0.0, "avg_logprob": -0.3391491449796237, "compression_ratio": 1.8810810810810812, "no_speech_prob": 1.2029393474222161e-05}, {"id": 111, "seek": 51956, "start": 540.7199999999999, "end": 542.9599999999999, "text": " Meg 1 or 1 a", "tokens": [9986, 502, 420, 502, 257], "temperature": 0.0, "avg_logprob": -0.3391491449796237, "compression_ratio": 1.8810810810810812, "no_speech_prob": 1.2029393474222161e-05}, {"id": 112, "seek": 51956, "start": 543.3599999999999, "end": 547.68, "text": " Meg 2 okay, I mean the same thing you're going to see that all the time so and", "tokens": [9986, 568, 1392, 11, 286, 914, 264, 912, 551, 291, 434, 516, 281, 536, 300, 439, 264, 565, 370, 293], "temperature": 0.0, "avg_logprob": -0.3391491449796237, "compression_ratio": 1.8810810810810812, "no_speech_prob": 1.2029393474222161e-05}, {"id": 113, "seek": 54768, "start": 547.68, "end": 552.3, "text": " Remember that equals 0.1. Oh point oh one", "tokens": [5459, 300, 6915, 1958, 13, 16, 13, 876, 935, 1954, 472], "temperature": 0.0, "avg_logprob": -0.2874788244565328, "compression_ratio": 1.8428571428571427, "no_speech_prob": 5.422165031632176e-06}, {"id": 114, "seek": 54768, "start": 555.1999999999999, "end": 556.76, "text": " Right", "tokens": [1779], "temperature": 0.0, "avg_logprob": -0.2874788244565328, "compression_ratio": 1.8428571428571427, "no_speech_prob": 5.422165031632176e-06}, {"id": 115, "seek": 54768, "start": 556.76, "end": 558.52, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2874788244565328, "compression_ratio": 1.8428571428571427, "no_speech_prob": 5.422165031632176e-06}, {"id": 116, "seek": 54768, "start": 558.52, "end": 563.68, "text": " Don't be confused by this text that it prints out here this this loss here is", "tokens": [1468, 380, 312, 9019, 538, 341, 2487, 300, 309, 22305, 484, 510, 341, 341, 4470, 510, 307], "temperature": 0.0, "avg_logprob": -0.2874788244565328, "compression_ratio": 1.8428571428571427, "no_speech_prob": 5.422165031632176e-06}, {"id": 117, "seek": 54768, "start": 563.92, "end": 569.92, "text": " The the final loss at the very end of it's not of any interest right so ignore this this is only interesting", "tokens": [440, 264, 2572, 4470, 412, 264, 588, 917, 295, 309, 311, 406, 295, 604, 1179, 558, 370, 11200, 341, 341, 307, 787, 1880], "temperature": 0.0, "avg_logprob": -0.2874788244565328, "compression_ratio": 1.8428571428571427, "no_speech_prob": 5.422165031632176e-06}, {"id": 118, "seek": 54768, "start": 570.3199999999999, "end": 576.9599999999999, "text": " When we're doing regular training, but not interesting for the learning rate finder the thing that's interesting for the learning rate finder is this", "tokens": [1133, 321, 434, 884, 3890, 3097, 11, 457, 406, 1880, 337, 264, 2539, 3314, 915, 260, 264, 551, 300, 311, 1880, 337, 264, 2539, 3314, 915, 260, 307, 341], "temperature": 0.0, "avg_logprob": -0.2874788244565328, "compression_ratio": 1.8428571428571427, "no_speech_prob": 5.422165031632176e-06}, {"id": 119, "seek": 57696, "start": 576.96, "end": 578.96, "text": " learn dot shed dot plot and", "tokens": [1466, 5893, 14951, 5893, 7542, 293], "temperature": 0.0, "avg_logprob": -0.2209812861222487, "compression_ratio": 1.7641921397379912, "no_speech_prob": 8.939561666920781e-06}, {"id": 120, "seek": 57696, "start": 579.64, "end": 584.2, "text": " Specifically we're not looking for the point where it's the lowest back to the point where it's the lowest", "tokens": [26058, 321, 434, 406, 1237, 337, 264, 935, 689, 309, 311, 264, 12437, 646, 281, 264, 935, 689, 309, 311, 264, 12437], "temperature": 0.0, "avg_logprob": -0.2209812861222487, "compression_ratio": 1.7641921397379912, "no_speech_prob": 8.939561666920781e-06}, {"id": 121, "seek": 57696, "start": 584.2, "end": 587.36, "text": " It's actually not getting better anymore, so that's too high a learning rate", "tokens": [467, 311, 767, 406, 1242, 1101, 3602, 11, 370, 300, 311, 886, 1090, 257, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.2209812861222487, "compression_ratio": 1.7641921397379912, "no_speech_prob": 8.939561666920781e-06}, {"id": 122, "seek": 57696, "start": 587.4000000000001, "end": 593.5400000000001, "text": " So I generally look to see like where is it the lowest and then I go back like one of magnitude so", "tokens": [407, 286, 5101, 574, 281, 536, 411, 689, 307, 309, 264, 12437, 293, 550, 286, 352, 646, 411, 472, 295, 15668, 370], "temperature": 0.0, "avg_logprob": -0.2209812861222487, "compression_ratio": 1.7641921397379912, "no_speech_prob": 8.939561666920781e-06}, {"id": 123, "seek": 57696, "start": 594.24, "end": 597.6, "text": " One e neg two would be a pretty good choice", "tokens": [1485, 308, 2485, 732, 576, 312, 257, 1238, 665, 3922], "temperature": 0.0, "avg_logprob": -0.2209812861222487, "compression_ratio": 1.7641921397379912, "no_speech_prob": 8.939561666920781e-06}, {"id": 124, "seek": 57696, "start": 598.12, "end": 602.9000000000001, "text": " Yeah, okay, so that's why you saw when we ran our", "tokens": [865, 11, 1392, 11, 370, 300, 311, 983, 291, 1866, 562, 321, 5872, 527], "temperature": 0.0, "avg_logprob": -0.2209812861222487, "compression_ratio": 1.7641921397379912, "no_speech_prob": 8.939561666920781e-06}, {"id": 125, "seek": 60290, "start": 602.9, "end": 608.4599999999999, "text": " Fit here we picked point oh one which is one a neg two", "tokens": [29263, 510, 321, 6183, 935, 1954, 472, 597, 307, 472, 257, 2485, 732], "temperature": 0.0, "avg_logprob": -0.22377411360593186, "compression_ratio": 1.6485355648535565, "no_speech_prob": 4.86041744807153e-06}, {"id": 126, "seek": 60290, "start": 610.18, "end": 616.98, "text": " So an important point to make here is like this. This is the one key number that we've learned to adjust", "tokens": [407, 364, 1021, 935, 281, 652, 510, 307, 411, 341, 13, 639, 307, 264, 472, 2141, 1230, 300, 321, 600, 3264, 281, 4369], "temperature": 0.0, "avg_logprob": -0.22377411360593186, "compression_ratio": 1.6485355648535565, "no_speech_prob": 4.86041744807153e-06}, {"id": 127, "seek": 60290, "start": 617.98, "end": 619.98, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.22377411360593186, "compression_ratio": 1.6485355648535565, "no_speech_prob": 4.86041744807153e-06}, {"id": 128, "seek": 60290, "start": 620.02, "end": 625.06, "text": " If you just adjust this number and nothing else most of the time you're going to be able to get pretty good results", "tokens": [759, 291, 445, 4369, 341, 1230, 293, 1825, 1646, 881, 295, 264, 565, 291, 434, 516, 281, 312, 1075, 281, 483, 1238, 665, 3542], "temperature": 0.0, "avg_logprob": -0.22377411360593186, "compression_ratio": 1.6485355648535565, "no_speech_prob": 4.86041744807153e-06}, {"id": 129, "seek": 60290, "start": 625.06, "end": 632.46, "text": " And this is like a very different message to what you would hear or see in any textbook or any video or any course", "tokens": [400, 341, 307, 411, 257, 588, 819, 3636, 281, 437, 291, 576, 1568, 420, 536, 294, 604, 25591, 420, 604, 960, 420, 604, 1164], "temperature": 0.0, "avg_logprob": -0.22377411360593186, "compression_ratio": 1.6485355648535565, "no_speech_prob": 4.86041744807153e-06}, {"id": 130, "seek": 63246, "start": 632.46, "end": 634.46, "text": " because", "tokens": [570], "temperature": 0.0, "avg_logprob": -0.15065231889781386, "compression_ratio": 1.7551020408163265, "no_speech_prob": 3.726619524968555e-06}, {"id": 131, "seek": 63246, "start": 634.7, "end": 641.7800000000001, "text": " Up until now there's been like dozens and dozens of these they're called hyper parameters dozens and dozens of hyper parameters to set and", "tokens": [5858, 1826, 586, 456, 311, 668, 411, 18431, 293, 18431, 295, 613, 436, 434, 1219, 9848, 9834, 18431, 293, 18431, 295, 9848, 9834, 281, 992, 293], "temperature": 0.0, "avg_logprob": -0.15065231889781386, "compression_ratio": 1.7551020408163265, "no_speech_prob": 3.726619524968555e-06}, {"id": 132, "seek": 63246, "start": 641.98, "end": 647.26, "text": " They've been thought of as highly sensitive and difficult to set so inside the fast AI library", "tokens": [814, 600, 668, 1194, 295, 382, 5405, 9477, 293, 2252, 281, 992, 370, 1854, 264, 2370, 7318, 6405], "temperature": 0.0, "avg_logprob": -0.15065231889781386, "compression_ratio": 1.7551020408163265, "no_speech_prob": 3.726619524968555e-06}, {"id": 133, "seek": 63246, "start": 647.34, "end": 649.82, "text": " We kind of do all that stuff for you", "tokens": [492, 733, 295, 360, 439, 300, 1507, 337, 291], "temperature": 0.0, "avg_logprob": -0.15065231889781386, "compression_ratio": 1.7551020408163265, "no_speech_prob": 3.726619524968555e-06}, {"id": 134, "seek": 63246, "start": 650.5, "end": 656.82, "text": " As much as we can and during the course we're going to learn that there are some more we can tweak to get slightly better results", "tokens": [1018, 709, 382, 321, 393, 293, 1830, 264, 1164, 321, 434, 516, 281, 1466, 300, 456, 366, 512, 544, 321, 393, 29879, 281, 483, 4748, 1101, 3542], "temperature": 0.0, "avg_logprob": -0.15065231889781386, "compression_ratio": 1.7551020408163265, "no_speech_prob": 3.726619524968555e-06}, {"id": 135, "seek": 63246, "start": 657.7800000000001, "end": 659.7800000000001, "text": " but it's kind of like", "tokens": [457, 309, 311, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.15065231889781386, "compression_ratio": 1.7551020408163265, "no_speech_prob": 3.726619524968555e-06}, {"id": 136, "seek": 65978, "start": 659.78, "end": 666.88, "text": " It's kind of in a funny situation here because for those of you that haven't done any deep learning before it's kind of like oh this is", "tokens": [467, 311, 733, 295, 294, 257, 4074, 2590, 510, 570, 337, 729, 295, 291, 300, 2378, 380, 1096, 604, 2452, 2539, 949, 309, 311, 733, 295, 411, 1954, 341, 307], "temperature": 0.0, "avg_logprob": -0.17493037021521365, "compression_ratio": 1.8546712802768166, "no_speech_prob": 2.902293999795802e-06}, {"id": 137, "seek": 65978, "start": 667.62, "end": 671.98, "text": " That's all there is to it. This is very easy and then when you talk to people outside this class", "tokens": [663, 311, 439, 456, 307, 281, 309, 13, 639, 307, 588, 1858, 293, 550, 562, 291, 751, 281, 561, 2380, 341, 1508], "temperature": 0.0, "avg_logprob": -0.17493037021521365, "compression_ratio": 1.8546712802768166, "no_speech_prob": 2.902293999795802e-06}, {"id": 138, "seek": 65978, "start": 671.98, "end": 677.1, "text": " They'll be like deep learning so difficult. There's so much to say it's a real art form and so that's why there's this", "tokens": [814, 603, 312, 411, 2452, 2539, 370, 2252, 13, 821, 311, 370, 709, 281, 584, 309, 311, 257, 957, 1523, 1254, 293, 370, 300, 311, 983, 456, 311, 341], "temperature": 0.0, "avg_logprob": -0.17493037021521365, "compression_ratio": 1.8546712802768166, "no_speech_prob": 2.902293999795802e-06}, {"id": 139, "seek": 65978, "start": 677.62, "end": 683.3199999999999, "text": " Difference right and so that the truth is that the learning rate really is the key thing to set and this ability to", "tokens": [35940, 5158, 558, 293, 370, 300, 264, 3494, 307, 300, 264, 2539, 3314, 534, 307, 264, 2141, 551, 281, 992, 293, 341, 3485, 281], "temperature": 0.0, "avg_logprob": -0.17493037021521365, "compression_ratio": 1.8546712802768166, "no_speech_prob": 2.902293999795802e-06}, {"id": 140, "seek": 65978, "start": 683.5, "end": 687.42, "text": " Use this trick to figure out how to set it although the paper is now", "tokens": [8278, 341, 4282, 281, 2573, 484, 577, 281, 992, 309, 4878, 264, 3035, 307, 586], "temperature": 0.0, "avg_logprob": -0.17493037021521365, "compression_ratio": 1.8546712802768166, "no_speech_prob": 2.902293999795802e-06}, {"id": 141, "seek": 68742, "start": 687.42, "end": 692.62, "text": " Probably 18 months old almost nobody knows about this paper", "tokens": [9210, 2443, 2493, 1331, 1920, 5079, 3255, 466, 341, 3035], "temperature": 0.0, "avg_logprob": -0.1743014749854502, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.5070557967410423e-05}, {"id": 142, "seek": 68742, "start": 692.62, "end": 698.5, "text": " And it was from a guy who's not from a famous research lab so most people kind of ignored it and in fact even this", "tokens": [400, 309, 390, 490, 257, 2146, 567, 311, 406, 490, 257, 4618, 2132, 2715, 370, 881, 561, 733, 295, 19735, 309, 293, 294, 1186, 754, 341], "temperature": 0.0, "avg_logprob": -0.1743014749854502, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.5070557967410423e-05}, {"id": 143, "seek": 68742, "start": 698.5, "end": 701.86, "text": " Particular technique was one sub part of a paper that was about something else", "tokens": [4100, 14646, 6532, 390, 472, 1422, 644, 295, 257, 3035, 300, 390, 466, 746, 1646], "temperature": 0.0, "avg_logprob": -0.1743014749854502, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.5070557967410423e-05}, {"id": 144, "seek": 68742, "start": 702.8199999999999, "end": 706.0999999999999, "text": " So again this idea of like this is how you can set the learning rate", "tokens": [407, 797, 341, 1558, 295, 411, 341, 307, 577, 291, 393, 992, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.1743014749854502, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.5070557967410423e-05}, {"id": 145, "seek": 68742, "start": 707.18, "end": 713.42, "text": " Really nobody outside this classroom just about knows about it obviously the guy who wrote it Leslie Smith knows about it", "tokens": [4083, 5079, 2380, 341, 7419, 445, 466, 3255, 466, 309, 2745, 264, 2146, 567, 4114, 309, 28140, 8538, 3255, 466, 309], "temperature": 0.0, "avg_logprob": -0.1743014749854502, "compression_ratio": 1.7142857142857142, "no_speech_prob": 2.5070557967410423e-05}, {"id": 146, "seek": 71342, "start": 713.42, "end": 720.62, "text": " So it's a good thing to tell your colleagues about this like here is actually a great way to set the learning rate and", "tokens": [407, 309, 311, 257, 665, 551, 281, 980, 428, 7734, 466, 341, 411, 510, 307, 767, 257, 869, 636, 281, 992, 264, 2539, 3314, 293], "temperature": 0.0, "avg_logprob": -0.14186510033563737, "compression_ratio": 1.82421875, "no_speech_prob": 7.766827366140205e-06}, {"id": 147, "seek": 71342, "start": 721.74, "end": 726.4599999999999, "text": " There's even been papers called like one of the famous papers is called no more pesky learning rates", "tokens": [821, 311, 754, 668, 10577, 1219, 411, 472, 295, 264, 4618, 10577, 307, 1219, 572, 544, 9262, 4133, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.14186510033563737, "compression_ratio": 1.82421875, "no_speech_prob": 7.766827366140205e-06}, {"id": 148, "seek": 71342, "start": 726.9, "end": 730.02, "text": " Which actually is a less effective technique than this one", "tokens": [3013, 767, 307, 257, 1570, 4942, 6532, 813, 341, 472], "temperature": 0.0, "avg_logprob": -0.14186510033563737, "compression_ratio": 1.82421875, "no_speech_prob": 7.766827366140205e-06}, {"id": 149, "seek": 71342, "start": 730.02, "end": 737.4399999999999, "text": " But this idea that like setting learning rates is is very difficult and fiddly is has been true for most of the kind of deep", "tokens": [583, 341, 1558, 300, 411, 3287, 2539, 6846, 307, 307, 588, 2252, 293, 283, 14273, 356, 307, 575, 668, 2074, 337, 881, 295, 264, 733, 295, 2452], "temperature": 0.0, "avg_logprob": -0.14186510033563737, "compression_ratio": 1.82421875, "no_speech_prob": 7.766827366140205e-06}, {"id": 150, "seek": 71342, "start": 737.4399999999999, "end": 739.02, "text": " learning history", "tokens": [2539, 2503], "temperature": 0.0, "avg_logprob": -0.14186510033563737, "compression_ratio": 1.82421875, "no_speech_prob": 7.766827366140205e-06}, {"id": 151, "seek": 71342, "start": 739.02, "end": 741.4599999999999, "text": " So here's the trick right go look at this plot", "tokens": [407, 510, 311, 264, 4282, 558, 352, 574, 412, 341, 7542], "temperature": 0.0, "avg_logprob": -0.14186510033563737, "compression_ratio": 1.82421875, "no_speech_prob": 7.766827366140205e-06}, {"id": 152, "seek": 74146, "start": 741.46, "end": 746.3000000000001, "text": " I'll find kind of the lowest to go back about a multiple of 10 and try that", "tokens": [286, 603, 915, 733, 295, 264, 12437, 281, 352, 646, 466, 257, 3866, 295, 1266, 293, 853, 300], "temperature": 0.0, "avg_logprob": -0.2294747887588129, "compression_ratio": 1.6142857142857143, "no_speech_prob": 2.2125184841570444e-05}, {"id": 153, "seek": 74146, "start": 746.58, "end": 751.22, "text": " Right and if that doesn't quite work you can always try you know going back another multiple of 10", "tokens": [1779, 293, 498, 300, 1177, 380, 1596, 589, 291, 393, 1009, 853, 291, 458, 516, 646, 1071, 3866, 295, 1266], "temperature": 0.0, "avg_logprob": -0.2294747887588129, "compression_ratio": 1.6142857142857143, "no_speech_prob": 2.2125184841570444e-05}, {"id": 154, "seek": 74146, "start": 751.22, "end": 753.22, "text": " But this has always worked for me so far", "tokens": [583, 341, 575, 1009, 2732, 337, 385, 370, 1400], "temperature": 0.0, "avg_logprob": -0.2294747887588129, "compression_ratio": 1.6142857142857143, "no_speech_prob": 2.2125184841570444e-05}, {"id": 155, "seek": 74146, "start": 760.0600000000001, "end": 761.3000000000001, "text": " What's", "tokens": [708, 311], "temperature": 0.0, "avg_logprob": -0.2294747887588129, "compression_ratio": 1.6142857142857143, "no_speech_prob": 2.2125184841570444e-05}, {"id": 156, "seek": 74146, "start": 761.3000000000001, "end": 768.22, "text": " Why does this learning rate this method work versus something else like momentum base or what's like the advantages?", "tokens": [1545, 775, 341, 2539, 3314, 341, 3170, 589, 5717, 746, 1646, 411, 11244, 3096, 420, 437, 311, 411, 264, 14906, 30], "temperature": 0.0, "avg_logprob": -0.2294747887588129, "compression_ratio": 1.6142857142857143, "no_speech_prob": 2.2125184841570444e-05}, {"id": 157, "seek": 76822, "start": 768.22, "end": 772.44, "text": " It is advantageous with this learning rate like technique versus scales", "tokens": [467, 307, 5002, 563, 365, 341, 2539, 3314, 411, 6532, 5717, 17408], "temperature": 0.0, "avg_logprob": -0.24518108367919922, "compression_ratio": 1.5898617511520738, "no_speech_prob": 1.0783164725580718e-05}, {"id": 158, "seek": 76822, "start": 778.62, "end": 780.6600000000001, "text": " That's a great question so", "tokens": [663, 311, 257, 869, 1168, 370], "temperature": 0.0, "avg_logprob": -0.24518108367919922, "compression_ratio": 1.5898617511520738, "no_speech_prob": 1.0783164725580718e-05}, {"id": 159, "seek": 76822, "start": 780.6600000000001, "end": 787.94, "text": " We're going to learn during this course about a number of ways of improving gradient descent like you mentioned momentum and Adam and so forth", "tokens": [492, 434, 516, 281, 1466, 1830, 341, 1164, 466, 257, 1230, 295, 2098, 295, 11470, 16235, 23475, 411, 291, 2835, 11244, 293, 7938, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.24518108367919922, "compression_ratio": 1.5898617511520738, "no_speech_prob": 1.0783164725580718e-05}, {"id": 160, "seek": 76822, "start": 788.14, "end": 794.0600000000001, "text": " This is orthogonal in fact so one of the things the fast AI library tries to do is figure out the right", "tokens": [639, 307, 41488, 294, 1186, 370, 472, 295, 264, 721, 264, 2370, 7318, 6405, 9898, 281, 360, 307, 2573, 484, 264, 558], "temperature": 0.0, "avg_logprob": -0.24518108367919922, "compression_ratio": 1.5898617511520738, "no_speech_prob": 1.0783164725580718e-05}, {"id": 161, "seek": 79406, "start": 794.06, "end": 798.9399999999999, "text": " gradient descent version and in fact behind the scenes this is actually using something called Adam and", "tokens": [16235, 23475, 3037, 293, 294, 1186, 2261, 264, 8026, 341, 307, 767, 1228, 746, 1219, 7938, 293], "temperature": 0.0, "avg_logprob": -0.16757644525095194, "compression_ratio": 1.8374558303886925, "no_speech_prob": 4.860411081608618e-06}, {"id": 162, "seek": 79406, "start": 799.4599999999999, "end": 802.8199999999999, "text": " So this technique is telling us. This is the best learning rate to use", "tokens": [407, 341, 6532, 307, 3585, 505, 13, 639, 307, 264, 1151, 2539, 3314, 281, 764], "temperature": 0.0, "avg_logprob": -0.16757644525095194, "compression_ratio": 1.8374558303886925, "no_speech_prob": 4.860411081608618e-06}, {"id": 163, "seek": 79406, "start": 803.66, "end": 804.7399999999999, "text": " given", "tokens": [2212], "temperature": 0.0, "avg_logprob": -0.16757644525095194, "compression_ratio": 1.8374558303886925, "no_speech_prob": 4.860411081608618e-06}, {"id": 164, "seek": 79406, "start": 804.7399999999999, "end": 808.42, "text": " What are the other tweaks you're using in this case the atom optimizer?", "tokens": [708, 366, 264, 661, 46664, 291, 434, 1228, 294, 341, 1389, 264, 12018, 5028, 6545, 30], "temperature": 0.0, "avg_logprob": -0.16757644525095194, "compression_ratio": 1.8374558303886925, "no_speech_prob": 4.860411081608618e-06}, {"id": 165, "seek": 79406, "start": 808.54, "end": 814.5, "text": " So it's not that there's some compromise between this and some other approaches this sits on top of those approaches", "tokens": [407, 309, 311, 406, 300, 456, 311, 512, 18577, 1296, 341, 293, 512, 661, 11587, 341, 12696, 322, 1192, 295, 729, 11587], "temperature": 0.0, "avg_logprob": -0.16757644525095194, "compression_ratio": 1.8374558303886925, "no_speech_prob": 4.860411081608618e-06}, {"id": 166, "seek": 79406, "start": 814.5, "end": 817.4599999999999, "text": " And you still have to set the learning rate when you use with other approaches", "tokens": [400, 291, 920, 362, 281, 992, 264, 2539, 3314, 562, 291, 764, 365, 661, 11587], "temperature": 0.0, "avg_logprob": -0.16757644525095194, "compression_ratio": 1.8374558303886925, "no_speech_prob": 4.860411081608618e-06}, {"id": 167, "seek": 79406, "start": 817.6999999999999, "end": 821.5799999999999, "text": " So we're trying to find the best kind of optimizer to use for a problem", "tokens": [407, 321, 434, 1382, 281, 915, 264, 1151, 733, 295, 5028, 6545, 281, 764, 337, 257, 1154], "temperature": 0.0, "avg_logprob": -0.16757644525095194, "compression_ratio": 1.8374558303886925, "no_speech_prob": 4.860411081608618e-06}, {"id": 168, "seek": 82158, "start": 821.58, "end": 826.58, "text": " But you still have to set the learning rate, and this is how we can do it and in fact this idea of using this", "tokens": [583, 291, 920, 362, 281, 992, 264, 2539, 3314, 11, 293, 341, 307, 577, 321, 393, 360, 309, 293, 294, 1186, 341, 1558, 295, 1228, 341], "temperature": 0.0, "avg_logprob": -0.1510647227255146, "compression_ratio": 1.5720524017467248, "no_speech_prob": 1.4738462596142199e-05}, {"id": 169, "seek": 82158, "start": 826.58, "end": 828.24, "text": " technique on top of", "tokens": [6532, 322, 1192, 295], "temperature": 0.0, "avg_logprob": -0.1510647227255146, "compression_ratio": 1.5720524017467248, "no_speech_prob": 1.4738462596142199e-05}, {"id": 170, "seek": 82158, "start": 828.24, "end": 834.34, "text": " More advanced optimizers like Adam might haven't even seen mentioned in a paper before so I think this is like a it's not a", "tokens": [5048, 7339, 5028, 22525, 411, 7938, 1062, 2378, 380, 754, 1612, 2835, 294, 257, 3035, 949, 370, 286, 519, 341, 307, 411, 257, 309, 311, 406, 257], "temperature": 0.0, "avg_logprob": -0.1510647227255146, "compression_ratio": 1.5720524017467248, "no_speech_prob": 1.4738462596142199e-05}, {"id": 171, "seek": 82158, "start": 834.58, "end": 840.5600000000001, "text": " Huge breakthrough it seems obvious, but nobody else seems to have tried it so as you can see it works well", "tokens": [37043, 22397, 309, 2544, 6322, 11, 457, 5079, 1646, 2544, 281, 362, 3031, 309, 370, 382, 291, 393, 536, 309, 1985, 731], "temperature": 0.0, "avg_logprob": -0.1510647227255146, "compression_ratio": 1.5720524017467248, "no_speech_prob": 1.4738462596142199e-05}, {"id": 172, "seek": 84056, "start": 840.56, "end": 851.52, "text": " When we use optimizers like Adam which are like adaptive learning rates, so when we set this learning rate", "tokens": [1133, 321, 764, 5028, 22525, 411, 7938, 597, 366, 411, 27912, 2539, 6846, 11, 370, 562, 321, 992, 341, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.24622131255735835, "compression_ratio": 1.7733990147783252, "no_speech_prob": 4.222779352858197e-06}, {"id": 173, "seek": 84056, "start": 851.52, "end": 854.92, "text": " Is it like initial learning rate because it changes during the people?", "tokens": [1119, 309, 411, 5883, 2539, 3314, 570, 309, 2962, 1830, 264, 561, 30], "temperature": 0.0, "avg_logprob": -0.24622131255735835, "compression_ratio": 1.7733990147783252, "no_speech_prob": 4.222779352858197e-06}, {"id": 174, "seek": 84056, "start": 859.04, "end": 861.0, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.24622131255735835, "compression_ratio": 1.7733990147783252, "no_speech_prob": 4.222779352858197e-06}, {"id": 175, "seek": 84056, "start": 861.0, "end": 865.56, "text": " We're going to be learning about things like Adam the details about it later in the class", "tokens": [492, 434, 516, 281, 312, 2539, 466, 721, 411, 7938, 264, 4365, 466, 309, 1780, 294, 264, 1508], "temperature": 0.0, "avg_logprob": -0.24622131255735835, "compression_ratio": 1.7733990147783252, "no_speech_prob": 4.222779352858197e-06}, {"id": 176, "seek": 84056, "start": 865.56, "end": 869.68, "text": " But the basic answer is no even with even the atom that there actually is a learning rate", "tokens": [583, 264, 3875, 1867, 307, 572, 754, 365, 754, 264, 12018, 300, 456, 767, 307, 257, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.24622131255735835, "compression_ratio": 1.7733990147783252, "no_speech_prob": 4.222779352858197e-06}, {"id": 177, "seek": 86968, "start": 869.68, "end": 871.8, "text": " It's just being", "tokens": [467, 311, 445, 885], "temperature": 0.0, "avg_logprob": -0.2200619684506769, "compression_ratio": 1.654054054054054, "no_speech_prob": 2.9309863748494536e-05}, {"id": 178, "seek": 86968, "start": 874.64, "end": 877.7399999999999, "text": " It's being basically divided by the the gradient", "tokens": [467, 311, 885, 1936, 6666, 538, 264, 264, 16235], "temperature": 0.0, "avg_logprob": -0.2200619684506769, "compression_ratio": 1.654054054054054, "no_speech_prob": 2.9309863748494536e-05}, {"id": 179, "seek": 86968, "start": 878.68, "end": 883.16, "text": " The average previous gradient and also the recent summer squareds of gradients", "tokens": [440, 4274, 3894, 16235, 293, 611, 264, 5162, 4266, 8889, 82, 295, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.2200619684506769, "compression_ratio": 1.654054054054054, "no_speech_prob": 2.9309863748494536e-05}, {"id": 180, "seek": 86968, "start": 883.16, "end": 885.9599999999999, "text": " So there's still like a number called the learning rate there", "tokens": [407, 456, 311, 920, 411, 257, 1230, 1219, 264, 2539, 3314, 456], "temperature": 0.0, "avg_logprob": -0.2200619684506769, "compression_ratio": 1.654054054054054, "no_speech_prob": 2.9309863748494536e-05}, {"id": 181, "seek": 86968, "start": 885.9599999999999, "end": 891.8199999999999, "text": " There isn't a even these so-called dynamic learning rate methods still have a learning rate", "tokens": [821, 1943, 380, 257, 754, 613, 370, 12, 11880, 8546, 2539, 3314, 7150, 920, 362, 257, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.2200619684506769, "compression_ratio": 1.654054054054054, "no_speech_prob": 2.9309863748494536e-05}, {"id": 182, "seek": 89182, "start": 891.82, "end": 896.74, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.21596466285594995, "compression_ratio": 1.4833333333333334, "no_speech_prob": 2.090437874358031e-06}, {"id": 183, "seek": 89182, "start": 899.82, "end": 901.82, "text": " The most important", "tokens": [440, 881, 1021], "temperature": 0.0, "avg_logprob": -0.21596466285594995, "compression_ratio": 1.4833333333333334, "no_speech_prob": 2.090437874358031e-06}, {"id": 184, "seek": 89182, "start": 902.7, "end": 905.46, "text": " Thing that you can do to make your model better", "tokens": [30902, 300, 291, 393, 360, 281, 652, 428, 2316, 1101], "temperature": 0.0, "avg_logprob": -0.21596466285594995, "compression_ratio": 1.4833333333333334, "no_speech_prob": 2.090437874358031e-06}, {"id": 185, "seek": 89182, "start": 906.2600000000001, "end": 908.6600000000001, "text": " Is to give it more data", "tokens": [1119, 281, 976, 309, 544, 1412], "temperature": 0.0, "avg_logprob": -0.21596466285594995, "compression_ratio": 1.4833333333333334, "no_speech_prob": 2.090437874358031e-06}, {"id": 186, "seek": 89182, "start": 909.3000000000001, "end": 914.58, "text": " So the challenge that happens is that these models have hundreds of millions of parameters", "tokens": [407, 264, 3430, 300, 2314, 307, 300, 613, 5245, 362, 6779, 295, 6803, 295, 9834], "temperature": 0.0, "avg_logprob": -0.21596466285594995, "compression_ratio": 1.4833333333333334, "no_speech_prob": 2.090437874358031e-06}, {"id": 187, "seek": 89182, "start": 915.62, "end": 920.62, "text": " And if you train them for a while they start to do what's called overfitting", "tokens": [400, 498, 291, 3847, 552, 337, 257, 1339, 436, 722, 281, 360, 437, 311, 1219, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.21596466285594995, "compression_ratio": 1.4833333333333334, "no_speech_prob": 2.090437874358031e-06}, {"id": 188, "seek": 92062, "start": 920.62, "end": 926.7, "text": " And so overfitting means that they're going to start to see like the specific details of the images you're giving them", "tokens": [400, 370, 670, 69, 2414, 1355, 300, 436, 434, 516, 281, 722, 281, 536, 411, 264, 2685, 4365, 295, 264, 5267, 291, 434, 2902, 552], "temperature": 0.0, "avg_logprob": -0.1334100740145793, "compression_ratio": 1.828996282527881, "no_speech_prob": 3.6688052205136046e-06}, {"id": 189, "seek": 92062, "start": 926.74, "end": 928.74, "text": " rather than the more general", "tokens": [2831, 813, 264, 544, 2674], "temperature": 0.0, "avg_logprob": -0.1334100740145793, "compression_ratio": 1.828996282527881, "no_speech_prob": 3.6688052205136046e-06}, {"id": 190, "seek": 92062, "start": 930.34, "end": 932.88, "text": " Learning that can transfer across to the validation center", "tokens": [15205, 300, 393, 5003, 2108, 281, 264, 24071, 3056], "temperature": 0.0, "avg_logprob": -0.1334100740145793, "compression_ratio": 1.828996282527881, "no_speech_prob": 3.6688052205136046e-06}, {"id": 191, "seek": 92062, "start": 933.54, "end": 938.86, "text": " So the best thing we can do to avoid overfitting is to find more data now", "tokens": [407, 264, 1151, 551, 321, 393, 360, 281, 5042, 670, 69, 2414, 307, 281, 915, 544, 1412, 586], "temperature": 0.0, "avg_logprob": -0.1334100740145793, "compression_ratio": 1.828996282527881, "no_speech_prob": 3.6688052205136046e-06}, {"id": 192, "seek": 92062, "start": 938.86, "end": 943.34, "text": " Obviously one way to do that would just be to collect more data from wherever you're getting it from or label more data", "tokens": [7580, 472, 636, 281, 360, 300, 576, 445, 312, 281, 2500, 544, 1412, 490, 8660, 291, 434, 1242, 309, 490, 420, 7645, 544, 1412], "temperature": 0.0, "avg_logprob": -0.1334100740145793, "compression_ratio": 1.828996282527881, "no_speech_prob": 3.6688052205136046e-06}, {"id": 193, "seek": 92062, "start": 943.78, "end": 949.14, "text": " But a really easy way that we should always do is to use something called data augmentation", "tokens": [583, 257, 534, 1858, 636, 300, 321, 820, 1009, 360, 307, 281, 764, 746, 1219, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.1334100740145793, "compression_ratio": 1.828996282527881, "no_speech_prob": 3.6688052205136046e-06}, {"id": 194, "seek": 94914, "start": 949.14, "end": 951.14, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.1693853295367697, "compression_ratio": 1.6908396946564885, "no_speech_prob": 2.8573015242727706e-06}, {"id": 195, "seek": 94914, "start": 951.14, "end": 953.74, "text": " Data augmentation is one of these things that's", "tokens": [11888, 14501, 19631, 307, 472, 295, 613, 721, 300, 311], "temperature": 0.0, "avg_logprob": -0.1693853295367697, "compression_ratio": 1.6908396946564885, "no_speech_prob": 2.8573015242727706e-06}, {"id": 196, "seek": 94914, "start": 954.54, "end": 959.54, "text": " In many courses it's not even mentioned at all or if it is it's kind of like an advanced topic right at the end", "tokens": [682, 867, 7712, 309, 311, 406, 754, 2835, 412, 439, 420, 498, 309, 307, 309, 311, 733, 295, 411, 364, 7339, 4829, 558, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.1693853295367697, "compression_ratio": 1.6908396946564885, "no_speech_prob": 2.8573015242727706e-06}, {"id": 197, "seek": 94914, "start": 959.54, "end": 964.18, "text": " But actually it's like the most important thing that you can do to make a better model", "tokens": [583, 767, 309, 311, 411, 264, 881, 1021, 551, 300, 291, 393, 360, 281, 652, 257, 1101, 2316], "temperature": 0.0, "avg_logprob": -0.1693853295367697, "compression_ratio": 1.6908396946564885, "no_speech_prob": 2.8573015242727706e-06}, {"id": 198, "seek": 94914, "start": 964.26, "end": 969.86, "text": " Okay, and so it's built into the fast AI library to make it very easy to do and so we're going to look at the details", "tokens": [1033, 11, 293, 370, 309, 311, 3094, 666, 264, 2370, 7318, 6405, 281, 652, 309, 588, 1858, 281, 360, 293, 370, 321, 434, 516, 281, 574, 412, 264, 4365], "temperature": 0.0, "avg_logprob": -0.1693853295367697, "compression_ratio": 1.6908396946564885, "no_speech_prob": 2.8573015242727706e-06}, {"id": 199, "seek": 94914, "start": 969.86, "end": 976.34, "text": " Of the code shortly, but the basic idea is that at this in our initial code", "tokens": [2720, 264, 3089, 13392, 11, 457, 264, 3875, 1558, 307, 300, 412, 341, 294, 527, 5883, 3089], "temperature": 0.0, "avg_logprob": -0.1693853295367697, "compression_ratio": 1.6908396946564885, "no_speech_prob": 2.8573015242727706e-06}, {"id": 200, "seek": 97634, "start": 976.34, "end": 978.34, "text": " we", "tokens": [321], "temperature": 0.0, "avg_logprob": -0.21873451232910157, "compression_ratio": 1.7296137339055795, "no_speech_prob": 6.962174211366801e-06}, {"id": 201, "seek": 97634, "start": 978.58, "end": 986.1, "text": " Had a line that said image classifier data from paths, and we passed in the path to our data and for transforms", "tokens": [12298, 257, 1622, 300, 848, 3256, 1508, 9902, 1412, 490, 14518, 11, 293, 321, 4678, 294, 264, 3100, 281, 527, 1412, 293, 337, 35592], "temperature": 0.0, "avg_logprob": -0.21873451232910157, "compression_ratio": 1.7296137339055795, "no_speech_prob": 6.962174211366801e-06}, {"id": 202, "seek": 97634, "start": 986.1, "end": 988.1, "text": " We passed in basically", "tokens": [492, 4678, 294, 1936], "temperature": 0.0, "avg_logprob": -0.21873451232910157, "compression_ratio": 1.7296137339055795, "no_speech_prob": 6.962174211366801e-06}, {"id": 203, "seek": 97634, "start": 988.3000000000001, "end": 991.9, "text": " The size in the architecture we'll look at this in more detail shortly", "tokens": [440, 2744, 294, 264, 9482, 321, 603, 574, 412, 341, 294, 544, 2607, 13392], "temperature": 0.0, "avg_logprob": -0.21873451232910157, "compression_ratio": 1.7296137339055795, "no_speech_prob": 6.962174211366801e-06}, {"id": 204, "seek": 97634, "start": 992.1800000000001, "end": 997.84, "text": " We just add one more parameter which is what kind of data augmentation do you want to do?", "tokens": [492, 445, 909, 472, 544, 13075, 597, 307, 437, 733, 295, 1412, 14501, 19631, 360, 291, 528, 281, 360, 30], "temperature": 0.0, "avg_logprob": -0.21873451232910157, "compression_ratio": 1.7296137339055795, "no_speech_prob": 6.962174211366801e-06}, {"id": 205, "seek": 97634, "start": 998.58, "end": 1000.0600000000001, "text": " and so", "tokens": [293, 370], "temperature": 0.0, "avg_logprob": -0.21873451232910157, "compression_ratio": 1.7296137339055795, "no_speech_prob": 6.962174211366801e-06}, {"id": 206, "seek": 97634, "start": 1000.0600000000001, "end": 1005.3000000000001, "text": " To understand data augmentation. It's maybe easiest to look at some pictures of data augmentation", "tokens": [1407, 1223, 1412, 14501, 19631, 13, 467, 311, 1310, 12889, 281, 574, 412, 512, 5242, 295, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.21873451232910157, "compression_ratio": 1.7296137339055795, "no_speech_prob": 6.962174211366801e-06}, {"id": 207, "seek": 100530, "start": 1005.3, "end": 1011.8199999999999, "text": " So what I've done here again. We'll look at the code in more detail later, but the basic idea is I've I've run", "tokens": [407, 437, 286, 600, 1096, 510, 797, 13, 492, 603, 574, 412, 264, 3089, 294, 544, 2607, 1780, 11, 457, 264, 3875, 1558, 307, 286, 600, 286, 600, 1190], "temperature": 0.0, "avg_logprob": -0.17951642285596142, "compression_ratio": 1.7875, "no_speech_prob": 1.5534935755567858e-06}, {"id": 208, "seek": 100530, "start": 1013.18, "end": 1015.02, "text": " I've built a", "tokens": [286, 600, 3094, 257], "temperature": 0.0, "avg_logprob": -0.17951642285596142, "compression_ratio": 1.7875, "no_speech_prob": 1.5534935755567858e-06}, {"id": 209, "seek": 100530, "start": 1015.02, "end": 1022.0999999999999, "text": " Data class like multiple times. I'm going to do it six times and each time. I'm going to plot the same cat and", "tokens": [11888, 1508, 411, 3866, 1413, 13, 286, 478, 516, 281, 360, 309, 2309, 1413, 293, 1184, 565, 13, 286, 478, 516, 281, 7542, 264, 912, 3857, 293], "temperature": 0.0, "avg_logprob": -0.17951642285596142, "compression_ratio": 1.7875, "no_speech_prob": 1.5534935755567858e-06}, {"id": 210, "seek": 100530, "start": 1022.74, "end": 1028.08, "text": " You can see that what happens is that this cat here is further over to the left", "tokens": [509, 393, 536, 300, 437, 2314, 307, 300, 341, 3857, 510, 307, 3052, 670, 281, 264, 1411], "temperature": 0.0, "avg_logprob": -0.17951642285596142, "compression_ratio": 1.7875, "no_speech_prob": 1.5534935755567858e-06}, {"id": 211, "seek": 102808, "start": 1028.08, "end": 1034.84, "text": " This one here is further over to the right and this one here is flipped horizontally and so forth so data augmentation", "tokens": [639, 472, 510, 307, 3052, 670, 281, 264, 558, 293, 341, 472, 510, 307, 26273, 33796, 293, 370, 5220, 370, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.19073549183932217, "compression_ratio": 1.8468468468468469, "no_speech_prob": 5.014711859985255e-06}, {"id": 212, "seek": 102808, "start": 1036.54, "end": 1044.52, "text": " Different types of image you're going to want different types of data augmentation right so for example if you were trying to recognize", "tokens": [20825, 3467, 295, 3256, 291, 434, 516, 281, 528, 819, 3467, 295, 1412, 14501, 19631, 558, 370, 337, 1365, 498, 291, 645, 1382, 281, 5521], "temperature": 0.0, "avg_logprob": -0.19073549183932217, "compression_ratio": 1.8468468468468469, "no_speech_prob": 5.014711859985255e-06}, {"id": 213, "seek": 102808, "start": 1045.5, "end": 1050.4199999999998, "text": " Letters and digits you wouldn't want to flip horizontally because like it's actually has a different meaning", "tokens": [961, 1559, 293, 27011, 291, 2759, 380, 528, 281, 7929, 33796, 570, 411, 309, 311, 767, 575, 257, 819, 3620], "temperature": 0.0, "avg_logprob": -0.19073549183932217, "compression_ratio": 1.8468468468468469, "no_speech_prob": 5.014711859985255e-06}, {"id": 214, "seek": 102808, "start": 1050.98, "end": 1053.6999999999998, "text": " Whereas on the other hand if you're looking at", "tokens": [13813, 322, 264, 661, 1011, 498, 291, 434, 1237, 412], "temperature": 0.0, "avg_logprob": -0.19073549183932217, "compression_ratio": 1.8468468468468469, "no_speech_prob": 5.014711859985255e-06}, {"id": 215, "seek": 105370, "start": 1053.7, "end": 1059.42, "text": " Photos of cats and dogs you probably don't want to flip vertically because cats aren't generally upside down", "tokens": [13919, 329, 295, 11111, 293, 7197, 291, 1391, 500, 380, 528, 281, 7929, 28450, 570, 11111, 3212, 380, 5101, 14119, 760], "temperature": 0.0, "avg_logprob": -0.19782668352127075, "compression_ratio": 1.7791164658634537, "no_speech_prob": 7.338192631323182e-07}, {"id": 216, "seek": 105370, "start": 1059.88, "end": 1065.78, "text": " Right where else if you're looking at there's a current Kaggle competition which is recognizing", "tokens": [1779, 689, 1646, 498, 291, 434, 1237, 412, 456, 311, 257, 2190, 48751, 22631, 6211, 597, 307, 18538], "temperature": 0.0, "avg_logprob": -0.19782668352127075, "compression_ratio": 1.7791164658634537, "no_speech_prob": 7.338192631323182e-07}, {"id": 217, "seek": 105370, "start": 1066.66, "end": 1068.66, "text": " icebergs in satellite images", "tokens": [38880, 82, 294, 16016, 5267], "temperature": 0.0, "avg_logprob": -0.19782668352127075, "compression_ratio": 1.7791164658634537, "no_speech_prob": 7.338192631323182e-07}, {"id": 218, "seek": 105370, "start": 1068.82, "end": 1076.54, "text": " You probably do want to flip them upside down because it doesn't really matter which way around the iceberg or the satellite was right so", "tokens": [509, 1391, 360, 528, 281, 7929, 552, 14119, 760, 570, 309, 1177, 380, 534, 1871, 597, 636, 926, 264, 38880, 420, 264, 16016, 390, 558, 370], "temperature": 0.0, "avg_logprob": -0.19782668352127075, "compression_ratio": 1.7791164658634537, "no_speech_prob": 7.338192631323182e-07}, {"id": 219, "seek": 105370, "start": 1078.26, "end": 1082.52, "text": " One of the examples of the transform sets we have is transforms side on", "tokens": [1485, 295, 264, 5110, 295, 264, 4088, 6352, 321, 362, 307, 35592, 1252, 322], "temperature": 0.0, "avg_logprob": -0.19782668352127075, "compression_ratio": 1.7791164658634537, "no_speech_prob": 7.338192631323182e-07}, {"id": 220, "seek": 108252, "start": 1082.52, "end": 1086.06, "text": " So in other words if you have photos that are like generally taken from the side", "tokens": [407, 294, 661, 2283, 498, 291, 362, 5787, 300, 366, 411, 5101, 2726, 490, 264, 1252], "temperature": 0.0, "avg_logprob": -0.19698818751743863, "compression_ratio": 1.763157894736842, "no_speech_prob": 3.3405037811462535e-06}, {"id": 221, "seek": 108252, "start": 1086.56, "end": 1091.4, "text": " Which generally means you want to be able to flip them horizontally but not vertically this is going to give you all the", "tokens": [3013, 5101, 1355, 291, 528, 281, 312, 1075, 281, 7929, 552, 33796, 457, 406, 28450, 341, 307, 516, 281, 976, 291, 439, 264], "temperature": 0.0, "avg_logprob": -0.19698818751743863, "compression_ratio": 1.763157894736842, "no_speech_prob": 3.3405037811462535e-06}, {"id": 222, "seek": 108252, "start": 1091.52, "end": 1097.5, "text": " Transforms you need for that so it'll flip them sideways rotate them by small amounts, but not too much and", "tokens": [27938, 82, 291, 643, 337, 300, 370, 309, 603, 7929, 552, 26092, 13121, 552, 538, 1359, 11663, 11, 457, 406, 886, 709, 293], "temperature": 0.0, "avg_logprob": -0.19698818751743863, "compression_ratio": 1.763157894736842, "no_speech_prob": 3.3405037811462535e-06}, {"id": 223, "seek": 108252, "start": 1098.02, "end": 1100.02, "text": " slightly bury their contrast and brightness", "tokens": [4748, 28919, 641, 8712, 293, 21367], "temperature": 0.0, "avg_logprob": -0.19698818751743863, "compression_ratio": 1.763157894736842, "no_speech_prob": 3.3405037811462535e-06}, {"id": 224, "seek": 108252, "start": 1101.68, "end": 1106.8, "text": " And slightly zoom in and out a little bit and move them around a little bit so each time. It's a slightly different", "tokens": [400, 4748, 8863, 294, 293, 484, 257, 707, 857, 293, 1286, 552, 926, 257, 707, 857, 370, 1184, 565, 13, 467, 311, 257, 4748, 819], "temperature": 0.0, "avg_logprob": -0.19698818751743863, "compression_ratio": 1.763157894736842, "no_speech_prob": 3.3405037811462535e-06}, {"id": 225, "seek": 110680, "start": 1106.8, "end": 1108.8, "text": " I fight with an edge", "tokens": [286, 2092, 365, 364, 4691], "temperature": 0.0, "avg_logprob": -0.39294273908748184, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0001464152883272618}, {"id": 226, "seek": 110680, "start": 1110.44, "end": 1112.44, "text": " Getting a couple of questions from people", "tokens": [13674, 257, 1916, 295, 1651, 490, 561], "temperature": 0.0, "avg_logprob": -0.39294273908748184, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0001464152883272618}, {"id": 227, "seek": 110680, "start": 1113.1599999999999, "end": 1119.8799999999999, "text": " About could you explain again the reason why you don't take the minimum of the loss curve? Yeah?", "tokens": [7769, 727, 291, 2903, 797, 264, 1778, 983, 291, 500, 380, 747, 264, 7285, 295, 264, 4470, 7605, 30, 865, 30], "temperature": 0.0, "avg_logprob": -0.39294273908748184, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0001464152883272618}, {"id": 228, "seek": 110680, "start": 1120.24, "end": 1123.32, "text": " But it's like the higher rate so yeah, and also could you", "tokens": [583, 309, 311, 411, 264, 2946, 3314, 370, 1338, 11, 293, 611, 727, 291], "temperature": 0.0, "avg_logprob": -0.39294273908748184, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0001464152883272618}, {"id": 229, "seek": 110680, "start": 1124.44, "end": 1126.44, "text": " people will understand if", "tokens": [561, 486, 1223, 498], "temperature": 0.0, "avg_logprob": -0.39294273908748184, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0001464152883272618}, {"id": 230, "seek": 112644, "start": 1126.44, "end": 1136.96, "text": " This works for every CNN for CNN's for every unit on it this being the learning rate finder. Yeah exactly yeah", "tokens": [639, 1985, 337, 633, 24859, 337, 24859, 311, 337, 633, 4985, 322, 309, 341, 885, 264, 2539, 3314, 915, 260, 13, 865, 2293, 1338], "temperature": 0.0, "avg_logprob": -0.3330767649524617, "compression_ratio": 1.3478260869565217, "no_speech_prob": 2.99440284834418e-06}, {"id": 231, "seek": 112644, "start": 1138.0800000000002, "end": 1139.8200000000002, "text": " Okay, great", "tokens": [1033, 11, 869], "temperature": 0.0, "avg_logprob": -0.3330767649524617, "compression_ratio": 1.3478260869565217, "no_speech_prob": 2.99440284834418e-06}, {"id": 232, "seek": 112644, "start": 1139.8200000000002, "end": 1141.4, "text": " Could you?", "tokens": [7497, 291, 30], "temperature": 0.0, "avg_logprob": -0.3330767649524617, "compression_ratio": 1.3478260869565217, "no_speech_prob": 2.99440284834418e-06}, {"id": 233, "seek": 112644, "start": 1141.4, "end": 1143.92, "text": " Put your hand up if there's a spare seat next to you", "tokens": [4935, 428, 1011, 493, 498, 456, 311, 257, 13798, 6121, 958, 281, 291], "temperature": 0.0, "avg_logprob": -0.3330767649524617, "compression_ratio": 1.3478260869565217, "no_speech_prob": 2.99440284834418e-06}, {"id": 234, "seek": 114392, "start": 1143.92, "end": 1155.52, "text": " So there was a question about the learning rate finder about why do we use the learning rate?", "tokens": [407, 456, 390, 257, 1168, 466, 264, 2539, 3314, 915, 260, 466, 983, 360, 321, 764, 264, 2539, 3314, 30], "temperature": 0.0, "avg_logprob": -0.22128234111087422, "compression_ratio": 1.720496894409938, "no_speech_prob": 1.994719013964641e-06}, {"id": 235, "seek": 114392, "start": 1155.52, "end": 1161.74, "text": " That's less than the lowest point and so the reason why is to understand what's going on with this learning rate finder", "tokens": [663, 311, 1570, 813, 264, 12437, 935, 293, 370, 264, 1778, 983, 307, 281, 1223, 437, 311, 516, 322, 365, 341, 2539, 3314, 915, 260], "temperature": 0.0, "avg_logprob": -0.22128234111087422, "compression_ratio": 1.720496894409938, "no_speech_prob": 1.994719013964641e-06}, {"id": 236, "seek": 114392, "start": 1164.68, "end": 1166.92, "text": " So let's go back to our picture here", "tokens": [407, 718, 311, 352, 646, 281, 527, 3036, 510], "temperature": 0.0, "avg_logprob": -0.22128234111087422, "compression_ratio": 1.720496894409938, "no_speech_prob": 1.994719013964641e-06}, {"id": 237, "seek": 114392, "start": 1168.5600000000002, "end": 1170.5600000000002, "text": " Like how do we figure out?", "tokens": [1743, 577, 360, 321, 2573, 484, 30], "temperature": 0.0, "avg_logprob": -0.22128234111087422, "compression_ratio": 1.720496894409938, "no_speech_prob": 1.994719013964641e-06}, {"id": 238, "seek": 117056, "start": 1170.56, "end": 1175.08, "text": " What learning rate to use right and so what we're going to do is we're going to take", "tokens": [708, 2539, 3314, 281, 764, 558, 293, 370, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 747], "temperature": 0.0, "avg_logprob": -0.19910360842334981, "compression_ratio": 2.0428571428571427, "no_speech_prob": 1.9637952846096596e-06}, {"id": 239, "seek": 117056, "start": 1175.44, "end": 1178.84, "text": " Steps and each time we're going to double", "tokens": [5470, 82, 293, 1184, 565, 321, 434, 516, 281, 3834], "temperature": 0.0, "avg_logprob": -0.19910360842334981, "compression_ratio": 2.0428571428571427, "no_speech_prob": 1.9637952846096596e-06}, {"id": 240, "seek": 117056, "start": 1179.36, "end": 1184.46, "text": " The learning rate so kind of double the amount by which we're multiplying the greater gradient so in other words", "tokens": [440, 2539, 3314, 370, 733, 295, 3834, 264, 2372, 538, 597, 321, 434, 30955, 264, 5044, 16235, 370, 294, 661, 2283], "temperature": 0.0, "avg_logprob": -0.19910360842334981, "compression_ratio": 2.0428571428571427, "no_speech_prob": 1.9637952846096596e-06}, {"id": 241, "seek": 117056, "start": 1184.46, "end": 1189.96, "text": " We'd go tiny step slightly bigger slightly bigger slightly bigger slightly bigger", "tokens": [492, 1116, 352, 5870, 1823, 4748, 3801, 4748, 3801, 4748, 3801, 4748, 3801], "temperature": 0.0, "avg_logprob": -0.19910360842334981, "compression_ratio": 2.0428571428571427, "no_speech_prob": 1.9637952846096596e-06}, {"id": 242, "seek": 117056, "start": 1191.48, "end": 1193.94, "text": " slightly bigger slightly bigger", "tokens": [4748, 3801, 4748, 3801], "temperature": 0.0, "avg_logprob": -0.19910360842334981, "compression_ratio": 2.0428571428571427, "no_speech_prob": 1.9637952846096596e-06}, {"id": 243, "seek": 119394, "start": 1193.94, "end": 1200.26, "text": " Okay, and so the question is the purpose of this is not to find the minimum", "tokens": [1033, 11, 293, 370, 264, 1168, 307, 264, 4334, 295, 341, 307, 406, 281, 915, 264, 7285], "temperature": 0.0, "avg_logprob": -0.1520747104323054, "compression_ratio": 1.75, "no_speech_prob": 2.1355401713663014e-07}, {"id": 244, "seek": 119394, "start": 1200.46, "end": 1206.98, "text": " The purpose of this is to figure out what learning rate is allowing us to decrease quickly right?", "tokens": [440, 4334, 295, 341, 307, 281, 2573, 484, 437, 2539, 3314, 307, 8293, 505, 281, 11514, 2661, 558, 30], "temperature": 0.0, "avg_logprob": -0.1520747104323054, "compression_ratio": 1.75, "no_speech_prob": 2.1355401713663014e-07}, {"id": 245, "seek": 119394, "start": 1207.5800000000002, "end": 1216.02, "text": " So the point at which the loss was lowest here is actually there right but that learning rate actually looks like it's probably too", "tokens": [407, 264, 935, 412, 597, 264, 4470, 390, 12437, 510, 307, 767, 456, 558, 457, 300, 2539, 3314, 767, 1542, 411, 309, 311, 1391, 886], "temperature": 0.0, "avg_logprob": -0.1520747104323054, "compression_ratio": 1.75, "no_speech_prob": 2.1355401713663014e-07}, {"id": 246, "seek": 119394, "start": 1216.02, "end": 1219.46, "text": " High it's going to just jump like probably backwards and forwards", "tokens": [5229, 309, 311, 516, 281, 445, 3012, 411, 1391, 12204, 293, 30126], "temperature": 0.0, "avg_logprob": -0.1520747104323054, "compression_ratio": 1.75, "no_speech_prob": 2.1355401713663014e-07}, {"id": 247, "seek": 121946, "start": 1219.46, "end": 1227.22, "text": " Okay, so instead what we do is we go back to the point where the learning rates quickly giving us a quick increase", "tokens": [1033, 11, 370, 2602, 437, 321, 360, 307, 321, 352, 646, 281, 264, 935, 689, 264, 2539, 6846, 2661, 2902, 505, 257, 1702, 3488], "temperature": 0.0, "avg_logprob": -0.12821518898010253, "compression_ratio": 1.8685446009389672, "no_speech_prob": 9.27635085190559e-07}, {"id": 248, "seek": 121946, "start": 1228.42, "end": 1230.42, "text": " in the loss", "tokens": [294, 264, 4470], "temperature": 0.0, "avg_logprob": -0.12821518898010253, "compression_ratio": 1.8685446009389672, "no_speech_prob": 9.27635085190559e-07}, {"id": 249, "seek": 121946, "start": 1230.74, "end": 1232.54, "text": " So here is", "tokens": [407, 510, 307], "temperature": 0.0, "avg_logprob": -0.12821518898010253, "compression_ratio": 1.8685446009389672, "no_speech_prob": 9.27635085190559e-07}, {"id": 250, "seek": 121946, "start": 1232.54, "end": 1238.06, "text": " So here is the actual learning rate increasing every single time we look at a new mini-batch, right?", "tokens": [407, 510, 307, 264, 3539, 2539, 3314, 5662, 633, 2167, 565, 321, 574, 412, 257, 777, 8382, 12, 65, 852, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12821518898010253, "compression_ratio": 1.8685446009389672, "no_speech_prob": 9.27635085190559e-07}, {"id": 251, "seek": 121946, "start": 1238.06, "end": 1243.42, "text": " So mini-batch or iteration versus learning rate and then here is learning rate versus loss", "tokens": [407, 8382, 12, 65, 852, 420, 24784, 5717, 2539, 3314, 293, 550, 510, 307, 2539, 3314, 5717, 4470], "temperature": 0.0, "avg_logprob": -0.12821518898010253, "compression_ratio": 1.8685446009389672, "no_speech_prob": 9.27635085190559e-07}, {"id": 252, "seek": 121946, "start": 1243.42, "end": 1247.02, "text": " So here's that point at the bottom where it was now already too high", "tokens": [407, 510, 311, 300, 935, 412, 264, 2767, 689, 309, 390, 586, 1217, 886, 1090], "temperature": 0.0, "avg_logprob": -0.12821518898010253, "compression_ratio": 1.8685446009389672, "no_speech_prob": 9.27635085190559e-07}, {"id": 253, "seek": 124702, "start": 1247.02, "end": 1252.34, "text": " Okay, and so here's the point where we go back a little bit and it's increasing nice and quickly", "tokens": [1033, 11, 293, 370, 510, 311, 264, 935, 689, 321, 352, 646, 257, 707, 857, 293, 309, 311, 5662, 1481, 293, 2661], "temperature": 0.0, "avg_logprob": -0.1838252203805106, "compression_ratio": 1.6569037656903767, "no_speech_prob": 7.1831841523817275e-06}, {"id": 254, "seek": 124702, "start": 1254.42, "end": 1256.42, "text": " We're going to learn about something called", "tokens": [492, 434, 516, 281, 1466, 466, 746, 1219], "temperature": 0.0, "avg_logprob": -0.1838252203805106, "compression_ratio": 1.6569037656903767, "no_speech_prob": 7.1831841523817275e-06}, {"id": 255, "seek": 124702, "start": 1257.1, "end": 1261.26, "text": " Stochastic gradient descent with restarts shortly where we're going to see like in a sense", "tokens": [745, 8997, 2750, 16235, 23475, 365, 1472, 11814, 13392, 689, 321, 434, 516, 281, 536, 411, 294, 257, 2020], "temperature": 0.0, "avg_logprob": -0.1838252203805106, "compression_ratio": 1.6569037656903767, "no_speech_prob": 7.1831841523817275e-06}, {"id": 256, "seek": 124702, "start": 1261.26, "end": 1267.56, "text": " You might want to go back to 1e neg 3 where it's actually even steeper still and maybe we would actually find this will", "tokens": [509, 1062, 528, 281, 352, 646, 281, 502, 68, 2485, 805, 689, 309, 311, 767, 754, 16841, 260, 920, 293, 1310, 321, 576, 767, 915, 341, 486], "temperature": 0.0, "avg_logprob": -0.1838252203805106, "compression_ratio": 1.6569037656903767, "no_speech_prob": 7.1831841523817275e-06}, {"id": 257, "seek": 124702, "start": 1268.58, "end": 1269.82, "text": " actually", "tokens": [767], "temperature": 0.0, "avg_logprob": -0.1838252203805106, "compression_ratio": 1.6569037656903767, "no_speech_prob": 7.1831841523817275e-06}, {"id": 258, "seek": 124702, "start": 1269.82, "end": 1271.82, "text": " Learn even quicker you could try it", "tokens": [17216, 754, 16255, 291, 727, 853, 309], "temperature": 0.0, "avg_logprob": -0.1838252203805106, "compression_ratio": 1.6569037656903767, "no_speech_prob": 7.1831841523817275e-06}, {"id": 259, "seek": 127182, "start": 1271.82, "end": 1277.6599999999999, "text": " But we're going to see later why actually using a higher number is going to give us a better generalization", "tokens": [583, 321, 434, 516, 281, 536, 1780, 983, 767, 1228, 257, 2946, 1230, 307, 516, 281, 976, 505, 257, 1101, 2674, 2144], "temperature": 0.0, "avg_logprob": -0.22146750511007107, "compression_ratio": 1.9853658536585366, "no_speech_prob": 7.5279135671735276e-06}, {"id": 260, "seek": 127182, "start": 1278.1799999999998, "end": 1280.1799999999998, "text": " So for now, let's put that aside", "tokens": [407, 337, 586, 11, 718, 311, 829, 300, 7359], "temperature": 0.0, "avg_logprob": -0.22146750511007107, "compression_ratio": 1.9853658536585366, "no_speech_prob": 7.5279135671735276e-06}, {"id": 261, "seek": 127182, "start": 1280.4199999999998, "end": 1285.84, "text": " Do you mean higher learning rate when you say higher do I mean higher learning rate when I say higher? Yeah", "tokens": [1144, 291, 914, 2946, 2539, 3314, 562, 291, 584, 2946, 360, 286, 914, 2946, 2539, 3314, 562, 286, 584, 2946, 30, 865], "temperature": 0.0, "avg_logprob": -0.22146750511007107, "compression_ratio": 1.9853658536585366, "no_speech_prob": 7.5279135671735276e-06}, {"id": 262, "seek": 127182, "start": 1289.7, "end": 1295.86, "text": " I mean higher learning rate so as we increase the iterations in the learning rate finder the learning rate is going up", "tokens": [286, 914, 2946, 2539, 3314, 370, 382, 321, 3488, 264, 36540, 294, 264, 2539, 3314, 915, 260, 264, 2539, 3314, 307, 516, 493], "temperature": 0.0, "avg_logprob": -0.22146750511007107, "compression_ratio": 1.9853658536585366, "no_speech_prob": 7.5279135671735276e-06}, {"id": 263, "seek": 127182, "start": 1295.86, "end": 1298.3999999999999, "text": " This is iterations versus learning rate", "tokens": [639, 307, 36540, 5717, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.22146750511007107, "compression_ratio": 1.9853658536585366, "no_speech_prob": 7.5279135671735276e-06}, {"id": 264, "seek": 129840, "start": 1298.4, "end": 1305.3600000000001, "text": " Okay, so as we do that as the learning rate increases and we plot it here the loss goes down", "tokens": [1033, 11, 370, 382, 321, 360, 300, 382, 264, 2539, 3314, 8637, 293, 321, 7542, 309, 510, 264, 4470, 1709, 760], "temperature": 0.0, "avg_logprob": -0.14610832214355468, "compression_ratio": 1.7854671280276817, "no_speech_prob": 5.7718862080946565e-06}, {"id": 265, "seek": 129840, "start": 1305.96, "end": 1308.4, "text": " Until we get to the point where the learning rate is too high", "tokens": [9088, 321, 483, 281, 264, 935, 689, 264, 2539, 3314, 307, 886, 1090], "temperature": 0.0, "avg_logprob": -0.14610832214355468, "compression_ratio": 1.7854671280276817, "no_speech_prob": 5.7718862080946565e-06}, {"id": 266, "seek": 129840, "start": 1308.4, "end": 1315.2800000000002, "text": " And at that point the loss is now getting worse because I asked the question because you were just indicating that you know", "tokens": [400, 412, 300, 935, 264, 4470, 307, 586, 1242, 5324, 570, 286, 2351, 264, 1168, 570, 291, 645, 445, 25604, 300, 291, 458], "temperature": 0.0, "avg_logprob": -0.14610832214355468, "compression_ratio": 1.7854671280276817, "no_speech_prob": 5.7718862080946565e-06}, {"id": 267, "seek": 129840, "start": 1315.2800000000002, "end": 1317.2800000000002, "text": " Even though the minimum was that 10 to the minus 1", "tokens": [2754, 1673, 264, 7285, 390, 300, 1266, 281, 264, 3175, 502], "temperature": 0.0, "avg_logprob": -0.14610832214355468, "compression_ratio": 1.7854671280276817, "no_speech_prob": 5.7718862080946565e-06}, {"id": 268, "seek": 129840, "start": 1318.0400000000002, "end": 1322.0, "text": " You were going to you suggest that we should choose 10 to the minus 2", "tokens": [509, 645, 516, 281, 291, 3402, 300, 321, 820, 2826, 1266, 281, 264, 3175, 568], "temperature": 0.0, "avg_logprob": -0.14610832214355468, "compression_ratio": 1.7854671280276817, "no_speech_prob": 5.7718862080946565e-06}, {"id": 269, "seek": 132200, "start": 1322.0, "end": 1328.56, "text": " But now you're saying that maybe we should go back the other way higher, so I didn't mean to say that I'm sorry if I said", "tokens": [583, 586, 291, 434, 1566, 300, 1310, 321, 820, 352, 646, 264, 661, 636, 2946, 11, 370, 286, 994, 380, 914, 281, 584, 300, 286, 478, 2597, 498, 286, 848], "temperature": 0.0, "avg_logprob": -0.26503824450306057, "compression_ratio": 1.726027397260274, "no_speech_prob": 8.530204468115699e-06}, {"id": 270, "seek": 132200, "start": 1328.56, "end": 1333.4, "text": " Something backwards, so I want to go back down to the lower learning rate", "tokens": [6595, 12204, 11, 370, 286, 528, 281, 352, 646, 760, 281, 264, 3126, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.26503824450306057, "compression_ratio": 1.726027397260274, "no_speech_prob": 8.530204468115699e-06}, {"id": 271, "seek": 132200, "start": 1333.64, "end": 1338.04, "text": " So possibly I said a higher when I meant higher in that's lower", "tokens": [407, 6264, 286, 848, 257, 2946, 562, 286, 4140, 2946, 294, 300, 311, 3126], "temperature": 0.0, "avg_logprob": -0.26503824450306057, "compression_ratio": 1.726027397260274, "no_speech_prob": 8.530204468115699e-06}, {"id": 272, "seek": 132200, "start": 1338.68, "end": 1341.2, "text": " Lower learning rate okay, thanks. Yeah", "tokens": [25523, 2539, 3314, 1392, 11, 3231, 13, 865], "temperature": 0.0, "avg_logprob": -0.26503824450306057, "compression_ratio": 1.726027397260274, "no_speech_prob": 8.530204468115699e-06}, {"id": 273, "seek": 132200, "start": 1343.04, "end": 1347.96, "text": " In the last class you said that the local all the local minima are the same and", "tokens": [682, 264, 1036, 1508, 291, 848, 300, 264, 2654, 439, 264, 2654, 4464, 64, 366, 264, 912, 293], "temperature": 0.0, "avg_logprob": -0.26503824450306057, "compression_ratio": 1.726027397260274, "no_speech_prob": 8.530204468115699e-06}, {"id": 274, "seek": 134796, "start": 1347.96, "end": 1355.76, "text": " And this graph also shows the same is that is that something that was observed or is that logic theory behind it?", "tokens": [400, 341, 4295, 611, 3110, 264, 912, 307, 300, 307, 300, 746, 300, 390, 13095, 420, 307, 300, 9952, 5261, 2261, 309, 30], "temperature": 0.0, "avg_logprob": -0.20398104321825636, "compression_ratio": 1.838862559241706, "no_speech_prob": 7.071764684951631e-06}, {"id": 275, "seek": 134796, "start": 1357.04, "end": 1359.04, "text": " That's not what this graph is showing", "tokens": [663, 311, 406, 437, 341, 4295, 307, 4099], "temperature": 0.0, "avg_logprob": -0.20398104321825636, "compression_ratio": 1.838862559241706, "no_speech_prob": 7.071764684951631e-06}, {"id": 276, "seek": 134796, "start": 1359.24, "end": 1363.76, "text": " This graph is simply showing that there's a point where if we increase the learning rate more", "tokens": [639, 4295, 307, 2935, 4099, 300, 456, 311, 257, 935, 689, 498, 321, 3488, 264, 2539, 3314, 544], "temperature": 0.0, "avg_logprob": -0.20398104321825636, "compression_ratio": 1.838862559241706, "no_speech_prob": 7.071764684951631e-06}, {"id": 277, "seek": 134796, "start": 1364.1200000000001, "end": 1368.52, "text": " Then it stops getting better, and it actually starts getting worse the idea that", "tokens": [1396, 309, 10094, 1242, 1101, 11, 293, 309, 767, 3719, 1242, 5324, 264, 1558, 300], "temperature": 0.0, "avg_logprob": -0.20398104321825636, "compression_ratio": 1.838862559241706, "no_speech_prob": 7.071764684951631e-06}, {"id": 278, "seek": 134796, "start": 1369.56, "end": 1372.24, "text": " all local minima are the same is a", "tokens": [439, 2654, 4464, 64, 366, 264, 912, 307, 257], "temperature": 0.0, "avg_logprob": -0.20398104321825636, "compression_ratio": 1.838862559241706, "no_speech_prob": 7.071764684951631e-06}, {"id": 279, "seek": 134796, "start": 1373.52, "end": 1375.52, "text": " totally separate issue and", "tokens": [3879, 4994, 2734, 293], "temperature": 0.0, "avg_logprob": -0.20398104321825636, "compression_ratio": 1.838862559241706, "no_speech_prob": 7.071764684951631e-06}, {"id": 280, "seek": 137552, "start": 1375.52, "end": 1380.08, "text": " And it's actually something we'll see a picture of shortly, so let's come back to that", "tokens": [400, 309, 311, 767, 746, 321, 603, 536, 257, 3036, 295, 13392, 11, 370, 718, 311, 808, 646, 281, 300], "temperature": 0.0, "avg_logprob": -0.35268707275390626, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.647617995738983e-05}, {"id": 281, "seek": 137552, "start": 1382.92, "end": 1388.04, "text": " Jeremy do we have to find the best learning rate every time we're going on?", "tokens": [17809, 360, 321, 362, 281, 915, 264, 1151, 2539, 3314, 633, 565, 321, 434, 516, 322, 30], "temperature": 0.0, "avg_logprob": -0.35268707275390626, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.647617995738983e-05}, {"id": 282, "seek": 137552, "start": 1389.2, "end": 1390.76, "text": " run on the poke", "tokens": [1190, 322, 264, 19712], "temperature": 0.0, "avg_logprob": -0.35268707275390626, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.647617995738983e-05}, {"id": 283, "seek": 137552, "start": 1390.76, "end": 1392.44, "text": " every time we're", "tokens": [633, 565, 321, 434], "temperature": 0.0, "avg_logprob": -0.35268707275390626, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.647617995738983e-05}, {"id": 284, "seek": 137552, "start": 1392.44, "end": 1399.6399999999999, "text": " Running on a poke and a pop so how many times should I run this like learning right fine in my training?", "tokens": [28136, 322, 257, 19712, 293, 257, 1665, 370, 577, 867, 1413, 820, 286, 1190, 341, 411, 2539, 558, 2489, 294, 452, 3097, 30], "temperature": 0.0, "avg_logprob": -0.35268707275390626, "compression_ratio": 1.6129032258064515, "no_speech_prob": 3.647617995738983e-05}, {"id": 285, "seek": 139964, "start": 1399.64, "end": 1406.16, "text": " That's a great question unit um I", "tokens": [663, 311, 257, 869, 1168, 4985, 1105, 286], "temperature": 0.0, "avg_logprob": -0.2617691453680935, "compression_ratio": 1.681081081081081, "no_speech_prob": 4.157266630500089e-06}, {"id": 286, "seek": 139964, "start": 1408.2800000000002, "end": 1410.5200000000002, "text": " I certainly run it once when I start", "tokens": [286, 3297, 1190, 309, 1564, 562, 286, 722], "temperature": 0.0, "avg_logprob": -0.2617691453680935, "compression_ratio": 1.681081081081081, "no_speech_prob": 4.157266630500089e-06}, {"id": 287, "seek": 139964, "start": 1412.44, "end": 1415.44, "text": " Later on in this class we're going to learn about unfreezing layers", "tokens": [11965, 322, 294, 341, 1508, 321, 434, 516, 281, 1466, 466, 3971, 701, 8781, 7914], "temperature": 0.0, "avg_logprob": -0.2617691453680935, "compression_ratio": 1.681081081081081, "no_speech_prob": 4.157266630500089e-06}, {"id": 288, "seek": 139964, "start": 1416.3200000000002, "end": 1423.3200000000002, "text": " And after I unfreeze layers I sometimes run it again if I do something to like change the thing", "tokens": [400, 934, 286, 3971, 701, 1381, 7914, 286, 2171, 1190, 309, 797, 498, 286, 360, 746, 281, 411, 1319, 264, 551], "temperature": 0.0, "avg_logprob": -0.2617691453680935, "compression_ratio": 1.681081081081081, "no_speech_prob": 4.157266630500089e-06}, {"id": 289, "seek": 139964, "start": 1423.3200000000002, "end": 1426.72, "text": " I'm training or change the way. I'm training it you may want to run it again", "tokens": [286, 478, 3097, 420, 1319, 264, 636, 13, 286, 478, 3097, 309, 291, 815, 528, 281, 1190, 309, 797], "temperature": 0.0, "avg_logprob": -0.2617691453680935, "compression_ratio": 1.681081081081081, "no_speech_prob": 4.157266630500089e-06}, {"id": 290, "seek": 142672, "start": 1426.72, "end": 1428.72, "text": " basically", "tokens": [1936], "temperature": 0.0, "avg_logprob": -0.2255491297295753, "compression_ratio": 1.553648068669528, "no_speech_prob": 3.089463007199811e-06}, {"id": 291, "seek": 142672, "start": 1428.8, "end": 1434.32, "text": " Or you know if you particularly if you've changed something about how you train like unfreezing layers", "tokens": [1610, 291, 458, 498, 291, 4098, 498, 291, 600, 3105, 746, 466, 577, 291, 3847, 411, 3971, 701, 8781, 7914], "temperature": 0.0, "avg_logprob": -0.2255491297295753, "compression_ratio": 1.553648068669528, "no_speech_prob": 3.089463007199811e-06}, {"id": 292, "seek": 142672, "start": 1434.32, "end": 1438.16, "text": " Which we're going to soon learn about and you're finding now the training is", "tokens": [3013, 321, 434, 516, 281, 2321, 1466, 466, 293, 291, 434, 5006, 586, 264, 3097, 307], "temperature": 0.0, "avg_logprob": -0.2255491297295753, "compression_ratio": 1.553648068669528, "no_speech_prob": 3.089463007199811e-06}, {"id": 293, "seek": 142672, "start": 1439.16, "end": 1444.2, "text": " Unstable or too slow no again. You can run it again. There's never any harm in", "tokens": [1156, 372, 712, 420, 886, 2964, 572, 797, 13, 509, 393, 1190, 309, 797, 13, 821, 311, 1128, 604, 6491, 294], "temperature": 0.0, "avg_logprob": -0.2255491297295753, "compression_ratio": 1.553648068669528, "no_speech_prob": 3.089463007199811e-06}, {"id": 294, "seek": 142672, "start": 1445.2, "end": 1447.2, "text": " Running it it doesn't take very long", "tokens": [28136, 309, 309, 1177, 380, 747, 588, 938], "temperature": 0.0, "avg_logprob": -0.2255491297295753, "compression_ratio": 1.553648068669528, "no_speech_prob": 3.089463007199811e-06}, {"id": 295, "seek": 142672, "start": 1448.16, "end": 1450.1200000000001, "text": " That's great question", "tokens": [663, 311, 869, 1168], "temperature": 0.0, "avg_logprob": -0.2255491297295753, "compression_ratio": 1.553648068669528, "no_speech_prob": 3.089463007199811e-06}, {"id": 296, "seek": 142672, "start": 1450.1200000000001, "end": 1452.1200000000001, "text": " Okay, so back to data augmentation", "tokens": [1033, 11, 370, 646, 281, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.2255491297295753, "compression_ratio": 1.553648068669528, "no_speech_prob": 3.089463007199811e-06}, {"id": 297, "seek": 145212, "start": 1452.12, "end": 1457.32, "text": " So if we add to our when we run this little transforms from model", "tokens": [407, 498, 321, 909, 281, 527, 562, 321, 1190, 341, 707, 35592, 490, 2316], "temperature": 0.0, "avg_logprob": -0.16042091491374563, "compression_ratio": 1.751111111111111, "no_speech_prob": 3.9669534999120515e-06}, {"id": 298, "seek": 145212, "start": 1458.4799999999998, "end": 1463.12, "text": " Function we pass in augmentation transforms we can pass in the main to a", "tokens": [11166, 882, 321, 1320, 294, 14501, 19631, 35592, 321, 393, 1320, 294, 264, 2135, 281, 257], "temperature": 0.0, "avg_logprob": -0.16042091491374563, "compression_ratio": 1.751111111111111, "no_speech_prob": 3.9669534999120515e-06}, {"id": 299, "seek": 145212, "start": 1463.84, "end": 1470.34, "text": " Transform side on or transforms top-down later on we'll learn about creating your own custom transform lists as well", "tokens": [27938, 1252, 322, 420, 35592, 1192, 12, 5093, 1780, 322, 321, 603, 1466, 466, 4084, 428, 1065, 2375, 4088, 14511, 382, 731], "temperature": 0.0, "avg_logprob": -0.16042091491374563, "compression_ratio": 1.751111111111111, "no_speech_prob": 3.9669534999120515e-06}, {"id": 300, "seek": 145212, "start": 1471.1999999999998, "end": 1475.04, "text": " But for now because we're taking pictures from the side of cats and dogs", "tokens": [583, 337, 586, 570, 321, 434, 1940, 5242, 490, 264, 1252, 295, 11111, 293, 7197], "temperature": 0.0, "avg_logprob": -0.16042091491374563, "compression_ratio": 1.751111111111111, "no_speech_prob": 3.9669534999120515e-06}, {"id": 301, "seek": 145212, "start": 1475.04, "end": 1479.9599999999998, "text": " We'll say transform side on and now each time we look at an image", "tokens": [492, 603, 584, 4088, 1252, 322, 293, 586, 1184, 565, 321, 574, 412, 364, 3256], "temperature": 0.0, "avg_logprob": -0.16042091491374563, "compression_ratio": 1.751111111111111, "no_speech_prob": 3.9669534999120515e-06}, {"id": 302, "seek": 147996, "start": 1479.96, "end": 1483.2, "text": " It's going to be zoomed in or out a little bit moved around a little bit", "tokens": [467, 311, 516, 281, 312, 8863, 292, 294, 420, 484, 257, 707, 857, 4259, 926, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.1415108801091759, "compression_ratio": 1.6877470355731226, "no_speech_prob": 3.844913862849353e-06}, {"id": 303, "seek": 147996, "start": 1483.88, "end": 1485.88, "text": " rotated a little bit", "tokens": [42146, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.1415108801091759, "compression_ratio": 1.6877470355731226, "no_speech_prob": 3.844913862849353e-06}, {"id": 304, "seek": 147996, "start": 1486.88, "end": 1491.92, "text": " Possibly flipped okay, and so what this does is it's not exactly creating new data", "tokens": [33112, 3545, 26273, 1392, 11, 293, 370, 437, 341, 775, 307, 309, 311, 406, 2293, 4084, 777, 1412], "temperature": 0.0, "avg_logprob": -0.1415108801091759, "compression_ratio": 1.6877470355731226, "no_speech_prob": 3.844913862849353e-06}, {"id": 305, "seek": 147996, "start": 1492.2, "end": 1496.1200000000001, "text": " But as far as the convolutional neural net is concerned", "tokens": [583, 382, 1400, 382, 264, 45216, 304, 18161, 2533, 307, 5922], "temperature": 0.0, "avg_logprob": -0.1415108801091759, "compression_ratio": 1.6877470355731226, "no_speech_prob": 3.844913862849353e-06}, {"id": 306, "seek": 147996, "start": 1496.1200000000001, "end": 1501.52, "text": " It's it's a different way of looking at this thing and it actually therefore allows it to learn", "tokens": [467, 311, 309, 311, 257, 819, 636, 295, 1237, 412, 341, 551, 293, 309, 767, 4412, 4045, 309, 281, 1466], "temperature": 0.0, "avg_logprob": -0.1415108801091759, "compression_ratio": 1.6877470355731226, "no_speech_prob": 3.844913862849353e-06}, {"id": 307, "seek": 147996, "start": 1502.32, "end": 1508.76, "text": " How to recognize cats or dogs from somewhat different angles right so when we do data augmentation", "tokens": [1012, 281, 5521, 11111, 420, 7197, 490, 8344, 819, 14708, 558, 370, 562, 321, 360, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.1415108801091759, "compression_ratio": 1.6877470355731226, "no_speech_prob": 3.844913862849353e-06}, {"id": 308, "seek": 150876, "start": 1508.76, "end": 1511.84, "text": " We're basically trying to say based on our domain knowledge", "tokens": [492, 434, 1936, 1382, 281, 584, 2361, 322, 527, 9274, 3601], "temperature": 0.0, "avg_logprob": -0.12746418223661535, "compression_ratio": 1.7164750957854407, "no_speech_prob": 5.093656454846496e-06}, {"id": 309, "seek": 150876, "start": 1513.32, "end": 1518.0, "text": " Here here are different ways that we can mess with this image that we know", "tokens": [1692, 510, 366, 819, 2098, 300, 321, 393, 2082, 365, 341, 3256, 300, 321, 458], "temperature": 0.0, "avg_logprob": -0.12746418223661535, "compression_ratio": 1.7164750957854407, "no_speech_prob": 5.093656454846496e-06}, {"id": 310, "seek": 150876, "start": 1518.42, "end": 1524.26, "text": " Still make it the same image. You know and that we could expect that you might actually see that kind of image in the real world", "tokens": [8291, 652, 309, 264, 912, 3256, 13, 509, 458, 293, 300, 321, 727, 2066, 300, 291, 1062, 767, 536, 300, 733, 295, 3256, 294, 264, 957, 1002], "temperature": 0.0, "avg_logprob": -0.12746418223661535, "compression_ratio": 1.7164750957854407, "no_speech_prob": 5.093656454846496e-06}, {"id": 311, "seek": 150876, "start": 1525.84, "end": 1530.96, "text": " So what we can do now is when we call this from paths function", "tokens": [407, 437, 321, 393, 360, 586, 307, 562, 321, 818, 341, 490, 14518, 2445], "temperature": 0.0, "avg_logprob": -0.12746418223661535, "compression_ratio": 1.7164750957854407, "no_speech_prob": 5.093656454846496e-06}, {"id": 312, "seek": 150876, "start": 1530.96, "end": 1537.28, "text": " Which we'll learn more about shortly we can now pass in this set of transforms which actually have these augmentations in", "tokens": [3013, 321, 603, 1466, 544, 466, 13392, 321, 393, 586, 1320, 294, 341, 992, 295, 35592, 597, 767, 362, 613, 29919, 763, 294], "temperature": 0.0, "avg_logprob": -0.12746418223661535, "compression_ratio": 1.7164750957854407, "no_speech_prob": 5.093656454846496e-06}, {"id": 313, "seek": 153728, "start": 1537.28, "end": 1539.28, "text": " now", "tokens": [586], "temperature": 0.0, "avg_logprob": -0.25403223037719724, "compression_ratio": 1.6324324324324324, "no_speech_prob": 3.6119593005423667e-06}, {"id": 314, "seek": 153728, "start": 1541.28, "end": 1546.08, "text": " So that's kind of we're going to start from scratch here. We do a fit and", "tokens": [407, 300, 311, 733, 295, 321, 434, 516, 281, 722, 490, 8459, 510, 13, 492, 360, 257, 3318, 293], "temperature": 0.0, "avg_logprob": -0.25403223037719724, "compression_ratio": 1.6324324324324324, "no_speech_prob": 3.6119593005423667e-06}, {"id": 315, "seek": 153728, "start": 1547.76, "end": 1549.76, "text": " initially the", "tokens": [9105, 264], "temperature": 0.0, "avg_logprob": -0.25403223037719724, "compression_ratio": 1.6324324324324324, "no_speech_prob": 3.6119593005423667e-06}, {"id": 316, "seek": 153728, "start": 1549.84, "end": 1557.52, "text": " Augmentations actually don't do anything and the reason initially they don't do anything is because we've got here something that says pre-compute", "tokens": [6088, 518, 763, 767, 500, 380, 360, 1340, 293, 264, 1778, 9105, 436, 500, 380, 360, 1340, 307, 570, 321, 600, 658, 510, 746, 300, 1619, 659, 12, 21541, 1169], "temperature": 0.0, "avg_logprob": -0.25403223037719724, "compression_ratio": 1.6324324324324324, "no_speech_prob": 3.6119593005423667e-06}, {"id": 317, "seek": 153728, "start": 1557.76, "end": 1560.6399999999999, "text": " Equals true, and we're going to come back to this lots of times", "tokens": [15624, 1124, 2074, 11, 293, 321, 434, 516, 281, 808, 646, 281, 341, 3195, 295, 1413], "temperature": 0.0, "avg_logprob": -0.25403223037719724, "compression_ratio": 1.6324324324324324, "no_speech_prob": 3.6119593005423667e-06}, {"id": 318, "seek": 156064, "start": 1560.64, "end": 1569.5200000000002, "text": " But basically what this is doing is do you remember this picture we saw where we learned each different layer", "tokens": [583, 1936, 437, 341, 307, 884, 307, 360, 291, 1604, 341, 3036, 321, 1866, 689, 321, 3264, 1184, 819, 4583], "temperature": 0.0, "avg_logprob": -0.19695832299404456, "compression_ratio": 1.5977011494252873, "no_speech_prob": 3.2887221550481627e-06}, {"id": 319, "seek": 156064, "start": 1570.5600000000002, "end": 1576.4, "text": " has these activations that basically look for you know anything from the middle of flowers to", "tokens": [575, 613, 2430, 763, 300, 1936, 574, 337, 291, 458, 1340, 490, 264, 2808, 295, 8085, 281], "temperature": 0.0, "avg_logprob": -0.19695832299404456, "compression_ratio": 1.5977011494252873, "no_speech_prob": 3.2887221550481627e-06}, {"id": 320, "seek": 156064, "start": 1577.5200000000002, "end": 1579.4, "text": " eyeballs of", "tokens": [43758, 295], "temperature": 0.0, "avg_logprob": -0.19695832299404456, "compression_ratio": 1.5977011494252873, "no_speech_prob": 3.2887221550481627e-06}, {"id": 321, "seek": 156064, "start": 1579.4, "end": 1584.8000000000002, "text": " Birds or whatever right and so literally what happens is that?", "tokens": [41456, 420, 2035, 558, 293, 370, 3736, 437, 2314, 307, 300, 30], "temperature": 0.0, "avg_logprob": -0.19695832299404456, "compression_ratio": 1.5977011494252873, "no_speech_prob": 3.2887221550481627e-06}, {"id": 322, "seek": 158480, "start": 1584.8, "end": 1591.52, "text": " the the later layers of this convolutional neural network have these things called activations and", "tokens": [264, 264, 1780, 7914, 295, 341, 45216, 304, 18161, 3209, 362, 613, 721, 1219, 2430, 763, 293], "temperature": 0.0, "avg_logprob": -0.20008426708179516, "compression_ratio": 1.737556561085973, "no_speech_prob": 1.1365607406332856e-06}, {"id": 323, "seek": 158480, "start": 1591.96, "end": 1597.68, "text": " Activation literally it's a number an activation is a number that says this feature like", "tokens": [28550, 399, 3736, 309, 311, 257, 1230, 364, 24433, 307, 257, 1230, 300, 1619, 341, 4111, 411], "temperature": 0.0, "avg_logprob": -0.20008426708179516, "compression_ratio": 1.737556561085973, "no_speech_prob": 1.1365607406332856e-06}, {"id": 324, "seek": 158480, "start": 1598.84, "end": 1604.8799999999999, "text": " Eyeball of bird is in this location with this level of confidence with its probability", "tokens": [21603, 3129, 295, 5255, 307, 294, 341, 4914, 365, 341, 1496, 295, 6687, 365, 1080, 8482], "temperature": 0.0, "avg_logprob": -0.20008426708179516, "compression_ratio": 1.737556561085973, "no_speech_prob": 1.1365607406332856e-06}, {"id": 325, "seek": 158480, "start": 1605.0, "end": 1608.6, "text": " right, and so we're going to see a lot of this later, but", "tokens": [558, 11, 293, 370, 321, 434, 516, 281, 536, 257, 688, 295, 341, 1780, 11, 457], "temperature": 0.0, "avg_logprob": -0.20008426708179516, "compression_ratio": 1.737556561085973, "no_speech_prob": 1.1365607406332856e-06}, {"id": 326, "seek": 158480, "start": 1609.52, "end": 1612.84, "text": " What we can do is we can say all right well in this", "tokens": [708, 321, 393, 360, 307, 321, 393, 584, 439, 558, 731, 294, 341], "temperature": 0.0, "avg_logprob": -0.20008426708179516, "compression_ratio": 1.737556561085973, "no_speech_prob": 1.1365607406332856e-06}, {"id": 327, "seek": 161284, "start": 1612.84, "end": 1620.08, "text": " We've got a pre-trained network remember and a pre-trained network is one where it's already learned to recognize certain things in this case", "tokens": [492, 600, 658, 257, 659, 12, 17227, 2001, 3209, 1604, 293, 257, 659, 12, 17227, 2001, 3209, 307, 472, 689, 309, 311, 1217, 3264, 281, 5521, 1629, 721, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.16370798960453323, "compression_ratio": 1.831541218637993, "no_speech_prob": 4.785053079103818e-06}, {"id": 328, "seek": 161284, "start": 1620.08, "end": 1625.56, "text": " It's learned to recognize the one and a half million images in the image net data set and so", "tokens": [467, 311, 3264, 281, 5521, 264, 472, 293, 257, 1922, 2459, 5267, 294, 264, 3256, 2533, 1412, 992, 293, 370], "temperature": 0.0, "avg_logprob": -0.16370798960453323, "compression_ratio": 1.831541218637993, "no_speech_prob": 4.785053079103818e-06}, {"id": 329, "seek": 161284, "start": 1625.9199999999998, "end": 1630.72, "text": " What we could do is we could take the the second last layer so the one which is like", "tokens": [708, 321, 727, 360, 307, 321, 727, 747, 264, 264, 1150, 1036, 4583, 370, 264, 472, 597, 307, 411], "temperature": 0.0, "avg_logprob": -0.16370798960453323, "compression_ratio": 1.831541218637993, "no_speech_prob": 4.785053079103818e-06}, {"id": 330, "seek": 161284, "start": 1631.32, "end": 1638.1599999999999, "text": " Got all of the information necessary to figure out what kind of thing a thing is and we can save those activations", "tokens": [5803, 439, 295, 264, 1589, 4818, 281, 2573, 484, 437, 733, 295, 551, 257, 551, 307, 293, 321, 393, 3155, 729, 2430, 763], "temperature": 0.0, "avg_logprob": -0.16370798960453323, "compression_ratio": 1.831541218637993, "no_speech_prob": 4.785053079103818e-06}, {"id": 331, "seek": 161284, "start": 1638.1599999999999, "end": 1642.4399999999998, "text": " So basically saving things saying you know there's this level of eyeballness", "tokens": [407, 1936, 6816, 721, 1566, 291, 458, 456, 311, 341, 1496, 295, 38868, 1287], "temperature": 0.0, "avg_logprob": -0.16370798960453323, "compression_ratio": 1.831541218637993, "no_speech_prob": 4.785053079103818e-06}, {"id": 332, "seek": 164244, "start": 1642.44, "end": 1650.44, "text": " Here and this level of dog's faceness here and this level of fluffy ear there and so forth and so we save for every image", "tokens": [1692, 293, 341, 1496, 295, 3000, 311, 1915, 15264, 510, 293, 341, 1496, 295, 22778, 1273, 456, 293, 370, 5220, 293, 370, 321, 3155, 337, 633, 3256], "temperature": 0.0, "avg_logprob": -0.25164351171376753, "compression_ratio": 1.8035714285714286, "no_speech_prob": 9.223379493050743e-06}, {"id": 333, "seek": 164244, "start": 1650.96, "end": 1652.28, "text": " these", "tokens": [613], "temperature": 0.0, "avg_logprob": -0.25164351171376753, "compression_ratio": 1.8035714285714286, "no_speech_prob": 9.223379493050743e-06}, {"id": 334, "seek": 164244, "start": 1652.28, "end": 1655.72, "text": " Activations and that we call them the pre computed activations", "tokens": [28550, 763, 293, 300, 321, 818, 552, 264, 659, 40610, 2430, 763], "temperature": 0.0, "avg_logprob": -0.25164351171376753, "compression_ratio": 1.8035714285714286, "no_speech_prob": 9.223379493050743e-06}, {"id": 335, "seek": 164244, "start": 1655.72, "end": 1661.04, "text": " And so the idea is now that when we want to create a new classifier", "tokens": [400, 370, 264, 1558, 307, 586, 300, 562, 321, 528, 281, 1884, 257, 777, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.25164351171376753, "compression_ratio": 1.8035714285714286, "no_speech_prob": 9.223379493050743e-06}, {"id": 336, "seek": 164244, "start": 1661.48, "end": 1665.52, "text": " Which can basically take advantage of these pre computed activations?", "tokens": [3013, 393, 1936, 747, 5002, 295, 613, 659, 40610, 2430, 763, 30], "temperature": 0.0, "avg_logprob": -0.25164351171376753, "compression_ratio": 1.8035714285714286, "no_speech_prob": 9.223379493050743e-06}, {"id": 337, "seek": 164244, "start": 1666.04, "end": 1669.8400000000001, "text": " We can just very quickly train. We'll learn all the details of this shortly", "tokens": [492, 393, 445, 588, 2661, 3847, 13, 492, 603, 1466, 439, 264, 4365, 295, 341, 13392], "temperature": 0.0, "avg_logprob": -0.25164351171376753, "compression_ratio": 1.8035714285714286, "no_speech_prob": 9.223379493050743e-06}, {"id": 338, "seek": 166984, "start": 1669.84, "end": 1677.1999999999998, "text": " We can very quickly train a simple linear model based on those and so that's what happens when we say pre compute equals true", "tokens": [492, 393, 588, 2661, 3847, 257, 2199, 8213, 2316, 2361, 322, 729, 293, 370, 300, 311, 437, 2314, 562, 321, 584, 659, 14722, 6915, 2074], "temperature": 0.0, "avg_logprob": -0.15905732241543857, "compression_ratio": 1.7743190661478598, "no_speech_prob": 5.338114533515181e-06}, {"id": 339, "seek": 166984, "start": 1677.1999999999998, "end": 1684.54, "text": " And that's why you may have noticed this week the first time that you run a model a new model", "tokens": [400, 300, 311, 983, 291, 815, 362, 5694, 341, 1243, 264, 700, 565, 300, 291, 1190, 257, 2316, 257, 777, 2316], "temperature": 0.0, "avg_logprob": -0.15905732241543857, "compression_ratio": 1.7743190661478598, "no_speech_prob": 5.338114533515181e-06}, {"id": 340, "seek": 166984, "start": 1684.9199999999998, "end": 1687.0, "text": " It takes a minute or two", "tokens": [467, 2516, 257, 3456, 420, 732], "temperature": 0.0, "avg_logprob": -0.15905732241543857, "compression_ratio": 1.7743190661478598, "no_speech_prob": 5.338114533515181e-06}, {"id": 341, "seek": 166984, "start": 1687.6799999999998, "end": 1694.22, "text": " Whereas you saw when I ran it took like five or ten seconds took you a minute or two and that's because it had to pre compute", "tokens": [13813, 291, 1866, 562, 286, 5872, 309, 1890, 411, 1732, 420, 2064, 3949, 1890, 291, 257, 3456, 420, 732, 293, 300, 311, 570, 309, 632, 281, 659, 14722], "temperature": 0.0, "avg_logprob": -0.15905732241543857, "compression_ratio": 1.7743190661478598, "no_speech_prob": 5.338114533515181e-06}, {"id": 342, "seek": 166984, "start": 1694.78, "end": 1699.8, "text": " These activations and just has to do that once if you're using like your own computer", "tokens": [1981, 2430, 763, 293, 445, 575, 281, 360, 300, 1564, 498, 291, 434, 1228, 411, 428, 1065, 3820], "temperature": 0.0, "avg_logprob": -0.15905732241543857, "compression_ratio": 1.7743190661478598, "no_speech_prob": 5.338114533515181e-06}, {"id": 343, "seek": 169980, "start": 1699.8, "end": 1701.8799999999999, "text": " Or AWS it just has to do it once ever", "tokens": [1610, 17650, 309, 445, 575, 281, 360, 309, 1564, 1562], "temperature": 0.0, "avg_logprob": -0.18004661339979905, "compression_ratio": 1.811659192825112, "no_speech_prob": 2.4060891519184224e-06}, {"id": 344, "seek": 169980, "start": 1702.48, "end": 1706.86, "text": " If you're using Cressel it actually has to do it once", "tokens": [759, 291, 434, 1228, 383, 735, 338, 309, 767, 575, 281, 360, 309, 1564], "temperature": 0.0, "avg_logprob": -0.18004661339979905, "compression_ratio": 1.811659192825112, "no_speech_prob": 2.4060891519184224e-06}, {"id": 345, "seek": 169980, "start": 1707.56, "end": 1711.44, "text": " every single time you rerun Cressel because Cressel uses a", "tokens": [633, 2167, 565, 291, 43819, 409, 383, 735, 338, 570, 383, 735, 338, 4960, 257], "temperature": 0.0, "avg_logprob": -0.18004661339979905, "compression_ratio": 1.811659192825112, "no_speech_prob": 2.4060891519184224e-06}, {"id": 346, "seek": 169980, "start": 1712.3999999999999, "end": 1714.32, "text": " Just for these pre computed activations", "tokens": [1449, 337, 613, 659, 40610, 2430, 763], "temperature": 0.0, "avg_logprob": -0.18004661339979905, "compression_ratio": 1.811659192825112, "no_speech_prob": 2.4060891519184224e-06}, {"id": 347, "seek": 169980, "start": 1714.32, "end": 1719.72, "text": " It uses a special little kind of scratch space that disappears each time you restart your Cressel instance", "tokens": [467, 4960, 257, 2121, 707, 733, 295, 8459, 1901, 300, 25527, 1184, 565, 291, 21022, 428, 383, 735, 338, 5197], "temperature": 0.0, "avg_logprob": -0.18004661339979905, "compression_ratio": 1.811659192825112, "no_speech_prob": 2.4060891519184224e-06}, {"id": 348, "seek": 169980, "start": 1720.24, "end": 1726.56, "text": " So other than the special case of Cressel generally speak you just have to run it once ever for a data set", "tokens": [407, 661, 813, 264, 2121, 1389, 295, 383, 735, 338, 5101, 1710, 291, 445, 362, 281, 1190, 309, 1564, 1562, 337, 257, 1412, 992], "temperature": 0.0, "avg_logprob": -0.18004661339979905, "compression_ratio": 1.811659192825112, "no_speech_prob": 2.4060891519184224e-06}, {"id": 349, "seek": 172656, "start": 1726.56, "end": 1728.56, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.15756183862686157, "compression_ratio": 1.7456140350877194, "no_speech_prob": 2.225267053290736e-06}, {"id": 350, "seek": 172656, "start": 1729.12, "end": 1734.1399999999999, "text": " So the issue with that is that since we've pre computed for each image", "tokens": [407, 264, 2734, 365, 300, 307, 300, 1670, 321, 600, 659, 40610, 337, 1184, 3256], "temperature": 0.0, "avg_logprob": -0.15756183862686157, "compression_ratio": 1.7456140350877194, "no_speech_prob": 2.225267053290736e-06}, {"id": 351, "seek": 172656, "start": 1734.72, "end": 1738.1599999999999, "text": " you know how much does it have an ear here, and how much does it have a", "tokens": [291, 458, 577, 709, 775, 309, 362, 364, 1273, 510, 11, 293, 577, 709, 775, 309, 362, 257], "temperature": 0.0, "avg_logprob": -0.15756183862686157, "compression_ratio": 1.7456140350877194, "no_speech_prob": 2.225267053290736e-06}, {"id": 352, "seek": 172656, "start": 1739.32, "end": 1745.08, "text": " Lizards eyeball there and so forth that means that data augmentations don't work right in other words even though", "tokens": [16480, 2287, 38868, 456, 293, 370, 5220, 300, 1355, 300, 1412, 29919, 763, 500, 380, 589, 558, 294, 661, 2283, 754, 1673], "temperature": 0.0, "avg_logprob": -0.15756183862686157, "compression_ratio": 1.7456140350877194, "no_speech_prob": 2.225267053290736e-06}, {"id": 353, "seek": 172656, "start": 1745.08, "end": 1751.82, "text": " We're trying to show it a different version of the cat each time we've pre computed the activations for a particular version of that cat", "tokens": [492, 434, 1382, 281, 855, 309, 257, 819, 3037, 295, 264, 3857, 1184, 565, 321, 600, 659, 40610, 264, 2430, 763, 337, 257, 1729, 3037, 295, 300, 3857], "temperature": 0.0, "avg_logprob": -0.15756183862686157, "compression_ratio": 1.7456140350877194, "no_speech_prob": 2.225267053290736e-06}, {"id": 354, "seek": 175182, "start": 1751.82, "end": 1757.8799999999999, "text": " So in order to use data augmentation. We just have to go learn dot pre compute equals false", "tokens": [407, 294, 1668, 281, 764, 1412, 14501, 19631, 13, 492, 445, 362, 281, 352, 1466, 5893, 659, 14722, 6915, 7908], "temperature": 0.0, "avg_logprob": -0.23587955898708768, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.1233731786196586e-06}, {"id": 355, "seek": 175182, "start": 1758.6399999999999, "end": 1760.98, "text": " Okay, and then we can run a few more", "tokens": [1033, 11, 293, 550, 321, 393, 1190, 257, 1326, 544], "temperature": 0.0, "avg_logprob": -0.23587955898708768, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.1233731786196586e-06}, {"id": 356, "seek": 175182, "start": 1761.78, "end": 1767.5, "text": " Epochs right and so you can see here that as we run more Epochs", "tokens": [462, 2259, 28346, 558, 293, 370, 291, 393, 536, 510, 300, 382, 321, 1190, 544, 462, 2259, 28346], "temperature": 0.0, "avg_logprob": -0.23587955898708768, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.1233731786196586e-06}, {"id": 357, "seek": 175182, "start": 1768.02, "end": 1770.6599999999999, "text": " The accuracy isn't particularly getting better", "tokens": [440, 14170, 1943, 380, 4098, 1242, 1101], "temperature": 0.0, "avg_logprob": -0.23587955898708768, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.1233731786196586e-06}, {"id": 358, "seek": 175182, "start": 1770.6599999999999, "end": 1777.86, "text": " All right, that's the bad news the good news is that you can see the the train loss, right?", "tokens": [1057, 558, 11, 300, 311, 264, 1578, 2583, 264, 665, 2583, 307, 300, 291, 393, 536, 264, 264, 3847, 4470, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.23587955898708768, "compression_ratio": 1.5687203791469195, "no_speech_prob": 2.1233731786196586e-06}, {"id": 359, "seek": 177786, "start": 1777.86, "end": 1783.62, "text": " This is like the a way of measuring the error of this model although. That's getting better the error is going down", "tokens": [639, 307, 411, 264, 257, 636, 295, 13389, 264, 6713, 295, 341, 2316, 4878, 13, 663, 311, 1242, 1101, 264, 6713, 307, 516, 760], "temperature": 0.0, "avg_logprob": -0.16904940647361552, "compression_ratio": 1.9437751004016064, "no_speech_prob": 1.040778485617011e-07}, {"id": 360, "seek": 177786, "start": 1784.02, "end": 1787.34, "text": " The validation error isn't going down", "tokens": [440, 24071, 6713, 1943, 380, 516, 760], "temperature": 0.0, "avg_logprob": -0.16904940647361552, "compression_ratio": 1.9437751004016064, "no_speech_prob": 1.040778485617011e-07}, {"id": 361, "seek": 177786, "start": 1787.9799999999998, "end": 1795.4599999999998, "text": " But we're not overfitting and overfitting would mean that the training loss is much lower than the validation loss", "tokens": [583, 321, 434, 406, 670, 69, 2414, 293, 670, 69, 2414, 576, 914, 300, 264, 3097, 4470, 307, 709, 3126, 813, 264, 24071, 4470], "temperature": 0.0, "avg_logprob": -0.16904940647361552, "compression_ratio": 1.9437751004016064, "no_speech_prob": 1.040778485617011e-07}, {"id": 362, "seek": 177786, "start": 1795.6599999999999, "end": 1798.02, "text": " And we're going to talk about that a lot during this course", "tokens": [400, 321, 434, 516, 281, 751, 466, 300, 257, 688, 1830, 341, 1164], "temperature": 0.0, "avg_logprob": -0.16904940647361552, "compression_ratio": 1.9437751004016064, "no_speech_prob": 1.040778485617011e-07}, {"id": 363, "seek": 177786, "start": 1798.02, "end": 1803.3799999999999, "text": " But the general idea here is if you're doing a much better job on the training set", "tokens": [583, 264, 2674, 1558, 510, 307, 498, 291, 434, 884, 257, 709, 1101, 1691, 322, 264, 3097, 992], "temperature": 0.0, "avg_logprob": -0.16904940647361552, "compression_ratio": 1.9437751004016064, "no_speech_prob": 1.040778485617011e-07}, {"id": 364, "seek": 180338, "start": 1803.38, "end": 1808.7800000000002, "text": " Then you are on the validation set that means your models not generalize so we're not at that point", "tokens": [1396, 291, 366, 322, 264, 24071, 992, 300, 1355, 428, 5245, 406, 2674, 1125, 370, 321, 434, 406, 412, 300, 935], "temperature": 0.0, "avg_logprob": -0.20039654781943875, "compression_ratio": 1.5826086956521739, "no_speech_prob": 1.7603388187126257e-06}, {"id": 365, "seek": 180338, "start": 1809.3000000000002, "end": 1812.5, "text": " Which is good, but we're not really improving", "tokens": [3013, 307, 665, 11, 457, 321, 434, 406, 534, 11470], "temperature": 0.0, "avg_logprob": -0.20039654781943875, "compression_ratio": 1.5826086956521739, "no_speech_prob": 1.7603388187126257e-06}, {"id": 366, "seek": 180338, "start": 1813.3000000000002, "end": 1816.14, "text": " So we're going to have to figure out how to deal with that", "tokens": [407, 321, 434, 516, 281, 362, 281, 2573, 484, 577, 281, 2028, 365, 300], "temperature": 0.0, "avg_logprob": -0.20039654781943875, "compression_ratio": 1.5826086956521739, "no_speech_prob": 1.7603388187126257e-06}, {"id": 367, "seek": 180338, "start": 1817.0600000000002, "end": 1821.66, "text": " Before we do I want to show you one other cool trick. I've added here", "tokens": [4546, 321, 360, 286, 528, 281, 855, 291, 472, 661, 1627, 4282, 13, 286, 600, 3869, 510], "temperature": 0.0, "avg_logprob": -0.20039654781943875, "compression_ratio": 1.5826086956521739, "no_speech_prob": 1.7603388187126257e-06}, {"id": 368, "seek": 180338, "start": 1822.22, "end": 1824.22, "text": " Cycle length equals one", "tokens": [10295, 2160, 4641, 6915, 472], "temperature": 0.0, "avg_logprob": -0.20039654781943875, "compression_ratio": 1.5826086956521739, "no_speech_prob": 1.7603388187126257e-06}, {"id": 369, "seek": 180338, "start": 1824.42, "end": 1826.7800000000002, "text": " And this is another really interesting idea", "tokens": [400, 341, 307, 1071, 534, 1880, 1558], "temperature": 0.0, "avg_logprob": -0.20039654781943875, "compression_ratio": 1.5826086956521739, "no_speech_prob": 1.7603388187126257e-06}, {"id": 370, "seek": 180338, "start": 1828.5, "end": 1830.5, "text": " Here's the basic idea", "tokens": [1692, 311, 264, 3875, 1558], "temperature": 0.0, "avg_logprob": -0.20039654781943875, "compression_ratio": 1.5826086956521739, "no_speech_prob": 1.7603388187126257e-06}, {"id": 371, "seek": 183050, "start": 1830.5, "end": 1833.46, "text": " Cycle length equals one enables a recent", "tokens": [10295, 2160, 4641, 6915, 472, 17077, 257, 5162], "temperature": 0.0, "avg_logprob": -0.2519466309320359, "compression_ratio": 1.8010204081632653, "no_speech_prob": 8.990940614239662e-07}, {"id": 372, "seek": 183050, "start": 1834.22, "end": 1841.86, "text": " fairly recent discovery in deep learning called stochastic gradient descent with restarts and the basic idea is this as you", "tokens": [6457, 5162, 12114, 294, 2452, 2539, 1219, 342, 8997, 2750, 16235, 23475, 365, 1472, 11814, 293, 264, 3875, 1558, 307, 341, 382, 291], "temperature": 0.0, "avg_logprob": -0.2519466309320359, "compression_ratio": 1.8010204081632653, "no_speech_prob": 8.990940614239662e-07}, {"id": 373, "seek": 183050, "start": 1842.74, "end": 1843.78, "text": " as", "tokens": [382], "temperature": 0.0, "avg_logprob": -0.2519466309320359, "compression_ratio": 1.8010204081632653, "no_speech_prob": 8.990940614239662e-07}, {"id": 374, "seek": 183050, "start": 1843.78, "end": 1849.78, "text": " You get closer and closer as you get closer and closer to the right spot", "tokens": [509, 483, 4966, 293, 4966, 382, 291, 483, 4966, 293, 4966, 281, 264, 558, 4008], "temperature": 0.0, "avg_logprob": -0.2519466309320359, "compression_ratio": 1.8010204081632653, "no_speech_prob": 8.990940614239662e-07}, {"id": 375, "seek": 183050, "start": 1850.5, "end": 1851.9, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.2519466309320359, "compression_ratio": 1.8010204081632653, "no_speech_prob": 8.990940614239662e-07}, {"id": 376, "seek": 183050, "start": 1851.9, "end": 1858.02, "text": " I'm getting closer and closer. I may want to start to decrease my learning rate right because I get closer", "tokens": [286, 478, 1242, 4966, 293, 4966, 13, 286, 815, 528, 281, 722, 281, 11514, 452, 2539, 3314, 558, 570, 286, 483, 4966], "temperature": 0.0, "avg_logprob": -0.2519466309320359, "compression_ratio": 1.8010204081632653, "no_speech_prob": 8.990940614239662e-07}, {"id": 377, "seek": 185802, "start": 1858.02, "end": 1865.12, "text": " I'm kind of like oh, I'm pretty close now, so let's let's slow down my steps to try to get exactly to the right spot", "tokens": [286, 478, 733, 295, 411, 1954, 11, 286, 478, 1238, 1998, 586, 11, 370, 718, 311, 718, 311, 2964, 760, 452, 4439, 281, 853, 281, 483, 2293, 281, 264, 558, 4008], "temperature": 0.0, "avg_logprob": -0.17061957684192028, "compression_ratio": 1.6490384615384615, "no_speech_prob": 8.851545203469868e-07}, {"id": 378, "seek": 185802, "start": 1865.5, "end": 1869.6399999999999, "text": " Right and so as we do more iterations", "tokens": [1779, 293, 370, 382, 321, 360, 544, 36540], "temperature": 0.0, "avg_logprob": -0.17061957684192028, "compression_ratio": 1.6490384615384615, "no_speech_prob": 8.851545203469868e-07}, {"id": 379, "seek": 185802, "start": 1872.46, "end": 1874.46, "text": " Our learning rate", "tokens": [2621, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.17061957684192028, "compression_ratio": 1.6490384615384615, "no_speech_prob": 8.851545203469868e-07}, {"id": 380, "seek": 185802, "start": 1874.62, "end": 1876.62, "text": " Perhaps should actually go", "tokens": [10517, 820, 767, 352], "temperature": 0.0, "avg_logprob": -0.17061957684192028, "compression_ratio": 1.6490384615384615, "no_speech_prob": 8.851545203469868e-07}, {"id": 381, "seek": 185802, "start": 1876.94, "end": 1883.1, "text": " Down right because as we go along we're getting closer and closer to where we want to be and we want to like get exactly", "tokens": [9506, 558, 570, 382, 321, 352, 2051, 321, 434, 1242, 4966, 293, 4966, 281, 689, 321, 528, 281, 312, 293, 321, 528, 281, 411, 483, 2293], "temperature": 0.0, "avg_logprob": -0.17061957684192028, "compression_ratio": 1.6490384615384615, "no_speech_prob": 8.851545203469868e-07}, {"id": 382, "seek": 185802, "start": 1883.1, "end": 1885.1, "text": " To the right spot okay", "tokens": [1407, 264, 558, 4008, 1392], "temperature": 0.0, "avg_logprob": -0.17061957684192028, "compression_ratio": 1.6490384615384615, "no_speech_prob": 8.851545203469868e-07}, {"id": 383, "seek": 188510, "start": 1885.1, "end": 1890.26, "text": " So the idea of decreasing the learning rate as you train is called learning rate", "tokens": [407, 264, 1558, 295, 23223, 264, 2539, 3314, 382, 291, 3847, 307, 1219, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.20397533459609815, "compression_ratio": 1.8529411764705883, "no_speech_prob": 3.520908933296596e-07}, {"id": 384, "seek": 188510, "start": 1891.1399999999999, "end": 1892.6599999999999, "text": " annealing and", "tokens": [22256, 4270, 293], "temperature": 0.0, "avg_logprob": -0.20397533459609815, "compression_ratio": 1.8529411764705883, "no_speech_prob": 3.520908933296596e-07}, {"id": 385, "seek": 188510, "start": 1892.6599999999999, "end": 1895.82, "text": " It's it's very very common very very popular", "tokens": [467, 311, 309, 311, 588, 588, 2689, 588, 588, 3743], "temperature": 0.0, "avg_logprob": -0.20397533459609815, "compression_ratio": 1.8529411764705883, "no_speech_prob": 3.520908933296596e-07}, {"id": 386, "seek": 188510, "start": 1896.62, "end": 1899.1399999999999, "text": " Everybody uses it basically all the time", "tokens": [7646, 4960, 309, 1936, 439, 264, 565], "temperature": 0.0, "avg_logprob": -0.20397533459609815, "compression_ratio": 1.8529411764705883, "no_speech_prob": 3.520908933296596e-07}, {"id": 387, "seek": 188510, "start": 1899.8999999999999, "end": 1902.54, "text": " The most common kind of learning rate annealing is", "tokens": [440, 881, 2689, 733, 295, 2539, 3314, 22256, 4270, 307], "temperature": 0.0, "avg_logprob": -0.20397533459609815, "compression_ratio": 1.8529411764705883, "no_speech_prob": 3.520908933296596e-07}, {"id": 388, "seek": 188510, "start": 1903.3799999999999, "end": 1905.1399999999999, "text": " Really horrendously hacky", "tokens": [4083, 49520, 5098, 10339, 88], "temperature": 0.0, "avg_logprob": -0.20397533459609815, "compression_ratio": 1.8529411764705883, "no_speech_prob": 3.520908933296596e-07}, {"id": 389, "seek": 188510, "start": 1905.1399999999999, "end": 1911.4599999999998, "text": " It's basically that researchers like pick a learning rate that seems to work for a while and then when it stops learning", "tokens": [467, 311, 1936, 300, 10309, 411, 1888, 257, 2539, 3314, 300, 2544, 281, 589, 337, 257, 1339, 293, 550, 562, 309, 10094, 2539], "temperature": 0.0, "avg_logprob": -0.20397533459609815, "compression_ratio": 1.8529411764705883, "no_speech_prob": 3.520908933296596e-07}, {"id": 390, "seek": 191146, "start": 1911.46, "end": 1916.94, "text": " Well, they drop it down by about ten times, and then they keep learning a bit more until it doesn't seem to be improving", "tokens": [1042, 11, 436, 3270, 309, 760, 538, 466, 2064, 1413, 11, 293, 550, 436, 1066, 2539, 257, 857, 544, 1826, 309, 1177, 380, 1643, 281, 312, 11470], "temperature": 0.0, "avg_logprob": -0.17732904542167233, "compression_ratio": 1.7218045112781954, "no_speech_prob": 2.3454278164081188e-07}, {"id": 391, "seek": 191146, "start": 1916.94, "end": 1918.94, "text": " And they drop it down by another ten times", "tokens": [400, 436, 3270, 309, 760, 538, 1071, 2064, 1413], "temperature": 0.0, "avg_logprob": -0.17732904542167233, "compression_ratio": 1.7218045112781954, "no_speech_prob": 2.3454278164081188e-07}, {"id": 392, "seek": 191146, "start": 1919.18, "end": 1927.98, "text": " That's what most academic research papers and most people in industry do so this would be like stepwise annealing very manual very annoying", "tokens": [663, 311, 437, 881, 7778, 2132, 10577, 293, 881, 561, 294, 3518, 360, 370, 341, 576, 312, 411, 1823, 3711, 22256, 4270, 588, 9688, 588, 11304], "temperature": 0.0, "avg_logprob": -0.17732904542167233, "compression_ratio": 1.7218045112781954, "no_speech_prob": 2.3454278164081188e-07}, {"id": 393, "seek": 191146, "start": 1928.42, "end": 1929.54, "text": " a", "tokens": [257], "temperature": 0.0, "avg_logprob": -0.17732904542167233, "compression_ratio": 1.7218045112781954, "no_speech_prob": 2.3454278164081188e-07}, {"id": 394, "seek": 191146, "start": 1929.54, "end": 1935.26, "text": " Better approach is simply to pick some kind of functional form like a line", "tokens": [15753, 3109, 307, 2935, 281, 1888, 512, 733, 295, 11745, 1254, 411, 257, 1622], "temperature": 0.0, "avg_logprob": -0.17732904542167233, "compression_ratio": 1.7218045112781954, "no_speech_prob": 2.3454278164081188e-07}, {"id": 395, "seek": 193526, "start": 1935.26, "end": 1940.7, "text": " It turns out that a really good functional form is one half of a cosine curve", "tokens": [467, 4523, 484, 300, 257, 534, 665, 11745, 1254, 307, 472, 1922, 295, 257, 23565, 7605], "temperature": 0.0, "avg_logprob": -0.14437290266448377, "compression_ratio": 1.7685950413223142, "no_speech_prob": 1.328774260400678e-06}, {"id": 396, "seek": 193526, "start": 1942.7, "end": 1949.26, "text": " Right and the reason why is that for a while when you're not very close you kind of have a really high learning rate", "tokens": [1779, 293, 264, 1778, 983, 307, 300, 337, 257, 1339, 562, 291, 434, 406, 588, 1998, 291, 733, 295, 362, 257, 534, 1090, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.14437290266448377, "compression_ratio": 1.7685950413223142, "no_speech_prob": 1.328774260400678e-06}, {"id": 397, "seek": 193526, "start": 1949.26, "end": 1954.5, "text": " And then as you do get close you kind of quickly drop down and do a few iterations with a really low", "tokens": [400, 550, 382, 291, 360, 483, 1998, 291, 733, 295, 2661, 3270, 760, 293, 360, 257, 1326, 36540, 365, 257, 534, 2295], "temperature": 0.0, "avg_logprob": -0.14437290266448377, "compression_ratio": 1.7685950413223142, "no_speech_prob": 1.328774260400678e-06}, {"id": 398, "seek": 193526, "start": 1954.94, "end": 1957.98, "text": " Learning rate and so this is called cosine annealing", "tokens": [15205, 3314, 293, 370, 341, 307, 1219, 23565, 22256, 4270], "temperature": 0.0, "avg_logprob": -0.14437290266448377, "compression_ratio": 1.7685950413223142, "no_speech_prob": 1.328774260400678e-06}, {"id": 399, "seek": 193526, "start": 1958.9, "end": 1963.02, "text": " So to those of you haven't done trigonometry for a while cosine basically looks", "tokens": [407, 281, 729, 295, 291, 2378, 380, 1096, 35386, 266, 34730, 337, 257, 1339, 23565, 1936, 1542], "temperature": 0.0, "avg_logprob": -0.14437290266448377, "compression_ratio": 1.7685950413223142, "no_speech_prob": 1.328774260400678e-06}, {"id": 400, "seek": 196302, "start": 1963.02, "end": 1965.54, "text": " Something like this right so we've picked", "tokens": [6595, 411, 341, 558, 370, 321, 600, 6183], "temperature": 0.0, "avg_logprob": -0.21708070148121228, "compression_ratio": 1.62, "no_speech_prob": 6.577918156835949e-07}, {"id": 401, "seek": 196302, "start": 1966.7, "end": 1968.7, "text": " one little half piece", "tokens": [472, 707, 1922, 2522], "temperature": 0.0, "avg_logprob": -0.21708070148121228, "compression_ratio": 1.62, "no_speech_prob": 6.577918156835949e-07}, {"id": 402, "seek": 196302, "start": 1969.42, "end": 1971.18, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.21708070148121228, "compression_ratio": 1.62, "no_speech_prob": 6.577918156835949e-07}, {"id": 403, "seek": 196302, "start": 1971.18, "end": 1973.18, "text": " So we're going to use cosine annealing", "tokens": [407, 321, 434, 516, 281, 764, 23565, 22256, 4270], "temperature": 0.0, "avg_logprob": -0.21708070148121228, "compression_ratio": 1.62, "no_speech_prob": 6.577918156835949e-07}, {"id": 404, "seek": 196302, "start": 1974.54, "end": 1976.54, "text": " But here's the thing", "tokens": [583, 510, 311, 264, 551], "temperature": 0.0, "avg_logprob": -0.21708070148121228, "compression_ratio": 1.62, "no_speech_prob": 6.577918156835949e-07}, {"id": 405, "seek": 196302, "start": 1976.74, "end": 1978.74, "text": " when you're in a very", "tokens": [562, 291, 434, 294, 257, 588], "temperature": 0.0, "avg_logprob": -0.21708070148121228, "compression_ratio": 1.62, "no_speech_prob": 6.577918156835949e-07}, {"id": 406, "seek": 196302, "start": 1979.5, "end": 1985.3, "text": " High dimensional space right and here. We're only able to show three dimensions right but in reality", "tokens": [5229, 18795, 1901, 558, 293, 510, 13, 492, 434, 787, 1075, 281, 855, 1045, 12819, 558, 457, 294, 4103], "temperature": 0.0, "avg_logprob": -0.21708070148121228, "compression_ratio": 1.62, "no_speech_prob": 6.577918156835949e-07}, {"id": 407, "seek": 196302, "start": 1985.3, "end": 1987.3, "text": " We've got hundreds of millions of dimensions", "tokens": [492, 600, 658, 6779, 295, 6803, 295, 12819], "temperature": 0.0, "avg_logprob": -0.21708070148121228, "compression_ratio": 1.62, "no_speech_prob": 6.577918156835949e-07}, {"id": 408, "seek": 196302, "start": 1988.02, "end": 1990.02, "text": " We've got lots of", "tokens": [492, 600, 658, 3195, 295], "temperature": 0.0, "avg_logprob": -0.21708070148121228, "compression_ratio": 1.62, "no_speech_prob": 6.577918156835949e-07}, {"id": 409, "seek": 196302, "start": 1990.22, "end": 1991.42, "text": " different", "tokens": [819], "temperature": 0.0, "avg_logprob": -0.21708070148121228, "compression_ratio": 1.62, "no_speech_prob": 6.577918156835949e-07}, {"id": 410, "seek": 199142, "start": 1991.42, "end": 1994.42, "text": " Fairly flat points they may not be actual local minima", "tokens": [12157, 356, 4962, 2793, 436, 815, 406, 312, 3539, 2654, 4464, 64], "temperature": 0.0, "avg_logprob": -0.15319526195526123, "compression_ratio": 1.5783783783783785, "no_speech_prob": 3.2377308798459126e-06}, {"id": 411, "seek": 199142, "start": 1994.42, "end": 2001.1200000000001, "text": " But they're fairly flat points all of which are pretty good right but they might differ in a really interesting way", "tokens": [583, 436, 434, 6457, 4962, 2793, 439, 295, 597, 366, 1238, 665, 558, 457, 436, 1062, 743, 294, 257, 534, 1880, 636], "temperature": 0.0, "avg_logprob": -0.15319526195526123, "compression_ratio": 1.5783783783783785, "no_speech_prob": 3.2377308798459126e-06}, {"id": 412, "seek": 199142, "start": 2001.22, "end": 2003.54, "text": " Which is that some of those flat points?", "tokens": [3013, 307, 300, 512, 295, 729, 4962, 2793, 30], "temperature": 0.0, "avg_logprob": -0.15319526195526123, "compression_ratio": 1.5783783783783785, "no_speech_prob": 3.2377308798459126e-06}, {"id": 413, "seek": 199142, "start": 2004.7, "end": 2006.7, "text": " Let me show you", "tokens": [961, 385, 855, 291], "temperature": 0.0, "avg_logprob": -0.15319526195526123, "compression_ratio": 1.5783783783783785, "no_speech_prob": 3.2377308798459126e-06}, {"id": 414, "seek": 199142, "start": 2012.8600000000001, "end": 2015.98, "text": " Let's imagine we've got a surface that looks something like this", "tokens": [961, 311, 3811, 321, 600, 658, 257, 3753, 300, 1542, 746, 411, 341], "temperature": 0.0, "avg_logprob": -0.15319526195526123, "compression_ratio": 1.5783783783783785, "no_speech_prob": 3.2377308798459126e-06}, {"id": 415, "seek": 201598, "start": 2015.98, "end": 2017.98, "text": " you", "tokens": [291], "temperature": 0.0, "avg_logprob": -0.2013645620907054, "compression_ratio": 1.6026200873362446, "no_speech_prob": 2.058040081465151e-06}, {"id": 416, "seek": 201598, "start": 2020.42, "end": 2023.9, "text": " Right now imagine that we kind of", "tokens": [1779, 586, 3811, 300, 321, 733, 295], "temperature": 0.0, "avg_logprob": -0.2013645620907054, "compression_ratio": 1.6026200873362446, "no_speech_prob": 2.058040081465151e-06}, {"id": 417, "seek": 201598, "start": 2024.42, "end": 2026.42, "text": " random guess started here and", "tokens": [4974, 2041, 1409, 510, 293], "temperature": 0.0, "avg_logprob": -0.2013645620907054, "compression_ratio": 1.6026200873362446, "no_speech_prob": 2.058040081465151e-06}, {"id": 418, "seek": 201598, "start": 2026.94, "end": 2031.34, "text": " Our initial therefore kind of learning rate annealing schedule got us down to here", "tokens": [2621, 5883, 4412, 733, 295, 2539, 3314, 22256, 4270, 7567, 658, 505, 760, 281, 510], "temperature": 0.0, "avg_logprob": -0.2013645620907054, "compression_ratio": 1.6026200873362446, "no_speech_prob": 2.058040081465151e-06}, {"id": 419, "seek": 201598, "start": 2032.26, "end": 2039.22, "text": " Now indeed that's a pretty nice low error right, but it probably doesn't generalize very well", "tokens": [823, 6451, 300, 311, 257, 1238, 1481, 2295, 6713, 558, 11, 457, 309, 1391, 1177, 380, 2674, 1125, 588, 731], "temperature": 0.0, "avg_logprob": -0.2013645620907054, "compression_ratio": 1.6026200873362446, "no_speech_prob": 2.058040081465151e-06}, {"id": 420, "seek": 203922, "start": 2039.22, "end": 2045.74, "text": " Which is to say if we use a different data set where things are just kind of slightly different in one of these directions", "tokens": [3013, 307, 281, 584, 498, 321, 764, 257, 819, 1412, 992, 689, 721, 366, 445, 733, 295, 4748, 819, 294, 472, 295, 613, 11095], "temperature": 0.0, "avg_logprob": -0.15143526168096633, "compression_ratio": 1.8528301886792453, "no_speech_prob": 2.058040536212502e-06}, {"id": 421, "seek": 203922, "start": 2046.34, "end": 2050.38, "text": " Suddenly it's a terrible solution right where else over here", "tokens": [21194, 309, 311, 257, 6237, 3827, 558, 689, 1646, 670, 510], "temperature": 0.0, "avg_logprob": -0.15143526168096633, "compression_ratio": 1.8528301886792453, "no_speech_prob": 2.058040536212502e-06}, {"id": 422, "seek": 203922, "start": 2051.1, "end": 2054.26, "text": " It's basically equally good in terms of loss", "tokens": [467, 311, 1936, 12309, 665, 294, 2115, 295, 4470], "temperature": 0.0, "avg_logprob": -0.15143526168096633, "compression_ratio": 1.8528301886792453, "no_speech_prob": 2.058040536212502e-06}, {"id": 423, "seek": 203922, "start": 2054.7, "end": 2061.0, "text": " Right, but it rather suggests that if you move if you have slightly different data sets that are slightly moved in different directions", "tokens": [1779, 11, 457, 309, 2831, 13409, 300, 498, 291, 1286, 498, 291, 362, 4748, 819, 1412, 6352, 300, 366, 4748, 4259, 294, 819, 11095], "temperature": 0.0, "avg_logprob": -0.15143526168096633, "compression_ratio": 1.8528301886792453, "no_speech_prob": 2.058040536212502e-06}, {"id": 424, "seek": 206100, "start": 2061.0, "end": 2069.62, "text": " It's still going to be good right so in other words we would expect this solution here is probably going to generalize better than", "tokens": [467, 311, 920, 516, 281, 312, 665, 558, 370, 294, 661, 2283, 321, 576, 2066, 341, 3827, 510, 307, 1391, 516, 281, 2674, 1125, 1101, 813], "temperature": 0.0, "avg_logprob": -0.18505080202792554, "compression_ratio": 1.7457627118644068, "no_speech_prob": 1.6028066056605894e-06}, {"id": 425, "seek": 206100, "start": 2069.62, "end": 2071.38, "text": " the spiky one", "tokens": [264, 637, 1035, 88, 472], "temperature": 0.0, "avg_logprob": -0.18505080202792554, "compression_ratio": 1.7457627118644068, "no_speech_prob": 1.6028066056605894e-06}, {"id": 426, "seek": 206100, "start": 2071.38, "end": 2078.34, "text": " So here's what we do is we've got like a bunch of different low bits right then our standard", "tokens": [407, 510, 311, 437, 321, 360, 307, 321, 600, 658, 411, 257, 3840, 295, 819, 2295, 9239, 558, 550, 527, 3832], "temperature": 0.0, "avg_logprob": -0.18505080202792554, "compression_ratio": 1.7457627118644068, "no_speech_prob": 1.6028066056605894e-06}, {"id": 427, "seek": 206100, "start": 2079.3, "end": 2085.14, "text": " Learning rate annealing approach will start to go downhill downhill downhill downhill downhill to one spot", "tokens": [15205, 3314, 22256, 4270, 3109, 486, 722, 281, 352, 29929, 29929, 29929, 29929, 29929, 281, 472, 4008], "temperature": 0.0, "avg_logprob": -0.18505080202792554, "compression_ratio": 1.7457627118644068, "no_speech_prob": 1.6028066056605894e-06}, {"id": 428, "seek": 208514, "start": 2085.14, "end": 2090.3799999999997, "text": " Right, but what we could do instead is use a learning rate schedule", "tokens": [1779, 11, 457, 437, 321, 727, 360, 2602, 307, 764, 257, 2539, 3314, 7567], "temperature": 0.0, "avg_logprob": -0.1863246254298998, "compression_ratio": 1.9270386266094421, "no_speech_prob": 3.288732159489882e-06}, {"id": 429, "seek": 208514, "start": 2091.66, "end": 2093.66, "text": " That looks like this", "tokens": [663, 1542, 411, 341], "temperature": 0.0, "avg_logprob": -0.1863246254298998, "compression_ratio": 1.9270386266094421, "no_speech_prob": 3.288732159489882e-06}, {"id": 430, "seek": 208514, "start": 2093.7, "end": 2099.3399999999997, "text": " Which is to say we do a cosine annealing and then suddenly jump up again and do a cosine annealing and then jump up again", "tokens": [3013, 307, 281, 584, 321, 360, 257, 23565, 22256, 4270, 293, 550, 5800, 3012, 493, 797, 293, 360, 257, 23565, 22256, 4270, 293, 550, 3012, 493, 797], "temperature": 0.0, "avg_logprob": -0.1863246254298998, "compression_ratio": 1.9270386266094421, "no_speech_prob": 3.288732159489882e-06}, {"id": 431, "seek": 208514, "start": 2099.3399999999997, "end": 2101.3399999999997, "text": " And so each time we jump up", "tokens": [400, 370, 1184, 565, 321, 3012, 493], "temperature": 0.0, "avg_logprob": -0.1863246254298998, "compression_ratio": 1.9270386266094421, "no_speech_prob": 3.288732159489882e-06}, {"id": 432, "seek": 208514, "start": 2101.66, "end": 2103.94, "text": " It means that if we're in a spiky bit", "tokens": [467, 1355, 300, 498, 321, 434, 294, 257, 637, 1035, 88, 857], "temperature": 0.0, "avg_logprob": -0.1863246254298998, "compression_ratio": 1.9270386266094421, "no_speech_prob": 3.288732159489882e-06}, {"id": 433, "seek": 208514, "start": 2103.94, "end": 2108.8599999999997, "text": " And then we suddenly increase the learning rate and it jumps now all the way over to here and", "tokens": [400, 550, 321, 5800, 3488, 264, 2539, 3314, 293, 309, 16704, 586, 439, 264, 636, 670, 281, 510, 293], "temperature": 0.0, "avg_logprob": -0.1863246254298998, "compression_ratio": 1.9270386266094421, "no_speech_prob": 3.288732159489882e-06}, {"id": 434, "seek": 208514, "start": 2109.46, "end": 2112.46, "text": " So then we kind of learning rate and near learning rate near that down to here", "tokens": [407, 550, 321, 733, 295, 2539, 3314, 293, 2651, 2539, 3314, 2651, 300, 760, 281, 510], "temperature": 0.0, "avg_logprob": -0.1863246254298998, "compression_ratio": 1.9270386266094421, "no_speech_prob": 3.288732159489882e-06}, {"id": 435, "seek": 211246, "start": 2112.46, "end": 2115.14, "text": " And then we jump up again to a high learning rate. Oh", "tokens": [400, 550, 321, 3012, 493, 797, 281, 257, 1090, 2539, 3314, 13, 876], "temperature": 0.0, "avg_logprob": -0.1799854107117385, "compression_ratio": 1.614213197969543, "no_speech_prob": 3.3405160593247274e-06}, {"id": 436, "seek": 211246, "start": 2115.86, "end": 2117.34, "text": " And it stays here", "tokens": [400, 309, 10834, 510], "temperature": 0.0, "avg_logprob": -0.1799854107117385, "compression_ratio": 1.614213197969543, "no_speech_prob": 3.3405160593247274e-06}, {"id": 437, "seek": 211246, "start": 2117.34, "end": 2120.86, "text": " Right so in other words each time we jump up the learning rate", "tokens": [1779, 370, 294, 661, 2283, 1184, 565, 321, 3012, 493, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.1799854107117385, "compression_ratio": 1.614213197969543, "no_speech_prob": 3.3405160593247274e-06}, {"id": 438, "seek": 211246, "start": 2121.02, "end": 2124.7200000000003, "text": " That means that if it's in a nasty spiky part of the surface", "tokens": [663, 1355, 300, 498, 309, 311, 294, 257, 17923, 637, 1035, 88, 644, 295, 264, 3753], "temperature": 0.0, "avg_logprob": -0.1799854107117385, "compression_ratio": 1.614213197969543, "no_speech_prob": 3.3405160593247274e-06}, {"id": 439, "seek": 211246, "start": 2124.7200000000003, "end": 2130.86, "text": " It's going to hop out of the spiky part and hopefully if we do that enough times. It'll eventually find a nice", "tokens": [467, 311, 516, 281, 3818, 484, 295, 264, 637, 1035, 88, 644, 293, 4696, 498, 321, 360, 300, 1547, 1413, 13, 467, 603, 4728, 915, 257, 1481], "temperature": 0.0, "avg_logprob": -0.1799854107117385, "compression_ratio": 1.614213197969543, "no_speech_prob": 3.3405160593247274e-06}, {"id": 440, "seek": 211246, "start": 2131.58, "end": 2133.58, "text": " smooth bowl", "tokens": [5508, 6571], "temperature": 0.0, "avg_logprob": -0.1799854107117385, "compression_ratio": 1.614213197969543, "no_speech_prob": 3.3405160593247274e-06}, {"id": 441, "seek": 213358, "start": 2133.58, "end": 2135.58, "text": " You", "tokens": [509], "temperature": 0.0, "avg_logprob": -0.3041675830709523, "compression_ratio": 1.4946808510638299, "no_speech_prob": 7.766732778691221e-06}, {"id": 442, "seek": 213358, "start": 2140.58, "end": 2149.02, "text": " Could you get the same effect by running multiple iterations through the different randomized starting points and eventually you explore all possible", "tokens": [7497, 291, 483, 264, 912, 1802, 538, 2614, 3866, 36540, 807, 264, 819, 38513, 2891, 2793, 293, 4728, 291, 6839, 439, 1944], "temperature": 0.0, "avg_logprob": -0.3041675830709523, "compression_ratio": 1.4946808510638299, "no_speech_prob": 7.766732778691221e-06}, {"id": 443, "seek": 213358, "start": 2150.66, "end": 2156.74, "text": " Yeah, so in fact that that's a great question and before this approach", "tokens": [865, 11, 370, 294, 1186, 300, 300, 311, 257, 869, 1168, 293, 949, 341, 3109], "temperature": 0.0, "avg_logprob": -0.3041675830709523, "compression_ratio": 1.4946808510638299, "no_speech_prob": 7.766732778691221e-06}, {"id": 444, "seek": 215674, "start": 2156.74, "end": 2165.08, "text": " Which is called stochastic gradient descent with restarts was was created", "tokens": [3013, 307, 1219, 342, 8997, 2750, 16235, 23475, 365, 1472, 11814, 390, 390, 2942], "temperature": 0.0, "avg_logprob": -0.21412442790137398, "compression_ratio": 1.6054054054054054, "no_speech_prob": 5.2553600653482135e-06}, {"id": 445, "seek": 215674, "start": 2165.7799999999997, "end": 2170.8199999999997, "text": " That's exactly what people used to do they used to create these things called ensembles where they would basically", "tokens": [663, 311, 2293, 437, 561, 1143, 281, 360, 436, 1143, 281, 1884, 613, 721, 1219, 12567, 2504, 904, 689, 436, 576, 1936], "temperature": 0.0, "avg_logprob": -0.21412442790137398, "compression_ratio": 1.6054054054054054, "no_speech_prob": 5.2553600653482135e-06}, {"id": 446, "seek": 215674, "start": 2172.3399999999997, "end": 2178.66, "text": " Relearn a whole new model ten times in the hope that one of them is like going to end up being better", "tokens": [1300, 306, 1083, 257, 1379, 777, 2316, 2064, 1413, 294, 264, 1454, 300, 472, 295, 552, 307, 411, 516, 281, 917, 493, 885, 1101], "temperature": 0.0, "avg_logprob": -0.21412442790137398, "compression_ratio": 1.6054054054054054, "no_speech_prob": 5.2553600653482135e-06}, {"id": 447, "seek": 215674, "start": 2179.5, "end": 2181.5, "text": " and so", "tokens": [293, 370], "temperature": 0.0, "avg_logprob": -0.21412442790137398, "compression_ratio": 1.6054054054054054, "no_speech_prob": 5.2553600653482135e-06}, {"id": 448, "seek": 218150, "start": 2181.5, "end": 2188.3, "text": " The cool thing about this stochastic gradient descent with restarts is that the model once we're in a reasonably good spot", "tokens": [440, 1627, 551, 466, 341, 342, 8997, 2750, 16235, 23475, 365, 1472, 11814, 307, 300, 264, 2316, 1564, 321, 434, 294, 257, 23551, 665, 4008], "temperature": 0.0, "avg_logprob": -0.192024697648718, "compression_ratio": 1.7053941908713692, "no_speech_prob": 3.5559446587285493e-06}, {"id": 449, "seek": 218150, "start": 2188.78, "end": 2190.82, "text": " Each time we jump up the learning rate", "tokens": [6947, 565, 321, 3012, 493, 264, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.192024697648718, "compression_ratio": 1.7053941908713692, "no_speech_prob": 3.5559446587285493e-06}, {"id": 450, "seek": 218150, "start": 2190.82, "end": 2197.14, "text": " It doesn't restart it actually hangs out in this nice part part of the space and then keeps getting better", "tokens": [467, 1177, 380, 21022, 309, 767, 35947, 484, 294, 341, 1481, 644, 644, 295, 264, 1901, 293, 550, 5965, 1242, 1101], "temperature": 0.0, "avg_logprob": -0.192024697648718, "compression_ratio": 1.7053941908713692, "no_speech_prob": 3.5559446587285493e-06}, {"id": 451, "seek": 218150, "start": 2197.42, "end": 2203.18, "text": " So interestingly it turns out that this approach where we do this a bunch of separate", "tokens": [407, 25873, 309, 4523, 484, 300, 341, 3109, 689, 321, 360, 341, 257, 3840, 295, 4994], "temperature": 0.0, "avg_logprob": -0.192024697648718, "compression_ratio": 1.7053941908713692, "no_speech_prob": 3.5559446587285493e-06}, {"id": 452, "seek": 218150, "start": 2203.7, "end": 2207.34, "text": " cosine and the only steps we end up with a better result", "tokens": [23565, 293, 264, 787, 4439, 321, 917, 493, 365, 257, 1101, 1874], "temperature": 0.0, "avg_logprob": -0.192024697648718, "compression_ratio": 1.7053941908713692, "no_speech_prob": 3.5559446587285493e-06}, {"id": 453, "seek": 220734, "start": 2207.34, "end": 2211.7400000000002, "text": " Than if we just randomly tried a few different starting points", "tokens": [18289, 498, 321, 445, 16979, 3031, 257, 1326, 819, 2891, 2793], "temperature": 0.0, "avg_logprob": -0.22825345327687818, "compression_ratio": 1.5515695067264574, "no_speech_prob": 4.63783590021194e-06}, {"id": 454, "seek": 220734, "start": 2212.38, "end": 2216.6600000000003, "text": " so it's a super neat trick and it's a", "tokens": [370, 309, 311, 257, 1687, 10654, 4282, 293, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.22825345327687818, "compression_ratio": 1.5515695067264574, "no_speech_prob": 4.63783590021194e-06}, {"id": 455, "seek": 220734, "start": 2217.5, "end": 2222.1000000000004, "text": " Fairly recent development, but and again almost nobody's heard of it", "tokens": [12157, 356, 5162, 3250, 11, 457, 293, 797, 1920, 5079, 311, 2198, 295, 309], "temperature": 0.0, "avg_logprob": -0.22825345327687818, "compression_ratio": 1.5515695067264574, "no_speech_prob": 4.63783590021194e-06}, {"id": 456, "seek": 220734, "start": 2222.82, "end": 2224.82, "text": " but I found like", "tokens": [457, 286, 1352, 411], "temperature": 0.0, "avg_logprob": -0.22825345327687818, "compression_ratio": 1.5515695067264574, "no_speech_prob": 4.63783590021194e-06}, {"id": 457, "seek": 220734, "start": 2225.1000000000004, "end": 2231.02, "text": " It's now like my superpower like using this along with the learning rate finder like I", "tokens": [467, 311, 586, 411, 452, 45765, 411, 1228, 341, 2051, 365, 264, 2539, 3314, 915, 260, 411, 286], "temperature": 0.0, "avg_logprob": -0.22825345327687818, "compression_ratio": 1.5515695067264574, "no_speech_prob": 4.63783590021194e-06}, {"id": 458, "seek": 220734, "start": 2232.02, "end": 2236.34, "text": " Can get better results than nearly anybody like in a Kaggle competition?", "tokens": [1664, 483, 1101, 3542, 813, 6217, 4472, 411, 294, 257, 48751, 22631, 6211, 30], "temperature": 0.0, "avg_logprob": -0.22825345327687818, "compression_ratio": 1.5515695067264574, "no_speech_prob": 4.63783590021194e-06}, {"id": 459, "seek": 223634, "start": 2236.34, "end": 2242.2400000000002, "text": " You know in the first week or two I can like jump in spend an hour or two and and back", "tokens": [509, 458, 294, 264, 700, 1243, 420, 732, 286, 393, 411, 3012, 294, 3496, 364, 1773, 420, 732, 293, 293, 646], "temperature": 0.0, "avg_logprob": -0.12925139013326392, "compression_ratio": 1.7325102880658436, "no_speech_prob": 9.080364179681055e-06}, {"id": 460, "seek": 223634, "start": 2242.38, "end": 2244.38, "text": " I've got a fantastically good result", "tokens": [286, 600, 658, 257, 4115, 22808, 665, 1874], "temperature": 0.0, "avg_logprob": -0.12925139013326392, "compression_ratio": 1.7325102880658436, "no_speech_prob": 9.080364179681055e-06}, {"id": 461, "seek": 223634, "start": 2244.7400000000002, "end": 2250.5, "text": " And so this is why I didn't pick the point where it's got the steepest slope", "tokens": [400, 370, 341, 307, 983, 286, 994, 380, 1888, 264, 935, 689, 309, 311, 658, 264, 16841, 377, 13525], "temperature": 0.0, "avg_logprob": -0.12925139013326392, "compression_ratio": 1.7325102880658436, "no_speech_prob": 9.080364179681055e-06}, {"id": 462, "seek": 223634, "start": 2250.5, "end": 2253.78, "text": " I actually tried to pick something kind of aggressively high", "tokens": [286, 767, 3031, 281, 1888, 746, 733, 295, 32024, 1090], "temperature": 0.0, "avg_logprob": -0.12925139013326392, "compression_ratio": 1.7325102880658436, "no_speech_prob": 9.080364179681055e-06}, {"id": 463, "seek": 223634, "start": 2253.78, "end": 2254.86, "text": " It's still getting down", "tokens": [467, 311, 920, 1242, 760], "temperature": 0.0, "avg_logprob": -0.12925139013326392, "compression_ratio": 1.7325102880658436, "no_speech_prob": 9.080364179681055e-06}, {"id": 464, "seek": 223634, "start": 2254.86, "end": 2260.94, "text": " But maybe like getting to the point where it's nearly too high and that's because I want to make because that's because when we do this", "tokens": [583, 1310, 411, 1242, 281, 264, 935, 689, 309, 311, 6217, 886, 1090, 293, 300, 311, 570, 286, 528, 281, 652, 570, 300, 311, 570, 562, 321, 360, 341], "temperature": 0.0, "avg_logprob": -0.12925139013326392, "compression_ratio": 1.7325102880658436, "no_speech_prob": 9.080364179681055e-06}, {"id": 465, "seek": 226094, "start": 2260.94, "end": 2266.04, "text": " Stochastic gradient descent with restarts this 10 to the negative 2", "tokens": [745, 8997, 2750, 16235, 23475, 365, 1472, 11814, 341, 1266, 281, 264, 3671, 568], "temperature": 0.0, "avg_logprob": -0.22519161701202392, "compression_ratio": 1.7055555555555555, "no_speech_prob": 5.0936337174789514e-06}, {"id": 466, "seek": 226094, "start": 2266.86, "end": 2268.78, "text": " represents the", "tokens": [8855, 264], "temperature": 0.0, "avg_logprob": -0.22519161701202392, "compression_ratio": 1.7055555555555555, "no_speech_prob": 5.0936337174789514e-06}, {"id": 467, "seek": 226094, "start": 2268.78, "end": 2274.58, "text": " Highest number that it uses so it goes up to 10 to the negative 2 and then goes down", "tokens": [5229, 377, 1230, 300, 309, 4960, 370, 309, 1709, 493, 281, 1266, 281, 264, 3671, 568, 293, 550, 1709, 760], "temperature": 0.0, "avg_logprob": -0.22519161701202392, "compression_ratio": 1.7055555555555555, "no_speech_prob": 5.0936337174789514e-06}, {"id": 468, "seek": 226094, "start": 2274.58, "end": 2279.48, "text": " And then up to 10 to negative 2 and then down so if I use to lower learning rate", "tokens": [400, 550, 493, 281, 1266, 281, 3671, 568, 293, 550, 760, 370, 498, 286, 764, 281, 3126, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.22519161701202392, "compression_ratio": 1.7055555555555555, "no_speech_prob": 5.0936337174789514e-06}, {"id": 469, "seek": 226094, "start": 2279.48, "end": 2283.5, "text": " It's not going to jump to a different part of the function", "tokens": [467, 311, 406, 516, 281, 3012, 281, 257, 819, 644, 295, 264, 2445], "temperature": 0.0, "avg_logprob": -0.22519161701202392, "compression_ratio": 1.7055555555555555, "no_speech_prob": 5.0936337174789514e-06}, {"id": 470, "seek": 228350, "start": 2283.5, "end": 2291.82, "text": " So I have a few questions, but the first one is how many times do you change the learning rate in one epoch?", "tokens": [407, 286, 362, 257, 1326, 1651, 11, 457, 264, 700, 472, 307, 577, 867, 1413, 360, 291, 1319, 264, 2539, 3314, 294, 472, 30992, 339, 30], "temperature": 0.0, "avg_logprob": -0.2189687676386002, "compression_ratio": 1.793991416309013, "no_speech_prob": 2.156803020625375e-06}, {"id": 471, "seek": 228350, "start": 2292.62, "end": 2295.94, "text": " We don't change the learning rate mode. Oh, sorry how many times do it?", "tokens": [492, 500, 380, 1319, 264, 2539, 3314, 4391, 13, 876, 11, 2597, 577, 867, 1413, 360, 309, 30], "temperature": 0.0, "avg_logprob": -0.2189687676386002, "compression_ratio": 1.793991416309013, "no_speech_prob": 2.156803020625375e-06}, {"id": 472, "seek": 228350, "start": 2295.94, "end": 2301.16, "text": " Okay, so in terms of this part here where it's going down we change the learning rate every single mini-match", "tokens": [1033, 11, 370, 294, 2115, 295, 341, 644, 510, 689, 309, 311, 516, 760, 321, 1319, 264, 2539, 3314, 633, 2167, 8382, 12, 76, 852], "temperature": 0.0, "avg_logprob": -0.2189687676386002, "compression_ratio": 1.793991416309013, "no_speech_prob": 2.156803020625375e-06}, {"id": 473, "seek": 228350, "start": 2301.58, "end": 2306.32, "text": " right and then the number of times we reset it is set by the", "tokens": [558, 293, 550, 264, 1230, 295, 1413, 321, 14322, 309, 307, 992, 538, 264], "temperature": 0.0, "avg_logprob": -0.2189687676386002, "compression_ratio": 1.793991416309013, "no_speech_prob": 2.156803020625375e-06}, {"id": 474, "seek": 228350, "start": 2307.2, "end": 2311.76, "text": " Cycle length parameter and so one means reset it after every epoch", "tokens": [10295, 2160, 4641, 13075, 293, 370, 472, 1355, 14322, 309, 934, 633, 30992, 339], "temperature": 0.0, "avg_logprob": -0.2189687676386002, "compression_ratio": 1.793991416309013, "no_speech_prob": 2.156803020625375e-06}, {"id": 475, "seek": 231176, "start": 2311.76, "end": 2316.4, "text": " so if I had two there it would reset it up to every two epochs and", "tokens": [370, 498, 286, 632, 732, 456, 309, 576, 14322, 309, 493, 281, 633, 732, 30992, 28346, 293], "temperature": 0.0, "avg_logprob": -0.25533320240138735, "compression_ratio": 1.728744939271255, "no_speech_prob": 2.7264520667813485e-06}, {"id": 476, "seek": 231176, "start": 2316.7200000000003, "end": 2323.6800000000003, "text": " Interestingly this this point that when we do the learning rate annealing that we actually change it every single batch it turns out to be", "tokens": [30564, 341, 341, 935, 300, 562, 321, 360, 264, 2539, 3314, 22256, 4270, 300, 321, 767, 1319, 309, 633, 2167, 15245, 309, 4523, 484, 281, 312], "temperature": 0.0, "avg_logprob": -0.25533320240138735, "compression_ratio": 1.728744939271255, "no_speech_prob": 2.7264520667813485e-06}, {"id": 477, "seek": 231176, "start": 2324.48, "end": 2328.0400000000004, "text": " Really critical to making this work and and it's again", "tokens": [4083, 4924, 281, 1455, 341, 589, 293, 293, 309, 311, 797], "temperature": 0.0, "avg_logprob": -0.25533320240138735, "compression_ratio": 1.728744939271255, "no_speech_prob": 2.7264520667813485e-06}, {"id": 478, "seek": 231176, "start": 2328.0400000000004, "end": 2331.76, "text": " It's very different to what nearly everybody in industry and academia has done before", "tokens": [467, 311, 588, 819, 281, 437, 6217, 2201, 294, 3518, 293, 28937, 575, 1096, 949], "temperature": 0.0, "avg_logprob": -0.25533320240138735, "compression_ratio": 1.728744939271255, "no_speech_prob": 2.7264520667813485e-06}, {"id": 479, "seek": 231176, "start": 2333.48, "end": 2339.76, "text": " When you any chance could you explain pre compute it was true because it's still", "tokens": [1133, 291, 604, 2931, 727, 291, 2903, 659, 14722, 309, 390, 2074, 570, 309, 311, 920], "temperature": 0.0, "avg_logprob": -0.25533320240138735, "compression_ratio": 1.728744939271255, "no_speech_prob": 2.7264520667813485e-06}, {"id": 480, "seek": 233976, "start": 2339.76, "end": 2344.5600000000004, "text": " Very confused. Yeah, we're going to come back to that multiple times in this course", "tokens": [4372, 9019, 13, 865, 11, 321, 434, 516, 281, 808, 646, 281, 300, 3866, 1413, 294, 341, 1164], "temperature": 0.0, "avg_logprob": -0.1837075288630714, "compression_ratio": 1.9583333333333333, "no_speech_prob": 3.844852471956983e-06}, {"id": 481, "seek": 233976, "start": 2344.5600000000004, "end": 2347.7200000000003, "text": " So the way this course is going to work is we're going to like do a really high level", "tokens": [407, 264, 636, 341, 1164, 307, 516, 281, 589, 307, 321, 434, 516, 281, 411, 360, 257, 534, 1090, 1496], "temperature": 0.0, "avg_logprob": -0.1837075288630714, "compression_ratio": 1.9583333333333333, "no_speech_prob": 3.844852471956983e-06}, {"id": 482, "seek": 233976, "start": 2348.0800000000004, "end": 2353.32, "text": " Version of each thing and then we're going to like come back to it in two or three lessons and then come back to it", "tokens": [35965, 295, 1184, 551, 293, 550, 321, 434, 516, 281, 411, 808, 646, 281, 309, 294, 732, 420, 1045, 8820, 293, 550, 808, 646, 281, 309], "temperature": 0.0, "avg_logprob": -0.1837075288630714, "compression_ratio": 1.9583333333333333, "no_speech_prob": 3.844852471956983e-06}, {"id": 483, "seek": 233976, "start": 2353.32, "end": 2358.5600000000004, "text": " At the end of the course and each time we're going to see like more of the math more of the code and get a deeper", "tokens": [1711, 264, 917, 295, 264, 1164, 293, 1184, 565, 321, 434, 516, 281, 536, 411, 544, 295, 264, 5221, 544, 295, 264, 3089, 293, 483, 257, 7731], "temperature": 0.0, "avg_logprob": -0.1837075288630714, "compression_ratio": 1.9583333333333333, "no_speech_prob": 3.844852471956983e-06}, {"id": 484, "seek": 233976, "start": 2358.5600000000004, "end": 2361.5600000000004, "text": " View okay, and we can talk about it also on the forums", "tokens": [13909, 1392, 11, 293, 321, 393, 751, 466, 309, 611, 322, 264, 26998], "temperature": 0.0, "avg_logprob": -0.1837075288630714, "compression_ratio": 1.9583333333333333, "no_speech_prob": 3.844852471956983e-06}, {"id": 485, "seek": 233976, "start": 2362.6400000000003, "end": 2364.6400000000003, "text": " during the week", "tokens": [1830, 264, 1243], "temperature": 0.0, "avg_logprob": -0.1837075288630714, "compression_ratio": 1.9583333333333333, "no_speech_prob": 3.844852471956983e-06}, {"id": 486, "seek": 236464, "start": 2364.64, "end": 2371.64, "text": " Our main goal is to generalize and we don't want to get those like narrow", "tokens": [2621, 2135, 3387, 307, 281, 2674, 1125, 293, 321, 500, 380, 528, 281, 483, 729, 411, 9432], "temperature": 0.0, "avg_logprob": -0.23588111788727517, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.3419341485132463e-05}, {"id": 487, "seek": 236464, "start": 2372.48, "end": 2380.0, "text": " Optimus yeah, that's a very good summary this method are we keeping track of the minima's and averaging them?", "tokens": [35013, 301, 1338, 11, 300, 311, 257, 588, 665, 12691, 341, 3170, 366, 321, 5145, 2837, 295, 264, 4464, 64, 311, 293, 47308, 552, 30], "temperature": 0.0, "avg_logprob": -0.23588111788727517, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.3419341485132463e-05}, {"id": 488, "seek": 236464, "start": 2380.92, "end": 2382.92, "text": " Something that's that's another", "tokens": [6595, 300, 311, 300, 311, 1071], "temperature": 0.0, "avg_logprob": -0.23588111788727517, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.3419341485132463e-05}, {"id": 489, "seek": 236464, "start": 2383.4, "end": 2391.0, "text": " Level of sophistication and indeed you can see there's something here called snapshot ensemble, so we're not doing it in the code right now", "tokens": [16872, 295, 15572, 399, 293, 6451, 291, 393, 536, 456, 311, 746, 510, 1219, 30163, 19492, 11, 370, 321, 434, 406, 884, 309, 294, 264, 3089, 558, 586], "temperature": 0.0, "avg_logprob": -0.23588111788727517, "compression_ratio": 1.5848214285714286, "no_speech_prob": 1.3419341485132463e-05}, {"id": 490, "seek": 239100, "start": 2391.0, "end": 2394.52, "text": " But yes, if you wanted to make this generalize even better", "tokens": [583, 2086, 11, 498, 291, 1415, 281, 652, 341, 2674, 1125, 754, 1101], "temperature": 0.0, "avg_logprob": -0.2269597053527832, "compression_ratio": 1.6723163841807909, "no_speech_prob": 2.406083694950212e-06}, {"id": 491, "seek": 239100, "start": 2394.88, "end": 2401.36, "text": " You can save the weights here and here and here and then take the average of the conditions", "tokens": [509, 393, 3155, 264, 17443, 510, 293, 510, 293, 510, 293, 550, 747, 264, 4274, 295, 264, 4487], "temperature": 0.0, "avg_logprob": -0.2269597053527832, "compression_ratio": 1.6723163841807909, "no_speech_prob": 2.406083694950212e-06}, {"id": 492, "seek": 239100, "start": 2401.84, "end": 2404.48, "text": " But for now, we're just going to pick the last one", "tokens": [583, 337, 586, 11, 321, 434, 445, 516, 281, 1888, 264, 1036, 472], "temperature": 0.0, "avg_logprob": -0.2269597053527832, "compression_ratio": 1.6723163841807909, "no_speech_prob": 2.406083694950212e-06}, {"id": 493, "seek": 239100, "start": 2409.72, "end": 2411.72, "text": " If you want to skip ahead", "tokens": [759, 291, 528, 281, 10023, 2286], "temperature": 0.0, "avg_logprob": -0.2269597053527832, "compression_ratio": 1.6723163841807909, "no_speech_prob": 2.406083694950212e-06}, {"id": 494, "seek": 239100, "start": 2414.08, "end": 2418.36, "text": " If you want to skip ahead there's a parameter called cycle save name", "tokens": [759, 291, 528, 281, 10023, 2286, 456, 311, 257, 13075, 1219, 6586, 3155, 1315], "temperature": 0.0, "avg_logprob": -0.2269597053527832, "compression_ratio": 1.6723163841807909, "no_speech_prob": 2.406083694950212e-06}, {"id": 495, "seek": 241836, "start": 2418.36, "end": 2424.28, "text": " Which you can add as well as cycle then and that will save a set of weights at the end of every", "tokens": [3013, 291, 393, 909, 382, 731, 382, 6586, 550, 293, 300, 486, 3155, 257, 992, 295, 17443, 412, 264, 917, 295, 633], "temperature": 0.0, "avg_logprob": -0.19186218675360622, "compression_ratio": 1.5404040404040404, "no_speech_prob": 4.495140728977276e-06}, {"id": 496, "seek": 241836, "start": 2425.08, "end": 2427.88, "text": " Learning rate cycle and then you can ensemble them", "tokens": [15205, 3314, 6586, 293, 550, 291, 393, 19492, 552], "temperature": 0.0, "avg_logprob": -0.19186218675360622, "compression_ratio": 1.5404040404040404, "no_speech_prob": 4.495140728977276e-06}, {"id": 497, "seek": 241836, "start": 2431.84, "end": 2433.84, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.19186218675360622, "compression_ratio": 1.5404040404040404, "no_speech_prob": 4.495140728977276e-06}, {"id": 498, "seek": 241836, "start": 2434.2000000000003, "end": 2436.2000000000003, "text": " So we've got a", "tokens": [407, 321, 600, 658, 257], "temperature": 0.0, "avg_logprob": -0.19186218675360622, "compression_ratio": 1.5404040404040404, "no_speech_prob": 4.495140728977276e-06}, {"id": 499, "seek": 241836, "start": 2436.4, "end": 2438.4, "text": " Pretty decent model here", "tokens": [10693, 8681, 2316, 510], "temperature": 0.0, "avg_logprob": -0.19186218675360622, "compression_ratio": 1.5404040404040404, "no_speech_prob": 4.495140728977276e-06}, {"id": 500, "seek": 241836, "start": 2438.7200000000003, "end": 2440.7200000000003, "text": " 99.3 percent accuracy", "tokens": [11803, 13, 18, 3043, 14170], "temperature": 0.0, "avg_logprob": -0.19186218675360622, "compression_ratio": 1.5404040404040404, "no_speech_prob": 4.495140728977276e-06}, {"id": 501, "seek": 241836, "start": 2440.84, "end": 2445.2400000000002, "text": " And we've gone through you know a few steps that have taken you know a minute or two to run", "tokens": [400, 321, 600, 2780, 807, 291, 458, 257, 1326, 4439, 300, 362, 2726, 291, 458, 257, 3456, 420, 732, 281, 1190], "temperature": 0.0, "avg_logprob": -0.19186218675360622, "compression_ratio": 1.5404040404040404, "no_speech_prob": 4.495140728977276e-06}, {"id": 502, "seek": 244524, "start": 2445.24, "end": 2448.4799999999996, "text": " And so from time to time I tend to save my weight", "tokens": [400, 370, 490, 565, 281, 565, 286, 3928, 281, 3155, 452, 3364], "temperature": 0.0, "avg_logprob": -0.1095383878339801, "compression_ratio": 1.825531914893617, "no_speech_prob": 3.187544280081056e-06}, {"id": 503, "seek": 244524, "start": 2448.4799999999996, "end": 2451.52, "text": " So if you go learn dot save and then pass in a file name", "tokens": [407, 498, 291, 352, 1466, 5893, 3155, 293, 550, 1320, 294, 257, 3991, 1315], "temperature": 0.0, "avg_logprob": -0.1095383878339801, "compression_ratio": 1.825531914893617, "no_speech_prob": 3.187544280081056e-06}, {"id": 504, "seek": 244524, "start": 2451.8799999999997, "end": 2456.2, "text": " It's going to go ahead and save that for you later on if you go learn dot load", "tokens": [467, 311, 516, 281, 352, 2286, 293, 3155, 300, 337, 291, 1780, 322, 498, 291, 352, 1466, 5893, 3677], "temperature": 0.0, "avg_logprob": -0.1095383878339801, "compression_ratio": 1.825531914893617, "no_speech_prob": 3.187544280081056e-06}, {"id": 505, "seek": 244524, "start": 2456.64, "end": 2461.8199999999997, "text": " You'll be straight back to where you came from okay, so it's good idea to do that from time to time", "tokens": [509, 603, 312, 2997, 646, 281, 689, 291, 1361, 490, 1392, 11, 370, 309, 311, 665, 1558, 281, 360, 300, 490, 565, 281, 565], "temperature": 0.0, "avg_logprob": -0.1095383878339801, "compression_ratio": 1.825531914893617, "no_speech_prob": 3.187544280081056e-06}, {"id": 506, "seek": 244524, "start": 2462.68, "end": 2464.68, "text": " This is a good time to mention", "tokens": [639, 307, 257, 665, 565, 281, 2152], "temperature": 0.0, "avg_logprob": -0.1095383878339801, "compression_ratio": 1.825531914893617, "no_speech_prob": 3.187544280081056e-06}, {"id": 507, "seek": 244524, "start": 2465.3599999999997, "end": 2467.3599999999997, "text": " What happens when you do this?", "tokens": [708, 2314, 562, 291, 360, 341, 30], "temperature": 0.0, "avg_logprob": -0.1095383878339801, "compression_ratio": 1.825531914893617, "no_speech_prob": 3.187544280081056e-06}, {"id": 508, "seek": 244524, "start": 2468.08, "end": 2473.16, "text": " When you go learn dot save when you create pre computed activations another thing", "tokens": [1133, 291, 352, 1466, 5893, 3155, 562, 291, 1884, 659, 40610, 2430, 763, 1071, 551], "temperature": 0.0, "avg_logprob": -0.1095383878339801, "compression_ratio": 1.825531914893617, "no_speech_prob": 3.187544280081056e-06}, {"id": 509, "seek": 247316, "start": 2473.16, "end": 2479.3199999999997, "text": " We'll learn about soon when you create resized images these are all creating various temporary files, okay?", "tokens": [492, 603, 1466, 466, 2321, 562, 291, 1884, 725, 1602, 5267, 613, 366, 439, 4084, 3683, 13413, 7098, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.2775320549533792, "compression_ratio": 1.4659090909090908, "no_speech_prob": 5.626395136459905e-07}, {"id": 510, "seek": 247316, "start": 2479.8399999999997, "end": 2482.08, "text": " and so what happens is", "tokens": [293, 370, 437, 2314, 307], "temperature": 0.0, "avg_logprob": -0.2775320549533792, "compression_ratio": 1.4659090909090908, "no_speech_prob": 5.626395136459905e-07}, {"id": 511, "seek": 247316, "start": 2484.64, "end": 2486.64, "text": " If we go to", "tokens": [759, 321, 352, 281], "temperature": 0.0, "avg_logprob": -0.2775320549533792, "compression_ratio": 1.4659090909090908, "no_speech_prob": 5.626395136459905e-07}, {"id": 512, "seek": 247316, "start": 2487.56, "end": 2489.56, "text": " data", "tokens": [1412], "temperature": 0.0, "avg_logprob": -0.2775320549533792, "compression_ratio": 1.4659090909090908, "no_speech_prob": 5.626395136459905e-07}, {"id": 513, "seek": 247316, "start": 2489.6, "end": 2491.6, "text": " and we go to", "tokens": [293, 321, 352, 281], "temperature": 0.0, "avg_logprob": -0.2775320549533792, "compression_ratio": 1.4659090909090908, "no_speech_prob": 5.626395136459905e-07}, {"id": 514, "seek": 247316, "start": 2492.56, "end": 2499.2799999999997, "text": " Dogs cats this is my data folder, and you'll see there's a folder here called TMP or the tomb and", "tokens": [35504, 11111, 341, 307, 452, 1412, 10820, 11, 293, 291, 603, 536, 456, 311, 257, 10820, 510, 1219, 314, 12224, 420, 264, 18712, 293], "temperature": 0.0, "avg_logprob": -0.2775320549533792, "compression_ratio": 1.4659090909090908, "no_speech_prob": 5.626395136459905e-07}, {"id": 515, "seek": 249928, "start": 2499.28, "end": 2505.76, "text": " And so this is automatically created and all of my pre computed activations end up in here", "tokens": [400, 370, 341, 307, 6772, 2942, 293, 439, 295, 452, 659, 40610, 2430, 763, 917, 493, 294, 510], "temperature": 0.0, "avg_logprob": -0.19221981272977942, "compression_ratio": 1.7289719626168225, "no_speech_prob": 2.4439791559416335e-06}, {"id": 516, "seek": 249928, "start": 2505.76, "end": 2511.52, "text": " I mentioned this because if if things aren't if you're getting weird errors", "tokens": [286, 2835, 341, 570, 498, 498, 721, 3212, 380, 498, 291, 434, 1242, 3657, 13603], "temperature": 0.0, "avg_logprob": -0.19221981272977942, "compression_ratio": 1.7289719626168225, "no_speech_prob": 2.4439791559416335e-06}, {"id": 517, "seek": 249928, "start": 2511.8, "end": 2517.36, "text": " it might be because you've got some like pre computed activations that like were only half completed or", "tokens": [309, 1062, 312, 570, 291, 600, 658, 512, 411, 659, 40610, 2430, 763, 300, 411, 645, 787, 1922, 7365, 420], "temperature": 0.0, "avg_logprob": -0.19221981272977942, "compression_ratio": 1.7289719626168225, "no_speech_prob": 2.4439791559416335e-06}, {"id": 518, "seek": 249928, "start": 2518.1200000000003, "end": 2523.7200000000003, "text": " Are in some way incompatible with what you're doing so you can always go ahead and just delete this", "tokens": [2014, 294, 512, 636, 40393, 267, 964, 365, 437, 291, 434, 884, 370, 291, 393, 1009, 352, 2286, 293, 445, 12097, 341], "temperature": 0.0, "avg_logprob": -0.19221981272977942, "compression_ratio": 1.7289719626168225, "no_speech_prob": 2.4439791559416335e-06}, {"id": 519, "seek": 252372, "start": 2523.72, "end": 2532.0, "text": " TMP this temporary directory and see if that causes your error to go away. This is the fast AI equivalent of turning it off and then on again", "tokens": [314, 12224, 341, 13413, 21120, 293, 536, 498, 300, 7700, 428, 6713, 281, 352, 1314, 13, 639, 307, 264, 2370, 7318, 10344, 295, 6246, 309, 766, 293, 550, 322, 797], "temperature": 0.0, "avg_logprob": -0.152228131501571, "compression_ratio": 1.7553956834532374, "no_speech_prob": 5.771876658400288e-06}, {"id": 520, "seek": 252372, "start": 2533.2, "end": 2539.16, "text": " You'll also see there's a directory called models, and that's where all of these when you say dot save with a model", "tokens": [509, 603, 611, 536, 456, 311, 257, 21120, 1219, 5245, 11, 293, 300, 311, 689, 439, 295, 613, 562, 291, 584, 5893, 3155, 365, 257, 2316], "temperature": 0.0, "avg_logprob": -0.152228131501571, "compression_ratio": 1.7553956834532374, "no_speech_prob": 5.771876658400288e-06}, {"id": 521, "seek": 252372, "start": 2539.16, "end": 2541.16, "text": " That's where that's going to go", "tokens": [663, 311, 689, 300, 311, 516, 281, 352], "temperature": 0.0, "avg_logprob": -0.152228131501571, "compression_ratio": 1.7553956834532374, "no_speech_prob": 5.771876658400288e-06}, {"id": 522, "seek": 252372, "start": 2542.04, "end": 2546.08, "text": " Actually it reminds me when this cast a gradient descent with restarts paper came out", "tokens": [5135, 309, 12025, 385, 562, 341, 4193, 257, 16235, 23475, 365, 1472, 11814, 3035, 1361, 484], "temperature": 0.0, "avg_logprob": -0.152228131501571, "compression_ratio": 1.7553956834532374, "no_speech_prob": 5.771876658400288e-06}, {"id": 523, "seek": 254608, "start": 2546.08, "end": 2553.84, "text": " I saw a tweet that was somebody who was like oh to make your deep learning work better turn it off and then on again", "tokens": [286, 1866, 257, 15258, 300, 390, 2618, 567, 390, 411, 1954, 281, 652, 428, 2452, 2539, 589, 1101, 1261, 309, 766, 293, 550, 322, 797], "temperature": 0.0, "avg_logprob": -0.2903742381504604, "compression_ratio": 1.656441717791411, "no_speech_prob": 5.771883479610551e-06}, {"id": 524, "seek": 254608, "start": 2556.36, "end": 2561.84, "text": " So if I want to see I want to retrain my model from squash again do I just delete everything?", "tokens": [407, 498, 286, 528, 281, 536, 286, 528, 281, 1533, 7146, 452, 2316, 490, 30725, 797, 360, 286, 445, 12097, 1203, 30], "temperature": 0.0, "avg_logprob": -0.2903742381504604, "compression_ratio": 1.656441717791411, "no_speech_prob": 5.771883479610551e-06}, {"id": 525, "seek": 254608, "start": 2562.92, "end": 2564.92, "text": " if you want to", "tokens": [498, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.2903742381504604, "compression_ratio": 1.656441717791411, "no_speech_prob": 5.771883479610551e-06}, {"id": 526, "seek": 254608, "start": 2569.68, "end": 2571.68, "text": " If you want to train your model from scratch", "tokens": [759, 291, 528, 281, 3847, 428, 2316, 490, 8459], "temperature": 0.0, "avg_logprob": -0.2903742381504604, "compression_ratio": 1.656441717791411, "no_speech_prob": 5.771883479610551e-06}, {"id": 527, "seek": 257168, "start": 2571.68, "end": 2579.04, "text": " There's generally no reason to delete the pre computed activations because the pre computed activations are", "tokens": [821, 311, 5101, 572, 1778, 281, 12097, 264, 659, 40610, 2430, 763, 570, 264, 659, 40610, 2430, 763, 366], "temperature": 0.0, "avg_logprob": -0.22020327660345262, "compression_ratio": 1.9220183486238531, "no_speech_prob": 6.962191946513485e-06}, {"id": 528, "seek": 257168, "start": 2579.8799999999997, "end": 2583.3199999999997, "text": " Without any training. That's what the pre trained model", "tokens": [9129, 604, 3097, 13, 663, 311, 437, 264, 659, 8895, 2316], "temperature": 0.0, "avg_logprob": -0.22020327660345262, "compression_ratio": 1.9220183486238531, "no_speech_prob": 6.962191946513485e-06}, {"id": 529, "seek": 257168, "start": 2584.56, "end": 2588.08, "text": " Created with the with the weights that you downloaded off the internet", "tokens": [11972, 292, 365, 264, 365, 264, 17443, 300, 291, 21748, 766, 264, 4705], "temperature": 0.0, "avg_logprob": -0.22020327660345262, "compression_ratio": 1.9220183486238531, "no_speech_prob": 6.962191946513485e-06}, {"id": 530, "seek": 257168, "start": 2589.16, "end": 2591.16, "text": " the only", "tokens": [264, 787], "temperature": 0.0, "avg_logprob": -0.22020327660345262, "compression_ratio": 1.9220183486238531, "no_speech_prob": 6.962191946513485e-06}, {"id": 531, "seek": 257168, "start": 2591.3199999999997, "end": 2597.44, "text": " Yeah, I mean the only reason you want to delete the pre computed activations if there was some error caused by like half", "tokens": [865, 11, 286, 914, 264, 787, 1778, 291, 528, 281, 12097, 264, 659, 40610, 2430, 763, 498, 456, 390, 512, 6713, 7008, 538, 411, 1922], "temperature": 0.0, "avg_logprob": -0.22020327660345262, "compression_ratio": 1.9220183486238531, "no_speech_prob": 6.962191946513485e-06}, {"id": 532, "seek": 257168, "start": 2597.9199999999996, "end": 2600.8599999999997, "text": " Creating them and crashing or some something like that", "tokens": [40002, 552, 293, 26900, 420, 512, 746, 411, 300], "temperature": 0.0, "avg_logprob": -0.22020327660345262, "compression_ratio": 1.9220183486238531, "no_speech_prob": 6.962191946513485e-06}, {"id": 533, "seek": 260086, "start": 2600.86, "end": 2608.78, "text": " As you change the size of your input change different architectures and so forth they all create different sets of activations with different file names", "tokens": [1018, 291, 1319, 264, 2744, 295, 428, 4846, 1319, 819, 6331, 1303, 293, 370, 5220, 436, 439, 1884, 819, 6352, 295, 2430, 763, 365, 819, 3991, 5288], "temperature": 0.0, "avg_logprob": -0.19316675225082708, "compression_ratio": 1.7661290322580645, "no_speech_prob": 5.093650088383583e-06}, {"id": 534, "seek": 260086, "start": 2608.78, "end": 2611.3, "text": " So you don't generally you shouldn't have to worry about it", "tokens": [407, 291, 500, 380, 5101, 291, 4659, 380, 362, 281, 3292, 466, 309], "temperature": 0.0, "avg_logprob": -0.19316675225082708, "compression_ratio": 1.7661290322580645, "no_speech_prob": 5.093650088383583e-06}, {"id": 535, "seek": 260086, "start": 2611.3, "end": 2616.84, "text": " And if you want to start training again from scratch all you have to do is create a new", "tokens": [400, 498, 291, 528, 281, 722, 3097, 797, 490, 8459, 439, 291, 362, 281, 360, 307, 1884, 257, 777], "temperature": 0.0, "avg_logprob": -0.19316675225082708, "compression_ratio": 1.7661290322580645, "no_speech_prob": 5.093650088383583e-06}, {"id": 536, "seek": 260086, "start": 2617.98, "end": 2623.1800000000003, "text": " Learn object so each time you go like con learner dot pre trained that creates a new", "tokens": [17216, 2657, 370, 1184, 565, 291, 352, 411, 416, 33347, 5893, 659, 8895, 300, 7829, 257, 777], "temperature": 0.0, "avg_logprob": -0.19316675225082708, "compression_ratio": 1.7661290322580645, "no_speech_prob": 5.093650088383583e-06}, {"id": 537, "seek": 260086, "start": 2623.82, "end": 2626.7000000000003, "text": " Object with with new sets of weights be trained from", "tokens": [24753, 365, 365, 777, 6352, 295, 17443, 312, 8895, 490], "temperature": 0.0, "avg_logprob": -0.19316675225082708, "compression_ratio": 1.7661290322580645, "no_speech_prob": 5.093650088383583e-06}, {"id": 538, "seek": 262670, "start": 2626.7, "end": 2628.7, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.2496425041785607, "compression_ratio": 1.5085714285714287, "no_speech_prob": 3.041565832972992e-06}, {"id": 539, "seek": 262670, "start": 2630.46, "end": 2633.46, "text": " So before our break we'll finish off by talking about", "tokens": [407, 949, 527, 1821, 321, 603, 2413, 766, 538, 1417, 466], "temperature": 0.0, "avg_logprob": -0.2496425041785607, "compression_ratio": 1.5085714285714287, "no_speech_prob": 3.041565832972992e-06}, {"id": 540, "seek": 262670, "start": 2634.54, "end": 2638.24, "text": " About fine tuning and differential learning rates", "tokens": [7769, 2489, 15164, 293, 15756, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.2496425041785607, "compression_ratio": 1.5085714285714287, "no_speech_prob": 3.041565832972992e-06}, {"id": 541, "seek": 262670, "start": 2639.06, "end": 2641.06, "text": " and so so far", "tokens": [293, 370, 370, 1400], "temperature": 0.0, "avg_logprob": -0.2496425041785607, "compression_ratio": 1.5085714285714287, "no_speech_prob": 3.041565832972992e-06}, {"id": 542, "seek": 262670, "start": 2643.14, "end": 2644.8999999999996, "text": " Everything we've done", "tokens": [5471, 321, 600, 1096], "temperature": 0.0, "avg_logprob": -0.2496425041785607, "compression_ratio": 1.5085714285714287, "no_speech_prob": 3.041565832972992e-06}, {"id": 543, "seek": 262670, "start": 2644.8999999999996, "end": 2652.52, "text": " Has not changed any of these pre trained filters right so we've used a pre trained model that already knows how to find", "tokens": [8646, 406, 3105, 604, 295, 613, 659, 8895, 15995, 558, 370, 321, 600, 1143, 257, 659, 8895, 2316, 300, 1217, 3255, 577, 281, 915], "temperature": 0.0, "avg_logprob": -0.2496425041785607, "compression_ratio": 1.5085714285714287, "no_speech_prob": 3.041565832972992e-06}, {"id": 544, "seek": 265252, "start": 2652.52, "end": 2658.2, "text": " At the early stages edges and gradients", "tokens": [1711, 264, 2440, 10232, 8819, 293, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.32873754681281325, "compression_ratio": 1.5514705882352942, "no_speech_prob": 5.255333690001862e-06}, {"id": 545, "seek": 265252, "start": 2658.92, "end": 2660.92, "text": " And then corners and curves", "tokens": [400, 550, 12413, 293, 19490], "temperature": 0.0, "avg_logprob": -0.32873754681281325, "compression_ratio": 1.5514705882352942, "no_speech_prob": 5.255333690001862e-06}, {"id": 546, "seek": 265252, "start": 2661.92, "end": 2663.92, "text": " And then repeating patterns", "tokens": [400, 550, 18617, 8294], "temperature": 0.0, "avg_logprob": -0.32873754681281325, "compression_ratio": 1.5514705882352942, "no_speech_prob": 5.255333690001862e-06}, {"id": 547, "seek": 265252, "start": 2665.0, "end": 2667.0, "text": " and bits of text and", "tokens": [293, 9239, 295, 2487, 293], "temperature": 0.0, "avg_logprob": -0.32873754681281325, "compression_ratio": 1.5514705882352942, "no_speech_prob": 5.255333690001862e-06}, {"id": 548, "seek": 265252, "start": 2667.7599999999998, "end": 2670.8, "text": " eventually eyeballs right we have not", "tokens": [4728, 43758, 558, 321, 362, 406], "temperature": 0.0, "avg_logprob": -0.32873754681281325, "compression_ratio": 1.5514705882352942, "no_speech_prob": 5.255333690001862e-06}, {"id": 549, "seek": 265252, "start": 2673.04, "end": 2676.84, "text": " Retrained any of those activations any of those features", "tokens": [11495, 31774, 604, 295, 729, 2430, 763, 604, 295, 729, 4122], "temperature": 0.0, "avg_logprob": -0.32873754681281325, "compression_ratio": 1.5514705882352942, "no_speech_prob": 5.255333690001862e-06}, {"id": 550, "seek": 267684, "start": 2676.84, "end": 2683.76, "text": " Or more specifically any of those weights in the convolutional kernels all we've done is we've learned", "tokens": [1610, 544, 4682, 604, 295, 729, 17443, 294, 264, 45216, 304, 23434, 1625, 439, 321, 600, 1096, 307, 321, 600, 3264], "temperature": 0.0, "avg_logprob": -0.17178706235663835, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.5294078821170842e-06}, {"id": 551, "seek": 267684, "start": 2684.4, "end": 2691.32, "text": " Some new layers that we've added on top of these things we've learned how to mix and match these pre trained features", "tokens": [2188, 777, 7914, 300, 321, 600, 3869, 322, 1192, 295, 613, 721, 321, 600, 3264, 577, 281, 2890, 293, 2995, 613, 659, 8895, 4122], "temperature": 0.0, "avg_logprob": -0.17178706235663835, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.5294078821170842e-06}, {"id": 552, "seek": 267684, "start": 2692.56, "end": 2695.6000000000004, "text": " now obviously it may turn out that", "tokens": [586, 2745, 309, 815, 1261, 484, 300], "temperature": 0.0, "avg_logprob": -0.17178706235663835, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.5294078821170842e-06}, {"id": 553, "seek": 267684, "start": 2696.44, "end": 2704.92, "text": " Your pictures have you know different kinds of eyeballs or faces or if you're using different kinds of images like satellite images?", "tokens": [2260, 5242, 362, 291, 458, 819, 3685, 295, 43758, 420, 8475, 420, 498, 291, 434, 1228, 819, 3685, 295, 5267, 411, 16016, 5267, 30], "temperature": 0.0, "avg_logprob": -0.17178706235663835, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.5294078821170842e-06}, {"id": 554, "seek": 270492, "start": 2704.92, "end": 2711.44, "text": " Totally different kinds of features all together right so if you're like training to recognize icebergs", "tokens": [22837, 819, 3685, 295, 4122, 439, 1214, 558, 370, 498, 291, 434, 411, 3097, 281, 5521, 38880, 82], "temperature": 0.0, "avg_logprob": -0.18622527519861856, "compression_ratio": 1.6706827309236947, "no_speech_prob": 1.2482657893997384e-06}, {"id": 555, "seek": 270492, "start": 2711.76, "end": 2716.92, "text": " You're probably want to go all the way back and learn you know all the way back to kind of different", "tokens": [509, 434, 1391, 528, 281, 352, 439, 264, 636, 646, 293, 1466, 291, 458, 439, 264, 636, 646, 281, 733, 295, 819], "temperature": 0.0, "avg_logprob": -0.18622527519861856, "compression_ratio": 1.6706827309236947, "no_speech_prob": 1.2482657893997384e-06}, {"id": 556, "seek": 270492, "start": 2717.28, "end": 2719.6, "text": " combinations of these simple gradients and edges", "tokens": [21267, 295, 613, 2199, 2771, 2448, 293, 8819], "temperature": 0.0, "avg_logprob": -0.18622527519861856, "compression_ratio": 1.6706827309236947, "no_speech_prob": 1.2482657893997384e-06}, {"id": 557, "seek": 270492, "start": 2721.04, "end": 2723.04, "text": " In our case is dogs versus cats", "tokens": [682, 527, 1389, 307, 7197, 5717, 11111], "temperature": 0.0, "avg_logprob": -0.18622527519861856, "compression_ratio": 1.6706827309236947, "no_speech_prob": 1.2482657893997384e-06}, {"id": 558, "seek": 270492, "start": 2723.6, "end": 2728.92, "text": " We're going to have some minor differences, but we still may find it's helpful to slightly tune some of these", "tokens": [492, 434, 516, 281, 362, 512, 6696, 7300, 11, 457, 321, 920, 815, 915, 309, 311, 4961, 281, 4748, 10864, 512, 295, 613], "temperature": 0.0, "avg_logprob": -0.18622527519861856, "compression_ratio": 1.6706827309236947, "no_speech_prob": 1.2482657893997384e-06}, {"id": 559, "seek": 270492, "start": 2730.2400000000002, "end": 2732.2000000000003, "text": " later layers as well", "tokens": [1780, 7914, 382, 731], "temperature": 0.0, "avg_logprob": -0.18622527519861856, "compression_ratio": 1.6706827309236947, "no_speech_prob": 1.2482657893997384e-06}, {"id": 560, "seek": 273220, "start": 2732.2, "end": 2736.2, "text": " So to tell the learner that we now want to start", "tokens": [407, 281, 980, 264, 33347, 300, 321, 586, 528, 281, 722], "temperature": 0.0, "avg_logprob": -0.22261173167127243, "compression_ratio": 1.6133333333333333, "no_speech_prob": 6.179380420689995e-07}, {"id": 561, "seek": 273220, "start": 2736.8799999999997, "end": 2739.68, "text": " Actually changing the convolutional filters themselves", "tokens": [5135, 4473, 264, 45216, 304, 15995, 2969], "temperature": 0.0, "avg_logprob": -0.22261173167127243, "compression_ratio": 1.6133333333333333, "no_speech_prob": 6.179380420689995e-07}, {"id": 562, "seek": 273220, "start": 2740.3199999999997, "end": 2746.9199999999996, "text": " We simply say unfreeze okay, so a frozen layer is a layer which is not trained which is not updated", "tokens": [492, 2935, 584, 3971, 701, 1381, 1392, 11, 370, 257, 12496, 4583, 307, 257, 4583, 597, 307, 406, 8895, 597, 307, 406, 10588], "temperature": 0.0, "avg_logprob": -0.22261173167127243, "compression_ratio": 1.6133333333333333, "no_speech_prob": 6.179380420689995e-07}, {"id": 563, "seek": 273220, "start": 2747.04, "end": 2750.04, "text": " Okay, so unfreeze unfreezes all of the layers", "tokens": [1033, 11, 370, 3971, 701, 1381, 3971, 701, 12214, 439, 295, 264, 7914], "temperature": 0.0, "avg_logprob": -0.22261173167127243, "compression_ratio": 1.6133333333333333, "no_speech_prob": 6.179380420689995e-07}, {"id": 564, "seek": 273220, "start": 2750.8799999999997, "end": 2753.52, "text": " now when you think about it, it's pretty obvious that", "tokens": [586, 562, 291, 519, 466, 309, 11, 309, 311, 1238, 6322, 300], "temperature": 0.0, "avg_logprob": -0.22261173167127243, "compression_ratio": 1.6133333333333333, "no_speech_prob": 6.179380420689995e-07}, {"id": 565, "seek": 273220, "start": 2755.48, "end": 2757.3999999999996, "text": " Layer one", "tokens": [35166, 472], "temperature": 0.0, "avg_logprob": -0.22261173167127243, "compression_ratio": 1.6133333333333333, "no_speech_prob": 6.179380420689995e-07}, {"id": 566, "seek": 273220, "start": 2757.3999999999996, "end": 2760.3599999999997, "text": " Right which is like a diagonal edge or a gradient", "tokens": [1779, 597, 307, 411, 257, 21539, 4691, 420, 257, 16235], "temperature": 0.0, "avg_logprob": -0.22261173167127243, "compression_ratio": 1.6133333333333333, "no_speech_prob": 6.179380420689995e-07}, {"id": 567, "seek": 276036, "start": 2760.36, "end": 2766.6800000000003, "text": " Probably doesn't need to change by much if at all right from the one and a half million images on image net", "tokens": [9210, 1177, 380, 643, 281, 1319, 538, 709, 498, 412, 439, 558, 490, 264, 472, 293, 257, 1922, 2459, 5267, 322, 3256, 2533], "temperature": 0.0, "avg_logprob": -0.1634302572770552, "compression_ratio": 1.7545454545454546, "no_speech_prob": 8.714329737813387e-07}, {"id": 568, "seek": 276036, "start": 2766.6800000000003, "end": 2770.8, "text": " It probably already is figured out pretty well how to find like edges and gradients", "tokens": [467, 1391, 1217, 307, 8932, 484, 1238, 731, 577, 281, 915, 411, 8819, 293, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.1634302572770552, "compression_ratio": 1.7545454545454546, "no_speech_prob": 8.714329737813387e-07}, {"id": 569, "seek": 276036, "start": 2770.92, "end": 2777.6400000000003, "text": " It probably already knows also like which kind of corners to look for and how to find which kinds of curves and so forth", "tokens": [467, 1391, 1217, 3255, 611, 411, 597, 733, 295, 12413, 281, 574, 337, 293, 577, 281, 915, 597, 3685, 295, 19490, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.1634302572770552, "compression_ratio": 1.7545454545454546, "no_speech_prob": 8.714329737813387e-07}, {"id": 570, "seek": 276036, "start": 2777.6400000000003, "end": 2782.2000000000003, "text": " So in other words these early layers probably need little if any", "tokens": [407, 294, 661, 2283, 613, 2440, 7914, 1391, 643, 707, 498, 604], "temperature": 0.0, "avg_logprob": -0.1634302572770552, "compression_ratio": 1.7545454545454546, "no_speech_prob": 8.714329737813387e-07}, {"id": 571, "seek": 276036, "start": 2782.96, "end": 2784.52, "text": " learning", "tokens": [2539], "temperature": 0.0, "avg_logprob": -0.1634302572770552, "compression_ratio": 1.7545454545454546, "no_speech_prob": 8.714329737813387e-07}, {"id": 572, "seek": 278452, "start": 2784.52, "end": 2791.6, "text": " Where else these later ones are much more likely to need more learning and this is universally true regardless of whether you're looking for", "tokens": [2305, 1646, 613, 1780, 2306, 366, 709, 544, 3700, 281, 643, 544, 2539, 293, 341, 307, 43995, 2074, 10060, 295, 1968, 291, 434, 1237, 337], "temperature": 0.0, "avg_logprob": -0.14469209247165257, "compression_ratio": 1.7412280701754386, "no_speech_prob": 7.690370011914638e-07}, {"id": 573, "seek": 278452, "start": 2792.36, "end": 2797.16, "text": " Satellite images of rainforests or icebergs or whether you're looking for cats versus dogs right?", "tokens": [318, 10810, 642, 5267, 295, 48531, 82, 420, 38880, 82, 420, 1968, 291, 434, 1237, 337, 11111, 5717, 7197, 558, 30], "temperature": 0.0, "avg_logprob": -0.14469209247165257, "compression_ratio": 1.7412280701754386, "no_speech_prob": 7.690370011914638e-07}, {"id": 574, "seek": 278452, "start": 2798.92, "end": 2802.8, "text": " So what we do is we create an array of learning rates", "tokens": [407, 437, 321, 360, 307, 321, 1884, 364, 10225, 295, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.14469209247165257, "compression_ratio": 1.7412280701754386, "no_speech_prob": 7.690370011914638e-07}, {"id": 575, "seek": 278452, "start": 2803.68, "end": 2810.64, "text": " Where we say okay? These are the learning rates to use for our additional layers that we've added on top", "tokens": [2305, 321, 584, 1392, 30, 1981, 366, 264, 2539, 6846, 281, 764, 337, 527, 4497, 7914, 300, 321, 600, 3869, 322, 1192], "temperature": 0.0, "avg_logprob": -0.14469209247165257, "compression_ratio": 1.7412280701754386, "no_speech_prob": 7.690370011914638e-07}, {"id": 576, "seek": 281064, "start": 2810.64, "end": 2815.6, "text": " these are the learning rates to use in the middle few layers and", "tokens": [613, 366, 264, 2539, 6846, 281, 764, 294, 264, 2808, 1326, 7914, 293], "temperature": 0.0, "avg_logprob": -0.2480506163377028, "compression_ratio": 2.044642857142857, "no_speech_prob": 1.5779560271766968e-06}, {"id": 577, "seek": 281064, "start": 2815.96, "end": 2819.3199999999997, "text": " These are the learning rates to use for the first few layers", "tokens": [1981, 366, 264, 2539, 6846, 281, 764, 337, 264, 700, 1326, 7914], "temperature": 0.0, "avg_logprob": -0.2480506163377028, "compression_ratio": 2.044642857142857, "no_speech_prob": 1.5779560271766968e-06}, {"id": 578, "seek": 281064, "start": 2819.3199999999997, "end": 2824.2799999999997, "text": " So these are the ones for the lights that represent like very basic geometric features", "tokens": [407, 613, 366, 264, 2306, 337, 264, 5811, 300, 2906, 411, 588, 3875, 33246, 4122], "temperature": 0.0, "avg_logprob": -0.2480506163377028, "compression_ratio": 2.044642857142857, "no_speech_prob": 1.5779560271766968e-06}, {"id": 579, "seek": 281064, "start": 2824.64, "end": 2828.7599999999998, "text": " These are the ones that are used to for the more complex", "tokens": [1981, 366, 264, 2306, 300, 366, 1143, 281, 337, 264, 544, 3997], "temperature": 0.0, "avg_logprob": -0.2480506163377028, "compression_ratio": 2.044642857142857, "no_speech_prob": 1.5779560271766968e-06}, {"id": 580, "seek": 281064, "start": 2829.16, "end": 2831.04, "text": " Kind of sophisticated", "tokens": [9242, 295, 16950], "temperature": 0.0, "avg_logprob": -0.2480506163377028, "compression_ratio": 2.044642857142857, "no_speech_prob": 1.5779560271766968e-06}, {"id": 581, "seek": 281064, "start": 2831.04, "end": 2836.04, "text": " Compolitional features and these are the ones that are used for the features that we've added and learn from scratch", "tokens": [6620, 401, 2628, 4122, 293, 613, 366, 264, 2306, 300, 366, 1143, 337, 264, 4122, 300, 321, 600, 3869, 293, 1466, 490, 8459], "temperature": 0.0, "avg_logprob": -0.2480506163377028, "compression_ratio": 2.044642857142857, "no_speech_prob": 1.5779560271766968e-06}, {"id": 582, "seek": 281064, "start": 2836.3599999999997, "end": 2839.04, "text": " Right so we can create an array of learning rates", "tokens": [1779, 370, 321, 393, 1884, 364, 10225, 295, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.2480506163377028, "compression_ratio": 2.044642857142857, "no_speech_prob": 1.5779560271766968e-06}, {"id": 583, "seek": 283904, "start": 2839.04, "end": 2840.44, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.20094444354375204, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.237751570850378e-06}, {"id": 584, "seek": 283904, "start": 2840.44, "end": 2843.6, "text": " Then when we call dot fit and pass in an array of learning rates", "tokens": [1396, 562, 321, 818, 5893, 3318, 293, 1320, 294, 364, 10225, 295, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.20094444354375204, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.237751570850378e-06}, {"id": 585, "seek": 283904, "start": 2843.6, "end": 2848.4, "text": " It's now going to use those different learning rates for different parts of the model", "tokens": [467, 311, 586, 516, 281, 764, 729, 819, 2539, 6846, 337, 819, 3166, 295, 264, 2316], "temperature": 0.0, "avg_logprob": -0.20094444354375204, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.237751570850378e-06}, {"id": 586, "seek": 283904, "start": 2849.64, "end": 2850.96, "text": " this is", "tokens": [341, 307], "temperature": 0.0, "avg_logprob": -0.20094444354375204, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.237751570850378e-06}, {"id": 587, "seek": 283904, "start": 2850.96, "end": 2852.96, "text": " not something that", "tokens": [406, 746, 300], "temperature": 0.0, "avg_logprob": -0.20094444354375204, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.237751570850378e-06}, {"id": 588, "seek": 283904, "start": 2853.36, "end": 2860.88, "text": " We've like invented, but I'd also say it's like it's so not that common that it doesn't even have a name as far as I know", "tokens": [492, 600, 411, 14479, 11, 457, 286, 1116, 611, 584, 309, 311, 411, 309, 311, 370, 406, 300, 2689, 300, 309, 1177, 380, 754, 362, 257, 1315, 382, 1400, 382, 286, 458], "temperature": 0.0, "avg_logprob": -0.20094444354375204, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.237751570850378e-06}, {"id": 589, "seek": 283904, "start": 2861.0, "end": 2863.0, "text": " So we're going to call it", "tokens": [407, 321, 434, 516, 281, 818, 309], "temperature": 0.0, "avg_logprob": -0.20094444354375204, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.237751570850378e-06}, {"id": 590, "seek": 283904, "start": 2863.52, "end": 2865.52, "text": " differential learning rates", "tokens": [15756, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.20094444354375204, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.237751570850378e-06}, {"id": 591, "seek": 286552, "start": 2865.52, "end": 2872.16, "text": " If it actually has a name or indeed if somebody's actually written a paper specifically talking about it. I don't know", "tokens": [759, 309, 767, 575, 257, 1315, 420, 6451, 498, 2618, 311, 767, 3720, 257, 3035, 4682, 1417, 466, 309, 13, 286, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.159101786436858, "compression_ratio": 1.718978102189781, "no_speech_prob": 7.646463927812874e-06}, {"id": 592, "seek": 286552, "start": 2872.88, "end": 2879.28, "text": " There's a great researcher called Jason Yusinski who who did write a paper about the kind of the idea that you might want different", "tokens": [821, 311, 257, 869, 21751, 1219, 11181, 398, 301, 38984, 567, 567, 630, 2464, 257, 3035, 466, 264, 733, 295, 264, 1558, 300, 291, 1062, 528, 819], "temperature": 0.0, "avg_logprob": -0.159101786436858, "compression_ratio": 1.718978102189781, "no_speech_prob": 7.646463927812874e-06}, {"id": 593, "seek": 286552, "start": 2879.28, "end": 2883.7599999999998, "text": " learning rates and showing why but I don't think any other libraries support it and", "tokens": [2539, 6846, 293, 4099, 983, 457, 286, 500, 380, 519, 604, 661, 15148, 1406, 309, 293], "temperature": 0.0, "avg_logprob": -0.159101786436858, "compression_ratio": 1.718978102189781, "no_speech_prob": 7.646463927812874e-06}, {"id": 594, "seek": 286552, "start": 2884.68, "end": 2888.0, "text": " Yeah, I don't know of a name for it having said that though", "tokens": [865, 11, 286, 500, 380, 458, 295, 257, 1315, 337, 309, 1419, 848, 300, 1673], "temperature": 0.0, "avg_logprob": -0.159101786436858, "compression_ratio": 1.718978102189781, "no_speech_prob": 7.646463927812874e-06}, {"id": 595, "seek": 286552, "start": 2888.84, "end": 2893.4, "text": " This ability to like unfreeze and then use these differential learning rates", "tokens": [639, 3485, 281, 411, 3971, 701, 1381, 293, 550, 764, 613, 15756, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.159101786436858, "compression_ratio": 1.718978102189781, "no_speech_prob": 7.646463927812874e-06}, {"id": 596, "seek": 289340, "start": 2893.4, "end": 2899.2000000000003, "text": " I found is like the secret to taking a pretty good model and turning it into an awesome", "tokens": [286, 1352, 307, 411, 264, 4054, 281, 1940, 257, 1238, 665, 2316, 293, 6246, 309, 666, 364, 3476], "temperature": 0.0, "avg_logprob": -0.3175259251748362, "compression_ratio": 1.59375, "no_speech_prob": 8.9395198301645e-06}, {"id": 597, "seek": 289340, "start": 2904.96, "end": 2906.96, "text": " So just to clarify", "tokens": [407, 445, 281, 17594], "temperature": 0.0, "avg_logprob": -0.3175259251748362, "compression_ratio": 1.59375, "no_speech_prob": 8.9395198301645e-06}, {"id": 598, "seek": 289340, "start": 2908.1600000000003, "end": 2914.06, "text": " So you have three numbers there right three hyper parameters the first one is the photo", "tokens": [407, 291, 362, 1045, 3547, 456, 558, 1045, 9848, 9834, 264, 700, 472, 307, 264, 5052], "temperature": 0.0, "avg_logprob": -0.3175259251748362, "compression_ratio": 1.59375, "no_speech_prob": 8.9395198301645e-06}, {"id": 599, "seek": 289340, "start": 2914.88, "end": 2918.56, "text": " Late models the model late layers the other way around", "tokens": [31220, 5245, 264, 2316, 3469, 7914, 264, 661, 636, 926], "temperature": 0.0, "avg_logprob": -0.3175259251748362, "compression_ratio": 1.59375, "no_speech_prob": 8.9395198301645e-06}, {"id": 600, "seek": 289340, "start": 2920.04, "end": 2921.6800000000003, "text": " so we", "tokens": [370, 321], "temperature": 0.0, "avg_logprob": -0.3175259251748362, "compression_ratio": 1.59375, "no_speech_prob": 8.9395198301645e-06}, {"id": 601, "seek": 292168, "start": 2921.68, "end": 2927.56, "text": " The short answer is many many right and they're kind of in groups and we're going to learn about the architecture", "tokens": [440, 2099, 1867, 307, 867, 867, 558, 293, 436, 434, 733, 295, 294, 3935, 293, 321, 434, 516, 281, 1466, 466, 264, 9482], "temperature": 0.0, "avg_logprob": -0.23442884010843712, "compression_ratio": 1.6825396825396826, "no_speech_prob": 8.139514648064505e-06}, {"id": 602, "seek": 292168, "start": 2927.56, "end": 2931.3399999999997, "text": " This is called a resnet or residual network. It kind of has resnet blocks", "tokens": [639, 307, 1219, 257, 725, 7129, 420, 27980, 3209, 13, 467, 733, 295, 575, 725, 7129, 8474], "temperature": 0.0, "avg_logprob": -0.23442884010843712, "compression_ratio": 1.6825396825396826, "no_speech_prob": 8.139514648064505e-06}, {"id": 603, "seek": 292168, "start": 2932.0, "end": 2936.04, "text": " And so what we're doing is we're grouping the blocks into three", "tokens": [400, 370, 437, 321, 434, 884, 307, 321, 434, 40149, 264, 8474, 666, 1045], "temperature": 0.0, "avg_logprob": -0.23442884010843712, "compression_ratio": 1.6825396825396826, "no_speech_prob": 8.139514648064505e-06}, {"id": 604, "seek": 292168, "start": 2936.8399999999997, "end": 2941.44, "text": " Groups and so this one is actually this first number is for the earliest layers", "tokens": [10500, 82, 293, 370, 341, 472, 307, 767, 341, 700, 1230, 307, 337, 264, 20573, 7914], "temperature": 0.0, "avg_logprob": -0.23442884010843712, "compression_ratio": 1.6825396825396826, "no_speech_prob": 8.139514648064505e-06}, {"id": 605, "seek": 292168, "start": 2943.64, "end": 2949.2599999999998, "text": " Yeah, the ones closest to the pixels represent like corners and edges and gradients, but why", "tokens": [865, 11, 264, 2306, 13699, 281, 264, 18668, 2906, 411, 12413, 293, 8819, 293, 2771, 2448, 11, 457, 983], "temperature": 0.0, "avg_logprob": -0.23442884010843712, "compression_ratio": 1.6825396825396826, "no_speech_prob": 8.139514648064505e-06}, {"id": 606, "seek": 294926, "start": 2949.26, "end": 2954.8, "text": " Why do you I thought those layers are frozen at first?", "tokens": [1545, 360, 291, 286, 1194, 729, 7914, 366, 12496, 412, 700, 30], "temperature": 0.0, "avg_logprob": -0.2989859207816746, "compression_ratio": 1.757936507936508, "no_speech_prob": 2.355217293370515e-05}, {"id": 607, "seek": 294926, "start": 2954.8, "end": 2960.76, "text": " They are right. So we just hit unfreeze unfreeze. So we so you're unfreezing them because you have kind of partially trained", "tokens": [814, 366, 558, 13, 407, 321, 445, 2045, 3971, 701, 1381, 3971, 701, 1381, 13, 407, 321, 370, 291, 434, 3971, 701, 8781, 552, 570, 291, 362, 733, 295, 18886, 8895], "temperature": 0.0, "avg_logprob": -0.2989859207816746, "compression_ratio": 1.757936507936508, "no_speech_prob": 2.355217293370515e-05}, {"id": 608, "seek": 294926, "start": 2961.7000000000003, "end": 2968.7400000000002, "text": " All the late layers. We've trained we've trained our added layers. Yes. Now you're retraining the whole set exactly", "tokens": [1057, 264, 3469, 7914, 13, 492, 600, 8895, 321, 600, 8895, 527, 3869, 7914, 13, 1079, 13, 823, 291, 434, 49356, 1760, 264, 1379, 992, 2293], "temperature": 0.0, "avg_logprob": -0.2989859207816746, "compression_ratio": 1.757936507936508, "no_speech_prob": 2.355217293370515e-05}, {"id": 609, "seek": 294926, "start": 2968.7400000000002, "end": 2973.5600000000004, "text": " I see so but say and the learning rate is particularly small for the early layers", "tokens": [286, 536, 370, 457, 584, 293, 264, 2539, 3314, 307, 4098, 1359, 337, 264, 2440, 7914], "temperature": 0.0, "avg_logprob": -0.2989859207816746, "compression_ratio": 1.757936507936508, "no_speech_prob": 2.355217293370515e-05}, {"id": 610, "seek": 294926, "start": 2973.5600000000004, "end": 2977.0200000000004, "text": " That's right, because you just kind of want to fine-tune it. Yeah", "tokens": [663, 311, 558, 11, 570, 291, 445, 733, 295, 528, 281, 2489, 12, 83, 2613, 309, 13, 865], "temperature": 0.0, "avg_logprob": -0.2989859207816746, "compression_ratio": 1.757936507936508, "no_speech_prob": 2.355217293370515e-05}, {"id": 611, "seek": 297702, "start": 2977.02, "end": 2979.34, "text": " Yeah, we probably don't want to change them at all", "tokens": [865, 11, 321, 1391, 500, 380, 528, 281, 1319, 552, 412, 439], "temperature": 0.0, "avg_logprob": -0.24890680746598678, "compression_ratio": 1.6318181818181818, "no_speech_prob": 7.071839718264528e-06}, {"id": 612, "seek": 297702, "start": 2980.1, "end": 2983.18, "text": " But you know if it does need to then then it can", "tokens": [583, 291, 458, 498, 309, 775, 643, 281, 550, 550, 309, 393], "temperature": 0.0, "avg_logprob": -0.24890680746598678, "compression_ratio": 1.6318181818181818, "no_speech_prob": 7.071839718264528e-06}, {"id": 613, "seek": 297702, "start": 2983.94, "end": 2985.94, "text": " Thanks, no problem", "tokens": [2561, 11, 572, 1154], "temperature": 0.0, "avg_logprob": -0.24890680746598678, "compression_ratio": 1.6318181818181818, "no_speech_prob": 7.071839718264528e-06}, {"id": 614, "seek": 297702, "start": 2988.78, "end": 2992.98, "text": " So using the differential inner rates how different from like grid search", "tokens": [407, 1228, 264, 15756, 7284, 6846, 577, 819, 490, 411, 10748, 3164], "temperature": 0.0, "avg_logprob": -0.24890680746598678, "compression_ratio": 1.6318181818181818, "no_speech_prob": 7.071839718264528e-06}, {"id": 615, "seek": 297702, "start": 2995.14, "end": 3001.06, "text": " There's no similarity to grid search so grid search is where we're trying to find the best hyper parameter", "tokens": [821, 311, 572, 32194, 281, 10748, 3164, 370, 10748, 3164, 307, 689, 321, 434, 1382, 281, 915, 264, 1151, 9848, 13075], "temperature": 0.0, "avg_logprob": -0.24890680746598678, "compression_ratio": 1.6318181818181818, "no_speech_prob": 7.071839718264528e-06}, {"id": 616, "seek": 297702, "start": 3001.86, "end": 3003.86, "text": " for something so for example", "tokens": [337, 746, 370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.24890680746598678, "compression_ratio": 1.6318181818181818, "no_speech_prob": 7.071839718264528e-06}, {"id": 617, "seek": 300386, "start": 3003.86, "end": 3006.58, "text": " you could kind of think of the", "tokens": [291, 727, 733, 295, 519, 295, 264], "temperature": 0.0, "avg_logprob": -0.19346793074356883, "compression_ratio": 1.6565656565656566, "no_speech_prob": 4.495130269788206e-06}, {"id": 618, "seek": 300386, "start": 3007.7400000000002, "end": 3013.9, "text": " Learning rate finder as a really sophisticated grid search, which is like trying lots and lots of learning rates to find which one is best", "tokens": [15205, 3314, 915, 260, 382, 257, 534, 16950, 10748, 3164, 11, 597, 307, 411, 1382, 3195, 293, 3195, 295, 2539, 6846, 281, 915, 597, 472, 307, 1151], "temperature": 0.0, "avg_logprob": -0.19346793074356883, "compression_ratio": 1.6565656565656566, "no_speech_prob": 4.495130269788206e-06}, {"id": 619, "seek": 300386, "start": 3014.98, "end": 3019.42, "text": " But this has nothing to do with that. This is actually for the entire training from now on", "tokens": [583, 341, 575, 1825, 281, 360, 365, 300, 13, 639, 307, 767, 337, 264, 2302, 3097, 490, 586, 322], "temperature": 0.0, "avg_logprob": -0.19346793074356883, "compression_ratio": 1.6565656565656566, "no_speech_prob": 4.495130269788206e-06}, {"id": 620, "seek": 300386, "start": 3019.6600000000003, "end": 3023.2000000000003, "text": " It's actually going to use a different learning rate for each layer", "tokens": [467, 311, 767, 516, 281, 764, 257, 819, 2539, 3314, 337, 1184, 4583], "temperature": 0.0, "avg_logprob": -0.19346793074356883, "compression_ratio": 1.6565656565656566, "no_speech_prob": 4.495130269788206e-06}, {"id": 621, "seek": 302320, "start": 3023.2, "end": 3031.48, "text": " And so I was wondering so you have a pre trained model then you have to use the same input", "tokens": [400, 370, 286, 390, 6359, 370, 291, 362, 257, 659, 8895, 2316, 550, 291, 362, 281, 764, 264, 912, 4846], "temperature": 0.0, "avg_logprob": -0.38188132059942814, "compression_ratio": 1.7136752136752136, "no_speech_prob": 8.217601134674624e-05}, {"id": 622, "seek": 302320, "start": 3033.0, "end": 3036.4399999999996, "text": " Dimensions right because I was thinking okay. Let's say you have this big", "tokens": [20975, 8302, 558, 570, 286, 390, 1953, 1392, 13, 961, 311, 584, 291, 362, 341, 955], "temperature": 0.0, "avg_logprob": -0.38188132059942814, "compression_ratio": 1.7136752136752136, "no_speech_prob": 8.217601134674624e-05}, {"id": 623, "seek": 302320, "start": 3037.0, "end": 3040.7999999999997, "text": " They use like big machines to train these things and you want to take advantage of it", "tokens": [814, 764, 411, 955, 8379, 281, 3847, 613, 721, 293, 291, 528, 281, 747, 5002, 295, 309], "temperature": 0.0, "avg_logprob": -0.38188132059942814, "compression_ratio": 1.7136752136752136, "no_speech_prob": 8.217601134674624e-05}, {"id": 624, "seek": 302320, "start": 3040.7999999999997, "end": 3047.16, "text": " How would you go about you know, you have like images that are like bigger than the ones that they used or we're going to be talking", "tokens": [1012, 576, 291, 352, 466, 291, 458, 11, 291, 362, 411, 5267, 300, 366, 411, 3801, 813, 264, 2306, 300, 436, 1143, 420, 321, 434, 516, 281, 312, 1417], "temperature": 0.0, "avg_logprob": -0.38188132059942814, "compression_ratio": 1.7136752136752136, "no_speech_prob": 8.217601134674624e-05}, {"id": 625, "seek": 302320, "start": 3047.16, "end": 3049.16, "text": " About sizes later", "tokens": [7769, 11602, 1780], "temperature": 0.0, "avg_logprob": -0.38188132059942814, "compression_ratio": 1.7136752136752136, "no_speech_prob": 8.217601134674624e-05}, {"id": 626, "seek": 304916, "start": 3049.16, "end": 3055.64, "text": " But the short answer is that with this library and the modern architectures were using we can use any size we like", "tokens": [583, 264, 2099, 1867, 307, 300, 365, 341, 6405, 293, 264, 4363, 6331, 1303, 645, 1228, 321, 393, 764, 604, 2744, 321, 411], "temperature": 0.0, "avg_logprob": -0.44220174153645836, "compression_ratio": 1.5654450261780104, "no_speech_prob": 9.515957572148181e-06}, {"id": 627, "seek": 304916, "start": 3059.24, "end": 3063.3599999999997, "text": " So Jeremy do we need a can we unfreeze just a specific layer", "tokens": [407, 17809, 360, 321, 643, 257, 393, 321, 3971, 701, 1381, 445, 257, 2685, 4583], "temperature": 0.0, "avg_logprob": -0.44220174153645836, "compression_ratio": 1.5654450261780104, "no_speech_prob": 9.515957572148181e-06}, {"id": 628, "seek": 304916, "start": 3063.8799999999997, "end": 3065.3199999999997, "text": " We can we're not doing it yet", "tokens": [492, 393, 321, 434, 406, 884, 309, 1939], "temperature": 0.0, "avg_logprob": -0.44220174153645836, "compression_ratio": 1.5654450261780104, "no_speech_prob": 9.515957572148181e-06}, {"id": 629, "seek": 304916, "start": 3065.3199999999997, "end": 3070.6, "text": " But if you wanted to you can type learn dot freeze underscore two and pass in a letter number", "tokens": [583, 498, 291, 1415, 281, 291, 393, 2010, 1466, 5893, 15959, 37556, 732, 293, 1320, 294, 257, 5063, 1230], "temperature": 0.0, "avg_logprob": -0.44220174153645836, "compression_ratio": 1.5654450261780104, "no_speech_prob": 9.515957572148181e-06}, {"id": 630, "seek": 307060, "start": 3070.6, "end": 3074.64, "text": " Learn dot freeze underscore two and passing a letter number", "tokens": [17216, 5893, 15959, 37556, 732, 293, 8437, 257, 5063, 1230], "temperature": 0.0, "avg_logprob": -0.19355903020719203, "compression_ratio": 1.5876777251184835, "no_speech_prob": 5.173850240680622e-06}, {"id": 631, "seek": 307060, "start": 3079.44, "end": 3083.8399999999997, "text": " Much to my surprise or at least initial my surprise it turns out I", "tokens": [12313, 281, 452, 6365, 420, 412, 1935, 5883, 452, 6365, 309, 4523, 484, 286], "temperature": 0.0, "avg_logprob": -0.19355903020719203, "compression_ratio": 1.5876777251184835, "no_speech_prob": 5.173850240680622e-06}, {"id": 632, "seek": 307060, "start": 3084.4, "end": 3090.2, "text": " Almost never need to do that. I almost never find it helpful, and I think it's because we're using differential learning rates", "tokens": [12627, 1128, 643, 281, 360, 300, 13, 286, 1920, 1128, 915, 309, 4961, 11, 293, 286, 519, 309, 311, 570, 321, 434, 1228, 15756, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.19355903020719203, "compression_ratio": 1.5876777251184835, "no_speech_prob": 5.173850240680622e-06}, {"id": 633, "seek": 307060, "start": 3091.04, "end": 3095.7999999999997, "text": " the the the optimizer can kind of learn just as much as it needs to", "tokens": [264, 264, 264, 5028, 6545, 393, 733, 295, 1466, 445, 382, 709, 382, 309, 2203, 281], "temperature": 0.0, "avg_logprob": -0.19355903020719203, "compression_ratio": 1.5876777251184835, "no_speech_prob": 5.173850240680622e-06}, {"id": 634, "seek": 309580, "start": 3095.8, "end": 3099.8, "text": " so yeah, it's", "tokens": [370, 1338, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.28417326698840506, "compression_ratio": 1.4823529411764707, "no_speech_prob": 6.5403824009990785e-06}, {"id": 635, "seek": 309580, "start": 3102.8, "end": 3107.76, "text": " What if you have a little data like very little data data yeah, it's still", "tokens": [708, 498, 291, 362, 257, 707, 1412, 411, 588, 707, 1412, 1412, 1338, 11, 309, 311, 920], "temperature": 0.0, "avg_logprob": -0.28417326698840506, "compression_ratio": 1.4823529411764707, "no_speech_prob": 6.5403824009990785e-06}, {"id": 636, "seek": 309580, "start": 3108.6800000000003, "end": 3112.02, "text": " Doesn't seem to help the one place. I have found it helpful is if", "tokens": [12955, 380, 1643, 281, 854, 264, 472, 1081, 13, 286, 362, 1352, 309, 4961, 307, 498], "temperature": 0.0, "avg_logprob": -0.28417326698840506, "compression_ratio": 1.4823529411764707, "no_speech_prob": 6.5403824009990785e-06}, {"id": 637, "seek": 309580, "start": 3112.84, "end": 3117.32, "text": " I'm using like a really big memory intensive model, and I'm like running out of GPU", "tokens": [286, 478, 1228, 411, 257, 534, 955, 4675, 18957, 2316, 11, 293, 286, 478, 411, 2614, 484, 295, 18407], "temperature": 0.0, "avg_logprob": -0.28417326698840506, "compression_ratio": 1.4823529411764707, "no_speech_prob": 6.5403824009990785e-06}, {"id": 638, "seek": 309580, "start": 3118.2400000000002, "end": 3120.2400000000002, "text": " freeze having", "tokens": [15959, 1419], "temperature": 0.0, "avg_logprob": -0.28417326698840506, "compression_ratio": 1.4823529411764707, "no_speech_prob": 6.5403824009990785e-06}, {"id": 639, "seek": 312024, "start": 3120.24, "end": 3126.7599999999998, "text": " The less layers you unfreeze the less memory it takes and the less time it takes so there's that kind of practical aspect", "tokens": [440, 1570, 7914, 291, 3971, 701, 1381, 264, 1570, 4675, 309, 2516, 293, 264, 1570, 565, 309, 2516, 370, 456, 311, 300, 733, 295, 8496, 4171], "temperature": 0.0, "avg_logprob": -0.19923778333162007, "compression_ratio": 1.6858407079646018, "no_speech_prob": 8.801010153547395e-06}, {"id": 640, "seek": 312024, "start": 3126.7599999999998, "end": 3129.8399999999997, "text": " So to make sure also I asked the question right?", "tokens": [407, 281, 652, 988, 611, 286, 2351, 264, 1168, 558, 30], "temperature": 0.0, "avg_logprob": -0.19923778333162007, "compression_ratio": 1.6858407079646018, "no_speech_prob": 8.801010153547395e-06}, {"id": 641, "seek": 312024, "start": 3130.6, "end": 3139.08, "text": " Can I just like unfreeze a specific layer? No you you can only unfreeze layers from layer in onwards", "tokens": [1664, 286, 445, 411, 3971, 701, 1381, 257, 2685, 4583, 30, 883, 291, 291, 393, 787, 3971, 701, 1381, 7914, 490, 4583, 294, 34230], "temperature": 0.0, "avg_logprob": -0.19923778333162007, "compression_ratio": 1.6858407079646018, "no_speech_prob": 8.801010153547395e-06}, {"id": 642, "seek": 312024, "start": 3141.12, "end": 3145.68, "text": " You could probably delve inside the library and freeze one unfreeze one layer, but I don't know why you would", "tokens": [509, 727, 1391, 43098, 1854, 264, 6405, 293, 15959, 472, 3971, 701, 1381, 472, 4583, 11, 457, 286, 500, 380, 458, 983, 291, 576], "temperature": 0.0, "avg_logprob": -0.19923778333162007, "compression_ratio": 1.6858407079646018, "no_speech_prob": 8.801010153547395e-06}, {"id": 643, "seek": 314568, "start": 3145.68, "end": 3147.68, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.1840193612234933, "compression_ratio": 1.548780487804878, "no_speech_prob": 1.0511446362215793e-06}, {"id": 644, "seek": 314568, "start": 3148.56, "end": 3153.7999999999997, "text": " So I'm really excited to be showing you guys this stuff because it's like it's something we've been kind of researching all year", "tokens": [407, 286, 478, 534, 2919, 281, 312, 4099, 291, 1074, 341, 1507, 570, 309, 311, 411, 309, 311, 746, 321, 600, 668, 733, 295, 24176, 439, 1064], "temperature": 0.0, "avg_logprob": -0.1840193612234933, "compression_ratio": 1.548780487804878, "no_speech_prob": 1.0511446362215793e-06}, {"id": 645, "seek": 314568, "start": 3153.7999999999997, "end": 3155.7999999999997, "text": " It's figuring out how to train", "tokens": [467, 311, 15213, 484, 577, 281, 3847], "temperature": 0.0, "avg_logprob": -0.1840193612234933, "compression_ratio": 1.548780487804878, "no_speech_prob": 1.0511446362215793e-06}, {"id": 646, "seek": 314568, "start": 3155.7999999999997, "end": 3161.72, "text": " State-of-the-art models, and we've kind of found these like tiny number of tricks and so once we do that", "tokens": [4533, 12, 2670, 12, 3322, 12, 446, 5245, 11, 293, 321, 600, 733, 295, 1352, 613, 411, 5870, 1230, 295, 11733, 293, 370, 1564, 321, 360, 300], "temperature": 0.0, "avg_logprob": -0.1840193612234933, "compression_ratio": 1.548780487804878, "no_speech_prob": 1.0511446362215793e-06}, {"id": 647, "seek": 314568, "start": 3161.96, "end": 3168.3199999999997, "text": " We now go learn dot fit right and you can see look at this we get right up to like 99.5", "tokens": [492, 586, 352, 1466, 5893, 3318, 558, 293, 291, 393, 536, 574, 412, 341, 321, 483, 558, 493, 281, 411, 11803, 13, 20], "temperature": 0.0, "avg_logprob": -0.1840193612234933, "compression_ratio": 1.548780487804878, "no_speech_prob": 1.0511446362215793e-06}, {"id": 648, "seek": 314568, "start": 3169.3999999999996, "end": 3171.3999999999996, "text": " Accuracy which is crazy", "tokens": [5725, 374, 2551, 597, 307, 3219], "temperature": 0.0, "avg_logprob": -0.1840193612234933, "compression_ratio": 1.548780487804878, "no_speech_prob": 1.0511446362215793e-06}, {"id": 649, "seek": 317140, "start": 3171.4, "end": 3179.2400000000002, "text": " There's one other trick you might see here that as well as using stochastic gradient descent with restarts a cycle length equals one", "tokens": [821, 311, 472, 661, 4282, 291, 1062, 536, 510, 300, 382, 731, 382, 1228, 342, 8997, 2750, 16235, 23475, 365, 1472, 11814, 257, 6586, 4641, 6915, 472], "temperature": 0.0, "avg_logprob": -0.19473754607879365, "compression_ratio": 1.8808510638297873, "no_speech_prob": 2.5215617824869696e-06}, {"id": 650, "seek": 317140, "start": 3179.6800000000003, "end": 3185.84, "text": " We've done three cycles so earlier on I lied to you. I said this is this is the number of epochs", "tokens": [492, 600, 1096, 1045, 17796, 370, 3071, 322, 286, 20101, 281, 291, 13, 286, 848, 341, 307, 341, 307, 264, 1230, 295, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.19473754607879365, "compression_ratio": 1.8808510638297873, "no_speech_prob": 2.5215617824869696e-06}, {"id": 651, "seek": 317140, "start": 3186.0, "end": 3191.4, "text": " It's actually the number of cycles right so if you said cycle length equals two it would do three", "tokens": [467, 311, 767, 264, 1230, 295, 17796, 558, 370, 498, 291, 848, 6586, 4641, 6915, 732, 309, 576, 360, 1045], "temperature": 0.0, "avg_logprob": -0.19473754607879365, "compression_ratio": 1.8808510638297873, "no_speech_prob": 2.5215617824869696e-06}, {"id": 652, "seek": 317140, "start": 3191.92, "end": 3195.32, "text": " cycles of each of two epochs or do six epochs", "tokens": [17796, 295, 1184, 295, 732, 30992, 28346, 420, 360, 2309, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.19473754607879365, "compression_ratio": 1.8808510638297873, "no_speech_prob": 2.5215617824869696e-06}, {"id": 653, "seek": 317140, "start": 3195.84, "end": 3199.6800000000003, "text": " So here I've said do three cycles yet somehow it's done seven epochs", "tokens": [407, 510, 286, 600, 848, 360, 1045, 17796, 1939, 6063, 309, 311, 1096, 3407, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.19473754607879365, "compression_ratio": 1.8808510638297873, "no_speech_prob": 2.5215617824869696e-06}, {"id": 654, "seek": 319968, "start": 3199.68, "end": 3205.04, "text": " and the reason why is I've got one last trick to show you which is cycle molt equals two and", "tokens": [293, 264, 1778, 983, 307, 286, 600, 658, 472, 1036, 4282, 281, 855, 291, 597, 307, 6586, 10739, 6915, 732, 293], "temperature": 0.0, "avg_logprob": -0.22623077598777977, "compression_ratio": 1.7333333333333334, "no_speech_prob": 6.962197858229047e-06}, {"id": 655, "seek": 319968, "start": 3205.3999999999996, "end": 3209.12, "text": " To tell you what that does. I'm simply going to draw you a picture show you the picture", "tokens": [1407, 980, 291, 437, 300, 775, 13, 286, 478, 2935, 516, 281, 2642, 291, 257, 3036, 855, 291, 264, 3036], "temperature": 0.0, "avg_logprob": -0.22623077598777977, "compression_ratio": 1.7333333333333334, "no_speech_prob": 6.962197858229047e-06}, {"id": 656, "seek": 319968, "start": 3209.9199999999996, "end": 3213.3199999999997, "text": " If I go learn dot chef dot plot learning rate there it is", "tokens": [759, 286, 352, 1466, 5893, 10530, 5893, 7542, 2539, 3314, 456, 309, 307], "temperature": 0.0, "avg_logprob": -0.22623077598777977, "compression_ratio": 1.7333333333333334, "no_speech_prob": 6.962197858229047e-06}, {"id": 657, "seek": 319968, "start": 3214.52, "end": 3217.6, "text": " Now you can see what cycle mode equals two is doing okay?", "tokens": [823, 291, 393, 536, 437, 6586, 4391, 6915, 732, 307, 884, 1392, 30], "temperature": 0.0, "avg_logprob": -0.22623077598777977, "compression_ratio": 1.7333333333333334, "no_speech_prob": 6.962197858229047e-06}, {"id": 658, "seek": 319968, "start": 3217.6, "end": 3226.0, "text": " It's it's in doubling the length of the cycle after each cycle and so in the paper that introduced this stochastic gradient descent with", "tokens": [467, 311, 309, 311, 294, 33651, 264, 4641, 295, 264, 6586, 934, 1184, 6586, 293, 370, 294, 264, 3035, 300, 7268, 341, 342, 8997, 2750, 16235, 23475, 365], "temperature": 0.0, "avg_logprob": -0.22623077598777977, "compression_ratio": 1.7333333333333334, "no_speech_prob": 6.962197858229047e-06}, {"id": 659, "seek": 319968, "start": 3226.12, "end": 3227.04, "text": " restarts", "tokens": [1472, 11814], "temperature": 0.0, "avg_logprob": -0.22623077598777977, "compression_ratio": 1.7333333333333334, "no_speech_prob": 6.962197858229047e-06}, {"id": 660, "seek": 322704, "start": 3227.04, "end": 3232.08, "text": " But the researcher kind of said hey, this is something that seems to sometimes work pretty well", "tokens": [583, 264, 21751, 733, 295, 848, 4177, 11, 341, 307, 746, 300, 2544, 281, 2171, 589, 1238, 731], "temperature": 0.0, "avg_logprob": -0.15443087559120328, "compression_ratio": 1.7344398340248963, "no_speech_prob": 1.392543822476e-06}, {"id": 661, "seek": 322704, "start": 3232.08, "end": 3235.52, "text": " And I've certainly found that often to be the case so basically", "tokens": [400, 286, 600, 3297, 1352, 300, 2049, 281, 312, 264, 1389, 370, 1936], "temperature": 0.0, "avg_logprob": -0.15443087559120328, "compression_ratio": 1.7344398340248963, "no_speech_prob": 1.392543822476e-06}, {"id": 662, "seek": 322704, "start": 3237.84, "end": 3241.52, "text": " Intuitively speaking if your cycle length is too short", "tokens": [5681, 1983, 3413, 4124, 498, 428, 6586, 4641, 307, 886, 2099], "temperature": 0.0, "avg_logprob": -0.15443087559120328, "compression_ratio": 1.7344398340248963, "no_speech_prob": 1.392543822476e-06}, {"id": 663, "seek": 322704, "start": 3242.08, "end": 3246.2799999999997, "text": " Right then it's kind of starts going down to find a good spot", "tokens": [1779, 550, 309, 311, 733, 295, 3719, 516, 760, 281, 915, 257, 665, 4008], "temperature": 0.0, "avg_logprob": -0.15443087559120328, "compression_ratio": 1.7344398340248963, "no_speech_prob": 1.392543822476e-06}, {"id": 664, "seek": 322704, "start": 3246.2799999999997, "end": 3250.96, "text": " And then it pops out and it goes down to try and find a good spot and pops out it never actually gets to find", "tokens": [400, 550, 309, 16795, 484, 293, 309, 1709, 760, 281, 853, 293, 915, 257, 665, 4008, 293, 16795, 484, 309, 1128, 767, 2170, 281, 915], "temperature": 0.0, "avg_logprob": -0.15443087559120328, "compression_ratio": 1.7344398340248963, "no_speech_prob": 1.392543822476e-06}, {"id": 665, "seek": 322704, "start": 3250.96, "end": 3252.96, "text": " A good spot right so earlier on", "tokens": [316, 665, 4008, 558, 370, 3071, 322], "temperature": 0.0, "avg_logprob": -0.15443087559120328, "compression_ratio": 1.7344398340248963, "no_speech_prob": 1.392543822476e-06}, {"id": 666, "seek": 325296, "start": 3252.96, "end": 3257.04, "text": " You want it to do that because it's trying to find the bit that's like smoother", "tokens": [509, 528, 309, 281, 360, 300, 570, 309, 311, 1382, 281, 915, 264, 857, 300, 311, 411, 28640], "temperature": 0.0, "avg_logprob": -0.1773230479313777, "compression_ratio": 1.76953125, "no_speech_prob": 2.6425789201312e-06}, {"id": 667, "seek": 325296, "start": 3257.28, "end": 3263.4, "text": " But then later on you want it to find do more exploring and then more exploring right so that's why this", "tokens": [583, 550, 1780, 322, 291, 528, 309, 281, 915, 360, 544, 12736, 293, 550, 544, 12736, 558, 370, 300, 311, 983, 341], "temperature": 0.0, "avg_logprob": -0.1773230479313777, "compression_ratio": 1.76953125, "no_speech_prob": 2.6425789201312e-06}, {"id": 668, "seek": 325296, "start": 3264.32, "end": 3269.88, "text": " Cycle molt equals two thing often seems to be a pretty good approach right so", "tokens": [10295, 2160, 10739, 6915, 732, 551, 2049, 2544, 281, 312, 257, 1238, 665, 3109, 558, 370], "temperature": 0.0, "avg_logprob": -0.1773230479313777, "compression_ratio": 1.76953125, "no_speech_prob": 2.6425789201312e-06}, {"id": 669, "seek": 325296, "start": 3270.56, "end": 3275.36, "text": " Suddenly we're introducing more and more hyper parameters having told you there aren't that many", "tokens": [21194, 321, 434, 15424, 544, 293, 544, 9848, 9834, 1419, 1907, 291, 456, 3212, 380, 300, 867], "temperature": 0.0, "avg_logprob": -0.1773230479313777, "compression_ratio": 1.76953125, "no_speech_prob": 2.6425789201312e-06}, {"id": 670, "seek": 325296, "start": 3275.4, "end": 3281.04, "text": " But but the reason is that like you can really get away with just taking a good learning rate", "tokens": [583, 457, 264, 1778, 307, 300, 411, 291, 393, 534, 483, 1314, 365, 445, 1940, 257, 665, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.1773230479313777, "compression_ratio": 1.76953125, "no_speech_prob": 2.6425789201312e-06}, {"id": 671, "seek": 328104, "start": 3281.04, "end": 3283.04, "text": " But then adding these", "tokens": [583, 550, 5127, 613], "temperature": 0.0, "avg_logprob": -0.23118418863374893, "compression_ratio": 1.5549738219895288, "no_speech_prob": 3.989710819496395e-07}, {"id": 672, "seek": 328104, "start": 3283.64, "end": 3285.4, "text": " extra tweaks", "tokens": [2857, 46664], "temperature": 0.0, "avg_logprob": -0.23118418863374893, "compression_ratio": 1.5549738219895288, "no_speech_prob": 3.989710819496395e-07}, {"id": 673, "seek": 328104, "start": 3285.4, "end": 3292.72, "text": " Really helps get that extra level up without any effort right and so in practice I find", "tokens": [4083, 3665, 483, 300, 2857, 1496, 493, 1553, 604, 4630, 558, 293, 370, 294, 3124, 286, 915], "temperature": 0.0, "avg_logprob": -0.23118418863374893, "compression_ratio": 1.5549738219895288, "no_speech_prob": 3.989710819496395e-07}, {"id": 674, "seek": 328104, "start": 3293.7599999999998, "end": 3298.04, "text": " This kind of three cycles starting at one molecules to", "tokens": [639, 733, 295, 1045, 17796, 2891, 412, 472, 13093, 281], "temperature": 0.0, "avg_logprob": -0.23118418863374893, "compression_ratio": 1.5549738219895288, "no_speech_prob": 3.989710819496395e-07}, {"id": 675, "seek": 328104, "start": 3299.2, "end": 3303.02, "text": " Works very very often to get a pretty decent model", "tokens": [27914, 588, 588, 2049, 281, 483, 257, 1238, 8681, 2316], "temperature": 0.0, "avg_logprob": -0.23118418863374893, "compression_ratio": 1.5549738219895288, "no_speech_prob": 3.989710819496395e-07}, {"id": 676, "seek": 328104, "start": 3304.7599999999998, "end": 3309.4, "text": " If it does doesn't then often I'll just do three cycles of length to", "tokens": [759, 309, 775, 1177, 380, 550, 2049, 286, 603, 445, 360, 1045, 17796, 295, 4641, 281], "temperature": 0.0, "avg_logprob": -0.23118418863374893, "compression_ratio": 1.5549738219895288, "no_speech_prob": 3.989710819496395e-07}, {"id": 677, "seek": 330940, "start": 3309.4, "end": 3314.08, "text": " With no mold like that's kind of like two things that seem to work a lot", "tokens": [2022, 572, 11102, 411, 300, 311, 733, 295, 411, 732, 721, 300, 1643, 281, 589, 257, 688], "temperature": 0.0, "avg_logprob": -0.24528999328613282, "compression_ratio": 1.5523809523809524, "no_speech_prob": 4.222810275678057e-06}, {"id": 678, "seek": 330940, "start": 3314.36, "end": 3316.84, "text": " There's not too much fiddling I find necessary", "tokens": [821, 311, 406, 886, 709, 283, 14273, 1688, 286, 915, 4818], "temperature": 0.0, "avg_logprob": -0.24528999328613282, "compression_ratio": 1.5523809523809524, "no_speech_prob": 4.222810275678057e-06}, {"id": 679, "seek": 330940, "start": 3317.2000000000003, "end": 3321.12, "text": " And as I say even even if you just if you use this line every time", "tokens": [400, 382, 286, 584, 754, 754, 498, 291, 445, 498, 291, 764, 341, 1622, 633, 565], "temperature": 0.0, "avg_logprob": -0.24528999328613282, "compression_ratio": 1.5523809523809524, "no_speech_prob": 4.222810275678057e-06}, {"id": 680, "seek": 330940, "start": 3321.32, "end": 3324.96, "text": " I'd be surprised if you didn't get a reasonable result so a question here", "tokens": [286, 1116, 312, 6100, 498, 291, 994, 380, 483, 257, 10585, 1874, 370, 257, 1168, 510], "temperature": 0.0, "avg_logprob": -0.24528999328613282, "compression_ratio": 1.5523809523809524, "no_speech_prob": 4.222810275678057e-06}, {"id": 681, "seek": 332496, "start": 3324.96, "end": 3336.56, "text": " Why does smoother services correlate to more generalized networks?", "tokens": [1545, 775, 28640, 3328, 48742, 281, 544, 44498, 9590, 30], "temperature": 0.0, "avg_logprob": -0.3335336264917406, "compression_ratio": 1.3875, "no_speech_prob": 6.240803941182094e-06}, {"id": 682, "seek": 332496, "start": 3339.64, "end": 3341.64, "text": " So it's kind of this um", "tokens": [407, 309, 311, 733, 295, 341, 1105], "temperature": 0.0, "avg_logprob": -0.3335336264917406, "compression_ratio": 1.3875, "no_speech_prob": 6.240803941182094e-06}, {"id": 683, "seek": 332496, "start": 3342.12, "end": 3347.78, "text": " This intuitive explanation. I tried to kill the whole thing. I tried to give back here, which is that", "tokens": [639, 21769, 10835, 13, 286, 3031, 281, 1961, 264, 1379, 551, 13, 286, 3031, 281, 976, 646, 510, 11, 597, 307, 300], "temperature": 0.0, "avg_logprob": -0.3335336264917406, "compression_ratio": 1.3875, "no_speech_prob": 6.240803941182094e-06}, {"id": 684, "seek": 332496, "start": 3348.8, "end": 3350.8, "text": " if you've got", "tokens": [498, 291, 600, 658], "temperature": 0.0, "avg_logprob": -0.3335336264917406, "compression_ratio": 1.3875, "no_speech_prob": 6.240803941182094e-06}, {"id": 685, "seek": 332496, "start": 3352.08, "end": 3354.08, "text": " Something spiky", "tokens": [6595, 637, 1035, 88], "temperature": 0.0, "avg_logprob": -0.3335336264917406, "compression_ratio": 1.3875, "no_speech_prob": 6.240803941182094e-06}, {"id": 686, "seek": 335408, "start": 3354.08, "end": 3356.84, "text": " All right, and so what this", "tokens": [1057, 558, 11, 293, 370, 437, 341], "temperature": 0.0, "avg_logprob": -0.21114415388840896, "compression_ratio": 1.743455497382199, "no_speech_prob": 3.966968961321982e-06}, {"id": 687, "seek": 335408, "start": 3358.92, "end": 3362.3199999999997, "text": " What this x-axis is showing is like how", "tokens": [708, 341, 2031, 12, 24633, 307, 4099, 307, 411, 577], "temperature": 0.0, "avg_logprob": -0.21114415388840896, "compression_ratio": 1.743455497382199, "no_speech_prob": 3.966968961321982e-06}, {"id": 688, "seek": 335408, "start": 3362.96, "end": 3371.02, "text": " How good is this at recognizing dogs versus cats as you change this particular parameter right and so to something to be generalizable", "tokens": [1012, 665, 307, 341, 412, 18538, 7197, 5717, 11111, 382, 291, 1319, 341, 1729, 13075, 558, 293, 370, 281, 746, 281, 312, 2674, 22395], "temperature": 0.0, "avg_logprob": -0.21114415388840896, "compression_ratio": 1.743455497382199, "no_speech_prob": 3.966968961321982e-06}, {"id": 689, "seek": 335408, "start": 3371.7999999999997, "end": 3378.04, "text": " It means that we wanted to work when we give it when we give it a slightly different data set and so a slightly different data set", "tokens": [467, 1355, 300, 321, 1415, 281, 589, 562, 321, 976, 309, 562, 321, 976, 309, 257, 4748, 819, 1412, 992, 293, 370, 257, 4748, 819, 1412, 992], "temperature": 0.0, "avg_logprob": -0.21114415388840896, "compression_ratio": 1.743455497382199, "no_speech_prob": 3.966968961321982e-06}, {"id": 690, "seek": 337804, "start": 3378.04, "end": 3386.68, "text": " May have a slightly different relationship between this parameter and how caddy versus doggy it is it may instead look a little bit like this", "tokens": [1891, 362, 257, 4748, 819, 2480, 1296, 341, 13075, 293, 577, 12209, 3173, 5717, 3000, 1480, 309, 307, 309, 815, 2602, 574, 257, 707, 857, 411, 341], "temperature": 0.0, "avg_logprob": -0.14024127214804463, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.1381312055600574e-06}, {"id": 691, "seek": 337804, "start": 3390.52, "end": 3394.7599999999998, "text": " Right so in other words if we end up at this point", "tokens": [1779, 370, 294, 661, 2283, 498, 321, 917, 493, 412, 341, 935], "temperature": 0.0, "avg_logprob": -0.14024127214804463, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.1381312055600574e-06}, {"id": 692, "seek": 337804, "start": 3395.4, "end": 3400.84, "text": " Right then it's not going to do a good job on this slightly different data set or else if we end up on this point", "tokens": [1779, 550, 309, 311, 406, 516, 281, 360, 257, 665, 1691, 322, 341, 4748, 819, 1412, 992, 420, 1646, 498, 321, 917, 493, 322, 341, 935], "temperature": 0.0, "avg_logprob": -0.14024127214804463, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.1381312055600574e-06}, {"id": 693, "seek": 340084, "start": 3400.84, "end": 3408.2400000000002, "text": " It's still going to do a good job on this data set", "tokens": [467, 311, 920, 516, 281, 360, 257, 665, 1691, 322, 341, 1412, 992], "temperature": 0.0, "avg_logprob": -0.1813014234815325, "compression_ratio": 1.8, "no_speech_prob": 3.3405196973035345e-06}, {"id": 694, "seek": 340084, "start": 3409.7200000000003, "end": 3413.36, "text": " Okay, so that's what cycle mult equals do okay?", "tokens": [1033, 11, 370, 300, 311, 437, 6586, 2120, 6915, 360, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1813014234815325, "compression_ratio": 1.8, "no_speech_prob": 3.3405196973035345e-06}, {"id": 695, "seek": 340084, "start": 3413.36, "end": 3415.36, "text": " So we've got one last thing before we're going to take a break", "tokens": [407, 321, 600, 658, 472, 1036, 551, 949, 321, 434, 516, 281, 747, 257, 1821], "temperature": 0.0, "avg_logprob": -0.1813014234815325, "compression_ratio": 1.8, "no_speech_prob": 3.3405196973035345e-06}, {"id": 696, "seek": 340084, "start": 3415.4, "end": 3420.48, "text": " Which is we're now going to take this model which has 99 and a half percent accuracy", "tokens": [3013, 307, 321, 434, 586, 516, 281, 747, 341, 2316, 597, 575, 11803, 293, 257, 1922, 3043, 14170], "temperature": 0.0, "avg_logprob": -0.1813014234815325, "compression_ratio": 1.8, "no_speech_prob": 3.3405196973035345e-06}, {"id": 697, "seek": 340084, "start": 3420.92, "end": 3426.36, "text": " And we're going to try and make it better still and what we're going to do is we're not actually going to change the model", "tokens": [400, 321, 434, 516, 281, 853, 293, 652, 309, 1101, 920, 293, 437, 321, 434, 516, 281, 360, 307, 321, 434, 406, 767, 516, 281, 1319, 264, 2316], "temperature": 0.0, "avg_logprob": -0.1813014234815325, "compression_ratio": 1.8, "no_speech_prob": 3.3405196973035345e-06}, {"id": 698, "seek": 342636, "start": 3426.36, "end": 3431.4, "text": " at all right, but instead we're going to look back at the", "tokens": [412, 439, 558, 11, 457, 2602, 321, 434, 516, 281, 574, 646, 412, 264], "temperature": 0.0, "avg_logprob": -0.18231037139892578, "compression_ratio": 1.6354166666666667, "no_speech_prob": 5.804986926705169e-07}, {"id": 699, "seek": 342636, "start": 3432.6800000000003, "end": 3436.92, "text": " Original visual visualization we did where we looked at some of our incorrect pictures", "tokens": [30022, 5056, 25801, 321, 630, 689, 321, 2956, 412, 512, 295, 527, 18424, 5242], "temperature": 0.0, "avg_logprob": -0.18231037139892578, "compression_ratio": 1.6354166666666667, "no_speech_prob": 5.804986926705169e-07}, {"id": 700, "seek": 342636, "start": 3440.7200000000003, "end": 3441.96, "text": " Now", "tokens": [823], "temperature": 0.0, "avg_logprob": -0.18231037139892578, "compression_ratio": 1.6354166666666667, "no_speech_prob": 5.804986926705169e-07}, {"id": 701, "seek": 342636, "start": 3441.96, "end": 3447.88, "text": " What I've done is I've printed out the whole of these incorrect pictures, but the key thing to realize is that", "tokens": [708, 286, 600, 1096, 307, 286, 600, 13567, 484, 264, 1379, 295, 613, 18424, 5242, 11, 457, 264, 2141, 551, 281, 4325, 307, 300], "temperature": 0.0, "avg_logprob": -0.18231037139892578, "compression_ratio": 1.6354166666666667, "no_speech_prob": 5.804986926705169e-07}, {"id": 702, "seek": 342636, "start": 3449.88, "end": 3453.96, "text": " Particularly in fact when we do the the validation set", "tokens": [32281, 294, 1186, 562, 321, 360, 264, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.18231037139892578, "compression_ratio": 1.6354166666666667, "no_speech_prob": 5.804986926705169e-07}, {"id": 703, "seek": 345396, "start": 3453.96, "end": 3455.96, "text": " all of our inputs", "tokens": [439, 295, 527, 15743], "temperature": 0.0, "avg_logprob": -0.1903981068095223, "compression_ratio": 1.7266666666666666, "no_speech_prob": 3.966954409406753e-06}, {"id": 704, "seek": 345396, "start": 3456.56, "end": 3461.68, "text": " To our model all the time have to be square right and the reason for that is", "tokens": [1407, 527, 2316, 439, 264, 565, 362, 281, 312, 3732, 558, 293, 264, 1778, 337, 300, 307], "temperature": 0.0, "avg_logprob": -0.1903981068095223, "compression_ratio": 1.7266666666666666, "no_speech_prob": 3.966954409406753e-06}, {"id": 705, "seek": 345396, "start": 3462.68, "end": 3464.56, "text": " That's kind of a minor technical detail", "tokens": [663, 311, 733, 295, 257, 6696, 6191, 2607], "temperature": 0.0, "avg_logprob": -0.1903981068095223, "compression_ratio": 1.7266666666666666, "no_speech_prob": 3.966954409406753e-06}, {"id": 706, "seek": 345396, "start": 3464.56, "end": 3470.56, "text": " But basically the GPU doesn't go very quickly if you have like different dimensions for different images", "tokens": [583, 1936, 264, 18407, 1177, 380, 352, 588, 2661, 498, 291, 362, 411, 819, 12819, 337, 819, 5267], "temperature": 0.0, "avg_logprob": -0.1903981068095223, "compression_ratio": 1.7266666666666666, "no_speech_prob": 3.966954409406753e-06}, {"id": 707, "seek": 345396, "start": 3470.56, "end": 3474.52, "text": " Because it needs things to be consistent so that every part of the GPU can do the same thing", "tokens": [1436, 309, 2203, 721, 281, 312, 8398, 370, 300, 633, 644, 295, 264, 18407, 393, 360, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.1903981068095223, "compression_ratio": 1.7266666666666666, "no_speech_prob": 3.966954409406753e-06}, {"id": 708, "seek": 345396, "start": 3474.52, "end": 3478.26, "text": " All right, and I think this is probably fixable, but it now that's the state of the technology", "tokens": [1057, 558, 11, 293, 286, 519, 341, 307, 1391, 3191, 712, 11, 457, 309, 586, 300, 311, 264, 1785, 295, 264, 2899], "temperature": 0.0, "avg_logprob": -0.1903981068095223, "compression_ratio": 1.7266666666666666, "no_speech_prob": 3.966954409406753e-06}, {"id": 709, "seek": 345396, "start": 3478.26, "end": 3483.32, "text": " We have so our validation set when we actually say for this particular thing is is the dog", "tokens": [492, 362, 370, 527, 24071, 992, 562, 321, 767, 584, 337, 341, 1729, 551, 307, 307, 264, 3000], "temperature": 0.0, "avg_logprob": -0.1903981068095223, "compression_ratio": 1.7266666666666666, "no_speech_prob": 3.966954409406753e-06}, {"id": 710, "seek": 348332, "start": 3483.32, "end": 3486.1600000000003, "text": " What we actually do to make it square is we just pick out?", "tokens": [708, 321, 767, 360, 281, 652, 309, 3732, 307, 321, 445, 1888, 484, 30], "temperature": 0.0, "avg_logprob": -0.14714092319294558, "compression_ratio": 1.7769516728624535, "no_speech_prob": 1.2878933830506867e-06}, {"id": 711, "seek": 348332, "start": 3486.6800000000003, "end": 3491.2400000000002, "text": " The square in the middle right so we would take off its two edges", "tokens": [440, 3732, 294, 264, 2808, 558, 370, 321, 576, 747, 766, 1080, 732, 8819], "temperature": 0.0, "avg_logprob": -0.14714092319294558, "compression_ratio": 1.7769516728624535, "no_speech_prob": 1.2878933830506867e-06}, {"id": 712, "seek": 348332, "start": 3491.2400000000002, "end": 3496.94, "text": " And so we take the whole height and then as much of the middle as we can and so you can see in this case", "tokens": [400, 370, 321, 747, 264, 1379, 6681, 293, 550, 382, 709, 295, 264, 2808, 382, 321, 393, 293, 370, 291, 393, 536, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.14714092319294558, "compression_ratio": 1.7769516728624535, "no_speech_prob": 1.2878933830506867e-06}, {"id": 713, "seek": 348332, "start": 3497.2400000000002, "end": 3499.2400000000002, "text": " We wouldn't actually see this dog's head", "tokens": [492, 2759, 380, 767, 536, 341, 3000, 311, 1378], "temperature": 0.0, "avg_logprob": -0.14714092319294558, "compression_ratio": 1.7769516728624535, "no_speech_prob": 1.2878933830506867e-06}, {"id": 714, "seek": 348332, "start": 3499.88, "end": 3507.88, "text": " All right, so I think the reason this was actually not correctly classified was because the validation set only got to see the body and the body", "tokens": [1057, 558, 11, 370, 286, 519, 264, 1778, 341, 390, 767, 406, 8944, 20627, 390, 570, 264, 24071, 992, 787, 658, 281, 536, 264, 1772, 293, 264, 1772], "temperature": 0.0, "avg_logprob": -0.14714092319294558, "compression_ratio": 1.7769516728624535, "no_speech_prob": 1.2878933830506867e-06}, {"id": 715, "seek": 348332, "start": 3508.8, "end": 3512.0800000000004, "text": " Doesn't look particularly dog like or cat like it's not at all", "tokens": [12955, 380, 574, 4098, 3000, 411, 420, 3857, 411, 309, 311, 406, 412, 439], "temperature": 0.0, "avg_logprob": -0.14714092319294558, "compression_ratio": 1.7769516728624535, "no_speech_prob": 1.2878933830506867e-06}, {"id": 716, "seek": 351208, "start": 3512.08, "end": 3515.64, "text": " Sure, sure what it is so what we're going to do", "tokens": [4894, 11, 988, 437, 309, 307, 370, 437, 321, 434, 516, 281, 360], "temperature": 0.0, "avg_logprob": -0.2144454680767256, "compression_ratio": 1.8080357142857142, "no_speech_prob": 1.6028078562158043e-06}, {"id": 717, "seek": 351208, "start": 3516.4, "end": 3521.98, "text": " When we calculate the predictions for our validation set is we're going to use something called test time augmentation", "tokens": [1133, 321, 8873, 264, 21264, 337, 527, 24071, 992, 307, 321, 434, 516, 281, 764, 746, 1219, 1500, 565, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.2144454680767256, "compression_ratio": 1.8080357142857142, "no_speech_prob": 1.6028078562158043e-06}, {"id": 718, "seek": 351208, "start": 3521.98, "end": 3527.48, "text": " And what this means is that every time we decide is this cat or a dog not in the training?", "tokens": [400, 437, 341, 1355, 307, 300, 633, 565, 321, 4536, 307, 341, 3857, 420, 257, 3000, 406, 294, 264, 3097, 30], "temperature": 0.0, "avg_logprob": -0.2144454680767256, "compression_ratio": 1.8080357142857142, "no_speech_prob": 1.6028078562158043e-06}, {"id": 719, "seek": 351208, "start": 3527.48, "end": 3531.22, "text": " But after we've trained the model is we're going to actually take", "tokens": [583, 934, 321, 600, 8895, 264, 2316, 307, 321, 434, 516, 281, 767, 747], "temperature": 0.0, "avg_logprob": -0.2144454680767256, "compression_ratio": 1.8080357142857142, "no_speech_prob": 1.6028078562158043e-06}, {"id": 720, "seek": 351208, "start": 3534.12, "end": 3535.64, "text": " For", "tokens": [1171], "temperature": 0.0, "avg_logprob": -0.2144454680767256, "compression_ratio": 1.8080357142857142, "no_speech_prob": 1.6028078562158043e-06}, {"id": 721, "seek": 351208, "start": 3535.64, "end": 3540.12, "text": " random data augmentations and remember the data augmentations move around and", "tokens": [4974, 1412, 29919, 763, 293, 1604, 264, 1412, 29919, 763, 1286, 926, 293], "temperature": 0.0, "avg_logprob": -0.2144454680767256, "compression_ratio": 1.8080357142857142, "no_speech_prob": 1.6028078562158043e-06}, {"id": 722, "seek": 354012, "start": 3540.12, "end": 3542.12, "text": " And so you mean and out and flip", "tokens": [400, 370, 291, 914, 293, 484, 293, 7929], "temperature": 0.0, "avg_logprob": -0.2188647420782792, "compression_ratio": 2.0707547169811322, "no_speech_prob": 2.0261334157112287e-06}, {"id": 723, "seek": 354012, "start": 3542.72, "end": 3547.4, "text": " Okay, so we're going to take four of them at random and we're going to take the original", "tokens": [1033, 11, 370, 321, 434, 516, 281, 747, 1451, 295, 552, 412, 4974, 293, 321, 434, 516, 281, 747, 264, 3380], "temperature": 0.0, "avg_logprob": -0.2188647420782792, "compression_ratio": 2.0707547169811322, "no_speech_prob": 2.0261334157112287e-06}, {"id": 724, "seek": 354012, "start": 3548.6, "end": 3550.16, "text": " unaugmented center crop image", "tokens": [517, 20056, 14684, 3056, 9086, 3256], "temperature": 0.0, "avg_logprob": -0.2188647420782792, "compression_ratio": 2.0707547169811322, "no_speech_prob": 2.0261334157112287e-06}, {"id": 725, "seek": 354012, "start": 3550.16, "end": 3556.08, "text": " And we're going to do a prediction for all of those and then we're going to take the average of those predictions", "tokens": [400, 321, 434, 516, 281, 360, 257, 17630, 337, 439, 295, 729, 293, 550, 321, 434, 516, 281, 747, 264, 4274, 295, 729, 21264], "temperature": 0.0, "avg_logprob": -0.2188647420782792, "compression_ratio": 2.0707547169811322, "no_speech_prob": 2.0261334157112287e-06}, {"id": 726, "seek": 354012, "start": 3556.12, "end": 3562.12, "text": " So we're going to say is this a cat is this a cat is this a cat is this a cat right and so hopefully", "tokens": [407, 321, 434, 516, 281, 584, 307, 341, 257, 3857, 307, 341, 257, 3857, 307, 341, 257, 3857, 307, 341, 257, 3857, 558, 293, 370, 4696], "temperature": 0.0, "avg_logprob": -0.2188647420782792, "compression_ratio": 2.0707547169811322, "no_speech_prob": 2.0261334157112287e-06}, {"id": 727, "seek": 354012, "start": 3562.24, "end": 3566.44, "text": " In one of those random ones we actually make sure that the face is there", "tokens": [682, 472, 295, 729, 4974, 2306, 321, 767, 652, 988, 300, 264, 1851, 307, 456], "temperature": 0.0, "avg_logprob": -0.2188647420782792, "compression_ratio": 2.0707547169811322, "no_speech_prob": 2.0261334157112287e-06}, {"id": 728, "seek": 356644, "start": 3566.44, "end": 3572.12, "text": " Zoomed in by a similar amount to other dog spaces at seen it's rotated by the amount that it expects to see it and so", "tokens": [13453, 292, 294, 538, 257, 2531, 2372, 281, 661, 3000, 7673, 412, 1612, 309, 311, 42146, 538, 264, 2372, 300, 309, 33280, 281, 536, 309, 293, 370], "temperature": 0.0, "avg_logprob": -0.22411979190886966, "compression_ratio": 1.7651245551601424, "no_speech_prob": 3.288728521511075e-06}, {"id": 729, "seek": 356644, "start": 3572.12, "end": 3574.12, "text": " forth and so to do that", "tokens": [5220, 293, 370, 281, 360, 300], "temperature": 0.0, "avg_logprob": -0.22411979190886966, "compression_ratio": 1.7651245551601424, "no_speech_prob": 3.288728521511075e-06}, {"id": 730, "seek": 356644, "start": 3575.2000000000003, "end": 3580.68, "text": " All we have to do is just call TTA TTA stands for test time augmentation", "tokens": [1057, 321, 362, 281, 360, 307, 445, 818, 314, 8241, 314, 8241, 7382, 337, 1500, 565, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.22411979190886966, "compression_ratio": 1.7651245551601424, "no_speech_prob": 3.288728521511075e-06}, {"id": 731, "seek": 356644, "start": 3581.2400000000002, "end": 3586.12, "text": " This term of like what are what do we call it when we're making predictions from a model?", "tokens": [639, 1433, 295, 411, 437, 366, 437, 360, 321, 818, 309, 562, 321, 434, 1455, 21264, 490, 257, 2316, 30], "temperature": 0.0, "avg_logprob": -0.22411979190886966, "compression_ratio": 1.7651245551601424, "no_speech_prob": 3.288728521511075e-06}, {"id": 732, "seek": 356644, "start": 3586.12, "end": 3591.16, "text": " We've trained sometimes. It's called inference time sometimes. It's called test time everybody seems have a different name", "tokens": [492, 600, 8895, 2171, 13, 467, 311, 1219, 38253, 565, 2171, 13, 467, 311, 1219, 1500, 565, 2201, 2544, 362, 257, 819, 1315], "temperature": 0.0, "avg_logprob": -0.22411979190886966, "compression_ratio": 1.7651245551601424, "no_speech_prob": 3.288728521511075e-06}, {"id": 733, "seek": 356644, "start": 3591.16, "end": 3595.76, "text": " So TTA and so when we do that we go learn dot TTA check the accuracy", "tokens": [407, 314, 8241, 293, 370, 562, 321, 360, 300, 321, 352, 1466, 5893, 314, 8241, 1520, 264, 14170], "temperature": 0.0, "avg_logprob": -0.22411979190886966, "compression_ratio": 1.7651245551601424, "no_speech_prob": 3.288728521511075e-06}, {"id": 734, "seek": 359576, "start": 3595.76, "end": 3600.92, "text": " And lo and behold we're now at 99 point six five percent, which is kind of crazy", "tokens": [400, 450, 293, 27234, 321, 434, 586, 412, 11803, 935, 2309, 1732, 3043, 11, 597, 307, 733, 295, 3219], "temperature": 0.0, "avg_logprob": -0.23027630595417767, "compression_ratio": 1.4479166666666667, "no_speech_prob": 1.6701191270840354e-05}, {"id": 735, "seek": 359576, "start": 3601.44, "end": 3603.44, "text": " Where's our green box?", "tokens": [2305, 311, 527, 3092, 2424, 30], "temperature": 0.0, "avg_logprob": -0.23027630595417767, "compression_ratio": 1.4479166666666667, "no_speech_prob": 1.6701191270840354e-05}, {"id": 736, "seek": 359576, "start": 3605.1200000000003, "end": 3607.6400000000003, "text": " But for every part we are only", "tokens": [583, 337, 633, 644, 321, 366, 787], "temperature": 0.0, "avg_logprob": -0.23027630595417767, "compression_ratio": 1.4479166666666667, "no_speech_prob": 1.6701191270840354e-05}, {"id": 737, "seek": 359576, "start": 3609.0, "end": 3615.82, "text": " Showing one type of augmentation of a particular image right so when we are training back here", "tokens": [6895, 278, 472, 2010, 295, 14501, 19631, 295, 257, 1729, 3256, 558, 370, 562, 321, 366, 3097, 646, 510], "temperature": 0.0, "avg_logprob": -0.23027630595417767, "compression_ratio": 1.4479166666666667, "no_speech_prob": 1.6701191270840354e-05}, {"id": 738, "seek": 359576, "start": 3615.82, "end": 3619.32, "text": " We're not doing any TTA right so TTA is not like", "tokens": [492, 434, 406, 884, 604, 314, 8241, 558, 370, 314, 8241, 307, 406, 411], "temperature": 0.0, "avg_logprob": -0.23027630595417767, "compression_ratio": 1.4479166666666667, "no_speech_prob": 1.6701191270840354e-05}, {"id": 739, "seek": 361932, "start": 3619.32, "end": 3626.42, "text": " You could and sometimes like I've written libraries where after each epoch I run TTA to see how well it's going", "tokens": [509, 727, 293, 2171, 411, 286, 600, 3720, 15148, 689, 934, 1184, 30992, 339, 286, 1190, 314, 8241, 281, 536, 577, 731, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.1721577289675878, "compression_ratio": 1.7724137931034483, "no_speech_prob": 2.4439812023047125e-06}, {"id": 740, "seek": 361932, "start": 3626.42, "end": 3628.7200000000003, "text": " But that's not what's happening here. I trained the whole thing", "tokens": [583, 300, 311, 406, 437, 311, 2737, 510, 13, 286, 8895, 264, 1379, 551], "temperature": 0.0, "avg_logprob": -0.1721577289675878, "compression_ratio": 1.7724137931034483, "no_speech_prob": 2.4439812023047125e-06}, {"id": 741, "seek": 361932, "start": 3629.44, "end": 3630.52, "text": " with", "tokens": [365], "temperature": 0.0, "avg_logprob": -0.1721577289675878, "compression_ratio": 1.7724137931034483, "no_speech_prob": 2.4439812023047125e-06}, {"id": 742, "seek": 361932, "start": 3630.52, "end": 3635.84, "text": " Training time augmentation which doesn't have a special name because that's what we mean when we say data augmentation", "tokens": [20620, 565, 14501, 19631, 597, 1177, 380, 362, 257, 2121, 1315, 570, 300, 311, 437, 321, 914, 562, 321, 584, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.1721577289675878, "compression_ratio": 1.7724137931034483, "no_speech_prob": 2.4439812023047125e-06}, {"id": 743, "seek": 361932, "start": 3635.84, "end": 3639.4, "text": " We need training time augmentation so here every time we showed it a picture", "tokens": [492, 643, 3097, 565, 14501, 19631, 370, 510, 633, 565, 321, 4712, 309, 257, 3036], "temperature": 0.0, "avg_logprob": -0.1721577289675878, "compression_ratio": 1.7724137931034483, "no_speech_prob": 2.4439812023047125e-06}, {"id": 744, "seek": 361932, "start": 3639.88, "end": 3644.28, "text": " We were randomly changing it a little bit so each epoch each of these seven epochs", "tokens": [492, 645, 16979, 4473, 309, 257, 707, 857, 370, 1184, 30992, 339, 1184, 295, 613, 3407, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.1721577289675878, "compression_ratio": 1.7724137931034483, "no_speech_prob": 2.4439812023047125e-06}, {"id": 745, "seek": 361932, "start": 3644.28, "end": 3646.44, "text": " It was seen slightly different versions of the picture", "tokens": [467, 390, 1612, 4748, 819, 9606, 295, 264, 3036], "temperature": 0.0, "avg_logprob": -0.1721577289675878, "compression_ratio": 1.7724137931034483, "no_speech_prob": 2.4439812023047125e-06}, {"id": 746, "seek": 364644, "start": 3646.44, "end": 3651.36, "text": " Having done that we now have a fully trained model. We then said okay", "tokens": [10222, 1096, 300, 321, 586, 362, 257, 4498, 8895, 2316, 13, 492, 550, 848, 1392], "temperature": 0.0, "avg_logprob": -0.1919805476095824, "compression_ratio": 1.7481751824817517, "no_speech_prob": 7.766818271193188e-06}, {"id": 747, "seek": 364644, "start": 3651.36, "end": 3656.2400000000002, "text": " Let's look at the validation set so TTA by default uses the validation set and said okay", "tokens": [961, 311, 574, 412, 264, 24071, 992, 370, 314, 8241, 538, 7576, 4960, 264, 24071, 992, 293, 848, 1392], "temperature": 0.0, "avg_logprob": -0.1919805476095824, "compression_ratio": 1.7481751824817517, "no_speech_prob": 7.766818271193188e-06}, {"id": 748, "seek": 364644, "start": 3656.2400000000002, "end": 3660.28, "text": " What are your predictions of which ones are cats and which ones are dogs and it did four?", "tokens": [708, 366, 428, 21264, 295, 597, 2306, 366, 11111, 293, 597, 2306, 366, 7197, 293, 309, 630, 1451, 30], "temperature": 0.0, "avg_logprob": -0.1919805476095824, "compression_ratio": 1.7481751824817517, "no_speech_prob": 7.766818271193188e-06}, {"id": 749, "seek": 364644, "start": 3661.0, "end": 3664.36, "text": " Predictions with different random augmentations plus one on the organ", "tokens": [32969, 15607, 365, 819, 4974, 29919, 763, 1804, 472, 322, 264, 1798], "temperature": 0.0, "avg_logprob": -0.1919805476095824, "compression_ratio": 1.7481751824817517, "no_speech_prob": 7.766818271193188e-06}, {"id": 750, "seek": 364644, "start": 3664.6, "end": 3670.04, "text": " Unauthorized version average them all together, and that's what we got and that's what we capture the accuracy problem", "tokens": [15491, 325, 2335, 1602, 3037, 4274, 552, 439, 1214, 11, 293, 300, 311, 437, 321, 658, 293, 300, 311, 437, 321, 7983, 264, 14170, 1154], "temperature": 0.0, "avg_logprob": -0.1919805476095824, "compression_ratio": 1.7481751824817517, "no_speech_prob": 7.766818271193188e-06}, {"id": 751, "seek": 364644, "start": 3670.04, "end": 3672.04, "text": " So is there a high probability of having?", "tokens": [407, 307, 456, 257, 1090, 8482, 295, 1419, 30], "temperature": 0.0, "avg_logprob": -0.1919805476095824, "compression_ratio": 1.7481751824817517, "no_speech_prob": 7.766818271193188e-06}, {"id": 752, "seek": 367204, "start": 3672.04, "end": 3676.2, "text": " a sample in TTA that was not shown in during training", "tokens": [257, 6889, 294, 314, 8241, 300, 390, 406, 4898, 294, 1830, 3097], "temperature": 0.0, "avg_logprob": -0.27739368072927817, "compression_ratio": 1.4827586206896552, "no_speech_prob": 9.665900506661274e-06}, {"id": 753, "seek": 367204, "start": 3677.6, "end": 3686.48, "text": " Yeah, actually every data augmented for image is is unique because the rotation could be like point zero three four degrees and", "tokens": [865, 11, 767, 633, 1412, 36155, 337, 3256, 307, 307, 3845, 570, 264, 12447, 727, 312, 411, 935, 4018, 1045, 1451, 5310, 293], "temperature": 0.0, "avg_logprob": -0.27739368072927817, "compression_ratio": 1.4827586206896552, "no_speech_prob": 9.665900506661274e-06}, {"id": 754, "seek": 367204, "start": 3686.92, "end": 3691.22, "text": " Zoom could be one point zero one six five so every time it's slightly different", "tokens": [13453, 727, 312, 472, 935, 4018, 472, 2309, 1732, 370, 633, 565, 309, 311, 4748, 819], "temperature": 0.0, "avg_logprob": -0.27739368072927817, "compression_ratio": 1.4827586206896552, "no_speech_prob": 9.665900506661274e-06}, {"id": 755, "seek": 367204, "start": 3691.96, "end": 3693.96, "text": " No problem. It's behind you", "tokens": [883, 1154, 13, 467, 311, 2261, 291], "temperature": 0.0, "avg_logprob": -0.27739368072927817, "compression_ratio": 1.4827586206896552, "no_speech_prob": 9.665900506661274e-06}, {"id": 756, "seek": 367204, "start": 3696.86, "end": 3698.44, "text": " What's your", "tokens": [708, 311, 428], "temperature": 0.0, "avg_logprob": -0.27739368072927817, "compression_ratio": 1.4827586206896552, "no_speech_prob": 9.665900506661274e-06}, {"id": 757, "seek": 369844, "start": 3698.44, "end": 3704.36, "text": " Why not use white padding or something like that this one of your white padding like this?", "tokens": [1545, 406, 764, 2418, 39562, 420, 746, 411, 300, 341, 472, 295, 428, 2418, 39562, 411, 341, 30], "temperature": 0.0, "avg_logprob": -0.281213713086341, "compression_ratio": 1.8609022556390977, "no_speech_prob": 7.071842446748633e-06}, {"id": 758, "seek": 369844, "start": 3704.36, "end": 3707.58, "text": " You know put like a white border around me. Oh padding is not", "tokens": [509, 458, 829, 411, 257, 2418, 7838, 926, 385, 13, 876, 39562, 307, 406], "temperature": 0.0, "avg_logprob": -0.281213713086341, "compression_ratio": 1.8609022556390977, "no_speech_prob": 7.071842446748633e-06}, {"id": 759, "seek": 369844, "start": 3707.84, "end": 3709.84, "text": " Yeah, so like there's lots of different types of", "tokens": [865, 11, 370, 411, 456, 311, 3195, 295, 819, 3467, 295], "temperature": 0.0, "avg_logprob": -0.281213713086341, "compression_ratio": 1.8609022556390977, "no_speech_prob": 7.071842446748633e-06}, {"id": 760, "seek": 369844, "start": 3710.32, "end": 3714.84, "text": " Augmentation you can do and so one of the things you can do is to add a border around it", "tokens": [6088, 19631, 291, 393, 360, 293, 370, 472, 295, 264, 721, 291, 393, 360, 307, 281, 909, 257, 7838, 926, 309], "temperature": 0.0, "avg_logprob": -0.281213713086341, "compression_ratio": 1.8609022556390977, "no_speech_prob": 7.071842446748633e-06}, {"id": 761, "seek": 369844, "start": 3715.96, "end": 3721.7200000000003, "text": " Basically adding a border around it in my experiments doesn't doesn't help it doesn't make it any less cat-like", "tokens": [8537, 5127, 257, 7838, 926, 309, 294, 452, 12050, 1177, 380, 1177, 380, 854, 309, 1177, 380, 652, 309, 604, 1570, 3857, 12, 4092], "temperature": 0.0, "avg_logprob": -0.281213713086341, "compression_ratio": 1.8609022556390977, "no_speech_prob": 7.071842446748633e-06}, {"id": 762, "seek": 369844, "start": 3721.8, "end": 3725.28, "text": " It's not the convolutional neural network doesn't seem to find it very interesting", "tokens": [467, 311, 406, 264, 45216, 304, 18161, 3209, 1177, 380, 1643, 281, 915, 309, 588, 1880], "temperature": 0.0, "avg_logprob": -0.281213713086341, "compression_ratio": 1.8609022556390977, "no_speech_prob": 7.071842446748633e-06}, {"id": 763, "seek": 369844, "start": 3725.7200000000003, "end": 3726.92, "text": " basically", "tokens": [1936], "temperature": 0.0, "avg_logprob": -0.281213713086341, "compression_ratio": 1.8609022556390977, "no_speech_prob": 7.071842446748633e-06}, {"id": 764, "seek": 372692, "start": 3726.92, "end": 3732.6800000000003, "text": " Something that I do do we'll see later is I do something called reflection padding which is where I add some borders that are the", "tokens": [6595, 300, 286, 360, 360, 321, 603, 536, 1780, 307, 286, 360, 746, 1219, 12914, 39562, 597, 307, 689, 286, 909, 512, 16287, 300, 366, 264], "temperature": 0.0, "avg_logprob": -0.15544988397966353, "compression_ratio": 1.6920289855072463, "no_speech_prob": 6.747983661625767e-06}, {"id": 765, "seek": 372692, "start": 3732.6800000000003, "end": 3739.2400000000002, "text": " Outside just reflected. It's a way to kind of make some bigger images works well with satellite imagery in particular", "tokens": [28218, 445, 15502, 13, 467, 311, 257, 636, 281, 733, 295, 652, 512, 3801, 5267, 1985, 731, 365, 16016, 24340, 294, 1729], "temperature": 0.0, "avg_logprob": -0.15544988397966353, "compression_ratio": 1.6920289855072463, "no_speech_prob": 6.747983661625767e-06}, {"id": 766, "seek": 372692, "start": 3739.92, "end": 3744.88, "text": " But yeah in general. I don't do I have a lot of padding instead. I do a bit of zooming", "tokens": [583, 1338, 294, 2674, 13, 286, 500, 380, 360, 286, 362, 257, 688, 295, 39562, 2602, 13, 286, 360, 257, 857, 295, 48226], "temperature": 0.0, "avg_logprob": -0.15544988397966353, "compression_ratio": 1.6920289855072463, "no_speech_prob": 6.747983661625767e-06}, {"id": 767, "seek": 372692, "start": 3748.4, "end": 3750.4, "text": " It's kind of follow-up to that last one, but", "tokens": [467, 311, 733, 295, 1524, 12, 1010, 281, 300, 1036, 472, 11, 457], "temperature": 0.0, "avg_logprob": -0.15544988397966353, "compression_ratio": 1.6920289855072463, "no_speech_prob": 6.747983661625767e-06}, {"id": 768, "seek": 372692, "start": 3750.92, "end": 3756.16, "text": " Rather than cropping just add white space because when you crop you lose the dog's face", "tokens": [16571, 813, 4848, 3759, 445, 909, 2418, 1901, 570, 562, 291, 9086, 291, 3624, 264, 3000, 311, 1851], "temperature": 0.0, "avg_logprob": -0.15544988397966353, "compression_ratio": 1.6920289855072463, "no_speech_prob": 6.747983661625767e-06}, {"id": 769, "seek": 375616, "start": 3756.16, "end": 3759.2, "text": " But if you added white space you wouldn't have yeah", "tokens": [583, 498, 291, 3869, 2418, 1901, 291, 2759, 380, 362, 1338], "temperature": 0.0, "avg_logprob": -0.24963430924849075, "compression_ratio": 1.558974358974359, "no_speech_prob": 4.356825684226351e-06}, {"id": 770, "seek": 375616, "start": 3759.2, "end": 3765.8599999999997, "text": " So that's that's where the the kind of the reflection padding or the zooming or whatever can help so there are ways in the fast", "tokens": [407, 300, 311, 300, 311, 689, 264, 264, 733, 295, 264, 12914, 39562, 420, 264, 48226, 420, 2035, 393, 854, 370, 456, 366, 2098, 294, 264, 2370], "temperature": 0.0, "avg_logprob": -0.24963430924849075, "compression_ratio": 1.558974358974359, "no_speech_prob": 4.356825684226351e-06}, {"id": 771, "seek": 375616, "start": 3765.8599999999997, "end": 3768.08, "text": " AI library when you do custom transforms of", "tokens": [7318, 6405, 562, 291, 360, 2375, 35592, 295], "temperature": 0.0, "avg_logprob": -0.24963430924849075, "compression_ratio": 1.558974358974359, "no_speech_prob": 4.356825684226351e-06}, {"id": 772, "seek": 375616, "start": 3768.96, "end": 3770.96, "text": " making that happen I", "tokens": [1455, 300, 1051, 286], "temperature": 0.0, "avg_logprob": -0.24963430924849075, "compression_ratio": 1.558974358974359, "no_speech_prob": 4.356825684226351e-06}, {"id": 773, "seek": 375616, "start": 3773.12, "end": 3775.12, "text": " Find that", "tokens": [11809, 300], "temperature": 0.0, "avg_logprob": -0.24963430924849075, "compression_ratio": 1.558974358974359, "no_speech_prob": 4.356825684226351e-06}, {"id": 774, "seek": 375616, "start": 3778.12, "end": 3781.22, "text": " It kind of depends on the image size you know but", "tokens": [467, 733, 295, 5946, 322, 264, 3256, 2744, 291, 458, 457], "temperature": 0.0, "avg_logprob": -0.24963430924849075, "compression_ratio": 1.558974358974359, "no_speech_prob": 4.356825684226351e-06}, {"id": 775, "seek": 378122, "start": 3781.22, "end": 3785.7799999999997, "text": " Generally speaking it seems that using TTA plus data augmentation", "tokens": [21082, 4124, 309, 2544, 300, 1228, 314, 8241, 1804, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.17747276370264903, "compression_ratio": 1.701067615658363, "no_speech_prob": 5.0936373554577585e-06}, {"id": 776, "seek": 378122, "start": 3786.5, "end": 3789.74, "text": " The best thing to do is to try to use as large image as possible", "tokens": [440, 1151, 551, 281, 360, 307, 281, 853, 281, 764, 382, 2416, 3256, 382, 1944], "temperature": 0.0, "avg_logprob": -0.17747276370264903, "compression_ratio": 1.701067615658363, "no_speech_prob": 5.0936373554577585e-06}, {"id": 777, "seek": 378122, "start": 3789.74, "end": 3792.9399999999996, "text": " And so if you kind of crop the thing down and put white borders on top and bottom", "tokens": [400, 370, 498, 291, 733, 295, 9086, 264, 551, 760, 293, 829, 2418, 16287, 322, 1192, 293, 2767], "temperature": 0.0, "avg_logprob": -0.17747276370264903, "compression_ratio": 1.701067615658363, "no_speech_prob": 5.0936373554577585e-06}, {"id": 778, "seek": 378122, "start": 3793.1, "end": 3799.2, "text": " It's now quite a lot smaller and so to make it as big as it was before you now have to use more GPU", "tokens": [467, 311, 586, 1596, 257, 688, 4356, 293, 370, 281, 652, 309, 382, 955, 382, 309, 390, 949, 291, 586, 362, 281, 764, 544, 18407], "temperature": 0.0, "avg_logprob": -0.17747276370264903, "compression_ratio": 1.701067615658363, "no_speech_prob": 5.0936373554577585e-06}, {"id": 779, "seek": 378122, "start": 3799.2, "end": 3802.7599999999998, "text": " And if you're going to use all that more GPU you could have zoomed in and used a bigger image so", "tokens": [400, 498, 291, 434, 516, 281, 764, 439, 300, 544, 18407, 291, 727, 362, 8863, 292, 294, 293, 1143, 257, 3801, 3256, 370], "temperature": 0.0, "avg_logprob": -0.17747276370264903, "compression_ratio": 1.701067615658363, "no_speech_prob": 5.0936373554577585e-06}, {"id": 780, "seek": 380276, "start": 3802.76, "end": 3811.1600000000003, "text": " In my playing around that doesn't seem to be generally as successful. Okay?", "tokens": [682, 452, 2433, 926, 300, 1177, 380, 1643, 281, 312, 5101, 382, 4406, 13, 1033, 30], "temperature": 0.0, "avg_logprob": -0.35599721064333056, "compression_ratio": 1.4259259259259258, "no_speech_prob": 1.3006730114284437e-05}, {"id": 781, "seek": 380276, "start": 3815.48, "end": 3821.28, "text": " There is a lot of interest on the topic of how to do the documentation in older than images", "tokens": [821, 307, 257, 688, 295, 1179, 322, 264, 4829, 295, 577, 281, 360, 264, 14333, 294, 4906, 813, 5267], "temperature": 0.0, "avg_logprob": -0.35599721064333056, "compression_ratio": 1.4259259259259258, "no_speech_prob": 1.3006730114284437e-05}, {"id": 782, "seek": 380276, "start": 3821.92, "end": 3823.92, "text": " In data that is not images", "tokens": [682, 1412, 300, 307, 406, 5267], "temperature": 0.0, "avg_logprob": -0.35599721064333056, "compression_ratio": 1.4259259259259258, "no_speech_prob": 1.3006730114284437e-05}, {"id": 783, "seek": 380276, "start": 3824.7200000000003, "end": 3826.7200000000003, "text": " Yeah", "tokens": [865], "temperature": 0.0, "avg_logprob": -0.35599721064333056, "compression_ratio": 1.4259259259259258, "no_speech_prob": 1.3006730114284437e-05}, {"id": 784, "seek": 380276, "start": 3827.44, "end": 3829.44, "text": " No one seems to know I actually", "tokens": [883, 472, 2544, 281, 458, 286, 767], "temperature": 0.0, "avg_logprob": -0.35599721064333056, "compression_ratio": 1.4259259259259258, "no_speech_prob": 1.3006730114284437e-05}, {"id": 785, "seek": 382944, "start": 3829.44, "end": 3831.44, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.22318869370680589, "compression_ratio": 1.6892857142857143, "no_speech_prob": 6.4389278122689575e-06}, {"id": 786, "seek": 382944, "start": 3832.56, "end": 3838.4, "text": " Asked some of my friends in the natural language processing community about this and we'll get to natural language processing in a couple of lessons", "tokens": [12320, 292, 512, 295, 452, 1855, 294, 264, 3303, 2856, 9007, 1768, 466, 341, 293, 321, 603, 483, 281, 3303, 2856, 9007, 294, 257, 1916, 295, 8820], "temperature": 0.0, "avg_logprob": -0.22318869370680589, "compression_ratio": 1.6892857142857143, "no_speech_prob": 6.4389278122689575e-06}, {"id": 787, "seek": 382944, "start": 3839.04, "end": 3841.7200000000003, "text": " You know it seems like it'd be really helpful. There's been a few", "tokens": [509, 458, 309, 2544, 411, 309, 1116, 312, 534, 4961, 13, 821, 311, 668, 257, 1326], "temperature": 0.0, "avg_logprob": -0.22318869370680589, "compression_ratio": 1.6892857142857143, "no_speech_prob": 6.4389278122689575e-06}, {"id": 788, "seek": 382944, "start": 3842.48, "end": 3848.52, "text": " Example like a very very few number of examples of people where papers would like try replacing synonyms for instance", "tokens": [24755, 781, 411, 257, 588, 588, 1326, 1230, 295, 5110, 295, 561, 689, 10577, 576, 411, 853, 19139, 5451, 2526, 2592, 337, 5197], "temperature": 0.0, "avg_logprob": -0.22318869370680589, "compression_ratio": 1.6892857142857143, "no_speech_prob": 6.4389278122689575e-06}, {"id": 789, "seek": 382944, "start": 3848.52, "end": 3854.12, "text": " but on the whole an understanding of like appropriate data augmentation for non image domains is", "tokens": [457, 322, 264, 1379, 364, 3701, 295, 411, 6854, 1412, 14501, 19631, 337, 2107, 3256, 25514, 307], "temperature": 0.0, "avg_logprob": -0.22318869370680589, "compression_ratio": 1.6892857142857143, "no_speech_prob": 6.4389278122689575e-06}, {"id": 790, "seek": 385412, "start": 3854.12, "end": 3858.44, "text": " Under researched and under underdeveloped", "tokens": [6974, 37098, 293, 833, 833, 35464, 292], "temperature": 0.0, "avg_logprob": -0.2790247235979353, "compression_ratio": 1.5675675675675675, "no_speech_prob": 2.6015875391749432e-06}, {"id": 791, "seek": 385412, "start": 3863.48, "end": 3871.44, "text": " The question was could couldn't we just use a sliding window to generate all the images so in that dog picture couldn't we generate three?", "tokens": [440, 1168, 390, 727, 2809, 380, 321, 445, 764, 257, 21169, 4910, 281, 8460, 439, 264, 5267, 370, 294, 300, 3000, 3036, 2809, 380, 321, 8460, 1045, 30], "temperature": 0.0, "avg_logprob": -0.2790247235979353, "compression_ratio": 1.5675675675675675, "no_speech_prob": 2.6015875391749432e-06}, {"id": 792, "seek": 385412, "start": 3872.48, "end": 3875.8399999999997, "text": " Parts of that wouldn't that be better yeah for PTA you mean", "tokens": [4100, 82, 295, 300, 2759, 380, 300, 312, 1101, 1338, 337, 430, 8241, 291, 914], "temperature": 0.0, "avg_logprob": -0.2790247235979353, "compression_ratio": 1.5675675675675675, "no_speech_prob": 2.6015875391749432e-06}, {"id": 793, "seek": 385412, "start": 3876.52, "end": 3878.94, "text": " Just just in general when you're creating your so", "tokens": [1449, 445, 294, 2674, 562, 291, 434, 4084, 428, 370], "temperature": 0.0, "avg_logprob": -0.2790247235979353, "compression_ratio": 1.5675675675675675, "no_speech_prob": 2.6015875391749432e-06}, {"id": 794, "seek": 387894, "start": 3878.94, "end": 3884.66, "text": " For training time I would say no that wouldn't be better because we're not going to get as much variation", "tokens": [1171, 3097, 565, 286, 576, 584, 572, 300, 2759, 380, 312, 1101, 570, 321, 434, 406, 516, 281, 483, 382, 709, 12990], "temperature": 0.0, "avg_logprob": -0.1665265401204427, "compression_ratio": 1.7007575757575757, "no_speech_prob": 7.88913712312933e-06}, {"id": 795, "seek": 387894, "start": 3884.86, "end": 3888.54, "text": " You know we want to have it like like one degree off five", "tokens": [509, 458, 321, 528, 281, 362, 309, 411, 411, 472, 4314, 766, 1732], "temperature": 0.0, "avg_logprob": -0.1665265401204427, "compression_ratio": 1.7007575757575757, "no_speech_prob": 7.88913712312933e-06}, {"id": 796, "seek": 387894, "start": 3888.54, "end": 3894.5, "text": " You know five degrees off ten pixels up like lots of slightly different versions, and so if you just had three standard", "tokens": [509, 458, 1732, 5310, 766, 2064, 18668, 493, 411, 3195, 295, 4748, 819, 9606, 11, 293, 370, 498, 291, 445, 632, 1045, 3832], "temperature": 0.0, "avg_logprob": -0.1665265401204427, "compression_ratio": 1.7007575757575757, "no_speech_prob": 7.88913712312933e-06}, {"id": 797, "seek": 387894, "start": 3894.9, "end": 3898.94, "text": " Ways then you're not giving it as many different ways of looking at the data", "tokens": [343, 3772, 550, 291, 434, 406, 2902, 309, 382, 867, 819, 2098, 295, 1237, 412, 264, 1412], "temperature": 0.0, "avg_logprob": -0.1665265401204427, "compression_ratio": 1.7007575757575757, "no_speech_prob": 7.88913712312933e-06}, {"id": 798, "seek": 387894, "start": 3899.78, "end": 3901.78, "text": " for test time augmentation", "tokens": [337, 1500, 565, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.1665265401204427, "compression_ratio": 1.7007575757575757, "no_speech_prob": 7.88913712312933e-06}, {"id": 799, "seek": 387894, "start": 3902.2200000000003, "end": 3907.26, "text": " Having fixed crop locations. I think probably would be better", "tokens": [10222, 6806, 9086, 9253, 13, 286, 519, 1391, 576, 312, 1101], "temperature": 0.0, "avg_logprob": -0.1665265401204427, "compression_ratio": 1.7007575757575757, "no_speech_prob": 7.88913712312933e-06}, {"id": 800, "seek": 390726, "start": 3907.26, "end": 3912.94, "text": " And I just haven't gotten around to writing that yet. I have a version in an old library", "tokens": [400, 286, 445, 2378, 380, 5768, 926, 281, 3579, 300, 1939, 13, 286, 362, 257, 3037, 294, 364, 1331, 6405], "temperature": 0.0, "avg_logprob": -0.158311094556536, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.2805294318241067e-05}, {"id": 801, "seek": 390726, "start": 3912.94, "end": 3915.0600000000004, "text": " I think having fixed crop locations", "tokens": [286, 519, 1419, 6806, 9086, 9253], "temperature": 0.0, "avg_logprob": -0.158311094556536, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.2805294318241067e-05}, {"id": 802, "seek": 390726, "start": 3915.7400000000002, "end": 3917.0200000000004, "text": " plus", "tokens": [1804], "temperature": 0.0, "avg_logprob": -0.158311094556536, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.2805294318241067e-05}, {"id": 803, "seek": 390726, "start": 3917.0200000000004, "end": 3918.7000000000003, "text": " random", "tokens": [4974], "temperature": 0.0, "avg_logprob": -0.158311094556536, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.2805294318241067e-05}, {"id": 804, "seek": 390726, "start": 3918.7000000000003, "end": 3921.5400000000004, "text": " Contrast brightness rotation changes might be better", "tokens": [4839, 4148, 21367, 12447, 2962, 1062, 312, 1101], "temperature": 0.0, "avg_logprob": -0.158311094556536, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.2805294318241067e-05}, {"id": 805, "seek": 390726, "start": 3923.6200000000003, "end": 3928.96, "text": " The reason I haven't got around to it yet is because in my testing it didn't seem to help in practice very much", "tokens": [440, 1778, 286, 2378, 380, 658, 926, 281, 309, 1939, 307, 570, 294, 452, 4997, 309, 994, 380, 1643, 281, 854, 294, 3124, 588, 709], "temperature": 0.0, "avg_logprob": -0.158311094556536, "compression_ratio": 1.6129032258064515, "no_speech_prob": 1.2805294318241067e-05}, {"id": 806, "seek": 392896, "start": 3928.96, "end": 3937.64, "text": " And it made the code a lot more complicated, so you know it's kind of it's an interesting question", "tokens": [400, 309, 1027, 264, 3089, 257, 688, 544, 6179, 11, 370, 291, 458, 309, 311, 733, 295, 309, 311, 364, 1880, 1168], "temperature": 0.0, "avg_logprob": -0.2384332563818955, "compression_ratio": 1.7135416666666667, "no_speech_prob": 2.586681512184441e-05}, {"id": 807, "seek": 392896, "start": 3943.42, "end": 3949.86, "text": " Yeah, that's a great question so the faster. I library is open source, and let's talk about it a bit more generally because", "tokens": [865, 11, 300, 311, 257, 869, 1168, 370, 264, 4663, 13, 286, 6405, 307, 1269, 4009, 11, 293, 718, 311, 751, 466, 309, 257, 857, 544, 5101, 570], "temperature": 0.0, "avg_logprob": -0.2384332563818955, "compression_ratio": 1.7135416666666667, "no_speech_prob": 2.586681512184441e-05}, {"id": 808, "seek": 392896, "start": 3951.7, "end": 3953.7, "text": " You know it's like the fact that", "tokens": [509, 458, 309, 311, 411, 264, 1186, 300], "temperature": 0.0, "avg_logprob": -0.2384332563818955, "compression_ratio": 1.7135416666666667, "no_speech_prob": 2.586681512184441e-05}, {"id": 809, "seek": 392896, "start": 3954.1, "end": 3957.06, "text": " The fact that we're using this library is kind of interesting and unusual", "tokens": [440, 1186, 300, 321, 434, 1228, 341, 6405, 307, 733, 295, 1880, 293, 10901], "temperature": 0.0, "avg_logprob": -0.2384332563818955, "compression_ratio": 1.7135416666666667, "no_speech_prob": 2.586681512184441e-05}, {"id": 810, "seek": 395706, "start": 3957.06, "end": 3961.7, "text": " and it sits on top of something called pie torch, right so", "tokens": [293, 309, 12696, 322, 1192, 295, 746, 1219, 1730, 27822, 11, 558, 370], "temperature": 0.0, "avg_logprob": -0.24470117568969726, "compression_ratio": 1.6180257510729614, "no_speech_prob": 2.521552687539952e-06}, {"id": 811, "seek": 395706, "start": 3962.94, "end": 3964.94, "text": " pie torch is a", "tokens": [1730, 27822, 307, 257], "temperature": 0.0, "avg_logprob": -0.24470117568969726, "compression_ratio": 1.6180257510729614, "no_speech_prob": 2.521552687539952e-06}, {"id": 812, "seek": 395706, "start": 3967.18, "end": 3970.94, "text": " Fairly recent development, and it's kind of I've noticed all the", "tokens": [12157, 356, 5162, 3250, 11, 293, 309, 311, 733, 295, 286, 600, 5694, 439, 264], "temperature": 0.0, "avg_logprob": -0.24470117568969726, "compression_ratio": 1.6180257510729614, "no_speech_prob": 2.521552687539952e-06}, {"id": 813, "seek": 395706, "start": 3971.58, "end": 3974.86, "text": " Researchers that I respect pretty much are now using high torch", "tokens": [43555, 300, 286, 3104, 1238, 709, 366, 586, 1228, 1090, 27822], "temperature": 0.0, "avg_logprob": -0.24470117568969726, "compression_ratio": 1.6180257510729614, "no_speech_prob": 2.521552687539952e-06}, {"id": 814, "seek": 395706, "start": 3974.86, "end": 3979.86, "text": " I found in part two of last year's course that a lot of the cutting-edge stuff", "tokens": [286, 1352, 294, 644, 732, 295, 1036, 1064, 311, 1164, 300, 257, 688, 295, 264, 6492, 12, 12203, 1507], "temperature": 0.0, "avg_logprob": -0.24470117568969726, "compression_ratio": 1.6180257510729614, "no_speech_prob": 2.521552687539952e-06}, {"id": 815, "seek": 395706, "start": 3979.86, "end": 3985.2999999999997, "text": " I wanted to teach I couldn't do it in keras and tensorflow, which is what we used to teach with", "tokens": [286, 1415, 281, 2924, 286, 2809, 380, 360, 309, 294, 350, 6985, 293, 40863, 10565, 11, 597, 307, 437, 321, 1143, 281, 2924, 365], "temperature": 0.0, "avg_logprob": -0.24470117568969726, "compression_ratio": 1.6180257510729614, "no_speech_prob": 2.521552687539952e-06}, {"id": 816, "seek": 398530, "start": 3985.3, "end": 3992.78, "text": " And so I had to switch the course to pie torch halfway through part two the problem was that", "tokens": [400, 370, 286, 632, 281, 3679, 264, 1164, 281, 1730, 27822, 15461, 807, 644, 732, 264, 1154, 390, 300], "temperature": 0.0, "avg_logprob": -0.19402250253929282, "compression_ratio": 1.8023715415019763, "no_speech_prob": 7.527824891440105e-06}, {"id": 817, "seek": 398530, "start": 3993.6600000000003, "end": 3998.38, "text": " Pie torch isn't very easy to use you have to write your own training loop from scratch", "tokens": [22914, 27822, 1943, 380, 588, 1858, 281, 764, 291, 362, 281, 2464, 428, 1065, 3097, 6367, 490, 8459], "temperature": 0.0, "avg_logprob": -0.19402250253929282, "compression_ratio": 1.8023715415019763, "no_speech_prob": 7.527824891440105e-06}, {"id": 818, "seek": 398530, "start": 3998.38, "end": 4002.36, "text": " I basically write everything from scratch all the stuff you see inside the past day our library", "tokens": [286, 1936, 2464, 1203, 490, 8459, 439, 264, 1507, 291, 536, 1854, 264, 1791, 786, 527, 6405], "temperature": 0.0, "avg_logprob": -0.19402250253929282, "compression_ratio": 1.8023715415019763, "no_speech_prob": 7.527824891440105e-06}, {"id": 819, "seek": 398530, "start": 4002.36, "end": 4003.98, "text": " We would have had to have written it", "tokens": [492, 576, 362, 632, 281, 362, 3720, 309], "temperature": 0.0, "avg_logprob": -0.19402250253929282, "compression_ratio": 1.8023715415019763, "no_speech_prob": 7.527824891440105e-06}, {"id": 820, "seek": 398530, "start": 4003.98, "end": 4010.46, "text": " You know to learn and so it really makes it very hard to learn deep learning when you have to write hundreds of lines", "tokens": [509, 458, 281, 1466, 293, 370, 309, 534, 1669, 309, 588, 1152, 281, 1466, 2452, 2539, 562, 291, 362, 281, 2464, 6779, 295, 3876], "temperature": 0.0, "avg_logprob": -0.19402250253929282, "compression_ratio": 1.8023715415019763, "no_speech_prob": 7.527824891440105e-06}, {"id": 821, "seek": 398530, "start": 4010.46, "end": 4012.46, "text": " of code to do anything so", "tokens": [295, 3089, 281, 360, 1340, 370], "temperature": 0.0, "avg_logprob": -0.19402250253929282, "compression_ratio": 1.8023715415019763, "no_speech_prob": 7.527824891440105e-06}, {"id": 822, "seek": 401246, "start": 4012.46, "end": 4013.9, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2320649334203417, "compression_ratio": 1.7191489361702128, "no_speech_prob": 4.495128450798802e-06}, {"id": 823, "seek": 401246, "start": 4013.9, "end": 4019.18, "text": " We decided to create a library on top of pie torches because we you know this", "tokens": [492, 3047, 281, 1884, 257, 6405, 322, 1192, 295, 1730, 3930, 3781, 570, 321, 291, 458, 341], "temperature": 0.0, "avg_logprob": -0.2320649334203417, "compression_ratio": 1.7191489361702128, "no_speech_prob": 4.495128450798802e-06}, {"id": 824, "seek": 401246, "start": 4019.9, "end": 4025.3, "text": " Our mission is to teach world-class deep learning so we wanted to show you like here's how you can be the best in the world", "tokens": [2621, 4447, 307, 281, 2924, 1002, 12, 11665, 2452, 2539, 370, 321, 1415, 281, 855, 291, 411, 510, 311, 577, 291, 393, 312, 264, 1151, 294, 264, 1002], "temperature": 0.0, "avg_logprob": -0.2320649334203417, "compression_ratio": 1.7191489361702128, "no_speech_prob": 4.495128450798802e-06}, {"id": 825, "seek": 401246, "start": 4025.3, "end": 4027.02, "text": " at doing X", "tokens": [412, 884, 1783], "temperature": 0.0, "avg_logprob": -0.2320649334203417, "compression_ratio": 1.7191489361702128, "no_speech_prob": 4.495128450798802e-06}, {"id": 826, "seek": 401246, "start": 4027.02, "end": 4030.94, "text": " And we found that a lot of the world-class stuff. We needed to show", "tokens": [400, 321, 1352, 300, 257, 688, 295, 264, 1002, 12, 11665, 1507, 13, 492, 2978, 281, 855], "temperature": 0.0, "avg_logprob": -0.2320649334203417, "compression_ratio": 1.7191489361702128, "no_speech_prob": 4.495128450798802e-06}, {"id": 827, "seek": 401246, "start": 4031.54, "end": 4035.14, "text": " Really needed pie torch or at least with pie torch it was far easier", "tokens": [4083, 2978, 1730, 27822, 420, 412, 1935, 365, 1730, 27822, 309, 390, 1400, 3571], "temperature": 0.0, "avg_logprob": -0.2320649334203417, "compression_ratio": 1.7191489361702128, "no_speech_prob": 4.495128450798802e-06}, {"id": 828, "seek": 401246, "start": 4035.82, "end": 4038.82, "text": " but then pie torch itself just wasn't suitable as a", "tokens": [457, 550, 1730, 27822, 2564, 445, 2067, 380, 12873, 382, 257], "temperature": 0.0, "avg_logprob": -0.2320649334203417, "compression_ratio": 1.7191489361702128, "no_speech_prob": 4.495128450798802e-06}, {"id": 829, "seek": 403882, "start": 4038.82, "end": 4042.02, "text": " a first thing to teach with for new", "tokens": [257, 700, 551, 281, 2924, 365, 337, 777], "temperature": 0.0, "avg_logprob": -0.23958071072896323, "compression_ratio": 1.6359832635983265, "no_speech_prob": 1.209862148243701e-06}, {"id": 830, "seek": 403882, "start": 4042.9, "end": 4044.9, "text": " For new deep learning practitioners", "tokens": [1171, 777, 2452, 2539, 25742], "temperature": 0.0, "avg_logprob": -0.23958071072896323, "compression_ratio": 1.6359832635983265, "no_speech_prob": 1.209862148243701e-06}, {"id": 831, "seek": 403882, "start": 4045.38, "end": 4047.7400000000002, "text": " So we built this library on top of pie torch", "tokens": [407, 321, 3094, 341, 6405, 322, 1192, 295, 1730, 27822], "temperature": 0.0, "avg_logprob": -0.23958071072896323, "compression_ratio": 1.6359832635983265, "no_speech_prob": 1.209862148243701e-06}, {"id": 832, "seek": 403882, "start": 4049.34, "end": 4052.76, "text": " Initially heavily influenced by Keras which is what we taught last year", "tokens": [29446, 10950, 15269, 538, 591, 6985, 597, 307, 437, 321, 5928, 1036, 1064], "temperature": 0.0, "avg_logprob": -0.23958071072896323, "compression_ratio": 1.6359832635983265, "no_speech_prob": 1.209862148243701e-06}, {"id": 833, "seek": 403882, "start": 4053.06, "end": 4056.86, "text": " But then we realized we could actually make things much much much easier than Keras", "tokens": [583, 550, 321, 5334, 321, 727, 767, 652, 721, 709, 709, 709, 3571, 813, 591, 6985], "temperature": 0.0, "avg_logprob": -0.23958071072896323, "compression_ratio": 1.6359832635983265, "no_speech_prob": 1.209862148243701e-06}, {"id": 834, "seek": 403882, "start": 4056.94, "end": 4060.06, "text": " So in Keras if you look back at last year's course notes", "tokens": [407, 294, 591, 6985, 498, 291, 574, 646, 412, 1036, 1064, 311, 1164, 5570], "temperature": 0.0, "avg_logprob": -0.23958071072896323, "compression_ratio": 1.6359832635983265, "no_speech_prob": 1.209862148243701e-06}, {"id": 835, "seek": 403882, "start": 4060.26, "end": 4064.1800000000003, "text": " You'll find that all of the code is two to three times longer", "tokens": [509, 603, 915, 300, 439, 295, 264, 3089, 307, 732, 281, 1045, 1413, 2854], "temperature": 0.0, "avg_logprob": -0.23958071072896323, "compression_ratio": 1.6359832635983265, "no_speech_prob": 1.209862148243701e-06}, {"id": 836, "seek": 406418, "start": 4064.18, "end": 4070.2999999999997, "text": " And there's lots more opportunities for mistakes because there's just a lot of things you have to get right", "tokens": [400, 456, 311, 3195, 544, 4786, 337, 8038, 570, 456, 311, 445, 257, 688, 295, 721, 291, 362, 281, 483, 558], "temperature": 0.0, "avg_logprob": -0.14269799864693974, "compression_ratio": 1.7008928571428572, "no_speech_prob": 4.785046712640906e-06}, {"id": 837, "seek": 406418, "start": 4071.54, "end": 4078.8199999999997, "text": " So we ended up kind of building this this this library in order to make it easier to get into deep learning", "tokens": [407, 321, 4590, 493, 733, 295, 2390, 341, 341, 341, 6405, 294, 1668, 281, 652, 309, 3571, 281, 483, 666, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.14269799864693974, "compression_ratio": 1.7008928571428572, "no_speech_prob": 4.785046712640906e-06}, {"id": 838, "seek": 406418, "start": 4078.98, "end": 4086.3399999999997, "text": " But also easier to get state-of-the-art results and then over the last year as we started developing on top of that we started", "tokens": [583, 611, 3571, 281, 483, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 293, 550, 670, 264, 1036, 1064, 382, 321, 1409, 6416, 322, 1192, 295, 300, 321, 1409], "temperature": 0.0, "avg_logprob": -0.14269799864693974, "compression_ratio": 1.7008928571428572, "no_speech_prob": 4.785046712640906e-06}, {"id": 839, "seek": 406418, "start": 4086.54, "end": 4089.22, "text": " Discovering that by using this library", "tokens": [40386, 278, 300, 538, 1228, 341, 6405], "temperature": 0.0, "avg_logprob": -0.14269799864693974, "compression_ratio": 1.7008928571428572, "no_speech_prob": 4.785046712640906e-06}, {"id": 840, "seek": 408922, "start": 4089.22, "end": 4096.42, "text": " It made us so much more productive that we actually started kind of developing new state-of-the-art results and new methods ourselves", "tokens": [467, 1027, 505, 370, 709, 544, 13304, 300, 321, 767, 1409, 733, 295, 6416, 777, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 293, 777, 7150, 4175], "temperature": 0.0, "avg_logprob": -0.17483039041167325, "compression_ratio": 1.6590909090909092, "no_speech_prob": 4.710861958301393e-06}, {"id": 841, "seek": 408922, "start": 4096.54, "end": 4103.84, "text": " And we started realizing that there's a whole bunch of like papers that have kind of been ignored or lost which when you use them", "tokens": [400, 321, 1409, 16734, 300, 456, 311, 257, 1379, 3840, 295, 411, 10577, 300, 362, 733, 295, 668, 19735, 420, 2731, 597, 562, 291, 764, 552], "temperature": 0.0, "avg_logprob": -0.17483039041167325, "compression_ratio": 1.6590909090909092, "no_speech_prob": 4.710861958301393e-06}, {"id": 842, "seek": 408922, "start": 4103.84, "end": 4110.139999999999, "text": " It could like automate or semi automate start like burning red finder. That's not in any other library, so", "tokens": [467, 727, 411, 31605, 420, 12909, 31605, 722, 411, 9488, 2182, 915, 260, 13, 663, 311, 406, 294, 604, 661, 6405, 11, 370], "temperature": 0.0, "avg_logprob": -0.17483039041167325, "compression_ratio": 1.6590909090909092, "no_speech_prob": 4.710861958301393e-06}, {"id": 843, "seek": 408922, "start": 4111.139999999999, "end": 4114.58, "text": " So I kind of got to the point where now not only is kind of fast AI", "tokens": [407, 286, 733, 295, 658, 281, 264, 935, 689, 586, 406, 787, 307, 733, 295, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.17483039041167325, "compression_ratio": 1.6590909090909092, "no_speech_prob": 4.710861958301393e-06}, {"id": 844, "seek": 411458, "start": 4114.58, "end": 4119.54, "text": " I let's just do things easier much easier than any other approach", "tokens": [286, 718, 311, 445, 360, 721, 3571, 709, 3571, 813, 604, 661, 3109], "temperature": 0.0, "avg_logprob": -0.20138424634933472, "compression_ratio": 1.625, "no_speech_prob": 6.854222647234565e-06}, {"id": 845, "seek": 411458, "start": 4119.82, "end": 4123.74, "text": " But at the same time it actually has a lot more", "tokens": [583, 412, 264, 912, 565, 309, 767, 575, 257, 688, 544], "temperature": 0.0, "avg_logprob": -0.20138424634933472, "compression_ratio": 1.625, "no_speech_prob": 6.854222647234565e-06}, {"id": 846, "seek": 411458, "start": 4124.38, "end": 4130.14, "text": " Kind of sophisticated stuff behind the scenes and anything else so so it's kind of an interesting mix", "tokens": [9242, 295, 16950, 1507, 2261, 264, 8026, 293, 1340, 1646, 370, 370, 309, 311, 733, 295, 364, 1880, 2890], "temperature": 0.0, "avg_logprob": -0.20138424634933472, "compression_ratio": 1.625, "no_speech_prob": 6.854222647234565e-06}, {"id": 847, "seek": 411458, "start": 4131.34, "end": 4134.98, "text": " So yeah, so we've released this library like at this stage", "tokens": [407, 1338, 11, 370, 321, 600, 4736, 341, 6405, 411, 412, 341, 3233], "temperature": 0.0, "avg_logprob": -0.20138424634933472, "compression_ratio": 1.625, "no_speech_prob": 6.854222647234565e-06}, {"id": 848, "seek": 411458, "start": 4134.98, "end": 4139.1, "text": " It's like very early version and so through this course by the end of this course", "tokens": [467, 311, 411, 588, 2440, 3037, 293, 370, 807, 341, 1164, 538, 264, 917, 295, 341, 1164], "temperature": 0.0, "avg_logprob": -0.20138424634933472, "compression_ratio": 1.625, "no_speech_prob": 6.854222647234565e-06}, {"id": 849, "seek": 411458, "start": 4139.1, "end": 4141.1, "text": " I hope as a group you know we all", "tokens": [286, 1454, 382, 257, 1594, 291, 458, 321, 439], "temperature": 0.0, "avg_logprob": -0.20138424634933472, "compression_ratio": 1.625, "no_speech_prob": 6.854222647234565e-06}, {"id": 850, "seek": 414110, "start": 4141.1, "end": 4144.84, "text": " A lot of people are already helping have developed it into something that's", "tokens": [316, 688, 295, 561, 366, 1217, 4315, 362, 4743, 309, 666, 746, 300, 311], "temperature": 0.0, "avg_logprob": -0.2598659825879474, "compression_ratio": 1.6261682242990654, "no_speech_prob": 1.9222628907300532e-05}, {"id": 851, "seek": 414110, "start": 4146.3, "end": 4148.5, "text": " You know really pretty stable and rough solid", "tokens": [509, 458, 534, 1238, 8351, 293, 5903, 5100], "temperature": 0.0, "avg_logprob": -0.2598659825879474, "compression_ratio": 1.6261682242990654, "no_speech_prob": 1.9222628907300532e-05}, {"id": 852, "seek": 414110, "start": 4149.14, "end": 4152.68, "text": " And yeah, anybody can then can use it", "tokens": [400, 1338, 11, 4472, 393, 550, 393, 764, 309], "temperature": 0.0, "avg_logprob": -0.2598659825879474, "compression_ratio": 1.6261682242990654, "no_speech_prob": 1.9222628907300532e-05}, {"id": 853, "seek": 414110, "start": 4153.660000000001, "end": 4159.3, "text": " To build your own models under an open source license as you can see it's available on github", "tokens": [1407, 1322, 428, 1065, 5245, 833, 364, 1269, 4009, 10476, 382, 291, 393, 536, 309, 311, 2435, 322, 290, 355, 836], "temperature": 0.0, "avg_logprob": -0.2598659825879474, "compression_ratio": 1.6261682242990654, "no_speech_prob": 1.9222628907300532e-05}, {"id": 854, "seek": 414110, "start": 4163.06, "end": 4170.34, "text": " Behind the scenes it's it's creating pytorch models and so pytorch models can then be exported", "tokens": [20475, 264, 8026, 309, 311, 309, 311, 4084, 25878, 284, 339, 5245, 293, 370, 25878, 284, 339, 5245, 393, 550, 312, 42055], "temperature": 0.0, "avg_logprob": -0.2598659825879474, "compression_ratio": 1.6261682242990654, "no_speech_prob": 1.9222628907300532e-05}, {"id": 855, "seek": 417034, "start": 4170.34, "end": 4172.58, "text": " into various different formats", "tokens": [666, 3683, 819, 25879], "temperature": 0.0, "avg_logprob": -0.25305709132441767, "compression_ratio": 1.6961538461538461, "no_speech_prob": 7.889165317465086e-06}, {"id": 856, "seek": 417034, "start": 4173.7, "end": 4177.9800000000005, "text": " Having said that like a lot of folks like if you want to do something on a mobile phone for example", "tokens": [10222, 848, 300, 411, 257, 688, 295, 4024, 411, 498, 291, 528, 281, 360, 746, 322, 257, 6013, 2593, 337, 1365], "temperature": 0.0, "avg_logprob": -0.25305709132441767, "compression_ratio": 1.6961538461538461, "no_speech_prob": 7.889165317465086e-06}, {"id": 857, "seek": 417034, "start": 4178.34, "end": 4180.34, "text": " You're probably going to need to use tensorflow", "tokens": [509, 434, 1391, 516, 281, 643, 281, 764, 40863, 10565], "temperature": 0.0, "avg_logprob": -0.25305709132441767, "compression_ratio": 1.6961538461538461, "no_speech_prob": 7.889165317465086e-06}, {"id": 858, "seek": 417034, "start": 4181.06, "end": 4183.06, "text": " and so", "tokens": [293, 370], "temperature": 0.0, "avg_logprob": -0.25305709132441767, "compression_ratio": 1.6961538461538461, "no_speech_prob": 7.889165317465086e-06}, {"id": 859, "seek": 417034, "start": 4183.18, "end": 4184.46, "text": " Later on in this course", "tokens": [11965, 322, 294, 341, 1164], "temperature": 0.0, "avg_logprob": -0.25305709132441767, "compression_ratio": 1.6961538461538461, "no_speech_prob": 7.889165317465086e-06}, {"id": 860, "seek": 417034, "start": 4184.46, "end": 4189.84, "text": " We're going to show like how some of the things that we're doing in the fast AI library you can do in Keras and", "tokens": [492, 434, 516, 281, 855, 411, 577, 512, 295, 264, 721, 300, 321, 434, 884, 294, 264, 2370, 7318, 6405, 291, 393, 360, 294, 591, 6985, 293], "temperature": 0.0, "avg_logprob": -0.25305709132441767, "compression_ratio": 1.6961538461538461, "no_speech_prob": 7.889165317465086e-06}, {"id": 861, "seek": 417034, "start": 4189.84, "end": 4193.1, "text": " TensorFlow so you can kind of get a sense of what the different libraries look like", "tokens": [37624, 370, 291, 393, 733, 295, 483, 257, 2020, 295, 437, 264, 819, 15148, 574, 411], "temperature": 0.0, "avg_logprob": -0.25305709132441767, "compression_ratio": 1.6961538461538461, "no_speech_prob": 7.889165317465086e-06}, {"id": 862, "seek": 417034, "start": 4193.900000000001, "end": 4196.22, "text": " Generally speaking the simple stuff", "tokens": [21082, 4124, 264, 2199, 1507], "temperature": 0.0, "avg_logprob": -0.25305709132441767, "compression_ratio": 1.6961538461538461, "no_speech_prob": 7.889165317465086e-06}, {"id": 863, "seek": 419622, "start": 4196.22, "end": 4204.1, "text": " Is like it'll take you a small number of days to learn to do it in Keras and tensorflow versus fast AI", "tokens": [1119, 411, 309, 603, 747, 291, 257, 1359, 1230, 295, 1708, 281, 1466, 281, 360, 309, 294, 591, 6985, 293, 40863, 10565, 5717, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.26736927032470703, "compression_ratio": 1.587962962962963, "no_speech_prob": 5.862701073056087e-06}, {"id": 864, "seek": 419622, "start": 4204.1, "end": 4206.860000000001, "text": " And pytorch and the more complex stuff often", "tokens": [400, 25878, 284, 339, 293, 264, 544, 3997, 1507, 2049], "temperature": 0.0, "avg_logprob": -0.26736927032470703, "compression_ratio": 1.587962962962963, "no_speech_prob": 5.862701073056087e-06}, {"id": 865, "seek": 419622, "start": 4207.62, "end": 4214.54, "text": " Just won't be possible so like if you need it to be in tensorflow you'll just have to kind of simplify it often a little", "tokens": [1449, 1582, 380, 312, 1944, 370, 411, 498, 291, 643, 309, 281, 312, 294, 40863, 10565, 291, 603, 445, 362, 281, 733, 295, 20460, 309, 2049, 257, 707], "temperature": 0.0, "avg_logprob": -0.26736927032470703, "compression_ratio": 1.587962962962963, "no_speech_prob": 5.862701073056087e-06}, {"id": 866, "seek": 419622, "start": 4214.54, "end": 4216.54, "text": " bit", "tokens": [857], "temperature": 0.0, "avg_logprob": -0.26736927032470703, "compression_ratio": 1.587962962962963, "no_speech_prob": 5.862701073056087e-06}, {"id": 867, "seek": 419622, "start": 4218.06, "end": 4220.9800000000005, "text": " But you know I think the more important thing to realize is", "tokens": [583, 291, 458, 286, 519, 264, 544, 1021, 551, 281, 4325, 307], "temperature": 0.0, "avg_logprob": -0.26736927032470703, "compression_ratio": 1.587962962962963, "no_speech_prob": 5.862701073056087e-06}, {"id": 868, "seek": 419622, "start": 4222.14, "end": 4223.58, "text": " every year", "tokens": [633, 1064], "temperature": 0.0, "avg_logprob": -0.26736927032470703, "compression_ratio": 1.587962962962963, "no_speech_prob": 5.862701073056087e-06}, {"id": 869, "seek": 422358, "start": 4223.58, "end": 4228.46, "text": " The kind of the the libraries that are available and which ones are the best totally changes", "tokens": [440, 733, 295, 264, 264, 15148, 300, 366, 2435, 293, 597, 2306, 366, 264, 1151, 3879, 2962], "temperature": 0.0, "avg_logprob": -0.2355689180308375, "compression_ratio": 1.8566176470588236, "no_speech_prob": 1.4738327081431635e-05}, {"id": 870, "seek": 422358, "start": 4228.46, "end": 4233.18, "text": " So like the main thing I hope that you get out of this course is an understanding of the concepts like", "tokens": [407, 411, 264, 2135, 551, 286, 1454, 300, 291, 483, 484, 295, 341, 1164, 307, 364, 3701, 295, 264, 10392, 411], "temperature": 0.0, "avg_logprob": -0.2355689180308375, "compression_ratio": 1.8566176470588236, "no_speech_prob": 1.4738327081431635e-05}, {"id": 871, "seek": 422358, "start": 4233.34, "end": 4238.3, "text": " Here's how you find the learning rate. Here's why differential learning rates are important. Here's how you do learning rate annealing", "tokens": [1692, 311, 577, 291, 915, 264, 2539, 3314, 13, 1692, 311, 983, 15756, 2539, 6846, 366, 1021, 13, 1692, 311, 577, 291, 360, 2539, 3314, 22256, 4270], "temperature": 0.0, "avg_logprob": -0.2355689180308375, "compression_ratio": 1.8566176470588236, "no_speech_prob": 1.4738327081431635e-05}, {"id": 872, "seek": 422358, "start": 4238.86, "end": 4243.0199999999995, "text": " You know here's what stochastic gradient descent with restarts does so on and so forth", "tokens": [509, 458, 510, 311, 437, 342, 8997, 2750, 16235, 23475, 365, 1472, 11814, 775, 370, 322, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.2355689180308375, "compression_ratio": 1.8566176470588236, "no_speech_prob": 1.4738327081431635e-05}, {"id": 873, "seek": 422358, "start": 4244.78, "end": 4247.58, "text": " Because you know by the time we do this course again next year", "tokens": [1436, 291, 458, 538, 264, 565, 321, 360, 341, 1164, 797, 958, 1064], "temperature": 0.0, "avg_logprob": -0.2355689180308375, "compression_ratio": 1.8566176470588236, "no_speech_prob": 1.4738327081431635e-05}, {"id": 874, "seek": 422358, "start": 4248.5, "end": 4250.94, "text": " You know the the library", "tokens": [509, 458, 264, 264, 6405], "temperature": 0.0, "avg_logprob": -0.2355689180308375, "compression_ratio": 1.8566176470588236, "no_speech_prob": 1.4738327081431635e-05}, {"id": 875, "seek": 425094, "start": 4250.94, "end": 4253.179999999999, "text": " Situation is going to be different again", "tokens": [22247, 307, 516, 281, 312, 819, 797], "temperature": 0.0, "avg_logprob": -0.24164905726352584, "compression_ratio": 1.564, "no_speech_prob": 1.8342037947149947e-05}, {"id": 876, "seek": 425094, "start": 4254.419999999999, "end": 4256.419999999999, "text": " That's a question", "tokens": [663, 311, 257, 1168], "temperature": 0.0, "avg_logprob": -0.24164905726352584, "compression_ratio": 1.564, "no_speech_prob": 1.8342037947149947e-05}, {"id": 877, "seek": 425094, "start": 4263.54, "end": 4264.62, "text": " Was", "tokens": [3027], "temperature": 0.0, "avg_logprob": -0.24164905726352584, "compression_ratio": 1.564, "no_speech_prob": 1.8342037947149947e-05}, {"id": 878, "seek": 425094, "start": 4264.62, "end": 4270.339999999999, "text": " Wondering if you've had an opinion on pyro which is uber's new release. I haven't looked at it", "tokens": [343, 684, 1794, 498, 291, 600, 632, 364, 4800, 322, 10664, 340, 597, 307, 344, 607, 311, 777, 4374, 13, 286, 2378, 380, 2956, 412, 309], "temperature": 0.0, "avg_logprob": -0.24164905726352584, "compression_ratio": 1.564, "no_speech_prob": 1.8342037947149947e-05}, {"id": 879, "seek": 425094, "start": 4270.339999999999, "end": 4275.339999999999, "text": " No, I'm very interested in probabilistic programming, and it's really cool. That's built on top of pytorch", "tokens": [883, 11, 286, 478, 588, 3102, 294, 31959, 3142, 9410, 11, 293, 309, 311, 534, 1627, 13, 663, 311, 3094, 322, 1192, 295, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.24164905726352584, "compression_ratio": 1.564, "no_speech_prob": 1.8342037947149947e-05}, {"id": 880, "seek": 425094, "start": 4275.339999999999, "end": 4279.94, "text": " So one of the things we'll learn about in this course is we'll see that pytorch is much more than just a deep learning library", "tokens": [407, 472, 295, 264, 721, 321, 603, 1466, 466, 294, 341, 1164, 307, 321, 603, 536, 300, 25878, 284, 339, 307, 709, 544, 813, 445, 257, 2452, 2539, 6405], "temperature": 0.0, "avg_logprob": -0.24164905726352584, "compression_ratio": 1.564, "no_speech_prob": 1.8342037947149947e-05}, {"id": 881, "seek": 427994, "start": 4279.94, "end": 4281.94, "text": " It actually lets us write", "tokens": [467, 767, 6653, 505, 2464], "temperature": 0.0, "avg_logprob": -0.22421656428156672, "compression_ratio": 1.4472361809045227, "no_speech_prob": 6.339069386740448e-06}, {"id": 882, "seek": 427994, "start": 4282.78, "end": 4284.78, "text": " arbitrary GPU", "tokens": [23211, 18407], "temperature": 0.0, "avg_logprob": -0.22421656428156672, "compression_ratio": 1.4472361809045227, "no_speech_prob": 6.339069386740448e-06}, {"id": 883, "seek": 427994, "start": 4285.66, "end": 4287.54, "text": " accelerated algorithms", "tokens": [29763, 14642], "temperature": 0.0, "avg_logprob": -0.22421656428156672, "compression_ratio": 1.4472361809045227, "no_speech_prob": 6.339069386740448e-06}, {"id": 884, "seek": 427994, "start": 4287.54, "end": 4294.339999999999, "text": " From scratch which we're actually going to do and pyro is a great example of what people are now doing with pytorch outside of the deep learning world", "tokens": [3358, 8459, 597, 321, 434, 767, 516, 281, 360, 293, 10664, 340, 307, 257, 869, 1365, 295, 437, 561, 366, 586, 884, 365, 25878, 284, 339, 2380, 295, 264, 2452, 2539, 1002], "temperature": 0.0, "avg_logprob": -0.22421656428156672, "compression_ratio": 1.4472361809045227, "no_speech_prob": 6.339069386740448e-06}, {"id": 885, "seek": 427994, "start": 4295.98, "end": 4301.74, "text": " Great okay, let's take a eight minute break, and we'll come back at 755", "tokens": [3769, 1392, 11, 718, 311, 747, 257, 3180, 3456, 1821, 11, 293, 321, 603, 808, 646, 412, 1614, 13622], "temperature": 0.0, "avg_logprob": -0.22421656428156672, "compression_ratio": 1.4472361809045227, "no_speech_prob": 6.339069386740448e-06}, {"id": 886, "seek": 430174, "start": 4301.74, "end": 4313.62, "text": " So 99 point six five percent accuracy", "tokens": [407, 11803, 935, 2309, 1732, 3043, 14170], "temperature": 0.0, "avg_logprob": -0.29291646321614584, "compression_ratio": 1.6287128712871286, "no_speech_prob": 1.3006609151489101e-05}, {"id": 887, "seek": 430174, "start": 4314.26, "end": 4316.98, "text": " What does that mean so in?", "tokens": [708, 775, 300, 914, 370, 294, 30], "temperature": 0.0, "avg_logprob": -0.29291646321614584, "compression_ratio": 1.6287128712871286, "no_speech_prob": 1.3006609151489101e-05}, {"id": 888, "seek": 430174, "start": 4317.94, "end": 4320.34, "text": " Classification when we do classification in machine learning", "tokens": [9471, 3774, 562, 321, 360, 21538, 294, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.29291646321614584, "compression_ratio": 1.6287128712871286, "no_speech_prob": 1.3006609151489101e-05}, {"id": 889, "seek": 430174, "start": 4320.66, "end": 4326.62, "text": " The it's really simple way to look at the result of a classification is what's called the confusion matrix", "tokens": [440, 309, 311, 534, 2199, 636, 281, 574, 412, 264, 1874, 295, 257, 21538, 307, 437, 311, 1219, 264, 15075, 8141], "temperature": 0.0, "avg_logprob": -0.29291646321614584, "compression_ratio": 1.6287128712871286, "no_speech_prob": 1.3006609151489101e-05}, {"id": 890, "seek": 432662, "start": 4326.62, "end": 4333.3, "text": " This is not just deep learning, but in any kind of classifier machine learning where we say okay? What was the actual truth?", "tokens": [639, 307, 406, 445, 2452, 2539, 11, 457, 294, 604, 733, 295, 1508, 9902, 3479, 2539, 689, 321, 584, 1392, 30, 708, 390, 264, 3539, 3494, 30], "temperature": 0.0, "avg_logprob": -0.1516956385999623, "compression_ratio": 1.7489711934156378, "no_speech_prob": 8.801001058600377e-06}, {"id": 891, "seek": 432662, "start": 4333.46, "end": 4339.18, "text": " There were a thousand cats and a thousand dogs and if the thousand actual cats", "tokens": [821, 645, 257, 4714, 11111, 293, 257, 4714, 7197, 293, 498, 264, 4714, 3539, 11111], "temperature": 0.0, "avg_logprob": -0.1516956385999623, "compression_ratio": 1.7489711934156378, "no_speech_prob": 8.801001058600377e-06}, {"id": 892, "seek": 432662, "start": 4339.46, "end": 4343.94, "text": " How many did we predict were cats this is obviously in the validation sets?", "tokens": [1012, 867, 630, 321, 6069, 645, 11111, 341, 307, 2745, 294, 264, 24071, 6352, 30], "temperature": 0.0, "avg_logprob": -0.1516956385999623, "compression_ratio": 1.7489711934156378, "no_speech_prob": 8.801001058600377e-06}, {"id": 893, "seek": 432662, "start": 4343.94, "end": 4346.3, "text": " This is the images that we didn't use to train with", "tokens": [639, 307, 264, 5267, 300, 321, 994, 380, 764, 281, 3847, 365], "temperature": 0.0, "avg_logprob": -0.1516956385999623, "compression_ratio": 1.7489711934156378, "no_speech_prob": 8.801001058600377e-06}, {"id": 894, "seek": 432662, "start": 4346.86, "end": 4352.42, "text": " It turns out there were 998 cats that we actually predicted as cats and two that we got wrong", "tokens": [467, 4523, 484, 456, 645, 11803, 23, 11111, 300, 321, 767, 19147, 382, 11111, 293, 732, 300, 321, 658, 2085], "temperature": 0.0, "avg_logprob": -0.1516956385999623, "compression_ratio": 1.7489711934156378, "no_speech_prob": 8.801001058600377e-06}, {"id": 895, "seek": 435242, "start": 4352.42, "end": 4359.9800000000005, "text": " Okay, and then for dogs there were 995 that we predicted were dogs and then five that we got wrong and so often these", "tokens": [1033, 11, 293, 550, 337, 7197, 456, 645, 11803, 20, 300, 321, 19147, 645, 7197, 293, 550, 1732, 300, 321, 658, 2085, 293, 370, 2049, 613], "temperature": 0.0, "avg_logprob": -0.1552771040536825, "compression_ratio": 1.76171875, "no_speech_prob": 4.222807547193952e-06}, {"id": 896, "seek": 435242, "start": 4360.7, "end": 4364.7, "text": " Confusion matrices can be helpful particularly if you've got like four or five classes", "tokens": [11701, 5704, 32284, 393, 312, 4961, 4098, 498, 291, 600, 658, 411, 1451, 420, 1732, 5359], "temperature": 0.0, "avg_logprob": -0.1552771040536825, "compression_ratio": 1.76171875, "no_speech_prob": 4.222807547193952e-06}, {"id": 897, "seek": 435242, "start": 4364.7, "end": 4370.36, "text": " You're trying to predict to see like which group you're having the most trouble with and you can see it uses color coding", "tokens": [509, 434, 1382, 281, 6069, 281, 536, 411, 597, 1594, 291, 434, 1419, 264, 881, 5253, 365, 293, 291, 393, 536, 309, 4960, 2017, 17720], "temperature": 0.0, "avg_logprob": -0.1552771040536825, "compression_ratio": 1.76171875, "no_speech_prob": 4.222807547193952e-06}, {"id": 898, "seek": 435242, "start": 4370.54, "end": 4376.66, "text": " To tell you you know to highlight the large the large bits you're going to help that the diagonal is", "tokens": [1407, 980, 291, 291, 458, 281, 5078, 264, 2416, 264, 2416, 9239, 291, 434, 516, 281, 854, 300, 264, 21539, 307], "temperature": 0.0, "avg_logprob": -0.1552771040536825, "compression_ratio": 1.76171875, "no_speech_prob": 4.222807547193952e-06}, {"id": 899, "seek": 435242, "start": 4377.5, "end": 4379.5, "text": " the highlighted section", "tokens": [264, 17173, 3541], "temperature": 0.0, "avg_logprob": -0.1552771040536825, "compression_ratio": 1.76171875, "no_speech_prob": 4.222807547193952e-06}, {"id": 900, "seek": 437950, "start": 4379.5, "end": 4383.1, "text": " So now that we've retrained the model it can be quite helpful now", "tokens": [407, 586, 300, 321, 600, 1533, 31774, 264, 2316, 309, 393, 312, 1596, 4961, 586], "temperature": 0.0, "avg_logprob": -0.17865020538044868, "compression_ratio": 1.7670682730923695, "no_speech_prob": 3.34051742356678e-06}, {"id": 901, "seek": 437950, "start": 4383.1, "end": 4390.1, "text": " That's better to actually look back and see like okay, which ones in particular were incorrect and we can see here", "tokens": [663, 311, 1101, 281, 767, 574, 646, 293, 536, 411, 1392, 11, 597, 2306, 294, 1729, 645, 18424, 293, 321, 393, 536, 510], "temperature": 0.0, "avg_logprob": -0.17865020538044868, "compression_ratio": 1.7670682730923695, "no_speech_prob": 3.34051742356678e-06}, {"id": 902, "seek": 437950, "start": 4390.66, "end": 4396.88, "text": " There were actually only two incorrect cats it prints out four by default so you can actually see these two", "tokens": [821, 645, 767, 787, 732, 18424, 11111, 309, 22305, 484, 1451, 538, 7576, 370, 291, 393, 767, 536, 613, 732], "temperature": 0.0, "avg_logprob": -0.17865020538044868, "compression_ratio": 1.7670682730923695, "no_speech_prob": 3.34051742356678e-06}, {"id": 903, "seek": 437950, "start": 4397.42, "end": 4403.34, "text": " Actually less than 0.5, so they weren't they weren't wrong. Okay, so it's actually only these two were wrong cats", "tokens": [5135, 1570, 813, 1958, 13, 20, 11, 370, 436, 4999, 380, 436, 4999, 380, 2085, 13, 1033, 11, 370, 309, 311, 767, 787, 613, 732, 645, 2085, 11111], "temperature": 0.0, "avg_logprob": -0.17865020538044868, "compression_ratio": 1.7670682730923695, "no_speech_prob": 3.34051742356678e-06}, {"id": 904, "seek": 437950, "start": 4403.86, "end": 4405.94, "text": " This one isn't obviously a cat at all", "tokens": [639, 472, 1943, 380, 2745, 257, 3857, 412, 439], "temperature": 0.0, "avg_logprob": -0.17865020538044868, "compression_ratio": 1.7670682730923695, "no_speech_prob": 3.34051742356678e-06}, {"id": 905, "seek": 440594, "start": 4405.94, "end": 4412.599999999999, "text": " This one is but it looks like it's got a lot of weird artifacts, and you can't see its eyeballs at all so", "tokens": [639, 472, 307, 457, 309, 1542, 411, 309, 311, 658, 257, 688, 295, 3657, 24617, 11, 293, 291, 393, 380, 536, 1080, 43758, 412, 439, 370], "temperature": 0.0, "avg_logprob": -0.14423499236235748, "compression_ratio": 1.768, "no_speech_prob": 7.527928573836107e-06}, {"id": 906, "seek": 440594, "start": 4413.099999999999, "end": 4419.36, "text": " And then here are the how many dogs where they were wrong there were five wrong dogs here are four of them", "tokens": [400, 550, 510, 366, 264, 577, 867, 7197, 689, 436, 645, 2085, 456, 645, 1732, 2085, 7197, 510, 366, 1451, 295, 552], "temperature": 0.0, "avg_logprob": -0.14423499236235748, "compression_ratio": 1.768, "no_speech_prob": 7.527928573836107e-06}, {"id": 907, "seek": 440594, "start": 4419.36, "end": 4421.36, "text": " That's not obviously a dog", "tokens": [663, 311, 406, 2745, 257, 3000], "temperature": 0.0, "avg_logprob": -0.14423499236235748, "compression_ratio": 1.768, "no_speech_prob": 7.527928573836107e-06}, {"id": 908, "seek": 440594, "start": 4421.82, "end": 4429.0599999999995, "text": " That looks like a mistake that looks like a mistake that one. I guess it doesn't have enough information, but I guess it's a mistake so", "tokens": [663, 1542, 411, 257, 6146, 300, 1542, 411, 257, 6146, 300, 472, 13, 286, 2041, 309, 1177, 380, 362, 1547, 1589, 11, 457, 286, 2041, 309, 311, 257, 6146, 370], "temperature": 0.0, "avg_logprob": -0.14423499236235748, "compression_ratio": 1.768, "no_speech_prob": 7.527928573836107e-06}, {"id": 909, "seek": 440594, "start": 4430.179999999999, "end": 4434.58, "text": " So we've done a pretty good job here of creating a good classifier", "tokens": [407, 321, 600, 1096, 257, 1238, 665, 1691, 510, 295, 4084, 257, 665, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.14423499236235748, "compression_ratio": 1.768, "no_speech_prob": 7.527928573836107e-06}, {"id": 910, "seek": 443458, "start": 4434.58, "end": 4436.58, "text": " I would based on", "tokens": [286, 576, 2361, 322], "temperature": 0.0, "avg_logprob": -0.17813897640147108, "compression_ratio": 1.7125382262996942, "no_speech_prob": 7.889178959885612e-06}, {"id": 911, "seek": 443458, "start": 4437.34, "end": 4441.66, "text": " Entering a lot of Kaggle competitions and comparing results. I've done to various research papers", "tokens": [10399, 278, 257, 688, 295, 48751, 22631, 26185, 293, 15763, 3542, 13, 286, 600, 1096, 281, 3683, 2132, 10577], "temperature": 0.0, "avg_logprob": -0.17813897640147108, "compression_ratio": 1.7125382262996942, "no_speech_prob": 7.889178959885612e-06}, {"id": 912, "seek": 443458, "start": 4441.66, "end": 4446.54, "text": " I can tell you it's it's a state-of-the-art classifier. It's it's right up there with the best in the world", "tokens": [286, 393, 980, 291, 309, 311, 309, 311, 257, 1785, 12, 2670, 12, 3322, 12, 446, 1508, 9902, 13, 467, 311, 309, 311, 558, 493, 456, 365, 264, 1151, 294, 264, 1002], "temperature": 0.0, "avg_logprob": -0.17813897640147108, "compression_ratio": 1.7125382262996942, "no_speech_prob": 7.889178959885612e-06}, {"id": 913, "seek": 443458, "start": 4447.0, "end": 4452.98, "text": " We're going to make it a little bit better in a moment, but here are the basic steps right so if you want to create a world-class", "tokens": [492, 434, 516, 281, 652, 309, 257, 707, 857, 1101, 294, 257, 1623, 11, 457, 510, 366, 264, 3875, 4439, 558, 370, 498, 291, 528, 281, 1884, 257, 1002, 12, 11665], "temperature": 0.0, "avg_logprob": -0.17813897640147108, "compression_ratio": 1.7125382262996942, "no_speech_prob": 7.889178959885612e-06}, {"id": 914, "seek": 443458, "start": 4452.98, "end": 4459.1, "text": " Image classifier the steps that we just went through was that we started our we turn data augmentation on", "tokens": [29903, 1508, 9902, 264, 4439, 300, 321, 445, 1437, 807, 390, 300, 321, 1409, 527, 321, 1261, 1412, 14501, 19631, 322], "temperature": 0.0, "avg_logprob": -0.17813897640147108, "compression_ratio": 1.7125382262996942, "no_speech_prob": 7.889178959885612e-06}, {"id": 915, "seek": 443458, "start": 4459.3, "end": 4464.22, "text": " By saying all transforms equals and you either say side on or top down depending on what you're doing", "tokens": [3146, 1566, 439, 35592, 6915, 293, 291, 2139, 584, 1252, 322, 420, 1192, 760, 5413, 322, 437, 291, 434, 884], "temperature": 0.0, "avg_logprob": -0.17813897640147108, "compression_ratio": 1.7125382262996942, "no_speech_prob": 7.889178959885612e-06}, {"id": 916, "seek": 446422, "start": 4464.22, "end": 4466.22, "text": " Start with pre compute equals true", "tokens": [6481, 365, 659, 14722, 6915, 2074], "temperature": 0.0, "avg_logprob": -0.20270329525596217, "compression_ratio": 1.7324561403508771, "no_speech_prob": 6.962186944292625e-06}, {"id": 917, "seek": 446422, "start": 4467.18, "end": 4469.14, "text": " Find a decent learning rate", "tokens": [11809, 257, 8681, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.20270329525596217, "compression_ratio": 1.7324561403508771, "no_speech_prob": 6.962186944292625e-06}, {"id": 918, "seek": 446422, "start": 4469.14, "end": 4475.04, "text": " We then train just like at one or two epochs which like takes a few seconds because we've got pre compute equals true", "tokens": [492, 550, 3847, 445, 411, 412, 472, 420, 732, 30992, 28346, 597, 411, 2516, 257, 1326, 3949, 570, 321, 600, 658, 659, 14722, 6915, 2074], "temperature": 0.0, "avg_logprob": -0.20270329525596217, "compression_ratio": 1.7324561403508771, "no_speech_prob": 6.962186944292625e-06}, {"id": 919, "seek": 446422, "start": 4475.46, "end": 4482.2, "text": " Then we turn off pre compute which allows us to use data augmentation to do another two or three epochs", "tokens": [1396, 321, 1261, 766, 659, 14722, 597, 4045, 505, 281, 764, 1412, 14501, 19631, 281, 360, 1071, 732, 420, 1045, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.20270329525596217, "compression_ratio": 1.7324561403508771, "no_speech_prob": 6.962186944292625e-06}, {"id": 920, "seek": 446422, "start": 4482.7, "end": 4484.7, "text": " generally with cycle length equals one", "tokens": [5101, 365, 6586, 4641, 6915, 472], "temperature": 0.0, "avg_logprob": -0.20270329525596217, "compression_ratio": 1.7324561403508771, "no_speech_prob": 6.962186944292625e-06}, {"id": 921, "seek": 446422, "start": 4485.1, "end": 4489.62, "text": " Then I unfreeze all the layers I then set the earlier layers to be like", "tokens": [1396, 286, 3971, 701, 1381, 439, 264, 7914, 286, 550, 992, 264, 3071, 7914, 281, 312, 411], "temperature": 0.0, "avg_logprob": -0.20270329525596217, "compression_ratio": 1.7324561403508771, "no_speech_prob": 6.962186944292625e-06}, {"id": 922, "seek": 448962, "start": 4489.62, "end": 4496.0599999999995, "text": " I have somewhere between a three times to ten times lower learning rate than the previous so in this case I did", "tokens": [286, 362, 4079, 1296, 257, 1045, 1413, 281, 2064, 1413, 3126, 2539, 3314, 813, 264, 3894, 370, 294, 341, 1389, 286, 630], "temperature": 0.0, "avg_logprob": -0.17313467253238782, "compression_ratio": 1.8622047244094488, "no_speech_prob": 3.5008351915166713e-06}, {"id": 923, "seek": 448962, "start": 4500.42, "end": 4504.26, "text": " Ten times right so it's like this was my learning rate that I found from learning rate finder", "tokens": [9380, 1413, 558, 370, 309, 311, 411, 341, 390, 452, 2539, 3314, 300, 286, 1352, 490, 2539, 3314, 915, 260], "temperature": 0.0, "avg_logprob": -0.17313467253238782, "compression_ratio": 1.8622047244094488, "no_speech_prob": 3.5008351915166713e-06}, {"id": 924, "seek": 448962, "start": 4504.26, "end": 4508.74, "text": " Then I went ten times smaller and then ten times smaller as a rule of thumb like", "tokens": [1396, 286, 1437, 2064, 1413, 4356, 293, 550, 2064, 1413, 4356, 382, 257, 4978, 295, 9298, 411], "temperature": 0.0, "avg_logprob": -0.17313467253238782, "compression_ratio": 1.8622047244094488, "no_speech_prob": 3.5008351915166713e-06}, {"id": 925, "seek": 448962, "start": 4509.26, "end": 4512.14, "text": " Knowing that you're starting with a pre trained image net model", "tokens": [25499, 300, 291, 434, 2891, 365, 257, 659, 8895, 3256, 2533, 2316], "temperature": 0.0, "avg_logprob": -0.17313467253238782, "compression_ratio": 1.8622047244094488, "no_speech_prob": 3.5008351915166713e-06}, {"id": 926, "seek": 448962, "start": 4512.42, "end": 4517.92, "text": " If you know if you can see that the things that you're now trying to classify are pretty similar to the kinds of things in", "tokens": [759, 291, 458, 498, 291, 393, 536, 300, 264, 721, 300, 291, 434, 586, 1382, 281, 33872, 366, 1238, 2531, 281, 264, 3685, 295, 721, 294], "temperature": 0.0, "avg_logprob": -0.17313467253238782, "compression_ratio": 1.8622047244094488, "no_speech_prob": 3.5008351915166713e-06}, {"id": 927, "seek": 451792, "start": 4517.92, "end": 4521.88, "text": " Image net I eat pictures of normal objects in normal environments", "tokens": [29903, 2533, 286, 1862, 5242, 295, 2710, 6565, 294, 2710, 12388], "temperature": 0.0, "avg_logprob": -0.1962068345811632, "compression_ratio": 1.8023715415019763, "no_speech_prob": 5.714993562833115e-07}, {"id": 928, "seek": 451792, "start": 4522.12, "end": 4529.22, "text": " You probably want about a 10x difference because you want those earlier layers like you think that the earlier layers are probably very good already", "tokens": [509, 1391, 528, 466, 257, 1266, 87, 2649, 570, 291, 528, 729, 3071, 7914, 411, 291, 519, 300, 264, 3071, 7914, 366, 1391, 588, 665, 1217], "temperature": 0.0, "avg_logprob": -0.1962068345811632, "compression_ratio": 1.8023715415019763, "no_speech_prob": 5.714993562833115e-07}, {"id": 929, "seek": 451792, "start": 4529.86, "end": 4535.3, "text": " Whereas if you're doing something like satellite imagery or medical imaging which is not at all like image net", "tokens": [13813, 498, 291, 434, 884, 746, 411, 16016, 24340, 420, 4625, 25036, 597, 307, 406, 412, 439, 411, 3256, 2533], "temperature": 0.0, "avg_logprob": -0.1962068345811632, "compression_ratio": 1.8023715415019763, "no_speech_prob": 5.714993562833115e-07}, {"id": 930, "seek": 451792, "start": 4535.3, "end": 4540.18, "text": " Then you probably want to be training those earlier layers a lot more so you might have like a just a 3x", "tokens": [1396, 291, 1391, 528, 281, 312, 3097, 729, 3071, 7914, 257, 688, 544, 370, 291, 1062, 362, 411, 257, 445, 257, 805, 87], "temperature": 0.0, "avg_logprob": -0.1962068345811632, "compression_ratio": 1.8023715415019763, "no_speech_prob": 5.714993562833115e-07}, {"id": 931, "seek": 451792, "start": 4540.78, "end": 4541.9, "text": " difference", "tokens": [2649], "temperature": 0.0, "avg_logprob": -0.1962068345811632, "compression_ratio": 1.8023715415019763, "no_speech_prob": 5.714993562833115e-07}, {"id": 932, "seek": 451792, "start": 4541.9, "end": 4543.46, "text": " so that's like", "tokens": [370, 300, 311, 411], "temperature": 0.0, "avg_logprob": -0.1962068345811632, "compression_ratio": 1.8023715415019763, "no_speech_prob": 5.714993562833115e-07}, {"id": 933, "seek": 454346, "start": 4543.46, "end": 4548.1, "text": " One change that I I make is to try to make it out of 10x or 3x", "tokens": [1485, 1319, 300, 286, 286, 652, 307, 281, 853, 281, 652, 309, 484, 295, 1266, 87, 420, 805, 87], "temperature": 0.0, "avg_logprob": -0.24432093446904962, "compression_ratio": 1.5204081632653061, "no_speech_prob": 5.862760190211702e-06}, {"id": 934, "seek": 454346, "start": 4551.42, "end": 4553.76, "text": " Yeah, so then after unfreezing", "tokens": [865, 11, 370, 550, 934, 3971, 701, 8781], "temperature": 0.0, "avg_logprob": -0.24432093446904962, "compression_ratio": 1.5204081632653061, "no_speech_prob": 5.862760190211702e-06}, {"id": 935, "seek": 454346, "start": 4554.72, "end": 4558.42, "text": " You can now call LR find again, right?", "tokens": [509, 393, 586, 818, 441, 49, 915, 797, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.24432093446904962, "compression_ratio": 1.5204081632653061, "no_speech_prob": 5.862760190211702e-06}, {"id": 936, "seek": 454346, "start": 4558.42, "end": 4563.76, "text": " And I actually didn't in this case, but like once you've unfrozen all the layers you've turned on differential learning rates", "tokens": [400, 286, 767, 994, 380, 294, 341, 1389, 11, 457, 411, 1564, 291, 600, 3971, 340, 2904, 439, 264, 7914, 291, 600, 3574, 322, 15756, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.24432093446904962, "compression_ratio": 1.5204081632653061, "no_speech_prob": 5.862760190211702e-06}, {"id": 937, "seek": 454346, "start": 4563.86, "end": 4567.66, "text": " You can then call LR find again, right?", "tokens": [509, 393, 550, 818, 441, 49, 915, 797, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.24432093446904962, "compression_ratio": 1.5204081632653061, "no_speech_prob": 5.862760190211702e-06}, {"id": 938, "seek": 456766, "start": 4567.66, "end": 4574.18, "text": " And so you can then check like oh does it still look like the same point I had last time is about right", "tokens": [400, 370, 291, 393, 550, 1520, 411, 1954, 775, 309, 920, 574, 411, 264, 912, 935, 286, 632, 1036, 565, 307, 466, 558], "temperature": 0.0, "avg_logprob": -0.24283929156441975, "compression_ratio": 1.816546762589928, "no_speech_prob": 7.33818922071805e-07}, {"id": 939, "seek": 456766, "start": 4574.86, "end": 4577.74, "text": " Something to note is that if you call LR find", "tokens": [6595, 281, 3637, 307, 300, 498, 291, 818, 441, 49, 915], "temperature": 0.0, "avg_logprob": -0.24283929156441975, "compression_ratio": 1.816546762589928, "no_speech_prob": 7.33818922071805e-07}, {"id": 940, "seek": 456766, "start": 4578.5, "end": 4584.94, "text": " Having set differential learning rates the thing that's actually going to print out is the learning rate of the last layers", "tokens": [10222, 992, 15756, 2539, 6846, 264, 551, 300, 311, 767, 516, 281, 4482, 484, 307, 264, 2539, 3314, 295, 264, 1036, 7914], "temperature": 0.0, "avg_logprob": -0.24283929156441975, "compression_ratio": 1.816546762589928, "no_speech_prob": 7.33818922071805e-07}, {"id": 941, "seek": 456766, "start": 4585.099999999999, "end": 4588.5, "text": " Right because you've got three different learning rates, so it's actually showing you the last layer", "tokens": [1779, 570, 291, 600, 658, 1045, 819, 2539, 6846, 11, 370, 309, 311, 767, 4099, 291, 264, 1036, 4583], "temperature": 0.0, "avg_logprob": -0.24283929156441975, "compression_ratio": 1.816546762589928, "no_speech_prob": 7.33818922071805e-07}, {"id": 942, "seek": 456766, "start": 4589.22, "end": 4590.16, "text": " So then yeah", "tokens": [407, 550, 1338], "temperature": 0.0, "avg_logprob": -0.24283929156441975, "compression_ratio": 1.816546762589928, "no_speech_prob": 7.33818922071805e-07}, {"id": 943, "seek": 456766, "start": 4590.16, "end": 4596.58, "text": " Then I train the full network with cycle multi calls to and to either it starts with the fitting or I run out of time", "tokens": [1396, 286, 3847, 264, 1577, 3209, 365, 6586, 4825, 5498, 281, 293, 281, 2139, 309, 3719, 365, 264, 15669, 420, 286, 1190, 484, 295, 565], "temperature": 0.0, "avg_logprob": -0.24283929156441975, "compression_ratio": 1.816546762589928, "no_speech_prob": 7.33818922071805e-07}, {"id": 944, "seek": 459658, "start": 4596.58, "end": 4603.42, "text": " Right so like let me show you that so let's do this again for a totally different data set so this morning", "tokens": [1779, 370, 411, 718, 385, 855, 291, 300, 370, 718, 311, 360, 341, 797, 337, 257, 3879, 819, 1412, 992, 370, 341, 2446], "temperature": 0.0, "avg_logprob": -0.2296441534291143, "compression_ratio": 1.7705627705627707, "no_speech_prob": 5.014690032112412e-06}, {"id": 945, "seek": 459658, "start": 4603.42, "end": 4608.96, "text": " I noticed that some of you on the forums were playing around with this playground Kaggle competition", "tokens": [286, 5694, 300, 512, 295, 291, 322, 264, 26998, 645, 2433, 926, 365, 341, 24646, 48751, 22631, 6211], "temperature": 0.0, "avg_logprob": -0.2296441534291143, "compression_ratio": 1.7705627705627707, "no_speech_prob": 5.014690032112412e-06}, {"id": 946, "seek": 459658, "start": 4609.38, "end": 4613.0599999999995, "text": " Very similar called dog breed identification", "tokens": [4372, 2531, 1219, 3000, 18971, 22065], "temperature": 0.0, "avg_logprob": -0.2296441534291143, "compression_ratio": 1.7705627705627707, "no_speech_prob": 5.014690032112412e-06}, {"id": 947, "seek": 459658, "start": 4613.66, "end": 4615.66, "text": " So the dog breed identification", "tokens": [407, 264, 3000, 18971, 22065], "temperature": 0.0, "avg_logprob": -0.2296441534291143, "compression_ratio": 1.7705627705627707, "no_speech_prob": 5.014690032112412e-06}, {"id": 948, "seek": 459658, "start": 4616.0199999999995, "end": 4617.66, "text": " Kaggle challenge", "tokens": [48751, 22631, 3430], "temperature": 0.0, "avg_logprob": -0.2296441534291143, "compression_ratio": 1.7705627705627707, "no_speech_prob": 5.014690032112412e-06}, {"id": 949, "seek": 459658, "start": 4617.66, "end": 4623.0199999999995, "text": " Is one where you don't actually have to decide which ones are cats and which ones are dogs they're all dogs", "tokens": [1119, 472, 689, 291, 500, 380, 767, 362, 281, 4536, 597, 2306, 366, 11111, 293, 597, 2306, 366, 7197, 436, 434, 439, 7197], "temperature": 0.0, "avg_logprob": -0.2296441534291143, "compression_ratio": 1.7705627705627707, "no_speech_prob": 5.014690032112412e-06}, {"id": 950, "seek": 462302, "start": 4623.02, "end": 4628.820000000001, "text": " But you have to decide what kind of dog it is, but there are 120 different breeds of dogs", "tokens": [583, 291, 362, 281, 4536, 437, 733, 295, 3000, 309, 307, 11, 457, 456, 366, 10411, 819, 41609, 295, 7197], "temperature": 0.0, "avg_logprob": -0.14660637954185748, "compression_ratio": 1.662037037037037, "no_speech_prob": 5.203544901632995e-07}, {"id": 951, "seek": 462302, "start": 4629.34, "end": 4631.34, "text": " okay, so", "tokens": [1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.14660637954185748, "compression_ratio": 1.662037037037037, "no_speech_prob": 5.203544901632995e-07}, {"id": 952, "seek": 462302, "start": 4631.780000000001, "end": 4633.780000000001, "text": " You know obviously this could be like", "tokens": [509, 458, 2745, 341, 727, 312, 411], "temperature": 0.0, "avg_logprob": -0.14660637954185748, "compression_ratio": 1.662037037037037, "no_speech_prob": 5.203544901632995e-07}, {"id": 953, "seek": 462302, "start": 4634.740000000001, "end": 4636.740000000001, "text": " different types of", "tokens": [819, 3467, 295], "temperature": 0.0, "avg_logprob": -0.14660637954185748, "compression_ratio": 1.662037037037037, "no_speech_prob": 5.203544901632995e-07}, {"id": 954, "seek": 462302, "start": 4637.26, "end": 4642.56, "text": " Cells and pathology slides it could be different kinds of cancers in CT scans it could be", "tokens": [383, 13677, 293, 3100, 1793, 9788, 309, 727, 312, 819, 3685, 295, 31063, 294, 19529, 35116, 309, 727, 312], "temperature": 0.0, "avg_logprob": -0.14660637954185748, "compression_ratio": 1.662037037037037, "no_speech_prob": 5.203544901632995e-07}, {"id": 955, "seek": 462302, "start": 4644.22, "end": 4649.780000000001, "text": " Different kinds of icebergs and satellite images whatever right as long as you've got some kind of labeled images", "tokens": [20825, 3685, 295, 38880, 82, 293, 16016, 5267, 2035, 558, 382, 938, 382, 291, 600, 658, 512, 733, 295, 21335, 5267], "temperature": 0.0, "avg_logprob": -0.14660637954185748, "compression_ratio": 1.662037037037037, "no_speech_prob": 5.203544901632995e-07}, {"id": 956, "seek": 464978, "start": 4649.78, "end": 4657.0599999999995, "text": " So I want to show you what I did this morning, so it took me about an hour basically to go end-to-end", "tokens": [407, 286, 528, 281, 855, 291, 437, 286, 630, 341, 2446, 11, 370, 309, 1890, 385, 466, 364, 1773, 1936, 281, 352, 917, 12, 1353, 12, 521], "temperature": 0.0, "avg_logprob": -0.20435577392578125, "compression_ratio": 1.6655290102389078, "no_speech_prob": 4.42544296674896e-06}, {"id": 957, "seek": 464978, "start": 4657.74, "end": 4660.54, "text": " From something I've never seen before so I", "tokens": [3358, 746, 286, 600, 1128, 1612, 949, 370, 286], "temperature": 0.0, "avg_logprob": -0.20435577392578125, "compression_ratio": 1.6655290102389078, "no_speech_prob": 4.42544296674896e-06}, {"id": 958, "seek": 464978, "start": 4661.5, "end": 4664.62, "text": " Downloaded the data from Kaggle, and I'll show you how to do that shortly", "tokens": [32282, 292, 264, 1412, 490, 48751, 22631, 11, 293, 286, 603, 855, 291, 577, 281, 360, 300, 13392], "temperature": 0.0, "avg_logprob": -0.20435577392578125, "compression_ratio": 1.6655290102389078, "no_speech_prob": 4.42544296674896e-06}, {"id": 959, "seek": 464978, "start": 4664.62, "end": 4667.78, "text": " But the short answer is there's something called Kaggle CLI", "tokens": [583, 264, 2099, 1867, 307, 456, 311, 746, 1219, 48751, 22631, 12855, 40], "temperature": 0.0, "avg_logprob": -0.20435577392578125, "compression_ratio": 1.6655290102389078, "no_speech_prob": 4.42544296674896e-06}, {"id": 960, "seek": 464978, "start": 4668.139999999999, "end": 4673.46, "text": " Which is a github project you can search for and if you read the docs you basically run kg download", "tokens": [3013, 307, 257, 290, 355, 836, 1716, 291, 393, 3164, 337, 293, 498, 291, 1401, 264, 45623, 291, 1936, 1190, 15696, 5484], "temperature": 0.0, "avg_logprob": -0.20435577392578125, "compression_ratio": 1.6655290102389078, "no_speech_prob": 4.42544296674896e-06}, {"id": 961, "seek": 464978, "start": 4673.54, "end": 4678.94, "text": " Provide the competition name and it'll grab all the data for you to your press or Amazon or whatever instance", "tokens": [15685, 482, 264, 6211, 1315, 293, 309, 603, 4444, 439, 264, 1412, 337, 291, 281, 428, 1886, 420, 6795, 420, 2035, 5197], "temperature": 0.0, "avg_logprob": -0.20435577392578125, "compression_ratio": 1.6655290102389078, "no_speech_prob": 4.42544296674896e-06}, {"id": 962, "seek": 467894, "start": 4678.94, "end": 4682.0599999999995, "text": " I put it in my data folder", "tokens": [286, 829, 309, 294, 452, 1412, 10820], "temperature": 0.0, "avg_logprob": -0.21667962531520896, "compression_ratio": 1.5621301775147929, "no_speech_prob": 2.5215626919816714e-06}, {"id": 963, "seek": 467894, "start": 4682.86, "end": 4684.259999999999, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.21667962531520896, "compression_ratio": 1.5621301775147929, "no_speech_prob": 2.5215626919816714e-06}, {"id": 964, "seek": 467894, "start": 4684.259999999999, "end": 4690.339999999999, "text": " I then went LS, and I saw that it's a little bit different to", "tokens": [286, 550, 1437, 36657, 11, 293, 286, 1866, 300, 309, 311, 257, 707, 857, 819, 281], "temperature": 0.0, "avg_logprob": -0.21667962531520896, "compression_ratio": 1.5621301775147929, "no_speech_prob": 2.5215626919816714e-06}, {"id": 965, "seek": 467894, "start": 4691.58, "end": 4698.96, "text": " Our previous data set it's not that there's a train folder which has a separate folder for each kind of dog", "tokens": [2621, 3894, 1412, 992, 309, 311, 406, 300, 456, 311, 257, 3847, 10820, 597, 575, 257, 4994, 10820, 337, 1184, 733, 295, 3000], "temperature": 0.0, "avg_logprob": -0.21667962531520896, "compression_ratio": 1.5621301775147929, "no_speech_prob": 2.5215626919816714e-06}, {"id": 966, "seek": 467894, "start": 4699.379999999999, "end": 4703.5, "text": " But instead it turned out there was a CSV file and the CSV file", "tokens": [583, 2602, 309, 3574, 484, 456, 390, 257, 48814, 3991, 293, 264, 48814, 3991], "temperature": 0.0, "avg_logprob": -0.21667962531520896, "compression_ratio": 1.5621301775147929, "no_speech_prob": 2.5215626919816714e-06}, {"id": 967, "seek": 470350, "start": 4703.5, "end": 4710.46, "text": " I read it in with pandas so pandas is the thing we use in Python to do structured data analysis like CSV files", "tokens": [286, 1401, 309, 294, 365, 4565, 296, 370, 4565, 296, 307, 264, 551, 321, 764, 294, 15329, 281, 360, 18519, 1412, 5215, 411, 48814, 7098], "temperature": 0.0, "avg_logprob": -0.1816647254814536, "compression_ratio": 1.7491039426523298, "no_speech_prob": 4.356859790277667e-06}, {"id": 968, "seek": 470350, "start": 4711.02, "end": 4714.14, "text": " So if you take pandas we call PD. That's pretty much universal", "tokens": [407, 498, 291, 747, 4565, 296, 321, 818, 10464, 13, 663, 311, 1238, 709, 11455], "temperature": 0.0, "avg_logprob": -0.1816647254814536, "compression_ratio": 1.7491039426523298, "no_speech_prob": 4.356859790277667e-06}, {"id": 969, "seek": 470350, "start": 4714.86, "end": 4717.38, "text": " PD dot read CSV reads in a CSV file", "tokens": [10464, 5893, 1401, 48814, 15700, 294, 257, 48814, 3991], "temperature": 0.0, "avg_logprob": -0.1816647254814536, "compression_ratio": 1.7491039426523298, "no_speech_prob": 4.356859790277667e-06}, {"id": 970, "seek": 470350, "start": 4717.74, "end": 4722.1, "text": " we can then take a look at it and you can see that basically it had like some kind of identifier and", "tokens": [321, 393, 550, 747, 257, 574, 412, 309, 293, 291, 393, 536, 300, 1936, 309, 632, 411, 512, 733, 295, 45690, 293], "temperature": 0.0, "avg_logprob": -0.1816647254814536, "compression_ratio": 1.7491039426523298, "no_speech_prob": 4.356859790277667e-06}, {"id": 971, "seek": 470350, "start": 4722.62, "end": 4726.24, "text": " Then the debris right so this is like a different way", "tokens": [1396, 264, 21942, 558, 370, 341, 307, 411, 257, 819, 636], "temperature": 0.0, "avg_logprob": -0.1816647254814536, "compression_ratio": 1.7491039426523298, "no_speech_prob": 4.356859790277667e-06}, {"id": 972, "seek": 472624, "start": 4726.24, "end": 4733.139999999999, "text": " This is the second main way that people kind of give you image labels one is to put different images into different folders", "tokens": [639, 307, 264, 1150, 2135, 636, 300, 561, 733, 295, 976, 291, 3256, 16949, 472, 307, 281, 829, 819, 5267, 666, 819, 31082], "temperature": 0.0, "avg_logprob": -0.13759724617004396, "compression_ratio": 1.670731707317073, "no_speech_prob": 8.990934361463587e-07}, {"id": 973, "seek": 472624, "start": 4733.3, "end": 4739.46, "text": " The second is generally to give you a some kind of file like a CSV file to tell you here's the image name", "tokens": [440, 1150, 307, 5101, 281, 976, 291, 257, 512, 733, 295, 3991, 411, 257, 48814, 3991, 281, 980, 291, 510, 311, 264, 3256, 1315], "temperature": 0.0, "avg_logprob": -0.13759724617004396, "compression_ratio": 1.670731707317073, "no_speech_prob": 8.990934361463587e-07}, {"id": 974, "seek": 472624, "start": 4739.46, "end": 4742.5199999999995, "text": " And here's the label okay, so", "tokens": [400, 510, 311, 264, 7645, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.13759724617004396, "compression_ratio": 1.670731707317073, "no_speech_prob": 8.990934361463587e-07}, {"id": 975, "seek": 472624, "start": 4744.36, "end": 4746.36, "text": " What I then did was I used", "tokens": [708, 286, 550, 630, 390, 286, 1143], "temperature": 0.0, "avg_logprob": -0.13759724617004396, "compression_ratio": 1.670731707317073, "no_speech_prob": 8.990934361463587e-07}, {"id": 976, "seek": 472624, "start": 4746.82, "end": 4754.0199999999995, "text": " Pandas again to create a pivot table which basically groups it up just to see how many of each breed there were and I sorted", "tokens": [16995, 296, 797, 281, 1884, 257, 14538, 3199, 597, 1936, 3935, 309, 493, 445, 281, 536, 577, 867, 295, 1184, 18971, 456, 645, 293, 286, 25462], "temperature": 0.0, "avg_logprob": -0.13759724617004396, "compression_ratio": 1.670731707317073, "no_speech_prob": 8.990934361463587e-07}, {"id": 977, "seek": 475402, "start": 4754.02, "end": 4757.580000000001, "text": " Them and so I saw okay. They've got like about a hundred", "tokens": [37354, 293, 370, 286, 1866, 1392, 13, 814, 600, 658, 411, 466, 257, 3262], "temperature": 0.0, "avg_logprob": -0.23317426183949347, "compression_ratio": 1.6419213973799127, "no_speech_prob": 9.276341188524384e-07}, {"id": 978, "seek": 475402, "start": 4758.22, "end": 4764.700000000001, "text": " Some of the more common breeds and some of the less common breeds that got like 60 or so okay?", "tokens": [2188, 295, 264, 544, 2689, 41609, 293, 512, 295, 264, 1570, 2689, 41609, 300, 658, 411, 4060, 420, 370, 1392, 30], "temperature": 0.0, "avg_logprob": -0.23317426183949347, "compression_ratio": 1.6419213973799127, "no_speech_prob": 9.276341188524384e-07}, {"id": 979, "seek": 475402, "start": 4765.22, "end": 4770.540000000001, "text": " Altogether there are 120 rows and I've been 120 different breeds represented okay, so", "tokens": [15992, 9622, 456, 366, 10411, 13241, 293, 286, 600, 668, 10411, 819, 41609, 10379, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.23317426183949347, "compression_ratio": 1.6419213973799127, "no_speech_prob": 9.276341188524384e-07}, {"id": 980, "seek": 475402, "start": 4771.540000000001, "end": 4774.14, "text": " I'm going to go through the steps right so", "tokens": [286, 478, 516, 281, 352, 807, 264, 4439, 558, 370], "temperature": 0.0, "avg_logprob": -0.23317426183949347, "compression_ratio": 1.6419213973799127, "no_speech_prob": 9.276341188524384e-07}, {"id": 981, "seek": 475402, "start": 4775.3, "end": 4780.540000000001, "text": " Enable data augmentation so to enable data augmentation when we call this transforms from model", "tokens": [2193, 712, 1412, 14501, 19631, 370, 281, 9528, 1412, 14501, 19631, 562, 321, 818, 341, 35592, 490, 2316], "temperature": 0.0, "avg_logprob": -0.23317426183949347, "compression_ratio": 1.6419213973799127, "no_speech_prob": 9.276341188524384e-07}, {"id": 982, "seek": 478054, "start": 4780.54, "end": 4785.66, "text": " Are you just pass in and org transforms in this case? I chose side on again", "tokens": [2014, 291, 445, 1320, 294, 293, 14045, 35592, 294, 341, 1389, 30, 286, 5111, 1252, 322, 797], "temperature": 0.0, "avg_logprob": -0.2553488003310337, "compression_ratio": 1.5982905982905984, "no_speech_prob": 2.443983021294116e-06}, {"id": 983, "seek": 478054, "start": 4785.66, "end": 4788.38, "text": " These are pictures of dots and stuff so there's side on photos", "tokens": [1981, 366, 5242, 295, 15026, 293, 1507, 370, 456, 311, 1252, 322, 5787], "temperature": 0.0, "avg_logprob": -0.2553488003310337, "compression_ratio": 1.5982905982905984, "no_speech_prob": 2.443983021294116e-06}, {"id": 984, "seek": 478054, "start": 4789.38, "end": 4791.7, "text": " I will talk about max zoom", "tokens": [286, 486, 751, 466, 11469, 8863], "temperature": 0.0, "avg_logprob": -0.2553488003310337, "compression_ratio": 1.5982905982905984, "no_speech_prob": 2.443983021294116e-06}, {"id": 985, "seek": 478054, "start": 4792.58, "end": 4796.78, "text": " More detail later, but max zoom basically says when you do the data augmentation", "tokens": [5048, 2607, 1780, 11, 457, 11469, 8863, 1936, 1619, 562, 291, 360, 264, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.2553488003310337, "compression_ratio": 1.5982905982905984, "no_speech_prob": 2.443983021294116e-06}, {"id": 986, "seek": 478054, "start": 4797.66, "end": 4802.1, "text": " We like zoom into it by up to one point one times", "tokens": [492, 411, 8863, 666, 309, 538, 493, 281, 472, 935, 472, 1413], "temperature": 0.0, "avg_logprob": -0.2553488003310337, "compression_ratio": 1.5982905982905984, "no_speech_prob": 2.443983021294116e-06}, {"id": 987, "seek": 478054, "start": 4802.38, "end": 4807.22, "text": " Okay, so randomly between one the original image size and one point one times", "tokens": [1033, 11, 370, 16979, 1296, 472, 264, 3380, 3256, 2744, 293, 472, 935, 472, 1413], "temperature": 0.0, "avg_logprob": -0.2553488003310337, "compression_ratio": 1.5982905982905984, "no_speech_prob": 2.443983021294116e-06}, {"id": 988, "seek": 480722, "start": 4807.22, "end": 4813.62, "text": " So it's not always cropping out in the middle or an edge, but it could be cropping out a smaller part, okay, so", "tokens": [407, 309, 311, 406, 1009, 4848, 3759, 484, 294, 264, 2808, 420, 364, 4691, 11, 457, 309, 727, 312, 4848, 3759, 484, 257, 4356, 644, 11, 1392, 11, 370], "temperature": 0.0, "avg_logprob": -0.15376515658396595, "compression_ratio": 1.8060344827586208, "no_speech_prob": 4.425457063916838e-06}, {"id": 989, "seek": 480722, "start": 4814.5, "end": 4818.22, "text": " Having done that the key step now is that rather than going from paths?", "tokens": [10222, 1096, 300, 264, 2141, 1823, 586, 307, 300, 2831, 813, 516, 490, 14518, 30], "temperature": 0.0, "avg_logprob": -0.15376515658396595, "compression_ratio": 1.8060344827586208, "no_speech_prob": 4.425457063916838e-06}, {"id": 990, "seek": 480722, "start": 4819.3, "end": 4825.780000000001, "text": " So previously we went from paths and that tells it that the names of the folders are the names of the labels we go", "tokens": [407, 8046, 321, 1437, 490, 14518, 293, 300, 5112, 309, 300, 264, 5288, 295, 264, 31082, 366, 264, 5288, 295, 264, 16949, 321, 352], "temperature": 0.0, "avg_logprob": -0.15376515658396595, "compression_ratio": 1.8060344827586208, "no_speech_prob": 4.425457063916838e-06}, {"id": 991, "seek": 480722, "start": 4825.780000000001, "end": 4830.9400000000005, "text": " From CSV and we pass in the CSV file that contains the labels", "tokens": [3358, 48814, 293, 321, 1320, 294, 264, 48814, 3991, 300, 8306, 264, 16949], "temperature": 0.0, "avg_logprob": -0.15376515658396595, "compression_ratio": 1.8060344827586208, "no_speech_prob": 4.425457063916838e-06}, {"id": 992, "seek": 480722, "start": 4831.46, "end": 4835.18, "text": " So we're passing in the path that contains all of the data", "tokens": [407, 321, 434, 8437, 294, 264, 3100, 300, 8306, 439, 295, 264, 1412], "temperature": 0.0, "avg_logprob": -0.15376515658396595, "compression_ratio": 1.8060344827586208, "no_speech_prob": 4.425457063916838e-06}, {"id": 993, "seek": 483518, "start": 4835.18, "end": 4841.08, "text": " The name of the folder that contains the training data the CSV that contains the labels", "tokens": [440, 1315, 295, 264, 10820, 300, 8306, 264, 3097, 1412, 264, 48814, 300, 8306, 264, 16949], "temperature": 0.0, "avg_logprob": -0.16568068022369056, "compression_ratio": 1.763157894736842, "no_speech_prob": 4.5659448915102985e-06}, {"id": 994, "seek": 483518, "start": 4842.66, "end": 4847.860000000001, "text": " We need to also tell it where the test set is if we want to submit to Kaggle later talk more about that next week", "tokens": [492, 643, 281, 611, 980, 309, 689, 264, 1500, 992, 307, 498, 321, 528, 281, 10315, 281, 48751, 22631, 1780, 751, 544, 466, 300, 958, 1243], "temperature": 0.0, "avg_logprob": -0.16568068022369056, "compression_ratio": 1.763157894736842, "no_speech_prob": 4.5659448915102985e-06}, {"id": 995, "seek": 483518, "start": 4849.16, "end": 4851.16, "text": " now this time", "tokens": [586, 341, 565], "temperature": 0.0, "avg_logprob": -0.16568068022369056, "compression_ratio": 1.763157894736842, "no_speech_prob": 4.5659448915102985e-06}, {"id": 996, "seek": 483518, "start": 4852.5, "end": 4857.9400000000005, "text": " The previous data set we had I had actually separated a validation set out into a separate folder", "tokens": [440, 3894, 1412, 992, 321, 632, 286, 632, 767, 12005, 257, 24071, 992, 484, 666, 257, 4994, 10820], "temperature": 0.0, "avg_logprob": -0.16568068022369056, "compression_ratio": 1.763157894736842, "no_speech_prob": 4.5659448915102985e-06}, {"id": 997, "seek": 483518, "start": 4858.3, "end": 4863.3, "text": " Right, but in this case you'll see that there is not a separate folder called validation", "tokens": [1779, 11, 457, 294, 341, 1389, 291, 603, 536, 300, 456, 307, 406, 257, 4994, 10820, 1219, 24071], "temperature": 0.0, "avg_logprob": -0.16568068022369056, "compression_ratio": 1.763157894736842, "no_speech_prob": 4.5659448915102985e-06}, {"id": 998, "seek": 486330, "start": 4863.3, "end": 4869.2, "text": " Validation right so we want to be able to track how good our performance is locally", "tokens": [7188, 327, 399, 558, 370, 321, 528, 281, 312, 1075, 281, 2837, 577, 665, 527, 3389, 307, 16143], "temperature": 0.0, "avg_logprob": -0.12786820291102619, "compression_ratio": 1.544186046511628, "no_speech_prob": 4.181181623152952e-07}, {"id": 999, "seek": 486330, "start": 4869.2, "end": 4873.34, "text": " So we're going to have to separate some of the images out to put it into a validation set", "tokens": [407, 321, 434, 516, 281, 362, 281, 4994, 512, 295, 264, 5267, 484, 281, 829, 309, 666, 257, 24071, 992], "temperature": 0.0, "avg_logprob": -0.12786820291102619, "compression_ratio": 1.544186046511628, "no_speech_prob": 4.181181623152952e-07}, {"id": 1000, "seek": 486330, "start": 4873.66, "end": 4880.46, "text": " Okay, so I do that at random and so up here. You can see I've basically opened up the CSV file", "tokens": [1033, 11, 370, 286, 360, 300, 412, 4974, 293, 370, 493, 510, 13, 509, 393, 536, 286, 600, 1936, 5625, 493, 264, 48814, 3991], "temperature": 0.0, "avg_logprob": -0.12786820291102619, "compression_ratio": 1.544186046511628, "no_speech_prob": 4.181181623152952e-07}, {"id": 1001, "seek": 486330, "start": 4882.42, "end": 4886.66, "text": " Turned it into a list of rows and then taken the length of that", "tokens": [7956, 292, 309, 666, 257, 1329, 295, 13241, 293, 550, 2726, 264, 4641, 295, 300], "temperature": 0.0, "avg_logprob": -0.12786820291102619, "compression_ratio": 1.544186046511628, "no_speech_prob": 4.181181623152952e-07}, {"id": 1002, "seek": 488666, "start": 4886.66, "end": 4893.5199999999995, "text": " Minus one because there's a header at the top right and so that's the number of rows in the CSV file", "tokens": [2829, 301, 472, 570, 456, 311, 257, 23117, 412, 264, 1192, 558, 293, 370, 300, 311, 264, 1230, 295, 13241, 294, 264, 48814, 3991], "temperature": 0.0, "avg_logprob": -0.13294478668563667, "compression_ratio": 1.6121495327102804, "no_speech_prob": 6.5403946791775525e-06}, {"id": 1003, "seek": 488666, "start": 4893.62, "end": 4900.22, "text": " Which must be the number of images that we have and then this is a fast AI thing get cross validation indexes", "tokens": [3013, 1633, 312, 264, 1230, 295, 5267, 300, 321, 362, 293, 550, 341, 307, 257, 2370, 7318, 551, 483, 3278, 24071, 8186, 279], "temperature": 0.0, "avg_logprob": -0.13294478668563667, "compression_ratio": 1.6121495327102804, "no_speech_prob": 6.5403946791775525e-06}, {"id": 1004, "seek": 488666, "start": 4900.5, "end": 4907.66, "text": " We'll talk about cross validation later, but basically if you call this and pass in a number. It's going to return to you", "tokens": [492, 603, 751, 466, 3278, 24071, 1780, 11, 457, 1936, 498, 291, 818, 341, 293, 1320, 294, 257, 1230, 13, 467, 311, 516, 281, 2736, 281, 291], "temperature": 0.0, "avg_logprob": -0.13294478668563667, "compression_ratio": 1.6121495327102804, "no_speech_prob": 6.5403946791775525e-06}, {"id": 1005, "seek": 488666, "start": 4908.58, "end": 4910.58, "text": " by default a", "tokens": [538, 7576, 257], "temperature": 0.0, "avg_logprob": -0.13294478668563667, "compression_ratio": 1.6121495327102804, "no_speech_prob": 6.5403946791775525e-06}, {"id": 1006, "seek": 491058, "start": 4910.58, "end": 4917.9, "text": " Random 20% of the rows to use as your validation set and you can pass in parameters to get different amounts right?", "tokens": [37603, 945, 4, 295, 264, 13241, 281, 764, 382, 428, 24071, 992, 293, 291, 393, 1320, 294, 9834, 281, 483, 819, 11663, 558, 30], "temperature": 0.0, "avg_logprob": -0.17088137831643363, "compression_ratio": 1.7186147186147187, "no_speech_prob": 4.289299795345869e-06}, {"id": 1007, "seek": 491058, "start": 4917.98, "end": 4920.78, "text": " so this is now going to grab 20% of the data and", "tokens": [370, 341, 307, 586, 516, 281, 4444, 945, 4, 295, 264, 1412, 293], "temperature": 0.0, "avg_logprob": -0.17088137831643363, "compression_ratio": 1.7186147186147187, "no_speech_prob": 4.289299795345869e-06}, {"id": 1008, "seek": 491058, "start": 4921.86, "end": 4928.0199999999995, "text": " Say all right. This is the this is the indexes the numbers of the files which we're going to use as a validation set", "tokens": [6463, 439, 558, 13, 639, 307, 264, 341, 307, 264, 8186, 279, 264, 3547, 295, 264, 7098, 597, 321, 434, 516, 281, 764, 382, 257, 24071, 992], "temperature": 0.0, "avg_logprob": -0.17088137831643363, "compression_ratio": 1.7186147186147187, "no_speech_prob": 4.289299795345869e-06}, {"id": 1009, "seek": 491058, "start": 4928.14, "end": 4930.14, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.17088137831643363, "compression_ratio": 1.7186147186147187, "no_speech_prob": 4.289299795345869e-06}, {"id": 1010, "seek": 491058, "start": 4931.1, "end": 4935.0199999999995, "text": " Now that we've got that in fact. Let's kind of run this so you can see what that looks like", "tokens": [823, 300, 321, 600, 658, 300, 294, 1186, 13, 961, 311, 733, 295, 1190, 341, 370, 291, 393, 536, 437, 300, 1542, 411], "temperature": 0.0, "avg_logprob": -0.17088137831643363, "compression_ratio": 1.7186147186147187, "no_speech_prob": 4.289299795345869e-06}, {"id": 1011, "seek": 493502, "start": 4935.02, "end": 4939.5, "text": " So valve indexes is", "tokens": [407, 15294, 8186, 279, 307], "temperature": 0.0, "avg_logprob": -0.316642372696488, "compression_ratio": 1.3046875, "no_speech_prob": 5.6823751037882175e-06}, {"id": 1012, "seek": 493502, "start": 4940.46, "end": 4944.580000000001, "text": " Just a big bunch of numbers okay, and so and is", "tokens": [1449, 257, 955, 3840, 295, 3547, 1392, 11, 293, 370, 293, 307], "temperature": 0.0, "avg_logprob": -0.316642372696488, "compression_ratio": 1.3046875, "no_speech_prob": 5.6823751037882175e-06}, {"id": 1013, "seek": 493502, "start": 4946.660000000001, "end": 4950.780000000001, "text": " 10,000 right and so we have about", "tokens": [1266, 11, 1360, 558, 293, 370, 321, 362, 466], "temperature": 0.0, "avg_logprob": -0.316642372696488, "compression_ratio": 1.3046875, "no_speech_prob": 5.6823751037882175e-06}, {"id": 1014, "seek": 493502, "start": 4951.700000000001, "end": 4954.4400000000005, "text": " 20% of those is going to be in the validation set", "tokens": [945, 4, 295, 729, 307, 516, 281, 312, 294, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.316642372696488, "compression_ratio": 1.3046875, "no_speech_prob": 5.6823751037882175e-06}, {"id": 1015, "seek": 493502, "start": 4955.3, "end": 4957.3, "text": " so when we call", "tokens": [370, 562, 321, 818], "temperature": 0.0, "avg_logprob": -0.316642372696488, "compression_ratio": 1.3046875, "no_speech_prob": 5.6823751037882175e-06}, {"id": 1016, "seek": 495730, "start": 4957.3, "end": 4961.3, "text": " From CSV", "tokens": [3358, 48814], "temperature": 0.0, "avg_logprob": -0.29745919825667044, "compression_ratio": 1.4452554744525548, "no_speech_prob": 5.862761099706404e-06}, {"id": 1017, "seek": 495730, "start": 4963.46, "end": 4970.14, "text": " We can pass in a parameter which is to tell it which indexes to treat as a validation set and so let's pass in those", "tokens": [492, 393, 1320, 294, 257, 13075, 597, 307, 281, 980, 309, 597, 8186, 279, 281, 2387, 382, 257, 24071, 992, 293, 370, 718, 311, 1320, 294, 729], "temperature": 0.0, "avg_logprob": -0.29745919825667044, "compression_ratio": 1.4452554744525548, "no_speech_prob": 5.862761099706404e-06}, {"id": 1018, "seek": 495730, "start": 4970.14, "end": 4972.14, "text": " indexes", "tokens": [8186, 279], "temperature": 0.0, "avg_logprob": -0.29745919825667044, "compression_ratio": 1.4452554744525548, "no_speech_prob": 5.862761099706404e-06}, {"id": 1019, "seek": 495730, "start": 4972.38, "end": 4975.26, "text": " One thing that's a little bit tricky here is that", "tokens": [1485, 551, 300, 311, 257, 707, 857, 12414, 510, 307, 300], "temperature": 0.0, "avg_logprob": -0.29745919825667044, "compression_ratio": 1.4452554744525548, "no_speech_prob": 5.862761099706404e-06}, {"id": 1020, "seek": 495730, "start": 4977.62, "end": 4979.62, "text": " The", "tokens": [440], "temperature": 0.0, "avg_logprob": -0.29745919825667044, "compression_ratio": 1.4452554744525548, "no_speech_prob": 5.862761099706404e-06}, {"id": 1021, "seek": 495730, "start": 4980.34, "end": 4982.34, "text": " File names", "tokens": [26196, 5288], "temperature": 0.0, "avg_logprob": -0.29745919825667044, "compression_ratio": 1.4452554744525548, "no_speech_prob": 5.862761099706404e-06}, {"id": 1022, "seek": 498234, "start": 4982.34, "end": 4988.5, "text": " Actually have I checked they actually have a dot JPG on the end and these obviously don't have a dot JPG", "tokens": [5135, 362, 286, 10033, 436, 767, 362, 257, 5893, 34336, 38, 322, 264, 917, 293, 613, 2745, 500, 380, 362, 257, 5893, 34336, 38], "temperature": 0.0, "avg_logprob": -0.17348748386496365, "compression_ratio": 1.6964285714285714, "no_speech_prob": 2.190766963394708e-06}, {"id": 1023, "seek": 498234, "start": 4988.82, "end": 4993.14, "text": " So you can pass in when you call from CSV you can pass in a suffix", "tokens": [407, 291, 393, 1320, 294, 562, 291, 818, 490, 48814, 291, 393, 1320, 294, 257, 3889, 970], "temperature": 0.0, "avg_logprob": -0.17348748386496365, "compression_ratio": 1.6964285714285714, "no_speech_prob": 2.190766963394708e-06}, {"id": 1024, "seek": 498234, "start": 4993.34, "end": 4998.7, "text": " That says that the labels don't actually contain the full file names you need to add this to them, okay?", "tokens": [663, 1619, 300, 264, 16949, 500, 380, 767, 5304, 264, 1577, 3991, 5288, 291, 643, 281, 909, 341, 281, 552, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.17348748386496365, "compression_ratio": 1.6964285714285714, "no_speech_prob": 2.190766963394708e-06}, {"id": 1025, "seek": 498234, "start": 5001.66, "end": 5005.22, "text": " So that's basically all I need to do to set up my data", "tokens": [407, 300, 311, 1936, 439, 286, 643, 281, 360, 281, 992, 493, 452, 1412], "temperature": 0.0, "avg_logprob": -0.17348748386496365, "compression_ratio": 1.6964285714285714, "no_speech_prob": 2.190766963394708e-06}, {"id": 1026, "seek": 498234, "start": 5005.9400000000005, "end": 5008.9400000000005, "text": " And as a lot of you have noticed during the week", "tokens": [400, 382, 257, 688, 295, 291, 362, 5694, 1830, 264, 1243], "temperature": 0.0, "avg_logprob": -0.17348748386496365, "compression_ratio": 1.6964285714285714, "no_speech_prob": 2.190766963394708e-06}, {"id": 1027, "seek": 500894, "start": 5008.94, "end": 5016.74, "text": " Inside that data object you can actually get access to the data set by call the training data set by saying train DS", "tokens": [15123, 300, 1412, 2657, 291, 393, 767, 483, 2105, 281, 264, 1412, 992, 538, 818, 264, 3097, 1412, 992, 538, 1566, 3847, 15816], "temperature": 0.0, "avg_logprob": -0.22246215457007998, "compression_ratio": 1.8398268398268398, "no_speech_prob": 2.4824673801049357e-06}, {"id": 1028, "seek": 500894, "start": 5017.299999999999, "end": 5021.28, "text": " And inside train DS is a whole bunch of things including the file names", "tokens": [400, 1854, 3847, 15816, 307, 257, 1379, 3840, 295, 721, 3009, 264, 3991, 5288], "temperature": 0.0, "avg_logprob": -0.22246215457007998, "compression_ratio": 1.8398268398268398, "no_speech_prob": 2.4824673801049357e-06}, {"id": 1029, "seek": 500894, "start": 5021.54, "end": 5028.12, "text": " Okay, so train DS dot file names contains all of the file names of everything in the training set and so here's like one", "tokens": [1033, 11, 370, 3847, 15816, 5893, 3991, 5288, 8306, 439, 295, 264, 3991, 5288, 295, 1203, 294, 264, 3097, 992, 293, 370, 510, 311, 411, 472], "temperature": 0.0, "avg_logprob": -0.22246215457007998, "compression_ratio": 1.8398268398268398, "no_speech_prob": 2.4824673801049357e-06}, {"id": 1030, "seek": 500894, "start": 5028.12, "end": 5030.86, "text": " Clowning okay, so here's an example of one file name", "tokens": [2033, 648, 278, 1392, 11, 370, 510, 311, 364, 1365, 295, 472, 3991, 1315], "temperature": 0.0, "avg_logprob": -0.22246215457007998, "compression_ratio": 1.8398268398268398, "no_speech_prob": 2.4824673801049357e-06}, {"id": 1031, "seek": 500894, "start": 5032.259999999999, "end": 5035.7, "text": " So I can now go ahead and open that file and take a look at it", "tokens": [407, 286, 393, 586, 352, 2286, 293, 1269, 300, 3991, 293, 747, 257, 574, 412, 309], "temperature": 0.0, "avg_logprob": -0.22246215457007998, "compression_ratio": 1.8398268398268398, "no_speech_prob": 2.4824673801049357e-06}, {"id": 1032, "seek": 503570, "start": 5035.7, "end": 5040.7, "text": " All right, so that's the next thing I did was to try and understand what my file my data set looks like and it", "tokens": [1057, 558, 11, 370, 300, 311, 264, 958, 551, 286, 630, 390, 281, 853, 293, 1223, 437, 452, 3991, 452, 1412, 992, 1542, 411, 293, 309], "temperature": 0.0, "avg_logprob": -0.16130442185835406, "compression_ratio": 1.8, "no_speech_prob": 8.398023965128232e-06}, {"id": 1033, "seek": 503570, "start": 5040.9, "end": 5044.94, "text": " Found an adorable puppy so that was very nice so feeling good about this", "tokens": [8207, 364, 18698, 18196, 370, 300, 390, 588, 1481, 370, 2633, 665, 466, 341], "temperature": 0.0, "avg_logprob": -0.16130442185835406, "compression_ratio": 1.8, "no_speech_prob": 8.398023965128232e-06}, {"id": 1034, "seek": 503570, "start": 5044.94, "end": 5049.92, "text": " I also want to know like how big are these files right like how big are the images?", "tokens": [286, 611, 528, 281, 458, 411, 577, 955, 366, 613, 7098, 558, 411, 577, 955, 366, 264, 5267, 30], "temperature": 0.0, "avg_logprob": -0.16130442185835406, "compression_ratio": 1.8, "no_speech_prob": 8.398023965128232e-06}, {"id": 1035, "seek": 503570, "start": 5050.22, "end": 5055.92, "text": " Because that's a key issue if they're huge and then I have to think really carefully about how to deal with huge images", "tokens": [1436, 300, 311, 257, 2141, 2734, 498, 436, 434, 2603, 293, 550, 286, 362, 281, 519, 534, 7500, 466, 577, 281, 2028, 365, 2603, 5267], "temperature": 0.0, "avg_logprob": -0.16130442185835406, "compression_ratio": 1.8, "no_speech_prob": 8.398023965128232e-06}, {"id": 1036, "seek": 503570, "start": 5055.92, "end": 5060.139999999999, "text": " That's really challenging if they're tiny well. That's also challenging", "tokens": [663, 311, 534, 7595, 498, 436, 434, 5870, 731, 13, 663, 311, 611, 7595], "temperature": 0.0, "avg_logprob": -0.16130442185835406, "compression_ratio": 1.8, "no_speech_prob": 8.398023965128232e-06}, {"id": 1037, "seek": 506014, "start": 5060.14, "end": 5066.54, "text": " Most of image net models are trained on either 224 by 224 or 299 by 299", "tokens": [4534, 295, 3256, 2533, 5245, 366, 8895, 322, 2139, 5853, 19, 538, 5853, 19, 420, 568, 8494, 538, 568, 8494], "temperature": 0.0, "avg_logprob": -0.183195197791384, "compression_ratio": 1.6725978647686832, "no_speech_prob": 1.4144713986752322e-06}, {"id": 1038, "seek": 506014, "start": 5066.900000000001, "end": 5071.820000000001, "text": " Images so anytime you have images in that kind of range. That's that's really hopeful", "tokens": [4331, 1660, 370, 13038, 291, 362, 5267, 294, 300, 733, 295, 3613, 13, 663, 311, 300, 311, 534, 20531], "temperature": 0.0, "avg_logprob": -0.183195197791384, "compression_ratio": 1.6725978647686832, "no_speech_prob": 1.4144713986752322e-06}, {"id": 1039, "seek": 506014, "start": 5071.820000000001, "end": 5077.18, "text": " You're probably not going to do too much different in this case the first image. I looked at was about the right size", "tokens": [509, 434, 1391, 406, 516, 281, 360, 886, 709, 819, 294, 341, 1389, 264, 700, 3256, 13, 286, 2956, 412, 390, 466, 264, 558, 2744], "temperature": 0.0, "avg_logprob": -0.183195197791384, "compression_ratio": 1.6725978647686832, "no_speech_prob": 1.4144713986752322e-06}, {"id": 1040, "seek": 506014, "start": 5077.18, "end": 5079.18, "text": " So I'm thinking oh, it's looking pretty hopeful", "tokens": [407, 286, 478, 1953, 1954, 11, 309, 311, 1237, 1238, 20531], "temperature": 0.0, "avg_logprob": -0.183195197791384, "compression_ratio": 1.6725978647686832, "no_speech_prob": 1.4144713986752322e-06}, {"id": 1041, "seek": 506014, "start": 5079.860000000001, "end": 5087.1, "text": " So what I did then is I created a dictionary comprehension now if you don't know about list comprehensions and dictionary comprehensions in Python", "tokens": [407, 437, 286, 630, 550, 307, 286, 2942, 257, 25890, 44991, 586, 498, 291, 500, 380, 458, 466, 1329, 10753, 8302, 293, 25890, 10753, 8302, 294, 15329], "temperature": 0.0, "avg_logprob": -0.183195197791384, "compression_ratio": 1.6725978647686832, "no_speech_prob": 1.4144713986752322e-06}, {"id": 1042, "seek": 508710, "start": 5087.1, "end": 5091.5, "text": " Go study them. They're the most useful thing super handy", "tokens": [1037, 2979, 552, 13, 814, 434, 264, 881, 4420, 551, 1687, 13239], "temperature": 0.0, "avg_logprob": -0.20254882176717123, "compression_ratio": 1.7203065134099618, "no_speech_prob": 4.092878953088075e-06}, {"id": 1043, "seek": 508710, "start": 5091.9800000000005, "end": 5095.26, "text": " You can see the basic idea here is that are going through all of the files", "tokens": [509, 393, 536, 264, 3875, 1558, 510, 307, 300, 366, 516, 807, 439, 295, 264, 7098], "temperature": 0.0, "avg_logprob": -0.20254882176717123, "compression_ratio": 1.7203065134099618, "no_speech_prob": 4.092878953088075e-06}, {"id": 1044, "seek": 508710, "start": 5095.26, "end": 5102.02, "text": " And I'm creating a dictionary that maps the name of the file to the size of that file", "tokens": [400, 286, 478, 4084, 257, 25890, 300, 11317, 264, 1315, 295, 264, 3991, 281, 264, 2744, 295, 300, 3991], "temperature": 0.0, "avg_logprob": -0.20254882176717123, "compression_ratio": 1.7203065134099618, "no_speech_prob": 4.092878953088075e-06}, {"id": 1045, "seek": 508710, "start": 5104.38, "end": 5106.46, "text": " Again this is a handy little Python feature", "tokens": [3764, 341, 307, 257, 13239, 707, 15329, 4111], "temperature": 0.0, "avg_logprob": -0.20254882176717123, "compression_ratio": 1.7203065134099618, "no_speech_prob": 4.092878953088075e-06}, {"id": 1046, "seek": 508710, "start": 5106.46, "end": 5109.04, "text": " Which I'll let you think learn about during the week if you don't know about it", "tokens": [3013, 286, 603, 718, 291, 519, 1466, 466, 1830, 264, 1243, 498, 291, 500, 380, 458, 466, 309], "temperature": 0.0, "avg_logprob": -0.20254882176717123, "compression_ratio": 1.7203065134099618, "no_speech_prob": 4.092878953088075e-06}, {"id": 1047, "seek": 508710, "start": 5109.04, "end": 5114.4400000000005, "text": " which is zip and using this special star notation is now going to take this dictionary and turn it into the", "tokens": [597, 307, 20730, 293, 1228, 341, 2121, 3543, 24657, 307, 586, 516, 281, 747, 341, 25890, 293, 1261, 309, 666, 264], "temperature": 0.0, "avg_logprob": -0.20254882176717123, "compression_ratio": 1.7203065134099618, "no_speech_prob": 4.092878953088075e-06}, {"id": 1048, "seek": 511444, "start": 5114.44, "end": 5117.799999999999, "text": " rows and the columns", "tokens": [13241, 293, 264, 13766], "temperature": 0.0, "avg_logprob": -0.2518850366274516, "compression_ratio": 1.5512820512820513, "no_speech_prob": 7.766881935822312e-06}, {"id": 1049, "seek": 511444, "start": 5118.719999999999, "end": 5123.839999999999, "text": " And so I can now turn those into numpy arrays and like okay here are the first five", "tokens": [400, 370, 286, 393, 586, 1261, 729, 666, 1031, 8200, 41011, 293, 411, 1392, 510, 366, 264, 700, 1732], "temperature": 0.0, "avg_logprob": -0.2518850366274516, "compression_ratio": 1.5512820512820513, "no_speech_prob": 7.766881935822312e-06}, {"id": 1050, "seek": 511444, "start": 5125.08, "end": 5127.44, "text": " row sizes for each of my images and", "tokens": [5386, 11602, 337, 1184, 295, 452, 5267, 293], "temperature": 0.0, "avg_logprob": -0.2518850366274516, "compression_ratio": 1.5512820512820513, "no_speech_prob": 7.766881935822312e-06}, {"id": 1051, "seek": 511444, "start": 5127.96, "end": 5133.919999999999, "text": " Then matplotlib is something you want to be very familiar with if you do any kind of data science on machine learning in Python", "tokens": [1396, 3803, 564, 310, 38270, 307, 746, 291, 528, 281, 312, 588, 4963, 365, 498, 291, 360, 604, 733, 295, 1412, 3497, 322, 3479, 2539, 294, 15329], "temperature": 0.0, "avg_logprob": -0.2518850366274516, "compression_ratio": 1.5512820512820513, "no_speech_prob": 7.766881935822312e-06}, {"id": 1052, "seek": 511444, "start": 5134.04, "end": 5141.12, "text": " Matplotlib we always refer to as PLT. So it's just a histogram and so I got a histogram of the", "tokens": [6789, 564, 310, 38270, 321, 1009, 2864, 281, 382, 6999, 51, 13, 407, 309, 311, 445, 257, 49816, 293, 370, 286, 658, 257, 49816, 295, 264], "temperature": 0.0, "avg_logprob": -0.2518850366274516, "compression_ratio": 1.5512820512820513, "no_speech_prob": 7.766881935822312e-06}, {"id": 1053, "seek": 514112, "start": 5141.12, "end": 5145.88, "text": " How high how many rows there are in each image so you can see here?", "tokens": [1012, 1090, 577, 867, 13241, 456, 366, 294, 1184, 3256, 370, 291, 393, 536, 510, 30], "temperature": 0.0, "avg_logprob": -0.12879937489827473, "compression_ratio": 1.7372262773722629, "no_speech_prob": 7.527912657678826e-06}, {"id": 1054, "seek": 514112, "start": 5145.88, "end": 5148.68, "text": " I'm kind of getting a sense before I start doing any modeling", "tokens": [286, 478, 733, 295, 1242, 257, 2020, 949, 286, 722, 884, 604, 15983], "temperature": 0.0, "avg_logprob": -0.12879937489827473, "compression_ratio": 1.7372262773722629, "no_speech_prob": 7.527912657678826e-06}, {"id": 1055, "seek": 514112, "start": 5148.68, "end": 5152.58, "text": " I kind of need to know what I'm modeling with and I can see some of the images are going to be like", "tokens": [286, 733, 295, 643, 281, 458, 437, 286, 478, 15983, 365, 293, 286, 393, 536, 512, 295, 264, 5267, 366, 516, 281, 312, 411], "temperature": 0.0, "avg_logprob": -0.12879937489827473, "compression_ratio": 1.7372262773722629, "no_speech_prob": 7.527912657678826e-06}, {"id": 1056, "seek": 514112, "start": 5153.12, "end": 5157.24, "text": " 2500 3000 pixels high but most of them seem to be around 500", "tokens": [41171, 20984, 18668, 1090, 457, 881, 295, 552, 1643, 281, 312, 926, 5923], "temperature": 0.0, "avg_logprob": -0.12879937489827473, "compression_ratio": 1.7372262773722629, "no_speech_prob": 7.527912657678826e-06}, {"id": 1057, "seek": 514112, "start": 5158.08, "end": 5163.04, "text": " So given that so few of them were bigger than a thousand I use standard numpy", "tokens": [407, 2212, 300, 370, 1326, 295, 552, 645, 3801, 813, 257, 4714, 286, 764, 3832, 1031, 8200], "temperature": 0.0, "avg_logprob": -0.12879937489827473, "compression_ratio": 1.7372262773722629, "no_speech_prob": 7.527912657678826e-06}, {"id": 1058, "seek": 514112, "start": 5163.92, "end": 5169.28, "text": " Slicing to just grab those that are smaller than a thousand and histogram that just to zoom in a little bit", "tokens": [318, 1050, 278, 281, 445, 4444, 729, 300, 366, 4356, 813, 257, 4714, 293, 49816, 300, 445, 281, 8863, 294, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.12879937489827473, "compression_ratio": 1.7372262773722629, "no_speech_prob": 7.527912657678826e-06}, {"id": 1059, "seek": 516928, "start": 5169.28, "end": 5176.36, "text": " And I can see here alright. It looks like yeah the vast majority around 500 and so this actually also prints out", "tokens": [400, 286, 393, 536, 510, 5845, 13, 467, 1542, 411, 1338, 264, 8369, 6286, 926, 5923, 293, 370, 341, 767, 611, 22305, 484], "temperature": 0.0, "avg_logprob": -0.29806416209151104, "compression_ratio": 1.5492957746478873, "no_speech_prob": 6.96218421580852e-06}, {"id": 1060, "seek": 516928, "start": 5176.96, "end": 5182.9, "text": " The histogram so I can actually go through and I can see here for 4,500 of them are about 450", "tokens": [440, 49816, 370, 286, 393, 767, 352, 807, 293, 286, 393, 536, 510, 337, 1017, 11, 7526, 295, 552, 366, 466, 26034], "temperature": 0.0, "avg_logprob": -0.29806416209151104, "compression_ratio": 1.5492957746478873, "no_speech_prob": 6.96218421580852e-06}, {"id": 1061, "seek": 516928, "start": 5183.0, "end": 5185.32, "text": " Okay, so I get about that sense about you now", "tokens": [1033, 11, 370, 286, 483, 466, 300, 2020, 466, 291, 586], "temperature": 0.0, "avg_logprob": -0.29806416209151104, "compression_ratio": 1.5492957746478873, "no_speech_prob": 6.96218421580852e-06}, {"id": 1062, "seek": 518532, "start": 5185.32, "end": 5197.16, "text": " So Jeremy how many images should we get in the validation set is always a 20%", "tokens": [407, 17809, 577, 867, 5267, 820, 321, 483, 294, 264, 24071, 992, 307, 1009, 257, 945, 4], "temperature": 0.0, "avg_logprob": -0.22094871958748238, "compression_ratio": 1.5, "no_speech_prob": 3.905424364347709e-06}, {"id": 1063, "seek": 518532, "start": 5199.2, "end": 5201.679999999999, "text": " So the size of the validation set like", "tokens": [407, 264, 2744, 295, 264, 24071, 992, 411], "temperature": 0.0, "avg_logprob": -0.22094871958748238, "compression_ratio": 1.5, "no_speech_prob": 3.905424364347709e-06}, {"id": 1064, "seek": 518532, "start": 5205.04, "end": 5212.12, "text": " Using 20% is fine unless you kind of feeling like oh my data is my data sets really small. I'm not sure that's enough", "tokens": [11142, 945, 4, 307, 2489, 5969, 291, 733, 295, 2633, 411, 1954, 452, 1412, 307, 452, 1412, 6352, 534, 1359, 13, 286, 478, 406, 988, 300, 311, 1547], "temperature": 0.0, "avg_logprob": -0.22094871958748238, "compression_ratio": 1.5, "no_speech_prob": 3.905424364347709e-06}, {"id": 1065, "seek": 521212, "start": 5212.12, "end": 5214.12, "text": " you", "tokens": [291], "temperature": 0.0, "avg_logprob": -0.19334991772969565, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.051146341524145e-06}, {"id": 1066, "seek": 521212, "start": 5214.2, "end": 5215.72, "text": " Know like", "tokens": [10265, 411], "temperature": 0.0, "avg_logprob": -0.19334991772969565, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.051146341524145e-06}, {"id": 1067, "seek": 521212, "start": 5215.72, "end": 5217.48, "text": " if you've got", "tokens": [498, 291, 600, 658], "temperature": 0.0, "avg_logprob": -0.19334991772969565, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.051146341524145e-06}, {"id": 1068, "seek": 521212, "start": 5217.48, "end": 5220.96, "text": " Basically think of it this way if you train like the same model multiple times", "tokens": [8537, 519, 295, 309, 341, 636, 498, 291, 3847, 411, 264, 912, 2316, 3866, 1413], "temperature": 0.0, "avg_logprob": -0.19334991772969565, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.051146341524145e-06}, {"id": 1069, "seek": 521212, "start": 5220.96, "end": 5227.68, "text": " And you're getting very different validation set results and your validation sets kind of small like smaller than a thousand or so", "tokens": [400, 291, 434, 1242, 588, 819, 24071, 992, 3542, 293, 428, 24071, 6352, 733, 295, 1359, 411, 4356, 813, 257, 4714, 420, 370], "temperature": 0.0, "avg_logprob": -0.19334991772969565, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.051146341524145e-06}, {"id": 1070, "seek": 521212, "start": 5228.92, "end": 5232.92, "text": " Then it's going to be quite hard to interpret how well you're doing now", "tokens": [1396, 309, 311, 516, 281, 312, 1596, 1152, 281, 7302, 577, 731, 291, 434, 884, 586], "temperature": 0.0, "avg_logprob": -0.19334991772969565, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.051146341524145e-06}, {"id": 1071, "seek": 521212, "start": 5232.92, "end": 5237.74, "text": " This is particularly true like if you're like if you care about the third decimal place of accuracy", "tokens": [639, 307, 4098, 2074, 411, 498, 291, 434, 411, 498, 291, 1127, 466, 264, 2636, 26601, 1081, 295, 14170], "temperature": 0.0, "avg_logprob": -0.19334991772969565, "compression_ratio": 1.7041666666666666, "no_speech_prob": 1.051146341524145e-06}, {"id": 1072, "seek": 523774, "start": 5237.74, "end": 5244.82, "text": " And you've got like a thousand things in your validation set then you're going about like a single image changing class is changing", "tokens": [400, 291, 600, 658, 411, 257, 4714, 721, 294, 428, 24071, 992, 550, 291, 434, 516, 466, 411, 257, 2167, 3256, 4473, 1508, 307, 4473], "temperature": 0.0, "avg_logprob": -0.23489255812561627, "compression_ratio": 1.8089430894308942, "no_speech_prob": 1.5779545492478064e-06}, {"id": 1073, "seek": 523774, "start": 5244.82, "end": 5246.82, "text": " You know is what you're looking at so", "tokens": [509, 458, 307, 437, 291, 434, 1237, 412, 370], "temperature": 0.0, "avg_logprob": -0.23489255812561627, "compression_ratio": 1.8089430894308942, "no_speech_prob": 1.5779545492478064e-06}, {"id": 1074, "seek": 523774, "start": 5247.46, "end": 5252.3, "text": " It's it really depends on like how accurate you how much difference you care about", "tokens": [467, 311, 309, 534, 5946, 322, 411, 577, 8559, 291, 577, 709, 2649, 291, 1127, 466], "temperature": 0.0, "avg_logprob": -0.23489255812561627, "compression_ratio": 1.8089430894308942, "no_speech_prob": 1.5779545492478064e-06}, {"id": 1075, "seek": 523774, "start": 5252.98, "end": 5254.62, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.23489255812561627, "compression_ratio": 1.8089430894308942, "no_speech_prob": 1.5779545492478064e-06}, {"id": 1076, "seek": 523774, "start": 5254.62, "end": 5259.38, "text": " Would say in general like at the point where you care about the difference train like I don't know point", "tokens": [6068, 584, 294, 2674, 411, 412, 264, 935, 689, 291, 1127, 466, 264, 2649, 3847, 411, 286, 500, 380, 458, 935], "temperature": 0.0, "avg_logprob": -0.23489255812561627, "compression_ratio": 1.8089430894308942, "no_speech_prob": 1.5779545492478064e-06}, {"id": 1077, "seek": 523774, "start": 5259.38, "end": 5263.34, "text": " Oh one and point oh two like the second decimal place you want that to represent like", "tokens": [876, 472, 293, 935, 1954, 732, 411, 264, 1150, 26601, 1081, 291, 528, 300, 281, 2906, 411], "temperature": 0.0, "avg_logprob": -0.23489255812561627, "compression_ratio": 1.8089430894308942, "no_speech_prob": 1.5779545492478064e-06}, {"id": 1078, "seek": 526334, "start": 5263.34, "end": 5270.42, "text": " Ten or twenty rows you know like I'm changing the class of like ten or twenty rows then", "tokens": [9380, 420, 7699, 13241, 291, 458, 411, 286, 478, 4473, 264, 1508, 295, 411, 2064, 420, 7699, 13241, 550], "temperature": 0.0, "avg_logprob": -0.22276902967883694, "compression_ratio": 1.5906976744186045, "no_speech_prob": 6.893589556966617e-07}, {"id": 1079, "seek": 526334, "start": 5271.14, "end": 5273.14, "text": " That's something you can be pretty confident on", "tokens": [663, 311, 746, 291, 393, 312, 1238, 6679, 322], "temperature": 0.0, "avg_logprob": -0.22276902967883694, "compression_ratio": 1.5906976744186045, "no_speech_prob": 6.893589556966617e-07}, {"id": 1080, "seek": 526334, "start": 5273.9800000000005, "end": 5275.9800000000005, "text": " So like most of the time", "tokens": [407, 411, 881, 295, 264, 565], "temperature": 0.0, "avg_logprob": -0.22276902967883694, "compression_ratio": 1.5906976744186045, "no_speech_prob": 6.893589556966617e-07}, {"id": 1081, "seek": 526334, "start": 5276.46, "end": 5279.14, "text": " You know given the data sizes. We normally have", "tokens": [509, 458, 2212, 264, 1412, 11602, 13, 492, 5646, 362], "temperature": 0.0, "avg_logprob": -0.22276902967883694, "compression_ratio": 1.5906976744186045, "no_speech_prob": 6.893589556966617e-07}, {"id": 1082, "seek": 526334, "start": 5280.06, "end": 5282.06, "text": " 20% things to work fine", "tokens": [945, 4, 721, 281, 589, 2489], "temperature": 0.0, "avg_logprob": -0.22276902967883694, "compression_ratio": 1.5906976744186045, "no_speech_prob": 6.893589556966617e-07}, {"id": 1083, "seek": 526334, "start": 5283.1, "end": 5286.42, "text": " But yeah, it's it's it's kind of a it depends a lot on", "tokens": [583, 1338, 11, 309, 311, 309, 311, 309, 311, 733, 295, 257, 309, 5946, 257, 688, 322], "temperature": 0.0, "avg_logprob": -0.22276902967883694, "compression_ratio": 1.5906976744186045, "no_speech_prob": 6.893589556966617e-07}, {"id": 1084, "seek": 526334, "start": 5287.82, "end": 5289.82, "text": " Specifically what you're doing and what you care about", "tokens": [26058, 437, 291, 434, 884, 293, 437, 291, 1127, 466], "temperature": 0.0, "avg_logprob": -0.22276902967883694, "compression_ratio": 1.5906976744186045, "no_speech_prob": 6.893589556966617e-07}, {"id": 1085, "seek": 528982, "start": 5289.82, "end": 5291.82, "text": " And", "tokens": [400], "temperature": 0.0, "avg_logprob": -0.17767663435502487, "compression_ratio": 1.6512455516014235, "no_speech_prob": 8.26778523332905e-06}, {"id": 1086, "seek": 528982, "start": 5291.82, "end": 5297.5, "text": " It's not it's not a deep learning specific question either you know so those who are interested in this kind of thing", "tokens": [467, 311, 406, 309, 311, 406, 257, 2452, 2539, 2685, 1168, 2139, 291, 458, 370, 729, 567, 366, 3102, 294, 341, 733, 295, 551], "temperature": 0.0, "avg_logprob": -0.17767663435502487, "compression_ratio": 1.6512455516014235, "no_speech_prob": 8.26778523332905e-06}, {"id": 1087, "seek": 528982, "start": 5297.5, "end": 5300.34, "text": " We've got to look into it a lot more detail in our machine learning course", "tokens": [492, 600, 658, 281, 574, 666, 309, 257, 688, 544, 2607, 294, 527, 3479, 2539, 1164], "temperature": 0.0, "avg_logprob": -0.17767663435502487, "compression_ratio": 1.6512455516014235, "no_speech_prob": 8.26778523332905e-06}, {"id": 1088, "seek": 528982, "start": 5301.219999999999, "end": 5304.639999999999, "text": " Which will also be available online?", "tokens": [3013, 486, 611, 312, 2435, 2950, 30], "temperature": 0.0, "avg_logprob": -0.17767663435502487, "compression_ratio": 1.6512455516014235, "no_speech_prob": 8.26778523332905e-06}, {"id": 1089, "seek": 528982, "start": 5307.38, "end": 5312.34, "text": " Okay, so I did the same thing for the columns just to make sure that these aren't like super wide and I got similar results", "tokens": [1033, 11, 370, 286, 630, 264, 912, 551, 337, 264, 13766, 445, 281, 652, 988, 300, 613, 3212, 380, 411, 1687, 4874, 293, 286, 658, 2531, 3542], "temperature": 0.0, "avg_logprob": -0.17767663435502487, "compression_ratio": 1.6512455516014235, "no_speech_prob": 8.26778523332905e-06}, {"id": 1090, "seek": 528982, "start": 5312.54, "end": 5317.5, "text": " And checked in and again found there kind of like four or five hundred seemed to be about the average size", "tokens": [400, 10033, 294, 293, 797, 1352, 456, 733, 295, 411, 1451, 420, 1732, 3262, 6576, 281, 312, 466, 264, 4274, 2744], "temperature": 0.0, "avg_logprob": -0.17767663435502487, "compression_ratio": 1.6512455516014235, "no_speech_prob": 8.26778523332905e-06}, {"id": 1091, "seek": 531750, "start": 5317.5, "end": 5320.3, "text": " So based on all of that I kind of thought okay", "tokens": [407, 2361, 322, 439, 295, 300, 286, 733, 295, 1194, 1392], "temperature": 0.0, "avg_logprob": -0.11703880876302719, "compression_ratio": 1.8333333333333333, "no_speech_prob": 5.173873887542868e-06}, {"id": 1092, "seek": 531750, "start": 5320.3, "end": 5325.5, "text": " This looks like a pretty normal kind of image data set that I can probably use pretty normal kinds of models on", "tokens": [639, 1542, 411, 257, 1238, 2710, 733, 295, 3256, 1412, 992, 300, 286, 393, 1391, 764, 1238, 2710, 3685, 295, 5245, 322], "temperature": 0.0, "avg_logprob": -0.11703880876302719, "compression_ratio": 1.8333333333333333, "no_speech_prob": 5.173873887542868e-06}, {"id": 1093, "seek": 531750, "start": 5325.5, "end": 5331.3, "text": " I was also particularly encouraged to see that when I looked at the dog that the dog like takes up most of the frame", "tokens": [286, 390, 611, 4098, 14658, 281, 536, 300, 562, 286, 2956, 412, 264, 3000, 300, 264, 3000, 411, 2516, 493, 881, 295, 264, 3920], "temperature": 0.0, "avg_logprob": -0.11703880876302719, "compression_ratio": 1.8333333333333333, "no_speech_prob": 5.173873887542868e-06}, {"id": 1094, "seek": 531750, "start": 5331.54, "end": 5334.18, "text": " Right so I'm not too worried about like cropping problems", "tokens": [1779, 370, 286, 478, 406, 886, 5804, 466, 411, 4848, 3759, 2740], "temperature": 0.0, "avg_logprob": -0.11703880876302719, "compression_ratio": 1.8333333333333333, "no_speech_prob": 5.173873887542868e-06}, {"id": 1095, "seek": 531750, "start": 5334.18, "end": 5335.42, "text": " You know", "tokens": [509, 458], "temperature": 0.0, "avg_logprob": -0.11703880876302719, "compression_ratio": 1.8333333333333333, "no_speech_prob": 5.173873887542868e-06}, {"id": 1096, "seek": 531750, "start": 5335.42, "end": 5341.74, "text": " If the if the dog was just like a tiny little piece of one little corner that I'd be thinking about doing different", "tokens": [759, 264, 498, 264, 3000, 390, 445, 411, 257, 5870, 707, 2522, 295, 472, 707, 4538, 300, 286, 1116, 312, 1953, 466, 884, 819], "temperature": 0.0, "avg_logprob": -0.11703880876302719, "compression_ratio": 1.8333333333333333, "no_speech_prob": 5.173873887542868e-06}, {"id": 1097, "seek": 534174, "start": 5341.74, "end": 5349.179999999999, "text": " You know maybe zooming in a lot more or something like a medical imaging that happens a lot like often the tumor or the cell", "tokens": [509, 458, 1310, 48226, 294, 257, 688, 544, 420, 746, 411, 257, 4625, 25036, 300, 2314, 257, 688, 411, 2049, 264, 22512, 420, 264, 2815], "temperature": 0.0, "avg_logprob": -0.17645296486475134, "compression_ratio": 1.6104417670682731, "no_speech_prob": 2.6016034553322243e-06}, {"id": 1098, "seek": 534174, "start": 5349.179999999999, "end": 5352.46, "text": " Whatever is like one tiny piece, and that's much more complex", "tokens": [8541, 307, 411, 472, 5870, 2522, 11, 293, 300, 311, 709, 544, 3997], "temperature": 0.0, "avg_logprob": -0.17645296486475134, "compression_ratio": 1.6104417670682731, "no_speech_prob": 2.6016034553322243e-06}, {"id": 1099, "seek": 534174, "start": 5353.099999999999, "end": 5359.42, "text": " So yeah based on all that this morning. I kind of thought like okay this this looks pretty standard so", "tokens": [407, 1338, 2361, 322, 439, 300, 341, 2446, 13, 286, 733, 295, 1194, 411, 1392, 341, 341, 1542, 1238, 3832, 370], "temperature": 0.0, "avg_logprob": -0.17645296486475134, "compression_ratio": 1.6104417670682731, "no_speech_prob": 2.6016034553322243e-06}, {"id": 1100, "seek": 534174, "start": 5360.0199999999995, "end": 5362.0199999999995, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.17645296486475134, "compression_ratio": 1.6104417670682731, "no_speech_prob": 2.6016034553322243e-06}, {"id": 1101, "seek": 534174, "start": 5363.139999999999, "end": 5369.179999999999, "text": " Went ahead and created a little function called get data that basically had my normal two lines of code in it", "tokens": [31809, 2286, 293, 2942, 257, 707, 2445, 1219, 483, 1412, 300, 1936, 632, 452, 2710, 732, 3876, 295, 3089, 294, 309], "temperature": 0.0, "avg_logprob": -0.17645296486475134, "compression_ratio": 1.6104417670682731, "no_speech_prob": 2.6016034553322243e-06}, {"id": 1102, "seek": 536918, "start": 5369.18, "end": 5374.66, "text": " but I made it so I could pass in a size and a batch size the", "tokens": [457, 286, 1027, 309, 370, 286, 727, 1320, 294, 257, 2744, 293, 257, 15245, 2744, 264], "temperature": 0.0, "avg_logprob": -0.16354516211976397, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.1659381925710477e-05}, {"id": 1103, "seek": 536918, "start": 5375.34, "end": 5378.42, "text": " Reason for this is that when I start working with a new data set", "tokens": [39693, 337, 341, 307, 300, 562, 286, 722, 1364, 365, 257, 777, 1412, 992], "temperature": 0.0, "avg_logprob": -0.16354516211976397, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.1659381925710477e-05}, {"id": 1104, "seek": 536918, "start": 5378.42, "end": 5383.54, "text": " I want everything to go super fast and so if I use small images. It's going to go super fast", "tokens": [286, 528, 1203, 281, 352, 1687, 2370, 293, 370, 498, 286, 764, 1359, 5267, 13, 467, 311, 516, 281, 352, 1687, 2370], "temperature": 0.0, "avg_logprob": -0.16354516211976397, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.1659381925710477e-05}, {"id": 1105, "seek": 536918, "start": 5384.18, "end": 5391.58, "text": " So I actually started out with size equals 64 just to create some super small images that just go like a second to", "tokens": [407, 286, 767, 1409, 484, 365, 2744, 6915, 12145, 445, 281, 1884, 512, 1687, 1359, 5267, 300, 445, 352, 411, 257, 1150, 281], "temperature": 0.0, "avg_logprob": -0.16354516211976397, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.1659381925710477e-05}, {"id": 1106, "seek": 536918, "start": 5391.58, "end": 5393.58, "text": " Run through and see how it went", "tokens": [8950, 807, 293, 536, 577, 309, 1437], "temperature": 0.0, "avg_logprob": -0.16354516211976397, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.1659381925710477e-05}, {"id": 1107, "seek": 539358, "start": 5393.58, "end": 5401.58, "text": " Later on I started using some big images and some and some also some bigger architectures at which point I started running out of GPU", "tokens": [11965, 322, 286, 1409, 1228, 512, 955, 5267, 293, 512, 293, 512, 611, 512, 3801, 6331, 1303, 412, 597, 935, 286, 1409, 2614, 484, 295, 18407], "temperature": 0.0, "avg_logprob": -0.21698483249597383, "compression_ratio": 1.8313253012048192, "no_speech_prob": 9.721512697069556e-07}, {"id": 1108, "seek": 539358, "start": 5401.58, "end": 5403.78, "text": " memory so I started getting these errors saying", "tokens": [4675, 370, 286, 1409, 1242, 613, 13603, 1566], "temperature": 0.0, "avg_logprob": -0.21698483249597383, "compression_ratio": 1.8313253012048192, "no_speech_prob": 9.721512697069556e-07}, {"id": 1109, "seek": 539358, "start": 5404.46, "end": 5410.68, "text": " Cuda out of memory error when you get a Cuda out of memory error the first thing you need to do is to go kernel", "tokens": [383, 11152, 484, 295, 4675, 6713, 562, 291, 483, 257, 383, 11152, 484, 295, 4675, 6713, 264, 700, 551, 291, 643, 281, 360, 307, 281, 352, 28256], "temperature": 0.0, "avg_logprob": -0.21698483249597383, "compression_ratio": 1.8313253012048192, "no_speech_prob": 9.721512697069556e-07}, {"id": 1110, "seek": 539358, "start": 5411.34, "end": 5417.58, "text": " Restart once you get a code an out-of-memory error on your GPU. You can't really recover from it right doesn't matter", "tokens": [13094, 446, 1564, 291, 483, 257, 3089, 364, 484, 12, 2670, 12, 17886, 827, 6713, 322, 428, 18407, 13, 509, 393, 380, 534, 8114, 490, 309, 558, 1177, 380, 1871], "temperature": 0.0, "avg_logprob": -0.21698483249597383, "compression_ratio": 1.8313253012048192, "no_speech_prob": 9.721512697069556e-07}, {"id": 1111, "seek": 539358, "start": 5417.58, "end": 5419.76, "text": " What you do you know you have to go restart?", "tokens": [708, 291, 360, 291, 458, 291, 362, 281, 352, 21022, 30], "temperature": 0.0, "avg_logprob": -0.21698483249597383, "compression_ratio": 1.8313253012048192, "no_speech_prob": 9.721512697069556e-07}, {"id": 1112, "seek": 541976, "start": 5419.76, "end": 5429.26, "text": " And once I restarted I then just changed my batch size to something smaller so when you call create your data object", "tokens": [400, 1564, 286, 21022, 292, 286, 550, 445, 3105, 452, 15245, 2744, 281, 746, 4356, 370, 562, 291, 818, 1884, 428, 1412, 2657], "temperature": 0.0, "avg_logprob": -0.2111147408173463, "compression_ratio": 1.691699604743083, "no_speech_prob": 2.684190576474066e-06}, {"id": 1113, "seek": 541976, "start": 5430.08, "end": 5433.68, "text": " You can pass in a batch size parameter", "tokens": [509, 393, 1320, 294, 257, 15245, 2744, 13075], "temperature": 0.0, "avg_logprob": -0.2111147408173463, "compression_ratio": 1.691699604743083, "no_speech_prob": 2.684190576474066e-06}, {"id": 1114, "seek": 541976, "start": 5434.12, "end": 5439.04, "text": " Okay, and like I normally use 64 until I get something that says out of memory", "tokens": [1033, 11, 293, 411, 286, 5646, 764, 12145, 1826, 286, 483, 746, 300, 1619, 484, 295, 4675], "temperature": 0.0, "avg_logprob": -0.2111147408173463, "compression_ratio": 1.691699604743083, "no_speech_prob": 2.684190576474066e-06}, {"id": 1115, "seek": 541976, "start": 5439.04, "end": 5443.0, "text": " And then I'll just have it if I still get out of memory. I'll just have it again, okay", "tokens": [400, 550, 286, 603, 445, 362, 309, 498, 286, 920, 483, 484, 295, 4675, 13, 286, 603, 445, 362, 309, 797, 11, 1392], "temperature": 0.0, "avg_logprob": -0.2111147408173463, "compression_ratio": 1.691699604743083, "no_speech_prob": 2.684190576474066e-06}, {"id": 1116, "seek": 541976, "start": 5444.4800000000005, "end": 5449.320000000001, "text": " So that's where I created this to allow me to like start making my size as bigger as I looked into it more", "tokens": [407, 300, 311, 689, 286, 2942, 341, 281, 2089, 385, 281, 411, 722, 1455, 452, 2744, 382, 3801, 382, 286, 2956, 666, 309, 544], "temperature": 0.0, "avg_logprob": -0.2111147408173463, "compression_ratio": 1.691699604743083, "no_speech_prob": 2.684190576474066e-06}, {"id": 1117, "seek": 544932, "start": 5449.32, "end": 5452.719999999999, "text": " And you know as I started running out of memory to decrease my batch size", "tokens": [400, 291, 458, 382, 286, 1409, 2614, 484, 295, 4675, 281, 11514, 452, 15245, 2744], "temperature": 0.0, "avg_logprob": -0.1952891811247795, "compression_ratio": 1.6798561151079137, "no_speech_prob": 6.540389222209342e-06}, {"id": 1118, "seek": 544932, "start": 5453.32, "end": 5454.639999999999, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.1952891811247795, "compression_ratio": 1.6798561151079137, "no_speech_prob": 6.540389222209342e-06}, {"id": 1119, "seek": 544932, "start": 5454.639999999999, "end": 5456.44, "text": " at this point", "tokens": [412, 341, 935], "temperature": 0.0, "avg_logprob": -0.1952891811247795, "compression_ratio": 1.6798561151079137, "no_speech_prob": 6.540389222209342e-06}, {"id": 1120, "seek": 544932, "start": 5456.44, "end": 5460.679999999999, "text": " You know I went through this a couple of iterations, but I basically found everything was working fine", "tokens": [509, 458, 286, 1437, 807, 341, 257, 1916, 295, 36540, 11, 457, 286, 1936, 1352, 1203, 390, 1364, 2489], "temperature": 0.0, "avg_logprob": -0.1952891811247795, "compression_ratio": 1.6798561151079137, "no_speech_prob": 6.540389222209342e-06}, {"id": 1121, "seek": 544932, "start": 5460.84, "end": 5464.639999999999, "text": " so once it's working fine I set size to 224 and", "tokens": [370, 1564, 309, 311, 1364, 2489, 286, 992, 2744, 281, 5853, 19, 293], "temperature": 0.0, "avg_logprob": -0.1952891811247795, "compression_ratio": 1.6798561151079137, "no_speech_prob": 6.540389222209342e-06}, {"id": 1122, "seek": 544932, "start": 5465.599999999999, "end": 5466.88, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.1952891811247795, "compression_ratio": 1.6798561151079137, "no_speech_prob": 6.540389222209342e-06}, {"id": 1123, "seek": 544932, "start": 5466.88, "end": 5470.0, "text": " Created my you know precomputed equals true first time", "tokens": [11972, 292, 452, 291, 458, 659, 1112, 2582, 292, 6915, 2074, 700, 565], "temperature": 0.0, "avg_logprob": -0.1952891811247795, "compression_ratio": 1.6798561151079137, "no_speech_prob": 6.540389222209342e-06}, {"id": 1124, "seek": 544932, "start": 5470.0, "end": 5472.96, "text": " I did that it took a minute to create the precomputed activations", "tokens": [286, 630, 300, 309, 1890, 257, 3456, 281, 1884, 264, 659, 1112, 2582, 292, 2430, 763], "temperature": 0.0, "avg_logprob": -0.1952891811247795, "compression_ratio": 1.6798561151079137, "no_speech_prob": 6.540389222209342e-06}, {"id": 1125, "seek": 544932, "start": 5472.96, "end": 5478.28, "text": " And then it ran through this in about four or five seconds, and you can see I was getting 83% accuracy", "tokens": [400, 550, 309, 5872, 807, 341, 294, 466, 1451, 420, 1732, 3949, 11, 293, 291, 393, 536, 286, 390, 1242, 30997, 4, 14170], "temperature": 0.0, "avg_logprob": -0.1952891811247795, "compression_ratio": 1.6798561151079137, "no_speech_prob": 6.540389222209342e-06}, {"id": 1126, "seek": 547828, "start": 5478.28, "end": 5485.48, "text": " Now remember accuracy means it's it's exactly right and so it's predicting out of 120 categories", "tokens": [823, 1604, 14170, 1355, 309, 311, 309, 311, 2293, 558, 293, 370, 309, 311, 32884, 484, 295, 10411, 10479], "temperature": 0.0, "avg_logprob": -0.2107967800564236, "compression_ratio": 1.75, "no_speech_prob": 4.222808001941303e-06}, {"id": 1127, "seek": 547828, "start": 5485.679999999999, "end": 5491.639999999999, "text": " It's predicting exactly right so when you see something with two classes is you know 80% accurate", "tokens": [467, 311, 32884, 2293, 558, 370, 562, 291, 536, 746, 365, 732, 5359, 307, 291, 458, 4688, 4, 8559], "temperature": 0.0, "avg_logprob": -0.2107967800564236, "compression_ratio": 1.75, "no_speech_prob": 4.222808001941303e-06}, {"id": 1128, "seek": 547828, "start": 5492.04, "end": 5496.5199999999995, "text": " Versus something with 120 classes is 80% accurate. They're very different", "tokens": [12226, 301, 746, 365, 10411, 5359, 307, 4688, 4, 8559, 13, 814, 434, 588, 819], "temperature": 0.0, "avg_logprob": -0.2107967800564236, "compression_ratio": 1.75, "no_speech_prob": 4.222808001941303e-06}, {"id": 1129, "seek": 547828, "start": 5497.2, "end": 5503.44, "text": " Levels you know so when I saw like 83% accuracy with just a precomputed classifier", "tokens": [16872, 82, 291, 458, 370, 562, 286, 1866, 411, 30997, 4, 14170, 365, 445, 257, 659, 1112, 2582, 292, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.2107967800564236, "compression_ratio": 1.75, "no_speech_prob": 4.222808001941303e-06}, {"id": 1130, "seek": 550344, "start": 5503.44, "end": 5511.599999999999, "text": " No, data augmentation. No one freezing anything else across 120 classes something. Oh this looks good, right so", "tokens": [883, 11, 1412, 14501, 19631, 13, 883, 472, 20200, 1340, 1646, 2108, 10411, 5359, 746, 13, 876, 341, 1542, 665, 11, 558, 370], "temperature": 0.0, "avg_logprob": -0.2782510685015328, "compression_ratio": 1.4757281553398058, "no_speech_prob": 4.157347575528547e-06}, {"id": 1131, "seek": 550344, "start": 5513.12, "end": 5518.74, "text": " Then I just kind of kept going through our little standard process right so then I turn", "tokens": [1396, 286, 445, 733, 295, 4305, 516, 807, 527, 707, 3832, 1399, 558, 370, 550, 286, 1261], "temperature": 0.0, "avg_logprob": -0.2782510685015328, "compression_ratio": 1.4757281553398058, "no_speech_prob": 4.157347575528547e-06}, {"id": 1132, "seek": 550344, "start": 5520.5199999999995, "end": 5522.5199999999995, "text": " Precompute off", "tokens": [6001, 21541, 1169, 766], "temperature": 0.0, "avg_logprob": -0.2782510685015328, "compression_ratio": 1.4757281553398058, "no_speech_prob": 4.157347575528547e-06}, {"id": 1133, "seek": 550344, "start": 5522.639999999999, "end": 5524.639999999999, "text": " Okay, and", "tokens": [1033, 11, 293], "temperature": 0.0, "avg_logprob": -0.2782510685015328, "compression_ratio": 1.4757281553398058, "no_speech_prob": 4.157347575528547e-06}, {"id": 1134, "seek": 550344, "start": 5525.16, "end": 5528.36, "text": " Cycle length equals one and I started doing a few more", "tokens": [10295, 2160, 4641, 6915, 472, 293, 286, 1409, 884, 257, 1326, 544], "temperature": 0.0, "avg_logprob": -0.2782510685015328, "compression_ratio": 1.4757281553398058, "no_speech_prob": 4.157347575528547e-06}, {"id": 1135, "seek": 550344, "start": 5529.16, "end": 5531.16, "text": " cycles a few more epochs", "tokens": [17796, 257, 1326, 544, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.2782510685015328, "compression_ratio": 1.4757281553398058, "no_speech_prob": 4.157347575528547e-06}, {"id": 1136, "seek": 553116, "start": 5531.16, "end": 5533.16, "text": " so remember an", "tokens": [370, 1604, 364], "temperature": 0.0, "avg_logprob": -0.25630320673403534, "compression_ratio": 1.6470588235294117, "no_speech_prob": 3.844910679617897e-06}, {"id": 1137, "seek": 553116, "start": 5533.8, "end": 5536.76, "text": " epoch is one pass through the data a", "tokens": [30992, 339, 307, 472, 1320, 807, 264, 1412, 257], "temperature": 0.0, "avg_logprob": -0.25630320673403534, "compression_ratio": 1.6470588235294117, "no_speech_prob": 3.844910679617897e-06}, {"id": 1138, "seek": 553116, "start": 5538.28, "end": 5540.04, "text": " cycle is", "tokens": [6586, 307], "temperature": 0.0, "avg_logprob": -0.25630320673403534, "compression_ratio": 1.6470588235294117, "no_speech_prob": 3.844910679617897e-06}, {"id": 1139, "seek": 553116, "start": 5540.04, "end": 5542.92, "text": " However many epochs you said is in a cycle", "tokens": [2908, 867, 30992, 28346, 291, 848, 307, 294, 257, 6586], "temperature": 0.0, "avg_logprob": -0.25630320673403534, "compression_ratio": 1.6470588235294117, "no_speech_prob": 3.844910679617897e-06}, {"id": 1140, "seek": 553116, "start": 5542.92, "end": 5547.5199999999995, "text": " It's one it's the learning rate going from the top that you asked for all the way down to zero", "tokens": [467, 311, 472, 309, 311, 264, 2539, 3314, 516, 490, 264, 1192, 300, 291, 2351, 337, 439, 264, 636, 760, 281, 4018], "temperature": 0.0, "avg_logprob": -0.25630320673403534, "compression_ratio": 1.6470588235294117, "no_speech_prob": 3.844910679617897e-06}, {"id": 1141, "seek": 553116, "start": 5547.76, "end": 5555.639999999999, "text": " So since here cycle length equals one a cycle and an epoch are the same okay, so I did I tried a few", "tokens": [407, 1670, 510, 6586, 4641, 6915, 472, 257, 6586, 293, 364, 30992, 339, 366, 264, 912, 1392, 11, 370, 286, 630, 286, 3031, 257, 1326], "temperature": 0.0, "avg_logprob": -0.25630320673403534, "compression_ratio": 1.6470588235294117, "no_speech_prob": 3.844910679617897e-06}, {"id": 1142, "seek": 553116, "start": 5556.16, "end": 5557.16, "text": " epochs", "tokens": [30992, 28346], "temperature": 0.0, "avg_logprob": -0.25630320673403534, "compression_ratio": 1.6470588235294117, "no_speech_prob": 3.844910679617897e-06}, {"id": 1143, "seek": 553116, "start": 5557.16, "end": 5558.16, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.25630320673403534, "compression_ratio": 1.6470588235294117, "no_speech_prob": 3.844910679617897e-06}, {"id": 1144, "seek": 555816, "start": 5558.16, "end": 5563.7, "text": " Did actually do the learning rate finder, and I found one in egg two again looks fine it often looks fine", "tokens": [2589, 767, 360, 264, 2539, 3314, 915, 260, 11, 293, 286, 1352, 472, 294, 3777, 732, 797, 1542, 2489, 309, 2049, 1542, 2489], "temperature": 0.0, "avg_logprob": -0.18454428393431385, "compression_ratio": 1.7012987012987013, "no_speech_prob": 2.6015995899797417e-06}, {"id": 1145, "seek": 555816, "start": 5564.44, "end": 5569.76, "text": " And I found it kind of kept improving so I tried five epochs, and I found my accuracy getting better", "tokens": [400, 286, 1352, 309, 733, 295, 4305, 11470, 370, 286, 3031, 1732, 30992, 28346, 11, 293, 286, 1352, 452, 14170, 1242, 1101], "temperature": 0.0, "avg_logprob": -0.18454428393431385, "compression_ratio": 1.7012987012987013, "no_speech_prob": 2.6015995899797417e-06}, {"id": 1146, "seek": 555816, "start": 5572.2, "end": 5573.68, "text": " So then I", "tokens": [407, 550, 286], "temperature": 0.0, "avg_logprob": -0.18454428393431385, "compression_ratio": 1.7012987012987013, "no_speech_prob": 2.6015995899797417e-06}, {"id": 1147, "seek": 555816, "start": 5573.68, "end": 5578.76, "text": " Saved that and I tried something which we haven't looked at before, but it's kind of cool", "tokens": [12346, 292, 300, 293, 286, 3031, 746, 597, 321, 2378, 380, 2956, 412, 949, 11, 457, 309, 311, 733, 295, 1627], "temperature": 0.0, "avg_logprob": -0.18454428393431385, "compression_ratio": 1.7012987012987013, "no_speech_prob": 2.6015995899797417e-06}, {"id": 1148, "seek": 555816, "start": 5579.76, "end": 5583.2, "text": " If you train something on a smaller size", "tokens": [759, 291, 3847, 746, 322, 257, 4356, 2744], "temperature": 0.0, "avg_logprob": -0.18454428393431385, "compression_ratio": 1.7012987012987013, "no_speech_prob": 2.6015995899797417e-06}, {"id": 1149, "seek": 558320, "start": 5583.2, "end": 5587.44, "text": " you can then actually call learn dot set data and", "tokens": [291, 393, 550, 767, 818, 1466, 5893, 992, 1412, 293], "temperature": 0.0, "avg_logprob": -0.17221245810250255, "compression_ratio": 1.6626984126984128, "no_speech_prob": 5.68240193388192e-06}, {"id": 1150, "seek": 558320, "start": 5588.16, "end": 5592.96, "text": " Pass in a larger size data set and that's going to take your model", "tokens": [10319, 294, 257, 4833, 2744, 1412, 992, 293, 300, 311, 516, 281, 747, 428, 2316], "temperature": 0.0, "avg_logprob": -0.17221245810250255, "compression_ratio": 1.6626984126984128, "no_speech_prob": 5.68240193388192e-06}, {"id": 1151, "seek": 558320, "start": 5592.96, "end": 5599.24, "text": " However, it's trained so far, and it's going to let you continue to train on on larger images", "tokens": [2908, 11, 309, 311, 8895, 370, 1400, 11, 293, 309, 311, 516, 281, 718, 291, 2354, 281, 3847, 322, 322, 4833, 5267], "temperature": 0.0, "avg_logprob": -0.17221245810250255, "compression_ratio": 1.6626984126984128, "no_speech_prob": 5.68240193388192e-06}, {"id": 1152, "seek": 558320, "start": 5599.24, "end": 5601.32, "text": " And I'll tell you something amazing", "tokens": [400, 286, 603, 980, 291, 746, 2243], "temperature": 0.0, "avg_logprob": -0.17221245810250255, "compression_ratio": 1.6626984126984128, "no_speech_prob": 5.68240193388192e-06}, {"id": 1153, "seek": 558320, "start": 5602.639999999999, "end": 5606.599999999999, "text": " This actually is another way you can get state-of-the-art results", "tokens": [639, 767, 307, 1071, 636, 291, 393, 483, 1785, 12, 2670, 12, 3322, 12, 446, 3542], "temperature": 0.0, "avg_logprob": -0.17221245810250255, "compression_ratio": 1.6626984126984128, "no_speech_prob": 5.68240193388192e-06}, {"id": 1154, "seek": 558320, "start": 5606.599999999999, "end": 5611.639999999999, "text": " And I've never seen this written in any paper or discussed anywhere as far as I know this is a new insight", "tokens": [400, 286, 600, 1128, 1612, 341, 3720, 294, 604, 3035, 420, 7152, 4992, 382, 1400, 382, 286, 458, 341, 307, 257, 777, 11269], "temperature": 0.0, "avg_logprob": -0.17221245810250255, "compression_ratio": 1.6626984126984128, "no_speech_prob": 5.68240193388192e-06}, {"id": 1155, "seek": 561164, "start": 5611.64, "end": 5616.240000000001, "text": " Basically, I've got a pre-trained model which in this case", "tokens": [8537, 11, 286, 600, 658, 257, 659, 12, 17227, 2001, 2316, 597, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.20331673873098274, "compression_ratio": 1.6724137931034482, "no_speech_prob": 8.139579222188331e-06}, {"id": 1156, "seek": 561164, "start": 5616.240000000001, "end": 5619.92, "text": " I've trained a few epochs with the size of 224 by 224", "tokens": [286, 600, 8895, 257, 1326, 30992, 28346, 365, 264, 2744, 295, 5853, 19, 538, 5853, 19], "temperature": 0.0, "avg_logprob": -0.20331673873098274, "compression_ratio": 1.6724137931034482, "no_speech_prob": 8.139579222188331e-06}, {"id": 1157, "seek": 561164, "start": 5619.92, "end": 5624.360000000001, "text": " And I'm now going to do a few more epochs with a size of 299 by 299", "tokens": [400, 286, 478, 586, 516, 281, 360, 257, 1326, 544, 30992, 28346, 365, 257, 2744, 295, 568, 8494, 538, 568, 8494], "temperature": 0.0, "avg_logprob": -0.20331673873098274, "compression_ratio": 1.6724137931034482, "no_speech_prob": 8.139579222188331e-06}, {"id": 1158, "seek": 561164, "start": 5624.88, "end": 5632.280000000001, "text": " Now I've got very little data kind of by deep learning standards only got 10,000 images right so with a 224 by", "tokens": [823, 286, 600, 658, 588, 707, 1412, 733, 295, 538, 2452, 2539, 7787, 787, 658, 1266, 11, 1360, 5267, 558, 370, 365, 257, 5853, 19, 538], "temperature": 0.0, "avg_logprob": -0.20331673873098274, "compression_ratio": 1.6724137931034482, "no_speech_prob": 8.139579222188331e-06}, {"id": 1159, "seek": 561164, "start": 5632.4400000000005, "end": 5639.38, "text": " 224 I kind of built this these final layers to try to find things that worked well at 224 by 224", "tokens": [5853, 19, 286, 733, 295, 3094, 341, 613, 2572, 7914, 281, 853, 281, 915, 721, 300, 2732, 731, 412, 5853, 19, 538, 5853, 19], "temperature": 0.0, "avg_logprob": -0.20331673873098274, "compression_ratio": 1.6724137931034482, "no_speech_prob": 8.139579222188331e-06}, {"id": 1160, "seek": 563938, "start": 5639.38, "end": 5642.22, "text": " But I go to 299 by 299 I", "tokens": [583, 286, 352, 281, 568, 8494, 538, 568, 8494, 286], "temperature": 0.0, "avg_logprob": -0.22270269651670713, "compression_ratio": 1.7718631178707225, "no_speech_prob": 3.2887237466638908e-06}, {"id": 1161, "seek": 563938, "start": 5643.22, "end": 5648.9400000000005, "text": " Basically if I overfit the floor I'm definitely not going to overfit now like I've changed the size of my images", "tokens": [8537, 498, 286, 670, 6845, 264, 4123, 286, 478, 2138, 406, 516, 281, 670, 6845, 586, 411, 286, 600, 3105, 264, 2744, 295, 452, 5267], "temperature": 0.0, "avg_logprob": -0.22270269651670713, "compression_ratio": 1.7718631178707225, "no_speech_prob": 3.2887237466638908e-06}, {"id": 1162, "seek": 563938, "start": 5648.9400000000005, "end": 5650.82, "text": " They're kind of like totally different", "tokens": [814, 434, 733, 295, 411, 3879, 819], "temperature": 0.0, "avg_logprob": -0.22270269651670713, "compression_ratio": 1.7718631178707225, "no_speech_prob": 3.2887237466638908e-06}, {"id": 1163, "seek": 563938, "start": 5650.82, "end": 5655.7, "text": " But like conceptually they're still picked the same kinds of pictures of the same kinds of things", "tokens": [583, 411, 3410, 671, 436, 434, 920, 6183, 264, 912, 3685, 295, 5242, 295, 264, 912, 3685, 295, 721], "temperature": 0.0, "avg_logprob": -0.22270269651670713, "compression_ratio": 1.7718631178707225, "no_speech_prob": 3.2887237466638908e-06}, {"id": 1164, "seek": 563938, "start": 5655.9800000000005, "end": 5663.58, "text": " So I found this trick of like starting training on small images for a few epochs and then switching to bigger images and continuing", "tokens": [407, 286, 1352, 341, 4282, 295, 411, 2891, 3097, 322, 1359, 5267, 337, 257, 1326, 30992, 28346, 293, 550, 16493, 281, 3801, 5267, 293, 9289], "temperature": 0.0, "avg_logprob": -0.22270269651670713, "compression_ratio": 1.7718631178707225, "no_speech_prob": 3.2887237466638908e-06}, {"id": 1165, "seek": 563938, "start": 5663.58, "end": 5667.1, "text": " Training is an amazingly effective way to avoid overfitting", "tokens": [20620, 307, 364, 31762, 4942, 636, 281, 5042, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.22270269651670713, "compression_ratio": 1.7718631178707225, "no_speech_prob": 3.2887237466638908e-06}, {"id": 1166, "seek": 566710, "start": 5667.1, "end": 5669.1, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.3027371883392334, "compression_ratio": 1.5240641711229947, "no_speech_prob": 8.530224476999138e-06}, {"id": 1167, "seek": 566710, "start": 5669.18, "end": 5672.34, "text": " It's like it's so easy and so obvious", "tokens": [467, 311, 411, 309, 311, 370, 1858, 293, 370, 6322], "temperature": 0.0, "avg_logprob": -0.3027371883392334, "compression_ratio": 1.5240641711229947, "no_speech_prob": 8.530224476999138e-06}, {"id": 1168, "seek": 566710, "start": 5672.34, "end": 5677.4800000000005, "text": " I don't understand why it's never been written about before maybe it's in some paper somewhere, and I haven't found it", "tokens": [286, 500, 380, 1223, 983, 309, 311, 1128, 668, 3720, 466, 949, 1310, 309, 311, 294, 512, 3035, 4079, 11, 293, 286, 2378, 380, 1352, 309], "temperature": 0.0, "avg_logprob": -0.3027371883392334, "compression_ratio": 1.5240641711229947, "no_speech_prob": 8.530224476999138e-06}, {"id": 1169, "seek": 566710, "start": 5677.4800000000005, "end": 5679.4800000000005, "text": " But it's I haven't seen it", "tokens": [583, 309, 311, 286, 2378, 380, 1612, 309], "temperature": 0.0, "avg_logprob": -0.3027371883392334, "compression_ratio": 1.5240641711229947, "no_speech_prob": 8.530224476999138e-06}, {"id": 1170, "seek": 566710, "start": 5683.58, "end": 5687.18, "text": " Would it be possible to do the same thing using let's take errors or", "tokens": [6068, 309, 312, 1944, 281, 360, 264, 912, 551, 1228, 718, 311, 747, 13603, 420], "temperature": 0.0, "avg_logprob": -0.3027371883392334, "compression_ratio": 1.5240641711229947, "no_speech_prob": 8.530224476999138e-06}, {"id": 1171, "seek": 566710, "start": 5687.780000000001, "end": 5689.9800000000005, "text": " TensorFlow as well to feed a", "tokens": [37624, 382, 731, 281, 3154, 257], "temperature": 0.0, "avg_logprob": -0.3027371883392334, "compression_ratio": 1.5240641711229947, "no_speech_prob": 8.530224476999138e-06}, {"id": 1172, "seek": 568998, "start": 5689.98, "end": 5695.94, "text": " Measure of different sides yeah, I think so like as long as you use one of these more modern", "tokens": [41436, 295, 819, 4881, 1338, 11, 286, 519, 370, 411, 382, 938, 382, 291, 764, 472, 295, 613, 544, 4363], "temperature": 0.0, "avg_logprob": -0.20658233642578125, "compression_ratio": 1.6496062992125984, "no_speech_prob": 2.0261256850062637e-06}, {"id": 1173, "seek": 568998, "start": 5696.5, "end": 5699.08, "text": " architectures what we call fully convolutional architectures", "tokens": [6331, 1303, 437, 321, 818, 4498, 45216, 304, 6331, 1303], "temperature": 0.0, "avg_logprob": -0.20658233642578125, "compression_ratio": 1.6496062992125984, "no_speech_prob": 2.0261256850062637e-06}, {"id": 1174, "seek": 568998, "start": 5699.099999999999, "end": 5705.719999999999, "text": " Which means not VGG and you'll see we don't use VGG in this course because it doesn't have this property, but most of the", "tokens": [3013, 1355, 406, 691, 27561, 293, 291, 603, 536, 321, 500, 380, 764, 691, 27561, 294, 341, 1164, 570, 309, 1177, 380, 362, 341, 4707, 11, 457, 881, 295, 264], "temperature": 0.0, "avg_logprob": -0.20658233642578125, "compression_ratio": 1.6496062992125984, "no_speech_prob": 2.0261256850062637e-06}, {"id": 1175, "seek": 568998, "start": 5706.7, "end": 5710.62, "text": " Architectures developed in the last couple of years can handle pretty much arbitrary sizes", "tokens": [29306, 1303, 4743, 294, 264, 1036, 1916, 295, 924, 393, 4813, 1238, 709, 23211, 11602], "temperature": 0.0, "avg_logprob": -0.20658233642578125, "compression_ratio": 1.6496062992125984, "no_speech_prob": 2.0261256850062637e-06}, {"id": 1176, "seek": 568998, "start": 5712.179999999999, "end": 5715.459999999999, "text": " Yeah, be worth trying yeah, I think it ought to work", "tokens": [865, 11, 312, 3163, 1382, 1338, 11, 286, 519, 309, 13416, 281, 589], "temperature": 0.0, "avg_logprob": -0.20658233642578125, "compression_ratio": 1.6496062992125984, "no_speech_prob": 2.0261256850062637e-06}, {"id": 1177, "seek": 571546, "start": 5715.46, "end": 5722.66, "text": " Okay, so I call get data again remember get data is that just the little function that I created back up here right get", "tokens": [1033, 11, 370, 286, 818, 483, 1412, 797, 1604, 483, 1412, 307, 300, 445, 264, 707, 2445, 300, 286, 2942, 646, 493, 510, 558, 483], "temperature": 0.0, "avg_logprob": -0.2660279767266635, "compression_ratio": 1.6774193548387097, "no_speech_prob": 5.862735179107403e-06}, {"id": 1178, "seek": 571546, "start": 5722.66, "end": 5725.94, "text": " Data is just this little function. That's why I just passed a different size to it", "tokens": [11888, 307, 445, 341, 707, 2445, 13, 663, 311, 983, 286, 445, 4678, 257, 819, 2744, 281, 309], "temperature": 0.0, "avg_logprob": -0.2660279767266635, "compression_ratio": 1.6774193548387097, "no_speech_prob": 5.862735179107403e-06}, {"id": 1179, "seek": 571546, "start": 5726.58, "end": 5728.58, "text": " and so I", "tokens": [293, 370, 286], "temperature": 0.0, "avg_logprob": -0.2660279767266635, "compression_ratio": 1.6774193548387097, "no_speech_prob": 5.862735179107403e-06}, {"id": 1180, "seek": 571546, "start": 5729.86, "end": 5734.02, "text": " Call freeze just to make sure that that everything set the last layer is frozen", "tokens": [7807, 15959, 445, 281, 652, 988, 300, 300, 1203, 992, 264, 1036, 4583, 307, 12496], "temperature": 0.0, "avg_logprob": -0.2660279767266635, "compression_ratio": 1.6774193548387097, "no_speech_prob": 5.862735179107403e-06}, {"id": 1181, "seek": 571546, "start": 5734.02, "end": 5738.06, "text": " I mean actually it already was at this point that really do anything and", "tokens": [286, 914, 767, 309, 1217, 390, 412, 341, 935, 300, 534, 360, 1340, 293], "temperature": 0.0, "avg_logprob": -0.2660279767266635, "compression_ratio": 1.6774193548387097, "no_speech_prob": 5.862735179107403e-06}, {"id": 1182, "seek": 573806, "start": 5738.06, "end": 5744.660000000001, "text": " You can see now with free compute off", "tokens": [509, 393, 536, 586, 365, 1737, 14722, 766], "temperature": 0.0, "avg_logprob": -0.1636298942565918, "compression_ratio": 1.8082191780821917, "no_speech_prob": 2.2959111447562464e-06}, {"id": 1183, "seek": 573806, "start": 5744.660000000001, "end": 5747.02, "text": " I've never got data augmentation working", "tokens": [286, 600, 1128, 658, 1412, 14501, 19631, 1364], "temperature": 0.0, "avg_logprob": -0.1636298942565918, "compression_ratio": 1.8082191780821917, "no_speech_prob": 2.2959111447562464e-06}, {"id": 1184, "seek": 573806, "start": 5747.02, "end": 5754.46, "text": " So I kind of run a few more epochs and what I noticed here is that the loss of my training set and the loss of", "tokens": [407, 286, 733, 295, 1190, 257, 1326, 544, 30992, 28346, 293, 437, 286, 5694, 510, 307, 300, 264, 4470, 295, 452, 3097, 992, 293, 264, 4470, 295], "temperature": 0.0, "avg_logprob": -0.1636298942565918, "compression_ratio": 1.8082191780821917, "no_speech_prob": 2.2959111447562464e-06}, {"id": 1185, "seek": 573806, "start": 5754.46, "end": 5760.3, "text": " My validation set my validation set loss is a lot lower than my training set. This is still just training the last layer", "tokens": [1222, 24071, 992, 452, 24071, 992, 4470, 307, 257, 688, 3126, 813, 452, 3097, 992, 13, 639, 307, 920, 445, 3097, 264, 1036, 4583], "temperature": 0.0, "avg_logprob": -0.1636298942565918, "compression_ratio": 1.8082191780821917, "no_speech_prob": 2.2959111447562464e-06}, {"id": 1186, "seek": 573806, "start": 5760.42, "end": 5765.14, "text": " So what this is telling me is I'm I'm under fitting right and so if I'm under fitting", "tokens": [407, 437, 341, 307, 3585, 385, 307, 286, 478, 286, 478, 833, 15669, 558, 293, 370, 498, 286, 478, 833, 15669], "temperature": 0.0, "avg_logprob": -0.1636298942565918, "compression_ratio": 1.8082191780821917, "no_speech_prob": 2.2959111447562464e-06}, {"id": 1187, "seek": 576514, "start": 5765.14, "end": 5768.02, "text": " It means this cycle length equals one is too short", "tokens": [467, 1355, 341, 6586, 4641, 6915, 472, 307, 886, 2099], "temperature": 0.0, "avg_logprob": -0.1981104800575658, "compression_ratio": 1.7300884955752212, "no_speech_prob": 5.093679646961391e-06}, {"id": 1188, "seek": 576514, "start": 5768.02, "end": 5773.660000000001, "text": " It means it's like finding something better pop popping out and it's like never getting a chance to zoom in properly", "tokens": [467, 1355, 309, 311, 411, 5006, 746, 1101, 1665, 18374, 484, 293, 309, 311, 411, 1128, 1242, 257, 2931, 281, 8863, 294, 6108], "temperature": 0.0, "avg_logprob": -0.1981104800575658, "compression_ratio": 1.7300884955752212, "no_speech_prob": 5.093679646961391e-06}, {"id": 1189, "seek": 576514, "start": 5773.900000000001, "end": 5778.1, "text": " So then I set cycle mult equals two to give it more time", "tokens": [407, 550, 286, 992, 6586, 2120, 6915, 732, 281, 976, 309, 544, 565], "temperature": 0.0, "avg_logprob": -0.1981104800575658, "compression_ratio": 1.7300884955752212, "no_speech_prob": 5.093679646961391e-06}, {"id": 1190, "seek": 576514, "start": 5778.1, "end": 5782.820000000001, "text": " So like the first time is one epoch the second one is two epochs", "tokens": [407, 411, 264, 700, 565, 307, 472, 30992, 339, 264, 1150, 472, 307, 732, 30992, 28346], "temperature": 0.0, "avg_logprob": -0.1981104800575658, "compression_ratio": 1.7300884955752212, "no_speech_prob": 5.093679646961391e-06}, {"id": 1191, "seek": 576514, "start": 5783.18, "end": 5789.280000000001, "text": " The third one is four epochs and you can see now the validation train and training are about the same", "tokens": [440, 2636, 472, 307, 1451, 30992, 28346, 293, 291, 393, 536, 586, 264, 24071, 3847, 293, 3097, 366, 466, 264, 912], "temperature": 0.0, "avg_logprob": -0.1981104800575658, "compression_ratio": 1.7300884955752212, "no_speech_prob": 5.093679646961391e-06}, {"id": 1192, "seek": 578928, "start": 5789.28, "end": 5796.48, "text": " Okay, so that's kind of thinking yeah, this is this is about the right track and so then I tried using test type", "tokens": [1033, 11, 370, 300, 311, 733, 295, 1953, 1338, 11, 341, 307, 341, 307, 466, 264, 558, 2837, 293, 370, 550, 286, 3031, 1228, 1500, 2010], "temperature": 0.0, "avg_logprob": -0.1955114511343149, "compression_ratio": 1.7372549019607844, "no_speech_prob": 3.9669575926382095e-06}, {"id": 1193, "seek": 578928, "start": 5797.08, "end": 5801.8, "text": " Augmentation to see if that gets any better still didn't actually help a hell of a lot just a tiny bit", "tokens": [6088, 19631, 281, 536, 498, 300, 2170, 604, 1101, 920, 994, 380, 767, 854, 257, 4921, 295, 257, 688, 445, 257, 5870, 857], "temperature": 0.0, "avg_logprob": -0.1955114511343149, "compression_ratio": 1.7372549019607844, "no_speech_prob": 3.9669575926382095e-06}, {"id": 1194, "seek": 578928, "start": 5802.719999999999, "end": 5806.48, "text": " And just kind of at this point. I'm thinking this is nearly done", "tokens": [400, 445, 733, 295, 412, 341, 935, 13, 286, 478, 1953, 341, 307, 6217, 1096], "temperature": 0.0, "avg_logprob": -0.1955114511343149, "compression_ratio": 1.7372549019607844, "no_speech_prob": 3.9669575926382095e-06}, {"id": 1195, "seek": 578928, "start": 5807.08, "end": 5811.32, "text": " So I just did it like you know one more cycle of two to see if it got any better and", "tokens": [407, 286, 445, 630, 309, 411, 291, 458, 472, 544, 6586, 295, 732, 281, 536, 498, 309, 658, 604, 1101, 293], "temperature": 0.0, "avg_logprob": -0.1955114511343149, "compression_ratio": 1.7372549019607844, "no_speech_prob": 3.9669575926382095e-06}, {"id": 1196, "seek": 578928, "start": 5811.679999999999, "end": 5814.8, "text": " It did get a little bit better and then I'm like, okay", "tokens": [467, 630, 483, 257, 707, 857, 1101, 293, 550, 286, 478, 411, 11, 1392], "temperature": 0.0, "avg_logprob": -0.1955114511343149, "compression_ratio": 1.7372549019607844, "no_speech_prob": 3.9669575926382095e-06}, {"id": 1197, "seek": 578928, "start": 5815.48, "end": 5817.4, "text": " That looks pretty good", "tokens": [663, 1542, 1238, 665], "temperature": 0.0, "avg_logprob": -0.1955114511343149, "compression_ratio": 1.7372549019607844, "no_speech_prob": 3.9669575926382095e-06}, {"id": 1198, "seek": 581740, "start": 5817.4, "end": 5819.4, "text": " I've got a validation set", "tokens": [286, 600, 658, 257, 24071, 992], "temperature": 0.0, "avg_logprob": -0.24548773143602454, "compression_ratio": 1.6894977168949772, "no_speech_prob": 4.637844995158957e-06}, {"id": 1199, "seek": 581740, "start": 5820.12, "end": 5822.12, "text": " loss of point one nine nine", "tokens": [4470, 295, 935, 472, 4949, 4949], "temperature": 0.0, "avg_logprob": -0.24548773143602454, "compression_ratio": 1.6894977168949772, "no_speech_prob": 4.637844995158957e-06}, {"id": 1200, "seek": 581740, "start": 5823.04, "end": 5826.599999999999, "text": " And so you'll notice here actually haven't tried unfreezing", "tokens": [400, 370, 291, 603, 3449, 510, 767, 2378, 380, 3031, 3971, 701, 8781], "temperature": 0.0, "avg_logprob": -0.24548773143602454, "compression_ratio": 1.6894977168949772, "no_speech_prob": 4.637844995158957e-06}, {"id": 1201, "seek": 581740, "start": 5827.16, "end": 5831.5199999999995, "text": " The reason why I was going to try to unfreezing and training more it didn't get me better", "tokens": [440, 1778, 983, 286, 390, 516, 281, 853, 281, 3971, 701, 8781, 293, 3097, 544, 309, 994, 380, 483, 385, 1101], "temperature": 0.0, "avg_logprob": -0.24548773143602454, "compression_ratio": 1.6894977168949772, "no_speech_prob": 4.637844995158957e-06}, {"id": 1202, "seek": 581740, "start": 5831.5199999999995, "end": 5837.92, "text": " and so the reason for this clearly is that this data set is so similar the image net that the", "tokens": [293, 370, 264, 1778, 337, 341, 4448, 307, 300, 341, 1412, 992, 307, 370, 2531, 264, 3256, 2533, 300, 264], "temperature": 0.0, "avg_logprob": -0.24548773143602454, "compression_ratio": 1.6894977168949772, "no_speech_prob": 4.637844995158957e-06}, {"id": 1203, "seek": 581740, "start": 5838.5199999999995, "end": 5841.48, "text": " Training the convolutional layers actually doesn't help in the slightest", "tokens": [20620, 264, 45216, 304, 7914, 767, 1177, 380, 854, 294, 264, 41040], "temperature": 0.0, "avg_logprob": -0.24548773143602454, "compression_ratio": 1.6894977168949772, "no_speech_prob": 4.637844995158957e-06}, {"id": 1204, "seek": 584148, "start": 5841.48, "end": 5847.5199999999995, "text": " And actually when I later looked into it it turns out that this competition is actually using", "tokens": [400, 767, 562, 286, 1780, 2956, 666, 309, 309, 4523, 484, 300, 341, 6211, 307, 767, 1228], "temperature": 0.0, "avg_logprob": -0.25004171817860704, "compression_ratio": 1.618421052631579, "no_speech_prob": 1.4970730262575671e-05}, {"id": 1205, "seek": 584148, "start": 5848.0, "end": 5853.98, "text": " A subset of image net so okay, so then if we check this out point one nine nine", "tokens": [316, 25993, 295, 3256, 2533, 370, 1392, 11, 370, 550, 498, 321, 1520, 341, 484, 935, 472, 4949, 4949], "temperature": 0.0, "avg_logprob": -0.25004171817860704, "compression_ratio": 1.618421052631579, "no_speech_prob": 1.4970730262575671e-05}, {"id": 1206, "seek": 584148, "start": 5854.639999999999, "end": 5856.5599999999995, "text": " against the leaderboard", "tokens": [1970, 264, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.25004171817860704, "compression_ratio": 1.618421052631579, "no_speech_prob": 1.4970730262575671e-05}, {"id": 1207, "seek": 584148, "start": 5856.5599999999995, "end": 5861.44, "text": " This is only a playground competition, so it's not like the best of here, but you know it's still interesting", "tokens": [639, 307, 787, 257, 24646, 6211, 11, 370, 309, 311, 406, 411, 264, 1151, 295, 510, 11, 457, 291, 458, 309, 311, 920, 1880], "temperature": 0.0, "avg_logprob": -0.25004171817860704, "compression_ratio": 1.618421052631579, "no_speech_prob": 1.4970730262575671e-05}, {"id": 1208, "seek": 584148, "start": 5861.959999999999, "end": 5863.959999999999, "text": " It gets us", "tokens": [467, 2170, 505], "temperature": 0.0, "avg_logprob": -0.25004171817860704, "compression_ratio": 1.618421052631579, "no_speech_prob": 1.4970730262575671e-05}, {"id": 1209, "seek": 584148, "start": 5865.28, "end": 5870.2, "text": " Somewhere around 10th 11th okay, and in fact we're", "tokens": [34500, 926, 1266, 392, 2975, 392, 1392, 11, 293, 294, 1186, 321, 434], "temperature": 0.0, "avg_logprob": -0.25004171817860704, "compression_ratio": 1.618421052631579, "no_speech_prob": 1.4970730262575671e-05}, {"id": 1210, "seek": 587020, "start": 5870.2, "end": 5875.86, "text": " Competing against I notice other this is a fast AI student. This is a fast AI student", "tokens": [6620, 9880, 1970, 286, 3449, 661, 341, 307, 257, 2370, 7318, 3107, 13, 639, 307, 257, 2370, 7318, 3107], "temperature": 0.0, "avg_logprob": -0.2536139054731889, "compression_ratio": 1.7797356828193833, "no_speech_prob": 8.939557119447272e-06}, {"id": 1211, "seek": 587020, "start": 5877.08, "end": 5881.32, "text": " These people up here. I know they actually posted that they they cheated they actually went", "tokens": [1981, 561, 493, 510, 13, 286, 458, 436, 767, 9437, 300, 436, 436, 28079, 436, 767, 1437], "temperature": 0.0, "avg_logprob": -0.2536139054731889, "compression_ratio": 1.7797356828193833, "no_speech_prob": 8.939557119447272e-06}, {"id": 1212, "seek": 587020, "start": 5881.32, "end": 5884.32, "text": " We downloaded the original images and trained to that so", "tokens": [492, 21748, 264, 3380, 5267, 293, 8895, 281, 300, 370], "temperature": 0.0, "avg_logprob": -0.2536139054731889, "compression_ratio": 1.7797356828193833, "no_speech_prob": 8.939557119447272e-06}, {"id": 1213, "seek": 587020, "start": 5887.0, "end": 5891.54, "text": " This is why this is a playground competition they call it it's not it's not real right you know", "tokens": [639, 307, 983, 341, 307, 257, 24646, 6211, 436, 818, 309, 309, 311, 406, 309, 311, 406, 957, 558, 291, 458], "temperature": 0.0, "avg_logprob": -0.2536139054731889, "compression_ratio": 1.7797356828193833, "no_speech_prob": 8.939557119447272e-06}, {"id": 1214, "seek": 587020, "start": 5891.54, "end": 5894.96, "text": " It's just to allow us to try things out, but you can basically see", "tokens": [467, 311, 445, 281, 2089, 505, 281, 853, 721, 484, 11, 457, 291, 393, 1936, 536], "temperature": 0.0, "avg_logprob": -0.2536139054731889, "compression_ratio": 1.7797356828193833, "no_speech_prob": 8.939557119447272e-06}, {"id": 1215, "seek": 587020, "start": 5896.16, "end": 5897.639999999999, "text": " out of", "tokens": [484, 295], "temperature": 0.0, "avg_logprob": -0.2536139054731889, "compression_ratio": 1.7797356828193833, "no_speech_prob": 8.939557119447272e-06}, {"id": 1216, "seek": 589764, "start": 5897.64, "end": 5901.52, "text": " 200 and something people where you know we're getting some very good results", "tokens": [2331, 293, 746, 561, 689, 291, 458, 321, 434, 1242, 512, 588, 665, 3542], "temperature": 0.0, "avg_logprob": -0.21083643244600844, "compression_ratio": 1.5764192139737991, "no_speech_prob": 2.0144872905802913e-05}, {"id": 1217, "seek": 589764, "start": 5903.04, "end": 5907.320000000001, "text": " Without doing anything remotely interesting or clever, and we haven't even used the whole data set", "tokens": [9129, 884, 1340, 20824, 1880, 420, 13494, 11, 293, 321, 2378, 380, 754, 1143, 264, 1379, 1412, 992], "temperature": 0.0, "avg_logprob": -0.21083643244600844, "compression_ratio": 1.5764192139737991, "no_speech_prob": 2.0144872905802913e-05}, {"id": 1218, "seek": 589764, "start": 5907.320000000001, "end": 5910.04, "text": " We've only used 80% of it like to get a better result", "tokens": [492, 600, 787, 1143, 4688, 4, 295, 309, 411, 281, 483, 257, 1101, 1874], "temperature": 0.0, "avg_logprob": -0.21083643244600844, "compression_ratio": 1.5764192139737991, "no_speech_prob": 2.0144872905802913e-05}, {"id": 1219, "seek": 589764, "start": 5910.04, "end": 5915.42, "text": " I would go back and remove that validation set and just rerun the same steps and then submit that", "tokens": [286, 576, 352, 646, 293, 4159, 300, 24071, 992, 293, 445, 43819, 409, 264, 912, 4439, 293, 550, 10315, 300], "temperature": 0.0, "avg_logprob": -0.21083643244600844, "compression_ratio": 1.5764192139737991, "no_speech_prob": 2.0144872905802913e-05}, {"id": 1220, "seek": 589764, "start": 5915.72, "end": 5917.72, "text": " That lets us use 100% of the data", "tokens": [663, 6653, 505, 764, 2319, 4, 295, 264, 1412], "temperature": 0.0, "avg_logprob": -0.21083643244600844, "compression_ratio": 1.5764192139737991, "no_speech_prob": 2.0144872905802913e-05}, {"id": 1221, "seek": 591772, "start": 5917.72, "end": 5919.72, "text": " What's our", "tokens": [708, 311, 527], "temperature": 0.0, "avg_logprob": -0.25958615038768357, "compression_ratio": 1.7954545454545454, "no_speech_prob": 1.5935785995679908e-05}, {"id": 1222, "seek": 591772, "start": 5926.4800000000005, "end": 5933.4800000000005, "text": " Last three questions the first one is like the class in this case is very it's not balanced unlike the dogs and cats", "tokens": [5264, 1045, 1651, 264, 700, 472, 307, 411, 264, 1508, 294, 341, 1389, 307, 588, 309, 311, 406, 13902, 8343, 264, 7197, 293, 11111], "temperature": 0.0, "avg_logprob": -0.25958615038768357, "compression_ratio": 1.7954545454545454, "no_speech_prob": 1.5935785995679908e-05}, {"id": 1223, "seek": 591772, "start": 5933.4800000000005, "end": 5938.8, "text": " It's not unbalanced like it's not totally balanced, but it's not bad right it's like", "tokens": [467, 311, 406, 517, 40251, 411, 309, 311, 406, 3879, 13902, 11, 457, 309, 311, 406, 1578, 558, 309, 311, 411], "temperature": 0.0, "avg_logprob": -0.25958615038768357, "compression_ratio": 1.7954545454545454, "no_speech_prob": 1.5935785995679908e-05}, {"id": 1224, "seek": 591772, "start": 5939.88, "end": 5946.08, "text": " Between 60 and 100 like it's it's it's it's not unbalanced enough that I would give it a second thought", "tokens": [18967, 4060, 293, 2319, 411, 309, 311, 309, 311, 309, 311, 309, 311, 406, 517, 40251, 1547, 300, 286, 576, 976, 309, 257, 1150, 1194], "temperature": 0.0, "avg_logprob": -0.25958615038768357, "compression_ratio": 1.7954545454545454, "no_speech_prob": 1.5935785995679908e-05}, {"id": 1225, "seek": 594608, "start": 5946.08, "end": 5948.08, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.17440390586853027, "compression_ratio": 1.5204081632653061, "no_speech_prob": 9.972602128982544e-06}, {"id": 1226, "seek": 594608, "start": 5952.6, "end": 5955.16, "text": " Yeah, let's get to that later in this course", "tokens": [865, 11, 718, 311, 483, 281, 300, 1780, 294, 341, 1164], "temperature": 0.0, "avg_logprob": -0.17440390586853027, "compression_ratio": 1.5204081632653061, "no_speech_prob": 9.972602128982544e-06}, {"id": 1227, "seek": 594608, "start": 5955.16, "end": 5960.72, "text": " And don't let me forget right the short answer is that there was a recent list of paper came out about two or three weeks", "tokens": [400, 500, 380, 718, 385, 2870, 558, 264, 2099, 1867, 307, 300, 456, 390, 257, 5162, 1329, 295, 3035, 1361, 484, 466, 732, 420, 1045, 3259], "temperature": 0.0, "avg_logprob": -0.17440390586853027, "compression_ratio": 1.5204081632653061, "no_speech_prob": 9.972602128982544e-06}, {"id": 1228, "seek": 594608, "start": 5960.72, "end": 5967.08, "text": " Ago on this and it said the best way to deal with very unbalanced data sets is to basically make copies of the", "tokens": [316, 1571, 322, 341, 293, 309, 848, 264, 1151, 636, 281, 2028, 365, 588, 517, 40251, 1412, 6352, 307, 281, 1936, 652, 14341, 295, 264], "temperature": 0.0, "avg_logprob": -0.17440390586853027, "compression_ratio": 1.5204081632653061, "no_speech_prob": 9.972602128982544e-06}, {"id": 1229, "seek": 594608, "start": 5968.16, "end": 5970.16, "text": " rare cases", "tokens": [5892, 3331], "temperature": 0.0, "avg_logprob": -0.17440390586853027, "compression_ratio": 1.5204081632653061, "no_speech_prob": 9.972602128982544e-06}, {"id": 1230, "seek": 594608, "start": 5973.5199999999995, "end": 5975.08, "text": " Yeah", "tokens": [865], "temperature": 0.0, "avg_logprob": -0.17440390586853027, "compression_ratio": 1.5204081632653061, "no_speech_prob": 9.972602128982544e-06}, {"id": 1231, "seek": 597508, "start": 5975.08, "end": 5980.92, "text": " My second question is I want to pin down a difference between pre compute was true and", "tokens": [1222, 1150, 1168, 307, 286, 528, 281, 5447, 760, 257, 2649, 1296, 659, 14722, 390, 2074, 293], "temperature": 0.0, "avg_logprob": -0.3818316252335258, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.00011410684965085238}, {"id": 1232, "seek": 597508, "start": 5982.84, "end": 5985.5, "text": " So you have these two options so when you beginning", "tokens": [407, 291, 362, 613, 732, 3956, 370, 562, 291, 2863], "temperature": 0.0, "avg_logprob": -0.3818316252335258, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.00011410684965085238}, {"id": 1233, "seek": 597508, "start": 5991.16, "end": 5997.32, "text": " Right right so it's and not only they're frozen they're pre computed so the data orientation doesn't do anything at that point", "tokens": [1779, 558, 370, 309, 311, 293, 406, 787, 436, 434, 12496, 436, 434, 659, 40610, 370, 264, 1412, 14764, 1177, 380, 360, 1340, 412, 300, 935], "temperature": 0.0, "avg_logprob": -0.3818316252335258, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.00011410684965085238}, {"id": 1234, "seek": 597508, "start": 5999.88, "end": 6002.72, "text": " Right before you own freeze everything", "tokens": [1779, 949, 291, 1065, 15959, 1203], "temperature": 0.0, "avg_logprob": -0.3818316252335258, "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.00011410684965085238}, {"id": 1235, "seek": 600272, "start": 6002.72, "end": 6008.04, "text": " What does exactly do like you only you only offer is the activation is that", "tokens": [708, 775, 2293, 360, 411, 291, 787, 291, 787, 2626, 307, 264, 24433, 307, 300], "temperature": 0.0, "avg_logprob": -0.23177329353664233, "compression_ratio": 1.6837606837606838, "no_speech_prob": 2.902273990912363e-06}, {"id": 1236, "seek": 600272, "start": 6008.6, "end": 6013.66, "text": " So we're going to learn more about the details as we look into the math and stuff in coming lessons", "tokens": [407, 321, 434, 516, 281, 1466, 544, 466, 264, 4365, 382, 321, 574, 666, 264, 5221, 293, 1507, 294, 1348, 8820], "temperature": 0.0, "avg_logprob": -0.23177329353664233, "compression_ratio": 1.6837606837606838, "no_speech_prob": 2.902273990912363e-06}, {"id": 1237, "seek": 600272, "start": 6013.66, "end": 6018.4800000000005, "text": " But basically what happened was we started with a pre trained network right?", "tokens": [583, 1936, 437, 2011, 390, 321, 1409, 365, 257, 659, 8895, 3209, 558, 30], "temperature": 0.0, "avg_logprob": -0.23177329353664233, "compression_ratio": 1.6837606837606838, "no_speech_prob": 2.902273990912363e-06}, {"id": 1238, "seek": 600272, "start": 6019.320000000001, "end": 6024.72, "text": " Which was kind of finding activations that had these kind of rich features", "tokens": [3013, 390, 733, 295, 5006, 2430, 763, 300, 632, 613, 733, 295, 4593, 4122], "temperature": 0.0, "avg_logprob": -0.23177329353664233, "compression_ratio": 1.6837606837606838, "no_speech_prob": 2.902273990912363e-06}, {"id": 1239, "seek": 600272, "start": 6026.0, "end": 6029.52, "text": " And we were adding then we add a couple of layers on the end of it", "tokens": [400, 321, 645, 5127, 550, 321, 909, 257, 1916, 295, 7914, 322, 264, 917, 295, 309], "temperature": 0.0, "avg_logprob": -0.23177329353664233, "compression_ratio": 1.6837606837606838, "no_speech_prob": 2.902273990912363e-06}, {"id": 1240, "seek": 602952, "start": 6029.52, "end": 6032.76, "text": " which which which start out random and", "tokens": [597, 597, 597, 722, 484, 4974, 293], "temperature": 0.0, "avg_logprob": -0.2591163032933285, "compression_ratio": 1.9406779661016949, "no_speech_prob": 1.628042696211196e-06}, {"id": 1241, "seek": 602952, "start": 6033.160000000001, "end": 6039.88, "text": " So with freeze equals with with everything frozen and indeed with pre compute equals true all we're learning", "tokens": [407, 365, 15959, 6915, 365, 365, 1203, 12496, 293, 6451, 365, 659, 14722, 6915, 2074, 439, 321, 434, 2539], "temperature": 0.0, "avg_logprob": -0.2591163032933285, "compression_ratio": 1.9406779661016949, "no_speech_prob": 1.628042696211196e-06}, {"id": 1242, "seek": 602952, "start": 6039.88, "end": 6045.4800000000005, "text": " Is to is those couple of layers that we've added and so with pre compute equals true", "tokens": [1119, 281, 307, 729, 1916, 295, 7914, 300, 321, 600, 3869, 293, 370, 365, 659, 14722, 6915, 2074], "temperature": 0.0, "avg_logprob": -0.2591163032933285, "compression_ratio": 1.9406779661016949, "no_speech_prob": 1.628042696211196e-06}, {"id": 1243, "seek": 602952, "start": 6045.4800000000005, "end": 6052.4400000000005, "text": " We actually pretty calculate like how much does this image have something that looks like this eyeball and looks like this face so forth", "tokens": [492, 767, 1238, 8873, 411, 577, 709, 775, 341, 3256, 362, 746, 300, 1542, 411, 341, 38868, 293, 1542, 411, 341, 1851, 370, 5220], "temperature": 0.0, "avg_logprob": -0.2591163032933285, "compression_ratio": 1.9406779661016949, "no_speech_prob": 1.628042696211196e-06}, {"id": 1244, "seek": 602952, "start": 6053.0, "end": 6057.360000000001, "text": " and therefore data augmentation doesn't do anything with pre compute equals true because", "tokens": [293, 4412, 1412, 14501, 19631, 1177, 380, 360, 1340, 365, 659, 14722, 6915, 2074, 570], "temperature": 0.0, "avg_logprob": -0.2591163032933285, "compression_ratio": 1.9406779661016949, "no_speech_prob": 1.628042696211196e-06}, {"id": 1245, "seek": 605736, "start": 6057.36, "end": 6061.08, "text": " You know we're actually showing exactly the same activations each time", "tokens": [509, 458, 321, 434, 767, 4099, 2293, 264, 912, 2430, 763, 1184, 565], "temperature": 0.0, "avg_logprob": -0.16066870358910892, "compression_ratio": 1.718045112781955, "no_speech_prob": 2.2959081888984656e-06}, {"id": 1246, "seek": 605736, "start": 6061.5599999999995, "end": 6065.24, "text": " We can then set pre compute equals false which means it's still only", "tokens": [492, 393, 550, 992, 659, 14722, 6915, 7908, 597, 1355, 309, 311, 920, 787], "temperature": 0.0, "avg_logprob": -0.16066870358910892, "compression_ratio": 1.718045112781955, "no_speech_prob": 2.2959081888984656e-06}, {"id": 1247, "seek": 605736, "start": 6066.04, "end": 6072.2, "text": " Training those last two layers that we added it's still frozen, but data augmentations now working", "tokens": [20620, 729, 1036, 732, 7914, 300, 321, 3869, 309, 311, 920, 12496, 11, 457, 1412, 29919, 763, 586, 1364], "temperature": 0.0, "avg_logprob": -0.16066870358910892, "compression_ratio": 1.718045112781955, "no_speech_prob": 2.2959081888984656e-06}, {"id": 1248, "seek": 605736, "start": 6072.2, "end": 6075.48, "text": " It's actually going through and recap letting all of the activations from scratch", "tokens": [467, 311, 767, 516, 807, 293, 20928, 8295, 439, 295, 264, 2430, 763, 490, 8459], "temperature": 0.0, "avg_logprob": -0.16066870358910892, "compression_ratio": 1.718045112781955, "no_speech_prob": 2.2959081888984656e-06}, {"id": 1249, "seek": 605736, "start": 6076.599999999999, "end": 6082.679999999999, "text": " And then finally when we unfreeze that's actually saying okay now you can go ahead and change all of these earlier", "tokens": [400, 550, 2721, 562, 321, 3971, 701, 1381, 300, 311, 767, 1566, 1392, 586, 291, 393, 352, 2286, 293, 1319, 439, 295, 613, 3071], "temperature": 0.0, "avg_logprob": -0.16066870358910892, "compression_ratio": 1.718045112781955, "no_speech_prob": 2.2959081888984656e-06}, {"id": 1250, "seek": 608268, "start": 6082.68, "end": 6088.68, "text": " convolutional filters so what you just do pre compute response and when you unfreeze the last layer", "tokens": [45216, 304, 15995, 370, 437, 291, 445, 360, 659, 14722, 4134, 293, 562, 291, 3971, 701, 1381, 264, 1036, 4583], "temperature": 0.0, "avg_logprob": -0.30304001316879736, "compression_ratio": 1.7280701754385965, "no_speech_prob": 4.289304797566729e-06}, {"id": 1251, "seek": 608268, "start": 6089.52, "end": 6093.34, "text": " So the only reason to have pre compute equals true is it's just much faster", "tokens": [407, 264, 787, 1778, 281, 362, 659, 14722, 6915, 2074, 307, 309, 311, 445, 709, 4663], "temperature": 0.0, "avg_logprob": -0.30304001316879736, "compression_ratio": 1.7280701754385965, "no_speech_prob": 4.289304797566729e-06}, {"id": 1252, "seek": 608268, "start": 6093.34, "end": 6096.88, "text": " So it's like it is it's about you know ten or more times faster", "tokens": [407, 309, 311, 411, 309, 307, 309, 311, 466, 291, 458, 2064, 420, 544, 1413, 4663], "temperature": 0.0, "avg_logprob": -0.30304001316879736, "compression_ratio": 1.7280701754385965, "no_speech_prob": 4.289304797566729e-06}, {"id": 1253, "seek": 608268, "start": 6097.280000000001, "end": 6103.84, "text": " So particularly if you're working with like quite a large data set you know it can save quite a bit of time, but it's never", "tokens": [407, 4098, 498, 291, 434, 1364, 365, 411, 1596, 257, 2416, 1412, 992, 291, 458, 309, 393, 3155, 1596, 257, 857, 295, 565, 11, 457, 309, 311, 1128], "temperature": 0.0, "avg_logprob": -0.30304001316879736, "compression_ratio": 1.7280701754385965, "no_speech_prob": 4.289304797566729e-06}, {"id": 1254, "seek": 608268, "start": 6105.04, "end": 6107.04, "text": " There's no like comp this like", "tokens": [821, 311, 572, 411, 715, 341, 411], "temperature": 0.0, "avg_logprob": -0.30304001316879736, "compression_ratio": 1.7280701754385965, "no_speech_prob": 4.289304797566729e-06}, {"id": 1255, "seek": 610704, "start": 6107.04, "end": 6114.36, "text": " Accuracy reason ever to use pre compute equals true. It's just a it's just a shortcut. It's also like quite handy if you're like", "tokens": [5725, 374, 2551, 1778, 1562, 281, 764, 659, 14722, 6915, 2074, 13, 467, 311, 445, 257, 309, 311, 445, 257, 24822, 13, 467, 311, 611, 411, 1596, 13239, 498, 291, 434, 411], "temperature": 0.0, "avg_logprob": -0.3326978804189948, "compression_ratio": 1.455958549222798, "no_speech_prob": 1.7502539776614867e-05}, {"id": 1256, "seek": 610704, "start": 6115.16, "end": 6119.8, "text": " Throwing together a quick model. You know it can take a few seconds to create it", "tokens": [22228, 278, 1214, 257, 1702, 2316, 13, 509, 458, 309, 393, 747, 257, 1326, 3949, 281, 1884, 309], "temperature": 0.0, "avg_logprob": -0.3326978804189948, "compression_ratio": 1.455958549222798, "no_speech_prob": 1.7502539776614867e-05}, {"id": 1257, "seek": 610704, "start": 6121.96, "end": 6123.96, "text": " My last question", "tokens": [1222, 1036, 1168], "temperature": 0.0, "avg_logprob": -0.3326978804189948, "compression_ratio": 1.455958549222798, "no_speech_prob": 1.7502539776614867e-05}, {"id": 1258, "seek": 610704, "start": 6128.0, "end": 6130.0, "text": " You have this stage", "tokens": [509, 362, 341, 3233], "temperature": 0.0, "avg_logprob": -0.3326978804189948, "compression_ratio": 1.455958549222798, "no_speech_prob": 1.7502539776614867e-05}, {"id": 1259, "seek": 610704, "start": 6131.56, "end": 6133.56, "text": " What if", "tokens": [708, 498], "temperature": 0.0, "avg_logprob": -0.3326978804189948, "compression_ratio": 1.455958549222798, "no_speech_prob": 1.7502539776614867e-05}, {"id": 1260, "seek": 610704, "start": 6133.56, "end": 6135.56, "text": " Like we just wanted to one", "tokens": [1743, 321, 445, 1415, 281, 472], "temperature": 0.0, "avg_logprob": -0.3326978804189948, "compression_ratio": 1.455958549222798, "no_speech_prob": 1.7502539776614867e-05}, {"id": 1261, "seek": 613556, "start": 6135.56, "end": 6138.280000000001, "text": " initial setting without", "tokens": [5883, 3287, 1553], "temperature": 0.0, "avg_logprob": -0.2551220957438151, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.7502296032034792e-05}, {"id": 1262, "seek": 613556, "start": 6139.400000000001, "end": 6141.400000000001, "text": " Checking out", "tokens": [6881, 278, 484], "temperature": 0.0, "avg_logprob": -0.2551220957438151, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.7502296032034792e-05}, {"id": 1263, "seek": 613556, "start": 6142.76, "end": 6147.54, "text": " Is that I mean if you wanted like if your question is like is there some shorter version of this", "tokens": [1119, 300, 286, 914, 498, 291, 1415, 411, 498, 428, 1168, 307, 411, 307, 456, 512, 11639, 3037, 295, 341], "temperature": 0.0, "avg_logprob": -0.2551220957438151, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.7502296032034792e-05}, {"id": 1264, "seek": 613556, "start": 6147.54, "end": 6151.780000000001, "text": " That's like a bit quicker and easier. I could like to lead a few things here", "tokens": [663, 311, 411, 257, 857, 16255, 293, 3571, 13, 286, 727, 411, 281, 1477, 257, 1326, 721, 510], "temperature": 0.0, "avg_logprob": -0.2551220957438151, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.7502296032034792e-05}, {"id": 1265, "seek": 613556, "start": 6159.64, "end": 6163.820000000001, "text": " Okay, I think this is a kind of a minimal version to get you a very good result", "tokens": [1033, 11, 286, 519, 341, 307, 257, 733, 295, 257, 13206, 3037, 281, 483, 291, 257, 588, 665, 1874], "temperature": 0.0, "avg_logprob": -0.2551220957438151, "compression_ratio": 1.5508021390374331, "no_speech_prob": 1.7502296032034792e-05}, {"id": 1266, "seek": 616382, "start": 6163.82, "end": 6168.34, "text": " Which is like don't worry about pre compute equals true because that's just saving a little bit of time", "tokens": [3013, 307, 411, 500, 380, 3292, 466, 659, 14722, 6915, 2074, 570, 300, 311, 445, 6816, 257, 707, 857, 295, 565], "temperature": 0.0, "avg_logprob": -0.1956600789670591, "compression_ratio": 1.6483516483516483, "no_speech_prob": 1.4510170331050176e-05}, {"id": 1267, "seek": 616382, "start": 6168.599999999999, "end": 6173.84, "text": " You know so so I still suggest use LR find at the start to find a good learning rate", "tokens": [509, 458, 370, 370, 286, 920, 3402, 764, 441, 49, 915, 412, 264, 722, 281, 915, 257, 665, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.1956600789670591, "compression_ratio": 1.6483516483516483, "no_speech_prob": 1.4510170331050176e-05}, {"id": 1268, "seek": 616382, "start": 6175.5199999999995, "end": 6181.66, "text": " By default everything is frozen from the start so then you can just go ahead and run a two or three epochs of cyclin pickles one", "tokens": [3146, 7576, 1203, 307, 12496, 490, 264, 722, 370, 550, 291, 393, 445, 352, 2286, 293, 1190, 257, 732, 420, 1045, 30992, 28346, 295, 19474, 259, 1888, 904, 472], "temperature": 0.0, "avg_logprob": -0.1956600789670591, "compression_ratio": 1.6483516483516483, "no_speech_prob": 1.4510170331050176e-05}, {"id": 1269, "seek": 616382, "start": 6182.5599999999995, "end": 6183.84, "text": " unfreeze", "tokens": [3971, 701, 1381], "temperature": 0.0, "avg_logprob": -0.1956600789670591, "compression_ratio": 1.6483516483516483, "no_speech_prob": 1.4510170331050176e-05}, {"id": 1270, "seek": 616382, "start": 6183.84, "end": 6190.84, "text": " And then train the rest of the network with differential learning rates, so it's basically three steps learning rate finder", "tokens": [400, 550, 3847, 264, 1472, 295, 264, 3209, 365, 15756, 2539, 6846, 11, 370, 309, 311, 1936, 1045, 4439, 2539, 3314, 915, 260], "temperature": 0.0, "avg_logprob": -0.1956600789670591, "compression_ratio": 1.6483516483516483, "no_speech_prob": 1.4510170331050176e-05}, {"id": 1271, "seek": 619084, "start": 6190.84, "end": 6192.84, "text": " train", "tokens": [3847], "temperature": 0.0, "avg_logprob": -0.3807305083877739, "compression_ratio": 1.6018099547511313, "no_speech_prob": 4.565913968690438e-06}, {"id": 1272, "seek": 619084, "start": 6192.860000000001, "end": 6195.72, "text": " frozen network with cycling pickles one and", "tokens": [12496, 3209, 365, 22425, 1888, 904, 472, 293], "temperature": 0.0, "avg_logprob": -0.3807305083877739, "compression_ratio": 1.6018099547511313, "no_speech_prob": 4.565913968690438e-06}, {"id": 1273, "seek": 619084, "start": 6196.32, "end": 6201.04, "text": " Then train unfrozen network with differential learning rates and cycle molecules two", "tokens": [1396, 3847, 3971, 340, 2904, 3209, 365, 15756, 2539, 6846, 293, 6586, 13093, 732], "temperature": 0.0, "avg_logprob": -0.3807305083877739, "compression_ratio": 1.6018099547511313, "no_speech_prob": 4.565913968690438e-06}, {"id": 1274, "seek": 619084, "start": 6201.72, "end": 6204.400000000001, "text": " so like that's something you could turn into I", "tokens": [370, 411, 300, 311, 746, 291, 727, 1261, 666, 286], "temperature": 0.0, "avg_logprob": -0.3807305083877739, "compression_ratio": 1.6018099547511313, "no_speech_prob": 4.565913968690438e-06}, {"id": 1275, "seek": 619084, "start": 6205.360000000001, "end": 6207.52, "text": " Guess five or six lines of code", "tokens": [17795, 1732, 420, 2309, 3876, 295, 3089], "temperature": 0.0, "avg_logprob": -0.3807305083877739, "compression_ratio": 1.6018099547511313, "no_speech_prob": 4.565913968690438e-06}, {"id": 1276, "seek": 619084, "start": 6208.32, "end": 6210.08, "text": " total", "tokens": [3217], "temperature": 0.0, "avg_logprob": -0.3807305083877739, "compression_ratio": 1.6018099547511313, "no_speech_prob": 4.565913968690438e-06}, {"id": 1277, "seek": 619084, "start": 6210.08, "end": 6212.24, "text": " I think it's a question behind you oh next door", "tokens": [286, 519, 309, 311, 257, 1168, 2261, 291, 1954, 958, 2853], "temperature": 0.0, "avg_logprob": -0.3807305083877739, "compression_ratio": 1.6018099547511313, "no_speech_prob": 4.565913968690438e-06}, {"id": 1278, "seek": 619084, "start": 6213.68, "end": 6219.72, "text": " By reducing the batch size does it only affect the speed of training? Yeah pretty much", "tokens": [3146, 12245, 264, 15245, 2744, 775, 309, 787, 3345, 264, 3073, 295, 3097, 30, 865, 1238, 709], "temperature": 0.0, "avg_logprob": -0.3807305083877739, "compression_ratio": 1.6018099547511313, "no_speech_prob": 4.565913968690438e-06}, {"id": 1279, "seek": 621972, "start": 6219.72, "end": 6224.84, "text": " So each batch and again we're going to see like all this stuff about pre computing batch sizes", "tokens": [407, 1184, 15245, 293, 797, 321, 434, 516, 281, 536, 411, 439, 341, 1507, 466, 659, 15866, 15245, 11602], "temperature": 0.0, "avg_logprob": -0.15523673118428982, "compression_ratio": 1.7317073170731707, "no_speech_prob": 2.6995704160981404e-07}, {"id": 1280, "seek": 621972, "start": 6224.84, "end": 6231.4400000000005, "text": " We dig into the details of the algorithms. It's going to make a lot more sense intuitively, but basically if you're showing it", "tokens": [492, 2528, 666, 264, 4365, 295, 264, 14642, 13, 467, 311, 516, 281, 652, 257, 688, 544, 2020, 46506, 11, 457, 1936, 498, 291, 434, 4099, 309], "temperature": 0.0, "avg_logprob": -0.15523673118428982, "compression_ratio": 1.7317073170731707, "no_speech_prob": 2.6995704160981404e-07}, {"id": 1281, "seek": 621972, "start": 6232.56, "end": 6238.240000000001, "text": " Less images each time then it's calculating the gradient with less images", "tokens": [18649, 5267, 1184, 565, 550, 309, 311, 28258, 264, 16235, 365, 1570, 5267], "temperature": 0.0, "avg_logprob": -0.15523673118428982, "compression_ratio": 1.7317073170731707, "no_speech_prob": 2.6995704160981404e-07}, {"id": 1282, "seek": 621972, "start": 6238.240000000001, "end": 6245.04, "text": " Which means it's less accurate which means like knowing which direction to go and how far to go in that direction is less accurate", "tokens": [3013, 1355, 309, 311, 1570, 8559, 597, 1355, 411, 5276, 597, 3513, 281, 352, 293, 577, 1400, 281, 352, 294, 300, 3513, 307, 1570, 8559], "temperature": 0.0, "avg_logprob": -0.15523673118428982, "compression_ratio": 1.7317073170731707, "no_speech_prob": 2.6995704160981404e-07}, {"id": 1283, "seek": 624504, "start": 6245.04, "end": 6249.84, "text": " So as you make the batch size baller you're basically making it kind of more volatile", "tokens": [407, 382, 291, 652, 264, 15245, 2744, 2594, 260, 291, 434, 1936, 1455, 309, 733, 295, 544, 34377], "temperature": 0.0, "avg_logprob": -0.25726805342004655, "compression_ratio": 1.6561085972850678, "no_speech_prob": 8.267760676972102e-06}, {"id": 1284, "seek": 624504, "start": 6250.76, "end": 6252.76, "text": " it's kind of like", "tokens": [309, 311, 733, 295, 411], "temperature": 0.0, "avg_logprob": -0.25726805342004655, "compression_ratio": 1.6561085972850678, "no_speech_prob": 8.267760676972102e-06}, {"id": 1285, "seek": 624504, "start": 6254.92, "end": 6256.92, "text": " It kind of impacts the", "tokens": [467, 733, 295, 11606, 264], "temperature": 0.0, "avg_logprob": -0.25726805342004655, "compression_ratio": 1.6561085972850678, "no_speech_prob": 8.267760676972102e-06}, {"id": 1286, "seek": 624504, "start": 6258.64, "end": 6261.56, "text": " Optimal learning rate that you would need to use but in practice", "tokens": [21455, 10650, 2539, 3314, 300, 291, 576, 643, 281, 764, 457, 294, 3124], "temperature": 0.0, "avg_logprob": -0.25726805342004655, "compression_ratio": 1.6561085972850678, "no_speech_prob": 8.267760676972102e-06}, {"id": 1287, "seek": 624504, "start": 6261.56, "end": 6266.64, "text": " We're only you know I generally find I'm only dividing the batch size by like two or at most four", "tokens": [492, 434, 787, 291, 458, 286, 5101, 915, 286, 478, 787, 26764, 264, 15245, 2744, 538, 411, 732, 420, 412, 881, 1451], "temperature": 0.0, "avg_logprob": -0.25726805342004655, "compression_ratio": 1.6561085972850678, "no_speech_prob": 8.267760676972102e-06}, {"id": 1288, "seek": 624504, "start": 6266.64, "end": 6270.6, "text": " It doesn't seem to change things very much should I reduce the line of rate?", "tokens": [467, 1177, 380, 1643, 281, 1319, 721, 588, 709, 820, 286, 5407, 264, 1622, 295, 3314, 30], "temperature": 0.0, "avg_logprob": -0.25726805342004655, "compression_ratio": 1.6561085972850678, "no_speech_prob": 8.267760676972102e-06}, {"id": 1289, "seek": 627060, "start": 6270.6, "end": 6277.280000000001, "text": " Accordingly if you if you change the batch size by much you can rerun the learning rate finder to see if it's changed if I'm", "tokens": [7328, 356, 498, 291, 498, 291, 1319, 264, 15245, 2744, 538, 709, 291, 393, 43819, 409, 264, 2539, 3314, 915, 260, 281, 536, 498, 309, 311, 3105, 498, 286, 478], "temperature": 0.0, "avg_logprob": -0.33553546970173465, "compression_ratio": 1.7907801418439717, "no_speech_prob": 2.1444073354359716e-05}, {"id": 1290, "seek": 627060, "start": 6277.280000000001, "end": 6280.84, "text": " Not sure if it you know since we're only generally looking at like a power of 10", "tokens": [1726, 988, 498, 309, 291, 458, 1670, 321, 434, 787, 5101, 1237, 412, 411, 257, 1347, 295, 1266], "temperature": 0.0, "avg_logprob": -0.33553546970173465, "compression_ratio": 1.7907801418439717, "no_speech_prob": 2.1444073354359716e-05}, {"id": 1291, "seek": 627060, "start": 6280.84, "end": 6284.96, "text": " It probably is not going to change things enough that you care because possibly", "tokens": [467, 1391, 307, 406, 516, 281, 1319, 721, 1547, 300, 291, 1127, 570, 6264], "temperature": 0.0, "avg_logprob": -0.33553546970173465, "compression_ratio": 1.7907801418439717, "no_speech_prob": 2.1444073354359716e-05}, {"id": 1292, "seek": 627060, "start": 6287.96, "end": 6289.96, "text": " This is sort of a conceptual", "tokens": [639, 307, 1333, 295, 257, 24106], "temperature": 0.0, "avg_logprob": -0.33553546970173465, "compression_ratio": 1.7907801418439717, "no_speech_prob": 2.1444073354359716e-05}, {"id": 1293, "seek": 627060, "start": 6290.4400000000005, "end": 6295.280000000001, "text": " Question so going back to the previous slide where you showed could you live better than I am sorry yeah", "tokens": [14464, 370, 516, 646, 281, 264, 3894, 4137, 689, 291, 4712, 727, 291, 1621, 1101, 813, 286, 669, 2597, 1338], "temperature": 0.0, "avg_logprob": -0.33553546970173465, "compression_ratio": 1.7907801418439717, "no_speech_prob": 2.1444073354359716e-05}, {"id": 1294, "seek": 627060, "start": 6295.280000000001, "end": 6299.92, "text": " This is more of a conceptual sort of basic question going back to your previous slide", "tokens": [639, 307, 544, 295, 257, 24106, 1333, 295, 3875, 1168, 516, 646, 281, 428, 3894, 4137], "temperature": 0.0, "avg_logprob": -0.33553546970173465, "compression_ratio": 1.7907801418439717, "no_speech_prob": 2.1444073354359716e-05}, {"id": 1295, "seek": 629992, "start": 6299.92, "end": 6301.92, "text": " Where you showed what the different layers were doing?", "tokens": [2305, 291, 4712, 437, 264, 819, 7914, 645, 884, 30], "temperature": 0.0, "avg_logprob": -0.2916496362579003, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.3631167348648887e-05}, {"id": 1296, "seek": 629992, "start": 6305.96, "end": 6310.26, "text": " Yeah, so this slide I understand right meaning of", "tokens": [865, 11, 370, 341, 4137, 286, 1223, 558, 3620, 295], "temperature": 0.0, "avg_logprob": -0.2916496362579003, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.3631167348648887e-05}, {"id": 1297, "seek": 629992, "start": 6310.8, "end": 6313.96, "text": " See the third column relative to the fourth column is that", "tokens": [3008, 264, 2636, 7738, 4972, 281, 264, 6409, 7738, 307, 300], "temperature": 0.0, "avg_logprob": -0.2916496362579003, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.3631167348648887e-05}, {"id": 1298, "seek": 629992, "start": 6314.8, "end": 6320.64, "text": " What you're interpreting what the layer is doing based on what images actually?", "tokens": [708, 291, 434, 37395, 437, 264, 4583, 307, 884, 2361, 322, 437, 5267, 767, 30], "temperature": 0.0, "avg_logprob": -0.2916496362579003, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.3631167348648887e-05}, {"id": 1299, "seek": 629992, "start": 6322.12, "end": 6324.2, "text": " Yeah, so we're going to look at this in more detail", "tokens": [865, 11, 370, 321, 434, 516, 281, 574, 412, 341, 294, 544, 2607], "temperature": 0.0, "avg_logprob": -0.2916496362579003, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.3631167348648887e-05}, {"id": 1300, "seek": 629992, "start": 6324.2, "end": 6328.12, "text": " So these these gray ones basically say this is kind of what the filter looks like", "tokens": [407, 613, 613, 10855, 2306, 1936, 584, 341, 307, 733, 295, 437, 264, 6608, 1542, 411], "temperature": 0.0, "avg_logprob": -0.2916496362579003, "compression_ratio": 1.6755555555555555, "no_speech_prob": 1.3631167348648887e-05}, {"id": 1301, "seek": 632812, "start": 6328.12, "end": 6333.84, "text": " So on the first layer you can say exactly what the filter looks like because the input to it at pixels", "tokens": [407, 322, 264, 700, 4583, 291, 393, 584, 2293, 437, 264, 6608, 1542, 411, 570, 264, 4846, 281, 309, 412, 18668], "temperature": 0.0, "avg_logprob": -0.19982602861192492, "compression_ratio": 2.0502092050209204, "no_speech_prob": 3.668845010906807e-06}, {"id": 1302, "seek": 632812, "start": 6334.0, "end": 6340.0199999999995, "text": " Right so you can absolutely say and remember we looked at what a convolutional kernel was like is that three by three thing", "tokens": [1779, 370, 291, 393, 3122, 584, 293, 1604, 321, 2956, 412, 437, 257, 45216, 304, 28256, 390, 411, 307, 300, 1045, 538, 1045, 551], "temperature": 0.0, "avg_logprob": -0.19982602861192492, "compression_ratio": 2.0502092050209204, "no_speech_prob": 3.668845010906807e-06}, {"id": 1303, "seek": 632812, "start": 6340.2, "end": 6342.92, "text": " So this looked like there's seven by seven kernels", "tokens": [407, 341, 2956, 411, 456, 311, 3407, 538, 3407, 23434, 1625], "temperature": 0.0, "avg_logprob": -0.19982602861192492, "compression_ratio": 2.0502092050209204, "no_speech_prob": 3.668845010906807e-06}, {"id": 1304, "seek": 632812, "start": 6342.92, "end": 6347.08, "text": " You can say this is actually what it looks like but later on it's combining", "tokens": [509, 393, 584, 341, 307, 767, 437, 309, 1542, 411, 457, 1780, 322, 309, 311, 21928], "temperature": 0.0, "avg_logprob": -0.19982602861192492, "compression_ratio": 2.0502092050209204, "no_speech_prob": 3.668845010906807e-06}, {"id": 1305, "seek": 632812, "start": 6347.08, "end": 6354.04, "text": " You know that the the input to it are themselves activations which are combinations of activations which are combinations of activations", "tokens": [509, 458, 300, 264, 264, 4846, 281, 309, 366, 2969, 2430, 763, 597, 366, 21267, 295, 2430, 763, 597, 366, 21267, 295, 2430, 763], "temperature": 0.0, "avg_logprob": -0.19982602861192492, "compression_ratio": 2.0502092050209204, "no_speech_prob": 3.668845010906807e-06}, {"id": 1306, "seek": 635404, "start": 6354.04, "end": 6359.5199999999995, "text": " So you can't draw it, but there's clever technique that's I learned Fergus created which allowed them to say", "tokens": [407, 291, 393, 380, 2642, 309, 11, 457, 456, 311, 13494, 6532, 300, 311, 286, 3264, 36790, 2942, 597, 4350, 552, 281, 584], "temperature": 0.0, "avg_logprob": -0.21377836983158904, "compression_ratio": 1.73046875, "no_speech_prob": 1.963795966730686e-06}, {"id": 1307, "seek": 635404, "start": 6359.56, "end": 6363.04, "text": " This is kind of what the filters tended to look like on average", "tokens": [639, 307, 733, 295, 437, 264, 15995, 34732, 281, 574, 411, 322, 4274], "temperature": 0.0, "avg_logprob": -0.21377836983158904, "compression_ratio": 1.73046875, "no_speech_prob": 1.963795966730686e-06}, {"id": 1308, "seek": 635404, "start": 6363.44, "end": 6370.1, "text": " Right so this is kind of what the filters look like and then here is specific examples of patches of image", "tokens": [1779, 370, 341, 307, 733, 295, 437, 264, 15995, 574, 411, 293, 550, 510, 307, 2685, 5110, 295, 26531, 295, 3256], "temperature": 0.0, "avg_logprob": -0.21377836983158904, "compression_ratio": 1.73046875, "no_speech_prob": 1.963795966730686e-06}, {"id": 1309, "seek": 635404, "start": 6370.56, "end": 6371.92, "text": " which", "tokens": [597], "temperature": 0.0, "avg_logprob": -0.21377836983158904, "compression_ratio": 1.73046875, "no_speech_prob": 1.963795966730686e-06}, {"id": 1310, "seek": 635404, "start": 6371.92, "end": 6378.98, "text": " Activated that filter highly so the pictures are the ones that I kind of find more useful because it tells you this", "tokens": [28550, 770, 300, 6608, 5405, 370, 264, 5242, 366, 264, 2306, 300, 286, 733, 295, 915, 544, 4420, 570, 309, 5112, 291, 341], "temperature": 0.0, "avg_logprob": -0.21377836983158904, "compression_ratio": 1.73046875, "no_speech_prob": 1.963795966730686e-06}, {"id": 1311, "seek": 637898, "start": 6378.98, "end": 6383.0199999999995, "text": " Kernel is kind of a unicycle wheel finder", "tokens": [40224, 338, 307, 733, 295, 257, 517, 2632, 2160, 5589, 915, 260], "temperature": 0.0, "avg_logprob": -0.2659088134765625, "compression_ratio": 1.3535353535353536, "no_speech_prob": 5.955068445473444e-06}, {"id": 1312, "seek": 637898, "start": 6398.9, "end": 6402.86, "text": " Well we'll come back well we may come back to that if not in this part in the next part that", "tokens": [1042, 321, 603, 808, 646, 731, 321, 815, 808, 646, 281, 300, 498, 406, 294, 341, 644, 294, 264, 958, 644, 300], "temperature": 0.0, "avg_logprob": -0.2659088134765625, "compression_ratio": 1.3535353535353536, "no_speech_prob": 5.955068445473444e-06}, {"id": 1313, "seek": 640286, "start": 6402.86, "end": 6411.259999999999, "text": " Probably in part two actually because this paper this paper uses to create these things this paper uses something called a deconvolution", "tokens": [9210, 294, 644, 732, 767, 570, 341, 3035, 341, 3035, 4960, 281, 1884, 613, 721, 341, 3035, 4960, 746, 1219, 257, 979, 266, 85, 3386], "temperature": 0.0, "avg_logprob": -0.19336466886559311, "compression_ratio": 1.6784140969162995, "no_speech_prob": 6.962093266338343e-06}, {"id": 1314, "seek": 640286, "start": 6411.94, "end": 6416.62, "text": " Which I'm pretty sure we won't do in this part, but we will do it in part two so if you're interested", "tokens": [3013, 286, 478, 1238, 988, 321, 1582, 380, 360, 294, 341, 644, 11, 457, 321, 486, 360, 309, 294, 644, 732, 370, 498, 291, 434, 3102], "temperature": 0.0, "avg_logprob": -0.19336466886559311, "compression_ratio": 1.6784140969162995, "no_speech_prob": 6.962093266338343e-06}, {"id": 1315, "seek": 640286, "start": 6417.86, "end": 6422.2, "text": " Check out the paper. It's in the notebook. There's a link to it. It's either in Fergus", "tokens": [6881, 484, 264, 3035, 13, 467, 311, 294, 264, 21060, 13, 821, 311, 257, 2113, 281, 309, 13, 467, 311, 2139, 294, 36790], "temperature": 0.0, "avg_logprob": -0.19336466886559311, "compression_ratio": 1.6784140969162995, "no_speech_prob": 6.962093266338343e-06}, {"id": 1316, "seek": 640286, "start": 6423.339999999999, "end": 6425.339999999999, "text": " It's a very clever technique", "tokens": [467, 311, 257, 588, 13494, 6532], "temperature": 0.0, "avg_logprob": -0.19336466886559311, "compression_ratio": 1.6784140969162995, "no_speech_prob": 6.962093266338343e-06}, {"id": 1317, "seek": 640286, "start": 6425.54, "end": 6427.38, "text": " and not", "tokens": [293, 406], "temperature": 0.0, "avg_logprob": -0.19336466886559311, "compression_ratio": 1.6784140969162995, "no_speech_prob": 6.962093266338343e-06}, {"id": 1318, "seek": 640286, "start": 6427.38, "end": 6429.38, "text": " terribly intuitive", "tokens": [22903, 21769], "temperature": 0.0, "avg_logprob": -0.19336466886559311, "compression_ratio": 1.6784140969162995, "no_speech_prob": 6.962093266338343e-06}, {"id": 1319, "seek": 642938, "start": 6429.38, "end": 6431.66, "text": " Right so", "tokens": [1779, 370], "temperature": 0.0, "avg_logprob": -0.2503215789794922, "compression_ratio": 1.6205128205128205, "no_speech_prob": 6.438937361963326e-06}, {"id": 1320, "seek": 642938, "start": 6437.1, "end": 6440.86, "text": " So you mentioned that it was good that the dog took up the full picture", "tokens": [407, 291, 2835, 300, 309, 390, 665, 300, 264, 3000, 1890, 493, 264, 1577, 3036], "temperature": 0.0, "avg_logprob": -0.2503215789794922, "compression_ratio": 1.6205128205128205, "no_speech_prob": 6.438937361963326e-06}, {"id": 1321, "seek": 642938, "start": 6440.86, "end": 6445.42, "text": " And it would have been a problem if it was kind of like off in one of the corners and really tiny", "tokens": [400, 309, 576, 362, 668, 257, 1154, 498, 309, 390, 733, 295, 411, 766, 294, 472, 295, 264, 12413, 293, 534, 5870], "temperature": 0.0, "avg_logprob": -0.2503215789794922, "compression_ratio": 1.6205128205128205, "no_speech_prob": 6.438937361963326e-06}, {"id": 1322, "seek": 642938, "start": 6446.3, "end": 6451.9800000000005, "text": " What what would you have done? What would your technique have been to try to make that work?", "tokens": [708, 437, 576, 291, 362, 1096, 30, 708, 576, 428, 6532, 362, 668, 281, 853, 281, 652, 300, 589, 30], "temperature": 0.0, "avg_logprob": -0.2503215789794922, "compression_ratio": 1.6205128205128205, "no_speech_prob": 6.438937361963326e-06}, {"id": 1323, "seek": 642938, "start": 6453.5, "end": 6455.5, "text": " Something that we'll learn about in part two", "tokens": [6595, 300, 321, 603, 1466, 466, 294, 644, 732], "temperature": 0.0, "avg_logprob": -0.2503215789794922, "compression_ratio": 1.6205128205128205, "no_speech_prob": 6.438937361963326e-06}, {"id": 1324, "seek": 645550, "start": 6455.5, "end": 6460.62, "text": " but basically there's a technique that allows you to kind of", "tokens": [457, 1936, 456, 311, 257, 6532, 300, 4045, 291, 281, 733, 295], "temperature": 0.0, "avg_logprob": -0.18407627027861925, "compression_ratio": 1.6385542168674698, "no_speech_prob": 8.139472811308224e-06}, {"id": 1325, "seek": 645550, "start": 6460.9, "end": 6465.7, "text": " Pick her out roughly which parts of an image are most likely to have the interesting things in them", "tokens": [14129, 720, 484, 9810, 597, 3166, 295, 364, 3256, 366, 881, 3700, 281, 362, 264, 1880, 721, 294, 552], "temperature": 0.0, "avg_logprob": -0.18407627027861925, "compression_ratio": 1.6385542168674698, "no_speech_prob": 8.139472811308224e-06}, {"id": 1326, "seek": 645550, "start": 6465.7, "end": 6470.76, "text": " And then you can like crop out those bits if you're interested in learning about it", "tokens": [400, 550, 291, 393, 411, 9086, 484, 729, 9239, 498, 291, 434, 3102, 294, 2539, 466, 309], "temperature": 0.0, "avg_logprob": -0.18407627027861925, "compression_ratio": 1.6385542168674698, "no_speech_prob": 8.139472811308224e-06}, {"id": 1327, "seek": 645550, "start": 6470.76, "end": 6476.46, "text": " We did cover it briefly in lesson 7 of part 1, but I'm going to actually do it", "tokens": [492, 630, 2060, 309, 10515, 294, 6898, 1614, 295, 644, 502, 11, 457, 286, 478, 516, 281, 767, 360, 309], "temperature": 0.0, "avg_logprob": -0.18407627027861925, "compression_ratio": 1.6385542168674698, "no_speech_prob": 8.139472811308224e-06}, {"id": 1328, "seek": 645550, "start": 6477.54, "end": 6479.54, "text": " Properly in part 2 of this course", "tokens": [27627, 356, 294, 644, 568, 295, 341, 1164], "temperature": 0.0, "avg_logprob": -0.18407627027861925, "compression_ratio": 1.6385542168674698, "no_speech_prob": 8.139472811308224e-06}, {"id": 1329, "seek": 647954, "start": 6479.54, "end": 6484.98, "text": " Because I didn't really cover it thoroughly enough. Yeah, maybe", "tokens": [1436, 286, 994, 380, 534, 2060, 309, 17987, 1547, 13, 865, 11, 1310], "temperature": 0.0, "avg_logprob": -0.27679654439290363, "compression_ratio": 1.4829268292682927, "no_speech_prob": 4.710792381956708e-06}, {"id": 1330, "seek": 647954, "start": 6486.14, "end": 6491.82, "text": " We'll find time to have a quick look at it, but we'll see I know you nets written some of the code that we need already", "tokens": [492, 603, 915, 565, 281, 362, 257, 1702, 574, 412, 309, 11, 457, 321, 603, 536, 286, 458, 291, 36170, 3720, 512, 295, 264, 3089, 300, 321, 643, 1217], "temperature": 0.0, "avg_logprob": -0.27679654439290363, "compression_ratio": 1.4829268292682927, "no_speech_prob": 4.710792381956708e-06}, {"id": 1331, "seek": 647954, "start": 6496.78, "end": 6502.9, "text": " So once I have something like this notebook, it's basically working I can", "tokens": [407, 1564, 286, 362, 746, 411, 341, 21060, 11, 309, 311, 1936, 1364, 286, 393], "temperature": 0.0, "avg_logprob": -0.27679654439290363, "compression_ratio": 1.4829268292682927, "no_speech_prob": 4.710792381956708e-06}, {"id": 1332, "seek": 647954, "start": 6505.14, "end": 6508.42, "text": " Immediately make it better by doing two things", "tokens": [34457, 652, 309, 1101, 538, 884, 732, 721], "temperature": 0.0, "avg_logprob": -0.27679654439290363, "compression_ratio": 1.4829268292682927, "no_speech_prob": 4.710792381956708e-06}, {"id": 1333, "seek": 650842, "start": 6508.42, "end": 6515.9400000000005, "text": " Assuming that the size image I was using is smaller than the average size of the image that we've been given", "tokens": [6281, 24919, 300, 264, 2744, 3256, 286, 390, 1228, 307, 4356, 813, 264, 4274, 2744, 295, 264, 3256, 300, 321, 600, 668, 2212], "temperature": 0.0, "avg_logprob": -0.16397451317828635, "compression_ratio": 1.7757847533632287, "no_speech_prob": 5.955093911325093e-06}, {"id": 1334, "seek": 650842, "start": 6515.9400000000005, "end": 6521.82, "text": " I can increase the size and as I showed before with the dog breeds you can actually increase it during training", "tokens": [286, 393, 3488, 264, 2744, 293, 382, 286, 4712, 949, 365, 264, 3000, 41609, 291, 393, 767, 3488, 309, 1830, 3097], "temperature": 0.0, "avg_logprob": -0.16397451317828635, "compression_ratio": 1.7757847533632287, "no_speech_prob": 5.955093911325093e-06}, {"id": 1335, "seek": 650842, "start": 6522.42, "end": 6526.7, "text": " The other thing I can do is to create is to use a better architecture", "tokens": [440, 661, 551, 286, 393, 360, 307, 281, 1884, 307, 281, 764, 257, 1101, 9482], "temperature": 0.0, "avg_logprob": -0.16397451317828635, "compression_ratio": 1.7757847533632287, "no_speech_prob": 5.955093911325093e-06}, {"id": 1336, "seek": 650842, "start": 6527.5, "end": 6532.38, "text": " Now an architecture we're going to talk a lot in this course about architectures, but basically", "tokens": [823, 364, 9482, 321, 434, 516, 281, 751, 257, 688, 294, 341, 1164, 466, 6331, 1303, 11, 457, 1936], "temperature": 0.0, "avg_logprob": -0.16397451317828635, "compression_ratio": 1.7757847533632287, "no_speech_prob": 5.955093911325093e-06}, {"id": 1337, "seek": 650842, "start": 6533.34, "end": 6535.34, "text": " there are", "tokens": [456, 366], "temperature": 0.0, "avg_logprob": -0.16397451317828635, "compression_ratio": 1.7757847533632287, "no_speech_prob": 5.955093911325093e-06}, {"id": 1338, "seek": 653534, "start": 6535.34, "end": 6544.14, "text": " Different ways of putting together like what size convolutional filters, and how are they connected just to each other and so forth and", "tokens": [20825, 2098, 295, 3372, 1214, 411, 437, 2744, 45216, 304, 15995, 11, 293, 577, 366, 436, 4582, 445, 281, 1184, 661, 293, 370, 5220, 293], "temperature": 0.0, "avg_logprob": -0.21120924345204528, "compression_ratio": 1.634517766497462, "no_speech_prob": 2.994414671775303e-06}, {"id": 1339, "seek": 653534, "start": 6546.22, "end": 6553.06, "text": " Different architectures have different like numbers of layers and sizes of kernels number of filters and so forth and so", "tokens": [20825, 6331, 1303, 362, 819, 411, 3547, 295, 7914, 293, 11602, 295, 23434, 1625, 1230, 295, 15995, 293, 370, 5220, 293, 370], "temperature": 0.0, "avg_logprob": -0.21120924345204528, "compression_ratio": 1.634517766497462, "no_speech_prob": 2.994414671775303e-06}, {"id": 1340, "seek": 653534, "start": 6554.74, "end": 6558.88, "text": " There are some the one that we've been using resnet 34 is a great", "tokens": [821, 366, 512, 264, 472, 300, 321, 600, 668, 1228, 725, 7129, 12790, 307, 257, 869], "temperature": 0.0, "avg_logprob": -0.21120924345204528, "compression_ratio": 1.634517766497462, "no_speech_prob": 2.994414671775303e-06}, {"id": 1341, "seek": 655888, "start": 6558.88, "end": 6566.36, "text": " Starting point and often a good finishing point because it's like it's pretty it doesn't have too many parameters often", "tokens": [16217, 935, 293, 2049, 257, 665, 12693, 935, 570, 309, 311, 411, 309, 311, 1238, 309, 1177, 380, 362, 886, 867, 9834, 2049], "temperature": 0.0, "avg_logprob": -0.17914922377642464, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.2887214729271363e-06}, {"id": 1342, "seek": 655888, "start": 6566.36, "end": 6569.52, "text": " It works pretty well with small amounts of data as we've seen and so forth", "tokens": [467, 1985, 1238, 731, 365, 1359, 11663, 295, 1412, 382, 321, 600, 1612, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.17914922377642464, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.2887214729271363e-06}, {"id": 1343, "seek": 655888, "start": 6571.28, "end": 6576.4800000000005, "text": " But there's actually an architecture that I really like called not resnet but resnext", "tokens": [583, 456, 311, 767, 364, 9482, 300, 286, 534, 411, 1219, 406, 725, 7129, 457, 725, 716, 734], "temperature": 0.0, "avg_logprob": -0.17914922377642464, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.2887214729271363e-06}, {"id": 1344, "seek": 655888, "start": 6576.88, "end": 6581.28, "text": " Which was actually the second place winner in last year's image net competition?", "tokens": [3013, 390, 767, 264, 1150, 1081, 8507, 294, 1036, 1064, 311, 3256, 2533, 6211, 30], "temperature": 0.0, "avg_logprob": -0.17914922377642464, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.2887214729271363e-06}, {"id": 1345, "seek": 655888, "start": 6582.0, "end": 6584.0, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.17914922377642464, "compression_ratio": 1.6367713004484306, "no_speech_prob": 3.2887214729271363e-06}, {"id": 1346, "seek": 658400, "start": 6584.0, "end": 6589.32, "text": " Like resnet you can put a number after the resnext to say like how big it is and", "tokens": [1743, 725, 7129, 291, 393, 829, 257, 1230, 934, 264, 725, 716, 734, 281, 584, 411, 577, 955, 309, 307, 293], "temperature": 0.0, "avg_logprob": -0.2138983836540809, "compression_ratio": 1.7745098039215685, "no_speech_prob": 1.034847855407861e-06}, {"id": 1347, "seek": 658400, "start": 6589.68, "end": 6594.68, "text": " Like my next step after resnet 34 is always resnext 50 now", "tokens": [1743, 452, 958, 1823, 934, 725, 7129, 12790, 307, 1009, 725, 716, 734, 2625, 586], "temperature": 0.0, "avg_logprob": -0.2138983836540809, "compression_ratio": 1.7745098039215685, "no_speech_prob": 1.034847855407861e-06}, {"id": 1348, "seek": 658400, "start": 6594.68, "end": 6598.8, "text": " You'll find resnext 50 takes like and take like twice as long as", "tokens": [509, 603, 915, 725, 716, 734, 2625, 2516, 411, 293, 747, 411, 6091, 382, 938, 382], "temperature": 0.0, "avg_logprob": -0.2138983836540809, "compression_ratio": 1.7745098039215685, "no_speech_prob": 1.034847855407861e-06}, {"id": 1349, "seek": 658400, "start": 6599.4, "end": 6604.24, "text": " Resnet 34 it can take like two to four times as much memory as", "tokens": [5015, 7129, 12790, 309, 393, 747, 411, 732, 281, 1451, 1413, 382, 709, 4675, 382], "temperature": 0.0, "avg_logprob": -0.2138983836540809, "compression_ratio": 1.7745098039215685, "no_speech_prob": 1.034847855407861e-06}, {"id": 1350, "seek": 658400, "start": 6604.76, "end": 6606.6, "text": " Resnet 34", "tokens": [5015, 7129, 12790], "temperature": 0.0, "avg_logprob": -0.2138983836540809, "compression_ratio": 1.7745098039215685, "no_speech_prob": 1.034847855407861e-06}, {"id": 1351, "seek": 658400, "start": 6606.6, "end": 6611.08, "text": " so what I wanted to do was I wanted to rerun that previous notebook with resnext and", "tokens": [370, 437, 286, 1415, 281, 360, 390, 286, 1415, 281, 43819, 409, 300, 3894, 21060, 365, 725, 716, 734, 293], "temperature": 0.0, "avg_logprob": -0.2138983836540809, "compression_ratio": 1.7745098039215685, "no_speech_prob": 1.034847855407861e-06}, {"id": 1352, "seek": 661108, "start": 6611.08, "end": 6614.72, "text": " Increasing the image size to turn on you know so here", "tokens": [30367, 3349, 264, 3256, 2744, 281, 1261, 322, 291, 458, 370, 510], "temperature": 0.0, "avg_logprob": -0.21468286346970944, "compression_ratio": 1.586466165413534, "no_speech_prob": 5.594292360910913e-06}, {"id": 1353, "seek": 661108, "start": 6614.72, "end": 6621.5599999999995, "text": " I just said architecture equals resnext 50 size equals to 99 and then I found that I had to take the batch size all", "tokens": [286, 445, 848, 9482, 6915, 725, 716, 734, 2625, 2744, 6915, 281, 11803, 293, 550, 286, 1352, 300, 286, 632, 281, 747, 264, 15245, 2744, 439], "temperature": 0.0, "avg_logprob": -0.21468286346970944, "compression_ratio": 1.586466165413534, "no_speech_prob": 5.594292360910913e-06}, {"id": 1354, "seek": 661108, "start": 6621.5599999999995, "end": 6627.32, "text": " The way back to 28 to get it to fit my GPU is 11 gig if you're using AWS or Cressul", "tokens": [440, 636, 646, 281, 7562, 281, 483, 309, 281, 3318, 452, 18407, 307, 2975, 8741, 498, 291, 434, 1228, 17650, 420, 383, 735, 425], "temperature": 0.0, "avg_logprob": -0.21468286346970944, "compression_ratio": 1.586466165413534, "no_speech_prob": 5.594292360910913e-06}, {"id": 1355, "seek": 661108, "start": 6627.32, "end": 6628.84, "text": " I think they're like", "tokens": [286, 519, 436, 434, 411], "temperature": 0.0, "avg_logprob": -0.21468286346970944, "compression_ratio": 1.586466165413534, "no_speech_prob": 5.594292360910913e-06}, {"id": 1356, "seek": 661108, "start": 6628.84, "end": 6631.18, "text": " 12 gigs they might be able to make this bit higher", "tokens": [2272, 34586, 436, 1062, 312, 1075, 281, 652, 341, 857, 2946], "temperature": 0.0, "avg_logprob": -0.21468286346970944, "compression_ratio": 1.586466165413534, "no_speech_prob": 5.594292360910913e-06}, {"id": 1357, "seek": 661108, "start": 6631.64, "end": 6636.4, "text": " But this is what I found I had to do so then I this is literally a copy of the previous notebook", "tokens": [583, 341, 307, 437, 286, 1352, 286, 632, 281, 360, 370, 550, 286, 341, 307, 3736, 257, 5055, 295, 264, 3894, 21060], "temperature": 0.0, "avg_logprob": -0.21468286346970944, "compression_ratio": 1.586466165413534, "no_speech_prob": 5.594292360910913e-06}, {"id": 1358, "seek": 663640, "start": 6636.4, "end": 6641.839999999999, "text": " So you can actually go file make a copy right and then rerun it with with these different parameters", "tokens": [407, 291, 393, 767, 352, 3991, 652, 257, 5055, 558, 293, 550, 43819, 409, 309, 365, 365, 613, 819, 9834], "temperature": 0.0, "avg_logprob": -0.2075498426282728, "compression_ratio": 1.6594202898550725, "no_speech_prob": 1.8448181435815059e-06}, {"id": 1359, "seek": 663640, "start": 6641.839999999999, "end": 6649.0199999999995, "text": " And so I deleted some of the pros and some of the exploratory stuff to see you know basically", "tokens": [400, 370, 286, 22981, 512, 295, 264, 6267, 293, 512, 295, 264, 24765, 4745, 1507, 281, 536, 291, 458, 1936], "temperature": 0.0, "avg_logprob": -0.2075498426282728, "compression_ratio": 1.6594202898550725, "no_speech_prob": 1.8448181435815059e-06}, {"id": 1360, "seek": 663640, "start": 6649.0199999999995, "end": 6653.24, "text": " I said everything else is the same all the same steps as before", "tokens": [286, 848, 1203, 1646, 307, 264, 912, 439, 264, 912, 4439, 382, 949], "temperature": 0.0, "avg_logprob": -0.2075498426282728, "compression_ratio": 1.6594202898550725, "no_speech_prob": 1.8448181435815059e-06}, {"id": 1361, "seek": 663640, "start": 6653.24, "end": 6658.0, "text": " There's my in fact you can kind of see what this minimum set of steps looks like I didn't need to worry about learning rate", "tokens": [821, 311, 452, 294, 1186, 291, 393, 733, 295, 536, 437, 341, 7285, 992, 295, 4439, 1542, 411, 286, 994, 380, 643, 281, 3292, 466, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.2075498426282728, "compression_ratio": 1.6594202898550725, "no_speech_prob": 1.8448181435815059e-06}, {"id": 1362, "seek": 663640, "start": 6658.0, "end": 6660.0, "text": " Find us, so I just left it as is so", "tokens": [11809, 505, 11, 370, 286, 445, 1411, 309, 382, 307, 370], "temperature": 0.0, "avg_logprob": -0.2075498426282728, "compression_ratio": 1.6594202898550725, "no_speech_prob": 1.8448181435815059e-06}, {"id": 1363, "seek": 663640, "start": 6660.36, "end": 6663.639999999999, "text": " transforms data equals learn equals fit", "tokens": [35592, 1412, 6915, 1466, 6915, 3318], "temperature": 0.0, "avg_logprob": -0.2075498426282728, "compression_ratio": 1.6594202898550725, "no_speech_prob": 1.8448181435815059e-06}, {"id": 1364, "seek": 666364, "start": 6663.64, "end": 6668.6, "text": " Precomputed equals false fit with cycling tickles one unfreeze", "tokens": [6001, 1112, 2582, 292, 6915, 7908, 3318, 365, 22425, 5204, 904, 472, 3971, 701, 1381], "temperature": 0.0, "avg_logprob": -0.2188650886967497, "compression_ratio": 1.6716981132075472, "no_speech_prob": 1.1125517630716786e-05}, {"id": 1365, "seek": 666364, "start": 6669.4400000000005, "end": 6671.320000000001, "text": " differential learning rates", "tokens": [15756, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.2188650886967497, "compression_ratio": 1.6716981132075472, "no_speech_prob": 1.1125517630716786e-05}, {"id": 1366, "seek": 666364, "start": 6671.320000000001, "end": 6673.76, "text": " Fit some more and you can see here", "tokens": [29263, 512, 544, 293, 291, 393, 536, 510], "temperature": 0.0, "avg_logprob": -0.2188650886967497, "compression_ratio": 1.6716981132075472, "no_speech_prob": 1.1125517630716786e-05}, {"id": 1367, "seek": 666364, "start": 6673.76, "end": 6679.52, "text": " I didn't do the cycle malt thing because I found like now that I'm using a bigger architecture", "tokens": [286, 994, 380, 360, 264, 6586, 45654, 551, 570, 286, 1352, 411, 586, 300, 286, 478, 1228, 257, 3801, 9482], "temperature": 0.0, "avg_logprob": -0.2188650886967497, "compression_ratio": 1.6716981132075472, "no_speech_prob": 1.1125517630716786e-05}, {"id": 1368, "seek": 666364, "start": 6679.52, "end": 6683.12, "text": " It's got more parameters. It was over fitting pretty quickly", "tokens": [467, 311, 658, 544, 9834, 13, 467, 390, 670, 15669, 1238, 2661], "temperature": 0.0, "avg_logprob": -0.2188650886967497, "compression_ratio": 1.6716981132075472, "no_speech_prob": 1.1125517630716786e-05}, {"id": 1369, "seek": 666364, "start": 6683.240000000001, "end": 6686.780000000001, "text": " So rather than like cycle length equals one never finding the right spot", "tokens": [407, 2831, 813, 411, 6586, 4641, 6915, 472, 1128, 5006, 264, 558, 4008], "temperature": 0.0, "avg_logprob": -0.2188650886967497, "compression_ratio": 1.6716981132075472, "no_speech_prob": 1.1125517630716786e-05}, {"id": 1370, "seek": 668678, "start": 6686.78, "end": 6693.94, "text": " It actually did find the right spot, and if I used longer cycle lengths. I found that my validation", "tokens": [467, 767, 630, 915, 264, 558, 4008, 11, 293, 498, 286, 1143, 2854, 6586, 26329, 13, 286, 1352, 300, 452, 24071], "temperature": 0.0, "avg_logprob": -0.27040634155273435, "compression_ratio": 1.5361702127659576, "no_speech_prob": 2.7264586606179364e-06}, {"id": 1371, "seek": 668678, "start": 6694.46, "end": 6698.099999999999, "text": " Error was higher than my training error. It was over fitting", "tokens": [3300, 2874, 390, 2946, 813, 452, 3097, 6713, 13, 467, 390, 670, 15669], "temperature": 0.0, "avg_logprob": -0.27040634155273435, "compression_ratio": 1.5361702127659576, "no_speech_prob": 2.7264586606179364e-06}, {"id": 1372, "seek": 668678, "start": 6699.46, "end": 6702.139999999999, "text": " So check this out though by using these you know", "tokens": [407, 1520, 341, 484, 1673, 538, 1228, 613, 291, 458], "temperature": 0.0, "avg_logprob": -0.27040634155273435, "compression_ratio": 1.5361702127659576, "no_speech_prob": 2.7264586606179364e-06}, {"id": 1373, "seek": 668678, "start": 6702.82, "end": 6707.2, "text": " Three steps I got plus TTA 99.75", "tokens": [6244, 4439, 286, 658, 1804, 314, 8241, 11803, 13, 11901], "temperature": 0.0, "avg_logprob": -0.27040634155273435, "compression_ratio": 1.5361702127659576, "no_speech_prob": 2.7264586606179364e-06}, {"id": 1374, "seek": 668678, "start": 6707.9, "end": 6715.719999999999, "text": " So what does that mean that means I have one incorrect dog for incorrect cats and when we look at the pictures of them", "tokens": [407, 437, 775, 300, 914, 300, 1355, 286, 362, 472, 18424, 3000, 337, 18424, 11111, 293, 562, 321, 574, 412, 264, 5242, 295, 552], "temperature": 0.0, "avg_logprob": -0.27040634155273435, "compression_ratio": 1.5361702127659576, "no_speech_prob": 2.7264586606179364e-06}, {"id": 1375, "seek": 671572, "start": 6715.72, "end": 6717.72, "text": " my", "tokens": [452], "temperature": 0.0, "avg_logprob": -0.22585708216616981, "compression_ratio": 1.5705521472392638, "no_speech_prob": 1.8448185983288568e-06}, {"id": 1376, "seek": 671572, "start": 6718.92, "end": 6724.88, "text": " Incorrect dog has a cat in it this one is not a either this one is not either, so I've actually got one", "tokens": [39120, 2554, 3000, 575, 257, 3857, 294, 309, 341, 472, 307, 406, 257, 2139, 341, 472, 307, 406, 2139, 11, 370, 286, 600, 767, 658, 472], "temperature": 0.0, "avg_logprob": -0.22585708216616981, "compression_ratio": 1.5705521472392638, "no_speech_prob": 1.8448185983288568e-06}, {"id": 1377, "seek": 671572, "start": 6725.76, "end": 6728.92, "text": " mistake and then my incorrect dog is", "tokens": [6146, 293, 550, 452, 18424, 3000, 307], "temperature": 0.0, "avg_logprob": -0.22585708216616981, "compression_ratio": 1.5705521472392638, "no_speech_prob": 1.8448185983288568e-06}, {"id": 1378, "seek": 671572, "start": 6729.96, "end": 6731.2, "text": " teeth", "tokens": [7798], "temperature": 0.0, "avg_logprob": -0.22585708216616981, "compression_ratio": 1.5705521472392638, "no_speech_prob": 1.8448185983288568e-06}, {"id": 1379, "seek": 671572, "start": 6731.2, "end": 6733.2, "text": " Right so like we're at a point", "tokens": [1779, 370, 411, 321, 434, 412, 257, 935], "temperature": 0.0, "avg_logprob": -0.22585708216616981, "compression_ratio": 1.5705521472392638, "no_speech_prob": 1.8448185983288568e-06}, {"id": 1380, "seek": 671572, "start": 6733.96, "end": 6739.320000000001, "text": " Where we're now able to train a classifier. That's so good that it has like", "tokens": [2305, 321, 434, 586, 1075, 281, 3847, 257, 1508, 9902, 13, 663, 311, 370, 665, 300, 309, 575, 411], "temperature": 0.0, "avg_logprob": -0.22585708216616981, "compression_ratio": 1.5705521472392638, "no_speech_prob": 1.8448185983288568e-06}, {"id": 1381, "seek": 673932, "start": 6739.32, "end": 6747.42, "text": " basically one mistake right and so when people say like we have superhuman image performance now", "tokens": [1936, 472, 6146, 558, 293, 370, 562, 561, 584, 411, 321, 362, 1687, 18796, 3256, 3389, 586], "temperature": 0.0, "avg_logprob": -0.15619046637352477, "compression_ratio": 1.6794871794871795, "no_speech_prob": 3.446563823672477e-06}, {"id": 1382, "seek": 673932, "start": 6747.42, "end": 6752.92, "text": " This is kind of what they're talking about right so if you're actually when I looked at the dog breed one", "tokens": [639, 307, 733, 295, 437, 436, 434, 1417, 466, 558, 370, 498, 291, 434, 767, 562, 286, 2956, 412, 264, 3000, 18971, 472], "temperature": 0.0, "avg_logprob": -0.15619046637352477, "compression_ratio": 1.6794871794871795, "no_speech_prob": 3.446563823672477e-06}, {"id": 1383, "seek": 673932, "start": 6752.92, "end": 6757.74, "text": " I did this morning. I was like it was it was getting the dog breeds much better than I ever could", "tokens": [286, 630, 341, 2446, 13, 286, 390, 411, 309, 390, 309, 390, 1242, 264, 3000, 41609, 709, 1101, 813, 286, 1562, 727], "temperature": 0.0, "avg_logprob": -0.15619046637352477, "compression_ratio": 1.6794871794871795, "no_speech_prob": 3.446563823672477e-06}, {"id": 1384, "seek": 673932, "start": 6759.88, "end": 6765.219999999999, "text": " So like it's this is what we can get to if you use a really modern architecture like resnext", "tokens": [407, 411, 309, 311, 341, 307, 437, 321, 393, 483, 281, 498, 291, 764, 257, 534, 4363, 9482, 411, 725, 716, 734], "temperature": 0.0, "avg_logprob": -0.15619046637352477, "compression_ratio": 1.6794871794871795, "no_speech_prob": 3.446563823672477e-06}, {"id": 1385, "seek": 676522, "start": 6765.22, "end": 6768.820000000001, "text": " And this only took I don't know like", "tokens": [400, 341, 787, 1890, 286, 500, 380, 458, 411], "temperature": 0.0, "avg_logprob": -0.31077954985878686, "compression_ratio": 1.5533980582524272, "no_speech_prob": 1.370952304569073e-06}, {"id": 1386, "seek": 676522, "start": 6769.900000000001, "end": 6772.42, "text": " Remember until like 20 minutes to train", "tokens": [5459, 1826, 411, 945, 2077, 281, 3847], "temperature": 0.0, "avg_logprob": -0.31077954985878686, "compression_ratio": 1.5533980582524272, "no_speech_prob": 1.370952304569073e-06}, {"id": 1387, "seek": 676522, "start": 6774.5, "end": 6776.5, "text": " So that's kind of where we're up to", "tokens": [407, 300, 311, 733, 295, 689, 321, 434, 493, 281], "temperature": 0.0, "avg_logprob": -0.31077954985878686, "compression_ratio": 1.5533980582524272, "no_speech_prob": 1.370952304569073e-06}, {"id": 1388, "seek": 676522, "start": 6778.66, "end": 6780.66, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.31077954985878686, "compression_ratio": 1.5533980582524272, "no_speech_prob": 1.370952304569073e-06}, {"id": 1389, "seek": 676522, "start": 6781.14, "end": 6783.14, "text": " If you wanted to do", "tokens": [759, 291, 1415, 281, 360], "temperature": 0.0, "avg_logprob": -0.31077954985878686, "compression_ratio": 1.5533980582524272, "no_speech_prob": 1.370952304569073e-06}, {"id": 1390, "seek": 676522, "start": 6783.18, "end": 6785.18, "text": " satellite imagery instead", "tokens": [16016, 24340, 2602], "temperature": 0.0, "avg_logprob": -0.31077954985878686, "compression_ratio": 1.5533980582524272, "no_speech_prob": 1.370952304569073e-06}, {"id": 1391, "seek": 676522, "start": 6785.46, "end": 6792.42, "text": " Right then it's the same thing and in fact the the planet satellite data sets already on Cresall if you're using Cresall you can", "tokens": [1779, 550, 309, 311, 264, 912, 551, 293, 294, 1186, 264, 264, 5054, 16016, 1412, 6352, 1217, 322, 383, 495, 336, 498, 291, 434, 1228, 383, 495, 336, 291, 393], "temperature": 0.0, "avg_logprob": -0.31077954985878686, "compression_ratio": 1.5533980582524272, "no_speech_prob": 1.370952304569073e-06}, {"id": 1392, "seek": 676522, "start": 6792.42, "end": 6794.42, "text": " jump straight there right and", "tokens": [3012, 2997, 456, 558, 293], "temperature": 0.0, "avg_logprob": -0.31077954985878686, "compression_ratio": 1.5533980582524272, "no_speech_prob": 1.370952304569073e-06}, {"id": 1393, "seek": 679442, "start": 6794.42, "end": 6796.42, "text": " I", "tokens": [286], "temperature": 0.0, "avg_logprob": -0.20412984387627964, "compression_ratio": 1.5972222222222223, "no_speech_prob": 6.339158062473871e-06}, {"id": 1394, "seek": 679442, "start": 6796.66, "end": 6801.74, "text": " Just linked it into data slash planet, and I can do exactly the same thing right I can", "tokens": [1449, 9408, 309, 666, 1412, 17330, 5054, 11, 293, 286, 393, 360, 2293, 264, 912, 551, 558, 286, 393], "temperature": 0.0, "avg_logprob": -0.20412984387627964, "compression_ratio": 1.5972222222222223, "no_speech_prob": 6.339158062473871e-06}, {"id": 1395, "seek": 679442, "start": 6804.9800000000005, "end": 6806.9800000000005, "text": " Image classifier from CSV", "tokens": [29903, 1508, 9902, 490, 48814], "temperature": 0.0, "avg_logprob": -0.20412984387627964, "compression_ratio": 1.5972222222222223, "no_speech_prob": 6.339158062473871e-06}, {"id": 1396, "seek": 679442, "start": 6807.46, "end": 6811.7, "text": " Right and you can see these three lines are actually exactly the same as my dog breed lines", "tokens": [1779, 293, 291, 393, 536, 613, 1045, 3876, 366, 767, 2293, 264, 912, 382, 452, 3000, 18971, 3876], "temperature": 0.0, "avg_logprob": -0.20412984387627964, "compression_ratio": 1.5972222222222223, "no_speech_prob": 6.339158062473871e-06}, {"id": 1397, "seek": 679442, "start": 6811.7, "end": 6818.86, "text": " You know how big how many lines are in the file grab my validation indexes this get data as you can see it's identical", "tokens": [509, 458, 577, 955, 577, 867, 3876, 366, 294, 264, 3991, 4444, 452, 24071, 8186, 279, 341, 483, 1412, 382, 291, 393, 536, 309, 311, 14800], "temperature": 0.0, "avg_logprob": -0.20412984387627964, "compression_ratio": 1.5972222222222223, "no_speech_prob": 6.339158062473871e-06}, {"id": 1398, "seek": 679442, "start": 6818.86, "end": 6820.86, "text": " Except I've changed", "tokens": [16192, 286, 600, 3105], "temperature": 0.0, "avg_logprob": -0.20412984387627964, "compression_ratio": 1.5972222222222223, "no_speech_prob": 6.339158062473871e-06}, {"id": 1399, "seek": 682086, "start": 6820.86, "end": 6825.42, "text": " Side on to top down the satellite images about top down so I can fit them", "tokens": [19026, 322, 281, 1192, 760, 264, 16016, 5267, 466, 1192, 760, 370, 286, 393, 3318, 552], "temperature": 0.0, "avg_logprob": -0.22555377340724325, "compression_ratio": 1.7846715328467153, "no_speech_prob": 3.90546074413578e-06}, {"id": 1400, "seek": 682086, "start": 6826.0599999999995, "end": 6832.78, "text": " Vertically and they still make sense right and so you can see here. I'm doing this trick run back to size equals 64 and", "tokens": [21044, 984, 293, 436, 920, 652, 2020, 558, 293, 370, 291, 393, 536, 510, 13, 286, 478, 884, 341, 4282, 1190, 646, 281, 2744, 6915, 12145, 293], "temperature": 0.0, "avg_logprob": -0.22555377340724325, "compression_ratio": 1.7846715328467153, "no_speech_prob": 3.90546074413578e-06}, {"id": 1401, "seek": 682086, "start": 6833.86, "end": 6841.38, "text": " Train a little bit first learning rate finder right and interestingly in this case you can see it. I want really high learning rates", "tokens": [28029, 257, 707, 857, 700, 2539, 3314, 915, 260, 558, 293, 25873, 294, 341, 1389, 291, 393, 536, 309, 13, 286, 528, 534, 1090, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.22555377340724325, "compression_ratio": 1.7846715328467153, "no_speech_prob": 3.90546074413578e-06}, {"id": 1402, "seek": 682086, "start": 6841.38, "end": 6848.219999999999, "text": " I don't know what it is about this particular data set. This is true, but it's clearly I can use super high learning rates", "tokens": [286, 500, 380, 458, 437, 309, 307, 466, 341, 1729, 1412, 992, 13, 639, 307, 2074, 11, 457, 309, 311, 4448, 286, 393, 764, 1687, 1090, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.22555377340724325, "compression_ratio": 1.7846715328467153, "no_speech_prob": 3.90546074413578e-06}, {"id": 1403, "seek": 684822, "start": 6848.22, "end": 6852.9800000000005, "text": " So I used a learning rate of point two and so I've trained for a while", "tokens": [407, 286, 1143, 257, 2539, 3314, 295, 935, 732, 293, 370, 286, 600, 8895, 337, 257, 1339], "temperature": 0.0, "avg_logprob": -0.2909289256180867, "compression_ratio": 1.6611570247933884, "no_speech_prob": 5.0147164074587636e-06}, {"id": 1404, "seek": 684822, "start": 6853.780000000001, "end": 6855.18, "text": " differential learning rates", "tokens": [15756, 2539, 6846], "temperature": 0.0, "avg_logprob": -0.2909289256180867, "compression_ratio": 1.6611570247933884, "no_speech_prob": 5.0147164074587636e-06}, {"id": 1405, "seek": 684822, "start": 6855.18, "end": 6857.740000000001, "text": " right and so remember I said like if the", "tokens": [558, 293, 370, 1604, 286, 848, 411, 498, 264], "temperature": 0.0, "avg_logprob": -0.2909289256180867, "compression_ratio": 1.6611570247933884, "no_speech_prob": 5.0147164074587636e-06}, {"id": 1406, "seek": 684822, "start": 6858.42, "end": 6863.72, "text": " Datasets very different to image net I probably want to train those middle layers a lot more", "tokens": [9315, 296, 1385, 588, 819, 281, 3256, 2533, 286, 1391, 528, 281, 3847, 729, 2808, 7914, 257, 688, 544], "temperature": 0.0, "avg_logprob": -0.2909289256180867, "compression_ratio": 1.6611570247933884, "no_speech_prob": 5.0147164074587636e-06}, {"id": 1407, "seek": 684822, "start": 6863.72, "end": 6869.5, "text": " So I'm using divided by three rather than divided by ten right doesn't that is the same thing cycle molecules two?", "tokens": [407, 286, 478, 1228, 6666, 538, 1045, 2831, 813, 6666, 538, 2064, 558, 1177, 380, 300, 307, 264, 912, 551, 6586, 13093, 732, 30], "temperature": 0.0, "avg_logprob": -0.2909289256180867, "compression_ratio": 1.6611570247933884, "no_speech_prob": 5.0147164074587636e-06}, {"id": 1408, "seek": 684822, "start": 6870.18, "end": 6872.18, "text": " right", "tokens": [558], "temperature": 0.0, "avg_logprob": -0.2909289256180867, "compression_ratio": 1.6611570247933884, "no_speech_prob": 5.0147164074587636e-06}, {"id": 1409, "seek": 684822, "start": 6872.26, "end": 6874.3, "text": " And then I was just kind of keeping an eye on it", "tokens": [400, 550, 286, 390, 445, 733, 295, 5145, 364, 3313, 322, 309], "temperature": 0.0, "avg_logprob": -0.2909289256180867, "compression_ratio": 1.6611570247933884, "no_speech_prob": 5.0147164074587636e-06}, {"id": 1410, "seek": 687430, "start": 6874.3, "end": 6880.1, "text": " So you can actually plot the loss if you go learn dot shed dot plot loss you can see here the here's the first cycle", "tokens": [407, 291, 393, 767, 7542, 264, 4470, 498, 291, 352, 1466, 5893, 14951, 5893, 7542, 4470, 291, 393, 536, 510, 264, 510, 311, 264, 700, 6586], "temperature": 0.0, "avg_logprob": -0.21765450450861565, "compression_ratio": 1.9241071428571428, "no_speech_prob": 7.071847448969493e-06}, {"id": 1411, "seek": 687430, "start": 6880.42, "end": 6886.3, "text": " Is the second cycle is the third cycle right so you can see it's like it's better pops out gets better pops out", "tokens": [1119, 264, 1150, 6586, 307, 264, 2636, 6586, 558, 370, 291, 393, 536, 309, 311, 411, 309, 311, 1101, 16795, 484, 2170, 1101, 16795, 484], "temperature": 0.0, "avg_logprob": -0.21765450450861565, "compression_ratio": 1.9241071428571428, "no_speech_prob": 7.071847448969493e-06}, {"id": 1412, "seek": 687430, "start": 6886.3, "end": 6889.22, "text": " It better pops out and each time it finds something better than the last time", "tokens": [467, 1101, 16795, 484, 293, 1184, 565, 309, 10704, 746, 1101, 813, 264, 1036, 565], "temperature": 0.0, "avg_logprob": -0.21765450450861565, "compression_ratio": 1.9241071428571428, "no_speech_prob": 7.071847448969493e-06}, {"id": 1413, "seek": 687430, "start": 6890.66, "end": 6895.38, "text": " Then set the size up to 128 and just repeat exactly the last few steps and", "tokens": [1396, 992, 264, 2744, 493, 281, 29810, 293, 445, 7149, 2293, 264, 1036, 1326, 4439, 293], "temperature": 0.0, "avg_logprob": -0.21765450450861565, "compression_ratio": 1.9241071428571428, "no_speech_prob": 7.071847448969493e-06}, {"id": 1414, "seek": 687430, "start": 6896.22, "end": 6898.22, "text": " then set up 256 and", "tokens": [550, 992, 493, 38882, 293], "temperature": 0.0, "avg_logprob": -0.21765450450861565, "compression_ratio": 1.9241071428571428, "no_speech_prob": 7.071847448969493e-06}, {"id": 1415, "seek": 687430, "start": 6899.02, "end": 6901.02, "text": " repeat the last two steps and", "tokens": [7149, 264, 1036, 732, 4439, 293], "temperature": 0.0, "avg_logprob": -0.21765450450861565, "compression_ratio": 1.9241071428571428, "no_speech_prob": 7.071847448969493e-06}, {"id": 1416, "seek": 690102, "start": 6901.02, "end": 6908.780000000001, "text": " Then do TTA and if you submit this then this gets about 30th place in this competition", "tokens": [1396, 360, 314, 8241, 293, 498, 291, 10315, 341, 550, 341, 2170, 466, 2217, 392, 1081, 294, 341, 6211], "temperature": 0.0, "avg_logprob": -0.17731392249632416, "compression_ratio": 1.5799086757990868, "no_speech_prob": 5.9551052800088655e-06}, {"id": 1417, "seek": 690102, "start": 6909.820000000001, "end": 6915.92, "text": " So these basic steps work super well this this thing where I went all the way back to a size of", "tokens": [407, 613, 3875, 4439, 589, 1687, 731, 341, 341, 551, 689, 286, 1437, 439, 264, 636, 646, 281, 257, 2744, 295], "temperature": 0.0, "avg_logprob": -0.17731392249632416, "compression_ratio": 1.5799086757990868, "no_speech_prob": 5.9551052800088655e-06}, {"id": 1418, "seek": 690102, "start": 6916.540000000001, "end": 6918.02, "text": " 64 I", "tokens": [12145, 286], "temperature": 0.0, "avg_logprob": -0.17731392249632416, "compression_ratio": 1.5799086757990868, "no_speech_prob": 5.9551052800088655e-06}, {"id": 1419, "seek": 690102, "start": 6918.02, "end": 6923.620000000001, "text": " Wouldn't do that if I was doing like dogs and cats or dog breeds because like this is so small", "tokens": [26291, 380, 360, 300, 498, 286, 390, 884, 411, 7197, 293, 11111, 420, 3000, 41609, 570, 411, 341, 307, 370, 1359], "temperature": 0.0, "avg_logprob": -0.17731392249632416, "compression_ratio": 1.5799086757990868, "no_speech_prob": 5.9551052800088655e-06}, {"id": 1420, "seek": 690102, "start": 6923.900000000001, "end": 6928.18, "text": " That if the thing I was working on is very similar to image net", "tokens": [663, 498, 264, 551, 286, 390, 1364, 322, 307, 588, 2531, 281, 3256, 2533], "temperature": 0.0, "avg_logprob": -0.17731392249632416, "compression_ratio": 1.5799086757990868, "no_speech_prob": 5.9551052800088655e-06}, {"id": 1421, "seek": 692818, "start": 6928.18, "end": 6933.62, "text": " I would kind of destroy those image net weights like 64 by 64 is so small", "tokens": [286, 576, 733, 295, 5293, 729, 3256, 2533, 17443, 411, 12145, 538, 12145, 307, 370, 1359], "temperature": 0.0, "avg_logprob": -0.2119673602985886, "compression_ratio": 1.6762589928057554, "no_speech_prob": 1.0451441085024271e-05}, {"id": 1422, "seek": 692818, "start": 6933.900000000001, "end": 6937.26, "text": " But in this case the satellite imagery data is so different to image net", "tokens": [583, 294, 341, 1389, 264, 16016, 24340, 1412, 307, 370, 819, 281, 3256, 2533], "temperature": 0.0, "avg_logprob": -0.2119673602985886, "compression_ratio": 1.6762589928057554, "no_speech_prob": 1.0451441085024271e-05}, {"id": 1423, "seek": 692818, "start": 6937.26, "end": 6942.12, "text": " You know I really found that it worked pretty well to start right back at these tiny images", "tokens": [509, 458, 286, 534, 1352, 300, 309, 2732, 1238, 731, 281, 722, 558, 646, 412, 613, 5870, 5267], "temperature": 0.0, "avg_logprob": -0.2119673602985886, "compression_ratio": 1.6762589928057554, "no_speech_prob": 1.0451441085024271e-05}, {"id": 1424, "seek": 692818, "start": 6942.820000000001, "end": 6944.820000000001, "text": " It really helped me to avoid overfitting", "tokens": [467, 534, 4254, 385, 281, 5042, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.2119673602985886, "compression_ratio": 1.6762589928057554, "no_speech_prob": 1.0451441085024271e-05}, {"id": 1425, "seek": 692818, "start": 6945.860000000001, "end": 6952.38, "text": " And interestingly using this kind of approach I actually found that even with using only 128 by 128", "tokens": [400, 25873, 1228, 341, 733, 295, 3109, 286, 767, 1352, 300, 754, 365, 1228, 787, 29810, 538, 29810], "temperature": 0.0, "avg_logprob": -0.2119673602985886, "compression_ratio": 1.6762589928057554, "no_speech_prob": 1.0451441085024271e-05}, {"id": 1426, "seek": 692818, "start": 6952.38, "end": 6957.360000000001, "text": " I was getting like much better cackle results than nearly everybody on the leaderboard", "tokens": [286, 390, 1242, 411, 709, 1101, 269, 501, 306, 3542, 813, 6217, 2201, 322, 264, 5263, 3787], "temperature": 0.0, "avg_logprob": -0.2119673602985886, "compression_ratio": 1.6762589928057554, "no_speech_prob": 1.0451441085024271e-05}, {"id": 1427, "seek": 695736, "start": 6957.36, "end": 6963.259999999999, "text": " And when I say 30th place, this is a very recent competition right and so I find like in", "tokens": [400, 562, 286, 584, 2217, 392, 1081, 11, 341, 307, 257, 588, 5162, 6211, 558, 293, 370, 286, 915, 411, 294], "temperature": 0.0, "avg_logprob": -0.20514592757591835, "compression_ratio": 1.6770428015564203, "no_speech_prob": 5.338110895536374e-06}, {"id": 1428, "seek": 695736, "start": 6963.88, "end": 6967.36, "text": " The last year like a lot of people have got a lot better at computer vision", "tokens": [440, 1036, 1064, 411, 257, 688, 295, 561, 362, 658, 257, 688, 1101, 412, 3820, 5201], "temperature": 0.0, "avg_logprob": -0.20514592757591835, "compression_ratio": 1.6770428015564203, "no_speech_prob": 5.338110895536374e-06}, {"id": 1429, "seek": 695736, "start": 6967.36, "end": 6972.08, "text": " And so the people in the top 50 in this competition were generally on assembling dozens of models", "tokens": [400, 370, 264, 561, 294, 264, 1192, 2625, 294, 341, 6211, 645, 5101, 322, 43867, 18431, 295, 5245], "temperature": 0.0, "avg_logprob": -0.20514592757591835, "compression_ratio": 1.6770428015564203, "no_speech_prob": 5.338110895536374e-06}, {"id": 1430, "seek": 695736, "start": 6972.48, "end": 6974.48, "text": " Lots of people on a team", "tokens": [15908, 295, 561, 322, 257, 1469], "temperature": 0.0, "avg_logprob": -0.20514592757591835, "compression_ratio": 1.6770428015564203, "no_speech_prob": 5.338110895536374e-06}, {"id": 1431, "seek": 695736, "start": 6974.5199999999995, "end": 6980.12, "text": " Lots of pre-processing specific satellite data and so forth so like to be able to get 30th", "tokens": [15908, 295, 659, 12, 41075, 278, 2685, 16016, 1412, 293, 370, 5220, 370, 411, 281, 312, 1075, 281, 483, 2217, 392], "temperature": 0.0, "avg_logprob": -0.20514592757591835, "compression_ratio": 1.6770428015564203, "no_speech_prob": 5.338110895536374e-06}, {"id": 1432, "seek": 695736, "start": 6980.639999999999, "end": 6982.96, "text": " Using this totally standard technique is pretty cool", "tokens": [11142, 341, 3879, 3832, 6532, 307, 1238, 1627], "temperature": 0.0, "avg_logprob": -0.20514592757591835, "compression_ratio": 1.6770428015564203, "no_speech_prob": 5.338110895536374e-06}, {"id": 1433, "seek": 698296, "start": 6982.96, "end": 6984.96, "text": " All right", "tokens": [1057, 558], "temperature": 0.0, "avg_logprob": -0.18143857082473897, "compression_ratio": 1.668, "no_speech_prob": 9.818181752052624e-06}, {"id": 1434, "seek": 698296, "start": 6986.8, "end": 6992.24, "text": " So now that we've got to this point right we've got through two lessons if you're still here", "tokens": [407, 586, 300, 321, 600, 658, 281, 341, 935, 558, 321, 600, 658, 807, 732, 8820, 498, 291, 434, 920, 510], "temperature": 0.0, "avg_logprob": -0.18143857082473897, "compression_ratio": 1.668, "no_speech_prob": 9.818181752052624e-06}, {"id": 1435, "seek": 698296, "start": 6992.24, "end": 6995.96, "text": " Then hopefully you're thinking okay. This is actually pretty useful", "tokens": [1396, 4696, 291, 434, 1953, 1392, 13, 639, 307, 767, 1238, 4420], "temperature": 0.0, "avg_logprob": -0.18143857082473897, "compression_ratio": 1.668, "no_speech_prob": 9.818181752052624e-06}, {"id": 1436, "seek": 698296, "start": 6995.96, "end": 7001.76, "text": " I want to do more in which case Cressul might not be where you want to stay", "tokens": [286, 528, 281, 360, 544, 294, 597, 1389, 383, 735, 425, 1062, 406, 312, 689, 291, 528, 281, 1754], "temperature": 0.0, "avg_logprob": -0.18143857082473897, "compression_ratio": 1.668, "no_speech_prob": 9.818181752052624e-06}, {"id": 1437, "seek": 698296, "start": 7002.16, "end": 7005.72, "text": " The issues with Cressul. I mean it's it's it's pretty handy", "tokens": [440, 2663, 365, 383, 735, 425, 13, 286, 914, 309, 311, 309, 311, 309, 311, 1238, 13239], "temperature": 0.0, "avg_logprob": -0.18143857082473897, "compression_ratio": 1.668, "no_speech_prob": 9.818181752052624e-06}, {"id": 1438, "seek": 698296, "start": 7005.72, "end": 7011.32, "text": " It's pretty cheap and something we haven't talked about much is paper space is another great choice by the way", "tokens": [467, 311, 1238, 7084, 293, 746, 321, 2378, 380, 2825, 466, 709, 307, 3035, 1901, 307, 1071, 869, 3922, 538, 264, 636], "temperature": 0.0, "avg_logprob": -0.18143857082473897, "compression_ratio": 1.668, "no_speech_prob": 9.818181752052624e-06}, {"id": 1439, "seek": 701132, "start": 7011.32, "end": 7018.08, "text": " Paper space is shortly going to be releasing Cressul like instant jupyter notebooks unfortunately. They're not ready quite yet", "tokens": [24990, 1901, 307, 13392, 516, 281, 312, 16327, 383, 735, 425, 411, 9836, 361, 1010, 88, 391, 43782, 7015, 13, 814, 434, 406, 1919, 1596, 1939], "temperature": 0.0, "avg_logprob": -0.2003154401425962, "compression_ratio": 1.6431095406360423, "no_speech_prob": 3.446545179031091e-06}, {"id": 1440, "seek": 701132, "start": 7018.5199999999995, "end": 7022.16, "text": " But they do have an ability to basically they have the best price", "tokens": [583, 436, 360, 362, 364, 3485, 281, 1936, 436, 362, 264, 1151, 3218], "temperature": 0.0, "avg_logprob": -0.2003154401425962, "compression_ratio": 1.6431095406360423, "no_speech_prob": 3.446545179031091e-06}, {"id": 1441, "seek": 701132, "start": 7023.2, "end": 7030.08, "text": " Performance relationship right now, and they you can SSH into them and use them so they're also a great choice", "tokens": [25047, 2480, 558, 586, 11, 293, 436, 291, 393, 12238, 39, 666, 552, 293, 764, 552, 370, 436, 434, 611, 257, 869, 3922], "temperature": 0.0, "avg_logprob": -0.2003154401425962, "compression_ratio": 1.6431095406360423, "no_speech_prob": 3.446545179031091e-06}, {"id": 1442, "seek": 701132, "start": 7030.08, "end": 7032.08, "text": " And probably by the time this is a MOOC", "tokens": [400, 1391, 538, 264, 565, 341, 307, 257, 49197, 34], "temperature": 0.0, "avg_logprob": -0.2003154401425962, "compression_ratio": 1.6431095406360423, "no_speech_prob": 3.446545179031091e-06}, {"id": 1443, "seek": 701132, "start": 7032.5599999999995, "end": 7039.0599999999995, "text": " We'll probably have a separate lesson showing you how to set up paper space because they're likely to be the great option", "tokens": [492, 603, 1391, 362, 257, 4994, 6898, 4099, 291, 577, 281, 992, 493, 3035, 1901, 570, 436, 434, 3700, 281, 312, 264, 869, 3614], "temperature": 0.0, "avg_logprob": -0.2003154401425962, "compression_ratio": 1.6431095406360423, "no_speech_prob": 3.446545179031091e-06}, {"id": 1444, "seek": 703906, "start": 7039.06, "end": 7042.660000000001, "text": " But at some point you're probably going to want to look at AWS a", "tokens": [583, 412, 512, 935, 291, 434, 1391, 516, 281, 528, 281, 574, 412, 17650, 257], "temperature": 0.0, "avg_logprob": -0.2096325092101365, "compression_ratio": 1.4821428571428572, "no_speech_prob": 3.966947133449139e-06}, {"id": 1445, "seek": 703906, "start": 7043.46, "end": 7045.14, "text": " couple of reasons why", "tokens": [1916, 295, 4112, 983], "temperature": 0.0, "avg_logprob": -0.2096325092101365, "compression_ratio": 1.4821428571428572, "no_speech_prob": 3.966947133449139e-06}, {"id": 1446, "seek": 703906, "start": 7045.14, "end": 7048.740000000001, "text": " The first is as you all know by now", "tokens": [440, 700, 307, 382, 291, 439, 458, 538, 586], "temperature": 0.0, "avg_logprob": -0.2096325092101365, "compression_ratio": 1.4821428571428572, "no_speech_prob": 3.966947133449139e-06}, {"id": 1447, "seek": 703906, "start": 7049.660000000001, "end": 7051.9800000000005, "text": " Amazon have been kind enough to donate about", "tokens": [6795, 362, 668, 733, 1547, 281, 17751, 466], "temperature": 0.0, "avg_logprob": -0.2096325092101365, "compression_ratio": 1.4821428571428572, "no_speech_prob": 3.966947133449139e-06}, {"id": 1448, "seek": 703906, "start": 7052.860000000001, "end": 7059.580000000001, "text": " $200,000 worth of compute time to this course so I want to say thank you very much to Amazon. We've all been given", "tokens": [1848, 7629, 11, 1360, 3163, 295, 14722, 565, 281, 341, 1164, 370, 286, 528, 281, 584, 1309, 291, 588, 709, 281, 6795, 13, 492, 600, 439, 668, 2212], "temperature": 0.0, "avg_logprob": -0.2096325092101365, "compression_ratio": 1.4821428571428572, "no_speech_prob": 3.966947133449139e-06}, {"id": 1449, "seek": 703906, "start": 7060.14, "end": 7062.14, "text": " Credits everybody who's here, so thanks very much", "tokens": [47560, 1208, 2201, 567, 311, 510, 11, 370, 3231, 588, 709], "temperature": 0.0, "avg_logprob": -0.2096325092101365, "compression_ratio": 1.4821428571428572, "no_speech_prob": 3.966947133449139e-06}, {"id": 1450, "seek": 706214, "start": 7062.14, "end": 7069.740000000001, "text": " AWS so sorry if you're on the MOOC we didn't get it for you, but everybody here is like", "tokens": [17650, 370, 2597, 498, 291, 434, 322, 264, 49197, 34, 321, 994, 380, 483, 309, 337, 291, 11, 457, 2201, 510, 307, 411], "temperature": 0.0, "avg_logprob": -0.2019499448629526, "compression_ratio": 1.7584745762711864, "no_speech_prob": 4.710800112661673e-06}, {"id": 1451, "seek": 706214, "start": 7069.9800000000005, "end": 7071.9800000000005, "text": " AWS credits for everybody so", "tokens": [17650, 16816, 337, 2201, 370], "temperature": 0.0, "avg_logprob": -0.2019499448629526, "compression_ratio": 1.7584745762711864, "no_speech_prob": 4.710800112661673e-06}, {"id": 1452, "seek": 706214, "start": 7072.14, "end": 7077.34, "text": " But you can get even if you're not here in person you can get AWS credits from lots of places", "tokens": [583, 291, 393, 483, 754, 498, 291, 434, 406, 510, 294, 954, 291, 393, 483, 17650, 16816, 490, 3195, 295, 3190], "temperature": 0.0, "avg_logprob": -0.2019499448629526, "compression_ratio": 1.7584745762711864, "no_speech_prob": 4.710800112661673e-06}, {"id": 1453, "seek": 706214, "start": 7077.860000000001, "end": 7083.04, "text": " GitHub has a student pack Google for GitHub student pack. That's like 150 bucks worth of credits", "tokens": [23331, 575, 257, 3107, 2844, 3329, 337, 23331, 3107, 2844, 13, 663, 311, 411, 8451, 11829, 3163, 295, 16816], "temperature": 0.0, "avg_logprob": -0.2019499448629526, "compression_ratio": 1.7584745762711864, "no_speech_prob": 4.710800112661673e-06}, {"id": 1454, "seek": 706214, "start": 7083.740000000001, "end": 7087.22, "text": " AWS educate and get credits these are all for students", "tokens": [17650, 16092, 293, 483, 16816, 613, 366, 439, 337, 1731], "temperature": 0.0, "avg_logprob": -0.2019499448629526, "compression_ratio": 1.7584745762711864, "no_speech_prob": 4.710800112661673e-06}, {"id": 1455, "seek": 706214, "start": 7087.820000000001, "end": 7090.34, "text": " So there's lots of places you can get started on AWS", "tokens": [407, 456, 311, 3195, 295, 3190, 291, 393, 483, 1409, 322, 17650], "temperature": 0.0, "avg_logprob": -0.2019499448629526, "compression_ratio": 1.7584745762711864, "no_speech_prob": 4.710800112661673e-06}, {"id": 1456, "seek": 709034, "start": 7090.34, "end": 7092.34, "text": " pretty much everybody", "tokens": [1238, 709, 2201], "temperature": 0.0, "avg_logprob": -0.18326624234517416, "compression_ratio": 1.5308641975308641, "no_speech_prob": 7.183170509961201e-06}, {"id": 1457, "seek": 709034, "start": 7093.54, "end": 7096.06, "text": " Everybody a lot of the people that you might work with", "tokens": [7646, 257, 688, 295, 264, 561, 300, 291, 1062, 589, 365], "temperature": 0.0, "avg_logprob": -0.18326624234517416, "compression_ratio": 1.5308641975308641, "no_speech_prob": 7.183170509961201e-06}, {"id": 1458, "seek": 709034, "start": 7096.82, "end": 7098.82, "text": " Will be using AWS", "tokens": [3099, 312, 1228, 17650], "temperature": 0.0, "avg_logprob": -0.18326624234517416, "compression_ratio": 1.5308641975308641, "no_speech_prob": 7.183170509961201e-06}, {"id": 1459, "seek": 709034, "start": 7099.14, "end": 7101.14, "text": " Because it's like super flexible", "tokens": [1436, 309, 311, 411, 1687, 11358], "temperature": 0.0, "avg_logprob": -0.18326624234517416, "compression_ratio": 1.5308641975308641, "no_speech_prob": 7.183170509961201e-06}, {"id": 1460, "seek": 709034, "start": 7101.18, "end": 7103.860000000001, "text": " Right now AWS has the fastest", "tokens": [1779, 586, 17650, 575, 264, 14573], "temperature": 0.0, "avg_logprob": -0.18326624234517416, "compression_ratio": 1.5308641975308641, "no_speech_prob": 7.183170509961201e-06}, {"id": 1461, "seek": 709034, "start": 7104.54, "end": 7107.7, "text": " Available GPU so you can get in the cloud their p3s", "tokens": [11667, 32699, 18407, 370, 291, 393, 483, 294, 264, 4588, 641, 280, 18, 82], "temperature": 0.0, "avg_logprob": -0.18326624234517416, "compression_ratio": 1.5308641975308641, "no_speech_prob": 7.183170509961201e-06}, {"id": 1462, "seek": 709034, "start": 7109.46, "end": 7111.42, "text": " They're kind of expensive at three bucks an hour", "tokens": [814, 434, 733, 295, 5124, 412, 1045, 11829, 364, 1773], "temperature": 0.0, "avg_logprob": -0.18326624234517416, "compression_ratio": 1.5308641975308641, "no_speech_prob": 7.183170509961201e-06}, {"id": 1463, "seek": 709034, "start": 7111.42, "end": 7116.18, "text": " But if you've got like a model where you've done all the steps before you're thinking this is looking pretty good", "tokens": [583, 498, 291, 600, 658, 411, 257, 2316, 689, 291, 600, 1096, 439, 264, 4439, 949, 291, 434, 1953, 341, 307, 1237, 1238, 665], "temperature": 0.0, "avg_logprob": -0.18326624234517416, "compression_ratio": 1.5308641975308641, "no_speech_prob": 7.183170509961201e-06}, {"id": 1464, "seek": 711618, "start": 7116.18, "end": 7122.26, "text": " You know for six bucks you could get a p3 for two hours and run at turbo speed right?", "tokens": [509, 458, 337, 2309, 11829, 291, 727, 483, 257, 280, 18, 337, 732, 2496, 293, 1190, 412, 20902, 3073, 558, 30], "temperature": 0.0, "avg_logprob": -0.22953718742438123, "compression_ratio": 1.5851851851851853, "no_speech_prob": 4.710861958301393e-06}, {"id": 1465, "seek": 711618, "start": 7124.22, "end": 7130.4400000000005, "text": " We didn't start with AWS because well a it's like twice as expensive as Cressel for the cheapest GPU", "tokens": [492, 994, 380, 722, 365, 17650, 570, 731, 257, 309, 311, 411, 6091, 382, 5124, 382, 383, 735, 338, 337, 264, 29167, 18407], "temperature": 0.0, "avg_logprob": -0.22953718742438123, "compression_ratio": 1.5851851851851853, "no_speech_prob": 4.710861958301393e-06}, {"id": 1466, "seek": 711618, "start": 7130.9800000000005, "end": 7138.820000000001, "text": " And being it takes some setup right, but I wanted to kind of go through and show you how to get your AWS", "tokens": [400, 885, 309, 2516, 512, 8657, 558, 11, 457, 286, 1415, 281, 733, 295, 352, 807, 293, 855, 291, 577, 281, 483, 428, 17650], "temperature": 0.0, "avg_logprob": -0.22953718742438123, "compression_ratio": 1.5851851851851853, "no_speech_prob": 4.710861958301393e-06}, {"id": 1467, "seek": 711618, "start": 7138.820000000001, "end": 7142.26, "text": " Setup and so we're going to be going slightly over time to do that", "tokens": [8928, 1010, 293, 370, 321, 434, 516, 281, 312, 516, 4748, 670, 565, 281, 360, 300], "temperature": 0.0, "avg_logprob": -0.22953718742438123, "compression_ratio": 1.5851851851851853, "no_speech_prob": 4.710861958301393e-06}, {"id": 1468, "seek": 711618, "start": 7142.26, "end": 7145.62, "text": " But I want to show you very quickly so feel free to go if you have to", "tokens": [583, 286, 528, 281, 855, 291, 588, 2661, 370, 841, 1737, 281, 352, 498, 291, 362, 281], "temperature": 0.0, "avg_logprob": -0.22953718742438123, "compression_ratio": 1.5851851851851853, "no_speech_prob": 4.710861958301393e-06}, {"id": 1469, "seek": 714562, "start": 7145.62, "end": 7151.82, "text": " But I want to show you very quickly how you can get your AWS set up right from scratch so", "tokens": [583, 286, 528, 281, 855, 291, 588, 2661, 577, 291, 393, 483, 428, 17650, 992, 493, 558, 490, 8459, 370], "temperature": 0.0, "avg_logprob": -0.261187215646108, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.6442085325252265e-05}, {"id": 1470, "seek": 714562, "start": 7153.3, "end": 7155.78, "text": " Basically you have to go to a console dot AWS", "tokens": [8537, 291, 362, 281, 352, 281, 257, 11076, 5893, 17650], "temperature": 0.0, "avg_logprob": -0.261187215646108, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.6442085325252265e-05}, {"id": 1471, "seek": 714562, "start": 7155.82, "end": 7162.66, "text": " But Amazon calm and it'll take you to the console right and so you can follow along on the video with this", "tokens": [583, 6795, 7151, 293, 309, 603, 747, 291, 281, 264, 11076, 558, 293, 370, 291, 393, 1524, 2051, 322, 264, 960, 365, 341], "temperature": 0.0, "avg_logprob": -0.261187215646108, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.6442085325252265e-05}, {"id": 1472, "seek": 714562, "start": 7162.66, "end": 7165.42, "text": " I'm going to do it very quickly from here. You have to go to", "tokens": [286, 478, 516, 281, 360, 309, 588, 2661, 490, 510, 13, 509, 362, 281, 352, 281], "temperature": 0.0, "avg_logprob": -0.261187215646108, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.6442085325252265e-05}, {"id": 1473, "seek": 714562, "start": 7166.3, "end": 7171.22, "text": " EC2 this is where you set up your instances and so from EC2", "tokens": [19081, 17, 341, 307, 689, 291, 992, 493, 428, 14519, 293, 370, 490, 19081, 17], "temperature": 0.0, "avg_logprob": -0.261187215646108, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.6442085325252265e-05}, {"id": 1474, "seek": 717122, "start": 7171.22, "end": 7178.02, "text": " You need to do what's called launching an instance so launching an instance means you're basically creating a computer", "tokens": [509, 643, 281, 360, 437, 311, 1219, 18354, 364, 5197, 370, 18354, 364, 5197, 1355, 291, 434, 1936, 4084, 257, 3820], "temperature": 0.0, "avg_logprob": -0.23688849727664374, "compression_ratio": 1.796812749003984, "no_speech_prob": 4.565938979794737e-06}, {"id": 1475, "seek": 717122, "start": 7178.42, "end": 7182.34, "text": " right you're creating a computer on Amazon so I say launch instance and", "tokens": [558, 291, 434, 4084, 257, 3820, 322, 6795, 370, 286, 584, 4025, 5197, 293], "temperature": 0.0, "avg_logprob": -0.23688849727664374, "compression_ratio": 1.796812749003984, "no_speech_prob": 4.565938979794737e-06}, {"id": 1476, "seek": 717122, "start": 7183.1, "end": 7186.26, "text": " What we've done is we've created a fast AI", "tokens": [708, 321, 600, 1096, 307, 321, 600, 2942, 257, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.23688849727664374, "compression_ratio": 1.796812749003984, "no_speech_prob": 4.565938979794737e-06}, {"id": 1477, "seek": 717122, "start": 7186.34, "end": 7190.9800000000005, "text": " It's kind of AMI and AMI is like a template for how your computer is going to be created", "tokens": [467, 311, 733, 295, 6475, 40, 293, 6475, 40, 307, 411, 257, 12379, 337, 577, 428, 3820, 307, 516, 281, 312, 2942], "temperature": 0.0, "avg_logprob": -0.23688849727664374, "compression_ratio": 1.796812749003984, "no_speech_prob": 4.565938979794737e-06}, {"id": 1478, "seek": 717122, "start": 7191.06, "end": 7194.5, "text": " So if you go to community AMIs and type in class AI", "tokens": [407, 498, 291, 352, 281, 1768, 6475, 6802, 293, 2010, 294, 1508, 7318], "temperature": 0.0, "avg_logprob": -0.23688849727664374, "compression_ratio": 1.796812749003984, "no_speech_prob": 4.565938979794737e-06}, {"id": 1479, "seek": 719450, "start": 7194.5, "end": 7200.94, "text": " You'll see that there's one there called fast AI part 1 version 2 for the P2", "tokens": [509, 603, 536, 300, 456, 311, 472, 456, 1219, 2370, 7318, 644, 502, 3037, 568, 337, 264, 430, 17], "temperature": 0.0, "avg_logprob": -0.2392924003994342, "compression_ratio": 1.625, "no_speech_prob": 3.8448952182079665e-06}, {"id": 1480, "seek": 719450, "start": 7201.46, "end": 7203.66, "text": " Okay, so I'm going to select that and", "tokens": [1033, 11, 370, 286, 478, 516, 281, 3048, 300, 293], "temperature": 0.0, "avg_logprob": -0.2392924003994342, "compression_ratio": 1.625, "no_speech_prob": 3.8448952182079665e-06}, {"id": 1481, "seek": 719450, "start": 7204.62, "end": 7211.62, "text": " Then we need to say what kind of computer do you want and so I can say I want a GPU compute computer and", "tokens": [1396, 321, 643, 281, 584, 437, 733, 295, 3820, 360, 291, 528, 293, 370, 286, 393, 584, 286, 528, 257, 18407, 14722, 3820, 293], "temperature": 0.0, "avg_logprob": -0.2392924003994342, "compression_ratio": 1.625, "no_speech_prob": 3.8448952182079665e-06}, {"id": 1482, "seek": 719450, "start": 7212.66, "end": 7217.14, "text": " Then I can say I want a P2x large. This is the cheapest", "tokens": [1396, 286, 393, 584, 286, 528, 257, 430, 17, 87, 2416, 13, 639, 307, 264, 29167], "temperature": 0.0, "avg_logprob": -0.2392924003994342, "compression_ratio": 1.625, "no_speech_prob": 3.8448952182079665e-06}, {"id": 1483, "seek": 719450, "start": 7218.02, "end": 7222.58, "text": " Reasonably effective for deep learning instance type they have and then I can say launch", "tokens": [39693, 1188, 4942, 337, 2452, 2539, 5197, 2010, 436, 362, 293, 550, 286, 393, 584, 4025], "temperature": 0.0, "avg_logprob": -0.2392924003994342, "compression_ratio": 1.625, "no_speech_prob": 3.8448952182079665e-06}, {"id": 1484, "seek": 722258, "start": 7222.58, "end": 7224.58, "text": " and", "tokens": [293], "temperature": 0.0, "avg_logprob": -0.20500994701774752, "compression_ratio": 1.7783505154639174, "no_speech_prob": 2.225276375611429e-06}, {"id": 1485, "seek": 722258, "start": 7224.58, "end": 7226.9, "text": " then I can say launch and", "tokens": [550, 286, 393, 584, 4025, 293], "temperature": 0.0, "avg_logprob": -0.20500994701774752, "compression_ratio": 1.7783505154639174, "no_speech_prob": 2.225276375611429e-06}, {"id": 1486, "seek": 722258, "start": 7227.54, "end": 7231.94, "text": " So at this point they ask you to choose a key pair", "tokens": [407, 412, 341, 935, 436, 1029, 291, 281, 2826, 257, 2141, 6119], "temperature": 0.0, "avg_logprob": -0.20500994701774752, "compression_ratio": 1.7783505154639174, "no_speech_prob": 2.225276375611429e-06}, {"id": 1487, "seek": 722258, "start": 7232.18, "end": 7238.82, "text": " Right now if you don't have the key pair you have to create one right so to create a key pair", "tokens": [1779, 586, 498, 291, 500, 380, 362, 264, 2141, 6119, 291, 362, 281, 1884, 472, 558, 370, 281, 1884, 257, 2141, 6119], "temperature": 0.0, "avg_logprob": -0.20500994701774752, "compression_ratio": 1.7783505154639174, "no_speech_prob": 2.225276375611429e-06}, {"id": 1488, "seek": 722258, "start": 7240.22, "end": 7242.22, "text": " You need to open your terminal", "tokens": [509, 643, 281, 1269, 428, 14709], "temperature": 0.0, "avg_logprob": -0.20500994701774752, "compression_ratio": 1.7783505154639174, "no_speech_prob": 2.225276375611429e-06}, {"id": 1489, "seek": 722258, "start": 7243.58, "end": 7245.58, "text": " If you don't have a", "tokens": [759, 291, 500, 380, 362, 257], "temperature": 0.0, "avg_logprob": -0.20500994701774752, "compression_ratio": 1.7783505154639174, "no_speech_prob": 2.225276375611429e-06}, {"id": 1490, "seek": 722258, "start": 7246.1, "end": 7250.26, "text": " Terminal if you've got a Mac or a Linux box you've definitely got one if you've got Windows", "tokens": [19835, 2071, 498, 291, 600, 658, 257, 5707, 420, 257, 18734, 2424, 291, 600, 2138, 658, 472, 498, 291, 600, 658, 8591], "temperature": 0.0, "avg_logprob": -0.20500994701774752, "compression_ratio": 1.7783505154639174, "no_speech_prob": 2.225276375611429e-06}, {"id": 1491, "seek": 725026, "start": 7250.26, "end": 7257.42, "text": " Hopefully you've got Ubuntu if you don't already have Ubuntu set up you can go to the Windows Store and", "tokens": [10429, 291, 600, 658, 30230, 45605, 498, 291, 500, 380, 1217, 362, 30230, 45605, 992, 493, 291, 393, 352, 281, 264, 8591, 17242, 293], "temperature": 0.0, "avg_logprob": -0.24652252197265626, "compression_ratio": 1.5688073394495412, "no_speech_prob": 2.22527432924835e-06}, {"id": 1492, "seek": 725026, "start": 7259.02, "end": 7260.780000000001, "text": " Click on", "tokens": [8230, 322], "temperature": 0.0, "avg_logprob": -0.24652252197265626, "compression_ratio": 1.5688073394495412, "no_speech_prob": 2.22527432924835e-06}, {"id": 1493, "seek": 725026, "start": 7260.780000000001, "end": 7262.900000000001, "text": " Ubuntu right we'll get it from the Windows Store", "tokens": [30230, 45605, 558, 321, 603, 483, 309, 490, 264, 8591, 17242], "temperature": 0.0, "avg_logprob": -0.24652252197265626, "compression_ratio": 1.5688073394495412, "no_speech_prob": 2.22527432924835e-06}, {"id": 1494, "seek": 725026, "start": 7263.780000000001, "end": 7267.06, "text": " So from there you basically go SSH", "tokens": [407, 490, 456, 291, 1936, 352, 12238, 39], "temperature": 0.0, "avg_logprob": -0.24652252197265626, "compression_ratio": 1.5688073394495412, "no_speech_prob": 2.22527432924835e-06}, {"id": 1495, "seek": 725026, "start": 7268.74, "end": 7270.74, "text": " Dash key gen and", "tokens": [23453, 2141, 1049, 293], "temperature": 0.0, "avg_logprob": -0.24652252197265626, "compression_ratio": 1.5688073394495412, "no_speech_prob": 2.22527432924835e-06}, {"id": 1496, "seek": 725026, "start": 7271.02, "end": 7276.02, "text": " That will create like a special password for your computer to be able to log into Amazon", "tokens": [663, 486, 1884, 411, 257, 2121, 11524, 337, 428, 3820, 281, 312, 1075, 281, 3565, 666, 6795], "temperature": 0.0, "avg_logprob": -0.24652252197265626, "compression_ratio": 1.5688073394495412, "no_speech_prob": 2.22527432924835e-06}, {"id": 1497, "seek": 725026, "start": 7276.02, "end": 7278.18, "text": " And then you just hit enter three times", "tokens": [400, 550, 291, 445, 2045, 3242, 1045, 1413], "temperature": 0.0, "avg_logprob": -0.24652252197265626, "compression_ratio": 1.5688073394495412, "no_speech_prob": 2.22527432924835e-06}, {"id": 1498, "seek": 727818, "start": 7278.18, "end": 7283.740000000001, "text": " Okay, and that's going to create for you your key that you can use to get into Amazon", "tokens": [1033, 11, 293, 300, 311, 516, 281, 1884, 337, 291, 428, 2141, 300, 291, 393, 764, 281, 483, 666, 6795], "temperature": 0.0, "avg_logprob": -0.20424937413743705, "compression_ratio": 1.6330645161290323, "no_speech_prob": 1.2289136748222518e-06}, {"id": 1499, "seek": 727818, "start": 7284.06, "end": 7287.900000000001, "text": " All right, so then what I do is I copy that key somewhere that I know where it is", "tokens": [1057, 558, 11, 370, 550, 437, 286, 360, 307, 286, 5055, 300, 2141, 4079, 300, 286, 458, 689, 309, 307], "temperature": 0.0, "avg_logprob": -0.20424937413743705, "compression_ratio": 1.6330645161290323, "no_speech_prob": 1.2289136748222518e-06}, {"id": 1500, "seek": 727818, "start": 7287.900000000001, "end": 7294.22, "text": " So it'll be in the dot SSH folder, and it's called ID RSA dot pub and so I'm going to copy it", "tokens": [407, 309, 603, 312, 294, 264, 5893, 12238, 39, 10820, 11, 293, 309, 311, 1219, 7348, 497, 8886, 5893, 1535, 293, 370, 286, 478, 516, 281, 5055, 309], "temperature": 0.0, "avg_logprob": -0.20424937413743705, "compression_ratio": 1.6330645161290323, "no_speech_prob": 1.2289136748222518e-06}, {"id": 1501, "seek": 727818, "start": 7294.9800000000005, "end": 7296.9800000000005, "text": " to", "tokens": [281], "temperature": 0.0, "avg_logprob": -0.20424937413743705, "compression_ratio": 1.6330645161290323, "no_speech_prob": 1.2289136748222518e-06}, {"id": 1502, "seek": 727818, "start": 7296.9800000000005, "end": 7298.740000000001, "text": " My hard drive", "tokens": [1222, 1152, 3332], "temperature": 0.0, "avg_logprob": -0.20424937413743705, "compression_ratio": 1.6330645161290323, "no_speech_prob": 1.2289136748222518e-06}, {"id": 1503, "seek": 727818, "start": 7298.740000000001, "end": 7303.64, "text": " So if you're in a macro and Linux it'll already be in an easy-to-find place. It'll be in your dot SSH folder", "tokens": [407, 498, 291, 434, 294, 257, 18887, 293, 18734, 309, 603, 1217, 312, 294, 364, 1858, 12, 1353, 12, 35072, 1081, 13, 467, 603, 312, 294, 428, 5893, 12238, 39, 10820], "temperature": 0.0, "avg_logprob": -0.20424937413743705, "compression_ratio": 1.6330645161290323, "no_speech_prob": 1.2289136748222518e-06}, {"id": 1504, "seek": 730364, "start": 7303.64, "end": 7306.64, "text": " I'm going to put that in documents", "tokens": [286, 478, 516, 281, 829, 300, 294, 8512], "temperature": 0.0, "avg_logprob": -0.2725713112775017, "compression_ratio": 1.5705128205128205, "no_speech_prob": 9.721508149596048e-07}, {"id": 1505, "seek": 730364, "start": 7308.68, "end": 7310.4800000000005, "text": " So from there", "tokens": [407, 490, 456], "temperature": 0.0, "avg_logprob": -0.2725713112775017, "compression_ratio": 1.5705128205128205, "no_speech_prob": 9.721508149596048e-07}, {"id": 1506, "seek": 730364, "start": 7310.4800000000005, "end": 7316.360000000001, "text": " Back in AWS you have to tell it that you've created this key so you can go to key pairs and", "tokens": [5833, 294, 17650, 291, 362, 281, 980, 309, 300, 291, 600, 2942, 341, 2141, 370, 291, 393, 352, 281, 2141, 15494, 293], "temperature": 0.0, "avg_logprob": -0.2725713112775017, "compression_ratio": 1.5705128205128205, "no_speech_prob": 9.721508149596048e-07}, {"id": 1507, "seek": 730364, "start": 7317.8, "end": 7322.96, "text": " You say import key pair, and you just browse to that file that you just created", "tokens": [509, 584, 974, 2141, 6119, 11, 293, 291, 445, 31442, 281, 300, 3991, 300, 291, 445, 2942], "temperature": 0.0, "avg_logprob": -0.2725713112775017, "compression_ratio": 1.5705128205128205, "no_speech_prob": 9.721508149596048e-07}, {"id": 1508, "seek": 730364, "start": 7325.4800000000005, "end": 7327.72, "text": " There it is I say import", "tokens": [821, 309, 307, 286, 584, 974], "temperature": 0.0, "avg_logprob": -0.2725713112775017, "compression_ratio": 1.5705128205128205, "no_speech_prob": 9.721508149596048e-07}, {"id": 1509, "seek": 732772, "start": 7327.72, "end": 7333.64, "text": " Okay, so if you've ever used SSH before you've already got the key pair", "tokens": [1033, 11, 370, 498, 291, 600, 1562, 1143, 12238, 39, 949, 291, 600, 1217, 658, 264, 2141, 6119], "temperature": 0.0, "avg_logprob": -0.16540617353460763, "compression_ratio": 1.7055837563451777, "no_speech_prob": 4.289299340598518e-06}, {"id": 1510, "seek": 732772, "start": 7333.64, "end": 7337.9400000000005, "text": " You don't have to do those steps if you've used AWS before you've already imported it", "tokens": [509, 500, 380, 362, 281, 360, 729, 4439, 498, 291, 600, 1143, 17650, 949, 291, 600, 1217, 25524, 309], "temperature": 0.0, "avg_logprob": -0.16540617353460763, "compression_ratio": 1.7055837563451777, "no_speech_prob": 4.289299340598518e-06}, {"id": 1511, "seek": 732772, "start": 7337.9400000000005, "end": 7342.04, "text": " You don't have to do that step. Maybe you haven't done any of those things you have to do both steps", "tokens": [509, 500, 380, 362, 281, 360, 300, 1823, 13, 2704, 291, 2378, 380, 1096, 604, 295, 729, 721, 291, 362, 281, 360, 1293, 4439], "temperature": 0.0, "avg_logprob": -0.16540617353460763, "compression_ratio": 1.7055837563451777, "no_speech_prob": 4.289299340598518e-06}, {"id": 1512, "seek": 732772, "start": 7343.8, "end": 7347.64, "text": " So now I could go ahead and launch my instance", "tokens": [407, 586, 286, 727, 352, 2286, 293, 4025, 452, 5197], "temperature": 0.0, "avg_logprob": -0.16540617353460763, "compression_ratio": 1.7055837563451777, "no_speech_prob": 4.289299340598518e-06}, {"id": 1513, "seek": 732772, "start": 7351.0, "end": 7354.12, "text": " Community am is search fast AI", "tokens": [10421, 669, 307, 3164, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.16540617353460763, "compression_ratio": 1.7055837563451777, "no_speech_prob": 4.289299340598518e-06}, {"id": 1514, "seek": 735412, "start": 7354.12, "end": 7356.12, "text": " select", "tokens": [3048], "temperature": 0.0, "avg_logprob": -0.19900726742214628, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.2377297429775354e-06}, {"id": 1515, "seek": 735412, "start": 7357.44, "end": 7359.24, "text": " Launch and", "tokens": [28119, 293], "temperature": 0.0, "avg_logprob": -0.19900726742214628, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.2377297429775354e-06}, {"id": 1516, "seek": 735412, "start": 7359.24, "end": 7361.24, "text": " So now it asks me", "tokens": [407, 586, 309, 8962, 385], "temperature": 0.0, "avg_logprob": -0.19900726742214628, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.2377297429775354e-06}, {"id": 1517, "seek": 735412, "start": 7361.48, "end": 7366.74, "text": " What's where's your key pair and I can choose that one that I just grabbed okay?", "tokens": [708, 311, 689, 311, 428, 2141, 6119, 293, 286, 393, 2826, 300, 472, 300, 286, 445, 18607, 1392, 30], "temperature": 0.0, "avg_logprob": -0.19900726742214628, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.2377297429775354e-06}, {"id": 1518, "seek": 735412, "start": 7369.5199999999995, "end": 7373.88, "text": " So this is going to go ahead and create a new computer for me to log into", "tokens": [407, 341, 307, 516, 281, 352, 2286, 293, 1884, 257, 777, 3820, 337, 385, 281, 3565, 666], "temperature": 0.0, "avg_logprob": -0.19900726742214628, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.2377297429775354e-06}, {"id": 1519, "seek": 735412, "start": 7374.88, "end": 7379.04, "text": " And you can see here. It says the following have been initiated and so if I click on that", "tokens": [400, 291, 393, 536, 510, 13, 467, 1619, 264, 3480, 362, 668, 28578, 293, 370, 498, 286, 2052, 322, 300], "temperature": 0.0, "avg_logprob": -0.19900726742214628, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.2377297429775354e-06}, {"id": 1520, "seek": 737904, "start": 7379.04, "end": 7383.24, "text": " It'll show me this new computer that I've created", "tokens": [467, 603, 855, 385, 341, 777, 3820, 300, 286, 600, 2942], "temperature": 0.0, "avg_logprob": -0.20251890329214242, "compression_ratio": 1.7377049180327868, "no_speech_prob": 1.844815642471076e-06}, {"id": 1521, "seek": 737904, "start": 7383.92, "end": 7385.16, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.20251890329214242, "compression_ratio": 1.7377049180327868, "no_speech_prob": 1.844815642471076e-06}, {"id": 1522, "seek": 737904, "start": 7385.16, "end": 7387.16, "text": " So to be able to log into it I", "tokens": [407, 281, 312, 1075, 281, 3565, 666, 309, 286], "temperature": 0.0, "avg_logprob": -0.20251890329214242, "compression_ratio": 1.7377049180327868, "no_speech_prob": 1.844815642471076e-06}, {"id": 1523, "seek": 737904, "start": 7388.32, "end": 7390.32, "text": " Need to know its IP address", "tokens": [16984, 281, 458, 1080, 8671, 2985], "temperature": 0.0, "avg_logprob": -0.20251890329214242, "compression_ratio": 1.7377049180327868, "no_speech_prob": 1.844815642471076e-06}, {"id": 1524, "seek": 737904, "start": 7391.0, "end": 7394.88, "text": " So here it is the IP address there okay, so I can copy that and", "tokens": [407, 510, 309, 307, 264, 8671, 2985, 456, 1392, 11, 370, 286, 393, 5055, 300, 293], "temperature": 0.0, "avg_logprob": -0.20251890329214242, "compression_ratio": 1.7377049180327868, "no_speech_prob": 1.844815642471076e-06}, {"id": 1525, "seek": 737904, "start": 7395.96, "end": 7398.28, "text": " That's the IP address of my computer", "tokens": [663, 311, 264, 8671, 2985, 295, 452, 3820], "temperature": 0.0, "avg_logprob": -0.20251890329214242, "compression_ratio": 1.7377049180327868, "no_speech_prob": 1.844815642471076e-06}, {"id": 1526, "seek": 737904, "start": 7398.96, "end": 7404.92, "text": " So to get to this computer I need to SSH to it so SSH into a computer means connecting to that computer", "tokens": [407, 281, 483, 281, 341, 3820, 286, 643, 281, 12238, 39, 281, 309, 370, 12238, 39, 666, 257, 3820, 1355, 11015, 281, 300, 3820], "temperature": 0.0, "avg_logprob": -0.20251890329214242, "compression_ratio": 1.7377049180327868, "no_speech_prob": 1.844815642471076e-06}, {"id": 1527, "seek": 740492, "start": 7404.92, "end": 7409.4800000000005, "text": " So that it's like you're typing that computer, so I type SSH and the username", "tokens": [407, 300, 309, 311, 411, 291, 434, 18444, 300, 3820, 11, 370, 286, 2010, 12238, 39, 293, 264, 30351], "temperature": 0.0, "avg_logprob": -0.17979112693241664, "compression_ratio": 1.693798449612403, "no_speech_prob": 4.029427145724185e-06}, {"id": 1528, "seek": 740492, "start": 7410.4800000000005, "end": 7412.4800000000005, "text": " For this instance is always Ubuntu", "tokens": [1171, 341, 5197, 307, 1009, 30230, 45605], "temperature": 0.0, "avg_logprob": -0.17979112693241664, "compression_ratio": 1.693798449612403, "no_speech_prob": 4.029427145724185e-06}, {"id": 1529, "seek": 740492, "start": 7413.2, "end": 7417.02, "text": " right and then I can paste in that IP address and", "tokens": [558, 293, 550, 286, 393, 9163, 294, 300, 8671, 2985, 293], "temperature": 0.0, "avg_logprob": -0.17979112693241664, "compression_ratio": 1.693798449612403, "no_speech_prob": 4.029427145724185e-06}, {"id": 1530, "seek": 740492, "start": 7417.52, "end": 7419.52, "text": " Then there's one more thing I have to do", "tokens": [1396, 456, 311, 472, 544, 551, 286, 362, 281, 360], "temperature": 0.0, "avg_logprob": -0.17979112693241664, "compression_ratio": 1.693798449612403, "no_speech_prob": 4.029427145724185e-06}, {"id": 1531, "seek": 740492, "start": 7419.64, "end": 7425.78, "text": " Which is I have to connect up the Jupiter notebook on that instance to the Jupiter notebook on my machine", "tokens": [3013, 307, 286, 362, 281, 1745, 493, 264, 24567, 21060, 322, 300, 5197, 281, 264, 24567, 21060, 322, 452, 3479], "temperature": 0.0, "avg_logprob": -0.17979112693241664, "compression_ratio": 1.693798449612403, "no_speech_prob": 4.029427145724185e-06}, {"id": 1532, "seek": 740492, "start": 7425.78, "end": 7430.16, "text": " And so to do that there's just a particular flag that I said okay", "tokens": [400, 370, 281, 360, 300, 456, 311, 445, 257, 1729, 7166, 300, 286, 848, 1392], "temperature": 0.0, "avg_logprob": -0.17979112693241664, "compression_ratio": 1.693798449612403, "no_speech_prob": 4.029427145724185e-06}, {"id": 1533, "seek": 740492, "start": 7430.16, "end": 7432.4400000000005, "text": " We can talk about it on the forums as to exactly what it does", "tokens": [492, 393, 751, 466, 309, 322, 264, 26998, 382, 281, 2293, 437, 309, 775], "temperature": 0.0, "avg_logprob": -0.17979112693241664, "compression_ratio": 1.693798449612403, "no_speech_prob": 4.029427145724185e-06}, {"id": 1534, "seek": 743244, "start": 7432.44, "end": 7438.16, "text": " But you just type minus L a to date a localhost a date a date", "tokens": [583, 291, 445, 2010, 3175, 441, 257, 281, 4002, 257, 2654, 6037, 257, 4002, 257, 4002], "temperature": 0.0, "avg_logprob": -0.22377971334194918, "compression_ratio": 1.6569037656903767, "no_speech_prob": 5.682329174305778e-06}, {"id": 1535, "seek": 743244, "start": 7438.36, "end": 7443.919999999999, "text": " Okay, so like once you've done it once you can like save that as an alias and type in the same thing every time", "tokens": [1033, 11, 370, 411, 1564, 291, 600, 1096, 309, 1564, 291, 393, 411, 3155, 300, 382, 364, 419, 4609, 293, 2010, 294, 264, 912, 551, 633, 565], "temperature": 0.0, "avg_logprob": -0.22377971334194918, "compression_ratio": 1.6569037656903767, "no_speech_prob": 5.682329174305778e-06}, {"id": 1536, "seek": 743244, "start": 7445.679999999999, "end": 7450.219999999999, "text": " So we can check here we can see it says that it's running so we should be able to now here enter", "tokens": [407, 321, 393, 1520, 510, 321, 393, 536, 309, 1619, 300, 309, 311, 2614, 370, 321, 820, 312, 1075, 281, 586, 510, 3242], "temperature": 0.0, "avg_logprob": -0.22377971334194918, "compression_ratio": 1.6569037656903767, "no_speech_prob": 5.682329174305778e-06}, {"id": 1537, "seek": 743244, "start": 7451.4, "end": 7456.24, "text": " First time ever which set we connect to it. It just checks. This is okay. I'll say yes", "tokens": [2386, 565, 1562, 597, 992, 321, 1745, 281, 309, 13, 467, 445, 13834, 13, 639, 307, 1392, 13, 286, 603, 584, 2086], "temperature": 0.0, "avg_logprob": -0.22377971334194918, "compression_ratio": 1.6569037656903767, "no_speech_prob": 5.682329174305778e-06}, {"id": 1538, "seek": 745624, "start": 7456.24, "end": 7460.88, "text": " And then that goes ahead and SSH is in", "tokens": [400, 550, 300, 1709, 2286, 293, 12238, 39, 307, 294], "temperature": 0.0, "avg_logprob": -0.17454608281453451, "compression_ratio": 1.6919642857142858, "no_speech_prob": 4.78505899081938e-06}, {"id": 1539, "seek": 745624, "start": 7462.5199999999995, "end": 7464.5199999999995, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.17454608281453451, "compression_ratio": 1.6919642857142858, "no_speech_prob": 4.78505899081938e-06}, {"id": 1540, "seek": 745624, "start": 7465.24, "end": 7468.96, "text": " This AMI is all set up for you alright", "tokens": [639, 6475, 40, 307, 439, 992, 493, 337, 291, 5845], "temperature": 0.0, "avg_logprob": -0.17454608281453451, "compression_ratio": 1.6919642857142858, "no_speech_prob": 4.78505899081938e-06}, {"id": 1541, "seek": 745624, "start": 7468.96, "end": 7473.94, "text": " So you'll find that the very first time you log in it takes a few extra seconds because it just kind of is getting everything", "tokens": [407, 291, 603, 915, 300, 264, 588, 700, 565, 291, 3565, 294, 309, 2516, 257, 1326, 2857, 3949, 570, 309, 445, 733, 295, 307, 1242, 1203], "temperature": 0.0, "avg_logprob": -0.17454608281453451, "compression_ratio": 1.6919642857142858, "no_speech_prob": 4.78505899081938e-06}, {"id": 1542, "seek": 745624, "start": 7473.94, "end": 7479.42, "text": " Set up, but once it's logged in you'll see there that there's a directory called fast AI", "tokens": [8928, 493, 11, 457, 1564, 309, 311, 27231, 294, 291, 603, 536, 456, 300, 456, 311, 257, 21120, 1219, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.17454608281453451, "compression_ratio": 1.6919642857142858, "no_speech_prob": 4.78505899081938e-06}, {"id": 1543, "seek": 747942, "start": 7479.42, "end": 7485.9, "text": " And the fast AI directory contains our fast AI repo that contains all the notebooks", "tokens": [400, 264, 2370, 7318, 21120, 8306, 527, 2370, 7318, 49040, 300, 8306, 439, 264, 43782], "temperature": 0.0, "avg_logprob": -0.25233823988172743, "compression_ratio": 1.7236180904522613, "no_speech_prob": 5.173886620468693e-06}, {"id": 1544, "seek": 747942, "start": 7487.14, "end": 7493.7, "text": " All the code etc so I can just go CD fast AI right first thing you do when you get in is to make sure", "tokens": [1057, 264, 3089, 5183, 370, 286, 393, 445, 352, 6743, 2370, 7318, 558, 700, 551, 291, 360, 562, 291, 483, 294, 307, 281, 652, 988], "temperature": 0.0, "avg_logprob": -0.25233823988172743, "compression_ratio": 1.7236180904522613, "no_speech_prob": 5.173886620468693e-06}, {"id": 1545, "seek": 747942, "start": 7493.7, "end": 7495.7, "text": " It's updated so you just go get pull", "tokens": [467, 311, 10588, 370, 291, 445, 352, 483, 2235], "temperature": 0.0, "avg_logprob": -0.25233823988172743, "compression_ratio": 1.7236180904522613, "no_speech_prob": 5.173886620468693e-06}, {"id": 1546, "seek": 747942, "start": 7496.7, "end": 7498.7, "text": " Right and that updates", "tokens": [1779, 293, 300, 9205], "temperature": 0.0, "avg_logprob": -0.25233823988172743, "compression_ratio": 1.7236180904522613, "no_speech_prob": 5.173886620468693e-06}, {"id": 1547, "seek": 747942, "start": 7499.02, "end": 7502.86, "text": " to make sure that your repo is the same as the most recent repo and", "tokens": [281, 652, 988, 300, 428, 49040, 307, 264, 912, 382, 264, 881, 5162, 49040, 293], "temperature": 0.0, "avg_logprob": -0.25233823988172743, "compression_ratio": 1.7236180904522613, "no_speech_prob": 5.173886620468693e-06}, {"id": 1548, "seek": 747942, "start": 7504.1, "end": 7505.46, "text": " So as you can see there we go", "tokens": [407, 382, 291, 393, 536, 456, 321, 352], "temperature": 0.0, "avg_logprob": -0.25233823988172743, "compression_ratio": 1.7236180904522613, "no_speech_prob": 5.173886620468693e-06}, {"id": 1549, "seek": 750546, "start": 7505.46, "end": 7511.18, "text": " Let's make sure it's got all the most recent code the second thing you should do is type conda and update", "tokens": [961, 311, 652, 988, 309, 311, 658, 439, 264, 881, 5162, 3089, 264, 1150, 551, 291, 820, 360, 307, 2010, 2224, 64, 293, 5623], "temperature": 0.0, "avg_logprob": -0.2066337145291842, "compression_ratio": 1.7826086956521738, "no_speech_prob": 2.6016055016953032e-06}, {"id": 1550, "seek": 750546, "start": 7511.46, "end": 7517.74, "text": " You can just do this maybe once a month or so and that makes sure that the libraries there are all the most recent libraries", "tokens": [509, 393, 445, 360, 341, 1310, 1564, 257, 1618, 420, 370, 293, 300, 1669, 988, 300, 264, 15148, 456, 366, 439, 264, 881, 5162, 15148], "temperature": 0.0, "avg_logprob": -0.2066337145291842, "compression_ratio": 1.7826086956521738, "no_speech_prob": 2.6016055016953032e-06}, {"id": 1551, "seek": 750546, "start": 7517.74, "end": 7523.12, "text": " I'm not going to run that so it takes a couple of minutes. Okay, and then the last step is to type to a notebook", "tokens": [286, 478, 406, 516, 281, 1190, 300, 370, 309, 2516, 257, 1916, 295, 2077, 13, 1033, 11, 293, 550, 264, 1036, 1823, 307, 281, 2010, 281, 257, 21060], "temperature": 0.0, "avg_logprob": -0.2066337145291842, "compression_ratio": 1.7826086956521738, "no_speech_prob": 2.6016055016953032e-06}, {"id": 1552, "seek": 750546, "start": 7525.9800000000005, "end": 7530.82, "text": " Okay, so this is going to go ahead and launch the jupyter notebook", "tokens": [1033, 11, 370, 341, 307, 516, 281, 352, 2286, 293, 4025, 264, 361, 1010, 88, 391, 21060], "temperature": 0.0, "avg_logprob": -0.2066337145291842, "compression_ratio": 1.7826086956521738, "no_speech_prob": 2.6016055016953032e-06}, {"id": 1553, "seek": 753082, "start": 7530.82, "end": 7536.46, "text": " Server on this machine again the first time I do it the first time you do everything on", "tokens": [25684, 322, 341, 3479, 797, 264, 700, 565, 286, 360, 309, 264, 700, 565, 291, 360, 1203, 322], "temperature": 0.0, "avg_logprob": -0.1667889093948623, "compression_ratio": 1.7660377358490567, "no_speech_prob": 1.0616035979182925e-05}, {"id": 1554, "seek": 753082, "start": 7537.0199999999995, "end": 7539.42, "text": " AWS it just takes like a minute or two", "tokens": [17650, 309, 445, 2516, 411, 257, 3456, 420, 732], "temperature": 0.0, "avg_logprob": -0.1667889093948623, "compression_ratio": 1.7660377358490567, "no_speech_prob": 1.0616035979182925e-05}, {"id": 1555, "seek": 753082, "start": 7540.179999999999, "end": 7545.259999999999, "text": " And then once you've done it in the future. It'll be just as fast as running it locally basically", "tokens": [400, 550, 1564, 291, 600, 1096, 309, 294, 264, 2027, 13, 467, 603, 312, 445, 382, 2370, 382, 2614, 309, 16143, 1936], "temperature": 0.0, "avg_logprob": -0.1667889093948623, "compression_ratio": 1.7660377358490567, "no_speech_prob": 1.0616035979182925e-05}, {"id": 1556, "seek": 753082, "start": 7546.34, "end": 7549.139999999999, "text": " So you can see it's going ahead and firing up the notebook", "tokens": [407, 291, 393, 536, 309, 311, 516, 2286, 293, 16045, 493, 264, 21060], "temperature": 0.0, "avg_logprob": -0.1667889093948623, "compression_ratio": 1.7660377358490567, "no_speech_prob": 1.0616035979182925e-05}, {"id": 1557, "seek": 753082, "start": 7549.139999999999, "end": 7554.36, "text": " And so what's going to happen is that because when we SSH into it we said to like connect our", "tokens": [400, 370, 437, 311, 516, 281, 1051, 307, 300, 570, 562, 321, 12238, 39, 666, 309, 321, 848, 281, 411, 1745, 527], "temperature": 0.0, "avg_logprob": -0.1667889093948623, "compression_ratio": 1.7660377358490567, "no_speech_prob": 1.0616035979182925e-05}, {"id": 1558, "seek": 753082, "start": 7555.099999999999, "end": 7560.259999999999, "text": " Notebook port to the remote notebook port. We're just going to be able to use this locally", "tokens": [11633, 2939, 2436, 281, 264, 8607, 21060, 2436, 13, 492, 434, 445, 516, 281, 312, 1075, 281, 764, 341, 16143], "temperature": 0.0, "avg_logprob": -0.1667889093948623, "compression_ratio": 1.7660377358490567, "no_speech_prob": 1.0616035979182925e-05}, {"id": 1559, "seek": 756026, "start": 7560.26, "end": 7564.3, "text": " So I see he says here copy paste this URL, so I'm going to grab that URL and", "tokens": [407, 286, 536, 415, 1619, 510, 5055, 9163, 341, 12905, 11, 370, 286, 478, 516, 281, 4444, 300, 12905, 293], "temperature": 0.0, "avg_logprob": -0.21054083650762384, "compression_ratio": 1.6019900497512438, "no_speech_prob": 8.801041985861957e-06}, {"id": 1560, "seek": 756026, "start": 7565.58, "end": 7567.58, "text": " I'm going to paste it into my browser", "tokens": [286, 478, 516, 281, 9163, 309, 666, 452, 11185], "temperature": 0.0, "avg_logprob": -0.21054083650762384, "compression_ratio": 1.6019900497512438, "no_speech_prob": 8.801041985861957e-06}, {"id": 1561, "seek": 756026, "start": 7571.18, "end": 7576.820000000001, "text": " And that's it okay, so this notebook is now actually not running on my machine", "tokens": [400, 300, 311, 309, 1392, 11, 370, 341, 21060, 307, 586, 767, 406, 2614, 322, 452, 3479], "temperature": 0.0, "avg_logprob": -0.21054083650762384, "compression_ratio": 1.6019900497512438, "no_speech_prob": 8.801041985861957e-06}, {"id": 1562, "seek": 756026, "start": 7577.06, "end": 7582.1, "text": " It's actually running on AWS okay using the AWS GPU. We've got a lot of memory", "tokens": [467, 311, 767, 2614, 322, 17650, 1392, 1228, 264, 17650, 18407, 13, 492, 600, 658, 257, 688, 295, 4675], "temperature": 0.0, "avg_logprob": -0.21054083650762384, "compression_ratio": 1.6019900497512438, "no_speech_prob": 8.801041985861957e-06}, {"id": 1563, "seek": 756026, "start": 7582.3, "end": 7585.08, "text": " It's not the fastest around but it's not terrible", "tokens": [467, 311, 406, 264, 14573, 926, 457, 309, 311, 406, 6237], "temperature": 0.0, "avg_logprob": -0.21054083650762384, "compression_ratio": 1.6019900497512438, "no_speech_prob": 8.801041985861957e-06}, {"id": 1564, "seek": 758508, "start": 7585.08, "end": 7591.4, "text": " You can always fire up a p3 if you want something that's super fast. This is costing me 90 cents a minute", "tokens": [509, 393, 1009, 2610, 493, 257, 280, 18, 498, 291, 528, 746, 300, 311, 1687, 2370, 13, 639, 307, 37917, 385, 4289, 14941, 257, 3456], "temperature": 0.0, "avg_logprob": -0.23539177964373334, "compression_ratio": 1.4615384615384615, "no_speech_prob": 4.637834990717238e-06}, {"id": 1565, "seek": 758508, "start": 7592.64, "end": 7596.5199999999995, "text": " Okay, so when you're finished, please don't forget to shut it down", "tokens": [1033, 11, 370, 562, 291, 434, 4335, 11, 1767, 500, 380, 2870, 281, 5309, 309, 760], "temperature": 0.0, "avg_logprob": -0.23539177964373334, "compression_ratio": 1.4615384615384615, "no_speech_prob": 4.637834990717238e-06}, {"id": 1566, "seek": 758508, "start": 7597.04, "end": 7599.68, "text": " right so to shut it down you can", "tokens": [558, 370, 281, 5309, 309, 760, 291, 393], "temperature": 0.0, "avg_logprob": -0.23539177964373334, "compression_ratio": 1.4615384615384615, "no_speech_prob": 4.637834990717238e-06}, {"id": 1567, "seek": 758508, "start": 7600.08, "end": 7604.0, "text": " Right click on it and say instant state", "tokens": [1779, 2052, 322, 309, 293, 584, 9836, 1785], "temperature": 0.0, "avg_logprob": -0.23539177964373334, "compression_ratio": 1.4615384615384615, "no_speech_prob": 4.637834990717238e-06}, {"id": 1568, "seek": 758508, "start": 7605.0, "end": 7606.48, "text": " stop", "tokens": [1590], "temperature": 0.0, "avg_logprob": -0.23539177964373334, "compression_ratio": 1.4615384615384615, "no_speech_prob": 4.637834990717238e-06}, {"id": 1569, "seek": 758508, "start": 7606.48, "end": 7607.88, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.23539177964373334, "compression_ratio": 1.4615384615384615, "no_speech_prob": 4.637834990717238e-06}, {"id": 1570, "seek": 758508, "start": 7607.88, "end": 7609.88, "text": " We've got 500 bucks of credit", "tokens": [492, 600, 658, 5923, 11829, 295, 5397], "temperature": 0.0, "avg_logprob": -0.23539177964373334, "compression_ratio": 1.4615384615384615, "no_speech_prob": 4.637834990717238e-06}, {"id": 1571, "seek": 760988, "start": 7609.88, "end": 7614.52, "text": " Assuming that you put your code down in the spreadsheet one thing", "tokens": [6281, 24919, 300, 291, 829, 428, 3089, 760, 294, 264, 27733, 472, 551], "temperature": 0.0, "avg_logprob": -0.2470390035751018, "compression_ratio": 1.5740740740740742, "no_speech_prob": 1.8631091734278016e-05}, {"id": 1572, "seek": 760988, "start": 7614.52, "end": 7619.04, "text": " I forgot to do the first time I showed you this by the way. I said make sure you choose a", "tokens": [286, 5298, 281, 360, 264, 700, 565, 286, 4712, 291, 341, 538, 264, 636, 13, 286, 848, 652, 988, 291, 2826, 257], "temperature": 0.0, "avg_logprob": -0.2470390035751018, "compression_ratio": 1.5740740740740742, "no_speech_prob": 1.8631091734278016e-05}, {"id": 1573, "seek": 760988, "start": 7620.2, "end": 7624.6, "text": " P2 the second time I went through I didn't choose p2 by mistake", "tokens": [430, 17, 264, 1150, 565, 286, 1437, 807, 286, 994, 380, 2826, 280, 17, 538, 6146], "temperature": 0.0, "avg_logprob": -0.2470390035751018, "compression_ratio": 1.5740740740740742, "no_speech_prob": 1.8631091734278016e-05}, {"id": 1574, "seek": 760988, "start": 7624.6, "end": 7629.86, "text": " So just don't forget to choose GPU compute p2 you have a question", "tokens": [407, 445, 500, 380, 2870, 281, 2826, 18407, 14722, 280, 17, 291, 362, 257, 1168], "temperature": 0.0, "avg_logprob": -0.2470390035751018, "compression_ratio": 1.5740740740740742, "no_speech_prob": 1.8631091734278016e-05}, {"id": 1575, "seek": 760988, "start": 7631.76, "end": 7634.86, "text": " My buddy said it's an hour. Thank you", "tokens": [1222, 10340, 848, 309, 311, 364, 1773, 13, 1044, 291], "temperature": 0.0, "avg_logprob": -0.2470390035751018, "compression_ratio": 1.5740740740740742, "no_speech_prob": 1.8631091734278016e-05}, {"id": 1576, "seek": 760988, "start": 7636.16, "end": 7638.16, "text": " 90 cents an hour", "tokens": [4289, 14941, 364, 1773], "temperature": 0.0, "avg_logprob": -0.2470390035751018, "compression_ratio": 1.5740740740740742, "no_speech_prob": 1.8631091734278016e-05}, {"id": 1577, "seek": 763816, "start": 7638.16, "end": 7643.04, "text": " It also costs like I don't know three or four bucks a month for the storage as well", "tokens": [467, 611, 5497, 411, 286, 500, 380, 458, 1045, 420, 1451, 11829, 257, 1618, 337, 264, 6725, 382, 731], "temperature": 0.0, "avg_logprob": -0.1782968961275541, "compression_ratio": 1.3025210084033614, "no_speech_prob": 3.0237166356528178e-05}, {"id": 1578, "seek": 764304, "start": 7643.04, "end": 7671.04, "text": " Thanks for checking that. All right. See you next week. Sorry. We're over", "tokens": [50364, 2561, 337, 8568, 300, 13, 1057, 558, 13, 3008, 291, 958, 1243, 13, 4919, 13, 492, 434, 670, 51764], "temperature": 0.0, "avg_logprob": -0.5050740469069708, "compression_ratio": 0.9605263157894737, "no_speech_prob": 0.00011576840915950015}], "language": "en"}