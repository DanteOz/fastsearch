{"text": " We'll be starting with notebook two. Sorry, I'm getting over a cold. Topic modeling with NMF and SVD. Okay, so I briefly showed this diagram last time of works of Shakespeare. There's different plays going across the top of this matrix. And then the rows are different vocabulary words. And this is a way that you can represent a body of work in a single matrix. And this is a type of bag of words approach because we're not looking at any of the syntax or the structure of the sentences. We're really just interested in the frequency of the words that show up with them. And so today we'll be talking about two different ways to decompose this into a tall thin matrix times a wide short matrix. And something I want to emphasize is the reason that we like kind of one reason we like matrix decompositions is that the matrices we're decomposing into have special properties that are helpful. So I wanted to start kind of just hypothetically thinking about an extreme case where suppose you wanted to reconstruct a matrix just using an outer product of two vectors. Let me switch over to that. So if you had kind of your words are still the rows, the documents are the columns. And as motivation, you could think of just doing this outer product of two vectors. This is not going to be a very good reconstruction. So actually I should put that as a very approximate. But the best you could do with this would be to have kind of this first vector be kind of the relative frequencies of each vocabulary word compared to out of the total word count. And then this could be the words per document. And so this would really just kind of reconstruct a very averaged version. If you wanted to kind of extend this to the next simplest case, that would be to let this be two columns and this one be two rows. And if you did that, what you would kind of be wanting to do is have your documents in two different clusters. And so you could be capturing the relative frequency of words for documents in one cluster with that first, you know, tall skinny matrix and the relative kind of words per document with the two. So this is just kind of some motivation of kind of thinking about these very simple cases. Yeah, so today will be actually I wanted to show. So here I've just got my import statements. And again, let me know if you have any problems getting your imports working or Jupyter Notebook running properly. I wanted to highlight. So I've linked to a few different resources that I thought were helpful. I particularly I almost used it in this class, but didn't. And there's this really nice called text analysis for the humanities and social sciences. And it's a pretty lengthy tutorial that kind of walks through using French and British literature. So it's got Victor Hugo and Jane Austen doing a word analysis. They use some different techniques. But if this is a topic that interests you, you might want to check that out. So we'll be using a built in data set from scikit learn today. I mentioned last time scikit learn has a lot of different built in data sets. And this is newsgroups. So newsgroups. This is kind of before the World Wide Web took off. These were kind of discussion boards and they're still in existence. People are talking about different topics. So you can kind of think of this as similar to something like Reddit, where you have people posting within different categories. And so we're using this kind of functionality from scikit learn to fetch the newsgroups data. And you can pass a parameter to say which categories you want. Today we're just going to look at four categories to keep it simpler. And something to note is we're getting the categories because it'll be helpful for us. But what we're doing is unstructured or sorry unsupervised machine learning. And so we're not actually using the categories. We're kind of going to be thinking about like, have we clustered these into topics that make sense? And we'll check with the categories to confirm that. So whenever you get data, it's great to just check what the dimension of it is. So here that's a 2034. So that's how many kind of separate posts we have. We'll just look at a few of them. So the categories and I want you to guess which category each post is from. The choices are atheism, miscellaneous religion, computer graphics or space. So let's look at this first one. I've noticed that if you only save a model with all your mapping planes carefully positioned to a point three DS file, which which category do you think this is from? Someone say it louder. Yes, from graphics. And this next one says it's talking about Koresh was a deranged fanatic who thought it was necessary to take a whole bunch of folks with him. Mentioned Jim Jones, which category do you think this is? Anyone? So this could be atheism or it could be religion and we'll see some overlap between those. Next one, I actually had to look this up. It's talking about Perry Joves, which is turns out a point in the orbit of a satellite of Jupiter. So what might that be? Space, exactly. And so that's always kind of good to just see like, okay, this is what what the data looks like. And you will see like we're seeing some kind of like numbers and things not not all the words are going to be helpful to us. So here we can check the target is giving us the label. And then scikit learn. So kind of if we wanted to make that matrix with all the word frequencies, we could write something ourselves, which would be a bit tedious to get those word frequencies. Scikit learn has a feature extraction count vectorizer that does that. So we'll just use their count vectorizer. So that's what's happening. This input cell count vectorizer. And we're passing in stop words to say that we don't want to don't want to include those since they don't contain information. But oh and stop words are words like is the A that don't have much meaning or don't really have much like kind of category or subject meaning. And so note after we do this, we check our so we kind of get our vectorizer and we're doing a vectorizer fit transform. We pass the newsgroups the training data. And then we're converting this to dense. And last time I talked about sparse versus dense matrices. We'll go into more detail on that in the future. But what scikit learn is returning is sparse and we're going to make it dense because that's easier for what we're doing today. And we can check the shape. And so we've still got 2000 documents and then it looks like we have 26,000 vocabulary words. Oh, that's a good question. I don't think we have any guarantees on what order it's in. Oh, actually, never mind when you get so there's a vocab dot or sorry vectorizer dot get feature names down here and putting that into an array and they are alphabetical. Let me actually pull up the. Oh, thank you. That too. What I wanted to illustrate. And there are a lot of a lot of features that we weren't using. So you could remove accents and do some sort of pre processing. Yeah, that's a good way to kind of find out more about your methods. Yeah, down here I am kind of use vectorizer get feature names, put those into vocab and then looking at the vocab, it's alphabetical. And so here's a section in the seas of some of the words that show up. Any questions so far? Do you want to stem words so that they both need so that you can reduce the features? For example, we have council and councils. That's a great suggestion. Yeah, so this is kind of a lazy approach where we have not. Not stemmed the words and stemming refers to kind of taking different variations of the same word and putting them on the same thing. So that's something like the word walk. You would take walking and walked and walks and kind of classify all those under walk. And we're not doing that here, but that would be a completely legitimate thing to do. OK, so first up, we're going to use singular value decomposition. And actually, I'm curious, have you covered SVD in previous courses? What context did you see it in? Image compression. OK, great. It's a great application of this. So Gilbert Strain has written a classic linear algebra textbook that SVD is not nearly as famous as it should be. And so here kind of the key thing with our decomposition is we're saying that we want the matrices we're decomposing into to be orthogonal. And the motivation behind that is that we're thinking that words that are frequently in one topic are probably less likely to be in other topics. Otherwise, they wouldn't be a good thing to kind of define that topic of what makes that topic unique or different from the others. And so that's a great question. What's orthogonal? That's any two. So if you have a collection of vectors, any two vectors that are different dot product is zero. The vector with itself. Oh, so orthonormal means that the dot product with itself is one and that the dot product with other vectors is zero. And a little bit confusing when you talk about matrices being orthogonal, that means that their columns and rows are worth the normal. This is a great nice diagram from a Facebook blog post that we're going to refer back to later. But so here they're talking about the words being the rows and then having hashtags as columns. And there are these are frequencies of how often word shows up in a post with that particular hashtag also in the post. And then you is the kind of first matrix you're getting. And the columns of you are orthogonal to each other. And then we've got this kind of middle diagonal matrix that the white part is all zeros. It only has values along the diagonal, which is blue. And that's giving you the relative importance. So kind of those values are called the singular values and tell you relative importance. And then the pink matrix on the right there, the rows are all orthonormal to one another. The rows are, yes. Oh, sorry, the rows are orthonormal. Something to note about SPD is that it's an exact decomposition when you're doing full SPD. And so that means you can completely recover your matrix and that this is also in this case, if R was equal to N. And so it's the same dimensions. This would be an equal sign. So SPD is very widely used. And we're going to see this CSVD in multiple, multiple of the lessons in this course. We're using it today for semantic analysis. It's also used in collaborative filtering and recommendations. The winners of the Netflix Prize were all kind of more complicated variations of SPD. Data compression, which you've seen, it can calculate the more Penrose pseudo inverse. So this is for matrices that don't have a true inverse. There's kind of the pseudo inverse. And you use SPD to find it. And then principal component analysis, which we'll also see. Yes. What does orthogonal mean like intuitively and why is that useful for this example? Good question. So what orthogonal means intuitively is that you're capturing very different information, very different directions. And so if you were thinking about directions, it's like perpendicular. You're not even having stuff that's somewhat in the same direction, but capturing very different. So in this context of thinking about topics or categories, it's we want very different topics and kind of finding what makes this different from everything else. And if you think of the kind of math definition of this dot product going to zero, it's that everything has canceled out. But thank you, Jeremy. So now I've. Oh, and this should show using percent time to time what I'm doing in Jupiter. These are called magics when you have a percent or double percent. And this is really handy and we'll use this some more to kind of get comparisons of how long things are taking. Yes. And so the question was, is it possible just to set it once and have it throughout the notebook? I don't know of a way to do that. Cool. OK. Me. So now I wanted to ask you, so we're using and this is from SciPy's Lin now just be. We get you S and VH back. Just take a moment to confirm that this is the decomposition of the input. So it's always good to kind of check that you're getting what you think. No, we don't. It will depend what you're doing. And this. Trying to remember, it's going to be later on that we're using the fact that we converted to dense. But a lot of a lot of operations can be done sparsely. Sure. And I'll there's a future lesson. There's a future lesson where I'll give more detail on exactly the SciPy sparse types. But just in general, with a sparse matrix. So you can kind of think of a dense matrix like an Excel spreadsheet where you have a square for every space in the matrix. And you're, you know, putting in the value of zero. You're storing a zero there and you kind of have that block of memory for a sparse one. You only store the non zero values. So there. It could it could conceivably just kind of be this list of you're going to need to have the row and coordinate because then to keep track of where it is. Actually, let me show the picture again. Yeah, so this is what a sparse matrix looks like. You have all these zeros. And so what you could do. Let me try this at writing on the screen. Okay, great. So what I was going to do is say so this is kind of showing the the dense storage because you wrote out all these zeros again and again. Instead, you could just be like, okay, it plays zero coordinate zero zero. I have a one a coordinate one one. I have a one at coordinate to two. I have a half and so on and kind of construct that list. And that would be sparse storage. We'll give more details around that. Okay, so do you guys have your answer for confirming the decomposition? And what did you do? So I want to say. Okay. Okay. So what I did. So I used NumPy has a method called NP.diag that will. So I thought you had kind of done a vector matrix product. Yeah, to get the same thing. Diag is handy. It takes in a vector and makes a square matrix C with those as the values. Actually, it's a little bit confusing. Diag can go in both directions. If you give it a vector, it returns a matrix. If you give it a matrix, it'll return a vector of what was on the diagonal. That's kind of a shortcut. Oh, okay. And so the question was about at. This is in Python three. At is matrix multiplication. And we talked briefly last time. About how you can have both Python two and Python three installed on your computer. I definitely recommend switching to Python three for this class or in general. I recommend Python three. But you don't have to. And it's fine to do this course in Python two. And if I was in Python two, I would use np.matmul, which is matrix multiplication. What is it from np.dot? From? From what? Oh, np.dot. Sure. So np.dot is kind of cool, but also kind of annoying. It does a lot of magic. So it tends to regardless of what dimensions you give it, it tries to find a way to make it work. So the documentation for np.dot describes all of those in detail. So sometimes that's good, but sometimes it's like you have a bargain you can barely realize because it's something that you can't believe. And then actually, yes? Oh. Could you also use the np.all course? Yes. Yeah, let me do that. And I actually, normally I prefer np.all close, so I'm kind of surprised that I did this subtraction here. Let me do that too. Oh. Hmm. Okay, well, I will look into this later. Did you use all close on yours? I might have changed these variables lower down and be getting something different. So, okay, great. Yeah, no, I like np.all close. And that's checking. It takes matrices and checks that all the, each element is similar. Great. Any questions? I thought these, I mean, so I actually haven't been running this as I go through. Oh, but, because my vectors have been converted to dense. I would prefer to keep going. I think that this is an artifact that I haven't been running the previous ones. Yes? Let me pass the microphone. Sorry. So, what is the function np.the linear, whatever the norm that norm is? Oh, that's a good question. So, norms in general give you kind of the distance between two things as a way of thinking of it. And this I would believe would default to the L2 norm, but that's a what? The Frobenius norm. The Frobenius norm, okay. And you can always pass an argument. But so what that's doing is we're taking the difference, and that would be a matrix of things that are close to zero, but I just wanted to see a single number. So, I used the norm to kind of get that down to a single number. And norms you can think of as size, typically. Yes? Yeah, so you're kind of squaring each element and adding them all together for the Frobenius norm. Okay, next up, I want you to confirm that u and v are orthonormal. So, just take a moment to write in some code that does that. Oh, so that's something I do. This depends on how people define it, but a lot of definitions of SVD distinguish that you're actually getting back the transpose of v as opposed to v itself. You can ignore that, but you will kind of see this difference about what people define as v versus v transpose. And the H stands for Hermitian, which is kind of the equivalent to transpose when you have complex numbers. But we'll be sticking with real numbers in here. Okay. Okay. Raise your hand if you want a little bit more time. Raise your hand if you have the answer. And I should be clear, when I say u and v are orthonormal, I don't mean to each other. I mean that the columns of u are orthonormal to the other columns, and the rows of v are orthonormal to the other rows. All right. Does someone want to share their answer? What they did? Great. Exactly. Yeah, thank you. So what you can do is multiply them by their transpose and then compare them to the identity. And the reason this works is multiplying u by u transpose is multiplying, and actually one of these I guess I have backwards. Each column should maybe be this. But the kind of multiplies each column of u. Yeah. So taking the transpose, you get the rows by the columns. That's kind of column of u by column of u with v. You're getting row of v by row of v. And then when they're kind of the same row by itself, that's the diagonal of the identity. That's why you're getting ones, different kind of two different vectors. Those are the off diagonals, the zeros. Any questions about that? And actually let me draw that briefly. So the idea is if this was u, you've got these columns. This is u transpose. When you multiply those together, you'll kind of end up taking this vector times this vector and so on. And so that should be one. And doing this two different ones, that's going to give you zero and so on. All right. And so now kind of confirmed that we got what we expected with the SVD. We can look at the singular values. And remember that these are giving us kind of this measure of importance. And notice that it drops off very quickly. So kind of there are some pretty high values for importance here with the first few singular values. And then it really drops off. And it's hard to know exactly kind of what these numbers correspond to, but it's helpful to see the relative importance. And the vector s of singular values is always ordered. So we kind of have the biggest values first. And I think this will be easier when we see the topics. But we can pass in. And so remember we're doing going back to the picture, u times s times v. Kind of larger ones are saying like, OK, these columns of u and these rows of v are more important or make up a bigger component of our original matrix. They contribute more to the original matrix. So we have this little helper method, show topics. And we're going to pass in our matrix v. And then it's going to look up the words that correspond to the values. And remember, our vocabulary words were in alphabetical order. This is basically going to find, OK, the largest values showed up in these particular columns, which corresponded to these vocabulary words. And so we get v, h. So we're looking at, sorry, the first five columns of v, h and the top eight topic words. So something to notice is one, this first topic is very weird. So we'll ignore that for a moment and look at the other ones. But so we have a topic that is largely represented by the words JPEG, GIF, file, color, quality, image, format. Thinking back to our categories, which category might that correspond to? Right, computer graphics. The next one is graphics, EDU, pub, mail, ray, FTP. And so any guesses? Yeah, I think that's also computer graphics. And so notice that they're kind of different, possibly different subgroups within a topic. But then Jesus, God, Matthew, people, atheists, atheism. This is possibly either religion or atheism. And then another one on graphics. So yeah, graphics is well represented. So let me try taking 10 topics to see if we can get some more. OK, here's a space one, space NASA, lunar Mars probe, moon mission. So there are some space ones in there, too. So yeah, we got topics that matched with the kind of clusters that we would expect, even though we had never passed the categories in. Any questions? Yes. So could you specify what each column of U is actually, like what column of U is? I think the row of VH will be the topics, right? Right, yeah, the rows of V are the topics, or VH. The columns of U correspond to the particular post. So we could use that if we wanted to look at a particular post and see, OK, how much of each topic shows up in that post. So from the beginning, kind of when we read somebody asking a question about computer graphics, we could look back and see. So this is, these are sometimes called embeddings, but U is kind of giving us, like OK, these are how the individual posts were embedded and into the, kind of with the topics, and then V is giving us, and these are how the words kind of correspond to those topics. So you could write sample words, or just each unit you were back in the box? Yeah, I'll try. That's a good question. I'll try writing a method. I'm on the fly, but what you would have to do is kind of pick off the largest entries, and then look up which, really you would want to tie it back to the words eventually. So I think then kind of look up, yeah, which topics are biggest, and then what words are in those topics. Any other questions? OK. So in your picture, R is the number of topics. Yes. Number of documents by number of topics, and then the next one is number of topics by number of book items. Yes. And this is actually jumping ahead because we were, right now our R is equal to N, but Jeremy was just pointing out that this matrix is, so the blue matrix is words are the rows, hashtags are the columns, the purple matrix, words are the rows, the columns are, what? Oh, sorry. I thought I said words. OK. Purple matrix, words are the rows, the columns are the topics. The pink matrix V, the rows are the topics, the columns are the hashtags. Oh, sorry. I see what you're saying. Sorry. Facebook example is different by hashtag, I mean document. OK. So let me say it one time with documents. Sorry about that. Purple is words by topics. Pink is topics by post. OK. Don't worry. So we're actually going to return to SBD more later in this lesson, but we're going to kind of take a break from SBD and look at NMF. And so I think it'll be good to revisit SBD later. So there'll be more chance to ask questions about SBD. So NMF stands for non-negative matrix factorization. And some motivation, we can look at decomposition of some faces. And here, kind of the red pixels are showing negative values on the faces. So we kind of have found almost the topic equivalent of face, you know, like what are different components of the face. But how do we interpret this? Like what does it mean to have a negative part of your face, you know, that you can like add these together to form faces? And so this is the motivation for NMF, that with a lot of data sets, having something negative doesn't really make sense and is hard to interpret. And notice in SBD, it was completely possible to have negative values. So here, we're kind of swapping out before, like the key thing with SBD is this orthogonality, you know, assuming that things are orthogonal to each other. Now we're going to have the key thing kind of be we want everything to be non-negative. So NMF is a factorization of a, and your original data set should be non-negative if you're using this, otherwise you won't be able to construct it. Also, if you have negatives in your original data set, the negatives probably make sense in that context. So we're just factoring into two matrices here, W and H, and we want each entry in W and H to be non-negative. And so for this face idea, here if each column of our original matrix was an actual person's face, what we would be capturing would be a matrix of different facial features, and then the relative importance of each of those features in a particular image. And I believe you saw the Eigenfaces data set in Yannett's machine learning class. So NMF is a hard problem. One is that it's kind of under constrained, so you could find different answers, and typically you'll add kind of additional constraints. It's also, it's NP-hard. So coming back to the problem that we're looking at today with topic modeling, so we have our original matrix again, words by documents, and here that'll get decomposed into matrix W that is words for the rows by topics as the columns, and then topics, importance indicators, so here topics would be the rows, and their importance would be the columns. Question? Is this the only way to do the matrix categorization for this one, or is this just a more, what are they approach way to do the double-hatch? I mean here you are breaking the words and document to approximate topics multiplied by the topic importance indicator, right? Is this the only way to do it, or is this the other way to do it? I would say actually neither, so this is an alternative to what we saw with SVD, and I'll talk about some of the pros and cons of NMF versus SVD later on, but I would say they're both valid approaches. I wanted you to kind of see a different way of tackling the problem. But you're labeling your topics and topic importance indicators, your labels, there's nothing mathematical about topics. Oh, right, and that's kind of a common way of interpreting what they mean. And here also with all of these, there's not a clear answer of what the number of topics should be. You know, even something with our data set where we knew we were bringing in these four categories, you know, we've seen that there are kind of multiple subtopics within computer graphics. There's also some overlap between religion and atheism, so it's often not going to be clear even kind of what the quote best dimension to use would be for the number of topics you're looking for. So in H, the number of columns is equal to the number of documents, so it's saying how important is each topic to that document. Right, yeah, so for each document, it's the relative importance of each topic. And you could think of that because when you multiply W and H together for a particular document, you are kind of taking a linear combination of the topics and you want to know what the coefficient is. So Scikit-Learn has a built-in NMF that we'll use first, and that's in the Scikit-Learn decomposition module, which we imported above. And kind of as I mentioned, we're telling it how many components we want, so that's kind of a decision we're having to make. It returns the, when we do kind of our classifier.fitTransform, that will return the W, and then we can get H just kind of stored in this components on our classifier. And this is non-exact, meaning we're not going to get, in most cases, we're not going to get our original matrix back perfectly. We're getting something as close as we can. That's a good question. We will head into that in a moment, if you have different ways to do this. So here we can check, we can check through our topics, makes sense again, looking from the second matrix H, and yes, they seem to be fitting with what we know our categories are. So we've got JPEG, image, GIF, file, color, very reasonable computer graphics topic. The third one is space, launch, satellite, NASA, commercial, reasonable space topic. Okay. I have a section on topic frequency, inverse document frequency, what? Oh, term frequency, inverse document frequency. I'm not going to go super in-depth into this. This is something that's highly, or not highly, but often comes up as a way to kind of normalize your inputs. I tried it, though, with what I was doing and didn't see a significant difference for this particular data set and these decompositions, but I think it's good to be aware of it. So here we kind of want to take into account how often a term appears in a document, how long the document it is, and also how common or rare the term is. So far we've just been dealing with these raw frequencies, which don't really take into account any of that. And so what TF-IDF does instead is it's got this term TF, which is the number of occurrences of a term in document over the number of words in a document. So if a document is super long, we're basically kind of giving relative less weight, as opposed to before, we would have just had a lot of high frequencies showing up. And then the IDF term takes the log of the number of documents divided by the number of documents with the term T in it. And that's a measure of how common or rare a word is. So if a word only shows up in very few documents, it's pretty rare and probably has more significance. So yeah, we won't go too much into this, but I wanted to let you know that it exists. And then I guess we'll break soon. Let me just kind of say non-negative matrix factorization. In summary, the benefits are it's fast and easy to use. The downsides are it took years of research and expertise to create. So this is a version to kind of get back to the question about stochastic gradient descent, a version that was not using stochastic gradient descent, but that is a lot more specific and optimized to NMF. And so it's great, but the people that created it had to have a lot of kind of expertise and knowledge to do so. So we'll look at kind of some alternative ways to calculate NMF ourselves. We'll come back, but let's break and meet back at 12.07. All right. We're going to look at kind of return to non-negative matrix factorization. So before, we were using scikit-learns implementation, which is very specific to the problem of non-negative matrix factorization. And we're going to try writing our own in NumPy. And we're going to use stochastic gradient descent, which is a lot more general. So first I'm going to introduce just kind of the, or remind you of the basic idea of standard gradient descent, which is you choose some weights to start, and then you have a loop that uses your weights to calculate a prediction, calculates the derivative of the loss, and the loss is also known as the error function or the cost. But that's what you're trying to minimize. And then you update the weights, and you kind of keep going through this loop to do that, and you get better and better weights. And the key here is that we're trying to decrease our loss, and the derivative is giving us the direction of steepest descent for our loss. And so for this, I'm going to use the gradient descent. Oh, yes. I'm going to use an Excel notebook. And this is something Jeremy originally developed for the deep learning course. And it's, I think it's very helpful, and it's good. I think many programmers can be kind of snobby about Excel, but Excel is really visual, and it's kind of a good way to see things. So I'm just going to kind of walk through gradient descent in here. Oh. Sorry, I misspoke. I'm going to do an IPython notebook. The Excel is for stochastic gradient descent, which we'll get to next. Although this is also from the deep learning course. And so this notebook is called gradient descent intro. It should also be on GitHub in the class repository. So here we've got a method for a line that takes in A, B, and X and returns A times X plus B. We're going to choose our slope and intercept. This is what typically you don't know. So we're going to kind of choose these to make a fake data set. And then this will let us check kind of how good our method is if we can get back to them. But in the real world, you don't know what the quote true A and B are. And that's what you're trying to figure out. So here we generate 30 random data points. So we've got this array X with 30 points. Y is just A times X plus B. We can plot them. They're perfectly aligned, which is what we would expect since we use the line method to create them. And then here we have a few methods. SSE stands for sum of squared errors. So that will take a kind of true Y and our predicted Y, subtract them and square it and sum it up. That's going to be our loss here. So loss method just calls the sum of squared errors. And then we can get the average loss over. So we have a bunch of points by taking the square root of the loss divided by the number of points that there were. So let's start off guessing negative 1 and 1. So I'm going to actually rerun this. Okay. All right. So our loss is 8.9. So we're not doing great to start. We'll choose what's called a learning rate. So 0.01 is where we're starting. That's something, though, that you'll typically adjust as you're going. And then each time we update, what we'll do is kind of make predictions using our guess for A and our guess for B. We know what the derivative of the loss is since this is a line or sorry, this is the derivative of sum of squared errors. So we're using that derivative. And then we're updating our guesses A and B. And then this is really neat. This is an animation that shows kind of what's happening. And this is a bit, let me stop this. So we're kind of starting down here when we've guessed 1 and 1 for our A and B, which is not a very close approximation at all. And we can see as it runs. So here inside of animate, we just have a for loop that's calling our update method from up here, UPD. And so we can see that the line gets closer and closer until it's a really accurate guess. Are there any questions about this? Oh, down here. Yeah. So the derivative is giving the direction of steepest descent. So we want to subtract that from our guess. The reason we use a learning rate is if this is a large number, we could end up kind of jumping back and forth from the true answer. And so it's kind of better to take smaller steps. And so the learning rate is basically the kind of the step size of like, OK, the derivative gives us the direction we want to go in. And learning rate is telling us how big a step to take in that direction. Any questions? OK, so this is standard gradient descent. There was nothing stochastic about that. And so the idea with SGD is so with standard gradient descent, we evaluated the loss on all of our data. In that example, we only had 30 data points. So it was very quick. In the most problems you'll use, you have way too much data. And it's really slow to evaluate the loss on every single point. And it's also kind of unnecessary, particularly when you're far away from the true answer, to do that much of an evaluation. And so stochastic gradient descent is the idea of evaluating the loss function on just a small sample of the data. And that's typically called a mini batch. And that's why it's stochastic, because depending kind of which batch you choose to or mini batch you calculate your loss on, you'll get different answers. But it turns out that kind of an aggregate, because this is part of a loop, it's good enough and you get a huge, huge improvement in speed. And so this is what I was going to show inside the Excel spreadsheet. And this one will start in a very similar way. So we're just in this Excel spreadsheet is also available in the GitHub repository. And there are tools like OpenOffice that you can kind of look at the spreadsheet without having Excel. So we've just chosen kind of an A and B to generate our data from. And this is helpful because it lets us see how good our answer is. But this is kind of the part where you would typically have a real data set. So I've got our data and then the basic SGD. We're going to make some guesses for what the intercept and slope are. Here's this learning rate, which is what we're going to multiply by to figure out how big a step to take. Our data has been copied in. And here are many batches of size one. So we're going to just calculate what the derivative of the loss is. So here's our prediction. Here's the error. And then we calculate the derivative of the error just at that single point and then update A and B for that. And so we can see this. You might want to increase the learning rate. This is going pretty slow. So the error is getting smaller. Oh, no, that's horrible. Okay, I won't do this on. I won't improvise this right now. But you can adjust the learning rate to take smaller, bigger steps. If you take too big a step, this is what can happen to you and you shoot off in the wrong direction. There are questions about stochastic gradient descent. And then this notebook includes a bunch of other optimization techniques. We won't be covering them in this class, but they might be of interest to you if you're interested in optimization. On the spreadsheet, you don't need to use the macros or the buttons. It's all in the spreadsheet. And the actual random macro is just automatically copying and pasting and calculating. So you can do it all that way. Yeah, that's a great point. I'm going to reset this. You want me to reset this? What this is do? Let me just run. Yeah. So as Jeremy was kind of saying, what this does is it takes the new A and B down from the very bottom and is what the macro does and then puts them, I guess, back up here for. For intercept and slope. Yeah, so you can see that's kind of picking off the column C. And so I would say stochastic gradient descent is a really, really general approach. It's something that you can apply to a lot of different problems. Anytime you have a derivative for your loss. And in fact, you actually don't even need to have a formula form for it. So I think it's something that's really useful, useful to know. And I've linked to a few few resources, including the SD like SDD lecture from Andrew Ingres Coursera class from the fast. I wiki. This is kind of a nice blog post on a variety of optimization algorithms. But so applying that to our specific problem of NMF, we're trying to decompose V into the product of two matrices where all their entries are non negative. So we're minimizing the Frobenius norm of V minus W H. We want to get that as close to zero as possible, because that would be in the W H is a good approximation of V. And then we really want W and H greater than or equal to zero. So in order to use SDD, we need to know the gradient of the loss function. And here we've looked that up and are just going to kind of use it. This is also something you could calculate if you like multivariable derivatives, multivariable calculus. So we've got. Kind of our vectors from before. This is still again the same matrix with the words as rows and the columns or documents. Down here we define the gradients. It takes in M W and H and it returns the gradient. We have the this is the gradient from W. And then we also have this penalty term and the penalty is going to be and this is for W or kind of for each of our matrices. So we want to have a penalty when W and H are negative. And that's what's happening here. So we call penalty, which has got this NP dot where if the matrix is greater than we're going to choose you is very close to zero. If the matrix is greater than you, there's zero penalty. We're happy because it's positive. However, if it's negative or super close to zero, we're assigning this penalty. And then we update W and H by calculating their gradients, which includes the penalty. And then just just like what happened in the kind of simple version with the line in the notebook, we do the learning rate, which is our step size times the gradient. And then just just like what happened in the kind of simple version with the line in the notebook, we do the learning rate, which is our step size times the gradient. This is just so that we can get a sense of how we're doing as we go along. So the first term is showing M minus W times H. So just kind of seeing how far apart they are, you know, is W H a good approximation of M. Then we're going to look at the minimum of W and the minimum of H. That's something where I don't know if we got like negative a thousand, that would be a really bad sign. Ideally, those would be zero. And then also the sum of how many of the terms are less than zero. Any questions so far about these methods we're defining? Sure. Yeah. So the because we kind of have two separate things we're trying to do. One, we want W times H to be really close to M, a good approximation. And the second is we want W and H to be positive. So we have these two separate goals and taking the derivative of the loss will help us get W times H close to M because that's kind of the particular derivative we've we've calculated. And so that's what's going on with this kind of this first part. So that's that for W. This is that for H. Then to deal with wanting W and H to be greater than zero. Here we're deciding, OK, we're going to have this penalty that is equal to zero if they're positive because we're happy. So no penalty. And that's what NP dot where does. NP dot where kind of takes a truth, true or false statement. If it's true, it uses the first value. So if W is greater than or equal to this tiny number that's close to zero, we go with the first value zero. There's no penalty. However, if W is less than you, the penalty is going to be larger for the further away that M is from you. So basically SGD can't do constrained optimization, but we have constraints. So we're taking it. People who spend their lives studying constrained optimization and can horrify it. As Rachel shows, it actually has some nice features. And you can think about like another way to think about it is there's not a clear or obvious way to take a derivative from OK, we want these things to be greater than zero. Whereas for having this M close to W times H, it's much easier to be like, OK, this is how far away they are. We can take a derivative of that. That's a good question. So here this will be negative. What we're doing with our W and H is. We're doing W minus equals DW. So that would come back as. So this is being returned in the grads method. We have like minus a negative is positive. But that's a good point. The issue is we could flip the signs on everything, but then you would also have to flip the signs on the derivative here. Yeah, but the. Minus a negative, it basically means the further the more negative it is, the bigger a positive number you add to W. OK, these are great questions. Thanks everyone. Yeah, so now to kind of start it, we need to choose random values for W and H. So we'll just use kind of a random normal to get back matrices of the size that we want. And then we're taking the absolute value of that because we should at least start with non zero or non negative matrices. And so we'll just kind of check the initial. And as a reminder, this report and this is just a few values that we thought it would be interesting to kind of monitor as we go along to see how we're doing. So this is how far away M and WHR is the first one. The second is the minimum of W, the minimum of H, and then the sum of their negative terms. Oh, the count of notes. Oh, OK, great. Count of their negative terms. So then we run update, which just calculates the gradients, updates them once. And we can check the the error has improved a little bit. So this is slightly smaller, 44.419 now as opposed to 44.439 before. Here we have introduced some negatives into W and H. And if we run this, and I'm actually going to do this for fewer. So one, as this runs, you'll notice it's a little bit slow. So we see that the product WH is getting closer to our matrix M. That's the first column is going down here. The negative terms actually kind of seem like they're saying about the same. I didn't do that many iterations. Anything the count is going up. So kind of a key thing here to note is that this is really slow and I ran this for longer and it continued to be very slow. And there's also a lot of parameter fiddling, because like right now, looking at this, I'm thinking I would probably want to increase the penalty on negative values just because these counts seem really high and like they're not going down. Particularly, well, I guess H is much bigger than W. So we have like a lot of different parameters. So even though we have this generic method that mostly seems headed in the right direction, it does have some shortcomings. Sorry about that. All right. So one way we could speed this up is to use PyTorch. PyTorch is a Python framework for dynamic neural networks with GPU acceleration. It was just released in January and many of the core contributors are on Facebook's AI team and it's used at Facebook as well as many other companies. Twitter is using it. But it actually has two purposes. So kind of in addition to being a deep learn and I think it's a really excellent deep learning framework. It's also a replacement for NumPy that uses the GPU. And a lot of the right now it has less functionality than NumPy just because it's so new, but a lot of people are actively working on it. And the methods it has are very similar to what NumPy has. So I have a link. So we'll kind of just be using it in this lesson. But it's something I really wanted to expose you to. And it's something that if you want you can use for your project. You could try to implement something else in PyTorch because I think it's a really, really nice framework. And I have a few links to find out more. So you do not have to be using a GPU for this course. That's fine. You will not get the speed up of running stuff on a GPU. Just in the code below here, whenever you see a.cuda, you'll need to delete that if you don't have a GPU. Oh, and so if you're interested in getting a GPU, well, one, so the MSAN program has a box with a few GPUs that people can share. And then something else that's great is AWS instances. And you can watch this setup lesson. This was from the deep learning course that kind of walks through how to request P2 from AWS. And what that does is it's letting you spin up a kind of computer in the cloud that has GPU capabilities. Yeah, and definitely feel free to ask if you want kind of more help or advice about that. But this is kind of thinking about this course of like the general goal of making matrix computations faster. Using a GPU is a great way to do that. I mean, we'll see in a moment. Oh, do you have a ballpark you want to throw out? Yes, significantly faster. So we have to do some imports from I guess also so PyTorch is in it's a Python library, but it's Torch is a Lua library that's been used in computer graphics for years. So it's even though PyTorch is newly released, it's kind of coming from this really well developed library that grew up around computer graphics, which have to do very fast matrix computations. We're also like really all indebted to the video gaming industry for kind of keeping GPU technology advancing so much. So something you'll notice with with PyTorch is well, I'll come back to this. So in PyTorch you have tensors and variables and they kind of have the same API meaning you can do the same things to them. However, variables remember or keep track of how they were created, which will be necessary later on. We'll find out about that. So we've just converted our vectors to dense V, then we're casting them. So V as type, np.float32 and putting that into a Torch tensor. Remember a tensor is just kind of a generalization of a matrix. And so we had to specify our type. And this is something that you'll see with a lot of these options to speed things up. They need to know the type. So here we kind of redefine the methods that we had above. And you'll notice that they're fairly similar. A key thing that's different, we have these dot mm for matrix multiply. So now this w dot mm h that's just doing w times h. Let me see what other differences jump out. Torch dot clamp is a method that takes a value and then we'll cut it off with a kind of minimum and or maximum that you feed in. So here we want m minus mu, but we're cutting it off at zero because if m is large and positive, we're happy. So it doesn't have to be large. If m is positive and bigger than u, we don't want any penalty. So that max zero will take effect and there's zero penalty. So this is handling that penalty from before. So here the dot sub underscore, that subtraction and it's a minus equals subtraction from the original. And this, you don't have to understand all the syntactic details of PyTorch, but I wanted you to get a general general feel for what's going on. So we need to kind of declare our float tensors and said this last time, but a float is basically a decimal as opposed to an integer. When we talk about data types. Again, this is in a little bit of a different order, but we're doing the same thing. W is being initialized with a normal random variable and then we're doing dot apps to do the absolute at the end. But that's kind of just saying the order that these happen in. Take the absolute value. Yes. Great. Thank you. I guess these are probably all still saved in here. So now you can see this is going much faster than before. And the one above, I actually probably should have run for. Yeah, so this one, each loop was only 10. I only did 50 total and that felt kind of slow. Now each loop is 100 and we're doing 1000 and that was 1000 iterations. That was much quicker. And so you see that these have improved. You still are going to need to do some more, but we're able to kind of get through a lot of iterations much quicker. Any questions? But this is as good as it gets. I thought we got better. Then here are the topics that we found. So objective, morality, values, moral, subjective, science, absolute, claim. I guess that maybe that's atheism. God, Jesus, Bible, believe, atheism, Christian, perhaps religion, miscellaneous, space, NASA, launch, shuttle, orbit, lunar, moon is a good space one. So these are some nice topics. So Jeremy was referencing what we got from the built in. So these are the topics from the built in. And again, since this is an unsupervised learning problem, we don't have an absolute measure of like, these are what great topics are. We're just kind of using our intuition about looking at them. Well, I mean, that's giving you how good your approximation is. Yeah, so these are these are good topics. Okay, so in that version, or actually I should ask, are there any other questions about this version? Yes. Can you pass the microphone? You can still use it, but you're not getting the speed up. So there's less, less reason to. I mean, you could still question of deep learning aside. If you're using it as a NumPy alternative, kind of the main motivation for that is the GPU speed up, but it will still work. Yes. Yeah. And also when you're using PyTorch, if you do have a GPU, you need to be sure to use.Cuda. Otherwise PyTorch will not put your stuff on the GPU, which is what you want to get the speed up. So you have to tell it to do that. And you will have to install PyTorch to be able to do this. But yeah, I recommend using Anaconda. So this version, note, so this was very similar to what we just did in what we had done in NumPy from scratch. We still had to know what the gradients were. And this can really be a pain, particularly if you have a more complicated method having to calculate the derivative. And so PyTorch has something really handy called AutoGrad. And AutoGrad will do automatic differentiation. So if you give it a method, it finds the derivative for you. And so this is great because you can use it on stuff where you don't even know what the derivative is. And so the approach that I'm about to show would work for, I think, almost any optimization problem. And so here we're going to have to use variables for this. And with so again, kind of the variable has the same API as a tensor, but it's got this memory of how it's how it's created, which is then can be used to automatically calculate the derivative. And you'll want to pass requires grad equals true. And so you're saying this this needs a gradient. So here we create calling them now Pw and Ph. And so then down here with our methods, notice we no longer have a grads method because we're not having to give it a formula for the gradient. We do still want a penalty term. So that's kind of taking our matrix, checking that it's less than zero. Or sorry, if it's if it's less than zero, we're taking its value. Otherwise, the value is zero or yeah, we're maxing it at zero. And then in this case, we ended up squaring that to add. And I we probably should have done this in the methods above to add a greater penalty because you would see above we were still kind of getting a lot more negative numbers than we would like. The reason we didn't do it before is because it kept making the gradient. Yeah, you can have more complicated formulas without the downside of knowing that you'll need their their gradient. Let's see. So report is basically the same. Yeah, again, we're having to use the dot CUDA to tell Pytorch put this on the GPU. And then here we're going to use an optimizer. So this is we're kind of telling it. This is how I want you to optimize things. Adam is I think from a few years ago, it's a relative it's a modern optimizer as opposed to, you know, HDD, which is kind of this more classic, classic technique. Give a learning rate. We can still get our report. And so so this this code here, this for loop is really useful. This is something that you could use with a lot of different problems. We just. Pytorch requires you to set your gradients to zero kind of manually with your optimizer. So our optimizer was saying the gradients are zero. We calculate the loss. This is a Pytorch method backward and kind of taking a backwards. That calculates your gradient. So it's kind of like we. Actually, this. Kind of surprises me because we're doing our optimizer step. I don't think you would want backwards afterwards now. So L equals loss calculates the value of the loss. L is now a variable, which is remembered how it is calculated. L.backward calculates all of the gradients. And so now that we have the gradients. Ah, OK. Yeah, Adam is stochastic, but it's a much fancier version. And there is a version of Adam in that Excel spreadsheet if you wanted to look at the details. And again, we're going to print out the report every every hundred steps. The other thing we're doing is every hundred steps were decreasing our learning rate. This is called learning rate annealing. But you see inside this nested loop, we're doing learning rate times equals 90 percent. So it's getting a little bit smaller each each hundred steps. And so, again, you want to calculate what your loss is, get the gradients and then use that to take a step. But note that these backward and step are kind of built in methods you're getting from PyTorch. We can check and we still have great topics. So space, NASA, shuttle, launch orbit, lunar, moon data. Well, we do have this one one random topic that we've been seeing. We saw with SVD as well. Yes. So with with. So the idea of decreasing it as you go is that you're getting kind of as you get more in the neighborhood of where you want to be. You want to take smaller steps so you don't overstep where you're going with stochastic gradient descent. Remember that your your directions are less accurate since you've just used a subset of your data. So there's kind of more of a risk of overstepping in the wrong direction. And it's OK that you know you're you're always going in slightly wrong direction, but it's close enough. Actually, let me. Yes, learning rate annealing. Clear the page. I was just going to show that. Like if this was the true direction of that, you want it to be going in with stochastic gradient descent. And what you're doing. Is you're going close to to that direction, but you're kind of zigzagging a bit around. And so you probably want your zigzags to get like a little bit. And a tighter here the as you start getting close to your goal. Yeah, and as Jeremy said, that's learning rate annealing. So now I want to kind of compare some of the approaches we've taken to talk about. Or first, are there any other questions on on pie torch or kind of what we've done with this automatic differentiation? OK, so using psych hits like it learns built in and math. And that was fast. We didn't have to deal with tuning parameters, which was nice. So, you know, in these later ones, there was this question of do we need to adjust our learning rate? Do we need to adjust kind of how much of a penalty we're putting on the negatives? But it used a lot of academic research, kind of the people that implemented it. And this is interesting. This is from another library. Specifically, a Python library for non negative matrix factorization. But it's really kind of neat. They show a list of like variations on NMF and like relative research. And this is still an area where a lot of research is happening and there's a lot going on. And so. Unless you want to specialize in NMF, this may be more detailed than you would want to go into to be able to build something like this. Using pie towards an SDD, it was nice that it was much quicker to implement. We didn't have to be kind of particular experts on NMF. The parameters were kind of fiddly and it was also not as fast as scikit learns built in one. But we did see that kind of going from numpy on this should say. Num by. Was slow enough that we wanted to switch to pie torch to get the improvement on the GPO. Questions about this? Oh, yes. Actually, hold on a moment. So in pie torch, looking at the loss function you called subtracted your variables. And so by actually just like doing the operation on variables, is that what is like calculating the gradient on that? And then we just call step or backwards and then it gives us the gradient. So yes, a backward is where it's calculating the gradient. Yeah. Good question. Okay, let's see. I'll start the next section, although we'll have to revisit this next time. Yeah, so that was that that was NMF. Now we're going to return to SVD. So we saw that something that we were doing in NMF that we weren't doing before with SVD is we were choosing D kind of a number of topics that we wanted. And so going back to these matrices. So we had this matrix that is words by documents. Actually, let me go back up to the NMF one. The picture that is up here. Okay, this picture. So words by documents. Then we're getting a matrix that's words by topics. And then this is topics by the importance indicators for each document. So it's basically topics by document. And we were getting to choose how many topics there were. And so we can get that with SVD as well. And it's called truncated SVD. So instead of getting this exact decomposition, we could just choose. I'm going to have R smaller than N. And that's the number of topics that I'm interested in. And remember that the singular values, which are the diagonal values in this matrix, sigma in the middle, that those are ordered from largest to smallest. And the larger ones kind of are contributing a lot more to this reconstruction of A. So we're going to keep the larger ones. And this is called truncated SVD. And then I also want to, this picture I've taken is from this blog post on the Facebook research page on fast randomized SVD. And we're going to be talking about some of the ideas in it below, but I encourage you to check that out. So some of the shortcomings for classical algorithms are matrices are really large. Data is often missing or slightly inaccurate. Why spend extra computational resources when kind of, if your data was not that precise to begin with, you don't need an exact solution because it's not a, you know, you have these errors in your input. Data transfer now plays a major role in the time of algorithms. So we talked about this last time that traditionally this idea of computational complexity or big O has been how speed is measured. And it's still definitely an important concept. But in practice, a lot of time is spent taking things kind of from disk or even from RAM into your cache or registers. And you really want to take advantage of that. And so techniques that require fewer passes over the data can be substantially faster, even if they technically have more steps in them. We also want to be able to take advantage of GPUs. And then a lot of the methods that are used on sparse or structured matrices are unstable. And we'll kind of be getting to Krylov subspace methods later. But the computational cost ends up being kind of more about what you're doing to stabilize your algorithm as opposed to kind of the actual algorithm you're doing itself. And these, actually I'm going to highlight a paper that we will be seeing a fair amount of. And that is a really nice paper is Halco, Finding Structure with Randomness. And it's about kind of using probabilistic techniques. Yes. So you said sparse matrix operations are difficult. Is that because you have them in a sparse format or are you talking about a dense matrix that has sparse variables? That's a good question. So it's this particular family of methods, Krylov subspace methods, which are really useful. Well, I guess, okay, so there are issues of when sparse things are kind of becoming dense. I think this is less about the sparsity and kind of more about the methods that are being used introduce this instability. We'll see examples of Krylov subspace methods later. But yeah, it's not so much having to convert between and actually working. Well, depending what you're doing, working with sparse, there are kind of like implementations that handle sparse data format very efficiently. So it's not a problem to be storing things sparsely. Yeah, so I just want to kind of introduce this idea of randomized algorithms. Maybe as they're kind of more stable, their performance guarantees don't depend on kind of these matrix properties. So the spectral properties of a matrix are kind of based on what the singular values are. And then a lot of matrix vector products can be done in parallel. And so we're about at time. I'll come back to these ideas on Tuesday. And so we will be using a randomized algorithm to calculate the truncated SPD more efficiently. Great. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.0, "text": " We'll be starting with notebook two.", "tokens": [492, 603, 312, 2891, 365, 21060, 732, 13], "temperature": 0.0, "avg_logprob": -0.3542133780086742, "compression_ratio": 1.074468085106383, "no_speech_prob": 0.010324153117835522}, {"id": 1, "seek": 0, "start": 5.0, "end": 20.0, "text": " Sorry, I'm getting over a cold. Topic modeling with NMF and SVD.", "tokens": [4919, 11, 286, 478, 1242, 670, 257, 3554, 13, 8840, 299, 15983, 365, 426, 44, 37, 293, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.3542133780086742, "compression_ratio": 1.074468085106383, "no_speech_prob": 0.010324153117835522}, {"id": 2, "seek": 2000, "start": 20.0, "end": 31.0, "text": " Okay, so I briefly showed this diagram last time of works of Shakespeare. There's different plays going across the top of this matrix.", "tokens": [1033, 11, 370, 286, 10515, 4712, 341, 10686, 1036, 565, 295, 1985, 295, 22825, 13, 821, 311, 819, 5749, 516, 2108, 264, 1192, 295, 341, 8141, 13], "temperature": 0.0, "avg_logprob": -0.0806960416643807, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.00016325728211086243}, {"id": 3, "seek": 2000, "start": 31.0, "end": 39.0, "text": " And then the rows are different vocabulary words. And this is a way that you can represent a body of work in a single matrix.", "tokens": [400, 550, 264, 13241, 366, 819, 19864, 2283, 13, 400, 341, 307, 257, 636, 300, 291, 393, 2906, 257, 1772, 295, 589, 294, 257, 2167, 8141, 13], "temperature": 0.0, "avg_logprob": -0.0806960416643807, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.00016325728211086243}, {"id": 4, "seek": 2000, "start": 39.0, "end": 47.0, "text": " And this is a type of bag of words approach because we're not looking at any of the syntax or the structure of the sentences.", "tokens": [400, 341, 307, 257, 2010, 295, 3411, 295, 2283, 3109, 570, 321, 434, 406, 1237, 412, 604, 295, 264, 28431, 420, 264, 3877, 295, 264, 16579, 13], "temperature": 0.0, "avg_logprob": -0.0806960416643807, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.00016325728211086243}, {"id": 5, "seek": 4700, "start": 47.0, "end": 52.0, "text": " We're really just interested in the frequency of the words that show up with them.", "tokens": [492, 434, 534, 445, 3102, 294, 264, 7893, 295, 264, 2283, 300, 855, 493, 365, 552, 13], "temperature": 0.0, "avg_logprob": -0.08746655603473107, "compression_ratio": 1.6962025316455696, "no_speech_prob": 4.7565688873874024e-05}, {"id": 6, "seek": 4700, "start": 52.0, "end": 60.0, "text": " And so today we'll be talking about two different ways to decompose this into a tall thin matrix times a wide short matrix.", "tokens": [400, 370, 965, 321, 603, 312, 1417, 466, 732, 819, 2098, 281, 22867, 541, 341, 666, 257, 6764, 5862, 8141, 1413, 257, 4874, 2099, 8141, 13], "temperature": 0.0, "avg_logprob": -0.08746655603473107, "compression_ratio": 1.6962025316455696, "no_speech_prob": 4.7565688873874024e-05}, {"id": 7, "seek": 4700, "start": 60.0, "end": 76.0, "text": " And something I want to emphasize is the reason that we like kind of one reason we like matrix decompositions is that the matrices we're decomposing into have special properties that are helpful.", "tokens": [400, 746, 286, 528, 281, 16078, 307, 264, 1778, 300, 321, 411, 733, 295, 472, 1778, 321, 411, 8141, 22867, 329, 2451, 307, 300, 264, 32284, 321, 434, 22867, 6110, 666, 362, 2121, 7221, 300, 366, 4961, 13], "temperature": 0.0, "avg_logprob": -0.08746655603473107, "compression_ratio": 1.6962025316455696, "no_speech_prob": 4.7565688873874024e-05}, {"id": 8, "seek": 7600, "start": 76.0, "end": 88.0, "text": " So I wanted to start kind of just hypothetically thinking about an extreme case where suppose you wanted to reconstruct a matrix just using an outer product of two vectors.", "tokens": [407, 286, 1415, 281, 722, 733, 295, 445, 24371, 22652, 1953, 466, 364, 8084, 1389, 689, 7297, 291, 1415, 281, 31499, 257, 8141, 445, 1228, 364, 10847, 1674, 295, 732, 18875, 13], "temperature": 0.0, "avg_logprob": -0.08333882225884331, "compression_ratio": 1.408450704225352, "no_speech_prob": 1.723012246657163e-05}, {"id": 9, "seek": 7600, "start": 88.0, "end": 94.0, "text": " Let me switch over to that.", "tokens": [961, 385, 3679, 670, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.08333882225884331, "compression_ratio": 1.408450704225352, "no_speech_prob": 1.723012246657163e-05}, {"id": 10, "seek": 9400, "start": 94.0, "end": 108.0, "text": " So if you had kind of your words are still the rows, the documents are the columns.", "tokens": [407, 498, 291, 632, 733, 295, 428, 2283, 366, 920, 264, 13241, 11, 264, 8512, 366, 264, 13766, 13], "temperature": 0.0, "avg_logprob": -0.10553772944324422, "compression_ratio": 1.4503311258278146, "no_speech_prob": 8.267458724731114e-06}, {"id": 11, "seek": 9400, "start": 108.0, "end": 114.0, "text": " And as motivation, you could think of just doing this outer product of two vectors. This is not going to be a very good reconstruction.", "tokens": [400, 382, 12335, 11, 291, 727, 519, 295, 445, 884, 341, 10847, 1674, 295, 732, 18875, 13, 639, 307, 406, 516, 281, 312, 257, 588, 665, 31565, 13], "temperature": 0.0, "avg_logprob": -0.10553772944324422, "compression_ratio": 1.4503311258278146, "no_speech_prob": 8.267458724731114e-06}, {"id": 12, "seek": 11400, "start": 114.0, "end": 135.0, "text": " So actually I should put that as a very approximate. But the best you could do with this would be to have kind of this first vector be kind of the relative frequencies of each vocabulary word compared to out of the total word count.", "tokens": [407, 767, 286, 820, 829, 300, 382, 257, 588, 30874, 13, 583, 264, 1151, 291, 727, 360, 365, 341, 576, 312, 281, 362, 733, 295, 341, 700, 8062, 312, 733, 295, 264, 4972, 20250, 295, 1184, 19864, 1349, 5347, 281, 484, 295, 264, 3217, 1349, 1207, 13], "temperature": 0.0, "avg_logprob": -0.07263498457651289, "compression_ratio": 1.5852272727272727, "no_speech_prob": 3.288561174485949e-06}, {"id": 13, "seek": 11400, "start": 135.0, "end": 140.0, "text": " And then this could be the words per document.", "tokens": [400, 550, 341, 727, 312, 264, 2283, 680, 4166, 13], "temperature": 0.0, "avg_logprob": -0.07263498457651289, "compression_ratio": 1.5852272727272727, "no_speech_prob": 3.288561174485949e-06}, {"id": 14, "seek": 14000, "start": 140.0, "end": 146.0, "text": " And so this would really just kind of reconstruct a very averaged version.", "tokens": [400, 370, 341, 576, 534, 445, 733, 295, 31499, 257, 588, 18247, 2980, 3037, 13], "temperature": 0.0, "avg_logprob": -0.09169481314864814, "compression_ratio": 1.4963503649635037, "no_speech_prob": 1.0952302545774728e-05}, {"id": 15, "seek": 14000, "start": 146.0, "end": 156.0, "text": " If you wanted to kind of extend this to the next simplest case, that would be to let this be two columns and this one be two rows.", "tokens": [759, 291, 1415, 281, 733, 295, 10101, 341, 281, 264, 958, 22811, 1389, 11, 300, 576, 312, 281, 718, 341, 312, 732, 13766, 293, 341, 472, 312, 732, 13241, 13], "temperature": 0.0, "avg_logprob": -0.09169481314864814, "compression_ratio": 1.4963503649635037, "no_speech_prob": 1.0952302545774728e-05}, {"id": 16, "seek": 15600, "start": 156.0, "end": 178.0, "text": " And if you did that, what you would kind of be wanting to do is have your documents in two different clusters. And so you could be capturing the relative frequency of words for documents in one cluster with that first, you know, tall skinny matrix and the relative kind of words per document with the two.", "tokens": [400, 498, 291, 630, 300, 11, 437, 291, 576, 733, 295, 312, 7935, 281, 360, 307, 362, 428, 8512, 294, 732, 819, 23313, 13, 400, 370, 291, 727, 312, 23384, 264, 4972, 7893, 295, 2283, 337, 8512, 294, 472, 13630, 365, 300, 700, 11, 291, 458, 11, 6764, 25193, 8141, 293, 264, 4972, 733, 295, 2283, 680, 4166, 365, 264, 732, 13], "temperature": 0.0, "avg_logprob": -0.07743882410454028, "compression_ratio": 1.6944444444444444, "no_speech_prob": 8.314966066791385e-07}, {"id": 17, "seek": 17800, "start": 178.0, "end": 191.0, "text": " So this is just kind of some motivation of kind of thinking about these very simple cases.", "tokens": [407, 341, 307, 445, 733, 295, 512, 12335, 295, 733, 295, 1953, 466, 613, 588, 2199, 3331, 13], "temperature": 0.0, "avg_logprob": -0.09570167405264718, "compression_ratio": 1.4585365853658536, "no_speech_prob": 4.222421011945698e-06}, {"id": 18, "seek": 17800, "start": 191.0, "end": 204.0, "text": " Yeah, so today will be actually I wanted to show. So here I've just got my import statements. And again, let me know if you have any problems getting your imports working or Jupyter Notebook running properly.", "tokens": [865, 11, 370, 965, 486, 312, 767, 286, 1415, 281, 855, 13, 407, 510, 286, 600, 445, 658, 452, 974, 12363, 13, 400, 797, 11, 718, 385, 458, 498, 291, 362, 604, 2740, 1242, 428, 41596, 1364, 420, 22125, 88, 391, 11633, 2939, 2614, 6108, 13], "temperature": 0.0, "avg_logprob": -0.09570167405264718, "compression_ratio": 1.4585365853658536, "no_speech_prob": 4.222421011945698e-06}, {"id": 19, "seek": 20400, "start": 204.0, "end": 213.0, "text": " I wanted to highlight. So I've linked to a few different resources that I thought were helpful. I particularly I almost used it in this class, but didn't.", "tokens": [286, 1415, 281, 5078, 13, 407, 286, 600, 9408, 281, 257, 1326, 819, 3593, 300, 286, 1194, 645, 4961, 13, 286, 4098, 286, 1920, 1143, 309, 294, 341, 1508, 11, 457, 994, 380, 13], "temperature": 0.0, "avg_logprob": -0.10959271022251674, "compression_ratio": 1.4352941176470588, "no_speech_prob": 5.254677034827182e-06}, {"id": 20, "seek": 20400, "start": 213.0, "end": 224.0, "text": " And there's this really nice called text analysis for the humanities and social sciences.", "tokens": [400, 456, 311, 341, 534, 1481, 1219, 2487, 5215, 337, 264, 36140, 293, 2093, 17677, 13], "temperature": 0.0, "avg_logprob": -0.10959271022251674, "compression_ratio": 1.4352941176470588, "no_speech_prob": 5.254677034827182e-06}, {"id": 21, "seek": 22400, "start": 224.0, "end": 235.0, "text": " And it's a pretty lengthy tutorial that kind of walks through using French and British literature. So it's got Victor Hugo and Jane Austen doing a word analysis.", "tokens": [400, 309, 311, 257, 1238, 35374, 7073, 300, 733, 295, 12896, 807, 1228, 5522, 293, 6221, 10394, 13, 407, 309, 311, 658, 15777, 32504, 293, 13048, 4126, 268, 884, 257, 1349, 5215, 13], "temperature": 0.0, "avg_logprob": -0.07612910346379356, "compression_ratio": 1.4270833333333333, "no_speech_prob": 5.173368663236033e-06}, {"id": 22, "seek": 22400, "start": 235.0, "end": 246.0, "text": " They use some different techniques. But if this is a topic that interests you, you might want to check that out.", "tokens": [814, 764, 512, 819, 7512, 13, 583, 498, 341, 307, 257, 4829, 300, 8847, 291, 11, 291, 1062, 528, 281, 1520, 300, 484, 13], "temperature": 0.0, "avg_logprob": -0.07612910346379356, "compression_ratio": 1.4270833333333333, "no_speech_prob": 5.173368663236033e-06}, {"id": 23, "seek": 24600, "start": 246.0, "end": 254.0, "text": " So we'll be using a built in data set from scikit learn today. I mentioned last time scikit learn has a lot of different built in data sets.", "tokens": [407, 321, 603, 312, 1228, 257, 3094, 294, 1412, 992, 490, 2180, 22681, 1466, 965, 13, 286, 2835, 1036, 565, 2180, 22681, 1466, 575, 257, 688, 295, 819, 3094, 294, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.11105025580169958, "compression_ratio": 1.7352941176470589, "no_speech_prob": 3.425077011343092e-05}, {"id": 24, "seek": 24600, "start": 254.0, "end": 265.0, "text": " And this is newsgroups. So newsgroups. This is kind of before the World Wide Web took off. These were kind of discussion boards and they're still in existence.", "tokens": [400, 341, 307, 2583, 17377, 82, 13, 407, 2583, 17377, 82, 13, 639, 307, 733, 295, 949, 264, 3937, 42543, 9573, 1890, 766, 13, 1981, 645, 733, 295, 5017, 13293, 293, 436, 434, 920, 294, 9123, 13], "temperature": 0.0, "avg_logprob": -0.11105025580169958, "compression_ratio": 1.7352941176470589, "no_speech_prob": 3.425077011343092e-05}, {"id": 25, "seek": 24600, "start": 265.0, "end": 275.0, "text": " People are talking about different topics. So you can kind of think of this as similar to something like Reddit, where you have people posting within different categories.", "tokens": [3432, 366, 1417, 466, 819, 8378, 13, 407, 291, 393, 733, 295, 519, 295, 341, 382, 2531, 281, 746, 411, 32210, 11, 689, 291, 362, 561, 15978, 1951, 819, 10479, 13], "temperature": 0.0, "avg_logprob": -0.11105025580169958, "compression_ratio": 1.7352941176470589, "no_speech_prob": 3.425077011343092e-05}, {"id": 26, "seek": 27500, "start": 275.0, "end": 281.0, "text": " And so we're using this kind of functionality from scikit learn to fetch the newsgroups data.", "tokens": [400, 370, 321, 434, 1228, 341, 733, 295, 14980, 490, 2180, 22681, 1466, 281, 23673, 264, 2583, 17377, 82, 1412, 13], "temperature": 0.0, "avg_logprob": -0.07234256346147139, "compression_ratio": 1.6455696202531647, "no_speech_prob": 7.138799264794216e-05}, {"id": 27, "seek": 27500, "start": 281.0, "end": 289.0, "text": " And you can pass a parameter to say which categories you want. Today we're just going to look at four categories to keep it simpler.", "tokens": [400, 291, 393, 1320, 257, 13075, 281, 584, 597, 10479, 291, 528, 13, 2692, 321, 434, 445, 516, 281, 574, 412, 1451, 10479, 281, 1066, 309, 18587, 13], "temperature": 0.0, "avg_logprob": -0.07234256346147139, "compression_ratio": 1.6455696202531647, "no_speech_prob": 7.138799264794216e-05}, {"id": 28, "seek": 27500, "start": 289.0, "end": 300.0, "text": " And something to note is we're getting the categories because it'll be helpful for us. But what we're doing is unstructured or sorry unsupervised machine learning.", "tokens": [400, 746, 281, 3637, 307, 321, 434, 1242, 264, 10479, 570, 309, 603, 312, 4961, 337, 505, 13, 583, 437, 321, 434, 884, 307, 18799, 46847, 420, 2597, 2693, 12879, 24420, 3479, 2539, 13], "temperature": 0.0, "avg_logprob": -0.07234256346147139, "compression_ratio": 1.6455696202531647, "no_speech_prob": 7.138799264794216e-05}, {"id": 29, "seek": 30000, "start": 300.0, "end": 307.0, "text": " And so we're not actually using the categories. We're kind of going to be thinking about like, have we clustered these into topics that make sense?", "tokens": [400, 370, 321, 434, 406, 767, 1228, 264, 10479, 13, 492, 434, 733, 295, 516, 281, 312, 1953, 466, 411, 11, 362, 321, 596, 38624, 613, 666, 8378, 300, 652, 2020, 30], "temperature": 0.0, "avg_logprob": -0.0711360216140747, "compression_ratio": 1.6099585062240664, "no_speech_prob": 6.7473370108928066e-06}, {"id": 30, "seek": 30000, "start": 307.0, "end": 316.0, "text": " And we'll check with the categories to confirm that. So whenever you get data, it's great to just check what the dimension of it is.", "tokens": [400, 321, 603, 1520, 365, 264, 10479, 281, 9064, 300, 13, 407, 5699, 291, 483, 1412, 11, 309, 311, 869, 281, 445, 1520, 437, 264, 10139, 295, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.0711360216140747, "compression_ratio": 1.6099585062240664, "no_speech_prob": 6.7473370108928066e-06}, {"id": 31, "seek": 30000, "start": 316.0, "end": 324.0, "text": " So here that's a 2034. So that's how many kind of separate posts we have. We'll just look at a few of them.", "tokens": [407, 510, 300, 311, 257, 945, 12249, 13, 407, 300, 311, 577, 867, 733, 295, 4994, 12300, 321, 362, 13, 492, 603, 445, 574, 412, 257, 1326, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.0711360216140747, "compression_ratio": 1.6099585062240664, "no_speech_prob": 6.7473370108928066e-06}, {"id": 32, "seek": 32400, "start": 324.0, "end": 338.0, "text": " So the categories and I want you to guess which category each post is from. The choices are atheism, miscellaneous religion, computer graphics or space.", "tokens": [407, 264, 10479, 293, 286, 528, 291, 281, 2041, 597, 7719, 1184, 2183, 307, 490, 13, 440, 7994, 366, 27033, 1434, 11, 3346, 4164, 15447, 7561, 11, 3820, 11837, 420, 1901, 13], "temperature": 0.0, "avg_logprob": -0.11025061430754485, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.014338512410177e-06}, {"id": 33, "seek": 32400, "start": 338.0, "end": 353.0, "text": " So let's look at this first one. I've noticed that if you only save a model with all your mapping planes carefully positioned to a point three DS file, which which category do you think this is from?", "tokens": [407, 718, 311, 574, 412, 341, 700, 472, 13, 286, 600, 5694, 300, 498, 291, 787, 3155, 257, 2316, 365, 439, 428, 18350, 14952, 7500, 24889, 281, 257, 935, 1045, 15816, 3991, 11, 597, 597, 7719, 360, 291, 519, 341, 307, 490, 30], "temperature": 0.0, "avg_logprob": -0.11025061430754485, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.014338512410177e-06}, {"id": 34, "seek": 35300, "start": 353.0, "end": 358.0, "text": " Someone say it louder. Yes, from graphics.", "tokens": [8734, 584, 309, 22717, 13, 1079, 11, 490, 11837, 13], "temperature": 0.0, "avg_logprob": -0.15344359923382195, "compression_ratio": 1.3142857142857143, "no_speech_prob": 1.4507611012959387e-05}, {"id": 35, "seek": 35300, "start": 358.0, "end": 368.0, "text": " And this next one says it's talking about Koresh was a deranged fanatic who thought it was necessary to take a whole bunch of folks with him.", "tokens": [400, 341, 958, 472, 1619, 309, 311, 1417, 466, 591, 2706, 71, 390, 257, 1163, 10296, 3429, 2399, 567, 1194, 309, 390, 4818, 281, 747, 257, 1379, 3840, 295, 4024, 365, 796, 13], "temperature": 0.0, "avg_logprob": -0.15344359923382195, "compression_ratio": 1.3142857142857143, "no_speech_prob": 1.4507611012959387e-05}, {"id": 36, "seek": 36800, "start": 368.0, "end": 394.0, "text": " Mentioned Jim Jones, which category do you think this is? Anyone? So this could be atheism or it could be religion and we'll see some overlap between those.", "tokens": [376, 1251, 292, 6637, 10512, 11, 597, 7719, 360, 291, 519, 341, 307, 30, 14643, 30, 407, 341, 727, 312, 27033, 1434, 420, 309, 727, 312, 7561, 293, 321, 603, 536, 512, 19959, 1296, 729, 13], "temperature": 0.0, "avg_logprob": -0.17977374792099, "compression_ratio": 1.2580645161290323, "no_speech_prob": 5.771470114268595e-06}, {"id": 37, "seek": 39400, "start": 394.0, "end": 409.0, "text": " Next one, I actually had to look this up. It's talking about Perry Joves, which is turns out a point in the orbit of a satellite of Jupiter. So what might that be? Space, exactly.", "tokens": [3087, 472, 11, 286, 767, 632, 281, 574, 341, 493, 13, 467, 311, 1417, 466, 17334, 3139, 977, 11, 597, 307, 4523, 484, 257, 935, 294, 264, 13991, 295, 257, 16016, 295, 24567, 13, 407, 437, 1062, 300, 312, 30, 8705, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.11323917613309972, "compression_ratio": 1.578740157480315, "no_speech_prob": 4.936689947498962e-06}, {"id": 38, "seek": 39400, "start": 409.0, "end": 423.0, "text": " And so that's always kind of good to just see like, okay, this is what what the data looks like. And you will see like we're seeing some kind of like numbers and things not not all the words are going to be helpful to us.", "tokens": [400, 370, 300, 311, 1009, 733, 295, 665, 281, 445, 536, 411, 11, 1392, 11, 341, 307, 437, 437, 264, 1412, 1542, 411, 13, 400, 291, 486, 536, 411, 321, 434, 2577, 512, 733, 295, 411, 3547, 293, 721, 406, 406, 439, 264, 2283, 366, 516, 281, 312, 4961, 281, 505, 13], "temperature": 0.0, "avg_logprob": -0.11323917613309972, "compression_ratio": 1.578740157480315, "no_speech_prob": 4.936689947498962e-06}, {"id": 39, "seek": 42300, "start": 423.0, "end": 431.0, "text": " So here we can check the target is giving us the label.", "tokens": [407, 510, 321, 393, 1520, 264, 3779, 307, 2902, 505, 264, 7645, 13], "temperature": 0.0, "avg_logprob": -0.11133223064875199, "compression_ratio": 1.5276073619631902, "no_speech_prob": 1.0289003512298223e-05}, {"id": 40, "seek": 42300, "start": 431.0, "end": 441.0, "text": " And then scikit learn. So kind of if we wanted to make that matrix with all the word frequencies, we could write something ourselves, which would be a bit tedious to get those word frequencies.", "tokens": [400, 550, 2180, 22681, 1466, 13, 407, 733, 295, 498, 321, 1415, 281, 652, 300, 8141, 365, 439, 264, 1349, 20250, 11, 321, 727, 2464, 746, 4175, 11, 597, 576, 312, 257, 857, 38284, 281, 483, 729, 1349, 20250, 13], "temperature": 0.0, "avg_logprob": -0.11133223064875199, "compression_ratio": 1.5276073619631902, "no_speech_prob": 1.0289003512298223e-05}, {"id": 41, "seek": 44100, "start": 441.0, "end": 454.0, "text": " Scikit learn has a feature extraction count vectorizer that does that. So we'll just use their count vectorizer. So that's what's happening. This input cell count vectorizer.", "tokens": [16942, 22681, 1466, 575, 257, 4111, 30197, 1207, 8062, 6545, 300, 775, 300, 13, 407, 321, 603, 445, 764, 641, 1207, 8062, 6545, 13, 407, 300, 311, 437, 311, 2737, 13, 639, 4846, 2815, 1207, 8062, 6545, 13], "temperature": 0.0, "avg_logprob": -0.10171145484561012, "compression_ratio": 1.5130434782608695, "no_speech_prob": 1.0782654499053024e-05}, {"id": 42, "seek": 45400, "start": 454.0, "end": 483.0, "text": " And we're passing in stop words to say that we don't want to don't want to include those since they don't contain information. But oh and stop words are words like is the A that don't have much meaning or don't really have much like kind of category or subject meaning.", "tokens": [400, 321, 434, 8437, 294, 1590, 2283, 281, 584, 300, 321, 500, 380, 528, 281, 500, 380, 528, 281, 4090, 729, 1670, 436, 500, 380, 5304, 1589, 13, 583, 1954, 293, 1590, 2283, 366, 2283, 411, 307, 264, 316, 300, 500, 380, 362, 709, 3620, 420, 500, 380, 534, 362, 709, 411, 733, 295, 7719, 420, 3983, 3620, 13], "temperature": 0.0, "avg_logprob": -0.16478824615478516, "compression_ratio": 1.670807453416149, "no_speech_prob": 3.5557657156459754e-06}, {"id": 43, "seek": 48300, "start": 483.0, "end": 494.0, "text": " And so note after we do this, we check our so we kind of get our vectorizer and we're doing a vectorizer fit transform. We pass the newsgroups the training data.", "tokens": [400, 370, 3637, 934, 321, 360, 341, 11, 321, 1520, 527, 370, 321, 733, 295, 483, 527, 8062, 6545, 293, 321, 434, 884, 257, 8062, 6545, 3318, 4088, 13, 492, 1320, 264, 2583, 17377, 82, 264, 3097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.11870272805757612, "compression_ratio": 1.7244094488188977, "no_speech_prob": 8.938893188314978e-06}, {"id": 44, "seek": 48300, "start": 494.0, "end": 509.0, "text": " And then we're converting this to dense. And last time I talked about sparse versus dense matrices. We'll go into more detail on that in the future. But what scikit learn is returning is sparse and we're going to make it dense because that's easier for what we're doing today.", "tokens": [400, 550, 321, 434, 29942, 341, 281, 18011, 13, 400, 1036, 565, 286, 2825, 466, 637, 11668, 5717, 18011, 32284, 13, 492, 603, 352, 666, 544, 2607, 322, 300, 294, 264, 2027, 13, 583, 437, 2180, 22681, 1466, 307, 12678, 307, 637, 11668, 293, 321, 434, 516, 281, 652, 309, 18011, 570, 300, 311, 3571, 337, 437, 321, 434, 884, 965, 13], "temperature": 0.0, "avg_logprob": -0.11870272805757612, "compression_ratio": 1.7244094488188977, "no_speech_prob": 8.938893188314978e-06}, {"id": 45, "seek": 50900, "start": 509.0, "end": 522.0, "text": " And we can check the shape. And so we've still got 2000 documents and then it looks like we have 26,000 vocabulary words.", "tokens": [400, 321, 393, 1520, 264, 3909, 13, 400, 370, 321, 600, 920, 658, 8132, 8512, 293, 550, 309, 1542, 411, 321, 362, 7551, 11, 1360, 19864, 2283, 13], "temperature": 0.0, "avg_logprob": -0.1152348936649791, "compression_ratio": 1.3571428571428572, "no_speech_prob": 2.2825823180028237e-05}, {"id": 46, "seek": 50900, "start": 522.0, "end": 527.0, "text": " Oh, that's a good question. I don't think we have any guarantees on what order it's in.", "tokens": [876, 11, 300, 311, 257, 665, 1168, 13, 286, 500, 380, 519, 321, 362, 604, 32567, 322, 437, 1668, 309, 311, 294, 13], "temperature": 0.0, "avg_logprob": -0.1152348936649791, "compression_ratio": 1.3571428571428572, "no_speech_prob": 2.2825823180028237e-05}, {"id": 47, "seek": 52700, "start": 527.0, "end": 541.0, "text": " Oh, actually, never mind when you get so there's a vocab dot or sorry vectorizer dot get feature names down here and putting that into an array and they are alphabetical.", "tokens": [876, 11, 767, 11, 1128, 1575, 562, 291, 483, 370, 456, 311, 257, 2329, 455, 5893, 420, 2597, 8062, 6545, 5893, 483, 4111, 5288, 760, 510, 293, 3372, 300, 666, 364, 10225, 293, 436, 366, 23339, 804, 13], "temperature": 0.0, "avg_logprob": -0.18623615713680491, "compression_ratio": 1.3916083916083917, "no_speech_prob": 1.593443812453188e-05}, {"id": 48, "seek": 52700, "start": 541.0, "end": 551.0, "text": " Let me actually pull up the.", "tokens": [961, 385, 767, 2235, 493, 264, 13], "temperature": 0.0, "avg_logprob": -0.18623615713680491, "compression_ratio": 1.3916083916083917, "no_speech_prob": 1.593443812453188e-05}, {"id": 49, "seek": 55100, "start": 551.0, "end": 559.0, "text": " Oh, thank you.", "tokens": [876, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.22366108213152205, "compression_ratio": 0.75, "no_speech_prob": 9.515382771496661e-06}, {"id": 50, "seek": 55100, "start": 559.0, "end": 563.0, "text": " That too.", "tokens": [663, 886, 13], "temperature": 0.0, "avg_logprob": -0.22366108213152205, "compression_ratio": 0.75, "no_speech_prob": 9.515382771496661e-06}, {"id": 51, "seek": 56300, "start": 563.0, "end": 585.0, "text": " What I wanted to illustrate.", "tokens": [708, 286, 1415, 281, 23221, 13], "temperature": 0.0, "avg_logprob": -0.3164797306060791, "compression_ratio": 0.7777777777777778, "no_speech_prob": 8.529552360414527e-06}, {"id": 52, "seek": 58500, "start": 585.0, "end": 599.0, "text": " And there are a lot of a lot of features that we weren't using. So you could remove accents and do some sort of pre processing.", "tokens": [400, 456, 366, 257, 688, 295, 257, 688, 295, 4122, 300, 321, 4999, 380, 1228, 13, 407, 291, 727, 4159, 35012, 293, 360, 512, 1333, 295, 659, 9007, 13], "temperature": 0.0, "avg_logprob": -0.10279310666597806, "compression_ratio": 1.4, "no_speech_prob": 4.092471499461681e-06}, {"id": 53, "seek": 58500, "start": 599.0, "end": 606.0, "text": " Yeah, that's a good way to kind of find out more about your methods.", "tokens": [865, 11, 300, 311, 257, 665, 636, 281, 733, 295, 915, 484, 544, 466, 428, 7150, 13], "temperature": 0.0, "avg_logprob": -0.10279310666597806, "compression_ratio": 1.4, "no_speech_prob": 4.092471499461681e-06}, {"id": 54, "seek": 60600, "start": 606.0, "end": 616.0, "text": " Yeah, down here I am kind of use vectorizer get feature names, put those into vocab and then looking at the vocab, it's alphabetical.", "tokens": [865, 11, 760, 510, 286, 669, 733, 295, 764, 8062, 6545, 483, 4111, 5288, 11, 829, 729, 666, 2329, 455, 293, 550, 1237, 412, 264, 2329, 455, 11, 309, 311, 23339, 804, 13], "temperature": 0.0, "avg_logprob": -0.15595977008342743, "compression_ratio": 1.421383647798742, "no_speech_prob": 6.04805609327741e-06}, {"id": 55, "seek": 60600, "start": 616.0, "end": 621.0, "text": " And so here's a section in the seas of some of the words that show up.", "tokens": [400, 370, 510, 311, 257, 3541, 294, 264, 22535, 295, 512, 295, 264, 2283, 300, 855, 493, 13], "temperature": 0.0, "avg_logprob": -0.15595977008342743, "compression_ratio": 1.421383647798742, "no_speech_prob": 6.04805609327741e-06}, {"id": 56, "seek": 60600, "start": 621.0, "end": 631.0, "text": " Any questions so far?", "tokens": [2639, 1651, 370, 1400, 30], "temperature": 0.0, "avg_logprob": -0.15595977008342743, "compression_ratio": 1.421383647798742, "no_speech_prob": 6.04805609327741e-06}, {"id": 57, "seek": 63100, "start": 631.0, "end": 642.0, "text": " Do you want to stem words so that they both need so that you can reduce the features? For example, we have council and councils.", "tokens": [1144, 291, 528, 281, 12312, 2283, 370, 300, 436, 1293, 643, 370, 300, 291, 393, 5407, 264, 4122, 30, 1171, 1365, 11, 321, 362, 9209, 293, 39187, 13], "temperature": 0.0, "avg_logprob": -0.2609636443001883, "compression_ratio": 1.6009174311926606, "no_speech_prob": 3.1683812267147005e-05}, {"id": 58, "seek": 63100, "start": 642.0, "end": 649.0, "text": " That's a great suggestion. Yeah, so this is kind of a lazy approach where we have not.", "tokens": [663, 311, 257, 869, 16541, 13, 865, 11, 370, 341, 307, 733, 295, 257, 14847, 3109, 689, 321, 362, 406, 13], "temperature": 0.0, "avg_logprob": -0.2609636443001883, "compression_ratio": 1.6009174311926606, "no_speech_prob": 3.1683812267147005e-05}, {"id": 59, "seek": 63100, "start": 649.0, "end": 656.0, "text": " Not stemmed the words and stemming refers to kind of taking different variations of the same word and putting them on the same thing.", "tokens": [1726, 12312, 1912, 264, 2283, 293, 12312, 2810, 14942, 281, 733, 295, 1940, 819, 17840, 295, 264, 912, 1349, 293, 3372, 552, 322, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.2609636443001883, "compression_ratio": 1.6009174311926606, "no_speech_prob": 3.1683812267147005e-05}, {"id": 60, "seek": 65600, "start": 656.0, "end": 665.0, "text": " So that's something like the word walk. You would take walking and walked and walks and kind of classify all those under walk.", "tokens": [407, 300, 311, 746, 411, 264, 1349, 1792, 13, 509, 576, 747, 4494, 293, 7628, 293, 12896, 293, 733, 295, 33872, 439, 729, 833, 1792, 13], "temperature": 0.0, "avg_logprob": -0.12136288250193876, "compression_ratio": 1.50354609929078, "no_speech_prob": 7.071177151374286e-06}, {"id": 61, "seek": 65600, "start": 665.0, "end": 678.0, "text": " And we're not doing that here, but that would be a completely legitimate thing to do.", "tokens": [400, 321, 434, 406, 884, 300, 510, 11, 457, 300, 576, 312, 257, 2584, 17956, 551, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.12136288250193876, "compression_ratio": 1.50354609929078, "no_speech_prob": 7.071177151374286e-06}, {"id": 62, "seek": 67800, "start": 678.0, "end": 688.0, "text": " OK, so first up, we're going to use singular value decomposition. And actually, I'm curious, have you covered SVD in previous courses?", "tokens": [2264, 11, 370, 700, 493, 11, 321, 434, 516, 281, 764, 20010, 2158, 48356, 13, 400, 767, 11, 286, 478, 6369, 11, 362, 291, 5343, 31910, 35, 294, 3894, 7712, 30], "temperature": 0.0, "avg_logprob": -0.11284062170213269, "compression_ratio": 1.329479768786127, "no_speech_prob": 1.1910278772120364e-06}, {"id": 63, "seek": 67800, "start": 688.0, "end": 693.0, "text": " What context did you see it in?", "tokens": [708, 4319, 630, 291, 536, 309, 294, 30], "temperature": 0.0, "avg_logprob": -0.11284062170213269, "compression_ratio": 1.329479768786127, "no_speech_prob": 1.1910278772120364e-06}, {"id": 64, "seek": 67800, "start": 693.0, "end": 700.0, "text": " Image compression. OK, great. It's a great application of this.", "tokens": [29903, 19355, 13, 2264, 11, 869, 13, 467, 311, 257, 869, 3861, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.11284062170213269, "compression_ratio": 1.329479768786127, "no_speech_prob": 1.1910278772120364e-06}, {"id": 65, "seek": 70000, "start": 700.0, "end": 709.0, "text": " So Gilbert Strain has written a classic linear algebra textbook that SVD is not nearly as famous as it should be.", "tokens": [407, 39003, 745, 7146, 575, 3720, 257, 7230, 8213, 21989, 25591, 300, 31910, 35, 307, 406, 6217, 382, 4618, 382, 309, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.0736389910237173, "compression_ratio": 1.6625, "no_speech_prob": 4.156403974775458e-06}, {"id": 66, "seek": 70000, "start": 709.0, "end": 719.0, "text": " And so here kind of the key thing with our decomposition is we're saying that we want the matrices we're decomposing into to be orthogonal.", "tokens": [400, 370, 510, 733, 295, 264, 2141, 551, 365, 527, 48356, 307, 321, 434, 1566, 300, 321, 528, 264, 32284, 321, 434, 22867, 6110, 666, 281, 312, 41488, 13], "temperature": 0.0, "avg_logprob": -0.0736389910237173, "compression_ratio": 1.6625, "no_speech_prob": 4.156403974775458e-06}, {"id": 67, "seek": 70000, "start": 719.0, "end": 728.0, "text": " And the motivation behind that is that we're thinking that words that are frequently in one topic are probably less likely to be in other topics.", "tokens": [400, 264, 12335, 2261, 300, 307, 300, 321, 434, 1953, 300, 2283, 300, 366, 10374, 294, 472, 4829, 366, 1391, 1570, 3700, 281, 312, 294, 661, 8378, 13], "temperature": 0.0, "avg_logprob": -0.0736389910237173, "compression_ratio": 1.6625, "no_speech_prob": 4.156403974775458e-06}, {"id": 68, "seek": 72800, "start": 728.0, "end": 735.0, "text": " Otherwise, they wouldn't be a good thing to kind of define that topic of what makes that topic unique or different from the others.", "tokens": [10328, 11, 436, 2759, 380, 312, 257, 665, 551, 281, 733, 295, 6964, 300, 4829, 295, 437, 1669, 300, 4829, 3845, 420, 819, 490, 264, 2357, 13], "temperature": 0.0, "avg_logprob": -0.07135298517015246, "compression_ratio": 1.5638297872340425, "no_speech_prob": 7.765791451674886e-06}, {"id": 69, "seek": 72800, "start": 735.0, "end": 739.0, "text": " And so that's a great question. What's orthogonal?", "tokens": [400, 370, 300, 311, 257, 869, 1168, 13, 708, 311, 41488, 30], "temperature": 0.0, "avg_logprob": -0.07135298517015246, "compression_ratio": 1.5638297872340425, "no_speech_prob": 7.765791451674886e-06}, {"id": 70, "seek": 72800, "start": 739.0, "end": 748.0, "text": " That's any two. So if you have a collection of vectors, any two vectors that are different dot product is zero.", "tokens": [663, 311, 604, 732, 13, 407, 498, 291, 362, 257, 5765, 295, 18875, 11, 604, 732, 18875, 300, 366, 819, 5893, 1674, 307, 4018, 13], "temperature": 0.0, "avg_logprob": -0.07135298517015246, "compression_ratio": 1.5638297872340425, "no_speech_prob": 7.765791451674886e-06}, {"id": 71, "seek": 74800, "start": 748.0, "end": 759.0, "text": " The vector with itself. Oh, so orthonormal means that the dot product with itself is one and that the dot product with other vectors is zero.", "tokens": [440, 8062, 365, 2564, 13, 876, 11, 370, 420, 11943, 24440, 1355, 300, 264, 5893, 1674, 365, 2564, 307, 472, 293, 300, 264, 5893, 1674, 365, 661, 18875, 307, 4018, 13], "temperature": 0.0, "avg_logprob": -0.1378118453487273, "compression_ratio": 1.652694610778443, "no_speech_prob": 9.422071798326215e-07}, {"id": 72, "seek": 74800, "start": 759.0, "end": 770.0, "text": " And a little bit confusing when you talk about matrices being orthogonal, that means that their columns and rows are worth the normal.", "tokens": [400, 257, 707, 857, 13181, 562, 291, 751, 466, 32284, 885, 41488, 11, 300, 1355, 300, 641, 13766, 293, 13241, 366, 3163, 264, 2710, 13], "temperature": 0.0, "avg_logprob": -0.1378118453487273, "compression_ratio": 1.652694610778443, "no_speech_prob": 9.422071798326215e-07}, {"id": 73, "seek": 77000, "start": 770.0, "end": 778.0, "text": " This is a great nice diagram from a Facebook blog post that we're going to refer back to later.", "tokens": [639, 307, 257, 869, 1481, 10686, 490, 257, 4384, 6968, 2183, 300, 321, 434, 516, 281, 2864, 646, 281, 1780, 13], "temperature": 0.0, "avg_logprob": -0.10036474466323853, "compression_ratio": 1.5628140703517588, "no_speech_prob": 2.7691544346453156e-06}, {"id": 74, "seek": 77000, "start": 778.0, "end": 786.0, "text": " But so here they're talking about the words being the rows and then having hashtags as columns.", "tokens": [583, 370, 510, 436, 434, 1417, 466, 264, 2283, 885, 264, 13241, 293, 550, 1419, 50016, 382, 13766, 13], "temperature": 0.0, "avg_logprob": -0.10036474466323853, "compression_ratio": 1.5628140703517588, "no_speech_prob": 2.7691544346453156e-06}, {"id": 75, "seek": 77000, "start": 786.0, "end": 794.0, "text": " And there are these are frequencies of how often word shows up in a post with that particular hashtag also in the post.", "tokens": [400, 456, 366, 613, 366, 20250, 295, 577, 2049, 1349, 3110, 493, 294, 257, 2183, 365, 300, 1729, 20379, 611, 294, 264, 2183, 13], "temperature": 0.0, "avg_logprob": -0.10036474466323853, "compression_ratio": 1.5628140703517588, "no_speech_prob": 2.7691544346453156e-06}, {"id": 76, "seek": 79400, "start": 794.0, "end": 807.0, "text": " And then you is the kind of first matrix you're getting. And the columns of you are orthogonal to each other.", "tokens": [400, 550, 291, 307, 264, 733, 295, 700, 8141, 291, 434, 1242, 13, 400, 264, 13766, 295, 291, 366, 41488, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.07955509821573893, "compression_ratio": 1.7962962962962963, "no_speech_prob": 7.765821465000045e-06}, {"id": 77, "seek": 79400, "start": 807.0, "end": 812.0, "text": " And then we've got this kind of middle diagonal matrix that the white part is all zeros.", "tokens": [400, 550, 321, 600, 658, 341, 733, 295, 2808, 21539, 8141, 300, 264, 2418, 644, 307, 439, 35193, 13], "temperature": 0.0, "avg_logprob": -0.07955509821573893, "compression_ratio": 1.7962962962962963, "no_speech_prob": 7.765821465000045e-06}, {"id": 78, "seek": 79400, "start": 812.0, "end": 818.0, "text": " It only has values along the diagonal, which is blue. And that's giving you the relative importance.", "tokens": [467, 787, 575, 4190, 2051, 264, 21539, 11, 597, 307, 3344, 13, 400, 300, 311, 2902, 291, 264, 4972, 7379, 13], "temperature": 0.0, "avg_logprob": -0.07955509821573893, "compression_ratio": 1.7962962962962963, "no_speech_prob": 7.765821465000045e-06}, {"id": 79, "seek": 79400, "start": 818.0, "end": 822.0, "text": " So kind of those values are called the singular values and tell you relative importance.", "tokens": [407, 733, 295, 729, 4190, 366, 1219, 264, 20010, 4190, 293, 980, 291, 4972, 7379, 13], "temperature": 0.0, "avg_logprob": -0.07955509821573893, "compression_ratio": 1.7962962962962963, "no_speech_prob": 7.765821465000045e-06}, {"id": 80, "seek": 82200, "start": 822.0, "end": 831.0, "text": " And then the pink matrix on the right there, the rows are all orthonormal to one another.", "tokens": [400, 550, 264, 7022, 8141, 322, 264, 558, 456, 11, 264, 13241, 366, 439, 420, 11943, 24440, 281, 472, 1071, 13], "temperature": 0.0, "avg_logprob": -0.14136844048133262, "compression_ratio": 1.5095541401273886, "no_speech_prob": 1.7879527831610176e-06}, {"id": 81, "seek": 82200, "start": 831.0, "end": 838.0, "text": " The rows are, yes. Oh, sorry, the rows are orthonormal.", "tokens": [440, 13241, 366, 11, 2086, 13, 876, 11, 2597, 11, 264, 13241, 366, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.14136844048133262, "compression_ratio": 1.5095541401273886, "no_speech_prob": 1.7879527831610176e-06}, {"id": 82, "seek": 82200, "start": 838.0, "end": 845.0, "text": " Something to note about SPD is that it's an exact decomposition when you're doing full SPD.", "tokens": [6595, 281, 3637, 466, 19572, 307, 300, 309, 311, 364, 1900, 48356, 562, 291, 434, 884, 1577, 19572, 13], "temperature": 0.0, "avg_logprob": -0.14136844048133262, "compression_ratio": 1.5095541401273886, "no_speech_prob": 1.7879527831610176e-06}, {"id": 83, "seek": 84500, "start": 845.0, "end": 857.0, "text": " And so that means you can completely recover your matrix and that this is also in this case, if R was equal to N.", "tokens": [400, 370, 300, 1355, 291, 393, 2584, 8114, 428, 8141, 293, 300, 341, 307, 611, 294, 341, 1389, 11, 498, 497, 390, 2681, 281, 426, 13], "temperature": 0.0, "avg_logprob": -0.1319507548683568, "compression_ratio": 1.4896907216494846, "no_speech_prob": 1.3287233286973787e-06}, {"id": 84, "seek": 84500, "start": 857.0, "end": 865.0, "text": " And so it's the same dimensions. This would be an equal sign.", "tokens": [400, 370, 309, 311, 264, 912, 12819, 13, 639, 576, 312, 364, 2681, 1465, 13], "temperature": 0.0, "avg_logprob": -0.1319507548683568, "compression_ratio": 1.4896907216494846, "no_speech_prob": 1.3287233286973787e-06}, {"id": 85, "seek": 84500, "start": 865.0, "end": 874.0, "text": " So SPD is very widely used. And we're going to see this CSVD in multiple, multiple of the lessons in this course.", "tokens": [407, 19572, 307, 588, 13371, 1143, 13, 400, 321, 434, 516, 281, 536, 341, 48814, 35, 294, 3866, 11, 3866, 295, 264, 8820, 294, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.1319507548683568, "compression_ratio": 1.4896907216494846, "no_speech_prob": 1.3287233286973787e-06}, {"id": 86, "seek": 87400, "start": 874.0, "end": 880.0, "text": " We're using it today for semantic analysis. It's also used in collaborative filtering and recommendations.", "tokens": [492, 434, 1228, 309, 965, 337, 47982, 5215, 13, 467, 311, 611, 1143, 294, 16555, 30822, 293, 10434, 13], "temperature": 0.0, "avg_logprob": -0.11162475177219935, "compression_ratio": 1.5588235294117647, "no_speech_prob": 2.4680424758116715e-05}, {"id": 87, "seek": 87400, "start": 880.0, "end": 886.0, "text": " The winners of the Netflix Prize were all kind of more complicated variations of SPD.", "tokens": [440, 17193, 295, 264, 12778, 22604, 645, 439, 733, 295, 544, 6179, 17840, 295, 19572, 13], "temperature": 0.0, "avg_logprob": -0.11162475177219935, "compression_ratio": 1.5588235294117647, "no_speech_prob": 2.4680424758116715e-05}, {"id": 88, "seek": 87400, "start": 886.0, "end": 891.0, "text": " Data compression, which you've seen, it can calculate the more Penrose pseudo inverse.", "tokens": [11888, 19355, 11, 597, 291, 600, 1612, 11, 309, 393, 8873, 264, 544, 10571, 37841, 35899, 17340, 13], "temperature": 0.0, "avg_logprob": -0.11162475177219935, "compression_ratio": 1.5588235294117647, "no_speech_prob": 2.4680424758116715e-05}, {"id": 89, "seek": 87400, "start": 891.0, "end": 896.0, "text": " So this is for matrices that don't have a true inverse. There's kind of the pseudo inverse.", "tokens": [407, 341, 307, 337, 32284, 300, 500, 380, 362, 257, 2074, 17340, 13, 821, 311, 733, 295, 264, 35899, 17340, 13], "temperature": 0.0, "avg_logprob": -0.11162475177219935, "compression_ratio": 1.5588235294117647, "no_speech_prob": 2.4680424758116715e-05}, {"id": 90, "seek": 89600, "start": 896.0, "end": 904.0, "text": " And you use SPD to find it. And then principal component analysis, which we'll also see. Yes.", "tokens": [400, 291, 764, 19572, 281, 915, 309, 13, 400, 550, 9716, 6542, 5215, 11, 597, 321, 603, 611, 536, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.18585640011411725, "compression_ratio": 1.5297029702970297, "no_speech_prob": 1.0782408935483545e-05}, {"id": 91, "seek": 89600, "start": 904.0, "end": 914.0, "text": " What does orthogonal mean like intuitively and why is that useful for this example?", "tokens": [708, 775, 41488, 914, 411, 46506, 293, 983, 307, 300, 4420, 337, 341, 1365, 30], "temperature": 0.0, "avg_logprob": -0.18585640011411725, "compression_ratio": 1.5297029702970297, "no_speech_prob": 1.0782408935483545e-05}, {"id": 92, "seek": 89600, "start": 914.0, "end": 924.0, "text": " Good question. So what orthogonal means intuitively is that you're capturing very different information, very different directions.", "tokens": [2205, 1168, 13, 407, 437, 41488, 1355, 46506, 307, 300, 291, 434, 23384, 588, 819, 1589, 11, 588, 819, 11095, 13], "temperature": 0.0, "avg_logprob": -0.18585640011411725, "compression_ratio": 1.5297029702970297, "no_speech_prob": 1.0782408935483545e-05}, {"id": 93, "seek": 92400, "start": 924.0, "end": 929.0, "text": " And so if you were thinking about directions, it's like perpendicular.", "tokens": [400, 370, 498, 291, 645, 1953, 466, 11095, 11, 309, 311, 411, 26734, 13], "temperature": 0.0, "avg_logprob": -0.058693905951271594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.954557309451047e-06}, {"id": 94, "seek": 92400, "start": 929.0, "end": 935.0, "text": " You're not even having stuff that's somewhat in the same direction, but capturing very different.", "tokens": [509, 434, 406, 754, 1419, 1507, 300, 311, 8344, 294, 264, 912, 3513, 11, 457, 23384, 588, 819, 13], "temperature": 0.0, "avg_logprob": -0.058693905951271594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.954557309451047e-06}, {"id": 95, "seek": 92400, "start": 935.0, "end": 946.0, "text": " So in this context of thinking about topics or categories, it's we want very different topics and kind of finding what makes this different from everything else.", "tokens": [407, 294, 341, 4319, 295, 1953, 466, 8378, 420, 10479, 11, 309, 311, 321, 528, 588, 819, 8378, 293, 733, 295, 5006, 437, 1669, 341, 819, 490, 1203, 1646, 13], "temperature": 0.0, "avg_logprob": -0.058693905951271594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 5.954557309451047e-06}, {"id": 96, "seek": 94600, "start": 946.0, "end": 956.0, "text": " And if you think of the kind of math definition of this dot product going to zero, it's that everything has canceled out.", "tokens": [400, 498, 291, 519, 295, 264, 733, 295, 5221, 7123, 295, 341, 5893, 1674, 516, 281, 4018, 11, 309, 311, 300, 1203, 575, 24839, 484, 13], "temperature": 0.0, "avg_logprob": -0.18069571540469215, "compression_ratio": 1.3964497041420119, "no_speech_prob": 3.2374875900131883e-06}, {"id": 97, "seek": 94600, "start": 956.0, "end": 962.0, "text": " But thank you, Jeremy.", "tokens": [583, 1309, 291, 11, 17809, 13], "temperature": 0.0, "avg_logprob": -0.18069571540469215, "compression_ratio": 1.3964497041420119, "no_speech_prob": 3.2374875900131883e-06}, {"id": 98, "seek": 94600, "start": 962.0, "end": 972.0, "text": " So now I've. Oh, and this should show using percent time to time what I'm doing in Jupiter.", "tokens": [407, 586, 286, 600, 13, 876, 11, 293, 341, 820, 855, 1228, 3043, 565, 281, 565, 437, 286, 478, 884, 294, 24567, 13], "temperature": 0.0, "avg_logprob": -0.18069571540469215, "compression_ratio": 1.3964497041420119, "no_speech_prob": 3.2374875900131883e-06}, {"id": 99, "seek": 97200, "start": 972.0, "end": 977.0, "text": " These are called magics when you have a percent or double percent.", "tokens": [1981, 366, 1219, 2258, 1167, 562, 291, 362, 257, 3043, 420, 3834, 3043, 13], "temperature": 0.0, "avg_logprob": -0.08363222068464252, "compression_ratio": 1.4814814814814814, "no_speech_prob": 9.221978871210013e-06}, {"id": 100, "seek": 97200, "start": 977.0, "end": 984.0, "text": " And this is really handy and we'll use this some more to kind of get comparisons of how long things are taking.", "tokens": [400, 341, 307, 534, 13239, 293, 321, 603, 764, 341, 512, 544, 281, 733, 295, 483, 33157, 295, 577, 938, 721, 366, 1940, 13], "temperature": 0.0, "avg_logprob": -0.08363222068464252, "compression_ratio": 1.4814814814814814, "no_speech_prob": 9.221978871210013e-06}, {"id": 101, "seek": 97200, "start": 984.0, "end": 992.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.08363222068464252, "compression_ratio": 1.4814814814814814, "no_speech_prob": 9.221978871210013e-06}, {"id": 102, "seek": 97200, "start": 992.0, "end": 996.0, "text": " And so the question was, is it possible just to set it once and have it throughout the notebook?", "tokens": [400, 370, 264, 1168, 390, 11, 307, 309, 1944, 445, 281, 992, 309, 1564, 293, 362, 309, 3710, 264, 21060, 30], "temperature": 0.0, "avg_logprob": -0.08363222068464252, "compression_ratio": 1.4814814814814814, "no_speech_prob": 9.221978871210013e-06}, {"id": 103, "seek": 99600, "start": 996.0, "end": 1003.0, "text": " I don't know of a way to do that.", "tokens": [286, 500, 380, 458, 295, 257, 636, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.2826394081115723, "compression_ratio": 1.3705882352941177, "no_speech_prob": 4.35644051322015e-06}, {"id": 104, "seek": 99600, "start": 1003.0, "end": 1006.0, "text": " Cool. OK.", "tokens": [8561, 13, 2264, 13], "temperature": 0.0, "avg_logprob": -0.2826394081115723, "compression_ratio": 1.3705882352941177, "no_speech_prob": 4.35644051322015e-06}, {"id": 105, "seek": 99600, "start": 1006.0, "end": 1009.0, "text": " Me.", "tokens": [1923, 13], "temperature": 0.0, "avg_logprob": -0.2826394081115723, "compression_ratio": 1.3705882352941177, "no_speech_prob": 4.35644051322015e-06}, {"id": 106, "seek": 99600, "start": 1009.0, "end": 1014.0, "text": " So now I wanted to ask you, so we're using and this is from SciPy's Lin now just be.", "tokens": [407, 586, 286, 1415, 281, 1029, 291, 11, 370, 321, 434, 1228, 293, 341, 307, 490, 16942, 47, 88, 311, 9355, 586, 445, 312, 13], "temperature": 0.0, "avg_logprob": -0.2826394081115723, "compression_ratio": 1.3705882352941177, "no_speech_prob": 4.35644051322015e-06}, {"id": 107, "seek": 99600, "start": 1014.0, "end": 1018.0, "text": " We get you S and VH back.", "tokens": [492, 483, 291, 318, 293, 691, 39, 646, 13], "temperature": 0.0, "avg_logprob": -0.2826394081115723, "compression_ratio": 1.3705882352941177, "no_speech_prob": 4.35644051322015e-06}, {"id": 108, "seek": 99600, "start": 1018.0, "end": 1023.0, "text": " Just take a moment to confirm that this is the decomposition of the input.", "tokens": [1449, 747, 257, 1623, 281, 9064, 300, 341, 307, 264, 48356, 295, 264, 4846, 13], "temperature": 0.0, "avg_logprob": -0.2826394081115723, "compression_ratio": 1.3705882352941177, "no_speech_prob": 4.35644051322015e-06}, {"id": 109, "seek": 102300, "start": 1023.0, "end": 1026.0, "text": " So it's always good to kind of check that you're getting what you think.", "tokens": [50364, 407, 309, 311, 1009, 665, 281, 733, 295, 1520, 300, 291, 434, 1242, 437, 291, 519, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07735152840614319, "compression_ratio": 1.0140845070422535, "no_speech_prob": 8.072104537859559e-05}, {"id": 110, "seek": 105300, "start": 1053.0, "end": 1060.0, "text": " No, we don't.", "tokens": [883, 11, 321, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.17774070910553433, "compression_ratio": 1.403973509933775, "no_speech_prob": 0.01910344883799553}, {"id": 111, "seek": 105300, "start": 1060.0, "end": 1064.0, "text": " It will depend what you're doing.", "tokens": [467, 486, 5672, 437, 291, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.17774070910553433, "compression_ratio": 1.403973509933775, "no_speech_prob": 0.01910344883799553}, {"id": 112, "seek": 105300, "start": 1064.0, "end": 1066.0, "text": " And this.", "tokens": [400, 341, 13], "temperature": 0.0, "avg_logprob": -0.17774070910553433, "compression_ratio": 1.403973509933775, "no_speech_prob": 0.01910344883799553}, {"id": 113, "seek": 105300, "start": 1066.0, "end": 1072.0, "text": " Trying to remember, it's going to be later on that we're using the fact that we converted to dense.", "tokens": [20180, 281, 1604, 11, 309, 311, 516, 281, 312, 1780, 322, 300, 321, 434, 1228, 264, 1186, 300, 321, 16424, 281, 18011, 13], "temperature": 0.0, "avg_logprob": -0.17774070910553433, "compression_ratio": 1.403973509933775, "no_speech_prob": 0.01910344883799553}, {"id": 114, "seek": 105300, "start": 1072.0, "end": 1076.0, "text": " But a lot of a lot of operations can be done sparsely.", "tokens": [583, 257, 688, 295, 257, 688, 295, 7705, 393, 312, 1096, 637, 685, 736, 13], "temperature": 0.0, "avg_logprob": -0.17774070910553433, "compression_ratio": 1.403973509933775, "no_speech_prob": 0.01910344883799553}, {"id": 115, "seek": 107600, "start": 1076.0, "end": 1098.0, "text": " Sure.", "tokens": [4894, 13], "temperature": 0.0, "avg_logprob": -0.4381546974182129, "compression_ratio": 0.38461538461538464, "no_speech_prob": 1.2021550901408773e-05}, {"id": 116, "seek": 109800, "start": 1098.0, "end": 1110.0, "text": " And I'll there's a future lesson. There's a future lesson where I'll give more detail on exactly the SciPy sparse types.", "tokens": [400, 286, 603, 456, 311, 257, 2027, 6898, 13, 821, 311, 257, 2027, 6898, 689, 286, 603, 976, 544, 2607, 322, 2293, 264, 16942, 47, 88, 637, 11668, 3467, 13], "temperature": 0.0, "avg_logprob": -0.15170666411682798, "compression_ratio": 1.591549295774648, "no_speech_prob": 2.52105678555381e-06}, {"id": 117, "seek": 109800, "start": 1110.0, "end": 1114.0, "text": " But just in general, with a sparse matrix.", "tokens": [583, 445, 294, 2674, 11, 365, 257, 637, 11668, 8141, 13], "temperature": 0.0, "avg_logprob": -0.15170666411682798, "compression_ratio": 1.591549295774648, "no_speech_prob": 2.52105678555381e-06}, {"id": 118, "seek": 109800, "start": 1114.0, "end": 1120.0, "text": " So you can kind of think of a dense matrix like an Excel spreadsheet where you have a square for every space in the matrix.", "tokens": [407, 291, 393, 733, 295, 519, 295, 257, 18011, 8141, 411, 364, 19060, 27733, 689, 291, 362, 257, 3732, 337, 633, 1901, 294, 264, 8141, 13], "temperature": 0.0, "avg_logprob": -0.15170666411682798, "compression_ratio": 1.591549295774648, "no_speech_prob": 2.52105678555381e-06}, {"id": 119, "seek": 109800, "start": 1120.0, "end": 1123.0, "text": " And you're, you know, putting in the value of zero.", "tokens": [400, 291, 434, 11, 291, 458, 11, 3372, 294, 264, 2158, 295, 4018, 13], "temperature": 0.0, "avg_logprob": -0.15170666411682798, "compression_ratio": 1.591549295774648, "no_speech_prob": 2.52105678555381e-06}, {"id": 120, "seek": 112300, "start": 1123.0, "end": 1128.0, "text": " You're storing a zero there and you kind of have that block of memory for a sparse one.", "tokens": [509, 434, 26085, 257, 4018, 456, 293, 291, 733, 295, 362, 300, 3461, 295, 4675, 337, 257, 637, 11668, 472, 13], "temperature": 0.0, "avg_logprob": -0.11127379033472631, "compression_ratio": 1.5683060109289617, "no_speech_prob": 1.3005694199819118e-05}, {"id": 121, "seek": 112300, "start": 1128.0, "end": 1131.0, "text": " You only store the non zero values.", "tokens": [509, 787, 3531, 264, 2107, 4018, 4190, 13], "temperature": 0.0, "avg_logprob": -0.11127379033472631, "compression_ratio": 1.5683060109289617, "no_speech_prob": 1.3005694199819118e-05}, {"id": 122, "seek": 112300, "start": 1131.0, "end": 1135.0, "text": " So there.", "tokens": [407, 456, 13], "temperature": 0.0, "avg_logprob": -0.11127379033472631, "compression_ratio": 1.5683060109289617, "no_speech_prob": 1.3005694199819118e-05}, {"id": 123, "seek": 112300, "start": 1135.0, "end": 1143.0, "text": " It could it could conceivably just kind of be this list of you're going to need to have the row and coordinate because then to keep track of where it is.", "tokens": [467, 727, 309, 727, 10413, 592, 1188, 445, 733, 295, 312, 341, 1329, 295, 291, 434, 516, 281, 643, 281, 362, 264, 5386, 293, 15670, 570, 550, 281, 1066, 2837, 295, 689, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.11127379033472631, "compression_ratio": 1.5683060109289617, "no_speech_prob": 1.3005694199819118e-05}, {"id": 124, "seek": 114300, "start": 1143.0, "end": 1160.0, "text": " Actually, let me show the picture again.", "tokens": [5135, 11, 718, 385, 855, 264, 3036, 797, 13], "temperature": 0.0, "avg_logprob": -0.1369927406311035, "compression_ratio": 1.2566371681415929, "no_speech_prob": 3.500730826999643e-06}, {"id": 125, "seek": 114300, "start": 1160.0, "end": 1163.0, "text": " Yeah, so this is what a sparse matrix looks like.", "tokens": [865, 11, 370, 341, 307, 437, 257, 637, 11668, 8141, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.1369927406311035, "compression_ratio": 1.2566371681415929, "no_speech_prob": 3.500730826999643e-06}, {"id": 126, "seek": 114300, "start": 1163.0, "end": 1165.0, "text": " You have all these zeros.", "tokens": [509, 362, 439, 613, 35193, 13], "temperature": 0.0, "avg_logprob": -0.1369927406311035, "compression_ratio": 1.2566371681415929, "no_speech_prob": 3.500730826999643e-06}, {"id": 127, "seek": 114300, "start": 1165.0, "end": 1166.0, "text": " And so what you could do.", "tokens": [400, 370, 437, 291, 727, 360, 13], "temperature": 0.0, "avg_logprob": -0.1369927406311035, "compression_ratio": 1.2566371681415929, "no_speech_prob": 3.500730826999643e-06}, {"id": 128, "seek": 116600, "start": 1166.0, "end": 1184.0, "text": " Let me try this at writing on the screen.", "tokens": [961, 385, 853, 341, 412, 3579, 322, 264, 2568, 13], "temperature": 0.0, "avg_logprob": -0.14189306589273307, "compression_ratio": 1.3741007194244603, "no_speech_prob": 7.337856686717714e-07}, {"id": 129, "seek": 116600, "start": 1184.0, "end": 1185.0, "text": " Okay, great.", "tokens": [1033, 11, 869, 13], "temperature": 0.0, "avg_logprob": -0.14189306589273307, "compression_ratio": 1.3741007194244603, "no_speech_prob": 7.337856686717714e-07}, {"id": 130, "seek": 116600, "start": 1185.0, "end": 1193.0, "text": " So what I was going to do is say so this is kind of showing the the dense storage because you wrote out all these zeros again and again.", "tokens": [407, 437, 286, 390, 516, 281, 360, 307, 584, 370, 341, 307, 733, 295, 4099, 264, 264, 18011, 6725, 570, 291, 4114, 484, 439, 613, 35193, 797, 293, 797, 13], "temperature": 0.0, "avg_logprob": -0.14189306589273307, "compression_ratio": 1.3741007194244603, "no_speech_prob": 7.337856686717714e-07}, {"id": 131, "seek": 119300, "start": 1193.0, "end": 1198.0, "text": " Instead, you could just be like, okay, it plays zero coordinate zero zero.", "tokens": [7156, 11, 291, 727, 445, 312, 411, 11, 1392, 11, 309, 5749, 4018, 15670, 4018, 4018, 13], "temperature": 0.0, "avg_logprob": -0.19752735427663295, "compression_ratio": 1.6385542168674698, "no_speech_prob": 8.85126667071745e-07}, {"id": 132, "seek": 119300, "start": 1198.0, "end": 1201.0, "text": " I have a one a coordinate one one.", "tokens": [286, 362, 257, 472, 257, 15670, 472, 472, 13], "temperature": 0.0, "avg_logprob": -0.19752735427663295, "compression_ratio": 1.6385542168674698, "no_speech_prob": 8.85126667071745e-07}, {"id": 133, "seek": 119300, "start": 1201.0, "end": 1204.0, "text": " I have a one at coordinate to two.", "tokens": [286, 362, 257, 472, 412, 15670, 281, 732, 13], "temperature": 0.0, "avg_logprob": -0.19752735427663295, "compression_ratio": 1.6385542168674698, "no_speech_prob": 8.85126667071745e-07}, {"id": 134, "seek": 119300, "start": 1204.0, "end": 1208.0, "text": " I have a half and so on and kind of construct that list.", "tokens": [286, 362, 257, 1922, 293, 370, 322, 293, 733, 295, 7690, 300, 1329, 13], "temperature": 0.0, "avg_logprob": -0.19752735427663295, "compression_ratio": 1.6385542168674698, "no_speech_prob": 8.85126667071745e-07}, {"id": 135, "seek": 119300, "start": 1208.0, "end": 1210.0, "text": " And that would be sparse storage.", "tokens": [400, 300, 576, 312, 637, 11668, 6725, 13], "temperature": 0.0, "avg_logprob": -0.19752735427663295, "compression_ratio": 1.6385542168674698, "no_speech_prob": 8.85126667071745e-07}, {"id": 136, "seek": 119300, "start": 1210.0, "end": 1219.0, "text": " We'll give more details around that.", "tokens": [492, 603, 976, 544, 4365, 926, 300, 13], "temperature": 0.0, "avg_logprob": -0.19752735427663295, "compression_ratio": 1.6385542168674698, "no_speech_prob": 8.85126667071745e-07}, {"id": 137, "seek": 121900, "start": 1219.0, "end": 1225.0, "text": " Okay, so do you guys have your answer for confirming the decomposition?", "tokens": [1033, 11, 370, 360, 291, 1074, 362, 428, 1867, 337, 42861, 264, 48356, 30], "temperature": 0.0, "avg_logprob": -0.17817493585439828, "compression_ratio": 1.069767441860465, "no_speech_prob": 2.212135586887598e-05}, {"id": 138, "seek": 121900, "start": 1225.0, "end": 1232.0, "text": " And what did you do?", "tokens": [400, 437, 630, 291, 360, 30], "temperature": 0.0, "avg_logprob": -0.17817493585439828, "compression_ratio": 1.069767441860465, "no_speech_prob": 2.212135586887598e-05}, {"id": 139, "seek": 123200, "start": 1232.0, "end": 1261.0, "text": " So I want to say.", "tokens": [407, 286, 528, 281, 584, 13], "temperature": 0.0, "avg_logprob": -0.3338716745376587, "compression_ratio": 0.68, "no_speech_prob": 5.8272384194424376e-05}, {"id": 140, "seek": 126100, "start": 1261.0, "end": 1264.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.6099255879720052, "compression_ratio": 0.6875, "no_speech_prob": 0.0001022171345539391}, {"id": 141, "seek": 126400, "start": 1264.0, "end": 1292.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2526466449101766, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.0002650831884238869}, {"id": 142, "seek": 129200, "start": 1292.0, "end": 1294.0, "text": " So what I did.", "tokens": [407, 437, 286, 630, 13], "temperature": 0.0, "avg_logprob": -0.20635286966959634, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.061450984707335e-05}, {"id": 143, "seek": 129200, "start": 1294.0, "end": 1301.0, "text": " So I used NumPy has a method called NP.diag that will.", "tokens": [407, 286, 1143, 22592, 47, 88, 575, 257, 3170, 1219, 38611, 13, 4504, 559, 300, 486, 13], "temperature": 0.0, "avg_logprob": -0.20635286966959634, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.061450984707335e-05}, {"id": 144, "seek": 129200, "start": 1301.0, "end": 1307.0, "text": " So I thought you had kind of done a vector matrix product.", "tokens": [407, 286, 1194, 291, 632, 733, 295, 1096, 257, 8062, 8141, 1674, 13], "temperature": 0.0, "avg_logprob": -0.20635286966959634, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.061450984707335e-05}, {"id": 145, "seek": 129200, "start": 1307.0, "end": 1310.0, "text": " Yeah, to get the same thing.", "tokens": [865, 11, 281, 483, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.20635286966959634, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.061450984707335e-05}, {"id": 146, "seek": 129200, "start": 1310.0, "end": 1312.0, "text": " Diag is handy.", "tokens": [8789, 559, 307, 13239, 13], "temperature": 0.0, "avg_logprob": -0.20635286966959634, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.061450984707335e-05}, {"id": 147, "seek": 129200, "start": 1312.0, "end": 1317.0, "text": " It takes in a vector and makes a square matrix C with those as the values.", "tokens": [467, 2516, 294, 257, 8062, 293, 1669, 257, 3732, 8141, 383, 365, 729, 382, 264, 4190, 13], "temperature": 0.0, "avg_logprob": -0.20635286966959634, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.061450984707335e-05}, {"id": 148, "seek": 129200, "start": 1317.0, "end": 1319.0, "text": " Actually, it's a little bit confusing.", "tokens": [5135, 11, 309, 311, 257, 707, 857, 13181, 13], "temperature": 0.0, "avg_logprob": -0.20635286966959634, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.061450984707335e-05}, {"id": 149, "seek": 131900, "start": 1319.0, "end": 1322.0, "text": " Diag can go in both directions.", "tokens": [8789, 559, 393, 352, 294, 1293, 11095, 13], "temperature": 0.0, "avg_logprob": -0.09232687676089933, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.24722007260425e-05}, {"id": 150, "seek": 131900, "start": 1322.0, "end": 1324.0, "text": " If you give it a vector, it returns a matrix.", "tokens": [759, 291, 976, 309, 257, 8062, 11, 309, 11247, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.09232687676089933, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.24722007260425e-05}, {"id": 151, "seek": 131900, "start": 1324.0, "end": 1331.0, "text": " If you give it a matrix, it'll return a vector of what was on the diagonal.", "tokens": [759, 291, 976, 309, 257, 8141, 11, 309, 603, 2736, 257, 8062, 295, 437, 390, 322, 264, 21539, 13], "temperature": 0.0, "avg_logprob": -0.09232687676089933, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.24722007260425e-05}, {"id": 152, "seek": 131900, "start": 1331.0, "end": 1335.0, "text": " That's kind of a shortcut.", "tokens": [663, 311, 733, 295, 257, 24822, 13], "temperature": 0.0, "avg_logprob": -0.09232687676089933, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.24722007260425e-05}, {"id": 153, "seek": 131900, "start": 1335.0, "end": 1336.0, "text": " Oh, okay.", "tokens": [876, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.09232687676089933, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.24722007260425e-05}, {"id": 154, "seek": 131900, "start": 1336.0, "end": 1338.0, "text": " And so the question was about at.", "tokens": [400, 370, 264, 1168, 390, 466, 412, 13], "temperature": 0.0, "avg_logprob": -0.09232687676089933, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.24722007260425e-05}, {"id": 155, "seek": 131900, "start": 1338.0, "end": 1341.0, "text": " This is in Python three.", "tokens": [639, 307, 294, 15329, 1045, 13], "temperature": 0.0, "avg_logprob": -0.09232687676089933, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.24722007260425e-05}, {"id": 156, "seek": 131900, "start": 1341.0, "end": 1344.0, "text": " At is matrix multiplication.", "tokens": [1711, 307, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.09232687676089933, "compression_ratio": 1.5706214689265536, "no_speech_prob": 2.24722007260425e-05}, {"id": 157, "seek": 134400, "start": 1344.0, "end": 1350.0, "text": " And we talked briefly last time.", "tokens": [400, 321, 2825, 10515, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.11971878051757813, "compression_ratio": 1.6440677966101696, "no_speech_prob": 2.9306735086720437e-05}, {"id": 158, "seek": 134400, "start": 1350.0, "end": 1355.0, "text": " About how you can have both Python two and Python three installed on your computer.", "tokens": [7769, 577, 291, 393, 362, 1293, 15329, 732, 293, 15329, 1045, 8899, 322, 428, 3820, 13], "temperature": 0.0, "avg_logprob": -0.11971878051757813, "compression_ratio": 1.6440677966101696, "no_speech_prob": 2.9306735086720437e-05}, {"id": 159, "seek": 134400, "start": 1355.0, "end": 1359.0, "text": " I definitely recommend switching to Python three for this class or in general.", "tokens": [286, 2138, 2748, 16493, 281, 15329, 1045, 337, 341, 1508, 420, 294, 2674, 13], "temperature": 0.0, "avg_logprob": -0.11971878051757813, "compression_ratio": 1.6440677966101696, "no_speech_prob": 2.9306735086720437e-05}, {"id": 160, "seek": 134400, "start": 1359.0, "end": 1361.0, "text": " I recommend Python three.", "tokens": [286, 2748, 15329, 1045, 13], "temperature": 0.0, "avg_logprob": -0.11971878051757813, "compression_ratio": 1.6440677966101696, "no_speech_prob": 2.9306735086720437e-05}, {"id": 161, "seek": 134400, "start": 1361.0, "end": 1362.0, "text": " But you don't have to.", "tokens": [583, 291, 500, 380, 362, 281, 13], "temperature": 0.0, "avg_logprob": -0.11971878051757813, "compression_ratio": 1.6440677966101696, "no_speech_prob": 2.9306735086720437e-05}, {"id": 162, "seek": 134400, "start": 1362.0, "end": 1365.0, "text": " And it's fine to do this course in Python two.", "tokens": [400, 309, 311, 2489, 281, 360, 341, 1164, 294, 15329, 732, 13], "temperature": 0.0, "avg_logprob": -0.11971878051757813, "compression_ratio": 1.6440677966101696, "no_speech_prob": 2.9306735086720437e-05}, {"id": 163, "seek": 136500, "start": 1365.0, "end": 1374.0, "text": " And if I was in Python two, I would use np.matmul, which is matrix multiplication.", "tokens": [400, 498, 286, 390, 294, 15329, 732, 11, 286, 576, 764, 297, 79, 13, 15677, 76, 425, 11, 597, 307, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.29461406071980795, "compression_ratio": 1.205128205128205, "no_speech_prob": 2.4296214178320952e-05}, {"id": 164, "seek": 136500, "start": 1374.0, "end": 1378.0, "text": " What is it from np.dot?", "tokens": [708, 307, 309, 490, 297, 79, 13, 43494, 30], "temperature": 0.0, "avg_logprob": -0.29461406071980795, "compression_ratio": 1.205128205128205, "no_speech_prob": 2.4296214178320952e-05}, {"id": 165, "seek": 136500, "start": 1378.0, "end": 1379.0, "text": " From?", "tokens": [3358, 30], "temperature": 0.0, "avg_logprob": -0.29461406071980795, "compression_ratio": 1.205128205128205, "no_speech_prob": 2.4296214178320952e-05}, {"id": 166, "seek": 136500, "start": 1379.0, "end": 1380.0, "text": " From what?", "tokens": [3358, 437, 30], "temperature": 0.0, "avg_logprob": -0.29461406071980795, "compression_ratio": 1.205128205128205, "no_speech_prob": 2.4296214178320952e-05}, {"id": 167, "seek": 136500, "start": 1380.0, "end": 1383.0, "text": " Oh, np.dot.", "tokens": [876, 11, 297, 79, 13, 43494, 13], "temperature": 0.0, "avg_logprob": -0.29461406071980795, "compression_ratio": 1.205128205128205, "no_speech_prob": 2.4296214178320952e-05}, {"id": 168, "seek": 136500, "start": 1383.0, "end": 1387.0, "text": " Sure.", "tokens": [4894, 13], "temperature": 0.0, "avg_logprob": -0.29461406071980795, "compression_ratio": 1.205128205128205, "no_speech_prob": 2.4296214178320952e-05}, {"id": 169, "seek": 138700, "start": 1387.0, "end": 1396.0, "text": " So np.dot is kind of cool, but also kind of annoying.", "tokens": [407, 297, 79, 13, 43494, 307, 733, 295, 1627, 11, 457, 611, 733, 295, 11304, 13], "temperature": 0.0, "avg_logprob": -0.326786811535175, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0004895513993687928}, {"id": 170, "seek": 138700, "start": 1396.0, "end": 1397.0, "text": " It does a lot of magic.", "tokens": [467, 775, 257, 688, 295, 5585, 13], "temperature": 0.0, "avg_logprob": -0.326786811535175, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0004895513993687928}, {"id": 171, "seek": 138700, "start": 1397.0, "end": 1398.0, "text": " So it tends to regardless of what dimensions you give it, it tries to find a way to make it work.", "tokens": [407, 309, 12258, 281, 10060, 295, 437, 12819, 291, 976, 309, 11, 309, 9898, 281, 915, 257, 636, 281, 652, 309, 589, 13], "temperature": 0.0, "avg_logprob": -0.326786811535175, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0004895513993687928}, {"id": 172, "seek": 138700, "start": 1398.0, "end": 1403.0, "text": " So the documentation for np.dot describes all of those in detail.", "tokens": [407, 264, 14333, 337, 297, 79, 13, 43494, 15626, 439, 295, 729, 294, 2607, 13], "temperature": 0.0, "avg_logprob": -0.326786811535175, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0004895513993687928}, {"id": 173, "seek": 138700, "start": 1403.0, "end": 1408.0, "text": " So sometimes that's good, but sometimes it's like you have a bargain you can barely realize", "tokens": [407, 2171, 300, 311, 665, 11, 457, 2171, 309, 311, 411, 291, 362, 257, 34302, 291, 393, 10268, 4325], "temperature": 0.0, "avg_logprob": -0.326786811535175, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0004895513993687928}, {"id": 174, "seek": 138700, "start": 1408.0, "end": 1414.0, "text": " because it's something that you can't believe.", "tokens": [570, 309, 311, 746, 300, 291, 393, 380, 1697, 13], "temperature": 0.0, "avg_logprob": -0.326786811535175, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0004895513993687928}, {"id": 175, "seek": 141400, "start": 1414.0, "end": 1422.0, "text": " And then actually, yes?", "tokens": [400, 550, 767, 11, 2086, 30], "temperature": 0.0, "avg_logprob": -0.26087741575379303, "compression_ratio": 1.3724137931034484, "no_speech_prob": 1.8339138478040695e-05}, {"id": 176, "seek": 141400, "start": 1422.0, "end": 1425.0, "text": " Oh.", "tokens": [876, 13], "temperature": 0.0, "avg_logprob": -0.26087741575379303, "compression_ratio": 1.3724137931034484, "no_speech_prob": 1.8339138478040695e-05}, {"id": 177, "seek": 141400, "start": 1425.0, "end": 1429.0, "text": " Could you also use the np.all course?", "tokens": [7497, 291, 611, 764, 264, 297, 79, 13, 336, 1164, 30], "temperature": 0.0, "avg_logprob": -0.26087741575379303, "compression_ratio": 1.3724137931034484, "no_speech_prob": 1.8339138478040695e-05}, {"id": 178, "seek": 141400, "start": 1429.0, "end": 1431.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.26087741575379303, "compression_ratio": 1.3724137931034484, "no_speech_prob": 1.8339138478040695e-05}, {"id": 179, "seek": 141400, "start": 1431.0, "end": 1433.0, "text": " Yeah, let me do that.", "tokens": [865, 11, 718, 385, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.26087741575379303, "compression_ratio": 1.3724137931034484, "no_speech_prob": 1.8339138478040695e-05}, {"id": 180, "seek": 141400, "start": 1433.0, "end": 1441.0, "text": " And I actually, normally I prefer np.all close, so I'm kind of surprised that I did this subtraction here.", "tokens": [400, 286, 767, 11, 5646, 286, 4382, 297, 79, 13, 336, 1998, 11, 370, 286, 478, 733, 295, 6100, 300, 286, 630, 341, 16390, 313, 510, 13], "temperature": 0.0, "avg_logprob": -0.26087741575379303, "compression_ratio": 1.3724137931034484, "no_speech_prob": 1.8339138478040695e-05}, {"id": 181, "seek": 144100, "start": 1441.0, "end": 1453.0, "text": " Let me do that too.", "tokens": [961, 385, 360, 300, 886, 13], "temperature": 0.0, "avg_logprob": -0.18921814895257716, "compression_ratio": 1.0978260869565217, "no_speech_prob": 6.399777339538559e-05}, {"id": 182, "seek": 144100, "start": 1453.0, "end": 1464.0, "text": " Oh.", "tokens": [876, 13], "temperature": 0.0, "avg_logprob": -0.18921814895257716, "compression_ratio": 1.0978260869565217, "no_speech_prob": 6.399777339538559e-05}, {"id": 183, "seek": 144100, "start": 1464.0, "end": 1465.0, "text": " Hmm.", "tokens": [8239, 13], "temperature": 0.0, "avg_logprob": -0.18921814895257716, "compression_ratio": 1.0978260869565217, "no_speech_prob": 6.399777339538559e-05}, {"id": 184, "seek": 144100, "start": 1465.0, "end": 1467.0, "text": " Okay, well, I will look into this later.", "tokens": [1033, 11, 731, 11, 286, 486, 574, 666, 341, 1780, 13], "temperature": 0.0, "avg_logprob": -0.18921814895257716, "compression_ratio": 1.0978260869565217, "no_speech_prob": 6.399777339538559e-05}, {"id": 185, "seek": 144100, "start": 1467.0, "end": 1470.0, "text": " Did you use all close on yours?", "tokens": [2589, 291, 764, 439, 1998, 322, 6342, 30], "temperature": 0.0, "avg_logprob": -0.18921814895257716, "compression_ratio": 1.0978260869565217, "no_speech_prob": 6.399777339538559e-05}, {"id": 186, "seek": 147000, "start": 1470.0, "end": 1475.0, "text": " I might have changed these variables lower down and be getting something different.", "tokens": [286, 1062, 362, 3105, 613, 9102, 3126, 760, 293, 312, 1242, 746, 819, 13], "temperature": 0.0, "avg_logprob": -0.13862189981672499, "compression_ratio": 1.3828571428571428, "no_speech_prob": 3.218437996110879e-05}, {"id": 187, "seek": 147000, "start": 1475.0, "end": 1477.0, "text": " So, okay, great.", "tokens": [407, 11, 1392, 11, 869, 13], "temperature": 0.0, "avg_logprob": -0.13862189981672499, "compression_ratio": 1.3828571428571428, "no_speech_prob": 3.218437996110879e-05}, {"id": 188, "seek": 147000, "start": 1477.0, "end": 1479.0, "text": " Yeah, no, I like np.all close.", "tokens": [865, 11, 572, 11, 286, 411, 297, 79, 13, 336, 1998, 13], "temperature": 0.0, "avg_logprob": -0.13862189981672499, "compression_ratio": 1.3828571428571428, "no_speech_prob": 3.218437996110879e-05}, {"id": 189, "seek": 147000, "start": 1479.0, "end": 1480.0, "text": " And that's checking.", "tokens": [400, 300, 311, 8568, 13], "temperature": 0.0, "avg_logprob": -0.13862189981672499, "compression_ratio": 1.3828571428571428, "no_speech_prob": 3.218437996110879e-05}, {"id": 190, "seek": 147000, "start": 1480.0, "end": 1487.0, "text": " It takes matrices and checks that all the, each element is similar.", "tokens": [467, 2516, 32284, 293, 13834, 300, 439, 264, 11, 1184, 4478, 307, 2531, 13], "temperature": 0.0, "avg_logprob": -0.13862189981672499, "compression_ratio": 1.3828571428571428, "no_speech_prob": 3.218437996110879e-05}, {"id": 191, "seek": 147000, "start": 1487.0, "end": 1488.0, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.13862189981672499, "compression_ratio": 1.3828571428571428, "no_speech_prob": 3.218437996110879e-05}, {"id": 192, "seek": 147000, "start": 1488.0, "end": 1490.0, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.13862189981672499, "compression_ratio": 1.3828571428571428, "no_speech_prob": 3.218437996110879e-05}, {"id": 193, "seek": 149000, "start": 1490.0, "end": 1504.0, "text": " I thought these, I mean, so I actually haven't been running this as I go through.", "tokens": [286, 1194, 613, 11, 286, 914, 11, 370, 286, 767, 2378, 380, 668, 2614, 341, 382, 286, 352, 807, 13], "temperature": 0.0, "avg_logprob": -0.12831875681877136, "compression_ratio": 1.310077519379845, "no_speech_prob": 2.857163735825452e-06}, {"id": 194, "seek": 149000, "start": 1504.0, "end": 1513.0, "text": " Oh, but, because my vectors have been converted to dense.", "tokens": [876, 11, 457, 11, 570, 452, 18875, 362, 668, 16424, 281, 18011, 13], "temperature": 0.0, "avg_logprob": -0.12831875681877136, "compression_ratio": 1.310077519379845, "no_speech_prob": 2.857163735825452e-06}, {"id": 195, "seek": 149000, "start": 1513.0, "end": 1516.0, "text": " I would prefer to keep going.", "tokens": [286, 576, 4382, 281, 1066, 516, 13], "temperature": 0.0, "avg_logprob": -0.12831875681877136, "compression_ratio": 1.310077519379845, "no_speech_prob": 2.857163735825452e-06}, {"id": 196, "seek": 151600, "start": 1516.0, "end": 1521.0, "text": " I think that this is an artifact that I haven't been running the previous ones.", "tokens": [286, 519, 300, 341, 307, 364, 34806, 300, 286, 2378, 380, 668, 2614, 264, 3894, 2306, 13], "temperature": 0.0, "avg_logprob": -0.2077975834117216, "compression_ratio": 1.422077922077922, "no_speech_prob": 1.2605911251739599e-05}, {"id": 197, "seek": 151600, "start": 1521.0, "end": 1522.0, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.2077975834117216, "compression_ratio": 1.422077922077922, "no_speech_prob": 1.2605911251739599e-05}, {"id": 198, "seek": 151600, "start": 1522.0, "end": 1527.0, "text": " Let me pass the microphone.", "tokens": [961, 385, 1320, 264, 10952, 13], "temperature": 0.0, "avg_logprob": -0.2077975834117216, "compression_ratio": 1.422077922077922, "no_speech_prob": 1.2605911251739599e-05}, {"id": 199, "seek": 151600, "start": 1527.0, "end": 1534.0, "text": " Sorry.", "tokens": [4919, 13], "temperature": 0.0, "avg_logprob": -0.2077975834117216, "compression_ratio": 1.422077922077922, "no_speech_prob": 1.2605911251739599e-05}, {"id": 200, "seek": 151600, "start": 1534.0, "end": 1543.0, "text": " So, what is the function np.the linear, whatever the norm that norm is?", "tokens": [407, 11, 437, 307, 264, 2445, 297, 79, 13, 3322, 8213, 11, 2035, 264, 2026, 300, 2026, 307, 30], "temperature": 0.0, "avg_logprob": -0.2077975834117216, "compression_ratio": 1.422077922077922, "no_speech_prob": 1.2605911251739599e-05}, {"id": 201, "seek": 151600, "start": 1543.0, "end": 1545.0, "text": " Oh, that's a good question.", "tokens": [876, 11, 300, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2077975834117216, "compression_ratio": 1.422077922077922, "no_speech_prob": 1.2605911251739599e-05}, {"id": 202, "seek": 154500, "start": 1545.0, "end": 1552.0, "text": " So, norms in general give you kind of the distance between two things as a way of thinking of it.", "tokens": [407, 11, 24357, 294, 2674, 976, 291, 733, 295, 264, 4560, 1296, 732, 721, 382, 257, 636, 295, 1953, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.11486611695125185, "compression_ratio": 1.656, "no_speech_prob": 2.0782701540156268e-05}, {"id": 203, "seek": 154500, "start": 1552.0, "end": 1558.0, "text": " And this I would believe would default to the L2 norm, but that's a what?", "tokens": [400, 341, 286, 576, 1697, 576, 7576, 281, 264, 441, 17, 2026, 11, 457, 300, 311, 257, 437, 30], "temperature": 0.0, "avg_logprob": -0.11486611695125185, "compression_ratio": 1.656, "no_speech_prob": 2.0782701540156268e-05}, {"id": 204, "seek": 154500, "start": 1558.0, "end": 1559.0, "text": " The Frobenius norm.", "tokens": [440, 25028, 1799, 4872, 2026, 13], "temperature": 0.0, "avg_logprob": -0.11486611695125185, "compression_ratio": 1.656, "no_speech_prob": 2.0782701540156268e-05}, {"id": 205, "seek": 154500, "start": 1559.0, "end": 1561.0, "text": " The Frobenius norm, okay.", "tokens": [440, 25028, 1799, 4872, 2026, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.11486611695125185, "compression_ratio": 1.656, "no_speech_prob": 2.0782701540156268e-05}, {"id": 206, "seek": 154500, "start": 1561.0, "end": 1563.0, "text": " And you can always pass an argument.", "tokens": [400, 291, 393, 1009, 1320, 364, 6770, 13], "temperature": 0.0, "avg_logprob": -0.11486611695125185, "compression_ratio": 1.656, "no_speech_prob": 2.0782701540156268e-05}, {"id": 207, "seek": 154500, "start": 1563.0, "end": 1569.0, "text": " But so what that's doing is we're taking the difference, and that would be a matrix of things that are close to zero,", "tokens": [583, 370, 437, 300, 311, 884, 307, 321, 434, 1940, 264, 2649, 11, 293, 300, 576, 312, 257, 8141, 295, 721, 300, 366, 1998, 281, 4018, 11], "temperature": 0.0, "avg_logprob": -0.11486611695125185, "compression_ratio": 1.656, "no_speech_prob": 2.0782701540156268e-05}, {"id": 208, "seek": 154500, "start": 1569.0, "end": 1571.0, "text": " but I just wanted to see a single number.", "tokens": [457, 286, 445, 1415, 281, 536, 257, 2167, 1230, 13], "temperature": 0.0, "avg_logprob": -0.11486611695125185, "compression_ratio": 1.656, "no_speech_prob": 2.0782701540156268e-05}, {"id": 209, "seek": 157100, "start": 1571.0, "end": 1576.0, "text": " So, I used the norm to kind of get that down to a single number.", "tokens": [407, 11, 286, 1143, 264, 2026, 281, 733, 295, 483, 300, 760, 281, 257, 2167, 1230, 13], "temperature": 0.0, "avg_logprob": -0.09050090729244172, "compression_ratio": 1.3694267515923566, "no_speech_prob": 2.0144336303928867e-05}, {"id": 210, "seek": 157100, "start": 1576.0, "end": 1581.0, "text": " And norms you can think of as size, typically.", "tokens": [400, 24357, 291, 393, 519, 295, 382, 2744, 11, 5850, 13], "temperature": 0.0, "avg_logprob": -0.09050090729244172, "compression_ratio": 1.3694267515923566, "no_speech_prob": 2.0144336303928867e-05}, {"id": 211, "seek": 157100, "start": 1581.0, "end": 1586.0, "text": " Yes?", "tokens": [1079, 30], "temperature": 0.0, "avg_logprob": -0.09050090729244172, "compression_ratio": 1.3694267515923566, "no_speech_prob": 2.0144336303928867e-05}, {"id": 212, "seek": 157100, "start": 1586.0, "end": 1596.0, "text": " Yeah, so you're kind of squaring each element and adding them all together for the Frobenius norm.", "tokens": [865, 11, 370, 291, 434, 733, 295, 2339, 1921, 1184, 4478, 293, 5127, 552, 439, 1214, 337, 264, 25028, 1799, 4872, 2026, 13], "temperature": 0.0, "avg_logprob": -0.09050090729244172, "compression_ratio": 1.3694267515923566, "no_speech_prob": 2.0144336303928867e-05}, {"id": 213, "seek": 159600, "start": 1596.0, "end": 1602.0, "text": " Okay, next up, I want you to confirm that u and v are orthonormal.", "tokens": [1033, 11, 958, 493, 11, 286, 528, 291, 281, 9064, 300, 344, 293, 371, 366, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.11583516597747803, "compression_ratio": 1.1981132075471699, "no_speech_prob": 5.143354428582825e-05}, {"id": 214, "seek": 159600, "start": 1602.0, "end": 1620.0, "text": " So, just take a moment to write in some code that does that.", "tokens": [407, 11, 445, 747, 257, 1623, 281, 2464, 294, 512, 3089, 300, 775, 300, 13], "temperature": 0.0, "avg_logprob": -0.11583516597747803, "compression_ratio": 1.1981132075471699, "no_speech_prob": 5.143354428582825e-05}, {"id": 215, "seek": 162000, "start": 1620.0, "end": 1626.0, "text": " Oh, so that's something I do.", "tokens": [876, 11, 370, 300, 311, 746, 286, 360, 13], "temperature": 0.0, "avg_logprob": -0.07882855036487318, "compression_ratio": 1.5151515151515151, "no_speech_prob": 1.644099029363133e-05}, {"id": 216, "seek": 162000, "start": 1626.0, "end": 1638.0, "text": " This depends on how people define it, but a lot of definitions of SVD distinguish that you're actually getting back the transpose of v as opposed to v itself.", "tokens": [639, 5946, 322, 577, 561, 6964, 309, 11, 457, 257, 688, 295, 21988, 295, 31910, 35, 20206, 300, 291, 434, 767, 1242, 646, 264, 25167, 295, 371, 382, 8851, 281, 371, 2564, 13], "temperature": 0.0, "avg_logprob": -0.07882855036487318, "compression_ratio": 1.5151515151515151, "no_speech_prob": 1.644099029363133e-05}, {"id": 217, "seek": 162000, "start": 1638.0, "end": 1647.0, "text": " You can ignore that, but you will kind of see this difference about what people define as v versus v transpose.", "tokens": [509, 393, 11200, 300, 11, 457, 291, 486, 733, 295, 536, 341, 2649, 466, 437, 561, 6964, 382, 371, 5717, 371, 25167, 13], "temperature": 0.0, "avg_logprob": -0.07882855036487318, "compression_ratio": 1.5151515151515151, "no_speech_prob": 1.644099029363133e-05}, {"id": 218, "seek": 164700, "start": 1647.0, "end": 1654.0, "text": " And the H stands for Hermitian, which is kind of the equivalent to transpose when you have complex numbers.", "tokens": [400, 264, 389, 7382, 337, 21842, 270, 952, 11, 597, 307, 733, 295, 264, 10344, 281, 25167, 562, 291, 362, 3997, 3547, 13], "temperature": 0.0, "avg_logprob": -0.0939989619784885, "compression_ratio": 1.1263157894736842, "no_speech_prob": 1.8056291082757525e-05}, {"id": 219, "seek": 165400, "start": 1654.0, "end": 1683.0, "text": " But we'll be sticking with real numbers in here.", "tokens": [583, 321, 603, 312, 13465, 365, 957, 3547, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.08156720797220866, "compression_ratio": 0.8571428571428571, "no_speech_prob": 0.00026098883245140314}, {"id": 220, "seek": 168300, "start": 1683.0, "end": 1708.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.6299332777659098, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.01227664016187191}, {"id": 221, "seek": 170800, "start": 1708.0, "end": 1734.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.4429255723953247, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.004877506289631128}, {"id": 222, "seek": 173400, "start": 1734.0, "end": 1763.0, "text": " Raise your hand if you want a little bit more time.", "tokens": [30062, 428, 1011, 498, 291, 528, 257, 707, 857, 544, 565, 13], "temperature": 0.0, "avg_logprob": -0.10454791784286499, "compression_ratio": 0.9107142857142857, "no_speech_prob": 4.7092189561226405e-06}, {"id": 223, "seek": 176300, "start": 1763.0, "end": 1774.0, "text": " Raise your hand if you have the answer.", "tokens": [30062, 428, 1011, 498, 291, 362, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.0870375449840839, "compression_ratio": 0.9069767441860465, "no_speech_prob": 7.645973710168619e-06}, {"id": 224, "seek": 177400, "start": 1774.0, "end": 1803.0, "text": " And I should be clear, when I say u and v are orthonormal, I don't mean to each other. I mean that the columns of u are orthonormal to the other columns, and the rows of v are orthonormal to the other rows.", "tokens": [400, 286, 820, 312, 1850, 11, 562, 286, 584, 344, 293, 371, 366, 420, 11943, 24440, 11, 286, 500, 380, 914, 281, 1184, 661, 13, 286, 914, 300, 264, 13766, 295, 344, 366, 420, 11943, 24440, 281, 264, 661, 13766, 11, 293, 264, 13241, 295, 371, 366, 420, 11943, 24440, 281, 264, 661, 13241, 13], "temperature": 0.0, "avg_logprob": -0.05231070922592939, "compression_ratio": 1.6612903225806452, "no_speech_prob": 1.891768079076428e-05}, {"id": 225, "seek": 180300, "start": 1803.0, "end": 1808.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.28509998321533203, "compression_ratio": 0.9428571428571428, "no_speech_prob": 9.310063614975661e-05}, {"id": 226, "seek": 180300, "start": 1808.0, "end": 1820.0, "text": " Does someone want to share their answer? What they did?", "tokens": [4402, 1580, 528, 281, 2073, 641, 1867, 30, 708, 436, 630, 30], "temperature": 0.0, "avg_logprob": -0.28509998321533203, "compression_ratio": 0.9428571428571428, "no_speech_prob": 9.310063614975661e-05}, {"id": 227, "seek": 182000, "start": 1820.0, "end": 1839.0, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.12407341798146566, "compression_ratio": 0.8, "no_speech_prob": 8.396629709750414e-06}, {"id": 228, "seek": 182000, "start": 1839.0, "end": 1845.0, "text": " Exactly. Yeah, thank you.", "tokens": [7587, 13, 865, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.12407341798146566, "compression_ratio": 0.8, "no_speech_prob": 8.396629709750414e-06}, {"id": 229, "seek": 184500, "start": 1845.0, "end": 1854.0, "text": " So what you can do is multiply them by their transpose and then compare them to the identity.", "tokens": [407, 437, 291, 393, 360, 307, 12972, 552, 538, 641, 25167, 293, 550, 6794, 552, 281, 264, 6575, 13], "temperature": 0.0, "avg_logprob": -0.11344261169433593, "compression_ratio": 1.5422535211267605, "no_speech_prob": 4.494787390285637e-06}, {"id": 230, "seek": 184500, "start": 1854.0, "end": 1872.0, "text": " And the reason this works is multiplying u by u transpose is multiplying, and actually one of these I guess I have backwards.", "tokens": [400, 264, 1778, 341, 1985, 307, 30955, 344, 538, 344, 25167, 307, 30955, 11, 293, 767, 472, 295, 613, 286, 2041, 286, 362, 12204, 13], "temperature": 0.0, "avg_logprob": -0.11344261169433593, "compression_ratio": 1.5422535211267605, "no_speech_prob": 4.494787390285637e-06}, {"id": 231, "seek": 187200, "start": 1872.0, "end": 1879.0, "text": " Each column should maybe be this.", "tokens": [6947, 7738, 820, 1310, 312, 341, 13], "temperature": 0.0, "avg_logprob": -0.14647121990428252, "compression_ratio": 1.5684931506849316, "no_speech_prob": 1.1299857760604937e-05}, {"id": 232, "seek": 187200, "start": 1879.0, "end": 1888.0, "text": " But the kind of multiplies each column of u. Yeah. So taking the transpose, you get the rows by the columns.", "tokens": [583, 264, 733, 295, 12788, 530, 1184, 7738, 295, 344, 13, 865, 13, 407, 1940, 264, 25167, 11, 291, 483, 264, 13241, 538, 264, 13766, 13], "temperature": 0.0, "avg_logprob": -0.14647121990428252, "compression_ratio": 1.5684931506849316, "no_speech_prob": 1.1299857760604937e-05}, {"id": 233, "seek": 187200, "start": 1888.0, "end": 1893.0, "text": " That's kind of column of u by column of u with v.", "tokens": [663, 311, 733, 295, 7738, 295, 344, 538, 7738, 295, 344, 365, 371, 13], "temperature": 0.0, "avg_logprob": -0.14647121990428252, "compression_ratio": 1.5684931506849316, "no_speech_prob": 1.1299857760604937e-05}, {"id": 234, "seek": 187200, "start": 1893.0, "end": 1895.0, "text": " You're getting row of v by row of v.", "tokens": [509, 434, 1242, 5386, 295, 371, 538, 5386, 295, 371, 13], "temperature": 0.0, "avg_logprob": -0.14647121990428252, "compression_ratio": 1.5684931506849316, "no_speech_prob": 1.1299857760604937e-05}, {"id": 235, "seek": 189500, "start": 1895.0, "end": 1903.0, "text": " And then when they're kind of the same row by itself, that's the diagonal of the identity.", "tokens": [400, 550, 562, 436, 434, 733, 295, 264, 912, 5386, 538, 2564, 11, 300, 311, 264, 21539, 295, 264, 6575, 13], "temperature": 0.0, "avg_logprob": -0.11184192039597203, "compression_ratio": 1.558139534883721, "no_speech_prob": 7.071577783790417e-06}, {"id": 236, "seek": 189500, "start": 1903.0, "end": 1907.0, "text": " That's why you're getting ones, different kind of two different vectors.", "tokens": [663, 311, 983, 291, 434, 1242, 2306, 11, 819, 733, 295, 732, 819, 18875, 13], "temperature": 0.0, "avg_logprob": -0.11184192039597203, "compression_ratio": 1.558139534883721, "no_speech_prob": 7.071577783790417e-06}, {"id": 237, "seek": 189500, "start": 1907.0, "end": 1910.0, "text": " Those are the off diagonals, the zeros.", "tokens": [3950, 366, 264, 766, 17405, 1124, 11, 264, 35193, 13], "temperature": 0.0, "avg_logprob": -0.11184192039597203, "compression_ratio": 1.558139534883721, "no_speech_prob": 7.071577783790417e-06}, {"id": 238, "seek": 189500, "start": 1910.0, "end": 1916.0, "text": " Any questions about that?", "tokens": [2639, 1651, 466, 300, 30], "temperature": 0.0, "avg_logprob": -0.11184192039597203, "compression_ratio": 1.558139534883721, "no_speech_prob": 7.071577783790417e-06}, {"id": 239, "seek": 189500, "start": 1916.0, "end": 1922.0, "text": " And actually let me draw that briefly.", "tokens": [400, 767, 718, 385, 2642, 300, 10515, 13], "temperature": 0.0, "avg_logprob": -0.11184192039597203, "compression_ratio": 1.558139534883721, "no_speech_prob": 7.071577783790417e-06}, {"id": 240, "seek": 192200, "start": 1922.0, "end": 1934.0, "text": " So the idea is if this was u, you've got these columns.", "tokens": [407, 264, 1558, 307, 498, 341, 390, 344, 11, 291, 600, 658, 613, 13766, 13], "temperature": 0.0, "avg_logprob": -0.1302694466154454, "compression_ratio": 1.4475524475524475, "no_speech_prob": 2.5070543415495194e-05}, {"id": 241, "seek": 192200, "start": 1934.0, "end": 1937.0, "text": " This is u transpose.", "tokens": [639, 307, 344, 25167, 13], "temperature": 0.0, "avg_logprob": -0.1302694466154454, "compression_ratio": 1.4475524475524475, "no_speech_prob": 2.5070543415495194e-05}, {"id": 242, "seek": 192200, "start": 1937.0, "end": 1944.0, "text": " When you multiply those together, you'll kind of end up taking this vector times this vector and so on.", "tokens": [1133, 291, 12972, 729, 1214, 11, 291, 603, 733, 295, 917, 493, 1940, 341, 8062, 1413, 341, 8062, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.1302694466154454, "compression_ratio": 1.4475524475524475, "no_speech_prob": 2.5070543415495194e-05}, {"id": 243, "seek": 192200, "start": 1944.0, "end": 1947.0, "text": " And so that should be one.", "tokens": [400, 370, 300, 820, 312, 472, 13], "temperature": 0.0, "avg_logprob": -0.1302694466154454, "compression_ratio": 1.4475524475524475, "no_speech_prob": 2.5070543415495194e-05}, {"id": 244, "seek": 194700, "start": 1947.0, "end": 1962.0, "text": " And doing this two different ones, that's going to give you zero and so on.", "tokens": [400, 884, 341, 732, 819, 2306, 11, 300, 311, 516, 281, 976, 291, 4018, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.10432759920756023, "compression_ratio": 1.4806629834254144, "no_speech_prob": 8.714168302503822e-07}, {"id": 245, "seek": 194700, "start": 1962.0, "end": 1963.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.10432759920756023, "compression_ratio": 1.4806629834254144, "no_speech_prob": 8.714168302503822e-07}, {"id": 246, "seek": 194700, "start": 1963.0, "end": 1968.0, "text": " And so now kind of confirmed that we got what we expected with the SVD.", "tokens": [400, 370, 586, 733, 295, 11341, 300, 321, 658, 437, 321, 5176, 365, 264, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.10432759920756023, "compression_ratio": 1.4806629834254144, "no_speech_prob": 8.714168302503822e-07}, {"id": 247, "seek": 194700, "start": 1968.0, "end": 1971.0, "text": " We can look at the singular values.", "tokens": [492, 393, 574, 412, 264, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.10432759920756023, "compression_ratio": 1.4806629834254144, "no_speech_prob": 8.714168302503822e-07}, {"id": 248, "seek": 194700, "start": 1971.0, "end": 1976.0, "text": " And remember that these are giving us kind of this measure of importance.", "tokens": [400, 1604, 300, 613, 366, 2902, 505, 733, 295, 341, 3481, 295, 7379, 13], "temperature": 0.0, "avg_logprob": -0.10432759920756023, "compression_ratio": 1.4806629834254144, "no_speech_prob": 8.714168302503822e-07}, {"id": 249, "seek": 197600, "start": 1976.0, "end": 1981.0, "text": " And notice that it drops off very quickly.", "tokens": [400, 3449, 300, 309, 11438, 766, 588, 2661, 13], "temperature": 0.0, "avg_logprob": -0.07791505951479256, "compression_ratio": 1.6540284360189574, "no_speech_prob": 1.0450865374878049e-05}, {"id": 250, "seek": 197600, "start": 1981.0, "end": 1988.0, "text": " So kind of there are some pretty high values for importance here with the first few singular values.", "tokens": [407, 733, 295, 456, 366, 512, 1238, 1090, 4190, 337, 7379, 510, 365, 264, 700, 1326, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.07791505951479256, "compression_ratio": 1.6540284360189574, "no_speech_prob": 1.0450865374878049e-05}, {"id": 251, "seek": 197600, "start": 1988.0, "end": 1990.0, "text": " And then it really drops off.", "tokens": [400, 550, 309, 534, 11438, 766, 13], "temperature": 0.0, "avg_logprob": -0.07791505951479256, "compression_ratio": 1.6540284360189574, "no_speech_prob": 1.0450865374878049e-05}, {"id": 252, "seek": 197600, "start": 1990.0, "end": 1998.0, "text": " And it's hard to know exactly kind of what these numbers correspond to, but it's helpful to see the relative importance.", "tokens": [400, 309, 311, 1152, 281, 458, 2293, 733, 295, 437, 613, 3547, 6805, 281, 11, 457, 309, 311, 4961, 281, 536, 264, 4972, 7379, 13], "temperature": 0.0, "avg_logprob": -0.07791505951479256, "compression_ratio": 1.6540284360189574, "no_speech_prob": 1.0450865374878049e-05}, {"id": 253, "seek": 197600, "start": 1998.0, "end": 2003.0, "text": " And the vector s of singular values is always ordered.", "tokens": [400, 264, 8062, 262, 295, 20010, 4190, 307, 1009, 8866, 13], "temperature": 0.0, "avg_logprob": -0.07791505951479256, "compression_ratio": 1.6540284360189574, "no_speech_prob": 1.0450865374878049e-05}, {"id": 254, "seek": 200300, "start": 2003.0, "end": 2008.0, "text": " So we kind of have the biggest values first.", "tokens": [407, 321, 733, 295, 362, 264, 3880, 4190, 700, 13], "temperature": 0.0, "avg_logprob": -0.17291619484884696, "compression_ratio": 1.3857142857142857, "no_speech_prob": 6.540204594784882e-06}, {"id": 255, "seek": 200300, "start": 2008.0, "end": 2014.0, "text": " And I think this will be easier when we see the topics.", "tokens": [400, 286, 519, 341, 486, 312, 3571, 562, 321, 536, 264, 8378, 13], "temperature": 0.0, "avg_logprob": -0.17291619484884696, "compression_ratio": 1.3857142857142857, "no_speech_prob": 6.540204594784882e-06}, {"id": 256, "seek": 200300, "start": 2014.0, "end": 2016.0, "text": " But we can pass in.", "tokens": [583, 321, 393, 1320, 294, 13], "temperature": 0.0, "avg_logprob": -0.17291619484884696, "compression_ratio": 1.3857142857142857, "no_speech_prob": 6.540204594784882e-06}, {"id": 257, "seek": 200300, "start": 2016.0, "end": 2025.0, "text": " And so remember we're doing going back to the picture, u times s times v.", "tokens": [400, 370, 1604, 321, 434, 884, 516, 646, 281, 264, 3036, 11, 344, 1413, 262, 1413, 371, 13], "temperature": 0.0, "avg_logprob": -0.17291619484884696, "compression_ratio": 1.3857142857142857, "no_speech_prob": 6.540204594784882e-06}, {"id": 258, "seek": 202500, "start": 2025.0, "end": 2039.0, "text": " Kind of larger ones are saying like, OK, these columns of u and these rows of v are more important or make up a bigger component of our original matrix.", "tokens": [9242, 295, 4833, 2306, 366, 1566, 411, 11, 2264, 11, 613, 13766, 295, 344, 293, 613, 13241, 295, 371, 366, 544, 1021, 420, 652, 493, 257, 3801, 6542, 295, 527, 3380, 8141, 13], "temperature": 0.0, "avg_logprob": -0.09119959191961603, "compression_ratio": 1.6188340807174888, "no_speech_prob": 1.6280140471280902e-06}, {"id": 259, "seek": 202500, "start": 2039.0, "end": 2042.0, "text": " They contribute more to the original matrix.", "tokens": [814, 10586, 544, 281, 264, 3380, 8141, 13], "temperature": 0.0, "avg_logprob": -0.09119959191961603, "compression_ratio": 1.6188340807174888, "no_speech_prob": 1.6280140471280902e-06}, {"id": 260, "seek": 202500, "start": 2042.0, "end": 2045.0, "text": " So we have this little helper method, show topics.", "tokens": [407, 321, 362, 341, 707, 36133, 3170, 11, 855, 8378, 13], "temperature": 0.0, "avg_logprob": -0.09119959191961603, "compression_ratio": 1.6188340807174888, "no_speech_prob": 1.6280140471280902e-06}, {"id": 261, "seek": 202500, "start": 2045.0, "end": 2048.0, "text": " And we're going to pass in our matrix v.", "tokens": [400, 321, 434, 516, 281, 1320, 294, 527, 8141, 371, 13], "temperature": 0.0, "avg_logprob": -0.09119959191961603, "compression_ratio": 1.6188340807174888, "no_speech_prob": 1.6280140471280902e-06}, {"id": 262, "seek": 202500, "start": 2048.0, "end": 2054.0, "text": " And then it's going to look up the words that correspond to the values.", "tokens": [400, 550, 309, 311, 516, 281, 574, 493, 264, 2283, 300, 6805, 281, 264, 4190, 13], "temperature": 0.0, "avg_logprob": -0.09119959191961603, "compression_ratio": 1.6188340807174888, "no_speech_prob": 1.6280140471280902e-06}, {"id": 263, "seek": 205400, "start": 2054.0, "end": 2058.0, "text": " And remember, our vocabulary words were in alphabetical order.", "tokens": [400, 1604, 11, 527, 19864, 2283, 645, 294, 23339, 804, 1668, 13], "temperature": 0.0, "avg_logprob": -0.1380558755662706, "compression_ratio": 1.380952380952381, "no_speech_prob": 3.6118156003794866e-06}, {"id": 264, "seek": 205400, "start": 2058.0, "end": 2071.0, "text": " This is basically going to find, OK, the largest values showed up in these particular columns, which corresponded to these vocabulary words.", "tokens": [639, 307, 1936, 516, 281, 915, 11, 2264, 11, 264, 6443, 4190, 4712, 493, 294, 613, 1729, 13766, 11, 597, 6805, 292, 281, 613, 19864, 2283, 13], "temperature": 0.0, "avg_logprob": -0.1380558755662706, "compression_ratio": 1.380952380952381, "no_speech_prob": 3.6118156003794866e-06}, {"id": 265, "seek": 207100, "start": 2071.0, "end": 2086.0, "text": " And so we get v, h. So we're looking at, sorry, the first five columns of v, h and the top eight topic words.", "tokens": [400, 370, 321, 483, 371, 11, 276, 13, 407, 321, 434, 1237, 412, 11, 2597, 11, 264, 700, 1732, 13766, 295, 371, 11, 276, 293, 264, 1192, 3180, 4829, 2283, 13], "temperature": 0.0, "avg_logprob": -0.12274471451254453, "compression_ratio": 1.5194805194805194, "no_speech_prob": 3.905276571458671e-06}, {"id": 266, "seek": 207100, "start": 2086.0, "end": 2094.0, "text": " So something to notice is one, this first topic is very weird.", "tokens": [407, 746, 281, 3449, 307, 472, 11, 341, 700, 4829, 307, 588, 3657, 13], "temperature": 0.0, "avg_logprob": -0.12274471451254453, "compression_ratio": 1.5194805194805194, "no_speech_prob": 3.905276571458671e-06}, {"id": 267, "seek": 207100, "start": 2094.0, "end": 2098.0, "text": " So we'll ignore that for a moment and look at the other ones.", "tokens": [407, 321, 603, 11200, 300, 337, 257, 1623, 293, 574, 412, 264, 661, 2306, 13], "temperature": 0.0, "avg_logprob": -0.12274471451254453, "compression_ratio": 1.5194805194805194, "no_speech_prob": 3.905276571458671e-06}, {"id": 268, "seek": 209800, "start": 2098.0, "end": 2108.0, "text": " But so we have a topic that is largely represented by the words JPEG, GIF, file, color, quality, image, format.", "tokens": [583, 370, 321, 362, 257, 4829, 300, 307, 11611, 10379, 538, 264, 2283, 508, 5208, 38, 11, 460, 12775, 11, 3991, 11, 2017, 11, 3125, 11, 3256, 11, 7877, 13], "temperature": 0.0, "avg_logprob": -0.11029998879683645, "compression_ratio": 1.3697916666666667, "no_speech_prob": 3.446433993303799e-06}, {"id": 269, "seek": 209800, "start": 2108.0, "end": 2113.0, "text": " Thinking back to our categories, which category might that correspond to?", "tokens": [24460, 646, 281, 527, 10479, 11, 597, 7719, 1062, 300, 6805, 281, 30], "temperature": 0.0, "avg_logprob": -0.11029998879683645, "compression_ratio": 1.3697916666666667, "no_speech_prob": 3.446433993303799e-06}, {"id": 270, "seek": 209800, "start": 2113.0, "end": 2116.0, "text": " Right, computer graphics.", "tokens": [1779, 11, 3820, 11837, 13], "temperature": 0.0, "avg_logprob": -0.11029998879683645, "compression_ratio": 1.3697916666666667, "no_speech_prob": 3.446433993303799e-06}, {"id": 271, "seek": 209800, "start": 2116.0, "end": 2125.0, "text": " The next one is graphics, EDU, pub, mail, ray, FTP.", "tokens": [440, 958, 472, 307, 11837, 11, 18050, 52, 11, 1535, 11, 10071, 11, 18592, 11, 479, 16804, 13], "temperature": 0.0, "avg_logprob": -0.11029998879683645, "compression_ratio": 1.3697916666666667, "no_speech_prob": 3.446433993303799e-06}, {"id": 272, "seek": 212500, "start": 2125.0, "end": 2130.0, "text": " And so any guesses?", "tokens": [400, 370, 604, 42703, 30], "temperature": 0.0, "avg_logprob": -0.0792144775390625, "compression_ratio": 1.547872340425532, "no_speech_prob": 6.1434111557900906e-06}, {"id": 273, "seek": 212500, "start": 2130.0, "end": 2132.0, "text": " Yeah, I think that's also computer graphics.", "tokens": [865, 11, 286, 519, 300, 311, 611, 3820, 11837, 13], "temperature": 0.0, "avg_logprob": -0.0792144775390625, "compression_ratio": 1.547872340425532, "no_speech_prob": 6.1434111557900906e-06}, {"id": 274, "seek": 212500, "start": 2132.0, "end": 2140.0, "text": " And so notice that they're kind of different, possibly different subgroups within a topic.", "tokens": [400, 370, 3449, 300, 436, 434, 733, 295, 819, 11, 6264, 819, 1422, 17377, 82, 1951, 257, 4829, 13], "temperature": 0.0, "avg_logprob": -0.0792144775390625, "compression_ratio": 1.547872340425532, "no_speech_prob": 6.1434111557900906e-06}, {"id": 275, "seek": 212500, "start": 2140.0, "end": 2146.0, "text": " But then Jesus, God, Matthew, people, atheists, atheism.", "tokens": [583, 550, 2705, 11, 1265, 11, 12434, 11, 561, 11, 27033, 1751, 11, 27033, 1434, 13], "temperature": 0.0, "avg_logprob": -0.0792144775390625, "compression_ratio": 1.547872340425532, "no_speech_prob": 6.1434111557900906e-06}, {"id": 276, "seek": 212500, "start": 2146.0, "end": 2152.0, "text": " This is possibly either religion or atheism.", "tokens": [639, 307, 6264, 2139, 7561, 420, 27033, 1434, 13], "temperature": 0.0, "avg_logprob": -0.0792144775390625, "compression_ratio": 1.547872340425532, "no_speech_prob": 6.1434111557900906e-06}, {"id": 277, "seek": 212500, "start": 2152.0, "end": 2154.0, "text": " And then another one on graphics.", "tokens": [400, 550, 1071, 472, 322, 11837, 13], "temperature": 0.0, "avg_logprob": -0.0792144775390625, "compression_ratio": 1.547872340425532, "no_speech_prob": 6.1434111557900906e-06}, {"id": 278, "seek": 215400, "start": 2154.0, "end": 2157.0, "text": " So yeah, graphics is well represented.", "tokens": [407, 1338, 11, 11837, 307, 731, 10379, 13], "temperature": 0.0, "avg_logprob": -0.15059151725163536, "compression_ratio": 1.3741935483870968, "no_speech_prob": 7.766349881421775e-06}, {"id": 279, "seek": 215400, "start": 2157.0, "end": 2165.0, "text": " So let me try taking 10 topics to see if we can get some more.", "tokens": [407, 718, 385, 853, 1940, 1266, 8378, 281, 536, 498, 321, 393, 483, 512, 544, 13], "temperature": 0.0, "avg_logprob": -0.15059151725163536, "compression_ratio": 1.3741935483870968, "no_speech_prob": 7.766349881421775e-06}, {"id": 280, "seek": 215400, "start": 2165.0, "end": 2171.0, "text": " OK, here's a space one, space NASA, lunar Mars probe, moon mission.", "tokens": [2264, 11, 510, 311, 257, 1901, 472, 11, 1901, 12077, 11, 32581, 9692, 22715, 11, 7135, 4447, 13], "temperature": 0.0, "avg_logprob": -0.15059151725163536, "compression_ratio": 1.3741935483870968, "no_speech_prob": 7.766349881421775e-06}, {"id": 281, "seek": 215400, "start": 2171.0, "end": 2180.0, "text": " So there are some space ones in there, too.", "tokens": [407, 456, 366, 512, 1901, 2306, 294, 456, 11, 886, 13], "temperature": 0.0, "avg_logprob": -0.15059151725163536, "compression_ratio": 1.3741935483870968, "no_speech_prob": 7.766349881421775e-06}, {"id": 282, "seek": 218000, "start": 2180.0, "end": 2187.0, "text": " So yeah, we got topics that matched with the kind of clusters that we would expect,", "tokens": [407, 1338, 11, 321, 658, 8378, 300, 21447, 365, 264, 733, 295, 23313, 300, 321, 576, 2066, 11], "temperature": 0.0, "avg_logprob": -0.09300194135526331, "compression_ratio": 1.2622950819672132, "no_speech_prob": 5.9546850934566464e-06}, {"id": 283, "seek": 218000, "start": 2187.0, "end": 2191.0, "text": " even though we had never passed the categories in.", "tokens": [754, 1673, 321, 632, 1128, 4678, 264, 10479, 294, 13], "temperature": 0.0, "avg_logprob": -0.09300194135526331, "compression_ratio": 1.2622950819672132, "no_speech_prob": 5.9546850934566464e-06}, {"id": 284, "seek": 218000, "start": 2191.0, "end": 2193.0, "text": " Any questions? Yes.", "tokens": [2639, 1651, 30, 1079, 13], "temperature": 0.0, "avg_logprob": -0.09300194135526331, "compression_ratio": 1.2622950819672132, "no_speech_prob": 5.9546850934566464e-06}, {"id": 285, "seek": 219300, "start": 2193.0, "end": 2211.0, "text": " So could you specify what each column of U is actually, like what column of U is?", "tokens": [407, 727, 291, 16500, 437, 1184, 7738, 295, 624, 307, 767, 11, 411, 437, 7738, 295, 624, 307, 30], "temperature": 0.0, "avg_logprob": -0.19853327567117257, "compression_ratio": 1.4285714285714286, "no_speech_prob": 2.710333137656562e-05}, {"id": 286, "seek": 219300, "start": 2211.0, "end": 2216.0, "text": " I think the row of VH will be the topics, right?", "tokens": [286, 519, 264, 5386, 295, 691, 39, 486, 312, 264, 8378, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19853327567117257, "compression_ratio": 1.4285714285714286, "no_speech_prob": 2.710333137656562e-05}, {"id": 287, "seek": 219300, "start": 2216.0, "end": 2221.0, "text": " Right, yeah, the rows of V are the topics, or VH.", "tokens": [1779, 11, 1338, 11, 264, 13241, 295, 691, 366, 264, 8378, 11, 420, 691, 39, 13], "temperature": 0.0, "avg_logprob": -0.19853327567117257, "compression_ratio": 1.4285714285714286, "no_speech_prob": 2.710333137656562e-05}, {"id": 288, "seek": 222100, "start": 2221.0, "end": 2228.0, "text": " The columns of U correspond to the particular post.", "tokens": [440, 13766, 295, 624, 6805, 281, 264, 1729, 2183, 13], "temperature": 0.0, "avg_logprob": -0.061502722950724814, "compression_ratio": 1.5549738219895288, "no_speech_prob": 1.384319784847321e-05}, {"id": 289, "seek": 222100, "start": 2228.0, "end": 2234.0, "text": " So we could use that if we wanted to look at a particular post and see, OK,", "tokens": [407, 321, 727, 764, 300, 498, 321, 1415, 281, 574, 412, 257, 1729, 2183, 293, 536, 11, 2264, 11], "temperature": 0.0, "avg_logprob": -0.061502722950724814, "compression_ratio": 1.5549738219895288, "no_speech_prob": 1.384319784847321e-05}, {"id": 290, "seek": 222100, "start": 2234.0, "end": 2238.0, "text": " how much of each topic shows up in that post.", "tokens": [577, 709, 295, 1184, 4829, 3110, 493, 294, 300, 2183, 13], "temperature": 0.0, "avg_logprob": -0.061502722950724814, "compression_ratio": 1.5549738219895288, "no_speech_prob": 1.384319784847321e-05}, {"id": 291, "seek": 222100, "start": 2238.0, "end": 2245.0, "text": " So from the beginning, kind of when we read somebody asking a question about computer graphics,", "tokens": [407, 490, 264, 2863, 11, 733, 295, 562, 321, 1401, 2618, 3365, 257, 1168, 466, 3820, 11837, 11], "temperature": 0.0, "avg_logprob": -0.061502722950724814, "compression_ratio": 1.5549738219895288, "no_speech_prob": 1.384319784847321e-05}, {"id": 292, "seek": 222100, "start": 2245.0, "end": 2247.0, "text": " we could look back and see.", "tokens": [321, 727, 574, 646, 293, 536, 13], "temperature": 0.0, "avg_logprob": -0.061502722950724814, "compression_ratio": 1.5549738219895288, "no_speech_prob": 1.384319784847321e-05}, {"id": 293, "seek": 224700, "start": 2247.0, "end": 2252.0, "text": " So this is, these are sometimes called embeddings, but U is kind of giving us,", "tokens": [407, 341, 307, 11, 613, 366, 2171, 1219, 12240, 29432, 11, 457, 624, 307, 733, 295, 2902, 505, 11], "temperature": 0.0, "avg_logprob": -0.29948146923168284, "compression_ratio": 1.6929460580912863, "no_speech_prob": 1.618515670998022e-05}, {"id": 294, "seek": 224700, "start": 2252.0, "end": 2259.0, "text": " like OK, these are how the individual posts were embedded and into the, kind of with the topics,", "tokens": [411, 2264, 11, 613, 366, 577, 264, 2609, 12300, 645, 16741, 293, 666, 264, 11, 733, 295, 365, 264, 8378, 11], "temperature": 0.0, "avg_logprob": -0.29948146923168284, "compression_ratio": 1.6929460580912863, "no_speech_prob": 1.618515670998022e-05}, {"id": 295, "seek": 224700, "start": 2259.0, "end": 2264.0, "text": " and then V is giving us, and these are how the words kind of correspond to those topics.", "tokens": [293, 550, 691, 307, 2902, 505, 11, 293, 613, 366, 577, 264, 2283, 733, 295, 6805, 281, 729, 8378, 13], "temperature": 0.0, "avg_logprob": -0.29948146923168284, "compression_ratio": 1.6929460580912863, "no_speech_prob": 1.618515670998022e-05}, {"id": 296, "seek": 224700, "start": 2264.0, "end": 2270.0, "text": " So you could write sample words, or just each unit you were back in the box?", "tokens": [407, 291, 727, 2464, 6889, 2283, 11, 420, 445, 1184, 4985, 291, 645, 646, 294, 264, 2424, 30], "temperature": 0.0, "avg_logprob": -0.29948146923168284, "compression_ratio": 1.6929460580912863, "no_speech_prob": 1.618515670998022e-05}, {"id": 297, "seek": 224700, "start": 2270.0, "end": 2272.0, "text": " Yeah, I'll try. That's a good question.", "tokens": [865, 11, 286, 603, 853, 13, 663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.29948146923168284, "compression_ratio": 1.6929460580912863, "no_speech_prob": 1.618515670998022e-05}, {"id": 298, "seek": 224700, "start": 2272.0, "end": 2273.0, "text": " I'll try writing a method.", "tokens": [286, 603, 853, 3579, 257, 3170, 13], "temperature": 0.0, "avg_logprob": -0.29948146923168284, "compression_ratio": 1.6929460580912863, "no_speech_prob": 1.618515670998022e-05}, {"id": 299, "seek": 227300, "start": 2273.0, "end": 2279.0, "text": " I'm on the fly, but what you would have to do is kind of pick off the largest entries,", "tokens": [286, 478, 322, 264, 3603, 11, 457, 437, 291, 576, 362, 281, 360, 307, 733, 295, 1888, 766, 264, 6443, 23041, 11], "temperature": 0.0, "avg_logprob": -0.11515557765960693, "compression_ratio": 1.5904255319148937, "no_speech_prob": 1.9524013623595238e-05}, {"id": 300, "seek": 227300, "start": 2279.0, "end": 2286.0, "text": " and then look up which, really you would want to tie it back to the words eventually.", "tokens": [293, 550, 574, 493, 597, 11, 534, 291, 576, 528, 281, 7582, 309, 646, 281, 264, 2283, 4728, 13], "temperature": 0.0, "avg_logprob": -0.11515557765960693, "compression_ratio": 1.5904255319148937, "no_speech_prob": 1.9524013623595238e-05}, {"id": 301, "seek": 227300, "start": 2286.0, "end": 2297.0, "text": " So I think then kind of look up, yeah, which topics are biggest, and then what words are in those topics.", "tokens": [407, 286, 519, 550, 733, 295, 574, 493, 11, 1338, 11, 597, 8378, 366, 3880, 11, 293, 550, 437, 2283, 366, 294, 729, 8378, 13], "temperature": 0.0, "avg_logprob": -0.11515557765960693, "compression_ratio": 1.5904255319148937, "no_speech_prob": 1.9524013623595238e-05}, {"id": 302, "seek": 227300, "start": 2297.0, "end": 2301.0, "text": " Any other questions?", "tokens": [2639, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.11515557765960693, "compression_ratio": 1.5904255319148937, "no_speech_prob": 1.9524013623595238e-05}, {"id": 303, "seek": 230100, "start": 2301.0, "end": 2306.0, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.18080671778265037, "compression_ratio": 1.5849056603773586, "no_speech_prob": 1.9523467926774174e-05}, {"id": 304, "seek": 230100, "start": 2306.0, "end": 2310.0, "text": " So in your picture, R is the number of topics.", "tokens": [407, 294, 428, 3036, 11, 497, 307, 264, 1230, 295, 8378, 13], "temperature": 0.0, "avg_logprob": -0.18080671778265037, "compression_ratio": 1.5849056603773586, "no_speech_prob": 1.9523467926774174e-05}, {"id": 305, "seek": 230100, "start": 2310.0, "end": 2312.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.18080671778265037, "compression_ratio": 1.5849056603773586, "no_speech_prob": 1.9523467926774174e-05}, {"id": 306, "seek": 230100, "start": 2312.0, "end": 2319.0, "text": " Number of documents by number of topics, and then the next one is number of topics by number of book items.", "tokens": [5118, 295, 8512, 538, 1230, 295, 8378, 11, 293, 550, 264, 958, 472, 307, 1230, 295, 8378, 538, 1230, 295, 1446, 4754, 13], "temperature": 0.0, "avg_logprob": -0.18080671778265037, "compression_ratio": 1.5849056603773586, "no_speech_prob": 1.9523467926774174e-05}, {"id": 307, "seek": 230100, "start": 2319.0, "end": 2325.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.18080671778265037, "compression_ratio": 1.5849056603773586, "no_speech_prob": 1.9523467926774174e-05}, {"id": 308, "seek": 232500, "start": 2325.0, "end": 2333.0, "text": " And this is actually jumping ahead because we were, right now our R is equal to N,", "tokens": [400, 341, 307, 767, 11233, 2286, 570, 321, 645, 11, 558, 586, 527, 497, 307, 2681, 281, 426, 11], "temperature": 0.0, "avg_logprob": -0.14030592337898587, "compression_ratio": 1.3383458646616542, "no_speech_prob": 1.2878446113973041e-06}, {"id": 309, "seek": 232500, "start": 2333.0, "end": 2350.0, "text": " but Jeremy was just pointing out that this matrix is, so the blue matrix is words are the rows,", "tokens": [457, 17809, 390, 445, 12166, 484, 300, 341, 8141, 307, 11, 370, 264, 3344, 8141, 307, 2283, 366, 264, 13241, 11], "temperature": 0.0, "avg_logprob": -0.14030592337898587, "compression_ratio": 1.3383458646616542, "no_speech_prob": 1.2878446113973041e-06}, {"id": 310, "seek": 235000, "start": 2350.0, "end": 2359.0, "text": " hashtags are the columns, the purple matrix, words are the rows, the columns are, what?", "tokens": [50016, 366, 264, 13766, 11, 264, 9656, 8141, 11, 2283, 366, 264, 13241, 11, 264, 13766, 366, 11, 437, 30], "temperature": 0.0, "avg_logprob": -0.14153696872569896, "compression_ratio": 1.6875, "no_speech_prob": 6.961972758290358e-06}, {"id": 311, "seek": 235000, "start": 2359.0, "end": 2362.0, "text": " Oh, sorry. I thought I said words. OK.", "tokens": [876, 11, 2597, 13, 286, 1194, 286, 848, 2283, 13, 2264, 13], "temperature": 0.0, "avg_logprob": -0.14153696872569896, "compression_ratio": 1.6875, "no_speech_prob": 6.961972758290358e-06}, {"id": 312, "seek": 235000, "start": 2362.0, "end": 2368.0, "text": " Purple matrix, words are the rows, the columns are the topics.", "tokens": [28483, 8141, 11, 2283, 366, 264, 13241, 11, 264, 13766, 366, 264, 8378, 13], "temperature": 0.0, "avg_logprob": -0.14153696872569896, "compression_ratio": 1.6875, "no_speech_prob": 6.961972758290358e-06}, {"id": 313, "seek": 236800, "start": 2368.0, "end": 2381.0, "text": " The pink matrix V, the rows are the topics, the columns are the hashtags. Oh, sorry.", "tokens": [440, 7022, 8141, 691, 11, 264, 13241, 366, 264, 8378, 11, 264, 13766, 366, 264, 50016, 13, 876, 11, 2597, 13], "temperature": 0.0, "avg_logprob": -0.12474457185659836, "compression_ratio": 1.4723926380368098, "no_speech_prob": 3.5557304727262817e-06}, {"id": 314, "seek": 236800, "start": 2381.0, "end": 2383.0, "text": " I see what you're saying. Sorry.", "tokens": [286, 536, 437, 291, 434, 1566, 13, 4919, 13], "temperature": 0.0, "avg_logprob": -0.12474457185659836, "compression_ratio": 1.4723926380368098, "no_speech_prob": 3.5557304727262817e-06}, {"id": 315, "seek": 236800, "start": 2383.0, "end": 2390.0, "text": " Facebook example is different by hashtag, I mean document.", "tokens": [4384, 1365, 307, 819, 538, 20379, 11, 286, 914, 4166, 13], "temperature": 0.0, "avg_logprob": -0.12474457185659836, "compression_ratio": 1.4723926380368098, "no_speech_prob": 3.5557304727262817e-06}, {"id": 316, "seek": 236800, "start": 2390.0, "end": 2396.0, "text": " OK. So let me say it one time with documents. Sorry about that.", "tokens": [2264, 13, 407, 718, 385, 584, 309, 472, 565, 365, 8512, 13, 4919, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.12474457185659836, "compression_ratio": 1.4723926380368098, "no_speech_prob": 3.5557304727262817e-06}, {"id": 317, "seek": 239600, "start": 2396.0, "end": 2417.0, "text": " Purple is words by topics. Pink is topics by post.", "tokens": [28483, 307, 2283, 538, 8378, 13, 17118, 307, 8378, 538, 2183, 13], "temperature": 0.0, "avg_logprob": -0.16138826608657836, "compression_ratio": 1.173913043478261, "no_speech_prob": 8.528511898475699e-06}, {"id": 318, "seek": 239600, "start": 2417.0, "end": 2423.0, "text": " OK. Don't worry. So we're actually going to return to SBD more later in this lesson,", "tokens": [2264, 13, 1468, 380, 3292, 13, 407, 321, 434, 767, 516, 281, 2736, 281, 26944, 35, 544, 1780, 294, 341, 6898, 11], "temperature": 0.0, "avg_logprob": -0.16138826608657836, "compression_ratio": 1.173913043478261, "no_speech_prob": 8.528511898475699e-06}, {"id": 319, "seek": 242300, "start": 2423.0, "end": 2427.0, "text": " but we're going to kind of take a break from SBD and look at NMF.", "tokens": [457, 321, 434, 516, 281, 733, 295, 747, 257, 1821, 490, 26944, 35, 293, 574, 412, 426, 44, 37, 13], "temperature": 0.0, "avg_logprob": -0.0903007374253384, "compression_ratio": 1.4820512820512821, "no_speech_prob": 1.3843029591953382e-05}, {"id": 320, "seek": 242300, "start": 2427.0, "end": 2431.0, "text": " And so I think it'll be good to revisit SBD later.", "tokens": [400, 370, 286, 519, 309, 603, 312, 665, 281, 32676, 26944, 35, 1780, 13], "temperature": 0.0, "avg_logprob": -0.0903007374253384, "compression_ratio": 1.4820512820512821, "no_speech_prob": 1.3843029591953382e-05}, {"id": 321, "seek": 242300, "start": 2431.0, "end": 2436.0, "text": " So there'll be more chance to ask questions about SBD.", "tokens": [407, 456, 603, 312, 544, 2931, 281, 1029, 1651, 466, 26944, 35, 13], "temperature": 0.0, "avg_logprob": -0.0903007374253384, "compression_ratio": 1.4820512820512821, "no_speech_prob": 1.3843029591953382e-05}, {"id": 322, "seek": 242300, "start": 2436.0, "end": 2440.0, "text": " So NMF stands for non-negative matrix factorization.", "tokens": [407, 426, 44, 37, 7382, 337, 2107, 12, 28561, 1166, 8141, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.0903007374253384, "compression_ratio": 1.4820512820512821, "no_speech_prob": 1.3843029591953382e-05}, {"id": 323, "seek": 242300, "start": 2440.0, "end": 2448.0, "text": " And some motivation, we can look at decomposition of some faces.", "tokens": [400, 512, 12335, 11, 321, 393, 574, 412, 48356, 295, 512, 8475, 13], "temperature": 0.0, "avg_logprob": -0.0903007374253384, "compression_ratio": 1.4820512820512821, "no_speech_prob": 1.3843029591953382e-05}, {"id": 324, "seek": 244800, "start": 2448.0, "end": 2454.0, "text": " And here, kind of the red pixels are showing negative values on the faces.", "tokens": [400, 510, 11, 733, 295, 264, 2182, 18668, 366, 4099, 3671, 4190, 322, 264, 8475, 13], "temperature": 0.0, "avg_logprob": -0.09227505245724239, "compression_ratio": 1.6910569105691058, "no_speech_prob": 1.4283232303569093e-05}, {"id": 325, "seek": 244800, "start": 2454.0, "end": 2461.0, "text": " So we kind of have found almost the topic equivalent of face, you know,", "tokens": [407, 321, 733, 295, 362, 1352, 1920, 264, 4829, 10344, 295, 1851, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.09227505245724239, "compression_ratio": 1.6910569105691058, "no_speech_prob": 1.4283232303569093e-05}, {"id": 326, "seek": 244800, "start": 2461.0, "end": 2463.0, "text": " like what are different components of the face.", "tokens": [411, 437, 366, 819, 6677, 295, 264, 1851, 13], "temperature": 0.0, "avg_logprob": -0.09227505245724239, "compression_ratio": 1.6910569105691058, "no_speech_prob": 1.4283232303569093e-05}, {"id": 327, "seek": 244800, "start": 2463.0, "end": 2465.0, "text": " But how do we interpret this?", "tokens": [583, 577, 360, 321, 7302, 341, 30], "temperature": 0.0, "avg_logprob": -0.09227505245724239, "compression_ratio": 1.6910569105691058, "no_speech_prob": 1.4283232303569093e-05}, {"id": 328, "seek": 244800, "start": 2465.0, "end": 2469.0, "text": " Like what does it mean to have a negative part of your face, you know,", "tokens": [1743, 437, 775, 309, 914, 281, 362, 257, 3671, 644, 295, 428, 1851, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.09227505245724239, "compression_ratio": 1.6910569105691058, "no_speech_prob": 1.4283232303569093e-05}, {"id": 329, "seek": 244800, "start": 2469.0, "end": 2472.0, "text": " that you can like add these together to form faces?", "tokens": [300, 291, 393, 411, 909, 613, 1214, 281, 1254, 8475, 30], "temperature": 0.0, "avg_logprob": -0.09227505245724239, "compression_ratio": 1.6910569105691058, "no_speech_prob": 1.4283232303569093e-05}, {"id": 330, "seek": 244800, "start": 2472.0, "end": 2477.0, "text": " And so this is the motivation for NMF, that with a lot of data sets,", "tokens": [400, 370, 341, 307, 264, 12335, 337, 426, 44, 37, 11, 300, 365, 257, 688, 295, 1412, 6352, 11], "temperature": 0.0, "avg_logprob": -0.09227505245724239, "compression_ratio": 1.6910569105691058, "no_speech_prob": 1.4283232303569093e-05}, {"id": 331, "seek": 247700, "start": 2477.0, "end": 2486.0, "text": " having something negative doesn't really make sense and is hard to interpret.", "tokens": [1419, 746, 3671, 1177, 380, 534, 652, 2020, 293, 307, 1152, 281, 7302, 13], "temperature": 0.0, "avg_logprob": -0.0964095906207436, "compression_ratio": 1.5124378109452736, "no_speech_prob": 3.966788881371031e-06}, {"id": 332, "seek": 247700, "start": 2486.0, "end": 2492.0, "text": " And notice in SBD, it was completely possible to have negative values.", "tokens": [400, 3449, 294, 26944, 35, 11, 309, 390, 2584, 1944, 281, 362, 3671, 4190, 13], "temperature": 0.0, "avg_logprob": -0.0964095906207436, "compression_ratio": 1.5124378109452736, "no_speech_prob": 3.966788881371031e-06}, {"id": 333, "seek": 247700, "start": 2492.0, "end": 2497.0, "text": " So here, we're kind of swapping out before, like the key thing with SBD is this orthogonality,", "tokens": [407, 510, 11, 321, 434, 733, 295, 1693, 10534, 484, 949, 11, 411, 264, 2141, 551, 365, 26944, 35, 307, 341, 38130, 266, 1860, 11], "temperature": 0.0, "avg_logprob": -0.0964095906207436, "compression_ratio": 1.5124378109452736, "no_speech_prob": 3.966788881371031e-06}, {"id": 334, "seek": 247700, "start": 2497.0, "end": 2501.0, "text": " you know, assuming that things are orthogonal to each other.", "tokens": [291, 458, 11, 11926, 300, 721, 366, 41488, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.0964095906207436, "compression_ratio": 1.5124378109452736, "no_speech_prob": 3.966788881371031e-06}, {"id": 335, "seek": 250100, "start": 2501.0, "end": 2507.0, "text": " Now we're going to have the key thing kind of be we want everything to be non-negative.", "tokens": [823, 321, 434, 516, 281, 362, 264, 2141, 551, 733, 295, 312, 321, 528, 1203, 281, 312, 2107, 12, 28561, 1166, 13], "temperature": 0.0, "avg_logprob": -0.06234637173739346, "compression_ratio": 1.6835443037974684, "no_speech_prob": 2.482300715200836e-06}, {"id": 336, "seek": 250100, "start": 2507.0, "end": 2513.0, "text": " So NMF is a factorization of a, and your original data set should be non-negative", "tokens": [407, 426, 44, 37, 307, 257, 5952, 2144, 295, 257, 11, 293, 428, 3380, 1412, 992, 820, 312, 2107, 12, 28561, 1166], "temperature": 0.0, "avg_logprob": -0.06234637173739346, "compression_ratio": 1.6835443037974684, "no_speech_prob": 2.482300715200836e-06}, {"id": 337, "seek": 250100, "start": 2513.0, "end": 2517.0, "text": " if you're using this, otherwise you won't be able to construct it.", "tokens": [498, 291, 434, 1228, 341, 11, 5911, 291, 1582, 380, 312, 1075, 281, 7690, 309, 13], "temperature": 0.0, "avg_logprob": -0.06234637173739346, "compression_ratio": 1.6835443037974684, "no_speech_prob": 2.482300715200836e-06}, {"id": 338, "seek": 250100, "start": 2517.0, "end": 2520.0, "text": " Also, if you have negatives in your original data set,", "tokens": [2743, 11, 498, 291, 362, 40019, 294, 428, 3380, 1412, 992, 11], "temperature": 0.0, "avg_logprob": -0.06234637173739346, "compression_ratio": 1.6835443037974684, "no_speech_prob": 2.482300715200836e-06}, {"id": 339, "seek": 250100, "start": 2520.0, "end": 2523.0, "text": " the negatives probably make sense in that context.", "tokens": [264, 40019, 1391, 652, 2020, 294, 300, 4319, 13], "temperature": 0.0, "avg_logprob": -0.06234637173739346, "compression_ratio": 1.6835443037974684, "no_speech_prob": 2.482300715200836e-06}, {"id": 340, "seek": 250100, "start": 2523.0, "end": 2527.0, "text": " So we're just factoring into two matrices here, W and H,", "tokens": [407, 321, 434, 445, 1186, 3662, 666, 732, 32284, 510, 11, 343, 293, 389, 11], "temperature": 0.0, "avg_logprob": -0.06234637173739346, "compression_ratio": 1.6835443037974684, "no_speech_prob": 2.482300715200836e-06}, {"id": 341, "seek": 252700, "start": 2527.0, "end": 2533.0, "text": " and we want each entry in W and H to be non-negative.", "tokens": [293, 321, 528, 1184, 8729, 294, 343, 293, 389, 281, 312, 2107, 12, 28561, 1166, 13], "temperature": 0.0, "avg_logprob": -0.056038769808682526, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.425222869031131e-06}, {"id": 342, "seek": 252700, "start": 2533.0, "end": 2541.0, "text": " And so for this face idea, here if each column of our original matrix was an actual person's face,", "tokens": [400, 370, 337, 341, 1851, 1558, 11, 510, 498, 1184, 7738, 295, 527, 3380, 8141, 390, 364, 3539, 954, 311, 1851, 11], "temperature": 0.0, "avg_logprob": -0.056038769808682526, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.425222869031131e-06}, {"id": 343, "seek": 252700, "start": 2541.0, "end": 2547.0, "text": " what we would be capturing would be a matrix of different facial features,", "tokens": [437, 321, 576, 312, 23384, 576, 312, 257, 8141, 295, 819, 15642, 4122, 11], "temperature": 0.0, "avg_logprob": -0.056038769808682526, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.425222869031131e-06}, {"id": 344, "seek": 252700, "start": 2547.0, "end": 2554.0, "text": " and then the relative importance of each of those features in a particular image.", "tokens": [293, 550, 264, 4972, 7379, 295, 1184, 295, 729, 4122, 294, 257, 1729, 3256, 13], "temperature": 0.0, "avg_logprob": -0.056038769808682526, "compression_ratio": 1.5765306122448979, "no_speech_prob": 4.425222869031131e-06}, {"id": 345, "seek": 255400, "start": 2554.0, "end": 2566.0, "text": " And I believe you saw the Eigenfaces data set in Yannett's machine learning class.", "tokens": [400, 286, 1697, 291, 1866, 264, 30586, 69, 2116, 1412, 992, 294, 398, 969, 3093, 311, 3479, 2539, 1508, 13], "temperature": 0.0, "avg_logprob": -0.14443585811517176, "compression_ratio": 1.418848167539267, "no_speech_prob": 2.260210976601229e-06}, {"id": 346, "seek": 255400, "start": 2566.0, "end": 2570.0, "text": " So NMF is a hard problem.", "tokens": [407, 426, 44, 37, 307, 257, 1152, 1154, 13], "temperature": 0.0, "avg_logprob": -0.14443585811517176, "compression_ratio": 1.418848167539267, "no_speech_prob": 2.260210976601229e-06}, {"id": 347, "seek": 255400, "start": 2570.0, "end": 2574.0, "text": " One is that it's kind of under constrained, so you could find different answers,", "tokens": [1485, 307, 300, 309, 311, 733, 295, 833, 38901, 11, 370, 291, 727, 915, 819, 6338, 11], "temperature": 0.0, "avg_logprob": -0.14443585811517176, "compression_ratio": 1.418848167539267, "no_speech_prob": 2.260210976601229e-06}, {"id": 348, "seek": 255400, "start": 2574.0, "end": 2578.0, "text": " and typically you'll add kind of additional constraints.", "tokens": [293, 5850, 291, 603, 909, 733, 295, 4497, 18491, 13], "temperature": 0.0, "avg_logprob": -0.14443585811517176, "compression_ratio": 1.418848167539267, "no_speech_prob": 2.260210976601229e-06}, {"id": 349, "seek": 255400, "start": 2578.0, "end": 2583.0, "text": " It's also, it's NP-hard.", "tokens": [467, 311, 611, 11, 309, 311, 38611, 12, 21491, 13], "temperature": 0.0, "avg_logprob": -0.14443585811517176, "compression_ratio": 1.418848167539267, "no_speech_prob": 2.260210976601229e-06}, {"id": 350, "seek": 258300, "start": 2583.0, "end": 2587.0, "text": " So coming back to the problem that we're looking at today with topic modeling,", "tokens": [407, 1348, 646, 281, 264, 1154, 300, 321, 434, 1237, 412, 965, 365, 4829, 15983, 11], "temperature": 0.0, "avg_logprob": -0.11792607625325521, "compression_ratio": 1.617801047120419, "no_speech_prob": 7.527321486122673e-06}, {"id": 351, "seek": 258300, "start": 2587.0, "end": 2591.0, "text": " so we have our original matrix again, words by documents,", "tokens": [370, 321, 362, 527, 3380, 8141, 797, 11, 2283, 538, 8512, 11], "temperature": 0.0, "avg_logprob": -0.11792607625325521, "compression_ratio": 1.617801047120419, "no_speech_prob": 7.527321486122673e-06}, {"id": 352, "seek": 258300, "start": 2591.0, "end": 2602.0, "text": " and here that'll get decomposed into matrix W that is words for the rows by topics as the columns,", "tokens": [293, 510, 300, 603, 483, 22867, 1744, 666, 8141, 343, 300, 307, 2283, 337, 264, 13241, 538, 8378, 382, 264, 13766, 11], "temperature": 0.0, "avg_logprob": -0.11792607625325521, "compression_ratio": 1.617801047120419, "no_speech_prob": 7.527321486122673e-06}, {"id": 353, "seek": 258300, "start": 2602.0, "end": 2608.0, "text": " and then topics, importance indicators, so here topics would be the rows,", "tokens": [293, 550, 8378, 11, 7379, 22176, 11, 370, 510, 8378, 576, 312, 264, 13241, 11], "temperature": 0.0, "avg_logprob": -0.11792607625325521, "compression_ratio": 1.617801047120419, "no_speech_prob": 7.527321486122673e-06}, {"id": 354, "seek": 260800, "start": 2608.0, "end": 2615.0, "text": " and their importance would be the columns.", "tokens": [293, 641, 7379, 576, 312, 264, 13766, 13], "temperature": 0.0, "avg_logprob": -0.16512846946716309, "compression_ratio": 0.896551724137931, "no_speech_prob": 2.4676115572219715e-05}, {"id": 355, "seek": 260800, "start": 2615.0, "end": 2619.0, "text": " Question?", "tokens": [14464, 30], "temperature": 0.0, "avg_logprob": -0.16512846946716309, "compression_ratio": 0.896551724137931, "no_speech_prob": 2.4676115572219715e-05}, {"id": 356, "seek": 261900, "start": 2619.0, "end": 2639.0, "text": " Is this the only way to do the matrix categorization for this one, or is this just a more, what are they approach way to do the double-hatch?", "tokens": [1119, 341, 264, 787, 636, 281, 360, 264, 8141, 19250, 2144, 337, 341, 472, 11, 420, 307, 341, 445, 257, 544, 11, 437, 366, 436, 3109, 636, 281, 360, 264, 3834, 12, 71, 852, 30], "temperature": 0.0, "avg_logprob": -0.38353592310196316, "compression_ratio": 1.3177570093457944, "no_speech_prob": 0.0001727230555843562}, {"id": 357, "seek": 263900, "start": 2639.0, "end": 2649.0, "text": " I mean here you are breaking the words and document to approximate topics multiplied by the topic importance indicator, right?", "tokens": [286, 914, 510, 291, 366, 7697, 264, 2283, 293, 4166, 281, 30874, 8378, 17207, 538, 264, 4829, 7379, 16961, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12258507441548468, "compression_ratio": 1.6081632653061224, "no_speech_prob": 1.1477603038656525e-05}, {"id": 358, "seek": 263900, "start": 2649.0, "end": 2652.0, "text": " Is this the only way to do it, or is this the other way to do it?", "tokens": [1119, 341, 264, 787, 636, 281, 360, 309, 11, 420, 307, 341, 264, 661, 636, 281, 360, 309, 30], "temperature": 0.0, "avg_logprob": -0.12258507441548468, "compression_ratio": 1.6081632653061224, "no_speech_prob": 1.1477603038656525e-05}, {"id": 359, "seek": 263900, "start": 2652.0, "end": 2657.0, "text": " I would say actually neither, so this is an alternative to what we saw with SVD,", "tokens": [286, 576, 584, 767, 9662, 11, 370, 341, 307, 364, 8535, 281, 437, 321, 1866, 365, 31910, 35, 11], "temperature": 0.0, "avg_logprob": -0.12258507441548468, "compression_ratio": 1.6081632653061224, "no_speech_prob": 1.1477603038656525e-05}, {"id": 360, "seek": 263900, "start": 2657.0, "end": 2664.0, "text": " and I'll talk about some of the pros and cons of NMF versus SVD later on,", "tokens": [293, 286, 603, 751, 466, 512, 295, 264, 6267, 293, 1014, 295, 426, 44, 37, 5717, 31910, 35, 1780, 322, 11], "temperature": 0.0, "avg_logprob": -0.12258507441548468, "compression_ratio": 1.6081632653061224, "no_speech_prob": 1.1477603038656525e-05}, {"id": 361, "seek": 263900, "start": 2664.0, "end": 2667.0, "text": " but I would say they're both valid approaches.", "tokens": [457, 286, 576, 584, 436, 434, 1293, 7363, 11587, 13], "temperature": 0.0, "avg_logprob": -0.12258507441548468, "compression_ratio": 1.6081632653061224, "no_speech_prob": 1.1477603038656525e-05}, {"id": 362, "seek": 266700, "start": 2667.0, "end": 2670.0, "text": " I wanted you to kind of see a different way of tackling the problem.", "tokens": [286, 1415, 291, 281, 733, 295, 536, 257, 819, 636, 295, 34415, 264, 1154, 13], "temperature": 0.0, "avg_logprob": -0.2322623037522839, "compression_ratio": 1.5348837209302326, "no_speech_prob": 5.862366833753185e-06}, {"id": 363, "seek": 266700, "start": 2670.0, "end": 2681.0, "text": " But you're labeling your topics and topic importance indicators, your labels, there's nothing mathematical about topics.", "tokens": [583, 291, 434, 40244, 428, 8378, 293, 4829, 7379, 22176, 11, 428, 16949, 11, 456, 311, 1825, 18894, 466, 8378, 13], "temperature": 0.0, "avg_logprob": -0.2322623037522839, "compression_ratio": 1.5348837209302326, "no_speech_prob": 5.862366833753185e-06}, {"id": 364, "seek": 266700, "start": 2681.0, "end": 2692.0, "text": " Oh, right, and that's kind of a common way of interpreting what they mean.", "tokens": [876, 11, 558, 11, 293, 300, 311, 733, 295, 257, 2689, 636, 295, 37395, 437, 436, 914, 13], "temperature": 0.0, "avg_logprob": -0.2322623037522839, "compression_ratio": 1.5348837209302326, "no_speech_prob": 5.862366833753185e-06}, {"id": 365, "seek": 269200, "start": 2692.0, "end": 2700.0, "text": " And here also with all of these, there's not a clear answer of what the number of topics should be.", "tokens": [400, 510, 611, 365, 439, 295, 613, 11, 456, 311, 406, 257, 1850, 1867, 295, 437, 264, 1230, 295, 8378, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.09369277379598963, "compression_ratio": 1.5779816513761469, "no_speech_prob": 1.3209034477768e-05}, {"id": 366, "seek": 269200, "start": 2700.0, "end": 2706.0, "text": " You know, even something with our data set where we knew we were bringing in these four categories,", "tokens": [509, 458, 11, 754, 746, 365, 527, 1412, 992, 689, 321, 2586, 321, 645, 5062, 294, 613, 1451, 10479, 11], "temperature": 0.0, "avg_logprob": -0.09369277379598963, "compression_ratio": 1.5779816513761469, "no_speech_prob": 1.3209034477768e-05}, {"id": 367, "seek": 269200, "start": 2706.0, "end": 2711.0, "text": " you know, we've seen that there are kind of multiple subtopics within computer graphics.", "tokens": [291, 458, 11, 321, 600, 1612, 300, 456, 366, 733, 295, 3866, 7257, 404, 1167, 1951, 3820, 11837, 13], "temperature": 0.0, "avg_logprob": -0.09369277379598963, "compression_ratio": 1.5779816513761469, "no_speech_prob": 1.3209034477768e-05}, {"id": 368, "seek": 269200, "start": 2711.0, "end": 2714.0, "text": " There's also some overlap between religion and atheism,", "tokens": [821, 311, 611, 512, 19959, 1296, 7561, 293, 27033, 1434, 11], "temperature": 0.0, "avg_logprob": -0.09369277379598963, "compression_ratio": 1.5779816513761469, "no_speech_prob": 1.3209034477768e-05}, {"id": 369, "seek": 271400, "start": 2714.0, "end": 2723.0, "text": " so it's often not going to be clear even kind of what the quote best dimension to use would be for the number of topics you're looking for.", "tokens": [370, 309, 311, 2049, 406, 516, 281, 312, 1850, 754, 733, 295, 437, 264, 6513, 1151, 10139, 281, 764, 576, 312, 337, 264, 1230, 295, 8378, 291, 434, 1237, 337, 13], "temperature": 0.0, "avg_logprob": -0.09511064080631032, "compression_ratio": 1.6047904191616766, "no_speech_prob": 3.4996965041500516e-06}, {"id": 370, "seek": 271400, "start": 2723.0, "end": 2727.0, "text": " So in H, the number of columns is equal to the number of documents,", "tokens": [407, 294, 389, 11, 264, 1230, 295, 13766, 307, 2681, 281, 264, 1230, 295, 8512, 11], "temperature": 0.0, "avg_logprob": -0.09511064080631032, "compression_ratio": 1.6047904191616766, "no_speech_prob": 3.4996965041500516e-06}, {"id": 371, "seek": 271400, "start": 2727.0, "end": 2733.0, "text": " so it's saying how important is each topic to that document.", "tokens": [370, 309, 311, 1566, 577, 1021, 307, 1184, 4829, 281, 300, 4166, 13], "temperature": 0.0, "avg_logprob": -0.09511064080631032, "compression_ratio": 1.6047904191616766, "no_speech_prob": 3.4996965041500516e-06}, {"id": 372, "seek": 273300, "start": 2733.0, "end": 2744.0, "text": " Right, yeah, so for each document, it's the relative importance of each topic.", "tokens": [1779, 11, 1338, 11, 370, 337, 1184, 4166, 11, 309, 311, 264, 4972, 7379, 295, 1184, 4829, 13], "temperature": 0.0, "avg_logprob": -0.08544212718342625, "compression_ratio": 1.3233082706766917, "no_speech_prob": 1.8446518197379191e-06}, {"id": 373, "seek": 273300, "start": 2744.0, "end": 2751.0, "text": " And you could think of that because when you multiply W and H together for a particular document,", "tokens": [400, 291, 727, 519, 295, 300, 570, 562, 291, 12972, 343, 293, 389, 1214, 337, 257, 1729, 4166, 11], "temperature": 0.0, "avg_logprob": -0.08544212718342625, "compression_ratio": 1.3233082706766917, "no_speech_prob": 1.8446518197379191e-06}, {"id": 374, "seek": 275100, "start": 2751.0, "end": 2764.0, "text": " you are kind of taking a linear combination of the topics and you want to know what the coefficient is.", "tokens": [291, 366, 733, 295, 1940, 257, 8213, 6562, 295, 264, 8378, 293, 291, 528, 281, 458, 437, 264, 17619, 307, 13], "temperature": 0.0, "avg_logprob": -0.1035717067433827, "compression_ratio": 1.4424242424242424, "no_speech_prob": 1.5778731494719977e-06}, {"id": 375, "seek": 275100, "start": 2764.0, "end": 2769.0, "text": " So Scikit-Learn has a built-in NMF that we'll use first,", "tokens": [407, 16942, 22681, 12, 11020, 1083, 575, 257, 3094, 12, 259, 426, 44, 37, 300, 321, 603, 764, 700, 11], "temperature": 0.0, "avg_logprob": -0.1035717067433827, "compression_ratio": 1.4424242424242424, "no_speech_prob": 1.5778731494719977e-06}, {"id": 376, "seek": 275100, "start": 2769.0, "end": 2777.0, "text": " and that's in the Scikit-Learn decomposition module, which we imported above.", "tokens": [293, 300, 311, 294, 264, 16942, 22681, 12, 11020, 1083, 48356, 10088, 11, 597, 321, 25524, 3673, 13], "temperature": 0.0, "avg_logprob": -0.1035717067433827, "compression_ratio": 1.4424242424242424, "no_speech_prob": 1.5778731494719977e-06}, {"id": 377, "seek": 277700, "start": 2777.0, "end": 2785.0, "text": " And kind of as I mentioned, we're telling it how many components we want, so that's kind of a decision we're having to make.", "tokens": [400, 733, 295, 382, 286, 2835, 11, 321, 434, 3585, 309, 577, 867, 6677, 321, 528, 11, 370, 300, 311, 733, 295, 257, 3537, 321, 434, 1419, 281, 652, 13], "temperature": 0.0, "avg_logprob": -0.13960797273660008, "compression_ratio": 1.6312849162011174, "no_speech_prob": 1.2217918992973864e-05}, {"id": 378, "seek": 277700, "start": 2785.0, "end": 2794.0, "text": " It returns the, when we do kind of our classifier.fitTransform, that will return the W,", "tokens": [467, 11247, 264, 11, 562, 321, 360, 733, 295, 527, 1508, 9902, 13, 6845, 33339, 837, 11, 300, 486, 2736, 264, 343, 11], "temperature": 0.0, "avg_logprob": -0.13960797273660008, "compression_ratio": 1.6312849162011174, "no_speech_prob": 1.2217918992973864e-05}, {"id": 379, "seek": 277700, "start": 2794.0, "end": 2803.0, "text": " and then we can get H just kind of stored in this components on our classifier.", "tokens": [293, 550, 321, 393, 483, 389, 445, 733, 295, 12187, 294, 341, 6677, 322, 527, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.13960797273660008, "compression_ratio": 1.6312849162011174, "no_speech_prob": 1.2217918992973864e-05}, {"id": 380, "seek": 280300, "start": 2803.0, "end": 2810.0, "text": " And this is non-exact, meaning we're not going to get, in most cases, we're not going to get our original matrix back perfectly.", "tokens": [400, 341, 307, 2107, 12, 3121, 578, 11, 3620, 321, 434, 406, 516, 281, 483, 11, 294, 881, 3331, 11, 321, 434, 406, 516, 281, 483, 527, 3380, 8141, 646, 6239, 13], "temperature": 0.0, "avg_logprob": -0.09459033765290913, "compression_ratio": 1.5055555555555555, "no_speech_prob": 4.784737029694952e-06}, {"id": 381, "seek": 280300, "start": 2810.0, "end": 2818.0, "text": " We're getting something as close as we can.", "tokens": [492, 434, 1242, 746, 382, 1998, 382, 321, 393, 13], "temperature": 0.0, "avg_logprob": -0.09459033765290913, "compression_ratio": 1.5055555555555555, "no_speech_prob": 4.784737029694952e-06}, {"id": 382, "seek": 280300, "start": 2818.0, "end": 2820.0, "text": " That's a good question.", "tokens": [663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.09459033765290913, "compression_ratio": 1.5055555555555555, "no_speech_prob": 4.784737029694952e-06}, {"id": 383, "seek": 280300, "start": 2820.0, "end": 2829.0, "text": " We will head into that in a moment, if you have different ways to do this.", "tokens": [492, 486, 1378, 666, 300, 294, 257, 1623, 11, 498, 291, 362, 819, 2098, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.09459033765290913, "compression_ratio": 1.5055555555555555, "no_speech_prob": 4.784737029694952e-06}, {"id": 384, "seek": 282900, "start": 2829.0, "end": 2842.0, "text": " So here we can check, we can check through our topics, makes sense again, looking from the second matrix H,", "tokens": [407, 510, 321, 393, 1520, 11, 321, 393, 1520, 807, 527, 8378, 11, 1669, 2020, 797, 11, 1237, 490, 264, 1150, 8141, 389, 11], "temperature": 0.0, "avg_logprob": -0.1761899598887269, "compression_ratio": 1.413978494623656, "no_speech_prob": 4.157144758210052e-06}, {"id": 385, "seek": 282900, "start": 2842.0, "end": 2848.0, "text": " and yes, they seem to be fitting with what we know our categories are.", "tokens": [293, 2086, 11, 436, 1643, 281, 312, 15669, 365, 437, 321, 458, 527, 10479, 366, 13], "temperature": 0.0, "avg_logprob": -0.1761899598887269, "compression_ratio": 1.413978494623656, "no_speech_prob": 4.157144758210052e-06}, {"id": 386, "seek": 282900, "start": 2848.0, "end": 2855.0, "text": " So we've got JPEG, image, GIF, file, color, very reasonable computer graphics topic.", "tokens": [407, 321, 600, 658, 508, 5208, 38, 11, 3256, 11, 460, 12775, 11, 3991, 11, 2017, 11, 588, 10585, 3820, 11837, 4829, 13], "temperature": 0.0, "avg_logprob": -0.1761899598887269, "compression_ratio": 1.413978494623656, "no_speech_prob": 4.157144758210052e-06}, {"id": 387, "seek": 285500, "start": 2855.0, "end": 2867.0, "text": " The third one is space, launch, satellite, NASA, commercial, reasonable space topic.", "tokens": [440, 2636, 472, 307, 1901, 11, 4025, 11, 16016, 11, 12077, 11, 6841, 11, 10585, 1901, 4829, 13], "temperature": 0.0, "avg_logprob": -0.1546178144567153, "compression_ratio": 1.536764705882353, "no_speech_prob": 1.4823057199464529e-06}, {"id": 388, "seek": 285500, "start": 2867.0, "end": 2877.0, "text": " Okay. I have a section on topic frequency, inverse document frequency, what?", "tokens": [1033, 13, 286, 362, 257, 3541, 322, 4829, 7893, 11, 17340, 4166, 7893, 11, 437, 30], "temperature": 0.0, "avg_logprob": -0.1546178144567153, "compression_ratio": 1.536764705882353, "no_speech_prob": 1.4823057199464529e-06}, {"id": 389, "seek": 285500, "start": 2877.0, "end": 2884.0, "text": " Oh, term frequency, inverse document frequency.", "tokens": [876, 11, 1433, 7893, 11, 17340, 4166, 7893, 13], "temperature": 0.0, "avg_logprob": -0.1546178144567153, "compression_ratio": 1.536764705882353, "no_speech_prob": 1.4823057199464529e-06}, {"id": 390, "seek": 288400, "start": 2884.0, "end": 2887.0, "text": " I'm not going to go super in-depth into this.", "tokens": [286, 478, 406, 516, 281, 352, 1687, 294, 12, 25478, 666, 341, 13], "temperature": 0.0, "avg_logprob": -0.06703243255615235, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.410564649035223e-06}, {"id": 391, "seek": 288400, "start": 2887.0, "end": 2894.0, "text": " This is something that's highly, or not highly, but often comes up as a way to kind of normalize your inputs.", "tokens": [639, 307, 746, 300, 311, 5405, 11, 420, 406, 5405, 11, 457, 2049, 1487, 493, 382, 257, 636, 281, 733, 295, 2710, 1125, 428, 15743, 13], "temperature": 0.0, "avg_logprob": -0.06703243255615235, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.410564649035223e-06}, {"id": 392, "seek": 288400, "start": 2894.0, "end": 2903.0, "text": " I tried it, though, with what I was doing and didn't see a significant difference for this particular data set and these decompositions,", "tokens": [286, 3031, 309, 11, 1673, 11, 365, 437, 286, 390, 884, 293, 994, 380, 536, 257, 4776, 2649, 337, 341, 1729, 1412, 992, 293, 613, 22867, 329, 2451, 11], "temperature": 0.0, "avg_logprob": -0.06703243255615235, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.410564649035223e-06}, {"id": 393, "seek": 288400, "start": 2903.0, "end": 2907.0, "text": " but I think it's good to be aware of it.", "tokens": [457, 286, 519, 309, 311, 665, 281, 312, 3650, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.06703243255615235, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.410564649035223e-06}, {"id": 394, "seek": 290700, "start": 2907.0, "end": 2915.0, "text": " So here we kind of want to take into account how often a term appears in a document, how long the document it is,", "tokens": [407, 510, 321, 733, 295, 528, 281, 747, 666, 2696, 577, 2049, 257, 1433, 7038, 294, 257, 4166, 11, 577, 938, 264, 4166, 309, 307, 11], "temperature": 0.0, "avg_logprob": -0.07259875624927122, "compression_ratio": 1.5380116959064327, "no_speech_prob": 7.811059390405717e-07}, {"id": 395, "seek": 290700, "start": 2915.0, "end": 2919.0, "text": " and also how common or rare the term is.", "tokens": [293, 611, 577, 2689, 420, 5892, 264, 1433, 307, 13], "temperature": 0.0, "avg_logprob": -0.07259875624927122, "compression_ratio": 1.5380116959064327, "no_speech_prob": 7.811059390405717e-07}, {"id": 396, "seek": 290700, "start": 2919.0, "end": 2928.0, "text": " So far we've just been dealing with these raw frequencies, which don't really take into account any of that.", "tokens": [407, 1400, 321, 600, 445, 668, 6260, 365, 613, 8936, 20250, 11, 597, 500, 380, 534, 747, 666, 2696, 604, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.07259875624927122, "compression_ratio": 1.5380116959064327, "no_speech_prob": 7.811059390405717e-07}, {"id": 397, "seek": 292800, "start": 2928.0, "end": 2940.0, "text": " And so what TF-IDF does instead is it's got this term TF, which is the number of occurrences of a term in document over the number of words in a document.", "tokens": [400, 370, 437, 40964, 12, 2777, 37, 775, 2602, 307, 309, 311, 658, 341, 1433, 40964, 11, 597, 307, 264, 1230, 295, 5160, 38983, 295, 257, 1433, 294, 4166, 670, 264, 1230, 295, 2283, 294, 257, 4166, 13], "temperature": 0.0, "avg_logprob": -0.10682084560394287, "compression_ratio": 1.5188679245283019, "no_speech_prob": 1.1725995818778756e-06}, {"id": 398, "seek": 292800, "start": 2940.0, "end": 2950.0, "text": " So if a document is super long, we're basically kind of giving relative less weight, as opposed to before, we would have just had a lot of high frequencies showing up.", "tokens": [407, 498, 257, 4166, 307, 1687, 938, 11, 321, 434, 1936, 733, 295, 2902, 4972, 1570, 3364, 11, 382, 8851, 281, 949, 11, 321, 576, 362, 445, 632, 257, 688, 295, 1090, 20250, 4099, 493, 13], "temperature": 0.0, "avg_logprob": -0.10682084560394287, "compression_ratio": 1.5188679245283019, "no_speech_prob": 1.1725995818778756e-06}, {"id": 399, "seek": 295000, "start": 2950.0, "end": 2958.0, "text": " And then the IDF term takes the log of the number of documents divided by the number of documents with the term T in it.", "tokens": [400, 550, 264, 7348, 37, 1433, 2516, 264, 3565, 295, 264, 1230, 295, 8512, 6666, 538, 264, 1230, 295, 8512, 365, 264, 1433, 314, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.038070692142970125, "compression_ratio": 1.5649717514124293, "no_speech_prob": 5.285349402583961e-07}, {"id": 400, "seek": 295000, "start": 2958.0, "end": 2961.0, "text": " And that's a measure of how common or rare a word is.", "tokens": [400, 300, 311, 257, 3481, 295, 577, 2689, 420, 5892, 257, 1349, 307, 13], "temperature": 0.0, "avg_logprob": -0.038070692142970125, "compression_ratio": 1.5649717514124293, "no_speech_prob": 5.285349402583961e-07}, {"id": 401, "seek": 295000, "start": 2961.0, "end": 2972.0, "text": " So if a word only shows up in very few documents, it's pretty rare and probably has more significance.", "tokens": [407, 498, 257, 1349, 787, 3110, 493, 294, 588, 1326, 8512, 11, 309, 311, 1238, 5892, 293, 1391, 575, 544, 17687, 13], "temperature": 0.0, "avg_logprob": -0.038070692142970125, "compression_ratio": 1.5649717514124293, "no_speech_prob": 5.285349402583961e-07}, {"id": 402, "seek": 297200, "start": 2972.0, "end": 2981.0, "text": " So yeah, we won't go too much into this, but I wanted to let you know that it exists.", "tokens": [407, 1338, 11, 321, 1582, 380, 352, 886, 709, 666, 341, 11, 457, 286, 1415, 281, 718, 291, 458, 300, 309, 8198, 13], "temperature": 0.0, "avg_logprob": -0.054864600852683736, "compression_ratio": 1.5348837209302326, "no_speech_prob": 5.013565896661021e-06}, {"id": 403, "seek": 297200, "start": 2981.0, "end": 2985.0, "text": " And then I guess we'll break soon.", "tokens": [400, 550, 286, 2041, 321, 603, 1821, 2321, 13], "temperature": 0.0, "avg_logprob": -0.054864600852683736, "compression_ratio": 1.5348837209302326, "no_speech_prob": 5.013565896661021e-06}, {"id": 404, "seek": 297200, "start": 2985.0, "end": 2988.0, "text": " Let me just kind of say non-negative matrix factorization.", "tokens": [961, 385, 445, 733, 295, 584, 2107, 12, 28561, 1166, 8141, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.054864600852683736, "compression_ratio": 1.5348837209302326, "no_speech_prob": 5.013565896661021e-06}, {"id": 405, "seek": 297200, "start": 2988.0, "end": 2992.0, "text": " In summary, the benefits are it's fast and easy to use.", "tokens": [682, 12691, 11, 264, 5311, 366, 309, 311, 2370, 293, 1858, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.054864600852683736, "compression_ratio": 1.5348837209302326, "no_speech_prob": 5.013565896661021e-06}, {"id": 406, "seek": 297200, "start": 2992.0, "end": 2996.0, "text": " The downsides are it took years of research and expertise to create.", "tokens": [440, 21554, 1875, 366, 309, 1890, 924, 295, 2132, 293, 11769, 281, 1884, 13], "temperature": 0.0, "avg_logprob": -0.054864600852683736, "compression_ratio": 1.5348837209302326, "no_speech_prob": 5.013565896661021e-06}, {"id": 407, "seek": 297200, "start": 2996.0, "end": 3000.0, "text": " So this is a version to kind of get back to the question about stochastic gradient descent,", "tokens": [407, 341, 307, 257, 3037, 281, 733, 295, 483, 646, 281, 264, 1168, 466, 342, 8997, 2750, 16235, 23475, 11], "temperature": 0.0, "avg_logprob": -0.054864600852683736, "compression_ratio": 1.5348837209302326, "no_speech_prob": 5.013565896661021e-06}, {"id": 408, "seek": 300000, "start": 3000.0, "end": 3008.0, "text": " a version that was not using stochastic gradient descent, but that is a lot more specific and optimized to NMF.", "tokens": [257, 3037, 300, 390, 406, 1228, 342, 8997, 2750, 16235, 23475, 11, 457, 300, 307, 257, 688, 544, 2685, 293, 26941, 281, 426, 44, 37, 13], "temperature": 0.0, "avg_logprob": -0.05749082565307617, "compression_ratio": 1.5255102040816326, "no_speech_prob": 2.225167008873541e-06}, {"id": 409, "seek": 300000, "start": 3008.0, "end": 3018.0, "text": " And so it's great, but the people that created it had to have a lot of kind of expertise and knowledge to do so.", "tokens": [400, 370, 309, 311, 869, 11, 457, 264, 561, 300, 2942, 309, 632, 281, 362, 257, 688, 295, 733, 295, 11769, 293, 3601, 281, 360, 370, 13], "temperature": 0.0, "avg_logprob": -0.05749082565307617, "compression_ratio": 1.5255102040816326, "no_speech_prob": 2.225167008873541e-06}, {"id": 410, "seek": 300000, "start": 3018.0, "end": 3023.0, "text": " So we'll look at kind of some alternative ways to calculate NMF ourselves.", "tokens": [407, 321, 603, 574, 412, 733, 295, 512, 8535, 2098, 281, 8873, 426, 44, 37, 4175, 13], "temperature": 0.0, "avg_logprob": -0.05749082565307617, "compression_ratio": 1.5255102040816326, "no_speech_prob": 2.225167008873541e-06}, {"id": 411, "seek": 302300, "start": 3023.0, "end": 3030.0, "text": " We'll come back, but let's break and meet back at 12.07.", "tokens": [492, 603, 808, 646, 11, 457, 718, 311, 1821, 293, 1677, 646, 412, 2272, 13, 16231, 13], "temperature": 0.0, "avg_logprob": -0.10581114888191223, "compression_ratio": 1.575609756097561, "no_speech_prob": 8.529546903446317e-06}, {"id": 412, "seek": 302300, "start": 3030.0, "end": 3031.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.10581114888191223, "compression_ratio": 1.575609756097561, "no_speech_prob": 8.529546903446317e-06}, {"id": 413, "seek": 302300, "start": 3031.0, "end": 3035.0, "text": " We're going to look at kind of return to non-negative matrix factorization.", "tokens": [492, 434, 516, 281, 574, 412, 733, 295, 2736, 281, 2107, 12, 28561, 1166, 8141, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.10581114888191223, "compression_ratio": 1.575609756097561, "no_speech_prob": 8.529546903446317e-06}, {"id": 414, "seek": 302300, "start": 3035.0, "end": 3039.0, "text": " So before, we were using scikit-learns implementation,", "tokens": [407, 949, 11, 321, 645, 1228, 2180, 22681, 12, 306, 1083, 82, 11420, 11], "temperature": 0.0, "avg_logprob": -0.10581114888191223, "compression_ratio": 1.575609756097561, "no_speech_prob": 8.529546903446317e-06}, {"id": 415, "seek": 302300, "start": 3039.0, "end": 3044.0, "text": " which is very specific to the problem of non-negative matrix factorization.", "tokens": [597, 307, 588, 2685, 281, 264, 1154, 295, 2107, 12, 28561, 1166, 8141, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.10581114888191223, "compression_ratio": 1.575609756097561, "no_speech_prob": 8.529546903446317e-06}, {"id": 416, "seek": 302300, "start": 3044.0, "end": 3049.0, "text": " And we're going to try writing our own in NumPy.", "tokens": [400, 321, 434, 516, 281, 853, 3579, 527, 1065, 294, 22592, 47, 88, 13], "temperature": 0.0, "avg_logprob": -0.10581114888191223, "compression_ratio": 1.575609756097561, "no_speech_prob": 8.529546903446317e-06}, {"id": 417, "seek": 304900, "start": 3049.0, "end": 3057.0, "text": " And we're going to use stochastic gradient descent, which is a lot more general.", "tokens": [400, 321, 434, 516, 281, 764, 342, 8997, 2750, 16235, 23475, 11, 597, 307, 257, 688, 544, 2674, 13], "temperature": 0.0, "avg_logprob": -0.13292519887288412, "compression_ratio": 1.6756756756756757, "no_speech_prob": 6.681400464003673e-07}, {"id": 418, "seek": 304900, "start": 3057.0, "end": 3065.0, "text": " So first I'm going to introduce just kind of the, or remind you of the basic idea of standard gradient descent,", "tokens": [407, 700, 286, 478, 516, 281, 5366, 445, 733, 295, 264, 11, 420, 4160, 291, 295, 264, 3875, 1558, 295, 3832, 16235, 23475, 11], "temperature": 0.0, "avg_logprob": -0.13292519887288412, "compression_ratio": 1.6756756756756757, "no_speech_prob": 6.681400464003673e-07}, {"id": 419, "seek": 304900, "start": 3065.0, "end": 3073.0, "text": " which is you choose some weights to start, and then you have a loop that uses your weights to calculate a prediction,", "tokens": [597, 307, 291, 2826, 512, 17443, 281, 722, 11, 293, 550, 291, 362, 257, 6367, 300, 4960, 428, 17443, 281, 8873, 257, 17630, 11], "temperature": 0.0, "avg_logprob": -0.13292519887288412, "compression_ratio": 1.6756756756756757, "no_speech_prob": 6.681400464003673e-07}, {"id": 420, "seek": 307300, "start": 3073.0, "end": 3080.0, "text": " calculates the derivative of the loss, and the loss is also known as the error function or the cost.", "tokens": [4322, 1024, 264, 13760, 295, 264, 4470, 11, 293, 264, 4470, 307, 611, 2570, 382, 264, 6713, 2445, 420, 264, 2063, 13], "temperature": 0.0, "avg_logprob": -0.06291661739349365, "compression_ratio": 1.8311111111111111, "no_speech_prob": 7.766606358927675e-06}, {"id": 421, "seek": 307300, "start": 3080.0, "end": 3082.0, "text": " But that's what you're trying to minimize.", "tokens": [583, 300, 311, 437, 291, 434, 1382, 281, 17522, 13], "temperature": 0.0, "avg_logprob": -0.06291661739349365, "compression_ratio": 1.8311111111111111, "no_speech_prob": 7.766606358927675e-06}, {"id": 422, "seek": 307300, "start": 3082.0, "end": 3091.0, "text": " And then you update the weights, and you kind of keep going through this loop to do that, and you get better and better weights.", "tokens": [400, 550, 291, 5623, 264, 17443, 11, 293, 291, 733, 295, 1066, 516, 807, 341, 6367, 281, 360, 300, 11, 293, 291, 483, 1101, 293, 1101, 17443, 13], "temperature": 0.0, "avg_logprob": -0.06291661739349365, "compression_ratio": 1.8311111111111111, "no_speech_prob": 7.766606358927675e-06}, {"id": 423, "seek": 307300, "start": 3091.0, "end": 3099.0, "text": " And the key here is that we're trying to decrease our loss, and the derivative is giving us the direction of steepest descent for our loss.", "tokens": [400, 264, 2141, 510, 307, 300, 321, 434, 1382, 281, 11514, 527, 4470, 11, 293, 264, 13760, 307, 2902, 505, 264, 3513, 295, 16841, 377, 23475, 337, 527, 4470, 13], "temperature": 0.0, "avg_logprob": -0.06291661739349365, "compression_ratio": 1.8311111111111111, "no_speech_prob": 7.766606358927675e-06}, {"id": 424, "seek": 309900, "start": 3099.0, "end": 3104.0, "text": " And so for this, I'm going to use the gradient descent.", "tokens": [400, 370, 337, 341, 11, 286, 478, 516, 281, 764, 264, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.14115510267369888, "compression_ratio": 1.45, "no_speech_prob": 2.026098627538886e-06}, {"id": 425, "seek": 309900, "start": 3104.0, "end": 3109.0, "text": " Oh, yes.", "tokens": [876, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.14115510267369888, "compression_ratio": 1.45, "no_speech_prob": 2.026098627538886e-06}, {"id": 426, "seek": 309900, "start": 3109.0, "end": 3112.0, "text": " I'm going to use an Excel notebook.", "tokens": [286, 478, 516, 281, 764, 364, 19060, 21060, 13], "temperature": 0.0, "avg_logprob": -0.14115510267369888, "compression_ratio": 1.45, "no_speech_prob": 2.026098627538886e-06}, {"id": 427, "seek": 309900, "start": 3112.0, "end": 3119.0, "text": " And this is something Jeremy originally developed for the deep learning course.", "tokens": [400, 341, 307, 746, 17809, 7993, 4743, 337, 264, 2452, 2539, 1164, 13], "temperature": 0.0, "avg_logprob": -0.14115510267369888, "compression_ratio": 1.45, "no_speech_prob": 2.026098627538886e-06}, {"id": 428, "seek": 309900, "start": 3119.0, "end": 3124.0, "text": " And it's, I think it's very helpful, and it's good.", "tokens": [400, 309, 311, 11, 286, 519, 309, 311, 588, 4961, 11, 293, 309, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.14115510267369888, "compression_ratio": 1.45, "no_speech_prob": 2.026098627538886e-06}, {"id": 429, "seek": 312400, "start": 3124.0, "end": 3132.0, "text": " I think many programmers can be kind of snobby about Excel, but Excel is really visual, and it's kind of a good way to see things.", "tokens": [286, 519, 867, 41504, 393, 312, 733, 295, 2406, 996, 2322, 466, 19060, 11, 457, 19060, 307, 534, 5056, 11, 293, 309, 311, 733, 295, 257, 665, 636, 281, 536, 721, 13], "temperature": 0.0, "avg_logprob": -0.07167087142000493, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.6118638035986805e-06}, {"id": 430, "seek": 312400, "start": 3132.0, "end": 3137.0, "text": " So I'm just going to kind of walk through gradient descent in here.", "tokens": [407, 286, 478, 445, 516, 281, 733, 295, 1792, 807, 16235, 23475, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.07167087142000493, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.6118638035986805e-06}, {"id": 431, "seek": 312400, "start": 3137.0, "end": 3141.0, "text": " Oh.", "tokens": [876, 13], "temperature": 0.0, "avg_logprob": -0.07167087142000493, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.6118638035986805e-06}, {"id": 432, "seek": 312400, "start": 3141.0, "end": 3144.0, "text": " Sorry, I misspoke.", "tokens": [4919, 11, 286, 1713, 48776, 13], "temperature": 0.0, "avg_logprob": -0.07167087142000493, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.6118638035986805e-06}, {"id": 433, "seek": 312400, "start": 3144.0, "end": 3146.0, "text": " I'm going to do an IPython notebook.", "tokens": [286, 478, 516, 281, 360, 364, 8671, 88, 11943, 21060, 13], "temperature": 0.0, "avg_logprob": -0.07167087142000493, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.6118638035986805e-06}, {"id": 434, "seek": 312400, "start": 3146.0, "end": 3151.0, "text": " The Excel is for stochastic gradient descent, which we'll get to next.", "tokens": [440, 19060, 307, 337, 342, 8997, 2750, 16235, 23475, 11, 597, 321, 603, 483, 281, 958, 13], "temperature": 0.0, "avg_logprob": -0.07167087142000493, "compression_ratio": 1.5592417061611374, "no_speech_prob": 3.6118638035986805e-06}, {"id": 435, "seek": 315100, "start": 3151.0, "end": 3154.0, "text": " Although this is also from the deep learning course.", "tokens": [5780, 341, 307, 611, 490, 264, 2452, 2539, 1164, 13], "temperature": 0.0, "avg_logprob": -0.07547553030999152, "compression_ratio": 1.4782608695652173, "no_speech_prob": 7.571018159069354e-07}, {"id": 436, "seek": 315100, "start": 3154.0, "end": 3157.0, "text": " And so this notebook is called gradient descent intro.", "tokens": [400, 370, 341, 21060, 307, 1219, 16235, 23475, 12897, 13], "temperature": 0.0, "avg_logprob": -0.07547553030999152, "compression_ratio": 1.4782608695652173, "no_speech_prob": 7.571018159069354e-07}, {"id": 437, "seek": 315100, "start": 3157.0, "end": 3163.0, "text": " It should also be on GitHub in the class repository.", "tokens": [467, 820, 611, 312, 322, 23331, 294, 264, 1508, 25841, 13], "temperature": 0.0, "avg_logprob": -0.07547553030999152, "compression_ratio": 1.4782608695652173, "no_speech_prob": 7.571018159069354e-07}, {"id": 438, "seek": 315100, "start": 3163.0, "end": 3171.0, "text": " So here we've got a method for a line that takes in A, B, and X and returns A times X plus B.", "tokens": [407, 510, 321, 600, 658, 257, 3170, 337, 257, 1622, 300, 2516, 294, 316, 11, 363, 11, 293, 1783, 293, 11247, 316, 1413, 1783, 1804, 363, 13], "temperature": 0.0, "avg_logprob": -0.07547553030999152, "compression_ratio": 1.4782608695652173, "no_speech_prob": 7.571018159069354e-07}, {"id": 439, "seek": 315100, "start": 3171.0, "end": 3176.0, "text": " We're going to choose our slope and intercept.", "tokens": [492, 434, 516, 281, 2826, 527, 13525, 293, 24700, 13], "temperature": 0.0, "avg_logprob": -0.07547553030999152, "compression_ratio": 1.4782608695652173, "no_speech_prob": 7.571018159069354e-07}, {"id": 440, "seek": 315100, "start": 3176.0, "end": 3180.0, "text": " This is what typically you don't know.", "tokens": [639, 307, 437, 5850, 291, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.07547553030999152, "compression_ratio": 1.4782608695652173, "no_speech_prob": 7.571018159069354e-07}, {"id": 441, "seek": 318000, "start": 3180.0, "end": 3183.0, "text": " So we're going to kind of choose these to make a fake data set.", "tokens": [407, 321, 434, 516, 281, 733, 295, 2826, 613, 281, 652, 257, 7592, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.07543038386924594, "compression_ratio": 1.5848214285714286, "no_speech_prob": 6.540009508171352e-06}, {"id": 442, "seek": 318000, "start": 3183.0, "end": 3188.0, "text": " And then this will let us check kind of how good our method is if we can get back to them.", "tokens": [400, 550, 341, 486, 718, 505, 1520, 733, 295, 577, 665, 527, 3170, 307, 498, 321, 393, 483, 646, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.07543038386924594, "compression_ratio": 1.5848214285714286, "no_speech_prob": 6.540009508171352e-06}, {"id": 443, "seek": 318000, "start": 3188.0, "end": 3193.0, "text": " But in the real world, you don't know what the quote true A and B are.", "tokens": [583, 294, 264, 957, 1002, 11, 291, 500, 380, 458, 437, 264, 6513, 2074, 316, 293, 363, 366, 13], "temperature": 0.0, "avg_logprob": -0.07543038386924594, "compression_ratio": 1.5848214285714286, "no_speech_prob": 6.540009508171352e-06}, {"id": 444, "seek": 318000, "start": 3193.0, "end": 3196.0, "text": " And that's what you're trying to figure out.", "tokens": [400, 300, 311, 437, 291, 434, 1382, 281, 2573, 484, 13], "temperature": 0.0, "avg_logprob": -0.07543038386924594, "compression_ratio": 1.5848214285714286, "no_speech_prob": 6.540009508171352e-06}, {"id": 445, "seek": 318000, "start": 3196.0, "end": 3201.0, "text": " So here we generate 30 random data points.", "tokens": [407, 510, 321, 8460, 2217, 4974, 1412, 2793, 13], "temperature": 0.0, "avg_logprob": -0.07543038386924594, "compression_ratio": 1.5848214285714286, "no_speech_prob": 6.540009508171352e-06}, {"id": 446, "seek": 318000, "start": 3201.0, "end": 3205.0, "text": " So we've got this array X with 30 points.", "tokens": [407, 321, 600, 658, 341, 10225, 1783, 365, 2217, 2793, 13], "temperature": 0.0, "avg_logprob": -0.07543038386924594, "compression_ratio": 1.5848214285714286, "no_speech_prob": 6.540009508171352e-06}, {"id": 447, "seek": 320500, "start": 3205.0, "end": 3211.0, "text": " Y is just A times X plus B. We can plot them.", "tokens": [398, 307, 445, 316, 1413, 1783, 1804, 363, 13, 492, 393, 7542, 552, 13], "temperature": 0.0, "avg_logprob": -0.074228554629208, "compression_ratio": 1.5141509433962264, "no_speech_prob": 6.144001417851541e-06}, {"id": 448, "seek": 320500, "start": 3211.0, "end": 3219.0, "text": " They're perfectly aligned, which is what we would expect since we use the line method to create them.", "tokens": [814, 434, 6239, 17962, 11, 597, 307, 437, 321, 576, 2066, 1670, 321, 764, 264, 1622, 3170, 281, 1884, 552, 13], "temperature": 0.0, "avg_logprob": -0.074228554629208, "compression_ratio": 1.5141509433962264, "no_speech_prob": 6.144001417851541e-06}, {"id": 449, "seek": 320500, "start": 3219.0, "end": 3222.0, "text": " And then here we have a few methods.", "tokens": [400, 550, 510, 321, 362, 257, 1326, 7150, 13], "temperature": 0.0, "avg_logprob": -0.074228554629208, "compression_ratio": 1.5141509433962264, "no_speech_prob": 6.144001417851541e-06}, {"id": 450, "seek": 320500, "start": 3222.0, "end": 3225.0, "text": " SSE stands for sum of squared errors.", "tokens": [318, 5879, 7382, 337, 2408, 295, 8889, 13603, 13], "temperature": 0.0, "avg_logprob": -0.074228554629208, "compression_ratio": 1.5141509433962264, "no_speech_prob": 6.144001417851541e-06}, {"id": 451, "seek": 320500, "start": 3225.0, "end": 3232.0, "text": " So that will take a kind of true Y and our predicted Y, subtract them and square it and sum it up.", "tokens": [407, 300, 486, 747, 257, 733, 295, 2074, 398, 293, 527, 19147, 398, 11, 16390, 552, 293, 3732, 309, 293, 2408, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.074228554629208, "compression_ratio": 1.5141509433962264, "no_speech_prob": 6.144001417851541e-06}, {"id": 452, "seek": 323200, "start": 3232.0, "end": 3235.0, "text": " That's going to be our loss here.", "tokens": [663, 311, 516, 281, 312, 527, 4470, 510, 13], "temperature": 0.0, "avg_logprob": -0.055003160085433565, "compression_ratio": 1.572972972972973, "no_speech_prob": 1.5293958313122857e-06}, {"id": 453, "seek": 323200, "start": 3235.0, "end": 3239.0, "text": " So loss method just calls the sum of squared errors.", "tokens": [407, 4470, 3170, 445, 5498, 264, 2408, 295, 8889, 13603, 13], "temperature": 0.0, "avg_logprob": -0.055003160085433565, "compression_ratio": 1.572972972972973, "no_speech_prob": 1.5293958313122857e-06}, {"id": 454, "seek": 323200, "start": 3239.0, "end": 3242.0, "text": " And then we can get the average loss over.", "tokens": [400, 550, 321, 393, 483, 264, 4274, 4470, 670, 13], "temperature": 0.0, "avg_logprob": -0.055003160085433565, "compression_ratio": 1.572972972972973, "no_speech_prob": 1.5293958313122857e-06}, {"id": 455, "seek": 323200, "start": 3242.0, "end": 3252.0, "text": " So we have a bunch of points by taking the square root of the loss divided by the number of points that there were.", "tokens": [407, 321, 362, 257, 3840, 295, 2793, 538, 1940, 264, 3732, 5593, 295, 264, 4470, 6666, 538, 264, 1230, 295, 2793, 300, 456, 645, 13], "temperature": 0.0, "avg_logprob": -0.055003160085433565, "compression_ratio": 1.572972972972973, "no_speech_prob": 1.5293958313122857e-06}, {"id": 456, "seek": 323200, "start": 3252.0, "end": 3259.0, "text": " So let's start off guessing negative 1 and 1.", "tokens": [407, 718, 311, 722, 766, 17939, 3671, 502, 293, 502, 13], "temperature": 0.0, "avg_logprob": -0.055003160085433565, "compression_ratio": 1.572972972972973, "no_speech_prob": 1.5293958313122857e-06}, {"id": 457, "seek": 325900, "start": 3259.0, "end": 3266.0, "text": " So I'm going to actually rerun this.", "tokens": [407, 286, 478, 516, 281, 767, 43819, 409, 341, 13], "temperature": 0.0, "avg_logprob": -0.14934292504953783, "compression_ratio": 1.4277777777777778, "no_speech_prob": 5.093631898489548e-06}, {"id": 458, "seek": 325900, "start": 3266.0, "end": 3270.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.14934292504953783, "compression_ratio": 1.4277777777777778, "no_speech_prob": 5.093631898489548e-06}, {"id": 459, "seek": 325900, "start": 3270.0, "end": 3271.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.14934292504953783, "compression_ratio": 1.4277777777777778, "no_speech_prob": 5.093631898489548e-06}, {"id": 460, "seek": 325900, "start": 3271.0, "end": 3272.0, "text": " So our loss is 8.9.", "tokens": [407, 527, 4470, 307, 1649, 13, 24, 13], "temperature": 0.0, "avg_logprob": -0.14934292504953783, "compression_ratio": 1.4277777777777778, "no_speech_prob": 5.093631898489548e-06}, {"id": 461, "seek": 325900, "start": 3272.0, "end": 3274.0, "text": " So we're not doing great to start.", "tokens": [407, 321, 434, 406, 884, 869, 281, 722, 13], "temperature": 0.0, "avg_logprob": -0.14934292504953783, "compression_ratio": 1.4277777777777778, "no_speech_prob": 5.093631898489548e-06}, {"id": 462, "seek": 325900, "start": 3274.0, "end": 3276.0, "text": " We'll choose what's called a learning rate.", "tokens": [492, 603, 2826, 437, 311, 1219, 257, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.14934292504953783, "compression_ratio": 1.4277777777777778, "no_speech_prob": 5.093631898489548e-06}, {"id": 463, "seek": 325900, "start": 3276.0, "end": 3278.0, "text": " So 0.01 is where we're starting.", "tokens": [407, 1958, 13, 10607, 307, 689, 321, 434, 2891, 13], "temperature": 0.0, "avg_logprob": -0.14934292504953783, "compression_ratio": 1.4277777777777778, "no_speech_prob": 5.093631898489548e-06}, {"id": 464, "seek": 325900, "start": 3278.0, "end": 3282.0, "text": " That's something, though, that you'll typically adjust as you're going.", "tokens": [663, 311, 746, 11, 1673, 11, 300, 291, 603, 5850, 4369, 382, 291, 434, 516, 13], "temperature": 0.0, "avg_logprob": -0.14934292504953783, "compression_ratio": 1.4277777777777778, "no_speech_prob": 5.093631898489548e-06}, {"id": 465, "seek": 328200, "start": 3282.0, "end": 3292.0, "text": " And then each time we update, what we'll do is kind of make predictions using our guess for A and our guess for B.", "tokens": [400, 550, 1184, 565, 321, 5623, 11, 437, 321, 603, 360, 307, 733, 295, 652, 21264, 1228, 527, 2041, 337, 316, 293, 527, 2041, 337, 363, 13], "temperature": 0.0, "avg_logprob": -0.07711248083429022, "compression_ratio": 1.7731958762886597, "no_speech_prob": 3.089445726800477e-06}, {"id": 466, "seek": 328200, "start": 3292.0, "end": 3300.0, "text": " We know what the derivative of the loss is since this is a line or sorry, this is the derivative of sum of squared errors.", "tokens": [492, 458, 437, 264, 13760, 295, 264, 4470, 307, 1670, 341, 307, 257, 1622, 420, 2597, 11, 341, 307, 264, 13760, 295, 2408, 295, 8889, 13603, 13], "temperature": 0.0, "avg_logprob": -0.07711248083429022, "compression_ratio": 1.7731958762886597, "no_speech_prob": 3.089445726800477e-06}, {"id": 467, "seek": 328200, "start": 3300.0, "end": 3302.0, "text": " So we're using that derivative.", "tokens": [407, 321, 434, 1228, 300, 13760, 13], "temperature": 0.0, "avg_logprob": -0.07711248083429022, "compression_ratio": 1.7731958762886597, "no_speech_prob": 3.089445726800477e-06}, {"id": 468, "seek": 328200, "start": 3302.0, "end": 3306.0, "text": " And then we're updating our guesses A and B.", "tokens": [400, 550, 321, 434, 25113, 527, 42703, 316, 293, 363, 13], "temperature": 0.0, "avg_logprob": -0.07711248083429022, "compression_ratio": 1.7731958762886597, "no_speech_prob": 3.089445726800477e-06}, {"id": 469, "seek": 328200, "start": 3306.0, "end": 3307.0, "text": " And then this is really neat.", "tokens": [400, 550, 341, 307, 534, 10654, 13], "temperature": 0.0, "avg_logprob": -0.07711248083429022, "compression_ratio": 1.7731958762886597, "no_speech_prob": 3.089445726800477e-06}, {"id": 470, "seek": 330700, "start": 3307.0, "end": 3312.0, "text": " This is an animation that shows kind of what's happening.", "tokens": [639, 307, 364, 9603, 300, 3110, 733, 295, 437, 311, 2737, 13], "temperature": 0.0, "avg_logprob": -0.07119501961602105, "compression_ratio": 1.4733727810650887, "no_speech_prob": 2.812958427966805e-06}, {"id": 471, "seek": 330700, "start": 3312.0, "end": 3316.0, "text": " And this is a bit, let me stop this.", "tokens": [400, 341, 307, 257, 857, 11, 718, 385, 1590, 341, 13], "temperature": 0.0, "avg_logprob": -0.07119501961602105, "compression_ratio": 1.4733727810650887, "no_speech_prob": 2.812958427966805e-06}, {"id": 472, "seek": 330700, "start": 3316.0, "end": 3324.0, "text": " So we're kind of starting down here when we've guessed 1 and 1 for our A and B, which is not a very close approximation at all.", "tokens": [407, 321, 434, 733, 295, 2891, 760, 510, 562, 321, 600, 21852, 502, 293, 502, 337, 527, 316, 293, 363, 11, 597, 307, 406, 257, 588, 1998, 28023, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.07119501961602105, "compression_ratio": 1.4733727810650887, "no_speech_prob": 2.812958427966805e-06}, {"id": 473, "seek": 330700, "start": 3324.0, "end": 3326.0, "text": " And we can see as it runs.", "tokens": [400, 321, 393, 536, 382, 309, 6676, 13], "temperature": 0.0, "avg_logprob": -0.07119501961602105, "compression_ratio": 1.4733727810650887, "no_speech_prob": 2.812958427966805e-06}, {"id": 474, "seek": 332600, "start": 3326.0, "end": 3337.0, "text": " So here inside of animate, we just have a for loop that's calling our update method from up here, UPD.", "tokens": [407, 510, 1854, 295, 36439, 11, 321, 445, 362, 257, 337, 6367, 300, 311, 5141, 527, 5623, 3170, 490, 493, 510, 11, 624, 17349, 13], "temperature": 0.0, "avg_logprob": -0.07335280437095493, "compression_ratio": 1.3402777777777777, "no_speech_prob": 1.0676706097001443e-06}, {"id": 475, "seek": 332600, "start": 3337.0, "end": 3346.0, "text": " And so we can see that the line gets closer and closer until it's a really accurate guess.", "tokens": [400, 370, 321, 393, 536, 300, 264, 1622, 2170, 4966, 293, 4966, 1826, 309, 311, 257, 534, 8559, 2041, 13], "temperature": 0.0, "avg_logprob": -0.07335280437095493, "compression_ratio": 1.3402777777777777, "no_speech_prob": 1.0676706097001443e-06}, {"id": 476, "seek": 334600, "start": 3346.0, "end": 3357.0, "text": " Are there any questions about this?", "tokens": [2014, 456, 604, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.09833134775576384, "compression_ratio": 1.2834645669291338, "no_speech_prob": 2.857230356312357e-06}, {"id": 477, "seek": 334600, "start": 3357.0, "end": 3359.0, "text": " Oh, down here.", "tokens": [876, 11, 760, 510, 13], "temperature": 0.0, "avg_logprob": -0.09833134775576384, "compression_ratio": 1.2834645669291338, "no_speech_prob": 2.857230356312357e-06}, {"id": 478, "seek": 334600, "start": 3359.0, "end": 3364.0, "text": " Yeah. So the derivative is giving the direction of steepest descent.", "tokens": [865, 13, 407, 264, 13760, 307, 2902, 264, 3513, 295, 16841, 377, 23475, 13], "temperature": 0.0, "avg_logprob": -0.09833134775576384, "compression_ratio": 1.2834645669291338, "no_speech_prob": 2.857230356312357e-06}, {"id": 479, "seek": 334600, "start": 3364.0, "end": 3368.0, "text": " So we want to subtract that from our guess.", "tokens": [407, 321, 528, 281, 16390, 300, 490, 527, 2041, 13], "temperature": 0.0, "avg_logprob": -0.09833134775576384, "compression_ratio": 1.2834645669291338, "no_speech_prob": 2.857230356312357e-06}, {"id": 480, "seek": 336800, "start": 3368.0, "end": 3377.0, "text": " The reason we use a learning rate is if this is a large number, we could end up kind of jumping back and forth from the true answer.", "tokens": [440, 1778, 321, 764, 257, 2539, 3314, 307, 498, 341, 307, 257, 2416, 1230, 11, 321, 727, 917, 493, 733, 295, 11233, 646, 293, 5220, 490, 264, 2074, 1867, 13], "temperature": 0.0, "avg_logprob": -0.03127559874821635, "compression_ratio": 1.7413793103448276, "no_speech_prob": 3.3930371046153596e-06}, {"id": 481, "seek": 336800, "start": 3377.0, "end": 3380.0, "text": " And so it's kind of better to take smaller steps.", "tokens": [400, 370, 309, 311, 733, 295, 1101, 281, 747, 4356, 4439, 13], "temperature": 0.0, "avg_logprob": -0.03127559874821635, "compression_ratio": 1.7413793103448276, "no_speech_prob": 3.3930371046153596e-06}, {"id": 482, "seek": 336800, "start": 3380.0, "end": 3387.0, "text": " And so the learning rate is basically the kind of the step size of like, OK, the derivative gives us the direction we want to go in.", "tokens": [400, 370, 264, 2539, 3314, 307, 1936, 264, 733, 295, 264, 1823, 2744, 295, 411, 11, 2264, 11, 264, 13760, 2709, 505, 264, 3513, 321, 528, 281, 352, 294, 13], "temperature": 0.0, "avg_logprob": -0.03127559874821635, "compression_ratio": 1.7413793103448276, "no_speech_prob": 3.3930371046153596e-06}, {"id": 483, "seek": 336800, "start": 3387.0, "end": 3393.0, "text": " And learning rate is telling us how big a step to take in that direction.", "tokens": [400, 2539, 3314, 307, 3585, 505, 577, 955, 257, 1823, 281, 747, 294, 300, 3513, 13], "temperature": 0.0, "avg_logprob": -0.03127559874821635, "compression_ratio": 1.7413793103448276, "no_speech_prob": 3.3930371046153596e-06}, {"id": 484, "seek": 336800, "start": 3393.0, "end": 3397.0, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.03127559874821635, "compression_ratio": 1.7413793103448276, "no_speech_prob": 3.3930371046153596e-06}, {"id": 485, "seek": 339700, "start": 3397.0, "end": 3399.0, "text": " OK, so this is standard gradient descent.", "tokens": [2264, 11, 370, 341, 307, 3832, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.09624783198038737, "compression_ratio": 1.4967741935483871, "no_speech_prob": 1.2804440302716102e-05}, {"id": 486, "seek": 339700, "start": 3399.0, "end": 3412.0, "text": " There was nothing stochastic about that.", "tokens": [821, 390, 1825, 342, 8997, 2750, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.09624783198038737, "compression_ratio": 1.4967741935483871, "no_speech_prob": 1.2804440302716102e-05}, {"id": 487, "seek": 339700, "start": 3412.0, "end": 3420.0, "text": " And so the idea with SGD is so with standard gradient descent, we evaluated the loss on all of our data.", "tokens": [400, 370, 264, 1558, 365, 34520, 35, 307, 370, 365, 3832, 16235, 23475, 11, 321, 25509, 264, 4470, 322, 439, 295, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.09624783198038737, "compression_ratio": 1.4967741935483871, "no_speech_prob": 1.2804440302716102e-05}, {"id": 488, "seek": 339700, "start": 3420.0, "end": 3423.0, "text": " In that example, we only had 30 data points.", "tokens": [682, 300, 1365, 11, 321, 787, 632, 2217, 1412, 2793, 13], "temperature": 0.0, "avg_logprob": -0.09624783198038737, "compression_ratio": 1.4967741935483871, "no_speech_prob": 1.2804440302716102e-05}, {"id": 489, "seek": 342300, "start": 3423.0, "end": 3429.0, "text": " So it was very quick. In the most problems you'll use, you have way too much data.", "tokens": [407, 309, 390, 588, 1702, 13, 682, 264, 881, 2740, 291, 603, 764, 11, 291, 362, 636, 886, 709, 1412, 13], "temperature": 0.0, "avg_logprob": -0.05835321907685182, "compression_ratio": 1.67578125, "no_speech_prob": 4.565572908177273e-06}, {"id": 490, "seek": 342300, "start": 3429.0, "end": 3433.0, "text": " And it's really slow to evaluate the loss on every single point.", "tokens": [400, 309, 311, 534, 2964, 281, 13059, 264, 4470, 322, 633, 2167, 935, 13], "temperature": 0.0, "avg_logprob": -0.05835321907685182, "compression_ratio": 1.67578125, "no_speech_prob": 4.565572908177273e-06}, {"id": 491, "seek": 342300, "start": 3433.0, "end": 3440.0, "text": " And it's also kind of unnecessary, particularly when you're far away from the true answer, to do that much of an evaluation.", "tokens": [400, 309, 311, 611, 733, 295, 19350, 11, 4098, 562, 291, 434, 1400, 1314, 490, 264, 2074, 1867, 11, 281, 360, 300, 709, 295, 364, 13344, 13], "temperature": 0.0, "avg_logprob": -0.05835321907685182, "compression_ratio": 1.67578125, "no_speech_prob": 4.565572908177273e-06}, {"id": 492, "seek": 342300, "start": 3440.0, "end": 3446.0, "text": " And so stochastic gradient descent is the idea of evaluating the loss function on just a small sample of the data.", "tokens": [400, 370, 342, 8997, 2750, 16235, 23475, 307, 264, 1558, 295, 27479, 264, 4470, 2445, 322, 445, 257, 1359, 6889, 295, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.05835321907685182, "compression_ratio": 1.67578125, "no_speech_prob": 4.565572908177273e-06}, {"id": 493, "seek": 342300, "start": 3446.0, "end": 3450.0, "text": " And that's typically called a mini batch.", "tokens": [400, 300, 311, 5850, 1219, 257, 8382, 15245, 13], "temperature": 0.0, "avg_logprob": -0.05835321907685182, "compression_ratio": 1.67578125, "no_speech_prob": 4.565572908177273e-06}, {"id": 494, "seek": 345000, "start": 3450.0, "end": 3461.0, "text": " And that's why it's stochastic, because depending kind of which batch you choose to or mini batch you calculate your loss on, you'll get different answers.", "tokens": [400, 300, 311, 983, 309, 311, 342, 8997, 2750, 11, 570, 5413, 733, 295, 597, 15245, 291, 2826, 281, 420, 8382, 15245, 291, 8873, 428, 4470, 322, 11, 291, 603, 483, 819, 6338, 13], "temperature": 0.0, "avg_logprob": -0.0679545455508762, "compression_ratio": 1.6008771929824561, "no_speech_prob": 2.947861503344029e-06}, {"id": 495, "seek": 345000, "start": 3461.0, "end": 3470.0, "text": " But it turns out that kind of an aggregate, because this is part of a loop, it's good enough and you get a huge, huge improvement in speed.", "tokens": [583, 309, 4523, 484, 300, 733, 295, 364, 26118, 11, 570, 341, 307, 644, 295, 257, 6367, 11, 309, 311, 665, 1547, 293, 291, 483, 257, 2603, 11, 2603, 10444, 294, 3073, 13], "temperature": 0.0, "avg_logprob": -0.0679545455508762, "compression_ratio": 1.6008771929824561, "no_speech_prob": 2.947861503344029e-06}, {"id": 496, "seek": 345000, "start": 3470.0, "end": 3477.0, "text": " And so this is what I was going to show inside the Excel spreadsheet.", "tokens": [400, 370, 341, 307, 437, 286, 390, 516, 281, 855, 1854, 264, 19060, 27733, 13], "temperature": 0.0, "avg_logprob": -0.0679545455508762, "compression_ratio": 1.6008771929824561, "no_speech_prob": 2.947861503344029e-06}, {"id": 497, "seek": 347700, "start": 3477.0, "end": 3481.0, "text": " And this one will start in a very similar way.", "tokens": [400, 341, 472, 486, 722, 294, 257, 588, 2531, 636, 13], "temperature": 0.0, "avg_logprob": -0.06099903583526611, "compression_ratio": 1.5887445887445888, "no_speech_prob": 2.7105243134428747e-05}, {"id": 498, "seek": 347700, "start": 3481.0, "end": 3487.0, "text": " So we're just in this Excel spreadsheet is also available in the GitHub repository.", "tokens": [407, 321, 434, 445, 294, 341, 19060, 27733, 307, 611, 2435, 294, 264, 23331, 25841, 13], "temperature": 0.0, "avg_logprob": -0.06099903583526611, "compression_ratio": 1.5887445887445888, "no_speech_prob": 2.7105243134428747e-05}, {"id": 499, "seek": 347700, "start": 3487.0, "end": 3494.0, "text": " And there are tools like OpenOffice that you can kind of look at the spreadsheet without having Excel.", "tokens": [400, 456, 366, 3873, 411, 7238, 29745, 573, 300, 291, 393, 733, 295, 574, 412, 264, 27733, 1553, 1419, 19060, 13], "temperature": 0.0, "avg_logprob": -0.06099903583526611, "compression_ratio": 1.5887445887445888, "no_speech_prob": 2.7105243134428747e-05}, {"id": 500, "seek": 347700, "start": 3494.0, "end": 3498.0, "text": " So we've just chosen kind of an A and B to generate our data from.", "tokens": [407, 321, 600, 445, 8614, 733, 295, 364, 316, 293, 363, 281, 8460, 527, 1412, 490, 13], "temperature": 0.0, "avg_logprob": -0.06099903583526611, "compression_ratio": 1.5887445887445888, "no_speech_prob": 2.7105243134428747e-05}, {"id": 501, "seek": 347700, "start": 3498.0, "end": 3502.0, "text": " And this is helpful because it lets us see how good our answer is.", "tokens": [400, 341, 307, 4961, 570, 309, 6653, 505, 536, 577, 665, 527, 1867, 307, 13], "temperature": 0.0, "avg_logprob": -0.06099903583526611, "compression_ratio": 1.5887445887445888, "no_speech_prob": 2.7105243134428747e-05}, {"id": 502, "seek": 350200, "start": 3502.0, "end": 3507.0, "text": " But this is kind of the part where you would typically have a real data set.", "tokens": [583, 341, 307, 733, 295, 264, 644, 689, 291, 576, 5850, 362, 257, 957, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.058409398145014696, "compression_ratio": 1.5313807531380754, "no_speech_prob": 4.936778168485034e-06}, {"id": 503, "seek": 350200, "start": 3507.0, "end": 3512.0, "text": " So I've got our data and then the basic SGD.", "tokens": [407, 286, 600, 658, 527, 1412, 293, 550, 264, 3875, 34520, 35, 13], "temperature": 0.0, "avg_logprob": -0.058409398145014696, "compression_ratio": 1.5313807531380754, "no_speech_prob": 4.936778168485034e-06}, {"id": 504, "seek": 350200, "start": 3512.0, "end": 3516.0, "text": " We're going to make some guesses for what the intercept and slope are.", "tokens": [492, 434, 516, 281, 652, 512, 42703, 337, 437, 264, 24700, 293, 13525, 366, 13], "temperature": 0.0, "avg_logprob": -0.058409398145014696, "compression_ratio": 1.5313807531380754, "no_speech_prob": 4.936778168485034e-06}, {"id": 505, "seek": 350200, "start": 3516.0, "end": 3523.0, "text": " Here's this learning rate, which is what we're going to multiply by to figure out how big a step to take.", "tokens": [1692, 311, 341, 2539, 3314, 11, 597, 307, 437, 321, 434, 516, 281, 12972, 538, 281, 2573, 484, 577, 955, 257, 1823, 281, 747, 13], "temperature": 0.0, "avg_logprob": -0.058409398145014696, "compression_ratio": 1.5313807531380754, "no_speech_prob": 4.936778168485034e-06}, {"id": 506, "seek": 350200, "start": 3523.0, "end": 3525.0, "text": " Our data has been copied in.", "tokens": [2621, 1412, 575, 668, 25365, 294, 13], "temperature": 0.0, "avg_logprob": -0.058409398145014696, "compression_ratio": 1.5313807531380754, "no_speech_prob": 4.936778168485034e-06}, {"id": 507, "seek": 350200, "start": 3525.0, "end": 3528.0, "text": " And here are many batches of size one.", "tokens": [400, 510, 366, 867, 15245, 279, 295, 2744, 472, 13], "temperature": 0.0, "avg_logprob": -0.058409398145014696, "compression_ratio": 1.5313807531380754, "no_speech_prob": 4.936778168485034e-06}, {"id": 508, "seek": 352800, "start": 3528.0, "end": 3533.0, "text": " So we're going to just calculate what the derivative of the loss is.", "tokens": [407, 321, 434, 516, 281, 445, 8873, 437, 264, 13760, 295, 264, 4470, 307, 13], "temperature": 0.0, "avg_logprob": -0.05512185015920865, "compression_ratio": 1.5857142857142856, "no_speech_prob": 7.766602720948867e-06}, {"id": 509, "seek": 352800, "start": 3533.0, "end": 3536.0, "text": " So here's our prediction.", "tokens": [407, 510, 311, 527, 17630, 13], "temperature": 0.0, "avg_logprob": -0.05512185015920865, "compression_ratio": 1.5857142857142856, "no_speech_prob": 7.766602720948867e-06}, {"id": 510, "seek": 352800, "start": 3536.0, "end": 3540.0, "text": " Here's the error.", "tokens": [1692, 311, 264, 6713, 13], "temperature": 0.0, "avg_logprob": -0.05512185015920865, "compression_ratio": 1.5857142857142856, "no_speech_prob": 7.766602720948867e-06}, {"id": 511, "seek": 352800, "start": 3540.0, "end": 3549.0, "text": " And then we calculate the derivative of the error just at that single point and then update A and B for that.", "tokens": [400, 550, 321, 8873, 264, 13760, 295, 264, 6713, 445, 412, 300, 2167, 935, 293, 550, 5623, 316, 293, 363, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.05512185015920865, "compression_ratio": 1.5857142857142856, "no_speech_prob": 7.766602720948867e-06}, {"id": 512, "seek": 354900, "start": 3549.0, "end": 3566.0, "text": " And so we can see this.", "tokens": [400, 370, 321, 393, 536, 341, 13], "temperature": 0.0, "avg_logprob": -0.08855215384035695, "compression_ratio": 1.2727272727272727, "no_speech_prob": 5.954763310000999e-06}, {"id": 513, "seek": 354900, "start": 3566.0, "end": 3569.0, "text": " You might want to increase the learning rate.", "tokens": [509, 1062, 528, 281, 3488, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.08855215384035695, "compression_ratio": 1.2727272727272727, "no_speech_prob": 5.954763310000999e-06}, {"id": 514, "seek": 354900, "start": 3569.0, "end": 3570.0, "text": " This is going pretty slow.", "tokens": [639, 307, 516, 1238, 2964, 13], "temperature": 0.0, "avg_logprob": -0.08855215384035695, "compression_ratio": 1.2727272727272727, "no_speech_prob": 5.954763310000999e-06}, {"id": 515, "seek": 354900, "start": 3570.0, "end": 3576.0, "text": " So the error is getting smaller.", "tokens": [407, 264, 6713, 307, 1242, 4356, 13], "temperature": 0.0, "avg_logprob": -0.08855215384035695, "compression_ratio": 1.2727272727272727, "no_speech_prob": 5.954763310000999e-06}, {"id": 516, "seek": 354900, "start": 3576.0, "end": 3578.0, "text": " Oh, no, that's horrible.", "tokens": [876, 11, 572, 11, 300, 311, 9263, 13], "temperature": 0.0, "avg_logprob": -0.08855215384035695, "compression_ratio": 1.2727272727272727, "no_speech_prob": 5.954763310000999e-06}, {"id": 517, "seek": 357800, "start": 3578.0, "end": 3582.0, "text": " Okay, I won't do this on.", "tokens": [1033, 11, 286, 1582, 380, 360, 341, 322, 13], "temperature": 0.0, "avg_logprob": -0.0917268524999204, "compression_ratio": 1.5521739130434782, "no_speech_prob": 2.295785861861077e-06}, {"id": 518, "seek": 357800, "start": 3582.0, "end": 3585.0, "text": " I won't improvise this right now.", "tokens": [286, 1582, 380, 29424, 908, 341, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.0917268524999204, "compression_ratio": 1.5521739130434782, "no_speech_prob": 2.295785861861077e-06}, {"id": 519, "seek": 357800, "start": 3585.0, "end": 3588.0, "text": " But you can adjust the learning rate to take smaller, bigger steps.", "tokens": [583, 291, 393, 4369, 264, 2539, 3314, 281, 747, 4356, 11, 3801, 4439, 13], "temperature": 0.0, "avg_logprob": -0.0917268524999204, "compression_ratio": 1.5521739130434782, "no_speech_prob": 2.295785861861077e-06}, {"id": 520, "seek": 357800, "start": 3588.0, "end": 3596.0, "text": " If you take too big a step, this is what can happen to you and you shoot off in the wrong direction.", "tokens": [759, 291, 747, 886, 955, 257, 1823, 11, 341, 307, 437, 393, 1051, 281, 291, 293, 291, 3076, 766, 294, 264, 2085, 3513, 13], "temperature": 0.0, "avg_logprob": -0.0917268524999204, "compression_ratio": 1.5521739130434782, "no_speech_prob": 2.295785861861077e-06}, {"id": 521, "seek": 357800, "start": 3596.0, "end": 3601.0, "text": " There are questions about stochastic gradient descent.", "tokens": [821, 366, 1651, 466, 342, 8997, 2750, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.0917268524999204, "compression_ratio": 1.5521739130434782, "no_speech_prob": 2.295785861861077e-06}, {"id": 522, "seek": 357800, "start": 3601.0, "end": 3605.0, "text": " And then this notebook includes a bunch of other optimization techniques.", "tokens": [400, 550, 341, 21060, 5974, 257, 3840, 295, 661, 19618, 7512, 13], "temperature": 0.0, "avg_logprob": -0.0917268524999204, "compression_ratio": 1.5521739130434782, "no_speech_prob": 2.295785861861077e-06}, {"id": 523, "seek": 360500, "start": 3605.0, "end": 3612.0, "text": " We won't be covering them in this class, but they might be of interest to you if you're interested in optimization.", "tokens": [492, 1582, 380, 312, 10322, 552, 294, 341, 1508, 11, 457, 436, 1062, 312, 295, 1179, 281, 291, 498, 291, 434, 3102, 294, 19618, 13], "temperature": 0.0, "avg_logprob": -0.24418729146321613, "compression_ratio": 1.671875, "no_speech_prob": 0.00019098589837085456}, {"id": 524, "seek": 360500, "start": 3612.0, "end": 3615.0, "text": " On the spreadsheet, you don't need to use the macros or the buttons.", "tokens": [1282, 264, 27733, 11, 291, 500, 380, 643, 281, 764, 264, 7912, 2635, 420, 264, 9905, 13], "temperature": 0.0, "avg_logprob": -0.24418729146321613, "compression_ratio": 1.671875, "no_speech_prob": 0.00019098589837085456}, {"id": 525, "seek": 360500, "start": 3615.0, "end": 3617.0, "text": " It's all in the spreadsheet.", "tokens": [467, 311, 439, 294, 264, 27733, 13], "temperature": 0.0, "avg_logprob": -0.24418729146321613, "compression_ratio": 1.671875, "no_speech_prob": 0.00019098589837085456}, {"id": 526, "seek": 360500, "start": 3617.0, "end": 3623.0, "text": " And the actual random macro is just automatically copying and pasting and calculating.", "tokens": [400, 264, 3539, 4974, 18887, 307, 445, 6772, 27976, 293, 1791, 278, 293, 28258, 13], "temperature": 0.0, "avg_logprob": -0.24418729146321613, "compression_ratio": 1.671875, "no_speech_prob": 0.00019098589837085456}, {"id": 527, "seek": 360500, "start": 3623.0, "end": 3625.0, "text": " So you can do it all that way.", "tokens": [407, 291, 393, 360, 309, 439, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.24418729146321613, "compression_ratio": 1.671875, "no_speech_prob": 0.00019098589837085456}, {"id": 528, "seek": 360500, "start": 3625.0, "end": 3627.0, "text": " Yeah, that's a great point.", "tokens": [865, 11, 300, 311, 257, 869, 935, 13], "temperature": 0.0, "avg_logprob": -0.24418729146321613, "compression_ratio": 1.671875, "no_speech_prob": 0.00019098589837085456}, {"id": 529, "seek": 360500, "start": 3627.0, "end": 3629.0, "text": " I'm going to reset this.", "tokens": [286, 478, 516, 281, 14322, 341, 13], "temperature": 0.0, "avg_logprob": -0.24418729146321613, "compression_ratio": 1.671875, "no_speech_prob": 0.00019098589837085456}, {"id": 530, "seek": 360500, "start": 3629.0, "end": 3630.0, "text": " You want me to reset this?", "tokens": [509, 528, 385, 281, 14322, 341, 30], "temperature": 0.0, "avg_logprob": -0.24418729146321613, "compression_ratio": 1.671875, "no_speech_prob": 0.00019098589837085456}, {"id": 531, "seek": 360500, "start": 3630.0, "end": 3632.0, "text": " What this is do?", "tokens": [708, 341, 307, 360, 30], "temperature": 0.0, "avg_logprob": -0.24418729146321613, "compression_ratio": 1.671875, "no_speech_prob": 0.00019098589837085456}, {"id": 532, "seek": 363200, "start": 3632.0, "end": 3635.0, "text": " Let me just run.", "tokens": [961, 385, 445, 1190, 13], "temperature": 0.0, "avg_logprob": -0.20014468554792733, "compression_ratio": 1.356164383561644, "no_speech_prob": 7.76634260546416e-06}, {"id": 533, "seek": 363200, "start": 3635.0, "end": 3646.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.20014468554792733, "compression_ratio": 1.356164383561644, "no_speech_prob": 7.76634260546416e-06}, {"id": 534, "seek": 363200, "start": 3646.0, "end": 3661.0, "text": " So as Jeremy was kind of saying, what this does is it takes the new A and B down from the very bottom and is what the macro does and then puts them, I guess, back up here for.", "tokens": [407, 382, 17809, 390, 733, 295, 1566, 11, 437, 341, 775, 307, 309, 2516, 264, 777, 316, 293, 363, 760, 490, 264, 588, 2767, 293, 307, 437, 264, 18887, 775, 293, 550, 8137, 552, 11, 286, 2041, 11, 646, 493, 510, 337, 13], "temperature": 0.0, "avg_logprob": -0.20014468554792733, "compression_ratio": 1.356164383561644, "no_speech_prob": 7.76634260546416e-06}, {"id": 535, "seek": 366100, "start": 3661.0, "end": 3665.0, "text": " For intercept and slope.", "tokens": [1171, 24700, 293, 13525, 13], "temperature": 0.0, "avg_logprob": -0.1420102763820339, "compression_ratio": 1.4764397905759161, "no_speech_prob": 2.9479242584784515e-06}, {"id": 536, "seek": 366100, "start": 3665.0, "end": 3678.0, "text": " Yeah, so you can see that's kind of picking off the column C.", "tokens": [865, 11, 370, 291, 393, 536, 300, 311, 733, 295, 8867, 766, 264, 7738, 383, 13], "temperature": 0.0, "avg_logprob": -0.1420102763820339, "compression_ratio": 1.4764397905759161, "no_speech_prob": 2.9479242584784515e-06}, {"id": 537, "seek": 366100, "start": 3678.0, "end": 3683.0, "text": " And so I would say stochastic gradient descent is a really, really general approach.", "tokens": [400, 370, 286, 576, 584, 342, 8997, 2750, 16235, 23475, 307, 257, 534, 11, 534, 2674, 3109, 13], "temperature": 0.0, "avg_logprob": -0.1420102763820339, "compression_ratio": 1.4764397905759161, "no_speech_prob": 2.9479242584784515e-06}, {"id": 538, "seek": 366100, "start": 3683.0, "end": 3686.0, "text": " It's something that you can apply to a lot of different problems.", "tokens": [467, 311, 746, 300, 291, 393, 3079, 281, 257, 688, 295, 819, 2740, 13], "temperature": 0.0, "avg_logprob": -0.1420102763820339, "compression_ratio": 1.4764397905759161, "no_speech_prob": 2.9479242584784515e-06}, {"id": 539, "seek": 366100, "start": 3686.0, "end": 3690.0, "text": " Anytime you have a derivative for your loss.", "tokens": [39401, 291, 362, 257, 13760, 337, 428, 4470, 13], "temperature": 0.0, "avg_logprob": -0.1420102763820339, "compression_ratio": 1.4764397905759161, "no_speech_prob": 2.9479242584784515e-06}, {"id": 540, "seek": 369000, "start": 3690.0, "end": 3693.0, "text": " And in fact, you actually don't even need to have a formula form for it.", "tokens": [400, 294, 1186, 11, 291, 767, 500, 380, 754, 643, 281, 362, 257, 8513, 1254, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.2018467277608892, "compression_ratio": 1.5360360360360361, "no_speech_prob": 7.811184445927211e-07}, {"id": 541, "seek": 369000, "start": 3693.0, "end": 3700.0, "text": " So I think it's something that's really useful, useful to know.", "tokens": [407, 286, 519, 309, 311, 746, 300, 311, 534, 4420, 11, 4420, 281, 458, 13], "temperature": 0.0, "avg_logprob": -0.2018467277608892, "compression_ratio": 1.5360360360360361, "no_speech_prob": 7.811184445927211e-07}, {"id": 542, "seek": 369000, "start": 3700.0, "end": 3711.0, "text": " And I've linked to a few few resources, including the SD like SDD lecture from Andrew Ingres Coursera class from the fast.", "tokens": [400, 286, 600, 9408, 281, 257, 1326, 1326, 3593, 11, 3009, 264, 14638, 411, 14638, 35, 7991, 490, 10110, 25731, 495, 383, 5067, 1663, 1508, 490, 264, 2370, 13], "temperature": 0.0, "avg_logprob": -0.2018467277608892, "compression_ratio": 1.5360360360360361, "no_speech_prob": 7.811184445927211e-07}, {"id": 543, "seek": 369000, "start": 3711.0, "end": 3712.0, "text": " I wiki.", "tokens": [286, 261, 9850, 13], "temperature": 0.0, "avg_logprob": -0.2018467277608892, "compression_ratio": 1.5360360360360361, "no_speech_prob": 7.811184445927211e-07}, {"id": 544, "seek": 369000, "start": 3712.0, "end": 3719.0, "text": " This is kind of a nice blog post on a variety of optimization algorithms.", "tokens": [639, 307, 733, 295, 257, 1481, 6968, 2183, 322, 257, 5673, 295, 19618, 14642, 13], "temperature": 0.0, "avg_logprob": -0.2018467277608892, "compression_ratio": 1.5360360360360361, "no_speech_prob": 7.811184445927211e-07}, {"id": 545, "seek": 371900, "start": 3719.0, "end": 3734.0, "text": " But so applying that to our specific problem of NMF, we're trying to decompose V into the product of two matrices where all their entries are non negative.", "tokens": [583, 370, 9275, 300, 281, 527, 2685, 1154, 295, 426, 44, 37, 11, 321, 434, 1382, 281, 22867, 541, 691, 666, 264, 1674, 295, 732, 32284, 689, 439, 641, 23041, 366, 2107, 3671, 13], "temperature": 0.0, "avg_logprob": -0.15178783316361277, "compression_ratio": 1.2601626016260163, "no_speech_prob": 1.952287675521802e-05}, {"id": 546, "seek": 373400, "start": 3734.0, "end": 3751.0, "text": " So we're minimizing the Frobenius norm of V minus W H. We want to get that as close to zero as possible, because that would be in the W H is a good approximation of V. And then we really want W and H greater than or equal to zero.", "tokens": [407, 321, 434, 46608, 264, 25028, 1799, 4872, 2026, 295, 691, 3175, 343, 389, 13, 492, 528, 281, 483, 300, 382, 1998, 281, 4018, 382, 1944, 11, 570, 300, 576, 312, 294, 264, 343, 389, 307, 257, 665, 28023, 295, 691, 13, 400, 550, 321, 534, 528, 343, 293, 389, 5044, 813, 420, 2681, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.14146825133777055, "compression_ratio": 1.4110429447852761, "no_speech_prob": 7.645628102181945e-06}, {"id": 547, "seek": 375100, "start": 3751.0, "end": 3771.0, "text": " So in order to use SDD, we need to know the gradient of the loss function. And here we've looked that up and are just going to kind of use it. This is also something you could calculate if you like multivariable derivatives, multivariable calculus.", "tokens": [407, 294, 1668, 281, 764, 14638, 35, 11, 321, 643, 281, 458, 264, 16235, 295, 264, 4470, 2445, 13, 400, 510, 321, 600, 2956, 300, 493, 293, 366, 445, 516, 281, 733, 295, 764, 309, 13, 639, 307, 611, 746, 291, 727, 8873, 498, 291, 411, 2120, 592, 3504, 712, 33733, 11, 2120, 592, 3504, 712, 33400, 13], "temperature": 0.0, "avg_logprob": -0.08067556350461898, "compression_ratio": 1.467455621301775, "no_speech_prob": 4.637486199499108e-06}, {"id": 548, "seek": 377100, "start": 3771.0, "end": 3788.0, "text": " So we've got. Kind of our vectors from before. This is still again the same matrix with the words as rows and the columns or documents.", "tokens": [407, 321, 600, 658, 13, 9242, 295, 527, 18875, 490, 949, 13, 639, 307, 920, 797, 264, 912, 8141, 365, 264, 2283, 382, 13241, 293, 264, 13766, 420, 8512, 13], "temperature": 0.0, "avg_logprob": -0.13730762986575856, "compression_ratio": 1.2272727272727273, "no_speech_prob": 3.23759877574048e-06}, {"id": 549, "seek": 378800, "start": 3788.0, "end": 3803.0, "text": " Down here we define the gradients. It takes in M W and H and it returns the gradient.", "tokens": [9506, 510, 321, 6964, 264, 2771, 2448, 13, 467, 2516, 294, 376, 343, 293, 389, 293, 309, 11247, 264, 16235, 13], "temperature": 0.0, "avg_logprob": -0.1683012580871582, "compression_ratio": 1.1333333333333333, "no_speech_prob": 8.529787010047585e-06}, {"id": 550, "seek": 380300, "start": 3803.0, "end": 3818.0, "text": " We have the this is the gradient from W. And then we also have this penalty term and the penalty is going to be and this is for W or kind of for each of our matrices.", "tokens": [492, 362, 264, 341, 307, 264, 16235, 490, 343, 13, 400, 550, 321, 611, 362, 341, 16263, 1433, 293, 264, 16263, 307, 516, 281, 312, 293, 341, 307, 337, 343, 420, 733, 295, 337, 1184, 295, 527, 32284, 13], "temperature": 0.0, "avg_logprob": -0.1281340843023256, "compression_ratio": 1.4434782608695653, "no_speech_prob": 1.154412984760711e-06}, {"id": 551, "seek": 381800, "start": 3818.0, "end": 3833.0, "text": " So we want to have a penalty when W and H are negative. And that's what's happening here. So we call penalty, which has got this NP dot where if the matrix is greater than we're going to choose you is very close to zero.", "tokens": [407, 321, 528, 281, 362, 257, 16263, 562, 343, 293, 389, 366, 3671, 13, 400, 300, 311, 437, 311, 2737, 510, 13, 407, 321, 818, 16263, 11, 597, 575, 658, 341, 38611, 5893, 689, 498, 264, 8141, 307, 5044, 813, 321, 434, 516, 281, 2826, 291, 307, 588, 1998, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.32300758361816406, "compression_ratio": 1.3924050632911393, "no_speech_prob": 9.514882549410686e-06}, {"id": 552, "seek": 383300, "start": 3833.0, "end": 3848.0, "text": " If the matrix is greater than you, there's zero penalty. We're happy because it's positive. However, if it's negative or super close to zero, we're assigning this penalty.", "tokens": [759, 264, 8141, 307, 5044, 813, 291, 11, 456, 311, 4018, 16263, 13, 492, 434, 2055, 570, 309, 311, 3353, 13, 2908, 11, 498, 309, 311, 3671, 420, 1687, 1998, 281, 4018, 11, 321, 434, 49602, 341, 16263, 13], "temperature": 0.0, "avg_logprob": -0.10659444054891896, "compression_ratio": 1.3255813953488371, "no_speech_prob": 3.784851514865295e-06}, {"id": 553, "seek": 384800, "start": 3848.0, "end": 3864.0, "text": " And then we update W and H by calculating their gradients, which includes the penalty. And then just just like what happened in the kind of simple version with the line in the notebook, we do the learning rate, which is our step size times the gradient.", "tokens": [400, 550, 321, 5623, 343, 293, 389, 538, 28258, 641, 2771, 2448, 11, 597, 5974, 264, 16263, 13, 400, 550, 445, 445, 411, 437, 2011, 294, 264, 733, 295, 2199, 3037, 365, 264, 1622, 294, 264, 21060, 11, 321, 360, 264, 2539, 3314, 11, 597, 307, 527, 1823, 2744, 1413, 264, 16235, 13], "temperature": 0.0, "avg_logprob": -0.0860720433686909, "compression_ratio": 1.5333333333333334, "no_speech_prob": 5.7147997267748e-07}, {"id": 554, "seek": 386400, "start": 3864.0, "end": 3879.0, "text": " And then just just like what happened in the kind of simple version with the line in the notebook, we do the learning rate, which is our step size times the gradient.", "tokens": [400, 550, 445, 445, 411, 437, 2011, 294, 264, 733, 295, 2199, 3037, 365, 264, 1622, 294, 264, 21060, 11, 321, 360, 264, 2539, 3314, 11, 597, 307, 527, 1823, 2744, 1413, 264, 16235, 13], "temperature": 0.0, "avg_logprob": -0.07971942119109325, "compression_ratio": 1.3833333333333333, "no_speech_prob": 8.186110562746762e-07}, {"id": 555, "seek": 387900, "start": 3879.0, "end": 3896.0, "text": " This is just so that we can get a sense of how we're doing as we go along. So the first term is showing M minus W times H. So just kind of seeing how far apart they are, you know, is W H a good approximation of M.", "tokens": [639, 307, 445, 370, 300, 321, 393, 483, 257, 2020, 295, 577, 321, 434, 884, 382, 321, 352, 2051, 13, 407, 264, 700, 1433, 307, 4099, 376, 3175, 343, 1413, 389, 13, 407, 445, 733, 295, 2577, 577, 1400, 4936, 436, 366, 11, 291, 458, 11, 307, 343, 389, 257, 665, 28023, 295, 376, 13], "temperature": 0.0, "avg_logprob": -0.10068398830937404, "compression_ratio": 1.610878661087866, "no_speech_prob": 1.6700818378012627e-05}, {"id": 556, "seek": 387900, "start": 3896.0, "end": 3904.0, "text": " Then we're going to look at the minimum of W and the minimum of H. That's something where I don't know if we got like negative a thousand, that would be a really bad sign.", "tokens": [1396, 321, 434, 516, 281, 574, 412, 264, 7285, 295, 343, 293, 264, 7285, 295, 389, 13, 663, 311, 746, 689, 286, 500, 380, 458, 498, 321, 658, 411, 3671, 257, 4714, 11, 300, 576, 312, 257, 534, 1578, 1465, 13], "temperature": 0.0, "avg_logprob": -0.10068398830937404, "compression_ratio": 1.610878661087866, "no_speech_prob": 1.6700818378012627e-05}, {"id": 557, "seek": 390400, "start": 3904.0, "end": 3928.0, "text": " Ideally, those would be zero. And then also the sum of how many of the terms are less than zero. Any questions so far about these methods we're defining?", "tokens": [40817, 11, 729, 576, 312, 4018, 13, 400, 550, 611, 264, 2408, 295, 577, 867, 295, 264, 2115, 366, 1570, 813, 4018, 13, 2639, 1651, 370, 1400, 466, 613, 7150, 321, 434, 17827, 30], "temperature": 0.0, "avg_logprob": -0.08837907565267462, "compression_ratio": 1.2966101694915255, "no_speech_prob": 8.397511919611134e-06}, {"id": 558, "seek": 392800, "start": 3928.0, "end": 3940.0, "text": " Sure. Yeah. So the because we kind of have two separate things we're trying to do. One, we want W times H to be really close to M, a good approximation.", "tokens": [4894, 13, 865, 13, 407, 264, 570, 321, 733, 295, 362, 732, 4994, 721, 321, 434, 1382, 281, 360, 13, 1485, 11, 321, 528, 343, 1413, 389, 281, 312, 534, 1998, 281, 376, 11, 257, 665, 28023, 13], "temperature": 0.0, "avg_logprob": -0.11995600650185033, "compression_ratio": 1.72, "no_speech_prob": 7.645822734048124e-06}, {"id": 559, "seek": 392800, "start": 3940.0, "end": 3957.0, "text": " And the second is we want W and H to be positive. So we have these two separate goals and taking the derivative of the loss will help us get W times H close to M because that's kind of the particular derivative we've we've calculated.", "tokens": [400, 264, 1150, 307, 321, 528, 343, 293, 389, 281, 312, 3353, 13, 407, 321, 362, 613, 732, 4994, 5493, 293, 1940, 264, 13760, 295, 264, 4470, 486, 854, 505, 483, 343, 1413, 389, 1998, 281, 376, 570, 300, 311, 733, 295, 264, 1729, 13760, 321, 600, 321, 600, 15598, 13], "temperature": 0.0, "avg_logprob": -0.11995600650185033, "compression_ratio": 1.72, "no_speech_prob": 7.645822734048124e-06}, {"id": 560, "seek": 395700, "start": 3957.0, "end": 3968.0, "text": " And so that's what's going on with this kind of this first part. So that's that for W. This is that for H. Then to deal with wanting W and H to be greater than zero.", "tokens": [400, 370, 300, 311, 437, 311, 516, 322, 365, 341, 733, 295, 341, 700, 644, 13, 407, 300, 311, 300, 337, 343, 13, 639, 307, 300, 337, 389, 13, 1396, 281, 2028, 365, 7935, 343, 293, 389, 281, 312, 5044, 813, 4018, 13], "temperature": 0.0, "avg_logprob": -0.08175247686880606, "compression_ratio": 1.601063829787234, "no_speech_prob": 5.771710675617214e-06}, {"id": 561, "seek": 395700, "start": 3968.0, "end": 3977.0, "text": " Here we're deciding, OK, we're going to have this penalty that is equal to zero if they're positive because we're happy. So no penalty.", "tokens": [1692, 321, 434, 17990, 11, 2264, 11, 321, 434, 516, 281, 362, 341, 16263, 300, 307, 2681, 281, 4018, 498, 436, 434, 3353, 570, 321, 434, 2055, 13, 407, 572, 16263, 13], "temperature": 0.0, "avg_logprob": -0.08175247686880606, "compression_ratio": 1.601063829787234, "no_speech_prob": 5.771710675617214e-06}, {"id": 562, "seek": 397700, "start": 3977.0, "end": 3988.0, "text": " And that's what NP dot where does. NP dot where kind of takes a truth, true or false statement. If it's true, it uses the first value.", "tokens": [400, 300, 311, 437, 38611, 5893, 689, 775, 13, 38611, 5893, 689, 733, 295, 2516, 257, 3494, 11, 2074, 420, 7908, 5629, 13, 759, 309, 311, 2074, 11, 309, 4960, 264, 700, 2158, 13], "temperature": 0.0, "avg_logprob": -0.11918887286119058, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.2878531379101332e-06}, {"id": 563, "seek": 397700, "start": 3988.0, "end": 3998.0, "text": " So if W is greater than or equal to this tiny number that's close to zero, we go with the first value zero. There's no penalty.", "tokens": [407, 498, 343, 307, 5044, 813, 420, 2681, 281, 341, 5870, 1230, 300, 311, 1998, 281, 4018, 11, 321, 352, 365, 264, 700, 2158, 4018, 13, 821, 311, 572, 16263, 13], "temperature": 0.0, "avg_logprob": -0.11918887286119058, "compression_ratio": 1.5411764705882354, "no_speech_prob": 1.2878531379101332e-06}, {"id": 564, "seek": 399800, "start": 3998.0, "end": 4014.0, "text": " However, if W is less than you, the penalty is going to be larger for the further away that M is from you.", "tokens": [2908, 11, 498, 343, 307, 1570, 813, 291, 11, 264, 16263, 307, 516, 281, 312, 4833, 337, 264, 3052, 1314, 300, 376, 307, 490, 291, 13], "temperature": 0.0, "avg_logprob": -0.12244528929392497, "compression_ratio": 1.1521739130434783, "no_speech_prob": 1.8328753867535852e-05}, {"id": 565, "seek": 401400, "start": 4014.0, "end": 4029.0, "text": " So basically SGD can't do constrained optimization, but we have constraints. So we're taking it. People who spend their lives studying constrained optimization and can horrify it.", "tokens": [407, 1936, 34520, 35, 393, 380, 360, 38901, 19618, 11, 457, 321, 362, 18491, 13, 407, 321, 434, 1940, 309, 13, 3432, 567, 3496, 641, 2909, 7601, 38901, 19618, 293, 393, 17582, 2505, 309, 13], "temperature": 0.0, "avg_logprob": -0.36087637681227464, "compression_ratio": 1.45, "no_speech_prob": 8.936565791373141e-06}, {"id": 566, "seek": 401400, "start": 4029.0, "end": 4036.0, "text": " As Rachel shows, it actually has some nice features.", "tokens": [1018, 14246, 3110, 11, 309, 767, 575, 512, 1481, 4122, 13], "temperature": 0.0, "avg_logprob": -0.36087637681227464, "compression_ratio": 1.45, "no_speech_prob": 8.936565791373141e-06}, {"id": 567, "seek": 403600, "start": 4036.0, "end": 4047.0, "text": " And you can think about like another way to think about it is there's not a clear or obvious way to take a derivative from OK, we want these things to be greater than zero.", "tokens": [400, 291, 393, 519, 466, 411, 1071, 636, 281, 519, 466, 309, 307, 456, 311, 406, 257, 1850, 420, 6322, 636, 281, 747, 257, 13760, 490, 2264, 11, 321, 528, 613, 721, 281, 312, 5044, 813, 4018, 13], "temperature": 0.0, "avg_logprob": -0.08940456991326319, "compression_ratio": 1.5494505494505495, "no_speech_prob": 3.3929914025065955e-06}, {"id": 568, "seek": 403600, "start": 4047.0, "end": 4056.0, "text": " Whereas for having this M close to W times H, it's much easier to be like, OK, this is how far away they are.", "tokens": [13813, 337, 1419, 341, 376, 1998, 281, 343, 1413, 389, 11, 309, 311, 709, 3571, 281, 312, 411, 11, 2264, 11, 341, 307, 577, 1400, 1314, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.08940456991326319, "compression_ratio": 1.5494505494505495, "no_speech_prob": 3.3929914025065955e-06}, {"id": 569, "seek": 405600, "start": 4056.0, "end": 4083.0, "text": " We can take a derivative of that.", "tokens": [492, 393, 747, 257, 13760, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.152545432249705, "compression_ratio": 0.8048780487804879, "no_speech_prob": 9.971885447157547e-06}, {"id": 570, "seek": 408300, "start": 4083.0, "end": 4094.0, "text": " That's a good question. So here this will be negative. What we're doing with our W and H is.", "tokens": [663, 311, 257, 665, 1168, 13, 407, 510, 341, 486, 312, 3671, 13, 708, 321, 434, 884, 365, 527, 343, 293, 389, 307, 13], "temperature": 0.0, "avg_logprob": -0.11434676729399583, "compression_ratio": 1.3943661971830985, "no_speech_prob": 1.5205633644654881e-05}, {"id": 571, "seek": 408300, "start": 4094.0, "end": 4100.0, "text": " We're doing W minus equals DW. So that would come back as.", "tokens": [492, 434, 884, 343, 3175, 6915, 45318, 13, 407, 300, 576, 808, 646, 382, 13], "temperature": 0.0, "avg_logprob": -0.11434676729399583, "compression_ratio": 1.3943661971830985, "no_speech_prob": 1.5205633644654881e-05}, {"id": 572, "seek": 408300, "start": 4100.0, "end": 4104.0, "text": " So this is being returned in the grads method.", "tokens": [407, 341, 307, 885, 8752, 294, 264, 2771, 82, 3170, 13], "temperature": 0.0, "avg_logprob": -0.11434676729399583, "compression_ratio": 1.3943661971830985, "no_speech_prob": 1.5205633644654881e-05}, {"id": 573, "seek": 410400, "start": 4104.0, "end": 4116.0, "text": " We have like minus a negative is positive. But that's a good point.", "tokens": [492, 362, 411, 3175, 257, 3671, 307, 3353, 13, 583, 300, 311, 257, 665, 935, 13], "temperature": 0.0, "avg_logprob": -0.0801451454559962, "compression_ratio": 1.450381679389313, "no_speech_prob": 7.071547315717908e-06}, {"id": 574, "seek": 410400, "start": 4116.0, "end": 4125.0, "text": " The issue is we could flip the signs on everything, but then you would also have to flip the signs on the derivative here.", "tokens": [440, 2734, 307, 321, 727, 7929, 264, 7880, 322, 1203, 11, 457, 550, 291, 576, 611, 362, 281, 7929, 264, 7880, 322, 264, 13760, 510, 13], "temperature": 0.0, "avg_logprob": -0.0801451454559962, "compression_ratio": 1.450381679389313, "no_speech_prob": 7.071547315717908e-06}, {"id": 575, "seek": 412500, "start": 4125.0, "end": 4149.0, "text": " Yeah, but the. Minus a negative, it basically means the further the more negative it is, the bigger a positive number you add to W.", "tokens": [865, 11, 457, 264, 13, 2829, 301, 257, 3671, 11, 309, 1936, 1355, 264, 3052, 264, 544, 3671, 309, 307, 11, 264, 3801, 257, 3353, 1230, 291, 909, 281, 343, 13], "temperature": 0.0, "avg_logprob": -0.16149617603846958, "compression_ratio": 1.2596153846153846, "no_speech_prob": 1.7602197885935311e-06}, {"id": 576, "seek": 414900, "start": 4149.0, "end": 4159.0, "text": " OK, these are great questions. Thanks everyone. Yeah, so now to kind of start it, we need to choose random values for W and H.", "tokens": [2264, 11, 613, 366, 869, 1651, 13, 2561, 1518, 13, 865, 11, 370, 586, 281, 733, 295, 722, 309, 11, 321, 643, 281, 2826, 4974, 4190, 337, 343, 293, 389, 13], "temperature": 0.0, "avg_logprob": -0.07425850835339777, "compression_ratio": 1.360759493670886, "no_speech_prob": 3.187293486917042e-06}, {"id": 577, "seek": 414900, "start": 4159.0, "end": 4165.0, "text": " So we'll just use kind of a random normal to get back matrices of the size that we want.", "tokens": [407, 321, 603, 445, 764, 733, 295, 257, 4974, 2710, 281, 483, 646, 32284, 295, 264, 2744, 300, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.07425850835339777, "compression_ratio": 1.360759493670886, "no_speech_prob": 3.187293486917042e-06}, {"id": 578, "seek": 416500, "start": 4165.0, "end": 4179.0, "text": " And then we're taking the absolute value of that because we should at least start with non zero or non negative matrices.", "tokens": [400, 550, 321, 434, 1940, 264, 8236, 2158, 295, 300, 570, 321, 820, 412, 1935, 722, 365, 2107, 4018, 420, 2107, 3671, 32284, 13], "temperature": 0.0, "avg_logprob": -0.07084518746484685, "compression_ratio": 1.625, "no_speech_prob": 7.811195246176794e-07}, {"id": 579, "seek": 416500, "start": 4179.0, "end": 4185.0, "text": " And so we'll just kind of check the initial. And as a reminder,", "tokens": [400, 370, 321, 603, 445, 733, 295, 1520, 264, 5883, 13, 400, 382, 257, 13548, 11], "temperature": 0.0, "avg_logprob": -0.07084518746484685, "compression_ratio": 1.625, "no_speech_prob": 7.811195246176794e-07}, {"id": 580, "seek": 416500, "start": 4185.0, "end": 4192.0, "text": " this report and this is just a few values that we thought it would be interesting to kind of monitor as we go along to see how we're doing.", "tokens": [341, 2275, 293, 341, 307, 445, 257, 1326, 4190, 300, 321, 1194, 309, 576, 312, 1880, 281, 733, 295, 6002, 382, 321, 352, 2051, 281, 536, 577, 321, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.07084518746484685, "compression_ratio": 1.625, "no_speech_prob": 7.811195246176794e-07}, {"id": 581, "seek": 419200, "start": 4192.0, "end": 4197.0, "text": " So this is how far away M and WHR is the first one.", "tokens": [407, 341, 307, 577, 1400, 1314, 376, 293, 8183, 49, 307, 264, 700, 472, 13], "temperature": 0.0, "avg_logprob": -0.11302819103002548, "compression_ratio": 1.5895522388059702, "no_speech_prob": 1.9220880858483724e-05}, {"id": 582, "seek": 419200, "start": 4197.0, "end": 4206.0, "text": " The second is the minimum of W, the minimum of H, and then the sum of their negative terms.", "tokens": [440, 1150, 307, 264, 7285, 295, 343, 11, 264, 7285, 295, 389, 11, 293, 550, 264, 2408, 295, 641, 3671, 2115, 13], "temperature": 0.0, "avg_logprob": -0.11302819103002548, "compression_ratio": 1.5895522388059702, "no_speech_prob": 1.9220880858483724e-05}, {"id": 583, "seek": 419200, "start": 4206.0, "end": 4214.0, "text": " Oh, the count of notes. Oh, OK, great. Count of their negative terms.", "tokens": [876, 11, 264, 1207, 295, 5570, 13, 876, 11, 2264, 11, 869, 13, 5247, 295, 641, 3671, 2115, 13], "temperature": 0.0, "avg_logprob": -0.11302819103002548, "compression_ratio": 1.5895522388059702, "no_speech_prob": 1.9220880858483724e-05}, {"id": 584, "seek": 421400, "start": 4214.0, "end": 4223.0, "text": " So then we run update, which just calculates the gradients, updates them once.", "tokens": [407, 550, 321, 1190, 5623, 11, 597, 445, 4322, 1024, 264, 2771, 2448, 11, 9205, 552, 1564, 13], "temperature": 0.0, "avg_logprob": -0.10784129773156118, "compression_ratio": 1.3398692810457515, "no_speech_prob": 8.139177225530148e-06}, {"id": 585, "seek": 421400, "start": 4223.0, "end": 4230.0, "text": " And we can check the the error has improved a little bit.", "tokens": [400, 321, 393, 1520, 264, 264, 6713, 575, 9689, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.10784129773156118, "compression_ratio": 1.3398692810457515, "no_speech_prob": 8.139177225530148e-06}, {"id": 586, "seek": 421400, "start": 4230.0, "end": 4238.0, "text": " So this is slightly smaller, 44.419 now as opposed to 44.439 before.", "tokens": [407, 341, 307, 4748, 4356, 11, 16408, 13, 19, 3405, 586, 382, 8851, 281, 16408, 13, 19, 12493, 949, 13], "temperature": 0.0, "avg_logprob": -0.10784129773156118, "compression_ratio": 1.3398692810457515, "no_speech_prob": 8.139177225530148e-06}, {"id": 587, "seek": 423800, "start": 4238.0, "end": 4254.0, "text": " Here we have introduced some negatives into W and H. And if we run this, and I'm actually going to do this for fewer.", "tokens": [1692, 321, 362, 7268, 512, 40019, 666, 343, 293, 389, 13, 400, 498, 321, 1190, 341, 11, 293, 286, 478, 767, 516, 281, 360, 341, 337, 13366, 13], "temperature": 0.0, "avg_logprob": -0.12094800612505745, "compression_ratio": 1.3014705882352942, "no_speech_prob": 2.4060434498096583e-06}, {"id": 588, "seek": 423800, "start": 4254.0, "end": 4262.0, "text": " So one, as this runs, you'll notice it's a little bit slow.", "tokens": [407, 472, 11, 382, 341, 6676, 11, 291, 603, 3449, 309, 311, 257, 707, 857, 2964, 13], "temperature": 0.0, "avg_logprob": -0.12094800612505745, "compression_ratio": 1.3014705882352942, "no_speech_prob": 2.4060434498096583e-06}, {"id": 589, "seek": 426200, "start": 4262.0, "end": 4270.0, "text": " So we see that the product WH is getting closer to our matrix M. That's the first column is going down here.", "tokens": [407, 321, 536, 300, 264, 1674, 8183, 307, 1242, 4966, 281, 527, 8141, 376, 13, 663, 311, 264, 700, 7738, 307, 516, 760, 510, 13], "temperature": 0.0, "avg_logprob": -0.12706302461170016, "compression_ratio": 1.4425287356321839, "no_speech_prob": 5.093582785775652e-06}, {"id": 590, "seek": 426200, "start": 4270.0, "end": 4277.0, "text": " The negative terms actually kind of seem like they're saying about the same.", "tokens": [440, 3671, 2115, 767, 733, 295, 1643, 411, 436, 434, 1566, 466, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.12706302461170016, "compression_ratio": 1.4425287356321839, "no_speech_prob": 5.093582785775652e-06}, {"id": 591, "seek": 426200, "start": 4277.0, "end": 4284.0, "text": " I didn't do that many iterations. Anything the count is going up.", "tokens": [286, 994, 380, 360, 300, 867, 36540, 13, 11998, 264, 1207, 307, 516, 493, 13], "temperature": 0.0, "avg_logprob": -0.12706302461170016, "compression_ratio": 1.4425287356321839, "no_speech_prob": 5.093582785775652e-06}, {"id": 592, "seek": 428400, "start": 4284.0, "end": 4295.0, "text": " So kind of a key thing here to note is that this is really slow and I ran this for longer and it continued to be very slow.", "tokens": [407, 733, 295, 257, 2141, 551, 510, 281, 3637, 307, 300, 341, 307, 534, 2964, 293, 286, 5872, 341, 337, 2854, 293, 309, 7014, 281, 312, 588, 2964, 13], "temperature": 0.0, "avg_logprob": -0.11576891506419462, "compression_ratio": 1.5826086956521739, "no_speech_prob": 4.860291028307984e-06}, {"id": 593, "seek": 428400, "start": 4295.0, "end": 4310.0, "text": " And there's also a lot of parameter fiddling, because like right now, looking at this, I'm thinking I would probably want to increase the penalty on negative values just because these counts seem really high and like they're not going down.", "tokens": [400, 456, 311, 611, 257, 688, 295, 13075, 283, 14273, 1688, 11, 570, 411, 558, 586, 11, 1237, 412, 341, 11, 286, 478, 1953, 286, 576, 1391, 528, 281, 3488, 264, 16263, 322, 3671, 4190, 445, 570, 613, 14893, 1643, 534, 1090, 293, 411, 436, 434, 406, 516, 760, 13], "temperature": 0.0, "avg_logprob": -0.11576891506419462, "compression_ratio": 1.5826086956521739, "no_speech_prob": 4.860291028307984e-06}, {"id": 594, "seek": 431000, "start": 4310.0, "end": 4317.0, "text": " Particularly, well, I guess H is much bigger than W. So we have like a lot of different parameters.", "tokens": [32281, 11, 731, 11, 286, 2041, 389, 307, 709, 3801, 813, 343, 13, 407, 321, 362, 411, 257, 688, 295, 819, 9834, 13], "temperature": 0.0, "avg_logprob": -0.1317012350438005, "compression_ratio": 1.4260355029585798, "no_speech_prob": 3.446456958045019e-06}, {"id": 595, "seek": 431000, "start": 4317.0, "end": 4329.0, "text": " So even though we have this generic method that mostly seems headed in the right direction, it does have some shortcomings.", "tokens": [407, 754, 1673, 321, 362, 341, 19577, 3170, 300, 5240, 2544, 12798, 294, 264, 558, 3513, 11, 309, 775, 362, 512, 2099, 49886, 13], "temperature": 0.0, "avg_logprob": -0.1317012350438005, "compression_ratio": 1.4260355029585798, "no_speech_prob": 3.446456958045019e-06}, {"id": 596, "seek": 431000, "start": 4329.0, "end": 4333.0, "text": " Sorry about that.", "tokens": [4919, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.1317012350438005, "compression_ratio": 1.4260355029585798, "no_speech_prob": 3.446456958045019e-06}, {"id": 597, "seek": 433300, "start": 4333.0, "end": 4347.0, "text": " All right. So one way we could speed this up is to use PyTorch. PyTorch is a Python framework for dynamic neural networks with GPU acceleration.", "tokens": [1057, 558, 13, 407, 472, 636, 321, 727, 3073, 341, 493, 307, 281, 764, 9953, 51, 284, 339, 13, 9953, 51, 284, 339, 307, 257, 15329, 8388, 337, 8546, 18161, 9590, 365, 18407, 17162, 13], "temperature": 0.0, "avg_logprob": -0.10408551163143581, "compression_ratio": 1.4603960396039604, "no_speech_prob": 1.1725559261321905e-06}, {"id": 598, "seek": 433300, "start": 4347.0, "end": 4358.0, "text": " It was just released in January and many of the core contributors are on Facebook's AI team and it's used at Facebook as well as many other companies.", "tokens": [467, 390, 445, 4736, 294, 7061, 293, 867, 295, 264, 4965, 45627, 366, 322, 4384, 311, 7318, 1469, 293, 309, 311, 1143, 412, 4384, 382, 731, 382, 867, 661, 3431, 13], "temperature": 0.0, "avg_logprob": -0.10408551163143581, "compression_ratio": 1.4603960396039604, "no_speech_prob": 1.1725559261321905e-06}, {"id": 599, "seek": 435800, "start": 4358.0, "end": 4368.0, "text": " Twitter is using it. But it actually has two purposes. So kind of in addition to being a deep learn and I think it's a really excellent deep learning framework.", "tokens": [5794, 307, 1228, 309, 13, 583, 309, 767, 575, 732, 9932, 13, 407, 733, 295, 294, 4500, 281, 885, 257, 2452, 1466, 293, 286, 519, 309, 311, 257, 534, 7103, 2452, 2539, 8388, 13], "temperature": 0.0, "avg_logprob": -0.09590138129468234, "compression_ratio": 1.595330739299611, "no_speech_prob": 2.1906312213104684e-06}, {"id": 600, "seek": 435800, "start": 4368.0, "end": 4374.0, "text": " It's also a replacement for NumPy that uses the GPU.", "tokens": [467, 311, 611, 257, 14419, 337, 22592, 47, 88, 300, 4960, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.09590138129468234, "compression_ratio": 1.595330739299611, "no_speech_prob": 2.1906312213104684e-06}, {"id": 601, "seek": 435800, "start": 4374.0, "end": 4382.0, "text": " And a lot of the right now it has less functionality than NumPy just because it's so new, but a lot of people are actively working on it.", "tokens": [400, 257, 688, 295, 264, 558, 586, 309, 575, 1570, 14980, 813, 22592, 47, 88, 445, 570, 309, 311, 370, 777, 11, 457, 257, 688, 295, 561, 366, 13022, 1364, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.09590138129468234, "compression_ratio": 1.595330739299611, "no_speech_prob": 2.1906312213104684e-06}, {"id": 602, "seek": 435800, "start": 4382.0, "end": 4386.0, "text": " And the methods it has are very similar to what NumPy has.", "tokens": [400, 264, 7150, 309, 575, 366, 588, 2531, 281, 437, 22592, 47, 88, 575, 13], "temperature": 0.0, "avg_logprob": -0.09590138129468234, "compression_ratio": 1.595330739299611, "no_speech_prob": 2.1906312213104684e-06}, {"id": 603, "seek": 438600, "start": 4386.0, "end": 4393.0, "text": " So I have a link. So we'll kind of just be using it in this lesson.", "tokens": [407, 286, 362, 257, 2113, 13, 407, 321, 603, 733, 295, 445, 312, 1228, 309, 294, 341, 6898, 13], "temperature": 0.0, "avg_logprob": -0.10659334161779382, "compression_ratio": 1.5829383886255923, "no_speech_prob": 5.25502991877147e-06}, {"id": 604, "seek": 438600, "start": 4393.0, "end": 4399.0, "text": " But it's something I really wanted to expose you to. And it's something that if you want you can use for your project.", "tokens": [583, 309, 311, 746, 286, 534, 1415, 281, 19219, 291, 281, 13, 400, 309, 311, 746, 300, 498, 291, 528, 291, 393, 764, 337, 428, 1716, 13], "temperature": 0.0, "avg_logprob": -0.10659334161779382, "compression_ratio": 1.5829383886255923, "no_speech_prob": 5.25502991877147e-06}, {"id": 605, "seek": 438600, "start": 4399.0, "end": 4405.0, "text": " You could try to implement something else in PyTorch because I think it's a really, really nice framework.", "tokens": [509, 727, 853, 281, 4445, 746, 1646, 294, 9953, 51, 284, 339, 570, 286, 519, 309, 311, 257, 534, 11, 534, 1481, 8388, 13], "temperature": 0.0, "avg_logprob": -0.10659334161779382, "compression_ratio": 1.5829383886255923, "no_speech_prob": 5.25502991877147e-06}, {"id": 606, "seek": 438600, "start": 4405.0, "end": 4410.0, "text": " And I have a few links to find out more.", "tokens": [400, 286, 362, 257, 1326, 6123, 281, 915, 484, 544, 13], "temperature": 0.0, "avg_logprob": -0.10659334161779382, "compression_ratio": 1.5829383886255923, "no_speech_prob": 5.25502991877147e-06}, {"id": 607, "seek": 441000, "start": 4410.0, "end": 4418.0, "text": " So you do not have to be using a GPU for this course. That's fine. You will not get the speed up of running stuff on a GPU.", "tokens": [407, 291, 360, 406, 362, 281, 312, 1228, 257, 18407, 337, 341, 1164, 13, 663, 311, 2489, 13, 509, 486, 406, 483, 264, 3073, 493, 295, 2614, 1507, 322, 257, 18407, 13], "temperature": 0.0, "avg_logprob": -0.11807105790323286, "compression_ratio": 1.43125, "no_speech_prob": 5.2232659072615206e-05}, {"id": 608, "seek": 441000, "start": 4418.0, "end": 4429.0, "text": " Just in the code below here, whenever you see a.cuda, you'll need to delete that if you don't have a GPU.", "tokens": [1449, 294, 264, 3089, 2507, 510, 11, 5699, 291, 536, 257, 2411, 66, 11152, 11, 291, 603, 643, 281, 12097, 300, 498, 291, 500, 380, 362, 257, 18407, 13], "temperature": 0.0, "avg_logprob": -0.11807105790323286, "compression_ratio": 1.43125, "no_speech_prob": 5.2232659072615206e-05}, {"id": 609, "seek": 442900, "start": 4429.0, "end": 4441.0, "text": " Oh, and so if you're interested in getting a GPU, well, one, so the MSAN program has a box with a few GPUs that people can share.", "tokens": [876, 11, 293, 370, 498, 291, 434, 3102, 294, 1242, 257, 18407, 11, 731, 11, 472, 11, 370, 264, 7395, 1770, 1461, 575, 257, 2424, 365, 257, 1326, 18407, 82, 300, 561, 393, 2073, 13], "temperature": 0.0, "avg_logprob": -0.11650138948021865, "compression_ratio": 1.4583333333333333, "no_speech_prob": 2.5858720618998632e-05}, {"id": 610, "seek": 442900, "start": 4441.0, "end": 4448.0, "text": " And then something else that's great is AWS instances. And you can watch this setup lesson.", "tokens": [400, 550, 746, 1646, 300, 311, 869, 307, 17650, 14519, 13, 400, 291, 393, 1159, 341, 8657, 6898, 13], "temperature": 0.0, "avg_logprob": -0.11650138948021865, "compression_ratio": 1.4583333333333333, "no_speech_prob": 2.5858720618998632e-05}, {"id": 611, "seek": 442900, "start": 4448.0, "end": 4455.0, "text": " This was from the deep learning course that kind of walks through how to request P2 from AWS.", "tokens": [639, 390, 490, 264, 2452, 2539, 1164, 300, 733, 295, 12896, 807, 577, 281, 5308, 430, 17, 490, 17650, 13], "temperature": 0.0, "avg_logprob": -0.11650138948021865, "compression_ratio": 1.4583333333333333, "no_speech_prob": 2.5858720618998632e-05}, {"id": 612, "seek": 445500, "start": 4455.0, "end": 4465.0, "text": " And what that does is it's letting you spin up a kind of computer in the cloud that has GPU capabilities.", "tokens": [400, 437, 300, 775, 307, 309, 311, 8295, 291, 6060, 493, 257, 733, 295, 3820, 294, 264, 4588, 300, 575, 18407, 10862, 13], "temperature": 0.0, "avg_logprob": -0.08289886183208889, "compression_ratio": 1.5482233502538072, "no_speech_prob": 8.013017577468418e-06}, {"id": 613, "seek": 445500, "start": 4465.0, "end": 4474.0, "text": " Yeah, and definitely feel free to ask if you want kind of more help or advice about that.", "tokens": [865, 11, 293, 2138, 841, 1737, 281, 1029, 498, 291, 528, 733, 295, 544, 854, 420, 5192, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.08289886183208889, "compression_ratio": 1.5482233502538072, "no_speech_prob": 8.013017577468418e-06}, {"id": 614, "seek": 445500, "start": 4474.0, "end": 4481.0, "text": " But this is kind of thinking about this course of like the general goal of making matrix computations faster.", "tokens": [583, 341, 307, 733, 295, 1953, 466, 341, 1164, 295, 411, 264, 2674, 3387, 295, 1455, 8141, 2807, 763, 4663, 13], "temperature": 0.0, "avg_logprob": -0.08289886183208889, "compression_ratio": 1.5482233502538072, "no_speech_prob": 8.013017577468418e-06}, {"id": 615, "seek": 448100, "start": 4481.0, "end": 4490.0, "text": " Using a GPU is a great way to do that.", "tokens": [11142, 257, 18407, 307, 257, 869, 636, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.13176856438318887, "compression_ratio": 1.1869918699186992, "no_speech_prob": 2.9020645797572797e-06}, {"id": 616, "seek": 448100, "start": 4490.0, "end": 4496.0, "text": " I mean, we'll see in a moment. Oh, do you have a ballpark you want to throw out?", "tokens": [286, 914, 11, 321, 603, 536, 294, 257, 1623, 13, 876, 11, 360, 291, 362, 257, 2594, 31239, 291, 528, 281, 3507, 484, 30], "temperature": 0.0, "avg_logprob": -0.13176856438318887, "compression_ratio": 1.1869918699186992, "no_speech_prob": 2.9020645797572797e-06}, {"id": 617, "seek": 448100, "start": 4496.0, "end": 4502.0, "text": " Yes, significantly faster.", "tokens": [1079, 11, 10591, 4663, 13], "temperature": 0.0, "avg_logprob": -0.13176856438318887, "compression_ratio": 1.1869918699186992, "no_speech_prob": 2.9020645797572797e-06}, {"id": 618, "seek": 450200, "start": 4502.0, "end": 4516.0, "text": " So we have to do some imports from I guess also so PyTorch is in it's a Python library, but it's Torch is a Lua library that's been used in computer graphics for years.", "tokens": [407, 321, 362, 281, 360, 512, 41596, 490, 286, 2041, 611, 370, 9953, 51, 284, 339, 307, 294, 309, 311, 257, 15329, 6405, 11, 457, 309, 311, 7160, 339, 307, 257, 441, 4398, 6405, 300, 311, 668, 1143, 294, 3820, 11837, 337, 924, 13], "temperature": 0.0, "avg_logprob": -0.10370780609466218, "compression_ratio": 1.691588785046729, "no_speech_prob": 2.642387471496477e-06}, {"id": 619, "seek": 450200, "start": 4516.0, "end": 4528.0, "text": " So it's even though PyTorch is newly released, it's kind of coming from this really well developed library that grew up around computer graphics, which have to do very fast matrix computations.", "tokens": [407, 309, 311, 754, 1673, 9953, 51, 284, 339, 307, 15109, 4736, 11, 309, 311, 733, 295, 1348, 490, 341, 534, 731, 4743, 6405, 300, 6109, 493, 926, 3820, 11837, 11, 597, 362, 281, 360, 588, 2370, 8141, 2807, 763, 13], "temperature": 0.0, "avg_logprob": -0.10370780609466218, "compression_ratio": 1.691588785046729, "no_speech_prob": 2.642387471496477e-06}, {"id": 620, "seek": 452800, "start": 4528.0, "end": 4540.0, "text": " We're also like really all indebted to the video gaming industry for kind of keeping GPU technology advancing so much.", "tokens": [492, 434, 611, 411, 534, 439, 24162, 4517, 292, 281, 264, 960, 9703, 3518, 337, 733, 295, 5145, 18407, 2899, 27267, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.0818798588771446, "compression_ratio": 1.3154362416107384, "no_speech_prob": 3.1872634735918837e-06}, {"id": 621, "seek": 452800, "start": 4540.0, "end": 4549.0, "text": " So something you'll notice with with PyTorch is well, I'll come back to this.", "tokens": [407, 746, 291, 603, 3449, 365, 365, 9953, 51, 284, 339, 307, 731, 11, 286, 603, 808, 646, 281, 341, 13], "temperature": 0.0, "avg_logprob": -0.0818798588771446, "compression_ratio": 1.3154362416107384, "no_speech_prob": 3.1872634735918837e-06}, {"id": 622, "seek": 454900, "start": 4549.0, "end": 4558.0, "text": " So in PyTorch you have tensors and variables and they kind of have the same API meaning you can do the same things to them.", "tokens": [407, 294, 9953, 51, 284, 339, 291, 362, 10688, 830, 293, 9102, 293, 436, 733, 295, 362, 264, 912, 9362, 3620, 291, 393, 360, 264, 912, 721, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.10105586051940918, "compression_ratio": 1.4705882352941178, "no_speech_prob": 5.954562311671907e-06}, {"id": 623, "seek": 454900, "start": 4558.0, "end": 4567.0, "text": " However, variables remember or keep track of how they were created, which will be necessary later on.", "tokens": [2908, 11, 9102, 1604, 420, 1066, 2837, 295, 577, 436, 645, 2942, 11, 597, 486, 312, 4818, 1780, 322, 13], "temperature": 0.0, "avg_logprob": -0.10105586051940918, "compression_ratio": 1.4705882352941178, "no_speech_prob": 5.954562311671907e-06}, {"id": 624, "seek": 456700, "start": 4567.0, "end": 4582.0, "text": " We'll find out about that. So we've just converted our vectors to dense V, then we're casting them. So V as type, np.float32 and putting that into a Torch tensor.", "tokens": [492, 603, 915, 484, 466, 300, 13, 407, 321, 600, 445, 16424, 527, 18875, 281, 18011, 691, 11, 550, 321, 434, 17301, 552, 13, 407, 691, 382, 2010, 11, 33808, 13, 43645, 267, 11440, 293, 3372, 300, 666, 257, 7160, 339, 40863, 13], "temperature": 0.0, "avg_logprob": -0.16129501282222689, "compression_ratio": 1.4125, "no_speech_prob": 9.079609299078584e-06}, {"id": 625, "seek": 456700, "start": 4582.0, "end": 4586.0, "text": " Remember a tensor is just kind of a generalization of a matrix.", "tokens": [5459, 257, 40863, 307, 445, 733, 295, 257, 2674, 2144, 295, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.16129501282222689, "compression_ratio": 1.4125, "no_speech_prob": 9.079609299078584e-06}, {"id": 626, "seek": 458600, "start": 4586.0, "end": 4599.0, "text": " And so we had to specify our type. And this is something that you'll see with a lot of these options to speed things up. They need to know the type.", "tokens": [400, 370, 321, 632, 281, 16500, 527, 2010, 13, 400, 341, 307, 746, 300, 291, 603, 536, 365, 257, 688, 295, 613, 3956, 281, 3073, 721, 493, 13, 814, 643, 281, 458, 264, 2010, 13], "temperature": 0.0, "avg_logprob": -0.11096873106779875, "compression_ratio": 1.6059113300492611, "no_speech_prob": 3.0892940685589565e-06}, {"id": 627, "seek": 458600, "start": 4599.0, "end": 4612.0, "text": " So here we kind of redefine the methods that we had above. And you'll notice that they're fairly similar. A key thing that's different, we have these dot mm for matrix multiply.", "tokens": [407, 510, 321, 733, 295, 38818, 533, 264, 7150, 300, 321, 632, 3673, 13, 400, 291, 603, 3449, 300, 436, 434, 6457, 2531, 13, 316, 2141, 551, 300, 311, 819, 11, 321, 362, 613, 5893, 11169, 337, 8141, 12972, 13], "temperature": 0.0, "avg_logprob": -0.11096873106779875, "compression_ratio": 1.6059113300492611, "no_speech_prob": 3.0892940685589565e-06}, {"id": 628, "seek": 461200, "start": 4612.0, "end": 4620.0, "text": " So now this w dot mm h that's just doing w times h.", "tokens": [407, 586, 341, 261, 5893, 11169, 276, 300, 311, 445, 884, 261, 1413, 276, 13], "temperature": 0.0, "avg_logprob": -0.20969645182291666, "compression_ratio": 1.130952380952381, "no_speech_prob": 4.9368454710929655e-06}, {"id": 629, "seek": 461200, "start": 4620.0, "end": 4626.0, "text": " Let me see what other differences jump out.", "tokens": [961, 385, 536, 437, 661, 7300, 3012, 484, 13], "temperature": 0.0, "avg_logprob": -0.20969645182291666, "compression_ratio": 1.130952380952381, "no_speech_prob": 4.9368454710929655e-06}, {"id": 630, "seek": 462600, "start": 4626.0, "end": 4645.0, "text": " Torch dot clamp is a method that takes a value and then we'll cut it off with a kind of minimum and or maximum that you feed in. So here we want m minus mu, but we're cutting it off at zero because if m is large and positive, we're happy.", "tokens": [7160, 339, 5893, 17690, 307, 257, 3170, 300, 2516, 257, 2158, 293, 550, 321, 603, 1723, 309, 766, 365, 257, 733, 295, 7285, 293, 420, 6674, 300, 291, 3154, 294, 13, 407, 510, 321, 528, 275, 3175, 2992, 11, 457, 321, 434, 6492, 309, 766, 412, 4018, 570, 498, 275, 307, 2416, 293, 3353, 11, 321, 434, 2055, 13], "temperature": 0.0, "avg_logprob": -0.12603868756975448, "compression_ratio": 1.4691358024691359, "no_speech_prob": 1.952467573573813e-05}, {"id": 631, "seek": 464500, "start": 4645.0, "end": 4660.0, "text": " So it doesn't have to be large. If m is positive and bigger than u, we don't want any penalty. So that max zero will take effect and there's zero penalty. So this is handling that penalty from before.", "tokens": [407, 309, 1177, 380, 362, 281, 312, 2416, 13, 759, 275, 307, 3353, 293, 3801, 813, 344, 11, 321, 500, 380, 528, 604, 16263, 13, 407, 300, 11469, 4018, 486, 747, 1802, 293, 456, 311, 4018, 16263, 13, 407, 341, 307, 13175, 300, 16263, 490, 949, 13], "temperature": 0.0, "avg_logprob": -0.09239892398609835, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.248245553142624e-06}, {"id": 632, "seek": 466000, "start": 4660.0, "end": 4676.0, "text": " So here the dot sub underscore, that subtraction and it's a minus equals subtraction from the original.", "tokens": [407, 510, 264, 5893, 1422, 37556, 11, 300, 16390, 313, 293, 309, 311, 257, 3175, 6915, 16390, 313, 490, 264, 3380, 13], "temperature": 0.0, "avg_logprob": -0.13010945916175842, "compression_ratio": 1.4761904761904763, "no_speech_prob": 5.043403348281572e-07}, {"id": 633, "seek": 466000, "start": 4676.0, "end": 4688.0, "text": " And this, you don't have to understand all the syntactic details of PyTorch, but I wanted you to get a general general feel for what's going on.", "tokens": [400, 341, 11, 291, 500, 380, 362, 281, 1223, 439, 264, 23980, 19892, 4365, 295, 9953, 51, 284, 339, 11, 457, 286, 1415, 291, 281, 483, 257, 2674, 2674, 841, 337, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.13010945916175842, "compression_ratio": 1.4761904761904763, "no_speech_prob": 5.043403348281572e-07}, {"id": 634, "seek": 468800, "start": 4688.0, "end": 4708.0, "text": " So we need to kind of declare our float tensors and said this last time, but a float is basically a decimal as opposed to an integer. When we talk about data types.", "tokens": [407, 321, 643, 281, 733, 295, 19710, 527, 15706, 10688, 830, 293, 848, 341, 1036, 565, 11, 457, 257, 15706, 307, 1936, 257, 26601, 382, 8851, 281, 364, 24922, 13, 1133, 321, 751, 466, 1412, 3467, 13], "temperature": 0.0, "avg_logprob": -0.09227605563838308, "compression_ratio": 1.3442622950819672, "no_speech_prob": 5.7716497394721955e-06}, {"id": 635, "seek": 470800, "start": 4708.0, "end": 4720.0, "text": " Again, this is in a little bit of a different order, but we're doing the same thing. W is being initialized with a normal random variable and then we're doing dot apps to do the absolute at the end.", "tokens": [3764, 11, 341, 307, 294, 257, 707, 857, 295, 257, 819, 1668, 11, 457, 321, 434, 884, 264, 912, 551, 13, 343, 307, 885, 5883, 1602, 365, 257, 2710, 4974, 7006, 293, 550, 321, 434, 884, 5893, 7733, 281, 360, 264, 8236, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.10588312149047852, "compression_ratio": 1.5815217391304348, "no_speech_prob": 2.1776222638436593e-05}, {"id": 636, "seek": 470800, "start": 4720.0, "end": 4735.0, "text": " But that's kind of just saying the order that these happen in. Take the absolute value. Yes.", "tokens": [583, 300, 311, 733, 295, 445, 1566, 264, 1668, 300, 613, 1051, 294, 13, 3664, 264, 8236, 2158, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.10588312149047852, "compression_ratio": 1.5815217391304348, "no_speech_prob": 2.1776222638436593e-05}, {"id": 637, "seek": 473500, "start": 4735.0, "end": 4744.0, "text": " Great. Thank you.", "tokens": [3769, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.14852845668792725, "compression_ratio": 0.971830985915493, "no_speech_prob": 3.1380081964016426e-06}, {"id": 638, "seek": 473500, "start": 4744.0, "end": 4750.0, "text": " I guess these are probably all still saved in here.", "tokens": [286, 2041, 613, 366, 1391, 439, 920, 6624, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.14852845668792725, "compression_ratio": 0.971830985915493, "no_speech_prob": 3.1380081964016426e-06}, {"id": 639, "seek": 475000, "start": 4750.0, "end": 4765.0, "text": " So now you can see this is going much faster than before. And the one above, I actually probably should have run for.", "tokens": [407, 586, 291, 393, 536, 341, 307, 516, 709, 4663, 813, 949, 13, 400, 264, 472, 3673, 11, 286, 767, 1391, 820, 362, 1190, 337, 13], "temperature": 0.0, "avg_logprob": -0.13283201853434246, "compression_ratio": 1.1470588235294117, "no_speech_prob": 3.9668129829806276e-06}, {"id": 640, "seek": 476500, "start": 4765.0, "end": 4782.0, "text": " Yeah, so this one, each loop was only 10. I only did 50 total and that felt kind of slow. Now each loop is 100 and we're doing 1000 and that was 1000 iterations. That was much quicker.", "tokens": [865, 11, 370, 341, 472, 11, 1184, 6367, 390, 787, 1266, 13, 286, 787, 630, 2625, 3217, 293, 300, 2762, 733, 295, 2964, 13, 823, 1184, 6367, 307, 2319, 293, 321, 434, 884, 9714, 293, 300, 390, 9714, 36540, 13, 663, 390, 709, 16255, 13], "temperature": 0.0, "avg_logprob": -0.11140983406154589, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.18738989335543e-06}, {"id": 641, "seek": 476500, "start": 4782.0, "end": 4792.0, "text": " And so you see that these have improved. You still are going to need to do some more, but we're able to kind of get through a lot of iterations much quicker.", "tokens": [400, 370, 291, 536, 300, 613, 362, 9689, 13, 509, 920, 366, 516, 281, 643, 281, 360, 512, 544, 11, 457, 321, 434, 1075, 281, 733, 295, 483, 807, 257, 688, 295, 36540, 709, 16255, 13], "temperature": 0.0, "avg_logprob": -0.11140983406154589, "compression_ratio": 1.6285714285714286, "no_speech_prob": 3.18738989335543e-06}, {"id": 642, "seek": 479200, "start": 4792.0, "end": 4800.0, "text": " Any questions? But this is as good as it gets. I thought we got better.", "tokens": [2639, 1651, 30, 583, 341, 307, 382, 665, 382, 309, 2170, 13, 286, 1194, 321, 658, 1101, 13], "temperature": 0.0, "avg_logprob": -0.19769106489239316, "compression_ratio": 1.168421052631579, "no_speech_prob": 1.5204326700768434e-05}, {"id": 643, "seek": 479200, "start": 4800.0, "end": 4805.0, "text": " Then here are the topics that we found.", "tokens": [1396, 510, 366, 264, 8378, 300, 321, 1352, 13], "temperature": 0.0, "avg_logprob": -0.19769106489239316, "compression_ratio": 1.168421052631579, "no_speech_prob": 1.5204326700768434e-05}, {"id": 644, "seek": 480500, "start": 4805.0, "end": 4829.0, "text": " So objective, morality, values, moral, subjective, science, absolute, claim. I guess that maybe that's atheism. God, Jesus, Bible, believe, atheism, Christian, perhaps religion, miscellaneous, space, NASA, launch, shuttle, orbit, lunar, moon is a good space one.", "tokens": [407, 10024, 11, 29106, 11, 4190, 11, 9723, 11, 25972, 11, 3497, 11, 8236, 11, 3932, 13, 286, 2041, 300, 1310, 300, 311, 27033, 1434, 13, 1265, 11, 2705, 11, 6544, 11, 1697, 11, 27033, 1434, 11, 5778, 11, 4317, 7561, 11, 3346, 4164, 15447, 11, 1901, 11, 12077, 11, 4025, 11, 26728, 11, 13991, 11, 32581, 11, 7135, 307, 257, 665, 1901, 472, 13], "temperature": 0.0, "avg_logprob": -0.10909643035004105, "compression_ratio": 1.4395604395604396, "no_speech_prob": 2.1566570467257407e-06}, {"id": 645, "seek": 482900, "start": 4829.0, "end": 4846.0, "text": " So these are some nice topics.", "tokens": [407, 613, 366, 512, 1481, 8378, 13], "temperature": 0.0, "avg_logprob": -0.10230448029258034, "compression_ratio": 0.7894736842105263, "no_speech_prob": 1.0450646186654922e-05}, {"id": 646, "seek": 484600, "start": 4846.0, "end": 4862.0, "text": " So Jeremy was referencing what we got from the built in. So these are the topics from the built in. And again, since this is an unsupervised learning problem, we don't have an absolute measure of like, these are what great topics are.", "tokens": [407, 17809, 390, 40582, 437, 321, 658, 490, 264, 3094, 294, 13, 407, 613, 366, 264, 8378, 490, 264, 3094, 294, 13, 400, 797, 11, 1670, 341, 307, 364, 2693, 12879, 24420, 2539, 1154, 11, 321, 500, 380, 362, 364, 8236, 3481, 295, 411, 11, 613, 366, 437, 869, 8378, 366, 13], "temperature": 0.0, "avg_logprob": -0.08057661459479533, "compression_ratio": 1.5416666666666667, "no_speech_prob": 4.029011506645475e-06}, {"id": 647, "seek": 484600, "start": 4862.0, "end": 4868.0, "text": " We're just kind of using our intuition about looking at them.", "tokens": [492, 434, 445, 733, 295, 1228, 527, 24002, 466, 1237, 412, 552, 13], "temperature": 0.0, "avg_logprob": -0.08057661459479533, "compression_ratio": 1.5416666666666667, "no_speech_prob": 4.029011506645475e-06}, {"id": 648, "seek": 486800, "start": 4868.0, "end": 4876.0, "text": " Well, I mean, that's giving you how good your approximation is.", "tokens": [1042, 11, 286, 914, 11, 300, 311, 2902, 291, 577, 665, 428, 28023, 307, 13], "temperature": 0.0, "avg_logprob": -0.1191206209121212, "compression_ratio": 1.1538461538461537, "no_speech_prob": 1.7776403183233924e-05}, {"id": 649, "seek": 486800, "start": 4876.0, "end": 4884.0, "text": " Yeah, so these are these are good topics.", "tokens": [865, 11, 370, 613, 366, 613, 366, 665, 8378, 13], "temperature": 0.0, "avg_logprob": -0.1191206209121212, "compression_ratio": 1.1538461538461537, "no_speech_prob": 1.7776403183233924e-05}, {"id": 650, "seek": 488400, "start": 4884.0, "end": 4901.0, "text": " Okay, so in that version, or actually I should ask, are there any other questions about this version? Yes. Can you pass the microphone?", "tokens": [1033, 11, 370, 294, 300, 3037, 11, 420, 767, 286, 820, 1029, 11, 366, 456, 604, 661, 1651, 466, 341, 3037, 30, 1079, 13, 1664, 291, 1320, 264, 10952, 30], "temperature": 0.0, "avg_logprob": -0.18473440058091106, "compression_ratio": 1.1946902654867257, "no_speech_prob": 2.111037065333221e-05}, {"id": 651, "seek": 490100, "start": 4901.0, "end": 4914.0, "text": " You can still use it, but you're not getting the speed up. So there's less, less reason to. I mean, you could still question of deep learning aside.", "tokens": [509, 393, 920, 764, 309, 11, 457, 291, 434, 406, 1242, 264, 3073, 493, 13, 407, 456, 311, 1570, 11, 1570, 1778, 281, 13, 286, 914, 11, 291, 727, 920, 1168, 295, 2452, 2539, 7359, 13], "temperature": 0.0, "avg_logprob": -0.14732820016366463, "compression_ratio": 1.5026455026455026, "no_speech_prob": 9.078928997041658e-06}, {"id": 652, "seek": 490100, "start": 4914.0, "end": 4923.0, "text": " If you're using it as a NumPy alternative, kind of the main motivation for that is the GPU speed up, but it will still work.", "tokens": [759, 291, 434, 1228, 309, 382, 257, 22592, 47, 88, 8535, 11, 733, 295, 264, 2135, 12335, 337, 300, 307, 264, 18407, 3073, 493, 11, 457, 309, 486, 920, 589, 13], "temperature": 0.0, "avg_logprob": -0.14732820016366463, "compression_ratio": 1.5026455026455026, "no_speech_prob": 9.078928997041658e-06}, {"id": 653, "seek": 490100, "start": 4923.0, "end": 4926.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.14732820016366463, "compression_ratio": 1.5026455026455026, "no_speech_prob": 9.078928997041658e-06}, {"id": 654, "seek": 490100, "start": 4926.0, "end": 4930.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.14732820016366463, "compression_ratio": 1.5026455026455026, "no_speech_prob": 9.078928997041658e-06}, {"id": 655, "seek": 493000, "start": 4930.0, "end": 4945.0, "text": " And also when you're using PyTorch, if you do have a GPU, you need to be sure to use.Cuda. Otherwise PyTorch will not put your stuff on the GPU, which is what you want to get the speed up. So you have to tell it to do that.", "tokens": [400, 611, 562, 291, 434, 1228, 9953, 51, 284, 339, 11, 498, 291, 360, 362, 257, 18407, 11, 291, 643, 281, 312, 988, 281, 764, 2411, 34, 11152, 13, 10328, 9953, 51, 284, 339, 486, 406, 829, 428, 1507, 322, 264, 18407, 11, 597, 307, 437, 291, 528, 281, 483, 264, 3073, 493, 13, 407, 291, 362, 281, 980, 309, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.11703353457980686, "compression_ratio": 1.5658536585365854, "no_speech_prob": 8.800637260719668e-06}, {"id": 656, "seek": 493000, "start": 4945.0, "end": 4951.0, "text": " And you will have to install PyTorch to be able to do this.", "tokens": [400, 291, 486, 362, 281, 3625, 9953, 51, 284, 339, 281, 312, 1075, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.11703353457980686, "compression_ratio": 1.5658536585365854, "no_speech_prob": 8.800637260719668e-06}, {"id": 657, "seek": 493000, "start": 4951.0, "end": 4958.0, "text": " But yeah, I recommend using Anaconda.", "tokens": [583, 1338, 11, 286, 2748, 1228, 1107, 326, 12233, 13], "temperature": 0.0, "avg_logprob": -0.11703353457980686, "compression_ratio": 1.5658536585365854, "no_speech_prob": 8.800637260719668e-06}, {"id": 658, "seek": 495800, "start": 4958.0, "end": 4969.0, "text": " So this version, note, so this was very similar to what we just did in what we had done in NumPy from scratch.", "tokens": [407, 341, 3037, 11, 3637, 11, 370, 341, 390, 588, 2531, 281, 437, 321, 445, 630, 294, 437, 321, 632, 1096, 294, 22592, 47, 88, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.1371395000513049, "compression_ratio": 1.4972677595628416, "no_speech_prob": 2.8129040856583742e-06}, {"id": 659, "seek": 495800, "start": 4969.0, "end": 4972.0, "text": " We still had to know what the gradients were.", "tokens": [492, 920, 632, 281, 458, 437, 264, 2771, 2448, 645, 13], "temperature": 0.0, "avg_logprob": -0.1371395000513049, "compression_ratio": 1.4972677595628416, "no_speech_prob": 2.8129040856583742e-06}, {"id": 660, "seek": 495800, "start": 4972.0, "end": 4980.0, "text": " And this can really be a pain, particularly if you have a more complicated method having to calculate the derivative.", "tokens": [400, 341, 393, 534, 312, 257, 1822, 11, 4098, 498, 291, 362, 257, 544, 6179, 3170, 1419, 281, 8873, 264, 13760, 13], "temperature": 0.0, "avg_logprob": -0.1371395000513049, "compression_ratio": 1.4972677595628416, "no_speech_prob": 2.8129040856583742e-06}, {"id": 661, "seek": 498000, "start": 4980.0, "end": 4988.0, "text": " And so PyTorch has something really handy called AutoGrad.", "tokens": [400, 370, 9953, 51, 284, 339, 575, 746, 534, 13239, 1219, 13738, 38, 6206, 13], "temperature": 0.0, "avg_logprob": -0.057342001732359545, "compression_ratio": 1.578723404255319, "no_speech_prob": 2.419840825496067e-07}, {"id": 662, "seek": 498000, "start": 4988.0, "end": 4997.0, "text": " And AutoGrad will do automatic differentiation. So if you give it a method, it finds the derivative for you.", "tokens": [400, 13738, 38, 6206, 486, 360, 12509, 38902, 13, 407, 498, 291, 976, 309, 257, 3170, 11, 309, 10704, 264, 13760, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.057342001732359545, "compression_ratio": 1.578723404255319, "no_speech_prob": 2.419840825496067e-07}, {"id": 663, "seek": 498000, "start": 4997.0, "end": 5002.0, "text": " And so this is great because you can use it on stuff where you don't even know what the derivative is.", "tokens": [400, 370, 341, 307, 869, 570, 291, 393, 764, 309, 322, 1507, 689, 291, 500, 380, 754, 458, 437, 264, 13760, 307, 13], "temperature": 0.0, "avg_logprob": -0.057342001732359545, "compression_ratio": 1.578723404255319, "no_speech_prob": 2.419840825496067e-07}, {"id": 664, "seek": 498000, "start": 5002.0, "end": 5009.0, "text": " And so the approach that I'm about to show would work for, I think, almost any optimization problem.", "tokens": [400, 370, 264, 3109, 300, 286, 478, 466, 281, 855, 576, 589, 337, 11, 286, 519, 11, 1920, 604, 19618, 1154, 13], "temperature": 0.0, "avg_logprob": -0.057342001732359545, "compression_ratio": 1.578723404255319, "no_speech_prob": 2.419840825496067e-07}, {"id": 665, "seek": 500900, "start": 5009.0, "end": 5014.0, "text": " And so here we're going to have to use variables for this.", "tokens": [400, 370, 510, 321, 434, 516, 281, 362, 281, 764, 9102, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.0933776749504937, "compression_ratio": 1.6073059360730593, "no_speech_prob": 3.5007110454898793e-06}, {"id": 666, "seek": 500900, "start": 5014.0, "end": 5022.0, "text": " And with so again, kind of the variable has the same API as a tensor, but it's got this memory of how it's how it's created,", "tokens": [400, 365, 370, 797, 11, 733, 295, 264, 7006, 575, 264, 912, 9362, 382, 257, 40863, 11, 457, 309, 311, 658, 341, 4675, 295, 577, 309, 311, 577, 309, 311, 2942, 11], "temperature": 0.0, "avg_logprob": -0.0933776749504937, "compression_ratio": 1.6073059360730593, "no_speech_prob": 3.5007110454898793e-06}, {"id": 667, "seek": 500900, "start": 5022.0, "end": 5027.0, "text": " which is then can be used to automatically calculate the derivative.", "tokens": [597, 307, 550, 393, 312, 1143, 281, 6772, 8873, 264, 13760, 13], "temperature": 0.0, "avg_logprob": -0.0933776749504937, "compression_ratio": 1.6073059360730593, "no_speech_prob": 3.5007110454898793e-06}, {"id": 668, "seek": 500900, "start": 5027.0, "end": 5038.0, "text": " And you'll want to pass requires grad equals true. And so you're saying this this needs a gradient.", "tokens": [400, 291, 603, 528, 281, 1320, 7029, 2771, 6915, 2074, 13, 400, 370, 291, 434, 1566, 341, 341, 2203, 257, 16235, 13], "temperature": 0.0, "avg_logprob": -0.0933776749504937, "compression_ratio": 1.6073059360730593, "no_speech_prob": 3.5007110454898793e-06}, {"id": 669, "seek": 503800, "start": 5038.0, "end": 5047.0, "text": " So here we create calling them now Pw and Ph.", "tokens": [407, 510, 321, 1884, 5141, 552, 586, 430, 86, 293, 2623, 13], "temperature": 0.0, "avg_logprob": -0.11824129104614257, "compression_ratio": 1.4427480916030535, "no_speech_prob": 1.5293868500521057e-06}, {"id": 670, "seek": 503800, "start": 5047.0, "end": 5060.0, "text": " And so then down here with our methods, notice we no longer have a grads method because we're not having to give it a formula for the gradient.", "tokens": [400, 370, 550, 760, 510, 365, 527, 7150, 11, 3449, 321, 572, 2854, 362, 257, 2771, 82, 3170, 570, 321, 434, 406, 1419, 281, 976, 309, 257, 8513, 337, 264, 16235, 13], "temperature": 0.0, "avg_logprob": -0.11824129104614257, "compression_ratio": 1.4427480916030535, "no_speech_prob": 1.5293868500521057e-06}, {"id": 671, "seek": 506000, "start": 5060.0, "end": 5076.0, "text": " We do still want a penalty term. So that's kind of taking our matrix, checking that it's less than zero.", "tokens": [492, 360, 920, 528, 257, 16263, 1433, 13, 407, 300, 311, 733, 295, 1940, 527, 8141, 11, 8568, 300, 309, 311, 1570, 813, 4018, 13], "temperature": 0.0, "avg_logprob": -0.18088316561570808, "compression_ratio": 1.5743243243243243, "no_speech_prob": 1.8161480284106801e-06}, {"id": 672, "seek": 506000, "start": 5076.0, "end": 5087.0, "text": " Or sorry, if it's if it's less than zero, we're taking its value. Otherwise, the value is zero or yeah, we're maxing it at zero.", "tokens": [1610, 2597, 11, 498, 309, 311, 498, 309, 311, 1570, 813, 4018, 11, 321, 434, 1940, 1080, 2158, 13, 10328, 11, 264, 2158, 307, 4018, 420, 1338, 11, 321, 434, 11469, 278, 309, 412, 4018, 13], "temperature": 0.0, "avg_logprob": -0.18088316561570808, "compression_ratio": 1.5743243243243243, "no_speech_prob": 1.8161480284106801e-06}, {"id": 673, "seek": 508700, "start": 5087.0, "end": 5091.0, "text": " And then in this case, we ended up squaring that to add.", "tokens": [400, 550, 294, 341, 1389, 11, 321, 4590, 493, 2339, 1921, 300, 281, 909, 13], "temperature": 0.0, "avg_logprob": -0.20448528636585583, "compression_ratio": 1.5631067961165048, "no_speech_prob": 1.3006418157601729e-05}, {"id": 674, "seek": 508700, "start": 5091.0, "end": 5104.0, "text": " And I we probably should have done this in the methods above to add a greater penalty because you would see above we were still kind of getting a lot more negative numbers than we would like.", "tokens": [400, 286, 321, 1391, 820, 362, 1096, 341, 294, 264, 7150, 3673, 281, 909, 257, 5044, 16263, 570, 291, 576, 536, 3673, 321, 645, 920, 733, 295, 1242, 257, 688, 544, 3671, 3547, 813, 321, 576, 411, 13], "temperature": 0.0, "avg_logprob": -0.20448528636585583, "compression_ratio": 1.5631067961165048, "no_speech_prob": 1.3006418157601729e-05}, {"id": 675, "seek": 508700, "start": 5104.0, "end": 5108.0, "text": " The reason we didn't do it before is because it kept making the gradient.", "tokens": [440, 1778, 321, 994, 380, 360, 309, 949, 307, 570, 309, 4305, 1455, 264, 16235, 13], "temperature": 0.0, "avg_logprob": -0.20448528636585583, "compression_ratio": 1.5631067961165048, "no_speech_prob": 1.3006418157601729e-05}, {"id": 676, "seek": 510800, "start": 5108.0, "end": 5121.0, "text": " Yeah, you can have more complicated formulas without the downside of knowing that you'll need their their gradient.", "tokens": [865, 11, 291, 393, 362, 544, 6179, 30546, 1553, 264, 25060, 295, 5276, 300, 291, 603, 643, 641, 641, 16235, 13], "temperature": 0.0, "avg_logprob": -0.116121254469219, "compression_ratio": 1.2519685039370079, "no_speech_prob": 2.406052999504027e-06}, {"id": 677, "seek": 510800, "start": 5121.0, "end": 5131.0, "text": " Let's see. So report is basically the same.", "tokens": [961, 311, 536, 13, 407, 2275, 307, 1936, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.116121254469219, "compression_ratio": 1.2519685039370079, "no_speech_prob": 2.406052999504027e-06}, {"id": 678, "seek": 513100, "start": 5131.0, "end": 5141.0, "text": " Yeah, again, we're having to use the dot CUDA to tell Pytorch put this on the GPU. And then here we're going to use an optimizer.", "tokens": [865, 11, 797, 11, 321, 434, 1419, 281, 764, 264, 5893, 29777, 7509, 281, 980, 430, 4328, 284, 339, 829, 341, 322, 264, 18407, 13, 400, 550, 510, 321, 434, 516, 281, 764, 364, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.20763105716345445, "compression_ratio": 1.5897435897435896, "no_speech_prob": 4.356632871349575e-06}, {"id": 679, "seek": 513100, "start": 5141.0, "end": 5146.0, "text": " So this is we're kind of telling it. This is how I want you to optimize things.", "tokens": [407, 341, 307, 321, 434, 733, 295, 3585, 309, 13, 639, 307, 577, 286, 528, 291, 281, 19719, 721, 13], "temperature": 0.0, "avg_logprob": -0.20763105716345445, "compression_ratio": 1.5897435897435896, "no_speech_prob": 4.356632871349575e-06}, {"id": 680, "seek": 513100, "start": 5146.0, "end": 5158.0, "text": " Adam is I think from a few years ago, it's a relative it's a modern optimizer as opposed to, you know, HDD, which is kind of this more classic, classic technique.", "tokens": [7938, 307, 286, 519, 490, 257, 1326, 924, 2057, 11, 309, 311, 257, 4972, 309, 311, 257, 4363, 5028, 6545, 382, 8851, 281, 11, 291, 458, 11, 12149, 35, 11, 597, 307, 733, 295, 341, 544, 7230, 11, 7230, 6532, 13], "temperature": 0.0, "avg_logprob": -0.20763105716345445, "compression_ratio": 1.5897435897435896, "no_speech_prob": 4.356632871349575e-06}, {"id": 681, "seek": 515800, "start": 5158.0, "end": 5163.0, "text": " Give a learning rate. We can still get our report.", "tokens": [5303, 257, 2539, 3314, 13, 492, 393, 920, 483, 527, 2275, 13], "temperature": 0.0, "avg_logprob": -0.11956836400407084, "compression_ratio": 1.5904761904761904, "no_speech_prob": 1.9946530755987624e-06}, {"id": 682, "seek": 515800, "start": 5163.0, "end": 5167.0, "text": " And so so this this code here, this for loop is really useful.", "tokens": [400, 370, 370, 341, 341, 3089, 510, 11, 341, 337, 6367, 307, 534, 4420, 13], "temperature": 0.0, "avg_logprob": -0.11956836400407084, "compression_ratio": 1.5904761904761904, "no_speech_prob": 1.9946530755987624e-06}, {"id": 683, "seek": 515800, "start": 5167.0, "end": 5170.0, "text": " This is something that you could use with a lot of different problems.", "tokens": [639, 307, 746, 300, 291, 727, 764, 365, 257, 688, 295, 819, 2740, 13], "temperature": 0.0, "avg_logprob": -0.11956836400407084, "compression_ratio": 1.5904761904761904, "no_speech_prob": 1.9946530755987624e-06}, {"id": 684, "seek": 515800, "start": 5170.0, "end": 5178.0, "text": " We just. Pytorch requires you to set your gradients to zero kind of manually with your optimizer.", "tokens": [492, 445, 13, 430, 4328, 284, 339, 7029, 291, 281, 992, 428, 2771, 2448, 281, 4018, 733, 295, 16945, 365, 428, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.11956836400407084, "compression_ratio": 1.5904761904761904, "no_speech_prob": 1.9946530755987624e-06}, {"id": 685, "seek": 515800, "start": 5178.0, "end": 5181.0, "text": " So our optimizer was saying the gradients are zero.", "tokens": [407, 527, 5028, 6545, 390, 1566, 264, 2771, 2448, 366, 4018, 13], "temperature": 0.0, "avg_logprob": -0.11956836400407084, "compression_ratio": 1.5904761904761904, "no_speech_prob": 1.9946530755987624e-06}, {"id": 686, "seek": 518100, "start": 5181.0, "end": 5192.0, "text": " We calculate the loss. This is a Pytorch method backward and kind of taking a backwards.", "tokens": [492, 8873, 264, 4470, 13, 639, 307, 257, 430, 4328, 284, 339, 3170, 23897, 293, 733, 295, 1940, 257, 12204, 13], "temperature": 0.0, "avg_logprob": -0.17546898523966473, "compression_ratio": 1.4076433121019107, "no_speech_prob": 1.760291297614458e-06}, {"id": 687, "seek": 518100, "start": 5192.0, "end": 5200.0, "text": " That calculates your gradient. So it's kind of like we.", "tokens": [663, 4322, 1024, 428, 16235, 13, 407, 309, 311, 733, 295, 411, 321, 13], "temperature": 0.0, "avg_logprob": -0.17546898523966473, "compression_ratio": 1.4076433121019107, "no_speech_prob": 1.760291297614458e-06}, {"id": 688, "seek": 518100, "start": 5200.0, "end": 5206.0, "text": " Actually, this. Kind of surprises me because we're doing our optimizer step.", "tokens": [5135, 11, 341, 13, 9242, 295, 22655, 385, 570, 321, 434, 884, 527, 5028, 6545, 1823, 13], "temperature": 0.0, "avg_logprob": -0.17546898523966473, "compression_ratio": 1.4076433121019107, "no_speech_prob": 1.760291297614458e-06}, {"id": 689, "seek": 520600, "start": 5206.0, "end": 5211.0, "text": " I don't think you would want backwards afterwards now.", "tokens": [286, 500, 380, 519, 291, 576, 528, 12204, 10543, 586, 13], "temperature": 0.0, "avg_logprob": -0.3239747661433808, "compression_ratio": 1.5696969696969696, "no_speech_prob": 7.483181252609938e-05}, {"id": 690, "seek": 520600, "start": 5211.0, "end": 5218.0, "text": " So L equals loss calculates the value of the loss. L is now a variable, which is remembered how it is calculated.", "tokens": [407, 441, 6915, 4470, 4322, 1024, 264, 2158, 295, 264, 4470, 13, 441, 307, 586, 257, 7006, 11, 597, 307, 13745, 577, 309, 307, 15598, 13], "temperature": 0.0, "avg_logprob": -0.3239747661433808, "compression_ratio": 1.5696969696969696, "no_speech_prob": 7.483181252609938e-05}, {"id": 691, "seek": 520600, "start": 5218.0, "end": 5224.0, "text": " L.backward calculates all of the gradients.", "tokens": [441, 13, 3207, 1007, 4322, 1024, 439, 295, 264, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.3239747661433808, "compression_ratio": 1.5696969696969696, "no_speech_prob": 7.483181252609938e-05}, {"id": 692, "seek": 520600, "start": 5224.0, "end": 5227.0, "text": " And so now that we have the gradients. Ah, OK.", "tokens": [400, 370, 586, 300, 321, 362, 264, 2771, 2448, 13, 2438, 11, 2264, 13], "temperature": 0.0, "avg_logprob": -0.3239747661433808, "compression_ratio": 1.5696969696969696, "no_speech_prob": 7.483181252609938e-05}, {"id": 693, "seek": 522700, "start": 5227.0, "end": 5246.0, "text": " Yeah, Adam is stochastic, but it's a much fancier version.", "tokens": [865, 11, 7938, 307, 342, 8997, 2750, 11, 457, 309, 311, 257, 709, 3429, 27674, 3037, 13], "temperature": 0.0, "avg_logprob": -0.10789881196132926, "compression_ratio": 1.275, "no_speech_prob": 3.3396534036000958e-06}, {"id": 694, "seek": 522700, "start": 5246.0, "end": 5251.0, "text": " And there is a version of Adam in that Excel spreadsheet if you wanted to look at the details.", "tokens": [400, 456, 307, 257, 3037, 295, 7938, 294, 300, 19060, 27733, 498, 291, 1415, 281, 574, 412, 264, 4365, 13], "temperature": 0.0, "avg_logprob": -0.10789881196132926, "compression_ratio": 1.275, "no_speech_prob": 3.3396534036000958e-06}, {"id": 695, "seek": 525100, "start": 5251.0, "end": 5260.0, "text": " And again, we're going to print out the report every every hundred steps.", "tokens": [400, 797, 11, 321, 434, 516, 281, 4482, 484, 264, 2275, 633, 633, 3262, 4439, 13], "temperature": 0.0, "avg_logprob": -0.11045900031701843, "compression_ratio": 1.688235294117647, "no_speech_prob": 3.2375794489780674e-06}, {"id": 696, "seek": 525100, "start": 5260.0, "end": 5265.0, "text": " The other thing we're doing is every hundred steps were decreasing our learning rate.", "tokens": [440, 661, 551, 321, 434, 884, 307, 633, 3262, 4439, 645, 23223, 527, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.11045900031701843, "compression_ratio": 1.688235294117647, "no_speech_prob": 3.2375794489780674e-06}, {"id": 697, "seek": 525100, "start": 5265.0, "end": 5272.0, "text": " This is called learning rate annealing. But you see inside this nested loop, we're doing learning rate times equals 90 percent.", "tokens": [639, 307, 1219, 2539, 3314, 22256, 4270, 13, 583, 291, 536, 1854, 341, 15646, 292, 6367, 11, 321, 434, 884, 2539, 3314, 1413, 6915, 4289, 3043, 13], "temperature": 0.0, "avg_logprob": -0.11045900031701843, "compression_ratio": 1.688235294117647, "no_speech_prob": 3.2375794489780674e-06}, {"id": 698, "seek": 527200, "start": 5272.0, "end": 5288.0, "text": " So it's getting a little bit smaller each each hundred steps.", "tokens": [407, 309, 311, 1242, 257, 707, 857, 4356, 1184, 1184, 3262, 4439, 13], "temperature": 0.0, "avg_logprob": -0.08822136339933975, "compression_ratio": 1.330708661417323, "no_speech_prob": 9.570604788677883e-07}, {"id": 699, "seek": 527200, "start": 5288.0, "end": 5295.0, "text": " And so, again, you want to calculate what your loss is, get the gradients and then use that to take a step.", "tokens": [400, 370, 11, 797, 11, 291, 528, 281, 8873, 437, 428, 4470, 307, 11, 483, 264, 2771, 2448, 293, 550, 764, 300, 281, 747, 257, 1823, 13], "temperature": 0.0, "avg_logprob": -0.08822136339933975, "compression_ratio": 1.330708661417323, "no_speech_prob": 9.570604788677883e-07}, {"id": 700, "seek": 529500, "start": 5295.0, "end": 5307.0, "text": " But note that these backward and step are kind of built in methods you're getting from PyTorch.", "tokens": [583, 3637, 300, 613, 23897, 293, 1823, 366, 733, 295, 3094, 294, 7150, 291, 434, 1242, 490, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.15038827680191905, "compression_ratio": 1.2875816993464053, "no_speech_prob": 4.222663392283721e-06}, {"id": 701, "seek": 529500, "start": 5307.0, "end": 5315.0, "text": " We can check and we still have great topics. So space, NASA, shuttle, launch orbit, lunar, moon data.", "tokens": [492, 393, 1520, 293, 321, 920, 362, 869, 8378, 13, 407, 1901, 11, 12077, 11, 26728, 11, 4025, 13991, 11, 32581, 11, 7135, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15038827680191905, "compression_ratio": 1.2875816993464053, "no_speech_prob": 4.222663392283721e-06}, {"id": 702, "seek": 531500, "start": 5315.0, "end": 5342.0, "text": " Well, we do have this one one random topic that we've been seeing. We saw with SVD as well. Yes.", "tokens": [1042, 11, 321, 360, 362, 341, 472, 472, 4974, 4829, 300, 321, 600, 668, 2577, 13, 492, 1866, 365, 31910, 35, 382, 731, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.151436185836792, "compression_ratio": 1.0786516853932584, "no_speech_prob": 0.00010383967310190201}, {"id": 703, "seek": 534200, "start": 5342.0, "end": 5354.0, "text": " So with with. So the idea of decreasing it as you go is that you're getting kind of as you get more in the neighborhood of where you want to be.", "tokens": [407, 365, 365, 13, 407, 264, 1558, 295, 23223, 309, 382, 291, 352, 307, 300, 291, 434, 1242, 733, 295, 382, 291, 483, 544, 294, 264, 7630, 295, 689, 291, 528, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.07803833484649658, "compression_ratio": 1.6778846153846154, "no_speech_prob": 9.366429367219098e-06}, {"id": 704, "seek": 534200, "start": 5354.0, "end": 5360.0, "text": " You want to take smaller steps so you don't overstep where you're going with stochastic gradient descent.", "tokens": [509, 528, 281, 747, 4356, 4439, 370, 291, 500, 380, 670, 16792, 689, 291, 434, 516, 365, 342, 8997, 2750, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.07803833484649658, "compression_ratio": 1.6778846153846154, "no_speech_prob": 9.366429367219098e-06}, {"id": 705, "seek": 534200, "start": 5360.0, "end": 5367.0, "text": " Remember that your your directions are less accurate since you've just used a subset of your data.", "tokens": [5459, 300, 428, 428, 11095, 366, 1570, 8559, 1670, 291, 600, 445, 1143, 257, 25993, 295, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.07803833484649658, "compression_ratio": 1.6778846153846154, "no_speech_prob": 9.366429367219098e-06}, {"id": 706, "seek": 536700, "start": 5367.0, "end": 5372.0, "text": " So there's kind of more of a risk of overstepping in the wrong direction.", "tokens": [407, 456, 311, 733, 295, 544, 295, 257, 3148, 295, 670, 2941, 3759, 294, 264, 2085, 3513, 13], "temperature": 0.0, "avg_logprob": -0.11830796733979256, "compression_ratio": 1.4675324675324675, "no_speech_prob": 2.6423811050335644e-06}, {"id": 707, "seek": 536700, "start": 5372.0, "end": 5377.0, "text": " And it's OK that you know you're you're always going in slightly wrong direction, but it's close enough.", "tokens": [400, 309, 311, 2264, 300, 291, 458, 291, 434, 291, 434, 1009, 516, 294, 4748, 2085, 3513, 11, 457, 309, 311, 1998, 1547, 13], "temperature": 0.0, "avg_logprob": -0.11830796733979256, "compression_ratio": 1.4675324675324675, "no_speech_prob": 2.6423811050335644e-06}, {"id": 708, "seek": 536700, "start": 5377.0, "end": 5392.0, "text": " Actually, let me. Yes, learning rate annealing.", "tokens": [5135, 11, 718, 385, 13, 1079, 11, 2539, 3314, 22256, 4270, 13], "temperature": 0.0, "avg_logprob": -0.11830796733979256, "compression_ratio": 1.4675324675324675, "no_speech_prob": 2.6423811050335644e-06}, {"id": 709, "seek": 539200, "start": 5392.0, "end": 5399.0, "text": " Clear the page. I was just going to show that.", "tokens": [14993, 264, 3028, 13, 286, 390, 445, 516, 281, 855, 300, 13], "temperature": 0.0, "avg_logprob": -0.09569684002134535, "compression_ratio": 1.5470588235294118, "no_speech_prob": 1.0348032901674742e-06}, {"id": 710, "seek": 539200, "start": 5399.0, "end": 5406.0, "text": " Like if this was the true direction of that, you want it to be going in with stochastic gradient descent.", "tokens": [1743, 498, 341, 390, 264, 2074, 3513, 295, 300, 11, 291, 528, 309, 281, 312, 516, 294, 365, 342, 8997, 2750, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.09569684002134535, "compression_ratio": 1.5470588235294118, "no_speech_prob": 1.0348032901674742e-06}, {"id": 711, "seek": 539200, "start": 5406.0, "end": 5414.0, "text": " And what you're doing. Is you're going close to to that direction, but you're kind of zigzagging a bit around.", "tokens": [400, 437, 291, 434, 884, 13, 1119, 291, 434, 516, 1998, 281, 281, 300, 3513, 11, 457, 291, 434, 733, 295, 38290, 43886, 3249, 257, 857, 926, 13], "temperature": 0.0, "avg_logprob": -0.09569684002134535, "compression_ratio": 1.5470588235294118, "no_speech_prob": 1.0348032901674742e-06}, {"id": 712, "seek": 541400, "start": 5414.0, "end": 5425.0, "text": " And so you probably want your zigzags to get like a little bit. And a tighter here the as you start getting close to your goal.", "tokens": [400, 370, 291, 1391, 528, 428, 38290, 89, 12109, 281, 483, 411, 257, 707, 857, 13, 400, 257, 30443, 510, 264, 382, 291, 722, 1242, 1998, 281, 428, 3387, 13], "temperature": 0.0, "avg_logprob": -0.2181200408935547, "compression_ratio": 1.3602941176470589, "no_speech_prob": 8.76412329375853e-08}, {"id": 713, "seek": 541400, "start": 5425.0, "end": 5440.0, "text": " Yeah, and as Jeremy said, that's learning rate annealing.", "tokens": [865, 11, 293, 382, 17809, 848, 11, 300, 311, 2539, 3314, 22256, 4270, 13], "temperature": 0.0, "avg_logprob": -0.2181200408935547, "compression_ratio": 1.3602941176470589, "no_speech_prob": 8.76412329375853e-08}, {"id": 714, "seek": 544000, "start": 5440.0, "end": 5444.0, "text": " So now I want to kind of compare some of the approaches we've taken to talk about.", "tokens": [407, 586, 286, 528, 281, 733, 295, 6794, 512, 295, 264, 11587, 321, 600, 2726, 281, 751, 466, 13], "temperature": 0.0, "avg_logprob": -0.20409003037672777, "compression_ratio": 1.4207650273224044, "no_speech_prob": 2.9478692340489943e-06}, {"id": 715, "seek": 544000, "start": 5444.0, "end": 5457.0, "text": " Or first, are there any other questions on on pie torch or kind of what we've done with this automatic differentiation?", "tokens": [1610, 700, 11, 366, 456, 604, 661, 1651, 322, 322, 1730, 27822, 420, 733, 295, 437, 321, 600, 1096, 365, 341, 12509, 38902, 30], "temperature": 0.0, "avg_logprob": -0.20409003037672777, "compression_ratio": 1.4207650273224044, "no_speech_prob": 2.9478692340489943e-06}, {"id": 716, "seek": 544000, "start": 5457.0, "end": 5464.0, "text": " OK, so using psych hits like it learns built in and math.", "tokens": [2264, 11, 370, 1228, 4681, 8664, 411, 309, 27152, 3094, 294, 293, 5221, 13], "temperature": 0.0, "avg_logprob": -0.20409003037672777, "compression_ratio": 1.4207650273224044, "no_speech_prob": 2.9478692340489943e-06}, {"id": 717, "seek": 546400, "start": 5464.0, "end": 5473.0, "text": " And that was fast. We didn't have to deal with tuning parameters, which was nice. So, you know, in these later ones, there was this question of do we need to adjust our learning rate?", "tokens": [400, 300, 390, 2370, 13, 492, 994, 380, 362, 281, 2028, 365, 15164, 9834, 11, 597, 390, 1481, 13, 407, 11, 291, 458, 11, 294, 613, 1780, 2306, 11, 456, 390, 341, 1168, 295, 360, 321, 643, 281, 4369, 527, 2539, 3314, 30], "temperature": 0.0, "avg_logprob": -0.11163663316047055, "compression_ratio": 1.5656108597285068, "no_speech_prob": 1.8737314348982181e-06}, {"id": 718, "seek": 546400, "start": 5473.0, "end": 5478.0, "text": " Do we need to adjust kind of how much of a penalty we're putting on the negatives?", "tokens": [1144, 321, 643, 281, 4369, 733, 295, 577, 709, 295, 257, 16263, 321, 434, 3372, 322, 264, 40019, 30], "temperature": 0.0, "avg_logprob": -0.11163663316047055, "compression_ratio": 1.5656108597285068, "no_speech_prob": 1.8737314348982181e-06}, {"id": 719, "seek": 546400, "start": 5478.0, "end": 5484.0, "text": " But it used a lot of academic research, kind of the people that implemented it.", "tokens": [583, 309, 1143, 257, 688, 295, 7778, 2132, 11, 733, 295, 264, 561, 300, 12270, 309, 13], "temperature": 0.0, "avg_logprob": -0.11163663316047055, "compression_ratio": 1.5656108597285068, "no_speech_prob": 1.8737314348982181e-06}, {"id": 720, "seek": 548400, "start": 5484.0, "end": 5494.0, "text": " And this is interesting. This is from another library.", "tokens": [400, 341, 307, 1880, 13, 639, 307, 490, 1071, 6405, 13], "temperature": 0.0, "avg_logprob": -0.23148906014182352, "compression_ratio": 1.3803680981595092, "no_speech_prob": 3.011442970546341e-07}, {"id": 721, "seek": 548400, "start": 5494.0, "end": 5498.0, "text": " Specifically, a Python library for non negative matrix factorization.", "tokens": [26058, 11, 257, 15329, 6405, 337, 2107, 3671, 8141, 5952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.23148906014182352, "compression_ratio": 1.3803680981595092, "no_speech_prob": 3.011442970546341e-07}, {"id": 722, "seek": 548400, "start": 5498.0, "end": 5507.0, "text": " But it's really kind of neat. They show a list of like variations on NMF and like relative research.", "tokens": [583, 309, 311, 534, 733, 295, 10654, 13, 814, 855, 257, 1329, 295, 411, 17840, 322, 426, 44, 37, 293, 411, 4972, 2132, 13], "temperature": 0.0, "avg_logprob": -0.23148906014182352, "compression_ratio": 1.3803680981595092, "no_speech_prob": 3.011442970546341e-07}, {"id": 723, "seek": 550700, "start": 5507.0, "end": 5514.0, "text": " And this is still an area where a lot of research is happening and there's a lot going on. And so.", "tokens": [400, 341, 307, 920, 364, 1859, 689, 257, 688, 295, 2132, 307, 2737, 293, 456, 311, 257, 688, 516, 322, 13, 400, 370, 13], "temperature": 0.0, "avg_logprob": -0.10904005595615932, "compression_ratio": 1.5420168067226891, "no_speech_prob": 5.626015990856104e-07}, {"id": 724, "seek": 550700, "start": 5514.0, "end": 5523.0, "text": " Unless you want to specialize in NMF, this may be more detailed than you would want to go into to be able to build something like this.", "tokens": [16581, 291, 528, 281, 37938, 294, 426, 44, 37, 11, 341, 815, 312, 544, 9942, 813, 291, 576, 528, 281, 352, 666, 281, 312, 1075, 281, 1322, 746, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.10904005595615932, "compression_ratio": 1.5420168067226891, "no_speech_prob": 5.626015990856104e-07}, {"id": 725, "seek": 550700, "start": 5523.0, "end": 5529.0, "text": " Using pie towards an SDD, it was nice that it was much quicker to implement.", "tokens": [11142, 1730, 3030, 364, 14638, 35, 11, 309, 390, 1481, 300, 309, 390, 709, 16255, 281, 4445, 13], "temperature": 0.0, "avg_logprob": -0.10904005595615932, "compression_ratio": 1.5420168067226891, "no_speech_prob": 5.626015990856104e-07}, {"id": 726, "seek": 550700, "start": 5529.0, "end": 5535.0, "text": " We didn't have to be kind of particular experts on NMF.", "tokens": [492, 994, 380, 362, 281, 312, 733, 295, 1729, 8572, 322, 426, 44, 37, 13], "temperature": 0.0, "avg_logprob": -0.10904005595615932, "compression_ratio": 1.5420168067226891, "no_speech_prob": 5.626015990856104e-07}, {"id": 727, "seek": 553500, "start": 5535.0, "end": 5544.0, "text": " The parameters were kind of fiddly and it was also not as fast as scikit learns built in one.", "tokens": [440, 9834, 645, 733, 295, 283, 14273, 356, 293, 309, 390, 611, 406, 382, 2370, 382, 2180, 22681, 27152, 3094, 294, 472, 13], "temperature": 0.0, "avg_logprob": -0.18853641086154513, "compression_ratio": 1.3166666666666667, "no_speech_prob": 1.8738096514425706e-06}, {"id": 728, "seek": 553500, "start": 5544.0, "end": 5551.0, "text": " But we did see that kind of going from numpy on this should say.", "tokens": [583, 321, 630, 536, 300, 733, 295, 516, 490, 1031, 8200, 322, 341, 820, 584, 13], "temperature": 0.0, "avg_logprob": -0.18853641086154513, "compression_ratio": 1.3166666666666667, "no_speech_prob": 1.8738096514425706e-06}, {"id": 729, "seek": 555100, "start": 5551.0, "end": 5566.0, "text": " Num by. Was slow enough that we wanted to switch to pie torch to get the improvement on the GPO.", "tokens": [22592, 538, 13, 3027, 2964, 1547, 300, 321, 1415, 281, 3679, 281, 1730, 27822, 281, 483, 264, 10444, 322, 264, 26039, 46, 13], "temperature": 0.0, "avg_logprob": -0.2576565886988784, "compression_ratio": 1.1568627450980393, "no_speech_prob": 4.93669904244598e-06}, {"id": 730, "seek": 555100, "start": 5566.0, "end": 5576.0, "text": " Questions about this?", "tokens": [27738, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.2576565886988784, "compression_ratio": 1.1568627450980393, "no_speech_prob": 4.93669904244598e-06}, {"id": 731, "seek": 557600, "start": 5576.0, "end": 5584.0, "text": " Oh, yes. Actually, hold on a moment.", "tokens": [876, 11, 2086, 13, 5135, 11, 1797, 322, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.4636243399927172, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.00010385247151134536}, {"id": 732, "seek": 557600, "start": 5584.0, "end": 5593.0, "text": " So in pie torch, looking at the loss function you called subtracted your variables.", "tokens": [407, 294, 1730, 27822, 11, 1237, 412, 264, 4470, 2445, 291, 1219, 16390, 292, 428, 9102, 13], "temperature": 0.0, "avg_logprob": -0.4636243399927172, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.00010385247151134536}, {"id": 733, "seek": 557600, "start": 5593.0, "end": 5603.0, "text": " And so by actually just like doing the operation on variables, is that what is like calculating the gradient on that?", "tokens": [400, 370, 538, 767, 445, 411, 884, 264, 6916, 322, 9102, 11, 307, 300, 437, 307, 411, 28258, 264, 16235, 322, 300, 30], "temperature": 0.0, "avg_logprob": -0.4636243399927172, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.00010385247151134536}, {"id": 734, "seek": 560300, "start": 5603.0, "end": 5609.0, "text": " And then we just call step or backwards and then it gives us the gradient.", "tokens": [400, 550, 321, 445, 818, 1823, 420, 12204, 293, 550, 309, 2709, 505, 264, 16235, 13], "temperature": 0.0, "avg_logprob": -0.2349115935238925, "compression_ratio": 1.3391304347826087, "no_speech_prob": 8.7476619228255e-05}, {"id": 735, "seek": 560300, "start": 5609.0, "end": 5614.0, "text": " So yes, a backward is where it's calculating the gradient.", "tokens": [407, 2086, 11, 257, 23897, 307, 689, 309, 311, 28258, 264, 16235, 13], "temperature": 0.0, "avg_logprob": -0.2349115935238925, "compression_ratio": 1.3391304347826087, "no_speech_prob": 8.7476619228255e-05}, {"id": 736, "seek": 560300, "start": 5614.0, "end": 5617.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2349115935238925, "compression_ratio": 1.3391304347826087, "no_speech_prob": 8.7476619228255e-05}, {"id": 737, "seek": 560300, "start": 5617.0, "end": 5628.0, "text": " Good question.", "tokens": [2205, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2349115935238925, "compression_ratio": 1.3391304347826087, "no_speech_prob": 8.7476619228255e-05}, {"id": 738, "seek": 562800, "start": 5628.0, "end": 5633.0, "text": " Okay, let's see.", "tokens": [1033, 11, 718, 311, 536, 13], "temperature": 0.0, "avg_logprob": -0.1390229204426641, "compression_ratio": 1.592964824120603, "no_speech_prob": 4.005889059044421e-05}, {"id": 739, "seek": 562800, "start": 5633.0, "end": 5640.0, "text": " I'll start the next section, although we'll have to revisit this next time.", "tokens": [286, 603, 722, 264, 958, 3541, 11, 4878, 321, 603, 362, 281, 32676, 341, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.1390229204426641, "compression_ratio": 1.592964824120603, "no_speech_prob": 4.005889059044421e-05}, {"id": 740, "seek": 562800, "start": 5640.0, "end": 5645.0, "text": " Yeah, so that was that that was NMF. Now we're going to return to SVD.", "tokens": [865, 11, 370, 300, 390, 300, 300, 390, 426, 44, 37, 13, 823, 321, 434, 516, 281, 2736, 281, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.1390229204426641, "compression_ratio": 1.592964824120603, "no_speech_prob": 4.005889059044421e-05}, {"id": 741, "seek": 562800, "start": 5645.0, "end": 5655.0, "text": " So we saw that something that we were doing in NMF that we weren't doing before with SVD is we were choosing D kind of a number of topics that we wanted.", "tokens": [407, 321, 1866, 300, 746, 300, 321, 645, 884, 294, 426, 44, 37, 300, 321, 4999, 380, 884, 949, 365, 31910, 35, 307, 321, 645, 10875, 413, 733, 295, 257, 1230, 295, 8378, 300, 321, 1415, 13], "temperature": 0.0, "avg_logprob": -0.1390229204426641, "compression_ratio": 1.592964824120603, "no_speech_prob": 4.005889059044421e-05}, {"id": 742, "seek": 565500, "start": 5655.0, "end": 5658.0, "text": " And so going back to these matrices.", "tokens": [400, 370, 516, 646, 281, 613, 32284, 13], "temperature": 0.0, "avg_logprob": -0.1500563737822742, "compression_ratio": 1.1926605504587156, "no_speech_prob": 7.4103659244428854e-06}, {"id": 743, "seek": 565500, "start": 5658.0, "end": 5664.0, "text": " So we had this matrix that is words by documents.", "tokens": [407, 321, 632, 341, 8141, 300, 307, 2283, 538, 8512, 13], "temperature": 0.0, "avg_logprob": -0.1500563737822742, "compression_ratio": 1.1926605504587156, "no_speech_prob": 7.4103659244428854e-06}, {"id": 744, "seek": 565500, "start": 5664.0, "end": 5679.0, "text": " Actually, let me go back up to the NMF one.", "tokens": [5135, 11, 718, 385, 352, 646, 493, 281, 264, 426, 44, 37, 472, 13], "temperature": 0.0, "avg_logprob": -0.1500563737822742, "compression_ratio": 1.1926605504587156, "no_speech_prob": 7.4103659244428854e-06}, {"id": 745, "seek": 567900, "start": 5679.0, "end": 5686.0, "text": " The picture that is up here.", "tokens": [440, 3036, 300, 307, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.1258438402606595, "compression_ratio": 1.5533333333333332, "no_speech_prob": 2.123307695001131e-06}, {"id": 746, "seek": 567900, "start": 5686.0, "end": 5690.0, "text": " Okay, this picture. So words by documents.", "tokens": [1033, 11, 341, 3036, 13, 407, 2283, 538, 8512, 13], "temperature": 0.0, "avg_logprob": -0.1258438402606595, "compression_ratio": 1.5533333333333332, "no_speech_prob": 2.123307695001131e-06}, {"id": 747, "seek": 567900, "start": 5690.0, "end": 5694.0, "text": " Then we're getting a matrix that's words by topics.", "tokens": [1396, 321, 434, 1242, 257, 8141, 300, 311, 2283, 538, 8378, 13], "temperature": 0.0, "avg_logprob": -0.1258438402606595, "compression_ratio": 1.5533333333333332, "no_speech_prob": 2.123307695001131e-06}, {"id": 748, "seek": 567900, "start": 5694.0, "end": 5699.0, "text": " And then this is topics by the importance indicators for each document.", "tokens": [400, 550, 341, 307, 8378, 538, 264, 7379, 22176, 337, 1184, 4166, 13], "temperature": 0.0, "avg_logprob": -0.1258438402606595, "compression_ratio": 1.5533333333333332, "no_speech_prob": 2.123307695001131e-06}, {"id": 749, "seek": 567900, "start": 5699.0, "end": 5702.0, "text": " So it's basically topics by document.", "tokens": [407, 309, 311, 1936, 8378, 538, 4166, 13], "temperature": 0.0, "avg_logprob": -0.1258438402606595, "compression_ratio": 1.5533333333333332, "no_speech_prob": 2.123307695001131e-06}, {"id": 750, "seek": 570200, "start": 5702.0, "end": 5709.0, "text": " And we were getting to choose how many topics there were.", "tokens": [400, 321, 645, 1242, 281, 2826, 577, 867, 8378, 456, 645, 13], "temperature": 0.0, "avg_logprob": -0.06302371866562788, "compression_ratio": 1.515625, "no_speech_prob": 3.6118124171480304e-06}, {"id": 751, "seek": 570200, "start": 5709.0, "end": 5712.0, "text": " And so we can get that with SVD as well.", "tokens": [400, 370, 321, 393, 483, 300, 365, 31910, 35, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.06302371866562788, "compression_ratio": 1.515625, "no_speech_prob": 3.6118124171480304e-06}, {"id": 752, "seek": 570200, "start": 5712.0, "end": 5714.0, "text": " And it's called truncated SVD.", "tokens": [400, 309, 311, 1219, 504, 409, 66, 770, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.06302371866562788, "compression_ratio": 1.515625, "no_speech_prob": 3.6118124171480304e-06}, {"id": 753, "seek": 570200, "start": 5714.0, "end": 5721.0, "text": " So instead of getting this exact decomposition, we could just choose.", "tokens": [407, 2602, 295, 1242, 341, 1900, 48356, 11, 321, 727, 445, 2826, 13], "temperature": 0.0, "avg_logprob": -0.06302371866562788, "compression_ratio": 1.515625, "no_speech_prob": 3.6118124171480304e-06}, {"id": 754, "seek": 570200, "start": 5721.0, "end": 5723.0, "text": " I'm going to have R smaller than N.", "tokens": [286, 478, 516, 281, 362, 497, 4356, 813, 426, 13], "temperature": 0.0, "avg_logprob": -0.06302371866562788, "compression_ratio": 1.515625, "no_speech_prob": 3.6118124171480304e-06}, {"id": 755, "seek": 570200, "start": 5723.0, "end": 5729.0, "text": " And that's the number of topics that I'm interested in.", "tokens": [400, 300, 311, 264, 1230, 295, 8378, 300, 286, 478, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.06302371866562788, "compression_ratio": 1.515625, "no_speech_prob": 3.6118124171480304e-06}, {"id": 756, "seek": 572900, "start": 5729.0, "end": 5736.0, "text": " And remember that the singular values, which are the diagonal values in this matrix,", "tokens": [400, 1604, 300, 264, 20010, 4190, 11, 597, 366, 264, 21539, 4190, 294, 341, 8141, 11], "temperature": 0.0, "avg_logprob": -0.04018356204032898, "compression_ratio": 1.6051282051282052, "no_speech_prob": 8.397912097279914e-06}, {"id": 757, "seek": 572900, "start": 5736.0, "end": 5740.0, "text": " sigma in the middle, that those are ordered from largest to smallest.", "tokens": [12771, 294, 264, 2808, 11, 300, 729, 366, 8866, 490, 6443, 281, 16998, 13], "temperature": 0.0, "avg_logprob": -0.04018356204032898, "compression_ratio": 1.6051282051282052, "no_speech_prob": 8.397912097279914e-06}, {"id": 758, "seek": 572900, "start": 5740.0, "end": 5747.0, "text": " And the larger ones kind of are contributing a lot more to this reconstruction of A.", "tokens": [400, 264, 4833, 2306, 733, 295, 366, 19270, 257, 688, 544, 281, 341, 31565, 295, 316, 13], "temperature": 0.0, "avg_logprob": -0.04018356204032898, "compression_ratio": 1.6051282051282052, "no_speech_prob": 8.397912097279914e-06}, {"id": 759, "seek": 572900, "start": 5747.0, "end": 5750.0, "text": " So we're going to keep the larger ones.", "tokens": [407, 321, 434, 516, 281, 1066, 264, 4833, 2306, 13], "temperature": 0.0, "avg_logprob": -0.04018356204032898, "compression_ratio": 1.6051282051282052, "no_speech_prob": 8.397912097279914e-06}, {"id": 760, "seek": 572900, "start": 5750.0, "end": 5754.0, "text": " And this is called truncated SVD.", "tokens": [400, 341, 307, 1219, 504, 409, 66, 770, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.04018356204032898, "compression_ratio": 1.6051282051282052, "no_speech_prob": 8.397912097279914e-06}, {"id": 761, "seek": 575400, "start": 5754.0, "end": 5764.0, "text": " And then I also want to, this picture I've taken is from this blog post on the Facebook research page", "tokens": [400, 550, 286, 611, 528, 281, 11, 341, 3036, 286, 600, 2726, 307, 490, 341, 6968, 2183, 322, 264, 4384, 2132, 3028], "temperature": 0.0, "avg_logprob": -0.09816483962230194, "compression_ratio": 1.47196261682243, "no_speech_prob": 6.74788770993473e-06}, {"id": 762, "seek": 575400, "start": 5764.0, "end": 5766.0, "text": " on fast randomized SVD.", "tokens": [322, 2370, 38513, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.09816483962230194, "compression_ratio": 1.47196261682243, "no_speech_prob": 6.74788770993473e-06}, {"id": 763, "seek": 575400, "start": 5766.0, "end": 5775.0, "text": " And we're going to be talking about some of the ideas in it below, but I encourage you to check that out.", "tokens": [400, 321, 434, 516, 281, 312, 1417, 466, 512, 295, 264, 3487, 294, 309, 2507, 11, 457, 286, 5373, 291, 281, 1520, 300, 484, 13], "temperature": 0.0, "avg_logprob": -0.09816483962230194, "compression_ratio": 1.47196261682243, "no_speech_prob": 6.74788770993473e-06}, {"id": 764, "seek": 575400, "start": 5775.0, "end": 5783.0, "text": " So some of the shortcomings for classical algorithms are matrices are really large.", "tokens": [407, 512, 295, 264, 2099, 49886, 337, 13735, 14642, 366, 32284, 366, 534, 2416, 13], "temperature": 0.0, "avg_logprob": -0.09816483962230194, "compression_ratio": 1.47196261682243, "no_speech_prob": 6.74788770993473e-06}, {"id": 765, "seek": 578300, "start": 5783.0, "end": 5788.0, "text": " Data is often missing or slightly inaccurate.", "tokens": [11888, 307, 2049, 5361, 420, 4748, 46443, 13], "temperature": 0.0, "avg_logprob": -0.08360446294148763, "compression_ratio": 1.5219512195121951, "no_speech_prob": 8.800775503914338e-06}, {"id": 766, "seek": 578300, "start": 5788.0, "end": 5797.0, "text": " Why spend extra computational resources when kind of, if your data was not that precise to begin with,", "tokens": [1545, 3496, 2857, 28270, 3593, 562, 733, 295, 11, 498, 428, 1412, 390, 406, 300, 13600, 281, 1841, 365, 11], "temperature": 0.0, "avg_logprob": -0.08360446294148763, "compression_ratio": 1.5219512195121951, "no_speech_prob": 8.800775503914338e-06}, {"id": 767, "seek": 578300, "start": 5797.0, "end": 5807.0, "text": " you don't need an exact solution because it's not a, you know, you have these errors in your input.", "tokens": [291, 500, 380, 643, 364, 1900, 3827, 570, 309, 311, 406, 257, 11, 291, 458, 11, 291, 362, 613, 13603, 294, 428, 4846, 13], "temperature": 0.0, "avg_logprob": -0.08360446294148763, "compression_ratio": 1.5219512195121951, "no_speech_prob": 8.800775503914338e-06}, {"id": 768, "seek": 578300, "start": 5807.0, "end": 5811.0, "text": " Data transfer now plays a major role in the time of algorithms.", "tokens": [11888, 5003, 586, 5749, 257, 2563, 3090, 294, 264, 565, 295, 14642, 13], "temperature": 0.0, "avg_logprob": -0.08360446294148763, "compression_ratio": 1.5219512195121951, "no_speech_prob": 8.800775503914338e-06}, {"id": 769, "seek": 581100, "start": 5811.0, "end": 5820.0, "text": " So we talked about this last time that traditionally this idea of computational complexity or big O has been how speed is measured.", "tokens": [407, 321, 2825, 466, 341, 1036, 565, 300, 19067, 341, 1558, 295, 28270, 14024, 420, 955, 422, 575, 668, 577, 3073, 307, 12690, 13], "temperature": 0.0, "avg_logprob": -0.0845330938508239, "compression_ratio": 1.5401785714285714, "no_speech_prob": 6.240391030587489e-06}, {"id": 770, "seek": 581100, "start": 5820.0, "end": 5822.0, "text": " And it's still definitely an important concept.", "tokens": [400, 309, 311, 920, 2138, 364, 1021, 3410, 13], "temperature": 0.0, "avg_logprob": -0.0845330938508239, "compression_ratio": 1.5401785714285714, "no_speech_prob": 6.240391030587489e-06}, {"id": 771, "seek": 581100, "start": 5822.0, "end": 5833.0, "text": " But in practice, a lot of time is spent taking things kind of from disk or even from RAM into your cache or registers.", "tokens": [583, 294, 3124, 11, 257, 688, 295, 565, 307, 4418, 1940, 721, 733, 295, 490, 12355, 420, 754, 490, 14561, 666, 428, 19459, 420, 38351, 13], "temperature": 0.0, "avg_logprob": -0.0845330938508239, "compression_ratio": 1.5401785714285714, "no_speech_prob": 6.240391030587489e-06}, {"id": 772, "seek": 581100, "start": 5833.0, "end": 5835.0, "text": " And you really want to take advantage of that.", "tokens": [400, 291, 534, 528, 281, 747, 5002, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.0845330938508239, "compression_ratio": 1.5401785714285714, "no_speech_prob": 6.240391030587489e-06}, {"id": 773, "seek": 583500, "start": 5835.0, "end": 5842.0, "text": " And so techniques that require fewer passes over the data can be substantially faster,", "tokens": [400, 370, 7512, 300, 3651, 13366, 11335, 670, 264, 1412, 393, 312, 30797, 4663, 11], "temperature": 0.0, "avg_logprob": -0.05161277453104655, "compression_ratio": 1.5360360360360361, "no_speech_prob": 3.726393288161489e-06}, {"id": 774, "seek": 583500, "start": 5842.0, "end": 5847.0, "text": " even if they technically have more steps in them.", "tokens": [754, 498, 436, 12120, 362, 544, 4439, 294, 552, 13], "temperature": 0.0, "avg_logprob": -0.05161277453104655, "compression_ratio": 1.5360360360360361, "no_speech_prob": 3.726393288161489e-06}, {"id": 775, "seek": 583500, "start": 5847.0, "end": 5851.0, "text": " We also want to be able to take advantage of GPUs.", "tokens": [492, 611, 528, 281, 312, 1075, 281, 747, 5002, 295, 18407, 82, 13], "temperature": 0.0, "avg_logprob": -0.05161277453104655, "compression_ratio": 1.5360360360360361, "no_speech_prob": 3.726393288161489e-06}, {"id": 776, "seek": 583500, "start": 5851.0, "end": 5859.0, "text": " And then a lot of the methods that are used on sparse or structured matrices are unstable.", "tokens": [400, 550, 257, 688, 295, 264, 7150, 300, 366, 1143, 322, 637, 11668, 420, 18519, 32284, 366, 23742, 13], "temperature": 0.0, "avg_logprob": -0.05161277453104655, "compression_ratio": 1.5360360360360361, "no_speech_prob": 3.726393288161489e-06}, {"id": 777, "seek": 583500, "start": 5859.0, "end": 5864.0, "text": " And we'll kind of be getting to Krylov subspace methods later.", "tokens": [400, 321, 603, 733, 295, 312, 1242, 281, 37747, 28257, 2090, 17940, 7150, 1780, 13], "temperature": 0.0, "avg_logprob": -0.05161277453104655, "compression_ratio": 1.5360360360360361, "no_speech_prob": 3.726393288161489e-06}, {"id": 778, "seek": 586400, "start": 5864.0, "end": 5872.0, "text": " But the computational cost ends up being kind of more about what you're doing to stabilize your algorithm", "tokens": [583, 264, 28270, 2063, 5314, 493, 885, 733, 295, 544, 466, 437, 291, 434, 884, 281, 31870, 428, 9284], "temperature": 0.0, "avg_logprob": -0.11474231437400535, "compression_ratio": 1.5868544600938967, "no_speech_prob": 2.521475380490301e-06}, {"id": 779, "seek": 586400, "start": 5872.0, "end": 5877.0, "text": " as opposed to kind of the actual algorithm you're doing itself.", "tokens": [382, 8851, 281, 733, 295, 264, 3539, 9284, 291, 434, 884, 2564, 13], "temperature": 0.0, "avg_logprob": -0.11474231437400535, "compression_ratio": 1.5868544600938967, "no_speech_prob": 2.521475380490301e-06}, {"id": 780, "seek": 586400, "start": 5877.0, "end": 5885.0, "text": " And these, actually I'm going to highlight a paper that we will be seeing a fair amount of.", "tokens": [400, 613, 11, 767, 286, 478, 516, 281, 5078, 257, 3035, 300, 321, 486, 312, 2577, 257, 3143, 2372, 295, 13], "temperature": 0.0, "avg_logprob": -0.11474231437400535, "compression_ratio": 1.5868544600938967, "no_speech_prob": 2.521475380490301e-06}, {"id": 781, "seek": 586400, "start": 5885.0, "end": 5893.0, "text": " And that is a really nice paper is Halco, Finding Structure with Randomness.", "tokens": [400, 300, 307, 257, 534, 1481, 3035, 307, 13896, 1291, 11, 31947, 745, 2885, 365, 37603, 1287, 13], "temperature": 0.0, "avg_logprob": -0.11474231437400535, "compression_ratio": 1.5868544600938967, "no_speech_prob": 2.521475380490301e-06}, {"id": 782, "seek": 589300, "start": 5893.0, "end": 5898.0, "text": " And it's about kind of using probabilistic techniques.", "tokens": [400, 309, 311, 466, 733, 295, 1228, 31959, 3142, 7512, 13], "temperature": 0.0, "avg_logprob": -0.15310631376324277, "compression_ratio": 1.4764705882352942, "no_speech_prob": 5.0641050620470196e-05}, {"id": 783, "seek": 589300, "start": 5898.0, "end": 5903.0, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.15310631376324277, "compression_ratio": 1.4764705882352942, "no_speech_prob": 5.0641050620470196e-05}, {"id": 784, "seek": 589300, "start": 5903.0, "end": 5906.0, "text": " So you said sparse matrix operations are difficult.", "tokens": [407, 291, 848, 637, 11668, 8141, 7705, 366, 2252, 13], "temperature": 0.0, "avg_logprob": -0.15310631376324277, "compression_ratio": 1.4764705882352942, "no_speech_prob": 5.0641050620470196e-05}, {"id": 785, "seek": 589300, "start": 5906.0, "end": 5915.0, "text": " Is that because you have them in a sparse format or are you talking about a dense matrix that has sparse variables?", "tokens": [1119, 300, 570, 291, 362, 552, 294, 257, 637, 11668, 7877, 420, 366, 291, 1417, 466, 257, 18011, 8141, 300, 575, 637, 11668, 9102, 30], "temperature": 0.0, "avg_logprob": -0.15310631376324277, "compression_ratio": 1.4764705882352942, "no_speech_prob": 5.0641050620470196e-05}, {"id": 786, "seek": 589300, "start": 5915.0, "end": 5916.0, "text": " That's a good question.", "tokens": [663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.15310631376324277, "compression_ratio": 1.4764705882352942, "no_speech_prob": 5.0641050620470196e-05}, {"id": 787, "seek": 591600, "start": 5916.0, "end": 5927.0, "text": " So it's this particular family of methods, Krylov subspace methods, which are really useful.", "tokens": [407, 309, 311, 341, 1729, 1605, 295, 7150, 11, 37747, 28257, 2090, 17940, 7150, 11, 597, 366, 534, 4420, 13], "temperature": 0.0, "avg_logprob": -0.06759546915690104, "compression_ratio": 1.577319587628866, "no_speech_prob": 3.187471975252265e-06}, {"id": 788, "seek": 591600, "start": 5927.0, "end": 5936.0, "text": " Well, I guess, okay, so there are issues of when sparse things are kind of becoming dense.", "tokens": [1042, 11, 286, 2041, 11, 1392, 11, 370, 456, 366, 2663, 295, 562, 637, 11668, 721, 366, 733, 295, 5617, 18011, 13], "temperature": 0.0, "avg_logprob": -0.06759546915690104, "compression_ratio": 1.577319587628866, "no_speech_prob": 3.187471975252265e-06}, {"id": 789, "seek": 591600, "start": 5936.0, "end": 5943.0, "text": " I think this is less about the sparsity and kind of more about the methods that are being used introduce this instability.", "tokens": [286, 519, 341, 307, 1570, 466, 264, 637, 685, 507, 293, 733, 295, 544, 466, 264, 7150, 300, 366, 885, 1143, 5366, 341, 34379, 13], "temperature": 0.0, "avg_logprob": -0.06759546915690104, "compression_ratio": 1.577319587628866, "no_speech_prob": 3.187471975252265e-06}, {"id": 790, "seek": 594300, "start": 5943.0, "end": 5947.0, "text": " We'll see examples of Krylov subspace methods later.", "tokens": [492, 603, 536, 5110, 295, 37747, 28257, 2090, 17940, 7150, 1780, 13], "temperature": 0.0, "avg_logprob": -0.08369709532937886, "compression_ratio": 1.514018691588785, "no_speech_prob": 3.3404555779270595e-06}, {"id": 791, "seek": 594300, "start": 5947.0, "end": 5952.0, "text": " But yeah, it's not so much having to convert between and actually working.", "tokens": [583, 1338, 11, 309, 311, 406, 370, 709, 1419, 281, 7620, 1296, 293, 767, 1364, 13], "temperature": 0.0, "avg_logprob": -0.08369709532937886, "compression_ratio": 1.514018691588785, "no_speech_prob": 3.3404555779270595e-06}, {"id": 792, "seek": 594300, "start": 5952.0, "end": 5962.0, "text": " Well, depending what you're doing, working with sparse, there are kind of like implementations that handle sparse data format very efficiently.", "tokens": [1042, 11, 5413, 437, 291, 434, 884, 11, 1364, 365, 637, 11668, 11, 456, 366, 733, 295, 411, 4445, 763, 300, 4813, 637, 11668, 1412, 7877, 588, 19621, 13], "temperature": 0.0, "avg_logprob": -0.08369709532937886, "compression_ratio": 1.514018691588785, "no_speech_prob": 3.3404555779270595e-06}, {"id": 793, "seek": 594300, "start": 5962.0, "end": 5971.0, "text": " So it's not a problem to be storing things sparsely.", "tokens": [407, 309, 311, 406, 257, 1154, 281, 312, 26085, 721, 637, 685, 736, 13], "temperature": 0.0, "avg_logprob": -0.08369709532937886, "compression_ratio": 1.514018691588785, "no_speech_prob": 3.3404555779270595e-06}, {"id": 794, "seek": 597100, "start": 5971.0, "end": 5975.0, "text": " Yeah, so I just want to kind of introduce this idea of randomized algorithms.", "tokens": [865, 11, 370, 286, 445, 528, 281, 733, 295, 5366, 341, 1558, 295, 38513, 14642, 13], "temperature": 0.0, "avg_logprob": -0.10525311231613159, "compression_ratio": 1.608294930875576, "no_speech_prob": 2.1567364001384703e-06}, {"id": 795, "seek": 597100, "start": 5975.0, "end": 5985.0, "text": " Maybe as they're kind of more stable, their performance guarantees don't depend on kind of these matrix properties.", "tokens": [2704, 382, 436, 434, 733, 295, 544, 8351, 11, 641, 3389, 32567, 500, 380, 5672, 322, 733, 295, 613, 8141, 7221, 13], "temperature": 0.0, "avg_logprob": -0.10525311231613159, "compression_ratio": 1.608294930875576, "no_speech_prob": 2.1567364001384703e-06}, {"id": 796, "seek": 597100, "start": 5985.0, "end": 5994.0, "text": " So the spectral properties of a matrix are kind of based on what the singular values are.", "tokens": [407, 264, 42761, 7221, 295, 257, 8141, 366, 733, 295, 2361, 322, 437, 264, 20010, 4190, 366, 13], "temperature": 0.0, "avg_logprob": -0.10525311231613159, "compression_ratio": 1.608294930875576, "no_speech_prob": 2.1567364001384703e-06}, {"id": 797, "seek": 597100, "start": 5994.0, "end": 6000.0, "text": " And then a lot of matrix vector products can be done in parallel.", "tokens": [400, 550, 257, 688, 295, 8141, 8062, 3383, 393, 312, 1096, 294, 8952, 13], "temperature": 0.0, "avg_logprob": -0.10525311231613159, "compression_ratio": 1.608294930875576, "no_speech_prob": 2.1567364001384703e-06}, {"id": 798, "seek": 600000, "start": 6000.0, "end": 6013.0, "text": " And so we're about at time. I'll come back to these ideas on Tuesday.", "tokens": [400, 370, 321, 434, 466, 412, 565, 13, 286, 603, 808, 646, 281, 613, 3487, 322, 10017, 13], "temperature": 0.0, "avg_logprob": -0.10861260240728204, "compression_ratio": 1.2406015037593985, "no_speech_prob": 2.2821965103503317e-05}, {"id": 799, "seek": 600000, "start": 6013.0, "end": 6023.0, "text": " And so we will be using a randomized algorithm to calculate the truncated SPD more efficiently.", "tokens": [400, 370, 321, 486, 312, 1228, 257, 38513, 9284, 281, 8873, 264, 504, 409, 66, 770, 19572, 544, 19621, 13], "temperature": 0.0, "avg_logprob": -0.10861260240728204, "compression_ratio": 1.2406015037593985, "no_speech_prob": 2.2821965103503317e-05}, {"id": 800, "seek": 602300, "start": 6023.0, "end": 6043.0, "text": " Great. Thank you.", "tokens": [50364, 3769, 13, 1044, 291, 13, 51364], "temperature": 0.0, "avg_logprob": -0.36039575934410095, "compression_ratio": 0.68, "no_speech_prob": 0.0005418949294835329}], "language": "en"}