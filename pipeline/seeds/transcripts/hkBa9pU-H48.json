{"text": " All right, welcome to lesson 6 where we're going to do a deep dive into computer vision, convolutional neural networks, what is a convolution, and we're also going to learn the final regularization tricks after last lesson learning about weight decay and slash L2 regularization. I want to start by showing you something that I'm really excited about and I've had a small hand in helping to create. For those of you that saw my talk on TED.com, you might have noticed this really interesting demo that we did about four years ago showing a way to quickly build models with unlabeled data. It's been four years, but we're finally at a point where we're ready to put this out in the world and let people use it. The first people we're going to let use it are you folks. The company is called Platform.ai and the reason I'm mentioning it here is that it's going to let you create models on different types of datasets to what you can do now, that is to say, datasets that you don't have labels for yet. We're actually going to help you label them. This is the first time this has been shown before, so I'm pretty thrilled about it. Let me give you a quick demo. If you go to Platform.ai and choose get started, you'll be able to create a new project. If you create a new project, you can either upload your own images. Uploading about 500 or so works pretty well. You can upload a few thousand, but to start upload 500 or so, they all have to be in a single folder. We're assuming that you've got a whole bunch of images that you haven't got any labels for. Or you can start with one of the existing collections if you want to play around. I've started with the cars collection, going back to what we did four years ago. This is what happens when you first go into Platform.ai and look at the collection of images you uploaded. A random sample of them will appear on the screen. As you'll recognize, probably, they are projected from a deep learning space into a 2D space using a pre-trained model. For this initial version, it's an ImageNet model we're using. As things move along, we'll be adding more and more pre-trained models. What I'm going to do is I want to add labels to this data set representing which angle a photo of the car was taken from, which is something that actually ImageNet is going to be really bad at, isn't it? Because ImageNet has learned to recognize the difference between cars versus bicycles, and ImageNet knows that the angle you take a photo on actually doesn't matter. So we want to try and create labels using the kind of thing that actually ImageNet specifically learned to ignore. So the projection that you see, we can click these layer buttons at the top to switch to user projection using a different layer of the neural net. And so here's the last layer, which is going to be a total waste of time for us because it's really going to be projecting things based on what kind of thing it thinks it is. And the first layer is probably going to be a waste of time for us as well because there's very little interesting semantic content there. But if I go into the middle in layer 3, we may well be able to find some differences there. So then what you can do is you can click on the projection button here, and you can actually just press up and down rather than just pressing the arrows at the top to switch between projections or left and right to switch between layers. And what you can do is you can basically look around until you notice that there's a projection which has kind of separated out things you're interested in. And so this one actually, I notice that it's got a whole bunch of cars that are kind of from the top front right over here. Okay, so if we zoom in a little bit, we can double check. It's like, yeah, that looks pretty good. They're all kind of front right. So we can click on here to go to selection mode, and we can kind of grab a few, and then you should check. And so what we're doing here is we're trying to take advantage of the combination of human plus machine. The machine's pretty good at quickly doing calculations, but as a human, I'm pretty good at looking at a lot of things at once and seeing the odd one out. So in this case, I'm looking for cars that aren't front right. And so by laying them all in front of me, I can do that really quickly. It's like, okay, definitely that one. So just click on the ones that you don't want. All right, it's all good. So then you can just go back. And so then what you can do is you can either put them into a new category by typing create new label, or you can click on one of the existing ones. So before I came, I just created a few. So here's front right. So I'll just click on it here. There we go. And so that's the basic idea, is that you kind of keep flicking through different layers or projections to try and find groups that represent the things you're interested in. And then over time, you'll start to realize that there are some things that are a little bit harder. So for example, I'm having trouble finding sides. So what I can do is I can see over here there's a few sides. So I can zoom in here and click on a couple of them, like this one and this one, that one, that one. Okay. And then I'll say find similar. And so this is going to basically look in that projection space, and not just at the images that are currently displayed, but all of the images that you uploaded. And hopefully I might be able to label now a few more side images at that point. So it's going through and checking all of the images that you uploaded to see if any of them have projections in this space which are similar to the ones I've selected. And hopefully we'll find a few more of what I'm interested in. Okay, so now if I want to try to find a projection that separates the sides from the front right, I can click on each of those two. And then over here, this button's now called switch to the projection that maximizes the distance between the labels. So now what this is going to do is try to find the best projection that separates out those classes. And so the goal here is to help me visually inspect and quickly find a bunch of things that I can use to label. So they're the key features, and it's done a good job. You can see down here we've now got a whole bunch of sides which I can now grab because I was having a lot of trouble finding them before. And it's always worth double checking. And it's kind of interesting to see how the neural nets behave. Like there seems to be more sports cars in this group than average as well. So it's kind of found side angles of sports cars. So that's kind of interesting. So then I can click, all right, so I've got those. So now I'll click side. And there we go. So once you've done that a few times, I find if you've got, you know, a hundred or so labels, you can then click on the train model button and it'll take a couple of minutes and come back and show you your train model. And after it's trained, which I did it on a smaller number of labels earlier, you can then switch this vary opacity button and it'll actually kind of fade out the ones that are already predicted pretty well. And it'll also give you an estimate as to how accurate it thinks the model is. The main reason I mention this for you is that so that you can now click the download button and it'll download the predictions, which is what we hope will be interesting to most people. But what I think will be interesting to you as deep learning students is it'll download your labels. So now you can use that labeled subset of data along with the unlabeled set that you haven't labeled yet to see if you can, you know, see if you can build a better model and platform AI is done for you. See if you can use that initial set of data to kind of get going, creating models of stuff, which you weren't able to label before. Clearly, there are some things that this system is better at than others. The things that require, you know, really zooming in closely and taking a very, very close inspection, this isn't going to work very well. This is really designed for things that the human eye can kind of pick up fairly readily. But we'd love to get feedback as well and you can click on the help button to get feedback and also there's a platform AI discussion topic in our forum where, so Ashak, if you can stand up, Ashak is the CEO of the company. He'll be there helping out, answering questions and so forth. So yeah, I hope people find that useful. It's been many years getting to this point and I'm glad we're finally there. Okay, so one of the reasons I wanted to mention this today is that we're going to be doing a big dive into convolutions later in this lesson. So I'm going to circle back to this to try and explain a little bit more about how that is working under the hood and give you kind of a sense of what's going on. But before we do, we have to finish off last week's discussion of regularization. And so we were talking about regularization specifically in the context of the tabular learner because the tabular learner, this was the forward method, sorry, this is the init method in the tabular learner and our goal was to understand everything here. And we're not quite there yet. Last week we were looking at the adult data set which is a really simple, kind of oversimple data set that's just for toy purposes. So this week let's look at a data set that's much more interesting, a Kaggle competition data set. So we know kind of what the best in the world and you know Kaggle competition results tend to be much harder to beat than academic state of the art results tend to be because a lot more people work on Kaggle competitions than most academic data sets. So it's a really good challenge to try and do well on a Kaggle competition data set. So this one, the Rossmann data set, they've got 3,000 drug stores in Europe and you're trying to predict how many products they're going to sell in the next couple of weeks. So one of the interesting things about this is that the test set for this is from a time period that is more recent than the training set. And this is really common, right, if you want to predict things there's no point predicting things that are in the middle of your training set. You want to predict things in the future. Another interesting thing about it is the evaluation metric they provided is the root mean squared percent error. So this is just a normal root mean squared error except we go actual minus prediction divided by actual. So in other words it's the percent error that we're taking the root mean squared of. So there's a couple of interesting features. Always interesting to look at the leaderboard. So the leaderboard, the winner was.1, the paper that we've roughly replicated was.105,.106, and 10th place out of 3,000 was.11ish, a bit less. So we're going to skip over a little bit, which is that the data that was provided here was they provided a small number of files but they also let competitors provide additional external data as long as they shared it with all the competitors. And so in practice the data set we're going to use contains, I can't remember, six or seven tables. The way that you join tables and stuff isn't really part of a deep learning course so I'm going to skip over it and instead I'm going to refer you to Introduction to Machine Learning for Coders which will take you step by step through the data preparation for this. We've provided it for you in Rossman DataClean so you'll see the whole process there and so you'll need to run through that notebook to create these pickle files that we read here. Can you see this in the back okay? I just want to mention one particularly interesting part of the Rossman DataClean notebook which is you'll see there's something that says add date part and I wanted to explain what's going on here. I've been mentioning for a while that we're going to look at time series and pretty much everybody who I've spoken to about it has assumed that I'm going to do some kind of recurrent neural network but I'm not. Interestingly the main academic group that studies time series is Econometrics but they tend to study one very specific kind of time series which is where the only data you have is a sequence of time points of one thing. That's the only thing you have is one sequence. In real life that's almost never the case. Normally we would have some information about the store that that represents or the people that it represents. We'd have metadata. We'd have sequences of other things measured at similar time periods or different time periods. And so most of the time I find in practice the state of the art results when it comes to competitions on kind of more real world data sets don't tend to use recurrent neural networks but instead they tend to take the time piece which in this case it was a date we were given in the data and they add a whole bunch of metadata. So in our case for example we've added day of week. So we were given a date. We've added day of week, year, month, week of year, day of month, day of week, day of year and then a bunch of Booleans is at the month start or end, quarter, year start or end, elapsed time since 1970 and so forth. If you run this one function add date part and pass it a date it'll add all of these columns to your data set for you. And so what that means is that let's take a very reasonable example. Featuring behavior probably changes on payday. Payday might be the 15th of the month. So if you have a thing here called this is day of month here then it'll be able to recognize every time something is a 15 there and associated it with a higher in this case embedding matrix value. So this way it basically, we can't expect a neural net to do all of our feature engineering for us. We can expect it to kind of find nonlinearities and interactions and stuff like that. But for something like taking a date like this and figuring out that the 15th of the month is something when interesting things happen it's much better if we can provide that information for it. So this is a really useful function to use and once you've done this you can treat many kinds of time series problems as regular tabular problems. I say many kinds, not all, if there's very complex kind of state involved in a time series such as equity trading or something like that it probably won't be the case or this won't be the only thing you need. But in this case it'll get us a really good result and in practice most of the time I find this works well. Tabular data is normally in pandas so we just stored them as standard Python pickle files. We can read them in, we can take a look at the first five records and so the key thing here is that we're trying to on a particular date for a particular store ID we want to predict the number of sales. Sales is the dependent variable. So the first thing I'm going to show you is something called preprocessors. You've already learned about transforms. Transforms are bits of code that run every time something is grabbed from a data set and so it's really good for data augmentation that we'll learn about today which is that it's going to get a different random value every time it's sampled. Preprocessors are like transforms but they're a little bit different which is that they run once before you do any training and really importantly they run once on the training set and then any kind of state or metadata that's created is then shared with the validation and test set. Let me give you an example. When we've been doing image recognition and we've had a set of classes for like all the different pet breeds and they've been turned into numbers, the thing that's actually doing that for us is a preprocessor that's being created in the background. So that makes sure that the classes for the training set are the same as the classes for the validation and the classes for the test set. So we're going to do something very similar here. For example, if we create a little small subset of the data for playing with, this is a really good idea when you start with a new data set. So I've just grabbed 2000 IDs at random and then I'm just going to grab a little training set and a little test set, half and half of those 2000 IDs. I'm just going to grab five columns and then we can just play around with this nice and easy. So here's the first few of those from the training set. As you can see one of them is called promoInterval and it has these strings and sometimes it's missing and pandas missing is nan. So the first preprocessor I'll show you is categorify and categorify does basically the same thing that that classes thing for image recognition does for our dependent variable. It's going to take these strings, it's going to find all of the possible unique values of it and it's going to create a list of them and then it's going to turn the strings into numbers. So if I call it on my training set that'll create categories there and then I call it on my test set, passing in test equals true, that makes sure it's going to use the same categories that I had before. And now when I say.head it looks exactly the same and that's because pandas has turned this into a categorical variable which internally is storing numbers but externally is showing me the strings but I can look inside promoInterval to look at the cat categories, this is all standard pandas here, to show me a list of all of what we would call classes in fast AI or would be called just categories in pandas. And so then if I look at the cat.codes you can see here this list here is the numbers that are actually stored, minus one, minus one, one, minus one, one. What are these minus ones? The minus ones represent NIN, they represent missing, so pandas uses the special code minus one to mean missing. Now as you know these are going to end up in an embedding matrix and we can't look up item minus one in an embedding matrix so internally in fast AI we add one to all of these. Another useful preprocessor is fixMissing and so again you can call it on the data frame, you can call it on the test, passing in test equals true, and this will create for everything that's missing, anything that has a missing value, it will create an additional column with the column name underscore NA, so competition distance underscore NA, and it will set it for true for any time that was missing. And then what we do is we replace competition distance with the median for those. Why do we do this? Well because very commonly the fact that something's missing is of itself interesting. Like it turns out the fact that this is missing helps you predict your outcome. So we certainly want to keep that information in a convenient Boolean column so that our deep learning model can use it to predict things. But then we need competition distance to be a continuous variable so we can use it in the continuous variable part of our model. So we can replace it with almost any number, right, because if it turns out that the missingness is important it can use the interaction of competition distance NA and competition distance to make predictions. So that's what fixMissing does. You don't have to manually call preprocessors yourself. When you call any kind of item list creator you can pass in a list of preprocessors which you can create like this. So this is saying, okay, I want to feel missing, I want to categorify, I want to normalize, so for continuous variables it'll subtract the mean and divide by the standard deviation to help it train more easily. And so you just say those are my procs and then you can just pass it in there and that's it. And later on you can go data.export and it'll save all the metadata for that data bunch so you can later on load it in knowing exactly what your category codes are, exactly what median values you use for replacing the missing values, and exactly what means and standard deviations you're normalized by. Okay, so the main thing you have to do if you want to create a data bunch of tabular data is find out or tell it what are your categorical variables and what are your continuous variables. And as we discussed last week briefly, your categorical variables are not just strings and things but also I include things like day of week and month and day of month, even though they're numbers, I make them categorical variables because for example, day of month, I don't think it's going to have a nice smooth curve. I think that the 15th of the month and the 1st of the month and the 30th of the month are probably going to have different purchasing behavior to other days of the month. And so therefore if I make it a categorical variable, it's going to end up creating an embedding matrix and those different days of the month can get different behaviors. So you've actually got to think carefully about which things should be categorical variables and on the whole, if in doubt and there are not too many levels in your category, that's called the cardinality. If your cardinality is not too high, I would put it as a categorical variable. You can always try and see which works best. So our final data frame that we're going to pass in is going to be our training set with the categorical variables and the continuous variables and the dependent variable and the date. And the date we're just going to use to create a validation set where we're basically going to say the validation set is going to be the same number of records at the end of the time period that the test set is for Kaggle. And so that way we should be able to validate our model nicely. Okay, so now we can create a tabular list. So this is our standard data block API that you've seen a few times from a data frame. Pass in all of that information, split it into valid versus train, label it with a dependent variable. And here's something I don't think you've seen before, label class. This is our dependent variable. And as you can see, this is sales. It's not a float. It's an int64. If this was a float, then FastAI would automatically know or guess that you want to do a regression. But this is not a float. It's an int. So FastAI is going to assume you want to do a classification. So when we label it, we have to tell it that the class of the labels we want is a list of floats, not a list of categories, which would otherwise be the default. So this is the thing that's going to automatically turn this into a regression problem for us. And then we create a data bunch. So I wanted to remind you again about doc, which is how we find out more information about this stuff. In this case, all of the labeling functions in the data blocks API will pass on any keywords they don't recognize to the label class. So one of the things I've passed in here is log. And so that's actually going to end up in float list. And so if I go doc float list, I can see a summary. And I can even jump into the full documentation. And it shows me here that log is something which, if true, it's going to take the logarithm of my dependent variable. Why am I doing that? Because this is the thing that's actually going to automatically take the log of my way. The reason I'm doing that is because, as I mentioned before, the evaluation metric is root mean squared percentage error. And fastai, either fastai nor PyTorch has a root mean squared percentage error loss function built in. I don't even know if such a loss function would work super well. But if you want to spend the time thinking about it, you'll notice that this ratio, if you first take the log of y and y hat, then becomes a difference rather than a ratio. So in other words, if you take the log of y, then this becomes root mean squared error. So that's what we're going to do. We're going to take the log of y. And then we're just going to use root mean squared error, which is the default for regression problems. So we won't even have to mention it. The reason that we have this here is because this is so common. Basically any time you're trying to predict something that's like a population or a dollar amount of sales, these kind of things tend to have long-tail distributions where you care more about percentage differences than exact differences, absolute differences. So you're very likely to want to do things with log equals true and to measure the root mean squared percent error. We've learned about the y range before, which is going to use that sigmoid to help us get in the right range. Because this time the y values are going to be taken the log of it first, we need to make sure that the y range we want is also the log. So I'm going to take the maximum of the sales column. I'm going to multiply it by a little bit. So that because remember how we said it's nice if your range is a bit wider than the range of the data, and then we're going to take the log. And that's going to be our maximum. So then our y range will be from zero to a bit more than the maximum. So now we've got our data bunch. We can create a tabular learner from it. And then we have to pass in our architecture. And as we briefly discussed, for a tabular model, our architecture is literally the most basic, fully connected network, just like we showed in this picture. It's an import, matrix multiply, non-linearity, matrix multiply, non-linearity, matrix multiply, non-linearity, done. One of the interesting things about this is that this competition is three years old, but I'm not aware of any significant advances, at least in terms of architecture, that would cause me to choose something different to what the third-placed folks did three years ago. We're still basically using simple, fully connected models for this problem. Now the intermediate weight matrix is going to have to go from a 1,000 activation input to a 500 activation output, which means it's going to have to be 500,000 elements in that weight matrix. That's an awful lot for a data set with only a few hundred thousand rows. So this is going to overfit, and we need to make sure it doesn't. So one way to make sure it doesn't, well, the way to make sure it doesn't is to use regularization, not to reduce the number of parameters, to use regularization. So one way to do that will be to use weight decay, which Fast.ai will use automatically, and you can vary it to something other than the default if you wish. It turns out in this case, we're going to want more regularization, and so we're going to pass in something called keys. This is going to provide dropout, and also this one here, mdrop, this is going to provide embedding dropout. So let's learn about what is dropout. But the short version is dropout is a kind of regularization. This is the dropout paper. Nitish, how do you say this, Suvasthava, it was Suvasthava's master's thesis under Jeffrey Hinton, and this picture from the original paper is a really good picture of what's going on. This first picture is a picture of a standard fully connected network. It's a picture of this. What each line shows is a multiplication of an activation times a weight. And then when you've got multiple arrows coming in, that represents a sum. So this activation here is the sum of all of these inputs times all of these activations. So that's what a normal fully connected neural net looks like. For dropout, we throw that away. At random, we throw away some percentage of the activations, not the weights, not the parameters. Remember there's only two types of number in a neural net, parameters, also called weights kind of, and activations. So we're going to throw away some activation. So you can see that when we throw away this activation, all of the things that were connected to it are gone too. For each mini-batch, we throw away a different subset of activations. How many do we throw away? We throw each one away with a probability p. A common value of p is 0.5. So what does that mean? And you'll see in this case, not only have they deleted at random some of these hidden layers, but they've actually deleted some of the inputs as well. Deleting the inputs is pretty unusual. Normally, we only delete activations in the hidden layers. So what does this do? Well, every time I have a mini-batch going through, I at random throw away some of the activations. I put them back, I throw away some different ones. So it means that no one activation can kind of memorize some part of the input. Because that's what happens if we overfit. If we overfit, some part of the model is basically learning to recognize a particular image rather than a feature in general or a particular item. With dropout, it's going to be very hard for it to do that. In fact, Geoffrey Hinton described one of the kind of part of the thinking behind this as follows. He said he noticed every time he went to his bank that all the tellers and staff moved around. And he realized the reason for this must be that they're trying to avoid fraud. If they keep moving them around, nobody can specialize so much in that one thing that they're doing that they can figure out kind of a conspiracy to defraud the bank. Now of course, it depends when you ask Hinton. At other times, he says that the reason for this was because he thought about how spiking neurons work. And he's a neuroscientist by training. There's a view that spiking neurons might help regularization and dropout is kind of a way of matching this idea of spiking neurons. I mean, it's interesting. When you actually ask people, where did your idea for some algorithm come from, it basically never comes from math. It always comes from intuition and kind of thinking about physical analogies and stuff like that. So anyway, the truth is a bunch of ideas, I guess, were all flowing around and they came up with this idea of dropout. But the important thing to know is it worked really, really well. And so we can use it in our models to get generalization for free. Now too much dropout, of course, is reducing the capacity of your model. So it's going to underfit. And so you've got to play around with different dropout values for each of your layers to decide. So in pretty much every fast AI learner, there's a parameter called P's, P S, which will be the P value for the dropout for each layer. So you can just pass in a list. Or you can pass in an int and it'll create a list with that value everywhere. Sometimes it's a little different for CNN, for example. It actually, if you pass in an int, it will use that for the last layer and half that value for the earlier layers. We basically try to do things that kind of represent best practice. But you can always pass in your own list to get exactly the dropout that you want. There is an interesting feature of dropout, which is that we talk about training time and test time. Test time we also call inference time. Training time is when we're actually doing those weight updates, the back propagation. In the training time, dropout works the way we just saw. At test time, we turn off dropout. We're not going to do dropout anymore because we want it to be as accurate as possible. We're not training, so we can't cause it to overfit when we're doing inference. So we remove dropout. But what that means is if previously p was 0.5, then half the activations were being removed, which means when they're all there, now our overall activation level is twice what it used to be. And so therefore in the paper, they suggest multiplying all of your weights at test time by p. Interestingly, you can dig into the PyTorch source code and you can find the actual C code where dropout is implemented. And here it is. And you can see what they're doing is something quite interesting. They first of all do a Bernoulli trial. So a Bernoulli trial is with probability 1 minus p, return the value 1, otherwise return the value 0. That's all it means. So in this case, p is the probability of dropout. So 1 minus p is the probability that we keep the activation. So we end up here with either a 1 or a 0. And then, this is interesting, we divide in place, remember underscore means in place in PyTorch, we divide in place that 1 or 0 by 1 minus p. If it's a 0, nothing happens, it's still 0. If it's a 1 and p was 0.5, that 1 now becomes 2. And then finally, we multiply in place our input by this noise, this dropout mask. So in other words, we actually don't do in PyTorch, we don't do the change at test time. We actually do the change at training time, which means that you don't have to do anything special at inference time with PyTorch. It's not just PyTorch, it's quite a common pattern. But it's kind of nice to look inside the PyTorch source code and see, you know, dropout, this incredibly cool, incredibly valuable thing is really just these three lines of code, which they do in C because I guess it ends up a bit faster when it's all fused together. But lots of libraries do it in Python, and that works well as well. You can even write your own dropout layer, and it should give exactly the same results as this. So that'd be a good exercise to try. See if you can create your own dropout layer in Python, and see if you can replicate the results that we get with this dropout layer. So that's dropout. And so in this case, we're going to use a tiny bit of dropout on the first layer and a little bit of dropout on the next layer, and then we're going to use special dropout on the embedding layer. Now why do we use special dropout on the embedding layer? So if you look inside the FastAI source code, here's our tabular model. You'll see that in the section that checks that there's some embeddings, we call each embedding and then we concatenate the embeddings into a single matrix, and then we call embedding dropout. An embedding dropout is simply just a dropout, right, so it's just an instance of a dropout module. This kind of makes sense, right? For continuous variables, that continuous variable is just in one column. You wouldn't want to do dropout on that because you're literally deleting the existence of that whole input, which is almost certainly not what you want. But for an embedding, an embedding is just effectively a matrix multiply by a one-hot encoded matrix. So it's just another layer. So it makes perfect sense to have dropout on the output of the embedding because you're putting dropout on those activations of that layer. And so you're basically saying, let's delete at random some of the results of that embedding, some of those activations. So that makes sense. The other reason we do it that way is because I did very extensive experiments about a year ago where on this data set, I tried lots of different ways of doing kind of everything. And you can actually see it here. I put it all in a spreadsheet, of course, Microsoft Excel, put them into a pivot table to summarize them all together to find out kind of which different choices and hyperparameters and architectures worked well and worked less well. And then I created all these little graphs. And these are like little summary training graphs for different combinations of hyperparameters and architectures. And I found that there was one of them which ended up consistently getting a good predictive accuracy. The kind of bumpiness of the training was pretty low. And you can see it's just a nice, smooth curve. And so this is an example of the kind of experiments that I do that end up in the Fast AI library. So embedding dropout is one of those things that I just found worked really well. And basically the results of these experiments is why it looks like this rather than something else. Well, it's a combination of these experiments, but then why did I do these particular experiments? Well, because it was very influenced by what worked well in that Kaggle prize winner's paper. But there were quite a few parts of that paper I thought there were some other choices they could have made. I wonder why they didn't. And I tried them out and found out what actually works and what doesn't work as well and found a few little improvements. So that's the kind of experiments that you can play around with as well when you try different models and architectures, different dropouts, layer numbers, number of activations and so forth. So having created our learner, we can type learn.model to take a look at it. And as you would expect in that there is a whole bunch of embeddings. Each of those embedding matrices tells you, well, this is the number of levels of the input for each input. And you can match these with your list catbars. So the first one will be store. So that's not surprising. There are 1,116 stores. And then the second number, of course, is the size of the embedding. And that's a number that you get to choose. And so Fast.ai has some defaults, which actually work really, really well nearly all the time. So I almost never change them. But when you create your tabular learner, you can absolutely pass in an embedding size dictionary which maps variable names to embedding sizes for anything where you want to override the defaults. And then we've got our embedding dropout layer. And then we've got a batch norm layer with 16 inputs. The 16 inputs make sense because we have 16 continuous variables. The length of cont names is 16. So this is something for our continuous variables. And specifically, it's over here, bn.cont on our continuous variables. And bn.cont is a batch norm 1D. What's that? Well, the first short answer is it's one of the things that I experimented with as to having batch normal not in this. And I found that it worked really well. And then specifically what it is is extremely unclear. Let me describe it to you. It's kind of a bit of regularization. It's kind of a bit of training helper. It's called batch normalization. And it comes from this paper. Actually, before I do this, I just want to mention one other really funny thing. Dropout, I mentioned it was a master's thesis. Not only was it a master's thesis, one of the most influential papers of the last 10 years, it was rejected from the main neural nets conference. What was then called NIPS, now called NeurIPS. I think it's very interesting because it's just a reminder that our academic community is generally extremely poor at recognizing which things are going to turn out to be important. Generally people are looking for stuff that are in the field that they're working on and understand. So Dropout kind of came out of left field. It's kind of hard to understand what's going on. And so that's kind of interesting. And so it's a reminder that if you just follow, as you kind of develop beyond being just a practitioner into actually doing your own research, don't just focus on the stuff everybody's talking about. Focus on the stuff you think might be interesting. Because the stuff everybody's talking about generally turns out not to be very interesting. The community is very poor at recognizing high impact papers when they come out. Batch normalization, on the other hand, was immediately recognized as high impact. I definitely remember everybody talking about it in 2015 when it came out. And that was because it was so obvious. They showed this picture showing the current then state-of-the-art ImageNet model inception. This is how long it took them to get a pretty good result. And then they tried the same thing with this new thing called BatchNorm, and they just did it way, way, way quickly. And so that was enough for pretty much everybody to go, wow, this is interesting. And specifically, they said this thing's called BatchNormalization, and it's accelerating training by reducing internal covariate shift. So what is internal covariate shift? Well, it doesn't matter. Because this is one of those things where researchers came up with some intuition and some idea about this thing they wanted to try. They did it. It worked well. The post-hoc added on some mathematical analysis to try and claim why it worked, and it turned out they were totally wrong. In the last two months, there's been two papers, so it took three years for people to really figure this out, and the last two months there's been two papers that have shown BatchNormalization doesn't reduce covariate shift at all. And even if it did, that has nothing to do with why it works. So I think that's kind of an interesting insight, again, which is why we should be focusing on being practitioners and experimentalists and developing an intuition. What BatchNorm does is what you see in this picture here, in this paper. Here are steps or batches, and here is loss. And here, the red line is what happens when you train without BatchNorm. Very, very bumpy. And here the blue line is what happens when you train with BatchNorm. Not very bumpy at all. What that means is you can increase your learning rate with BatchNorm, because these big bumps represent times that you're really at risk of your set of weights jumping off into some awful part of the weight space that it can never get out of again. So if it's less bumpy, then you can train at a higher learning rate. So that's actually what's going on. And here's what it is. This is the algorithm. And it's really simple. The algorithm is going to take a mini-batch. So we have a mini-batch. And remember, this is a layer, so the thing coming into it is activations. So it's a layer, and it's going to take in some activations. And so the activations, it's calling x1, x2, x3, and so forth. The first thing we do is we find the mean of those activations. Sum divided by the count, that's just the mean. And the second thing we do is we find the variance of those activations. Different squared divided by the mean is the variance. And then we normalize. So the values minus the mean divided by the deviation is the normalized version. OK, it turns out that bit's actually not that important. We used to think it was. But it turns out it's not. The really important bit is the next bit. We take those values and we add a vector of biases. They call it beta here. And we've seen that before. We've used a bias term before. OK, so we're just going to add a bias term as per usual. And then we're going to use another thing that's a lot like a bias term. But rather than adding it, we're going to multiply by it. So there's these parameters gamma and beta, which are learnable parameters. There's only two kinds of number, activations and parameters. These are parameters. OK, they're things that are learned with gradient descent. This is just a normal bias layer, beta. And this is a multiplicative bias layer. Nobody calls it that, but that's all it is. It's just like bias, but we multiply rather than add. That's all batch norm is. That's what the layer does. So why is that able to achieve this fantastic result? I'm not sure anybody has exactly written this down before. If they have, I apologize for failing to cite it because I haven't seen it. But let me explain what's actually going on here. The value of our predictions y hat is some function of our various weights. There could be millions of them, weight 1 million. And it's also a function, of course, of the inputs to our layer. This function here is our neural net function, whatever is going on in our neural net. And then our loss, let's say it's mean squared error, is just our actuals minus our predicted squared. So let's say we're trying to predict movie review outcomes, and they're between 1 and 5. And we've been trying to train our model, and the activations at the very end are currently between minus 1 and 1. So they're way off where they need to be. The scale is off, the mean is off. So what can we do? One thing we could do would be to try and come up with a new set of weights that cause the spread to increase and cause the mean to increase as well. And that's going to be really hard to do, because remember all these weights interact in very intricate ways. We've got all those nonlinearities, and they all combine together. So to kind of just move up is going to require navigating through this complex landscape. And we use all these tricks like momentum and Adam and stuff like that to help us. But it still requires a lot of twiddling around to get there. So that's going to take a long time, and it's going to be bumpy. But what if we did this? What if we went times G plus B? We added two more parameter vectors. Or now it's really easy. In order to increase the scale, that number has a direct gradient to increase the scale. To change the mean, that number has a direct gradient to change the mean. There's no interactions or complexities. It's just straight up and down, straight in and out. And that's what Batch Norm does. So Batch Norm is basically making it easier for it to do this really important thing, which is to shift the outputs up and down and in and out. And that's why we end up with these results. So those details in some ways don't matter terribly. The really important thing to know is you definitely want to use it. Or if not it, something like it. There's various other types of normalization around nowadays. But Batch Norm works great. The other main normalization type we use in Fast AI is something called Weight Norm, which is much more just in the last few months' development. So that's Batch Norm. And so what we do is we create a Batch Norm layer for every continuous variable. In Cont is the number of continuous variables. In Fast AI, n underscore something always means the count of that thing. Cont always means continuous. So then here is where we use it. We grab our continuous variables and we throw them through a Batch Norm layer. And so then over here you can see it in our model. One interesting thing is this momentum here. This is not momentum like in optimization, but this is momentum as in exponentially weighted moving average. Basically this mean and standard deviation, we don't actually use a different mean and standard deviation for every mini-batch. If we did, it would vary so much that it would be very hard to train. So instead we take an exponentially weighted moving average of the mean and standard deviation. And if you don't remember what I mean by that, look back at last week's lesson to remind yourself about exponentially weighted moving averages, which we implemented in Excel for the momentum and Adam gradient squared terms. You can vary the amount of momentum in a Batch Norm layer by passing a different value to the constructor in PyTorch. If you use a smaller number, it means that the mean and standard deviation will vary less from mini-batch to mini-batch, and that will have less of a regularization effect. A larger number will mean the variation will be greater for mini-batch to mini-batch. That will have more of a regularization effect. So as well as this thing of training more nicely because it's parameterized better, this momentum term in the mean and standard deviation is the thing that adds this nice regularization piece. When you add Batch Norm, you should also be able to use a higher learning rate. So that's our model. So then you can go LRFind, you can have a look, and then you can go fit, you can save it, you can plot the losses, you can fit a bit more, and we end up at.103. Tenth place in the competition was.108, so it's looking good. Again, take it with a slight grain of salt because what you actually need to do is use the real training set and submit it to Kaggle, but you can see we're very much amongst the kind of cutting edge of models, at least as of 2015, and as I say, there haven't really been any architectural improvements since then. There wasn't Batch Norm when this was around, so the fact we added Batch Norm means that we should get better results, and certainly more quickly. And if I remember correctly, in their model, they had to train at a lower learning rate for quite a lot longer. As you can see, this is about less than 45 minutes of training, so that's nice and fast. Any questions? In what proportion would you use dropout versus other regularization errors like weight decay, L2 norms, etc.? So remember that L2 regularization and weight decay are kind of two ways of doing the same thing, and we should always use the weight decay version, not the L2 regularization version. So there's weight decay, there's Batch Norm, which kind of has a regularizing effect, there's data augmentation, which we'll see soon, and there's dropout. So Batch Norm we pretty much always want, so that's easy. Data augmentation we'll see in a moment. So then it's really between dropout versus weight decay. I have no idea. I don't think I've seen anybody provide a compelling study of how to combine those two things. Can you always use one instead of the other? Why? Why not? I don't think anybody has figured that out. I think in practice it seems that you generally want a bit of both. You pretty much always want some weight decay, but you often also want a bit of dropout. But honestly, I don't know why. I've not seen anybody really explain why or how to decide. So this is one of these things you have to try out and kind of get a feel for what tends to work for your kinds of problems. I think the defaults that we provide in most of our learners should work pretty well in most situations, but yeah, definitely play around with it. The next kind of regularization we're going to look at is data augmentation. And data augmentation is one of the least well-studied types of regularization, but it's the kind that I think I'm kind of the most excited about. The reason I'm kind of the most excited about it is that there's basically almost no cost to it. You can do data augmentation and get better generalization without it taking longer to train, without underfitting, to an extent at least. So let me explain. So what we're going to do now is we're going to come back to computer vision and we're going to come back to our pets data set again. So let's load it in. Our pets data set, the images were inside the images subfolder. And I'm going to call getTransforms as per usual, but when we call getTransforms, there's a whole long list of things that we can provide. And so far, we haven't been varying that much at all. But in order to really understand data augmentation, I'm going to kind of ratchet up all of the defaults. So there's a parameter here for what's the probability of an affine transform happening, what's the probability of a lighting transform happening, so I set them both to one. So they're all going to get transformed, they're going to do more rotation, more zoom, more lighting transforms, and more warping. What do all those mean? So you should check the documentation, and you do that by typing doc, and there's the brief documentation, but the real documentation is in docs. So I'll click on show in docs, and here it is. And so this tells you what all of those do, but generally the most interesting parts of the docs tend to be at the top, where you kind of get the summaries of what's going on. So here there's something called list of transforms, and here you can see every transform has something showing you lots of different values of it. So here's brightness. So make sure you read these, and remember, these notebooks, you can open up and run this code yourself and get this output. All of these HTML documentation documents are auto-generated from the notebooks in the docs underscore source directory in the fast AI repo. So you will see the exact same cats if you try this. Silverman really likes cats, so there's a lot of cats in the documentation. And I think because he's been so awesome at creating great documentation, he gets to pick the cats. So for example, looking at different values of brightness, what I do here is I look to see two things. The first is, for which of these levels of transformation is it still clear what the picture is a picture of? So this is kind of getting to a point where it's pretty unclear. This is possibly getting a little unclear. The second thing I do is I look at the actual dataset that I'm modelling, or particularly the dataset that I'll be using as a validation set, and I try to get a sense of what the variation in this case in lighting is. So if they're nearly all professionally taking photos, I would probably want them all to be about in the middle. But if they're photos that are taken by some pretty amateur photographers, they're likely to be some that are very overexposed, some very underexposed. So you should pick a value of this data augmentation for brightness that both allows the image to still be seen clearly, and also represents the kind of data that you're going to be using this to model on in practice. So you've got to see the same thing for contrast. It would be unusual to have a dataset with such ridiculous contrast, but perhaps you do, in which case you should use data augmentation up to that level. But if you don't, then you shouldn't. This one called dihedral is just one that does every possible rotation and flip, and so obviously most of your pictures are not going to be upside down cats. So you probably would say, hey, this doesn't make sense. I won't use this for this dataset. But if you're looking at satellite images, of course you would. On the other hand, flip makes perfect sense. So you would include that. A lot of things that you can do with fast.ai lets you pick a padding mode, and this is what padding mode looks like. You can pick zeros, you can pick border, which just replicates, or you can pick reflection, which as you can see is as if the last little few pixels are in a mirror. Reflections nearly always better, by the way. I don't know that anybody else has really studied this, but we have studied it in some depth. Haven't actually written a paper about it, but just enough for our own purposes to say reflection works best most of the time. So that's the default. Then there's a really cool bunch of perspective warping ones, which I'll probably show you by using symmetric warp. If you look at the kind of the, we've added black borders to this so it's more obvious for what's going on. And as you can see, what symmetric warp is doing, it's as if the camera is being moved above or to the side of the object and literally warping the whole thing like that, right? So the cool thing is that as you can see, each of these pictures, it's as if this cat was being taken kind of from different angles, right? So they're all kind of optically sensible, right? And so this is a really great type of data augmentation. It's also one which I don't know of any other library that does it, or at least certainly one that does it in a way that's both fast and keeps the image crisp, as it is in fast AI. So this is like, if you're looking to win a Kaggle competition, this is the kind of thing that's going to get you above the people that aren't using the fast AI library. So having looked at all that, we are going to add this, have a little get data function that just does the usual data block stuff, but we're going to add padding mode explicitly so that we can turn on padding mode of zeros just so we can see what's going on better. Fast AI has this handy little function called plotMulti, which is going to create a 3x3 grid of plots, and each one will contain the result of calling this function, which will receive the plot coordinates and the axis. And so I'm actually going to plot the exact same thing in every box, but because this is a training data set, it's going to use data augmentation. And so you can see the same doggy using lots of different kinds of data augmentation. And so you can see why this is going to work really well, because these pictures all look pretty different. But we didn't have to do any extra hand labeling or anything. That's like free extra data. So data augmentation is really, really great. And one of the big opportunities for research is to figure out ways to do data augmentation in other domains. So how can you do data augmentation with text data, or genomic data, or histopathology data, or whatever? Almost nobody's looking at that, and to me it's one of the biggest opportunities that could let you decrease data requirements by like 5 to 10x. So here's the same thing again, but with reflection padding instead of zero padding. And you can kind of see, like see this doggy's legs are actually being reflected at the bottom here. So reflection padding tends to create images that are kind of much more naturally reasonable. Like in the real world you don't get black borders like this. So they do seem to work better. Okay, so because we're going to study convolutional neural networks, we are going to create a convolutional neural network. You know how to create them. So I'll go ahead and create one. I will fit it for a little bit. I will unfreeze it. I will then create a larger version of the data set, 352x352, and fit for a little bit more. And I will save it. Okay. So we have a CNN, and we're going to try and figure out what's going on in our CNN. And the way we're going to try and figure it out is specifically that we're going to try to learn how to create this picture. This is a heat map. This is a picture which shows me what part of the image that the CNN focused on when it was trying to decide what this picture is. So we're going to make this heat map from scratch. When we, so we're kind of at a point now in the course where I'm assuming that if you've got to this point, you know, and you're still here, thank you, then you're interested enough that you're prepared to kind of dig into some of these details. So we're actually going to learn how to create this heat map without almost any fast AI stuff. We're going to use pure kind of tensor arithmetic in PyTorch, and we're going to try and use that to really understand what's going on. So to warn you, none of it's rocket science, but a lot of it's going to look really new. So don't expect to get it the first time, but expect to like listen, jump into the notebook, try a few things, test things out, look particularly at like tensor shapes and inputs and outputs to check your understanding, then go back and listen again, and kind of try it a few times because you will get there, right? It's just that there's going to be a lot of new concepts because we haven't done that much stuff in pure PyTorch. Okay, so what we're going to do is we're going to have a seven minute break, and then we're going to come back and we're going to learn all about the innards of a CNN. So I'll see you at 7.50. So let's learn about convolutional neural networks. You know, the funny thing is it's pretty unusual to get close to the end of a course and only then look at convolutions, but like when you think about it, knowing actually how batch norm works or how dropout works or how convolutions work isn't nearly as important as knowing how it all goes together and what to do with them and how to figure out how to do those things better. But it's, you know, we're kind of at a point now where we want to be able to do things like that, and although, you know, we're adding this functionality directly into the library so you can kind of run a function to do that, you know, the more you do, the more you'll find things that you want to do a little bit differently to how we do them. Or there'll be something in your domain where you think like, oh, I could do a slight variation of that. So you're kind of getting to a point in your experience now where it helps to know how to do more stuff yourself, and that means you need to understand what's really going on behind the scenes. So what's really going on behind the scenes is that we are creating a neural network that looks a lot like this, right, but rather than doing a matrix multiply here and here and here, we're actually going to do instead a convolution, and a convolution is just a kind of matrix multiply which has some interesting properties. You should definitely check out this website, satosa.io.ev, explained visually, where we have stolen this beautiful animation. It's actually a JavaScript thing that you can actually play around with yourself in order to show you how convolutions work, and it's actually showing you a convolution as we move around these little red squares. So here's a picture, a black and white or a grayscale picture, and so each 3x3 bit of this picture, as this red thing moves around, it shows you a different 3x3 part. It shows you over here the values of the pixels. So in Fast.ai's case, our pixel values are between 0 and 1. In this case, they're between 0 and 255. So here are 9 pixel values. This area is pretty white, so they're pretty high numbers. And so as we move around, you can see the 9 big numbers change, and you can also see their colors change. Up here, there's another 9 numbers, and you can see those in the little x1, x2, x1. Here we are, 1, 2, 1. Now what you might see going on is as we move this little red block, as these numbers change, we then multiply them by the corresponding numbers up here. And so let's start using some nomenclature. The thing up here we are going to call the kernel, the convolutional kernel. So we're going to take each little 3x3 part of this image, and we're going to do an element-wise multiplication of each of the 9 pixels that we're mousing over with each of the 9 items in our kernel. And so once we multiply each set together, we can then add them all up. And that is what's shown on the right. As the little bunch of red things move over there, you can see there's one red thing that appears over here. The reason there's one red thing over here is because each set of 9, after getting through the element-wise multiplication of the kernel, get added together to create one output. So therefore, the size of this image has one pixel less on each edge than the original, as you can see. See how this black border is on it? That's because at the edge, the 3x3 kernel can't quite go any further. So the furthest you can go is to end up with a dot in the middle, just off the corner. So why are we doing this? Well perhaps you can see what's happened. This face has turned into some white parts outlining the horizontal edges. How? Well the how is just by doing this element-wise multiplication of each set of 9 pixels with this kernel, adding them together and sticking the result in the corresponding spot over here. Why is that creating white spots where the horizontal edges are? Well let's think about it. Let's look up here. So if we're just in this little bit here, then the spots above it are all pretty white, so they have high numbers. So the bits above it, the big numbers, are getting multiplied by 1, 2, 1. So that's going to create a big number. And the ones in the middle are all 0s, so I don't care about that. And then the ones underneath are all small numbers because they're all close to 0, so that really doesn't do much at all. So therefore, that little set there is going to end up with bright white. Whereas on the other side, down here, you've got light pixels underneath, so they're going to get a lot of negative. Dark pixels on top, which are very small, so not much happens. So therefore, over here we're going to end up with very negative. So this thing where we take each 3x3 area and element-wise multiply them with a kernel and add each of those up together to create one output is called a convolution. That's it. That's a convolution. So that might look familiar to you, because what we did back a while ago is we looked at that Zeiler and Fergus paper where we saw each different layer and we visualized what the weights were doing. Do you remember how the first layer was basically finding diagonal edges and gradients? That's because that's all a convolution can do. Each of our layers is just a convolution. So the first layer can do nothing more than this kind of thing. But the nice thing is the next layer could then take the results of this and it could kind of combine one channel, the output of one convolutional field is called a channel. So it could take one channel that found top edges and another channel that finds left edges and then the layer above that could take those two as input and create something that finds top left corners. As we saw when we looked at those Zeiler and Fergus visualizations. So let's take a look at this from another angle or quite a few other angles. And we're going to look at a fantastic post from a guy called Matt Clinesmith who was actually a student in the first year that we did this course. And he wrote this as a part of his project work back then. And what he's going to show here is here is our image. It's a 3x3 image and our kernel is a 2x2 kernel. And what we're going to do is we're going to apply this kernel to the top left 2x2 part of this image. And so the pink bit will be correspondingly multiplied by the pink bit, the green by the green and so forth. And they all get added up together to create this top left and the output. So in other words, P equals alpha times A, beta times B, gamma times D, delta times E. There it is. Plus B which is a bias. Okay, that's fine. That's just a normal bias. So you can see how basically each of these output pixels is a result of some different linear equation. That makes sense? And you can see these same four weights are being moved around because this is our convolutional kernel. Here's another way of looking at it from that, which is here is a classic neural network view. And so P now is a result of multiplying every one of these inputs by a weight and then adding them all together. The gray ones are going to have a value of zero. Because remember P was only connected to A, B, D and E. A, B, D and E. So in other words, remembering that this represents a matrix multiplication, therefore we can represent this as a matrix multiplication. So here is our list of pixels in our 3x3 image flattened out into a vector. And here is a matrix vector multiplication plus bias. And then a whole bunch of them we're just going to set to zero. So you can see here we've got a 0, 0, 0, 0, 0 which corresponds to 0, 0, 0, 0, 0. So in other words, a convolution is just a matrix multiplication where two things happen. Some of the entries are set to zero all the time and all of the ones of the same color always have the same weight. So when you've got multiple things with the same weight, that's called weight tying. So clearly we could implement a convolution using matrix multiplication, but we don't because it's slow. So in practice our libraries have specific convolution functions that we use. And they're basically doing this, which is this, which is this equation, which is the same as this matrix multiplication. And as we discussed, we have to think about padding because if you have a 3x3 kernel and a 3x3 image, then that can only create one pixel of output. There's only one place that this 3x3 can go. So if we want to create more than one pixel of output, we have to do something called padding which is to put additional numbers all around the outside. So what most libraries do is that they just put a layer of zeros, not a layer, a bunch of zeros all around the outside. So for a 3x3 kernel, a single zero on every edge piece here. And so once you've padded it like that, you can now move your 3x3 kernel all the way across and give you the same output size that you started with. Okay? Now, as we mentioned in Fast.ai, we don't normally necessarily use zero padding. Where possible, we use reflection padding. Although for these simple convolutions, we often use zero padding because it doesn't matter too much in a big image. It doesn't make too much difference. Okay. So that's what a convolution is. So a convolutional neural network wouldn't be very interesting if it can only create top edges. So we have to take it a little bit further. So if we have an input, and it might be standard kind of red, green, blue picture, then we can create a kernel, a 3x3 kernel, like so. And then we could pass that kernel over all of the different pixels. But if you think about it, we actually don't have a 2D input anymore. We have a 3D input, a rank 3 tensor. So we probably don't want to use the same kernel values for each of red and green and blue because, for example, if we're creating a green frog detector, we would want more activations on the green than we would on the blue. Or if we're trying to find something that can actually find a gradient that goes from green to blue, then the different kernels for each channel need to have different values in. So therefore, we need to create a 3x3x3 kernel. So this is still our kernel, and we're still going to vary it across the height and the width. But rather than doing an element-wise multiplication of nine things, we're going to do an element-wise multiplication of 27 things. 3x3x3. And we're still going to then add them up into a single number. So as we pass this cube over this and the kind of like little bit that's going to be sitting behind it, as we do that part of the convolution, it's still going to create just one number. Because we do an element-wise multiplication of all 27 and add them all together. So we can do that across the whole single unit padded input. And so we started with 1, 2, 3, 4, 5 by 5. So we're going to end up with an output that's also 5 by 5. But now, our input was three channels, and our output is only one channel. Now, we're not going to be able to do very much with just one channel, because all we've done now is found a top edge. How are we going to find a side edge and a gradient and an area of constant white? Well, we're going to have to create another kernel. And we're going to have to do that, convolved over the input, and that's going to create another 5 by 5. And then we can just stack those together across this as another axis. And we can do that lots and lots of times. And that's going to give us another rank 3 tensor output. So that's what happens in practice. In practice, we start with an input which is h by w by, for images, 3. We pass it through a bunch of convolutional kernels, and we can pick how many we want. And it gives us back an output of height by width by however many kernels we had. And so often that might be something like 16 in the first layer. And so now we've got 16 channels, they're called, 16 channels representing things like how much left edge was on this pixel, how much top edge was on this pixel, how much blue to red gradient was on this set of 27, well, 9 pixels each with RGB. And so then you can just do the same thing, right? You can have another bunch of kernels. And that's going to create another output, rank 3 tensor. Again, height by width by whatever, might still be 16. Now what we really like to do is as we get deeper in the network, we actually want to have more and more channels. We want to be able to find like a richer and richer set of features so that after a few, as we saw in the Zeiler and Ferguss paper, by layer 4 or 5, we've kind of got eyeball detectors and fur detectors and things, right? So you really need a lot of channels. So in order to avoid our memory going out of control, from time to time, we create a convolution where we don't step over every single set of 3 by 3, but instead we skip over 2 at a time. So we would start with a 3 by 3 centered at 2, 2, and then we'd jump over to 2, 4, 2, 6, 2, 8, and so forth. And that's called a stride 2 convolution. And so what that does is it looks exactly the same, right? It's still just a bunch of kernels. But we're just jumping over 2 at a time, right? We're skipping every alternate input pixel. And so the output from that will be h over 2 by w over 2. And so when we do that, we generally create twice as many kernels. So we can now have, say, 32 activations in each of those spots. And so that's what modern convolutional neural networks kind of tend to look like, right? And so we can actually see that if we go into our pets and we grab our CNN, right, and we're going to take a look at this particular cat. So if we go x, y equals valid data set, some index, so let's just grab the 0th. We'll go dot show, and we'll print out the value of y. Apparently this cat is of category main-koon. So until a week ago, I was not at all familiar that there's a cat called a main-koon. Having spent all week with this particular cat, I am now deeply familiar with this main-koon. So we can, if we go learn.summary, remember that our input we asked for was 352 by 352 pixels. Generally speaking, the very first convolution tends to have a stride 2. So after the first layer, it's 176 by 176. So this is learn.summary. We'll print out for you the output shape after every layer. 176 by 176. And the first set of convolutions has 64 activations. And we can actually see that if we type in learn.model, you can see here it's a 2D conv with three input channels and 64 output channels, at a stride of 2. And interestingly, it actually starts with a kernel size of 7 by 7. So like nearly all of the convolutions are 3 by 3. See, they're all 3 by 3. For reasons we'll talk about in part 2, we often use a larger kernel for the very first one. If you use a larger kernel, you have to use more padding. So we have to use kernel size, int divide by 2 padding to make sure we don't lose anything. So we now have 64 output channels. And since it was stride 2, it's now 176 by 176. And then as we go along, you'll see that from time to time, we have, go from 88 by 88 to 40 by 44, the grid size. So that was a 2D conv. And then when we do that, we generally double the number of channels. So we keep going through a few more cons. And as you can see, they've got batch norm and value. It's kind of pretty standard. And eventually we do it again. Another stride 2 conv, which again doubles. We've now got 512 by 11 by 11. And that's basically where we finish the main part of the network. We end up with 512 channels, 11 by 11. Okay, so we're actually at a point where we're going to be able to do this heat map now. So let's try and work through it. Before we do, I want to show you how you can do your own manual convolutions, because it's kind of fun. So we're going to start with this picture of a main coon. And I've created a convolutional kernel. And so as you can see, this one has a right edge and a bottom edge with positive numbers. And just inside that, it's got negative numbers. So I'm thinking this should show me bottom right edges. So that's my tensor. Now one complexity is that that 3 by 3 kernel cannot be used for this purpose, because I need two more dimensions. The first is I need the third dimension to say how to combine the red, green, and blue. So what I do is I say.expand, this is my 3 by 3, and I pop another 3 on the start. What.expand does is it says create a 3 by 3 by 3 tensor by simply copying this one three times. I mean honestly, it doesn't actually copy it. It pretends to have copied it, but it just basically refers to the same block of memory. So it kind of copies it in a memory efficient way. So this one here is now three copies of that. And the reason for that is that I want to treat red and green and blue the same way for this little manual kernel I'm showing you. And then we need one more axis, because rather than actually having a separate kernel, like I kind of printed these as if they were multiple kernels, what we actually do is we use a rank 4 tensor. And so the very first axis is for every separate kernel that we have. So in this case, I'm just going to create one kernel. So to do a convolution, I still have to put this unit axis on the front. So you can see k.shape is now 1, 3, 3, 3. So it's a 3 by 3 kernel. There are three of them. And then that's just the one kernel that I have. So it kind of takes a while to get the feel for these higher dimensional tensors, because we're not used to writing out the 4D tensor. But just think of them like this. A 4D tensor is just a bunch of 3D tensors sitting on top of each other. So this is our 4D tensor. And then you can just call conv2d, passing in some image. And so the image I'm going to use is the first part of my validation data set and the kernel. There's one more trick, which is that in PyTorch, pretty much everything is expecting to work on a mini-batch, not on an individual thing. So in our case, we have to create a mini-batch of size 1. So our original image is 3 channels by 352 by 352, height by width. Remember, PyTorch is channel by height by width. I want to create a mini-batch. So I need to create a rank 4 tensor, where the first axis is 1. In other words, it's a mini-batch of size 1, because that's what PyTorch expects. So there's something you can do in both PyTorch and NumPy, which is you can index into an array or a tensor with a special value none. And that creates a new unit axis in that point. So T is my image of dimensions 3 by 352 by 352. T none is a rank 4 tensor, a mini-batch of one image of 1 by 3 by 352 by 352. And so now I can go Conv2D and get back my cat, specifically my main croon. So that's how you can play around with convolutions yourself. So how are we going to do this to create a heat map? This is where things get fun. So what I mentioned was that I basically have my input, red, green, blue, and it goes through a bunch of convolutional layers. I'll just write a little line to say a convolutional layer to create activations which have more and more channels and eventually smaller and smaller height by width. Until eventually, remember we looked at the summary, we ended up with something which was 11 by 11 by 512. There's a whole bunch more layers that we skipped over. Now there are 37 classes, because remember data.c is the number of classes we have, and we can see that at the end here we end up with 37 features in our model. So that means that we end up with a probability for every one of the 37 breeds of cat and dog. So it's a vector of length 37. That's our final output that we need, because that's what we're going to compare implicitly to our one hot encoded matrix, which will have a 1 in the location for main croon. So somehow we need to get from this 11 by 11 by 512 to this 37. And so the way we do it is we actually take the average of every one of these 11 by 11 faces. We just take the main. So we're going to take the main of this first face, take the main, and that gets us one value, and then we'll take the second of the 512 faces and take that main, and that'll give us one more value. So we'll do that for every face, and that will give us a 512 long vector. And so now all we need to do is pop that through a single matrix multiply of 512 by 37, and that's going to give us an output vector of length 37. So this step here where we take the average of each face is called average pooling. So let's go back to our model and take a look. Here it is. Here is our final 512, and here is, we'll talk about what a concat pooling is in part two. For now we'll just focus on, this is a fast AI specialty. Everybody else just does this. Average pool. Average pool 2D with an output size of one. So here it is. Average pool 2D with an output size of one. And then, again, there's a bit of a special fast AI thing that we actually have two layers here, but normally people then just have the one linear layer with the input of 512 and the output of 37. So what that means is that this little box over here where we want a one for main coon, we've got to have a box over here which needs to have a high value in that place so that the loss will be low. So if we're going to have a high value there, the only way to get it is with this matrix multiplication is that it's going to represent a simple weighted linear combination of all of the 512 values here. So if we're going to be able to say, I'm pretty confident this is a main coon, just by taking the weighted sum of a bunch of inputs, those inputs are going to have to represent features like how fluffy is it, what color is its nose, how long is its legs, how pointy it is, all the kinds of things that can be used. Because for the other thing which figures out, is this a bulldog, it's got to use exactly the same kind of 512 inputs with a different set of weights, because that's all a matrix multiplication is. It's just a bunch of weighted sums, a different weighted sum for each output. So therefore we know that this, potentially dozens or even hundreds of layers of convolutions must have eventually come up with an 11 by 11 face for each of these features saying, in this little bit here, how much is that part of the image like a pointy ear? How much is it fluffy? How much is it like a long leg? How much is it like a very red nose? So that's what all of those things must represent. So each face is what we call, each of these represents a different feature. So the outputs of these we can think of as different features. So what we really want to know then is not so much what's the average across the 11 by 11 to get this set of outputs, but what we really want to know is what's in each of these 11 by 11 spots. So what if instead of averaging across the 11 by 11, let's instead average across the 512. If we average across the 512, that's going to give us a single 11 by 11 matrix. And each item, each grid point in that 11 by 11 matrix will be the average of how activated was that area. When it came to figuring out that this was a main Kuhn, how many signs of main Kuhnishness was there in that part of the 11 by 11 grid? And so that's actually what we do to create our heat map. So I think maybe the easiest way is to kind of work backwards. Here's our heat map. And it comes from something called average activations. And it's just a little bit of Matplotlib and FastAI. FastAI to show the image, and then Matplotlib to take the heat map which we passed in, which was called average activations, HM for heat map. Alpha.6 means make it a bit transparent. And Matplotlib extent means expand it from 11 by 11 to 352 by 352. Use bilinear interpolation so it's not all blocky. And use a different color map to kind of highlight things. So that's just a Matplotlib. It's not important. The key thing here is that average activations is the 11 by 11 matrix we wanted. Here it is. Average activations.shape is 11 by 11. So to get there we took the mean of activations across dimension 0, which is what I just said, in PyTorch the channel dimension is the first dimension. So the mean across dimension 0 took us from something of size 512 by 11 by 11, as promised, to something of 11 by 11. So therefore activations, acts, contains the activations we're averaging. Where did they come from? They came from something called a hook. So a hook is a really cool, more advanced PyTorch feature that lets you, as the name suggests, hook into the PyTorch machinery itself and run any arbitrary Python code you want to. It's a really amazing and nifty thing. Because you know normally when we do a forward pass through a PyTorch module, it gives us this set of outputs. But we know that in the process it's calculated these. So what I would like to do is I would like to hook into that forward pass and tell PyTorch, hey, when you calculate this, can you store it for me please? So what is this? This is the output of the convolutional part of the model. So the convolutional part of the model, which is everything before the average pull, is basically all of that. And so thinking back to transfer learning, remember with transfer learning, we actually cut off everything after the convolutional part of the model and replaced it with our own little bit. So with Fast.ai, the original convolutional part of the model is always going to be the first thing in the model. And specifically, it's always going to be called, assuming, so in this case I'm taking my model and I'm just going to call it M. So you can see M is this big thing, but always, at least in Fast.ai, always M0 will be the convolutional part of the model. So in this case we created a, let's go back and see, we created a ResNet34. So the main part of the ResNet34, the pre-trained bit we hold onto is in M0, and so this is basically it. This is the printout of the ResNet34, and at the end of it there is the 512 activations. So in other words, what we want to do is we want to grab M0 and we want to hook its output. So this is a really useful thing to be able to do. So Fast.ai has actually created something to do it for you, which is literally you say hook output and you pass in the PyTorch module that you want to hook the output of. And so most likely the thing you'll want to hook is the convolutional part of the model, and that's always going to be M0, or learn.model0. So we give that hook a name. Don't worry about this part, we'll learn about it next week. So having hooked the output, we now need to actually do the forward pass. And so remember in PyTorch to actually get it to calculate something, which is called doing the forward pass, you just act as if the model is a function. So we just pass in our x mini-batch. So we already had a main-coon image called x, but we can't quite pass that into our model. It has to be normalized and turned into a mini-batch and put onto the GPU. So Fast.ai has a thing called a data bunch, which we have in data, and you can always say data.one item to create a mini-batch with one thing in it. And as an exercise at home, you could try to create a mini-batch without using data.one item to make sure that you kind of learn how to normalize and stuff yourself if you want to. But this is how you can create a mini-batch with just one thing in it. And then I can pop that onto the GPU by saying.cuda. That's what I pass to my model. And so the predictions I get out actually don't care about, because the predictions is this thing, which is not what I want. So I'm not actually going to do anything with the predictions. The thing I care about is the hook that I just created. Now one thing to be aware of is that when you hook something in PyTorch, that means every single time you run that model, assuming you're hooking outputs, it's storing those outputs. And so you want to remove the hook when you've got what you want, because otherwise if you use the model again, it's going to keep hooking more and more outputs, which will be slow and memory intensive. So we've created this thing. Python calls it a context manager. You can use any hook as a context manager at the end of that with block. It'll remove the hook. So we've got our hook. And so now PyTorch hooks, sorry, FastAI hooks, always give you something called, or at least the output hooks, always give you something called.stored, which is where it stores away the thing you asked it to hook. And so that's where the activations now are. So we did a forward pass after hooking the output of the convolutional section of the model. We grabbed what it stored. We checked the shape. It was 512 by 11 by 11, as we predicted. We then took the mean of the channel access to get an 11 by 11 tensor. And then if we look at that, that's our picture. So there's a lot to unpack. But if you take your time going through these two sections, the convolutional kernel section and the heat map section of this notebook, like running those lines of code and changing them around a little bit, and remember the most important thing to look at is shape. You might have noticed when I'm showing you these notebooks, I very often print out the shape. And when you look at the shape, you want to be looking at how many axes are there. What's the rank of the tensor? And how many things are there in each axis? And try and think why. Try going back to the printout of the summary. Try going back to the actual list of the layers. Try and go back and think about the actual picture we drew. And think about what's actually going on. So that's a lot of technical content. So what I'm going to do now is switch from technical content to something much more important, unless we have some questions first. Because in the next lesson, we're going to be looking at generative models, both text and image generative models. And generative models are where you can create a new piece of text or a new image or a new video or a new sound. And as you probably are aware, this is the area that deep learning has developed the most in in the last 12 months. So we're now at a point where we can generate realistic looking videos, images, audio, and to some extent even text. And so there are many things in this journey which have ethical considerations, but perhaps this area of generative modeling is one of the largest ones. So before I got into it, I wanted to specifically touch on ethics and data science. Most of the stuff I'm showing you actually comes from Rachel. And Rachel has a really cool TEDx San Francisco talk that you can check out on YouTube. And a more extensive analysis of ethical principles and bias principles in AI, which you can find at this talk here. And she has a playlist that you can check out. We've already touched on an example of bias, which was this gender shades study, where if you remember, for example, lighter male skin people on IBM's main computer vision system, 99.7% accurate. And darker females are some hundreds of times less accurate in terms of error. So like extraordinary differences. And so it's interesting to kind of like, okay, it's first of all important to be aware that not only can this happen technically, that this can happen on a massive company's rolled out publicly available, highly marketed system that hundreds of quality control people have studied and lots of people are using. It's out there in the wild. They all look kind of crazy. And so it's interesting to think about why. And so one of the reasons why is that the data we feed these things. We tend to use, me included, a lot of these data sets kind of unthinkingly. But like ImageNet, which is the basis of like a lot of the computer vision stuff we do, is over half American and Great Britain. Like when it comes to the countries that actually have most of the population in the world, I can't even see them here. They're somewhere in these impossibly thin lines. Because remember, these data sets are being created almost exclusively by people in US, Great Britain, and nowadays increasingly also China. So there's a lot of bias in the content we're creating because of a bias in the kind of people that are creating that content, even when in theory it's being created in a very kind of neutral way. But you can't argue with the data, right? It's obviously not neutral at all. And so when you have biased data creating biased algorithms, you then need to say like, well, what are we doing with that? So we've spent a lot of time talking about image recognition. So a couple of years ago, this company DeepLint advertised their image recognition system, which can be used to do mass surveillance on large crowds of people. And any person passing through who is a person of interest in theory. And so putting aside even the question of like, is it a good idea to have such a system? You've got to think, is it a good idea to have such a system where certain kinds of people are 300 times more likely to be misidentified? And then thinking about it, so this is now starting to happen in America, right? These systems are being rolled out. And so there are now systems in America that will identify a person of interest in a video and send a ping to the local police. And so these systems are extremely inaccurate and extremely biased. And what happens then, of course, is if you're in a predominantly black neighborhood where the probability of successfully recognizing you is much lower, and you're much more likely to be surrounded by black people, and so suddenly all of these black people are popping up as persons of interest or in a video of a person of interest, all the people in the video are all recognized as in the vicinity of the person of interest, you suddenly get all these pings going off the local police department, causing the police to run down there, and therefore likely to lead to a larger number of arrests, which is then likely to feed back into the data being used to develop the systems. So this is happening right now. And so thankfully a very small number of people are actually bothering to look into these things. I mean ridiculously small, but at least it's better than nothing. And so for example, now one of the best ways that people get publicity is to do kind of funny experiments like, let's try the mugshot image recognition system that's being widely used and try it against the members of Congress and find out that there are 28 members of Congress who would have been identified by this system, obviously incorrectly. Oh, I didn't know that. Okay, members of Congress, black members of Congress, not at all surprised to hear that. Thank you, Rachel. We see this kind of bias in a lot of the systems we use, not just image recognition, but text translation when you convert she is a doctor, he is a nurse, into Turkish, you quite correctly get a gender in specific pronoun because that's what Turkish uses. You could then take that and feed it back into Turkish with your gender in specific pronoun and you will now get he is a doctor, she is a nurse. So the bias again, this is in a massively widely rolled out carefully studied system and it's not like even these kind of things like a little one off things that then get fixed quickly. These issues have been identified in Google Translate for a very long time and they're still there and they don't get fixed. So the kind of results of this are, in my opinion, quite terrifying because what's happening is that in many countries, including America, where I'm speaking from now, algorithms are increasingly being used for all kinds of public policy, judicial and so forth purposes. For example, there's a system called Compass, which is very widely used to decide who's going to jail. And it does that in a couple of ways. It tells judges what sentencing guidelines they should use for particular cases and it tells them also which people the system says should be let out on bail. But here's the thing, white people, it keeps on saying let this person out even though they end up reoffending and vice versa. It's systematically out by double compared to what it should be in terms of getting it wrong with white people versus black people. So this is kind of horrifying because, I mean, amongst other things, the data that it's using in this system is literally asking people questions about things like, did any of your parents ever go to jail, do any of your friends do drugs? Like, they're asking questions about other people who they have no control over. So not only are these systems biased, very systematically biased, but they're also being done on the basis of data which is totally out of your control. So this is kind of, did you want to add something to that? Oh yeah, are your parents divorced is another question that's being used to decide whether you go to jail or not. So when we raise these issues kind of on Twitter or in talks or whatever, there's always a few people, always white men, a few people who will always say like, that's just the way the world is, that's just reflecting what the data shows. But when you actually look at it, it's not. It's actually systematically erroneous, and systematically erroneous against people of color, minorities, the people who are less involved in creating the systems that these products are based on. Sometimes this can go a really long way. So for example, in Myanmar, there was a genocide of the Rohingya people. And that genocide was very heavily created by Facebook. Not because anybody at Facebook wanted it, I mean, heavens know. I know a lot of people at Facebook, I have a lot of friends at Facebook, they're really trying to do the right thing. They're really trying to create a product that people like, but not in a thoughtful enough way. Because when you roll out something where literally in Myanmar, a country that most people didn't have, most, maybe half of people didn't have electricity until very recently, and you say, hey, you can all have free internet, as long as it's just Facebook. You've got to think carefully about what you're doing. And then you use algorithms to feed people the stuff they will click on. And of course, what people click on is stuff which is controversial, stuff that makes their blood boil. So when they actually started asking the generals in the Myanmar army that were literally throwing babies onto bonfires, they were saying, we know that these are not humans. We know that they are animals, because we read the news, we read the internet. Because this is the stories that the algorithms are pushing. And the algorithms are pushing the stories because the algorithms are good. They know how to create eyeballs, how to get people watching, and how to get people clicking. And again, nobody at Facebook said, let's cause a massive genocide in Myanmar. They said, let's maximize the engagement of people in this new market on our platform. So they very successfully maximized engagement. Yes. It's just important to note, people warned executives at Facebook how the platform was being used to incite violence as far back as 2013, 2014, 2015. In 2015, someone even warned executives that Facebook could be used in Myanmar in the same way that the radio broadcasts were used in Rwanda during the Rwandan genocide. And as of 2015, Facebook only had four contractors who spoke Burmese working for them. They really did not put many resources into the issue at all, even though they were getting very alarming warnings about it. So why does this happen? A part of the issue is that ethics is complicated. And you will not find Rachel or I telling you how to do ethics, how do you fix this? We don't know. We can just give you things to think about. Another part of the problem we keep hearing is, it's not my problem, I'm just a researcher, I'm just a techie, I'm just building a data set. I'm not part of the problem, I'm part of this foundation that's far enough away that I can imagine that I'm not part of this. But if you're creating ImageNet and you want it to be successful, you want lots of people to use it, you want lots of people to build products on it, lots of people to do research on top of it, if you're trying to create something that people are using, you want them to use, then please try to make it something that won't cause massive amounts of harm and doesn't have massive amounts of bias. And it can actually come back and bite you in the ass. The Volkswagen engineer who ended up actually encoding the thing that made them systematically cheat on their diesel emissions tests, on their pollution tests, ended up in jail. Not because it was their decision to cheat on the tests, but because their manager told them to write that code, and they wrote the code. And therefore they were the ones that ended up being criminally responsible and they were the ones that were jailed. So if you do in some way a shitty thing that ends up causing trouble, that can absolutely come back around and get you in trouble as well. Sometimes it can cause huge amounts of trouble. So if we go back to World War II, then this was one of the first great opportunities for IBM to show off their amazing tabulating system. And they had a huge client in Nazi Germany. And Nazi Germany used this amazing new tabulating system to encode all of the different types of Jews that they had in the country and all the different types of problem people. So Jews were 8, gypsies were 12, then different outcomes were coded, executions were a 4, death in a gas chamber was 6. A Swiss judge ruled that IBM was actively involved facilitating the commission of these crimes against humanity. So there are absolutely plenty of examples of people building data processing technology that are directly causing deaths. Sometimes millions of deaths. So we don't want to be one of those people. And so you might have thought, oh, you know, I'm just creating some data processing software and somebody else is thinking I'm just a salesperson and somebody else is thinking I'm just the biz dev person opening new markets, but it all comes together. So we need to care. And so one of the things we need to care about is getting humans back in the loop. And so when we pull humans out of the loop, it's one of the first times that trouble happens. I don't know if you remember, I remember this very clearly when I first heard that Facebook was firing the human editors that were responsible for basically curating the news that ended up on the Facebook pages. And I've got to say at the time I thought that's a recipe for disaster because I've seen again and again that humans can be the person in the loop that can realize this isn't right. You know, it's very hard to create an algorithm that can recognize this isn't right or else humans are very good at that. And we saw that's what happened. Right after Facebook fired the human editors, the nature of stories on Facebook dramatically changed that you started seeing this proliferation of conspiracy theories and the kind of the algorithms went crazy with recommending more and more controversial topics. And of course that changed people's consumption behavior, causing them to want more and more controversial topics. So one of the really interesting places this comes in, and Cathy O'Neill who's got a great book called Weapons of Math Destruction, thank you Rachel, and many others have pointed out, is that what happens to algorithms is that they end up impacting people. For example, Compass sentencing guidelines go to a judge. Now you can say the algorithm is very good, in Compass's case it isn't, it actually turned out to be about as bad as random because it's a black box and all that. But even if it was very good, you could then say, well, you know, the judge is getting the algorithm, otherwise they'd just be getting a person, people also give bad advice, so what? Humans respond differently to algorithms. It's very common, particularly for a human that is not very familiar with the technology themselves, like a judge, to see like, oh, that's what the computer says. The computer looked it up and it figured this out. It's extremely difficult to get a non-technical audience to look at a computer recommendation and come up with a nuanced decision-making process. So what we see is that algorithms are often put into place with no appeals process. They're often used to massively scale up decision-making systems because they're cheap. And then the people that are using the outputs of those algorithms tend to give them more credence than they deserve because very often they're being used by people that don't have the technical competence to judge them themselves. So great example, right, was here's an example of somebody who lost their healthcare. And they lost their healthcare because of an error in a new algorithm that was systematically failing to recognize that there are many people that need help with, was it Alzheimer's? Cerebral palsy and diabetes. Thanks, Rachel. And so this system, which had this error that was later discovered, was cutting off these people from the home care that they needed so that cerebral palsy victims no longer had the care they needed. So their life was destroyed, basically. And so when the person that created that algorithm with the error was asked about this and was specifically said, should they have found a better way to communicate the system, the strengths, the failures and so forth, he said, yeah, I should probably also dust under my bed. That was there. That was the level of interest they had. And this is extremely common. I hear this all the time. And it's much easier to kind of see it from afar and say, okay, after the problems happened, I can see that that's a really shitty thing to say. But it can be very difficult when you're kind of in the middle of it. I just want to say one more thing about that example. And that's that this was a case where it was separate. There was someone who created the algorithm. Then I think different people implemented the software. And this is in use in over half of the 50 states. And then there was also the particular policy decisions made by that state. And so this is one of those situations where nobody felt responsible because the algorithm creators like, oh, no, it's the policy decisions of the state that were bad. And the state can be like, oh, no, it's the ones who implemented the software. And so everyone's just kind of pointing fingers and not taking responsibility. And in some ways, maybe it's unfair. But I would argue the person who is creating the data set and the person who is implementing the algorithm is the person best placed to get out there and say, hey, here are the things you need to be careful of and make sure that they're a part of the implementation process. So we've also seen this with YouTube, right? It's kind of similar to what happened with Facebook. And we're now seeing, we've heard examples of students watching the Fast AI courses who say, hey, Jeremy and Rachel watching the Fast AI courses really enjoyed them. And at the end of one of them, the YouTube autoplay fed me across to a conspiracy theory. And what happens is that once the system decides that you like the conspiracy theories, it's going to just feed you more and more. And then what happens is that, please go on. Just briefly, you don't even have to like conspiracy theories. The goal is to get as many people hooked on conspiracy theories as possible, is what the algorithm is trying to do, kind of whether or not you've expressed interest. Right. And so the interesting thing, again, is I know plenty of people involved in YouTube recommendation systems. None of them are wanting to promote conspiracy theories. But people click on them, right? And people share them. And what tends to happen is also people that are into conspiracy theories consume a lot more YouTube media. So it actually is very good at finding a market that watches a lot of hours of YouTube. And then it makes that market watch even more. So this is an example of a feedback loop. And the New York Times is now describing YouTube as perhaps the most powerful radicalizing instrument of the 21st century. I can tell you, my friends that worked on the YouTube recommendation system did not think they were creating the most powerful radicalizing instrument of the 21st century. And to be honest, most of them today, when I talk to them, still think they're not. They think it's all bullshit. You know, not all of them. But a lot of them now are at the point where they just feel like they're the victims here. People are unfairly, you know, they don't get it. They don't understand what we're trying to do. It's very, very difficult when you're right out there in the heart of it. So you've got to be thinking from right at the start, what are the possible unintended consequences of what you're working on? And as the technical people involved, how can you get out in front and make sure that people are aware of them? And I just also wanted to say that in particular, many of these conspiracy theories are promoting white supremacy, they're, you know, kind of far-right ethno-nationalism, anti-science. And I think, I don't know, maybe five or ten years ago, I would have thought conspiracy theories are more a more fringe thing. But we're seeing the kind of huge societal impact it can have for many people to believe these. Yeah, and you know, partly it's you see them on YouTube all the time, it starts to feel a lot more normal, right? So one of the things that people are doing to try to say like how to fix this problem is to explicitly get involved in talking to the people who might or will be impacted by the kind of decision-making processes that you're enabling. So for example, there was a really cool thing recently where literally statisticians and data scientists got together with people who had been inside the criminal system, I had gone through the bail and sentencing process of criminals themselves, and talking to the lawyers who worked with them and put them together with the data scientists and actually kind of put together a timeline of how exactly does it work and where exactly the places that there are inputs and how do people respond to them and who's involved. This is really cool, right? This is the only way for you as a kind of a data product developer to actually know how your data product's going to be working. A really great example of somebody who did a great job here was Evanistola at Meetup who said hey, a lot of men are going to our tech meetups and if we use a recommendation system naively, it's going to recommend more tech meetups to men, which is going to cause more men to go to them, and then when women do try to go, they'll be like oh my god there's so many men here, which is going to cause more men to go to the tech meetups. So showing recommendations to men and therefore not showing them to women. So what Evan and Meetup decided was to make an explicit product decision that this would not even be representing the actual true preferences of people, it would be creating a real data runaway feedback loop, so let's explicitly stop it before it happens and not recommend less tech meetups to women and more tech meetups to men. And so I think that's really cool. It's like saying we don't have to be slaves to the algorithm, we actually get to decide. Another thing that people can do to help is regulation. And normally when we talk about regulation, there's a natural reaction of how do you regulate these things? It's ridiculous, you can't regulate AI. But actually when you look at it again and again, and this fantastic paper called Datasheets for Datasets has lots of examples of this, there are many many examples of industries where people thought they couldn't be regulated, people thought that's just how it was, like cars, people died in cars all the time because they literally had sharp metal knobs on dashboards, steering columns weren't collapsible, and all of the discussion in the community was that's just how cars are and when people die in cars it's because of the people. But then eventually the regulations did come in, and today driving is dramatically safer, like dozens and dozens of times safer than it was before. So often there are things we can do through policy. So to summarize, we are part of the 0.3 to 0.5% of the world that knows how to code. We have a skill that very few other people do. Not only that, we now know how to code deep learning algorithms, which is like the most powerful kind of code I know. So I'm hoping that we can explicitly think about at least not making the world worse and perhaps explicitly making it better. And so why is this interesting to you as an audience in particular? And that's because Fast.ai in particular is trying to make it easy for domain experts to use deep learning. And so this picture of the goats here is an example of one of our international fellows from a previous course who was a goat dairy farmer and told us that they were going to use deep learning on their remote Canadian island to help study utter disease in goats. And to me this is a great example of like a domain experts problem which nobody else even knows about, let alone know that it's a computer vision problem that can be solved with deep learning. So in your field, whatever it is, you probably know a lot more now about the opportunities in your field to make it a hell of a lot better than it was before. You'll probably be able to come up with all kinds of cool product ideas, maybe build a startup or create a new product group in your company or whatever. But also please be thinking about what that's going to mean in practice and think about where can you put humans in the loop? Where can you put those pressure release valves? Who are the people you can talk to who could be impacted who could help you understand? And get the kind of humanities folks involved to understand history and psychology and sociology and so forth. So that's our plea to you. If you've got this far, you're definitely at a point now where you're ready to make a serious impact on the world. So I hope we can make sure that that's a positive impact. See you next week. If you can make it here with just one practice session.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.4, "text": " All right, welcome to lesson 6 where we're going to do a deep dive into computer vision,", "tokens": [1057, 558, 11, 2928, 281, 6898, 1386, 689, 321, 434, 516, 281, 360, 257, 2452, 9192, 666, 3820, 5201, 11], "temperature": 0.0, "avg_logprob": -0.16667276620864868, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.025942165404558182}, {"id": 1, "seek": 0, "start": 9.4, "end": 18.080000000000002, "text": " convolutional neural networks, what is a convolution, and we're also going to learn the final regularization", "tokens": [45216, 304, 18161, 9590, 11, 437, 307, 257, 45216, 11, 293, 321, 434, 611, 516, 281, 1466, 264, 2572, 3890, 2144], "temperature": 0.0, "avg_logprob": -0.16667276620864868, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.025942165404558182}, {"id": 2, "seek": 0, "start": 18.080000000000002, "end": 27.560000000000002, "text": " tricks after last lesson learning about weight decay and slash L2 regularization.", "tokens": [11733, 934, 1036, 6898, 2539, 466, 3364, 21039, 293, 17330, 441, 17, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.16667276620864868, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.025942165404558182}, {"id": 3, "seek": 2756, "start": 27.56, "end": 32.76, "text": " I want to start by showing you something that I'm really excited about and I've had a small", "tokens": [286, 528, 281, 722, 538, 4099, 291, 746, 300, 286, 478, 534, 2919, 466, 293, 286, 600, 632, 257, 1359], "temperature": 0.0, "avg_logprob": -0.12179739335004021, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.00014649046352133155}, {"id": 4, "seek": 2756, "start": 32.76, "end": 36.2, "text": " hand in helping to create.", "tokens": [1011, 294, 4315, 281, 1884, 13], "temperature": 0.0, "avg_logprob": -0.12179739335004021, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.00014649046352133155}, {"id": 5, "seek": 2756, "start": 36.2, "end": 43.0, "text": " For those of you that saw my talk on TED.com, you might have noticed this really interesting", "tokens": [1171, 729, 295, 291, 300, 1866, 452, 751, 322, 43036, 13, 1112, 11, 291, 1062, 362, 5694, 341, 534, 1880], "temperature": 0.0, "avg_logprob": -0.12179739335004021, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.00014649046352133155}, {"id": 6, "seek": 2756, "start": 43.0, "end": 49.599999999999994, "text": " demo that we did about four years ago showing a way to quickly build models with unlabeled", "tokens": [10723, 300, 321, 630, 466, 1451, 924, 2057, 4099, 257, 636, 281, 2661, 1322, 5245, 365, 32118, 18657, 292], "temperature": 0.0, "avg_logprob": -0.12179739335004021, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.00014649046352133155}, {"id": 7, "seek": 2756, "start": 49.599999999999994, "end": 52.34, "text": " data.", "tokens": [1412, 13], "temperature": 0.0, "avg_logprob": -0.12179739335004021, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.00014649046352133155}, {"id": 8, "seek": 2756, "start": 52.34, "end": 57.36, "text": " It's been four years, but we're finally at a point where we're ready to put this out", "tokens": [467, 311, 668, 1451, 924, 11, 457, 321, 434, 2721, 412, 257, 935, 689, 321, 434, 1919, 281, 829, 341, 484], "temperature": 0.0, "avg_logprob": -0.12179739335004021, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.00014649046352133155}, {"id": 9, "seek": 5736, "start": 57.36, "end": 59.2, "text": " in the world and let people use it.", "tokens": [294, 264, 1002, 293, 718, 561, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.13033324540263475, "compression_ratio": 1.705069124423963, "no_speech_prob": 8.34798629512079e-05}, {"id": 10, "seek": 5736, "start": 59.2, "end": 64.8, "text": " The first people we're going to let use it are you folks.", "tokens": [440, 700, 561, 321, 434, 516, 281, 718, 764, 309, 366, 291, 4024, 13], "temperature": 0.0, "avg_logprob": -0.13033324540263475, "compression_ratio": 1.705069124423963, "no_speech_prob": 8.34798629512079e-05}, {"id": 11, "seek": 5736, "start": 64.8, "end": 68.88, "text": " The company is called Platform.ai and the reason I'm mentioning it here is that it's", "tokens": [440, 2237, 307, 1219, 28707, 13, 1301, 293, 264, 1778, 286, 478, 18315, 309, 510, 307, 300, 309, 311], "temperature": 0.0, "avg_logprob": -0.13033324540263475, "compression_ratio": 1.705069124423963, "no_speech_prob": 8.34798629512079e-05}, {"id": 12, "seek": 5736, "start": 68.88, "end": 74.68, "text": " going to let you create models on different types of datasets to what you can do now,", "tokens": [516, 281, 718, 291, 1884, 5245, 322, 819, 3467, 295, 42856, 281, 437, 291, 393, 360, 586, 11], "temperature": 0.0, "avg_logprob": -0.13033324540263475, "compression_ratio": 1.705069124423963, "no_speech_prob": 8.34798629512079e-05}, {"id": 13, "seek": 5736, "start": 74.68, "end": 78.56, "text": " that is to say, datasets that you don't have labels for yet.", "tokens": [300, 307, 281, 584, 11, 42856, 300, 291, 500, 380, 362, 16949, 337, 1939, 13], "temperature": 0.0, "avg_logprob": -0.13033324540263475, "compression_ratio": 1.705069124423963, "no_speech_prob": 8.34798629512079e-05}, {"id": 14, "seek": 5736, "start": 78.56, "end": 81.88, "text": " We're actually going to help you label them.", "tokens": [492, 434, 767, 516, 281, 854, 291, 7645, 552, 13], "temperature": 0.0, "avg_logprob": -0.13033324540263475, "compression_ratio": 1.705069124423963, "no_speech_prob": 8.34798629512079e-05}, {"id": 15, "seek": 8188, "start": 81.88, "end": 95.0, "text": " This is the first time this has been shown before, so I'm pretty thrilled about it.", "tokens": [639, 307, 264, 700, 565, 341, 575, 668, 4898, 949, 11, 370, 286, 478, 1238, 18744, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.16565573842901932, "compression_ratio": 1.3377483443708609, "no_speech_prob": 7.295976047316799e-06}, {"id": 16, "seek": 8188, "start": 95.0, "end": 97.56, "text": " Let me give you a quick demo.", "tokens": [961, 385, 976, 291, 257, 1702, 10723, 13], "temperature": 0.0, "avg_logprob": -0.16565573842901932, "compression_ratio": 1.3377483443708609, "no_speech_prob": 7.295976047316799e-06}, {"id": 17, "seek": 8188, "start": 97.56, "end": 107.08, "text": " If you go to Platform.ai and choose get started, you'll be able to create a new project.", "tokens": [759, 291, 352, 281, 28707, 13, 1301, 293, 2826, 483, 1409, 11, 291, 603, 312, 1075, 281, 1884, 257, 777, 1716, 13], "temperature": 0.0, "avg_logprob": -0.16565573842901932, "compression_ratio": 1.3377483443708609, "no_speech_prob": 7.295976047316799e-06}, {"id": 18, "seek": 10708, "start": 107.08, "end": 112.67999999999999, "text": " If you create a new project, you can either upload your own images.", "tokens": [759, 291, 1884, 257, 777, 1716, 11, 291, 393, 2139, 6580, 428, 1065, 5267, 13], "temperature": 0.0, "avg_logprob": -0.11625524248395647, "compression_ratio": 1.6371308016877637, "no_speech_prob": 1.0614708116918337e-05}, {"id": 19, "seek": 10708, "start": 112.67999999999999, "end": 115.52, "text": " Uploading about 500 or so works pretty well.", "tokens": [624, 21132, 8166, 466, 5923, 420, 370, 1985, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.11625524248395647, "compression_ratio": 1.6371308016877637, "no_speech_prob": 1.0614708116918337e-05}, {"id": 20, "seek": 10708, "start": 115.52, "end": 121.03999999999999, "text": " You can upload a few thousand, but to start upload 500 or so, they all have to be in a", "tokens": [509, 393, 6580, 257, 1326, 4714, 11, 457, 281, 722, 6580, 5923, 420, 370, 11, 436, 439, 362, 281, 312, 294, 257], "temperature": 0.0, "avg_logprob": -0.11625524248395647, "compression_ratio": 1.6371308016877637, "no_speech_prob": 1.0614708116918337e-05}, {"id": 21, "seek": 10708, "start": 121.03999999999999, "end": 124.03999999999999, "text": " single folder.", "tokens": [2167, 10820, 13], "temperature": 0.0, "avg_logprob": -0.11625524248395647, "compression_ratio": 1.6371308016877637, "no_speech_prob": 1.0614708116918337e-05}, {"id": 22, "seek": 10708, "start": 124.03999999999999, "end": 126.52, "text": " We're assuming that you've got a whole bunch of images that you haven't got any labels", "tokens": [492, 434, 11926, 300, 291, 600, 658, 257, 1379, 3840, 295, 5267, 300, 291, 2378, 380, 658, 604, 16949], "temperature": 0.0, "avg_logprob": -0.11625524248395647, "compression_ratio": 1.6371308016877637, "no_speech_prob": 1.0614708116918337e-05}, {"id": 23, "seek": 10708, "start": 126.52, "end": 127.52, "text": " for.", "tokens": [337, 13], "temperature": 0.0, "avg_logprob": -0.11625524248395647, "compression_ratio": 1.6371308016877637, "no_speech_prob": 1.0614708116918337e-05}, {"id": 24, "seek": 10708, "start": 127.52, "end": 131.24, "text": " Or you can start with one of the existing collections if you want to play around.", "tokens": [1610, 291, 393, 722, 365, 472, 295, 264, 6741, 16641, 498, 291, 528, 281, 862, 926, 13], "temperature": 0.0, "avg_logprob": -0.11625524248395647, "compression_ratio": 1.6371308016877637, "no_speech_prob": 1.0614708116918337e-05}, {"id": 25, "seek": 13124, "start": 131.24, "end": 139.36, "text": " I've started with the cars collection, going back to what we did four years ago.", "tokens": [286, 600, 1409, 365, 264, 5163, 5765, 11, 516, 646, 281, 437, 321, 630, 1451, 924, 2057, 13], "temperature": 0.0, "avg_logprob": -0.11626320574657027, "compression_ratio": 1.497737556561086, "no_speech_prob": 5.954951120656915e-06}, {"id": 26, "seek": 13124, "start": 139.36, "end": 145.0, "text": " This is what happens when you first go into Platform.ai and look at the collection of", "tokens": [639, 307, 437, 2314, 562, 291, 700, 352, 666, 28707, 13, 1301, 293, 574, 412, 264, 5765, 295], "temperature": 0.0, "avg_logprob": -0.11626320574657027, "compression_ratio": 1.497737556561086, "no_speech_prob": 5.954951120656915e-06}, {"id": 27, "seek": 13124, "start": 145.0, "end": 146.4, "text": " images you uploaded.", "tokens": [5267, 291, 17135, 13], "temperature": 0.0, "avg_logprob": -0.11626320574657027, "compression_ratio": 1.497737556561086, "no_speech_prob": 5.954951120656915e-06}, {"id": 28, "seek": 13124, "start": 146.4, "end": 150.60000000000002, "text": " A random sample of them will appear on the screen.", "tokens": [316, 4974, 6889, 295, 552, 486, 4204, 322, 264, 2568, 13], "temperature": 0.0, "avg_logprob": -0.11626320574657027, "compression_ratio": 1.497737556561086, "no_speech_prob": 5.954951120656915e-06}, {"id": 29, "seek": 13124, "start": 150.60000000000002, "end": 158.88, "text": " As you'll recognize, probably, they are projected from a deep learning space into a 2D space", "tokens": [1018, 291, 603, 5521, 11, 1391, 11, 436, 366, 26231, 490, 257, 2452, 2539, 1901, 666, 257, 568, 35, 1901], "temperature": 0.0, "avg_logprob": -0.11626320574657027, "compression_ratio": 1.497737556561086, "no_speech_prob": 5.954951120656915e-06}, {"id": 30, "seek": 15888, "start": 158.88, "end": 161.4, "text": " using a pre-trained model.", "tokens": [1228, 257, 659, 12, 17227, 2001, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11254121742996515, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.2029005119984504e-05}, {"id": 31, "seek": 15888, "start": 161.4, "end": 164.32, "text": " For this initial version, it's an ImageNet model we're using.", "tokens": [1171, 341, 5883, 3037, 11, 309, 311, 364, 29903, 31890, 2316, 321, 434, 1228, 13], "temperature": 0.0, "avg_logprob": -0.11254121742996515, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.2029005119984504e-05}, {"id": 32, "seek": 15888, "start": 164.32, "end": 170.16, "text": " As things move along, we'll be adding more and more pre-trained models.", "tokens": [1018, 721, 1286, 2051, 11, 321, 603, 312, 5127, 544, 293, 544, 659, 12, 17227, 2001, 5245, 13], "temperature": 0.0, "avg_logprob": -0.11254121742996515, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.2029005119984504e-05}, {"id": 33, "seek": 15888, "start": 170.16, "end": 176.18, "text": " What I'm going to do is I want to add labels to this data set representing which angle", "tokens": [708, 286, 478, 516, 281, 360, 307, 286, 528, 281, 909, 16949, 281, 341, 1412, 992, 13460, 597, 5802], "temperature": 0.0, "avg_logprob": -0.11254121742996515, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.2029005119984504e-05}, {"id": 34, "seek": 15888, "start": 176.18, "end": 181.35999999999999, "text": " a photo of the car was taken from, which is something that actually ImageNet is going", "tokens": [257, 5052, 295, 264, 1032, 390, 2726, 490, 11, 597, 307, 746, 300, 767, 29903, 31890, 307, 516], "temperature": 0.0, "avg_logprob": -0.11254121742996515, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.2029005119984504e-05}, {"id": 35, "seek": 15888, "start": 181.35999999999999, "end": 183.4, "text": " to be really bad at, isn't it?", "tokens": [281, 312, 534, 1578, 412, 11, 1943, 380, 309, 30], "temperature": 0.0, "avg_logprob": -0.11254121742996515, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.2029005119984504e-05}, {"id": 36, "seek": 18340, "start": 183.4, "end": 189.08, "text": " Because ImageNet has learned to recognize the difference between cars versus bicycles,", "tokens": [1436, 29903, 31890, 575, 3264, 281, 5521, 264, 2649, 1296, 5163, 5717, 47913, 11], "temperature": 0.0, "avg_logprob": -0.08888958409889457, "compression_ratio": 1.7258064516129032, "no_speech_prob": 2.994408077938715e-06}, {"id": 37, "seek": 18340, "start": 189.08, "end": 193.64000000000001, "text": " and ImageNet knows that the angle you take a photo on actually doesn't matter.", "tokens": [293, 29903, 31890, 3255, 300, 264, 5802, 291, 747, 257, 5052, 322, 767, 1177, 380, 1871, 13], "temperature": 0.0, "avg_logprob": -0.08888958409889457, "compression_ratio": 1.7258064516129032, "no_speech_prob": 2.994408077938715e-06}, {"id": 38, "seek": 18340, "start": 193.64000000000001, "end": 198.9, "text": " So we want to try and create labels using the kind of thing that actually ImageNet specifically", "tokens": [407, 321, 528, 281, 853, 293, 1884, 16949, 1228, 264, 733, 295, 551, 300, 767, 29903, 31890, 4682], "temperature": 0.0, "avg_logprob": -0.08888958409889457, "compression_ratio": 1.7258064516129032, "no_speech_prob": 2.994408077938715e-06}, {"id": 39, "seek": 18340, "start": 198.9, "end": 200.84, "text": " learned to ignore.", "tokens": [3264, 281, 11200, 13], "temperature": 0.0, "avg_logprob": -0.08888958409889457, "compression_ratio": 1.7258064516129032, "no_speech_prob": 2.994408077938715e-06}, {"id": 40, "seek": 18340, "start": 200.84, "end": 206.72, "text": " So the projection that you see, we can click these layer buttons at the top to switch to", "tokens": [407, 264, 22743, 300, 291, 536, 11, 321, 393, 2052, 613, 4583, 9905, 412, 264, 1192, 281, 3679, 281], "temperature": 0.0, "avg_logprob": -0.08888958409889457, "compression_ratio": 1.7258064516129032, "no_speech_prob": 2.994408077938715e-06}, {"id": 41, "seek": 18340, "start": 206.72, "end": 212.4, "text": " user projection using a different layer of the neural net.", "tokens": [4195, 22743, 1228, 257, 819, 4583, 295, 264, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.08888958409889457, "compression_ratio": 1.7258064516129032, "no_speech_prob": 2.994408077938715e-06}, {"id": 42, "seek": 21240, "start": 212.4, "end": 216.84, "text": " And so here's the last layer, which is going to be a total waste of time for us because", "tokens": [400, 370, 510, 311, 264, 1036, 4583, 11, 597, 307, 516, 281, 312, 257, 3217, 5964, 295, 565, 337, 505, 570], "temperature": 0.0, "avg_logprob": -0.08421444702148438, "compression_ratio": 1.8721804511278195, "no_speech_prob": 1.3630819012178108e-05}, {"id": 43, "seek": 21240, "start": 216.84, "end": 222.04000000000002, "text": " it's really going to be projecting things based on what kind of thing it thinks it is.", "tokens": [309, 311, 534, 516, 281, 312, 43001, 721, 2361, 322, 437, 733, 295, 551, 309, 7309, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.08421444702148438, "compression_ratio": 1.8721804511278195, "no_speech_prob": 1.3630819012178108e-05}, {"id": 44, "seek": 21240, "start": 222.04000000000002, "end": 224.96, "text": " And the first layer is probably going to be a waste of time for us as well because there's", "tokens": [400, 264, 700, 4583, 307, 1391, 516, 281, 312, 257, 5964, 295, 565, 337, 505, 382, 731, 570, 456, 311], "temperature": 0.0, "avg_logprob": -0.08421444702148438, "compression_ratio": 1.8721804511278195, "no_speech_prob": 1.3630819012178108e-05}, {"id": 45, "seek": 21240, "start": 224.96, "end": 229.24, "text": " very little interesting semantic content there.", "tokens": [588, 707, 1880, 47982, 2701, 456, 13], "temperature": 0.0, "avg_logprob": -0.08421444702148438, "compression_ratio": 1.8721804511278195, "no_speech_prob": 1.3630819012178108e-05}, {"id": 46, "seek": 21240, "start": 229.24, "end": 235.32, "text": " But if I go into the middle in layer 3, we may well be able to find some differences", "tokens": [583, 498, 286, 352, 666, 264, 2808, 294, 4583, 805, 11, 321, 815, 731, 312, 1075, 281, 915, 512, 7300], "temperature": 0.0, "avg_logprob": -0.08421444702148438, "compression_ratio": 1.8721804511278195, "no_speech_prob": 1.3630819012178108e-05}, {"id": 47, "seek": 21240, "start": 235.32, "end": 236.46, "text": " there.", "tokens": [456, 13], "temperature": 0.0, "avg_logprob": -0.08421444702148438, "compression_ratio": 1.8721804511278195, "no_speech_prob": 1.3630819012178108e-05}, {"id": 48, "seek": 21240, "start": 236.46, "end": 240.68, "text": " So then what you can do is you can click on the projection button here, and you can actually", "tokens": [407, 550, 437, 291, 393, 360, 307, 291, 393, 2052, 322, 264, 22743, 2960, 510, 11, 293, 291, 393, 767], "temperature": 0.0, "avg_logprob": -0.08421444702148438, "compression_ratio": 1.8721804511278195, "no_speech_prob": 1.3630819012178108e-05}, {"id": 49, "seek": 24068, "start": 240.68, "end": 246.72, "text": " just press up and down rather than just pressing the arrows at the top to switch between projections", "tokens": [445, 1886, 493, 293, 760, 2831, 813, 445, 12417, 264, 19669, 412, 264, 1192, 281, 3679, 1296, 32371], "temperature": 0.0, "avg_logprob": -0.08661202022007533, "compression_ratio": 1.7366071428571428, "no_speech_prob": 8.139574674714822e-06}, {"id": 50, "seek": 24068, "start": 246.72, "end": 249.12, "text": " or left and right to switch between layers.", "tokens": [420, 1411, 293, 558, 281, 3679, 1296, 7914, 13], "temperature": 0.0, "avg_logprob": -0.08661202022007533, "compression_ratio": 1.7366071428571428, "no_speech_prob": 8.139574674714822e-06}, {"id": 51, "seek": 24068, "start": 249.12, "end": 255.04000000000002, "text": " And what you can do is you can basically look around until you notice that there's a projection", "tokens": [400, 437, 291, 393, 360, 307, 291, 393, 1936, 574, 926, 1826, 291, 3449, 300, 456, 311, 257, 22743], "temperature": 0.0, "avg_logprob": -0.08661202022007533, "compression_ratio": 1.7366071428571428, "no_speech_prob": 8.139574674714822e-06}, {"id": 52, "seek": 24068, "start": 255.04000000000002, "end": 258.16, "text": " which has kind of separated out things you're interested in.", "tokens": [597, 575, 733, 295, 12005, 484, 721, 291, 434, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.08661202022007533, "compression_ratio": 1.7366071428571428, "no_speech_prob": 8.139574674714822e-06}, {"id": 53, "seek": 24068, "start": 258.16, "end": 269.16, "text": " And so this one actually, I notice that it's got a whole bunch of cars that are kind of", "tokens": [400, 370, 341, 472, 767, 11, 286, 3449, 300, 309, 311, 658, 257, 1379, 3840, 295, 5163, 300, 366, 733, 295], "temperature": 0.0, "avg_logprob": -0.08661202022007533, "compression_ratio": 1.7366071428571428, "no_speech_prob": 8.139574674714822e-06}, {"id": 54, "seek": 26916, "start": 269.16, "end": 273.72, "text": " from the top front right over here.", "tokens": [490, 264, 1192, 1868, 558, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.14444627834640386, "compression_ratio": 1.701818181818182, "no_speech_prob": 1.7502465198049322e-05}, {"id": 55, "seek": 26916, "start": 273.72, "end": 278.20000000000005, "text": " Okay, so if we zoom in a little bit, we can double check.", "tokens": [1033, 11, 370, 498, 321, 8863, 294, 257, 707, 857, 11, 321, 393, 3834, 1520, 13], "temperature": 0.0, "avg_logprob": -0.14444627834640386, "compression_ratio": 1.701818181818182, "no_speech_prob": 1.7502465198049322e-05}, {"id": 56, "seek": 26916, "start": 278.20000000000005, "end": 279.72, "text": " It's like, yeah, that looks pretty good.", "tokens": [467, 311, 411, 11, 1338, 11, 300, 1542, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.14444627834640386, "compression_ratio": 1.701818181818182, "no_speech_prob": 1.7502465198049322e-05}, {"id": 57, "seek": 26916, "start": 279.72, "end": 281.02000000000004, "text": " They're all kind of front right.", "tokens": [814, 434, 439, 733, 295, 1868, 558, 13], "temperature": 0.0, "avg_logprob": -0.14444627834640386, "compression_ratio": 1.701818181818182, "no_speech_prob": 1.7502465198049322e-05}, {"id": 58, "seek": 26916, "start": 281.02000000000004, "end": 287.12, "text": " So we can click on here to go to selection mode, and we can kind of grab a few, and then", "tokens": [407, 321, 393, 2052, 322, 510, 281, 352, 281, 9450, 4391, 11, 293, 321, 393, 733, 295, 4444, 257, 1326, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.14444627834640386, "compression_ratio": 1.701818181818182, "no_speech_prob": 1.7502465198049322e-05}, {"id": 59, "seek": 26916, "start": 287.12, "end": 289.04, "text": " you should check.", "tokens": [291, 820, 1520, 13], "temperature": 0.0, "avg_logprob": -0.14444627834640386, "compression_ratio": 1.701818181818182, "no_speech_prob": 1.7502465198049322e-05}, {"id": 60, "seek": 26916, "start": 289.04, "end": 292.24, "text": " And so what we're doing here is we're trying to take advantage of the combination of human", "tokens": [400, 370, 437, 321, 434, 884, 510, 307, 321, 434, 1382, 281, 747, 5002, 295, 264, 6562, 295, 1952], "temperature": 0.0, "avg_logprob": -0.14444627834640386, "compression_ratio": 1.701818181818182, "no_speech_prob": 1.7502465198049322e-05}, {"id": 61, "seek": 26916, "start": 292.24, "end": 293.72, "text": " plus machine.", "tokens": [1804, 3479, 13], "temperature": 0.0, "avg_logprob": -0.14444627834640386, "compression_ratio": 1.701818181818182, "no_speech_prob": 1.7502465198049322e-05}, {"id": 62, "seek": 26916, "start": 293.72, "end": 298.64000000000004, "text": " The machine's pretty good at quickly doing calculations, but as a human, I'm pretty good", "tokens": [440, 3479, 311, 1238, 665, 412, 2661, 884, 20448, 11, 457, 382, 257, 1952, 11, 286, 478, 1238, 665], "temperature": 0.0, "avg_logprob": -0.14444627834640386, "compression_ratio": 1.701818181818182, "no_speech_prob": 1.7502465198049322e-05}, {"id": 63, "seek": 29864, "start": 298.64, "end": 302.0, "text": " at looking at a lot of things at once and seeing the odd one out.", "tokens": [412, 1237, 412, 257, 688, 295, 721, 412, 1564, 293, 2577, 264, 7401, 472, 484, 13], "temperature": 0.0, "avg_logprob": -0.12389475582567461, "compression_ratio": 1.8745762711864407, "no_speech_prob": 1.8631470084073953e-05}, {"id": 64, "seek": 29864, "start": 302.0, "end": 305.12, "text": " So in this case, I'm looking for cars that aren't front right.", "tokens": [407, 294, 341, 1389, 11, 286, 478, 1237, 337, 5163, 300, 3212, 380, 1868, 558, 13], "temperature": 0.0, "avg_logprob": -0.12389475582567461, "compression_ratio": 1.8745762711864407, "no_speech_prob": 1.8631470084073953e-05}, {"id": 65, "seek": 29864, "start": 305.12, "end": 307.4, "text": " And so by laying them all in front of me, I can do that really quickly.", "tokens": [400, 370, 538, 14903, 552, 439, 294, 1868, 295, 385, 11, 286, 393, 360, 300, 534, 2661, 13], "temperature": 0.0, "avg_logprob": -0.12389475582567461, "compression_ratio": 1.8745762711864407, "no_speech_prob": 1.8631470084073953e-05}, {"id": 66, "seek": 29864, "start": 307.4, "end": 308.52, "text": " It's like, okay, definitely that one.", "tokens": [467, 311, 411, 11, 1392, 11, 2138, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.12389475582567461, "compression_ratio": 1.8745762711864407, "no_speech_prob": 1.8631470084073953e-05}, {"id": 67, "seek": 29864, "start": 308.52, "end": 311.71999999999997, "text": " So just click on the ones that you don't want.", "tokens": [407, 445, 2052, 322, 264, 2306, 300, 291, 500, 380, 528, 13], "temperature": 0.0, "avg_logprob": -0.12389475582567461, "compression_ratio": 1.8745762711864407, "no_speech_prob": 1.8631470084073953e-05}, {"id": 68, "seek": 29864, "start": 311.71999999999997, "end": 315.28, "text": " All right, it's all good.", "tokens": [1057, 558, 11, 309, 311, 439, 665, 13], "temperature": 0.0, "avg_logprob": -0.12389475582567461, "compression_ratio": 1.8745762711864407, "no_speech_prob": 1.8631470084073953e-05}, {"id": 69, "seek": 29864, "start": 315.28, "end": 316.84, "text": " So then you can just go back.", "tokens": [407, 550, 291, 393, 445, 352, 646, 13], "temperature": 0.0, "avg_logprob": -0.12389475582567461, "compression_ratio": 1.8745762711864407, "no_speech_prob": 1.8631470084073953e-05}, {"id": 70, "seek": 29864, "start": 316.84, "end": 320.9, "text": " And so then what you can do is you can either put them into a new category by typing create", "tokens": [400, 370, 550, 437, 291, 393, 360, 307, 291, 393, 2139, 829, 552, 666, 257, 777, 7719, 538, 18444, 1884], "temperature": 0.0, "avg_logprob": -0.12389475582567461, "compression_ratio": 1.8745762711864407, "no_speech_prob": 1.8631470084073953e-05}, {"id": 71, "seek": 29864, "start": 320.9, "end": 323.76, "text": " new label, or you can click on one of the existing ones.", "tokens": [777, 7645, 11, 420, 291, 393, 2052, 322, 472, 295, 264, 6741, 2306, 13], "temperature": 0.0, "avg_logprob": -0.12389475582567461, "compression_ratio": 1.8745762711864407, "no_speech_prob": 1.8631470084073953e-05}, {"id": 72, "seek": 29864, "start": 323.76, "end": 325.52, "text": " So before I came, I just created a few.", "tokens": [407, 949, 286, 1361, 11, 286, 445, 2942, 257, 1326, 13], "temperature": 0.0, "avg_logprob": -0.12389475582567461, "compression_ratio": 1.8745762711864407, "no_speech_prob": 1.8631470084073953e-05}, {"id": 73, "seek": 29864, "start": 325.52, "end": 326.52, "text": " So here's front right.", "tokens": [407, 510, 311, 1868, 558, 13], "temperature": 0.0, "avg_logprob": -0.12389475582567461, "compression_ratio": 1.8745762711864407, "no_speech_prob": 1.8631470084073953e-05}, {"id": 74, "seek": 32652, "start": 326.52, "end": 328.71999999999997, "text": " So I'll just click on it here.", "tokens": [407, 286, 603, 445, 2052, 322, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.1372265158028438, "compression_ratio": 1.6538461538461537, "no_speech_prob": 8.01334226707695e-06}, {"id": 75, "seek": 32652, "start": 328.71999999999997, "end": 330.76, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.1372265158028438, "compression_ratio": 1.6538461538461537, "no_speech_prob": 8.01334226707695e-06}, {"id": 76, "seek": 32652, "start": 330.76, "end": 336.15999999999997, "text": " And so that's the basic idea, is that you kind of keep flicking through different layers", "tokens": [400, 370, 300, 311, 264, 3875, 1558, 11, 307, 300, 291, 733, 295, 1066, 932, 10401, 807, 819, 7914], "temperature": 0.0, "avg_logprob": -0.1372265158028438, "compression_ratio": 1.6538461538461537, "no_speech_prob": 8.01334226707695e-06}, {"id": 77, "seek": 32652, "start": 336.15999999999997, "end": 339.64, "text": " or projections to try and find groups that represent the things you're interested in.", "tokens": [420, 32371, 281, 853, 293, 915, 3935, 300, 2906, 264, 721, 291, 434, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.1372265158028438, "compression_ratio": 1.6538461538461537, "no_speech_prob": 8.01334226707695e-06}, {"id": 78, "seek": 32652, "start": 339.64, "end": 343.12, "text": " And then over time, you'll start to realize that there are some things that are a little", "tokens": [400, 550, 670, 565, 11, 291, 603, 722, 281, 4325, 300, 456, 366, 512, 721, 300, 366, 257, 707], "temperature": 0.0, "avg_logprob": -0.1372265158028438, "compression_ratio": 1.6538461538461537, "no_speech_prob": 8.01334226707695e-06}, {"id": 79, "seek": 32652, "start": 343.12, "end": 345.44, "text": " bit harder.", "tokens": [857, 6081, 13], "temperature": 0.0, "avg_logprob": -0.1372265158028438, "compression_ratio": 1.6538461538461537, "no_speech_prob": 8.01334226707695e-06}, {"id": 80, "seek": 32652, "start": 345.44, "end": 348.47999999999996, "text": " So for example, I'm having trouble finding sides.", "tokens": [407, 337, 1365, 11, 286, 478, 1419, 5253, 5006, 4881, 13], "temperature": 0.0, "avg_logprob": -0.1372265158028438, "compression_ratio": 1.6538461538461537, "no_speech_prob": 8.01334226707695e-06}, {"id": 81, "seek": 32652, "start": 348.47999999999996, "end": 352.03999999999996, "text": " So what I can do is I can see over here there's a few sides.", "tokens": [407, 437, 286, 393, 360, 307, 286, 393, 536, 670, 510, 456, 311, 257, 1326, 4881, 13], "temperature": 0.0, "avg_logprob": -0.1372265158028438, "compression_ratio": 1.6538461538461537, "no_speech_prob": 8.01334226707695e-06}, {"id": 82, "seek": 35204, "start": 352.04, "end": 361.08000000000004, "text": " So I can zoom in here and click on a couple of them, like this one and this one, that", "tokens": [407, 286, 393, 8863, 294, 510, 293, 2052, 322, 257, 1916, 295, 552, 11, 411, 341, 472, 293, 341, 472, 11, 300], "temperature": 0.0, "avg_logprob": -0.17558847154889787, "compression_ratio": 1.5925925925925926, "no_speech_prob": 2.443973244226072e-06}, {"id": 83, "seek": 35204, "start": 361.08000000000004, "end": 364.08000000000004, "text": " one, that one.", "tokens": [472, 11, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.17558847154889787, "compression_ratio": 1.5925925925925926, "no_speech_prob": 2.443973244226072e-06}, {"id": 84, "seek": 35204, "start": 364.08000000000004, "end": 365.08000000000004, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.17558847154889787, "compression_ratio": 1.5925925925925926, "no_speech_prob": 2.443973244226072e-06}, {"id": 85, "seek": 35204, "start": 365.08000000000004, "end": 367.08000000000004, "text": " And then I'll say find similar.", "tokens": [400, 550, 286, 603, 584, 915, 2531, 13], "temperature": 0.0, "avg_logprob": -0.17558847154889787, "compression_ratio": 1.5925925925925926, "no_speech_prob": 2.443973244226072e-06}, {"id": 86, "seek": 35204, "start": 367.08000000000004, "end": 373.28000000000003, "text": " And so this is going to basically look in that projection space, and not just at the", "tokens": [400, 370, 341, 307, 516, 281, 1936, 574, 294, 300, 22743, 1901, 11, 293, 406, 445, 412, 264], "temperature": 0.0, "avg_logprob": -0.17558847154889787, "compression_ratio": 1.5925925925925926, "no_speech_prob": 2.443973244226072e-06}, {"id": 87, "seek": 35204, "start": 373.28000000000003, "end": 376.98, "text": " images that are currently displayed, but all of the images that you uploaded.", "tokens": [5267, 300, 366, 4362, 16372, 11, 457, 439, 295, 264, 5267, 300, 291, 17135, 13], "temperature": 0.0, "avg_logprob": -0.17558847154889787, "compression_ratio": 1.5925925925925926, "no_speech_prob": 2.443973244226072e-06}, {"id": 88, "seek": 37698, "start": 376.98, "end": 383.0, "text": " And hopefully I might be able to label now a few more side images at that point.", "tokens": [400, 4696, 286, 1062, 312, 1075, 281, 7645, 586, 257, 1326, 544, 1252, 5267, 412, 300, 935, 13], "temperature": 0.0, "avg_logprob": -0.07529509984529935, "compression_ratio": 1.56, "no_speech_prob": 1.0129693691851571e-05}, {"id": 89, "seek": 37698, "start": 383.0, "end": 389.56, "text": " So it's going through and checking all of the images that you uploaded to see if any", "tokens": [407, 309, 311, 516, 807, 293, 8568, 439, 295, 264, 5267, 300, 291, 17135, 281, 536, 498, 604], "temperature": 0.0, "avg_logprob": -0.07529509984529935, "compression_ratio": 1.56, "no_speech_prob": 1.0129693691851571e-05}, {"id": 90, "seek": 37698, "start": 389.56, "end": 396.16, "text": " of them have projections in this space which are similar to the ones I've selected.", "tokens": [295, 552, 362, 32371, 294, 341, 1901, 597, 366, 2531, 281, 264, 2306, 286, 600, 8209, 13], "temperature": 0.0, "avg_logprob": -0.07529509984529935, "compression_ratio": 1.56, "no_speech_prob": 1.0129693691851571e-05}, {"id": 91, "seek": 37698, "start": 396.16, "end": 399.56, "text": " And hopefully we'll find a few more of what I'm interested in.", "tokens": [400, 4696, 321, 603, 915, 257, 1326, 544, 295, 437, 286, 478, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.07529509984529935, "compression_ratio": 1.56, "no_speech_prob": 1.0129693691851571e-05}, {"id": 92, "seek": 39956, "start": 399.56, "end": 410.6, "text": " Okay, so now if I want to try to find a projection that separates the sides from the front right,", "tokens": [1033, 11, 370, 586, 498, 286, 528, 281, 853, 281, 915, 257, 22743, 300, 34149, 264, 4881, 490, 264, 1868, 558, 11], "temperature": 0.0, "avg_logprob": -0.10127752282646264, "compression_ratio": 1.6826923076923077, "no_speech_prob": 6.338884304568637e-06}, {"id": 93, "seek": 39956, "start": 410.6, "end": 412.64, "text": " I can click on each of those two.", "tokens": [286, 393, 2052, 322, 1184, 295, 729, 732, 13], "temperature": 0.0, "avg_logprob": -0.10127752282646264, "compression_ratio": 1.6826923076923077, "no_speech_prob": 6.338884304568637e-06}, {"id": 94, "seek": 39956, "start": 412.64, "end": 416.2, "text": " And then over here, this button's now called switch to the projection that maximizes the", "tokens": [400, 550, 670, 510, 11, 341, 2960, 311, 586, 1219, 3679, 281, 264, 22743, 300, 5138, 5660, 264], "temperature": 0.0, "avg_logprob": -0.10127752282646264, "compression_ratio": 1.6826923076923077, "no_speech_prob": 6.338884304568637e-06}, {"id": 95, "seek": 39956, "start": 416.2, "end": 418.02, "text": " distance between the labels.", "tokens": [4560, 1296, 264, 16949, 13], "temperature": 0.0, "avg_logprob": -0.10127752282646264, "compression_ratio": 1.6826923076923077, "no_speech_prob": 6.338884304568637e-06}, {"id": 96, "seek": 39956, "start": 418.02, "end": 422.56, "text": " So now what this is going to do is try to find the best projection that separates out", "tokens": [407, 586, 437, 341, 307, 516, 281, 360, 307, 853, 281, 915, 264, 1151, 22743, 300, 34149, 484], "temperature": 0.0, "avg_logprob": -0.10127752282646264, "compression_ratio": 1.6826923076923077, "no_speech_prob": 6.338884304568637e-06}, {"id": 97, "seek": 39956, "start": 422.56, "end": 423.56, "text": " those classes.", "tokens": [729, 5359, 13], "temperature": 0.0, "avg_logprob": -0.10127752282646264, "compression_ratio": 1.6826923076923077, "no_speech_prob": 6.338884304568637e-06}, {"id": 98, "seek": 42356, "start": 423.56, "end": 429.96, "text": " And so the goal here is to help me visually inspect and quickly find a bunch of things", "tokens": [400, 370, 264, 3387, 510, 307, 281, 854, 385, 19622, 15018, 293, 2661, 915, 257, 3840, 295, 721], "temperature": 0.0, "avg_logprob": -0.16966860781433762, "compression_ratio": 1.5636363636363637, "no_speech_prob": 1.1125441233161837e-05}, {"id": 99, "seek": 42356, "start": 429.96, "end": 433.96, "text": " that I can use to label.", "tokens": [300, 286, 393, 764, 281, 7645, 13], "temperature": 0.0, "avg_logprob": -0.16966860781433762, "compression_ratio": 1.5636363636363637, "no_speech_prob": 1.1125441233161837e-05}, {"id": 100, "seek": 42356, "start": 433.96, "end": 438.26, "text": " So they're the key features, and it's done a good job.", "tokens": [407, 436, 434, 264, 2141, 4122, 11, 293, 309, 311, 1096, 257, 665, 1691, 13], "temperature": 0.0, "avg_logprob": -0.16966860781433762, "compression_ratio": 1.5636363636363637, "no_speech_prob": 1.1125441233161837e-05}, {"id": 101, "seek": 42356, "start": 438.26, "end": 443.08, "text": " You can see down here we've now got a whole bunch of sides which I can now grab because", "tokens": [509, 393, 536, 760, 510, 321, 600, 586, 658, 257, 1379, 3840, 295, 4881, 597, 286, 393, 586, 4444, 570], "temperature": 0.0, "avg_logprob": -0.16966860781433762, "compression_ratio": 1.5636363636363637, "no_speech_prob": 1.1125441233161837e-05}, {"id": 102, "seek": 42356, "start": 443.08, "end": 446.42, "text": " I was having a lot of trouble finding them before.", "tokens": [286, 390, 1419, 257, 688, 295, 5253, 5006, 552, 949, 13], "temperature": 0.0, "avg_logprob": -0.16966860781433762, "compression_ratio": 1.5636363636363637, "no_speech_prob": 1.1125441233161837e-05}, {"id": 103, "seek": 42356, "start": 446.42, "end": 448.16, "text": " And it's always worth double checking.", "tokens": [400, 309, 311, 1009, 3163, 3834, 8568, 13], "temperature": 0.0, "avg_logprob": -0.16966860781433762, "compression_ratio": 1.5636363636363637, "no_speech_prob": 1.1125441233161837e-05}, {"id": 104, "seek": 44816, "start": 448.16, "end": 455.28000000000003, "text": " And it's kind of interesting to see how the neural nets behave.", "tokens": [400, 309, 311, 733, 295, 1880, 281, 536, 577, 264, 18161, 36170, 15158, 13], "temperature": 0.0, "avg_logprob": -0.18178475697835286, "compression_ratio": 1.7276595744680852, "no_speech_prob": 6.048831892258022e-06}, {"id": 105, "seek": 44816, "start": 455.28000000000003, "end": 458.48, "text": " Like there seems to be more sports cars in this group than average as well.", "tokens": [1743, 456, 2544, 281, 312, 544, 6573, 5163, 294, 341, 1594, 813, 4274, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.18178475697835286, "compression_ratio": 1.7276595744680852, "no_speech_prob": 6.048831892258022e-06}, {"id": 106, "seek": 44816, "start": 458.48, "end": 461.68, "text": " So it's kind of found side angles of sports cars.", "tokens": [407, 309, 311, 733, 295, 1352, 1252, 14708, 295, 6573, 5163, 13], "temperature": 0.0, "avg_logprob": -0.18178475697835286, "compression_ratio": 1.7276595744680852, "no_speech_prob": 6.048831892258022e-06}, {"id": 107, "seek": 44816, "start": 461.68, "end": 462.90000000000003, "text": " So that's kind of interesting.", "tokens": [407, 300, 311, 733, 295, 1880, 13], "temperature": 0.0, "avg_logprob": -0.18178475697835286, "compression_ratio": 1.7276595744680852, "no_speech_prob": 6.048831892258022e-06}, {"id": 108, "seek": 44816, "start": 462.90000000000003, "end": 464.44000000000005, "text": " So then I can click, all right, so I've got those.", "tokens": [407, 550, 286, 393, 2052, 11, 439, 558, 11, 370, 286, 600, 658, 729, 13], "temperature": 0.0, "avg_logprob": -0.18178475697835286, "compression_ratio": 1.7276595744680852, "no_speech_prob": 6.048831892258022e-06}, {"id": 109, "seek": 44816, "start": 464.44000000000005, "end": 467.40000000000003, "text": " So now I'll click side.", "tokens": [407, 586, 286, 603, 2052, 1252, 13], "temperature": 0.0, "avg_logprob": -0.18178475697835286, "compression_ratio": 1.7276595744680852, "no_speech_prob": 6.048831892258022e-06}, {"id": 110, "seek": 44816, "start": 467.40000000000003, "end": 468.40000000000003, "text": " And there we go.", "tokens": [400, 456, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.18178475697835286, "compression_ratio": 1.7276595744680852, "no_speech_prob": 6.048831892258022e-06}, {"id": 111, "seek": 44816, "start": 468.40000000000003, "end": 475.22, "text": " So once you've done that a few times, I find if you've got, you know, a hundred or so labels,", "tokens": [407, 1564, 291, 600, 1096, 300, 257, 1326, 1413, 11, 286, 915, 498, 291, 600, 658, 11, 291, 458, 11, 257, 3262, 420, 370, 16949, 11], "temperature": 0.0, "avg_logprob": -0.18178475697835286, "compression_ratio": 1.7276595744680852, "no_speech_prob": 6.048831892258022e-06}, {"id": 112, "seek": 47522, "start": 475.22, "end": 479.56, "text": " you can then click on the train model button and it'll take a couple of minutes and come", "tokens": [291, 393, 550, 2052, 322, 264, 3847, 2316, 2960, 293, 309, 603, 747, 257, 1916, 295, 2077, 293, 808], "temperature": 0.0, "avg_logprob": -0.10885529439957416, "compression_ratio": 1.7624113475177305, "no_speech_prob": 7.527803973061964e-06}, {"id": 113, "seek": 47522, "start": 479.56, "end": 481.96000000000004, "text": " back and show you your train model.", "tokens": [646, 293, 855, 291, 428, 3847, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10885529439957416, "compression_ratio": 1.7624113475177305, "no_speech_prob": 7.527803973061964e-06}, {"id": 114, "seek": 47522, "start": 481.96000000000004, "end": 486.0, "text": " And after it's trained, which I did it on a smaller number of labels earlier, you can", "tokens": [400, 934, 309, 311, 8895, 11, 597, 286, 630, 309, 322, 257, 4356, 1230, 295, 16949, 3071, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.10885529439957416, "compression_ratio": 1.7624113475177305, "no_speech_prob": 7.527803973061964e-06}, {"id": 115, "seek": 47522, "start": 486.0, "end": 492.72, "text": " then switch this vary opacity button and it'll actually kind of fade out the ones that are", "tokens": [550, 3679, 341, 10559, 41693, 2960, 293, 309, 603, 767, 733, 295, 21626, 484, 264, 2306, 300, 366], "temperature": 0.0, "avg_logprob": -0.10885529439957416, "compression_ratio": 1.7624113475177305, "no_speech_prob": 7.527803973061964e-06}, {"id": 116, "seek": 47522, "start": 492.72, "end": 494.40000000000003, "text": " already predicted pretty well.", "tokens": [1217, 19147, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.10885529439957416, "compression_ratio": 1.7624113475177305, "no_speech_prob": 7.527803973061964e-06}, {"id": 117, "seek": 47522, "start": 494.40000000000003, "end": 499.32000000000005, "text": " And it'll also give you an estimate as to how accurate it thinks the model is.", "tokens": [400, 309, 603, 611, 976, 291, 364, 12539, 382, 281, 577, 8559, 309, 7309, 264, 2316, 307, 13], "temperature": 0.0, "avg_logprob": -0.10885529439957416, "compression_ratio": 1.7624113475177305, "no_speech_prob": 7.527803973061964e-06}, {"id": 118, "seek": 47522, "start": 499.32000000000005, "end": 503.68, "text": " The main reason I mention this for you is that so that you can now click the download", "tokens": [440, 2135, 1778, 286, 2152, 341, 337, 291, 307, 300, 370, 300, 291, 393, 586, 2052, 264, 5484], "temperature": 0.0, "avg_logprob": -0.10885529439957416, "compression_ratio": 1.7624113475177305, "no_speech_prob": 7.527803973061964e-06}, {"id": 119, "seek": 50368, "start": 503.68, "end": 507.92, "text": " button and it'll download the predictions, which is what we hope will be interesting", "tokens": [2960, 293, 309, 603, 5484, 264, 21264, 11, 597, 307, 437, 321, 1454, 486, 312, 1880], "temperature": 0.0, "avg_logprob": -0.10075162151667076, "compression_ratio": 1.8716981132075472, "no_speech_prob": 1.4063628441363107e-05}, {"id": 120, "seek": 50368, "start": 507.92, "end": 508.92, "text": " to most people.", "tokens": [281, 881, 561, 13], "temperature": 0.0, "avg_logprob": -0.10075162151667076, "compression_ratio": 1.8716981132075472, "no_speech_prob": 1.4063628441363107e-05}, {"id": 121, "seek": 50368, "start": 508.92, "end": 512.2, "text": " But what I think will be interesting to you as deep learning students is it'll download", "tokens": [583, 437, 286, 519, 486, 312, 1880, 281, 291, 382, 2452, 2539, 1731, 307, 309, 603, 5484], "temperature": 0.0, "avg_logprob": -0.10075162151667076, "compression_ratio": 1.8716981132075472, "no_speech_prob": 1.4063628441363107e-05}, {"id": 122, "seek": 50368, "start": 512.2, "end": 513.84, "text": " your labels.", "tokens": [428, 16949, 13], "temperature": 0.0, "avg_logprob": -0.10075162151667076, "compression_ratio": 1.8716981132075472, "no_speech_prob": 1.4063628441363107e-05}, {"id": 123, "seek": 50368, "start": 513.84, "end": 520.92, "text": " So now you can use that labeled subset of data along with the unlabeled set that you", "tokens": [407, 586, 291, 393, 764, 300, 21335, 25993, 295, 1412, 2051, 365, 264, 32118, 18657, 292, 992, 300, 291], "temperature": 0.0, "avg_logprob": -0.10075162151667076, "compression_ratio": 1.8716981132075472, "no_speech_prob": 1.4063628441363107e-05}, {"id": 124, "seek": 50368, "start": 520.92, "end": 525.12, "text": " haven't labeled yet to see if you can, you know, see if you can build a better model", "tokens": [2378, 380, 21335, 1939, 281, 536, 498, 291, 393, 11, 291, 458, 11, 536, 498, 291, 393, 1322, 257, 1101, 2316], "temperature": 0.0, "avg_logprob": -0.10075162151667076, "compression_ratio": 1.8716981132075472, "no_speech_prob": 1.4063628441363107e-05}, {"id": 125, "seek": 50368, "start": 525.12, "end": 527.32, "text": " and platform AI is done for you.", "tokens": [293, 3663, 7318, 307, 1096, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.10075162151667076, "compression_ratio": 1.8716981132075472, "no_speech_prob": 1.4063628441363107e-05}, {"id": 126, "seek": 50368, "start": 527.32, "end": 531.48, "text": " See if you can use that initial set of data to kind of get going, creating models of stuff,", "tokens": [3008, 498, 291, 393, 764, 300, 5883, 992, 295, 1412, 281, 733, 295, 483, 516, 11, 4084, 5245, 295, 1507, 11], "temperature": 0.0, "avg_logprob": -0.10075162151667076, "compression_ratio": 1.8716981132075472, "no_speech_prob": 1.4063628441363107e-05}, {"id": 127, "seek": 53148, "start": 531.48, "end": 534.4, "text": " which you weren't able to label before.", "tokens": [597, 291, 4999, 380, 1075, 281, 7645, 949, 13], "temperature": 0.0, "avg_logprob": -0.17879180188448923, "compression_ratio": 1.7250996015936255, "no_speech_prob": 7.527706202381523e-06}, {"id": 128, "seek": 53148, "start": 534.4, "end": 539.96, "text": " Clearly, there are some things that this system is better at than others.", "tokens": [24120, 11, 456, 366, 512, 721, 300, 341, 1185, 307, 1101, 412, 813, 2357, 13], "temperature": 0.0, "avg_logprob": -0.17879180188448923, "compression_ratio": 1.7250996015936255, "no_speech_prob": 7.527706202381523e-06}, {"id": 129, "seek": 53148, "start": 539.96, "end": 545.08, "text": " The things that require, you know, really zooming in closely and taking a very, very", "tokens": [440, 721, 300, 3651, 11, 291, 458, 11, 534, 48226, 294, 8185, 293, 1940, 257, 588, 11, 588], "temperature": 0.0, "avg_logprob": -0.17879180188448923, "compression_ratio": 1.7250996015936255, "no_speech_prob": 7.527706202381523e-06}, {"id": 130, "seek": 53148, "start": 545.08, "end": 547.6800000000001, "text": " close inspection, this isn't going to work very well.", "tokens": [1998, 22085, 11, 341, 1943, 380, 516, 281, 589, 588, 731, 13], "temperature": 0.0, "avg_logprob": -0.17879180188448923, "compression_ratio": 1.7250996015936255, "no_speech_prob": 7.527706202381523e-06}, {"id": 131, "seek": 53148, "start": 547.6800000000001, "end": 553.12, "text": " This is really designed for things that the human eye can kind of pick up fairly readily.", "tokens": [639, 307, 534, 4761, 337, 721, 300, 264, 1952, 3313, 393, 733, 295, 1888, 493, 6457, 26336, 13], "temperature": 0.0, "avg_logprob": -0.17879180188448923, "compression_ratio": 1.7250996015936255, "no_speech_prob": 7.527706202381523e-06}, {"id": 132, "seek": 53148, "start": 553.12, "end": 558.44, "text": " But we'd love to get feedback as well and you can click on the help button to get feedback", "tokens": [583, 321, 1116, 959, 281, 483, 5824, 382, 731, 293, 291, 393, 2052, 322, 264, 854, 2960, 281, 483, 5824], "temperature": 0.0, "avg_logprob": -0.17879180188448923, "compression_ratio": 1.7250996015936255, "no_speech_prob": 7.527706202381523e-06}, {"id": 133, "seek": 55844, "start": 558.44, "end": 565.6400000000001, "text": " and also there's a platform AI discussion topic in our forum where, so Ashak, if you", "tokens": [293, 611, 456, 311, 257, 3663, 7318, 5017, 4829, 294, 527, 17542, 689, 11, 370, 10279, 514, 11, 498, 291], "temperature": 0.0, "avg_logprob": -0.21906626501748727, "compression_ratio": 1.4669811320754718, "no_speech_prob": 2.3551339836558327e-05}, {"id": 134, "seek": 55844, "start": 565.6400000000001, "end": 568.0, "text": " can stand up, Ashak is the CEO of the company.", "tokens": [393, 1463, 493, 11, 10279, 514, 307, 264, 9282, 295, 264, 2237, 13], "temperature": 0.0, "avg_logprob": -0.21906626501748727, "compression_ratio": 1.4669811320754718, "no_speech_prob": 2.3551339836558327e-05}, {"id": 135, "seek": 55844, "start": 568.0, "end": 573.6400000000001, "text": " He'll be there helping out, answering questions and so forth.", "tokens": [634, 603, 312, 456, 4315, 484, 11, 13430, 1651, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.21906626501748727, "compression_ratio": 1.4669811320754718, "no_speech_prob": 2.3551339836558327e-05}, {"id": 136, "seek": 55844, "start": 573.6400000000001, "end": 577.6400000000001, "text": " So yeah, I hope people find that useful.", "tokens": [407, 1338, 11, 286, 1454, 561, 915, 300, 4420, 13], "temperature": 0.0, "avg_logprob": -0.21906626501748727, "compression_ratio": 1.4669811320754718, "no_speech_prob": 2.3551339836558327e-05}, {"id": 137, "seek": 55844, "start": 577.6400000000001, "end": 584.6400000000001, "text": " It's been many years getting to this point and I'm glad we're finally there.", "tokens": [467, 311, 668, 867, 924, 1242, 281, 341, 935, 293, 286, 478, 5404, 321, 434, 2721, 456, 13], "temperature": 0.0, "avg_logprob": -0.21906626501748727, "compression_ratio": 1.4669811320754718, "no_speech_prob": 2.3551339836558327e-05}, {"id": 138, "seek": 58464, "start": 584.64, "end": 592.24, "text": " Okay, so one of the reasons I wanted to mention this today is that we're going to be doing", "tokens": [1033, 11, 370, 472, 295, 264, 4112, 286, 1415, 281, 2152, 341, 965, 307, 300, 321, 434, 516, 281, 312, 884], "temperature": 0.0, "avg_logprob": -0.10070090293884278, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.00010225540609098971}, {"id": 139, "seek": 58464, "start": 592.24, "end": 596.36, "text": " a big dive into convolutions later in this lesson.", "tokens": [257, 955, 9192, 666, 3754, 15892, 1780, 294, 341, 6898, 13], "temperature": 0.0, "avg_logprob": -0.10070090293884278, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.00010225540609098971}, {"id": 140, "seek": 58464, "start": 596.36, "end": 599.78, "text": " So I'm going to circle back to this to try and explain a little bit more about how that", "tokens": [407, 286, 478, 516, 281, 6329, 646, 281, 341, 281, 853, 293, 2903, 257, 707, 857, 544, 466, 577, 300], "temperature": 0.0, "avg_logprob": -0.10070090293884278, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.00010225540609098971}, {"id": 141, "seek": 58464, "start": 599.78, "end": 604.4399999999999, "text": " is working under the hood and give you kind of a sense of what's going on.", "tokens": [307, 1364, 833, 264, 13376, 293, 976, 291, 733, 295, 257, 2020, 295, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.10070090293884278, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.00010225540609098971}, {"id": 142, "seek": 58464, "start": 604.4399999999999, "end": 610.6, "text": " But before we do, we have to finish off last week's discussion of regularization.", "tokens": [583, 949, 321, 360, 11, 321, 362, 281, 2413, 766, 1036, 1243, 311, 5017, 295, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.10070090293884278, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.00010225540609098971}, {"id": 143, "seek": 61060, "start": 610.6, "end": 615.72, "text": " And so we were talking about regularization specifically in the context of the tabular", "tokens": [400, 370, 321, 645, 1417, 466, 3890, 2144, 4682, 294, 264, 4319, 295, 264, 4421, 1040], "temperature": 0.0, "avg_logprob": -0.18265808329862707, "compression_ratio": 1.7447698744769875, "no_speech_prob": 3.071602986892685e-05}, {"id": 144, "seek": 61060, "start": 615.72, "end": 620.48, "text": " learner because the tabular learner, this was the forward method, sorry, this is the", "tokens": [33347, 570, 264, 4421, 1040, 33347, 11, 341, 390, 264, 2128, 3170, 11, 2597, 11, 341, 307, 264], "temperature": 0.0, "avg_logprob": -0.18265808329862707, "compression_ratio": 1.7447698744769875, "no_speech_prob": 3.071602986892685e-05}, {"id": 145, "seek": 61060, "start": 620.48, "end": 627.16, "text": " init method in the tabular learner and our goal was to understand everything here.", "tokens": [3157, 3170, 294, 264, 4421, 1040, 33347, 293, 527, 3387, 390, 281, 1223, 1203, 510, 13], "temperature": 0.0, "avg_logprob": -0.18265808329862707, "compression_ratio": 1.7447698744769875, "no_speech_prob": 3.071602986892685e-05}, {"id": 146, "seek": 61060, "start": 627.16, "end": 629.5600000000001, "text": " And we're not quite there yet.", "tokens": [400, 321, 434, 406, 1596, 456, 1939, 13], "temperature": 0.0, "avg_logprob": -0.18265808329862707, "compression_ratio": 1.7447698744769875, "no_speech_prob": 3.071602986892685e-05}, {"id": 147, "seek": 61060, "start": 629.5600000000001, "end": 635.86, "text": " Last week we were looking at the adult data set which is a really simple, kind of oversimple", "tokens": [5264, 1243, 321, 645, 1237, 412, 264, 5075, 1412, 992, 597, 307, 257, 534, 2199, 11, 733, 295, 15488, 332, 781], "temperature": 0.0, "avg_logprob": -0.18265808329862707, "compression_ratio": 1.7447698744769875, "no_speech_prob": 3.071602986892685e-05}, {"id": 148, "seek": 61060, "start": 635.86, "end": 638.64, "text": " data set that's just for toy purposes.", "tokens": [1412, 992, 300, 311, 445, 337, 12058, 9932, 13], "temperature": 0.0, "avg_logprob": -0.18265808329862707, "compression_ratio": 1.7447698744769875, "no_speech_prob": 3.071602986892685e-05}, {"id": 149, "seek": 63864, "start": 638.64, "end": 642.64, "text": " So this week let's look at a data set that's much more interesting, a Kaggle competition", "tokens": [407, 341, 1243, 718, 311, 574, 412, 257, 1412, 992, 300, 311, 709, 544, 1880, 11, 257, 48751, 22631, 6211], "temperature": 0.0, "avg_logprob": -0.11602528889973958, "compression_ratio": 1.9638009049773755, "no_speech_prob": 2.4296106857946143e-05}, {"id": 150, "seek": 63864, "start": 642.64, "end": 643.64, "text": " data set.", "tokens": [1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.11602528889973958, "compression_ratio": 1.9638009049773755, "no_speech_prob": 2.4296106857946143e-05}, {"id": 151, "seek": 63864, "start": 643.64, "end": 649.08, "text": " So we know kind of what the best in the world and you know Kaggle competition results tend", "tokens": [407, 321, 458, 733, 295, 437, 264, 1151, 294, 264, 1002, 293, 291, 458, 48751, 22631, 6211, 3542, 3928], "temperature": 0.0, "avg_logprob": -0.11602528889973958, "compression_ratio": 1.9638009049773755, "no_speech_prob": 2.4296106857946143e-05}, {"id": 152, "seek": 63864, "start": 649.08, "end": 653.46, "text": " to be much harder to beat than academic state of the art results tend to be because a lot", "tokens": [281, 312, 709, 6081, 281, 4224, 813, 7778, 1785, 295, 264, 1523, 3542, 3928, 281, 312, 570, 257, 688], "temperature": 0.0, "avg_logprob": -0.11602528889973958, "compression_ratio": 1.9638009049773755, "no_speech_prob": 2.4296106857946143e-05}, {"id": 153, "seek": 63864, "start": 653.46, "end": 657.3199999999999, "text": " more people work on Kaggle competitions than most academic data sets.", "tokens": [544, 561, 589, 322, 48751, 22631, 26185, 813, 881, 7778, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.11602528889973958, "compression_ratio": 1.9638009049773755, "no_speech_prob": 2.4296106857946143e-05}, {"id": 154, "seek": 63864, "start": 657.3199999999999, "end": 662.12, "text": " So it's a really good challenge to try and do well on a Kaggle competition data set.", "tokens": [407, 309, 311, 257, 534, 665, 3430, 281, 853, 293, 360, 731, 322, 257, 48751, 22631, 6211, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.11602528889973958, "compression_ratio": 1.9638009049773755, "no_speech_prob": 2.4296106857946143e-05}, {"id": 155, "seek": 66212, "start": 662.12, "end": 669.68, "text": " So this one, the Rossmann data set, they've got 3,000 drug stores in Europe and you're", "tokens": [407, 341, 472, 11, 264, 16140, 14912, 1412, 992, 11, 436, 600, 658, 805, 11, 1360, 4110, 9512, 294, 3315, 293, 291, 434], "temperature": 0.0, "avg_logprob": -0.11636722296999212, "compression_ratio": 1.7471264367816093, "no_speech_prob": 4.289093794795917e-06}, {"id": 156, "seek": 66212, "start": 669.68, "end": 675.96, "text": " trying to predict how many products they're going to sell in the next couple of weeks.", "tokens": [1382, 281, 6069, 577, 867, 3383, 436, 434, 516, 281, 3607, 294, 264, 958, 1916, 295, 3259, 13], "temperature": 0.0, "avg_logprob": -0.11636722296999212, "compression_ratio": 1.7471264367816093, "no_speech_prob": 4.289093794795917e-06}, {"id": 157, "seek": 66212, "start": 675.96, "end": 682.12, "text": " So one of the interesting things about this is that the test set for this is from a time", "tokens": [407, 472, 295, 264, 1880, 721, 466, 341, 307, 300, 264, 1500, 992, 337, 341, 307, 490, 257, 565], "temperature": 0.0, "avg_logprob": -0.11636722296999212, "compression_ratio": 1.7471264367816093, "no_speech_prob": 4.289093794795917e-06}, {"id": 158, "seek": 66212, "start": 682.12, "end": 685.88, "text": " period that is more recent than the training set.", "tokens": [2896, 300, 307, 544, 5162, 813, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.11636722296999212, "compression_ratio": 1.7471264367816093, "no_speech_prob": 4.289093794795917e-06}, {"id": 159, "seek": 66212, "start": 685.88, "end": 688.6, "text": " And this is really common, right, if you want to predict things there's no point predicting", "tokens": [400, 341, 307, 534, 2689, 11, 558, 11, 498, 291, 528, 281, 6069, 721, 456, 311, 572, 935, 32884], "temperature": 0.0, "avg_logprob": -0.11636722296999212, "compression_ratio": 1.7471264367816093, "no_speech_prob": 4.289093794795917e-06}, {"id": 160, "seek": 66212, "start": 688.6, "end": 690.6800000000001, "text": " things that are in the middle of your training set.", "tokens": [721, 300, 366, 294, 264, 2808, 295, 428, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.11636722296999212, "compression_ratio": 1.7471264367816093, "no_speech_prob": 4.289093794795917e-06}, {"id": 161, "seek": 69068, "start": 690.68, "end": 694.76, "text": " You want to predict things in the future.", "tokens": [509, 528, 281, 6069, 721, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.09339377023641346, "compression_ratio": 1.8755364806866952, "no_speech_prob": 2.3922188120195642e-05}, {"id": 162, "seek": 69068, "start": 694.76, "end": 699.0799999999999, "text": " Another interesting thing about it is the evaluation metric they provided is the root", "tokens": [3996, 1880, 551, 466, 309, 307, 264, 13344, 20678, 436, 5649, 307, 264, 5593], "temperature": 0.0, "avg_logprob": -0.09339377023641346, "compression_ratio": 1.8755364806866952, "no_speech_prob": 2.3922188120195642e-05}, {"id": 163, "seek": 69068, "start": 699.0799999999999, "end": 700.8399999999999, "text": " mean squared percent error.", "tokens": [914, 8889, 3043, 6713, 13], "temperature": 0.0, "avg_logprob": -0.09339377023641346, "compression_ratio": 1.8755364806866952, "no_speech_prob": 2.3922188120195642e-05}, {"id": 164, "seek": 69068, "start": 700.8399999999999, "end": 705.2399999999999, "text": " So this is just a normal root mean squared error except we go actual minus prediction", "tokens": [407, 341, 307, 445, 257, 2710, 5593, 914, 8889, 6713, 3993, 321, 352, 3539, 3175, 17630], "temperature": 0.0, "avg_logprob": -0.09339377023641346, "compression_ratio": 1.8755364806866952, "no_speech_prob": 2.3922188120195642e-05}, {"id": 165, "seek": 69068, "start": 705.2399999999999, "end": 706.2399999999999, "text": " divided by actual.", "tokens": [6666, 538, 3539, 13], "temperature": 0.0, "avg_logprob": -0.09339377023641346, "compression_ratio": 1.8755364806866952, "no_speech_prob": 2.3922188120195642e-05}, {"id": 166, "seek": 69068, "start": 706.2399999999999, "end": 712.1999999999999, "text": " So in other words it's the percent error that we're taking the root mean squared of.", "tokens": [407, 294, 661, 2283, 309, 311, 264, 3043, 6713, 300, 321, 434, 1940, 264, 5593, 914, 8889, 295, 13], "temperature": 0.0, "avg_logprob": -0.09339377023641346, "compression_ratio": 1.8755364806866952, "no_speech_prob": 2.3922188120195642e-05}, {"id": 167, "seek": 69068, "start": 712.1999999999999, "end": 714.4399999999999, "text": " So there's a couple of interesting features.", "tokens": [407, 456, 311, 257, 1916, 295, 1880, 4122, 13], "temperature": 0.0, "avg_logprob": -0.09339377023641346, "compression_ratio": 1.8755364806866952, "no_speech_prob": 2.3922188120195642e-05}, {"id": 168, "seek": 69068, "start": 714.4399999999999, "end": 716.5799999999999, "text": " Always interesting to look at the leaderboard.", "tokens": [11270, 1880, 281, 574, 412, 264, 5263, 3787, 13], "temperature": 0.0, "avg_logprob": -0.09339377023641346, "compression_ratio": 1.8755364806866952, "no_speech_prob": 2.3922188120195642e-05}, {"id": 169, "seek": 71658, "start": 716.58, "end": 724.4000000000001, "text": " So the leaderboard, the winner was.1, the paper that we've roughly replicated was.105,", "tokens": [407, 264, 5263, 3787, 11, 264, 8507, 390, 2411, 16, 11, 264, 3035, 300, 321, 600, 9810, 46365, 390, 2411, 3279, 20, 11], "temperature": 0.0, "avg_logprob": -0.18386256858094097, "compression_ratio": 1.4197530864197532, "no_speech_prob": 9.368336577608716e-06}, {"id": 170, "seek": 71658, "start": 724.4000000000001, "end": 739.8000000000001, "text": ".106, and 10th place out of 3,000 was.11ish, a bit less.", "tokens": [2411, 3279, 21, 11, 293, 1266, 392, 1081, 484, 295, 805, 11, 1360, 390, 2411, 5348, 742, 11, 257, 857, 1570, 13], "temperature": 0.0, "avg_logprob": -0.18386256858094097, "compression_ratio": 1.4197530864197532, "no_speech_prob": 9.368336577608716e-06}, {"id": 171, "seek": 71658, "start": 739.8000000000001, "end": 745.88, "text": " So we're going to skip over a little bit, which is that the data that was provided here", "tokens": [407, 321, 434, 516, 281, 10023, 670, 257, 707, 857, 11, 597, 307, 300, 264, 1412, 300, 390, 5649, 510], "temperature": 0.0, "avg_logprob": -0.18386256858094097, "compression_ratio": 1.4197530864197532, "no_speech_prob": 9.368336577608716e-06}, {"id": 172, "seek": 74588, "start": 745.88, "end": 754.84, "text": " was they provided a small number of files but they also let competitors provide additional", "tokens": [390, 436, 5649, 257, 1359, 1230, 295, 7098, 457, 436, 611, 718, 18333, 2893, 4497], "temperature": 0.0, "avg_logprob": -0.11401596069335937, "compression_ratio": 1.681992337164751, "no_speech_prob": 5.09362780576339e-06}, {"id": 173, "seek": 74588, "start": 754.84, "end": 758.08, "text": " external data as long as they shared it with all the competitors.", "tokens": [8320, 1412, 382, 938, 382, 436, 5507, 309, 365, 439, 264, 18333, 13], "temperature": 0.0, "avg_logprob": -0.11401596069335937, "compression_ratio": 1.681992337164751, "no_speech_prob": 5.09362780576339e-06}, {"id": 174, "seek": 74588, "start": 758.08, "end": 762.08, "text": " And so in practice the data set we're going to use contains, I can't remember, six or", "tokens": [400, 370, 294, 3124, 264, 1412, 992, 321, 434, 516, 281, 764, 8306, 11, 286, 393, 380, 1604, 11, 2309, 420], "temperature": 0.0, "avg_logprob": -0.11401596069335937, "compression_ratio": 1.681992337164751, "no_speech_prob": 5.09362780576339e-06}, {"id": 175, "seek": 74588, "start": 762.08, "end": 764.68, "text": " seven tables.", "tokens": [3407, 8020, 13], "temperature": 0.0, "avg_logprob": -0.11401596069335937, "compression_ratio": 1.681992337164751, "no_speech_prob": 5.09362780576339e-06}, {"id": 176, "seek": 74588, "start": 764.68, "end": 769.56, "text": " The way that you join tables and stuff isn't really part of a deep learning course so I'm", "tokens": [440, 636, 300, 291, 3917, 8020, 293, 1507, 1943, 380, 534, 644, 295, 257, 2452, 2539, 1164, 370, 286, 478], "temperature": 0.0, "avg_logprob": -0.11401596069335937, "compression_ratio": 1.681992337164751, "no_speech_prob": 5.09362780576339e-06}, {"id": 177, "seek": 74588, "start": 769.56, "end": 773.46, "text": " going to skip over it and instead I'm going to refer you to Introduction to Machine Learning", "tokens": [516, 281, 10023, 670, 309, 293, 2602, 286, 478, 516, 281, 2864, 291, 281, 27193, 882, 281, 22155, 15205], "temperature": 0.0, "avg_logprob": -0.11401596069335937, "compression_ratio": 1.681992337164751, "no_speech_prob": 5.09362780576339e-06}, {"id": 178, "seek": 77346, "start": 773.46, "end": 779.2800000000001, "text": " for Coders which will take you step by step through the data preparation for this.", "tokens": [337, 383, 378, 433, 597, 486, 747, 291, 1823, 538, 1823, 807, 264, 1412, 13081, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.1931266422513165, "compression_ratio": 1.5767195767195767, "no_speech_prob": 7.766613634885289e-06}, {"id": 179, "seek": 77346, "start": 779.2800000000001, "end": 791.0, "text": " We've provided it for you in Rossman DataClean so you'll see the whole process there and", "tokens": [492, 600, 5649, 309, 337, 291, 294, 16140, 1601, 11888, 34, 28499, 370, 291, 603, 536, 264, 1379, 1399, 456, 293], "temperature": 0.0, "avg_logprob": -0.1931266422513165, "compression_ratio": 1.5767195767195767, "no_speech_prob": 7.766613634885289e-06}, {"id": 180, "seek": 77346, "start": 791.0, "end": 796.6, "text": " so you'll need to run through that notebook to create these pickle files that we read", "tokens": [370, 291, 603, 643, 281, 1190, 807, 300, 21060, 281, 1884, 613, 31433, 7098, 300, 321, 1401], "temperature": 0.0, "avg_logprob": -0.1931266422513165, "compression_ratio": 1.5767195767195767, "no_speech_prob": 7.766613634885289e-06}, {"id": 181, "seek": 77346, "start": 796.6, "end": 797.6, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.1931266422513165, "compression_ratio": 1.5767195767195767, "no_speech_prob": 7.766613634885289e-06}, {"id": 182, "seek": 77346, "start": 797.6, "end": 802.9200000000001, "text": " Can you see this in the back okay?", "tokens": [1664, 291, 536, 341, 294, 264, 646, 1392, 30], "temperature": 0.0, "avg_logprob": -0.1931266422513165, "compression_ratio": 1.5767195767195767, "no_speech_prob": 7.766613634885289e-06}, {"id": 183, "seek": 80292, "start": 802.92, "end": 809.52, "text": " I just want to mention one particularly interesting part of the Rossman DataClean notebook which", "tokens": [286, 445, 528, 281, 2152, 472, 4098, 1880, 644, 295, 264, 16140, 1601, 11888, 34, 28499, 21060, 597], "temperature": 0.0, "avg_logprob": -0.11496169567108154, "compression_ratio": 1.7079037800687284, "no_speech_prob": 1.3631006368086673e-05}, {"id": 184, "seek": 80292, "start": 809.52, "end": 813.28, "text": " is you'll see there's something that says add date part and I wanted to explain what's", "tokens": [307, 291, 603, 536, 456, 311, 746, 300, 1619, 909, 4002, 644, 293, 286, 1415, 281, 2903, 437, 311], "temperature": 0.0, "avg_logprob": -0.11496169567108154, "compression_ratio": 1.7079037800687284, "no_speech_prob": 1.3631006368086673e-05}, {"id": 185, "seek": 80292, "start": 813.28, "end": 814.28, "text": " going on here.", "tokens": [516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.11496169567108154, "compression_ratio": 1.7079037800687284, "no_speech_prob": 1.3631006368086673e-05}, {"id": 186, "seek": 80292, "start": 814.28, "end": 818.56, "text": " I've been mentioning for a while that we're going to look at time series and pretty much", "tokens": [286, 600, 668, 18315, 337, 257, 1339, 300, 321, 434, 516, 281, 574, 412, 565, 2638, 293, 1238, 709], "temperature": 0.0, "avg_logprob": -0.11496169567108154, "compression_ratio": 1.7079037800687284, "no_speech_prob": 1.3631006368086673e-05}, {"id": 187, "seek": 80292, "start": 818.56, "end": 821.7199999999999, "text": " everybody who I've spoken to about it has assumed that I'm going to do some kind of", "tokens": [2201, 567, 286, 600, 10759, 281, 466, 309, 575, 15895, 300, 286, 478, 516, 281, 360, 512, 733, 295], "temperature": 0.0, "avg_logprob": -0.11496169567108154, "compression_ratio": 1.7079037800687284, "no_speech_prob": 1.3631006368086673e-05}, {"id": 188, "seek": 80292, "start": 821.7199999999999, "end": 825.92, "text": " recurrent neural network but I'm not.", "tokens": [18680, 1753, 18161, 3209, 457, 286, 478, 406, 13], "temperature": 0.0, "avg_logprob": -0.11496169567108154, "compression_ratio": 1.7079037800687284, "no_speech_prob": 1.3631006368086673e-05}, {"id": 189, "seek": 80292, "start": 825.92, "end": 831.88, "text": " Interestingly the main academic group that studies time series is Econometrics but they", "tokens": [30564, 264, 2135, 7778, 1594, 300, 5313, 565, 2638, 307, 462, 1671, 649, 10716, 457, 436], "temperature": 0.0, "avg_logprob": -0.11496169567108154, "compression_ratio": 1.7079037800687284, "no_speech_prob": 1.3631006368086673e-05}, {"id": 190, "seek": 83188, "start": 831.88, "end": 837.64, "text": " tend to study one very specific kind of time series which is where the only data you have", "tokens": [3928, 281, 2979, 472, 588, 2685, 733, 295, 565, 2638, 597, 307, 689, 264, 787, 1412, 291, 362], "temperature": 0.0, "avg_logprob": -0.16110626567493785, "compression_ratio": 1.8672199170124482, "no_speech_prob": 9.972100087907165e-06}, {"id": 191, "seek": 83188, "start": 837.64, "end": 841.16, "text": " is a sequence of time points of one thing.", "tokens": [307, 257, 8310, 295, 565, 2793, 295, 472, 551, 13], "temperature": 0.0, "avg_logprob": -0.16110626567493785, "compression_ratio": 1.8672199170124482, "no_speech_prob": 9.972100087907165e-06}, {"id": 192, "seek": 83188, "start": 841.16, "end": 843.76, "text": " That's the only thing you have is one sequence.", "tokens": [663, 311, 264, 787, 551, 291, 362, 307, 472, 8310, 13], "temperature": 0.0, "avg_logprob": -0.16110626567493785, "compression_ratio": 1.8672199170124482, "no_speech_prob": 9.972100087907165e-06}, {"id": 193, "seek": 83188, "start": 843.76, "end": 846.2, "text": " In real life that's almost never the case.", "tokens": [682, 957, 993, 300, 311, 1920, 1128, 264, 1389, 13], "temperature": 0.0, "avg_logprob": -0.16110626567493785, "compression_ratio": 1.8672199170124482, "no_speech_prob": 9.972100087907165e-06}, {"id": 194, "seek": 83188, "start": 846.2, "end": 850.22, "text": " Normally we would have some information about the store that that represents or the people", "tokens": [17424, 321, 576, 362, 512, 1589, 466, 264, 3531, 300, 300, 8855, 420, 264, 561], "temperature": 0.0, "avg_logprob": -0.16110626567493785, "compression_ratio": 1.8672199170124482, "no_speech_prob": 9.972100087907165e-06}, {"id": 195, "seek": 83188, "start": 850.22, "end": 851.22, "text": " that it represents.", "tokens": [300, 309, 8855, 13], "temperature": 0.0, "avg_logprob": -0.16110626567493785, "compression_ratio": 1.8672199170124482, "no_speech_prob": 9.972100087907165e-06}, {"id": 196, "seek": 83188, "start": 851.22, "end": 852.96, "text": " We'd have metadata.", "tokens": [492, 1116, 362, 26603, 13], "temperature": 0.0, "avg_logprob": -0.16110626567493785, "compression_ratio": 1.8672199170124482, "no_speech_prob": 9.972100087907165e-06}, {"id": 197, "seek": 83188, "start": 852.96, "end": 856.4, "text": " We'd have sequences of other things measured at similar time periods or different time", "tokens": [492, 1116, 362, 22978, 295, 661, 721, 12690, 412, 2531, 565, 13804, 420, 819, 565], "temperature": 0.0, "avg_logprob": -0.16110626567493785, "compression_ratio": 1.8672199170124482, "no_speech_prob": 9.972100087907165e-06}, {"id": 198, "seek": 83188, "start": 856.4, "end": 859.6, "text": " periods.", "tokens": [13804, 13], "temperature": 0.0, "avg_logprob": -0.16110626567493785, "compression_ratio": 1.8672199170124482, "no_speech_prob": 9.972100087907165e-06}, {"id": 199, "seek": 85960, "start": 859.6, "end": 866.36, "text": " And so most of the time I find in practice the state of the art results when it comes", "tokens": [400, 370, 881, 295, 264, 565, 286, 915, 294, 3124, 264, 1785, 295, 264, 1523, 3542, 562, 309, 1487], "temperature": 0.0, "avg_logprob": -0.10406825175652137, "compression_ratio": 1.7148936170212765, "no_speech_prob": 3.726566092154826e-06}, {"id": 200, "seek": 85960, "start": 866.36, "end": 871.16, "text": " to competitions on kind of more real world data sets don't tend to use recurrent neural", "tokens": [281, 26185, 322, 733, 295, 544, 957, 1002, 1412, 6352, 500, 380, 3928, 281, 764, 18680, 1753, 18161], "temperature": 0.0, "avg_logprob": -0.10406825175652137, "compression_ratio": 1.7148936170212765, "no_speech_prob": 3.726566092154826e-06}, {"id": 201, "seek": 85960, "start": 871.16, "end": 876.9200000000001, "text": " networks but instead they tend to take the time piece which in this case it was a date", "tokens": [9590, 457, 2602, 436, 3928, 281, 747, 264, 565, 2522, 597, 294, 341, 1389, 309, 390, 257, 4002], "temperature": 0.0, "avg_logprob": -0.10406825175652137, "compression_ratio": 1.7148936170212765, "no_speech_prob": 3.726566092154826e-06}, {"id": 202, "seek": 85960, "start": 876.9200000000001, "end": 881.62, "text": " we were given in the data and they add a whole bunch of metadata.", "tokens": [321, 645, 2212, 294, 264, 1412, 293, 436, 909, 257, 1379, 3840, 295, 26603, 13], "temperature": 0.0, "avg_logprob": -0.10406825175652137, "compression_ratio": 1.7148936170212765, "no_speech_prob": 3.726566092154826e-06}, {"id": 203, "seek": 85960, "start": 881.62, "end": 884.64, "text": " So in our case for example we've added day of week.", "tokens": [407, 294, 527, 1389, 337, 1365, 321, 600, 3869, 786, 295, 1243, 13], "temperature": 0.0, "avg_logprob": -0.10406825175652137, "compression_ratio": 1.7148936170212765, "no_speech_prob": 3.726566092154826e-06}, {"id": 204, "seek": 85960, "start": 884.64, "end": 886.48, "text": " So we were given a date.", "tokens": [407, 321, 645, 2212, 257, 4002, 13], "temperature": 0.0, "avg_logprob": -0.10406825175652137, "compression_ratio": 1.7148936170212765, "no_speech_prob": 3.726566092154826e-06}, {"id": 205, "seek": 88648, "start": 886.48, "end": 894.44, "text": " We've added day of week, year, month, week of year, day of month, day of week, day of", "tokens": [492, 600, 3869, 786, 295, 1243, 11, 1064, 11, 1618, 11, 1243, 295, 1064, 11, 786, 295, 1618, 11, 786, 295, 1243, 11, 786, 295], "temperature": 0.0, "avg_logprob": -0.13273171134617018, "compression_ratio": 1.7076271186440677, "no_speech_prob": 7.2960378929565195e-06}, {"id": 206, "seek": 88648, "start": 894.44, "end": 898.76, "text": " year and then a bunch of Booleans is at the month start or end, quarter, year start or", "tokens": [1064, 293, 550, 257, 3840, 295, 23351, 24008, 307, 412, 264, 1618, 722, 420, 917, 11, 6555, 11, 1064, 722, 420], "temperature": 0.0, "avg_logprob": -0.13273171134617018, "compression_ratio": 1.7076271186440677, "no_speech_prob": 7.2960378929565195e-06}, {"id": 207, "seek": 88648, "start": 898.76, "end": 903.2, "text": " end, elapsed time since 1970 and so forth.", "tokens": [917, 11, 806, 2382, 292, 565, 1670, 14577, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.13273171134617018, "compression_ratio": 1.7076271186440677, "no_speech_prob": 7.2960378929565195e-06}, {"id": 208, "seek": 88648, "start": 903.2, "end": 907.72, "text": " If you run this one function add date part and pass it a date it'll add all of these", "tokens": [759, 291, 1190, 341, 472, 2445, 909, 4002, 644, 293, 1320, 309, 257, 4002, 309, 603, 909, 439, 295, 613], "temperature": 0.0, "avg_logprob": -0.13273171134617018, "compression_ratio": 1.7076271186440677, "no_speech_prob": 7.2960378929565195e-06}, {"id": 209, "seek": 88648, "start": 907.72, "end": 910.48, "text": " columns to your data set for you.", "tokens": [13766, 281, 428, 1412, 992, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.13273171134617018, "compression_ratio": 1.7076271186440677, "no_speech_prob": 7.2960378929565195e-06}, {"id": 210, "seek": 88648, "start": 910.48, "end": 916.2, "text": " And so what that means is that let's take a very reasonable example.", "tokens": [400, 370, 437, 300, 1355, 307, 300, 718, 311, 747, 257, 588, 10585, 1365, 13], "temperature": 0.0, "avg_logprob": -0.13273171134617018, "compression_ratio": 1.7076271186440677, "no_speech_prob": 7.2960378929565195e-06}, {"id": 211, "seek": 91620, "start": 916.2, "end": 918.36, "text": " Featuring behavior probably changes on payday.", "tokens": [3697, 267, 1345, 5223, 1391, 2962, 322, 1689, 810, 13], "temperature": 0.0, "avg_logprob": -0.19686220206466376, "compression_ratio": 1.5655737704918034, "no_speech_prob": 3.169147385051474e-05}, {"id": 212, "seek": 91620, "start": 918.36, "end": 921.2800000000001, "text": " Payday might be the 15th of the month.", "tokens": [11431, 810, 1062, 312, 264, 2119, 392, 295, 264, 1618, 13], "temperature": 0.0, "avg_logprob": -0.19686220206466376, "compression_ratio": 1.5655737704918034, "no_speech_prob": 3.169147385051474e-05}, {"id": 213, "seek": 91620, "start": 921.2800000000001, "end": 926.88, "text": " So if you have a thing here called this is day of month here then it'll be able to recognize", "tokens": [407, 498, 291, 362, 257, 551, 510, 1219, 341, 307, 786, 295, 1618, 510, 550, 309, 603, 312, 1075, 281, 5521], "temperature": 0.0, "avg_logprob": -0.19686220206466376, "compression_ratio": 1.5655737704918034, "no_speech_prob": 3.169147385051474e-05}, {"id": 214, "seek": 91620, "start": 926.88, "end": 933.6400000000001, "text": " every time something is a 15 there and associated it with a higher in this case embedding matrix", "tokens": [633, 565, 746, 307, 257, 2119, 456, 293, 6615, 309, 365, 257, 2946, 294, 341, 1389, 12240, 3584, 8141], "temperature": 0.0, "avg_logprob": -0.19686220206466376, "compression_ratio": 1.5655737704918034, "no_speech_prob": 3.169147385051474e-05}, {"id": 215, "seek": 91620, "start": 933.6400000000001, "end": 935.44, "text": " value.", "tokens": [2158, 13], "temperature": 0.0, "avg_logprob": -0.19686220206466376, "compression_ratio": 1.5655737704918034, "no_speech_prob": 3.169147385051474e-05}, {"id": 216, "seek": 91620, "start": 935.44, "end": 942.32, "text": " So this way it basically, we can't expect a neural net to do all of our feature engineering", "tokens": [407, 341, 636, 309, 1936, 11, 321, 393, 380, 2066, 257, 18161, 2533, 281, 360, 439, 295, 527, 4111, 7043], "temperature": 0.0, "avg_logprob": -0.19686220206466376, "compression_ratio": 1.5655737704918034, "no_speech_prob": 3.169147385051474e-05}, {"id": 217, "seek": 91620, "start": 942.32, "end": 943.32, "text": " for us.", "tokens": [337, 505, 13], "temperature": 0.0, "avg_logprob": -0.19686220206466376, "compression_ratio": 1.5655737704918034, "no_speech_prob": 3.169147385051474e-05}, {"id": 218, "seek": 94332, "start": 943.32, "end": 947.2800000000001, "text": " We can expect it to kind of find nonlinearities and interactions and stuff like that.", "tokens": [492, 393, 2066, 309, 281, 733, 295, 915, 2107, 28263, 1088, 293, 13280, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.13970376014709474, "compression_ratio": 1.708, "no_speech_prob": 5.862766101927264e-06}, {"id": 219, "seek": 94332, "start": 947.2800000000001, "end": 954.0400000000001, "text": " But for something like taking a date like this and figuring out that the 15th of the", "tokens": [583, 337, 746, 411, 1940, 257, 4002, 411, 341, 293, 15213, 484, 300, 264, 2119, 392, 295, 264], "temperature": 0.0, "avg_logprob": -0.13970376014709474, "compression_ratio": 1.708, "no_speech_prob": 5.862766101927264e-06}, {"id": 220, "seek": 94332, "start": 954.0400000000001, "end": 958.08, "text": " month is something when interesting things happen it's much better if we can provide", "tokens": [1618, 307, 746, 562, 1880, 721, 1051, 309, 311, 709, 1101, 498, 321, 393, 2893], "temperature": 0.0, "avg_logprob": -0.13970376014709474, "compression_ratio": 1.708, "no_speech_prob": 5.862766101927264e-06}, {"id": 221, "seek": 94332, "start": 958.08, "end": 960.2, "text": " that information for it.", "tokens": [300, 1589, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.13970376014709474, "compression_ratio": 1.708, "no_speech_prob": 5.862766101927264e-06}, {"id": 222, "seek": 94332, "start": 960.2, "end": 964.94, "text": " So this is a really useful function to use and once you've done this you can treat many", "tokens": [407, 341, 307, 257, 534, 4420, 2445, 281, 764, 293, 1564, 291, 600, 1096, 341, 291, 393, 2387, 867], "temperature": 0.0, "avg_logprob": -0.13970376014709474, "compression_ratio": 1.708, "no_speech_prob": 5.862766101927264e-06}, {"id": 223, "seek": 94332, "start": 964.94, "end": 968.6, "text": " kinds of time series problems as regular tabular problems.", "tokens": [3685, 295, 565, 2638, 2740, 382, 3890, 4421, 1040, 2740, 13], "temperature": 0.0, "avg_logprob": -0.13970376014709474, "compression_ratio": 1.708, "no_speech_prob": 5.862766101927264e-06}, {"id": 224, "seek": 96860, "start": 968.6, "end": 975.44, "text": " I say many kinds, not all, if there's very complex kind of state involved in a time series", "tokens": [286, 584, 867, 3685, 11, 406, 439, 11, 498, 456, 311, 588, 3997, 733, 295, 1785, 3288, 294, 257, 565, 2638], "temperature": 0.0, "avg_logprob": -0.16929993402390253, "compression_ratio": 1.6150793650793651, "no_speech_prob": 6.438927357521607e-06}, {"id": 225, "seek": 96860, "start": 975.44, "end": 981.36, "text": " such as equity trading or something like that it probably won't be the case or this won't", "tokens": [1270, 382, 10769, 9529, 420, 746, 411, 300, 309, 1391, 1582, 380, 312, 264, 1389, 420, 341, 1582, 380], "temperature": 0.0, "avg_logprob": -0.16929993402390253, "compression_ratio": 1.6150793650793651, "no_speech_prob": 6.438927357521607e-06}, {"id": 226, "seek": 96860, "start": 981.36, "end": 983.44, "text": " be the only thing you need.", "tokens": [312, 264, 787, 551, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.16929993402390253, "compression_ratio": 1.6150793650793651, "no_speech_prob": 6.438927357521607e-06}, {"id": 227, "seek": 96860, "start": 983.44, "end": 989.6800000000001, "text": " But in this case it'll get us a really good result and in practice most of the time I", "tokens": [583, 294, 341, 1389, 309, 603, 483, 505, 257, 534, 665, 1874, 293, 294, 3124, 881, 295, 264, 565, 286], "temperature": 0.0, "avg_logprob": -0.16929993402390253, "compression_ratio": 1.6150793650793651, "no_speech_prob": 6.438927357521607e-06}, {"id": 228, "seek": 96860, "start": 989.6800000000001, "end": 992.4, "text": " find this works well.", "tokens": [915, 341, 1985, 731, 13], "temperature": 0.0, "avg_logprob": -0.16929993402390253, "compression_ratio": 1.6150793650793651, "no_speech_prob": 6.438927357521607e-06}, {"id": 229, "seek": 96860, "start": 992.4, "end": 997.3000000000001, "text": " Tabular data is normally in pandas so we just stored them as standard Python pickle files.", "tokens": [14106, 1040, 1412, 307, 5646, 294, 4565, 296, 370, 321, 445, 12187, 552, 382, 3832, 15329, 31433, 7098, 13], "temperature": 0.0, "avg_logprob": -0.16929993402390253, "compression_ratio": 1.6150793650793651, "no_speech_prob": 6.438927357521607e-06}, {"id": 230, "seek": 99730, "start": 997.3, "end": 1002.24, "text": " We can read them in, we can take a look at the first five records and so the key thing", "tokens": [492, 393, 1401, 552, 294, 11, 321, 393, 747, 257, 574, 412, 264, 700, 1732, 7724, 293, 370, 264, 2141, 551], "temperature": 0.0, "avg_logprob": -0.108889990990315, "compression_ratio": 1.693798449612403, "no_speech_prob": 6.643137112405384e-06}, {"id": 231, "seek": 99730, "start": 1002.24, "end": 1008.04, "text": " here is that we're trying to on a particular date for a particular store ID we want to", "tokens": [510, 307, 300, 321, 434, 1382, 281, 322, 257, 1729, 4002, 337, 257, 1729, 3531, 7348, 321, 528, 281], "temperature": 0.0, "avg_logprob": -0.108889990990315, "compression_ratio": 1.693798449612403, "no_speech_prob": 6.643137112405384e-06}, {"id": 232, "seek": 99730, "start": 1008.04, "end": 1009.8399999999999, "text": " predict the number of sales.", "tokens": [6069, 264, 1230, 295, 5763, 13], "temperature": 0.0, "avg_logprob": -0.108889990990315, "compression_ratio": 1.693798449612403, "no_speech_prob": 6.643137112405384e-06}, {"id": 233, "seek": 99730, "start": 1009.8399999999999, "end": 1014.5999999999999, "text": " Sales is the dependent variable.", "tokens": [23467, 307, 264, 12334, 7006, 13], "temperature": 0.0, "avg_logprob": -0.108889990990315, "compression_ratio": 1.693798449612403, "no_speech_prob": 6.643137112405384e-06}, {"id": 234, "seek": 99730, "start": 1014.5999999999999, "end": 1018.8399999999999, "text": " So the first thing I'm going to show you is something called preprocessors.", "tokens": [407, 264, 700, 551, 286, 478, 516, 281, 855, 291, 307, 746, 1219, 2666, 340, 45700, 13], "temperature": 0.0, "avg_logprob": -0.108889990990315, "compression_ratio": 1.693798449612403, "no_speech_prob": 6.643137112405384e-06}, {"id": 235, "seek": 99730, "start": 1018.8399999999999, "end": 1020.8, "text": " You've already learned about transforms.", "tokens": [509, 600, 1217, 3264, 466, 35592, 13], "temperature": 0.0, "avg_logprob": -0.108889990990315, "compression_ratio": 1.693798449612403, "no_speech_prob": 6.643137112405384e-06}, {"id": 236, "seek": 99730, "start": 1020.8, "end": 1026.52, "text": " Transforms are bits of code that run every time something is grabbed from a data set", "tokens": [27938, 82, 366, 9239, 295, 3089, 300, 1190, 633, 565, 746, 307, 18607, 490, 257, 1412, 992], "temperature": 0.0, "avg_logprob": -0.108889990990315, "compression_ratio": 1.693798449612403, "no_speech_prob": 6.643137112405384e-06}, {"id": 237, "seek": 102652, "start": 1026.52, "end": 1030.72, "text": " and so it's really good for data augmentation that we'll learn about today which is that", "tokens": [293, 370, 309, 311, 534, 665, 337, 1412, 14501, 19631, 300, 321, 603, 1466, 466, 965, 597, 307, 300], "temperature": 0.0, "avg_logprob": -0.10451351679288425, "compression_ratio": 1.7813765182186234, "no_speech_prob": 9.22324488783488e-06}, {"id": 238, "seek": 102652, "start": 1030.72, "end": 1035.04, "text": " it's going to get a different random value every time it's sampled.", "tokens": [309, 311, 516, 281, 483, 257, 819, 4974, 2158, 633, 565, 309, 311, 3247, 15551, 13], "temperature": 0.0, "avg_logprob": -0.10451351679288425, "compression_ratio": 1.7813765182186234, "no_speech_prob": 9.22324488783488e-06}, {"id": 239, "seek": 102652, "start": 1035.04, "end": 1039.8799999999999, "text": " Preprocessors are like transforms but they're a little bit different which is that they", "tokens": [6001, 41075, 830, 366, 411, 35592, 457, 436, 434, 257, 707, 857, 819, 597, 307, 300, 436], "temperature": 0.0, "avg_logprob": -0.10451351679288425, "compression_ratio": 1.7813765182186234, "no_speech_prob": 9.22324488783488e-06}, {"id": 240, "seek": 102652, "start": 1039.8799999999999, "end": 1047.72, "text": " run once before you do any training and really importantly they run once on the training", "tokens": [1190, 1564, 949, 291, 360, 604, 3097, 293, 534, 8906, 436, 1190, 1564, 322, 264, 3097], "temperature": 0.0, "avg_logprob": -0.10451351679288425, "compression_ratio": 1.7813765182186234, "no_speech_prob": 9.22324488783488e-06}, {"id": 241, "seek": 102652, "start": 1047.72, "end": 1054.7, "text": " set and then any kind of state or metadata that's created is then shared with the validation", "tokens": [992, 293, 550, 604, 733, 295, 1785, 420, 26603, 300, 311, 2942, 307, 550, 5507, 365, 264, 24071], "temperature": 0.0, "avg_logprob": -0.10451351679288425, "compression_ratio": 1.7813765182186234, "no_speech_prob": 9.22324488783488e-06}, {"id": 242, "seek": 102652, "start": 1054.7, "end": 1056.08, "text": " and test set.", "tokens": [293, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.10451351679288425, "compression_ratio": 1.7813765182186234, "no_speech_prob": 9.22324488783488e-06}, {"id": 243, "seek": 105608, "start": 1056.08, "end": 1057.62, "text": " Let me give you an example.", "tokens": [961, 385, 976, 291, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.09646473487798315, "compression_ratio": 1.8633333333333333, "no_speech_prob": 4.637819529307308e-06}, {"id": 244, "seek": 105608, "start": 1057.62, "end": 1062.6399999999999, "text": " When we've been doing image recognition and we've had a set of classes for like all the", "tokens": [1133, 321, 600, 668, 884, 3256, 11150, 293, 321, 600, 632, 257, 992, 295, 5359, 337, 411, 439, 264], "temperature": 0.0, "avg_logprob": -0.09646473487798315, "compression_ratio": 1.8633333333333333, "no_speech_prob": 4.637819529307308e-06}, {"id": 245, "seek": 105608, "start": 1062.6399999999999, "end": 1067.28, "text": " different pet breeds and they've been turned into numbers, the thing that's actually doing", "tokens": [819, 3817, 41609, 293, 436, 600, 668, 3574, 666, 3547, 11, 264, 551, 300, 311, 767, 884], "temperature": 0.0, "avg_logprob": -0.09646473487798315, "compression_ratio": 1.8633333333333333, "no_speech_prob": 4.637819529307308e-06}, {"id": 246, "seek": 105608, "start": 1067.28, "end": 1070.6799999999998, "text": " that for us is a preprocessor that's being created in the background.", "tokens": [300, 337, 505, 307, 257, 2666, 340, 25432, 300, 311, 885, 2942, 294, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.09646473487798315, "compression_ratio": 1.8633333333333333, "no_speech_prob": 4.637819529307308e-06}, {"id": 247, "seek": 105608, "start": 1070.6799999999998, "end": 1074.8, "text": " So that makes sure that the classes for the training set are the same as the classes for", "tokens": [407, 300, 1669, 988, 300, 264, 5359, 337, 264, 3097, 992, 366, 264, 912, 382, 264, 5359, 337], "temperature": 0.0, "avg_logprob": -0.09646473487798315, "compression_ratio": 1.8633333333333333, "no_speech_prob": 4.637819529307308e-06}, {"id": 248, "seek": 105608, "start": 1074.8, "end": 1077.9399999999998, "text": " the validation and the classes for the test set.", "tokens": [264, 24071, 293, 264, 5359, 337, 264, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.09646473487798315, "compression_ratio": 1.8633333333333333, "no_speech_prob": 4.637819529307308e-06}, {"id": 249, "seek": 105608, "start": 1077.9399999999998, "end": 1080.6399999999999, "text": " So we're going to do something very similar here.", "tokens": [407, 321, 434, 516, 281, 360, 746, 588, 2531, 510, 13], "temperature": 0.0, "avg_logprob": -0.09646473487798315, "compression_ratio": 1.8633333333333333, "no_speech_prob": 4.637819529307308e-06}, {"id": 250, "seek": 105608, "start": 1080.6399999999999, "end": 1086.0, "text": " For example, if we create a little small subset of the data for playing with, this is a really", "tokens": [1171, 1365, 11, 498, 321, 1884, 257, 707, 1359, 25993, 295, 264, 1412, 337, 2433, 365, 11, 341, 307, 257, 534], "temperature": 0.0, "avg_logprob": -0.09646473487798315, "compression_ratio": 1.8633333333333333, "no_speech_prob": 4.637819529307308e-06}, {"id": 251, "seek": 108600, "start": 1086.0, "end": 1088.32, "text": " good idea when you start with a new data set.", "tokens": [665, 1558, 562, 291, 722, 365, 257, 777, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.16925347157013723, "compression_ratio": 1.7886178861788617, "no_speech_prob": 1.0451364687469322e-05}, {"id": 252, "seek": 108600, "start": 1088.32, "end": 1095.52, "text": " So I've just grabbed 2000 IDs at random and then I'm just going to grab a little training", "tokens": [407, 286, 600, 445, 18607, 8132, 48212, 412, 4974, 293, 550, 286, 478, 445, 516, 281, 4444, 257, 707, 3097], "temperature": 0.0, "avg_logprob": -0.16925347157013723, "compression_ratio": 1.7886178861788617, "no_speech_prob": 1.0451364687469322e-05}, {"id": 253, "seek": 108600, "start": 1095.52, "end": 1098.16, "text": " set and a little test set, half and half of those 2000 IDs.", "tokens": [992, 293, 257, 707, 1500, 992, 11, 1922, 293, 1922, 295, 729, 8132, 48212, 13], "temperature": 0.0, "avg_logprob": -0.16925347157013723, "compression_ratio": 1.7886178861788617, "no_speech_prob": 1.0451364687469322e-05}, {"id": 254, "seek": 108600, "start": 1098.16, "end": 1102.96, "text": " I'm just going to grab five columns and then we can just play around with this nice and", "tokens": [286, 478, 445, 516, 281, 4444, 1732, 13766, 293, 550, 321, 393, 445, 862, 926, 365, 341, 1481, 293], "temperature": 0.0, "avg_logprob": -0.16925347157013723, "compression_ratio": 1.7886178861788617, "no_speech_prob": 1.0451364687469322e-05}, {"id": 255, "seek": 108600, "start": 1102.96, "end": 1103.96, "text": " easy.", "tokens": [1858, 13], "temperature": 0.0, "avg_logprob": -0.16925347157013723, "compression_ratio": 1.7886178861788617, "no_speech_prob": 1.0451364687469322e-05}, {"id": 256, "seek": 108600, "start": 1103.96, "end": 1107.84, "text": " So here's the first few of those from the training set.", "tokens": [407, 510, 311, 264, 700, 1326, 295, 729, 490, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.16925347157013723, "compression_ratio": 1.7886178861788617, "no_speech_prob": 1.0451364687469322e-05}, {"id": 257, "seek": 108600, "start": 1107.84, "end": 1113.88, "text": " As you can see one of them is called promoInterval and it has these strings and sometimes it's", "tokens": [1018, 291, 393, 536, 472, 295, 552, 307, 1219, 26750, 13406, 3337, 293, 309, 575, 613, 13985, 293, 2171, 309, 311], "temperature": 0.0, "avg_logprob": -0.16925347157013723, "compression_ratio": 1.7886178861788617, "no_speech_prob": 1.0451364687469322e-05}, {"id": 258, "seek": 111388, "start": 1113.88, "end": 1119.96, "text": " missing and pandas missing is nan.", "tokens": [5361, 293, 4565, 296, 5361, 307, 14067, 13], "temperature": 0.0, "avg_logprob": -0.10775938901034268, "compression_ratio": 1.7733333333333334, "no_speech_prob": 7.296299372683279e-06}, {"id": 259, "seek": 111388, "start": 1119.96, "end": 1125.0400000000002, "text": " So the first preprocessor I'll show you is categorify and categorify does basically the", "tokens": [407, 264, 700, 2666, 340, 25432, 286, 603, 855, 291, 307, 19250, 2505, 293, 19250, 2505, 775, 1936, 264], "temperature": 0.0, "avg_logprob": -0.10775938901034268, "compression_ratio": 1.7733333333333334, "no_speech_prob": 7.296299372683279e-06}, {"id": 260, "seek": 111388, "start": 1125.0400000000002, "end": 1130.5200000000002, "text": " same thing that that classes thing for image recognition does for our dependent variable.", "tokens": [912, 551, 300, 300, 5359, 551, 337, 3256, 11150, 775, 337, 527, 12334, 7006, 13], "temperature": 0.0, "avg_logprob": -0.10775938901034268, "compression_ratio": 1.7733333333333334, "no_speech_prob": 7.296299372683279e-06}, {"id": 261, "seek": 111388, "start": 1130.5200000000002, "end": 1134.72, "text": " It's going to take these strings, it's going to find all of the possible unique values", "tokens": [467, 311, 516, 281, 747, 613, 13985, 11, 309, 311, 516, 281, 915, 439, 295, 264, 1944, 3845, 4190], "temperature": 0.0, "avg_logprob": -0.10775938901034268, "compression_ratio": 1.7733333333333334, "no_speech_prob": 7.296299372683279e-06}, {"id": 262, "seek": 111388, "start": 1134.72, "end": 1138.96, "text": " of it and it's going to create a list of them and then it's going to turn the strings into", "tokens": [295, 309, 293, 309, 311, 516, 281, 1884, 257, 1329, 295, 552, 293, 550, 309, 311, 516, 281, 1261, 264, 13985, 666], "temperature": 0.0, "avg_logprob": -0.10775938901034268, "compression_ratio": 1.7733333333333334, "no_speech_prob": 7.296299372683279e-06}, {"id": 263, "seek": 111388, "start": 1138.96, "end": 1140.0600000000002, "text": " numbers.", "tokens": [3547, 13], "temperature": 0.0, "avg_logprob": -0.10775938901034268, "compression_ratio": 1.7733333333333334, "no_speech_prob": 7.296299372683279e-06}, {"id": 264, "seek": 114006, "start": 1140.06, "end": 1144.96, "text": " So if I call it on my training set that'll create categories there and then I call it", "tokens": [407, 498, 286, 818, 309, 322, 452, 3097, 992, 300, 603, 1884, 10479, 456, 293, 550, 286, 818, 309], "temperature": 0.0, "avg_logprob": -0.1086921896985782, "compression_ratio": 1.71875, "no_speech_prob": 3.1875420063443016e-06}, {"id": 265, "seek": 114006, "start": 1144.96, "end": 1149.84, "text": " on my test set, passing in test equals true, that makes sure it's going to use the same", "tokens": [322, 452, 1500, 992, 11, 8437, 294, 1500, 6915, 2074, 11, 300, 1669, 988, 309, 311, 516, 281, 764, 264, 912], "temperature": 0.0, "avg_logprob": -0.1086921896985782, "compression_ratio": 1.71875, "no_speech_prob": 3.1875420063443016e-06}, {"id": 266, "seek": 114006, "start": 1149.84, "end": 1152.6399999999999, "text": " categories that I had before.", "tokens": [10479, 300, 286, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.1086921896985782, "compression_ratio": 1.71875, "no_speech_prob": 3.1875420063443016e-06}, {"id": 267, "seek": 114006, "start": 1152.6399999999999, "end": 1158.0, "text": " And now when I say.head it looks exactly the same and that's because pandas has turned", "tokens": [400, 586, 562, 286, 584, 2411, 1934, 309, 1542, 2293, 264, 912, 293, 300, 311, 570, 4565, 296, 575, 3574], "temperature": 0.0, "avg_logprob": -0.1086921896985782, "compression_ratio": 1.71875, "no_speech_prob": 3.1875420063443016e-06}, {"id": 268, "seek": 114006, "start": 1158.0, "end": 1165.08, "text": " this into a categorical variable which internally is storing numbers but externally is showing", "tokens": [341, 666, 257, 19250, 804, 7006, 597, 19501, 307, 26085, 3547, 457, 40899, 307, 4099], "temperature": 0.0, "avg_logprob": -0.1086921896985782, "compression_ratio": 1.71875, "no_speech_prob": 3.1875420063443016e-06}, {"id": 269, "seek": 116508, "start": 1165.08, "end": 1171.76, "text": " me the strings but I can look inside promoInterval to look at the cat categories, this is all", "tokens": [385, 264, 13985, 457, 286, 393, 574, 1854, 26750, 13406, 3337, 281, 574, 412, 264, 3857, 10479, 11, 341, 307, 439], "temperature": 0.0, "avg_logprob": -0.17296358255239633, "compression_ratio": 1.7934272300469483, "no_speech_prob": 4.157327111897757e-06}, {"id": 270, "seek": 116508, "start": 1171.76, "end": 1178.1999999999998, "text": " standard pandas here, to show me a list of all of what we would call classes in fast", "tokens": [3832, 4565, 296, 510, 11, 281, 855, 385, 257, 1329, 295, 439, 295, 437, 321, 576, 818, 5359, 294, 2370], "temperature": 0.0, "avg_logprob": -0.17296358255239633, "compression_ratio": 1.7934272300469483, "no_speech_prob": 4.157327111897757e-06}, {"id": 271, "seek": 116508, "start": 1178.1999999999998, "end": 1182.72, "text": " AI or would be called just categories in pandas.", "tokens": [7318, 420, 576, 312, 1219, 445, 10479, 294, 4565, 296, 13], "temperature": 0.0, "avg_logprob": -0.17296358255239633, "compression_ratio": 1.7934272300469483, "no_speech_prob": 4.157327111897757e-06}, {"id": 272, "seek": 116508, "start": 1182.72, "end": 1188.8799999999999, "text": " And so then if I look at the cat.codes you can see here this list here is the numbers", "tokens": [400, 370, 550, 498, 286, 574, 412, 264, 3857, 13, 66, 4789, 291, 393, 536, 510, 341, 1329, 510, 307, 264, 3547], "temperature": 0.0, "avg_logprob": -0.17296358255239633, "compression_ratio": 1.7934272300469483, "no_speech_prob": 4.157327111897757e-06}, {"id": 273, "seek": 116508, "start": 1188.8799999999999, "end": 1192.48, "text": " that are actually stored, minus one, minus one, one, minus one, one.", "tokens": [300, 366, 767, 12187, 11, 3175, 472, 11, 3175, 472, 11, 472, 11, 3175, 472, 11, 472, 13], "temperature": 0.0, "avg_logprob": -0.17296358255239633, "compression_ratio": 1.7934272300469483, "no_speech_prob": 4.157327111897757e-06}, {"id": 274, "seek": 119248, "start": 1192.48, "end": 1196.16, "text": " What are these minus ones?", "tokens": [708, 366, 613, 3175, 2306, 30], "temperature": 0.0, "avg_logprob": -0.1726608844030471, "compression_ratio": 1.7362637362637363, "no_speech_prob": 2.812982529576402e-06}, {"id": 275, "seek": 119248, "start": 1196.16, "end": 1203.1200000000001, "text": " The minus ones represent NIN, they represent missing, so pandas uses the special code minus", "tokens": [440, 3175, 2306, 2906, 426, 1464, 11, 436, 2906, 5361, 11, 370, 4565, 296, 4960, 264, 2121, 3089, 3175], "temperature": 0.0, "avg_logprob": -0.1726608844030471, "compression_ratio": 1.7362637362637363, "no_speech_prob": 2.812982529576402e-06}, {"id": 276, "seek": 119248, "start": 1203.1200000000001, "end": 1204.64, "text": " one to mean missing.", "tokens": [472, 281, 914, 5361, 13], "temperature": 0.0, "avg_logprob": -0.1726608844030471, "compression_ratio": 1.7362637362637363, "no_speech_prob": 2.812982529576402e-06}, {"id": 277, "seek": 119248, "start": 1204.64, "end": 1210.38, "text": " Now as you know these are going to end up in an embedding matrix and we can't look up", "tokens": [823, 382, 291, 458, 613, 366, 516, 281, 917, 493, 294, 364, 12240, 3584, 8141, 293, 321, 393, 380, 574, 493], "temperature": 0.0, "avg_logprob": -0.1726608844030471, "compression_ratio": 1.7362637362637363, "no_speech_prob": 2.812982529576402e-06}, {"id": 278, "seek": 119248, "start": 1210.38, "end": 1218.96, "text": " item minus one in an embedding matrix so internally in fast AI we add one to all of these.", "tokens": [3174, 3175, 472, 294, 364, 12240, 3584, 8141, 370, 19501, 294, 2370, 7318, 321, 909, 472, 281, 439, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.1726608844030471, "compression_ratio": 1.7362637362637363, "no_speech_prob": 2.812982529576402e-06}, {"id": 279, "seek": 121896, "start": 1218.96, "end": 1224.48, "text": " Another useful preprocessor is fixMissing and so again you can call it on the data frame,", "tokens": [3996, 4420, 2666, 340, 25432, 307, 3191, 47205, 278, 293, 370, 797, 291, 393, 818, 309, 322, 264, 1412, 3920, 11], "temperature": 0.0, "avg_logprob": -0.14189999373917728, "compression_ratio": 1.811659192825112, "no_speech_prob": 9.080359632207546e-06}, {"id": 280, "seek": 121896, "start": 1224.48, "end": 1230.64, "text": " you can call it on the test, passing in test equals true, and this will create for everything", "tokens": [291, 393, 818, 309, 322, 264, 1500, 11, 8437, 294, 1500, 6915, 2074, 11, 293, 341, 486, 1884, 337, 1203], "temperature": 0.0, "avg_logprob": -0.14189999373917728, "compression_ratio": 1.811659192825112, "no_speech_prob": 9.080359632207546e-06}, {"id": 281, "seek": 121896, "start": 1230.64, "end": 1234.8, "text": " that's missing, anything that has a missing value, it will create an additional column", "tokens": [300, 311, 5361, 11, 1340, 300, 575, 257, 5361, 2158, 11, 309, 486, 1884, 364, 4497, 7738], "temperature": 0.0, "avg_logprob": -0.14189999373917728, "compression_ratio": 1.811659192825112, "no_speech_prob": 9.080359632207546e-06}, {"id": 282, "seek": 121896, "start": 1234.8, "end": 1239.92, "text": " with the column name underscore NA, so competition distance underscore NA, and it will set it", "tokens": [365, 264, 7738, 1315, 37556, 16585, 11, 370, 6211, 4560, 37556, 16585, 11, 293, 309, 486, 992, 309], "temperature": 0.0, "avg_logprob": -0.14189999373917728, "compression_ratio": 1.811659192825112, "no_speech_prob": 9.080359632207546e-06}, {"id": 283, "seek": 121896, "start": 1239.92, "end": 1244.3600000000001, "text": " for true for any time that was missing.", "tokens": [337, 2074, 337, 604, 565, 300, 390, 5361, 13], "temperature": 0.0, "avg_logprob": -0.14189999373917728, "compression_ratio": 1.811659192825112, "no_speech_prob": 9.080359632207546e-06}, {"id": 284, "seek": 124436, "start": 1244.36, "end": 1250.58, "text": " And then what we do is we replace competition distance with the median for those.", "tokens": [400, 550, 437, 321, 360, 307, 321, 7406, 6211, 4560, 365, 264, 26779, 337, 729, 13], "temperature": 0.0, "avg_logprob": -0.11608811642261262, "compression_ratio": 1.6639344262295082, "no_speech_prob": 5.507572495844215e-06}, {"id": 285, "seek": 124436, "start": 1250.58, "end": 1251.58, "text": " Why do we do this?", "tokens": [1545, 360, 321, 360, 341, 30], "temperature": 0.0, "avg_logprob": -0.11608811642261262, "compression_ratio": 1.6639344262295082, "no_speech_prob": 5.507572495844215e-06}, {"id": 286, "seek": 124436, "start": 1251.58, "end": 1257.8, "text": " Well because very commonly the fact that something's missing is of itself interesting.", "tokens": [1042, 570, 588, 12719, 264, 1186, 300, 746, 311, 5361, 307, 295, 2564, 1880, 13], "temperature": 0.0, "avg_logprob": -0.11608811642261262, "compression_ratio": 1.6639344262295082, "no_speech_prob": 5.507572495844215e-06}, {"id": 287, "seek": 124436, "start": 1257.8, "end": 1263.12, "text": " Like it turns out the fact that this is missing helps you predict your outcome.", "tokens": [1743, 309, 4523, 484, 264, 1186, 300, 341, 307, 5361, 3665, 291, 6069, 428, 9700, 13], "temperature": 0.0, "avg_logprob": -0.11608811642261262, "compression_ratio": 1.6639344262295082, "no_speech_prob": 5.507572495844215e-06}, {"id": 288, "seek": 124436, "start": 1263.12, "end": 1267.1599999999999, "text": " So we certainly want to keep that information in a convenient Boolean column so that our", "tokens": [407, 321, 3297, 528, 281, 1066, 300, 1589, 294, 257, 10851, 23351, 28499, 7738, 370, 300, 527], "temperature": 0.0, "avg_logprob": -0.11608811642261262, "compression_ratio": 1.6639344262295082, "no_speech_prob": 5.507572495844215e-06}, {"id": 289, "seek": 124436, "start": 1267.1599999999999, "end": 1270.32, "text": " deep learning model can use it to predict things.", "tokens": [2452, 2539, 2316, 393, 764, 309, 281, 6069, 721, 13], "temperature": 0.0, "avg_logprob": -0.11608811642261262, "compression_ratio": 1.6639344262295082, "no_speech_prob": 5.507572495844215e-06}, {"id": 290, "seek": 127032, "start": 1270.32, "end": 1274.56, "text": " But then we need competition distance to be a continuous variable so we can use it in", "tokens": [583, 550, 321, 643, 6211, 4560, 281, 312, 257, 10957, 7006, 370, 321, 393, 764, 309, 294], "temperature": 0.0, "avg_logprob": -0.08821444936317972, "compression_ratio": 1.782426778242678, "no_speech_prob": 3.7266247545630904e-06}, {"id": 291, "seek": 127032, "start": 1274.56, "end": 1276.4399999999998, "text": " the continuous variable part of our model.", "tokens": [264, 10957, 7006, 644, 295, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.08821444936317972, "compression_ratio": 1.782426778242678, "no_speech_prob": 3.7266247545630904e-06}, {"id": 292, "seek": 127032, "start": 1276.4399999999998, "end": 1281.6399999999999, "text": " So we can replace it with almost any number, right, because if it turns out that the missingness", "tokens": [407, 321, 393, 7406, 309, 365, 1920, 604, 1230, 11, 558, 11, 570, 498, 309, 4523, 484, 300, 264, 5361, 1287], "temperature": 0.0, "avg_logprob": -0.08821444936317972, "compression_ratio": 1.782426778242678, "no_speech_prob": 3.7266247545630904e-06}, {"id": 293, "seek": 127032, "start": 1281.6399999999999, "end": 1287.08, "text": " is important it can use the interaction of competition distance NA and competition distance", "tokens": [307, 1021, 309, 393, 764, 264, 9285, 295, 6211, 4560, 16585, 293, 6211, 4560], "temperature": 0.0, "avg_logprob": -0.08821444936317972, "compression_ratio": 1.782426778242678, "no_speech_prob": 3.7266247545630904e-06}, {"id": 294, "seek": 127032, "start": 1287.08, "end": 1288.72, "text": " to make predictions.", "tokens": [281, 652, 21264, 13], "temperature": 0.0, "avg_logprob": -0.08821444936317972, "compression_ratio": 1.782426778242678, "no_speech_prob": 3.7266247545630904e-06}, {"id": 295, "seek": 127032, "start": 1288.72, "end": 1291.76, "text": " So that's what fixMissing does.", "tokens": [407, 300, 311, 437, 3191, 47205, 278, 775, 13], "temperature": 0.0, "avg_logprob": -0.08821444936317972, "compression_ratio": 1.782426778242678, "no_speech_prob": 3.7266247545630904e-06}, {"id": 296, "seek": 127032, "start": 1291.76, "end": 1295.36, "text": " You don't have to manually call preprocessors yourself.", "tokens": [509, 500, 380, 362, 281, 16945, 818, 2666, 340, 45700, 1803, 13], "temperature": 0.0, "avg_logprob": -0.08821444936317972, "compression_ratio": 1.782426778242678, "no_speech_prob": 3.7266247545630904e-06}, {"id": 297, "seek": 129536, "start": 1295.36, "end": 1307.3799999999999, "text": " When you call any kind of item list creator you can pass in a list of preprocessors which", "tokens": [1133, 291, 818, 604, 733, 295, 3174, 1329, 14181, 291, 393, 1320, 294, 257, 1329, 295, 2666, 340, 45700, 597], "temperature": 0.0, "avg_logprob": -0.14559195457248514, "compression_ratio": 1.6733870967741935, "no_speech_prob": 2.406091425655177e-06}, {"id": 298, "seek": 129536, "start": 1307.3799999999999, "end": 1310.36, "text": " you can create like this.", "tokens": [291, 393, 1884, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.14559195457248514, "compression_ratio": 1.6733870967741935, "no_speech_prob": 2.406091425655177e-06}, {"id": 299, "seek": 129536, "start": 1310.36, "end": 1315.02, "text": " So this is saying, okay, I want to feel missing, I want to categorify, I want to normalize,", "tokens": [407, 341, 307, 1566, 11, 1392, 11, 286, 528, 281, 841, 5361, 11, 286, 528, 281, 19250, 2505, 11, 286, 528, 281, 2710, 1125, 11], "temperature": 0.0, "avg_logprob": -0.14559195457248514, "compression_ratio": 1.6733870967741935, "no_speech_prob": 2.406091425655177e-06}, {"id": 300, "seek": 129536, "start": 1315.02, "end": 1319.04, "text": " so for continuous variables it'll subtract the mean and divide by the standard deviation", "tokens": [370, 337, 10957, 9102, 309, 603, 16390, 264, 914, 293, 9845, 538, 264, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.14559195457248514, "compression_ratio": 1.6733870967741935, "no_speech_prob": 2.406091425655177e-06}, {"id": 301, "seek": 129536, "start": 1319.04, "end": 1320.7199999999998, "text": " to help it train more easily.", "tokens": [281, 854, 309, 3847, 544, 3612, 13], "temperature": 0.0, "avg_logprob": -0.14559195457248514, "compression_ratio": 1.6733870967741935, "no_speech_prob": 2.406091425655177e-06}, {"id": 302, "seek": 129536, "start": 1320.7199999999998, "end": 1324.8799999999999, "text": " And so you just say those are my procs and then you can just pass it in there and that's", "tokens": [400, 370, 291, 445, 584, 729, 366, 452, 9510, 82, 293, 550, 291, 393, 445, 1320, 309, 294, 456, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.14559195457248514, "compression_ratio": 1.6733870967741935, "no_speech_prob": 2.406091425655177e-06}, {"id": 303, "seek": 132488, "start": 1324.88, "end": 1325.88, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.10906865861680773, "compression_ratio": 1.7300884955752212, "no_speech_prob": 3.4465469980204944e-06}, {"id": 304, "seek": 132488, "start": 1325.88, "end": 1331.16, "text": " And later on you can go data.export and it'll save all the metadata for that data bunch", "tokens": [400, 1780, 322, 291, 393, 352, 1412, 13, 3121, 2707, 293, 309, 603, 3155, 439, 264, 26603, 337, 300, 1412, 3840], "temperature": 0.0, "avg_logprob": -0.10906865861680773, "compression_ratio": 1.7300884955752212, "no_speech_prob": 3.4465469980204944e-06}, {"id": 305, "seek": 132488, "start": 1331.16, "end": 1336.6000000000001, "text": " so you can later on load it in knowing exactly what your category codes are, exactly what", "tokens": [370, 291, 393, 1780, 322, 3677, 309, 294, 5276, 2293, 437, 428, 7719, 14211, 366, 11, 2293, 437], "temperature": 0.0, "avg_logprob": -0.10906865861680773, "compression_ratio": 1.7300884955752212, "no_speech_prob": 3.4465469980204944e-06}, {"id": 306, "seek": 132488, "start": 1336.6000000000001, "end": 1340.8000000000002, "text": " median values you use for replacing the missing values, and exactly what means and standard", "tokens": [26779, 4190, 291, 764, 337, 19139, 264, 5361, 4190, 11, 293, 2293, 437, 1355, 293, 3832], "temperature": 0.0, "avg_logprob": -0.10906865861680773, "compression_ratio": 1.7300884955752212, "no_speech_prob": 3.4465469980204944e-06}, {"id": 307, "seek": 132488, "start": 1340.8000000000002, "end": 1344.8000000000002, "text": " deviations you're normalized by.", "tokens": [31219, 763, 291, 434, 48704, 538, 13], "temperature": 0.0, "avg_logprob": -0.10906865861680773, "compression_ratio": 1.7300884955752212, "no_speech_prob": 3.4465469980204944e-06}, {"id": 308, "seek": 132488, "start": 1344.8000000000002, "end": 1350.1200000000001, "text": " Okay, so the main thing you have to do if you want to create a data bunch of tabular", "tokens": [1033, 11, 370, 264, 2135, 551, 291, 362, 281, 360, 498, 291, 528, 281, 1884, 257, 1412, 3840, 295, 4421, 1040], "temperature": 0.0, "avg_logprob": -0.10906865861680773, "compression_ratio": 1.7300884955752212, "no_speech_prob": 3.4465469980204944e-06}, {"id": 309, "seek": 135012, "start": 1350.12, "end": 1355.9199999999998, "text": " data is find out or tell it what are your categorical variables and what are your continuous", "tokens": [1412, 307, 915, 484, 420, 980, 309, 437, 366, 428, 19250, 804, 9102, 293, 437, 366, 428, 10957], "temperature": 0.0, "avg_logprob": -0.14099447117295377, "compression_ratio": 1.8316831683168318, "no_speech_prob": 3.1381275675812503e-06}, {"id": 310, "seek": 135012, "start": 1355.9199999999998, "end": 1357.9199999999998, "text": " variables.", "tokens": [9102, 13], "temperature": 0.0, "avg_logprob": -0.14099447117295377, "compression_ratio": 1.8316831683168318, "no_speech_prob": 3.1381275675812503e-06}, {"id": 311, "seek": 135012, "start": 1357.9199999999998, "end": 1364.1999999999998, "text": " And as we discussed last week briefly, your categorical variables are not just strings", "tokens": [400, 382, 321, 7152, 1036, 1243, 10515, 11, 428, 19250, 804, 9102, 366, 406, 445, 13985], "temperature": 0.0, "avg_logprob": -0.14099447117295377, "compression_ratio": 1.8316831683168318, "no_speech_prob": 3.1381275675812503e-06}, {"id": 312, "seek": 135012, "start": 1364.1999999999998, "end": 1371.1999999999998, "text": " and things but also I include things like day of week and month and day of month, even", "tokens": [293, 721, 457, 611, 286, 4090, 721, 411, 786, 295, 1243, 293, 1618, 293, 786, 295, 1618, 11, 754], "temperature": 0.0, "avg_logprob": -0.14099447117295377, "compression_ratio": 1.8316831683168318, "no_speech_prob": 3.1381275675812503e-06}, {"id": 313, "seek": 135012, "start": 1371.1999999999998, "end": 1376.7199999999998, "text": " though they're numbers, I make them categorical variables because for example, day of month,", "tokens": [1673, 436, 434, 3547, 11, 286, 652, 552, 19250, 804, 9102, 570, 337, 1365, 11, 786, 295, 1618, 11], "temperature": 0.0, "avg_logprob": -0.14099447117295377, "compression_ratio": 1.8316831683168318, "no_speech_prob": 3.1381275675812503e-06}, {"id": 314, "seek": 137672, "start": 1376.72, "end": 1380.28, "text": " I don't think it's going to have a nice smooth curve.", "tokens": [286, 500, 380, 519, 309, 311, 516, 281, 362, 257, 1481, 5508, 7605, 13], "temperature": 0.0, "avg_logprob": -0.0887299562111879, "compression_ratio": 1.9598393574297188, "no_speech_prob": 1.0783182005980052e-05}, {"id": 315, "seek": 137672, "start": 1380.28, "end": 1385.1200000000001, "text": " I think that the 15th of the month and the 1st of the month and the 30th of the month", "tokens": [286, 519, 300, 264, 2119, 392, 295, 264, 1618, 293, 264, 502, 372, 295, 264, 1618, 293, 264, 2217, 392, 295, 264, 1618], "temperature": 0.0, "avg_logprob": -0.0887299562111879, "compression_ratio": 1.9598393574297188, "no_speech_prob": 1.0783182005980052e-05}, {"id": 316, "seek": 137672, "start": 1385.1200000000001, "end": 1390.28, "text": " are probably going to have different purchasing behavior to other days of the month.", "tokens": [366, 1391, 516, 281, 362, 819, 20906, 5223, 281, 661, 1708, 295, 264, 1618, 13], "temperature": 0.0, "avg_logprob": -0.0887299562111879, "compression_ratio": 1.9598393574297188, "no_speech_prob": 1.0783182005980052e-05}, {"id": 317, "seek": 137672, "start": 1390.28, "end": 1394.24, "text": " And so therefore if I make it a categorical variable, it's going to end up creating an", "tokens": [400, 370, 4412, 498, 286, 652, 309, 257, 19250, 804, 7006, 11, 309, 311, 516, 281, 917, 493, 4084, 364], "temperature": 0.0, "avg_logprob": -0.0887299562111879, "compression_ratio": 1.9598393574297188, "no_speech_prob": 1.0783182005980052e-05}, {"id": 318, "seek": 137672, "start": 1394.24, "end": 1399.6000000000001, "text": " embedding matrix and those different days of the month can get different behaviors.", "tokens": [12240, 3584, 8141, 293, 729, 819, 1708, 295, 264, 1618, 393, 483, 819, 15501, 13], "temperature": 0.0, "avg_logprob": -0.0887299562111879, "compression_ratio": 1.9598393574297188, "no_speech_prob": 1.0783182005980052e-05}, {"id": 319, "seek": 137672, "start": 1399.6000000000001, "end": 1403.96, "text": " So you've actually got to think carefully about which things should be categorical variables", "tokens": [407, 291, 600, 767, 658, 281, 519, 7500, 466, 597, 721, 820, 312, 19250, 804, 9102], "temperature": 0.0, "avg_logprob": -0.0887299562111879, "compression_ratio": 1.9598393574297188, "no_speech_prob": 1.0783182005980052e-05}, {"id": 320, "seek": 140396, "start": 1403.96, "end": 1409.08, "text": " and on the whole, if in doubt and there are not too many levels in your category, that's", "tokens": [293, 322, 264, 1379, 11, 498, 294, 6385, 293, 456, 366, 406, 886, 867, 4358, 294, 428, 7719, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.12398030207707332, "compression_ratio": 1.8860294117647058, "no_speech_prob": 1.0783145626191981e-05}, {"id": 321, "seek": 140396, "start": 1409.08, "end": 1410.56, "text": " called the cardinality.", "tokens": [1219, 264, 2920, 259, 1860, 13], "temperature": 0.0, "avg_logprob": -0.12398030207707332, "compression_ratio": 1.8860294117647058, "no_speech_prob": 1.0783145626191981e-05}, {"id": 322, "seek": 140396, "start": 1410.56, "end": 1415.3600000000001, "text": " If your cardinality is not too high, I would put it as a categorical variable.", "tokens": [759, 428, 2920, 259, 1860, 307, 406, 886, 1090, 11, 286, 576, 829, 309, 382, 257, 19250, 804, 7006, 13], "temperature": 0.0, "avg_logprob": -0.12398030207707332, "compression_ratio": 1.8860294117647058, "no_speech_prob": 1.0783145626191981e-05}, {"id": 323, "seek": 140396, "start": 1415.3600000000001, "end": 1419.1000000000001, "text": " You can always try and see which works best.", "tokens": [509, 393, 1009, 853, 293, 536, 597, 1985, 1151, 13], "temperature": 0.0, "avg_logprob": -0.12398030207707332, "compression_ratio": 1.8860294117647058, "no_speech_prob": 1.0783145626191981e-05}, {"id": 324, "seek": 140396, "start": 1419.1000000000001, "end": 1423.44, "text": " So our final data frame that we're going to pass in is going to be our training set with", "tokens": [407, 527, 2572, 1412, 3920, 300, 321, 434, 516, 281, 1320, 294, 307, 516, 281, 312, 527, 3097, 992, 365], "temperature": 0.0, "avg_logprob": -0.12398030207707332, "compression_ratio": 1.8860294117647058, "no_speech_prob": 1.0783145626191981e-05}, {"id": 325, "seek": 140396, "start": 1423.44, "end": 1427.6000000000001, "text": " the categorical variables and the continuous variables and the dependent variable and the", "tokens": [264, 19250, 804, 9102, 293, 264, 10957, 9102, 293, 264, 12334, 7006, 293, 264], "temperature": 0.0, "avg_logprob": -0.12398030207707332, "compression_ratio": 1.8860294117647058, "no_speech_prob": 1.0783145626191981e-05}, {"id": 326, "seek": 140396, "start": 1427.6000000000001, "end": 1429.3600000000001, "text": " date.", "tokens": [4002, 13], "temperature": 0.0, "avg_logprob": -0.12398030207707332, "compression_ratio": 1.8860294117647058, "no_speech_prob": 1.0783145626191981e-05}, {"id": 327, "seek": 140396, "start": 1429.3600000000001, "end": 1433.24, "text": " And the date we're just going to use to create a validation set where we're basically going", "tokens": [400, 264, 4002, 321, 434, 445, 516, 281, 764, 281, 1884, 257, 24071, 992, 689, 321, 434, 1936, 516], "temperature": 0.0, "avg_logprob": -0.12398030207707332, "compression_ratio": 1.8860294117647058, "no_speech_prob": 1.0783145626191981e-05}, {"id": 328, "seek": 143324, "start": 1433.24, "end": 1438.7, "text": " to say the validation set is going to be the same number of records at the end of the time", "tokens": [281, 584, 264, 24071, 992, 307, 516, 281, 312, 264, 912, 1230, 295, 7724, 412, 264, 917, 295, 264, 565], "temperature": 0.0, "avg_logprob": -0.13331321277449618, "compression_ratio": 1.6297709923664123, "no_speech_prob": 4.56590441899607e-06}, {"id": 329, "seek": 143324, "start": 1438.7, "end": 1441.4, "text": " period that the test set is for Kaggle.", "tokens": [2896, 300, 264, 1500, 992, 307, 337, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.13331321277449618, "compression_ratio": 1.6297709923664123, "no_speech_prob": 4.56590441899607e-06}, {"id": 330, "seek": 143324, "start": 1441.4, "end": 1445.6, "text": " And so that way we should be able to validate our model nicely.", "tokens": [400, 370, 300, 636, 321, 820, 312, 1075, 281, 29562, 527, 2316, 9594, 13], "temperature": 0.0, "avg_logprob": -0.13331321277449618, "compression_ratio": 1.6297709923664123, "no_speech_prob": 4.56590441899607e-06}, {"id": 331, "seek": 143324, "start": 1445.6, "end": 1449.8, "text": " Okay, so now we can create a tabular list.", "tokens": [1033, 11, 370, 586, 321, 393, 1884, 257, 4421, 1040, 1329, 13], "temperature": 0.0, "avg_logprob": -0.13331321277449618, "compression_ratio": 1.6297709923664123, "no_speech_prob": 4.56590441899607e-06}, {"id": 332, "seek": 143324, "start": 1449.8, "end": 1454.36, "text": " So this is our standard data block API that you've seen a few times from a data frame.", "tokens": [407, 341, 307, 527, 3832, 1412, 3461, 9362, 300, 291, 600, 1612, 257, 1326, 1413, 490, 257, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.13331321277449618, "compression_ratio": 1.6297709923664123, "no_speech_prob": 4.56590441899607e-06}, {"id": 333, "seek": 143324, "start": 1454.36, "end": 1459.88, "text": " Pass in all of that information, split it into valid versus train, label it with a dependent", "tokens": [10319, 294, 439, 295, 300, 1589, 11, 7472, 309, 666, 7363, 5717, 3847, 11, 7645, 309, 365, 257, 12334], "temperature": 0.0, "avg_logprob": -0.13331321277449618, "compression_ratio": 1.6297709923664123, "no_speech_prob": 4.56590441899607e-06}, {"id": 334, "seek": 143324, "start": 1459.88, "end": 1462.2, "text": " variable.", "tokens": [7006, 13], "temperature": 0.0, "avg_logprob": -0.13331321277449618, "compression_ratio": 1.6297709923664123, "no_speech_prob": 4.56590441899607e-06}, {"id": 335, "seek": 146220, "start": 1462.2, "end": 1467.7, "text": " And here's something I don't think you've seen before, label class.", "tokens": [400, 510, 311, 746, 286, 500, 380, 519, 291, 600, 1612, 949, 11, 7645, 1508, 13], "temperature": 0.0, "avg_logprob": -0.13908731937408447, "compression_ratio": 1.6502242152466369, "no_speech_prob": 1.0289439160260372e-05}, {"id": 336, "seek": 146220, "start": 1467.7, "end": 1470.04, "text": " This is our dependent variable.", "tokens": [639, 307, 527, 12334, 7006, 13], "temperature": 0.0, "avg_logprob": -0.13908731937408447, "compression_ratio": 1.6502242152466369, "no_speech_prob": 1.0289439160260372e-05}, {"id": 337, "seek": 146220, "start": 1470.04, "end": 1472.56, "text": " And as you can see, this is sales.", "tokens": [400, 382, 291, 393, 536, 11, 341, 307, 5763, 13], "temperature": 0.0, "avg_logprob": -0.13908731937408447, "compression_ratio": 1.6502242152466369, "no_speech_prob": 1.0289439160260372e-05}, {"id": 338, "seek": 146220, "start": 1472.56, "end": 1474.48, "text": " It's not a float.", "tokens": [467, 311, 406, 257, 15706, 13], "temperature": 0.0, "avg_logprob": -0.13908731937408447, "compression_ratio": 1.6502242152466369, "no_speech_prob": 1.0289439160260372e-05}, {"id": 339, "seek": 146220, "start": 1474.48, "end": 1476.28, "text": " It's an int64.", "tokens": [467, 311, 364, 560, 19395, 13], "temperature": 0.0, "avg_logprob": -0.13908731937408447, "compression_ratio": 1.6502242152466369, "no_speech_prob": 1.0289439160260372e-05}, {"id": 340, "seek": 146220, "start": 1476.28, "end": 1483.28, "text": " If this was a float, then FastAI would automatically know or guess that you want to do a regression.", "tokens": [759, 341, 390, 257, 15706, 11, 550, 15968, 48698, 576, 6772, 458, 420, 2041, 300, 291, 528, 281, 360, 257, 24590, 13], "temperature": 0.0, "avg_logprob": -0.13908731937408447, "compression_ratio": 1.6502242152466369, "no_speech_prob": 1.0289439160260372e-05}, {"id": 341, "seek": 146220, "start": 1483.28, "end": 1484.7, "text": " But this is not a float.", "tokens": [583, 341, 307, 406, 257, 15706, 13], "temperature": 0.0, "avg_logprob": -0.13908731937408447, "compression_ratio": 1.6502242152466369, "no_speech_prob": 1.0289439160260372e-05}, {"id": 342, "seek": 146220, "start": 1484.7, "end": 1485.7, "text": " It's an int.", "tokens": [467, 311, 364, 560, 13], "temperature": 0.0, "avg_logprob": -0.13908731937408447, "compression_ratio": 1.6502242152466369, "no_speech_prob": 1.0289439160260372e-05}, {"id": 343, "seek": 146220, "start": 1485.7, "end": 1488.54, "text": " So FastAI is going to assume you want to do a classification.", "tokens": [407, 15968, 48698, 307, 516, 281, 6552, 291, 528, 281, 360, 257, 21538, 13], "temperature": 0.0, "avg_logprob": -0.13908731937408447, "compression_ratio": 1.6502242152466369, "no_speech_prob": 1.0289439160260372e-05}, {"id": 344, "seek": 148854, "start": 1488.54, "end": 1493.82, "text": " So when we label it, we have to tell it that the class of the labels we want is a list", "tokens": [407, 562, 321, 7645, 309, 11, 321, 362, 281, 980, 309, 300, 264, 1508, 295, 264, 16949, 321, 528, 307, 257, 1329], "temperature": 0.0, "avg_logprob": -0.0787998614924969, "compression_ratio": 1.6752136752136753, "no_speech_prob": 2.0261302324797725e-06}, {"id": 345, "seek": 148854, "start": 1493.82, "end": 1499.0, "text": " of floats, not a list of categories, which would otherwise be the default.", "tokens": [295, 37878, 11, 406, 257, 1329, 295, 10479, 11, 597, 576, 5911, 312, 264, 7576, 13], "temperature": 0.0, "avg_logprob": -0.0787998614924969, "compression_ratio": 1.6752136752136753, "no_speech_prob": 2.0261302324797725e-06}, {"id": 346, "seek": 148854, "start": 1499.0, "end": 1504.84, "text": " So this is the thing that's going to automatically turn this into a regression problem for us.", "tokens": [407, 341, 307, 264, 551, 300, 311, 516, 281, 6772, 1261, 341, 666, 257, 24590, 1154, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.0787998614924969, "compression_ratio": 1.6752136752136753, "no_speech_prob": 2.0261302324797725e-06}, {"id": 347, "seek": 148854, "start": 1504.84, "end": 1510.1399999999999, "text": " And then we create a data bunch.", "tokens": [400, 550, 321, 1884, 257, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.0787998614924969, "compression_ratio": 1.6752136752136753, "no_speech_prob": 2.0261302324797725e-06}, {"id": 348, "seek": 148854, "start": 1510.1399999999999, "end": 1515.1599999999999, "text": " So I wanted to remind you again about doc, which is how we find out more information", "tokens": [407, 286, 1415, 281, 4160, 291, 797, 466, 3211, 11, 597, 307, 577, 321, 915, 484, 544, 1589], "temperature": 0.0, "avg_logprob": -0.0787998614924969, "compression_ratio": 1.6752136752136753, "no_speech_prob": 2.0261302324797725e-06}, {"id": 349, "seek": 148854, "start": 1515.1599999999999, "end": 1516.1599999999999, "text": " about this stuff.", "tokens": [466, 341, 1507, 13], "temperature": 0.0, "avg_logprob": -0.0787998614924969, "compression_ratio": 1.6752136752136753, "no_speech_prob": 2.0261302324797725e-06}, {"id": 350, "seek": 151616, "start": 1516.16, "end": 1521.52, "text": " In this case, all of the labeling functions in the data blocks API will pass on any keywords", "tokens": [682, 341, 1389, 11, 439, 295, 264, 40244, 6828, 294, 264, 1412, 8474, 9362, 486, 1320, 322, 604, 21009], "temperature": 0.0, "avg_logprob": -0.08360448750582608, "compression_ratio": 1.6819787985865724, "no_speech_prob": 2.4824682895996375e-06}, {"id": 351, "seek": 151616, "start": 1521.52, "end": 1524.76, "text": " they don't recognize to the label class.", "tokens": [436, 500, 380, 5521, 281, 264, 7645, 1508, 13], "temperature": 0.0, "avg_logprob": -0.08360448750582608, "compression_ratio": 1.6819787985865724, "no_speech_prob": 2.4824682895996375e-06}, {"id": 352, "seek": 151616, "start": 1524.76, "end": 1527.3600000000001, "text": " So one of the things I've passed in here is log.", "tokens": [407, 472, 295, 264, 721, 286, 600, 4678, 294, 510, 307, 3565, 13], "temperature": 0.0, "avg_logprob": -0.08360448750582608, "compression_ratio": 1.6819787985865724, "no_speech_prob": 2.4824682895996375e-06}, {"id": 353, "seek": 151616, "start": 1527.3600000000001, "end": 1530.3200000000002, "text": " And so that's actually going to end up in float list.", "tokens": [400, 370, 300, 311, 767, 516, 281, 917, 493, 294, 15706, 1329, 13], "temperature": 0.0, "avg_logprob": -0.08360448750582608, "compression_ratio": 1.6819787985865724, "no_speech_prob": 2.4824682895996375e-06}, {"id": 354, "seek": 151616, "start": 1530.3200000000002, "end": 1533.76, "text": " And so if I go doc float list, I can see a summary.", "tokens": [400, 370, 498, 286, 352, 3211, 15706, 1329, 11, 286, 393, 536, 257, 12691, 13], "temperature": 0.0, "avg_logprob": -0.08360448750582608, "compression_ratio": 1.6819787985865724, "no_speech_prob": 2.4824682895996375e-06}, {"id": 355, "seek": 151616, "start": 1533.76, "end": 1536.2, "text": " And I can even jump into the full documentation.", "tokens": [400, 286, 393, 754, 3012, 666, 264, 1577, 14333, 13], "temperature": 0.0, "avg_logprob": -0.08360448750582608, "compression_ratio": 1.6819787985865724, "no_speech_prob": 2.4824682895996375e-06}, {"id": 356, "seek": 151616, "start": 1536.2, "end": 1542.48, "text": " And it shows me here that log is something which, if true, it's going to take the logarithm", "tokens": [400, 309, 3110, 385, 510, 300, 3565, 307, 746, 597, 11, 498, 2074, 11, 309, 311, 516, 281, 747, 264, 41473, 32674], "temperature": 0.0, "avg_logprob": -0.08360448750582608, "compression_ratio": 1.6819787985865724, "no_speech_prob": 2.4824682895996375e-06}, {"id": 357, "seek": 151616, "start": 1542.48, "end": 1544.88, "text": " of my dependent variable.", "tokens": [295, 452, 12334, 7006, 13], "temperature": 0.0, "avg_logprob": -0.08360448750582608, "compression_ratio": 1.6819787985865724, "no_speech_prob": 2.4824682895996375e-06}, {"id": 358, "seek": 151616, "start": 1544.88, "end": 1545.88, "text": " Why am I doing that?", "tokens": [1545, 669, 286, 884, 300, 30], "temperature": 0.0, "avg_logprob": -0.08360448750582608, "compression_ratio": 1.6819787985865724, "no_speech_prob": 2.4824682895996375e-06}, {"id": 359, "seek": 154588, "start": 1545.88, "end": 1549.64, "text": " Because this is the thing that's actually going to automatically take the log of my", "tokens": [1436, 341, 307, 264, 551, 300, 311, 767, 516, 281, 6772, 747, 264, 3565, 295, 452], "temperature": 0.0, "avg_logprob": -0.16082419562585576, "compression_ratio": 1.7027027027027026, "no_speech_prob": 3.5559673960960936e-06}, {"id": 360, "seek": 154588, "start": 1549.64, "end": 1550.8400000000001, "text": " way.", "tokens": [636, 13], "temperature": 0.0, "avg_logprob": -0.16082419562585576, "compression_ratio": 1.7027027027027026, "no_speech_prob": 3.5559673960960936e-06}, {"id": 361, "seek": 154588, "start": 1550.8400000000001, "end": 1558.16, "text": " The reason I'm doing that is because, as I mentioned before, the evaluation metric is", "tokens": [440, 1778, 286, 478, 884, 300, 307, 570, 11, 382, 286, 2835, 949, 11, 264, 13344, 20678, 307], "temperature": 0.0, "avg_logprob": -0.16082419562585576, "compression_ratio": 1.7027027027027026, "no_speech_prob": 3.5559673960960936e-06}, {"id": 362, "seek": 154588, "start": 1558.16, "end": 1561.24, "text": " root mean squared percentage error.", "tokens": [5593, 914, 8889, 9668, 6713, 13], "temperature": 0.0, "avg_logprob": -0.16082419562585576, "compression_ratio": 1.7027027027027026, "no_speech_prob": 3.5559673960960936e-06}, {"id": 363, "seek": 154588, "start": 1561.24, "end": 1568.7600000000002, "text": " And fastai, either fastai nor PyTorch has a root mean squared percentage error loss", "tokens": [400, 2370, 1301, 11, 2139, 2370, 1301, 6051, 9953, 51, 284, 339, 575, 257, 5593, 914, 8889, 9668, 6713, 4470], "temperature": 0.0, "avg_logprob": -0.16082419562585576, "compression_ratio": 1.7027027027027026, "no_speech_prob": 3.5559673960960936e-06}, {"id": 364, "seek": 154588, "start": 1568.7600000000002, "end": 1570.4, "text": " function built in.", "tokens": [2445, 3094, 294, 13], "temperature": 0.0, "avg_logprob": -0.16082419562585576, "compression_ratio": 1.7027027027027026, "no_speech_prob": 3.5559673960960936e-06}, {"id": 365, "seek": 154588, "start": 1570.4, "end": 1573.96, "text": " I don't even know if such a loss function would work super well.", "tokens": [286, 500, 380, 754, 458, 498, 1270, 257, 4470, 2445, 576, 589, 1687, 731, 13], "temperature": 0.0, "avg_logprob": -0.16082419562585576, "compression_ratio": 1.7027027027027026, "no_speech_prob": 3.5559673960960936e-06}, {"id": 366, "seek": 157396, "start": 1573.96, "end": 1579.56, "text": " But if you want to spend the time thinking about it, you'll notice that this ratio, if", "tokens": [583, 498, 291, 528, 281, 3496, 264, 565, 1953, 466, 309, 11, 291, 603, 3449, 300, 341, 8509, 11, 498], "temperature": 0.0, "avg_logprob": -0.08793551371647762, "compression_ratio": 1.876, "no_speech_prob": 6.854246294096811e-06}, {"id": 367, "seek": 157396, "start": 1579.56, "end": 1585.52, "text": " you first take the log of y and y hat, then becomes a difference rather than a ratio.", "tokens": [291, 700, 747, 264, 3565, 295, 288, 293, 288, 2385, 11, 550, 3643, 257, 2649, 2831, 813, 257, 8509, 13], "temperature": 0.0, "avg_logprob": -0.08793551371647762, "compression_ratio": 1.876, "no_speech_prob": 6.854246294096811e-06}, {"id": 368, "seek": 157396, "start": 1585.52, "end": 1591.52, "text": " So in other words, if you take the log of y, then this becomes root mean squared error.", "tokens": [407, 294, 661, 2283, 11, 498, 291, 747, 264, 3565, 295, 288, 11, 550, 341, 3643, 5593, 914, 8889, 6713, 13], "temperature": 0.0, "avg_logprob": -0.08793551371647762, "compression_ratio": 1.876, "no_speech_prob": 6.854246294096811e-06}, {"id": 369, "seek": 157396, "start": 1591.52, "end": 1592.52, "text": " So that's what we're going to do.", "tokens": [407, 300, 311, 437, 321, 434, 516, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.08793551371647762, "compression_ratio": 1.876, "no_speech_prob": 6.854246294096811e-06}, {"id": 370, "seek": 157396, "start": 1592.52, "end": 1595.04, "text": " We're going to take the log of y.", "tokens": [492, 434, 516, 281, 747, 264, 3565, 295, 288, 13], "temperature": 0.0, "avg_logprob": -0.08793551371647762, "compression_ratio": 1.876, "no_speech_prob": 6.854246294096811e-06}, {"id": 371, "seek": 157396, "start": 1595.04, "end": 1599.58, "text": " And then we're just going to use root mean squared error, which is the default for regression", "tokens": [400, 550, 321, 434, 445, 516, 281, 764, 5593, 914, 8889, 6713, 11, 597, 307, 264, 7576, 337, 24590], "temperature": 0.0, "avg_logprob": -0.08793551371647762, "compression_ratio": 1.876, "no_speech_prob": 6.854246294096811e-06}, {"id": 372, "seek": 157396, "start": 1599.58, "end": 1600.58, "text": " problems.", "tokens": [2740, 13], "temperature": 0.0, "avg_logprob": -0.08793551371647762, "compression_ratio": 1.876, "no_speech_prob": 6.854246294096811e-06}, {"id": 373, "seek": 157396, "start": 1600.58, "end": 1603.02, "text": " So we won't even have to mention it.", "tokens": [407, 321, 1582, 380, 754, 362, 281, 2152, 309, 13], "temperature": 0.0, "avg_logprob": -0.08793551371647762, "compression_ratio": 1.876, "no_speech_prob": 6.854246294096811e-06}, {"id": 374, "seek": 160302, "start": 1603.02, "end": 1607.32, "text": " The reason that we have this here is because this is so common.", "tokens": [440, 1778, 300, 321, 362, 341, 510, 307, 570, 341, 307, 370, 2689, 13], "temperature": 0.0, "avg_logprob": -0.11614131927490234, "compression_ratio": 1.6284584980237153, "no_speech_prob": 5.862767466169316e-06}, {"id": 375, "seek": 160302, "start": 1607.32, "end": 1613.1, "text": " Basically any time you're trying to predict something that's like a population or a dollar", "tokens": [8537, 604, 565, 291, 434, 1382, 281, 6069, 746, 300, 311, 411, 257, 4415, 420, 257, 7241], "temperature": 0.0, "avg_logprob": -0.11614131927490234, "compression_ratio": 1.6284584980237153, "no_speech_prob": 5.862767466169316e-06}, {"id": 376, "seek": 160302, "start": 1613.1, "end": 1618.7, "text": " amount of sales, these kind of things tend to have long-tail distributions where you", "tokens": [2372, 295, 5763, 11, 613, 733, 295, 721, 3928, 281, 362, 938, 12, 14430, 37870, 689, 291], "temperature": 0.0, "avg_logprob": -0.11614131927490234, "compression_ratio": 1.6284584980237153, "no_speech_prob": 5.862767466169316e-06}, {"id": 377, "seek": 160302, "start": 1618.7, "end": 1624.08, "text": " care more about percentage differences than exact differences, absolute differences.", "tokens": [1127, 544, 466, 9668, 7300, 813, 1900, 7300, 11, 8236, 7300, 13], "temperature": 0.0, "avg_logprob": -0.11614131927490234, "compression_ratio": 1.6284584980237153, "no_speech_prob": 5.862767466169316e-06}, {"id": 378, "seek": 160302, "start": 1624.08, "end": 1629.66, "text": " So you're very likely to want to do things with log equals true and to measure the root", "tokens": [407, 291, 434, 588, 3700, 281, 528, 281, 360, 721, 365, 3565, 6915, 2074, 293, 281, 3481, 264, 5593], "temperature": 0.0, "avg_logprob": -0.11614131927490234, "compression_ratio": 1.6284584980237153, "no_speech_prob": 5.862767466169316e-06}, {"id": 379, "seek": 162966, "start": 1629.66, "end": 1633.68, "text": " mean squared percent error.", "tokens": [914, 8889, 3043, 6713, 13], "temperature": 0.0, "avg_logprob": -0.11736781256539482, "compression_ratio": 1.7567567567567568, "no_speech_prob": 4.785073087987257e-06}, {"id": 380, "seek": 162966, "start": 1633.68, "end": 1638.88, "text": " We've learned about the y range before, which is going to use that sigmoid to help us get", "tokens": [492, 600, 3264, 466, 264, 288, 3613, 949, 11, 597, 307, 516, 281, 764, 300, 4556, 3280, 327, 281, 854, 505, 483], "temperature": 0.0, "avg_logprob": -0.11736781256539482, "compression_ratio": 1.7567567567567568, "no_speech_prob": 4.785073087987257e-06}, {"id": 381, "seek": 162966, "start": 1638.88, "end": 1641.14, "text": " in the right range.", "tokens": [294, 264, 558, 3613, 13], "temperature": 0.0, "avg_logprob": -0.11736781256539482, "compression_ratio": 1.7567567567567568, "no_speech_prob": 4.785073087987257e-06}, {"id": 382, "seek": 162966, "start": 1641.14, "end": 1646.3400000000001, "text": " Because this time the y values are going to be taken the log of it first, we need to make", "tokens": [1436, 341, 565, 264, 288, 4190, 366, 516, 281, 312, 2726, 264, 3565, 295, 309, 700, 11, 321, 643, 281, 652], "temperature": 0.0, "avg_logprob": -0.11736781256539482, "compression_ratio": 1.7567567567567568, "no_speech_prob": 4.785073087987257e-06}, {"id": 383, "seek": 162966, "start": 1646.3400000000001, "end": 1649.3600000000001, "text": " sure that the y range we want is also the log.", "tokens": [988, 300, 264, 288, 3613, 321, 528, 307, 611, 264, 3565, 13], "temperature": 0.0, "avg_logprob": -0.11736781256539482, "compression_ratio": 1.7567567567567568, "no_speech_prob": 4.785073087987257e-06}, {"id": 384, "seek": 162966, "start": 1649.3600000000001, "end": 1653.1200000000001, "text": " So I'm going to take the maximum of the sales column.", "tokens": [407, 286, 478, 516, 281, 747, 264, 6674, 295, 264, 5763, 7738, 13], "temperature": 0.0, "avg_logprob": -0.11736781256539482, "compression_ratio": 1.7567567567567568, "no_speech_prob": 4.785073087987257e-06}, {"id": 385, "seek": 162966, "start": 1653.1200000000001, "end": 1655.0400000000002, "text": " I'm going to multiply it by a little bit.", "tokens": [286, 478, 516, 281, 12972, 309, 538, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.11736781256539482, "compression_ratio": 1.7567567567567568, "no_speech_prob": 4.785073087987257e-06}, {"id": 386, "seek": 162966, "start": 1655.0400000000002, "end": 1659.0800000000002, "text": " So that because remember how we said it's nice if your range is a bit wider than the", "tokens": [407, 300, 570, 1604, 577, 321, 848, 309, 311, 1481, 498, 428, 3613, 307, 257, 857, 11842, 813, 264], "temperature": 0.0, "avg_logprob": -0.11736781256539482, "compression_ratio": 1.7567567567567568, "no_speech_prob": 4.785073087987257e-06}, {"id": 387, "seek": 165908, "start": 1659.08, "end": 1662.6799999999998, "text": " range of the data, and then we're going to take the log.", "tokens": [3613, 295, 264, 1412, 11, 293, 550, 321, 434, 516, 281, 747, 264, 3565, 13], "temperature": 0.0, "avg_logprob": -0.11366901030907264, "compression_ratio": 1.7417840375586855, "no_speech_prob": 9.516154932498466e-06}, {"id": 388, "seek": 165908, "start": 1662.6799999999998, "end": 1664.84, "text": " And that's going to be our maximum.", "tokens": [400, 300, 311, 516, 281, 312, 527, 6674, 13], "temperature": 0.0, "avg_logprob": -0.11366901030907264, "compression_ratio": 1.7417840375586855, "no_speech_prob": 9.516154932498466e-06}, {"id": 389, "seek": 165908, "start": 1664.84, "end": 1672.0, "text": " So then our y range will be from zero to a bit more than the maximum.", "tokens": [407, 550, 527, 288, 3613, 486, 312, 490, 4018, 281, 257, 857, 544, 813, 264, 6674, 13], "temperature": 0.0, "avg_logprob": -0.11366901030907264, "compression_ratio": 1.7417840375586855, "no_speech_prob": 9.516154932498466e-06}, {"id": 390, "seek": 165908, "start": 1672.0, "end": 1674.0, "text": " So now we've got our data bunch.", "tokens": [407, 586, 321, 600, 658, 527, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.11366901030907264, "compression_ratio": 1.7417840375586855, "no_speech_prob": 9.516154932498466e-06}, {"id": 391, "seek": 165908, "start": 1674.0, "end": 1676.72, "text": " We can create a tabular learner from it.", "tokens": [492, 393, 1884, 257, 4421, 1040, 33347, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.11366901030907264, "compression_ratio": 1.7417840375586855, "no_speech_prob": 9.516154932498466e-06}, {"id": 392, "seek": 165908, "start": 1676.72, "end": 1679.28, "text": " And then we have to pass in our architecture.", "tokens": [400, 550, 321, 362, 281, 1320, 294, 527, 9482, 13], "temperature": 0.0, "avg_logprob": -0.11366901030907264, "compression_ratio": 1.7417840375586855, "no_speech_prob": 9.516154932498466e-06}, {"id": 393, "seek": 165908, "start": 1679.28, "end": 1686.8799999999999, "text": " And as we briefly discussed, for a tabular model, our architecture is literally the most", "tokens": [400, 382, 321, 10515, 7152, 11, 337, 257, 4421, 1040, 2316, 11, 527, 9482, 307, 3736, 264, 881], "temperature": 0.0, "avg_logprob": -0.11366901030907264, "compression_ratio": 1.7417840375586855, "no_speech_prob": 9.516154932498466e-06}, {"id": 394, "seek": 168688, "start": 1686.88, "end": 1692.4, "text": " basic, fully connected network, just like we showed in this picture.", "tokens": [3875, 11, 4498, 4582, 3209, 11, 445, 411, 321, 4712, 294, 341, 3036, 13], "temperature": 0.0, "avg_logprob": -0.1423927182736604, "compression_ratio": 1.7632850241545894, "no_speech_prob": 6.240864422579762e-06}, {"id": 395, "seek": 168688, "start": 1692.4, "end": 1700.1200000000001, "text": " It's an import, matrix multiply, non-linearity, matrix multiply, non-linearity, matrix multiply,", "tokens": [467, 311, 364, 974, 11, 8141, 12972, 11, 2107, 12, 1889, 17409, 11, 8141, 12972, 11, 2107, 12, 1889, 17409, 11, 8141, 12972, 11], "temperature": 0.0, "avg_logprob": -0.1423927182736604, "compression_ratio": 1.7632850241545894, "no_speech_prob": 6.240864422579762e-06}, {"id": 396, "seek": 168688, "start": 1700.1200000000001, "end": 1703.5200000000002, "text": " non-linearity, done.", "tokens": [2107, 12, 1889, 17409, 11, 1096, 13], "temperature": 0.0, "avg_logprob": -0.1423927182736604, "compression_ratio": 1.7632850241545894, "no_speech_prob": 6.240864422579762e-06}, {"id": 397, "seek": 168688, "start": 1703.5200000000002, "end": 1707.7600000000002, "text": " One of the interesting things about this is that this competition is three years old,", "tokens": [1485, 295, 264, 1880, 721, 466, 341, 307, 300, 341, 6211, 307, 1045, 924, 1331, 11], "temperature": 0.0, "avg_logprob": -0.1423927182736604, "compression_ratio": 1.7632850241545894, "no_speech_prob": 6.240864422579762e-06}, {"id": 398, "seek": 168688, "start": 1707.7600000000002, "end": 1713.72, "text": " but I'm not aware of any significant advances, at least in terms of architecture, that would", "tokens": [457, 286, 478, 406, 3650, 295, 604, 4776, 25297, 11, 412, 1935, 294, 2115, 295, 9482, 11, 300, 576], "temperature": 0.0, "avg_logprob": -0.1423927182736604, "compression_ratio": 1.7632850241545894, "no_speech_prob": 6.240864422579762e-06}, {"id": 399, "seek": 171372, "start": 1713.72, "end": 1719.1200000000001, "text": " cause me to choose something different to what the third-placed folks did three years", "tokens": [3082, 385, 281, 2826, 746, 819, 281, 437, 264, 2636, 12, 564, 3839, 4024, 630, 1045, 924], "temperature": 0.0, "avg_logprob": -0.1400080352533059, "compression_ratio": 1.401098901098901, "no_speech_prob": 3.555893272277899e-06}, {"id": 400, "seek": 171372, "start": 1719.1200000000001, "end": 1720.1200000000001, "text": " ago.", "tokens": [2057, 13], "temperature": 0.0, "avg_logprob": -0.1400080352533059, "compression_ratio": 1.401098901098901, "no_speech_prob": 3.555893272277899e-06}, {"id": 401, "seek": 171372, "start": 1720.1200000000001, "end": 1727.08, "text": " We're still basically using simple, fully connected models for this problem.", "tokens": [492, 434, 920, 1936, 1228, 2199, 11, 4498, 4582, 5245, 337, 341, 1154, 13], "temperature": 0.0, "avg_logprob": -0.1400080352533059, "compression_ratio": 1.401098901098901, "no_speech_prob": 3.555893272277899e-06}, {"id": 402, "seek": 171372, "start": 1727.08, "end": 1736.92, "text": " Now the intermediate weight matrix is going to have to go from a 1,000 activation input", "tokens": [823, 264, 19376, 3364, 8141, 307, 516, 281, 362, 281, 352, 490, 257, 502, 11, 1360, 24433, 4846], "temperature": 0.0, "avg_logprob": -0.1400080352533059, "compression_ratio": 1.401098901098901, "no_speech_prob": 3.555893272277899e-06}, {"id": 403, "seek": 173692, "start": 1736.92, "end": 1744.04, "text": " to a 500 activation output, which means it's going to have to be 500,000 elements in that", "tokens": [281, 257, 5923, 24433, 5598, 11, 597, 1355, 309, 311, 516, 281, 362, 281, 312, 5923, 11, 1360, 4959, 294, 300], "temperature": 0.0, "avg_logprob": -0.11091912644250053, "compression_ratio": 1.7850877192982457, "no_speech_prob": 4.092879407835426e-06}, {"id": 404, "seek": 173692, "start": 1744.04, "end": 1745.04, "text": " weight matrix.", "tokens": [3364, 8141, 13], "temperature": 0.0, "avg_logprob": -0.11091912644250053, "compression_ratio": 1.7850877192982457, "no_speech_prob": 4.092879407835426e-06}, {"id": 405, "seek": 173692, "start": 1745.04, "end": 1750.1200000000001, "text": " That's an awful lot for a data set with only a few hundred thousand rows.", "tokens": [663, 311, 364, 11232, 688, 337, 257, 1412, 992, 365, 787, 257, 1326, 3262, 4714, 13241, 13], "temperature": 0.0, "avg_logprob": -0.11091912644250053, "compression_ratio": 1.7850877192982457, "no_speech_prob": 4.092879407835426e-06}, {"id": 406, "seek": 173692, "start": 1750.1200000000001, "end": 1754.2, "text": " So this is going to overfit, and we need to make sure it doesn't.", "tokens": [407, 341, 307, 516, 281, 670, 6845, 11, 293, 321, 643, 281, 652, 988, 309, 1177, 380, 13], "temperature": 0.0, "avg_logprob": -0.11091912644250053, "compression_ratio": 1.7850877192982457, "no_speech_prob": 4.092879407835426e-06}, {"id": 407, "seek": 173692, "start": 1754.2, "end": 1757.6000000000001, "text": " So one way to make sure it doesn't, well, the way to make sure it doesn't is to use", "tokens": [407, 472, 636, 281, 652, 988, 309, 1177, 380, 11, 731, 11, 264, 636, 281, 652, 988, 309, 1177, 380, 307, 281, 764], "temperature": 0.0, "avg_logprob": -0.11091912644250053, "compression_ratio": 1.7850877192982457, "no_speech_prob": 4.092879407835426e-06}, {"id": 408, "seek": 173692, "start": 1757.6000000000001, "end": 1763.96, "text": " regularization, not to reduce the number of parameters, to use regularization.", "tokens": [3890, 2144, 11, 406, 281, 5407, 264, 1230, 295, 9834, 11, 281, 764, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.11091912644250053, "compression_ratio": 1.7850877192982457, "no_speech_prob": 4.092879407835426e-06}, {"id": 409, "seek": 176396, "start": 1763.96, "end": 1769.52, "text": " So one way to do that will be to use weight decay, which Fast.ai will use automatically,", "tokens": [407, 472, 636, 281, 360, 300, 486, 312, 281, 764, 3364, 21039, 11, 597, 15968, 13, 1301, 486, 764, 6772, 11], "temperature": 0.0, "avg_logprob": -0.1286198216625768, "compression_ratio": 1.7489711934156378, "no_speech_prob": 4.029422598250676e-06}, {"id": 410, "seek": 176396, "start": 1769.52, "end": 1773.3600000000001, "text": " and you can vary it to something other than the default if you wish.", "tokens": [293, 291, 393, 10559, 309, 281, 746, 661, 813, 264, 7576, 498, 291, 3172, 13], "temperature": 0.0, "avg_logprob": -0.1286198216625768, "compression_ratio": 1.7489711934156378, "no_speech_prob": 4.029422598250676e-06}, {"id": 411, "seek": 176396, "start": 1773.3600000000001, "end": 1777.24, "text": " It turns out in this case, we're going to want more regularization, and so we're going", "tokens": [467, 4523, 484, 294, 341, 1389, 11, 321, 434, 516, 281, 528, 544, 3890, 2144, 11, 293, 370, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.1286198216625768, "compression_ratio": 1.7489711934156378, "no_speech_prob": 4.029422598250676e-06}, {"id": 412, "seek": 176396, "start": 1777.24, "end": 1780.44, "text": " to pass in something called keys.", "tokens": [281, 1320, 294, 746, 1219, 9317, 13], "temperature": 0.0, "avg_logprob": -0.1286198216625768, "compression_ratio": 1.7489711934156378, "no_speech_prob": 4.029422598250676e-06}, {"id": 413, "seek": 176396, "start": 1780.44, "end": 1785.72, "text": " This is going to provide dropout, and also this one here, mdrop, this is going to provide", "tokens": [639, 307, 516, 281, 2893, 3270, 346, 11, 293, 611, 341, 472, 510, 11, 275, 23332, 11, 341, 307, 516, 281, 2893], "temperature": 0.0, "avg_logprob": -0.1286198216625768, "compression_ratio": 1.7489711934156378, "no_speech_prob": 4.029422598250676e-06}, {"id": 414, "seek": 176396, "start": 1785.72, "end": 1787.52, "text": " embedding dropout.", "tokens": [12240, 3584, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.1286198216625768, "compression_ratio": 1.7489711934156378, "no_speech_prob": 4.029422598250676e-06}, {"id": 415, "seek": 176396, "start": 1787.52, "end": 1790.96, "text": " So let's learn about what is dropout.", "tokens": [407, 718, 311, 1466, 466, 437, 307, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.1286198216625768, "compression_ratio": 1.7489711934156378, "no_speech_prob": 4.029422598250676e-06}, {"id": 416, "seek": 179096, "start": 1790.96, "end": 1794.96, "text": " But the short version is dropout is a kind of regularization.", "tokens": [583, 264, 2099, 3037, 307, 3270, 346, 307, 257, 733, 295, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.1679349798905222, "compression_ratio": 1.6, "no_speech_prob": 7.0718119786761235e-06}, {"id": 417, "seek": 179096, "start": 1794.96, "end": 1796.68, "text": " This is the dropout paper.", "tokens": [639, 307, 264, 3270, 346, 3035, 13], "temperature": 0.0, "avg_logprob": -0.1679349798905222, "compression_ratio": 1.6, "no_speech_prob": 7.0718119786761235e-06}, {"id": 418, "seek": 179096, "start": 1796.68, "end": 1808.64, "text": " Nitish, how do you say this, Suvasthava, it was Suvasthava's master's thesis under Jeffrey", "tokens": [37942, 742, 11, 577, 360, 291, 584, 341, 11, 2746, 7967, 392, 4061, 11, 309, 390, 2746, 7967, 392, 4061, 311, 4505, 311, 22288, 833, 28721], "temperature": 0.0, "avg_logprob": -0.1679349798905222, "compression_ratio": 1.6, "no_speech_prob": 7.0718119786761235e-06}, {"id": 419, "seek": 179096, "start": 1808.64, "end": 1814.56, "text": " Hinton, and this picture from the original paper is a really good picture of what's going", "tokens": [389, 12442, 11, 293, 341, 3036, 490, 264, 3380, 3035, 307, 257, 534, 665, 3036, 295, 437, 311, 516], "temperature": 0.0, "avg_logprob": -0.1679349798905222, "compression_ratio": 1.6, "no_speech_prob": 7.0718119786761235e-06}, {"id": 420, "seek": 179096, "start": 1814.56, "end": 1815.8600000000001, "text": " on.", "tokens": [322, 13], "temperature": 0.0, "avg_logprob": -0.1679349798905222, "compression_ratio": 1.6, "no_speech_prob": 7.0718119786761235e-06}, {"id": 421, "seek": 179096, "start": 1815.8600000000001, "end": 1819.52, "text": " This first picture is a picture of a standard fully connected network.", "tokens": [639, 700, 3036, 307, 257, 3036, 295, 257, 3832, 4498, 4582, 3209, 13], "temperature": 0.0, "avg_logprob": -0.1679349798905222, "compression_ratio": 1.6, "no_speech_prob": 7.0718119786761235e-06}, {"id": 422, "seek": 181952, "start": 1819.52, "end": 1822.52, "text": " It's a picture of this.", "tokens": [467, 311, 257, 3036, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.09802816532276294, "compression_ratio": 1.6331658291457287, "no_speech_prob": 1.5936266208882444e-05}, {"id": 423, "seek": 181952, "start": 1822.52, "end": 1829.0, "text": " What each line shows is a multiplication of an activation times a weight.", "tokens": [708, 1184, 1622, 3110, 307, 257, 27290, 295, 364, 24433, 1413, 257, 3364, 13], "temperature": 0.0, "avg_logprob": -0.09802816532276294, "compression_ratio": 1.6331658291457287, "no_speech_prob": 1.5936266208882444e-05}, {"id": 424, "seek": 181952, "start": 1829.0, "end": 1832.58, "text": " And then when you've got multiple arrows coming in, that represents a sum.", "tokens": [400, 550, 562, 291, 600, 658, 3866, 19669, 1348, 294, 11, 300, 8855, 257, 2408, 13], "temperature": 0.0, "avg_logprob": -0.09802816532276294, "compression_ratio": 1.6331658291457287, "no_speech_prob": 1.5936266208882444e-05}, {"id": 425, "seek": 181952, "start": 1832.58, "end": 1840.66, "text": " So this activation here is the sum of all of these inputs times all of these activations.", "tokens": [407, 341, 24433, 510, 307, 264, 2408, 295, 439, 295, 613, 15743, 1413, 439, 295, 613, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.09802816532276294, "compression_ratio": 1.6331658291457287, "no_speech_prob": 1.5936266208882444e-05}, {"id": 426, "seek": 181952, "start": 1840.66, "end": 1845.48, "text": " So that's what a normal fully connected neural net looks like.", "tokens": [407, 300, 311, 437, 257, 2710, 4498, 4582, 18161, 2533, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.09802816532276294, "compression_ratio": 1.6331658291457287, "no_speech_prob": 1.5936266208882444e-05}, {"id": 427, "seek": 184548, "start": 1845.48, "end": 1850.52, "text": " For dropout, we throw that away.", "tokens": [1171, 3270, 346, 11, 321, 3507, 300, 1314, 13], "temperature": 0.0, "avg_logprob": -0.11392559051513672, "compression_ratio": 1.8341232227488151, "no_speech_prob": 1.9033676608160022e-06}, {"id": 428, "seek": 184548, "start": 1850.52, "end": 1858.08, "text": " At random, we throw away some percentage of the activations, not the weights, not the", "tokens": [1711, 4974, 11, 321, 3507, 1314, 512, 9668, 295, 264, 2430, 763, 11, 406, 264, 17443, 11, 406, 264], "temperature": 0.0, "avg_logprob": -0.11392559051513672, "compression_ratio": 1.8341232227488151, "no_speech_prob": 1.9033676608160022e-06}, {"id": 429, "seek": 184548, "start": 1858.08, "end": 1859.08, "text": " parameters.", "tokens": [9834, 13], "temperature": 0.0, "avg_logprob": -0.11392559051513672, "compression_ratio": 1.8341232227488151, "no_speech_prob": 1.9033676608160022e-06}, {"id": 430, "seek": 184548, "start": 1859.08, "end": 1864.08, "text": " Remember there's only two types of number in a neural net, parameters, also called weights", "tokens": [5459, 456, 311, 787, 732, 3467, 295, 1230, 294, 257, 18161, 2533, 11, 9834, 11, 611, 1219, 17443], "temperature": 0.0, "avg_logprob": -0.11392559051513672, "compression_ratio": 1.8341232227488151, "no_speech_prob": 1.9033676608160022e-06}, {"id": 431, "seek": 184548, "start": 1864.08, "end": 1866.6, "text": " kind of, and activations.", "tokens": [733, 295, 11, 293, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.11392559051513672, "compression_ratio": 1.8341232227488151, "no_speech_prob": 1.9033676608160022e-06}, {"id": 432, "seek": 184548, "start": 1866.6, "end": 1869.16, "text": " So we're going to throw away some activation.", "tokens": [407, 321, 434, 516, 281, 3507, 1314, 512, 24433, 13], "temperature": 0.0, "avg_logprob": -0.11392559051513672, "compression_ratio": 1.8341232227488151, "no_speech_prob": 1.9033676608160022e-06}, {"id": 433, "seek": 184548, "start": 1869.16, "end": 1874.0, "text": " So you can see that when we throw away this activation, all of the things that were connected", "tokens": [407, 291, 393, 536, 300, 562, 321, 3507, 1314, 341, 24433, 11, 439, 295, 264, 721, 300, 645, 4582], "temperature": 0.0, "avg_logprob": -0.11392559051513672, "compression_ratio": 1.8341232227488151, "no_speech_prob": 1.9033676608160022e-06}, {"id": 434, "seek": 187400, "start": 1874.0, "end": 1877.88, "text": " to it are gone too.", "tokens": [281, 309, 366, 2780, 886, 13], "temperature": 0.0, "avg_logprob": -0.1514197622026716, "compression_ratio": 1.3831168831168832, "no_speech_prob": 2.601603910079575e-06}, {"id": 435, "seek": 187400, "start": 1877.88, "end": 1884.8, "text": " For each mini-batch, we throw away a different subset of activations.", "tokens": [1171, 1184, 8382, 12, 65, 852, 11, 321, 3507, 1314, 257, 819, 25993, 295, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.1514197622026716, "compression_ratio": 1.3831168831168832, "no_speech_prob": 2.601603910079575e-06}, {"id": 436, "seek": 187400, "start": 1884.8, "end": 1886.68, "text": " How many do we throw away?", "tokens": [1012, 867, 360, 321, 3507, 1314, 30], "temperature": 0.0, "avg_logprob": -0.1514197622026716, "compression_ratio": 1.3831168831168832, "no_speech_prob": 2.601603910079575e-06}, {"id": 437, "seek": 187400, "start": 1886.68, "end": 1892.12, "text": " We throw each one away with a probability p.", "tokens": [492, 3507, 1184, 472, 1314, 365, 257, 8482, 280, 13], "temperature": 0.0, "avg_logprob": -0.1514197622026716, "compression_ratio": 1.3831168831168832, "no_speech_prob": 2.601603910079575e-06}, {"id": 438, "seek": 187400, "start": 1892.12, "end": 1896.82, "text": " A common value of p is 0.5.", "tokens": [316, 2689, 2158, 295, 280, 307, 1958, 13, 20, 13], "temperature": 0.0, "avg_logprob": -0.1514197622026716, "compression_ratio": 1.3831168831168832, "no_speech_prob": 2.601603910079575e-06}, {"id": 439, "seek": 187400, "start": 1896.82, "end": 1897.82, "text": " So what does that mean?", "tokens": [407, 437, 775, 300, 914, 30], "temperature": 0.0, "avg_logprob": -0.1514197622026716, "compression_ratio": 1.3831168831168832, "no_speech_prob": 2.601603910079575e-06}, {"id": 440, "seek": 189782, "start": 1897.82, "end": 1904.8, "text": " And you'll see in this case, not only have they deleted at random some of these hidden", "tokens": [400, 291, 603, 536, 294, 341, 1389, 11, 406, 787, 362, 436, 22981, 412, 4974, 512, 295, 613, 7633], "temperature": 0.0, "avg_logprob": -0.12452318191528321, "compression_ratio": 1.7255813953488373, "no_speech_prob": 6.786716539863846e-07}, {"id": 441, "seek": 189782, "start": 1904.8, "end": 1908.8799999999999, "text": " layers, but they've actually deleted some of the inputs as well.", "tokens": [7914, 11, 457, 436, 600, 767, 22981, 512, 295, 264, 15743, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12452318191528321, "compression_ratio": 1.7255813953488373, "no_speech_prob": 6.786716539863846e-07}, {"id": 442, "seek": 189782, "start": 1908.8799999999999, "end": 1911.12, "text": " Deleting the inputs is pretty unusual.", "tokens": [5831, 9880, 264, 15743, 307, 1238, 10901, 13], "temperature": 0.0, "avg_logprob": -0.12452318191528321, "compression_ratio": 1.7255813953488373, "no_speech_prob": 6.786716539863846e-07}, {"id": 443, "seek": 189782, "start": 1911.12, "end": 1917.96, "text": " Normally, we only delete activations in the hidden layers.", "tokens": [17424, 11, 321, 787, 12097, 2430, 763, 294, 264, 7633, 7914, 13], "temperature": 0.0, "avg_logprob": -0.12452318191528321, "compression_ratio": 1.7255813953488373, "no_speech_prob": 6.786716539863846e-07}, {"id": 444, "seek": 189782, "start": 1917.96, "end": 1918.96, "text": " So what does this do?", "tokens": [407, 437, 775, 341, 360, 30], "temperature": 0.0, "avg_logprob": -0.12452318191528321, "compression_ratio": 1.7255813953488373, "no_speech_prob": 6.786716539863846e-07}, {"id": 445, "seek": 189782, "start": 1918.96, "end": 1924.24, "text": " Well, every time I have a mini-batch going through, I at random throw away some of the", "tokens": [1042, 11, 633, 565, 286, 362, 257, 8382, 12, 65, 852, 516, 807, 11, 286, 412, 4974, 3507, 1314, 512, 295, 264], "temperature": 0.0, "avg_logprob": -0.12452318191528321, "compression_ratio": 1.7255813953488373, "no_speech_prob": 6.786716539863846e-07}, {"id": 446, "seek": 189782, "start": 1924.24, "end": 1925.24, "text": " activations.", "tokens": [2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.12452318191528321, "compression_ratio": 1.7255813953488373, "no_speech_prob": 6.786716539863846e-07}, {"id": 447, "seek": 192524, "start": 1925.24, "end": 1929.8, "text": " I put them back, I throw away some different ones.", "tokens": [286, 829, 552, 646, 11, 286, 3507, 1314, 512, 819, 2306, 13], "temperature": 0.0, "avg_logprob": -0.20493152141571044, "compression_ratio": 1.6243654822335025, "no_speech_prob": 1.7880603309095022e-06}, {"id": 448, "seek": 192524, "start": 1929.8, "end": 1938.44, "text": " So it means that no one activation can kind of memorize some part of the input.", "tokens": [407, 309, 1355, 300, 572, 472, 24433, 393, 733, 295, 27478, 512, 644, 295, 264, 4846, 13], "temperature": 0.0, "avg_logprob": -0.20493152141571044, "compression_ratio": 1.6243654822335025, "no_speech_prob": 1.7880603309095022e-06}, {"id": 449, "seek": 192524, "start": 1938.44, "end": 1940.2, "text": " Because that's what happens if we overfit.", "tokens": [1436, 300, 311, 437, 2314, 498, 321, 670, 6845, 13], "temperature": 0.0, "avg_logprob": -0.20493152141571044, "compression_ratio": 1.6243654822335025, "no_speech_prob": 1.7880603309095022e-06}, {"id": 450, "seek": 192524, "start": 1940.2, "end": 1947.0, "text": " If we overfit, some part of the model is basically learning to recognize a particular image rather", "tokens": [759, 321, 670, 6845, 11, 512, 644, 295, 264, 2316, 307, 1936, 2539, 281, 5521, 257, 1729, 3256, 2831], "temperature": 0.0, "avg_logprob": -0.20493152141571044, "compression_ratio": 1.6243654822335025, "no_speech_prob": 1.7880603309095022e-06}, {"id": 451, "seek": 192524, "start": 1947.0, "end": 1952.32, "text": " than a feature in general or a particular item.", "tokens": [813, 257, 4111, 294, 2674, 420, 257, 1729, 3174, 13], "temperature": 0.0, "avg_logprob": -0.20493152141571044, "compression_ratio": 1.6243654822335025, "no_speech_prob": 1.7880603309095022e-06}, {"id": 452, "seek": 195232, "start": 1952.32, "end": 1957.04, "text": " With dropout, it's going to be very hard for it to do that.", "tokens": [2022, 3270, 346, 11, 309, 311, 516, 281, 312, 588, 1152, 337, 309, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.11798908060247248, "compression_ratio": 1.6431372549019607, "no_speech_prob": 6.8542431108653545e-06}, {"id": 453, "seek": 195232, "start": 1957.04, "end": 1965.48, "text": " In fact, Geoffrey Hinton described one of the kind of part of the thinking behind this", "tokens": [682, 1186, 11, 26119, 7950, 389, 12442, 7619, 472, 295, 264, 733, 295, 644, 295, 264, 1953, 2261, 341], "temperature": 0.0, "avg_logprob": -0.11798908060247248, "compression_ratio": 1.6431372549019607, "no_speech_prob": 6.8542431108653545e-06}, {"id": 454, "seek": 195232, "start": 1965.48, "end": 1966.48, "text": " as follows.", "tokens": [382, 10002, 13], "temperature": 0.0, "avg_logprob": -0.11798908060247248, "compression_ratio": 1.6431372549019607, "no_speech_prob": 6.8542431108653545e-06}, {"id": 455, "seek": 195232, "start": 1966.48, "end": 1970.6, "text": " He said he noticed every time he went to his bank that all the tellers and staff moved", "tokens": [634, 848, 415, 5694, 633, 565, 415, 1437, 281, 702, 3765, 300, 439, 264, 980, 433, 293, 3525, 4259], "temperature": 0.0, "avg_logprob": -0.11798908060247248, "compression_ratio": 1.6431372549019607, "no_speech_prob": 6.8542431108653545e-06}, {"id": 456, "seek": 195232, "start": 1970.6, "end": 1971.96, "text": " around.", "tokens": [926, 13], "temperature": 0.0, "avg_logprob": -0.11798908060247248, "compression_ratio": 1.6431372549019607, "no_speech_prob": 6.8542431108653545e-06}, {"id": 457, "seek": 195232, "start": 1971.96, "end": 1975.72, "text": " And he realized the reason for this must be that they're trying to avoid fraud.", "tokens": [400, 415, 5334, 264, 1778, 337, 341, 1633, 312, 300, 436, 434, 1382, 281, 5042, 14560, 13], "temperature": 0.0, "avg_logprob": -0.11798908060247248, "compression_ratio": 1.6431372549019607, "no_speech_prob": 6.8542431108653545e-06}, {"id": 458, "seek": 195232, "start": 1975.72, "end": 1980.32, "text": " If they keep moving them around, nobody can specialize so much in that one thing that", "tokens": [759, 436, 1066, 2684, 552, 926, 11, 5079, 393, 37938, 370, 709, 294, 300, 472, 551, 300], "temperature": 0.0, "avg_logprob": -0.11798908060247248, "compression_ratio": 1.6431372549019607, "no_speech_prob": 6.8542431108653545e-06}, {"id": 459, "seek": 198032, "start": 1980.32, "end": 1984.4399999999998, "text": " they're doing that they can figure out kind of a conspiracy to defraud the bank.", "tokens": [436, 434, 884, 300, 436, 393, 2573, 484, 733, 295, 257, 20439, 281, 1060, 424, 532, 264, 3765, 13], "temperature": 0.0, "avg_logprob": -0.16285653399606037, "compression_ratio": 1.6563706563706564, "no_speech_prob": 2.4060882424237207e-06}, {"id": 460, "seek": 198032, "start": 1984.4399999999998, "end": 1987.76, "text": " Now of course, it depends when you ask Hinton.", "tokens": [823, 295, 1164, 11, 309, 5946, 562, 291, 1029, 389, 12442, 13], "temperature": 0.0, "avg_logprob": -0.16285653399606037, "compression_ratio": 1.6563706563706564, "no_speech_prob": 2.4060882424237207e-06}, {"id": 461, "seek": 198032, "start": 1987.76, "end": 1991.8, "text": " At other times, he says that the reason for this was because he thought about how spiking", "tokens": [1711, 661, 1413, 11, 415, 1619, 300, 264, 1778, 337, 341, 390, 570, 415, 1194, 466, 577, 637, 13085], "temperature": 0.0, "avg_logprob": -0.16285653399606037, "compression_ratio": 1.6563706563706564, "no_speech_prob": 2.4060882424237207e-06}, {"id": 462, "seek": 198032, "start": 1991.8, "end": 1992.8, "text": " neurons work.", "tokens": [22027, 589, 13], "temperature": 0.0, "avg_logprob": -0.16285653399606037, "compression_ratio": 1.6563706563706564, "no_speech_prob": 2.4060882424237207e-06}, {"id": 463, "seek": 198032, "start": 1992.8, "end": 1996.0, "text": " And he's a neuroscientist by training.", "tokens": [400, 415, 311, 257, 28813, 5412, 468, 538, 3097, 13], "temperature": 0.0, "avg_logprob": -0.16285653399606037, "compression_ratio": 1.6563706563706564, "no_speech_prob": 2.4060882424237207e-06}, {"id": 464, "seek": 198032, "start": 1996.0, "end": 2000.28, "text": " There's a view that spiking neurons might help regularization and dropout is kind of", "tokens": [821, 311, 257, 1910, 300, 637, 13085, 22027, 1062, 854, 3890, 2144, 293, 3270, 346, 307, 733, 295], "temperature": 0.0, "avg_logprob": -0.16285653399606037, "compression_ratio": 1.6563706563706564, "no_speech_prob": 2.4060882424237207e-06}, {"id": 465, "seek": 198032, "start": 2000.28, "end": 2004.24, "text": " a way of matching this idea of spiking neurons.", "tokens": [257, 636, 295, 14324, 341, 1558, 295, 637, 13085, 22027, 13], "temperature": 0.0, "avg_logprob": -0.16285653399606037, "compression_ratio": 1.6563706563706564, "no_speech_prob": 2.4060882424237207e-06}, {"id": 466, "seek": 198032, "start": 2004.24, "end": 2006.6399999999999, "text": " I mean, it's interesting.", "tokens": [286, 914, 11, 309, 311, 1880, 13], "temperature": 0.0, "avg_logprob": -0.16285653399606037, "compression_ratio": 1.6563706563706564, "no_speech_prob": 2.4060882424237207e-06}, {"id": 467, "seek": 200664, "start": 2006.64, "end": 2014.3600000000001, "text": " When you actually ask people, where did your idea for some algorithm come from, it basically", "tokens": [1133, 291, 767, 1029, 561, 11, 689, 630, 428, 1558, 337, 512, 9284, 808, 490, 11, 309, 1936], "temperature": 0.0, "avg_logprob": -0.11457176208496093, "compression_ratio": 1.6048387096774193, "no_speech_prob": 2.090443558699917e-06}, {"id": 468, "seek": 200664, "start": 2014.3600000000001, "end": 2016.5600000000002, "text": " never comes from math.", "tokens": [1128, 1487, 490, 5221, 13], "temperature": 0.0, "avg_logprob": -0.11457176208496093, "compression_ratio": 1.6048387096774193, "no_speech_prob": 2.090443558699917e-06}, {"id": 469, "seek": 200664, "start": 2016.5600000000002, "end": 2021.1200000000001, "text": " It always comes from intuition and kind of thinking about physical analogies and stuff", "tokens": [467, 1009, 1487, 490, 24002, 293, 733, 295, 1953, 466, 4001, 16660, 530, 293, 1507], "temperature": 0.0, "avg_logprob": -0.11457176208496093, "compression_ratio": 1.6048387096774193, "no_speech_prob": 2.090443558699917e-06}, {"id": 470, "seek": 200664, "start": 2021.1200000000001, "end": 2022.1200000000001, "text": " like that.", "tokens": [411, 300, 13], "temperature": 0.0, "avg_logprob": -0.11457176208496093, "compression_ratio": 1.6048387096774193, "no_speech_prob": 2.090443558699917e-06}, {"id": 471, "seek": 200664, "start": 2022.1200000000001, "end": 2026.5600000000002, "text": " So anyway, the truth is a bunch of ideas, I guess, were all flowing around and they", "tokens": [407, 4033, 11, 264, 3494, 307, 257, 3840, 295, 3487, 11, 286, 2041, 11, 645, 439, 13974, 926, 293, 436], "temperature": 0.0, "avg_logprob": -0.11457176208496093, "compression_ratio": 1.6048387096774193, "no_speech_prob": 2.090443558699917e-06}, {"id": 472, "seek": 200664, "start": 2026.5600000000002, "end": 2028.6000000000001, "text": " came up with this idea of dropout.", "tokens": [1361, 493, 365, 341, 1558, 295, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.11457176208496093, "compression_ratio": 1.6048387096774193, "no_speech_prob": 2.090443558699917e-06}, {"id": 473, "seek": 200664, "start": 2028.6000000000001, "end": 2033.92, "text": " But the important thing to know is it worked really, really well.", "tokens": [583, 264, 1021, 551, 281, 458, 307, 309, 2732, 534, 11, 534, 731, 13], "temperature": 0.0, "avg_logprob": -0.11457176208496093, "compression_ratio": 1.6048387096774193, "no_speech_prob": 2.090443558699917e-06}, {"id": 474, "seek": 203392, "start": 2033.92, "end": 2041.3200000000002, "text": " And so we can use it in our models to get generalization for free.", "tokens": [400, 370, 321, 393, 764, 309, 294, 527, 5245, 281, 483, 2674, 2144, 337, 1737, 13], "temperature": 0.0, "avg_logprob": -0.12890151568821498, "compression_ratio": 1.6326530612244898, "no_speech_prob": 5.422096819529543e-06}, {"id": 475, "seek": 203392, "start": 2041.3200000000002, "end": 2044.92, "text": " Now too much dropout, of course, is reducing the capacity of your model.", "tokens": [823, 886, 709, 3270, 346, 11, 295, 1164, 11, 307, 12245, 264, 6042, 295, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12890151568821498, "compression_ratio": 1.6326530612244898, "no_speech_prob": 5.422096819529543e-06}, {"id": 476, "seek": 203392, "start": 2044.92, "end": 2046.28, "text": " So it's going to underfit.", "tokens": [407, 309, 311, 516, 281, 833, 6845, 13], "temperature": 0.0, "avg_logprob": -0.12890151568821498, "compression_ratio": 1.6326530612244898, "no_speech_prob": 5.422096819529543e-06}, {"id": 477, "seek": 203392, "start": 2046.28, "end": 2049.92, "text": " And so you've got to play around with different dropout values for each of your layers to", "tokens": [400, 370, 291, 600, 658, 281, 862, 926, 365, 819, 3270, 346, 4190, 337, 1184, 295, 428, 7914, 281], "temperature": 0.0, "avg_logprob": -0.12890151568821498, "compression_ratio": 1.6326530612244898, "no_speech_prob": 5.422096819529543e-06}, {"id": 478, "seek": 203392, "start": 2049.92, "end": 2051.2400000000002, "text": " decide.", "tokens": [4536, 13], "temperature": 0.0, "avg_logprob": -0.12890151568821498, "compression_ratio": 1.6326530612244898, "no_speech_prob": 5.422096819529543e-06}, {"id": 479, "seek": 203392, "start": 2051.2400000000002, "end": 2059.12, "text": " So in pretty much every fast AI learner, there's a parameter called P's, P S, which will be", "tokens": [407, 294, 1238, 709, 633, 2370, 7318, 33347, 11, 456, 311, 257, 13075, 1219, 430, 311, 11, 430, 318, 11, 597, 486, 312], "temperature": 0.0, "avg_logprob": -0.12890151568821498, "compression_ratio": 1.6326530612244898, "no_speech_prob": 5.422096819529543e-06}, {"id": 480, "seek": 203392, "start": 2059.12, "end": 2061.84, "text": " the P value for the dropout for each layer.", "tokens": [264, 430, 2158, 337, 264, 3270, 346, 337, 1184, 4583, 13], "temperature": 0.0, "avg_logprob": -0.12890151568821498, "compression_ratio": 1.6326530612244898, "no_speech_prob": 5.422096819529543e-06}, {"id": 481, "seek": 206184, "start": 2061.84, "end": 2064.36, "text": " So you can just pass in a list.", "tokens": [407, 291, 393, 445, 1320, 294, 257, 1329, 13], "temperature": 0.0, "avg_logprob": -0.14269029048451207, "compression_ratio": 1.7211155378486056, "no_speech_prob": 3.844896127702668e-06}, {"id": 482, "seek": 206184, "start": 2064.36, "end": 2070.8, "text": " Or you can pass in an int and it'll create a list with that value everywhere.", "tokens": [1610, 291, 393, 1320, 294, 364, 560, 293, 309, 603, 1884, 257, 1329, 365, 300, 2158, 5315, 13], "temperature": 0.0, "avg_logprob": -0.14269029048451207, "compression_ratio": 1.7211155378486056, "no_speech_prob": 3.844896127702668e-06}, {"id": 483, "seek": 206184, "start": 2070.8, "end": 2073.44, "text": " Sometimes it's a little different for CNN, for example.", "tokens": [4803, 309, 311, 257, 707, 819, 337, 24859, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.14269029048451207, "compression_ratio": 1.7211155378486056, "no_speech_prob": 3.844896127702668e-06}, {"id": 484, "seek": 206184, "start": 2073.44, "end": 2078.02, "text": " It actually, if you pass in an int, it will use that for the last layer and half that", "tokens": [467, 767, 11, 498, 291, 1320, 294, 364, 560, 11, 309, 486, 764, 300, 337, 264, 1036, 4583, 293, 1922, 300], "temperature": 0.0, "avg_logprob": -0.14269029048451207, "compression_ratio": 1.7211155378486056, "no_speech_prob": 3.844896127702668e-06}, {"id": 485, "seek": 206184, "start": 2078.02, "end": 2079.7000000000003, "text": " value for the earlier layers.", "tokens": [2158, 337, 264, 3071, 7914, 13], "temperature": 0.0, "avg_logprob": -0.14269029048451207, "compression_ratio": 1.7211155378486056, "no_speech_prob": 3.844896127702668e-06}, {"id": 486, "seek": 206184, "start": 2079.7000000000003, "end": 2083.84, "text": " We basically try to do things that kind of represent best practice.", "tokens": [492, 1936, 853, 281, 360, 721, 300, 733, 295, 2906, 1151, 3124, 13], "temperature": 0.0, "avg_logprob": -0.14269029048451207, "compression_ratio": 1.7211155378486056, "no_speech_prob": 3.844896127702668e-06}, {"id": 487, "seek": 206184, "start": 2083.84, "end": 2088.0, "text": " But you can always pass in your own list to get exactly the dropout that you want.", "tokens": [583, 291, 393, 1009, 1320, 294, 428, 1065, 1329, 281, 483, 2293, 264, 3270, 346, 300, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.14269029048451207, "compression_ratio": 1.7211155378486056, "no_speech_prob": 3.844896127702668e-06}, {"id": 488, "seek": 208800, "start": 2088.0, "end": 2094.64, "text": " There is an interesting feature of dropout, which is that we talk about training time", "tokens": [821, 307, 364, 1880, 4111, 295, 3270, 346, 11, 597, 307, 300, 321, 751, 466, 3097, 565], "temperature": 0.0, "avg_logprob": -0.11132963435856376, "compression_ratio": 1.8549618320610688, "no_speech_prob": 5.6823914746928494e-06}, {"id": 489, "seek": 208800, "start": 2094.64, "end": 2095.92, "text": " and test time.", "tokens": [293, 1500, 565, 13], "temperature": 0.0, "avg_logprob": -0.11132963435856376, "compression_ratio": 1.8549618320610688, "no_speech_prob": 5.6823914746928494e-06}, {"id": 490, "seek": 208800, "start": 2095.92, "end": 2097.76, "text": " Test time we also call inference time.", "tokens": [9279, 565, 321, 611, 818, 38253, 565, 13], "temperature": 0.0, "avg_logprob": -0.11132963435856376, "compression_ratio": 1.8549618320610688, "no_speech_prob": 5.6823914746928494e-06}, {"id": 491, "seek": 208800, "start": 2097.76, "end": 2101.84, "text": " Training time is when we're actually doing those weight updates, the back propagation.", "tokens": [20620, 565, 307, 562, 321, 434, 767, 884, 729, 3364, 9205, 11, 264, 646, 38377, 13], "temperature": 0.0, "avg_logprob": -0.11132963435856376, "compression_ratio": 1.8549618320610688, "no_speech_prob": 5.6823914746928494e-06}, {"id": 492, "seek": 208800, "start": 2101.84, "end": 2107.2, "text": " In the training time, dropout works the way we just saw.", "tokens": [682, 264, 3097, 565, 11, 3270, 346, 1985, 264, 636, 321, 445, 1866, 13], "temperature": 0.0, "avg_logprob": -0.11132963435856376, "compression_ratio": 1.8549618320610688, "no_speech_prob": 5.6823914746928494e-06}, {"id": 493, "seek": 208800, "start": 2107.2, "end": 2110.2, "text": " At test time, we turn off dropout.", "tokens": [1711, 1500, 565, 11, 321, 1261, 766, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.11132963435856376, "compression_ratio": 1.8549618320610688, "no_speech_prob": 5.6823914746928494e-06}, {"id": 494, "seek": 208800, "start": 2110.2, "end": 2113.2, "text": " We're not going to do dropout anymore because we want it to be as accurate as possible.", "tokens": [492, 434, 406, 516, 281, 360, 3270, 346, 3602, 570, 321, 528, 309, 281, 312, 382, 8559, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.11132963435856376, "compression_ratio": 1.8549618320610688, "no_speech_prob": 5.6823914746928494e-06}, {"id": 495, "seek": 208800, "start": 2113.2, "end": 2117.64, "text": " We're not training, so we can't cause it to overfit when we're doing inference.", "tokens": [492, 434, 406, 3097, 11, 370, 321, 393, 380, 3082, 309, 281, 670, 6845, 562, 321, 434, 884, 38253, 13], "temperature": 0.0, "avg_logprob": -0.11132963435856376, "compression_ratio": 1.8549618320610688, "no_speech_prob": 5.6823914746928494e-06}, {"id": 496, "seek": 211764, "start": 2117.64, "end": 2119.96, "text": " So we remove dropout.", "tokens": [407, 321, 4159, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.12179833366757348, "compression_ratio": 1.6081632653061224, "no_speech_prob": 2.6016050469479524e-06}, {"id": 497, "seek": 211764, "start": 2119.96, "end": 2126.16, "text": " But what that means is if previously p was 0.5, then half the activations were being", "tokens": [583, 437, 300, 1355, 307, 498, 8046, 280, 390, 1958, 13, 20, 11, 550, 1922, 264, 2430, 763, 645, 885], "temperature": 0.0, "avg_logprob": -0.12179833366757348, "compression_ratio": 1.6081632653061224, "no_speech_prob": 2.6016050469479524e-06}, {"id": 498, "seek": 211764, "start": 2126.16, "end": 2130.74, "text": " removed, which means when they're all there, now our overall activation level is twice", "tokens": [7261, 11, 597, 1355, 562, 436, 434, 439, 456, 11, 586, 527, 4787, 24433, 1496, 307, 6091], "temperature": 0.0, "avg_logprob": -0.12179833366757348, "compression_ratio": 1.6081632653061224, "no_speech_prob": 2.6016050469479524e-06}, {"id": 499, "seek": 211764, "start": 2130.74, "end": 2132.52, "text": " what it used to be.", "tokens": [437, 309, 1143, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.12179833366757348, "compression_ratio": 1.6081632653061224, "no_speech_prob": 2.6016050469479524e-06}, {"id": 500, "seek": 211764, "start": 2132.52, "end": 2137.7599999999998, "text": " And so therefore in the paper, they suggest multiplying all of your weights at test time", "tokens": [400, 370, 4412, 294, 264, 3035, 11, 436, 3402, 30955, 439, 295, 428, 17443, 412, 1500, 565], "temperature": 0.0, "avg_logprob": -0.12179833366757348, "compression_ratio": 1.6081632653061224, "no_speech_prob": 2.6016050469479524e-06}, {"id": 501, "seek": 211764, "start": 2137.7599999999998, "end": 2138.7599999999998, "text": " by p.", "tokens": [538, 280, 13], "temperature": 0.0, "avg_logprob": -0.12179833366757348, "compression_ratio": 1.6081632653061224, "no_speech_prob": 2.6016050469479524e-06}, {"id": 502, "seek": 211764, "start": 2138.7599999999998, "end": 2145.6, "text": " Interestingly, you can dig into the PyTorch source code and you can find the actual C", "tokens": [30564, 11, 291, 393, 2528, 666, 264, 9953, 51, 284, 339, 4009, 3089, 293, 291, 393, 915, 264, 3539, 383], "temperature": 0.0, "avg_logprob": -0.12179833366757348, "compression_ratio": 1.6081632653061224, "no_speech_prob": 2.6016050469479524e-06}, {"id": 503, "seek": 214560, "start": 2145.6, "end": 2148.8199999999997, "text": " code where dropout is implemented.", "tokens": [3089, 689, 3270, 346, 307, 12270, 13], "temperature": 0.0, "avg_logprob": -0.11414084264210292, "compression_ratio": 1.7935779816513762, "no_speech_prob": 1.2606856216734741e-05}, {"id": 504, "seek": 214560, "start": 2148.8199999999997, "end": 2150.4, "text": " And here it is.", "tokens": [400, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.11414084264210292, "compression_ratio": 1.7935779816513762, "no_speech_prob": 1.2606856216734741e-05}, {"id": 505, "seek": 214560, "start": 2150.4, "end": 2153.08, "text": " And you can see what they're doing is something quite interesting.", "tokens": [400, 291, 393, 536, 437, 436, 434, 884, 307, 746, 1596, 1880, 13], "temperature": 0.0, "avg_logprob": -0.11414084264210292, "compression_ratio": 1.7935779816513762, "no_speech_prob": 1.2606856216734741e-05}, {"id": 506, "seek": 214560, "start": 2153.08, "end": 2155.8399999999997, "text": " They first of all do a Bernoulli trial.", "tokens": [814, 700, 295, 439, 360, 257, 10781, 263, 16320, 7308, 13], "temperature": 0.0, "avg_logprob": -0.11414084264210292, "compression_ratio": 1.7935779816513762, "no_speech_prob": 1.2606856216734741e-05}, {"id": 507, "seek": 214560, "start": 2155.8399999999997, "end": 2161.8399999999997, "text": " So a Bernoulli trial is with probability 1 minus p, return the value 1, otherwise return", "tokens": [407, 257, 10781, 263, 16320, 7308, 307, 365, 8482, 502, 3175, 280, 11, 2736, 264, 2158, 502, 11, 5911, 2736], "temperature": 0.0, "avg_logprob": -0.11414084264210292, "compression_ratio": 1.7935779816513762, "no_speech_prob": 1.2606856216734741e-05}, {"id": 508, "seek": 214560, "start": 2161.8399999999997, "end": 2163.2, "text": " the value 0.", "tokens": [264, 2158, 1958, 13], "temperature": 0.0, "avg_logprob": -0.11414084264210292, "compression_ratio": 1.7935779816513762, "no_speech_prob": 1.2606856216734741e-05}, {"id": 509, "seek": 214560, "start": 2163.2, "end": 2164.62, "text": " That's all it means.", "tokens": [663, 311, 439, 309, 1355, 13], "temperature": 0.0, "avg_logprob": -0.11414084264210292, "compression_ratio": 1.7935779816513762, "no_speech_prob": 1.2606856216734741e-05}, {"id": 510, "seek": 214560, "start": 2164.62, "end": 2168.7, "text": " So in this case, p is the probability of dropout.", "tokens": [407, 294, 341, 1389, 11, 280, 307, 264, 8482, 295, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.11414084264210292, "compression_ratio": 1.7935779816513762, "no_speech_prob": 1.2606856216734741e-05}, {"id": 511, "seek": 214560, "start": 2168.7, "end": 2173.14, "text": " So 1 minus p is the probability that we keep the activation.", "tokens": [407, 502, 3175, 280, 307, 264, 8482, 300, 321, 1066, 264, 24433, 13], "temperature": 0.0, "avg_logprob": -0.11414084264210292, "compression_ratio": 1.7935779816513762, "no_speech_prob": 1.2606856216734741e-05}, {"id": 512, "seek": 217314, "start": 2173.14, "end": 2177.7599999999998, "text": " So we end up here with either a 1 or a 0.", "tokens": [407, 321, 917, 493, 510, 365, 2139, 257, 502, 420, 257, 1958, 13], "temperature": 0.0, "avg_logprob": -0.1047417732977098, "compression_ratio": 1.538888888888889, "no_speech_prob": 2.058038944596774e-06}, {"id": 513, "seek": 217314, "start": 2177.7599999999998, "end": 2182.7599999999998, "text": " And then, this is interesting, we divide in place, remember underscore means in place", "tokens": [400, 550, 11, 341, 307, 1880, 11, 321, 9845, 294, 1081, 11, 1604, 37556, 1355, 294, 1081], "temperature": 0.0, "avg_logprob": -0.1047417732977098, "compression_ratio": 1.538888888888889, "no_speech_prob": 2.058038944596774e-06}, {"id": 514, "seek": 217314, "start": 2182.7599999999998, "end": 2187.16, "text": " in PyTorch, we divide in place that 1 or 0 by 1 minus p.", "tokens": [294, 9953, 51, 284, 339, 11, 321, 9845, 294, 1081, 300, 502, 420, 1958, 538, 502, 3175, 280, 13], "temperature": 0.0, "avg_logprob": -0.1047417732977098, "compression_ratio": 1.538888888888889, "no_speech_prob": 2.058038944596774e-06}, {"id": 515, "seek": 217314, "start": 2187.16, "end": 2190.92, "text": " If it's a 0, nothing happens, it's still 0.", "tokens": [759, 309, 311, 257, 1958, 11, 1825, 2314, 11, 309, 311, 920, 1958, 13], "temperature": 0.0, "avg_logprob": -0.1047417732977098, "compression_ratio": 1.538888888888889, "no_speech_prob": 2.058038944596774e-06}, {"id": 516, "seek": 217314, "start": 2190.92, "end": 2196.44, "text": " If it's a 1 and p was 0.5, that 1 now becomes 2.", "tokens": [759, 309, 311, 257, 502, 293, 280, 390, 1958, 13, 20, 11, 300, 502, 586, 3643, 568, 13], "temperature": 0.0, "avg_logprob": -0.1047417732977098, "compression_ratio": 1.538888888888889, "no_speech_prob": 2.058038944596774e-06}, {"id": 517, "seek": 219644, "start": 2196.44, "end": 2204.48, "text": " And then finally, we multiply in place our input by this noise, this dropout mask.", "tokens": [400, 550, 2721, 11, 321, 12972, 294, 1081, 527, 4846, 538, 341, 5658, 11, 341, 3270, 346, 6094, 13], "temperature": 0.0, "avg_logprob": -0.11276288399329552, "compression_ratio": 1.7099236641221374, "no_speech_prob": 5.896405923522252e-07}, {"id": 518, "seek": 219644, "start": 2204.48, "end": 2210.8, "text": " So in other words, we actually don't do in PyTorch, we don't do the change at test time.", "tokens": [407, 294, 661, 2283, 11, 321, 767, 500, 380, 360, 294, 9953, 51, 284, 339, 11, 321, 500, 380, 360, 264, 1319, 412, 1500, 565, 13], "temperature": 0.0, "avg_logprob": -0.11276288399329552, "compression_ratio": 1.7099236641221374, "no_speech_prob": 5.896405923522252e-07}, {"id": 519, "seek": 219644, "start": 2210.8, "end": 2215.2200000000003, "text": " We actually do the change at training time, which means that you don't have to do anything", "tokens": [492, 767, 360, 264, 1319, 412, 3097, 565, 11, 597, 1355, 300, 291, 500, 380, 362, 281, 360, 1340], "temperature": 0.0, "avg_logprob": -0.11276288399329552, "compression_ratio": 1.7099236641221374, "no_speech_prob": 5.896405923522252e-07}, {"id": 520, "seek": 219644, "start": 2215.2200000000003, "end": 2217.4, "text": " special at inference time with PyTorch.", "tokens": [2121, 412, 38253, 565, 365, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.11276288399329552, "compression_ratio": 1.7099236641221374, "no_speech_prob": 5.896405923522252e-07}, {"id": 521, "seek": 219644, "start": 2217.4, "end": 2219.96, "text": " It's not just PyTorch, it's quite a common pattern.", "tokens": [467, 311, 406, 445, 9953, 51, 284, 339, 11, 309, 311, 1596, 257, 2689, 5102, 13], "temperature": 0.0, "avg_logprob": -0.11276288399329552, "compression_ratio": 1.7099236641221374, "no_speech_prob": 5.896405923522252e-07}, {"id": 522, "seek": 219644, "start": 2219.96, "end": 2225.52, "text": " But it's kind of nice to look inside the PyTorch source code and see, you know, dropout, this", "tokens": [583, 309, 311, 733, 295, 1481, 281, 574, 1854, 264, 9953, 51, 284, 339, 4009, 3089, 293, 536, 11, 291, 458, 11, 3270, 346, 11, 341], "temperature": 0.0, "avg_logprob": -0.11276288399329552, "compression_ratio": 1.7099236641221374, "no_speech_prob": 5.896405923522252e-07}, {"id": 523, "seek": 222552, "start": 2225.52, "end": 2231.0, "text": " incredibly cool, incredibly valuable thing is really just these three lines of code,", "tokens": [6252, 1627, 11, 6252, 8263, 551, 307, 534, 445, 613, 1045, 3876, 295, 3089, 11], "temperature": 0.0, "avg_logprob": -0.10667742888132731, "compression_ratio": 1.7084870848708487, "no_speech_prob": 1.0289117199135944e-05}, {"id": 524, "seek": 222552, "start": 2231.0, "end": 2235.6, "text": " which they do in C because I guess it ends up a bit faster when it's all fused together.", "tokens": [597, 436, 360, 294, 383, 570, 286, 2041, 309, 5314, 493, 257, 857, 4663, 562, 309, 311, 439, 283, 4717, 1214, 13], "temperature": 0.0, "avg_logprob": -0.10667742888132731, "compression_ratio": 1.7084870848708487, "no_speech_prob": 1.0289117199135944e-05}, {"id": 525, "seek": 222552, "start": 2235.6, "end": 2239.12, "text": " But lots of libraries do it in Python, and that works well as well.", "tokens": [583, 3195, 295, 15148, 360, 309, 294, 15329, 11, 293, 300, 1985, 731, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.10667742888132731, "compression_ratio": 1.7084870848708487, "no_speech_prob": 1.0289117199135944e-05}, {"id": 526, "seek": 222552, "start": 2239.12, "end": 2244.48, "text": " You can even write your own dropout layer, and it should give exactly the same results", "tokens": [509, 393, 754, 2464, 428, 1065, 3270, 346, 4583, 11, 293, 309, 820, 976, 2293, 264, 912, 3542], "temperature": 0.0, "avg_logprob": -0.10667742888132731, "compression_ratio": 1.7084870848708487, "no_speech_prob": 1.0289117199135944e-05}, {"id": 527, "seek": 222552, "start": 2244.48, "end": 2245.48, "text": " as this.", "tokens": [382, 341, 13], "temperature": 0.0, "avg_logprob": -0.10667742888132731, "compression_ratio": 1.7084870848708487, "no_speech_prob": 1.0289117199135944e-05}, {"id": 528, "seek": 222552, "start": 2245.48, "end": 2248.12, "text": " So that'd be a good exercise to try.", "tokens": [407, 300, 1116, 312, 257, 665, 5380, 281, 853, 13], "temperature": 0.0, "avg_logprob": -0.10667742888132731, "compression_ratio": 1.7084870848708487, "no_speech_prob": 1.0289117199135944e-05}, {"id": 529, "seek": 222552, "start": 2248.12, "end": 2253.44, "text": " See if you can create your own dropout layer in Python, and see if you can replicate the", "tokens": [3008, 498, 291, 393, 1884, 428, 1065, 3270, 346, 4583, 294, 15329, 11, 293, 536, 498, 291, 393, 25356, 264], "temperature": 0.0, "avg_logprob": -0.10667742888132731, "compression_ratio": 1.7084870848708487, "no_speech_prob": 1.0289117199135944e-05}, {"id": 530, "seek": 225344, "start": 2253.44, "end": 2259.0, "text": " results that we get with this dropout layer.", "tokens": [3542, 300, 321, 483, 365, 341, 3270, 346, 4583, 13], "temperature": 0.0, "avg_logprob": -0.07717689903833533, "compression_ratio": 1.9937106918238994, "no_speech_prob": 3.7852928471693303e-06}, {"id": 531, "seek": 225344, "start": 2259.0, "end": 2262.2400000000002, "text": " So that's dropout.", "tokens": [407, 300, 311, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.07717689903833533, "compression_ratio": 1.9937106918238994, "no_speech_prob": 3.7852928471693303e-06}, {"id": 532, "seek": 225344, "start": 2262.2400000000002, "end": 2266.92, "text": " And so in this case, we're going to use a tiny bit of dropout on the first layer and", "tokens": [400, 370, 294, 341, 1389, 11, 321, 434, 516, 281, 764, 257, 5870, 857, 295, 3270, 346, 322, 264, 700, 4583, 293], "temperature": 0.0, "avg_logprob": -0.07717689903833533, "compression_ratio": 1.9937106918238994, "no_speech_prob": 3.7852928471693303e-06}, {"id": 533, "seek": 225344, "start": 2266.92, "end": 2271.96, "text": " a little bit of dropout on the next layer, and then we're going to use special dropout", "tokens": [257, 707, 857, 295, 3270, 346, 322, 264, 958, 4583, 11, 293, 550, 321, 434, 516, 281, 764, 2121, 3270, 346], "temperature": 0.0, "avg_logprob": -0.07717689903833533, "compression_ratio": 1.9937106918238994, "no_speech_prob": 3.7852928471693303e-06}, {"id": 534, "seek": 225344, "start": 2271.96, "end": 2273.88, "text": " on the embedding layer.", "tokens": [322, 264, 12240, 3584, 4583, 13], "temperature": 0.0, "avg_logprob": -0.07717689903833533, "compression_ratio": 1.9937106918238994, "no_speech_prob": 3.7852928471693303e-06}, {"id": 535, "seek": 225344, "start": 2273.88, "end": 2276.68, "text": " Now why do we use special dropout on the embedding layer?", "tokens": [823, 983, 360, 321, 764, 2121, 3270, 346, 322, 264, 12240, 3584, 4583, 30], "temperature": 0.0, "avg_logprob": -0.07717689903833533, "compression_ratio": 1.9937106918238994, "no_speech_prob": 3.7852928471693303e-06}, {"id": 536, "seek": 227668, "start": 2276.68, "end": 2290.04, "text": " So if you look inside the FastAI source code, here's our tabular model.", "tokens": [407, 498, 291, 574, 1854, 264, 15968, 48698, 4009, 3089, 11, 510, 311, 527, 4421, 1040, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1517842478222317, "compression_ratio": 1.6, "no_speech_prob": 3.1381091503135394e-06}, {"id": 537, "seek": 227668, "start": 2290.04, "end": 2294.0, "text": " You'll see that in the section that checks that there's some embeddings, we call each", "tokens": [509, 603, 536, 300, 294, 264, 3541, 300, 13834, 300, 456, 311, 512, 12240, 29432, 11, 321, 818, 1184], "temperature": 0.0, "avg_logprob": -0.1517842478222317, "compression_ratio": 1.6, "no_speech_prob": 3.1381091503135394e-06}, {"id": 538, "seek": 227668, "start": 2294.0, "end": 2299.16, "text": " embedding and then we concatenate the embeddings into a single matrix, and then we call embedding", "tokens": [12240, 3584, 293, 550, 321, 1588, 7186, 473, 264, 12240, 29432, 666, 257, 2167, 8141, 11, 293, 550, 321, 818, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.1517842478222317, "compression_ratio": 1.6, "no_speech_prob": 3.1381091503135394e-06}, {"id": 539, "seek": 227668, "start": 2299.16, "end": 2300.22, "text": " dropout.", "tokens": [3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.1517842478222317, "compression_ratio": 1.6, "no_speech_prob": 3.1381091503135394e-06}, {"id": 540, "seek": 230022, "start": 2300.22, "end": 2308.2, "text": " An embedding dropout is simply just a dropout, right, so it's just an instance of a dropout", "tokens": [1107, 12240, 3584, 3270, 346, 307, 2935, 445, 257, 3270, 346, 11, 558, 11, 370, 309, 311, 445, 364, 5197, 295, 257, 3270, 346], "temperature": 0.0, "avg_logprob": -0.11692179804262907, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.436747083971568e-06}, {"id": 541, "seek": 230022, "start": 2308.2, "end": 2310.7999999999997, "text": " module.", "tokens": [10088, 13], "temperature": 0.0, "avg_logprob": -0.11692179804262907, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.436747083971568e-06}, {"id": 542, "seek": 230022, "start": 2310.7999999999997, "end": 2312.52, "text": " This kind of makes sense, right?", "tokens": [639, 733, 295, 1669, 2020, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.11692179804262907, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.436747083971568e-06}, {"id": 543, "seek": 230022, "start": 2312.52, "end": 2317.5, "text": " For continuous variables, that continuous variable is just in one column.", "tokens": [1171, 10957, 9102, 11, 300, 10957, 7006, 307, 445, 294, 472, 7738, 13], "temperature": 0.0, "avg_logprob": -0.11692179804262907, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.436747083971568e-06}, {"id": 544, "seek": 230022, "start": 2317.5, "end": 2321.2799999999997, "text": " You wouldn't want to do dropout on that because you're literally deleting the existence of", "tokens": [509, 2759, 380, 528, 281, 360, 3270, 346, 322, 300, 570, 291, 434, 3736, 48946, 264, 9123, 295], "temperature": 0.0, "avg_logprob": -0.11692179804262907, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.436747083971568e-06}, {"id": 545, "seek": 230022, "start": 2321.2799999999997, "end": 2325.0, "text": " that whole input, which is almost certainly not what you want.", "tokens": [300, 1379, 4846, 11, 597, 307, 1920, 3297, 406, 437, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.11692179804262907, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.436747083971568e-06}, {"id": 546, "seek": 232500, "start": 2325.0, "end": 2331.64, "text": " But for an embedding, an embedding is just effectively a matrix multiply by a one-hot", "tokens": [583, 337, 364, 12240, 3584, 11, 364, 12240, 3584, 307, 445, 8659, 257, 8141, 12972, 538, 257, 472, 12, 12194], "temperature": 0.0, "avg_logprob": -0.09072626818407763, "compression_ratio": 1.8767123287671232, "no_speech_prob": 7.338196041928313e-07}, {"id": 547, "seek": 232500, "start": 2331.64, "end": 2333.36, "text": " encoded matrix.", "tokens": [2058, 12340, 8141, 13], "temperature": 0.0, "avg_logprob": -0.09072626818407763, "compression_ratio": 1.8767123287671232, "no_speech_prob": 7.338196041928313e-07}, {"id": 548, "seek": 232500, "start": 2333.36, "end": 2335.16, "text": " So it's just another layer.", "tokens": [407, 309, 311, 445, 1071, 4583, 13], "temperature": 0.0, "avg_logprob": -0.09072626818407763, "compression_ratio": 1.8767123287671232, "no_speech_prob": 7.338196041928313e-07}, {"id": 549, "seek": 232500, "start": 2335.16, "end": 2339.58, "text": " So it makes perfect sense to have dropout on the output of the embedding because you're", "tokens": [407, 309, 1669, 2176, 2020, 281, 362, 3270, 346, 322, 264, 5598, 295, 264, 12240, 3584, 570, 291, 434], "temperature": 0.0, "avg_logprob": -0.09072626818407763, "compression_ratio": 1.8767123287671232, "no_speech_prob": 7.338196041928313e-07}, {"id": 550, "seek": 232500, "start": 2339.58, "end": 2342.52, "text": " putting dropout on those activations of that layer.", "tokens": [3372, 3270, 346, 322, 729, 2430, 763, 295, 300, 4583, 13], "temperature": 0.0, "avg_logprob": -0.09072626818407763, "compression_ratio": 1.8767123287671232, "no_speech_prob": 7.338196041928313e-07}, {"id": 551, "seek": 232500, "start": 2342.52, "end": 2350.62, "text": " And so you're basically saying, let's delete at random some of the results of that embedding,", "tokens": [400, 370, 291, 434, 1936, 1566, 11, 718, 311, 12097, 412, 4974, 512, 295, 264, 3542, 295, 300, 12240, 3584, 11], "temperature": 0.0, "avg_logprob": -0.09072626818407763, "compression_ratio": 1.8767123287671232, "no_speech_prob": 7.338196041928313e-07}, {"id": 552, "seek": 232500, "start": 2350.62, "end": 2352.1, "text": " some of those activations.", "tokens": [512, 295, 729, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.09072626818407763, "compression_ratio": 1.8767123287671232, "no_speech_prob": 7.338196041928313e-07}, {"id": 553, "seek": 232500, "start": 2352.1, "end": 2354.32, "text": " So that makes sense.", "tokens": [407, 300, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.09072626818407763, "compression_ratio": 1.8767123287671232, "no_speech_prob": 7.338196041928313e-07}, {"id": 554, "seek": 235432, "start": 2354.32, "end": 2359.44, "text": " The other reason we do it that way is because I did very extensive experiments about a year", "tokens": [440, 661, 1778, 321, 360, 309, 300, 636, 307, 570, 286, 630, 588, 13246, 12050, 466, 257, 1064], "temperature": 0.0, "avg_logprob": -0.1371340172313084, "compression_ratio": 1.6254545454545455, "no_speech_prob": 4.3568552428041585e-06}, {"id": 555, "seek": 235432, "start": 2359.44, "end": 2368.36, "text": " ago where on this data set, I tried lots of different ways of doing kind of everything.", "tokens": [2057, 689, 322, 341, 1412, 992, 11, 286, 3031, 3195, 295, 819, 2098, 295, 884, 733, 295, 1203, 13], "temperature": 0.0, "avg_logprob": -0.1371340172313084, "compression_ratio": 1.6254545454545455, "no_speech_prob": 4.3568552428041585e-06}, {"id": 556, "seek": 235432, "start": 2368.36, "end": 2370.44, "text": " And you can actually see it here.", "tokens": [400, 291, 393, 767, 536, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.1371340172313084, "compression_ratio": 1.6254545454545455, "no_speech_prob": 4.3568552428041585e-06}, {"id": 557, "seek": 235432, "start": 2370.44, "end": 2374.2000000000003, "text": " I put it all in a spreadsheet, of course, Microsoft Excel, put them into a pivot table", "tokens": [286, 829, 309, 439, 294, 257, 27733, 11, 295, 1164, 11, 8116, 19060, 11, 829, 552, 666, 257, 14538, 3199], "temperature": 0.0, "avg_logprob": -0.1371340172313084, "compression_ratio": 1.6254545454545455, "no_speech_prob": 4.3568552428041585e-06}, {"id": 558, "seek": 235432, "start": 2374.2000000000003, "end": 2379.1600000000003, "text": " to summarize them all together to find out kind of which different choices and hyperparameters", "tokens": [281, 20858, 552, 439, 1214, 281, 915, 484, 733, 295, 597, 819, 7994, 293, 9848, 2181, 335, 6202], "temperature": 0.0, "avg_logprob": -0.1371340172313084, "compression_ratio": 1.6254545454545455, "no_speech_prob": 4.3568552428041585e-06}, {"id": 559, "seek": 235432, "start": 2379.1600000000003, "end": 2381.96, "text": " and architectures worked well and worked less well.", "tokens": [293, 6331, 1303, 2732, 731, 293, 2732, 1570, 731, 13], "temperature": 0.0, "avg_logprob": -0.1371340172313084, "compression_ratio": 1.6254545454545455, "no_speech_prob": 4.3568552428041585e-06}, {"id": 560, "seek": 238196, "start": 2381.96, "end": 2384.64, "text": " And then I created all these little graphs.", "tokens": [400, 550, 286, 2942, 439, 613, 707, 24877, 13], "temperature": 0.0, "avg_logprob": -0.11667825464616742, "compression_ratio": 1.667870036101083, "no_speech_prob": 1.1911029105249327e-06}, {"id": 561, "seek": 238196, "start": 2384.64, "end": 2388.6, "text": " And these are like little summary training graphs for different combinations of hyperparameters", "tokens": [400, 613, 366, 411, 707, 12691, 3097, 24877, 337, 819, 21267, 295, 9848, 2181, 335, 6202], "temperature": 0.0, "avg_logprob": -0.11667825464616742, "compression_ratio": 1.667870036101083, "no_speech_prob": 1.1911029105249327e-06}, {"id": 562, "seek": 238196, "start": 2388.6, "end": 2389.88, "text": " and architectures.", "tokens": [293, 6331, 1303, 13], "temperature": 0.0, "avg_logprob": -0.11667825464616742, "compression_ratio": 1.667870036101083, "no_speech_prob": 1.1911029105249327e-06}, {"id": 563, "seek": 238196, "start": 2389.88, "end": 2395.64, "text": " And I found that there was one of them which ended up consistently getting a good predictive", "tokens": [400, 286, 1352, 300, 456, 390, 472, 295, 552, 597, 4590, 493, 14961, 1242, 257, 665, 35521], "temperature": 0.0, "avg_logprob": -0.11667825464616742, "compression_ratio": 1.667870036101083, "no_speech_prob": 1.1911029105249327e-06}, {"id": 564, "seek": 238196, "start": 2395.64, "end": 2397.2400000000002, "text": " accuracy.", "tokens": [14170, 13], "temperature": 0.0, "avg_logprob": -0.11667825464616742, "compression_ratio": 1.667870036101083, "no_speech_prob": 1.1911029105249327e-06}, {"id": 565, "seek": 238196, "start": 2397.2400000000002, "end": 2401.08, "text": " The kind of bumpiness of the training was pretty low.", "tokens": [440, 733, 295, 9961, 1324, 295, 264, 3097, 390, 1238, 2295, 13], "temperature": 0.0, "avg_logprob": -0.11667825464616742, "compression_ratio": 1.667870036101083, "no_speech_prob": 1.1911029105249327e-06}, {"id": 566, "seek": 238196, "start": 2401.08, "end": 2405.94, "text": " And you can see it's just a nice, smooth curve.", "tokens": [400, 291, 393, 536, 309, 311, 445, 257, 1481, 11, 5508, 7605, 13], "temperature": 0.0, "avg_logprob": -0.11667825464616742, "compression_ratio": 1.667870036101083, "no_speech_prob": 1.1911029105249327e-06}, {"id": 567, "seek": 238196, "start": 2405.94, "end": 2411.56, "text": " And so this is an example of the kind of experiments that I do that end up in the Fast AI library.", "tokens": [400, 370, 341, 307, 364, 1365, 295, 264, 733, 295, 12050, 300, 286, 360, 300, 917, 493, 294, 264, 15968, 7318, 6405, 13], "temperature": 0.0, "avg_logprob": -0.11667825464616742, "compression_ratio": 1.667870036101083, "no_speech_prob": 1.1911029105249327e-06}, {"id": 568, "seek": 241156, "start": 2411.56, "end": 2416.36, "text": " So embedding dropout is one of those things that I just found worked really well.", "tokens": [407, 12240, 3584, 3270, 346, 307, 472, 295, 729, 721, 300, 286, 445, 1352, 2732, 534, 731, 13], "temperature": 0.0, "avg_logprob": -0.1528533241965554, "compression_ratio": 1.7043795620437956, "no_speech_prob": 7.071798336255597e-06}, {"id": 569, "seek": 241156, "start": 2416.36, "end": 2423.72, "text": " And basically the results of these experiments is why it looks like this rather than something", "tokens": [400, 1936, 264, 3542, 295, 613, 12050, 307, 983, 309, 1542, 411, 341, 2831, 813, 746], "temperature": 0.0, "avg_logprob": -0.1528533241965554, "compression_ratio": 1.7043795620437956, "no_speech_prob": 7.071798336255597e-06}, {"id": 570, "seek": 241156, "start": 2423.72, "end": 2424.72, "text": " else.", "tokens": [1646, 13], "temperature": 0.0, "avg_logprob": -0.1528533241965554, "compression_ratio": 1.7043795620437956, "no_speech_prob": 7.071798336255597e-06}, {"id": 571, "seek": 241156, "start": 2424.72, "end": 2429.08, "text": " Well, it's a combination of these experiments, but then why did I do these particular experiments?", "tokens": [1042, 11, 309, 311, 257, 6562, 295, 613, 12050, 11, 457, 550, 983, 630, 286, 360, 613, 1729, 12050, 30], "temperature": 0.0, "avg_logprob": -0.1528533241965554, "compression_ratio": 1.7043795620437956, "no_speech_prob": 7.071798336255597e-06}, {"id": 572, "seek": 241156, "start": 2429.08, "end": 2435.64, "text": " Well, because it was very influenced by what worked well in that Kaggle prize winner's", "tokens": [1042, 11, 570, 309, 390, 588, 15269, 538, 437, 2732, 731, 294, 300, 48751, 22631, 12818, 8507, 311], "temperature": 0.0, "avg_logprob": -0.1528533241965554, "compression_ratio": 1.7043795620437956, "no_speech_prob": 7.071798336255597e-06}, {"id": 573, "seek": 241156, "start": 2435.64, "end": 2436.64, "text": " paper.", "tokens": [3035, 13], "temperature": 0.0, "avg_logprob": -0.1528533241965554, "compression_ratio": 1.7043795620437956, "no_speech_prob": 7.071798336255597e-06}, {"id": 574, "seek": 241156, "start": 2436.64, "end": 2441.4, "text": " But there were quite a few parts of that paper I thought there were some other choices they", "tokens": [583, 456, 645, 1596, 257, 1326, 3166, 295, 300, 3035, 286, 1194, 456, 645, 512, 661, 7994, 436], "temperature": 0.0, "avg_logprob": -0.1528533241965554, "compression_ratio": 1.7043795620437956, "no_speech_prob": 7.071798336255597e-06}, {"id": 575, "seek": 244140, "start": 2441.4, "end": 2442.4, "text": " could have made.", "tokens": [727, 362, 1027, 13], "temperature": 0.0, "avg_logprob": -0.057873860924644804, "compression_ratio": 1.69140625, "no_speech_prob": 3.041570380446501e-06}, {"id": 576, "seek": 244140, "start": 2442.4, "end": 2443.4, "text": " I wonder why they didn't.", "tokens": [286, 2441, 983, 436, 994, 380, 13], "temperature": 0.0, "avg_logprob": -0.057873860924644804, "compression_ratio": 1.69140625, "no_speech_prob": 3.041570380446501e-06}, {"id": 577, "seek": 244140, "start": 2443.4, "end": 2448.6800000000003, "text": " And I tried them out and found out what actually works and what doesn't work as well and found", "tokens": [400, 286, 3031, 552, 484, 293, 1352, 484, 437, 767, 1985, 293, 437, 1177, 380, 589, 382, 731, 293, 1352], "temperature": 0.0, "avg_logprob": -0.057873860924644804, "compression_ratio": 1.69140625, "no_speech_prob": 3.041570380446501e-06}, {"id": 578, "seek": 244140, "start": 2448.6800000000003, "end": 2450.96, "text": " a few little improvements.", "tokens": [257, 1326, 707, 13797, 13], "temperature": 0.0, "avg_logprob": -0.057873860924644804, "compression_ratio": 1.69140625, "no_speech_prob": 3.041570380446501e-06}, {"id": 579, "seek": 244140, "start": 2450.96, "end": 2455.6, "text": " So that's the kind of experiments that you can play around with as well when you try", "tokens": [407, 300, 311, 264, 733, 295, 12050, 300, 291, 393, 862, 926, 365, 382, 731, 562, 291, 853], "temperature": 0.0, "avg_logprob": -0.057873860924644804, "compression_ratio": 1.69140625, "no_speech_prob": 3.041570380446501e-06}, {"id": 580, "seek": 244140, "start": 2455.6, "end": 2460.6, "text": " different models and architectures, different dropouts, layer numbers, number of activations", "tokens": [819, 5245, 293, 6331, 1303, 11, 819, 3270, 7711, 11, 4583, 3547, 11, 1230, 295, 2430, 763], "temperature": 0.0, "avg_logprob": -0.057873860924644804, "compression_ratio": 1.69140625, "no_speech_prob": 3.041570380446501e-06}, {"id": 581, "seek": 244140, "start": 2460.6, "end": 2463.12, "text": " and so forth.", "tokens": [293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.057873860924644804, "compression_ratio": 1.69140625, "no_speech_prob": 3.041570380446501e-06}, {"id": 582, "seek": 244140, "start": 2463.12, "end": 2468.04, "text": " So having created our learner, we can type learn.model to take a look at it.", "tokens": [407, 1419, 2942, 527, 33347, 11, 321, 393, 2010, 1466, 13, 8014, 338, 281, 747, 257, 574, 412, 309, 13], "temperature": 0.0, "avg_logprob": -0.057873860924644804, "compression_ratio": 1.69140625, "no_speech_prob": 3.041570380446501e-06}, {"id": 583, "seek": 246804, "start": 2468.04, "end": 2472.08, "text": " And as you would expect in that there is a whole bunch of embeddings.", "tokens": [400, 382, 291, 576, 2066, 294, 300, 456, 307, 257, 1379, 3840, 295, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.13051202474546827, "compression_ratio": 1.7073170731707317, "no_speech_prob": 4.28922840001178e-06}, {"id": 584, "seek": 246804, "start": 2472.08, "end": 2477.12, "text": " Each of those embedding matrices tells you, well, this is the number of levels of the", "tokens": [6947, 295, 729, 12240, 3584, 32284, 5112, 291, 11, 731, 11, 341, 307, 264, 1230, 295, 4358, 295, 264], "temperature": 0.0, "avg_logprob": -0.13051202474546827, "compression_ratio": 1.7073170731707317, "no_speech_prob": 4.28922840001178e-06}, {"id": 585, "seek": 246804, "start": 2477.12, "end": 2479.48, "text": " input for each input.", "tokens": [4846, 337, 1184, 4846, 13], "temperature": 0.0, "avg_logprob": -0.13051202474546827, "compression_ratio": 1.7073170731707317, "no_speech_prob": 4.28922840001178e-06}, {"id": 586, "seek": 246804, "start": 2479.48, "end": 2484.38, "text": " And you can match these with your list catbars.", "tokens": [400, 291, 393, 2995, 613, 365, 428, 1329, 3857, 42162, 13], "temperature": 0.0, "avg_logprob": -0.13051202474546827, "compression_ratio": 1.7073170731707317, "no_speech_prob": 4.28922840001178e-06}, {"id": 587, "seek": 246804, "start": 2484.38, "end": 2486.24, "text": " So the first one will be store.", "tokens": [407, 264, 700, 472, 486, 312, 3531, 13], "temperature": 0.0, "avg_logprob": -0.13051202474546827, "compression_ratio": 1.7073170731707317, "no_speech_prob": 4.28922840001178e-06}, {"id": 588, "seek": 246804, "start": 2486.24, "end": 2487.24, "text": " So that's not surprising.", "tokens": [407, 300, 311, 406, 8830, 13], "temperature": 0.0, "avg_logprob": -0.13051202474546827, "compression_ratio": 1.7073170731707317, "no_speech_prob": 4.28922840001178e-06}, {"id": 589, "seek": 246804, "start": 2487.24, "end": 2489.88, "text": " There are 1,116 stores.", "tokens": [821, 366, 502, 11, 5348, 21, 9512, 13], "temperature": 0.0, "avg_logprob": -0.13051202474546827, "compression_ratio": 1.7073170731707317, "no_speech_prob": 4.28922840001178e-06}, {"id": 590, "seek": 246804, "start": 2489.88, "end": 2494.12, "text": " And then the second number, of course, is the size of the embedding.", "tokens": [400, 550, 264, 1150, 1230, 11, 295, 1164, 11, 307, 264, 2744, 295, 264, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.13051202474546827, "compression_ratio": 1.7073170731707317, "no_speech_prob": 4.28922840001178e-06}, {"id": 591, "seek": 246804, "start": 2494.12, "end": 2496.52, "text": " And that's a number that you get to choose.", "tokens": [400, 300, 311, 257, 1230, 300, 291, 483, 281, 2826, 13], "temperature": 0.0, "avg_logprob": -0.13051202474546827, "compression_ratio": 1.7073170731707317, "no_speech_prob": 4.28922840001178e-06}, {"id": 592, "seek": 249652, "start": 2496.52, "end": 2504.0, "text": " And so Fast.ai has some defaults, which actually work really, really well nearly all the time.", "tokens": [400, 370, 15968, 13, 1301, 575, 512, 7576, 82, 11, 597, 767, 589, 534, 11, 534, 731, 6217, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.11727721716767998, "compression_ratio": 1.5948275862068966, "no_speech_prob": 3.785295803027111e-06}, {"id": 593, "seek": 249652, "start": 2504.0, "end": 2505.8, "text": " So I almost never change them.", "tokens": [407, 286, 1920, 1128, 1319, 552, 13], "temperature": 0.0, "avg_logprob": -0.11727721716767998, "compression_ratio": 1.5948275862068966, "no_speech_prob": 3.785295803027111e-06}, {"id": 594, "seek": 249652, "start": 2505.8, "end": 2511.56, "text": " But when you create your tabular learner, you can absolutely pass in an embedding size", "tokens": [583, 562, 291, 1884, 428, 4421, 1040, 33347, 11, 291, 393, 3122, 1320, 294, 364, 12240, 3584, 2744], "temperature": 0.0, "avg_logprob": -0.11727721716767998, "compression_ratio": 1.5948275862068966, "no_speech_prob": 3.785295803027111e-06}, {"id": 595, "seek": 249652, "start": 2511.56, "end": 2518.12, "text": " dictionary which maps variable names to embedding sizes for anything where you want to override", "tokens": [25890, 597, 11317, 7006, 5288, 281, 12240, 3584, 11602, 337, 1340, 689, 291, 528, 281, 42321], "temperature": 0.0, "avg_logprob": -0.11727721716767998, "compression_ratio": 1.5948275862068966, "no_speech_prob": 3.785295803027111e-06}, {"id": 596, "seek": 249652, "start": 2518.12, "end": 2522.54, "text": " the defaults.", "tokens": [264, 7576, 82, 13], "temperature": 0.0, "avg_logprob": -0.11727721716767998, "compression_ratio": 1.5948275862068966, "no_speech_prob": 3.785295803027111e-06}, {"id": 597, "seek": 249652, "start": 2522.54, "end": 2525.52, "text": " And then we've got our embedding dropout layer.", "tokens": [400, 550, 321, 600, 658, 527, 12240, 3584, 3270, 346, 4583, 13], "temperature": 0.0, "avg_logprob": -0.11727721716767998, "compression_ratio": 1.5948275862068966, "no_speech_prob": 3.785295803027111e-06}, {"id": 598, "seek": 252552, "start": 2525.52, "end": 2529.8, "text": " And then we've got a batch norm layer with 16 inputs.", "tokens": [400, 550, 321, 600, 658, 257, 15245, 2026, 4583, 365, 3165, 15743, 13], "temperature": 0.0, "avg_logprob": -0.1629543192246381, "compression_ratio": 1.7134831460674158, "no_speech_prob": 1.0677000545911142e-06}, {"id": 599, "seek": 252552, "start": 2529.8, "end": 2538.6, "text": " The 16 inputs make sense because we have 16 continuous variables.", "tokens": [440, 3165, 15743, 652, 2020, 570, 321, 362, 3165, 10957, 9102, 13], "temperature": 0.0, "avg_logprob": -0.1629543192246381, "compression_ratio": 1.7134831460674158, "no_speech_prob": 1.0677000545911142e-06}, {"id": 600, "seek": 252552, "start": 2538.6, "end": 2541.44, "text": " The length of cont names is 16.", "tokens": [440, 4641, 295, 660, 5288, 307, 3165, 13], "temperature": 0.0, "avg_logprob": -0.1629543192246381, "compression_ratio": 1.7134831460674158, "no_speech_prob": 1.0677000545911142e-06}, {"id": 601, "seek": 252552, "start": 2541.44, "end": 2544.52, "text": " So this is something for our continuous variables.", "tokens": [407, 341, 307, 746, 337, 527, 10957, 9102, 13], "temperature": 0.0, "avg_logprob": -0.1629543192246381, "compression_ratio": 1.7134831460674158, "no_speech_prob": 1.0677000545911142e-06}, {"id": 602, "seek": 252552, "start": 2544.52, "end": 2551.28, "text": " And specifically, it's over here, bn.cont on our continuous variables.", "tokens": [400, 4682, 11, 309, 311, 670, 510, 11, 272, 77, 13, 9000, 322, 527, 10957, 9102, 13], "temperature": 0.0, "avg_logprob": -0.1629543192246381, "compression_ratio": 1.7134831460674158, "no_speech_prob": 1.0677000545911142e-06}, {"id": 603, "seek": 252552, "start": 2551.28, "end": 2555.36, "text": " And bn.cont is a batch norm 1D.", "tokens": [400, 272, 77, 13, 9000, 307, 257, 15245, 2026, 502, 35, 13], "temperature": 0.0, "avg_logprob": -0.1629543192246381, "compression_ratio": 1.7134831460674158, "no_speech_prob": 1.0677000545911142e-06}, {"id": 604, "seek": 255536, "start": 2555.36, "end": 2556.36, "text": " What's that?", "tokens": [708, 311, 300, 30], "temperature": 0.0, "avg_logprob": -0.11265913339761588, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.507569312612759e-06}, {"id": 605, "seek": 255536, "start": 2556.36, "end": 2563.1200000000003, "text": " Well, the first short answer is it's one of the things that I experimented with as to", "tokens": [1042, 11, 264, 700, 2099, 1867, 307, 309, 311, 472, 295, 264, 721, 300, 286, 5120, 292, 365, 382, 281], "temperature": 0.0, "avg_logprob": -0.11265913339761588, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.507569312612759e-06}, {"id": 606, "seek": 255536, "start": 2563.1200000000003, "end": 2565.56, "text": " having batch normal not in this.", "tokens": [1419, 15245, 2710, 406, 294, 341, 13], "temperature": 0.0, "avg_logprob": -0.11265913339761588, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.507569312612759e-06}, {"id": 607, "seek": 255536, "start": 2565.56, "end": 2568.04, "text": " And I found that it worked really well.", "tokens": [400, 286, 1352, 300, 309, 2732, 534, 731, 13], "temperature": 0.0, "avg_logprob": -0.11265913339761588, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.507569312612759e-06}, {"id": 608, "seek": 255536, "start": 2568.04, "end": 2573.58, "text": " And then specifically what it is is extremely unclear.", "tokens": [400, 550, 4682, 437, 309, 307, 307, 4664, 25636, 13], "temperature": 0.0, "avg_logprob": -0.11265913339761588, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.507569312612759e-06}, {"id": 609, "seek": 255536, "start": 2573.58, "end": 2574.98, "text": " Let me describe it to you.", "tokens": [961, 385, 6786, 309, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.11265913339761588, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.507569312612759e-06}, {"id": 610, "seek": 255536, "start": 2574.98, "end": 2577.56, "text": " It's kind of a bit of regularization.", "tokens": [467, 311, 733, 295, 257, 857, 295, 3890, 2144, 13], "temperature": 0.0, "avg_logprob": -0.11265913339761588, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.507569312612759e-06}, {"id": 611, "seek": 255536, "start": 2577.56, "end": 2580.46, "text": " It's kind of a bit of training helper.", "tokens": [467, 311, 733, 295, 257, 857, 295, 3097, 36133, 13], "temperature": 0.0, "avg_logprob": -0.11265913339761588, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.507569312612759e-06}, {"id": 612, "seek": 255536, "start": 2580.46, "end": 2584.6200000000003, "text": " It's called batch normalization.", "tokens": [467, 311, 1219, 15245, 2710, 2144, 13], "temperature": 0.0, "avg_logprob": -0.11265913339761588, "compression_ratio": 1.672811059907834, "no_speech_prob": 5.507569312612759e-06}, {"id": 613, "seek": 258462, "start": 2584.62, "end": 2587.0, "text": " And it comes from this paper.", "tokens": [400, 309, 1487, 490, 341, 3035, 13], "temperature": 0.0, "avg_logprob": -0.21690769990285239, "compression_ratio": 1.6055045871559632, "no_speech_prob": 6.540327831316972e-06}, {"id": 614, "seek": 258462, "start": 2587.0, "end": 2591.96, "text": " Actually, before I do this, I just want to mention one other really funny thing.", "tokens": [5135, 11, 949, 286, 360, 341, 11, 286, 445, 528, 281, 2152, 472, 661, 534, 4074, 551, 13], "temperature": 0.0, "avg_logprob": -0.21690769990285239, "compression_ratio": 1.6055045871559632, "no_speech_prob": 6.540327831316972e-06}, {"id": 615, "seek": 258462, "start": 2591.96, "end": 2595.64, "text": " Dropout, I mentioned it was a master's thesis.", "tokens": [17675, 346, 11, 286, 2835, 309, 390, 257, 4505, 311, 22288, 13], "temperature": 0.0, "avg_logprob": -0.21690769990285239, "compression_ratio": 1.6055045871559632, "no_speech_prob": 6.540327831316972e-06}, {"id": 616, "seek": 258462, "start": 2595.64, "end": 2600.72, "text": " Not only was it a master's thesis, one of the most influential papers of the last 10", "tokens": [1726, 787, 390, 309, 257, 4505, 311, 22288, 11, 472, 295, 264, 881, 22215, 10577, 295, 264, 1036, 1266], "temperature": 0.0, "avg_logprob": -0.21690769990285239, "compression_ratio": 1.6055045871559632, "no_speech_prob": 6.540327831316972e-06}, {"id": 617, "seek": 258462, "start": 2600.72, "end": 2605.7599999999998, "text": " years, it was rejected from the main neural nets conference.", "tokens": [924, 11, 309, 390, 15749, 490, 264, 2135, 18161, 36170, 7586, 13], "temperature": 0.0, "avg_logprob": -0.21690769990285239, "compression_ratio": 1.6055045871559632, "no_speech_prob": 6.540327831316972e-06}, {"id": 618, "seek": 258462, "start": 2605.7599999999998, "end": 2608.72, "text": " What was then called NIPS, now called NeurIPS.", "tokens": [708, 390, 550, 1219, 18482, 6273, 11, 586, 1219, 1734, 374, 40, 6273, 13], "temperature": 0.0, "avg_logprob": -0.21690769990285239, "compression_ratio": 1.6055045871559632, "no_speech_prob": 6.540327831316972e-06}, {"id": 619, "seek": 260872, "start": 2608.72, "end": 2618.2799999999997, "text": " I think it's very interesting because it's just a reminder that our academic community", "tokens": [286, 519, 309, 311, 588, 1880, 570, 309, 311, 445, 257, 13548, 300, 527, 7778, 1768], "temperature": 0.0, "avg_logprob": -0.12845247442072089, "compression_ratio": 1.7041666666666666, "no_speech_prob": 7.889072549005505e-06}, {"id": 620, "seek": 260872, "start": 2618.2799999999997, "end": 2626.68, "text": " is generally extremely poor at recognizing which things are going to turn out to be important.", "tokens": [307, 5101, 4664, 4716, 412, 18538, 597, 721, 366, 516, 281, 1261, 484, 281, 312, 1021, 13], "temperature": 0.0, "avg_logprob": -0.12845247442072089, "compression_ratio": 1.7041666666666666, "no_speech_prob": 7.889072549005505e-06}, {"id": 621, "seek": 260872, "start": 2626.68, "end": 2630.7599999999998, "text": " Generally people are looking for stuff that are in the field that they're working on and", "tokens": [21082, 561, 366, 1237, 337, 1507, 300, 366, 294, 264, 2519, 300, 436, 434, 1364, 322, 293], "temperature": 0.0, "avg_logprob": -0.12845247442072089, "compression_ratio": 1.7041666666666666, "no_speech_prob": 7.889072549005505e-06}, {"id": 622, "seek": 260872, "start": 2630.7599999999998, "end": 2631.7599999999998, "text": " understand.", "tokens": [1223, 13], "temperature": 0.0, "avg_logprob": -0.12845247442072089, "compression_ratio": 1.7041666666666666, "no_speech_prob": 7.889072549005505e-06}, {"id": 623, "seek": 260872, "start": 2631.7599999999998, "end": 2633.3599999999997, "text": " So Dropout kind of came out of left field.", "tokens": [407, 17675, 346, 733, 295, 1361, 484, 295, 1411, 2519, 13], "temperature": 0.0, "avg_logprob": -0.12845247442072089, "compression_ratio": 1.7041666666666666, "no_speech_prob": 7.889072549005505e-06}, {"id": 624, "seek": 260872, "start": 2633.3599999999997, "end": 2636.3999999999996, "text": " It's kind of hard to understand what's going on.", "tokens": [467, 311, 733, 295, 1152, 281, 1223, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.12845247442072089, "compression_ratio": 1.7041666666666666, "no_speech_prob": 7.889072549005505e-06}, {"id": 625, "seek": 260872, "start": 2636.3999999999996, "end": 2638.04, "text": " And so that's kind of interesting.", "tokens": [400, 370, 300, 311, 733, 295, 1880, 13], "temperature": 0.0, "avg_logprob": -0.12845247442072089, "compression_ratio": 1.7041666666666666, "no_speech_prob": 7.889072549005505e-06}, {"id": 626, "seek": 263804, "start": 2638.04, "end": 2646.44, "text": " And so it's a reminder that if you just follow, as you kind of develop beyond being just a", "tokens": [400, 370, 309, 311, 257, 13548, 300, 498, 291, 445, 1524, 11, 382, 291, 733, 295, 1499, 4399, 885, 445, 257], "temperature": 0.0, "avg_logprob": -0.15119747320810953, "compression_ratio": 1.7407407407407407, "no_speech_prob": 6.854108960396843e-06}, {"id": 627, "seek": 263804, "start": 2646.44, "end": 2651.7799999999997, "text": " practitioner into actually doing your own research, don't just focus on the stuff everybody's", "tokens": [32125, 666, 767, 884, 428, 1065, 2132, 11, 500, 380, 445, 1879, 322, 264, 1507, 2201, 311], "temperature": 0.0, "avg_logprob": -0.15119747320810953, "compression_ratio": 1.7407407407407407, "no_speech_prob": 6.854108960396843e-06}, {"id": 628, "seek": 263804, "start": 2651.7799999999997, "end": 2653.02, "text": " talking about.", "tokens": [1417, 466, 13], "temperature": 0.0, "avg_logprob": -0.15119747320810953, "compression_ratio": 1.7407407407407407, "no_speech_prob": 6.854108960396843e-06}, {"id": 629, "seek": 263804, "start": 2653.02, "end": 2655.04, "text": " Focus on the stuff you think might be interesting.", "tokens": [21862, 322, 264, 1507, 291, 519, 1062, 312, 1880, 13], "temperature": 0.0, "avg_logprob": -0.15119747320810953, "compression_ratio": 1.7407407407407407, "no_speech_prob": 6.854108960396843e-06}, {"id": 630, "seek": 263804, "start": 2655.04, "end": 2659.8, "text": " Because the stuff everybody's talking about generally turns out not to be very interesting.", "tokens": [1436, 264, 1507, 2201, 311, 1417, 466, 5101, 4523, 484, 406, 281, 312, 588, 1880, 13], "temperature": 0.0, "avg_logprob": -0.15119747320810953, "compression_ratio": 1.7407407407407407, "no_speech_prob": 6.854108960396843e-06}, {"id": 631, "seek": 263804, "start": 2659.8, "end": 2667.8, "text": " The community is very poor at recognizing high impact papers when they come out.", "tokens": [440, 1768, 307, 588, 4716, 412, 18538, 1090, 2712, 10577, 562, 436, 808, 484, 13], "temperature": 0.0, "avg_logprob": -0.15119747320810953, "compression_ratio": 1.7407407407407407, "no_speech_prob": 6.854108960396843e-06}, {"id": 632, "seek": 266780, "start": 2667.8, "end": 2673.52, "text": " Batch normalization, on the other hand, was immediately recognized as high impact.", "tokens": [363, 852, 2710, 2144, 11, 322, 264, 661, 1011, 11, 390, 4258, 9823, 382, 1090, 2712, 13], "temperature": 0.0, "avg_logprob": -0.11789314686751165, "compression_ratio": 1.5850340136054422, "no_speech_prob": 1.9525376046658494e-05}, {"id": 633, "seek": 266780, "start": 2673.52, "end": 2677.36, "text": " I definitely remember everybody talking about it in 2015 when it came out.", "tokens": [286, 2138, 1604, 2201, 1417, 466, 309, 294, 7546, 562, 309, 1361, 484, 13], "temperature": 0.0, "avg_logprob": -0.11789314686751165, "compression_ratio": 1.5850340136054422, "no_speech_prob": 1.9525376046658494e-05}, {"id": 634, "seek": 266780, "start": 2677.36, "end": 2679.1600000000003, "text": " And that was because it was so obvious.", "tokens": [400, 300, 390, 570, 309, 390, 370, 6322, 13], "temperature": 0.0, "avg_logprob": -0.11789314686751165, "compression_ratio": 1.5850340136054422, "no_speech_prob": 1.9525376046658494e-05}, {"id": 635, "seek": 266780, "start": 2679.1600000000003, "end": 2685.96, "text": " They showed this picture showing the current then state-of-the-art ImageNet model inception.", "tokens": [814, 4712, 341, 3036, 4099, 264, 2190, 550, 1785, 12, 2670, 12, 3322, 12, 446, 29903, 31890, 2316, 49834, 13], "temperature": 0.0, "avg_logprob": -0.11789314686751165, "compression_ratio": 1.5850340136054422, "no_speech_prob": 1.9525376046658494e-05}, {"id": 636, "seek": 266780, "start": 2685.96, "end": 2691.1400000000003, "text": " This is how long it took them to get a pretty good result.", "tokens": [639, 307, 577, 938, 309, 1890, 552, 281, 483, 257, 1238, 665, 1874, 13], "temperature": 0.0, "avg_logprob": -0.11789314686751165, "compression_ratio": 1.5850340136054422, "no_speech_prob": 1.9525376046658494e-05}, {"id": 637, "seek": 266780, "start": 2691.1400000000003, "end": 2694.84, "text": " And then they tried the same thing with this new thing called BatchNorm, and they just", "tokens": [400, 550, 436, 3031, 264, 912, 551, 365, 341, 777, 551, 1219, 363, 852, 45, 687, 11, 293, 436, 445], "temperature": 0.0, "avg_logprob": -0.11789314686751165, "compression_ratio": 1.5850340136054422, "no_speech_prob": 1.9525376046658494e-05}, {"id": 638, "seek": 266780, "start": 2694.84, "end": 2697.4, "text": " did it way, way, way quickly.", "tokens": [630, 309, 636, 11, 636, 11, 636, 2661, 13], "temperature": 0.0, "avg_logprob": -0.11789314686751165, "compression_ratio": 1.5850340136054422, "no_speech_prob": 1.9525376046658494e-05}, {"id": 639, "seek": 269740, "start": 2697.4, "end": 2701.96, "text": " And so that was enough for pretty much everybody to go, wow, this is interesting.", "tokens": [400, 370, 300, 390, 1547, 337, 1238, 709, 2201, 281, 352, 11, 6076, 11, 341, 307, 1880, 13], "temperature": 0.0, "avg_logprob": -0.10368291466636995, "compression_ratio": 1.6309963099630995, "no_speech_prob": 1.8162169226343394e-06}, {"id": 640, "seek": 269740, "start": 2701.96, "end": 2705.96, "text": " And specifically, they said this thing's called BatchNormalization, and it's accelerating", "tokens": [400, 4682, 11, 436, 848, 341, 551, 311, 1219, 363, 852, 45, 24440, 2144, 11, 293, 309, 311, 34391], "temperature": 0.0, "avg_logprob": -0.10368291466636995, "compression_ratio": 1.6309963099630995, "no_speech_prob": 1.8162169226343394e-06}, {"id": 641, "seek": 269740, "start": 2705.96, "end": 2709.96, "text": " training by reducing internal covariate shift.", "tokens": [3097, 538, 12245, 6920, 49851, 473, 5513, 13], "temperature": 0.0, "avg_logprob": -0.10368291466636995, "compression_ratio": 1.6309963099630995, "no_speech_prob": 1.8162169226343394e-06}, {"id": 642, "seek": 269740, "start": 2709.96, "end": 2712.4, "text": " So what is internal covariate shift?", "tokens": [407, 437, 307, 6920, 49851, 473, 5513, 30], "temperature": 0.0, "avg_logprob": -0.10368291466636995, "compression_ratio": 1.6309963099630995, "no_speech_prob": 1.8162169226343394e-06}, {"id": 643, "seek": 269740, "start": 2712.4, "end": 2714.48, "text": " Well, it doesn't matter.", "tokens": [1042, 11, 309, 1177, 380, 1871, 13], "temperature": 0.0, "avg_logprob": -0.10368291466636995, "compression_ratio": 1.6309963099630995, "no_speech_prob": 1.8162169226343394e-06}, {"id": 644, "seek": 269740, "start": 2714.48, "end": 2719.36, "text": " Because this is one of those things where researchers came up with some intuition and", "tokens": [1436, 341, 307, 472, 295, 729, 721, 689, 10309, 1361, 493, 365, 512, 24002, 293], "temperature": 0.0, "avg_logprob": -0.10368291466636995, "compression_ratio": 1.6309963099630995, "no_speech_prob": 1.8162169226343394e-06}, {"id": 645, "seek": 269740, "start": 2719.36, "end": 2721.56, "text": " some idea about this thing they wanted to try.", "tokens": [512, 1558, 466, 341, 551, 436, 1415, 281, 853, 13], "temperature": 0.0, "avg_logprob": -0.10368291466636995, "compression_ratio": 1.6309963099630995, "no_speech_prob": 1.8162169226343394e-06}, {"id": 646, "seek": 269740, "start": 2721.56, "end": 2722.56, "text": " They did it.", "tokens": [814, 630, 309, 13], "temperature": 0.0, "avg_logprob": -0.10368291466636995, "compression_ratio": 1.6309963099630995, "no_speech_prob": 1.8162169226343394e-06}, {"id": 647, "seek": 269740, "start": 2722.56, "end": 2723.56, "text": " It worked well.", "tokens": [467, 2732, 731, 13], "temperature": 0.0, "avg_logprob": -0.10368291466636995, "compression_ratio": 1.6309963099630995, "no_speech_prob": 1.8162169226343394e-06}, {"id": 648, "seek": 272356, "start": 2723.56, "end": 2727.92, "text": " The post-hoc added on some mathematical analysis to try and claim why it worked, and it turned", "tokens": [440, 2183, 12, 71, 905, 3869, 322, 512, 18894, 5215, 281, 853, 293, 3932, 983, 309, 2732, 11, 293, 309, 3574], "temperature": 0.0, "avg_logprob": -0.12728848348137076, "compression_ratio": 1.7195945945945945, "no_speech_prob": 4.785070359503152e-06}, {"id": 649, "seek": 272356, "start": 2727.92, "end": 2730.44, "text": " out they were totally wrong.", "tokens": [484, 436, 645, 3879, 2085, 13], "temperature": 0.0, "avg_logprob": -0.12728848348137076, "compression_ratio": 1.7195945945945945, "no_speech_prob": 4.785070359503152e-06}, {"id": 650, "seek": 272356, "start": 2730.44, "end": 2734.52, "text": " In the last two months, there's been two papers, so it took three years for people to really", "tokens": [682, 264, 1036, 732, 2493, 11, 456, 311, 668, 732, 10577, 11, 370, 309, 1890, 1045, 924, 337, 561, 281, 534], "temperature": 0.0, "avg_logprob": -0.12728848348137076, "compression_ratio": 1.7195945945945945, "no_speech_prob": 4.785070359503152e-06}, {"id": 651, "seek": 272356, "start": 2734.52, "end": 2738.92, "text": " figure this out, and the last two months there's been two papers that have shown BatchNormalization", "tokens": [2573, 341, 484, 11, 293, 264, 1036, 732, 2493, 456, 311, 668, 732, 10577, 300, 362, 4898, 363, 852, 45, 24440, 2144], "temperature": 0.0, "avg_logprob": -0.12728848348137076, "compression_ratio": 1.7195945945945945, "no_speech_prob": 4.785070359503152e-06}, {"id": 652, "seek": 272356, "start": 2738.92, "end": 2741.62, "text": " doesn't reduce covariate shift at all.", "tokens": [1177, 380, 5407, 49851, 473, 5513, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.12728848348137076, "compression_ratio": 1.7195945945945945, "no_speech_prob": 4.785070359503152e-06}, {"id": 653, "seek": 272356, "start": 2741.62, "end": 2748.0, "text": " And even if it did, that has nothing to do with why it works.", "tokens": [400, 754, 498, 309, 630, 11, 300, 575, 1825, 281, 360, 365, 983, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.12728848348137076, "compression_ratio": 1.7195945945945945, "no_speech_prob": 4.785070359503152e-06}, {"id": 654, "seek": 272356, "start": 2748.0, "end": 2752.4, "text": " So I think that's kind of an interesting insight, again, which is why we should be focusing", "tokens": [407, 286, 519, 300, 311, 733, 295, 364, 1880, 11269, 11, 797, 11, 597, 307, 983, 321, 820, 312, 8416], "temperature": 0.0, "avg_logprob": -0.12728848348137076, "compression_ratio": 1.7195945945945945, "no_speech_prob": 4.785070359503152e-06}, {"id": 655, "seek": 275240, "start": 2752.4, "end": 2758.08, "text": " on being practitioners and experimentalists and developing an intuition.", "tokens": [322, 885, 25742, 293, 17069, 1751, 293, 6416, 364, 24002, 13], "temperature": 0.0, "avg_logprob": -0.12260729532975417, "compression_ratio": 1.87, "no_speech_prob": 4.35684069088893e-06}, {"id": 656, "seek": 275240, "start": 2758.08, "end": 2763.28, "text": " What BatchNorm does is what you see in this picture here, in this paper.", "tokens": [708, 363, 852, 45, 687, 775, 307, 437, 291, 536, 294, 341, 3036, 510, 11, 294, 341, 3035, 13], "temperature": 0.0, "avg_logprob": -0.12260729532975417, "compression_ratio": 1.87, "no_speech_prob": 4.35684069088893e-06}, {"id": 657, "seek": 275240, "start": 2763.28, "end": 2768.12, "text": " Here are steps or batches, and here is loss.", "tokens": [1692, 366, 4439, 420, 15245, 279, 11, 293, 510, 307, 4470, 13], "temperature": 0.0, "avg_logprob": -0.12260729532975417, "compression_ratio": 1.87, "no_speech_prob": 4.35684069088893e-06}, {"id": 658, "seek": 275240, "start": 2768.12, "end": 2772.32, "text": " And here, the red line is what happens when you train without BatchNorm.", "tokens": [400, 510, 11, 264, 2182, 1622, 307, 437, 2314, 562, 291, 3847, 1553, 363, 852, 45, 687, 13], "temperature": 0.0, "avg_logprob": -0.12260729532975417, "compression_ratio": 1.87, "no_speech_prob": 4.35684069088893e-06}, {"id": 659, "seek": 275240, "start": 2772.32, "end": 2774.04, "text": " Very, very bumpy.", "tokens": [4372, 11, 588, 49400, 13], "temperature": 0.0, "avg_logprob": -0.12260729532975417, "compression_ratio": 1.87, "no_speech_prob": 4.35684069088893e-06}, {"id": 660, "seek": 275240, "start": 2774.04, "end": 2777.28, "text": " And here the blue line is what happens when you train with BatchNorm.", "tokens": [400, 510, 264, 3344, 1622, 307, 437, 2314, 562, 291, 3847, 365, 363, 852, 45, 687, 13], "temperature": 0.0, "avg_logprob": -0.12260729532975417, "compression_ratio": 1.87, "no_speech_prob": 4.35684069088893e-06}, {"id": 661, "seek": 275240, "start": 2777.28, "end": 2779.76, "text": " Not very bumpy at all.", "tokens": [1726, 588, 49400, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.12260729532975417, "compression_ratio": 1.87, "no_speech_prob": 4.35684069088893e-06}, {"id": 662, "seek": 277976, "start": 2779.76, "end": 2786.84, "text": " What that means is you can increase your learning rate with BatchNorm, because these big bumps", "tokens": [708, 300, 1355, 307, 291, 393, 3488, 428, 2539, 3314, 365, 363, 852, 45, 687, 11, 570, 613, 955, 27719], "temperature": 0.0, "avg_logprob": -0.07800987310576857, "compression_ratio": 1.7028112449799198, "no_speech_prob": 3.237748614992597e-06}, {"id": 663, "seek": 277976, "start": 2786.84, "end": 2791.86, "text": " represent times that you're really at risk of your set of weights jumping off into some", "tokens": [2906, 1413, 300, 291, 434, 534, 412, 3148, 295, 428, 992, 295, 17443, 11233, 766, 666, 512], "temperature": 0.0, "avg_logprob": -0.07800987310576857, "compression_ratio": 1.7028112449799198, "no_speech_prob": 3.237748614992597e-06}, {"id": 664, "seek": 277976, "start": 2791.86, "end": 2795.1000000000004, "text": " awful part of the weight space that it can never get out of again.", "tokens": [11232, 644, 295, 264, 3364, 1901, 300, 309, 393, 1128, 483, 484, 295, 797, 13], "temperature": 0.0, "avg_logprob": -0.07800987310576857, "compression_ratio": 1.7028112449799198, "no_speech_prob": 3.237748614992597e-06}, {"id": 665, "seek": 277976, "start": 2795.1000000000004, "end": 2799.0800000000004, "text": " So if it's less bumpy, then you can train at a higher learning rate.", "tokens": [407, 498, 309, 311, 1570, 49400, 11, 550, 291, 393, 3847, 412, 257, 2946, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.07800987310576857, "compression_ratio": 1.7028112449799198, "no_speech_prob": 3.237748614992597e-06}, {"id": 666, "seek": 277976, "start": 2799.0800000000004, "end": 2802.0600000000004, "text": " So that's actually what's going on.", "tokens": [407, 300, 311, 767, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.07800987310576857, "compression_ratio": 1.7028112449799198, "no_speech_prob": 3.237748614992597e-06}, {"id": 667, "seek": 277976, "start": 2802.0600000000004, "end": 2803.1200000000003, "text": " And here's what it is.", "tokens": [400, 510, 311, 437, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.07800987310576857, "compression_ratio": 1.7028112449799198, "no_speech_prob": 3.237748614992597e-06}, {"id": 668, "seek": 277976, "start": 2803.1200000000003, "end": 2804.1200000000003, "text": " This is the algorithm.", "tokens": [639, 307, 264, 9284, 13], "temperature": 0.0, "avg_logprob": -0.07800987310576857, "compression_ratio": 1.7028112449799198, "no_speech_prob": 3.237748614992597e-06}, {"id": 669, "seek": 277976, "start": 2804.1200000000003, "end": 2806.2400000000002, "text": " And it's really simple.", "tokens": [400, 309, 311, 534, 2199, 13], "temperature": 0.0, "avg_logprob": -0.07800987310576857, "compression_ratio": 1.7028112449799198, "no_speech_prob": 3.237748614992597e-06}, {"id": 670, "seek": 280624, "start": 2806.24, "end": 2810.2, "text": " The algorithm is going to take a mini-batch.", "tokens": [440, 9284, 307, 516, 281, 747, 257, 8382, 12, 65, 852, 13], "temperature": 0.0, "avg_logprob": -0.13065525716986537, "compression_ratio": 1.8181818181818181, "no_speech_prob": 8.530237209924962e-06}, {"id": 671, "seek": 280624, "start": 2810.2, "end": 2811.9199999999996, "text": " So we have a mini-batch.", "tokens": [407, 321, 362, 257, 8382, 12, 65, 852, 13], "temperature": 0.0, "avg_logprob": -0.13065525716986537, "compression_ratio": 1.8181818181818181, "no_speech_prob": 8.530237209924962e-06}, {"id": 672, "seek": 280624, "start": 2811.9199999999996, "end": 2818.4799999999996, "text": " And remember, this is a layer, so the thing coming into it is activations.", "tokens": [400, 1604, 11, 341, 307, 257, 4583, 11, 370, 264, 551, 1348, 666, 309, 307, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.13065525716986537, "compression_ratio": 1.8181818181818181, "no_speech_prob": 8.530237209924962e-06}, {"id": 673, "seek": 280624, "start": 2818.4799999999996, "end": 2822.4799999999996, "text": " So it's a layer, and it's going to take in some activations.", "tokens": [407, 309, 311, 257, 4583, 11, 293, 309, 311, 516, 281, 747, 294, 512, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.13065525716986537, "compression_ratio": 1.8181818181818181, "no_speech_prob": 8.530237209924962e-06}, {"id": 674, "seek": 280624, "start": 2822.4799999999996, "end": 2826.8799999999997, "text": " And so the activations, it's calling x1, x2, x3, and so forth.", "tokens": [400, 370, 264, 2430, 763, 11, 309, 311, 5141, 2031, 16, 11, 2031, 17, 11, 2031, 18, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.13065525716986537, "compression_ratio": 1.8181818181818181, "no_speech_prob": 8.530237209924962e-06}, {"id": 675, "seek": 280624, "start": 2826.8799999999997, "end": 2830.06, "text": " The first thing we do is we find the mean of those activations.", "tokens": [440, 700, 551, 321, 360, 307, 321, 915, 264, 914, 295, 729, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.13065525716986537, "compression_ratio": 1.8181818181818181, "no_speech_prob": 8.530237209924962e-06}, {"id": 676, "seek": 280624, "start": 2830.06, "end": 2831.8399999999997, "text": " Sum divided by the count, that's just the mean.", "tokens": [8626, 6666, 538, 264, 1207, 11, 300, 311, 445, 264, 914, 13], "temperature": 0.0, "avg_logprob": -0.13065525716986537, "compression_ratio": 1.8181818181818181, "no_speech_prob": 8.530237209924962e-06}, {"id": 677, "seek": 283184, "start": 2831.84, "end": 2836.52, "text": " And the second thing we do is we find the variance of those activations.", "tokens": [400, 264, 1150, 551, 321, 360, 307, 321, 915, 264, 21977, 295, 729, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.13771824743233474, "compression_ratio": 1.7767441860465116, "no_speech_prob": 4.495138000493171e-06}, {"id": 678, "seek": 283184, "start": 2836.52, "end": 2838.92, "text": " Different squared divided by the mean is the variance.", "tokens": [20825, 8889, 6666, 538, 264, 914, 307, 264, 21977, 13], "temperature": 0.0, "avg_logprob": -0.13771824743233474, "compression_ratio": 1.7767441860465116, "no_speech_prob": 4.495138000493171e-06}, {"id": 679, "seek": 283184, "start": 2838.92, "end": 2840.2000000000003, "text": " And then we normalize.", "tokens": [400, 550, 321, 2710, 1125, 13], "temperature": 0.0, "avg_logprob": -0.13771824743233474, "compression_ratio": 1.7767441860465116, "no_speech_prob": 4.495138000493171e-06}, {"id": 680, "seek": 283184, "start": 2840.2000000000003, "end": 2846.6000000000004, "text": " So the values minus the mean divided by the deviation is the normalized version.", "tokens": [407, 264, 4190, 3175, 264, 914, 6666, 538, 264, 25163, 307, 264, 48704, 3037, 13], "temperature": 0.0, "avg_logprob": -0.13771824743233474, "compression_ratio": 1.7767441860465116, "no_speech_prob": 4.495138000493171e-06}, {"id": 681, "seek": 283184, "start": 2846.6000000000004, "end": 2850.08, "text": " OK, it turns out that bit's actually not that important.", "tokens": [2264, 11, 309, 4523, 484, 300, 857, 311, 767, 406, 300, 1021, 13], "temperature": 0.0, "avg_logprob": -0.13771824743233474, "compression_ratio": 1.7767441860465116, "no_speech_prob": 4.495138000493171e-06}, {"id": 682, "seek": 283184, "start": 2850.08, "end": 2851.08, "text": " We used to think it was.", "tokens": [492, 1143, 281, 519, 309, 390, 13], "temperature": 0.0, "avg_logprob": -0.13771824743233474, "compression_ratio": 1.7767441860465116, "no_speech_prob": 4.495138000493171e-06}, {"id": 683, "seek": 283184, "start": 2851.08, "end": 2852.44, "text": " But it turns out it's not.", "tokens": [583, 309, 4523, 484, 309, 311, 406, 13], "temperature": 0.0, "avg_logprob": -0.13771824743233474, "compression_ratio": 1.7767441860465116, "no_speech_prob": 4.495138000493171e-06}, {"id": 684, "seek": 283184, "start": 2852.44, "end": 2855.08, "text": " The really important bit is the next bit.", "tokens": [440, 534, 1021, 857, 307, 264, 958, 857, 13], "temperature": 0.0, "avg_logprob": -0.13771824743233474, "compression_ratio": 1.7767441860465116, "no_speech_prob": 4.495138000493171e-06}, {"id": 685, "seek": 285508, "start": 2855.08, "end": 2861.88, "text": " We take those values and we add a vector of biases.", "tokens": [492, 747, 729, 4190, 293, 321, 909, 257, 8062, 295, 32152, 13], "temperature": 0.0, "avg_logprob": -0.1325517453645405, "compression_ratio": 1.7304347826086957, "no_speech_prob": 3.2377481602452463e-06}, {"id": 686, "seek": 285508, "start": 2861.88, "end": 2862.88, "text": " They call it beta here.", "tokens": [814, 818, 309, 9861, 510, 13], "temperature": 0.0, "avg_logprob": -0.1325517453645405, "compression_ratio": 1.7304347826086957, "no_speech_prob": 3.2377481602452463e-06}, {"id": 687, "seek": 285508, "start": 2862.88, "end": 2864.04, "text": " And we've seen that before.", "tokens": [400, 321, 600, 1612, 300, 949, 13], "temperature": 0.0, "avg_logprob": -0.1325517453645405, "compression_ratio": 1.7304347826086957, "no_speech_prob": 3.2377481602452463e-06}, {"id": 688, "seek": 285508, "start": 2864.04, "end": 2865.36, "text": " We've used a bias term before.", "tokens": [492, 600, 1143, 257, 12577, 1433, 949, 13], "temperature": 0.0, "avg_logprob": -0.1325517453645405, "compression_ratio": 1.7304347826086957, "no_speech_prob": 3.2377481602452463e-06}, {"id": 689, "seek": 285508, "start": 2865.36, "end": 2869.5, "text": " OK, so we're just going to add a bias term as per usual.", "tokens": [2264, 11, 370, 321, 434, 445, 516, 281, 909, 257, 12577, 1433, 382, 680, 7713, 13], "temperature": 0.0, "avg_logprob": -0.1325517453645405, "compression_ratio": 1.7304347826086957, "no_speech_prob": 3.2377481602452463e-06}, {"id": 690, "seek": 285508, "start": 2869.5, "end": 2873.08, "text": " And then we're going to use another thing that's a lot like a bias term.", "tokens": [400, 550, 321, 434, 516, 281, 764, 1071, 551, 300, 311, 257, 688, 411, 257, 12577, 1433, 13], "temperature": 0.0, "avg_logprob": -0.1325517453645405, "compression_ratio": 1.7304347826086957, "no_speech_prob": 3.2377481602452463e-06}, {"id": 691, "seek": 285508, "start": 2873.08, "end": 2876.48, "text": " But rather than adding it, we're going to multiply by it.", "tokens": [583, 2831, 813, 5127, 309, 11, 321, 434, 516, 281, 12972, 538, 309, 13], "temperature": 0.0, "avg_logprob": -0.1325517453645405, "compression_ratio": 1.7304347826086957, "no_speech_prob": 3.2377481602452463e-06}, {"id": 692, "seek": 285508, "start": 2876.48, "end": 2881.88, "text": " So there's these parameters gamma and beta, which are learnable parameters.", "tokens": [407, 456, 311, 613, 9834, 15546, 293, 9861, 11, 597, 366, 1466, 712, 9834, 13], "temperature": 0.0, "avg_logprob": -0.1325517453645405, "compression_ratio": 1.7304347826086957, "no_speech_prob": 3.2377481602452463e-06}, {"id": 693, "seek": 288188, "start": 2881.88, "end": 2886.6, "text": " There's only two kinds of number, activations and parameters.", "tokens": [821, 311, 787, 732, 3685, 295, 1230, 11, 2430, 763, 293, 9834, 13], "temperature": 0.0, "avg_logprob": -0.15791764660416363, "compression_ratio": 1.6592920353982301, "no_speech_prob": 4.425460701895645e-06}, {"id": 694, "seek": 288188, "start": 2886.6, "end": 2887.6, "text": " These are parameters.", "tokens": [1981, 366, 9834, 13], "temperature": 0.0, "avg_logprob": -0.15791764660416363, "compression_ratio": 1.6592920353982301, "no_speech_prob": 4.425460701895645e-06}, {"id": 695, "seek": 288188, "start": 2887.6, "end": 2891.54, "text": " OK, they're things that are learned with gradient descent.", "tokens": [2264, 11, 436, 434, 721, 300, 366, 3264, 365, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.15791764660416363, "compression_ratio": 1.6592920353982301, "no_speech_prob": 4.425460701895645e-06}, {"id": 696, "seek": 288188, "start": 2891.54, "end": 2894.94, "text": " This is just a normal bias layer, beta.", "tokens": [639, 307, 445, 257, 2710, 12577, 4583, 11, 9861, 13], "temperature": 0.0, "avg_logprob": -0.15791764660416363, "compression_ratio": 1.6592920353982301, "no_speech_prob": 4.425460701895645e-06}, {"id": 697, "seek": 288188, "start": 2894.94, "end": 2897.04, "text": " And this is a multiplicative bias layer.", "tokens": [400, 341, 307, 257, 17596, 1166, 12577, 4583, 13], "temperature": 0.0, "avg_logprob": -0.15791764660416363, "compression_ratio": 1.6592920353982301, "no_speech_prob": 4.425460701895645e-06}, {"id": 698, "seek": 288188, "start": 2897.04, "end": 2899.2400000000002, "text": " Nobody calls it that, but that's all it is.", "tokens": [9297, 5498, 309, 300, 11, 457, 300, 311, 439, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.15791764660416363, "compression_ratio": 1.6592920353982301, "no_speech_prob": 4.425460701895645e-06}, {"id": 699, "seek": 288188, "start": 2899.2400000000002, "end": 2903.28, "text": " It's just like bias, but we multiply rather than add.", "tokens": [467, 311, 445, 411, 12577, 11, 457, 321, 12972, 2831, 813, 909, 13], "temperature": 0.0, "avg_logprob": -0.15791764660416363, "compression_ratio": 1.6592920353982301, "no_speech_prob": 4.425460701895645e-06}, {"id": 700, "seek": 288188, "start": 2903.28, "end": 2905.32, "text": " That's all batch norm is.", "tokens": [663, 311, 439, 15245, 2026, 307, 13], "temperature": 0.0, "avg_logprob": -0.15791764660416363, "compression_ratio": 1.6592920353982301, "no_speech_prob": 4.425460701895645e-06}, {"id": 701, "seek": 288188, "start": 2905.32, "end": 2907.44, "text": " That's what the layer does.", "tokens": [663, 311, 437, 264, 4583, 775, 13], "temperature": 0.0, "avg_logprob": -0.15791764660416363, "compression_ratio": 1.6592920353982301, "no_speech_prob": 4.425460701895645e-06}, {"id": 702, "seek": 290744, "start": 2907.44, "end": 2913.96, "text": " So why is that able to achieve this fantastic result?", "tokens": [407, 983, 307, 300, 1075, 281, 4584, 341, 5456, 1874, 30], "temperature": 0.0, "avg_logprob": -0.08023464295171923, "compression_ratio": 1.3757225433526012, "no_speech_prob": 2.684178753042943e-06}, {"id": 703, "seek": 290744, "start": 2913.96, "end": 2920.4, "text": " I'm not sure anybody has exactly written this down before.", "tokens": [286, 478, 406, 988, 4472, 575, 2293, 3720, 341, 760, 949, 13], "temperature": 0.0, "avg_logprob": -0.08023464295171923, "compression_ratio": 1.3757225433526012, "no_speech_prob": 2.684178753042943e-06}, {"id": 704, "seek": 290744, "start": 2920.4, "end": 2924.64, "text": " If they have, I apologize for failing to cite it because I haven't seen it.", "tokens": [759, 436, 362, 11, 286, 12328, 337, 18223, 281, 37771, 309, 570, 286, 2378, 380, 1612, 309, 13], "temperature": 0.0, "avg_logprob": -0.08023464295171923, "compression_ratio": 1.3757225433526012, "no_speech_prob": 2.684178753042943e-06}, {"id": 705, "seek": 290744, "start": 2924.64, "end": 2928.36, "text": " But let me explain what's actually going on here.", "tokens": [583, 718, 385, 2903, 437, 311, 767, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.08023464295171923, "compression_ratio": 1.3757225433526012, "no_speech_prob": 2.684178753042943e-06}, {"id": 706, "seek": 292836, "start": 2928.36, "end": 2944.4, "text": " The value of our predictions y hat is some function of our various weights.", "tokens": [440, 2158, 295, 527, 21264, 288, 2385, 307, 512, 2445, 295, 527, 3683, 17443, 13], "temperature": 0.0, "avg_logprob": -0.12583233328426585, "compression_ratio": 1.446969696969697, "no_speech_prob": 5.4221454774960876e-06}, {"id": 707, "seek": 292836, "start": 2944.4, "end": 2948.2400000000002, "text": " There could be millions of them, weight 1 million.", "tokens": [821, 727, 312, 6803, 295, 552, 11, 3364, 502, 2459, 13], "temperature": 0.0, "avg_logprob": -0.12583233328426585, "compression_ratio": 1.446969696969697, "no_speech_prob": 5.4221454774960876e-06}, {"id": 708, "seek": 292836, "start": 2948.2400000000002, "end": 2952.7200000000003, "text": " And it's also a function, of course, of the inputs to our layer.", "tokens": [400, 309, 311, 611, 257, 2445, 11, 295, 1164, 11, 295, 264, 15743, 281, 527, 4583, 13], "temperature": 0.0, "avg_logprob": -0.12583233328426585, "compression_ratio": 1.446969696969697, "no_speech_prob": 5.4221454774960876e-06}, {"id": 709, "seek": 295272, "start": 2952.72, "end": 2958.3599999999997, "text": " This function here is our neural net function, whatever is going on in our neural net.", "tokens": [639, 2445, 510, 307, 527, 18161, 2533, 2445, 11, 2035, 307, 516, 322, 294, 527, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.13892571767171225, "compression_ratio": 1.5885714285714285, "no_speech_prob": 4.860414264840074e-06}, {"id": 710, "seek": 295272, "start": 2958.3599999999997, "end": 2964.3599999999997, "text": " And then our loss, let's say it's mean squared error, is just our actuals minus our predicted", "tokens": [400, 550, 527, 4470, 11, 718, 311, 584, 309, 311, 914, 8889, 6713, 11, 307, 445, 527, 3539, 82, 3175, 527, 19147], "temperature": 0.0, "avg_logprob": -0.13892571767171225, "compression_ratio": 1.5885714285714285, "no_speech_prob": 4.860414264840074e-06}, {"id": 711, "seek": 295272, "start": 2964.3599999999997, "end": 2970.1, "text": " squared.", "tokens": [8889, 13], "temperature": 0.0, "avg_logprob": -0.13892571767171225, "compression_ratio": 1.5885714285714285, "no_speech_prob": 4.860414264840074e-06}, {"id": 712, "seek": 295272, "start": 2970.1, "end": 2975.72, "text": " So let's say we're trying to predict movie review outcomes, and they're between 1 and", "tokens": [407, 718, 311, 584, 321, 434, 1382, 281, 6069, 3169, 3131, 10070, 11, 293, 436, 434, 1296, 502, 293], "temperature": 0.0, "avg_logprob": -0.13892571767171225, "compression_ratio": 1.5885714285714285, "no_speech_prob": 4.860414264840074e-06}, {"id": 713, "seek": 295272, "start": 2975.72, "end": 2979.74, "text": " 5.", "tokens": [1025, 13], "temperature": 0.0, "avg_logprob": -0.13892571767171225, "compression_ratio": 1.5885714285714285, "no_speech_prob": 4.860414264840074e-06}, {"id": 714, "seek": 297974, "start": 2979.74, "end": 2985.4399999999996, "text": " And we've been trying to train our model, and the activations at the very end are currently", "tokens": [400, 321, 600, 668, 1382, 281, 3847, 527, 2316, 11, 293, 264, 2430, 763, 412, 264, 588, 917, 366, 4362], "temperature": 0.0, "avg_logprob": -0.07567314540638644, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.338112714525778e-06}, {"id": 715, "seek": 297974, "start": 2985.4399999999996, "end": 2990.2, "text": " between minus 1 and 1.", "tokens": [1296, 3175, 502, 293, 502, 13], "temperature": 0.0, "avg_logprob": -0.07567314540638644, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.338112714525778e-06}, {"id": 716, "seek": 297974, "start": 2990.2, "end": 2992.08, "text": " So they're way off where they need to be.", "tokens": [407, 436, 434, 636, 766, 689, 436, 643, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.07567314540638644, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.338112714525778e-06}, {"id": 717, "seek": 297974, "start": 2992.08, "end": 2994.7999999999997, "text": " The scale is off, the mean is off.", "tokens": [440, 4373, 307, 766, 11, 264, 914, 307, 766, 13], "temperature": 0.0, "avg_logprob": -0.07567314540638644, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.338112714525778e-06}, {"id": 718, "seek": 297974, "start": 2994.7999999999997, "end": 2996.64, "text": " So what can we do?", "tokens": [407, 437, 393, 321, 360, 30], "temperature": 0.0, "avg_logprob": -0.07567314540638644, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.338112714525778e-06}, {"id": 719, "seek": 297974, "start": 2996.64, "end": 3002.62, "text": " One thing we could do would be to try and come up with a new set of weights that cause", "tokens": [1485, 551, 321, 727, 360, 576, 312, 281, 853, 293, 808, 493, 365, 257, 777, 992, 295, 17443, 300, 3082], "temperature": 0.0, "avg_logprob": -0.07567314540638644, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.338112714525778e-06}, {"id": 720, "seek": 297974, "start": 3002.62, "end": 3007.0, "text": " the spread to increase and cause the mean to increase as well.", "tokens": [264, 3974, 281, 3488, 293, 3082, 264, 914, 281, 3488, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.07567314540638644, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.338112714525778e-06}, {"id": 721, "seek": 300700, "start": 3007.0, "end": 3010.28, "text": " And that's going to be really hard to do, because remember all these weights interact", "tokens": [400, 300, 311, 516, 281, 312, 534, 1152, 281, 360, 11, 570, 1604, 439, 613, 17443, 4648], "temperature": 0.0, "avg_logprob": -0.1387920232919546, "compression_ratio": 1.7517482517482517, "no_speech_prob": 6.144143753772369e-06}, {"id": 722, "seek": 300700, "start": 3010.28, "end": 3012.28, "text": " in very intricate ways.", "tokens": [294, 588, 38015, 2098, 13], "temperature": 0.0, "avg_logprob": -0.1387920232919546, "compression_ratio": 1.7517482517482517, "no_speech_prob": 6.144143753772369e-06}, {"id": 723, "seek": 300700, "start": 3012.28, "end": 3015.38, "text": " We've got all those nonlinearities, and they all combine together.", "tokens": [492, 600, 658, 439, 729, 2107, 28263, 1088, 11, 293, 436, 439, 10432, 1214, 13], "temperature": 0.0, "avg_logprob": -0.1387920232919546, "compression_ratio": 1.7517482517482517, "no_speech_prob": 6.144143753772369e-06}, {"id": 724, "seek": 300700, "start": 3015.38, "end": 3021.36, "text": " So to kind of just move up is going to require navigating through this complex landscape.", "tokens": [407, 281, 733, 295, 445, 1286, 493, 307, 516, 281, 3651, 32054, 807, 341, 3997, 9661, 13], "temperature": 0.0, "avg_logprob": -0.1387920232919546, "compression_ratio": 1.7517482517482517, "no_speech_prob": 6.144143753772369e-06}, {"id": 725, "seek": 300700, "start": 3021.36, "end": 3026.32, "text": " And we use all these tricks like momentum and Adam and stuff like that to help us.", "tokens": [400, 321, 764, 439, 613, 11733, 411, 11244, 293, 7938, 293, 1507, 411, 300, 281, 854, 505, 13], "temperature": 0.0, "avg_logprob": -0.1387920232919546, "compression_ratio": 1.7517482517482517, "no_speech_prob": 6.144143753772369e-06}, {"id": 726, "seek": 300700, "start": 3026.32, "end": 3029.92, "text": " But it still requires a lot of twiddling around to get there.", "tokens": [583, 309, 920, 7029, 257, 688, 295, 683, 14273, 1688, 926, 281, 483, 456, 13], "temperature": 0.0, "avg_logprob": -0.1387920232919546, "compression_ratio": 1.7517482517482517, "no_speech_prob": 6.144143753772369e-06}, {"id": 727, "seek": 300700, "start": 3029.92, "end": 3034.72, "text": " So that's going to take a long time, and it's going to be bumpy.", "tokens": [407, 300, 311, 516, 281, 747, 257, 938, 565, 11, 293, 309, 311, 516, 281, 312, 49400, 13], "temperature": 0.0, "avg_logprob": -0.1387920232919546, "compression_ratio": 1.7517482517482517, "no_speech_prob": 6.144143753772369e-06}, {"id": 728, "seek": 300700, "start": 3034.72, "end": 3036.4, "text": " But what if we did this?", "tokens": [583, 437, 498, 321, 630, 341, 30], "temperature": 0.0, "avg_logprob": -0.1387920232919546, "compression_ratio": 1.7517482517482517, "no_speech_prob": 6.144143753772369e-06}, {"id": 729, "seek": 303640, "start": 3036.4, "end": 3043.92, "text": " What if we went times G plus B?", "tokens": [708, 498, 321, 1437, 1413, 460, 1804, 363, 30], "temperature": 0.0, "avg_logprob": -0.1456123743301783, "compression_ratio": 1.697142857142857, "no_speech_prob": 5.955115284450585e-06}, {"id": 730, "seek": 303640, "start": 3043.92, "end": 3047.88, "text": " We added two more parameter vectors.", "tokens": [492, 3869, 732, 544, 13075, 18875, 13], "temperature": 0.0, "avg_logprob": -0.1456123743301783, "compression_ratio": 1.697142857142857, "no_speech_prob": 5.955115284450585e-06}, {"id": 731, "seek": 303640, "start": 3047.88, "end": 3050.26, "text": " Or now it's really easy.", "tokens": [1610, 586, 309, 311, 534, 1858, 13], "temperature": 0.0, "avg_logprob": -0.1456123743301783, "compression_ratio": 1.697142857142857, "no_speech_prob": 5.955115284450585e-06}, {"id": 732, "seek": 303640, "start": 3050.26, "end": 3058.96, "text": " In order to increase the scale, that number has a direct gradient to increase the scale.", "tokens": [682, 1668, 281, 3488, 264, 4373, 11, 300, 1230, 575, 257, 2047, 16235, 281, 3488, 264, 4373, 13], "temperature": 0.0, "avg_logprob": -0.1456123743301783, "compression_ratio": 1.697142857142857, "no_speech_prob": 5.955115284450585e-06}, {"id": 733, "seek": 303640, "start": 3058.96, "end": 3062.28, "text": " To change the mean, that number has a direct gradient to change the mean.", "tokens": [1407, 1319, 264, 914, 11, 300, 1230, 575, 257, 2047, 16235, 281, 1319, 264, 914, 13], "temperature": 0.0, "avg_logprob": -0.1456123743301783, "compression_ratio": 1.697142857142857, "no_speech_prob": 5.955115284450585e-06}, {"id": 734, "seek": 303640, "start": 3062.28, "end": 3064.2400000000002, "text": " There's no interactions or complexities.", "tokens": [821, 311, 572, 13280, 420, 48705, 13], "temperature": 0.0, "avg_logprob": -0.1456123743301783, "compression_ratio": 1.697142857142857, "no_speech_prob": 5.955115284450585e-06}, {"id": 735, "seek": 306424, "start": 3064.24, "end": 3067.64, "text": " It's just straight up and down, straight in and out.", "tokens": [467, 311, 445, 2997, 493, 293, 760, 11, 2997, 294, 293, 484, 13], "temperature": 0.0, "avg_logprob": -0.11697985168196197, "compression_ratio": 1.794979079497908, "no_speech_prob": 1.4510271284962073e-05}, {"id": 736, "seek": 306424, "start": 3067.64, "end": 3070.0, "text": " And that's what Batch Norm does.", "tokens": [400, 300, 311, 437, 363, 852, 8702, 775, 13], "temperature": 0.0, "avg_logprob": -0.11697985168196197, "compression_ratio": 1.794979079497908, "no_speech_prob": 1.4510271284962073e-05}, {"id": 737, "seek": 306424, "start": 3070.0, "end": 3074.8399999999997, "text": " So Batch Norm is basically making it easier for it to do this really important thing,", "tokens": [407, 363, 852, 8702, 307, 1936, 1455, 309, 3571, 337, 309, 281, 360, 341, 534, 1021, 551, 11], "temperature": 0.0, "avg_logprob": -0.11697985168196197, "compression_ratio": 1.794979079497908, "no_speech_prob": 1.4510271284962073e-05}, {"id": 738, "seek": 306424, "start": 3074.8399999999997, "end": 3078.8399999999997, "text": " which is to shift the outputs up and down and in and out.", "tokens": [597, 307, 281, 5513, 264, 23930, 493, 293, 760, 293, 294, 293, 484, 13], "temperature": 0.0, "avg_logprob": -0.11697985168196197, "compression_ratio": 1.794979079497908, "no_speech_prob": 1.4510271284962073e-05}, {"id": 739, "seek": 306424, "start": 3078.8399999999997, "end": 3082.0, "text": " And that's why we end up with these results.", "tokens": [400, 300, 311, 983, 321, 917, 493, 365, 613, 3542, 13], "temperature": 0.0, "avg_logprob": -0.11697985168196197, "compression_ratio": 1.794979079497908, "no_speech_prob": 1.4510271284962073e-05}, {"id": 740, "seek": 306424, "start": 3082.0, "end": 3085.24, "text": " So those details in some ways don't matter terribly.", "tokens": [407, 729, 4365, 294, 512, 2098, 500, 380, 1871, 22903, 13], "temperature": 0.0, "avg_logprob": -0.11697985168196197, "compression_ratio": 1.794979079497908, "no_speech_prob": 1.4510271284962073e-05}, {"id": 741, "seek": 306424, "start": 3085.24, "end": 3090.3599999999997, "text": " The really important thing to know is you definitely want to use it.", "tokens": [440, 534, 1021, 551, 281, 458, 307, 291, 2138, 528, 281, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.11697985168196197, "compression_ratio": 1.794979079497908, "no_speech_prob": 1.4510271284962073e-05}, {"id": 742, "seek": 306424, "start": 3090.3599999999997, "end": 3091.9599999999996, "text": " Or if not it, something like it.", "tokens": [1610, 498, 406, 309, 11, 746, 411, 309, 13], "temperature": 0.0, "avg_logprob": -0.11697985168196197, "compression_ratio": 1.794979079497908, "no_speech_prob": 1.4510271284962073e-05}, {"id": 743, "seek": 309196, "start": 3091.96, "end": 3095.8, "text": " There's various other types of normalization around nowadays.", "tokens": [821, 311, 3683, 661, 3467, 295, 2710, 2144, 926, 13434, 13], "temperature": 0.0, "avg_logprob": -0.1552684827782642, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.1842841558973305e-05}, {"id": 744, "seek": 309196, "start": 3095.8, "end": 3099.32, "text": " But Batch Norm works great.", "tokens": [583, 363, 852, 8702, 1985, 869, 13], "temperature": 0.0, "avg_logprob": -0.1552684827782642, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.1842841558973305e-05}, {"id": 745, "seek": 309196, "start": 3099.32, "end": 3104.12, "text": " The other main normalization type we use in Fast AI is something called Weight Norm, which", "tokens": [440, 661, 2135, 2710, 2144, 2010, 321, 764, 294, 15968, 7318, 307, 746, 1219, 44464, 8702, 11, 597], "temperature": 0.0, "avg_logprob": -0.1552684827782642, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.1842841558973305e-05}, {"id": 746, "seek": 309196, "start": 3104.12, "end": 3110.96, "text": " is much more just in the last few months' development.", "tokens": [307, 709, 544, 445, 294, 264, 1036, 1326, 2493, 6, 3250, 13], "temperature": 0.0, "avg_logprob": -0.1552684827782642, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.1842841558973305e-05}, {"id": 747, "seek": 309196, "start": 3110.96, "end": 3112.96, "text": " So that's Batch Norm.", "tokens": [407, 300, 311, 363, 852, 8702, 13], "temperature": 0.0, "avg_logprob": -0.1552684827782642, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.1842841558973305e-05}, {"id": 748, "seek": 309196, "start": 3112.96, "end": 3119.6, "text": " And so what we do is we create a Batch Norm layer for every continuous variable.", "tokens": [400, 370, 437, 321, 360, 307, 321, 1884, 257, 363, 852, 8702, 4583, 337, 633, 10957, 7006, 13], "temperature": 0.0, "avg_logprob": -0.1552684827782642, "compression_ratio": 1.5294117647058822, "no_speech_prob": 1.1842841558973305e-05}, {"id": 749, "seek": 311960, "start": 3119.6, "end": 3121.92, "text": " In Cont is the number of continuous variables.", "tokens": [682, 4839, 307, 264, 1230, 295, 10957, 9102, 13], "temperature": 0.0, "avg_logprob": -0.12730655335543448, "compression_ratio": 1.7323420074349443, "no_speech_prob": 1.2218833035149146e-05}, {"id": 750, "seek": 311960, "start": 3121.92, "end": 3127.0, "text": " In Fast AI, n underscore something always means the count of that thing.", "tokens": [682, 15968, 7318, 11, 297, 37556, 746, 1009, 1355, 264, 1207, 295, 300, 551, 13], "temperature": 0.0, "avg_logprob": -0.12730655335543448, "compression_ratio": 1.7323420074349443, "no_speech_prob": 1.2218833035149146e-05}, {"id": 751, "seek": 311960, "start": 3127.0, "end": 3128.52, "text": " Cont always means continuous.", "tokens": [4839, 1009, 1355, 10957, 13], "temperature": 0.0, "avg_logprob": -0.12730655335543448, "compression_ratio": 1.7323420074349443, "no_speech_prob": 1.2218833035149146e-05}, {"id": 752, "seek": 311960, "start": 3128.52, "end": 3129.8399999999997, "text": " So then here is where we use it.", "tokens": [407, 550, 510, 307, 689, 321, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.12730655335543448, "compression_ratio": 1.7323420074349443, "no_speech_prob": 1.2218833035149146e-05}, {"id": 753, "seek": 311960, "start": 3129.8399999999997, "end": 3133.72, "text": " We grab our continuous variables and we throw them through a Batch Norm layer.", "tokens": [492, 4444, 527, 10957, 9102, 293, 321, 3507, 552, 807, 257, 363, 852, 8702, 4583, 13], "temperature": 0.0, "avg_logprob": -0.12730655335543448, "compression_ratio": 1.7323420074349443, "no_speech_prob": 1.2218833035149146e-05}, {"id": 754, "seek": 311960, "start": 3133.72, "end": 3139.16, "text": " And so then over here you can see it in our model.", "tokens": [400, 370, 550, 670, 510, 291, 393, 536, 309, 294, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12730655335543448, "compression_ratio": 1.7323420074349443, "no_speech_prob": 1.2218833035149146e-05}, {"id": 755, "seek": 311960, "start": 3139.16, "end": 3141.16, "text": " One interesting thing is this momentum here.", "tokens": [1485, 1880, 551, 307, 341, 11244, 510, 13], "temperature": 0.0, "avg_logprob": -0.12730655335543448, "compression_ratio": 1.7323420074349443, "no_speech_prob": 1.2218833035149146e-05}, {"id": 756, "seek": 311960, "start": 3141.16, "end": 3147.0, "text": " This is not momentum like in optimization, but this is momentum as in exponentially weighted", "tokens": [639, 307, 406, 11244, 411, 294, 19618, 11, 457, 341, 307, 11244, 382, 294, 37330, 32807], "temperature": 0.0, "avg_logprob": -0.12730655335543448, "compression_ratio": 1.7323420074349443, "no_speech_prob": 1.2218833035149146e-05}, {"id": 757, "seek": 311960, "start": 3147.0, "end": 3149.3199999999997, "text": " moving average.", "tokens": [2684, 4274, 13], "temperature": 0.0, "avg_logprob": -0.12730655335543448, "compression_ratio": 1.7323420074349443, "no_speech_prob": 1.2218833035149146e-05}, {"id": 758, "seek": 314932, "start": 3149.32, "end": 3155.6000000000004, "text": " Basically this mean and standard deviation, we don't actually use a different mean and", "tokens": [8537, 341, 914, 293, 3832, 25163, 11, 321, 500, 380, 767, 764, 257, 819, 914, 293], "temperature": 0.0, "avg_logprob": -0.08639656979104747, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.0615975952532608e-05}, {"id": 759, "seek": 314932, "start": 3155.6000000000004, "end": 3157.92, "text": " standard deviation for every mini-batch.", "tokens": [3832, 25163, 337, 633, 8382, 12, 65, 852, 13], "temperature": 0.0, "avg_logprob": -0.08639656979104747, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.0615975952532608e-05}, {"id": 760, "seek": 314932, "start": 3157.92, "end": 3163.1600000000003, "text": " If we did, it would vary so much that it would be very hard to train.", "tokens": [759, 321, 630, 11, 309, 576, 10559, 370, 709, 300, 309, 576, 312, 588, 1152, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.08639656979104747, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.0615975952532608e-05}, {"id": 761, "seek": 314932, "start": 3163.1600000000003, "end": 3169.4, "text": " So instead we take an exponentially weighted moving average of the mean and standard deviation.", "tokens": [407, 2602, 321, 747, 364, 37330, 32807, 2684, 4274, 295, 264, 914, 293, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.08639656979104747, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.0615975952532608e-05}, {"id": 762, "seek": 314932, "start": 3169.4, "end": 3173.2000000000003, "text": " And if you don't remember what I mean by that, look back at last week's lesson to remind", "tokens": [400, 498, 291, 500, 380, 1604, 437, 286, 914, 538, 300, 11, 574, 646, 412, 1036, 1243, 311, 6898, 281, 4160], "temperature": 0.0, "avg_logprob": -0.08639656979104747, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.0615975952532608e-05}, {"id": 763, "seek": 317320, "start": 3173.2, "end": 3179.48, "text": " yourself about exponentially weighted moving averages, which we implemented in Excel for", "tokens": [1803, 466, 37330, 32807, 2684, 42257, 11, 597, 321, 12270, 294, 19060, 337], "temperature": 0.0, "avg_logprob": -0.10036419583605481, "compression_ratio": 1.4780701754385965, "no_speech_prob": 5.014640009903815e-06}, {"id": 764, "seek": 317320, "start": 3179.48, "end": 3190.24, "text": " the momentum and Adam gradient squared terms.", "tokens": [264, 11244, 293, 7938, 16235, 8889, 2115, 13], "temperature": 0.0, "avg_logprob": -0.10036419583605481, "compression_ratio": 1.4780701754385965, "no_speech_prob": 5.014640009903815e-06}, {"id": 765, "seek": 317320, "start": 3190.24, "end": 3195.24, "text": " You can vary the amount of momentum in a Batch Norm layer by passing a different value to", "tokens": [509, 393, 10559, 264, 2372, 295, 11244, 294, 257, 363, 852, 8702, 4583, 538, 8437, 257, 819, 2158, 281], "temperature": 0.0, "avg_logprob": -0.10036419583605481, "compression_ratio": 1.4780701754385965, "no_speech_prob": 5.014640009903815e-06}, {"id": 766, "seek": 317320, "start": 3195.24, "end": 3197.52, "text": " the constructor in PyTorch.", "tokens": [264, 47479, 294, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.10036419583605481, "compression_ratio": 1.4780701754385965, "no_speech_prob": 5.014640009903815e-06}, {"id": 767, "seek": 317320, "start": 3197.52, "end": 3201.68, "text": " If you use a smaller number, it means that the mean and standard deviation will vary", "tokens": [759, 291, 764, 257, 4356, 1230, 11, 309, 1355, 300, 264, 914, 293, 3832, 25163, 486, 10559], "temperature": 0.0, "avg_logprob": -0.10036419583605481, "compression_ratio": 1.4780701754385965, "no_speech_prob": 5.014640009903815e-06}, {"id": 768, "seek": 320168, "start": 3201.68, "end": 3207.12, "text": " less from mini-batch to mini-batch, and that will have less of a regularization effect.", "tokens": [1570, 490, 8382, 12, 65, 852, 281, 8382, 12, 65, 852, 11, 293, 300, 486, 362, 1570, 295, 257, 3890, 2144, 1802, 13], "temperature": 0.0, "avg_logprob": -0.09073636191231864, "compression_ratio": 1.9166666666666667, "no_speech_prob": 3.822726648650132e-05}, {"id": 769, "seek": 320168, "start": 3207.12, "end": 3211.24, "text": " A larger number will mean the variation will be greater for mini-batch to mini-batch.", "tokens": [316, 4833, 1230, 486, 914, 264, 12990, 486, 312, 5044, 337, 8382, 12, 65, 852, 281, 8382, 12, 65, 852, 13], "temperature": 0.0, "avg_logprob": -0.09073636191231864, "compression_ratio": 1.9166666666666667, "no_speech_prob": 3.822726648650132e-05}, {"id": 770, "seek": 320168, "start": 3211.24, "end": 3213.52, "text": " That will have more of a regularization effect.", "tokens": [663, 486, 362, 544, 295, 257, 3890, 2144, 1802, 13], "temperature": 0.0, "avg_logprob": -0.09073636191231864, "compression_ratio": 1.9166666666666667, "no_speech_prob": 3.822726648650132e-05}, {"id": 771, "seek": 320168, "start": 3213.52, "end": 3218.16, "text": " So as well as this thing of training more nicely because it's parameterized better,", "tokens": [407, 382, 731, 382, 341, 551, 295, 3097, 544, 9594, 570, 309, 311, 13075, 1602, 1101, 11], "temperature": 0.0, "avg_logprob": -0.09073636191231864, "compression_ratio": 1.9166666666666667, "no_speech_prob": 3.822726648650132e-05}, {"id": 772, "seek": 320168, "start": 3218.16, "end": 3224.3599999999997, "text": " this momentum term in the mean and standard deviation is the thing that adds this nice", "tokens": [341, 11244, 1433, 294, 264, 914, 293, 3832, 25163, 307, 264, 551, 300, 10860, 341, 1481], "temperature": 0.0, "avg_logprob": -0.09073636191231864, "compression_ratio": 1.9166666666666667, "no_speech_prob": 3.822726648650132e-05}, {"id": 773, "seek": 320168, "start": 3224.3599999999997, "end": 3227.62, "text": " regularization piece.", "tokens": [3890, 2144, 2522, 13], "temperature": 0.0, "avg_logprob": -0.09073636191231864, "compression_ratio": 1.9166666666666667, "no_speech_prob": 3.822726648650132e-05}, {"id": 774, "seek": 322762, "start": 3227.62, "end": 3232.04, "text": " When you add Batch Norm, you should also be able to use a higher learning rate.", "tokens": [1133, 291, 909, 363, 852, 8702, 11, 291, 820, 611, 312, 1075, 281, 764, 257, 2946, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.1744816670050988, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9772492780466564e-05}, {"id": 775, "seek": 322762, "start": 3232.04, "end": 3233.56, "text": " So that's our model.", "tokens": [407, 300, 311, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1744816670050988, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9772492780466564e-05}, {"id": 776, "seek": 322762, "start": 3233.56, "end": 3239.16, "text": " So then you can go LRFind, you can have a look, and then you can go fit, you can save", "tokens": [407, 550, 291, 393, 352, 441, 49, 37, 471, 11, 291, 393, 362, 257, 574, 11, 293, 550, 291, 393, 352, 3318, 11, 291, 393, 3155], "temperature": 0.0, "avg_logprob": -0.1744816670050988, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9772492780466564e-05}, {"id": 777, "seek": 322762, "start": 3239.16, "end": 3247.04, "text": " it, you can plot the losses, you can fit a bit more, and we end up at.103.", "tokens": [309, 11, 291, 393, 7542, 264, 15352, 11, 291, 393, 3318, 257, 857, 544, 11, 293, 321, 917, 493, 412, 2411, 3279, 18, 13], "temperature": 0.0, "avg_logprob": -0.1744816670050988, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9772492780466564e-05}, {"id": 778, "seek": 322762, "start": 3247.04, "end": 3253.52, "text": " Tenth place in the competition was.108, so it's looking good.", "tokens": [314, 17966, 1081, 294, 264, 6211, 390, 2411, 3279, 23, 11, 370, 309, 311, 1237, 665, 13], "temperature": 0.0, "avg_logprob": -0.1744816670050988, "compression_ratio": 1.5833333333333333, "no_speech_prob": 2.9772492780466564e-05}, {"id": 779, "seek": 325352, "start": 3253.52, "end": 3261.24, "text": " Again, take it with a slight grain of salt because what you actually need to do is use", "tokens": [3764, 11, 747, 309, 365, 257, 4036, 12837, 295, 5139, 570, 437, 291, 767, 643, 281, 360, 307, 764], "temperature": 0.0, "avg_logprob": -0.10905978161355724, "compression_ratio": 1.5699658703071673, "no_speech_prob": 1.6187104847631417e-05}, {"id": 780, "seek": 325352, "start": 3261.24, "end": 3267.36, "text": " the real training set and submit it to Kaggle, but you can see we're very much amongst the", "tokens": [264, 957, 3097, 992, 293, 10315, 309, 281, 48751, 22631, 11, 457, 291, 393, 536, 321, 434, 588, 709, 12918, 264], "temperature": 0.0, "avg_logprob": -0.10905978161355724, "compression_ratio": 1.5699658703071673, "no_speech_prob": 1.6187104847631417e-05}, {"id": 781, "seek": 325352, "start": 3267.36, "end": 3273.36, "text": " kind of cutting edge of models, at least as of 2015, and as I say, there haven't really", "tokens": [733, 295, 6492, 4691, 295, 5245, 11, 412, 1935, 382, 295, 7546, 11, 293, 382, 286, 584, 11, 456, 2378, 380, 534], "temperature": 0.0, "avg_logprob": -0.10905978161355724, "compression_ratio": 1.5699658703071673, "no_speech_prob": 1.6187104847631417e-05}, {"id": 782, "seek": 325352, "start": 3273.36, "end": 3276.08, "text": " been any architectural improvements since then.", "tokens": [668, 604, 26621, 13797, 1670, 550, 13], "temperature": 0.0, "avg_logprob": -0.10905978161355724, "compression_ratio": 1.5699658703071673, "no_speech_prob": 1.6187104847631417e-05}, {"id": 783, "seek": 325352, "start": 3276.08, "end": 3279.64, "text": " There wasn't Batch Norm when this was around, so the fact we added Batch Norm means that", "tokens": [821, 2067, 380, 363, 852, 8702, 562, 341, 390, 926, 11, 370, 264, 1186, 321, 3869, 363, 852, 8702, 1355, 300], "temperature": 0.0, "avg_logprob": -0.10905978161355724, "compression_ratio": 1.5699658703071673, "no_speech_prob": 1.6187104847631417e-05}, {"id": 784, "seek": 325352, "start": 3279.64, "end": 3282.46, "text": " we should get better results, and certainly more quickly.", "tokens": [321, 820, 483, 1101, 3542, 11, 293, 3297, 544, 2661, 13], "temperature": 0.0, "avg_logprob": -0.10905978161355724, "compression_ratio": 1.5699658703071673, "no_speech_prob": 1.6187104847631417e-05}, {"id": 785, "seek": 328246, "start": 3282.46, "end": 3286.12, "text": " And if I remember correctly, in their model, they had to train at a lower learning rate", "tokens": [400, 498, 286, 1604, 8944, 11, 294, 641, 2316, 11, 436, 632, 281, 3847, 412, 257, 3126, 2539, 3314], "temperature": 0.0, "avg_logprob": -0.15058132697796, "compression_ratio": 1.4684684684684686, "no_speech_prob": 6.962060069781728e-06}, {"id": 786, "seek": 328246, "start": 3286.12, "end": 3287.7200000000003, "text": " for quite a lot longer.", "tokens": [337, 1596, 257, 688, 2854, 13], "temperature": 0.0, "avg_logprob": -0.15058132697796, "compression_ratio": 1.4684684684684686, "no_speech_prob": 6.962060069781728e-06}, {"id": 787, "seek": 328246, "start": 3287.7200000000003, "end": 3296.2, "text": " As you can see, this is about less than 45 minutes of training, so that's nice and fast.", "tokens": [1018, 291, 393, 536, 11, 341, 307, 466, 1570, 813, 6905, 2077, 295, 3097, 11, 370, 300, 311, 1481, 293, 2370, 13], "temperature": 0.0, "avg_logprob": -0.15058132697796, "compression_ratio": 1.4684684684684686, "no_speech_prob": 6.962060069781728e-06}, {"id": 788, "seek": 328246, "start": 3296.2, "end": 3300.34, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.15058132697796, "compression_ratio": 1.4684684684684686, "no_speech_prob": 6.962060069781728e-06}, {"id": 789, "seek": 328246, "start": 3300.34, "end": 3305.76, "text": " In what proportion would you use dropout versus other regularization errors like weight decay,", "tokens": [682, 437, 16068, 576, 291, 764, 3270, 346, 5717, 661, 3890, 2144, 13603, 411, 3364, 21039, 11], "temperature": 0.0, "avg_logprob": -0.15058132697796, "compression_ratio": 1.4684684684684686, "no_speech_prob": 6.962060069781728e-06}, {"id": 790, "seek": 328246, "start": 3305.76, "end": 3310.12, "text": " L2 norms, etc.?", "tokens": [441, 17, 24357, 11, 5183, 41401], "temperature": 0.0, "avg_logprob": -0.15058132697796, "compression_ratio": 1.4684684684684686, "no_speech_prob": 6.962060069781728e-06}, {"id": 791, "seek": 331012, "start": 3310.12, "end": 3315.4, "text": " So remember that L2 regularization and weight decay are kind of two ways of doing the same", "tokens": [407, 1604, 300, 441, 17, 3890, 2144, 293, 3364, 21039, 366, 733, 295, 732, 2098, 295, 884, 264, 912], "temperature": 0.0, "avg_logprob": -0.09423969008705833, "compression_ratio": 1.7936507936507937, "no_speech_prob": 6.5400058701925445e-06}, {"id": 792, "seek": 331012, "start": 3315.4, "end": 3321.08, "text": " thing, and we should always use the weight decay version, not the L2 regularization version.", "tokens": [551, 11, 293, 321, 820, 1009, 764, 264, 3364, 21039, 3037, 11, 406, 264, 441, 17, 3890, 2144, 3037, 13], "temperature": 0.0, "avg_logprob": -0.09423969008705833, "compression_ratio": 1.7936507936507937, "no_speech_prob": 6.5400058701925445e-06}, {"id": 793, "seek": 331012, "start": 3321.08, "end": 3327.72, "text": " So there's weight decay, there's Batch Norm, which kind of has a regularizing effect, there's", "tokens": [407, 456, 311, 3364, 21039, 11, 456, 311, 363, 852, 8702, 11, 597, 733, 295, 575, 257, 3890, 3319, 1802, 11, 456, 311], "temperature": 0.0, "avg_logprob": -0.09423969008705833, "compression_ratio": 1.7936507936507937, "no_speech_prob": 6.5400058701925445e-06}, {"id": 794, "seek": 331012, "start": 3327.72, "end": 3333.48, "text": " data augmentation, which we'll see soon, and there's dropout.", "tokens": [1412, 14501, 19631, 11, 597, 321, 603, 536, 2321, 11, 293, 456, 311, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.09423969008705833, "compression_ratio": 1.7936507936507937, "no_speech_prob": 6.5400058701925445e-06}, {"id": 795, "seek": 333348, "start": 3333.48, "end": 3340.16, "text": " So Batch Norm we pretty much always want, so that's easy.", "tokens": [407, 363, 852, 8702, 321, 1238, 709, 1009, 528, 11, 370, 300, 311, 1858, 13], "temperature": 0.0, "avg_logprob": -0.13589936002678826, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.2218873962410726e-05}, {"id": 796, "seek": 333348, "start": 3340.16, "end": 3342.12, "text": " Data augmentation we'll see in a moment.", "tokens": [11888, 14501, 19631, 321, 603, 536, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.13589936002678826, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.2218873962410726e-05}, {"id": 797, "seek": 333348, "start": 3342.12, "end": 3345.56, "text": " So then it's really between dropout versus weight decay.", "tokens": [407, 550, 309, 311, 534, 1296, 3270, 346, 5717, 3364, 21039, 13], "temperature": 0.0, "avg_logprob": -0.13589936002678826, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.2218873962410726e-05}, {"id": 798, "seek": 333348, "start": 3345.56, "end": 3347.12, "text": " I have no idea.", "tokens": [286, 362, 572, 1558, 13], "temperature": 0.0, "avg_logprob": -0.13589936002678826, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.2218873962410726e-05}, {"id": 799, "seek": 333348, "start": 3347.12, "end": 3355.6, "text": " I don't think I've seen anybody provide a compelling study of how to combine those two", "tokens": [286, 500, 380, 519, 286, 600, 1612, 4472, 2893, 257, 20050, 2979, 295, 577, 281, 10432, 729, 732], "temperature": 0.0, "avg_logprob": -0.13589936002678826, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.2218873962410726e-05}, {"id": 800, "seek": 333348, "start": 3355.6, "end": 3356.6, "text": " things.", "tokens": [721, 13], "temperature": 0.0, "avg_logprob": -0.13589936002678826, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.2218873962410726e-05}, {"id": 801, "seek": 333348, "start": 3356.6, "end": 3358.28, "text": " Can you always use one instead of the other?", "tokens": [1664, 291, 1009, 764, 472, 2602, 295, 264, 661, 30], "temperature": 0.0, "avg_logprob": -0.13589936002678826, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.2218873962410726e-05}, {"id": 802, "seek": 333348, "start": 3358.28, "end": 3359.28, "text": " Why?", "tokens": [1545, 30], "temperature": 0.0, "avg_logprob": -0.13589936002678826, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.2218873962410726e-05}, {"id": 803, "seek": 333348, "start": 3359.28, "end": 3360.92, "text": " Why not?", "tokens": [1545, 406, 30], "temperature": 0.0, "avg_logprob": -0.13589936002678826, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.2218873962410726e-05}, {"id": 804, "seek": 333348, "start": 3360.92, "end": 3363.36, "text": " I don't think anybody has figured that out.", "tokens": [286, 500, 380, 519, 4472, 575, 8932, 300, 484, 13], "temperature": 0.0, "avg_logprob": -0.13589936002678826, "compression_ratio": 1.5569620253164558, "no_speech_prob": 1.2218873962410726e-05}, {"id": 805, "seek": 336336, "start": 3363.36, "end": 3372.36, "text": " I think in practice it seems that you generally want a bit of both.", "tokens": [286, 519, 294, 3124, 309, 2544, 300, 291, 5101, 528, 257, 857, 295, 1293, 13], "temperature": 0.0, "avg_logprob": -0.1283163282606337, "compression_ratio": 1.5483870967741935, "no_speech_prob": 5.95504616285325e-06}, {"id": 806, "seek": 336336, "start": 3372.36, "end": 3378.6, "text": " You pretty much always want some weight decay, but you often also want a bit of dropout.", "tokens": [509, 1238, 709, 1009, 528, 512, 3364, 21039, 11, 457, 291, 2049, 611, 528, 257, 857, 295, 3270, 346, 13], "temperature": 0.0, "avg_logprob": -0.1283163282606337, "compression_ratio": 1.5483870967741935, "no_speech_prob": 5.95504616285325e-06}, {"id": 807, "seek": 336336, "start": 3378.6, "end": 3381.2000000000003, "text": " But honestly, I don't know why.", "tokens": [583, 6095, 11, 286, 500, 380, 458, 983, 13], "temperature": 0.0, "avg_logprob": -0.1283163282606337, "compression_ratio": 1.5483870967741935, "no_speech_prob": 5.95504616285325e-06}, {"id": 808, "seek": 336336, "start": 3381.2000000000003, "end": 3385.2000000000003, "text": " I've not seen anybody really explain why or how to decide.", "tokens": [286, 600, 406, 1612, 4472, 534, 2903, 983, 420, 577, 281, 4536, 13], "temperature": 0.0, "avg_logprob": -0.1283163282606337, "compression_ratio": 1.5483870967741935, "no_speech_prob": 5.95504616285325e-06}, {"id": 809, "seek": 336336, "start": 3385.2000000000003, "end": 3390.96, "text": " So this is one of these things you have to try out and kind of get a feel for what tends", "tokens": [407, 341, 307, 472, 295, 613, 721, 291, 362, 281, 853, 484, 293, 733, 295, 483, 257, 841, 337, 437, 12258], "temperature": 0.0, "avg_logprob": -0.1283163282606337, "compression_ratio": 1.5483870967741935, "no_speech_prob": 5.95504616285325e-06}, {"id": 810, "seek": 339096, "start": 3390.96, "end": 3394.2400000000002, "text": " to work for your kinds of problems.", "tokens": [281, 589, 337, 428, 3685, 295, 2740, 13], "temperature": 0.0, "avg_logprob": -0.11814210581225018, "compression_ratio": 1.6411483253588517, "no_speech_prob": 7.071717391227139e-06}, {"id": 811, "seek": 339096, "start": 3394.2400000000002, "end": 3399.16, "text": " I think the defaults that we provide in most of our learners should work pretty well in", "tokens": [286, 519, 264, 7576, 82, 300, 321, 2893, 294, 881, 295, 527, 23655, 820, 589, 1238, 731, 294], "temperature": 0.0, "avg_logprob": -0.11814210581225018, "compression_ratio": 1.6411483253588517, "no_speech_prob": 7.071717391227139e-06}, {"id": 812, "seek": 339096, "start": 3399.16, "end": 3406.56, "text": " most situations, but yeah, definitely play around with it.", "tokens": [881, 6851, 11, 457, 1338, 11, 2138, 862, 926, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.11814210581225018, "compression_ratio": 1.6411483253588517, "no_speech_prob": 7.071717391227139e-06}, {"id": 813, "seek": 339096, "start": 3406.56, "end": 3413.52, "text": " The next kind of regularization we're going to look at is data augmentation.", "tokens": [440, 958, 733, 295, 3890, 2144, 321, 434, 516, 281, 574, 412, 307, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.11814210581225018, "compression_ratio": 1.6411483253588517, "no_speech_prob": 7.071717391227139e-06}, {"id": 814, "seek": 339096, "start": 3413.52, "end": 3420.04, "text": " And data augmentation is one of the least well-studied types of regularization, but", "tokens": [400, 1412, 14501, 19631, 307, 472, 295, 264, 1935, 731, 12, 28349, 1091, 3467, 295, 3890, 2144, 11, 457], "temperature": 0.0, "avg_logprob": -0.11814210581225018, "compression_ratio": 1.6411483253588517, "no_speech_prob": 7.071717391227139e-06}, {"id": 815, "seek": 342004, "start": 3420.04, "end": 3425.64, "text": " it's the kind that I think I'm kind of the most excited about.", "tokens": [309, 311, 264, 733, 300, 286, 519, 286, 478, 733, 295, 264, 881, 2919, 466, 13], "temperature": 0.0, "avg_logprob": -0.11577025125193041, "compression_ratio": 1.6861702127659575, "no_speech_prob": 8.664454071549699e-06}, {"id": 816, "seek": 342004, "start": 3425.64, "end": 3433.92, "text": " The reason I'm kind of the most excited about it is that there's basically almost no cost", "tokens": [440, 1778, 286, 478, 733, 295, 264, 881, 2919, 466, 309, 307, 300, 456, 311, 1936, 1920, 572, 2063], "temperature": 0.0, "avg_logprob": -0.11577025125193041, "compression_ratio": 1.6861702127659575, "no_speech_prob": 8.664454071549699e-06}, {"id": 817, "seek": 342004, "start": 3433.92, "end": 3435.16, "text": " to it.", "tokens": [281, 309, 13], "temperature": 0.0, "avg_logprob": -0.11577025125193041, "compression_ratio": 1.6861702127659575, "no_speech_prob": 8.664454071549699e-06}, {"id": 818, "seek": 342004, "start": 3435.16, "end": 3439.96, "text": " You can do data augmentation and get better generalization without it taking longer to", "tokens": [509, 393, 360, 1412, 14501, 19631, 293, 483, 1101, 2674, 2144, 1553, 309, 1940, 2854, 281], "temperature": 0.0, "avg_logprob": -0.11577025125193041, "compression_ratio": 1.6861702127659575, "no_speech_prob": 8.664454071549699e-06}, {"id": 819, "seek": 342004, "start": 3439.96, "end": 3445.12, "text": " train, without underfitting, to an extent at least.", "tokens": [3847, 11, 1553, 833, 69, 2414, 11, 281, 364, 8396, 412, 1935, 13], "temperature": 0.0, "avg_logprob": -0.11577025125193041, "compression_ratio": 1.6861702127659575, "no_speech_prob": 8.664454071549699e-06}, {"id": 820, "seek": 342004, "start": 3445.12, "end": 3447.72, "text": " So let me explain.", "tokens": [407, 718, 385, 2903, 13], "temperature": 0.0, "avg_logprob": -0.11577025125193041, "compression_ratio": 1.6861702127659575, "no_speech_prob": 8.664454071549699e-06}, {"id": 821, "seek": 344772, "start": 3447.72, "end": 3451.48, "text": " So what we're going to do now is we're going to come back to computer vision and we're", "tokens": [407, 437, 321, 434, 516, 281, 360, 586, 307, 321, 434, 516, 281, 808, 646, 281, 3820, 5201, 293, 321, 434], "temperature": 0.0, "avg_logprob": -0.16388060251871744, "compression_ratio": 1.8185840707964602, "no_speech_prob": 1.4285285942605697e-05}, {"id": 822, "seek": 344772, "start": 3451.48, "end": 3454.24, "text": " going to come back to our pets data set again.", "tokens": [516, 281, 808, 646, 281, 527, 19897, 1412, 992, 797, 13], "temperature": 0.0, "avg_logprob": -0.16388060251871744, "compression_ratio": 1.8185840707964602, "no_speech_prob": 1.4285285942605697e-05}, {"id": 823, "seek": 344772, "start": 3454.24, "end": 3456.24, "text": " So let's load it in.", "tokens": [407, 718, 311, 3677, 309, 294, 13], "temperature": 0.0, "avg_logprob": -0.16388060251871744, "compression_ratio": 1.8185840707964602, "no_speech_prob": 1.4285285942605697e-05}, {"id": 824, "seek": 344772, "start": 3456.24, "end": 3460.3199999999997, "text": " Our pets data set, the images were inside the images subfolder.", "tokens": [2621, 19897, 1412, 992, 11, 264, 5267, 645, 1854, 264, 5267, 1422, 18353, 260, 13], "temperature": 0.0, "avg_logprob": -0.16388060251871744, "compression_ratio": 1.8185840707964602, "no_speech_prob": 1.4285285942605697e-05}, {"id": 825, "seek": 344772, "start": 3460.3199999999997, "end": 3467.6, "text": " And I'm going to call getTransforms as per usual, but when we call getTransforms, there's", "tokens": [400, 286, 478, 516, 281, 818, 483, 33339, 837, 82, 382, 680, 7713, 11, 457, 562, 321, 818, 483, 33339, 837, 82, 11, 456, 311], "temperature": 0.0, "avg_logprob": -0.16388060251871744, "compression_ratio": 1.8185840707964602, "no_speech_prob": 1.4285285942605697e-05}, {"id": 826, "seek": 344772, "start": 3467.6, "end": 3471.8799999999997, "text": " a whole long list of things that we can provide.", "tokens": [257, 1379, 938, 1329, 295, 721, 300, 321, 393, 2893, 13], "temperature": 0.0, "avg_logprob": -0.16388060251871744, "compression_ratio": 1.8185840707964602, "no_speech_prob": 1.4285285942605697e-05}, {"id": 827, "seek": 344772, "start": 3471.8799999999997, "end": 3474.6, "text": " And so far, we haven't been varying that much at all.", "tokens": [400, 370, 1400, 11, 321, 2378, 380, 668, 22984, 300, 709, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.16388060251871744, "compression_ratio": 1.8185840707964602, "no_speech_prob": 1.4285285942605697e-05}, {"id": 828, "seek": 347460, "start": 3474.6, "end": 3480.68, "text": " But in order to really understand data augmentation, I'm going to kind of ratchet up all of the", "tokens": [583, 294, 1668, 281, 534, 1223, 1412, 14501, 19631, 11, 286, 478, 516, 281, 733, 295, 45885, 493, 439, 295, 264], "temperature": 0.0, "avg_logprob": -0.15052857270111908, "compression_ratio": 1.8706896551724137, "no_speech_prob": 8.013431397557724e-06}, {"id": 829, "seek": 347460, "start": 3480.68, "end": 3482.2, "text": " defaults.", "tokens": [7576, 82, 13], "temperature": 0.0, "avg_logprob": -0.15052857270111908, "compression_ratio": 1.8706896551724137, "no_speech_prob": 8.013431397557724e-06}, {"id": 830, "seek": 347460, "start": 3482.2, "end": 3489.3199999999997, "text": " So there's a parameter here for what's the probability of an affine transform happening,", "tokens": [407, 456, 311, 257, 13075, 510, 337, 437, 311, 264, 8482, 295, 364, 2096, 533, 4088, 2737, 11], "temperature": 0.0, "avg_logprob": -0.15052857270111908, "compression_ratio": 1.8706896551724137, "no_speech_prob": 8.013431397557724e-06}, {"id": 831, "seek": 347460, "start": 3489.3199999999997, "end": 3493.04, "text": " what's the probability of a lighting transform happening, so I set them both to one.", "tokens": [437, 311, 264, 8482, 295, 257, 9577, 4088, 2737, 11, 370, 286, 992, 552, 1293, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.15052857270111908, "compression_ratio": 1.8706896551724137, "no_speech_prob": 8.013431397557724e-06}, {"id": 832, "seek": 347460, "start": 3493.04, "end": 3496.52, "text": " So they're all going to get transformed, they're going to do more rotation, more zoom, more", "tokens": [407, 436, 434, 439, 516, 281, 483, 16894, 11, 436, 434, 516, 281, 360, 544, 12447, 11, 544, 8863, 11, 544], "temperature": 0.0, "avg_logprob": -0.15052857270111908, "compression_ratio": 1.8706896551724137, "no_speech_prob": 8.013431397557724e-06}, {"id": 833, "seek": 347460, "start": 3496.52, "end": 3500.2799999999997, "text": " lighting transforms, and more warping.", "tokens": [9577, 35592, 11, 293, 544, 1516, 3381, 13], "temperature": 0.0, "avg_logprob": -0.15052857270111908, "compression_ratio": 1.8706896551724137, "no_speech_prob": 8.013431397557724e-06}, {"id": 834, "seek": 347460, "start": 3500.2799999999997, "end": 3502.72, "text": " What do all those mean?", "tokens": [708, 360, 439, 729, 914, 30], "temperature": 0.0, "avg_logprob": -0.15052857270111908, "compression_ratio": 1.8706896551724137, "no_speech_prob": 8.013431397557724e-06}, {"id": 835, "seek": 350272, "start": 3502.72, "end": 3507.72, "text": " So you should check the documentation, and you do that by typing doc, and there's the", "tokens": [407, 291, 820, 1520, 264, 14333, 11, 293, 291, 360, 300, 538, 18444, 3211, 11, 293, 456, 311, 264], "temperature": 0.0, "avg_logprob": -0.1333434365012429, "compression_ratio": 1.7523809523809524, "no_speech_prob": 1.0783065590658225e-05}, {"id": 836, "seek": 350272, "start": 3507.72, "end": 3511.3599999999997, "text": " brief documentation, but the real documentation is in docs.", "tokens": [5353, 14333, 11, 457, 264, 957, 14333, 307, 294, 45623, 13], "temperature": 0.0, "avg_logprob": -0.1333434365012429, "compression_ratio": 1.7523809523809524, "no_speech_prob": 1.0783065590658225e-05}, {"id": 837, "seek": 350272, "start": 3511.3599999999997, "end": 3516.66, "text": " So I'll click on show in docs, and here it is.", "tokens": [407, 286, 603, 2052, 322, 855, 294, 45623, 11, 293, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1333434365012429, "compression_ratio": 1.7523809523809524, "no_speech_prob": 1.0783065590658225e-05}, {"id": 838, "seek": 350272, "start": 3516.66, "end": 3521.7999999999997, "text": " And so this tells you what all of those do, but generally the most interesting parts of", "tokens": [400, 370, 341, 5112, 291, 437, 439, 295, 729, 360, 11, 457, 5101, 264, 881, 1880, 3166, 295], "temperature": 0.0, "avg_logprob": -0.1333434365012429, "compression_ratio": 1.7523809523809524, "no_speech_prob": 1.0783065590658225e-05}, {"id": 839, "seek": 350272, "start": 3521.7999999999997, "end": 3526.3599999999997, "text": " the docs tend to be at the top, where you kind of get the summaries of what's going", "tokens": [264, 45623, 3928, 281, 312, 412, 264, 1192, 11, 689, 291, 733, 295, 483, 264, 8367, 4889, 295, 437, 311, 516], "temperature": 0.0, "avg_logprob": -0.1333434365012429, "compression_ratio": 1.7523809523809524, "no_speech_prob": 1.0783065590658225e-05}, {"id": 840, "seek": 350272, "start": 3526.3599999999997, "end": 3527.3599999999997, "text": " on.", "tokens": [322, 13], "temperature": 0.0, "avg_logprob": -0.1333434365012429, "compression_ratio": 1.7523809523809524, "no_speech_prob": 1.0783065590658225e-05}, {"id": 841, "seek": 352736, "start": 3527.36, "end": 3535.48, "text": " So here there's something called list of transforms, and here you can see every transform has something", "tokens": [407, 510, 456, 311, 746, 1219, 1329, 295, 35592, 11, 293, 510, 291, 393, 536, 633, 4088, 575, 746], "temperature": 0.0, "avg_logprob": -0.12970717748006186, "compression_ratio": 1.6964285714285714, "no_speech_prob": 2.1567943804257084e-06}, {"id": 842, "seek": 352736, "start": 3535.48, "end": 3539.2400000000002, "text": " showing you lots of different values of it.", "tokens": [4099, 291, 3195, 295, 819, 4190, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.12970717748006186, "compression_ratio": 1.6964285714285714, "no_speech_prob": 2.1567943804257084e-06}, {"id": 843, "seek": 352736, "start": 3539.2400000000002, "end": 3541.7000000000003, "text": " So here's brightness.", "tokens": [407, 510, 311, 21367, 13], "temperature": 0.0, "avg_logprob": -0.12970717748006186, "compression_ratio": 1.6964285714285714, "no_speech_prob": 2.1567943804257084e-06}, {"id": 844, "seek": 352736, "start": 3541.7000000000003, "end": 3546.98, "text": " So make sure you read these, and remember, these notebooks, you can open up and run this", "tokens": [407, 652, 988, 291, 1401, 613, 11, 293, 1604, 11, 613, 43782, 11, 291, 393, 1269, 493, 293, 1190, 341], "temperature": 0.0, "avg_logprob": -0.12970717748006186, "compression_ratio": 1.6964285714285714, "no_speech_prob": 2.1567943804257084e-06}, {"id": 845, "seek": 352736, "start": 3546.98, "end": 3549.1200000000003, "text": " code yourself and get this output.", "tokens": [3089, 1803, 293, 483, 341, 5598, 13], "temperature": 0.0, "avg_logprob": -0.12970717748006186, "compression_ratio": 1.6964285714285714, "no_speech_prob": 2.1567943804257084e-06}, {"id": 846, "seek": 352736, "start": 3549.1200000000003, "end": 3555.1600000000003, "text": " All of these HTML documentation documents are auto-generated from the notebooks in the", "tokens": [1057, 295, 613, 17995, 14333, 8512, 366, 8399, 12, 21848, 770, 490, 264, 43782, 294, 264], "temperature": 0.0, "avg_logprob": -0.12970717748006186, "compression_ratio": 1.6964285714285714, "no_speech_prob": 2.1567943804257084e-06}, {"id": 847, "seek": 355516, "start": 3555.16, "end": 3560.2, "text": " docs underscore source directory in the fast AI repo.", "tokens": [45623, 37556, 4009, 21120, 294, 264, 2370, 7318, 49040, 13], "temperature": 0.0, "avg_logprob": -0.16681119228931182, "compression_ratio": 1.5974025974025974, "no_speech_prob": 2.6425730084156385e-06}, {"id": 848, "seek": 355516, "start": 3560.2, "end": 3565.52, "text": " So you will see the exact same cats if you try this.", "tokens": [407, 291, 486, 536, 264, 1900, 912, 11111, 498, 291, 853, 341, 13], "temperature": 0.0, "avg_logprob": -0.16681119228931182, "compression_ratio": 1.5974025974025974, "no_speech_prob": 2.6425730084156385e-06}, {"id": 849, "seek": 355516, "start": 3565.52, "end": 3570.7599999999998, "text": " Silverman really likes cats, so there's a lot of cats in the documentation.", "tokens": [15861, 1601, 534, 5902, 11111, 11, 370, 456, 311, 257, 688, 295, 11111, 294, 264, 14333, 13], "temperature": 0.0, "avg_logprob": -0.16681119228931182, "compression_ratio": 1.5974025974025974, "no_speech_prob": 2.6425730084156385e-06}, {"id": 850, "seek": 355516, "start": 3570.7599999999998, "end": 3574.0, "text": " And I think because he's been so awesome at creating great documentation, he gets to pick", "tokens": [400, 286, 519, 570, 415, 311, 668, 370, 3476, 412, 4084, 869, 14333, 11, 415, 2170, 281, 1888], "temperature": 0.0, "avg_logprob": -0.16681119228931182, "compression_ratio": 1.5974025974025974, "no_speech_prob": 2.6425730084156385e-06}, {"id": 851, "seek": 355516, "start": 3574.0, "end": 3578.08, "text": " the cats.", "tokens": [264, 11111, 13], "temperature": 0.0, "avg_logprob": -0.16681119228931182, "compression_ratio": 1.5974025974025974, "no_speech_prob": 2.6425730084156385e-06}, {"id": 852, "seek": 355516, "start": 3578.08, "end": 3584.3999999999996, "text": " So for example, looking at different values of brightness, what I do here is I look to", "tokens": [407, 337, 1365, 11, 1237, 412, 819, 4190, 295, 21367, 11, 437, 286, 360, 510, 307, 286, 574, 281], "temperature": 0.0, "avg_logprob": -0.16681119228931182, "compression_ratio": 1.5974025974025974, "no_speech_prob": 2.6425730084156385e-06}, {"id": 853, "seek": 358440, "start": 3584.4, "end": 3585.8, "text": " see two things.", "tokens": [536, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.11485429095406817, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.3419816241366789e-05}, {"id": 854, "seek": 358440, "start": 3585.8, "end": 3592.6800000000003, "text": " The first is, for which of these levels of transformation is it still clear what the", "tokens": [440, 700, 307, 11, 337, 597, 295, 613, 4358, 295, 9887, 307, 309, 920, 1850, 437, 264], "temperature": 0.0, "avg_logprob": -0.11485429095406817, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.3419816241366789e-05}, {"id": 855, "seek": 358440, "start": 3592.6800000000003, "end": 3594.6, "text": " picture is a picture of?", "tokens": [3036, 307, 257, 3036, 295, 30], "temperature": 0.0, "avg_logprob": -0.11485429095406817, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.3419816241366789e-05}, {"id": 856, "seek": 358440, "start": 3594.6, "end": 3597.28, "text": " So this is kind of getting to a point where it's pretty unclear.", "tokens": [407, 341, 307, 733, 295, 1242, 281, 257, 935, 689, 309, 311, 1238, 25636, 13], "temperature": 0.0, "avg_logprob": -0.11485429095406817, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.3419816241366789e-05}, {"id": 857, "seek": 358440, "start": 3597.28, "end": 3600.12, "text": " This is possibly getting a little unclear.", "tokens": [639, 307, 6264, 1242, 257, 707, 25636, 13], "temperature": 0.0, "avg_logprob": -0.11485429095406817, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.3419816241366789e-05}, {"id": 858, "seek": 358440, "start": 3600.12, "end": 3605.4, "text": " The second thing I do is I look at the actual dataset that I'm modelling, or particularly", "tokens": [440, 1150, 551, 286, 360, 307, 286, 574, 412, 264, 3539, 28872, 300, 286, 478, 42253, 11, 420, 4098], "temperature": 0.0, "avg_logprob": -0.11485429095406817, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.3419816241366789e-05}, {"id": 859, "seek": 358440, "start": 3605.4, "end": 3609.08, "text": " the dataset that I'll be using as a validation set, and I try to get a sense of what the", "tokens": [264, 28872, 300, 286, 603, 312, 1228, 382, 257, 24071, 992, 11, 293, 286, 853, 281, 483, 257, 2020, 295, 437, 264], "temperature": 0.0, "avg_logprob": -0.11485429095406817, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.3419816241366789e-05}, {"id": 860, "seek": 358440, "start": 3609.08, "end": 3612.7200000000003, "text": " variation in this case in lighting is.", "tokens": [12990, 294, 341, 1389, 294, 9577, 307, 13], "temperature": 0.0, "avg_logprob": -0.11485429095406817, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.3419816241366789e-05}, {"id": 861, "seek": 361272, "start": 3612.72, "end": 3617.4399999999996, "text": " So if they're nearly all professionally taking photos, I would probably want them all to", "tokens": [407, 498, 436, 434, 6217, 439, 27941, 1940, 5787, 11, 286, 576, 1391, 528, 552, 439, 281], "temperature": 0.0, "avg_logprob": -0.10959884712287972, "compression_ratio": 1.7318007662835249, "no_speech_prob": 5.682261416950496e-06}, {"id": 862, "seek": 361272, "start": 3617.4399999999996, "end": 3619.3199999999997, "text": " be about in the middle.", "tokens": [312, 466, 294, 264, 2808, 13], "temperature": 0.0, "avg_logprob": -0.10959884712287972, "compression_ratio": 1.7318007662835249, "no_speech_prob": 5.682261416950496e-06}, {"id": 863, "seek": 361272, "start": 3619.3199999999997, "end": 3625.72, "text": " But if they're photos that are taken by some pretty amateur photographers, they're likely", "tokens": [583, 498, 436, 434, 5787, 300, 366, 2726, 538, 512, 1238, 29339, 33835, 11, 436, 434, 3700], "temperature": 0.0, "avg_logprob": -0.10959884712287972, "compression_ratio": 1.7318007662835249, "no_speech_prob": 5.682261416950496e-06}, {"id": 864, "seek": 361272, "start": 3625.72, "end": 3629.3999999999996, "text": " to be some that are very overexposed, some very underexposed.", "tokens": [281, 312, 512, 300, 366, 588, 38657, 87, 79, 1744, 11, 512, 588, 674, 323, 87, 79, 1744, 13], "temperature": 0.0, "avg_logprob": -0.10959884712287972, "compression_ratio": 1.7318007662835249, "no_speech_prob": 5.682261416950496e-06}, {"id": 865, "seek": 361272, "start": 3629.3999999999996, "end": 3635.64, "text": " So you should pick a value of this data augmentation for brightness that both allows the image to", "tokens": [407, 291, 820, 1888, 257, 2158, 295, 341, 1412, 14501, 19631, 337, 21367, 300, 1293, 4045, 264, 3256, 281], "temperature": 0.0, "avg_logprob": -0.10959884712287972, "compression_ratio": 1.7318007662835249, "no_speech_prob": 5.682261416950496e-06}, {"id": 866, "seek": 361272, "start": 3635.64, "end": 3640.52, "text": " still be seen clearly, and also represents the kind of data that you're going to be using", "tokens": [920, 312, 1612, 4448, 11, 293, 611, 8855, 264, 733, 295, 1412, 300, 291, 434, 516, 281, 312, 1228], "temperature": 0.0, "avg_logprob": -0.10959884712287972, "compression_ratio": 1.7318007662835249, "no_speech_prob": 5.682261416950496e-06}, {"id": 867, "seek": 364052, "start": 3640.52, "end": 3643.0, "text": " this to model on in practice.", "tokens": [341, 281, 2316, 322, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.15024488530260452, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.9222619812353514e-05}, {"id": 868, "seek": 364052, "start": 3643.0, "end": 3644.92, "text": " So you've got to see the same thing for contrast.", "tokens": [407, 291, 600, 658, 281, 536, 264, 912, 551, 337, 8712, 13], "temperature": 0.0, "avg_logprob": -0.15024488530260452, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.9222619812353514e-05}, {"id": 869, "seek": 364052, "start": 3644.92, "end": 3649.96, "text": " It would be unusual to have a dataset with such ridiculous contrast, but perhaps you", "tokens": [467, 576, 312, 10901, 281, 362, 257, 28872, 365, 1270, 11083, 8712, 11, 457, 4317, 291], "temperature": 0.0, "avg_logprob": -0.15024488530260452, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.9222619812353514e-05}, {"id": 870, "seek": 364052, "start": 3649.96, "end": 3653.84, "text": " do, in which case you should use data augmentation up to that level.", "tokens": [360, 11, 294, 597, 1389, 291, 820, 764, 1412, 14501, 19631, 493, 281, 300, 1496, 13], "temperature": 0.0, "avg_logprob": -0.15024488530260452, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.9222619812353514e-05}, {"id": 871, "seek": 364052, "start": 3653.84, "end": 3661.02, "text": " But if you don't, then you shouldn't.", "tokens": [583, 498, 291, 500, 380, 11, 550, 291, 4659, 380, 13], "temperature": 0.0, "avg_logprob": -0.15024488530260452, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.9222619812353514e-05}, {"id": 872, "seek": 364052, "start": 3661.02, "end": 3666.96, "text": " This one called dihedral is just one that does every possible rotation and flip, and", "tokens": [639, 472, 1219, 1026, 71, 24764, 307, 445, 472, 300, 775, 633, 1944, 12447, 293, 7929, 11, 293], "temperature": 0.0, "avg_logprob": -0.15024488530260452, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.9222619812353514e-05}, {"id": 873, "seek": 366696, "start": 3666.96, "end": 3671.2400000000002, "text": " so obviously most of your pictures are not going to be upside down cats.", "tokens": [370, 2745, 881, 295, 428, 5242, 366, 406, 516, 281, 312, 14119, 760, 11111, 13], "temperature": 0.0, "avg_logprob": -0.15705264242071854, "compression_ratio": 1.641732283464567, "no_speech_prob": 2.482448053342523e-06}, {"id": 874, "seek": 366696, "start": 3671.2400000000002, "end": 3673.56, "text": " So you probably would say, hey, this doesn't make sense.", "tokens": [407, 291, 1391, 576, 584, 11, 4177, 11, 341, 1177, 380, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.15705264242071854, "compression_ratio": 1.641732283464567, "no_speech_prob": 2.482448053342523e-06}, {"id": 875, "seek": 366696, "start": 3673.56, "end": 3675.6, "text": " I won't use this for this dataset.", "tokens": [286, 1582, 380, 764, 341, 337, 341, 28872, 13], "temperature": 0.0, "avg_logprob": -0.15705264242071854, "compression_ratio": 1.641732283464567, "no_speech_prob": 2.482448053342523e-06}, {"id": 876, "seek": 366696, "start": 3675.6, "end": 3680.2, "text": " But if you're looking at satellite images, of course you would.", "tokens": [583, 498, 291, 434, 1237, 412, 16016, 5267, 11, 295, 1164, 291, 576, 13], "temperature": 0.0, "avg_logprob": -0.15705264242071854, "compression_ratio": 1.641732283464567, "no_speech_prob": 2.482448053342523e-06}, {"id": 877, "seek": 366696, "start": 3680.2, "end": 3682.8, "text": " On the other hand, flip makes perfect sense.", "tokens": [1282, 264, 661, 1011, 11, 7929, 1669, 2176, 2020, 13], "temperature": 0.0, "avg_logprob": -0.15705264242071854, "compression_ratio": 1.641732283464567, "no_speech_prob": 2.482448053342523e-06}, {"id": 878, "seek": 366696, "start": 3682.8, "end": 3688.2, "text": " So you would include that.", "tokens": [407, 291, 576, 4090, 300, 13], "temperature": 0.0, "avg_logprob": -0.15705264242071854, "compression_ratio": 1.641732283464567, "no_speech_prob": 2.482448053342523e-06}, {"id": 879, "seek": 366696, "start": 3688.2, "end": 3693.56, "text": " A lot of things that you can do with fast.ai lets you pick a padding mode, and this is", "tokens": [316, 688, 295, 721, 300, 291, 393, 360, 365, 2370, 13, 1301, 6653, 291, 1888, 257, 39562, 4391, 11, 293, 341, 307], "temperature": 0.0, "avg_logprob": -0.15705264242071854, "compression_ratio": 1.641732283464567, "no_speech_prob": 2.482448053342523e-06}, {"id": 880, "seek": 366696, "start": 3693.56, "end": 3694.8, "text": " what padding mode looks like.", "tokens": [437, 39562, 4391, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.15705264242071854, "compression_ratio": 1.641732283464567, "no_speech_prob": 2.482448053342523e-06}, {"id": 881, "seek": 369480, "start": 3694.8, "end": 3701.96, "text": " You can pick zeros, you can pick border, which just replicates, or you can pick reflection,", "tokens": [509, 393, 1888, 35193, 11, 291, 393, 1888, 7838, 11, 597, 445, 3248, 299, 1024, 11, 420, 291, 393, 1888, 12914, 11], "temperature": 0.0, "avg_logprob": -0.13323375826976339, "compression_ratio": 1.7169811320754718, "no_speech_prob": 9.665720426710322e-06}, {"id": 882, "seek": 369480, "start": 3701.96, "end": 3707.28, "text": " which as you can see is as if the last little few pixels are in a mirror.", "tokens": [597, 382, 291, 393, 536, 307, 382, 498, 264, 1036, 707, 1326, 18668, 366, 294, 257, 8013, 13], "temperature": 0.0, "avg_logprob": -0.13323375826976339, "compression_ratio": 1.7169811320754718, "no_speech_prob": 9.665720426710322e-06}, {"id": 883, "seek": 369480, "start": 3707.28, "end": 3710.0, "text": " Reflections nearly always better, by the way.", "tokens": [16957, 1809, 626, 6217, 1009, 1101, 11, 538, 264, 636, 13], "temperature": 0.0, "avg_logprob": -0.13323375826976339, "compression_ratio": 1.7169811320754718, "no_speech_prob": 9.665720426710322e-06}, {"id": 884, "seek": 369480, "start": 3710.0, "end": 3714.36, "text": " I don't know that anybody else has really studied this, but we have studied it in some", "tokens": [286, 500, 380, 458, 300, 4472, 1646, 575, 534, 9454, 341, 11, 457, 321, 362, 9454, 309, 294, 512], "temperature": 0.0, "avg_logprob": -0.13323375826976339, "compression_ratio": 1.7169811320754718, "no_speech_prob": 9.665720426710322e-06}, {"id": 885, "seek": 369480, "start": 3714.36, "end": 3715.36, "text": " depth.", "tokens": [7161, 13], "temperature": 0.0, "avg_logprob": -0.13323375826976339, "compression_ratio": 1.7169811320754718, "no_speech_prob": 9.665720426710322e-06}, {"id": 886, "seek": 369480, "start": 3715.36, "end": 3718.8, "text": " Haven't actually written a paper about it, but just enough for our own purposes to say", "tokens": [23770, 380, 767, 3720, 257, 3035, 466, 309, 11, 457, 445, 1547, 337, 527, 1065, 9932, 281, 584], "temperature": 0.0, "avg_logprob": -0.13323375826976339, "compression_ratio": 1.7169811320754718, "no_speech_prob": 9.665720426710322e-06}, {"id": 887, "seek": 369480, "start": 3718.8, "end": 3721.52, "text": " reflection works best most of the time.", "tokens": [12914, 1985, 1151, 881, 295, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.13323375826976339, "compression_ratio": 1.7169811320754718, "no_speech_prob": 9.665720426710322e-06}, {"id": 888, "seek": 369480, "start": 3721.52, "end": 3724.6800000000003, "text": " So that's the default.", "tokens": [407, 300, 311, 264, 7576, 13], "temperature": 0.0, "avg_logprob": -0.13323375826976339, "compression_ratio": 1.7169811320754718, "no_speech_prob": 9.665720426710322e-06}, {"id": 889, "seek": 372468, "start": 3724.68, "end": 3732.0, "text": " Then there's a really cool bunch of perspective warping ones, which I'll probably show you", "tokens": [1396, 456, 311, 257, 534, 1627, 3840, 295, 4585, 1516, 3381, 2306, 11, 597, 286, 603, 1391, 855, 291], "temperature": 0.0, "avg_logprob": -0.14165097362590287, "compression_ratio": 1.636734693877551, "no_speech_prob": 5.307320680003613e-05}, {"id": 890, "seek": 372468, "start": 3732.0, "end": 3734.64, "text": " by using symmetric warp.", "tokens": [538, 1228, 32330, 36030, 13], "temperature": 0.0, "avg_logprob": -0.14165097362590287, "compression_ratio": 1.636734693877551, "no_speech_prob": 5.307320680003613e-05}, {"id": 891, "seek": 372468, "start": 3734.64, "end": 3738.16, "text": " If you look at the kind of the, we've added black borders to this so it's more obvious", "tokens": [759, 291, 574, 412, 264, 733, 295, 264, 11, 321, 600, 3869, 2211, 16287, 281, 341, 370, 309, 311, 544, 6322], "temperature": 0.0, "avg_logprob": -0.14165097362590287, "compression_ratio": 1.636734693877551, "no_speech_prob": 5.307320680003613e-05}, {"id": 892, "seek": 372468, "start": 3738.16, "end": 3739.72, "text": " for what's going on.", "tokens": [337, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.14165097362590287, "compression_ratio": 1.636734693877551, "no_speech_prob": 5.307320680003613e-05}, {"id": 893, "seek": 372468, "start": 3739.72, "end": 3744.52, "text": " And as you can see, what symmetric warp is doing, it's as if the camera is being moved", "tokens": [400, 382, 291, 393, 536, 11, 437, 32330, 36030, 307, 884, 11, 309, 311, 382, 498, 264, 2799, 307, 885, 4259], "temperature": 0.0, "avg_logprob": -0.14165097362590287, "compression_ratio": 1.636734693877551, "no_speech_prob": 5.307320680003613e-05}, {"id": 894, "seek": 372468, "start": 3744.52, "end": 3753.44, "text": " above or to the side of the object and literally warping the whole thing like that, right?", "tokens": [3673, 420, 281, 264, 1252, 295, 264, 2657, 293, 3736, 1516, 3381, 264, 1379, 551, 411, 300, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14165097362590287, "compression_ratio": 1.636734693877551, "no_speech_prob": 5.307320680003613e-05}, {"id": 895, "seek": 375344, "start": 3753.44, "end": 3758.28, "text": " So the cool thing is that as you can see, each of these pictures, it's as if this cat", "tokens": [407, 264, 1627, 551, 307, 300, 382, 291, 393, 536, 11, 1184, 295, 613, 5242, 11, 309, 311, 382, 498, 341, 3857], "temperature": 0.0, "avg_logprob": -0.09942847910061689, "compression_ratio": 1.7263513513513513, "no_speech_prob": 1.3631072761199903e-05}, {"id": 896, "seek": 375344, "start": 3758.28, "end": 3761.44, "text": " was being taken kind of from different angles, right?", "tokens": [390, 885, 2726, 733, 295, 490, 819, 14708, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.09942847910061689, "compression_ratio": 1.7263513513513513, "no_speech_prob": 1.3631072761199903e-05}, {"id": 897, "seek": 375344, "start": 3761.44, "end": 3765.2400000000002, "text": " So they're all kind of optically sensible, right?", "tokens": [407, 436, 434, 439, 733, 295, 2427, 984, 25380, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.09942847910061689, "compression_ratio": 1.7263513513513513, "no_speech_prob": 1.3631072761199903e-05}, {"id": 898, "seek": 375344, "start": 3765.2400000000002, "end": 3769.34, "text": " And so this is a really great type of data augmentation.", "tokens": [400, 370, 341, 307, 257, 534, 869, 2010, 295, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.09942847910061689, "compression_ratio": 1.7263513513513513, "no_speech_prob": 1.3631072761199903e-05}, {"id": 899, "seek": 375344, "start": 3769.34, "end": 3773.0, "text": " It's also one which I don't know of any other library that does it, or at least certainly", "tokens": [467, 311, 611, 472, 597, 286, 500, 380, 458, 295, 604, 661, 6405, 300, 775, 309, 11, 420, 412, 1935, 3297], "temperature": 0.0, "avg_logprob": -0.09942847910061689, "compression_ratio": 1.7263513513513513, "no_speech_prob": 1.3631072761199903e-05}, {"id": 900, "seek": 375344, "start": 3773.0, "end": 3778.56, "text": " one that does it in a way that's both fast and keeps the image crisp, as it is in fast", "tokens": [472, 300, 775, 309, 294, 257, 636, 300, 311, 1293, 2370, 293, 5965, 264, 3256, 22952, 11, 382, 309, 307, 294, 2370], "temperature": 0.0, "avg_logprob": -0.09942847910061689, "compression_ratio": 1.7263513513513513, "no_speech_prob": 1.3631072761199903e-05}, {"id": 901, "seek": 375344, "start": 3778.56, "end": 3779.56, "text": " AI.", "tokens": [7318, 13], "temperature": 0.0, "avg_logprob": -0.09942847910061689, "compression_ratio": 1.7263513513513513, "no_speech_prob": 1.3631072761199903e-05}, {"id": 902, "seek": 375344, "start": 3779.56, "end": 3782.32, "text": " So this is like, if you're looking to win a Kaggle competition, this is the kind of", "tokens": [407, 341, 307, 411, 11, 498, 291, 434, 1237, 281, 1942, 257, 48751, 22631, 6211, 11, 341, 307, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.09942847910061689, "compression_ratio": 1.7263513513513513, "no_speech_prob": 1.3631072761199903e-05}, {"id": 903, "seek": 378232, "start": 3782.32, "end": 3788.6400000000003, "text": " thing that's going to get you above the people that aren't using the fast AI library.", "tokens": [551, 300, 311, 516, 281, 483, 291, 3673, 264, 561, 300, 3212, 380, 1228, 264, 2370, 7318, 6405, 13], "temperature": 0.0, "avg_logprob": -0.11520185253836891, "compression_ratio": 1.703883495145631, "no_speech_prob": 3.726590193764423e-06}, {"id": 904, "seek": 378232, "start": 3788.6400000000003, "end": 3796.96, "text": " So having looked at all that, we are going to add this, have a little get data function", "tokens": [407, 1419, 2956, 412, 439, 300, 11, 321, 366, 516, 281, 909, 341, 11, 362, 257, 707, 483, 1412, 2445], "temperature": 0.0, "avg_logprob": -0.11520185253836891, "compression_ratio": 1.703883495145631, "no_speech_prob": 3.726590193764423e-06}, {"id": 905, "seek": 378232, "start": 3796.96, "end": 3802.36, "text": " that just does the usual data block stuff, but we're going to add padding mode explicitly", "tokens": [300, 445, 775, 264, 7713, 1412, 3461, 1507, 11, 457, 321, 434, 516, 281, 909, 39562, 4391, 20803], "temperature": 0.0, "avg_logprob": -0.11520185253836891, "compression_ratio": 1.703883495145631, "no_speech_prob": 3.726590193764423e-06}, {"id": 906, "seek": 378232, "start": 3802.36, "end": 3808.96, "text": " so that we can turn on padding mode of zeros just so we can see what's going on better.", "tokens": [370, 300, 321, 393, 1261, 322, 39562, 4391, 295, 35193, 445, 370, 321, 393, 536, 437, 311, 516, 322, 1101, 13], "temperature": 0.0, "avg_logprob": -0.11520185253836891, "compression_ratio": 1.703883495145631, "no_speech_prob": 3.726590193764423e-06}, {"id": 907, "seek": 380896, "start": 3808.96, "end": 3813.7200000000003, "text": " Fast AI has this handy little function called plotMulti, which is going to create a 3x3 grid", "tokens": [15968, 7318, 575, 341, 13239, 707, 2445, 1219, 7542, 44, 723, 72, 11, 597, 307, 516, 281, 1884, 257, 805, 87, 18, 10748], "temperature": 0.0, "avg_logprob": -0.11093070818030316, "compression_ratio": 1.7384615384615385, "no_speech_prob": 5.771852556790691e-06}, {"id": 908, "seek": 380896, "start": 3813.7200000000003, "end": 3820.4, "text": " of plots, and each one will contain the result of calling this function, which will receive", "tokens": [295, 28609, 11, 293, 1184, 472, 486, 5304, 264, 1874, 295, 5141, 341, 2445, 11, 597, 486, 4774], "temperature": 0.0, "avg_logprob": -0.11093070818030316, "compression_ratio": 1.7384615384615385, "no_speech_prob": 5.771852556790691e-06}, {"id": 909, "seek": 380896, "start": 3820.4, "end": 3823.18, "text": " the plot coordinates and the axis.", "tokens": [264, 7542, 21056, 293, 264, 10298, 13], "temperature": 0.0, "avg_logprob": -0.11093070818030316, "compression_ratio": 1.7384615384615385, "no_speech_prob": 5.771852556790691e-06}, {"id": 910, "seek": 380896, "start": 3823.18, "end": 3827.16, "text": " And so I'm actually going to plot the exact same thing in every box, but because this", "tokens": [400, 370, 286, 478, 767, 516, 281, 7542, 264, 1900, 912, 551, 294, 633, 2424, 11, 457, 570, 341], "temperature": 0.0, "avg_logprob": -0.11093070818030316, "compression_ratio": 1.7384615384615385, "no_speech_prob": 5.771852556790691e-06}, {"id": 911, "seek": 380896, "start": 3827.16, "end": 3830.6, "text": " is a training data set, it's going to use data augmentation.", "tokens": [307, 257, 3097, 1412, 992, 11, 309, 311, 516, 281, 764, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.11093070818030316, "compression_ratio": 1.7384615384615385, "no_speech_prob": 5.771852556790691e-06}, {"id": 912, "seek": 380896, "start": 3830.6, "end": 3838.86, "text": " And so you can see the same doggy using lots of different kinds of data augmentation.", "tokens": [400, 370, 291, 393, 536, 264, 912, 3000, 1480, 1228, 3195, 295, 819, 3685, 295, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.11093070818030316, "compression_ratio": 1.7384615384615385, "no_speech_prob": 5.771852556790691e-06}, {"id": 913, "seek": 383886, "start": 3838.86, "end": 3842.6800000000003, "text": " And so you can see why this is going to work really well, because these pictures all look", "tokens": [400, 370, 291, 393, 536, 983, 341, 307, 516, 281, 589, 534, 731, 11, 570, 613, 5242, 439, 574], "temperature": 0.0, "avg_logprob": -0.13122635097294064, "compression_ratio": 1.5874439461883407, "no_speech_prob": 5.862766101927264e-06}, {"id": 914, "seek": 383886, "start": 3842.6800000000003, "end": 3845.2000000000003, "text": " pretty different.", "tokens": [1238, 819, 13], "temperature": 0.0, "avg_logprob": -0.13122635097294064, "compression_ratio": 1.5874439461883407, "no_speech_prob": 5.862766101927264e-06}, {"id": 915, "seek": 383886, "start": 3845.2000000000003, "end": 3849.8, "text": " But we didn't have to do any extra hand labeling or anything.", "tokens": [583, 321, 994, 380, 362, 281, 360, 604, 2857, 1011, 40244, 420, 1340, 13], "temperature": 0.0, "avg_logprob": -0.13122635097294064, "compression_ratio": 1.5874439461883407, "no_speech_prob": 5.862766101927264e-06}, {"id": 916, "seek": 383886, "start": 3849.8, "end": 3852.58, "text": " That's like free extra data.", "tokens": [663, 311, 411, 1737, 2857, 1412, 13], "temperature": 0.0, "avg_logprob": -0.13122635097294064, "compression_ratio": 1.5874439461883407, "no_speech_prob": 5.862766101927264e-06}, {"id": 917, "seek": 383886, "start": 3852.58, "end": 3856.0, "text": " So data augmentation is really, really great.", "tokens": [407, 1412, 14501, 19631, 307, 534, 11, 534, 869, 13], "temperature": 0.0, "avg_logprob": -0.13122635097294064, "compression_ratio": 1.5874439461883407, "no_speech_prob": 5.862766101927264e-06}, {"id": 918, "seek": 383886, "start": 3856.0, "end": 3862.02, "text": " And one of the big opportunities for research is to figure out ways to do data augmentation", "tokens": [400, 472, 295, 264, 955, 4786, 337, 2132, 307, 281, 2573, 484, 2098, 281, 360, 1412, 14501, 19631], "temperature": 0.0, "avg_logprob": -0.13122635097294064, "compression_ratio": 1.5874439461883407, "no_speech_prob": 5.862766101927264e-06}, {"id": 919, "seek": 383886, "start": 3862.02, "end": 3863.7000000000003, "text": " in other domains.", "tokens": [294, 661, 25514, 13], "temperature": 0.0, "avg_logprob": -0.13122635097294064, "compression_ratio": 1.5874439461883407, "no_speech_prob": 5.862766101927264e-06}, {"id": 920, "seek": 386370, "start": 3863.7, "end": 3872.96, "text": " So how can you do data augmentation with text data, or genomic data, or histopathology data,", "tokens": [407, 577, 393, 291, 360, 1412, 14501, 19631, 365, 2487, 1412, 11, 420, 1049, 21401, 1412, 11, 420, 1758, 27212, 1793, 1412, 11], "temperature": 0.0, "avg_logprob": -0.13079263203179659, "compression_ratio": 1.4450867052023122, "no_speech_prob": 1.154456299445883e-06}, {"id": 921, "seek": 386370, "start": 3872.96, "end": 3876.48, "text": " or whatever?", "tokens": [420, 2035, 30], "temperature": 0.0, "avg_logprob": -0.13079263203179659, "compression_ratio": 1.4450867052023122, "no_speech_prob": 1.154456299445883e-06}, {"id": 922, "seek": 386370, "start": 3876.48, "end": 3879.62, "text": " Almost nobody's looking at that, and to me it's one of the biggest opportunities that", "tokens": [12627, 5079, 311, 1237, 412, 300, 11, 293, 281, 385, 309, 311, 472, 295, 264, 3880, 4786, 300], "temperature": 0.0, "avg_logprob": -0.13079263203179659, "compression_ratio": 1.4450867052023122, "no_speech_prob": 1.154456299445883e-06}, {"id": 923, "seek": 386370, "start": 3879.62, "end": 3887.52, "text": " could let you decrease data requirements by like 5 to 10x.", "tokens": [727, 718, 291, 11514, 1412, 7728, 538, 411, 1025, 281, 1266, 87, 13], "temperature": 0.0, "avg_logprob": -0.13079263203179659, "compression_ratio": 1.4450867052023122, "no_speech_prob": 1.154456299445883e-06}, {"id": 924, "seek": 388752, "start": 3887.52, "end": 3893.68, "text": " So here's the same thing again, but with reflection padding instead of zero padding.", "tokens": [407, 510, 311, 264, 912, 551, 797, 11, 457, 365, 12914, 39562, 2602, 295, 4018, 39562, 13], "temperature": 0.0, "avg_logprob": -0.17365924171779468, "compression_ratio": 1.6726457399103138, "no_speech_prob": 9.422405469194928e-07}, {"id": 925, "seek": 388752, "start": 3893.68, "end": 3898.92, "text": " And you can kind of see, like see this doggy's legs are actually being reflected at the bottom", "tokens": [400, 291, 393, 733, 295, 536, 11, 411, 536, 341, 3000, 1480, 311, 5668, 366, 767, 885, 15502, 412, 264, 2767], "temperature": 0.0, "avg_logprob": -0.17365924171779468, "compression_ratio": 1.6726457399103138, "no_speech_prob": 9.422405469194928e-07}, {"id": 926, "seek": 388752, "start": 3898.92, "end": 3900.96, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.17365924171779468, "compression_ratio": 1.6726457399103138, "no_speech_prob": 9.422405469194928e-07}, {"id": 927, "seek": 388752, "start": 3900.96, "end": 3908.24, "text": " So reflection padding tends to create images that are kind of much more naturally reasonable.", "tokens": [407, 12914, 39562, 12258, 281, 1884, 5267, 300, 366, 733, 295, 709, 544, 8195, 10585, 13], "temperature": 0.0, "avg_logprob": -0.17365924171779468, "compression_ratio": 1.6726457399103138, "no_speech_prob": 9.422405469194928e-07}, {"id": 928, "seek": 388752, "start": 3908.24, "end": 3910.98, "text": " Like in the real world you don't get black borders like this.", "tokens": [1743, 294, 264, 957, 1002, 291, 500, 380, 483, 2211, 16287, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.17365924171779468, "compression_ratio": 1.6726457399103138, "no_speech_prob": 9.422405469194928e-07}, {"id": 929, "seek": 388752, "start": 3910.98, "end": 3914.12, "text": " So they do seem to work better.", "tokens": [407, 436, 360, 1643, 281, 589, 1101, 13], "temperature": 0.0, "avg_logprob": -0.17365924171779468, "compression_ratio": 1.6726457399103138, "no_speech_prob": 9.422405469194928e-07}, {"id": 930, "seek": 391412, "start": 3914.12, "end": 3919.7599999999998, "text": " Okay, so because we're going to study convolutional neural networks, we are going to create a", "tokens": [1033, 11, 370, 570, 321, 434, 516, 281, 2979, 45216, 304, 18161, 9590, 11, 321, 366, 516, 281, 1884, 257], "temperature": 0.0, "avg_logprob": -0.1569144831294507, "compression_ratio": 1.7450980392156863, "no_speech_prob": 6.540366030094447e-06}, {"id": 931, "seek": 391412, "start": 3919.7599999999998, "end": 3921.6, "text": " convolutional neural network.", "tokens": [45216, 304, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.1569144831294507, "compression_ratio": 1.7450980392156863, "no_speech_prob": 6.540366030094447e-06}, {"id": 932, "seek": 391412, "start": 3921.6, "end": 3923.0, "text": " You know how to create them.", "tokens": [509, 458, 577, 281, 1884, 552, 13], "temperature": 0.0, "avg_logprob": -0.1569144831294507, "compression_ratio": 1.7450980392156863, "no_speech_prob": 6.540366030094447e-06}, {"id": 933, "seek": 391412, "start": 3923.0, "end": 3924.64, "text": " So I'll go ahead and create one.", "tokens": [407, 286, 603, 352, 2286, 293, 1884, 472, 13], "temperature": 0.0, "avg_logprob": -0.1569144831294507, "compression_ratio": 1.7450980392156863, "no_speech_prob": 6.540366030094447e-06}, {"id": 934, "seek": 391412, "start": 3924.64, "end": 3926.56, "text": " I will fit it for a little bit.", "tokens": [286, 486, 3318, 309, 337, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.1569144831294507, "compression_ratio": 1.7450980392156863, "no_speech_prob": 6.540366030094447e-06}, {"id": 935, "seek": 391412, "start": 3926.56, "end": 3928.8399999999997, "text": " I will unfreeze it.", "tokens": [286, 486, 3971, 701, 1381, 309, 13], "temperature": 0.0, "avg_logprob": -0.1569144831294507, "compression_ratio": 1.7450980392156863, "no_speech_prob": 6.540366030094447e-06}, {"id": 936, "seek": 391412, "start": 3928.8399999999997, "end": 3934.2799999999997, "text": " I will then create a larger version of the data set, 352x352, and fit for a little bit", "tokens": [286, 486, 550, 1884, 257, 4833, 3037, 295, 264, 1412, 992, 11, 6976, 17, 87, 8794, 17, 11, 293, 3318, 337, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.1569144831294507, "compression_ratio": 1.7450980392156863, "no_speech_prob": 6.540366030094447e-06}, {"id": 937, "seek": 391412, "start": 3934.2799999999997, "end": 3937.7599999999998, "text": " more.", "tokens": [544, 13], "temperature": 0.0, "avg_logprob": -0.1569144831294507, "compression_ratio": 1.7450980392156863, "no_speech_prob": 6.540366030094447e-06}, {"id": 938, "seek": 391412, "start": 3937.7599999999998, "end": 3939.4, "text": " And I will save it.", "tokens": [400, 286, 486, 3155, 309, 13], "temperature": 0.0, "avg_logprob": -0.1569144831294507, "compression_ratio": 1.7450980392156863, "no_speech_prob": 6.540366030094447e-06}, {"id": 939, "seek": 391412, "start": 3939.4, "end": 3940.4, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1569144831294507, "compression_ratio": 1.7450980392156863, "no_speech_prob": 6.540366030094447e-06}, {"id": 940, "seek": 394040, "start": 3940.4, "end": 3946.84, "text": " So we have a CNN, and we're going to try and figure out what's going on in our CNN.", "tokens": [407, 321, 362, 257, 24859, 11, 293, 321, 434, 516, 281, 853, 293, 2573, 484, 437, 311, 516, 322, 294, 527, 24859, 13], "temperature": 0.0, "avg_logprob": -0.1184167752320739, "compression_ratio": 1.7444444444444445, "no_speech_prob": 6.540375125041464e-06}, {"id": 941, "seek": 394040, "start": 3946.84, "end": 3950.2000000000003, "text": " And the way we're going to try and figure it out is specifically that we're going to", "tokens": [400, 264, 636, 321, 434, 516, 281, 853, 293, 2573, 309, 484, 307, 4682, 300, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.1184167752320739, "compression_ratio": 1.7444444444444445, "no_speech_prob": 6.540375125041464e-06}, {"id": 942, "seek": 394040, "start": 3950.2000000000003, "end": 3957.88, "text": " try to learn how to create this picture.", "tokens": [853, 281, 1466, 577, 281, 1884, 341, 3036, 13], "temperature": 0.0, "avg_logprob": -0.1184167752320739, "compression_ratio": 1.7444444444444445, "no_speech_prob": 6.540375125041464e-06}, {"id": 943, "seek": 394040, "start": 3957.88, "end": 3959.92, "text": " This is a heat map.", "tokens": [639, 307, 257, 3738, 4471, 13], "temperature": 0.0, "avg_logprob": -0.1184167752320739, "compression_ratio": 1.7444444444444445, "no_speech_prob": 6.540375125041464e-06}, {"id": 944, "seek": 394040, "start": 3959.92, "end": 3966.96, "text": " This is a picture which shows me what part of the image that the CNN focused on when", "tokens": [639, 307, 257, 3036, 597, 3110, 385, 437, 644, 295, 264, 3256, 300, 264, 24859, 5178, 322, 562], "temperature": 0.0, "avg_logprob": -0.1184167752320739, "compression_ratio": 1.7444444444444445, "no_speech_prob": 6.540375125041464e-06}, {"id": 945, "seek": 396696, "start": 3966.96, "end": 3971.6, "text": " it was trying to decide what this picture is.", "tokens": [309, 390, 1382, 281, 4536, 437, 341, 3036, 307, 13], "temperature": 0.0, "avg_logprob": -0.10571111658568023, "compression_ratio": 1.6521739130434783, "no_speech_prob": 4.0294362406712025e-06}, {"id": 946, "seek": 396696, "start": 3971.6, "end": 3978.92, "text": " So we're going to make this heat map from scratch.", "tokens": [407, 321, 434, 516, 281, 652, 341, 3738, 4471, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.10571111658568023, "compression_ratio": 1.6521739130434783, "no_speech_prob": 4.0294362406712025e-06}, {"id": 947, "seek": 396696, "start": 3978.92, "end": 3985.28, "text": " When we, so we're kind of at a point now in the course where I'm assuming that if you've", "tokens": [1133, 321, 11, 370, 321, 434, 733, 295, 412, 257, 935, 586, 294, 264, 1164, 689, 286, 478, 11926, 300, 498, 291, 600], "temperature": 0.0, "avg_logprob": -0.10571111658568023, "compression_ratio": 1.6521739130434783, "no_speech_prob": 4.0294362406712025e-06}, {"id": 948, "seek": 396696, "start": 3985.28, "end": 3991.26, "text": " got to this point, you know, and you're still here, thank you, then you're interested enough", "tokens": [658, 281, 341, 935, 11, 291, 458, 11, 293, 291, 434, 920, 510, 11, 1309, 291, 11, 550, 291, 434, 3102, 1547], "temperature": 0.0, "avg_logprob": -0.10571111658568023, "compression_ratio": 1.6521739130434783, "no_speech_prob": 4.0294362406712025e-06}, {"id": 949, "seek": 396696, "start": 3991.26, "end": 3994.0, "text": " that you're prepared to kind of dig into some of these details.", "tokens": [300, 291, 434, 4927, 281, 733, 295, 2528, 666, 512, 295, 613, 4365, 13], "temperature": 0.0, "avg_logprob": -0.10571111658568023, "compression_ratio": 1.6521739130434783, "no_speech_prob": 4.0294362406712025e-06}, {"id": 950, "seek": 399400, "start": 3994.0, "end": 3999.84, "text": " So we're actually going to learn how to create this heat map without almost any fast AI stuff.", "tokens": [407, 321, 434, 767, 516, 281, 1466, 577, 281, 1884, 341, 3738, 4471, 1553, 1920, 604, 2370, 7318, 1507, 13], "temperature": 0.0, "avg_logprob": -0.08839871861913183, "compression_ratio": 1.6747967479674797, "no_speech_prob": 1.5779456816744641e-06}, {"id": 951, "seek": 399400, "start": 3999.84, "end": 4005.08, "text": " We're going to use pure kind of tensor arithmetic in PyTorch, and we're going to try and use", "tokens": [492, 434, 516, 281, 764, 6075, 733, 295, 40863, 42973, 294, 9953, 51, 284, 339, 11, 293, 321, 434, 516, 281, 853, 293, 764], "temperature": 0.0, "avg_logprob": -0.08839871861913183, "compression_ratio": 1.6747967479674797, "no_speech_prob": 1.5779456816744641e-06}, {"id": 952, "seek": 399400, "start": 4005.08, "end": 4007.0, "text": " that to really understand what's going on.", "tokens": [300, 281, 534, 1223, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.08839871861913183, "compression_ratio": 1.6747967479674797, "no_speech_prob": 1.5779456816744641e-06}, {"id": 953, "seek": 399400, "start": 4007.0, "end": 4013.1, "text": " So to warn you, none of it's rocket science, but a lot of it's going to look really new.", "tokens": [407, 281, 12286, 291, 11, 6022, 295, 309, 311, 13012, 3497, 11, 457, 257, 688, 295, 309, 311, 516, 281, 574, 534, 777, 13], "temperature": 0.0, "avg_logprob": -0.08839871861913183, "compression_ratio": 1.6747967479674797, "no_speech_prob": 1.5779456816744641e-06}, {"id": 954, "seek": 399400, "start": 4013.1, "end": 4020.0, "text": " So don't expect to get it the first time, but expect to like listen, jump into the notebook,", "tokens": [407, 500, 380, 2066, 281, 483, 309, 264, 700, 565, 11, 457, 2066, 281, 411, 2140, 11, 3012, 666, 264, 21060, 11], "temperature": 0.0, "avg_logprob": -0.08839871861913183, "compression_ratio": 1.6747967479674797, "no_speech_prob": 1.5779456816744641e-06}, {"id": 955, "seek": 402000, "start": 4020.0, "end": 4025.38, "text": " try a few things, test things out, look particularly at like tensor shapes and inputs and outputs", "tokens": [853, 257, 1326, 721, 11, 1500, 721, 484, 11, 574, 4098, 412, 411, 40863, 10854, 293, 15743, 293, 23930], "temperature": 0.0, "avg_logprob": -0.13009087026935734, "compression_ratio": 1.7377049180327868, "no_speech_prob": 7.889127118687611e-06}, {"id": 956, "seek": 402000, "start": 4025.38, "end": 4029.44, "text": " to check your understanding, then go back and listen again, and kind of try it a few", "tokens": [281, 1520, 428, 3701, 11, 550, 352, 646, 293, 2140, 797, 11, 293, 733, 295, 853, 309, 257, 1326], "temperature": 0.0, "avg_logprob": -0.13009087026935734, "compression_ratio": 1.7377049180327868, "no_speech_prob": 7.889127118687611e-06}, {"id": 957, "seek": 402000, "start": 4029.44, "end": 4033.36, "text": " times because you will get there, right?", "tokens": [1413, 570, 291, 486, 483, 456, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13009087026935734, "compression_ratio": 1.7377049180327868, "no_speech_prob": 7.889127118687611e-06}, {"id": 958, "seek": 402000, "start": 4033.36, "end": 4037.32, "text": " It's just that there's going to be a lot of new concepts because we haven't done that", "tokens": [467, 311, 445, 300, 456, 311, 516, 281, 312, 257, 688, 295, 777, 10392, 570, 321, 2378, 380, 1096, 300], "temperature": 0.0, "avg_logprob": -0.13009087026935734, "compression_ratio": 1.7377049180327868, "no_speech_prob": 7.889127118687611e-06}, {"id": 959, "seek": 402000, "start": 4037.32, "end": 4039.28, "text": " much stuff in pure PyTorch.", "tokens": [709, 1507, 294, 6075, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.13009087026935734, "compression_ratio": 1.7377049180327868, "no_speech_prob": 7.889127118687611e-06}, {"id": 960, "seek": 402000, "start": 4039.28, "end": 4044.4, "text": " Okay, so what we're going to do is we're going to have a seven minute break, and then we're", "tokens": [1033, 11, 370, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 362, 257, 3407, 3456, 1821, 11, 293, 550, 321, 434], "temperature": 0.0, "avg_logprob": -0.13009087026935734, "compression_ratio": 1.7377049180327868, "no_speech_prob": 7.889127118687611e-06}, {"id": 961, "seek": 402000, "start": 4044.4, "end": 4047.92, "text": " going to come back and we're going to learn all about the innards of a CNN.", "tokens": [516, 281, 808, 646, 293, 321, 434, 516, 281, 1466, 439, 466, 264, 7714, 2287, 295, 257, 24859, 13], "temperature": 0.0, "avg_logprob": -0.13009087026935734, "compression_ratio": 1.7377049180327868, "no_speech_prob": 7.889127118687611e-06}, {"id": 962, "seek": 404792, "start": 4047.92, "end": 4053.2000000000003, "text": " So I'll see you at 7.50.", "tokens": [407, 286, 603, 536, 291, 412, 1614, 13, 2803, 13], "temperature": 0.0, "avg_logprob": -0.08610837800162179, "compression_ratio": 1.4576271186440677, "no_speech_prob": 4.637804522644728e-06}, {"id": 963, "seek": 404792, "start": 4053.2000000000003, "end": 4059.64, "text": " So let's learn about convolutional neural networks.", "tokens": [407, 718, 311, 1466, 466, 45216, 304, 18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.08610837800162179, "compression_ratio": 1.4576271186440677, "no_speech_prob": 4.637804522644728e-06}, {"id": 964, "seek": 404792, "start": 4059.64, "end": 4068.32, "text": " You know, the funny thing is it's pretty unusual to get close to the end of a course and only", "tokens": [509, 458, 11, 264, 4074, 551, 307, 309, 311, 1238, 10901, 281, 483, 1998, 281, 264, 917, 295, 257, 1164, 293, 787], "temperature": 0.0, "avg_logprob": -0.08610837800162179, "compression_ratio": 1.4576271186440677, "no_speech_prob": 4.637804522644728e-06}, {"id": 965, "seek": 404792, "start": 4068.32, "end": 4074.76, "text": " then look at convolutions, but like when you think about it, knowing actually how batch", "tokens": [550, 574, 412, 3754, 15892, 11, 457, 411, 562, 291, 519, 466, 309, 11, 5276, 767, 577, 15245], "temperature": 0.0, "avg_logprob": -0.08610837800162179, "compression_ratio": 1.4576271186440677, "no_speech_prob": 4.637804522644728e-06}, {"id": 966, "seek": 407476, "start": 4074.76, "end": 4083.4, "text": " norm works or how dropout works or how convolutions work isn't nearly as important as knowing", "tokens": [2026, 1985, 420, 577, 3270, 346, 1985, 420, 577, 3754, 15892, 589, 1943, 380, 6217, 382, 1021, 382, 5276], "temperature": 0.0, "avg_logprob": -0.10977554321289062, "compression_ratio": 1.5988700564971752, "no_speech_prob": 3.071734317927621e-05}, {"id": 967, "seek": 407476, "start": 4083.4, "end": 4087.0800000000004, "text": " how it all goes together and what to do with them and how to figure out how to do those", "tokens": [577, 309, 439, 1709, 1214, 293, 437, 281, 360, 365, 552, 293, 577, 281, 2573, 484, 577, 281, 360, 729], "temperature": 0.0, "avg_logprob": -0.10977554321289062, "compression_ratio": 1.5988700564971752, "no_speech_prob": 3.071734317927621e-05}, {"id": 968, "seek": 407476, "start": 4087.0800000000004, "end": 4089.92, "text": " things better.", "tokens": [721, 1101, 13], "temperature": 0.0, "avg_logprob": -0.10977554321289062, "compression_ratio": 1.5988700564971752, "no_speech_prob": 3.071734317927621e-05}, {"id": 969, "seek": 407476, "start": 4089.92, "end": 4099.16, "text": " But it's, you know, we're kind of at a point now where we want to be able to do things", "tokens": [583, 309, 311, 11, 291, 458, 11, 321, 434, 733, 295, 412, 257, 935, 586, 689, 321, 528, 281, 312, 1075, 281, 360, 721], "temperature": 0.0, "avg_logprob": -0.10977554321289062, "compression_ratio": 1.5988700564971752, "no_speech_prob": 3.071734317927621e-05}, {"id": 970, "seek": 409916, "start": 4099.16, "end": 4105.0, "text": " like that, and although, you know, we're adding this functionality directly into the library", "tokens": [411, 300, 11, 293, 4878, 11, 291, 458, 11, 321, 434, 5127, 341, 14980, 3838, 666, 264, 6405], "temperature": 0.0, "avg_logprob": -0.0937781086674443, "compression_ratio": 1.8101694915254238, "no_speech_prob": 3.48066896549426e-05}, {"id": 971, "seek": 409916, "start": 4105.0, "end": 4109.24, "text": " so you can kind of run a function to do that, you know, the more you do, the more you'll", "tokens": [370, 291, 393, 733, 295, 1190, 257, 2445, 281, 360, 300, 11, 291, 458, 11, 264, 544, 291, 360, 11, 264, 544, 291, 603], "temperature": 0.0, "avg_logprob": -0.0937781086674443, "compression_ratio": 1.8101694915254238, "no_speech_prob": 3.48066896549426e-05}, {"id": 972, "seek": 409916, "start": 4109.24, "end": 4113.2, "text": " find things that you want to do a little bit differently to how we do them.", "tokens": [915, 721, 300, 291, 528, 281, 360, 257, 707, 857, 7614, 281, 577, 321, 360, 552, 13], "temperature": 0.0, "avg_logprob": -0.0937781086674443, "compression_ratio": 1.8101694915254238, "no_speech_prob": 3.48066896549426e-05}, {"id": 973, "seek": 409916, "start": 4113.2, "end": 4117.0, "text": " Or there'll be something in your domain where you think like, oh, I could do a slight variation", "tokens": [1610, 456, 603, 312, 746, 294, 428, 9274, 689, 291, 519, 411, 11, 1954, 11, 286, 727, 360, 257, 4036, 12990], "temperature": 0.0, "avg_logprob": -0.0937781086674443, "compression_ratio": 1.8101694915254238, "no_speech_prob": 3.48066896549426e-05}, {"id": 974, "seek": 409916, "start": 4117.0, "end": 4118.0, "text": " of that.", "tokens": [295, 300, 13], "temperature": 0.0, "avg_logprob": -0.0937781086674443, "compression_ratio": 1.8101694915254238, "no_speech_prob": 3.48066896549426e-05}, {"id": 975, "seek": 409916, "start": 4118.0, "end": 4122.68, "text": " So you're kind of getting to a point in your experience now where it helps to know how", "tokens": [407, 291, 434, 733, 295, 1242, 281, 257, 935, 294, 428, 1752, 586, 689, 309, 3665, 281, 458, 577], "temperature": 0.0, "avg_logprob": -0.0937781086674443, "compression_ratio": 1.8101694915254238, "no_speech_prob": 3.48066896549426e-05}, {"id": 976, "seek": 409916, "start": 4122.68, "end": 4127.26, "text": " to do more stuff yourself, and that means you need to understand what's really going", "tokens": [281, 360, 544, 1507, 1803, 11, 293, 300, 1355, 291, 643, 281, 1223, 437, 311, 534, 516], "temperature": 0.0, "avg_logprob": -0.0937781086674443, "compression_ratio": 1.8101694915254238, "no_speech_prob": 3.48066896549426e-05}, {"id": 977, "seek": 412726, "start": 4127.26, "end": 4132.3, "text": " on behind the scenes.", "tokens": [322, 2261, 264, 8026, 13], "temperature": 0.0, "avg_logprob": -0.11450301170349121, "compression_ratio": 1.4962406015037595, "no_speech_prob": 8.39792573970044e-06}, {"id": 978, "seek": 412726, "start": 4132.3, "end": 4142.68, "text": " So what's really going on behind the scenes is that we are creating a neural network that", "tokens": [407, 437, 311, 534, 516, 322, 2261, 264, 8026, 307, 300, 321, 366, 4084, 257, 18161, 3209, 300], "temperature": 0.0, "avg_logprob": -0.11450301170349121, "compression_ratio": 1.4962406015037595, "no_speech_prob": 8.39792573970044e-06}, {"id": 979, "seek": 412726, "start": 4142.68, "end": 4150.400000000001, "text": " looks a lot like this, right, but rather than doing a matrix multiply here and here and", "tokens": [1542, 257, 688, 411, 341, 11, 558, 11, 457, 2831, 813, 884, 257, 8141, 12972, 510, 293, 510, 293], "temperature": 0.0, "avg_logprob": -0.11450301170349121, "compression_ratio": 1.4962406015037595, "no_speech_prob": 8.39792573970044e-06}, {"id": 980, "seek": 415040, "start": 4150.4, "end": 4158.759999999999, "text": " here, we're actually going to do instead a convolution, and a convolution is just a kind", "tokens": [510, 11, 321, 434, 767, 516, 281, 360, 2602, 257, 45216, 11, 293, 257, 45216, 307, 445, 257, 733], "temperature": 0.0, "avg_logprob": -0.15726682874891493, "compression_ratio": 1.5657894736842106, "no_speech_prob": 1.0782952813315205e-05}, {"id": 981, "seek": 415040, "start": 4158.759999999999, "end": 4164.24, "text": " of matrix multiply which has some interesting properties.", "tokens": [295, 8141, 12972, 597, 575, 512, 1880, 7221, 13], "temperature": 0.0, "avg_logprob": -0.15726682874891493, "compression_ratio": 1.5657894736842106, "no_speech_prob": 1.0782952813315205e-05}, {"id": 982, "seek": 415040, "start": 4164.24, "end": 4170.5599999999995, "text": " You should definitely check out this website, satosa.io.ev, explained visually, where we", "tokens": [509, 820, 2138, 1520, 484, 341, 3144, 11, 3227, 6447, 13, 1004, 13, 13379, 11, 8825, 19622, 11, 689, 321], "temperature": 0.0, "avg_logprob": -0.15726682874891493, "compression_ratio": 1.5657894736842106, "no_speech_prob": 1.0782952813315205e-05}, {"id": 983, "seek": 415040, "start": 4170.5599999999995, "end": 4173.4, "text": " have stolen this beautiful animation.", "tokens": [362, 15900, 341, 2238, 9603, 13], "temperature": 0.0, "avg_logprob": -0.15726682874891493, "compression_ratio": 1.5657894736842106, "no_speech_prob": 1.0782952813315205e-05}, {"id": 984, "seek": 415040, "start": 4173.4, "end": 4178.08, "text": " It's actually a JavaScript thing that you can actually play around with yourself in", "tokens": [467, 311, 767, 257, 15778, 551, 300, 291, 393, 767, 862, 926, 365, 1803, 294], "temperature": 0.0, "avg_logprob": -0.15726682874891493, "compression_ratio": 1.5657894736842106, "no_speech_prob": 1.0782952813315205e-05}, {"id": 985, "seek": 417808, "start": 4178.08, "end": 4184.16, "text": " order to show you how convolutions work, and it's actually showing you a convolution as", "tokens": [1668, 281, 855, 291, 577, 3754, 15892, 589, 11, 293, 309, 311, 767, 4099, 291, 257, 45216, 382], "temperature": 0.0, "avg_logprob": -0.14394191303084383, "compression_ratio": 1.7191489361702128, "no_speech_prob": 1.497079119872069e-05}, {"id": 986, "seek": 417808, "start": 4184.16, "end": 4186.32, "text": " we move around these little red squares.", "tokens": [321, 1286, 926, 613, 707, 2182, 19368, 13], "temperature": 0.0, "avg_logprob": -0.14394191303084383, "compression_ratio": 1.7191489361702128, "no_speech_prob": 1.497079119872069e-05}, {"id": 987, "seek": 417808, "start": 4186.32, "end": 4193.0, "text": " So here's a picture, a black and white or a grayscale picture, and so each 3x3 bit of", "tokens": [407, 510, 311, 257, 3036, 11, 257, 2211, 293, 2418, 420, 257, 677, 3772, 37088, 3036, 11, 293, 370, 1184, 805, 87, 18, 857, 295], "temperature": 0.0, "avg_logprob": -0.14394191303084383, "compression_ratio": 1.7191489361702128, "no_speech_prob": 1.497079119872069e-05}, {"id": 988, "seek": 417808, "start": 4193.0, "end": 4197.84, "text": " this picture, as this red thing moves around, it shows you a different 3x3 part.", "tokens": [341, 3036, 11, 382, 341, 2182, 551, 6067, 926, 11, 309, 3110, 291, 257, 819, 805, 87, 18, 644, 13], "temperature": 0.0, "avg_logprob": -0.14394191303084383, "compression_ratio": 1.7191489361702128, "no_speech_prob": 1.497079119872069e-05}, {"id": 989, "seek": 417808, "start": 4197.84, "end": 4203.12, "text": " It shows you over here the values of the pixels.", "tokens": [467, 3110, 291, 670, 510, 264, 4190, 295, 264, 18668, 13], "temperature": 0.0, "avg_logprob": -0.14394191303084383, "compression_ratio": 1.7191489361702128, "no_speech_prob": 1.497079119872069e-05}, {"id": 990, "seek": 417808, "start": 4203.12, "end": 4206.96, "text": " So in Fast.ai's case, our pixel values are between 0 and 1.", "tokens": [407, 294, 15968, 13, 1301, 311, 1389, 11, 527, 19261, 4190, 366, 1296, 1958, 293, 502, 13], "temperature": 0.0, "avg_logprob": -0.14394191303084383, "compression_ratio": 1.7191489361702128, "no_speech_prob": 1.497079119872069e-05}, {"id": 991, "seek": 420696, "start": 4206.96, "end": 4209.52, "text": " In this case, they're between 0 and 255.", "tokens": [682, 341, 1389, 11, 436, 434, 1296, 1958, 293, 3552, 20, 13], "temperature": 0.0, "avg_logprob": -0.16013658156088734, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.4970874872233253e-05}, {"id": 992, "seek": 420696, "start": 4209.52, "end": 4212.56, "text": " So here are 9 pixel values.", "tokens": [407, 510, 366, 1722, 19261, 4190, 13], "temperature": 0.0, "avg_logprob": -0.16013658156088734, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.4970874872233253e-05}, {"id": 993, "seek": 420696, "start": 4212.56, "end": 4216.76, "text": " This area is pretty white, so they're pretty high numbers.", "tokens": [639, 1859, 307, 1238, 2418, 11, 370, 436, 434, 1238, 1090, 3547, 13], "temperature": 0.0, "avg_logprob": -0.16013658156088734, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.4970874872233253e-05}, {"id": 994, "seek": 420696, "start": 4216.76, "end": 4222.72, "text": " And so as we move around, you can see the 9 big numbers change, and you can also see", "tokens": [400, 370, 382, 321, 1286, 926, 11, 291, 393, 536, 264, 1722, 955, 3547, 1319, 11, 293, 291, 393, 611, 536], "temperature": 0.0, "avg_logprob": -0.16013658156088734, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.4970874872233253e-05}, {"id": 995, "seek": 420696, "start": 4222.72, "end": 4226.76, "text": " their colors change.", "tokens": [641, 4577, 1319, 13], "temperature": 0.0, "avg_logprob": -0.16013658156088734, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.4970874872233253e-05}, {"id": 996, "seek": 420696, "start": 4226.76, "end": 4233.84, "text": " Up here, there's another 9 numbers, and you can see those in the little x1, x2, x1.", "tokens": [5858, 510, 11, 456, 311, 1071, 1722, 3547, 11, 293, 291, 393, 536, 729, 294, 264, 707, 2031, 16, 11, 2031, 17, 11, 2031, 16, 13], "temperature": 0.0, "avg_logprob": -0.16013658156088734, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.4970874872233253e-05}, {"id": 997, "seek": 420696, "start": 4233.84, "end": 4236.12, "text": " Here we are, 1, 2, 1.", "tokens": [1692, 321, 366, 11, 502, 11, 568, 11, 502, 13], "temperature": 0.0, "avg_logprob": -0.16013658156088734, "compression_ratio": 1.599056603773585, "no_speech_prob": 1.4970874872233253e-05}, {"id": 998, "seek": 423612, "start": 4236.12, "end": 4241.8, "text": " Now what you might see going on is as we move this little red block, as these numbers change,", "tokens": [823, 437, 291, 1062, 536, 516, 322, 307, 382, 321, 1286, 341, 707, 2182, 3461, 11, 382, 613, 3547, 1319, 11], "temperature": 0.0, "avg_logprob": -0.0794749404444839, "compression_ratio": 1.6488888888888888, "no_speech_prob": 7.52793039282551e-06}, {"id": 999, "seek": 423612, "start": 4241.8, "end": 4247.08, "text": " we then multiply them by the corresponding numbers up here.", "tokens": [321, 550, 12972, 552, 538, 264, 11760, 3547, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.0794749404444839, "compression_ratio": 1.6488888888888888, "no_speech_prob": 7.52793039282551e-06}, {"id": 1000, "seek": 423612, "start": 4247.08, "end": 4250.16, "text": " And so let's start using some nomenclature.", "tokens": [400, 370, 718, 311, 722, 1228, 512, 297, 4726, 3474, 1503, 13], "temperature": 0.0, "avg_logprob": -0.0794749404444839, "compression_ratio": 1.6488888888888888, "no_speech_prob": 7.52793039282551e-06}, {"id": 1001, "seek": 423612, "start": 4250.16, "end": 4257.12, "text": " The thing up here we are going to call the kernel, the convolutional kernel.", "tokens": [440, 551, 493, 510, 321, 366, 516, 281, 818, 264, 28256, 11, 264, 45216, 304, 28256, 13], "temperature": 0.0, "avg_logprob": -0.0794749404444839, "compression_ratio": 1.6488888888888888, "no_speech_prob": 7.52793039282551e-06}, {"id": 1002, "seek": 423612, "start": 4257.12, "end": 4262.599999999999, "text": " So we're going to take each little 3x3 part of this image, and we're going to do an element-wise", "tokens": [407, 321, 434, 516, 281, 747, 1184, 707, 805, 87, 18, 644, 295, 341, 3256, 11, 293, 321, 434, 516, 281, 360, 364, 4478, 12, 3711], "temperature": 0.0, "avg_logprob": -0.0794749404444839, "compression_ratio": 1.6488888888888888, "no_speech_prob": 7.52793039282551e-06}, {"id": 1003, "seek": 426260, "start": 4262.6, "end": 4271.200000000001, "text": " multiplication of each of the 9 pixels that we're mousing over with each of the 9 items", "tokens": [27290, 295, 1184, 295, 264, 1722, 18668, 300, 321, 434, 275, 24220, 670, 365, 1184, 295, 264, 1722, 4754], "temperature": 0.0, "avg_logprob": -0.07532532718203483, "compression_ratio": 1.8114035087719298, "no_speech_prob": 5.955115284450585e-06}, {"id": 1004, "seek": 426260, "start": 4271.200000000001, "end": 4273.4400000000005, "text": " in our kernel.", "tokens": [294, 527, 28256, 13], "temperature": 0.0, "avg_logprob": -0.07532532718203483, "compression_ratio": 1.8114035087719298, "no_speech_prob": 5.955115284450585e-06}, {"id": 1005, "seek": 426260, "start": 4273.4400000000005, "end": 4277.84, "text": " And so once we multiply each set together, we can then add them all up.", "tokens": [400, 370, 1564, 321, 12972, 1184, 992, 1214, 11, 321, 393, 550, 909, 552, 439, 493, 13], "temperature": 0.0, "avg_logprob": -0.07532532718203483, "compression_ratio": 1.8114035087719298, "no_speech_prob": 5.955115284450585e-06}, {"id": 1006, "seek": 426260, "start": 4277.84, "end": 4280.320000000001, "text": " And that is what's shown on the right.", "tokens": [400, 300, 307, 437, 311, 4898, 322, 264, 558, 13], "temperature": 0.0, "avg_logprob": -0.07532532718203483, "compression_ratio": 1.8114035087719298, "no_speech_prob": 5.955115284450585e-06}, {"id": 1007, "seek": 426260, "start": 4280.320000000001, "end": 4284.360000000001, "text": " As the little bunch of red things move over there, you can see there's one red thing that", "tokens": [1018, 264, 707, 3840, 295, 2182, 721, 1286, 670, 456, 11, 291, 393, 536, 456, 311, 472, 2182, 551, 300], "temperature": 0.0, "avg_logprob": -0.07532532718203483, "compression_ratio": 1.8114035087719298, "no_speech_prob": 5.955115284450585e-06}, {"id": 1008, "seek": 426260, "start": 4284.360000000001, "end": 4286.0, "text": " appears over here.", "tokens": [7038, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.07532532718203483, "compression_ratio": 1.8114035087719298, "no_speech_prob": 5.955115284450585e-06}, {"id": 1009, "seek": 426260, "start": 4286.0, "end": 4291.360000000001, "text": " The reason there's one red thing over here is because each set of 9, after getting through", "tokens": [440, 1778, 456, 311, 472, 2182, 551, 670, 510, 307, 570, 1184, 992, 295, 1722, 11, 934, 1242, 807], "temperature": 0.0, "avg_logprob": -0.07532532718203483, "compression_ratio": 1.8114035087719298, "no_speech_prob": 5.955115284450585e-06}, {"id": 1010, "seek": 429136, "start": 4291.36, "end": 4298.719999999999, "text": " the element-wise multiplication of the kernel, get added together to create one output.", "tokens": [264, 4478, 12, 3711, 27290, 295, 264, 28256, 11, 483, 3869, 1214, 281, 1884, 472, 5598, 13], "temperature": 0.0, "avg_logprob": -0.11842726483757113, "compression_ratio": 1.4974874371859297, "no_speech_prob": 9.7215354344371e-07}, {"id": 1011, "seek": 429136, "start": 4298.719999999999, "end": 4306.88, "text": " So therefore, the size of this image has one pixel less on each edge than the original,", "tokens": [407, 4412, 11, 264, 2744, 295, 341, 3256, 575, 472, 19261, 1570, 322, 1184, 4691, 813, 264, 3380, 11], "temperature": 0.0, "avg_logprob": -0.11842726483757113, "compression_ratio": 1.4974874371859297, "no_speech_prob": 9.7215354344371e-07}, {"id": 1012, "seek": 429136, "start": 4306.88, "end": 4307.88, "text": " as you can see.", "tokens": [382, 291, 393, 536, 13], "temperature": 0.0, "avg_logprob": -0.11842726483757113, "compression_ratio": 1.4974874371859297, "no_speech_prob": 9.7215354344371e-07}, {"id": 1013, "seek": 429136, "start": 4307.88, "end": 4309.92, "text": " See how this black border is on it?", "tokens": [3008, 577, 341, 2211, 7838, 307, 322, 309, 30], "temperature": 0.0, "avg_logprob": -0.11842726483757113, "compression_ratio": 1.4974874371859297, "no_speech_prob": 9.7215354344371e-07}, {"id": 1014, "seek": 429136, "start": 4309.92, "end": 4315.719999999999, "text": " That's because at the edge, the 3x3 kernel can't quite go any further.", "tokens": [663, 311, 570, 412, 264, 4691, 11, 264, 805, 87, 18, 28256, 393, 380, 1596, 352, 604, 3052, 13], "temperature": 0.0, "avg_logprob": -0.11842726483757113, "compression_ratio": 1.4974874371859297, "no_speech_prob": 9.7215354344371e-07}, {"id": 1015, "seek": 431572, "start": 4315.72, "end": 4324.56, "text": " So the furthest you can go is to end up with a dot in the middle, just off the corner.", "tokens": [407, 264, 2687, 36356, 291, 393, 352, 307, 281, 917, 493, 365, 257, 5893, 294, 264, 2808, 11, 445, 766, 264, 4538, 13], "temperature": 0.0, "avg_logprob": -0.12486571615392511, "compression_ratio": 1.5450236966824644, "no_speech_prob": 3.905466655851342e-06}, {"id": 1016, "seek": 431572, "start": 4324.56, "end": 4326.04, "text": " So why are we doing this?", "tokens": [407, 983, 366, 321, 884, 341, 30], "temperature": 0.0, "avg_logprob": -0.12486571615392511, "compression_ratio": 1.5450236966824644, "no_speech_prob": 3.905466655851342e-06}, {"id": 1017, "seek": 431572, "start": 4326.04, "end": 4328.2, "text": " Well perhaps you can see what's happened.", "tokens": [1042, 4317, 291, 393, 536, 437, 311, 2011, 13], "temperature": 0.0, "avg_logprob": -0.12486571615392511, "compression_ratio": 1.5450236966824644, "no_speech_prob": 3.905466655851342e-06}, {"id": 1018, "seek": 431572, "start": 4328.2, "end": 4336.4400000000005, "text": " This face has turned into some white parts outlining the horizontal edges.", "tokens": [639, 1851, 575, 3574, 666, 512, 2418, 3166, 484, 31079, 264, 12750, 8819, 13], "temperature": 0.0, "avg_logprob": -0.12486571615392511, "compression_ratio": 1.5450236966824644, "no_speech_prob": 3.905466655851342e-06}, {"id": 1019, "seek": 431572, "start": 4336.4400000000005, "end": 4338.84, "text": " How?", "tokens": [1012, 30], "temperature": 0.0, "avg_logprob": -0.12486571615392511, "compression_ratio": 1.5450236966824644, "no_speech_prob": 3.905466655851342e-06}, {"id": 1020, "seek": 431572, "start": 4338.84, "end": 4343.88, "text": " Well the how is just by doing this element-wise multiplication of each set of 9 pixels with", "tokens": [1042, 264, 577, 307, 445, 538, 884, 341, 4478, 12, 3711, 27290, 295, 1184, 992, 295, 1722, 18668, 365], "temperature": 0.0, "avg_logprob": -0.12486571615392511, "compression_ratio": 1.5450236966824644, "no_speech_prob": 3.905466655851342e-06}, {"id": 1021, "seek": 434388, "start": 4343.88, "end": 4348.36, "text": " this kernel, adding them together and sticking the result in the corresponding spot over", "tokens": [341, 28256, 11, 5127, 552, 1214, 293, 13465, 264, 1874, 294, 264, 11760, 4008, 670], "temperature": 0.0, "avg_logprob": -0.11639554640826057, "compression_ratio": 1.5735294117647058, "no_speech_prob": 3.3405303838662803e-06}, {"id": 1022, "seek": 434388, "start": 4348.36, "end": 4350.22, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.11639554640826057, "compression_ratio": 1.5735294117647058, "no_speech_prob": 3.3405303838662803e-06}, {"id": 1023, "seek": 434388, "start": 4350.22, "end": 4356.16, "text": " Why is that creating white spots where the horizontal edges are?", "tokens": [1545, 307, 300, 4084, 2418, 10681, 689, 264, 12750, 8819, 366, 30], "temperature": 0.0, "avg_logprob": -0.11639554640826057, "compression_ratio": 1.5735294117647058, "no_speech_prob": 3.3405303838662803e-06}, {"id": 1024, "seek": 434388, "start": 4356.16, "end": 4358.24, "text": " Well let's think about it.", "tokens": [1042, 718, 311, 519, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.11639554640826057, "compression_ratio": 1.5735294117647058, "no_speech_prob": 3.3405303838662803e-06}, {"id": 1025, "seek": 434388, "start": 4358.24, "end": 4361.34, "text": " Let's look up here.", "tokens": [961, 311, 574, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.11639554640826057, "compression_ratio": 1.5735294117647058, "no_speech_prob": 3.3405303838662803e-06}, {"id": 1026, "seek": 434388, "start": 4361.34, "end": 4369.4800000000005, "text": " So if we're just in this little bit here, then the spots above it are all pretty white,", "tokens": [407, 498, 321, 434, 445, 294, 341, 707, 857, 510, 11, 550, 264, 10681, 3673, 309, 366, 439, 1238, 2418, 11], "temperature": 0.0, "avg_logprob": -0.11639554640826057, "compression_ratio": 1.5735294117647058, "no_speech_prob": 3.3405303838662803e-06}, {"id": 1027, "seek": 434388, "start": 4369.4800000000005, "end": 4371.400000000001, "text": " so they have high numbers.", "tokens": [370, 436, 362, 1090, 3547, 13], "temperature": 0.0, "avg_logprob": -0.11639554640826057, "compression_ratio": 1.5735294117647058, "no_speech_prob": 3.3405303838662803e-06}, {"id": 1028, "seek": 437140, "start": 4371.4, "end": 4375.32, "text": " So the bits above it, the big numbers, are getting multiplied by 1, 2, 1.", "tokens": [407, 264, 9239, 3673, 309, 11, 264, 955, 3547, 11, 366, 1242, 17207, 538, 502, 11, 568, 11, 502, 13], "temperature": 0.0, "avg_logprob": -0.18214842958270377, "compression_ratio": 1.709090909090909, "no_speech_prob": 6.3391471485374495e-06}, {"id": 1029, "seek": 437140, "start": 4375.32, "end": 4378.679999999999, "text": " So that's going to create a big number.", "tokens": [407, 300, 311, 516, 281, 1884, 257, 955, 1230, 13], "temperature": 0.0, "avg_logprob": -0.18214842958270377, "compression_ratio": 1.709090909090909, "no_speech_prob": 6.3391471485374495e-06}, {"id": 1030, "seek": 437140, "start": 4378.679999999999, "end": 4381.44, "text": " And the ones in the middle are all 0s, so I don't care about that.", "tokens": [400, 264, 2306, 294, 264, 2808, 366, 439, 1958, 82, 11, 370, 286, 500, 380, 1127, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.18214842958270377, "compression_ratio": 1.709090909090909, "no_speech_prob": 6.3391471485374495e-06}, {"id": 1031, "seek": 437140, "start": 4381.44, "end": 4386.28, "text": " And then the ones underneath are all small numbers because they're all close to 0, so", "tokens": [400, 550, 264, 2306, 7223, 366, 439, 1359, 3547, 570, 436, 434, 439, 1998, 281, 1958, 11, 370], "temperature": 0.0, "avg_logprob": -0.18214842958270377, "compression_ratio": 1.709090909090909, "no_speech_prob": 6.3391471485374495e-06}, {"id": 1032, "seek": 437140, "start": 4386.28, "end": 4388.24, "text": " that really doesn't do much at all.", "tokens": [300, 534, 1177, 380, 360, 709, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.18214842958270377, "compression_ratio": 1.709090909090909, "no_speech_prob": 6.3391471485374495e-06}, {"id": 1033, "seek": 437140, "start": 4388.24, "end": 4396.12, "text": " So therefore, that little set there is going to end up with bright white.", "tokens": [407, 4412, 11, 300, 707, 992, 456, 307, 516, 281, 917, 493, 365, 4730, 2418, 13], "temperature": 0.0, "avg_logprob": -0.18214842958270377, "compression_ratio": 1.709090909090909, "no_speech_prob": 6.3391471485374495e-06}, {"id": 1034, "seek": 439612, "start": 4396.12, "end": 4403.76, "text": " Whereas on the other side, down here, you've got light pixels underneath, so they're going", "tokens": [13813, 322, 264, 661, 1252, 11, 760, 510, 11, 291, 600, 658, 1442, 18668, 7223, 11, 370, 436, 434, 516], "temperature": 0.0, "avg_logprob": -0.13104757622106752, "compression_ratio": 1.5030674846625767, "no_speech_prob": 8.139631063386332e-06}, {"id": 1035, "seek": 439612, "start": 4403.76, "end": 4406.04, "text": " to get a lot of negative.", "tokens": [281, 483, 257, 688, 295, 3671, 13], "temperature": 0.0, "avg_logprob": -0.13104757622106752, "compression_ratio": 1.5030674846625767, "no_speech_prob": 8.139631063386332e-06}, {"id": 1036, "seek": 439612, "start": 4406.04, "end": 4412.28, "text": " Dark pixels on top, which are very small, so not much happens.", "tokens": [9563, 18668, 322, 1192, 11, 597, 366, 588, 1359, 11, 370, 406, 709, 2314, 13], "temperature": 0.0, "avg_logprob": -0.13104757622106752, "compression_ratio": 1.5030674846625767, "no_speech_prob": 8.139631063386332e-06}, {"id": 1037, "seek": 439612, "start": 4412.28, "end": 4417.62, "text": " So therefore, over here we're going to end up with very negative.", "tokens": [407, 4412, 11, 670, 510, 321, 434, 516, 281, 917, 493, 365, 588, 3671, 13], "temperature": 0.0, "avg_logprob": -0.13104757622106752, "compression_ratio": 1.5030674846625767, "no_speech_prob": 8.139631063386332e-06}, {"id": 1038, "seek": 441762, "start": 4417.62, "end": 4431.88, "text": " So this thing where we take each 3x3 area and element-wise multiply them with a kernel", "tokens": [407, 341, 551, 689, 321, 747, 1184, 805, 87, 18, 1859, 293, 4478, 12, 3711, 12972, 552, 365, 257, 28256], "temperature": 0.0, "avg_logprob": -0.11174544421109286, "compression_ratio": 1.4420289855072463, "no_speech_prob": 2.1907744667259976e-06}, {"id": 1039, "seek": 441762, "start": 4431.88, "end": 4440.76, "text": " and add each of those up together to create one output is called a convolution.", "tokens": [293, 909, 1184, 295, 729, 493, 1214, 281, 1884, 472, 5598, 307, 1219, 257, 45216, 13], "temperature": 0.0, "avg_logprob": -0.11174544421109286, "compression_ratio": 1.4420289855072463, "no_speech_prob": 2.1907744667259976e-06}, {"id": 1040, "seek": 441762, "start": 4440.76, "end": 4441.76, "text": " That's it.", "tokens": [663, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.11174544421109286, "compression_ratio": 1.4420289855072463, "no_speech_prob": 2.1907744667259976e-06}, {"id": 1041, "seek": 441762, "start": 4441.76, "end": 4442.94, "text": " That's a convolution.", "tokens": [663, 311, 257, 45216, 13], "temperature": 0.0, "avg_logprob": -0.11174544421109286, "compression_ratio": 1.4420289855072463, "no_speech_prob": 2.1907744667259976e-06}, {"id": 1042, "seek": 444294, "start": 4442.94, "end": 4450.16, "text": " So that might look familiar to you, because what we did back a while ago is we looked", "tokens": [407, 300, 1062, 574, 4963, 281, 291, 11, 570, 437, 321, 630, 646, 257, 1339, 2057, 307, 321, 2956], "temperature": 0.0, "avg_logprob": -0.16204970815907355, "compression_ratio": 1.606837606837607, "no_speech_prob": 6.74800048727775e-06}, {"id": 1043, "seek": 444294, "start": 4450.16, "end": 4456.639999999999, "text": " at that Zeiler and Fergus paper where we saw each different layer and we visualized what", "tokens": [412, 300, 4853, 5441, 293, 36790, 3035, 689, 321, 1866, 1184, 819, 4583, 293, 321, 5056, 1602, 437], "temperature": 0.0, "avg_logprob": -0.16204970815907355, "compression_ratio": 1.606837606837607, "no_speech_prob": 6.74800048727775e-06}, {"id": 1044, "seek": 444294, "start": 4456.639999999999, "end": 4457.639999999999, "text": " the weights were doing.", "tokens": [264, 17443, 645, 884, 13], "temperature": 0.0, "avg_logprob": -0.16204970815907355, "compression_ratio": 1.606837606837607, "no_speech_prob": 6.74800048727775e-06}, {"id": 1045, "seek": 444294, "start": 4457.639999999999, "end": 4463.4, "text": " Do you remember how the first layer was basically finding diagonal edges and gradients?", "tokens": [1144, 291, 1604, 577, 264, 700, 4583, 390, 1936, 5006, 21539, 8819, 293, 2771, 2448, 30], "temperature": 0.0, "avg_logprob": -0.16204970815907355, "compression_ratio": 1.606837606837607, "no_speech_prob": 6.74800048727775e-06}, {"id": 1046, "seek": 444294, "start": 4463.4, "end": 4466.04, "text": " That's because that's all a convolution can do.", "tokens": [663, 311, 570, 300, 311, 439, 257, 45216, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.16204970815907355, "compression_ratio": 1.606837606837607, "no_speech_prob": 6.74800048727775e-06}, {"id": 1047, "seek": 444294, "start": 4466.04, "end": 4468.16, "text": " Each of our layers is just a convolution.", "tokens": [6947, 295, 527, 7914, 307, 445, 257, 45216, 13], "temperature": 0.0, "avg_logprob": -0.16204970815907355, "compression_ratio": 1.606837606837607, "no_speech_prob": 6.74800048727775e-06}, {"id": 1048, "seek": 446816, "start": 4468.16, "end": 4473.8, "text": " So the first layer can do nothing more than this kind of thing.", "tokens": [407, 264, 700, 4583, 393, 360, 1825, 544, 813, 341, 733, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.11580683203304515, "compression_ratio": 1.933920704845815, "no_speech_prob": 3.138133479296812e-06}, {"id": 1049, "seek": 446816, "start": 4473.8, "end": 4478.0, "text": " But the nice thing is the next layer could then take the results of this and it could", "tokens": [583, 264, 1481, 551, 307, 264, 958, 4583, 727, 550, 747, 264, 3542, 295, 341, 293, 309, 727], "temperature": 0.0, "avg_logprob": -0.11580683203304515, "compression_ratio": 1.933920704845815, "no_speech_prob": 3.138133479296812e-06}, {"id": 1050, "seek": 446816, "start": 4478.0, "end": 4484.76, "text": " kind of combine one channel, the output of one convolutional field is called a channel.", "tokens": [733, 295, 10432, 472, 2269, 11, 264, 5598, 295, 472, 45216, 304, 2519, 307, 1219, 257, 2269, 13], "temperature": 0.0, "avg_logprob": -0.11580683203304515, "compression_ratio": 1.933920704845815, "no_speech_prob": 3.138133479296812e-06}, {"id": 1051, "seek": 446816, "start": 4484.76, "end": 4489.5599999999995, "text": " So it could take one channel that found top edges and another channel that finds left", "tokens": [407, 309, 727, 747, 472, 2269, 300, 1352, 1192, 8819, 293, 1071, 2269, 300, 10704, 1411], "temperature": 0.0, "avg_logprob": -0.11580683203304515, "compression_ratio": 1.933920704845815, "no_speech_prob": 3.138133479296812e-06}, {"id": 1052, "seek": 446816, "start": 4489.5599999999995, "end": 4494.5599999999995, "text": " edges and then the layer above that could take those two as input and create something", "tokens": [8819, 293, 550, 264, 4583, 3673, 300, 727, 747, 729, 732, 382, 4846, 293, 1884, 746], "temperature": 0.0, "avg_logprob": -0.11580683203304515, "compression_ratio": 1.933920704845815, "no_speech_prob": 3.138133479296812e-06}, {"id": 1053, "seek": 446816, "start": 4494.5599999999995, "end": 4497.599999999999, "text": " that finds top left corners.", "tokens": [300, 10704, 1192, 1411, 12413, 13], "temperature": 0.0, "avg_logprob": -0.11580683203304515, "compression_ratio": 1.933920704845815, "no_speech_prob": 3.138133479296812e-06}, {"id": 1054, "seek": 449760, "start": 4497.6, "end": 4502.06, "text": " As we saw when we looked at those Zeiler and Fergus visualizations.", "tokens": [1018, 321, 1866, 562, 321, 2956, 412, 729, 4853, 5441, 293, 36790, 5056, 14455, 13], "temperature": 0.0, "avg_logprob": -0.14707991811964247, "compression_ratio": 1.6305220883534137, "no_speech_prob": 1.4510179425997194e-05}, {"id": 1055, "seek": 449760, "start": 4502.06, "end": 4506.92, "text": " So let's take a look at this from another angle or quite a few other angles.", "tokens": [407, 718, 311, 747, 257, 574, 412, 341, 490, 1071, 5802, 420, 1596, 257, 1326, 661, 14708, 13], "temperature": 0.0, "avg_logprob": -0.14707991811964247, "compression_ratio": 1.6305220883534137, "no_speech_prob": 1.4510179425997194e-05}, {"id": 1056, "seek": 449760, "start": 4506.92, "end": 4511.320000000001, "text": " And we're going to look at a fantastic post from a guy called Matt Clinesmith who was", "tokens": [400, 321, 434, 516, 281, 574, 412, 257, 5456, 2183, 490, 257, 2146, 1219, 7397, 2033, 1652, 41708, 567, 390], "temperature": 0.0, "avg_logprob": -0.14707991811964247, "compression_ratio": 1.6305220883534137, "no_speech_prob": 1.4510179425997194e-05}, {"id": 1057, "seek": 449760, "start": 4511.320000000001, "end": 4515.4800000000005, "text": " actually a student in the first year that we did this course.", "tokens": [767, 257, 3107, 294, 264, 700, 1064, 300, 321, 630, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.14707991811964247, "compression_ratio": 1.6305220883534137, "no_speech_prob": 1.4510179425997194e-05}, {"id": 1058, "seek": 449760, "start": 4515.4800000000005, "end": 4522.08, "text": " And he wrote this as a part of his project work back then.", "tokens": [400, 415, 4114, 341, 382, 257, 644, 295, 702, 1716, 589, 646, 550, 13], "temperature": 0.0, "avg_logprob": -0.14707991811964247, "compression_ratio": 1.6305220883534137, "no_speech_prob": 1.4510179425997194e-05}, {"id": 1059, "seek": 449760, "start": 4522.08, "end": 4525.42, "text": " And what he's going to show here is here is our image.", "tokens": [400, 437, 415, 311, 516, 281, 855, 510, 307, 510, 307, 527, 3256, 13], "temperature": 0.0, "avg_logprob": -0.14707991811964247, "compression_ratio": 1.6305220883534137, "no_speech_prob": 1.4510179425997194e-05}, {"id": 1060, "seek": 452542, "start": 4525.42, "end": 4531.24, "text": " It's a 3x3 image and our kernel is a 2x2 kernel.", "tokens": [467, 311, 257, 805, 87, 18, 3256, 293, 527, 28256, 307, 257, 568, 87, 17, 28256, 13], "temperature": 0.0, "avg_logprob": -0.1006923889627262, "compression_ratio": 1.7447916666666667, "no_speech_prob": 5.5075593081710394e-06}, {"id": 1061, "seek": 452542, "start": 4531.24, "end": 4535.32, "text": " And what we're going to do is we're going to apply this kernel to the top left 2x2 part", "tokens": [400, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 3079, 341, 28256, 281, 264, 1192, 1411, 568, 87, 17, 644], "temperature": 0.0, "avg_logprob": -0.1006923889627262, "compression_ratio": 1.7447916666666667, "no_speech_prob": 5.5075593081710394e-06}, {"id": 1062, "seek": 452542, "start": 4535.32, "end": 4536.84, "text": " of this image.", "tokens": [295, 341, 3256, 13], "temperature": 0.0, "avg_logprob": -0.1006923889627262, "compression_ratio": 1.7447916666666667, "no_speech_prob": 5.5075593081710394e-06}, {"id": 1063, "seek": 452542, "start": 4536.84, "end": 4542.6, "text": " And so the pink bit will be correspondingly multiplied by the pink bit, the green by the", "tokens": [400, 370, 264, 7022, 857, 486, 312, 11760, 356, 17207, 538, 264, 7022, 857, 11, 264, 3092, 538, 264], "temperature": 0.0, "avg_logprob": -0.1006923889627262, "compression_ratio": 1.7447916666666667, "no_speech_prob": 5.5075593081710394e-06}, {"id": 1064, "seek": 452542, "start": 4542.6, "end": 4544.6, "text": " green and so forth.", "tokens": [3092, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1006923889627262, "compression_ratio": 1.7447916666666667, "no_speech_prob": 5.5075593081710394e-06}, {"id": 1065, "seek": 452542, "start": 4544.6, "end": 4550.86, "text": " And they all get added up together to create this top left and the output.", "tokens": [400, 436, 439, 483, 3869, 493, 1214, 281, 1884, 341, 1192, 1411, 293, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.1006923889627262, "compression_ratio": 1.7447916666666667, "no_speech_prob": 5.5075593081710394e-06}, {"id": 1066, "seek": 455086, "start": 4550.86, "end": 4563.0, "text": " So in other words, P equals alpha times A, beta times B, gamma times D, delta times E.", "tokens": [407, 294, 661, 2283, 11, 430, 6915, 8961, 1413, 316, 11, 9861, 1413, 363, 11, 15546, 1413, 413, 11, 8289, 1413, 462, 13], "temperature": 0.0, "avg_logprob": -0.2500360038843048, "compression_ratio": 1.4923076923076923, "no_speech_prob": 3.2377149636886315e-06}, {"id": 1067, "seek": 455086, "start": 4563.0, "end": 4565.0, "text": " There it is.", "tokens": [821, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.2500360038843048, "compression_ratio": 1.4923076923076923, "no_speech_prob": 3.2377149636886315e-06}, {"id": 1068, "seek": 455086, "start": 4565.0, "end": 4566.48, "text": " Plus B which is a bias.", "tokens": [7721, 363, 597, 307, 257, 12577, 13], "temperature": 0.0, "avg_logprob": -0.2500360038843048, "compression_ratio": 1.4923076923076923, "no_speech_prob": 3.2377149636886315e-06}, {"id": 1069, "seek": 455086, "start": 4566.48, "end": 4568.24, "text": " Okay, that's fine.", "tokens": [1033, 11, 300, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.2500360038843048, "compression_ratio": 1.4923076923076923, "no_speech_prob": 3.2377149636886315e-06}, {"id": 1070, "seek": 455086, "start": 4568.24, "end": 4570.38, "text": " That's just a normal bias.", "tokens": [663, 311, 445, 257, 2710, 12577, 13], "temperature": 0.0, "avg_logprob": -0.2500360038843048, "compression_ratio": 1.4923076923076923, "no_speech_prob": 3.2377149636886315e-06}, {"id": 1071, "seek": 455086, "start": 4570.38, "end": 4576.94, "text": " So you can see how basically each of these output pixels is a result of some different", "tokens": [407, 291, 393, 536, 577, 1936, 1184, 295, 613, 5598, 18668, 307, 257, 1874, 295, 512, 819], "temperature": 0.0, "avg_logprob": -0.2500360038843048, "compression_ratio": 1.4923076923076923, "no_speech_prob": 3.2377149636886315e-06}, {"id": 1072, "seek": 455086, "start": 4576.94, "end": 4577.94, "text": " linear equation.", "tokens": [8213, 5367, 13], "temperature": 0.0, "avg_logprob": -0.2500360038843048, "compression_ratio": 1.4923076923076923, "no_speech_prob": 3.2377149636886315e-06}, {"id": 1073, "seek": 455086, "start": 4577.94, "end": 4579.679999999999, "text": " That makes sense?", "tokens": [663, 1669, 2020, 30], "temperature": 0.0, "avg_logprob": -0.2500360038843048, "compression_ratio": 1.4923076923076923, "no_speech_prob": 3.2377149636886315e-06}, {"id": 1074, "seek": 457968, "start": 4579.68, "end": 4584.84, "text": " And you can see these same four weights are being moved around because this is our convolutional", "tokens": [400, 291, 393, 536, 613, 912, 1451, 17443, 366, 885, 4259, 926, 570, 341, 307, 527, 45216, 304], "temperature": 0.0, "avg_logprob": -0.16100718099859695, "compression_ratio": 1.5219512195121951, "no_speech_prob": 2.0261297777324216e-06}, {"id": 1075, "seek": 457968, "start": 4584.84, "end": 4587.4800000000005, "text": " kernel.", "tokens": [28256, 13], "temperature": 0.0, "avg_logprob": -0.16100718099859695, "compression_ratio": 1.5219512195121951, "no_speech_prob": 2.0261297777324216e-06}, {"id": 1076, "seek": 457968, "start": 4587.4800000000005, "end": 4593.200000000001, "text": " Here's another way of looking at it from that, which is here is a classic neural network", "tokens": [1692, 311, 1071, 636, 295, 1237, 412, 309, 490, 300, 11, 597, 307, 510, 307, 257, 7230, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.16100718099859695, "compression_ratio": 1.5219512195121951, "no_speech_prob": 2.0261297777324216e-06}, {"id": 1077, "seek": 457968, "start": 4593.200000000001, "end": 4595.320000000001, "text": " view.", "tokens": [1910, 13], "temperature": 0.0, "avg_logprob": -0.16100718099859695, "compression_ratio": 1.5219512195121951, "no_speech_prob": 2.0261297777324216e-06}, {"id": 1078, "seek": 457968, "start": 4595.320000000001, "end": 4603.66, "text": " And so P now is a result of multiplying every one of these inputs by a weight and then adding", "tokens": [400, 370, 430, 586, 307, 257, 1874, 295, 30955, 633, 472, 295, 613, 15743, 538, 257, 3364, 293, 550, 5127], "temperature": 0.0, "avg_logprob": -0.16100718099859695, "compression_ratio": 1.5219512195121951, "no_speech_prob": 2.0261297777324216e-06}, {"id": 1079, "seek": 457968, "start": 4603.66, "end": 4605.4400000000005, "text": " them all together.", "tokens": [552, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.16100718099859695, "compression_ratio": 1.5219512195121951, "no_speech_prob": 2.0261297777324216e-06}, {"id": 1080, "seek": 460544, "start": 4605.44, "end": 4610.759999999999, "text": " The gray ones are going to have a value of zero.", "tokens": [440, 10855, 2306, 366, 516, 281, 362, 257, 2158, 295, 4018, 13], "temperature": 0.0, "avg_logprob": -0.12293842259575338, "compression_ratio": 1.5900621118012421, "no_speech_prob": 3.6688493310066406e-06}, {"id": 1081, "seek": 460544, "start": 4610.759999999999, "end": 4621.96, "text": " Because remember P was only connected to A, B, D and E. A, B, D and E.", "tokens": [1436, 1604, 430, 390, 787, 4582, 281, 316, 11, 363, 11, 413, 293, 462, 13, 316, 11, 363, 11, 413, 293, 462, 13], "temperature": 0.0, "avg_logprob": -0.12293842259575338, "compression_ratio": 1.5900621118012421, "no_speech_prob": 3.6688493310066406e-06}, {"id": 1082, "seek": 460544, "start": 4621.96, "end": 4630.839999999999, "text": " So in other words, remembering that this represents a matrix multiplication, therefore we can", "tokens": [407, 294, 661, 2283, 11, 20719, 300, 341, 8855, 257, 8141, 27290, 11, 4412, 321, 393], "temperature": 0.0, "avg_logprob": -0.12293842259575338, "compression_ratio": 1.5900621118012421, "no_speech_prob": 3.6688493310066406e-06}, {"id": 1083, "seek": 460544, "start": 4630.839999999999, "end": 4635.0, "text": " represent this as a matrix multiplication.", "tokens": [2906, 341, 382, 257, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.12293842259575338, "compression_ratio": 1.5900621118012421, "no_speech_prob": 3.6688493310066406e-06}, {"id": 1084, "seek": 463500, "start": 4635.0, "end": 4643.16, "text": " So here is our list of pixels in our 3x3 image flattened out into a vector.", "tokens": [407, 510, 307, 527, 1329, 295, 18668, 294, 527, 805, 87, 18, 3256, 24183, 292, 484, 666, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.12241630992670169, "compression_ratio": 1.5363128491620113, "no_speech_prob": 3.8449152270914055e-06}, {"id": 1085, "seek": 463500, "start": 4643.16, "end": 4648.24, "text": " And here is a matrix vector multiplication plus bias.", "tokens": [400, 510, 307, 257, 8141, 8062, 27290, 1804, 12577, 13], "temperature": 0.0, "avg_logprob": -0.12241630992670169, "compression_ratio": 1.5363128491620113, "no_speech_prob": 3.8449152270914055e-06}, {"id": 1086, "seek": 463500, "start": 4648.24, "end": 4651.36, "text": " And then a whole bunch of them we're just going to set to zero.", "tokens": [400, 550, 257, 1379, 3840, 295, 552, 321, 434, 445, 516, 281, 992, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.12241630992670169, "compression_ratio": 1.5363128491620113, "no_speech_prob": 3.8449152270914055e-06}, {"id": 1087, "seek": 463500, "start": 4651.36, "end": 4662.26, "text": " So you can see here we've got a 0, 0, 0, 0, 0 which corresponds to 0, 0, 0, 0, 0.", "tokens": [407, 291, 393, 536, 510, 321, 600, 658, 257, 1958, 11, 1958, 11, 1958, 11, 1958, 11, 1958, 597, 23249, 281, 1958, 11, 1958, 11, 1958, 11, 1958, 11, 1958, 13], "temperature": 0.0, "avg_logprob": -0.12241630992670169, "compression_ratio": 1.5363128491620113, "no_speech_prob": 3.8449152270914055e-06}, {"id": 1088, "seek": 466226, "start": 4662.26, "end": 4669.92, "text": " So in other words, a convolution is just a matrix multiplication where two things happen.", "tokens": [407, 294, 661, 2283, 11, 257, 45216, 307, 445, 257, 8141, 27290, 689, 732, 721, 1051, 13], "temperature": 0.0, "avg_logprob": -0.11898055943575772, "compression_ratio": 1.799043062200957, "no_speech_prob": 3.237751570850378e-06}, {"id": 1089, "seek": 466226, "start": 4669.92, "end": 4675.3, "text": " Some of the entries are set to zero all the time and all of the ones of the same color", "tokens": [2188, 295, 264, 23041, 366, 992, 281, 4018, 439, 264, 565, 293, 439, 295, 264, 2306, 295, 264, 912, 2017], "temperature": 0.0, "avg_logprob": -0.11898055943575772, "compression_ratio": 1.799043062200957, "no_speech_prob": 3.237751570850378e-06}, {"id": 1090, "seek": 466226, "start": 4675.3, "end": 4677.8, "text": " always have the same weight.", "tokens": [1009, 362, 264, 912, 3364, 13], "temperature": 0.0, "avg_logprob": -0.11898055943575772, "compression_ratio": 1.799043062200957, "no_speech_prob": 3.237751570850378e-06}, {"id": 1091, "seek": 466226, "start": 4677.8, "end": 4683.96, "text": " So when you've got multiple things with the same weight, that's called weight tying.", "tokens": [407, 562, 291, 600, 658, 3866, 721, 365, 264, 912, 3364, 11, 300, 311, 1219, 3364, 32405, 13], "temperature": 0.0, "avg_logprob": -0.11898055943575772, "compression_ratio": 1.799043062200957, "no_speech_prob": 3.237751570850378e-06}, {"id": 1092, "seek": 466226, "start": 4683.96, "end": 4690.64, "text": " So clearly we could implement a convolution using matrix multiplication, but we don't", "tokens": [407, 4448, 321, 727, 4445, 257, 45216, 1228, 8141, 27290, 11, 457, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.11898055943575772, "compression_ratio": 1.799043062200957, "no_speech_prob": 3.237751570850378e-06}, {"id": 1093, "seek": 469064, "start": 4690.64, "end": 4692.320000000001, "text": " because it's slow.", "tokens": [570, 309, 311, 2964, 13], "temperature": 0.0, "avg_logprob": -0.12730291015223452, "compression_ratio": 1.5906735751295338, "no_speech_prob": 7.889186235843226e-06}, {"id": 1094, "seek": 469064, "start": 4692.320000000001, "end": 4699.88, "text": " So in practice our libraries have specific convolution functions that we use.", "tokens": [407, 294, 3124, 527, 15148, 362, 2685, 45216, 6828, 300, 321, 764, 13], "temperature": 0.0, "avg_logprob": -0.12730291015223452, "compression_ratio": 1.5906735751295338, "no_speech_prob": 7.889186235843226e-06}, {"id": 1095, "seek": 469064, "start": 4699.88, "end": 4706.76, "text": " And they're basically doing this, which is this, which is this equation, which is the", "tokens": [400, 436, 434, 1936, 884, 341, 11, 597, 307, 341, 11, 597, 307, 341, 5367, 11, 597, 307, 264], "temperature": 0.0, "avg_logprob": -0.12730291015223452, "compression_ratio": 1.5906735751295338, "no_speech_prob": 7.889186235843226e-06}, {"id": 1096, "seek": 469064, "start": 4706.76, "end": 4710.52, "text": " same as this matrix multiplication.", "tokens": [912, 382, 341, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.12730291015223452, "compression_ratio": 1.5906735751295338, "no_speech_prob": 7.889186235843226e-06}, {"id": 1097, "seek": 469064, "start": 4710.52, "end": 4716.68, "text": " And as we discussed, we have to think about padding because if you have a 3x3 kernel and", "tokens": [400, 382, 321, 7152, 11, 321, 362, 281, 519, 466, 39562, 570, 498, 291, 362, 257, 805, 87, 18, 28256, 293], "temperature": 0.0, "avg_logprob": -0.12730291015223452, "compression_ratio": 1.5906735751295338, "no_speech_prob": 7.889186235843226e-06}, {"id": 1098, "seek": 471668, "start": 4716.68, "end": 4721.64, "text": " a 3x3 image, then that can only create one pixel of output.", "tokens": [257, 805, 87, 18, 3256, 11, 550, 300, 393, 787, 1884, 472, 19261, 295, 5598, 13], "temperature": 0.0, "avg_logprob": -0.11144823186537799, "compression_ratio": 1.7699530516431925, "no_speech_prob": 5.507560217665741e-06}, {"id": 1099, "seek": 471668, "start": 4721.64, "end": 4725.56, "text": " There's only one place that this 3x3 can go.", "tokens": [821, 311, 787, 472, 1081, 300, 341, 805, 87, 18, 393, 352, 13], "temperature": 0.0, "avg_logprob": -0.11144823186537799, "compression_ratio": 1.7699530516431925, "no_speech_prob": 5.507560217665741e-06}, {"id": 1100, "seek": 471668, "start": 4725.56, "end": 4729.200000000001, "text": " So if we want to create more than one pixel of output, we have to do something called", "tokens": [407, 498, 321, 528, 281, 1884, 544, 813, 472, 19261, 295, 5598, 11, 321, 362, 281, 360, 746, 1219], "temperature": 0.0, "avg_logprob": -0.11144823186537799, "compression_ratio": 1.7699530516431925, "no_speech_prob": 5.507560217665741e-06}, {"id": 1101, "seek": 471668, "start": 4729.200000000001, "end": 4734.68, "text": " padding which is to put additional numbers all around the outside.", "tokens": [39562, 597, 307, 281, 829, 4497, 3547, 439, 926, 264, 2380, 13], "temperature": 0.0, "avg_logprob": -0.11144823186537799, "compression_ratio": 1.7699530516431925, "no_speech_prob": 5.507560217665741e-06}, {"id": 1102, "seek": 471668, "start": 4734.68, "end": 4739.96, "text": " So what most libraries do is that they just put a layer of zeros, not a layer, a bunch", "tokens": [407, 437, 881, 15148, 360, 307, 300, 436, 445, 829, 257, 4583, 295, 35193, 11, 406, 257, 4583, 11, 257, 3840], "temperature": 0.0, "avg_logprob": -0.11144823186537799, "compression_ratio": 1.7699530516431925, "no_speech_prob": 5.507560217665741e-06}, {"id": 1103, "seek": 471668, "start": 4739.96, "end": 4742.66, "text": " of zeros all around the outside.", "tokens": [295, 35193, 439, 926, 264, 2380, 13], "temperature": 0.0, "avg_logprob": -0.11144823186537799, "compression_ratio": 1.7699530516431925, "no_speech_prob": 5.507560217665741e-06}, {"id": 1104, "seek": 474266, "start": 4742.66, "end": 4748.5599999999995, "text": " So for a 3x3 kernel, a single zero on every edge piece here.", "tokens": [407, 337, 257, 805, 87, 18, 28256, 11, 257, 2167, 4018, 322, 633, 4691, 2522, 510, 13], "temperature": 0.0, "avg_logprob": -0.1506013572216034, "compression_ratio": 1.5201793721973094, "no_speech_prob": 2.0261252302589128e-06}, {"id": 1105, "seek": 474266, "start": 4748.5599999999995, "end": 4753.88, "text": " And so once you've padded it like that, you can now move your 3x3 kernel all the way across", "tokens": [400, 370, 1564, 291, 600, 6887, 9207, 309, 411, 300, 11, 291, 393, 586, 1286, 428, 805, 87, 18, 28256, 439, 264, 636, 2108], "temperature": 0.0, "avg_logprob": -0.1506013572216034, "compression_ratio": 1.5201793721973094, "no_speech_prob": 2.0261252302589128e-06}, {"id": 1106, "seek": 474266, "start": 4753.88, "end": 4757.76, "text": " and give you the same output size that you started with.", "tokens": [293, 976, 291, 264, 912, 5598, 2744, 300, 291, 1409, 365, 13], "temperature": 0.0, "avg_logprob": -0.1506013572216034, "compression_ratio": 1.5201793721973094, "no_speech_prob": 2.0261252302589128e-06}, {"id": 1107, "seek": 474266, "start": 4757.76, "end": 4758.76, "text": " Okay?", "tokens": [1033, 30], "temperature": 0.0, "avg_logprob": -0.1506013572216034, "compression_ratio": 1.5201793721973094, "no_speech_prob": 2.0261252302589128e-06}, {"id": 1108, "seek": 474266, "start": 4758.76, "end": 4765.2, "text": " Now, as we mentioned in Fast.ai, we don't normally necessarily use zero padding.", "tokens": [823, 11, 382, 321, 2835, 294, 15968, 13, 1301, 11, 321, 500, 380, 5646, 4725, 764, 4018, 39562, 13], "temperature": 0.0, "avg_logprob": -0.1506013572216034, "compression_ratio": 1.5201793721973094, "no_speech_prob": 2.0261252302589128e-06}, {"id": 1109, "seek": 474266, "start": 4765.2, "end": 4767.8, "text": " Where possible, we use reflection padding.", "tokens": [2305, 1944, 11, 321, 764, 12914, 39562, 13], "temperature": 0.0, "avg_logprob": -0.1506013572216034, "compression_ratio": 1.5201793721973094, "no_speech_prob": 2.0261252302589128e-06}, {"id": 1110, "seek": 476780, "start": 4767.8, "end": 4772.72, "text": " Although for these simple convolutions, we often use zero padding because it doesn't", "tokens": [5780, 337, 613, 2199, 3754, 15892, 11, 321, 2049, 764, 4018, 39562, 570, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.15143100936691484, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.1015912377843051e-06}, {"id": 1111, "seek": 476780, "start": 4772.72, "end": 4774.8, "text": " matter too much in a big image.", "tokens": [1871, 886, 709, 294, 257, 955, 3256, 13], "temperature": 0.0, "avg_logprob": -0.15143100936691484, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.1015912377843051e-06}, {"id": 1112, "seek": 476780, "start": 4774.8, "end": 4777.92, "text": " It doesn't make too much difference.", "tokens": [467, 1177, 380, 652, 886, 709, 2649, 13], "temperature": 0.0, "avg_logprob": -0.15143100936691484, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.1015912377843051e-06}, {"id": 1113, "seek": 476780, "start": 4777.92, "end": 4780.84, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.15143100936691484, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.1015912377843051e-06}, {"id": 1114, "seek": 476780, "start": 4780.84, "end": 4786.08, "text": " So that's what a convolution is.", "tokens": [407, 300, 311, 437, 257, 45216, 307, 13], "temperature": 0.0, "avg_logprob": -0.15143100936691484, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.1015912377843051e-06}, {"id": 1115, "seek": 476780, "start": 4786.08, "end": 4793.820000000001, "text": " So a convolutional neural network wouldn't be very interesting if it can only create", "tokens": [407, 257, 45216, 304, 18161, 3209, 2759, 380, 312, 588, 1880, 498, 309, 393, 787, 1884], "temperature": 0.0, "avg_logprob": -0.15143100936691484, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.1015912377843051e-06}, {"id": 1116, "seek": 476780, "start": 4793.820000000001, "end": 4796.72, "text": " top edges.", "tokens": [1192, 8819, 13], "temperature": 0.0, "avg_logprob": -0.15143100936691484, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.1015912377843051e-06}, {"id": 1117, "seek": 479672, "start": 4796.72, "end": 4801.76, "text": " So we have to take it a little bit further.", "tokens": [407, 321, 362, 281, 747, 309, 257, 707, 857, 3052, 13], "temperature": 0.0, "avg_logprob": -0.1666075110435486, "compression_ratio": 1.3009708737864079, "no_speech_prob": 4.936947334499564e-06}, {"id": 1118, "seek": 479672, "start": 4801.76, "end": 4824.12, "text": " So if we have an input, and it might be standard kind of red, green, blue picture, then we", "tokens": [407, 498, 321, 362, 364, 4846, 11, 293, 309, 1062, 312, 3832, 733, 295, 2182, 11, 3092, 11, 3344, 3036, 11, 550, 321], "temperature": 0.0, "avg_logprob": -0.1666075110435486, "compression_ratio": 1.3009708737864079, "no_speech_prob": 4.936947334499564e-06}, {"id": 1119, "seek": 482412, "start": 4824.12, "end": 4834.12, "text": " can create a kernel, a 3x3 kernel, like so.", "tokens": [393, 1884, 257, 28256, 11, 257, 805, 87, 18, 28256, 11, 411, 370, 13], "temperature": 0.0, "avg_logprob": -0.12455581216251149, "compression_ratio": 1.422077922077922, "no_speech_prob": 1.933351541083539e-06}, {"id": 1120, "seek": 482412, "start": 4834.12, "end": 4839.28, "text": " And then we could pass that kernel over all of the different pixels.", "tokens": [400, 550, 321, 727, 1320, 300, 28256, 670, 439, 295, 264, 819, 18668, 13], "temperature": 0.0, "avg_logprob": -0.12455581216251149, "compression_ratio": 1.422077922077922, "no_speech_prob": 1.933351541083539e-06}, {"id": 1121, "seek": 482412, "start": 4839.28, "end": 4843.22, "text": " But if you think about it, we actually don't have a 2D input anymore.", "tokens": [583, 498, 291, 519, 466, 309, 11, 321, 767, 500, 380, 362, 257, 568, 35, 4846, 3602, 13], "temperature": 0.0, "avg_logprob": -0.12455581216251149, "compression_ratio": 1.422077922077922, "no_speech_prob": 1.933351541083539e-06}, {"id": 1122, "seek": 482412, "start": 4843.22, "end": 4847.2, "text": " We have a 3D input, a rank 3 tensor.", "tokens": [492, 362, 257, 805, 35, 4846, 11, 257, 6181, 805, 40863, 13], "temperature": 0.0, "avg_logprob": -0.12455581216251149, "compression_ratio": 1.422077922077922, "no_speech_prob": 1.933351541083539e-06}, {"id": 1123, "seek": 484720, "start": 4847.2, "end": 4854.0, "text": " So we probably don't want to use the same kernel values for each of red and green and", "tokens": [407, 321, 1391, 500, 380, 528, 281, 764, 264, 912, 28256, 4190, 337, 1184, 295, 2182, 293, 3092, 293], "temperature": 0.0, "avg_logprob": -0.10978858947753906, "compression_ratio": 1.7675438596491229, "no_speech_prob": 1.1726393722710782e-06}, {"id": 1124, "seek": 484720, "start": 4854.0, "end": 4858.44, "text": " blue because, for example, if we're creating a green frog detector, we would want more", "tokens": [3344, 570, 11, 337, 1365, 11, 498, 321, 434, 4084, 257, 3092, 17259, 25712, 11, 321, 576, 528, 544], "temperature": 0.0, "avg_logprob": -0.10978858947753906, "compression_ratio": 1.7675438596491229, "no_speech_prob": 1.1726393722710782e-06}, {"id": 1125, "seek": 484720, "start": 4858.44, "end": 4863.2, "text": " activations on the green than we would on the blue.", "tokens": [2430, 763, 322, 264, 3092, 813, 321, 576, 322, 264, 3344, 13], "temperature": 0.0, "avg_logprob": -0.10978858947753906, "compression_ratio": 1.7675438596491229, "no_speech_prob": 1.1726393722710782e-06}, {"id": 1126, "seek": 484720, "start": 4863.2, "end": 4866.0199999999995, "text": " Or if we're trying to find something that can actually find a gradient that goes from", "tokens": [1610, 498, 321, 434, 1382, 281, 915, 746, 300, 393, 767, 915, 257, 16235, 300, 1709, 490], "temperature": 0.0, "avg_logprob": -0.10978858947753906, "compression_ratio": 1.7675438596491229, "no_speech_prob": 1.1726393722710782e-06}, {"id": 1127, "seek": 484720, "start": 4866.0199999999995, "end": 4872.16, "text": " green to blue, then the different kernels for each channel need to have different values", "tokens": [3092, 281, 3344, 11, 550, 264, 819, 23434, 1625, 337, 1184, 2269, 643, 281, 362, 819, 4190], "temperature": 0.0, "avg_logprob": -0.10978858947753906, "compression_ratio": 1.7675438596491229, "no_speech_prob": 1.1726393722710782e-06}, {"id": 1128, "seek": 484720, "start": 4872.16, "end": 4873.16, "text": " in.", "tokens": [294, 13], "temperature": 0.0, "avg_logprob": -0.10978858947753906, "compression_ratio": 1.7675438596491229, "no_speech_prob": 1.1726393722710782e-06}, {"id": 1129, "seek": 487316, "start": 4873.16, "end": 4883.4, "text": " So therefore, we need to create a 3x3x3 kernel.", "tokens": [407, 4412, 11, 321, 643, 281, 1884, 257, 805, 87, 18, 87, 18, 28256, 13], "temperature": 0.0, "avg_logprob": -0.21604955764043898, "compression_ratio": 1.6726190476190477, "no_speech_prob": 2.6841928502108203e-06}, {"id": 1130, "seek": 487316, "start": 4883.4, "end": 4891.48, "text": " So this is still our kernel, and we're still going to vary it across the height and the", "tokens": [407, 341, 307, 920, 527, 28256, 11, 293, 321, 434, 920, 516, 281, 10559, 309, 2108, 264, 6681, 293, 264], "temperature": 0.0, "avg_logprob": -0.21604955764043898, "compression_ratio": 1.6726190476190477, "no_speech_prob": 2.6841928502108203e-06}, {"id": 1131, "seek": 487316, "start": 4891.48, "end": 4892.639999999999, "text": " width.", "tokens": [11402, 13], "temperature": 0.0, "avg_logprob": -0.21604955764043898, "compression_ratio": 1.6726190476190477, "no_speech_prob": 2.6841928502108203e-06}, {"id": 1132, "seek": 487316, "start": 4892.639999999999, "end": 4897.92, "text": " But rather than doing an element-wise multiplication of nine things, we're going to do an element-wise", "tokens": [583, 2831, 813, 884, 364, 4478, 12, 3711, 27290, 295, 4949, 721, 11, 321, 434, 516, 281, 360, 364, 4478, 12, 3711], "temperature": 0.0, "avg_logprob": -0.21604955764043898, "compression_ratio": 1.6726190476190477, "no_speech_prob": 2.6841928502108203e-06}, {"id": 1133, "seek": 487316, "start": 4897.92, "end": 4900.36, "text": " multiplication of 27 things.", "tokens": [27290, 295, 7634, 721, 13], "temperature": 0.0, "avg_logprob": -0.21604955764043898, "compression_ratio": 1.6726190476190477, "no_speech_prob": 2.6841928502108203e-06}, {"id": 1134, "seek": 487316, "start": 4900.36, "end": 4901.36, "text": " 3x3x3.", "tokens": [805, 87, 18, 87, 18, 13], "temperature": 0.0, "avg_logprob": -0.21604955764043898, "compression_ratio": 1.6726190476190477, "no_speech_prob": 2.6841928502108203e-06}, {"id": 1135, "seek": 490136, "start": 4901.36, "end": 4905.679999999999, "text": " And we're still going to then add them up into a single number.", "tokens": [400, 321, 434, 920, 516, 281, 550, 909, 552, 493, 666, 257, 2167, 1230, 13], "temperature": 0.0, "avg_logprob": -0.1238583860726192, "compression_ratio": 1.6435643564356435, "no_speech_prob": 2.521563601476373e-06}, {"id": 1136, "seek": 490136, "start": 4905.679999999999, "end": 4912.759999999999, "text": " So as we pass this cube over this and the kind of like little bit that's going to be", "tokens": [407, 382, 321, 1320, 341, 13728, 670, 341, 293, 264, 733, 295, 411, 707, 857, 300, 311, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.1238583860726192, "compression_ratio": 1.6435643564356435, "no_speech_prob": 2.521563601476373e-06}, {"id": 1137, "seek": 490136, "start": 4912.759999999999, "end": 4922.679999999999, "text": " sitting behind it, as we do that part of the convolution, it's still going to create just", "tokens": [3798, 2261, 309, 11, 382, 321, 360, 300, 644, 295, 264, 45216, 11, 309, 311, 920, 516, 281, 1884, 445], "temperature": 0.0, "avg_logprob": -0.1238583860726192, "compression_ratio": 1.6435643564356435, "no_speech_prob": 2.521563601476373e-06}, {"id": 1138, "seek": 490136, "start": 4922.679999999999, "end": 4923.679999999999, "text": " one number.", "tokens": [472, 1230, 13], "temperature": 0.0, "avg_logprob": -0.1238583860726192, "compression_ratio": 1.6435643564356435, "no_speech_prob": 2.521563601476373e-06}, {"id": 1139, "seek": 490136, "start": 4923.679999999999, "end": 4929.599999999999, "text": " Because we do an element-wise multiplication of all 27 and add them all together.", "tokens": [1436, 321, 360, 364, 4478, 12, 3711, 27290, 295, 439, 7634, 293, 909, 552, 439, 1214, 13], "temperature": 0.0, "avg_logprob": -0.1238583860726192, "compression_ratio": 1.6435643564356435, "no_speech_prob": 2.521563601476373e-06}, {"id": 1140, "seek": 492960, "start": 4929.6, "end": 4937.200000000001, "text": " So we can do that across the whole single unit padded input.", "tokens": [407, 321, 393, 360, 300, 2108, 264, 1379, 2167, 4985, 6887, 9207, 4846, 13], "temperature": 0.0, "avg_logprob": -0.12947398484355271, "compression_ratio": 1.5797101449275361, "no_speech_prob": 8.99093720363453e-07}, {"id": 1141, "seek": 492960, "start": 4937.200000000001, "end": 4940.4400000000005, "text": " And so we started with 1, 2, 3, 4, 5 by 5.", "tokens": [400, 370, 321, 1409, 365, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 538, 1025, 13], "temperature": 0.0, "avg_logprob": -0.12947398484355271, "compression_ratio": 1.5797101449275361, "no_speech_prob": 8.99093720363453e-07}, {"id": 1142, "seek": 492960, "start": 4940.4400000000005, "end": 4947.88, "text": " So we're going to end up with an output that's also 5 by 5.", "tokens": [407, 321, 434, 516, 281, 917, 493, 365, 364, 5598, 300, 311, 611, 1025, 538, 1025, 13], "temperature": 0.0, "avg_logprob": -0.12947398484355271, "compression_ratio": 1.5797101449275361, "no_speech_prob": 8.99093720363453e-07}, {"id": 1143, "seek": 492960, "start": 4947.88, "end": 4953.04, "text": " But now, our input was three channels, and our output is only one channel.", "tokens": [583, 586, 11, 527, 4846, 390, 1045, 9235, 11, 293, 527, 5598, 307, 787, 472, 2269, 13], "temperature": 0.0, "avg_logprob": -0.12947398484355271, "compression_ratio": 1.5797101449275361, "no_speech_prob": 8.99093720363453e-07}, {"id": 1144, "seek": 492960, "start": 4953.04, "end": 4957.64, "text": " Now, we're not going to be able to do very much with just one channel, because all we've", "tokens": [823, 11, 321, 434, 406, 516, 281, 312, 1075, 281, 360, 588, 709, 365, 445, 472, 2269, 11, 570, 439, 321, 600], "temperature": 0.0, "avg_logprob": -0.12947398484355271, "compression_ratio": 1.5797101449275361, "no_speech_prob": 8.99093720363453e-07}, {"id": 1145, "seek": 495764, "start": 4957.64, "end": 4961.04, "text": " done now is found a top edge.", "tokens": [1096, 586, 307, 1352, 257, 1192, 4691, 13], "temperature": 0.0, "avg_logprob": -0.16171982349493566, "compression_ratio": 1.6604938271604939, "no_speech_prob": 1.5779564819240477e-06}, {"id": 1146, "seek": 495764, "start": 4961.04, "end": 4967.68, "text": " How are we going to find a side edge and a gradient and an area of constant white?", "tokens": [1012, 366, 321, 516, 281, 915, 257, 1252, 4691, 293, 257, 16235, 293, 364, 1859, 295, 5754, 2418, 30], "temperature": 0.0, "avg_logprob": -0.16171982349493566, "compression_ratio": 1.6604938271604939, "no_speech_prob": 1.5779564819240477e-06}, {"id": 1147, "seek": 495764, "start": 4967.68, "end": 4974.4800000000005, "text": " Well, we're going to have to create another kernel.", "tokens": [1042, 11, 321, 434, 516, 281, 362, 281, 1884, 1071, 28256, 13], "temperature": 0.0, "avg_logprob": -0.16171982349493566, "compression_ratio": 1.6604938271604939, "no_speech_prob": 1.5779564819240477e-06}, {"id": 1148, "seek": 495764, "start": 4974.4800000000005, "end": 4981.84, "text": " And we're going to have to do that, convolved over the input, and that's going to create", "tokens": [400, 321, 434, 516, 281, 362, 281, 360, 300, 11, 3754, 29110, 670, 264, 4846, 11, 293, 300, 311, 516, 281, 1884], "temperature": 0.0, "avg_logprob": -0.16171982349493566, "compression_ratio": 1.6604938271604939, "no_speech_prob": 1.5779564819240477e-06}, {"id": 1149, "seek": 495764, "start": 4981.84, "end": 4984.64, "text": " another 5 by 5.", "tokens": [1071, 1025, 538, 1025, 13], "temperature": 0.0, "avg_logprob": -0.16171982349493566, "compression_ratio": 1.6604938271604939, "no_speech_prob": 1.5779564819240477e-06}, {"id": 1150, "seek": 498464, "start": 4984.64, "end": 4992.84, "text": " And then we can just stack those together across this as another axis.", "tokens": [400, 550, 321, 393, 445, 8630, 729, 1214, 2108, 341, 382, 1071, 10298, 13], "temperature": 0.0, "avg_logprob": -0.13402948209217616, "compression_ratio": 1.4892086330935252, "no_speech_prob": 3.237737701056176e-06}, {"id": 1151, "seek": 498464, "start": 4992.84, "end": 4995.4400000000005, "text": " And we can do that lots and lots of times.", "tokens": [400, 321, 393, 360, 300, 3195, 293, 3195, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.13402948209217616, "compression_ratio": 1.4892086330935252, "no_speech_prob": 3.237737701056176e-06}, {"id": 1152, "seek": 498464, "start": 4995.4400000000005, "end": 5004.4800000000005, "text": " And that's going to give us another rank 3 tensor output.", "tokens": [400, 300, 311, 516, 281, 976, 505, 1071, 6181, 805, 40863, 5598, 13], "temperature": 0.0, "avg_logprob": -0.13402948209217616, "compression_ratio": 1.4892086330935252, "no_speech_prob": 3.237737701056176e-06}, {"id": 1153, "seek": 498464, "start": 5004.4800000000005, "end": 5006.72, "text": " So that's what happens in practice.", "tokens": [407, 300, 311, 437, 2314, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.13402948209217616, "compression_ratio": 1.4892086330935252, "no_speech_prob": 3.237737701056176e-06}, {"id": 1154, "seek": 500672, "start": 5006.72, "end": 5020.0, "text": " In practice, we start with an input which is h by w by, for images, 3.", "tokens": [682, 3124, 11, 321, 722, 365, 364, 4846, 597, 307, 276, 538, 261, 538, 11, 337, 5267, 11, 805, 13], "temperature": 0.0, "avg_logprob": -0.16610263375674977, "compression_ratio": 1.4142011834319526, "no_speech_prob": 1.816213853089721e-06}, {"id": 1155, "seek": 500672, "start": 5020.0, "end": 5026.72, "text": " We pass it through a bunch of convolutional kernels, and we can pick how many we want.", "tokens": [492, 1320, 309, 807, 257, 3840, 295, 45216, 304, 23434, 1625, 11, 293, 321, 393, 1888, 577, 867, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.16610263375674977, "compression_ratio": 1.4142011834319526, "no_speech_prob": 1.816213853089721e-06}, {"id": 1156, "seek": 500672, "start": 5026.72, "end": 5036.7, "text": " And it gives us back an output of height by width by however many kernels we had.", "tokens": [400, 309, 2709, 505, 646, 364, 5598, 295, 6681, 538, 11402, 538, 4461, 867, 23434, 1625, 321, 632, 13], "temperature": 0.0, "avg_logprob": -0.16610263375674977, "compression_ratio": 1.4142011834319526, "no_speech_prob": 1.816213853089721e-06}, {"id": 1157, "seek": 503670, "start": 5036.7, "end": 5043.12, "text": " And so often that might be something like 16 in the first layer.", "tokens": [400, 370, 2049, 300, 1062, 312, 746, 411, 3165, 294, 264, 700, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1343214835649655, "compression_ratio": 1.6524064171122994, "no_speech_prob": 6.240862148843007e-06}, {"id": 1158, "seek": 503670, "start": 5043.12, "end": 5052.24, "text": " And so now we've got 16 channels, they're called, 16 channels representing things like", "tokens": [400, 370, 586, 321, 600, 658, 3165, 9235, 11, 436, 434, 1219, 11, 3165, 9235, 13460, 721, 411], "temperature": 0.0, "avg_logprob": -0.1343214835649655, "compression_ratio": 1.6524064171122994, "no_speech_prob": 6.240862148843007e-06}, {"id": 1159, "seek": 503670, "start": 5052.24, "end": 5056.679999999999, "text": " how much left edge was on this pixel, how much top edge was on this pixel, how much", "tokens": [577, 709, 1411, 4691, 390, 322, 341, 19261, 11, 577, 709, 1192, 4691, 390, 322, 341, 19261, 11, 577, 709], "temperature": 0.0, "avg_logprob": -0.1343214835649655, "compression_ratio": 1.6524064171122994, "no_speech_prob": 6.240862148843007e-06}, {"id": 1160, "seek": 503670, "start": 5056.679999999999, "end": 5065.84, "text": " blue to red gradient was on this set of 27, well, 9 pixels each with RGB.", "tokens": [3344, 281, 2182, 16235, 390, 322, 341, 992, 295, 7634, 11, 731, 11, 1722, 18668, 1184, 365, 31231, 13], "temperature": 0.0, "avg_logprob": -0.1343214835649655, "compression_ratio": 1.6524064171122994, "no_speech_prob": 6.240862148843007e-06}, {"id": 1161, "seek": 506584, "start": 5065.84, "end": 5067.6, "text": " And so then you can just do the same thing, right?", "tokens": [400, 370, 550, 291, 393, 445, 360, 264, 912, 551, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13988085226579147, "compression_ratio": 1.5388349514563107, "no_speech_prob": 1.7061763628589688e-06}, {"id": 1162, "seek": 506584, "start": 5067.6, "end": 5076.52, "text": " You can have another bunch of kernels.", "tokens": [509, 393, 362, 1071, 3840, 295, 23434, 1625, 13], "temperature": 0.0, "avg_logprob": -0.13988085226579147, "compression_ratio": 1.5388349514563107, "no_speech_prob": 1.7061763628589688e-06}, {"id": 1163, "seek": 506584, "start": 5076.52, "end": 5079.96, "text": " And that's going to create another output, rank 3 tensor.", "tokens": [400, 300, 311, 516, 281, 1884, 1071, 5598, 11, 6181, 805, 40863, 13], "temperature": 0.0, "avg_logprob": -0.13988085226579147, "compression_ratio": 1.5388349514563107, "no_speech_prob": 1.7061763628589688e-06}, {"id": 1164, "seek": 506584, "start": 5079.96, "end": 5087.52, "text": " Again, height by width by whatever, might still be 16.", "tokens": [3764, 11, 6681, 538, 11402, 538, 2035, 11, 1062, 920, 312, 3165, 13], "temperature": 0.0, "avg_logprob": -0.13988085226579147, "compression_ratio": 1.5388349514563107, "no_speech_prob": 1.7061763628589688e-06}, {"id": 1165, "seek": 506584, "start": 5087.52, "end": 5092.32, "text": " Now what we really like to do is as we get deeper in the network, we actually want to", "tokens": [823, 437, 321, 534, 411, 281, 360, 307, 382, 321, 483, 7731, 294, 264, 3209, 11, 321, 767, 528, 281], "temperature": 0.0, "avg_logprob": -0.13988085226579147, "compression_ratio": 1.5388349514563107, "no_speech_prob": 1.7061763628589688e-06}, {"id": 1166, "seek": 506584, "start": 5092.32, "end": 5094.46, "text": " have more and more channels.", "tokens": [362, 544, 293, 544, 9235, 13], "temperature": 0.0, "avg_logprob": -0.13988085226579147, "compression_ratio": 1.5388349514563107, "no_speech_prob": 1.7061763628589688e-06}, {"id": 1167, "seek": 509446, "start": 5094.46, "end": 5098.64, "text": " We want to be able to find like a richer and richer set of features so that after a few,", "tokens": [492, 528, 281, 312, 1075, 281, 915, 411, 257, 29021, 293, 29021, 992, 295, 4122, 370, 300, 934, 257, 1326, 11], "temperature": 0.0, "avg_logprob": -0.16562430878989717, "compression_ratio": 1.6067415730337078, "no_speech_prob": 8.664497727295384e-06}, {"id": 1168, "seek": 509446, "start": 5098.64, "end": 5103.2, "text": " as we saw in the Zeiler and Ferguss paper, by layer 4 or 5, we've kind of got eyeball", "tokens": [382, 321, 1866, 294, 264, 4853, 5441, 293, 10728, 70, 2023, 3035, 11, 538, 4583, 1017, 420, 1025, 11, 321, 600, 733, 295, 658, 38868], "temperature": 0.0, "avg_logprob": -0.16562430878989717, "compression_ratio": 1.6067415730337078, "no_speech_prob": 8.664497727295384e-06}, {"id": 1169, "seek": 509446, "start": 5103.2, "end": 5105.56, "text": " detectors and fur detectors and things, right?", "tokens": [46866, 293, 2687, 46866, 293, 721, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16562430878989717, "compression_ratio": 1.6067415730337078, "no_speech_prob": 8.664497727295384e-06}, {"id": 1170, "seek": 509446, "start": 5105.56, "end": 5108.22, "text": " So you really need a lot of channels.", "tokens": [407, 291, 534, 643, 257, 688, 295, 9235, 13], "temperature": 0.0, "avg_logprob": -0.16562430878989717, "compression_ratio": 1.6067415730337078, "no_speech_prob": 8.664497727295384e-06}, {"id": 1171, "seek": 509446, "start": 5108.22, "end": 5115.08, "text": " So in order to avoid our memory going out of control, from time to time, we create a", "tokens": [407, 294, 1668, 281, 5042, 527, 4675, 516, 484, 295, 1969, 11, 490, 565, 281, 565, 11, 321, 1884, 257], "temperature": 0.0, "avg_logprob": -0.16562430878989717, "compression_ratio": 1.6067415730337078, "no_speech_prob": 8.664497727295384e-06}, {"id": 1172, "seek": 509446, "start": 5115.08, "end": 5123.72, "text": " convolution where we don't step over every single set of 3 by 3, but instead we skip", "tokens": [45216, 689, 321, 500, 380, 1823, 670, 633, 2167, 992, 295, 805, 538, 805, 11, 457, 2602, 321, 10023], "temperature": 0.0, "avg_logprob": -0.16562430878989717, "compression_ratio": 1.6067415730337078, "no_speech_prob": 8.664497727295384e-06}, {"id": 1173, "seek": 512372, "start": 5123.72, "end": 5126.68, "text": " over 2 at a time.", "tokens": [670, 568, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.18602756831956946, "compression_ratio": 1.445054945054945, "no_speech_prob": 6.339125320664607e-06}, {"id": 1174, "seek": 512372, "start": 5126.68, "end": 5132.8, "text": " So we would start with a 3 by 3 centered at 2, 2, and then we'd jump over to 2, 4, 2,", "tokens": [407, 321, 576, 722, 365, 257, 805, 538, 805, 18988, 412, 568, 11, 568, 11, 293, 550, 321, 1116, 3012, 670, 281, 568, 11, 1017, 11, 568, 11], "temperature": 0.0, "avg_logprob": -0.18602756831956946, "compression_ratio": 1.445054945054945, "no_speech_prob": 6.339125320664607e-06}, {"id": 1175, "seek": 512372, "start": 5132.8, "end": 5136.0, "text": " 6, 2, 8, and so forth.", "tokens": [1386, 11, 568, 11, 1649, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.18602756831956946, "compression_ratio": 1.445054945054945, "no_speech_prob": 6.339125320664607e-06}, {"id": 1176, "seek": 512372, "start": 5136.0, "end": 5142.4800000000005, "text": " And that's called a stride 2 convolution.", "tokens": [400, 300, 311, 1219, 257, 1056, 482, 568, 45216, 13], "temperature": 0.0, "avg_logprob": -0.18602756831956946, "compression_ratio": 1.445054945054945, "no_speech_prob": 6.339125320664607e-06}, {"id": 1177, "seek": 512372, "start": 5142.4800000000005, "end": 5146.0, "text": " And so what that does is it looks exactly the same, right?", "tokens": [400, 370, 437, 300, 775, 307, 309, 1542, 2293, 264, 912, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18602756831956946, "compression_ratio": 1.445054945054945, "no_speech_prob": 6.339125320664607e-06}, {"id": 1178, "seek": 512372, "start": 5146.0, "end": 5150.16, "text": " It's still just a bunch of kernels.", "tokens": [467, 311, 920, 445, 257, 3840, 295, 23434, 1625, 13], "temperature": 0.0, "avg_logprob": -0.18602756831956946, "compression_ratio": 1.445054945054945, "no_speech_prob": 6.339125320664607e-06}, {"id": 1179, "seek": 515016, "start": 5150.16, "end": 5157.4, "text": " But we're just jumping over 2 at a time, right?", "tokens": [583, 321, 434, 445, 11233, 670, 568, 412, 257, 565, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12051261417449467, "compression_ratio": 1.4304635761589404, "no_speech_prob": 4.092833023605635e-06}, {"id": 1180, "seek": 515016, "start": 5157.4, "end": 5161.84, "text": " We're skipping every alternate input pixel.", "tokens": [492, 434, 31533, 633, 18873, 4846, 19261, 13], "temperature": 0.0, "avg_logprob": -0.12051261417449467, "compression_ratio": 1.4304635761589404, "no_speech_prob": 4.092833023605635e-06}, {"id": 1181, "seek": 515016, "start": 5161.84, "end": 5169.24, "text": " And so the output from that will be h over 2 by w over 2.", "tokens": [400, 370, 264, 5598, 490, 300, 486, 312, 276, 670, 568, 538, 261, 670, 568, 13], "temperature": 0.0, "avg_logprob": -0.12051261417449467, "compression_ratio": 1.4304635761589404, "no_speech_prob": 4.092833023605635e-06}, {"id": 1182, "seek": 515016, "start": 5169.24, "end": 5173.2, "text": " And so when we do that, we generally create twice as many kernels.", "tokens": [400, 370, 562, 321, 360, 300, 11, 321, 5101, 1884, 6091, 382, 867, 23434, 1625, 13], "temperature": 0.0, "avg_logprob": -0.12051261417449467, "compression_ratio": 1.4304635761589404, "no_speech_prob": 4.092833023605635e-06}, {"id": 1183, "seek": 517320, "start": 5173.2, "end": 5180.44, "text": " So we can now have, say, 32 activations in each of those spots.", "tokens": [407, 321, 393, 586, 362, 11, 584, 11, 8858, 2430, 763, 294, 1184, 295, 729, 10681, 13], "temperature": 0.0, "avg_logprob": -0.07980669140815735, "compression_ratio": 1.515625, "no_speech_prob": 9.13248243250564e-07}, {"id": 1184, "seek": 517320, "start": 5180.44, "end": 5186.139999999999, "text": " And so that's what modern convolutional neural networks kind of tend to look like, right?", "tokens": [400, 370, 300, 311, 437, 4363, 45216, 304, 18161, 9590, 733, 295, 3928, 281, 574, 411, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.07980669140815735, "compression_ratio": 1.515625, "no_speech_prob": 9.13248243250564e-07}, {"id": 1185, "seek": 517320, "start": 5186.139999999999, "end": 5197.96, "text": " And so we can actually see that if we go into our pets and we grab our CNN, right, and we're", "tokens": [400, 370, 321, 393, 767, 536, 300, 498, 321, 352, 666, 527, 19897, 293, 321, 4444, 527, 24859, 11, 558, 11, 293, 321, 434], "temperature": 0.0, "avg_logprob": -0.07980669140815735, "compression_ratio": 1.515625, "no_speech_prob": 9.13248243250564e-07}, {"id": 1186, "seek": 517320, "start": 5197.96, "end": 5200.9, "text": " going to take a look at this particular cat.", "tokens": [516, 281, 747, 257, 574, 412, 341, 1729, 3857, 13], "temperature": 0.0, "avg_logprob": -0.07980669140815735, "compression_ratio": 1.515625, "no_speech_prob": 9.13248243250564e-07}, {"id": 1187, "seek": 520090, "start": 5200.9, "end": 5206.28, "text": " So if we go x, y equals valid data set, some index, so let's just grab the 0th.", "tokens": [407, 498, 321, 352, 2031, 11, 288, 6915, 7363, 1412, 992, 11, 512, 8186, 11, 370, 718, 311, 445, 4444, 264, 1958, 392, 13], "temperature": 0.0, "avg_logprob": -0.17239418736210577, "compression_ratio": 1.6, "no_speech_prob": 8.66455229697749e-06}, {"id": 1188, "seek": 520090, "start": 5206.28, "end": 5209.719999999999, "text": " We'll go dot show, and we'll print out the value of y.", "tokens": [492, 603, 352, 5893, 855, 11, 293, 321, 603, 4482, 484, 264, 2158, 295, 288, 13], "temperature": 0.0, "avg_logprob": -0.17239418736210577, "compression_ratio": 1.6, "no_speech_prob": 8.66455229697749e-06}, {"id": 1189, "seek": 520090, "start": 5209.719999999999, "end": 5213.96, "text": " Apparently this cat is of category main-koon.", "tokens": [16755, 341, 3857, 307, 295, 7719, 2135, 12, 74, 4106, 13], "temperature": 0.0, "avg_logprob": -0.17239418736210577, "compression_ratio": 1.6, "no_speech_prob": 8.66455229697749e-06}, {"id": 1190, "seek": 520090, "start": 5213.96, "end": 5218.86, "text": " So until a week ago, I was not at all familiar that there's a cat called a main-koon.", "tokens": [407, 1826, 257, 1243, 2057, 11, 286, 390, 406, 412, 439, 4963, 300, 456, 311, 257, 3857, 1219, 257, 2135, 12, 74, 4106, 13], "temperature": 0.0, "avg_logprob": -0.17239418736210577, "compression_ratio": 1.6, "no_speech_prob": 8.66455229697749e-06}, {"id": 1191, "seek": 520090, "start": 5218.86, "end": 5226.679999999999, "text": " Having spent all week with this particular cat, I am now deeply familiar with this main-koon.", "tokens": [10222, 4418, 439, 1243, 365, 341, 1729, 3857, 11, 286, 669, 586, 8760, 4963, 365, 341, 2135, 12, 74, 4106, 13], "temperature": 0.0, "avg_logprob": -0.17239418736210577, "compression_ratio": 1.6, "no_speech_prob": 8.66455229697749e-06}, {"id": 1192, "seek": 522668, "start": 5226.68, "end": 5236.92, "text": " So we can, if we go learn.summary, remember that our input we asked for was 352 by 352", "tokens": [407, 321, 393, 11, 498, 321, 352, 1466, 13, 82, 40879, 822, 11, 1604, 300, 527, 4846, 321, 2351, 337, 390, 6976, 17, 538, 6976, 17], "temperature": 0.0, "avg_logprob": -0.1589738638094156, "compression_ratio": 1.5247524752475248, "no_speech_prob": 1.9637927834992297e-06}, {"id": 1193, "seek": 522668, "start": 5236.92, "end": 5237.92, "text": " pixels.", "tokens": [18668, 13], "temperature": 0.0, "avg_logprob": -0.1589738638094156, "compression_ratio": 1.5247524752475248, "no_speech_prob": 1.9637927834992297e-06}, {"id": 1194, "seek": 522668, "start": 5237.92, "end": 5241.400000000001, "text": " Generally speaking, the very first convolution tends to have a stride 2.", "tokens": [21082, 4124, 11, 264, 588, 700, 45216, 12258, 281, 362, 257, 1056, 482, 568, 13], "temperature": 0.0, "avg_logprob": -0.1589738638094156, "compression_ratio": 1.5247524752475248, "no_speech_prob": 1.9637927834992297e-06}, {"id": 1195, "seek": 522668, "start": 5241.400000000001, "end": 5246.84, "text": " So after the first layer, it's 176 by 176.", "tokens": [407, 934, 264, 700, 4583, 11, 309, 311, 3282, 21, 538, 3282, 21, 13], "temperature": 0.0, "avg_logprob": -0.1589738638094156, "compression_ratio": 1.5247524752475248, "no_speech_prob": 1.9637927834992297e-06}, {"id": 1196, "seek": 522668, "start": 5246.84, "end": 5247.84, "text": " So this is learn.summary.", "tokens": [407, 341, 307, 1466, 13, 82, 40879, 822, 13], "temperature": 0.0, "avg_logprob": -0.1589738638094156, "compression_ratio": 1.5247524752475248, "no_speech_prob": 1.9637927834992297e-06}, {"id": 1197, "seek": 522668, "start": 5247.84, "end": 5251.6, "text": " We'll print out for you the output shape after every layer.", "tokens": [492, 603, 4482, 484, 337, 291, 264, 5598, 3909, 934, 633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1589738638094156, "compression_ratio": 1.5247524752475248, "no_speech_prob": 1.9637927834992297e-06}, {"id": 1198, "seek": 522668, "start": 5251.6, "end": 5253.92, "text": " 176 by 176.", "tokens": [3282, 21, 538, 3282, 21, 13], "temperature": 0.0, "avg_logprob": -0.1589738638094156, "compression_ratio": 1.5247524752475248, "no_speech_prob": 1.9637927834992297e-06}, {"id": 1199, "seek": 525392, "start": 5253.92, "end": 5259.4, "text": " And the first set of convolutions has 64 activations.", "tokens": [400, 264, 700, 992, 295, 3754, 15892, 575, 12145, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.12189965370373848, "compression_ratio": 1.5303867403314917, "no_speech_prob": 2.5612744138925336e-06}, {"id": 1200, "seek": 525392, "start": 5259.4, "end": 5270.16, "text": " And we can actually see that if we type in learn.model, you can see here it's a 2D conv", "tokens": [400, 321, 393, 767, 536, 300, 498, 321, 2010, 294, 1466, 13, 8014, 338, 11, 291, 393, 536, 510, 309, 311, 257, 568, 35, 3754], "temperature": 0.0, "avg_logprob": -0.12189965370373848, "compression_ratio": 1.5303867403314917, "no_speech_prob": 2.5612744138925336e-06}, {"id": 1201, "seek": 525392, "start": 5270.16, "end": 5280.32, "text": " with three input channels and 64 output channels, at a stride of 2.", "tokens": [365, 1045, 4846, 9235, 293, 12145, 5598, 9235, 11, 412, 257, 1056, 482, 295, 568, 13], "temperature": 0.0, "avg_logprob": -0.12189965370373848, "compression_ratio": 1.5303867403314917, "no_speech_prob": 2.5612744138925336e-06}, {"id": 1202, "seek": 525392, "start": 5280.32, "end": 5283.52, "text": " And interestingly, it actually starts with a kernel size of 7 by 7.", "tokens": [400, 25873, 11, 309, 767, 3719, 365, 257, 28256, 2744, 295, 1614, 538, 1614, 13], "temperature": 0.0, "avg_logprob": -0.12189965370373848, "compression_ratio": 1.5303867403314917, "no_speech_prob": 2.5612744138925336e-06}, {"id": 1203, "seek": 528352, "start": 5283.52, "end": 5286.240000000001, "text": " So like nearly all of the convolutions are 3 by 3.", "tokens": [407, 411, 6217, 439, 295, 264, 3754, 15892, 366, 805, 538, 805, 13], "temperature": 0.0, "avg_logprob": -0.17165726762476977, "compression_ratio": 1.6300813008130082, "no_speech_prob": 1.7231288438779302e-05}, {"id": 1204, "seek": 528352, "start": 5286.240000000001, "end": 5289.240000000001, "text": " See, they're all 3 by 3.", "tokens": [3008, 11, 436, 434, 439, 805, 538, 805, 13], "temperature": 0.0, "avg_logprob": -0.17165726762476977, "compression_ratio": 1.6300813008130082, "no_speech_prob": 1.7231288438779302e-05}, {"id": 1205, "seek": 528352, "start": 5289.240000000001, "end": 5293.84, "text": " For reasons we'll talk about in part 2, we often use a larger kernel for the very first", "tokens": [1171, 4112, 321, 603, 751, 466, 294, 644, 568, 11, 321, 2049, 764, 257, 4833, 28256, 337, 264, 588, 700], "temperature": 0.0, "avg_logprob": -0.17165726762476977, "compression_ratio": 1.6300813008130082, "no_speech_prob": 1.7231288438779302e-05}, {"id": 1206, "seek": 528352, "start": 5293.84, "end": 5295.200000000001, "text": " one.", "tokens": [472, 13], "temperature": 0.0, "avg_logprob": -0.17165726762476977, "compression_ratio": 1.6300813008130082, "no_speech_prob": 1.7231288438779302e-05}, {"id": 1207, "seek": 528352, "start": 5295.200000000001, "end": 5297.92, "text": " If you use a larger kernel, you have to use more padding.", "tokens": [759, 291, 764, 257, 4833, 28256, 11, 291, 362, 281, 764, 544, 39562, 13], "temperature": 0.0, "avg_logprob": -0.17165726762476977, "compression_ratio": 1.6300813008130082, "no_speech_prob": 1.7231288438779302e-05}, {"id": 1208, "seek": 528352, "start": 5297.92, "end": 5304.200000000001, "text": " So we have to use kernel size, int divide by 2 padding to make sure we don't lose anything.", "tokens": [407, 321, 362, 281, 764, 28256, 2744, 11, 560, 9845, 538, 568, 39562, 281, 652, 988, 321, 500, 380, 3624, 1340, 13], "temperature": 0.0, "avg_logprob": -0.17165726762476977, "compression_ratio": 1.6300813008130082, "no_speech_prob": 1.7231288438779302e-05}, {"id": 1209, "seek": 528352, "start": 5304.200000000001, "end": 5309.22, "text": " So we now have 64 output channels.", "tokens": [407, 321, 586, 362, 12145, 5598, 9235, 13], "temperature": 0.0, "avg_logprob": -0.17165726762476977, "compression_ratio": 1.6300813008130082, "no_speech_prob": 1.7231288438779302e-05}, {"id": 1210, "seek": 528352, "start": 5309.22, "end": 5312.88, "text": " And since it was stride 2, it's now 176 by 176.", "tokens": [400, 1670, 309, 390, 1056, 482, 568, 11, 309, 311, 586, 3282, 21, 538, 3282, 21, 13], "temperature": 0.0, "avg_logprob": -0.17165726762476977, "compression_ratio": 1.6300813008130082, "no_speech_prob": 1.7231288438779302e-05}, {"id": 1211, "seek": 531288, "start": 5312.88, "end": 5321.24, "text": " And then as we go along, you'll see that from time to time, we have, go from 88 by 88 to", "tokens": [400, 550, 382, 321, 352, 2051, 11, 291, 603, 536, 300, 490, 565, 281, 565, 11, 321, 362, 11, 352, 490, 24587, 538, 24587, 281], "temperature": 0.0, "avg_logprob": -0.16264157599591195, "compression_ratio": 1.495049504950495, "no_speech_prob": 4.860406079387758e-06}, {"id": 1212, "seek": 531288, "start": 5321.24, "end": 5324.32, "text": " 40 by 44, the grid size.", "tokens": [3356, 538, 16408, 11, 264, 10748, 2744, 13], "temperature": 0.0, "avg_logprob": -0.16264157599591195, "compression_ratio": 1.495049504950495, "no_speech_prob": 4.860406079387758e-06}, {"id": 1213, "seek": 531288, "start": 5324.32, "end": 5326.36, "text": " So that was a 2D conv.", "tokens": [407, 300, 390, 257, 568, 35, 3754, 13], "temperature": 0.0, "avg_logprob": -0.16264157599591195, "compression_ratio": 1.495049504950495, "no_speech_prob": 4.860406079387758e-06}, {"id": 1214, "seek": 531288, "start": 5326.36, "end": 5335.4800000000005, "text": " And then when we do that, we generally double the number of channels.", "tokens": [400, 550, 562, 321, 360, 300, 11, 321, 5101, 3834, 264, 1230, 295, 9235, 13], "temperature": 0.0, "avg_logprob": -0.16264157599591195, "compression_ratio": 1.495049504950495, "no_speech_prob": 4.860406079387758e-06}, {"id": 1215, "seek": 531288, "start": 5335.4800000000005, "end": 5339.2, "text": " So we keep going through a few more cons.", "tokens": [407, 321, 1066, 516, 807, 257, 1326, 544, 1014, 13], "temperature": 0.0, "avg_logprob": -0.16264157599591195, "compression_ratio": 1.495049504950495, "no_speech_prob": 4.860406079387758e-06}, {"id": 1216, "seek": 531288, "start": 5339.2, "end": 5341.400000000001, "text": " And as you can see, they've got batch norm and value.", "tokens": [400, 382, 291, 393, 536, 11, 436, 600, 658, 15245, 2026, 293, 2158, 13], "temperature": 0.0, "avg_logprob": -0.16264157599591195, "compression_ratio": 1.495049504950495, "no_speech_prob": 4.860406079387758e-06}, {"id": 1217, "seek": 534140, "start": 5341.4, "end": 5344.12, "text": " It's kind of pretty standard.", "tokens": [467, 311, 733, 295, 1238, 3832, 13], "temperature": 0.0, "avg_logprob": -0.1501293182373047, "compression_ratio": 1.5277777777777777, "no_speech_prob": 5.862761554453755e-06}, {"id": 1218, "seek": 534140, "start": 5344.12, "end": 5346.48, "text": " And eventually we do it again.", "tokens": [400, 4728, 321, 360, 309, 797, 13], "temperature": 0.0, "avg_logprob": -0.1501293182373047, "compression_ratio": 1.5277777777777777, "no_speech_prob": 5.862761554453755e-06}, {"id": 1219, "seek": 534140, "start": 5346.48, "end": 5349.639999999999, "text": " Another stride 2 conv, which again doubles.", "tokens": [3996, 1056, 482, 568, 3754, 11, 597, 797, 31634, 13], "temperature": 0.0, "avg_logprob": -0.1501293182373047, "compression_ratio": 1.5277777777777777, "no_speech_prob": 5.862761554453755e-06}, {"id": 1220, "seek": 534140, "start": 5349.639999999999, "end": 5354.12, "text": " We've now got 512 by 11 by 11.", "tokens": [492, 600, 586, 658, 1025, 4762, 538, 2975, 538, 2975, 13], "temperature": 0.0, "avg_logprob": -0.1501293182373047, "compression_ratio": 1.5277777777777777, "no_speech_prob": 5.862761554453755e-06}, {"id": 1221, "seek": 534140, "start": 5354.12, "end": 5359.2, "text": " And that's basically where we finish the main part of the network.", "tokens": [400, 300, 311, 1936, 689, 321, 2413, 264, 2135, 644, 295, 264, 3209, 13], "temperature": 0.0, "avg_logprob": -0.1501293182373047, "compression_ratio": 1.5277777777777777, "no_speech_prob": 5.862761554453755e-06}, {"id": 1222, "seek": 534140, "start": 5359.2, "end": 5364.12, "text": " We end up with 512 channels, 11 by 11.", "tokens": [492, 917, 493, 365, 1025, 4762, 9235, 11, 2975, 538, 2975, 13], "temperature": 0.0, "avg_logprob": -0.1501293182373047, "compression_ratio": 1.5277777777777777, "no_speech_prob": 5.862761554453755e-06}, {"id": 1223, "seek": 534140, "start": 5364.12, "end": 5369.5199999999995, "text": " Okay, so we're actually at a point where we're going to be able to do this heat map now.", "tokens": [1033, 11, 370, 321, 434, 767, 412, 257, 935, 689, 321, 434, 516, 281, 312, 1075, 281, 360, 341, 3738, 4471, 586, 13], "temperature": 0.0, "avg_logprob": -0.1501293182373047, "compression_ratio": 1.5277777777777777, "no_speech_prob": 5.862761554453755e-06}, {"id": 1224, "seek": 536952, "start": 5369.52, "end": 5372.120000000001, "text": " So let's try and work through it.", "tokens": [407, 718, 311, 853, 293, 589, 807, 309, 13], "temperature": 0.0, "avg_logprob": -0.0995392309171017, "compression_ratio": 1.6304347826086956, "no_speech_prob": 1.5534875501543866e-06}, {"id": 1225, "seek": 536952, "start": 5372.120000000001, "end": 5378.88, "text": " Before we do, I want to show you how you can do your own manual convolutions, because it's", "tokens": [4546, 321, 360, 11, 286, 528, 281, 855, 291, 577, 291, 393, 360, 428, 1065, 9688, 3754, 15892, 11, 570, 309, 311], "temperature": 0.0, "avg_logprob": -0.0995392309171017, "compression_ratio": 1.6304347826086956, "no_speech_prob": 1.5534875501543866e-06}, {"id": 1226, "seek": 536952, "start": 5378.88, "end": 5380.6, "text": " kind of fun.", "tokens": [733, 295, 1019, 13], "temperature": 0.0, "avg_logprob": -0.0995392309171017, "compression_ratio": 1.6304347826086956, "no_speech_prob": 1.5534875501543866e-06}, {"id": 1227, "seek": 536952, "start": 5380.6, "end": 5384.660000000001, "text": " So we're going to start with this picture of a main coon.", "tokens": [407, 321, 434, 516, 281, 722, 365, 341, 3036, 295, 257, 2135, 598, 266, 13], "temperature": 0.0, "avg_logprob": -0.0995392309171017, "compression_ratio": 1.6304347826086956, "no_speech_prob": 1.5534875501543866e-06}, {"id": 1228, "seek": 536952, "start": 5384.660000000001, "end": 5388.6, "text": " And I've created a convolutional kernel.", "tokens": [400, 286, 600, 2942, 257, 45216, 304, 28256, 13], "temperature": 0.0, "avg_logprob": -0.0995392309171017, "compression_ratio": 1.6304347826086956, "no_speech_prob": 1.5534875501543866e-06}, {"id": 1229, "seek": 536952, "start": 5388.6, "end": 5394.4400000000005, "text": " And so as you can see, this one has a right edge and a bottom edge with positive numbers.", "tokens": [400, 370, 382, 291, 393, 536, 11, 341, 472, 575, 257, 558, 4691, 293, 257, 2767, 4691, 365, 3353, 3547, 13], "temperature": 0.0, "avg_logprob": -0.0995392309171017, "compression_ratio": 1.6304347826086956, "no_speech_prob": 1.5534875501543866e-06}, {"id": 1230, "seek": 536952, "start": 5394.4400000000005, "end": 5396.860000000001, "text": " And just inside that, it's got negative numbers.", "tokens": [400, 445, 1854, 300, 11, 309, 311, 658, 3671, 3547, 13], "temperature": 0.0, "avg_logprob": -0.0995392309171017, "compression_ratio": 1.6304347826086956, "no_speech_prob": 1.5534875501543866e-06}, {"id": 1231, "seek": 539686, "start": 5396.86, "end": 5403.799999999999, "text": " So I'm thinking this should show me bottom right edges.", "tokens": [407, 286, 478, 1953, 341, 820, 855, 385, 2767, 558, 8819, 13], "temperature": 0.0, "avg_logprob": -0.10556856155395508, "compression_ratio": 1.4761904761904763, "no_speech_prob": 5.338115443009883e-06}, {"id": 1232, "seek": 539686, "start": 5403.799999999999, "end": 5405.839999999999, "text": " So that's my tensor.", "tokens": [407, 300, 311, 452, 40863, 13], "temperature": 0.0, "avg_logprob": -0.10556856155395508, "compression_ratio": 1.4761904761904763, "no_speech_prob": 5.338115443009883e-06}, {"id": 1233, "seek": 539686, "start": 5405.839999999999, "end": 5416.48, "text": " Now one complexity is that that 3 by 3 kernel cannot be used for this purpose, because I", "tokens": [823, 472, 14024, 307, 300, 300, 805, 538, 805, 28256, 2644, 312, 1143, 337, 341, 4334, 11, 570, 286], "temperature": 0.0, "avg_logprob": -0.10556856155395508, "compression_ratio": 1.4761904761904763, "no_speech_prob": 5.338115443009883e-06}, {"id": 1234, "seek": 539686, "start": 5416.48, "end": 5418.08, "text": " need two more dimensions.", "tokens": [643, 732, 544, 12819, 13], "temperature": 0.0, "avg_logprob": -0.10556856155395508, "compression_ratio": 1.4761904761904763, "no_speech_prob": 5.338115443009883e-06}, {"id": 1235, "seek": 539686, "start": 5418.08, "end": 5426.219999999999, "text": " The first is I need the third dimension to say how to combine the red, green, and blue.", "tokens": [440, 700, 307, 286, 643, 264, 2636, 10139, 281, 584, 577, 281, 10432, 264, 2182, 11, 3092, 11, 293, 3344, 13], "temperature": 0.0, "avg_logprob": -0.10556856155395508, "compression_ratio": 1.4761904761904763, "no_speech_prob": 5.338115443009883e-06}, {"id": 1236, "seek": 542622, "start": 5426.22, "end": 5435.400000000001, "text": " So what I do is I say.expand, this is my 3 by 3, and I pop another 3 on the start.", "tokens": [407, 437, 286, 360, 307, 286, 584, 2411, 15952, 474, 11, 341, 307, 452, 805, 538, 805, 11, 293, 286, 1665, 1071, 805, 322, 264, 722, 13], "temperature": 0.0, "avg_logprob": -0.127096485208582, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.3006881999899633e-05}, {"id": 1237, "seek": 542622, "start": 5435.400000000001, "end": 5444.4400000000005, "text": " What.expand does is it says create a 3 by 3 by 3 tensor by simply copying this one three", "tokens": [708, 2411, 15952, 474, 775, 307, 309, 1619, 1884, 257, 805, 538, 805, 538, 805, 40863, 538, 2935, 27976, 341, 472, 1045], "temperature": 0.0, "avg_logprob": -0.127096485208582, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.3006881999899633e-05}, {"id": 1238, "seek": 542622, "start": 5444.4400000000005, "end": 5445.4400000000005, "text": " times.", "tokens": [1413, 13], "temperature": 0.0, "avg_logprob": -0.127096485208582, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.3006881999899633e-05}, {"id": 1239, "seek": 542622, "start": 5445.4400000000005, "end": 5447.84, "text": " I mean honestly, it doesn't actually copy it.", "tokens": [286, 914, 6095, 11, 309, 1177, 380, 767, 5055, 309, 13], "temperature": 0.0, "avg_logprob": -0.127096485208582, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.3006881999899633e-05}, {"id": 1240, "seek": 542622, "start": 5447.84, "end": 5453.16, "text": " It pretends to have copied it, but it just basically refers to the same block of memory.", "tokens": [467, 1162, 2581, 281, 362, 25365, 309, 11, 457, 309, 445, 1936, 14942, 281, 264, 912, 3461, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.127096485208582, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.3006881999899633e-05}, {"id": 1241, "seek": 542622, "start": 5453.16, "end": 5455.96, "text": " So it kind of copies it in a memory efficient way.", "tokens": [407, 309, 733, 295, 14341, 309, 294, 257, 4675, 7148, 636, 13], "temperature": 0.0, "avg_logprob": -0.127096485208582, "compression_ratio": 1.6177777777777778, "no_speech_prob": 1.3006881999899633e-05}, {"id": 1242, "seek": 545596, "start": 5455.96, "end": 5462.24, "text": " So this one here is now three copies of that.", "tokens": [407, 341, 472, 510, 307, 586, 1045, 14341, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.08710710781136739, "compression_ratio": 1.6359649122807018, "no_speech_prob": 5.507584319275338e-06}, {"id": 1243, "seek": 545596, "start": 5462.24, "end": 5466.84, "text": " And the reason for that is that I want to treat red and green and blue the same way", "tokens": [400, 264, 1778, 337, 300, 307, 300, 286, 528, 281, 2387, 2182, 293, 3092, 293, 3344, 264, 912, 636], "temperature": 0.0, "avg_logprob": -0.08710710781136739, "compression_ratio": 1.6359649122807018, "no_speech_prob": 5.507584319275338e-06}, {"id": 1244, "seek": 545596, "start": 5466.84, "end": 5470.3, "text": " for this little manual kernel I'm showing you.", "tokens": [337, 341, 707, 9688, 28256, 286, 478, 4099, 291, 13], "temperature": 0.0, "avg_logprob": -0.08710710781136739, "compression_ratio": 1.6359649122807018, "no_speech_prob": 5.507584319275338e-06}, {"id": 1245, "seek": 545596, "start": 5470.3, "end": 5476.36, "text": " And then we need one more axis, because rather than actually having a separate kernel, like", "tokens": [400, 550, 321, 643, 472, 544, 10298, 11, 570, 2831, 813, 767, 1419, 257, 4994, 28256, 11, 411], "temperature": 0.0, "avg_logprob": -0.08710710781136739, "compression_ratio": 1.6359649122807018, "no_speech_prob": 5.507584319275338e-06}, {"id": 1246, "seek": 545596, "start": 5476.36, "end": 5482.04, "text": " I kind of printed these as if they were multiple kernels, what we actually do is we use a rank", "tokens": [286, 733, 295, 13567, 613, 382, 498, 436, 645, 3866, 23434, 1625, 11, 437, 321, 767, 360, 307, 321, 764, 257, 6181], "temperature": 0.0, "avg_logprob": -0.08710710781136739, "compression_ratio": 1.6359649122807018, "no_speech_prob": 5.507584319275338e-06}, {"id": 1247, "seek": 545596, "start": 5482.04, "end": 5483.9800000000005, "text": " 4 tensor.", "tokens": [1017, 40863, 13], "temperature": 0.0, "avg_logprob": -0.08710710781136739, "compression_ratio": 1.6359649122807018, "no_speech_prob": 5.507584319275338e-06}, {"id": 1248, "seek": 548398, "start": 5483.98, "end": 5490.679999999999, "text": " And so the very first axis is for every separate kernel that we have.", "tokens": [400, 370, 264, 588, 700, 10298, 307, 337, 633, 4994, 28256, 300, 321, 362, 13], "temperature": 0.0, "avg_logprob": -0.09803957409328884, "compression_ratio": 1.6280193236714975, "no_speech_prob": 1.4063904927752446e-05}, {"id": 1249, "seek": 548398, "start": 5490.679999999999, "end": 5493.04, "text": " So in this case, I'm just going to create one kernel.", "tokens": [407, 294, 341, 1389, 11, 286, 478, 445, 516, 281, 1884, 472, 28256, 13], "temperature": 0.0, "avg_logprob": -0.09803957409328884, "compression_ratio": 1.6280193236714975, "no_speech_prob": 1.4063904927752446e-05}, {"id": 1250, "seek": 548398, "start": 5493.04, "end": 5498.759999999999, "text": " So to do a convolution, I still have to put this unit axis on the front.", "tokens": [407, 281, 360, 257, 45216, 11, 286, 920, 362, 281, 829, 341, 4985, 10298, 322, 264, 1868, 13], "temperature": 0.0, "avg_logprob": -0.09803957409328884, "compression_ratio": 1.6280193236714975, "no_speech_prob": 1.4063904927752446e-05}, {"id": 1251, "seek": 548398, "start": 5498.759999999999, "end": 5503.0, "text": " So you can see k.shape is now 1, 3, 3, 3.", "tokens": [407, 291, 393, 536, 350, 13, 82, 42406, 307, 586, 502, 11, 805, 11, 805, 11, 805, 13], "temperature": 0.0, "avg_logprob": -0.09803957409328884, "compression_ratio": 1.6280193236714975, "no_speech_prob": 1.4063904927752446e-05}, {"id": 1252, "seek": 548398, "start": 5503.0, "end": 5505.7, "text": " So it's a 3 by 3 kernel.", "tokens": [407, 309, 311, 257, 805, 538, 805, 28256, 13], "temperature": 0.0, "avg_logprob": -0.09803957409328884, "compression_ratio": 1.6280193236714975, "no_speech_prob": 1.4063904927752446e-05}, {"id": 1253, "seek": 548398, "start": 5505.7, "end": 5507.48, "text": " There are three of them.", "tokens": [821, 366, 1045, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.09803957409328884, "compression_ratio": 1.6280193236714975, "no_speech_prob": 1.4063904927752446e-05}, {"id": 1254, "seek": 548398, "start": 5507.48, "end": 5509.919999999999, "text": " And then that's just the one kernel that I have.", "tokens": [400, 550, 300, 311, 445, 264, 472, 28256, 300, 286, 362, 13], "temperature": 0.0, "avg_logprob": -0.09803957409328884, "compression_ratio": 1.6280193236714975, "no_speech_prob": 1.4063904927752446e-05}, {"id": 1255, "seek": 550992, "start": 5509.92, "end": 5514.68, "text": " So it kind of takes a while to get the feel for these higher dimensional tensors, because", "tokens": [407, 309, 733, 295, 2516, 257, 1339, 281, 483, 264, 841, 337, 613, 2946, 18795, 10688, 830, 11, 570], "temperature": 0.0, "avg_logprob": -0.11647966504096985, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.289306616556132e-06}, {"id": 1256, "seek": 550992, "start": 5514.68, "end": 5517.88, "text": " we're not used to writing out the 4D tensor.", "tokens": [321, 434, 406, 1143, 281, 3579, 484, 264, 1017, 35, 40863, 13], "temperature": 0.0, "avg_logprob": -0.11647966504096985, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.289306616556132e-06}, {"id": 1257, "seek": 550992, "start": 5517.88, "end": 5519.96, "text": " But just think of them like this.", "tokens": [583, 445, 519, 295, 552, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.11647966504096985, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.289306616556132e-06}, {"id": 1258, "seek": 550992, "start": 5519.96, "end": 5526.4400000000005, "text": " A 4D tensor is just a bunch of 3D tensors sitting on top of each other.", "tokens": [316, 1017, 35, 40863, 307, 445, 257, 3840, 295, 805, 35, 10688, 830, 3798, 322, 1192, 295, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.11647966504096985, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.289306616556132e-06}, {"id": 1259, "seek": 550992, "start": 5526.4400000000005, "end": 5529.72, "text": " So this is our 4D tensor.", "tokens": [407, 341, 307, 527, 1017, 35, 40863, 13], "temperature": 0.0, "avg_logprob": -0.11647966504096985, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.289306616556132e-06}, {"id": 1260, "seek": 550992, "start": 5529.72, "end": 5535.62, "text": " And then you can just call conv2d, passing in some image.", "tokens": [400, 550, 291, 393, 445, 818, 3754, 17, 67, 11, 8437, 294, 512, 3256, 13], "temperature": 0.0, "avg_logprob": -0.11647966504096985, "compression_ratio": 1.588235294117647, "no_speech_prob": 4.289306616556132e-06}, {"id": 1261, "seek": 553562, "start": 5535.62, "end": 5541.24, "text": " And so the image I'm going to use is the first part of my validation data set and the kernel.", "tokens": [400, 370, 264, 3256, 286, 478, 516, 281, 764, 307, 264, 700, 644, 295, 452, 24071, 1412, 992, 293, 264, 28256, 13], "temperature": 0.0, "avg_logprob": -0.10398233504522414, "compression_ratio": 1.5299145299145298, "no_speech_prob": 1.6280479258057312e-06}, {"id": 1262, "seek": 553562, "start": 5541.24, "end": 5547.96, "text": " There's one more trick, which is that in PyTorch, pretty much everything is expecting to work", "tokens": [821, 311, 472, 544, 4282, 11, 597, 307, 300, 294, 9953, 51, 284, 339, 11, 1238, 709, 1203, 307, 9650, 281, 589], "temperature": 0.0, "avg_logprob": -0.10398233504522414, "compression_ratio": 1.5299145299145298, "no_speech_prob": 1.6280479258057312e-06}, {"id": 1263, "seek": 553562, "start": 5547.96, "end": 5552.98, "text": " on a mini-batch, not on an individual thing.", "tokens": [322, 257, 8382, 12, 65, 852, 11, 406, 322, 364, 2609, 551, 13], "temperature": 0.0, "avg_logprob": -0.10398233504522414, "compression_ratio": 1.5299145299145298, "no_speech_prob": 1.6280479258057312e-06}, {"id": 1264, "seek": 553562, "start": 5552.98, "end": 5557.92, "text": " So in our case, we have to create a mini-batch of size 1.", "tokens": [407, 294, 527, 1389, 11, 321, 362, 281, 1884, 257, 8382, 12, 65, 852, 295, 2744, 502, 13], "temperature": 0.0, "avg_logprob": -0.10398233504522414, "compression_ratio": 1.5299145299145298, "no_speech_prob": 1.6280479258057312e-06}, {"id": 1265, "seek": 553562, "start": 5557.92, "end": 5563.0, "text": " So our original image is 3 channels by 352 by 352, height by width.", "tokens": [407, 527, 3380, 3256, 307, 805, 9235, 538, 6976, 17, 538, 6976, 17, 11, 6681, 538, 11402, 13], "temperature": 0.0, "avg_logprob": -0.10398233504522414, "compression_ratio": 1.5299145299145298, "no_speech_prob": 1.6280479258057312e-06}, {"id": 1266, "seek": 556300, "start": 5563.0, "end": 5566.56, "text": " Remember, PyTorch is channel by height by width.", "tokens": [5459, 11, 9953, 51, 284, 339, 307, 2269, 538, 6681, 538, 11402, 13], "temperature": 0.0, "avg_logprob": -0.11567006148691253, "compression_ratio": 1.632, "no_speech_prob": 2.1233663574093953e-06}, {"id": 1267, "seek": 556300, "start": 5566.56, "end": 5567.56, "text": " I want to create a mini-batch.", "tokens": [286, 528, 281, 1884, 257, 8382, 12, 65, 852, 13], "temperature": 0.0, "avg_logprob": -0.11567006148691253, "compression_ratio": 1.632, "no_speech_prob": 2.1233663574093953e-06}, {"id": 1268, "seek": 556300, "start": 5567.56, "end": 5573.4, "text": " So I need to create a rank 4 tensor, where the first axis is 1.", "tokens": [407, 286, 643, 281, 1884, 257, 6181, 1017, 40863, 11, 689, 264, 700, 10298, 307, 502, 13], "temperature": 0.0, "avg_logprob": -0.11567006148691253, "compression_ratio": 1.632, "no_speech_prob": 2.1233663574093953e-06}, {"id": 1269, "seek": 556300, "start": 5573.4, "end": 5578.16, "text": " In other words, it's a mini-batch of size 1, because that's what PyTorch expects.", "tokens": [682, 661, 2283, 11, 309, 311, 257, 8382, 12, 65, 852, 295, 2744, 502, 11, 570, 300, 311, 437, 9953, 51, 284, 339, 33280, 13], "temperature": 0.0, "avg_logprob": -0.11567006148691253, "compression_ratio": 1.632, "no_speech_prob": 2.1233663574093953e-06}, {"id": 1270, "seek": 556300, "start": 5578.16, "end": 5583.28, "text": " So there's something you can do in both PyTorch and NumPy, which is you can index into an", "tokens": [407, 456, 311, 746, 291, 393, 360, 294, 1293, 9953, 51, 284, 339, 293, 22592, 47, 88, 11, 597, 307, 291, 393, 8186, 666, 364], "temperature": 0.0, "avg_logprob": -0.11567006148691253, "compression_ratio": 1.632, "no_speech_prob": 2.1233663574093953e-06}, {"id": 1271, "seek": 556300, "start": 5583.28, "end": 5587.24, "text": " array or a tensor with a special value none.", "tokens": [10225, 420, 257, 40863, 365, 257, 2121, 2158, 6022, 13], "temperature": 0.0, "avg_logprob": -0.11567006148691253, "compression_ratio": 1.632, "no_speech_prob": 2.1233663574093953e-06}, {"id": 1272, "seek": 556300, "start": 5587.24, "end": 5592.98, "text": " And that creates a new unit axis in that point.", "tokens": [400, 300, 7829, 257, 777, 4985, 10298, 294, 300, 935, 13], "temperature": 0.0, "avg_logprob": -0.11567006148691253, "compression_ratio": 1.632, "no_speech_prob": 2.1233663574093953e-06}, {"id": 1273, "seek": 559298, "start": 5592.98, "end": 5598.719999999999, "text": " So T is my image of dimensions 3 by 352 by 352.", "tokens": [407, 314, 307, 452, 3256, 295, 12819, 805, 538, 6976, 17, 538, 6976, 17, 13], "temperature": 0.0, "avg_logprob": -0.1733956734339396, "compression_ratio": 1.3958333333333333, "no_speech_prob": 9.368619430460967e-06}, {"id": 1274, "seek": 559298, "start": 5598.719999999999, "end": 5606.719999999999, "text": " T none is a rank 4 tensor, a mini-batch of one image of 1 by 3 by 352 by 352.", "tokens": [314, 6022, 307, 257, 6181, 1017, 40863, 11, 257, 8382, 12, 65, 852, 295, 472, 3256, 295, 502, 538, 805, 538, 6976, 17, 538, 6976, 17, 13], "temperature": 0.0, "avg_logprob": -0.1733956734339396, "compression_ratio": 1.3958333333333333, "no_speech_prob": 9.368619430460967e-06}, {"id": 1275, "seek": 559298, "start": 5606.719999999999, "end": 5620.719999999999, "text": " And so now I can go Conv2D and get back my cat, specifically my main croon.", "tokens": [400, 370, 586, 286, 393, 352, 2656, 85, 17, 35, 293, 483, 646, 452, 3857, 11, 4682, 452, 2135, 4848, 266, 13], "temperature": 0.0, "avg_logprob": -0.1733956734339396, "compression_ratio": 1.3958333333333333, "no_speech_prob": 9.368619430460967e-06}, {"id": 1276, "seek": 562072, "start": 5620.72, "end": 5627.280000000001, "text": " So that's how you can play around with convolutions yourself.", "tokens": [407, 300, 311, 577, 291, 393, 862, 926, 365, 3754, 15892, 1803, 13], "temperature": 0.0, "avg_logprob": -0.11926643053690593, "compression_ratio": 1.2521739130434784, "no_speech_prob": 3.288726929895347e-06}, {"id": 1277, "seek": 562072, "start": 5627.280000000001, "end": 5631.0, "text": " So how are we going to do this to create a heat map?", "tokens": [407, 577, 366, 321, 516, 281, 360, 341, 281, 1884, 257, 3738, 4471, 30], "temperature": 0.0, "avg_logprob": -0.11926643053690593, "compression_ratio": 1.2521739130434784, "no_speech_prob": 3.288726929895347e-06}, {"id": 1278, "seek": 562072, "start": 5631.0, "end": 5641.400000000001, "text": " This is where things get fun.", "tokens": [639, 307, 689, 721, 483, 1019, 13], "temperature": 0.0, "avg_logprob": -0.11926643053690593, "compression_ratio": 1.2521739130434784, "no_speech_prob": 3.288726929895347e-06}, {"id": 1279, "seek": 564140, "start": 5641.4, "end": 5654.5599999999995, "text": " So what I mentioned was that I basically have my input, red, green, blue, and it goes through", "tokens": [407, 437, 286, 2835, 390, 300, 286, 1936, 362, 452, 4846, 11, 2182, 11, 3092, 11, 3344, 11, 293, 309, 1709, 807], "temperature": 0.0, "avg_logprob": -0.17042380885074013, "compression_ratio": 1.4768211920529801, "no_speech_prob": 1.5294024251488736e-06}, {"id": 1280, "seek": 564140, "start": 5654.5599999999995, "end": 5656.719999999999, "text": " a bunch of convolutional layers.", "tokens": [257, 3840, 295, 45216, 304, 7914, 13], "temperature": 0.0, "avg_logprob": -0.17042380885074013, "compression_ratio": 1.4768211920529801, "no_speech_prob": 1.5294024251488736e-06}, {"id": 1281, "seek": 564140, "start": 5656.719999999999, "end": 5664.5599999999995, "text": " I'll just write a little line to say a convolutional layer to create activations which have more", "tokens": [286, 603, 445, 2464, 257, 707, 1622, 281, 584, 257, 45216, 304, 4583, 281, 1884, 2430, 763, 597, 362, 544], "temperature": 0.0, "avg_logprob": -0.17042380885074013, "compression_ratio": 1.4768211920529801, "no_speech_prob": 1.5294024251488736e-06}, {"id": 1282, "seek": 566456, "start": 5664.56, "end": 5676.160000000001, "text": " and more channels and eventually smaller and smaller height by width.", "tokens": [293, 544, 9235, 293, 4728, 4356, 293, 4356, 6681, 538, 11402, 13], "temperature": 0.0, "avg_logprob": -0.16524111140858044, "compression_ratio": 1.353846153846154, "no_speech_prob": 1.2805207916244399e-05}, {"id": 1283, "seek": 566456, "start": 5676.160000000001, "end": 5680.04, "text": " Until eventually, remember we looked at the summary, we ended up with something which", "tokens": [9088, 4728, 11, 1604, 321, 2956, 412, 264, 12691, 11, 321, 4590, 493, 365, 746, 597], "temperature": 0.0, "avg_logprob": -0.16524111140858044, "compression_ratio": 1.353846153846154, "no_speech_prob": 1.2805207916244399e-05}, {"id": 1284, "seek": 566456, "start": 5680.04, "end": 5691.0, "text": " was 11 by 11 by 512.", "tokens": [390, 2975, 538, 2975, 538, 1025, 4762, 13], "temperature": 0.0, "avg_logprob": -0.16524111140858044, "compression_ratio": 1.353846153846154, "no_speech_prob": 1.2805207916244399e-05}, {"id": 1285, "seek": 569100, "start": 5691.0, "end": 5696.08, "text": " There's a whole bunch more layers that we skipped over.", "tokens": [821, 311, 257, 1379, 3840, 544, 7914, 300, 321, 30193, 670, 13], "temperature": 0.0, "avg_logprob": -0.11319335575761466, "compression_ratio": 1.411764705882353, "no_speech_prob": 8.530162631359417e-06}, {"id": 1286, "seek": 569100, "start": 5696.08, "end": 5710.96, "text": " Now there are 37 classes, because remember data.c is the number of classes we have, and", "tokens": [823, 456, 366, 13435, 5359, 11, 570, 1604, 1412, 13, 66, 307, 264, 1230, 295, 5359, 321, 362, 11, 293], "temperature": 0.0, "avg_logprob": -0.11319335575761466, "compression_ratio": 1.411764705882353, "no_speech_prob": 8.530162631359417e-06}, {"id": 1287, "seek": 569100, "start": 5710.96, "end": 5715.84, "text": " we can see that at the end here we end up with 37 features in our model.", "tokens": [321, 393, 536, 300, 412, 264, 917, 510, 321, 917, 493, 365, 13435, 4122, 294, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11319335575761466, "compression_ratio": 1.411764705882353, "no_speech_prob": 8.530162631359417e-06}, {"id": 1288, "seek": 571584, "start": 5715.84, "end": 5721.12, "text": " So that means that we end up with a probability for every one of the 37 breeds of cat and", "tokens": [407, 300, 1355, 300, 321, 917, 493, 365, 257, 8482, 337, 633, 472, 295, 264, 13435, 41609, 295, 3857, 293], "temperature": 0.0, "avg_logprob": -0.13007310458592006, "compression_ratio": 1.4851485148514851, "no_speech_prob": 2.295904778293334e-06}, {"id": 1289, "seek": 571584, "start": 5721.12, "end": 5723.360000000001, "text": " dog.", "tokens": [3000, 13], "temperature": 0.0, "avg_logprob": -0.13007310458592006, "compression_ratio": 1.4851485148514851, "no_speech_prob": 2.295904778293334e-06}, {"id": 1290, "seek": 571584, "start": 5723.360000000001, "end": 5725.72, "text": " So it's a vector of length 37.", "tokens": [407, 309, 311, 257, 8062, 295, 4641, 13435, 13], "temperature": 0.0, "avg_logprob": -0.13007310458592006, "compression_ratio": 1.4851485148514851, "no_speech_prob": 2.295904778293334e-06}, {"id": 1291, "seek": 571584, "start": 5725.72, "end": 5730.400000000001, "text": " That's our final output that we need, because that's what we're going to compare implicitly", "tokens": [663, 311, 527, 2572, 5598, 300, 321, 643, 11, 570, 300, 311, 437, 321, 434, 516, 281, 6794, 26947, 356], "temperature": 0.0, "avg_logprob": -0.13007310458592006, "compression_ratio": 1.4851485148514851, "no_speech_prob": 2.295904778293334e-06}, {"id": 1292, "seek": 571584, "start": 5730.400000000001, "end": 5738.96, "text": " to our one hot encoded matrix, which will have a 1 in the location for main croon.", "tokens": [281, 527, 472, 2368, 2058, 12340, 8141, 11, 597, 486, 362, 257, 502, 294, 264, 4914, 337, 2135, 4848, 266, 13], "temperature": 0.0, "avg_logprob": -0.13007310458592006, "compression_ratio": 1.4851485148514851, "no_speech_prob": 2.295904778293334e-06}, {"id": 1293, "seek": 573896, "start": 5738.96, "end": 5747.52, "text": " So somehow we need to get from this 11 by 11 by 512 to this 37.", "tokens": [407, 6063, 321, 643, 281, 483, 490, 341, 2975, 538, 2975, 538, 1025, 4762, 281, 341, 13435, 13], "temperature": 0.0, "avg_logprob": -0.12090332125440056, "compression_ratio": 1.5857988165680474, "no_speech_prob": 1.06160232462571e-05}, {"id": 1294, "seek": 573896, "start": 5747.52, "end": 5755.32, "text": " And so the way we do it is we actually take the average of every one of these 11 by 11", "tokens": [400, 370, 264, 636, 321, 360, 309, 307, 321, 767, 747, 264, 4274, 295, 633, 472, 295, 613, 2975, 538, 2975], "temperature": 0.0, "avg_logprob": -0.12090332125440056, "compression_ratio": 1.5857988165680474, "no_speech_prob": 1.06160232462571e-05}, {"id": 1295, "seek": 573896, "start": 5755.32, "end": 5756.32, "text": " faces.", "tokens": [8475, 13], "temperature": 0.0, "avg_logprob": -0.12090332125440056, "compression_ratio": 1.5857988165680474, "no_speech_prob": 1.06160232462571e-05}, {"id": 1296, "seek": 573896, "start": 5756.32, "end": 5757.92, "text": " We just take the main.", "tokens": [492, 445, 747, 264, 2135, 13], "temperature": 0.0, "avg_logprob": -0.12090332125440056, "compression_ratio": 1.5857988165680474, "no_speech_prob": 1.06160232462571e-05}, {"id": 1297, "seek": 573896, "start": 5757.92, "end": 5766.16, "text": " So we're going to take the main of this first face, take the main, and that gets us one", "tokens": [407, 321, 434, 516, 281, 747, 264, 2135, 295, 341, 700, 1851, 11, 747, 264, 2135, 11, 293, 300, 2170, 505, 472], "temperature": 0.0, "avg_logprob": -0.12090332125440056, "compression_ratio": 1.5857988165680474, "no_speech_prob": 1.06160232462571e-05}, {"id": 1298, "seek": 576616, "start": 5766.16, "end": 5771.92, "text": " value, and then we'll take the second of the 512 faces and take that main, and that'll", "tokens": [2158, 11, 293, 550, 321, 603, 747, 264, 1150, 295, 264, 1025, 4762, 8475, 293, 747, 300, 2135, 11, 293, 300, 603], "temperature": 0.0, "avg_logprob": -0.14376824242728098, "compression_ratio": 1.5206611570247934, "no_speech_prob": 8.139656529237982e-06}, {"id": 1299, "seek": 576616, "start": 5771.92, "end": 5772.92, "text": " give us one more value.", "tokens": [976, 505, 472, 544, 2158, 13], "temperature": 0.0, "avg_logprob": -0.14376824242728098, "compression_ratio": 1.5206611570247934, "no_speech_prob": 8.139656529237982e-06}, {"id": 1300, "seek": 576616, "start": 5772.92, "end": 5784.12, "text": " So we'll do that for every face, and that will give us a 512 long vector.", "tokens": [407, 321, 603, 360, 300, 337, 633, 1851, 11, 293, 300, 486, 976, 505, 257, 1025, 4762, 938, 8062, 13], "temperature": 0.0, "avg_logprob": -0.14376824242728098, "compression_ratio": 1.5206611570247934, "no_speech_prob": 8.139656529237982e-06}, {"id": 1301, "seek": 578412, "start": 5784.12, "end": 5798.0, "text": " And so now all we need to do is pop that through a single matrix multiply of 512 by 37, and", "tokens": [400, 370, 586, 439, 321, 643, 281, 360, 307, 1665, 300, 807, 257, 2167, 8141, 12972, 295, 1025, 4762, 538, 13435, 11, 293], "temperature": 0.0, "avg_logprob": -0.07845086436117849, "compression_ratio": 1.4024390243902438, "no_speech_prob": 1.6028074014684535e-06}, {"id": 1302, "seek": 578412, "start": 5798.0, "end": 5803.44, "text": " that's going to give us an output vector of length 37.", "tokens": [300, 311, 516, 281, 976, 505, 364, 5598, 8062, 295, 4641, 13435, 13], "temperature": 0.0, "avg_logprob": -0.07845086436117849, "compression_ratio": 1.4024390243902438, "no_speech_prob": 1.6028074014684535e-06}, {"id": 1303, "seek": 578412, "start": 5803.44, "end": 5812.8, "text": " So this step here where we take the average of each face is called average pooling.", "tokens": [407, 341, 1823, 510, 689, 321, 747, 264, 4274, 295, 1184, 1851, 307, 1219, 4274, 7005, 278, 13], "temperature": 0.0, "avg_logprob": -0.07845086436117849, "compression_ratio": 1.4024390243902438, "no_speech_prob": 1.6028074014684535e-06}, {"id": 1304, "seek": 581280, "start": 5812.8, "end": 5815.28, "text": " So let's go back to our model and take a look.", "tokens": [407, 718, 311, 352, 646, 281, 527, 2316, 293, 747, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.16579871618447184, "compression_ratio": 1.6824644549763033, "no_speech_prob": 5.0936332627316006e-06}, {"id": 1305, "seek": 581280, "start": 5815.28, "end": 5816.88, "text": " Here it is.", "tokens": [1692, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.16579871618447184, "compression_ratio": 1.6824644549763033, "no_speech_prob": 5.0936332627316006e-06}, {"id": 1306, "seek": 581280, "start": 5816.88, "end": 5823.76, "text": " Here is our final 512, and here is, we'll talk about what a concat pooling is in part", "tokens": [1692, 307, 527, 2572, 1025, 4762, 11, 293, 510, 307, 11, 321, 603, 751, 466, 437, 257, 1588, 267, 7005, 278, 307, 294, 644], "temperature": 0.0, "avg_logprob": -0.16579871618447184, "compression_ratio": 1.6824644549763033, "no_speech_prob": 5.0936332627316006e-06}, {"id": 1307, "seek": 581280, "start": 5823.76, "end": 5824.76, "text": " two.", "tokens": [732, 13], "temperature": 0.0, "avg_logprob": -0.16579871618447184, "compression_ratio": 1.6824644549763033, "no_speech_prob": 5.0936332627316006e-06}, {"id": 1308, "seek": 581280, "start": 5824.76, "end": 5827.320000000001, "text": " For now we'll just focus on, this is a fast AI specialty.", "tokens": [1171, 586, 321, 603, 445, 1879, 322, 11, 341, 307, 257, 2370, 7318, 22000, 13], "temperature": 0.0, "avg_logprob": -0.16579871618447184, "compression_ratio": 1.6824644549763033, "no_speech_prob": 5.0936332627316006e-06}, {"id": 1309, "seek": 581280, "start": 5827.320000000001, "end": 5828.8, "text": " Everybody else just does this.", "tokens": [7646, 1646, 445, 775, 341, 13], "temperature": 0.0, "avg_logprob": -0.16579871618447184, "compression_ratio": 1.6824644549763033, "no_speech_prob": 5.0936332627316006e-06}, {"id": 1310, "seek": 581280, "start": 5828.8, "end": 5830.18, "text": " Average pool.", "tokens": [316, 3623, 7005, 13], "temperature": 0.0, "avg_logprob": -0.16579871618447184, "compression_ratio": 1.6824644549763033, "no_speech_prob": 5.0936332627316006e-06}, {"id": 1311, "seek": 581280, "start": 5830.18, "end": 5833.22, "text": " Average pool 2D with an output size of one.", "tokens": [316, 3623, 7005, 568, 35, 365, 364, 5598, 2744, 295, 472, 13], "temperature": 0.0, "avg_logprob": -0.16579871618447184, "compression_ratio": 1.6824644549763033, "no_speech_prob": 5.0936332627316006e-06}, {"id": 1312, "seek": 581280, "start": 5833.22, "end": 5835.04, "text": " So here it is.", "tokens": [407, 510, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.16579871618447184, "compression_ratio": 1.6824644549763033, "no_speech_prob": 5.0936332627316006e-06}, {"id": 1313, "seek": 581280, "start": 5835.04, "end": 5840.62, "text": " Average pool 2D with an output size of one.", "tokens": [316, 3623, 7005, 568, 35, 365, 364, 5598, 2744, 295, 472, 13], "temperature": 0.0, "avg_logprob": -0.16579871618447184, "compression_ratio": 1.6824644549763033, "no_speech_prob": 5.0936332627316006e-06}, {"id": 1314, "seek": 584062, "start": 5840.62, "end": 5850.5599999999995, "text": " And then, again, there's a bit of a special fast AI thing that we actually have two layers", "tokens": [400, 550, 11, 797, 11, 456, 311, 257, 857, 295, 257, 2121, 2370, 7318, 551, 300, 321, 767, 362, 732, 7914], "temperature": 0.0, "avg_logprob": -0.18343471225939298, "compression_ratio": 1.5026455026455026, "no_speech_prob": 4.637840902432799e-06}, {"id": 1315, "seek": 584062, "start": 5850.5599999999995, "end": 5857.24, "text": " here, but normally people then just have the one linear layer with the input of 512 and", "tokens": [510, 11, 457, 5646, 561, 550, 445, 362, 264, 472, 8213, 4583, 365, 264, 4846, 295, 1025, 4762, 293], "temperature": 0.0, "avg_logprob": -0.18343471225939298, "compression_ratio": 1.5026455026455026, "no_speech_prob": 4.637840902432799e-06}, {"id": 1316, "seek": 584062, "start": 5857.24, "end": 5862.68, "text": " the output of 37.", "tokens": [264, 5598, 295, 13435, 13], "temperature": 0.0, "avg_logprob": -0.18343471225939298, "compression_ratio": 1.5026455026455026, "no_speech_prob": 4.637840902432799e-06}, {"id": 1317, "seek": 584062, "start": 5862.68, "end": 5869.48, "text": " So what that means is that this little box over here where we want a one for main coon,", "tokens": [407, 437, 300, 1355, 307, 300, 341, 707, 2424, 670, 510, 689, 321, 528, 257, 472, 337, 2135, 598, 266, 11], "temperature": 0.0, "avg_logprob": -0.18343471225939298, "compression_ratio": 1.5026455026455026, "no_speech_prob": 4.637840902432799e-06}, {"id": 1318, "seek": 586948, "start": 5869.48, "end": 5876.32, "text": " we've got to have a box over here which needs to have a high value in that place so that", "tokens": [321, 600, 658, 281, 362, 257, 2424, 670, 510, 597, 2203, 281, 362, 257, 1090, 2158, 294, 300, 1081, 370, 300], "temperature": 0.0, "avg_logprob": -0.08984873093754413, "compression_ratio": 1.6185567010309279, "no_speech_prob": 6.339153515000362e-06}, {"id": 1319, "seek": 586948, "start": 5876.32, "end": 5878.04, "text": " the loss will be low.", "tokens": [264, 4470, 486, 312, 2295, 13], "temperature": 0.0, "avg_logprob": -0.08984873093754413, "compression_ratio": 1.6185567010309279, "no_speech_prob": 6.339153515000362e-06}, {"id": 1320, "seek": 586948, "start": 5878.04, "end": 5884.08, "text": " So if we're going to have a high value there, the only way to get it is with this matrix", "tokens": [407, 498, 321, 434, 516, 281, 362, 257, 1090, 2158, 456, 11, 264, 787, 636, 281, 483, 309, 307, 365, 341, 8141], "temperature": 0.0, "avg_logprob": -0.08984873093754413, "compression_ratio": 1.6185567010309279, "no_speech_prob": 6.339153515000362e-06}, {"id": 1321, "seek": 586948, "start": 5884.08, "end": 5891.4, "text": " multiplication is that it's going to represent a simple weighted linear combination of all", "tokens": [27290, 307, 300, 309, 311, 516, 281, 2906, 257, 2199, 32807, 8213, 6562, 295, 439], "temperature": 0.0, "avg_logprob": -0.08984873093754413, "compression_ratio": 1.6185567010309279, "no_speech_prob": 6.339153515000362e-06}, {"id": 1322, "seek": 586948, "start": 5891.4, "end": 5895.0, "text": " of the 512 values here.", "tokens": [295, 264, 1025, 4762, 4190, 510, 13], "temperature": 0.0, "avg_logprob": -0.08984873093754413, "compression_ratio": 1.6185567010309279, "no_speech_prob": 6.339153515000362e-06}, {"id": 1323, "seek": 589500, "start": 5895.0, "end": 5900.72, "text": " So if we're going to be able to say, I'm pretty confident this is a main coon, just by taking", "tokens": [407, 498, 321, 434, 516, 281, 312, 1075, 281, 584, 11, 286, 478, 1238, 6679, 341, 307, 257, 2135, 598, 266, 11, 445, 538, 1940], "temperature": 0.0, "avg_logprob": -0.11571859879927202, "compression_ratio": 1.6356275303643724, "no_speech_prob": 7.690354095757357e-07}, {"id": 1324, "seek": 589500, "start": 5900.72, "end": 5905.0, "text": " the weighted sum of a bunch of inputs, those inputs are going to have to represent features", "tokens": [264, 32807, 2408, 295, 257, 3840, 295, 15743, 11, 729, 15743, 366, 516, 281, 362, 281, 2906, 4122], "temperature": 0.0, "avg_logprob": -0.11571859879927202, "compression_ratio": 1.6356275303643724, "no_speech_prob": 7.690354095757357e-07}, {"id": 1325, "seek": 589500, "start": 5905.0, "end": 5912.96, "text": " like how fluffy is it, what color is its nose, how long is its legs, how pointy it is, all", "tokens": [411, 577, 22778, 307, 309, 11, 437, 2017, 307, 1080, 6690, 11, 577, 938, 307, 1080, 5668, 11, 577, 935, 88, 309, 307, 11, 439], "temperature": 0.0, "avg_logprob": -0.11571859879927202, "compression_ratio": 1.6356275303643724, "no_speech_prob": 7.690354095757357e-07}, {"id": 1326, "seek": 589500, "start": 5912.96, "end": 5915.3, "text": " the kinds of things that can be used.", "tokens": [264, 3685, 295, 721, 300, 393, 312, 1143, 13], "temperature": 0.0, "avg_logprob": -0.11571859879927202, "compression_ratio": 1.6356275303643724, "no_speech_prob": 7.690354095757357e-07}, {"id": 1327, "seek": 589500, "start": 5915.3, "end": 5919.66, "text": " Because for the other thing which figures out, is this a bulldog, it's got to use exactly", "tokens": [1436, 337, 264, 661, 551, 597, 9624, 484, 11, 307, 341, 257, 4693, 14833, 11, 309, 311, 658, 281, 764, 2293], "temperature": 0.0, "avg_logprob": -0.11571859879927202, "compression_ratio": 1.6356275303643724, "no_speech_prob": 7.690354095757357e-07}, {"id": 1328, "seek": 591966, "start": 5919.66, "end": 5925.639999999999, "text": " the same kind of 512 inputs with a different set of weights, because that's all a matrix", "tokens": [264, 912, 733, 295, 1025, 4762, 15743, 365, 257, 819, 992, 295, 17443, 11, 570, 300, 311, 439, 257, 8141], "temperature": 0.0, "avg_logprob": -0.13515960636423596, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.628047584745218e-06}, {"id": 1329, "seek": 591966, "start": 5925.639999999999, "end": 5927.92, "text": " multiplication is.", "tokens": [27290, 307, 13], "temperature": 0.0, "avg_logprob": -0.13515960636423596, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.628047584745218e-06}, {"id": 1330, "seek": 591966, "start": 5927.92, "end": 5935.0199999999995, "text": " It's just a bunch of weighted sums, a different weighted sum for each output.", "tokens": [467, 311, 445, 257, 3840, 295, 32807, 34499, 11, 257, 819, 32807, 2408, 337, 1184, 5598, 13], "temperature": 0.0, "avg_logprob": -0.13515960636423596, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.628047584745218e-06}, {"id": 1331, "seek": 591966, "start": 5935.0199999999995, "end": 5944.2, "text": " So therefore we know that this, potentially dozens or even hundreds of layers of convolutions", "tokens": [407, 4412, 321, 458, 300, 341, 11, 7263, 18431, 420, 754, 6779, 295, 7914, 295, 3754, 15892], "temperature": 0.0, "avg_logprob": -0.13515960636423596, "compression_ratio": 1.516304347826087, "no_speech_prob": 1.628047584745218e-06}, {"id": 1332, "seek": 594420, "start": 5944.2, "end": 5953.36, "text": " must have eventually come up with an 11 by 11 face for each of these features saying,", "tokens": [1633, 362, 4728, 808, 493, 365, 364, 2975, 538, 2975, 1851, 337, 1184, 295, 613, 4122, 1566, 11], "temperature": 0.0, "avg_logprob": -0.08902589841322466, "compression_ratio": 1.6329787234042554, "no_speech_prob": 9.81817993306322e-06}, {"id": 1333, "seek": 594420, "start": 5953.36, "end": 5961.04, "text": " in this little bit here, how much is that part of the image like a pointy ear?", "tokens": [294, 341, 707, 857, 510, 11, 577, 709, 307, 300, 644, 295, 264, 3256, 411, 257, 935, 88, 1273, 30], "temperature": 0.0, "avg_logprob": -0.08902589841322466, "compression_ratio": 1.6329787234042554, "no_speech_prob": 9.81817993306322e-06}, {"id": 1334, "seek": 594420, "start": 5961.04, "end": 5963.24, "text": " How much is it fluffy?", "tokens": [1012, 709, 307, 309, 22778, 30], "temperature": 0.0, "avg_logprob": -0.08902589841322466, "compression_ratio": 1.6329787234042554, "no_speech_prob": 9.81817993306322e-06}, {"id": 1335, "seek": 594420, "start": 5963.24, "end": 5965.28, "text": " How much is it like a long leg?", "tokens": [1012, 709, 307, 309, 411, 257, 938, 1676, 30], "temperature": 0.0, "avg_logprob": -0.08902589841322466, "compression_ratio": 1.6329787234042554, "no_speech_prob": 9.81817993306322e-06}, {"id": 1336, "seek": 594420, "start": 5965.28, "end": 5968.48, "text": " How much is it like a very red nose?", "tokens": [1012, 709, 307, 309, 411, 257, 588, 2182, 6690, 30], "temperature": 0.0, "avg_logprob": -0.08902589841322466, "compression_ratio": 1.6329787234042554, "no_speech_prob": 9.81817993306322e-06}, {"id": 1337, "seek": 594420, "start": 5968.48, "end": 5970.639999999999, "text": " So that's what all of those things must represent.", "tokens": [407, 300, 311, 437, 439, 295, 729, 721, 1633, 2906, 13], "temperature": 0.0, "avg_logprob": -0.08902589841322466, "compression_ratio": 1.6329787234042554, "no_speech_prob": 9.81817993306322e-06}, {"id": 1338, "seek": 597064, "start": 5970.64, "end": 5980.04, "text": " So each face is what we call, each of these represents a different feature.", "tokens": [407, 1184, 1851, 307, 437, 321, 818, 11, 1184, 295, 613, 8855, 257, 819, 4111, 13], "temperature": 0.0, "avg_logprob": -0.11081471359520628, "compression_ratio": 1.5845070422535212, "no_speech_prob": 8.664594133733772e-06}, {"id": 1339, "seek": 597064, "start": 5980.04, "end": 5987.06, "text": " So the outputs of these we can think of as different features.", "tokens": [407, 264, 23930, 295, 613, 321, 393, 519, 295, 382, 819, 4122, 13], "temperature": 0.0, "avg_logprob": -0.11081471359520628, "compression_ratio": 1.5845070422535212, "no_speech_prob": 8.664594133733772e-06}, {"id": 1340, "seek": 597064, "start": 5987.06, "end": 5995.96, "text": " So what we really want to know then is not so much what's the average across the 11 by", "tokens": [407, 437, 321, 534, 528, 281, 458, 550, 307, 406, 370, 709, 437, 311, 264, 4274, 2108, 264, 2975, 538], "temperature": 0.0, "avg_logprob": -0.11081471359520628, "compression_ratio": 1.5845070422535212, "no_speech_prob": 8.664594133733772e-06}, {"id": 1341, "seek": 599596, "start": 5995.96, "end": 6001.44, "text": " 11 to get this set of outputs, but what we really want to know is what's in each of these", "tokens": [2975, 281, 483, 341, 992, 295, 23930, 11, 457, 437, 321, 534, 528, 281, 458, 307, 437, 311, 294, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.10443474333963276, "compression_ratio": 1.6626506024096386, "no_speech_prob": 3.3405106023565168e-06}, {"id": 1342, "seek": 599596, "start": 6001.44, "end": 6003.6, "text": " 11 by 11 spots.", "tokens": [2975, 538, 2975, 10681, 13], "temperature": 0.0, "avg_logprob": -0.10443474333963276, "compression_ratio": 1.6626506024096386, "no_speech_prob": 3.3405106023565168e-06}, {"id": 1343, "seek": 599596, "start": 6003.6, "end": 6011.4, "text": " So what if instead of averaging across the 11 by 11, let's instead average across the", "tokens": [407, 437, 498, 2602, 295, 47308, 2108, 264, 2975, 538, 2975, 11, 718, 311, 2602, 4274, 2108, 264], "temperature": 0.0, "avg_logprob": -0.10443474333963276, "compression_ratio": 1.6626506024096386, "no_speech_prob": 3.3405106023565168e-06}, {"id": 1344, "seek": 599596, "start": 6011.4, "end": 6013.4, "text": " 512.", "tokens": [1025, 4762, 13], "temperature": 0.0, "avg_logprob": -0.10443474333963276, "compression_ratio": 1.6626506024096386, "no_speech_prob": 3.3405106023565168e-06}, {"id": 1345, "seek": 599596, "start": 6013.4, "end": 6020.86, "text": " If we average across the 512, that's going to give us a single 11 by 11 matrix.", "tokens": [759, 321, 4274, 2108, 264, 1025, 4762, 11, 300, 311, 516, 281, 976, 505, 257, 2167, 2975, 538, 2975, 8141, 13], "temperature": 0.0, "avg_logprob": -0.10443474333963276, "compression_ratio": 1.6626506024096386, "no_speech_prob": 3.3405106023565168e-06}, {"id": 1346, "seek": 602086, "start": 6020.86, "end": 6028.92, "text": " And each item, each grid point in that 11 by 11 matrix will be the average of how activated", "tokens": [400, 1184, 3174, 11, 1184, 10748, 935, 294, 300, 2975, 538, 2975, 8141, 486, 312, 264, 4274, 295, 577, 18157], "temperature": 0.0, "avg_logprob": -0.0878318644118035, "compression_ratio": 1.5706806282722514, "no_speech_prob": 2.9022962735325564e-06}, {"id": 1347, "seek": 602086, "start": 6028.92, "end": 6029.92, "text": " was that area.", "tokens": [390, 300, 1859, 13], "temperature": 0.0, "avg_logprob": -0.0878318644118035, "compression_ratio": 1.5706806282722514, "no_speech_prob": 2.9022962735325564e-06}, {"id": 1348, "seek": 602086, "start": 6029.92, "end": 6038.0, "text": " When it came to figuring out that this was a main Kuhn, how many signs of main Kuhnishness", "tokens": [1133, 309, 1361, 281, 15213, 484, 300, 341, 390, 257, 2135, 591, 3232, 77, 11, 577, 867, 7880, 295, 2135, 591, 3232, 77, 742, 1287], "temperature": 0.0, "avg_logprob": -0.0878318644118035, "compression_ratio": 1.5706806282722514, "no_speech_prob": 2.9022962735325564e-06}, {"id": 1349, "seek": 602086, "start": 6038.0, "end": 6043.94, "text": " was there in that part of the 11 by 11 grid?", "tokens": [390, 456, 294, 300, 644, 295, 264, 2975, 538, 2975, 10748, 30], "temperature": 0.0, "avg_logprob": -0.0878318644118035, "compression_ratio": 1.5706806282722514, "no_speech_prob": 2.9022962735325564e-06}, {"id": 1350, "seek": 602086, "start": 6043.94, "end": 6047.719999999999, "text": " And so that's actually what we do to create our heat map.", "tokens": [400, 370, 300, 311, 767, 437, 321, 360, 281, 1884, 527, 3738, 4471, 13], "temperature": 0.0, "avg_logprob": -0.0878318644118035, "compression_ratio": 1.5706806282722514, "no_speech_prob": 2.9022962735325564e-06}, {"id": 1351, "seek": 604772, "start": 6047.72, "end": 6052.16, "text": " So I think maybe the easiest way is to kind of work backwards.", "tokens": [407, 286, 519, 1310, 264, 12889, 636, 307, 281, 733, 295, 589, 12204, 13], "temperature": 0.0, "avg_logprob": -0.18616550940054435, "compression_ratio": 1.6816143497757847, "no_speech_prob": 5.862752004759386e-06}, {"id": 1352, "seek": 604772, "start": 6052.16, "end": 6054.240000000001, "text": " Here's our heat map.", "tokens": [1692, 311, 527, 3738, 4471, 13], "temperature": 0.0, "avg_logprob": -0.18616550940054435, "compression_ratio": 1.6816143497757847, "no_speech_prob": 5.862752004759386e-06}, {"id": 1353, "seek": 604772, "start": 6054.240000000001, "end": 6058.240000000001, "text": " And it comes from something called average activations.", "tokens": [400, 309, 1487, 490, 746, 1219, 4274, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.18616550940054435, "compression_ratio": 1.6816143497757847, "no_speech_prob": 5.862752004759386e-06}, {"id": 1354, "seek": 604772, "start": 6058.240000000001, "end": 6062.320000000001, "text": " And it's just a little bit of Matplotlib and FastAI.", "tokens": [400, 309, 311, 445, 257, 707, 857, 295, 6789, 564, 310, 38270, 293, 15968, 48698, 13], "temperature": 0.0, "avg_logprob": -0.18616550940054435, "compression_ratio": 1.6816143497757847, "no_speech_prob": 5.862752004759386e-06}, {"id": 1355, "seek": 604772, "start": 6062.320000000001, "end": 6069.280000000001, "text": " FastAI to show the image, and then Matplotlib to take the heat map which we passed in, which", "tokens": [15968, 48698, 281, 855, 264, 3256, 11, 293, 550, 6789, 564, 310, 38270, 281, 747, 264, 3738, 4471, 597, 321, 4678, 294, 11, 597], "temperature": 0.0, "avg_logprob": -0.18616550940054435, "compression_ratio": 1.6816143497757847, "no_speech_prob": 5.862752004759386e-06}, {"id": 1356, "seek": 604772, "start": 6069.280000000001, "end": 6073.280000000001, "text": " was called average activations, HM for heat map.", "tokens": [390, 1219, 4274, 2430, 763, 11, 389, 44, 337, 3738, 4471, 13], "temperature": 0.0, "avg_logprob": -0.18616550940054435, "compression_ratio": 1.6816143497757847, "no_speech_prob": 5.862752004759386e-06}, {"id": 1357, "seek": 604772, "start": 6073.280000000001, "end": 6076.88, "text": " Alpha.6 means make it a bit transparent.", "tokens": [20588, 13, 21, 1355, 652, 309, 257, 857, 12737, 13], "temperature": 0.0, "avg_logprob": -0.18616550940054435, "compression_ratio": 1.6816143497757847, "no_speech_prob": 5.862752004759386e-06}, {"id": 1358, "seek": 607688, "start": 6076.88, "end": 6084.4800000000005, "text": " And Matplotlib extent means expand it from 11 by 11 to 352 by 352.", "tokens": [400, 6789, 564, 310, 38270, 8396, 1355, 5268, 309, 490, 2975, 538, 2975, 281, 6976, 17, 538, 6976, 17, 13], "temperature": 0.0, "avg_logprob": -0.1575780041449893, "compression_ratio": 1.59375, "no_speech_prob": 6.854228558950126e-06}, {"id": 1359, "seek": 607688, "start": 6084.4800000000005, "end": 6087.2, "text": " Use bilinear interpolation so it's not all blocky.", "tokens": [8278, 8588, 533, 289, 44902, 399, 370, 309, 311, 406, 439, 3461, 88, 13], "temperature": 0.0, "avg_logprob": -0.1575780041449893, "compression_ratio": 1.59375, "no_speech_prob": 6.854228558950126e-06}, {"id": 1360, "seek": 607688, "start": 6087.2, "end": 6089.6, "text": " And use a different color map to kind of highlight things.", "tokens": [400, 764, 257, 819, 2017, 4471, 281, 733, 295, 5078, 721, 13], "temperature": 0.0, "avg_logprob": -0.1575780041449893, "compression_ratio": 1.59375, "no_speech_prob": 6.854228558950126e-06}, {"id": 1361, "seek": 607688, "start": 6089.6, "end": 6090.6, "text": " So that's just a Matplotlib.", "tokens": [407, 300, 311, 445, 257, 6789, 564, 310, 38270, 13], "temperature": 0.0, "avg_logprob": -0.1575780041449893, "compression_ratio": 1.59375, "no_speech_prob": 6.854228558950126e-06}, {"id": 1362, "seek": 607688, "start": 6090.6, "end": 6091.6, "text": " It's not important.", "tokens": [467, 311, 406, 1021, 13], "temperature": 0.0, "avg_logprob": -0.1575780041449893, "compression_ratio": 1.59375, "no_speech_prob": 6.854228558950126e-06}, {"id": 1363, "seek": 607688, "start": 6091.6, "end": 6097.6, "text": " The key thing here is that average activations is the 11 by 11 matrix we wanted.", "tokens": [440, 2141, 551, 510, 307, 300, 4274, 2430, 763, 307, 264, 2975, 538, 2975, 8141, 321, 1415, 13], "temperature": 0.0, "avg_logprob": -0.1575780041449893, "compression_ratio": 1.59375, "no_speech_prob": 6.854228558950126e-06}, {"id": 1364, "seek": 607688, "start": 6097.6, "end": 6098.6, "text": " Here it is.", "tokens": [1692, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.1575780041449893, "compression_ratio": 1.59375, "no_speech_prob": 6.854228558950126e-06}, {"id": 1365, "seek": 607688, "start": 6098.6, "end": 6102.72, "text": " Average activations.shape is 11 by 11.", "tokens": [316, 3623, 2430, 763, 13, 82, 42406, 307, 2975, 538, 2975, 13], "temperature": 0.0, "avg_logprob": -0.1575780041449893, "compression_ratio": 1.59375, "no_speech_prob": 6.854228558950126e-06}, {"id": 1366, "seek": 610272, "start": 6102.72, "end": 6109.0, "text": " So to get there we took the mean of activations across dimension 0, which is what I just said,", "tokens": [407, 281, 483, 456, 321, 1890, 264, 914, 295, 2430, 763, 2108, 10139, 1958, 11, 597, 307, 437, 286, 445, 848, 11], "temperature": 0.0, "avg_logprob": -0.13937736280036694, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.411175829474814e-06}, {"id": 1367, "seek": 610272, "start": 6109.0, "end": 6113.16, "text": " in PyTorch the channel dimension is the first dimension.", "tokens": [294, 9953, 51, 284, 339, 264, 2269, 10139, 307, 264, 700, 10139, 13], "temperature": 0.0, "avg_logprob": -0.13937736280036694, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.411175829474814e-06}, {"id": 1368, "seek": 610272, "start": 6113.16, "end": 6120.4400000000005, "text": " So the mean across dimension 0 took us from something of size 512 by 11 by 11, as promised,", "tokens": [407, 264, 914, 2108, 10139, 1958, 1890, 505, 490, 746, 295, 2744, 1025, 4762, 538, 2975, 538, 2975, 11, 382, 10768, 11], "temperature": 0.0, "avg_logprob": -0.13937736280036694, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.411175829474814e-06}, {"id": 1369, "seek": 610272, "start": 6120.4400000000005, "end": 6122.6, "text": " to something of 11 by 11.", "tokens": [281, 746, 295, 2975, 538, 2975, 13], "temperature": 0.0, "avg_logprob": -0.13937736280036694, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.411175829474814e-06}, {"id": 1370, "seek": 610272, "start": 6122.6, "end": 6127.64, "text": " So therefore activations, acts, contains the activations we're averaging.", "tokens": [407, 4412, 2430, 763, 11, 10672, 11, 8306, 264, 2430, 763, 321, 434, 47308, 13], "temperature": 0.0, "avg_logprob": -0.13937736280036694, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.411175829474814e-06}, {"id": 1371, "seek": 610272, "start": 6127.64, "end": 6130.280000000001, "text": " Where did they come from?", "tokens": [2305, 630, 436, 808, 490, 30], "temperature": 0.0, "avg_logprob": -0.13937736280036694, "compression_ratio": 1.7488151658767772, "no_speech_prob": 7.411175829474814e-06}, {"id": 1372, "seek": 613028, "start": 6130.28, "end": 6133.48, "text": " They came from something called a hook.", "tokens": [814, 1361, 490, 746, 1219, 257, 6328, 13], "temperature": 0.0, "avg_logprob": -0.09281375963393956, "compression_ratio": 1.505813953488372, "no_speech_prob": 9.51618221733952e-06}, {"id": 1373, "seek": 613028, "start": 6133.48, "end": 6144.44, "text": " So a hook is a really cool, more advanced PyTorch feature that lets you, as the name", "tokens": [407, 257, 6328, 307, 257, 534, 1627, 11, 544, 7339, 9953, 51, 284, 339, 4111, 300, 6653, 291, 11, 382, 264, 1315], "temperature": 0.0, "avg_logprob": -0.09281375963393956, "compression_ratio": 1.505813953488372, "no_speech_prob": 9.51618221733952e-06}, {"id": 1374, "seek": 613028, "start": 6144.44, "end": 6151.4, "text": " suggests, hook into the PyTorch machinery itself and run any arbitrary Python code you", "tokens": [13409, 11, 6328, 666, 264, 9953, 51, 284, 339, 27302, 2564, 293, 1190, 604, 23211, 15329, 3089, 291], "temperature": 0.0, "avg_logprob": -0.09281375963393956, "compression_ratio": 1.505813953488372, "no_speech_prob": 9.51618221733952e-06}, {"id": 1375, "seek": 613028, "start": 6151.4, "end": 6152.4, "text": " want to.", "tokens": [528, 281, 13], "temperature": 0.0, "avg_logprob": -0.09281375963393956, "compression_ratio": 1.505813953488372, "no_speech_prob": 9.51618221733952e-06}, {"id": 1376, "seek": 613028, "start": 6152.4, "end": 6156.139999999999, "text": " It's a really amazing and nifty thing.", "tokens": [467, 311, 257, 534, 2243, 293, 297, 37177, 551, 13], "temperature": 0.0, "avg_logprob": -0.09281375963393956, "compression_ratio": 1.505813953488372, "no_speech_prob": 9.51618221733952e-06}, {"id": 1377, "seek": 615614, "start": 6156.14, "end": 6164.08, "text": " Because you know normally when we do a forward pass through a PyTorch module, it gives us", "tokens": [1436, 291, 458, 5646, 562, 321, 360, 257, 2128, 1320, 807, 257, 9953, 51, 284, 339, 10088, 11, 309, 2709, 505], "temperature": 0.0, "avg_logprob": -0.10797037680943807, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.282778950757347e-05}, {"id": 1378, "seek": 615614, "start": 6164.08, "end": 6166.02, "text": " this set of outputs.", "tokens": [341, 992, 295, 23930, 13], "temperature": 0.0, "avg_logprob": -0.10797037680943807, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.282778950757347e-05}, {"id": 1379, "seek": 615614, "start": 6166.02, "end": 6170.76, "text": " But we know that in the process it's calculated these.", "tokens": [583, 321, 458, 300, 294, 264, 1399, 309, 311, 15598, 613, 13], "temperature": 0.0, "avg_logprob": -0.10797037680943807, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.282778950757347e-05}, {"id": 1380, "seek": 615614, "start": 6170.76, "end": 6177.84, "text": " So what I would like to do is I would like to hook into that forward pass and tell PyTorch,", "tokens": [407, 437, 286, 576, 411, 281, 360, 307, 286, 576, 411, 281, 6328, 666, 300, 2128, 1320, 293, 980, 9953, 51, 284, 339, 11], "temperature": 0.0, "avg_logprob": -0.10797037680943807, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.282778950757347e-05}, {"id": 1381, "seek": 615614, "start": 6177.84, "end": 6182.92, "text": " hey, when you calculate this, can you store it for me please?", "tokens": [4177, 11, 562, 291, 8873, 341, 11, 393, 291, 3531, 309, 337, 385, 1767, 30], "temperature": 0.0, "avg_logprob": -0.10797037680943807, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.282778950757347e-05}, {"id": 1382, "seek": 615614, "start": 6182.92, "end": 6184.280000000001, "text": " So what is this?", "tokens": [407, 437, 307, 341, 30], "temperature": 0.0, "avg_logprob": -0.10797037680943807, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.282778950757347e-05}, {"id": 1383, "seek": 618428, "start": 6184.28, "end": 6188.599999999999, "text": " This is the output of the convolutional part of the model.", "tokens": [639, 307, 264, 5598, 295, 264, 45216, 304, 644, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1410029833434058, "compression_ratio": 1.7019867549668874, "no_speech_prob": 4.1573339331080206e-06}, {"id": 1384, "seek": 618428, "start": 6188.599999999999, "end": 6193.04, "text": " So the convolutional part of the model, which is everything before the average pull, is", "tokens": [407, 264, 45216, 304, 644, 295, 264, 2316, 11, 597, 307, 1203, 949, 264, 4274, 2235, 11, 307], "temperature": 0.0, "avg_logprob": -0.1410029833434058, "compression_ratio": 1.7019867549668874, "no_speech_prob": 4.1573339331080206e-06}, {"id": 1385, "seek": 618428, "start": 6193.04, "end": 6197.92, "text": " basically all of that.", "tokens": [1936, 439, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.1410029833434058, "compression_ratio": 1.7019867549668874, "no_speech_prob": 4.1573339331080206e-06}, {"id": 1386, "seek": 618428, "start": 6197.92, "end": 6207.679999999999, "text": " And so thinking back to transfer learning, remember with transfer learning, we actually", "tokens": [400, 370, 1953, 646, 281, 5003, 2539, 11, 1604, 365, 5003, 2539, 11, 321, 767], "temperature": 0.0, "avg_logprob": -0.1410029833434058, "compression_ratio": 1.7019867549668874, "no_speech_prob": 4.1573339331080206e-06}, {"id": 1387, "seek": 620768, "start": 6207.68, "end": 6215.12, "text": " cut off everything after the convolutional part of the model and replaced it with our", "tokens": [1723, 766, 1203, 934, 264, 45216, 304, 644, 295, 264, 2316, 293, 10772, 309, 365, 527], "temperature": 0.0, "avg_logprob": -0.14374250951020615, "compression_ratio": 1.7461928934010151, "no_speech_prob": 2.026124775511562e-06}, {"id": 1388, "seek": 620768, "start": 6215.12, "end": 6217.16, "text": " own little bit.", "tokens": [1065, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.14374250951020615, "compression_ratio": 1.7461928934010151, "no_speech_prob": 2.026124775511562e-06}, {"id": 1389, "seek": 620768, "start": 6217.16, "end": 6223.360000000001, "text": " So with Fast.ai, the original convolutional part of the model is always going to be the", "tokens": [407, 365, 15968, 13, 1301, 11, 264, 3380, 45216, 304, 644, 295, 264, 2316, 307, 1009, 516, 281, 312, 264], "temperature": 0.0, "avg_logprob": -0.14374250951020615, "compression_ratio": 1.7461928934010151, "no_speech_prob": 2.026124775511562e-06}, {"id": 1390, "seek": 620768, "start": 6223.360000000001, "end": 6225.5, "text": " first thing in the model.", "tokens": [700, 551, 294, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14374250951020615, "compression_ratio": 1.7461928934010151, "no_speech_prob": 2.026124775511562e-06}, {"id": 1391, "seek": 620768, "start": 6225.5, "end": 6232.16, "text": " And specifically, it's always going to be called, assuming, so in this case I'm taking", "tokens": [400, 4682, 11, 309, 311, 1009, 516, 281, 312, 1219, 11, 11926, 11, 370, 294, 341, 1389, 286, 478, 1940], "temperature": 0.0, "avg_logprob": -0.14374250951020615, "compression_ratio": 1.7461928934010151, "no_speech_prob": 2.026124775511562e-06}, {"id": 1392, "seek": 620768, "start": 6232.16, "end": 6236.400000000001, "text": " my model and I'm just going to call it M.", "tokens": [452, 2316, 293, 286, 478, 445, 516, 281, 818, 309, 376, 13], "temperature": 0.0, "avg_logprob": -0.14374250951020615, "compression_ratio": 1.7461928934010151, "no_speech_prob": 2.026124775511562e-06}, {"id": 1393, "seek": 623640, "start": 6236.4, "end": 6250.719999999999, "text": " So you can see M is this big thing, but always, at least in Fast.ai, always M0 will be the", "tokens": [407, 291, 393, 536, 376, 307, 341, 955, 551, 11, 457, 1009, 11, 412, 1935, 294, 15968, 13, 1301, 11, 1009, 376, 15, 486, 312, 264], "temperature": 0.0, "avg_logprob": -0.11537296541275517, "compression_ratio": 1.564516129032258, "no_speech_prob": 4.289306161808781e-06}, {"id": 1394, "seek": 623640, "start": 6250.719999999999, "end": 6252.28, "text": " convolutional part of the model.", "tokens": [45216, 304, 644, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11537296541275517, "compression_ratio": 1.564516129032258, "no_speech_prob": 4.289306161808781e-06}, {"id": 1395, "seek": 623640, "start": 6252.28, "end": 6259.4, "text": " So in this case we created a, let's go back and see, we created a ResNet34.", "tokens": [407, 294, 341, 1389, 321, 2942, 257, 11, 718, 311, 352, 646, 293, 536, 11, 321, 2942, 257, 5015, 31890, 12249, 13], "temperature": 0.0, "avg_logprob": -0.11537296541275517, "compression_ratio": 1.564516129032258, "no_speech_prob": 4.289306161808781e-06}, {"id": 1396, "seek": 623640, "start": 6259.4, "end": 6265.719999999999, "text": " So the main part of the ResNet34, the pre-trained bit we hold onto is in M0, and so this is", "tokens": [407, 264, 2135, 644, 295, 264, 5015, 31890, 12249, 11, 264, 659, 12, 17227, 2001, 857, 321, 1797, 3911, 307, 294, 376, 15, 11, 293, 370, 341, 307], "temperature": 0.0, "avg_logprob": -0.11537296541275517, "compression_ratio": 1.564516129032258, "no_speech_prob": 4.289306161808781e-06}, {"id": 1397, "seek": 626572, "start": 6265.72, "end": 6266.72, "text": " basically it.", "tokens": [1936, 309, 13], "temperature": 0.0, "avg_logprob": -0.10705155441441487, "compression_ratio": 1.6354679802955665, "no_speech_prob": 9.368637620355003e-06}, {"id": 1398, "seek": 626572, "start": 6266.72, "end": 6273.76, "text": " This is the printout of the ResNet34, and at the end of it there is the 512 activations.", "tokens": [639, 307, 264, 4482, 346, 295, 264, 5015, 31890, 12249, 11, 293, 412, 264, 917, 295, 309, 456, 307, 264, 1025, 4762, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.10705155441441487, "compression_ratio": 1.6354679802955665, "no_speech_prob": 9.368637620355003e-06}, {"id": 1399, "seek": 626572, "start": 6273.76, "end": 6284.7, "text": " So in other words, what we want to do is we want to grab M0 and we want to hook its output.", "tokens": [407, 294, 661, 2283, 11, 437, 321, 528, 281, 360, 307, 321, 528, 281, 4444, 376, 15, 293, 321, 528, 281, 6328, 1080, 5598, 13], "temperature": 0.0, "avg_logprob": -0.10705155441441487, "compression_ratio": 1.6354679802955665, "no_speech_prob": 9.368637620355003e-06}, {"id": 1400, "seek": 626572, "start": 6284.7, "end": 6286.400000000001, "text": " So this is a really useful thing to be able to do.", "tokens": [407, 341, 307, 257, 534, 4420, 551, 281, 312, 1075, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.10705155441441487, "compression_ratio": 1.6354679802955665, "no_speech_prob": 9.368637620355003e-06}, {"id": 1401, "seek": 626572, "start": 6286.400000000001, "end": 6290.56, "text": " So Fast.ai has actually created something to do it for you, which is literally you say", "tokens": [407, 15968, 13, 1301, 575, 767, 2942, 746, 281, 360, 309, 337, 291, 11, 597, 307, 3736, 291, 584], "temperature": 0.0, "avg_logprob": -0.10705155441441487, "compression_ratio": 1.6354679802955665, "no_speech_prob": 9.368637620355003e-06}, {"id": 1402, "seek": 629056, "start": 6290.56, "end": 6296.72, "text": " hook output and you pass in the PyTorch module that you want to hook the output of.", "tokens": [6328, 5598, 293, 291, 1320, 294, 264, 9953, 51, 284, 339, 10088, 300, 291, 528, 281, 6328, 264, 5598, 295, 13], "temperature": 0.0, "avg_logprob": -0.12125245515290681, "compression_ratio": 1.668103448275862, "no_speech_prob": 7.646454832865857e-06}, {"id": 1403, "seek": 629056, "start": 6296.72, "end": 6302.9400000000005, "text": " And so most likely the thing you'll want to hook is the convolutional part of the model,", "tokens": [400, 370, 881, 3700, 264, 551, 291, 603, 528, 281, 6328, 307, 264, 45216, 304, 644, 295, 264, 2316, 11], "temperature": 0.0, "avg_logprob": -0.12125245515290681, "compression_ratio": 1.668103448275862, "no_speech_prob": 7.646454832865857e-06}, {"id": 1404, "seek": 629056, "start": 6302.9400000000005, "end": 6308.02, "text": " and that's always going to be M0, or learn.model0.", "tokens": [293, 300, 311, 1009, 516, 281, 312, 376, 15, 11, 420, 1466, 13, 8014, 338, 15, 13], "temperature": 0.0, "avg_logprob": -0.12125245515290681, "compression_ratio": 1.668103448275862, "no_speech_prob": 7.646454832865857e-06}, {"id": 1405, "seek": 629056, "start": 6308.02, "end": 6310.4800000000005, "text": " So we give that hook a name.", "tokens": [407, 321, 976, 300, 6328, 257, 1315, 13], "temperature": 0.0, "avg_logprob": -0.12125245515290681, "compression_ratio": 1.668103448275862, "no_speech_prob": 7.646454832865857e-06}, {"id": 1406, "seek": 629056, "start": 6310.4800000000005, "end": 6313.52, "text": " Don't worry about this part, we'll learn about it next week.", "tokens": [1468, 380, 3292, 466, 341, 644, 11, 321, 603, 1466, 466, 309, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.12125245515290681, "compression_ratio": 1.668103448275862, "no_speech_prob": 7.646454832865857e-06}, {"id": 1407, "seek": 629056, "start": 6313.52, "end": 6318.64, "text": " So having hooked the output, we now need to actually do the forward pass.", "tokens": [407, 1419, 20410, 264, 5598, 11, 321, 586, 643, 281, 767, 360, 264, 2128, 1320, 13], "temperature": 0.0, "avg_logprob": -0.12125245515290681, "compression_ratio": 1.668103448275862, "no_speech_prob": 7.646454832865857e-06}, {"id": 1408, "seek": 631864, "start": 6318.64, "end": 6322.700000000001, "text": " And so remember in PyTorch to actually get it to calculate something, which is called", "tokens": [400, 370, 1604, 294, 9953, 51, 284, 339, 281, 767, 483, 309, 281, 8873, 746, 11, 597, 307, 1219], "temperature": 0.0, "avg_logprob": -0.1244909928576781, "compression_ratio": 1.5478260869565217, "no_speech_prob": 8.9396226030658e-06}, {"id": 1409, "seek": 631864, "start": 6322.700000000001, "end": 6327.6, "text": " doing the forward pass, you just act as if the model is a function.", "tokens": [884, 264, 2128, 1320, 11, 291, 445, 605, 382, 498, 264, 2316, 307, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1244909928576781, "compression_ratio": 1.5478260869565217, "no_speech_prob": 8.9396226030658e-06}, {"id": 1410, "seek": 631864, "start": 6327.6, "end": 6331.820000000001, "text": " So we just pass in our x mini-batch.", "tokens": [407, 321, 445, 1320, 294, 527, 2031, 8382, 12, 65, 852, 13], "temperature": 0.0, "avg_logprob": -0.1244909928576781, "compression_ratio": 1.5478260869565217, "no_speech_prob": 8.9396226030658e-06}, {"id": 1411, "seek": 631864, "start": 6331.820000000001, "end": 6342.12, "text": " So we already had a main-coon image called x, but we can't quite pass that into our model.", "tokens": [407, 321, 1217, 632, 257, 2135, 12, 48092, 3256, 1219, 2031, 11, 457, 321, 393, 380, 1596, 1320, 300, 666, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1244909928576781, "compression_ratio": 1.5478260869565217, "no_speech_prob": 8.9396226030658e-06}, {"id": 1412, "seek": 631864, "start": 6342.12, "end": 6348.46, "text": " It has to be normalized and turned into a mini-batch and put onto the GPU.", "tokens": [467, 575, 281, 312, 48704, 293, 3574, 666, 257, 8382, 12, 65, 852, 293, 829, 3911, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.1244909928576781, "compression_ratio": 1.5478260869565217, "no_speech_prob": 8.9396226030658e-06}, {"id": 1413, "seek": 634846, "start": 6348.46, "end": 6353.0, "text": " So Fast.ai has a thing called a data bunch, which we have in data, and you can always", "tokens": [407, 15968, 13, 1301, 575, 257, 551, 1219, 257, 1412, 3840, 11, 597, 321, 362, 294, 1412, 11, 293, 291, 393, 1009], "temperature": 0.0, "avg_logprob": -0.1311451319990487, "compression_ratio": 1.8433179723502304, "no_speech_prob": 9.080414201889653e-06}, {"id": 1414, "seek": 634846, "start": 6353.0, "end": 6358.9, "text": " say data.one item to create a mini-batch with one thing in it.", "tokens": [584, 1412, 13, 546, 3174, 281, 1884, 257, 8382, 12, 65, 852, 365, 472, 551, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.1311451319990487, "compression_ratio": 1.8433179723502304, "no_speech_prob": 9.080414201889653e-06}, {"id": 1415, "seek": 634846, "start": 6358.9, "end": 6364.56, "text": " And as an exercise at home, you could try to create a mini-batch without using data.one", "tokens": [400, 382, 364, 5380, 412, 1280, 11, 291, 727, 853, 281, 1884, 257, 8382, 12, 65, 852, 1553, 1228, 1412, 13, 546], "temperature": 0.0, "avg_logprob": -0.1311451319990487, "compression_ratio": 1.8433179723502304, "no_speech_prob": 9.080414201889653e-06}, {"id": 1416, "seek": 634846, "start": 6364.56, "end": 6369.26, "text": " item to make sure that you kind of learn how to normalize and stuff yourself if you want", "tokens": [3174, 281, 652, 988, 300, 291, 733, 295, 1466, 577, 281, 2710, 1125, 293, 1507, 1803, 498, 291, 528], "temperature": 0.0, "avg_logprob": -0.1311451319990487, "compression_ratio": 1.8433179723502304, "no_speech_prob": 9.080414201889653e-06}, {"id": 1417, "seek": 634846, "start": 6369.26, "end": 6370.26, "text": " to.", "tokens": [281, 13], "temperature": 0.0, "avg_logprob": -0.1311451319990487, "compression_ratio": 1.8433179723502304, "no_speech_prob": 9.080414201889653e-06}, {"id": 1418, "seek": 634846, "start": 6370.26, "end": 6375.24, "text": " But this is how you can create a mini-batch with just one thing in it.", "tokens": [583, 341, 307, 577, 291, 393, 1884, 257, 8382, 12, 65, 852, 365, 445, 472, 551, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.1311451319990487, "compression_ratio": 1.8433179723502304, "no_speech_prob": 9.080414201889653e-06}, {"id": 1419, "seek": 637524, "start": 6375.24, "end": 6380.04, "text": " And then I can pop that onto the GPU by saying.cuda.", "tokens": [400, 550, 286, 393, 1665, 300, 3911, 264, 18407, 538, 1566, 2411, 66, 11152, 13], "temperature": 0.0, "avg_logprob": -0.12381816696334672, "compression_ratio": 1.64, "no_speech_prob": 9.368663086206652e-06}, {"id": 1420, "seek": 637524, "start": 6380.04, "end": 6381.48, "text": " That's what I pass to my model.", "tokens": [663, 311, 437, 286, 1320, 281, 452, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12381816696334672, "compression_ratio": 1.64, "no_speech_prob": 9.368663086206652e-06}, {"id": 1421, "seek": 637524, "start": 6381.48, "end": 6391.04, "text": " And so the predictions I get out actually don't care about, because the predictions", "tokens": [400, 370, 264, 21264, 286, 483, 484, 767, 500, 380, 1127, 466, 11, 570, 264, 21264], "temperature": 0.0, "avg_logprob": -0.12381816696334672, "compression_ratio": 1.64, "no_speech_prob": 9.368663086206652e-06}, {"id": 1422, "seek": 637524, "start": 6391.04, "end": 6394.139999999999, "text": " is this thing, which is not what I want.", "tokens": [307, 341, 551, 11, 597, 307, 406, 437, 286, 528, 13], "temperature": 0.0, "avg_logprob": -0.12381816696334672, "compression_ratio": 1.64, "no_speech_prob": 9.368663086206652e-06}, {"id": 1423, "seek": 637524, "start": 6394.139999999999, "end": 6396.639999999999, "text": " So I'm not actually going to do anything with the predictions.", "tokens": [407, 286, 478, 406, 767, 516, 281, 360, 1340, 365, 264, 21264, 13], "temperature": 0.0, "avg_logprob": -0.12381816696334672, "compression_ratio": 1.64, "no_speech_prob": 9.368663086206652e-06}, {"id": 1424, "seek": 637524, "start": 6396.639999999999, "end": 6401.12, "text": " The thing I care about is the hook that I just created.", "tokens": [440, 551, 286, 1127, 466, 307, 264, 6328, 300, 286, 445, 2942, 13], "temperature": 0.0, "avg_logprob": -0.12381816696334672, "compression_ratio": 1.64, "no_speech_prob": 9.368663086206652e-06}, {"id": 1425, "seek": 640112, "start": 6401.12, "end": 6406.94, "text": " Now one thing to be aware of is that when you hook something in PyTorch, that means", "tokens": [823, 472, 551, 281, 312, 3650, 295, 307, 300, 562, 291, 6328, 746, 294, 9953, 51, 284, 339, 11, 300, 1355], "temperature": 0.0, "avg_logprob": -0.1193686092601103, "compression_ratio": 1.7203065134099618, "no_speech_prob": 8.013440492504742e-06}, {"id": 1426, "seek": 640112, "start": 6406.94, "end": 6412.64, "text": " every single time you run that model, assuming you're hooking outputs, it's storing those", "tokens": [633, 2167, 565, 291, 1190, 300, 2316, 11, 11926, 291, 434, 1106, 5953, 23930, 11, 309, 311, 26085, 729], "temperature": 0.0, "avg_logprob": -0.1193686092601103, "compression_ratio": 1.7203065134099618, "no_speech_prob": 8.013440492504742e-06}, {"id": 1427, "seek": 640112, "start": 6412.64, "end": 6413.64, "text": " outputs.", "tokens": [23930, 13], "temperature": 0.0, "avg_logprob": -0.1193686092601103, "compression_ratio": 1.7203065134099618, "no_speech_prob": 8.013440492504742e-06}, {"id": 1428, "seek": 640112, "start": 6413.64, "end": 6417.76, "text": " And so you want to remove the hook when you've got what you want, because otherwise if you", "tokens": [400, 370, 291, 528, 281, 4159, 264, 6328, 562, 291, 600, 658, 437, 291, 528, 11, 570, 5911, 498, 291], "temperature": 0.0, "avg_logprob": -0.1193686092601103, "compression_ratio": 1.7203065134099618, "no_speech_prob": 8.013440492504742e-06}, {"id": 1429, "seek": 640112, "start": 6417.76, "end": 6420.96, "text": " use the model again, it's going to keep hooking more and more outputs, which will be slow", "tokens": [764, 264, 2316, 797, 11, 309, 311, 516, 281, 1066, 1106, 5953, 544, 293, 544, 23930, 11, 597, 486, 312, 2964], "temperature": 0.0, "avg_logprob": -0.1193686092601103, "compression_ratio": 1.7203065134099618, "no_speech_prob": 8.013440492504742e-06}, {"id": 1430, "seek": 640112, "start": 6420.96, "end": 6422.54, "text": " and memory intensive.", "tokens": [293, 4675, 18957, 13], "temperature": 0.0, "avg_logprob": -0.1193686092601103, "compression_ratio": 1.7203065134099618, "no_speech_prob": 8.013440492504742e-06}, {"id": 1431, "seek": 640112, "start": 6422.54, "end": 6425.76, "text": " So we've created this thing.", "tokens": [407, 321, 600, 2942, 341, 551, 13], "temperature": 0.0, "avg_logprob": -0.1193686092601103, "compression_ratio": 1.7203065134099618, "no_speech_prob": 8.013440492504742e-06}, {"id": 1432, "seek": 640112, "start": 6425.76, "end": 6427.62, "text": " Python calls it a context manager.", "tokens": [15329, 5498, 309, 257, 4319, 6598, 13], "temperature": 0.0, "avg_logprob": -0.1193686092601103, "compression_ratio": 1.7203065134099618, "no_speech_prob": 8.013440492504742e-06}, {"id": 1433, "seek": 642762, "start": 6427.62, "end": 6432.4, "text": " You can use any hook as a context manager at the end of that with block.", "tokens": [509, 393, 764, 604, 6328, 382, 257, 4319, 6598, 412, 264, 917, 295, 300, 365, 3461, 13], "temperature": 0.0, "avg_logprob": -0.12403606485437464, "compression_ratio": 1.7293577981651376, "no_speech_prob": 3.668845465654158e-06}, {"id": 1434, "seek": 642762, "start": 6432.4, "end": 6435.8, "text": " It'll remove the hook.", "tokens": [467, 603, 4159, 264, 6328, 13], "temperature": 0.0, "avg_logprob": -0.12403606485437464, "compression_ratio": 1.7293577981651376, "no_speech_prob": 3.668845465654158e-06}, {"id": 1435, "seek": 642762, "start": 6435.8, "end": 6438.04, "text": " So we've got our hook.", "tokens": [407, 321, 600, 658, 527, 6328, 13], "temperature": 0.0, "avg_logprob": -0.12403606485437464, "compression_ratio": 1.7293577981651376, "no_speech_prob": 3.668845465654158e-06}, {"id": 1436, "seek": 642762, "start": 6438.04, "end": 6444.8, "text": " And so now PyTorch hooks, sorry, FastAI hooks, always give you something called, or at least", "tokens": [400, 370, 586, 9953, 51, 284, 339, 26485, 11, 2597, 11, 15968, 48698, 26485, 11, 1009, 976, 291, 746, 1219, 11, 420, 412, 1935], "temperature": 0.0, "avg_logprob": -0.12403606485437464, "compression_ratio": 1.7293577981651376, "no_speech_prob": 3.668845465654158e-06}, {"id": 1437, "seek": 642762, "start": 6444.8, "end": 6448.0599999999995, "text": " the output hooks, always give you something called.stored, which is where it stores away", "tokens": [264, 5598, 26485, 11, 1009, 976, 291, 746, 1219, 2411, 372, 2769, 11, 597, 307, 689, 309, 9512, 1314], "temperature": 0.0, "avg_logprob": -0.12403606485437464, "compression_ratio": 1.7293577981651376, "no_speech_prob": 3.668845465654158e-06}, {"id": 1438, "seek": 642762, "start": 6448.0599999999995, "end": 6450.2, "text": " the thing you asked it to hook.", "tokens": [264, 551, 291, 2351, 309, 281, 6328, 13], "temperature": 0.0, "avg_logprob": -0.12403606485437464, "compression_ratio": 1.7293577981651376, "no_speech_prob": 3.668845465654158e-06}, {"id": 1439, "seek": 642762, "start": 6450.2, "end": 6454.2, "text": " And so that's where the activations now are.", "tokens": [400, 370, 300, 311, 689, 264, 2430, 763, 586, 366, 13], "temperature": 0.0, "avg_logprob": -0.12403606485437464, "compression_ratio": 1.7293577981651376, "no_speech_prob": 3.668845465654158e-06}, {"id": 1440, "seek": 645420, "start": 6454.2, "end": 6458.72, "text": " So we did a forward pass after hooking the output of the convolutional section of the", "tokens": [407, 321, 630, 257, 2128, 1320, 934, 1106, 5953, 264, 5598, 295, 264, 45216, 304, 3541, 295, 264], "temperature": 0.0, "avg_logprob": -0.10250462426079644, "compression_ratio": 1.556701030927835, "no_speech_prob": 1.9637921013782034e-06}, {"id": 1441, "seek": 645420, "start": 6458.72, "end": 6459.84, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.10250462426079644, "compression_ratio": 1.556701030927835, "no_speech_prob": 1.9637921013782034e-06}, {"id": 1442, "seek": 645420, "start": 6459.84, "end": 6462.5599999999995, "text": " We grabbed what it stored.", "tokens": [492, 18607, 437, 309, 12187, 13], "temperature": 0.0, "avg_logprob": -0.10250462426079644, "compression_ratio": 1.556701030927835, "no_speech_prob": 1.9637921013782034e-06}, {"id": 1443, "seek": 645420, "start": 6462.5599999999995, "end": 6463.5599999999995, "text": " We checked the shape.", "tokens": [492, 10033, 264, 3909, 13], "temperature": 0.0, "avg_logprob": -0.10250462426079644, "compression_ratio": 1.556701030927835, "no_speech_prob": 1.9637921013782034e-06}, {"id": 1444, "seek": 645420, "start": 6463.5599999999995, "end": 6468.22, "text": " It was 512 by 11 by 11, as we predicted.", "tokens": [467, 390, 1025, 4762, 538, 2975, 538, 2975, 11, 382, 321, 19147, 13], "temperature": 0.0, "avg_logprob": -0.10250462426079644, "compression_ratio": 1.556701030927835, "no_speech_prob": 1.9637921013782034e-06}, {"id": 1445, "seek": 645420, "start": 6468.22, "end": 6477.0199999999995, "text": " We then took the mean of the channel access to get an 11 by 11 tensor.", "tokens": [492, 550, 1890, 264, 914, 295, 264, 2269, 2105, 281, 483, 364, 2975, 538, 2975, 40863, 13], "temperature": 0.0, "avg_logprob": -0.10250462426079644, "compression_ratio": 1.556701030927835, "no_speech_prob": 1.9637921013782034e-06}, {"id": 1446, "seek": 645420, "start": 6477.0199999999995, "end": 6482.0199999999995, "text": " And then if we look at that, that's our picture.", "tokens": [400, 550, 498, 321, 574, 412, 300, 11, 300, 311, 527, 3036, 13], "temperature": 0.0, "avg_logprob": -0.10250462426079644, "compression_ratio": 1.556701030927835, "no_speech_prob": 1.9637921013782034e-06}, {"id": 1447, "seek": 648202, "start": 6482.02, "end": 6486.42, "text": " So there's a lot to unpack.", "tokens": [407, 456, 311, 257, 688, 281, 26699, 13], "temperature": 0.0, "avg_logprob": -0.11782567254428206, "compression_ratio": 1.7226277372262773, "no_speech_prob": 5.093644631415373e-06}, {"id": 1448, "seek": 648202, "start": 6486.42, "end": 6491.8, "text": " But if you take your time going through these two sections, the convolutional kernel section", "tokens": [583, 498, 291, 747, 428, 565, 516, 807, 613, 732, 10863, 11, 264, 45216, 304, 28256, 3541], "temperature": 0.0, "avg_logprob": -0.11782567254428206, "compression_ratio": 1.7226277372262773, "no_speech_prob": 5.093644631415373e-06}, {"id": 1449, "seek": 648202, "start": 6491.8, "end": 6495.88, "text": " and the heat map section of this notebook, like running those lines of code and changing", "tokens": [293, 264, 3738, 4471, 3541, 295, 341, 21060, 11, 411, 2614, 729, 3876, 295, 3089, 293, 4473], "temperature": 0.0, "avg_logprob": -0.11782567254428206, "compression_ratio": 1.7226277372262773, "no_speech_prob": 5.093644631415373e-06}, {"id": 1450, "seek": 648202, "start": 6495.88, "end": 6501.9400000000005, "text": " them around a little bit, and remember the most important thing to look at is shape.", "tokens": [552, 926, 257, 707, 857, 11, 293, 1604, 264, 881, 1021, 551, 281, 574, 412, 307, 3909, 13], "temperature": 0.0, "avg_logprob": -0.11782567254428206, "compression_ratio": 1.7226277372262773, "no_speech_prob": 5.093644631415373e-06}, {"id": 1451, "seek": 648202, "start": 6501.9400000000005, "end": 6505.76, "text": " You might have noticed when I'm showing you these notebooks, I very often print out the", "tokens": [509, 1062, 362, 5694, 562, 286, 478, 4099, 291, 613, 43782, 11, 286, 588, 2049, 4482, 484, 264], "temperature": 0.0, "avg_logprob": -0.11782567254428206, "compression_ratio": 1.7226277372262773, "no_speech_prob": 5.093644631415373e-06}, {"id": 1452, "seek": 648202, "start": 6505.76, "end": 6506.76, "text": " shape.", "tokens": [3909, 13], "temperature": 0.0, "avg_logprob": -0.11782567254428206, "compression_ratio": 1.7226277372262773, "no_speech_prob": 5.093644631415373e-06}, {"id": 1453, "seek": 648202, "start": 6506.76, "end": 6510.860000000001, "text": " And when you look at the shape, you want to be looking at how many axes are there.", "tokens": [400, 562, 291, 574, 412, 264, 3909, 11, 291, 528, 281, 312, 1237, 412, 577, 867, 35387, 366, 456, 13], "temperature": 0.0, "avg_logprob": -0.11782567254428206, "compression_ratio": 1.7226277372262773, "no_speech_prob": 5.093644631415373e-06}, {"id": 1454, "seek": 651086, "start": 6510.86, "end": 6512.58, "text": " What's the rank of the tensor?", "tokens": [708, 311, 264, 6181, 295, 264, 40863, 30], "temperature": 0.0, "avg_logprob": -0.14749283037687602, "compression_ratio": 1.819672131147541, "no_speech_prob": 2.4824698812153656e-06}, {"id": 1455, "seek": 651086, "start": 6512.58, "end": 6515.0599999999995, "text": " And how many things are there in each axis?", "tokens": [400, 577, 867, 721, 366, 456, 294, 1184, 10298, 30], "temperature": 0.0, "avg_logprob": -0.14749283037687602, "compression_ratio": 1.819672131147541, "no_speech_prob": 2.4824698812153656e-06}, {"id": 1456, "seek": 651086, "start": 6515.0599999999995, "end": 6517.679999999999, "text": " And try and think why.", "tokens": [400, 853, 293, 519, 983, 13], "temperature": 0.0, "avg_logprob": -0.14749283037687602, "compression_ratio": 1.819672131147541, "no_speech_prob": 2.4824698812153656e-06}, {"id": 1457, "seek": 651086, "start": 6517.679999999999, "end": 6521.42, "text": " Try going back to the printout of the summary.", "tokens": [6526, 516, 646, 281, 264, 4482, 346, 295, 264, 12691, 13], "temperature": 0.0, "avg_logprob": -0.14749283037687602, "compression_ratio": 1.819672131147541, "no_speech_prob": 2.4824698812153656e-06}, {"id": 1458, "seek": 651086, "start": 6521.42, "end": 6524.2, "text": " Try going back to the actual list of the layers.", "tokens": [6526, 516, 646, 281, 264, 3539, 1329, 295, 264, 7914, 13], "temperature": 0.0, "avg_logprob": -0.14749283037687602, "compression_ratio": 1.819672131147541, "no_speech_prob": 2.4824698812153656e-06}, {"id": 1459, "seek": 651086, "start": 6524.2, "end": 6527.04, "text": " Try and go back and think about the actual picture we drew.", "tokens": [6526, 293, 352, 646, 293, 519, 466, 264, 3539, 3036, 321, 12804, 13], "temperature": 0.0, "avg_logprob": -0.14749283037687602, "compression_ratio": 1.819672131147541, "no_speech_prob": 2.4824698812153656e-06}, {"id": 1460, "seek": 651086, "start": 6527.04, "end": 6533.82, "text": " And think about what's actually going on.", "tokens": [400, 519, 466, 437, 311, 767, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.14749283037687602, "compression_ratio": 1.819672131147541, "no_speech_prob": 2.4824698812153656e-06}, {"id": 1461, "seek": 651086, "start": 6533.82, "end": 6536.86, "text": " So that's a lot of technical content.", "tokens": [407, 300, 311, 257, 688, 295, 6191, 2701, 13], "temperature": 0.0, "avg_logprob": -0.14749283037687602, "compression_ratio": 1.819672131147541, "no_speech_prob": 2.4824698812153656e-06}, {"id": 1462, "seek": 653686, "start": 6536.86, "end": 6541.28, "text": " So what I'm going to do now is switch from technical content to something much more important,", "tokens": [407, 437, 286, 478, 516, 281, 360, 586, 307, 3679, 490, 6191, 2701, 281, 746, 709, 544, 1021, 11], "temperature": 0.0, "avg_logprob": -0.15531244434294153, "compression_ratio": 1.5, "no_speech_prob": 3.905460289388429e-06}, {"id": 1463, "seek": 653686, "start": 6541.28, "end": 6546.799999999999, "text": " unless we have some questions first.", "tokens": [5969, 321, 362, 512, 1651, 700, 13], "temperature": 0.0, "avg_logprob": -0.15531244434294153, "compression_ratio": 1.5, "no_speech_prob": 3.905460289388429e-06}, {"id": 1464, "seek": 653686, "start": 6546.799999999999, "end": 6558.82, "text": " Because in the next lesson, we're going to be looking at generative models, both text", "tokens": [1436, 294, 264, 958, 6898, 11, 321, 434, 516, 281, 312, 1237, 412, 1337, 1166, 5245, 11, 1293, 2487], "temperature": 0.0, "avg_logprob": -0.15531244434294153, "compression_ratio": 1.5, "no_speech_prob": 3.905460289388429e-06}, {"id": 1465, "seek": 653686, "start": 6558.82, "end": 6561.2, "text": " and image generative models.", "tokens": [293, 3256, 1337, 1166, 5245, 13], "temperature": 0.0, "avg_logprob": -0.15531244434294153, "compression_ratio": 1.5, "no_speech_prob": 3.905460289388429e-06}, {"id": 1466, "seek": 656120, "start": 6561.2, "end": 6572.179999999999, "text": " And generative models are where you can create a new piece of text or a new image or a new", "tokens": [400, 1337, 1166, 5245, 366, 689, 291, 393, 1884, 257, 777, 2522, 295, 2487, 420, 257, 777, 3256, 420, 257, 777], "temperature": 0.0, "avg_logprob": -0.12427285409742786, "compression_ratio": 1.5302013422818792, "no_speech_prob": 8.529981641913764e-06}, {"id": 1467, "seek": 656120, "start": 6572.179999999999, "end": 6574.58, "text": " video or a new sound.", "tokens": [960, 420, 257, 777, 1626, 13], "temperature": 0.0, "avg_logprob": -0.12427285409742786, "compression_ratio": 1.5302013422818792, "no_speech_prob": 8.529981641913764e-06}, {"id": 1468, "seek": 656120, "start": 6574.58, "end": 6579.5, "text": " And as you probably are aware, this is the area that deep learning has developed the", "tokens": [400, 382, 291, 1391, 366, 3650, 11, 341, 307, 264, 1859, 300, 2452, 2539, 575, 4743, 264], "temperature": 0.0, "avg_logprob": -0.12427285409742786, "compression_ratio": 1.5302013422818792, "no_speech_prob": 8.529981641913764e-06}, {"id": 1469, "seek": 656120, "start": 6579.5, "end": 6582.139999999999, "text": " most in in the last 12 months.", "tokens": [881, 294, 294, 264, 1036, 2272, 2493, 13], "temperature": 0.0, "avg_logprob": -0.12427285409742786, "compression_ratio": 1.5302013422818792, "no_speech_prob": 8.529981641913764e-06}, {"id": 1470, "seek": 658214, "start": 6582.14, "end": 6594.14, "text": " So we're now at a point where we can generate realistic looking videos, images, audio, and", "tokens": [407, 321, 434, 586, 412, 257, 935, 689, 321, 393, 8460, 12465, 1237, 2145, 11, 5267, 11, 6278, 11, 293], "temperature": 0.0, "avg_logprob": -0.1023797842172476, "compression_ratio": 1.4944444444444445, "no_speech_prob": 2.090391262754565e-06}, {"id": 1471, "seek": 658214, "start": 6594.14, "end": 6597.54, "text": " to some extent even text.", "tokens": [281, 512, 8396, 754, 2487, 13], "temperature": 0.0, "avg_logprob": -0.1023797842172476, "compression_ratio": 1.4944444444444445, "no_speech_prob": 2.090391262754565e-06}, {"id": 1472, "seek": 658214, "start": 6597.54, "end": 6606.04, "text": " And so there are many things in this journey which have ethical considerations, but perhaps", "tokens": [400, 370, 456, 366, 867, 721, 294, 341, 4671, 597, 362, 18890, 24070, 11, 457, 4317], "temperature": 0.0, "avg_logprob": -0.1023797842172476, "compression_ratio": 1.4944444444444445, "no_speech_prob": 2.090391262754565e-06}, {"id": 1473, "seek": 658214, "start": 6606.04, "end": 6609.84, "text": " this area of generative modeling is one of the largest ones.", "tokens": [341, 1859, 295, 1337, 1166, 15983, 307, 472, 295, 264, 6443, 2306, 13], "temperature": 0.0, "avg_logprob": -0.1023797842172476, "compression_ratio": 1.4944444444444445, "no_speech_prob": 2.090391262754565e-06}, {"id": 1474, "seek": 660984, "start": 6609.84, "end": 6617.78, "text": " So before I got into it, I wanted to specifically touch on ethics and data science.", "tokens": [407, 949, 286, 658, 666, 309, 11, 286, 1415, 281, 4682, 2557, 322, 19769, 293, 1412, 3497, 13], "temperature": 0.0, "avg_logprob": -0.08880823359769933, "compression_ratio": 1.5217391304347827, "no_speech_prob": 9.8175542007084e-06}, {"id": 1475, "seek": 660984, "start": 6617.78, "end": 6622.66, "text": " Most of the stuff I'm showing you actually comes from Rachel.", "tokens": [4534, 295, 264, 1507, 286, 478, 4099, 291, 767, 1487, 490, 14246, 13], "temperature": 0.0, "avg_logprob": -0.08880823359769933, "compression_ratio": 1.5217391304347827, "no_speech_prob": 9.8175542007084e-06}, {"id": 1476, "seek": 660984, "start": 6622.66, "end": 6629.860000000001, "text": " And Rachel has a really cool TEDx San Francisco talk that you can check out on YouTube.", "tokens": [400, 14246, 575, 257, 534, 1627, 43036, 87, 5271, 12279, 751, 300, 291, 393, 1520, 484, 322, 3088, 13], "temperature": 0.0, "avg_logprob": -0.08880823359769933, "compression_ratio": 1.5217391304347827, "no_speech_prob": 9.8175542007084e-06}, {"id": 1477, "seek": 660984, "start": 6629.860000000001, "end": 6636.68, "text": " And a more extensive analysis of ethical principles and bias principles in AI, which you can find", "tokens": [400, 257, 544, 13246, 5215, 295, 18890, 9156, 293, 12577, 9156, 294, 7318, 11, 597, 291, 393, 915], "temperature": 0.0, "avg_logprob": -0.08880823359769933, "compression_ratio": 1.5217391304347827, "no_speech_prob": 9.8175542007084e-06}, {"id": 1478, "seek": 660984, "start": 6636.68, "end": 6638.66, "text": " at this talk here.", "tokens": [412, 341, 751, 510, 13], "temperature": 0.0, "avg_logprob": -0.08880823359769933, "compression_ratio": 1.5217391304347827, "no_speech_prob": 9.8175542007084e-06}, {"id": 1479, "seek": 663866, "start": 6638.66, "end": 6642.5199999999995, "text": " And she has a playlist that you can check out.", "tokens": [400, 750, 575, 257, 16788, 300, 291, 393, 1520, 484, 13], "temperature": 0.0, "avg_logprob": -0.1457744687795639, "compression_ratio": 1.3595505617977528, "no_speech_prob": 1.2028880519210361e-05}, {"id": 1480, "seek": 663866, "start": 6642.5199999999995, "end": 6649.22, "text": " We've already touched on an example of bias, which was this gender shades study, where", "tokens": [492, 600, 1217, 9828, 322, 364, 1365, 295, 12577, 11, 597, 390, 341, 7898, 20639, 2979, 11, 689], "temperature": 0.0, "avg_logprob": -0.1457744687795639, "compression_ratio": 1.3595505617977528, "no_speech_prob": 1.2028880519210361e-05}, {"id": 1481, "seek": 663866, "start": 6649.22, "end": 6657.66, "text": " if you remember, for example, lighter male skin people on IBM's main computer vision", "tokens": [498, 291, 1604, 11, 337, 1365, 11, 11546, 7133, 3178, 561, 322, 23487, 311, 2135, 3820, 5201], "temperature": 0.0, "avg_logprob": -0.1457744687795639, "compression_ratio": 1.3595505617977528, "no_speech_prob": 1.2028880519210361e-05}, {"id": 1482, "seek": 663866, "start": 6657.66, "end": 6662.0599999999995, "text": " system, 99.7% accurate.", "tokens": [1185, 11, 11803, 13, 22, 4, 8559, 13], "temperature": 0.0, "avg_logprob": -0.1457744687795639, "compression_ratio": 1.3595505617977528, "no_speech_prob": 1.2028880519210361e-05}, {"id": 1483, "seek": 666206, "start": 6662.06, "end": 6668.780000000001, "text": " And darker females are some hundreds of times less accurate in terms of error.", "tokens": [400, 12741, 21529, 366, 512, 6779, 295, 1413, 1570, 8559, 294, 2115, 295, 6713, 13], "temperature": 0.0, "avg_logprob": -0.14094450333539177, "compression_ratio": 1.6016597510373445, "no_speech_prob": 3.6688052205136046e-06}, {"id": 1484, "seek": 666206, "start": 6668.780000000001, "end": 6671.68, "text": " So like extraordinary differences.", "tokens": [407, 411, 10581, 7300, 13], "temperature": 0.0, "avg_logprob": -0.14094450333539177, "compression_ratio": 1.6016597510373445, "no_speech_prob": 3.6688052205136046e-06}, {"id": 1485, "seek": 666206, "start": 6671.68, "end": 6676.34, "text": " And so it's interesting to kind of like, okay, it's first of all important to be aware that", "tokens": [400, 370, 309, 311, 1880, 281, 733, 295, 411, 11, 1392, 11, 309, 311, 700, 295, 439, 1021, 281, 312, 3650, 300], "temperature": 0.0, "avg_logprob": -0.14094450333539177, "compression_ratio": 1.6016597510373445, "no_speech_prob": 3.6688052205136046e-06}, {"id": 1486, "seek": 666206, "start": 6676.34, "end": 6683.660000000001, "text": " not only can this happen technically, that this can happen on a massive company's rolled", "tokens": [406, 787, 393, 341, 1051, 12120, 11, 300, 341, 393, 1051, 322, 257, 5994, 2237, 311, 14306], "temperature": 0.0, "avg_logprob": -0.14094450333539177, "compression_ratio": 1.6016597510373445, "no_speech_prob": 3.6688052205136046e-06}, {"id": 1487, "seek": 666206, "start": 6683.660000000001, "end": 6689.72, "text": " out publicly available, highly marketed system that hundreds of quality control people have", "tokens": [484, 14843, 2435, 11, 5405, 49089, 1185, 300, 6779, 295, 3125, 1969, 561, 362], "temperature": 0.0, "avg_logprob": -0.14094450333539177, "compression_ratio": 1.6016597510373445, "no_speech_prob": 3.6688052205136046e-06}, {"id": 1488, "seek": 668972, "start": 6689.72, "end": 6692.820000000001, "text": " studied and lots of people are using.", "tokens": [9454, 293, 3195, 295, 561, 366, 1228, 13], "temperature": 0.0, "avg_logprob": -0.10919750861401828, "compression_ratio": 1.6271929824561404, "no_speech_prob": 2.902296046158881e-06}, {"id": 1489, "seek": 668972, "start": 6692.820000000001, "end": 6694.02, "text": " It's out there in the wild.", "tokens": [467, 311, 484, 456, 294, 264, 4868, 13], "temperature": 0.0, "avg_logprob": -0.10919750861401828, "compression_ratio": 1.6271929824561404, "no_speech_prob": 2.902296046158881e-06}, {"id": 1490, "seek": 668972, "start": 6694.02, "end": 6700.34, "text": " They all look kind of crazy.", "tokens": [814, 439, 574, 733, 295, 3219, 13], "temperature": 0.0, "avg_logprob": -0.10919750861401828, "compression_ratio": 1.6271929824561404, "no_speech_prob": 2.902296046158881e-06}, {"id": 1491, "seek": 668972, "start": 6700.34, "end": 6702.22, "text": " And so it's interesting to think about why.", "tokens": [400, 370, 309, 311, 1880, 281, 519, 466, 983, 13], "temperature": 0.0, "avg_logprob": -0.10919750861401828, "compression_ratio": 1.6271929824561404, "no_speech_prob": 2.902296046158881e-06}, {"id": 1492, "seek": 668972, "start": 6702.22, "end": 6706.6, "text": " And so one of the reasons why is that the data we feed these things.", "tokens": [400, 370, 472, 295, 264, 4112, 983, 307, 300, 264, 1412, 321, 3154, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.10919750861401828, "compression_ratio": 1.6271929824561404, "no_speech_prob": 2.902296046158881e-06}, {"id": 1493, "seek": 668972, "start": 6706.6, "end": 6713.04, "text": " We tend to use, me included, a lot of these data sets kind of unthinkingly.", "tokens": [492, 3928, 281, 764, 11, 385, 5556, 11, 257, 688, 295, 613, 1412, 6352, 733, 295, 517, 21074, 12163, 13], "temperature": 0.0, "avg_logprob": -0.10919750861401828, "compression_ratio": 1.6271929824561404, "no_speech_prob": 2.902296046158881e-06}, {"id": 1494, "seek": 668972, "start": 6713.04, "end": 6718.18, "text": " But like ImageNet, which is the basis of like a lot of the computer vision stuff we do,", "tokens": [583, 411, 29903, 31890, 11, 597, 307, 264, 5143, 295, 411, 257, 688, 295, 264, 3820, 5201, 1507, 321, 360, 11], "temperature": 0.0, "avg_logprob": -0.10919750861401828, "compression_ratio": 1.6271929824561404, "no_speech_prob": 2.902296046158881e-06}, {"id": 1495, "seek": 671818, "start": 6718.18, "end": 6723.26, "text": " is over half American and Great Britain.", "tokens": [307, 670, 1922, 2665, 293, 3769, 12960, 13], "temperature": 0.0, "avg_logprob": -0.19225517908732095, "compression_ratio": 1.4536585365853658, "no_speech_prob": 2.6425586838740855e-06}, {"id": 1496, "seek": 671818, "start": 6723.26, "end": 6731.14, "text": " Like when it comes to the countries that actually have most of the population in the world,", "tokens": [1743, 562, 309, 1487, 281, 264, 3517, 300, 767, 362, 881, 295, 264, 4415, 294, 264, 1002, 11], "temperature": 0.0, "avg_logprob": -0.19225517908732095, "compression_ratio": 1.4536585365853658, "no_speech_prob": 2.6425586838740855e-06}, {"id": 1497, "seek": 671818, "start": 6731.14, "end": 6732.5, "text": " I can't even see them here.", "tokens": [286, 393, 380, 754, 536, 552, 510, 13], "temperature": 0.0, "avg_logprob": -0.19225517908732095, "compression_ratio": 1.4536585365853658, "no_speech_prob": 2.6425586838740855e-06}, {"id": 1498, "seek": 671818, "start": 6732.5, "end": 6735.14, "text": " They're somewhere in these impossibly thin lines.", "tokens": [814, 434, 4079, 294, 613, 38802, 3545, 5862, 3876, 13], "temperature": 0.0, "avg_logprob": -0.19225517908732095, "compression_ratio": 1.4536585365853658, "no_speech_prob": 2.6425586838740855e-06}, {"id": 1499, "seek": 671818, "start": 6735.14, "end": 6742.66, "text": " Because remember, these data sets are being created almost exclusively by people in US,", "tokens": [1436, 1604, 11, 613, 1412, 6352, 366, 885, 2942, 1920, 20638, 538, 561, 294, 2546, 11], "temperature": 0.0, "avg_logprob": -0.19225517908732095, "compression_ratio": 1.4536585365853658, "no_speech_prob": 2.6425586838740855e-06}, {"id": 1500, "seek": 674266, "start": 6742.66, "end": 6748.26, "text": " Great Britain, and nowadays increasingly also China.", "tokens": [3769, 12960, 11, 293, 13434, 12980, 611, 3533, 13], "temperature": 0.0, "avg_logprob": -0.09554050135058026, "compression_ratio": 1.613861386138614, "no_speech_prob": 2.7693872652889695e-06}, {"id": 1501, "seek": 674266, "start": 6748.26, "end": 6753.58, "text": " So there's a lot of bias in the content we're creating because of a bias in the kind of", "tokens": [407, 456, 311, 257, 688, 295, 12577, 294, 264, 2701, 321, 434, 4084, 570, 295, 257, 12577, 294, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.09554050135058026, "compression_ratio": 1.613861386138614, "no_speech_prob": 2.7693872652889695e-06}, {"id": 1502, "seek": 674266, "start": 6753.58, "end": 6759.0599999999995, "text": " people that are creating that content, even when in theory it's being created in a very", "tokens": [561, 300, 366, 4084, 300, 2701, 11, 754, 562, 294, 5261, 309, 311, 885, 2942, 294, 257, 588], "temperature": 0.0, "avg_logprob": -0.09554050135058026, "compression_ratio": 1.613861386138614, "no_speech_prob": 2.7693872652889695e-06}, {"id": 1503, "seek": 674266, "start": 6759.0599999999995, "end": 6760.0599999999995, "text": " kind of neutral way.", "tokens": [733, 295, 10598, 636, 13], "temperature": 0.0, "avg_logprob": -0.09554050135058026, "compression_ratio": 1.613861386138614, "no_speech_prob": 2.7693872652889695e-06}, {"id": 1504, "seek": 674266, "start": 6760.0599999999995, "end": 6762.62, "text": " But you can't argue with the data, right?", "tokens": [583, 291, 393, 380, 9695, 365, 264, 1412, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.09554050135058026, "compression_ratio": 1.613861386138614, "no_speech_prob": 2.7693872652889695e-06}, {"id": 1505, "seek": 674266, "start": 6762.62, "end": 6767.42, "text": " It's obviously not neutral at all.", "tokens": [467, 311, 2745, 406, 10598, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.09554050135058026, "compression_ratio": 1.613861386138614, "no_speech_prob": 2.7693872652889695e-06}, {"id": 1506, "seek": 676742, "start": 6767.42, "end": 6774.9, "text": " And so when you have biased data creating biased algorithms, you then need to say like,", "tokens": [400, 370, 562, 291, 362, 28035, 1412, 4084, 28035, 14642, 11, 291, 550, 643, 281, 584, 411, 11], "temperature": 0.0, "avg_logprob": -0.14251923561096191, "compression_ratio": 1.5565610859728507, "no_speech_prob": 5.862626949237892e-06}, {"id": 1507, "seek": 676742, "start": 6774.9, "end": 6776.06, "text": " well, what are we doing with that?", "tokens": [731, 11, 437, 366, 321, 884, 365, 300, 30], "temperature": 0.0, "avg_logprob": -0.14251923561096191, "compression_ratio": 1.5565610859728507, "no_speech_prob": 5.862626949237892e-06}, {"id": 1508, "seek": 676742, "start": 6776.06, "end": 6779.74, "text": " So we've spent a lot of time talking about image recognition.", "tokens": [407, 321, 600, 4418, 257, 688, 295, 565, 1417, 466, 3256, 11150, 13], "temperature": 0.0, "avg_logprob": -0.14251923561096191, "compression_ratio": 1.5565610859728507, "no_speech_prob": 5.862626949237892e-06}, {"id": 1509, "seek": 676742, "start": 6779.74, "end": 6785.9400000000005, "text": " So a couple of years ago, this company DeepLint advertised their image recognition system,", "tokens": [407, 257, 1916, 295, 924, 2057, 11, 341, 2237, 14895, 43, 686, 42310, 641, 3256, 11150, 1185, 11], "temperature": 0.0, "avg_logprob": -0.14251923561096191, "compression_ratio": 1.5565610859728507, "no_speech_prob": 5.862626949237892e-06}, {"id": 1510, "seek": 676742, "start": 6785.9400000000005, "end": 6792.62, "text": " which can be used to do mass surveillance on large crowds of people.", "tokens": [597, 393, 312, 1143, 281, 360, 2758, 18475, 322, 2416, 26070, 295, 561, 13], "temperature": 0.0, "avg_logprob": -0.14251923561096191, "compression_ratio": 1.5565610859728507, "no_speech_prob": 5.862626949237892e-06}, {"id": 1511, "seek": 679262, "start": 6792.62, "end": 6798.78, "text": " And any person passing through who is a person of interest in theory.", "tokens": [400, 604, 954, 8437, 807, 567, 307, 257, 954, 295, 1179, 294, 5261, 13], "temperature": 0.0, "avg_logprob": -0.12125571568806966, "compression_ratio": 1.6830357142857142, "no_speech_prob": 1.5445442841155455e-05}, {"id": 1512, "seek": 679262, "start": 6798.78, "end": 6806.2, "text": " And so putting aside even the question of like, is it a good idea to have such a system?", "tokens": [400, 370, 3372, 7359, 754, 264, 1168, 295, 411, 11, 307, 309, 257, 665, 1558, 281, 362, 1270, 257, 1185, 30], "temperature": 0.0, "avg_logprob": -0.12125571568806966, "compression_ratio": 1.6830357142857142, "no_speech_prob": 1.5445442841155455e-05}, {"id": 1513, "seek": 679262, "start": 6806.2, "end": 6810.0599999999995, "text": " You've got to think, is it a good idea to have such a system where certain kinds of", "tokens": [509, 600, 658, 281, 519, 11, 307, 309, 257, 665, 1558, 281, 362, 1270, 257, 1185, 689, 1629, 3685, 295], "temperature": 0.0, "avg_logprob": -0.12125571568806966, "compression_ratio": 1.6830357142857142, "no_speech_prob": 1.5445442841155455e-05}, {"id": 1514, "seek": 679262, "start": 6810.0599999999995, "end": 6815.42, "text": " people are 300 times more likely to be misidentified?", "tokens": [561, 366, 6641, 1413, 544, 3700, 281, 312, 3346, 40613, 30], "temperature": 0.0, "avg_logprob": -0.12125571568806966, "compression_ratio": 1.6830357142857142, "no_speech_prob": 1.5445442841155455e-05}, {"id": 1515, "seek": 679262, "start": 6815.42, "end": 6820.98, "text": " And then thinking about it, so this is now starting to happen in America, right?", "tokens": [400, 550, 1953, 466, 309, 11, 370, 341, 307, 586, 2891, 281, 1051, 294, 3374, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12125571568806966, "compression_ratio": 1.6830357142857142, "no_speech_prob": 1.5445442841155455e-05}, {"id": 1516, "seek": 682098, "start": 6820.98, "end": 6823.0599999999995, "text": " These systems are being rolled out.", "tokens": [1981, 3652, 366, 885, 14306, 484, 13], "temperature": 0.0, "avg_logprob": -0.09275648752848308, "compression_ratio": 1.634517766497462, "no_speech_prob": 1.1125407581857871e-05}, {"id": 1517, "seek": 682098, "start": 6823.0599999999995, "end": 6830.299999999999, "text": " And so there are now systems in America that will identify a person of interest in a video", "tokens": [400, 370, 456, 366, 586, 3652, 294, 3374, 300, 486, 5876, 257, 954, 295, 1179, 294, 257, 960], "temperature": 0.0, "avg_logprob": -0.09275648752848308, "compression_ratio": 1.634517766497462, "no_speech_prob": 1.1125407581857871e-05}, {"id": 1518, "seek": 682098, "start": 6830.299999999999, "end": 6834.0199999999995, "text": " and send a ping to the local police.", "tokens": [293, 2845, 257, 26151, 281, 264, 2654, 3804, 13], "temperature": 0.0, "avg_logprob": -0.09275648752848308, "compression_ratio": 1.634517766497462, "no_speech_prob": 1.1125407581857871e-05}, {"id": 1519, "seek": 682098, "start": 6834.0199999999995, "end": 6839.5, "text": " And so these systems are extremely inaccurate and extremely biased.", "tokens": [400, 370, 613, 3652, 366, 4664, 46443, 293, 4664, 28035, 13], "temperature": 0.0, "avg_logprob": -0.09275648752848308, "compression_ratio": 1.634517766497462, "no_speech_prob": 1.1125407581857871e-05}, {"id": 1520, "seek": 682098, "start": 6839.5, "end": 6845.219999999999, "text": " And what happens then, of course, is if you're in a predominantly black neighborhood where", "tokens": [400, 437, 2314, 550, 11, 295, 1164, 11, 307, 498, 291, 434, 294, 257, 29893, 2211, 7630, 689], "temperature": 0.0, "avg_logprob": -0.09275648752848308, "compression_ratio": 1.634517766497462, "no_speech_prob": 1.1125407581857871e-05}, {"id": 1521, "seek": 684522, "start": 6845.22, "end": 6851.860000000001, "text": " the probability of successfully recognizing you is much lower, and you're much more likely", "tokens": [264, 8482, 295, 10727, 18538, 291, 307, 709, 3126, 11, 293, 291, 434, 709, 544, 3700], "temperature": 0.0, "avg_logprob": -0.09680303555090451, "compression_ratio": 1.9246861924686192, "no_speech_prob": 9.080331437871791e-06}, {"id": 1522, "seek": 684522, "start": 6851.860000000001, "end": 6858.400000000001, "text": " to be surrounded by black people, and so suddenly all of these black people are popping up as", "tokens": [281, 312, 13221, 538, 2211, 561, 11, 293, 370, 5800, 439, 295, 613, 2211, 561, 366, 18374, 493, 382], "temperature": 0.0, "avg_logprob": -0.09680303555090451, "compression_ratio": 1.9246861924686192, "no_speech_prob": 9.080331437871791e-06}, {"id": 1523, "seek": 684522, "start": 6858.400000000001, "end": 6863.34, "text": " persons of interest or in a video of a person of interest, all the people in the video are", "tokens": [14453, 295, 1179, 420, 294, 257, 960, 295, 257, 954, 295, 1179, 11, 439, 264, 561, 294, 264, 960, 366], "temperature": 0.0, "avg_logprob": -0.09680303555090451, "compression_ratio": 1.9246861924686192, "no_speech_prob": 9.080331437871791e-06}, {"id": 1524, "seek": 684522, "start": 6863.34, "end": 6868.3, "text": " all recognized as in the vicinity of the person of interest, you suddenly get all these pings", "tokens": [439, 9823, 382, 294, 264, 42387, 295, 264, 954, 295, 1179, 11, 291, 5800, 483, 439, 613, 280, 1109], "temperature": 0.0, "avg_logprob": -0.09680303555090451, "compression_ratio": 1.9246861924686192, "no_speech_prob": 9.080331437871791e-06}, {"id": 1525, "seek": 684522, "start": 6868.3, "end": 6873.9800000000005, "text": " going off the local police department, causing the police to run down there, and therefore", "tokens": [516, 766, 264, 2654, 3804, 5882, 11, 9853, 264, 3804, 281, 1190, 760, 456, 11, 293, 4412], "temperature": 0.0, "avg_logprob": -0.09680303555090451, "compression_ratio": 1.9246861924686192, "no_speech_prob": 9.080331437871791e-06}, {"id": 1526, "seek": 687398, "start": 6873.98, "end": 6879.179999999999, "text": " likely to lead to a larger number of arrests, which is then likely to feed back into the", "tokens": [3700, 281, 1477, 281, 257, 4833, 1230, 295, 48813, 11, 597, 307, 550, 3700, 281, 3154, 646, 666, 264], "temperature": 0.0, "avg_logprob": -0.11356738034416647, "compression_ratio": 1.6546184738955823, "no_speech_prob": 8.939604413171764e-06}, {"id": 1527, "seek": 687398, "start": 6879.179999999999, "end": 6882.66, "text": " data being used to develop the systems.", "tokens": [1412, 885, 1143, 281, 1499, 264, 3652, 13], "temperature": 0.0, "avg_logprob": -0.11356738034416647, "compression_ratio": 1.6546184738955823, "no_speech_prob": 8.939604413171764e-06}, {"id": 1528, "seek": 687398, "start": 6882.66, "end": 6885.299999999999, "text": " So this is happening right now.", "tokens": [407, 341, 307, 2737, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.11356738034416647, "compression_ratio": 1.6546184738955823, "no_speech_prob": 8.939604413171764e-06}, {"id": 1529, "seek": 687398, "start": 6885.299999999999, "end": 6890.299999999999, "text": " And so thankfully a very small number of people are actually bothering to look into these", "tokens": [400, 370, 27352, 257, 588, 1359, 1230, 295, 561, 366, 767, 31432, 281, 574, 666, 613], "temperature": 0.0, "avg_logprob": -0.11356738034416647, "compression_ratio": 1.6546184738955823, "no_speech_prob": 8.939604413171764e-06}, {"id": 1530, "seek": 687398, "start": 6890.299999999999, "end": 6891.299999999999, "text": " things.", "tokens": [721, 13], "temperature": 0.0, "avg_logprob": -0.11356738034416647, "compression_ratio": 1.6546184738955823, "no_speech_prob": 8.939604413171764e-06}, {"id": 1531, "seek": 687398, "start": 6891.299999999999, "end": 6893.5, "text": " I mean ridiculously small, but at least it's better than nothing.", "tokens": [286, 914, 41358, 1359, 11, 457, 412, 1935, 309, 311, 1101, 813, 1825, 13], "temperature": 0.0, "avg_logprob": -0.11356738034416647, "compression_ratio": 1.6546184738955823, "no_speech_prob": 8.939604413171764e-06}, {"id": 1532, "seek": 687398, "start": 6893.5, "end": 6898.58, "text": " And so for example, now one of the best ways that people get publicity is to do kind of", "tokens": [400, 370, 337, 1365, 11, 586, 472, 295, 264, 1151, 2098, 300, 561, 483, 37264, 307, 281, 360, 733, 295], "temperature": 0.0, "avg_logprob": -0.11356738034416647, "compression_ratio": 1.6546184738955823, "no_speech_prob": 8.939604413171764e-06}, {"id": 1533, "seek": 689858, "start": 6898.58, "end": 6906.9, "text": " funny experiments like, let's try the mugshot image recognition system that's being widely", "tokens": [4074, 12050, 411, 11, 718, 311, 853, 264, 23610, 18402, 3256, 11150, 1185, 300, 311, 885, 13371], "temperature": 0.0, "avg_logprob": -0.21630082231886844, "compression_ratio": 1.6781115879828326, "no_speech_prob": 4.425448423717171e-06}, {"id": 1534, "seek": 689858, "start": 6906.9, "end": 6912.58, "text": " used and try it against the members of Congress and find out that there are 28 members of", "tokens": [1143, 293, 853, 309, 1970, 264, 2679, 295, 6426, 293, 915, 484, 300, 456, 366, 7562, 2679, 295], "temperature": 0.0, "avg_logprob": -0.21630082231886844, "compression_ratio": 1.6781115879828326, "no_speech_prob": 4.425448423717171e-06}, {"id": 1535, "seek": 689858, "start": 6912.58, "end": 6917.82, "text": " Congress who would have been identified by this system, obviously incorrectly.", "tokens": [6426, 567, 576, 362, 668, 9234, 538, 341, 1185, 11, 2745, 42892, 13], "temperature": 0.0, "avg_logprob": -0.21630082231886844, "compression_ratio": 1.6781115879828326, "no_speech_prob": 4.425448423717171e-06}, {"id": 1536, "seek": 689858, "start": 6917.82, "end": 6921.5, "text": " Oh, I didn't know that.", "tokens": [876, 11, 286, 994, 380, 458, 300, 13], "temperature": 0.0, "avg_logprob": -0.21630082231886844, "compression_ratio": 1.6781115879828326, "no_speech_prob": 4.425448423717171e-06}, {"id": 1537, "seek": 689858, "start": 6921.5, "end": 6926.62, "text": " Okay, members of Congress, black members of Congress, not at all surprised to hear that.", "tokens": [1033, 11, 2679, 295, 6426, 11, 2211, 2679, 295, 6426, 11, 406, 412, 439, 6100, 281, 1568, 300, 13], "temperature": 0.0, "avg_logprob": -0.21630082231886844, "compression_ratio": 1.6781115879828326, "no_speech_prob": 4.425448423717171e-06}, {"id": 1538, "seek": 689858, "start": 6926.62, "end": 6928.54, "text": " Thank you, Rachel.", "tokens": [1044, 291, 11, 14246, 13], "temperature": 0.0, "avg_logprob": -0.21630082231886844, "compression_ratio": 1.6781115879828326, "no_speech_prob": 4.425448423717171e-06}, {"id": 1539, "seek": 692854, "start": 6928.54, "end": 6934.42, "text": " We see this kind of bias in a lot of the systems we use, not just image recognition, but text", "tokens": [492, 536, 341, 733, 295, 12577, 294, 257, 688, 295, 264, 3652, 321, 764, 11, 406, 445, 3256, 11150, 11, 457, 2487], "temperature": 0.0, "avg_logprob": -0.1143771181202898, "compression_ratio": 1.7621145374449338, "no_speech_prob": 3.59114128514193e-05}, {"id": 1540, "seek": 692854, "start": 6934.42, "end": 6940.14, "text": " translation when you convert she is a doctor, he is a nurse, into Turkish, you quite correctly", "tokens": [12853, 562, 291, 7620, 750, 307, 257, 4631, 11, 415, 307, 257, 14012, 11, 666, 18565, 11, 291, 1596, 8944], "temperature": 0.0, "avg_logprob": -0.1143771181202898, "compression_ratio": 1.7621145374449338, "no_speech_prob": 3.59114128514193e-05}, {"id": 1541, "seek": 692854, "start": 6940.14, "end": 6945.26, "text": " get a gender in specific pronoun because that's what Turkish uses.", "tokens": [483, 257, 7898, 294, 2685, 14144, 570, 300, 311, 437, 18565, 4960, 13], "temperature": 0.0, "avg_logprob": -0.1143771181202898, "compression_ratio": 1.7621145374449338, "no_speech_prob": 3.59114128514193e-05}, {"id": 1542, "seek": 692854, "start": 6945.26, "end": 6949.2, "text": " You could then take that and feed it back into Turkish with your gender in specific", "tokens": [509, 727, 550, 747, 300, 293, 3154, 309, 646, 666, 18565, 365, 428, 7898, 294, 2685], "temperature": 0.0, "avg_logprob": -0.1143771181202898, "compression_ratio": 1.7621145374449338, "no_speech_prob": 3.59114128514193e-05}, {"id": 1543, "seek": 692854, "start": 6949.2, "end": 6953.74, "text": " pronoun and you will now get he is a doctor, she is a nurse.", "tokens": [14144, 293, 291, 486, 586, 483, 415, 307, 257, 4631, 11, 750, 307, 257, 14012, 13], "temperature": 0.0, "avg_logprob": -0.1143771181202898, "compression_ratio": 1.7621145374449338, "no_speech_prob": 3.59114128514193e-05}, {"id": 1544, "seek": 695374, "start": 6953.74, "end": 6960.54, "text": " So the bias again, this is in a massively widely rolled out carefully studied system", "tokens": [407, 264, 12577, 797, 11, 341, 307, 294, 257, 29379, 13371, 14306, 484, 7500, 9454, 1185], "temperature": 0.0, "avg_logprob": -0.1293689577203048, "compression_ratio": 1.5707070707070707, "no_speech_prob": 5.1738134061452e-06}, {"id": 1545, "seek": 695374, "start": 6960.54, "end": 6964.62, "text": " and it's not like even these kind of things like a little one off things that then get", "tokens": [293, 309, 311, 406, 411, 754, 613, 733, 295, 721, 411, 257, 707, 472, 766, 721, 300, 550, 483], "temperature": 0.0, "avg_logprob": -0.1293689577203048, "compression_ratio": 1.5707070707070707, "no_speech_prob": 5.1738134061452e-06}, {"id": 1546, "seek": 695374, "start": 6964.62, "end": 6965.86, "text": " fixed quickly.", "tokens": [6806, 2661, 13], "temperature": 0.0, "avg_logprob": -0.1293689577203048, "compression_ratio": 1.5707070707070707, "no_speech_prob": 5.1738134061452e-06}, {"id": 1547, "seek": 695374, "start": 6965.86, "end": 6969.78, "text": " These issues have been identified in Google Translate for a very long time and they're", "tokens": [1981, 2663, 362, 668, 9234, 294, 3329, 6531, 17593, 337, 257, 588, 938, 565, 293, 436, 434], "temperature": 0.0, "avg_logprob": -0.1293689577203048, "compression_ratio": 1.5707070707070707, "no_speech_prob": 5.1738134061452e-06}, {"id": 1548, "seek": 695374, "start": 6969.78, "end": 6974.46, "text": " still there and they don't get fixed.", "tokens": [920, 456, 293, 436, 500, 380, 483, 6806, 13], "temperature": 0.0, "avg_logprob": -0.1293689577203048, "compression_ratio": 1.5707070707070707, "no_speech_prob": 5.1738134061452e-06}, {"id": 1549, "seek": 697446, "start": 6974.46, "end": 6984.66, "text": " So the kind of results of this are, in my opinion, quite terrifying because what's happening", "tokens": [407, 264, 733, 295, 3542, 295, 341, 366, 11, 294, 452, 4800, 11, 1596, 18106, 570, 437, 311, 2737], "temperature": 0.0, "avg_logprob": -0.14153496785597366, "compression_ratio": 1.5224489795918368, "no_speech_prob": 3.393021415831754e-06}, {"id": 1550, "seek": 697446, "start": 6984.66, "end": 6990.62, "text": " is that in many countries, including America, where I'm speaking from now, algorithms are", "tokens": [307, 300, 294, 867, 3517, 11, 3009, 3374, 11, 689, 286, 478, 4124, 490, 586, 11, 14642, 366], "temperature": 0.0, "avg_logprob": -0.14153496785597366, "compression_ratio": 1.5224489795918368, "no_speech_prob": 3.393021415831754e-06}, {"id": 1551, "seek": 697446, "start": 6990.62, "end": 6997.66, "text": " increasingly being used for all kinds of public policy, judicial and so forth purposes.", "tokens": [12980, 885, 1143, 337, 439, 3685, 295, 1908, 3897, 11, 26581, 293, 370, 5220, 9932, 13], "temperature": 0.0, "avg_logprob": -0.14153496785597366, "compression_ratio": 1.5224489795918368, "no_speech_prob": 3.393021415831754e-06}, {"id": 1552, "seek": 697446, "start": 6997.66, "end": 7002.18, "text": " For example, there's a system called Compass, which is very widely used to decide who's", "tokens": [1171, 1365, 11, 456, 311, 257, 1185, 1219, 50179, 11, 597, 307, 588, 13371, 1143, 281, 4536, 567, 311], "temperature": 0.0, "avg_logprob": -0.14153496785597366, "compression_ratio": 1.5224489795918368, "no_speech_prob": 3.393021415831754e-06}, {"id": 1553, "seek": 697446, "start": 7002.18, "end": 7003.18, "text": " going to jail.", "tokens": [516, 281, 10511, 13], "temperature": 0.0, "avg_logprob": -0.14153496785597366, "compression_ratio": 1.5224489795918368, "no_speech_prob": 3.393021415831754e-06}, {"id": 1554, "seek": 700318, "start": 7003.18, "end": 7005.700000000001, "text": " And it does that in a couple of ways.", "tokens": [400, 309, 775, 300, 294, 257, 1916, 295, 2098, 13], "temperature": 0.0, "avg_logprob": -0.12897384313889493, "compression_ratio": 1.575609756097561, "no_speech_prob": 1.3005724213144276e-05}, {"id": 1555, "seek": 700318, "start": 7005.700000000001, "end": 7011.02, "text": " It tells judges what sentencing guidelines they should use for particular cases and it", "tokens": [467, 5112, 14449, 437, 2279, 13644, 12470, 436, 820, 764, 337, 1729, 3331, 293, 309], "temperature": 0.0, "avg_logprob": -0.12897384313889493, "compression_ratio": 1.575609756097561, "no_speech_prob": 1.3005724213144276e-05}, {"id": 1556, "seek": 700318, "start": 7011.02, "end": 7018.9800000000005, "text": " tells them also which people the system says should be let out on bail.", "tokens": [5112, 552, 611, 597, 561, 264, 1185, 1619, 820, 312, 718, 484, 322, 19313, 13], "temperature": 0.0, "avg_logprob": -0.12897384313889493, "compression_ratio": 1.575609756097561, "no_speech_prob": 1.3005724213144276e-05}, {"id": 1557, "seek": 700318, "start": 7018.9800000000005, "end": 7024.860000000001, "text": " But here's the thing, white people, it keeps on saying let this person out even though", "tokens": [583, 510, 311, 264, 551, 11, 2418, 561, 11, 309, 5965, 322, 1566, 718, 341, 954, 484, 754, 1673], "temperature": 0.0, "avg_logprob": -0.12897384313889493, "compression_ratio": 1.575609756097561, "no_speech_prob": 1.3005724213144276e-05}, {"id": 1558, "seek": 700318, "start": 7024.860000000001, "end": 7027.9400000000005, "text": " they end up reoffending and vice versa.", "tokens": [436, 917, 493, 319, 4506, 2029, 293, 11964, 25650, 13], "temperature": 0.0, "avg_logprob": -0.12897384313889493, "compression_ratio": 1.575609756097561, "no_speech_prob": 1.3005724213144276e-05}, {"id": 1559, "seek": 702794, "start": 7027.94, "end": 7034.0599999999995, "text": " It's systematically out by double compared to what it should be in terms of getting it", "tokens": [467, 311, 39531, 484, 538, 3834, 5347, 281, 437, 309, 820, 312, 294, 2115, 295, 1242, 309], "temperature": 0.0, "avg_logprob": -0.12522351741790771, "compression_ratio": 1.5445544554455446, "no_speech_prob": 1.2878882671429892e-06}, {"id": 1560, "seek": 702794, "start": 7034.0599999999995, "end": 7039.259999999999, "text": " wrong with white people versus black people.", "tokens": [2085, 365, 2418, 561, 5717, 2211, 561, 13], "temperature": 0.0, "avg_logprob": -0.12522351741790771, "compression_ratio": 1.5445544554455446, "no_speech_prob": 1.2878882671429892e-06}, {"id": 1561, "seek": 702794, "start": 7039.259999999999, "end": 7048.54, "text": " So this is kind of horrifying because, I mean, amongst other things, the data that it's using", "tokens": [407, 341, 307, 733, 295, 40227, 570, 11, 286, 914, 11, 12918, 661, 721, 11, 264, 1412, 300, 309, 311, 1228], "temperature": 0.0, "avg_logprob": -0.12522351741790771, "compression_ratio": 1.5445544554455446, "no_speech_prob": 1.2878882671429892e-06}, {"id": 1562, "seek": 702794, "start": 7048.54, "end": 7055.299999999999, "text": " in this system is literally asking people questions about things like, did any of your", "tokens": [294, 341, 1185, 307, 3736, 3365, 561, 1651, 466, 721, 411, 11, 630, 604, 295, 428], "temperature": 0.0, "avg_logprob": -0.12522351741790771, "compression_ratio": 1.5445544554455446, "no_speech_prob": 1.2878882671429892e-06}, {"id": 1563, "seek": 705530, "start": 7055.3, "end": 7059.62, "text": " parents ever go to jail, do any of your friends do drugs?", "tokens": [3152, 1562, 352, 281, 10511, 11, 360, 604, 295, 428, 1855, 360, 7766, 30], "temperature": 0.0, "avg_logprob": -0.1652083613655784, "compression_ratio": 1.6055045871559632, "no_speech_prob": 1.8341725080972537e-05}, {"id": 1564, "seek": 705530, "start": 7059.62, "end": 7065.54, "text": " Like, they're asking questions about other people who they have no control over.", "tokens": [1743, 11, 436, 434, 3365, 1651, 466, 661, 561, 567, 436, 362, 572, 1969, 670, 13], "temperature": 0.0, "avg_logprob": -0.1652083613655784, "compression_ratio": 1.6055045871559632, "no_speech_prob": 1.8341725080972537e-05}, {"id": 1565, "seek": 705530, "start": 7065.54, "end": 7072.62, "text": " So not only are these systems biased, very systematically biased, but they're also being", "tokens": [407, 406, 787, 366, 613, 3652, 28035, 11, 588, 39531, 28035, 11, 457, 436, 434, 611, 885], "temperature": 0.0, "avg_logprob": -0.1652083613655784, "compression_ratio": 1.6055045871559632, "no_speech_prob": 1.8341725080972537e-05}, {"id": 1566, "seek": 705530, "start": 7072.62, "end": 7077.860000000001, "text": " done on the basis of data which is totally out of your control.", "tokens": [1096, 322, 264, 5143, 295, 1412, 597, 307, 3879, 484, 295, 428, 1969, 13], "temperature": 0.0, "avg_logprob": -0.1652083613655784, "compression_ratio": 1.6055045871559632, "no_speech_prob": 1.8341725080972537e-05}, {"id": 1567, "seek": 705530, "start": 7077.860000000001, "end": 7081.06, "text": " So this is kind of, did you want to add something to that?", "tokens": [407, 341, 307, 733, 295, 11, 630, 291, 528, 281, 909, 746, 281, 300, 30], "temperature": 0.0, "avg_logprob": -0.1652083613655784, "compression_ratio": 1.6055045871559632, "no_speech_prob": 1.8341725080972537e-05}, {"id": 1568, "seek": 708106, "start": 7081.06, "end": 7085.5, "text": " Oh yeah, are your parents divorced is another question that's being used to decide whether", "tokens": [876, 1338, 11, 366, 428, 3152, 27670, 307, 1071, 1168, 300, 311, 885, 1143, 281, 4536, 1968], "temperature": 0.0, "avg_logprob": -0.11706793308258057, "compression_ratio": 1.673728813559322, "no_speech_prob": 1.42851340569905e-05}, {"id": 1569, "seek": 708106, "start": 7085.5, "end": 7087.700000000001, "text": " you go to jail or not.", "tokens": [291, 352, 281, 10511, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.11706793308258057, "compression_ratio": 1.673728813559322, "no_speech_prob": 1.42851340569905e-05}, {"id": 1570, "seek": 708106, "start": 7087.700000000001, "end": 7095.34, "text": " So when we raise these issues kind of on Twitter or in talks or whatever, there's always a", "tokens": [407, 562, 321, 5300, 613, 2663, 733, 295, 322, 5794, 420, 294, 6686, 420, 2035, 11, 456, 311, 1009, 257], "temperature": 0.0, "avg_logprob": -0.11706793308258057, "compression_ratio": 1.673728813559322, "no_speech_prob": 1.42851340569905e-05}, {"id": 1571, "seek": 708106, "start": 7095.34, "end": 7102.400000000001, "text": " few people, always white men, a few people who will always say like, that's just the", "tokens": [1326, 561, 11, 1009, 2418, 1706, 11, 257, 1326, 561, 567, 486, 1009, 584, 411, 11, 300, 311, 445, 264], "temperature": 0.0, "avg_logprob": -0.11706793308258057, "compression_ratio": 1.673728813559322, "no_speech_prob": 1.42851340569905e-05}, {"id": 1572, "seek": 708106, "start": 7102.400000000001, "end": 7106.740000000001, "text": " way the world is, that's just reflecting what the data shows.", "tokens": [636, 264, 1002, 307, 11, 300, 311, 445, 23543, 437, 264, 1412, 3110, 13], "temperature": 0.0, "avg_logprob": -0.11706793308258057, "compression_ratio": 1.673728813559322, "no_speech_prob": 1.42851340569905e-05}, {"id": 1573, "seek": 708106, "start": 7106.740000000001, "end": 7109.740000000001, "text": " But when you actually look at it, it's not.", "tokens": [583, 562, 291, 767, 574, 412, 309, 11, 309, 311, 406, 13], "temperature": 0.0, "avg_logprob": -0.11706793308258057, "compression_ratio": 1.673728813559322, "no_speech_prob": 1.42851340569905e-05}, {"id": 1574, "seek": 710974, "start": 7109.74, "end": 7117.74, "text": " It's actually systematically erroneous, and systematically erroneous against people of", "tokens": [467, 311, 767, 39531, 1189, 26446, 563, 11, 293, 39531, 1189, 26446, 563, 1970, 561, 295], "temperature": 0.0, "avg_logprob": -0.1335114930805407, "compression_ratio": 1.6062176165803108, "no_speech_prob": 4.637756774172885e-06}, {"id": 1575, "seek": 710974, "start": 7117.74, "end": 7123.98, "text": " color, minorities, the people who are less involved in creating the systems that these", "tokens": [2017, 11, 30373, 11, 264, 561, 567, 366, 1570, 3288, 294, 4084, 264, 3652, 300, 613], "temperature": 0.0, "avg_logprob": -0.1335114930805407, "compression_ratio": 1.6062176165803108, "no_speech_prob": 4.637756774172885e-06}, {"id": 1576, "seek": 710974, "start": 7123.98, "end": 7127.86, "text": " products are based on.", "tokens": [3383, 366, 2361, 322, 13], "temperature": 0.0, "avg_logprob": -0.1335114930805407, "compression_ratio": 1.6062176165803108, "no_speech_prob": 4.637756774172885e-06}, {"id": 1577, "seek": 710974, "start": 7127.86, "end": 7131.139999999999, "text": " Sometimes this can go a really long way.", "tokens": [4803, 341, 393, 352, 257, 534, 938, 636, 13], "temperature": 0.0, "avg_logprob": -0.1335114930805407, "compression_ratio": 1.6062176165803108, "no_speech_prob": 4.637756774172885e-06}, {"id": 1578, "seek": 710974, "start": 7131.139999999999, "end": 7138.139999999999, "text": " So for example, in Myanmar, there was a genocide of the Rohingya people.", "tokens": [407, 337, 1365, 11, 294, 42725, 11, 456, 390, 257, 31867, 295, 264, 3101, 571, 3016, 561, 13], "temperature": 0.0, "avg_logprob": -0.1335114930805407, "compression_ratio": 1.6062176165803108, "no_speech_prob": 4.637756774172885e-06}, {"id": 1579, "seek": 713814, "start": 7138.14, "end": 7143.58, "text": " And that genocide was very heavily created by Facebook.", "tokens": [400, 300, 31867, 390, 588, 10950, 2942, 538, 4384, 13], "temperature": 0.0, "avg_logprob": -0.14613667275141745, "compression_ratio": 1.7131147540983607, "no_speech_prob": 2.8129484235250857e-06}, {"id": 1580, "seek": 713814, "start": 7143.58, "end": 7146.34, "text": " Not because anybody at Facebook wanted it, I mean, heavens know.", "tokens": [1726, 570, 4472, 412, 4384, 1415, 309, 11, 286, 914, 11, 26011, 458, 13], "temperature": 0.0, "avg_logprob": -0.14613667275141745, "compression_ratio": 1.7131147540983607, "no_speech_prob": 2.8129484235250857e-06}, {"id": 1581, "seek": 713814, "start": 7146.34, "end": 7150.660000000001, "text": " I know a lot of people at Facebook, I have a lot of friends at Facebook, they're really", "tokens": [286, 458, 257, 688, 295, 561, 412, 4384, 11, 286, 362, 257, 688, 295, 1855, 412, 4384, 11, 436, 434, 534], "temperature": 0.0, "avg_logprob": -0.14613667275141745, "compression_ratio": 1.7131147540983607, "no_speech_prob": 2.8129484235250857e-06}, {"id": 1582, "seek": 713814, "start": 7150.660000000001, "end": 7152.3, "text": " trying to do the right thing.", "tokens": [1382, 281, 360, 264, 558, 551, 13], "temperature": 0.0, "avg_logprob": -0.14613667275141745, "compression_ratio": 1.7131147540983607, "no_speech_prob": 2.8129484235250857e-06}, {"id": 1583, "seek": 713814, "start": 7152.3, "end": 7157.02, "text": " They're really trying to create a product that people like, but not in a thoughtful", "tokens": [814, 434, 534, 1382, 281, 1884, 257, 1674, 300, 561, 411, 11, 457, 406, 294, 257, 21566], "temperature": 0.0, "avg_logprob": -0.14613667275141745, "compression_ratio": 1.7131147540983607, "no_speech_prob": 2.8129484235250857e-06}, {"id": 1584, "seek": 713814, "start": 7157.02, "end": 7158.38, "text": " enough way.", "tokens": [1547, 636, 13], "temperature": 0.0, "avg_logprob": -0.14613667275141745, "compression_ratio": 1.7131147540983607, "no_speech_prob": 2.8129484235250857e-06}, {"id": 1585, "seek": 713814, "start": 7158.38, "end": 7163.34, "text": " Because when you roll out something where literally in Myanmar, a country that most", "tokens": [1436, 562, 291, 3373, 484, 746, 689, 3736, 294, 42725, 11, 257, 1941, 300, 881], "temperature": 0.0, "avg_logprob": -0.14613667275141745, "compression_ratio": 1.7131147540983607, "no_speech_prob": 2.8129484235250857e-06}, {"id": 1586, "seek": 716334, "start": 7163.34, "end": 7169.9400000000005, "text": " people didn't have, most, maybe half of people didn't have electricity until very recently,", "tokens": [561, 994, 380, 362, 11, 881, 11, 1310, 1922, 295, 561, 994, 380, 362, 10356, 1826, 588, 3938, 11], "temperature": 0.0, "avg_logprob": -0.15313519683538698, "compression_ratio": 1.6489795918367347, "no_speech_prob": 1.0952523552987259e-05}, {"id": 1587, "seek": 716334, "start": 7169.9400000000005, "end": 7174.18, "text": " and you say, hey, you can all have free internet, as long as it's just Facebook.", "tokens": [293, 291, 584, 11, 4177, 11, 291, 393, 439, 362, 1737, 4705, 11, 382, 938, 382, 309, 311, 445, 4384, 13], "temperature": 0.0, "avg_logprob": -0.15313519683538698, "compression_ratio": 1.6489795918367347, "no_speech_prob": 1.0952523552987259e-05}, {"id": 1588, "seek": 716334, "start": 7174.18, "end": 7177.56, "text": " You've got to think carefully about what you're doing.", "tokens": [509, 600, 658, 281, 519, 7500, 466, 437, 291, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.15313519683538698, "compression_ratio": 1.6489795918367347, "no_speech_prob": 1.0952523552987259e-05}, {"id": 1589, "seek": 716334, "start": 7177.56, "end": 7182.26, "text": " And then you use algorithms to feed people the stuff they will click on.", "tokens": [400, 550, 291, 764, 14642, 281, 3154, 561, 264, 1507, 436, 486, 2052, 322, 13], "temperature": 0.0, "avg_logprob": -0.15313519683538698, "compression_ratio": 1.6489795918367347, "no_speech_prob": 1.0952523552987259e-05}, {"id": 1590, "seek": 716334, "start": 7182.26, "end": 7187.38, "text": " And of course, what people click on is stuff which is controversial, stuff that makes their", "tokens": [400, 295, 1164, 11, 437, 561, 2052, 322, 307, 1507, 597, 307, 17323, 11, 1507, 300, 1669, 641], "temperature": 0.0, "avg_logprob": -0.15313519683538698, "compression_ratio": 1.6489795918367347, "no_speech_prob": 1.0952523552987259e-05}, {"id": 1591, "seek": 716334, "start": 7187.38, "end": 7188.56, "text": " blood boil.", "tokens": [3390, 13329, 13], "temperature": 0.0, "avg_logprob": -0.15313519683538698, "compression_ratio": 1.6489795918367347, "no_speech_prob": 1.0952523552987259e-05}, {"id": 1592, "seek": 718856, "start": 7188.56, "end": 7194.9400000000005, "text": " So when they actually started asking the generals in the Myanmar army that were literally throwing", "tokens": [407, 562, 436, 767, 1409, 3365, 264, 41346, 294, 264, 42725, 7267, 300, 645, 3736, 10238], "temperature": 0.0, "avg_logprob": -0.1288681139891175, "compression_ratio": 1.8701923076923077, "no_speech_prob": 5.507363766810158e-06}, {"id": 1593, "seek": 718856, "start": 7194.9400000000005, "end": 7200.8, "text": " babies onto bonfires, they were saying, we know that these are not humans.", "tokens": [10917, 3911, 4428, 36197, 11, 436, 645, 1566, 11, 321, 458, 300, 613, 366, 406, 6255, 13], "temperature": 0.0, "avg_logprob": -0.1288681139891175, "compression_ratio": 1.8701923076923077, "no_speech_prob": 5.507363766810158e-06}, {"id": 1594, "seek": 718856, "start": 7200.8, "end": 7207.02, "text": " We know that they are animals, because we read the news, we read the internet.", "tokens": [492, 458, 300, 436, 366, 4882, 11, 570, 321, 1401, 264, 2583, 11, 321, 1401, 264, 4705, 13], "temperature": 0.0, "avg_logprob": -0.1288681139891175, "compression_ratio": 1.8701923076923077, "no_speech_prob": 5.507363766810158e-06}, {"id": 1595, "seek": 718856, "start": 7207.02, "end": 7212.700000000001, "text": " Because this is the stories that the algorithms are pushing.", "tokens": [1436, 341, 307, 264, 3676, 300, 264, 14642, 366, 7380, 13], "temperature": 0.0, "avg_logprob": -0.1288681139891175, "compression_ratio": 1.8701923076923077, "no_speech_prob": 5.507363766810158e-06}, {"id": 1596, "seek": 718856, "start": 7212.700000000001, "end": 7216.02, "text": " And the algorithms are pushing the stories because the algorithms are good.", "tokens": [400, 264, 14642, 366, 7380, 264, 3676, 570, 264, 14642, 366, 665, 13], "temperature": 0.0, "avg_logprob": -0.1288681139891175, "compression_ratio": 1.8701923076923077, "no_speech_prob": 5.507363766810158e-06}, {"id": 1597, "seek": 721602, "start": 7216.02, "end": 7221.580000000001, "text": " They know how to create eyeballs, how to get people watching, and how to get people clicking.", "tokens": [814, 458, 577, 281, 1884, 43758, 11, 577, 281, 483, 561, 1976, 11, 293, 577, 281, 483, 561, 9697, 13], "temperature": 0.0, "avg_logprob": -0.1889525946084555, "compression_ratio": 1.56, "no_speech_prob": 1.9946701286244206e-06}, {"id": 1598, "seek": 721602, "start": 7221.580000000001, "end": 7228.660000000001, "text": " And again, nobody at Facebook said, let's cause a massive genocide in Myanmar.", "tokens": [400, 797, 11, 5079, 412, 4384, 848, 11, 718, 311, 3082, 257, 5994, 31867, 294, 42725, 13], "temperature": 0.0, "avg_logprob": -0.1889525946084555, "compression_ratio": 1.56, "no_speech_prob": 1.9946701286244206e-06}, {"id": 1599, "seek": 721602, "start": 7228.660000000001, "end": 7237.06, "text": " They said, let's maximize the engagement of people in this new market on our platform.", "tokens": [814, 848, 11, 718, 311, 19874, 264, 8742, 295, 561, 294, 341, 777, 2142, 322, 527, 3663, 13], "temperature": 0.0, "avg_logprob": -0.1889525946084555, "compression_ratio": 1.56, "no_speech_prob": 1.9946701286244206e-06}, {"id": 1600, "seek": 721602, "start": 7237.06, "end": 7241.22, "text": " So they very successfully maximized engagement.", "tokens": [407, 436, 588, 10727, 5138, 1602, 8742, 13], "temperature": 0.0, "avg_logprob": -0.1889525946084555, "compression_ratio": 1.56, "no_speech_prob": 1.9946701286244206e-06}, {"id": 1601, "seek": 721602, "start": 7241.22, "end": 7243.22, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.1889525946084555, "compression_ratio": 1.56, "no_speech_prob": 1.9946701286244206e-06}, {"id": 1602, "seek": 724322, "start": 7243.22, "end": 7250.900000000001, "text": " It's just important to note, people warned executives at Facebook how the platform was", "tokens": [467, 311, 445, 1021, 281, 3637, 11, 561, 21284, 28485, 412, 4384, 577, 264, 3663, 390], "temperature": 0.0, "avg_logprob": -0.14234222509922126, "compression_ratio": 1.5560975609756098, "no_speech_prob": 5.384510586736724e-05}, {"id": 1603, "seek": 724322, "start": 7250.900000000001, "end": 7256.38, "text": " being used to incite violence as far back as 2013, 2014, 2015.", "tokens": [885, 1143, 281, 834, 642, 6270, 382, 1400, 646, 382, 9012, 11, 8227, 11, 7546, 13], "temperature": 0.0, "avg_logprob": -0.14234222509922126, "compression_ratio": 1.5560975609756098, "no_speech_prob": 5.384510586736724e-05}, {"id": 1604, "seek": 724322, "start": 7256.38, "end": 7261.820000000001, "text": " In 2015, someone even warned executives that Facebook could be used in Myanmar in the same", "tokens": [682, 7546, 11, 1580, 754, 21284, 28485, 300, 4384, 727, 312, 1143, 294, 42725, 294, 264, 912], "temperature": 0.0, "avg_logprob": -0.14234222509922126, "compression_ratio": 1.5560975609756098, "no_speech_prob": 5.384510586736724e-05}, {"id": 1605, "seek": 724322, "start": 7261.820000000001, "end": 7268.06, "text": " way that the radio broadcasts were used in Rwanda during the Rwandan genocide.", "tokens": [636, 300, 264, 6477, 9975, 82, 645, 1143, 294, 497, 86, 5575, 1830, 264, 497, 33114, 282, 31867, 13], "temperature": 0.0, "avg_logprob": -0.14234222509922126, "compression_ratio": 1.5560975609756098, "no_speech_prob": 5.384510586736724e-05}, {"id": 1606, "seek": 726806, "start": 7268.06, "end": 7275.620000000001, "text": " And as of 2015, Facebook only had four contractors who spoke Burmese working for them.", "tokens": [400, 382, 295, 7546, 11, 4384, 787, 632, 1451, 28377, 567, 7179, 7031, 76, 1130, 1364, 337, 552, 13], "temperature": 0.0, "avg_logprob": -0.1648881302939521, "compression_ratio": 1.427860696517413, "no_speech_prob": 9.816342753765639e-06}, {"id": 1607, "seek": 726806, "start": 7275.620000000001, "end": 7279.580000000001, "text": " They really did not put many resources into the issue at all, even though they were getting", "tokens": [814, 534, 630, 406, 829, 867, 3593, 666, 264, 2734, 412, 439, 11, 754, 1673, 436, 645, 1242], "temperature": 0.0, "avg_logprob": -0.1648881302939521, "compression_ratio": 1.427860696517413, "no_speech_prob": 9.816342753765639e-06}, {"id": 1608, "seek": 726806, "start": 7279.580000000001, "end": 7284.740000000001, "text": " very alarming warnings about it.", "tokens": [588, 44043, 30009, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.1648881302939521, "compression_ratio": 1.427860696517413, "no_speech_prob": 9.816342753765639e-06}, {"id": 1609, "seek": 726806, "start": 7284.740000000001, "end": 7288.02, "text": " So why does this happen?", "tokens": [407, 983, 775, 341, 1051, 30], "temperature": 0.0, "avg_logprob": -0.1648881302939521, "compression_ratio": 1.427860696517413, "no_speech_prob": 9.816342753765639e-06}, {"id": 1610, "seek": 726806, "start": 7288.02, "end": 7292.42, "text": " A part of the issue is that ethics is complicated.", "tokens": [316, 644, 295, 264, 2734, 307, 300, 19769, 307, 6179, 13], "temperature": 0.0, "avg_logprob": -0.1648881302939521, "compression_ratio": 1.427860696517413, "no_speech_prob": 9.816342753765639e-06}, {"id": 1611, "seek": 729242, "start": 7292.42, "end": 7300.78, "text": " And you will not find Rachel or I telling you how to do ethics, how do you fix this?", "tokens": [400, 291, 486, 406, 915, 14246, 420, 286, 3585, 291, 577, 281, 360, 19769, 11, 577, 360, 291, 3191, 341, 30], "temperature": 0.0, "avg_logprob": -0.10597328344980876, "compression_ratio": 1.7672413793103448, "no_speech_prob": 1.777544093783945e-05}, {"id": 1612, "seek": 729242, "start": 7300.78, "end": 7301.78, "text": " We don't know.", "tokens": [492, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.10597328344980876, "compression_ratio": 1.7672413793103448, "no_speech_prob": 1.777544093783945e-05}, {"id": 1613, "seek": 729242, "start": 7301.78, "end": 7305.54, "text": " We can just give you things to think about.", "tokens": [492, 393, 445, 976, 291, 721, 281, 519, 466, 13], "temperature": 0.0, "avg_logprob": -0.10597328344980876, "compression_ratio": 1.7672413793103448, "no_speech_prob": 1.777544093783945e-05}, {"id": 1614, "seek": 729242, "start": 7305.54, "end": 7310.3, "text": " Another part of the problem we keep hearing is, it's not my problem, I'm just a researcher,", "tokens": [3996, 644, 295, 264, 1154, 321, 1066, 4763, 307, 11, 309, 311, 406, 452, 1154, 11, 286, 478, 445, 257, 21751, 11], "temperature": 0.0, "avg_logprob": -0.10597328344980876, "compression_ratio": 1.7672413793103448, "no_speech_prob": 1.777544093783945e-05}, {"id": 1615, "seek": 729242, "start": 7310.3, "end": 7313.54, "text": " I'm just a techie, I'm just building a data set.", "tokens": [286, 478, 445, 257, 7553, 414, 11, 286, 478, 445, 2390, 257, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.10597328344980876, "compression_ratio": 1.7672413793103448, "no_speech_prob": 1.777544093783945e-05}, {"id": 1616, "seek": 729242, "start": 7313.54, "end": 7318.4800000000005, "text": " I'm not part of the problem, I'm part of this foundation that's far enough away that I can", "tokens": [286, 478, 406, 644, 295, 264, 1154, 11, 286, 478, 644, 295, 341, 7030, 300, 311, 1400, 1547, 1314, 300, 286, 393], "temperature": 0.0, "avg_logprob": -0.10597328344980876, "compression_ratio": 1.7672413793103448, "no_speech_prob": 1.777544093783945e-05}, {"id": 1617, "seek": 729242, "start": 7318.4800000000005, "end": 7320.9800000000005, "text": " imagine that I'm not part of this.", "tokens": [3811, 300, 286, 478, 406, 644, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.10597328344980876, "compression_ratio": 1.7672413793103448, "no_speech_prob": 1.777544093783945e-05}, {"id": 1618, "seek": 732098, "start": 7320.98, "end": 7327.78, "text": " But if you're creating ImageNet and you want it to be successful, you want lots of people", "tokens": [583, 498, 291, 434, 4084, 29903, 31890, 293, 291, 528, 309, 281, 312, 4406, 11, 291, 528, 3195, 295, 561], "temperature": 0.0, "avg_logprob": -0.09799264824908713, "compression_ratio": 1.965065502183406, "no_speech_prob": 3.5351604310562834e-05}, {"id": 1619, "seek": 732098, "start": 7327.78, "end": 7330.74, "text": " to use it, you want lots of people to build products on it, lots of people to do research", "tokens": [281, 764, 309, 11, 291, 528, 3195, 295, 561, 281, 1322, 3383, 322, 309, 11, 3195, 295, 561, 281, 360, 2132], "temperature": 0.0, "avg_logprob": -0.09799264824908713, "compression_ratio": 1.965065502183406, "no_speech_prob": 3.5351604310562834e-05}, {"id": 1620, "seek": 732098, "start": 7330.74, "end": 7337.179999999999, "text": " on top of it, if you're trying to create something that people are using, you want them to use,", "tokens": [322, 1192, 295, 309, 11, 498, 291, 434, 1382, 281, 1884, 746, 300, 561, 366, 1228, 11, 291, 528, 552, 281, 764, 11], "temperature": 0.0, "avg_logprob": -0.09799264824908713, "compression_ratio": 1.965065502183406, "no_speech_prob": 3.5351604310562834e-05}, {"id": 1621, "seek": 732098, "start": 7337.179999999999, "end": 7341.98, "text": " then please try to make it something that won't cause massive amounts of harm and doesn't", "tokens": [550, 1767, 853, 281, 652, 309, 746, 300, 1582, 380, 3082, 5994, 11663, 295, 6491, 293, 1177, 380], "temperature": 0.0, "avg_logprob": -0.09799264824908713, "compression_ratio": 1.965065502183406, "no_speech_prob": 3.5351604310562834e-05}, {"id": 1622, "seek": 732098, "start": 7341.98, "end": 7343.9, "text": " have massive amounts of bias.", "tokens": [362, 5994, 11663, 295, 12577, 13], "temperature": 0.0, "avg_logprob": -0.09799264824908713, "compression_ratio": 1.965065502183406, "no_speech_prob": 3.5351604310562834e-05}, {"id": 1623, "seek": 732098, "start": 7343.9, "end": 7347.62, "text": " And it can actually come back and bite you in the ass.", "tokens": [400, 309, 393, 767, 808, 646, 293, 7988, 291, 294, 264, 1256, 13], "temperature": 0.0, "avg_logprob": -0.09799264824908713, "compression_ratio": 1.965065502183406, "no_speech_prob": 3.5351604310562834e-05}, {"id": 1624, "seek": 734762, "start": 7347.62, "end": 7354.78, "text": " The Volkswagen engineer who ended up actually encoding the thing that made them systematically", "tokens": [440, 39856, 11403, 567, 4590, 493, 767, 43430, 264, 551, 300, 1027, 552, 39531], "temperature": 0.0, "avg_logprob": -0.10260784503110905, "compression_ratio": 1.92, "no_speech_prob": 6.048725026630564e-06}, {"id": 1625, "seek": 734762, "start": 7354.78, "end": 7361.58, "text": " cheat on their diesel emissions tests, on their pollution tests, ended up in jail.", "tokens": [17470, 322, 641, 21258, 14607, 6921, 11, 322, 641, 16727, 6921, 11, 4590, 493, 294, 10511, 13], "temperature": 0.0, "avg_logprob": -0.10260784503110905, "compression_ratio": 1.92, "no_speech_prob": 6.048725026630564e-06}, {"id": 1626, "seek": 734762, "start": 7361.58, "end": 7366.98, "text": " Not because it was their decision to cheat on the tests, but because their manager told", "tokens": [1726, 570, 309, 390, 641, 3537, 281, 17470, 322, 264, 6921, 11, 457, 570, 641, 6598, 1907], "temperature": 0.0, "avg_logprob": -0.10260784503110905, "compression_ratio": 1.92, "no_speech_prob": 6.048725026630564e-06}, {"id": 1627, "seek": 734762, "start": 7366.98, "end": 7370.62, "text": " them to write that code, and they wrote the code.", "tokens": [552, 281, 2464, 300, 3089, 11, 293, 436, 4114, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.10260784503110905, "compression_ratio": 1.92, "no_speech_prob": 6.048725026630564e-06}, {"id": 1628, "seek": 734762, "start": 7370.62, "end": 7374.78, "text": " And therefore they were the ones that ended up being criminally responsible and they were", "tokens": [400, 4412, 436, 645, 264, 2306, 300, 4590, 493, 885, 19044, 379, 6250, 293, 436, 645], "temperature": 0.0, "avg_logprob": -0.10260784503110905, "compression_ratio": 1.92, "no_speech_prob": 6.048725026630564e-06}, {"id": 1629, "seek": 734762, "start": 7374.78, "end": 7376.86, "text": " the ones that were jailed.", "tokens": [264, 2306, 300, 645, 10511, 292, 13], "temperature": 0.0, "avg_logprob": -0.10260784503110905, "compression_ratio": 1.92, "no_speech_prob": 6.048725026630564e-06}, {"id": 1630, "seek": 737686, "start": 7376.86, "end": 7384.08, "text": " So if you do in some way a shitty thing that ends up causing trouble, that can absolutely", "tokens": [407, 498, 291, 360, 294, 512, 636, 257, 30748, 551, 300, 5314, 493, 9853, 5253, 11, 300, 393, 3122], "temperature": 0.0, "avg_logprob": -0.10187906860023417, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.2377081424783682e-06}, {"id": 1631, "seek": 737686, "start": 7384.08, "end": 7388.62, "text": " come back around and get you in trouble as well.", "tokens": [808, 646, 926, 293, 483, 291, 294, 5253, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.10187906860023417, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.2377081424783682e-06}, {"id": 1632, "seek": 737686, "start": 7388.62, "end": 7391.139999999999, "text": " Sometimes it can cause huge amounts of trouble.", "tokens": [4803, 309, 393, 3082, 2603, 11663, 295, 5253, 13], "temperature": 0.0, "avg_logprob": -0.10187906860023417, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.2377081424783682e-06}, {"id": 1633, "seek": 737686, "start": 7391.139999999999, "end": 7398.139999999999, "text": " So if we go back to World War II, then this was one of the first great opportunities for", "tokens": [407, 498, 321, 352, 646, 281, 3937, 3630, 6351, 11, 550, 341, 390, 472, 295, 264, 700, 869, 4786, 337], "temperature": 0.0, "avg_logprob": -0.10187906860023417, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.2377081424783682e-06}, {"id": 1634, "seek": 737686, "start": 7398.139999999999, "end": 7402.94, "text": " IBM to show off their amazing tabulating system.", "tokens": [23487, 281, 855, 766, 641, 2243, 4421, 12162, 1185, 13], "temperature": 0.0, "avg_logprob": -0.10187906860023417, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.2377081424783682e-06}, {"id": 1635, "seek": 737686, "start": 7402.94, "end": 7406.219999999999, "text": " And they had a huge client in Nazi Germany.", "tokens": [400, 436, 632, 257, 2603, 6423, 294, 23592, 7244, 13], "temperature": 0.0, "avg_logprob": -0.10187906860023417, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.2377081424783682e-06}, {"id": 1636, "seek": 740622, "start": 7406.22, "end": 7411.780000000001, "text": " And Nazi Germany used this amazing new tabulating system to encode all of the different types", "tokens": [400, 23592, 7244, 1143, 341, 2243, 777, 4421, 12162, 1185, 281, 2058, 1429, 439, 295, 264, 819, 3467], "temperature": 0.0, "avg_logprob": -0.1583717371288099, "compression_ratio": 1.546875, "no_speech_prob": 2.546570794947911e-05}, {"id": 1637, "seek": 740622, "start": 7411.780000000001, "end": 7416.22, "text": " of Jews that they had in the country and all the different types of problem people.", "tokens": [295, 11041, 300, 436, 632, 294, 264, 1941, 293, 439, 264, 819, 3467, 295, 1154, 561, 13], "temperature": 0.0, "avg_logprob": -0.1583717371288099, "compression_ratio": 1.546875, "no_speech_prob": 2.546570794947911e-05}, {"id": 1638, "seek": 740622, "start": 7416.22, "end": 7423.66, "text": " So Jews were 8, gypsies were 12, then different outcomes were coded, executions were a 4,", "tokens": [407, 11041, 645, 1649, 11, 15823, 1878, 530, 645, 2272, 11, 550, 819, 10070, 645, 34874, 11, 4454, 3666, 645, 257, 1017, 11], "temperature": 0.0, "avg_logprob": -0.1583717371288099, "compression_ratio": 1.546875, "no_speech_prob": 2.546570794947911e-05}, {"id": 1639, "seek": 740622, "start": 7423.66, "end": 7426.860000000001, "text": " death in a gas chamber was 6.", "tokens": [2966, 294, 257, 4211, 13610, 390, 1386, 13], "temperature": 0.0, "avg_logprob": -0.1583717371288099, "compression_ratio": 1.546875, "no_speech_prob": 2.546570794947911e-05}, {"id": 1640, "seek": 742686, "start": 7426.86, "end": 7437.74, "text": " A Swiss judge ruled that IBM was actively involved facilitating the commission of these", "tokens": [316, 21965, 6995, 20077, 300, 23487, 390, 13022, 3288, 47558, 264, 9221, 295, 613], "temperature": 0.0, "avg_logprob": -0.11409241181832773, "compression_ratio": 1.4804469273743017, "no_speech_prob": 2.8129929887654725e-06}, {"id": 1641, "seek": 742686, "start": 7437.74, "end": 7440.139999999999, "text": " crimes against humanity.", "tokens": [13916, 1970, 10243, 13], "temperature": 0.0, "avg_logprob": -0.11409241181832773, "compression_ratio": 1.4804469273743017, "no_speech_prob": 2.8129929887654725e-06}, {"id": 1642, "seek": 742686, "start": 7440.139999999999, "end": 7448.219999999999, "text": " So there are absolutely plenty of examples of people building data processing technology", "tokens": [407, 456, 366, 3122, 7140, 295, 5110, 295, 561, 2390, 1412, 9007, 2899], "temperature": 0.0, "avg_logprob": -0.11409241181832773, "compression_ratio": 1.4804469273743017, "no_speech_prob": 2.8129929887654725e-06}, {"id": 1643, "seek": 742686, "start": 7448.219999999999, "end": 7453.339999999999, "text": " that are directly causing deaths.", "tokens": [300, 366, 3838, 9853, 13027, 13], "temperature": 0.0, "avg_logprob": -0.11409241181832773, "compression_ratio": 1.4804469273743017, "no_speech_prob": 2.8129929887654725e-06}, {"id": 1644, "seek": 742686, "start": 7453.339999999999, "end": 7455.82, "text": " Sometimes millions of deaths.", "tokens": [4803, 6803, 295, 13027, 13], "temperature": 0.0, "avg_logprob": -0.11409241181832773, "compression_ratio": 1.4804469273743017, "no_speech_prob": 2.8129929887654725e-06}, {"id": 1645, "seek": 745582, "start": 7455.82, "end": 7458.5, "text": " So we don't want to be one of those people.", "tokens": [407, 321, 500, 380, 528, 281, 312, 472, 295, 729, 561, 13], "temperature": 0.0, "avg_logprob": -0.1102503091096878, "compression_ratio": 1.8566037735849057, "no_speech_prob": 5.738285472034477e-05}, {"id": 1646, "seek": 745582, "start": 7458.5, "end": 7462.0199999999995, "text": " And so you might have thought, oh, you know, I'm just creating some data processing software", "tokens": [400, 370, 291, 1062, 362, 1194, 11, 1954, 11, 291, 458, 11, 286, 478, 445, 4084, 512, 1412, 9007, 4722], "temperature": 0.0, "avg_logprob": -0.1102503091096878, "compression_ratio": 1.8566037735849057, "no_speech_prob": 5.738285472034477e-05}, {"id": 1647, "seek": 745582, "start": 7462.0199999999995, "end": 7465.86, "text": " and somebody else is thinking I'm just a salesperson and somebody else is thinking I'm just the", "tokens": [293, 2618, 1646, 307, 1953, 286, 478, 445, 257, 5763, 10813, 293, 2618, 1646, 307, 1953, 286, 478, 445, 264], "temperature": 0.0, "avg_logprob": -0.1102503091096878, "compression_ratio": 1.8566037735849057, "no_speech_prob": 5.738285472034477e-05}, {"id": 1648, "seek": 745582, "start": 7465.86, "end": 7469.54, "text": " biz dev person opening new markets, but it all comes together.", "tokens": [7390, 1905, 954, 5193, 777, 8383, 11, 457, 309, 439, 1487, 1214, 13], "temperature": 0.0, "avg_logprob": -0.1102503091096878, "compression_ratio": 1.8566037735849057, "no_speech_prob": 5.738285472034477e-05}, {"id": 1649, "seek": 745582, "start": 7469.54, "end": 7472.9, "text": " So we need to care.", "tokens": [407, 321, 643, 281, 1127, 13], "temperature": 0.0, "avg_logprob": -0.1102503091096878, "compression_ratio": 1.8566037735849057, "no_speech_prob": 5.738285472034477e-05}, {"id": 1650, "seek": 745582, "start": 7472.9, "end": 7479.5199999999995, "text": " And so one of the things we need to care about is getting humans back in the loop.", "tokens": [400, 370, 472, 295, 264, 721, 321, 643, 281, 1127, 466, 307, 1242, 6255, 646, 294, 264, 6367, 13], "temperature": 0.0, "avg_logprob": -0.1102503091096878, "compression_ratio": 1.8566037735849057, "no_speech_prob": 5.738285472034477e-05}, {"id": 1651, "seek": 745582, "start": 7479.5199999999995, "end": 7484.62, "text": " And so when we pull humans out of the loop, it's one of the first times that trouble happens.", "tokens": [400, 370, 562, 321, 2235, 6255, 484, 295, 264, 6367, 11, 309, 311, 472, 295, 264, 700, 1413, 300, 5253, 2314, 13], "temperature": 0.0, "avg_logprob": -0.1102503091096878, "compression_ratio": 1.8566037735849057, "no_speech_prob": 5.738285472034477e-05}, {"id": 1652, "seek": 748462, "start": 7484.62, "end": 7490.0199999999995, "text": " I don't know if you remember, I remember this very clearly when I first heard that Facebook", "tokens": [286, 500, 380, 458, 498, 291, 1604, 11, 286, 1604, 341, 588, 4448, 562, 286, 700, 2198, 300, 4384], "temperature": 0.0, "avg_logprob": -0.13405491908391318, "compression_ratio": 1.6512605042016806, "no_speech_prob": 2.8856253265985288e-05}, {"id": 1653, "seek": 748462, "start": 7490.0199999999995, "end": 7497.42, "text": " was firing the human editors that were responsible for basically curating the news that ended", "tokens": [390, 16045, 264, 1952, 31446, 300, 645, 6250, 337, 1936, 1262, 990, 264, 2583, 300, 4590], "temperature": 0.0, "avg_logprob": -0.13405491908391318, "compression_ratio": 1.6512605042016806, "no_speech_prob": 2.8856253265985288e-05}, {"id": 1654, "seek": 748462, "start": 7497.42, "end": 7501.38, "text": " up on the Facebook pages.", "tokens": [493, 322, 264, 4384, 7183, 13], "temperature": 0.0, "avg_logprob": -0.13405491908391318, "compression_ratio": 1.6512605042016806, "no_speech_prob": 2.8856253265985288e-05}, {"id": 1655, "seek": 748462, "start": 7501.38, "end": 7505.86, "text": " And I've got to say at the time I thought that's a recipe for disaster because I've", "tokens": [400, 286, 600, 658, 281, 584, 412, 264, 565, 286, 1194, 300, 311, 257, 6782, 337, 11293, 570, 286, 600], "temperature": 0.0, "avg_logprob": -0.13405491908391318, "compression_ratio": 1.6512605042016806, "no_speech_prob": 2.8856253265985288e-05}, {"id": 1656, "seek": 748462, "start": 7505.86, "end": 7512.14, "text": " seen again and again that humans can be the person in the loop that can realize this isn't", "tokens": [1612, 797, 293, 797, 300, 6255, 393, 312, 264, 954, 294, 264, 6367, 300, 393, 4325, 341, 1943, 380], "temperature": 0.0, "avg_logprob": -0.13405491908391318, "compression_ratio": 1.6512605042016806, "no_speech_prob": 2.8856253265985288e-05}, {"id": 1657, "seek": 748462, "start": 7512.14, "end": 7513.14, "text": " right.", "tokens": [558, 13], "temperature": 0.0, "avg_logprob": -0.13405491908391318, "compression_ratio": 1.6512605042016806, "no_speech_prob": 2.8856253265985288e-05}, {"id": 1658, "seek": 751314, "start": 7513.14, "end": 7518.860000000001, "text": " You know, it's very hard to create an algorithm that can recognize this isn't right or else", "tokens": [509, 458, 11, 309, 311, 588, 1152, 281, 1884, 364, 9284, 300, 393, 5521, 341, 1943, 380, 558, 420, 1646], "temperature": 0.0, "avg_logprob": -0.15695525082674894, "compression_ratio": 1.7171717171717171, "no_speech_prob": 3.5007917631446617e-06}, {"id": 1659, "seek": 751314, "start": 7518.860000000001, "end": 7520.22, "text": " humans are very good at that.", "tokens": [6255, 366, 588, 665, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.15695525082674894, "compression_ratio": 1.7171717171717171, "no_speech_prob": 3.5007917631446617e-06}, {"id": 1660, "seek": 751314, "start": 7520.22, "end": 7521.700000000001, "text": " And we saw that's what happened.", "tokens": [400, 321, 1866, 300, 311, 437, 2011, 13], "temperature": 0.0, "avg_logprob": -0.15695525082674894, "compression_ratio": 1.7171717171717171, "no_speech_prob": 3.5007917631446617e-06}, {"id": 1661, "seek": 751314, "start": 7521.700000000001, "end": 7527.5, "text": " Right after Facebook fired the human editors, the nature of stories on Facebook dramatically", "tokens": [1779, 934, 4384, 11777, 264, 1952, 31446, 11, 264, 3687, 295, 3676, 322, 4384, 17548], "temperature": 0.0, "avg_logprob": -0.15695525082674894, "compression_ratio": 1.7171717171717171, "no_speech_prob": 3.5007917631446617e-06}, {"id": 1662, "seek": 751314, "start": 7527.5, "end": 7533.06, "text": " changed that you started seeing this proliferation of conspiracy theories and the kind of the", "tokens": [3105, 300, 291, 1409, 2577, 341, 24398, 44987, 295, 20439, 13667, 293, 264, 733, 295, 264], "temperature": 0.0, "avg_logprob": -0.15695525082674894, "compression_ratio": 1.7171717171717171, "no_speech_prob": 3.5007917631446617e-06}, {"id": 1663, "seek": 751314, "start": 7533.06, "end": 7537.780000000001, "text": " algorithms went crazy with recommending more and more controversial topics.", "tokens": [14642, 1437, 3219, 365, 30559, 544, 293, 544, 17323, 8378, 13], "temperature": 0.0, "avg_logprob": -0.15695525082674894, "compression_ratio": 1.7171717171717171, "no_speech_prob": 3.5007917631446617e-06}, {"id": 1664, "seek": 751314, "start": 7537.780000000001, "end": 7542.58, "text": " And of course that changed people's consumption behavior, causing them to want more and more", "tokens": [400, 295, 1164, 300, 3105, 561, 311, 12126, 5223, 11, 9853, 552, 281, 528, 544, 293, 544], "temperature": 0.0, "avg_logprob": -0.15695525082674894, "compression_ratio": 1.7171717171717171, "no_speech_prob": 3.5007917631446617e-06}, {"id": 1665, "seek": 754258, "start": 7542.58, "end": 7546.42, "text": " controversial topics.", "tokens": [17323, 8378, 13], "temperature": 0.0, "avg_logprob": -0.15404611610504518, "compression_ratio": 1.4736842105263157, "no_speech_prob": 1.2606719792529475e-05}, {"id": 1666, "seek": 754258, "start": 7546.42, "end": 7551.7, "text": " So one of the really interesting places this comes in, and Cathy O'Neill who's got a great", "tokens": [407, 472, 295, 264, 534, 1880, 3190, 341, 1487, 294, 11, 293, 39799, 422, 6, 15496, 373, 567, 311, 658, 257, 869], "temperature": 0.0, "avg_logprob": -0.15404611610504518, "compression_ratio": 1.4736842105263157, "no_speech_prob": 1.2606719792529475e-05}, {"id": 1667, "seek": 754258, "start": 7551.7, "end": 7560.1, "text": " book called Weapons of Math Destruction, thank you Rachel, and many others have pointed out,", "tokens": [1446, 1219, 492, 48071, 295, 15776, 16339, 3826, 11, 1309, 291, 14246, 11, 293, 867, 2357, 362, 10932, 484, 11], "temperature": 0.0, "avg_logprob": -0.15404611610504518, "compression_ratio": 1.4736842105263157, "no_speech_prob": 1.2606719792529475e-05}, {"id": 1668, "seek": 754258, "start": 7560.1, "end": 7567.74, "text": " is that what happens to algorithms is that they end up impacting people.", "tokens": [307, 300, 437, 2314, 281, 14642, 307, 300, 436, 917, 493, 29963, 561, 13], "temperature": 0.0, "avg_logprob": -0.15404611610504518, "compression_ratio": 1.4736842105263157, "no_speech_prob": 1.2606719792529475e-05}, {"id": 1669, "seek": 754258, "start": 7567.74, "end": 7572.22, "text": " For example, Compass sentencing guidelines go to a judge.", "tokens": [1171, 1365, 11, 50179, 2279, 13644, 12470, 352, 281, 257, 6995, 13], "temperature": 0.0, "avg_logprob": -0.15404611610504518, "compression_ratio": 1.4736842105263157, "no_speech_prob": 1.2606719792529475e-05}, {"id": 1670, "seek": 757222, "start": 7572.22, "end": 7578.3, "text": " Now you can say the algorithm is very good, in Compass's case it isn't, it actually turned", "tokens": [823, 291, 393, 584, 264, 9284, 307, 588, 665, 11, 294, 50179, 311, 1389, 309, 1943, 380, 11, 309, 767, 3574], "temperature": 0.0, "avg_logprob": -0.17086618596857245, "compression_ratio": 1.676056338028169, "no_speech_prob": 1.8631470084073953e-05}, {"id": 1671, "seek": 757222, "start": 7578.3, "end": 7583.38, "text": " out to be about as bad as random because it's a black box and all that.", "tokens": [484, 281, 312, 466, 382, 1578, 382, 4974, 570, 309, 311, 257, 2211, 2424, 293, 439, 300, 13], "temperature": 0.0, "avg_logprob": -0.17086618596857245, "compression_ratio": 1.676056338028169, "no_speech_prob": 1.8631470084073953e-05}, {"id": 1672, "seek": 757222, "start": 7583.38, "end": 7587.58, "text": " But even if it was very good, you could then say, well, you know, the judge is getting", "tokens": [583, 754, 498, 309, 390, 588, 665, 11, 291, 727, 550, 584, 11, 731, 11, 291, 458, 11, 264, 6995, 307, 1242], "temperature": 0.0, "avg_logprob": -0.17086618596857245, "compression_ratio": 1.676056338028169, "no_speech_prob": 1.8631470084073953e-05}, {"id": 1673, "seek": 757222, "start": 7587.58, "end": 7592.7, "text": " the algorithm, otherwise they'd just be getting a person, people also give bad advice, so", "tokens": [264, 9284, 11, 5911, 436, 1116, 445, 312, 1242, 257, 954, 11, 561, 611, 976, 1578, 5192, 11, 370], "temperature": 0.0, "avg_logprob": -0.17086618596857245, "compression_ratio": 1.676056338028169, "no_speech_prob": 1.8631470084073953e-05}, {"id": 1674, "seek": 757222, "start": 7592.7, "end": 7593.9800000000005, "text": " what?", "tokens": [437, 30], "temperature": 0.0, "avg_logprob": -0.17086618596857245, "compression_ratio": 1.676056338028169, "no_speech_prob": 1.8631470084073953e-05}, {"id": 1675, "seek": 757222, "start": 7593.9800000000005, "end": 7596.58, "text": " Humans respond differently to algorithms.", "tokens": [35809, 4196, 7614, 281, 14642, 13], "temperature": 0.0, "avg_logprob": -0.17086618596857245, "compression_ratio": 1.676056338028169, "no_speech_prob": 1.8631470084073953e-05}, {"id": 1676, "seek": 757222, "start": 7596.58, "end": 7601.1, "text": " It's very common, particularly for a human that is not very familiar with the technology", "tokens": [467, 311, 588, 2689, 11, 4098, 337, 257, 1952, 300, 307, 406, 588, 4963, 365, 264, 2899], "temperature": 0.0, "avg_logprob": -0.17086618596857245, "compression_ratio": 1.676056338028169, "no_speech_prob": 1.8631470084073953e-05}, {"id": 1677, "seek": 760110, "start": 7601.1, "end": 7606.620000000001, "text": " themselves, like a judge, to see like, oh, that's what the computer says.", "tokens": [2969, 11, 411, 257, 6995, 11, 281, 536, 411, 11, 1954, 11, 300, 311, 437, 264, 3820, 1619, 13], "temperature": 0.0, "avg_logprob": -0.10505022785880348, "compression_ratio": 1.6018099547511313, "no_speech_prob": 9.516137652099133e-06}, {"id": 1678, "seek": 760110, "start": 7606.620000000001, "end": 7610.18, "text": " The computer looked it up and it figured this out.", "tokens": [440, 3820, 2956, 309, 493, 293, 309, 8932, 341, 484, 13], "temperature": 0.0, "avg_logprob": -0.10505022785880348, "compression_ratio": 1.6018099547511313, "no_speech_prob": 9.516137652099133e-06}, {"id": 1679, "seek": 760110, "start": 7610.18, "end": 7617.02, "text": " It's extremely difficult to get a non-technical audience to look at a computer recommendation", "tokens": [467, 311, 4664, 2252, 281, 483, 257, 2107, 12, 29113, 804, 4034, 281, 574, 412, 257, 3820, 11879], "temperature": 0.0, "avg_logprob": -0.10505022785880348, "compression_ratio": 1.6018099547511313, "no_speech_prob": 9.516137652099133e-06}, {"id": 1680, "seek": 760110, "start": 7617.02, "end": 7621.700000000001, "text": " and come up with a nuanced decision-making process.", "tokens": [293, 808, 493, 365, 257, 45115, 3537, 12, 12402, 1399, 13], "temperature": 0.0, "avg_logprob": -0.10505022785880348, "compression_ratio": 1.6018099547511313, "no_speech_prob": 9.516137652099133e-06}, {"id": 1681, "seek": 760110, "start": 7621.700000000001, "end": 7628.5, "text": " So what we see is that algorithms are often put into place with no appeals process.", "tokens": [407, 437, 321, 536, 307, 300, 14642, 366, 2049, 829, 666, 1081, 365, 572, 32603, 1399, 13], "temperature": 0.0, "avg_logprob": -0.10505022785880348, "compression_ratio": 1.6018099547511313, "no_speech_prob": 9.516137652099133e-06}, {"id": 1682, "seek": 762850, "start": 7628.5, "end": 7635.7, "text": " They're often used to massively scale up decision-making systems because they're cheap.", "tokens": [814, 434, 2049, 1143, 281, 29379, 4373, 493, 3537, 12, 12402, 3652, 570, 436, 434, 7084, 13], "temperature": 0.0, "avg_logprob": -0.089828003777398, "compression_ratio": 1.6735537190082646, "no_speech_prob": 5.955067081231391e-06}, {"id": 1683, "seek": 762850, "start": 7635.7, "end": 7639.62, "text": " And then the people that are using the outputs of those algorithms tend to give them more", "tokens": [400, 550, 264, 561, 300, 366, 1228, 264, 23930, 295, 729, 14642, 3928, 281, 976, 552, 544], "temperature": 0.0, "avg_logprob": -0.089828003777398, "compression_ratio": 1.6735537190082646, "no_speech_prob": 5.955067081231391e-06}, {"id": 1684, "seek": 762850, "start": 7639.62, "end": 7643.86, "text": " credence than they deserve because very often they're being used by people that don't have", "tokens": [3864, 655, 813, 436, 9948, 570, 588, 2049, 436, 434, 885, 1143, 538, 561, 300, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.089828003777398, "compression_ratio": 1.6735537190082646, "no_speech_prob": 5.955067081231391e-06}, {"id": 1685, "seek": 762850, "start": 7643.86, "end": 7646.94, "text": " the technical competence to judge them themselves.", "tokens": [264, 6191, 39965, 281, 6995, 552, 2969, 13], "temperature": 0.0, "avg_logprob": -0.089828003777398, "compression_ratio": 1.6735537190082646, "no_speech_prob": 5.955067081231391e-06}, {"id": 1686, "seek": 762850, "start": 7646.94, "end": 7654.14, "text": " So great example, right, was here's an example of somebody who lost their healthcare.", "tokens": [407, 869, 1365, 11, 558, 11, 390, 510, 311, 364, 1365, 295, 2618, 567, 2731, 641, 8884, 13], "temperature": 0.0, "avg_logprob": -0.089828003777398, "compression_ratio": 1.6735537190082646, "no_speech_prob": 5.955067081231391e-06}, {"id": 1687, "seek": 765414, "start": 7654.14, "end": 7661.22, "text": " And they lost their healthcare because of an error in a new algorithm that was systematically", "tokens": [400, 436, 2731, 641, 8884, 570, 295, 364, 6713, 294, 257, 777, 9284, 300, 390, 39531], "temperature": 0.0, "avg_logprob": -0.12500438690185547, "compression_ratio": 1.639676113360324, "no_speech_prob": 2.1233604456938338e-06}, {"id": 1688, "seek": 765414, "start": 7661.22, "end": 7669.5, "text": " failing to recognize that there are many people that need help with, was it Alzheimer's?", "tokens": [18223, 281, 5521, 300, 456, 366, 867, 561, 300, 643, 854, 365, 11, 390, 309, 27932, 311, 30], "temperature": 0.0, "avg_logprob": -0.12500438690185547, "compression_ratio": 1.639676113360324, "no_speech_prob": 2.1233604456938338e-06}, {"id": 1689, "seek": 765414, "start": 7669.5, "end": 7671.14, "text": " Cerebral palsy and diabetes.", "tokens": [383, 323, 32728, 43806, 88, 293, 13881, 13], "temperature": 0.0, "avg_logprob": -0.12500438690185547, "compression_ratio": 1.639676113360324, "no_speech_prob": 2.1233604456938338e-06}, {"id": 1690, "seek": 765414, "start": 7671.14, "end": 7673.18, "text": " Thanks, Rachel.", "tokens": [2561, 11, 14246, 13], "temperature": 0.0, "avg_logprob": -0.12500438690185547, "compression_ratio": 1.639676113360324, "no_speech_prob": 2.1233604456938338e-06}, {"id": 1691, "seek": 765414, "start": 7673.18, "end": 7678.660000000001, "text": " And so this system, which had this error that was later discovered, was cutting off these", "tokens": [400, 370, 341, 1185, 11, 597, 632, 341, 6713, 300, 390, 1780, 6941, 11, 390, 6492, 766, 613], "temperature": 0.0, "avg_logprob": -0.12500438690185547, "compression_ratio": 1.639676113360324, "no_speech_prob": 2.1233604456938338e-06}, {"id": 1692, "seek": 765414, "start": 7678.660000000001, "end": 7683.900000000001, "text": " people from the home care that they needed so that cerebral palsy victims no longer had", "tokens": [561, 490, 264, 1280, 1127, 300, 436, 2978, 370, 300, 43561, 43806, 88, 11448, 572, 2854, 632], "temperature": 0.0, "avg_logprob": -0.12500438690185547, "compression_ratio": 1.639676113360324, "no_speech_prob": 2.1233604456938338e-06}, {"id": 1693, "seek": 768390, "start": 7683.9, "end": 7684.9, "text": " the care they needed.", "tokens": [264, 1127, 436, 2978, 13], "temperature": 0.0, "avg_logprob": -0.1342572593688965, "compression_ratio": 1.6694915254237288, "no_speech_prob": 3.966960775869666e-06}, {"id": 1694, "seek": 768390, "start": 7684.9, "end": 7689.7, "text": " So their life was destroyed, basically.", "tokens": [407, 641, 993, 390, 8937, 11, 1936, 13], "temperature": 0.0, "avg_logprob": -0.1342572593688965, "compression_ratio": 1.6694915254237288, "no_speech_prob": 3.966960775869666e-06}, {"id": 1695, "seek": 768390, "start": 7689.7, "end": 7695.339999999999, "text": " And so when the person that created that algorithm with the error was asked about this and was", "tokens": [400, 370, 562, 264, 954, 300, 2942, 300, 9284, 365, 264, 6713, 390, 2351, 466, 341, 293, 390], "temperature": 0.0, "avg_logprob": -0.1342572593688965, "compression_ratio": 1.6694915254237288, "no_speech_prob": 3.966960775869666e-06}, {"id": 1696, "seek": 768390, "start": 7695.339999999999, "end": 7702.099999999999, "text": " specifically said, should they have found a better way to communicate the system, the", "tokens": [4682, 848, 11, 820, 436, 362, 1352, 257, 1101, 636, 281, 7890, 264, 1185, 11, 264], "temperature": 0.0, "avg_logprob": -0.1342572593688965, "compression_ratio": 1.6694915254237288, "no_speech_prob": 3.966960775869666e-06}, {"id": 1697, "seek": 768390, "start": 7702.099999999999, "end": 7706.62, "text": " strengths, the failures and so forth, he said, yeah, I should probably also dust under my", "tokens": [16986, 11, 264, 20774, 293, 370, 5220, 11, 415, 848, 11, 1338, 11, 286, 820, 1391, 611, 8634, 833, 452], "temperature": 0.0, "avg_logprob": -0.1342572593688965, "compression_ratio": 1.6694915254237288, "no_speech_prob": 3.966960775869666e-06}, {"id": 1698, "seek": 768390, "start": 7706.62, "end": 7708.219999999999, "text": " bed.", "tokens": [2901, 13], "temperature": 0.0, "avg_logprob": -0.1342572593688965, "compression_ratio": 1.6694915254237288, "no_speech_prob": 3.966960775869666e-06}, {"id": 1699, "seek": 768390, "start": 7708.219999999999, "end": 7709.44, "text": " That was there.", "tokens": [663, 390, 456, 13], "temperature": 0.0, "avg_logprob": -0.1342572593688965, "compression_ratio": 1.6694915254237288, "no_speech_prob": 3.966960775869666e-06}, {"id": 1700, "seek": 768390, "start": 7709.44, "end": 7711.679999999999, "text": " That was the level of interest they had.", "tokens": [663, 390, 264, 1496, 295, 1179, 436, 632, 13], "temperature": 0.0, "avg_logprob": -0.1342572593688965, "compression_ratio": 1.6694915254237288, "no_speech_prob": 3.966960775869666e-06}, {"id": 1701, "seek": 771168, "start": 7711.68, "end": 7714.42, "text": " And this is extremely common.", "tokens": [400, 341, 307, 4664, 2689, 13], "temperature": 0.0, "avg_logprob": -0.156433427837533, "compression_ratio": 1.726384364820847, "no_speech_prob": 1.3845394278177992e-05}, {"id": 1702, "seek": 771168, "start": 7714.42, "end": 7716.02, "text": " I hear this all the time.", "tokens": [286, 1568, 341, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.156433427837533, "compression_ratio": 1.726384364820847, "no_speech_prob": 1.3845394278177992e-05}, {"id": 1703, "seek": 771168, "start": 7716.02, "end": 7720.54, "text": " And it's much easier to kind of see it from afar and say, okay, after the problems happened,", "tokens": [400, 309, 311, 709, 3571, 281, 733, 295, 536, 309, 490, 41795, 293, 584, 11, 1392, 11, 934, 264, 2740, 2011, 11], "temperature": 0.0, "avg_logprob": -0.156433427837533, "compression_ratio": 1.726384364820847, "no_speech_prob": 1.3845394278177992e-05}, {"id": 1704, "seek": 771168, "start": 7720.54, "end": 7722.780000000001, "text": " I can see that that's a really shitty thing to say.", "tokens": [286, 393, 536, 300, 300, 311, 257, 534, 30748, 551, 281, 584, 13], "temperature": 0.0, "avg_logprob": -0.156433427837533, "compression_ratio": 1.726384364820847, "no_speech_prob": 1.3845394278177992e-05}, {"id": 1705, "seek": 771168, "start": 7722.780000000001, "end": 7727.42, "text": " But it can be very difficult when you're kind of in the middle of it.", "tokens": [583, 309, 393, 312, 588, 2252, 562, 291, 434, 733, 295, 294, 264, 2808, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.156433427837533, "compression_ratio": 1.726384364820847, "no_speech_prob": 1.3845394278177992e-05}, {"id": 1706, "seek": 771168, "start": 7727.42, "end": 7731.1, "text": " I just want to say one more thing about that example.", "tokens": [286, 445, 528, 281, 584, 472, 544, 551, 466, 300, 1365, 13], "temperature": 0.0, "avg_logprob": -0.156433427837533, "compression_ratio": 1.726384364820847, "no_speech_prob": 1.3845394278177992e-05}, {"id": 1707, "seek": 771168, "start": 7731.1, "end": 7733.62, "text": " And that's that this was a case where it was separate.", "tokens": [400, 300, 311, 300, 341, 390, 257, 1389, 689, 309, 390, 4994, 13], "temperature": 0.0, "avg_logprob": -0.156433427837533, "compression_ratio": 1.726384364820847, "no_speech_prob": 1.3845394278177992e-05}, {"id": 1708, "seek": 771168, "start": 7733.62, "end": 7736.14, "text": " There was someone who created the algorithm.", "tokens": [821, 390, 1580, 567, 2942, 264, 9284, 13], "temperature": 0.0, "avg_logprob": -0.156433427837533, "compression_ratio": 1.726384364820847, "no_speech_prob": 1.3845394278177992e-05}, {"id": 1709, "seek": 771168, "start": 7736.14, "end": 7738.46, "text": " Then I think different people implemented the software.", "tokens": [1396, 286, 519, 819, 561, 12270, 264, 4722, 13], "temperature": 0.0, "avg_logprob": -0.156433427837533, "compression_ratio": 1.726384364820847, "no_speech_prob": 1.3845394278177992e-05}, {"id": 1710, "seek": 771168, "start": 7738.46, "end": 7740.9800000000005, "text": " And this is in use in over half of the 50 states.", "tokens": [400, 341, 307, 294, 764, 294, 670, 1922, 295, 264, 2625, 4368, 13], "temperature": 0.0, "avg_logprob": -0.156433427837533, "compression_ratio": 1.726384364820847, "no_speech_prob": 1.3845394278177992e-05}, {"id": 1711, "seek": 774098, "start": 7740.98, "end": 7745.219999999999, "text": " And then there was also the particular policy decisions made by that state.", "tokens": [400, 550, 456, 390, 611, 264, 1729, 3897, 5327, 1027, 538, 300, 1785, 13], "temperature": 0.0, "avg_logprob": -0.13494094812645102, "compression_ratio": 1.7950819672131149, "no_speech_prob": 1.2804391190002207e-05}, {"id": 1712, "seek": 774098, "start": 7745.219999999999, "end": 7749.0599999999995, "text": " And so this is one of those situations where nobody felt responsible because the algorithm", "tokens": [400, 370, 341, 307, 472, 295, 729, 6851, 689, 5079, 2762, 6250, 570, 264, 9284], "temperature": 0.0, "avg_logprob": -0.13494094812645102, "compression_ratio": 1.7950819672131149, "no_speech_prob": 1.2804391190002207e-05}, {"id": 1713, "seek": 774098, "start": 7749.0599999999995, "end": 7753.86, "text": " creators like, oh, no, it's the policy decisions of the state that were bad.", "tokens": [16039, 411, 11, 1954, 11, 572, 11, 309, 311, 264, 3897, 5327, 295, 264, 1785, 300, 645, 1578, 13], "temperature": 0.0, "avg_logprob": -0.13494094812645102, "compression_ratio": 1.7950819672131149, "no_speech_prob": 1.2804391190002207e-05}, {"id": 1714, "seek": 774098, "start": 7753.86, "end": 7758.0199999999995, "text": " And the state can be like, oh, no, it's the ones who implemented the software.", "tokens": [400, 264, 1785, 393, 312, 411, 11, 1954, 11, 572, 11, 309, 311, 264, 2306, 567, 12270, 264, 4722, 13], "temperature": 0.0, "avg_logprob": -0.13494094812645102, "compression_ratio": 1.7950819672131149, "no_speech_prob": 1.2804391190002207e-05}, {"id": 1715, "seek": 774098, "start": 7758.0199999999995, "end": 7763.339999999999, "text": " And so everyone's just kind of pointing fingers and not taking responsibility.", "tokens": [400, 370, 1518, 311, 445, 733, 295, 12166, 7350, 293, 406, 1940, 6357, 13], "temperature": 0.0, "avg_logprob": -0.13494094812645102, "compression_ratio": 1.7950819672131149, "no_speech_prob": 1.2804391190002207e-05}, {"id": 1716, "seek": 774098, "start": 7763.339999999999, "end": 7765.419999999999, "text": " And in some ways, maybe it's unfair.", "tokens": [400, 294, 512, 2098, 11, 1310, 309, 311, 17019, 13], "temperature": 0.0, "avg_logprob": -0.13494094812645102, "compression_ratio": 1.7950819672131149, "no_speech_prob": 1.2804391190002207e-05}, {"id": 1717, "seek": 776542, "start": 7765.42, "end": 7771.42, "text": " But I would argue the person who is creating the data set and the person who is implementing", "tokens": [583, 286, 576, 9695, 264, 954, 567, 307, 4084, 264, 1412, 992, 293, 264, 954, 567, 307, 18114], "temperature": 0.0, "avg_logprob": -0.12395486745748434, "compression_ratio": 1.684981684981685, "no_speech_prob": 6.540265985677252e-06}, {"id": 1718, "seek": 776542, "start": 7771.42, "end": 7778.18, "text": " the algorithm is the person best placed to get out there and say, hey, here are the things", "tokens": [264, 9284, 307, 264, 954, 1151, 7074, 281, 483, 484, 456, 293, 584, 11, 4177, 11, 510, 366, 264, 721], "temperature": 0.0, "avg_logprob": -0.12395486745748434, "compression_ratio": 1.684981684981685, "no_speech_prob": 6.540265985677252e-06}, {"id": 1719, "seek": 776542, "start": 7778.18, "end": 7784.4800000000005, "text": " you need to be careful of and make sure that they're a part of the implementation process.", "tokens": [291, 643, 281, 312, 5026, 295, 293, 652, 988, 300, 436, 434, 257, 644, 295, 264, 11420, 1399, 13], "temperature": 0.0, "avg_logprob": -0.12395486745748434, "compression_ratio": 1.684981684981685, "no_speech_prob": 6.540265985677252e-06}, {"id": 1720, "seek": 776542, "start": 7784.4800000000005, "end": 7787.06, "text": " So we've also seen this with YouTube, right?", "tokens": [407, 321, 600, 611, 1612, 341, 365, 3088, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12395486745748434, "compression_ratio": 1.684981684981685, "no_speech_prob": 6.540265985677252e-06}, {"id": 1721, "seek": 776542, "start": 7787.06, "end": 7789.26, "text": " It's kind of similar to what happened with Facebook.", "tokens": [467, 311, 733, 295, 2531, 281, 437, 2011, 365, 4384, 13], "temperature": 0.0, "avg_logprob": -0.12395486745748434, "compression_ratio": 1.684981684981685, "no_speech_prob": 6.540265985677252e-06}, {"id": 1722, "seek": 776542, "start": 7789.26, "end": 7794.62, "text": " And we're now seeing, we've heard examples of students watching the Fast AI courses who", "tokens": [400, 321, 434, 586, 2577, 11, 321, 600, 2198, 5110, 295, 1731, 1976, 264, 15968, 7318, 7712, 567], "temperature": 0.0, "avg_logprob": -0.12395486745748434, "compression_ratio": 1.684981684981685, "no_speech_prob": 6.540265985677252e-06}, {"id": 1723, "seek": 779462, "start": 7794.62, "end": 7799.14, "text": " say, hey, Jeremy and Rachel watching the Fast AI courses really enjoyed them.", "tokens": [584, 11, 4177, 11, 17809, 293, 14246, 1976, 264, 15968, 7318, 7712, 534, 4626, 552, 13], "temperature": 0.0, "avg_logprob": -0.1213676041247798, "compression_ratio": 1.7130801687763713, "no_speech_prob": 1.4063442904443946e-05}, {"id": 1724, "seek": 779462, "start": 7799.14, "end": 7806.34, "text": " And at the end of one of them, the YouTube autoplay fed me across to a conspiracy theory.", "tokens": [400, 412, 264, 917, 295, 472, 295, 552, 11, 264, 3088, 31090, 8376, 4636, 385, 2108, 281, 257, 20439, 5261, 13], "temperature": 0.0, "avg_logprob": -0.1213676041247798, "compression_ratio": 1.7130801687763713, "no_speech_prob": 1.4063442904443946e-05}, {"id": 1725, "seek": 779462, "start": 7806.34, "end": 7811.26, "text": " And what happens is that once the system decides that you like the conspiracy theories, it's", "tokens": [400, 437, 2314, 307, 300, 1564, 264, 1185, 14898, 300, 291, 411, 264, 20439, 13667, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.1213676041247798, "compression_ratio": 1.7130801687763713, "no_speech_prob": 1.4063442904443946e-05}, {"id": 1726, "seek": 779462, "start": 7811.26, "end": 7813.3, "text": " going to just feed you more and more.", "tokens": [516, 281, 445, 3154, 291, 544, 293, 544, 13], "temperature": 0.0, "avg_logprob": -0.1213676041247798, "compression_ratio": 1.7130801687763713, "no_speech_prob": 1.4063442904443946e-05}, {"id": 1727, "seek": 779462, "start": 7813.3, "end": 7818.7, "text": " And then what happens is that, please go on.", "tokens": [400, 550, 437, 2314, 307, 300, 11, 1767, 352, 322, 13], "temperature": 0.0, "avg_logprob": -0.1213676041247798, "compression_ratio": 1.7130801687763713, "no_speech_prob": 1.4063442904443946e-05}, {"id": 1728, "seek": 779462, "start": 7818.7, "end": 7821.98, "text": " Just briefly, you don't even have to like conspiracy theories.", "tokens": [1449, 10515, 11, 291, 500, 380, 754, 362, 281, 411, 20439, 13667, 13], "temperature": 0.0, "avg_logprob": -0.1213676041247798, "compression_ratio": 1.7130801687763713, "no_speech_prob": 1.4063442904443946e-05}, {"id": 1729, "seek": 782198, "start": 7821.98, "end": 7826.099999999999, "text": " The goal is to get as many people hooked on conspiracy theories as possible, is what the", "tokens": [440, 3387, 307, 281, 483, 382, 867, 561, 20410, 322, 20439, 13667, 382, 1944, 11, 307, 437, 264], "temperature": 0.0, "avg_logprob": -0.15091686406411414, "compression_ratio": 1.7703180212014133, "no_speech_prob": 9.368382961838506e-06}, {"id": 1730, "seek": 782198, "start": 7826.099999999999, "end": 7830.0599999999995, "text": " algorithm is trying to do, kind of whether or not you've expressed interest.", "tokens": [9284, 307, 1382, 281, 360, 11, 733, 295, 1968, 420, 406, 291, 600, 12675, 1179, 13], "temperature": 0.0, "avg_logprob": -0.15091686406411414, "compression_ratio": 1.7703180212014133, "no_speech_prob": 9.368382961838506e-06}, {"id": 1731, "seek": 782198, "start": 7830.0599999999995, "end": 7831.0599999999995, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.15091686406411414, "compression_ratio": 1.7703180212014133, "no_speech_prob": 9.368382961838506e-06}, {"id": 1732, "seek": 782198, "start": 7831.0599999999995, "end": 7834.219999999999, "text": " And so the interesting thing, again, is I know plenty of people involved in YouTube", "tokens": [400, 370, 264, 1880, 551, 11, 797, 11, 307, 286, 458, 7140, 295, 561, 3288, 294, 3088], "temperature": 0.0, "avg_logprob": -0.15091686406411414, "compression_ratio": 1.7703180212014133, "no_speech_prob": 9.368382961838506e-06}, {"id": 1733, "seek": 782198, "start": 7834.219999999999, "end": 7835.219999999999, "text": " recommendation systems.", "tokens": [11879, 3652, 13], "temperature": 0.0, "avg_logprob": -0.15091686406411414, "compression_ratio": 1.7703180212014133, "no_speech_prob": 9.368382961838506e-06}, {"id": 1734, "seek": 782198, "start": 7835.219999999999, "end": 7838.9, "text": " None of them are wanting to promote conspiracy theories.", "tokens": [14492, 295, 552, 366, 7935, 281, 9773, 20439, 13667, 13], "temperature": 0.0, "avg_logprob": -0.15091686406411414, "compression_ratio": 1.7703180212014133, "no_speech_prob": 9.368382961838506e-06}, {"id": 1735, "seek": 782198, "start": 7838.9, "end": 7841.58, "text": " But people click on them, right?", "tokens": [583, 561, 2052, 322, 552, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15091686406411414, "compression_ratio": 1.7703180212014133, "no_speech_prob": 9.368382961838506e-06}, {"id": 1736, "seek": 782198, "start": 7841.58, "end": 7843.74, "text": " And people share them.", "tokens": [400, 561, 2073, 552, 13], "temperature": 0.0, "avg_logprob": -0.15091686406411414, "compression_ratio": 1.7703180212014133, "no_speech_prob": 9.368382961838506e-06}, {"id": 1737, "seek": 782198, "start": 7843.74, "end": 7849.54, "text": " And what tends to happen is also people that are into conspiracy theories consume a lot", "tokens": [400, 437, 12258, 281, 1051, 307, 611, 561, 300, 366, 666, 20439, 13667, 14732, 257, 688], "temperature": 0.0, "avg_logprob": -0.15091686406411414, "compression_ratio": 1.7703180212014133, "no_speech_prob": 9.368382961838506e-06}, {"id": 1738, "seek": 782198, "start": 7849.54, "end": 7851.82, "text": " more YouTube media.", "tokens": [544, 3088, 3021, 13], "temperature": 0.0, "avg_logprob": -0.15091686406411414, "compression_ratio": 1.7703180212014133, "no_speech_prob": 9.368382961838506e-06}, {"id": 1739, "seek": 785182, "start": 7851.82, "end": 7856.82, "text": " So it actually is very good at finding a market that watches a lot of hours of YouTube.", "tokens": [407, 309, 767, 307, 588, 665, 412, 5006, 257, 2142, 300, 17062, 257, 688, 295, 2496, 295, 3088, 13], "temperature": 0.0, "avg_logprob": -0.09708975878628817, "compression_ratio": 1.797709923664122, "no_speech_prob": 2.726449793044594e-06}, {"id": 1740, "seek": 785182, "start": 7856.82, "end": 7859.299999999999, "text": " And then it makes that market watch even more.", "tokens": [400, 550, 309, 1669, 300, 2142, 1159, 754, 544, 13], "temperature": 0.0, "avg_logprob": -0.09708975878628817, "compression_ratio": 1.797709923664122, "no_speech_prob": 2.726449793044594e-06}, {"id": 1741, "seek": 785182, "start": 7859.299999999999, "end": 7863.299999999999, "text": " So this is an example of a feedback loop.", "tokens": [407, 341, 307, 364, 1365, 295, 257, 5824, 6367, 13], "temperature": 0.0, "avg_logprob": -0.09708975878628817, "compression_ratio": 1.797709923664122, "no_speech_prob": 2.726449793044594e-06}, {"id": 1742, "seek": 785182, "start": 7863.299999999999, "end": 7868.86, "text": " And the New York Times is now describing YouTube as perhaps the most powerful radicalizing", "tokens": [400, 264, 1873, 3609, 11366, 307, 586, 16141, 3088, 382, 4317, 264, 881, 4005, 12001, 3319], "temperature": 0.0, "avg_logprob": -0.09708975878628817, "compression_ratio": 1.797709923664122, "no_speech_prob": 2.726449793044594e-06}, {"id": 1743, "seek": 785182, "start": 7868.86, "end": 7870.7, "text": " instrument of the 21st century.", "tokens": [7198, 295, 264, 5080, 372, 4901, 13], "temperature": 0.0, "avg_logprob": -0.09708975878628817, "compression_ratio": 1.797709923664122, "no_speech_prob": 2.726449793044594e-06}, {"id": 1744, "seek": 785182, "start": 7870.7, "end": 7875.9, "text": " I can tell you, my friends that worked on the YouTube recommendation system did not", "tokens": [286, 393, 980, 291, 11, 452, 1855, 300, 2732, 322, 264, 3088, 11879, 1185, 630, 406], "temperature": 0.0, "avg_logprob": -0.09708975878628817, "compression_ratio": 1.797709923664122, "no_speech_prob": 2.726449793044594e-06}, {"id": 1745, "seek": 785182, "start": 7875.9, "end": 7881.34, "text": " think they were creating the most powerful radicalizing instrument of the 21st century.", "tokens": [519, 436, 645, 4084, 264, 881, 4005, 12001, 3319, 7198, 295, 264, 5080, 372, 4901, 13], "temperature": 0.0, "avg_logprob": -0.09708975878628817, "compression_ratio": 1.797709923664122, "no_speech_prob": 2.726449793044594e-06}, {"id": 1746, "seek": 788134, "start": 7881.34, "end": 7887.9400000000005, "text": " And to be honest, most of them today, when I talk to them, still think they're not.", "tokens": [400, 281, 312, 3245, 11, 881, 295, 552, 965, 11, 562, 286, 751, 281, 552, 11, 920, 519, 436, 434, 406, 13], "temperature": 0.0, "avg_logprob": -0.10889983663753587, "compression_ratio": 1.7692307692307692, "no_speech_prob": 2.1781845134682953e-05}, {"id": 1747, "seek": 788134, "start": 7887.9400000000005, "end": 7888.9400000000005, "text": " They think it's all bullshit.", "tokens": [814, 519, 309, 311, 439, 22676, 13], "temperature": 0.0, "avg_logprob": -0.10889983663753587, "compression_ratio": 1.7692307692307692, "no_speech_prob": 2.1781845134682953e-05}, {"id": 1748, "seek": 788134, "start": 7888.9400000000005, "end": 7891.22, "text": " You know, not all of them.", "tokens": [509, 458, 11, 406, 439, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.10889983663753587, "compression_ratio": 1.7692307692307692, "no_speech_prob": 2.1781845134682953e-05}, {"id": 1749, "seek": 788134, "start": 7891.22, "end": 7895.88, "text": " But a lot of them now are at the point where they just feel like they're the victims here.", "tokens": [583, 257, 688, 295, 552, 586, 366, 412, 264, 935, 689, 436, 445, 841, 411, 436, 434, 264, 11448, 510, 13], "temperature": 0.0, "avg_logprob": -0.10889983663753587, "compression_ratio": 1.7692307692307692, "no_speech_prob": 2.1781845134682953e-05}, {"id": 1750, "seek": 788134, "start": 7895.88, "end": 7897.74, "text": " People are unfairly, you know, they don't get it.", "tokens": [3432, 366, 17019, 356, 11, 291, 458, 11, 436, 500, 380, 483, 309, 13], "temperature": 0.0, "avg_logprob": -0.10889983663753587, "compression_ratio": 1.7692307692307692, "no_speech_prob": 2.1781845134682953e-05}, {"id": 1751, "seek": 788134, "start": 7897.74, "end": 7900.06, "text": " They don't understand what we're trying to do.", "tokens": [814, 500, 380, 1223, 437, 321, 434, 1382, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.10889983663753587, "compression_ratio": 1.7692307692307692, "no_speech_prob": 2.1781845134682953e-05}, {"id": 1752, "seek": 788134, "start": 7900.06, "end": 7903.74, "text": " It's very, very difficult when you're right out there in the heart of it.", "tokens": [467, 311, 588, 11, 588, 2252, 562, 291, 434, 558, 484, 456, 294, 264, 1917, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.10889983663753587, "compression_ratio": 1.7692307692307692, "no_speech_prob": 2.1781845134682953e-05}, {"id": 1753, "seek": 788134, "start": 7903.74, "end": 7908.1, "text": " So you've got to be thinking from right at the start, what are the possible unintended", "tokens": [407, 291, 600, 658, 281, 312, 1953, 490, 558, 412, 264, 722, 11, 437, 366, 264, 1944, 49902], "temperature": 0.0, "avg_logprob": -0.10889983663753587, "compression_ratio": 1.7692307692307692, "no_speech_prob": 2.1781845134682953e-05}, {"id": 1754, "seek": 788134, "start": 7908.1, "end": 7910.860000000001, "text": " consequences of what you're working on?", "tokens": [10098, 295, 437, 291, 434, 1364, 322, 30], "temperature": 0.0, "avg_logprob": -0.10889983663753587, "compression_ratio": 1.7692307692307692, "no_speech_prob": 2.1781845134682953e-05}, {"id": 1755, "seek": 791086, "start": 7910.86, "end": 7915.5, "text": " And as the technical people involved, how can you get out in front and make sure that", "tokens": [400, 382, 264, 6191, 561, 3288, 11, 577, 393, 291, 483, 484, 294, 1868, 293, 652, 988, 300], "temperature": 0.0, "avg_logprob": -0.1555246950975105, "compression_ratio": 1.707236842105263, "no_speech_prob": 4.831659680348821e-05}, {"id": 1756, "seek": 791086, "start": 7915.5, "end": 7918.099999999999, "text": " people are aware of them?", "tokens": [561, 366, 3650, 295, 552, 30], "temperature": 0.0, "avg_logprob": -0.1555246950975105, "compression_ratio": 1.707236842105263, "no_speech_prob": 4.831659680348821e-05}, {"id": 1757, "seek": 791086, "start": 7918.099999999999, "end": 7922.0599999999995, "text": " And I just also wanted to say that in particular, many of these conspiracy theories are promoting", "tokens": [400, 286, 445, 611, 1415, 281, 584, 300, 294, 1729, 11, 867, 295, 613, 20439, 13667, 366, 16383], "temperature": 0.0, "avg_logprob": -0.1555246950975105, "compression_ratio": 1.707236842105263, "no_speech_prob": 4.831659680348821e-05}, {"id": 1758, "seek": 791086, "start": 7922.0599999999995, "end": 7928.62, "text": " white supremacy, they're, you know, kind of far-right ethno-nationalism, anti-science.", "tokens": [2418, 35572, 11, 436, 434, 11, 291, 458, 11, 733, 295, 1400, 12, 1938, 6468, 1771, 12, 43387, 1434, 11, 6061, 12, 82, 6699, 13], "temperature": 0.0, "avg_logprob": -0.1555246950975105, "compression_ratio": 1.707236842105263, "no_speech_prob": 4.831659680348821e-05}, {"id": 1759, "seek": 791086, "start": 7928.62, "end": 7932.299999999999, "text": " And I think, I don't know, maybe five or ten years ago, I would have thought conspiracy", "tokens": [400, 286, 519, 11, 286, 500, 380, 458, 11, 1310, 1732, 420, 2064, 924, 2057, 11, 286, 576, 362, 1194, 20439], "temperature": 0.0, "avg_logprob": -0.1555246950975105, "compression_ratio": 1.707236842105263, "no_speech_prob": 4.831659680348821e-05}, {"id": 1760, "seek": 791086, "start": 7932.299999999999, "end": 7933.98, "text": " theories are more a more fringe thing.", "tokens": [13667, 366, 544, 257, 544, 38764, 551, 13], "temperature": 0.0, "avg_logprob": -0.1555246950975105, "compression_ratio": 1.707236842105263, "no_speech_prob": 4.831659680348821e-05}, {"id": 1761, "seek": 791086, "start": 7933.98, "end": 7938.219999999999, "text": " But we're seeing the kind of huge societal impact it can have for many people to believe", "tokens": [583, 321, 434, 2577, 264, 733, 295, 2603, 33472, 2712, 309, 393, 362, 337, 867, 561, 281, 1697], "temperature": 0.0, "avg_logprob": -0.1555246950975105, "compression_ratio": 1.707236842105263, "no_speech_prob": 4.831659680348821e-05}, {"id": 1762, "seek": 791086, "start": 7938.219999999999, "end": 7939.219999999999, "text": " these.", "tokens": [613, 13], "temperature": 0.0, "avg_logprob": -0.1555246950975105, "compression_ratio": 1.707236842105263, "no_speech_prob": 4.831659680348821e-05}, {"id": 1763, "seek": 793922, "start": 7939.22, "end": 7943.18, "text": " Yeah, and you know, partly it's you see them on YouTube all the time, it starts to feel", "tokens": [865, 11, 293, 291, 458, 11, 17031, 309, 311, 291, 536, 552, 322, 3088, 439, 264, 565, 11, 309, 3719, 281, 841], "temperature": 0.0, "avg_logprob": -0.12683590327467875, "compression_ratio": 1.6329588014981273, "no_speech_prob": 1.241086829395499e-05}, {"id": 1764, "seek": 793922, "start": 7943.18, "end": 7945.66, "text": " a lot more normal, right?", "tokens": [257, 688, 544, 2710, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12683590327467875, "compression_ratio": 1.6329588014981273, "no_speech_prob": 1.241086829395499e-05}, {"id": 1765, "seek": 793922, "start": 7945.66, "end": 7950.34, "text": " So one of the things that people are doing to try to say like how to fix this problem", "tokens": [407, 472, 295, 264, 721, 300, 561, 366, 884, 281, 853, 281, 584, 411, 577, 281, 3191, 341, 1154], "temperature": 0.0, "avg_logprob": -0.12683590327467875, "compression_ratio": 1.6329588014981273, "no_speech_prob": 1.241086829395499e-05}, {"id": 1766, "seek": 793922, "start": 7950.34, "end": 7955.66, "text": " is to explicitly get involved in talking to the people who might or will be impacted by", "tokens": [307, 281, 20803, 483, 3288, 294, 1417, 281, 264, 561, 567, 1062, 420, 486, 312, 15653, 538], "temperature": 0.0, "avg_logprob": -0.12683590327467875, "compression_ratio": 1.6329588014981273, "no_speech_prob": 1.241086829395499e-05}, {"id": 1767, "seek": 793922, "start": 7955.66, "end": 7958.820000000001, "text": " the kind of decision-making processes that you're enabling.", "tokens": [264, 733, 295, 3537, 12, 12402, 7555, 300, 291, 434, 23148, 13], "temperature": 0.0, "avg_logprob": -0.12683590327467875, "compression_ratio": 1.6329588014981273, "no_speech_prob": 1.241086829395499e-05}, {"id": 1768, "seek": 793922, "start": 7958.820000000001, "end": 7964.9800000000005, "text": " So for example, there was a really cool thing recently where literally statisticians and", "tokens": [407, 337, 1365, 11, 456, 390, 257, 534, 1627, 551, 3938, 689, 3736, 29588, 2567, 293], "temperature": 0.0, "avg_logprob": -0.12683590327467875, "compression_ratio": 1.6329588014981273, "no_speech_prob": 1.241086829395499e-05}, {"id": 1769, "seek": 796498, "start": 7964.98, "end": 7972.0199999999995, "text": " data scientists got together with people who had been inside the criminal system, I had", "tokens": [1412, 7708, 658, 1214, 365, 561, 567, 632, 668, 1854, 264, 8628, 1185, 11, 286, 632], "temperature": 0.0, "avg_logprob": -0.1102178686916238, "compression_ratio": 1.8102766798418972, "no_speech_prob": 1.341951974609401e-05}, {"id": 1770, "seek": 796498, "start": 7972.0199999999995, "end": 7976.5, "text": " gone through the bail and sentencing process of criminals themselves, and talking to the", "tokens": [2780, 807, 264, 19313, 293, 2279, 13644, 1399, 295, 23474, 2969, 11, 293, 1417, 281, 264], "temperature": 0.0, "avg_logprob": -0.1102178686916238, "compression_ratio": 1.8102766798418972, "no_speech_prob": 1.341951974609401e-05}, {"id": 1771, "seek": 796498, "start": 7976.5, "end": 7980.9, "text": " lawyers who worked with them and put them together with the data scientists and actually", "tokens": [16219, 567, 2732, 365, 552, 293, 829, 552, 1214, 365, 264, 1412, 7708, 293, 767], "temperature": 0.0, "avg_logprob": -0.1102178686916238, "compression_ratio": 1.8102766798418972, "no_speech_prob": 1.341951974609401e-05}, {"id": 1772, "seek": 796498, "start": 7980.9, "end": 7987.099999999999, "text": " kind of put together a timeline of how exactly does it work and where exactly the places", "tokens": [733, 295, 829, 1214, 257, 12933, 295, 577, 2293, 775, 309, 589, 293, 689, 2293, 264, 3190], "temperature": 0.0, "avg_logprob": -0.1102178686916238, "compression_ratio": 1.8102766798418972, "no_speech_prob": 1.341951974609401e-05}, {"id": 1773, "seek": 796498, "start": 7987.099999999999, "end": 7990.66, "text": " that there are inputs and how do people respond to them and who's involved.", "tokens": [300, 456, 366, 15743, 293, 577, 360, 561, 4196, 281, 552, 293, 567, 311, 3288, 13], "temperature": 0.0, "avg_logprob": -0.1102178686916238, "compression_ratio": 1.8102766798418972, "no_speech_prob": 1.341951974609401e-05}, {"id": 1774, "seek": 796498, "start": 7990.66, "end": 7991.66, "text": " This is really cool, right?", "tokens": [639, 307, 534, 1627, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1102178686916238, "compression_ratio": 1.8102766798418972, "no_speech_prob": 1.341951974609401e-05}, {"id": 1775, "seek": 799166, "start": 7991.66, "end": 7997.98, "text": " This is the only way for you as a kind of a data product developer to actually know", "tokens": [639, 307, 264, 787, 636, 337, 291, 382, 257, 733, 295, 257, 1412, 1674, 10754, 281, 767, 458], "temperature": 0.0, "avg_logprob": -0.14210887946704825, "compression_ratio": 1.7, "no_speech_prob": 3.611918828028138e-06}, {"id": 1776, "seek": 799166, "start": 7997.98, "end": 7999.9, "text": " how your data product's going to be working.", "tokens": [577, 428, 1412, 1674, 311, 516, 281, 312, 1364, 13], "temperature": 0.0, "avg_logprob": -0.14210887946704825, "compression_ratio": 1.7, "no_speech_prob": 3.611918828028138e-06}, {"id": 1777, "seek": 799166, "start": 7999.9, "end": 8005.9, "text": " A really great example of somebody who did a great job here was Evanistola at Meetup", "tokens": [316, 534, 869, 1365, 295, 2618, 567, 630, 257, 869, 1691, 510, 390, 22613, 468, 4711, 412, 22963, 1010], "temperature": 0.0, "avg_logprob": -0.14210887946704825, "compression_ratio": 1.7, "no_speech_prob": 3.611918828028138e-06}, {"id": 1778, "seek": 799166, "start": 8005.9, "end": 8013.74, "text": " who said hey, a lot of men are going to our tech meetups and if we use a recommendation", "tokens": [567, 848, 4177, 11, 257, 688, 295, 1706, 366, 516, 281, 527, 7553, 1677, 7528, 293, 498, 321, 764, 257, 11879], "temperature": 0.0, "avg_logprob": -0.14210887946704825, "compression_ratio": 1.7, "no_speech_prob": 3.611918828028138e-06}, {"id": 1779, "seek": 799166, "start": 8013.74, "end": 8019.46, "text": " system naively, it's going to recommend more tech meetups to men, which is going to cause", "tokens": [1185, 1667, 3413, 11, 309, 311, 516, 281, 2748, 544, 7553, 1677, 7528, 281, 1706, 11, 597, 307, 516, 281, 3082], "temperature": 0.0, "avg_logprob": -0.14210887946704825, "compression_ratio": 1.7, "no_speech_prob": 3.611918828028138e-06}, {"id": 1780, "seek": 801946, "start": 8019.46, "end": 8023.18, "text": " more men to go to them, and then when women do try to go, they'll be like oh my god there's", "tokens": [544, 1706, 281, 352, 281, 552, 11, 293, 550, 562, 2266, 360, 853, 281, 352, 11, 436, 603, 312, 411, 1954, 452, 3044, 456, 311], "temperature": 0.0, "avg_logprob": -0.16198937552315848, "compression_ratio": 1.7408906882591093, "no_speech_prob": 3.6687811189040076e-06}, {"id": 1781, "seek": 801946, "start": 8023.18, "end": 8029.22, "text": " so many men here, which is going to cause more men to go to the tech meetups.", "tokens": [370, 867, 1706, 510, 11, 597, 307, 516, 281, 3082, 544, 1706, 281, 352, 281, 264, 7553, 1677, 7528, 13], "temperature": 0.0, "avg_logprob": -0.16198937552315848, "compression_ratio": 1.7408906882591093, "no_speech_prob": 3.6687811189040076e-06}, {"id": 1782, "seek": 801946, "start": 8029.22, "end": 8036.42, "text": " So showing recommendations to men and therefore not showing them to women.", "tokens": [407, 4099, 10434, 281, 1706, 293, 4412, 406, 4099, 552, 281, 2266, 13], "temperature": 0.0, "avg_logprob": -0.16198937552315848, "compression_ratio": 1.7408906882591093, "no_speech_prob": 3.6687811189040076e-06}, {"id": 1783, "seek": 801946, "start": 8036.42, "end": 8044.14, "text": " So what Evan and Meetup decided was to make an explicit product decision that this would", "tokens": [407, 437, 22613, 293, 22963, 1010, 3047, 390, 281, 652, 364, 13691, 1674, 3537, 300, 341, 576], "temperature": 0.0, "avg_logprob": -0.16198937552315848, "compression_ratio": 1.7408906882591093, "no_speech_prob": 3.6687811189040076e-06}, {"id": 1784, "seek": 801946, "start": 8044.14, "end": 8049.42, "text": " not even be representing the actual true preferences of people, it would be creating a real data", "tokens": [406, 754, 312, 13460, 264, 3539, 2074, 21910, 295, 561, 11, 309, 576, 312, 4084, 257, 957, 1412], "temperature": 0.0, "avg_logprob": -0.16198937552315848, "compression_ratio": 1.7408906882591093, "no_speech_prob": 3.6687811189040076e-06}, {"id": 1785, "seek": 804942, "start": 8049.42, "end": 8058.06, "text": " runaway feedback loop, so let's explicitly stop it before it happens and not recommend", "tokens": [1190, 10318, 5824, 6367, 11, 370, 718, 311, 20803, 1590, 309, 949, 309, 2314, 293, 406, 2748], "temperature": 0.0, "avg_logprob": -0.133947871980213, "compression_ratio": 1.5833333333333333, "no_speech_prob": 9.22326489671832e-06}, {"id": 1786, "seek": 804942, "start": 8058.06, "end": 8063.22, "text": " less tech meetups to women and more tech meetups to men.", "tokens": [1570, 7553, 1677, 7528, 281, 2266, 293, 544, 7553, 1677, 7528, 281, 1706, 13], "temperature": 0.0, "avg_logprob": -0.133947871980213, "compression_ratio": 1.5833333333333333, "no_speech_prob": 9.22326489671832e-06}, {"id": 1787, "seek": 804942, "start": 8063.22, "end": 8065.34, "text": " And so I think that's really cool.", "tokens": [400, 370, 286, 519, 300, 311, 534, 1627, 13], "temperature": 0.0, "avg_logprob": -0.133947871980213, "compression_ratio": 1.5833333333333333, "no_speech_prob": 9.22326489671832e-06}, {"id": 1788, "seek": 804942, "start": 8065.34, "end": 8071.66, "text": " It's like saying we don't have to be slaves to the algorithm, we actually get to decide.", "tokens": [467, 311, 411, 1566, 321, 500, 380, 362, 281, 312, 18394, 281, 264, 9284, 11, 321, 767, 483, 281, 4536, 13], "temperature": 0.0, "avg_logprob": -0.133947871980213, "compression_ratio": 1.5833333333333333, "no_speech_prob": 9.22326489671832e-06}, {"id": 1789, "seek": 804942, "start": 8071.66, "end": 8077.06, "text": " Another thing that people can do to help is regulation.", "tokens": [3996, 551, 300, 561, 393, 360, 281, 854, 307, 15062, 13], "temperature": 0.0, "avg_logprob": -0.133947871980213, "compression_ratio": 1.5833333333333333, "no_speech_prob": 9.22326489671832e-06}, {"id": 1790, "seek": 807706, "start": 8077.06, "end": 8081.820000000001, "text": " And normally when we talk about regulation, there's a natural reaction of how do you regulate", "tokens": [400, 5646, 562, 321, 751, 466, 15062, 11, 456, 311, 257, 3303, 5480, 295, 577, 360, 291, 24475], "temperature": 0.0, "avg_logprob": -0.15073017920217208, "compression_ratio": 1.773972602739726, "no_speech_prob": 1.4285171346273273e-05}, {"id": 1791, "seek": 807706, "start": 8081.820000000001, "end": 8082.820000000001, "text": " these things?", "tokens": [613, 721, 30], "temperature": 0.0, "avg_logprob": -0.15073017920217208, "compression_ratio": 1.773972602739726, "no_speech_prob": 1.4285171346273273e-05}, {"id": 1792, "seek": 807706, "start": 8082.820000000001, "end": 8084.740000000001, "text": " It's ridiculous, you can't regulate AI.", "tokens": [467, 311, 11083, 11, 291, 393, 380, 24475, 7318, 13], "temperature": 0.0, "avg_logprob": -0.15073017920217208, "compression_ratio": 1.773972602739726, "no_speech_prob": 1.4285171346273273e-05}, {"id": 1793, "seek": 807706, "start": 8084.740000000001, "end": 8089.740000000001, "text": " But actually when you look at it again and again, and this fantastic paper called Datasheets", "tokens": [583, 767, 562, 291, 574, 412, 309, 797, 293, 797, 11, 293, 341, 5456, 3035, 1219, 9315, 296, 675, 1385], "temperature": 0.0, "avg_logprob": -0.15073017920217208, "compression_ratio": 1.773972602739726, "no_speech_prob": 1.4285171346273273e-05}, {"id": 1794, "seek": 807706, "start": 8089.740000000001, "end": 8095.5, "text": " for Datasets has lots of examples of this, there are many many examples of industries", "tokens": [337, 9315, 296, 1385, 575, 3195, 295, 5110, 295, 341, 11, 456, 366, 867, 867, 5110, 295, 13284], "temperature": 0.0, "avg_logprob": -0.15073017920217208, "compression_ratio": 1.773972602739726, "no_speech_prob": 1.4285171346273273e-05}, {"id": 1795, "seek": 807706, "start": 8095.5, "end": 8099.9400000000005, "text": " where people thought they couldn't be regulated, people thought that's just how it was, like", "tokens": [689, 561, 1194, 436, 2809, 380, 312, 26243, 11, 561, 1194, 300, 311, 445, 577, 309, 390, 11, 411], "temperature": 0.0, "avg_logprob": -0.15073017920217208, "compression_ratio": 1.773972602739726, "no_speech_prob": 1.4285171346273273e-05}, {"id": 1796, "seek": 807706, "start": 8099.9400000000005, "end": 8106.700000000001, "text": " cars, people died in cars all the time because they literally had sharp metal knobs on dashboards,", "tokens": [5163, 11, 561, 4539, 294, 5163, 439, 264, 565, 570, 436, 3736, 632, 8199, 5760, 46999, 322, 8240, 17228, 11], "temperature": 0.0, "avg_logprob": -0.15073017920217208, "compression_ratio": 1.773972602739726, "no_speech_prob": 1.4285171346273273e-05}, {"id": 1797, "seek": 810670, "start": 8106.7, "end": 8112.26, "text": " steering columns weren't collapsible, and all of the discussion in the community was", "tokens": [14823, 13766, 4999, 380, 16567, 964, 11, 293, 439, 295, 264, 5017, 294, 264, 1768, 390], "temperature": 0.0, "avg_logprob": -0.11485666452452194, "compression_ratio": 1.6150442477876106, "no_speech_prob": 8.267720659205224e-06}, {"id": 1798, "seek": 810670, "start": 8112.26, "end": 8117.2, "text": " that's just how cars are and when people die in cars it's because of the people.", "tokens": [300, 311, 445, 577, 5163, 366, 293, 562, 561, 978, 294, 5163, 309, 311, 570, 295, 264, 561, 13], "temperature": 0.0, "avg_logprob": -0.11485666452452194, "compression_ratio": 1.6150442477876106, "no_speech_prob": 8.267720659205224e-06}, {"id": 1799, "seek": 810670, "start": 8117.2, "end": 8123.86, "text": " But then eventually the regulations did come in, and today driving is dramatically safer,", "tokens": [583, 550, 4728, 264, 12563, 630, 808, 294, 11, 293, 965, 4840, 307, 17548, 15856, 11], "temperature": 0.0, "avg_logprob": -0.11485666452452194, "compression_ratio": 1.6150442477876106, "no_speech_prob": 8.267720659205224e-06}, {"id": 1800, "seek": 810670, "start": 8123.86, "end": 8127.98, "text": " like dozens and dozens of times safer than it was before.", "tokens": [411, 18431, 293, 18431, 295, 1413, 15856, 813, 309, 390, 949, 13], "temperature": 0.0, "avg_logprob": -0.11485666452452194, "compression_ratio": 1.6150442477876106, "no_speech_prob": 8.267720659205224e-06}, {"id": 1801, "seek": 810670, "start": 8127.98, "end": 8131.3, "text": " So often there are things we can do through policy.", "tokens": [407, 2049, 456, 366, 721, 321, 393, 360, 807, 3897, 13], "temperature": 0.0, "avg_logprob": -0.11485666452452194, "compression_ratio": 1.6150442477876106, "no_speech_prob": 8.267720659205224e-06}, {"id": 1802, "seek": 813130, "start": 8131.3, "end": 8141.860000000001, "text": " So to summarize, we are part of the 0.3 to 0.5% of the world that knows how to code.", "tokens": [407, 281, 20858, 11, 321, 366, 644, 295, 264, 1958, 13, 18, 281, 1958, 13, 20, 4, 295, 264, 1002, 300, 3255, 577, 281, 3089, 13], "temperature": 0.0, "avg_logprob": -0.08831801272854947, "compression_ratio": 1.6025641025641026, "no_speech_prob": 6.643350843660301e-06}, {"id": 1803, "seek": 813130, "start": 8141.860000000001, "end": 8144.22, "text": " We have a skill that very few other people do.", "tokens": [492, 362, 257, 5389, 300, 588, 1326, 661, 561, 360, 13], "temperature": 0.0, "avg_logprob": -0.08831801272854947, "compression_ratio": 1.6025641025641026, "no_speech_prob": 6.643350843660301e-06}, {"id": 1804, "seek": 813130, "start": 8144.22, "end": 8147.74, "text": " Not only that, we now know how to code deep learning algorithms, which is like the most", "tokens": [1726, 787, 300, 11, 321, 586, 458, 577, 281, 3089, 2452, 2539, 14642, 11, 597, 307, 411, 264, 881], "temperature": 0.0, "avg_logprob": -0.08831801272854947, "compression_ratio": 1.6025641025641026, "no_speech_prob": 6.643350843660301e-06}, {"id": 1805, "seek": 813130, "start": 8147.74, "end": 8150.62, "text": " powerful kind of code I know.", "tokens": [4005, 733, 295, 3089, 286, 458, 13], "temperature": 0.0, "avg_logprob": -0.08831801272854947, "compression_ratio": 1.6025641025641026, "no_speech_prob": 6.643350843660301e-06}, {"id": 1806, "seek": 813130, "start": 8150.62, "end": 8156.22, "text": " So I'm hoping that we can explicitly think about at least not making the world worse", "tokens": [407, 286, 478, 7159, 300, 321, 393, 20803, 519, 466, 412, 1935, 406, 1455, 264, 1002, 5324], "temperature": 0.0, "avg_logprob": -0.08831801272854947, "compression_ratio": 1.6025641025641026, "no_speech_prob": 6.643350843660301e-06}, {"id": 1807, "seek": 813130, "start": 8156.22, "end": 8160.14, "text": " and perhaps explicitly making it better.", "tokens": [293, 4317, 20803, 1455, 309, 1101, 13], "temperature": 0.0, "avg_logprob": -0.08831801272854947, "compression_ratio": 1.6025641025641026, "no_speech_prob": 6.643350843660301e-06}, {"id": 1808, "seek": 816014, "start": 8160.14, "end": 8164.200000000001, "text": " And so why is this interesting to you as an audience in particular?", "tokens": [400, 370, 983, 307, 341, 1880, 281, 291, 382, 364, 4034, 294, 1729, 30], "temperature": 0.0, "avg_logprob": -0.15226282792932846, "compression_ratio": 1.7109375, "no_speech_prob": 2.144337850040756e-05}, {"id": 1809, "seek": 816014, "start": 8164.200000000001, "end": 8171.22, "text": " And that's because Fast.ai in particular is trying to make it easy for domain experts", "tokens": [400, 300, 311, 570, 15968, 13, 1301, 294, 1729, 307, 1382, 281, 652, 309, 1858, 337, 9274, 8572], "temperature": 0.0, "avg_logprob": -0.15226282792932846, "compression_ratio": 1.7109375, "no_speech_prob": 2.144337850040756e-05}, {"id": 1810, "seek": 816014, "start": 8171.22, "end": 8172.76, "text": " to use deep learning.", "tokens": [281, 764, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.15226282792932846, "compression_ratio": 1.7109375, "no_speech_prob": 2.144337850040756e-05}, {"id": 1811, "seek": 816014, "start": 8172.76, "end": 8177.740000000001, "text": " And so this picture of the goats here is an example of one of our international fellows", "tokens": [400, 370, 341, 3036, 295, 264, 34219, 510, 307, 364, 1365, 295, 472, 295, 527, 5058, 35595], "temperature": 0.0, "avg_logprob": -0.15226282792932846, "compression_ratio": 1.7109375, "no_speech_prob": 2.144337850040756e-05}, {"id": 1812, "seek": 816014, "start": 8177.740000000001, "end": 8184.46, "text": " from a previous course who was a goat dairy farmer and told us that they were going to", "tokens": [490, 257, 3894, 1164, 567, 390, 257, 23608, 21276, 17891, 293, 1907, 505, 300, 436, 645, 516, 281], "temperature": 0.0, "avg_logprob": -0.15226282792932846, "compression_ratio": 1.7109375, "no_speech_prob": 2.144337850040756e-05}, {"id": 1813, "seek": 816014, "start": 8184.46, "end": 8189.860000000001, "text": " use deep learning on their remote Canadian island to help study utter disease in goats.", "tokens": [764, 2452, 2539, 322, 641, 8607, 12641, 6077, 281, 854, 2979, 17567, 4752, 294, 34219, 13], "temperature": 0.0, "avg_logprob": -0.15226282792932846, "compression_ratio": 1.7109375, "no_speech_prob": 2.144337850040756e-05}, {"id": 1814, "seek": 818986, "start": 8189.86, "end": 8195.38, "text": " And to me this is a great example of like a domain experts problem which nobody else", "tokens": [400, 281, 385, 341, 307, 257, 869, 1365, 295, 411, 257, 9274, 8572, 1154, 597, 5079, 1646], "temperature": 0.0, "avg_logprob": -0.10925777753194173, "compression_ratio": 1.6528301886792454, "no_speech_prob": 1.1300373444100842e-05}, {"id": 1815, "seek": 818986, "start": 8195.38, "end": 8199.039999999999, "text": " even knows about, let alone know that it's a computer vision problem that can be solved", "tokens": [754, 3255, 466, 11, 718, 3312, 458, 300, 309, 311, 257, 3820, 5201, 1154, 300, 393, 312, 13041], "temperature": 0.0, "avg_logprob": -0.10925777753194173, "compression_ratio": 1.6528301886792454, "no_speech_prob": 1.1300373444100842e-05}, {"id": 1816, "seek": 818986, "start": 8199.039999999999, "end": 8200.279999999999, "text": " with deep learning.", "tokens": [365, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.10925777753194173, "compression_ratio": 1.6528301886792454, "no_speech_prob": 1.1300373444100842e-05}, {"id": 1817, "seek": 818986, "start": 8200.279999999999, "end": 8209.74, "text": " So in your field, whatever it is, you probably know a lot more now about the opportunities", "tokens": [407, 294, 428, 2519, 11, 2035, 309, 307, 11, 291, 1391, 458, 257, 688, 544, 586, 466, 264, 4786], "temperature": 0.0, "avg_logprob": -0.10925777753194173, "compression_ratio": 1.6528301886792454, "no_speech_prob": 1.1300373444100842e-05}, {"id": 1818, "seek": 818986, "start": 8209.74, "end": 8213.14, "text": " in your field to make it a hell of a lot better than it was before.", "tokens": [294, 428, 2519, 281, 652, 309, 257, 4921, 295, 257, 688, 1101, 813, 309, 390, 949, 13], "temperature": 0.0, "avg_logprob": -0.10925777753194173, "compression_ratio": 1.6528301886792454, "no_speech_prob": 1.1300373444100842e-05}, {"id": 1819, "seek": 818986, "start": 8213.14, "end": 8217.66, "text": " You'll probably be able to come up with all kinds of cool product ideas, maybe build a", "tokens": [509, 603, 1391, 312, 1075, 281, 808, 493, 365, 439, 3685, 295, 1627, 1674, 3487, 11, 1310, 1322, 257], "temperature": 0.0, "avg_logprob": -0.10925777753194173, "compression_ratio": 1.6528301886792454, "no_speech_prob": 1.1300373444100842e-05}, {"id": 1820, "seek": 821766, "start": 8217.66, "end": 8222.26, "text": " startup or create a new product group in your company or whatever.", "tokens": [18578, 420, 1884, 257, 777, 1674, 1594, 294, 428, 2237, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.15340193191377244, "compression_ratio": 1.76, "no_speech_prob": 4.8603196773910895e-06}, {"id": 1821, "seek": 821766, "start": 8222.26, "end": 8228.26, "text": " But also please be thinking about what that's going to mean in practice and think about", "tokens": [583, 611, 1767, 312, 1953, 466, 437, 300, 311, 516, 281, 914, 294, 3124, 293, 519, 466], "temperature": 0.0, "avg_logprob": -0.15340193191377244, "compression_ratio": 1.76, "no_speech_prob": 4.8603196773910895e-06}, {"id": 1822, "seek": 821766, "start": 8228.26, "end": 8230.82, "text": " where can you put humans in the loop?", "tokens": [689, 393, 291, 829, 6255, 294, 264, 6367, 30], "temperature": 0.0, "avg_logprob": -0.15340193191377244, "compression_ratio": 1.76, "no_speech_prob": 4.8603196773910895e-06}, {"id": 1823, "seek": 821766, "start": 8230.82, "end": 8233.78, "text": " Where can you put those pressure release valves?", "tokens": [2305, 393, 291, 829, 729, 3321, 4374, 34950, 30], "temperature": 0.0, "avg_logprob": -0.15340193191377244, "compression_ratio": 1.76, "no_speech_prob": 4.8603196773910895e-06}, {"id": 1824, "seek": 821766, "start": 8233.78, "end": 8239.039999999999, "text": " Who are the people you can talk to who could be impacted who could help you understand?", "tokens": [2102, 366, 264, 561, 291, 393, 751, 281, 567, 727, 312, 15653, 567, 727, 854, 291, 1223, 30], "temperature": 0.0, "avg_logprob": -0.15340193191377244, "compression_ratio": 1.76, "no_speech_prob": 4.8603196773910895e-06}, {"id": 1825, "seek": 821766, "start": 8239.039999999999, "end": 8244.18, "text": " And get the kind of humanities folks involved to understand history and psychology and sociology", "tokens": [400, 483, 264, 733, 295, 36140, 4024, 3288, 281, 1223, 2503, 293, 15105, 293, 41744], "temperature": 0.0, "avg_logprob": -0.15340193191377244, "compression_ratio": 1.76, "no_speech_prob": 4.8603196773910895e-06}, {"id": 1826, "seek": 821766, "start": 8244.18, "end": 8245.76, "text": " and so forth.", "tokens": [293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.15340193191377244, "compression_ratio": 1.76, "no_speech_prob": 4.8603196773910895e-06}, {"id": 1827, "seek": 824576, "start": 8245.76, "end": 8247.9, "text": " So that's our plea to you.", "tokens": [407, 300, 311, 527, 42152, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.15439907242270076, "compression_ratio": 1.455128205128205, "no_speech_prob": 1.383681592415087e-05}, {"id": 1828, "seek": 824576, "start": 8247.9, "end": 8252.2, "text": " If you've got this far, you're definitely at a point now where you're ready to make", "tokens": [759, 291, 600, 658, 341, 1400, 11, 291, 434, 2138, 412, 257, 935, 586, 689, 291, 434, 1919, 281, 652], "temperature": 0.0, "avg_logprob": -0.15439907242270076, "compression_ratio": 1.455128205128205, "no_speech_prob": 1.383681592415087e-05}, {"id": 1829, "seek": 824576, "start": 8252.2, "end": 8254.800000000001, "text": " a serious impact on the world.", "tokens": [257, 3156, 2712, 322, 264, 1002, 13], "temperature": 0.0, "avg_logprob": -0.15439907242270076, "compression_ratio": 1.455128205128205, "no_speech_prob": 1.383681592415087e-05}, {"id": 1830, "seek": 824576, "start": 8254.800000000001, "end": 8257.94, "text": " So I hope we can make sure that that's a positive impact.", "tokens": [407, 286, 1454, 321, 393, 652, 988, 300, 300, 311, 257, 3353, 2712, 13], "temperature": 0.0, "avg_logprob": -0.15439907242270076, "compression_ratio": 1.455128205128205, "no_speech_prob": 1.383681592415087e-05}, {"id": 1831, "seek": 824576, "start": 8257.94, "end": 8258.94, "text": " See you next week.", "tokens": [3008, 291, 958, 1243, 13], "temperature": 0.0, "avg_logprob": -0.15439907242270076, "compression_ratio": 1.455128205128205, "no_speech_prob": 1.383681592415087e-05}, {"id": 1832, "seek": 825894, "start": 8258.94, "end": 8276.34, "text": " If you can make it here with just one practice session.", "tokens": [759, 291, 393, 652, 309, 510, 365, 445, 472, 3124, 5481, 13], "temperature": 1.0, "avg_logprob": -2.760053825378418, "compression_ratio": 0.9565217391304348, "no_speech_prob": 0.00032586781890131533}], "language": "en"}