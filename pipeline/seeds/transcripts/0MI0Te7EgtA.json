{"text": " So I'm going to talk today about something I don't think I've really discussed before, which is my journey to deep learning. So nowadays I am all deep learning all the time. And a lot of people seem to kind of assume that anybody who's doing deep learning kind of jumps straight into it without looking at anything else. But actually, at least for me, it was a many decade journey to this point. And it's because I've done so many other things and seen how much easier my life is with deep learning that I've become such an evangelist for this technology. So I actually started out my career at McKinsey & Company, which is a management consulting firm. And quite unusually, I started there when I was 18. And that was a challenge because in strategy consulting, people are generally really leveraging their expertise and experience. And of course, I didn't have either of those things. So I really had to rely on data analysis right from the start. And so what happened was from the very start of my career, I was really relying very heavily on applied data analysis to answer real world questions. And so in a consulting context, you don't have that much time. And you're talking to people who really have a lot of domain expertise and you have to be able to communicate in a way that actually matters to them. And I used a variety of techniques in my data analysis. One of my favorite things to use was this was just before pivot tables appeared. And when they appeared, that was something I used a lot, used various kind of database tools and so forth. But I did actually use machine learning quite a bit and had a lot of success with that. Most of the machine learning that I was doing was based on kind of logistic or linear regression or something like that. And rather than show you something I did back then, because I can't because it was all proprietary, let me give you an example from the computational pathologist paper from Andy Beck and Daphne Coller and others at Stanford. This was maybe 2011 or something like that, 2012. And they developed a five-year survival model for breast cancer, I believe it was. And the inputs to their five-year survival model were histopathology slices, stained slides. And they built a five-year survival predictive model that was very significantly better than anything that had come before. And what they described in their paper is the way they went about it was what I would call nowadays kind of classic machine learning. They used a regularized logistic regression and they fed into that logistic regression, if I remember correctly, thousands I think of features. And the features they built were built by a large expert team of pathologists, scientists, mathematicians and so forth that worked together to think about what kinds of features might be interesting and how do we encode them. So that things like relationships of contiguous epithelial regions with underlying nuclear objects or characteristics of epithelial nuclei and epithelial cytoplasm, characteristics of stromal nuclei and stromal matrix and so on and so forth. So it took many people, many years to create these features and come up with them and implement them and test them. And then the actual modeling process was fairly straightforward. They took images from patients that stayed alive for five years and they took images from those that didn't and then used a fairly standard logistic, regularized logistic regression to build a classifier. So basically to create parameters around these different features. And to be clear, this approach worked well for this particular case and worked well for me for years for many, many projects. And it's kind of a perfectly reasonable bread and butter technique that you can certainly still use today in a very similar way. I spent a lot of time studying how to get the most out of this. One nice trick that a lot of people are not as familiar with as they should be is what do you do with these inputs in these cases and how do you transform them so that you can handle nonlinearities. A lot of people use polynomials for that. And actually polynomials are generally a terrible choice. Nearly always the best choice it turns out is actually to use something called natural cubic splines. And natural cubic splines are basically where you split your data set into kind of sections of the domain and you connect each section up with a cubic. So each of these bits between dots of cubics and you create the basis such that these cubics connect up with each other and their gradients connect up. And one of the interesting things that makes them natural splines is that the endpoints are actually linear rather than cubic, which actually makes these extrapolate outside the input domain really nicely. You can see as you add more and more knots with just two knots you start out with a line and as you add more knots you start to get more and more opportunities for curves. One of the cool things about natural splines, they're also called restricted cubic splines, is that actually you don't have to think at all about where to put the knot points. It turns out that there's basically a set of quantiles where you can put the knot points pretty reliably depending on how many knot points you want, which is independent of the data and nearly always works. So this was a nice trick. And then another nice trick is if you do use regularized regression, particularly L1 regularized regression I really like, you don't even have to be that careful about the number of parameters you include a lot of the time. So you can often include quite a lot of transformations, including actually, sorry, not transformations, so interactions, including interactions of natural cubic spline terms. So this is an approach that I used a lot and had a lot of success with. But then in the, I think it's 1999 that the first paper appeared in the early, in kind of 2000, it started getting popular, was random forests. And random forests, this is a picture from Terence Parr's excellent DTreeViz package. So forests are ensembles of decision trees, as I'm sure most of you know. And so for an example of a decision tree, this is some data from the Kaggle competition, which is trying to predict the auction price of heavy industrial equipment. And you can see here that a decision tree has done a split on this binary variable of coupler system. And then for those which, I guess, don't have a coupler system, it did a binary split on year made. And those which then were made in early years, then we can see immediately the sale price. So this is the thing we're trying to predict the sale price. And so in this case, we can see that it's in just four splits, it's successfully found some things, which this is actually the log of sale price has done a really good job of splitting out the log of sale price. I actually used these single decision trees a little bit in the kind of early and mid 90s, but they were a nightmare to find something that fit adequately, but didn't overfit. And random forests then came along thanks to Breiman, who a very interesting guy. He was originally a math professor at Berkeley. And then he went out into industry and was basically a consultant, I think, for years and then came back to Berkeley to do statistics. And he was incredibly effective in creating like really practical algorithms. And the random forest is one that's really been world changing, incredibly simple. You just randomly pick a subset of your data and you then train a model, train it, you know, just create a decision tree with a subset. You save it and then you repeat steps one, two, and three again and again and again, creating lots and lots of decision trees on different random subsets of the data. And it turns out that if you average the results of all these models, you get predictions that are unbiased, accurate, and don't overfit. And it's a really, really cool approach. So basically, as soon as this came out, I added it to my arsenal. One of the really nice things about this is how quickly you can implement it. We implemented it in like a day, basically. So this came out when I was running a company called Optimal Decision which I built to help insurers come up with better prices, which is the most important thing for insurers to do. One of the interesting things about this for me is that we never actually deployed a random forest. What we did was we used random forests to understand the data. And then we use that understanding of the data to then go back and basically build more traditional regression models with the particular terms and transformations and interactions that the random forest found were important. So basically, this is one of the cool things that you get out of a random forest. It's a feature importance plot. And it shows you, so this is again the same data set, the auction price data set from Kaggle. It shows you which are the most important features. And the nice thing about this is you don't have to do any transformations or think about interactions or nonlinearities because they're using decision trees behind the scenes. It all just works. And so I kind of developed this pretty simple approach where I would first create a random forest and I would then find which features and so forth are useful. I then use partial dependence plots to kind of look at the shapes of them. And then I would go back and kind of for the continuous variables that matter, create the cubic splines and create the interactions and then do a regression. And so this basic kind of trick was incredibly powerful. And I used it, a variance of it, in the early days of Kaggle amongst other things and got to number one in the world and won a number of competitions. And funnily enough, actually, back in 2011, I described my approaches to Kaggle competitions in Melbourne at the Melbourne R Meetup. And you can still find that talk on YouTube. And it's actually still pretty much just as relevant today as it was at that time. So this is 2011. And I became the chief scientist and president at Kaggle. And we took it over to the US and got venture capital and built into quite a successful business. But something interesting that happened as chief scientist of Kaggle, I was getting to see all the competitions up close. And seven years ago, there was a competition, Dogs vs Cats, which you can still see actually on the Dogs vs Cats Kaggle page, it describes the state of the art approach for recognizing dogs vs cats as being around about 80% accuracy. And so that was based on the academic papers that had tackled this problem at the time. And then in this competition that just ran for three months, eight teams reached 98% accuracy and one nearly got to 99% accuracy. So if you think about this as a 20% error rate, and this is basically a 1% error rate, so this competition brought the state of the art down by about 20 times in three months, which is really extraordinary. It's really unheard of to see an academic state of the art result that has been carefully studied, slashed by 20 acts by somebody working for just three months on the problem. That's normally something that might take decades or hundreds of years if it's possible at all. So something clearly happened here. And of course, what happened was deep learning. And Pierre actually had developed one of the early deep learning libraries. And actually, even this kind of signal on Kaggle was in some ways a little late. If you actually look at Pierre's Google Scholar, you'll see that it was actually back in 2011 that him and Jan LeCun had already produced a system that was better than human performance at recognizing traffic signs. And so this was actually the first time that I noticed this really extraordinary thing, which was deep learning being better than humans at very human tasks, like looking at pictures. And so in 2011, I thought, wow, that's super interesting, but it's hard to do anything with that information because there wasn't any open source software or even any commercial software available to actually do it. There was J\u00fcrgen Schmidt Huber's lab had a kind of like a DLL or something or a library you could buy from them to do it, although they didn't even have a demo. There wasn't any online services and there wasn't any, nobody had published anywhere like the actual recipe book of like, how the hell do you do these things? And so that was a huge challenge. It's exciting to see that this is possible, but then it's like, well, what do I do about it? But one of the cool things is that at this exact moment, this dogs and cats moment is when two really accessible open source libraries appeared, allowing people to actually create their own deep learning models for the first time. And critically, they were built on top of CUDA, which was a dramatically more convenient way of programming GPUs than had previously existed. So kind of things started to come together, really seven years ago, a little bit. I had been interested in neural networks since the very start of my career. And in fact, in consulting, I worked with one of the big Australian banks on implementing a neural network in the early to mid 90s to help with targeted marketing, not a very exciting application, I'll give you. But it really struck me at the time that this was a technology which I felt like at some point would probably take over just about everything else in terms of my area of interest around predictive modeling. And we actually had quite a bit of success with it even then. So that's like 30 years ago now, nearly. But there were some issues back then. For one thing, we had to buy custom hardware that cost millions of dollars. We really needed a lot of data, millions of data points. So in a retail bank, we could do that. And yeah, it was even then there were things that just weren't quite working as well as we would expect. And so as it turned out, the key problem was that back then everybody was relying on this math result called the universal approximation theorem, which said that a neural network could solve any given problem, computable problem to any arbitrary level of accuracy. And it only needs one hidden layer. And this is one of the many, many times in deep learning history where theory has been used in totally inappropriate ways. And the problem with this theory was that although this was theoretically true, in practice, a neural network with one hidden layer requires far too many nodes to be useful most of the time. And what we actually need is lots of hidden layers. And that turns out to be much more efficient. So anyway, I did kind of feel like for those 20 years, at some point, neural networks are going to reappear in my life because of this infinitely flexible function, the fact that they can solve any given problem in theory. And then along with this infinitely flexible function, we combine it with gradient descent, which is this all-purpose parameter fitting algorithm. And again, there was a problem with theory here, which is I spent many, many years focused on operations research and optimization. And operations research generally focused on, again, kind of theoretical questions of what is proofably able to find the definite maximum or minimum of a function. And gradient descent doesn't do that, particularly stochastic gradient descent. And so a lot of people were kind of ignoring it. But the thing is, the question we should be asking is not what can we prove, but what actually works in practice. And the people who, the very small number of people who were working on neural networks and gradient descent throughout the 90s and early 2000s, despite the theory that said it's a terrible idea, actually were finding it was working really well. Unfortunately, you know, academia around machine learning has tended to be much more driven by theory than results, at least for a long time it was. I still think it is too much. And so the fact that there were people like Hinton and Lacoon saying, look, here's a model that's better than anything in the world at solving this problem. But, you know, based on theory, we can't exactly prove why, but it really works. Those were not getting published, unfortunately. Anyway, so things gradually began to change. And one of the big things that changed was that finally, in the, you know, kind of around 2014, 2015, we started to see some software appearing that allowed us to conveniently train these things on GPUs, which allowed us to use, you know, relatively inexpensive computers to actually get pretty good results. So although the theory didn't really change at this point, what did change is just more people could try things out and be like, oh, okay, this is actually practically really helpful. To people outside of the world of neural networks, this all seemed very sudden. It seemed like there was this sudden fad around deep learning where people were suddenly going, wow, this is amazing. And so people who had seen other fads quite reasonably thought, well, this one will pass too. But the difference with this fad is it's actually been under development for many, many, many decades. So this was the first neural network to be built and it was back in 1957 that it was built. And continually, for all those decades, there were people working on making neural nets really work in practice. So what was happening in 2015 was not a sudden, here's this new thing where I got organo flock to, but it was actually here's this old thing, which we finally got to the point where it's actually really working. And so it's not a new fad at all, but it's really the result of decades of hard work of solving lots of problems and finally getting to a point where things are making sense. But what has happened since kind of 2015 is the ability of these infinitely flexible functions has suddenly started to become clear even to a lay person, because you can just look at what they're doing and it's mind blowing. So for example, if you look at OpenAI's Dali, this is a model that's been trained on pairs of pictures and captions such that you can now write any arbitrary sentence. So if you write an illustration of a baby daikon radish in a tutu walking a dog, Dali will draw pictures of what you described for you. And here are some actual non-cherry picked pictures of that. And so to be clear, this is all out of domain. So Dali has never seen illustrations of baby daikon radishes yet or radishes and tutus or let alone any of this combination of things. It's creating these entirely from scratch. By the same token, it's never seen an avocado shaped chair before as best as I know, but if you type in an armchair in the shape of an avocado, it creates these pictures for you from scratch. And so, you know, it's really cool now that we can actually kind of show, we can actually say, look what computers can do and look what computers can do if you use deep learning. And to anybody who's grown up in the kind of pre deep learning era, this just looks like magic. You know, it's like, this is not things that I believe computers can do. But here we are. This is the theoretically universally capable model actually doing things that we've trained it to do. So in the last few years, we're now starting to see, you know, many times every year examples of computers doing things which we're being told computers won't be able to do in our lifetime. So for example, I was repeatedly told by experts that in my lifetime, we would never see a computer win a game of Go against an expert. And of course, we're now at the point where AlphaGo Zero got to that point in three days. And it's so far ahead of the best expert now that, you know, it's kind of makes the world's best experts look like total beginners. And one of the really interesting things about AlphaGo Zero is that if you actually look at the source code for it, here it is. And the source code for the key thing, which is like the thing that figures out whether a Go board is a good position or not, fits on one slide. And furthermore, if you've done any deep learning, you'll recognize it as looking almost exactly like a standard computer vision model. And so one of the things which people who are not themselves deep learning practitioners don't quite realize is that deep learning on the whole is not a huge collection of somewhat disconnected but slightly connected kind of tricks. It's actually, you know, every deep learning model I build looks almost exactly like every other model I build with fairly minor differences. And I train them in nearly exactly the same way with fairly minor differences. And so deep learning has become this incredibly flexible skill that if you have it, you can turn your attention to lots of different domain areas and rapidly get incredibly good results. So at this point, deep learning is now the best approach in the world for all kinds of applications. I'm not going to read them all. And this is by no means a complete list. It's far longer than this. But these are some examples of the kinds of things that deep learning is better at than any other known approach. So why am I spending so much time in my life now on deep learning? Because it really feels to me like a very dramatic step change in human capability, like the development of electricity, for example. And I would like to think that when I see a very dramatic step change in human capability, I'm going to spend my time working on figuring out how best to take advantage of that capability. Because there's going to be so many world-changing breakthroughs that come out of that. And particularly as somebody who's built a few companies, as an entrepreneur, the number one thing for an entrepreneur to find and that investors look for is, is there something you can build now that people couldn't build before in terms of as a company? And with deep learning, the answer to that is yes, across tens of thousands or hundreds of thousands of areas. Because it's like, okay, suddenly there's tools which we couldn't automate before and now we can, or we can make hundreds of times more productive or so forth. So to me, it's a very obvious thing that this is what I want to spend all my time on. And when I talk to students or interested entrepreneurs, I always say, this is the thing which is making lots and lots of people extremely rich and is solving lots and lots of important societal problems. And we're just seeing the tip of the iceberg. So as soon as I got to the point that I realized this, I decided to start a company to actually do something important. And so I got very excited about the opportunities in medicine and I created the first deep learning and medicine company called Inletic. And I didn't know anything about medicine and I didn't know any people in medicine. So I got together a group of three other people and me and we decided to hack together a quick deep learning model that would see if we can predict the malignancy of nodules in lung CT scans. And it turned out that we could. And in fact, the algorithm that we built for this company that I ended up calling Inletic had a better false positive rate and a better false negative rate than actually a panel of four trained radiologists. And so this was at a time when deep learning in medicine and deep learning in radiology was unheard of. There were basically no papers about it. There were certainly no startups about it. No one was talking about it. And so this finding got some attention. And this was really important to me because my biggest goal with Inletic was to kind of get deep learning and medicine on the map because I felt like it could save a lot of lives. So I wanted to get a lot of as much attention around this as possible. And yeah, very quickly, lots and lots of people were writing about this new company. And as a result, very quickly, deep learning, particularly in radiology, took off. And within two years, the main radiology conference had a huge stream around AI. It was lines out the door. They created a whole new journal for it and so forth. And so that was really exciting for me to see how we could help kind of put a technology on the map. In some ways, this is great, but in some ways it was kind of disappointing because there were so many other areas where deep learning should have been on the map and it wasn't. And there's no way that I could create companies around every possible area. So instead I thought, well, what I want to do is make it easy for other people to create companies and products and solutions using deep learning, particularly because at that time, nearly everybody I knew in the deep learning world were young white men from one of a small number of exclusive universities. And the problem with that is that there's a lot of societally important problems to solve, which that group of people just weren't familiar with. And even if they were both familiar with them and interested with them, interested in them, they didn't know how to find the data for those or what the kind of constraints and implementation are and so forth. So Dr. Rachel Thomas and I decided to create a new organization that would focus on one thing, which was making deep learning accessible. And so basically the idea was to say, okay, if this really is a step change in human capability, which happens from time to time in technology history, what can we do to help people use this technology regardless of their background? And so there was a lot of constraints that we had to help remove. So the first thing we did was we thought, okay, let's at least make it so that what people already know about how to build deep learning models is as available as possible. So at this time, there weren't any courses or any kind of easy ways in to get going with deep learning. And we had a theory, which was we thought you don't need a Stanford PhD to be an effective deep learning practitioner. You don't need years and years of graduate level math training. We thought that we might be able to build a course that would allow people with just a year of coding background to become effective deep learning practitioners. Now at this time, so what is this about 2014 or 2015? Can't quite remember, maybe 2015. Nothing like this existed. And this was a really controversial hypothesis. And to be clear, we weren't sure we were right, but this was a feeling we had. So we thought, let's give it a go. So the first thing we created was a fast AI practical deep learning course. And certainly one thing we immediately saw, which was thrilling, and we certainly didn't know what would happen, was that it was popular. A lot of people took the course. We made it freely available online with no ads, make it as accessible as possible since that's our mission. And I said to that first class, if you create something interesting with deep learning, please tell us about it on our forums. So we created a forum so that students could communicate with each other. And we got thousands of replies. And I remember one of the first ones we got, I think this was one of the first, was somebody who tried to recognize cricket pictures from baseball pictures. And they had, I think it was like 100% accuracy or maybe 99% accuracy. And one of the really interesting things was that they only used 30 training images. And so this is like exciting to us to see somebody like building a model and not only that, building it with far less data than people used to think was necessary. And then suddenly we were being flattered with all these cool models people were building. So a Trident ad and Tobago to different types of people model, a zucchini and cucumber model. This is a really interesting one. This person actually managed to predict what part of the world a satellite photo was from with over 110 classes with 85% accuracy, which is extraordinary. A Panama bus recognizer, a batik cloth recognizer. Some of the things were clearly actually going to be very useful in practice. This was something useful for disaster resilience, which was recognizing the state of buildings in this place in Tanzania. We saw people even right at the start of the course breaking state of the art results. So this is on Devangari character recognition. This person said, wow, I just got a new state of the art result, which is really exciting. We saw people doing the same symbol, us getting state of the art results on audio classification. And then even we started to hear from some of these students in the first year that they were taking their ideas back to their companies. And in this case, a software engineer went back to his company, he was working at Splunk and built a new model, which basically took mouse movements and mouse clicks and turned them into pictures and then classified them and used this to help with fraud. And we know about this because it was so successful that it ended up being a patented product and Splunk created a blog about this cool new technology that was built by a software engineer with no previous background in this area. And we saw startups appearing. So for example, this startup called Envision appeared from one of our students and it's still going strong. I just looked it up before this. And so, yeah, it was really cool to see how people from all walks of life were actually able to get started with deep learning. And these courses got really popular and so we started redoing them every year. So we'd build a new course from scratch because things were moving so fast that within a year there was so much new stuff to cover that we had built a completely new course. And so there's many millions of views at this point and people are loving them based on what they're telling YouTube anyway. So this has been really a pleasure to see. We ended up turning the course into a book as well, along with my friend, Sylvain Gouger, and people are really liking the book as well. So the next step after getting people started with what do we already know by putting that into a course is we wanted to push the boundaries beyond what we already know. And so one of the things that came up was a lot of students or potential students were saying, I don't think it's worth me getting involved in deep learning because it takes too much compute and too much data. And it's, you know, unless you're a Google or Facebook, you can't do it. And this became particularly an issue when Google released their TPUs and put out a big PR exercise saying, okay, you know, TPUs are so great that nobody else, you know, can pretty much do anything useful in deep learning now. And so we decided to enter this international competition called Dawnbench that Google and Intel had entered to see if we could beat them, like be faster than them at training deep learning models. And so that was April 2018. And we won on many of the axes of the competition. The cheapest, the fastest GPU, fastest single machine. And then we followed this up with additional results that were actually 40% faster than Google's best TPU results. And so this was exciting because here's a picture of us and our students working on this project together. It was a really cool time. You know, because we really wanted to push back against this narrative that you have to be Google. And so it got a lot of media attention, which was great. And it really, the finding here was using common sense is more important than using vast amounts of money and compute. It was really cool to see also that a lot of the big companies noticed what we were doing and bought in our ideas. So Nvidia, when they started promoting how great their GPUs were, they started talking about, you know, how good they were with the additional ideas that we had developed with our students. So academic research became a critical component of fast AI's work. And we did similar research to drive breakthroughs in natural language processing, in tabular modeling and lots of other areas. So then the question is, okay, well with all these, you know, now that we've actually pushed the boundaries beyond what's already known to say, okay, we can actually do get better results with less data and less compute more quickly. How do we put that into the hands of everybody so that everybody can use these insights? So that's why we decided to build a software library called fast AI. So that was just in 2018 that version one came out, but it immediately got a lot of attention. It got supported by all the big cloud services. And we were able to show that compared to Keras, for example, it was much more accurate, much faster, far, far less lines of code. And we really tried to make it as accessible as possible. So the, you know, this is some of the documentation from fast AI. You can see that not only do you get the normal kind of API documentation, but it's actually, you know, got pictures of exactly what's going on. It's got links to the papers that it's implementing. And also all of the code for all of the pictures is all directly there as well. And one of the really nice things is that every single page of the documentation has a link to let you actually open that page of documentation as an interactive notebook because the entire thing is built with interactive notebooks. So you can then get the exact same thing, but now you can experiment with it and you can see all the source code there. So we really took the kind of approaches that we found worked well in our course of having students do lots of experiments and lots of coding and making that a kind of part of our documentation as well is to let people really play, play with everything themselves and experiment and see how it all works. So incorporating all of this research into the software was super successful. We started hearing from people saying, okay, well I've just started with fast AI and I've started pulling across some of my TensorFlow models and I don't understand why is everything so much better? You know, what's, what's going on here? So people were really noticing that they were getting dramatically better results. So this person said the same thing. Yup. We used to try to use TensorFlow. We spent months tweaking our model. We switched to fast AI and within a couple of days we were getting better results. So by kind of combining the research with the software, we were able to provide a software library that let people get started more quickly. And then version two, which has been around for a bit over a year now, was a very dramatic advance further still. There's a whole academic paper that you can read describing all the deep design approaches which we've used. One of the really nice things about it is that basically regardless of what you're trying to do with fast AI, you can use almost exactly the same code. So for example, here's the code necessary to recognize dogs from cats. Here's the code necessary to build a segmentation model. It's basically the same lines of code. Here's a code to classify text movie reviews, almost the same lines of code. Here's the code necessary to do collaborative filtering, almost the same lines of code. So I said earlier that kind of under the covers, different models look more similar than different with deep learning. And so with fast AI, we've really tried to surface that so that you learn one API and you can use it anywhere. That's not enough for researchers or people that really need to dig deeper. So one of the really nice things about that is that underneath this applications layer is a tiered or layered API where you can go in and change anything. And I'm not going to describe it in too much detail, but for example, part of this mid tier API is a new two-way callback system which basically allows you at any point when you're training a model to see exactly what it's doing and to change literally anything that it's doing. You can skip parts of the training process, you can change the gradients, you can change the data and so forth. And so with this new approach, we're able to implement, for example, this is from a paper called mixup, we're able to implement mixup data augmentation in just a few lines of code. And if you compare that to the actual original Facebook paper, not only was it far more lines of code, this is what it looks like from their research paper without using callbacks, but it's also far less flexible because everything's hard coded or else with this approach, you can mix and match really easily. Another example of this layered API is we built a new approach to creating new optimizers using just two concepts, stats and steppers. I won't go into the details, but in short, this is what a particular optimizer called AdamW looks like in PyTorch. And this took about two years between the paper being released and Facebook releasing the AdamW implementation. Our implementation was released within a day of the paper and it consists of these one, two, three, four, five words. Because we're leveraging this layered API for optimizers, it's basically really easy to utilize the components to quickly implement new papers. Here's another example of an optimizer. This one's called lamb. This came from a Google paper. And one of the really cool things you might notice is that there's a very close match between the lines of code in our implementation and the lines of math in the algorithm in the paper. So anyway, there's a little summary of both what I'm doing now with FastAI and how I got there and why. And yeah, I'm happy to take any questions. Thanks, Jeremy. So yeah, really interesting kind of historical view of where you came from. So I guess, so I'll start with a quick thing. So I mentioned, so like in deep learning, that obviously there's very similar structures and code and solving problems. But how do you incorporate things like knowledge about the problem? Obviously the type of architecture that would have to go in there would come from the context of the problem. Yeah, that's a great question. So there's a number of really interesting ways of incorporating knowledge about the problem. And it's a really important thing to do because this is like, this is how you kind of get a whole lot of extra performance and need less data and less time, the more of that knowledge you can incorporate. So yeah, one way is certainly to directly implement it in the architecture. So for example, a very popular architecture for computer vision is convolutional architecture. And the convolution, a 2D convolution is taking advantage of our domain knowledge, which is that there's generally autocorrelation across pixels in both the X and Y dimensions. And so we're basically mapping a set of weights across groups of pixels that are all next to each other. There's a really wide range of interesting ways of incorporating all kinds of domain knowledge into architectures. And there's lots of geometry based approaches of doing that within natural language processing. There's lots of autoregressive approaches there. That's one area. An area I am extremely fond of is data augmentation. And in particular, there's been a huge kind of improvement in the last 12 months or so in how much we can do with a tiny amount of data by using something called self-supervised learning and in particular using something called contrastive loss. And what this is doing is you basically come up with really, try to come up with really thoughtful data augmentation approaches where you can say like, okay, so for example, in NLP, one of the approaches is to translate each sentence with a translation model into a different language, and then translate it back again. So you're now going to get a different version of the same sentence, but it shouldn't mean the same thing. And so then with contrastive loss, it basically says you add a part to the loss function that says those two different sentences should have the same result in our model. And so with something called UDA, which is basically adding contrastive loss and self-supervised learning to NLP, they were able to get results for movie review classification with just 20 labeled examples that were better than the previous state of the art using 25,000 labeled examples. Anyway, there's lots of ways we can incorporate domain knowledge into models, but there's a couple of ones that I like. Yes, I guess there are a couple of questions about interpretability. So one of the questions that came up is it's hard to explain to stakeholders. And so how can you convince them that deep learning is worth adopting? I mean, obviously you can show predictive performance, but is there any other ways that you can do that? Sure. So my view is that deep learning models are much more interpretable and explainable than most regression models, for example. Generally speaking, the traditional approach to people thinking the right way to understand, for example, regression models is to look at their coefficients. And I've always told people don't do that because in almost any real world regression problem, you've got coefficients representing interactions, you've got coefficients on things that are co-linear, you've got coefficients on various different bases of a transformed nonlinear variable. None of the coefficients can be understood independently because they can only be understood as how they combine with all the other things that are related. So I generally really dislike it when people try and explain a regression by looking at coefficients. To me, the right way to understand a model is to do the same thing we would do to understand a person, which is to ask questions. And so whether it's a regression or a random forest or a deep learning model, you can generally easily ask questions like what would happen if I made this variable on this row a little bit bigger or a little bit smaller or things like that, which actually are much easier to do in deep learning because in deep learning, those are just questions about the derivative of the input. And so you can actually get them much more quickly and easily. You can also do really interesting things with deep learning around showing which things are similar to each other kind of in the deep learning feature space. And you can build really cool applications for domain experts then, which can give them a lot of comfort. So you can say, yes, it's accurate. But I can also show you which parts of the input are particularly important in this case, which other inputs are similar to this one. And often we find like, for example, in the medical space, doctors will kind of go, wow, that's really clever the way it recognized that this patient and this patient were similar. A lot of doctors wouldn't have noticed that. It's actually this subtle thing going on. And I guess we're right at 11 o'clock. But maybe one last question that somebody brought up is, is there any future research opportunities in cross machine learning and quantum computing? You can think about it. It's an interesting question. No, probably not one I've got any expertise on. It could well be an interesting question, but I'm not the right person to ask. One thing I do want to mention is I have just moved back to Australia after 10 years in San Francisco. And I am extremely keen to see Australia become an absolute knowledge hub around deep learning. And I would particularly love to see our fast AI software. Just like when you think about TensorFlow, you kind of have this whole ecosystem around it, around Google and startups and all this. I would love to see Australia become, you know, that fast AI is kind of the homegrown library and that people here will really take it to heart and help us make it brilliant. It's all open source, you know, and we've got a Discord channel where we all chat about it. And, you know, any organizations that are interested in, you know, taking advantage of this free open source library, I would love to support them and see like, you know, academic institutions. I'd love to see this become a really successful ecosystem here in Australia. Great. It seems like it's going to be quite useful to solve lots of problems. So I think it would be good to do. So there are still some questions in the chat. We'll have the chat transcript and if there's any questions that Jeremy might be worth addressing from there, we can think about posting responses to those if there's anything in there. But we can do that after the fact. So thank you everybody for coming and thank Jeremy for joining us today. Thanks so much. Yeah, it's been great. Bye all.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.04, "text": " So I'm going to talk today about something I don't think I've really discussed before,", "tokens": [407, 286, 478, 516, 281, 751, 965, 466, 746, 286, 500, 380, 519, 286, 600, 534, 7152, 949, 11], "temperature": 0.0, "avg_logprob": -0.15411476916577443, "compression_ratio": 1.599078341013825, "no_speech_prob": 0.009550802409648895}, {"id": 1, "seek": 0, "start": 6.04, "end": 15.88, "text": " which is my journey to deep learning. So nowadays I am all deep learning all the time. And a", "tokens": [597, 307, 452, 4671, 281, 2452, 2539, 13, 407, 13434, 286, 669, 439, 2452, 2539, 439, 264, 565, 13, 400, 257], "temperature": 0.0, "avg_logprob": -0.15411476916577443, "compression_ratio": 1.599078341013825, "no_speech_prob": 0.009550802409648895}, {"id": 2, "seek": 0, "start": 15.88, "end": 22.96, "text": " lot of people seem to kind of assume that anybody who's doing deep learning kind of", "tokens": [688, 295, 561, 1643, 281, 733, 295, 6552, 300, 4472, 567, 311, 884, 2452, 2539, 733, 295], "temperature": 0.0, "avg_logprob": -0.15411476916577443, "compression_ratio": 1.599078341013825, "no_speech_prob": 0.009550802409648895}, {"id": 3, "seek": 0, "start": 22.96, "end": 26.64, "text": " jumps straight into it without looking at anything else. But actually, at least for", "tokens": [16704, 2997, 666, 309, 1553, 1237, 412, 1340, 1646, 13, 583, 767, 11, 412, 1935, 337], "temperature": 0.0, "avg_logprob": -0.15411476916577443, "compression_ratio": 1.599078341013825, "no_speech_prob": 0.009550802409648895}, {"id": 4, "seek": 2664, "start": 26.64, "end": 36.28, "text": " me, it was a many decade journey to this point. And it's because I've done so many other things", "tokens": [385, 11, 309, 390, 257, 867, 10378, 4671, 281, 341, 935, 13, 400, 309, 311, 570, 286, 600, 1096, 370, 867, 661, 721], "temperature": 0.0, "avg_logprob": -0.09246922230374986, "compression_ratio": 1.431578947368421, "no_speech_prob": 1.3414712157100439e-05}, {"id": 5, "seek": 2664, "start": 36.28, "end": 45.08, "text": " and seen how much easier my life is with deep learning that I've become such an evangelist", "tokens": [293, 1612, 577, 709, 3571, 452, 993, 307, 365, 2452, 2539, 300, 286, 600, 1813, 1270, 364, 24546, 468], "temperature": 0.0, "avg_logprob": -0.09246922230374986, "compression_ratio": 1.431578947368421, "no_speech_prob": 1.3414712157100439e-05}, {"id": 6, "seek": 2664, "start": 45.08, "end": 54.68, "text": " for this technology. So I actually started out my career at McKinsey & Company, which", "tokens": [337, 341, 2899, 13, 407, 286, 767, 1409, 484, 452, 3988, 412, 21765, 259, 7399, 3693, 13918, 11, 597], "temperature": 0.0, "avg_logprob": -0.09246922230374986, "compression_ratio": 1.431578947368421, "no_speech_prob": 1.3414712157100439e-05}, {"id": 7, "seek": 5468, "start": 54.68, "end": 64.44, "text": " is a management consulting firm. And quite unusually, I started there when I was 18.", "tokens": [307, 257, 4592, 23682, 6174, 13, 400, 1596, 10054, 671, 11, 286, 1409, 456, 562, 286, 390, 2443, 13], "temperature": 0.0, "avg_logprob": -0.10532376074021862, "compression_ratio": 1.4835164835164836, "no_speech_prob": 2.045703695330303e-05}, {"id": 8, "seek": 5468, "start": 64.44, "end": 72.44, "text": " And that was a challenge because in strategy consulting, people are generally really leveraging", "tokens": [400, 300, 390, 257, 3430, 570, 294, 5206, 23682, 11, 561, 366, 5101, 534, 32666], "temperature": 0.0, "avg_logprob": -0.10532376074021862, "compression_ratio": 1.4835164835164836, "no_speech_prob": 2.045703695330303e-05}, {"id": 9, "seek": 5468, "start": 72.44, "end": 78.88, "text": " their expertise and experience. And of course, I didn't have either of those things. So I", "tokens": [641, 11769, 293, 1752, 13, 400, 295, 1164, 11, 286, 994, 380, 362, 2139, 295, 729, 721, 13, 407, 286], "temperature": 0.0, "avg_logprob": -0.10532376074021862, "compression_ratio": 1.4835164835164836, "no_speech_prob": 2.045703695330303e-05}, {"id": 10, "seek": 7888, "start": 78.88, "end": 90.47999999999999, "text": " really had to rely on data analysis right from the start. And so what happened was from", "tokens": [534, 632, 281, 10687, 322, 1412, 5215, 558, 490, 264, 722, 13, 400, 370, 437, 2011, 390, 490], "temperature": 0.0, "avg_logprob": -0.13371620803582865, "compression_ratio": 1.5889570552147239, "no_speech_prob": 8.93838114279788e-06}, {"id": 11, "seek": 7888, "start": 90.47999999999999, "end": 98.24, "text": " the very start of my career, I was really relying very heavily on applied data analysis", "tokens": [264, 588, 722, 295, 452, 3988, 11, 286, 390, 534, 24140, 588, 10950, 322, 6456, 1412, 5215], "temperature": 0.0, "avg_logprob": -0.13371620803582865, "compression_ratio": 1.5889570552147239, "no_speech_prob": 8.93838114279788e-06}, {"id": 12, "seek": 7888, "start": 98.24, "end": 103.16, "text": " to answer real world questions. And so in a consulting context, you don't have that", "tokens": [281, 1867, 957, 1002, 1651, 13, 400, 370, 294, 257, 23682, 4319, 11, 291, 500, 380, 362, 300], "temperature": 0.0, "avg_logprob": -0.13371620803582865, "compression_ratio": 1.5889570552147239, "no_speech_prob": 8.93838114279788e-06}, {"id": 13, "seek": 10316, "start": 103.16, "end": 108.84, "text": " much time. And you're talking to people who really have a lot of domain expertise and", "tokens": [709, 565, 13, 400, 291, 434, 1417, 281, 561, 567, 534, 362, 257, 688, 295, 9274, 11769, 293], "temperature": 0.0, "avg_logprob": -0.12202851365252239, "compression_ratio": 1.599078341013825, "no_speech_prob": 5.093193522043293e-06}, {"id": 14, "seek": 10316, "start": 108.84, "end": 117.0, "text": " you have to be able to communicate in a way that actually matters to them. And I used", "tokens": [291, 362, 281, 312, 1075, 281, 7890, 294, 257, 636, 300, 767, 7001, 281, 552, 13, 400, 286, 1143], "temperature": 0.0, "avg_logprob": -0.12202851365252239, "compression_ratio": 1.599078341013825, "no_speech_prob": 5.093193522043293e-06}, {"id": 15, "seek": 10316, "start": 117.0, "end": 123.36, "text": " a variety of techniques in my data analysis. One of my favorite things to use was this", "tokens": [257, 5673, 295, 7512, 294, 452, 1412, 5215, 13, 1485, 295, 452, 2954, 721, 281, 764, 390, 341], "temperature": 0.0, "avg_logprob": -0.12202851365252239, "compression_ratio": 1.599078341013825, "no_speech_prob": 5.093193522043293e-06}, {"id": 16, "seek": 10316, "start": 123.36, "end": 130.0, "text": " was just before pivot tables appeared. And when they appeared, that was something I used", "tokens": [390, 445, 949, 14538, 8020, 8516, 13, 400, 562, 436, 8516, 11, 300, 390, 746, 286, 1143], "temperature": 0.0, "avg_logprob": -0.12202851365252239, "compression_ratio": 1.599078341013825, "no_speech_prob": 5.093193522043293e-06}, {"id": 17, "seek": 13000, "start": 130.0, "end": 137.74, "text": " a lot, used various kind of database tools and so forth. But I did actually use machine", "tokens": [257, 688, 11, 1143, 3683, 733, 295, 8149, 3873, 293, 370, 5220, 13, 583, 286, 630, 767, 764, 3479], "temperature": 0.0, "avg_logprob": -0.09178268164396286, "compression_ratio": 1.5773809523809523, "no_speech_prob": 1.4823176570644137e-06}, {"id": 18, "seek": 13000, "start": 137.74, "end": 147.24, "text": " learning quite a bit and had a lot of success with that. Most of the machine learning that", "tokens": [2539, 1596, 257, 857, 293, 632, 257, 688, 295, 2245, 365, 300, 13, 4534, 295, 264, 3479, 2539, 300], "temperature": 0.0, "avg_logprob": -0.09178268164396286, "compression_ratio": 1.5773809523809523, "no_speech_prob": 1.4823176570644137e-06}, {"id": 19, "seek": 13000, "start": 147.24, "end": 154.48, "text": " I was doing was based on kind of logistic or linear regression or something like that.", "tokens": [286, 390, 884, 390, 2361, 322, 733, 295, 3565, 3142, 420, 8213, 24590, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.09178268164396286, "compression_ratio": 1.5773809523809523, "no_speech_prob": 1.4823176570644137e-06}, {"id": 20, "seek": 15448, "start": 154.48, "end": 166.12, "text": " And rather than show you something I did back then, because I can't because it was all proprietary,", "tokens": [400, 2831, 813, 855, 291, 746, 286, 630, 646, 550, 11, 570, 286, 393, 380, 570, 309, 390, 439, 38992, 11], "temperature": 0.0, "avg_logprob": -0.11879344608472742, "compression_ratio": 1.3402777777777777, "no_speech_prob": 3.3404082842025673e-06}, {"id": 21, "seek": 15448, "start": 166.12, "end": 176.0, "text": " let me give you an example from the computational pathologist paper from Andy Beck and Daphne", "tokens": [718, 385, 976, 291, 364, 1365, 490, 264, 28270, 3100, 9201, 3035, 490, 13285, 19184, 293, 413, 13957, 716], "temperature": 0.0, "avg_logprob": -0.11879344608472742, "compression_ratio": 1.3402777777777777, "no_speech_prob": 3.3404082842025673e-06}, {"id": 22, "seek": 17600, "start": 176.0, "end": 185.84, "text": " Coller and others at Stanford. This was maybe 2011 or something like that, 2012. And they", "tokens": [4586, 260, 293, 2357, 412, 20374, 13, 639, 390, 1310, 10154, 420, 746, 411, 300, 11, 9125, 13, 400, 436], "temperature": 0.0, "avg_logprob": -0.19920359487118927, "compression_ratio": 1.292857142857143, "no_speech_prob": 4.4252697080082726e-06}, {"id": 23, "seek": 17600, "start": 185.84, "end": 194.36, "text": " developed a five-year survival model for breast cancer, I believe it was. And the inputs to", "tokens": [4743, 257, 1732, 12, 5294, 12559, 2316, 337, 9934, 5592, 11, 286, 1697, 309, 390, 13, 400, 264, 15743, 281], "temperature": 0.0, "avg_logprob": -0.19920359487118927, "compression_ratio": 1.292857142857143, "no_speech_prob": 4.4252697080082726e-06}, {"id": 24, "seek": 19436, "start": 194.36, "end": 207.48000000000002, "text": " their five-year survival model were histopathology slices, stained slides. And they built a five-year", "tokens": [641, 1732, 12, 5294, 12559, 2316, 645, 1758, 27212, 1793, 19793, 11, 39924, 9788, 13, 400, 436, 3094, 257, 1732, 12, 5294], "temperature": 0.0, "avg_logprob": -0.0834071295601981, "compression_ratio": 1.5865921787709498, "no_speech_prob": 3.1380413929582573e-06}, {"id": 25, "seek": 19436, "start": 207.48000000000002, "end": 213.08, "text": " survival predictive model that was very significantly better than anything that had come before.", "tokens": [12559, 35521, 2316, 300, 390, 588, 10591, 1101, 813, 1340, 300, 632, 808, 949, 13], "temperature": 0.0, "avg_logprob": -0.0834071295601981, "compression_ratio": 1.5865921787709498, "no_speech_prob": 3.1380413929582573e-06}, {"id": 26, "seek": 19436, "start": 213.08, "end": 220.52, "text": " And what they described in their paper is the way they went about it was what I would", "tokens": [400, 437, 436, 7619, 294, 641, 3035, 307, 264, 636, 436, 1437, 466, 309, 390, 437, 286, 576], "temperature": 0.0, "avg_logprob": -0.0834071295601981, "compression_ratio": 1.5865921787709498, "no_speech_prob": 3.1380413929582573e-06}, {"id": 27, "seek": 22052, "start": 220.52, "end": 229.24, "text": " call nowadays kind of classic machine learning. They used a regularized logistic regression", "tokens": [818, 13434, 733, 295, 7230, 3479, 2539, 13, 814, 1143, 257, 3890, 1602, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.12152558167775472, "compression_ratio": 1.560693641618497, "no_speech_prob": 1.4063194612390362e-05}, {"id": 28, "seek": 22052, "start": 229.24, "end": 235.32000000000002, "text": " and they fed into that logistic regression, if I remember correctly, thousands I think", "tokens": [293, 436, 4636, 666, 300, 3565, 3142, 24590, 11, 498, 286, 1604, 8944, 11, 5383, 286, 519], "temperature": 0.0, "avg_logprob": -0.12152558167775472, "compression_ratio": 1.560693641618497, "no_speech_prob": 1.4063194612390362e-05}, {"id": 29, "seek": 22052, "start": 235.32000000000002, "end": 246.68, "text": " of features. And the features they built were built by a large expert team of pathologists,", "tokens": [295, 4122, 13, 400, 264, 4122, 436, 3094, 645, 3094, 538, 257, 2416, 5844, 1469, 295, 3100, 12256, 11], "temperature": 0.0, "avg_logprob": -0.12152558167775472, "compression_ratio": 1.560693641618497, "no_speech_prob": 1.4063194612390362e-05}, {"id": 30, "seek": 24668, "start": 246.68, "end": 252.8, "text": " scientists, mathematicians and so forth that worked together to think about what kinds", "tokens": [7708, 11, 32811, 2567, 293, 370, 5220, 300, 2732, 1214, 281, 519, 466, 437, 3685], "temperature": 0.0, "avg_logprob": -0.0991850121076717, "compression_ratio": 1.7417840375586855, "no_speech_prob": 2.840363777067978e-05}, {"id": 31, "seek": 24668, "start": 252.8, "end": 257.32, "text": " of features might be interesting and how do we encode them. So that things like relationships", "tokens": [295, 4122, 1062, 312, 1880, 293, 577, 360, 321, 2058, 1429, 552, 13, 407, 300, 721, 411, 6159], "temperature": 0.0, "avg_logprob": -0.0991850121076717, "compression_ratio": 1.7417840375586855, "no_speech_prob": 2.840363777067978e-05}, {"id": 32, "seek": 24668, "start": 257.32, "end": 263.28000000000003, "text": " of contiguous epithelial regions with underlying nuclear objects or characteristics of epithelial", "tokens": [295, 660, 30525, 2388, 355, 338, 831, 10682, 365, 14217, 8179, 6565, 420, 10891, 295, 2388, 355, 338, 831], "temperature": 0.0, "avg_logprob": -0.0991850121076717, "compression_ratio": 1.7417840375586855, "no_speech_prob": 2.840363777067978e-05}, {"id": 33, "seek": 24668, "start": 263.28000000000003, "end": 269.2, "text": " nuclei and epithelial cytoplasm, characteristics of stromal nuclei and stromal matrix and so", "tokens": [49919, 293, 2388, 355, 338, 831, 3185, 19337, 7743, 76, 11, 10891, 295, 1056, 298, 304, 49919, 293, 1056, 298, 304, 8141, 293, 370], "temperature": 0.0, "avg_logprob": -0.0991850121076717, "compression_ratio": 1.7417840375586855, "no_speech_prob": 2.840363777067978e-05}, {"id": 34, "seek": 26920, "start": 269.2, "end": 278.64, "text": " on and so forth. So it took many people, many years to create these features and come up", "tokens": [322, 293, 370, 5220, 13, 407, 309, 1890, 867, 561, 11, 867, 924, 281, 1884, 613, 4122, 293, 808, 493], "temperature": 0.0, "avg_logprob": -0.12138178348541259, "compression_ratio": 1.6854460093896713, "no_speech_prob": 2.99434145745181e-06}, {"id": 35, "seek": 26920, "start": 278.64, "end": 284.68, "text": " with them and implement them and test them. And then the actual modeling process was fairly", "tokens": [365, 552, 293, 4445, 552, 293, 1500, 552, 13, 400, 550, 264, 3539, 15983, 1399, 390, 6457], "temperature": 0.0, "avg_logprob": -0.12138178348541259, "compression_ratio": 1.6854460093896713, "no_speech_prob": 2.99434145745181e-06}, {"id": 36, "seek": 26920, "start": 284.68, "end": 291.52, "text": " straightforward. They took images from patients that stayed alive for five years and they", "tokens": [15325, 13, 814, 1890, 5267, 490, 4209, 300, 9181, 5465, 337, 1732, 924, 293, 436], "temperature": 0.0, "avg_logprob": -0.12138178348541259, "compression_ratio": 1.6854460093896713, "no_speech_prob": 2.99434145745181e-06}, {"id": 37, "seek": 26920, "start": 291.52, "end": 295.8, "text": " took images from those that didn't and then used a fairly standard logistic, regularized", "tokens": [1890, 5267, 490, 729, 300, 994, 380, 293, 550, 1143, 257, 6457, 3832, 3565, 3142, 11, 3890, 1602], "temperature": 0.0, "avg_logprob": -0.12138178348541259, "compression_ratio": 1.6854460093896713, "no_speech_prob": 2.99434145745181e-06}, {"id": 38, "seek": 29580, "start": 295.8, "end": 302.72, "text": " logistic regression to build a classifier. So basically to create parameters around these", "tokens": [3565, 3142, 24590, 281, 1322, 257, 1508, 9902, 13, 407, 1936, 281, 1884, 9834, 926, 613], "temperature": 0.0, "avg_logprob": -0.11383756796518961, "compression_ratio": 1.543859649122807, "no_speech_prob": 1.300646999879973e-05}, {"id": 39, "seek": 29580, "start": 302.72, "end": 311.32, "text": " different features. And to be clear, this approach worked well for this particular case", "tokens": [819, 4122, 13, 400, 281, 312, 1850, 11, 341, 3109, 2732, 731, 337, 341, 1729, 1389], "temperature": 0.0, "avg_logprob": -0.11383756796518961, "compression_ratio": 1.543859649122807, "no_speech_prob": 1.300646999879973e-05}, {"id": 40, "seek": 29580, "start": 311.32, "end": 322.76, "text": " and worked well for me for years for many, many projects. And it's kind of a perfectly", "tokens": [293, 2732, 731, 337, 385, 337, 924, 337, 867, 11, 867, 4455, 13, 400, 309, 311, 733, 295, 257, 6239], "temperature": 0.0, "avg_logprob": -0.11383756796518961, "compression_ratio": 1.543859649122807, "no_speech_prob": 1.300646999879973e-05}, {"id": 41, "seek": 32276, "start": 322.76, "end": 329.12, "text": " reasonable bread and butter technique that you can certainly still use today in a very", "tokens": [10585, 5961, 293, 5517, 6532, 300, 291, 393, 3297, 920, 764, 965, 294, 257, 588], "temperature": 0.0, "avg_logprob": -0.10546561021071214, "compression_ratio": 1.5142857142857142, "no_speech_prob": 2.7530366423889063e-05}, {"id": 42, "seek": 32276, "start": 329.12, "end": 340.4, "text": " similar way. I spent a lot of time studying how to get the most out of this. One nice", "tokens": [2531, 636, 13, 286, 4418, 257, 688, 295, 565, 7601, 577, 281, 483, 264, 881, 484, 295, 341, 13, 1485, 1481], "temperature": 0.0, "avg_logprob": -0.10546561021071214, "compression_ratio": 1.5142857142857142, "no_speech_prob": 2.7530366423889063e-05}, {"id": 43, "seek": 32276, "start": 340.4, "end": 349.24, "text": " trick that a lot of people are not as familiar with as they should be is what do you do with", "tokens": [4282, 300, 257, 688, 295, 561, 366, 406, 382, 4963, 365, 382, 436, 820, 312, 307, 437, 360, 291, 360, 365], "temperature": 0.0, "avg_logprob": -0.10546561021071214, "compression_ratio": 1.5142857142857142, "no_speech_prob": 2.7530366423889063e-05}, {"id": 44, "seek": 34924, "start": 349.24, "end": 355.48, "text": " these inputs in these cases and how do you transform them so that you can handle nonlinearities.", "tokens": [613, 15743, 294, 613, 3331, 293, 577, 360, 291, 4088, 552, 370, 300, 291, 393, 4813, 2107, 28263, 1088, 13], "temperature": 0.0, "avg_logprob": -0.19264934723635754, "compression_ratio": 1.759433962264151, "no_speech_prob": 5.2242394303902984e-05}, {"id": 45, "seek": 34924, "start": 355.48, "end": 361.8, "text": " A lot of people use polynomials for that. And actually polynomials are generally a terrible", "tokens": [316, 688, 295, 561, 764, 22560, 12356, 337, 300, 13, 400, 767, 22560, 12356, 366, 5101, 257, 6237], "temperature": 0.0, "avg_logprob": -0.19264934723635754, "compression_ratio": 1.759433962264151, "no_speech_prob": 5.2242394303902984e-05}, {"id": 46, "seek": 34924, "start": 361.8, "end": 366.88, "text": " choice. Nearly always the best choice it turns out is actually to use something called natural", "tokens": [3922, 13, 38000, 1009, 264, 1151, 3922, 309, 4523, 484, 307, 767, 281, 764, 746, 1219, 3303], "temperature": 0.0, "avg_logprob": -0.19264934723635754, "compression_ratio": 1.759433962264151, "no_speech_prob": 5.2242394303902984e-05}, {"id": 47, "seek": 34924, "start": 366.88, "end": 373.92, "text": " cubic splines. And natural cubic splines are basically where you split your data set into", "tokens": [28733, 4732, 1652, 13, 400, 3303, 28733, 4732, 1652, 366, 1936, 689, 291, 7472, 428, 1412, 992, 666], "temperature": 0.0, "avg_logprob": -0.19264934723635754, "compression_ratio": 1.759433962264151, "no_speech_prob": 5.2242394303902984e-05}, {"id": 48, "seek": 37392, "start": 373.92, "end": 384.72, "text": " kind of sections of the domain and you connect each section up with a cubic. So each of these", "tokens": [733, 295, 10863, 295, 264, 9274, 293, 291, 1745, 1184, 3541, 493, 365, 257, 28733, 13, 407, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.1254063456891531, "compression_ratio": 1.814070351758794, "no_speech_prob": 8.939337021729443e-06}, {"id": 49, "seek": 37392, "start": 384.72, "end": 391.32, "text": " bits between dots of cubics and you create the basis such that these cubics connect up", "tokens": [9239, 1296, 15026, 295, 10057, 1167, 293, 291, 1884, 264, 5143, 1270, 300, 613, 10057, 1167, 1745, 493], "temperature": 0.0, "avg_logprob": -0.1254063456891531, "compression_ratio": 1.814070351758794, "no_speech_prob": 8.939337021729443e-06}, {"id": 50, "seek": 37392, "start": 391.32, "end": 395.36, "text": " with each other and their gradients connect up. And one of the interesting things that", "tokens": [365, 1184, 661, 293, 641, 2771, 2448, 1745, 493, 13, 400, 472, 295, 264, 1880, 721, 300], "temperature": 0.0, "avg_logprob": -0.1254063456891531, "compression_ratio": 1.814070351758794, "no_speech_prob": 8.939337021729443e-06}, {"id": 51, "seek": 37392, "start": 395.36, "end": 402.74, "text": " makes them natural splines is that the endpoints are actually linear rather than cubic, which", "tokens": [1669, 552, 3303, 4732, 1652, 307, 300, 264, 917, 20552, 366, 767, 8213, 2831, 813, 28733, 11, 597], "temperature": 0.0, "avg_logprob": -0.1254063456891531, "compression_ratio": 1.814070351758794, "no_speech_prob": 8.939337021729443e-06}, {"id": 52, "seek": 40274, "start": 402.74, "end": 411.76, "text": " actually makes these extrapolate outside the input domain really nicely. You can see as", "tokens": [767, 1669, 613, 48224, 473, 2380, 264, 4846, 9274, 534, 9594, 13, 509, 393, 536, 382], "temperature": 0.0, "avg_logprob": -0.12685696872664087, "compression_ratio": 1.765, "no_speech_prob": 2.0784342268598266e-05}, {"id": 53, "seek": 40274, "start": 411.76, "end": 415.76, "text": " you add more and more knots with just two knots you start out with a line and as you", "tokens": [291, 909, 544, 293, 544, 27426, 365, 445, 732, 27426, 291, 722, 484, 365, 257, 1622, 293, 382, 291], "temperature": 0.0, "avg_logprob": -0.12685696872664087, "compression_ratio": 1.765, "no_speech_prob": 2.0784342268598266e-05}, {"id": 54, "seek": 40274, "start": 415.76, "end": 423.28000000000003, "text": " add more knots you start to get more and more opportunities for curves. One of the cool", "tokens": [909, 544, 27426, 291, 722, 281, 483, 544, 293, 544, 4786, 337, 19490, 13, 1485, 295, 264, 1627], "temperature": 0.0, "avg_logprob": -0.12685696872664087, "compression_ratio": 1.765, "no_speech_prob": 2.0784342268598266e-05}, {"id": 55, "seek": 40274, "start": 423.28000000000003, "end": 427.76, "text": " things about natural splines, they're also called restricted cubic splines, is that actually", "tokens": [721, 466, 3303, 4732, 1652, 11, 436, 434, 611, 1219, 20608, 28733, 4732, 1652, 11, 307, 300, 767], "temperature": 0.0, "avg_logprob": -0.12685696872664087, "compression_ratio": 1.765, "no_speech_prob": 2.0784342268598266e-05}, {"id": 56, "seek": 42776, "start": 427.76, "end": 433.44, "text": " you don't have to think at all about where to put the knot points. It turns out that", "tokens": [291, 500, 380, 362, 281, 519, 412, 439, 466, 689, 281, 829, 264, 16966, 2793, 13, 467, 4523, 484, 300], "temperature": 0.0, "avg_logprob": -0.07853292309960654, "compression_ratio": 1.6635514018691588, "no_speech_prob": 5.01466820423957e-06}, {"id": 57, "seek": 42776, "start": 433.44, "end": 441.12, "text": " there's basically a set of quantiles where you can put the knot points pretty reliably", "tokens": [456, 311, 1936, 257, 992, 295, 4426, 4680, 689, 291, 393, 829, 264, 16966, 2793, 1238, 49927], "temperature": 0.0, "avg_logprob": -0.07853292309960654, "compression_ratio": 1.6635514018691588, "no_speech_prob": 5.01466820423957e-06}, {"id": 58, "seek": 42776, "start": 441.12, "end": 444.98, "text": " depending on how many knot points you want, which is independent of the data and nearly", "tokens": [5413, 322, 577, 867, 16966, 2793, 291, 528, 11, 597, 307, 6695, 295, 264, 1412, 293, 6217], "temperature": 0.0, "avg_logprob": -0.07853292309960654, "compression_ratio": 1.6635514018691588, "no_speech_prob": 5.01466820423957e-06}, {"id": 59, "seek": 42776, "start": 444.98, "end": 452.88, "text": " always works. So this was a nice trick. And then another nice trick is if you do use regularized", "tokens": [1009, 1985, 13, 407, 341, 390, 257, 1481, 4282, 13, 400, 550, 1071, 1481, 4282, 307, 498, 291, 360, 764, 3890, 1602], "temperature": 0.0, "avg_logprob": -0.07853292309960654, "compression_ratio": 1.6635514018691588, "no_speech_prob": 5.01466820423957e-06}, {"id": 60, "seek": 45288, "start": 452.88, "end": 458.08, "text": " regression, particularly L1 regularized regression I really like, you don't even have to be that", "tokens": [24590, 11, 4098, 441, 16, 3890, 1602, 24590, 286, 534, 411, 11, 291, 500, 380, 754, 362, 281, 312, 300], "temperature": 0.0, "avg_logprob": -0.15289112458746118, "compression_ratio": 1.7058823529411764, "no_speech_prob": 9.080346899281722e-06}, {"id": 61, "seek": 45288, "start": 458.08, "end": 462.64, "text": " careful about the number of parameters you include a lot of the time. So you can often", "tokens": [5026, 466, 264, 1230, 295, 9834, 291, 4090, 257, 688, 295, 264, 565, 13, 407, 291, 393, 2049], "temperature": 0.0, "avg_logprob": -0.15289112458746118, "compression_ratio": 1.7058823529411764, "no_speech_prob": 9.080346899281722e-06}, {"id": 62, "seek": 45288, "start": 462.64, "end": 469.71999999999997, "text": " include quite a lot of transformations, including actually, sorry, not transformations, so interactions,", "tokens": [4090, 1596, 257, 688, 295, 34852, 11, 3009, 767, 11, 2597, 11, 406, 34852, 11, 370, 13280, 11], "temperature": 0.0, "avg_logprob": -0.15289112458746118, "compression_ratio": 1.7058823529411764, "no_speech_prob": 9.080346899281722e-06}, {"id": 63, "seek": 45288, "start": 469.71999999999997, "end": 480.48, "text": " including interactions of natural cubic spline terms. So this is an approach that I used", "tokens": [3009, 13280, 295, 3303, 28733, 4732, 533, 2115, 13, 407, 341, 307, 364, 3109, 300, 286, 1143], "temperature": 0.0, "avg_logprob": -0.15289112458746118, "compression_ratio": 1.7058823529411764, "no_speech_prob": 9.080346899281722e-06}, {"id": 64, "seek": 48048, "start": 480.48, "end": 489.92, "text": " a lot and had a lot of success with. But then in the, I think it's 1999 that the first paper", "tokens": [257, 688, 293, 632, 257, 688, 295, 2245, 365, 13, 583, 550, 294, 264, 11, 286, 519, 309, 311, 19952, 300, 264, 700, 3035], "temperature": 0.0, "avg_logprob": -0.21007492807176378, "compression_ratio": 1.4148936170212767, "no_speech_prob": 7.4110303103225306e-06}, {"id": 65, "seek": 48048, "start": 489.92, "end": 496.32, "text": " appeared in the early, in kind of 2000, it started getting popular, was random forests.", "tokens": [8516, 294, 264, 2440, 11, 294, 733, 295, 8132, 11, 309, 1409, 1242, 3743, 11, 390, 4974, 21700, 13], "temperature": 0.0, "avg_logprob": -0.21007492807176378, "compression_ratio": 1.4148936170212767, "no_speech_prob": 7.4110303103225306e-06}, {"id": 66, "seek": 48048, "start": 496.32, "end": 503.64000000000004, "text": " And random forests, this is a picture from Terence Parr's excellent DTreeViz package.", "tokens": [400, 4974, 21700, 11, 341, 307, 257, 3036, 490, 6564, 655, 47890, 311, 7103, 413, 51, 701, 53, 590, 7372, 13], "temperature": 0.0, "avg_logprob": -0.21007492807176378, "compression_ratio": 1.4148936170212767, "no_speech_prob": 7.4110303103225306e-06}, {"id": 67, "seek": 50364, "start": 503.64, "end": 510.76, "text": " So forests are ensembles of decision trees, as I'm sure most of you know. And so for an", "tokens": [407, 21700, 366, 12567, 2504, 904, 295, 3537, 5852, 11, 382, 286, 478, 988, 881, 295, 291, 458, 13, 400, 370, 337, 364], "temperature": 0.0, "avg_logprob": -0.13399707569795496, "compression_ratio": 1.510989010989011, "no_speech_prob": 1.4822977618678124e-06}, {"id": 68, "seek": 50364, "start": 510.76, "end": 521.24, "text": " example of a decision tree, this is some data from the Kaggle competition, which is trying", "tokens": [1365, 295, 257, 3537, 4230, 11, 341, 307, 512, 1412, 490, 264, 48751, 22631, 6211, 11, 597, 307, 1382], "temperature": 0.0, "avg_logprob": -0.13399707569795496, "compression_ratio": 1.510989010989011, "no_speech_prob": 1.4822977618678124e-06}, {"id": 69, "seek": 50364, "start": 521.24, "end": 527.64, "text": " to predict the auction price of heavy industrial equipment. And you can see here that a decision", "tokens": [281, 6069, 264, 24139, 3218, 295, 4676, 9987, 5927, 13, 400, 291, 393, 536, 510, 300, 257, 3537], "temperature": 0.0, "avg_logprob": -0.13399707569795496, "compression_ratio": 1.510989010989011, "no_speech_prob": 1.4822977618678124e-06}, {"id": 70, "seek": 52764, "start": 527.64, "end": 534.0, "text": " tree has done a split on this binary variable of coupler system. And then for those which,", "tokens": [4230, 575, 1096, 257, 7472, 322, 341, 17434, 7006, 295, 1384, 22732, 1185, 13, 400, 550, 337, 729, 597, 11], "temperature": 0.0, "avg_logprob": -0.14279859809465306, "compression_ratio": 1.765, "no_speech_prob": 1.544576298329048e-05}, {"id": 71, "seek": 52764, "start": 534.0, "end": 539.42, "text": " I guess, don't have a coupler system, it did a binary split on year made. And those which", "tokens": [286, 2041, 11, 500, 380, 362, 257, 1384, 22732, 1185, 11, 309, 630, 257, 17434, 7472, 322, 1064, 1027, 13, 400, 729, 597], "temperature": 0.0, "avg_logprob": -0.14279859809465306, "compression_ratio": 1.765, "no_speech_prob": 1.544576298329048e-05}, {"id": 72, "seek": 52764, "start": 539.42, "end": 547.6, "text": " then were made in early years, then we can see immediately the sale price. So this is", "tokens": [550, 645, 1027, 294, 2440, 924, 11, 550, 321, 393, 536, 4258, 264, 8680, 3218, 13, 407, 341, 307], "temperature": 0.0, "avg_logprob": -0.14279859809465306, "compression_ratio": 1.765, "no_speech_prob": 1.544576298329048e-05}, {"id": 73, "seek": 52764, "start": 547.6, "end": 552.72, "text": " the thing we're trying to predict the sale price. And so in this case, we can see that", "tokens": [264, 551, 321, 434, 1382, 281, 6069, 264, 8680, 3218, 13, 400, 370, 294, 341, 1389, 11, 321, 393, 536, 300], "temperature": 0.0, "avg_logprob": -0.14279859809465306, "compression_ratio": 1.765, "no_speech_prob": 1.544576298329048e-05}, {"id": 74, "seek": 55272, "start": 552.72, "end": 559.32, "text": " it's in just four splits, it's successfully found some things, which this is actually", "tokens": [309, 311, 294, 445, 1451, 37741, 11, 309, 311, 10727, 1352, 512, 721, 11, 597, 341, 307, 767], "temperature": 0.0, "avg_logprob": -0.1584399870072288, "compression_ratio": 1.6388888888888888, "no_speech_prob": 6.540223239426268e-06}, {"id": 75, "seek": 55272, "start": 559.32, "end": 563.76, "text": " the log of sale price has done a really good job of splitting out the log of sale price.", "tokens": [264, 3565, 295, 8680, 3218, 575, 1096, 257, 534, 665, 1691, 295, 30348, 484, 264, 3565, 295, 8680, 3218, 13], "temperature": 0.0, "avg_logprob": -0.1584399870072288, "compression_ratio": 1.6388888888888888, "no_speech_prob": 6.540223239426268e-06}, {"id": 76, "seek": 55272, "start": 563.76, "end": 572.12, "text": " I actually used these single decision trees a little bit in the kind of early and mid", "tokens": [286, 767, 1143, 613, 2167, 3537, 5852, 257, 707, 857, 294, 264, 733, 295, 2440, 293, 2062], "temperature": 0.0, "avg_logprob": -0.1584399870072288, "compression_ratio": 1.6388888888888888, "no_speech_prob": 6.540223239426268e-06}, {"id": 77, "seek": 55272, "start": 572.12, "end": 582.0, "text": " 90s, but they were a nightmare to find something that fit adequately, but didn't overfit. And", "tokens": [4289, 82, 11, 457, 436, 645, 257, 18724, 281, 915, 746, 300, 3318, 41822, 11, 457, 994, 380, 670, 6845, 13, 400], "temperature": 0.0, "avg_logprob": -0.1584399870072288, "compression_ratio": 1.6388888888888888, "no_speech_prob": 6.540223239426268e-06}, {"id": 78, "seek": 58200, "start": 582.0, "end": 589.36, "text": " random forests then came along thanks to Breiman, who a very interesting guy. He was originally", "tokens": [4974, 21700, 550, 1361, 2051, 3231, 281, 7090, 25504, 11, 567, 257, 588, 1880, 2146, 13, 634, 390, 7993], "temperature": 0.0, "avg_logprob": -0.1405840608262524, "compression_ratio": 1.7011494252873562, "no_speech_prob": 2.4678400222910568e-05}, {"id": 79, "seek": 58200, "start": 589.36, "end": 594.56, "text": " a math professor at Berkeley. And then he went out into industry and was basically a", "tokens": [257, 5221, 8304, 412, 23684, 13, 400, 550, 415, 1437, 484, 666, 3518, 293, 390, 1936, 257], "temperature": 0.0, "avg_logprob": -0.1405840608262524, "compression_ratio": 1.7011494252873562, "no_speech_prob": 2.4678400222910568e-05}, {"id": 80, "seek": 58200, "start": 594.56, "end": 600.56, "text": " consultant, I think, for years and then came back to Berkeley to do statistics. And he", "tokens": [24676, 11, 286, 519, 11, 337, 924, 293, 550, 1361, 646, 281, 23684, 281, 360, 12523, 13, 400, 415], "temperature": 0.0, "avg_logprob": -0.1405840608262524, "compression_ratio": 1.7011494252873562, "no_speech_prob": 2.4678400222910568e-05}, {"id": 81, "seek": 58200, "start": 600.56, "end": 606.08, "text": " was incredibly effective in creating like really practical algorithms. And the random", "tokens": [390, 6252, 4942, 294, 4084, 411, 534, 8496, 14642, 13, 400, 264, 4974], "temperature": 0.0, "avg_logprob": -0.1405840608262524, "compression_ratio": 1.7011494252873562, "no_speech_prob": 2.4678400222910568e-05}, {"id": 82, "seek": 58200, "start": 606.08, "end": 611.68, "text": " forest is one that's really been world changing, incredibly simple. You just randomly pick", "tokens": [6719, 307, 472, 300, 311, 534, 668, 1002, 4473, 11, 6252, 2199, 13, 509, 445, 16979, 1888], "temperature": 0.0, "avg_logprob": -0.1405840608262524, "compression_ratio": 1.7011494252873562, "no_speech_prob": 2.4678400222910568e-05}, {"id": 83, "seek": 61168, "start": 611.68, "end": 618.16, "text": " a subset of your data and you then train a model, train it, you know, just create a decision", "tokens": [257, 25993, 295, 428, 1412, 293, 291, 550, 3847, 257, 2316, 11, 3847, 309, 11, 291, 458, 11, 445, 1884, 257, 3537], "temperature": 0.0, "avg_logprob": -0.10647983765334226, "compression_ratio": 1.7918781725888324, "no_speech_prob": 1.4063389244256541e-05}, {"id": 84, "seek": 61168, "start": 618.16, "end": 624.52, "text": " tree with a subset. You save it and then you repeat steps one, two, and three again and", "tokens": [4230, 365, 257, 25993, 13, 509, 3155, 309, 293, 550, 291, 7149, 4439, 472, 11, 732, 11, 293, 1045, 797, 293], "temperature": 0.0, "avg_logprob": -0.10647983765334226, "compression_ratio": 1.7918781725888324, "no_speech_prob": 1.4063389244256541e-05}, {"id": 85, "seek": 61168, "start": 624.52, "end": 628.04, "text": " again and again, creating lots and lots of decision trees on different random subsets", "tokens": [797, 293, 797, 11, 4084, 3195, 293, 3195, 295, 3537, 5852, 322, 819, 4974, 2090, 1385], "temperature": 0.0, "avg_logprob": -0.10647983765334226, "compression_ratio": 1.7918781725888324, "no_speech_prob": 1.4063389244256541e-05}, {"id": 86, "seek": 61168, "start": 628.04, "end": 633.8399999999999, "text": " of the data. And it turns out that if you average the results of all these models, you", "tokens": [295, 264, 1412, 13, 400, 309, 4523, 484, 300, 498, 291, 4274, 264, 3542, 295, 439, 613, 5245, 11, 291], "temperature": 0.0, "avg_logprob": -0.10647983765334226, "compression_ratio": 1.7918781725888324, "no_speech_prob": 1.4063389244256541e-05}, {"id": 87, "seek": 63384, "start": 633.84, "end": 647.08, "text": " get predictions that are unbiased, accurate, and don't overfit. And it's a really, really", "tokens": [483, 21264, 300, 366, 517, 5614, 1937, 11, 8559, 11, 293, 500, 380, 670, 6845, 13, 400, 309, 311, 257, 534, 11, 534], "temperature": 0.0, "avg_logprob": -0.13894544763767974, "compression_ratio": 1.5739130434782609, "no_speech_prob": 2.078316902043298e-05}, {"id": 88, "seek": 63384, "start": 647.08, "end": 653.0400000000001, "text": " cool approach. So basically, as soon as this came out, I added it to my arsenal. One of", "tokens": [1627, 3109, 13, 407, 1936, 11, 382, 2321, 382, 341, 1361, 484, 11, 286, 3869, 309, 281, 452, 42227, 13, 1485, 295], "temperature": 0.0, "avg_logprob": -0.13894544763767974, "compression_ratio": 1.5739130434782609, "no_speech_prob": 2.078316902043298e-05}, {"id": 89, "seek": 63384, "start": 653.0400000000001, "end": 656.84, "text": " the really nice things about this is how quickly you can implement it. We implemented it in", "tokens": [264, 534, 1481, 721, 466, 341, 307, 577, 2661, 291, 393, 4445, 309, 13, 492, 12270, 309, 294], "temperature": 0.0, "avg_logprob": -0.13894544763767974, "compression_ratio": 1.5739130434782609, "no_speech_prob": 2.078316902043298e-05}, {"id": 90, "seek": 63384, "start": 656.84, "end": 663.8000000000001, "text": " like a day, basically. So this came out when I was running a company called Optimal Decision", "tokens": [411, 257, 786, 11, 1936, 13, 407, 341, 1361, 484, 562, 286, 390, 2614, 257, 2237, 1219, 21455, 10650, 12427, 1991], "temperature": 0.0, "avg_logprob": -0.13894544763767974, "compression_ratio": 1.5739130434782609, "no_speech_prob": 2.078316902043298e-05}, {"id": 91, "seek": 66380, "start": 663.8, "end": 672.0799999999999, "text": " which I built to help insurers come up with better prices, which is the most important", "tokens": [597, 286, 3094, 281, 854, 1028, 14198, 808, 493, 365, 1101, 7901, 11, 597, 307, 264, 881, 1021], "temperature": 0.0, "avg_logprob": -0.1447357433598216, "compression_ratio": 1.7254901960784315, "no_speech_prob": 6.338956154650077e-06}, {"id": 92, "seek": 66380, "start": 672.0799999999999, "end": 680.4799999999999, "text": " thing for insurers to do. One of the interesting things about this for me is that we never", "tokens": [551, 337, 1028, 14198, 281, 360, 13, 1485, 295, 264, 1880, 721, 466, 341, 337, 385, 307, 300, 321, 1128], "temperature": 0.0, "avg_logprob": -0.1447357433598216, "compression_ratio": 1.7254901960784315, "no_speech_prob": 6.338956154650077e-06}, {"id": 93, "seek": 66380, "start": 680.4799999999999, "end": 688.3599999999999, "text": " actually deployed a random forest. What we did was we used random forests to understand", "tokens": [767, 17826, 257, 4974, 6719, 13, 708, 321, 630, 390, 321, 1143, 4974, 21700, 281, 1223], "temperature": 0.0, "avg_logprob": -0.1447357433598216, "compression_ratio": 1.7254901960784315, "no_speech_prob": 6.338956154650077e-06}, {"id": 94, "seek": 66380, "start": 688.3599999999999, "end": 693.28, "text": " the data. And then we use that understanding of the data to then go back and basically", "tokens": [264, 1412, 13, 400, 550, 321, 764, 300, 3701, 295, 264, 1412, 281, 550, 352, 646, 293, 1936], "temperature": 0.0, "avg_logprob": -0.1447357433598216, "compression_ratio": 1.7254901960784315, "no_speech_prob": 6.338956154650077e-06}, {"id": 95, "seek": 69328, "start": 693.28, "end": 700.16, "text": " build more traditional regression models with the particular terms and transformations and", "tokens": [1322, 544, 5164, 24590, 5245, 365, 264, 1729, 2115, 293, 34852, 293], "temperature": 0.0, "avg_logprob": -0.09447233604662346, "compression_ratio": 1.8, "no_speech_prob": 1.4284817552834284e-05}, {"id": 96, "seek": 69328, "start": 700.16, "end": 704.9599999999999, "text": " interactions that the random forest found were important. So basically, this is one", "tokens": [13280, 300, 264, 4974, 6719, 1352, 645, 1021, 13, 407, 1936, 11, 341, 307, 472], "temperature": 0.0, "avg_logprob": -0.09447233604662346, "compression_ratio": 1.8, "no_speech_prob": 1.4284817552834284e-05}, {"id": 97, "seek": 69328, "start": 704.9599999999999, "end": 712.1999999999999, "text": " of the cool things that you get out of a random forest. It's a feature importance plot. And", "tokens": [295, 264, 1627, 721, 300, 291, 483, 484, 295, 257, 4974, 6719, 13, 467, 311, 257, 4111, 7379, 7542, 13, 400], "temperature": 0.0, "avg_logprob": -0.09447233604662346, "compression_ratio": 1.8, "no_speech_prob": 1.4284817552834284e-05}, {"id": 98, "seek": 69328, "start": 712.1999999999999, "end": 717.16, "text": " it shows you, so this is again the same data set, the auction price data set from Kaggle.", "tokens": [309, 3110, 291, 11, 370, 341, 307, 797, 264, 912, 1412, 992, 11, 264, 24139, 3218, 1412, 992, 490, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.09447233604662346, "compression_ratio": 1.8, "no_speech_prob": 1.4284817552834284e-05}, {"id": 99, "seek": 69328, "start": 717.16, "end": 722.48, "text": " It shows you which are the most important features. And the nice thing about this is", "tokens": [467, 3110, 291, 597, 366, 264, 881, 1021, 4122, 13, 400, 264, 1481, 551, 466, 341, 307], "temperature": 0.0, "avg_logprob": -0.09447233604662346, "compression_ratio": 1.8, "no_speech_prob": 1.4284817552834284e-05}, {"id": 100, "seek": 72248, "start": 722.48, "end": 729.16, "text": " you don't have to do any transformations or think about interactions or nonlinearities", "tokens": [291, 500, 380, 362, 281, 360, 604, 34852, 420, 519, 466, 13280, 420, 2107, 28263, 1088], "temperature": 0.0, "avg_logprob": -0.10660920824323382, "compression_ratio": 1.5954545454545455, "no_speech_prob": 2.5214797005901346e-06}, {"id": 101, "seek": 72248, "start": 729.16, "end": 735.48, "text": " because they're using decision trees behind the scenes. It all just works. And so I kind", "tokens": [570, 436, 434, 1228, 3537, 5852, 2261, 264, 8026, 13, 467, 439, 445, 1985, 13, 400, 370, 286, 733], "temperature": 0.0, "avg_logprob": -0.10660920824323382, "compression_ratio": 1.5954545454545455, "no_speech_prob": 2.5214797005901346e-06}, {"id": 102, "seek": 72248, "start": 735.48, "end": 741.6800000000001, "text": " of developed this pretty simple approach where I would first create a random forest and I", "tokens": [295, 4743, 341, 1238, 2199, 3109, 689, 286, 576, 700, 1884, 257, 4974, 6719, 293, 286], "temperature": 0.0, "avg_logprob": -0.10660920824323382, "compression_ratio": 1.5954545454545455, "no_speech_prob": 2.5214797005901346e-06}, {"id": 103, "seek": 72248, "start": 741.6800000000001, "end": 747.0, "text": " would then find which features and so forth are useful. I then use partial dependence", "tokens": [576, 550, 915, 597, 4122, 293, 370, 5220, 366, 4420, 13, 286, 550, 764, 14641, 31704], "temperature": 0.0, "avg_logprob": -0.10660920824323382, "compression_ratio": 1.5954545454545455, "no_speech_prob": 2.5214797005901346e-06}, {"id": 104, "seek": 74700, "start": 747.0, "end": 752.6, "text": " plots to kind of look at the shapes of them. And then I would go back and kind of for the", "tokens": [28609, 281, 733, 295, 574, 412, 264, 10854, 295, 552, 13, 400, 550, 286, 576, 352, 646, 293, 733, 295, 337, 264], "temperature": 0.0, "avg_logprob": -0.15120829413918888, "compression_ratio": 1.6540284360189574, "no_speech_prob": 1.9333288037159946e-06}, {"id": 105, "seek": 74700, "start": 752.6, "end": 758.04, "text": " continuous variables that matter, create the cubic splines and create the interactions", "tokens": [10957, 9102, 300, 1871, 11, 1884, 264, 28733, 4732, 1652, 293, 1884, 264, 13280], "temperature": 0.0, "avg_logprob": -0.15120829413918888, "compression_ratio": 1.6540284360189574, "no_speech_prob": 1.9333288037159946e-06}, {"id": 106, "seek": 74700, "start": 758.04, "end": 766.56, "text": " and then do a regression. And so this basic kind of trick was incredibly powerful. And", "tokens": [293, 550, 360, 257, 24590, 13, 400, 370, 341, 3875, 733, 295, 4282, 390, 6252, 4005, 13, 400], "temperature": 0.0, "avg_logprob": -0.15120829413918888, "compression_ratio": 1.6540284360189574, "no_speech_prob": 1.9333288037159946e-06}, {"id": 107, "seek": 74700, "start": 766.56, "end": 775.68, "text": " I used it, a variance of it, in the early days of Kaggle amongst other things and got", "tokens": [286, 1143, 309, 11, 257, 21977, 295, 309, 11, 294, 264, 2440, 1708, 295, 48751, 22631, 12918, 661, 721, 293, 658], "temperature": 0.0, "avg_logprob": -0.15120829413918888, "compression_ratio": 1.6540284360189574, "no_speech_prob": 1.9333288037159946e-06}, {"id": 108, "seek": 77568, "start": 775.68, "end": 781.56, "text": " to number one in the world and won a number of competitions. And funnily enough, actually,", "tokens": [281, 1230, 472, 294, 264, 1002, 293, 1582, 257, 1230, 295, 26185, 13, 400, 1019, 77, 953, 1547, 11, 767, 11], "temperature": 0.0, "avg_logprob": -0.14003169897830847, "compression_ratio": 1.478021978021978, "no_speech_prob": 7.76654724177206e-06}, {"id": 109, "seek": 77568, "start": 781.56, "end": 788.7199999999999, "text": " back in 2011, I described my approaches to Kaggle competitions in Melbourne at the Melbourne", "tokens": [646, 294, 10154, 11, 286, 7619, 452, 11587, 281, 48751, 22631, 26185, 294, 27496, 412, 264, 27496], "temperature": 0.0, "avg_logprob": -0.14003169897830847, "compression_ratio": 1.478021978021978, "no_speech_prob": 7.76654724177206e-06}, {"id": 110, "seek": 77568, "start": 788.7199999999999, "end": 796.8399999999999, "text": " R Meetup. And you can still find that talk on YouTube. And it's actually still pretty", "tokens": [497, 22963, 1010, 13, 400, 291, 393, 920, 915, 300, 751, 322, 3088, 13, 400, 309, 311, 767, 920, 1238], "temperature": 0.0, "avg_logprob": -0.14003169897830847, "compression_ratio": 1.478021978021978, "no_speech_prob": 7.76654724177206e-06}, {"id": 111, "seek": 79684, "start": 796.84, "end": 809.36, "text": " much just as relevant today as it was at that time. So this is 2011. And I became the chief", "tokens": [709, 445, 382, 7340, 965, 382, 309, 390, 412, 300, 565, 13, 407, 341, 307, 10154, 13, 400, 286, 3062, 264, 9588], "temperature": 0.0, "avg_logprob": -0.0898167833368829, "compression_ratio": 1.313868613138686, "no_speech_prob": 4.6372674660233315e-06}, {"id": 112, "seek": 79684, "start": 809.36, "end": 819.08, "text": " scientist and president at Kaggle. And we took it over to the US and got venture capital", "tokens": [12662, 293, 3868, 412, 48751, 22631, 13, 400, 321, 1890, 309, 670, 281, 264, 2546, 293, 658, 18474, 4238], "temperature": 0.0, "avg_logprob": -0.0898167833368829, "compression_ratio": 1.313868613138686, "no_speech_prob": 4.6372674660233315e-06}, {"id": 113, "seek": 81908, "start": 819.08, "end": 827.8000000000001, "text": " and built into quite a successful business. But something interesting that happened as", "tokens": [293, 3094, 666, 1596, 257, 4406, 1606, 13, 583, 746, 1880, 300, 2011, 382], "temperature": 0.0, "avg_logprob": -0.13069409132003784, "compression_ratio": 1.583710407239819, "no_speech_prob": 8.79877643455984e-06}, {"id": 114, "seek": 81908, "start": 827.8000000000001, "end": 835.08, "text": " chief scientist of Kaggle, I was getting to see all the competitions up close. And seven", "tokens": [9588, 12662, 295, 48751, 22631, 11, 286, 390, 1242, 281, 536, 439, 264, 26185, 493, 1998, 13, 400, 3407], "temperature": 0.0, "avg_logprob": -0.13069409132003784, "compression_ratio": 1.583710407239819, "no_speech_prob": 8.79877643455984e-06}, {"id": 115, "seek": 81908, "start": 835.08, "end": 844.36, "text": " years ago, there was a competition, Dogs vs Cats, which you can still see actually on", "tokens": [924, 2057, 11, 456, 390, 257, 6211, 11, 35504, 12041, 40902, 11, 597, 291, 393, 920, 536, 767, 322], "temperature": 0.0, "avg_logprob": -0.13069409132003784, "compression_ratio": 1.583710407239819, "no_speech_prob": 8.79877643455984e-06}, {"id": 116, "seek": 81908, "start": 844.36, "end": 848.48, "text": " the Dogs vs Cats Kaggle page, it describes the state of the art approach for recognizing", "tokens": [264, 35504, 12041, 40902, 48751, 22631, 3028, 11, 309, 15626, 264, 1785, 295, 264, 1523, 3109, 337, 18538], "temperature": 0.0, "avg_logprob": -0.13069409132003784, "compression_ratio": 1.583710407239819, "no_speech_prob": 8.79877643455984e-06}, {"id": 117, "seek": 84848, "start": 848.48, "end": 857.12, "text": " dogs vs cats as being around about 80% accuracy. And so that was based on the academic papers", "tokens": [7197, 12041, 11111, 382, 885, 926, 466, 4688, 4, 14170, 13, 400, 370, 300, 390, 2361, 322, 264, 7778, 10577], "temperature": 0.0, "avg_logprob": -0.08118664423624675, "compression_ratio": 1.6118721461187215, "no_speech_prob": 3.611482270571287e-06}, {"id": 118, "seek": 84848, "start": 857.12, "end": 861.96, "text": " that had tackled this problem at the time. And then in this competition that just ran", "tokens": [300, 632, 9426, 1493, 341, 1154, 412, 264, 565, 13, 400, 550, 294, 341, 6211, 300, 445, 5872], "temperature": 0.0, "avg_logprob": -0.08118664423624675, "compression_ratio": 1.6118721461187215, "no_speech_prob": 3.611482270571287e-06}, {"id": 119, "seek": 84848, "start": 861.96, "end": 872.12, "text": " for three months, eight teams reached 98% accuracy and one nearly got to 99% accuracy.", "tokens": [337, 1045, 2493, 11, 3180, 5491, 6488, 20860, 4, 14170, 293, 472, 6217, 658, 281, 11803, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.08118664423624675, "compression_ratio": 1.6118721461187215, "no_speech_prob": 3.611482270571287e-06}, {"id": 120, "seek": 84848, "start": 872.12, "end": 878.1800000000001, "text": " So if you think about this as a 20% error rate, and this is basically a 1% error rate,", "tokens": [407, 498, 291, 519, 466, 341, 382, 257, 945, 4, 6713, 3314, 11, 293, 341, 307, 1936, 257, 502, 4, 6713, 3314, 11], "temperature": 0.0, "avg_logprob": -0.08118664423624675, "compression_ratio": 1.6118721461187215, "no_speech_prob": 3.611482270571287e-06}, {"id": 121, "seek": 87818, "start": 878.18, "end": 884.2399999999999, "text": " so this competition brought the state of the art down by about 20 times in three months,", "tokens": [370, 341, 6211, 3038, 264, 1785, 295, 264, 1523, 760, 538, 466, 945, 1413, 294, 1045, 2493, 11], "temperature": 0.0, "avg_logprob": -0.1170472502708435, "compression_ratio": 1.6227272727272728, "no_speech_prob": 4.710389021056471e-06}, {"id": 122, "seek": 87818, "start": 884.2399999999999, "end": 890.76, "text": " which is really extraordinary. It's really unheard of to see an academic state of the", "tokens": [597, 307, 534, 10581, 13, 467, 311, 534, 517, 42915, 295, 281, 536, 364, 7778, 1785, 295, 264], "temperature": 0.0, "avg_logprob": -0.1170472502708435, "compression_ratio": 1.6227272727272728, "no_speech_prob": 4.710389021056471e-06}, {"id": 123, "seek": 87818, "start": 890.76, "end": 899.12, "text": " art result that has been carefully studied, slashed by 20 acts by somebody working for", "tokens": [1523, 1874, 300, 575, 668, 7500, 9454, 11, 1061, 12219, 538, 945, 10672, 538, 2618, 1364, 337], "temperature": 0.0, "avg_logprob": -0.1170472502708435, "compression_ratio": 1.6227272727272728, "no_speech_prob": 4.710389021056471e-06}, {"id": 124, "seek": 87818, "start": 899.12, "end": 905.0799999999999, "text": " just three months on the problem. That's normally something that might take decades or hundreds", "tokens": [445, 1045, 2493, 322, 264, 1154, 13, 663, 311, 5646, 746, 300, 1062, 747, 7878, 420, 6779], "temperature": 0.0, "avg_logprob": -0.1170472502708435, "compression_ratio": 1.6227272727272728, "no_speech_prob": 4.710389021056471e-06}, {"id": 125, "seek": 90508, "start": 905.08, "end": 912.36, "text": " of years if it's possible at all. So something clearly happened here. And of course, what", "tokens": [295, 924, 498, 309, 311, 1944, 412, 439, 13, 407, 746, 4448, 2011, 510, 13, 400, 295, 1164, 11, 437], "temperature": 0.0, "avg_logprob": -0.14939563218937363, "compression_ratio": 1.6363636363636365, "no_speech_prob": 8.267522389360238e-06}, {"id": 126, "seek": 90508, "start": 912.36, "end": 921.9200000000001, "text": " happened was deep learning. And Pierre actually had developed one of the early deep learning", "tokens": [2011, 390, 2452, 2539, 13, 400, 28461, 767, 632, 4743, 472, 295, 264, 2440, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.14939563218937363, "compression_ratio": 1.6363636363636365, "no_speech_prob": 8.267522389360238e-06}, {"id": 127, "seek": 90508, "start": 921.9200000000001, "end": 928.1600000000001, "text": " libraries. And actually, even this kind of signal on Kaggle was in some ways a little", "tokens": [15148, 13, 400, 767, 11, 754, 341, 733, 295, 6358, 322, 48751, 22631, 390, 294, 512, 2098, 257, 707], "temperature": 0.0, "avg_logprob": -0.14939563218937363, "compression_ratio": 1.6363636363636365, "no_speech_prob": 8.267522389360238e-06}, {"id": 128, "seek": 90508, "start": 928.1600000000001, "end": 934.0, "text": " late. If you actually look at Pierre's Google Scholar, you'll see that it was actually back", "tokens": [3469, 13, 759, 291, 767, 574, 412, 28461, 311, 3329, 2065, 15276, 11, 291, 603, 536, 300, 309, 390, 767, 646], "temperature": 0.0, "avg_logprob": -0.14939563218937363, "compression_ratio": 1.6363636363636365, "no_speech_prob": 8.267522389360238e-06}, {"id": 129, "seek": 93400, "start": 934.0, "end": 943.88, "text": " in 2011 that him and Jan LeCun had already produced a system that was better than human", "tokens": [294, 10154, 300, 796, 293, 4956, 1456, 34, 409, 632, 1217, 7126, 257, 1185, 300, 390, 1101, 813, 1952], "temperature": 0.0, "avg_logprob": -0.1461839505604335, "compression_ratio": 1.4715909090909092, "no_speech_prob": 5.09282745042583e-06}, {"id": 130, "seek": 93400, "start": 943.88, "end": 950.12, "text": " performance at recognizing traffic signs. And so this was actually the first time that", "tokens": [3389, 412, 18538, 6419, 7880, 13, 400, 370, 341, 390, 767, 264, 700, 565, 300], "temperature": 0.0, "avg_logprob": -0.1461839505604335, "compression_ratio": 1.4715909090909092, "no_speech_prob": 5.09282745042583e-06}, {"id": 131, "seek": 93400, "start": 950.12, "end": 961.36, "text": " I noticed this really extraordinary thing, which was deep learning being better than", "tokens": [286, 5694, 341, 534, 10581, 551, 11, 597, 390, 2452, 2539, 885, 1101, 813], "temperature": 0.0, "avg_logprob": -0.1461839505604335, "compression_ratio": 1.4715909090909092, "no_speech_prob": 5.09282745042583e-06}, {"id": 132, "seek": 96136, "start": 961.36, "end": 976.48, "text": " humans at very human tasks, like looking at pictures. And so in 2011, I thought, wow,", "tokens": [6255, 412, 588, 1952, 9608, 11, 411, 1237, 412, 5242, 13, 400, 370, 294, 10154, 11, 286, 1194, 11, 6076, 11], "temperature": 0.0, "avg_logprob": -0.1383588155110677, "compression_ratio": 1.4475138121546962, "no_speech_prob": 4.068295311299153e-05}, {"id": 133, "seek": 96136, "start": 976.48, "end": 980.8000000000001, "text": " that's super interesting, but it's hard to do anything with that information because", "tokens": [300, 311, 1687, 1880, 11, 457, 309, 311, 1152, 281, 360, 1340, 365, 300, 1589, 570], "temperature": 0.0, "avg_logprob": -0.1383588155110677, "compression_ratio": 1.4475138121546962, "no_speech_prob": 4.068295311299153e-05}, {"id": 134, "seek": 96136, "start": 980.8000000000001, "end": 987.04, "text": " there wasn't any open source software or even any commercial software available to actually", "tokens": [456, 2067, 380, 604, 1269, 4009, 4722, 420, 754, 604, 6841, 4722, 2435, 281, 767], "temperature": 0.0, "avg_logprob": -0.1383588155110677, "compression_ratio": 1.4475138121546962, "no_speech_prob": 4.068295311299153e-05}, {"id": 135, "seek": 98704, "start": 987.04, "end": 993.52, "text": " do it. There was J\u00fcrgen Schmidt Huber's lab had a kind of like a DLL or something or a", "tokens": [360, 309, 13, 821, 390, 508, 1655, 1766, 42621, 389, 10261, 311, 2715, 632, 257, 733, 295, 411, 257, 413, 24010, 420, 746, 420, 257], "temperature": 0.0, "avg_logprob": -0.22960202353341239, "compression_ratio": 1.4475138121546962, "no_speech_prob": 5.954659627604997e-06}, {"id": 136, "seek": 98704, "start": 993.52, "end": 1001.5999999999999, "text": " library you could buy from them to do it, although they didn't even have a demo. There", "tokens": [6405, 291, 727, 2256, 490, 552, 281, 360, 309, 11, 4878, 436, 994, 380, 754, 362, 257, 10723, 13, 821], "temperature": 0.0, "avg_logprob": -0.22960202353341239, "compression_ratio": 1.4475138121546962, "no_speech_prob": 5.954659627604997e-06}, {"id": 137, "seek": 98704, "start": 1001.5999999999999, "end": 1009.1999999999999, "text": " wasn't any online services and there wasn't any, nobody had published anywhere like the", "tokens": [2067, 380, 604, 2950, 3328, 293, 456, 2067, 380, 604, 11, 5079, 632, 6572, 4992, 411, 264], "temperature": 0.0, "avg_logprob": -0.22960202353341239, "compression_ratio": 1.4475138121546962, "no_speech_prob": 5.954659627604997e-06}, {"id": 138, "seek": 100920, "start": 1009.2, "end": 1018.0, "text": " actual recipe book of like, how the hell do you do these things? And so that was a huge", "tokens": [3539, 6782, 1446, 295, 411, 11, 577, 264, 4921, 360, 291, 360, 613, 721, 30, 400, 370, 300, 390, 257, 2603], "temperature": 0.0, "avg_logprob": -0.12241288748654452, "compression_ratio": 1.614678899082569, "no_speech_prob": 3.5006289635930443e-06}, {"id": 139, "seek": 100920, "start": 1018.0, "end": 1023.88, "text": " challenge. It's exciting to see that this is possible, but then it's like, well, what", "tokens": [3430, 13, 467, 311, 4670, 281, 536, 300, 341, 307, 1944, 11, 457, 550, 309, 311, 411, 11, 731, 11, 437], "temperature": 0.0, "avg_logprob": -0.12241288748654452, "compression_ratio": 1.614678899082569, "no_speech_prob": 3.5006289635930443e-06}, {"id": 140, "seek": 100920, "start": 1023.88, "end": 1028.3600000000001, "text": " do I do about it? But one of the cool things is that at this exact moment, this dogs and", "tokens": [360, 286, 360, 466, 309, 30, 583, 472, 295, 264, 1627, 721, 307, 300, 412, 341, 1900, 1623, 11, 341, 7197, 293], "temperature": 0.0, "avg_logprob": -0.12241288748654452, "compression_ratio": 1.614678899082569, "no_speech_prob": 3.5006289635930443e-06}, {"id": 141, "seek": 100920, "start": 1028.3600000000001, "end": 1037.38, "text": " cats moment is when two really accessible open source libraries appeared, allowing people", "tokens": [11111, 1623, 307, 562, 732, 534, 9515, 1269, 4009, 15148, 8516, 11, 8293, 561], "temperature": 0.0, "avg_logprob": -0.12241288748654452, "compression_ratio": 1.614678899082569, "no_speech_prob": 3.5006289635930443e-06}, {"id": 142, "seek": 103738, "start": 1037.38, "end": 1045.0, "text": " to actually create their own deep learning models for the first time. And critically,", "tokens": [281, 767, 1884, 641, 1065, 2452, 2539, 5245, 337, 264, 700, 565, 13, 400, 22797, 11], "temperature": 0.0, "avg_logprob": -0.1161143981804282, "compression_ratio": 1.4162162162162162, "no_speech_prob": 2.3319614683714462e-06}, {"id": 143, "seek": 103738, "start": 1045.0, "end": 1051.88, "text": " they were built on top of CUDA, which was a dramatically more convenient way of programming", "tokens": [436, 645, 3094, 322, 1192, 295, 29777, 7509, 11, 597, 390, 257, 17548, 544, 10851, 636, 295, 9410], "temperature": 0.0, "avg_logprob": -0.1161143981804282, "compression_ratio": 1.4162162162162162, "no_speech_prob": 2.3319614683714462e-06}, {"id": 144, "seek": 103738, "start": 1051.88, "end": 1058.88, "text": " GPUs than had previously existed. So kind of things started to come together, really", "tokens": [18407, 82, 813, 632, 8046, 13135, 13, 407, 733, 295, 721, 1409, 281, 808, 1214, 11, 534], "temperature": 0.0, "avg_logprob": -0.1161143981804282, "compression_ratio": 1.4162162162162162, "no_speech_prob": 2.3319614683714462e-06}, {"id": 145, "seek": 105888, "start": 1058.88, "end": 1070.5600000000002, "text": " seven years ago, a little bit. I had been interested in neural networks since the very", "tokens": [3407, 924, 2057, 11, 257, 707, 857, 13, 286, 632, 668, 3102, 294, 18161, 9590, 1670, 264, 588], "temperature": 0.0, "avg_logprob": -0.1147061641399677, "compression_ratio": 1.5083798882681565, "no_speech_prob": 3.041363697775523e-06}, {"id": 146, "seek": 105888, "start": 1070.5600000000002, "end": 1079.3200000000002, "text": " start of my career. And in fact, in consulting, I worked with one of the big Australian banks", "tokens": [722, 295, 452, 3988, 13, 400, 294, 1186, 11, 294, 23682, 11, 286, 2732, 365, 472, 295, 264, 955, 13337, 10237], "temperature": 0.0, "avg_logprob": -0.1147061641399677, "compression_ratio": 1.5083798882681565, "no_speech_prob": 3.041363697775523e-06}, {"id": 147, "seek": 105888, "start": 1079.3200000000002, "end": 1088.1200000000001, "text": " on implementing a neural network in the early to mid 90s to help with targeted marketing,", "tokens": [322, 18114, 257, 18161, 3209, 294, 264, 2440, 281, 2062, 4289, 82, 281, 854, 365, 15045, 6370, 11], "temperature": 0.0, "avg_logprob": -0.1147061641399677, "compression_ratio": 1.5083798882681565, "no_speech_prob": 3.041363697775523e-06}, {"id": 148, "seek": 108812, "start": 1088.12, "end": 1097.0, "text": " not a very exciting application, I'll give you. But it really struck me at the time that", "tokens": [406, 257, 588, 4670, 3861, 11, 286, 603, 976, 291, 13, 583, 309, 534, 13159, 385, 412, 264, 565, 300], "temperature": 0.0, "avg_logprob": -0.14055329699848973, "compression_ratio": 1.5104602510460252, "no_speech_prob": 1.952334241650533e-05}, {"id": 149, "seek": 108812, "start": 1097.0, "end": 1102.9199999999998, "text": " this was a technology which I felt like at some point would probably take over just about", "tokens": [341, 390, 257, 2899, 597, 286, 2762, 411, 412, 512, 935, 576, 1391, 747, 670, 445, 466], "temperature": 0.0, "avg_logprob": -0.14055329699848973, "compression_ratio": 1.5104602510460252, "no_speech_prob": 1.952334241650533e-05}, {"id": 150, "seek": 108812, "start": 1102.9199999999998, "end": 1108.1599999999999, "text": " everything else in terms of my area of interest around predictive modeling. And we actually", "tokens": [1203, 1646, 294, 2115, 295, 452, 1859, 295, 1179, 926, 35521, 15983, 13, 400, 321, 767], "temperature": 0.0, "avg_logprob": -0.14055329699848973, "compression_ratio": 1.5104602510460252, "no_speech_prob": 1.952334241650533e-05}, {"id": 151, "seek": 108812, "start": 1108.1599999999999, "end": 1117.3999999999999, "text": " had quite a bit of success with it even then. So that's like 30 years ago now, nearly. But", "tokens": [632, 1596, 257, 857, 295, 2245, 365, 309, 754, 550, 13, 407, 300, 311, 411, 2217, 924, 2057, 586, 11, 6217, 13, 583], "temperature": 0.0, "avg_logprob": -0.14055329699848973, "compression_ratio": 1.5104602510460252, "no_speech_prob": 1.952334241650533e-05}, {"id": 152, "seek": 111740, "start": 1117.4, "end": 1120.88, "text": " there were some issues back then. For one thing, we had to buy custom hardware that", "tokens": [456, 645, 512, 2663, 646, 550, 13, 1171, 472, 551, 11, 321, 632, 281, 2256, 2375, 8837, 300], "temperature": 0.0, "avg_logprob": -0.12603123982747397, "compression_ratio": 1.6027397260273972, "no_speech_prob": 1.2028458513668738e-05}, {"id": 153, "seek": 111740, "start": 1120.88, "end": 1126.2, "text": " cost millions of dollars. We really needed a lot of data, millions of data points. So", "tokens": [2063, 6803, 295, 3808, 13, 492, 534, 2978, 257, 688, 295, 1412, 11, 6803, 295, 1412, 2793, 13, 407], "temperature": 0.0, "avg_logprob": -0.12603123982747397, "compression_ratio": 1.6027397260273972, "no_speech_prob": 1.2028458513668738e-05}, {"id": 154, "seek": 111740, "start": 1126.2, "end": 1135.16, "text": " in a retail bank, we could do that. And yeah, it was even then there were things that just", "tokens": [294, 257, 10800, 3765, 11, 321, 727, 360, 300, 13, 400, 1338, 11, 309, 390, 754, 550, 456, 645, 721, 300, 445], "temperature": 0.0, "avg_logprob": -0.12603123982747397, "compression_ratio": 1.6027397260273972, "no_speech_prob": 1.2028458513668738e-05}, {"id": 155, "seek": 111740, "start": 1135.16, "end": 1141.8000000000002, "text": " weren't quite working as well as we would expect. And so as it turned out, the key problem", "tokens": [4999, 380, 1596, 1364, 382, 731, 382, 321, 576, 2066, 13, 400, 370, 382, 309, 3574, 484, 11, 264, 2141, 1154], "temperature": 0.0, "avg_logprob": -0.12603123982747397, "compression_ratio": 1.6027397260273972, "no_speech_prob": 1.2028458513668738e-05}, {"id": 156, "seek": 114180, "start": 1141.8, "end": 1147.9199999999998, "text": " was that back then everybody was relying on this math result called the universal approximation", "tokens": [390, 300, 646, 550, 2201, 390, 24140, 322, 341, 5221, 1874, 1219, 264, 11455, 28023], "temperature": 0.0, "avg_logprob": -0.11312397932394958, "compression_ratio": 1.6160714285714286, "no_speech_prob": 1.983154288609512e-05}, {"id": 157, "seek": 114180, "start": 1147.9199999999998, "end": 1153.44, "text": " theorem, which said that a neural network could solve any given problem, computable", "tokens": [20904, 11, 597, 848, 300, 257, 18161, 3209, 727, 5039, 604, 2212, 1154, 11, 2807, 712], "temperature": 0.0, "avg_logprob": -0.11312397932394958, "compression_ratio": 1.6160714285714286, "no_speech_prob": 1.983154288609512e-05}, {"id": 158, "seek": 114180, "start": 1153.44, "end": 1162.3999999999999, "text": " problem to any arbitrary level of accuracy. And it only needs one hidden layer. And this", "tokens": [1154, 281, 604, 23211, 1496, 295, 14170, 13, 400, 309, 787, 2203, 472, 7633, 4583, 13, 400, 341], "temperature": 0.0, "avg_logprob": -0.11312397932394958, "compression_ratio": 1.6160714285714286, "no_speech_prob": 1.983154288609512e-05}, {"id": 159, "seek": 114180, "start": 1162.3999999999999, "end": 1170.0, "text": " is one of the many, many times in deep learning history where theory has been used in totally", "tokens": [307, 472, 295, 264, 867, 11, 867, 1413, 294, 2452, 2539, 2503, 689, 5261, 575, 668, 1143, 294, 3879], "temperature": 0.0, "avg_logprob": -0.11312397932394958, "compression_ratio": 1.6160714285714286, "no_speech_prob": 1.983154288609512e-05}, {"id": 160, "seek": 117000, "start": 1170.0, "end": 1175.92, "text": " inappropriate ways. And the problem with this theory was that although this was theoretically", "tokens": [26723, 2098, 13, 400, 264, 1154, 365, 341, 5261, 390, 300, 4878, 341, 390, 29400], "temperature": 0.0, "avg_logprob": -0.09881884051907447, "compression_ratio": 1.6011560693641618, "no_speech_prob": 7.183059551607585e-06}, {"id": 161, "seek": 117000, "start": 1175.92, "end": 1184.24, "text": " true, in practice, a neural network with one hidden layer requires far too many nodes to", "tokens": [2074, 11, 294, 3124, 11, 257, 18161, 3209, 365, 472, 7633, 4583, 7029, 1400, 886, 867, 13891, 281], "temperature": 0.0, "avg_logprob": -0.09881884051907447, "compression_ratio": 1.6011560693641618, "no_speech_prob": 7.183059551607585e-06}, {"id": 162, "seek": 117000, "start": 1184.24, "end": 1189.56, "text": " be useful most of the time. And what we actually need is lots of hidden layers. And that turns", "tokens": [312, 4420, 881, 295, 264, 565, 13, 400, 437, 321, 767, 643, 307, 3195, 295, 7633, 7914, 13, 400, 300, 4523], "temperature": 0.0, "avg_logprob": -0.09881884051907447, "compression_ratio": 1.6011560693641618, "no_speech_prob": 7.183059551607585e-06}, {"id": 163, "seek": 118956, "start": 1189.56, "end": 1201.32, "text": " out to be much more efficient. So anyway, I did kind of feel like for those 20 years,", "tokens": [484, 281, 312, 709, 544, 7148, 13, 407, 4033, 11, 286, 630, 733, 295, 841, 411, 337, 729, 945, 924, 11], "temperature": 0.0, "avg_logprob": -0.1330888867378235, "compression_ratio": 1.4086021505376345, "no_speech_prob": 2.8572669634741032e-06}, {"id": 164, "seek": 118956, "start": 1201.32, "end": 1207.76, "text": " at some point, neural networks are going to reappear in my life because of this infinitely", "tokens": [412, 512, 935, 11, 18161, 9590, 366, 516, 281, 35638, 14881, 294, 452, 993, 570, 295, 341, 36227], "temperature": 0.0, "avg_logprob": -0.1330888867378235, "compression_ratio": 1.4086021505376345, "no_speech_prob": 2.8572669634741032e-06}, {"id": 165, "seek": 118956, "start": 1207.76, "end": 1215.96, "text": " flexible function, the fact that they can solve any given problem in theory. And then", "tokens": [11358, 2445, 11, 264, 1186, 300, 436, 393, 5039, 604, 2212, 1154, 294, 5261, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.1330888867378235, "compression_ratio": 1.4086021505376345, "no_speech_prob": 2.8572669634741032e-06}, {"id": 166, "seek": 121596, "start": 1215.96, "end": 1219.8, "text": " along with this infinitely flexible function, we combine it with gradient descent, which", "tokens": [2051, 365, 341, 36227, 11358, 2445, 11, 321, 10432, 309, 365, 16235, 23475, 11, 597], "temperature": 0.0, "avg_logprob": -0.12570406595865885, "compression_ratio": 1.6822429906542056, "no_speech_prob": 2.3320487798628164e-06}, {"id": 167, "seek": 121596, "start": 1219.8, "end": 1227.44, "text": " is this all-purpose parameter fitting algorithm. And again, there was a problem with theory", "tokens": [307, 341, 439, 12, 42601, 13075, 15669, 9284, 13, 400, 797, 11, 456, 390, 257, 1154, 365, 5261], "temperature": 0.0, "avg_logprob": -0.12570406595865885, "compression_ratio": 1.6822429906542056, "no_speech_prob": 2.3320487798628164e-06}, {"id": 168, "seek": 121596, "start": 1227.44, "end": 1234.8, "text": " here, which is I spent many, many years focused on operations research and optimization. And", "tokens": [510, 11, 597, 307, 286, 4418, 867, 11, 867, 924, 5178, 322, 7705, 2132, 293, 19618, 13, 400], "temperature": 0.0, "avg_logprob": -0.12570406595865885, "compression_ratio": 1.6822429906542056, "no_speech_prob": 2.3320487798628164e-06}, {"id": 169, "seek": 121596, "start": 1234.8, "end": 1240.92, "text": " operations research generally focused on, again, kind of theoretical questions of what", "tokens": [7705, 2132, 5101, 5178, 322, 11, 797, 11, 733, 295, 20864, 1651, 295, 437], "temperature": 0.0, "avg_logprob": -0.12570406595865885, "compression_ratio": 1.6822429906542056, "no_speech_prob": 2.3320487798628164e-06}, {"id": 170, "seek": 124092, "start": 1240.92, "end": 1251.0, "text": " is proofably able to find the definite maximum or minimum of a function. And gradient descent", "tokens": [307, 8177, 1188, 1075, 281, 915, 264, 25131, 6674, 420, 7285, 295, 257, 2445, 13, 400, 16235, 23475], "temperature": 0.0, "avg_logprob": -0.10871769400203929, "compression_ratio": 1.5885714285714285, "no_speech_prob": 3.882840974256396e-05}, {"id": 171, "seek": 124092, "start": 1251.0, "end": 1259.3600000000001, "text": " doesn't do that, particularly stochastic gradient descent. And so a lot of people were kind", "tokens": [1177, 380, 360, 300, 11, 4098, 342, 8997, 2750, 16235, 23475, 13, 400, 370, 257, 688, 295, 561, 645, 733], "temperature": 0.0, "avg_logprob": -0.10871769400203929, "compression_ratio": 1.5885714285714285, "no_speech_prob": 3.882840974256396e-05}, {"id": 172, "seek": 124092, "start": 1259.3600000000001, "end": 1266.6000000000001, "text": " of ignoring it. But the thing is, the question we should be asking is not what can we prove,", "tokens": [295, 26258, 309, 13, 583, 264, 551, 307, 11, 264, 1168, 321, 820, 312, 3365, 307, 406, 437, 393, 321, 7081, 11], "temperature": 0.0, "avg_logprob": -0.10871769400203929, "compression_ratio": 1.5885714285714285, "no_speech_prob": 3.882840974256396e-05}, {"id": 173, "seek": 126660, "start": 1266.6, "end": 1272.76, "text": " but what actually works in practice. And the people who, the very small number of people", "tokens": [457, 437, 767, 1985, 294, 3124, 13, 400, 264, 561, 567, 11, 264, 588, 1359, 1230, 295, 561], "temperature": 0.0, "avg_logprob": -0.1603386519385166, "compression_ratio": 1.5459770114942528, "no_speech_prob": 1.4509376342175528e-05}, {"id": 174, "seek": 126660, "start": 1272.76, "end": 1279.7199999999998, "text": " who were working on neural networks and gradient descent throughout the 90s and early 2000s,", "tokens": [567, 645, 1364, 322, 18161, 9590, 293, 16235, 23475, 3710, 264, 4289, 82, 293, 2440, 8132, 82, 11], "temperature": 0.0, "avg_logprob": -0.1603386519385166, "compression_ratio": 1.5459770114942528, "no_speech_prob": 1.4509376342175528e-05}, {"id": 175, "seek": 126660, "start": 1279.7199999999998, "end": 1287.52, "text": " despite the theory that said it's a terrible idea, actually were finding it was working", "tokens": [7228, 264, 5261, 300, 848, 309, 311, 257, 6237, 1558, 11, 767, 645, 5006, 309, 390, 1364], "temperature": 0.0, "avg_logprob": -0.1603386519385166, "compression_ratio": 1.5459770114942528, "no_speech_prob": 1.4509376342175528e-05}, {"id": 176, "seek": 128752, "start": 1287.52, "end": 1297.76, "text": " really well. Unfortunately, you know, academia around machine learning has tended to be much", "tokens": [534, 731, 13, 8590, 11, 291, 458, 11, 28937, 926, 3479, 2539, 575, 34732, 281, 312, 709], "temperature": 0.0, "avg_logprob": -0.15852464135013408, "compression_ratio": 1.4432432432432432, "no_speech_prob": 5.95490246269037e-06}, {"id": 177, "seek": 128752, "start": 1297.76, "end": 1303.36, "text": " more driven by theory than results, at least for a long time it was. I still think it is", "tokens": [544, 9555, 538, 5261, 813, 3542, 11, 412, 1935, 337, 257, 938, 565, 309, 390, 13, 286, 920, 519, 309, 307], "temperature": 0.0, "avg_logprob": -0.15852464135013408, "compression_ratio": 1.4432432432432432, "no_speech_prob": 5.95490246269037e-06}, {"id": 178, "seek": 128752, "start": 1303.36, "end": 1310.48, "text": " too much. And so the fact that there were people like Hinton and Lacoon saying, look,", "tokens": [886, 709, 13, 400, 370, 264, 1186, 300, 456, 645, 561, 411, 389, 12442, 293, 40113, 4106, 1566, 11, 574, 11], "temperature": 0.0, "avg_logprob": -0.15852464135013408, "compression_ratio": 1.4432432432432432, "no_speech_prob": 5.95490246269037e-06}, {"id": 179, "seek": 131048, "start": 1310.48, "end": 1320.0, "text": " here's a model that's better than anything in the world at solving this problem. But,", "tokens": [510, 311, 257, 2316, 300, 311, 1101, 813, 1340, 294, 264, 1002, 412, 12606, 341, 1154, 13, 583, 11], "temperature": 0.0, "avg_logprob": -0.1348848025004069, "compression_ratio": 1.5589519650655022, "no_speech_prob": 1.834225986385718e-05}, {"id": 180, "seek": 131048, "start": 1320.0, "end": 1326.1200000000001, "text": " you know, based on theory, we can't exactly prove why, but it really works. Those were", "tokens": [291, 458, 11, 2361, 322, 5261, 11, 321, 393, 380, 2293, 7081, 983, 11, 457, 309, 534, 1985, 13, 3950, 645], "temperature": 0.0, "avg_logprob": -0.1348848025004069, "compression_ratio": 1.5589519650655022, "no_speech_prob": 1.834225986385718e-05}, {"id": 181, "seek": 131048, "start": 1326.1200000000001, "end": 1333.1200000000001, "text": " not getting published, unfortunately. Anyway, so things gradually began to change. And one", "tokens": [406, 1242, 6572, 11, 7015, 13, 5684, 11, 370, 721, 13145, 4283, 281, 1319, 13, 400, 472], "temperature": 0.0, "avg_logprob": -0.1348848025004069, "compression_ratio": 1.5589519650655022, "no_speech_prob": 1.834225986385718e-05}, {"id": 182, "seek": 131048, "start": 1333.1200000000001, "end": 1339.42, "text": " of the big things that changed was that finally, in the, you know, kind of around 2014, 2015,", "tokens": [295, 264, 955, 721, 300, 3105, 390, 300, 2721, 11, 294, 264, 11, 291, 458, 11, 733, 295, 926, 8227, 11, 7546, 11], "temperature": 0.0, "avg_logprob": -0.1348848025004069, "compression_ratio": 1.5589519650655022, "no_speech_prob": 1.834225986385718e-05}, {"id": 183, "seek": 133942, "start": 1339.42, "end": 1343.94, "text": " we started to see some software appearing that allowed us to conveniently train these", "tokens": [321, 1409, 281, 536, 512, 4722, 19870, 300, 4350, 505, 281, 44375, 3847, 613], "temperature": 0.0, "avg_logprob": -0.08157888054847717, "compression_ratio": 1.5765765765765767, "no_speech_prob": 1.321112722507678e-05}, {"id": 184, "seek": 133942, "start": 1343.94, "end": 1351.22, "text": " things on GPUs, which allowed us to use, you know, relatively inexpensive computers to", "tokens": [721, 322, 18407, 82, 11, 597, 4350, 505, 281, 764, 11, 291, 458, 11, 7226, 28382, 10807, 281], "temperature": 0.0, "avg_logprob": -0.08157888054847717, "compression_ratio": 1.5765765765765767, "no_speech_prob": 1.321112722507678e-05}, {"id": 185, "seek": 133942, "start": 1351.22, "end": 1359.16, "text": " actually get pretty good results. So although the theory didn't really change at this point,", "tokens": [767, 483, 1238, 665, 3542, 13, 407, 4878, 264, 5261, 994, 380, 534, 1319, 412, 341, 935, 11], "temperature": 0.0, "avg_logprob": -0.08157888054847717, "compression_ratio": 1.5765765765765767, "no_speech_prob": 1.321112722507678e-05}, {"id": 186, "seek": 133942, "start": 1359.16, "end": 1362.64, "text": " what did change is just more people could try things out and be like, oh, okay, this", "tokens": [437, 630, 1319, 307, 445, 544, 561, 727, 853, 721, 484, 293, 312, 411, 11, 1954, 11, 1392, 11, 341], "temperature": 0.0, "avg_logprob": -0.08157888054847717, "compression_ratio": 1.5765765765765767, "no_speech_prob": 1.321112722507678e-05}, {"id": 187, "seek": 136264, "start": 1362.64, "end": 1370.24, "text": " is actually practically really helpful. To people outside of the world of neural networks,", "tokens": [307, 767, 15667, 534, 4961, 13, 1407, 561, 2380, 295, 264, 1002, 295, 18161, 9590, 11], "temperature": 0.0, "avg_logprob": -0.11219975977768133, "compression_ratio": 1.6210045662100456, "no_speech_prob": 6.0486217989819124e-06}, {"id": 188, "seek": 136264, "start": 1370.24, "end": 1376.88, "text": " this all seemed very sudden. It seemed like there was this sudden fad around deep learning", "tokens": [341, 439, 6576, 588, 3990, 13, 467, 6576, 411, 456, 390, 341, 3990, 283, 345, 926, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.11219975977768133, "compression_ratio": 1.6210045662100456, "no_speech_prob": 6.0486217989819124e-06}, {"id": 189, "seek": 136264, "start": 1376.88, "end": 1382.5200000000002, "text": " where people were suddenly going, wow, this is amazing. And so people who had seen other", "tokens": [689, 561, 645, 5800, 516, 11, 6076, 11, 341, 307, 2243, 13, 400, 370, 561, 567, 632, 1612, 661], "temperature": 0.0, "avg_logprob": -0.11219975977768133, "compression_ratio": 1.6210045662100456, "no_speech_prob": 6.0486217989819124e-06}, {"id": 190, "seek": 136264, "start": 1382.5200000000002, "end": 1389.0400000000002, "text": " fads quite reasonably thought, well, this one will pass too. But the difference with", "tokens": [283, 5834, 1596, 23551, 1194, 11, 731, 11, 341, 472, 486, 1320, 886, 13, 583, 264, 2649, 365], "temperature": 0.0, "avg_logprob": -0.11219975977768133, "compression_ratio": 1.6210045662100456, "no_speech_prob": 6.0486217989819124e-06}, {"id": 191, "seek": 138904, "start": 1389.04, "end": 1395.72, "text": " this fad is it's actually been under development for many, many, many decades. So this was", "tokens": [341, 283, 345, 307, 309, 311, 767, 668, 833, 3250, 337, 867, 11, 867, 11, 867, 7878, 13, 407, 341, 390], "temperature": 0.0, "avg_logprob": -0.12131795778379335, "compression_ratio": 1.617391304347826, "no_speech_prob": 6.643197593803052e-06}, {"id": 192, "seek": 138904, "start": 1395.72, "end": 1404.36, "text": " the first neural network to be built and it was back in 1957 that it was built. And continually,", "tokens": [264, 700, 18161, 3209, 281, 312, 3094, 293, 309, 390, 646, 294, 46256, 300, 309, 390, 3094, 13, 400, 22277, 11], "temperature": 0.0, "avg_logprob": -0.12131795778379335, "compression_ratio": 1.617391304347826, "no_speech_prob": 6.643197593803052e-06}, {"id": 193, "seek": 138904, "start": 1404.36, "end": 1412.56, "text": " for all those decades, there were people working on making neural nets really work in practice.", "tokens": [337, 439, 729, 7878, 11, 456, 645, 561, 1364, 322, 1455, 18161, 36170, 534, 589, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.12131795778379335, "compression_ratio": 1.617391304347826, "no_speech_prob": 6.643197593803052e-06}, {"id": 194, "seek": 138904, "start": 1412.56, "end": 1418.84, "text": " So what was happening in 2015 was not a sudden, here's this new thing where I got organo", "tokens": [407, 437, 390, 2737, 294, 7546, 390, 406, 257, 3990, 11, 510, 311, 341, 777, 551, 689, 286, 658, 1798, 78], "temperature": 0.0, "avg_logprob": -0.12131795778379335, "compression_ratio": 1.617391304347826, "no_speech_prob": 6.643197593803052e-06}, {"id": 195, "seek": 141884, "start": 1418.84, "end": 1425.6399999999999, "text": " flock to, but it was actually here's this old thing, which we finally got to the point", "tokens": [34819, 281, 11, 457, 309, 390, 767, 510, 311, 341, 1331, 551, 11, 597, 321, 2721, 658, 281, 264, 935], "temperature": 0.0, "avg_logprob": -0.12106654860756615, "compression_ratio": 1.6418604651162791, "no_speech_prob": 1.2029261597490404e-05}, {"id": 196, "seek": 141884, "start": 1425.6399999999999, "end": 1432.84, "text": " where it's actually really working. And so it's not a new fad at all, but it's really", "tokens": [689, 309, 311, 767, 534, 1364, 13, 400, 370, 309, 311, 406, 257, 777, 283, 345, 412, 439, 11, 457, 309, 311, 534], "temperature": 0.0, "avg_logprob": -0.12106654860756615, "compression_ratio": 1.6418604651162791, "no_speech_prob": 1.2029261597490404e-05}, {"id": 197, "seek": 141884, "start": 1432.84, "end": 1437.72, "text": " the result of decades of hard work of solving lots of problems and finally getting to a", "tokens": [264, 1874, 295, 7878, 295, 1152, 589, 295, 12606, 3195, 295, 2740, 293, 2721, 1242, 281, 257], "temperature": 0.0, "avg_logprob": -0.12106654860756615, "compression_ratio": 1.6418604651162791, "no_speech_prob": 1.2029261597490404e-05}, {"id": 198, "seek": 141884, "start": 1437.72, "end": 1448.3799999999999, "text": " point where things are making sense. But what has happened since kind of 2015 is the ability", "tokens": [935, 689, 721, 366, 1455, 2020, 13, 583, 437, 575, 2011, 1670, 733, 295, 7546, 307, 264, 3485], "temperature": 0.0, "avg_logprob": -0.12106654860756615, "compression_ratio": 1.6418604651162791, "no_speech_prob": 1.2029261597490404e-05}, {"id": 199, "seek": 144838, "start": 1448.38, "end": 1456.0, "text": " of these infinitely flexible functions has suddenly started to become clear even to a", "tokens": [295, 613, 36227, 11358, 6828, 575, 5800, 1409, 281, 1813, 1850, 754, 281, 257], "temperature": 0.0, "avg_logprob": -0.13543200773351333, "compression_ratio": 1.5304347826086957, "no_speech_prob": 1.4738509889866691e-05}, {"id": 200, "seek": 144838, "start": 1456.0, "end": 1461.2, "text": " lay person, because you can just look at what they're doing and it's mind blowing. So for", "tokens": [2360, 954, 11, 570, 291, 393, 445, 574, 412, 437, 436, 434, 884, 293, 309, 311, 1575, 15068, 13, 407, 337], "temperature": 0.0, "avg_logprob": -0.13543200773351333, "compression_ratio": 1.5304347826086957, "no_speech_prob": 1.4738509889866691e-05}, {"id": 201, "seek": 144838, "start": 1461.2, "end": 1468.0600000000002, "text": " example, if you look at OpenAI's Dali, this is a model that's been trained on pairs of", "tokens": [1365, 11, 498, 291, 574, 412, 7238, 48698, 311, 413, 5103, 11, 341, 307, 257, 2316, 300, 311, 668, 8895, 322, 15494, 295], "temperature": 0.0, "avg_logprob": -0.13543200773351333, "compression_ratio": 1.5304347826086957, "no_speech_prob": 1.4738509889866691e-05}, {"id": 202, "seek": 144838, "start": 1468.0600000000002, "end": 1476.2800000000002, "text": " pictures and captions such that you can now write any arbitrary sentence. So if you write", "tokens": [5242, 293, 44832, 1270, 300, 291, 393, 586, 2464, 604, 23211, 8174, 13, 407, 498, 291, 2464], "temperature": 0.0, "avg_logprob": -0.13543200773351333, "compression_ratio": 1.5304347826086957, "no_speech_prob": 1.4738509889866691e-05}, {"id": 203, "seek": 147628, "start": 1476.28, "end": 1484.72, "text": " an illustration of a baby daikon radish in a tutu walking a dog, Dali will draw pictures", "tokens": [364, 22645, 295, 257, 3186, 1120, 1035, 266, 31136, 294, 257, 3672, 84, 4494, 257, 3000, 11, 413, 5103, 486, 2642, 5242], "temperature": 0.0, "avg_logprob": -0.12618152131425572, "compression_ratio": 1.6794258373205742, "no_speech_prob": 9.51590027398197e-06}, {"id": 204, "seek": 147628, "start": 1484.72, "end": 1490.04, "text": " of what you described for you. And here are some actual non-cherry picked pictures of", "tokens": [295, 437, 291, 7619, 337, 291, 13, 400, 510, 366, 512, 3539, 2107, 12, 339, 5318, 6183, 5242, 295], "temperature": 0.0, "avg_logprob": -0.12618152131425572, "compression_ratio": 1.6794258373205742, "no_speech_prob": 9.51590027398197e-06}, {"id": 205, "seek": 147628, "start": 1490.04, "end": 1497.16, "text": " that. And so to be clear, this is all out of domain. So Dali has never seen illustrations", "tokens": [300, 13, 400, 370, 281, 312, 1850, 11, 341, 307, 439, 484, 295, 9274, 13, 407, 413, 5103, 575, 1128, 1612, 34540], "temperature": 0.0, "avg_logprob": -0.12618152131425572, "compression_ratio": 1.6794258373205742, "no_speech_prob": 9.51590027398197e-06}, {"id": 206, "seek": 147628, "start": 1497.16, "end": 1502.48, "text": " of baby daikon radishes yet or radishes and tutus or let alone any of this combination", "tokens": [295, 3186, 1120, 1035, 266, 2843, 16423, 1939, 420, 2843, 16423, 293, 3672, 301, 420, 718, 3312, 604, 295, 341, 6562], "temperature": 0.0, "avg_logprob": -0.12618152131425572, "compression_ratio": 1.6794258373205742, "no_speech_prob": 9.51590027398197e-06}, {"id": 207, "seek": 150248, "start": 1502.48, "end": 1510.6, "text": " of things. It's creating these entirely from scratch. By the same token, it's never seen", "tokens": [295, 721, 13, 467, 311, 4084, 613, 7696, 490, 8459, 13, 3146, 264, 912, 14862, 11, 309, 311, 1128, 1612], "temperature": 0.0, "avg_logprob": -0.10985775832291488, "compression_ratio": 1.7156862745098038, "no_speech_prob": 1.9523904484231025e-05}, {"id": 208, "seek": 150248, "start": 1510.6, "end": 1514.68, "text": " an avocado shaped chair before as best as I know, but if you type in an armchair in", "tokens": [364, 27041, 13475, 6090, 949, 382, 1151, 382, 286, 458, 11, 457, 498, 291, 2010, 294, 364, 3726, 17892, 294], "temperature": 0.0, "avg_logprob": -0.10985775832291488, "compression_ratio": 1.7156862745098038, "no_speech_prob": 1.9523904484231025e-05}, {"id": 209, "seek": 150248, "start": 1514.68, "end": 1524.72, "text": " the shape of an avocado, it creates these pictures for you from scratch. And so, you", "tokens": [264, 3909, 295, 364, 27041, 11, 309, 7829, 613, 5242, 337, 291, 490, 8459, 13, 400, 370, 11, 291], "temperature": 0.0, "avg_logprob": -0.10985775832291488, "compression_ratio": 1.7156862745098038, "no_speech_prob": 1.9523904484231025e-05}, {"id": 210, "seek": 150248, "start": 1524.72, "end": 1531.88, "text": " know, it's really cool now that we can actually kind of show, we can actually say, look what", "tokens": [458, 11, 309, 311, 534, 1627, 586, 300, 321, 393, 767, 733, 295, 855, 11, 321, 393, 767, 584, 11, 574, 437], "temperature": 0.0, "avg_logprob": -0.10985775832291488, "compression_ratio": 1.7156862745098038, "no_speech_prob": 1.9523904484231025e-05}, {"id": 211, "seek": 153188, "start": 1531.88, "end": 1538.48, "text": " computers can do and look what computers can do if you use deep learning. And to anybody", "tokens": [10807, 393, 360, 293, 574, 437, 10807, 393, 360, 498, 291, 764, 2452, 2539, 13, 400, 281, 4472], "temperature": 0.0, "avg_logprob": -0.15932764165541705, "compression_ratio": 1.6824644549763033, "no_speech_prob": 6.643136202910682e-06}, {"id": 212, "seek": 153188, "start": 1538.48, "end": 1543.2800000000002, "text": " who's grown up in the kind of pre deep learning era, this just looks like magic. You know,", "tokens": [567, 311, 7709, 493, 294, 264, 733, 295, 659, 2452, 2539, 4249, 11, 341, 445, 1542, 411, 5585, 13, 509, 458, 11], "temperature": 0.0, "avg_logprob": -0.15932764165541705, "compression_ratio": 1.6824644549763033, "no_speech_prob": 6.643136202910682e-06}, {"id": 213, "seek": 153188, "start": 1543.2800000000002, "end": 1549.6000000000001, "text": " it's like, this is not things that I believe computers can do. But here we are. This is", "tokens": [309, 311, 411, 11, 341, 307, 406, 721, 300, 286, 1697, 10807, 393, 360, 13, 583, 510, 321, 366, 13, 639, 307], "temperature": 0.0, "avg_logprob": -0.15932764165541705, "compression_ratio": 1.6824644549763033, "no_speech_prob": 6.643136202910682e-06}, {"id": 214, "seek": 153188, "start": 1549.6000000000001, "end": 1560.7600000000002, "text": " the theoretically universally capable model actually doing things that we've trained it", "tokens": [264, 29400, 43995, 8189, 2316, 767, 884, 721, 300, 321, 600, 8895, 309], "temperature": 0.0, "avg_logprob": -0.15932764165541705, "compression_ratio": 1.6824644549763033, "no_speech_prob": 6.643136202910682e-06}, {"id": 215, "seek": 156076, "start": 1560.76, "end": 1570.16, "text": " to do. So in the last few years, we're now starting to see, you know, many times every", "tokens": [281, 360, 13, 407, 294, 264, 1036, 1326, 924, 11, 321, 434, 586, 2891, 281, 536, 11, 291, 458, 11, 867, 1413, 633], "temperature": 0.0, "avg_logprob": -0.10575166639390883, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.1442518118419684e-05}, {"id": 216, "seek": 156076, "start": 1570.16, "end": 1574.6, "text": " year examples of computers doing things which we're being told computers won't be able to", "tokens": [1064, 5110, 295, 10807, 884, 721, 597, 321, 434, 885, 1907, 10807, 1582, 380, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.10575166639390883, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.1442518118419684e-05}, {"id": 217, "seek": 156076, "start": 1574.6, "end": 1581.48, "text": " do in our lifetime. So for example, I was repeatedly told by experts that in my lifetime,", "tokens": [360, 294, 527, 11364, 13, 407, 337, 1365, 11, 286, 390, 18227, 1907, 538, 8572, 300, 294, 452, 11364, 11], "temperature": 0.0, "avg_logprob": -0.10575166639390883, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.1442518118419684e-05}, {"id": 218, "seek": 156076, "start": 1581.48, "end": 1588.48, "text": " we would never see a computer win a game of Go against an expert. And of course, we're", "tokens": [321, 576, 1128, 536, 257, 3820, 1942, 257, 1216, 295, 1037, 1970, 364, 5844, 13, 400, 295, 1164, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.10575166639390883, "compression_ratio": 1.6572769953051643, "no_speech_prob": 2.1442518118419684e-05}, {"id": 219, "seek": 158848, "start": 1588.48, "end": 1594.88, "text": " now at the point where AlphaGo Zero got to that point in three days. And it's so far", "tokens": [586, 412, 264, 935, 689, 20588, 12104, 17182, 658, 281, 300, 935, 294, 1045, 1708, 13, 400, 309, 311, 370, 1400], "temperature": 0.0, "avg_logprob": -0.09776840852887443, "compression_ratio": 1.7227722772277227, "no_speech_prob": 3.905342964571901e-06}, {"id": 220, "seek": 158848, "start": 1594.88, "end": 1600.4, "text": " ahead of the best expert now that, you know, it's kind of makes the world's best experts", "tokens": [2286, 295, 264, 1151, 5844, 586, 300, 11, 291, 458, 11, 309, 311, 733, 295, 1669, 264, 1002, 311, 1151, 8572], "temperature": 0.0, "avg_logprob": -0.09776840852887443, "compression_ratio": 1.7227722772277227, "no_speech_prob": 3.905342964571901e-06}, {"id": 221, "seek": 158848, "start": 1600.4, "end": 1608.08, "text": " look like total beginners. And one of the really interesting things about AlphaGo Zero", "tokens": [574, 411, 3217, 26992, 13, 400, 472, 295, 264, 534, 1880, 721, 466, 20588, 12104, 17182], "temperature": 0.0, "avg_logprob": -0.09776840852887443, "compression_ratio": 1.7227722772277227, "no_speech_prob": 3.905342964571901e-06}, {"id": 222, "seek": 158848, "start": 1608.08, "end": 1614.88, "text": " is that if you actually look at the source code for it, here it is. And the source code", "tokens": [307, 300, 498, 291, 767, 574, 412, 264, 4009, 3089, 337, 309, 11, 510, 309, 307, 13, 400, 264, 4009, 3089], "temperature": 0.0, "avg_logprob": -0.09776840852887443, "compression_ratio": 1.7227722772277227, "no_speech_prob": 3.905342964571901e-06}, {"id": 223, "seek": 161488, "start": 1614.88, "end": 1619.0400000000002, "text": " for the key thing, which is like the thing that figures out whether a Go board is a good", "tokens": [337, 264, 2141, 551, 11, 597, 307, 411, 264, 551, 300, 9624, 484, 1968, 257, 1037, 3150, 307, 257, 665], "temperature": 0.0, "avg_logprob": -0.09858107292789152, "compression_ratio": 1.6391304347826088, "no_speech_prob": 1.0348387604608433e-06}, {"id": 224, "seek": 161488, "start": 1619.0400000000002, "end": 1627.4, "text": " position or not, fits on one slide. And furthermore, if you've done any deep learning, you'll recognize", "tokens": [2535, 420, 406, 11, 9001, 322, 472, 4137, 13, 400, 3052, 3138, 11, 498, 291, 600, 1096, 604, 2452, 2539, 11, 291, 603, 5521], "temperature": 0.0, "avg_logprob": -0.09858107292789152, "compression_ratio": 1.6391304347826088, "no_speech_prob": 1.0348387604608433e-06}, {"id": 225, "seek": 161488, "start": 1627.4, "end": 1637.5600000000002, "text": " it as looking almost exactly like a standard computer vision model. And so one of the things", "tokens": [309, 382, 1237, 1920, 2293, 411, 257, 3832, 3820, 5201, 2316, 13, 400, 370, 472, 295, 264, 721], "temperature": 0.0, "avg_logprob": -0.09858107292789152, "compression_ratio": 1.6391304347826088, "no_speech_prob": 1.0348387604608433e-06}, {"id": 226, "seek": 161488, "start": 1637.5600000000002, "end": 1644.7600000000002, "text": " which people who are not themselves deep learning practitioners don't quite realize is that", "tokens": [597, 561, 567, 366, 406, 2969, 2452, 2539, 25742, 500, 380, 1596, 4325, 307, 300], "temperature": 0.0, "avg_logprob": -0.09858107292789152, "compression_ratio": 1.6391304347826088, "no_speech_prob": 1.0348387604608433e-06}, {"id": 227, "seek": 164476, "start": 1644.76, "end": 1650.64, "text": " deep learning on the whole is not a huge collection of somewhat disconnected but slightly connected", "tokens": [2452, 2539, 322, 264, 1379, 307, 406, 257, 2603, 5765, 295, 8344, 29426, 457, 4748, 4582], "temperature": 0.0, "avg_logprob": -0.07164558172225952, "compression_ratio": 1.7777777777777777, "no_speech_prob": 5.954986136202933e-06}, {"id": 228, "seek": 164476, "start": 1650.64, "end": 1658.04, "text": " kind of tricks. It's actually, you know, every deep learning model I build looks almost exactly", "tokens": [733, 295, 11733, 13, 467, 311, 767, 11, 291, 458, 11, 633, 2452, 2539, 2316, 286, 1322, 1542, 1920, 2293], "temperature": 0.0, "avg_logprob": -0.07164558172225952, "compression_ratio": 1.7777777777777777, "no_speech_prob": 5.954986136202933e-06}, {"id": 229, "seek": 164476, "start": 1658.04, "end": 1663.28, "text": " like every other model I build with fairly minor differences. And I train them in nearly", "tokens": [411, 633, 661, 2316, 286, 1322, 365, 6457, 6696, 7300, 13, 400, 286, 3847, 552, 294, 6217], "temperature": 0.0, "avg_logprob": -0.07164558172225952, "compression_ratio": 1.7777777777777777, "no_speech_prob": 5.954986136202933e-06}, {"id": 230, "seek": 164476, "start": 1663.28, "end": 1670.76, "text": " exactly the same way with fairly minor differences. And so deep learning has become this incredibly", "tokens": [2293, 264, 912, 636, 365, 6457, 6696, 7300, 13, 400, 370, 2452, 2539, 575, 1813, 341, 6252], "temperature": 0.0, "avg_logprob": -0.07164558172225952, "compression_ratio": 1.7777777777777777, "no_speech_prob": 5.954986136202933e-06}, {"id": 231, "seek": 167076, "start": 1670.76, "end": 1677.8, "text": " flexible skill that if you have it, you can turn your attention to lots of different domain", "tokens": [11358, 5389, 300, 498, 291, 362, 309, 11, 291, 393, 1261, 428, 3202, 281, 3195, 295, 819, 9274], "temperature": 0.0, "avg_logprob": -0.11641819910569624, "compression_ratio": 1.555084745762712, "no_speech_prob": 6.854170351289213e-06}, {"id": 232, "seek": 167076, "start": 1677.8, "end": 1687.56, "text": " areas and rapidly get incredibly good results. So at this point, deep learning is now the", "tokens": [3179, 293, 12910, 483, 6252, 665, 3542, 13, 407, 412, 341, 935, 11, 2452, 2539, 307, 586, 264], "temperature": 0.0, "avg_logprob": -0.11641819910569624, "compression_ratio": 1.555084745762712, "no_speech_prob": 6.854170351289213e-06}, {"id": 233, "seek": 167076, "start": 1687.56, "end": 1694.6, "text": " best approach in the world for all kinds of applications. I'm not going to read them all.", "tokens": [1151, 3109, 294, 264, 1002, 337, 439, 3685, 295, 5821, 13, 286, 478, 406, 516, 281, 1401, 552, 439, 13], "temperature": 0.0, "avg_logprob": -0.11641819910569624, "compression_ratio": 1.555084745762712, "no_speech_prob": 6.854170351289213e-06}, {"id": 234, "seek": 167076, "start": 1694.6, "end": 1699.2, "text": " And this is by no means a complete list. It's far longer than this. But these are some examples", "tokens": [400, 341, 307, 538, 572, 1355, 257, 3566, 1329, 13, 467, 311, 1400, 2854, 813, 341, 13, 583, 613, 366, 512, 5110], "temperature": 0.0, "avg_logprob": -0.11641819910569624, "compression_ratio": 1.555084745762712, "no_speech_prob": 6.854170351289213e-06}, {"id": 235, "seek": 169920, "start": 1699.2, "end": 1708.16, "text": " of the kinds of things that deep learning is better at than any other known approach.", "tokens": [295, 264, 3685, 295, 721, 300, 2452, 2539, 307, 1101, 412, 813, 604, 661, 2570, 3109, 13], "temperature": 0.0, "avg_logprob": -0.08655329219630507, "compression_ratio": 1.4772727272727273, "no_speech_prob": 3.4462843814253574e-06}, {"id": 236, "seek": 169920, "start": 1708.16, "end": 1715.72, "text": " So why am I spending so much time in my life now on deep learning? Because it really feels", "tokens": [407, 983, 669, 286, 6434, 370, 709, 565, 294, 452, 993, 586, 322, 2452, 2539, 30, 1436, 309, 534, 3417], "temperature": 0.0, "avg_logprob": -0.08655329219630507, "compression_ratio": 1.4772727272727273, "no_speech_prob": 3.4462843814253574e-06}, {"id": 237, "seek": 169920, "start": 1715.72, "end": 1724.56, "text": " to me like a very dramatic step change in human capability, like the development of", "tokens": [281, 385, 411, 257, 588, 12023, 1823, 1319, 294, 1952, 13759, 11, 411, 264, 3250, 295], "temperature": 0.0, "avg_logprob": -0.08655329219630507, "compression_ratio": 1.4772727272727273, "no_speech_prob": 3.4462843814253574e-06}, {"id": 238, "seek": 172456, "start": 1724.56, "end": 1730.76, "text": " electricity, for example. And I would like to think that when I see a very dramatic step", "tokens": [10356, 11, 337, 1365, 13, 400, 286, 576, 411, 281, 519, 300, 562, 286, 536, 257, 588, 12023, 1823], "temperature": 0.0, "avg_logprob": -0.10208340813131893, "compression_ratio": 1.5782608695652174, "no_speech_prob": 1.593343768035993e-05}, {"id": 239, "seek": 172456, "start": 1730.76, "end": 1738.24, "text": " change in human capability, I'm going to spend my time working on figuring out how best to", "tokens": [1319, 294, 1952, 13759, 11, 286, 478, 516, 281, 3496, 452, 565, 1364, 322, 15213, 484, 577, 1151, 281], "temperature": 0.0, "avg_logprob": -0.10208340813131893, "compression_ratio": 1.5782608695652174, "no_speech_prob": 1.593343768035993e-05}, {"id": 240, "seek": 172456, "start": 1738.24, "end": 1744.44, "text": " take advantage of that capability. Because there's going to be so many world-changing", "tokens": [747, 5002, 295, 300, 13759, 13, 1436, 456, 311, 516, 281, 312, 370, 867, 1002, 12, 27123], "temperature": 0.0, "avg_logprob": -0.10208340813131893, "compression_ratio": 1.5782608695652174, "no_speech_prob": 1.593343768035993e-05}, {"id": 241, "seek": 172456, "start": 1744.44, "end": 1751.32, "text": " breakthroughs that come out of that. And particularly as somebody who's built a few companies, as", "tokens": [22397, 82, 300, 808, 484, 295, 300, 13, 400, 4098, 382, 2618, 567, 311, 3094, 257, 1326, 3431, 11, 382], "temperature": 0.0, "avg_logprob": -0.10208340813131893, "compression_ratio": 1.5782608695652174, "no_speech_prob": 1.593343768035993e-05}, {"id": 242, "seek": 175132, "start": 1751.32, "end": 1756.24, "text": " an entrepreneur, the number one thing for an entrepreneur to find and that investors", "tokens": [364, 14307, 11, 264, 1230, 472, 551, 337, 364, 14307, 281, 915, 293, 300, 11519], "temperature": 0.0, "avg_logprob": -0.11173721722194127, "compression_ratio": 1.6543778801843319, "no_speech_prob": 6.853823379060486e-06}, {"id": 243, "seek": 175132, "start": 1756.24, "end": 1762.46, "text": " look for is, is there something you can build now that people couldn't build before in terms", "tokens": [574, 337, 307, 11, 307, 456, 746, 291, 393, 1322, 586, 300, 561, 2809, 380, 1322, 949, 294, 2115], "temperature": 0.0, "avg_logprob": -0.11173721722194127, "compression_ratio": 1.6543778801843319, "no_speech_prob": 6.853823379060486e-06}, {"id": 244, "seek": 175132, "start": 1762.46, "end": 1769.48, "text": " of as a company? And with deep learning, the answer to that is yes, across tens of thousands", "tokens": [295, 382, 257, 2237, 30, 400, 365, 2452, 2539, 11, 264, 1867, 281, 300, 307, 2086, 11, 2108, 10688, 295, 5383], "temperature": 0.0, "avg_logprob": -0.11173721722194127, "compression_ratio": 1.6543778801843319, "no_speech_prob": 6.853823379060486e-06}, {"id": 245, "seek": 175132, "start": 1769.48, "end": 1775.32, "text": " or hundreds of thousands of areas. Because it's like, okay, suddenly there's tools which", "tokens": [420, 6779, 295, 5383, 295, 3179, 13, 1436, 309, 311, 411, 11, 1392, 11, 5800, 456, 311, 3873, 597], "temperature": 0.0, "avg_logprob": -0.11173721722194127, "compression_ratio": 1.6543778801843319, "no_speech_prob": 6.853823379060486e-06}, {"id": 246, "seek": 177532, "start": 1775.32, "end": 1781.3999999999999, "text": " we couldn't automate before and now we can, or we can make hundreds of times more productive", "tokens": [321, 2809, 380, 31605, 949, 293, 586, 321, 393, 11, 420, 321, 393, 652, 6779, 295, 1413, 544, 13304], "temperature": 0.0, "avg_logprob": -0.1016988754272461, "compression_ratio": 1.5990990990990992, "no_speech_prob": 1.9032935369978077e-06}, {"id": 247, "seek": 177532, "start": 1781.3999999999999, "end": 1787.08, "text": " or so forth. So to me, it's a very obvious thing that this is what I want to spend all", "tokens": [420, 370, 5220, 13, 407, 281, 385, 11, 309, 311, 257, 588, 6322, 551, 300, 341, 307, 437, 286, 528, 281, 3496, 439], "temperature": 0.0, "avg_logprob": -0.1016988754272461, "compression_ratio": 1.5990990990990992, "no_speech_prob": 1.9032935369978077e-06}, {"id": 248, "seek": 177532, "start": 1787.08, "end": 1796.3999999999999, "text": " my time on. And when I talk to students or interested entrepreneurs, I always say, this", "tokens": [452, 565, 322, 13, 400, 562, 286, 751, 281, 1731, 420, 3102, 12639, 11, 286, 1009, 584, 11, 341], "temperature": 0.0, "avg_logprob": -0.1016988754272461, "compression_ratio": 1.5990990990990992, "no_speech_prob": 1.9032935369978077e-06}, {"id": 249, "seek": 177532, "start": 1796.3999999999999, "end": 1802.6, "text": " is the thing which is making lots and lots of people extremely rich and is solving lots", "tokens": [307, 264, 551, 597, 307, 1455, 3195, 293, 3195, 295, 561, 4664, 4593, 293, 307, 12606, 3195], "temperature": 0.0, "avg_logprob": -0.1016988754272461, "compression_ratio": 1.5990990990990992, "no_speech_prob": 1.9032935369978077e-06}, {"id": 250, "seek": 180260, "start": 1802.6, "end": 1808.62, "text": " and lots of important societal problems. And we're just seeing the tip of the iceberg.", "tokens": [293, 3195, 295, 1021, 33472, 2740, 13, 400, 321, 434, 445, 2577, 264, 4125, 295, 264, 38880, 13], "temperature": 0.0, "avg_logprob": -0.10560937154860724, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.7878943481264287e-06}, {"id": 251, "seek": 180260, "start": 1808.62, "end": 1819.8799999999999, "text": " So as soon as I got to the point that I realized this, I decided to start a company to actually", "tokens": [407, 382, 2321, 382, 286, 658, 281, 264, 935, 300, 286, 5334, 341, 11, 286, 3047, 281, 722, 257, 2237, 281, 767], "temperature": 0.0, "avg_logprob": -0.10560937154860724, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.7878943481264287e-06}, {"id": 252, "seek": 180260, "start": 1819.8799999999999, "end": 1824.7199999999998, "text": " do something important. And so I got very excited about the opportunities in medicine", "tokens": [360, 746, 1021, 13, 400, 370, 286, 658, 588, 2919, 466, 264, 4786, 294, 7195], "temperature": 0.0, "avg_logprob": -0.10560937154860724, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.7878943481264287e-06}, {"id": 253, "seek": 180260, "start": 1824.7199999999998, "end": 1831.9199999999998, "text": " and I created the first deep learning and medicine company called Inletic. And I didn't", "tokens": [293, 286, 2942, 264, 700, 2452, 2539, 293, 7195, 2237, 1219, 682, 2631, 299, 13, 400, 286, 994, 380], "temperature": 0.0, "avg_logprob": -0.10560937154860724, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.7878943481264287e-06}, {"id": 254, "seek": 183192, "start": 1831.92, "end": 1837.6000000000001, "text": " know anything about medicine and I didn't know any people in medicine. So I got together", "tokens": [458, 1340, 466, 7195, 293, 286, 994, 380, 458, 604, 561, 294, 7195, 13, 407, 286, 658, 1214], "temperature": 0.0, "avg_logprob": -0.09497306530292217, "compression_ratio": 1.52, "no_speech_prob": 6.338736056932248e-06}, {"id": 255, "seek": 183192, "start": 1837.6000000000001, "end": 1846.68, "text": " a group of three other people and me and we decided to hack together a quick deep learning", "tokens": [257, 1594, 295, 1045, 661, 561, 293, 385, 293, 321, 3047, 281, 10339, 1214, 257, 1702, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.09497306530292217, "compression_ratio": 1.52, "no_speech_prob": 6.338736056932248e-06}, {"id": 256, "seek": 183192, "start": 1846.68, "end": 1856.16, "text": " model that would see if we can predict the malignancy of nodules in lung CT scans. And", "tokens": [2316, 300, 576, 536, 498, 321, 393, 6069, 264, 2806, 788, 6717, 295, 15224, 3473, 294, 16730, 19529, 35116, 13, 400], "temperature": 0.0, "avg_logprob": -0.09497306530292217, "compression_ratio": 1.52, "no_speech_prob": 6.338736056932248e-06}, {"id": 257, "seek": 185616, "start": 1856.16, "end": 1862.48, "text": " it turned out that we could. And in fact, the algorithm that we built for this company", "tokens": [309, 3574, 484, 300, 321, 727, 13, 400, 294, 1186, 11, 264, 9284, 300, 321, 3094, 337, 341, 2237], "temperature": 0.0, "avg_logprob": -0.12178826051599839, "compression_ratio": 1.6869158878504673, "no_speech_prob": 1.4509266293316614e-05}, {"id": 258, "seek": 185616, "start": 1862.48, "end": 1867.44, "text": " that I ended up calling Inletic had a better false positive rate and a better false negative", "tokens": [300, 286, 4590, 493, 5141, 682, 2631, 299, 632, 257, 1101, 7908, 3353, 3314, 293, 257, 1101, 7908, 3671], "temperature": 0.0, "avg_logprob": -0.12178826051599839, "compression_ratio": 1.6869158878504673, "no_speech_prob": 1.4509266293316614e-05}, {"id": 259, "seek": 185616, "start": 1867.44, "end": 1874.4, "text": " rate than actually a panel of four trained radiologists. And so this was at a time when", "tokens": [3314, 813, 767, 257, 4831, 295, 1451, 8895, 16335, 12256, 13, 400, 370, 341, 390, 412, 257, 565, 562], "temperature": 0.0, "avg_logprob": -0.12178826051599839, "compression_ratio": 1.6869158878504673, "no_speech_prob": 1.4509266293316614e-05}, {"id": 260, "seek": 185616, "start": 1874.4, "end": 1880.92, "text": " deep learning in medicine and deep learning in radiology was unheard of. There were basically", "tokens": [2452, 2539, 294, 7195, 293, 2452, 2539, 294, 16335, 1793, 390, 517, 42915, 295, 13, 821, 645, 1936], "temperature": 0.0, "avg_logprob": -0.12178826051599839, "compression_ratio": 1.6869158878504673, "no_speech_prob": 1.4509266293316614e-05}, {"id": 261, "seek": 188092, "start": 1880.92, "end": 1886.3600000000001, "text": " no papers about it. There were certainly no startups about it. No one was talking about", "tokens": [572, 10577, 466, 309, 13, 821, 645, 3297, 572, 28041, 466, 309, 13, 883, 472, 390, 1417, 466], "temperature": 0.0, "avg_logprob": -0.10925464095356309, "compression_ratio": 1.6926070038910506, "no_speech_prob": 3.726368049683515e-06}, {"id": 262, "seek": 188092, "start": 1886.3600000000001, "end": 1893.4, "text": " it. And so this finding got some attention. And this was really important to me because", "tokens": [309, 13, 400, 370, 341, 5006, 658, 512, 3202, 13, 400, 341, 390, 534, 1021, 281, 385, 570], "temperature": 0.0, "avg_logprob": -0.10925464095356309, "compression_ratio": 1.6926070038910506, "no_speech_prob": 3.726368049683515e-06}, {"id": 263, "seek": 188092, "start": 1893.4, "end": 1898.8400000000001, "text": " my biggest goal with Inletic was to kind of get deep learning and medicine on the map", "tokens": [452, 3880, 3387, 365, 682, 2631, 299, 390, 281, 733, 295, 483, 2452, 2539, 293, 7195, 322, 264, 4471], "temperature": 0.0, "avg_logprob": -0.10925464095356309, "compression_ratio": 1.6926070038910506, "no_speech_prob": 3.726368049683515e-06}, {"id": 264, "seek": 188092, "start": 1898.8400000000001, "end": 1902.8000000000002, "text": " because I felt like it could save a lot of lives. So I wanted to get a lot of as much", "tokens": [570, 286, 2762, 411, 309, 727, 3155, 257, 688, 295, 2909, 13, 407, 286, 1415, 281, 483, 257, 688, 295, 382, 709], "temperature": 0.0, "avg_logprob": -0.10925464095356309, "compression_ratio": 1.6926070038910506, "no_speech_prob": 3.726368049683515e-06}, {"id": 265, "seek": 188092, "start": 1902.8000000000002, "end": 1909.48, "text": " attention around this as possible. And yeah, very quickly, lots and lots of people were", "tokens": [3202, 926, 341, 382, 1944, 13, 400, 1338, 11, 588, 2661, 11, 3195, 293, 3195, 295, 561, 645], "temperature": 0.0, "avg_logprob": -0.10925464095356309, "compression_ratio": 1.6926070038910506, "no_speech_prob": 3.726368049683515e-06}, {"id": 266, "seek": 190948, "start": 1909.48, "end": 1918.04, "text": " writing about this new company. And as a result, very quickly, deep learning, particularly", "tokens": [3579, 466, 341, 777, 2237, 13, 400, 382, 257, 1874, 11, 588, 2661, 11, 2452, 2539, 11, 4098], "temperature": 0.0, "avg_logprob": -0.1171493814952338, "compression_ratio": 1.469945355191257, "no_speech_prob": 2.53596880384066e-07}, {"id": 267, "seek": 190948, "start": 1918.04, "end": 1927.88, "text": " in radiology, took off. And within two years, the main radiology conference had a huge stream", "tokens": [294, 16335, 1793, 11, 1890, 766, 13, 400, 1951, 732, 924, 11, 264, 2135, 16335, 1793, 7586, 632, 257, 2603, 4309], "temperature": 0.0, "avg_logprob": -0.1171493814952338, "compression_ratio": 1.469945355191257, "no_speech_prob": 2.53596880384066e-07}, {"id": 268, "seek": 190948, "start": 1927.88, "end": 1936.56, "text": " around AI. It was lines out the door. They created a whole new journal for it and so", "tokens": [926, 7318, 13, 467, 390, 3876, 484, 264, 2853, 13, 814, 2942, 257, 1379, 777, 6708, 337, 309, 293, 370], "temperature": 0.0, "avg_logprob": -0.1171493814952338, "compression_ratio": 1.469945355191257, "no_speech_prob": 2.53596880384066e-07}, {"id": 269, "seek": 193656, "start": 1936.56, "end": 1944.28, "text": " forth. And so that was really exciting for me to see how we could help kind of put a", "tokens": [5220, 13, 400, 370, 300, 390, 534, 4670, 337, 385, 281, 536, 577, 321, 727, 854, 733, 295, 829, 257], "temperature": 0.0, "avg_logprob": -0.12110848146326401, "compression_ratio": 1.6398104265402844, "no_speech_prob": 7.766514499962796e-06}, {"id": 270, "seek": 193656, "start": 1944.28, "end": 1951.6799999999998, "text": " technology on the map. In some ways, this is great, but in some ways it was kind of", "tokens": [2899, 322, 264, 4471, 13, 682, 512, 2098, 11, 341, 307, 869, 11, 457, 294, 512, 2098, 309, 390, 733, 295], "temperature": 0.0, "avg_logprob": -0.12110848146326401, "compression_ratio": 1.6398104265402844, "no_speech_prob": 7.766514499962796e-06}, {"id": 271, "seek": 193656, "start": 1951.6799999999998, "end": 1956.3999999999999, "text": " disappointing because there were so many other areas where deep learning should have been", "tokens": [25054, 570, 456, 645, 370, 867, 661, 3179, 689, 2452, 2539, 820, 362, 668], "temperature": 0.0, "avg_logprob": -0.12110848146326401, "compression_ratio": 1.6398104265402844, "no_speech_prob": 7.766514499962796e-06}, {"id": 272, "seek": 193656, "start": 1956.3999999999999, "end": 1963.32, "text": " on the map and it wasn't. And there's no way that I could create companies around every", "tokens": [322, 264, 4471, 293, 309, 2067, 380, 13, 400, 456, 311, 572, 636, 300, 286, 727, 1884, 3431, 926, 633], "temperature": 0.0, "avg_logprob": -0.12110848146326401, "compression_ratio": 1.6398104265402844, "no_speech_prob": 7.766514499962796e-06}, {"id": 273, "seek": 196332, "start": 1963.32, "end": 1969.08, "text": " possible area. So instead I thought, well, what I want to do is make it easy for other", "tokens": [1944, 1859, 13, 407, 2602, 286, 1194, 11, 731, 11, 437, 286, 528, 281, 360, 307, 652, 309, 1858, 337, 661], "temperature": 0.0, "avg_logprob": -0.10813355849961102, "compression_ratio": 1.4831460674157304, "no_speech_prob": 6.0482570916065015e-06}, {"id": 274, "seek": 196332, "start": 1969.08, "end": 1976.48, "text": " people to create companies and products and solutions using deep learning, particularly", "tokens": [561, 281, 1884, 3431, 293, 3383, 293, 6547, 1228, 2452, 2539, 11, 4098], "temperature": 0.0, "avg_logprob": -0.10813355849961102, "compression_ratio": 1.4831460674157304, "no_speech_prob": 6.0482570916065015e-06}, {"id": 275, "seek": 196332, "start": 1976.48, "end": 1986.4399999999998, "text": " because at that time, nearly everybody I knew in the deep learning world were young white", "tokens": [570, 412, 300, 565, 11, 6217, 2201, 286, 2586, 294, 264, 2452, 2539, 1002, 645, 2037, 2418], "temperature": 0.0, "avg_logprob": -0.10813355849961102, "compression_ratio": 1.4831460674157304, "no_speech_prob": 6.0482570916065015e-06}, {"id": 276, "seek": 198644, "start": 1986.44, "end": 1994.16, "text": " men from one of a small number of exclusive universities. And the problem with that is", "tokens": [1706, 490, 472, 295, 257, 1359, 1230, 295, 13005, 11779, 13, 400, 264, 1154, 365, 300, 307], "temperature": 0.0, "avg_logprob": -0.10116066583772985, "compression_ratio": 1.6956521739130435, "no_speech_prob": 3.704418122651987e-05}, {"id": 277, "seek": 198644, "start": 1994.16, "end": 1999.96, "text": " that there's a lot of societally important problems to solve, which that group of people", "tokens": [300, 456, 311, 257, 688, 295, 14051, 379, 1021, 2740, 281, 5039, 11, 597, 300, 1594, 295, 561], "temperature": 0.0, "avg_logprob": -0.10116066583772985, "compression_ratio": 1.6956521739130435, "no_speech_prob": 3.704418122651987e-05}, {"id": 278, "seek": 198644, "start": 1999.96, "end": 2004.8400000000001, "text": " just weren't familiar with. And even if they were both familiar with them and interested", "tokens": [445, 4999, 380, 4963, 365, 13, 400, 754, 498, 436, 645, 1293, 4963, 365, 552, 293, 3102], "temperature": 0.0, "avg_logprob": -0.10116066583772985, "compression_ratio": 1.6956521739130435, "no_speech_prob": 3.704418122651987e-05}, {"id": 279, "seek": 198644, "start": 2004.8400000000001, "end": 2010.96, "text": " with them, interested in them, they didn't know how to find the data for those or what", "tokens": [365, 552, 11, 3102, 294, 552, 11, 436, 994, 380, 458, 577, 281, 915, 264, 1412, 337, 729, 420, 437], "temperature": 0.0, "avg_logprob": -0.10116066583772985, "compression_ratio": 1.6956521739130435, "no_speech_prob": 3.704418122651987e-05}, {"id": 280, "seek": 201096, "start": 2010.96, "end": 2017.2, "text": " the kind of constraints and implementation are and so forth. So Dr. Rachel Thomas and", "tokens": [264, 733, 295, 18491, 293, 11420, 366, 293, 370, 5220, 13, 407, 2491, 13, 14246, 8500, 293], "temperature": 0.0, "avg_logprob": -0.08361108899116516, "compression_ratio": 1.5486725663716814, "no_speech_prob": 1.0613584890961647e-05}, {"id": 281, "seek": 201096, "start": 2017.2, "end": 2024.0, "text": " I decided to create a new organization that would focus on one thing, which was making", "tokens": [286, 3047, 281, 1884, 257, 777, 4475, 300, 576, 1879, 322, 472, 551, 11, 597, 390, 1455], "temperature": 0.0, "avg_logprob": -0.08361108899116516, "compression_ratio": 1.5486725663716814, "no_speech_prob": 1.0613584890961647e-05}, {"id": 282, "seek": 201096, "start": 2024.0, "end": 2029.8, "text": " deep learning accessible. And so basically the idea was to say, okay, if this really", "tokens": [2452, 2539, 9515, 13, 400, 370, 1936, 264, 1558, 390, 281, 584, 11, 1392, 11, 498, 341, 534], "temperature": 0.0, "avg_logprob": -0.08361108899116516, "compression_ratio": 1.5486725663716814, "no_speech_prob": 1.0613584890961647e-05}, {"id": 283, "seek": 201096, "start": 2029.8, "end": 2038.1000000000001, "text": " is a step change in human capability, which happens from time to time in technology history,", "tokens": [307, 257, 1823, 1319, 294, 1952, 13759, 11, 597, 2314, 490, 565, 281, 565, 294, 2899, 2503, 11], "temperature": 0.0, "avg_logprob": -0.08361108899116516, "compression_ratio": 1.5486725663716814, "no_speech_prob": 1.0613584890961647e-05}, {"id": 284, "seek": 203810, "start": 2038.1, "end": 2046.6, "text": " what can we do to help people use this technology regardless of their background? And so there", "tokens": [437, 393, 321, 360, 281, 854, 561, 764, 341, 2899, 10060, 295, 641, 3678, 30, 400, 370, 456], "temperature": 0.0, "avg_logprob": -0.11281486055744228, "compression_ratio": 1.4864864864864864, "no_speech_prob": 1.3006098015466705e-05}, {"id": 285, "seek": 203810, "start": 2046.6, "end": 2055.64, "text": " was a lot of constraints that we had to help remove. So the first thing we did was we thought,", "tokens": [390, 257, 688, 295, 18491, 300, 321, 632, 281, 854, 4159, 13, 407, 264, 700, 551, 321, 630, 390, 321, 1194, 11], "temperature": 0.0, "avg_logprob": -0.11281486055744228, "compression_ratio": 1.4864864864864864, "no_speech_prob": 1.3006098015466705e-05}, {"id": 286, "seek": 203810, "start": 2055.64, "end": 2061.8399999999997, "text": " okay, let's at least make it so that what people already know about how to build deep", "tokens": [1392, 11, 718, 311, 412, 1935, 652, 309, 370, 300, 437, 561, 1217, 458, 466, 577, 281, 1322, 2452], "temperature": 0.0, "avg_logprob": -0.11281486055744228, "compression_ratio": 1.4864864864864864, "no_speech_prob": 1.3006098015466705e-05}, {"id": 287, "seek": 206184, "start": 2061.84, "end": 2068.76, "text": " learning models is as available as possible. So at this time, there weren't any courses", "tokens": [2539, 5245, 307, 382, 2435, 382, 1944, 13, 407, 412, 341, 565, 11, 456, 4999, 380, 604, 7712], "temperature": 0.0, "avg_logprob": -0.08448894725126378, "compression_ratio": 1.5964125560538116, "no_speech_prob": 1.903355041577015e-06}, {"id": 288, "seek": 206184, "start": 2068.76, "end": 2075.52, "text": " or any kind of easy ways in to get going with deep learning. And we had a theory, which", "tokens": [420, 604, 733, 295, 1858, 2098, 294, 281, 483, 516, 365, 2452, 2539, 13, 400, 321, 632, 257, 5261, 11, 597], "temperature": 0.0, "avg_logprob": -0.08448894725126378, "compression_ratio": 1.5964125560538116, "no_speech_prob": 1.903355041577015e-06}, {"id": 289, "seek": 206184, "start": 2075.52, "end": 2083.7000000000003, "text": " was we thought you don't need a Stanford PhD to be an effective deep learning practitioner.", "tokens": [390, 321, 1194, 291, 500, 380, 643, 257, 20374, 14476, 281, 312, 364, 4942, 2452, 2539, 32125, 13], "temperature": 0.0, "avg_logprob": -0.08448894725126378, "compression_ratio": 1.5964125560538116, "no_speech_prob": 1.903355041577015e-06}, {"id": 290, "seek": 206184, "start": 2083.7000000000003, "end": 2090.2200000000003, "text": " You don't need years and years of graduate level math training. We thought that we might", "tokens": [509, 500, 380, 643, 924, 293, 924, 295, 8080, 1496, 5221, 3097, 13, 492, 1194, 300, 321, 1062], "temperature": 0.0, "avg_logprob": -0.08448894725126378, "compression_ratio": 1.5964125560538116, "no_speech_prob": 1.903355041577015e-06}, {"id": 291, "seek": 209022, "start": 2090.22, "end": 2095.64, "text": " be able to build a course that would allow people with just a year of coding background", "tokens": [312, 1075, 281, 1322, 257, 1164, 300, 576, 2089, 561, 365, 445, 257, 1064, 295, 17720, 3678], "temperature": 0.0, "avg_logprob": -0.1700235160000353, "compression_ratio": 1.5260869565217392, "no_speech_prob": 2.6015695766545832e-06}, {"id": 292, "seek": 209022, "start": 2095.64, "end": 2102.9199999999996, "text": " to become effective deep learning practitioners. Now at this time, so what is this about 2014", "tokens": [281, 1813, 4942, 2452, 2539, 25742, 13, 823, 412, 341, 565, 11, 370, 437, 307, 341, 466, 8227], "temperature": 0.0, "avg_logprob": -0.1700235160000353, "compression_ratio": 1.5260869565217392, "no_speech_prob": 2.6015695766545832e-06}, {"id": 293, "seek": 209022, "start": 2102.9199999999996, "end": 2109.24, "text": " or 2015? Can't quite remember, maybe 2015. Nothing like this existed. And this was a", "tokens": [420, 7546, 30, 1664, 380, 1596, 1604, 11, 1310, 7546, 13, 6693, 411, 341, 13135, 13, 400, 341, 390, 257], "temperature": 0.0, "avg_logprob": -0.1700235160000353, "compression_ratio": 1.5260869565217392, "no_speech_prob": 2.6015695766545832e-06}, {"id": 294, "seek": 209022, "start": 2109.24, "end": 2116.04, "text": " really controversial hypothesis. And to be clear, we weren't sure we were right, but", "tokens": [534, 17323, 17291, 13, 400, 281, 312, 1850, 11, 321, 4999, 380, 988, 321, 645, 558, 11, 457], "temperature": 0.0, "avg_logprob": -0.1700235160000353, "compression_ratio": 1.5260869565217392, "no_speech_prob": 2.6015695766545832e-06}, {"id": 295, "seek": 211604, "start": 2116.04, "end": 2121.96, "text": " this was a feeling we had. So we thought, let's give it a go. So the first thing we", "tokens": [341, 390, 257, 2633, 321, 632, 13, 407, 321, 1194, 11, 718, 311, 976, 309, 257, 352, 13, 407, 264, 700, 551, 321], "temperature": 0.0, "avg_logprob": -0.1514958787238461, "compression_ratio": 1.5954545454545455, "no_speech_prob": 1.095250354410382e-05}, {"id": 296, "seek": 211604, "start": 2121.96, "end": 2130.56, "text": " created was a fast AI practical deep learning course. And certainly one thing we immediately", "tokens": [2942, 390, 257, 2370, 7318, 8496, 2452, 2539, 1164, 13, 400, 3297, 472, 551, 321, 4258], "temperature": 0.0, "avg_logprob": -0.1514958787238461, "compression_ratio": 1.5954545454545455, "no_speech_prob": 1.095250354410382e-05}, {"id": 297, "seek": 211604, "start": 2130.56, "end": 2135.44, "text": " saw, which was thrilling, and we certainly didn't know what would happen, was that it", "tokens": [1866, 11, 597, 390, 39347, 11, 293, 321, 3297, 994, 380, 458, 437, 576, 1051, 11, 390, 300, 309], "temperature": 0.0, "avg_logprob": -0.1514958787238461, "compression_ratio": 1.5954545454545455, "no_speech_prob": 1.095250354410382e-05}, {"id": 298, "seek": 211604, "start": 2135.44, "end": 2141.36, "text": " was popular. A lot of people took the course. We made it freely available online with no", "tokens": [390, 3743, 13, 316, 688, 295, 561, 1890, 264, 1164, 13, 492, 1027, 309, 16433, 2435, 2950, 365, 572], "temperature": 0.0, "avg_logprob": -0.1514958787238461, "compression_ratio": 1.5954545454545455, "no_speech_prob": 1.095250354410382e-05}, {"id": 299, "seek": 214136, "start": 2141.36, "end": 2148.88, "text": " ads, make it as accessible as possible since that's our mission. And I said to that first", "tokens": [10342, 11, 652, 309, 382, 9515, 382, 1944, 1670, 300, 311, 527, 4447, 13, 400, 286, 848, 281, 300, 700], "temperature": 0.0, "avg_logprob": -0.13912978452794691, "compression_ratio": 1.640552995391705, "no_speech_prob": 2.282668174302671e-05}, {"id": 300, "seek": 214136, "start": 2148.88, "end": 2154.28, "text": " class, if you create something interesting with deep learning, please tell us about it", "tokens": [1508, 11, 498, 291, 1884, 746, 1880, 365, 2452, 2539, 11, 1767, 980, 505, 466, 309], "temperature": 0.0, "avg_logprob": -0.13912978452794691, "compression_ratio": 1.640552995391705, "no_speech_prob": 2.282668174302671e-05}, {"id": 301, "seek": 214136, "start": 2154.28, "end": 2160.28, "text": " on our forums. So we created a forum so that students could communicate with each other.", "tokens": [322, 527, 26998, 13, 407, 321, 2942, 257, 17542, 370, 300, 1731, 727, 7890, 365, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.13912978452794691, "compression_ratio": 1.640552995391705, "no_speech_prob": 2.282668174302671e-05}, {"id": 302, "seek": 214136, "start": 2160.28, "end": 2167.6800000000003, "text": " And we got thousands of replies. And I remember one of the first ones we got, I think this", "tokens": [400, 321, 658, 5383, 295, 42289, 13, 400, 286, 1604, 472, 295, 264, 700, 2306, 321, 658, 11, 286, 519, 341], "temperature": 0.0, "avg_logprob": -0.13912978452794691, "compression_ratio": 1.640552995391705, "no_speech_prob": 2.282668174302671e-05}, {"id": 303, "seek": 216768, "start": 2167.68, "end": 2175.64, "text": " was one of the first, was somebody who tried to recognize cricket pictures from baseball", "tokens": [390, 472, 295, 264, 700, 11, 390, 2618, 567, 3031, 281, 5521, 31626, 5242, 490, 14323], "temperature": 0.0, "avg_logprob": -0.15727151158344316, "compression_ratio": 1.658878504672897, "no_speech_prob": 1.4284645658335648e-05}, {"id": 304, "seek": 216768, "start": 2175.64, "end": 2184.3199999999997, "text": " pictures. And they had, I think it was like 100% accuracy or maybe 99% accuracy. And one", "tokens": [5242, 13, 400, 436, 632, 11, 286, 519, 309, 390, 411, 2319, 4, 14170, 420, 1310, 11803, 4, 14170, 13, 400, 472], "temperature": 0.0, "avg_logprob": -0.15727151158344316, "compression_ratio": 1.658878504672897, "no_speech_prob": 1.4284645658335648e-05}, {"id": 305, "seek": 216768, "start": 2184.3199999999997, "end": 2189.8399999999997, "text": " of the really interesting things was that they only used 30 training images. And so", "tokens": [295, 264, 534, 1880, 721, 390, 300, 436, 787, 1143, 2217, 3097, 5267, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.15727151158344316, "compression_ratio": 1.658878504672897, "no_speech_prob": 1.4284645658335648e-05}, {"id": 306, "seek": 216768, "start": 2189.8399999999997, "end": 2194.8599999999997, "text": " this is like exciting to us to see somebody like building a model and not only that, building", "tokens": [341, 307, 411, 4670, 281, 505, 281, 536, 2618, 411, 2390, 257, 2316, 293, 406, 787, 300, 11, 2390], "temperature": 0.0, "avg_logprob": -0.15727151158344316, "compression_ratio": 1.658878504672897, "no_speech_prob": 1.4284645658335648e-05}, {"id": 307, "seek": 219486, "start": 2194.86, "end": 2201.2400000000002, "text": " it with far less data than people used to think was necessary. And then suddenly we", "tokens": [309, 365, 1400, 1570, 1412, 813, 561, 1143, 281, 519, 390, 4818, 13, 400, 550, 5800, 321], "temperature": 0.0, "avg_logprob": -0.13970470428466797, "compression_ratio": 1.6036036036036037, "no_speech_prob": 1.4284250028140377e-05}, {"id": 308, "seek": 219486, "start": 2201.2400000000002, "end": 2209.28, "text": " were being flattered with all these cool models people were building. So a Trident ad and", "tokens": [645, 885, 932, 13939, 365, 439, 613, 1627, 5245, 561, 645, 2390, 13, 407, 257, 1765, 1078, 614, 293], "temperature": 0.0, "avg_logprob": -0.13970470428466797, "compression_ratio": 1.6036036036036037, "no_speech_prob": 1.4284250028140377e-05}, {"id": 309, "seek": 219486, "start": 2209.28, "end": 2215.6800000000003, "text": " Tobago to different types of people model, a zucchini and cucumber model. This is a really", "tokens": [26350, 6442, 281, 819, 3467, 295, 561, 2316, 11, 257, 44781, 293, 28725, 2316, 13, 639, 307, 257, 534], "temperature": 0.0, "avg_logprob": -0.13970470428466797, "compression_ratio": 1.6036036036036037, "no_speech_prob": 1.4284250028140377e-05}, {"id": 310, "seek": 219486, "start": 2215.6800000000003, "end": 2221.08, "text": " interesting one. This person actually managed to predict what part of the world a satellite", "tokens": [1880, 472, 13, 639, 954, 767, 6453, 281, 6069, 437, 644, 295, 264, 1002, 257, 16016], "temperature": 0.0, "avg_logprob": -0.13970470428466797, "compression_ratio": 1.6036036036036037, "no_speech_prob": 1.4284250028140377e-05}, {"id": 311, "seek": 222108, "start": 2221.08, "end": 2227.84, "text": " photo was from with over 110 classes with 85% accuracy, which is extraordinary. A Panama", "tokens": [5052, 390, 490, 365, 670, 20154, 5359, 365, 14695, 4, 14170, 11, 597, 307, 10581, 13, 316, 41202], "temperature": 0.0, "avg_logprob": -0.14977632922890746, "compression_ratio": 1.5148936170212766, "no_speech_prob": 3.3206357329618186e-05}, {"id": 312, "seek": 222108, "start": 2227.84, "end": 2234.04, "text": " bus recognizer, a batik cloth recognizer. Some of the things were clearly actually going", "tokens": [1255, 3068, 6545, 11, 257, 7362, 1035, 13619, 3068, 6545, 13, 2188, 295, 264, 721, 645, 4448, 767, 516], "temperature": 0.0, "avg_logprob": -0.14977632922890746, "compression_ratio": 1.5148936170212766, "no_speech_prob": 3.3206357329618186e-05}, {"id": 313, "seek": 222108, "start": 2234.04, "end": 2238.2799999999997, "text": " to be very useful in practice. This was something useful for disaster resilience, which was", "tokens": [281, 312, 588, 4420, 294, 3124, 13, 639, 390, 746, 4420, 337, 11293, 19980, 11, 597, 390], "temperature": 0.0, "avg_logprob": -0.14977632922890746, "compression_ratio": 1.5148936170212766, "no_speech_prob": 3.3206357329618186e-05}, {"id": 314, "seek": 222108, "start": 2238.2799999999997, "end": 2248.16, "text": " recognizing the state of buildings in this place in Tanzania. We saw people even right", "tokens": [18538, 264, 1785, 295, 7446, 294, 341, 1081, 294, 42420, 5609, 13, 492, 1866, 561, 754, 558], "temperature": 0.0, "avg_logprob": -0.14977632922890746, "compression_ratio": 1.5148936170212766, "no_speech_prob": 3.3206357329618186e-05}, {"id": 315, "seek": 224816, "start": 2248.16, "end": 2257.12, "text": " at the start of the course breaking state of the art results. So this is on Devangari", "tokens": [412, 264, 722, 295, 264, 1164, 7697, 1785, 295, 264, 1523, 3542, 13, 407, 341, 307, 322, 9096, 656, 3504], "temperature": 0.0, "avg_logprob": -0.1493904338163488, "compression_ratio": 1.6941747572815533, "no_speech_prob": 4.005108348792419e-05}, {"id": 316, "seek": 224816, "start": 2257.12, "end": 2261.12, "text": " character recognition. This person said, wow, I just got a new state of the art result,", "tokens": [2517, 11150, 13, 639, 954, 848, 11, 6076, 11, 286, 445, 658, 257, 777, 1785, 295, 264, 1523, 1874, 11], "temperature": 0.0, "avg_logprob": -0.1493904338163488, "compression_ratio": 1.6941747572815533, "no_speech_prob": 4.005108348792419e-05}, {"id": 317, "seek": 224816, "start": 2261.12, "end": 2266.12, "text": " which is really exciting. We saw people doing the same symbol, us getting state of the art", "tokens": [597, 307, 534, 4670, 13, 492, 1866, 561, 884, 264, 912, 5986, 11, 505, 1242, 1785, 295, 264, 1523], "temperature": 0.0, "avg_logprob": -0.1493904338163488, "compression_ratio": 1.6941747572815533, "no_speech_prob": 4.005108348792419e-05}, {"id": 318, "seek": 224816, "start": 2266.12, "end": 2272.64, "text": " results on audio classification. And then even we started to hear from some of these", "tokens": [3542, 322, 6278, 21538, 13, 400, 550, 754, 321, 1409, 281, 1568, 490, 512, 295, 613], "temperature": 0.0, "avg_logprob": -0.1493904338163488, "compression_ratio": 1.6941747572815533, "no_speech_prob": 4.005108348792419e-05}, {"id": 319, "seek": 227264, "start": 2272.64, "end": 2279.68, "text": " students in the first year that they were taking their ideas back to their companies.", "tokens": [1731, 294, 264, 700, 1064, 300, 436, 645, 1940, 641, 3487, 646, 281, 641, 3431, 13], "temperature": 0.0, "avg_logprob": -0.12746907472610475, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00011764078226406127}, {"id": 320, "seek": 227264, "start": 2279.68, "end": 2286.7999999999997, "text": " And in this case, a software engineer went back to his company, he was working at Splunk", "tokens": [400, 294, 341, 1389, 11, 257, 4722, 11403, 1437, 646, 281, 702, 2237, 11, 415, 390, 1364, 412, 19788, 3197], "temperature": 0.0, "avg_logprob": -0.12746907472610475, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00011764078226406127}, {"id": 321, "seek": 227264, "start": 2286.7999999999997, "end": 2291.6, "text": " and built a new model, which basically took mouse movements and mouse clicks and turned", "tokens": [293, 3094, 257, 777, 2316, 11, 597, 1936, 1890, 9719, 9981, 293, 9719, 18521, 293, 3574], "temperature": 0.0, "avg_logprob": -0.12746907472610475, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00011764078226406127}, {"id": 322, "seek": 227264, "start": 2291.6, "end": 2299.96, "text": " them into pictures and then classified them and used this to help with fraud. And we know", "tokens": [552, 666, 5242, 293, 550, 20627, 552, 293, 1143, 341, 281, 854, 365, 14560, 13, 400, 321, 458], "temperature": 0.0, "avg_logprob": -0.12746907472610475, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00011764078226406127}, {"id": 323, "seek": 229996, "start": 2299.96, "end": 2305.16, "text": " about this because it was so successful that it ended up being a patented product and Splunk", "tokens": [466, 341, 570, 309, 390, 370, 4406, 300, 309, 4590, 493, 885, 257, 1947, 6003, 1674, 293, 19788, 3197], "temperature": 0.0, "avg_logprob": -0.16009992361068726, "compression_ratio": 1.579646017699115, "no_speech_prob": 2.3912802134873345e-05}, {"id": 324, "seek": 229996, "start": 2305.16, "end": 2313.48, "text": " created a blog about this cool new technology that was built by a software engineer with", "tokens": [2942, 257, 6968, 466, 341, 1627, 777, 2899, 300, 390, 3094, 538, 257, 4722, 11403, 365], "temperature": 0.0, "avg_logprob": -0.16009992361068726, "compression_ratio": 1.579646017699115, "no_speech_prob": 2.3912802134873345e-05}, {"id": 325, "seek": 229996, "start": 2313.48, "end": 2318.2400000000002, "text": " no previous background in this area. And we saw startups appearing. So for example, this", "tokens": [572, 3894, 3678, 294, 341, 1859, 13, 400, 321, 1866, 28041, 19870, 13, 407, 337, 1365, 11, 341], "temperature": 0.0, "avg_logprob": -0.16009992361068726, "compression_ratio": 1.579646017699115, "no_speech_prob": 2.3912802134873345e-05}, {"id": 326, "seek": 229996, "start": 2318.2400000000002, "end": 2325.12, "text": " startup called Envision appeared from one of our students and it's still going strong.", "tokens": [18578, 1219, 2193, 6763, 8516, 490, 472, 295, 527, 1731, 293, 309, 311, 920, 516, 2068, 13], "temperature": 0.0, "avg_logprob": -0.16009992361068726, "compression_ratio": 1.579646017699115, "no_speech_prob": 2.3912802134873345e-05}, {"id": 327, "seek": 232512, "start": 2325.12, "end": 2333.0, "text": " I just looked it up before this. And so, yeah, it was really cool to see how people from", "tokens": [286, 445, 2956, 309, 493, 949, 341, 13, 400, 370, 11, 1338, 11, 309, 390, 534, 1627, 281, 536, 577, 561, 490], "temperature": 0.0, "avg_logprob": -0.13571954614975873, "compression_ratio": 1.6, "no_speech_prob": 4.264210656401701e-05}, {"id": 328, "seek": 232512, "start": 2333.0, "end": 2343.2, "text": " all walks of life were actually able to get started with deep learning. And these courses", "tokens": [439, 12896, 295, 993, 645, 767, 1075, 281, 483, 1409, 365, 2452, 2539, 13, 400, 613, 7712], "temperature": 0.0, "avg_logprob": -0.13571954614975873, "compression_ratio": 1.6, "no_speech_prob": 4.264210656401701e-05}, {"id": 329, "seek": 232512, "start": 2343.2, "end": 2348.16, "text": " got really popular and so we started redoing them every year. So we'd build a new course", "tokens": [658, 534, 3743, 293, 370, 321, 1409, 29956, 278, 552, 633, 1064, 13, 407, 321, 1116, 1322, 257, 777, 1164], "temperature": 0.0, "avg_logprob": -0.13571954614975873, "compression_ratio": 1.6, "no_speech_prob": 4.264210656401701e-05}, {"id": 330, "seek": 232512, "start": 2348.16, "end": 2352.92, "text": " from scratch because things were moving so fast that within a year there was so much", "tokens": [490, 8459, 570, 721, 645, 2684, 370, 2370, 300, 1951, 257, 1064, 456, 390, 370, 709], "temperature": 0.0, "avg_logprob": -0.13571954614975873, "compression_ratio": 1.6, "no_speech_prob": 4.264210656401701e-05}, {"id": 331, "seek": 235292, "start": 2352.92, "end": 2359.88, "text": " new stuff to cover that we had built a completely new course. And so there's many millions of", "tokens": [777, 1507, 281, 2060, 300, 321, 632, 3094, 257, 2584, 777, 1164, 13, 400, 370, 456, 311, 867, 6803, 295], "temperature": 0.0, "avg_logprob": -0.1557155137651422, "compression_ratio": 1.5866666666666667, "no_speech_prob": 6.853683316876413e-06}, {"id": 332, "seek": 235292, "start": 2359.88, "end": 2367.84, "text": " views at this point and people are loving them based on what they're telling YouTube", "tokens": [6809, 412, 341, 935, 293, 561, 366, 9344, 552, 2361, 322, 437, 436, 434, 3585, 3088], "temperature": 0.0, "avg_logprob": -0.1557155137651422, "compression_ratio": 1.5866666666666667, "no_speech_prob": 6.853683316876413e-06}, {"id": 333, "seek": 235292, "start": 2367.84, "end": 2375.6800000000003, "text": " anyway. So this has been really a pleasure to see. We ended up turning the course into", "tokens": [4033, 13, 407, 341, 575, 668, 534, 257, 6834, 281, 536, 13, 492, 4590, 493, 6246, 264, 1164, 666], "temperature": 0.0, "avg_logprob": -0.1557155137651422, "compression_ratio": 1.5866666666666667, "no_speech_prob": 6.853683316876413e-06}, {"id": 334, "seek": 235292, "start": 2375.6800000000003, "end": 2380.2400000000002, "text": " a book as well, along with my friend, Sylvain Gouger, and people are really liking the book", "tokens": [257, 1446, 382, 731, 11, 2051, 365, 452, 1277, 11, 3902, 14574, 491, 460, 513, 260, 11, 293, 561, 366, 534, 16933, 264, 1446], "temperature": 0.0, "avg_logprob": -0.1557155137651422, "compression_ratio": 1.5866666666666667, "no_speech_prob": 6.853683316876413e-06}, {"id": 335, "seek": 238024, "start": 2380.24, "end": 2391.2, "text": " as well. So the next step after getting people started with what do we already know by putting", "tokens": [382, 731, 13, 407, 264, 958, 1823, 934, 1242, 561, 1409, 365, 437, 360, 321, 1217, 458, 538, 3372], "temperature": 0.0, "avg_logprob": -0.1154091131119501, "compression_ratio": 1.6188340807174888, "no_speech_prob": 1.3210740689828526e-05}, {"id": 336, "seek": 238024, "start": 2391.2, "end": 2396.2799999999997, "text": " that into a course is we wanted to push the boundaries beyond what we already know. And", "tokens": [300, 666, 257, 1164, 307, 321, 1415, 281, 2944, 264, 13180, 4399, 437, 321, 1217, 458, 13, 400], "temperature": 0.0, "avg_logprob": -0.1154091131119501, "compression_ratio": 1.6188340807174888, "no_speech_prob": 1.3210740689828526e-05}, {"id": 337, "seek": 238024, "start": 2396.2799999999997, "end": 2403.0, "text": " so one of the things that came up was a lot of students or potential students were saying,", "tokens": [370, 472, 295, 264, 721, 300, 1361, 493, 390, 257, 688, 295, 1731, 420, 3995, 1731, 645, 1566, 11], "temperature": 0.0, "avg_logprob": -0.1154091131119501, "compression_ratio": 1.6188340807174888, "no_speech_prob": 1.3210740689828526e-05}, {"id": 338, "seek": 238024, "start": 2403.0, "end": 2406.7999999999997, "text": " I don't think it's worth me getting involved in deep learning because it takes too much", "tokens": [286, 500, 380, 519, 309, 311, 3163, 385, 1242, 3288, 294, 2452, 2539, 570, 309, 2516, 886, 709], "temperature": 0.0, "avg_logprob": -0.1154091131119501, "compression_ratio": 1.6188340807174888, "no_speech_prob": 1.3210740689828526e-05}, {"id": 339, "seek": 240680, "start": 2406.8, "end": 2412.7200000000003, "text": " compute and too much data. And it's, you know, unless you're a Google or Facebook, you can't", "tokens": [14722, 293, 886, 709, 1412, 13, 400, 309, 311, 11, 291, 458, 11, 5969, 291, 434, 257, 3329, 420, 4384, 11, 291, 393, 380], "temperature": 0.0, "avg_logprob": -0.11914426903975638, "compression_ratio": 1.584070796460177, "no_speech_prob": 6.92021640134044e-05}, {"id": 340, "seek": 240680, "start": 2412.7200000000003, "end": 2418.7200000000003, "text": " do it. And this became particularly an issue when Google released their TPUs and put out", "tokens": [360, 309, 13, 400, 341, 3062, 4098, 364, 2734, 562, 3329, 4736, 641, 314, 8115, 82, 293, 829, 484], "temperature": 0.0, "avg_logprob": -0.11914426903975638, "compression_ratio": 1.584070796460177, "no_speech_prob": 6.92021640134044e-05}, {"id": 341, "seek": 240680, "start": 2418.7200000000003, "end": 2425.0800000000004, "text": " a big PR exercise saying, okay, you know, TPUs are so great that nobody else, you know,", "tokens": [257, 955, 11568, 5380, 1566, 11, 1392, 11, 291, 458, 11, 314, 8115, 82, 366, 370, 869, 300, 5079, 1646, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.11914426903975638, "compression_ratio": 1.584070796460177, "no_speech_prob": 6.92021640134044e-05}, {"id": 342, "seek": 240680, "start": 2425.0800000000004, "end": 2430.96, "text": " can pretty much do anything useful in deep learning now. And so we decided to enter this", "tokens": [393, 1238, 709, 360, 1340, 4420, 294, 2452, 2539, 586, 13, 400, 370, 321, 3047, 281, 3242, 341], "temperature": 0.0, "avg_logprob": -0.11914426903975638, "compression_ratio": 1.584070796460177, "no_speech_prob": 6.92021640134044e-05}, {"id": 343, "seek": 243096, "start": 2430.96, "end": 2437.68, "text": " international competition called Dawnbench that Google and Intel had entered to see if", "tokens": [5058, 6211, 1219, 26001, 47244, 300, 3329, 293, 19762, 632, 9065, 281, 536, 498], "temperature": 0.0, "avg_logprob": -0.1877420925703205, "compression_ratio": 1.4886363636363635, "no_speech_prob": 1.3630560715682805e-05}, {"id": 344, "seek": 243096, "start": 2437.68, "end": 2445.08, "text": " we could beat them, like be faster than them at training deep learning models. And so that", "tokens": [321, 727, 4224, 552, 11, 411, 312, 4663, 813, 552, 412, 3097, 2452, 2539, 5245, 13, 400, 370, 300], "temperature": 0.0, "avg_logprob": -0.1877420925703205, "compression_ratio": 1.4886363636363635, "no_speech_prob": 1.3630560715682805e-05}, {"id": 345, "seek": 243096, "start": 2445.08, "end": 2458.68, "text": " was April 2018. And we won on many of the axes of the competition. The cheapest, the", "tokens": [390, 6929, 6096, 13, 400, 321, 1582, 322, 867, 295, 264, 35387, 295, 264, 6211, 13, 440, 29167, 11, 264], "temperature": 0.0, "avg_logprob": -0.1877420925703205, "compression_ratio": 1.4886363636363635, "no_speech_prob": 1.3630560715682805e-05}, {"id": 346, "seek": 245868, "start": 2458.68, "end": 2463.52, "text": " fastest GPU, fastest single machine. And then we followed this up with additional results", "tokens": [14573, 18407, 11, 14573, 2167, 3479, 13, 400, 550, 321, 6263, 341, 493, 365, 4497, 3542], "temperature": 0.0, "avg_logprob": -0.10404736318705994, "compression_ratio": 1.5411255411255411, "no_speech_prob": 1.1478407031972893e-05}, {"id": 347, "seek": 245868, "start": 2463.52, "end": 2469.08, "text": " that were actually 40% faster than Google's best TPU results. And so this was exciting", "tokens": [300, 645, 767, 3356, 4, 4663, 813, 3329, 311, 1151, 314, 8115, 3542, 13, 400, 370, 341, 390, 4670], "temperature": 0.0, "avg_logprob": -0.10404736318705994, "compression_ratio": 1.5411255411255411, "no_speech_prob": 1.1478407031972893e-05}, {"id": 348, "seek": 245868, "start": 2469.08, "end": 2476.0, "text": " because here's a picture of us and our students working on this project together. It was a", "tokens": [570, 510, 311, 257, 3036, 295, 505, 293, 527, 1731, 1364, 322, 341, 1716, 1214, 13, 467, 390, 257], "temperature": 0.0, "avg_logprob": -0.10404736318705994, "compression_ratio": 1.5411255411255411, "no_speech_prob": 1.1478407031972893e-05}, {"id": 349, "seek": 245868, "start": 2476.0, "end": 2482.8399999999997, "text": " really cool time. You know, because we really wanted to push back against this narrative", "tokens": [534, 1627, 565, 13, 509, 458, 11, 570, 321, 534, 1415, 281, 2944, 646, 1970, 341, 9977], "temperature": 0.0, "avg_logprob": -0.10404736318705994, "compression_ratio": 1.5411255411255411, "no_speech_prob": 1.1478407031972893e-05}, {"id": 350, "seek": 248284, "start": 2482.84, "end": 2491.6400000000003, "text": " that you have to be Google. And so it got a lot of media attention, which was great.", "tokens": [300, 291, 362, 281, 312, 3329, 13, 400, 370, 309, 658, 257, 688, 295, 3021, 3202, 11, 597, 390, 869, 13], "temperature": 0.0, "avg_logprob": -0.10193608701229095, "compression_ratio": 1.5180722891566265, "no_speech_prob": 3.1200139346765354e-05}, {"id": 351, "seek": 248284, "start": 2491.6400000000003, "end": 2499.32, "text": " And it really, the finding here was using common sense is more important than using", "tokens": [400, 309, 534, 11, 264, 5006, 510, 390, 1228, 2689, 2020, 307, 544, 1021, 813, 1228], "temperature": 0.0, "avg_logprob": -0.10193608701229095, "compression_ratio": 1.5180722891566265, "no_speech_prob": 3.1200139346765354e-05}, {"id": 352, "seek": 248284, "start": 2499.32, "end": 2507.04, "text": " vast amounts of money and compute. It was really cool to see also that a lot of the", "tokens": [8369, 11663, 295, 1460, 293, 14722, 13, 467, 390, 534, 1627, 281, 536, 611, 300, 257, 688, 295, 264], "temperature": 0.0, "avg_logprob": -0.10193608701229095, "compression_ratio": 1.5180722891566265, "no_speech_prob": 3.1200139346765354e-05}, {"id": 353, "seek": 250704, "start": 2507.04, "end": 2514.32, "text": " big companies noticed what we were doing and bought in our ideas. So Nvidia, when they", "tokens": [955, 3431, 5694, 437, 321, 645, 884, 293, 4243, 294, 527, 3487, 13, 407, 46284, 11, 562, 436], "temperature": 0.0, "avg_logprob": -0.11501574907146517, "compression_ratio": 1.5170454545454546, "no_speech_prob": 2.6015250114141963e-06}, {"id": 354, "seek": 250704, "start": 2514.32, "end": 2519.7599999999998, "text": " started promoting how great their GPUs were, they started talking about, you know, how", "tokens": [1409, 16383, 577, 869, 641, 18407, 82, 645, 11, 436, 1409, 1417, 466, 11, 291, 458, 11, 577], "temperature": 0.0, "avg_logprob": -0.11501574907146517, "compression_ratio": 1.5170454545454546, "no_speech_prob": 2.6015250114141963e-06}, {"id": 355, "seek": 250704, "start": 2519.7599999999998, "end": 2528.84, "text": " good they were with the additional ideas that we had developed with our students. So academic", "tokens": [665, 436, 645, 365, 264, 4497, 3487, 300, 321, 632, 4743, 365, 527, 1731, 13, 407, 7778], "temperature": 0.0, "avg_logprob": -0.11501574907146517, "compression_ratio": 1.5170454545454546, "no_speech_prob": 2.6015250114141963e-06}, {"id": 356, "seek": 252884, "start": 2528.84, "end": 2537.76, "text": " research became a critical component of fast AI's work. And we did similar research to", "tokens": [2132, 3062, 257, 4924, 6542, 295, 2370, 7318, 311, 589, 13, 400, 321, 630, 2531, 2132, 281], "temperature": 0.0, "avg_logprob": -0.11026602352366728, "compression_ratio": 1.5512820512820513, "no_speech_prob": 6.6431389313947875e-06}, {"id": 357, "seek": 252884, "start": 2537.76, "end": 2546.88, "text": " drive breakthroughs in natural language processing, in tabular modeling and lots of other areas.", "tokens": [3332, 22397, 82, 294, 3303, 2856, 9007, 11, 294, 4421, 1040, 15983, 293, 3195, 295, 661, 3179, 13], "temperature": 0.0, "avg_logprob": -0.11026602352366728, "compression_ratio": 1.5512820512820513, "no_speech_prob": 6.6431389313947875e-06}, {"id": 358, "seek": 252884, "start": 2546.88, "end": 2550.88, "text": " So then the question is, okay, well with all these, you know, now that we've actually pushed", "tokens": [407, 550, 264, 1168, 307, 11, 1392, 11, 731, 365, 439, 613, 11, 291, 458, 11, 586, 300, 321, 600, 767, 9152], "temperature": 0.0, "avg_logprob": -0.11026602352366728, "compression_ratio": 1.5512820512820513, "no_speech_prob": 6.6431389313947875e-06}, {"id": 359, "seek": 252884, "start": 2550.88, "end": 2557.0, "text": " the boundaries beyond what's already known to say, okay, we can actually do get better", "tokens": [264, 13180, 4399, 437, 311, 1217, 2570, 281, 584, 11, 1392, 11, 321, 393, 767, 360, 483, 1101], "temperature": 0.0, "avg_logprob": -0.11026602352366728, "compression_ratio": 1.5512820512820513, "no_speech_prob": 6.6431389313947875e-06}, {"id": 360, "seek": 255700, "start": 2557.0, "end": 2562.32, "text": " results with less data and less compute more quickly. How do we put that into the hands", "tokens": [3542, 365, 1570, 1412, 293, 1570, 14722, 544, 2661, 13, 1012, 360, 321, 829, 300, 666, 264, 2377], "temperature": 0.0, "avg_logprob": -0.10292920612153553, "compression_ratio": 1.5258620689655173, "no_speech_prob": 2.9479092518158723e-06}, {"id": 361, "seek": 255700, "start": 2562.32, "end": 2569.32, "text": " of everybody so that everybody can use these insights? So that's why we decided to build", "tokens": [295, 2201, 370, 300, 2201, 393, 764, 613, 14310, 30, 407, 300, 311, 983, 321, 3047, 281, 1322], "temperature": 0.0, "avg_logprob": -0.10292920612153553, "compression_ratio": 1.5258620689655173, "no_speech_prob": 2.9479092518158723e-06}, {"id": 362, "seek": 255700, "start": 2569.32, "end": 2575.8, "text": " a software library called fast AI. So that was just in 2018 that version one came out,", "tokens": [257, 4722, 6405, 1219, 2370, 7318, 13, 407, 300, 390, 445, 294, 6096, 300, 3037, 472, 1361, 484, 11], "temperature": 0.0, "avg_logprob": -0.10292920612153553, "compression_ratio": 1.5258620689655173, "no_speech_prob": 2.9479092518158723e-06}, {"id": 363, "seek": 255700, "start": 2575.8, "end": 2583.48, "text": " but it immediately got a lot of attention. It got supported by all the big cloud services.", "tokens": [457, 309, 4258, 658, 257, 688, 295, 3202, 13, 467, 658, 8104, 538, 439, 264, 955, 4588, 3328, 13], "temperature": 0.0, "avg_logprob": -0.10292920612153553, "compression_ratio": 1.5258620689655173, "no_speech_prob": 2.9479092518158723e-06}, {"id": 364, "seek": 258348, "start": 2583.48, "end": 2589.2, "text": " And we were able to show that compared to Keras, for example, it was much more accurate,", "tokens": [400, 321, 645, 1075, 281, 855, 300, 5347, 281, 591, 6985, 11, 337, 1365, 11, 309, 390, 709, 544, 8559, 11], "temperature": 0.0, "avg_logprob": -0.09446151360221532, "compression_ratio": 1.592760180995475, "no_speech_prob": 5.507240985025419e-06}, {"id": 365, "seek": 258348, "start": 2589.2, "end": 2600.0, "text": " much faster, far, far less lines of code. And we really tried to make it as accessible", "tokens": [709, 4663, 11, 1400, 11, 1400, 1570, 3876, 295, 3089, 13, 400, 321, 534, 3031, 281, 652, 309, 382, 9515], "temperature": 0.0, "avg_logprob": -0.09446151360221532, "compression_ratio": 1.592760180995475, "no_speech_prob": 5.507240985025419e-06}, {"id": 366, "seek": 258348, "start": 2600.0, "end": 2605.72, "text": " as possible. So the, you know, this is some of the documentation from fast AI. You can", "tokens": [382, 1944, 13, 407, 264, 11, 291, 458, 11, 341, 307, 512, 295, 264, 14333, 490, 2370, 7318, 13, 509, 393], "temperature": 0.0, "avg_logprob": -0.09446151360221532, "compression_ratio": 1.592760180995475, "no_speech_prob": 5.507240985025419e-06}, {"id": 367, "seek": 258348, "start": 2605.72, "end": 2611.44, "text": " see that not only do you get the normal kind of API documentation, but it's actually, you", "tokens": [536, 300, 406, 787, 360, 291, 483, 264, 2710, 733, 295, 9362, 14333, 11, 457, 309, 311, 767, 11, 291], "temperature": 0.0, "avg_logprob": -0.09446151360221532, "compression_ratio": 1.592760180995475, "no_speech_prob": 5.507240985025419e-06}, {"id": 368, "seek": 261144, "start": 2611.44, "end": 2615.44, "text": " know, got pictures of exactly what's going on. It's got links to the papers that it's", "tokens": [458, 11, 658, 5242, 295, 2293, 437, 311, 516, 322, 13, 467, 311, 658, 6123, 281, 264, 10577, 300, 309, 311], "temperature": 0.0, "avg_logprob": -0.09405986000509824, "compression_ratio": 1.8319672131147542, "no_speech_prob": 1.9524879462551326e-05}, {"id": 369, "seek": 261144, "start": 2615.44, "end": 2620.92, "text": " implementing. And also all of the code for all of the pictures is all directly there", "tokens": [18114, 13, 400, 611, 439, 295, 264, 3089, 337, 439, 295, 264, 5242, 307, 439, 3838, 456], "temperature": 0.0, "avg_logprob": -0.09405986000509824, "compression_ratio": 1.8319672131147542, "no_speech_prob": 1.9524879462551326e-05}, {"id": 370, "seek": 261144, "start": 2620.92, "end": 2626.2000000000003, "text": " as well. And one of the really nice things is that every single page of the documentation", "tokens": [382, 731, 13, 400, 472, 295, 264, 534, 1481, 721, 307, 300, 633, 2167, 3028, 295, 264, 14333], "temperature": 0.0, "avg_logprob": -0.09405986000509824, "compression_ratio": 1.8319672131147542, "no_speech_prob": 1.9524879462551326e-05}, {"id": 371, "seek": 261144, "start": 2626.2000000000003, "end": 2631.68, "text": " has a link to let you actually open that page of documentation as an interactive notebook", "tokens": [575, 257, 2113, 281, 718, 291, 767, 1269, 300, 3028, 295, 14333, 382, 364, 15141, 21060], "temperature": 0.0, "avg_logprob": -0.09405986000509824, "compression_ratio": 1.8319672131147542, "no_speech_prob": 1.9524879462551326e-05}, {"id": 372, "seek": 261144, "start": 2631.68, "end": 2636.56, "text": " because the entire thing is built with interactive notebooks. So you can then get the exact same", "tokens": [570, 264, 2302, 551, 307, 3094, 365, 15141, 43782, 13, 407, 291, 393, 550, 483, 264, 1900, 912], "temperature": 0.0, "avg_logprob": -0.09405986000509824, "compression_ratio": 1.8319672131147542, "no_speech_prob": 1.9524879462551326e-05}, {"id": 373, "seek": 263656, "start": 2636.56, "end": 2642.12, "text": " thing, but now you can experiment with it and you can see all the source code there.", "tokens": [551, 11, 457, 586, 291, 393, 5120, 365, 309, 293, 291, 393, 536, 439, 264, 4009, 3089, 456, 13], "temperature": 0.0, "avg_logprob": -0.10213394165039062, "compression_ratio": 1.7254901960784315, "no_speech_prob": 1.994702643060009e-06}, {"id": 374, "seek": 263656, "start": 2642.12, "end": 2647.2799999999997, "text": " So we really took the kind of approaches that we found worked well in our course of having", "tokens": [407, 321, 534, 1890, 264, 733, 295, 11587, 300, 321, 1352, 2732, 731, 294, 527, 1164, 295, 1419], "temperature": 0.0, "avg_logprob": -0.10213394165039062, "compression_ratio": 1.7254901960784315, "no_speech_prob": 1.994702643060009e-06}, {"id": 375, "seek": 263656, "start": 2647.2799999999997, "end": 2653.72, "text": " students do lots of experiments and lots of coding and making that a kind of part of our", "tokens": [1731, 360, 3195, 295, 12050, 293, 3195, 295, 17720, 293, 1455, 300, 257, 733, 295, 644, 295, 527], "temperature": 0.0, "avg_logprob": -0.10213394165039062, "compression_ratio": 1.7254901960784315, "no_speech_prob": 1.994702643060009e-06}, {"id": 376, "seek": 263656, "start": 2653.72, "end": 2659.12, "text": " documentation as well is to let people really play, play with everything themselves and", "tokens": [14333, 382, 731, 307, 281, 718, 561, 534, 862, 11, 862, 365, 1203, 2969, 293], "temperature": 0.0, "avg_logprob": -0.10213394165039062, "compression_ratio": 1.7254901960784315, "no_speech_prob": 1.994702643060009e-06}, {"id": 377, "seek": 265912, "start": 2659.12, "end": 2667.52, "text": " experiment and see how it all works. So incorporating all of this research into the software was", "tokens": [5120, 293, 536, 577, 309, 439, 1985, 13, 407, 33613, 439, 295, 341, 2132, 666, 264, 4722, 390], "temperature": 0.0, "avg_logprob": -0.10829898922942406, "compression_ratio": 1.5397489539748954, "no_speech_prob": 1.788022473192541e-06}, {"id": 378, "seek": 265912, "start": 2667.52, "end": 2672.48, "text": " super successful. We started hearing from people saying, okay, well I've just started", "tokens": [1687, 4406, 13, 492, 1409, 4763, 490, 561, 1566, 11, 1392, 11, 731, 286, 600, 445, 1409], "temperature": 0.0, "avg_logprob": -0.10829898922942406, "compression_ratio": 1.5397489539748954, "no_speech_prob": 1.788022473192541e-06}, {"id": 379, "seek": 265912, "start": 2672.48, "end": 2678.52, "text": " with fast AI and I've started pulling across some of my TensorFlow models and I don't understand", "tokens": [365, 2370, 7318, 293, 286, 600, 1409, 8407, 2108, 512, 295, 452, 37624, 5245, 293, 286, 500, 380, 1223], "temperature": 0.0, "avg_logprob": -0.10829898922942406, "compression_ratio": 1.5397489539748954, "no_speech_prob": 1.788022473192541e-06}, {"id": 380, "seek": 265912, "start": 2678.52, "end": 2684.24, "text": " why is everything so much better? You know, what's, what's going on here? So people were", "tokens": [983, 307, 1203, 370, 709, 1101, 30, 509, 458, 11, 437, 311, 11, 437, 311, 516, 322, 510, 30, 407, 561, 645], "temperature": 0.0, "avg_logprob": -0.10829898922942406, "compression_ratio": 1.5397489539748954, "no_speech_prob": 1.788022473192541e-06}, {"id": 381, "seek": 268424, "start": 2684.24, "end": 2693.12, "text": " really noticing that they were getting dramatically better results. So this person said the same", "tokens": [534, 21814, 300, 436, 645, 1242, 17548, 1101, 3542, 13, 407, 341, 954, 848, 264, 912], "temperature": 0.0, "avg_logprob": -0.1118302228974133, "compression_ratio": 1.6589861751152073, "no_speech_prob": 1.6027321407818818e-06}, {"id": 382, "seek": 268424, "start": 2693.12, "end": 2697.9599999999996, "text": " thing. Yup. We used to try to use TensorFlow. We spent months tweaking our model. We switched", "tokens": [551, 13, 13593, 13, 492, 1143, 281, 853, 281, 764, 37624, 13, 492, 4418, 2493, 6986, 2456, 527, 2316, 13, 492, 16858], "temperature": 0.0, "avg_logprob": -0.1118302228974133, "compression_ratio": 1.6589861751152073, "no_speech_prob": 1.6027321407818818e-06}, {"id": 383, "seek": 268424, "start": 2697.9599999999996, "end": 2703.0, "text": " to fast AI and within a couple of days we were getting better results. So by kind of", "tokens": [281, 2370, 7318, 293, 1951, 257, 1916, 295, 1708, 321, 645, 1242, 1101, 3542, 13, 407, 538, 733, 295], "temperature": 0.0, "avg_logprob": -0.1118302228974133, "compression_ratio": 1.6589861751152073, "no_speech_prob": 1.6027321407818818e-06}, {"id": 384, "seek": 268424, "start": 2703.0, "end": 2710.8799999999997, "text": " combining the research with the software, we were able to provide a software library", "tokens": [21928, 264, 2132, 365, 264, 4722, 11, 321, 645, 1075, 281, 2893, 257, 4722, 6405], "temperature": 0.0, "avg_logprob": -0.1118302228974133, "compression_ratio": 1.6589861751152073, "no_speech_prob": 1.6027321407818818e-06}, {"id": 385, "seek": 271088, "start": 2710.88, "end": 2717.0, "text": " that let people get started more quickly. And then version two, which has been around", "tokens": [300, 718, 561, 483, 1409, 544, 2661, 13, 400, 550, 3037, 732, 11, 597, 575, 668, 926], "temperature": 0.0, "avg_logprob": -0.122348184938784, "compression_ratio": 1.5391304347826087, "no_speech_prob": 5.682344635715708e-06}, {"id": 386, "seek": 271088, "start": 2717.0, "end": 2721.6800000000003, "text": " for a bit over a year now, was a very dramatic advance further still. There's a whole academic", "tokens": [337, 257, 857, 670, 257, 1064, 586, 11, 390, 257, 588, 12023, 7295, 3052, 920, 13, 821, 311, 257, 1379, 7778], "temperature": 0.0, "avg_logprob": -0.122348184938784, "compression_ratio": 1.5391304347826087, "no_speech_prob": 5.682344635715708e-06}, {"id": 387, "seek": 271088, "start": 2721.6800000000003, "end": 2730.08, "text": " paper that you can read describing all the deep design approaches which we've used. One", "tokens": [3035, 300, 291, 393, 1401, 16141, 439, 264, 2452, 1715, 11587, 597, 321, 600, 1143, 13, 1485], "temperature": 0.0, "avg_logprob": -0.122348184938784, "compression_ratio": 1.5391304347826087, "no_speech_prob": 5.682344635715708e-06}, {"id": 388, "seek": 271088, "start": 2730.08, "end": 2735.36, "text": " of the really nice things about it is that basically regardless of what you're trying", "tokens": [295, 264, 534, 1481, 721, 466, 309, 307, 300, 1936, 10060, 295, 437, 291, 434, 1382], "temperature": 0.0, "avg_logprob": -0.122348184938784, "compression_ratio": 1.5391304347826087, "no_speech_prob": 5.682344635715708e-06}, {"id": 389, "seek": 273536, "start": 2735.36, "end": 2742.2400000000002, "text": " to do with fast AI, you can use almost exactly the same code. So for example, here's the", "tokens": [281, 360, 365, 2370, 7318, 11, 291, 393, 764, 1920, 2293, 264, 912, 3089, 13, 407, 337, 1365, 11, 510, 311, 264], "temperature": 0.0, "avg_logprob": -0.10399386015805331, "compression_ratio": 1.8190954773869348, "no_speech_prob": 7.889118933235295e-06}, {"id": 390, "seek": 273536, "start": 2742.2400000000002, "end": 2749.48, "text": " code necessary to recognize dogs from cats. Here's the code necessary to build a segmentation", "tokens": [3089, 4818, 281, 5521, 7197, 490, 11111, 13, 1692, 311, 264, 3089, 4818, 281, 1322, 257, 9469, 399], "temperature": 0.0, "avg_logprob": -0.10399386015805331, "compression_ratio": 1.8190954773869348, "no_speech_prob": 7.889118933235295e-06}, {"id": 391, "seek": 273536, "start": 2749.48, "end": 2757.76, "text": " model. It's basically the same lines of code. Here's a code to classify text movie reviews,", "tokens": [2316, 13, 467, 311, 1936, 264, 912, 3876, 295, 3089, 13, 1692, 311, 257, 3089, 281, 33872, 2487, 3169, 10229, 11], "temperature": 0.0, "avg_logprob": -0.10399386015805331, "compression_ratio": 1.8190954773869348, "no_speech_prob": 7.889118933235295e-06}, {"id": 392, "seek": 273536, "start": 2757.76, "end": 2762.6800000000003, "text": " almost the same lines of code. Here's the code necessary to do collaborative filtering,", "tokens": [1920, 264, 912, 3876, 295, 3089, 13, 1692, 311, 264, 3089, 4818, 281, 360, 16555, 30822, 11], "temperature": 0.0, "avg_logprob": -0.10399386015805331, "compression_ratio": 1.8190954773869348, "no_speech_prob": 7.889118933235295e-06}, {"id": 393, "seek": 276268, "start": 2762.68, "end": 2769.52, "text": " almost the same lines of code. So I said earlier that kind of under the covers, different models", "tokens": [1920, 264, 912, 3876, 295, 3089, 13, 407, 286, 848, 3071, 300, 733, 295, 833, 264, 10538, 11, 819, 5245], "temperature": 0.0, "avg_logprob": -0.08469459622405297, "compression_ratio": 1.6088888888888888, "no_speech_prob": 5.014626367483288e-06}, {"id": 394, "seek": 276268, "start": 2769.52, "end": 2773.7999999999997, "text": " look more similar than different with deep learning. And so with fast AI, we've really", "tokens": [574, 544, 2531, 813, 819, 365, 2452, 2539, 13, 400, 370, 365, 2370, 7318, 11, 321, 600, 534], "temperature": 0.0, "avg_logprob": -0.08469459622405297, "compression_ratio": 1.6088888888888888, "no_speech_prob": 5.014626367483288e-06}, {"id": 395, "seek": 276268, "start": 2773.7999999999997, "end": 2782.8399999999997, "text": " tried to surface that so that you learn one API and you can use it anywhere. That's not", "tokens": [3031, 281, 3753, 300, 370, 300, 291, 1466, 472, 9362, 293, 291, 393, 764, 309, 4992, 13, 663, 311, 406], "temperature": 0.0, "avg_logprob": -0.08469459622405297, "compression_ratio": 1.6088888888888888, "no_speech_prob": 5.014626367483288e-06}, {"id": 396, "seek": 276268, "start": 2782.8399999999997, "end": 2788.22, "text": " enough for researchers or people that really need to dig deeper. So one of the really nice", "tokens": [1547, 337, 10309, 420, 561, 300, 534, 643, 281, 2528, 7731, 13, 407, 472, 295, 264, 534, 1481], "temperature": 0.0, "avg_logprob": -0.08469459622405297, "compression_ratio": 1.6088888888888888, "no_speech_prob": 5.014626367483288e-06}, {"id": 397, "seek": 278822, "start": 2788.22, "end": 2794.2, "text": " things about that is that underneath this applications layer is a tiered or layered", "tokens": [721, 466, 300, 307, 300, 7223, 341, 5821, 4583, 307, 257, 12362, 292, 420, 34666], "temperature": 0.0, "avg_logprob": -0.12868623506455196, "compression_ratio": 1.5585585585585586, "no_speech_prob": 6.143983227957506e-06}, {"id": 398, "seek": 278822, "start": 2794.2, "end": 2801.56, "text": " API where you can go in and change anything. And I'm not going to describe it in too much", "tokens": [9362, 689, 291, 393, 352, 294, 293, 1319, 1340, 13, 400, 286, 478, 406, 516, 281, 6786, 309, 294, 886, 709], "temperature": 0.0, "avg_logprob": -0.12868623506455196, "compression_ratio": 1.5585585585585586, "no_speech_prob": 6.143983227957506e-06}, {"id": 399, "seek": 278822, "start": 2801.56, "end": 2810.7999999999997, "text": " detail, but for example, part of this mid tier API is a new two-way callback system", "tokens": [2607, 11, 457, 337, 1365, 11, 644, 295, 341, 2062, 12362, 9362, 307, 257, 777, 732, 12, 676, 818, 3207, 1185], "temperature": 0.0, "avg_logprob": -0.12868623506455196, "compression_ratio": 1.5585585585585586, "no_speech_prob": 6.143983227957506e-06}, {"id": 400, "seek": 278822, "start": 2810.7999999999997, "end": 2815.68, "text": " which basically allows you at any point when you're training a model to see exactly what", "tokens": [597, 1936, 4045, 291, 412, 604, 935, 562, 291, 434, 3097, 257, 2316, 281, 536, 2293, 437], "temperature": 0.0, "avg_logprob": -0.12868623506455196, "compression_ratio": 1.5585585585585586, "no_speech_prob": 6.143983227957506e-06}, {"id": 401, "seek": 281568, "start": 2815.68, "end": 2821.9199999999996, "text": " it's doing and to change literally anything that it's doing. You can skip parts of the", "tokens": [309, 311, 884, 293, 281, 1319, 3736, 1340, 300, 309, 311, 884, 13, 509, 393, 10023, 3166, 295, 264], "temperature": 0.0, "avg_logprob": -0.13159547103078742, "compression_ratio": 1.7799043062200957, "no_speech_prob": 5.8627147154766135e-06}, {"id": 402, "seek": 281568, "start": 2821.9199999999996, "end": 2828.8799999999997, "text": " training process, you can change the gradients, you can change the data and so forth. And", "tokens": [3097, 1399, 11, 291, 393, 1319, 264, 2771, 2448, 11, 291, 393, 1319, 264, 1412, 293, 370, 5220, 13, 400], "temperature": 0.0, "avg_logprob": -0.13159547103078742, "compression_ratio": 1.7799043062200957, "no_speech_prob": 5.8627147154766135e-06}, {"id": 403, "seek": 281568, "start": 2828.8799999999997, "end": 2837.9199999999996, "text": " so with this new approach, we're able to implement, for example, this is from a paper called mixup,", "tokens": [370, 365, 341, 777, 3109, 11, 321, 434, 1075, 281, 4445, 11, 337, 1365, 11, 341, 307, 490, 257, 3035, 1219, 2890, 1010, 11], "temperature": 0.0, "avg_logprob": -0.13159547103078742, "compression_ratio": 1.7799043062200957, "no_speech_prob": 5.8627147154766135e-06}, {"id": 404, "seek": 281568, "start": 2837.9199999999996, "end": 2843.48, "text": " we're able to implement mixup data augmentation in just a few lines of code. And if you compare", "tokens": [321, 434, 1075, 281, 4445, 2890, 1010, 1412, 14501, 19631, 294, 445, 257, 1326, 3876, 295, 3089, 13, 400, 498, 291, 6794], "temperature": 0.0, "avg_logprob": -0.13159547103078742, "compression_ratio": 1.7799043062200957, "no_speech_prob": 5.8627147154766135e-06}, {"id": 405, "seek": 284348, "start": 2843.48, "end": 2850.08, "text": " that to the actual original Facebook paper, not only was it far more lines of code, this", "tokens": [300, 281, 264, 3539, 3380, 4384, 3035, 11, 406, 787, 390, 309, 1400, 544, 3876, 295, 3089, 11, 341], "temperature": 0.0, "avg_logprob": -0.12050470893765673, "compression_ratio": 1.5414847161572052, "no_speech_prob": 2.3687312022957485e-06}, {"id": 406, "seek": 284348, "start": 2850.08, "end": 2855.14, "text": " is what it looks like from their research paper without using callbacks, but it's also", "tokens": [307, 437, 309, 1542, 411, 490, 641, 2132, 3035, 1553, 1228, 818, 17758, 11, 457, 309, 311, 611], "temperature": 0.0, "avg_logprob": -0.12050470893765673, "compression_ratio": 1.5414847161572052, "no_speech_prob": 2.3687312022957485e-06}, {"id": 407, "seek": 284348, "start": 2855.14, "end": 2861.36, "text": " far less flexible because everything's hard coded or else with this approach, you can", "tokens": [1400, 1570, 11358, 570, 1203, 311, 1152, 34874, 420, 1646, 365, 341, 3109, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.12050470893765673, "compression_ratio": 1.5414847161572052, "no_speech_prob": 2.3687312022957485e-06}, {"id": 408, "seek": 284348, "start": 2861.36, "end": 2868.2400000000002, "text": " mix and match really easily. Another example of this layered API is we built a new approach", "tokens": [2890, 293, 2995, 534, 3612, 13, 3996, 1365, 295, 341, 34666, 9362, 307, 321, 3094, 257, 777, 3109], "temperature": 0.0, "avg_logprob": -0.12050470893765673, "compression_ratio": 1.5414847161572052, "no_speech_prob": 2.3687312022957485e-06}, {"id": 409, "seek": 286824, "start": 2868.24, "end": 2874.8399999999997, "text": " to creating new optimizers using just two concepts, stats and steppers. I won't go into", "tokens": [281, 4084, 777, 5028, 22525, 1228, 445, 732, 10392, 11, 18152, 293, 2126, 15226, 13, 286, 1582, 380, 352, 666], "temperature": 0.0, "avg_logprob": -0.15216598510742188, "compression_ratio": 1.6, "no_speech_prob": 2.190736267948523e-06}, {"id": 410, "seek": 286824, "start": 2874.8399999999997, "end": 2881.12, "text": " the details, but in short, this is what a particular optimizer called AdamW looks like", "tokens": [264, 4365, 11, 457, 294, 2099, 11, 341, 307, 437, 257, 1729, 5028, 6545, 1219, 7938, 54, 1542, 411], "temperature": 0.0, "avg_logprob": -0.15216598510742188, "compression_ratio": 1.6, "no_speech_prob": 2.190736267948523e-06}, {"id": 411, "seek": 286824, "start": 2881.12, "end": 2888.9599999999996, "text": " in PyTorch. And this took about two years between the paper being released and Facebook", "tokens": [294, 9953, 51, 284, 339, 13, 400, 341, 1890, 466, 732, 924, 1296, 264, 3035, 885, 4736, 293, 4384], "temperature": 0.0, "avg_logprob": -0.15216598510742188, "compression_ratio": 1.6, "no_speech_prob": 2.190736267948523e-06}, {"id": 412, "seek": 286824, "start": 2888.9599999999996, "end": 2895.7999999999997, "text": " releasing the AdamW implementation. Our implementation was released within a day of the paper and", "tokens": [16327, 264, 7938, 54, 11420, 13, 2621, 11420, 390, 4736, 1951, 257, 786, 295, 264, 3035, 293], "temperature": 0.0, "avg_logprob": -0.15216598510742188, "compression_ratio": 1.6, "no_speech_prob": 2.190736267948523e-06}, {"id": 413, "seek": 289580, "start": 2895.8, "end": 2902.8, "text": " it consists of these one, two, three, four, five words. Because we're leveraging this", "tokens": [309, 14689, 295, 613, 472, 11, 732, 11, 1045, 11, 1451, 11, 1732, 2283, 13, 1436, 321, 434, 32666, 341], "temperature": 0.0, "avg_logprob": -0.10687028950658338, "compression_ratio": 1.5361702127659576, "no_speech_prob": 1.1300317964924034e-05}, {"id": 414, "seek": 289580, "start": 2902.8, "end": 2911.6800000000003, "text": " layered API for optimizers, it's basically really easy to utilize the components to quickly", "tokens": [34666, 9362, 337, 5028, 22525, 11, 309, 311, 1936, 534, 1858, 281, 16117, 264, 6677, 281, 2661], "temperature": 0.0, "avg_logprob": -0.10687028950658338, "compression_ratio": 1.5361702127659576, "no_speech_prob": 1.1300317964924034e-05}, {"id": 415, "seek": 289580, "start": 2911.6800000000003, "end": 2917.6400000000003, "text": " implement new papers. Here's another example of an optimizer. This one's called lamb. This", "tokens": [4445, 777, 10577, 13, 1692, 311, 1071, 1365, 295, 364, 5028, 6545, 13, 639, 472, 311, 1219, 10097, 13, 639], "temperature": 0.0, "avg_logprob": -0.10687028950658338, "compression_ratio": 1.5361702127659576, "no_speech_prob": 1.1300317964924034e-05}, {"id": 416, "seek": 289580, "start": 2917.6400000000003, "end": 2924.2000000000003, "text": " came from a Google paper. And one of the really cool things you might notice is that there's", "tokens": [1361, 490, 257, 3329, 3035, 13, 400, 472, 295, 264, 534, 1627, 721, 291, 1062, 3449, 307, 300, 456, 311], "temperature": 0.0, "avg_logprob": -0.10687028950658338, "compression_ratio": 1.5361702127659576, "no_speech_prob": 1.1300317964924034e-05}, {"id": 417, "seek": 292420, "start": 2924.2, "end": 2931.52, "text": " a very close match between the lines of code in our implementation and the lines of math", "tokens": [257, 588, 1998, 2995, 1296, 264, 3876, 295, 3089, 294, 527, 11420, 293, 264, 3876, 295, 5221], "temperature": 0.0, "avg_logprob": -0.16289127736851788, "compression_ratio": 1.4860335195530727, "no_speech_prob": 2.2252575035963673e-06}, {"id": 418, "seek": 292420, "start": 2931.52, "end": 2942.3599999999997, "text": " in the algorithm in the paper. So anyway, there's a little summary of both what I'm", "tokens": [294, 264, 9284, 294, 264, 3035, 13, 407, 4033, 11, 456, 311, 257, 707, 12691, 295, 1293, 437, 286, 478], "temperature": 0.0, "avg_logprob": -0.16289127736851788, "compression_ratio": 1.4860335195530727, "no_speech_prob": 2.2252575035963673e-06}, {"id": 419, "seek": 292420, "start": 2942.3599999999997, "end": 2952.7599999999998, "text": " doing now with FastAI and how I got there and why. And yeah, I'm happy to take any questions.", "tokens": [884, 586, 365, 15968, 48698, 293, 577, 286, 658, 456, 293, 983, 13, 400, 1338, 11, 286, 478, 2055, 281, 747, 604, 1651, 13], "temperature": 0.0, "avg_logprob": -0.16289127736851788, "compression_ratio": 1.4860335195530727, "no_speech_prob": 2.2252575035963673e-06}, {"id": 420, "seek": 295276, "start": 2952.76, "end": 2961.76, "text": " Thanks, Jeremy. So yeah, really interesting kind of historical view of where you came", "tokens": [2561, 11, 17809, 13, 407, 1338, 11, 534, 1880, 733, 295, 8584, 1910, 295, 689, 291, 1361], "temperature": 0.0, "avg_logprob": -0.24118781812263257, "compression_ratio": 1.417989417989418, "no_speech_prob": 2.2824857296654955e-05}, {"id": 421, "seek": 295276, "start": 2961.76, "end": 2970.8, "text": " from. So I guess, so I'll start with a quick thing. So I mentioned, so like in deep learning,", "tokens": [490, 13, 407, 286, 2041, 11, 370, 286, 603, 722, 365, 257, 1702, 551, 13, 407, 286, 2835, 11, 370, 411, 294, 2452, 2539, 11], "temperature": 0.0, "avg_logprob": -0.24118781812263257, "compression_ratio": 1.417989417989418, "no_speech_prob": 2.2824857296654955e-05}, {"id": 422, "seek": 295276, "start": 2970.8, "end": 2979.4, "text": " that obviously there's very similar structures and code and solving problems. But how do", "tokens": [300, 2745, 456, 311, 588, 2531, 9227, 293, 3089, 293, 12606, 2740, 13, 583, 577, 360], "temperature": 0.0, "avg_logprob": -0.24118781812263257, "compression_ratio": 1.417989417989418, "no_speech_prob": 2.2824857296654955e-05}, {"id": 423, "seek": 297940, "start": 2979.4, "end": 2987.8, "text": " you incorporate things like knowledge about the problem? Obviously the type of architecture", "tokens": [291, 16091, 721, 411, 3601, 466, 264, 1154, 30, 7580, 264, 2010, 295, 9482], "temperature": 0.0, "avg_logprob": -0.1688367770268367, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.1125370292575099e-05}, {"id": 424, "seek": 297940, "start": 2987.8, "end": 2991.64, "text": " that would have to go in there would come from the context of the problem.", "tokens": [300, 576, 362, 281, 352, 294, 456, 576, 808, 490, 264, 4319, 295, 264, 1154, 13], "temperature": 0.0, "avg_logprob": -0.1688367770268367, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.1125370292575099e-05}, {"id": 425, "seek": 297940, "start": 2991.64, "end": 2999.48, "text": " Yeah, that's a great question. So there's a number of really interesting ways of incorporating", "tokens": [865, 11, 300, 311, 257, 869, 1168, 13, 407, 456, 311, 257, 1230, 295, 534, 1880, 2098, 295, 33613], "temperature": 0.0, "avg_logprob": -0.1688367770268367, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.1125370292575099e-05}, {"id": 426, "seek": 297940, "start": 2999.48, "end": 3006.6800000000003, "text": " knowledge about the problem. And it's a really important thing to do because this is like,", "tokens": [3601, 466, 264, 1154, 13, 400, 309, 311, 257, 534, 1021, 551, 281, 360, 570, 341, 307, 411, 11], "temperature": 0.0, "avg_logprob": -0.1688367770268367, "compression_ratio": 1.6842105263157894, "no_speech_prob": 1.1125370292575099e-05}, {"id": 427, "seek": 300668, "start": 3006.68, "end": 3011.7599999999998, "text": " this is how you kind of get a whole lot of extra performance and need less data and less", "tokens": [341, 307, 577, 291, 733, 295, 483, 257, 1379, 688, 295, 2857, 3389, 293, 643, 1570, 1412, 293, 1570], "temperature": 0.0, "avg_logprob": -0.11142628533499581, "compression_ratio": 1.695067264573991, "no_speech_prob": 2.6015254661615472e-06}, {"id": 428, "seek": 300668, "start": 3011.7599999999998, "end": 3020.12, "text": " time, the more of that knowledge you can incorporate. So yeah, one way is certainly to directly", "tokens": [565, 11, 264, 544, 295, 300, 3601, 291, 393, 16091, 13, 407, 1338, 11, 472, 636, 307, 3297, 281, 3838], "temperature": 0.0, "avg_logprob": -0.11142628533499581, "compression_ratio": 1.695067264573991, "no_speech_prob": 2.6015254661615472e-06}, {"id": 429, "seek": 300668, "start": 3020.12, "end": 3028.9199999999996, "text": " implement it in the architecture. So for example, a very popular architecture for computer vision", "tokens": [4445, 309, 294, 264, 9482, 13, 407, 337, 1365, 11, 257, 588, 3743, 9482, 337, 3820, 5201], "temperature": 0.0, "avg_logprob": -0.11142628533499581, "compression_ratio": 1.695067264573991, "no_speech_prob": 2.6015254661615472e-06}, {"id": 430, "seek": 300668, "start": 3028.9199999999996, "end": 3035.8799999999997, "text": " is convolutional architecture. And the convolution, a 2D convolution is taking advantage of our", "tokens": [307, 45216, 304, 9482, 13, 400, 264, 45216, 11, 257, 568, 35, 45216, 307, 1940, 5002, 295, 527], "temperature": 0.0, "avg_logprob": -0.11142628533499581, "compression_ratio": 1.695067264573991, "no_speech_prob": 2.6015254661615472e-06}, {"id": 431, "seek": 303588, "start": 3035.88, "end": 3040.92, "text": " domain knowledge, which is that there's generally autocorrelation across pixels in both the", "tokens": [9274, 3601, 11, 597, 307, 300, 456, 311, 5101, 45833, 284, 4419, 399, 2108, 18668, 294, 1293, 264], "temperature": 0.0, "avg_logprob": -0.09328908096125096, "compression_ratio": 1.643835616438356, "no_speech_prob": 3.426483090152033e-05}, {"id": 432, "seek": 303588, "start": 3040.92, "end": 3050.6400000000003, "text": " X and Y dimensions. And so we're basically mapping a set of weights across groups of", "tokens": [1783, 293, 398, 12819, 13, 400, 370, 321, 434, 1936, 18350, 257, 992, 295, 17443, 2108, 3935, 295], "temperature": 0.0, "avg_logprob": -0.09328908096125096, "compression_ratio": 1.643835616438356, "no_speech_prob": 3.426483090152033e-05}, {"id": 433, "seek": 303588, "start": 3050.6400000000003, "end": 3056.76, "text": " pixels that are all next to each other. There's a really wide range of interesting ways of", "tokens": [18668, 300, 366, 439, 958, 281, 1184, 661, 13, 821, 311, 257, 534, 4874, 3613, 295, 1880, 2098, 295], "temperature": 0.0, "avg_logprob": -0.09328908096125096, "compression_ratio": 1.643835616438356, "no_speech_prob": 3.426483090152033e-05}, {"id": 434, "seek": 303588, "start": 3056.76, "end": 3065.12, "text": " incorporating all kinds of domain knowledge into architectures. And there's lots of geometry", "tokens": [33613, 439, 3685, 295, 9274, 3601, 666, 6331, 1303, 13, 400, 456, 311, 3195, 295, 18426], "temperature": 0.0, "avg_logprob": -0.09328908096125096, "compression_ratio": 1.643835616438356, "no_speech_prob": 3.426483090152033e-05}, {"id": 435, "seek": 306512, "start": 3065.12, "end": 3070.72, "text": " based approaches of doing that within natural language processing. There's lots of autoregressive", "tokens": [2361, 11587, 295, 884, 300, 1951, 3303, 2856, 9007, 13, 821, 311, 3195, 295, 1476, 418, 3091, 488], "temperature": 0.0, "avg_logprob": -0.07178402680617113, "compression_ratio": 1.4945054945054945, "no_speech_prob": 1.1478457963676192e-05}, {"id": 436, "seek": 306512, "start": 3070.72, "end": 3080.44, "text": " approaches there. That's one area. An area I am extremely fond of is data augmentation.", "tokens": [11587, 456, 13, 663, 311, 472, 1859, 13, 1107, 1859, 286, 669, 4664, 9557, 295, 307, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.07178402680617113, "compression_ratio": 1.4945054945054945, "no_speech_prob": 1.1478457963676192e-05}, {"id": 437, "seek": 306512, "start": 3080.44, "end": 3089.56, "text": " And in particular, there's been a huge kind of improvement in the last 12 months or so", "tokens": [400, 294, 1729, 11, 456, 311, 668, 257, 2603, 733, 295, 10444, 294, 264, 1036, 2272, 2493, 420, 370], "temperature": 0.0, "avg_logprob": -0.07178402680617113, "compression_ratio": 1.4945054945054945, "no_speech_prob": 1.1478457963676192e-05}, {"id": 438, "seek": 308956, "start": 3089.56, "end": 3097.2, "text": " in how much we can do with a tiny amount of data by using something called self-supervised", "tokens": [294, 577, 709, 321, 393, 360, 365, 257, 5870, 2372, 295, 1412, 538, 1228, 746, 1219, 2698, 12, 48172, 24420], "temperature": 0.0, "avg_logprob": -0.12960150155676417, "compression_ratio": 1.6445497630331753, "no_speech_prob": 6.643142569373595e-06}, {"id": 439, "seek": 308956, "start": 3097.2, "end": 3102.16, "text": " learning and in particular using something called contrastive loss. And what this is", "tokens": [2539, 293, 294, 1729, 1228, 746, 1219, 8712, 488, 4470, 13, 400, 437, 341, 307], "temperature": 0.0, "avg_logprob": -0.12960150155676417, "compression_ratio": 1.6445497630331753, "no_speech_prob": 6.643142569373595e-06}, {"id": 440, "seek": 308956, "start": 3102.16, "end": 3107.68, "text": " doing is you basically come up with really, try to come up with really thoughtful data", "tokens": [884, 307, 291, 1936, 808, 493, 365, 534, 11, 853, 281, 808, 493, 365, 534, 21566, 1412], "temperature": 0.0, "avg_logprob": -0.12960150155676417, "compression_ratio": 1.6445497630331753, "no_speech_prob": 6.643142569373595e-06}, {"id": 441, "seek": 308956, "start": 3107.68, "end": 3114.2, "text": " augmentation approaches where you can say like, okay, so for example, in NLP, one of", "tokens": [14501, 19631, 11587, 689, 291, 393, 584, 411, 11, 1392, 11, 370, 337, 1365, 11, 294, 426, 45196, 11, 472, 295], "temperature": 0.0, "avg_logprob": -0.12960150155676417, "compression_ratio": 1.6445497630331753, "no_speech_prob": 6.643142569373595e-06}, {"id": 442, "seek": 311420, "start": 3114.2, "end": 3119.7599999999998, "text": " the approaches is to translate each sentence with a translation model into a different", "tokens": [264, 11587, 307, 281, 13799, 1184, 8174, 365, 257, 12853, 2316, 666, 257, 819], "temperature": 0.0, "avg_logprob": -0.07957758623010971, "compression_ratio": 1.7991967871485943, "no_speech_prob": 1.8738513745120144e-06}, {"id": 443, "seek": 311420, "start": 3119.7599999999998, "end": 3123.64, "text": " language, and then translate it back again. So you're now going to get a different version", "tokens": [2856, 11, 293, 550, 13799, 309, 646, 797, 13, 407, 291, 434, 586, 516, 281, 483, 257, 819, 3037], "temperature": 0.0, "avg_logprob": -0.07957758623010971, "compression_ratio": 1.7991967871485943, "no_speech_prob": 1.8738513745120144e-06}, {"id": 444, "seek": 311420, "start": 3123.64, "end": 3127.48, "text": " of the same sentence, but it shouldn't mean the same thing. And so then with contrastive", "tokens": [295, 264, 912, 8174, 11, 457, 309, 4659, 380, 914, 264, 912, 551, 13, 400, 370, 550, 365, 8712, 488], "temperature": 0.0, "avg_logprob": -0.07957758623010971, "compression_ratio": 1.7991967871485943, "no_speech_prob": 1.8738513745120144e-06}, {"id": 445, "seek": 311420, "start": 3127.48, "end": 3131.8799999999997, "text": " loss, it basically says you add a part to the loss function that says those two different", "tokens": [4470, 11, 309, 1936, 1619, 291, 909, 257, 644, 281, 264, 4470, 2445, 300, 1619, 729, 732, 819], "temperature": 0.0, "avg_logprob": -0.07957758623010971, "compression_ratio": 1.7991967871485943, "no_speech_prob": 1.8738513745120144e-06}, {"id": 446, "seek": 311420, "start": 3131.8799999999997, "end": 3141.24, "text": " sentences should have the same result in our model. And so with something called UDA, which", "tokens": [16579, 820, 362, 264, 912, 1874, 294, 527, 2316, 13, 400, 370, 365, 746, 1219, 624, 7509, 11, 597], "temperature": 0.0, "avg_logprob": -0.07957758623010971, "compression_ratio": 1.7991967871485943, "no_speech_prob": 1.8738513745120144e-06}, {"id": 447, "seek": 314124, "start": 3141.24, "end": 3147.7599999999998, "text": " is basically adding contrastive loss and self-supervised learning to NLP, they were able to get results", "tokens": [307, 1936, 5127, 8712, 488, 4470, 293, 2698, 12, 48172, 24420, 2539, 281, 426, 45196, 11, 436, 645, 1075, 281, 483, 3542], "temperature": 0.0, "avg_logprob": -0.10661656357521235, "compression_ratio": 1.5397489539748954, "no_speech_prob": 8.800602699921e-06}, {"id": 448, "seek": 314124, "start": 3147.7599999999998, "end": 3153.2, "text": " for movie review classification with just 20 labeled examples that were better than", "tokens": [337, 3169, 3131, 21538, 365, 445, 945, 21335, 5110, 300, 645, 1101, 813], "temperature": 0.0, "avg_logprob": -0.10661656357521235, "compression_ratio": 1.5397489539748954, "no_speech_prob": 8.800602699921e-06}, {"id": 449, "seek": 314124, "start": 3153.2, "end": 3159.08, "text": " the previous state of the art using 25,000 labeled examples. Anyway, there's lots of", "tokens": [264, 3894, 1785, 295, 264, 1523, 1228, 3552, 11, 1360, 21335, 5110, 13, 5684, 11, 456, 311, 3195, 295], "temperature": 0.0, "avg_logprob": -0.10661656357521235, "compression_ratio": 1.5397489539748954, "no_speech_prob": 8.800602699921e-06}, {"id": 450, "seek": 314124, "start": 3159.08, "end": 3163.9599999999996, "text": " ways we can incorporate domain knowledge into models, but there's a couple of ones that", "tokens": [2098, 321, 393, 16091, 9274, 3601, 666, 5245, 11, 457, 456, 311, 257, 1916, 295, 2306, 300], "temperature": 0.0, "avg_logprob": -0.10661656357521235, "compression_ratio": 1.5397489539748954, "no_speech_prob": 8.800602699921e-06}, {"id": 451, "seek": 314124, "start": 3163.9599999999996, "end": 3164.9599999999996, "text": " I like.", "tokens": [286, 411, 13], "temperature": 0.0, "avg_logprob": -0.10661656357521235, "compression_ratio": 1.5397489539748954, "no_speech_prob": 8.800602699921e-06}, {"id": 452, "seek": 316496, "start": 3164.96, "end": 3174.52, "text": " Yes, I guess there are a couple of questions about interpretability. So one of the questions", "tokens": [1079, 11, 286, 2041, 456, 366, 257, 1916, 295, 1651, 466, 7302, 2310, 13, 407, 472, 295, 264, 1651], "temperature": 0.0, "avg_logprob": -0.1789169806938667, "compression_ratio": 1.5406698564593302, "no_speech_prob": 1.3630959074362181e-05}, {"id": 453, "seek": 316496, "start": 3174.52, "end": 3181.16, "text": " that came up is it's hard to explain to stakeholders. And so how can you convince them that deep", "tokens": [300, 1361, 493, 307, 309, 311, 1152, 281, 2903, 281, 17779, 13, 400, 370, 577, 393, 291, 13447, 552, 300, 2452], "temperature": 0.0, "avg_logprob": -0.1789169806938667, "compression_ratio": 1.5406698564593302, "no_speech_prob": 1.3630959074362181e-05}, {"id": 454, "seek": 316496, "start": 3181.16, "end": 3186.96, "text": " learning is worth adopting? I mean, obviously you can show predictive performance, but is", "tokens": [2539, 307, 3163, 32328, 30, 286, 914, 11, 2745, 291, 393, 855, 35521, 3389, 11, 457, 307], "temperature": 0.0, "avg_logprob": -0.1789169806938667, "compression_ratio": 1.5406698564593302, "no_speech_prob": 1.3630959074362181e-05}, {"id": 455, "seek": 316496, "start": 3186.96, "end": 3191.48, "text": " there any other ways that you can do that?", "tokens": [456, 604, 661, 2098, 300, 291, 393, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.1789169806938667, "compression_ratio": 1.5406698564593302, "no_speech_prob": 1.3630959074362181e-05}, {"id": 456, "seek": 319148, "start": 3191.48, "end": 3197.16, "text": " Sure. So my view is that deep learning models are much more interpretable and explainable", "tokens": [4894, 13, 407, 452, 1910, 307, 300, 2452, 2539, 5245, 366, 709, 544, 7302, 712, 293, 2903, 712], "temperature": 0.0, "avg_logprob": -0.09369712747553344, "compression_ratio": 1.7470355731225296, "no_speech_prob": 8.939448889577761e-06}, {"id": 457, "seek": 319148, "start": 3197.16, "end": 3203.64, "text": " than most regression models, for example. Generally speaking, the traditional approach", "tokens": [813, 881, 24590, 5245, 11, 337, 1365, 13, 21082, 4124, 11, 264, 5164, 3109], "temperature": 0.0, "avg_logprob": -0.09369712747553344, "compression_ratio": 1.7470355731225296, "no_speech_prob": 8.939448889577761e-06}, {"id": 458, "seek": 319148, "start": 3203.64, "end": 3209.44, "text": " to people thinking the right way to understand, for example, regression models is to look", "tokens": [281, 561, 1953, 264, 558, 636, 281, 1223, 11, 337, 1365, 11, 24590, 5245, 307, 281, 574], "temperature": 0.0, "avg_logprob": -0.09369712747553344, "compression_ratio": 1.7470355731225296, "no_speech_prob": 8.939448889577761e-06}, {"id": 459, "seek": 319148, "start": 3209.44, "end": 3214.66, "text": " at their coefficients. And I've always told people don't do that because in almost any", "tokens": [412, 641, 31994, 13, 400, 286, 600, 1009, 1907, 561, 500, 380, 360, 300, 570, 294, 1920, 604], "temperature": 0.0, "avg_logprob": -0.09369712747553344, "compression_ratio": 1.7470355731225296, "no_speech_prob": 8.939448889577761e-06}, {"id": 460, "seek": 319148, "start": 3214.66, "end": 3219.52, "text": " real world regression problem, you've got coefficients representing interactions, you've", "tokens": [957, 1002, 24590, 1154, 11, 291, 600, 658, 31994, 13460, 13280, 11, 291, 600], "temperature": 0.0, "avg_logprob": -0.09369712747553344, "compression_ratio": 1.7470355731225296, "no_speech_prob": 8.939448889577761e-06}, {"id": 461, "seek": 321952, "start": 3219.52, "end": 3224.2, "text": " got coefficients on things that are co-linear, you've got coefficients on various different", "tokens": [658, 31994, 322, 721, 300, 366, 598, 12, 28263, 11, 291, 600, 658, 31994, 322, 3683, 819], "temperature": 0.0, "avg_logprob": -0.10203027725219727, "compression_ratio": 1.721698113207547, "no_speech_prob": 1.2411214811436366e-05}, {"id": 462, "seek": 321952, "start": 3224.2, "end": 3230.56, "text": " bases of a transformed nonlinear variable. None of the coefficients can be understood", "tokens": [17949, 295, 257, 16894, 2107, 28263, 7006, 13, 14492, 295, 264, 31994, 393, 312, 7320], "temperature": 0.0, "avg_logprob": -0.10203027725219727, "compression_ratio": 1.721698113207547, "no_speech_prob": 1.2411214811436366e-05}, {"id": 463, "seek": 321952, "start": 3230.56, "end": 3235.72, "text": " independently because they can only be understood as how they combine with all the other things", "tokens": [21761, 570, 436, 393, 787, 312, 7320, 382, 577, 436, 10432, 365, 439, 264, 661, 721], "temperature": 0.0, "avg_logprob": -0.10203027725219727, "compression_ratio": 1.721698113207547, "no_speech_prob": 1.2411214811436366e-05}, {"id": 464, "seek": 321952, "start": 3235.72, "end": 3244.12, "text": " that are related. So I generally really dislike it when people try and explain a regression", "tokens": [300, 366, 4077, 13, 407, 286, 5101, 534, 26006, 309, 562, 561, 853, 293, 2903, 257, 24590], "temperature": 0.0, "avg_logprob": -0.10203027725219727, "compression_ratio": 1.721698113207547, "no_speech_prob": 1.2411214811436366e-05}, {"id": 465, "seek": 324412, "start": 3244.12, "end": 3251.24, "text": " by looking at coefficients. To me, the right way to understand a model is to do the same", "tokens": [538, 1237, 412, 31994, 13, 1407, 385, 11, 264, 558, 636, 281, 1223, 257, 2316, 307, 281, 360, 264, 912], "temperature": 0.0, "avg_logprob": -0.10166847821578238, "compression_ratio": 1.74, "no_speech_prob": 3.1380814107251354e-06}, {"id": 466, "seek": 324412, "start": 3251.24, "end": 3256.6, "text": " thing we would do to understand a person, which is to ask questions. And so whether", "tokens": [551, 321, 576, 360, 281, 1223, 257, 954, 11, 597, 307, 281, 1029, 1651, 13, 400, 370, 1968], "temperature": 0.0, "avg_logprob": -0.10166847821578238, "compression_ratio": 1.74, "no_speech_prob": 3.1380814107251354e-06}, {"id": 467, "seek": 324412, "start": 3256.6, "end": 3261.3199999999997, "text": " it's a regression or a random forest or a deep learning model, you can generally easily", "tokens": [309, 311, 257, 24590, 420, 257, 4974, 6719, 420, 257, 2452, 2539, 2316, 11, 291, 393, 5101, 3612], "temperature": 0.0, "avg_logprob": -0.10166847821578238, "compression_ratio": 1.74, "no_speech_prob": 3.1380814107251354e-06}, {"id": 468, "seek": 324412, "start": 3261.3199999999997, "end": 3266.72, "text": " ask questions like what would happen if I made this variable on this row a little bit", "tokens": [1029, 1651, 411, 437, 576, 1051, 498, 286, 1027, 341, 7006, 322, 341, 5386, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.10166847821578238, "compression_ratio": 1.74, "no_speech_prob": 3.1380814107251354e-06}, {"id": 469, "seek": 324412, "start": 3266.72, "end": 3272.88, "text": " bigger or a little bit smaller or things like that, which actually are much easier to do", "tokens": [3801, 420, 257, 707, 857, 4356, 420, 721, 411, 300, 11, 597, 767, 366, 709, 3571, 281, 360], "temperature": 0.0, "avg_logprob": -0.10166847821578238, "compression_ratio": 1.74, "no_speech_prob": 3.1380814107251354e-06}, {"id": 470, "seek": 327288, "start": 3272.88, "end": 3276.56, "text": " in deep learning because in deep learning, those are just questions about the derivative", "tokens": [294, 2452, 2539, 570, 294, 2452, 2539, 11, 729, 366, 445, 1651, 466, 264, 13760], "temperature": 0.0, "avg_logprob": -0.12424718340237935, "compression_ratio": 1.764, "no_speech_prob": 3.844843831757316e-06}, {"id": 471, "seek": 327288, "start": 3276.56, "end": 3282.0, "text": " of the input. And so you can actually get them much more quickly and easily. You can", "tokens": [295, 264, 4846, 13, 400, 370, 291, 393, 767, 483, 552, 709, 544, 2661, 293, 3612, 13, 509, 393], "temperature": 0.0, "avg_logprob": -0.12424718340237935, "compression_ratio": 1.764, "no_speech_prob": 3.844843831757316e-06}, {"id": 472, "seek": 327288, "start": 3282.0, "end": 3287.28, "text": " also do really interesting things with deep learning around showing which things are similar", "tokens": [611, 360, 534, 1880, 721, 365, 2452, 2539, 926, 4099, 597, 721, 366, 2531], "temperature": 0.0, "avg_logprob": -0.12424718340237935, "compression_ratio": 1.764, "no_speech_prob": 3.844843831757316e-06}, {"id": 473, "seek": 327288, "start": 3287.28, "end": 3296.6400000000003, "text": " to each other kind of in the deep learning feature space. And you can build really cool", "tokens": [281, 1184, 661, 733, 295, 294, 264, 2452, 2539, 4111, 1901, 13, 400, 291, 393, 1322, 534, 1627], "temperature": 0.0, "avg_logprob": -0.12424718340237935, "compression_ratio": 1.764, "no_speech_prob": 3.844843831757316e-06}, {"id": 474, "seek": 327288, "start": 3296.6400000000003, "end": 3301.2400000000002, "text": " applications for domain experts then, which can give them a lot of comfort. So you can", "tokens": [5821, 337, 9274, 8572, 550, 11, 597, 393, 976, 552, 257, 688, 295, 3400, 13, 407, 291, 393], "temperature": 0.0, "avg_logprob": -0.12424718340237935, "compression_ratio": 1.764, "no_speech_prob": 3.844843831757316e-06}, {"id": 475, "seek": 330124, "start": 3301.24, "end": 3308.24, "text": " say, yes, it's accurate. But I can also show you which parts of the input are particularly", "tokens": [584, 11, 2086, 11, 309, 311, 8559, 13, 583, 286, 393, 611, 855, 291, 597, 3166, 295, 264, 4846, 366, 4098], "temperature": 0.0, "avg_logprob": -0.132560430254255, "compression_ratio": 1.6771653543307086, "no_speech_prob": 3.6473247746471316e-05}, {"id": 476, "seek": 330124, "start": 3308.24, "end": 3314.56, "text": " important in this case, which other inputs are similar to this one. And often we find", "tokens": [1021, 294, 341, 1389, 11, 597, 661, 15743, 366, 2531, 281, 341, 472, 13, 400, 2049, 321, 915], "temperature": 0.0, "avg_logprob": -0.132560430254255, "compression_ratio": 1.6771653543307086, "no_speech_prob": 3.6473247746471316e-05}, {"id": 477, "seek": 330124, "start": 3314.56, "end": 3318.3199999999997, "text": " like, for example, in the medical space, doctors will kind of go, wow, that's really clever", "tokens": [411, 11, 337, 1365, 11, 294, 264, 4625, 1901, 11, 8778, 486, 733, 295, 352, 11, 6076, 11, 300, 311, 534, 13494], "temperature": 0.0, "avg_logprob": -0.132560430254255, "compression_ratio": 1.6771653543307086, "no_speech_prob": 3.6473247746471316e-05}, {"id": 478, "seek": 330124, "start": 3318.3199999999997, "end": 3322.68, "text": " the way it recognized that this patient and this patient were similar. A lot of doctors", "tokens": [264, 636, 309, 9823, 300, 341, 4537, 293, 341, 4537, 645, 2531, 13, 316, 688, 295, 8778], "temperature": 0.0, "avg_logprob": -0.132560430254255, "compression_ratio": 1.6771653543307086, "no_speech_prob": 3.6473247746471316e-05}, {"id": 479, "seek": 330124, "start": 3322.68, "end": 3327.2799999999997, "text": " wouldn't have noticed that. It's actually this subtle thing going on.", "tokens": [2759, 380, 362, 5694, 300, 13, 467, 311, 767, 341, 13743, 551, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.132560430254255, "compression_ratio": 1.6771653543307086, "no_speech_prob": 3.6473247746471316e-05}, {"id": 480, "seek": 332728, "start": 3327.28, "end": 3335.0400000000004, "text": " And I guess we're right at 11 o'clock. But maybe one last question that somebody brought", "tokens": [400, 286, 2041, 321, 434, 558, 412, 2975, 277, 6, 9023, 13, 583, 1310, 472, 1036, 1168, 300, 2618, 3038], "temperature": 0.0, "avg_logprob": -0.26494793315510173, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.00014199112774804235}, {"id": 481, "seek": 332728, "start": 3335.0400000000004, "end": 3343.6800000000003, "text": " up is, is there any future research opportunities in cross machine learning and quantum computing?", "tokens": [493, 307, 11, 307, 456, 604, 2027, 2132, 4786, 294, 3278, 3479, 2539, 293, 13018, 15866, 30], "temperature": 0.0, "avg_logprob": -0.26494793315510173, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.00014199112774804235}, {"id": 482, "seek": 332728, "start": 3343.6800000000003, "end": 3346.2400000000002, "text": " You can think about it. It's an interesting question.", "tokens": [509, 393, 519, 466, 309, 13, 467, 311, 364, 1880, 1168, 13], "temperature": 0.0, "avg_logprob": -0.26494793315510173, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.00014199112774804235}, {"id": 483, "seek": 332728, "start": 3346.2400000000002, "end": 3351.5600000000004, "text": " No, probably not one I've got any expertise on. It could well be an interesting question,", "tokens": [883, 11, 1391, 406, 472, 286, 600, 658, 604, 11769, 322, 13, 467, 727, 731, 312, 364, 1880, 1168, 11], "temperature": 0.0, "avg_logprob": -0.26494793315510173, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.00014199112774804235}, {"id": 484, "seek": 332728, "start": 3351.5600000000004, "end": 3354.5600000000004, "text": " but I'm not the right person to ask.", "tokens": [457, 286, 478, 406, 264, 558, 954, 281, 1029, 13], "temperature": 0.0, "avg_logprob": -0.26494793315510173, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.00014199112774804235}, {"id": 485, "seek": 335456, "start": 3354.56, "end": 3361.56, "text": " One thing I do want to mention is I have just moved back to Australia after 10 years in", "tokens": [1485, 551, 286, 360, 528, 281, 2152, 307, 286, 362, 445, 4259, 646, 281, 7060, 934, 1266, 924, 294], "temperature": 0.0, "avg_logprob": -0.15646554231643678, "compression_ratio": 1.5148936170212766, "no_speech_prob": 8.34385646157898e-05}, {"id": 486, "seek": 335456, "start": 3361.56, "end": 3371.52, "text": " San Francisco. And I am extremely keen to see Australia become an absolute knowledge", "tokens": [5271, 12279, 13, 400, 286, 669, 4664, 20297, 281, 536, 7060, 1813, 364, 8236, 3601], "temperature": 0.0, "avg_logprob": -0.15646554231643678, "compression_ratio": 1.5148936170212766, "no_speech_prob": 8.34385646157898e-05}, {"id": 487, "seek": 335456, "start": 3371.52, "end": 3379.48, "text": " hub around deep learning. And I would particularly love to see our fast AI software. Just like", "tokens": [11838, 926, 2452, 2539, 13, 400, 286, 576, 4098, 959, 281, 536, 527, 2370, 7318, 4722, 13, 1449, 411], "temperature": 0.0, "avg_logprob": -0.15646554231643678, "compression_ratio": 1.5148936170212766, "no_speech_prob": 8.34385646157898e-05}, {"id": 488, "seek": 335456, "start": 3379.48, "end": 3383.7599999999998, "text": " when you think about TensorFlow, you kind of have this whole ecosystem around it, around", "tokens": [562, 291, 519, 466, 37624, 11, 291, 733, 295, 362, 341, 1379, 11311, 926, 309, 11, 926], "temperature": 0.0, "avg_logprob": -0.15646554231643678, "compression_ratio": 1.5148936170212766, "no_speech_prob": 8.34385646157898e-05}, {"id": 489, "seek": 338376, "start": 3383.76, "end": 3390.2400000000002, "text": " Google and startups and all this. I would love to see Australia become, you know, that", "tokens": [3329, 293, 28041, 293, 439, 341, 13, 286, 576, 959, 281, 536, 7060, 1813, 11, 291, 458, 11, 300], "temperature": 0.0, "avg_logprob": -0.15757119764975452, "compression_ratio": 1.7333333333333334, "no_speech_prob": 2.3920229068608023e-05}, {"id": 490, "seek": 338376, "start": 3390.2400000000002, "end": 3395.2000000000003, "text": " fast AI is kind of the homegrown library and that people here will really take it to heart", "tokens": [2370, 7318, 307, 733, 295, 264, 1280, 38413, 6405, 293, 300, 561, 510, 486, 534, 747, 309, 281, 1917], "temperature": 0.0, "avg_logprob": -0.15757119764975452, "compression_ratio": 1.7333333333333334, "no_speech_prob": 2.3920229068608023e-05}, {"id": 491, "seek": 338376, "start": 3395.2000000000003, "end": 3403.2400000000002, "text": " and help us make it brilliant. It's all open source, you know, and we've got a Discord", "tokens": [293, 854, 505, 652, 309, 10248, 13, 467, 311, 439, 1269, 4009, 11, 291, 458, 11, 293, 321, 600, 658, 257, 32623], "temperature": 0.0, "avg_logprob": -0.15757119764975452, "compression_ratio": 1.7333333333333334, "no_speech_prob": 2.3920229068608023e-05}, {"id": 492, "seek": 338376, "start": 3403.2400000000002, "end": 3407.6800000000003, "text": " channel where we all chat about it. And, you know, any organizations that are interested", "tokens": [2269, 689, 321, 439, 5081, 466, 309, 13, 400, 11, 291, 458, 11, 604, 6150, 300, 366, 3102], "temperature": 0.0, "avg_logprob": -0.15757119764975452, "compression_ratio": 1.7333333333333334, "no_speech_prob": 2.3920229068608023e-05}, {"id": 493, "seek": 338376, "start": 3407.6800000000003, "end": 3413.44, "text": " in, you know, taking advantage of this free open source library, I would love to support", "tokens": [294, 11, 291, 458, 11, 1940, 5002, 295, 341, 1737, 1269, 4009, 6405, 11, 286, 576, 959, 281, 1406], "temperature": 0.0, "avg_logprob": -0.15757119764975452, "compression_ratio": 1.7333333333333334, "no_speech_prob": 2.3920229068608023e-05}, {"id": 494, "seek": 341344, "start": 3413.44, "end": 3419.32, "text": " them and see like, you know, academic institutions. I'd love to see this become a really successful", "tokens": [552, 293, 536, 411, 11, 291, 458, 11, 7778, 8142, 13, 286, 1116, 959, 281, 536, 341, 1813, 257, 534, 4406], "temperature": 0.0, "avg_logprob": -0.22252456263491982, "compression_ratio": 1.582995951417004, "no_speech_prob": 4.90765851282049e-05}, {"id": 495, "seek": 341344, "start": 3419.32, "end": 3421.32, "text": " ecosystem here in Australia.", "tokens": [11311, 510, 294, 7060, 13], "temperature": 0.0, "avg_logprob": -0.22252456263491982, "compression_ratio": 1.582995951417004, "no_speech_prob": 4.90765851282049e-05}, {"id": 496, "seek": 341344, "start": 3421.32, "end": 3427.88, "text": " Great. It seems like it's going to be quite useful to solve lots of problems. So I think", "tokens": [3769, 13, 467, 2544, 411, 309, 311, 516, 281, 312, 1596, 4420, 281, 5039, 3195, 295, 2740, 13, 407, 286, 519], "temperature": 0.0, "avg_logprob": -0.22252456263491982, "compression_ratio": 1.582995951417004, "no_speech_prob": 4.90765851282049e-05}, {"id": 497, "seek": 341344, "start": 3427.88, "end": 3434.2400000000002, "text": " it would be good to do. So there are still some questions in the chat. We'll have the", "tokens": [309, 576, 312, 665, 281, 360, 13, 407, 456, 366, 920, 512, 1651, 294, 264, 5081, 13, 492, 603, 362, 264], "temperature": 0.0, "avg_logprob": -0.22252456263491982, "compression_ratio": 1.582995951417004, "no_speech_prob": 4.90765851282049e-05}, {"id": 498, "seek": 341344, "start": 3434.2400000000002, "end": 3440.64, "text": " chat transcript and if there's any questions that Jeremy might be worth addressing from", "tokens": [5081, 24444, 293, 498, 456, 311, 604, 1651, 300, 17809, 1062, 312, 3163, 14329, 490], "temperature": 0.0, "avg_logprob": -0.22252456263491982, "compression_ratio": 1.582995951417004, "no_speech_prob": 4.90765851282049e-05}, {"id": 499, "seek": 344064, "start": 3440.64, "end": 3446.7999999999997, "text": " there, we can think about posting responses to those if there's anything in there. But", "tokens": [456, 11, 321, 393, 519, 466, 15978, 13019, 281, 729, 498, 456, 311, 1340, 294, 456, 13, 583], "temperature": 0.0, "avg_logprob": -0.20853986313093953, "compression_ratio": 1.451219512195122, "no_speech_prob": 6.906672206241637e-05}, {"id": 500, "seek": 344064, "start": 3446.7999999999997, "end": 3453.52, "text": " we can do that after the fact. So thank you everybody for coming and thank Jeremy for", "tokens": [321, 393, 360, 300, 934, 264, 1186, 13, 407, 1309, 291, 2201, 337, 1348, 293, 1309, 17809, 337], "temperature": 0.0, "avg_logprob": -0.20853986313093953, "compression_ratio": 1.451219512195122, "no_speech_prob": 6.906672206241637e-05}, {"id": 501, "seek": 344064, "start": 3453.52, "end": 3454.52, "text": " joining us today.", "tokens": [5549, 505, 965, 13], "temperature": 0.0, "avg_logprob": -0.20853986313093953, "compression_ratio": 1.451219512195122, "no_speech_prob": 6.906672206241637e-05}, {"id": 502, "seek": 344064, "start": 3454.52, "end": 3455.52, "text": " Thanks so much.", "tokens": [2561, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.20853986313093953, "compression_ratio": 1.451219512195122, "no_speech_prob": 6.906672206241637e-05}, {"id": 503, "seek": 344064, "start": 3455.52, "end": 3456.52, "text": " Yeah, it's been great.", "tokens": [865, 11, 309, 311, 668, 869, 13], "temperature": 0.0, "avg_logprob": -0.20853986313093953, "compression_ratio": 1.451219512195122, "no_speech_prob": 6.906672206241637e-05}, {"id": 504, "seek": 345652, "start": 3456.52, "end": 3471.2, "text": " Bye all.", "tokens": [50364, 4621, 439, 13, 51098], "temperature": 0.0, "avg_logprob": -0.857218345006307, "compression_ratio": 0.5, "no_speech_prob": 5.3416824812302366e-05}], "language": "en"}